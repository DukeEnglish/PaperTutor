[
    {
        "title": "Words Worth a Thousand Pictures: Measuring and Understanding Perceptual Variability in Text-to-Image Generation",
        "authors": "Raphael TangXinyu ZhangLixinyu XuYao LuWenyan LiPontus StenetorpJimmy LinFerhan Ture",
        "links": "http://arxiv.org/abs/2406.08482v1",
        "entry_id": "http://arxiv.org/abs/2406.08482v1",
        "pdf_url": "http://arxiv.org/pdf/2406.08482v1",
        "summary": "Diffusion models are the state of the art in text-to-image generation, but\ntheir perceptual variability remains understudied. In this paper, we examine\nhow prompts affect image variability in black-box diffusion-based models. We\npropose W1KP, a human-calibrated measure of variability in a set of images,\nbootstrapped from existing image-pair perceptual distances. Current datasets do\nnot cover recent diffusion models, thus we curate three test sets for\nevaluation. Our best perceptual distance outperforms nine baselines by up to 18\npoints in accuracy, and our calibration matches graded human judgements 78% of\nthe time. Using W1KP, we study prompt reusability and show that Imagen prompts\ncan be reused for 10-50 random seeds before new images become too similar to\nalready generated images, while Stable Diffusion XL and DALL-E 3 can be reused\n50-200 times. Lastly, we analyze 56 linguistic features of real prompts,\nfinding that the prompt's length, CLIP embedding norm, concreteness, and word\nsenses influence variability most. As far as we are aware, we are the first to\nanalyze diffusion variability from a visuolinguistic perspective. Our project\npage is at http://w1kp.com",
        "updated": "2024-06-12 17:59:27 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.08482v1"
    },
    {
        "title": "What If We Recaption Billions of Web Images with LLaMA-3?",
        "authors": "Xianhang LiHaoqin TuMude HuiZeyu WangBingchen ZhaoJunfei XiaoSucheng RenJieru MeiQing LiuHuangjie ZhengYuyin ZhouCihang Xie",
        "links": "http://arxiv.org/abs/2406.08478v1",
        "entry_id": "http://arxiv.org/abs/2406.08478v1",
        "pdf_url": "http://arxiv.org/pdf/2406.08478v1",
        "summary": "Web-crawled image-text pairs are inherently noisy. Prior studies demonstrate\nthat semantically aligning and enriching textual descriptions of these pairs\ncan significantly enhance model training across various vision-language tasks,\nparticularly text-to-image generation. However, large-scale investigations in\nthis area remain predominantly closed-source. Our paper aims to bridge this\ncommunity effort, leveraging the powerful and \\textit{open-sourced} LLaMA-3, a\nGPT-4 level LLM. Our recaptioning pipeline is simple: first, we fine-tune a\nLLaMA-3-8B powered LLaVA-1.5 and then employ it to recaption 1.3 billion images\nfrom the DataComp-1B dataset. Our empirical results confirm that this enhanced\ndataset, Recap-DataComp-1B, offers substantial benefits in training advanced\nvision-language models. For discriminative models like CLIP, we observe\nenhanced zero-shot performance in cross-modal retrieval tasks. For generative\nmodels like text-to-image Diffusion Transformers, the generated images exhibit\na significant improvement in alignment with users' text instructions,\nespecially in following complex queries. Our project page is\nhttps://www.haqtu.me/Recap-Datacomp-1B/",
        "updated": "2024-06-12 17:59:07 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.08478v1"
    },
    {
        "title": "Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing",
        "authors": "Zhangchen XuFengqing JiangLuyao NiuYuntian DengRadha PoovendranYejin ChoiBill Yuchen Lin",
        "links": "http://arxiv.org/abs/2406.08464v1",
        "entry_id": "http://arxiv.org/abs/2406.08464v1",
        "pdf_url": "http://arxiv.org/pdf/2406.08464v1",
        "summary": "High-quality instruction data is critical for aligning large language models\n(LLMs). Although some models, such as Llama-3-Instruct, have open weights,\ntheir alignment data remain private, which hinders the democratization of AI.\nHigh human labor costs and a limited, predefined scope for prompting prevent\nexisting open-source data creation methods from scaling effectively,\npotentially limiting the diversity and quality of public alignment datasets. Is\nit possible to synthesize high-quality instruction data at scale by extracting\nit directly from an aligned LLM? We present a self-synthesis method for\ngenerating large-scale alignment data named Magpie. Our key observation is that\naligned LLMs like Llama-3-Instruct can generate a user query when we input only\nthe left-side templates up to the position reserved for user messages, thanks\nto their auto-regressive nature. We use this method to prompt Llama-3-Instruct\nand generate 4 million instructions along with their corresponding responses.\nWe perform a comprehensive analysis of the extracted data and select 300K\nhigh-quality instances. To compare Magpie data with other public instruction\ndatasets, we fine-tune Llama-3-8B-Base with each dataset and evaluate the\nperformance of the fine-tuned models. Our results indicate that in some tasks,\nmodels fine-tuned with Magpie perform comparably to the official\nLlama-3-8B-Instruct, despite the latter being enhanced with 10 million data\npoints through supervised fine-tuning (SFT) and subsequent feedback learning.\nWe also show that using Magpie solely for SFT can surpass the performance of\nprevious public datasets utilized for both SFT and preference optimization,\nsuch as direct preference optimization with UltraFeedback. This advantage is\nevident on alignment benchmarks such as AlpacaEval, ArenaHard, and WildBench.",
        "updated": "2024-06-12 17:52:30 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.08464v1"
    },
    {
        "title": "The Impact of Initialization on LoRA Finetuning Dynamics",
        "authors": "Soufiane HayouNikhil GhoshBin Yu",
        "links": "http://arxiv.org/abs/2406.08447v1",
        "entry_id": "http://arxiv.org/abs/2406.08447v1",
        "pdf_url": "http://arxiv.org/pdf/2406.08447v1",
        "summary": "In this paper, we study the role of initialization in Low Rank Adaptation\n(LoRA) as originally introduced in Hu et al. (2021). Essentially, to start from\nthe pretrained model as initialization for finetuning, one can either\ninitialize B to zero and A to random (default initialization in PEFT package),\nor vice-versa. In both cases, the product BA is equal to zero at\ninitialization, which makes finetuning starts from the pretrained model. These\ntwo initialization schemes are seemingly similar. They should in-principle\nyield the same performance and share the same optimal learning rate. We\ndemonstrate that this is an incorrect intuition and that the first scheme\n(initializing B to zero and A to random) on average yields better performance\ncompared to the other scheme. Our theoretical analysis shows that the reason\nbehind this might be that the first initialization allows the use of larger\nlearning rates (without causing output instability) compared to the second\ninitialization, resulting in more efficient learning of the first scheme. We\nvalidate our results with extensive experiments on LLMs.",
        "updated": "2024-06-12 17:38:20 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.08447v1"
    },
    {
        "title": "OLMES: A Standard for Language Model Evaluations",
        "authors": "Yuling GuOyvind TafjordBailey KuehlDany HaddadJesse DodgeHannaneh Hajishirzi",
        "links": "http://arxiv.org/abs/2406.08446v1",
        "entry_id": "http://arxiv.org/abs/2406.08446v1",
        "pdf_url": "http://arxiv.org/pdf/2406.08446v1",
        "summary": "Progress in AI is often demonstrated by new models claiming improved\nperformance on tasks measuring model capabilities. Evaluating language models\nin particular is challenging, as small changes to how a model is evaluated on a\ntask can lead to large changes in measured performance. There is no common\nstandard setup, so different models are evaluated on the same tasks in\ndifferent ways, leading to claims about which models perform best not being\nreproducible. We propose OLMES, a completely documented, practical, open\nstandard for reproducible LLM evaluations. In developing this standard, we\nidentify and review the varying factors in evaluation practices adopted by the\ncommunity - such as details of prompt formatting, choice of in-context\nexamples, probability normalizations, and task formulation. In particular,\nOLMES supports meaningful comparisons between smaller base models that require\nthe unnatural \"cloze\" formulation of multiple-choice questions against larger\nmodels that can utilize the original formulation. OLMES includes\nwell-considered recommendations guided by results from existing literature as\nwell as new experiments investigating open questions.",
        "updated": "2024-06-12 17:37:09 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.08446v1"
    }
]