[
    {
        "title": "ICE-G: Image Conditional Editing of 3D Gaussian Splats",
        "authors": "Vishnu JaganathanHannah Hanyun HuangMuhammad Zubair IrshadVarun JampaniAmit RajZsolt Kira",
        "links": "http://arxiv.org/abs/2406.08488v1",
        "entry_id": "http://arxiv.org/abs/2406.08488v1",
        "pdf_url": "http://arxiv.org/pdf/2406.08488v1",
        "summary": "Recently many techniques have emerged to create high quality 3D assets and\nscenes. When it comes to editing of these objects, however, existing approaches\nare either slow, compromise on quality, or do not provide enough customization.\nWe introduce a novel approach to quickly edit a 3D model from a single\nreference view. Our technique first segments the edit image, and then matches\nsemantically corresponding regions across chosen segmented dataset views using\nDINO features. A color or texture change from a particular region of the edit\nimage can then be applied to other views automatically in a semantically\nsensible manner. These edited views act as an updated dataset to further train\nand re-style the 3D scene. The end-result is therefore an edited 3D model. Our\nframework enables a wide variety of editing tasks such as manual local edits,\ncorrespondence based style transfer from any example image, and a combination\nof different styles from multiple example images. We use Gaussian Splats as our\nprimary 3D representation due to their speed and ease of local editing, but our\ntechnique works for other methods such as NeRFs as well. We show through\nmultiple examples that our method produces higher quality results while\noffering fine-grained control of editing. Project page: ice-gaussian.github.io",
        "updated": "2024-06-12 17:59:52 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.08488v1"
    },
    {
        "title": "Beyond LLaVA-HD: Diving into High-Resolution Large Multimodal Models",
        "authors": "Yi-Fan ZhangQingsong WenChaoyou FuXue WangZhang ZhangLiang WangRong Jin",
        "links": "http://arxiv.org/abs/2406.08487v1",
        "entry_id": "http://arxiv.org/abs/2406.08487v1",
        "pdf_url": "http://arxiv.org/pdf/2406.08487v1",
        "summary": "Seeing clearly with high resolution is a foundation of Large Multimodal\nModels (LMMs), which has been proven to be vital for visual perception and\nreasoning. Existing works usually employ a straightforward resolution upscaling\nmethod, where the image consists of global and local branches, with the latter\nbeing the sliced image patches but resized to the same resolution as the\nformer. This means that higher resolution requires more local patches,\nresulting in exorbitant computational expenses, and meanwhile, the dominance of\nlocal image tokens may diminish the global context. In this paper, we dive into\nthe problems and propose a new framework as well as an elaborate optimization\nstrategy. Specifically, we extract contextual information from the global view\nusing a mixture of adapters, based on the observation that different adapters\nexcel at different tasks. With regard to local patches, learnable query\nembeddings are introduced to reduce image tokens, the most important tokens\naccounting for the user question will be further selected by a similarity-based\nselector. Our empirical results demonstrate a `less is more' pattern, where\n\\textit{utilizing fewer but more informative local image tokens leads to\nimproved performance}. Besides, a significant challenge lies in the training\nstrategy, as simultaneous end-to-end training of the global mining block and\nlocal compression block does not yield optimal results. We thus advocate for an\nalternating training way, ensuring balanced learning between global and local\naspects. Finally, we also introduce a challenging dataset with high\nrequirements for image detail, enhancing the training of the local compression\nlayer. The proposed method, termed LMM with Sophisticated Tasks, Local image\ncompression, and Mixture of global Experts (SliME), achieves leading\nperformance across various benchmarks with only 2 million training data.",
        "updated": "2024-06-12 17:59:49 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.08487v1"
    },
    {
        "title": "On Evaluating Adversarial Robustness of Volumetric Medical Segmentation Models",
        "authors": "Hashmat Shadab MalikNuman SaeedAsif HanifMuzammal NaseerMohammad YaqubSalman KhanFahad Shahbaz Khan",
        "links": "http://arxiv.org/abs/2406.08486v1",
        "entry_id": "http://arxiv.org/abs/2406.08486v1",
        "pdf_url": "http://arxiv.org/pdf/2406.08486v1",
        "summary": "Volumetric medical segmentation models have achieved significant success on\norgan and tumor-based segmentation tasks in recent years. However, their\nvulnerability to adversarial attacks remains largely unexplored, raising\nserious concerns regarding the real-world deployment of tools employing such\nmodels in the healthcare sector. This underscores the importance of\ninvestigating the robustness of existing models. In this context, our work aims\nto empirically examine the adversarial robustness across current volumetric\nsegmentation architectures, encompassing Convolutional, Transformer, and\nMamba-based models. We extend this investigation across four volumetric\nsegmentation datasets, evaluating robustness under both white box and black box\nadversarial attacks. Overall, we observe that while both pixel and\nfrequency-based attacks perform reasonably well under white box setting, the\nlatter performs significantly better under transfer-based black box attacks.\nAcross our experiments, we observe transformer-based models show higher\nrobustness than convolution-based models with Mamba-based models being the most\nvulnerable. Additionally, we show that large-scale training of volumetric\nsegmentation models improves the model's robustness against adversarial\nattacks. The code and pretrained models will be made available at\nhttps://github.com/HashmatShadab/Robustness-of-Volumetric-Medical-Segmentation-Models.",
        "updated": "2024-06-12 17:59:42 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.08486v1"
    },
    {
        "title": "Words Worth a Thousand Pictures: Measuring and Understanding Perceptual Variability in Text-to-Image Generation",
        "authors": "Raphael TangXinyu ZhangLixinyu XuYao LuWenyan LiPontus StenetorpJimmy LinFerhan Ture",
        "links": "http://arxiv.org/abs/2406.08482v1",
        "entry_id": "http://arxiv.org/abs/2406.08482v1",
        "pdf_url": "http://arxiv.org/pdf/2406.08482v1",
        "summary": "Diffusion models are the state of the art in text-to-image generation, but\ntheir perceptual variability remains understudied. In this paper, we examine\nhow prompts affect image variability in black-box diffusion-based models. We\npropose W1KP, a human-calibrated measure of variability in a set of images,\nbootstrapped from existing image-pair perceptual distances. Current datasets do\nnot cover recent diffusion models, thus we curate three test sets for\nevaluation. Our best perceptual distance outperforms nine baselines by up to 18\npoints in accuracy, and our calibration matches graded human judgements 78% of\nthe time. Using W1KP, we study prompt reusability and show that Imagen prompts\ncan be reused for 10-50 random seeds before new images become too similar to\nalready generated images, while Stable Diffusion XL and DALL-E 3 can be reused\n50-200 times. Lastly, we analyze 56 linguistic features of real prompts,\nfinding that the prompt's length, CLIP embedding norm, concreteness, and word\nsenses influence variability most. As far as we are aware, we are the first to\nanalyze diffusion variability from a visuolinguistic perspective. Our project\npage is at http://w1kp.com",
        "updated": "2024-06-12 17:59:27 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.08482v1"
    },
    {
        "title": "Enhancing End-to-End Autonomous Driving with Latent World Model",
        "authors": "Yingyan LiLue FanJiawei HeYuqi WangYuntao ChenZhaoxiang ZhangTieniu Tan",
        "links": "http://arxiv.org/abs/2406.08481v1",
        "entry_id": "http://arxiv.org/abs/2406.08481v1",
        "pdf_url": "http://arxiv.org/pdf/2406.08481v1",
        "summary": "End-to-end autonomous driving has garnered widespread attention. Current\nend-to-end approaches largely rely on the supervision from perception tasks\nsuch as detection, tracking, and map segmentation to aid in learning scene\nrepresentations. However, these methods require extensive annotations,\nhindering the data scalability. To address this challenge, we propose a novel\nself-supervised method to enhance end-to-end driving without the need for\ncostly labels. Specifically, our framework \\textbf{LAW} uses a LAtent World\nmodel to predict future latent features based on the predicted ego actions and\nthe latent feature of the current frame. The predicted latent features are\nsupervised by the actually observed features in the future. This supervision\njointly optimizes the latent feature learning and action prediction, which\ngreatly enhances the driving performance. As a result, our approach achieves\nstate-of-the-art performance in both open-loop and closed-loop benchmarks\nwithout costly annotations.",
        "updated": "2024-06-12 17:59:21 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.08481v1"
    }
]