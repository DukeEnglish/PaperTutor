[
    {
        "title": "ICE-G: Image Conditional Editing of 3D Gaussian Splats",
        "authors": "Vishnu JaganathanHannah Hanyun HuangMuhammad Zubair IrshadVarun JampaniAmit RajZsolt Kira",
        "links": "http://arxiv.org/abs/2406.08488v1",
        "entry_id": "http://arxiv.org/abs/2406.08488v1",
        "pdf_url": "http://arxiv.org/pdf/2406.08488v1",
        "summary": "Recently many techniques have emerged to create high quality 3D assets and\nscenes. When it comes to editing of these objects, however, existing approaches\nare either slow, compromise on quality, or do not provide enough customization.\nWe introduce a novel approach to quickly edit a 3D model from a single\nreference view. Our technique first segments the edit image, and then matches\nsemantically corresponding regions across chosen segmented dataset views using\nDINO features. A color or texture change from a particular region of the edit\nimage can then be applied to other views automatically in a semantically\nsensible manner. These edited views act as an updated dataset to further train\nand re-style the 3D scene. The end-result is therefore an edited 3D model. Our\nframework enables a wide variety of editing tasks such as manual local edits,\ncorrespondence based style transfer from any example image, and a combination\nof different styles from multiple example images. We use Gaussian Splats as our\nprimary 3D representation due to their speed and ease of local editing, but our\ntechnique works for other methods such as NeRFs as well. We show through\nmultiple examples that our method produces higher quality results while\noffering fine-grained control of editing. Project page: ice-gaussian.github.io",
        "updated": "2024-06-12 17:59:52 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.08488v1"
    },
    {
        "title": "Real2Code: Reconstruct Articulated Objects via Code Generation",
        "authors": "Zhao MandiYijia WengDominik BauerShuran Song",
        "links": "http://arxiv.org/abs/2406.08474v1",
        "entry_id": "http://arxiv.org/abs/2406.08474v1",
        "pdf_url": "http://arxiv.org/pdf/2406.08474v1",
        "summary": "We present Real2Code, a novel approach to reconstructing articulated objects\nvia code generation. Given visual observations of an object, we first\nreconstruct its part geometry using an image segmentation model and a shape\ncompletion model. We then represent the object parts with oriented bounding\nboxes, which are input to a fine-tuned large language model (LLM) to predict\njoint articulation as code. By leveraging pre-trained vision and language\nmodels, our approach scales elegantly with the number of articulated parts, and\ngeneralizes from synthetic training data to real world objects in unstructured\nenvironments. Experimental results demonstrate that Real2Code significantly\noutperforms previous state-of-the-art in reconstruction accuracy, and is the\nfirst approach to extrapolate beyond objects' structural complexity in the\ntraining set, and reconstructs objects with up to 10 articulated parts. When\nincorporated with a stereo reconstruction model, Real2Code also generalizes to\nreal world objects from a handful of multi-view RGB images, without the need\nfor depth or camera information.",
        "updated": "2024-06-12 17:57:06 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.08474v1"
    },
    {
        "title": "Strategies for Pretraining Neural Operators",
        "authors": "Anthony ZhouCooper LorsungAmirPouya HemmasianAmir Barati Farimani",
        "links": "http://arxiv.org/abs/2406.08473v1",
        "entry_id": "http://arxiv.org/abs/2406.08473v1",
        "pdf_url": "http://arxiv.org/pdf/2406.08473v1",
        "summary": "Pretraining for partial differential equation (PDE) modeling has recently\nshown promise in scaling neural operators across datasets to improve\ngeneralizability and performance. Despite these advances, our understanding of\nhow pretraining affects neural operators is still limited; studies generally\npropose tailored architectures and datasets that make it challenging to compare\nor examine different pretraining frameworks. To address this, we compare\nvarious pretraining methods without optimizing architecture choices to\ncharacterize pretraining dynamics on different models and datasets as well as\nto understand its scaling and generalization behavior. We find that pretraining\nis highly dependent on model and dataset choices, but in general transfer\nlearning or physics-based pretraining strategies work best. In addition,\npretraining performance can be further improved by using data augmentations.\nLastly, pretraining is additionally beneficial when fine-tuning in scarce data\nregimes or when generalizing to downstream data similar to the pretraining\ndistribution. Through providing insights into pretraining neural operators for\nphysics prediction, we hope to motivate future work in developing and\nevaluating pretraining methods for PDEs.",
        "updated": "2024-06-12 17:56:46 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.08473v1"
    },
    {
        "title": "RILe: Reinforced Imitation Learning",
        "authors": "Mert AlbabaSammy ChristenChristoph GebhardtThomas LangarekMichael J. BlackOtmar Hilliges",
        "links": "http://arxiv.org/abs/2406.08472v1",
        "entry_id": "http://arxiv.org/abs/2406.08472v1",
        "pdf_url": "http://arxiv.org/pdf/2406.08472v1",
        "summary": "Reinforcement Learning has achieved significant success in generating complex\nbehavior but often requires extensive reward function engineering. Adversarial\nvariants of Imitation Learning and Inverse Reinforcement Learning offer an\nalternative by learning policies from expert demonstrations via a\ndiscriminator. Employing discriminators increases their data- and computational\nefficiency over the standard approaches; however, results in sensitivity to\nimperfections in expert data. We propose RILe, a teacher-student system that\nachieves both robustness to imperfect data and efficiency. In RILe, the student\nlearns an action policy while the teacher dynamically adjusts a reward function\nbased on the student's performance and its alignment with expert\ndemonstrations. By tailoring the reward function to both performance of the\nstudent and expert similarity, our system reduces dependence on the\ndiscriminator and, hence, increases robustness against data imperfections.\nExperiments show that RILe outperforms existing methods by 2x in settings with\nlimited or noisy expert data.",
        "updated": "2024-06-12 17:56:31 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.08472v1"
    },
    {
        "title": "PAL: Pluralistic Alignment Framework for Learning from Heterogeneous Preferences",
        "authors": "Daiwei ChenYi ChenAniket RegeRamya Korlakai Vinayak",
        "links": "http://arxiv.org/abs/2406.08469v1",
        "entry_id": "http://arxiv.org/abs/2406.08469v1",
        "pdf_url": "http://arxiv.org/pdf/2406.08469v1",
        "summary": "Large foundation models pretrained on raw web-scale data are not readily\ndeployable without additional step of extensive alignment to human preferences.\nSuch alignment is typically done by collecting large amounts of pairwise\ncomparisons from humans (\"Do you prefer output A or B?\") and learning a reward\nmodel or a policy with the Bradley-Terry-Luce (BTL) model as a proxy for a\nhuman's underlying implicit preferences. These methods generally suffer from\nassuming a universal preference shared by all humans, which lacks the\nflexibility of adapting to plurality of opinions and preferences. In this work,\nwe propose PAL, a framework to model human preference complementary to existing\npretraining strategies, which incorporates plurality from the ground up. We\npropose using the ideal point model as a lens to view alignment using\npreference comparisons. Together with our novel reformulation and using mixture\nmodeling, our framework captures the plurality of population preferences while\nsimultaneously learning a common preference latent space across different\npreferences, which can few-shot generalize to new, unseen users. Our approach\nenables us to use the penultimate-layer representation of large foundation\nmodels and simple MLP layers to learn reward functions that are on-par with the\nexisting large state-of-the-art reward models, thereby enhancing efficiency of\nreward modeling significantly. We show that PAL achieves competitive reward\nmodel accuracy compared to strong baselines on 1) Language models with Summary\ndataset ; 2) Image Generative models with Pick-a-Pic dataset ; 3) A new\nsemisynthetic heterogeneous dataset generated using Anthropic Personas.\nFinally, our experiments also highlight the shortcoming of current preference\ndatasets that are created using rigid rubrics which wash away heterogeneity,\nand call for more nuanced data collection approaches.",
        "updated": "2024-06-12 17:54:54 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.08469v1"
    }
]