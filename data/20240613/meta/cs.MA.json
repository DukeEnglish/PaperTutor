[
    {
        "title": "Adaptive Swarm Mesh Refinement using Deep Reinforcement Learning with Local Rewards",
        "authors": "Niklas FreymuthPhilipp DahlingerTobias WürthSimon ReischLuise KärgerGerhard Neumann",
        "links": "http://arxiv.org/abs/2406.08440v1",
        "entry_id": "http://arxiv.org/abs/2406.08440v1",
        "pdf_url": "http://arxiv.org/pdf/2406.08440v1",
        "summary": "Simulating physical systems is essential in engineering, but analytical\nsolutions are limited to straightforward problems. Consequently, numerical\nmethods like the Finite Element Method (FEM) are widely used. However, the FEM\nbecomes computationally expensive as problem complexity and accuracy demands\nincrease. Adaptive Mesh Refinement (AMR) improves the FEM by dynamically\nallocating mesh elements on the domain, balancing computational speed and\naccuracy. Classical AMR depends on heuristics or expensive error estimators,\nlimiting its use in complex simulations. While learning-based AMR methods are\npromising, they currently only scale to simple problems. In this work, we\nformulate AMR as a system of collaborating, homogeneous agents that iteratively\nsplit into multiple new agents. This agent-wise perspective enables a spatial\nreward formulation focused on reducing the maximum mesh element error. Our\napproach, Adaptive Swarm Mesh Refinement (ASMR), offers efficient, stable\noptimization and generates highly adaptive meshes at user-defined resolution\nduring inference. Extensive experiments, including volumetric meshes and\nNeumann boundary conditions, demonstrate that ASMR exceeds heuristic approaches\nand learned baselines, matching the performance of expensive error-based oracle\nAMR strategies. ASMR additionally generalizes to different domains during\ninference, and produces meshes that simulate up to 2 orders of magnitude faster\nthan uniform refinements in more demanding settings.",
        "updated": "2024-06-12 17:26:54 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.08440v1"
    },
    {
        "title": "Efficient Adaptation in Mixed-Motive Environments via Hierarchical Opponent Modeling and Planning",
        "authors": "Yizhe HuangAnji LiuFanqi KongYaodong YangSong-Chun ZhuXue Feng",
        "links": "http://arxiv.org/abs/2406.08002v1",
        "entry_id": "http://arxiv.org/abs/2406.08002v1",
        "pdf_url": "http://arxiv.org/pdf/2406.08002v1",
        "summary": "Despite the recent successes of multi-agent reinforcement learning (MARL)\nalgorithms, efficiently adapting to co-players in mixed-motive environments\nremains a significant challenge. One feasible approach is to hierarchically\nmodel co-players' behavior based on inferring their characteristics. However,\nthese methods often encounter difficulties in efficient reasoning and\nutilization of inferred information. To address these issues, we propose\nHierarchical Opponent modeling and Planning (HOP), a novel multi-agent\ndecision-making algorithm that enables few-shot adaptation to unseen policies\nin mixed-motive environments. HOP is hierarchically composed of two modules: an\nopponent modeling module that infers others' goals and learns corresponding\ngoal-conditioned policies, and a planning module that employs Monte Carlo Tree\nSearch (MCTS) to identify the best response. Our approach improves efficiency\nby updating beliefs about others' goals both across and within episodes and by\nusing information from the opponent modeling module to guide planning.\nExperimental results demonstrate that in mixed-motive environments, HOP\nexhibits superior few-shot adaptation capabilities when interacting with\nvarious unseen agents, and excels in self-play scenarios. Furthermore, the\nemergence of social intelligence during our experiments underscores the\npotential of our approach in complex multi-agent environments.",
        "updated": "2024-06-12 08:48:06 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.08002v1"
    },
    {
        "title": "Carbon Market Simulation with Adaptive Mechanism Design",
        "authors": "Han WangWenhao LiHongyuan ZhaBaoxiang Wang",
        "links": "http://arxiv.org/abs/2406.07875v1",
        "entry_id": "http://arxiv.org/abs/2406.07875v1",
        "pdf_url": "http://arxiv.org/pdf/2406.07875v1",
        "summary": "A carbon market is a market-based tool that incentivizes economic agents to\nalign individual profits with the global utility, i.e., reducing carbon\nemissions to tackle climate change.\n  \\textit{Cap and trade} stands as a critical principle based on allocating and\ntrading carbon allowances (carbon emission credit), enabling economic agents to\nfollow planned emissions and penalizing excess emissions.\n  A central authority is responsible for introducing and allocating those\nallowances in cap and trade.\n  However, the complexity of carbon market dynamics makes accurate simulation\nintractable, which in turn hinders the design of effective allocation\nstrategies.\n  To address this, we propose an adaptive mechanism design framework,\nsimulating the market using hierarchical, model-free multi-agent reinforcement\nlearning (MARL).\n  Government agents allocate carbon credits, while enterprises engage in\neconomic activities and carbon trading.\n  This framework illustrates agents' behavior comprehensively.\n  Numerical results show MARL enables government agents to balance\nproductivity, equality, and carbon emissions.\n  Our project is available at\n\\url{https://github.com/xwanghan/Carbon-Simulator}.",
        "updated": "2024-06-12 05:08:51 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.07875v1"
    },
    {
        "title": "Multi-agent Reinforcement Learning with Deep Networks for Diverse Q-Vectors",
        "authors": "Zhenglong LuoZhiyong ChenJames Welsh",
        "links": "http://arxiv.org/abs/2406.07848v1",
        "entry_id": "http://arxiv.org/abs/2406.07848v1",
        "pdf_url": "http://arxiv.org/pdf/2406.07848v1",
        "summary": "Multi-agent reinforcement learning (MARL) has become a significant research\ntopic due to its ability to facilitate learning in complex environments. In\nmulti-agent tasks, the state-action value, commonly referred to as the Q-value,\ncan vary among agents because of their individual rewards, resulting in a\nQ-vector. Determining an optimal policy is challenging, as it involves more\nthan just maximizing a single Q-value. Various optimal policies, such as a Nash\nequilibrium, have been studied in this context. Algorithms like Nash Q-learning\nand Nash Actor-Critic have shown effectiveness in these scenarios. This paper\nextends this research by proposing a deep Q-networks (DQN) algorithm capable of\nlearning various Q-vectors using Max, Nash, and Maximin strategies. The\neffectiveness of this approach is demonstrated in an environment where dual\nrobotic arms collaborate to lift a pot.",
        "updated": "2024-06-12 03:30:10 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.07848v1"
    },
    {
        "title": "Choreographing the Rhythms of Observation: Dynamics for Ranged Observer Bipartite-Unipartite SpatioTemporal (ROBUST) Networks",
        "authors": "Ted Edward Holmberg",
        "links": "http://arxiv.org/abs/2406.07473v1",
        "entry_id": "http://arxiv.org/abs/2406.07473v1",
        "pdf_url": "http://arxiv.org/pdf/2406.07473v1",
        "summary": "Existing network analysis methods struggle to optimize observer placements in\ndynamic environments with limited visibility. This dissertation introduces the\nnovel ROBUST (Ranged Observer Bipartite-Unipartite SpatioTemporal) framework,\noffering a significant advancement in modeling, analyzing, and optimizing\nobserver networks within complex spatiotemporal domains. ROBUST leverages a\nunique bipartite-unipartite approach, distinguishing between observer and\nobservable entities while incorporating spatial constraints and temporal\ndynamics.\n  This research extends spatiotemporal network theory by introducing novel\ngraph-based measures, including myopic degree, spatial closeness centrality,\nand edge length proportion. These measures, coupled with advanced clustering\ntechniques like Proximal Recurrence, provide insights into network structure,\nresilience, and the effectiveness of observer placements. The ROBUST framework\ndemonstrates superior resource allocation and strategic responsiveness compared\nto conventional models. Case studies in oceanographic monitoring, urban safety\nnetworks, and multi-agent path planning showcases its practical applicability\nand adaptability. Results demonstrate significant improvements in coverage,\nresponse times, and overall network efficiency.\n  This work paves the way for future research in incorporating imperfect\nknowledge, refining temporal pathing methodologies, and expanding the scope of\napplications. By bridging theoretical advancements with practical solutions,\nROBUST stands as a significant contribution to the field, promising to inform\nand inspire ongoing and future endeavors in network optimization and\nmulti-agent system planning.",
        "updated": "2024-06-11 17:20:01 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.07473v1"
    }
]