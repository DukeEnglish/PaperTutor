[
    {
        "title": "Scaling Laws in Linear Regression: Compute, Parameters, and Data",
        "authors": "Licong LinJingfeng WuSham M. KakadePeter L. BartlettJason D. Lee",
        "links": "http://arxiv.org/abs/2406.08466v1",
        "entry_id": "http://arxiv.org/abs/2406.08466v1",
        "pdf_url": "http://arxiv.org/pdf/2406.08466v1",
        "summary": "Empirically, large-scale deep learning models often satisfy a neural scaling\nlaw: the test error of the trained model improves polynomially as the model\nsize and data size grow. However, conventional wisdom suggests the test error\nconsists of approximation, bias, and variance errors, where the variance error\nincreases with model size. This disagrees with the general form of neural\nscaling laws, which predict that increasing model size monotonically improves\nperformance.\n  We study the theory of scaling laws in an infinite dimensional linear\nregression setup. Specifically, we consider a model with $M$ parameters as a\nlinear function of sketched covariates. The model is trained by one-pass\nstochastic gradient descent (SGD) using $N$ data. Assuming the optimal\nparameter satisfies a Gaussian prior and the data covariance matrix has a\npower-law spectrum of degree $a>1$, we show that the reducible part of the test\nerror is $\\Theta(M^{-(a-1)} + N^{-(a-1)/a})$. The variance error, which\nincreases with $M$, is dominated by the other errors due to the implicit\nregularization of SGD, thus disappearing from the bound. Our theory is\nconsistent with the empirical neural scaling laws and verified by numerical\nsimulation.",
        "updated": "2024-06-12 17:53:29 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.08466v1"
    },
    {
        "title": "The Impact of Initialization on LoRA Finetuning Dynamics",
        "authors": "Soufiane HayouNikhil GhoshBin Yu",
        "links": "http://arxiv.org/abs/2406.08447v1",
        "entry_id": "http://arxiv.org/abs/2406.08447v1",
        "pdf_url": "http://arxiv.org/pdf/2406.08447v1",
        "summary": "In this paper, we study the role of initialization in Low Rank Adaptation\n(LoRA) as originally introduced in Hu et al. (2021). Essentially, to start from\nthe pretrained model as initialization for finetuning, one can either\ninitialize B to zero and A to random (default initialization in PEFT package),\nor vice-versa. In both cases, the product BA is equal to zero at\ninitialization, which makes finetuning starts from the pretrained model. These\ntwo initialization schemes are seemingly similar. They should in-principle\nyield the same performance and share the same optimal learning rate. We\ndemonstrate that this is an incorrect intuition and that the first scheme\n(initializing B to zero and A to random) on average yields better performance\ncompared to the other scheme. Our theoretical analysis shows that the reason\nbehind this might be that the first initialization allows the use of larger\nlearning rates (without causing output instability) compared to the second\ninitialization, resulting in more efficient learning of the first scheme. We\nvalidate our results with extensive experiments on LLMs.",
        "updated": "2024-06-12 17:38:20 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.08447v1"
    },
    {
        "title": "Nyström Kernel Stein Discrepancy",
        "authors": "Florian KalinkeZoltan SzaboBharath K. Sriperumbudur",
        "links": "http://arxiv.org/abs/2406.08401v1",
        "entry_id": "http://arxiv.org/abs/2406.08401v1",
        "pdf_url": "http://arxiv.org/pdf/2406.08401v1",
        "summary": "Kernel methods underpin many of the most successful approaches in data\nscience and statistics, and they allow representing probability measures as\nelements of a reproducing kernel Hilbert space without loss of information.\nRecently, the kernel Stein discrepancy (KSD), which combines Stein's method\nwith kernel techniques, gained considerable attention. Through the Stein\noperator, KSD allows the construction of powerful goodness-of-fit tests where\nit is sufficient to know the target distribution up to a multiplicative\nconstant. However, the typical U- and V-statistic-based KSD estimators suffer\nfrom a quadratic runtime complexity, which hinders their application in\nlarge-scale settings. In this work, we propose a Nystr\\\"om-based KSD\nacceleration -- with runtime $\\mathcal O\\!\\left(mn+m^3\\right)$ for $n$ samples\nand $m\\ll n$ Nystr\\\"om points -- , show its $\\sqrt{n}$-consistency under the\nnull with a classical sub-Gaussian assumption, and demonstrate its\napplicability for goodness-of-fit testing on a suite of benchmarks.",
        "updated": "2024-06-12 16:50:12 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.08401v1"
    },
    {
        "title": "Differentiable Cost-Parameterized Monge Map Estimators",
        "authors": "Samuel HowardGeorge DeligiannidisPatrick RebeschiniJames Thornton",
        "links": "http://arxiv.org/abs/2406.08399v1",
        "entry_id": "http://arxiv.org/abs/2406.08399v1",
        "pdf_url": "http://arxiv.org/pdf/2406.08399v1",
        "summary": "Within the field of optimal transport (OT), the choice of ground cost is\ncrucial to ensuring that the optimality of a transport map corresponds to\nusefulness in real-world applications. It is therefore desirable to use known\ninformation to tailor cost functions and hence learn OT maps which are adapted\nto the problem at hand. By considering a class of neural ground costs whose\nMonge maps have a known form, we construct a differentiable Monge map estimator\nwhich can be optimized to be consistent with known information about an OT map.\nIn doing so, we simultaneously learn both an OT map estimator and a\ncorresponding adapted cost function. Through suitable choices of loss function,\nour method provides a general approach for incorporating prior information\nabout the Monge map itself when learning adapted OT maps and cost functions.",
        "updated": "2024-06-12 16:47:54 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.08399v1"
    },
    {
        "title": "Large Language Models Must Be Taught to Know What They Don't Know",
        "authors": "Sanyam KapoorNate GruverManley RobertsKatherine CollinsArka PalUmang BhattAdrian WellerSamuel DooleyMicah GoldblumAndrew Gordon Wilson",
        "links": "http://arxiv.org/abs/2406.08391v1",
        "entry_id": "http://arxiv.org/abs/2406.08391v1",
        "pdf_url": "http://arxiv.org/pdf/2406.08391v1",
        "summary": "When using large language models (LLMs) in high-stakes applications, we need\nto know when we can trust their predictions. Some works argue that prompting\nhigh-performance LLMs is sufficient to produce calibrated uncertainties, while\nothers introduce sampling methods that can be prohibitively expensive. In this\nwork, we first argue that prompting on its own is insufficient to achieve good\ncalibration and then show that fine-tuning on a small dataset of correct and\nincorrect answers can create an uncertainty estimate with good generalization\nand small computational overhead. We show that a thousand graded examples are\nsufficient to outperform baseline methods and that training through the\nfeatures of a model is necessary for good performance and tractable for large\nopen-source models when using LoRA. We also investigate the mechanisms that\nenable reliable LLM uncertainty estimation, finding that many models can be\nused as general-purpose uncertainty estimators, applicable not just to their\nown uncertainties but also the uncertainty of other models. Lastly, we show\nthat uncertainty estimates inform human use of LLMs in human-AI collaborative\nsettings through a user study.",
        "updated": "2024-06-12 16:41:31 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.08391v1"
    }
]