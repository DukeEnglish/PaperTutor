Multi-agent Reinforcement Learning with Deep
Networks for Diverse Q-Vectors
Zhenglong Luo, Zhiyong Chen, and James Welsh
Abstract—Multi-agent reinforcement learning (MARL) has with the growth of the state and action space. In MARL, this
become a significant research topic due to its ability to facilitate issueisexacerbatedasthenumberofagentsincreases,leading
learningincomplexenvironments.Inmulti-agenttasks,thestate-
to a substantial increase in the size of the virtual Q-table.
action value, commonly referred to as the Q-value, can vary
Consequently, the algorithm is only practical for relatively
among agents because of their individual rewards, resulting in
a Q-vector. Determining an optimal policy is challenging, as it small, discrete state and action spaces.
involves more than just maximizing a single Q-value. Various In light of this, the development of neural networks has led
optimal policies, such as a Nash equilibrium, have been studied to the proposal of deep Q-networks (DQN) [10] as a method
inthiscontext.AlgorithmslikeNashQ-learningandNashActor-
to replace the Q-table with a neural network to approximate
Critic have shown effectiveness in these scenarios. This paper
the Q-value. This methodology is applicable to large-scale,
extends this research by proposing a deep Q-networks (DQN)
algorithm capable of learning various Q-vectors using Max, continuousstateandactionspaces.Additionally,deeplearning
Nash,andMaximinstrategies.Theeffectivenessofthisapproach neuralnetworksutilizeexperiencereplaytechniques,whichal-
is demonstrated in an environment where dual robotic arms lownetworkstostorepreviousexperiencesandderivetraining
collaborate to lift a pot.
samplesfromtheseexperiences.DQNshavebeensuccessfully
IndexTerms—Multi-agentreinforcementlearning(MARL),Q-
applied in various fields, demonstrating their versatility and
value, Q-vector, Nash equilibrium, Maximin, Deep Q-networks
effectiveness [13], [17], [18]. (DQN)
BuildingontheadvantagesofDQNsinhandlinglargestate
I. INTRODUCTION andactionspaces,researchershaveattemptedtointegratethem
with MARL. Some relevant works are discussed as follows.
Traditionalreinforcementlearning(RL)algorithmstypically
For example, the method proposed in [15] uses a neural
focus on training a single agent to optimize its behavior in
network to approximate the Q-value function, enabling agents
isolation. However, many real-world scenarios involve mul-
to learn effective policies in complex environments. However,
tiple agents that must learn to interact with each other and
this approximation often overlooks the influence of other
their environment. Thus, multi-agent reinforcement learning
agents on the environment. Although this approach simplifies
(MARL) is motivated by the need to develop intelligent
modeling and enhances robustness, ignoring the policies and
systems capable of interacting and collaborating in complex
behaviorsofotheragentscanleadtofailureinfindingoptimal
environments.
cooperation or competition strategies in complex interactive
In the early literature of MARL, Hu and Wellman [8]
environments.
extended Q-learning to a non-cooperative multi-agent setting
In [4], a unique communication channel for a multi-agent
by leveraging the framework of a generic stochastic game.
environmentwasdesigned,allowingagentstosendandreceive
In this approach, a learning agent maintains a Q-function
informationoverasharedcommunicationchannel.Eachagent
representing joint actions and updates it based on Nash equi-
decides its next action based on the received communication
libriumbehavior.Experimentalvalidationwasconductedusing
information and its own state. Additionally, a joint training
two distinct two-player grid games. A comparison of offline
frameworkwasproposedtooptimizethepoliciesofindividual
learningperformancerevealedthatagentsemployingtheNash
agents by maximizing the cumulative rewards of the team.
Q-learning method were more inclined to converge to the
The work in [6] introduced important sampling weights and a
joint optimal path compared to those utilizing single-agent Q-
goal network to stabilize the experience replay process. The
learning.Notably,whenbothagentsadoptedNashQ-learning,
stability of this approach was validated in cooperative navi-
their performance exhibited a significant enhancement over
gation and cooperative communication tasks. These methods
instances where only single-agent Q-learning was employed.
are based on DQN, with modifications to the communication
This multi-agent Q-learning algorithm builds on the tra-
methods and experience replay process.
ditional Q-learning method [19]. By constructing virtual Q-
Deep recurrent Q-learning (DRQN) and a meta-learning
tables, it effectively represents the Q-values corresponding to
approach based on DQN were proposed in [7] and [1], re-
each action in each state. However, this approach has a sig-
spectively, to address challenges in multi-agent environments.
nificant limitation: the virtual Q-table expands proportionally
DRQNexcelsinchasegamesbyeffectivelyhandlingpartially
observable Markov decision processes. The meta-learning
The Authors are with the School of Engineering, The University of
algorithm, designed for non-smooth competitive multi-agent
Newcastle, Callaghan, NSW 2308, Australia. Z. Chen is the corresponding
author.E-mail:zhiyong.chen@newcastle.edu.au. environments, enables agents to rapidly adapt to new tasks
4202
nuJ
21
]IA.sc[
1v84870.6042:viXrawith only a few gradient updates. such as friction, gravity, and the vibration of the robotic claw
The work in [14] integrates DQN and policy gradient grip due to joint movement. Therefore, validation in such an
(PG) methods to promote cooperation in complex social environment highlights the reliability of our algorithm.
dilemmas. The authors introduced an algorithm for multi- The remaining sections are organized as follows. Section II
agent environments that uses independent DQNs and policy introduces various optimal Q-vectors based on Max, Nash,
gradients, allowing each agent to independently learn and and Maximin strategies and formulates the MARL target. The
optimize its policy. In addition, the counterfactual multi- DQN-based algorithm is discussed in Section III. Section IV
agent policy gradients (COMA) algorithm, proposed in [5], presents experimental results from a robotic arm lifting a pot
evaluates each agent’s contribution when sharing a global tasktodemonstratetheeffectivenessofthealgorithm.Finally,
reward.TraditionalPGtechniquesoftensufferfromhighvari- the paper is concluded in Section V.
ance and low performance in multi-agent settings due to the
interdependence of agents’ rewards, with variance increasing II. OPTIMALQ-VECTORS
exponentially with the number of agents. To mitigate this, Consider n ≥ 2 agents in a non-cooperative task. At
deep deterministic policy gradient (DDPG) was adapted for each time instant t, the actions of the agents are denoted
multi-agentenvironmentsin[12].Furthermore,[11]combined as a1,...,an, and the rewards are r1,...,rn. The complete
t t t t
Hindsight experience replay [2] and DDPG into the dual-arm state of the multi-agent system is represented as s . The
t
deep deterministic policy gradient algorithm. This algorithm behaviors of multiple agents are modeled as Markov de-
was validated in dual robotic arm simulation environments cision processes with a state transition probability function
similar to those used in this paper, though our robotic arms p(s |s ,a1,...,an).
t+1 t t t
operate based on real physical parameters.
For example, in the task of two robotic manipulators lifting
The methods discussed above are based on DQN for multi-
a pot, the reward for each robot is given by:
agentenvironments,buttheyarenotdirectlydesignedfortasks
requiring multi-agents to learn a joint optimal path, such as ri =r +r +ri +ri . (1)
t height,t angle,t gripper,t action,t
achieving a Nash equilibrium, as addressed in [8]. A direct
Here, r represents the reward for the pot’s height, which
extension of Nash Q-learning to DQN was proposed in [3], height
increases as the pot is lifted, and r represents the reward
demonstrating its feasibility in a complex game that studies angle
for the pot’s tilt angle, which decreases as the tilt angle
the behavior of electronic exchanges. This approach employs
increases. These rewards are shared by both robots. Addition-
two neural networks with different structures: the main neural
ally, ri represents the distance from the robot gripper to
networkoutputstheactionandadvantagefunctionA,whilethe gripper
the pot’s handle, where the reward increases as the distance
secondary neural network outputs the state value function ap-
decreases. Conversely, ri accounts for the action cost,
proximationV.Accordingtothepaper,thisstructureresembles action
with a more negative value associated with larger actions.
the actor-critic algorithm [9], where the network outputting
The future reward of agent i, also known as the return, is
the action and A-value functions as the actor network, and
defined as:
the network outputting the V-value functions as the critic
network.TheNashequilibriumisthensolvedusingtheoutput ∞
(cid:88)
Ri = γkri . (2)
of the critic network to optimize the actor network. This t t+k+1
motivatesustoexploreadirectDQNapproachformulti-agent k=0
environmentsthatretainsthetraditionalDQNstructure,rather Here,thediscountingfactorγ ∈[0,1]penalizesrewardsinthe
than adopting an actor-critic framework. future. The policy for agent i is denoted as πi(ai|s ), which
t t
Specifically, in this paper, we define various Q-vectors generatesai basedonthecurrentstates .Fortheconvenience
t t
based on Max, Nash, and Maximin strategies, along with the of presentation, denote a = (a1,··· ,an), r = (r1,··· ,rn),
corresponding optimal actions. To learn the optimal policies and π = (π1,··· ,πn). The Q-value of the state-action pair
associated with each Q-vector, we develop a DQN-based for agent i, following the policy π, is defined as:
algorithm.ThisalgorithmretainsthetraditionalDQNstructure
but replaces the maximization of a single Q-value with the Qi π(s,a)=E π[R ti|s t =s,a t =a]. (3)
optimization of a Q-vector. This modification more intuitively
Denote Q=(Q1,··· ,Qn).
reflects the impact of incorporating game theory algorithms
In this multi-environment context, the Q-vector cannot be
intotraditionalsingle-agentreinforcementlearningalgorithms
defined based solely on a single maximum return. Instead,
in a multi-agent environment. The effectiveness of our algo-
we consider three different definitions of optimal Q-vectors,
rithm is demonstrated in the Mujoco simulation environment
denoted as Q∗ =(Q∗1,··· ,Q∗n) in this paper.
using the task ‘two arms lift’ [16]. It is noteworthy that
such a dual robotic arm operation is rarely used to verify • Max Q-vector:
The max Q-vector is based on each agent’s individual
MARLalgorithmsduetoitscomplexity.Instead,mostMARL
maximum policy. For each agent i, we define
algorithms are tested in environments like tables, games, and
mathematical models of electronic transactions. The physical
π =(π1 ,··· ,πn )=argmaxQi(s,a).
robotic environment presents additional training challenges, max[i] max[i] max[i] π πThe policy π max[i] is designed to maximize the Q- • MaxQ-vector(P max):theoperatorP isdenotedasP max,
value for agent i, but it cannot be directly implemented which maps Q (s,a) to a∗, denoted as a∗ , according
ϕ max
because, in general, π (a |s ) ̸= π (a |s ) for to:
max[i] t t max[j] t t
i ̸= j. However, one way to implement the policy
a∗ =(a1 ,··· ,an )
π is to have it provide actions only for agent i, max max[1] max[n]
max[i]
that is, π mi ax[i](ai t|s t). Thus, it forms a combined policy a max[i] =(a1 max[i],··· ,an max[i])=argm aaxQi ϕ(s,a).
as follows:
• Nash Q-vector (P nash): the operator P is denoted as
π∗ =(π1 ,··· ,πn ). P , which maps Q (s,a) to a∗, denoted as a∗ ,
max max[1] max[n] nash ϕ nash
according to:
Based on this policy, the corresponding Q-vector is the
max Q-vector, that is, Qi(s,a∗ )≥Qi(s,(ai,a∗−i )), ∀ai,∀i.
ϕ nash ϕ nash
Q∗ max(s,a)=Q πmax(s,a). • Maximin Q-vector (P mm): the operator P is denoted
as P , which maps Q (s,a) to a∗, denoted as a∗ ,
• Nash Q-vector: accorm dm ing to: ϕ mm
A Nash policy is one where each agent chooses their
optimal policy given the policies chosen by other agents, a∗ =(a∗1 ,··· ,a∗n )
mm mm mm
such that no agent can benefit by unilaterally changing a∗i =argmaxminQi(s,(ai,a−i)).
mm ϕ
their own policy. A joint multi-agent Nash policy π∗ ai a−i
nash
is defined as a policy satisfying: With the Q-vectors and the corresponding action selection
operator P defined, we are ready to introduce the multi-agent
Qi (s,a)≥Qi (s,a), ∀πi,∀i.
π n∗ ash (πi,π n∗ a− si h) reinforcement learning algorithms.
Here, the notation π−i denotes the polices taken by all III. ALGORITHMS
other agents, that is,
A DQN framework is used to develop the reinforcement
π−i =(π1,··· ,πi−1,πi+1,··· ,πn). learning algorithms based on Q-vectors. The learning process
using DQN is composed of three main components:
The corresponding Nash Q-vector is defined when the
• Experience reply buffer B;
agents follow the joint Nash policy π∗ , as follows:
nash • Prediction networks Q ϕ(s,a), parameterized by ϕ;
Q∗ nash(s,a)=Q π∗ (s,a). • Target networks Q θ(s,a), parameterized by θ.
nash
In a multi-agent scenario, both Q (s,a) and Q (s,a) gen-
• Maximin Q-vector: ϕ θ
erateQ-vectors.TheseQ-vectorsconsistofnneuralnetworks,
The maximin Q-value of an agent is the highest Q-value
oneforeachagent.Thelearningalgorithmisdiscussedbelow.
that an agent can be sure to obtain without knowing the
1) Measure the state s and input it into the prediction
policiesoftheotheragents,andthecorrespondingpolicy t
networktoobtaintheQ-vectorforallpossibleactionsin
is,
state s . This Q-vector is denoted as Q (s ,a), where
π∗i =argmaxminQi (s,a). (4) ϕ reprt esents the parameters of the preϕ dt ictit on network
mm (πi,π−i) t
πi π−i
at time t.
It forms a joint maximin policy π∗ =
mm 2) Select the optimal action according to (5):
(π∗1 ,··· ,π∗n ), and the corresponding maximin
mm mm
Q-vector is a∗ =P{Q ϕt(s t,a)}. (6)
Q∗ (s,a)=Q (s,a). Noting P has different operations corresponding to the
mm π m∗ m definitionoftheQ-vector.Then,choosetheactionusing
The primary objective is to develop reinforcement learning theepsilon-greedypolicy.Withaprobabilityofϵ,select
strategies using deep neural networks to learn the optimal a random action a , and with a probability of 1 − ϵ,
t
policies associated with each optimal Q-vector. The state- choose a =a∗.
t
action value function is estimated by a neural network pa- 3) Perform the action a and move to a new state s .
t t+1
rameterized by ϕ and the value is denoted as Q ϕ(s,a). In Notethata isanactionvectorconsistingoftheactions
t
the conventional single-agent scenario, the optimal action a
of all agents.
can be found by selecting the action with the maximum Q- 4) Record the action a and the reward r from all
t t+1
value, defined as: a∗ = argmax {Q (s,a)}. In the multi-
a ϕ agents and update the experience replay buffer as
agent case, the selection of action based on the Q-vector is {s ,s ,a ,r }→B.
t t+1 t t+1
more complicated, denoted as: 5) Sample some random batches {s,s ,a,r} of transi-
next
a∗ =P{Q (s,a)}. (5) tionsfromthereplaybufferandcalculatethetargetusing
ϕ
the target network:
The specific definition of the operator P is provided below,
r+γQ (s ,a∗) (7)
along with the different definitions of Q-vectors. θt nextHere, θ represents the parameters of the target network
t
at time t. The optimal action is found using the predic-
tion network as:
a∗ =P{Q (s ,a)}.
ϕt next
6) According to
Q∗(s ,a )=E[r ]+γQ∗(s ,π∗(a |s )),
t t t+1 t+1 t+1 t
the target in (7) is used for training the prediction
network by minimizing the loss defined as:
L={Q (s,a)−[r+γQ (s ,a∗)]}2.
ϕt θt next
In particular, gradient descent is performed with respect
to the prediction network parameters to minimize this
loss. Consequently, the network parameters are updated
from ϕ to ϕ .
t t+1
7) If t ̸= 0 mod C (i.e., t is not a multiple of C), keep
the target network unchanged: θ = θ . Otherwise, Fig. 1: Schematic diagram of UR5e joints.
t+1 t
if t = 0 mod C (i.e., after every C iterations), copy
the prediction network weights to the target network
A. Experimental Environment
weights: θ =ϕ .
t+1 t+1
To align the simulation with the real robotic arm system
Repeat these steps for M episodes.
in the lab, the UR5e model was chosen to represent the
Inamulti-agentscenario,bothQ (s,a)andQ (s,a)gener- robotic arms in the environment, with its joints illustrated in
ϕ θ
ate Q-vectors. These Q-vectors consist of n neural networks, Figure 1. The grippers were left at their default settings. The
one for each agent. Specifically, each agent i can maintain parameter‘envconfiguration‘wassetto‘single’topositionthe
a pair of neural networks, Qi(s,a) and Qi(s,a), and ex- two robotic arms on opposite sides of the table. Additionally,
ϕ θ
changeQ-valueswithotheragentstoimplementthealgorithm. ‘hasrenderer’wassetto‘True’toenablereal-timeobservation
Alternatively, each agent can maintain a complete copy of oftheroboticarmmovements.The‘controlfrequency‘param-
the prediction and target networks, Q (s,a) and Q (s,a), eter, which determines the frequency of the input signal, was
ϕ θ
independently,withoutneedingtorequestQ-valuesfromother setto‘controlfrequency=4’.The‘horizon‘parameter,defining
agents. theendofthecurrentcycleafteraspecifiednumberofactions
The above Q-vector based learning algorithm is introduced havebeenexecutedandtheenvironmentinitialized,wassetto
in the basic DQN framework. It can also be implemented ‘horizon=100’.Thesesettingsallowedthetworoboticarmsto
with variants of DQN frameworks. For instance, it can be raise the pot to its highest position while following the ideal
implementedwiththeduelingDQN(DDQN)algorithm,where path, thereby ensuring effective training completion.
a neural network produces the state-value V(s) and the ad- Inthe‘2armsliftapot’Robosuiteenvironment,tworobotic
vantage function A(s,a), which together determine the state- arms were tested in the experiments, creating a multi-agent
action value Q(s,a). This DDQN algorithm is used in our environment. For convenience of presentation, the first arm
experiments. with i = 1 is referred to as the left arm, and the second
arm with i = 2 is referred to as the right arm. The main
task is for the two robotic arms to lift the centrally located
IV. EXPERIMENTS
pot simultaneously by controlling the joints and keeping the
To evaluate the effectiveness of Q-vector based algorithms pot as smooth as possible in the process. The state s ∈ R12
t
with various definitions of Q-vector, we conducted experi- consists of the angles of six joints of both robotic arms. The
ments using the Robosuite environment [20]. Robosuite is a initial position is shown in Figure 2, and the position when
modular simulation framework built on the MUJOCO physics the pot is lifted is depicted in Figure 3.
engine [16], providing several pre-built simulation environ- Thevariable‘JointVelocity’,providedbyRobosuite,isused
mentssuchas‘2armsliftapot’and‘blocklifting’.Addition- asthecontrolaction.Specifically,weexercisedactionsontwo
ally, the authors have developed a robot arm model based on joints of each robotic arm, i.e., ai = (ai1,ai2) ∈ R2, rather
t t t
real structures, ensuring accurate representation of the robot than all six joints, where ai1 is Joint 2 and ai2 is Joint 3 in
t t
arm’s size and dynamic performance of its individual joints Figure 1. This deliberate choice guarantees the attainment of
and grippers, thus establishing it as our primary simulation the experimental objective: evaluating the training efficiency
environment. and facilitating the observation of performance disparitiesto four combinations of actions: (0,0), (−0.1,0), (0−0.1),
and (−0.1,−0.1). We can establish Q-vector variation tables,
whichindicatethecorrespondingactionsbasedontheoptimal
policies,i.e.,Max,Nash,andMaximin.Theseoptimalpolicies
reflect the corresponding behaviors of the two arms. It is ex-
pected that the proposed learning algorithms can successfully
learn these behaviors in all scenarios.
Below, we examine two cases: one for c = (−5,−5) and
another for c=(0,−5).
1) Case 1: Balanced Action Costs: When one arm acts to
elevatewhiletheotherremainsimmobile,theircontributionto
the height of the pot results in a reward increase denoted by
p >0. When both arms act to elevate, they jointly contribute
1
totheheightofthepot,resultinginarewardincreasedenoted
by p > p . With the reward weights set accordingly, it is
2 1
Fig. 2: Initial position of the robot arms lifting task. reasonable to assume p 1 > p 2−5 and p 1 > 5. The balanced
cost of action c=(−5,−5) is considered here, meaning that
the elevation action of each arm incurs an equal cost of −5.
The Q-vector variations corresponding to three different
typesofQ-vectorsarethenlistedinTableI.Theexperimental
results are presented in Figures 4 through 7. In the figures,
the return of the left arm is shown in orange, the return of the
right arm is depicted in green, and their total is represented in
blue. Each experiment was repeated six times.
Rightarm
Max:(0,0)
0 -0.1
Leftarm
0 (0, 0) (p1, p1−5)
-0.1 (p1−5, p1) (p2−5, p2−5)
Nash:(−0.1,0) Rightarm
or(0,−0.1) 0 -0.1
Leftarm
0 (0, 0) (p1, p1−5)
-0.1 (p1−5, p1) (p2−5, p2−5)
Maximin: Rightarm
Fig. 3: Final position of the robot arms lifting task. (−0.1,−0.1) 0 -0.1
Leftarm
0 (0, 0) (p1, p1−5)
-0.1 (p1−5, p1) (p2−5, p2−5)
among various algorithms. In this manner, the movement
TABLE I: Q-vector variations and optimal actions in Case 1.
of each joint is discretized into two distinct actions: one
involving elevation and the other involving immobility, that
is, ai1,ai2 ∈{−0.1,0}. First, for the Max Q-vector, it is shown in Table I that the
t t
The reward rt is defined as (1). There is no action cost optimal action is (0,0). The return for this case is depicted in
i
associated with Joint 3. Let the action cost of Joint 2 be ci. Figure 4. With the learned optimal policy, the arms remained
More specifically, ri = 0 or ci, when ai1 = 0 or −0.1, stationaryintheinitialpositionorexecutedonlyminormove-
action t
respectively. We set two types of action costs to evaluate the ments, without lifting either arm. This outcome aligns with
algorithms: c = (c1,c2) = (−5,−5) and c = (0,−5). In the the expected behavior of the optimal action (0,0). It is worth
first case, the action of each agent’s Joint 2 has the same cost mentioning that in one experiment, one arm was lifted, which
of−5.Inthesecondcase,theactionofJoint2oftheleftarm was considered an unsuccessful learning instance.
is free of cost, while that of the right arm has a cost of −5. Secondly, for the Nash Q-vector, Table I shows that the
optimal action is (−0.1,0) or (0,−0.1). Our algorithms
B. Results and Evaluation
randomly select one of these multiple optimal actions. The
Forthetwojointsofthearms,theactiona ∈R4,witheach experimental results are presented in Figures 5 and 6. In
t
joint action taking two possible actions: elevation (−0.1) and Figure 5, which includes four experiments, it is observed that
immobility (0). This results in 16 combinations of actions. the right arm is lifted with the learned policy, aligning with
Since the two joints of each arm contribute to the robot’s theoptimalaction(0,−0.1).Figure6showsthatthetiltangle
behavior in a similar manner, we combine them as a virtual of the pot was excessively steep due to one arm being lifted.
action for simplicity of explanation. Essentially, each arm Consequently, one robot arm would cease motion until the
has two actions: elevation (−0.1) and immobility (0), leading other arm continued to move, thereby reducing the tilt angleFig. 4: Profile of Max Q-vectors in Case 1: no arm lifted. Fig. 5: Profile of Nash Q-vectors in Case 1: right arm lifted.
Fig. 6: Profile of Nash Q-vectors in Case 1: right arm lifted Fig. 7: Profile of Maximin Q-vectors in Case 1: two arms
and then left arm lifted. lifted.
Rightarm
beforeresumingaction.Thisbehavioralignswiththeexistence Max:(−0.1,0)
0 -0.1
of two optimal actions.
Leftarm
0 (0, 0) (p1, p1−5)
Thirdly, for the Maximin Q-vector, the optimal action is -0.1 (p1, p1) (p2, p2−5)
(−0.1,−0.1), which implies that both arms are lifted. This Nash:(−0.1,0) Rightarm
0 -0.1
objectivewassuccessfullyaccomplishedbythelearnedpolicy,
as demonstrated in Figure 7. In this case, the pot was lifted Leftarm -00 .1 (p( 10 ,, p0) 1) (( pp 21 ,, pp 21 −− 55 ))
as shown in Figure 3. These experiments verify that all the Maximin: Rightarm
Q-vector-based optimal policies can be successfully learned (−0.1,−0.1) 0 -0.1
by the proposed Dueling DQN algorithm in Case 1. Leftarm
0 (0, 0) (p1, p1−5)
-0.1 (p1, p1) (p2, p2−5)
2) Case2:UnbalancedActionCosts: Theunbalancedcost
of action c = (0,−5) is considered in this case, meaning
TABLE II: Q-vector variations and optimal actions in Case 2.
that the elevation action of the left arm incurs no cost,
while the same action for the right arm incurs a cost of −5.
Consequently,theQ-vectorvariationsaremodifiedasshownin
Table II. The optimal actions change for some Q-vectors, and This behavior was successfully learned and is shown in
theexperimentalresultsarepresentedinFigures8through11. Figure 8.
First, for the Max Q-vector, the optimal action has changed Secondly, for the Nash Q-vector, the optimal action has
from (0,0) to (−0.1,0). The new optimal action implies that also changed to (−0.1,0). Similarly, it was observed in the
the left arm is lifted while the right arm remains stationary. experiments that the left arm was predominantly lifted, asFig. 8: Profile of Max Q-vectors in Case 2: left arm lifted. Fig. 9: Profile of Nash Q-vectors in Case 2: left arm lifted.
Fig. 10: Profile of Nash Q-vectors in Case 2: left arm lifted Fig. 11: Profile of Maximin Q-vectors in Case 2: two arms
and then right arm lifted. lifted.
shown in Figure 9. However, in some experiments as shown rightarmislifted.Theyeventuallyliftedthepottothehighest
in Figure 10, it was observed that the left arm was lifted position.
first, followed by the right arm. Eventually, both arms worked
Rightarm
together to lift the pot. This behavior can be explained as Nash:(0,−0.1)
0 -0.1
follows.
Leftarm
0 (0, 0) (p1+δ, p1+δ−5)
When the left arm is lifted alone, the action (−0.1,0) -0.1 (p1, p1−δ) (p2, p2−5)
indicates that the left robotic arm continues to lift, resulting
in the right robotic arm not being able to continue to grip TABLEIII:ModifiedQ-vectorvariationsandtheNashaction.
the object, and the right side’s distance reward decreases,
represented by −δ with δ > 0. The action (0,−0.1) means Finally, for the Maximin Q-vector, the optimal action re-
that the right robotic arm makes a lifting action and the left mains unchanged. Similar to Case 1, both arms were simul-
arm stays, then the tilt angle of the task target decreases, taneously lifted, raising the object to its highest point, as
implying an extra reward to both arms, denoted as +δ. With shown in Figure 3. The training performance is presented in
the reward weights set accordingly, δ is sufficiently large to Figure11,verifyingtheeffectivenessofthelearningalgorithm.
satisfy p − δ < p − 5 and p + δ > p . With these
1 2 1 2
extra modifications in the reward, the Q-vector variations are
V. CONCLUSION
listed in Table III, implying a different Nash action (0,−0.1). This paper presents a novel DQN algorithm designed to
Following this action, the left arm remains stationary and the learn Q-vectors using Max, Nash, and Maximin strategies,addressing the complexities of deriving optimal policies in
MARL environments. The proposed method effectively han-
dles the varying Q-values resulting from individual agent
rewards and demonstrates its efficacy in a dual robotic arm
lifting scenario. Future work will focus on scaling the al-
gorithm to larger multi-agent systems, exploring additional
game-theoretic strategies, and applying the approach to real-
worldapplicationstofurthervalidateandenhanceitspractical
utility.
REFERENCES
[1] M. Al-Shedivat, T. Bansal, Y. Burda, I. Sutskever, I. Mordatch, and
P. Abbeel. Continuous adaptation via meta-learning in nonstationary
andcompetitiveenvironments. arXivpreprintarXiv:1710.03641,2017.
[2] M.Andrychowicz,F.Wolski,A.Ray,J.Schneider,R.Fong,P.Welinder,
B. McGrew, J. Tobin, O. Pieter Abbeel, and W. Zaremba. Hindsight
experiencereplay. arXivpreprintarXiv:1707.01495,2017.
[3] P. Casgrain, B. Ning, and S. Jaimungal. Deep q-learning for nash
equilibria: Nash-dqn. Applied Mathematical Finance, 29(1):62–78,
2022.
[4] J. Foerster, I. A. Assael, N. De Freitas, and S. Whiteson. Learning to
communicate with deep multi-agent reinforcement learning. Advances
inneuralinformationprocessingsystems,29,2016.
[5] J. Foerster, G. Farquhar, T. Afouras, N. Nardelli, and S. Whiteson.
Counterfactualmulti-agentpolicygradients.InProceedingsoftheAAAI
conferenceonartificialintelligence,volume32,2018.
[6] J. Foerster, N. Nardelli, G. Farquhar, T. Afouras, P. H. Torr, P. Kohli,
andS.Whiteson. Stabilisingexperiencereplayfordeepmulti-agentre-
inforcementlearning. InInternationalconferenceonmachinelearning,
pages1146–1155.PMLR,2017.
[7] M. Hausknecht and P. Stone. Deep recurrent q-learning for partially
observablemdps. arXivpreprintarXiv:1507.06527,2015.
[8] J. Hu and M. P. Wellman. Nash q-learning for general-sum stochastic
games.Journalofmachinelearningresearch,4(Nov):1039–1069,2003.
[9] V.R.KondaandJ.N.Tsitsiklis. Actor-criticalgorithms. Advancesin
neuralinformationprocessingsystems,pages1008–1014,2000.
[10] Y. LeCun, Y. Bengio, and G. Hinton. Deep learning. nature,
521(7553):436–444,2015.
[11] L.Liu,Q.Liu,Y.Song,B.Pang,X.Yuan,andQ.Xu. Acollaborative
controlmethodofdual-armrobotsbasedondeepreinforcementlearning.
AppliedSciences,11(4):1816,2021.
[12] R.Lowe,Y.I.Wu,A.Tamar,J.Harb,O.PieterAbbeel,andI.Mordatch.
Multi-agentactor-criticformixedcooperative-competitiveenvironments.
Advancesinneuralinformationprocessingsystems,30,2017.
[13] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang,
A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton, et al. Mastering
thegameofgowithouthumanknowledge. nature,550(7676):354–359,
2017.
[14] R. S. Sutton, D. McAllester, S. Singh, and Y. Mansour. Policy gra-
dientmethodsforreinforcementlearningwithfunctionapproximation.
Advancesinneuralinformationprocessingsystems,12,1999.
[15] A. Tampuu, T. Matiisen, D. Kodelja, I. Kuzovkin, K. Korjus, J. Aru,
J. Aru, and R. Vicente. Multiagent cooperation and competition with
deepreinforcementlearning. PloSone,12(4):e0172395,2017.
[16] E.Todorov,T.Erez,andY.Tassa.Mujoco:Aphysicsengineformodel-
basedcontrol.In2012IEEE/RSJinternationalconferenceonintelligent
robotsandsystems,pages5026–5033.IEEE,2012.
[17] H. Van Hasselt, A. Guez, and D. Silver. Deep reinforcement learning
with double q-learning. In Proceedings of the AAAI conference on
artificialintelligence,volume30,2016.
[18] S.Wang,H.Liu,P.H.Gomes,andB.Krishnamachari. Deepreinforce-
ment learning for dynamic multichannel access in wireless networks.
IEEE Transactions on Cognitive Communications and Networking,
4(2):257–265,2018.
[19] C.J.WatkinsandP.Dayan. Q-learning. Machinelearning,8:279–292,
1992.
[20] Y. Zhu, J. Wong, A. Mandlekar, and R. Mart´ın-Mart´ın. robosuite: A
modularsimulationframeworkandbenchmarkforrobotlearning.arXiv
preprintarXiv:2009.12293,2020.