Beyond LLaVA-HD: Diving into High-Resolution
Large Multimodal Models
Yi-FanZhang1,2, QingsongWen3, ChaoyouFu4, XueWang5,
ZhangZhang1,2, LiangWang1,2, RongJin6
1StateKeyLaboratoryofMultimodalArtificialIntelligenceSystems(MAIS),InstituteofAutomation
2SchoolofArtificialIntelligence,UniversityofChineseAcademyofSciences(UCAS)
3SquirrelAILearning;4TencentYoutuLab;5AlibabaGroup;6MetaAI
https://github.com/yfzhang114/SliME
Abstract
SeeingclearlywithhighresolutionisafoundationofLargeMultimodalModels
(LMMs),whichhasbeenproventobevitalforvisualperceptionandreasoning.Ex-
istingworksusuallyemployastraightforwardresolutionupscalingmethod,where
theimageconsistsofglobalandlocalbranches,withthelatterbeingthesliced
imagepatchesbutresizedtothesameresolutionastheformer. Thismeansthat
higherresolutionrequiresmorelocalpatches,resultinginexorbitantcomputational
expenses, and meanwhile, the dominance of local image tokens may diminish
theglobalcontext. Inthispaper, wediveintotheproblemsandproposeanew
frameworkaswellasanelaborateoptimizationstrategy. Specifically,weextract
contextualinformationfromtheglobalviewusingamixtureofadapters,based
ontheobservationthatdifferentadaptersexcelatdifferenttasks. Withregardto
localpatches,learnablequeryembeddingsareintroducedtoreduceimagetokens,
themostimportanttokensaccountingfortheuserquestionwillbefurtherselected
byasimilarity-basedselector. Ourempiricalresultsdemonstratea‘lessismore’
pattern,whereutilizingfewerbutmoreinformativelocalimagetokensleadstoim-
provedperformance. Besides,asignificantchallengeliesinthetrainingstrategy,as
simultaneousend-to-endtrainingoftheglobalminingblockandlocalcompression
blockdoesnotyieldoptimalresults. Wethusadvocateforanalternatingtraining
way,ensuringbalancedlearningbetweenglobalandlocalaspects. Finally,wealso
introduceachallengingdatasetwithhighrequirementsforimagedetail,enhancing
thetrainingofthelocalcompressionlayer. Theproposedmethod,termedLMM
withSophisticatedTasks,Localimagecompression,andMixtureofglobalExperts
(SliME), achieves leading performance across various benchmarks with only 2
milliontrainingdata.
1 Introduction
In the past years, we are fortunate to witness a great flourish of LMMs [2, 10, 34]. However,
theystillstrugglewithcomplexvisualperception[58,14]andreasoningtasks[27,53]. Empirical
studieshaveshownthatemployinghigherresolutionsisagoodsolution[3,32,29,41]. Approaches
likeLLaVA-Next[33]segmenthigh-resolutionimagesintomultiplepatches, encodingeachone
independently before concatenating all local patch tokens with the original global image tokens,
albeitatanescalatedcomputationalcost. TheothermodelslikeMonkey[29]andLLaVA-UHD[51]
alsosplitimagesintopatches,butsubsequentlycompressthemtoavoidredundanttokens. Insuch
cases,forhigh-resolutionimages,thelocalimagetokensdominatethefeaturespace. Forexample,in
a1024×1024imagedividedinto9patches,theglobalimagetokenaccountsforonly1/10.
4202
nuJ
21
]VC.sc[
1v78480.6042:viXraIncontrast,ourcoreideapositsthatglobalinformationshouldbeprioritized,thusweaimtoextract
andretainasmuchglobalcontextaspossiblewhileenhancingitwithlocalimagedetails. Inthis
study,weinitiallysegmenttheimagesintopatchesaccordingtotheirresolution. Theimagetokens
arethencategorizedintotwogroups: theglobalviewandlocalpatches. Fortheformer,wepreserve
thetokencounttoretainallcontextualinformationandutilizeamixtureofadapterstofurtherexplore
global context. As displayed in Fig. 2 (b), we employ a Multilayer Perceptron (MLP) to project
image features into the feature space of the LLM, and a set of learnable queries named qformer
areemployedtoextractcrucialglobalinformation. Softlymixingoutputsfromthetwoadapters
aidstheLLMincomprehendingtheglobalcontextmoreeffectively. Consideringthelocalpatches,
theyprovideadditionalimagedetailsbutarecompressedusingaqueryingtransformertomitigate
computationalcosts. AsshowninFig.2(d),atext-guidedrouterisfurtherproposedtoselectthe
mostrelevantlocalimagetokenscorrespondingtotheinputinstructionorquestion,therebyavoiding
excessiveimagetokensandfocusingonpertinentimageinformation.
Atthesametime,wefindthatitischallengingtotraintheglobalprojectionandlocalcompression
simultaneously. The simplicity of the projection layer makes it easy to train, but also causes the
modeltodegeneraterapidlyduetoover-relianceonglobalfeaturesandneglectoflocalcounterparts.
Weformalizethisasabi-linearproblemandtheoreticallyshowthatsimultaneouslyupdatingboth
blocksdoesnotconvergetooptimalresults. Instead, weproposetoalternativelytraintheglobal
projectionblockandlocalcompressionblock,bywhichweensurethatbothglobalandlocalfeatures
areeffectivelylearnedandutilized.
Thetrainingisknowntobedata-driven. Thecurrentdatainstancesprimarilystemfromreal-world
captions,generalQA,andalimitednumberofreal-worldconversationssampledfromrobustLLMs.
Mostoftheseinstancesrevolvearoundbasicperception,recognition,andreasoningtasks,suchas
understandingrelationshipsamongobjects. However,therearetwonotableflaws:firstly,thetasksare
notchallengingenoughandlargelylackintricatevisualreasoningtasks;secondly,manyquestions
onlypertaintospecificobjectsoractions,neglectingtheneedforallimagedetails. Thislimitation
hampersthefullutilizationofthecapabilitiesofferedbyourhigh-resolutionframework. Tothisend,
thispapermeticulouslygathersandfiltersdatasetstocreatetheScienceandMathematicalReasoning
dataset(SMR),whichencompassesninechallengingtasksspanningnaturalscience,mathematical
problems, and scientific chart comprehension. Some of these tasks provide complete reasoning
paths,compellingthemodeltoarticulatetheentirereasoningprocess. Importantly,manyimagesin
theSMRdatasetcontainrichannotations. Completingsuchintricatereasoningtasksnecessitatesa
thoroughunderstandingofimagedetails,whichwillgreatlybenefitthetrainingofourframework.
LMMwithSophisticatedTasks,Localimageaugmentation,andMixtureofglobalExperts(SliME)
canbereadilyinstantiatedwitharangeofLLMs.Extensiveempiricalstudiesvalidatetheeffectiveness
ofourproposedmethod. Remarkably,ourapproachachievesleadingperformanceacrossvarious
settings,evenmatchingtheperformanceofwell-establishedmodelssuchasGeminiPro[46]and
Qwen-VL-Plus[3]inabout10benchmarkswithonly8BLLMand2milliondata. Theseresults
underscorethepotentialofSliMEtosetnewbenchmarks,highlightingitsadvancedcapabilities.
2 Method
WedelineateourmethodaimedatenhancingLMMs’imageunderstandingcapabilitiesinthissection.
Weutilizeadaptiveslicingtoscaleinputresolution,andrefineglobalcontextviaasoftmixtureof
experts. Additionally,wecompresslocalfeaturesusingaquerytransformerarchitecture1,select
featuresoptimallywithatext-guidedrouter,andemployanalternatingtrainingschemetooptimize
the bilinear optimization problem. These strategies collectively improve both the computational
efficiencyandperformanceofLMMs. Furthermore,weintroducetheSMRdataset,knownforits
challengingnatureandhighdemandforunderstandingimagedetails,makingitanidealchoicefor
traininghigh-resolutionframeworks.
1Inthiscontext,theabbreviation’qformer’referstoqueryformer,whereweutilizelearnablequeryembed-
dingsasdescribedinpreviousworks[2,17],ratherthanemployingtheQformerapproach[10]
22.1 RefiningGlobalContextwithaSoftMixtureofExperts
ScalingInputResolutionbyAdaptiveSlicing. Initially,weexplorevariousgridoptionsforslicing
images,similartoLLaVA-Next,butwithfinergranularity(seeFig.2(a)). Weinvestigateresolutions
rangingfrom336×(m,n)withm = 1,n = 1tom = 6,n = 6todeterminethemostefficient
option. Toprovideaglobalcontext,wepadandresizetheimagetoauniformsizeof336×336and
concatenateitwithlocalfeatures. ForimageswithshapesW andH,weiteratethroughallavailable
partitionstrategies. Forinstance,whenusingthestrategym∗n,theresizescalecanbecalculatedas
s=min{m∗336/W,n∗336/H}. Theutilizedresolutionafterscalingwillbemin{W ∗H,W ∗
s∗H∗s},andthewastedresolutionwillbem∗336∗n∗336−min{W ∗H,W ∗s∗H∗s}. We
selectthebestpartitionbymaximizingtheutilizedresolutionandminimizingthewastedresolution
whentheutilizedresolutionisthesame.
WhynotCompressGlobalImageTokensforEfficiency? Ourapproachisinspiredbyempirical
observations, consistentwith previous findings[56]: whenemployingattention-based modelsas
adapterstoreducetokensorbridgethemodalitygap,amoreintricatehyper-parametersearchmaybe
requiredtoachieveperformancecomparabletosimplerMLP.AsdepictedinFig.1,replacingthe
MLPadapterofLLaVA-v1.5withthequeryformerofthesamenumberoftokensyieldssignificantly
inferiorperformanceonmostbenchmarks.AsimplerprojectorcompelstheLLMtobetterunderstand
visualinputs,leadingtoenhancedgeneralization[30]. Consequently,werefrainfromreducingtoken
numbersforglobalimagesandinsteadpreserveallglobalinformationthroughsimpleprojection.
GlobalContextRefinementbySoftMixtureofExperts. Althoughqueryformerisinferiorto
MLPonmostbenchmarks,thelearnablequeryembeddingsandattentionmechanismallowfora
differentfeatureselectionstrategy,andonsomebenchmarkssuchasScienceQA(SQA)[38],query
formerachievesbetterperformance. Buildingontheinsightsfromouranalysis,weproposeanovel
approachtorefineglobalcontextfeaturesbyleveragingthestrengthsofbothMLPandqueryformer
adapters. Specifically, weemployanoisyMixtureofExperts(MOE)frameworktocombinethe
benefitsofthesetwotypesofframeworks. Inthisframework,forfeaturexfromthevisionencoder,
alearnedgatingnetworkGdeterminestheweightsfortwoadapters: G(x) f (x)+G(x) f (x).
0 m 1 q
The gating network learns to dynamically adjust the importance of each adapter based on the
input feature. To prevent the gating network from predominantly activating the same adapter,
we introduce learnable noise during training2. This is achieved through the following equation:
G(x)=Softmax(cid:0) {(x·W ) +Normal(0,1)·Softplus((x·W ) )}2 (cid:1) .
g i noise i i=1
2.2 LocalFeatureMiningwithCompressionandSelection
Local Feature Compression. In our approach to lo-
calfeaturecompression,weimplementaqueryformer
architectureleveragingN queryembeddings,denoted
q
as M
Q
∈ RNq×DI. Here, D
I
represents the dimen-
sionalityoftheimagefeaturesobtainedfromthevision
encoder. Notably,westrategicallysetN tobesmaller
q
than the original token count derived from the vision
encoder. Thisdeliberatereductionservestoalleviatethe
computationalburdenwhilepreservingessentialinfor-
mation. Throughtheapplicationofanattentionmecha-
nism,weorchestratetheinteractionbetweenthesequery
embeddings and the local image features. Here, the
LLaVA qformer qformer w alt Local Local w alt
learnableembeddingsactasthequeryforattention,di-
rectingthemodel’sfocustowardspertinentaspectsof Figure 1: Significance of Alternating
thelocalfeatures. Theresultantoutcomeisacondensed Training. The reported values represent
representation of local features, consisting of N to- theperformanceratioofbaselinestothe
q
kens. Byjudiciouslybalancingcomputationalefficiency bestone: Localwithalternatingtraining.
with information retention, our compression strategy
enhancesthescalabilityandeffectivenessofLMMsinhandlingdiversetasks.
2Asmentionedearlier,thetrainingofMLPiseasytoconvergequickly,potentiallycausingthegatednetwork
toassignhigherweighttoMLP,hinderingfulltrainingofqueryformer.
3Text-GuidedRouter. Ourapproachseekstofurtheralleviatecomputationalburdenbyfeatureselec-
tion. Wearguethatnotallpartsofthelocalimagearerelevanttothequestionsposed. Forinstance,
in Fig. 2(d), the question "What breed is the dog?" pertains only to specific local image regions,
indicatingthatdiscardingirrelevantfeaturescansignificantlyreduceabundantimageinformation. In
thiswork,weexploreasimplecosine-similarityroutingstrategyforitssimplicityandeffectiveness.
Given the text embedding z
x
∈ RLx×D and the projected local image feature z
v
∈ RLv×D, we
compute scores as S ∈ RLv×Lx = z vz xT. Averaging text tokens and applying softmax to image
tokensyieldsS
cosine
∈RLv. Oncescoresorrelevanceindicatorsareobtainedforeachlocalfeature,
weemployanadaptiveselectionstrategy. Specifically,wesortscoresfromhighesttolowestand
selectfeaturesuntiltheaccumulatedscoresurpassesathresholdγ. Thishyperparameterbalancesthe
efficiencyandcompletenessoflocalfeatures. Ourexperimentsrevealthatselectingspecificlocal
featuresdoesnotdiminishperformance.Onthecontrary,bydisregardingirrelevantfeaturesandusing
fewertokens,weachievesuperiorperformanceacrossmostbenchmarks. Duringtraining,Gaussian
noisefromN(0,0.1)isaddedtotheselectionscoretomaintainthediversityofrepresentations.
AlternatingTrainingScheme. Ourtrainingmethodologyforthevision-languageadapterandlocal
compressionlayerinvolvesanuancedthree-stageprocess. Initially,inStageI(seeFig.2(b)),the
adapter undergoes training using the global image. Subsequently, in Stage II (see Fig. 2(c)), the
adapterremainsfixedwhilethelocalcompressionlayerisexclusivelytrainedusinglocalpatches.
Finally,inStageIII(seeFig.2(d)),bothglobalandlocalfeaturesaresimultaneouslytrained. While
wedelveintotheoreticalunderpinningsinSection2.3,empiricalinsightsalsobearsignificance. Our
experimentationrevealsthatsimultaneouslytrainingtheadapterandlocalcompressionlayerina
singlestageyieldssuboptimalperformance. Thisdiscrepancyarisesfromthemodel’spredominant
focus on global features, as the global feature requires only projection with no information loss,
makingiteasiertolearn. Hence,weconfinetheuseoflocalpatchestoStageIIforcompression
layertraining. Thisapproachensuressequentiallearning,firstprojection,thencompressionoflocal
features(LocalvsLocalw.altinFig.1).Additionally,alternatingtrainingcanbridgetheperformance
gapbetweentwocommonadapters: MLPandqueryformer,asmentionedearlier. Specifically,when
employingattention-basedmodelsasadapters,whichoffermoreflexibilitybutmayexhibitinferior
performancecomparedtosimpleMLPadapters[56],wefindthatalternatingtrainingsignificantly
enhancesperformance(queryformervsqueryformerw.alt3inFig.1).Suchaschememayilluminate
futurework,facilitatingthetrainingofmorecomplexyetflexibleadapteroptions.
2.3 ImportanceofAlternatingTrainingforOptimizingBilinearFunctions
Takeaway:AlternatingTrainingispivotalforthesuccessofSliME.Ourdemonstrationinthis
subsectionwillalsoshedlightonwhyit’scommonpracticetoinitiallyfreezeonemodalityinmulti-
modallearningandoptimizetheadapterofonemodalitybeforeengaginginjointoptimization
acrossmultiplemodalities. AlltheproofcanbefoundinappendixC.
Bilinearformsareprevalentindeeplearningmodels,especiallyinmulti-modallearningwherethe
representationfromtwodifferentmodalitiesisfrequentlyalignedthroughadotproduct. Letthe
targetmatrixX ∈ Rd×d beexpressedasX = ab⊤ +ba⊤, wherea,b ∈ Rd aretwonormalized
vectors. Ourobjectiveistofindtherank-1matrixtoapproximateX,whichleadstothefollowing
optimizationproblem:
min L(u,v)=
1(cid:12) (cid:12)uv⊤−X(cid:12) (cid:12)2
u,v∈Rd 2
InLMMs,thevisionencoderandadaptercanbeperceivedasthevisionmodality,whileothersare
categorizedasthetextmodality,andthetargetXcanbeseenasthebestLMM.Withinourframework,
wetreattheadapterandlocalcompressionlayerasdistinctfunctions,aimingtoapproximatethe
optimalmodalityadaptationparameter. Werecognizethatassumingbothaandbtobemerelyvectors
is a simplification that may not fully capture the complexity of the entire model. However, this
simplificationallowsustoanalyzetheproblemmoreeffectivelyandderivevaluableinsightsfromit.
3Firstly,themodellearnsqueryembeddingstointeractwithimagefeatures,andsecondly,itprojectsthe
imagefeaturestotheLLMdimension.Similarly,inthisalternatingtrainingapproach,wefirstlearntheprojection
headandthenfocusonlearningattentionmechanismparametersandqueryembeddings.
4LLMs
Adaptive Slicing Adapter
MLP Q-former
Visual Encoder
Router
The breed of the
dog in the picture
is what?
(a) AdaptiveSlicing. (b) StageI.
LLMs LLMs
Adapter Global & Top-k Local Features
LocalCompressionLayer
VisualEncoder Adapter
Text-Guided
Local Compression Layer
Router
Adaptive The breed of the VisualEncoder
Slicing dog in t wh he a p t?icture is T inh te h b er pe ie cd t uo rf e t ih se w d ho ag t ?
(c) StageII. (d) StageIII.
Figure2: InnovativeTrainingApproach: (b)Refiningthevisualadapterwithmixtureofexperts,
(c)optimizinglocalcompressionlayers,and(d)instructionsfine-tuning. Here,SliMEefficiently
processesimagesviaslicing,projection,andselectingpertinentlocalfeaturesalignedwithquery.
ItiswellknownthattheoptimalsolutionforuisalignedwiththetopeigenvectorofXX⊤,i.e.
(cid:18) (cid:19)
1 a⊤b
XX⊤ =(ab⊤+b⊤a)(ba⊤+a⊤b)=A A⊤
a⊤b 1
(cid:124) (cid:123)(cid:122) (cid:125)
:=M
whereA=(a,b)∈Rd×2 andM ∈R2×2. SinceX isconstructedthroughaandb,uhastoliein
thesubspacespannedbyaandbandthuscanbewrittenuas
(cid:18) (cid:19)
α
u=αa+βb=A
β
(cid:124) (cid:123)(cid:122) (cid:125)
:=z
Hence,theoptimalsolutionforz =(α,β)⊤shouldbealignedwiththelargesteigenvectorofmatrix
M. Letu andv betheinitialsolutionandisgiveninthefollowingform
0 0
u =α a+β b, v =β a+α b
0 0 0 0 0 0
whereα ,β ∈Raretwoscales. Here,weutilizethefactthatuandvhavetolieinthesubspace
0 0
spannedbyaandb. Thenwestatethefollowingtheorem:
Theorem1. Usingthegradientdescentmethod,weupdatethesolutionu andv as
t t
u =u
−η(cid:0)
u
v⊤−X(cid:1)
v , v =v
−η(cid:0)
v
u⊤−X(cid:1)
u (1)
t+1 t t t t t+1 t t t t
SimultaneouslyupdatinguandvusingEq.(1)islessidealforoptimizingtheobjectivefunctionof
bilinearform,asthegradientdescentupdatedoesnotnecessarilyconvergetotheoptimalsolution.
Wewilldemonstratethattheissuewithgradientdescent(ormoreaccurately,simultaneouslyupdating
uandv)canbeeffectivelyaddressedbyalternatingoptimization. Specifically,wewilloptimizev
withfixedu,andthenoptimizeuwithfixedv. Wewillshowthatthisapproachconvergestothe
optimalsolutionbyalternatingoptimization.
Theorem2. Letu = α a+β b. Werewritethesequentialsolutionu obtainedbyalternating
0 0 0 t
optimizationasu =α a+β b. z =(α ,β )⊤evolvesoveriterationsbyz = 1 M2z
t t t t t t t+1 |ut|2|vt|2 t
Thatis,alternatingoptimizationensuresthatz ∝M2tz ,implyingthatz isguaranteedtoconverge
t 0 t
tothelargesteigenvectorofM,thusresolvingthelimitationofgradientdescent.
5Science QA #12.7K Textbook QA#9.5K AI2 Diagrams#12K Chart QA#28.3K DVQA #200K
Natural sci Se on cc ie a, l L sa cin eg nu ca ege science, Lif Pe hS yc si ie cn ac l e S, cE iea nr cth e S tec xie tbn oc oe k a snd PD ri ia mg ara rym N U an tud re ar lst Sa cn id enin cg e opl eo rg ai tc ioa nl sa n Ud na dr ei rt sh tm ane dti ic ng chart understanding
Q c A S Ba u. .u r ba ae b sns i t cd t ae oi eo nl
m
en ci ms e p: o( eC ) un.o t nam drp yl ete the statement. Tungsten Q p A M Ba ..u er Ge mt N s oo bt lui f r go ct a in h l n Bee: e a oW N r
d
uh yca lt e i us s t ?he outer surrounding Q t s w ih nh ou e a pu e ra ols k dm pt si u,o o s lewn u aen: h
t
it aiI
o
cf o n
n
ht f ?h i l noe a crr r g re g ea ew an sie esr m e a sudden decrease in Q d b d bi a a au f r r rfe e k ?as r bt e ni n ldo uc n ete h: ebW e th wa et eis nt lh ige hl tea bs lut e Q H b thau o er rwe s es ?at mi ro a en n: y
C D.
.
C Ne ul cl
l
M eoe lum sbrane S tuh nri amp;Tuna;Squid 39.32 5
Arxiv QA #100K GeoQA3& GeoQA+ #76K TabMWP#30K
ArXivpapers spanning various scientific domains Arithmetic, Trigonometric, Theorem Formula, Constant mathematical reasoning on tabular data
Question: What can be inferred about Question:如图,四边形ABCD内接于⊙O, Question: Each vase has2 tulips. How many
thearticle'smotion in figure c? 如果它的一个外角∠DCE=62°, tulips are in 4 vases?
In figure c, the trajectory forms a closed 那么∠BOD=() Count by twos.
loop with a direction of motion indicated, A. 124°B. 120°C. 62°D. 31° Use the chart:
suggesting circular motion. So the thereare8tulips
answer is particle exhibits circular 四边形ABCD内接于⊙O,∴∠A=∠DCE= in4vases.
motionwithchangingvelocity 62°∴∠BOD=2∠A=124°.故选A
Figure3: Theusedscienceandmathematicalreasoningtasksinthispaper.
2.4 ExpandingDatasetScopewithChallengingReasoningTasks
Generation of Source Data and Instruction Data. The creation of SMR involves a meticu-
lous amalgamation of publicly available datasets, comprising Arxiv-QA [26], ScienceQA [38],
MATH-Vision[50],TextBookQA[21],GeoQA3[6],Geometry3K[37],TabMWP[39],DVQA[19],
AI2D[20],andChartVQA[40]. Thevarietyofquestiontypesandassociatedimagessourcedfrom
thesedatasetsisdepictedinFig.3,presentingadistinctivechallengetoexistinginstructiondatasets,
asillustratedinFig.6. ThedisparitiesbetweenSMRandconventionalinstructiontuningdatasets
manifestintwokeyaspects: (1)ChallengingReasoningTasks. ManyofthetasksinPhysical/Social
scienceandmathematicsdemandadvancedreasoningabilities. Notably,datasetssuchasArxiv-QA,
GeoQA3,andTabMWPoffercompletereasoningpaths,includingintermediatestepsforderiving
finalresults. Insuchcases, themodelistaskednotonlywithmasteringfoundationalknowledge
butalsowitharticulatingcomplexreasoningprocesses—anotablymoredemandingendeavor. (2)
DemandforImageDetailUnderstanding. Alltasksnecessitateaprofoundunderstandingofvisual
detailsbecausemanyimagescontainrichannotationinformationorquestionsrequiringcomprehen-
sivevisualanalysis. Thisaspectisparticularlybeneficialfortrainingourhigh-resolutionframework.
FurtherelucidationondatasetsandspecificconstructionmethodologiescanbefoundinappendixA.
Toensuretheaccuracyofourdata,wecarefullyfilteritaftercollection. Thisinvolvesidentifyingand
fixingissueslikeblurryimagesorjumbledtext,unrelatedimage-textpairs,andincorrectreasoning
pathsthatcan’tleadtocorrectanswersormightleadtowrongconclusions. Forthelatter,weuse
GPT-4Vtocreatenew,accuratereasoningpaths.
StatisticsandAnalysis. InFig.4,weillustratethedifferencesinstatisticsbetweenSMRandexisting
instructiontuningdatasets. Tostandardizemulti-roundconversations,weaggregatethemintoone-
roundandcalculatetheaveragelength. WeemployLLaVAtodeterminethemaximumlengthsuch
that99%ofthedatafallswithintheinterval. LLaVA[32]comprises665Kinstructiontuningdata
instancescharacterizedbyshortqueriesandanswers. Similarly,LLaVAR[57]exhibitscomparable
patternstoLLaVA.Conversely,ShareGPT4V[7],amorecomprehensivedatasetderivedfrom100K
high-qualitycaptionsgeneratedbyadvancedGPT4-Visionmodels,featureslongergenerationlengths,
indicativeofmoredetailedandcomplexcaptions. Incontrast,SMRdemonstrateslongerquerytexts
comparedtoexistingtrainingcorpora,reflectingtheneedfordetaileddescriptionsorbackground
informationtoelucidatescientificormathematicalproblems. Additionally,sincesomeofourdatasets
focussolelyonquestion-answertasks,approximately50%ofinstancesfeatureshorteranswerlengths.
However,owingtothetaskcomplexity,particularlythosedatasetswithextensivereasoningpaths
aimingtotrainmodelstocomprehendintricatechainsofreasoning,SMRexhibitsahigherratioof
longersentencescomparedtoLLaVAandLLaVAR.
3 Experiment
SliME is evaluated against both open-source and closed-source models across various domains,
encompassingGeneralQAandOpen-endedGeneration,MathReasoning,Science,andHallucination
benchmarks, totaling 15 benchmarks. We conduct a comprehensive analysis of the evaluation
benchmarks,theirassociatedmetrics,andthetraininghyperparametersforboththeinitialvision-
60.4 Dataset Dataset
LLaVA ShareGPT4V 0.75 LLaVA ShareGPT4V
LLaVAR SMR 0.50 LLaVAR SMR
0.2
0.25
0.0 0.00
1 4 8 11 14 17 20 24 27 30 12 36 60 84 108 132 156 180 204 228
Lengths Lengths
(a) Averagelengthofquerytext. (b) Averagelengthofanswertext.
Figure4: Comparisonoftheaveragelengthsofqueryandanswertextsacrossdifferentdatasets.
Table 1: Comparison with SoTA methods on academic-task-oriented datasets and bench-
marksforinstruction-followingLMMs. VQAT: TextVQA[44],MMB:MMBench[35],MMBC:
MMBench-Chinese[35];MMMU : validationsetofMMMU[55];LLaVAW: LLaVA-Bench(In-
v
the-Wild)[34];MMEP,C: PerceptionandCognitioninMME[12]. Thebestresultismarkedinbold,
andthesecondbestresultisunderlined. †meansusingLoRAduringtheinstructiontuningphase.
Method LLM VQAT GQA VQAv2 MMB MMBC MMMUv LLaVAW MMEP MMEC MM-Vet
InstructBLIP[10] Vicuna-7B 50.10 49.20 - 36.00 23.70 32.90 60.90 - - 26.20
Qwen-VL[3] Qwen-7B 63.80 59.30 78.80 38.20 7.40 35.90 - - - -
LLaVA-1.5[32] Vicuna-7B 58.20 62.00 78.50 64.30 58.30 - 65.40 1510 - 30.50
LLaVA-1.5[32] Vicuna-13B 61.30 63.30 80.00 67.70 63.60 36.40 72.50 1531 295 35.40
ShareGPT4V[7] Vicuna-7B - - 80.60 68.80 62.20 - 72.60 1567 303 37.60
LLaVA-1.5[32] Llama3-8B 58.94 61.94 79.49 72.94 67.70 38.00 70.50 1544 328 34.80
WithHighResolution
OtterHD-8B[23] Fuyu-8B - - - 58.30 - - - 1223 331 26.30
Monkey[29] Qwen-7B - 60.70 80.30 72.40 67.50 - - 1522 401 33.00
LLaVA-HD[32] Vicuna-13B 62.50 64.70 81.80 68.80 61.90 - 72.00 1500 - -
LLaVA-NeXT[33] Vicuna-13B - - - - - 35.90 72.30 1575 316 48.40
Ours
SliME-7B Vicuna-7B 64.39 63.13 80.32 69.32 61.85 37.20 76.10 1544 383 35.40
SliME-8B Llama3-8B 64.76 63.94 80.69 75.00 71.80 41.20 73.90 1578 337 37.40
SliME-8B† Llama3-8B 65.26 63.94 80.79 75.42 70.96 40.80 64.90 1573 346 36.80
SliME-13B Vicuna-13B 66.11 63.60 80.43 71.13 65.20 38.00 73.10 1606 293 41.20
Privatemodels
Gemini Pro[46] - 74.60 - - 73.60 74.30 47.90 - 1496 436 59.20
Qwen-VL-Plus[3] - - - - - 68.00 45.20 - 1681 502 -
GPT-4V[43] - 78.00 - - 77.00 74.40 58.10 - 1409 517 56.80
language alignment pretraining and the subsequent visual instruction tuning stages. Details are
providedinAppendixD.Thetrainingprocessutilizes8×A100(80G)GPUs.
3.1 NumericalResults
GeneralQAandOpen-endedGeneration. WeassesstheperformanceofSliMEacrossarangeof
benchmarks,encompassingbothacademic-task-orientedassessmentsandrecentbenchmarkstailored
forinstruction-followingLLMs,totaling9inall. OurresultsdemonstratethatSliMEconsistently
achieves superior performance across all benchmarks, even when compared to LLMs of similar
scale,despiteutilizingsignificantlysmallerpretrainingandinstructiontuningdatasetsthanother
methods[29,3,23]. Notably,SliME-8BevensurpassesGeminiProonseveralbenchmarkssuchas
MMBandMME.PriorresearchhasfrequentlyindicatedthattheLoRAmodelperformscomparably
tofullfine-tuning,atrendthatholdstrueacrossmanyofourbenchmarks. However,weobservethat
modelstrainedwithLoRAstruggleininstruction-followingtasks. Thisobservationisbolsteredby
theperformancegapobservedintheLLaVA-benchbetweenSliME-8BandSliME-8B†. Furthermore,
inourevaluationofMathVerse,wefindthatwhilethemodelpromptexplicitlyrequestedconcise
answers,SliME-8B† consistentlygeneratesresponseswithintermediatereasoning,abehaviorabsent
inSliME-8B.WedelveintoadetailedanalysisoftheseinconsistenciesinappendixD.2.
Scientific, Mathematical, andHallucination. Wefurtherassessthehallucinatorypropertyand
mathematicalproficiencyofSliME.AsdemonstratedinTable.2,SliMEachievesstate-of-the-art
performance,comparabletoGeminiPro,acrossallthreemathematicalbenchmarks. Moreover,its
performanceontheScienceQA-Imgsplitandhallucinationbenchmarksisparticularlynoteworthy,
affirmingtheefficacyoftheproposedSliME.
7
oitar
tnuoC
elpmaS
oitar
tnuoC
elpmaSTable2: ComparisonwithSoTAmethodsonScienceandMathematicalReasoningbenchmarks.
SQAI: ScienceQA-IMG;MMEH: thesumofscoresinexistence,count,position,colorsplitsof
MMEbenchmark. †meansusingLoRAduringtheinstructiontuningphase
ScienceandMathematicalReasoning Hallucation
Method LLM MathVerse MathVista MathVision ScienceQA POPE AMBER MMEH
InstructBLIP[10] Vicuna-7B - 25.30 - 60.50 - 81.70 -
Qwen-VL[3] Qwen-7B - 33.80 10.53 67.10 - 84.90 606
LLaVA-1.5 Vicuna-7B - 26.10 10.20 66.80 85.90 74.70 -
LLaVA-1.5 Vicuna-13B 7.60 26.10 13.10 71.60 85.90 - 643
ShareGPT4V Vicuna-7B 13.10 25.80 12.50 68.40 - - -
LLaVA-1.5 LLama3-8B 13.80 28.40 14.75 77.64 85.10 85.00 634
WithHighResolution
OtterHD-8B[23] Fuyu-8B - 23.40 - - 86.00 89.10 -
Monkey[29] Qwen-7B - 34.80 - 69.40 - 86.00 -
LLaVA-HD[32] Vicuna-13B - - - 71.00 86.30 - -
Ours
SliME-7B Vicuna-7B 17.50 37.50 16.12 76.80 85.40 87.80 633
SliME-8B LLama3-8B 22.90 43.30 16.78 84.18 86.00 88.90 671
SliME-8B† LLama3-8B 21.80 43.60 16.12 84.13 86.00 90.10 645
SliME-13B Vicuna-13B 19.00 40.80 18.09 80.17 86.30 89.40 673
Privatemodels
Gemini Pro[46] - 22.30 45.20 17.11 - - - 560
Qwen-VL-Plus[3] - 11.80 43.30 - - - - 670
GPT-4V[43] - 38.30 49.90 22.37 - - 87.40 595
Table 3: Ablation results on the global MOE Table4: Ablationresultsondifferenttreatments
,thetokennumberofthecompressionlayer , forglobalandlocalfeatures ,andtwodifferent
therouterparameterγ andthetrainingdata . trainingstrategies .
Dataset POPE GQA SQA VQAT AMBER Dataset POPE GQA SQA VQAT AMBER
Baseline 83.80 61.94 74.30 58.94 87.70
Baseline 84.20 61.94 74.30 58.94 87.70
Global MOE 84.10 62.58 77.00 60.57 88.70
LLaVA-HD 85.00 62.48 72.97 61.48 88.20
Nq=64 83.80 62.91 76.15 63.09 87.60 Monkey 83.10 60.70 73.85 62.14 86.90
Nq=196 84.00 62.51 75.56 62.48 87.40
Nq=144 84.20 62.96 77.09 63.58 88.20 E2E 82.90 61.90 74.12 59.69 86.00
γ=90% 84.20 62.74 76.05 64.05 88.10 only Global 82.10 62.43 75.26 43.65 84.30
γ=75% 84.50 63.09 77.44 63.83 88.40 only Local 43.40 38.70 71.24 58.22 78.20
γ=50% 84.90 63.11 77.09 62.75 88.40
Alternating 84.20 62.96 77.44 63.58 88.40
With SMR 84.50 62.58 82.29 59.89 88.40 only Global 84.10 62.78 76.01 59.42 87.30
Final 84.9 63.94 84.18 64.76 88.90 only Local 82.10 53.21 76.23 62.45 85.60
3.2 AblationStudiesandAnalysis
Why Different Strategies for Global and Local Treatment are Necessary? When comparing
twostrategiesfromLLaVA-HD[32]andMonkey[29]withidenticalhyperparametersandslicing
strategies, it becomes apparent why different treatment strategies for global and local features
are essential. LLaVA-HD does not compress local features, in contrast, all image features are
directlyprojectedbyanMLP,resultinginamaximalcontextsizeof4096. Thatis,thisapproach
significantly increases both training and inference times. Conversely, Monkey compresses all
global and local image tokens using 144 learnable query embeddings, akin to SliME. Despite
LLaVA-HDintroducingmoreimagetokens,ourapproachoutperformsit. Forinstance,intheSQA
dataset,only globalofSliMEachievescommendableperformance,highlightingtheimportance
of global context in SQA tasks. However, as image features are primarily dominated by local
imagedetails,LLaVA-HDisdetrimentaltotheSQAdataset. Conversely,fordatasetslikeVQAT,
whichdemandmoreimagedetails,LLaVA-HDconsistentlyachievesperformancegains. Notably,
LLaVA-HD slightly outperforms SliME in the POPE benchmark. This is likely because POPE
questionsaresimplisticandfocusonsingleobjectsintheimage. Therefore,evenwithsomeloss
of local detail information, LLaVA-HD can answer such questions more effectively. Monkey’s
approach, compressing all features, surpasses LLaVA-HD in SQA and VQAT by nearly 1 point.
However,itperformsinferiorlyinotherbenchmarks,emphasizingtheimportanceofmaintaining
the global context without compression. In contrast, SliME maintains all the global context and
providesadditionalimagedetailwithcompression,yieldingpromisingresultsregardlessofwhether
thedatasetsprioritizeglobalcontextorlocaldetails.
8ImpactofAlternatingTrainingonPerformance.Inthispart,weinvestigatetheeffectofalternating
training on model performance. To assess the significance of Alternating Training, we initially
comparetheperformancedirectly(lines4&7inTable.3),revealingasubstantialperformancegap
betweenthem. Tofurtherexplorethisphenomenon,weisolatetheglobalandlocalfeaturesasimage
tokensrespectivelytoassesstheamountofimageinformationprovidedbyeach. Notably,forthe
modelthatistrainedend-to-end,weobservethatutilizingonlyglobalfeaturesyieldssatisfactory
results,whilethelocalfeaturesareinadequatelytrained,resultinginpoorperformanceacrossmost
benchmarks. Conversely, whenemployingonlylocalfeaturesforSliME,performanceimproves
markedly. Thisimprovementcanbeattributedtothemodel’sdedicatedlearningoflocalfeature
compression,resultinginwell-trainedlocalfeatures. Despitethisimprovement,utilizingonlylocal
featuresprovesinsufficientacrossbenchmarks,underscoringthecrucialofglobalview.
EffectofNumberofReserveTokens. Additionally,wevalidateourhypothesisthatmoreimage
tokensdonotalwaysyieldsuperiorresults.Forinstance,whenγissetto75%,consistentperformance
gainsareevidentacrossmostbenchmarks. Thisindicatesthatbydiscardingirrelevantimagetokens
andpaddingtokens,themodelcanfocusmoreonthosemostpertinenttotheposedquestion.
Ablation Studies. In Table 3, we provide a comprehensive analysis of the effectiveness of each
componentwithinSliME.Throughtheutilizationofvariousadaptersforglobalfeatures,wehave
witnessedanotableimprovementinimageunderstanding,resultinginenhancedperformanceacross
arangeofbenchmarks. Thisunderscoresthesignificanceofleveragingcomplexadaptersforbetter
extractionofglobalinformation. Theinclusionofadetailedlocalcompressionlayerhasproven
particularlyadvantageousfortasksdemandingintricatevisualanalysis,suchasTextVQA.Increasing
N from64to144furtheramplifiesperformance. However,asthenumberoflocalfeaturetokens
q
increases,performancedoesnotconsistentlyrise. Thisisattributedtotheovershadowingofglobal
contextbytheabundanceoflocaltokens,asevidencedbyperformancedipsacrossmostbenchmarks.
Notably,excessivelocaldetailcanalsohinderperformancegainsinVQAT. Therefore,weadopt144
asourdefaultsetting. Lastly,theproposedSMRdatademonstratesasignificantenhancementinthe
model’sreasoningability,especiallyevidentinmathematicalandscientificdatasets,whilemitigating
hallucination. Importantly,rigorouscheckshavebeenconductedonalltrainingdatatoensurethere
arenoconcernsregardingdataleakagebetweentrainingandtestinstances.
Qualitative Results of
SliMEinhigh-resolution
image perception are il- Query: What is written at the top of the
lustrated in Fig. 5, and yellow sticker on the fridge?
Figs. 9 to 9. These re-
sults emphasize the im- The yellow sticker on the fridge has the word "Warning"
written at the top.
portanceoflocalfeatures
to SliME, as they enrich
The yellow sticker on the fridge says"no smoking"
image details and facili-
tateadeeperunderstand-
ingofvisioninformation.
Figure5: High-resolutionimageperception.
Moreover, the ultimate
SliME demonstrates ro-
bustopen-generationcapabilities,includingtaskssuchascodegenerationfromflowcharts(Fig.10),
storycreationbasedonimages(Fig.11),andprovidingsuggestions(Fig.12).
4 ConclusionandDiscussion
Inthispaper,wedelveintoelucidatingtheintricaciesofdesigninglargemultimodalmodels,witha
specificfocusonhigh-resolutionimageprocessing. Unlikepreviousstudiesthattreatboththeglobal
viewandslicedlocalimagepatchesindiscriminately,ourapproachinvolvesprojectingandextracting
globalcontextusingamixtureofexperts,allwithoutanyfeaturecompression. Thismethodology
isrootedinthebeliefthatglobalcontextencapsulatesthemajorityofimageinformationandholds
greatersignificancethanlocalpatches. Localfeaturesundergocompressionandselectionbasedon
theirrelevancetothequery,therebymitigatingcomputationcosts. Althoughtrainingtheframework
end-to-endinitiallyyieldssubparperformance,weaddressthisbyformulatingtheproblemintoabi-
levelformulationandemployinganalternatingtrainingstrategy. Thisstrategicmaneuvercircumvents
optimizationdilemmasinherentinend-to-endtraining. Ourframework,dubbedSliME,demonstrates
9promising performance across more than 10 benchmarks and even matches the performance of
proprietaryLMMstrainedonsignificantlylargerdatasets,allwithonly2milliontrainingdatapoints.
LimitationandFutureWork. Onemainlimitationliesinthe3-stagetrainingapproach. While
alternatingtrainingprovessuperiortoE2Etrainingboththeoreticallyandempirically,itinevitably
extendsthetrainingduration. Apromisingavenueforimprovementinvolvesdelvingdeeperinto
optimizationmethodstailoredforsuchabilinearformulation[59],potentiallyconvertingthealternat-
ingtrainingstrategyintoasoftconstraintwithinthegradientduringE2Etraining. Anotherfruitful
directionforfutureresearchisimagetokenreduction. Giventhatexistingstudiesconsolidatealllocal
andglobalfeaturesintoLLMs,thecomputationalcostforprocessingvery-high-resolutionimages
becomes prohibitively high. Therefore, an open question remains: can we further reduce image
tokens,drawinginspirationfromtechniquessuchastokenmerging[4]incomputervision? Bydoing
so,wemaypreservesufficientlocaldetailswithoutnecessitatingadditionalimagetokensforLLMs.
References
[1] RohanAnil,AndrewMDai,OrhanFirat,MelvinJohnson,DmitryLepikhin,AlexandrePassos,
SiamakShakeri,EmanuelTaropa,PaigeBailey,ZhifengChen,etal. Palm2technicalreport.
arXivpreprintarXiv:2305.10403,2023.
[2] AnasAwadalla,IrenaGao,JoshGardner,JackHessel,YusufHanafy,WanrongZhu,Kalyani
Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, et al. Openflamingo: An open-
source framework for training large autoregressive vision-language models. arXiv preprint
arXiv:2308.01390,2023.
[3] JinzeBai,ShuaiBai,ShushengYang,ShijieWang,SinanTan,PengWang,JunyangLin,Chang
Zhou, and Jingren Zhou. Qwen-vl: A frontier large vision-language model with versatile
abilities. arXivpreprintarXiv:2308.12966,2023.
[4] DanielBolya,Cheng-YangFu,XiaoliangDai,PeizhaoZhang,ChristophFeichtenhofer,and
JudyHoffman. Tokenmerging: YourViTbutfaster. InICLR,2023.
[5] TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhariwal,
ArvindNeelakantan,PranavShyam,GirishSastry,AmandaAskell,etal. Languagemodelsare
few-shotlearners. NeurIPS,2020.
[6] JiaqiChen,JianhengTang,JinghuiQin,XiaodanLiang,LingboLiu,EricPXing,andLiangLin.
Geoqa: Ageometricquestionansweringbenchmarktowardsmultimodalnumericalreasoning.
arXivpreprintarXiv:2105.14517,2021.
[7] LinChen,JisongLi,XiaoyiDong,PanZhang,ConghuiHe,JiaqiWang,FengZhao,andDahua
Lin. Sharegpt4v: Improvinglargemulti-modalmodelswithbettercaptions. arXivpreprint
arXiv:2311.12793,2023.
[8] Wei-LinChiang,ZhuohanLi,ZiLin,YingSheng,ZhanghaoWu,HaoZhang,LianminZheng,
SiyuanZhuang,YonghaoZhuang,JosephEGonzalez,etal. Vicuna: Anopen-sourcechatbot
impressinggpt-4with90%*chatgptquality. Seehttps://vicuna.lmsys.org(accessed14April
2023),2023.
[9] AakankshaChowdhery,SharanNarang,JacobDevlin,MaartenBosma,GauravMishra,Adam
Roberts,PaulBarham,HyungWonChung,CharlesSutton,SebastianGehrmann,etal. Palm:
Scalinglanguagemodelingwithpathways. JMLR,2023.
[10] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng
Wang, BoyangLi, PascaleNFung, andStevenHoi. Instructblip: Towardsgeneral-purpose
vision-languagemodelswithinstructiontuning. NeurIPS,2024.
[11] DannyDriess,FeiXia,MehdiSMSajjadi,CoreyLynch,AakankshaChowdhery,BrianIchter,
Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied
multimodallanguagemodel. arXivpreprintarXiv:2303.03378,2023.
10[12] ChaoyouFu,PeixianChen,YunhangShen,YuleiQin,MengdanZhang,XuLin,JinruiYang,
Xiawu Zheng, Ke Li, Xing Sun, Yunsheng Wu, and Rongrong Ji. Mme: A comprehensive
evaluationbenchmarkformultimodallargelanguagemodels. arXivpreprintarXiv:2306.13394,
2023.
[13] YashGoyal,TejasKhot,DouglasSummers-Stay,DhruvBatra,andDeviParikh. Makingthe
vinvqamatter: Elevatingtheroleofimageunderstandinginvisualquestionanswering. In
CVPR,2017.
[14] Qidong Huang, Xiaoyi Dong, Pan Zhang, Bin Wang, Conghui He, Jiaqi Wang, Dahua Lin,
Weiming Zhang, and Nenghai Yu. Opera: Alleviating hallucination in multi-modal large
languagemodelsviaover-trustpenaltyandretrospection-allocation. CVPR,2024.
[15] Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual
reasoningandcompositionalquestionanswering. InCVPR,pages6700–6709,2019.
[16] Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual
reasoningandcompositionalquestionanswering. CVPR,2019.
[17] AndrewJaegle, FelixGimeno, AndrewBrock, AndrewZisserman, OriolVinyals, andJoao
Carreira. Perceiver: Generalperceptionwithiterativeattention,2021.
[18] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh
Chaplot,DiegodelasCasas,FlorianBressand,GiannaLengyel,GuillaumeLample,Lucile
Saulnier,etal. Mistral7b. arXivpreprintarXiv:2310.06825,2023.
[19] KushalKafle,BrianPrice,ScottCohen,andChristopherKanan. Dvqa: Understandingdata
visualizationsviaquestionanswering. InCVPR,2018.
[20] AniruddhaKembhavi,MikeSalvato,EricKolve,MinjoonSeo,HannanehHajishirzi,andAli
Farhadi. Adiagramisworthadozenimages. InECCV.Springer,2016.
[21] AniruddhaKembhavi,MinjoonSeo,DustinSchwenk,JonghyunChoi,AliFarhadi,andHan-
naneh Hajishirzi. Are you smarter than a sixth grader? textbook question answering for
multimodalmachinecomprehension. InCVPR,2017.
[22] Hugo Laurençon, Lucile Saulnier, Léo Tronchon, Stas Bekman, Amanpreet Singh, Anton
Lozhkov,ThomasWang,SiddharthKaramcheti,AlexanderRush,DouweKiela,etal. Obelics:
Anopenweb-scalefiltereddatasetofinterleavedimage-textdocuments. NeurIPS,2024.
[23] BoLi,PeiyuanZhang,JingkangYang,YuanhanZhang,FanyiPu,andZiweiLiu. Otterhd: A
high-resolutionmulti-modalitymodel. arXivpreprintarXiv:2311.04219,2023.
[24] BoLi,YuanhanZhang,LiangyuChen,JinghaoWang,JingkangYang,andZiweiLiu. Otter: A
multi-modalmodelwithin-contextinstructiontuning. arXivpreprintarXiv:2305.03726,2023.
[25] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-
image pre-training with frozen image encoders and large language models. arXiv preprint
arXiv:2301.12597,2023.
[26] Lei Li, Yuqi Wang, Runxin Xu, Peiyi Wang, Xiachong Feng, Lingpeng Kong, and Qi Liu.
Multimodalarxiv: Adatasetforimprovingscientificcomprehensionoflargevision-language
models. arXivpreprintarXiv:2403.00231,2024.
[27] Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, Ruihang Chu,
Shaoteng Liu, and Jiaya Jia. Mini-gemini: Mining the potential of multi-modality vision
languagemodels. arXivpreprintarXiv:2403.18814,2024.
[28] YifanLi,YifanDu,KunZhou,JinpengWang,WayneXinZhao,andJi-RongWen. Evaluating
objecthallucinationinlargevision-languagemodels. arXivpreprintarXiv:2305.10355,2023.
[29] ZhangLi,BiaoYang,QiangLiu,ZhiyinMa,ShuoZhang,JingxuYang,YaboSun,Yuliang
Liu,andXiangBai. Monkey: Imageresolutionandtextlabelareimportantthingsforlarge
multi-modalmodels. arXivpreprintarXiv:2311.06607,2023.
11[30] JiLin,HongxuYin,WeiPing,YaoLu,PavloMolchanov,AndrewTao,HuiziMao,JanKautz,
MohammadShoeybi,andSongHan. Vila: Onpre-trainingforvisuallanguagemodels,2023.
[31] Tsung-YiLin, MichaelMaire, SergeBelongie, JamesHays, PietroPerona, DevaRamanan,
PiotrDollár,andCLawrenceZitnick. Microsoftcoco: Commonobjectsincontext. InECCV,
2014.
[32] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual
instructiontuning. arXivpreprintarXiv:2310.03744,2023.
[33] HaotianLiu,ChunyuanLi,YuhengLi,BoLi,YuanhanZhang,ShengShen,andYongJaeLee.
Llava-next: Improvedreasoning,ocr,andworldknowledge,January2024.
[34] HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee. Visualinstructiontuning. arXiv
preprintarXiv:2304.08485,2023.
[35] YuanLiu,HaodongDuan,YuanhanZhang,BoLi,SongyangZhang,WangboZhao,YikeYuan,
JiaqiWang,ConghuiHe,ZiweiLiu,KaiChen,andDahuaLin. Mmbench: Isyourmulti-modal
modelanall-aroundplayer?,2024.
[36] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao
Cheng,Kai-WeiChang,MichelGalley,andJianfengGao. Mathvista: Evaluatingmathematical
reasoningoffoundationmodelsinvisualcontexts. InICLR,2024.
[37] PanLu,RanGong,ShibiaoJiang,LiangQiu,SiyuanHuang,XiaodanLiang,andSong-Chun
Zhu. Inter-gps: Interpretablegeometryproblemsolvingwithformallanguageandsymbolic
reasoning. arXivpreprintarXiv:2105.04165,2021.
[38] PanLu,SwaroopMishra,TanglinXia,LiangQiu,Kai-WeiChang,Song-ChunZhu,Oyvind
Tafjord,PeterClark,andAshwinKalyan. Learntoexplain: Multimodalreasoningviathought
chainsforsciencequestionanswering. NeurIPS,2022.
[39] PanLu,LiangQiu,Kai-WeiChang,YingNianWu,Song-ChunZhu,TanmayRajpurohit,Peter
Clark,andAshwinKalyan. Dynamicpromptlearningviapolicygradientforsemi-structured
mathematicalreasoning. arXivpreprintarXiv:2209.14610,2022.
[40] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: A
benchmarkforquestionansweringaboutchartswithvisualandlogicalreasoning.arXivpreprint
arXiv:2203.10244,2022.
[41] BrandonMcKinzie,ZheGan,Jean-PhilippeFauconnier,SamDodge,BowenZhang,Philipp
Dufter,DhrutiShah,XianzhiDu,FutangPeng,FlorisWeers,etal. Mm1: Methods,analysis&
insightsfrommultimodalllmpre-training. arXivpreprintarXiv:2403.09611,2024.
[42] Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman,
TevenLeScao,MSaifulBari,ShengShen,Zheng-XinYong,HaileySchoelkopf,etal. Crosslin-
gualgeneralizationthroughmultitaskfinetuning. arXivpreprintarXiv:2211.01786,2022.
[43] OpenAI. Gpt-4technicalreport. 2023.
[44] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi
Parikh,andMarcusRohrbach. Towardsvqamodelsthatcanread. InCVPR,2019.
[45] RohanTaori,IshaanGulrajani,TianyiZhang,YannDubois,XuechenLi,CarlosGuestrin,Percy
Liang,andTatsunoriBHashimoto. Stanfordalpaca: Aninstruction-followingllamamodel,
2023.
[46] GeminiTeam,RohanAnil,SebastianBorgeaud,YonghuiWu,Jean-BaptisteAlayrac,JiahuiYu,
RaduSoricut,JohanSchalkwyk,AndrewMDai,AnjaHauth,etal. Gemini: afamilyofhighly
capablemultimodalmodels. arXivpreprintarXiv:2312.11805,2023.
[47] HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,Timo-
théeLacroix,BaptisteRozière,NamanGoyal,EricHambro,FaisalAzhar,etal. Llama: Open
andefficientfoundationlanguagemodels. arXivpreprintarXiv:2302.13971,2023.
12[48] HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,
NikolayBashlykov,SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,etal. Llama2: Open
foundationandfine-tunedchatmodels. arXivpreprintarXiv:2307.09288,2023.
[49] Junyang Wang, Yuhang Wang, Guohai Xu, Jing Zhang, Yukai Gu, Haitao Jia, Ming Yan,
JiZhang,andJitaoSang. Anllm-freemulti-dimensionalbenchmarkformllmshallucination
evaluation. arXivpreprintarXiv:2311.07397,2023.
[50] KeWang,JuntingPan,WeikangShi,ZimuLu,MingjieZhan,andHongshengLi. Measuring
multimodalmathematicalreasoningwithmath-visiondataset. arXivpreprintarXiv:2402.14804,
2024.
[51] Ruyi Xu, Yuan Yao, Zonghao Guo, Junbo Cui, Zanlin Ni, Chunjiang Ge, Tat-Seng Chua,
ZhiyuanLiu, andGaoHuang. LLaVA-UHD:anlmmperceivinganyaspectratioandhigh-
resolutionimages. arXivpreprintarXiv:2403.11703,2024.
[52] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang,
Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large
languagemodelswithmultimodality. arXivpreprintarXiv:2304.14178,2023.
[53] ShukangYin,ChaoyouFu,SiruiZhao,KeLi,XingSun,TongXu,andEnhongChen. Asurvey
onmultimodallargelanguagemodels. arXivpreprintarXiv:2306.13549,2023.
[54] WeihaoYu,ZhengyuanYang,LinjieLi,JianfengWang,KevinLin,ZichengLiu,XinchaoWang,
andLijuanWang. Mm-vet: Evaluatinglargemultimodalmodelsforintegratedcapabilities. In
ICML,2024.
[55] XiangYue,YuanshengNi,KaiZhang,TianyuZheng,RuoqiLiu,GeZhang,SamuelStevens,
DongfuJiang,WeimingRen,YuxuanSun,CongWei,BotaoYu,RuibinYuan,RenliangSun,
MingYin,BoyuanZheng,ZhenzhuYang,YiboLiu,WenhaoHuang,HuanSun,YuSu,and
WenhuChen. Mmmu: Amassivemulti-disciplinemultimodalunderstandingandreasoning
benchmarkforexpertagi. arXivpreprintarXiv:2311.16502,2023.
[56] YanZeng,HanboZhang,JianiZheng,JiangnanXia,GuoqiangWei,YangWei,YuchenZhang,
andTaoKong. Whatmattersintrainingagpt4-stylelanguagemodelwithmultimodalinputs?
arXivpreprintarXiv:2307.02469,2023.
[57] YanzheZhang,RuiyiZhang,JiuxiangGu,YufanZhou,NedimLipka,DiyiYang,andTongSun.
Llavar: Enhancedvisualinstructiontuningfortext-richimageunderstanding. arXivpreprint
arXiv:2306.17107,2023.
[58] Yi-FanZhang,WeichenYu,QingsongWen,XueWang,ZhangZhang,LiangWang,RongJin,
andTieniuTan. Debiasinglargevisuallanguagemodels. arXivpreprintarXiv:2403.05262,
2024.
[59] BoZhao,RobertM.Gower,RobinWalters,andRoseYu. Improvingconvergenceandgeneral-
izationusingparametersymmetries,2024.
[60] BoZhao,BoyaWu,andTiejunHuang.Svit:Scalingupvisualinstructiontuning.arXivpreprint
arXiv:2307.04087,2023.
[61] DeyaoZhu, JunChen, XiaoqianShen, XiangLi, andMohamedElhoseiny. Minigpt-4: En-
hancingvision-languageunderstandingwithadvancedlargelanguagemodels. arXivpreprint
arXiv:2304.10592,2023.
13GQA COCO Captions OCR-VQA Flickr30k
Visual reasoning and compositional Object recognition, Scene understanding Optical character recognition Object recognition, Scene
question answering, understanding
Question: Whattypeoffruitintheimageis Question: Please Question:Whowrotethisbook? Question: Please
r A Q t rho en u eu d s en f w s r od t ue r? i io r t g: n ro :A en eWp ntp hh ?l eae t r ic go hl to sr i dis e , d A R Pe in e ts a csc hdwr ii e e wb r se h: tt it oh lh e e se w t hmI im ena ga un g maae t pt. t ab h ia e rt A Q t Aon nu cs se ow ws ote ei kor ri: :n n g:J Y ? ea I ssc o thb isG bri oe or k related d A o on ne ns o tsc ow nr aei eb r f we o: a ot A sh t t e w em h b aI ainm l se ksa ht eg a toe n .l. dd is n g
Answer: Green Looks on.
A-OK-VQA
TextVQA Visual Genome VQA v2 OK-VQA
commonsenseandworld
VisualReasoning Recognizing and Reasoning Commonsense knowledge Outside Knowledge knowledge
Question:
which processor
Brand is featured
on the top left?
Anwer: intel
Figure6: Visualperceptionandreasoningtasksinexistingvisualinstructiontuningdatasets.
A RelatedWork
MultimodalLargeLanguageModelshaveundergonesignificantevolution,initiallyrootedinBERT-
basedlanguagedecodersandlaterincorporatingadvancementsinLLMs.LeveragingadvancedLLMs
suchasGPTs[43,5],PaLM[9,1],BLOOM[42],LLaMA[47,48],Alpaca[45],Vicuna[8],and
Mistral[18],MultimodalLargeLanguageModels(LVLMs)exhibitenhancedcapabilitiesandperfor-
mance,particularlythroughend-to-endtrainingtechniques. Recentmodeldevelopments,including
Flamingo[2],PaLI[22],PaLM-E[11],BLIP-2[25],InstructBLIP[10],Otter[24],MiniGPT-4[61],
mPLUG-Owl[52],LLaVA[34],andQWen-VL[3],bringuniqueperspectivestochallengessuchas
scalingpre-training,enhancinginstruction-followingcapabilities,andovercomingalignmentissues.
Our work is built upon LLaVA [34], but enhances all training datasets, model architecture, and
alignmentstrategies,achievingstate-of-the-artperformanceamongexistingLLMs.
AdvancementsinVisualInstructionTuning: Theefficacyofmultimodalmodelsheavilyrelies
on the availability of high-quality image-text data for fine-tuning, a process known as visual in-
struction tuning [34]. Previous studies have highlighted the limitations of constructing training
sets solely based on existing Visual Question Answering (VQA) datasets [15], often resulting in
degraded model performance. In an effort to address this, MiniGPT-4 [61] meticulously curated
3,500high-qualityimage-textpairsthrougharefinementprocessusingChatGPT,leadingtomore
naturalandreliableresponsespost-fine-tuning. Inapioneeringinitiative,LLaVA[34]systematically
constructedtheLLaVA-Instruct-150Kdatasetforvisualinstructiontuning. EmployingGPT-4,they
generated questions and answers by providing image-level captions and object bounding boxes
fromtheCOCOdataset[31]. Todelvedeeperintotext-richimages,LLaVAR[57]collected422K
noisyinstruction-followinginstancesusingOpticalCharacterRecognition(OCR)resultsand16K
high-qualityinstancesusingGPT-4. InstructBLIP[10]amalgamated26publicdatasets,including
LLaVA-Instruct-150K, to construct visual instruction tuning data. Innovatively, SVIT [60] con-
structedacomprehensivedatasetcomprising4.2millionvisualinstructiontuninginstances,including
conversationquestion-answerpairs,complexreasoningQApairs,referringQApairs,anddetailed
imagedescriptions. However,manyofthesepublicdatasetspredominantlyfocusonvisualperception
and image captioning. In this study, we elevate the complexity of the Visual Instruction Tuning
processbyincorporatingninediversedatasets,encompassingscientificquestions,mathematical/chart
reasoningtasks,andevenfullreasoningpaths. ThisaugmentationaimstoenhanceLMMstoachieve
significantlyimprovedreasoningcapabilities.
B Visualinstructiontuningdatasets
Arxiv-QA[26]4isadatasetfeaturingdiverseimagesfromscientificdomains. LeveragingGPT-4V,
theauthorsgeneratedinstruction-tuningdatasetsforgeneratingQApairsbasedonfiguresextracted
fromscientificpapers. Afterfilteringoutinvalidsamples,thedatasetconsistsof100KQApairs. The
questionsinthedatasethaveanaveragewordcountof16.98,whiletheoptionsforeachquestion
4https://huggingface.co/datasets/MMInstruction/ArxivQA
14haveanaveragewordcountof31.86. Onaverage,thereare4.20optionsperquestion. ForArxivQA,
weutilizerationalesasanswersforsupervisedfine-tuning.
ScienceQA[38]5 isadatasetcharacterizedbyrichdomaindiversityacrossthreesubjects: natural
science,languagescience,andsocialscience. Questionswithineachsubjectarecategorizedfirst
bytopic(e.g.,Biology,Physics,Chemistry)andthenfurthercategorizedbysubtopics(e.g.,Plants,
Cells,Animals)andskills(e.g.,Classifyfruitsandvegetablesasplantparts,Identifycountriesof
Africa). ScienceQAencompasses26topics,127categories,and379skills,providingcomprehensive
coverageacrossvariousdomains. ForScienceQA,weutilizeboththequestionandtheinstructions
onhowtosolvethequestionastheprompt.
MATH-Vision[50]6isanintricatelyassembledcompilationof3,040meticulouslyselectedmathe-
maticalproblemsaccompaniedbyvisualcontextssourcedfromauthenticmathcompetitions. Encom-
passing16distinctmathematicaldisciplinesandgradedacross5levelsofdifficulty,thedatasetoffers
acomprehensiveanddiversearrayofchallengestoassessthemathematicalreasoningcapabilitiesof
LargeMultimodalModels(LMMs).
AI2D[20]7AI2Diagrams(AI2D)isadatasetofover5000gradeschoolsciencediagramswithover
150000richannotations,theirgroundtruthsyntacticparses,andmorethan15000corresponding
multiplechoicequestions.
TextBookQA[21]8 isderivedfrommiddleschoolsciencecurriculaandcomprises1,076lessons
extractedfromLifeScience,EarthScience,andPhysicalSciencetextbooks.Thedatasetencompasses
a total of 26,260 questions, with 12,567 of them accompanied by diagrams. We leverage the
instructionalcontentofimageswiththepromptthatchosenfromtable5andconsiderquestionswith
accompanyingdiagramsnecessaryforansweringasVQAtasks.
GeoQA3[6]9 isadatasetcomprising4,998diversereal-worldgeometricproblemssourcedfrom
Chinesemiddleschoolexams. Eachproblemisfurtherannotatedwithspecificprogramsthatdescribe
theproblem-solvingprocess. Thedatasetencompassesthreemainproblemtypes: anglecalculation,
lengthcalculation,andothers,whichincludevarioustypesofproblemssuchasareacalculation.
TheGeometry3KDataset[37]10comprises3,002SAT-styleproblemssourcedfromtwohigh-school
textbooks covering diverse graph and goal types. Additionally, each problem in Geometry3K is
annotatedwithdensedescriptionsinformallanguage.
TabularMathWordProblems(TabMWP)[39]11introducesanoveldatasetcontaining38,431open-
domain grade-level problems requiring mathematical reasoning on both textual and tabular data.
EachquestioninTabMWPisassociatedwithatabularcontext,presentedintheformofanimage,
semi-structuredtext,andastructuredtable. Questionsarecategorizedintofree-textandmulti-choice
types,witheachproblemannotatedwithgoldsolutionselucidatingthemulti-stepreasoningprocess.
DataVisualizationQuestionAnswering(DVQA)[19]12isadatasetdesignedtoassessvariousaspects
ofbarchartunderstandinginaquestionansweringframework. UnlikeVisualQuestionAnswering
(VQA),DVQAnecessitatesprocessingwordsandanswersuniquetoaparticularbarchart. DVQA
facilitates the automatic querying of extensive repositories of charts within scientific documents,
webpages,andbusinessreports. Werandomlyselected100Kdatafromthereasoningsplitsforour
experiments.
ChartVQA[40]13servesasabenchmarkforquestionansweringaboutchartswithvisualandlogical
reasoning. Weexclusivelyselectedhuman-authoredQApairsandexcludedallmachine-generated
question-answerpairstoensuredataquality,resultinginatotalof9.6Kquestion-answerpairs.
5https://huggingface.co/datasets/derek-thomas/ScienceQA
6https://huggingface.co/datasets/mathvision/mathvision
7https://github.com/allenai/dqa-net
8https://allenai.org/data/tqa
9https://github.com/chen-judge/GeoQA
10https://github.com/lupantech/InterGPS
11https://github.com/lupantech/PromptPG
12https://github.com/kushalkafle/DVQA_dataset
13https://github.com/vis-nlp/ChartQA
15Table5: Thelistofinstructionsfordetailedimagedescription.
Index Description
1 Describethefollowingimageindetail
2 Provideadetaileddescriptionofthegivenimage
3 Giveanelaborateexplanationoftheimageyousee
4 Shareacomprehensiverundownofthepresentedimage
5 Offerathoroughanalysisoftheimage
6 Explainthevariousaspectsoftheimagebeforeyou
7 Clarifythecontentsofthedisplayedimagewithgreatdetail
8 Characterizetheimageusingawell-detaileddescription
9 Breakdowntheelementsoftheimageinadetailedmanner
10 Walkthroughtheimportantdetailsoftheimage
11 Portraytheimagewitharich,descriptivenarrative
12 Narratethecontentsoftheimagewithprecision
13 Analyzetheimageinacomprehensiveanddetailedmanner
14 Illustratetheimagethroughadescriptiveexplanation
15 Examinetheimagecloselyandshareitsdetails
16 Writeanexhaustivedepictionofthegivenimage
C ProofofTheoreticalStatements
C.1 GradientDescentforOptimizingBilinearProblem
Letu andv betheinitialsolutionandisgiveninthefollowingform
0 0
u =α a+β b, v =β a+α b (2)
0 0 0 0 0 0
where α ,β ∈ R are two scales. Here, we use the fact that u and v have to lie in the subspace
0 0
spannedbyaandb. Usingthegradientdescentmethod,weupdatethesolutionu andv as
t t
u =u
−η(cid:0)
u
v⊤−X(cid:1)
v , v =v
−η(cid:0)
v
u⊤−X(cid:1)
u
t+1 t t t t t+1 t t t t
Thefollowingtheoremdescribesthedynamicsofuandvovertheiterationsofgradientdescent.
Theorem3. Definez =(α ,β )⊤. UndertheinitializationofuandvgiveninEq.2,wehave,for
t t t
allt≥0
u =α a+β b, v =β a+α b
t t t 0 t t
with|u |2 =|v |2,where
t t
(cid:18) (cid:19)
1+η(1−|u |2) ηa⊤b
z = t z (3)
t+1 ηa⊤b 1+η(1−|u |2) t
t
(cid:124) (cid:123)(cid:122) (cid:125)
:=Ft
Proof. Weprovetheresultbyinduction. Itiseasytoseethatthetheoremholdfort=0. Wewill
thenassumeitistrueforthecaseoftandshowitalsoholdsfort+1. Wefirsthaveu as
t+1
u = u
−η(cid:0)
u
v⊤−X(cid:1)
v
t+1 t t t 0
= u
(cid:0)
1−η|v
|2(cid:1)
+ηXv
t t t
= u
(cid:0)
1−η|v
|2(cid:1) +η(cid:0) ab⊤+ba⊤(cid:1)
(β a+α b)
t t t t
= u (cid:0) 1−η|u |2(cid:1) +η(cid:0) β b⊤aa+β b+α a+α a⊤bb(cid:1)
t t t t t t
= (cid:0) (1−η|u |2+η)α +ηa⊤bβ (cid:1) a+(cid:0) (1−η|u |2+η)β +a⊤bα (cid:1) b
t t t t t t
whereweutilizetheassumption|v |=|u |. Hence,bywritingu =α a+β b,wehave
t t t+1 t+1 t+1
(cid:18) (cid:19) (cid:18) (cid:19)(cid:18) (cid:19)
α 1+η(1−|u |2) ηa⊤b α
t+1 = t t
β ηa⊤b 1+η(1−|u |2) β
t+1 t t
16Similarly,weapplygradientdescenttoupdatev asfollows
t
v = v
−η(cid:0)
v
u⊤−X(cid:1)
u
t+1 t t t t
= v
(cid:0)
1−η|u
|2(cid:1)
+ηXu
t t t
= v
(cid:0)
1−η|u
|2(cid:1) +η(cid:0) ab⊤+ba⊤(cid:1)
(α a+β b)
t t t t
= v (cid:0) 1−η|v |2(cid:1) +η(cid:0) b⊤aα a+α b+β a+a⊤bβ b(cid:1)
t t t t t t
= (cid:0) (1−η|v |2+η)β +ηa⊤bα (cid:1) a+(cid:0) (1−η|v |2+η)α +a⊤bβ (cid:1) b
t t t t t t
Thus,itiseasytoseethatv =β a+α b.
t+1 t+1 t+1
Next, we will show that the updating in Eq. 3 will not converge the largest eigenvector of M.
Let λ > λ denote the largest and smallest eigenvalues of M, and let w and w denote the
+ − + −
correspondingeigenvectors,respectively. Wehave
λ =1+a⊤b, λ =1−a⊤b
+ −
and
(cid:18) (cid:19) (cid:18) (cid:19)
1 1 1 1
w = √ , w = √
+ 2 1 − 2 −1
Letτ =z⊤w andν =z⊤w . Usingthesenotation,wecanwrite|u |2as
t t + t t − t
|u |2 =z⊤Mz =(cid:0) 1+a⊤b(cid:1) τ2+(cid:0) 1−a⊤b(cid:1) ν2 (4)
t t t t t
Thefollowinglemmadescribeshowτ andν evolvesoveriterations.
t t
Lemma1. Wehave
τ =(cid:0) 1+η(1+a⊤b−|u |2)(cid:1) τ , ν =(cid:0) 1+η(1−a⊤b−|u |2)(cid:1) ν (5)
t+1 t t t+1 t t
Proof. AccordingtoEq.3,wecanwrite
F =(1−η|u |2)I+ηM
t t
andthereforecandecomposeitintothefollowingform
F =(1−η|u |2+ηλ )w w⊤+(1−η|u |2+ηλ )w w⊤
t t + + + t − − −
Sincez =τ w +ν w ,wehave
t + + t −
F z =(1−η|u |2+ηλ )τ w +(1−η|u |2+ηλ )ν w
t t t + t + t − t −
andtherefore
τ =(cid:0) 1+η(1+a⊤b−|u |2)(cid:1) τ , ν =(cid:0) 1+η(1−a⊤b−|u |2)(cid:1) ν
t+1 t t t+1 t t
Whenηisverysmall,wecanwritetheupdatingequationinEq.5asdifferentialequations,i.e.
dτ t =(cid:0) 1+a⊤b−|u |2(cid:1) τ , dν t =(cid:0) 1−a⊤b−|u |2(cid:1) ν (6)
dt t t dt t t
ThefollowingrevealstheconvergencepropertyofEq.6.
Theorem4. ODEinEq.6havetwoconvergedsolutions,withonebeingτ → 1,ν → 0andthe
t t
otherbeingτ →0,ν →1whent→∞
t t
Proof. Usingtheexpressionof|u |2inEq.4,wehave
t
d|u t|2 = 2(cid:0) 1+a⊤b(cid:1) τ dτ t +2(cid:0) 1−a⊤b(cid:1) ν dν t
dt t dt t dt
= 2(cid:0) 1+a⊤b(cid:1)(cid:0) 1+a⊤b−|u |2(cid:1) τ2+2(cid:0) 1−a⊤b(cid:1)(cid:0) 1−a⊤b−|u |2(cid:1) ν2
t t t t
= 2(1−|u |2)|u |2+2a⊤b(cid:0) (1+a⊤b)τ2−(1−a⊤b)ν2(cid:1)
t t t t
17WhentheODEconvergestoafixpointu ,weshouldhave
∗
2(1−|u |2)|u |2+2a⊤b(cid:0) (1+a⊤b)τ2−(1−a⊤b)ν2(cid:1) =0 (7)
∗ ∗ ∗ ∗
Atthesametime,accordingtoEq.6,whenODEconverges,weshouldhaveeither
|u |2 =1+a⊤b, v =0
∗ ∗
or
|u |2 =1−a⊤b, τ =0.
∗ ∗
WecompletetheproofbycombiningthesetwoconditionswiththeconditioninEq.7.
AsindicatedbyTheorem4,theiterativeupdatingbygradientdescentdoesnotnecessarilyconverge
totheoptimalsolution,i.e.,τ =1andν =0,renderingthesimultaneousupdatingofuandvless
∗ ∗
idealforoptimizingtheobjectivefunctionofthebilinearform.
C.2 OptimizingBilinearProblembyAlternatingOptimization
Wewilldemonstratethattheissuewithgradientdescent(ormoreprecisely,simultaneouslyupdating
uandv)canbeeffectivelyaddressedthroughalternatingoptimization. Specifically,wewilloptimize
v with u fixed, and then optimize u with v fixed. We will illustrate that this approach leads to
convergencetowardstheoptimalsolution. Letu =α a+β b. Wedenotethesequentialsolution
0 0 0
u obtainedthroughalternatingoptimizationasu =α a+β b. Thefollowingtheoremdescribes
t t t t
theevolutionofz =(α ,β )⊤overiterations.
t t t
Theorem5. Wehave
1
z = M2z
t+1 |u |2|v |2 t
t t
Proof. Letu =α a+β b. Byfixingu ,wehavetheoptimalsolutionforv
t t t t t
1
v = Xu
t |u |2 t
t
= 1 (cid:0) a⊤bα a+α b+β a+a⊤bβ b(cid:1)
|u |2 t t t t
t
Bywritingv =β′a+α′b,wehave
t t t
(cid:18) (cid:19) (cid:18) (cid:19)(cid:18) (cid:19)
α′ 1 1 a⊤b α
t = t
β t′ |u t|2 a⊤b 1 β t
Wethenfixv andfindtheoptimalsolutionforu ,i.e.
t t+1
1
u = Xv
t+1 |v |2 t
t
=
1 (cid:0) a⊤bβ′a+β′b+α′a+a⊤bα′b(cid:1)
|v |2 t t t t
t
Bywritingu =α a+β b,wehave
t+1 t+1 t+1
(cid:18)
α
(cid:19)
1
(cid:18)
1 a⊤b
(cid:19)(cid:18)
α′
(cid:19)
1
(cid:18)
1 a⊤b
(cid:19)2(cid:18)
α
(cid:19)
t+1 = t = t
β t+1 |v t|2 a⊤b 1 β t′ |u t|2|v t|2 a⊤b 1 β t
AccordingtoTheorem5,byalternatingoptimization,wehave
z ∝M2tz
t 0
implying that z will guarantee to converge to the largest eigenvector of M, which resolves the
t
limitationofgradientdescent.
18D Experiments
D.1 ExperimentalDetails
General QA and Open-ended Generation BenchmarksTheMMMUdataset[55]comprises
meticulouslycuratedmultimodalquestionstotaling11.5K,sourcedfromcollegeexams,quizzes,
and textbooks. Spanning six core disciplines—Art and Design, Business, Science, Health and
Medicine, HumanitiesandSocialScience, andTechandEngineering—itcovers30subjectsand
183subfields. Withafocusonadvancedperceptionandreasoningembeddedwithdomain-specific
knowledge, MMMU challenges models to tackle tasks akin to those encountered by experts. It
incorporates30highlydiverseimagetypes,includingcharts,diagrams,maps,tables,musicsheets,
andchemicalstructures,divergingfromexistingbenchmarksbyemphasizingsophisticatedperception
andreasoning. TheMMEdataset[12]distinguishesitselfbyconcurrentlyevaluatingperceptionand
cognitioncapabilities,encompassingtaskssuchasOCR,coarse-grainedobjectrecognition(including
existence, count, position, and color), and fine-grained object recognition (encompassing movie
posters,celebrities,scenes,landmarks,andartworks). Withatotalof14subtasks,MMEprovidesa
comprehensiveevaluation,cateringtotheneedforathoroughassessmentofMLLMsacrossdiverse
modalitiesandcognitivedomains. Additionally,traditionalbenchmarkssuchasMMBench-EN[35],
MMVet[54],GQA[16],VQA[13],andText-VQA[44]wereutilizedtogaugethemodel’svisual
understandingandreasoningabilities. Furtherevaluationsincludedtestingthemodel’sproficiencyin
ChineseanditsunderstandingofChineseculturethroughtheMMBench-CNtest[35],alongwith
assessingconversationabilitiesviaLLaVA-Bench[34].
Math Reasoning and Science QA Benchmarks: MathVista[36]presentsamultifacetedbench-
mark that integrates challenges from diverse mathematical and visual tasks. Comprising 6,141
examples,itamalgamatesdatafrom28existingmultimodaldatasetsalongsidetheintroductionof
threenoveldatasets: IQTest,FunctionQA,andPaperQA.Solvingthesetasksdemandsintricatevisual
comprehensionandcompositionalreasoning,posingsignificantchallengesevenforstate-of-the-art
foundationmodels. MathVision[50]meticulouslycuratesacollectionof3,040high-qualitymathe-
maticalproblemswithvisualcontextsdrawnfromrealmathcompetitions. Encompassing16distinct
mathematicaldisciplinesandgradedacross5difficultylevels,thisdatasetoffersacomprehensiveand
diverserangeofchallengesforassessingthemathematicalreasoningabilitiesofLMMs. MathVerse
providesaninclusivevisualmathbenchmarkbycollecting2,612high-quality,multi-subjectmath
problemswithaccompanyingdiagramssourcedfrompubliclyavailablematerials. Eachproblem
undergoestransformationbyhumanannotators,resultinginsixdistinctversionswithvaryinglevelsof
informationcontentinmultimodality,culminatingin15Ktestsamples. ThisapproachenablesMath-
VersetothoroughlyevaluatetheabilityofLMMstocomprehendvisualdiagramsformathematical
reasoning. ScienceQA[38]covers26topics,127categories,and379skills,offeringcomprehensive
coverageacrossvariousdomains.
HallucinationTheexistenceandcountsubsetsoftheMMEdataset[12]areapplicableforobject-
levelhallucination,whilethepositionandcolorsubsetsaresuitableforattribute-levelhallucination.
POPE[28]introducesastreamlinedmethodforevaluatingobjecthallucinationinLMMs. Inthis
assessmentframework,LMMsaretaskedwithdeterminingthepresenceofaspecificobjectinagiven
image. Thebenchmarkensuresabalancedratiobetweenqueriesprobingexistentandnon-existent
objects(i.e.,50%each). AMBER[49]isanLLM-freeMulti-dimensionalBenchmarkforLMMs
hallucinationevaluation,whichcanbeusedtoevaluatefirthediscriminativetaskincludingexistence,
attributeandrelationhallucination.
Weprovideathoroughexaminationoftheevaluationbenchmarksemployed,accompaniedbytheir
respectivemetricsasdetailedintable6. Additionally,weoutlinethetraininghyperparametersfor
boththeinitialvision-languagealignmentpretrainingandthesubsequentvisualinstructiontuning
stagesintable7.
D.2 ExtendedExperiments
InstructionFollowingConcernsRegardingFine-tuningwithLoRA.AsillustratedinFig.7,within
theMathVersebenchmark,despiteourpromptexplicitlyrequestingthemodelto“Pleasedirectly
answerthequestionandprovidethecorrectoptionletter”,SliME-8B† consistentlygeneratesthe
entirereasoningpath. Whilethisdoesnotnecessarilyresultinincorrectanswers,itdivergesfromour
intendedapproach. Intriguingly,whenwemodifythepromptto“Pleasedirectlyanswerthequestion
19Table6: Summaryoftheevaluationbenchmarks.
Data Responseformattingprompts Metric
Text-VQA Answerthequestionusingasinglewordorphrase. Accuracy(↑)
GQA Answerthequestionusingasinglewordorphrase. Accuracy(↑)
VQA-v2 Answerthequestionusingasinglewordorphrase. Accuracy(↑)
MMBench,MMBench-CN Answerwiththeoption’sletterfromthegivenchoicesdirectly. Accuracy(↑)
MMMU(Multi-choice) Answerwiththeoption’sletterfromthegivenchoicesdirectly. Accuracy(↑)
MMMU(Shortanswer) Answerthequestionusingasinglewordorphrase. Accuracy(↑)
LLaVA-Bench - ScoreRatiocomparedtoGPT-4(↑)
MME Answerthequestionusingasinglewordorphrase. TotalScore(↑)
Science-QA Answerwiththeoption’sletterfromthegivenchoicesdirectly. IMG-Accuracy(↑)
MathVision Answerthequestionusingasinglewordorphrase. Accuracy(↑)
MathVista - Accuracy(↑)
MathVerse - Accuracy(↑)
POPE Answerthequestionusingasinglewordorphrase. F1score(↑)
AMBER Answerthequestionusingasinglewordorphrase. Accuracy(↑)
Table7: HyperparametersofSliME.Thehyper-parametersofVicuna-13BissimilartoVicuna-7B.
LLM Vicuna-7B Llama3-8B Llama3-70B(LoRA)
StageI&II StageIII StageI&II StageIII StageI&II StageIII
batchsize 256 128 256 128 256 128
lr 1.00E-03 2.00E-05 1.00E-03 1.00E-05 1.00E-03 1.00E-04
DeepspeedZeROStage 2 3 2 3 3 3offload
lrschedule CosineAnnealingwithLinearWarmup
lrwarmupratio 0.03
WeightDecay 0
Epoch 1
Optimizer AdamW
Precision BF16
usingasinglewordorphraseandprovidethecorrectoptionletter”,themodelrespondswithjust
"C,"highlightingthesensitivityofSliME-8B†. Additionally,weobservedthattheperformanceof
SliME-8B†isinferiortothatofSliME-8B,asexemplifiedinFig.7(b). Thus,whileLoRAfine-tuning
maintains similar performance across most benchmarks, the constrained parameter updates and
limitedalterationstotheparametersofLLMsmaymakeitmorechallengingthanfullyfine-tuningto
strictlyadheretoinstructions,particularlywhenimagepromptsareinvolved.
20Image: Text: Describe this painting in detail.
Image: Text: Please directly answer the question
and provide the correct option letter, e.g.,
A, B, C, D.
Q A C hu = oe s i3 ct 6i eo . s0n :,: tA hs e ns h ao nw gn le i Cn t ih se e f qi ug au lr e to, i (f ) angle T Th he e i bm aca kg ge r i os u a n dp o fert ar ta ui rt eo sf aa lw ano dm sca an p p ea win it te hd m in o uth ne ta R ine sn aa nis ds a an rc ie v es rt .y Tle h. eT ph ae i nw tio nm g a in s eis x ed ce up ti ec dte id n w oii lt h o nl o an wg oh oa dir e na n pd a na e r led . dress.
A:36°B:54°C:60°D:27°
The image is a captivating representation of a woman's face, painted in the distinctive style of the Renaissance
解:∵AB∥CD,∴∠BEF=∠1=50°,∵EG平分 period.Thewoman'sfaceisthecentralfocusofthepainting,herfeaturesrenderedwithmeticulousdetail.Herhair,
∠BEF,∴∠BEG=\frac{1}{2}∠BEF=25°,∴∠2=∠BEG+∠G=25°+90°=115°,故选:C．. a rich shade of red, cascades down her shoulders, adding a vibrant contrast to the otherwise muted palette of the
painting.The background is a serene landscape, with a distant mountain range under a pale blue sky. The painting
isexecutedinoiloncanvas,amediumthatallowsforthesubtleblendingofcolorsandthecreationofdepthand
C texture. The overall composition and the artist's use of color and light are characteristic of the Renaissance genre.
(a) MathVerse. (b) LLaVA-Bench.
Figure7: FailurecasesofSliME-8B†.
Query: When does it say
it'll be open?
It says that the smoothie shop will be open soon.
The sign says it will be open at 11:00.
Figure8: High-resolutionimageperception.
Query: What is the advertisement in the white board?
The advertisement in the white board is for Southern Homes.
The advertisement on the white board is for a dental office
.
Query:Whatisthenumberforsouthernhomes?
The number for Southern Homes is 648-Home.
The number for Southern Homes is 648-3958.
Figure9: High-resolutionimageperception.
21Query: Can you translate this into Python code?
Certainly! Here's the Python code that corresponds to the flowchart in the image:
This code defines a function `read_guess()` to read the user's guess, a function
`generate_random_number()`togeneratearandomnumberbetween1and10,a
function`check_guess()`tocheckiftheguessiscorrectornot,andafunction
`play_game()`toplaythegame.The`play_game()`functioncallstheotherfunctionsand
prints the result of the game.
Figure10: Codegeneration.
Query: My son drew this,can you tell a short bedtime story based on his drawing?
Once upon a time, there was a man named Jack who loved the sea. He lived in a small coastal town where he
spentmostofhisdaysfishingandsailing.Jackwasaskilledsailorandknewthewaterslikethebackofhishand.
Oneday,Jackdecidedtogoonasoloadventure.Hesetsailonhissmallboat,the"SeaDragon,"andheaded
out into the open waters. The wind was in his favor, and the "Sea Dragon" glided smoothly across the waves.
As Jack sailed further and further from the shore, he encountered a group of playful dolphins. They swam
alongside the "Sea Dragon," leaping and splashing in the water. Jack laughed with joy, feeling at one with
nature.
Suddenly, afiercestormrolledin.Thewindshowled,andthewavescrashedagainstthe"SeaDragon."Jack
held on tight, using all his skill and experience to navigate the treacherous waters.
Just when it seemed like all hope was lost, the storm subsided, and the sun broke through the clouds. Jack
breathed a sigh of relief and looked around at the calm sea. He spotted a small island in the distance and
decided to explore it.
Asheapproachedtheisland,Jacknoticedthatitwascoveredinlushgreenforestsandsandybeaches.He
anchored the "Sea Dragon" and went ashore, eager to explore the island.
Jack spent the day exploring the island, discovering hidden coves and secret waterfalls. He even found a hidden
treasure chest filled with gold and jewels.
As the sun began to set, Jack returned to the "Sea Dragon" and set sail for home. He was tired but content,
knowingthathehadexperiencedatrulymagicaladventure.
Fromthatdayon,Jacksailedtheseas,alwaysseekingoutnewadventuresanddiscoveringhiddenwonders.And
he never forgot the magical island and the treasure that lay within. The end.
Figure11: Storywriting.
Query: These holes are appearing around the screws in my deck. Is it rotten? Should I get my boards replaced?
Yes, the image shows that the wooden boards have holes
around the screws, which indicates that the wood is
rotten. It is advisable to replace the rotten boards to
ensure the safety and structural integrity of your deck.
Rotten wood can weaken the deck's support and lead to
accidents or further damage. It is essential to address
this issue promptly to prevent any potential hazards.
Figure12: Suggestion.
22