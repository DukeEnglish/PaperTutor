SonicID: User Identification on Smart Glasses with Acoustic Sensing
KELI,
CornellUniversity,USA
DEVANSHAGARWAL,
CornellUniversity,USA
RUIDONGZHANG,
CornellUniversity,USA
VIPINGUNDA,
CornellUniversity,USA
TIANJUNMO,
CornellUniversity,USA
SAIFMAHMUD,
CornellUniversity,USA
BOAOCHEN,
CornellUniversity,USA
FRANÇOISGUIMBRETIÈRE,
CornellUniversity,USA
CHENGZHANG,
CornellUniversity,USA
Smartglasseshavebecomemoreprevalentastheyprovideanincreasingnumberofapplicationsforusers.Theystore
varioustypesofprivateinformationorcanaccessitviaconnectionsestablishedwithotherdevices.Therefore,thereisa
growingneedforuseridentificationonsmartglasses.Inthispaper,weintroducealow-powerandminimally-obtrusive
systemcalledSonicID,designedtoauthenticateusersonglasses.SonicIDextractsuniquebiometricinformationfromusers
byscanningtheirfaceswithultrasonicwavesandutilizesthisinformationtodistinguishbetweendifferentusers,powered
byacustomizedbinaryclassifierwiththeResNet-18architecture.SonicIDcanauthenticateuserswithin0.12seconds,with
anenergyconsumptionof19.8mAspertrial.Auserstudyinvolving24participantsconfirmsthatSonicIDachievesatrue
positiverateof96.5%,afalsepositiverateof4.1%,andabalancedaccuracyof96.2%usingjust4minutesoftrainingdata
collectedforeachnewuser.Thisperformanceisrelativelyconsistentacrossdifferentremountingsessionsanddays.Given
thispromisingperformance,wefurtherdiscussthepotentialapplicationsofSonicIDandmethodstoimproveitsperformance
inthefuture.
AdditionalKeyWordsandPhrases:UserIdentification,SmartGlasses,AcousticSensing,MachineLearning
1 INTRODUCTION
Smartglasses,suchasVuzixSmartGlasses[52],LenovoThinkRealityA3[27],MetaRay-Ban[35],SnapSpectacles
3[45],etc.,havebeengrowinginpopularityandprovidinguserswithmoreandmoreinteractiveapplicationsin
recentyears.Similartosmartphonesandotherwearabledevices(e.g.smartwatchesandVRheadsets),smart
glassesstarttostoreusers’personaldataandprivateinformationincludingtheirprivatemessages,photos,videos
andbanksaccounts.Leakageofprivateinformationandinconveniencemightbeinducedtocertainusersifbad
actorsobtainthisinformationfromtheirsmartglasses.Moreover,somesmartglassesareconnectedtoother
devicessuchassmartphonesandsmartTVs.Itispossibleforsmartglasseswithoutuserauthenticationtobe
utilizedbybadactorstogiveunwantedcommandstoordisplayinappropriatecontentsontheseconnected
devices.Therefore,thereisanincreasingneedforimplementinguserauthenticationmethodsonsmartglasses.
However,itisdifficulttodirectlyapplytraditionalfrontal-camera-basedauthenticationtechniques,e.g.FaceID,
onglassesbecausetheyrequireplacingacamerainfrontoftheusertocapturetheirface.Researchershave
put efforts into deploying cameras on glasses to capture biometric information of the user from their facial
contours[31],eyemovements[63],oririsfeatures[30].Thoughthesecamera-basedmethodsachievesatisfactory
performanceofuserauthenticationonglasses,theyrequireinstrumentingacamerawhichisrelativelylarge
Authors’addresses:KeLi,CornellUniversity,Ithaca,USA,kl975@cornell.edu;DevanshAgarwal,CornellUniversity,Ithaca,USA,da398@
cornell.edu;RuidongZhang,CornellUniversity,Ithaca,USA,rz379@cornell.edu;VipinGunda,CornellUniversity,Ithaca,USA,vg245@cornell.
edu;TianjunMo,CornellUniversity,Ithaca,USA,tm659@cornell.edu;SaifMahmud,CornellUniversity,Ithaca,USA,sm2446@cornell.edu;
BoaoChen,CornellUniversity,Ithaca,USA,bc526@cornell.edu;FrançoisGuimbretière,CornellUniversity,Ithaca,USA,fvg3@cornell.edu;
ChengZhang,CornellUniversity,Ithaca,USA,chengzhang@cornell.edu.
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article.Publicationdate:2024.
4202
nuJ
21
]CH.sc[
1v37280.6042:viXra2 • Lietal.
comparedtothesmallsizeofglassesandmightpartiallyblocktheuser’sview.Furthermore,thecamerason
commoditysmartglasses,thoughcanbesmallerinsize,usuallypointforwardstocapturetheenvironmental
information.Barriersexistiftheyaretobedirectlyusedtocapturethebiometricinformationoftheuser’sface.
Password-basedauthenticationmethodshavebeenexploredonsmartglassesaswell,especiallywithvoiceinput
[7,29,59].Inaddition,behavioralbiometrics,suchastappinggestures[20],touchgestures[22,39],andvoice
commands[39],havebeenleveragedtoauthenticateusers.Thesemethodsrequiretheuser’sinteractionandare
notablyvulnerabletoshoulder-surfingattackssincethesegesturesareenteredonatouchpadandthevocalized
commandscanbeoverheardbybystanders.
Inthispaper,wepresentSonicID,alow-powerandminimally-obtrusivesolutiontouserauthenticationon
smartglasses,whichidentifiesusersautomaticallyassoonastheyputontheglasses,usingactiveacousticsensing.
Weplaceonepairofspeakerandmicrophoneoneachsideofapairofglasses.Thesensorsareinstrumentedon
thehingesoftheglasses,pointingdownwardsattheuser’sface.ThecoresensingprincipleofSonicIDistouse
asonar-liketechniquetoscantheshapeoftheuser’sfaceasthebiometricinformationforauthentication.Similar
tohowFaceIDscanstheuser’sfacewithinfraredcameras,whenauserwearstheglassesinstrumentedwith
SonicID,itscanstheuser’sfaceautomaticallywithacousticsignals,andthencomparesscannedpatternswith
theprofilesofregistereduserstodeterminewhetherornotthisusercanhaveaccesstothedevice.
Specifically,thespeakersontheglassesemitencodedinaudibleFrequencyModulatedContinuousWaves
(FMCW)towardstheuser’sfaceaftertheyputontheglasses.Thesignalscontinuouslyandrepeatedlysweepat
differentfrequencyrangesontwosidestoavoidinterference.Thetransmittedsignalsarereflectedbytheuser’s
faceandcapturedbythemicrophonesontheglasses.Aftersignalprocessingusingthereceivedsignalsand
transmittedsignals,SonicIDobtainsuniqueacousticpatternsrelatedtothisuser’sface.Consideringeveryone
hasdifferentshapesoffaces,theuser’sbiometricinformationisincludedinthecapturedacousticpatternswhich
canbelearnedtodistinguishthisuserfromotherusersandthusauthenticatethemsecurely.InSonicID,wefeed
theprocessedacousticpatternsintoacustomizedbinaryclassifierusingtheResNet-18architecturetoextract
distinctfeaturessoastoauthenticateusers.Theclassifieristrainedwiththetrainingdatacollectedwhenauser
registersonthesmartglassesanditkeepsauthenticatingusersinlaterusageofthedevicejustliketraditional
userauthenticationtechniques.
TovalidatetheperformanceofourSonicIDsystem,weconductedauserstudywith24participants.Thestudy
resultsdemonstratethatanewuserneedstoprovide4minutesofdatatotrainingthebinaryclassifieronthe
firstdaywhentheyusethesmartglasses,toachieveatruepositiverateof96.5%,afalsepositiverateof4.1%and
abalancedaccuracyof96.2%.Inthisstudy,wealsoaskedallparticipantstocomebacktotesttheperformanceof
thesystemontwootherdifferentdays.Theresultsshowthattheperformanceofthesystemremainsrelatively
consistentacrossthreedifferentdays,especiallyforthefalsepositiverate.Ifwecontinuetoreducetheamount
oftrainingdatato1minuteforeachparticipant,atruepositiverateof90.3%,afalsepositiverateof1.6%and
abalancedaccuracyof94.3%areachievedonDay1.ThisstudyvalidatesthatSonicIDachievesasatisfactory
andstableauthenticationperformanceonsmartglasses.Comparedwithfrontal-camera-basedauthentication
techniquessuchasFaceID,SonicIDdoesnotneedacameratobeplacedinfrontoftheuser.Othertraditional
authenticationmethodswhichtakefingerprintsorpasswordsasinputcaneliminatetheneedforafrontalcamera.
However,theyrequireusers’operationsforauthenticationwhileSonicIDautomaticallylogsvalidusersinwhen
theyputthedeviceon.Moreover,SonicIDiscapableofsuccessfullyauthenticatinguserswithin0.12seconds
withaenergyconsumptionof19.8mAseverytimeitauthenticatestheuser.Thislow-powerfeatureguarantees
thatSonicIDcanworkapproximately28,000timestheoreticallywiththebatteryofMetaRay-Ban(154mAh)if
nootheroperationsarerunningonthedevice.
Themaincontributionsofthispaperaresummarizedasfollows:
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article.Publicationdate:2024.SonicID:UserIdentificationonSmartGlasseswithAcousticSensing • 3
• We proposed a low-power and minimally-obtrusive solution to the user authentication task on smart
glasses,usingactiveacousticsensing.Itdemonstratedthattheshapeoftheuser’sfacecanbescannedwith
acousticsignalsasbiometricinformationforauthentication.
• Weprototypedthesystemwithonepairofspeakerandmicrophoneoneachsideoftheglasses.Thesystem
maintainslow-power,consumingonly19.8mAsofenergyeverytimeitauthenticatestheuser.
• Weconductedauserstudywith24participants,validatingthatonly4minutesoftrainingdataforeach
newuserguaranteesthatthesystemachievesatruepositiverateof96.5%,afalsepositiverateof4.1%and
abalancedaccuracyof96.2%.Theperformanceremainsrelativelyconsistentinevaluationsacrossdifferent
days.
2 RELATEDWORK
Inthissection,wepresenttherelatedworkofSonicIDinthefollowingthreescopes:(1)Authenticationmethods
onnon-wearables;(2)Authenticationmethodsonwearabledevices;(3)Comparisonbetweenourworkandother
authenticationmethodsonwearables.
2.1 AuthenticationonNon-WearableDevices
Authenticationmechanismsarecriticalindeviceslikesmartphones,tablets,andcomputersthatstoresensi-
tivepersonalinformation,provideaccesstobankingservices,andcontrolsecuritysystems[24].Traditional
authenticationmethods,suchaspasswordsandPersonalIdentificationNumbers(PINs),areprevalentbutnot
withoutshortcomings.Usersmayforgetthesecredentials,andtheyaresusceptibletosecuritybreaches,including
shoulder-surfingattacks[49].Analternativeapproach,pattern-basedpasswords,leveragesthepictorialsuperior-
ityeffectbyallowinguserstoremembershapes,offeringamoreintuitiverecallprocess[23,36,46].However,
thesepatternsarevulnerabletobothshoulder-surfingandsmudgeattacks[6].
Biometric authentication has gained popularity due to its perceived security and ease of use. Fingerprint
sensingiswidelyusedincurrentdevices[48].Facerecognitiontechnology,althoughincreasinglycommon,canbe
compromisedusingphotographsorvideosoftheuser[16].Tocounterthis,systemslikeApple’sFaceIDemploy
3Dreconstructionoffacialfeatures[47].Irisscansofferanotherbiometricalternative,notedfortheiruniqueness
and difficulty to replicate [13]. Additionally, multimodal biometric systems have been explored, combining
different physical attributes for enhanced security. For instance, Rokita et al. [40] integrates face and hand
images,whileMarceletal.[34]combinesfaceandspeechrecognition.EchoPrint[65]mergesfacialrecognition
withacousticanalysistocounterspoofingattempts.VoiceLive[61]andVoiceGesture[60]utilizeuniquevocal
characteristicsandarticulatorygestures,respectively,tothwartreplayattacks.AirSign[43]innovativelyemploys
acousticsensingandmotionsensorsforairsignature-basedauthentication.
Behavioralbiometricsalsoprovidepromisingavenuesforauthentication.Thesemethodsanalyzeuser-specific
behaviors, such as pattern input dynamics [14], activity patterns [33, 50], gait [66], and even unique hand
movementsandorientations[44].BreathPrint[11]furtherextendsthisdomainbyusingaudiofeaturesfrom
individualbreathingpatterns.Suchtechniquesoffercontinuousandpassiveauthentication,enhancingboth
securityanduserconvenience.
However,thesematureauthenticationtechnologiesaboveusuallycannotbeeasilytransplantedtowearable
devicesconsideringtheirsmallsizeandlimitedbatterycapacity.
2.2 AuthenticationonWearableDevices
Inrecentyears,theproliferationofwearabledevices,suchassmartwatches,smartrings,earphones,andsmart
glasses,hasbeennotable.Thesedevices,ashighlightedby[38],areincreasinglyintegratingintoourdailylives,
storingsensitivedatarangingfromSMSmessages[15]tohealthinformation[2].Moreover,theirapplication
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article.Publicationdate:2024.4 • Lietal.
Table1. SonicIDandOtherAuthenticationMethodsonWearables.SomepapersreportedEqualErrorRate(EER)which
wasconvertedtoBalancedAccuracy(𝐵𝐴𝐶 =1−𝐸𝐸𝑅)inthistable.Thebalancedaccuracyinthistableisthecross-session
performanceifthesystemwasevaluatedacrossdifferentremountingsessions.
Form Biometrics User Cross- Cross- Study Balanced
Project Sensors
Factor Extracted Interaction? Session? Day? Participants Accuracy
SonicID Glasses Acoustics Face No Yes Yes 24 96.2%
Isobeetal.[21] Glasses Acoustics Nose No Yes No 11 91.0%
C-Auth[31] Glasses Camera Face No Yes Yes 20 96.5%
Zhangetal.[63] Glasses Camera Eyes Yes Yes Yes 30 93.1%
Kawasakietal.[25] Glasses Optical Blink Yes No No 7 93.6%
EarEcho[17] Earables Acoustics EarCanal No Yes Yes 20 94.5%
ToothSonic[54] Earables Acoustics Toothprint Yes No No 25 92.9%-98.9%
WristAcoustic[19] Wristband Acoustics Wrist Yes Yes Yes 25 95.0%
incriticalfunctions,suchascontactlesspayments[51]andauthorizingaccesstolaptops[42],underscoresthe
necessityofrobustaccesscontrolsystems.Consequently,therehasbeenasignificantthrustinresearchtowards
developingeffectiveauthenticationmethodsforthesedevices.
2.2.1 SmartGlasses. Toenhancethesecurityofsmartglasses,avarietyofnovelauthenticationmethodshave
beeninvestigated.TraditionalPIN-basedmechanismsarenotablyvulnerabletoshoulder-surfingattacks[49].
This issue is exacerbated in smart glasses, where the PIN is entered on a touchpad, making it more visible
tobystanders[10,20].Alternativeapproachessuchasvoice-basedPINentryhavebeenexplored[7,29,59].
ThesemethodsemployaciphertomapthePINtorandomdigits,obscuringthepasswordfromeavesdroppers.
However, this technique necessitates mental computations from users, potentially diminishing usability [7].
Camera-basedauthenticationmethodshavealsobeenafocusofresearch.TheGlassOTP[10]systemutilizes
thecamerainGoogleGlassestoscanaQRcodeontheuser’ssmartphoneforauthentication,althoughthis
methodrequirestheusertocarryasmartphone.C-Auth[31],whichusesadownward-facingcamerainthe
glassestocapturefacialcontoursforauthentication.Zhangetal.[63]developedacontinuousauthentication
systembasedoneyemovementresponsetovisualstimuli,detectedbyacamera.Anotherapproachinvolves
irisrecognition,whereinternalinfraredcamerasareusedforauthentication[30].Behavioralbiometricshave
alsobeenexploredforsmartglassauthentication.Jagmohanetal.[22]demonstratedagesture-basedcontinuous
authenticationsystem.TheGlassGuardsystem[39],utilizesbehavioralbiometricsderivedfromtouchgestures
andvoicecommandsforcontinuousauthentication.Kawasakietal.[25]introducedanauthenticationmethodby
observingtheskindeformationduringblinking,employingaphotoreflectortomeasureblinks.Isobeetal.[21]
introducedaninnovativeapproachusingactiveacousticsensing.Thismethodinvolvestransmittingacoustic
signalsthroughthenoseusingspeakersintegratedintothenosepadsoftheglasses,withthereceivedsignals
capturedbymicrophones.Theacousticstructureofthenoseservesasabiometricidentifier,thoughthistechnique
canbeaffectedbythedrynessofthenose.Furthermore,thisprototypewasnottestedacrossdifferentdaysso
thestabilityofthesystemisunclear.
2.2.2 OtherWearableDevices. Inthedomainofsmartwatches,researchershaveextensivelyexploredbiometric-
basedauthenticationmethods.Forinstance,Corneliusetal.[12]investigatedtheuseofon-wristbioimpedance
responses for user authentication. Zhao et al. [64] employed photoplethysmography signals for continuous
authentication.AnotherinnovativeapproachbyWatanabeetal.[56]utilizedactiveacousticsensing,requiring
userstoperformfourdistincthandposesforauthentication.Further,Leeetal.[26]proposedamethodleveraging
vibration responses, measured through accelerometer and gyroscope sensors, to authenticate users. Recent
advancementshaveintroducedmoresophisticatedtechniques.WristAcoustic[19],constructsaclassifierbasedon
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article.Publicationdate:2024.SonicID:UserIdentificationonSmartGlasseswithAcousticSensing • 5
threehandposes,utilizingacuesignalemittedfromasurfacetransducerandrecordedbyacontactmicrophone.
Additionally,WristConduct,[41],innovativelyusessoundwavestransmittedthroughthewristbones,employing
aboneconductionspeakerandalaryngophoneforauthenticationpurposes.
In addition to on-device authentication methods, researchers have explored approaches that incorporate
secondarydevicestofacilitateuserauthentication.Nymiband[1]isawrist-wornwearabledevicethatintegrates
fingerprintrecognition,servingasanauthenticationmechanismforvariousdevicesandapplications.Furthermore,
ear-based authentication systems such as EarAE [57] and EarEcho [17] utilize acoustic sensing techniques
to capture the unique structure of the ear canal, thereby offering a biometric signature for authentication
purposes.ToothSonic[54],employsanearable-basedsystemthatutilizestoothprint-inducedsonicwavesforuser
verification.Similarly,Amesakaetal.[3]havedemonstratedthepotentialofusingsoundleakagesignalsfrom
hearablestocapturetheacousticcharacteristicsoftheearcanal,auricle,orhand,whichcanthenbeemployed
forauthentication.EarDynamic[55]leveragesacousticsensinginearablestoassessboththestaticanddynamic
motionsoftheearcanalduringspeech,providinganovelmethodforauthentication.
2.3 ComparisonbetweenSonicIDandOtherAuthenticationMethodsonGlasses
To better compare our work with prior work of user authentication on glasses, we list all the projects that
are most related to SonicID in Tab. 1. As shown in the table, SonicID does not need users’ interaction to
performauthenticationandachievescomparablebalancedaccuracy,ifnotbetter,topriorworkonglasses.This
performanceisevaluatedwithavalidnumberofparticipantsandremainsrelativelyconsistentacrossdifferent
remountingsessionsanddays.Inthemeantime,SonicIDmaintainslow-powerandminimally-obtrusiveduetothe
advantagesofacousticsensors.Insummary,SonicIDcontributestothefieldbypresentingthefirstacoustic-based
methodonsmartglassesthatscansusers’facestoobtainbiometricinformationforuseridentification.
3 FACIALBIOMETIRCINFORMATIONEXTRACTIONWITHACTIVEACOUSTICSENSING
Inthissection,wefirstintroducebackgroundonusingface-basedbiometricinformationforauthentication,
followedbythecoresensingtechniqueandalgorithmofSonicIDtoobtainuniqueacousticfeaturesofeachuser.
ThenwepresentthedeeplearningmodelusedinSonicIDtoauthenticateusersandthemetricsusedtoevaluate
theperformanceofoursystem.
3.1 Face-basedBiometricAuthentication
Biometricsareunqiuephysicalcharacteristicsthatcanbeusedforpersonalrecognition[37].Face,asoneofthe
mostimportantpartsofhumanbody,hasbeenacrucialsourcefromwhichbiometricinformationisextracted
becausethegeometriesofthefacialsurfacesofdifferentpeoplearedifferent.Extensiveresearchhasbeencarried
outtoutilizethisbiometricinformationfromtheuser’sfaceforauthentication,especiallycapturedbyfrontal
cameras[8,9].Oneofthemostcommonlyusedauthenticationmethodsbasedonfacialbiometricscapturedby
camerasisApple’sFaceID[5].Itdeliversimpressiveperformanceinauthenticationandhasadvantagesofbeing
moreconvenientandsecurecomparedwithpassword-basedmethods.ThecoresensingpricinpleofSonicIDis
similartoFaceID.Itinstrumentsacousticsensorsonsmartglassestoscantheuser’sfacein3Dwithultrasonic
soundwavesinsteadofinfraredcamerasandusestheobtainedbiometricinformationforauthentication.The
subsequentsubsectionsintroducetheprincipleofoperationofSonicIDindetail.
3.2 SignalTransmissionandReception
SonicIDadoptsactiveacousticsensingtoscanauser’sfaceassoonastheywearthedevice.Activeacoustic
sensingincludesanemitter(e.g.speakers)totransmitencodedsignalsandareceiver(e.g.microphones)toreceive
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article.Publicationdate:2024.6 • Lietal.
(a)TransmittedSignal(18-21kHz) (b)ReceivedSignal(18-21kHz) (c)Cross-correlationResult (d)EchoProfile
Fig.1. FMCWSignalTransmission,ReceptionandProcessing.
thereflectedsignals.Bycomparingthereceivedsignalsandthetransmittedsignals,onecanobtainandanalyze
thestatusandchangeoftheobjectsintheenvironmentthatthesignalsaretargetedat.
InSonicID,wechoosesignalsthatsweepwithinacertainfrequencyrangeastransmittedsignals.Considering
SonicIDisdeployedonsmartglasseswhichisquiteclosetouserswhentheyputiton,wedecidetosweep
thesignalsintheultrasonicfrequencybands,i.e.over18kHz,toalleviatetheimpactoftheaudiblesoundon
users’dailyactivities.Inordertoobtainricherinformationfromusers’face,weputonepairofspeakerand
microphoneoneachsideoftheglasses.Thespeakersontwosidesaredesignedtotransmitencodedsignals
indifferentultrasonicfrequencyrangestoavoidinterference.Wepick18-21kHzforthespeakerontheright
sideand21.5-24.5kHzforthespeakerontheleftsothattheyareinaudibletomostpeopleandcompatiblewith
theaudiointerfacesonmostcommoditydevices.Asamplingrateof50kHzisselectedtosupportthesetwo
frequencyrangesandthesignalssweepfromthelowerfrequencyboundary(18/21.5kHz)tothehigherfrequency
boundary(21/24.5kHz)every12ms.Therefore,eachsweepcontainsN=600samples(50𝑘𝐻𝑧×12𝑚𝑠).Fig.1(a)
demonstratesoneofthetransmittedsignalsthatsweepsfrom18-21kHz.Thesesignalsarecommonlyusedand
namedasFrequencyModulatedContinuousWaves(FMCW)inthefield.
Inpractice,thetwospeakersemittheencodedsignalsthatsweepintwofrequencyrangesrepeatedlytowards
theuser’sfacewhentheuserputonthedevice.Thesignalsarereflectedbytheuser’sfaceandcapturedbythe
twomicrophonesthatareplacednexttothespeakers.Byanalyzingthedifferencesbetweenthereceivedsignals
andtransmittedsignals,wecanobtainuniqueacousticfeaturesrelatedtothisspecificuser,whichwillthusbe
usedforuseridentification.AdetaileddescriptionofhowSonicIDgeneratestheacousticfeaturesispresented
below.
3.3 EchoProfileCalculation
Thetwomicrophonescapturetwochannelsofaudioontwosidesoftheglasses.Anexampleofthereceived
signalinonechannel(18-21kHz)isshowninFig.1(b).Afterreceivingthesignals,wefirstapplytwoButterworth
band-passfilters,whicharefrom18-21kHzand21.5-24.5kHz,onthesignalsseparatelytofilteroutthenoises
thatareoutsidethefrequencyrangeofourinterest.Byapplyingtwoband-passfiltersontwochannelsofsignals,
wegetfourchannelsoffilteredsignals,twoofwhichrepresentthesignalsthattravelonthesamesideoftheface
whiletheothertworepresentthesignalsthattravelacrossthefacefromonesidetotheothersideoftheface.
Theacousticpatternsassociatedtotheuser’sfacethatwewouldliketoobtainarecalledEchoProfiles,which
havebeendemonstratedinseveralpreviouspapers[4,28,32,53].Accordingtopriorwork,wecontinuously
calculatethecross-correlationbetweenthereceivedsignalsandthetransmittedsignalsusingEq.1.
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article.Publicationdate:2024.SonicID:UserIdentificationonSmartGlasseswithAcousticSensing • 7
𝑁−1
∑︁
𝐶(𝑛) =𝑇𝑥(𝑛)∗𝑅𝑥(𝑛) = 𝑇𝑥(𝑚)·𝑅𝑥(𝑚+𝑛),𝑛 ≥ 0 (1)
𝑚=0
where𝑇𝑥(𝑛) isthetransmittedsignaland𝑅𝑥(𝑛) isthefilteredreceivedsignal.Sincetherearefourchannels
offilteredreceivedsignalsaftertheband-passfilters,wecalculatethecross-correlationbetweeneachoneof
thesechannelsandthecorrespondingtransmittedsignal,leadingtofourchannelsofacousticpatternsrelatedto
differentspeaker-microphonelinks.Fig.1(c)demonstrates𝐶(𝑛)obtainedafterthecross-correlationcalculationin
onechannel.Thestrengthof𝐶(𝑛)correlatestothestrengthofthesignalsthatthesystemreceivesatdifferenttime.
The0-pointisthedirectpathwhichmeansthatthesignaltravelsfromthespeakerdirectlytothemicrophonevia
solidsincetheyareonthesameglasses.Thepositivesamplesrepresentthesignalsthatarriveatthesystemlater
thanthedirectpath.Theyusuallytravelintheairandarereflectedbytheobjectsinthesurroundingsofthe
system.Thenegativesamplesrepresentthosearrivingearlierthanthedirectpathandtheycomefromprevious
frame(s) of the transmitted signals, which are being emitted continuously and repeatedly. Two consecutive
samplesare0.02msapart(1÷50𝑘𝐻𝑧)andweonlyshowthecenter120samplesinFig.1(c).
Wethenreshapetheone-dimensional𝐶(𝑛) bytheframesizeN=600samplestoformatwo-dimensional
array.ThisarrayofpatternsisdefinedasEchoProfile,whichincludesthebiometricinformationofthefaceof
theuserwhoiswearingthedevice.Fig.1(d)showsanexampleoftheechoprofilethatisproperlyreshaped
fromFig.1(c).Inechoprofiles,they-axisisthedistance,whichshowsthestrengthofthesignalsthatisreflected
fromdifferentdistancesawayfromtheSonicIDsystemonglasses.Becausedifferentusershavedifferentfacial
shapeandcontours,theechoprofileshowsthefacialscanningresultofeachuserandisdistinctiveforeachuser.
Consideringtheaveragelengthofpeople’sfaces,wecrop70samplesofechoprofilesfrom-15samplesto55
samplesastheacousticpatterntoauthenticateusersinsteadofusingthefullrangeofit,whichscansthearea
atadistanceof0cmto18.7cmfromthesystem(55𝑠𝑎𝑚𝑝𝑙𝑒𝑠 ÷50𝑘𝐻𝑧×340𝑚/𝑠 ÷2).Wedividethedistance
by2becausethesignalstravelround-wayfromthesystemtothefaceandbacktothesystem.18.7cmisthe
maximumone-waydistancefromthesystemthatwewouldliketoscan.Asstatedabove,thenegativesamples
arefrompreviousframesandtheoreticallydonotcontainmuchusefulinformation.However,westillinclude15
samplesfromthenegativesidebecauseourtransmittedsignalsarenotinfiniteinthefrequencydomainand
thustheacousticpatternsdiffusetothenegativesidetosomeextentwhenthecross-correlationiscalculated.
Thecroppedechoprofilesarefedintoadeeplearningmodeltoauthenticateusers,whichisintroducedinthe
subsectionbelow.
3.4 MachineLearningModelforAuthentication
Though the acoustic data is originally one-dimensional, we reshape the echo profiles to make them two-
dimensional images. In order to extract features that are unique to each user from these two-dimensional
echoprofiles,wedecidetoadoptadeeplearningmodelbasedontheResNet-18architecturebecauseConvolu-
tionalNeuralNetworks(CNN)areknownforbeinggoodatdistinguishingdifferentclasses.Hence,weconstruct
themodelusingtheResNet-18architectureastheencodertoextractfeaturesfromechoprofilesandusinga
fully-connectedlayerasthedecoder.Themodelservesasabinaryclassifierwhichclassifiestherealuseras
positiveandtheattackersasnegative.Whiletrainingthemodelforanewuser,thepositiveinstancescomefrom
thedatathatthisuserprovideswhenhe/sheregistersonthedeviceandthenegativeinstancescomefroman
existingdatasetwhichwaspre-collectedfromdifferentpeople.
Theinputfedintothemodelisthereshapedandcroppedechoprofiles.Wetake10600-sampleframesasone
instance,whichcorrespondsto0.12sintime(10×600𝑠𝑎𝑚𝑝𝑙𝑒𝑠÷50𝑘𝐻𝑧).Therefore,thesizeofeachinstanceas
inputis4×10×70,where4representsthe4channelsofaudiodataand70representsthenumberofthecropped
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article.Publicationdate:2024.8 • Lietal.
(a)HardwareComponents (b)ExploreSensorPositions (c)Prototype (d) User Wearing Proto-
type
Fig.2. HardwareComponentsandPrototypeofSonicID.
verticalsamples.Duringtesting,thepositiveinstancesarestillprovidedbytherealuserwhilethenegative
instancescomefromtheunknownattackersthathavenotbeenseenbythemodel.
IntheCNNmodel,weuseanAdamoptimizer,acosineschedulerandsettheinitiallearningratetobe0.0002.
Themodelistrainedtominimizethecross-entropylossfor50epochs.Randomverticalshiftisincludedand
randomnoiseisaddedasthedataaugmentationmethodstoboostthegeneralizabilityofthemodel.
3.5 EvaluationMetrics
ToevaluatetheperformanceofourmodelinSonicID,weadoptthreecommonlyusedmetricsinpriorauthenti-
cationresearch,whichareTruePositiveRate(TPR),FalsePositiveRate(FPR)andBalancedAccuracy(BAC).The
euqationstocalculatethesethreevaluesareasfollows.
𝑇𝑟𝑢𝑒 𝑃𝑜𝑠𝑖𝑡𝑖𝑣𝑒𝑠
𝑇𝑟𝑢𝑒 𝑃𝑜𝑠𝑖𝑡𝑖𝑣𝑒𝑅𝑎𝑡𝑒 (𝑇𝑃𝑅) = (2)
𝑇𝑟𝑢𝑒 𝑃𝑜𝑠𝑖𝑡𝑖𝑣𝑒𝑠+𝐹𝑎𝑙𝑠𝑒 𝑁𝑒𝑔𝑎𝑡𝑖𝑣𝑒𝑠
𝐹𝑎𝑙𝑠𝑒 𝑃𝑜𝑠𝑖𝑡𝑖𝑣𝑒𝑠
𝐹𝑎𝑙𝑠𝑒 𝑃𝑜𝑠𝑖𝑡𝑖𝑣𝑒𝑅𝑎𝑡𝑒 (𝐹𝑃𝑅) = (3)
𝐹𝑎𝑙𝑠𝑒 𝑃𝑜𝑠𝑖𝑡𝑖𝑣𝑒𝑠+𝑇𝑟𝑢𝑒 𝑁𝑒𝑔𝑎𝑡𝑖𝑣𝑒𝑠
𝑇𝑃𝑅+(1−𝐹𝑃𝑅)
𝐵𝑎𝑙𝑎𝑛𝑐𝑒𝑑𝐴𝑐𝑐𝑢𝑟𝑎𝑐𝑦 (𝐵𝐴𝐶) = (4)
2
TPRevaluatestheabilityofthesystemtoauthenticatetherealuserswhileFPRevaluatestheabilityofthe
systemtoprotectitselffrombeingattacked.BACgivesabalancedevaluationbetweenthetwometricsabove.
4 PROTOTYPEDESIGNANDIMPLEMENTATION
Thissectionpresentsthedesignandimplementationofthehardwareprototype.First,weintroducethemicro-
controllerandsensorsusedinthesystem.Next,wepresenthowweprototypethesystemontheglassesform
factor.
4.1 MCUandSensors
Fig.2(a)demonstratesthehardwarecomponentsusedintheSonicIDsystem.Inordertoimplementthesignal
transmissionandreceptionmethodsintroducedinSec.3.2,weusethespeakerOWR-05049T-38D1toemitencoded
signalsandthemicrophoneICS-434342toreceivereflectedsignals.WecustomizePrintedCircuitBoards(PCB)for
1https://www.bitfoic.com/detail/owr05049t38d-14578
2https://invensense.tdk.com/products/ics-43434/
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article.Publicationdate:2024.SonicID:UserIdentificationonSmartGlasseswithAcousticSensing • 9
thespeakersandmicrophonestomakethemminimally-obtrusiveandcompatiblewithdifferentmicro-controllers.
Teeny4.13 isutilizedasthemicro-controllerintheSonicIDsystemtomanagetheoperationofspeakersand
microphonesbecauseofitsgoodperformanceinprocessingaudiodata.TwooftheamplifiersMAX98357A4are
usedinthesystemtosupportasmanyas2speakersand8microphones.SpecificPCBsarealsodesignedfor
theseamplifierstomakethemeasilyfittheTeensy4.1board.SpeakersandmicrophonesareconnectedtoTeensy
4.1viaFlexiblePrintedCircuit(FPC)cablesandtheycommunicatewitheachotherwiththeInter-ICSound(I2S)
buses.Thetransmittedsignalispre-programmedintothememoryofTeensy4.1andthereceivedaudiodatais
storedintotheon-boardSDcardonTeensy4.1.
4.2 FormFactor
AfterweselecttheappropriatecomponentsfortheSonicIDsystem,thenextstepistoprototypeSonicIDonthe
glassesformfactor.InordertovalidatethefeasibilityofSonicID,wedecidetodeploythesystemonapairof
commodityglasses.Sincewewanttoscantheuser’sfacetoobtaintheirbiometricinformation,wewouldliketo
pointthespeakersandmicrophonesdownwards,emittingsignalsdirectlytowardstheuser’sface.Thereare
onlylimitedspaceonglasseswherewecanplacethesensors,facingdownwards,asshowninFig.2(b):(1)the
bridgeoftheglassframe;(2)thebottomoftheglassframe;(3)thehingesoftheglasses.Ifthesensorsareplaced
atposition(1)or(2),thereisachancethatthesensorsmaytouchtheuser’sfaceorblocktheviewoftheuser
becausethesetwopositionsarerightinfrontoftheuser’sface.Therefore,wedeterminetoputthesensorsatthe
hingesoftheglasses,wherethereissomespacetoinstrumentsensorswithoutlettingthemtouchtheuser’s
faceorblocktheuser’sview.Moreover,thissensorpositionhasbeenleveragedinmultiplepriorworktoplace
acousticsensorstotrackfacialexpressions[4]andbodyposes[32].WecanpotentiallyintegrateSonicIDinto
theirsystemstorealizeactivitytrackinganduserauthenticationwithonesetofhardware.
AsdisplayedinFig.2(c),weputonepairofspeakerandmicrophoneoneachsideoftheglassestoobtain
richerinformationandthespeakerandmicrophoneareclosetoeachothertobettercapturethereflectedsignals.
Themicro-controllerTeensy4.1isalsofixedtotheleftlegoftheglasseswithhotglueandtapestomakethe
entiresystemwearable.TheFPCcablesconnectingTeensy4.1andthespeakersandmicrophonesareproperly
restrainedandalignedtotheglassframe.Thefinalprototypelookssimilartoapairofcommercialsmartglasses
andiscomfortabletowear,asshowninFig.2(d).
5 USERSTUDY
ToevaluatetheperformanceofSonicID,wedesignedandconductedauserstudyacrossseveraldays.Thegoalof
thestudyistovalidateSonicID’scapabilityofauthenticatingusersandprotectingthesystemfromattacks.
5.1 StudyDesign
We used the prototype described in Sec. 4 to conduct the user study. In addition to the prototype, a laptop
(MacBookPro,13-inch,AppleM1chip)wasusedinthestudytoshowtheinstructionvideototheparticipants.
Thestudywasconductedinanexperimentroomoncampus.Whenthestudystarted,theparticipantscamein
andwasintroducedthedetailedstudyproceduresbeforesigningtheconsentform.Thentheysatinfrontof
thelaptopandworetheprototype.Someparticipantshadlonghairwhichsometimesgotstuckbetweenthe
hardwarecomponentsandtheglasseswhentheyworethedevicesoweprovidedhairtiesforparticipantstoput
theirhairupiftheirhairwaslong.
Aftertheprototypewasproperlyworn,thedatacollectionprocessstarted.Aninstructionvideowasplayed
onthelaptop.Theparticipantswerefirstinstructedtostaystillfor10secondswhenthesystemscannedtheir
3https://www.pjrc.com/store/teensy41.html
4https://www.analog.com/en/products/max98357a.html
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article.Publicationdate:2024.10 • Lietal.
facecontinuouslyandthenaskedtoremountthedevicewithin10seconds.Remountingmeanttakingoffthe
prototypeandputtingitbackon.Thisprocedurewasdesignedtosimulatereal-lifewearingexperiencesince
usersofwearabledevicesfrequentlyremountthedeviceeveryday.Thisaddedvariancetothecollecteddata.
Aftertheprototypewasremounted,theparticipantsstayedstillforanother10secondsandrepeatedtheprocess
above.Thedatacollectionlastedfor12minutesforeachparticipantwhere3610-secondremountingsessionsof
datawascollected.
Theparticipantswereaskedtocomebackontwootherdaysafterthefirstdayandcollecteddatatwomore
timeswiththesameproceduresbecausewewouldliketoevaluatethestabilityofSonicIDacrossdifferentdays.
Aftertheparticipantsweredonewiththestudyonallthreedays,theyfilledoutaquestionnairetoprovidetheir
demographicinformationandfeedbackonusingtheprototypeandthesystem.
5.2 Participants
TheuserstudywasapprovedbytheInstitutionalReviewBoard(IRB)ofthehomeinstitutionoftheresearchers.
Wesuccessfullyrecruited24participants(13females,9malesand2non-binaries)withanaverageageof23.8
yearsold,rangingfrom20yearsoldto30yearsold.Foreachparticipant,wecollected18minutesofdata,which
weredividedin10810-secondremountingsessionsacrossthreedays.Hence,432minutesofdatawascollected
intotalforthisuserstudy.NotethatthedataP11collectedonDay3waslostduetothehardwareissuesowe
askedtheparticipanttocomebackagainandredothestudyonDay3.
6 EVALUATION
Inthissection,wepresenttheevaluationresultsoftheuserstudystatedinSec.5.Wecomparetheperformance
ofSonicIDwithdifferentsettings.Moreover,weevaluatetheusabilityofthesystembyanalyzingtheresponses
tothequestionnairescollectedafterthestudy.
6.1 StudyResults
6.1.1 Cross-session Performance. In the user study, we collected 36 sessions (6 minutes) of data from each
participantoneachday.WefirstevaluatedSonicID’sperformanceacrossdifferentremountingsessionsonthe
sameday.Outofthe36sessionswecollectedonDay1,weconsideredthefirst6sessions(1minute)ofdataas
thepracticesessionsanddumpedthemwhileweevaluatedthesystem.Amongtheremaining30sessions,we
usedthefirst24sessions(4minutes)ofdataaspositivesamplestotrainthedeeplearningmodeldescribedin
Sec.3.4.Wewouldliketoscanusers’facewhiletheyarestayingstill.However,theycanmoveand/orblink
unconsciouslyduringthestudy,whichmighthaveanimpactonthescanningresults.Therefore,wedividedeach
10-secondsessionintomultipleinstancesof0.12s,asdiscussedinSec.3.4,andpickedtopfiveinstanceswiththe
smallestmovementsbycomparingtheenergychangewithintheseinstances.Five0.12-secondinstancesfrom
eachsessionwereactuallyusedfortrainingthemodel.Thenegativesamplesusedtotrainthemodelcamefrom
adatasetwecollectedpriortotheuserstudy.Tocollectthisdataset,weranapilotstudyofthesameprocedures
astheformaluserstudywith17people,including7researchersand10peoplefromtheirnetworks.Amongthese
people,7ofthemcollecteddataonthreedays,2collecteddataontwodaysandtheremaining8collecteddata
ononeday.Eachpersoncontributed5minutesofdataofstayingstilloneachday.Allthesedatainthedataset
wereusedfortrainingthemodelasnegativesamples.Thesameprocesswasconductedtoselectthemoststatic
instancesfromallthesessionswhiletraining.
Afterthemodelwasproperlytrainedforeachparticipantintheuserstudy,theremaining6sessions(1minute)
ofdatafromthisparticipantwasadoptedaspositivesamplestotestthemodel.Thenegativesamplesfortesting
camefromtheother23participantsintheuserstudy.Eachofthese23participantscontributed5minutesofdata
totestthemodel.Inthisway,these"attackers"duringevaluationwerenotseenbythemodelinthetraining
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article.Publicationdate:2024.SonicID:UserIdentificationonSmartGlasseswithAcousticSensing • 11
Fig.3. TruePositiveRateforallParticipantsacrossThreeDays.
Fig.4. FalsePositiveRateforallParticipantsacrossThreeDays.
Fig.5. BalancedAccuracyforallParticipantsacrossThreeDays.
process.Staticinstanceswerealsoselectedfromallsessionsfortesting.Themodelmadebinaryclassifications
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article.Publicationdate:2024.12 • Lietal.
toauthenticateusersandcomputedthethreemetricsintroducedinSec.3.5toevaluatetheresults.Wetrained
an individual model for each participant and the average TPR, FPR and BAC across 24 participants is 96.5%
(std=9.0%),4.1%(std=4.6%)and96.2%(std=5.6%).ThedetailedmetricsforeveryparticipantareshowninFig.3,
Fig4andFig.5respectively.
The average FPR of 4.1% indicates that the system rejects false users with a success rate of over 95%. It
demonstratestheabilityofSonicIDtoprotectthesmartglassesfrombeingattackedbybadactors.Theaverage
TPRof96.5%meansthatthesystemauthenticatestherealuserssuccessfullywitharateofover95%.Itshows
theabilityofSonicIDtorecognizetrueusersofthesmartglasses.BACgivesabalancedevaluationofthetwo
metricsabove.
6.1.2 Cross-dayPerformance. TheresultsabovevalidatedtheperformanceofSonicIDacrossdifferentremounting
sessionsonthesameday.Thenweevaluatedtheperformanceacrossdifferentdays,whichwasmeanttovalidate
thestabilityofSonicID.OnDay2andDay3,wealsocollected6minutesofdatafromeachparticipantusingthe
sameproceduresasDay1.TobeconsistentwithDay1,wealsodroppedthefirst1minuteofdatafromevery
participant.Thenthestaticinstancespickedfromtheremaining5minutesofdatawereusedfortestingthe
cross-dayperformanceofSonicID.WedidnotretrainthemodelsanddirectlyusedthemodelstrainedonDay1.
Eachparticipantprovidedhis/herown5minutesofdataaspositivesamplestotestthemodelforTPRonDay2
andDay3separately.Thenegativesamplesstillcamefromtheother23participantsintheuserstudytotestFPR.
TheresultsshowedthattheaverageTPR,FPRandBACforDay2across24participantsare91.3%(std=11.9%),
4.5% (std=4.8%) and 93.4% (std=6.2%) while those for Day 3 are 80.7% (std=18.3%), 4.2% (std=5.1%) and 88.3%
(std=10.1%).TheindividualstudyresultsforeachparticipantaredisplayedinFig.3,Fig4andFig.5respectively.
Weranaone-wayrepeatedmeasuresANOVAtestamongTPRonthreedaysforall24participantsandidentified
astatisticallysignificantdifference(𝐹(2,69) = 7.99,𝑝 = 0.001 < 0.05).Specifically,weconductedarepeated
measurest-testbetweenTPRonDay1andDay2,andwedidnotfindaquitestatisticallysignificantdifference
(𝑡(23) =1.75,𝑝 =0.09>0.05).Whilethesamerepeatedmeasurest-testwasconductedbetweenTPRonDay1
andDay3,astatisticallysignificantdifferencewasfound(𝑡(23) =4.00,𝑝 =0.0006<0.05).Thisindicatesthat
TPRremainsconsistentonDay2butdecreasesonDay3,comparedtothatonDay1.However,theaverage
TPRisstillover80%evenonDay3.Underthecaseswhentherealusersfailtoauthenticatethemselvesintothe
smartglasses,theycanadjusttheglassespositionorremountthedeviceonce,andtheywilllikelylogintothe
systemsuccessfully.ThedecreaseofTPRcanbeattributedtothelackofpositivetrainingdatafromtheuser.
OnepotentialsolutiontoimprovingTPRistocontinuouslycollectthebiomentricinformationofusers’face
whiletheyareusingthedeviceandupdatethemodelaccordinglytoincreasethesuccessrateofauthentication.
Weplantoexplorethismoreinthefuture.
Similarly,wealsoranaone-wayrepeatedmeasuresANOVAtestamongFPRonthreedaysforall24participants
anddidnotfindastatisticallysignificantdifference(𝐹(2,69) =0.39,𝑝 =0.68>0.05).Thisvalidatesthestability
ofSonicIDtoprotectthesystemfromattackersacrossdifferentdays.Insuchauthenticationsystems,itisusually
morecrucialtokeepFPRlowerthankeepingTPRhigherincaseofatradeoffsincewegenerallydonotwant
otherpeopletohaveeasyaccesstoourpersonaldevices.
InFig.3andFig.4,wecanseethatP20hasthelowestTPR,especiallyonDay3whileP12hasthehighest
FPR.Bycheckingthevideosoftheparticipantsrecordedduringthestudyforreference,wefiguredthatP12and
P20havelonghairandbangswhichmadeitharderforthemtoputtheglassesbacktothesamepositionevery
timeafterremounting.Thisnegativelyaffectedtheperformanceofthesystemonthem.InSec.7.4,wediscussed
severalpotentialmethodstoimprovethesystemperformanceandstabilityinfuture.
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article.Publicationdate:2024.SonicID:UserIdentificationonSmartGlasseswithAcousticSensing • 13
(a)TruePositiveRate (b)FalsePositiveRate (c)BalancedAccuracy
Fig.6. AverageTPR,FPRandBACacross24ParticipantswithDifferentAmountsofTrainingData.
6.2 ImpactofDifferentAmountsofTrainingData
Intheevaluationresultsabove,weadopted4minutesoftrainingdatafromeachparticipant,whichleadsto
satisfactoryperformance.However,inpractice,thefewerdataanewuserneedstoprovidetoregisteronthe
device,themoreuser-friendlythesystemwillbe.Therefore,weconductedanotherexperimenttoexplorethe
impactofdifferentamountsoftrainingdataonthesystemperformance.TheaverageTPR,FPRandBACamong
24participantswithdifferentamountsoftrainingdataisillustratedinFig.6.Asshowninthefigure,bothTPR
andFPRdecreasedwhenwereducedtheamountofdatatotrainthemodel.From4minutesofdatato1minute
ofdata,TPRdropsfrom96.5%to90.3%onDay1,from91.3%to72.3%onDay2andfrom80.7%to50.8%onDay3
respectively.Inthemeantime,theFPRalsodecreasesfrom4.1%to1.6%onDay1,from4.5%to2.3%onDay2and
from4.2%to1.9%onDay3.Hence,BACchangesfrom96.2%to94.3%onDay1,from93.4%to85%onDay2and
from88.3%to74.4%onDay3correspondingly.Theseresultsindicatesthattherealusershavemoredifficulty
authenticatingthemselvesintothedevicebutthesystemprotectsthedevicefromunknownusersbetter,with
feweramountsoftrainingdata.SinceFPRisamoreimportantmetricinauthenticationsystemsasdiscussed
above,wecanjustcollect1minuteoftrainingdatawhenanewuserregistersonthedevice,leadingtoaTPRof
90.3%andaFPRof1.6%.Withtimepassingby,theauthenticationmechanismmighttendtofailmorefrequently
whentheregistereduserswanttousethedevice.However,newtrainingdatacanbecontinuouslycollected
everytimewhentheuserwearsthedeviceandmakessomeoperations.Themodelwillbeupdatedaccordingly
tomakesurethattheregisteredusershavenoproblemgettingaccesstothedevice.
6.3 ComparisonofTwoSidesofSensors
WhenwedesignedtheprototypefortheSonicIDsystem,weinstrumentedonepairofspeakerandmicrophone
oneachsideoftheglassestocollectcompletebiometricinformationfromusers’face.However,manycommercial
smartglassesonlyhavethespeakerandmicrophoneononesideofthedevice.Asaresult,wewouldliketo
experiment on how the system performs when only one side of sensors are utilized. As designed in Sec. 3,
four speaker-microphone channels are input into the deep learning model when two pairs of speakers and
microphonesaredeployed.Ifweonlywanttouseonpairofspeakerandmicrophoneoneitherrightsideor
leftsideoftheglasses,wecanjustinputonespeaker-microphonechannelrelatedtothispairofspeakerand
microphoneintothemodel.TheexperimentresultsaredemonstratedinTab.2.
Aswecanseeinthetable,thesystemperformancegetsworsewhenonlyonesideofsensorsareused.We
believethisisbecausedeployingthesensorsononesideonlyextractinformationfromthechannelofsignal
thattravelsonthesamesideofthefacewhileinstrumentingsensorsonbothsidesalsoobtaininformationfrom
thechannelofsignalthattravelsacrossusers’facefromonesidetotheotherside.Thisvarietyofbiometric
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article.Publicationdate:2024.14 • Lietal.
Table2. AverageTPR,FPRandBACacross24ParticipantswithDifferentChannelsofSignals.DataFormat:TPR/FPR/BAC.
DifferentChannels Day1 Day2 Day3
BothSides 96.5%/4.1%/96.2% 91.3%/4.5%/93.4% 80.7%/4.2%/88.3%
RightSide 91.8%/3.5%/94.2% 69.7%/4.1%/82.8% 61.1%/3.9%/78.6%
LeftSide 81.1%/2.5%/89.3% 38.7%/3.1%/67.8% 21.8%/2.4%/59.7%
informationhelpsboostthesystemperformance.Inaddition,onethingwenoticedisthatrightsidehasmuch
betterperformancethanleftside.Givenpeople’sfacearemostlysymmetric,wespeculatethatthisdiscrepancy
might be caused by the different frequency ranges we use on two sides (right side: 18-21 kHz and left side:
21.5-24.5kHz).However,webelievethatmorethoroughexperimentsareneededtoverifythisassumption.Even
thoughonesideofsensorscausethesystemperformancetodrop,theperformanceonDay1usingrightside
sensorsarestillrelativelycomparabletousingbothsidessensors.ThismeansthatitispossibleforSonicIDto
onlyincludesensorsononesideandmoretrainingdatacanbecollectedautomaticallywhenusersarewearing
thedevicetoimprovethesystemperformanceonfollowingdays.
6.4 Usability
Aquestionnairewasdistributedtoparticipantsaftertheuserstudytocollecttheirdemographicinformationand
feedbackontheprototypedwearabledevice.First,theparticipantsevaluatedtheoverallexperienceofwearing
thedevicebyansweringthequestion"Howcomfortableisthiswearabledevicetoweararoundtheface?(0
mostuncomfortable,5mostcomfortable)".Among24participants,theaverageratingforthisquestionis3.6
(std=0.7),indicatingthatthedevicewiththeSonicIDsystemisoverallcomfortabletoweararoundtheface.Next
wespecificallyaskedparticipantstoevaluatetheweightofthedevicewiththequestion"Howacceptabledo
youfindtheweightofourwearabledevice?(0mostunacceptable,5mostacceptable)".Theaverageratingis3.8
(std=0.9),validatingthattheweightofthedevicedoesnotcausemuchburdenonusers.
IntheSonicIDsystem,weutilizedtwoultrasonicsignalsinthefrequencyrangesover18kHz.Theoretically,
mostpeoplearenotcapableofhearingthesoundsemittedfromthesystem.However,duetothelimitationofthe
hardwarecomponents,theremightbesomefrequencyleakageintothelowerfrequencyrangeandsomeusers
mightbeabletohearthesound.Asaresult,weaskedtheparticipantstoalsoanswerthequestion"Canyouhear
thesoundemittedfromoursystem?".18outof24participantsanswered"No"tothisquestionwhiletheother6
participantsreported"Yes".Forthoseparticipantswhoanswered"Yes"tothisquestion,afollow-upquestion"If
yes,howcomfortabledoyoufeelabouttheemittedsound?(0mostuncomfortable,5mostcomfortable)"was
asked.Theaverageratingforthisquestionamong6participantsis4.3(std=0.5).Eventhoughsomeusersmight
hearthesoundfromthesystem,theygenerallyfindthissoundnotbotheringthemalot.Sincetheechoprofilewe
useasaninstancetoauthenticateusersisasshortas0.12sasdiscussedinSec.3.4,wedonotexpectthesound
totroubletheuserscontinuously.Furthermore,itispossibletolowerthestrengthofthetransmittedsignalsto
makethemlessnoticeablebyusers.
Finally,theparticipantsprovidedsomeopen-endedcommentsonthewearabledeviceweprototyped.Most
participantsfoundthisdeviceeasytowearandnodiscomfortwhilewearingit.Someparticipantsreportedthat
theglasseswasalittleimbalancedduetotheplacementofthemicro-controller.Someparticipantsstatedthattheir
hairwastakenawaybythemicro-controllerwhentheytakenoffthedevice.Otherparticipantsbelievedthatthe
placementofthemicro-controllerandthewirescanbeimprovedtomaketheprototypemorecomfortable.These
issuescanbeaddressedbybalancingthetwosidesoftheglassesandbetterincorporatingthemicro-controller
intotheglasseswithsomecasesorcovers.Besides,someparticipantsthoughtthattheglasseswasabittightfor
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article.Publicationdate:2024.SonicID:UserIdentificationonSmartGlasseswithAcousticSensing • 15
them.ThisistheproblemofthecommodityglassesweuseinsteadoftheSonicIDsystemitself.However,wecan
potentiallyimplementseveralprototypesofthesystemonglassesofdifferentsizestotacklethisproblem.
7 DISCUSSION
7.1 PowerConsumption
WewantedtoevaluatethepowerconsumptionoftheSonicIDsysteminoperation.Forthispurpose,wemeasured
thecurrentthatflowedthroughsystemwithacurrentranger5 whenthesystemwasoperating.Thecurrent
wasmeasuredat165.4mAwithavoltageat3.3V.Thisgaveusanaveragepowerconsumptionof545.8mW.
ConsideringthatSonicIDonlyneedsaninstanceof0.12stoauthenticateuserseverytime,itconsumes19.8mAs
ofenergyforeachauthenticationtrial.IfSonicIDisdeployedonMetaRay-Banwithabatterycapacityof154
mAh,theauthenticationsystemcanbeactivatedaround28,000timesintheory,ifSonicIDisusedalone.
7.2 HealthImplications
Eventhoughwepickedthesignalsintheultrasonicfrequencyrangestomaketheminaudibletotheusers,they
may still cause health concerns because the system is close to users’ ears when they wear it. Therefore, we
usedtheNIOSHsoundlevelmeterApp6tomeasurethesoundlevelofthesignalsemittedfromthesystem.We
keptthesystemtransmittingsignalscontinuouslyandplacedthemicrophonesofthesmartphonewiththeApp
runningatadistance,whereusers’earswouldbeiftheyworetheglasses,fromthespeakersoftheSonicID
system.Thesoundlevelwasmeasuredat65.8dB.Basedonthefindingsin[18],therecommendedexposure
limitforultrasonicsignalsaround20kHzis75dB.ThisindicatesthatourSonicIDsystemissafetowearfor
thegeneralpublicgiventhattheauthenticationprocessonlylastsforashortperiodoftimewhenusersstart
tooperatethedevice.Whatismore,thesignalstrengthcanbereducedtofurtherrestrainitsimpactonusers’
health.
7.3 Applications
SonicIDproposesalow-powerandminimally-obtrusivesolutiontouserauthenticationonsmartglasses.Mean-
while,thecurrentprototypeisrelativelycheap(around$50)withalotofpotentialsforfurthermodification
andoptimizationwithanevencheapercost.Therefore,weexpectthesystemtobeappliedonmorewearable
devicessuchasVRheadsets,ARglasses,etc.Theauthenticationpipelinecanalsobeutilizedinscenariosbeyond
that when users trying to log into the device. It can be used anytime when you need user identification on
wearabledevices,includingmakingpayments,loggingintoasocialaccount,changingthesettingsofthedevice,
etc.DespitethepromisingapplicationspaceofSonicID,itisimportanttokeepimprovingitsperformanceto
makesurethatthefalsepositivesamplesareasfewaspossible.
7.4 PotentialMethodstoImprovePerformance
AsevaluatedinSec.6,althoughtheFPRisconsistentlybelow5%acrossseveraldaysinevaluation,theTPR
reducesonDay2andDay3comparedwithDay1forsomeparticipants.Webelievethisisduetoseveralreasons.
Firstly,thepositivesampleswecollectedfromeachparticipanteverydaywerejustfromthe5minutesofdata.
ThesmallsizeofthedatacanleadtofluctuationsintheevaluationresultsinTPR.Secondly,wenoticedthatTPR
isespeciallylowforP20whohaslonghairandbangs.Thehaircansometimescoverthesystemandpreventthe
participantfromputtingtheglassesbacktothesamepositionontheirheadeverytimetheyremountthedevice.
Infuture,weplantoexploreseveralpotentialmethodstoimprovetheperformanceofthesystem.
5https://lowpowerlab.com/guide/currentranger/
6https://www.cdc.gov/niosh/topics/noise/app.html
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article.Publicationdate:2024.16 • Lietal.
7.4.1 ExploreMovement-basedAuthentication. InSec.6,weremovedinformationofblinkingandothermove-
mentsfromthedataduringtrainingandtestingtoguaranteeavalidfacialscanningoutcome.However,this
informationcanalsobetakenadvantageoftoauthenticateusers.Forinstance,theblinkinginformationalone
canbeexploitedasauniquebiometricpatternbecausedifferentusersblinkdifferently.Furthermore,wecan
exploreothermovement-basedauthenticationmethodstoaddanotherlayerofprotection.Usersofoursystem
canselectaspecificfacialgesture(e.g.smile,tongueout,blink,etc.)astheirpasswordtothesystem.Oursystem
recognizesboththeirpasswordandtheirbiometricinformationwhenperformingthispasswordtoauthenticate
users.Thiskindoftwo-layerauthenticationsystemsmakethesystemmoresecureandarepossibletoimplement
giventhattherearealreadysomeworkvalidatingthatdeployingacousticsensingonwearablescanhelptrack
users’facialmovements[4,28,58,62].SonicIDcanpotentiallybeintegratedintotheseexistingsystemstorealize
userauthenticationwithoutaddingextrahardware.
7.4.2 Continuously Collect Training Data. Another potential way to improve the system performance is to
continuouslycollecttrainingdatawhiletheuseroperatestheglasses.Currently,SonicIDrunsaregistering
process for new users to collect training data. This small amount of training data limits the stability of the
systemacrossdifferentdaysduetothelackofvariance,especiallyforTPR.SonicIDcanalleviatethisproblem
by continuously collecting the biometric information from the user’s face while they use the smart glasses.
The system can scan their face occasionally when they use the device and pick the static instances to train
andfine-tunethemodel.Thishelpsreducetheburdenofcollectingtoomuchtrainingdatabeforeusingthe
authenticationsystem.
7.4.3 Pre-collectMoreDatainPilotStudy. Whilewetrainedthemodelsintheuserstudy,thenegativesamples
camefromthe17peoplefromwhomwecollecteddatainthepilotstudy.Ifwecanrecruitmorevolunteers
tocollectnegativesamplesinthisdataset,thesystemperformance,especiallyFPR,canbeimprovedsincethe
modelwilllearntheacousticpatternsbetterfrommoredata.Inthemeantime,thisdoesnotaddextraburdenof
collectingmoretrainingdataonthenewusers.
7.5 LimitationandFutureWork
SonicID provides a low-power, effortless, and minimally-obtrusive solution to user authentication on smart
glassesthatworksrelativelyconsistentlywellacrossdifferentremountingsessionanddifferentdays.However,
therearestillsomelimitationofthiswork.
7.5.1 ImpactofLongHair. ThelonghairandbangsofusersmightimpacttheperformanceofSonicIDand
evencoverthesystem.Inthestudy,participantswithlonghairwereaskedtoputtheirhairupwithahairtie.
Nevertheless,someparticipantsexperienceddifficultyputtingtheglassesbacktothesamewearingposition
everytimeandhadworseperformanceinTPRorFPRcomparedwithotherparticipants.Inthefuture,weplanto
furtherupgradetheprototypebyplacingtheMCUandsensorsatabetterpositionandcoveringthemwitha
casesothattheyarenoteasilyimpactedbylonghairandbangs.
7.5.2 AmountofTrainingDataNeeded. Sec.6evaluatestheperformanceofSonicIDwithdifferentamountsof
trainingdata.With1minuteofdata,SonicIDachievessatisfactoryperformanceonDay1buttheperformance,
especiallyTPRdecreasesonDay2andDay3.Onesolutiontothisproblemistocollectmoretrainingdatawhen
theuseroperatesthedevicecontinuouslyinthefirstcoupledaysofwearingthedeviceasdiscussedinSec.7.4.2.
Moreover,whenweareabletocollectdatafrommoreparticipantsinthefuture,themodelcanbetterlearnthe
acousticpatternsandmakebetterauthentication.
7.5.3 AdapttoCommoditySmartGlasses. TovalidatethefeasibilityofSonicID,wedeployedthesystemona
commonpairofglassesandconductedthestudy.Infuture,weplantodirectlyusethebuilt-inspeakersand
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article.Publicationdate:2024.SonicID:UserIdentificationonSmartGlasseswithAcousticSensing • 17
microphonesoncommercialsmartglassestoimplementthesystemsothatitisdirectlyapplicableandreadyto
usebycustomers.
8 CONCLUSION
Inthispaper,wepresentanacoustic-basedsolutiontouseridentificationonsmartglasses,calledSonicID.It
adoptsactiveacousticsensingtoscantheuser’sfacesothatbiometricinformationisobtainedfromtheuser
to authenticate them. A customized binary classifier based on the ResNet-18 architecture is used to extract
uniquefeaturesrelatedtoeachuserfromtheacousticpatterns.Auserstudywith24participantsvalidatedthe
performanceofSonicIDacrossdifferentremountingsessionsanddays.SonicIDauthenticatestheuserwithin
0.12secondassoonashe/sheputsonthedeviceandonlyconsumes19.8mAsofenergyforeachauthentication
trial.WefurtherexploreSonicID’sperformanceunderdifferentsettingsanddiscussthepotentialapplicationsof
SonicIDandhowitcanbedeployedoncommercialsmartglassesinthefuture.
REFERENCES
[1] [n.d.].Nymi. https://www.nymi.com/nymi-band
[2] MohamedAbdelhamid.2021. FitnessTrackerInformationandPrivacyManagement:EmpiricalStudy. JournalofMedicalInternet
Research23(2021).
[3] TakashiAmesaka,HirokiWatanabe,MasanoriSugimoto,YutaSugiura,andBuntarouShizuki.2023.UserAuthenticationMethodfor
HearablesUsingSoundLeakageSignals.InProceedingsofthe2023ACMInternationalSymposiumonWearableComputers(Cancun,
QuintanaRoo,Mexico)(ISWC’23).AssociationforComputingMachinery,NewYork,NY,USA,119–123. https://doi.org/10.1145/
3594738.3611376
[4] AnonymousAuthors[ToAppear].2024.EyeEcho:ContinuousandLow-powerFacialExpressionTrackingonGlasses.InProceedingsof
theSIGCHIConferenceonHumanFactorsinComputingSystems(CHI’24).AssociationforComputingMachinery,NewYork,NY,USA.
[5] Apple.2024.AboutFaceIDadvancedtechnology. RetrievedFeb1,2024fromhttps://support.apple.com/en-us/102381
[6] AdamJ.Aviv,KatherineGibson,EvanMossop,MattBlaze,andJonathanM.Smith.2010. SmudgeAttacksonSmartphoneTouch
Screens.In4thUSENIXWorkshoponOffensiveTechnologies(WOOT10).USENIXAssociation,Washington,DC. https://www.usenix.org/
conference/woot10/smudge-attacks-smartphone-touch-screens
[7] DanielVBailey,MarkusDürmuth,andChristofPaar.2014.Typingpasswordswithvoicerecognition:HowtoauthenticatetoGoogle
Glass.InProc.oftheSymposiumonUsablePrivacyandSecurity.1–2.
[8] CBeumierandMAcheroy.2000. Automatic3Dfaceauthentication. ImageandVisionComputing18,4(2000),315–321. https:
//doi.org/10.1016/S0262-8856(99)00052-9
[9] M.Bicego,A.Lagorio,E.Grosso,andM.Tistarelli.2006.OntheUseofSIFTFeaturesforFaceAuthentication.In2006Conferenceon
ComputerVisionandPatternRecognitionWorkshop(CVPRW’06).35–35. https://doi.org/10.1109/CVPRW.2006.149
[10] PanChan,TziporaHalevi,andNasirMemon.2015. GlassOTP:SecureandConvenientUserAuthenticationonGoogleGlass.In
FinancialCryptographyandDataSecurity,MichaelBrenner,NicolasChristin,BenjaminJohnson,andKurtRohloff(Eds.).SpringerBerlin
Heidelberg,Berlin,Heidelberg,298–308.
[11] JagmohanChauhan,YiningHu,SurangaSeneviratne,ArchanMisra,ArunaPrasadSeneviratne,andYoungkiLee.2017.BreathPrint:
BreathingAcoustics-basedUserAuthentication.Proceedingsofthe15thAnnualInternationalConferenceonMobileSystems,Applications,
andServices(2017).
[12] CoryCornelius,RonaldPeterson,JosephSkinner,RyanHalter,andDavidKotz.2014. Awearablesystemthatknowswhowearsit
(MobiSys’14).AssociationforComputingMachinery,NewYork,NY,USA,55–67. https://doi.org/10.1145/2594368.2594369
[13] JohnG.Daugman.1994.Biometricpersonalidentificationsystembasedonirisanalysis.USPatent5,291,560(1994).
[14] AlexanderDeLuca,AlinaHang,FrederikBrudy,ChristianLindner,andHeinrichHussmann.2012.Touchmeonceandiknowit’syou!
implicitauthenticationbasedontouchscreenpatterns.InProceedingsoftheSIGCHIConferenceonHumanFactorsinComputingSystems
(Austin,Texas,USA)(CHI’12).AssociationforComputingMachinery,NewYork,NY,USA,987–996. https://doi.org/10.1145/2207676.
2208544
[15] QuangDo,BenMartini,andKim-KwangRaymondChoo.2017.Isthedataonyourwearabledevicesecure?AnAndroidWearsmartwatch
casestudy.Software:PracticeandExperience47(2017),391–403. https://api.semanticscholar.org/CorpusID:30963224
[16] NguyenMinhDucandBuiQuangMinh.2009. Yourfaceisnotyourpasswordfaceauthenticationbypassinglenovo–asus–toshiba.
BlackHatBriefings4(2009),158.
[17] YangGao,WeiWang,VirV.Phoha,WeiSun,andZhanpengJin.2019.EarEcho:UsingEarCanalEchoforWearableAuthentication.
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.3,3,Article81(sep2019),24pages. https://doi.org/10.1145/3351239
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article.Publicationdate:2024.18 • Lietal.
[18] CarlHoward,ColinHansen,andAZander.2005.Areviewofcurrentairborneultrasoundexposurelimits.TheJournalofOccupational
HealthandSafety-AustraliaandNewZealand21(012005),253–257.
[19] JunHoHuh,HyejinShin,HongMinKim,EunyongCheon,YoungeunSong,Choong-HoonLee,andIanOakley.2023.WristAcoustic:
Through-WristAcousticResponseBasedAuthenticationforSmartwatches.Proc.ACMInteract.Mob.WearableUbiquitousTechnol.6,4,
Article167(jan2023),34pages. https://doi.org/10.1145/3569473
[20] MD.RaselIslam,DoyoungLee,LizaSuraiyaJahan,andIanOakley.2018. GlassPass:TappingGesturestoUnlockSmartGlasses.In
Proceedingsofthe9thAugmentedHumanInternationalConference(Seoul,RepublicofKorea)(AH’18).AssociationforComputing
Machinery,NewYork,NY,USA,Article16,8pages. https://doi.org/10.1145/3174910.3174936
[21] KaitoIsobeandKazuyaMurao.2023. PersonalIdentificationMethodusingActiveAcousticSensingappliedtotheNosepadof
Eyeglasses.InAdjunctProceedingsofthe2022ACMInternationalJointConferenceonPervasiveandUbiquitousComputingandthe2022
ACMInternationalSymposiumonWearableComputers(Cambridge,UnitedKingdom)(UbiComp/ISWC’22Adjunct).Associationfor
ComputingMachinery,NewYork,NY,USA,345–348. https://doi.org/10.1145/3544793.3560400
[22] ChauhanJagmohan,HassanAsghar,AnirbanMahanti,andDaliKaafar.2016.Gesture-BasedContinuousAuthenticationforWearable
Devices:TheSmartGlassesUseCase.648–665. https://doi.org/10.1007/978-3-319-39555-5_35
[23] IanJermyn,AlainMayer,FabianMonrose,MichaelK.Reiter,andAvielRubin.1999.TheDesignandAnalysisofGraphicalPasswords.In
8thUSENIXSecuritySymposium(USENIXSecurity99).USENIXAssociation,Washington,D.C. https://www.usenix.org/conference/8th-
usenix-security-symposium/design-and-analysis-graphical-passwords
[24] AmyK.Karlson,A.J.BernheimBrush,andStuartSchechter.2009.Caniborrowyourphone?understandingconcernswhensharing
mobilephones.InProceedingsoftheSIGCHIConferenceonHumanFactorsinComputingSystems(Boston,MA,USA)(CHI’09).Association
forComputingMachinery,NewYork,NY,USA,1647–1650. https://doi.org/10.1145/1518701.1518953
[25] YoheiKawasakiandYutaSugiura.2022.PersonalIdentificationandAuthenticationUsingBlinkwithSmartGlasses.In202261stAnnual
ConferenceoftheSocietyofInstrumentandControlEngineers(SICE).1251–1256. https://doi.org/10.23919/SICE56594.2022.9905842
[26] SunwooLee,WonsukChoi,andDongHoonLee.2021.UsableUserAuthenticationonaSmartwatchusingVibration.InProceedingsof
the2021ACMSIGSACConferenceonComputerandCommunicationsSecurity(VirtualEvent,RepublicofKorea)(CCS’21).Association
forComputingMachinery,NewYork,NY,USA,304–319. https://doi.org/10.1145/3460120.3484553
[27] Lenovo.2024. ThinkRealityA3SmartGlasses. RetrievedJan17,2024fromhttps://www.lenovo.com/us/en/p/smart-devices/virtual-
reality/thinkreality-a3/wmd00000500
[28] KeLi,RuidongZhang,BoLiang,FrançoisGuimbretière,andChengZhang.2022.EarIO:ALow-powerAcousticSensingEarablefor
ContinuouslyTrackingDetailedFacialMovements.Proc.ACMInteract.Mob.WearableUbiquitousTechnol.6,2,Article62(jul2022),
24pages. https://doi.org/10.1145/3534621
[29] YanLi,YaoCheng,WeizhiMeng,YingjiuLi,andRobertH.Deng.2021.DesigningLeakage-ResilientPasswordEntryonHead-Mounted
SmartWearableGlassDevices.IEEETransactionsonInformationForensicsandSecurity16(2021),307–321. https://doi.org/10.1109/TIFS.
2020.3013212
[30] YunghuiLiandHuangPo-Jen.2017. AnAccurateandEfficientUserAuthenticationMechanismonSmartGlassesBasedonIris
Recognition.MobileInformationSystems2017(072017),1–14. https://doi.org/10.1155/2017/1281020
[31] HyunchulLim,GuilinHu,RichardJin,HaoChen,RyanMao,RuidongZhang,andChengZhang.2023.C-Auth:ExploringtheFeasibility
ofUsingEgocentricViewofFaceContourforUserAuthenticationonGlasses.InProceedingsofthe2023ACMInternationalSymposium
onWearableComputers(Cancun,QuintanaRoo,Mexico)(ISWC’23).AssociationforComputingMachinery,NewYork,NY,USA,6–10.
https://doi.org/10.1145/3594738.3611355
[32] SaifMahmud,KeLi,GuilinHu,HaoChen,RichardJin,RuidongZhang,FrançoisGuimbretière,andChengZhang.2023.PoseSonic:3D
UpperBodyPoseEstimationThroughEgocentricAcousticSensingonSmartglasses. Proc.ACMInteract.Mob.WearableUbiquitous
Technol.7,3,Article111(sep2023),28pages. https://doi.org/10.1145/3610895
[33] MaryamNaseerMalik,MuhammadAwaisAzam,MuhammadEhatisham-Ul-Haq,WaleedEjaz,andAsraKhalid.2019. ADLAuth:
PassiveAuthenticationBasedonActivityofDailyLivingUsingHeterogeneousSensinginSmartCities.Sensors19,11(2019). https:
//doi.org/10.3390/s19112466
[34] SébastienMarcel,ChrisMccool,CosminAtanasoaei,FlavioTarsetti,JanPesan,PavelMatejka,JanCernocky,MikaHelistekangas,and
MarkusTurtinen.2010.MOBIO:MobileBiometricFaceandSpeakerAuthentication.(012010).
[35] Meta. 2024. Ray-Ban Meta Smart Glasses. Retrieved Jan 17, 2024 from https://www.ray-ban.com/usa/ray-ban-meta-
smart-glasses?cid=PM-SGA_000000-1.US-RayBanStories-EN-B-Related-Exact_RayBan%26Facebook_Related_Meta+smart+
glasses&s_kwcid=AL!16196!3!676854207500!e!!g!!meta%20smart%20glasses!15590003234!133748971840&gad_source=1&gclid=
CjwKCAiAkp6tBhB5EiwANTCx1B7MGSwjuu82g3hU5VUdn46SBbLT3IU-C5WVAOiqcdn02Ni1nfNWeRoCJ0oQAvD_BwE&gclsrc=
aw.ds
[36] DouglasNelson,ValerieReed,andJohnWalling.1976.Pictorialsuperiorityeffect.Journalofexperimentalpsychology.Humanlearning
andmemory2(091976),523–8. https://doi.org/10.1037/0278-7393.2.5.523
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article.Publicationdate:2024.SonicID:UserIdentificationonSmartGlasseswithAcousticSensing • 19
[37] TheDepartmentofHomelandSecurity.2024.Biometrics. RetrievedFeb1,2024fromhttps://www.dhs.gov/biometrics#:~:text=Biometrics%
20are%20unique%20physical%20characteristics,be%20used%20for%20automated%20recognition.
[38] AleksandrOmetov,ViktoriiaShubina,LucieKlus,JustynaSkibinska,SalwaSaafi,PavelPascacio,LauraFlueratoru,DarwinQuezada-
Gaibor,NadezhdaChukhno,OlgaChukhno,AsadAli,AsmaChanna,EkaterinaSvertoka,WaleedBinQaim,RaúlCasanovaMarqués,
SylviaHolcer,JoaquínTorres-Sospedra,SvenCasteleyn,GiuseppeRuggeri,GiuseppeAraniti,RadimBurget,JiriHosek,andElenaSimona
Lohan.2021. ASurveyonWearableTechnology:History,State-of-the-ArtandCurrentChallenges. Comput.Networks193(2021),
108074.
[39] GePeng,GangZhou,DavidT.Nguyen,XinQi,QingYang,andShuangquanWang.2017. ContinuousAuthenticationWithTouch
BehavioralBiometricsandVoiceonWearableGlasses. IEEETransactionsonHuman-MachineSystems47,3(2017),404–416. https:
//doi.org/10.1109/THMS.2016.2623562
[40] JoannaRokita,AdamKrzyżak,andC.Y.Suen.2008.CellPhonesPersonalAuthenticationSystemsUsingMultimodalBiometrics.InImage
AnalysisandRecognition,AurélioCampilhoandMohamedKamel(Eds.).SpringerBerlinHeidelberg,Berlin,Heidelberg,1013–1022.
[41] JessicaSehrt,FengYiLu,LeonardHusske,AntonRoesler,andValentinSchwind.2022.WristConduct:BiometricUserAuthentication
UsingBoneConductionattheWrist.InProceedingsofMenschUndComputer2022(Darmstadt,Germany)(MuC’22).Associationfor
ComputingMachinery,NewYork,NY,USA,371–375. https://doi.org/10.1145/3543758.3547542
[42] MuhammadShahzadandMunindarP.Singh.2017. ContinuousAuthenticationandAuthorizationfortheInternetofThings. IEEE
InternetComputing21,2(2017),86–90. https://doi.org/10.1109/MIC.2017.33
[43] YuboShao,TinghanYang,HeWang,andJianzhuMa.2020.AirSign:SmartphoneAuthenticationbySigningintheAir.Sensors(Basel,
Switzerland)21(2020).
[44] ZdeňkaSitová,JaroslavŠeděnka,QingYang,GePeng,GangZhou,PaoloGasti,andKiranS.Balagani.2016.HMOG:NewBehavioral
BiometricFeaturesforContinuousAuthenticationofSmartphoneUsers.IEEETransactionsonInformationForensicsandSecurity11,5
(2016),877–892. https://doi.org/10.1109/TIFS.2015.2506542
[45] Snap.2024.TheNextGenerationofSpectacles. RetrievedJan31,2024fromhttps://www.spectacles.com/
[46] LionelStanding.1973. Learning10,000Pictures. TheQuarterlyjournalofexperimentalpsychology 25(061973),207–22. https:
//doi.org/10.1080/14640747308400340
[47] AppleSupport.[n.d.].AboutFaceIDadvancedtechnology. https://support.apple.com/en-us/HT208108
[48] AppleSupport.[n.d.].Apple.UseTouchIDoniPhoneandiPad. https://support.apple.com/en-us/HT201371
[49] FurkanTari,A.AntOzok,andStephenH.Holden.2006.Acomparisonofperceivedandrealshoulder-surfingrisksbetweenalphanumeric
andgraphicalpasswords.InProceedingsoftheSecondSymposiumonUsablePrivacyandSecurity(Pittsburgh,Pennsylvania,USA)(SOUPS
’06).AssociationforComputingMachinery,NewYork,NY,USA,56–66. https://doi.org/10.1145/1143120.1143128
[50] MuhammadEhatishamulHaq,MuhammadAwaisAzam,JonathanKok-KengLoo,KaiShuang,SyedIslam,UsmanNaeem,andYasar
Amin.2017.AuthenticationofSmartphoneUsersBasedonActivityRecognitionandMobileSensing.Sensors(Basel,Switzerland)17
(2017).
[51] ImdadullahHidayaturRehman,ArshadAhmad,FahimAkhter,andMohdZiaurRehman.2022.ExaminingConsumers’Adoptionof
SmartWearablePayments.SAGEOpen12,3(2022),21582440221117796. https://doi.org/10.1177/21582440221117796
[52] Vuzix.2024.VuzixSmartGlasses. RetrievedJan31,2024fromhttps://www.vuzix.com/pages/smart-glasses
[53] TianbenWang,DaqingZhang,YuanqingZheng,TaoGu,XingsheZhou,andBernadetteDorizzi.2018.C-FMCWBasedContactless
RespirationDetectionUsingAcousticSignal.Proc.ACMInteract.Mob.WearableUbiquitousTechnol.1,4,Article170(jan2018),20pages.
https://doi.org/10.1145/3161188
[54] ZiWang,YiliRen,YingyingChen,andJieYang.2022.ToothSonic:EarableAuthenticationviaAcousticToothprint.Proc.ACMInteract.
Mob.WearableUbiquitousTechnol.6,2,Article78(jul2022),24pages. https://doi.org/10.1145/3534606
[55] ZiWang,ShengTan,LinghanZhang,YiliRen,ZhiWang,andJieYang.2021.EarDynamic:AnEarCanalDeformationBasedContinuous
UserAuthenticationUsingIn-EarWearables.Proc.ACMInteract.Mob.WearableUbiquitousTechnol.5,1,Article39(mar2021),27pages.
https://doi.org/10.1145/3448098
[56] HirokiWatanabe,HiroakiKakizawa,andMasanoriSugimoto.2021.UserAuthenticationMethodUsingActiveAcousticSensing.Journal
ofInformationProcessing29(012021),370–379. https://doi.org/10.2197/ipsjjip.29.370
[57] YuliWuandJingHe.2023. EarAE:AnAutoencoderbasedUserAuthenticationusingEarphones.In2023IEEE/CICInternational
ConferenceonCommunicationsinChina(ICCC).1–6. https://doi.org/10.1109/ICCC57788.2023.10233591
[58] WentaoXie,QianZhang,andJinZhang.2021.Acoustic-BasedUpperFacialActionRecognitionforSmartEyewear.InProceedingsofthe
ACMonInteractive,Mobile,WearableandUbiquitousTechnologies(IMWUT),Vol.5.Article41,28pages. https://doi.org/10.1145/3448105
[59] DhruvKumarYadav,BeatriceIonascu,SaiVamsiKrishnaOngole,AditiRoy,andNasirMemon.2015.DesignandAnalysisofShoulder
SurfingResistantPINBasedAuthenticationMechanismsonGoogleGlass.InFinancialCryptographyandDataSecurity,MichaelBrenner,
NicolasChristin,BenjaminJohnson,andKurtRohloff(Eds.).SpringerBerlinHeidelberg,Berlin,Heidelberg,281–297.
[60] LinghanZhang,ShengTan,andJieYang.2017.HearingYourVoiceisNotEnough:AnArticulatoryGestureBasedLivenessDetection
forVoiceAuthentication(CCS’17).AssociationforComputingMachinery,NewYork,NY,USA,57–71. https://doi.org/10.1145/3133956.
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article.Publicationdate:2024.20 • Lietal.
3133962
[61] LinghanZhang,ShengTan,JieYang,andYingyingChen.2016.VoiceLive:APhonemeLocalizationbasedLivenessDetectionforVoice
AuthenticationonSmartphones.InProceedingsofthe2016ACMSIGSACConferenceonComputerandCommunicationsSecurity(Vienna,
Austria)(CCS’16).AssociationforComputingMachinery,NewYork,NY,USA,1080–1091. https://doi.org/10.1145/2976749.2978296
[62] ShijiaZhang,TaitingLu,HaoZhou,YilinLiu,RunzeLiu,andMahanthGowda.2023.IAmanEarphoneandICanHearMyUsersFace:
FacialLandmarkTrackingUsingSmartEarphones.ACMTrans.InternetThings(TIOT)(Aug2023). https://doi.org/10.1145/3614438
[63] YongtouZhang,WenHu,WeitaoXu,ChunTungChou,andJiankunHu.2018. ContinuousAuthenticationUsingEyeMovement
ResponseofImplicitVisualStimuli. Proc.ACMInteract.Mob.WearableUbiquitousTechnol.1,4,Article177(jan2018),22pages.
https://doi.org/10.1145/3161410
[64] TianmingZhao,YanWang,JianLiu,YingyingChen,JerryCheng,andJiadiYu.2020. TrueHeart:ContinuousAuthenticationon
Wrist-wornWearablesUsingPPG-basedBiometrics.InIEEEINFOCOM2020-IEEEConferenceonComputerCommunications.30–39.
https://doi.org/10.1109/INFOCOM41043.2020.9155526
[65] BingZhou,JayLohokare,RuipengGao,andFanYe.2018. EchoPrint:Two-factorAuthenticationusingAcousticsandVisionon
Smartphones.InProceedingsofthe24thAnnualInternationalConferenceonMobileComputingandNetworking(NewDelhi,India)
(MobiCom’18).AssociationforComputingMachinery,NewYork,NY,USA,321–336. https://doi.org/10.1145/3241539.3241575
[66] QinZou,YanlingWang,QianWang,YiZhao,andQingquanLi.2020.DeepLearning-BasedGaitRecognitionUsingSmartphonesinthe
Wild. arXiv:1811.00338[cs.LG]
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article.Publicationdate:2024.