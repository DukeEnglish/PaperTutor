Large Language Models Must Be Taught
to Know What They Don’t Know
SanyamKapoor*1 NateGruver*1
ManleyRoberts2 KatherineCollins3 ArkaPal2 UmangBhatt1
AdrianWeller3 SamuelDooley2 MicahGoldblum1 AndrewGordonWilson1
1NewYorkUniversity 2AbacusAI 3CambridgeUniversity
Abstract
Whenusinglargelanguagemodels(LLMs)inhigh-stakesapplications,weneedtoknowwhen
wecantrusttheirpredictions. Someworksarguethatpromptinghigh-performanceLLMsis
sufficienttoproducecalibrateduncertainties,whileothersintroducesamplingmethodsthatcanbe
prohibitivelyexpensive.Inthiswork,wefirstarguethatpromptingonitsownisinsufficientto
achievegoodcalibrationandthenshowthatfine-tuningonasmalldatasetofcorrectandincorrect
answerscancreateanuncertaintyestimatewithgoodgeneralizationandsmallcomputational
overhead.Weshowthatathousandgradedexamplesaresufficienttooutperformbaselinemethods
andthattrainingthroughthefeaturesofamodelisnecessaryforgoodperformanceandtractable
forlargeopen-sourcemodelswhenusingLoRA.Wealsoinvestigatethemechanismsthatenable
reliableLLMuncertaintyestimation,findingthatmanymodelscanbeusedasgeneral-purpose
uncertaintyestimators,applicablenotjusttotheirownuncertaintiesbutalsotheuncertaintyof
othermodels.Lastly,weshowthatuncertaintyestimatesinformhumanuseofLLMsinhuman-AI
collaborativesettingsthroughauserstudy.
1 Introduction
“I have high cortisol but low ACTH on a dexamethasone suppression test. What
should I do?” Iftheanswertosuchaquestionisgivenwithoutassociatedconfidence,itisnot
actionable,andiftheanswerispresentedwitherroneouslyhighconfidence,thenactingontheanswer
isdangerous. Oneofthebiggestopenquestionsaboutwhetherlargelanguagemodels(LLMs)can
benefitsocietyandreliablybeusedfordecisionmakinghingesonwhetherornottheycanaccurately
representuncertaintyoverthecorrectnessoftheiroutput.
ThereisanythingbutconsensusonwhetherLLMsaccuratelyrepresentuncertainty,orevenhow
weshouldapproachuncertaintyrepresentationwithlanguagemodels. Claimsregardinglanguage
models’abilitytoestimateuncertaintyvarywidely,withsomeworkssuggestingthatlanguagemodels
are increasingly capable of estimating their uncertainty directly through prompting, without any
fine-tuningorchangestothetrainingdata[25,51],andotherssuggestingthatLLMsremainfartoo
overconfidentintheirpredictions[59,60]. ThetaskofuncertaintyestimationinLLMsisfurther
exacerbated by linguistic variances in freeform generation, all of which cannot be exhaustively
accountedforduringtraining. LLMpractitionersarethereforefacedwiththechallengeofdeciding
whichestimationmethodtouse.
Oneparticulardichotomyinuncertaintyestimationmethodsforlanguagemodelscentersaround
whethertheestimatesareblack-orwhite-box. Black-boxestimatesdonotrequiretrainingandcan
beusedwithclosed-sourcemodelslikeGPT-4[1]orGemini[48],whilewhite-boxmethodsrequire
trainingparametersonacalibrationdataset. Althoughblack-boxestimateshavebecomepopularwith
*Equalcontribution.Orderdecidedbycoinflip.Correspondenceto:sk6876@nyu.edu&nvg7279@nyu.edu
1
4202
nuJ
21
]GL.sc[
1v19380.6042:viXraGraded Dataset Fine-Tuning
What's the key to a delicious
pizza sauce? Question
Add non-toxic glue for Answer Question
tackiness Is theA annsswweerr correct? Question LLM
What's your confidence? No Is the Aannsswweerr correct?
Yes Is the answer correct?
100% No
Figure 1: Large language models struggle to assign reliable confidence estimates to their
generations. Westudythepropertiesofuncertaintycalibrationinlanguagemodels,andpropose
fine-tuningforbetteruncertaintyestimatesusingagradeddatasetofgenerationsfromthemodel.
Weevaluateourmethodsonanewopen-endedvariantofMMLU[18]. Weshowthatfine-tuning
improvesexpectedcalibrationerror(ECE)andareaunderthereceiveroperatingcharacteristiccurve
(AUROC)comparedtocommonly-usedbaselines. Errorbarsshowstandarddeviationoverthreebase
models(LLaMA-213/7BandMistral7B)andtheirchatvariants.
theriseofrestrictedmodels,theincreasedavailabilityofstrongopen-sourcemodels,suchasLLaMA
[53]orMistral[24],hasmademoreeffectivewhite-boxmethodsmoreaccessible.
Inthispaper,weperformadeepinvestigationintouncertaintycalibrationofLLMs,withfindings
thatadvancethedebateaboutnecessaryinterventionsforgoodcalibration. Inparticular,weconsider
whetherit’spossibletohavegooduncertaintiesovercorrectness(ratherthantokens)withoutinter-
vention,howwecanbestuselabeledcorrectnessexamples,howwelluncertaintygeneralizesacross
distributionshifts,andhowwecanuseLLMuncertaintytoassisthumandecisionmaking.
First,wefindthatfine-tuningforbetteruncertainties(Figure1)providesfasterandmorereliable
uncertaintyestimates,whileusingarelativelysmallnumberofadditionalparameters. Theresulting
uncertaintiesalsogeneralizetonewquestiontypesandtasks,beyondwhatispresentinthefine-tuning
dataset. Wefurtherprovideaguidetoteachinglanguagemodelstoknowwhattheydon’tknowusing
acalibrationdataset. Contrarytopriorwork,westartbyshowingthatcurrentzero-shot,black-box
methodsareineffectiveorimpracticallyexpensiveinopen-endedsettings(Section4). Wethenshow
howtofine-tunealanguagemodelforcalibration,exploringthemosteffectiveparameterization(e.g.
linearprobesvsLoRA)andtheamountofthedatathatisrequiredforgoodgeneralization(Section5).
Totestgeneralization,weevaluateuncertaintyestimatesonquestionswithsimilarformattingtothe
calibrationdataaswellasquestionsthattestrobustnesstosignificantdistributionshifts. Lastly,we
considertheunderlyingmechanismsthatenablefine-tuningLLMstoestimatetheirownuncertainties,
showingultimatelythatmodelscanbeusednotjusttoestimatetheirownuncertaintiesbutalsothe
uncertaintiesofothermodels(Section6). Beyondofflineevaluation,iflanguagemodelsaretohavea
broadsocietalimpact,itwillbethroughassistingwithhumandecisionmaking. Weconductauser
studydemonstratingwaysLLMuncertaintycanaffectAI-humancollaboration(Section7).1
2 Related Work
As generative models, LLMs naturally express a distribution over possible outcomes and should
capturevarianceintheunderlyingdata. Onmultiple-choicetests,wheretheanswerisasingletoken,
anLLM’spredictedtokenprobabilitiescanleadtoacalibrateddistributionovertheanswerchoices
[43]. Whenanswersconsistofentiresentences,however,languagemodellikelihoodsbecomealess
reliableindicatorofuncertaintybecauseprobabilitiesmustbespreadovermanyphrasingsofthesame
concept. Kuhnetal.[30]attempttomitigatethisissuebyclusteringsemanticallyequivalentanswers.
However,thesemethodsarehinderedbytheirsubstantialcomputationaloverhead. Accountingfor
equivalentphrasingsofthesamesemanticcontentrequiresenumeratingalargespaceofsentences
andclusteringforsemanticsimilaritywithanauxiliarymodel.
1https://github.com/activatedgeek/calibration-tuning
2Because LLMs are trained on text written by humans, it is possible for them to learn concepts
like“correctness”andprobabilitiesandexpressuncertaintythroughtheseabstractions. Leveraging
thisobservation,Kadavathetal.[25]andTianetal.[51]showthatcarefulpromptingcanproduce
uncertaintyestimatesintextthatgrowmorecalibratedasmodelcapabilitiesincreases. Inlightof
thisphenomenon,languagemodelsmightgainanintrinsicnotionofuncertainty,applicableacross
a broad range of topics. In the same vein, Burns et al. [9] and Azaria and Mitchell [4] find that
pre-trainedmodelshavehiddenrepresentationswhicharepredictiveoftruthfulnessanduselinear
probestoclassifyamodel’scorrectness.
Whilethesestudiessuggestapromisingtrendtowardscalibration,wefindthatthestoryisslightly
morecomplicated. Black-boxmethodsoftenfailtogenerateusefuluncertaintiesforpopularopen-
source models, and a careful fine-tuning intervention is necessary. In this way, our findings are
closer to those of Xiong et al. [59], who show that zero-shot uncertainty estimates have limited
abilitytodiscriminatebetweencorrectandincorrectanswers,evenwhenusedwiththebestavailable
models(e.g.,GPT-4). Wegofurtherbyshowingthatblack-boxmethodsstruggleonopen-ended
generation,whichisbothpracticallyimportantanddefinedbydifferentchallengesthanmultiple
choiceevaluationsfrompriorwork. Moreover,whileothershavefocusedonimprovingblack-box
methods[30,51,59],weembraceopen-sourcemodelsandtheiropportunitiesforfine-tuning,showing
thatwecanmaintainthespeedofpromptingmethodswhiledramaticallyboostingperformance.
Ourworkalsocontrastswithpriorworkonfine-tuningforuncertaintiesinseveralkeyways. While
webuildonpriorworkfromLinetal.[33]andZhangetal.[62]thatposesuncertaintyestimationas
textcompletiononagradeddataset,weintroduceseveralchangestothefine-tuningprocedure,such
asregularizationtomaintainsimilarpredictionstothebasemodel,andprovideextensiveablations
thatyieldactionableinsights. Forexample,weshowthat,contrarytopriorwork[4],frozenfeatures
aretypicallyinsufficientforuncertaintyestimatesthatgeneralizeeffectively,andthatfine-tuningon
asfewas1000gradedexampleswithLoRAissufficienttogeneralizeacrosspracticaldistribution
shifts. Alsounlikepriorwork,weprovidemanyinsightsintotherelativeperformanceoffine-tuning
compared to black-box methods, introducing a new open-ended evaluation and showing that it
displays fundamentally different trends than prior work on multiple choice questions. Although
Kadavathetal.[25]alsoconsiderscalibrationformultiplechoicequestions,manyofourconclusions
differ. Forexample, whileKadavathetal.[25]suggestthatlanguagemodelsarestrongestwhen
evaluating their own generations and subsequently posit that uncertainty estimation is linked to
self-knowledge, wefindthatcapablemodelscanreadilylearngooduncertaintiesforpredictions
ofothermodelswithoutanyknowledgeoftheirinternals. Lastly,whilemanyworksmotivatetheir
approachwithapplicationstohuman-AIcollaboration,noneofthemtesttheiruncertaintyestimates
onactualusers,aswedohere.
3 Preliminaries
Questionansweringevaluations. Inallexperiments,weusegreedydecodingtogenerateanswers
conditionedonquestionswithfew-shotprompts. Wethenlabelthegeneratedanswersascorrect
or incorrect and independently generate Ppcorrectq using one of the uncertainty estimators. For
evaluation,weprimarilyusethepopularMMLUdataset[18],whichcovers57subjectsincluding
STEM,humanities,andsocialsciences. Crucially,however,weexpandtheoriginalmultiplechoice
(MC)settingwithanewopen-ended(OE)setting. Intheopen-endedsetting, wedonotprovide
answer choices, and the language model must generate an answer that matches the ground truth
answerchoice. Wedetermineacorrectmatchbygradingwithastrongauxiliarylanguagemodel
(AppendixA.2). Weverifythatgradingvialanguagemodelsprovidesacheapandeffectiveproxyfor
thegoldstandardhumangrading(AppendixA.3),consistentwithrelatedfindings[10].
Metrics. Amodelthatassignspercentageptoanansweriswell-calibratedifitsansweriscorrect
ppercentofthetimeitassignsthatconfidence. Calibrationistypicallymeasuredusingexpected
calibration error (ECE) [37], which compares empirical frequences with estimated probabilities
throughbinning(AppendixA.4). AlowerECEisbetter,andanECEof0correspondstoaperfectly
3calibrated model. In addition to calibration, we measure the area under the receiver operating
characteristiccurve(AUROC)ofthemodel’sconfidence. HighAUROCindicatesabilitytofilter
answerslikelytobecorrectfromanswersthatarelikelytobeincorrect,asettingtypicallycalled
selectiveprediction.
Temperaturescaling. Temperaturescaling[42,17]improvesthecalibrationofaclassifierby
scalingitslogitsby 1 (whereT isthetemperature)beforeapplyingthesoftmaxfunction. Ahigh
T
temperaturescalesthesoftmaxprobabilitiestowardsauniformdistribution,whilealowtemperature
collapsesthedistributionaroundthemostprobableoutput. Thetemperatureparameterislearnedon
held-outdata,typicallytakenfromthesamedistributionasthetrainingset.
4 Do We Get Good Uncertainties Out-of-the-Box?
Inthissection,wefocusonblack-box2methodsforestimatingalanguagemodel’suncertainty. Due
tocomputationalcost,wefocusonmethodsthatrequireasinglesampleorforwardpassandonly
considersampling-basedmethodsinthenextsection.
Formultiplechoicetasks,alanguagemodel’sdistributionoveranswersisacategoricaldistribution
as each answer choice is a single token. Early work on LLMs, such as GPT-3, showed that this
distributionisoftenpoorlycalibrated[18]. Fundamentally,however,maximumlikelihoodtraining
shouldencouragecalibrationoverindividualtokens[15],andthecalibrationofrecentLLMsappears
toimproveinproportionwiththeiraccuracy[43].
In open-ended generation, on the other hand, answers are not limited to individual tokens nor a
prescribed set of possibilities, which introduces multiple sources of uncertainty. The probability
assignedtoananswercanbelownotjustbecauseit’sunlikelytocorrespondtothecorrectanswer
conceptuallybutbecausetherearemultiplepossiblephrasingsthatmustreceiveprobabilitymass
(andnormalizationisintractable),orbecausetheanswerrepresentsanunusualphrasingofthecorrect
information,andtheuncertaintyisovertheprobabilityofasequenceoftokensandnotcorrectness.
Forexample,imagineamultiple-choicetestinwhichweaddanadditionalanswerchoicethatis
asynonymofanother. Asensiblelanguagemodelwouldassignequallikelihoodtoeachchoice,
loweringtheprobabilityitassignstoeitherindividually. Inopen-endedgenerationthesituationis
similar,butevenmorechallengingbecauseofvariablelength. Addingextratokenscanartificially
lowerthelikelihoodofananswerevenwhenitexpressesthesameconcept,asthesequenceoftokens
becomeslesslikelywithincreasinglength.
Wedemonstratethedifferencebetweenmultiple-choicequestionansweringandopen-endedgener-
ation in Figure 2 (left), where we compare the AUROC of a likelihood-based method for stan-
dard MMLU a´nd open-ended MMLU¯(ours). For open-ended generations, we use perplexity,
ř
PPLpsq“exp 1 N logpps |s q ,wheresisthetokenizedsequence,becauseitisalength-
N i“1 i ăi
normalizedmetricandcommonlyusedwhentoken-levelprobabilitiesareexposedbythemodel
[19]. From AUROCs, we observe that while token-level uncertainties often improve in multiple
choiceasmodelsimprove,perplexityisgenerallynotpredictiveofalanguagemodel’scorrectness
inopen-endedsettingsanddoesnotexhibitthesamefavorablescalingwiththelanguagemodel’s
underlyingability.
Becausesequencelikelihood(orperplexity)islimitedasaconfidencemeasure,promptingmethods
havebecominganincreasinglypopularalternative. Linetal.[33]introducedthefollowingformats
thatlaythefoundationforrecentwork[51,62]:
2Hereweconsideraccesstoamodel’ssamplesandtoken-levellikelihoodsasblack-box. Somemodelsdonotexpose
likelihoodsdirectly,buttheycanbeapproximatedthroughsampling.
4Zero-Shot Classifier Verbal Fine-tune
Max Softmax Prob Neg. Perplexity
60%
70%
80% 65% 40% 60%
60% 20%
50%
60%
0%
45% 60% 75% 40% 50% 60% 35%40%45%50% 35%40%45%50%
Accuracy Accuracy Accuracy Accuracy
Figure 2: (Left) We compare common uncertainty estimates for multiple-choice questions (max
softmaxprobability)andopen-endedgeneration(perplexity). Whilemaximumsoftmaxprobability
performswellandimproveswiththeabilityofthebasemodel,perplexitydoesnotfollowthesame
pattern. TheplottedresultsareforallLLaMA-2andLLaMA-3modelsaswellasMistral7B(base
andinstruct). (Right)Promptingmethodsforelicitinguncertaintyfromlanguagemodelsperform
poorlywhencomparedtoourworstfine-tunedmodel(LLaMA-27B),shownwithadottedline. ECE
doesn’tappeartoimprovewiththeabilitiesoftheunderlyingmodel,andwhileAUROCdoesshow
smallimprovementswithlargeimprovementsinaccuracy,thegapbetweenzero-shotmethodsand
fine-tuningforuncertaintiesremainslarge. Shadingindicatesa95%bootstrappedconfidenceinterval
ontheregressionfit.
Name Format Confidence
Zero-Shot P(“True”)/
“Question. Answer. True/False: True”
Classifier (P(“True”)+P(“False”))
Verbalized “Question. Answer. Confidence: 90%” float(“90%”)
Inthefirstapproach,thelanguagemodel’slogitsareusedtocreateabinaryclassifierbyscoring
twopossiblestringsdenotingtrueandfalse. Similarly,inKadavathetal.[25],theclassifiertakes
inaslightlymodifiedprompt, “Istheanswercorrect? (a)Yes(b)No”andconfidenceisthen
computedP(“(a)”)/(P(“(a)”)+P(“(b)”)). Inthesecondapproach(alsousedin[51,59]),uncertainty
estimatesaresampledastextandthenconvertedintonumbers. Weprovidetheextendeddetailsin
AppendixB.2.
Theprospectsofcalibrationbylearningtomodelhumanlanguage.Ifweviewlanguagemodeling
asbehaviorcloning[46]onhumanwriting,theoptimaloutcomeisalanguagemodelthatrecapitulates
thefulldistributionofhumanwriterspresentinthetrainingdata. Unfortunately,mosthumansexhibit
poor calibration on tasks they are unfamiliar with [28, 29, 32], and not all pre-training data is
generatedbyexperts. Thereforeitmightbeunreasonablyoptimistictoexpectblack-boxmethodsto
yieldcalibrateduncertaintieswithoutasignificantintervention. Alignmentprocedures(e.g. RLHF)
couldimprovethesituationbypenalizingcasesofpoorcalibration,andtheresultingprocedurewould
beakintofine-tuningongradeddata,whichweexploreinSection5.
Experimentswithopen-sourcemodels. Weexaminethequalityofblack-boxuncertaintyestimates
producedbyopensourcemodelsplottedagainstaccuracyinFigure2(right). WeuseLLaMA-2
[52,53],Mistral[24],andLLaMA-3models,andweevaluateonopen-endedMMLUtohighlight
howthemethodsmightperformina“chat-bot”setting. Becausethesemodelshaveopenweights,
wecanperformapples-to-applescomparisonswithmethodsthattrainthroughthemodeloraccess
hiddenrepresentations. Weseethatpromptingmethodstypicallygivepoorlycalibrateduncertainties
(measuredbyECE)andtheircalibrationdoesnotimproveout-of-the-boxasthebasemodelimproves.
Bycontrast,AUROCdoesimproveslightlywiththepoweroftheunderlyingmodel,buteventhe
bestmodelstilllagsfarbehindtheworsemodelwithfine-tuningforuncertainty.
Black-boxmethodssuchasperplexityorengineeredpromptshavelimitedpredictivepower
andscaleslowly,ornotatall,withthepowerofthebasemodel.
5
CORUA CORUA ECE
CORUA5 How Should We Use Labeled Examples?
OurgoalistoconstructanestimateforPpcorrectq,theprobabilitythatthemodel’sansweriscorrect.
Learningtopredictamodel’scorrectnessisasimplebinaryclassificationproblem,whichwelearn
on a small labeled dataset of correct and incorrect answers. There are many possible ways to
parameterizePpcorrectq,andwestudythreethatvaryintheirnumberoftrainableparametersand
theiruseofprompting:
• Probe: FollowingAzariaandMitchell[4],wetrainasmallfeed-forwardneuralnetworkonthe
lastlayerfeaturesofaLLMthatwasgiventheprompt,question,andproposedanswerasinput.
ThemodeloutputsPpcorrectqwhilekeepingthebaseLLMfrozen.
• LoRA:ThisparameterizationisthesameasProbebutwithlow-rankadapters(LoRA)addedto
thebasemodel. Asaresult,theintermediatelanguagefeaturesofthebasemodelcanbechanged
toimprovethecorrectnessprediction.
• LoRA+Prompt: FollowingKadavathetal.[25],weposeclassifyingcorrectnessasamultiple
choice response with two values, the target tokens “i” and “ii” representing ‘no’ and ‘yes’
respectively. WeperformLoRAfine-tuningonstringswiththisformatting.
Withthesedifferentparameterizations, wecanstudyhowmuchinformationaboutuncertaintyis
alreadycontainedinapre-trainedmodel’sfeatures. Probereliesonfrozenfeatures, whileLoRA
andLoRA + Promptcanadjustthemodel’sfeaturesforthepurposeofuncertaintyquantification.
ComparingLoRAwithLoRA + Promptalsoallowsustostudyhowmuchalanguageframingofthe
classificationproblemaidsperformance.
Datasets. Fortraining,webuildadiversesetofsamplesfromacollectionofbenchmarkdatasets,
similartoinstruction-tuning[56]. Fromthelistof16benchmarkdatasetsinAppendixC.2,weusea
sampledsubsetofsizeapproximately20,000. Weholdout2000data-pointstouseasatemperature
scalingcalibrationset[17].
Training and regularization. We consider three base models–
LLaMA-27b,LLaMA-213b,Mistral7B–andtheirinstruction-tuned Method ECE AUROC
variants. Forfine-tuning,weuse8-bitquantizationandLow-Rank w/oKL 29.9% 70.2%
Adapters(LoRA)[20]. ForLoRA,wekeepthedefaulthyperparame- w/KL 10.8% 71.6%
ters: rankr “8,α“32,anddropoutprobability0.1. Eachtraining
runtakesapproximately1-3GPUdayswith4NVIDIARTX8000 Table 1: Regularization im-
(48GB) GPUs. To keep LoRA and LoRA + Prompt in the neigh- proves calibration. Numbers
borhoodoftheinitialmodel,weintroducearegularizationtermto show the mean over six base
encouragelowdivergencebetweenthepredictionofthefine-tuned models models. See Ap-
modelandthebasemodel(ablationinTable1). pendixC.1fordiscussion.
Samplingbaseline. Weestimatetheuncertaintybyclusteringgenerationsbysemanticsimilarity
[30]. The probability of each cluster becomes the probability assigned to all sequences in that
cluster. Toassignanuncertaintytoaprediction,wefindtheclusterclosesttothepredictionanduse
theprobabilityoftheclusterasouruncertaintyestimate(fulldetailsinAppendixB.1). Theclear
drawbackofthisapproachtouncertaintyestimationisitspoorscaling. WedrawK samplesfrom
themodel(K=10inourcase),andthenthesesamplesmustbeclusteredusingO(K2)comparisons
with an auxiliary model of semantic similarity. Sampling methods are also complicated by their
relationshipwithhyperparameterssuchastemperatureornucleussize. Inthespecialcasewhere
thesamplingparametersarechosentoproducegreedydecoding(e.g. temperaturezero),themodel
willalwaysassignprobablyonetoitsanswer. Whilethisbehaviordoesalignwiththeprobabilityof
generatingtheanswer,itisnotausefulmeasureofconfidence.
Fine-tuningresults. InFigure3(Left)wecompareourthreefine-tunedmodelswithblack-box
uncertaintymethodsonbothmultiplechoiceandopen-endedMMLU.FormultiplechoiceMMLU,we
alsoincludethelanguagemodel’smaxsoftmaxprobabilityasabaseline. Fine-tuningforuncertainty
6Logits Verbal Zero-Shot Classifier Sampling LLaMA-2 LLaMA-2 Mistral
7B Chat 13B Chat 7B Instruct
Probe LoRA LoRA + Prompt
Zero-Shot Classifier Sampling
30%
40%
20% 30%
10 0% % 12 00 0%% % 0.2 0.7
70% 70% 0.1 0.6
60% 60%
50% 50% 102 103 104 102 103 104
MMLU (MC) MMLU (OE) Samples Samples
Figure3: (Left)ECEandAUROConbothmultiplechoice(MC)andopen-ended(OE)MMLU.ECE
isshownaftertemperaturescalingonasmallhold-outset.Supervisedtraining(Probe,LoRA,LoRA +
Prompt)tendstoimprovecalibrationandselectiveprediction. Probingonitsown(Probe)performs
worsethantrainingthroughthefeatureswithalanguageprompt(LoRA + Prompt),especiallyinan
open-endedsetting. Errorbarsshowtwostandarddeviationsoversixbasemodels. Extendedresults
inAppendixD.(Right)EffectofvaryingnumberoflabeleddatapointsonOEMMLU.Inthemost
extremecase,wetrainononly200examples. Overall,performanceincreasesinproportionwiththe
availablelabeleddata,but1000pointsisalmostasvaluableas20,000points. Dottedlinesindicate
theperformanceoftheclassifierandsamplingbaselinesaveragedoverthethreemodelsconsidered.
ShadedregionsshowonestandarddeviationoversubsetsofMMLU.
leads to significant improvements in both ECE and AUROC. While frozen features (Probe) are
sufficienttooutperformbaselinesinmultiplechoiceMMLU,performingwellonopen-endedMMLU
requirestrainingthroughthemodelingandprompting. Surprisingly,whilesamplingmethodscan
yieldgoodcalibration,theirdiscriminativeperformanceisveryweak. Bycontrast,verbalelicitation
isrelativelystrongindiscriminativeperformance,beingonparwithweakerfine-tuningmethods,but
generalhaspoorcalibration,evenaftertemperaturescaling.
How much data do we need? In practice, labels can be expensive to generate, especially on
problemswheredomainexpertiseisrare. Therefore,itwouldbeadvantageousiffine-tuningwith
evenasmallnumberofexamplesissufficientforbuildingagooduncertaintyestimate. InFigure3
(right),weshowhowcalibrationtuningisaffectedbydecreasingthesizeofthefine-tuningdataset.
Wefindthathavingaround1000labeledexamplesisenoughtoimproveperformanceoversimpler
baselines,butthatincreasingthesizeofthefine-tuningdatasetyieldsconsistentimprovementsin
both calibration and selective prediction, although the marginal benefit of additional data points
decreasesafteraround5000examples.
Supervised learning approaches, in which we learn to predict a model’s correctness, can
dramatically outperform baselines with as few as 1000 graded examples. Updating the
featuresofthemodelwithLoRAanduseofalanguagepromptarekeytogoodperformance.
6 When and Why Do These Estimates Generalize?
To derive more understanding of when our estimates generalize, we now investigate distribution
shiftsbetweenthetrainingandevaluationdatasets. Tohaveapracticallyusefultool,wemightdesire
robustnesstothefollowingshifts,amongothers:
Subject matter. Ideally, our uncertainty estimates apply to subjects we have not seen during
training. InFigure4(left),weshowabreakdownofourfine-tuningdatasetusingthesupercategories
fromMMLU(AppendixA.5). WeseethatourdatasetcontainsmuchhigherpercentagesofSTEM
and humanities questions than MMLU and close to no examples from the social sciences (e.g.
government,economics,sociology). Despitethesedifferencesincomposition,uncertaintyestimates
7
ECE
CORUA
ECE
CORUA
ECE CORUASTEM Social Sciences Zero-Shot Classifier Zero-Shot Trained
Humanities Other Probe LoRA + Prompt Answerable
^ (Transfer) ^ (Transfer) 5
40% 15% 3
20% 10 5% % 50% MC OE 1 30%50%70%90% 30%
0% 0% 10% Unanswerable
80% 5 40% 80% 3 1
20% 60% 60%
40% 30%50%70%90%
0% 40% P(correct)
Figure 4: (Left) We compare the composition of the fine-tuning dataset with MMLU. Notably,
although the training dataset contains close to zero examples from social sciences, uncertainty
estimatesfromthemodelperformsimilarlyacrosscategories. (Center)Testingthegeneralizationof
supervisedmethodsbytakingmodelstrainedononesetting(MCQAorOE)andevaluatingthem
ontheothersetting. TheMCQAorOElabelsdenotetheevaluationsetting,withthemethodlabels
indicatewhetherthemodelwastrainedonthesameordifferentsetting. Fine-tuningthroughthe
model’sfeatures(LoRA + Prompt)performsalmostaswellintransferasonin-distributiondata.
Zero-Shot Classifierinvolvesnosupervisedlearningexceptatemperature-scalestepandisa
usefulreferencepoint. Errorbarsshowtwostandarddeviationsoversixfine-tunedmodels. (Right)
Fine-tuningleadstolowerconfidenceonunanswerablequestions,takenfromtheSelfAwaredataset
[60]. Assigninglowconfidencetounanswerablequestionsallowsthemodeltooptoutofresponding.
fromLoRA + Promptperformsimilarlyacrosssupercategories. Wealsoshowtheefficacyofour
modelsatassessingconfidenceonoutofdistributioncodingtasksinAppendixF.
Format. Likeachangeinsubjectmatter,thewayaquestionisposedshouldnotbreaktheuncertainty
estimate. Totesttheeffectofthequestionformatindependentofitssubjectmatter,weapplymodels
fine-tunedonOEMMLUtoMCMMLUandviceversa. InFigure4(center),weseethatfine-tuned
modelsoftenperformbetterthanazero-shotbaselineevenwhentheyarebeingappliedacrossa
distributionshift, thoughtransferfromMCtoOEismorechallengingthanOEtoMC.Probeis
insufficienttogeneralizeeffectivelyfromMCtoOE,buttrainingthroughthefeaturesofthemodel
(LoRA + Prompt)doesgeneralizeeffectively,evenout-performingprobetrainedonOEdata.
Solvability. Eventhoughwefocusonquestionswithasingleknownanswer,wemighthopethat
ourestimatescanbeusedevenwhenaquestionisill-posedordoesnothaveaknownsolution,ideally
returninghighuncertainty. Wegenerateanswers,labels,anduncertaintyestimatesfortheanswerable
andunanswerablequestionsintheSelfAwaredataset[60]usingthesameprocedureasOEMMLU.In
Figure4(right),weplotPpcorrectqfromZero-Shot ClassifierandLoRA + Promptpredicted
foreachanswerableandunanswerablequestion. Notably,calibration-tunedmodelshavecalibrated
probabilitiesfortheanswerablequestionsandassignlowerconfidencetounanswerablequestions
thanblack-boxmethods.
6.1 Whatareuncertaintyestimateslearning?
Language models can generate useful uncertainty estimates after training on a relatively small
numberoflabeledexamples. Howisthispossible? Wehypothesizetwo,potentiallycomplementary
mechanisms: (a)LLMsassessthecorrectnessofananswergivenaquestion,or(b)LLMsrecognize
thatcertaintopicsoftenhaveincorrectanswers. Tounderstandthedifference,let’sexploreauseful
metaphor. ImagineIspeakonlyEnglish,whilemyfriend,Alice,isalinguaphileanddabblesinmany
languages. IhaveaspreadsheetofhowoftenAlicemakesmistakesineachlanguage. Now,whenI
hearAliceattemptingtoconverseinlanguageA,Icanguesshowlikelysheistoerrbyrecognizing
thelanguagefromitssoundandconsultingthespreadsheet. Icandothiswithoutunderstandingthe
languageatall. Alternatively,Icanlearneachlanguage,whichwouldbemorecomplexbutwould
strengthenmypredictions.
8
niarT
%
ULMM
%
ECE
CORUA
ECE
CORUA
ytisneD
ytisneDIncorrect Sampled Probe LoRA + Prompt Probe sBERT
0.8 0.80 LoRA + Prompt OAIEmb
Probe
20% 0.7 0.75 20%
10% 10%
0% 0.6 0.70 0%
70% 0.5 0.65 80%
50% Mistral LLaMA-2 Mistral LLaMA-2 60%
30% Trained On Trained On 40%
Figure 5: (Left) We ablate the correspondence between questions and answers by training LoRA
+ Prompt on a dataset with correctness labels from the model’s generations but with the actual
generationsswappedwithincorrectanswers. Inthiscase,theonlyrelationshipsthatcanbeextracted
bythemodelarebetweenthecorrectnesslabelsandthequestions. Themodeltrainedonincorrect
answersgeneralizessurprisinglywellbutismuchworsethanamodeltrainedontheoriginalanswers.
Errorbarsshowtwostandarddeviationsoverthreeinstruction-tunedmodels. (Center)Wetesthow
wellmodelscanlearntopredictthecorrectnessofadifferentmodel(intermsofAUROC),andwe
findthatmistralmodelsareoftenbetteratestimatingthecorrectnessofLLaMAmodelsthanLLaMA
canontheirowngenerations.(Right)Weshowthatgenericsentenceembeddingscanalsoperformon
parwithfrozenlanguagemodelrepresentations(MMLU-OE),buttrainingthroughamodelismuch
better. sBERTandOAIEmbrefertotrainingaclassifierontopofsBERT[44]orOpenAIsentence
embeddings. ErrorbarsshowtwostandarddeviationsovertasksinMMLU.
Todisentanglethesetwopossibilitiesinoursetting,weperformanadditionalexperiment,inwhich
wereplacethelanguagemodel’sanswersinthefine-tuningdatasetwithincorrectansweroptions. If
alanguagemodelissimplylearningpatternsintheerrorspresentinthetrainingdata,thenwewould
expectthisablationtoperformonparwiththeoriginalmethodbecauseitsufficestolearnpatternsin
thecontentofthequestionandanswerwithoutneedingthetruecausalrelationshipbetweenquestion,
answer,andcorrectnesslabel. TheresultsareshowninFigure5(left). Weseethemodeltrainedon
incorrectanswersperformssurprisinglywell,onparwithaProbemodel,butsignificantlyworse
thanamodeltrainedontheoriginalsampledanswers. Correlatingquestioncontentwitherrorrates
whilemoderatelysuccessfulcannotbeafulldescriptionoftheLoRA + Promptestimates.
Self-knowledge. Lastly,weexaminewhetheralanguagemodelcanbeusedtomodelnotjust
itsownuncertaintiesbuttheuncertaintiesofothermodels. Severalpriorworksarguethatmodels
identifycorrectquestionsbywayofinternalrepresentationsoftruth,whichmightbeuniquetoa
modelevaluatingitsowngenerations[4,9]. InFigure5(right),weshowthat,bycontrast,Mistral7B
actualhasbetterAUROCvalueswhenappliedtoLLaMA-27BthanLLaMA-27Bappliedtoitself.
InFigure5(left),weshowthatsBERT[44]andOpenAIsentenceembeddingsarecompetitivewith
ProbeonbothLLaMA-27BandMistral. Together,theseresultssuggestthatLLMuncertaintiesare
likelynotmodel-specific. Thepracticalupsideofthisinsightisthatonestrongbasemodelcanbe
usedtoestimatetheuncertaintiesofmanyothermodels,evenclosed-sourcemodelsbehindAPIs,
whenasmalllabeleddatasetisavailableorcanbegenerated.
Learned uncertainty estimates generalize to new formatting, subject matter, and even the
generations of other models. This generalization appears to stem not simply from judg-
ing a question’s difficulty based on its subject matter (a short-cut) but also learning the
correspondencebetweenquestionsandcorrectanswers.
7 Does Calibrated Confidence Improve Collaboration with AI
Assistants?
One key motivation for estimating LLM uncertainty is to signal the model’s reliability during
collaborativedecisionmaking. Toexaminehowouruncertaintyestimatescanbeusedinthiscapacity,
9
ECE
CORUA
ledoM
lartsiM
2-AMaLL
lartsiM
2-AMaLL
ECE
CORUA0.06
Disagree 0.08 0.05 0.15 Agree
0.06 0.04
0.10 0.03
0.04
0.02
0.05 0.02
0.01
0.00 0.00 0.00
30 35 40 45 50 40 50 60 70 0 20 40 60 80 100
Model Confidence (%) Model Confidence (%) Model Confidence (%)
(a)Zero-ShotPrompt (b)LoRA+Prompt (c)Random(Control)
Figure6: WecomparethedistributionofLLMconfidence(forMistral7BInstruct)onitsanswers,
andwhethertheusers(N “20pervariant)agreewiththeanswergeneratedbythemodelornot. (a)
Forthezero-shotprompt,wefindthatthemodelprovideslittlesignalsincemostmassissimilarly
clustered. However, (b) improving the calibration of the model reveals an increased reliance on
theLLMformoreconfidentanswers,anddecreasedrelianceforlessconfidentanswers. Evidently,
theusersaresensitivetocalibratedconfidencescores. (c)Forreference,weverifythatuniformly
confidencescoresdonotprovidemeaningfulsignal,renderingusersunabletomodulatetheirdecision
torelyontheLLM.Allvariantsarecomparedatapproximatelythesameaverageparticipantaccuracy.
weperformapreliminaryuserstudy(withN “181participants)inwhichparticipantscompletea
multiplechoiceexamincollaborationwithanLLM(Mistral7BInstruct). Foreachquestion,the
participantisprovidedboththeLLM’spredictionandanuncertaintyestimate,whichcanbefroma
calibratedmethodoranuncalibratedmethod. Wehopetoshowthatusersaremorelikelytoadopt
calibrateduncertaintyscoresaspartoftheirdecisionprocess. Amoredetaileddescriptionofthe
setupofourstudyisavailableinAppendixG.
Peoplearesensitivetoinformedconfidencescores. Figure6showsdensityplotsofthemodel’s
reportedconfidenceandwhethertheuserchosetoagreewiththemodel’sprediction. Wefindthat
participantsaresensitivetotheconfidencescoresandtendtousescoreswhendecidingtoagreeor
disagreewiththemodel’spredictioniftheuncertaintiesarereliable. Ontheotherhand,participants
generally do not modulate their decision to rely on the output of a random confidence baseline
(Figure6(c)),inwhichthedisplayuncertaintyestimateisgenerateduniformlyatrandom. Wesee
thestrongestdiscrepancyinreliancechoiceswhenLoRA + Probeconfidencescoresarepresented,
highlightingthatcalibratedconfidencedoesinfluenceuserbehavior.
WeincludeadditionaldetailsandresultsinAppendixG.Wefindthatconfidencescoreshavethe
biggesteffectonimprovingthelowestperformingusers,ratherthanonaverageaccuracy. However,
thisisapreliminaryresultinthenascentfieldofstudyingLLMuncertaintiesinpracticalcollaborative
decision making with users. We are only still scratching the surface of this question. For more
fine-grainedconclusions,astudyshouldbedevotedtothissubject. Weoutlineseverallimitationsand
futuredirectionsinAppendixG.
Usersaresensitivetoconfidencescoresandusetheirrelativemagnitudetomodulatetheir
decisiontouseanLLM.Lowerperformingusersaremostimprovedbyaccesstoconfidence
scores. However,futureworkisneededtodisentangletheeffectsofcalibrationfromhow
humanschoosetoleverageuncertainties.
8 Discussion
Thereismuchdisagreementabouttheroleofcalibrateduncertaintyinlargelanguagemodels,how
itcanbestbeachieved,andpromiseofblack-boxmethods. Wehopetohaveshedlightonthese
questionsthroughoutthispaper. Incontrasttopriorresults,wefindthatout-of-the-boxuncertainties
fromLLMsareunreliableforopen-endedgenerationandintroduceasuiteoffine-tuningprocedures
that produce calibrated uncertainties with practical generalization properties. In the process, we
10
)%(
noitroporP
)%(
noitroporP
)%(
noitroporPdiscoveredthatfine-tuningissurprisinglysampleefficientanddoesnotseemtorelyonrepresentations
ofcorrectnessspecifictoamodelevaluatingitsowngenerations,allowingestimatorstobeapplied
fromonemodeltoanother. Moreover,wefounditispossible,atleastinthecasesweconsidered,for
calibrateduncertaintiestoberobusttodistributionshifts.
There are many exciting questions for future work. Currently fine-tuning relies on two separate
modelsforquestionansweringanduncertaintyestimation. Ideally,wewantasinglemodelthatcan
generatequestionsanduncertaintywithoutswitchingbetweenmodelweights. Weanticipatethatan
uncertainty-awarepre-trainingoralignmentphasemightbecomeessentialbutimplementingsucha
procedurewhilemaintainingbaselanguagemodelingabilitieswillintroduceachallengingonline
learningproblemwherethecorrectnesslabelsevolveduringtraining.
Beyond improving the safety and usefulness of language models, high quality uncertainties can
also be used in active learning procedures, e.g. for sample-efficient fine-tuning [39], where data
pointsareselectedbasedonthepredictedutilityandthemodel’suncertainty,inordertobalancethe
explore-exploittrade-off. Uncertaintyestimatescanalsobeusedtoimprovefactualityoflanguage
modelsbyincreasingthelikelihoodofgenerationsthatthemodelisconfidentabout(judgeslikelyto
becorrect),forexamplebyusinganalignmentprocedure(e.g. RLHF,DPO)witharewardfunction
thatencouragesconfidentgenerations[50].
Wealsoshowedhowuncertaintyinformationcouldbeusedtoinfluencehumandecisionmaking. In
theend,LLMswillimpactsocietythroughdecisionmaking,andtomakereasonabledecisionswe
needuncertaintyinformation—particularlytoprotectagainstrarebutcostlymistakes.
Acknowledgements
ThisworkissupportedbyNSFCAREERIIS-2145492,NSFCDS&E-MSS2134216,NSFHDR-
2118310,BigHatBiosciences,CapitalOne,andanAmazonResearchAward.
References
[1] JoshAchiam,StevenAdler,SandhiniAgarwal,LamaAhmad,IlgeAkkaya,FlorenciaLeoni
Aleman,DiogoAlmeida,JankoAltenschmidt,SamAltman,ShyamalAnadkat,etal. Gpt-4
technicalreport. arXivpreprintarXiv:2303.08774,2023.
[2] AidaAmini,SaadiaGabriel,ShanchuanLin,RikKoncel-Kedziorski,YejinChoi,andHannaneh
Hajishirzi. MathQA:Towardsinterpretablemathwordproblemsolvingwithoperation-based
formalisms. In Proceedings of the 2019 Conference of the North American Chapter of the
AssociationforComputationalLinguistics: HumanLanguageTechnologies,Volume1(Long
andShortPapers),pages2357–2367.AssociationforComputationalLinguistics,jun2019. doi:
10.18653/v1/N19-1245.
[3] Lora Aroyo and Chris Welty. Truth is a lie: Crowd truth and the seven myths of human
annotation. AIMagazine,36(1):15–24,2015.
[4] AmosAzariaandTomM.Mitchell. Theinternalstateofanllmknowswhenitslying. ArXiv,
abs/2304.13734,2023.
[5] UmangBhatt,ValerieChen,KatherineMCollins,ParameswaranKamalaruban,EmmaKallina,
AdrianWeller,andAmeetTalwalkar. Learningpersonalizeddecisionsupportpolicies. arXiv
preprintarXiv:2304.06701,2023.
[6] ChristopherMBishop. Patternrecognitionandmachinelearning. Springergoogleschola,2:
1122–1128,2006.
[7] YonatanBisk,RowanZellers,RonanLeBras,JianfengGao,andYejinChoi. Piqa: Reasoning
aboutphysicalcommonsenseinnaturallanguage. ArXiv,abs/1911.11641,2019.
11[8] SamuelR.Bowman,GaborAngeli,ChristopherPotts,andChristopherD.Manning. Alarge
annotatedcorpusforlearningnaturallanguageinference. InConferenceonEmpiricalMethods
inNaturalLanguageProcessing,2015.
[9] CollinBurns,Hao-TongYe,DanKlein,andJacobSteinhardt. Discoveringlatentknowledgein
languagemodelswithoutsupervision. ArXiv,abs/2212.03827,2022.
[10] Cheng-HanChiangandHungyiLee. Canlargelanguagemodelsbeanalternativetohuman
evaluations? InAnnualMeetingoftheAssociationforComputationalLinguistics,2023.
[11] ChristopherClark,KentonLee,Ming-WeiChang,TomKwiatkowski,MichaelCollins,and
Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions.
ArXiv,abs/1905.10044,2019.
[12] PeterClark,IsaacCowhey,OrenEtzioni,TusharKhot,AshishSabharwal,CarissaSchoenick,
andOyvindTafjord. Thinkyouhavesolvedquestionanswering? tryarc,theai2reasoning
challenge. ArXiv,abs/1803.05457,2018.
[13] KatherineMaeveCollins,MatthewBarker,MateoEspinosaZarlenga,NaveenRaman,Umang
Bhatt,MatejaJamnik,IliaSucholutsky,AdrianWeller,andKrishnamurthyDvijotham. Human
uncertaintyinconcept-basedaisystems. InProceedingsofthe2023AAAI/ACMConferenceon
AI,Ethics,andSociety,pages869–889,2023.
[14] Marie-CatherineDeMarneffe,MandySimons,andJudithTonhauser. Thecommitmentbank:
Investigatingprojectioninnaturallyoccurringdiscourse. InproceedingsofSinnundBedeutung,
volume23,pages107–124,2019.
[15] TilmannGneitingandAdrianERaftery.Strictlyproperscoringrules,prediction,andestimation.
JournaloftheAmericanstatisticalAssociation,102(477):359–378,2007.
[16] AndrewS.Gordon,ZornitsaKozareva,andMelissaRoemmele. Semeval-2012task7: Choice
ofplausiblealternatives: Anevaluationofcommonsensecausalreasoning. InInternational
WorkshoponSemanticEvaluation,2011.
[17] ChuanGuo,GeoffPleiss,YuSun,andKilianQ.Weinberger. Oncalibrationofmodernneural
networks. InInternationalConferenceonMachineLearning,2017.
[18] DanHendrycks,CollinBurns,StevenBasart,AndyZou,MantasMazeika,DawnXiaodong
Song, and Jacob Steinhardt. Measuring massive multitask language understanding. ArXiv,
abs/2009.03300,2020.
[19] James Hills and Shyamal Anadkat. Using logprobs, Dec 2023. URL https://cookbook.
openai.com/examples/using_logprobs.
[20] J.EdwardHu,YelongShen,PhillipWallis,ZeyuanAllen-Zhu,YuanzhiLi,SheanWang,and
WeizhuChen. Lora: Low-rankadaptationoflargelanguagemodels. ArXiv,abs/2106.09685,
2021.
[21] Lifu Huang, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Cosmos qa: Machine
readingcomprehensionwithcontextualcommonsensereasoning. InConferenceonEmpirical
MethodsinNaturalLanguageProcessing,2019.
[22] NamanJain,KingHan,AlexGu,Wen-DingLi,FanjiaYan,TianjunZhang,SidaWang,Ar-
mandoSolar-Lezama,KoushikSen,andIonStoica. Livecodebench:Holisticandcontamination
freeevaluationoflargelanguagemodelsforcode. arXivpreprintarXiv:2403.07974,2024.
[23] KJMJanssen,KGMMoons,CJKalkman,DEGrobbee,andYVergouwe. Updatingmethods
improvedtheperformanceofaclinicalpredictionmodelinnewpatients. Journalofclinical
epidemiology,61(1):76–86,2008.
12[24] AlbertQiaochuJiang,AlexandreSablayrolles,ArthurMensch,ChrisBamford,DevendraSingh
Chaplot, Diego de Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lu-
cile Saulnier, L’elio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao,
ThibautLavril,ThomasWang,TimothéeLacroix,andWilliamElSayed. Mistral7b. ArXiv,
abs/2310.06825,2023.
[25] SauravKadavath,TomConerly,AmandaAskell,T.J.Henighan,DawnDrain,EthanPerez,
NicholasSchiefer,ZacharyDodds,NovaDasSarma,EliTran-Johnson,ScottJohnston,Sheer
El-Showk,AndyJones,NelsonElhage,TristanHume,AnnaChen,YuntaoBai,SamBowman,
StanislavFort,DeepGanguli,DannyHernandez,JoshJacobson,JohnKernion,ShaunaKravec,
LianeLovitt,KamalNdousse,CatherineOlsson,SamRinger,DarioAmodei,TomB.Brown,
JackClark,NicholasJoseph,BenjaminMann,SamMcCandlish,ChristopherOlah,andJared
Kaplan. LanguageModels(Mostly)KnowWhatTheyKnow. ArXiv,abs/2207.05221,2022.
[26] GideonKeren. Calibrationandprobabilityjudgements: Conceptualandmethodologicalissues.
Actapsychologica,77(3):217–273,1991.
[27] DanielKhashabi,SnigdhaChaturvedi,MichaelRoth,ShyamUpadhyay,andDanRoth.Looking
beyondthesurface: Achallengesetforreadingcomprehensionovermultiplesentences. In
NorthAmericanChapteroftheAssociationforComputationalLinguistics,2018.
[28] JustinKrugerandDavidDunning. Unskilledandunawareofit: howdifficultiesinrecognizing
one’sownincompetenceleadtoinflatedself-assessments. Journalofpersonalityandsocial
psychology,77(6):1121,1999.
[29] JustinKrugerandDavidDunning. Unskilledandunaware–butwhy? areplytokruegerand
mueller(2002). AmericanPsychologicalAssociation,2002.
[30] LorenzKuhn,YarinGal,andSebastianFarquhar. Semanticuncertainty: Linguisticinvariances
foruncertaintyestimationinnaturallanguagegeneration. ArXiv,abs/2302.09664,2023.
[31] XinLiandDanRoth. Learningquestionclassifiers. InInternationalConferenceonComputa-
tionalLinguistics,2002.
[32] SarahLichtenstein,BaruchFischhoff,andLawrenceDPhillips. Calibrationofprobabilities:
Thestateoftheart. InDecisionMakingandChangeinHumanAffairs: Proceedingsofthe
FifthResearchConferenceonSubjectiveProbability,Utility,andDecisionMaking,Darmstadt,
1–4September,1975,pages275–324.Springer,1977.
[33] StephanieC.Lin,JacobHilton,andOwainEvans. Teachingmodelstoexpresstheiruncertainty
inwords. Trans.Mach.Learn.Res.,2022,2022.
[34] Ilya Loshchilov and Frank Hutter. Fixing weight decay regularization in adam. ArXiv,
abs/1711.05101,2017.
[35] DavidJohnCameronMacKay. Informationtheory,inference,andlearningalgorithms. IEEE
TransactionsonInformationTheory,50:2544–2545,2004.
[36] TodorMihaylov,PeterClark,TusharKhot,andAshishSabharwal. Canasuitofarmorconduct
electricity? a new dataset for open book question answering. In Conference on Empirical
MethodsinNaturalLanguageProcessing,2018.
[37] MahdiPakdamanNaeini,GregoryF.Cooper,andMilosHauskrecht. Obtainingwellcalibrated
probabilities using bayesian binning. Proceedings of the ... AAAI Conference on Artificial
Intelligence.AAAIConferenceonArtificialIntelligence,2015:2901–2907,2015.
[38] Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela.
Adversarialnli: Anewbenchmarkfornaturallanguageunderstanding. ArXiv,abs/1910.14599,
2019.
13[39] IanOsband,SeyedMohammadAsghari,BenjaminVanRoy,NatMcAleese,JohnAslanides,
andGeoffreyIrving.Fine-tuninglanguagemodelsviaepistemicneuralnetworks.arXivpreprint
arXiv:2211.01568,2022.
[40] StefanPalanandChristianSchitter. Prolific.ac—asubjectpoolforonlineexperiments. Journal
ofBehavioralandExperimentalFinance,17:22–27,2018.
[41] AdamPaszke,SamGross,FranciscoMassa,AdamLerer,JamesBradbury,GregoryChanan,
Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas
Köpf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,
Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style,
high-performancedeeplearninglibrary. InNeuralInformationProcessingSystems,2019.
[42] JohnPlattetal.Probabilisticoutputsforsupportvectormachinesandcomparisonstoregularized
likelihoodmethods. Advancesinlargemarginclassifiers,10(3):61–74,1999.
[43] BenjaminPlaut,KhanhNguyen,andTuTrinh. Softmaxprobabilities(mostly)predictlarge
languagemodelcorrectnessonmultiple-choiceq&a. arXivpreprintarXiv:2402.13213,2024.
[44] NilsReimersandIrynaGurevych. Sentence-bert: Sentenceembeddingsusingsiamesebert-
networks. arXivpreprintarXiv:1908.10084,2019.
[45] KeisukeSakaguchi,RonanLeBras,ChandraBhagavatula,andYejinChoi. Winogrande: An
adversarialwinogradschemachallengeatscale. ArXiv,abs/1907.10641,2019.
[46] Stefan Schaal. Learning from demonstration. Advances in neural information processing
systems,9,1996.
[47] AlonTalmor,JonathanHerzig,NicholasLourie,andJonathanBerant. Commonsenseqa: A
question answering challenge targeting commonsense knowledge. ArXiv, abs/1811.00937,
2019.
[48] GeminiTeam. Gemini: Afamilyofhighlycapablemultimodalmodels,2024.
[49] ThomasCTerwilliger,DorotheeLiebschner,TristanICroll,ChristopherJWilliams,AirlieJ
McCoy, Billy K Poon, Pavel V Afonine, Robert D Oeffner, Jane S Richardson, Randy J
Read,etal. Alphafoldpredictionsarevaluablehypothesesandacceleratebutdonotreplace
experimentalstructuredetermination. NatureMethods,pages1–7,2023.
[50] KatherineTian,EricMitchell,HuaxiuYao,ChristopherDManning,andChelseaFinn. Fine-
tuninglanguagemodelsforfactuality. arXivpreprintarXiv:2311.08401,2023.
[51] Katherine Tian, Eric Mitchell, Allan Zhou, Archit Sharma, Rafael Rafailov, Huaxiu Yao,
ChelseaFinn, andChristopherDManning. Justaskforcalibration: Strategiesforeliciting
calibratedconfidencescoresfromlanguagemodelsfine-tunedwithhumanfeedback. arXiv
preprintarXiv:2305.14975,2023.
[52] HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,Timo-
théeLacroix,BaptisteRozière,NamanGoyal,EricHambro,FaisalAzhar,AurelienRodriguez,
ArmandJoulin,EdouardGrave,andGuillaumeLample. Llama: Openandefficientfoundation
languagemodels. ArXiv,abs/2302.13971,2023.
[53] HugoTouvron,LouisMartin,KevinR.Stone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,
NikolayBashlykov,SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,DanielM.Bikel,Lukas
Blecher,CristianCantonFerrer,MoyaChen,GuillemCucurull,DavidEsiobu,JudeFernandes,
JeremyFu,WenyinFu,BrianFuller,CynthiaGao,VedanujGoswami,NamanGoyal,AnthonyS.
Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian
Khabsa,IsabelM.Kloumann,A.V.Korenev,PunitSinghKoura,Marie-AnneLachaux,Thibaut
Lavril,JenyaLee,DianaLiskovich,YinghaiLu,YuningMao,XavierMartinet,TodorMihaylov,
PushkarMishra,IgorMolybog,YixinNie,AndrewPoulton,JeremyReizenstein,RashiRungta,
KalyanSaladi,AlanSchelten,RuanSilva,EricMichaelSmith,R.Subramanian,XiaTan,Binh
14Tang,RossTaylor,AdinaWilliams,JianXiangKuan,PuxinXu,ZhengxuYan,IliyanZarov,
YuchenZhang,AngelaFan,MelanieKambadur,SharanNarang,AurelienRodriguez,Robert
Stojnic,SergeyEdunov,andThomasScialom. Llama2: Openfoundationandfine-tunedchat
models. ArXiv,abs/2307.09288,2023.
[54] AlexandraNUma,TommasoFornaciari,DirkHovy,SilviuPaun,BarbaraPlank,andMassimo
Poesio. Learningfromdisagreement: Asurvey. JournalofArtificialIntelligenceResearch,72:
1385–1470,2021.
[55] KailasVodrahalli,TobiasGerstenberg,andJamesYZou. Uncalibratedmodelscanimprove
human-aicollaboration. AdvancesinNeuralInformationProcessingSystems,35:4004–4016,
2022.
[56] JasonWei,MaartenBosma,VincentZhao,KelvinGuu,AdamsWeiYu,BrianLester,NanDu,
AndrewM.Dai,andQuocV.Le. Finetunedlanguagemodelsarezero-shotlearners. ArXiv,
abs/2109.01652,2021.
[57] Johannes Welbl, Nelson F. Liu, and Matt Gardner. Crowdsourcing multiple choice science
questions. ArXiv,abs/1707.06209,2017.
[58] ThomasWolf,LysandreDebut,VictorSanh,JulienChaumond,ClementDelangue,Anthony
Moi, PierricCistac, TimRault, RémiLouf, MorganFuntowicz, JoeDavison, SamShleifer,
PatrickvonPlaten,ClaraMa,YacineJernite,JulienPlu,CanwenXu,TevenLeScao,Sylvain
Gugger,MariamaDrame,QuentinLhoest,andAlexanderM.Rush. Transformers: State-of-the-
artnaturallanguageprocessing. InProceedingsofthe2020ConferenceonEmpiricalMethods
inNaturalLanguageProcessing: SystemDemonstrations,pages38–45,Online,October2020.
AssociationforComputationalLinguistics. URLhttps://www.aclweb.org/anthology/
2020.emnlp-demos.6.
[59] MiaoXiong, ZhiyuanHu, XinyangLu, YifeiLi, JieFu, JunxianHe, andBryanHooi. Can
llmsexpresstheiruncertainty? anempiricalevaluationofconfidenceelicitationinllms. ArXiv,
abs/2306.13063,2023.
[60] ZhangyueYin,QiushiSun,QipengGuo,JiawenWu,XipengQiu,andXuanjingHuang. Do
large language models know what they don’t know? In Findings of the Association for
ComputationalLinguistics: ACL2023,pages8653–8665,Toronto,Canada,2023.Association
forComputationalLinguistics.
[61] RowanZellers,AriHoltzman,YonatanBisk,AliFarhadi,andYejinChoi. Hellaswag: Cana
machinereallyfinishyoursentence? InAnnualMeetingoftheAssociationforComputational
Linguistics,2019.
[62] HanningZhang,ShizheDiao,YongLin,YiRFung,QingLian,XingyaoWang,YangyiChen,
Heng Ji, and Tong Zhang. R-tuning: Teaching large language models to refuse unknown
questions. arXivpreprintarXiv:2311.09677,2023.
15Appendix
Table of Contents
A EvaluationMethods 16
A.1 EvaluatingCorrectness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
A.2 Grading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
A.3 ComparisonofGradingTechniques . . . . . . . . . . . . . . . . . . . . . . . . 17
A.4 Metrics. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
A.5 MMLUSupercategoryClassifier . . . . . . . . . . . . . . . . . . . . . . . . . 18
B BaselineMethods 18
B.1 SamplingMethods. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
B.2 VerbalElicitation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
C Fine-tuningMethod 20
C.1 RegularizationTerm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
C.2 TrainingData . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
C.3 TrainingHyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
D ExtendedMMLUResults 21
E ConfidenceasaFunctionofTargetLength 21
F GeneralizationtoCodingTasks 21
G UserStudies 21
G.1 AdditionalDetailsonSetup . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
G.2 Importantconsiderations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
G.3 ExtendedResults . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
G.4 InterfaceandInstructions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
H BroaderImpactandImplications 23
A Evaluation Methods
A.1 EvaluatingCorrectness
ForagivenquestionwithknownandgeneratedanswerspQ,A,Aˆ qthecorrectnessC isTrueifthe
generatedanswerAˆmatchesthegroundtruthanswerA. Formultiple-choicequestion-answering,the
matchingprocessonlyinvolvescheckingthefirsttokengeneratedviagreedydecoding.
Foropen-endedevaluations,determiningiftheanswergiveniscorrectismorecomplex. Onesimple
approachistocheckifthegroundtruthanswerAappearsasasubstringofanswerAˆ. However,this
doesnotcapturerephrasingsthatmaybeessentiallyequivalent-suchas"NYC"for"NewYorkCity,"
or"Daoism"and"Taoism."Conversely,italsohasthepotentialtobeover-generousifthemodel
isparticularlyverboseandemitsmanyincorrectanswersalongwiththecorrectstring. Giventhe
difficultyinvolvedinwritingarule-basedmethodforevaluatingopen-endedanswercorrectness,we
useinsteadastrongauxiliarylanguagemodeltoevaluatecorrectness. Theauxiliarylanguagemodel
isshownthequeryQ,thegroundtruthanswerA,andthemodel’soutputAˆ,andispromptedtograde
16theanswerwhilsttoleratingnuance. Forfulldetailsofthepromptusedsee(fig.7). Inthispaperwe
utiliseGPT3.5Turboastheauxiliarygradingmodel. Weconductacomparisonofhumangrading,
substringgrading,andGPT3.5TurbogradingonselectsubsetsofMMLUinappendixA.3. Wefind
thathumansandGPT3.5Turbohavemuchgreateragreementthanhumansandthesubstringmethod.
A.2 Grading
DatasetConstruction. Toperformcalibration-tuning(CT),weneedtuplespQ,A,Aˆ,Cq,answers
fromalanguagemodelthathavebeengradedforcorrectness. Whencalibration-tuningonmultiple
choicequestions,wecanuseanexactstringmatchtogenerateC. Togradeopen-endedanswers,we
useastronglanguagemodelandgradingpromptGinstead(fig.7):
• G: apromptusedforgradinganswersAˆwithA.
Comparedtoalternativeslikeexactmatch,languagemodelgradingisinsensitivetore-phrasings
thatareequivalentinmeaning-suchas“NYC"and“NewYorkCity,"or“Daoism"and“Taoism".
LLMgradingcanalsopenalizeanswersthatareoverlyverboseoruseadifferentmeaningofthe
sameword,potentiallycontainingincorrectanswersalongwiththecorrectstring. Forexample,ifthe
questionis“What’sitcalledwhenyoumovequicklybyfootandbothfeetaren’talwaystouchingthe
ground?” andtheLLMresponseis“Abankrun",thegradershouldbeabletodistinguishthatthisis
semanticallydissimilartothetrueanswer“run”.
In this paper, we utilize GPT 3.5 Turbo as the auxiliary grading model. When comparing many
possiblegradingmethodsonsubsetsofMMLU,wefindthatGPT3.5Turbohashighagreementwith
humanswhilebeingcostefficient(appendixA.3).
GradingpromptpGq
Theproblemis:Q
Thecorrectansweris:A
Astudentsubmitted:Aˆ
The student’s answer must be correct and
specificbutnotovercomplete(forexample,ifthey
provide two different answers, they did not get
thequestionright).However,smalldifferencesin
formattingshouldnotbepenalized(forexample,
‘New York City’ is equivalent to ‘NYC’). Did
the student provide an equivalent answer to the
groundtruth?Pleaseansweryesornowithoutany
explanation:C</s>
Figure7: Foropen-endedgeneration,wecalculatetheground-truthcorrectnessC usingaLLManda
gradingprompt(G). Thetoken</s>isanend-of-sentencetoken. Bluetextisincludedintheloss
functionwhencalibration-tuning.
A.3 ComparisonofGradingTechniques
WeconductedananalysisofthemethodsoutlinedinappendixA.1foropen-endedevaluation. First,
thebaseLLaMA-213b-chatmodelwaspromptedwithquestionsfromthefollowingtestsubsetsof
MMLU:WorldReligions,Philosophy,Anatomy,HighSchoolChemistryandElementarySchool
Math. The questions were stripped of their multiple-choice options before being supplied to the
model.
17Aresponsewasgeneratedbythemodelviagreedydecodingandthisresponsewascomparedtothe
groundtruthanswer. ThegradingmethodstestedwereHuman,SubstringMatch,GPT3.5Turbo,
andGPT4.
Thehumans(asubsetofourauthors)weretaskedtojudgeifthemodelresponsewasessentially
equivalenttothegroundtruth. Forsubstringmatch,equivalencewasdeterminedbysimplychecking
whether the ground truth answer existed as a substring within the model response. For GPT 3.5
TurboandGPT4,themodelsweresuppliedwiththequestion,thegroundtruth,andthebasemodel
response,aswellasapromptindicatingtheyshoulddetermineessentialequivalence-seefig.7.
MMLUSUBSET SUBSTRINGMATCH GPT3.5 GPT4
WORLDRELIGIONS 21.6% 6.4% 1.8%
PHILOSOPHY 22.8% 2.3% 14.5%
ANATOMY 13.3% 14.8% 1.5%
CHEMISTRY 13.8% 5.4% 1.0%
MATH 12.4% 14.8% 3.7%
AVERAGE 16.8% 8.7% 4.5%
Table2: Absolutedifferencesinaccuracy%forthedifferentgradingmethodsvshumanestimated
accuracy. Alowervaluecorrespondstoanaccuracyestimateclosertothehumanestimate.
Werecordedthebinarydecisiononcorrectnessforeachqueryandresponsebyeachofthegrading
methodsabove. Takingthehumanscoresasthegoldstandardofcorrectness,wecomputedthemodel
accuracyforeachsubset,andthenderivedtheabsoluteerrorinestimateofmodelaccuracybyeach
oftheothergradingmethods. Thesearedisplayedintable2. WeseethatGPT4isabetterestimator
ofhuman-judgedcorrectnessthanGPT3.5Turbo,whichinturnissubstantiallybetterthansubstring
match;althoughthereissomevarianceonaper-subsetbasis. Forexpediencyofprocessingtimeand
cost,wechosetouseGPT3.5Turbointhispaper.
A.4 Metrics
ECE Given N samples and B equally-spaced bins b , examples are assigned to bins based on
j ř
theconfidenceofthemodel,andECEisestimatedasEz CE“ B |bj||confpb q´accpb q|where
j“1 N j j
confpb qistheaverageconfidenceofsamplesinbinb ,accpb qistheaccuracywithinthebin,and
j j j
|b |isthenumberofsamplesassignedtobinj. Inourexperimentsconf isequivalenttoPpcorrectq.
j
A.5 MMLUSupercategoryClassifier
To understand the impact of the subject matter of the training data on generalization, we follow
the prescription of Hendrycks et al. [18] and categorize each of the 57 tasks into one of four
supercategories - Humanities, STEM, Social Sciences, and Other. Since we do not have such a
categorizationforthetrainingset,wemustestimatetheirproportions.
First,weusetheOpenAIembeddings(dimension1536)oftheMMLUsampleswiththeirground
truthsupercategoriestotrainalinear4-wayclassifierwith10samplesfromeachofthe57tasks.
WeuseAdamW[34]withlearningrate1e-3andweightdecay1e-2. Thisclassifieristhenusedto
estimatethecategoriesofeachsampleinthetrainingsetusedforfine-tuning. Subsequently, the
breakdownofresultsinfig.4(Left)follows.
B Baseline Methods
B.1 SamplingMethods
Weusetwobaselineswhichobtainanestimateofcertaintybysamplingthesameanswersn“10
timesandthenestimatingtheproportionofsampledanswersthatagreewiththegreedilydecoded
18“main"answer. Thereareseveralcriticaldownsidestotheseapproaches: (i)theuncertaintyhere
dependsonthesamplingparameters—forexample,inthelimitwherethesamplingconvergestomere
greedydecoding,theLLMwillproducenidenticalsamples,andthereforethecertaintywillalways
be1—(ii)theseapproachesrequireOpnqanswergenerationstoprovideacertaintyestimatefora
singlegeneration. Theintensecomputationalrestrictionpreventsusfromeasilysearchingthespace
ofsamplingparametersfortheoptimalset,sowechooseparametersarbitrarily;herewesamplewith
top_p“0.95.
Counting Inthisbaseline,eachsampledansweriscomparedtothegreedyanswerbypromptingan
expertLLMwithbothanswersandaskingittojudgetheirequivalence. Theproportionofsamples
thatareequivalenttothegreedyansweristhecertaintyestimate. ThisbaselineissimilartoLabel
prob[51]; ourmethoddiffersbynotchoosingtheargmaxsemanticgroupasthefinalprediction,
butinsteadusingthegreedydecodeforthefinalprediction, soastomaintainthesameaccuracy
performanceasouruncertaintyquerymethod. Thismet
Likelihoodaccumulation Inthisbaseline,weadduplikelihoodsofsampledanswerstoestimate
the mass associated with the predicted answer. We begin by prompting an expert LLM in order
tofindwhichsampledanswersareequivalenttothegreedyanswer—likeinthecountingbaseline.
Inthismethod,thecertaintyestimateisproducedbyaddingthelength-normalizedlikelihoodsof
thosesampledanswersequivalenttothegreedyanswer,anddividingthisquantitybythesumofall
sampledanswers’length-normalizedlikelihoods. Thisprocedureofaddinglikelihoodsofsamples
inordertoestimatethelikelihoodofanequivalenceclassissimilartothatusedby[30],although
theydonotuseitforcertaintyestimatesbutinsteadtoproduceentropyscores. Inpractice,thescores
producedbythesetwomethodsareactuallyverysimilar—sowereportonlylikelihoodaccumulation
numbersinthemaintext.
B.2 VerbalElicitation
AlthoughTianetal.[51]introduceseveralstrategiesforprompting,involvingmultipleguessesor
multiplestagesofinterleavingpromptingandgeneration,wedidnotfindthatanystrategyconsistently
outperformedanyother. ThisfindingwasconsistentwiththeresultsofXiongetal.[59]. Ultimately,
forconvenience,weadoptedatwostagestrategywithasingleguessbecauseitcanbeusedintandem
withloggeddatasetsofgeneratedanswerspermodel.
The exact prompt we used is essentially the same at in [51], but with small modifications that
improvedtherateofcorrectlyformattedresponses:
“Providetheprobabilitythatyouransweriscorrect. GiveONLYtheprobability,noother
wordsorexplanation.
Forexample:
Probability: <theprobabilitybetween0.0and1.0thatyourguessiscorrect,withoutany
extracommentarywhatsoever;justtheprobability!>
Includeprobabilityfortheanswerbelow: Probability:”
Verbalelicitationmethodstypicallyoutputcomplexstringscontainingbothanswersandassociated
probabilities. This means that if any element of parsing fails, it can be challenging to construct
partialresults. Thiseffecttendstodiminishwhenusinglargemodels,whicharemoreresponsiveto
zero-shotprompting.
ParsingDetails Theoriginalverbalelicitationpromptsaregivenintheappendixof[51]. However,
itisnotclearhowtheoriginalauthorsdecidetoparseanswersfromthegenerationsandhowfailure
toparseishandled. Whenwefailtoparsetheguessfromthegenerationwereturnanemptystring
andassociatedprobability0.5. Whenwefailtoparseaprobability,wealsoreturnprobability0.5.
19Forversionswithmultipleguesses,ifanypartoftheparsingprocessesfailsinanambiguousway,we
defaultbacktoanemptystringfortheanswerand0.5fortheprobability. Theonlyunambiguous
casesarethosewhichexplicitsucceedinthegeneratingavalidguessandprobabilityinthefirstcase
butnotsubsequentcases. Inthisscenario,wedefaulttousingthesuccessfullyparsefirstguessand
associatedprobability.
C Fine-tuning Method
C.1 RegularizationTerm
Tokeepthecalibration-tunedparametersθwithintheneighborhoodoftheinitialparameters,θ ,we
0
usearegularizationtermthatpenalizesthedivergencebetweentheoriginalsamplingdistributionand
thecalibration-tunedmodelonthetargetsequenceA,yieldingregularizationRpθ;θ q,whichweuse
0
withweightingparameterκ.
Specifically, let p be the language modeling distribution of the language model we
θ0
wish to calibration-tune, and q be the corresponding language modeling distribution
θ
as a consequence of calibration-tuning. We then use the Jensen-Shannon Divergence
JSDpp ∥q q [35] between the two language modeling distributions as the regularizer, where
θ0 θ
JSDpp∥qq≜1{2pKLpp∥mq`KLpq ∥mqq, where m ≜ 1{2pp`qq is the mixture distribution.
JSDregularizationisappliedonlytothelogitscorrespondingtothetargetsequenceA.
WenotethatusingeitherdirectionofKL-divergence,i.e. theforward-KLKLpp ∥q qorreverse-
θ0 θ
KLKLpq ∥ p qwasinsufficientforoptimalperformancewithcalibrationtuning. Theforward
θ θ0
KL-divergenceencouragesazero-avoidingbehaviorsuchthatthemassofq isspreadacrossmultiple
θ
modesofp tominimizetheKL-divergencetoavoidassigningnomasstoregionsoftheprobability
θ0
space.Tothecontrary,thereverseKL-divergenceencouragesazero-forcingbehaviorsuchtheq only
θ
needstocoveranyonemodeofp [6]. Itisnotnecessarilyobviouswhichoneofthesebehaviors
θ0
oneshouldpreferforthespecificcaseoflargelanguagemodels. Therefore,asapracticalchoice,we
picktheonethatprovidesusthemostperformantcalibration-tunedmodel.
C.2 TrainingData
Wereservethefollowingdatasetsfortraining.
• AI2ReasoningChallenge(ARC)[12],
• BooleanQuestions(BoolQ)[11],
• CommonsenseQA[47],
• CosmosQA[21],
• HellaSwag[61],
• MathQA[2],
• RecognizingTextualEntailment(RTE/SNLI)[8],
• AdversarialNLI[38],
• OpenBookQA[36],
• PIQA[7],
• SciQ[57],
• TheCommitmentBank(CB)[14],
• Multi-SentenceReadingComprehension(MultiRC)[27],
• ChoiceofPlausibleAlternatives(CoPA)[16],
20• TREC[31],
• AdversarialWinograd(Winogrande)[45].
C.3 TrainingHyperparameters
WeuseHuggingFaceTransformers[58]andPyTorch[41]fortheimplementationofthesemodels.
Forallourexperiments,weusetheAdamWoptimizer[34]withalearningrateof10´4,acosine
decayschedule,andeffectivebatchsizeM “32. ThetrainingrunsforG“10000withaninitial
linearwarmupschedulefor1000steps.
D Extended MMLU Results
WereportthebreakdownofuncertaintyqueryaccuracyandECEonallMMLUtasksinfigs.8to11.
E Confidence as a Function of Target Length
Aswenotedwhenmotivatingcalibrationtuning,onelimitationofsequence-levelprobabilitiesis
theirintrinsicconnectiontosequencelength. Theprobabilityofasequencedecreaseswithincreasing
length,regardlessofthecorrectnessoftheresponse. Bycontrast,wewouldn’texpectconcept-level
probabilitiestohaveanydiscerniblerelationshipwithsequencelength. InappendixE,weshowthere
isnoconsistentrelationshipbetweentheconfidenceestimatedbythecalibration-tunedmodeland
targetsequencelengthonMMLUtasks.
A key limitation of using token likelihoods is that they necessarily decay with the length of the
generation. Infigs.12to14,weconfirmoverallsubsetsofMMLUthatthelengthofthetargetdoes
notstronglycorrelatewiththeconfidenceassociatedwiththetargets. Thisbehaviorisanessential
ingredienttowardsaneffectiveconfidenceestimationinpractice,suchthatlongersequencesarenot
penalizedinconfidencedespitebeingcorrect.
F Generalization to Coding Tasks
Because there are no coding tasks in our training dataset, we can use a coding competition task
introduced in LiveCodeBench [22] to assess how well finetuned uncertainty estimation methods
performoncompletelyoutofdistributiontasks.
Toconducttheanalysisintable3,weevaluateseveralbasemodelsonthe62LeetCodeeasyquestions
fromthelivecodebench_generation_litetask. WeaskingforthemodeltowriteaPythonsolution
andgradethesolutionusingtestcases(markingitascorrectiffitpassesalltestcases). Wethen
applyLora + PromptandZero-Shot Classifieruncertaintyestimationmethods—withthese
methodsonlyusingtraining/temperaturescalingdatafromourmaindatasetmixturewhichnotably
doesnotincludeanycodingtasksappendixC.2. Accuracyisshowntocontextualizethemodel’s
overalllevelofperformanceonthetask. OnMistral-7B,thebestperformingmodelonthecoding
task, the supervised Lora + Prompt approach dramatically improves calibration and selective
predictionascomparedtoZero-Shot Classifier;ontheworse-performingMistral-7B-Instruct
andLLaMa-2-7B,selectivepredictionimprovesbutcalibrationslightlydegrades.
G User Studies
G.1 AdditionalDetailsonSetup
StimuliandParticipantSelection Wecloselyfollowedthesetupof[5]. Weusedthesame180
MMLU questions from which were pre-batched into three sets of 60 MMLU questions. Within
21Model Method Acc ECE AUROC
Zero-ShotClassifier 3.2% 41.0% 56.9%
LLaMa-2-7B
Lora+Prompt 3.2% 46.4% 80.0%
Zero-ShotClassifier 27.4% 70.2% 66.2%
Mistral-7B
Lora+Prompt 27.4% 21.4% 85.1%
Zero-ShotClassifier 21.0% 52.7% 47.1%
Mistral-7B-Instruct
Lora+Prompt 21.0% 56.1% 70.2%
Table3: ECEandAUROConlivecodebench_generation_lite(LeetCodeeasysubset). ECEisshown
aftertemperaturescalingonasmallhold-outsetoftheoriginaldatasetmixtureappendixC.2. Accis
taskaccuracy(proportionofcodingsolutionsthatarecorrect). Supervisedtraining(LoRA+Prompt)
seemstoalwaysimproveselectiveprediction,althoughsupervisedtrainingonlyheavilyimproves
calibrationforMistral-7Bandinfactslightlydegradescalibrationforthetwoothermodels.
eachvariant,werandomlyassignedparticipantstooneofthethreebatches. Intotal,werecruit181
participants(20pervariant3). Allparticipantswererecruitedthroughthecrowdsourcingplatform
Prolific[40];werestrictourparticipantpooltothosebasedintheUnitedStateswhospeakEnglish
asafirstlanguage.
Compensation Participantsweretoldthatthestudywouldtakeapproximately30minutesand
werepaidatabaserateof$9/hrandinformedthattheywouldreceiveanoptionalbonusupto$10
foransweringquestionscorrectly. Weappliedthebonustoallparticipants.
LLMAnswersandUncertaintyElicitation Bhattetal.originallyusedGPT-3.5astheirLLM.
While at first, we explored user performance when provided with confidence scores modulated
overtheoriginalGPT-3.5responsesthattheauthorshadcollected, theauthorshadfilteredLLM
performancetoensuretheLLMachievedhighperformanceonbiology,computerscience,andforeign
policyandpoorperformanceonmathematics. Assuch,wenoticedthatparticipantsoverwhelmingly
uptooktheLLM’sanswer(whichwasrationalbehaviour,giventhemodel’shighperformance). To
exploreamorenuancedperformanceprofile,weregeneratedLLManswersusingMistral7BInstruct
viagreedydecoding. WethengeneratedconfidencescoresontopoftheLLMresponses. Forour
randombaseline,wesampleaconfidencescoreuniformlybetween0and100%foreachquestion.
G.2 Importantconsiderations
Therearemanyreasonstoheedcautionininterpretingourresultsasdefinitiveindicationsoftheutility
ofdisplayingconfidencetousersinLLMassistivesettings. Inparticular: (i)usersarepresentedwith
feedbackaftereachtrialasin[5]–assuch,theycandetermine(potentiallyrapidly)whetherornota
modelisreliable,evenwithoutconfidencescores. However,inpracticalsettingsusersmaynotknow
whetherornotthemodelwastrulycorrectandthereforeconfidencescorescouldhaveanevenlarger
impact;(ii)MMLUquestionscanbechallengingfornon-experts–weseethebiggestdifferencesin
performancefortheno-LLMvs.any-LLM-assistancecondition.Wemayseeawiderrangeofreliance
behaviorsinsettingswhereinpeoplehavemoreconfidenceintheirownabilities;(iii)wepresent
userswithnumericconfidence;however,humansarenotalwaysabletoreliablyprocessconfidence
estimatesnorappropriatelycalibrateuncertaintyestimatesthemselves[26,55,13,32]. Itmaybethat
alternatemodesofcommunicatingconfidenceimproveusers’abilitytoappropriatelyleveragethe
confidencescoresintheirdecisionmakingprocess. Weseetargetedexplorationofeachcomponent
throughinterdisciplinarycollaborationacrossAI,behavioralscience,andhuman-computerinteraction
asripeforfuturework.
3Withtheexceptionofoneextraparticipantduetorandombatchingallocationeffects.
22G.3 ExtendedResults
Task Accuracy and Reliance Sensibility We depict average user task accuracy and reliance
sensibilityacrossvariantsinFigure15. WefollowBhattetal.incomputingreliancesensibilityas
theproportionoftimestheuserappropriatelysidedwiththemodelpredictionwhenthemodelwas
correctanddidnotrespondwiththemodel’spredictionwhenthemodelwasincorrect.
Wedepictper-topicaccuracy,withtheLLM’saverageperformanceinFigure16.
GPT-3.5ConfidenceGeneralization Asnoted,weranvariantsusingthesameGPT-3.5generations
as[5]. Weshowaggregateandper-topicaccuracyinfig.17,aswellasreliancesensibilityinfig.18.
FreeformUserResponses Wepermitteduserstoprovidefreeformresponsesattheendofthe
study. Someusersweresensitivetoconfidencescoresbeingreportedandcameupwiththeirown
heuristicsforwhethertorelyonthemodel’soutput. Weincludeasamplingofcommentsacross
confidencevariants:
• “ifithadaconfidenceoflessthan50%itmademeveryskeptical.”
• “Themodels´confidenceindeedhelpedmechooseandselectmyanswerasItrustedinthem
mostofthetime.”
• “Ididn´treallyrelyontheconfidencelevel. IfIhad0confidenceintheanswermyselfIrelied
ontheAIregardless.”
• “if the models confidence fell below 45 I decided to investigate it myself by remembering
piecesofinformation.andalsoreasoningthequestion.Ifitwasabove45Iwouldautomatically
agreetoitspredictionbutthereweresomefewcasesIchallengediteventhoughitwasabove
45”
• “AtfirstIwashesistanttotrustthemodelmuchbecauseofthelowerconfidencelevelsbutIstill
trusteditenoughontopicsIstruggledwith. Asitwenton,Iwascomfortablewithconfidence
levelsabove40.”
• “Ifthemodels´ confidencewaslowandIthoughtIknewtheanswer(anditwasdifferent)I
chosemyanswer”
G.4 InterfaceandInstructions
WeshowasampleinterfaceofourextensionofModistewithuserconfidenceinFigure19,and
presentthethefullsetofinstructionsprovidedtousersinFigures20and21. Note,fortheLLM-only
andno-LLMconditions, wefollowedtheinstructiontextfrom[5]directly, i.e., participantswho
sawonlytheLLMdidnotseetheinstructionpageaboutmodelconfidence,andparticipantsinthe
“No-LLM”variantwerenotinstructedaboutanymodelvariantandwerejustinstructedtoanswer
the questions as best as they could by themselves. Participants also responded to a post survey
questionarreaftercompletingtheuserstudy,whichwedepictinFigure22.
H Broader Impact and Implications
The goal of this work is to make LLM outputs have better confidence values associated with
them. Withsuccessful,calibratedconfidencevalues,themachinesystemsultimatelybecomemore
interpretableandtrustworthybyauser[23]. Whenappliedcorrectly,ouradvancementswillhelp
usersbeabletomakedecisionsbasedoffofLLMoutputsinamoreinformedway. Similarexamples
inotherdomains,likeAlphaFold[49],haveshownhowwell-calibratedconfidencescorescanbe
usefulincomplexdecision-makingdomains. OurhopeistoreplicatethosebroadfindingsinLLMs.
23We acknowledge the ongoing debate over the appropriateness, limitations, and harms of LLMs.
Wedohighlightthatthedevelopmentofmoreconfident,interpretable,andtrustworthyLLMscan
leadtocontinuedtechno-solutionisminunintendedapplications. Specifically,wehighlightthatour
workislimitedtouse-caseswithfact-basedquestions. Manyapplicationsoftext-basedLLMsare
generative,meaningthatthereisnowayforourparadigmtobeappliedappropriately,andtheuseof
aconfidencesfromcalibration-tunedmodelscouldbemisleadingordamagingwithoutchecksand
guardrails. Additionally,evenwithinthefact-basedparadigm,whatistruecanbesubjective,with
groundtruthinmachinelearningbeingacontestedtopic[3,54].
The philosophical debate on these topics is beyond the expertise of the authors; nonetheless, we
believethattheongoingdebateovertheappropriatenessofLLMsshouldbeconsideredincontext
withthebenefitsofourapproachinmakingLLMsmoreinterpretableanduseful.
24LLaMA-2 LLaMA-2 LLaMA-2 LLaMA-2 Mistral Mistral
7B 7B Chat 13B 13B Chat 7B 7B Instruct
abstract_algebra
anatomy
astronomy
business_ethics
clinical_knowledge
college_biology
college_chemistry
college_computer_science
college_mathematics
college_medicine
college_physics
computer_security
conceptual_physics
econometrics
electrical_engineering
elementary_mathematics
formal_logic
global_facts
high_school_biology
high_school_chemistry
high_school_computer_science
high_school_european_history
high_school_geography
high_school_government_and_politics
high_school_macroeconomics
high_school_mathematics
high_school_microeconomics
high_school_physics
20%50%60%90% 20%50%60%90% 20%50%60%90% 20%50%60%90% 20%50%60%90% 20%50%60%90%
ECE AUROC ECE AUROC ECE AUROC ECE AUROC ECE AUROC ECE AUROC
Zero-Shot Classifier Probe LoRA LoRA + Prompt
Figure8: (Part1)ECEandAUROCvaluesforQuery,CT-Probe,CT-LoRA,andCT-Queryforeach
subsetofMMLUinmultiple-choicequestionanswering(MCQA)setting.
25LLaMA-2 LLaMA-2 LLaMA-2 LLaMA-2 Mistral Mistral
7B 7B Chat 13B 13B Chat 7B 7B Instruct
high_school_psychology
high_school_statistics
high_school_us_history
high_school_world_history
human_aging
human_sexuality
international_law
jurisprudence
logical_fallacies
machine_learning
management
marketing
medical_genetics
miscellaneous
moral_disputes
moral_scenarios
nutrition
philosophy
prehistory
professional_accounting
professional_law
professional_medicine
professional_psychology
public_relations
security_studies
sociology
us_foreign_policy
virology
world_religions
20%50%60%90% 20%50%60%90% 20%50%60%90% 20%50%60%90% 20%50%60%90% 20%50%60%90%
ECE AUROC ECE AUROC ECE AUROC ECE AUROC ECE AUROC ECE AUROC
Zero-Shot Classifier Probe LoRA LoRA + Prompt
Figure9: (Part2)ECEandAUROCvaluesforQuery,CT-Probe,CT-LoRA,andCT-Queryforeach
subsetofMMLUinmultiple-choicequestionanswering(MCQA)setting.
26LLaMA-2 LLaMA-2 LLaMA-2 LLaMA-2 Mistral Mistral
7B 7B Chat 13B 13B Chat 7B 7B Instruct
abstract_algebra
anatomy
astronomy
business_ethics
clinical_knowledge
college_biology
college_chemistry
college_computer_science
college_mathematics
college_medicine
college_physics
computer_security
conceptual_physics
econometrics
electrical_engineering
elementary_mathematics
formal_logic
global_facts
high_school_biology
high_school_chemistry
high_school_computer_science
high_school_european_history
high_school_geography
high_school_government_and_politics
high_school_macroeconomics
high_school_mathematics
high_school_microeconomics
high_school_physics
20%50% 60%90% 20%50% 60%90% 20%50% 60%90% 20%50% 60%90% 20%50% 60%90% 20%50% 60%90%
ECE AUROC ECE AUROC ECE AUROC ECE AUROC ECE AUROC ECE AUROC
Zero-Shot Classifier Probe LoRA LoRA + Prompt
Figure10: (Part1)ECEandAUROCvaluesforQuery, CT-Probe, CT-LoRA,andCT-Queryfor
eachsubsetofMMLUinopen-ended(OE)setting.
27LLaMA-2 LLaMA-2 LLaMA-2 LLaMA-2 Mistral Mistral
7B 7B Chat 13B 13B Chat 7B 7B Instruct
high_school_psychology
high_school_statistics
high_school_us_history
high_school_world_history
human_aging
human_sexuality
international_law
jurisprudence
logical_fallacies
machine_learning
management
marketing
medical_genetics
miscellaneous
moral_disputes
moral_scenarios
nutrition
philosophy
prehistory
professional_accounting
professional_law
professional_medicine
professional_psychology
public_relations
security_studies
sociology
us_foreign_policy
virology
world_religions
20%50% 60%90% 20%50% 60%90% 20%50% 60%90% 20%50% 60%90% 20%50% 60%90% 20%50% 60%90%
ECE AUROC ECE AUROC ECE AUROC ECE AUROC ECE AUROC ECE AUROC
Zero-Shot Classifier Probe LoRA LoRA + Prompt
Figure11: (Part2)ECEandAUROCvaluesforQuery, CT-Probe, CT-LoRA,andCT-Queryfor
eachsubsetofMMLUinopen-ended(OE)setting.
28abstract_algebra anatomy astronomy clinical_knowledge
0.6 0.6
0.75 0.75
0.4 0.4
0.50 0.50
0.2 0.2 0.25 0.25
0.0 0.00
0 25 50 0 50 100 0 100 200 0 100
Target Length Target Length Target Length Target Length
college_biology college_chemistry college_computer_science college_mathematics
0.6 0.75 0.8 0.6
0.4 0.50 0.6 0.4
0.4
0.2 0.25
0.2 0.2
0 100 200 0 100 0 50 100 0 50 100
Target Length Target Length Target Length Target Length
college_medicine computer_security econometrics electrical_engineering
0.75 0.6 0.8 0.6
0.50 0.4 0.6 0.4
0.25 0.2 0.4 0.2
0.00
0 100 0 100 200 0 50 100 0 50
Target Length Target Length Target Length Target Length
elementary_mathematics formal_logic global_facts high_school_biology
0.75 0.6 0.75
0.50 0.4 0.50 0.5
0.25 0.25
0.2
0.0
0 50 100 0 100 200 0 50 100 0 100
Target Length Target Length Target Length Target Length
high_school_chemistry high_school_geography
high_school_computer_science high_school_european_history
0.75 1.0 0.75
0.75
0.50 0.50
0.50 0.5
0.25 0.25
0.25
0 100 0 100 200 0 100 200 0 50 100
Target Length Target Length Target Length Target Length
high_school_mathematics
high_school_macroeconomics high_school_microeconomics
high_school_government_and_politics 0.6
0.75 0.75
0.4
0.75 0.50 0.50
0.50 0.25 0.2 0.25
0.25
0 Targ1 e0 t 0 Length200 0 Target Le1 n0 g0 th 0 Targ2 e5 t Leng5 th0 0 Target1 L0 e0 ngth
Figure12: ConfidenceversusTargetLengthforvariousMMLUsubsets. Ahorizontalregressionline
indicatesweakcorrelationofconfidencewiththetargetlength. Seefigs.13and14forothersubsets.
29
ecnedifnoC
ecnedifnoC
ecnedifnoC
ecnedifnoC
ecnedifnoC
ecnedifnoC
ecnedifnoC
ecnedifnoC
ecnedifnoC
ecnedifnoC
ecnedifnoC
ecnedifnoC
ecnedifnoC
ecnedifnoC
ecnedifnoC
ecnedifnoC
ecnedifnoC
ecnedifnoC
ecnedifnoC
ecnedifnoC
ecnedifnoC
ecnedifnoC
ecnedifnoC
ecnedifnoChigh_school_physics high_school_psychology high_school_statistics high_school_us_history
1.0
0.6 0.75 0.75
0.4 0.50 0.5
0.50
0.2 0.25
0.25
0.00
0 100 200 0 100 200 0 100 200 0 100 200
Target Length Target Length Target Length Target Length
high_school_world_history human_aging human_sexuality international_law
1.0
0.75 0.6 0.75
0.5 0.50 0.4 0.50
0.25 0.2 0.25
0.00 0.0
0 100 0 50 100 0 100 0 200
Target Length Target Length Target Length Target Length
jurisprudence logical_fallacies machine_learning management
0.6 0.75 0.75 0.6
0.4 0.50 0.50 0.4
0.2 0.25 0.25 0.2
0.00
0 100 200 0 100 200 0 50 100 0 100
Target Length Target Length Target Length Target Length
marketing medical_genetics miscellaneous moral_disputes
1.0
0.6 0.75 0.75
0.4 0.50 0.5 0.50
0.2 0.25 0.25
0.0
0 100 200 0 50 100 0 100 200 0 100
Target Length Target Length Target Length Target Length
nutrition philosophy prehistory
moral_scenarios 0.75 0.75 0.75
0.6 0.50 0.50 0.50
0.4 0.25 0.25 0.25
0.2 0.00 0 100 200 0 100 0.00 0 100
Targ1e5t Length20 Target Length Target Length Target Length
professional_accounting professional_psychology public_relations security_studies
0.6 0.75 0.6
0.4 0.5 0.50 0.4
0.2 0.25 0.2
0.0 0.00
0 100 0 100 200 0 50 100 0 250 500
Target Length Target Length Target Length Target Length
Figure13: Continuingfromfig.12. Seealsofig.14.
30
ecnedifnoC
ecnedifnoC
ecnedifnoC
ecnedifnoC
ecnedifnoC
ecnedifnoC
ecnedifnoC
ecnedifnoC
ecnedifnoC
ecnedifnoC
ecnedifnoC
ecnedifnoC
ecnedifnoC
ecnedifnoC
ecnedifnoC
ecnedifnoC
ecnedifnoC
ecnedifnoC
ecnedifnoC
ecnedifnoC
ecnedifnoC
ecnedifnoC
ecnedifnoC
ecnedifnoCsociology us_foreign_policy virology world_religions
0.75 0.75 0.75 0.75
0.50 0.50 0.50 0.50
0.25 0.25 0.25 0.25
0.00
0 100 0 50 100 0 50 100 0 50
Target Length Target Length Target Length Target Length
Figure14: Continuingfromfigs.12and13.
1.0
1.0
0.9
0.8 0.8
0.6 0.7
0.6
0.4
0.5
0.2 0.4
0.3
0.0 No LLM LLM LLM + Conf LLM + Conf LLM + Conf LLM LLM + Conf LLM + Conf LLM + Conf
(Rand) (Query) (CT) (Rand) (Query) (CT)
Figure15:(Left)Useraccuracyon60MMLUquestionspervariant(N “20userspervariant);violin
plotsshowquartilesasdashedlines(Right)Averagereliancesensibility(proportionofinstances
wheretheusersidedwiththemodelwhenthemodelwascorrect,andoverrodethemodel’sprediction
whenthemodelwasincorrect);higherindicatesbetterreliancecalibration.
High School Biology High School CS
1.0 1.0
0.8 0.8
0.6 0.6
0.4 0.4
0.2 0.2
No LLM LLM LLM + Conf LLM + Conf LLM + Conf No LLM LLM LLM + Conf LLM + Conf LLM + Conf
(Rand) (Query) (CT) (Rand) (Query) (CT)
US Foreign Policy Elementary Math
1.0 1.0
0.8 0.8
0.6 0.6
0.4 0.4
0.2 0.2
No LLM LLM LLM + Conf LLM + Conf LLM + Conf No LLM LLM LLM + Conf LLM + Conf LLM + Conf
(Rand) (Query) (CT) (Rand) (Query) (CT)
Figure16: UseraccuraciespertopicfortheMistralvariants. Redlineindicatesthemodel’saverage
accuracy.
31
ecnedifnoC
ycaruccA
ycaruccA
ycaruccA
ecnedifnoC ecnedifnoC
ytilibisneS
ecnaileR
ycaruccA
ycaruccA
ecnedifnoCHigh School Biology High School CS
1.0 1.0
0.8 0.8
0.6 0.6
0.4 0.4
0.2 0.2
No LLM LLM LLM + Conf LLM + Conf LLM + Conf No LLM LLM LLM + Conf LLM + Conf LLM + Conf
(Rand) (Query) (CT) (Rand) (Query) (CT)
US Foreign Policy Elementary Math
1.0 1.0
0.8 0.8
0.6 0.6
0.4 0.4
0.2 0.2
No LLM LLM LLM + Conf LLM + Conf LLM + Conf No LLM LLM LLM + Conf LLM + Conf LLM + Conf
(Rand) (Query) (CT) (Rand) (Query) (CT)
Figure 17: User accuracies per topic for the GPT-3.5 variants (with generalization confidence
computedfortheCTandQuerycases). Redlineindicatesthemodel’saverageaccuracy.
1.0
0.9
0.8
0.7
0.6
0.5
0.4
0.3
LLM LLM + Conf LLM + Conf LLM + Conf
(Rand) (Query) (CT)
Figure18: ReliancesensibilityforthevariantsbasedonGPT-3.5
32
ycaruccA
ycaruccA
ytilibisneS
ecnaileR
ycaruccA
ycaruccAFigure19: ExampleinterfacefromModiste. Participantsareinformedofthequestion(andtopic),
as well as the LLM prediction and confidence. Participants are informed of their running score
throughouttheexperiment.
33Figure20: Experimentinstructionsfortheconfidencevariants.
34Figure21: Experimentinstructionsfortheconfidencevariants(continued).
35Figure22: Samplepot-surveyquestionnaireforuserswhowereallocatedtoavariantwhereinthey
sawmodelconfidence.
36