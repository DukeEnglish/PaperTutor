Nyström Kernel Stein Discrepancy
Florian Kalinke1, Zoltán Szabó2, Bharath K. Sriperumbudur3
1Karlsruhe Institute of Technology, Karlsruhe, Germany
2London School of Economics, London, UK
3The Pennsylvania State University, University Park, PA 16802, USA
florian.kalinke@kit.edu z.szabo@lse.ac.uk bks18@psu.edu
Abstract
Kernel methods underpin many of the most successful approaches in data science and
statistics,andtheyallowrepresentingprobabilitymeasuresaselementsofareproducingkernel
Hilbertspacewithoutlossofinformation. Recently,thekernelSteindiscrepancy(KSD),which
combines Stein’s method with kernel techniques, gained considerable attention. Through
the Stein operator, KSD allows the construction of powerful goodness-of-fit tests where it
is sufficient to know the target distribution up to a multiplicative constant. However, the
typical U- and V-statistic-based KSD estimators suffer from a quadratic runtime complexity,
which hinders their application in large-sca`le settings.˘In this work, we propose a Nyström-
based KSD acceleration—with runtime O mn`m3 for n samples and m ! n Nyström
?
points—, show its n-consistency under the null with a classical sub-Gaussian assumption,
and demonstrate its applicability for goodness-of-fit testing on a suite of benchmarks.
1 Introduction
The kernel mean embedding, which involves mapping probabilities into a reproducing kernel
Hilbert spaces (RKHS; Aronszajn 1950) has found far-reaching applications in the last 20 years.
For example, it allows to measure the discrepancy between probability distributions through
maximum mean discrepancy (MMD; Smola et al. 2007, Gretton et al. 2012), defined as the
distance between the corresponding mean embeddings, which underpins powerful two-sample
tests. MMD is also known as energy distance [Székely and Rizzo, 2004, 2005, Baringhaus and
Franz, 2004] in the statistics literature; see Sejdinovic et al. [2013] for the equivalence. We refer
to [Muandet et al., 2017] for a recent overview of kernel mean embeddings.
In addition to two-sample tests, testing for goodness-of-fit (GoF; Ingster and Suslina 2003,
Lehmann and Romano 2021) is also of central importance in data science and statistics, which
involves testing H : Q “ P vs. H : Q ‰ P based on samples from an unknown sampling
0 1
distributionQanda(fixedknown)targetdistributionP. ClassicalGoFtests,e.g.,theKolmogorov-
Smirnov test [Kolmogorov, 1933, Smirnov, 1948], or the test for normality by Baringhaus and
Henze [1988], usually require explicit knowledge of the target distribution. However, in practical
applications, the target distribution is frequently only known up to a normalizing constant.
ExamplesincludevalidatingtheoutputofMarkovChainMonteCarlo(MCMC)samplers[Welling
andTeh,2011,Bardenetetal.,2014,Korattikaraetal.,2014],orassessingdeepgenerativemodels
[Koller and Friedman, 2009, Salakhutdinov, 2015]. In all these examples, one desires a powerful
test, even though the normalization constant might be difficult to obtain.
A recent approach to tackle GoF testing involves applying a Stein operator [Stein, 1972, Chen,
2021, Anastasiou et al., 2023] to functions in an RKHS and using them as test functions to
1
4202
nuJ
21
]LM.tats[
1v10480.6042:viXrameasure the discrepancy between distributions, referred to as kernel Stein discrepancies (KSD;
Chwialkowski et al. 2016, Liu et al. 2016). An empirical estimator of KSD can be used as a
test statistic to address the GoF problem. In particular, the Langevin Stein operator [Gorham
and Mackey, 2015, Chwialkowski et al., 2016, Liu et al., 2016, Oates et al., 2017, Gorham and
Mackey, 2017] in combination with the kernel mean embedding gives rise to a KSD on the
Euclidean space Rd, which we consider in this work. As a test statistic, KSD has many desirable
properties. In particular, KSD requires only knowledge of the derivative of the score function
of the target distribution — implying that KSD is agnostic to the normalization of the target
and therefore does not require solving, either analytically or numerically, complex normalization
integrals in Bayesian settings. This property has led to its widespread use, e.g., for assessing and
improvingsamplequality[GorhamandMackey,2015,Chenetal.,2018,2019,Futamietal.,2019,
Gorhametal.,2020],validatingMCMCmethods[Coullonetal.,2023],comparingdeepgenerative
models [Lim et al., 2019], detecting out-of-distribution inputs [Nalisnick et al., 2019], assessing
Bayesian seismic inversion [Izzatullah et al., 2020], modeling counterfactuals [Martinez-Taboada
and Kennedy, 2023], and explaining predictions [Sarvmaili et al., 2024]. GoF testing with KSDs
has been explored on Euclidean data [Liu et al., 2016, Chwialkowski et al., 2016], discrete data
[Yang et al., 2018], point processes [Yang et al., 2019], time-to-event data [Fernandez et al., 2020],
graph data [Xu and Reinert, 2021], sequential models [Baum et al., 2023], and functional data
[Wynneetal.,2024]. TheKSDstatistichasalsobeenextendedtotheconditionalcase[Jitkrittum
et al., 2020].
Estimators for Langevin Stein operator-based KSD exist. But, the classical U-statistic-
[Liu et al., 2016] and V-statistic-based [Chwialkowski et al., 2016] estimators have a runtime
complexity that scales quadratically with the number of samples of the sampling distribution,
which limits their deployment to large-scale settings. To address this bottleneck, Chwialkowski
et al. [2016] introduced a linear-time statistic that suffers from low statistical power compared to
its quadratic-time counterpart. Jitkrittum et al. [2017] proposed the finite set Stein discrepancy
(FSSD), a linear-time approach that replaces the RKHS-norm by the L -norm approximated by
2
sampling; the sampling can either be random (FSSD-rand) or optimized w.r.t. a power proxy
(FSSD-opt). Another approach [Huggins and Mackey, 2018] is employing the random Fourier
feature (RFF; Rahimi and Recht 2007) method to accelerate the KSD estimation. However, it is
known [Chwialkowski et al., 2015, Proposition 1] that the resulting statistic fails to distinguish a
large class of measures. Huggins and Mackey [2018] generalize the idea of replacing the RKHS-
norm by going from L -norms to L ones, to obtain feature Stein discrepancies. They present an
2 p
efficient approximation, random feature Stein discrepancies (RFSD), which is a near-linear time
estimator. However,successfuldeploymentofthemethoddependsonagoodchoiceofparameters,
which, while the authors provide guidelines, can be challenging to select and tune in practice.
Our work alleviates these severe bottlenecks. We employ the Nyström method [Williams
?
and Seeger, 2001] to accelerate KSD estimation and show the n-consistency of our proposed
estimator under the null. The main technical challenge is that the Stein kernel (induced by the
Langevin Stein operator and the original kernel) is typically unbounded while existing statistical
Nyström analysis [Rudi et al., 2015, Chatalic et al., 2022, Sterge and Sriperumbudur, 2022,
Kalinke and Szabó, 2023] usually considers bounded kernels. To tackle unbounded kernels, we
select a classical sub-Gaussian assumption, which we impose on the feature map associated to the
kernel, and show that existing methods of analysis can successfully be extended to handle this
novel case. In this sense, our work, besides Della Vecchia et al. [2021], which requires a similar
sub-Gaussian condition for analyzing empirical risk minimization on random subspaces, is a first
step in analyzing the consistency of the unbounded case in the Nyström setting.
Our main contributions can be summarized as follows.
1. We introduce a Nyström-based acceleration of kernel Stein discrepancy. The proposed
2` ˘
estimator runs in mn`m3 time, with n samples and m!n Nyström points.
? O
2. We prove the n-consistency under the null of our estimator in a classical sub-Gaussian
setting, which extends (in a non-trivial fashion) existing results for Nyström-based methods
[Rudi et al., 2015, Chatalic et al., 2022, Sterge and Sriperumbudur, 2022, Kalinke and Szabó,
2023] focusing on bounded kernels.
3. Weperformanextensivesuiteofexperimentstodemonstratetheapplicabilityoftheproposed
method. Our proposed approach achieves competitive results throughout all experiments.
The paper is structured as follows. We introduce the notations used throughout the article
(Section 2) followed by recalling the classical quadratic-time KSD estimators (Section 3). In
Section 4.1, we detail our proposed Nyström-based estimator, alongside with its adaptation
to a modified wild bootstrap goodness-of-fit test (Section 4.2), and our theoretical guarantees
(Section 4.3). Experiments demonstrating the efficiency of our Nyström-KSD estimator are
provided in Section 5. Proofs are deferred to the appendices.
2 Notations
In this section, we introduce our notations. Let rNs :“ t1,...,Nu for a positive integer N.
For a ,a ě 0, a À a (resp. a Á a ) means that a ď ca (resp. a ě c1a ) for an absolute
1 2 1 2 1 2 1 2 1 2
constant c ą 0 (resp. c1 ą 0), and we write a — a iff. a À a and a Á a . We write
1 2 1 2 1 2
1 for the indicator function and tt¨uu for a multiset. The n-dimensional vector of ones is
p¨q
denoted by 1 “ p1,...,1qT P Rn , that of n zeros by 0 “ p0,...,0qT P Rn. The identity
n n
matrix is I
n
P Rnˆn. For a matrix A P Rd1ˆd2, A´ P Rd2ˆd1 denotes its (Moore-Penrose)
pseudo-inverse, and AT P Rd2ˆd1 stands for the transpose of A. We write A´1 P Rdˆd for
the inverse of a non-singular matrix A P Rdˆd. For a differentiable function f : Rd Ñ R , let
´ ¯
d
fpxq“ Bfpxq PRd.
∇x Bxi
i“1
Let p ,τ q be a topological space and pτ q the corresponding Borel σ-algebra. Probability
X X
measuresXin this article are meant w.r.t. tBhe measurable space p , pτ Xqq a `nd a ˘re written as
`p q; for instance, the set of Borel probability measures onXRdBis ` Rd . The n-fold
Mpro1 duXct measure of P P `p q is denoted by Pn P `p nq. For a sMequ1 ence of real-valued
r inan pd ro om bav ba ilr ii ta yb .le Ts hX en ua nn itdM ba a1 ls leqX inue anc He ilo bf ep rtos si pti av ce er n-s i, sM X den1 n“ oX tOedPp br ynq Bm pea qn “s t th fat PX rnn |is }fb }oun ďd 1ed
u.
The reproducing kernel Hilbert space with k : HRd ˆ Rd Ñ R as tHhe reproduHcing keH rnel is
denoted by . Throughout the paper, k is assumed to be measurable and to be separable;
k k
for instanceH, a continuous kernel k : Rd ˆ Rd Ñ R implies the latter pHroperty [Steinwart
and Christmann, 2008, Lemma 4.33]. The canonical feature map ϕ : Rd Ñ is defined
k k
as x ÞÑ kp¨,xq; with this feature map kpx,yq “ xkp¨,xq,kp¨,yqy “ xϕ pxq,ϕ pH yqy for all
x,y P Rd. The Gram matrix associated with the
observationsH pxk
qn
Pk ` Rd˘ nk andH kk
ernel k
i i“1
is K “ rkpx ,x qsn P Rnˆn. Given a closed linear subspace U Ď , the (orthogonal)
projek c,n ti,n
on of h
Pi j oi, nj“ U1
is denoted by P h P U; u “ P h is the
uniqHuk
e vector such that
k U U
h´uKU. ForanyH uPU,}h´P h} ď}h´u} ,thatis,P histheclosestelementinU toh.
A linear operator A: Ñ isU calH lekd boundedH ifk }A} :“suU p }Ah} ă8; the set of
Ñ
boundedlinH eak ropeH rak
torsisdenotedby p
q.op AnAP}h p} Hk“ q1 iscalleH dk
positive(shortly
k k k k
H A ě 0)Hif it is self-adjoint (A˚ “ A, where A˚ PL H p q is definLedHby xAf,gy “ xf,A˚gy
for all f,g P ), and xAh,hy ě 0 for all h PL Hk . If A ě 0, then thereH ek xists a uniqH uk e
Hk Hk Hk
B ě 0 such that B2 “ řA; we write B “ A1 2 and call B the square root of A. An A P p kq
L H
is called trace-class if iPIxpA˚Aq1 2e i,e iy
Hk
ă8 for some countable orthonormal basis (ONB)
3ř
p oe
pi
eq
riP aI
too rf AHwk, ita hn ed igi en nvt ah li us ec sa ps λe qtrpA ,q tr: p“ Aq“iPřIxAe
i
λ,e .iy
AH nk
oă pe8 ra. t1 orF Aor Pa pself- qad isjo ci an lt let dra cc oe m-c pl aa css
t
i iPI iPI i L Hk
if tAh|hPBp qu is compact, where ¨ denotes the closure. A trace class operator is compact,
k
and a compacHt positive operator A has largest eigenvalue }A} . For any A P p q, it holds
op L Hk
that }A˚A} “}A}2 (which is called the C˚ property).
op op
Themeanembeddingofa şprobabilitymeasurePP `pRdqintotheRKHSassociatedtokernel
k [D: ieR sd teˆ laR ndd Ñ UhR l,1is 97µ
7k
,p CP hq a“
pteR rd
Iϕ
I.k
2p ]x .q Td hP epx mq eP
anHek
le, mw ehM ne tre µ1 th pPe qin et xe ig str sal iffis .şmea }n ϕt pin xqB }och dn Pe pr x’s qs ăen 8se
[Diestel and Uhl, 1977, p. 45; Theorem 2]; this conditik on is satisfied fR od r ink stancH ekwhen ϕ is
k
bou Ln ed ted f, ,gin Pothe .r Tw ho er id rs tes nu sp ox rP pR rd o} dϕ uk cp tx iq s} H wkrită te8 n.
asfbg P b ,where b isthetensor
k k k k k
productHilbeHrtspace;further,fbg : Ñ definesaranHk-onHeoperatorbHyhÞÑH fxg,hy . It
is known that kb
k
is also an `RK ˘HHSk [BerHlink et and Thomas-Agnan, 2004, Theorem 13]. H Gkiven
a probability mHeasuHre P P ` Rd and a kernel k : RdˆRd Ñ R, the uncentered covariance
operator
M1
ż
CP,k “ ϕ kpxqbϕ kpxqdPpxqP kb
k
(1)
Rd H H
ş
Cex Pi ,s kts `if
λI
`X
,
} wϕ ˘hk ep rx eq} I2 Hkdd eP nopx teq să th8 e; idC eP n,k titi ys a
o
´pp eo rs ai tt oiv re at nr ¯a dce λ-cl ąass 0.op Ter ha eto er. ffeW cte ivede dfi in me eC nsP i, ok, nλ o“
f
P P M`
1
Rd is defined as NP,kpλq :“ tr CP,kC P´ ,k1
,λ
ď trpC λP,kq.2 With p ě 1 and a real-
valued random varia “şble X : pΩ, A,Pq ‰Ñ pR, BpτRqq, where BpτRq denotes the Borel σ-field on
R, let }X} “ |Xpωq|pdPpωq p1. For p P t1,2u, let ψ puq “ eup ´ 1 and }X} :“
! LppPq ´Ω ¯ ) p ψp
inf C ą0|E X„Pψ
p
|X C| ď1 . A real-valued random variable X „ P P M` 1pRq is called
sub-exponential if }X} ă8 and sub-Gaussian if }X} ă8. In the following, we specialize
Definition 2 by Koltchinψ s1kii and Lounici [2017] stated forψ2Banach spaces to (reproducing kernel)
Hilbert spaces by using the Riesz representation theorem. A centered random variable X „
QP `p q taking values in an RKHS is called sub-Gaussian iff. there exists a universal
consMtan1 t CH ąk 0 such that Hk
› › › ›
› xX,uy › ďC› xX,uy › ă8 (2)
Hk ψ2 Hk L2pQq
holds for all uP .
k
H
3 Problem formulation
We now introduce our quantity of interest, the kernel Stein discrepancy. Let d :“ˆd be
ř Hk i“1Hk
the product RKHS with inner product defined by xf,gy “ d xf ,g y for f “pf qd ,g“
` ˘ Hd
k
i“1 i i Hk i i“1
pg qd P d. Let P,QP ` Rd be fixed; we refer to P as the target distribution and to Q as
thi ei s“ a1 mplHink gdistribution.MA1
ssumethatPisabsolutelycontinuousw.r.t.theLebesguemeasureλ
d
on Rd; let the corresponding Radon-Nikodym derivative—in other words, the probability density
1Thetrace-classpropertyandthevalueoftrpAqisindependentofthespecificONBchosen. Theseparabilityof
H
k
impliestheexistenceofacounta´bleONBin¯it.
ř ř
2Thisinequalityisimpliedbytr CP,kC P´ ,k1
,λ
“
iPI
λiλ `i
λ
ď λ1 iPIλi“ trpC λP,kq ,wherepλiqiPI denotethe
eigenvaluesofCP,k.
4dP
function (pdf) of P—be p“ . We assume that p is continuously differentiable with support
dλ
d
Rd, ppxqą0 for all xPRd, and lim fpxqppxq“0 for all f P . The last property holds
for instance if p is bounded and lim
}} xx }} ÑÑ 88
fpxq“0 `for all f P Hk.
˘THhk
e Stein operator is defined
as pT fqpxq“x rlogppxqs,fpxqy`x fpxq,1 y f P d, xPRd . With this notation at hand,
for
alp
l f P d
an∇dx
xPRd one has
∇x d Hk
Hk
pT fqpxq“xf,ξ pxqy , ξ pxq“r plogppxqqϕ pxq` ϕ pxqsP d,
p p Hd
k
p ∇x k ∇x k Hk
with the kernel
h px,yq“xξ pxq,ξ pyqy “xh p¨,xq,h p¨,yqy , x,yPRd; (3)
p p p Hd
k
p p Hhp
notice that ξ pxq and h p¨,xq map to different feature spaces ( d and , respectively) but
yield the
samp
e kernel
hp
. The kernel Stein discrepancy (KSD)
[HCk hwialkHowh sp
ki et al., 2016, Liu
p
et al., 2016] then is defined as
S ppQq“ fPBsu pHp dqxf,E X„Qξ ppXqy
Hd
k
“}E X„Qξ ppXq}
Hd
k
“}E X„Qh pp¨,Xq}
Hhp
, (4)
k
where the last equality follows from (3). By the construction of KSD,
S ppPq“}E X„Ph pp¨,Xq}
Hhp
“0, (5)
that is, the expected value of the Stein feature map under the target distribution P is zero.
Given a sample Qˆ “tx un „Qn, the popular V-statistic-based estimator [Chwialkowski
n i i“1
et al., 2016, Section 2.2] is constructed by replacing Q with the empirical measure Qˆ ; it takes
n
the form
´ ¯
1
ÿn
S2 Qˆ “ h px ,x q, (6)
p n n2 p i j
i,j“1
` ˘
and can be computed in n2 time. The corresponding U-statistic-based estima ´tor [ ¯Liu
O
et al., 2016, (14)] has a similar expression but omits the diagonal terms, that is, S2 Qˆ “
ř ` ˘ p,u n
1 n h px ,x q; italsohasaruntimecostsof n2 . Forlarge-scaleapplications, the
npn´1q 1ďi‰jďn p i j O
quadratic runtime is a significant bottleneck; this is the shortcoming we tackle in the following.
4 Proposed Nyström-KSD
To enable the efficient estimation of (4), we propose a Nyström technique-based estimator in
Section 4.1 and an accelerated wild bootstrap test in Section 4.2. In Section 4.3, our consistency
results are collected.
4.1 The Nyström-KSD estimator
WeconsiderasubsampleQ˜ “ttx˜ ,...,x˜ uuofsizem(sampledwithreplacement),theso-called
m 1 m
Nyström sample, of the original sample Qˆ “tx ,...,x u; the tilde indicates a relabeling. The
n 1 n
Nyström sample spans “spantξ px˜ q|iPrmsuĂ d, with feature map ξ and associated
kernel h defined in (3)H. p,m p i Hk p
p
5One idea is to restrict the space over which one takes the supremum in (4), that is, one could
consider
S ppQq( “4) sup xf,E X„Qξ ppXqy
Hd
p «iq sup xf,E X„Qξ ppXqy
Hd
fPBpHd kq
A
k
E
fPBpH bp,mq k
p «iiq sup f,E ξ pXq pi “iiq βTK´1 β , (7)
fPBpHp,mq X„Qˆ n p Hd k p hp,m,m p
withβ “ 1K 1 . Thedetailsareasfollows. Inapproximationpiq,werestrictthesupremum
p n hp,m,n n
to Ă d, in step piiq the empirical measure Qˆ is used instead of Q, and piiiq is detailed
in
AHpp, pm endiHxk
C. However, one can see that the
intuin
tive estimator (7) has limited applicability,
as the inverse of K does not necessarily exist (for instance, by the non-strictly positive
hp,m,m
definiteness of k or the sampling with replacement); hence, we take an alternative route which
computationally leads to a similar expression and also allows statistical analysis.
The best approximation of S pQq in RKHS-norm-sense, when using m Nyström samples,
p
c spo au nld thbe p¨o ,b x˜ta qin |ied Prb my sc uo Ănsideri .ng At sh Qe o isrt uh no kgo nn owal np ir noj pe rc ati co tn iceof anE dX„ oQ nlh yp ap¨ v, aX ilq abo ln et vo iaHsh ap m,m pl: e“
s
p i Hhp
Qˆ „Qn, we consider the orthogonal projection of E h p¨,Xq onto instead. In other
won
rds, we aim to find the weights α “ pα qm P
RX m„ tQˆ hnatp correspondHtohp t, hm
e minimum norm
i i“1
solution of the cost function
› ›
›
› 1
ÿn ÿm ›
›
min › h p¨,x q´ α h p¨,x˜ q› , (8)
αPRm› ln ooio“oo1oomp oooooooi
n i“1
i p i ›
Hhp
“E X„Qˆnhpp¨,Xq
which gives rise to the squared KSD estimator3
› ›
´ ¯ › ›ÿm › ›2 ›
›
›
›2
S˜2 Qˆ :“› α h p¨,x˜ q› p“›P E h p¨,Xqq› q. (9)
p n ›
i“1
i p i ›
Hhp,m
Hhp,m X„Qˆ
n
p
Hhp,m
Lemma 1 (Nyström-KSD Estimator). The squared KSD estimator (9) takes the form
´ ¯
S˜2 Qˆ “βTK´ β , (10)
p n p hp,m,m p
where we define β “ 1K 1 PRm, K “rh px˜ ,x˜ qsm PRmˆm, and K “
p n hp,m,n n hp,m,m p i j i,j“1 hp,m,n
rh px˜ ,x qsm,n PRmˆn.
p i j i,j“1
Remark 1.
(a) Runtime complexity. The computation of (10) consists of calculating β , pseudo-inverting
p
K , and obtaining the quadratic form βTK´ β . The calculation of β requires
hp,m,m p hp,m,m p p
pmnq operations, due to the multiplication of an mˆn matrix with a vector of length n.
O
Inverting the mˆm matrix K costs pm3q,4 dominating the cost of pm2q needed for
the computation of K .
Thp h, em, qm
uadraticO form βTK´ β has a compO utational cost of
` ˘ hp,m,m ` p ˘hp,m,m p ` ˘
m2 . Hence, (10) can be computed in mn`m3 , which means that for m“o n2{3 ,
O O
our proposed Nyström-KSD estimator guarantees an asymptotic speedup.
´ ¯
3S˜ p2 Qˆ n indicatesdependenceonQˆ n.
4Althoughfasteralgorithmsformatrixinversionexist,e.g.,Strassen’salgorithm,weconsidertheruntimethat
onetypicallyencountersinpractice.
6(b) Comparison of (7) and (10). The two presented Nyström approaches result in comparable
estimators — the inverse becomes a pseudo-inverse — but the projection perspective lends
itself to a principled analysis that we detail in Section 4.3.
(c) Comparison of (6) and (10). The Nyström estimator (10) recovers the V-statistic-based
estimator (6) when no subsampling is performed and provided that K is invertible.
hp,n,n
4.2 Nyström bootstrap testing
In this section, we discuss how one can use (10) for accelerated goodness-of-fit testing. We
recall that the goal of goodness-of-fit testing is to test H : Q “ P versus H : Q ‰ P, given
0 1
samples Qˆ “tx ,...,x u and target distribution P. Recall that KSD relies on score functions
n 1 n
( rlogppxqs); hence knowing P up to a multiplicative constant is enough. To use the Nyström-
x
b∇ased estimator (10) for goodness-of-fit testing, we propose to obtain its null distribution by a
Nyström-based bootstrap procedure. Our method builds on the existing test for the V-statistic-
based KSD, detailed in Chwialkowski et al. [2016, Section 2.2], which we quote in the following.
Define the bootstrapped statistic by
1
ÿn
B “ w w h px ,x q, (11)
n n2 i j p i j
i,j“1
with w Pt´1,1u an auxiliary Markov chain defined by
i
w “1 w ´1 w , where U i. „i.d. Unifp0,1q, (12)
i pUią0.5q i´1 pUiď0.5q i´1 i
that is, w changes sign with probability 0.5. The test procedure is as follows.
i
1. Calculate the test statistic (6).
2. ObtainDwildbootstrapsamplestB uD with(11)andestimatethe1´αempiricalquantile
n,i i“1
of these samples.
3. Reject the null hypothesis if the test statistic (6) exceeds the quantile.
` ˘ ` ˘
(11) requires n2 computations, which yields a total cost of Dn2 for obtaining D
bootstrap samplesO, rendering large-scale goodness-of-fit tests unpractiOcal.
We propose the Nyström-based bootstrap
1
BNys “ wTK K´ K w, (13)
n n2 hp,n,m hp,m,m hp,m,n
with w “ pw qn P Rn collecting the w -s (i P rns) defined in (12); K p“ KT q and
i i“1 i hp,n,m hp,m,n
K are defined as in Lemma 1. The approximation is based on the fact [Williams and
hp,m,m
Seeger, 2001] that K K´ K is a low-rank approximation of K , that is,
hp,n,m hp,m,m hp,m,n hp,n,n
K K´ K «K . Hence, our proposed procedure (13) approximates (11) but
hp,n,m hp,m,m hp,m,n` ˘hp,n,n` ˘
reduces the cost from n2 to
`
nm˘`m3 if one computes from left to right (also refer to
O O
Remark 1); in `the `case of m˘˘“ o n2{3 this guarantees an asymptotic speedup. We obtain a
total cost of D nm`m3 for obtaining the wild bootstrap samples. This acceleration allows
KSD-based gOoodness-of-fit tests to be applied on large data sets.
4.3 Guarantees
This section is dedicated to the statistical behavior of the proposed Nyström-KSD estimator (10).
7The existing analysis of Nyström estimators [Rudi et al., 2015, Chatalic et al., 2022, Sterge
and Sriperumbudur, 2022, Kalinke and Szabó, 2023] considers bounded kernels only. Indeed, if
wsu hp ix chPR wd e}h ip ncp¨ l, ux dq e} H heh rpeă fo8
r
c, ot nh ve ec no in es ni cs ete on fcy coo mf( p1 a0 r) isi os nim
.
pliedbyChatalicetal.[2022,Theorem4.1],
Theorem 1. Assume the setting of Lemma 1, CQ,hp ‰0, mě4 Nyström samples, and bounded
Stein feature map (sup xPRd}h pp¨,xq}
Hhp
“: K ă 8). Then, for any δ P p0,1q it holds with
probability at least 1´δ that
ˇ ´ ¯ˇ a d ˆ ˙
ˇ ˇ c c c logpm{δq 12K2logpm{δq
ˇS ppQq´S˜ p Qˆ n ˇď ?1
n
` m2 ` 3
m
NQ,hp
m
,
´ › › ¯
when měmax
67,12K2› CQ,hp›´ op1
logpm{δq, where c 1, c 2, and c
3
are positive constants.
However, in practice, the feature map of KSD is typically unbounded and Theorem 1 is not
applicable, as it is illustrated in the following example with the frequently-used Gaussian kernel.
Example 1 (KSD yields unbounded kernel). Consider univariate data (d“1), unnormalized
target density ppxq“e´x2{2, and the Gaussian kernel kpx,yq“e´γpx´yq2. Notice that, by using
(3), we have
}ξ
pxq}2
“h
px,xq“x2`constxÑ Ñ8
8.
p Hd p
k
Remark 2. In fact, a more general result holds: If one considers a bounded continuously
differentiable translation-invariant kernel k, the induced Stein kernel is only bounded provided
ř
that the target density ppxq has tails that are no thinner than e´ d i“1|xi| [Hagrass et al., 2024,
Remark 2], which clearly rules out Gaussian targets.
For analyzing the setting of unbounded feature maps, we make the following assumption.
Assu `mp ˘tion 1. Assume that the Stein feature map h pp¨,Xq and the sam›
›
pling distribution›
›
QP
` Rd are (i) sub-Gaussian in the sense of (2), that is, the condition ›xh p¨,Xq,uy › À
› ›M1 ›
›
› ›p Hhp› ›ψ2
›xh p¨,Xq,uy › ă8 holds for all uP , and (ii) it holds that 0ă›}h p¨,Xq} › À
›
›
p H› ›hp L2pQq Hhp p Hhp ψ2
›}h p¨,Xq} › ă8.
p Hhp L2pQq
We elaborate further on Assumption 1 in Remark 3(c), after we state our following main
result.
Theorem 2. Let Assumption 1 hold, CQ,hp ‰0, assume the setting of Lemma 1, mě4 Nyström
samples, and P“Q. Then, for any δ Pp0,1q it holds with probability at least 1´δ that
b ` ˘ d ` ˘
ˇ ´ ¯ˇ
ˇ
ˇS pQq´S˜ Qˆ
ˇ
ˇÀ
tr CQ,hp logp6{δq
`
tr CQ,hp logp6{δq
`
p p n
n n
b ` ˘ g f ˜ ` ˘¸
`
tr CQ,hp log mp12n{δqlogp12{δqf e
NQ,hp
ctr C mQ,hp
!› › ` ˘ )
› ›´1
when mÁmax CQ,hp
op
tr CQ,hp ,logp3{δq , where cą0 is a constant.
8To interpret the consistency guarantee of Theorem 2, we consider the three terms on the
` ˘
r.h.s. w.r.t. the magnitude of m. The first two terms converg ˆe with ˙n´1{2 , independent of
O
the choice of m. By using the universal upper bound NQ,hp`
ctrpC ˘mQ,hpq
Àm on the effective
dimension, the last term reveals that an overall rate of n´1{2 can only be achieved with
further assumptions regarding the rate of decay of the effOective dimension if one also requires
` ˘
m“o n2{3 — as is necessary for a speed-up, see Remark 1(a). Indeed, the rate of decay of the
effective dimension can be linked to the rate of decay of the eigenvalues of the covariance operator
[Della Vecchia et al., 2021, Proposition 4, 5], which is known to frequently decay exponentially, or,
atleast,polynomially. Inthissense,thelasttermactsasabalance,whichtakesthecharacteristics
of the data and of the kernel into account.
` ˘
The next corollary shows that an overall rate of n´1{2 can be achieved, depending on the
properties of the covariance operator. O
Corollary 1. In the setting of Theorem 2, assume that the spectrum of the covariance operator
eC xQ p, oh np ed ne tic aa ly lys ,e ii mth pe lr yi( ni g) tp ho aly t,n Nom Q,i ha pl ply λ, qi Àmp loly gi pn 1g `th c λa 1t qN foQ r,h sp op mλ eqÀ c 1λ ą´γ 0.fo Tr hs eo nm ite hγ olP dsp0 t, h1 as t, or (ii)
ˇ ´ ¯ˇ ˆ ˙
ˇ ˇ 1
ˇS ppQq´S˜ p Qˆ n ˇ“ Q ? ,
O n
1 1 1
assuming that the number of N ˆyströ ˆm points satisfie ˙s (i) mÁn2´γ log2´γ˙p12n{δqlog2´γp12{δq in
? 1{2
the first case, or (ii) mÁ n log 1` c1n logp12n{δqlogp12{δq in the second case.
ctrpCQ,hpq
Remark 3.
(a) Runtime bene`fit.˘Recall that — see Remark 1(a) —, our proposed Nyström estimator (10)
requires m“o n2{3 Nyström samples to achieve a speed-up. Hence, in the case of polynomial
decay, an asymptotic speed-up with a statistical accuracy that matches the quadratic time
estimator (6) is guaranteed for γ ă 1{2; in the case of exponential decay, large enough n
always suffices.
(b) Comparison of Theorem 1 and Theorem 2. The key takeaway of Theorem 2 is that the
boundedness requirement on the kernel can be relaxed to a sub-Gaussian assumption for KSD
estimation.
(c) Sub-Gaussian assumption. Key to the proof of Theorem 2 is having an adequate notion
of non-boundedness of the feature map. One approach—common for controlling unbounded
real-valued random variables— is to impose a sub-Gaussian assumption. In Hilbert spaces,
various definitions of sub-Gaussian behavior have been investigated [Talagrand, 1987, Fukuda,
1990, Antonini, 1997]; see Giorgobiani et al. [2020] for a recent survey. Classically, these
require centered random variables. We note that with (5) the mean-zero requirement is satisfied
for P“Q. Among the definitions of sub-Gaussianity, we carefully selected (i) Koltchinskii
and Lounici [2017, Def. 2], and (ii) Assumption 1(ii).5 Specifically, (ii) is needed for our key
Lemma B.3 which roughly states that sub-Gaussian vectors whitened by C´1{2 inherit the
Q,hp,λ
sub-Gaussian property, which opens the door for proving Theorem 2.
5Theformerconditionisalsoreferredtoassub-Gaussian in Fukuda’s sense [Giorgobianietal.,2020,Def.1].
95 Experiments
We verify the viability of our proposed method, abbreviated as N-KSD in this section, by
comparing its runtime and its power to existing methods: the quadratic time KSD [Liu et al.,
2016, Chwialkowski et al., 2016], the linear-time goodness-of-fit test finite set Stein discrepancy
(FSSD; Jitkrittum et al. 2017), and the near-linear-time goodness-of-fit test using random feature
Stein discrepancy (L1 IMQ, L2 SechExp; Huggins and Mackey 2018).6 For FSSD, we consider
randomizedtestlocations(FSSD-rand)andoptimizedtestlocations(FSSD-opt);theoptimalityis
meantw.r.t.apowerproxydetailedinthecitedwork[Jitkrittumetal.,2017]. Forallcompetitors,
we use the settings and implementations provided by the respective authors. We do not compare
againstRFFKSD,asHugginsandMackey[2018]showthatRFSDs,towhichwecompareagainst,
achieve a better performance on the same set of experiments. We use the well-known Gaussian
´ ¯
kernel kpx,yq “ exp ´γ}x´y}2 (γ ą 0) with the median heuristic [Garreau et al., 2018],
Rd
´ ¯
β
and the IMQ kernel kpx,yq“ c2`}x´y}2 [Gorham and Mackey, 2017], with the choices
Rd
of β ă0 and cą0 detailed in the respective experiment description. To approximate the null
distribution of N-KSD, we perform a bootstrap with (13), setting D “ 500. To allow an easy
comparison, our experiments replicate goodness-of-fit testing experiments from Chwialkowski
et al. [2016], Jitkrittum et al. [2017] and Huggins and Mackey [2018]. We ran all experiments on
a PC with Ubuntu 20.04, 124GB RAM, and 32 cores with 2GHz each.
?
Runtime. We set m“ n for N-KSD. As per recommendation, we fix the number of test
locationsJ “10forL1IMQ,L2SechExp, andbothFSSDmethods. Theruntime, seeFigure1(a)
for the average over 10 repetitions (the error bars indicate the estimated 95% quantile), behaves
as predicted by the complexity analysis. The proposed approach runs orders of magnitudes faster
than the quadratic time KSD estimator (6), and, for small samples sizes (nď500), N-KSD is the
fastest among all tested methods. From n“2000, all (near-)linear-time approaches are faster
(excluding FSSD-opt, which has a relatively large fixed cost). Still, N-KSD achieves competitive
runtime results even for n“5000.
Laplace vs. standard normal. We fix the target distribution P“ p0 ,I q and obtain
´ ¯ d d
N
d
n“1000 samples from the alternative Q“Lap 0,?1 , a product of d Laplace distributions.
2
We test H :Q“P vs. H :Q‰P with a level of α“0.05. We set the kernel parameters c and
0 1
β for KSD IMQ and N-KSD IMQ as per the recommendation for L1 IMQ in the corresponding
experiment by Huggins and Mackey [2018]. Figure 1(b) reports the power (obtained over 500
draws of the data) of the different approaches. KSD Gauss and its approximation N-KSD Gauss
perform similarly but their power diminishes from d “ 3. KSD IMQ achieves full power for
?
all tested dimensions and performs best overall. N-KSD IMQ (m“4 n) achieves comparable
results, with a small decline from D “15. Our proposed method outperforms all existing KSD
accelerations.
Student-t vs. standard normal. The setup is similar to that of the previous experiment,
but we consider samples from Q a multivariate student-t distribution with 5 degrees of freedom,
set n“2000, and repeat the experiment 250 times to estimate the power. We show the results in
Figure 1(c). All approaches employing the Gaussian kernel quickly loose in power; all techniques
utilizing the IMQ kernel, including N-KSD IMQ, achieve comparably high power throughout.
Restricted Boltzmann machine (RBM). Similar to Liu and Wang [2016], Jitkrittum
et al. [2017], we consider the case where the target P is the non-normalized density of an RBM;
the samples Qˆ are obtained from the same RBM perturbed by independent Gaussian noise
n
with variance σ2. For σ2 “0, H :Q“P holds, and for σ2 ą0, implying that the alternative
0
6Allthecodereplicatingourexperimentsisavailableathttps://github.com/FlopsKa/nystroem-ksd.
10N-KSDIMQ KSDIMQ L1IMQ FSSD-optGauss
N-KSDGauss KSDGauss L2SechExp FSSD-randGauss
1.0
101
0.8
100
0.6
10−1 0.4
10−2 0.2
0.0
0 2000 4000 5 10 15 20 5 10 15 20 0.00 0.02 0.04 0.06
Samplesn Dimensiond(Laplace) Dimensiond(Student-t) Perturbationσ(RBM)
(a) (b) (c) (d)
Figure 1: Comparison of goodness-of-fit tests w.r.t. their runtime and their power.
H : Q ‰ P holds, the goal is to detect that the n “ 1000 samples come from a forged RBM.
1
For the IMQ kernel (L1 IMQ, N-KSD IMQ, KSD IMQ), we set c“1 and β “´1{2. We show
the results in Figure 1(d), using 100 repetitions to obtain the power. KSD with the IMQ and
?
with the Gaussian kernel performs best. Our proposed Nyström-based method (m“4 n) nearly
matches its performance with both kernels while requiring only a fraction of the runtime. All
other approaches achieve less power for σ Pt0.02,0.04u.
These experiments demonstrate the efficiency of the proposed Nyström-KSD method.
Acknowledgements
This work was supported by the pilot program Core-Informatics of the Helmholtz Associa-
tion (HGF). BKS is partially supported by the National Science Foundation (NSF) CAREER
award DMS-1945396.
References
Andreas Anastasiou, Alessandro Barp, François-Xavier Briol, Bruno Ebner, Robert E Gaunt,
Fatemeh Ghaderinezhad, Jackson Gorham, Arthur Gretton, Christophe Ley, Qiang Liu, et al.
Stein’smethodmeetscomputationalstatistics: Areviewofsomerecentdevelopments.Statistical
Science, 38(1):120–139, 2023.
RitaGiulianoAntonini. SubgaussianrandomvariablesinHilbertspaces. RendicontidelSeminario
Matematico della Università di Padova, 98:89–99, 1997.
Nachman Aronszajn. Theory of reproducing kernels. Transactions of the American Mathematical
Society, 68:337–404, 1950.
Rémi Bardenet, Arnaud Doucet, and Christopher C. Holmes. Towards scaling up Markov chain
Monte Carlo: an adaptive subsampling approach. In International Conference on Machine
Learning (ICML), pages 405–413, 2014.
Ludwig Baringhaus and Carsten Franz. On a new multivariate two-sample test. Journal of
Multivariate Analysis, 88:190–206, 2004.
Ludwig Baringhaus and Norbert Henze. A consistent test for multivariate normality based on the
empirical characteristic function. Metrika. International Journal for Theoretical and Applied
Statistics, 35(6):339–348, 1988.
11
sdnocesniemitnuR
rewoPJerome Baum, Heishiro Kanagawa, and Arthur Gretton. A kernel Stein test of goodness of fit for
sequentialmodels. InInternational Conference on Machine Learning (ICML),pages1936–1953,
2023.
Alain Berlinet and Christine Thomas-Agnan. Reproducing Kernel Hilbert Spaces in Probability
and Statistics. Kluwer, 2004.
Clément Canonne. Tail bounds for maximum of sub-Gaussian random variables. Mathematics
Stack Exchange, 2021. https://math.stackexchange.com/q/4002713 (version: 2023-12-21).
Antoine Chatalic, Nicolas Schreuder, Alessandro Rudi, and Lorenzo Rosasco. Nyström kernel
meanembeddings. InInternational Conference on Machine Learning (ICML),pages3006–3024,
2022.
Louis H. Y. Chen. Stein’s method of normal approximation: Some recollections and reflections.
The Annals of Statistics, 49(4):1850–1863, 2021.
Wilson Ye Chen, Lester Mackey, Jackson Gorham, Francois-Xavier Briol, and Chris J. Oates.
Stein points. In International Conference on Machine Learning (ICML), pages 844–853, 2018.
WilsonYeChen,AlessandroBarp,Francois-XavierBriol,JacksonGorham,MarkGirolami,Lester
Mackey, and Chris Oates. Stein point Markov chain Monte Carlo. In International Conference
on Machine Learning (ICML), pages 1011–1021, 2019.
Kacper Chwialkowski, Aaditya Ramdas, Dino Sejdinovic, and Arthur Gretton. Fast two-sample
testingwithanalyticrepresentationsofprobabilitymeasures. InAdvancesinNeuralInformation
Processing Systems (NeurIPS), pages 1972–1980, 2015.
Kacper Chwialkowski, Heiko Strathmann, and Arthur Gretton. A kernel test of goodness of fit.
In International Conference on Machine Learning (ICML), pages 2606–2615, 2016.
Jeremie Coullon, Leah F. South, and Christopher Nemeth. Efficient and generalizable tuning
strategies for stochastic gradient MCMC. Statistics and Computing, 33(3):66, 2023.
Andrea Della Vecchia, Jaouad Mourtada, Ernesto De Vito, and Lorenzo Rosasco. Regularized
ERMonrandomsubspaces. InInternational Conference on Artificial Intelligence and Statistics
(AISTATS), pages 4006–4014, 2021.
Joseph Diestel and John Jerry Uhl. Vector Measures. American Mathematical Society, 1977.
TamaraFernandez,NicolasRivera,WenkaiXu,andArthurGretton. KernelizedSteindiscrepancy
testsofgoodness-of-fitfortime-to-eventdata. InInternational Conference on Machine Learning
(ICML), pages 3112–3122, 2020.
Ryoji Fukuda. Exponential integrability of sub-Gaussian vectors. Probability Theory and Related
Fields, 85(4):505–521, 1990.
Futoshi Futami, Zhenghang Cui, Issei Sato, and Masashi Sugiyama. Bayesian posterior approxi-
mation via greedy particle optimization. In AAAI Conference on Artificial Intelligence, pages
3606–3613, 2019.
Damien Garreau, Wittawat Jitkrittum, and Motonobu Kanagawa. Large sample analysis of the
median heuristic. Technical report, 2018. https://arxiv.org/abs/1707.07269.
12GeorgeGiorgobiani,VakhtangKvaratskhelia,andVajaTarieladze. Notesonsub-Gaussianrandom
elements. In Applications of Mathematics and Informatics in Natural Sciences and Engineering
(AMINSE), pages 197–203, 2020.
Jackson Gorham and Lester Mackey. Measuring sample quality with Stein’s method. In Advances
in Neural Information Processing Systems (NeurIPS), pages 226–234, 2015.
Jackson Gorham and Lester Mackey. Measuring sample quality with kernels. In International
Conference on Machine Learning (ICML), pages 1292–1301, 2017.
Jackson Gorham, Anant Raj, and Lester Mackey. Stochastic Stein discrepancies. In Advances in
Neural Information Processing Systems (NeurIPS), pages 17931–17942, 2020.
Arthur Gretton, Karsten Borgwardt, Malte Rasch, Bernhard Schölkopf, and Alexander Smola. A
kernel two-sample test. Journal of Machine Learning Research, 13(25):723–773, 2012.
Omar Hagrass, Bharath Sriperumbudur, and Krishnakumar Balasubramanian. Minimax optimal
goodness-of-fit testing with kernel Stein discrepancy. Technical report, 2024. https://arxiv.
org/abs/2404.08278.
Jonathan H. Huggins and Lester Mackey. Random feature Stein discrepancies. In Advances in
Neural Information Processing Systems (NeurIPS), pages 1899–1909, 2018.
Yuri I. Ingster and Irina A. Suslina. Nonparametric goodness-of-fit testing under Gaussian models.
Springer, 2003.
Muhammad Izzatullah, Ricardo Baptista, Lester Mackey, Youssef Marzouk, and Daniel Peter.
Bayesian seismic inversion: Measuring Langevin MCMC sample quality with kernels. In SEG
International Exposition and Annual Meeting, 2020.
Wittawat Jitkrittum, Wenkai Xu, Zoltán Szabó, Kenji Fukumizu, and Arthur Gretton. A
linear-time kernel goodness-of-fit test. In Advances in Neural Information Processing Systems
(NeurIPS), pages 262–271, 2017.
Wittawat Jitkrittum, Heishiro Kanagawa, and Bernhard Schölkopf. Testing goodness of fit of
conditional density models with kernels. In Conference on Uncertainty in Artificial Intelligence
(UAI), pages 221–230, 2020.
Florian Kalinke and Zoltán Szabó. Nyström M-Hilbert-Schmidt independence criterion. In
Conference on Uncertainty in Artificial Intelligence (UAI), pages 1005–1015, 2023.
Daphne Koller and Nir Friedman. Probabilistic graphical models. MIT Press, 2009.
Andrey N. Kolmogorov. Sulla determinazione empirica delle leggi di probabilita. Giornale
dell’Istituto Italiano degli Attuari, 4(1), 1933.
Vladimir Koltchinskii and Karim Lounici. Concentration inequalities and moment bounds for
sample covariance operators. Bernoulli, 23(1):110–133, 2017.
Anoop Korattikara, Yutian Chen, and Max Welling. Austerity in MCMC land: Cutting the
Metropolis-Hastings budget. In International Conference on Machine Learning (ICML), pages
181–189, 2014.
Erich L. Lehmann and Joseph P. Romano. Testing statistical hypotheses. Springer, 2021.
13Jen Ning Lim, Makoto Yamada, Bernhard Schölkopf, and Wittawat Jitkrittum. Kernel Stein
tests for multiple model comparison. In Advances in Neural Information Processing Systems
(NeurIPS), pages 2243–2253, 2019.
Qiang Liu and Dilin Wang. Stein variational gradient descent: A general purpose Bayesian
inference algorithm. In Advances in Neural Information Processing Systems (NeurIPS), pages
2378–2386, 2016.
Qiang Liu, Jason Lee, and Michael Jordan. A kernelized Stein discrepancy for goodness-of-fit
tests. In International Conference on Machine Learning (ICML), pages 276–284, 2016.
Diego Martinez-Taboada and Edward H Kennedy. Counterfactual density estimation using kernel
Stein discrepancies. Technical report, 2023. https://arxiv.org/abs/2309.16129.
Krikamol Muandet, Kenji Fukumizu, Bharath Sriperumbudur, Bernhard Schölkopf, et al. Kernel
mean embedding of distributions: A review and beyond. Foundations and Trends in Machine
Learning, 10(1-2):1–141, 2017.
Eric T. Nalisnick, Akihiro Matsukawa, Yee Whye Teh, and Balaji Lakshminarayanan. Detecting
out-of-distributioninputstodeepgenerativemodelsusingatestfortypicality. Technicalreport,
2019. http://arxiv.org/abs/1906.02994.
Chris J. Oates, Mark Girolami, and Nicolas Chopin. Control functionals for Monte Carlo
integration. Journal of the Royal Statistical Society: Series B, 79(3):695–718, 2017.
Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Advances
in Neural Information Processing Systems (NeurIPS), pages 1177–1184, 2007.
AlessandroRudi,RaffaelloCamoriano,andLorenzoRosasco.Lessismore: Nyströmcomputational
regularization. In Advances in Neural Information Processing Systems (NeurIPS), pages 1657–
1665, 2015.
Ruslan Salakhutdinov. Learning deep generative models. Annual Review of Statistics and Its
Application, 2:361–385, 2015.
Mahtab Sarvmaili, Hassan Sajjad, and Ga Wu. Data-centric prediction explanation via kernelized
Stein discrepancy. Technical report, 2024. https://arxiv.org/abs/2403.15576.
Dino Sejdinovic, Bharath Sriperumbudur, Arthur Gretton, and Kenji Fukumizu. Equivalence
of distance-based and RKHS-based statistics in hypothesis testing. Annals of Statistics, 41:
2263–2291, 2013.
Nikolai V. Smirnov. Table for estimating the goodness of fit of empirical distributions. Annals of
Mathematical Statistics, 19:279–281, 1948.
Alexander Smola, Arthur Gretton, Le Song, and Bernhard Schölkopf. A Hilbert space embedding
for distributions. In Algorithmic Learning Theory (ALT), pages 13–31, 2007.
Bharath K. Sriperumbudur and Nicholas Sterge. Approximate kernel PCA: computational versus
statistical trade-off. The Annals of Statistics, 50(5):2713–2736, 2022.
Charles Stein. A bound for the error in the normal approximation to the distribution of a sum of
dependentrandomvariables. InBerkeleySymposiumonMathematicalStatisticsandProbability,
pages 583–602, 1972.
14Ingo Steinwart and Andreas Christmann. Support Vector Machines. Springer, 2008.
Nicholas Sterge and Bharath K. Sriperumbudur. Statistical optimality and computational
efficiency of Nyström kernel PCA. Journal of Machine Learning Research, 23(337):1–32, 2022.
Gábor Székely and Maria Rizzo. Testing for equal distributions in high dimension. InterStat, 5:
1249–1272, 2004.
Gábor Székely and Maria Rizzo. A new test for multivariate normality. Journal of Multivariate
Analysis, 93:58–80, 2005.
Michel Talagrand. Regularity of Gaussian processes. Acta Mathematica, 159(1-2):99–149, 1987.
Aad van der Vaart and Jon Wellner. Weak Convergence and Empirical Processes: With Applica-
tions to Statistics. Springer, 1996.
Roman Vershynin. High-Dimensional Probability. Cambridge University Press, 2018.
Max Welling and Yee Whye Teh. Bayesian learning via stochastic gradient Langevin dynamics.
In International Conference on Machine Learning (ICML), pages 681–688, 2011.
Christopher Williams and Matthias Seeger. Using the Nyström method to speed up kernel
machines. In Advances in Neural Information Processing Systems (NeurIPS), pages 682–688,
2001.
George Wynne, Mikołaj Kasprzak, and Andrew B Duncan. A spectral representation of kernel
Stein discrepancy with application to goodness-of-fit tests for measures on infinite dimensional
Hilbert spaces. Bernoulli, 2024. (to appear).
Wenkai Xu and Gesine Reinert. A Stein goodness-of-test for exponential random graph models.
In International Conference on Artificial Intelligence and Statistics (AISTATS), pages 415–423,
2021.
Jiasen Yang, Qiang Liu, Vinayak Rao, and Jennifer Neville. Goodness-of-fit testing for discrete
distributions via Stein discrepancy. In International Conference on Machine Learning (ICML),
pages 5561–5570, 2018.
Jiasen Yang, Vinayak A. Rao, and Jennifer Neville. A Stein-Papangelou goodness-of-fit test for
pointprocesses. InInternationalConferenceonArtificialIntelligenceandStatistics(AISTATS),
pages 226–235, 2019.
Vadim Yurinsky. Sums and Gaussian vectors. Springer, 1995.
A Proofs
This section is dedicated to the proofs of our results in the main text. Lemma 1 is proved in
Section A.1. We prove our main result (Theorem 2) in Section A.2; Corollary 1 is shown in
Section A.3.
15A.1 Proof of Lemma 1
By (4), KSD is the norm of the mean embedding of Q under h p¨,¨q, that is,
p
›ż ›
› › › ›
S ppQq“› › Rdh pp¨,xqdQpxq› ›
Hhp
“› µ hppQq›
Hhp
. (A.1)
Hence, with Chatalic et al. [2022, (5)], the optimization problem (8) has the solution α “
pα qm “ 1K´ K 1 PRm. Now, using (A.1), we have
i i“1 n hp,m,m hp,m,n n
› › C G
› ›ÿm › ›2 ÿm ÿm
› α h p¨,x˜ q› p “aq α h p¨,x˜ q, α h p¨,x˜ q
› i p i › i p i i p i
i“1 Hhp,m i“1 i“1 Hhp,m
ÿm ÿm ÿm ÿm
p “bq α α xh p¨,x˜ q,h p¨,x˜ qy p “cq α α h px˜ ,x˜ qp “dq αTK α
i j p i p j Hhp,m i j p i j hp,m,m
i“1j“1 i“1j“1
p “eq 1 1TK K´ K K´ K 1 “βTK´ β .
n2 n hp,n,mloohoo po,omoo,omoooooohopm,moo,omooooohoo po,omooo,omn hp,m,n n p hp,m,m p
“K´
hp,m,m
In (a) we used that }¨} is inner product induced, (b) follows from the linearity of the inner
Hhp,m
product, (c) is implied by the reproducing property, (d) is by the definition the Gram matrix,
in (e) we made use of the explicit form of α, the symmetry of Gram matrices, the property
KT “K , and that the Moore-Penrose inverse satisfies A´AA´ “A´ for any matrix
A.hp,m,n hp,n,m
A.2 Proof of Theorem 2
Contrasting the existing related work [Rudi et al., 2015, Chatalic et al., 2022, Sterge and
Sriperumbudur, 2022, Kalinke and Szabó, 2023], we do not impose a boundedness assumption on
thefeaturemap. Thisrelaxationleadstonewtechnicaldifficultiesthatweresolveinthefollowing.
Still, we start our analysis from a known decomposition Chatalic et al. [2022, Lemma 4.1].
´ ¯ ´ ¯
To simplify notation, let µ :“ µ pQq, µˆ :“ µ Qˆ , and µˆNys :“ P µ Qˆ .
hp hp hp hp n hp Hhp,m hp n
First, we decompose the error as follows.
ˇ ˇ ˇS ppQ
p
ďcq q´
› ›
µS˜ p´ ´Qˆ µˆn¯ˇ ˇ ˇ
›
›p “aqˇ ˇ ˇ ˇ› › `µ
h
› ›
›p
µˆ› ›
Hh ´p
´ µˆN› › ›µ yˆ sN
h
› ›
›pys› › › Hhpˇ ˇ ˇ ˇp ďbq› › ›µ
hp
´µˆN hpys› › ›
Hhp
hp hp Hhp › hp hp Hhp˜ ¸›
p “dq› › µ ´µˆ › › `› › ›´ I´P ¯ µˆ ´ 1 ÿm h p¨,x˜ q › › ›
hp hp Hhp › Hhp,m hp m p i ›
›i“1 ˜ Hhp ¸›
p ďeq› › µ ´µˆ › › `› › ›´ I´P ¯ C1{2 › › › › › ›C´1{2 µˆ ´ 1 ÿm h p¨,x˜ q › › › (A..2)
loohoopoooomhoopoooHoo ho pn looooooooooH ooohopoo,mmoooooQ oo,ohoopo,oλooooonp› Q,hp,λ hp m p i ›
looooooooooooooooooooooooomio“oo1ooooooooooooooooooHooo ho pn
“:t1 “:t2
“:t3
( aa n) gi ls ei im nep ql uie ad litb yy y( iA el. d1) (ca )n ;d in( (9 d) ); ,( wb e) ufo sl elo tw hs edfr io sm tribth ue tivre ev pe rr ose pet rr ti yan ag nl de i in nte rq ou da uli ct ey; 1˘řµˆ
h mp
a hnd pt ¨,h x˜e t qr Pi-
m i“1 p i
16whoseprojectionontotheorthogonalcomplementof iszero;in(e)I “C1{2 C´1{2
wHah sp, im
ntroduced and we used the definition of the
operatorHnh op r, mm
.
Q,hp,λ Q,hp,λ
We next obtain individual probabilistic bounds for the three terms t , t , and t , which we
1 2 3
subsequently combine by union bound. We will then conclude the proof by showing that all
assumptions that we imposed along the way are satisfied.
• T ule ar tm iont c1 o. unT teh re pafi rr tst µter .m Tm oe ba os uu nre ds tt hh ie s d de ev vi ia at ti io on n o › › µˆf an ´e µmp › ›irical “me › › ›a 1n řµˆ nhp rt ho pi ¨t ,s xp qo ´p-
›
›
hp hp hp Hhp n i“1 p i
lE oXoo„ooQoohmpopoo¨,ooXoonqs› , we will use the Bernstein inequality (Theorem D.4) with the η
i
:“
Hhp
“0
h p¨,x q P pi P rnsq centered random variables, by gaining control on the moments of
Yp :“}hi p¨,H Xh qp } . This is what we elaborate next.
p Hhp
By Assumption 1(ii), Y is sub-Gaussian; hence it is also sub-exponential (Lemma D.2(3)), and
therefore (Lemma B.2) it satisfies the Bernstein condition
1 ?
E|Y|p ď p!σ2Bp´2 ă8, with σ “ 2}Y} , B “}Y} ,
2 ψ1 ψ1
for any pě2. Notice that
´ ¯
B “}Y} ψ1 p Àaq }Y} ψ2 p Àbq }Y} L2pQq p “cq E X„Q b}h pp¨,Xq}2 Hhp 1{2
` ˘
p “dq rE X„Qtrph pp¨,Xqbh pp¨,Xqqs1{2 p “eq tr CQ,hp ,
(a)followsfromLemmaD.2(3),(b)isimpliedbyAssumption1(ii),(c)comesfromthedefinition
of Y, in (d) we used Lemma D.1. Steinwart and Christmann [2008, (A.32)] allow bs commuting
` ˘
tr and integration, which, with (1), yields (e). As σ —B, we also got that σ À tr CQ,hp .
Having obtained a bound on the moments, we can apply Bernstein’s inequality for separable
Hilbert spaces (Yurinsky 1995; recalled in Theorem D.4) to the centered η “h p¨,x qP -s
(iPrns), and obtain that for any δ Pp0,1q it holds with probability at
lei
ast
1p ´δ{3i thaHthp
› › b ` ˘ d ` ˘
› › µ ´µˆ › › “› › ›1 ÿn η › › › À tr CQ,hp logp6{δq ` tr CQ,hp logp6{δq . (A.3)
hp hp Hhp ›n i› n n
i“1 Hhp
› ›
• Term t 2. Assume that 0 ă λ ď › CQ,hp› op. Then, we can handle the second term with
Lemma B.1 and obtain that for any δ Pp0,1q it holds with probability at least 1´δ{3 that
›´ ¯ ›
› › ?
› I´P C1{2 › À λ (A.4)
Hhp,m Q,hp,λ
op
" *
provided that mÁmax
trpCQ,hpq
,logp3{δq .
λ
• Term t . The third term depends on the sample px qn i. „i.d.Q and on the Nyström selection
3 i i“1
pi qm i. „i.d. Unifprnsq “: Λ; with this notation x˜ “ x (j P rms). Our goal is to take both
j j“1 j ij
sources of randomness into account.
17Fixed x -s, randomness in i -s: Let the sample px qn be fixed. As the px qm -s are
i j i i“1 ij j“1
i.i.d.,
› ˜ ¸› › ›
› › 1 ÿm › › › › 1 ÿm ” ` ˘ı› ›
t “›C´1{2 µˆ ´ h p¨,x˜ q › “› C´1{2 h p¨,x˜ q´µˆ ›
3 › Q,hp,λ hp m p i › ›m loooQoo,hoopo,oλoooooopomooooioooooooohoopoon›
i“1 Hhp i“1 Hhp
ř
:“Yi
measures the concentration of the sum 1 m Y around its expectation, which is zero as
m i“1 i
E rh p¨,x qs“µˆ with J „Λ. Notice that
J p J hp
› ›
› ` ˘›
max}Y } “ max›C´1{2 h p¨,x˜ q´µˆ ›
iPrms
i Hhp
iPrms«
Q,hp,λ p i hp
Hhp ff
›
›
›
› 1
ÿn ›
›
›
›
ď max ›C´1{2 h p¨,x˜ q› ` ›C´1{2 h p¨,x q›
iPrms
›
Q,hp,λ p i
›Hhp n
j“1
Q,hp,λ p j
Hhp
› ›
ď2max›C´1{2 h p¨,x q› “:K “Kpx ,...,x q,
iPrns
Q,hp,λ p i
Hhp
1 n
where we used the triangle inequality and homogeneity of the norm. An application of
Theorem D.5 yields that, conditioned on the sample px qn , it holds that
˜ a ˇi i“1 ¸
ˇ
2logp12{δq ˇ δ
Λm pi qm : t ďK ? ˇ px qn ě1´ . (A.5)
j j“1 3 m ˇ i i“1 6
› ›
› ›
Randomness in x -s: Let Z :“ ›C´1{2 h p¨,x q› (i P rns) with px qn i. „i.d. Q. By
i i Q,hp,λ p i
Hhp
i i“1
Assumpt ›ion 1(i ›i) and Lemma B.3, the Z i-s are sub-Gaussian random variable provided that
0ăλď› CQ,hp› op. Hence, by Lemma B.6, with probability at least 1´δ{6, it holds that
b
K “2max|Z |À }Z }2 logp12n{δq.
iPrns
i 1 ψ2
We now bound }Z }2 :
1 ψ2
}Z 1}2 ψ2p “aq› › › ›› › ›C Q´ ,1 h{ p2 ,λh pp¨,x 1q› › › Hhp› › › ›2 ψ2p Àbq› › › ›› › ›C Q´ ,1 h{ p2 ,λh pp¨,x 1q› › › Hhp› › › ›2 L2pQqp “cq tr´ C Q´ ,1 hp,λCQ,hp¯ .
(a) follows from the definition of Z . (b) is implied by Assumption 1(ii) and Lemma B.3. (c)
1
comes from Lemma B.4 with A:“C´1 . We have shown that
ˆ
cQ,hp ´,λ
¯ ˙
δ
Qn px iqn
i“1
: K À tr C Q´ ,1 hp,λCQ,hp logp12n{δq ě1´ 6. (A.6)
Combination: We now combine the intermediate results. Let
$ c ´ ¯ ,
A“’’&´
px qn ,pi qm
¯
: t À
tr C Q´ ,1 hp,λCQ,hp
?logp12n{δqlogp12{δq//.
,
’’% i i“1 j j“1 3 m //-
" c ´ ¯ *
B “ px iqn
i“1
: K À tr C Q´ ,1 hp,λCQ,hp logp12n{δq ,
# a +
´ ¯
2logp12{δq
C “ px qn ,pi qm : t ďK ? ĎA.
i i“1 j j“1 3 m
18Then, with QnbΛm denoting the product measure of Qn and Λm, we obtain
ż
pQnbΛmqpAq“E QnrΛmpA|px iqn i“1qs“ ΛmpA|px iqn i“1qdQnpx 1,...,x nq
ż pRdżqn
ě ΛmpA|px qn qdQnpx ,...,x qě ΛmpC |px qn qdQnpx ,...,x q
i i“1 1 n i i“1 1 n
ˆB ˙ B
paq δ pbq
ě 1´ QnpBq ě p1´δ{6q2 “1´δ{3`δ2{62 ą1´δ{3. (A.7)
6
(a) is implied by the uniform lower bound established in (A.5). (b) was shown in (A.6).
Combination of t 1, t 2, and t 3. To conclude, we ´use decompos ¯ition (A.2), and union bound
(A.3), (A.4), and (A.7). Further, we observe that tr C Q´ ,1 hp,λCQ,hp “ NQ,hppλq, and obtain that
with probability at least 1´δ it holds that
b ` ˘ d ` ˘
ˇ ´ ¯ˇ
ˇ
ˇS pQq´S˜ Qˆ
ˇ
ˇÀ
tr CQ,hp logp6{δq
`
tr CQ,hp logp6{δq
`
p p n
n n
c
`
λ NQ,hppλqlogp12n{δqlogp12{δq
m
" *
› ›
provided that m Á max trpC λQ,hpq ,logp3{δq and 0 ă λ ď › CQ,hp›
op
both hold. Now, spe-
cializing λ! “ ctrpC mQ,h `pq for ˘s ›ome a ›bso )lute constant c ą 0, all constraints are satisfied for
mÁmax logp3{δq,tr CQ,hp › CQ,hp› o´ p1 . Using our choice of λ, after rearranging, we get the
stated claim.
A.3 Proof of Corollary 1
The proof is split into two parts. The first one considers the polynomial decay assumption, the
second one is about the exponential decay assumption.
?
• Polynomialdecay. The n-consistencyofthefirsttwoaddendsinTheorem2wasestablished
inthediscussionfollowingthestatement. Hence,welimitourconsiderationstothelastaddend.
A ths esu lam ste at dh dat enNdQ i, nhp Tpλ hq eoÀ reλ m´γ 2f yo ir elγ dsP tp h0 a, t1s. Observing that the trace expression is constant,
g
f ˆ ˙
f
f elogp12{δqlogp12n{δq NQ,hp ctrpC mQ,hpq paqc
logp12{δqlogp12n{δq pbq
ˆ
1
˙
À “ ? ,
m2 m2´γ O n
with (a) implied by the polynomial decay assumption and (b) follows from our choice of
mÁn2´1
γ
log2´1 γp12n{δqlog2´1 γp12{δq. This derivation confirms the first stated result.
• Exponential decay. Assume it holds that NQ,hppλq À logp1``c 1{λq˘. Observe that as per
the discussion following Theorem 2, the first two addends are n´1{2 . For the last addend,
O
19again noticing that the trace is constant, we have
g g
f ˆ ˙ f ˆ ˙
f f
f elogp12{δqlogp12n{δq NQ,hp ctrpC mQ,hpq paqf elogp12{δqlogp12n{δqlog 1` ctrpc C1m
Q,hpq
À
m2 m2
g
f ˆ ˙
f
f elogp12{δqlogp12n{δqlog 1` c1n ˆ ˙
p Àbq ctrpCQ,hpq p “cq ?1
,
m2 O n
where (a) uses the exponential decay assumption. (b) uses that něm and that the logarithm
is a monotonically increasing function. (c) follows from our choice of
g
f ˜ ¸
f
mÁ? ne log 1` `c 1n ˘ logp12n{δqlogp12{δq,
ctr CQ,hp
finishing the proof of the corollary.
B Auxiliary results
Thissectioncollectsourauxiliaryresults. LemmaB.1buildsonRudietal.[2015,Lemma6],which
assumes bounded feature maps, and on Della Vecchia et al. [2021, Lemma 5], which is stated in
the context of leverage scores. Lemma B.2 states that a sub-exponential random variable satisfies
Bernstein’s conditions, and Lemma B.3 shows that a “whitened” sub-Gaussian vector remains
sub-Gaussian. Lemma B.4 expresses the L -norm of the RKHS-norm as trace. In Lemma B.5,
2
we show how tensor products interplay with linearly transformed vectors. Lemma B.6 is about
the maximum of real-valued sub-Gaussian random variables; it is a slightly altered restatement of
Canonne [2021].
› ›
› ›
Lemma B.1. Let Assumption 1 hold, and assume 0ăλď CQ,hp op. Then, for any δ Pp0,1q,
it holds with probability at least 1´δ that
›´ ¯ ›
› ›2
› I´P C1{2 › Àλ
Hhp,m Q,hp,λ
op
" *
provided that mÁmax
trpCQ,hpq
,logp1{δq .
λ
ř
Proof. Let the covariance of the Nyström samples be C˜ “ 1 m h p¨,x˜ qbh p¨,x˜ q,
´ ´ ¯ ¯ Qˆ n,hp,m m i“1 p i p i
and βpλq “ λ
max
C Q´ ,1 h{ p2
,λ
CQ,hp ´C˜
Qˆ n,hp,m
C Q´ ,1 h{ p2
,λ
. The application of Rudi et al. [2015,
Proposition 3 and Proposition 7] (recalled in Theorem D.6) yields the non-probabilistic inequality
›´ ¯ ›
› ›2 λ
› I´P C1{2 › ď , (B.8)
Hhp,m Q,hp,λ op 1´βpλq
when βpλqă1. βpλq measures the deviation—in the sense of λ p¨q—of an empirical covariance
max
to the population one. We will establish that our setting satisfies the conditions of Koltchinskii
20and Lounici [2017, Theorem 9] (recalled in Theorem D.3), with which we will show that the
stronger requirement
› ´ ¯ ›
paq› ›
βpλq ď ››C Q´ ,1 h{ p2
,λ
CQ,hp ´C˜
Qˆ n,hp,m
C Q´ ,1 h{ p2 ,λ›
op ›
› ›
p “bq› ›C Q´ ,1 h{ p2 ,λCQ,hpC Q´ ,1 h{ p2 ,λ´C Q´ ,1 h{ p2 ,λC˜
Qˆ
n,hp,mC Q´ ,1 h{ p2 ,λ›
op ›
p
“cq›
›
›
›m1
ÿm
C Q´ ,1 h{ p2 ,λh pp¨,x˜ iqbh pp¨,x˜ iqC Q´ ,1 h{ p2 ,λ´C Q´ ,1 h{ p2 ,λCQ,hpC Q´ ,1 h{ p2
,λ›
›
›
›
ă1 (B.9)
i“1 op
issatisfiedformlargeenough. In(a),weusedthatthespectralradiusisboundedbytheoperator
norm. (b) is the distributive property, and (c) uses linearity.
In the following we prove (B.9). Let the Nyström selection be pi qm i. „i.d. Unifprnsq; with
j j“1
this notation x˜ “ x (j P rmsq. First, we condition on a fixed choice of pi qm -s. Let us
j ij j j“1
define the Y “ C´1{2 h p¨,Xq P random variable with X „ Q, and consider the samples
`Q,hp,λ˘ p Hhp
y :“C´1{2 h ¨,x P . Y has covariance
ij Q,hp,λ p ij Hhp
” ı
E X„QrY bYsp “aqE X„Q C Q´ ,1 h{ p2 ,λh pp¨,Xqbh pp¨,XqC Q´ ,1 h{ p2 ,λ p “bq C Q´ ,1 h{ p2 ,λCQ,hpC Q´ ,1 h{ p2 ,λ,
where (a) is implied by Lemma B.5 and (b) follows from Steinwart and Christmann [2008, (A.32)]
by swapping the expectation with bounded linear operators. Similarly, we observe that the
ř
empirical covariance is 1 m y by “ C´1{2 C˜ C´1{2 . In other words, this setup
brings us into the
settingm
of
Tj“ h1 eorij
em
Di .j
3,
andQ, ih tp, rλ emQˆ anin,h sp, tm
o
shQ o,h wp,λ
that Y is sub-Gaussian in the
sense of (2). Notice that Y satisfies both conditions:
• Mean-zero: By P“Q and (5), EY “C´1{2 Eh p¨,Xq“0.
Q›,hp,λ p › › ›
› › › ›
• (2) holds: Our goal now is to show that ›xY,uy › À›xY,uy › ă8 holds for all
Hhp ψ2 Hhp L2pQq
u P . Let u P be arbitrary, and v “ C´1{2 u P ; v is well-defined due to the
inverHtih bp ility of CQ,hH p,h λp . Using this notation, we hQ a, vh ep,λ Hhp
› › ›xY,u pp “ “fcy q qH › › ›
› › ›
›h x Ap h› › › Cpψ Q´p2 ¨
,1
h,p “ {a X p2q ,λq› › › › hA , pvC py ¨Q H´ ,,1 Xhh{ pp2 q› › ›,λ
ψ
,h
2
up Àp d Eqp H¨ › › ›, x hX h
p› › ›
›pq Lp, 2¨u , pQXE qH q
p
“h g,p qv› › › ›
› ›
›yψ xH2
Yh
,pp “b u› › ›q
L
y› › › › H2A
p
hQh pqp “p
› ›
›e Lqp¨ 2› › › ›, pAX Qh qq p
p
ă, hpC q¨, 8Q´ X,1 h .{ qp2 ,,λ Cu Q´E ,1 hH { p2h ,λp u› › › › ψ E2 Hhp› › › ›
L2pQq
In (a), we use the definition of Y. (b) follows from the self-adjointness of C´1{2 . (c) uses
Q,hp,λ
the definition of v. (d) is implied by Assumption 1(i) holding for all v P . In (e) we
Hhp
again used the definition of v. (f) is implied by the self-adjointness of C´1{2 . (g) is based
›
›
›
›
Q,hp,λ
on the definition of Y. (h) holds as ›xh p¨,Xq,vy › ă8 by Assumption 1(i) and the
expressions in-between were
equalities.p
This
estabH lih sp heL s2tpQ hq
e sub-Gaussianity of Y.
Now, we can invoke Koltchinskii and Lounici [2017, Theorem 9] (recalled in Theorem D.3),
21and, by using (B.9), we obtain that with probability at least 1´δ it holds that
$g ,
βpλqÀ› ›
›C Q´ ,1 h{ p2 ,λCQ,hpC Q´ ,1 h{ p2
,λ› ›
›
opmax’’& ’’%f f er´ C Q´ ,1 h{ p2 ,λC mQ,hpC Q´ ,1 h{ p2 ,λ¯ ,c logp m1{δq//.
//-
g
f ´ ¯
“max# l› › ›C oooQ´ oo,1 ho{ op2 o,oλooC ooQ oo, mhpoC oooQ´ o,o1 ho{ op2 o,oλoo› › › o1 ooo{ pn2f etr C Q´ ,1 h mp,λCQ,hp ,
ă1 c +
› ›
› › logp1{δq
l›C ooQ o´ o,o1 ho{ op2 o,oλooC ooQ om,hpoC oooQ´ oo,1 ho{ opo2 ,oλoo›
ooonp m
(B.10)
ă1
! ´ ¯ )
prov Wi ›d eed net xh tat simm pě lifm ya tx her
r
›eqC uQ´ ir,1 eh{ mp2 ,λ eC ntQ s,h op nC Q m´ ,1 h .{ p2 , Bλ y, ul so ig np g1{ tδ hq at, 0w ăher λe ďrpA ›
›
Cq Q:“ ,hp}› ›t Ar op p}A
op
“q.
: λ 1, we have
› ›
that ›C Q´ ,1 h{ p2 ,λCQ,hpC Q´ ,1 h{ p2 ,λ›
op
“ λ1λ `1
λ
ě 1 2. Now also observing that
´ ¯ ` ˘
1
tr C Q´ ,1 hpλCQ,hp ď λtr CQ,hp (B.11)
and by upper bounding the nominator and lower bounding the denominator in r:
´ ¯
r´ C Q´ ,1 h{ p2 ,λCQ,hpC Q´ ,1 h{ p2 ,λ¯ “ t › › ›r C Q´C ,1 hQ´ { p2,1 ,h λ{ p2 C,λ QC ,hQ p,h Cp Q´C ,1 hQ´ { p2,1 ,h λ{ p2 › › ›,λ
op
ď trpC λ 1 2Q,hpq ,
␣ ` ˘ (
we can take m ě max λ2 tr CQ,hp ,logp1{δq . To sum up, we got by (B.10) and (B.11) that,
conditioned on the Nyström selection, with probability of at least 1´δ it holds that
# ` ˘ +
› ´ ¯ ›
› ›C Q´ ,1 h{ p2 ,λ CQ,hp ´C˜ Q,hp,m C Q´ ,1 h{ p2 ,λ› ›2
op
Àmax tr C λmQ,hp ,logp m1{δq .
We remove the conditioning by taking the expectation over all Nyström selections. For
" *
mÁmax
trpCQ,hpq
,logp1{δq , we have that βpλqă1, which, together with (B.8), implies the
λ
stated result.
Lemma B.2 (Sub-exponential satisfies Bernstein conditions). Let Y be a real-valued random
?
variable which is sub-exponential, i.e. }Y} ă 8. Let σ :“ 2}Y} , B :“ }Y} ą 0. Then
ψ1 ψ1 ψ1
the Bernstein condition
1
E|Y|p ď p!σ2Bp´2 ă8
2
holds for any pě2.
Proof. For any pě2, we have
„ ˆ ˙ ȷ ´ ¯
|Y|p paq |Y| 1 ? 2
E|Y|p “p!BpE ă p!Bp Eexp ´1 “ p!Bp´2 2B ,
Bpp! looooooooooomBooooooooooon 2
pbq
ď1
22where in (a) we use that xn ăex´1 holds for all n,xą0, (b) follows from the definition of the
n!
sub-exponential Orlicz norm.
The next lemma shows that the “whitened” random variable C´1{2 h p¨,Xq inherits Assump-
Q,hp,λ p
tion 1(ii) from h p¨,Xq.
p
Lemma B.3. LetAssump› ›tion1(› ›ii)hold. LetX „Q, CQ,hp bethecovarianceoperator, CQ,hp,λ “
CQ,hp `λI, and 0ăλď CQ,hp op. Then, there exists an absolute constant cą0 such that
› › › ›› › ›C Q´ ,1 h{ p2 ,λh pp¨,Xq› › › Hhp› › › ›
ψ2
p ď:q c› › › ›› › ›C Q´ ,1 h{ p2 ,λh pp¨,Xq› › › Hhp› › › ›
L2pQq
p ă;q 8.
Proof. We will refer to the ’ď’ in the statement by p:q, and to the ’ă’ by p;q. First, we observe
that by the imposed Assumption 1(ii), there exists a constant c ą0 such that
1
› › › ›
› › › ›
›}h p¨,Xq} › ďc ›}h p¨,Xq} › ă8. (B.12)
p Hhp ψ2 1 p Hhp L2pQq
which, by the definition of the }¨} -norm, implies that
ψ2
¨ ˛
˚ }h p¨,Xq}2 ‹
E X„Qexp˚ ˝ ›
›
p Hh› ›p
2
‹ ‚ď2. (B.13)
c2›}h p¨,Xq} ›
1 p Hhp L2pQq
Our goal for p:q is to prove that there exists cą0 such that
› ›
› ›2
›C´1{2 h p¨,Xq›
E X„QexppfpX;cqqď2, with fpX;cq:“ c2› › › ›› › ›C Q´ ,Q 1 h{, ph 2 ,p λ, hλ pp p¨,Xq› › › HH hph› › › ›p 2 L2pQq,
and we will show p;q along the way. We bound the terms in the numerator and the denominator
o •f f Up pX p; ec rq bs oe up nar dat oe nly › › ›, Ca Q´n ,d 1 h{ p2c ,λh ho pos pe ¨,c Xą q› › ›2 H0 ha pt ,t ah ne de › › › ›n › › ›d C. Q´ ,1 h{ p2 ,λh pp¨,Xq› › › Hhp› › › ›
L2pQq
ă8:
› › › › › ›
› ›C´1{2 h p¨,Xq› ›2 p ďaq› ›C´1{2 › ›2 }h p¨,Xq}2 p “bq› ›C´1 › › }h p¨,Xq}2 .(B.14)
Q,hp,λ p
Hhp
Q,hp,λ
op
p Hhp Q,hp,λ
op
p Hhp
› › › ›
› › › ›
(a) is implied by ›C´1{2 h p¨,Xq› ď ›C´1{2 › }h p¨,Xq} following from the defi-
Q,hp,λ p
Hhp
Q,h ›p,λ
op
›p
›
Hhp
›
› ›2 › ›
nition of the operator norm, (b) comes from ›C´1{2 › “ ›C´1 › holding by the C˚
Q,hp,λ
op
Q,hp,λ
op
property and the self-adjointness of C´1{2 . Specifically, we got that
Q,hp,λ
› › › ›› › ›C Q´ ,1 h{ p2 ,λh pp¨,Xq› › › Hhp› › › ›
L2pQq
p ďaq c› › › ›c ›› › ›C Q´ ,1
hp,λ
›› › ›
op
›}h pp¨,Xq}
Hhp
›› › › ›
L2pQq
p “bq › ›C´1 › › › ›}h p¨,Xq} › › p ăcq 8,
Q,hp,λ op p Hhp L2pQq
23where (a) holds by (B.14) and the monotonicity of the integration, (b) is implied by the
› ›
› ›
homogeneity of norms. (c), i.e., p;q, holds since ›}h p¨,Xq} › ă8 by our assumption
p Hhp L2pQq
recalled in (B.12).
• Lower bound on › › › ›› › ›C Q´ ,1 h{ p2 ,λh pp¨,Xq› › › Hhp› › › ›2 L2pQq:
› › › ›› › ›C Q´ ,1 h{ p2 ,λh pp¨,Xq› › › Hhp› › › ›2
L2pQq
p “aq tr´ C Q´ ,1 hp,λCQ,hp¯ p “bq iÿ
PI
λ
iλ `i
λ
p ěcq iÿ
PI
2› › CQλ ,i hp› ›
op
p “dq t › ›rpCQ,h› ›pq . (B.15)
2 CQ,hp
op
( aa r) ei ts hs eta et iged eni vn alL ue em
s
m ofa CB Q. ,4 hp. .(b pc) qh io sld ims pb ly iet dhe byei λg ien ďva › ›l Cue Q- ,b hpa › ›s oe pd hfo
o
›r ldm ino gf ›ft oh re at lr l, pw
λ
ih qe iPr Ie p bλ yiq
ti hPI
e
compact positive property of CQ,hp and the assumption 0ăλď› CQ,hp› op. pdq follows again
from the eigenvalue-based form of the tr.
One can now rewrite trpCQ,hpq in the resulting expression (B.15) as
„ż ȷ ż
trpCQ,hpqp “aq tr h pp¨,xqbh pp¨,xqdQpxq p “bq trrh pp¨,xqbh pp¨,xqsdQpxq
ż Rd › Rd ›
p “cq }h p¨,xq}2 dQpxqp “dq› ›}h p¨,Xqq} › ›2 .
Rd p Hhp p Hhp L2pQq
In (a) we used the definition of CQ,hp, in (b) we flipped the integration and the tr [Steinwart
and Christmann, 2008, (A.32)]. (c) follows from Lemma D.1 by choosing f :“h p¨,xq, in (d)
p
we used the definition of }¨} .
L2pQq
Combining this form with (B.15), we got the lower bound
› ›
› › › ›› › ›C Q´ ,1 h{ p2 ,λh pp¨,Xq› › › Hhp› › › ›2
L2pQq
ě › ›}h pp 2¨, › ›X CQq} ,hH ph› ›p op› ›2 L2pQq . (B.16)
Taking into account the upper bound (B.14) and the lower bound (B.16), we arrived at
fpX;cqď
2› › CQ,hp› ›
o
›p› › ›C Q´ ,1 hp,λ› › › op} ›h pp¨,Xq}2
Hhp
,
› ›2
c2›}h p¨,Xq} ›
p Hd
k L2pQq
which by the monotonicity of the exponential function, means that
¨ ˛
E X„QexppfpX;cqqďE X„Qexp˚ ˚ ˚ ˚ ˝l2 oo› › oC ooQ oo, oh oop o› › ooo op mc2› › › oC ooQ´ oo,1 oh op oo, oλ o› › › ooo op n›
›
}h pp¨,Xq}2 H › ›h 2p ‹ ‹ ‹ ‹ ‚p ďaq 2.
›}h p¨,Xq} ›
“ c1
2
1
p Hhp L2pQq
In paq, we set c2 “2› › CQ,hp› › op› › ›C Q´ ,1 hp,λ› › › opc2
1
and used (B.13).
24ş
oL pe em ram tora
.
B Th.4 en. › › ›L › ›e At 1X {2h„ pp¨Q ,X, C q› ›Q H,h hpp› › ›2 L“ 2pQR qd “h p trp¨ `, Ax Cqb Q,hh pp ˘ .p¨,xqdQpxq, and A P Lp Hkq a positive
Proof. We have the chain of equalities
› › › ›› › ›A1{2h pp¨,Xq› › › › › › ›2 p “aqż › › ›A1{2h pp¨,xq› › ›2 dQpxq
ż ”´Hhp L2pQq ¯Rd ´ ¯H ıd k
p “bq tr A1{2h p¨,xq b A1{2h p¨,xq dQpxq
p p
R„dż ´ ¯ ´ ¯ ȷ
p “cq tr A1{2h p¨,xq b A1{2h p¨,xq dQpxq
p p
„żRd ȷ
p “dq tr A1{2ph p¨,xqbh p¨,xqqA1{2dQpxq
p p
„ Rd ˆż ˙ ȷ ´ ¯
p “eq tr A1{2 h pp¨,xqbh pp¨,xqdQpxq A1{2 p “fq tr A1{2CQ,hpA1{2
Rd
` ˘
pgq
“ tr ACQ,hp .
The details are as follows. In (a) we used the definition of }¨} , (b) follows from Lemma D.1
L2pQq
by choosing f :“ A1{2h p¨,xq, in (c) the integration was swapped with the tr [Steinwart and
p
Christmann, 2008, (A.32)]. (d) holds by Lemma B.5 with choosing a:“b:“h p¨,xq, C :“D :“
p
A1{2 and using the self-adjointness of A and hence that of A1{2. In (e) the integration was flipped
with A1{2 [Steinwart and Christmann, 2008, (A.32)]. (f) holds by the definition of CQ,hp. The
cyclic invariance property of the trace implies (g), which concludes the derivation.
´ ¯
The following lemma is a natural generalization of the property pCaqpDbqT “C abT DT,
where C,DPRdˆd and a,bPRd.
Lemma B.5 (Tensor product of linearly transformed vectors). Let be a Hilbert space and
H
C,D P p q. Then for any a,b P , pCaqbpDbq “ CpabbqD˚. Specifically, when D is
L H H
self-adjoint, it holds that pCaqbpDbq“CpabbqD.
Proof. Let hP be arbitrary and fixed. Then,
H
paq
rpCaqbpDbqsphq “ CaxDb,hy ,
H
rCpabbqD˚sphq“CpabbqpD˚hqp “bq Caxb,D˚hy p “cq CaxDb,hy .
H H
In (a) and (b), we used the definition of b, (c) follows from the definition of the adjoint and by
the property pD˚q˚ “ D. The shown equality of rpCaqbpDbqsphq “ rCpabbqD˚sphq for any
hP proves the claimed statement.
H
Lemma B.6 (Maximum of sub-Gaussia ´n random variab bles). Let pX iqn
i“1
i. „i ¯.d.P be real-valued
sub-Gaussian random variables. Then P max |X |À }X }2 logp2n{δq ě1´δ holds for
iPrns i 1 ψ2
any δ Pp0,1q.
25Proof. Let c ą 0 be an absolute constant. As X is sub-Gaussian, by Vershynin [2018, Propo-
1
sition a2.5.2], there exists K
1
ď c}X 1}
ψ2
such that Pp|X 1| ě tq ď 2e´
Kt2
12 for all t ě 0. Let
u“ K2plogp2nq`tq
1
ˆ ˙
P max|X i|ěu p ďaq
ÿn
Pp|X i|ěuqp ďbq 2ne´ Ku2 12 p “cq e´t,
iPrns
i“1
where (a) uses that the probability of a maximum exceeding a value is less than the sum of
the probability of its arguments exceeding the value, (b) uses the mentioned property of sub-
Gaussian random variables, and (c) is our ´definition of u. S aolving for δ :“¯e´t gives t“logp1{δq,
and considering the complement yields P max |X |ď K2logp2n{δq ě1´δ. Using that
iPrns i 1
K ďc}X } concludes the proof.
1 1 ψ2
C Derivation of (7)
We detail the derivation of (7).piiiq in the following. Indeed, for f P d and αPRd, we obtain
Hk
sup
A
f,E ξ
pXqE
p “iq sup 1
ÿn
xf,ξ px qy
}f} Hp,mď1 X„Qˆ n p CHd k }f}2 Hp,mď1n i“ G1 p i Hd k
piiq 1
ÿn ÿm
“ sup α ξ px˜ q,ξ px q
i p j p i
n
αTKhp,mαď1 i“1 j“1 Hd
k
pi “iiq sup
ÿm
α 1
ÿn
h px˜ ,x qp “ivq sup αTβ p “vq sup γTK´1{2 β
αTKhp,mαď1j“1
i ln ooio“oo1ooomp oooj oooooi
n αTKhp,mαď1
p
}γ}2 2ď1
hp,m,m p
b
“:βp,j
p “viq βTK´1 β ,
p hp,m,m p
where piq follows from the definition of the sample distribution Qˆ and linearity of the inner
n
product, piiq uses that f P , and piiiq follows from the linearity of the inner product and the
p,m
definition of h . By defininHg β “pβ qm “ 1K 1 , we obtain pivq; in pvq, we introduce
p p p,j j“1 n hp,m,n n
γ “K1{2 α, and pviq is implied by the Cauchy-Schwarz inequality.
hp,m,m
D External statements
This section collects the external statements that we use. Lemma D.1 gives equivalent norms
for f bf. We collect properties of Orlicz norms in Lemma D.2. Theorem D.3 is about the
concentration of the empirical covariance, and Theorem D.4 recalls the Bernstein’s inequality for
separable Hilbert spaces. Theorem D.5 is a concentration result for bounded random variables in
aseparableHilbertspace. TheoremD.6recallsthecombinationofRudietal.[2015,Proposition3
and Proposition 7].
Lemma D.1 (Lemma B.8; Sriperumbudur and Sterge 2022). Define B “f bf, where f P
and is a separable Hilbert space. Then }B} “}B} “trB “}f}2 . H
H op HbH H
26We refer to the following sources for the items in Lemma D.2. Item 1 is Vershynin [2018,
Lemma 2.6.8], Item 2 is Vershynin [2018, Exercise 2.7.10], Item 3 recalls van der Vaart and
Wellner [1996, p. 95], and Item 4 is Vershynin [2018, Lemma 2.7.6].
Lemma D.2 (Collection of Orlicz properties). Let X be a real-valued random variable.
1. If X is sub-Gaussian, then X´EX is also sub-Gaussian, and
}X´EX} ď}X} `}EX} À}X} .
ψ2 ψ2 ψ2 ψ2
2. If X is sub-exponential, then X´EX is also sub-exponential, and satisfies
}X´EX} ď}X} `}EX} À}X} .
ψ1 ψ1 ψ1 ψ1
?
3. If X is sub-Gaussian, it is sub-exponential. Specifically, it holds that }X} ď log2}X} .
ψ1 ψ2
4. X is sub-Gaussian if and only if X2 is sub-exponential. Moreover,
› ›
› X2› “}X}2
.
ψ1 ψ2
Theorem D.3 (Theorem 9; Koltchinskii and Lounici 2017). Let X,X ,...,X be i.i.d. square
1 n
integrable centered random vectors in ařHilbert space with covariance operator C. Let the
empirical covariance operator be Cˆ “ 1 n X bX . H If X is sub-Gaussian, then there exists a
n n i“1 i i
constant cą0 such that, for all δ Pp0,1q, with probability at least 1´δ,
˜c c ¸
› ›
› › rpCq logp1{δq
›Cˆ ´C› ďc}C} max , ,
n op op n n
provided that němaxtrpCq,logp1{δqu, where rpCq:“ trpCq.
}C}
op
The following theorem by Yurinsky [1995] is quoted from Sriperumbudur and Sterge [2022].
Theorem D.4 (BernsteinboundforseparableHilbertspaces;Theorem3.3.4;Yurinsky1995). Let
pΩ, ,Pqbeaprobabilityspace, aseparableHilbertspace, B ą0, σ ą0, andη ,...,η :ΩÑ
1 n
A H H
centered i.i.d. random variables that satisfy
1
E}η }p ď p!σ2Bp´2
1 H 2
for all pě2. Then, for any δ Pp0,1q it holds with probability at least 1´δ that
› › c
›
›1
ÿn ›
› 2Blogp2{δq 2σ2logp2{δq
› η › ď ` .
›n i› n n
i“1 H
Theorem D.5 (Concentration in separable Hilbert spaces; Lemma E.1; Chatalic et al. 2022).
Let X ,...,X be i.i.d. random variables with zero mean in a separable Hilbert space p ,}¨} q
1 n H H
such that max }X } ďb almost surely, for some bą0. Then for any δ Pp0,1q, it holds with
iPrns i H
probability at least 1´δ that
› › a
›
›1
ÿn ›
› 2logp2{δq
› X › ďb ? .
›n i› n
i“1 H
The following combination of Rudi et al. [2015, Proposition 3 and Proposition 7] is standard.
In the following, we adapt it to our notation and give a short proof to ensure self-containedness.
27ş
Theorem D.6 (Proposition 3, Proposition 7; Rudi et al. 201 ř5). Let CQ,hp “ Rdh pp¨,xqb
h pp¨,xqdQpxq, CQ,hp,λ´“ CQ,hp´`λI (λ ą 0), C˜
¯Qˆ n,hp,m
“
¯
m1 m i“1h pp¨,x˜ iqbh pp¨,x˜ iq. For
λą0, let βpλq“λ
max
C Q´ ,1 h{ p2
,λ
CQ,hp ´C˜
Qˆ n,hp,m
C Q´ ,1 h{ p2
,λ
, px˜ iqm
i“1
are the Nyström points, as
defined in Section 4.1, :“ spanth p¨,x˜ q|iPrmsu, and P denotes the orthogonal
Hhp,m p i Hhp,m
projection onto . Then, it holds that
Hhp,m
›´ ¯ ›
› ›2 λ
› I´P C1{2 › ď ,
Hhp,m Q,hp,λ op 1´βpλq
when βpλqă1.
Proof. Define the sampling operator Z
m
:
Hhp
Ñ Rm by f ÞÑ ?1 mpfpx˜ iqqm i“1. Its adjoint
Z ?1m˚ m:řR
m
i“m 1Ñ
α ihH ph pp
¨,( x˜s ie qe
.
S Rt ee cr ag le la thn ad tS Hri hp pe ,mrum “b su pd au nr t[ h2 p0 p2 ¨2 ,, x˜L iqem |im Pa rmA. s7 u(i a) n] dis ng oi tv ie cn etb hy atα ra“ ngpα
ei
Pq Hm
i“ h1
pÞÑ
“
rangeZ˚. We obtain
m
› › › › › ›
› ›` I´P ˘ C1{2 › ›2 p ďaq λ› ›pZ˚Z `λIq´1{2 C1{2 › ›2 p “bq λ› ›C˜´1{2 C1{2 › ›2 (D.17)
hp,m Q,hp,λ op m m Q,hp,λ op Qˆ n,hp,λ Q,hp,λ op
where (a) follows from Rudi et al. [2015, Proposition 3] with X :“ C1{2 therein. (b) is by
Q,hp,λ
Sterge and Sriperumbudur [2022, Lemma A.7(iv)].
Applying the second inequality in the statement of Rudi et al. [2015, Proposition 7] to (D.17)
(we specialize A:“C˜
Qˆ n,hp
and B :“CQ,hp therein), we obtain
› ›
› ›2 λ
λ›C˜´1{2 C1{2 › ď , (D.18)
Qˆ n,hp,λ Q,hp,λ op 1´βpλq
when βpλqă1. The combination of (D.17) and (D.18) yields the stated claim.
28