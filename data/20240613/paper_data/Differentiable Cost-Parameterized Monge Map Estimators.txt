Differentiable Cost-Parameterized Monge Map Estimators
SamuelHoward1 GeorgeDeligiannidis1 PatrickRebeschini1 JamesThornton12
Abstract Ground Truth Squared-Euclidean Learned Map Estimator
5 5 5
Within the field of optimal transport (OT), the
choiceofgroundcostiscrucialtoensuringthat 0 0 0
theoptimalityofatransportmapcorrespondsto
usefulnessinreal-worldapplications. Itisthere- 5 5 5
5 0 5 5 0 5 5 0 5
foredesirabletouseknowninformationtotailor 5 5 5
costfunctionsandhencelearnOTmapswhichare
adaptedtotheproblemathand. Byconsideringa 0 0 0
classofneuralgroundcostswhoseMongemaps
haveaknownform,weconstructadifferentiable
5 0 5 0 5 5 0 5
Mongemapestimatorwhichcanbeoptimizedto
be consistent with known information about an 5 5 5
OT map. In doing so, we simultaneously learn 0 0 0
bothanOTmapestimatorandacorresponding
5 5 5
adaptedcostfunction. Throughsuitablechoices
10 0 10 0 10 0
oflossfunction,ourmethodprovidesageneralap-
source target map estimator T
proachforincorporatingpriorinformationabout
theMongemapitself whenlearningadaptedOT
Figure1.Usingthesquared-Euclideancostcanresultinincorrect
mapsandcostfunctions. mapestimatorsthatdonotalignwithknownground-truthmap-
pings.Byoptimizingadifferentiablecost-parameterizedMonge
map estimator to resemble known information, we can obtain
1.Introduction Mongemapestimatorsandcorrespondingcostfunctionswhich
areconsistentwiththeground-truth.
Optimaltransport.Mappingsamplesbetweentwodatasets
inameaningfulwayisoneofthemostfundamentaltasks
usedgroundcostincomputationalOTmethods,duetoits
across the sciences. Optimal transport (OT) provides a
desirabletheoreticalpropertiesandeaseofimplementation.
principledframeworkforsuchmappingproblems,andhas
Itallowstheuseoftheclosed-formtransportmapfromBre-
enjoyedsuccessinmanyfieldsincludingthroughoutbiology
nier’stheorem[Brenier,1987],andanelegantconnection
[Schiebingeretal.,2019],andextensivelyinmachinelearn-
to convex analysis upon which many methods are based
ingwithapplicationsingenerativemodelling[DeBortoli
[Makkuvaetal.,2020,Korotinetal.,2021a]. However,the
etal.,2021,Genevayetal.,2018],differentiablesorting[Cu-
squared-Euclideancostcanbeanarbitrarychoiceandmay
turietal.,2019],clustering[Genevayetal.,2019],resam-
notbesuitableforthegivenproblem[Genevayetal.,2018].
pling[Corenflosetal.,2021]andselfsupervisedlearning
Moreover,itcanresultinerroneousmappingsthatdonot
[Caronetal.,2020].
agreewithknownground-truthinformation[Somnathetal.,
Fortwoprobabilitymeasuresµandν onRd,theOTprob- 2023],asdemonstratedinFigure1. Recently,severalworks
lemfindsthemostefficientwaytotransportµtoν relative haveproposedmethodstoestimateOTmapsformoregen-
toagroundcostfunctionc:Rd×Rd →R. Thechoiceof eralgroundcosts[Fanetal.,2023,Asadulaevetal.,2023,
costfunctionthereforedeterminesthenotionofoptimality. Cuturietal.,2023,UsciddaandCuturi,2023],howeverit
remainsunclearhowsuchacostshouldbechosen.
ChoiceofGroundCost. Thesquared-Euclideandistance
c(x,y) = 1∥x−y∥2 is the default and most commonly Cost Learning. Ambiguity surrounding the choice of
2
groundcosthasmotivatedrecentinterestinlearningadapted
1DepartmentofStatistics,UniversityofOxford2Apple.Corre-
costfunctionsbyleveragingassumedadditionalinformation
spondenceto:SamuelHoward<howard@stats.ox.ac.uk>.
aboutthetransportmap(seeAppendixB.2foradiscussion
Preprint. of related work). A frequently studied setting is inverse
1
4202
nuJ
21
]LM.tats[
1v99380.6042:viXraDifferentiableCost-ParameterizedMongeMapEstimators
OT, which aims to learn cost functions from paired sam- costamongstmapsT :Rd →Rdpushingµontoν:
plesassumedtocomefromatransportcoupling. Access
(cid:90)
tocompletelypaireddatasetsishoweverunlikelytoarise min c(x,T(x))dµ(x). (1)
in practical applications. Instead, we may aim to learn T:T#µ=ν Rd
animprovedcostfunctionfrompartialinformationabout
Kantorovich. Amoregeneralformulation,permittingmass
themapping,suchasalimitednumberofpairedpointsor
splitting,byKantorovich[1942]seeksajointdistribution
knownstructureinthemapdisplacements. Thismotivatesa π⋆ ∈ P(Rd × Rd) minimizing the cost over the set of
needforgeneralcostlearningframeworksthatareableto
couplingsΓ(µ,ν)betweenmarginalsµandν:
learnfromavarietyoftypesofavailableinformation.
(cid:90)(cid:90)
One general approach for learning cost functions is a bi- min c(x,y)dπ(x,y). (2)
levelapproachentailingsolvingaregularizedOTproblem π∈Γ(µ,ν) Rd×Rd
for given cost function with Sinkhorn’s algorithm, then
The Kantorovich formulation admits the following dual
differentiatingthroughthissolutiontooptimizethecost(see
formulation,whereR ={(f,g)∈L1(µ)×L1(ν):f(x)+
c
AppendixB.2.2). InmanyOTapplicationshowever, one
g(y) ≤ c(x,y) µ⊗ν a.e.}. The solutions (f⋆,g⋆) to the
wishestoapproximatetheMongemapT⋆itself,ratherthan
dualproblemareknownastheKantorovichpotentials,
thecostfunctionalone. Thiscanbeusedtotransportout-
of-samplepoints,enablingapplicationssuchasgenerative (cid:26)(cid:90) (cid:90) (cid:27)
sup fdµ+ gdν . (3)
modelling and prediction [Bunne et al., 2023]. While a
learnedcostcanbeplugged-intoanOTmapestimator,if
(f,g)∈Rc
anaccurateapproximationisnotobtainedthentheresulting
Closed-form Monge maps. The following theorem de-
estimatormaynotagreewiththeinformationusedtolearn
tailssufficientconditionsforequivalenceoftheMongeand
thecost.
Kantorovichproblems,andmotivatessolvingfortheKan-
Contributions. Weintroduceanalternativeapproachfor torovichpotentialstoobtaintheMongemapping.
learningadaptedcostfunctionsandOTmapsbyinsteadop-
Theorem2.1(Theorem1.17,[Santambrogio,2015]). For
timizingacost-parameterizedMongemapestimatordirectly
measuresµandν onacompactdomainΩ⊂Rdandacost
to be consistent with known information. This approach
oftheformc(x,y) = h(x−y)forastrictlyconvexfunc-
simultaneouslylearnsamapestimatorandacorresponding
tionh,thereexistsanoptimalplanπ⋆fortheKantorovich
costfunction,ensuringthattheresultingmaphasthedesired
problem. Ifµisabsolutelycontinuousand∂Ωisnegligible,
properties. Wemakethefollowingcontributions:
thenthisoptimalplanisuniqueandoftheform(Id,T⋆)#µ,
thereexistsaKantorovichpotentialf⋆,and
• We propose a differentiable structured Monge map T⋆(x)=x−(∇h)−1(∇f⋆(x)). (4)
estimator,incorporatingcostsc(x,y)=h(x−y)for
strictlyconvexh. WeparameterizehusinganInput
Entropic OT. The entropic OT problem instead smooths
ConvexNeuralNetwork(ICNN)[Amosetal.,2017],
thetransportplanbyaddinganentropicpenaltytermto(2),
h ,andusethedifferentiableentropicmapestimator
θ
[PooladianandNiles-Weed,2021,Cuturietal.,2023] (cid:90)(cid:90)
tofacilitategradientbasedtraining. π⋆ = argmin c(x,y)dπ(x,y)+εKL(π|µ⊗ν).
π∈Γ(µ,ν) Rd×Rd
• We then extend our construction to costs c(x,y) =
This relaxes the constraints on the dual potentials. The
h(Φ (x)−Φ (y))forinvertiblefunctionsΦ,provid-
µ ν · solutions (f⋆,g⋆) to the dual problem are known as the
ingadditionalflexibilitywhilstretainingastructured ε ε
entropicpotentials,
Mongemap.
(cid:26)(cid:90) (cid:90) (cid:27)
• Weshowcasehowourapproachcanincorporatepar- sup fdµ+ gdν+R(f,g) , (5)
tiallyknowninformation,includingaligningmapping f∈C(X)
g∈C(Y)
estimatorswithknowndataassociations,andencour-
aging desirable properties in the resulting transport (cid:82)(cid:82) f(x)+g(y)−c(x,y)
whereR(f,g):=−ε e ε dµ(x)dν(y).
map.
EntropicOTapproachesandhenceapproximatesOTinthe
2.BackgroundonOptimalTransport limit as ε ↘ 0, but the entropic solution enjoys the key
benefit of differentiability with respect to the inputs and
Monge. The original OT formulation given by Monge enables efficient computation for discrete measures µˆ,νˆ
[1781]seeksamapT⋆minimizingthetotaltransportation usingSinkhorn’salgorithm[Cuturi,2013].
2DifferentiableCost-ParameterizedMongeMapEstimators
3.ADifferentiableMongeMapEstimator Choiceoflossfunction. Bychoosinganappropriateloss
function L(θ), we can incorporate desired explicit biases
Weconsiderlearningcostsoftheformc(x,y)=h (x−y)
θ intothelearnedcostthroughmapestimatorTε. Thetrain-
forastrictlyconvexh ,forwhichtheMongemaphasthe θ
θ ingprocedureisdescribedinAlgorithm1inAppendixA.
form in (4). This enables direct optimization according
Therearemanypossibletraininglossesthatcanbeusedto
toconditionsonthemapitself,ensuringthattheresulting
encouragesomedesiredbehaviour. Anaturalobjective,for
mappingisconsistentwithknownpriorinformation.
example,istoensurethelearnedmappingcorrectlymatches
Suchcostsallowtheuseofmethodssuchastheentropic knownpairedpoints. Givenaknownsubsetofpairedpoints
mappingestimator[PooladianandNiles-Weed,2021,Cu- (x i,y i)N i=1, the loss L(θ) := N1 (cid:80)N i=1∥T θε(x i)−y i∥2
2
en-
turietal.,2023]andc-rectifiedflow[Liu,2022],ensuring couragesthelearnedmaptorespecttheknownpairs. We
reliableestimationofthecorrespondingOTmap. Incon- provideotherchoicesoflossfunctionssuitableforarange
trast,learningOTmapestimatorsforarbitrarycostsrequires ofproblemsinAppendixA.2.
atrade-offbetweendistributionfittingandoptimality(see Wecanalsoconstructthereversemapestimator(cid:0) Tε(x)(cid:1)−1
.
AppendixB.1.1),andiftheresultingmappingisinaccurate θ
Wefinditbeneficialtotrainusingbothmappingsjointly;
itmaynotbeconsistentwiththeinformationfromwhichthe
seeAppendixA.2.3fordetails.
costwaslearned. Moreover,additionalknowninformation
aboutanOTmapwillnotuniquelydetermineacostfunc- Warmstarting the inner optimizations. Each iteration
tion. Byconsideringconvexcosts,weprovideaninductive ofourprocedureinvolvessolvingtwotypesofinneropti-
biastowardssimplerandmoreinterpretablecosts. mizationproblem,(1)thediscreteentropy-regularizedOT
problemusingSinkhorn,and(2)theevaluationof(∇h)−1.
3.1.Differentiable,Cost-ParameterizedTransportMaps While both problems are convex and thus can be solved
easily,naiveinitializationswouldresultinunnecessarycom-
Weparameterizeh usinganInputConvexNeuralNetwork
θ putational cost. We instead warmstart each optimization
(ICNN) [Amos et al., 2017]. We also enforce α-strong
fromthecorrespondingsolutionsatthepreviousiteration,
convexityand(optionally)symmetryofh (seeAppendix
θ whichsignificantlyimprovestrainingspeed[Amos,2022].
A.1). Weusetheentropicpotentialfˆε[PooladianandNiles-
θ
Weed,2021,Cuturietal.,2023]forthediscreteempirical
3.2.AugmentingwithDiffeomorphisms
measuresµˆ,νˆasadifferentiableproxyfortheKantorovich
potentialf⋆.ThisisconstructedfromtheSinkhornpotential In the presence of extensive information about the map,
g⋆solvingthediscreteOTproblembetweenµˆ,νˆ: the choice of strictly convex costs c(x,y) = h(x − y)
ε
fˆε(x)=−εlog(cid:88) exp(cid:18) (g ε⋆) j −h θ(x−y j)(cid:19) . (6) c ca on stb the ao tv ae llr oly wsre ts ht eri Mcti ov ne g, eas mt ah per te obm ea cy on no sit stb ee nta wc io thnv thex e
θ ε
j knowninformation. Wethereforeextendourframeworkby
firsttransformingthemarginalmeasuresusingdiffeomor-
With(4),thisgivestheentropicmappingestimator
phisms Φ ,Φ , then applying our method to µ˜ =Φ #µ
µ ν µ
Tε(x)=x−(∇h )−1(∇fˆε(x)). (7) andν˜=Φ #ν. ThefollowingextensionofTheorem2.1,
θ θ θ ν
provedinAppendixE,showsthatthisamountstolearning
Differentiability. Wecandifferentiatethroughtheoutput
amapwithcostc(x,y)=h(Φ (x)−Φ (y)).
µ ν
ofSinkhorn’salgorithmusingimplicitdifferentiation[Luise
Theorem3.2. UndertheconditionsofTheorem2.1witha
etal.,2018],orbyunrollingtheiterates[AdamsandZemel,
costoftheformc(x,y)=h(Φ (x)−Φ (y))forastrictly
2011,Flamaryetal.,2018].Wealsonotethefollowingwell- µ ν
convex function h, the optimal plan is unique and of the
known relation, enabling differentiation through (∇h)−1.
form(Id,T⋆)#µ,andT⋆canbewrittenas
Proposition3.1. Forastrictlyconvexfunctionh,wehave (cid:20) (cid:21)
T⋆(x)=Φ−1 Φ (x)−(∇h)−1◦∇f⋆◦Φ (x) . (9)
(∇h)−1(x)=argmin(cid:8) h(z)−⟨z,x⟩(cid:9)
. (8)
ν µ µ
z
WeparameterizeΦ ,Φ usingnormalizingflows[Rezende
Asweenforceα-strongconvexityofh,theminimizationin µ ν
andMohamed,2015],andtrainend-to-endalongwiththe
(8)canbesolvedefficientlyusingnumericalmethods. We
procedureinAlgorithm1. Thisincreasestheexpressivity
canthenuseimplicitdifferentiation(seeChapter10,Blon-
of the class of cost functions we consider. As we would
del and Roulet [2024]) to differentiate through this inner
prefertolearnsimplercosts,wecantakeΦ =Φ which
minimizationwithrespecttothecostparameters. Weimple- µ ν
preservessymmetrywhenhissymmetric. Theorem3.2re-
mentthisusingtheJAXoptlibary[Blondeletal.,2022].
semblessimilarrecentresultswhichuselearnableinvertible
We now have a Monge map estimator that is end-to-end matrices[Kleinetal.,2024,Prop3.] andfixedmirror-maps
differentiablewithrespecttoitscostfunctionparameters. [RankinandWong,2023].
3DifferentiableCost-ParameterizedMongeMapEstimators
Cost h( (x) (y)), optimizing map Cost h( (x) (y)), optimizing matrix MLP cost c(x,y), optimizing matrix
0 0 0
Pre cell-differentiation,
5 5 5 Post cell-differentiation,
Mapping T#
Ground Truth
10 10 10 Predictions T(x)
15 15 15
5 10 15 20 25 30 5 10 15 20 25 30 5 10 15 20 25 30
PCA 1 PCA 1 PCA 1
Figure2.(left)Byoptimizingthemapdirectly,welearnaMongemapestimatorwhichagreeswiththeLive-seqtrajectories.(middle)
Optimizingaccordingtothecouplingmatrixwithacosth(Φ(x)−Φ(y))transportssomepointscorrectly,butstrugglestolearnagood
costoverall(seeTable1).(right)ForageneralMLPcost,itisdifficulttoobtainagoodmappingestimator.
Table1.Results from the Live-seq experiment, over 10 initial-
4.Experiments
izationseeds. Lowerisbetter. Unsurprisingly,theMLPcostis
abletoplacemostmassalongthecorrectpairings.Theresulting
Inverse OT. In Figure 1 we verify that our method can
costishoweverunstructuredanddifficulttointerpret, anditis
indeedlearnvalidcostfunctionsintheInverseOTsetting
difficult to approximate the corresponding OT map. For costs
using synthetic 2d distributions. We are able to learn a
h(Φ(x)−Φ(y)), the resulting entropic mappings show an im-
consistentmapestimatorfortheT-shapedatasetusingonly
provedfittothetarget,butoptimizingthemaplearnsbettercosts
asymmetricconvexcostc(x,y)=h(x−y). Forthemoon andhassignificantlybetteralignmentwiththegroundtruth.
and spiral datasets, we require costs of form c(x,y) =
h(Φ (x)−Φ (y)). SolvingthediscreteOTproblemonthe OPTIMIZINGMAP OPTIMIZINGMATRIX
µ ν h(Φ(x)−Φ(y)) h(Φ(x)−Φ(y)) MLPc(x,y)
trainingdatasetsusingthelearnedcostrecoversthecorrect
pairs,demonstratingthatthelearnedcostsareindeedvalid. I.M. 0.095±0.035 0.153±0.012 0.051±0.000
RMSE 2.02±2.21 16.9±1.6 6.91±2.52
Limited labelled pairs. Optimal transport is commonly S (Tˆ#µ,ν) 7.73±3.02 4.94±0.85 19.8±2.8
ε
usedtoinfersingle-celltranscriptometrajectoriesbetween
unaligneddatasets,undertheassumptionthatacellpopula- matorusingtheMongeGapregularizer[UsciddaandCuturi,
tionhasmoved‘efficiently’betweentheobservedtimesteps 2023]. Optimizingthemapdirectlyconsistentlyobtained
[Schiebingeretal.,2019,Bunneetal.,2023]. Therecent both an interpretable cost function and a good transport
Live-seqprofilingtechnique[Chenetal.,2022]avoidsthe estimatoragreeingwiththeobservedtrajectories.
destructionofmeasuredcells,enablinganumberofindivid-
GiventhescarcityofknownLive-seqtrajectories,wecannot
ualcelltrajectoriestobetraced. Thesetrajectoriesappear
testout-of-sampleperformanceoftheresultingmapestima-
nottoagreewiththosepredictedbyOTmethodsthatusethe
tor. Wethereforeperformthesameprocedureonsynthetic
squared-Euclideancost. Nevertheless,theassumptionthat
datawithdifferentnumbersoflabelledpairsinAppendix
cellsmove‘efficiently’isreasonable,raisingthequestionof
D.1,reportingresultsontrainingandnewly-sampleddata.
whetheradifferentchoiceofground-costismoresuitable.
Additional Experiments. We provide additional experi-
Tolearnanexpressivebutinterpretablecost,weconsider
mentsinAppendixD,includingtheabilitytoinducedesir-
learningsymmetriccostsoftheformh(Φ(x)−Φ(y)). We
ablepropertiesthatholdonthemapdisplacementsthem-
fittheMongemapestimatortothefirst10principalcom-
selves,suchasbeinglow-rankork-directional.
ponents of the Live-seq data consisting of cells pre- and
post cell-differentiation, so that it agrees with the known
5.Conclusion
trajectories. We plot the resulting mapping estimators in
Figure 2. In Table 1 we report the total incorrectly trans-
WehaveintroducedaMongemapestimatorparameterized
ported mass in the resulting coupling (denoted I.M.; see
by a learnable cost function, which is differentiable with
Appendix C), as a measure of the validity of the learned
respectitsparameters. Thisenableslearningadaptedcost
cost. To assess the adapted OT map, we also report the
functions andOT mapestimators through gradientbased
RMSE of the predictions for the known cell trajectories,
trainingsothatthemapisconsistentwithknownpriorin-
and the Sinkhorn divergence S (Tˆ#µ,ν). Experimental
ε formation. Wehavedemonstratedtheabilitytolearncosts
details are provided in Appendix C. We compare against
byaligningwithpaireddatasamples,andbyinducingdesir-
costslearnedbyoptimizingtheSinkhorncouplingmatrix
ablepropertiesontheMongemapestimatoritself. Future
tominimizetheincorrectmassreportedinTable1. Wefirst
directionsinclude: conductingfurthercomparisonsofopti-
use the same cost parameterization h(Φ(x)−Φ(y)) and
mizingtheentropicmapcomparedtothecouplingmatrix,
obtainamapusingtheentropicmappingestimator. Wealso
andinvestigatingtheuseofadaptedcostsandOTmapsin
useanunstructuredMLPcostc(x,y)andalearnamapesti-
downstreamtasks.
4
2
ACPDifferentiableCost-ParameterizedMongeMapEstimators
Acknowledgements Guillaume Carlier, Arnaud Dupuy, Alfred Galichon, and
YifeiSun. SISTA:Learningoptimaltransportcostsun-
SamuelHowardissupportedbytheEPSRCCDTinModern
dersparsityconstraints. CommunicationsonPureand
StatisticsandStatisticalMachineLearning[grantnumber
AppliedMathematics,76,2020.
EP/S023151/1]. Patrick Rebeschini was funded by UK
Research and Innovation (UKRI) under the UK govern- MathildeCaron,IshanMisra,JulienMairal,PriyaGoyal,Pi-
ment’sHorizonEuropefundingguarantee[grantnumber otrBojanowski,andArmandJoulin. Unsupervisedlearn-
EP/Y028333/1]. ingofvisualfeaturesbycontrastingclusterassignments.
AdvancesinNeuralInformationProcessingSystems,33:
References 9912–9924,2020.
RyanPrescottAdamsandRichardSZemel. Rankingvia WanzeChen,OraneGuillaume-Gentil,PernilleYdeRainer,
sinkhornpropagation. arXivpreprintarXiv:1106.1925, ChristophG.Ga¨belein,WouterSaelens,VincentGardeux,
2011. Amanda Klaeger, Riccardo Dainese, Magda Zachara,
TomasoZambelli,JuliaA.Vorholt,andBartDeplancke.
BrandonAmos. Tutorialonamortizedoptimization. Found. Live-seq enables temporal transcriptomic recording of
TrendsMach.Learn.,16:592–732,2022. singlecells. volume608ofNature,pages733–740,2022.
Wei-Ting Chiu, Pei Wang, and Patrick Shafto. Discrete
BrandonAmos,LeiXu,andJ.ZicoKolter. Inputconvex
neuralnetworks. InProceedingsofthe34thInternational probabilisticinverseoptimaltransport. InProceedingsof
ConferenceonMachineLearning,2017.
the39thInternationalConferenceonMachineLearning,
2022.
FranciscoAndrade,GabrielPeyre´,andClaricePoon. Spar-
AdrienCorenflos,JamesThornton,GeorgeDeligiannidis,
sistency for inverse optimal transport. In The Twelfth
andArnaudDoucet. Differentiableparticlefilteringvia
InternationalConferenceonLearningRepresentations,
entropy-regularizedoptimaltransport. InInternational
2024.
Conference on Machine Learning, pages 2100–2111.
PMLR,2021.
AripAsadulaev,AlexanderKorotin,VageEgiazarian,Petr
Mokrov,andEvgenyBurnaev. Neuraloptimaltransport Marco Cuturi. Sinkhorn distances: Lightspeed compu-
withgeneralcostfunctionals,2023. tation of optimal transport. In C.J. Burges, L. Bottou,
M.Welling,Z.Ghahramani,andK.Q.Weinberger,edi-
Mathieu Blondel and Vincent Roulet. The elements of tors,AdvancesinNeuralInformationProcessingSystems,
differentiableprogramming,2024.
2013.
Mathieu Blondel, Quentin Berthet, Marco Cuturi, Roy MarcoCuturiandDavidAvis. Groundmetriclearning. J.
Frostig, StephanHoyer, FelipeLlinares-Lopez, Fabian Mach.Learn.Res.,15(1),2014.
Pedregosa,andJean-PhilippeVert. Efficientandmodular
implicitdifferentiation. InAdvancesinNeuralInforma- MarcoCuturi,OlivierTeboul,andJean-PhilippeVert. Dif-
tionProcessingSystems,2022. ferentiablerankingandsortingusingoptimaltransport.
Advancesinneuralinformationprocessingsystems,32,
Y.Brenier. Decompositionpolaireetrearrangementmono- 2019.
tonedeschampsdevecteurs. C.R.Acad.Sci.ParisSer.I
Marco Cuturi, Laetitia Meng-Papaxanthos, Yingtao Tian,
Math.,305,1987.
CharlotteBunne,GeoffDavis,andOlivierTeboul. Opti-
malTransportTools(OTT):AJAXtoolboxforallthings
Jane Bromley, Isabelle Guyon, Yann LeCun, Eduard
Wasserstein. arXivpreprintarXiv:2201.12324,2022.
Sa¨ckinger,andRoopakShah.Signatureverificationusing
a”siamese”timedelayneuralnetwork. InAdvancesin Marco Cuturi, Michal Klein, and Pierre Ablin. Monge,
NeuralInformationProcessingSystems,1993.
BregmanandOccam: Interpretableoptimaltransportin
high-dimensionswithfeature-sparsemaps. InInterna-
CharlotteBunne,StefanG.Stark,GabrieleGut,JacoboSara- tionalConferenceonMachineLearning,2023.
biadelCastillo,MitchLevesque,Kjong-VanLehmann,
Lucas Pelkmans, Andreas Krause, and Gunnar Ra¨tsch. Valentin De Bortoli, James Thornton, Jeremy Heng, and
Learningsingle-cellperturbationresponsesusingneural ArnaudDoucet. Diffusionschro¨dingerbridgewithappli-
optimaltransport. volume20ofNatureMethods,pages cationstoscore-basedgenerativemodeling. Advancesin
1759–1768,2023. NeuralInformationProcessingSystems,34,2021.
5DifferentiableCost-ParameterizedMongeMapEstimators
ArnaudDupuyandAlfredGalichon. Personalitytraitsand AlexanderKorotin,LingxiaoLi,AudeGenevay,JustinM
themarriagemarket. JournalofPoliticalEconomy,122, Solomon,AlexanderFilippov,andEvgenyBurnaev. Do
2014. neural optimal transport solvers work? a continuous
Wasserstein-2benchmark. 2021b.
ArnaudDupuy,AlfredGalichon,andYifeiSun. Estimating
matching affinity matrices under low-rank constraints. RuilinLi,XiaojingYe,HaominZhou,andHongyuanZha.
InformationandInference: AJournaloftheIMA,8(4), Learningtomatchviainverseoptimaltransport. J.Mach.
2019. Learn.Res.,20(1),2019.
Jiaojiao Fan, Shu Liu, Shaojun Ma, Hao-Min Zhou, and
QiangLiu. Rectifiedflow: Amarginalpreservingapproach
YongxinChen. NeuralMongemapestimationanditsap-
tooptimaltransport,2022.
plications. TransactionsonMachineLearningResearch,
2023. RuishanLiu,AkshayBalsubramani,andJamesZou. Learn-
ingtransportcostfromsubsetcorrespondence. InInter-
Re´mi Flamary, Marco Cuturi, Nicolas Courty, and Alain
nationalConferenceonLearningRepresentations,2020.
Rakotomamonjy. Wassersteindiscriminantanalysis. Ma-
chineLearning,107(12):1923–1945,2018.
Giulia Luise, Alessandro Rudi, Massimiliano Pontil, and
CarloCiliberto. Differentialpropertiesofsinkhornap-
Alfred Galichon and Bernard Salanie´. Cupid’s Invisible
proximationforlearningwithWassersteindistance. Ad-
Hand:SocialSurplusandIdentificationinMatchingMod-
vances in Neural Information Processing Systems, 31,
els. TheReviewofEconomicStudies,89(5),2021.
2018.
AudeGenevay,MarcoCuturi,GabrielPeyre´,andFrancisR.
Bach. Stochastic optimization for large-scale optimal ShaojunMa, HaodongSun, XiaojingYe,HongyuanZha,
transport. In Neural Information Processing Systems, andHaominZhou. Learningcostfunctionsforoptimal
2016. transport,2021.
AudeGenevay,GabrielPeyre´,andMarcoCuturi. Learning AshokMakkuva,AmirhosseinTaghvaei,SewoongOh,and
generativemodelswithsinkhorndivergences. InInterna- JasonLee. Optimaltransportmappingviainputconvex
tionalConferenceonArtificialIntelligenceandStatistics, neuralnetworks. InProceedingsofthe37thInternational
pages1608–1617.PMLR,2018. ConferenceonMachineLearning,2020.
AudeGenevay, GabrielDulac-Arnold, andJean-Philippe GaspardMonge. Me´moiresurlathe´oriedesde´blaisetdes
Vert. Differentiable deep clustering with cluster size remblais. Histoiredel’Acade´mieRoyaledesSciences,
constraints. arXivpreprintarXiv:1910.09036,2019. pages666–704,1781.
MatthieuHeitz,NicolasBonneel,DavidCoeurjolly,Marco
George Papamakarios, Theo Pavlakou, and Iain Murray.
Cuturi, and Gabriel Peyre´. Ground metric learning on
Masked autoregressive flow for density estimation. In
graphs. JournalofMathematicalImagingandVision,63
I.Guyon,U.VonLuxburg,S.Bengio,H.Wallach,R.Fer-
(1),2020.
gus,S.Vishwanathan,andR.Garnett,editors,Advances
inNeuralInformationProcessingSystems,2017.
Piotr Indyk, Jiˇr´ı Matousˇek Matousˇek, and Anastasios
Sidiropoulo. Low-distortionembeddingsoffinitemetric
Franc¸ois-PierrePatyandMarcoCuturi.Regularizedoptimal
spaces. Handbookofdiscreteandcomputationalgeome-
transport is ground cost adversarial. In International
try,pages211–231,2017.
ConferenceonMachineLearning,2020.
L.V.Kantorovich. Ontranslationofmass. Proceedingsof
GabrielPeyre´ andMarcoCuturi. Computationaloptimal
theUSSRAcademyofSciences,37,1942.
transport. FoundationsandTrendsinMachineLearning,
Michal Klein, Aram-Alexandre Pooladian, Pierre Ablin, 11(5-6):355–602,2019.
Euge`neNdiaye,JonathanNiles-Weed,andMarcoCuturi.
Learning elastic costs to shape Monge displacements, Silviu Pitis, Harris Chan, Kiarash Jamali, and Jimmy Ba.
2024. Aninductivebiasfordistances: Neuralnetsthatrespect
thetriangleinequality. InInternationalConferenceon
Alexander Korotin, Vage Egiazarian, Arip Asadulaev, LearningRepresentations,2020.
Alexander Safin, and Evgeny Burnaev. Wasserstein-2
generative networks. In International Conference on Aram-AlexandrePooladianandJonathanNiles-Weed. En-
LearningRepresentations,2021a. tropicestimationofoptimaltransportmaps,2021.
6DifferentiableCost-ParameterizedMongeMapEstimators
Aram-AlexandrePooladian,CarlesDomingo-Enrich,Ricky FanWangandLeonidasJ.Guibas.Supervisedearthmover’s
T.Q.Chen, andBrandonAmos. Neuraloptimaltrans- distancelearninganditscomputervisionapplications. In
portwithLagrangiancosts. InICMLWorkshoponNew ComputerVision–ECCV2012,pages442–455.Springer
FrontiersinLearning,Control,andDynamicalSystems, BerlinHeidelberg,2012.
2023.
YunhaiXiao,ZengxinWei,andZhiguoWang. Alimited
Cale Rankin and Ting-Kam Leonard Wong. Bregman- memorybfgs-typemethodforlarge-scaleunconstrained
Wasserstein divergence: geometry and applications. optimization. Computers&MathematicswithApplica-
arXivpreprintarXiv:2302.05833,2023. tions,56(4):1001–1009,2008.
Danilo Rezende and Shakir Mohamed. Variational infer-
encewithnormalizingflows. InProceedingsofthe32nd
InternationalConferenceonMachineLearning,2015.
F. Santambrogio. Optimal Transport for Applied Mathe-
maticians: CalculusofVariations,PDEs,andModeling.
SpringerInternationalPublishing,2015.
Geoffrey Schiebinger, Jian Shu, Marcin Tabaka, Brian
Cleary, Vidya Subramanian, Aryeh Solomon, Joshua
Gould, Siyan Liu, Stacie Lin, Peter Berube, Lia Lee,
JennyChen,JustinBrumbaugh,PhilippeRigollet,Konrad
Hochedlinger,RudolfJaenisch,AvivRegev,andEricS.
Lander. Optimal-transport analysis of single-cell gene
expressionidentifiesdevelopmentaltrajectoriesinrepro-
gramming. Cell,176(4),2019.
VivienSeguy,BharathBhushanDamodaran,RemiFlamary,
Nicolas Courty, Antoine Rolet, and Mathieu Blondel.
Large-ScaleOptimalTransportandMappingEstimation.
InInternationalConferenceonLearningRepresentations,
2018.
Liangliang Shi, Gu Zhang, Haoyu Zhen, Jintao Fan, and
JunchiYan. Understandingandgeneralizingcontrastive
learningfromtheinverseoptimaltransportperspective.
InProceedingsofthe40thInternationalConferenceon
MachineLearning,2023.
Vignesh Ram Somnath, Matteo Pariset, Ya-Ping Hsieh,
MariaRodriguezMartinez,AndreasKrause,andChar-
lotteBunne. AligneddiffusionSchro¨dingerbridges. In
The39thConferenceonUncertaintyinArtificialIntelli-
gence,2023.
Andrew M. Stuart and Marie-Therese Wolfram. Inverse
optimaltransport. SIAMJ.Appl.Math.,80,2019.
AmirhosseinTaghvaeiandAminJalali. 2-Wassersteinap-
proximationviarestrictedconvexpotentialswithapplica-
tiontoimprovedtrainingforgans,2019.
The´oUsciddaandMarcoCuturi. TheMongegap: Aregu-
larizertolearnalltransportmaps. InInternationalCon-
ferenceonMachineLearning,2023.
ChrisWaites. Jax-flows: Normalizingflowsinjax,2020.
7DifferentiableCost-ParameterizedMongeMapEstimators
A.Method
Algorithm1Differentiablecost-parameterizedentropicmappingestimator
(cid:80) (cid:80)
Input: Empiricalmeasuresµˆ = δ ,νˆ= δ ,
i xi j yj
ParameterizedICNNcosth ;
θ
Entropyregularizationscalingvalueε˜;
LossfunctionL(θ);
RegularizingfunctionR(θ);
whilenotconvergeddo
1. ApplySinkhorn’salgorithmbetweentheempiricalmeasuresµˆ,νˆwithentropyregularizationvalueε(thecost
matrixmeanscaledbyε˜),usingthecurrentparameterizedfunctionh ,
θ
2. UseSinkhornoutputg⋆toconstructtheentropicpotentialestimator
ε
fˆε(x)=−εlog(cid:88) exp(cid:18) (g ε⋆) j −h θ(x−y j)(cid:19) , (10)
θ ε
j
3. Constructtheentropicmappingestimator
Tε(x)=x−(∇h )−1◦(∇fˆε)(x), (11)
θ θ θ
4. UpdateθbygradientdescentaccordingtoalossfunctionL(θ)+R(θ)bydifferentiatingthroughSinkhornand
(∇h)−1usingimplicitdifferentiation.
endwhile
WeimplementthedifferentiationthroughSinkhornusingtheOTT-JAXlibrary[Cuturietal.,2022]. WeusetheL-BFGS
solver[Xiaoetal.,2008]fromtheJAXoptlibrary[Blondeletal.,2022]toevaluateandimplicitlydifferentiatethroughthe
innerminimizationwhenevaluating(∇h )−1.
θ
A.1.Costfunctionparameterization
A.1.1.CONVEXFUNCTIONh
Input Convex Neural Networks. ICNNs [Amos et al., 2017] are a class of neural networks f : Rd → R with an
θ
architecturethatensuresthatthemappingx(cid:55)→f (x)isconvex. AnICNNconsistsofkfeedforwardlayers,whereforeach
θ
layeri=0,...,k−1theactivationsaregivenby
z =σ (W(z)z +W(x)x+b ), f (x)=z . (12)
i+1 i i i i i θ k
The network ensures convexity by consisting only of non-negative sums and compositions of convex non-decreasing
functionswithconvexfunctions. Itthereforerequiresthenonlinearactivationfunctionstobeconvexandnon-decreasing,
andtheweightmatricesW(z)tobenon-negative(nosuchrequirementsarerequiredforthepassthroughmatricesW(x)).
i i
Thefirstlayerdoesnotrequireanadditionalpassthroughlayer,sowehaveW(z) =0.
0
ICNNshavepreviouslybeenutilisedincomputationaloptimaltransportmethodstoparameterizeKantorovichpotentials,
whichviaareparameterizationareknowntobeconvexinthecaseofthesquared-Euclideancost[Makkuvaetal.,2020,
Korotinetal.,2021a]. Althoughmathematicallyelegant,Korotinetal.[2021b]notethatconstrainingthepotentialstobe
ICNNscaninfactresultinworseperformanceversusvanillaMLPs. Weremarkthatinourcase,theuseofICNNsinstead
ofMLPstoparameterizethecostiscrucialtoourmethodology,asitensuresthattheinneroptimizationin(8)isconvexand
canbesolvednumerically.
Symmetry. TheadditionalinformationusedtolearnanadaptedMongemapandcostwillnotuniquelydeterminethe
costfunction. Thisisespeciallythecasewhenthereislittleadditionalinformation. Wethereforewishtofavourlearning
simplerandmoreinterpretablecosts. Translation-invariantconvexcostsc(x,y)=h(x−y)provideagoodinductivebias
towardssuchcosts. Wemayalsowishforthecosttobesymmetric;thiscanbeoptionallyenforcedbyparameterizingas
h(z)=h˜ (z)+h˜ (−z)foranICNNh˜ .
θ θ θ
8DifferentiableCost-ParameterizedMongeMapEstimators
α-strongconvexity. Theclosed-formoftheMongemapin(4)requiresstrictconvexityofh. Weenforcethisbyaddinga
quadratictermα∥x−y∥2tothecostfunction. Thisalsoensuresthattheinnerminimization(8)isstronglyconvexandthus
2
theuniquesolutioncanbesolvedforefficientlywithnumericalmethods.
Parameterizingtheconvexconjugate.Weremarkthatafterhavinglearnedanadaptedtransportmap,evaluatingthelearned
mappingatapointxrequiresaminimizationproblemtoevaluate(∇h)−1. Thiscanbeavoidedbyinsteadparameterizing
theconvexconjugateh∗ratherthanh,andusingtherelation(∇h)−1 =∇h∗. Whileavoidingtheinnerminimizationwhen
evaluatingthemap,thisinsteadrequiresaninnerminimizationtoevaluateeachentryofthecostmatrixC =c(x ,y ),
i,j i j
whichisthenfedintoSinkhornateachtrainingiteration. Theresultisthatsignificantlymoreinnerminimizationsare
requiredduringtraining,thoughwiththebenefitthattheresultinglearnedmapcanbeevaluateddirectly. Thechoiceto
parameterizehorh∗shouldthereforedependonwhetherfastevaluationofthelearnedmappingispreferredovertraining
speed.
A.1.2.DIFFEOMORPHISMSΦ µ,Φ
ν
InSection3.2,weextendourframeworktoincorporatecostfunctionsoftheformc(x,y) = h(Φ (x)−Φ (y)). Inour
µ ν
experiments,weusetheMADEflowarchitecture[Papamakariosetal.,2017]toparameterizeΦ ,Φ ,implementedusing
µ ν
theJax-Flowslibrary[Waites,2020].
Relationtometriclearning. WhentakingΦ = Φ thecostc(x,y) = h(Φ(x)−Φ(y))resemblesaSiamesenetwork
µ ν
[Bromleyetal.,1993],acommonapproachinthemetriclearningliterature. SiamesenetworksseektolearnencodersΦso
thattheresultingfunctiond(x,y)= 1∥Φ(x)−Φ(y)∥2isanimprovednotionofdistanceforthedata. However,Indyketal.
2 2
[2017]notethatsuchcompositionscanbeinsufficientlyexpressivetopreciselymodelcertainmetrics. Motivatedbythis,
Pitisetal.[2020]replacethesquared-EuclideandistancewithamodifiedICNN,providingamoreexpressiveclassofneural
distances.
Ourmotivationissimilar. NotethatwecoulduseanICNNψtodirectlyparameterizeaMongemapforacostc(x,y)=
1∥Φ (x)−Φ (y)∥2accordingtothecompositionΦ−1◦∇ψ◦Φ (viaBrenier’stheoremandachangeofvariableasin
2 µ ν 2 ν µ
Theorem3.2). However,byincorporatinganICNNweaddflexibilityinthecostparameterization,allowingustofavour
simplerandmoreinterpretablelearnedcosts.
A.2.ChoiceofLossFunction
Toillustratethegeneralityofourmethod,weprovideherepotentialchoicesoflossfunctionsuitablefordifferentapplications.
Theyencouragethemapestimatortobeconsistentwithknowninformationordesiredbehaviour. Weremarkthatthese
suggestionsareonlyexamplesofpossiblelossfunctionsthatcanbeused,andapractitionerisfreetochooseanydifferentiable
lossfunctionsuitablefortheirneeds. Inparticular,ourmethodismostsuitablewhenbehaviourisdesiredtoholdonthe
mapestimatoritself,asthisistheobjectbeingoptimized.
A.2.1.LABELLEDDATAPOINTS
Paired datapoints. Consider assuming access to a subset {(x ,y )}N of µˆ,νˆ consisting of known pairings from the
i i i=1
optimalmapping, soy = T⋆(x ). WecanoptimizeourmaptoagreewiththeobservedpairingsbypenalizingtheL
i i 2
distancebetweenthepredictionsandtheknowntargets,
N
1 (cid:88)
L(θ):= ∥Tε(x )−y ∥2.
N θ i i 2
i=1
Inthecasewhereallsamplesinµˆ,νˆarepaired,thisrecoverstheInverseOTsetting. Notethatduringtraining,weonlyneed
toevaluateTεatthepaireddatapoints.
θ
Subset-to-subsetcorrespondence. Liuetal.[2020]considerasettinginwhichcertainsourcesubsetsareknowntomapto
(cid:83) (cid:83)
certaintargetsubsets. Ifourempiricalsourceandtargetdistributionsareµ= µ ,ν = ν withµ knowntomaptoν ,
i i i i i i
thenthelossfunctioncanbechosenas
(cid:88)
L(θ)= S (Tε#µˆ,νˆ),
ε˜ θ i i
i
whereS denotestheSinkhorndivergence[Genevayetal.,2018]withregularizationparameterε˜.
ε˜
9DifferentiableCost-ParameterizedMongeMapEstimators
A.2.2.PROPERTIESOFMAPDISPLACEMENTS
Low-rankdisplacements. ToencouragethemapestimatorTεtotransportalongalowerp-dimensionalsubspace,wecan
θ
penalizethemagnitudeofthetrailingsingularvaluesσ(θ)ofthedisplacementmatrix(Tε(x )−x ) ,
i θ i i i
L(θ)=(cid:88) ∥σ(θ)∥2.
i
i>p
k-directionaldisplacements. Wecanencouragethedisplacementstolieprimarilyalongatmostkdistinctdirections. We
canparameterizekdirectionsvϕ =(vϕ,...,vϕ),thenmaximizethecumulativesmoothedminimumofthecosinedistance
1 k
d (u,v)=1− u·v ,
cos ∥u∥∥v∥
L(θ,ϕ)=−(cid:88) LogSumExp(cid:104) −d (cid:0) Tε(x )−x ,vϕ(cid:1)(cid:105) .
cos θ i i j
j
i
A.2.3.REVERSEMAPESTIMATORS
WehavepresentedourmethodasoptimizingaccordingtoalossontheforwardmappingestimatorL(θ)=L(Tε). However,
θ
wecanalsoconstructthereverseentropicmappingestimatoras
gˆε(y)=−εlog(cid:88) exp(cid:18) (f ε⋆) i−h θ(x i−y)(cid:19) , (Tε)−1(y)=y−(∇h˜ )−1◦(∇gˆε)(y),
θ ε θ θ θ
i
whereh˜ (z)=h (−z). Thereversemapestimatorshouldalsobeconsistentwiththeknowninformation,soweoptimize
θ θ
accordingtothesumofthelossesoftheforwardandbackwardsmappingestimators,
L(θ)= 1 L(Tε)+ 1 L(cid:0) (Tε)−1(cid:1) .
2 θ 2 θ
A.3.Regularization
Toensurestabletraining,weaddaregularizationtermR(θ)tothelossusedduringtraining.
Costmatrix. Duringoptimization,theproceduremaylearnacostfunctionwhichplacesverylargeabsolutevalueson
certaindisplacementsinordertoencouragethedesiredbehaviour. AlthoughtheOTmapisinvarianttoscalingofthecost
function,ifsuchvaluesbecomeextremethiscanresultinnumericalinstability. Topreventthis,weaddaregularizationterm
penalizingthesoftmaxofthelargestabsolutevalueinthecostmatrixC ,
θ
(cid:0) (cid:1)2
λ LogSumExp |c (x ,y )| .
max θ i j
i,j
Thevalueofλ canbetakentobeverysmall,sothatithasminimaleffectonoptimizationapartfrompreventingextreme
max
values.
Flows. Recallthatoftenweprefertolearnsymmetriccostfunctions. However,whenthereisalargeamountofavailable
informationaboutthetransportmap,itmightnotbepossibletofitsuchinformationwithasymmetriccost. Insuchcases,
wecanusetwoseparateflows,andwecanencouragethecosttowardssymmetricitybyaddingaregularizingfunctionso
thatΦ ≈Φ ,
µ ν
(cid:88) (cid:88)
∥Φ (x )−Φ (x )∥2+ ∥Φ (y )−Φ (y )∥2.
µ i ν i µ j ν j
i j
Aswewishtoprefersimplercostfunctions,wecancontrolthecomplexityoftheflowbyregularizingusingtheDirichlet
energy,
1(cid:88) 1(cid:88)
∥∇Φ (x )∥2+ ∥∇Φ (y )∥2.
2 µ i 2 2 ν j 2
i j
10DifferentiableCost-ParameterizedMongeMapEstimators
B.RelatedWork
B.1.MongeMapEstimation
Often in applications it is desired to construct an estimator for the Monge map itself. This motivates our approach of
optimizingaMongemapestimatordirectly,toensurethattheresultingmappingagreeswithknowninformation. Wehere
provideanoverviewofmethodstoconstructaMongemapestimator.
B.1.1.NEURALOTMAPESTIMATION
ManyapproachesutiliseneuralnetworkstoparameterizetheKantorovichpotentials,oralternativelythetransportmapitself.
Squared-Euclideancost. Inthecaseofthesquared-Euclideancostc(x,y)= 1∥x−y∥2,theclosedformexpressionfor
2
theMongemapgiveninTheorem2.1reducestothecelebratedBrenier’stheorem,T⋆(x)=x−∇f⋆(x)[Brenier,1987].
Inthiscase,itiscommontoreparameterizethepotentialsbylettingψ = 1∥x∥2−f(x)andφ(y)= 1∥y∥2−g(y). The
2 2
Kantorovichdualproblem(3)becomes
(cid:26)(cid:90) (cid:90) (cid:27)
inf ψdµ+ φdν , (13)
(ψ,φ)∈Φ˜
whereΦ˜ = {(ψ,φ) ∈ L1(µ)×L1(ν) : ψ(x)+φ(y) ≥ ⟨x,y⟩µ⊗ν a.e.}.Thereexistsanoptimalpotentialψ⋆ thatis
convex[Santambrogio,2015],andtheMongemapisgivenbyT⋆(x)=∇ψ⋆(x).
The dual objective (13) and the link to convex analysis form the basis for many successful computational techniques,
thoughasaresulttheyarerestrictedtoonlythesquared-Euclideancost. TaghvaeiandJalali[2019],Makkuvaetal.[2020],
Korotin et al. [2021a] parameterize the dual potentials in (13) using ICNNs. Taghvaei and Jalali [2019] solve for the
convexconjugateψ∗asaninnerminimizationstep,andsubsequentwork[Makkuvaetal.,2020]insteadparameterizeboth
potentialsasICNNsandoptimizeaminimaxobjective. Korotinetal.[2021a]removetheminimaxobjectivebyaddinga
cycle-consistencyregularizertoencouragethepotentialstobeconvex-conjugateuptoaconstant. WhiletheuseofICNNs
toparameterizethepotentialsisappealingfromamathematicalstandpoint,resultsinKorotinetal.[2021b]suggestthatit
mayhinderoptimization.
Althoughthesquared-Euclideancosthasdesirabletheoreticalproperties,itmaynotbeanappropriatechoiceinapplications.
Somnathetal.[2023]commentthatmethodsapproximatingthesquared-EuclideanOTmapcanresultinerroneousmatchings
whengroundtruthcouplingsareknown(demonstratedinFigure1),andGenevayetal.[2018]haveshownittohavepoor
discriminativepropertiesforimagedata. Thismotivateslearninganimprovedcostfunctionmoresuitablefortheproblemat
hand.
General Cost Functions. Recently, several alternative methods have been proposed that are suitable for general cost
functions. Fanetal.[2023]andAsadulaevetal.[2023]optimizeasaddlepointformulationoftheOTproblem,directly
parameterizingthetransportmapandadualpotentialusingneuralnetworks. UsciddaandCuturi[2023]takeanalternative
approach,insteadparameterizingthemappingwithaneuralnetworkandtrainingsothatitfitstothesourcemeasure,with
aMongeGapregularizerthatencouragesthemaptobeoptimalwithrespecttothechosencost. Suchmethodspresent
atrade-offbetweenfittingtothedistribution, andoptimalitywithrespecttothechosencost, bothofwhichneedtobe
minimizediftheresultingmapistobeanaccurateapproximationtotheOTmap.
B.1.2.ENTROPICMAPESTIMATION
EntropicOT.TheentropyregularizedOTproblemaddsaentropicpenaltytermtotheprimalobjective,
(cid:90)(cid:90)
π⋆ = arginf c(x,y)dπ(x,y)+εKL(π|µ⊗ν). (14)
ε
π∈Γ(µ,ν) Rd×Rd
Notethatonlythesupportofthereferencemeasureµ⊗ν affectstheoptimizationproblem[Peyre´ andCuturi,2019]. The
entropyregularization‘blurs’thesolution,enablingdifferentiabilityofboththeentropy-regularizedOTcostandplanwith
respecttotheinputmeasuresandcostfunction. Asaresult,optimaltransporthasenjoyedsuccessinmachinelearning
applicationsforwhichdifferentiabilityisrequired.
ThedualformulationoftheunregularizedKantorovichproblem(3)hasstrictconstraintsonthepotentials,leadingtoa
difficultoptimizationproblem.Theentropic-regularizeddualformulationrelaxestheseconstraints,makingitmoreamenable
11DifferentiableCost-ParameterizedMongeMapEstimators
tooptimization,
(cid:26)(cid:90) (cid:90) (cid:90)(cid:90) (cid:27)
f(x)+g(y)−c(x,y)
sup fdµ+ gdν−ε e ε dµ(x)dν(y) .
f∈C(X)
g∈C(Y)
Stochasticdualoptimization. Genevayetal.[2016]optimizethisdualobjectivewithstochasticgradientdescentusing
samplesfromthemeasures. Inthecaseofcontinuousmeasures,theyparameterizethepotentialsaskernelexpansionsina
ReproducingKernelHilbertSpace. Seguyetal.[2018]insteadproposeusingneuralnetworkstoparameterizethepotentials,
andalsoconstructadeterministicmapusinganeuralnetworktoapproximatetheresultingbarycentricprojection. Inthe
caseofthesquared-Euclideancost,thisbarycentricprojectioncorrespondstotheMongemap.
Sinkhorn. Inpractice,weoftenhaveaccesstodiscreteempiricalapproximationstounderlyingcontinuousdistributions. In
(cid:80) (cid:80)
thediscretecasewithmeasuresµ˜ = a δ ,ν˜= b δ andcostmatrixC =c(x ,y ),theentropy-regularizedOT
i i xi j j yj i,j i j
problemcanbewrittenas
(cid:88)
Π⋆ = min ⟨Π,C⟩−εH(Π), H(Π)=− Π (logΠ −1). (15)
ε i,j i,j
Π∈U(a,b)
i,j
TheSinkhornalgorithm[Cuturi,2013]providesanefficientwaytocomputethesolutiontothediscreteentropy-regularized
OTproblem,andisalsowell-suitedforparallelizationonGPUstosolvemultipleOTproblemssimultaneously. Froman
arbitaryinitializationg(0)andusingthekernelmatrixK =exp(cid:0)−Ci,j(cid:1) ,theSinkhorniteratesaredefinedas
i,j ε
f(ℓ+1) =εloga−εlog(cid:0) Keg(ℓ)/ε(cid:1) , g(ℓ+1) =εlogb−εlog(cid:0) K⊤ef(ℓ+1)/ε(cid:1) . (16)
Entropicmappingestimator. Recallthatforacostfunctionc(x,y)=h(x−y)forstrictlyconvexh,theMongemaphas
form
T⋆(x)=x−(∇h)−1(∇f⋆(x)),
wheref⋆ istheKantorovichpotential. TheKantorovichpotentialscanbechosentosatisfythefollowingh-conjugacy
property[Santambrogio,2015],whichisunfortunatelyadifficultpropertytoenforce,
f⋆(x)=min(cid:8) h(x−y)−g⋆(y)(cid:9) g⋆(y)=min(cid:8) h(x−y)−f⋆(x)(cid:9)
.
y x
Intheentropicallyregularizedcase,theentropicpotentialsf⋆,g⋆insteadsatisfythefollowingrelation,whichisasoftmin
ε ε
relaxationoftheaboveh-conjugacyproperty,
(cid:90) (cid:90)
f ε⋆(x)=−εlog egε⋆(y)− εh(x−y) dν(y), g ε⋆(y)=−εlog efε⋆(x)− εh(x−y) dµ(x).
ThissuggestsusingtheentropicpotentialsinplaceofthetrueKantorovichpotential. Inpractice,weonlyhaveaccessto
discreteempiricalmeasuresµˆ,νˆwhenconstructingaMongemapestimator. Theentropicmappingestimatortherefore
solvesthediscreteentropicOTproblembetweenµˆ,νˆusingSinkhorn,andusestheoutputtoconstructtheentropicpotential
fˆ whichisusedinplaceoff∗intheMongemapexpression. Theresultingentropicmapestimatoris
ε
Tε(x)=x−(∇h)−1(∇fˆ(x)).
h ε
ThisestimatorwasproposedinPooladianandNiles-Weed[2021]forthesquared-Euclideancost,alongwithfinite-sample
guaranteesonitsperformance. ItwasextendedtogeneralconvexfunctionshinCuturietal.[2023],andKleinetal.[2024]
demonstratetheversatilityandgoodperformanceoftheresultingestimator.
AsitisconstructedfromtheSinkhorniterations,theentropicmappingestimatorcanbecomputedefficientlyincomparison
to the alternative approaches outlined above. As we can differentiate through the Sinkhorn iterations, it is therefore
suitableforouraimofconstructingadifferentiableMongemapestimator. Ouradditionalcontributionslieintheuseof
ICNNstoparameterizeh,andintheobservationthattheevaluationof(∇h)−1canbedifferentiatedthroughusingimplicit
differentiation,enablingend-to-enddifferentiabilityoftheestimatorwithrespecttothecostfunctionparameters.
12DifferentiableCost-ParameterizedMongeMapEstimators
B.2.CostLearning
Methodstolearnanadaptedcostfunctionshaveattractedattentionsincetheproblemwasintroducedby[CuturiandAvis,
2014],butremainrelativelyunexploredincomparisontostandardOT.Existingmethodsintheliteratureconsideravariety
ofdifferentproblemsettings.
B.2.1.INVERSEOPTIMALTRANSPORT
Inverseoptimaltransport(iOT)aimstolearnacostfunctiongivenpairedsamples(x ,y )fromtheoptimalorentropic
i i
coupling.
Optimizingadualobjective. ExistingiOTapproachestypicallyassumeaccesstosamplesfromtheentropiccoupling
(x ,y ) ∼ π , which form an empirical joint measure πˆ = 1 (cid:80) δ . They then perform maximum likelihood
i i ε n n i (xi,yi)
estimationforaparameterizedcostc byminimizingthenegativelog-likelihoodgivenbytheconvexfunction
θ
(cid:8) (cid:9)
ℓ(θ)=−⟨c ,πˆ ⟩+ sup ⟨c ,π⟩−εH(π) .
θ n θ
π∈U(a,b)
Thecostisusuallyparameterizedasalinearcombinationofconvexbasisfunctions,andaconvexregularizerR(θ)isadded
toencouragetheparametertobesparse[Carlieretal.,2020,Andradeetal.,2024]orlow-rank[Dupuyetal.,2019]. To
avoidabileveloptimizationprocedure,theproblemisreformulatedasminimizingthedualobjective
(cid:90) (cid:90) (cid:0)f(x)+g(y)−c(x,y)(cid:1)
J(θ,f,g)= c (x,y)−f(x)−g(y)dπˆ (x,y)+ε exp dµ(x)dν(y)+λR(θ). (17)
θ n ε
Maetal.[2021]optimizeasimilardualobjective,andparameterizeboththecostandtheKantorovichpotentialsusing
MLPs. Thenumericalalgorithmproposedin[Andradeetal.,2024]usestheknownformoftheoptimalpotentialgtoinstead
optimizeasemi-dualformulationof(17),whichresultsinabetter-conditionedoptimizationproblem.
DiscreteinverseOThasbeenconsideredfromtheperspectiveofmatchingproblems[Lietal.,2019],contrastivelearning
[Shietal.,2023],andeconomics[DupuyandGalichon,2014,GalichonandSalanie´,2021],aswellasfromaBayesian
perspectiveinStuartandWolfram[2019]. Ratherthanlearningaspecificcostmatrix,Chiuetal.[2022]provideatheoretical
analysisofthesetofpossiblecostmatrices.
TheinverseOTsettingassumesaccesstopairedsamplesfromantransportplan,whichisanunlikelyscenarioinpractice. It
ismorelikelythatwehavepartialinformationaboutdataassociations,suchasasmallersubsetofpairedpoints.Alternatively,
wemayleverageknowninformationaboutthetransportmapspecifictotheproblemathandasaninductivebias. Theabove
inverseOTmethodsareunabletoutilisesuchpartialinformation.
B.2.2.DIFFERENTIATINGTHROUGHSINKHORN
Analternativeapproachforlearningcostfunctionsistooptimizeaccordingalossthatisafunctionoftheentropy-regularized
couplingmatrix,whichcanbeobtainedusingSinkhorn’salgorithm. Thecostparameteristhenupdatedthroughthebilevel
optimizationproblembyunrollingtheSinkhorniterations,orusingimplicitdifferentiation. DifferentiatingthroughSinkhorn
providesamoreflexiblecost-learningapproachincomparisontoinverseOT,asaspecificlossfunctioncanbechosento
encouragethedesiredbehaviourinthecouplingmatrix.
Liuetal.[2020]usethisproceduretolearnacostfunctionfromknownsubsetcorrespondences. TheyuseSinkhorn’s
algorithmtosolvefortheentropiccouplingmatrixaccordingtothecurrentparameterizedcostfunction. Thelossisthenthe
sumofthesquaresoftheentriesintheresultingmatrixcorrespondingtoincorrectmappingsbetweentheknownsubset
assignments. Intheextremecase,thisrecoversinverseOTandthelossfunctionisthesumofthesquaresoftheoff-diagonal
elements.
Klein et al. [2024] differentiate through Sinkhorn to instead learn cost functions based on structural assumptions on
the displacements. They minimize a loss L(θ) = ⟨πε,M(θ)⟩, where the matrix entries M(θ) = τ (x −y ) are a
θ ij θ i j
parameterizedconvexregularizationfunctionevaluatedonthedisplacements. Assuch,theyaimtolearnacostsothat
theresultingcouplingplaceslowmassonvalueswithlargeregularizationvalue. Inparticular,theyconsiderregularizing
functionsτ⊥(z)= 1∥A⊥z∥2promotingdisplacementsinthespanoftheorthonormalmatrixA,whichtheyaimtolearn.
A 2 2
AsdiscussedinAppendixB.1,itisoftenthecasethatwewishtoobtainanestimatorfortheMongemapitself. Learned
costscanbepluggedintoMongemapestimators,thoughitisonlyrecentlythatsuchsolvershavebeendevelopedthat
13DifferentiableCost-ParameterizedMongeMapEstimators
canhandlegeneralground-costfunctions(seeAppendixB.1.1). Suchestimatorscanbedifficulttointerpretastheyusea
neuralnetworkratherthanaclosed-formmapping,andiftheyfailtolearnaccurateapproximationstotheOTmapthen
theresultingmappingmaynotdisplaythedesiredproperties. Moreover,itcouldbedesiredforpropertiestoholdinthe
displacementsofthemappingestimatoritself,ratherthanforthedisplacementsinthecouplingmatrixwhicharefixed
accordingtotheempiricaldistributions. Incontrasttotheaforementionedapproaches,weoptimizeamapestimatoritself.
Thisavoidstheneedforatwo-stepprocedureandinsteadoptimizestheobjectofinterestdirectly,ensuringthattheresulting
mappingdisplaysthedesiredbehaviour.
B.2.3.ALTERNATIVEAPPROACHESTOCOSTLEARNING
Learningimprovedcostfunctionshasbeenconsideredfromthesupervisedmetric-learningperspective,aimingtolearna
groundcostbetweenhistogramsthatagreeswithlabelled‘similarity’coefficientsbetweenpairings[CuturiandAvis,2014,
WangandGuibas,2012]. Heitzetal.[2020]andPooladianetal.[2023]learnacostfromobservationsofadensitythatis
assumedtobeevolvingoptimally. [Heitzetal.,2020]considerdiscretemeasuressupportedongraphs,inwhichthecostis
givenbyageodesiconthegraphparameterizedbyweightsoneachedge,whereasPooladianetal.[2023]learnaRiemannian
metricontheunderlyingcontinuousspace. Genevayetal.[2018],PatyandCuturi[2020]consideranadversarialsettingin
whichthecostischosentomaximizeitsdiscriminativepower.
C.Experimentaldetails
C.1.InverseOT
WetrainaccordingtotheInverseOTlossfunctiongiveninAppendixA.2.1using128pairssampledfrom2-dimensional
T-shape,moon,andspiraldistributions. Weplottheground-truthpairingsandlearnedpredictionsfor32newly-sampled
testpointsinFigure1,alongwiththesquared-Euclideanentropicmapestimatorasacomparison. Inallexperiments,we
parameterizehtobesymmetricand0.01-stronglyconvex,anduseanICNNwithhiddenlayersofsize[32,32]. Wetrain
for500iterations. Forthemoonandspiraldatasetsweusearelativeepsilonvalueof0.01; fortheT-shapedatasetwe
decaytherelativeepsilonfrom0.05to0.002. WeareabletolearnamappingestimatorresemblingtheT-shapedatausing
asimplesymmetriccostoftheformc(x,y) = h(x−y). Forthemoonandspiraldatasets,werequirecostsoftheform
c(x,y)=h(Φ (x)−Φ (y)).
µ ν
C.2.AligningOTmapstoLive-seqtrajectories
OTforcellprofiling. Tracingindividualcelltranscriptometrajectoriesisanimportantprobleminbiologicalapplications,
andaimstoidentifyandpredictresponsesofcellstobiologicalprocessesorexternaltreatments. Mostprofilingtechniques
resultinthedestructionofthecell. Theresultingdatathereforeconsistonlyofindividual‘snapshots’oftheoverallcell
populationwithoutalignment. Inordertoinferhowindividualcellshavedevelopedbetweenthesesnapshots, optimal
transportmethodscanbeusedasaninductivebiastoaligntheobserveddistributions,asitisassumedthatthecellpopulation
hasmoved‘efficiently’inthetimebetweenobservations[Schiebingeretal.,2019,Bunneetal.,2023]. Suchmethods
typicallyusethesquared-Euclideancost,butitisunclearwhetherthisisasuitablenotionofdistanceforgene-expression
data.
Live-seq. Recently,Chenetal.[2022]proposetheLive-seqprofilingtechnique,whichconductsgeneticprofilingusingonly
asmallextractofcytoplasmfromthecell. Thisdoesnotrequirethedestructionofthecellandhasminimaleffectonits
development,thereforeenablingindividualcelltrajectoriestobeobserved. Asonlycellswhichprovideviablesamples
atbothtimestepscanbetraced,thenumberofindividualcelltrajectoriesthatareobservedisfarsmallerthanthenumber
ofobservationsineachcellpopulation. Chenetal.[2022]observethetrajectoriesof12individualcellsbeforeandafter
treatment,alongwithmanymoreunpairedsamples. Itisthereforedesirabletousethisobserveddatatolearnanimproved
costfunctionandcorrespondingOTmap,whichcanthenbeusedindownstreamtaskssuchastrajectorypredictionor
lineagetracing. InverseOTmethodscouldbeusedtolearnfromonlythepairedsamples,buttheyareunabletomakeuseof
theunpairedmajorityinthecellpopulations.
Werestrictourattentiontotheadiposestemandprogenitorcell(ASPC)populationsbeforeandaftercell-differentiation,as
thesecontainthemajorityofobservedcelltrajectories. Oursourceandtargetdistributionsthereforeconsistof121and72
cellmeasurementsrespectively,including9trajectorypairingstracingthesamecell.
Experimentdetails. WetrainaMongemapestimatorforasymmetriccostofformh(Φ(x)−Φ(y))toagreewiththese
14DifferentiableCost-ParameterizedMongeMapEstimators
observedtrajectories,usingthefirst10principalcomponents. Weusearelativeepsilonvalueof0.01,enforce0.01-strong
convexityofh,anduseanICNNwithhiddendimensions[64,64,64]. WetrainboththeICNNsandflowsusingtheAdam
optimizerwithlearningrates3e-3and1e-3respectively,andtrainfor1000iterations.
Forcomparison,weconsideroptimizingthecouplingmatrixratherthanthemapping. Toensureafaircomparison,wefirst
considerusingthesamecostparameterizationh(Φ(x)−Φ(y)),andweusetheentropicmappingestimatortoobtaina
Mongemapestimator. Wealsocomparetolearningageneralunstructuredcostc(x,y)parameterizedbyanMLP.Toobtain
amapestimatorforthiscost,weusetheMongegapregularizer[UsciddaandCuturi,2023]. Whenoptimizingthecoupling
matrix,wemaximizethetotal‘correctmass’accordingtotheknowntrajectories. Thatis,weusethelossfunction
(cid:40)
1 ifx isknowntomaptoy
L(θ)=−⟨M,πε⟩, M = i j
θ ij 0 otherwise.
In Figure 2, we compare the predicted trajectories of the resulting mapping estimators to see whether they are indeed
consistentwiththeknownground-truthtrajectories. InTable1wereporttheRMSEofthepredictionsoftheknownpoints,
andalsotheSinkhorndivergencebetweenthepredictedpointsandthetargetdistribution,averagedover5initializations.
Wealsoreportthetotalincorrectlytransportedmassintheresultingentropy-regularizedcouplingmatrices,definedas
(cid:40)
1 ifx isknowntomaptoy forj ̸=j′,orx isknowntomaptoy fori̸=i′
⟨M,πε⟩, M = i j′ i′ j
θ ij 0 otherwise.
Foragoodlearnedcost,thisvalueshouldbelowastheentropictransportplanwilltransportalargeamountofthemassfor
apointx toitsknownendpointy . Notethatasthecouplingmatrixhasthecorrectmarginals,minimizingthisquantityis
i j
equivalenttotheobjectiveusedwhenoptimizingaccordingtothematrix.
Results. Byoptimizingthemapdirectly,weareabletolearnamappingthatalignswiththeknownpairings(Figure2),
whichshouldbeexpectedgiventhatthiswastheobjectiveusedduringtraining. TheincorrectlytransportedmassinTable1
isalsolow,indicatingthatwehavealsolearnedagoodcostfunctionwhichplacesmassalongtheknowntrajectorypairings.
Incontrast,themapsobtainedbyoptimizingthematrixdonotalignwellwiththeground-truth. Forcostsh(Φ(x)−Φ(y))
optimizingthematrixoftenstrugglestolearnagoodcost,asevidencedbylargeamountsofincorrectlytransportmassin
Table1. Someofthepointsforthecosth(Φ(x)−Φ(y))areapproximatelycorrect,indicatingthatthelearnedcosthas
placedthemasscorrectlyalongsomerowsofthecouplingmatrixbuthasconvergedtoalocalminimum. ThegeneralMLP
c(x,y)learnsacostthatplacesmassalongthecorrecttrajectories,whichisunsurprisinggiventheflexibilityofsuchacost
parameterization. However,asthecostisunstructureditisdifficulttointerpret. Moreover,theunstructuredcostmeansthat
itisdifficulttoobtainanestimatorfortheOTmap,andconsequentlythemapobtainedusingtheMongeGapregularizer
doesnotalignwellwiththeLive-seqtrajectoriesfromwhichthecostwaslearned.
D.AdditionalExperiments
D.1.SyntheticLimitedLabelledPairs
WevalidatethefindingsfromtheLive-seqexperimentbyperformingthesameprocedureonsimilarsyntheticdatasets,
whichenablesustoevaluateperformanceonnewlysampledpoints. Wealsoinvestigatetheeffectofincreasingthenumber
ofpairedpointsontheout-of-sampleperformanceofthelearnedmapping.
Datageneration. WegeneratethesourceandtargetdatasetsbypushingsamplesfromaU = Unif([0,1]d)distribution
throughtworespectiverandomlygenerateddiffeomorphismsΦ ,Φ ,sowehaveµ=Φ #U,ν =Φ #U. Thiscorresponds
1 2 1 2
toacontinuoustransformationfromthesourcetothetargetviathecompositionofthediffeomorphisms,ν =(Φ ◦Φ−1)#µ.
2 1
Notethatwhileitislikelynotthecasethatsuchmappingsconstituteanoptimaltransportmap,suchtransformationsprovide
datadistributionsthatresembletheLive-seqdata. Intheexperiments,thedatasetsconsistofanunpairedmajority(i.e. most
aregeneratedbyindependentlysamplingfromU andpushingthroughthecorrespondingmapping). Asubsetofthedatasets
arepaired,meaningthaty =Φ ◦Φ−1(x). SuchpairssimulatetheknowntrajectoriesintheLive-seqdata.
2 1
Experimentaldetails. Weperformtheprocedurein2,4and6dimensions,with32,128and256sourcesamplesand40,
160and320targetsamplesrespectively. Asubsetofthesedatasetsareknownpairings. Weusethesameexperimental
setupasfortheLive-seqdata. InFigure3,wereporttheincorrectlytransportedmassintheentropy-regularizedcoupling
matrixforthetrainingdataaspreviously,aswellastheRMSEerrorfortheobservedpairedpoints. Wealsogenerateunseen
15DifferentiableCost-ParameterizedMongeMapEstimators
Incorrect Mass Train RMSE Test Sinkhorn Divergence Test RMSE
0.10
0.3 0.4 0.08 0.5
0.3
0.2 0.06 0.4
0.2
0.04 0.3
0.1 0.1
0.02 0.2
4 8 16 4 8 16 4 8 16 4 8 16
0.14 0.9
0.6
0.15 0.12 0.8
0.4
0.10 0.10 0.7
0.2 0.6
0.05 0.08
8 16 32 8 16 32 8 16 32 8 16 32
0.12 0.8
0.10 0.16 0.9
0.6
0.08 0.14 0.8
0.4
0.06 0.12
0.7
0.2
0.04 0.10
0.6
16 32 64 16 32 64 16 32 64 16 32 64
Number of pairs Number of pairs Number of pairs Number of pairs
h( (x) (y)), optimizing map h( (x) (y)), optimizing matrix MLP c(x,y), optimizing matrix
Figure3.Resultsforthesyntheticlimitedlabelledpairsexperiment. Optimizingthemapestimatorresultsinamappingthataligns
significantlybetterwiththeknownpairedpoints,andalsodemonstratesimprovedpredictionforout-of-samplepoints.Thelearnedcost
appearscomparabletothoselearnedbyoptimizingthecoupling.
testsamplesµ˜,ν˜consistingoffullypairedpoints,andreporttheSinkhorndivergenceS (Tˆ#µ˜,ν˜)andtheRMSEofthe
ε
resultingmappingestimatorTˆforthesenewly-sampledpoints. Theresultsareaveragedover5differentrandomly-generated
datadistributions.
Results. TheresultsareconsistentwiththeobservationsintheLive-seqexperiment. Optimizingthemappingappearsto
resultingoodlearnedcosts. Theamountofmassplacedalongincorrectdirectionsinthecouplingmatrixisgenerallysimilar
tothoselearnedbyoptimizingthematrix,whichareinfactoptimizedtominimizethisobjective.
TheOTmapestimatorobtainedfromoptimizingthemappingshowssignificantlybetteralignmentwiththeknownpairings,
whichisasexpectedgiventhatistheobjectivebeingoptimized. Theresultingmapestimatoralsoprovidesbetterout-of-
sampleperformanceonthenewly-sampledtestpoints. Incontrast,thoselearnedfromoptimizingthematrixaremuch
lessconsistentwiththeknownground-truthsandgenerallyperformworseontheout-of-samplepoints. Optimizingthe
mappingalsoappearstoresultinamappinggivingalowerSinkhorndivergencewhentransportingthenewlysampled
points,demonstratinganimprovedfittothetargetatadistributionallevel.
We also remark that the entropic mapping obtained from the cost h(Φ(x) − Φ(y)) learned by optimizing the matrix
occasionallyfailedtogiveanappropriatemapping(givingverylargeSinkhorndivergencesbetweenthemappedpointsand
thetargetdistribution). Thisispresumablybecauseofapoorchoiceofεwhenconstructingtheestimator. Wedisregard
suchresultswhencalculatingtheaveragesinTable3. Incontrast,optimizingthemapdirectlyensuredthatthefinalentropic
mappingwasalwaysreasonable.
D.2.Inducingpropertiesonthetransportmapdisplacements
As we are optimizing according to the OT mapping estimator itself, we can also optimize to encourage properties we
wishtoholdontheresultingdisplacementsthemselves. Thisallowsustoleverageknowledgeaboutthestructureofthe
mappingasaninductivebiaswhenlearningadaptedcostfunctionsandOTmapestimators. Wedemonstratetheabilityto
inducelow-rankand2-directionaldisplacementsbytrainingaccordingtothelossfunctionsproposedinAppendixA.2.2,
withanadditionSinkhorndivergencetermS (Tˆ#µ,ν)toensureagoodfittothetargetdistribution. Wetrainthemap
ϵ
estimatorsusing3-dimensionalempiricalmeasures,eachconsistingof128datapoints,anduseasymmetriccostoftheform
16
04=m,23=n,D2
061=m,821=n,D4
023=m,652=n,D6DifferentiableCost-ParameterizedMongeMapEstimators
Squared-Euclidean Low-rank map estimator 2-directional map estimator
2.0
1.5 1.5 1.5
1.0 1.0 1.0
0.5 0.5 0.5
0.0 0.0 0.0
0.5
1.5 1.5 1.5
1.0 1.0 1.0
1.0 1.0 1.0
0.5 0.5 0.5 0.5 0.5 0.5
0.0 0.0 0.0
0.5 0.0 0.5 0.0 0.5 0.0
= (12.7, 3.68, 1.69) = (12.9, 6.05, 0.306)
source target map estimator T
Figure4.(middle)Thelearnedlow-rankmappingexhibitsdisplacementsprimarilyalonga2-dimensionalplane,asdemonstratedbythe
smallfinalsingularvalue.(right)Thedisplacementsofthelearned2-directionalmappingoccurprimarilyalongthedisplayeddirections,
whichwerelearnedduringtraining.
c(x,y) = h(x−y)withα = 0.01andanICNNwithhiddendimensions[64,64]. Figure4plotsthelearnedmappings
appliedto128newlysampledpoints,againwiththesquared-Euclideanentropicmapestimatorasacomparison. Weseethat
thelearnedmappingsdisplaythedesiredstructuralproperties.
E.Proofs
Proposition3.1. Forastrictlyconvexfunctionh,wehave
(∇h)−1(x)=argmin(cid:8) h(z)−⟨z,x⟩(cid:9)
. (8)
z
Proof. Fixx∈Rd. Ashisstrictlyconvex,soisthefunctiong (z)=h(z)−⟨z,x⟩. Denotetheuniqueminimizerofg (z)
x x
byz∗(x),whichisuniquelydeterminedbythefirst-orderoptimalitycondition,
∇g (z∗(x))=∇h(z∗(x))−x=0. (18)
x
Rearrangingandinverting∇h,weobtain(∇h)−1(x)=z∗(x)asrequired.
Theorem3.2. UndertheconditionsofTheorem2.1withacostoftheformc(x,y) = h(Φ (x)−Φ (y))forastrictly
µ ν
convexfunctionh,theoptimalplanisuniqueandoftheform(Id,T⋆)#µ,andT⋆canbewrittenas
(cid:20) (cid:21)
T⋆(x)=Φ−1 Φ (x)−(∇h)−1◦∇f⋆◦Φ (x) . (9)
ν µ µ
Proof. Definethepush-forwardmeasuresµ˜ =Φ #µ,ν˜=Φ #ν andconsiderthefollowingtwoKantorovichproblems,
µ ν
denotingtheoriginalproblem(K)andatransformedversion(K˜).
(cid:90)(cid:90)
arginf h(Φ (x)−Φ (y))dπ(x,y) (K)
µ ν
π∈Γ(µ,ν) Rd×Rd
(cid:90)(cid:90)
arginf h(x−y)dπ˜(x,y) (K˜)
π˜∈Γ(µ˜,ν˜) Rd×Rd
17DifferentiableCost-ParameterizedMongeMapEstimators
WecanconstructamappingF :Γ(µ,ν)→Γ(µ˜,ν˜)betweentherespectivesetsofadmissibletransportplansdefinedas
π (cid:55)→π˜ =(Φ ⊗Φ )#π. Thefactthatthecouplingπ˜ hasthecorrectmarginalsµ˜,ν˜isaconsequenceofthedefinitionof
µ ν
push-forward;foratestfunctionφ∈C∞(Rd),wehave
(cid:90)(cid:90) (cid:90)(cid:90)
φ(x)dπ˜(x,y)= φ(x)d(Φ ⊗Φ )#π(x,y)
µ ν
Rd×Rd Rd×Rd
(cid:90)(cid:90)
= φ(Φ (x))dπ(x,y)
µ
Rd×Rd
(cid:90)
= φ(Φ (x))dµ(x)
µ
Rd
(cid:90)
= φ(x)dµ˜(x).
Rd
Thisshowsthatπ˜ hascorrectfirstmarginalµ˜anditcanbeshownsimilarlythatthesecondmarginalisν˜,confirmingthat
π˜ ∈ Γ(µ˜,ν˜). ConsidertoothemappingGgivenbyπ˜ (cid:55)→ π = (Φ−1 ⊗Φ−1)#π˜, whichdefinesamapfromΓ(µ˜,ν˜)to
µ ν
Γ(µ,ν). Foratestfunctionφ∈C∞(Rd×Rd),
(cid:90)(cid:90) (cid:90)(cid:90)
φ(x,y)d(Φ−1⊗Φ−1)#(Φ ⊗Φ )#π(x,y)= φ(Φ−1(x),Φ−1(y))d(Φ ⊗Φ )#π(x,y)
µ ν µ ν µ ν µ ν
Rd×Rd Rd×Rd
(cid:90)(cid:90)
= φ((Φ ◦Φ−1)(x),(Φ ◦Φ−1)(y))dπ(x,y)
µ µ ν ν
Rd×Rd
(cid:90)(cid:90)
= φ(x,y)dπ(x,y).
Rd×Rd
ThisshowsthatG◦F = Id,andsimilarlywecanshowthatF ◦G = Id. WethusconcludeG = F−1,andthatF isa
bijectionbetweenΓ(µ,ν)andΓ(µ˜,ν˜).
DefineI andI tobetherespectiveinfimumsfortheKantorovichproblems(K)and(K˜). Foranyπ ∈ Γ(µ,ν),the
K K˜
aboveshowsthatπ˜ =F(π)isanadmissibletransportplanfor(K˜)andthus
(cid:90)(cid:90) (cid:90)(cid:90)
I ≤ h(x−y)dπ˜(x,y)= h(Φ (x)−Φ (y))dπ(x,y). (19)
K˜ µ ν
Rd×Rd Rd×Rd
NotethatasΦ ,Φ arediffeomorphisms,thetransformedKantorovichproblem(K˜)satisfiestheconditionsofTheorem2.1
µ ν
sohasauniquesolutionπ˜⋆. Lettingπˆ =F−1(π˜⋆),wehave
(cid:90)(cid:90) (cid:90)(cid:90)
h(Φ (x)−Φ (y))dπˆ(x,y)= h(x−y)dπ˜⋆(x,y)=I . (20)
µ ν K˜
Rd×Rd Rd×Rd
Weseethatπˆ attainsthelowerboundin(19),andisthereforeanoptimalplanfortheoriginalKantorovichproblem(K).
For uniqueness, note that if there are two such optimal plans π⋆,π⋆ ∈ Γ(µ,ν), then (again using the definition of
1 1
pushforwards) we have that both F(π⋆),F(π⋆) attain the infimum I . By uniqueness of π˜⋆, we must have F(π⋆) =
1 2 K˜ 1
F(π⋆)=π˜⋆,andinvertingF weseeπ⋆ =π⋆asrequired.
2 1 2
Thestructureoftheoptimalplanπ⋆followsfromthatofπ˜⋆. RecallfromTheorem2.1thatπ˜⋆isoftheform(Id,T˜⋆)where
T˜⋆ =Id−(∇h)−1◦(∇f⋆)fortheKantorovichpotentialf⋆. Applyingtheaboveresult,wethereforeseethattheoptimal
planπ⋆forouroriginalKantorovichproblem(K)isalsooftheform(Id,T⋆). ThemapT⋆isnowgivenby
(cid:20) (cid:21)
T⋆(x)=Φ−1 Φ (x)−(∇h)−1◦∇f⋆◦Φ (x) ,
ν µ µ
wheref⋆istheKantorovichpotentialforthetransformedKantorovichproblem(K˜).
18