RMem: Restricted Memory Banks Improve Video Object Segmentation
JunbaoZhou‚àó ZiqiPang‚àó Yu-XiongWang
UniversityofIllinoisUrbana-Champaign
{junbaoz,ziqip2,yxw}@illinois.edu
Abstract Input Videos
With recent video object segmentation (VOS) bench-
Video
marks evolving to challenging scenarios, we revisit a sim- Frames
ple but overlooked strategy: restricting the size of mem-
GT
orybanks. Thisdivergesfromtheprevalentpracticeofex-
Masks
panding memory banks to accommodate extensive histor-
Conventional Unrestricted Memory
ical information. Our specially designed ‚Äúmemory deci-
Long-term ‚àÖ Append Append
phering‚Äùstudyoffersapivotalinsightunderpinningsucha Memory
strategy: expandingmemorybanks, whileseeminglybene- Predicted
ficial, actually increases the difficulty for VOS modules to Masks
decode relevant features due to the confusion from redun-
Our Restricted Memory
dant information. By restricting memory banks to a lim- Long-term Append Select
Memory ‚àÖ Update
ited number of essential frames, we achieve a notable im-
provementinVOSaccuracy. Thisprocessbalancestheim- Predicted
Masks
portance and freshness of frames to maintain an informa-
tivememorybankwithinaboundedcapacity. Additionally, Figure1.Inlightofchallengingobjectstatechanges[40,47,53],
restrictedmemorybanksreducethetraining-inferencedis- werethinktheconventionalVOSapproachofcontinuouslyaccu-
crepancyinmemorylengthscomparedwithcontinuousex- mulatingthefeaturesintomemorybanks:despitecapturingallthe
information, it complicates the deciphering of relevant features.
pansion.Thisfostersnewopportunitiesintemporalreason-
Conversely,restrictedmemorybankssignificantlyenhanceVOS.
ing and enables us to introduce the previously overlooked
‚Äútemporalpositionalembedding.‚Äù Finally,ourinsightsare
based VOS framework [9, 11, 51]. Typically, the memory
embodied in ‚ÄúRMem‚Äù (‚ÄúR‚Äù for restricted), a simple yet
banks are managed via the simple intuition of expansion,
effective VOS modification that excels at challenging VOS
continuouslyappendingnewlysampledframesasthevideo
scenarios and establishes new state of the art for object
progresses. While this approach is intended to encompass
state changes (on the VOST dataset) and long videos (on
allhistoricalinformation,therebyenhancingVOS,wereal-
theLongVideosdataset). Ourcodeanddemoareavailable
izeitspotentiallimitation:asvideosbecomelongerormore
athttps://restricted-memory.github.io/.
complex, these expanding memory banks may overwhelm
thecapabilityofVOSmodulestodiscernreliablefeatures.
1.Introduction
We investigate this hypothesis by conducting a pilot
Therapidprogressofvideoobjectsegmentation(VOS)al-
study,named‚Äúmemorydeciphering,‚Äùtoquantifythedecod-
gorithms has motivated the creation of more challenging
ing capability of VOS modules. In our analysis, we con-
benchmarks, as exemplified by VOST [40] on more com-
tinue to use object segmentation as the proxy to VOS, but
plicated videos with significant object state changes and
shift the prediction target to decoding the object mask at
the Long Videos dataset [27] featuring extremely long du-
the initial frame (frame 0) from the memory bank. This
ration. Thesebenchmarkselevatethespatio-temporalmod-
choice is deliberate based on the principle of controlling
elingandpromptustoreassessconventionalVOSdesigns:
variables: (1) In the VOS framework, the information of
can learning-based VOS modules effectively decipher his-
frame0isimplicitlypropagatedtosubsequentframes, en-
toricalinformationinsuchchallengingscenarios?
suring the presence of relevant information for decoding;
Todelveintothisissue,itisessentialtofocusonmemory
(2)Thispredictiontargetisconsistentacrossframesandal-
banks,whicharecentraltostoringpastfeaturesandfeeding
lowsforafaircomparisonofdecodingefficacyundervary-
inputtoVOSmodules,andarefundamentalinthememory-
ing memory sizes. Intuitively, the later frames have rigor-
*Equalcontribution. ouslyricherinformationthantheearlierframesbecauseof
1
4202
nuJ
21
]VC.sc[
1v67480.6042:viXraa larger memory bank, and are thus expected to produce 2.RelatedWork
better decoding results. However, our observation shows VOS benchmarks. VOS has evolved through several
theopposite: theeffectivenessofVOSmodulesindecipher- benchmarks. DAVIS [35, 36] is the first exhibiting diver-
inginformationdiminisheswithincreasinglylargememory sity and quality, surpassing early benchmarks [5, 25, 41].
banks. Intriguingly, this degradation can be mitigated by YoutubeVOS [46] further scales up by collecting more
selectingasmallnumberofrelevantframesinthememory videos. AlthoughtheyhaveenabledgreatprogressinVOS,
bank,andweobserveasignificantlybetterconcentrationof theirlimiteddifficultyandvideolengthshavespurredmore
attentionscoresonrelevantframesandregions. Therefore, challengingdatasets. Forexample, theaveragedurationin
our systematic study reveals a pivotal insight: the expan- LVOS [20] is more than 500 frames and the Long Videos
sionofmemorybankscomplicatesthedecipheringofVOS dataset [27] further extends it to over 1,000 frames, and
modulesprimarilyduetoredundantinformation. MOSE[15]increasesthedifficultybyselectingvideoswith
Inspiredbysuchaninsight,wevalidateitspracticalsig- crowdsandocclusions. Toevaluateourinsightonthemost
nificance through a simple approach: restricting memory demanding scenarios, we highlight object state changes
banks to a fixed number of frames. Our concise memory involving noticeable transformations in the existence, ap-
bank facilitates better spatio-temporal modeling and adap- pearance, and shapes. Studies on state changes, e.g., VS-
tationtoobjecttransformationaccordingtotheanalysisof COS [53], mostly utilize ego-centric datasets [13, 14, 18].
complex object state changes [40], as illustrated in Fig. 1. In this paper, we primarily select the recent VOST [40].
Theeffectivenessofourmethodstemsfromacuratedmem- It combines multiple datasets and provides accurate an-
oryconciselyfocusingtheattentionofVOSmodulesonrel- notations. Notably, VOST shows higher complexity and
evantinformation.Basedonthis,wedelveintotheupdating longerdurationthanpreviousYoutubeVOSandDAVIS.We
processwhennewfeaturesarrive.Ourstrategybalancesthe mainlyconcentrateonthechallengingbenchmarks.
relevanceandfreshnessofframefeatures,drawinginspira- Memory-basedVOS. Memorybanksarefundamentalfor
tionfromtheupperconfidencebound(UCB)algorithm[3] VOS.Earlierapproaches[4,6,30,38,43]treatVOSason-
frommulti-armbanditproblems. line learning and finetune networks with memorized fea-
In addition to enhancing the accuracy, restricted mem- tures. Some others [7, 21, 44, 48, 50, 52] approach VOS
orybanksreduce discrepanciesinmemorylengthsbetween as template matching but struggle with occluded or dy-
training and inference when compared with conventional namically changing objects. Consequently, recent meth-
methods. Typically, VOS modules are trained on short odsmostlyfocusonmemoryreadingviaeitherpixel-level
clipswithafewmemoryframes,soourrestrictedmemory or object-level attention [42]. Object-level memory read-
bankbetteralignswiththissetup,evenwhenhandlingsig- ing [1, 2, 12], inspired by Mask2Former [8], excels at ef-
nificantly longer videos during inference. This alignment ficiency. However, it is less effective for delicate masks
opensupopportunitiestorevisittechniquesrelyingontem- or complex scenarios, e.g., VOST [40], where the objects
poralsynchronizationbetweentrainingandinference. Asa arefrequentlysmallorcluttered. Incomparison,pixel-wise
compellingexample,weintroducetemporalpositionalem- memoryreading[9,11,17,27,33,39,45,49,51]ismore
bedding to explicitly capture the ordering of memory fea- adopted for its reliable segmentation and it typically asso-
tures‚Äìacriticalaspectoftenoverlookedbypreviousmeth- ciatesthecurrentframetomemoryfeatureswithattention.
ods‚Äìleadingtosuperiortemporalreasoning. Our work differs from previous studies by focusing more
Inconclusion,wemakethefollowingcontributions: onthegeneralinsightsofdrawbacksofexpandingmemory
1. Weintroducethenovelmemorydecipheringanalysisto banksandplug-and-playstrategiestomitigatesuchissues,
systematicallyrevealthedrawbacksofexpandingmem- insteadofdedicatedmemoryreadingarchitectures.
orybanksforVOSmodulesindecodinginformation. Restricted Memory Banks. Previous studies approach
2. Our revisit of restricting memory banks notably en- restricting memory banks mostly from the efficiency as-
hancesVOSaccuracyforchallengingcases,cooperated pect [9, 26, 27]. A notable representative, XMem [9],
with a memory update strategy balancing the relevance adopts a hierarchical architecture with customized modifi-
andfreshnessofframes. cations like memory potentiation. In contrast to prior ef-
3. Benefiting from smaller training-inference gaps, we in- forts, our work explicitly reveal and highlight the accu-
troduce the previously overlooked temporal positional racybenefitsofrestrictedmemorybanksthroughreducing
embeddingtocapturetheorderofmemoryframes. redundant information, rather than emphasizing efficiency.
Collectively, our insights lead to a simple yet strong VOS Moreover, our RMem demonstrates such an insight with a
method: ‚ÄúRMem,‚Äù which is plug-and-play for memory- simpleplug-and-playenhancementtotheVOSframework,
based VOS methods. Our extensive experiments show its avoidinganynoticeableincreaseorrelianceonspecialoper-
strengths and establish new state of the art on VOST [40] atorsasinXMem.WefurthersuggestthatRMem‚Äôsbenefits
forobjectstatechangesandtheLongVideosdataset[27]. arenotconstrainedtoVOS,whererecentworks[37]apply-
2(a) Conventional and Regular VOS (c) Attention Analysis
ùë°=0 ùë°=32 ùë°=64 ùë°=128 ùë°=256 Frame 0 Closest Frame in
mem len=0 mem len=4 mem len=8 mem len=16 mem len=32 (ùêπ!) Memory ùêåùêπ‚Äô:"
Video 0.056
Expanding
Frames
Memory
(ùêº")
Mask of
Target Restricted 0.247
Object
(ùëÜ3 ") Memory
(b) Our ‚ÄúMemory Deciphering‚Äù Analysis on Expanding Memory
Mask Quality (ùêΩ#$%&)
GT 0.86
Decoded
Frame 0 0.85
(ùëÜ!") Expanding Memory Bank 0.84
Restricted Memory Bank 0.83
Figure2.SketchofPilotStudy.Ourmemorydecipheringanalysisemulatesdecodingthemaskonframe0fromthememorybankfeatures
toquantifytheimpactofagrowingmemorybankonVOSmodules, wherethe‚Äúdesiredresults‚Äùinthefigurearethegroundtruth. For
a video shown in Block (a), we visualize its decoding results in Block (b): the masks degrade both quantitatively (yellow curve) and
qualitatively,deviatingfromthedesiredresults. However,selectingasetofconciseframesmitigatesthisissue(bluecurveinBlock(b)).
Therefore,weconjecturethatthedrawbackofagrowingmemorybankliesinconfusingtheattentionofVOSmodules. InBlock(c),we
useredlinestoindicatehighlyweightedassociationsinattention,withthicknessdenotingtheattentionscorevalues. Asillustrated,the
queryF focuseslessonitsmostrelevantframeafterthememorybankexpands,withtheattentionscoredroppingfrom0.247to0.056.
0
(2ndrowshowsground-truthmasksS(cid:101)tasthereference.J meanistheaverageJaccardbetweenS 0t andS(cid:101)0overallvideos.)
inglargelanguagemodelstolongvideosnoticethesimilar setting. Therefore,wepurposefullydesignourmemoryde-
benefitsofcondensingmemoryandselectingframes. cipheringanalysisasdecodingthemaskoftheinitialframe
(frame0)fromthefeaturesstoredinthememorybank.
3.PilotStudy: MemoryDecipheringAnalysis
This section devises our pilot experiments on how an ex- Moreprecisely,ourpilotstudyisformulatedas,
panding memory bank influences the decoding capability St =D‚Ä≤(F ,M[F ]), (2)
0 0 1:t
of VOS modules. Our design emulates the task of VOS where D‚Ä≤(¬∑) is an additional VOS decoder trained for the
butmakesseveralmodificationsguidedbytheprincipleof
objective in Eqn. 2. In practice, we use the original VOS
controllingvariables: thepredictiontargetsandVOSmod- decoder D(¬∑) to conduct regular VOS as Eqn. 1, and then
ules are aligned across our pilot experiments, while only employD‚Ä≤(¬∑)onlyfordecipheringthemaskS0forframe0,
t
the frames in the memory bank vary. Such a comparison toavoidinfluencingtheoriginalVOS.M[F ]containsthe
1:t
enablesacleananalysisandrevealsthecoreinsight: VOS storedfeaturesbetweenframes1tot. Notethatthefeature
moduleshavelimitedcapabilitytodecodeagrowingmem- offrame0isexcludedfromtheinputM[F ]toavoidD‚Ä≤
1:t
orybank.
fromtriviallyrelyingonsingle-framememory.
NotationandFormulationofVOS. Weconsidertheex-
Before delving into the experiments, we emphasize our
istingVOSframeworkasamemory-basedencoder-decoder
reasonsforchoosingthisformulation. (1)Presenceofrele-
network: the encoder E(¬∑) is a visual backbone encoding
vantinformation.TheprocedureinEqn.1resemblespropa-
theimageI atframetintothefeatureF ;andthen,thede-
t t gatingthemasksfromhistoricalframestothecurrentframe
coderD(¬∑)convertsF intothesegmentationS viareading
t t t,indicatingthatM[F ]containstheinformationaboutthe
1:t
thefeaturesstoredinthememoryM[F ],asbelow,
0:t‚àí1 mask at frame 0. Therefore, decoding the mask on frame
F t =E(I t), S t =D(F t,M[F 0:t‚àí1]). (1) 0 from M[F 1:t] is not a random guess, but should achieve
Here,M[F ]generallycomesfromsavingthefeatures high-quality results. (2) Identical prediction target. Our
0:t‚àí1
atacertainfrequency[11,26,51],andtheVOSdecoderis predictiontargetremainsidenticalforeveryframeandvary-
usuallyspecialtransformers[42], e.g., LSTTinAOT[51]. ing memory size. (3) Cooperating with regular VOS. We
The final objective of VOS is to minimize the difference
utilizeD‚Ä≤(¬∑)asastand-aloneVOSdecodersothattheorig-
betweenthepredictedmaskS tandgroundtruthS(cid:101)t. inal VOS process remains unchanged and our pilot study
canutilizethesamememorybank.
DesignofOurMemoryDecipheringAnalysis. Ourpi-
lot study separates the variables of the VOS module D(¬∑) Implementation. WeselecttherecentVOST[40]dataset
andthepredictiontargetS(cid:101)t toclearlyanalyzetheinfluence tohighlightchallengingobjectstatechanges.Itslongvideo
ofthememorybankM[F ]underacontrollingvariable durationandcomplexscenariospushthelimitsofVOSde-
0:t‚àí1
3(a) VOS Framework (b) Memory Update
‚ãØ ‚ãØ
Relevance Select & Remove Freshness
VOS Feature Q VOS Output
Encoder Decoder Append
‚ãØ ‚ãØ
Memory Update
K V (c) Temporal Positional Embedding
Memory Bank
Memory Bank with Size ‚ãØ ‚ãØ V DeV cO odS er
‚ãØ ‚ãØ K
Positional Embedding
‚ãØ ‚ãØ
Figure3. RMemOverview. (a)RMemrevisitsrestrictingmemorybankstoenhanceVOS(Sec.4.1),motivatedbytheinsightfromour
pilotstudy.(b)Tomaintainaninformativememorybank,webalanceboththerelevanceandfreshnessofframeswhenupdatingthelatest
features(Sec.4.2). (c)Benefitingfromsmallermemorysizegapsbetweentrainingandinference, weintroducepreviouslyoverlooked
temporalpositionalembeddingtoencodetheordersofframesexplicitly(Sec.4.3),whichenhancesspatio-temporalreasoning.
codersindecipheringmemory. ThenweadoptAOT[51]as 4.MethodofRMem
the VOS encoder-decoder, a popular baseline and the top Motivated by our insight from the pilot study, we propose
method on VOST. Emulating Eqn. 2, we initialize D‚Ä≤(¬∑) astraightforwardapproachhighlightingaconcisememory
from AOT‚Äôs pretrained decoder D(¬∑), and then supervise bank: restricting the memory with a constant frame num-
S t0 with a segmentation loss between the ground truth S(cid:101)0. ber (Sec. 4.1). We then explore the strategies to update
MoreimplementationdetailsareinSec.B. the memory bank to constantly digest incoming features
HypothesisandExpectations. Withanexpandingmem- and remove obsolete frames (Sec. 4.2). Finally, the re-
ory bank, the information in M[F ] becomes rigorously strictedmemorybankdecreasesthegapbetweenthemem-
1:t
richer at later frames while the prediction target is un- ory lengths across the training and inference stages. This
changed. Therefore,wenaturallyexpectthedecodedmask enablespreviouslyoverlookedtechniques,andwepropose
St toillustratestableorbetteraccuracyatlaterframes,as- a compelling example of temporal positional embedding
0
sumingthattheVOSdecoderD(¬∑)iscapableofextracting (Sec.4.3). Theoverviewofourmethod‚ÄúRMem‚Äù(‚ÄúR‚Äùfor
therelevantfeaturesfromanincreasinglylargeM[F ]. ‚ÄúRestricted‚Äù)isinFig.3.
1:t
ResultsandAnalysis. Contrastingtheexpectationabove, 4.1.RestrictingMemoryBanksforVOS
weobservethatmasksSt degradewithagrowingmemory
0 Design. As indicated in our pilot study (Sec. 3), VOS
bank, as shown in Fig. 2 (b). To verify that the growing
moduleshavelimitedcapabilitytoprocesslargequantities
memory bank is indeed the cause of degradation, we em-
of features and thus benefit from a concise memory bank
pirically bound the memory bank to 8 frames containing
with less redundant information. To verify this in actual
themostrelevantandlatestinformation, intuitively: first7
VOS systems, we develop the simple approach of restrict-
frames and the latest frame in M[F ]. According to the
1:t ingthememorybanktoafixedframenumber. Inpractice,a
bluecurveinFig.2(b),restrictingthememoryonlytostore
pre-definedsmallconstantnumberKisthemaximumnum-
concisefeatureseffectivelyavoidsdegradation.
berofframesamemorybankcanstore,asshowninFig.3.
Inspiredbyaddressingthedegradationissue,wepropose
The simplicity of our approach makes it a plug-and-play
thattheredundantinformationisthemainnegativeimpact
enhancementfortheexistingVOSframework.
of an expanding memory bank. Otherwise, the degrada-
At an arbitrary frame t, we simplify the notation of the
tionshouldnotdisappearsimplyafterweselectasubsetof
memory bank by denoting M[F ] as Mt, containing
intuitively relevant frames. More specifically, this closely 0:t‚àí1
K ‚â§ K frames. AnaturalissueofboundedmemoryMt
relates to how VOS methods utilize attention mechanisms t
isthatK canreachthelimitK atsufficientlylarget,mak-
to read from memory banks, where the redundant features t
ingthedigestionofnewlyarrivingfeaturesnon-trivial, es-
decrease the attention scores on relevant frames. As di-
pecially when the quality of information is vital for VOS,
rectevidence,weanalyzetheattentionscoresfordecoding
accordingtohowweaddressdegradationinthepilotstudy
St in Fig. 2 (c) and observe that the attention scores be-
0 (Sec. 3). Our baseline adopts an intuitively simple yet ef-
tweenF anditsmostrelevantmemoryfeature(firstframe
0 fective approach (we explore better strategies in Sec. 4.2):
inM[F ])haveworseconcentrationonthecorrectobject
1:t selectingthemostreliableframe(frame0)andtemporally
andbecomescatteredinalongermemorybank. Therefore,
most relevant frames (closest frames). Formally, updating
weconcludethatrestrictingthememorybankswithacon-
thememorybankisasbelowwhenK =K:
cisesetofrelevantfeaturespotentiallybenefitsthedecoding t
Mt+1 =Concat(Mt, Mt , F ), (3)
ofVOSmodulesviamorepreciseattention. 0 2:Kt‚àí1 t
4where Mt and F are the closest frames, and Mt is sum of scores as the relevance of a frame in the memory:
removedt2 o:K ct r‚àí ea1 teanavt ailableslot,asshowninFig.3(b1 ). R = sum(St), where St is the slice of attention scores
k k k
correspondingtoMt. ComparedtoXMem[9],whichalso
Discussion. Ourrestrictedmemoryisarevisittoprevious k
usesattentionscoresforselection,ourdesigndiffersinse-
methods [26, 27]. However, we are distinct in emphasiz-
lectingattheframelevelinsteadofthepixellevel,whichis
ing accuracy instead of efficiency. In addition, our RMem
simplerandalreadyeffective(asinSec.5.4).
alsosimplifiesthem[9,26,27]bytreatingeachframeasa
(cid:112)
constituent feature map instead of breaking it into smaller AsforthesecondterminUCB, (2logT)/t j,wemod-
regions or pixels [9]; thus, our strategy can directly apply ify it by defining t j as the times a frame has stayed in the
to a wider range of models. Although more sophisticated memory bank and T as the sum of all the frames‚Äô staying
strategiesmightfurtherimproveouraccuracy,asimpleap- time.Thisfreshnesstermpenalizeslong-stayingframesand
proachisalreadyeffective(Sec.5.3). allows refreshing from the latest information. Finally, O k
combinesitwiththerelevancetermR viaaweightŒ±bal-
4.2.MemoryUpdate k
ancingtheirnumericalscales.
Updatingtheincomingframestothememorybankprovides
informativecuesforVOSmodulestodecode.Althoughour 4.3.MemorywithTemporalAwareness
baseline(Eqn.3)hasalreadycooperatedwiththebounded Motivation. In addition to accommodating the decoding
memorybank,weinvestigatebettermethodsforupdating. capability of VOS modules, restricting the memory bank
Challenges of Memory Update. As shown in our pilot systematically decreases the training-inference discrepan-
study (Sec. 3), improving the conciseness of information cies in memory lengths. Specifically, the VOS algorithms
heavily influences the decoding efficacy of VOS modules. aregenerallytrainedonshortvideoclipswithafewframes
Therefore, naive heuristics of random selection or keeping inthememory,whilethevideosaremuchlongerduringin-
thelatestframesareunreliable(asinSec.5.4,memoryup- ferencetime. Therefore,thenumberofframesinthemem-
date analysis), since they fail to consider the relevance of orybankdivergesmoresignificantlywithoutourrestriction.
frames(random)orsufferfromdriftingofknowledge(lat- Suchtemporalalignmentbetweentrainingandinference
est). To this end, we propose the principles that consider opens new opportunities for VOS. As a compelling exam-
both relevant prototypical features and fresh incoming in- ple, we introduce temporal positional embedding (PE) to
formationfromthelatestframes. enhancespatio-temporalreasoning. Specifically,wenotice
that previous approaches [9, 11, 51] overlook the order of
Memory Update Inspired by Multi-arm Bandits. Our
framesinthememory,i.e.,thetemporalrelationshipamong
memoryupdateproblemcanbestatedashowtoselectand
theframesarenotexplicitlyconsidered,whilespatialPEis
deletethemostobsoleteframek fromKcandidatestocre-
d
widelyadopted. Consideringthevitalroleofordersintem-
ateslotsforincomingfeatures. Althoughnotexactlyiden-
poralmodeling,whichiscommonlyaddressedwithtempo-
tical,thisproblemanalogizesmulti-armbandit[23],which
ralPEinvideo-basedtasks,weconjecturethatthedistinc-
also concerns optimizing the reward by selecting from a
tion of memory sizes between training and inference hin-
fixednumberofcandidates.Itsmostinspiringinsightforus
derspreviousmethodsfromemployingtemporalPE.
isbalancingtheexploitationandexplorationwiththeupper
confidence bound (UCB) algorithm [3], whose maximiza- Design. TheobjectiveoftemporalPEistoembedexplicit
tionobjectiveO foranoptionkisasbelow, temporalawarenessintomemoryandguidetheattentionin
k
(cid:112) Eqn. 5. Although restriction on the memory bank allevi-
O =R + (2logT)/t , (4)
k k k
atesthetraining-inferenceshift,thechallengesoftemporal
where R is option k‚Äôs average reward, T is the total
k PE still exist: the optimal memory size K, though much
timestamps, and t is the number of timestamps select-
k smallerthanexpanding,canstillbelargerthanthetraining-
ing k. When applying to our VOS, we re-define R
k timememorysizeK ; (2)theframesinthememoryare
as the relevance of a frame for reliable VOS and con- train
(cid:112) varying from 1 to K. To address them, our solution is in-
sider (2logT)/t asthefreshnessofmemory,intuitively.
k spiredbyhowViT[16]useslearnablePEandinterpolation
Then,thedeletedframek ischosenaccordingtothesmall-
d toaddressdifferentimageresolutions. Similarly,weinitial-
estO .Inpractice,wedefinetherelevancetermR using
the
at1 t: eK
ntion scores between frame Mt and
currenk
t VOS
izethePEaccordingtoK train,denotedasP(cid:101)0:Ktrain‚àí1,andthe
k queryF havingadedicatedPEP . Then,thetemporalPE
target F , to quantify the contribution of features from the t q
t forthememorybankMt isPt .
memory. Underthecontextoftransformers,weassumede- 0:Kt‚àí1 0:Kt‚àí1
(cid:40)
codingthememorybankisas
Pt =
P(cid:101)0:Kt‚àí1, K
t
‚â§K
train (6)
F tD =Attn(Q=F t,K=Mt,V=Mt), (5) 0:Kt‚àí1 Interp(P(cid:101)0:Ktrain‚àí1,K t), K t >K train
andassumethatStisthescores(aftersoftmax)betweenF
t
where ‚ÄúInterp(¬∑)‚Äù interpolates P(cid:101)0:Ktrain‚àí1 to K
t
via nearest
andMt, computedinsidetheattention. Then, wetreatthe neighbor. Finally,temporalPEenhancestheoriginalatten-
5tioninEqn.5byaugmentingthekeyandvalues, identical Jtr J
toourconceptualillustrationinFig.3(c): OSMNMatch[48] 7.0 8.7
OSMNTune[48] 17.6 23.0
F tD =Attn(Q=F t+P q, CRW[22] 13.9 23.7
CFBI[50] 32.0 45.0
K=Mt 0:Kt‚àí1+P 0t :Kt‚àí1, (7)
CFBI+[52] 32.6 46.0
V=Mt ). XMem[9] 33.8 44.1
0:Kt‚àí1 HODORImg[1] 13.9 24.2
HODORVid[1] 25.4 37.1
The above design contains two critical choices. (1) We
AOT[51] 36.4 48.7
usetherelativeindex{k =0,...,K ‚àí2}insidethememory
t AOTŒ® 37.0 49.2
insteadoftheframeindexttoavoidtheshiftbetweentrain- AOTŒ®+RMem(Ours) 39.8 50.5
ingandinference.(2)UsinglearnablePEinsteadofFourier DeAOTŒ® 37.6 50.1
featuresfitsbettertoalimitedtraininglength,K . DeAOTŒ®+RMem(Ours) 40.4 51.8
train
Table1. ComparisonswithpreviousmethodsonVOST[40]. Our
5.Experiments RMemshowsadvantagesonbothoverallquality(J)andaddress-
5.1.DatasetsandEvaluationMetrics ing object state changes (J tr). (If not specified, the results are
fromVOST‚Äôsimplementation,Œ®denotesourimplementation.)
VOST. WeprimarilyutilizetherecentVOST[40]dataset
J&F J F
thatconcentratesonchallengingobjectstatechanges. Itcu-
CFBI[50] 53.5 50.9 56.1
ratesover700videoscoveringdiverseobjectstatechanges, CFBI+[52] 50.9 47.9 53.8
e.g.,changingappearance,occlusions,crowdedobjects,and STM[34] 80.6 79.9 81.3
MiVOS[10] 81.1 80.2 82.0
fastmotion.InVOST,theevaluationmetricsareJ andJ ,
tr AFB-URR[27] 83.7 82.9 84.5
resemblingtheaverageJaccardoveralltheframesandthe STCN[11] 87.3 85.4 89.2
harderlast25%framescorrespondingtostatechanges. XMem[9] 89.8 88.0 91.6
AOT[51] 84.3 83.2 85.4
Long Videos Dataset. We use the Long Videos AOTŒ® 86.7 85.5 87.9
dataset[27]toevaluatelong-termunderstanding,similarto AOTŒ®+RMem(Ours) 90.3 88.5 92.1
XMem [9]. It contains 3 validationvideos with more than DeAOTŒ® 89.4 87.4 91.4
1kframes.J,F(boundaryFmeasure),andJ&F(average DeAOTŒ®+RMem(Ours) 91.5 89.8 93.3
ofJ,F)areconsideredforevaluation. Table 2. Comparison with previous methods on Long Videos
dataset[27]. ForbothbaselinesofAOTandDeAOT,ourRMem
LVOS. We also experiment with the recent LVOS [20]
showssignificantimprovement. (Withoutmention,theresultsare
datasetandincludetheresultsinSec.C.5.
fromXMem[9],Œ®denotesourimplementation.)
Regular and Short Video Datasets. YoutubeVOS [46]
qualityforthewholevideo(J)andmaintainsbetterrobust-
and DAVIS [35, 36] are two earlier datasets with shorter
nessforthestate-changingframes(J ). Thisisespecially
durationandeasierscenarioscomparedwithVOST.Inthis tr
clearwhencomparedtoAOT[51]:theimprovementisover
paper,weusethemasthepretrainingdatasetsforVOSTand
‚àº3%withourplug-and-playmodifications.
theLongVideosdatasetfollowingstandardpractice[9,40],
andconductanalysisinadditiontothechallengingdatasets. LongVideosDataset. AsourRMemlimitsmemoryca-
pacity, a natural suspicion is that our memory bank per-
5.2.BaselinesandImplementationDetails
formsworseinstoringinformationandstruggleswithlong-
Our proposed RMem is a simple and plug-and-play en-
termmodeling. However,ourcomparisoninTable2shows
hancementfortheVOSframework. Withoutlossofgener-
the opposite. On the Long Videos dataset, our RMem not
ality,weselectAOT[51]andDeAOT[49]asthemainbase-
only improves upon the baseline AOT and DeAOT mod-
linebecauseofitstopperformanceonVOST(asinTable1)
els but also outperforms the state of the art XMem [9]
andsimplicity. ItadoptsResNet-50[19]asitsencoderand
model,whichutilizesspeciallydesignedhierarchicalmem-
aspeciallydesigned‚Äúlongshortterm-transformer‚Äù(LSTT)
orybanksandmemorymanipulationoperators. Therefore,
as its decoder. For the memory bank, the original AOT
thisfurthersupportsourinsightonkeepingaconcisemem-
digests the latest frame and expands the memory continu-
ory bank to accommodate the limited capability of VOS
ously, while RMem restricts its size to 8 frames. We also
modulestoaddressexpandingmemorybanks.
employRMemonotherVOSmethodsinadditiontoAOT.
5.4.AblationStudies
MoredetailsonmodelsandimplementationinSec.B.
EffectofRMemComponents. WeanalyzeeachRMem
5.3.State-of-the-artComparisons
componentrespecttoAOTandDeAOTbaselines,asinTa-
VOST. In Table 1, we compare RMem with previous ble 3. (1) Restricting memory banks. The most important
methods on VOST. Our approach establishes new state of insightfromourpilotstudy(Sec.3)istomaintainaconcise
theartonthischallengingbenchmarkwithasignificantim- memory bank with relevant information, which motivates
provement. Notably,oursimplestrategyincreasestheVOS ourrevisitofrestrictingmemorybanks(Sec.4.1). Accord-
6AOT DeAOT
ID RM TPE MU Jtr J Jtr J 50.5 50.3
1 Baseline 37.0 49.2 37.6 50.9 50.0 49.8 49.6
49.5 49.3
2 ‚úì 38.7 50.3 38.8 51.0
3 ‚úì ‚úì 39.7 50.3 40.0 51.7 49.0 48.7 Unlimited Memory
4 ‚úì ‚úì 39.4 50.3 39.0 51.4 48.5 Restricted Memory
5 ‚úì ‚úì ‚úì 39.8 50.5 40.4 51.8
39.0 38.7
Table3. AblationstudiesofRMemonVOST.Startingfromthe 38.0
38.0 37.6
AOT and DeAOT baselines, all of the components improve the 37.0
tr37.0
performance, especially the harder object state-changing frames
(J ). RM:restrictingmemorybanks. TPE:temporalpositional 36.0
tr 34.7 Unlimited Memory
embedding.MU:memoryupdatewiththeUCBalgorithm. 35.0 Restricted Memory
Method Variant Jtr J 3 5 8 12 20
0th 35.9 48.9 Figure4. ImpactofmemorybanksizeonVOS,testedonVOST.
1st 38.7 50.3   ¬É ¬ö  	 ¬î ¬É ¬è ¬á   ¬ó ¬è ¬Ñ ¬á ¬î  ¬ã ¬ê   ¬á ¬è ¬ë ¬î ¬õ   ¬É ¬ê ¬ç ¬ï
Withmoreframesintherestrictedmemory,theaccuracyfirstin-
Remove Middle 38.3 50.2
Latest 35.7 48.5 creasesandthendecreasesuntilitapproximatesunrestrictedmem-
Random 38.0 50.0 ory.ThissupportsthelimiteddecipheringcapabilityofVOSmod-
Relev 39.1 50.1 ulesandourinsightintorestrictingmemorybanks.
UCB
Relev+Fresh 39.4 50.3
Table4. Ablationstudyofdifferentmemoryupdatingstrategies
Method Jtr J
onVOST.Weanalyzedeletingaframeinthememorybasedon AOT 37.0 49.2
AOT+RM 38.7 50.3
heuristics(‚ÄúRemove‚Äù)orguidedbytherelevanceandfreshnessof
theUCBalgorithm(‚ÄúUCB‚Äù).Ourfinalmemoryupdatingstrategy AOT+SinCosPE 37.2 48.3
AOT+LearnablePE 36.7 49.4
usingbothrelevanceandfreshnessachievesthebestperformance.
AOT+RM+SinCosPE 37.9 48.9
AOT+RM+LearnablePE 39.7 50.3
ingtoTable3(row1and2),aboundedmemorybankleads
Table5. ComparisonoftemporalPEstrategiesonVOST.Based
tosignificantenhancementinthelongandcomplexVOST
onrestrictedmemory(‚ÄúRM‚Äù),ourlearnabletemporalPE(‚ÄúLearn-
videos. (2) Temporal positional embedding. In Table 3,
able‚Äù)isbetterthanusinghigh-frequencyFourierfeatures(‚ÄúSin-
we illustrate that adding positional embedding (Sec. 4.3)
Cos‚Äù).Notably,restrictingmemoryisessentialforPE.
greatly benefits the spatio-temporal modeling, especially
the harder J tr for state changes. (3) Memory update. We critical,whereremovingthelatestframeleadstotheworst
refresh the memory banks by balancing the relevance and accuracy. (3)Randomlyremovingframesperformssurpris-
freshness of frames (Sec. 4.2), inspired by the UCB algo- inglywellbutisstillworsethanourbaseline(removingthe
rithm[3]. Inrows4androws5ofTable3,suchastrategy 1st frame,inSec.4.1). (4)Usingattentionscorestoreflect
effectivelybooststheoverallperformance. therelevancebetterremovesredundantfeatures(‚ÄúRelev‚Äù),
Analysis on Frame Numbers of Memory Banks. We and it is further enhanced with the freshness term, where
verify a direct implication of our insight: an expanding freshnessisespeciallyeffectivetoavoidframesfromstay-
memorybankelevatesthedifficultyofVOSmodulestode- ing long time in the memory bank, supported by the Long
code information. Specifically, we observe the VOS accu- Videodataset.Finally,thebeststrategyisourUCB-inspired
racy under various sizes of memory banks. To avoid the algorithmcombiningrelevanceandfreshness.
influenceofhyper-parametertuning,weutilizethebaseline Temporal Positional Embedding Strategies. We intro-
memory update strategy in Sec. 4.1. As in Fig. 4, the per- duce using learnable temporal PE to address the varied
formancefirstimprovesfromricherinformation.Thenboth frames in the memory banks of VOS in Sec. 4.3. In Ta-
J and J tr decrease when the length of memory exceeds ble 5, we analyze another PE strategy of encoding the in-
the capability of learned AOT modules, until they become dexintohigh-frequencyfeatureswithSinCosfunctionsand
similartounrestrictedmemory. Consequently,theseresults
finditperformsworse.ThisisbecauseSinCosiscommonly
directlysupportourinsightofrestrictingmemorybanks. usedinscenariosofalargenumberorcontinuousspaceof
Memory Update Analysis. Maintaining an informative coordinates(e.g.,NeRF[31]),whilelearnableembeddings
memorybankiscriticalfortheVOSaccuracy,andwepro- can better handle a small number of slots (e.g., ViT [16]),
poseaUCB-inspiredalgorithminSec.4.2.Table4analyzes as in the limited memory length during the VOS training.
thekeyintuitionanddesignchoiceswithAOT.(1)Theini- Furthermore, we highlight that temporal PE requires re-
tial frame is critical in keeping the provided ground-truth strictedmemorytofunctionwellbecauseofbettertraining-
information: removingthe0-thframeleadstoanaccuracy inferencetemporalalignmentinmemorylengths. Thissup-
drop, and is more profound when scenarios are complex ports our intuition in Sec. 4.3 and suggests the emerging
(VOST). (2) Guaranteeing the freshness of information is opportunitiesfromrestrictingmemorybanks.
7(a) Cut Tomato (b) Tear Aluminum Foil
ùë°! ùë°" ùë°# ùë°$ ùë°! ùë°" ùë°# ùë°$
Video
Frames
Ground
Truth
w/o
RMem
w/
RMem
Figure5. (Bestviewedzoom-inwithcolor.) QualitativeVOSresultsforobjectstatechangesonVOST[40]. Weprovidetwoexamples
showingthechallengesofobjectstatechanges,includingslicing,occlusions,distractionfromsimilarobjects(othertomatoes),andshape
changes.Forbothscenarios,usingRMemshowsadvantagesinrobustlymaintainingthemasksofthetargetobjects,ashighlighted.(White
pixelsareannotatedbyVOSTdenoting‚Äúignored‚Äùregionsforevaluation,whicharehardandambiguousevenforhumanannotators.)
Method J&F Jtr J MaxMem‚Üì FPS illustratesanotherdifficultyofobjectshapetransformation
AOT 85.2 82.5 87.9 4.46G 13.67 andsplittingbetweentheboxandthealuminum. Although
AOT+RMem(Ours) 85.2 82.4 88.0 2.34G 15.57 the baseline model without RMem can correctly segment
DeAOT 85.2 82.3 88.1 2.24G 25.11 theboxatthebeginningofsplitting(column2),itgradually
DeAOT+RMem(Ours) 85.3 82.4 88.2 1.53G 27.42
losestrackoftheboxandcanonlyconcentrateonthedom-
Table6. RMemmaintainstheaccuracyonDAVIS2017whilebe-
inant object. However, our model enhanced with RMem
ingmoreefficient,indicatingthatRMemcanbegenerallyapplied,
robustly segments the small regions of the box, indicating
notlimitedtochallengingscenarios.Thisalsoalignswiththeprior
worksandsuggeststhatnothavingdemandingdatasetswaspoten- thatitsattentionassociationwithrelevanthistoricalframes
tially why the accuracy benefits of memory restriction were not isstillstablebecauseofourrestrictedmemory. Therefore,
clearlyrevealedpreviously. weconcludethatthequantitativeresultsrevealthedifficul-
tiesofobjectstatechangesandsupporttheeffectivenessof
Analysis on Regular and Short Video Benchmarks.
ourapproach.
We highlight the improvement on long and complex VOS
datasets, but we also supplement our analysis on the regu- 6.Conclusion
larandshortvideodatasetDAVIS2017. AsinTable6,our This paper reveals the drawbacks of expanding memory
RMemhasrelativelythesameperformancebuteffectively banks, a conventional design in VOS. Our insight stems
improves the efficiency. Compared with our improvement from a novel ‚Äúmemory deciphering‚Äù analysis, which sug-
on VOST and the Long Video dataset, we conjecture that gests that the redundant information in growing memory
the learned VOS modules (AOT and DeAOT) are already banksconfusestheattentionofVOSmodulesandelevates
capableofhandlingshortervideodurationandlesscompli- the difficulty of feature decoding. Then, we propose the
cated scenarios, even without our concise memory banks. simple enhancement for VOS named RMem. At its core
Additionally,thispotentiallyexplainsthatpreviousstudies is restricting the size of memory banks, accompanied by
exploring restricting memory banks [25, 27] have not ex- UCB-inspired memory update strategies and temporal po-
plicitlydiscovereditsbenefits,probablyduetonotconsid- sitional embedding to enhance spatio-temporal reasoning.
eringlongerandmorechallengingdatasetslikeVOST. Extensiveevaluationontherecentchallengingdatasets,in-
5.5.QualitativeResults cluding VOST and the Long Videos dataset, supports our
insightandeffectivenessofRMem.
WevisualizeontworepresentativevideosfromVOST[40]
that require robust spatio-temporal reasoning in Fig. 5. LimitationsandFutureWork. Ourpaperprioritizesthe
Video (a) is the kitchen behavior of cutting a tomato into analysis of memory banks and illustrates our insight with
slices, and it illustrates the challenges of splitting objects, a straightforward approach. Therefore, interesting future
occlusions from hands, and visual distraction from other work is to combine the intuition from more sophisticated
tomatoes. Without our RMem, the baseline AOT model methods,suchasXMem[9]. Furthermore,ourexploration
fails to maintain the masks for the separated tomato slice, mainly adapts memory banks to cooperate with the capa-
while using RMem correctly remembers this slice at the bilityofVOSmodules,whilehowtoimprovethedecoding
later stage of the video (columns 3 and 4). Such regions abilityofVOSmodulesforahugememorybankistheal-
arehighlightedwiththeyellowarrows. Theothervideo(b) ternativedirectionandinterestingfuturework.
8References worth16x16words: Transformersforimagerecognitionat
scale. InICLR,2021. 5,7
[1] AliAthar, JonathonLuiten, AlexanderHermans, DevaRa-
[17] Brendan Duke, Abdalla Ahmed, Christian Wolf, Parham
manan,andBastianLeibe. HODOR:High-levelobjectde-
Aarabi,andGrahamWTaylor. SSTVos: Sparsespatiotem-
scriptors for object re-segmentation in video learned from
poraltransformersforvideoobjectsegmentation. InCVPR,
staticimages. InCVPR,2022. 2,6,14
2021. 2
[2] AliAthar, AlexanderHermans, JonathonLuiten, DevaRa-
[18] Kristen Grauman, Andrew Westbury, Eugene Byrne,
manan, andBastianLeibe. TarVis: Aunifiedapproachfor
Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson
target-basedvideosegmentation. InCVPR,2023. 2
Hamburger,HaoJiang,MiaoLiu,XingyuLiu,etal.Ego4D:
[3] Peter Auer. Using confidence bounds for exploitation-
Around the world in 3,000 hours of egocentric video. In
explorationtrade-offs. JMLR,3(Nov):397‚Äì422,2002. 2,5,
CVPR,2022. 2
7,12
[19] KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun.
[4] Goutam Bhat, Felix Ja¬®remo Lawin, Martin Danelljan, An-
Deep residual learning for image recognition. In CVPR,
dreasRobinson,MichaelFelsberg,LucVanGool,andRadu
2016. 6,11
Timofte. Learningwhattolearnforvideoobjectsegmenta-
tion. InECCV,2020. 2 [20] Lingyi Hong, Wenchao Chen, Zhongying Liu, Wei Zhang,
[5] Thomas Brox and Jitendra Malik. Object segmentation by Pinxue Guo, Zhaoyu Chen, and Wenqiang Zhang. LVOS:
longtermanalysisofpointtrajectories. InECCV,2010. 2 A benchmark for long-term video object segmentation. In
ICCV,2023. 2,6,14
[6] Sergi Caelles, Kevis-Kokitsi Maninis, Jordi Pont-Tuset,
LauraLeal-Taixe¬¥,DanielCremers,andLucVanGool. One- [21] Yuan-Ting Hu, Jia-Bin Huang, and Alexander G Schwing.
shotvideoobjectsegmentation. InCVPR,2017. 2 Videomatch: Matchingbasedvideoobjectsegmentation. In
[7] Yuhua Chen, Jordi Pont-Tuset, Alberto Montes, and Luc ECCV,2018. 2
Van Gool. Blazingly fast video object segmentation with [22] AllanJabri,AndrewOwens,andAlexeiEfros. Space-time
pixel-wisemetriclearning. InCVPR,2018. 2 correspondenceasacontrastiverandomwalk. InNeurIPS,
[8] BowenCheng,IshanMisra,AlexanderGSchwing,Alexan- 2020. 6
der Kirillov, and Rohit Girdhar. Masked-attention mask [23] Michael N Katehakis and Arthur F Veinott Jr. The multi-
transformer for universal image segmentation. In CVPR, armed bandit problem: decomposition and computation.
2022. 2 MathematicsofOperationsResearch,12(2):262‚Äì268,1987.
[9] Ho Kei Cheng and Alexander G Schwing. XMem: Long- 5
term video object segmentation with an atkinson-shiffrin [24] Diederik P Kingma and Jimmy Ba. Adam: A method for
memorymodel. InECCV,2022. 1, 2, 5, 6, 8, 12, 13, 14, stochasticoptimization. ICLR,2014. 12
15
[25] FuxinLi,TaeyoungKim,AhmadHumayun,DavidTsai,and
[10] HoKeiCheng,Yu-WingTai,andChi-KeungTang.Modular JamesMRehg.Videosegmentationbytrackingmanyfigure-
interactive video object segmentation: Interaction-to-mask, groundsegments. InICCV,2013. 2,8
propagationanddifference-awarefusion. InCVPR,2021. 6
[26] YuLi,ZhuoranShen,andYingShan. Fastvideoobjectseg-
[11] HoKeiCheng,Yu-WingTai,andChi-KeungTang.Rethink-
mentationusingtheglobalcontextmodule. InECCV,2020.
ing space-time networks with improved memory coverage
2,3,5
forefficientvideoobjectsegmentation.InNeurIPS,2021.1,
[27] Yongqing Liang, Xin Li, Navid Jafari, and Jim Chen.
2,3,5,6
Video object segmentation with adaptive feature bank and
[12] Ho Kei Cheng, Seoung Wug Oh, Brian Price, Joon-Young
uncertain-regionrefinement. InNeurIPS,2020. 1,2,5,6,8,
Lee, and Alexander Schwing. Putting the object back into
11,12,13
videoobjectsegmentation. InCVPR,2024. 2
[28] Tsung-Yi Lin, Piotr Dolla¬¥r, Ross Girshick, Kaiming He,
[13] Dima Damen, Hazel Doughty, Giovanni Maria Farinella,
Bharath Hariharan, and Serge Belongie. Feature pyramid
Sanja Fidler, Antonino Furnari, Evangelos Kazakos, Da-
networksforobjectdetection. InCVPR,2017. 11
vide Moltisanti, Jonathan Munro, Toby Perrett, Will Price,
[29] IlyaLoshchilovandFrankHutter. Decoupledweightdecay
and Michael Wray. Scaling egocentric vision: The EPIC-
regularization. arXivpreprintarXiv:1711.05101,2017. 12
KITCHENSdataset. InECCV,2018. 2
[14] AhmadDarkhalil,DandanShan,BinZhu,JianMa,Amlan [30] Kevis-Kokitsi Maninis, Sergi Caelles, Yuhua Chen, Jordi
Kar,RichardHiggins,SanjaFidler,DavidFouhey,andDima Pont-Tuset, Laura Leal-Taixe¬¥, Daniel Cremers, and Luc
Damen. EPIC-KITCHENSVISORbenchmark: Videoseg- VanGool. Videoobjectsegmentationwithouttemporalin-
mentationsandobjectrelations. InNeurIPS,2022. 2 formation. TPAMI,41(6):1515‚Äì1530,2018. 2
[15] Henghui Ding, Chang Liu, Shuting He, Xudong Jiang, [31] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,
PhilipHSTorr,andSongBai.Mose:Anewdatasetforvideo JonathanTBarron,RaviRamamoorthi,andRenNg. NeRF:
objectsegmentationincomplexscenes. InICCV,2023. 2 Representingscenesasneuralradiancefieldsforviewsyn-
[16] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, thesis. InECCV,2020. 7
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, [32] Sebastian Nowozin. Optimal decisions from probabilistic
MostafaDehghani,MatthiasMinderer,GeorgHeigold,Syl- models: the intersection-over-union case. In CVPR, 2014.
vainGelly,JakobUszkoreit,andNeilHoulsby. Animageis 11
9[33] SeoungWugOh, Joon-YoungLee, KalyanSunkavalli, and [50] Zongxin Yang, Yunchao Wei, and Yi Yang. Collaborative
SeonJooKim. Fastvideoobjectsegmentationbyreference- video object segmentation by foreground-background inte-
guidedmaskpropagation. InCVPR,2018. 2 gration. InECCV,2020. 2,6,12
[34] SeoungWugOh,Joon-YoungLee,NingXu,andSeonJoo [51] ZongxinYang,YunchaoWei,andYiYang. Associatingob-
Kim. Videoobjectsegmentationusingspace-timememory jects with transformers for video object segmentation. In
networks. InICCV,2019. 6 NeurIPS,2021. 1,2,3,4,5,6,11,14
[35] FedericoPerazzi,JordiPont-Tuset,BrianMcWilliams,Luc [52] Zongxin Yang, Yunchao Wei, and Yi Yang. Collabora-
VanGool,MarkusGross,andAlexanderSorkine-Hornung. tive video object segmentation by multi-scale foreground-
Abenchmarkdatasetandevaluationmethodologyforvideo backgroundintegration. TPAMI,44(9):4701‚Äì4712,2021. 2,
objectsegmentation. InCVPR,2016. 2,6 6
[36] JordiPont-Tuset,FedericoPerazzi,SergiCaelles,PabloAr- [53] JiangweiYu,XiangLi,XinranZhao,HongmingZhang,and
bela¬¥ez, Alex Sorkine-Hornung, and Luc Van Gool. The Yu-XiongWang. Videostate-changingobjectsegmentation.
2017DAVISchallengeonvideoobjectsegmentation. arXiv InICCV,2023. 1,2
preprintarXiv:1704.00675,2017. 2,6,11,12,14
[37] Rui Qian, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Shuan-
gruiDing,DahuaLin,andJiaqiWang.Streaminglongvideo
understanding with large language models. arXiv preprint
arXiv:2405.16009,2024. 2
[38] Andreas Robinson, Felix Jaremo Lawin, Martin Danelljan,
FahadShahbazKhan,andMichaelFelsberg. Learningfast
androbusttargetmodelsforvideoobjectsegmentation. In
CVPR,2020. 2
[39] HongjeSeong,JunhyukHyun,andEuntaiKim. Kernelized
memorynetworkforvideoobjectsegmentation. InECCV,
2020. 2
[40] PavelTokmakov, JieLi, andAdrienGaidon. Breakingthe
‚Äúobject‚Äùinvideoobjectsegmentation. InCVPR,2023. 1,2,
3,6,8,11,12
[41] David Tsai, Matthew Flagg, Atsushi Nakazawa, and
JamesMRehg. Motioncoherenttrackingusingmulti-label
mrfoptimization. IJCV,100(Dec):190‚Äì202,2012. 2
[42] AshishVaswani,NoamShazeer,NikiParmar,JakobUszko-
reit,LlionJones,AidanNGomez,≈ÅukaszKaiser,andIllia
Polosukhin. Attentionisallyouneed. InNeurIPS,2017. 2,
3,11
[43] PaulVoigtlaenderandBastianLeibe. Onlineadaptationof
convolutional neural networks for video object segmenta-
tion. InBMVC,2017. 2
[44] Paul Voigtlaender, Yuning Chai, Florian Schroff, Hartwig
Adam,BastianLeibe,andLiang-ChiehChen.FeelVOS:Fast
end-to-end embedding learning for video object segmenta-
tion. InCVPR,2019. 2
[45] Haozhe Xie, Hongxun Yao, Shangchen Zhou, Shengping
Zhang,andWenxiuSun.Efficientregionalmemorynetwork
forvideoobjectsegmentation. InCVPR,2021. 2
[46] Ning Xu, Linjie Yang, Yuchen Fan, Jianchao Yang,
Dingcheng Yue, Yuchen Liang, Brian Price, Scott Cohen,
and Thomas Huang. Youtube-VOS: Sequence-to-sequence
videoobjectsegmentation. InECCV,2018. 2,6,11,12,14
[47] ZihuiXue,KumarAshutosh,andKristenGrauman. Learn-
ingobjectstatechangesinvideos: Anopen-worldperspec-
tive. InCVPR,2024. 1
[48] LinjieYang, YanranWang, XuehanXiong, JianchaoYang,
andAggelosKKatsaggelos. Efficientvideoobjectsegmen-
tationvianetworkmodulation. InCVPR,2018. 2,6
[49] Zongxin Yang and Yi Yang. Decoupling features in hi-
erarchical propagation for video object segmentation. In
NeurIPS,2022. 2,6,11,14
10Appendix theresolutionoftheoriginalinputimage,respectively. Fol-
lowingthepracticeofAOTandDeAOT,thedeepestfeature
Ourappendixcoveradditionalanalysis,implementationde-
map F16 is the input to the decoder for memory reading,
tails,anddiscussionasbelow:
and{F4,F8}areprovidedtothesegmentationheadasin-
(A) Demo Video. We provide a demo video at
putforpredictinghigh-qualitymasks.
https://youtu.be/mFjGSPXmXdA showing multiple
Decoder. AOT and DeAOT utilize a specially-designed
challengingVOSexamples(Sec.A).
transformer [42] to conduct associative memory reading,
(B) Implementation details. We explain the detailed
named ‚ÄúLong Short-term Transformer‚Äù (LSTT). LSTT
model architectures and the procedures for training
comprises three consecutive transformer layers to enhance
andinference(Sec.B).
featuresinthecurrentframewiththememorybank.Adopt-
(C) Additional ablation studies. This section provides
ingthesamenotationsasEqn.1,weconceptuallyillustrate
moreanalysisandexperimentalresults(Sec.C).
thisprocessasEqn.H:
(D) Additiondiscussiononlimitationsandfuturework.
Weofferamoredetaileddiscussionofthelimitations
andpotentialfuturedirections(Sec.D). F t(l+1) =Attn(Q=F t(l),
K=M(l)[F ], (H)
0:t‚àí1
A.DemoVideo
V=M(l)[F ]),
0:t‚àí1
Inhttps://youtu.be/mFjGSPXmXdA,weprovidefourqual- wherethesuperscript(l)denotesthelayerindexofLSTT,
itative comparison examples between the baseline models ranging from 0 to 2. After the above process, We keep
(AOT[51]andDeAOT[49])andourRMem,withtheobject the implementation details identical to the original AOT
state changes from both VOST [40] and the Long Videos and DeAOT. Please refer to them for more detailed con-
dataset [27]. Notably, these examples illustrate four chal- figuration. Finally,theoutputfeatureF(3) replacesthefea-
t
lengingscenarios:(1)Objectambiguity:objectshavesim- ture map F16 from the encoder not enhanced with spatio-
ilar appearances; (2) Slicing: an object is cut into multi- temporalinformation.
pleslices;(3)Appearancechanges: anobjecthaschanged
Segmentation Head. To maintain high-resolution seg-
itsshapeandappearances,leadingtoincorrectVOSmasks.
mentationmasks, thesegmentationprocessinvolvesafea-
(4)Suddenshapechanges: theviewpointchangesquickly
ture pyramid network (FPN) [28]. It accepts F(4) as the
andcausesvariationinshapesofthetargetobject. Thefour t
input feature, uses {F8,F16} as shortcut inputs, and up-
examplesdemonstratethatRMemeffectivelyimprovesthe
samplesthemviathecombinationofaconvolutionallayer
spatio-temporalreasoningofVOS.
andabi-linearup-samplinglayer.
B.ImplementationDetails Temporal Positional Embedding. We introduce tempo-
ral positional embedding (TPE) in Sec. 4.3 to enhance the
WedescribetheoutlineofimplementationofAOT[51]and spatio-temporalreasoningabilityofmodels.Inpractice,we
DeAOT[49]baselinesinSec.5.2. Thissectionprovidesin- initialize end-to-end learnable embeddings with the same
depthdetailsoftheimplementationandtheconfigurationof numbertothememorylengthduringthetrainingtime(e.g.,
RMem. 4inVOST)andthesamedimensiontothefeatureF ,mark-
t
ingthePEofeachplaceinthememorybank. Forsimplic-
B.1.ModelArchitecture
ity, the three LSTT layers in Eqn. H share the same set of
AOT and DeAOT share the common architecture of the TPE.
memory-based VOS framework. As conceptualized in B.2.Training
Eqn.1, wedisassembletheVOSframeworkintothemod-
ulesofanencoderE(¬∑)encodingimagesintofeaturemaps, LossFunctions. Ourtrainingprocedureutilizesthesame
a decoder D(¬∑) extracting information from the memory loss functions as AOT and DeAOT: the combination of
bank, and a segmentation head translating the output from bootstrapped cross-entropy loss and soft Jaccard loss [32].
decoder into masks. Please note that we have additionally Bothlosstermsareaveraged1:1asthefinallossvalue.
decoupledthesegmentationheadfromthedecoderforclar-
VOST. The training on VOST [40] follows the orig-
ity,comparedwithEqn.1.
inal practice of VOST‚Äôs authors, where the models
Encoder. Identical to VOST [40], we adopt ResNet- are fine-tuned on VOST with pretrained weights from
50[19]astheencoder,whichachievescompetitiveperfor- DAVIS2017 [36] and Youtube2019 [46]. As VOST high-
mance while efficient enough to operate on Long Videos. lights spatio-temporal modeling, we follow the authors‚Äô
The multiple stages in the ResNet encoder produce 3 lev- implementation of AOT by using a long sequence length
els of feature maps {F4,F8,F16} with 1/4, 1/8, and 1/16 of 15 frames during training and this accordingly enables
114 frames in the memory bank. It leverages exponential terminethefrequencyofupdatingmemorybanksbyL/30
moving averages (EMA) for parameter updates to stabi- toavoidCUDAmemoryissues, whichissimilartothein-
lize the training process. The whole training process uses ference procedure on VOST. Our RMem shares the same
AdamW [24, 29] optimizer, and lasts 20,000 steps with a inference setting as baseline, only restricting the memory
batchsizeof8, on4√óA40GPUs. Theinitiallearningrate banksizeto8frames. Then,thememoryupdatestrategyis
is2√ó10‚àí4anditgraduallydecaysto2√ó10‚àí5accordingto identicaltoVOST,asdescribedinSec.B.4.
a polynomial pattern [50]. To avoid overfitting, we set the
B.4.MemoryUpdate
learningrateoftheencoderas0.1oftheothercomponents.
The weight decay is 0.07, which is also identical to AOT As is described in Sec. 4.2, our RMem balances the rele-
andDeAOT. vance and freshness of frames in the memory bank using
Long Videos Dataset. Following the standard prac- ouralgorithminspiredbyUCB[3].
tice [9, 27], we first train the AOT and DeAOT models on Relevance. AsmentionedinSec.4.2,weusetheattention
the DAVIS2017 [36] and YoutubeVOS2019 dataset [46], scoresfromthetransformersinthedecoderEqn.5toreflect
then conduct inference on the Long Videos dataset [27]. therelevanceofamemoryframeR . SincetheLSTTde-
k
However, to support the training of positional embedding, coderinAOTandDeAOThasthreetransformerlayers,we
we extend the length of training samples from the original intuitively select the attention scores from the 0-th trans-
5 frames to 9 frames, to support 4 frames in the memory former because it is closest to the original image embed-
banks during the training time. Please note that we also dings F and memory features Mt (ablation in Sec. C.3).
t
re-trainthebaselinesunderthesamesetuptoensureafair To stabilize the relevance term and avoid fluctuations, we
comparison. The training procedure leverages the simi- furtherapplythemovingaveragetechniquetotherelevance
lar optimization setting as described above for the VOST term. SupposeR‚Ä≤ denotestherelevancevaluesofamem-
k
dataset, including the AdamW [24, 29] optimizer, weight ory frame k derived from the latest timestamp, the conse-
decay of 0.07, polynomial learning rate decay [50] from quentrelevancetermR isupdatedvia:
k
2 √ó 10‚àí4 to 2 √ó 10‚àí5, 0.1 scaling of the encoder learn- ‚Ä≤
R ‚Üê‚àí(1‚àíŒª)R +ŒªR , (I)
k k k
ingrate,andEMAparameterupdates. Theonlydifference
wherewesetŒª = 0.8forbothVOSTandtheLongVideos
from VOST is training 100,000 steps with a batch size of
dataset. Aswehavenoticed,usingmovingaverageforsta-
16, following the implementation of the original AOT and
bilizationisacommontechniqueforVOSonlongvideos,
DeAOTonDAVIS2017andYoutubeVOS2019datasets.
suchasinAFB-URR[27].
B.3.Inference. Freshness. To balance the numerical scales of the rele-
vance and freshness terms, we slightly modify Eqn. 4 as
VOST. Instead of appending features into memory at below,
(cid:115)
a fixed frequency of 5 frames, the authors of VOST logT
O =R +Œ± , (J)
developed a different strategy than on DAVIS2017 and j j t +B
j
YoutubeVOS2019 to address the CUDA memory issue
where B smooths the numerical ranges of the freshness
causedbyhigherresolutionandlongervideoduration: the
term,andŒ±controlstheindividualcontributionofrelevance
memory bank is bounded by 30 frames and the frequency
andfreshness. Inpractice, weset B = 8and Œ± = 1.5 for
of updating memory banks is accordingly L/30, where L
bothVOSTandtheLongVideosdataset. Detailedablation
is the length of the video. For our RMem, we follow the
studiesonthevaluesofŒ±areillustratedinSec.C.2.
frequencyofmemoryupdatessetbyVOST,butboundsthe
size of memory banks to 9 frames, which is significantly C.SupplementalAblationStudies
smaller than the original cap of 30 frames. Therefore, our
RMemneedstoupdatethememorybanksbyremovingthe C.1.MemoryUpdateontheLongVideosDataset
obsoleteframes,andwedescribethedetailsofmemoryup-
We analyze the memory update strategies on the Long
dateinSec.B.4below.
Videos dataset [27] using our AOT baseline in Table A, in
Long Videos Dataset. When comparing to the other ap- addition to the analysis on VOST [40] (Table 4). (1) No-
proachesontheLongVideosdataset(Table2),weprimar- tably, we observe consistent improvement from our UCB-
ily rely on the VOS performance evaluated by XMem [9]. inspiredmemoryupdatestrategycombiningbothrelevance
However, we re-implement the baselines of AOT and and freshness of frames in the memory. (2) Similar to the
DeAOT for a fair comparison with RMem, since XMem resultsonVOST,ourbaselineofremovingthe1-stframein
hasnotreleasedthecodeforevaluatingbothmethods. No- thememoryhascompetitiveperformancebutisinferiorto
tably, our re-implementation achieves better performance ourfinalUCB-inspiredstrategy.(3)TheanalysisinTableA
comparedtoXMem‚Äôsreportednumbers.Inpractice,wede- alsorevealsseveralintriguingdifferencesbetweentheLong
12VideosdatasetandVOST.Specifically,VOSThighlyrelies Varied ùõº Values on VOST
ontherelevanceofframesandthereliableinformationfrom 50.6 50.5 50.5
the0-thframesbecauseofitscomplexityinscenarios,while 50.5 50.5
theLongVideosdatasethighlightstheutilityoffreshnessof 50.4
framesasaconsequenceofextremelylongvideoduration. 50.3
50.2 50.2
50.1
50.1
Method Variants J&F J F 50.1
50.0
0th 88.1 86.3 89.9 0.0 0.5 1.0 1.5 2.0 5.0
1st 88.3 86.6 90.1
Varied ùõº Values on the Long Videos Dataset
Remove Middle 86.6 85.5 87.9
90.0
Latest 85.4 84.1 86.7 89.5 89.5 89.5 89.4
Random 87.7 86.6 88.9
89.0
Relev 86.9 85.4 88.3 88.5
UCB
Relev+Fresh 89.5 87.8 91.2 88.0
TableA.Ablationstudyofdifferentmemoryupdatingstrategies 87.5 87.2 87.2
ontheLongVideosdataset, inadditiontoVOST(Table4). We 87.0 86.9
analyzedeletingaframeinthememorybasedonheuristics(‚ÄúRe- 86.5
0.0 0.5 1.0 1.5 2.0 5.0
move‚Äù)orguidedbytherelevanceandfreshnessoftheUCBal-
gorithm(‚ÄúUCB‚Äù).Ourfinalmemoryupdatingstrategyusingboth Figure A. Analysis on relevance and freshness for memory up-
relevanceandfreshnessachievesthebestperformance. date, on VOST and the Long Videos dataset. The performance
varieswithdifferentŒ±values(fromEqn.J),anditillustratesthe
importanceofthetrade-offbetweenrelevanceandfreshness.
C.2.BalancingRelevanceandFreshness
VOST LongVideo
Methods
As mentioned in Sec. 4.2 and Sec. B.4, we balance rele- J tr J J&F J F
vance and freshness when updating the memory banks via AOT+RMem(0th) 39.8 50.5 90.3 88.5 92.1
Eqn. J. Fig. A analyzes the performance under different Œ± AOT+RMem(Mean) 39.6 50.3 89.8 88.2 91.5
valuesonbothVOSTandtheLongVideosdataset. Specifi- DeAOT+RMem(0th) 40.4 51.8 91.5 89.8 93.3
cally,alargerŒ±denotesrelyingmoreonthefreshnessterm. DeAOT+RMem(Mean) 40.6 52.0 90.3 88.7 92.0
A proper Œ± is essential for the UCB-inspired algorithm
TableB.Analysisontherelevancecalculation.‚Äú0-th‚Äùand‚ÄúMean‚Äù
to improve memory update for both VOST and the Long denoteusingtheattentionscoresfromthe0-thtransformerlayer
Videosdataset,andweempiricallyselectŒ± = 1.5because ortheaverageattentionscoresfromallofthethreelayers.
it generalizes better to both of the datasets. Interestingly,
Fig. A also reveals the difference between VOST and the C.4.AnalysisonTraining-Inferencealignment
Long Videos dataset: VOST has more complex scenarios
AsdiscussedinSec.4.3,thepurposeoftemporalpositional
andhighlightstheutilityofrelevance,whilethelongvideo
embedding is to align the gap between training and infer-
dataset relies more on freshness due to its extremely long
ence,asVOSmodelsaretrainedonshortvideosbutinfer-
video duration. Nonetheless, our final Œ± = 1.5 achieves
encing on unlimited videos. However, it is also valuable
properbalanceforbothdomains.
to explore whether it is another approach to address this
training-inferencegap. WecomparedourRestrictedMem-
C.3.RelevanceCalculation
ory (RM) with 2 approaches: (1) Longer Memory (LM):
Ourrelevancetermformemoryupdateusesattentionscores train the model with longer video clips so that the model
to reflect the importance of a frame, similar to previous can fit better on a larger memory bank. (2) More Steps
works[9,27]. However,LSTThasthreetransformerlayers (MS):trainthemodelwithmoresteps. AsisshowninTa-
andenablestwointuitivestrategiesofrelevancecalculation: ble C, LM certainly is effective in mitigating the training-
(1)directlyusingthe0-thlayer;and(2)computingtheav- inferencegap,butitisstillworsethanourRM.MSexhibits
erage attention scores of all the transformer layers. Table overfittingwithtoomanytrainingsteps,thusnotcapableof
B compares these two strategies on VOST and the Long addressingthisissue. However,MScanstillgainimprove-
Videosdataset. Weobservethatusingthe0-thlayerforrel- ment through our RM, proving our method‚Äôs effectiveness
evancecalculationhasanadvantageinmostofthescenar- fromanotherperspective.
ios. Weconjecturethatthe0-thtransformerhasthelargest
C.5.AnalysisonLVOS
fidelitytothefeaturesofimagesandmemorybanks.There-
fore,ourRMemempiricallyselectsthe0-thtransformerfor Since the Long Videos dataset only features 3 testing
relevance,asdescribedinSec.B.4. videos,whichisnotabletofullydemonstratetheeffective-
13URM RM performancewithlongertrainingclips(TableE,rows1and
Model TrainMemLen Step
J tr J J tr J 3). Undersuchasetupandfaircomparison,ourfullRMem
AOT 4 20k 37.0 49.2 38.6 50.2 hasmaintainedcomparableVOSqualitycomparedwiththe
baseline (rows 3 and 4). In conclusion, our RMem is also
AOT-LM 6 20k 38.2 49.9 39.8 50.1
applicable for YoutubeVOS2019, although tuning the op-
AOT-MS 4 40k 36.6 48.6 37.8 48.0
timal hyper-parameters for training with longer sequence
Table C. Analysis of 2 approaches to address training-inference
lengthsisfuturework.
gap. ‚ÄúURM‚Äù for unrstricted memory and ‚ÄúRM‚Äù for restricted
memory. Our‚ÄúRM‚Äùisstillthebestwaytoaligntrainingandin-
ference. Index Method G J s J u F s F u
1 DeAOT 85.9 84.6 89.4 80.8 88.9
ness of our method, we further report our model‚Äôs perfor- 2 DeAOT+RMem 85.9 84.6 89.4 80.8 88.9
manceonLVOSdataset[20],whichcontains50longvideos
3 DeAOTŒ® 85.6 84.8 80.0 89.7 88.0
inthevalidationset. 4 DeAOTŒ®+RMem 85.5 84.6 79.8 89.4 88.2
TableE.AnalysisonYoutubeVOS2019showsthat,althoughnot
Methods J&F J F
theprimaryfocusofthispaper,ourRMemisalsoapplicablefor
AOT 63.6 57.6 69.5 YoutubeVOS2019 with comparable performance with baselines.
AOT+TPE 64.5 58.9 70.0 We first apply restricted memory banks to the original DeAOT
checkpoint (rows 1 and 2). To enable temporal positional em-
AOT+RMem 66.1 60.5 71.7
bedding(TPE),wetrainDeAOTunderalongersequencelength
TableD.ResultsonthevalidationsetofLVOSdataset. and denote such models with ‚ÄúŒ®‚Äù (rows 3 and 4). The sub-
AsisshowninTableD,ourRMemstillholdsthehigh- scripts ‚Äús‚Äù and ‚Äúu‚Äù denote the ‚Äúseen‚Äù and ‚Äúunseen‚Äù subsets of
YoutubeVOS2019,respectively.
est performance compared to the AOT baseline. Besides,
our TPE (temporal positional embedding) exhibits consid-
D. Additional Discussion on Limitations and
erableimprovements,whichprovesthatTPEiseffectivein
FutureWork
aligning the training-inference gap, given that the average
durationinLVOSismuchlongerthanothervideodatasets.
WebrieflyoutlinedthelimitationsofourstudyinSec.6due
tospacelimits. Thissectionelaboratesonmoredetails.
C.6.AnalysisonYoutubeVOS2019
As mentioned in Sec. 6, we prioritize the analysis of
Our study concentrates on improving the VOS accuracy memorybanks,andRMemisdesignedasastraightforward
for long and/or complex VOS scenarios. Meanwhile, we instantiation to demonstrate our insight. For this purpose,
also supplement with analysis on shorter, simpler bench- our study primarily engages with state-of-the-art methods
marks. As indicated in Table 6, our RMem demonstrates like AOT [51] and DeAOT [49]. This choice is grounded,
comparable performance to baselines without RMem on especiallywhencommonVOSstudiesarebuiltuponasin-
DAVIS2017[36],withanotableincreaseinefficiency.This gle or few preceding approaches due to the complexity
resultunderlinestheadaptabilityofourapproachacrossdif- of the framework, such as XMem [9], HODOR [1], and
ferentregimes. DeAOT [49]. One potential limitation could be that our
Further analysis is conducted in the section using the RMem might implicitly depend on the transformer mech-
YoutubeVOS2019 [46] benchmark, with shorter video du- anisms and the affinity calculation in self-attention, which
rationandeasierscenarios. InTableE,weevaluatetwoset- are adopted in AOT and DeAOT. These mechanisms na-
tings: (1)theinfluenceofonlyrestrictingthememorybank tivelysupportthetemporalpositionalembeddingandalign
sizes; and (2) the effect of the full RMem with temporal withourkeymotivationoffocusingtheattentionscoreson
positional embedding. Table E (rows 1 and 2) shows that: relevant frames (Sec. 3 and Fig. 2). While future endeav-
bylimitingthememorybankswiththeoriginalcheckpoint ors could explore adapting RMem for various VOS meth-
providedbyDeAOT‚Äôsauthors,wemaintainthesameVOS ods beyond the ones using transformers, near-future VOS
quality. Thisfindingsuggeststhatconstrainingthememory methodswilllikelycontinuetoemployatransformer-based
banksisaregime-independentstrategy. framework, making our current RMem design compatible
A key aspect of our RMem is temporal positional em- withthem.
bedding(TPE),whichnecessitatesend-to-endmodeltrain- Another aspect mentioned in Sec. 6 is the potential for
ingonextendedsequences. AsinSec.B.2,weincreasethe enhancing RMem with more advanced techniques. While
trainingsequencelengthfrom5framesto9frameswithout the current simplicity of our approach effectively demon-
tuning the hyper-parameters, ensuring a 4-frame memory strates our core insights into managing memory bank ca-
bankduringthetrainingstage.However,thisintroducesop- pacities, we acknowledge that it can benefit from a more
timizationchallenges,asreflectedinthedecreasedDeAOT sophisticated design. As especially pointed out in Sec. 6,
14XMem [9] exhibits an intricate design for efficiently ex-
panding memory banks. Though more complex than our
current method of simply bounding memory bank sizes,
such advancements could offer greater flexibility and po-
tentiallyimproveVOS.
Lastly, as discussed in Sec. 6, another option for en-
hancement lies in improving the decoding capabilities of
the VOS framework. Our study maintains the original de-
sign of existing methods for a fair comparison, yet future
researchcouldexplorescalingormodifyingVOSarchitec-
turestofurthermitigatethechallengesposedbyexpanding
memorybanks.
15