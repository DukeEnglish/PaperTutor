Enhancing End-to-End Autonomous Driving with
Latent World Model
YingyanLi1,2 LueFan1,2 JiaweiHe1,2 YuqiWang1,2 YuntaoChen3
ZhaoxiangZhang1,2,3(cid:66) TieniuTan1,2
1InstituteofAutomation,ChineseAcademyofSciences
2UniversityofChineseAcademyofSciences
3CAIR,HKISI,CAS
Email:liyingyan2021@ia.ac.cn
Code:https://github.com/BraveGroup/LAW
Abstract
End-to-endautonomousdrivinghasgarneredwidespreadattention. Currentend-
to-endapproacheslargelyrelyonthesupervisionfromperceptiontaskssuchas
detection,tracking,andmapsegmentationtoaidinlearningscenerepresentations.
However, these methods require extensive annotations, hindering the data scal-
ability. To address this challenge, we propose a novel self-supervised method
to enhance end-to-end driving without the need for costly labels. Specifically,
ourframeworkLAWusesaLAtentWorldmodeltopredictfuturelatentfeatures
basedonthepredictedegoactionsandthelatentfeatureofthecurrentframe. The
predictedlatentfeaturesaresupervisedbytheactuallyobservedfeaturesinthe
future. Thissupervisionjointlyoptimizesthelatentfeaturelearningandaction
prediction,whichgreatlyenhancesthedrivingperformance. Asaresult,ourap-
proachachievesstate-of-the-artperformanceinbothopen-loopandclosed-loop
benchmarkswithoutcostlyannotations.
1 Introduction
End-to-end autonomous driving [15, 22, 30, 40, 14] is increasingly recognized for its potential
advantagesovertraditionalmethods. Thetraditionalplannerscannotaccesstheoriginalsensordata.
This leads to information loss and error accumulation [15, 22]. In contrast, end-to-end planners
processsensordatatodirectlyoutputplanningdecisions,whichisshownasapromisingareafor
furtherexploration.
Mostend-to-endautonomousdrivingmethods[15,22,14,30],thoughoperatinginanend-to-end
fashion, leverage a variety of auxiliary tasks such as detection, tracking, and map segmentation.
Theseauxiliarytaskshelpthemodellearnbetterscenerepresentations. However,theyrequirealarge
amountofmanualannotations,whichisquiteexpensiveandlimitsthedatascalability. Incontrast,a
fewend-to-endmethods[35,4,40]donotadoptperceptiontasksandonlylearnfromrecordeddriving
videosandtrajectories. Theseapproachescanleveragealargeamountofavailabledata,makingita
promisingdirection. However,usingonlylimitedguidancefromtrajectoriesmakesitdifficultforthe
networktolearneffectivescenerepresentationsandachieveoptimaldrivingperformance.
Toaddressthisissue,weenhanceend-to-enddrivingthroughself-supervisedlearning,asillustrated
in Fig. 1. Traditional self-supervised methods [10, 6] in imaging typically concentrate on static,
single-frameimages. However,autonomousdrivinginvolvesadynamicseriesofinputs,makingit
Preprint.Underreview.
4202
nuJ
21
]VC.sc[
1v18480.6042:viXraEncoder Planner Encoder Planner
Input Latent Predicted Input Latent Predicted
Image Feature Trajectory Image Feature Trajectory
Supervised by
Self-Supervised
Extensive Annotations
Latent Latent
Occupancy Motion Depth
Prediction Prediction Estimation Feature of T Latent Feature of T+1
World Supervise
Model
Predicted Predicted Latent
Detection Tracking Map Seg.
Trajectory of T Feature of T+1
(a) Previous Auxiliary Tasks (b) Our Self-supervised Task
Figure1: Thecomparisonbetweenthepreviousauxiliarytasksandourlatentpredictiontask.
Whilepreviousworksin(a)relyonauxiliaryperceptiontaskswithextensiveannotations,weaimto
enhancetheend-to-enddrivingmodelthroughthelatentworldmodelin(b). Duringtraining,we
obtainthelatentfeaturefromthefutureframetojointlysupervisethelatentfeatureandpredicted
trajectoryofthecurrentframe. Seg.: Segmentation.
essentialtousetemporaldataeffectively. Akeyskillindrivingispredictingfutureconditionsbased
onthecurrentsurroundings. Inspiredbythis,weproposeaself-supervisedtaskaimedatforecasting
latentfeatures. Specifically,alatentworldmodelisdevelopedtoforecastfuturestatesbasedonthe
currentstatesandegoactions,wherethestatesarerepresentedasthelatentscenefeatureswithinthe
network. Duringtraining,weextractthelatentfeatureofthefutureframetosupervisethepredicted
latentfeaturefromthelatentworldmodel. Asaresult,wejointlyoptimizethelatentfeaturelearning
andtrajectorypredictionofthecurrentframe.
Moreover,weestablishasimpleyetstrongplannertoextractview-wiselatentfeaturesandserve
asthetestbedoftheproposedlatentworldmodel. Unlikepreviousmethods,thisplannerdoesnot
incorporate ad-hoc modules and perception-related branches, making it more straightforward to
understandtheinnerworkingsofthelatentworldmodel. Giventhisplannerandthelatentworld
model,wehavesideproducts. Sincethelatentworldmodeliscapableofpredictingfutureviewlatent
features,wecanskipthefeatureextractionprocessofsomeviewsinthefutureframeandusethe
predictedfuturesoftheseviewsasasubstitution. Byskippingthefeatureextractionforcertainviews,
weenhancetheefficiencyoftheentirepipeline. Todeterminewhichviewsshouldbesubstituted,we
proposeaviewselectionstrategy. Combinedwithviewlatentsubstitution,thisstrategysignificantly
speedsupthewholepipelinewithminimalperformanceloss.
Insummary,ourmaincontributionsareasfollows:
• WeproposeaLAtentWorldmodelforself-supervisedlearningthatenhancesthetrainingof
end-to-endautonomousdrivingframework.
• Basedonthelatentworldmodel,wefurtherproposeaviewselectionstrategy,whichgreatly
acceleratesthepipelinewhileincurringminimalperformanceloss.
• OurframeworkLAWachievesstate-of-the-artresultsonbothopen-loopandclosed-loop
benchmarkswithoutmanualannotations.
2 RelatedWorks
2.1 End-to-EndAutonomousDriving
We divide end-to-end autonomous driving methods [15, 22, 31, 35] into two categories, explicit
methodsandimplicitmethods,dependingonwhetherperformingtraditionalperceptiontasks.
2Explicitend-to-endmethods[2,30,19,34]performmultipleperceptiontaskssimultaneously,such
asdetection[24,17],tracking[45,37],mapsegmentation[15,22]andoccupancyprediction[38,18].
Asapioneeringwork,P3[32]employsadifferentiablesemanticoccupancyrepresentationasacost
factor in the motion planning process. Following this, ST-P3 [14] introduces a spatial-temporal
featurelearningapproachtogeneratemorerepresentativefeaturesforperception,prediction,and
planningtasksconcurrently. Then,manyworks[15,30,20,19]focusonperformingdetectionand
BEVmapsegmentationtasksbasedontheBEVfeaturemap. Asarepresentative,UniAD[15]inte-
gratesmultiplemodules,includingtrackingandmotionprediction,tosupportgoal-drivenplanning.
VAD[22]exploresvectorizedscenerepresentationforplanningpurposes.
Implicitend-to-endmethods[35,4,43,40]presentapromisingdirectionastheyavoidutilizinga
largenumberofperceptionannotations. Earlyimplicitend-to-endmethods[43,35]primarilyrelied
onreinforcementlearning. Forinstance,MaRLn[35]designedareinforcementlearningalgorithm
basedonimplicitaffordances,whileLBC[4]trainedareinforcementlearningexpertusingprivileged
(ground-truthperception)information. Usingtrajectorydatageneratedbythereinforcementlearning
expert, TCP [40] combined a trajectory waypoint branch with a direct control branch, achieving
good performance. However, implicit end-to-end methods often suffer from inadequate scene
representationcapabilities. Ourworkaimstoaddressthisissuethroughlatentprediction.
2.2 WorldModelinAutonomousDriving
Existingworldmodelsinautonomousdrivingcanbecategorizedintotwotypes: image-basedworld
modelsandoccupancy-basedworldmodels. Image-basedworldmodels[11,39,12]aimtoenrich
theautonomousdrivingdatasetthroughgenerativeapproaches. GAIA-1[12]isagenerativeworld
modelthatutilizesvideo, text, andactioninputstocreaterealisticdrivingscenarios. MILE[11]
produces urban driving videos by leveraging 3D geometry as an inductive bias. Drive-WM [39]
utilizes a diffusion model to predict future images and plans based on these predicted images.
Copilot4D[42]tokenizessensorobservationswithVQVAE[36]andthenpredictsthefuturevia
discretediffusion.Anothercategoryinvolvesoccupancy-basedworldmodels[44,29].OccWorld[44]
and DriveWorld [29] use the world model to predict the occupancy, which requires occupancy
annotations. Onthecontrary,ourproposedlatentworldmodelrequiresnomanualannotations.
3 Preliminary
End-to-End Autonomous Driving In the task of end-to-end autonomous driving, the objective
is to estimate the future trajectory of the ego vehicle in the form of waypoints. Formally, let
I = {I1,I2,...,IN} be the set of N surrounding multi-view images captured at time step t.
t t t t
WeexpectthemodeltopredictasequenceofwaypointsW = {w1,w2,...,wM}, whereeach
t t t t
waypointwi =(xi,yi)representsthepredictedBEVpositionoftheegovehicleattimestept+i.
t t t
M representsthenumberoffuturepositionsoftheegovehiclethatthemodelaimstopredict.
WorldModelInautonomousdrivingtasks,aworldmodelaimstopredictfuturestatesbasedonthe
currentstateandactions. Tobespecific,letFˆ representthefeaturesextractedfromthecurrentframe
t
attimestept,W ={w1,w2,...,wM}denotethesequenceofplannedwaypointsbytheplanner,
t t t t
theworldmodelpredictsfeaturesFˆ ofthefutureframeusingFˆ andW .
t+1 t t
4 Method
Theoverallmethodologyisdividedintothreeparts. First,wedevelopastrongandgeneralend-to-end
plannerinSec.4.1toextractlatent1. Next,basedontheend-to-endplanner,weintroduceaworld
modeltopredictlatentinSec.4.2. Finally,thepredictedlatentscansubstituteforsomeunimportant
latentssoweproposeaviewselectionapproachinSec.4.3.
4.1 End-to-EndPlannerwithLatentExtraction
Toextracteffectivelatentfeature,weintroduceageneralandstrongend-to-endplanner. Initially,
N-viewimagesareprocessedthroughanimagebackbonetoextracttheirrespectivefeaturerepre-
1Theterms"latent"and"latentfeature"conveythesamemeaning.
3Waypoint
Queries
Q
Waypoints K,V
K,V Q
Obs.
End-to-End View View View Latents
T Planner Features Queries Latents from T-1
Supervise
Latent
World
Model
K,V Q
Obs. Pred. Action-based
End-to-End View View View View Latent View
T+1
Planner Features Queries Latents Latents Prediction Latents
Figure2: Theoverallframework. Initailly,wedevelopanend-to-enddrivingframeworktoextract
viewlatentsandpredictwaypoints. Then,wepredicttheviewlatentsofthenextframeviathelatent
worldmodel. Thepredictedviewlatentissupervisedbytheobservedviewlatentofthenextframe.
Obs.: Observed,Pred.: Predicted.
sentations. FollowingPETR[25],wegenerate3Dpositionembeddingsfortheseimagefeatures.
Thesepositionembeddingsareintegratedwiththeimagefeaturestouniquelyidentifyeachview. The
enrichedimagefeaturesaredenotedasF={f1,f2,...,fN}.
Then,weemployaviewattentionmechanismtocompressFintoobservedviewlatentV. Here,we
usetheterm"observed"todistinguishthisviewlatentfromothersdiscussedlaterinthepaper. Tobe
specific,forN views,thereareN learnableviewqueriesQ ={q1 ,q2 ,...,qN }. Each
view view view view
viewqueryqi undergoesacross-attentionwithitscorrespondingimagefeaturefi,resultinginN
view
observedviewlatentV={v1,v2,...,vN},where
vi =CrossAttention(qi ,fi,fi). (1)
view
fiservesasthekeyandvalueofthecrossattention. Next,weperformthetemporalaggregationtothe
observedviewlatent. TheobservedviewlatentVisenhancedbyahistoricalviewlatentH,whichis
generatedfromthepreviousframe(thedetailswillbediscussedinSec.4.2). Inthisway,wehave
E=V+H, (2)
where E is named enhanced view latent. Given E, we develop a waypoint decoder to decode
waypoints. This module uses waypoint queries to extract relevant information from E. To be
specific, we initialize M waypoint queries, Q = {q1 ,q2 ,...,qM}, where each query is a
wp wp wp wp
learnableembedding. ThesewaypointqueriesinteractwithEthroughacross-attentionmechanism.
The updated waypoint queries are then passed through an MLP head to output the waypoints
W={w1,w2,...,wM},whichisformulatedas:
wj =MLP(CrossAttention(qj ,E,E)). (3)
wp
Duringtraining,weusetheL1losstomeasurethediscrepancybetweenthepredictedwaypointsand
thegroundtruthwaypointsas:
M
(cid:88)
L = ∥wj −wj,GT∥ , (4)
waypoint t t 1
j=1
Theproposedend-to-endplannerextractsthelatentsimplyandeffectively,whichservesasagood
testbedofthelatentworldmodel.
4
Encoder
Encoder
weiV
weiV
noitnettA
noitnettA
laropmeT
noitagerggA
tniopyaW redoceD4.2 WorldModelforLatentPrediction
Inthissection,weutilizethelatentworldmodeltopredicttheviewlatentsofthefutureframe. To
beginwith,wegenerateaction-basedviewlatentsbasedontheenhancedviewlatentsE andpredicted
t
waypointsW . Specifically,letE ={e1,e2,...,eN},weconvertW ={w1,w2,...,wM}into
t t t t t t t t t
a one-dimensional vector w ∈ R2M. We then concatenate ei and w along the feature channel
(cid:101)t t (cid:101)t
dimension. TheconcatenatedvectoristransformedbyanMLPtoformai,whichmatchesthefeature
t
channeldimensionofei. Formally,theaction-basedviewlatentofthei-thviewisdenotedas:
t
ai =MLP([ei,w ]), (5)
t t (cid:101)t
where [·,·] denotes the concatenating operation. The overall action-based view latent is A =
t
{a1,a2,...,aN}. Subsequently,givenA ,weobtainthepredictedviewlatentP oftheframe
t t t t t+1
t+1bythelatentworldmodel:
P =LatentWorldModel(A ). (6)
t+1 t
Thenetworkarchitectureofthelatentworldmodelisatransformerdecoder,whichconsistsoftwo
blocks. Eachblockcontainsaself-attentionandFFNmodule. Theself-attentionisperformedinthe
viewdimension. Duringtraining,weusetheend-to-endplannertoextracttheobservedviewlatent
V offramet+1. V servesasthesupervisionofP usinganL2lossfunction:
t+1 t+1 t+1
N
(cid:88)
L = ∥pi −vi ∥ , (7)
latent t+1 t+1 2
i=1
whereP ={p1 ,...,pN }andV ={v1 ,...,vN }.
t+1 t+1 t+1 t+1 t+1 t+1
Besides, givenA , weencodethetemporalinformationintothehistoryviewlatentH . H
t t+1 t+1
is used to enhance the observed view latent V through Eq. (2). To be specific, we conduct
t+1
self-attentiononA intheviewdimension,obtaining
t
H =SelfAttention(A ). (8)
t+1 t
H andP servedistinctfunctions. H aimstoencodetemporalinformationasaresidual,
t+1 t+1 t+1
whereasP isdesignedtopredicttheviewlatentofthefutureframe. Inaddition,P serves
t+1 t+1
asagoodsubstitutefortheobservedviewlatentoffutureframes,whichinspiresustoproposethe
conceptofviewselectionwithlatentsubstitution.
4.3 ViewSelectionviaLatentSubstitution
We propose a view selection approach thanks to the effective view latent predicted by the world
model. Taking multi-view videos as input, this approach dynamically selects some informative
viewstoextractfeatures. Theotherviewsarenotprocessedandtheircorrespondingviewlatents
aresubstitutedbythepredictedviewlatentfromtheworldmodel. AsshowninFig.3,thissection
consistsofthreecomponents. First,givenseveralpotentialviewselectionstrategies,theSelection
RewardPredictioncomponentpredictstherewardsofthesestrategiesandchoosesthestrategywith
thehighestreward. Then,thePlannerwithSelectedViewspredictsthetrajectorygiventheselected
views. Duringtraining,weproposeaSelectionRewardLabelingmodule,whichassignsareward
labeltoeachselectionstrategy.
SelectionRewardPredictionAsillustratedinFig.3(a)and(b),weintroducearewardprediction
moduledesignedtoestimatetherewardassociatedwitheachselectionstrategy. Therewardquantita-
tivelyreflectstheeffectivenessoftheplanningoutcomesobtainedusingeachstrategy. Indetail,we
defineK selectionqueries. TheseK selectionqueriescorrespondtoK potentialselectionstrategies.
Eachselectionqueryisalearnableembedding. Eachstrategyselectsspecificviewsforprocessing
while discarding the rest. Then, we update the selection queries by performing cross-attention
betweenthequeriesandtheviewlatentP predictedbytheworldmodel. Theupdatedselection
t+1
queriesarefedintoanMLPheadtopredicttherewards. Giventheserewards,wechoosethestrategy
withthehighestpredictedreward. Thestrategyischosenattheframetandtheviewsselectedbythis
strategyserveastheinputofthePlannerwithSelectedViewsatframet+1.
PlannerwithSelectedViewsThisplannertakesselectedviewsasinputtoproducewaypointsas
Fig.3(c)shows. ItsharesthesameweightsastheplannerinSec.4.1. Tobespecific,LetN represent
5Substitute
（a) Sele Pc rt eio dn ic R tioe nward RH eig wh ae rs dt （c) K, QV V Ai te tnw .. SP ell ea cn tn ee dr Vw ieit wh s
Reward
Prediction Waypoint
K,V Module V Ai te tnw .. Decoder
Q
Pred. Obs. Pred.
Pred. Selection
Rewards View View
View Queries Selected View View
Latents Latents Waypoints
TLatents T+1 Views Features Queries
（b) （d) Supervise Process All Views Training Only
During Training
L2 Selection
Dist. K,V Different Reward
Strategies Labeling
Q
View Selection Reward Predicted Waypoint View Selection
Strategies Queries Labels GT WaypointsWaypoints Queries Strategies Obs. Pred.
Figure3: Thepipelineofend-to-enddrivingwithviewselection. Giventhepredictedviewlatents,
(a)predictrewardsfortheselectionqueriesdefinedin(b),whereeachquerycorrespondstoaselection
strategy. Then,weadopttheselectionstrategywiththebestrewardstoselectviews,whichservesas
theinputoftheplannerin(c). Duringtraining,weproposeaselectionrewardlabelingmodulein(d)
tolabeleachselectionstrategy. Pred.: Predicted,Obs.: Observed. L2Dist.: L2Distance.
thetotalnumberofviews. DefineS ⊆{1,2,...,N}asthesetofindicesforviewsthatareselected.
S¯isdenotedasthecomplementofS. Givenv ∈E andvˆ ∈P ,thecombinedviewlatents
i t+1 j t+1
v areformulatedas:
combined
(cid:77) (cid:77)
v = v ⊕ vˆ , (9)
combined i j
i∈S j∈S¯
(cid:76)
where denotes the concatenation operation over the latents in the view dimension. Then, the
combinedviewlatentsarepassedthroughthesamepipelineinSec.4.1topredicttrajectories. Based
on the Planner with Selected Views, we propose a selection reward labeling module to label the
selectionstrategies.
SelectionRewardLabelingWeintroducetherewardlabelingapproachasFig3(d)shows. Specifi-
cally,forthek-thstrategy,thecorrespondingselectedviewsarefedintothePlannerwithSelected
Viewstopredictwaypointswˆ . Therewardlabeldˆ ofk-thstrategyisdefinedastheL2distance
k k
betweenthepredictedwaypointswˆ andthegroundtruthwaypointswˆGT as:
k
dˆ =−∥wˆ −wˆGT∥ . (10)
k k 2
Thelargerdˆ is,thecloserwˆ aretothegroundtruthwaypointswˆGT. Duringtraining,weusethe
k k
L1losstolearntherewards,formulatedasL =(cid:80)K ||d −dˆ || ,
reward k=1 k k 1
Insummary,thetotallossofourframeworkis:
L =L +L +L , (11)
total waypoint latent reward
whereL isanoptionallossdependingonwhetherusingtheviewselectionapproach. Theweight
reward
ofeachlossisdiscussedintheimplementationdetails.
5 Experiments
5.1 Setup
Open-loopBenchmarkTheopen-loopbenchmarkusesrecordedvideostreamsofexpertdrivers
alongwiththecorrespondingtrajectoriesoftheegovehicle. Weconductourexperimentsonthe
nuScenesdataset[1],whichcomprises1,000drivingscenes. Inlinewithpreviousworks[14,16,22],
we employ Displacement Error (DE) and Collision Rate (CR) to comprehensively evaluate the
6
Encoder
tniopyaW redoceDTable1: ComparisonwithState-of-the-artmethodsontheopen-loopnuScenes[1]Benchmark.
FPSofST-P3,VADandLAWaretestedontheNVIDIAGeforceRTX3090GPU.FPSofUniADis
testedontheNVIDIATeslaA100GPU.‡:LiDAR-basedmethods. Wedonotuseanyhistoricalego
statusinformation.
L2(m)↓ Collision(%)↓
Method Latency(ms) FPS
1s 2s 3s Avg. 1s 2s 3s Avg.
NMP‡[41] - - 2.31 - - - 1.92 - - -
SA-NMP‡[41] - - 2.05 - - - 1.59 - - -
FF‡[13] 0.55 1.20 2.54 1.43 0.06 0.17 1.07 0.43 - -
EO‡[23] 0.67 1.36 2.78 1.60 0.04 0.09 0.88 0.33 - -
ST-P3[14] 1.33 2.11 2.90 2.11 0.23 0.62 1.27 0.71 628.3 1.6
UniAD[15] 0.48 0.96 1.65 1.03 0.05 0.17 0.71 0.31 555.6 1.8
VAD[22] 0.41 0.70 1.05 0.72 0.07 0.17 0.41 0.22 224.3 4.5
LAW 0.26 0.57 1.01 0.61 0.14 0.21 0.54 0.30 51.2 19.5
planningperformance. TheDisplacementErrormeasurestheL2distancebetweenthepredicted
trajectoryandtheGTtrajectory. TheCollisionRatequantifiestherateofcollisionsthatoccurwith
otherobjectswhenfollowingthepredictedtrajectory.
Closed-loopBenchmarkClosed-loopevaluationisessentialtoautonomousdrivingasitconstantly
updatesthesensorinputsbasedonthedrivingactions. Thetrainingdatasetiscollectedfromthe
CARLA [9] simulator (version 0.9.10.1) using the teacher model Roach [43] following [40, 20],
resultingin189Kframes. Weusethewidely-usedTown05Longbenchmark[20,33,11]toassess
theclosed-loopdrivingperformance. Weusetheofficialmetrics: RouteCompletion(RC)represents
thepercentageoftheroutecompletedbytheautonomousagent. InfractionScore(IS)quantifiesthe
numberofinfractionsaswellasviolationsoftrafficrules. AhigherInfractionScoreindicatesbetter
adherencetosafedrivingpractices. DrivingScore(DS)istheprimarymetricusedtoevaluateoverall
performance. ItiscalculatedastheproductofRouteCompletionandInfractionScore.
Table2: Performanceonclosed-loopTown05LongbenchmarkonCARLA.Expert: Imitation
learningfromthedrivingtrajectoriesofaprivilegedexpert. Seg.: SemanticSegmentation. Map.:
BEVMapSegmentation. Dep.: DepthEstimation. Det.: 3DObjectDetection. LatentPrediction: our
proposedself-supervisedtask.
Method Supervision DS↑ RC↑ IS↑
CILRS[7] Expert 7.8±0.3 10.3±0.0 0.75±0.05
LBC[5] Expert 12.3±2.0 31.9±2.2 0.66±0.02
Transfuser[30] Expert,Dep.,Seg.,Map.,Det. 31.0±3.6 47.5±5.3 0.77±0.04
Roach[43] Expert 41.6±1.8 96.4±2.1 0.43±0.03
LAV[3] Expert,Seg.,Map.,Det. 46.5±2.3 69.8±2.3 0.73±0.02
TCP[40] Expert 57.2±1.5 80.4±1.5 0.73±0.02
MILE[11] Expert,Map.,Det. 61.1±3.2 97.4±0.8 0.63±0.03
ThinkTwice[21] Expert,Dep.,Seg.,Det. 65.0±1.7 95.5±2.0 0.69±0.05
DriveAdapter[20] Expert,Map.,Det. 65.9±- 94.4±- 0.72±-
Interfuser[33] Expert,Map.,Det. 68.3±1.9 95.0±2.9 -
LAW Expert,LatentPrediction 70.1±2.6 97.8±0.9 0.72±0.03
ImplementationDetailsThedefaultconfigurationofLAWdoesnotincludeviewselectionunless
specified. For the open-loop benchmark, we use Swin-Transformer-Tiny [26] (Swin-T) as the
backbone. Theinputimageisresizedto800×320. WeemployaCosineAnnealing[27]learning
rateschedulewithaninitiallearningrateof5e-5. AdamW[28]optimizerisutilizedwithaweight
decayof0.01andwetrainthemodelwithbatchsize8for12epochson8RTX3090GPUs. The
weightofthewaypointlossandlatentpredictionlossaresetto1.0. Asfortheplannerwithselected
views,wefinetuneitwiththerewardlossbasedontheLAW.Wesettheinitiallearningrateto5e-6
andtrainforanadditional6epochs. Theweightoftherewardlossissetto1.0. Fortheclosed-loop
benchmark. AndweuseResNet-34asthebackbonefollowing [40]forafaircomparison. Weuse
7Table 3: Ablation study on latent prediction. The latent world model receives two types of
inputs: viewlatentsandpredictedtrajectory. Noinputreferstonotutilizingtheworldmodel. Agg.:
Aggregation. Pred.: Predicted. Traj.: Trajectory.
InputofWorldModel L2(m)↓ Collision(%)↓
TemporalAgg.
ViewLatent Pred.Traj. 1s 2s 3s Avg. 1s 2s 3s Avg.
- - - 0.44 0.95 1.65 1.01 0.27 0.57 1.32 0.72
✓ - - 0.32 0.67 1.14 0.71 0.20 0.30 0.73 0.41
✓ ✓ - 0.30 0.64 1.12 0.68 0.18 0.27 0.66 0.37
✓ ✓ ✓ 0.26 0.57 1.01 0.61 0.14 0.21 0.54 0.30
theTCPhead[40]following[20]. Thesizeoftheinputimageis900×256. TheoptimizerisAdam.
Thelearningrateissetto1e-4andtheweightdecayis1e-7. Wetrainthemodelwithbatchsize128
for60epochs. Thelearningrateisreducedbyafactorof2after30epochs.
5.2 ComparisonwithState-of-the-artMethods
Fortheopen-loopbenchmark,wecompareLAWwithseveralstate-of-the-artmethods,including
UniAD [15], VAD [22] on the nuScenes dataset. The results are summarized in Table 1. LAW
outperforms UniAD and VAD in terms of the average L2 displacement error over 1s, 2s, and 3s
predictionhorizons. Moreover,ourmethodachievesremarkablereal-timeperformancewithalatency
of30.9ms,highlightingtheefficiencyofourapproach. Fortheclosed-loopbenchmark,asshownin
Table2,ourproposedmethodoutperformsallexistingmethods. Notably,ourapproachsurpasses
previous leading methods such as ThinkTwice [21] and DriveAdapter [20], which incorporate
extensivesupervisionfromdepthestimation,semanticsegmentation,andmapsegmentation.
5.3 AblationStudy
Latent Prediction In this ablation study, we investigate the effectiveness of the latent
prediction. The results are presented in Table 3. Initially, we only use the view la-
tent as input of the world model, which means omitting the predicted trajectory component.
As shown in the third row of Table 3, this
approachresultsinaslightperformanceim-
provement compared with the model with- Table 4: Ablation study on latent prediction on
outthelatentprediction. Whenweinclude Town05Longbenchmark.
thepredictedtrajectoryaspartoftheinput
Latent
(fourthrowofTable3),performanceissig- DS↑ RC↑ IS↑
Prediction
nificantlyenhanced.Itshowsthatanaccurate
predictionoffuturelatentsrequirestheincor- × 67.9±2.1 98.6±0.8 0.68±0.02
porationofdrivingactions,highlightingthe ✓ 70.1±2.6 97.8±0.9 0.72±0.03
rationality of using the latent world model.
Additionally, we provide an ablation study
on the latent prediction in the closed-loop setting, as depicted in Table 4. Notably, we observed
substantialimprovementsintheInfractionScore. Thisindicatesthatthecapabilitytopredictfuture
scenarioseffectivelyaidsinmitigatingpotentialcollisions.
NetworkArchitectureofLatentWorldModelTovalidatetheimpactofthenetworkarchitecture
ofthelatentworldmodel,weconductexperimentsasshowninTable5. Firstly,itisevidentthat
a single-layer neural network, represented as Linear Projection, is not adequate for fulfilling the
functionsoftheworldmodel,resultinginsignificantlydegradedperformance. Thetwo-layerMLP
shows considerable improvement in performance. However, it lacks the capability to facilitate
interactionsamonglatentsfromdifferentviews. Therefore,weusethetransformerdecoderasour
defaultnetworkarchitecture,whichachievesthebestresultsamongthetestedarchitectures. This
suggeststhatforanyparticularview,incorporatinginformationfrommultipleadjacentviewscan
enhancethepredictionofitsfuturelatent.
TheTimeHorizonofLatentWorldModelInthisexperiment, theworldmodelpredictslatent
features at three distinct future time horizons: 0.5 seconds, 1.5 seconds, and 3.0 seconds. This
correspondstothefirst,third,andsixthfutureframesfromthecurrentframe,giventhatkeyframes
8Table5: Ablationstudyondifferentnetworkarchitectureofthelatentworldmodel. Linear
Projectionmeansasingle-layernetwork. Arch.: Architecture.
L2(m)↓ Collision(%)↓
WorldModelArch.
1s 2s 3s Avg. 1s 2s 3s Avg.
LinearProjection 0.31 0.65 1.14 0.70 0.26 0.34 0.66 0.42
Two-layerMLP 0.27 0.58 1.07 0.64 0.17 0.23 0.59 0.33
TransformerDecoder 0.26 0.57 1.01 0.61 0.14 0.21 0.54 0.30
occurevery0.5secondsinthenuScenesdataset. Theresults,displayedinTable6,showthatthe
modelachievesthebestperformanceatthe1.5-secondhorizon. Thereasonsareasfollows. The
0.5-secondintervaltypicallypresentssceneswithminimalchanges,providinginsufficientdynamic
contenttoimprovefeaturelearning. Incontrast,the3.0-secondintervalincreasesthecomplexityof
thepredictiontask,whichhindersbetterfeaturelearning. Thisconclusionalignswithobservations
fromMAE[10],wherebothexcessivelylowandhighmaskratiosnegativelyimpacttheabilityofthe
network.
Table6: Ablationstudyonthedifferenttimehorizonsforlatentprediction. Theworldmodelis
capableofpredictinglatentatvariousfuturetimehorizonsfromthepresentmoment.
L2(m)↓ Collision(%)↓
TimeHorizon
1s 2s 3s Avg. 1s 2s 3s Avg.
0.5s 0.26 0.57 1.01 0.61 0.14 0.21 0.54 0.30
1.5s 0.26 0.54 0.93 0.58 0.14 0.17 0.45 0.25
3.0s 0.28 0.59 1.01 0.63 0.13 0.20 0.48 0.27
ViewSelectionToablatetheeffectivenessofourviewselectionapproach,weconducttheexperiments
showninTable7. Wetrainourmodelwiththeviewselectionmoduleandthentestitwithseveral
strategies: 1)frontviewandarandomview,2)frontviewandaviewselectedbyourviewselection
module,3)frontviewandaviewselectedbasedontherewardslabelinSec.4.3. Thisrewardlabel
isgeneratedwiththehelpofGTtrajectoryandthisexperimentservesastheupperbound. 4)all
sixviews. ThereasonforfixingthefrontviewwillbediscussedintheappendixA.1. Theresults
demonstratethattheselectionmadebyourviewselectionmodulesignificantlyoutperformsrandom
selectionsandcloselyapproachestheupperboundsetbytheGT.
Table 7: Ablation study on the view selection approach. We use the model trained with the
selectionrewardpredictionmoduleforinference. GTview: Weadopttheviewselectionstrategy
withtherewardlabelinsteadofwiththepredictedreward.
L2(m)↓ Collision(%)↓
Selectedviews Latency(ms)↓
1s 2s 3s Avg. 1s 2s 3s Avg.
Front+arandomview 0.36 0.73 1.23 0.77 0.16 0.27 0.78 0.40 30.9
Front+predictedview 0.30 0.64 1.10 0.68 0.16 0.25 0.72 0.38 30.9
Front+GTview 0.28 0.56 0.97 0.60 0.15 0.22 0.61 0.33 30.9
Sixviews 0.26 0.57 1.01 0.61 0.14 0.21 0.54 0.30 51.2
6 ConclusionandLimitation
Inconclusion,thispaperintroducesanovelself-supervisedapproachusingthelatentworldmodel.
This approach enhances the learning of scene representations in end-to-end autonomous driving
systemswithoutcostlyannotations. Althoughourmethodhasdemonstratedpromisingoutcomes
oncurrentbenchmarks,itisconstrainedbythelimitedvolumeofdatautilized. Infuturework,we
aimtoenhancethescalabilityofourapproachbyapplyingittolargerandmorediversedatasets.
Leveraginglarge-scaledata,weintendtoemploythelatentworldmodelforpretraining.
9References
[1] HolgerCaesar,VarunBankiti,AlexHLang,SourabhVora,VeniceErinLiong,QiangXu,AnushKrishnan,
YuPan,GiancarloBaldan,andOscarBeijbom. nuscenes:Amultimodaldatasetforautonomousdriving.
InCVPR,2020.
[2] SergioCasas,AbbasSadat,andRaquelUrtasun. Mp3: Aunifiedmodeltomap,perceive,predictand
plan. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages
14403–14412,2021.
[3] Dian Chen and Philipp Krähenbühl. Learning from all vehicles. In Proceedings of the IEEE/CVF
ConferenceonComputerVisionandPatternRecognition,pages17222–17231,2022.
[4] DianChen,BradyZhou,VladlenKoltun,andPhilippKrähenbühl. Learningbycheating. InConference
onRobotLearning,pages66–75.PMLR,2020.
[5] DianChen,BradyZhou,VladlenKoltun,andPhilippKrähenbühl. Learningbycheating. InCoRL,pages
66–75.PMLR,2020.
[6] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastivelearningofvisualrepresentations. InInternationalconferenceonmachinelearning,pages
1597–1607.PMLR,2020.
[7] FelipeCodevilla, EderSantana, AntonioMLópez, andAdrienGaidon. Exploringthelimitationsof
behaviorcloningforautonomousdriving. InICCV,2019.
[8] MMDetection3DContributors. MMDetection3D:OpenMMLabNext-generationPlatformforGeneral3D
ObjectDetection. https://github.com/open-mmlab/mmdetection3d,2020.
[9] AlexeyDosovitskiy,GermanRos,FelipeCodevilla,AntonioLopez,andVladlenKoltun. Carla:Anopen
urbandrivingsimulator. InCoRL,2017.
[10] KaimingHe,XinleiChen,SainingXie,YanghaoLi,PiotrDollár,andRossGirshick.Maskedautoencoders
arescalablevisionlearners. InCVPR,pages16000–16009,2022.
[11] AnthonyHu,GianlucaCorrado,NicolasGriffiths,ZakMurez,CorinaGurau,HudsonYeo,AlexKendall,
RobertoCipolla,andJamieShotton. Model-basedimitationlearningforurbandriving. NeurIPS,2022.
[12] AnthonyHu,LloydRussell,HudsonYeo,ZakMurez,GeorgeFedoseev,AlexKendall,JamieShotton,
and Gianluca Corrado. Gaia-1: A generative world model for autonomous driving. arXiv preprint
arXiv:2309.17080,2023.
[13] PeiyunHu,AaronHuang,JohnDolan,DavidHeld,andDevaRamanan. Safelocalmotionplanningwith
self-supervisedfreespaceforecasting. InCVPR,2021.
[14] ShengchaoHu,LiChen,PenghaoWu,HongyangLi,JunchiYan,andDachengTao. St-p3:End-to-end
vision-basedautonomousdrivingviaspatial-temporalfeaturelearning. InECCV,2022.
[15] YihanHu,JiazhiYang,LiChen,KeyuLi,ChonghaoSima,XizhouZhu,SiqiChai,SenyaoDu,Tianwei
Lin,WenhaiWang,etal. Goal-orientedautonomousdriving. arXivpreprintarXiv:2212.10156,2022.
[16] YihanHu,JiazhiYang,LiChen,KeyuLi,ChonghaoSima,XizhouZhu,SiqiChai,SenyaoDu,Tianwei
Lin,WenhaiWang,LeweiLu,XiaosongJia,QiangLiu,JifengDai,YuQiao,andHongyangLi. Planning-
orientedautonomousdriving. InCVPR,2023.
[17] JunjieHuang,GuanHuang,ZhengZhu,andDalongDu. Bevdet: High-performancemulti-camera3d
objectdetectioninbird-eye-view. arXivpreprintarXiv:2112.11790,2021.
[18] YuanhuiHuang,WenzhaoZheng,YunpengZhang,JieZhou,andJiwenLu. Tri-perspectiveviewfor
vision-based3dsemanticoccupancyprediction. InProceedingsoftheIEEE/CVFconferenceoncomputer
visionandpatternrecognition,pages9223–9232,2023.
[19] BernhardJaeger,KashyapChitta,andAndreasGeiger. Hiddenbiasesofend-to-enddrivingmodels. In
ICCV,2023.
[20] XiaosongJia,YuluGao,LiChen,JunchiYan,PatrickLangechuanLiu,andHongyangLi. Driveadapter:
Breakingthecouplingbarrierofperceptionandplanninginend-to-endautonomousdriving. InICCV,
2023.
10[21] XiaosongJia,PenghaoWu,LiChen,JiangweiXie,ConghuiHe,JunchiYan,andHongyangLi. Think
twicebeforedriving:towardsscalabledecodersforend-to-endautonomousdriving. InCVPR,2023.
[22] BoJiang,ShaoyuChen,QingXu,BenchengLiao,JiajieChen,HelongZhou,QianZhang,WenyuLiu,
ChangHuang,andXinggangWang. Vad:Vectorizedscenerepresentationforefficientautonomousdriving.
InICCV,2023.
[23] TarashaKhurana,PeiyunHu,AchalDave,JasonZiglar,DavidHeld,andDevaRamanan. Differentiable
raycastingforself-supervisedoccupancyforecasting. InECCV,2022.
[24] ZhiqiLi,WenhaiWang,HongyangLi,EnzeXie,ChonghaoSima,TongLu,QiaoYu,andJifengDai.
Bevformer:Learningbird’s-eye-viewrepresentationfrommulti-cameraimagesviaspatiotemporaltrans-
formers. arXivpreprintarXiv:2203.17270,2022.
[25] YingfeiLiu,TiancaiWang,XiangyuZhang,andJianSun. Petr:Positionembeddingtransformationfor
multi-view3dobjectdetection. InEuropeanConferenceonComputerVision,pages531–548.Springer,
2022.
[26] ZeLiu,YutongLin,YueCao,HanHu,YixuanWei,ZhengZhang,StephenLin,andBainingGuo. Swin
transformer: Hierarchicalvisiontransformerusingshiftedwindows. InProceedingsoftheIEEE/CVF
InternationalConferenceonComputerVision(ICCV),2021.
[27] IlyaLoshchilovandFrankHutter. Sgdr:Stochasticgradientdescentwithwarmrestarts. arXivpreprint
arXiv:1608.03983,2016.
[28] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint
arXiv:1711.05101,2017.
[29] ChenMin,DaweiZhao,LiangXiao,JianZhao,XinliXu,ZhengZhu,LeiJin,JianshuLi,YulanGuo,
JunliangXing,etal. Driveworld:4dpre-trainedsceneunderstandingviaworldmodelsforautonomous
driving. arXivpreprintarXiv:2405.04390,2024.
[30] AdityaPrakash,KashyapChitta,andAndreasGeiger. Multi-modalfusiontransformerforend-to-end
autonomousdriving. InCVPR,2021.
[31] KatrinRenz,KashyapChitta,Otniel-BogdanMercea,AKoepke,ZeynepAkata,andAndreasGeiger.Plant:
Explainableplanningtransformersviaobject-levelrepresentations. arXivpreprintarXiv:2210.14222,
2022.
[32] AbbasSadat,SergioCasas,MengyeRen,XinyuWu,PranaabDhawan,andRaquelUrtasun. Perceive,
predict,andplan:Safemotionplanningthroughinterpretablesemanticrepresentations. InECCV,2020.
[33] HaoShao,LetianWang,RuoBingChen,HongshengLi,andYuLiu. Safety-enhancedautonomousdriving
usinginterpretablesensorfusiontransformer. CoRL,2022.
[34] HaoShao,LetianWang,RuobingChen,StevenLWaslander,HongshengLi,andYuLiu. ReasonNet:
End-to-EndDrivingwithTemporalandGlobalReasoning. InCVPR,pages13723–13733,2023.
[35] MarinToromanoff,EmilieWirbel,andFabienMoutarde. End-to-endmodel-freereinforcementlearning
forurbandrivingusingimplicitaffordances. InCVPR,2020.
[36] AaronVanDenOord,OriolVinyals,etal. Neuraldiscreterepresentationlearning. Advancesinneural
informationprocessingsystems,30,2017.
[37] QitaiWang,YuntaoChen,ZiqiPang,NaiyanWang,andZhaoxiangZhang. Immortaltracker:Tracklet
neverdies. arXivpreprintarXiv:2111.13672,2021.
[38] YuqiWang,YuntaoChen,XingyuLiao,LueFan,andZhaoxiangZhang. Panoocc: Unifiedoccupancy
representationforcamera-based3dpanopticsegmentation. arXivpreprintarXiv:2306.10013,2023.
[39] YuqiWang,JiaweiHe,LueFan,HongxinLi,YuntaoChen,andZhaoxiangZhang. Drivingintothefuture:
Multiviewvisualforecastingandplanningwithworldmodelforautonomousdriving. arXivpreprint
arXiv:2311.17918,2023.
[40] PenghaoWu,XiaosongJia,LiChen,JunchiYan,HongyangLi,andYuQiao. Trajectory-guidedcontrol
predictionforend-to-endautonomousdriving:asimpleyetstrongbaseline. NeurIPS,2022.
[41] WenyuanZeng, WenjieLuo, SimonSuo, AbbasSadat, BinYang, SergioCasas, andRaquelUrtasun.
End-to-endinterpretableneuralmotionplanner. InCVPR,2019.
11[42] LunjunZhang,YuwenXiong,ZeYang,SergioCasas,RuiHu,andRaquelUrtasun. Learningunsupervised
worldmodelsforautonomousdrivingviadiscretediffusion. arXivpreprintarXiv:2311.01017,2023.
[43] ZhejunZhang,AlexanderLiniger,DengxinDai,FisherYu,andLucVanGool. End-to-endurbandriving
byimitatingareinforcementlearningcoach. InICCV,2021.
[44] WenzhaoZheng,WeiliangChen,YuanhuiHuang,BoruiZhang,YueqiDuan,andJiwenLu. Occworld:
Learninga3doccupancyworldmodelforautonomousdriving. arXivpreprintarXiv:2311.16038,2023.
[45] XingyiZhou,VladlenKoltun,andPhilippKrähenbühl.Trackingobjectsaspoints.InEuropeanconference
oncomputervision,pages474–490.Springer,2020.
125.6% 3.9% 4.0%
Fixed Views 5.4% 8.0% Backbone
0.80 Dynamic Views Waypoint
Decoder
0.75 View
Selection
0.70 Latent
Prediction
0.65 Temporal
Module
View
0.60
Attention
1 2 3 4 5 6 73.1%
Number of Views
(a) Different Selection Strategies (b) Detailed Latency Analysis
Figure4: (a)Comparisonbetweendifferentselectionstrategies. (b)Detailedlatencyanalysis.
A Appendix
A.1 MoreAnalysisofViewSelection
Inourviewselectionapproach,wealwaysusethefrontviewanddynamicallychooseoneadditional
viewfromtheotherfiveviews. Thereasonfordoingthisisdrivenbythefollowingexperiments.
FixingFrontViewHelpsThissectionpresentstheexperimentaljustificationforalwayschoosing
thefrontcameraviewasoneoftheinputviews. Specifically,giventhetrainedend-to-endplanner
withoutrewardloss,weconducttwoview-selectionstrategiesonit. Thefirststrategyalwaysuses
thefrontcameraandselectsonerandomviewfromtheremainingfivecameras. Thesecondstrategy
randomlychoosestwocameras. TheresultsareshowninTable8. Thisexperimentshowsthatthe
strategyoffixingthefrontviewandrandomlyselectinganadditionalviewoutperformstherandom
selectionoftwoviews. Thissuperioritycanbeattributedtothefactthatthefrontviewcanprovide
morecrucialinformationfortheplanningtask,especiallyinforward-drivingscenarios.
Table8: Theimportanceoffrontview.
L2(m)↓ Collision(%)↓
SelectedViews
1s 2s 3s Avg. 1s 2s 3s Avg.
Tworandomviews 0.39 0.79 1.32 0.83 0.15 0.28 0.91 0.45
Front+arandomview 0.36 0.74 1.24 0.78 0.16 0.28 0.82 0.42
Different Selection Strategy Reducing the number of fixed cameras can also alleviate the com-
putationalburden. Therefore,itisnaturaltoquestionhowtheviewselectionapproachcompares
toaconfigurationutilizingareducednumberoffixedcameras. Toinvestigatethis, wecarryout
experimentsasillustratedinFig.4(a). Thespecificsettingsoftheexperimentsareasfollows. Weset
upfourgroupsofexperimentswiththenumberofviewsusedbeing1,2,4,and6. Whenusingonly
oneview,wealwaysusethefrontcamera. Fortwoviews,thefixed-viewmodelistrainedandtested
usingthefrontandbackcameras,whilethedynamic-viewmodel(i.e.,ourmethod)fixesthefront
cameraandthenselectsthemostinformativeviewfromtheremainingfivecameras. Forfourviews,
thefixed-viewmodelusesthefront,front-left,front-right,andbackcameras,whilethedynamic-view
modelfixesthefrontcameraandchoosesthreeadditionalcamerasfromtheremainingfive. Finally,
for six views, we use all available cameras. The results demonstrate that our dynamic selection
methodconsistentlyoutperformsfixed-viewsettingswiththesamenumberofviews. Thisindicates
thatourmethodiscapableofselectingtheinformativeviewsfrommultipleoptions.
Latency Breakdown Our Planner with Selected Views attains an impressive speed of 32.4 FPS.
Herewepresentthedetailedlatencyofeachmodule. WetestitontheNVIDIAGeforceRTX3090
GPUwithbatchsize1. Thecodeisbasedonthemmdetection3d[8]. Thespecificlatencyassociated
witheachmoduleinourmodelisdetailedinFig.4(b). Asillustratedinthefigure,thebackbone
constitutesthemajorityofthemodel’slatency. Reducingthenumberofinputviewsleadstoalinear
13
)m(orrE
2L
.gvA（a) Dense crowd （b) Cut-in
（c) Following closely （d) Crosswalk
Figure5: Thevisualizationofviewselection. Weoutlinetheselectedviewsinorangeboxes.
decreaseinthecostofthebackbone. Sinceweonlyhavetwoviewspassingthroughthebackbone,
ourview-selectionmethodsubstantiallybooststheefficiency.
ViewSelectionVisualizationAsdepictedinFig.5,wepresentavisualizationanalysisoffourtypical
cases. Fromthesevisualizations,wederivetwokeyinsights: 1)Ourmethodhasapreferencefor
viewswithvisuallysalientobjects. Asdemonstratedincases(a),(b),and(c),themodeltendsto
selectviewsthatfeaturegroupsofpeople,vehicleswiththepotentialtocutin,orvehiclesatrisk
of a rear-end collision. This preference arises because such objects have a significant impact on
drivingbehavior. Throughrewardlearning,ourmodelcanidentifytheviewthatismostimportantat
acertainmoment,bythecluesofferedbythesesalientobjects. 2)Ourmodellearnspriorknowledge
ofspecificscenarios. Humandrivershavepriorknowledgeabouttheworld,suchastheexpectation
ofpedestrianssuddenlyappearingatcrosswalks,whichnecessitatesslowerdriving. Ourviewlatent
reconstruction module can also learn similar priors. For instance, in case (d), the view selection
model,aidedbythereconstructionmodule,reasonablyfocusesonthecrosswalk.
14