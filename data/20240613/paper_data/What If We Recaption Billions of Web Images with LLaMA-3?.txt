What If We Recaption Billions of Web Images
with LLaMA-3?
XianhangLi⋆1 HaoqinTu⋆1 MudeHui⋆1 ZeyuWang⋆1 BingchenZhao⋆2 JunfeiXiao3
SuchengRen3 JieruMei3 QingLiu4 HuangjieZheng5 YuyinZhou1 CihangXie1
⋆equaltechnicalcontribution
1UCSantaCruz 2UniversityofEdinburgh 3JHU 4Adobe 5UTAustin
Original:Cafe Van goch" stock image Original:MoyieBCrailway Original:Deluxe Twin Room
Ours:The first image shows the exterior of a Ours:Ayellowtrainwithredandblueaccent Ours:A modern hotel room with a large bed
cafe with a yellow and green facade. The cafe sistravelingalongarailwaytrackthatrunsp covered in white and red bedding, a nightstand
has a large glass window with red frames, and aralleltoabodyofwater.Thetrainissurroun with a lamp, and a chair with a small table in
the sign above it reads 'Cafe Van Gogh'. There dedbyalandscapewithautumnalcolors,fea front of it. The room has a large window with
are several hanging lights with a warm glow, turingtreeswithyellowandorangeleaves.Th purple curtains, a white ceiling with a light
and the tables outside are set with white erailwaytrackisborderedbyagrassyareawi fixture, and a dark wall with a patterned
tablecloths, plates, and glasses. The chairs thsomebushesandadirtmoundontheleft wallpaper. There are two colorful rugs on the
are black with a woven seat. The floor is side.Thewateriscalm,reflectingthetrainan floor, and the room is well-lit with natural light
wooden, and there is a small potted plant… dthetrees. coming through the window.
Original: Western Kingbird Original:Deluxe Twin Room
Ours: A small, gray and yellow bird Ours:A slice of cake with a caramel
with a black beak and black eyes is glaze and white frosting is placed on a
perched on a brown branch. The bird white plate. The cake is garnished with
has a fluffy appearance with a mix of a dollop of whipped cream on top
gray and yellow feathers on its body. Word distributions of our recaptions and a sprinkle of powdered sugar. The
T foh ce u b sa gc rk eg er no ,u sn ud g i gs e a sts ino gft a, o nu at t- uo rf a- l and theoriginal captions. p a l wat ae t i es r mon a a rk w ih ni tt he es u br of ta tc oe m, a rin gd h tt h ce or re n eis r
environment. that reads 'Recipe Spain.com'."
Original:siesta key florida picture Original: EldoraSpeedway(OH)6/8 Original:Buckle / gesp“The trikeBrotherhood”
Ours:A bustling beach scene with numerous Ours:Agroupofracecarsisspeedingonadi Ours:A silver motorcycle charm with intricate
people enjoying the sun and sand. The beach rttrack.Theleadcarisredwiththename'Lu details is attached to a red background with the
is lined with colorful umbrellas and sun cas'ontheside,followedbyabluecarwitht words 'The Tike Brotherhood' and 'Blood &
loungers, and the water is a vibrant henumber'21'andablackcarwiththenumb Honor' engraved in a cursive script. The charm
turquoise. The sky is partly cloudy, and the er'2'.Thecarsarekickingupdustastheyrac features a motorcycle with a sidecar, and the
overall atmosphere is lively and crowded. earoundthetrack. background has a wood grain texture."
Figure1:ExamplesoftheoriginalcaptionandourrecaptioninDataComp-1B,andworddistributions.
Preprint.Underreview.
4202
nuJ
21
]VC.sc[
1v87480.6042:viXraAbstract
Web-crawledimage-textpairsareinherentlynoisy. Priorstudiesdemonstratethat
semanticallyaligningandenrichingtextualdescriptionsofthesepairscansignifi-
cantlyenhancemodeltrainingacrossvariousvision-languagetasks,particularly
text-to-imagegeneration. However,large-scaleinvestigationsinthisarearemain
predominantly closed-source. Our paper aims to bridge this community effort,
leveragingthepowerfulandopen-sourced LLaMA-3,aGPT-4levelLLM.Our
recaptioningpipelineissimple:first,wefine-tuneaLLaMA-3-8BpoweredLLaVA-
1.5andthenemployittorecaption∼1.3billionimagesfromtheDataComp-1B
dataset.Ourempiricalresultsconfirmthatthisenhanceddataset,Recap-DataComp-
1B,offerssubstantialbenefitsintrainingadvancedvision-languagemodels. For
discriminativemodelslikeCLIP,weobserveenhancedzero-shotperformancein
cross-modalretrievaltasks. Forgenerativemodelsliketext-to-imageDiffusion
Transformers,thegeneratedimagesexhibitasignificantimprovementinalignment
withusers’textinstructions,especiallyinfollowingcomplexqueries. Ourproject
pageishttps://www.haqtu.me/Recap-Datacomp-1B/.
1 Introduction
The exponential growth in data availability is one of the most paramount factors in driving the
monumentalsuccessesofdeeplearningoverthepastdecade[13,32,6,57,19,17]. Typically,this
data is sourced through web crawling with simple filtering mechanisms in place. While such an
approachhasfacilitatedlarge-scaledatacollection,exemplifiedbycollectionslikeLAION-400M[57]
andLAION-5B[57]withbillionsofimage-textrecords,ithasinadvertentlycompromiseddataquality.
AsillustratedinFigure1,theseinternet-crawledimage-textpairsfrequentlyexhibitmisalignments
betweenimagesandtheircorrespondingtextualcontent,andoften,thetextualdescriptionsarebrief
andlackdetailedinformation.
To mitigate the noise present in web-crawled data, enhancements through post-processing—
implemented via human-in-the-loop systems [61, 70] or automated pipelines [57, 28, 27]—are
crucial, which help to train the advanced vision-language foundation models. Notably, both the
close-sourcedDALL-E3[41]andSORA[42]incorporateadvancedcaptioningtechniquestore-label
theirtrainingdatasets,acrucialstephighlightedintheirtechnicalreports. Despitevariouseffortsto
open-sourceandreplicatethesemethodologies[9,28,27,35,69,16,51],thecommunitycontinues
tofacesignificantchallengesinaccessinghigh-quality,well-alignedimage-textdataatscale(e.g.,at
thebillionlevel)fortrainingadvancedvision-languagefoundationmodels.
Thispaperendeavorstocontributetothiscommunityinitiative,inspiredspecificallybytherelease
of LLaMA-3 [62], a model demonstrating GPT-4-level capabilities across a variety of linguistic
tasks. Additionally,recentstudieshaveshownthatleveragingLLaMA-3cansignificantlyenhance
modelperformanceonvision-languagetasks[34,65],comparabletothoseachievedbyGPT-4V[1].
In response, we employ LLaMA-3 to develop our advanced captioner model. Our approach is
straightforward: we first train a LLaMA-3-powered Llava model to act as an image captioner,
whichisthenutilizedtorecaptiontheentireDataComp-1Bdataset. AsdepictedinFigure1, the
resultingdataset,dubbedRecap-DataComp-1B,featuresenhancedtextualdescriptionsandimproved
alignmentwithcorrespondingimages,clearlysurpassingitsweb-crawledcounterparts. Thesequality
enhancementsarefurtherquantitativelyverifiedinSection4.
ComprehensiveevaluationshighlightthesignificantimprovementsthatRecap-DataComp-1Bcon-
tributestothetrainingofadvancedvision-languagefoundationmodels. Notably,thisdatasetenables
CLIPmodelstoachievesignificantenhancementsintheirzero-shotcross-modalretrievalcapabilities.
It also enhances the alignment between generated images and text instructions in text-to-image
generative models pre-trained on our dataset. We hope that the release of Recap-DataComp-1B
will catalyze further developments in advanced vision-language foundation models, particularly
encouragingthedevelopmentwithintheopen-sourcecommunity.
2Better
Tasks
CLIP Models
Retrieval
Image-Caption
{ Classification
Recaption
… …
LLaVA-
LLaMA3
Image
DataComp-1B Recaptioned { Generation
DataComp-1B … …
Better
Diffusion Models
Figure2: TheillustrationofourrecaptioningpipelineonDataComp-1B.WeuseLLaMA-3-powered
LLaVA to reception images, which enables us to train stronger CLIP models and Text-to-Image
Diffusionmodels.
2 Relatedworks
Vision-LanguageFoundationModels. CLIP[47]isoneofthepioneeringfoundationmodelsto
connectimageandtext. Bytrainingonmillions,andevenbillions,ofimage-textpairs[6,14,17,19,
56–59],CLIPmarkedlyshowcasesexcessivelystrongzero-shotcapacities,andfurthermore,laysthe
cornerstoneforbuildingmoreadvancedvision-languagefoundationmodels[3,28,27,63,35,34,10,
4,65]. Apartfromdiscriminativevision-languageunderstanding,text-to-imagegenerationmodels
[15,40,41,45,48–50,53,68]havetransformedthefieldofAI-generatedcontent,facilitatingthe
creationofhigh-qualityimagesfromnaturallanguagedescriptions.
EnhancingImage-TextData. Web-crawledimage-textdata[57,19,17]commonlyfacetheprob-
lemsofimage-textmisalignmentandthelow-qualityoftextualdescriptions. Typically,thereare
two popular ways for improving the quality of these image-text pairs: 1) data filtering removes
misalignedimage-textpairsusingvariousmethodssuchascleaningstrategies[56,19,64],pretrained
models[28,57,19],andhuman-assistedsystems[61,70,77]; 2)datarecaptioningimprovesthe
textualqualityofimage-textpairviageneratingnewcaptions,whichisthefocusofthispaper. To
recaptiondata,LaCLIP[16]utilizeslargelanguagemodels(LLMs)likeChatGPTtorewritetheorig-
inalcaptions;Nguyenetal.[39]employBLIP2[27]torecaptionimages. Morerecently,advanced
largemultimodalmodelshavebeenappliedtofurtherenhancethequalityofimagecaptioning. For
example, ShareGPT4V[9]employsGPT-4Vtocreatehighlydescriptivecaptionsfromcarefully
craftedpromptsandcorrespondingimageinputs;theresultingdatasethassignificantlybenefitedthe
trainingofvariousmodels[7,76,12,31,18]. However,scalingsuchpromptingwithGPT-4Vto
billionsofrecordsislesspractical,asitwilldrasticallyincreasethemonetarycost(ofintensively
callingOpenAIAPIs)bymorethan10,000×.
Ourpapermostlyfollowstheapproachpresentedin[8,38,76,12],whereadvancedopen-source
multimodalmodelslikeLLaVA[35]areemployedforrecaptioningpurposes. However,ourapproach
is distinguished by two major aspects: 1) we strongly enhance the LLM module in LLaVA, i.e.,
buildingwithLLaMA-3;and2)ourrecaptioningeffortsareexecutedonabillion-scaledataset.
3 RecaptioningPipeline
OurrecaptioningpipelineiscenteredaroundtheadvancedLLMLLaMA-3[62],whichachieves
exceptionally strong performance in language understanding, reasoning, code generation, math
problems, etc. [11, 60]. Specifically, we utilize the LLaVA framework [35] to fully harness its
capabilitiesforvisualunderstanding. Wedescribethedetailedtrainingproceduresbelow.
3.1 Modeldetails
ModelConfiguration. WefollowthesetupofLLaVA-1.5[33]tobuildourcaptionermodel,except
thatweuseLLaMA-3-8Basthelanguagedecoderbecauseofitssuperiorperformance. Thevisual
branchofCLIPViT-L/14[46]isusedasthevisionencoder. TwotrainableMLPlayersareemployed
ontopofthevisionencodertoprojectvisualfeaturesintothelanguageembeddingspace.
2-Stage Training. We also follow LLaVA-1.5 [33] for model training. Essentially we conduct
instruction-tuningonthepre-trainedLLMwithitsoriginalauto-regressivetrainingobjective. Inthe
3Table1: PerformancecomparisonofLLaVA.
Model LLaVA-1.5-7B LLaVA-1.5-13B LLaVA-1.5-LLaMA3-8B(ours) GPT-4V
MMMU 33.6 36.4 45.2 56.8
MM-Vet 33.9 36.3 37.8 44.6
firststage,onlytheprojectionMLPistrained;inthesecondstage,wefine-tuneboththeprojection
MLPandthelanguagedecoder. Notethatthevisionencoderremainsfrozenallthetime. Following
the protocols in LLaVA [33], 558k image-text pairings filtered from LAION [56], CC [6],and
SBU [43] are used as training data in the first stage; then 665k instructions-following data from
LLaVA-1.5 [33], containingimage-grounded conversation, image descriptions, and image-based
complex reasoning tasks, are used for the second stage of training. To help our model generate
higher-qualitycaptions,weusetheimage-textpairsfromHQ-Editdataset[21]forfurthertuning.
Evaluations. ToprobethevisualunderstandingandreasoningabilityofourLLaVA-1.5-LLaMA3-
8Bmodel,weoptfortwocomprehensivemulti-modalevaluationbenchmarks,MMMU[72]and
MM-Vet[71]. Thesebenchmarksassessabroadrangeofcapabilitiessuchasrecognition,spatial
awareness,OCR,knowledge,andlanguagegeneration. AsreportedinTable1,onbothbenchmarks,
our LLaVA-1.5-LLaMA3-8B model surpasses the vanilla LLaVA-1.5-7B model by a significant
margin. TheseresultsalsosubstantiallybeattheconsiderablylargerLLaVA-1.5-13Bmodel,clearly
demonstratingthesuperiorvisualunderstandingandreasoningabilityofourmodel.
3.2 RecaptioningDataComp-1B
With this advanced LLaVA model, we next use it to generate captions in a scalable and detailed
manner,giventhevisualinput,andthefollowingtextprompt:
"Pleasegenerateadetailedcaptionofthisimage. Pleasebeasdescriptiveaspossible."
Asforthedataset,weoptforDataComp-1B [19],awidelyaccessible,large-scalevision-language
datasetcomprising∼1.3billionweb-crawledimage-textpairs. Toensureitsquality,DataComp-1Bis
alreadyacuratedsubsetfromamuchlargercollectionof12.8billionimage-textpairsandhasbeen
subjectedtorigorouspreprocessingwhichincludessafetychecks,deduplication,CLIPscorefiltering,
andimage-basedfiltering. Despitetheseefforts,thequalityoftheoriginalcaptionsinDataComp-1B
stillexhibitsrelativelylowquality.
Weapplyourwell-trainedLLaVA-1.5-LLaMA3-8BmodeltorecaptiontheentireDataComp-1B
dataset.Specifically,captionsaregeneratedauto-regressivelyviagreedydecoding,withthemaximum
outputtokenlengthsetto128. WetermthisnewlyrecaptioneddatasetRecap-DataComp-1B.
4 AnalyzingRecap-DataComp-1B
ThissectioncollectsandpresentsaquantitativeanalysisofourgeneratedcaptionsonDataComp-
1B. We primarily focus on two aspects: 1) the inherent features of the captions, including word
distributionsandaveragelengths;and2)thesemanticqualityofthecaptions,evaluatedintermsof
thematchingsimilaritybetweenimagesandcaptionsandtheinherenttextualqualityofthecaptions.
4.1 Word&LengthDistribution
We begin our analysis by comparing the word frequency distributions between our recaptioned
contentandthatfromtheoriginalDataComp-1B,asillustratedinFigure1,analyzingarandomly
sampledsubsetofapproximately0.35billioninstances. Ourfindingsrevealthattherecaptioned
contentdisplaysaconsiderablyrichervocabulary,capturing82.86%tokensofthewordcollections
frombothoursandtheoriginalcaptiondata. Additionally,thereisanoticeablevarietyintheusageof
nounsandadjectivesinourcaptions(e.g.,“white”and“background”). Wearguethatthisincreased
lexical diversity is a direct consequence of the extended length of our data. We thus present the
distributionofcaptionlengthsinFigure3tohighlightthisdifference. Onaverage,ourrecaptioned
datademonstratesalongersequencelengthof49.43,whereastheoriginalDataCompcaptionshavea
muchshorterlengthof10.22. TheseobservationsvalidatethatourRecap-DataComp-1Bsurpasses
theoriginalDataComp-1Bversionintermsofbothcaptionlengthanddiversity.
4Figure 3: Average length distributions of both the original captions and our recaptioned data in
DataComp-1B.
4.2 GPT-4V&CLIPEvaluations
Next,weevaluatethesemanticqualityofrecaptionedcontentusingtwomodels: 1)CLIP[47],which
measuresthesemanticsimilaritybetweencaptionsandimages,and2)GPT-4V[2],whichassesses
thefluencyandalignmentofcaptionswiththegivenimages.
FortheCLIPevaluation,weanalyzeasubsetof180,000image-textpairs. Interestingly,wenote
that,whenusingthestandardCLIP-largemodelwith∼428Mparametersforthismeasurement,our
recaptionedcontentperformsjustcomparablytotheoriginalcaptions(49.57vs.50.43). Weattribute
thisresultprimarilytothelimitationsofthestandardCLIPmodel,whichistrainedon‘short’captions
andmayinadequatelycapturethenuancesinsemanticsimilarityforlongercaptions. Toprobedeeper
intosemanticalignmentbetweenlongcaptionsandimages,weutilizetheLongCLIP-Largemodel
[76],whichisspecificallyfine-tunedtohandlelongercaptions. Withthissetup,theLongCLIPscore
ofournewlygeneratedcaptionimpressivelyattains89.91,nearly9×higherthantheLongCLIPscore
oftheoriginalDataCompcaptions(i.e.,only10.09).
Inaddition,toevaluateboththetextualqualityandthealignmentofthecaptionswiththeircorre-
spondingimages,werandomlyselect10,000instancesforGPT-4V[2]evaluation,employingthe
promptingstrategyoutlinedbelow(CAPTIONisthetextualinput),asper[44,26].
GPT-4VEvaluationInstruction:
[ImageCaption]
CAPTION
Rate whether the caption is of high-quality and fluent and correctly matches the given
image. Theratingshouldbe1-5,where1isincorrectandnotfluentatall,and5iscorrectand
veryfluent. Trytojustgiveanumericalrating.
Yourresponseshouldbeintheformat:
Rating: (int)
We can observe that our recaptioned content achieves markedly superior ratings, registering an
averageratingincreaseof0.43(from3.71to4.14). TogetherwiththefindingsfromSection4.1,
thisconfirmsthesuperiorqualityofournewlygeneratedcaptions,intermsoflength,vocabulary
diversity,semantics,andimage-textalignment.
5 CLIP
CLIP[47]standsasawidelyutilizedvision-languagemodel,whereanimageencoderandatext
encoderarejointlytrainedtopredictcorrectmatchesacrossentirebatchesofimage-textpairs. Inthis
section,wedelveintotheadvantagesoftrainingCLIPmodelswithourRecap-DataComp-1Bdataset.
WeanticipatethatCLIPmodelstrainedonthisdatasetwillexhibitsuperiorzero-shotcross-modal
retrieval capabilities and enhanced text understanding, especially with long and complex textual
inputs,giventheimprovedqualityofourrecaptions.
5Table2: Recap-CLIPmodelconfigurationsusedinourpaper.
Embed VisionTransformer TextTransformer #params(M)
model dim layers width heads layers width heads vision text total
S/16 384 12 384 6 12 384 6 22 33 55
B/16 512 12 768 12 12 512 8 86 53 141
L/16 768 24 1024 16 12 768 12 303 109 414
H/14 1024 32 1280 16 24 1024 16 631 334 967
Table3: Trainwithmixedcaptions. WechooseRecap-CLIP-B/16forthisablation. Largerp
representsahigherratiooftheoriginalcaption. Wereporttop-1zero-shotclassificationaccuracyon
ImageNet-1Kandtop-1recallforretrievaltasks. *concat: Concattwotypesofcaptions.
ImageNet-1K COCOR@1 Flickr30KR@1
mixedratiop
Validation I→T T→I I→T T→I
0.0 36.0 53.0 34.1 74.1 53.5
0.1 58.4 60.9 40.5 83.9 65.5
0.2 62.5 61.7 41.4 85.8 65.7
0.3 65.1 62.7 42.6 86.3 67.0
0.4 66.7 62.6 42.5 87.4 67.7
0.5 67.2 61.9 42.7 85.9 66.7
0.6 68.0 62.2 42.4 86.0 67.4
0.7 68.4 60.7 42.3 86.3 66.9
0.8 69.2 61.5 42.2 85.2 66.9
0.9 69.2 60.6 41.1 86.0 65.7
1.0 69.7 57.3 37.7 84.2 63.0
*concat 43.3 57.8 35.6 80.2 56.4
5.1 Experimentsettings
Training. Forreference,wetermtheCLIPmodeltrainedonourRecap-DataComp-1Bdatasetas
Recap-CLIP. Our training pipeline primarily follows CLIPA [30, 29], which incorporates a two-
statetraining, i.e., apre-trainingprocesswithasmallimagesizefollowedbyafine-tuningstage
incorporatingalargerimageresolution. Wesetthetexttokenlengthto128toaccommodatethe
learningoflongcaptionspresentedinRecap-DataComp-1B.Weconductexperimentsusingthree
modelscales: S/16,B/16,andL/16,withdetailslistedinTable2. TheAdamW[37]optimizerisused
fortraining. Inthepre-trainingphase,themodelistrainedwith2.56billionsampleswithareduced
imagesizeof112,includingawarm-upphaseinvolving51.2millionsamples. Thebatchsizeand
baselearningratearesetto32,768and8e-6,respectively. Forthesubsequentfine-tuningphase,we
increasetheimagesizeto224andtrainthemodelon128millionsampleswitha25.6millionsample
warm-up. Here,weadjustthebatchsizeto16,384andthelearningrateto4e-7.
Evaluation. The efficacy of Recap-CLIP is gauged via several metrics. We evaluate zero-shot
image classification on the ImageNet-1K dataset [52] and assess zero-shot cross-modal retrieval
performance using the validation set of MSCOCO 2014 [32] and the test set of Flickr30K [66]1,
followingtheestablishedpractices[47,30,74,75].
Wepresentourresultsfromthreeaspects.First,weexploretheimpactsofdifferingmixratiosbetween
originalcaptionsandourenhancedrecaptionsonCLIPperformance. Next,weanalyzetheeffectsof
enlargingthesizeoftheCLIPtextencoder. Lastly,weinvestigatethetextunderstandingcapability
ofourRecap-CLIP,viatestingonVG-Attribute[73],whichevaluatesattributesunderstandingability,
andUrban1K[76],whichteststhemodel’sabilitytohandlelongtext.
5.2 TrainingwithMixedCaptions
AspointedoutbyDALL-E3[41],blendingboththebriefgenuinecaptionsandthelonginformative
generatedcaptionscaneffectivelypreventthemodelfromunwantedoverfittingtorecaptiondata.
Therefore,weherebyfirststudytheeffectofvaryingmixratiosbetweentheoriginalcaptionsandour
1WeemploythewidelyusedKarpathysplit[24]ofMSCOCOandFlickr30K.
6Table4: Trainwithlargertextencoder. Wesetp = 0.8forrecaption-basedmodels. Wereport
zero-shottop-1accuracyonImageNet-1Kandtop-1recallonCOCOandFlickr30K.
ImageNet-1K COCOR@1 Flickr30KR@1
visionencoder textencoder re-caption
Validation I→T T→I I→T T→I
small (cid:37) 60.7 49.2 30.1 73.5 53.3
S/16 small (cid:34) 60.2 53.7 34.3 78.6 57.9
base (cid:34) 61.7 56.4 34.8 79.7 59.1
+1.5% +2.7% +0.5% +1.1% +1.2%
base (cid:37) 69.7 57.3 37.7 84.2 63,0
B/16 base (cid:34) 69.2 61.5 42.2 85.2 66.9
large (cid:34) 69.8 62.9 42.8 86.7 67.3
+0.6% +1.4% +0.6% +1.5% +0.4%
large (cid:37) 74.1 60.2 41.9 86.0 68.5
L/16 large (cid:34) 73.8 64.3 46.1 88.3 70.5
huge (cid:34) 74.2 66.0 46.6 89.9 72.7
+0.4% +1.7% +0.5% +1.6% +2.2%
recaptionsonthetrainingoftheRecap-CLIPB/16model,asdetailedinTable2. Specifically,for
eachsampleinatrainingbatch,werandomlysampletheoriginalcaptionwithprobability0≤p≤1
andourcaptionswithprobability1−p,referringtothemixedratio:
(cid:26)
Original withprobabilityp
Caption=
Recaption withprobability1−p
Thisstrategyensuresthateachbatchcontainsamixtureofourrecaptionandtheoriginalcaptions
controlled by probability p. The randomness allows each sample to encounter different captions
acrosstrainingepochs,potentiallyenhancingthemodel’sgeneralization.
Mainresults. OurfindingsaresummarizedinTable3. Weobservethatreducingthemixedratio
p(i.e.,increasingtheproportionofourrecaptiondata)initiallyleadstoanimprovementfollowed
by a decline in cross-modal retrieval performance. This initial improvement suggests that high-
qualityrecaptioneddataeffectivelyenhancescontrastivelearning. However,thesubsequentdecrease
indicatesthattheoriginalcaptionsfromDataComp-1Bprovidenecessarytrainingregularization,
preventingthemodelfromoverlyadaptingtothespecificqualitiesoftherecaptiondata. Interestingly,
wealsoobservethattheperformanceofCLIPisrelativelyinsensitivetocertainvariationsinthe
mixratiop,asevidencedbytheconsistentenhancementoverthebaseline(i.e.p=1.0)acrossallfour
differentcross-modalretrievalmetricswhenvaryingpfrom0.2(80%recaptiondata)to0.9(10%
recaptiondata). Forinstance,settingpat0.9and0.2bothyieldsasimilarperformanceenhancement
of∼3.5%,withthepeakperformanceoccurringatp=0.5,whichdeliversasubstantial∼5%boost.
Butmeanwhile,wenotethatincorporatingourrecaptions(negatively)affectsthezero-shotclassi-
ficationtask,exemplifiedbytheconsistentperformancedegradationacrossvaryingpvaluesfrom
0to0.9. Thephenomenonissimilarlyobservedintherecentwork[76]wheretheynotedirectly
fine-tuningonlongtextcansignificantlyhurttheCLIPperformanceandthereforeproposeseveral
techniques for enhancing learning with long texts. In this study, given our primary focus is on
assessingthequalityofRecap-DataComp-1B,wechoosetheratiop = 0.8tostrikeapromising
balancebetweentheclassificationperformance(i.e.,onlymarginallydrops0.5%)andthecross-modal
retrievalperformance(i.e.,withasignificant3.4%boostonaverage)forlaterablations.
5.3 TrainingwithLargerTextEncoder
We hereby investigate how the size of the text encoder affects models trained on a mixture of
the original captions and our recaptions (with p = 0.8). Specifically, we keep the architectural
configurationofthevisionbranchasinTable2andonlytwitchthetextencoder. Forinstance,in
thecaseoftheS/16model,wechangefromasmallertextencoderwith33Mparameterstoalarger,
base-sizedonewith53Mparameters.
MainResults Ourresults,asshowninTable4,highlightthatenlargingthetextencodercanfurther
enhanceperformanceacrossallmodelscales. Theaverageimprovementforadoptingalargertext
encoderinretrievaltasksis1.4%,1.0%,and1.5%forsmall,base,andlargemodels,respectively,
suggestingthatlargertextencoderscanhelptheCLIPmodellearnbetterfromsemanticallyrich
captions.
7Table5: Largertextencoderwithdifferentmixedratios. WechooseRecap-CLIP-B/16withlarge
textencoderforthisablation.
ImageNet-1K COCOR@1 Flickr30KR@1
mixedratiop
Validation I→T T→I I→T T→I
0.5 68.5 64.3 43.4 86.8 67.8
0.6 69.2 64.4 43.2 87.5 68.8
0.7 69.3 63.2 42.7 88.0 68.2
0.8 69.8 62.9 42.8 86.7 67.3
Table6: ComparisonontheUrban-1KandVG-Attributebenchmark.
Urban-1K VG
method re-caption
I→T T→I Attribute
OpenAI-CLIP-B/16[47] (cid:37) 67.4 53.3 62.6
OpenCLIP-B/16[22] (cid:37) 62.5 63.1 59.9
(cid:37) 53.2 50.9 57.1
Recap-CLIP-B/16
(cid:34) 85.0 87.3 66.4
+31.8% +36.4% +9.1%
(cid:37) 69.8 64.6 60.1
Recap-CLIP-L/16
(cid:34) 89.0 91.8 66.8
+19.2% +27.2% +6.7%
Moreover,were-assessthebalancedratioofrecaptiondatausingalargertextencoder. Specifically,
wegraduallyincreasetheratioofrecaptiondatafrom20%to50%,utilizingtheRecap-CLIP-B/16
modelwiththelargetextencoder. TheresultsarepresentedinTable5. Comparedtothepriorresults
whereanoptimalratioisachievedatp=0.8,usingalargertextencodercanfurtherpushthisoptimal
ratiotop=0.6. Inotherwords,thisresultconcludesthat,comparedtothevanillaversion,astronger
cross-modalretrievalperformancecanbeachievedif1)morerecaptionsareusedand2)alargertext
encoderisused.
5.4 Moreevaluationsontextunderstanding
RecentworksdemonstratethatCLIPmodelssufferfrompoorlongcontextunderstandinganddelicate
attributeunderstanding[73,76]. Giventhelong,enriched,andbetter-alignedcaptions,weexpect
Recap-CLIPtoexhibitbettertextunderstandingcapability. Thus,weevaluateourRecap-CLIPmodel
ontwobenchmarks: (1)Urban1K[76],along-captionimage-textretrievalbenchmarkthatcontains
1kurbanimagesandcorrespondingGPT-4Vcaptions;(2)VG-Attribution[73],amodifiedversionof
VisualGenome[25]totestmodelabilitiestoattributepropertiestoobjects. Theresultsareshownin
Tab.6.
WeobserveconsistentsignificantimprovementifthemodelistrainedonourRecap-Datacomp-1B
dataset. Forbothtext-to-imageandimage-to-textretrievalonUrban-1Kdataset,ourRecap-CLIP
modelssurpassthevanillabaselinebyatleast19%andsometimesuptoanastonishinglyhigh36%.
OntheVG-attributiondataset,itisworthnotingthatourRecap-CLIPbringsaperformanceboost
veryclosetothatoftheNegCLIPfine-tuning[73](e.g.∼9%vs.10%),alightweightdownstream
fine-tuningprocessdesignedtoboostCLIPabilitytounderstandattributeandorder. Nonetheless,itis
noteworthythatourRecap-CLIPisnaturallyequippedwithbettertextunderstandingability,without
anyspecifictargetedfine-tuning,indicatingtheimportanceofbettercaptionsinweb-scaledata.
6 Text-to-ImageGeneration
Ithasbeenknowntotheresearchcommunitythattrainingwithgenerated(high-quality)pseudo-
captionsimprovestext-to-imagegenerativemodelsintermsofgenerationqualityandpromptfol-
lowingability[8,7,5], primarilyduetothelowinformationandhighnoisedensitypresentedin
theoriginalweb-crawledcaptions. Therefore,weevaluatethequalityofourgeneratedcaptionsby
trainingText-to-Image(T2I)generativemodelsonRecap-DataComp-1Bforfurtherjustification. We
expecttheenrichedinformationinthegenerateddescriptionstobetteralignthevisualcontentin
images,andthusimprovetheperformanceoftheT2Imodels.
8Table7: Text-to-ImageevaluationonCOCO-30KresultsofDiT-BASE/4,trainedwithdifferentmix
ratiosonRecap-DataComp-1B.NoteforGPT-4VScore,weuseasubsetof3Kfortheevaluation.
Training Evaluation
Raw OurCOCO-Recap
mixedratiop
FID↓ CLIPScore↑ FID↓ CLIPScore↑ Recap-ClipScore↑ GPT-4VScore↑
0.00 37.6 29.2 27.8 32.5 28.3 2.53
−8.4 +3.1% +8.4% +1.1
0.05 38.5 29.1 27.9 32.5 28.0 2.51
0.10 36.0 29.7 27.2 32.7 28.2 2.51
0.15 35.8 29.9 28.2 33.0 28.1 2.45
0.20 35.8 29.8 28.4 32.7 28.0 2.53
0.50 35.3 29.3 30.2 31.9 26.7 2.13
0.75 31.3 29.4 32.7 31.2 25.8 1.89
1.00 32.5 28.9 36.2 29.3 19.9 1.40
Training. WeadoptDiffusionTransformers(DiT)[45]asourT2Imodel,wherethetextcondition
isfirstlyextractedwithaCLIPtextencoder[47],andtheninjectedintoeachDiTblockwiththe
cross-attentiondesign. Specifically,wefollowtheimagepreprocessingpipelineinDiT[45],where
theimagesarepreprocessedtohaveasquareresolutionof256. Themodelistrainedonvisuallatent
extractedusingapretrainedauto-encoderwithadownsamplingratioof8[50]. Similartothesetup
inpreviousexperiments,thetrainingtextconsistsofamixtureofrawcaptionsfromDatacomp-1B,
withaspecifiedproportionp,andtherestofthecaptionsreplacedbyrefinedcaptionsfromRecap-
Datacomp-1B.Moreover,thetrainingbatchsizeis2048,andtheAdamWoptimizer[37]isusedwith
aconstant1e-4learningrate,withoutanywarm-upscheduleorweightdecay. Wenametheresulting
modelRecap-DiT.
Evaluation. Forsampling,wesettheclassifier-freeguidancescaleas10anduse250DDPMstepsto
generate30kimageswithcaptionsfromMSCOCOandourimprovedgeneratedcaptionsforzero-shot
generationevaluation. WecalculateFréchetInceptionDistance(FID)[20]withthereferenceimages
fromMSCOCO[32]andCLIPscorewithbothOpenAIViT-B/32model[47]andourownRecap-
CLIPViT-L/16model,followingtheestablishedpipelineinpriorT2Iworks[5,67,54,23,36,78,55].
Additionally,followingtheGPT-4VmetricintroducedinSection4.2,werandomlyselectasubsetof
3,000ourgeneratedimagesforGPT-4Vevaluation.
Mainresults. WereportourobservationsinTab.7. Interestingly,whenusingrawCOCOcaptions
togenerate30,000imagesforevaluation,themodeltrainedwithdataintegratedwithourRecap-
Datacomp (for p < 1) demonstrates a better CLIP score, indicating improved vision-language
alignment. However,thereisnosignificantimprovementobservedintermsofFID.Ourhypothesis
isthatthemodeladaptstothemoreinformativeanddescriptiveprompts,andcouldunleashitsfull
potentialonlywhensimilarinformativetestingpromptsareprovided.
Therefore,inanothersetting,weevaluateimagesgeneratedusingourLLaVA-1.5-LLaMA3-8Brecap-
tionedversionoftherawCOCOcaptions. Here,weobserveconsistentandsignificantimprovements
inbothFIDandCLIPscores,particularlywhenmorethanhalfoftherecaptioneddataareintegrated
intothetrainingdataset. Notably, modelstrainedonRecap-Datacomp-1B(p = 0)surpassthose
trainedonthevanillaDatacomp-1B(p=1)byalargemargin,withimprovementsobservedinFID
(-8.4),CLIPscore(+3.1),Recap-CLIPscore(+8.4),andGPT-4Vscore(+1.1). Theseobservations
justifythatRecap-Datacomp-1Bbetterrevealsthepotentialoftext-to-imagemodelsingenerating
imageswithhighvisualqualityandimprovedalignmentwithtextualconditions.
Largermodels. Wefurthertrainalargermodel,DiT-L/2,for1epochwithamixedratioofp=0.0,
whilekeepingothertrainingparametersunchanged. ThemodelachievesanFIDof25.14andaCLIP
Scoreof34.82. InFigure4,wevisuallycomparethegeneratedresultsofDiT-L/2andDiT-B/4at
p=0.0. Itisevidentthatalthoughthequantitativescoresmaynotshowsubstantialimprovement,as
wescaleupthemodel,thereisanoticeableenhancementinthealignmentbetweenthegenerated
imagesandthecorrespondingtext,i.e.,thisimprovedalignmentresultsinhigher-qualityimagesthat
areabletocaptureandexpressmoreintricatedetails. TheseresultsconfirmthatDiTmodelstrained
onourrecaptionDataComp-1Bexhibitrobustscalabilityfortext-to-imagegenerativetasks.
9DiT-B/4 DiT-L/2 DiT-B/4 DiT-L/2
A group of elephantsis grazing in a grassy field with A giraffestands in a lush green field with tall grass
trees in the background. and treesin the background.
A black dog is standing in a sunny backyard with a Two brown bears are playfully splashing water at
variety of potted plants and flowersaround it. The each other in a river.
dog is wearing a blue collar and is looking towards
the right side of the image.
A white kitchen with a wooden cabinet above the A silver bus with a futuristic design is parked on the
sink, a white door with a window, and a white wall. side of the road. The bus features a prominent pink
The sink is filled with various items including a bottle, and black color schemewith a metallic finish. There is
a cup, and a spoon. There is a paper towel roll on one people standing near the bus. The bus has a large
the counter, and the floor appears to be tiled. windowthat covers most of its side, and there are
two small windows on the front.
Figure4: VisualcomparisonofgenerateresultsfromDiT-L/2andDiT-B/4atp=0.0,DiT-L/2has
bettertextcomprehensionandimagegenerationthanDiT-B/4. Wemarkentitiesintheinstruction.
7 Conclusion
ThispaAp esmrailnl atirropldanuec ies sflyRineg cloawp o-Dvera ata gCraossmy fpie-ld1 wBit,ha a large-sAc paelresoinmina ag beladcka rtiadisnegt japcakeitr eandd wwhitithe pdaenttas iisl edtextual
descripdtieonsnes f,orgesetn ine trhaet ebadckugrsoiunngd. tThhee suLnL isa sMhinAing- 3-powereriddinLg laa bvroawmn hoodrseel w.itOh au wrhcitoe mpaptcrhe ohne itns sfaivcee. analysis
through the trees, creating a warm glow. The airplane The rider is wearing a black helmet and is positioned
revealsthat,comparedtotheoriginal,web-crawledtextualdata,thesegenerateddescriptionsalign
is positioned in the center of the image, with its in the center of the image. The horse is galloping
moreacwcinugrsa stperleyadw anitdh thteh ceoicrkpciot crlreeasrlpy ovinsidblien. gThiem fiealdg esandaacrroessm a ogrraessyd feietaldi lweitdh. yUelltoiwli fzloinwgersR sceacttaepre-dD ataComp-
1Bfortarpapienairns gto rbees au llatneddinign stcripo,n asnids ttheen ftoreensth ias ncementsactrhorosusghvoaurti. oInu tshem boacdkegrlosu,nndo, tthaebrely arCe tLreIePs,apnda rat icularly
inimagceo-mtopo-tseedx tofa tnaldl, stteraxigt-htto tr-eiemstahgaet crreeattrei eav naaltutraals ks,andwionodteenx tfe-ntoce-.i mageDiffusionmodels,specifically
barrier.
in their ability to follow more closely to user-provided text instructions. By providing this high-
quality,publiclyavailable,large-scaleimage-textdataset,wehopetoinspireongoingresearchand
developmentthatwillpushtheboundariesofdevelopingvision-languagefoundationmodels,more
particularlyintheopen-sourcecommunity.
Acknowledge
ThisworkispartiallysupportedbyagiftfromAdobe,TPUResearchCloud(TRC)program,Google
CloudResearchCreditsprogram,AWSCloudCreditforResearchprogram,EdinburghInternational
DataFacility(EIDF)andtheData-DrivenInnovationProgrammeattheUniversityofEdinburgh.
10References
[1] JoshAchiam,StevenAdler,SandhiniAgarwal,LamaAhmad,IlgeAkkaya,FlorenciaLeoniAleman,
DiogoAlmeida,JankoAltenschmidt,SamAltman,ShyamalAnadkat,etal. GPT-4technicalreport. arXiv
preprintarXiv:2303.08774,2023.
[2] JoshAchiam,StevenAdler,SandhiniAgarwal,LamaAhmad,IlgeAkkaya,FlorenciaLeoniAleman,
DiogoAlmeida,JankoAltenschmidt,SamAltman,ShyamalAnadkat,etal. GPT-4V(ision)systemcard.
OpenAIResearchBlog,2023.
[3] Jean-BaptisteAlayrac,JeffDonahue,PaulineLuc,AntoineMiech,IainBarr,YanaHasson,KarelLenc,
ArthurMensch,KatherineMillican,MalcolmReynolds,etal. Flamingo: avisuallanguagemodelfor
few-shotlearning. InNeurIPS,2022.
[4] JinzeBai,ShuaiBai,ShushengYang,ShijieWang,SinanTan,PengWang,JunyangLin,ChangZhou,
andJingrenZhou. Qwen-VL:AVersatileVision-LanguageModelforUnderstanding,Localization,Text
Reading,andBeyond. arXivpreprintarXiv:2308.12966,2023.
[5] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang
Zhuang,JoyceLee,YufeiGuo,etal. Improvingimagegenerationwithbettercaptions. ComputerScience.
https://cdn.openai.com/papers/dall-e-3.pdf,2023.
[6] SoravitChangpinyo,PiyushSharma,NanDing,andRaduSoricut. Conceptual12m:Pushingweb-scale
image-textpre-trainingtorecognizelong-tailvisualconcepts. InCVPR,2021.
[7] JunsongChen,ChongjianGe,EnzeXie,YueWu,LeweiYao,XiaozheRen,ZhongdaoWang,PingLuo,
HuchuanLu,andZhenguoLi. Pixart-\sigma: Weak-to-strongtrainingofdiffusiontransformerfor4k
text-to-imagegeneration. arXivpreprintarXiv:2403.04692,2024.
[8] JunsongChen,JinchengYU,ChongjianGE,LeweiYao,EnzeXie,ZhongdaoWang,JamesKwok,Ping
Luo,HuchuanLu,andZhenguoLi.Pixart-$\alpha$:Fasttrainingofdiffusiontransformerforphotorealistic
text-to-imagesynthesis. InICLR,2024.
[9] LinChen,JisongLi,XiaoyiDong,PanZhang,ConghuiHe,JiaqiWang,FengZhao,andDahuaLin.
Sharegpt4v:Improvinglargemulti-modalmodelswithbettercaptions. arXivpreprintarXiv:2311.12793,
2023.
[10] ZheChen,JiannanWu,WenhaiWang,WeijieSu,GuoChen,SenXing,MuyanZhong,QinglongZhang,
XizhouZhu,LeweiLu,BinLi,PingLuo,TongLu,YuQiao,andJifengDai. Internvl:Scalingupvision
foundationmodelsandaligningforgenericvisual-linguistictasks. arXivpreprintarXiv:2312.14238,2023.
[11] Wei-LinChiang,LianminZheng,YingSheng,AnastasiosNikolasAngelopoulos,TianleLi,DachengLi,
HaoZhang,BanghuaZhu,MichaelJordan,JosephE.Gonzalez,andIonStoica. Chatbotarena:Anopen
platformforevaluatingllmsbyhumanpreference. arXivpreprintarXiv:2403.04132,2024.
[12] XiangxiangChu,LimengQiao,XinyuZhang,ShuangXu,FeiWei,YangYang,XiaofeiSun,YimingHu,
XinyangLin,BoZhang,etal. Mobilevlmv2: Fasterandstrongerbaselineforvisionlanguagemodel.
arXivpreprintarXiv:2402.03766,2024.
[13] JiaDeng,WeiDong,RichardSocher,Li-JiaLi,KaiLi,andLiFei-Fei.Imagenet:Alarge-scalehierarchical
imagedatabase. InCVPR,2009.
[14] KaranDesai,GauravKaul,ZubinAysola,andJustinJohnson. Redcaps: Web-curatedimage-textdata
createdbythepeople,forthepeople. arXivpreprintarXiv:2111.11431,2021.
[15] MingDing,ZhuoyiYang,WenyiHong,WendiZheng,ChangZhou,DaYin,JunyangLin,XuZou,Zhou
Shao,HongxiaYang,etal. Cogview:Masteringtext-to-imagegenerationviatransformers. InNeurIPS,
2021.
[16] LijieFan,DilipKrishnan,PhillipIsola,DinaKatabi,andYonglongTian. Improvingcliptrainingwith
languagerewrites. InNeurIPS,2024.
[17] AlexFang,AlbinMadappallyJose,AmitJain,LudwigSchmidt,AlexanderToshev,andVaishaalShankar.
Datafilteringnetworks. arXivpreprintarXiv:2309.17425,2023.
[18] ZhengcongFei,MingyuanFan,ChangqianYu,DebangLi,YouqiangZhang,andJunshiHuang. Dimba:
Transformer-mambadiffusionmodels. arXivpreprintarXiv:2406.01159,2024.
11[19] SamirYitzhakGadre,GabrielIlharco,AlexFang,JonathanHayase,GeorgiosSmyrnis,ThaoNguyen,
RyanMarten,MitchellWortsman,DhrubaGhosh,JieyuZhang,etal. Datacomp: Insearchofthenext
generationofmultimodaldatasets. arXivpreprintarXiv:2304.14108,2023.
[20] MartinHeusel,HubertRamsauer,ThomasUnterthiner,BernhardNessler,andSeppHochreiter. Gans
trainedbyatwotime-scaleupdateruleconvergetoalocalnashequilibrium. InNeurIPS,2017.
[21] MudeHui,SiweiYang,BingchenZhao,YichunShi,HengWang,PengWang,YuyinZhou,andCihang
Xie. Hq-edit:Ahigh-qualitydatasetforinstruction-basedimageediting. arXivpreprintarXiv:2404.09990,
2024.
[22] GabrielIlharco,MitchellWortsman,RossWightman,CadeGordon,NicholasCarlini,RohanTaori,Achal
Dave,VaishaalShankar,HongseokNamkoong,JohnMiller,HannanehHajishirzi,AliFarhadi,andLudwig
Schmidt. Openclip. arXivpreprintarXiv:2212.07143,2021.
[23] MingukKang,Jun-YanZhu,RichardZhang,JaesikPark,EliShechtman,SylvainParis,andTaesungPark.
ScalingupGANsfortext-to-imagesynthesis. InCVPR,2023.
[24] AndrejKarpathyandLiFei-Fei. Deepvisual-semanticalignmentsforgeneratingimagedescriptions. In
CVPR,2015.
[25] RanjayKrishna,YukeZhu,OliverGroth,JustinJohnson,KenjiHata,JoshuaKravitz,StephanieChen,
YannisKalantidis,Li-JiaLi,DavidAShamma,etal. Visualgenome: Connectinglanguageandvision
usingcrowdsourceddenseimageannotations. InIJCV,2017.
[26] TonyLee,YifanMai,ChiHeemWong,JosselinSomervilleRoberts,MichihiroYasunaga,FaarzanKaiyom,
RishiBommasani,andPercyLiang. Thefirststepstoholisticevaluationofvision-languagemodels,May
2024.
[27] JunnanLi,DongxuLi,SilvioSavarese,andStevenHoi.Blip-2:Bootstrappinglanguage-imagepre-training
withfrozenimageencodersandlargelanguagemodels. InICML,2023.
[28] JunnanLi,DongxuLi,CaimingXiong,andStevenHoi. Blip:Bootstrappinglanguage-imagepre-training
forunifiedvision-languageunderstandingandgeneration. InICML,2022.
[29] XianhangLi,ZeyuWang,andCihangXie. CLIPA-v2: ScalingCLIPTrainingwith81.1%Zero-shot
ImageNetAccuracywithina$10,000Budget;AnExtra$4,000Unlocks81.8%Accuracy. arXivpreprint
arXiv:2306.15658,2023.
[30] XianhangLi,ZeyuWang,andCihangXie. AnInverseScalingLawforCLIPTraining. InNeurIPS,2024.
[31] BinLin,ZhenyuTang,YangYe,JiaxiCui,BinZhu,PengJin,JinfaHuang,JunwuZhang,MunanNing,and
LiYuan.Moe-llava:Mixtureofexpertsforlargevision-languagemodels.arXivpreprintarXiv:2401.15947,
2024.
[32] Tsung-YiLin,MichaelMaire,SergeBelongie,JamesHays,PietroPerona,DevaRamanan,PiotrDollár,
andCLawrenceZitnick. Microsoftcoco:Commonobjectsincontext. InECCV,2014.
[33] HaotianLiu,ChunyuanLi,YuhengLi,andYongJaeLee. Improvedbaselineswithvisualinstruction
tuning. InNeurIPS2023WorkshoponInstructionTuningandInstructionFollowing,2023.
[34] HaotianLiu,ChunyuanLi,YuhengLi,BoLi,YuanhanZhang,ShengShen,andYongJaeLee. Llava-next:
Improvedreasoning,ocr,andworldknowledge. https://llava-vl.github.io/blog/2024-01-30-llava-next/,
January2024.
[35] HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee. Visualinstructiontuning. InNeurIPS,2024.
[36] XingchaoLiu,XiwenZhang,JianzhuMa,JianPeng,andQiangLiu. Instaflow:Onestepisenoughfor
high-qualitydiffusion-basedtext-to-imagegeneration. arXivpreprintarXiv:2309.06380,2023.
[37] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint
arXiv:1711.05101,2017.
[38] GuansongLu,YuanfanGuo,JianhuaHan,MinzheNiu,YihanZeng,SongcenXu,ZeyiHuang,Zhao
Zhong,WeiZhang,andHangXu. Pangu-draw:Advancingresource-efficienttext-to-imagesynthesiswith
time-decoupledtrainingandreusablecoop-diffusion. arXivpreprintarXiv:2312.16486,2023.
[39] ThaoNguyen,SamirYitzhakGadre,GabrielIlharco,SewoongOh,andLudwigSchmidt. Improving
multimodaldatasetswithimagecaptioning. InNeurIPS,2024.
12[40] AlexNichol,PrafullaDhariwal,AdityaRamesh,PranavShyam,PamelaMishkin,BobMcGrew,Ilya
Sutskever,andMarkChen. Glide:Towardsphotorealisticimagegenerationandeditingwithtext-guided
diffusionmodels. arXivpreprintarXiv:2112.10741,2021.
[41] OpenAI. Dall·e3systemcard. OpenAIResearchBlog,2023.
[42] OpenAI. Videogenerationmodelsasworldsimulators. OpenAIResearchBlog,2024.
[43] VicenteOrdonez,GirishKulkarni,andTamaraBerg.Im2text:Describingimagesusing1millioncaptioned
photographs. InNeurIPS,2011.
[44] PiotrPadlewski,MaxBain,MatthewHenderson,ZhongkaiZhu,NishantRelan,HaiPham,DonovanOng,
KaloyanAleksiev,AitorOrmazabal,SamuelPhua,etal. Vibe-eval:Ahardevaluationsuiteformeasuring
progressofmultimodallanguagemodels. arXivpreprintarXiv:2405.02287,2024.
[45] WilliamPeeblesandSainingXie. Scalablediffusionmodelswithtransformers. InICCV,2023.
[46] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,SandhiniAgarwal,Girish
Sastry,AmandaAskell,PamelaMishkin,JackClark,etal. Learningtransferablevisualmodelsfrom
naturallanguagesupervision. InICML,2021.
[47] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,SandhiniAgarwal,Girish
Sastry,AmandaAskell,PamelaMishkin,JackClark,GretchenKrueger,andIlyaSutskever. Learning
transferablevisualmodelsfromnaturallanguagesupervision. InICML,2021.
[48] AdityaRamesh,PrafullaDhariwal,AlexNichol,CaseyChu,andMarkChen.Hierarchicaltext-conditional
imagegenerationwithcliplatents. arXivpreprintarXiv::2204.06125,2022.
[49] AdityaRamesh,MikhailPavlov,GabrielGoh,ScottGray,ChelseaVoss,AlecRadford,MarkChen,and
IlyaSutskever. Zero-shottext-to-imagegeneration. InICML,2021.
[50] RobinRombach,AndreasBlattmann,DominikLorenz,PatrickEsser,andBjörnOmmer. High-resolution
imagesynthesiswithlatentdiffusionmodels. InCVPR,2022.
[51] NoamRotstein,DavidBensaid,ShakedBrody,RoyGanz,andRonKimmel. Fusecap:Leveraginglarge
languagemodelstofusevisualdataintoenrichedimagecaptions. arXivpreprintarXiv:2305.17718,2023.
[52] OlgaRussakovsky,JiaDeng,HaoSu,JonathanKrause,SanjeevSatheesh,SeanMa,ZhihengHuang,
AndrejKarpathy,AdityaKhosla,MichaelBernstein,AlexanderC.Berg,andLiFei-Fei. ImageNetLarge
ScaleVisualRecognitionChallenge. InIJCV,2015.
[53] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar
Ghasemipour,RaphaelGontijoLopes,BurcuKaragolAyan,TimSalimans,etal. Photorealistictext-to-
imagediffusionmodelswithdeeplanguageunderstanding. InNeurIPS,2022.
[54] AxelSauer,TeroKarras,SamuliLaine,AndreasGeiger,andTimoAila. StyleGAN-T:Unlockingthe
powerofGANsforfastlarge-scaletext-to-imagesynthesis. arXivpreprintarXiv:2301.09515,2023.
[55] AxelSauer,DominikLorenz,A.Blattmann,andRobinRombach. Adversarialdiffusiondistillation. ArXiv,
abs/2311.17042,2023.
[56] ChristophSchuhmann,RomainBeaumont,RichardVencu,CadeGordon,RossWightman,MehdiCherti,
TheoCoombes,AarushKatta,ClaytonMullis,MitchellWortsman,etal. Laion-5b:Anopenlarge-scale
datasetfortrainingnextgenerationimage-textmodels. InNeurIPS,2022.
[57] ChristophSchuhmann,RichardVencu,RomainBeaumont,RobertKaczmarczyk,ClaytonMullis,Aarush
Katta,TheoCoombes,JeniaJitsev,andAranKomatsuzaki. Laion-400m:Opendatasetofclip-filtered400
millionimage-textpairs. arXivpreprintarXiv:2111.02114,2021.
[58] PiyushSharma, NanDing, SebastianGoodman, andRaduSoricut. Conceptualcaptions: Acleaned,
hypernymed,imagealt-textdatasetforautomaticimagecaptioning. InACL,2018.
[59] KrishnaSrinivasan,KarthikRaman,JiecaoChen,MichaelBendersky,andMarcNajork. Wit:Wikipedia-
basedimagetextdatasetformultimodalmultilingualmachinelearning. InSIGIR,2021.
[60] IngridStevens. Llama3’sperformancebenchmarkvaluesexplained. https://medium.com/. Accessed:
2024-06-05.
13[61] ZhiqingSun,ShengShen,ShengcaoCao,HaotianLiu,ChunyuanLi,YikangShen,ChuangGan,Liang-
YanGui,Yu-XiongWang,YimingYang,etal.Aligninglargemultimodalmodelswithfactuallyaugmented
rlhf. arXivpreprintarXiv:2309.14525,2023.
[62] Meta LLaMA Team. Introducing Meta Llama 3: The most capable openly available LLM to date.
https://ai.meta.com/blog/meta-llama-3/,2024.
[63] ZiruiWang,JiahuiYu,AdamsWeiYu,ZihangDai,YuliaTsvetkov,andYuanCao. Simvlm:Simplevisual
languagemodelpretrainingwithweaksupervision. InICLR,2022.
[64] HuXu,SainingXie,XiaoqingTan,Po-YaoHuang,RussellHowes,VasuSharma,Shang-WenLi,Gargi
Ghosh,LukeZettlemoyer,andChristophFeichtenhofer. Demystifyingclipdata. InICLR,2023.
[65] RuyiXu,YuanYao,ZonghaoGuo,JunboCui,ZanlinNi,ChunjiangGe,Tat-SengChua,ZhiyuanLiu,and
GaoHuang. LLaVA-UHD:anlmmperceivinganyaspectratioandhigh-resolutionimages. arXivpreprint
arXiv:2403.11703,2024.
[66] Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual
denotations:Newsimilaritymetricsforsemanticinferenceovereventdescriptions. InTACL,2014.
[67] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan,
AlexanderKu,YinfeiYang,BurcuKaragolAyan,etal. Scalingautoregressivemodelsforcontent-rich
text-to-imagegeneration. InTMLR,2022.
[68] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan,
AlexanderKu,YinfeiYang,BurcuKaragolAyan,BenHutchinson,WeiHan,ZaranaParekh,XinLi,Han
Zhang,JasonBaldridge,andYonghuiWu. Scalingautoregressivemodelsforcontent-richtext-to-image
generation. InTMLR,2022.
[69] Qiying Yu, Quan Sun, Xiaosong Zhang, Yufeng Cui, Fan Zhang, Xinlong Wang, and Jingjing Liu.
Capsfusion:Rethinkingimage-textdataatscale. arXivpreprintarXiv:2310.20550,2023.
[70] TianyuYu,YuanYao,HaoyeZhang,TaiwenHe,YifengHan,GanquCui,JinyiHu,ZhiyuanLiu,Hai-Tao
Zheng,MaosongSun,etal. Rlhf-v:Towardstrustworthymllmsviabehavioralignmentfromfine-grained
correctionalhumanfeedback. arXivpreprintarXiv:2312.00849,2023.
[71] WeihaoYu,ZhengyuanYang,LinjieLi,JianfengWang,KevinLin,ZichengLiu,XinchaoWang,and
LijuanWang. Mm-vet:Evaluatinglargemultimodalmodelsforintegratedcapabilities. InICML,2024.
[72] XiangYue,YuanshengNi,KaiZhang,TianyuZheng,RuoqiLiu,GeZhang,SamuelStevens,Dongfu
Jiang,WeimingRen,YuxuanSun,CongWei,BotaoYu,RuibinYuan,RenliangSun,MingYin,Boyuan
Zheng,ZhenzhuYang,YiboLiu,WenhaoHuang,HuanSun,YuSu,andWenhuChen. Mmmu:Amassive
multi-disciplinemultimodalunderstandingandreasoningbenchmarkforexpertagi. InCVPR,2024.
[73] MertYuksekgonul,FedericoBianchi,PratyushaKalluri,DanJurafsky,andJamesZou. Whenandwhy
vision-languagemodelsbehavelikebags-of-words,andwhattodoaboutit? InICLR,2022.
[74] XiaohuaZhai,BasilMustafa,AlexanderKolesnikov,andLucasBeyer. Sigmoidlossforlanguageimage
pre-training. InICCV,2023.
[75] XiaohuaZhai,XiaoWang,BasilMustafa,AndreasSteiner,DanielKeysers,AlexanderKolesnikov,and
LucasBeyer. Lit:Zero-shottransferwithlocked-imagetexttuning. InCVPR,2022.
[76] BeichenZhang, PanZhang, XiaoyiDong, YuhangZang, andJiaqiWang. Long-clip: Unlockingthe
long-textcapabilityofclip. arXivpreprintarXiv:2403.15378,2024.
[77] LeiZhang,FangxunShu,SuchengRen,BingchenZhao,HaoJiang,andCihangXie. Compress&align:
Curatingimage-textdatawithhumanknowledge. arXivpreprintarXiv:2312.06726,2023.
[78] MingyuanZhou,ZhendongWang,HuangjieZheng,andHaiHuang. Longandshortguidanceinscore
identitydistillationforone-steptext-to-imagegeneration. arXivpreprintarXiv:2406.01561,2024.
14