WikiWorkshop(11thedition)–June20,2024
ORES-Inspect: A technology probe for machine learning audits on enwiki
ZacharyLevonian LaurenHagen LuLi
DigitalHarborFoundation UniversityofMinnesota UniversityofPennsylvania
JadaLilleboe SolvejgWastvedt AaronHalfaker LorenTerveen
UniversityofMinnesota UniversityofMinnesota MicrosoftResearch UniversityofMinnesota
Abstract addressthosebarriers,wearebuildingORES-Inspect,an
open-source3tooltoauditthebehavioroftheORESedit
Auditing the machine learning (ML) models
qualitymodelforEnglishWikipedia.
usedonWikipediaisimportantforensuringthat
Intheconsensus-drivenWikipediacontext,thedevel-
vandalism-detection processes remain fair and
opersofML-drivensystemslikeORESareenthusiastic
effective. However, conducting audits is chal-
aboutreceivingcommunityinputonproblemsorpoten-
lenging because stakeholders have diverse pri- tialareasforimprovement. Thus,thekeydesignobjective
orities and assembling evidence for a model’s
for ORES-Inspect is to address problems (a) and (b) by
[in]efficacyistechnicallycomplex. Wedesigned
making it easy to identify high-quality quantitative evi-
aninterfacetoenableeditorstolearnaboutand
dence of the ORES edit quality model’s behaviors. We
audittheperformanceoftheORESeditquality
are developing ORES-Inspect as a “technology probe”
model. ORES-Inspect1 is an open-source web
toreflectontheprocessofconductingMLauditsinthe
tool and a provocative technology probe for re-
Wikipediacontextbyhighlightingthebenefitsandchal-
searching how editors think about auditing the
lengesofcollectingquantitativeevidenceofsystembugs
many ML models used on Wikipedia. We de-
(Hutchinsonetal.,2003).
scribethedesignofORES-Inspectandourplans
Functionally,ORES-Inspectisalabelinginterfacefor
forfurtherresearchwiththissystem.
individualWikipediaedits. Thekeyintuitionisthatany
Wikipediausermaybeinterestedinauditingasystemlike Keywords: machinelearning,auditing,tools,ORES,
ORES,butdifferentauditorswillhavedifferentpriorities
editquality
(e.g. are new editors unfairly targeted, is vandalism on
Introduction stubs missed more often than on larger articles, etc.).
Forthatreason,theprocessofauditingistheprocessof
ORES is a widely-used service for building and host- quantifyingone’sintuitionsandidentifyingevidencethat
ingmachinelearningmodelsrequestedbytheWikipedia asinglemisclassificationrepresentsapatternthatshould
community (Halfaker and Geiger, 2020). Of particular bechanged. Therefore,wedesignedORES-Inspectasa
relevanceistheeditqualitymodel,whichmakespredic- provocation: itisdesignedtoeducateeditorsabouthow
tionsaboutthequalityofindividualWikipediaeditsand MLmodelscanbeauditedandhowtotranslateintuitions
isusedinothersystemsforvandalismdetectionandre- intohigh-qualityevidence.
moval. ORES’editqualitypredictionsdirectlyinfluence
To fulfill this educational objective and to make au-
thelikelihoodofaneditbeingreverted(TeBlunthuisetal.,
ditingtractableforusers,wedesignedtheinterface(Fig-
2020). Thisimpactisanotablesuccessforcommunity-
ure 1) around four phases of activity. We will describe
centered and participatory machine learning processes:
our design decisions, the data, and our future analysis
ORESishostinganincreasingnumberofmodels.2
plans in the remainder of this extended abstract, but we
AkeychallengeforORESandothermachinelearning
concludethisintroductionwiththeverbatimcontentsof
servicesisthatitishardtodetermineifamodeliscon-
theinfopanelshowntoORES-Inspectusersonfirstlogin:
sistentlyproducingreasonableoutputs. Inotherwords,it
ORESfindsvandalism. ORESisamachinelearning
ishardtoauditthesemodels. Therearemanybarriersto
model that gives every edit on Wikipedia a score from
auditingcomplexmachinelearningsystemslikeORES:
0 (least likely to be damaging) to 1 (most likely to be
(a)identifyingarelevantsampleofincorrectpredictions,
damaging). Score predictions are used to highlight the
(b)determiningifthoseincorrectpredictionsrepresenta
Recent Changes feed and in other places to find and re-
patternofundesiredbehavior(a“bug”),and(c)convinc-
vertvandalism. ORES-InspecthelpsyouauditORESby
ing system designers to fix the undesired behavior. To
looking at score predictions and determining if they are
1https://ores-inspect.toolforge.org correct. AuditORESinfoursteps:
2ORES is being replaced with LiftWing, but this work is
applicabletoanyrevscoringmodel. 3https://github.com/levon003/wiki-ores-feedback
©Copyrightheldbytheowner/author(s),publishedunderCreativeCommonsCCBY4.0License
4202
nuJ
21
]CH.sc[
1v35480.6042:viXraWikiWorkshop(11thedition)–June20,2024
Figure1: TheORES-Inspectinterfaceandloginpage,asaccessibleviaToolforge.
©Copyrightheldbytheowner/author(s),publishedunderCreativeCommonsCCBY4.0LicenseWikiWorkshop(11thedition)–June20,2024
1. Filter: Choose which edits to look at. ORES- sampling of revisions. By then inspecting and labeling
Inspect shows you all human edits on mainspace specificrevisionsasdamagingornotdamaging,auditors
articles by default, but you can filter down to look create quantitative estimates of the prevalance of false
only at edits on particular pages (such as pages re- positives and/or false negatives for a specific subset of
lated to LGBT history) or from particular editors pages,edits,oreditors.
(suchasnewcomers).
Discussion&FutureWork
Or, usethefiltercontrolstochoosesomethingelse
entirely,likeboteditsonTalkpages! Otherresearch-driveninterfacesforworkingwithORES
include Wikibench for curating and discussing training
2. Focus: When an edit is damaging, it is usually
data(Kuoetal.,2024)andORESExplorerforexploring
reverted by the editor community. ORES-Inspect
fairness trade-offs induced by model thresholding deci-
helpsyoufocusoncaseswherethecommunitybe-
sions(Yeetal.,2021). Wefocusonauditingmodelsthat
haviordisagreeswiththeORESprediction.
arealreadyinusewithanemphasisonbuildingquanti-
If you choose to look at Unexpected Reverts, tativeevidenceofMLsystembugs. Inourexperienceas
you’re looking at edits that ORES thinks are non- Wikipediaeditors,weobservethatmostfeedbackonML
damaging... but that the community reverted any- systems happens on the basis of a single bad prediction
way. noticed while focused on other editing work. ORES-
Inspectaimstobeatoolforturningthosesingletonsinto
If you choose to look at Unexpected Consensus,
rigorousandusefulaudits,andwehavealreadyfoundthe
you’relookingateditsthatORESthinksaredamag-
toolhelpfulforreflectingonhowone’sopinionsonedit
ing... butthatthecommunitydidn’trevert.
qualitymightdivergefromconsensus. Aswecontinueto
3. Inspect: Lookatindividualeditsandlabelthemas developORES-Inspect,weintendtoconductinterviews
damaging(“Iwouldrevertthis.”) ornotdamaging. witheditorsandsharetheresultsofauditsconductedwith
See if you can find a pattern of errors in ORES’ thetool,aimingtogeneratediscussiononhowandwhen
predictions. MLmodelefficacyshouldbeevaluatedbyeditors.
4. Discuss: View a summary of your edit labels by Acknowledgements
clicking“ViewAnnotationHistory”. Howoftendid
ORESmisclassifytheeditsyoulookedat? We would like to thank Haiyi Zhu, Phyllis Gan, and
IsaacJohnsonfortheircontributionsandtheWikiWork-
YoucandiscussyourresultswiththeORESdevel-
shop reviewersfor usefulfeedback. Thiswork waspar-
opers. If you change the filters, you can compare
tiallysupportedbytheNationalScienceFoundationunder
twogroupsofeditstoidentifybias(“Arenewcom-
GrantNo.1908688.
ers’editsmisclassifiedmoreoftenthanexperienced
editors’?”)
References
Implementation&Data
[HalfakerandGeiger2020] AaronHalfakerandR.Stuart
ORES-InspectisaReactandPythonapphostedonTool- Geiger. 2020. ORES:LoweringBarrierswithPartic-
forge. Auditorsusefilterstofocustheirattentiononspe- ipatoryMachineLearninginWikipedia. ACMHum.-
Comput.Interact.,4(CSCW2):148:1–148:37,October.
cific properties of articles (namespace, category, size),
ofedits(size,markedasminor),orofusers(registration
[Hutchinsonetal.2003] Hilary Hutchinson, Wendy
status, bot status). ORES-Inspect is based on the 35.6 Mackay, Bo Westerlund, Benjamin B. Bederson,
millionnon-botenwikieditsin2019andthecorrespond- AllisonDruin,CatherinePlaisant,MichelBeaudouin-
ing prediction made by ORES at the time of the edit,4 Lafon, Ste´phane Conversy, Helen Evans, Heiko
Hansen,NicolasRoussel,andBjo¨rnEiderba¨ck. 2003.
but auditors focus on only those revisions that have al-
Technology probes: inspiring design for and with
readyreceivedattentionbythecommunity: Unexpected families. InCHI’03,pages17–24.ACM,April.
ConsensuseditsarepredictedtobedamagingbyORES
but were not reverted within 1 year, while Unexpected [Kuoetal.2024] Tzu-Sheng Kuo, Aaron Halfaker, Zirui
Cheng,JiwooKim,Meng-HsinWu,TongshuangWu,
Revertswerepredictedtobenon-damagingbyORESbut
KennethHolstein, andHaiyiZhu. 2024. Wikibench:
were reverted. By focusing on these two categories, we
Community-Driven Data Curation for AI Evaluation
focus on identifying false positives and false negatives onWikipedia,February. arXiv:2402.14147[cs].
respectively with a much higher precision than random
[TeBlunthuisetal.2020] Nathan TeBlunthuis, Ben-
4HistoricalORESpredictionswereonlyavailableuntilthe jamin Mako Hill, and Aaron Halfaker. 2020.
endof2019. The effects of algorithmic flagging on fairness:
©Copyrightheldbytheowner/author(s),publishedunderCreativeCommonsCCBY4.0LicenseWikiWorkshop(11thedition)–June20,2024
quasi-experimental evidence from Wikipedia.
arXiv:2006.03121[cs],June.
[Yeetal.2021] Zining Ye, Xinran Yuan, Shaurya Gaur,
Aaron Halfaker, Jodi Forlizzi, and Haiyi Zhu. 2021.
WikipediaORESExplorer: VisualizingTrade-offsFor
DesigningApplicationsWithMachineLearningAPI.
InDIS’21,DIS’21,pages1554–1565.ACM,June.
©Copyrightheldbytheowner/author(s),publishedunderCreativeCommonsCCBY4.0License