ICE-G: Image Conditional Editing of 3D Gaussian Splats
VishnuJaganathan1,HannahHanyunHuang1,MuhammadZubairIrshad1,2,
VarunJampani3,AmitRaj4,ZsoltKira1
1GeorgiaInstituteofTechnology,2ToyotaResearchInstitute,3StabilityAI,4GoogleResearch
{vjaganathan3, hhuang474, mirshad7, zkira}@gatech.edu,
varunjampani@gmail.com, amitraj93.github.io
Figure1.Ourmethod,ICE-G,allowsforquickcolorortextureeditstoa3Dscenegivenasinglestyleimage,ormaskselectiononasingle
view.WeshowtworenderedviewsofmaskselecteditingfortheGardenScenewhereweapplystonetexturetothetableandfallcolorsto
thegrass(left).Wealsoshowtworendersofcorrespondancebasededitingwherewecantransferthecolorofthebluecartothelegoandthe
textureofthegrasstothetable(right).
Abstract Theseeditedviewsactasanupdateddatasettofurthertrain
Recentlymanytechniqueshaveemergedtocreatehigh and re-style the 3D scene. The end-result is therefore an
quality 3D assets and scenes. When it comes to editing edited 3D model. Our framework enables a wide variety
of these objects, however, existing approaches are either ofeditingtaskssuchasmanuallocaledits,correspondence
slow,compromiseonquality,ordonotprovideenoughcus- basedstyletransferfromanyexampleimage,andacombi-
tomization. Weintroduceanovelapproachtoquicklyedita nationofdifferentstylesfrommultipleexampleimages. We
3Dmodelfromasinglereferenceview. Ourtechniquefirst useGaussianSplatsasourprimary3Drepresentationdue
segmentstheeditimage,andthenmatchessemanticallycor- totheirspeedandeaseoflocalediting,butourtechnique
respondingregionsacrosschosensegmenteddatasetviews worksforothermethodssuchasNeRFsaswell. Weshow
using DINO features. A color or texture change from a throughmultipleexamplesthatourmethodproduceshigher
particularregionoftheeditimagecanthenbeappliedto qualityresultswhileofferingfinegrainedcontrolofediting.
otherviewsautomaticallyinasemanticallysensiblemanner. Projectpage: ice-gaussian.github.io
1
4202
nuJ
21
]VC.sc[
1v88480.6042:viXra1.Introduction to transfer style from. To find these matches, we utilize
acustomheuristicwhichminimizesthedistancebetween
Editing of 3D scenes and models is an area of growing
thesemaskregionsinanextractedDINO[4]featurespace.
importanceasapplicationslikeroboticssimulation,video
Wethencopyovercolorsbychangingthehueandcopyover
games, and virtual reality grow in popularity. Editable
texturesbyrefittingthemwithTextureReformer[27]. To
3Drepresentationscanleadtodynamicandcustomizable
applytheseupdatesontothe3Dmodel(e.g. GaussianSplat),
environmentsintheseapplications,allowingartists,devel-
wethenfinetuneitusingL1andSSIMlossesforcolor,and
opers, and researchers alike to quickly iterate on projects
aNearestNeighborFeatureMatching(NNFM)loss[30]for
andproducevaluablecontent.
texture.
Recently, Gaussian Splats [12] have emerged as a
WegenerateresultsforobjectsandscenesintheNeRF
powerful method to represent 3D objects and scenes,
Synthetic [18], MipNeRF-360 [2], and RefNeRF [24]
allowingforfasttrainingandpreservationofhigh-quality
datasetsandcompareagainstcolorandtextureeditingbase-
details. Priortothis,NeRFs(NeuralRadianceFields)[18]
linestoshowqualitativeimprovementofourmethod. Over-
have been used extensively to create scenes, and many
all,ourmaincontributionsare: 1)Weprovideaflexibleand
techniqueshavebeenintroducedtoeditthecolorandtexture
expressivemodeofspecifyingedits,leveragingSAMand
of NeRFs. However, such editing has thus far been slow
usingaDINO-basedheuristictomatchimageregionstothe
andlimitedinthetypesofeditspossible. Ourworkseeks
editingimageinamultiviewconsistentmanner,and2)We
todevelopageneralmethodthatworksonbothSplatsand
providefinegrainedcontrolofchoosingcolorsandtextures
NeRFs,supportingfastandhigh-qualitystyleedits.
foreachpartofthesegmentededitingview.
NeRFeditingworkscanbecategorizedbytheireditingin-
terfacesintotext-basedorimage-basedmethods. Text-based 2.RelatedWorks
approachesusetext-imagemodelsforguidance,delivering
resultsfaithfultopromptsbutlimitedbytheambiguityof Thereareafewworksthataimtoedit3Dmodels,andthey
textdescriptionsfor3Dscenes. Thisleadstouncertainties fallintoafewcategories. Firstarediffusion-basedediting
inconveyingspecificcolors,styles,ortextures,suchasthe methods, which broadly try to lift inconsistent 2D image
exactshadeof”lightblue”ortheprecisepatternofa”sand editsfromtextpromptsinto3Dviaspecializedlosses. There
texture.” arealsolocaltextureeditingmethodsthatareabletotarget
Our image-based editing approach addresses the ambi- regions and apply textures from a source image. Finally,
guitiesoftext-basedmethods,yetcurrenttechniqueslimit sinceourmethodiscapableofcoloreditingaswell,there
modificationstoasinglestyleimagefortheentiresceneand are purely color edit methods we compare to that usually
lacktheabilitytotransfercolorortexturebetweendifferent applymanuallyspecifiedcolorstoimages.
imagepartsorspecifythemperregion.Additionally,3Dedit-
2.1.2DPriors
ingclassificationsbasedonchanges—color,texture,shape,
ortheircombinations—showthatshapemodificationsoften InstructNeRF2NeRF[10]adaptstheInstructPix2pix[3]2D
reduce image quality by converting 2D guidance into 3D editing model to 3D, enabling edits in color, shape, and
usingmethodslikeScoreDistillationSampling(SDS)[19] texture based on text prompts. Initially applied to select
or Iterative Dataset Update (IDU) [10], which generalize datasetimages,theseeditsmaylackmultiviewconsistency
featuresattheexpenseofdetail. butachieve3DuniformitythroughIterativeDatasetUpdate
Inthispaper,weproposeamethodthataimstotakethe (IDU),progressivelyrefiningmoredatasetexamples. While
textureand/orcolorofdifferentsegmentedregionsfroman this method supports extensive shape and color modifica-
editingimage,andtransferthemtocorrespondingsegmented tions, it tends to fall short in result quality and detailed
regionsofasampledsetof2Dimagesfromtheoriginalscene texturerendering.
datasetina3Dconsistentmanner. Togeneratehighquality Vox-E [21] uses a voxel grid and diffusion model for
results,werestrictourmethodtoeditingcolorandtexture updates,focusingonlargefeatureedits. Itprocessesviews
whilepreservingshape. Thiseditingimagecaneitherbea withtext-guidednoisepredictionstoalignedits,butstruggles
totallydifferentobjectoraneditedviewfromtheoriginal withfinetexture/coloradjustments,oftenresultinginblocky
dataset. texturesorunintendedareaexpansions.
Todothis,givena3Dmodel(e.g. SplatorNeRF),we Blended-NeRF [9] blends new objects or textures into
proposetosampleandeditasubsetoftheoriginaldataasa scenes, guided by CLIP [20] losses to match text inputs
preprocessingstep. Specifically,weusetheSegmentAny- within a chosen 3D scene region. It modifies the scene’s
thingModel(SAM)[13]tofindcorrespondingregionsof MLPwithCLIPlossandblendscolorsanddensitiesforthe
boththeeditingimageandthesampledviews. Foreachre- edits. While it achieves realistic textures, as a text-based
gionofeachsampledviewfromtheoriginaldataset,wehave method,itfaceschallengesinaccuratelyconveyingcomplex
tofindthebestcorrespondingregionfromtheeditingimage texturesorspecificregionswithoutimageinput.
22.2.LocalTextureEditing descriptiontoanLLMtoselectregionsofinterest,andthen
appliesa2Ddiffusionpriortoeditvariousviews. Another
S2RF[15]introduceslocaltextureeditingforspecificscene
similarlynamedpaperGaussianEditor: SwiftandControl-
types,utilizinganobjectdetectionmodelalongwithSAM
lable3DEditingwithGaussianSplatting[5]introducesHi-
for precise region masking. This method applies NNFM
erarchicalGaussianSplatting,atechniquetoallowmorefine
loss from ARF for style/texture transfer onto masked ar-
grainededitingvia2Ddiffusionpriors. Theusermustselect
eas,demonstratingthecapabilitytoapplyvariedtexturesto
pointsonthescreen,andchangevisualaspectsmanuallyin
differentsceneparts.
2Dfortheedittobecarriedoverto3D.
Semantic-drivenImage-basedNeRFEditing(SINE)[1]
Instruct-GS2GSEditing: Editing3DGaussianSplatting
offersamethodfor3Dtextureediting, leveragingaprior-
SceneswithInstructions[23]isbasedoffInstructNerf2Nerf
guidededitingfieldcombinedwithoriginalviews. Itusesa
andinheritsthesameadvantagesanddisadvantagesofthat
ViT[6]extractedstylefeaturestoadjusttextures,enabling
method. The authors use the same IDU method, but tune
localized edits. While it supports seamless rendering by
somehyperparameterstosuitGaussianSplatting. Another
mergingtemplateNeRFwiththeeditingfield,theprocess
paperTIP-Editor: AnAccurate3DEditorFollowingBoth
demands12hoursoftrainingpersceneandfacescompati-
Text-PromptsAndImage-Prompts[31]usesLoRAtoperson-
bilityissueswithGaussianSplatsduetoitsuniquerendering
alizeadiffusionmodelwiththestyleofareferenceimage,
approach.
and then uses this along with a user prompt to generate
2.3.ColorEditing 2D edits. These edits are additionally bounded by a user
specifiedregiontocontainedits.
DecomposingNeRFforEditingviaFeatureFieldDistilla-
Overall,thesemethodsshowsomeinterestingresultson
tion[14]allowscoloreditingofNeRFsusingtextprompts.
editingusing2DDiffusionpriors,butsometimessufferthe
Itgeneratesafeaturefieldforselectingandalteringcolorsin
quality downgrade associated with diffusion models, and
3Dregions. UtilizingCLIP-LSeg[17]andDINO[4]as2D
arenotabletotrasnferstylegloballyfromastandalone2D
teachernetworks,itlearnsanextrafeaturefieldintegrated
image.
intotheoriginalNeRF,applyingupdatesthroughphotomet-
ricandfeaturelossfunctions. Thisapproachenablessoft3D
3.Method
segmentationviaadotproductbetweenanencodedquery
andthefeaturefield,facilitatingtext-specifiedcoloreditsin
3Dregionsthroughmodifiedrenderingfunctions.
CLIP-NeRF[26]learnsaconditionalNeRFrepresenta-
tionthataimstoseparateappearanceandshapeinformation.
CLIPembeddingsarepassedthroughappearanceandshape
mappers which extract the respective information and ad-
ditively combine them with the conditional NeRF. These
mappinglayersaretrainedalongwiththeNeRFviaaCLIP
similaritylossiteratingoverrandomlysampledNeRFviews.
This method primarily edits color, but also shows minor
shapechangesonobjectslikecarsandchairs.
RecolorNeRF[8]aimstodecomposethesceneintoaset
ofpure-coloredlayers,andeditingthatpallettochangethe
colorofthescene. Thismethodachievesaestheticresults,
but cannot distinguish between two different objects that
havethesamecolorinascene. ProteusNeRF[25]isable
Figure 2. The user supplied style image is segmented and its
torapidlyeditthecolorofaNeRFbyselectingamasked
maskedregionsarematchedwithmaskedregionsofsampleddatset
region and change its color, propogating the change into
viewsviaDINOcorrespondences.Thecolor/textureisthentrans-
3D. ICE-NeRF [16] finetunes the NeRF with the desired
ferredtothosematchingregions,andthesplatiseditedwiththis
color edits, introducing techniques to preserve multiview
updateddataset.
consistencyandavoidunwantedcolorchanges.
Ourmethodsupportsdifferenttypesof3Dmodels,and
2.4.ConcurrentWorkinGaussianSplatEditing
weprimarilydemonstrateitontopofGaussianSplatting[12]
Recently many methods have emerged that show editing duetoitsfavorablespeed.Wealsoimplementourmethodon
capbilitiesonGaussianSplats. OnesuchexampleisGaus- aregularNeRFframework[28]fortimecomparison. There
sianEditor: Editing3DGaussiansDelicatelywithTextIn- aretwomaininterfaces,oneformanualtexture/colorediting
structions[7]. Thispipelinefeedstheuserpromptandscene and anotherfor automaticallytransferring theseattributes
3fromanexampleimageasshowninFigure2. Theprocess teacher. Thestudentmodellearnsfrominputdatausingstan-
differsonlyforcreatingtheeditimage,butusesthesame dardmethods,whiletheteachermodelupdatesitsweights
segmentation,partmatching,andtexture/colorlossesacross through an exponential moving average of the student’s
both. Aftermakingchangestotheeditview,orchoosingthe weights,ensuringstableupdates. Thisprocessencourages
conditionalimage,thealgorithmisrunonanumberofsam- thestudenttolearngeneralizableandrobustfeaturesbypre-
pledimageswherethestyleistransferredtotheserandomly dicting the more stable teacher outputs. DINO facilitates
sampledviews. Colorisnaturallymultiviewconsistentsince effectiveViTtrainingwithoutlabeleddata,leadingtomodels
onlythehueischanged,andtheunderlyinggrayscaleispre- thatbetterfocusonrelevantimageparts. Themethod’sabil-
served,sostandardL1/SSIMlossisusedtopushthecolor itytoidentifypixel-wiseimagecorrespondencesisfurther
updates. Since this is not the case for transferred texture demonstratedin [29].
updates,weemploytheNearestNeighborFeatureMatching
(NNFM),originallyproposedinARF[30],tomakethetex-
turechange3Dconsistent. Texturechangesaredonewith 3.1.4 TextureReformer
thisNNFMlossinafirstroundofiterations,andthencolor
is changed with L1/SSIM losses in a second round, since Texture Reformer [27] introduces View-Specific Texture
wefindthatmorevividcoloristransferredviastandardloss Reformation(VSTR)fortransferringtexturesbetweenim-
functionsthanNNFM. ageregions. Byutilizingsourceandtargetsemanticmasks
alongwithVGGfeatureextraction[22],itoverlaystextures
3.1.Preliminaries fromoneareatoanother,adjustingtothenewshape’scon-
tours. Thetechniqueemployspatchgridsandconvolution
3.1.1 GaussianSplatting
fortextureapplication,withstatisticalrefinementsensuring
Gaussian Splats [12] is a recent 3D scene representation realisticintegrationwithinthetargetedmaskedregions.
techniquethatallowforfastertrainingandrendering. The
sceneisrepresentedasacollectionof3DGaussians,which
aredefinedbyposition, covariance, opacity, andcolor. A 3.1.5 NNFMLoss
givenviewisrenderedfromadifferentiablerasterizer,which
Artistic Radiance Fields (ARF) [30] offers a method for
returns any given 2D view of this set of gaussians given
infusing3DNeRFsceneswithstyleelementsfrom2Dim-
standardNeRF-styleviewingparameters. Sincetheraster-
ages. Byprocessing2Dsceneviewsalongsidestyleimages
izer is differentiable, edits to the returned 2D image are
throughaVGG-16encoder,itappliesanovelNNFMloss
backpropagated, andmaketheappropriatechangestothe
to match local features between the two, diverging from
underlying gaussian representation. This method has the
traditionalGrammatrixlossesthatblendstyledetailsglob-
benefitofquicktrainingtimeandproducingmorerealistic
ally. Thislocalmatchingtechniqueensuresthepreservation
texturesthanNeRFsinmanycases. Webaseourmethodoff
of texture specifics, marking a notable advancement over
ofGaussianSplats,butthetechniqueworksforNeRFsas
previousapproaches.
well.
3.2.2DEditing
3.1.2 SAM
There are two options for editing. Firstly, we can take a
The Segment Anything Model (SAM) [13] is a zero shot
differentconditionalimage(s),andcopystylesfromallparts
imagesegmentationmodel. ItconsistsofaViTencoderand
ontothetargetobjectsviews. Inthisapproach,weusethe
maskdecoderthatproducesthemaskofeachinstance. This
texture-reformermoduletobringallsourcetexturesontoa
decoderisconditionedoneitherspecificpoints, abox, or
squarearray,sotheycanbecroppedtothesizeofthetarget
texttoproducevariousmasks. Forthepurposeofseparating
masksasnecessary. Wealsostorewhichcolorscorrespond
all parts of an object or scene, prompting with a grid of
with which mask ids. Secondly, there is manual editing,
points is most effective. This produces distinct masked
wherewestartwithanyarbitraryviewofthetargetobject,
regions,whichcanbeusedaseditingregionstoapplynew
andassigndifferentstylestodifferentregions. Inbothcases,
colorsandtextures.
weseektogenerateamappingofmaskidtocolor/textureto
findandcopythesestylestotheappropriateregionsinthe
nextsteps.Whenediting,wecanspecifywhetherwewantto
3.1.3 DINO
copyonlythecolororthetextureaswell.Thiswilldetermine
Self-Distillation with No Labels (DINO) [4] is a self- whetherweusetheTextureReformermoduletoextracttex-
supervised technique for training Vision Transformers tures,andwhetherweuseNNFMlossorL1lossaloneinthe
(ViTs) [6] where a single ViT acts as both student and downstreamstyleapplyingsteps,asopposedtosequentially.
43.3.Segmentation 3.6.ApplyingEdits
The Segment Anything Model (SAM) [13] is an encoder- 3.6.1 ApplyingColor
decodermodelthatcanbepromptedwithseveralgridpoints
Applyingacolorchangetoaregionofaviewimageisdone
tomakemasksofmostidentifiablepartsofanimage. We
intheHSVrepresentation.Inthisrepresentationtheimageis
useSAMtosegmentboththeeditimageandsampledviews
splitupintothethreechannelsofHue,Saturation,andValue,
intotheircomponentparts,sinceitisthestateoftheartat
ratherthanthestandardRGB.Thehuecontrolswhatcolor
thistaskandrunsfairlyquickly. Sinceweprovideanoption
isexpressed,thesaturationcontrolshowstrongthecoloris,
tomanuallyspecifywhichmaskstoedit,inourmaskpro-
andthevaluecontrolshowlightordarkitis. Thegrayscale
cessingstep,weallowuserstospecifyalimitofNmasksfor
ofanimage,whichcontainsthetextureoftheoriginalview
simplicity. WechoosethelargestN-1masksandgroupthe
is the value. Therefore, to edit the color in a given target
restoftheimageintotheNthmask. Thisbasicallyenables
region,wecopyovertheaveragehueandsaturationvalues
theusertoseparatetheeditingviewintoanynumberofparts
ofthesourceregion,whileleavingthevaluealone. Ifthe
tohavecontroloverfinegrainedfeatures. Wefirstsegment
user wants to brighten or darken a view overall, that can
theeditingimageandstorethosemasks,andsegmenteach
also be achieved by shifting the value field by a specified
datasetviewasweiterateoverit.
constant.
Oncethiseditismadeontheviews,weusethisasthe
3.4.DINOMaskMatching
data for training the edited 3D model. The loss function
WeuseDINOfeaturestofindwhichisthebesteditingimage usedisstandardStructuralSimilarityIndex(SSIM)andL1
region to copy style from for each region of each of the interpolationusedtotrainGaussianSplatting:
sampleddatasetviews. ExtractedDINOfeatureshavebeen
showntofindcorrespondingpixelsbetweentwoimages [4]. L GS =λL 1+(1−λ)L SSIM (2)
Weuseasimilarfeatureextractiontechnique,butcreatea
customheuristictomeasurethedistancebetweentwomasks 3.6.2 ApplyingTexture
in the DINO feature space. First, we extract the DINO
Fortexture,weeitherhavemanuallyspecifiedatexturefrom
featurevectorforeachofthemasksintheeditingimage,and
apatternimage,orweautomaticallyextractedtexturefrom
storethisinformationasitdoesnotchange. Wheniterating
amatchingregionandexpandeditasapatternimagewith
overagivendatasetimage,weextractDINOfeaturesafter
texture reformer. In either case we can crop this image
segmentation, and find the best matching region with the
sizedtexturetofitthemaskregionandaddit. Thetexture
followingheuristic:
willhavethesamepatterncroppedtodifferentviewpoints,
1 (cid:88) andsoisnot3Dconsistent. However,weagaintraina3D
M =argmin (D(i)−D(E))2 (1)
P N modelusingthisdataandtheNNFMloss,andoverseveral
E E
i∈P iterations thiswill blendthe imageto be so. We find that
usingNNFMalonecausesdegradationinimagequalityand
To find the best match M for a given part P of a sampled
artifacts,andsoweregularizeitwiththeoriginalGaussian
dataset view, we find the editing image part E which is
Splattrainingloss:
closestintheDINOfeaturespaceD.
L =L +αL (3)
3.5.TextureReformer texture NNFM GS
Weuse thetexture reformer [27] moduleto copytextures Thistexturetransferringoftenimprintsthecorrectpattern
fromtheeditingimage. Sincewewillbeobtainingmasked ontheGaussianSplat,butleadstocolorappearingwashed
regions (see next section), we can use that for our source out. Thus, wefollowupwithsomeiterationsofthecolor
semantic map, and the editing image itself for our source applyingstage,copyingovertheaveragehueandsaturation
texture. Forourtargetsemanticmask,wecanusetheentire ofthetextureimage.
blank image of the same size. When applying textures to
variousdifferentregionsofthedifferentdatasetviews,we 4.Results
cansimplycropthisfullsizedtexturetoshape. Thereason
4.1.ExperimentDetails
wedothis,ratherthanmappingthetexturetoeachindividual
semanticmaskforeachview,isbecausewefindempirically WediscoveredinourablationstudyinFigure3,thatsam-
itdoesnotmatter,andtimeissavedbyjustdoingthisonce. plingaround20%oftheimagesinthedatasetforeditingis
Running Texture Reformer per view does not lead to any sufficientforagoodqualityresult. Thecoloreditingstageis
morenaturallyviewconsistentresultswithoutNNFMloss. runforaround2000iterations,andthetextureeditingtakes
ThisisdoneaftertheeditimageissegmentedinFigure2. 3000iterationstofullystylizetheGaussianSplatlikethe
5editinginputs. FortheL1+SSIMportionoftheloss,weuse thexyplane,extendingupwardstotheship’sstart,withthe
theGaussianSplatimplementationdefaultinterpolation. For prompt’dunesofsand’. Thechair’sboxisabovetheseat
textureloss,wefindthatadding50%oftheoriginallossas cushion,includingthearmrests,using’awoodenchair’. The
aregularizertotheNNFMlossworksbest. mic’sstandisboxedwith’awoodenstand’. Forthehotdog,
theboxspansthexyplane,extendingverticallytothehot-
dogs’start.
InVox-E:Weshouldspecifyourpromptaswhatwewant
the final image to be, as this is the input to the diffusion
guidance. Fortheshipweuse‘ashipinsand’. Forthechair
weuse‘achairwithawoodenback’. Forthemicweuse
‘amicrophonewithawoodenstand’,andforthehotdogwe
use‘ahotdogonabluegraniteplate’.
Figure3. Comparingdifferentdatasetsamplingratesforturning
theroadtoariver.Sampling5%or10%ofimagesfromadataset
toeditresultsinnumerousartifactsandotherdegradations,and
qualitypeaksataround20%sampling.
4.2.1 Analysis
Vox-E allows users to use text prompts for editing object
voxelgridsbuthasnotablelimitations.Thetextpromptcan’t
focuseditsonspecificareas,leadingtounwantedchanges,
suchasunnecessarycoloringinthechairandmicrophone
examplesasinFigure5. Italsostruggleswithtexturerep-
resentation,producingrough,pixelatedtexturesthatdon’t
Figure4.Addingtexturetothegardentablefrommip-NeRF360
matchtheintendededits, asseenintheplateandshipex-
(left).Usingpaintingstotexturethetable(right).
amples. Additionally,whileVox-Ecanchangeshapes,this
sometimesresultsinunintendedalterations.
4.2.TextureEditing
BlendedNeRFcanproducehigh-qualityvisualsbutsuf-
fersfromunintendedartifactsandshapedistortionsduetoits
editregionbeingbox-shaped,makingpreciseeditsdifficult
inintertwinedareas. Thisissueisevidentinexampleslike
the ship, where sand spills out improperly, the chair with
misplacedwoodenpanels, amicwithfuzzyartifacts, and
ahotdogplateturnedsquare. Unlikebox-basededits,our
mask-basedapproachallowsformorepreciseregionmodifi-
cations. Additionally,BlendedNeRFstruggleswithtexture
definition, failing to produce detailed contours in sand or
realistic wood grain, as highlighted in the ship and wood
examples.
Figure5.Comparisonofourmethodinlocaltextureeditingofours
andbaselines.
In our method: For the ship, we selected the mask that
correspondstothewaterandindicatethatthesandtexture
shouldbeappliedthere. Forthechair, weselecttheback
and armrests. For the mic we select the stand and for the
hotdogweselecttheplate.
In BlendedNeRF: To edit, a 3D box region and a corre- Figure6. Makinganicehelmetandagoldfoilcoffeecupfrom
spondingpromptarerequired. Fortheship,theboxcovers RefNeRF.
6notfinelydetailorrecolorsmallpartsofimages,exemplified
byunchangedcolorsintheinstruments8anddifficultyin
differentiatingtheshipfromitssurroundingwater. Addition-
ally,tryingtospecifyexactRGBcolorsthroughtextoften
leadstodiscrepanciesbetweenintendedandactualcolors.
CLIP-NeRFproducesattractiveresultsbutcandeviate
frompreciseprompts. Forinstance,arequestforashipin
purple water resulted in both the ship and a bucket being
Figure7.Applyingasnowandicetexturetothescene(left),and coloredpurpleinsteadofjustthewater.Thisindicatesachal-
turningthestumpscenetofall(right) lengewiththeprecisionoftext-basededitingcomparedto
directmaskandcoloradjustments. Otherexamplesinclude
4.3.ColorEditing
unintendedadditionslikeagoldpatternonachairmeantto
be simple, minimal changes to drum colors, and an unex-
pectedlycoloredpotonaplant,showcasingthelimitations
inaccuratelyreflectinguserintentionsthroughtext/vector
embedding.
4.4.InheritedLimitations
We inherit a few limitations from pretrained components
utilizedinourmethod. SAM,whichperformssegmentation
onselectedviewscansometimesfailtogeneratefinegrained
masksfromcertainangles,lumpingtogethertwopartsofan
object. Whenthishappens,aneditthatwouldhavenormally
been constrained to one area in that particular view can
sometimesbleedintootherareas. Thisisrareinmostscenes,
butcanoccurincomplicatedscenes,orobjectsectionswith
ill-definedboundaries. Also,usingSAMtoselectmasked
regionsmeansthatourmethodcannotperformeditstothe
3Dgeometryoftheobject.
Figure8. Showinglocalandcorrespondancebasedcolorediting
acrossoursandbaselines. The NNFMloss function is great at copying texture and
overallstylefromasourceareatodestinationarea,butmakes
In our method: The purple color is applied to the water
theresultunreflective. ThisisseeninFigure6,wherethe
regionoftheimagefortheship. Forthechair,drums,and
originalsurfaceofobjectswasreflective,andapplyinganew
plant,appropriatecolorregionsareautomaticallyextracted
textureunintentionallyoverwrotethoseeffects. Likewise,if
withmaskmatchingandappliedontothedatasetviews.
theeditimage’stexturecontainsanysuchlightscattering
InD.F.Fields: Thismethodusesatextphraseforthenew
effects,thesearenotcarriedoverontotheGaussianSplat.
color and a filter phrase for object selection, where sim-
ply naming the object works best due to D.F. Fields’ dif- 4.5.Data
ficulty in selecting object subparts. Adding ’background’
We use publically available synthetic datasets from
tothefilterphraseimprovesperformance. Colorprompts
NeRF[18]andRefNeRF[24],aswellasrealscenesfrom
arespecific,like’purplewater’fortheship,’browndrums’,
Mip-NeRF[2]andNeRDS360[11]. Weuseinternetimages
’golden plant’, and ’blue and gold chair’. Excessively de-
undercc-licenseforthestyleconditioning.
tailedpromptstendtoreduceeffectiveness.
InCLIP-NeRF:InCLIP-NeRF,wespecifyapromptasa 4.6.ComputationTime
sentenceofwhatwewanttoseeintheresultsentence. For
Wefindthatourmethodperformsmuchfasterwhenimple-
theship,thisis‘ashipinpurplewater’. Forthechair,itis‘a
mented on Gaussian Splats, showing that color and style
chairwithbluecushionsandagoldframe’. Forthedrumsis
losses can be applied faster on this representation. In our
‘browndrumsetwithbronzecymbols’,andfortheficusitis
experiments,runningourmethodontopofstandardNeRFs
‘plantwithfall-coloredleaves’.
tookmoreiterationstotransferstyle,andeachiterationalso
ranslower,showingthatitiseasiertochangecolorandtex-
4.3.1 Analysis
tureonaGaussianSplat. Weincludetimingsfortheother
DFFieldseffectivelychangescolorsacrossbroadareasbut baselines we tested in Table 3, along with the timing for
struggleswithprecisioninsmallerregions. Thismodelcan- SINEfromthatpaper.
7users. Ourplatedesignalsowonmajoritypreference,with
AvgTime(Mins) BlendedNeRF’sversionturningsquareandVox-Eerasing
Vox-E 52 condiments.Similarly,ourmethodwasthetopchoiceforthe
DFFields 33 mic,asBlendedNeRF’seditsintroducedunwantedartifacts.
CLIP-NeRF 35
BlendedNeRF 118 4.7.2 Color
SINE 720
HerewetestagainstDistilledFeatureFieldsandCLIP-NeRF,
Ours(NeRF) 40
withthreeglobalstyletransferexamplesfromconditional
Ours(GS) 21
images,andonelocalcoloreditingexampleasinFigure8.
Table1.Averageruntimesweobservedforobtainingqualityresults Fortheglobalcolortransfer,weexplaintheconceptofcor-
foreachmethodonasingleNVIDIAA40GPU. respondenceinsimpleEnglish,byaskingtheusertoselect
theresultwhichtakesonthecolorschemeoftheeditimage
4.7.UserStudy
appliedontotheoriginal. Forthelocalcolortransferonthe
shipexample,wementionthatthegoalistoturnthewater
Intheuserstudy,weseektounderstandhowusersperceive
intheimagepurple.
ourmethodascomparedwithleadingbaselines. Sincethe
textpromptswechoseforeachofthesebaselinesdetailed Object Ours DFFields CLIP-NeRF
inSections4.2and4.3areafaithfulrepresentationofthe
Ship 84.2% 7.9% 7.9%
edit we intend to express with the conditional image we
Chair 73.7% 5.3% 21.1%
useforourmethod,wecancompareagainstthesebaselines
Drums 73.7% 10.5% 15.8%
accurately. Wesolicitfeedbackontheuserpreferencesfrom
Plant 65.8% 2.6% 31.6%
38people,andaskedabouttheirexpertisewithgenerative
models. TheratingswererequestedviaaGoogleForm. Ten Table 3. Percent of users who preferred each method for color
werefamiliarwithgenerativecomputervisionandtwenty- editing.
eightwerenot.
In color editing, our method again won over 60% of user
preferenceineachscenario. Fortheship,ourrecolorwas
4.7.1 Texture favored as Distilled Feature Fields partially recolored the
trayborderandCLIP-NeRFmistakenlycoloredtheship,not
AsdisplayedinFigure5inthepaper,wetestonthebaselines
thewater.Ourchairwaspreferredforitsaccurategoldframe
ofVox-EandBlendedNeRF.Foreachofthetextureediting
andbluecushion,matchingthethrone,thoughCLIP-NeRF
instructions,weasktheusertochoosetheresultthatbest
alsoattracted21.1%ofuserswithitsintriguing,albeitun-
transfers the texture shown onto the specified area of the
intended,result. BothDFFieldsandCLIP-NeRFstruggled
image,andspecifythefollowinginstructions:
withcoloringthedrumscorrectly,leadingtolowpreference.
• Turningthewaterintosand
Fortheplant,DFFieldsfailedtoalteritsgreencolor,while
• Turningthechairbackandframeintowood
CLIP-NeRF’sreddishfallcolorscaughtsomeinterest,but
• Turningtheplatebluegranite
overall,ourmethodwasseenasmostaccuratelyreflecting
• Turnthemicstandwood
theintendededits.
Object Ours BlendedNeRF Vox-E
5.Conclusion
Ship 86.8% 7.9% 5.3%
Chair 63.2% 26.3% 10.5% Inthiswork,wehaveintroducedarobustandflexiblemethod
foreditingcolorandtextureof3Dimagesandscenes. We
Mic 68.4% 13.2% 18.4%
provideinterfacestocopystylefromaneditimageormanu-
Plate 73.7% 5.3% 21.1%
allyspecifychanges,enablingcreativeappearanceediting
for a variety of applications. Our key innovation, DINO-
Table2. Percentofuserswhopreferredeachmethodfortexture
based mask matching, runs quickly and contains edits to
editing.
discreteregions,leadingtohigherqualitythanothermeth-
ods. Future work could explore how to make 3D consis-
Inallcases,ourmethodwasfavoredbymostusers. Forthe
tentshapechangestothesediscreteregionsinadditionto
shipexample, itreceivedhighpreferenceduetoBlended-
color and texture, without compromising on resulting 3D
NeRF’ssandspillingoutofboundsandVox-E’sunrepresen-
scene quality like most other current methods do. Over-
tativegrainytexture. Inthechairscenario,63.2%preferred all, we showcase our method’s unique input expressivity
ourmethod,notingitprovidedareasonabletexture,whereas and resulting 3D model quality on a variety of objects
BlendedNeRF was a close second. Vox-E’s inaccuracies, and scenes, proving it is well suited for creative applica-
suchasmiscoloringpartsofthemic,werenotedbyattentive tions.
8References [15] Dishani Lahiri, Neeraj Panse, and Moneish Kumar. S2rf:
Semanticallystylizedradiancefields,2023. 3
[1] Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan,
[16] Jae-HyeokLeeandDae-ShikKim. Ice-nerf:Interactivecolor
ZesongYang, HujunBao, GuofengZhang, andZhaopeng
editingofnerfsviadecomposition-awareweightoptimization.
Cui. Sine: Semantic-drivenimage-basednerfeditingwith
InProceedingsoftheIEEE/CVFInternationalConference
prior-guidededitingfield.InTheIEEE/CVFComputerVision
onComputerVision(ICCV),pages3491–3501,2023. 3
andPatternRecognitionConference(CVPR),2023. 3
[17] Boyi Li, Kilian Q Weinberger, Serge Belongie, Vladlen
[2] JonathanT.Barron, BenMildenhall, DorVerbin, PratulP.
Koltun, and Rene Ranftl. Language-driven semantic seg-
Srinivasan,andPeterHedman. Mip-nerf360: Unbounded
mentation. InInternationalConferenceonLearningRepre-
anti-aliasedneuralradiancefields. CVPR,2022. 2,7
sentations,2022. 3
[3] TimBrooks,AleksanderHolynski,andAlexeiA.Efros. In- [18] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,
structpix2pix:Learningtofollowimageeditinginstructions. JonathanT.Barron,RaviRamamoorthi,andRenNg. Nerf:
InCVPR,2023. 2 Representingscenesasneuralradiancefieldsforviewsynthe-
[4] MathildeCaron,HugoTouvron,IshanMisra,Herve´Je´gou, sis. InECCV,2020. 2,7
JulienMairal,PiotrBojanowski,andArmandJoulin. Emerg- [19] BenPoole,AjayJain,JonathanT.Barron,andBenMildenhall.
ingpropertiesinself-supervisedvisiontransformers. InPro- Dreamfusion:Text-to-3dusing2ddiffusion. arXiv,2022. 2
ceedingsoftheInternationalConferenceonComputerVision [20] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
(ICCV),2021. 2,3,4,5 Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
[5] YiwenChen,ZilongChen,ChiZhang,FengWang,Xiaofeng Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen
Yang,YikaiWang,ZhongangCai,LeiYang,HuapingLiu, Krueger, and Ilya Sutskever. Learning transferable visual
andGuoshengLin. Gaussianeditor: Swiftandcontrollable modelsfromnaturallanguagesupervision,2021. 2
3deditingwithgaussiansplatting,2023. 3 [21] Etai Sella, Gal Fiebelman, Peter Hedman, and Hadar
[6] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Averbuch-Elor. Vox-e: Text-guidedvoxeleditingof3dob-
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, jects,2023. 2
MostafaDehghani,MatthiasMinderer,GeorgHeigold,Syl- [22] Karen Simonyan and Andrew Zisserman. Very deep con-
vainGelly,JakobUszkoreit,andNeilHoulsby. Animageis volutional networks for large-scale image recognition. In
worth16x16words: Transformersforimagerecognitionat InternationalConferenceonLearningRepresentations,2015.
scale. ICLR,2021. 3,4 4
[7] JieminFang,JunjieWang,XiaopengZhang,LingxiXie,and [23] CyrusVachhaandAyaanHaque. Instruct-gs2gs:Editing3d
QiTian.Gaussianeditor:Editing3dgaussiansdelicatelywith gaussiansplatswithinstructions,2024. 3
textinstructions. InCVPR,2024. 3 [24] DorVerbin,PeterHedman,BenMildenhall,ToddZickler,
[8] BingchenGong,YuehaoWang,XiaoguangHan,andQiDou. Jonathan T. Barron, and Pratul P. Srinivasan. Ref-NeRF:
Recolornerf:Layerdecomposedradiancefieldsforefficient Structuredview-dependentappearanceforneuralradiance
coloreditingof3dscenes. arXivpreprintarXiv:2301.07958, fields. CVPR,2022. 2,7
2023. 3 [25] BinglunWang,NiladriShekharDutt,andNiloyJMitra. Pro-
[9] OriGordon,OmriAvrahami,andDaniLischinski. Blended- teusnerf:Fastlightweightnerfeditingusing3d-awareimage
nerf: Zero-shotobjectgenerationandblendinginexisting context. arXivpreprintarXiv:2310.09965,2023. 3
neuralradiancefields.arXivpreprintarXiv:2306.12760,2023. [26] CanWang,MengleiChai,MingmingHe,DongdongChen,
2 andJingLiao.Clip-nerf:Text-and-imagedrivenmanipulation
[10] Ayaan Haque, Matthew Tancik, Alexei Efros, Aleksander ofneuralradiancefields. arXivpreprintarXiv:2112.05139,
Holynski,andAngjooKanazawa. Instruct-nerf2nerf:Editing 2021. 3
3dsceneswithinstructions. InProceedingsoftheIEEE/CVF [27] ZhizhongWang, LeiZhao, HaiboChen, AilinLi, Zhiwen
InternationalConferenceonComputerVision,2023. 2 Zuo,WeiXing,andDongmingLu.Texturereformer:towards
[11] MuhammadZubairIrshad,SergeyZakharov,KatherineLiu, fastanduniversalinteractivetexturetransfer. InProceedings
VitorGuizilini,ThomasKollar,AdrienGaidon,ZsoltKira, oftheAAAIConferenceonArtificialIntelligence,pages2624–
andRaresAmbrus. Neo360: Neuralfieldsforsparseview 2632,2022. 2,4,5
synthesisofoutdoorscenes. 2023. 7 [28] LinYen-Chen. Nerf-pytorch. https://github.com/
[12] BernhardKerbl,GeorgiosKopanas,ThomasLeimku¨hler,and yenchenlin/nerf-pytorch/,2020. 3
GeorgeDrettakis.3dgaussiansplattingforreal-timeradiance [29] JunyiZhang,CharlesHerrmann,JunhwaHur,LuisaPolania
fieldrendering. ACMTransactionsonGraphics,42(4),2023. Cabrera,VarunJampani,DeqingSun,andMing-HsuanYang.
2,3,4 Ataleoftwofeatures:Stablediffusioncomplementsdinofor
[13] AlexanderKirillov,EricMintun,NikhilaRavi,HanziMao, zero-shotsemanticcorrespondence. 2023. 4
ChloeRolland,LauraGustafson,TeteXiao,SpencerWhite- [30] KaiZhang,NickKolkin,SaiBi,FujunLuan,ZexiangXu,Eli
head,AlexanderC.Berg,Wan-YenLo,PiotrDolla´r,andRoss Shechtman,andNoahSnavely. Arf:Artisticradiancefields,
Girshick. Segmentanything,2023. 2,4,5 2022. 2,4
[31] JingyuZhuang,DiKang,Yan-PeiCao,GuanbinLi,Liang
[14] SosukeKobayashi,EiichiMatsumoto,andVincentSitzmann.
Lin,andYingShan. Tip-editor:Anaccurate3deditorfollow-
Decomposingnerfforeditingviafeaturefielddistillation. In
ingbothtext-promptsandimage-prompts,2024. 3
AdvancesinNeuralInformationProcessingSystems,2022. 3
96.Appendix 6.2.AdditionalEditingResults
6.1.UserWorkflow
Figure10. Ourmethodisabletoperformboundedandaccurate
colorchangesacrossviews,evenincasesofaclutteredbackground
withnumerousobjectmasks.
Figure11.Toastingthetoastwithoutaffectingtoaster.
Figure9.Userinitiallychoosesanimagetoautomaticallyextract
correspondencesfrom,ormakesanedittoanexistingview.
10Figure12. Turningthesidewalklightbluewithoutaffectingthe
street.
Figure13.Turningjusttheyellowcarred.
Figure 14. One limitation of our method is that it struggles to
accuratelycopytexturesfromlayeredobjectslikeliquidbehinda
glass.
11