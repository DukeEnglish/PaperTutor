MAGPIE: Alignment Data Synthesis from Scratch by
Prompting Aligned LLMs with Nothing
ZhangchenXu♠ FengqingJiang♠ LuyaoNiu♠ YuntianDeng♢
RadhaPoovendran♠ YejinChoi♠♢ BillYuchenLin♢
♠UniversityofWashington ♢AllenInstituteforAI
https://magpie-align.github.io/
https://hf.co/magpie-align
Abstract
High-qualityinstructiondataiscriticalforaligninglargelanguagemodels(LLMs).
Althoughsomemodels,suchasLlama-3-Instruct,haveopenweights,theiralign-
mentdataremainprivate,whichhindersthedemocratizationofAI.Highhuman
laborcostsandalimited,predefinedscopeforpromptingpreventexistingopen-
source data creation methods from scaling effectively, potentially limiting the
diversity and quality of public alignment datasets. Is it possible to synthesize
high-quality instruction data at scale by extracting it directly from an aligned
LLM?Wepresentaself-synthesismethodforgeneratinglarge-scalealignmentdata
namedMAGPIE. OurkeyobservationisthatalignedLLMslikeLlama-3-Instruct
can generate a user query when we input only the left-side templates up to the
positionreservedforusermessages,thankstotheirauto-regressivenature. Weuse
thismethodtopromptLlama-3-Instructandgenerate4millioninstructionsalong
withtheircorrespondingresponses. Weperformacomprehensiveanalysisofthe
extracteddataandselect300Khigh-qualityinstances. TocompareMAGPIEdata
withotherpublicinstructiondatasets(e.g., ShareGPT,WildChat, Evol-Instruct,
UltraChat,OpenHermes,Tulu-V2-Mix),wefine-tuneLlama-3-8B-Basewitheach
datasetandevaluatetheperformanceofthefine-tunedmodels. Ourresultsindicate
thatinsometasks,modelsfine-tunedwith MAGPIE performcomparablytothe
officialLlama-3-8B-Instruct,despitethelatterbeingenhancedwith10milliondata
points through supervised fine-tuning (SFT) and subsequent feedback learning.
WealsoshowthatusingMAGPIEsolelyforSFTcansurpasstheperformanceof
previouspublicdatasetsutilizedforbothSFTandpreferenceoptimization,such
asdirectpreferenceoptimizationwithUltraFeedback. Thisadvantageisevident
onalignmentbenchmarkssuchasAlpacaEval,ArenaHard,andWildBench,and
importantly,itisachievedwithoutcompromisingperformanceonreasoningtasks
likeMMLU-Redux,despitethealignmenttax.
1 Introduction
Largelanguagemodels(LLMs)suchasGPT-4[1]andLlama-3[40]havebecomeintegraltoAI
applicationsduetotheirexceptionalperformanceonawidearrayoftasksbyfollowinginstructions.
ThesuccessofLLMsisheavilyreliantonthedatausedforinstructionfine-tuning,whichequipsthem
tohandleadiverserangeoftasks,includingthosenotencounteredduringtraining. Theeffectiveness
ofthisinstructiontuningdependscruciallyonaccesstohigh-qualityinstructiondatasets. However,
thealignmentdatasetsusedforfine-tuningmodelslikeLlama-3-Instructaretypicallyprivate,even
whenthemodelweightsareopen,whichimpedesthedemocratizationofAIandlimitsscientific
researchforunderstandingandenhancingLLMalignment.
To address the challenges in constructing such datasets, researchers have developed two main
approaches. Thefirsttypeofmethodinvolveshumanefforttogenerateandcurateinstructiondata
4202
nuJ
21
]LC.sc[
1v46480.6042:viXra30% AlpacaEval 2
Instruction
Step 1 < <| |s et na dr _t h_ eh ae da ed re _r i_ di |d >|>user W ush ea tt
o
m ba ut ie ldr i aa ls
n
es sh to ?uld I 25%25.08
22.92 22.66
(Length
S
FC To n +t Dro Pl O)
Step 2 <|start_header_id|>user Response 20% SFT + RLHF
<|end_header_id|> Building a nest! That’s a 18.36 What materials should I wonderful project! …… SFT Only
use to build a nest? 15% 14.62
Instruction: What materials
<|start_header_id|> LLM should I use to build a nest? 10.90
assistant<|end_header_id|> Response: Building a nest! 10% 9.949.919.73
MAGPIEThat’s a wonderful project!
……
“Other birds collect twigs for their nests. Magpies acquire jewels for theirs.” Filters SFT Mag5 Llp% i ae m- aP -r 3-o Ins Mtr au gct pi Ulte r- aA Fir eedbac Wk ild EvCh ol a It ns Otr pu ec nt Her Tm ue ls u V2 M Six hareGPT
Figure1: Thisfigureillustratestheprocessofself-synthesizinginstructiondatafromalignedLLMs
(e.g.,Llama-3-8B-Instruct)tocreateahigh-qualityinstructiondataset. InStep1,weinputonlythe
pre-querytemplateintothealignedLLMandgenerateaninstructionalongwithitsresponseusing
auto-regressivegeneration. InStep2,weuseacombinationofapost-querytemplateandanother
pre-querytemplatetowraptheinstructionfromStep1,promptingtheLLMtogeneratethequery
forthesecondturn. Thiscompletestheconstructionoftheinstructiondataset. MAGPIEefficiently
generatesdiverseandhigh-qualityinstructiondata. Ourexperimentalresultsshowthat MAGPIE
outperformsotherpublicdatasetsforaligningLlama-3-8B-base.
[14,26,64,65,66],whichisbothtime-consumingandlabor-intensive[37]. Incontrast,thesecond
typeofmethodusesLLMstoproducesyntheticinstructions[16,31,46,47,53,55,58,59].Although
thesemethodsreducehumaneffort,itssuccessheavilydependsonpromptengineeringandthecareful
selectionofinitialseedquestions. Thediversityofsyntheticdatatendstodecreaseasthedatasetsize
grows. Despiteongoingefforts,thescalablecreationofhigh-qualityanddiverseinstructiondatasets
continuestobeachallengingproblem.
Isitpossibletosynthesizehigh-qualityinstructionsatscalebydirectlyextractingdatafromadvanced
alignedLLMsthemselves? AtypicalinputtoanalignedLLMcontainsthreekeycomponents:thepre-
querytemplate,thequery,andthepost-querytemplate. Forinstance,aninputtoLlama-2-chatcould
be“[INST]Hi! [/INST]”,where[INST]isthepre-querytemplateand[/INST]isthepost-query
template. ThesetemplatesarepredefinedbythecreatorsofthealignedLLMstoensurethecorrect
promptingofthemodels. Weobservethatwhenweonlyinputthepre-querytemplatetoaligned
LLMssuchasLlama-3-Instruct,theyself-synthesizeauserqueryduetotheirauto-regressivenature.
Ourpreliminaryexperimentsindicatethattheserandomuserqueriesareofhighqualityandgreat
diversity,suggestingthattheabilitieslearnedduringthealignmentprocessareeffectivelyutilized.
Basedonthesefindings,wedevelopedaself-synthesismethodtoconstructhigh-qualityinstruction
datasetsatscale,namedMAGPIE(asillustratedinFigure1). Unlikeexistingmethods,ourapproach
doesnotrely onpromptengineeringor seedquestions. Instead, itdirectlyconstructsinstruction
databypromptingalignedLLMswithapre-querytemplateforsamplinginstructions. Weapplied
thismethodtotheLlama-3-8B-InstructandLlama-3-70B-Instructmodels,creatingtwoinstruction
datasets: MAGPIE-AirandMAGPIE-Pro,respectively.
OurMAGPIE-AirandMAGPIE-Prodatasetswerecreatedusing206and614GPUhours,respectively,
without requiring any human intervention or API access to production LLMs like GPT-4. Addi-
tionally,wegeneratedtwomulti-turninstructiondatasets,MAGPIE-Air-MTandMAGPIE-Pro-MT,
which contain sequences of multi-turn instructions and responses. The statistics and advantages
of our instruction datasets compared to existing ones are summarized in Table 1. We perform a
comprehensiveanalysisofthegenerateddata,allowingpractitionerstofilterandselectdatainstances
fromthesedatasetsforfine-tuningaccordingtotheirparticularneeds.
TocompareMAGPIEdatawithotherpublicinstructiondatasets(e.g.,ShareGPT[10],WildChat[64],
EvolInstruct[58], UltraChat[16], OpenHermes[49], TuluV2Mix[24])andvariouspreference
tuning strategies with UltraFeedback [13], we fine-tune the Llama-3-8B-Base model with each
datasetandassesstheperformanceoftheresultantmodelsonLLMalignmentbenchmarkssuchas
AlpacaEval2[33],Arena-Hard[32],andWildBench[34]. Ourresultsshowthatmodelsfine-tuned
withMAGPIEachievesuperiorperformance,evensurpassingtheofficialLlama-3-8B-Instructmodel
onAlpacaEval,whichwasfine-tunedwithover10milliondatapointsforsupervisedfine-tuning
(SFT)andfollow-upfeedbacklearning. Notonlydoes MAGPIE excelinSFTalonecomparedto
priorpublicdatasetsthatincorporatebothSFTandpreferenceoptimization(e.g.,directpreference
2
etaR
niW
lortnoC
htgneLTable1:StatisticsofinstructiondatasetsgeneratedbyMAGPIEcomparedtootherinstructiondatasets.
Tokensarecountedusingthetiktokenlibrary[42].
Instruction Human Response
DatasetName #Convs #Turns #Tokens/Turn #TotalTokens
Source Effort Generator
Alpaca[47] 52K 1 Low text-davinci-003 67.38±54.88 3.5M
Synthetic EvolInstruct[58] 143K 1 Low ChatGPT 473.33±330.13 68M
UltraChat[16] 208K 3.16 Low GhatGPT 376.58±177.81 238M
Dolly[14] 15K 1 High ChatGPT 94.61±135.84 1.42M
Human
ShareGPT[66] 112K 4.79 High ChatGPT 465.38±368.37 201M
WildChat[64] 652K 2.52 High GPT-3.5&GPT-4 727.09±818.84 852M
LMSYS-Chat-1M[65] 1M 2.01 High Mix 260.37±346.97 496M
Deita[38] 9.5K 22.02 - Mix 372.78±182.97 74M
Mixture OpenHermes[49] 243K 1 - Mix 297.86±258.45 72M
TuluV2Mixture[24] 326K 2.31 - Mix 411.94±447.48 285M
Llama-3-MAGPIE-Air 3M 1 No Llama-3-8B 426.39±217.39 1.28B
MAGPIE L Ll la am ma a- -3 3- -M MA AG GP PI IE E- -A Pri or-MT 3 10 M0K 2
1
N No
o
LL ll aa mm aa -- 33 -- 78 0B
B
46 71 80 .. 08 00 ±± 29 10 1. .6 01
9
43 76 76 MM
Llama-3-MAGPIE-Pro-MT 300K 2 No Llama-3-70B 554.53±133.64 333M
optimizationwithUltraFeedback[13]),butitalsodeliversthebestresultswhenevaluatedagainst
six baseline instruction datasets and four preference tuning methods (DPO [44], IPO [2], KTO
[19],andORPO[23]withtheUltraFeedbackdataset). Thesefindingsshowtheexceptionalquality
ofinstructiondatageneratedby MAGPIE,enablingittooutperformeventheofficial,extensively
optimizedLLMs.
2 MAGPIE: AScalableMethodtoSynthesizeInstructionData
OverviewofMAGPIE. Inwhatfollows,wedescribeourmethod,MAGPIE,tosynthesizeinstruction
data for fine-tuning LLMs. An instance of instruction data consists of at least one or multiple
instruction-responsepairs. Eachpairspecifiestherolesofinstructionproviderandfollower,along
with their instruction and response. As shown in Figure 1, MAGPIE consists of two steps: (1)
instructiongeneration,and(2)responsegeneration. ThepipelineofMAGPIEcanbefullyautomated
withoutanyhumanintervention. GiventhedatageneratedbyMAGPIE,practitionersmaycustomize
andbuildtheirownpersonalizedinstructiondatasetaccordingly(seeSection3andAppendixBfor
moredetails). Wedetaileachstepinthefollowing.
Step1: InstructionGeneration. Thegoalofthisstepistogenerateaninstructionforeachinstance
ofinstructiondata. Givenanopen-weightalignedLLM(e.g.,Llama-3-70B-Instruct),MAGPIEcrafts
aninputqueryintheformatofthepredefinedinstructiontemplateoftheLLM.Thisquerydefines
only the role of instruction provider (e.g., user), and does not provide any instruction. Note that
theauto-regressiveLLMhasbeenfine-tunedusinginstructiondataintheformatofthepredefined
instructiontemplate. Thus,theLLMautonomouslygeneratesaninstructionwhenthequerycrafted
byMAGPIEisgivenasaninput. MAGPIEstopsgeneratingtheinstructiononcetheLLMproduces
anend-of-sequencetoken. SendingthecraftedquerytotheLLMmultipletimesleadstoasetof
instructions. Comparedwithexistingsyntheticapproaches[16,31,47,53,55,58,59],MAGPIEdoes
notrequirespecificpromptengineeringtechniquessincethecraftedqueryfollowstheformatofthe
predefinedinstructiontemplate. Inaddition,MAGPIEautonomouslygeneratesinstructionswithout
usinganyseedquestion,ensuringthediversityofgeneratedinstructions.
Step2: ResponseGeneration. Thegoalofthisstepistogenerateresponsestotheinstructions
obtainedfromStep1. MAGPIEsendstheseinstructionstotheLLMtogeneratethecorresponding
responses. Combiningtherolesofinstructionproviderandfollower,theinstructionsfromStep1,and
theresponsesgeneratedinStep2yieldstheinstructiondataset. Detaileddiscussiononthegeneration
configurationcanbefoundinAppendixD.
ExtensionsofMAGPIE. MAGPIEcanbereadilyextendedtogeneratemulti-turninstructiondatasets
andpreferencedatasets. Inaddition,practitionerscanspecifythetaskrequestedbytheinstructions.
WedeferthedetaileddiscussionontheseextensionstoAppendixA.
360
40
(a) Input Length of MAGPIE-Air (in tokens)
20
0
(b) Output Length of MAGPIE-Air (in tokens) 20
40
60
(c) Input Length of MAGPIE-Pro (in tokens)
60 40 20 0 20 40 60
Alpaca Evol Instruct UltraChat Magpie
Figure3: Thisfigurecomparesthet-SNEplotofMAGPIE-Pro
with those of Alpaca, Evol Instruct, and UltraChat, each of
whichissampledwith10,000instructions. Thet-SNEplotof
(d) Input Length of MAGPIE-Pro (in tokens)
MAGPIE-Proencompassestheareacoveredbytheotherplots,
Figure 2: Lengths of instructions
demonstratingthecomprehensivecoverageofMAGPIE-Pro.
andresponsesinMAGPIE-Air/Pro.
3 DatasetAnalysis
WeapplyMAGPIEtotheLlama-3-8B-InstructandLlama-3-70B-Instructmodelstoconstructtwo
instructiondatasets: MAGPIE-Airand MAGPIE-Pro, respectively. Examplesofinstancesinboth
datasetscanbefoundinAppendixG.Inthissection,wepresentacomprehensivestatisticalanalysis
of the MAGPIE-Air and MAGPIE-Pro datasets. An overview of the lengths of instructions and
responsesofthedatain MAGPIE-Airand MAGPIE-ProispresentedinFigure2. Inwhatfollows,
wefirstassessthebreadthofMAGPIE-Probyanalyzingitscoverage. Wethendiscusstheattributes
ofMAGPIE-Pro,includingtopiccoverage,difficulty,quality,andsimilarityofinstructions,aswell
asqualityofresponse. Finally,weprovidethesafetyanalysisandcostanalysis. Usingourdataset
analysis, practitioners can customize and configure their own datasets for fine-tuning LLMs. In
AppendixB,weshowcasetheprocessofcustomizingandfilteringaninstructiondatasetbasedon
ouranalysis. Specifically,weselect300KinstancesfromMAGPIE-ProandMAGPIE-Air-Filtered,
yieldingdatasetsMAGPIE-Pro-300KandMAGPIE-Air-300K-Filtered,respectively.
3.1 DatasetCoverage
Wefollowtheapproachin[64]andanalyzethecoverageofMAGPIE-Prointheembeddingspace.
Specifically,weusetheall-mpnet-base-v2embeddingmodel1tocalculatetheinputembeddings,
andemployt-SNE[51]toprojecttheseembeddingsintoatwo-dimensionalspace. Weadoptthree
syntheticdatasetsasbaselines,includingAlpaca[47],EvolInstruct[58],andUltraChat[16],to
demonstratethecoverageofMAGPIE-Pro.
Figure3presentsthet-SNEplotsofMAGPIE-Pro,Alpaca,EvolInstruct,andUltraChat. Eacht-SNE
plotisgeneratedbyrandomlysampling10,000instructionsfromtheassociateddataset. Weobserve
that the t-SNE plot of MAGPIE-Pro encompasses the area covered by the plots of Alpaca, Evol
Instruct,andUltraChat. ThissuggeststhatMAGPIE-Proprovidesabroaderormorediverserange
oftopics,highlightingitsextensivecoverageacrossvariedthemesandsubjects. Wealsofollowthe
practicein[53]andpresentthemostcommonverbsandtheirtopdirectnounobjectsininstructions
in Appendix C, indicating the diverse topic coverage of MAGPIE dataset. Coverage analysis of
MAGPIE-AircanalsobefoundinAppendixC.
1https://huggingface.co/sentence-transformers/all-mpnet-base-v2
43.2 DatasetAttributes
Attribute: TaskCategoriesofInstructions.
WeuseLlama-3-8B-InstructtocategorizetheinstancesinMAGPIE-Pro(seeFigure7inAppendix
C.1fordetail). ThepromptsusedtoqueryLlama-3-8B-InstructcanbefoundinAppendixF.Our
observations indicate that over half of the tasks in MAGPIE-Pro pertain to information seeking,
makingitthepredominantcategory. Thisisfollowedbytasksinvolvingcreativewriting, advice
seeking, planning, and math. This distribution over the task categories aligns with the practical
requestsfromhumanusers[33].
Attribute: Quality of Instructions. We use the Llama-3-
8B-Instructmodeltoassessthequalityofeachinstructionin
MAGPIE-Air and MAGPIE-Pro, categorizing them as ‘very
poor’,‘poor’,‘average’,‘good’,and‘excellent’. Wepresent
thehistogramsofqualitiesforbothdatasetsinFigure4-(a).We
havethefollowingtwoobservations. First,bothdatasetsare
ofhighquality,withthemajorityofinstancesrated‘average’
or higher. In addition, the overall quality of MAGPIE-Pro (a) Statistics on Input Quality
surpassesthatofMAGPIE-Air. Wehypothesizethatthisisdue
totheenhancedcapabilitiesofLlama-3-70Bcomparedwith
Llama-3-8B.
Attribute: Difficulty of Instructions. We use the Llama-
3-8B-Instructmodeltoratethedifficultyofeachinstruction
in MAGPIE-Air and MAGPIE-Pro. Each instruction can be
labeledas‘veryeasy’,‘easy’,‘medium’,‘hard’,or‘veryhard’. (b) Statistics on Input Difficulty
Figure4-(b)presentsthehistogramsofthelevelsofdifficulty Figure4: Thestatisticsofinputdif-
forMAGPIE-AirandMAGPIE-Pro. Weobservethatthedis- ficultyandquality.
tributionsacrossdifficultylevelsaresimilarforMAGPIE-Air
andMAGPIE-Pro. SomeinstructionsinMAGPIE-ProaremorechallengingthanthoseinMAGPIE-Air
becauseMAGPIE-Proisgeneratedbyamorecapablemodel(Llama-3-70B-Instruct).
Attribute: InstructionSimilarity. Wequantifythesimilarity
amonginstructionsgeneratedby MAGPIE toremoverepeti-
tiveinstructions. Wemeasurethesimilarityusingminimum
neighbordistanceintheembeddingspace. Specifically,we
firstrepresentallinstructionsintheembeddingspaceusing
theall-mpnet-base-v2embeddingmodel. Foranygiven
instruction,wethencalculatetheminimumdistancefromthe (a) Min Neighbor Distance of MAGPIE-Air
instruction to its nearest neighbors in the embedding space
usingFacebookAISimilaritySearch(FAISS)[17]. Themin-
imumneighbordistancesofinstructionsinMAGPIE-Airafter
removingrepetitionsaresummarizedinFigure5-(a).
Attribute: QualityofResponses. Weassessthequalityof (b) Reward Difference of Base Model and Instruct Model
responsesusingametricnamedrewarddifference. Foreach Figure 5: This figure summarizes
instanceinourdataset,therewarddifferenceiscalculatedas theminimumneighbordistancesand
r∗−r base,wherer∗istherewardassignedbyarewardmodel
rewarddifferences.
totheresponseinourdataset,andr istherewardassigned
base
bythesamemodeltotheresponsegeneratedbytheLlama-3basemodelforthesameinstruction. We
useURIAL[35]toelicitresponsesfromthebasemodel. Apositiverewarddifferenceindicatesthat
theresponsefromourdatasetisofhigherquality,andcouldpotentiallybenefitinstructiontuning.
Inourexperiments,wefollow[29]anduseFsfairX-LLaMA3-RM-v0.1[57]asourrewardmodel.
OurresultsontherewarddifferencearepresentedinFigure5-(b).
3.3 SafetyAnalysis
WeuseLlama-Guard-2[48]toanalyzethesafetyof MAGPIE-Airand MAGPIE-Pro. Ourresults
indicatethatbothdatasetsarepredominantlysafe,withlessthan1%ofthedatapotentiallycontaining
harmfulinstructionsorresponses. PleaserefertoAppendixC.2fordetailedsafetyanalysis.
53.4 CostAnalysis
WeperformexperimentsonaserverwithfourNVIDIAA100-SXM4-80GBGPUs,anAMDEPYC
776364-CoreProcessor, and512GBofRAM,usingtheVLLMinferenceframework[28]. The
modelsareloadedinthebfloat16format.
Whencreatingthe3MMAGPIE-Airdataset,ourMAGPIEspent1.55and50hourstogeneratethe
instructions(Step1)andresponses(Step2),respectively. Forthe1MMAGPIE-Prodataset,MAGPIE
used3.5and150hourstogeneratetheinstructions(Step1)andresponses(Step2), respectively.
Compared to existing approaches to create instruction datasets, the pipeline of MAGPIE is fully
automatedwithoutanyhumaninterventionorAPIaccesstoadvancedcommercialmodelssuchas
GPT-4[1]. Consequently,MAGPIEiscost-effectiveandscalable. Onaverage,implementingMAGPIE
onacloudserver2wouldincurcostsof$0.12and$1.1per1,000datainstancesforMAGPIE-Airand
MAGPIE-Pro,respectively.
3.5 AdditionalAnalysis
Additionaldatasetanalysis, includingtheimpactofgenerationconfigurationsonthequalityand
difficultyofthegeneratedinstructions,isdetailedinAppendixC.3.
4 PerformanceAnalysis
Inthissection,weevaluatethequalityofdatasetsgeneratedbyMAGPIEbyutilizingthemtofine-tune
modelfamiliesincludingLlama-3[40]andQwen1.5[3].
4.1 ExperimentalSetups.
BaselinesforInstructionTuning. WecomparethefamilyofdatasetsgeneratedbyMAGPIEwith
sixstate-of-the-artopen-sourceinstructiontuningdatasets: ShareGPT[10],WildChat[64],Evol
Instruct[58],UltraChat[16],OpenHermes[49],andTuluV2Mix[24]. ShareGPTandWildChat
are representative human-written datasets containing 112K and 652K high-quality multi-round
conversationsbetweenhumansandGPT,respectively. EvolInstructandUltraChatarerepresentative
open-source synthetic datasets. Following [39], we use the 208K sanitized version of Ultrachat
providedbyHuggingFace3. OpenHermesandTuluV2Mixarecrowd-sourceddatasetsconsistingof
amixofdiverseopen-sourcealignmentdatasets,with243Kand326Kconversations,respectively.
Wenotethattoensurefaircomparisoninvolvingdatasetsofdifferentsizes,weprovidetheresults
ofMAGPIE-Pro-200K-FilteredandMAGPIE-Pro-100K-Filtered,whichcontainsthefirst200Kand
100KconversationsfromMAGPIE-Pro-300K-Filtered. Detaileddiscussiononhowtogeneratethese
datasetscanbefoundinAppendixB.
BaselinesforInstructionandPreferenceTuning. Wecomparethemodelsfine-tunedusingdata
generatedbyMAGPIEwithpreferenceoptimizationbaselines,includingDPO[44],IPO[2],KTO
[19]andORPO[23]. Specifically,wefollow[39]andusethemodelsfine-tunedwiththeUltraChat
dataset(forinstructiontuning)andUltrafeedbackdataset(forpreferenceoptimization)[13].
Fine-TuningDetails. Wefollow[50]anduseacosinelearningrateschedulewithaninitiallearning
rateof2×10−5whenfine-tuningLlama-3andQwen1.5models. Themaximumsequencelength
is8192. Thefine-tuningprocessisconductedusingfourNVIDIAA100GPUswith80Gmemory,
andtheeffectivebatchsizeis32. Themodelsarefine-tunedfor2epochs. Wefollowtheofficial
instructiontemplatesofeachmodel.
EvaluationBenchmarks. Weevaluatetheperformanceofthefine-tunedmodelsusingtwowidely-
adoptedinstruction-followingbenchmarks: AlpacaEval2[33]andArena-Hard[32]. AlpacaEval
2consistsof805representativeinstructionschosenfromrealuserinteractions. Arena-Hardisan
enhancedversionofMT-Bench[66],containing500challenginguserqueries. Bothbenchmarks
employaGPTevaluatortoassessresponsesgeneratedbythemodelofinterestandabaselinemodel.
Specifically,weuseGPT-4-Turbo(1106)andLlama-3-8B-InstructasbaselinesforAlpacaEval2. By
default,Arena-HardusesGPT-4(0314)asitsbaselinemodel.
2https://lambdalabs.com/service/gpu-cloud
3https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k
6Table2: Thistablecomparestheperformanceofmodelsinstruction-tunedontheLlama-8Bbase
modelsusingourdatasetsandbaselinedatasets. Weobservethatmodelsfine-tunedwithourdatasets
significantlyoutperformthosefine-tunedwithbaselinedatasetsofthesameorderofmagnitudein
termsofdatasize. Inaddition,ourfine-tunedmodelsachievecomparableperformancetotheofficial
alignedmodel,despiteonlyundergoingSFTwithamuchsmallerdataset. Numbersinboldindicate
thatMAGPIEoutperformstheofficialLlama-3-8B-Instructmodel.
AlpacaEval2 Arena-Hard
AlignmentSetup GPT-4-Turbo(1106) Llama-3-8B-Instruct
#Convs
(BaseLLM=Llama-3-8B)
LC(%) WR(%) SD LC(%) WR(%) SD WR(%)
SFT +ShareGPT[10] 112K 9.73 7.2 0.81 27.26 18.32 1.18 6.5
+EvolInstruct[58] 143K 8.52 6.25 0.76 20.16 14.98 1.1 5.1
+OpenHermes[49] 243K 9.94 6.27 0.73 29.19 17.92 1.16 4.4
+TuluV2Mix[24] 326K 9.91 7.94 0.86 24.28 18.64 1.18 5.4
+WildChat[64] 652K 14.62 10.58 0.92 34.85 26.57 1.32 8.7
+UltraChat[16] 208K 8.29 5.44 0.71 23.95 15.12 1.11 3.6
$
+*PO +UltraFeedback(DPO)[13,44] 64K 18.36 17.33 1.14 44.42 42.36 1.46 14.8
+UltraFeedback(IPO)[2,13] 64K 17.46 16.13 1.11 41.66 38.45 1.43 14.2
+UltraFeedback(KTO)[13,19] 64K 15.81 14.62 1.05 41.33 38.32 1.42 12.2
+UltraFeedback(ORPO)[13,23] 64K 13.23 12.57 0.99 30.62 28.27 1.35 10.9
SFT +MAGPIE(Ours)
Air-300K-Raw 300K 21.99 21.65 1.21 48.63 48.06 1.42 15.8
Air-300K-Filtered 300K 22.66 23.99 1.24 49.27 50.8 1.44 14.9
Pro-300K-Raw 300K 21.65 22.19 1.2 49.65 50.84 1.42 15.9
Pro-100K-Filtered 100K 20.47 24.52 1.25 47.92 52.75 1.43 17.2
Pro-200K-Filtered 200K 22.11 26.02 1.26 51.17 56.76 1.41 15.9
Pro-300K-Filtered 300K 25.08 29.47 1.35 52.12 53.43 1.44 18.9
Llama-3-8B-Instruct(SFT+RLHF) >10M4 22.92 22.57 1.26 50 50 - 20.6
Metrics. Weadopttwometricstomeasurethecapabilitiesofinstruction-followingoffine-tunedmod-
els. Thefirstmetricisthewinrate(WR),whichcalculatesthefractionofresponsesthatarefavored
bytheGPTevaluator. ThismetricisappliedinbothbenchmarksincludingAlpacaEval2andArena-
Hard. Thesecondmetricisthelength-controlledwinrate(LC)[18],adebiasedversionofWR.
TheGPTevaluatorconsidersthelengthsofresponses
generated by the baseline model and model under Creative
Tasks
evaluationwhencomputingLC.Byaccountingfor
response length, LC reduces its impact on the win
rate. This metric is specifically applied to the Al-
pacaEval2benchmark[33].
Coding
DetailedExperimentalSetups. Weprovidemore & Debugging Reasoning &
Planning
detaileddescriptionsofourexperimentalsetups,in-
cludingmorefine-tuningdetailsanddecodinghyper-
parametersinAppendixD.
4.2 ExperimentalResults
Math & Data
Info Seeking
MAGPIEdatasetsoutperformothers.
In Table 2, we first compare the performance of
Llama-3modelsfine-tunedwithdatasetsgenerated
by MAGPIE against those fine-tuned with baseline Figure6: Thisfigureshowstheperformance
datasets. UsingtheAlpacaEval2evaluationbench- breakdownbycategoryofMAGPIE-Proand
mark,weobservethatbothLCandWRofourfine- baselinesonWildBench.
tunedmodelssurpassthosefine-tunedwithbaseline
instructiondatasets,regardlessofthechoiceofthe
baselinemodel. ThisindicatesthatthedatasetsgeneratedbyMAGPIEareofhigherquality,leadingto
significantlyenhancedinstruction-followingcapabilities. Asimilarobservationismadewhenusing
theArena-Hardevaluationbenchmark. WehighlightthattheLlama-3modelsfine-tunedwithdatasets
generatedbyMAGPIEoutperformeventhosemodelsthathaveundergonepreferenceoptimization
4https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct
7Table3: Thistablecomparestheperformanceofmodelsinstruction-tunedontheQwenbasemodels
usingtheMAGPIE-Pro-300K-Filtereddatasetandtheofficialinstruction-tunedmodels. TheQwen
basemodelenhancedwithMAGPIEconsistentlyoutperformstheofficialinstruction-tunedmodel.
AlpacaEval2
GPT-4-Turbo(1106) OfficialAlignedModelasRef.
AlignmentSetup
LC(%) WR(%) SD LC(%) WR(%) SD
Qwen1.5-4B-Chat 5.89 4.74 0.67 50 50 -
Qwen1.5-4B
BaseModel+MAGPIE 9.1 10.96 0.93 68.09 72.42 1.42
Qwen1.5-7B-Chat 14.75 11.77 0.97 50 50 -
Qwen1.5-7B
BaseModel+MAGPIE 15.10 18.51 1.14 46.28 58.53 1.44
(e.g.,instructiontuningcombinedwithDPO),whichemphasizesthehighqualityofdatagenerated
byMAGPIE.
To investigate the advantages of MAGPIE across different task categories, we also compare the
performanceofmodelsfine-tunedwithMAGPIE-ProcomparedwithbaselinedatasetsusingWild-
Benchbenchmark[34]. Thisbenchmarkconsistsof1024taskscarefullyselectedfromreal-world
human-LLMconversationlogs. TheresultsaredemonstratedinFigure6. WeobservethatMAGPIE
consistentlyoutperformsbaselinedatasetsacrosscategories.
Modelsfine-tunedwithdatageneratedbyMAGPIEachievecomparableperformancetothe
officialalignedmodel,butwithfewerdata. InTable2,wecomparetheperformanceofmodels
fine-tunedwithdatageneratedbyMAGPIEagainsttheofficialalignedmodel(Llama-3-8B-Instruct).
WeobservethattheLlama-3-8Bbasemodelfine-tunedwithdatafromMAGPIEoutperformsLlama-
3-8B-instructusingtheAlpacaEval2benchmark. Forexample,usingtheMAGPIE-Pro-300K-Filtered
datasettofine-tunetheLlama-3-8BbasemodelresultsinWC29.47%againstGPT-4-Turbo(1106).
Furthermore,whenLlama-3-8B-InstructischosenasthebaselinemodelofAlpacaEval2,weobserve
thatWCofLlama-3-8Bbasemodelsfine-tunedwithdatafromMAGPIEexceeds50%,indicatinga
preferenceforourfine-tunedmodelsovertheofficialalignedmodel. Finally,wehighlightthatour
fine-tuningprocessusesnomorethan300Kdata,whereastheofficialalignedmodelsarefine-tuned
with more than 10M data samples. This demonstrates the high quality of the data generated by
MAGPIE. UsingtheArena-Hardbenchmark,weobservethata1.7%differencebetweentheWR
achievedusingourfine-tunedmodelandtheofficialalignedmodel. Weattributethisdiscrepancy
tothefractionofcoding-relatedinstructionsinourdataset. Webelievethatthisgapcouldbeeasily
bridgedasweincreasethesizeofdatasets.
Bothdataquantityandqualitymattertocapabilitiesofinstruction-following. Inwhatfollows,
wecomparewithinthefamilyofdatasetsgeneratedbyMAGPIEinTable2. Thesedatasetsdiffer
insizes,deploymentoffiltering,andmodelsusedtogeneratedata. Weobservethatasthesizeof
datasetincreases,theperformanceoffine-tunedmodelimproves,indicatingthatdataquantityplaysa
criticalroleinenhancinginstruction-followingcapabilities. Furthermore,themodelfine-tunedwith
MAGPIE-Pro-300K-Filteredoutperformthosefine-tunedwiththesameamountofrawdata. This
demonstratestheeffectivenessofourfilteringtechnique,andunderscorestheimportanceofdata
quality. Finally,weobservethatthemodelsfine-tunedwithMAGPIE-Proconsistentlyoutperform
thosefine-tunedwithMAGPIE-Air. ThereasonisthatMAGPIE-Proisgeneratedbythemorecapable
model,i.e.,Llama-3-70B-Instruct.
MAGPIEcanenhancetheperformanceofotherbackbonemodels. Table3illustratestheefficacy
ofMAGPIEwhenappliedtogenerateinstructiondatasetandfine-tuneotherbackbonemodels,i.e.,
Qwen1.5-4BandQwen1.5-7B.Theresultsdemonstratethatourfine-tunedmodelsachievebetter
performancethantheofficialalignedmodels,whichhaveundergoneinstructionandpreferencetuning.
TheseresultsunderscoretheeffectivenessofMAGPIEandthequalityofitsgeneratedinstructions.
AdditionalExperimentalResults.WedeferadditionalexperimentalresultsandanalysisofMAGPIE-
Air-MTandMAGPIE-Pro-MTtoAppendixE.1. Additionally,theperformanceofMAGPIEacross
variousotherbenchmarksisreportedinAppendixE.3.
5 RelatedWork
LLMAlignment. Instructiontuning[56]andpreferencetuning[5]arewidelyusedtoalignthe
responsesofLLMswithhumanvalues. Instructiontuningutilizesaninstructiondatasettofine-tune
8LLMs,whereeachinstructiondataconsistsofoneturnormultipleturnsofinstructionsanddesired
responses. Theperformanceofinstructiontuningheavilyreliesonthequalityofinstructiondata
[47,53,67]. PreferencetuningfurtherimprovesresponsesofLLMsusingreinforcementlearning
humanfeedback(RLHF)[5]orpreferenceoptimization[2,19,23,44]basedonapreferencedataset.
AlignmentDatasetConstruction. Weclassifytheexistingmethodsofcreatingdatasetsformodel
alignmentintotwomaincategories: humaninteractionswithLLMsandsyntheticinstructiongen-
eration. Tocreatedatasetsforalignment,previousstudieshavecollectedhumaninteractionswith
LLMs [14, 64, 65, 66, 26]. However, manually crafting instructions is not only time-consuming
andlabor-intensive,butmayalsoincorporatetoxiccontent[64]. Anothercategoryofapproaches
[53,47,58,59,55,46]focusonpromptingLLMstogeneratesyntheticinstructiondatasets,begin-
ningwithasmallsetofhuman-annotatedseedinstructionsandexpandingthesethroughfew-shot
prompting. However,thesemethodsfaceadiversitychallenge,asfew-shotpromptingoftenresultsin
newinstructionsthataretoosimilartotheoriginalseedquestions[31]. Toenhancecoverage,some
research[16,31]summarizesworldknowledgeandemploysittogeneratesyntheticdatasets. We
notethatourMAGPIEdatasetalsobelongstothesyntheticdataset. However,weleveragetheprompt
templatewithnorequirementforseedquestionsorpromptengineering.
Comparedtotheabovetwomaincategories,alignmentdatacanalsobegeneratedbytransforming
existingdata[54,45,20].However,theconstrainedvarietyofNLPtasksinthesedatasetsmayimpede
theabilityoftunedLLMstogeneralizeinreal-worldscenarios[31]. Therearealsomixturedatasets
(e.g., [24, 49, 38, 67]) that combine or select high-quality instruction data from various existing
open-sourceinstructiondatasetstoenhancecoverage[24,49]and/orimproveoverallperformance
[38,67]. Therearealsodataconstructionmethodsfocusingonimprovingthereasoningandmath
abilities[61,62],whichcanbefurthermergedwithMAGPIEforcreatingabettermixtureofdatafor
instructiontuning.
Training Data Extraction. Language models have the capability to memorize examples from
theirtrainingdatasets,potentiallyenablingmalicioususerstoextractprivateinformation[8,7,9].
Pioneeringwork[27,9,41]hasdemonstratedthatitispossibletoextractprivatepre-trainingdata
fromBERT[15],GPT-2[43],andChatGPT[1],respectively. Yuetal. [60]proposeseveraltricks
including adjusting sampling strategies to better extract training datasets from language models.
Recently,Kassemet. al. [25]proposeablack-boxpromptoptimizationmethodthatusesanattacker
LLMtoextracthighlevelsofmemorizationinavictimLLM.Wangetal. [52]leveragemembership
inferenceattack(MIA)toextractfine-tuningdatasetsfromfine-tunedlanguagemodels. Baietal. [4]
extractsthetrainingdatasetofproductionlanguagemodelsviaspecialcharacters(e.g.,structural
symbolsofJSONfiles,and,#inemailsandonlineposts). Differentfromthepriorwork,weaimto
createpubliclyavailablealignmentdatasetswithminimalhumaneffortbyleveragingtheremarkable
generationcapabilitiesofLLMs,ratherthanextractingprivatetrainingdatafromLLMs.
6 LimitationsandEthicalConsiderations
Limitations. Incertainscenarios,usersmayaimtofine-tuneLLMsusingdomain-specificinstruction
data. Investigating how to configure MAGPIE to efficiently generate the desired domain-specific
instructions(e.g.,mathproblems)issubjecttoourfuturework. Also,thereisstillagapbetween
Magpie-tunedLLMsandofficialLlama-3-InstructondatasetssuchasWildBenchandMMLU,which
suggestthatweshouldfocusonproducingharderreasoningtasksandfeedbacklearningdata.
LicenseandLegality. TheinstructiondatasetsgeneratedbyMAGPIEinthispaperaresubjectto
CCBY-NClicenseandMetaLlama3Communitylicense. Whileusersarepermittedtodistribute,
adapt,andfurtherdevelopourmethodMAGPIE,itistheresponsibilityoftheuserstoapplyMAGPIE
toLLMsincompliancewiththeassociatedlicenseagreement. Weherebydisclaimanyliabilityfor
misuseofdatageneratedbyusersofMAGPIE.
SocietalImpactandPotentialHarmfulConsequences. Theprimaryobjectiveofthispaperis
to develop a scalable method to synthesize instruction data to enhance the instruction-following
capabilities of LLMs, and thus align them with human values. However, the data generated by
MAGPIEmaycontainharmfulinstructionsand/orresponses,whichmayleadtounsafebehaviorsif
usedrawininstructiontuning. Ourempiricalevaluationsindicatethatsuchharmfuldatainstances
constitute less than 1% of the dataset. To mitigate this risk, we develop a filtering technique in
AppendixBtoidentifyandremovetheseinstances.
97 Conclusion
Inthispaper,wedevelopedascalablemethod,MAGPIE,tosynthesizeinstructiondataforfine-tuning
largelanguagemodels. MAGPIEleveragedthepredefinedinstructiontemplatesofopen-weightLLMs
andcraftedapromptspecifyingonlytheroleofinstructionprovider. Giventhecraftedprompt,the
LLMthengenerateddetailedinstructionsduetotheirauto-regressivenature. MAGPIEthensentthe
generatedinstructionstotheLLMtogeneratecorrespondingresponses. Thesepairsofinstructions
andresponsesconstitutedtheinstructiondataset.WeusedLlama-3-8B-instructtolabeltheinstruction
datasetanddevelopedafilteringtechniquetoselecteffectivedatainstancesforinstructiontuning.
Wefine-tunedtheLlama-3-8Bbasemodelusingtheselecteddata,anddemonstratedthatthefine-
tunedmodeloutperformedthosefine-tunedusingallbaselines. Moreover,ourfine-tunedmodels
outperformed the official aligned model, Llama-3-8B-Instruct, which has been instruction-tuned
andpreference-optimizedusingmorethan10Mdatainstances. Thishighlightedthequalityofthe
instructiondatasynthesizedbyMAGPIE.
8 Acknowledgement
TheresearchofZ.Xu,F.Jiang,L.Niu,andR.PoovendranispartiallysupportedbytheNational
ScienceFoundation(NSF)AIInstituteforAgent-basedCyberThreatIntelligenceandOperation
(ACTION)undergrantIIS2229876. TheresearchofY.ChoiispartiallysupportedbytheNational
ScienceFoundation(NSF)undergrantDMS-2134012(ScalingLawsofDeepLearning)andthe
OfficeofNavalResearch(ONR)undergrantN00014-24-1-2207(SymbolicKnowledgeDistillation
ofLLMsforAll: DiverseScales,Skills,andValues).
ThisworkissupportedinpartbyfundsprovidedbytheNationalScienceFoundation,Departmentof
HomelandSecurity,andIBM.Anyopinions,findings,andconclusionsorrecommendationsexpressed
inthismaterialarethoseoftheauthor(s)anddonotnecessarilyreflecttheviewsoftheNSForits
federalagencyandindustrypartners.
References
[1] JoshAchiam,StevenAdler,SandhiniAgarwal,LamaAhmad,IlgeAkkaya,FlorenciaLeoni
Aleman,DiogoAlmeida,JankoAltenschmidt,SamAltman,ShyamalAnadkat,etal. Gpt-4
technicalreport. arXivpreprintarXiv:2303.08774,2023.
[2] MohammadGheshlaghiAzar,ZhaohanDanielGuo,BilalPiot,RemiMunos,MarkRowland,
MichalValko,andDanieleCalandriello. Ageneraltheoreticalparadigmtounderstandlearning
fromhumanpreferences. InInternationalConferenceonArtificialIntelligenceandStatistics,
pages4447–4455.PMLR,2024.
[3] JinzeBai,ShuaiBai,YunfeiChu,ZeyuCui,KaiDang,XiaodongDeng,YangFan,Wenbin
Ge,YuHan,FeiHuang,BinyuanHui,LuoJi,MeiLi,JunyangLin,RunjiLin,DayihengLiu,
GaoLiu,ChengqiangLu,KemingLu,JianxinMa,RuiMen,XingzhangRen,XuanchengRen,
ChuanqiTan,SinanTan,JianhongTu,PengWang,ShijieWang,WeiWang,ShengguangWu,
BenfengXu,JinXu,AnYang,HaoYang,JianYang,ShushengYang,YangYao,BowenYu,
HongyiYuan,ZhengYuan,JianweiZhang,XingxuanZhang,YichangZhang,ZhenruZhang,
ChangZhou,JingrenZhou,XiaohuanZhou,andTianhangZhu. Qwentechnicalreport. arXiv
preprintarXiv:2309.16609,2023.
[4] YangBai,GePei,JindongGu,YongYang,andXingjunMa. Specialcharactersattack: Toward
scalabletrainingdataextractionfromlargelanguagemodels. arXivpreprintarXiv:2405.05990,
2024.
[5] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma,
DawnDrain,StanislavFort,DeepGanguli,TomHenighan,NicholasJoseph,SauravKadavath,
JacksonKernion,TomConerly,SheerEl-Showk,NelsonElhage,ZacHatfield-Dodds,Danny
Hernandez,TristanHume,ScottJohnston,ShaunaKravec,LianeLovitt,NeelNanda,Catherine
Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann,
andJaredKaplan. Trainingahelpfulandharmlessassistantwithreinforcementlearningfrom
humanfeedback,2022.
10[6] EdwardBeeching,ClémentineFourrier,NathanHabib,SheonHan,NathanLambert,Nazneen
Rajani,OmarSanseviero,LewisTunstall,andThomasWolf. Openllmleaderboard. https:
//huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard,2023.
[7] StellaBiderman,UsvsnPrashanth,LintangSutawika,HaileySchoelkopf,QuentinAnthony,
ShivanshuPurohit,andEdwardRaff. Emergentandpredictablememorizationinlargelanguage
models. AdvancesinNeuralInformationProcessingSystems,36,2023.
[8] HannahBrown,KatherineLee,FatemehsadatMireshghallah,RezaShokri,andFlorianTramèr.
Whatdoesitmeanforalanguagemodeltopreserveprivacy? InProceedingsofthe2022ACM
ConferenceonFairness,Accountability,andTransparency,pages2280–2292,2022.
[9] NicholasCarlini,FlorianTramer,EricWallace,MatthewJagielski,ArielHerbert-Voss,Kather-
ineLee,AdamRoberts,TomBrown,DawnSong,UlfarErlingsson,etal. Extractingtraining
datafromlargelanguagemodels. In30thUSENIXSecuritySymposium(USENIXSecurity21),
pages2633–2650,2021.
[10] Wei-LinChiang,ZhuohanLi,ZiLin,YingSheng,ZhanghaoWu,HaoZhang,LianminZheng,
SiyuanZhuang,YonghaoZhuang,JosephE.Gonzalez,IonStoica,andEricP.Xing. Vicuna:
Anopen-sourcechatbotimpressinggpt-4with90%*chatgptquality,March2023.
[11] PeterClark,IsaacCowhey,OrenEtzioni,TusharKhot,AshishSabharwal,CarissaSchoenick,
andOyvindTafjord. Thinkyouhavesolvedquestionanswering? tryarc,theai2reasoning
challenge. arXivpreprintarXiv:1803.05457,2018.
[12] KarlCobbe,VineetKosaraju,MohammadBavarian,MarkChen,HeewooJun,LukaszKaiser,
MatthiasPlappert,JerryTworek,JacobHilton,ReiichiroNakano,etal. Trainingverifiersto
solvemathwordproblems. arXivpreprintarXiv:2110.14168,2021.
[13] GanquCui,LifanYuan,NingDing,GuanmingYao,WeiZhu,YuanNi,GuotongXie,Zhiyuan
Liu,andMaosongSun. Ultrafeedback: Boostinglanguagemodelswithhigh-qualityfeedback.
arXivpreprintarXiv:2310.01377,2023.
[14] Databricks. Databricksdolly-15k,2023.
[15] JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova. Bert: Pre-trainingof
deepbidirectionaltransformersforlanguageunderstanding. arXivpreprintarXiv:1810.04805,
2018.
[16] NingDing,YulinChen,BokaiXu,YujiaQin,ZhiZheng,ShengdingHu,ZhiyuanLiu,Maosong
Sun,andBowenZhou. Enhancingchatlanguagemodelsbyscalinghigh-qualityinstructional
conversations. arXivpreprintarXiv:2305.14233,2023.
[17] Matthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff Johnson, Gergely Szilvasy, Pierre-
EmmanuelMazaré,MariaLomeli,LucasHosseini,andHervéJégou. Thefaisslibrary. 2024.
[18] YannDubois,BalázsGalambosi,PercyLiang,andTatsunoriBHashimoto. Length-controlled
alpacaeval: Asimplewaytodebiasautomaticevaluators. arXivpreprintarXiv:2404.04475,
2024.
[19] KawinEthayarajh, WinnieXu, NiklasMuennighoff, DanJurafsky, andDouweKiela. Kto:
Modelalignmentasprospecttheoreticoptimization. arXivpreprintarXiv:2402.01306,2024.
[20] SaumyaGandhi,RituGala,VijayViswanathan,TongshuangWu,andGrahamNeubig. Better
syntheticdatabyretrievingandtransformingexistingdatasets.arXivpreprintarXiv:2404.14361,
2024.
[21] AryoPradiptaGema,JoshuaOngJunLeang,GiwonHong,AlessioDevoto,AlbertoCarloMaria
Mancino,RohitSaxena,XuanliHe,YuZhao,XiaotangDu,MohammadRezaGhasemiMadani,
etal. Arewedonewithmmlu? arXivpreprintarXiv:2406.04127,2024.
[22] DanHendrycks,CollinBurns,StevenBasart,AndyZou,MantasMazeika,DawnSong,and
Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint
arXiv:2009.03300,2020.
11[23] JiwooHong,NoahLee,andJamesThorne. Reference-freemonolithicpreferenceoptimization
withoddsratio. arXivpreprintarXiv:2403.07691,2024.
[24] HamishIvison,YizhongWang,ValentinaPyatkin,NathanLambert,MatthewPeters,Pradeep
Dasigi, Joel Jang, David Wadden, Noah A Smith, Iz Beltagy, et al. Camels in a changing
climate: Enhancinglmadaptationwithtulu2. arXivpreprintarXiv:2311.10702,2023.
[25] AlyMKassem,OmarMahmoud,NiloofarMireshghallah,HyunwooKim,YuliaTsvetkov,Yejin
Choi,SherifSaad,andSantuRana. Alpacaagainstvicuna:Usingllmstouncovermemorization
ofllms. arXivpreprintarXiv:2403.04801,2024.
[26] AndreasKöpf,YannicKilcher,DimitrivonRütte,SotirisAnagnostidis,ZhiRuiTam,Keith
Stevens,AbdullahBarhoum,DucNguyen,OliverStanley,RichárdNagyfi,ShahulES,Sameer
Suri,DavidGlushkov,ArnavDantuluri,AndrewMaguire,ChristophSchuhmann,HuuNguyen,
andAlexanderMattick. Openassistantconversations-democratizinglargelanguagemodel
alignment. InA.Oh,T.Naumann,A.Globerson,K.Saenko,M.Hardt,andS.Levine,editors,
AdvancesinNeuralInformationProcessingSystems,volume36,pages47669–47681.Curran
Associates,Inc.,2023.
[27] KalpeshKrishna,GauravSinghTomar,AnkurP.Parikh,NicolasPapernot,andMohitIyyer.
Thievesonsesamestreet! modelextractionofbert-basedapis. InInternationalConferenceon
LearningRepresentations,2020.
[28] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu,
JosephE.Gonzalez,HaoZhang,andIonStoica. Efficientmemorymanagementforlargelan-
guagemodelservingwithpagedattention. InProceedingsoftheACMSIGOPS29thSymposium
onOperatingSystemsPrinciples,2023.
[29] NathanLambert,ValentinaPyatkin,JacobMorrison,LJMiranda,BillYuchenLin,Khyathi
Chandu,NouhaDziri,SachinKumar,TomZick,YejinChoi,etal. Rewardbench: Evaluating
rewardmodelsforlanguagemodeling. arXivpreprintarXiv:2403.13787,2024.
[30] HectorLevesque,ErnestDavis,andLeoraMorgenstern. Thewinogradschemachallenge. In
Thirteenthinternationalconferenceontheprinciplesofknowledgerepresentationandreasoning,
2012.
[31] HaoranLi,QingxiuDong,ZhengyangTang,ChaojunWang,XingxingZhang,HaoyangHuang,
ShaohanHuang, XiaolongHuang, ZeqiangHuang, DongdongZhang, etal. Syntheticdata
(almost) from scratch: Generalized instruction tuning for language models. arXiv preprint
arXiv:2402.13064,2024.
[32] TianleLi,Wei-LinChiang,EvanFrick,LisaDunlap,BanghuaZhu,JosephE.Gonzalez,and
IonStoica. Fromlivedatatohigh-qualitybenchmarks: Thearena-hardpipeline,April2024.
[33] XuechenLi,TianyiZhang,YannDubois,RohanTaori,IshaanGulrajani,CarlosGuestrin,Percy
Liang,andTatsunoriB.Hashimoto.Alpacaeval:Anautomaticevaluatorofinstruction-following
models. https://github.com/tatsu-lab/alpaca_eval,2023.
[34] Bill Yuchen Lin, Khyathi Chandu, Faeze Brahman, Yuntian Deng, Abhilasha Ravichander,
ValentinaPyatkin,RonanLeBras,andYejinChoi. Wildbench: Benchmarkinglanguagemodels
withchallengingtasksfromrealusersinthewild,2024.
[35] BillYuchenLin, AbhilashaRavichander, XimingLu, NouhaDziri, MelanieSclar, Khyathi
Chandu,ChandraBhagavatula,andYejinChoi. Theunlockingspellonbasellms: Rethinking
alignmentviain-contextlearning. arXivpreprintarXiv:2312.01552,2023.
[36] StephanieLin, JacobHilton, andOwainEvans. Truthfulqa: Measuringhowmodelsmimic
humanfalsehoods. arXivpreprintarXiv:2109.07958,2021.
[37] RuiboLiu,JerryWei,FangyuLiu,ChengleiSi,YanzheZhang,JinmengRao,StevenZheng,
DaiyiPeng,DiyiYang,DennyZhou,etal. Bestpracticesandlessonslearnedonsyntheticdata
forlanguagemodels. arXivpreprintarXiv:2404.07503,2024.
12[38] WeiLiu,WeihaoZeng,KeqingHe,YongJiang,andJunxianHe. Whatmakesgooddatafor
alignment? acomprehensivestudyofautomaticdataselectionininstructiontuning. InThe
TwelfthInternationalConferenceonLearningRepresentations,2024.
[39] YuMeng,MengzhouXia,andDanqiChen. Simpo: Simplepreferenceoptimizationwitha
reference-freereward. arXivpreprintarXiv:2405.14734,2024.
[40] Meta. Llama3. https://ai.meta.com/blog/meta-llama-3/,2024.
[41] MiladNasr,NicholasCarlini,JonathanHayase,MatthewJagielski,AFederCooper,DaphneIp-
polito, Christopher A Choquette-Choo, Eric Wallace, Florian Tramèr, and Katherine Lee.
Scalable extraction of training data from (production) language models. arXiv preprint
arXiv:2311.17035,2023.
[42] OpenAI. Tiktoken. https://github.com/openai/tiktoken,2024.
[43] AlecRadford,KarthikNarasimhan,TimSalimans,IlyaSutskever,etal. Improvinglanguage
understandingbygenerativepre-training. 2018.
[44] RafaelRafailov,ArchitSharma,EricMitchell,ChristopherDManning,StefanoErmon,and
ChelseaFinn. Directpreferenceoptimization: Yourlanguagemodelissecretlyarewardmodel.
InThirty-seventhConferenceonNeuralInformationProcessingSystems,2023.
[45] VictorSanh,AlbertWebson,ColinRaffel,StephenBach,LintangSutawika,ZaidAlyafeai,
AntoineChaffin,ArnaudStiegler,ArunRaja,MananDey,MSaifulBari,CanwenXu,Urmish
Thakker,ShanyaSharmaSharma,ElizaSzczechla,TaewoonKim,GunjanChhablani,Nihal
Nayak,DebajyotiDatta,JonathanChang,MikeTian-JianJiang,HanWang,MatteoManica,
Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala
Neeraj,JosRozen,AbheeshtSharma,AndreaSantilli,ThibaultFevry,JasonAlanFries,Ryan
Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M Rush.
Multitaskpromptedtrainingenableszero-shottaskgeneralization. InInternationalConference
onLearningRepresentations,2022.
[46] ZhiqingSun,YikangShen,QinhongZhou,HongxinZhang,ZhenfangChen,DavidCox,Yiming
Yang,andChuangGan. Principle-drivenself-alignmentoflanguagemodelsfromscratchwith
minimalhumansupervision. AdvancesinNeuralInformationProcessingSystems,36,2023.
[47] RohanTaori,IshaanGulrajani,TianyiZhang,YannDubois,XuechenLi,CarlosGuestrin,Percy
Liang,andTatsunoriB.Hashimoto. Stanfordalpaca: Aninstruction-followingllamamodel.
https://github.com/tatsu-lab/stanford_alpaca,2023.
[48] Llama Team. Meta llama guard 2. https://github.com/meta-llama/PurpleLlama/
blob/main/Llama-Guard2/MODEL_CARD.md,2024.
[49] Teknium. Openhermesdataset,2023.
[50] HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,
NikolayBashlykov,SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,etal. Llama2: Open
foundationandfine-tunedchatmodels. arXivpreprintarXiv:2307.09288,2023.
[51] LaurensVanderMaatenandGeoffreyHinton. Visualizingdatausingt-sne. Journalofmachine
learningresearch,9(11),2008.
[52] Jeffrey G Wang, Jason Wang, Marvin Li, and Seth Neel. Pandora’s white-box: Increased
trainingdataleakageinopenllms. arXivpreprintarXiv:2402.17012,2024.
[53] YizhongWang,YeganehKordi,SwaroopMishra,AlisaLiu,NoahA.Smith,DanielKhashabi,
and Hannaneh Hajishirzi. Self-instruct: Aligning language models with self-generated in-
structions. InProceedingsofthe61stAnnualMeetingoftheAssociationforComputational
Linguistics(Volume1: LongPapers),pages13484–13508,Toronto,Canada,2023.Association
forComputationalLinguistics.
13[54] YizhongWang,SwaroopMishra,PegahAlipoormolabashi,YeganehKordi,AmirrezaMirzaei,
AtharvaNaik,ArjunAshok,ArutSelvanDhanasekaran,AnjanaArunkumar,DavidStap,Eshaan
Pathak,GiannisKaramanolakis,HaizhiLai,IshanPurohit,IshaniMondal,JacobAnderson,
KirbyKuznia,KrimaDoshi,KuntalKumarPal,MaitreyaPatel,MehradMoradshahi,Mihir
Parmar,MiraliPurohit,NeerajVarshney,PhaniRohithaKaza,PulkitVerma,RavsehajSingh
Puri,RushangKaria,SavanDoshi,ShailajaKeyurSampat,SiddharthaMishra,SujanReddyA,
SumantaPatro,TanayDixit,andXudongShen. Super-NaturalInstructions: Generalizationvia
declarativeinstructionson1600+NLPtasks. InYoavGoldberg,ZornitsaKozareva,andYue
Zhang,editors,Proceedingsofthe2022ConferenceonEmpiricalMethodsinNaturalLanguage
Processing,pages5085–5109,AbuDhabi,UnitedArabEmirates,December2022.Association
forComputationalLinguistics.
[55] ZifengWang,Chun-LiangLi,VincentPerot,LongTLe,JinMiao,ZizhaoZhang,Chen-Yu
Lee,andTomasPfister. Codeclm: Aligninglanguagemodelswithtailoredsyntheticdata. arXiv
preprintarXiv:2404.05875,2024.
[56] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan
Du,AndrewM.Dai,andQuocVLe. Finetunedlanguagemodelsarezero-shotlearners. In
InternationalConferenceonLearningRepresentations,2022.
[57] WeiXiong,HanzeDong,ChenluYe,ZiqiWang,HanZhong,HengJi,NanJiang,andTong
Zhang. Iterativepreferencelearningfromhumanfeedback: Bridgingtheoryandpracticefor
rlhfunderkl-constraint,2024.
[58] CanXu,QingfengSun,KaiZheng,XiuboGeng,PuZhao,JiazhanFeng,ChongyangTao,and
DaxinJiang. Wizardlm: Empoweringlargelanguagemodelstofollowcomplexinstructions.
arXivpreprintarXiv:2304.12244,2023.
[59] CanwenXu,DayaGuo,NanDuan,andJulianMcAuley. Baize: Anopen-sourcechatmodel
withparameter-efficienttuningonself-chatdata. InHoudaBouamor,JuanPino,andKalika
Bali,editors,Proceedingsofthe2023ConferenceonEmpiricalMethodsinNaturalLanguage
Processing, pages 6268–6278, Singapore, December 2023. Association for Computational
Linguistics.
[60] Weichen Yu, Tianyu Pang, Qian Liu, Chao Du, Bingyi Kang, Yan Huang, Min Lin, and
ShuichengYan.Bagoftricksfortrainingdataextractionfromlanguagemodels.InInternational
ConferenceonMachineLearning,pages40306–40320.PMLR,2023.
[61] XiangYue,XingweiQu,GeZhang,YaoFu,WenhaoHuang,HuanSun,YuSu,andWenhu
Chen. Mammoth: Buildingmathgeneralistmodelsthroughhybridinstructiontuning. ArXiv,
abs/2309.05653,2023.
[62] XiangYue,TuneyZheng,GeZhang,andWenhuChen. Mammoth2: Scalinginstructionsfrom
theweb,2024.
[63] RowanZellers,AriHoltzman,YonatanBisk,AliFarhadi,andYejinChoi. Hellaswag: Cana
machinereallyfinishyoursentence? arXivpreprintarXiv:1905.07830,2019.
[64] WentingZhao,XiangRen,JackHessel,ClaireCardie,YejinChoi,andYuntianDeng. Wildchat:
1mchatGPTinteractionlogsinthewild. InTheTwelfthInternationalConferenceonLearning
Representations,2024.
[65] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Tianle Li, Siyuan Zhuang, Zhanghao Wu,
YonghaoZhuang, ZhuohanLi, ZiLin, EricXing, JosephE.Gonzalez, IonStoica, andHao
Zhang. LMSYS-chat-1m: Alarge-scalereal-worldLLMconversationdataset. InTheTwelfth
InternationalConferenceonLearningRepresentations,2024.
[66] LianminZheng,Wei-LinChiang,YingSheng,SiyuanZhuang,ZhanghaoWu,YonghaoZhuang,
ZiLin,ZhuohanLi,DachengLi,EricXing,HaoZhang,JosephE.Gonzalez,andIonStoica.
JudgingLLM-as-a-judgewithMT-benchandchatbotarena. InThirty-seventhConferenceon
NeuralInformationProcessingSystemsDatasetsandBenchmarksTrack,2023.
[67] ChuntingZhou,PengfeiLiu,PuxinXu,SrinivasanIyer,JiaoSun,YuningMao,XuezheMa,
Avia Efrat, Ping Yu, Lili Yu, et al. Lima: Less is more for alignment. Advances in Neural
InformationProcessingSystems,36,2023.
14A MAGPIE Extension
Inthissection,weexploretheextensionofMAGPIE. Wefirstoutlinetheprocessforconstructinga
multi-turndataset(MAGPIE-MT).Wethendiscussmethodsforcontrollinginstructiontasksusing
MAGPIE. Finally,wewillbrieflydiscusshowtodevelopapreferenceoptimizationdatasetbasedon
MAGPIE.
A.1 BuildingMulti-TurnMAGPIE
ToconstructMAGPIE-MT,weinitiallyfollowSteps1and2togeneratethefirstturnofinstructionand
response. Forsubsequentturns,weappendthepre-querytemplatetotheendofthefullpromptfrom
thepreviousroundofcommunication. Wehaveobservedthatthemodelmayoccasionallyforgetits
roleastheuser,especiallyforthe8Bmodel.Tomitigatethis,weemployasystempromptdesignedto
controlthebehavioroftheLLMandreinforceitsawarenessofthemulti-roundconversationcontext.
ThefullpromptforbuildingtheinstructionsofMAGPIE-MTcanbefoundinFigure11inAppendix
F. We follow the procedure described in Step 2 of Section 2 to generate responses and yield the
multi-turninstructiondataset.
A.2 ControlInstructionTasksofMAGPIE
Insomescenarios,usersmaywishtofine-tunelargelanguagemodels(LLMs)usingdomain-specific
instruction data, such as code or mathematical content, to enhance performance within specific
domains. Inthissection,weintroducealightweightandeffectivemethodtocontrolthetaskcategory
of generated instructions. Our approach involves guiding LLMs through the system prompt by
specifyingthatitisachatbottailoredforaparticulardomainandoutliningthetypesofuserqueriesit
mightencounter. Weprovideanexampleofasystempromptdesignedtocontrolthegenerationof
math-relatedinstructions,asillustratedinFigure12withinAppendixF.
A.3 BuildingPreferenceOptimizationDatasetwithMAGPIE
MAGPIE can be readily adapted to create preference datasets by integrating responses generated
bytheinstructmodelwiththosefromthebasemodel. Specifically,utilizingtherewarddifference
outlinedinSection3,apreferencedatasetcanbeassembledbydesignatingtheresponsefromthe
instructmodelasthepreferredresponse,andtheresponsefromthebasemodelasthelesspreferred
one,providedthatr∗−r
base
>0. Wewillsoonopen-sourceMAGPIE-PO,apreferenceoptimization
datasettofurtheralignLLMswithhumanpreferences.
B FilterSetups
Inthissection,weexplorepotentialfilterconfigurationsforselectinghigh-qualityinstructionaldata
forfine-tuningpurposes. Weprovidethefollowingmetricstoenableuserstocustomizetheirfiltered
MAGPIEdataset:
1. InputLength: Thetotalnumberofcharactersintheinstructions.
2. OutputLength: Thetotalnumberofcharactersintheresponses.
3. TaskCategory: Thespecificcategoryoftheinstructions. SeeAppendixC.1fordetails.
4. InputQuality: Theclarity, specificity, andcoherenceoftheinstructions, ratedas‘very
poor’,‘poor’,‘average’,‘good’,and‘excellent’.
5. Input Difficulty: The level of knowledge required to address the task described in the
instruction,ratedas‘veryeasy’,‘easy’,‘medium’,‘hard’,or‘veryhard’.
6. MinimumNeighborDistance: Theembeddingdistancetothenearestneighbor. Canbe
usedforfilteringoutrepetitiveorsimilarinstances.
7. Reward: Denotedasr∗. SeeSection3fordetails. Thismetriccanbeusedtofilterout
low-qualityresponses,suchasrepetitionsorrefusals.
8. RewardDifference: Denotedasr∗−r . SeeSection3fordetails.
base
15Weprovideseveraloff-the-shelfconfigurations,asdemonstratedinTable4. Wedeferthedetailed
performanceanalysisofeachfilterconfigurationforMAGPIE-ProtoAppendixE.2.
Table4: Differentfilterconfigurationsweprovide. WenotethattheOutputLengthfilterisapplied
last. Specifically,thisfilterselectsthekinstancesofthelongestresponses. Inourexperiments,we
empiricallysetτ =−12,andτ =0.
1 2
Input Output Task Input Input MinNeighbor Reward
SourceDataset FilterName #Convs Reward
Length Length Category Quality Difficulty Distance Difference
MAGPIE-Air Filter 300K - Longest - ≥good ≥medium >0 - >τ2
Filter 300K - Longest - ≥average - >0 >τ1 -
Filter2 300K - Longest - ≥good ≥easy >0 >τ1 -
MAGPIE-Pro Filter3 300K - Longest - - - >0 >τ1 -
Filter4 300K - Longest - ≥good ≥easy >0 - >τ2
Filter5 338K - - - ≥good ≥easy >0 >τ1 -
Filter6 200K - Longest - - 50%easy+50%>easy >0 >τ1 -
C MoreDatasetAnalysis
Thissectionprovidesadditionaldatasetanalysis,complementingthediscussionsinSection3.
C.1 AdditionalAnalysisonDatasetCoverageandAttributes.
TaskCategoriesofMAGPIE-ProandMAGPIE-Air. Figure7illustratesthetaskcategorydistribu-
tionsforMAGPIE-ProandMAGPIE-Air,aslabeledbyLlama-3-Instruct. Weobservethatthetask
category distributions of these two datasets are largely similar, however, MAGPIE-Pro exhibits a
higherpercentageofcreativewritingtasks.
CodiDat na R gA o n lal eN y &ones Oi t Bs Ph l De rar es y ai b in u REg g ng d ei sin tt ag i or sn Mm og i
n
an ig
tn hg
DaE td a Ci ti A CR on n Be Rg a rl da o ry ils as i ei eo ns nni
g
aPl s n ta tg &y o ii r vn DOmg eti en bh Wg ue gr rgs i itn ig
ng
Math
Information Information
Planning
Seeking Seeking
Planning
SeA ed kv inic
ge
A
W
ritC
i
nr
ge ati
v
e
Seekd
i
nv gice
(a)TaskcategoriesofMAGPIE-Pro. (b)TaskcategoriesofMAGPIE-Air.
Figure7: ThisfigurevisualizesthetaskcategoryofMAGPIE-ProandMAGPIE-Airbytopictags.
VisualizationofRootVerbsandTheirDirectNounObjects. Figure8visualizesthetopcommon
root verbs and their direct noun objects of MAGPIE-Air dataset. This indicates the diverse topic
coverageofMAGPIE-Air.
C.2 AdditionalSafetyAnalysis
Table5illustratesthepercentageofdifferentunsafecategoriesofMAGPIE-AirandMAGPIE-Pro,
as labeled by Llama-Guard-2 [48]. We have two key observations. First, the proportion of data
containingpotentiallyharmfulqueriesisminimal,withlessthan1%forbothdatasets. Second,the
majorityofunsaferesponsesfallintothecategoryofspecializedadvice,whichincludesresponses
thatmayofferspecializedfinancial,medical,orlegaladvice,orsuggestthatdangerousactivitiesor
objectsaresafe.
16p ro issue friend
b
le
m dataset
trip
party trouble
plan have
wedding
vacation
project
st no or sv cy rie esl p st a by ook help kindlw istscripttype r issi uete n paie ne problemd symptom lotexperience conceptexplain cre differenca ete importanceprocessg principleam eprovid woe rldcharacterd stb oe ryxu a so ysm temi pl ld eg ini fov rmat e tioa nk res lise s tm t exea plana atira onsur mt mk c aryup hecu s por scn red eghs oei eaes jdsi eteg e crn tga repm ortpaph esrtudeyousma eod dh elov mei apc petae sxs a kmt pe lec kip ndo listurj seo clasg tsrb ipcarea busm inec psroh sjcehe cbaa tlnongen lgt ey lk ip ss stti ot tt reynr ypo ealm dp to oetr oglo v dl yel iba da lc lp p es eh am fc rop t tr mt is a aoa cr da i shlr oey o l oeot t wpnr hp jns er ohhew pe f dctw g oms a otvet e te nug caob eu ea setmpsrr ro erm oailjtet rn rdcieeptt
Figure8: Thisfiguredemonstratesthetop20mostcommonrootverbs(shownintheinnercircle)
andtheirtop5directnounobjects(shownintheoutercircle)withintheMAGPIE-Airdataset. This
indicatesthatMAGPIEencompassesabroadrangeoftopics.
Table5: ThistableshowsthepercentageofdifferentunsafecategoriesofMAGPIE-AirandMAGPIE-
ProtaggedbyLlama-Guard-2[48]model.
Dataset Safe V Ci ro imle en st No Cn r- iV mio el sent Sex C- rR imel ea sted C Eh xi pld loS ite ax tiu oa nl Sp Aec di va il ciz eed Privacy In Pt re oll pe ec rt tu yal Ind Wis ec ari pm onin sate Hate SS eu li fc -i Hde ar& m CS oe nx tu ea nl t Others
MAGPIE-Air 99.128% 0.001% 0.073% 0.003% 0.000% 0.636% 0.022% 0.026% 0.038% 0.001% 0.002% 0.009% 0.062%
MAGPIE-Pro 99.347% 0.001% 0.049% 0.002% 0.000% 0.446% 0.015% 0.074% 0.014% 0.001% 0.004% 0.011% 0.036%
C.3 AblationAnalysisonGenerationConfigurations
2.30 0.55
3.4
3.192 3.294 3.293 2.213 2.226 2.224 0.328 0.329 0.317
2.28 0.50
3.2 2.26 0.45
3.121 3.180 3.178 3.0 2.268 2.237 2.258 0.412 0.403 0.389
2.24 0.40
2.8
2.882 2.902 2.975 2.295 2.283 2.269 2.22 0.501 0.492 0.477 0.35
2.6
1 0.995 0.99 1 0.995 0.99 2.20 1 0.995 0.99 0.30
Top-p Top-p Top-p
(a) Average Quality Scores of (b)AverageDifficultyScoresof (c)AverageMinimumNeighbor
MAGPIE-Air MAGPIE-Air DistancesofMAGPIE-Air
Figure9: Thisfigureillustratestheimpactofvaryingdecodingparametersonthequality,difficulty,
anddiversityofgeneratedinstructions. Weobservethatwhilehighertemperatureandtop-pvalues
may decrease the overall quality, they tend to increase both the difficulty and diversity of the
instructions.
AblationAnalysisonDecodingParameters. Weconductanablationanalysisonthedecoding
parametersusedingeneratinginstructionwith MAGPIE. Specifically,weusethreedifferenttem-
peratures(i.e.,1,1.1,and1.2)andtop-pvalues(i.e.,1,0.995,and0.99)duringStep1ofMAGPIE.
Weusethreemetrics,AverageQualityScore,AverageDifficultyScoreandAverageMinimum
NeighborDistancetocharacterizethequality,difficulty,anddiversityofinstructionsusingdifferent
decodingparameters. TheAverageQualityScoreiscalculatedbyaveragingtheratingsofalldata
withinaspecifictemperature-top-ppair,onascalefrom1(‘verypoor’)to5(‘excellent’). Similarly,
theAverageDifficultyScoreisratedonascalefrom1(‘veryeasy’)to5(‘veryhard’). TheAverage
17
erutarepmeT
1
1.1
2.1
erocS ytilauQ
egarevA
erutarepmeT
1
1.1
2.1
erocS ytluciffiD
egarevA
erutarepmeT
1
1.1
2.1
ecnatsiD
robigieN
muminiM
egarevAMinimumNeighborDistanceiscalculatedbyaveragingtheminimumneighbordistances,asdefined
inSection3,foralldatageneratedusingthesamedecodingparameters.
The findings are summarized in Figure 9. We observe that
highertemperatureandtop-pvaluesmayslightlydecreasethe
overallqualityofinstructions,whilesimultaneouslyincreasing
thedifficultyandremarkablyenhancingthediversityofthein-
structionsgenerated. Theselectionofthesehyper-parameters
shouldbetailoredtotheuser’sspecificrequirements,balanc-
ingthetrade-offsbetweenquality,difficulty,anddiversity.
(a) Comparison of Input Quality w/wo System Prompts
AblationAnalysisontheSystemPrompt. Figure10com-
parestheuseofsystempromptcomparedwithnotusingitin
Step1ofMAGPIE. SincetheLlama-3modeldoesnothavean
officialsystemprompt,weusethedefaultsystempromptfrom
Vicuna[10]: Achatbetweenacurioususerandanartificial
intelligence assistant. The assistant gives helpful, detailed,
andpoliteanswerstotheuser’squestions. Weobservethat
(b) Comparison of Input Difficulty w/wo System Prompts
usingasystempromptgenerallyresultsinadecreaseinthe
overallqualityofinstructions,andtheinstructionsareeasier. Figure10: Thisfigurecomparesthe
Consequently,werecommendnotappendingsystemprompts inputqualityanddifficultywithand
indefaultsettings. withoutsystemprompts.
D DetailedExperimentalSetups
D.1 ExperimentalSetupsforGeneratingMAGPIE-AirandMAGPIE-Pro
AsdetailedinAppendixC.3,varyingdecodingparametersinStep1cansignificantlyinfluencethe
quality, difficulty, and diversity of the generated instructions. To optimize the trade-offs among
these attributes, we employ diverse decoding parameters for the generation of MAGPIE-Air and
MAGPIE-Pro. Table6presentstheconfigurationsofMAGPIE-AirandMAGPIE-Pro,showcasinghow
diversedecodingparametersshapeeachdataset.
WeemploygreedydecodingtogenerateresponsesinStep2forMAGPIE-AirandMAGPIE-Pro. The
intuitionisthatthewordwiththehighestprobabilityismorelikelytooriginatefromthemodel’s
trainingdataset.
Table6: ThistabledemonstratestheconfigurationsofgeneratinginstructionsofMAGPIE-Airand
MAGPIE-Prodatasetswithvaryingdecodingparameters.
DecodingParameters
Dataset Total#Convs
Temperature Top-p #Convs
1.0 1.00 300K
1.0 0.995 300K
1.0 0.990 300K
1.1 1.00 300K
1.1 0.995 300K
1.1 0.990 300K
MAGPIE-Air 3M
1.2 1.00 300K
1.2 0.995 300K
1.2 0.990 300K
1.25 1.00 100K
1.25 0.995 100K
1.25 0.990 100K
1.0 1.00 300K
1.1 0.995 300K
MAGPIE-Pro 1M
1.2 0.995 300K
1.25 0.990 100K
18D.2 ExperimentalSetupsforInstructionTuning
SupervisedFine-TuningHyper-parameters. Table7demonstratesthedetailedsupervisedfine-
tuninghyper-parameters. TheseexperimentswereconductedusingAxolotl5.
Table7: Thistableshowsthehyper-parametersforsupervisedfine-tuning.
Hyper-parameter Value
LearningRate 2×10−5
NumberofEpochs 2
NumberofDevices 4
Per-deviceBatchSize 1
GradientAccumulationSteps 8
EffectiveBatchSize 32
Optimizer Adamwwithβs=(0.9,0.999)andϵ=10−8
LearningRateScheduler cosine
WarmupSteps 100
MaxSequenceLength 8192
Decodingparametersforevaluationbenchmarks. ForArena-Hard[32]andWildBench[34],we
follow its default setting and use greedy decoding for all settings. For AlpacaEval 2 [33] which
allows the model provider to specify decoding parameters, we also employ greedy decoding in
all experiments with a slightly increased repetition penalty (RP = 1.2) to mitigate the potential
repetitiveoutputsduringthegeneration.
E AdditionalExperimentalResults
E.1 PerformanceofMAGPIE-MT
Table 8 compares the performance of MAGPIE-Air-MT and MAGPIE-Pro-MT with their respec-
tivesingle-turncounterparts. Weobservethatthemulti-turndatasetshaveenhancedperformance,
particularlyintheArena-Hardbenchmark.
Table 8: This table compares the performance of the multi-turn versions, MAGPIE-Air-MT and
MAGPIE-Pro-MT,withtheirsingle-turncounterparts. Allmodelsareinstruction-tunedontheLlama-
8Bbasemodels.
AlpacaEval2 Arena-Hard
GPT-4-Turbo(1106) Llama-3-8B-Instruct
Dataset
LC(%) WR(%) SD LC(%) WR(%) SD WR(%)
Single-Turn 22.66 23.99 1.24 49.27 50.80 1.44 14.9
MAGPIE-Air
MT 22.98 24.02 1.27 49.63 51.42 1.40 15.5
Single-Turn 25.15 26.50 1.30 50.52 52.98 1.43 18.9
MAGPIE-Pro
MT 24.21 25.19 1.28 52.92 54.80 1.41 20.4
E.2 AblationAnalysisonFilterDesigns
WeconductanablationanalysisonvariousfilterdesignswithinMAGPIE-Protoassesstheirimpact
ontheperformanceofsupervisedfine-tunedmodels. TheresultsarepresentedinTable9. Weobserve
thatdifferentfilteringstrategiesyieldoptimalperformanceondifferentbenchmarks,andnosingle
filterconsistentlyachievesthebestperformanceacrossallbenchmarks. Therefore,determininghow
toselectinstructionaldatatoenhancetheperformanceinsupervisedfine-tuningisaninteresting
topicforfutureresearch.
5https://github.com/OpenAccess-AI-Collective/axolotl
19Table9: Thistablecomparestheperformanceofdifferentfilterdesignswithin MAGPIE-Pro. All
modelsareinstruction-tunedontheLlama-8Bbasemodels.
AlpacaEval2 Arena-Hard
GPT-4-Turbo(1106) Llama-3-8B-Instruct
DatasetandFilter
LC(%) WR(%) SD LC(%) WR(%) SD WR(%)
Filter 25.08 29.47 1.35 52.12 53.43 1.44 18.9
Filter2 25.15 26.50 1.30 50.52 52.98 1.43 18.9
Filter3 23.90 25.21 1.25 51.45 53.64 1.41 18.3
MAGPIE-Pro
Filter4 24.20 25.33 1.27 52.43 54.34 1.43 17.9
Filter5 24.85 25.12 1.26 52.12 53.43 1.44 18.4
Filter6 23.20 28.43 1.26 51.34 57.29 1.41 17.9
E.3 PerformanceofMAGPIEonMoreBenchmarks
Wereporttheperformanceofmodelsfine-tunedusing MAGPIE-Airand MAGPIE-Pro, evaluated
acrossarangeoftasksfeaturedontheHuggingfaceOpenLLMLeaderboard[6]inTable10. The
tasks includes MMLU [22], ARC [11], HellaSwag [63], TruthfulQA [36], Winogard [30], and
GSM8K[12]. WealsoperformexperimentsonMMLU-Redux[21]withzero-shotprompting. We
usethedefaultgreedydecodingwithRP =1forallsetups. Ourexperimentalresultsdemonstrate
thatmodelsfine-tunedwithMAGPIE-AirandMAGPIE-Proachievecomparableperformancetothe
officialinstructmodelandotherbaselinesdespitethealignmenttax.
Table10: Thistablecomparestheperformanceofmodelsinstruction-tunedon MAGPIE-Airand
MAGPIE-Proagainstbaselinesandofficialinstructmodelacrossvariousdownstreambenchmarks.
Allmodelsareinstruction-tunedontheLlama-8Bbasemodels.
AlignmentSetup MMLU(5) ARC(25) HellaSwag(10) TruthfulQA(0) Winograd(5) GSM8K(5) MMLU-Redux(0)
ShareGPT 66.03 58.45 81.50 52.34 74.03 48.67 50.68
EvolInstruct 65.62 60.75 82.70 52.87 76.16 42.91 52.73
OpenHermes 65.42 62.29 82.15 50.85 75.61 47.16 46.07
TuluV2Mix 66.34 59.22 82.80 47.99 76.16 58.07 46.97
WildChat 65.95 59.22 81.39 53.18 75.30 48.75 52.59
UltraChat 65.23 62.12 81.68 52.76 75.53 50.57 50.75
MAGPIE-Air-300K-Filtered 64.45 61.01 79.90 53.48 72.38 52.24 52.34
MAGPIE-Pro-100K-Filtered 65.31 60.32 81.18 51.11 73.32 50.42 52.56
MAGPIE-Pro-200K-Filtered 64.98 61.26 80.71 51.82 73.16 47.76 51.44
MAGPIE-Pro-300K-Filtered 64.25 60.41 80.52 52.46 73.32 47.92 52.16
Llama-3-8B-Instruct 67.82 61.52 78.67 52.47 72.14 71.72 58.60
F PromptTemplates
F.1 PromptTemplatesforMAGPIEExtension
ThissectionpresentstheprompttemplateusedtogenerateMAGPIE-MTandcontrolinstructiontasks,
asdetailedinFigure11andFigure12,respectively.
F.2 PromptTemplatesforEvaluation
Here,wepresenttheprompttemplateemployedtogeneratetaskcategories,quality,anddifficulty,as
detailedinFigure13,Figure14,andFigure15,respectively. Theplaceholderinputrepresentsthe
instructionstobeevaluated.
20PromptforgeneratingMAGPIE-MT
<|begin_of_text|><|start_header_id|>system<|end_header_id|>
YouareahelpfulAlassistant.Theuserwillengageinamulti−roundconversationwithyou,
askinginitialquestionsandfollowingupwithadditionalrelatedquestions.Yourgoalis
toprovidethorough,relevantandinsightfulresponsestohelptheuserwiththeir
queries.<|eot_id|><|start_header_id|>user<|end_header_id|>
{instruction}<|eot_id|><|start_header_id|>assistant<|end_header_id|>
{response}<|eot_id|><|start_header_id|>user<|end_header_id|>
Figure11: PromptforgeneratingMAGPIE-MT.Theplaceholder{instruction}and{response}
arefromthefirstturn.
Promptforcontrollinginstructiontasks
<|begin_of_text|><|start_header_id|>system<|end_header_id|>
YouareanAIassistantdesignedtoprovidehelpful,step−by−stepguidanceonsolvingmath
problems.Theuserwillaskyouawiderangeofcomplexmathematicalquestions.Your
purposeistoassistusersinunderstandingmathematicalconcepts,workingthrough
equations,andarrivingatthecorrectsolutions.<|eot_id|><|start_header_id|>user<|
end_header_id|>
Figure12: Promptforcontrollinginstructiontasks. Inthisexample,wecontrolLLMstogenerate
instructionsrelatedtomath.
21Promptforgeneratingtaskcategories
#Instruction
Pleaselabelthetasktagsfortheuserquery.
##UserQuery
‘‘‘{input}‘‘‘
##Taggingtheuserinput
Pleaselabelthetasktagsfortheuserquery.Youwillneedtoanalyzetheuserqueryand
selectthemostrelevanttasktagfromthelistbelow.
all_task_tags=[
"Informationseeking",#Usersaskforspecificinformationorfactsaboutvarioustopics.
"Reasoning",#Queriesrequirelogicalthinking,problem−solving,orprocessingof
complexideas.
"Planning",#Usersneedassistanceincreatingplansorstrategiesforactivitiesand
projects.
"Editing",#Involvesediting,rephrasing,proofreading,orothertasksrelatedtothe
compositionofgeneralwrittencontent.
"Coding&Debugging",#Usersseekhelpwithwriting,reviewing,orfixingcodein
programming.
"Math",#Queriesrelatedtomathematicalconcepts,problems,andcalculations.
"Roleplaying",#UsersengageinscenariosrequiringChatGPTtoadoptacharacteror
persona.
"Dataanalysis",#Requestsinvolveinterpretingdata,statistics,orperforminganalytical
tasks.
"Creativewriting",#Usersseekassistancewithcraftingstories,poems,orother
creativetexts.
"Adviceseeking",#Usersaskforrecommendationsorguidanceonvariouspersonalor
professionalissues.
"Brainstorming",#Involvesgeneratingideas,creativethinking,orexploringpossibilities.
"Others"#Anyqueriesthatdonotfitintotheabovecategoriesorareofamiscellaneous
nature.
]
##OutputFormat:
Notethatyoucanonlyselectasingleprimarytag.Otherapplicabletagscanbeaddedto
thelistofothertags.
Now,pleaseoutputyourtagsbelowinajsonformatbyfillingintheplaceholdersin<...>:
‘‘‘
{{
"primary_tag":"<primarytag>",
"other_tags":["<tag1>","<tag2>",...]
}}
‘‘‘
Figure13: Promptforgeneratingtaskcategories
22Promptforgeneratingqualityofinstructions
#Instruction
Youneedtoratethequalityoftheuserquerybasedonitsclarity,specificity,andcoherence.
Theratingscaleisasfollows:
−verypoor:Thequeryisunclear,vague,orincoherent.Itlacksessentialinformationand
context.
−poor:Thequeryissomewhatunclearorlacksimportantdetails.Itrequiressignificant
clarification.
−average:Thequeryismoderatelyclearandspecific.Itmayrequiresomeadditional
informationforacompleteunderstanding.
−good:Thequeryisclear,specific,andmostlywell−formed.Itprovidessufficientcontextfor
understandingtheuser’sintent.
−excellent:Thequeryisveryclear,specific,andwell−articulated.Itcontainsallthe
necessaryinformationandcontextforprovidingacomprehensiveresponse.
##UserQuery
‘‘‘{input}‘‘‘
##OutputFormat
Giventheuserquery,youfirstneedtogiveanassessment,highlightingthestrengthsand/or
weaknessesoftheuserquery.Then,youneedtooutputaratingfromverypoorto
excellentbyfillingintheplaceholdersin[...]:
‘‘‘
{{
"explanation":"[...]",
"input_quality":"[verypoor/poor/average/good/excellent]"
}}
‘‘‘
’’’
Figure14: Promptforgeneratingqualityofinstructions
Promptforgeneratingdifficultyofinstructions
#Instruction
Youfirstneedtoidentifythegivenuserintentandthenlabelthedifficultyleveloftheuser
querybasedonthecontentoftheuserquery.
##UserQuery
‘‘‘{input}‘‘‘
##OutputFormat
Giventheuserquery,inyouroutput,youfirstneedtoidentifytheuserintentandthe
knowledgeneededtosolvethetaskintheuserquery.Then,ratethedifficultylevelof
theuserqueryas‘veryeasy‘,‘easy‘,‘medium‘,‘hard‘,or‘veryhard‘.
Now,pleaseoutputtheuserintentanddifficultylevelbelowinajsonformatbyfillinginthe
placeholdersin[...]:
‘‘‘
{{
"intent":"Theuserwantsto[....]",
"knowledge":"Tosolvethisproblem,themodelsneedtoknow[....]",
"difficulty":"[veryeasy/easy/medium/hard/veryhard]"
}}
‘‘‘
Figure15: Promptforgeneratingdifficultyofinstructions
23G MAGPIE Examples
Inthissection,wedemonstrateseveralrepresentativeinstructionsfromMAGPIE-Prowithdifferent
taskcategories.
InformationSeeking
Afewdaysago,IwasatarestaurantandIgotacupofcoffee. However,whenIwenttotake
asip,Irealizeditwasalittletoohot. So,Idecidedtoletitcooldownforafewminutes. AsI
waited,Inoticedthatcoffeeisactuallytwodifferentcolors. Thepartthatwasclosesttothe
surfaceofthecoffeeisalightercolor,andthepartthatisdeeperisadarkercolor. Haveyou
everobservedthisphenomenonbefore?
Reasoning
Youhave3switchesbuttheyarenotlabeled. Eachswitchcorrespondstooneofthreelight
bulbsinaroom. Eachlightbulbiseitheronoroff. Youcanturntheswitchesonandoffas
manytimesasyouwant,butyoucanonlyentertheroomonetimetoobservethebulbs. How
canyoufigureoutwhichswitchcorrespondstowhichlightbulb?
Planning
YouaretheFounderofaFinancialPlanningCompany. Asaprofessionalfinancialadvisor,
youarescheduledtomeetanewclienttomorrow. Specifically,whatareyouplanningtodoto
prepareforthismeeting?
Editing
Whatisthebestwaytore-writethesentence: "Wecallthisthe‘core’productorthe‘core’
offering"usingproperquotationmarksandavoidingtheword"this"?
Coding&Debugging
WriteaPythonprogramthatcalculatesthetotalcostofacustomer’sorder. Theprogram
shouldaskforthecustomer’sname,thenumberofitemstheywanttopurchase,andtheprice
ofeachitem. Itshouldthencalculatethetotalcostbymultiplyingthenumberofitemsbythe
priceofeachitemandadding8%salestax. Theprogramshoulddisplaythecustomer’sname,
thenumberofitems,thepriceofeachitem,andthetotalcost,includingsalestax.
Math
Inthefollowingproblem,pleaseuseintegerstosolveit. Awatertankhas1000Lofwater.
Onthefirstday,1/5ofthewaterwasdrained. Onthesecondday,3/10oftheremainingwater
wasdrained. Onthethirdday,2/5oftheremainingwaterwasdrained. Onthefourthday,3/4
oftheremainingwaterwasdrained. Howmanylitersofwaterareleftafterthefourthday?
RolePlaying
Inthisgame,youwillbethehost,andIwillbethecontestant. Youwillaskmeaseriesof
questions,andIwilltrytoanswerthemcorrectly. Thequestionswillbemultiplechoice,and
Iwillhavea25%chanceofgettingthecorrectanswerifIjustrandomlyguess. However,I
amaclevercontestant,andIwilltrytouselogicandreasoningtoincreasemychancesof
gettingthecorrectanswer.
24DataAnalysis
Thepersonnelmanageratacompanyistaskedwithfindingtheaveragesalaryofnewhires.
She has collected data on the salaries of 13 new hires. She wants to know if there is a
statisticaldifferencebetweentheaveragesalaryofnewhiresandthenationalaveragesalary.
Thenationalaveragesalaryis$45,000. Thesampleofnewhireshasameansalaryof$42,800
andastandarddeviationof$4,200.
CreativeWriting
Writeaparagraphaboutamythicalcreaturethatyoucreated. Thecreatureissmall,nolarger
thanahousecat. Ithasshimmeringscalesthatreflectlight,andcanemitasoft,pulsingglow
fromitsbody. Ithaslarge,roundeyesthatseemtoseerightthroughyou,butwithagentle
kindness. Ithasasoft,melodiousvoice,andcancommunicatewithhumansthroughaform
oftelepathy.
AdviceSeeking
Howdoyouhandlestressandoverwhelm?
Brainstorming
Canyougivemesomeideasforaspontaneous,funandmemorablebirthdaycelebrationfor
mypartner?
Others
Whatdoes"sdrawkcaB"mean?
25