RILe: Reinforced Imitation Learning
MertAlbaba1,2 SammyChristen1 ChristophGebhardt1 ThomasLangarek1
MichaelJ.Black2 OtmarHilliges1
1ETHZürich 2MaxPlanckInstituteforIntelligentSystems
{balbaba, sammyc, cgebhard, thomalan, otmarh}@ethz.ch, black@tue.mpg.de
Abstract
ReinforcementLearninghasachievedsignificantsuccessingeneratingcomplex
behaviorbutoftenrequiresextensiverewardfunctionengineering.Adversarialvari-
antsofImitationLearningandInverseReinforcementLearningofferanalternative
bylearningpoliciesfromexpertdemonstrationsviaadiscriminator. Employing
discriminatorsincreasestheirdata-andcomputationalefficiencyoverthestandard
approaches; however, results in sensitivity to imperfections in expert data. We
proposeRILe,ateacher-studentsystemthatachievesbothrobustnesstoimperfect
dataandefficiency. InRILe,thestudentlearnsanactionpolicywhiletheteacher
dynamicallyadjustsarewardfunctionbasedonthestudent’sperformanceandits
alignmentwithexpertdemonstrations. Bytailoringtherewardfunctiontoboth
performanceofthestudentandexpertsimilarity,oursystemreducesdependence
onthediscriminatorand,hence,increasesrobustnessagainstdataimperfections.
ExperimentsshowthatRILeoutperformsexistingmethodsby2xinsettingswith
limitedornoisyexpertdata.
1 Introduction
ReinforcementLearning(RL)offersaframeworkforlearningbehaviorbymaximizingareward
function. In recent years, deep reinforcement learning has demonstrated remarkable success in
replicating complex behaviors, including playing Atari games, chess, and Go [1, 2]. However,
designingarewardfunctionisatediousandchallengingtask,aspredictingthepolicyoutcomefrom
amanuallycraftedrewardfunctionisdifficult.
Toaddressthisissue,ImitationLearning(IL)leveragesexpertdemonstrationsforlearningapolicy.
Since vast amounts of expert data are required to learn expert behaviors accurately, Adversarial
Imitation Learning (AIL) approaches are proposed as a data-efficient alternative to IL [3]. AIL
employsadiscriminatortomeasuresimilaritybetweenlearnedbehaviorandexpertbehavior,and
rewardstheagentintrainingaccordingly. However,thediscriminatortendstooverfittothedynamics
ofthedemonstrationsetting,limitingitsabilitytogeneralizetonewenvironments[4]. Furthermore,
AILisvulnerabletonoiseorimperfectionsinexpertdata,asthediscriminatorrewardstheagentto
perfectlymimicexpertdata,includingitsflaws.
InverseReinforcementLearning(IRL)isanotherapproachtoalleviaterewardengineering.Incontrast
toIL,whichdirectlylearnsexpertbehavior,IRLseekstoinfertheunderlyingrewardfunctionthat
leadstoexpertbehavior.Thus,therewardfunctionandtheagentaretrainediteratively,withupdatesto
therewardfunctionbasedontheagent’sbehavior. ThisiterativeprocessrendersIRLcomputationally
expensive[5]. AdversarialInverseReinforcementLearning(AIRL)[4]addressesthisinefficiencyby
introducingadiscriminatorthatallowsforsimultaneouslearningofthepolicyandrewardfunction.
Byimplementingaspecialstructureonthediscriminator,AIRLcanaccuratelyrecoverthereward
functionofanexpert,therebyimprovinggeneralizationtoenvironmentswithdynamicsthatvary
from the demonstration setting, unlike AIL. However, in contrast to IRL, the reward function in
AIRLisnotlearnedindependentlybutisdirectlyderivedfromtheoutputofthediscriminator. Like
Preprint.Underreview.
4202
nuJ
21
]GL.sc[
1v27480.6042:viXraAIL,AIRLsuffersfromthedata-sensitivityissueassociatedwiththediscriminator,whichlimitsits
performancewhenexpertdataisimperfect.
Tolearnbehaviorsfromimperfectdatainacomputationallyefficientway,weproposeReinforced
ImitationLearning(RILe). TheprimarymotivationofRILeistomaintaincomputationalefficiency
and scalability through an adversarial framework, while learning a reward function from expert
behavior(akintoIRL)asaseparatemechanism. Ourframeworkcomprisestwointeractingagents: a
studentagentthatlearnsareplicatingpolicyandateacheragentthatlearnsarewardfunctionand
guidesthestudentagentduringtraining. Forcomputationalefficiency,weemployadiscriminatorthat
distinguishesexpertdatafromstudentroll-outsastherewardfunctionoftheteacher. Thecoreidea
behindRILeisthatthediscriminatorguidestherewardlearningprocessinsteadofdirectlyguiding
theactionpolicylearning. OurcontributionsoverAILandAIRLaretwo-fold:
1. Welearntherewardfunction(teacheragent)ofthereplicatingpolicy(studentagent)asan
independentfunctioninanadversarialIL/IRLsetting. UsingRL,theteacheragentlearnsto
providerewardsthatmaximizethecumulativerewardfromthediscriminator. Theresulting
rewardfunctioncanprovidemeaningfulguidanceevenwhenthediscriminatoroverfitsto
noisyexpertdata, enablingthereplicatingpolicytonavigateandovercomesub-optimal
statessuggestedbyimperfectexpertdemonstrations.
2. ThelearnedrewardfunctioninRILeaddsanextralayerofflexibilitytoadversarialIL/IRL
settingsbyallowingtheteachertocustomizerewardsaccordingtothedevelopmentalstage
ofthereplicatingagent,facilitatingmoreaccurateimitationofexpertbehavior(i.e.,"online
learnedrewardshaping").
Our experimental evaluation compares RILe to the state-of-the-art methods of AIL and AIRL,
specificallyGAIL[3]andAIRL[4],acrossthreedistinctsetsofexperiments: (1)imitatingnoisy
expertdatainagridworld,(2)imitatingmotion-capturedataincontinuouscontroltasks,and(3)
learningtoplayimage-basedAtarigameswithdiscreteactions. Experimentalresultsrevealthatour
approachoutperformsbaselinesby2x,especiallywhenexpertdataislimited. Wefurtherdemonstrate
thatRILesuccessfullylearnsexpert-likebehaviorfromnoisyandmisleadingexpertdata,whereas
thebaselinemethodsfailtodoso.
2 RelatedWork
We review the literature on learning expert behavior from demonstrations. Commonly, expert
demonstrationsaresourcedeitherthroughdirectqueriestotheexpertinanyobservablestateorby
collectingsampletrajectoriesdemonstratedbytheexpert.Wepresentrelatedworkthatalignswiththe
mostprevalentapproachesofthelattersetting,namelyImitationLearningandInverseReinforcement
Learning. BothILandIRLformtheconceptualfoundationofRILe.
Offlinereinforcementlearningalsolearnspoliciesfromdata,whichmayincludeexpertdemonstra-
tions. Incontrasttooursetting,itsmaingoalistolearnapolicywithoutanyonlineinteractionswith
theenvironment. Wereferthereaderto[6]foranoverviewofofflineRL.
ImitationLearning TheearliestworkonimitationlearningintroducedBehavioralCloning(BC)
[7],whichaimstolearnapolicycongruentwithexpertdemonstrationsthroughsupervisedlearning.
DAgger[8]proposestheaggregationofexpertdemonstrationswithpolicyexperiencesduringthe
trainingofthepolicyforimprovinggeneralizationoverexpertdemonstrations. GAILintroducesand
adversarialimitationlearningmethod,whereadiscriminatoraimstounderstandwhetherqueried
behavior stems from a policy or from expert demonstrations, while a generator tries to fool the
discriminatorbylearningapolicythatexhibitsexpert-likebehavior[3]. InfoGAILextendsupon
GAILbyextractinglatentfactorsfromexpertbehaviorandemployingthemduringimitationlearning
[9]. DeepQ-learningfromDemonstrations(DQfD)proposestofirstpre-trainthelearningagent
using expert demonstrations, followed by a subsequent policy optimization through interactions
withtheenvironment[10]. ValueDiceintroducesanoff-policyimitationlearningmethodusinga
distribution-matchingobjectivebetweenpolicyandexpertbehavior[11].
Although thefield ofimitation learninghas seenadvancements, the needfor high-qualityexpert
dataanddataefficacyremainopenchallenges[5]. Moreover,thelimitedgeneralizationcapability
ofILapproachespersists[12]. WeaddresstheselimitationsrelatedtoIL’sdataefficacyanddata
2sensitivitybyintroducinganintermediaryteacheragentthatadaptsitselfbasedonthecurrentstatus
ofthestudentagent,enablingtheteachertoeffectivelyguidethestudentbeyondthespacecovered
bytheexpertstate-actionpairs.
InverseReinforcementLearning IRLisintroducedin[13]tolearntheintrinsicrewardfunction
ofanexpertandacquiretheexpertpolicyfromitbyiterativelyoptimizinganagentspolicyandthen
reward function. Apprenticeship learning builds on IRL and is proposed to represent the reward
functionasalinearcombinationoffeatures[14]. MaximumEntropyInverseReinforcementLearning
explicitlyaddressesthenoiseinexpertdemonstrationstobetterrecovertherewardfunction[15].
Several works extended IRL by incorporating negative demonstrations into the learning process
to further improve behavior modeling [16–18]. Guided Cost Learning uses a neural network to
approximatetherewardfunctionbyusingmaximumentropymethodsacrosscontinuousstate-action
spaces. [19]. AIRLproposesanadversarialrewardlearningframeworktoaddressthescalability
issuesofclassicalapproaches[4]. IQ-Learncombinesexpertdatawithagentexperiencestolearn
bothrewardandpolicywithasingleQ-function[20].Additionally,apipelineisproposedthatenables
IRL to handle unstructured, real-world data [21]. XIRL introduces cross-embodiment scenarios,
openingupanewdirectioninIRLresearch[22].
DespitetheadvancementsinIRL,thecomputationalefficacyofthelearningprocessandscalabilityto
complexproblemsremainopenchallenges[23]. Themainreasonfortheselimitationsistheiterative
sequential learning framework employed in IRL. We solve this efficacy problem by learning the
policyandtherewardfunction, viatrainingastudentagentandateacheragent, inasinglejoint
learningprocess.
3 Background
3.1 Preliminaries
AstandardMarkovDecisionProcess(MDP)isdefinedby(S,A,R,T,K,γ). S isthestatespace
thatconsistsofallpossibleenvironmentstatess, andAisactionspacethatcontainsallpossible
environmentactionsa. R = R(s,a) : S × A → Ristherewardfunction. T = {P(·|s,a)}is
thetransitiondynamicswhereP(·|s,a)isdefinedasanunknownstatestatetransitionprobability
functionupontakingactiona∈Ainstates∈S.K(s)istheinitialstatedistribution,i.e.,s ∼K(s)
0
and γ is the discount factor. The policy π = π(a|s) : S → A is a mapping from states to
actions. Inthiswork,weconsiderγ-discountedinfinitehorizonsettings. Following[3],expectation
with respect to the policy π ∈ Π refers to the expectation when actions are sampled from π(s):
E [R(s,a)] ≜ E [(cid:80)∞ γtR(s ,a )],wheres issampledfromaninitialstatedistributionK(s),
π π t=0 t t 0
a isgivenbyπ(·|s )ands isdeterminedbytheunknowntransitionmodelasP(·|s ,a ). The
t t t+1 t t
unknownrewardfunctionR(s,a)generatesarewardgivenastate-actionpair(s,a). Weconsidera
settingwhereR=R(s,a)isparameterizedbyθasR (s,a)∈R[19].
θ
Ourworkconsidersanimitationlearningproblemfromexperttrajectories,eachconsistingofstatess
andactionsa. Thesetofexperttrajectoriesτ aresampledfromanexpertpolicyπ ∈Π,where
E E
Πisthesetofallpossiblepolicies. Weassumethatwehaveaccesstomexperttrajectories,allof
whichhaventime-steps,τ ={(si,ai),(si,ai),...,(si,ai)}m .
E 0 0 1 1 n n i=1
3.2 ReinforcementLearning(RL)
Reinforcementlearningseekstofindanoptimalpolicythatmaximizesthediscountedcumulative
rewardgivenfromtherewardfunctionR=R(s,a). Inthiswork,weincorporateentropyregular-
izationusingtheγ-discountedcasualentropyfunctionH(π) = E [−logπ(a|s)][3,24]. TheRL
π
problemwithaparameterizedrewardfunctionandentropyregularizationisdefinedas
RL(R (s,a))=π∗ =argmaxE [R (s,a)]+H(π). (1)
θ π θ
π
33.3 InverseReinforcementLearning(IRL)
Givensampletrajectoriesτ ofanoptimalexpertpolicyπ ,inversereinforcementlearningIRL(τ )
E E E
seekstorecoverarewardfunctionR∗(s,a)thatmaximallyrewardsbehaviorthatwassampledfrom
θ
theexpertpolicyπ . Inotherwords,IRLaimstofindarewardfunctionR∗(s,a)thatsatisfies
E θ
∞ ∞
(cid:88) (cid:88)
E [ γtR∗(s ,a )]≥E [ γtR∗(s ,a )] ∀π. (2)
πE θ t t π θ t t
t=0 t=0
When IRL recovers the optimal reward function, R∗(s,a) would result in a policy that ex-
θ
hibits expert behavior when optimized with reinforcement learning: RL(R∗(s,a)) = π∗ =
argmax E [(cid:80)∞ γtR∗(s ,a )]. θ
π π t=0 θ t t
Since only the trajectories created by the expert policy are observed, expectations are estimated
fromthesamplesinτ . Inotherwords,IRLseekstofindtherewardfunctioninwhichtheexpert
E
policyperformsbetterthananyotherpolicy,wheretheexpectationoftheexpertisestimatedthrough
samples. WithentropyregularizationH(π),maximumcasualentropyinversereinforcementlearning
[15]canbedefinedas
(cid:16) (cid:17)
IRL(τ )= argmax E [R (s,a)]−max(E [R (s,a)]+H(π)) . (3)
E
Rθ(s,a)∈R
s,a∈τE θ
π
π θ
3.4 AdversarialImitationLearning(AIL)
Incontrasttoinversereinforcementlearning,imitationlearningaimstodirectlyapproximatethe
expertpolicyfromgivenexperttrajectorysamples. Itcanbeformulatedas
IL(τ )=argminE [L(π(·|s),a)], (4)
E (s,a)∼τE
π
whereLisalossfunction,thatcapturesthedifferencebetweenpolicyandexpertdata.
GAIL[3]extendsimitationlearningtoanadversarialsettingbyquantifyingthedifferencebetween
policies of the agent and the expert with a discriminator D (s,a), parameterized by ϕ. The dis-
ϕ
criminator’sfunctionistodifferentiatebetweenexpert-generatedstate-actionpairs(s,a)∼τ and
E
non-expertstate-actionpairs(s,a)∈/ τ . Itsoptimizationproblemisdefinedas
E
maxE [log(D (s,a))]+E [log(1−D (s,a))]. (5)
ϕ
(s,a)∼τE ϕ (s,a)∈/τE ϕ
Usingthediscriminator,thegoalofGAIListofindtheoptimalpolicythatminimizesthisdifference
metricwhilemaximizinganentropyconstraintbytrainingthediscriminatorandthepolicyatthesame
time. Theoptimizationproblemcanbeformulatedasazero-sumgamebetweenthediscriminator
D (s,a)andthepolicyπ,representedby
ϕ
minmaxE [logD (s,a)]+E [log(1−D (s,a))]−λH(π). (6)
π ϕ
π ϕ τE ϕ
Inotherwords,therewardfunctionthatismaximizedbythepolicyisdefinedasasimilarityfunction,
expressedasR(s,a)=−log(D (s,a)).
ϕ
4 RILe: ReinforcedImitationLearning
WeproposeReinforcedImitationLearning(RILe)tolearntherewardfunctionandacquireapolicy
thatemulatesexpert-likebehaviorsimultaneouslyinonelearningprocess. Ourframeworkconsistsof
threekeycomponents: astudentagent,adiscriminator,andateacheragent(Figure1).
Intuitively,thestudentagentlearnstheactionpolicyandtheteacheragentlearnstherewardfunction.
BothagentsaretrainedsimultaneouslyviaRLwiththehelpofanadversarialdiscriminator. The
discriminatorenablestrainingthepolicyandrewardfunctioninonelearningprocessandpermits
4Figure1: Frameworkoverview. Theframeworkconsistsofthreekeycomponents: astudentagent,
ateacheragent,andadiscriminator. (1)Thestudentagentlearnsapolicyπ byinteractingwithan
S
environment. (2)BasedonthestateactionpairofthestudentagentsT =(sS,aS),theteacheragent
learnsarewardfunctionasapolicyπ by(3)choosinganactionaT thatthenbecomestherewardof
T
thestudentagentaT =rS. (4)TheteacheragentforwardsthesT =(sS,aS)tothediscriminator.
(5)Thediscriminatorcomparesstudentstate-actionpairwithexpertdemonstrationssD and(6)the
rewardstheteacherbasedontheirsimilarityrT =D(sT).
replicatingexpertbehaviorthroughsimilarityratherthansupervisedstateactionmapping. However,
uniquelyinRILe,thediscriminatorguidestherewardlearningprocessinsteadofdirectlyguiding
policy learning. This setup enables the teacher agent to learn a reward function separately by
continuously adapting to changes in the student’s policy and guiding the student based on the
discriminator’sfeedback. Asaresult,theteachercanunderstanddifferentregionsofthestate-action
spacethroughstudent’sinteractions,effectivelyguidingitbeyondthepotentiallynoisyandimperfect
expertstate-actionpairs.
In the following, we define the components of RILe and explain how they can efficiently learn
behaviorfromimperfectdata.
StudentAgent Thestudentagentaimstolearnapolicyπ byinteractingwithanenvironmentina
S
standardRLsettingwithinanMDP.ForeachofitsactionsaS ∈A,theenvironmentreturnsanew
statesS ∈S. However,ratherthanfromahand-craftedrewardfunction,thestudentagentreceives
itsrewardfromthepolicyoftheteacheragentπ . Therefore,therewardfunctionisrepresented
T
bytheteacherpolicy. Thus,thestudentagentisguidedbytheactionsoftheteacheragent,i.e.,the
actionoftheteacheristherewardofthestudent: rS =π ((sS,aS)). Theoptimizationproblemof
T
thestudentagentisthendefinedas
min−E [π
(cid:0) (sS,aS)(cid:1)
]. (7)
πS
(sS,aS)∼πS T
Discriminator Thediscriminatordifferentiatesbetweenexpert-generatedstate-actionpairs(s,a)∼
τ and state-action pairs stemming from the policy of the student (s,a) ∼ π . In RILe, the
E S
discriminator is defined as a feed-forward deep neural network, parameterized by ϕ. Hence, the
optimizationproblemis
maxE [log(D (s,a))]+E [log(1−D (s,a))]. (8)
ϕ
(s,a)∼τE ϕ (s,a)∼πS ϕ
To provide effective guidance, the discriminator needs to accurately distinguish whether a given
state-actionpairoriginatesfromtheexpertdistribution(s,a)∼τ ornot(s,a)∈/ τ . Thefeasibility
E E
of this discrimination has been demonstrated by GAIL [3]. The according lemma and proof are
presentedintheAppendixA.
5TeacherAgent Theteacheragentaimstoguidethestudenttoapproximateexpertbehaviorby
operatingasitsrewardmechanism. Sincetheteacheragenttakestheroleofarewardfunctionfor
thestudent,anewMDPisdefinedfortheteacheragent: MDP : (S ,A ,R ,T ,K,γ),where
T T T T T
S : S xAisthestatespacethatconsistsall possiblestateactionpairsfromthestandardMDP,
T
definedin3.1. A istheactionspace,amappingfromS → R,sotheactionisascalarvalue.
T T
R (s,a) is the reward function where s ∈ S and a ∈ A . T = {P(·|s,a)} is the transition
T T T T
dynamics where P(·|s,a) is defined as the state distribution upon taking action a ∈ A in state
T
s∈S . K istheinitialstatedistribution.
T T
Theteacheragentlearnsapolicyπ thatproducesadequaterewardsignalstoguidethestudentagent,
T
bylearninginastandardRLsetting,withinMDP . SincethestatespaceofMDP isdefinedover
T T
state-actionpairsofMDP ,thestateoftheteachercomprisesthestate-actionpairofthestudent
S
sT =(sS,aS)∈S . ItgeneratesascalaractionaT whichisgiventothestudentagentasreward
T
rS,andboundedbetween-1to1. Tohelptheteacherunderstandhowitsactionsimpacttherewardit
receives,wedefinetherewardfunctionsuchthatitmultipliesthediscriminator’soutputbyasigmoid
functiondependentontheteacher’sactions. Therefore,theteacheragent’srewardfunctionisdefined
as RT = log(D (sT))σ(aT), where D (sT) is the output of the discriminator and σ(aT) is the
ϕ ϕ
sigmoidfunctionappliedtotheteacher’sownactions. Consideringonlythediscriminator’soutputin
theteacheragent’srewardfunctionwouldresultinarewardindependentoftheteacher’sactions.
Theoptimizationproblemoftheteachercanbedefinedas
maxE [log(D (s,a))σ(aT)]. (9)
πT (s a, Ta ∼)∼ ππ TS ϕ
RILe RILecombinesthethreecomponentsdefinedpreviouslyinordertofindastudentpolicythat
mimicsexpertbehaviorspresentedinτ . InRILe,thestudentpolicyπ andtheteacherpolicyπ
E S T
canbetrainedviaanysingle-agentonlinereinforcementlearningmethod. Thetrainingalgorithmis
giveninAppendixC.Overall,thestudentagentaimstorecovertheoptimalpolicyπ∗ definedas
S
(cid:34) ∞ (cid:35)
π∗ =argmaxE (cid:88) γt[π (cid:0) (sS,aS)(cid:1) . (10)
S (sS,aS)∼πS T t t
πS
t=0
Atthesametime,theteacheragentaimstorecovertheoptimalpolicyπ∗ as
T
(cid:34) ∞ (cid:35)
(cid:88)
π∗ =argmaxE γt[log(D (sT))σ(aT) . (11)
T πT asT T∼ ∼π πS
T t=0
ϕ t
To prove that the student agent can learn expert-like behavior, we need to show that the teacher
agentlearnstogivehigherrewardstostudentexperiencesthatmatchwiththeexpertstate-actionpair
distribution,asthiswouldenableastudentpolicytoeventuallymimicexpertbehavior.
Lemma1:GiventhediscriminatorD ϕ,theteacheragentoptimizesitspolicyπθT viapolicygradients
toproviderewardsthatguidethestudentagenttomatchexpert’sstate-actiondistributions.
Proof for Lemma 1 The assumptions are presented in Appendix A. The expert’s state-action
distributionisdenotedbyp (s,a). Theroleoftheteacheristoprovidearewardsignaltothe
expert
studentthatencouragestheapproximationofp (s,a)ascloselyaspossible.
expert
WehaveD :S×A→[0,1]asthediscriminator,parameterizedbyϕ,whichoutputsthelikelihood
ϕ
that a given state-action pair (s,a) originates from the expert, as opposed to the student. The
teacher’spolicyπθT,parameterizedbyθ T,aimstomaximizethelikelihoodunderthediscriminator’s
assessment,thusencouragingthestudentagenttogeneratestate-actionpairsdrawnfromadistribution
resemblingp (s,a).
expert
TheValueandQfunctionsoftheteacher,conditionedontherewardsprovidedbythediscriminator,
are defined in terms of expected cumulative discriminator rewards. The value function for the
teacher’spolicyparametersθ atstates ,simplifiedfromsT forbetterreadability,isgivenby:
T t t
(cid:34) ∞ (cid:35)
(cid:88)
VθT(s )=E γkr |s , (12)
t πθT t+k t
k=0
6wherer =D (s ). Similarly,theQ-functionfortakingactiona instates andthenfollowing
t+k ϕ t+k t t
policyπθT canbewrittenas:
QθT(s ,a )=D (s )+γE (cid:2) VθT(s )|s ,a (cid:3) . (13)
t t ϕ t πθT t+1 t t
Theteacher’spolicyoptimizationisdonebymaximizingthefollowingclippedsurrogateobjective
function:
LCLIP(θ T)=E
(st,at)∼πθT(cid:104) min(cid:16) ππ θTθ oT ld(a (at| ts |st)
t)AθTold(s t,a t),
(14)
clip(
ππ θTθ oT ld(a (at| ts |st)
t),1−ϵ,1+ϵ)AθTold(s t,a
t)(cid:17)(cid:105)
,
whereAθTold(s t,a t)istheadvantagefunctioncomputedasQθTold(s t,a t)−VθTold(s t),definedwith
respecttotheteacher’soldpolicyparametersθ .
Told
Byexpressingtheadvantageusingtherewardfromthediscriminator,weexplicitlytiethepolicy
gradientupdatestothediscriminator’soutput,emphasizingtheshapingofπθS tomatchp expert(s,a):
AθTold(s t,a t)=D ϕ(s t)+γE
πθS
(cid:2) VθTold(s t+1)(cid:3) −VθTold(s t). (15)
AθTold((sS t,aS t),a t)=D ϕ((sS t,aS t))+γE
πθS
(cid:2) VθTold((sS t+1,aS t+1))(cid:3) −VθTold(sS t,aS t). (16)
Duringeachpolicyupdate,theobjectiveinEquation14ismaximized,drivingparameterupdates
tofavoractionsthatelicithigherrewardsfromthediscriminator–effectivelytheactionsthatbetter
alignwithexpertbehavior:
θ ←θ +α ∇ LCLIP(θ ). (17)
T T π θT T
TheupdateruleinEquation17,drivenbycumulativerewardsfromthediscriminator,incrementally
adaptstheteacher’spolicytoreinforcestudentbehaviorsthatareindistinguishablefromthoseofthe
expertaccordingtoD . Theteacher’spolicyisguidedtofacilitatethestudent’sapproximationof
ϕ
p (s,a),therebyfulfillingitsroleintheimitationlearningprocess.
expert
5 Experiments
WeevaluatetheperformanceofRILebyaddressingthreekeyquestions. First,canRILerecovera
feasiblepolicywhentrainedtoimitatefromnoisyexpertdata? Toanswerthisquestion,wecompare
RILe’sperformancewithcommonimitationlearningbaselinesinagridworldsettingwherepolicies
arelearnedfromnoisyexpertdata. Second,canRILeuseexpert-dataexplicitlytoimitateexpert
behavior? We use MuJoCo [25, 26] to evaluate RILe’s performance when expert data is leaked
totheagents. Third,isRILeefficientandscalabletohigh-dimensionalcontinuouscontroltasks?
WeanswerthisbyusingLocoMujoco[27]asthebenchmarkforevaluatingRILe’seffectivenessin
imitatingmotion-capturedatawithincontinuousroboticcontroltasks.
BaselinesWecomparedRILewithfourbaselinemethods: Behavioralcloning(BC[7,28]),adversar-
ialimitationlearning(GAIL[3]),adversarialinversereinforcementlearning(AIRL[4])andinverse
reinforcementlearning(IQ-Learn[20]).
5.1 NoisyExpertData
TodemonstratetheadvantageofusingRLtolearntherewardfunctioninRILe,asopposedtoderiving
therewarddirectlyfromthediscriminatorinAILandAIRL,wedesigneda5x5MiniGridexperiment.
Thegridconsistsof4lavatilesthatimmediatelykilltheagentifitstepsinit,representingterminal
conditions. Thegoalconditionoftheenvironmentisreachingthegreentile.
Theexpertdemonstrationsareimperfect,depictinganexpertthatpassesthroughalavatilewithout
beingkilledandstillreachesthegreengoaltile.Usingthisdata,wetrainedtheadversarialapproaches
withaperfectdiscriminator,whichprovidesarewardof0.99ifthevisitedstate-actionpairstems
fromtheexpertand0.01otherwise. Thesevalueswerechosenover1and0becausebothAIRLand
GAILusethelogarithmofthediscriminatoroutputtocalculaterewards.
7(a)Experttraj. (b)RILetraj. (c)GAILtraj. (d)AIRLtraj. (e)IQLearntraj.
(f)RILeval. (g)GAILval. (h)AIRLval. (i)IQLearnval.
Figure2: Ina5x5gridenvironmentwithlava,(a)theexperttrajectoryischaracterizedbynoisydata
thatpassesthroughlavawithoutresultingindeath. (c)GAIL,(d)AIRLand(e)IQLearnlearnto
imitatetheexpert’spathprecisely,leadingthemtoeithergetstucknearthelavaorenteritandperish.
(b)RILeavoidsthenoisydata,bettermimicstheexpertinlaterstages,andsuccessfullyreachesthe
goal. Subfigures(f-i)displaythevaluetablesforRILe,GAIL,AIRL,andIQLearnrespectively. The
optimalpath,derivedfromtherewardoftheteacherordiscriminator,ishighlightedwithgreenlines.
ResultsarepresentedinFig.2. Thevaluegraphs(Fig.2e-g)areattainedbycomputingthevalue
(cid:80) (cid:80)
ofeachgridcellc as D(c ,a)forAIRLandGAIL,and π (c ,a)forRILe. Fig.2a
i a∈A i a∈A T i
showstheexperttrajectory.
GAIL(Fig.2c),AIRL(Fig.2d)andIQLearn(Fig.2e)failtoreachthegoal,astheiragentseither
becomestuckoraredirectedintolava. Incontrast, RILe(Fig.2d)successfullyreachesthegoal,
demonstratingitsabilitytonavigatearoundimperfectionsinexpertdata. Thedifferenceinthevalue
graphsbetweenRILeandthebaselinesintuitivelyexplainsthisoutcome.InAILandAIRL(Fig.2f-g),
theoptimalpaths,definedbytheactionsmostrewardedbytheirdiscriminators,followthenoisy
expertdataperfectly. Similarly,inIQLearn,theagenttriestomatchexpertstate-actionsasclosely
aspossible,minimizinganydeviationfromtheexperttrajectory. Incontrast,RILe’steacheragent,
trainedusingRL,addsanextradegreeoffreedomintheadversarialIL/IRLsetting. Byproviding
rewardsthatmaximizecumulativereturnsfromthediscriminator,ratherthanderivingthereward
directlyfromitsoutput,thevaluegraph(Fig.2f)canlearntocircumventthelavatileinorderto
follow the expert trajectory to the goal. Consequently, the optimal path of the student agent can
overcomethesub-optimalstatesuggestedbythenoisyexpertdemonstration. Sincethestudentagent
isguidedbytheteachertoalsomatchtheexperttrajectory,itremainsclosetothispathafterpassing
thelavatiles.
5.2 ExpertDataLeakageinTraining
WeinvestigatewhetherRILecanuseexpertdataexplicitlytoimitateexpertbehaviorbetter. Thus,
weanalyzehowtheperformanceandconvergencetimewillbeeffectedwhenweleakexpertdatato
thereplaybuffersofbothstudentandteacheragent. Forthisexperiment,weemployedMuJoCo’s
Humanoidenvironment[26,25]andusedasingletrajectoryfromtheexpertdataof[20]. Theexpert
dataisleakedtoagents’replaybufferswithfollowingpercentages: 25%,50%,75%,100%. 25%
signifiesthat25%ofthereplaybuffer’sdataissourcedfromtheexperttrajectory,whiletheremaining
75%originatesfromtheagents’interactionswiththeenvironment. 100%indicatesthattheagents’
environmentinteractionsareentirelydisregarded,withonlyexpertdatabeingusedfortraining.
AsshowninFigure3,leakingexpertdatatobuffersslightlyworsenedperformancebutsignificantly
reducedtheconvergencetime. However,whenenvironmentinteractionswereentirelyreplacedwith
expertdata,thepolicy’sperformancedeclinedsubstantially. Forcomparison,resultsfromIQLearn
and Behavioral Cloning are also presented, as both methods rely exclusively on expert data for
training. Neithermethod,however,achievedthesamelevelofperformanceasRILe.
8NormalizedScores NormalizedSteps
Figure3: Scores(left)representingthetotalrewardsachieved,andconvergencesteps(right)repre-
sentingthenumberofstepstoconvergence,arepresentedagainstthepercentageofleakedexpert
data. ForRILe,thex-axisshowsthepercentageofleakedexpertdatainthestudentagent’sbuffer,
whilethey-axisshowsitintheteacheragent’sbuffer. Scores/Stepsarenormalizedbyno-leakage
trainingresultsofRILe. BehavioralCloningfailstoachieveacomparablescoretoRILe. While
IQLearnconvergesfasterthantheno-leakageRILe,itstillfallsshortintermsofoverallperformance.
5.3 Motion-CaptureDataImitationforRoboticContinuousControl
InLocoMujoco,theobjectiveistolearnpoliciesthatcanaccuratelyimitatethecollectedMoCap
dataofvariousrobotictasks. LocoMujocoischallengingdueitshighdimensionality. TheMoCap
dataonlyconsistsofstatesoftherobot,notitsactions,whichpreventstheuseofBehavioralCloning
andIQLearninthisanalysis,sincetheybothrequirestohavefullexpertdatawithstatesandactions.
ItshouldbenotedthatLocoMujocoenvironmentsdonothaveterminalorgoalconditions.
ResultsarepresentedinTable1forbothtestseedsandvalidationseeds. RILeoutperformsbaselines
especiallyfornewinitialconditionsintroducedbyvaryingseeds.
Table1: TestandvalidationresultsonsevenLocoMujocotasks.
TestSeeds ValidationSeeds
RILe GAIL AIRL RILe GAIL AIRL Expert
AtlasWalk 870.6 792.7 300.5 895.4 918.6 356 1000
AtlasCarry 850.8 669.3 256.4 889.7 974.2 271.9 1000
TalosWalk 842.5 442.3 102.1 884.7 675.5 103.4 1000
TalosCarry 220.1 186.3 134.2 503.3 338.5 74.1 1000
UnitreeH1Walk 898.3 950.2 568.1 980.7 965.1 716.2 1000
UnitreeH1Carry 788.3 634.6 130.5 850.6 637.4 140.9 1000
HumanoidWalk 831.3 181.4 80.1 970.3 216.2 78.2 1000
6 Discussion
We have demonstrated through our experiments that RILe outperforms baseline models across
varioussettings. WhileRILesurpassesthestate-of-the-artadversarialIL/IRLmethods,italsohasits
limitations. Themainchallengeistrainingthepolicywithachangingrewardfunction,whichleads
tounstablepolicyupdatesforthestudentagent. Toaddressthisissue,weformulatetheteacher’s
rewardbycorrelatingitsactionswiththeoutputofthediscriminator,insteadofonlyrelyingonthe
discriminator. However, instabilityremainsachallenge andcausesslower learningoftheaction
policyinRILecomparedtoAIL.Futureworkshouldestablishboundsfortherewardagentupdates
toenhancethestabilityofthestudentagent’slearningprocess.
Moreover,balancingthelearningratesofthediscriminatorandthepoliciesischallenging,particularly
becausethediscriminatortendstooverfitthedataquickly. Thisoverfittingmakesitdifficultforthe
teacheragenttofindarewardforthestudentthatcanchallengethediscriminator. Whileadjusting
thediscriminator’slearningrateonacase-by-casebasishelps, futureresearchshouldfocusona
fundamentalsolution,suchasreplacingthediscriminatorwithamorerobustsimilaritycomponent.
9UniquelyinRILe,thediscriminatorguidestherewardlearningprocessinsteadofdirectlyguiding
policylearning. Therefore,therewardfunctionislearned,notdirectlyderivedfromthediscriminator.
ThisdistinctionallowsRILetoprovidemeaningfulguidanceevenwhenthediscriminatoroverfitsto
noisyexpertdata. IntheLocoMujocobenchmark,whereperfectexpertdataisavailable,weargue
thatRILe’sabilitytoaccountfortheagent’sexplorationeffortsduringtraining,inadditiontoits
behavioralsimilaritywiththeexpert,resultsinasuperiorgeneralizationcapabilitycomparedtoAIL
andAIRL.
References
[1] V.Mnih,K.Kavukcuoglu,D.Silver,A.Graves,I.Antonoglou,D.Wierstra,andM.Riedmiller,
“Playingatariwithdeepreinforcementlearning,”arXivpreprintarXiv:1312.5602,2013.
[2] D.Silver,T.Hubert,J.Schrittwieser,I.Antonoglou,M.Lai,A.Guez,M.Lanctot,L.Sifre,
D.Kumaran,T.Graepel,etal.,“Ageneralreinforcementlearningalgorithmthatmasterschess,
shogi,andgothroughself-play,”Science,vol.362,no.6419,pp.1140–1144,2018.
[3] J.HoandS.Ermon,“Generativeadversarialimitationlearning,”Advancesinneuralinformation
processingsystems,vol.29,2016.
[4] J.Fu,K.Luo,andS.Levine,“Learningrobustrewardswithadverserialinversereinforcement
learning,”inInternationalConferenceonLearningRepresentations,2018.
[5] B.Zheng,S.Verma,J.Zhou,I.W.Tsang,andF.Chen,“Imitationlearning:Progress,taxonomies
andchallenges,” IEEETransactionsonNeuralNetworksandLearningSystems, no.99, pp.
1–16,2022.
[6] S.Levine,A.Kumar,G.Tucker,andJ.Fu,“Offlinereinforcementlearning: Tutorial,review,
andperspectivesonopenproblems,”arXivpreprintarXiv:2005.01643,2020.
[7] M.BainandC.Sammut,“Aframeworkforbehaviouralcloning.”inMachineIntelligence15,
1995,pp.103–129.
[8] S.Ross,G.Gordon,andD.Bagnell,“Areductionofimitationlearningandstructuredprediction
to no-regret online learning,” in Proceedings of the fourteenth international conference on
artificialintelligenceandstatistics. JMLRWorkshopandConferenceProceedings,2011,pp.
627–635.
[9] Y.Li,J.Song,andS.Ermon,“Infogail: Interpretableimitationlearningfromvisualdemonstra-
tions,”AdvancesinNeuralInformationProcessingSystems,vol.30,2017.
[10] T. Hester, M. Vecerik, O. Pietquin, M. Lanctot, T. Schaul, B. Piot, D. Horgan, J. Quan,
A.Sendonaris,I.Osband,etal.,“Deepq-learningfromdemonstrations,”inProceedingsofthe
AAAIConferenceonArtificialIntelligence,vol.32,no.1,2018.
[11] I. Kostrikov, O. Nachum, and J. Tompson, “Imitation learning via off-policy distribution
matching,”inInternationalConferenceonLearningRepresentations,2020.
[12] S.Toyer,R.Shah,A.Critch,andS.Russell,“Themagicalbenchmarkforrobustimitation,”
AdvancesinNeuralInformationProcessingSystems,vol.33,pp.18284–18295,2020.
[13] A.Y.NgandS.J.Russell,“Algorithmsforinversereinforcementlearning,”inProceedingsof
theSeventeenthInternationalConferenceonMachineLearning,2000,pp.663–670.
[14] P. Abbeel and A. Y. Ng, “Apprenticeship learning via inverse reinforcement learning,” in
Proceedingsofthetwenty-firstinternationalconferenceonMachinelearning,2004,p.1.
[15] B. D. Ziebart, A. L. Maas, J. A. Bagnell, A. K. Dey, et al., “Maximum entropy inverse
reinforcementlearning.”inProceedingsoftheAAAIConferenceonArtificialIntelligence,vol.8.
Chicago,IL,USA,2008,pp.1433–1438.
[16] K.Lee,S.Choi,andS.Oh,“Inversereinforcementlearningwithleveragedgaussianprocesses,”
in2016IEEE/RSJInternationalConferenceonIntelligentRobotsandSystems(IROS). IEEE,
2016,pp.3907–3912.
10[17] K. Shiarlis, J. Messias, and S. Whiteson, “Inverse reinforcement learning from failure,” in
Proceedingsofthe2016InternationalConferenceonAutonomousAgents&MultiagentSystems,
2016,pp.1060–1068.
[18] K.Bogert, J.F.-S.Lin, P.Doshi, andD.Kulic, “Expectation-maximizationforinverserein-
forcementlearningwithhiddendata,”inProceedingsofthe2016InternationalConferenceon
AutonomousAgents&MultiagentSystems,2016,pp.1034–1042.
[19] C.Finn, S.Levine, andP.Abbeel, “Guidedcostlearning: Deepinverseoptimalcontrolvia
policy optimization,” in International conference on machine learning. PMLR, 2016, pp.
49–58.
[20] D.Garg,S.Chakraborty,C.Cundy,J.Song,andS.Ermon,“Iq-learn: Inversesoft-qlearningfor
imitation,”AdvancesinNeuralInformationProcessingSystems,vol.34,pp.4028–4039,2021.
[21] A. S. Chen, S. Nair, and C. Finn, “Learning generalizable robotic reward functions from"
in-the-wild"humanvideos,”inRobotics: ScienceandSystems,2021.
[22] K.Zakka,A.Zeng,P.Florence,J.Tompson,J.Bohg,andD.Dwibedi,“Xirl:Cross-embodiment
inversereinforcementlearning,”inConferenceonRobotLearning. PMLR,2022,pp.537–546.
[23] S.AroraandP.Doshi,“Asurveyofinversereinforcementlearning: Challenges,methodsand
progress,”ArtificialIntelligence,vol.297,p.103500,2021.
[24] M. Bloem and N. Bambos, “Infinite time horizon maximum causal entropy inverse
reinforcementlearning,”53rdIEEEConferenceonDecisionandControl,pp.4911–4916,2014.
[Online].Available: https://api.semanticscholar.org/CorpusID:14981371
[25] G.Brockman,V.Cheung,L.Pettersson,J.Schneider,J.Schulman,J.Tang,andW.Zaremba,
“Openaigym,”2016.
[26] E.Todorov,T.Erez,andY.Tassa,“Mujoco: Aphysicsengineformodel-basedcontrol,”in
2012IEEE/RSJinternationalconferenceonintelligentrobotsandsystems. IEEE,2012,pp.
5026–5033.
[27] F. Al-Hafez, G. Zhao, J. Peters, and D. Tateo, “Locomujoco: A comprehensive imitation
learningbenchmarkforlocomotion,”in6thRobotLearningWorkshop,NeurIPS,2023.
[28] S. Ross and D. Bagnell, “Efficient reductions for imitation learning,” in Proceedings of the
thirteenthinternationalconferenceonartificialintelligenceandstatistics. JMLRWorkshop
andConferenceProceedings,2010,pp.661–668.
11A JustificationofRILe
Assumptions:
• Thediscriminatorlosscurveiscomplexandthediscriminatorfunction,D (s,a),issuffi-
ϕ
cientlyexpressivesinceitisparameterizedbyaneuralnetworkwithadequatecapacity.
• For the teacher’s and student’s policy functions (πθT) and (πθS), and the Q-functions
(QθS), each is Lipschitz continuous with respect to its parameters with constants
(L ),(L ),and(L ),respectively. Thismeansforall(s,a)andforanypairofparameter
θT θS Q
settings(θ,θ′):[|πθ(s,a)−πθ′(s,a)|≤L |θ−θ′|,][|Qθ(s,a)−Qθ′(s,a)|≤L |θ−θ′|.]
θ Q
A.1 Lemma2:
ThediscriminatorD ,parameterizedbyϕwillconvergetoafunctionthatestimatestheprobability
ϕ
ofastate-actionpairbeinggeneratedbytheexpertpolicy,whentrainedonsamplesgeneratedby
bothastudentpolicyπθS andanexpertpolicyπ E.
ProofforLemma2: Thetrainingobjectiveforthediscriminatorisframedasabinaryclassification
problem over mini-batches from expert demonstrations and student-generated trajectories. The
discriminator’slossfunctionL (ϕ)isthebinarycross-entropyloss,whichforamini-batchofsizen
D
isdefinedas:
n
1 (cid:88)(cid:104) (cid:105)
L (ϕ)=− y log(D (s ,a ))+(1−y )log(1−D (s ,a )) , (18)
D n i ϕ i i i ϕ i i
i=1
where(s ,a )aresampledstate-actionpairsfromthecombinedreplaybufferD =D ∪D ,with
i i S E
correspondinglabelsy indicatingwhetherthepairisfromtheexpert(y =1)orthestudent(y =0).
i i i
ThestochasticgradientdescentupdateruleforoptimizingL (ϕ)isthengivenby:
D
ϕ←ϕ−η ∇ L (ϕ), (19)
D ϕ D
whereη isthelearningrateforthediscriminator.
D
Throughiterativeupdates,D (s,a)willconvergetoP(π |s,a),providedtheminimizationofL (ϕ)
ϕ E D
progressesaccordingtothetheoreticalfoundationsofstochasticgradientdescent. Theconvergence
reliesontheassumptionthatthestudent’spolicyπθS andtheexpertpolicyπ
E
inducestationary
distributionsofstate-actionpairs,suchthatthediscriminator’sdatasourceisconsistentlyrepresenting
bothpoliciesovertime.
ByminimizingL (ϕ),weseekϕ∗suchthat:
D
ϕ∗ =argminL (ϕ). (20)
D
ϕ
Undertypicalconditionsforconvergenceinstochasticgradientdescent,theconvergencetoalocal
minimum or saddle point can be guaranteed. The discriminator’s ability to distinguish between
studentandexpertpairsimprovesasL (ϕ)isminimized,implyingthatlim Dϕ∗(s,a)=
D nbatch→∞
P(π |s,a),wheren isthenumberofbatches.
E batch
B ComputeResources
ForthetrainingofRILeandbaselines,followingcomputationalsourcesareemployed:
• AMDEPYC774264-CoreProcessor
• 1xNvidiaRTX6000GPU
• 32GBMemory
12C Algorithm
Algorithm1RILeTrainingProcess
1: Initializestudentpolicyπ S andteacherpolicyπ T withrandomweights,andthediscriminatorD
withrandomweights.
2: InitializeanemptyreplaybufferB
3: foreachiterationdo
4: Sampletrajectoryτ S usingcurrentstudentpolicyπ S
5: Storeτ S inreplaybufferB
6: foreachtransition(s,a)inτ S do
7: CalculatestudentrewardRS usingteacherpolicy:
RS ←π (21)
T
8: Updateπ S usingpolicygradientwithrewardRS
9: endfor
10: SampleabatchoftransitionsfromB
11: TraindiscriminatorDtoclassifystudentandexperttransitions
maxE [log(D(s,a))]+E [log(1−D(s,a))] (22)
D
πS πE
12: foreachtransition(s,a)inτ S do
13: CalculateteacherrewardRT usingdiscriminator:
RT ←log(D(s,a)) (23)
14: Updateπ T usingpolicygradientwithrewardRT
15: endfor
16: endfor
13