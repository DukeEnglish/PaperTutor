Real2Code: Reconstruct Articulated Objects
via Code Generation
ZhaoMandi YijiaWeng DominikBauer ShuranSong
StanfordUniversity StanfordUniversity ColumbiaUniversity StanfordUniversity
Abstract
We present Real2Code, a novel approach to reconstructing articulated objects
viacodegeneration. Givenvisualobservationsofanobject,wefirstreconstruct
its part geometry using an image segmentation model and a shape completion
model. Wethenrepresenttheobjectpartswithorientedboundingboxes,whichare
inputtoafine-tunedlargelanguagemodel(LLM)topredictjointarticulationas
code. Byleveragingpre-trainedvisionandlanguagemodels,ourapproachscales
elegantly with the number of articulated parts, and generalizes from synthetic
training data to real world objects in unstructured environments. Experimental
resultsdemonstratethatReal2Codesignificantlyoutperformspreviousstate-of-
the-artinreconstructionaccuracy,andisthefirstapproachtoextrapolatebeyond
objects’structuralcomplexityinthetrainingset,andreconstructsobjectswithup
to 10 articulated parts. When incorporated with a stereo reconstruction model,
Real2Code also generalizes to real world objects from a handful of multi-view
RGBimages,withouttheneedfordepthorcamerainformation. 1
In: Unstructured RGBs Out: Interactable Digital Twins
Figure1: Weproposeanovelmethodforreconstructingarticulatedobjectsviacodegeneration,
leveragingpre-trainedlargelanguagemodels(LLMs). Real2Codetakesvisualobservationsasinput,
andperformsbothpart-levelgeometryreconstructionandjointprediction. Whenevaluatedonan
extensivesetofrealandsyntheticobjectswithvaryinglevelofkinematiccomplexity,Real2Codecan
reconstructcomplexarticulatedobjectswithupto10parts,andgeneralizetorealworldobjectsfrom
ahandfulofpose-freeRGBimages.
1 Introduction
Theabilitytoreconstructreal-worldobjectsinsimulation(real-to-sim)promisesvariousdownstream
applications:automatingassetcreationforbuildingVR/ARexperiences,enablingembodiedagentsto
verifytheirinteractioninsimulationbeforeexecutionintherealworld[1–3],orbuildinglarge-scale
Real2Code 1ProjectWebsite:https://real2code.github.io
4202
nuJ
21
]VC.sc[
1v47480.6042:viXra
Real2Code
Real2Codesimulationenvironmentsthatsupportdata-drivenpolicylearning[4]. Weareparticularlyinterested
in articulated objects, for both their ubiquity in household and industrial settings and the unique
challengestheyposeincontrasttosingle-bodyrigidobjects. Toreconstructarticulatedobjects,prior
learning-basedmethodstypicallytrainsupervised[5]ortest-time-optimized[6]modelsonsynthetic
objectswithsimplearticulationstructures(i.e.,oneortwomovingpartsperobject). Thisresults
inlimitedgeneralizationabilitytoobjectswithmorecomplexvisualappearancesandkinematics.
Inaddition,priorworkonlyprovidesobjectpartreconstructionsoflimitedquality: theextracted
meshesareoftenincompleteandthepredictedarticulationparametersrequiremanualcleanupbefore
beingusableforsimulation.
Wepropose Real2Code, a novelapproach to address theabove limitations. We representobject
articulationwithcodeprograms,anduselanguagemodelingtopredictthesecodeprogramsfrom
visualobservations. Thisformulationscaleselegantlywithobjects’structuralcomplexity: toprocess
anarticulatedobjectwithmultiplejoints,priormethodswouldrequireeitherchangingtheoutput
dimensionoftheirarticulationpredictionmodel,orrunmultipleinferencesonpairsofbefore-and
after-interactionobservationstopredictonejointatatime. Incontrast,thenext-tokenprediction
formulationinlanguagemodelingallowsgeneratingarbitrary-lengthoutputs,i.e.,themodelarchi-
tecture needs no adjustment to handle varying number of object joints. Whereas prior work on
shapeprograms[7]needstodefinetask-specificcodesyntax,werepresentobjectswithsimulation
codeinPython,whichtakesadvantageofrecentprogressinlargelanguagemodels(LLMs)thatare
pre-trainedwithcodegenerationcapabilities.
Althoughcapableatcodegeneration,LLMspre-trainedontextarenotasequippedatpredicting
accuratenumericalvaluesfromspatialgeometryinformation,whichisrequiredinourtaskinorder
toobtainarticulatedjointparameters. Toaddressthis,weproposetouseorientedboundingboxes
(OBBs)asanabstractionlayerthatsummarizestherawsensoryobservationtotheLLMinaconcise
yetprecisemanner. Givenpartialobservationsofanobject,wefirstperformpart-levelsegmentation
andreconstructionviaacombinationof2Dsegmentationanda3Dshapecompletionmodel;next,
OBBsareextractedfromthereconstructedobjectparts,andserveasinputtotheLLM.Insteadof
havingtoregresstocontinuousvalues,theLLMscanpredictjointsasaclassificationproblemby
selectingtheclosestOBBrotationaxisandboxedges.
Inunstructuredrealworldenvironments,anotherchallengeisthelackofaccuratedepthandcamera
information. Toaddressthis,weincorporateapre-traineddensestereoreconstructionmodel,namely
DUSt3R[8], intoourpipeline: weshowthedense2D-to-3Dpoint-mappredictionfromDUSt3R
canbecombinedwithourfine-tunedSAMmodeltoachieveview-consistent3Dsegmentation. As
aresult,Real2Codecanthenreconstructrealworldobjectsfromonlyahandfulofpose-freeRGB
images.
Formoresystematicevaluation,wevalidateReal2Codeonthewell-establishedPartNet-Mobility
dataset[9],usinganextensivetestsetofunseenobjectsthatcontainvariousnumbersofarticulated
parts.Comparedtothepriorstateoftheart,Real2Codesignificantlyimprovesboth3Dreconstruction
andjointpredictionaccuracy. Real2Codeistheonlyapproachtoreliablyreconstructobjectswith
more than three articulated parts, whereas prior methods fail completely on such objects. Fig. 1
highlightsourresultsonbothsyntheticmulti-partobjects(leftcolumnofinputimages),wherewe
showReal2Codecanreconstructobjectswithupto10articulatedparts,andgeneralizetorealworld
objects(rightcolumnofinputimages)usingRGBimagescapturedfromin-the-wildfurnitureobjects.
Insummary,ourcontributionsarethreefold:
1. WepresentReal2Code,anovelapproachtoreconstructingarticulatedobjectsfromahandfulof
unstructuredRGBimages. Weformulatejointpredictionasacodegenerationproblemandadapt
pre-trainedlargelanguagemodelstospecializeinthistask.
2. Weaddresspartreconstructionviakinematic-awareview-consistentimagesegmentationanda
learned3Dshapecompletionmodel,whichleadstohigh-qualitymeshextractionthatgeneralizesto
multi-partreal-worldobjects.
3. EmpiricalresultsdemonstratethatReal2Codesignificantlyoutperformsthepriorstateoftheart
atbotharticulationestimationandpartreconstruction. Tothebestofourknowledge,Real2Codeis
thefirstmethodtoaccuratelypredictobjectswithmorethanthreeparts,andgeneralizesbeyondthe
trainingdatasettoobjectswithupto10articulatedparts.
22 RelatedWork
LLMs for Visual Tasks. Pre-trained LLMs have been used for visual reasoning and grounding
tasks[10,11]. LLMs’code-generationcapabilityhasalsobeenexploitedforgeneratingprograms
thatsolvevisualtasks[12–14]. Theseworksusezero-shotpre-trainedLLMssuchasGPTs[15,16]
and require prompt engineering, such as providing in-context examples, to guide the model to
generatedesirableoutputs;incontrast,wedirectlyfine-tunetheweightsofacode-generationmodel
tospecializeinourarticulationpredictiontask,anddonotuseanyhand-craftedprompting.
ShapePrograms. Code-likeprogramshavebeenstudiedincomputervisionasacompactrepresen-
tationfor2Dand3Dshapes. Amainchallengeforlearningcodeprogramsisthelackofsupervision,
andpriorworkhasexploredusinglearneddifferentiablecodeexecutor[7],pseudo-labeling[17],dif-
ferentiablerendering[18],imitationlearningoncodesequences[19],orreinforcementlearning[20].
Morerecentworkhasexploredconstructinglarge-scaledatasetsofshapes[21]orscenelayouts[22]
andtrainsupervisedLLM-likemodelstogeneratecodeoutputs. Incontrasttoours,thesepriorwork
focusesoneitherindividualobjectshapesorscene-levelroomlayouts,butdoesnotestimatejoint
articulations. Inaddition,insteadofthetask-specificcodeprograms,suchascustomarily-designed
languagesyntax[7,17,22]orComputer-AidedDesign(CAD)code[19,21],werepresentobject
articulationwithPythoncodethat1)closesmatchesthepre-trainingdistributionofcode-generation
LLMs, which allows fine-tuning with limited data, and 2) can be directly executed by a physics
simulator[23],whichmakesthereconstructionmoreusableforandrequireslessmanualcleanup.
ArticulationModelEstimation. Priorworkhasinvestigatedestimatingposeandjointpropertiesof
articulatedobjectswithoutfullreconstruction. Acommonsetupistoassumephysicalinteractions
onanobjecttoinferitsarticulationinformation: classicalsampling-basedalgorithms[24,25]are
proposedtoestimatejointparametersbasedonsensoryinputsfromanobject’sdifferentconfiguration
states; other learning-based methods train end-to-end models to predict part-level segmentation,
kinematic structure, object part poses, or articulated joint parameters [26–36]. Some propose
specializedneuralnetworkarchitecturestoimproveestimationperformance[37–39]. Otherwork
focusesonlearningtoproposethemostinformativephysicalinteractionsonanobjecttohelprobot
manipulation[40],ortobetterisolateandsegmentarticulatedpartsandjoints[41]. Thesearticulation
estimationtasksprovideusefulmetricsfor3Dshapereasoning[28],andthepredictedobjectpose
andjointinformationareshownusefulforrobottasklearning[42–45]. However,priorworktypically
handlesobjectswithsimplestructure(i.e.,oneortwomovingparts)anddoesnotaddressfullobject
reconstruction.Incontrast,ourmethodhandlesobjectswithmorethantenmovingparts,andperforms
shapereconstructionviaextractingpartmeshes.
ArticulatedObjectReconstruction. Themostcloselyrelatedtooursaremethodsthatreconstruct
boththegeometryandjointsofarticulatedobjects. Apopularapproachistotrainend-to-endmodels
onsyntheticdatatojointlysegmentarticulatedpartsandpredictjointparameters,assumingeither
observationsfrominteractions[5,46–48]orsingle-stage[49–51]observations. Anotherapproach
usesper-objectoptimization[6,36]withouttraining. Basedonobservationsoftheobjectattwoor
moredifferentjointstates,itoptimizesforjointparameterstomatchobservedmotioncorrespondence
andoptionallyperforms3Dreconstructionbyextractingfromlearnedneuralrenderingfields. Most
existingmethodsassumeasinglejointanddonotscalewellwithnumberofjoints: forexample,to
handleanobjectwithN joints,methodslikeDitto[5]wouldneedtomovetheN jointsonebyone,
recordtheobservationsbeforeandaftereachinteraction,andrunN inferencesoneachobservation
pair. PARIS[6]wouldneedtooptimizeN neuralfieldsandjointparameters,whichmayleadto
amuch morecomplexoptimizationlandscape. The approachpresented in[36]handles multiple
jointsbutrequiresacompletesequenceofpoint-cloudobservationsandisnotabletoreconstruct3D
shapes.
3 Method
Weaddresstheproblemofreconstructingmulti-partarticulatedobjectsfromvisualobservations. An
articulatedobjectiscomposedofasetofrigid-bodypartsthatareconnectedviajoints. Weassume
jointtypesareeitherprismaticorrevolute: aprismaticjointisparameterizedbyajointaxisup ∈R3
and a translation offset d; a revolute joint is parameterized by a position pr ∈ R, a rotation axis
ur ∈R3,andarotationangleθ. ForanobjectwithN movingparts,weassumeeachtobeconnected
withitsparentviaexactlyone1-DoFjoint. Therefore,thetransformationbetweeneachpart’sframe
3Output: Part Mesh & Joints
…
Pose-free RGBs
Segmented Part Part Oriented
Code output
Point Clouds Meshes Bounding Boxes
Figure 2: Overview of our proposed pipeline. Given unstructured multi-view RGB images, we
leveragethepre-trainedDUSt3Rmodel[8]toobtaindense2D-to-3Dpointmaps,anduseafine-tuned
2Dsegmentationmodel[52]toperformpart-levelsegmentationandprojecttosegmented3Dpoint
clouds. A learned shape-completion model takes partial point cloud inputs and predicts a dense
occupancyfield,whichisusedforpart-levelmeshextraction. Wefine-tunealargelanguagemodel
(LLM)[53]thattakesmeshinformationintheformoforientedboundingboxes,andoutputsfull
codedescriptionsoftheobjectthatcandirectlybeexecutedinsimulation.
anditsparent’sframeisuniquelydeterminedbythejointparameters—thisobservationmotivates
ourOBB-basedformulationdescribedinSec.3.2. Toobtainvisualinputofoursystem,weassume
Segmented Part Part Oriented
anobjectismanipulatedsuchthateachofitsarticulatedjointsisatanon-zerostate,i.e.,d > 0or
Point Clouds Meshes Bounding Boxes
θ >0,andrecordasetofRGB(andoptionallydepth)imagesoftheobject. Oursystemoutputsaset
of3Dmeshes–eachareconstructionoftheobject’sparts–andalistofjointtypesandparameters
representedascode. Themeshesandjointscanthenbeusedtocreatetheobject’sdigitaltwinin
simulationandenabledownstreamapplications.
Fig. 2 provides an overview of our proposed method. Real2Code consists of two main steps:
reconstructionofobjectparts’geometry(Sec.3.1)andjointestimationviaLLMcodegeneration
(Sec.3.2). Betweenthetwosteps,theorientedboundingboxes(OBBs)oftheobjectpartsserveasan
abstractionlayer,enablingtheLLMtoreasonabout3Dspatialinformationandpredictaccuratejoint
parameters. Weintroducethetwomainstepsinthefollowingtwosections.
3.1 PartReconstruction
Toreconstructanobject’spart-levelshapegeometry,weproposea2D-to-3Dapproachthatiscategory-
agnosticandabletoaddressobjectswitharbitrarynumberofparts. First,wefine-tuneaSAMmodel
thatgenerates2DsegmentationsfromRGBimages,andprojectsthemto3Dpointclouds. Next,we
trainashapecompletionmodelthatextractswatertightmeshesfromthepartially-observedpoint
clouds.
3.1.1 Kinematics-awarePartSegmentation
Weleverageapre-trained2Dsegmentationmodeltofirstsegmentobjectpartsbasedontheirkinematic
structure. Thisdesignismotivatedbytheneedto1)generalizetorealworlddata,and2)scalably
segment arbitrary number of object parts. In contrast to prior works that train 3D segmentation
modelswithlimitedsyntheticdata[5,9,54],theSAM[52]modelwaspre-trainedonamuchlarger
scaledatasetandhencegeneralizeswelltoin-the-wildrealworldimages. Further,incontrasttoprior
worksthatinferarticulationfrommultipleobjectstates,weleverageSAM’sstrongpriortoidentify
objectpartswithouttheneedformulti-stepinteractions.
However,becauseSAM[52]isoriginallydesignedforiterativeuserprompting,itszero-shotpredic-
tionsdisplayuncontrollablegranularityonarticulatedobjects,i.e.,segmentingunnecessarydetails
thatrequireadditionaluserinputtorefine. Toaddressthis,weproposetoadaptthepre-trainedSAM
usingthePartNet-Mobility[9,54]dataset: whilekeepingthemodel’sheavy-weightimageencoder
frozen,wefine-tunethelightweightprompt-decoderlayerofSAMtotakeanimageandonesampled
2Dpointpromptasinput,andpredictthecorrespondingmaskthatmatchestheobject’skinematic
structure. Moredetailsonthefine-tuningprocessarereportedinappendixA.2.
3.1.2 Test-timePromptingforView-consistentSegmentation.
4The point-based segmentation de-
scribed above allows our fine-tuned
SAMtoscaleeasilywiththenumber SAM
Aggregated
oftheobjectparts. However,thisfor- Part Point Cloud
View#1
mulation also inherently lacks view
consistency, as SAM is unaware of
theobjectpartcorrespondencesacross
different camera views. To address SAM
Sampled 3D Points
this,weintroduceatest-timeprompt- on Object Surface
View#N
ingproceduretoprojectpredicted2D
masksintoaview-consistent3Dseg- Figure 3: View-consistent segmentation. Illustration of our
mentation. Wediscusstwodifferent methodfortest-timepromptingthefine-tunedSAMmodel. We
i on fp du et ps te htti an ng dsb caas me ed ro an dt ah te a:av 1a )il Mab uil li tt iy - fi prr ost jes ca V tm ie ewp a# c1l e h3 pD oinp to oi on nn t ots noS ea mf o2r fp o tDl hedm e p2 RD ar t Gp t h o seie Bn gt m f eio nmtre ag gr ePo sru ojcen catd ptot o au b drifj efe edrc ent ftr vp oieo mwin dt + ikc feyfl pVo eoie ru inw etd # nps 2 rt o, mpt View Consistent Mask
camera views, which are used to prompt the model to generate Predicted Mask#2
viewRGB-DandCameraInput:we
view-consistentsegmentations.
first coarsely sample 2D points on SAM Project to 3D SAM
using depth
each RGB image and run the SAM
model to obtain the background masks. This allows us to segment the foreground object in the
differentviewsandsample3Dpointsuniformlyonthepointcloud. Next,weprojecteachsuch3D
point back onto each image, and obtain view-consistent 2D points for SAM prompting. Further,
werankthemodelpredictedmasksbasedontheconfidenceandstabilityscoresproposedin[52],
andfilterthemusingnon-maximumsuppression(NMS)toproducethefinal3Dsegmentation. 2)
Multi-viewUnstructuredRGBInput. Tohandlerealworldsettingswhichoftenlackhigh-quality
depthandcamerainformation,weadoptamulti-viewstereoreconstructionmodeltoachievepart
segmentationfromunstructured,pose-freeRGBimages. WeusetherecentlyproposedDUSt3R[8]
model,whichispre-trainedtopredictdense3Dpoint-mapsfromRGBinputimages. Wethensample
2DpointsfromoneRGBimageandfindeachpoint’scorrespondingpointineveryotherRGBimages
vianearest-neighbor.MoredetailsaredescribedinappendixA.3.Thisoverallprocedureofprojecting
between3Dto2DpromptingissimilartoSA3D[55],whichsamplesonaNeRF[56]fieldanduses
inverserenderingtoeffectivelypromptSAMin3D.
3.1.3 Part-levelShapeCompletion.
Duetofrequentself-occlusioninarticulatedobjects,e.g. theinsideofadrawerisoftennotvisible,
RGB-Dinputdoesnotprovidefullobservationofeachobjectpart,andsubsequentlyasegmented
pointclouddoesnotrecovercompleteshapegeometries. Thismotivateslearningashapecompletion
modeltoobtainwatertightmeshes. Becausepart-segmentationisalreadyhandledintheprevious
step,weheretackleshapecompletionontheobjectpartlevel. WebuildontopofConvolutional
Occupancy Nets [57]: the model architecture consists of a PointNet++[58] point-cloud encoder,
followedbya3DUnet[59]encoderandalinearMLPdecoderthatpredictslogitsforoccupancy.
Weusetheground-truthpartmeshesfromPartNet-Mobility[9]togenerateadatasetofpartialpoint
cloudinputsandoccupancylabels. WenormalizetheoccupancygridusingpartialOBBsextracted
fromtheinputpointcloudtoavoidunder-fittingthesmaller-sizedmeshes. MarchingCubes[60]is
usedtoextractthecompletedpartmeshesfrompredictedoccupancy. SeeappendixA.2formore
detailsonourshapecompletiondatasetandmodeltraining.
3.2 ArticulationPredictionviaCodeGeneration
Givenasetofsegmentedobjectparts,thenextstepistopredictthearticulationstructurethatconnects
theparts. OurapproachofusingLLMcodegenerationoffersseveraladvantages: first,codeisa
compactrepresentationforarticulation,andwhencombinedwithLLM’sabilitytopredictarbitrary-
lengthoutputs,itscaleselegantlywiththecomplexityofanobject’skinematicstructure; second,
pre-trainedLLMsareequippedwithstrongpriorsforbothcommon-senseobjectsandforgenerating
syntactically correct code, making them easily adaptable to our task; lastly, the LLM-generated
code can be directly executed in simulation, removing the need for manual cleanup of predicted
jointparameters,whichisrequiredbypriorwork[5]. Thefollowingsub-sectionsfirstintroduceour
formulationofjointparametersw.r.t. OBBs,thendiscussourproposedfine-tuningprocedurethat
adaptsapre-trainedLLMtoourarticulationpredictiontask.
5
… …Extract OBBs
CodeLlama
Select OBB axis & edge
Helper Function
Compute Joints
Figure4: ArticulationPredictionasCode. Wefine-tuneaCodellamamodelthattakesinoriented
boundingboxes(OBBs)forsegmentedpartsasinput,andgeneratesjointpredictionsviaselecting
OBBrotationaxesandedges(modelgenerationishighlightedingreen). Ahelperfunctionisusedto
computetheabsolutejointaxisandpositionthatassemblestheobjectpartsinsimulation
3.2.1 OrientedBoundingBoxasInputAbstraction. Helper Function
Articulationpredictionrequiresnumericalprecisionatjointparameters(i.e.,positionandrotation)
andreasoningfromrawsensoryinput,butanLLMpre-trainedontextisnotnaturallyadeptatthese
challenges. WeaddressthisExbtyrarcet pOrreiseenntetdin gthesensoryinput(objectpointclouds)asasetoforiented
Bounding Boxes (OBBs)
boundingboxes(OBBs),eachrepresentingasegmentedandcompletedobjectpart. Comparedto
alternative object representations such as 3D point clouds or 2D images, OBBs strike a balance
betweencompactness(i.e.,donotrequireanextrafeatureencoder)andpreciseness(i.e.,provide
numerical3Dposeinformation). Further,OBBsprovideareferenceforjointinformation. Recall
thattheposeofanobjectpartisdeterminedbyits1-DoFjointatanon-zerostate—wecanhence
recoverjointparametersfromtheobserveddisplacementofobjectparts. GivenanOBBofapart
connectedtoitsparent,thOeutjpouint:taxiswillbeparalleltooneofthethreeaxesoftheOBB’srotation
matrix regardless of its jJooiinntt Ptyarpaem.etWeres aasl Csoodoebserve many common articulated objects consist of
cuboid-likeparts(e.g. doorsorlaptops),hencethepositionoftheircorrespondingrevolutejoints
willlieclosetooneoftheOBBedges. Combiningtheseobservations,wecanre-formulatethejoint
axispredictionproblembyselectinganOBBrotationaxisasthejointaxisand,forrevolutejoints,
choosinganOBBedgeparalleltotheaxisasthejointposition. SeeFig.4foranillustration.
3.2.2 Fine-tuningaCodeGenerationLLM.
Wenowhaveaninputformulationthateffectivelyconvertsaregressiontask(i.e.,predictingcontin-
uousvalues)toaneasierclassificationtask(i.e.,selectingaxesandedges)forLLMs. Weusethe
7B-CodeLlama[53]modelforitsopen-source-availabilityandbuilt-inpriorsforcodegeneration.
Weconstructafine-tuningdatasetusingPartNet[9]objects(thesameassetusedtogenerateourseg-
mentationandshapecompletiondataabove),wherethenativeURDFfilesareconvertedintoMJCF
code[61]that1)isinthemorecompactPythonsyntax,2)canbeexecutedinMuJoCo[23]physics
simulation,and3)haseachobject’sjointsassignedwithrespecttothecorrespondingpart’sOBB
information. TheLLMtakesalistofpart-OBBinformation(i.e.,center,rotation,andhalf-lengths)as
input,andoutputsjointpredictionsasalist,whereeachlinecontainsindicesintotheaxesandedges
ofanOBB.MoredetailsonLLMfine-tuningcanbefoundinappendixA.2
4 Experiments
WeevaluateReal2Codeandcomparetobaselinemethodstovalidatetheeffectivenessofourapproach.
Sec. 4.2 describes experiments on our kinematics-aware 2D image segmentation and 3D shape
completionmodels.Sec.4.3evaluatesourfine-tunedcode-generationmodelonarticulationprediction.
6Category Laptop Box Fridge Furniture Furniture
NumberofParts 2 2 2-3 2-4 5-15
Metric whole part whole part whole part whole part whole part
Real2Code+gtSeg 0.57 2.33 1.54 7.65 0.51 2.04 1.46 13.3 5.84 16.8
Ditto 2.54 2.04 1.73 82.82 2.80 462.25 2.25 1105.86 2.21 4608.08
PARIS 84.29 206.31 15.35 158.73 20.63 1297.27 6.02 544.64 11.44 816.86
Real2Code-SegOnly 1.74 7.19 11.46 10.52 0.90 23.44 17.43 206.49 N/A N/A
Real2Code(Ours) 0.44 3.02 1.31 5.94 0.60 1.28 3.47 65.79 19.70 118.58
Table1: WeevaluatesurfacereconstructionqualitybymeasuringChamfer-Distance(CD)between
predictedandground-truthmeshes. Resultsarereportedseparatelyforeachobjectcategory,where
wetakeaverageCDofobjects’entiresurfacereconstruction(‘whole’column)andofallpartwholes
(‘part’column). ObjectsfromStorage-FurnitureandTablearereportedunderFurnitureanddivided
basedonthenumberofparts.
Sec.4.5containsablationstudiesthatprovideadditionalinsightsintoourmethod. InSec.4.6,we
testourtrainedmodelsonrealworldobjectdataandqualitativelydemonstratethegeneralization
abilityofReal2Code.
4.1 ExperimentSetup
Datasets. WeuseassetsfromfivecategoriesinPartNet-Mobility[9]dataset: Laptop,Box,Refrigera-
tor,Storage-FurnitureandTable. Thesamesplitof467trainand35testobjectsareusedtoconstruct
ourimagesegmentation,shapecompletion,andcodedatasets. WeuseBlender[62,63]torender
RGB-Dandground-truthsegmentationmasksforeachobject. TheRGB-Dimagesandmasksare
thenusedtogeneratepart-levelpointcloudsaspartialobservations. Forcodedata,weextractOBBs
frompartmeshesandprocesseachobject’srawURDFfileintoPythonMJCF[61]code,wherethe
rotationandpositionofeachjointarerelativetotheOBBofthechildpartthatthisjointconnectsto
theparentpart. RefertoappendixA.1formoredetailsonourdatasetconstruction.
Baselines. WecompareReal2Codetothefollowingbaselinemethods:
• PARIS[6]isthepriorstate-of-the-artforarticulatedobjectreconstruction. Ittakesmulti-view
RGBobservationsofatwo-partarticulatedobjectattwodifferentjointstates,thenoptimizesper-part
NeRF-basedreconstructionsandjointparametersbasedonmotioncuesfromthetwoobservedstates.
Werenderourtestobjectsattworandomjointstates,runtheirproposedoptimizationprocedurewith
5randominitializationseeds,andreporttheaverageperformance. Tohandlemorecomplexobjects,
wemodifytheirmethodtooptimizeformorethantwopartsatonce. However,weobservethattheir
designofoptimizingoneneuralfieldforeachpartrunsoutofmemorywhenthenumberofjoints
exceeds4.
• Ditto[5]isanend-to-endlearningmethodthattakesinapairofbefore-andafter-interaction
pointcloudinputsandpredictsimplicitpartshapereconstructionsandjointparameters. Notably,
Dittoassumesonlyoneobjectpartismovedatatime,whichrequiresstep-by-stepinteractionsand
observations,makingevaluationlessefficient. ForanobjectwithN joints,wemoveonepartata
time,renderthecorrespondingN pairsofpointcloudobservations,andruntheirpre-trainedmodel
N timestoobtainjointparameterpredictionsandreconstructionsofmovedparts.
• GPT-4[16]isrepresentativeofrecentstate-of-the-artLLMswithstrongreasoningandcode-
generationcapability. Weuseitasareferenceforzero-shotLLMperformanceonourdesiredtask
withoutfine-tuning. WepromptitwiththesamecodeheaderusedinourLLMfine-tuningdataset,
plusadditionalinstructionsonhowtoformattheoutput,whichourfine-tunedLLMdoesnotneed.
4.2 PartSegmentationandReconstructionExperiments
3DPart-levelShapeCompletion.
FollowingthepromptingproceduredescribedinSec.3.1,wefirstrunourfine-tunedSAMonimages
from the test set of 35 unseen objects and obtain segmented part point clouds. We observe that,
becausewerankandfilterthemaskpredictions(i.e.,prioritizehighpredictedconfidencescoreand
stability score), the low-quality masks have less impact on the final segmented point-cloud after
72Parts(15) 3Parts(9) 4-5Parts(6) 6-15Parts(7)
rot↓ pos↓ type↑ rot↓ pos↓ type↑ rot↓ pos↓ type↑ rot↓ pos↓ type↑
Real2Code+gtBB 0.0 0.07 0.93 0.0 0.04 1.00 0.0 0.04 1.00 11.6 0.03 0.94
Ditto 40.04 4.04 0.57 35.57 2.47 0.70 49.77 3.20 0.43 63.06 4.16 0.30
PARIS 48.44 2.67 0.51 32.35 3.63 0.84 55.97 2.14 0.43 N/A N/A N/A
GPT4 57.3 0.26 0.73 10.0 0.08 0.61 45.0 0.21 0.51 30.0 0.05 0.71
Real2Code(Ours) 7.5 0.08 0.80 0.0 0.04 0.89 0.63 0.07 0.97 30.2 0.05 0.89
Table 2: Joint prediction results from Real2Code and baseline methods, grouped by the number
of moving parts in each object. We remark that Real2Code consistently outperforms baseline
methodsacrossobjectswithdifferentkinematicstructures;onobjectswith4ormoremovingparts,
Real2Codepredictsjointsaccuratelywhereasbaselinemethodsfail.
theprojectionstep. Next,weusethesegmentedpartpointcloudsasinputtoevaluateourlearned
shape completion model, and use Marching Cubes [60] on the occupancy predictions to extract
meshes. Followingpriorwork[48,5,6],weuniformlysample10,000pointsontheextractedmesh
surfaceandreporttheaverageChamferDistancebetweentheextractedandground-truthpartmeshes
inTab.1. Becausethepredictionsaresemantics-agnostic,wegeneratepermutationsofthesetof
predictedmeshesandtakethepermutationthatresultsinlowesterror;thesamelogicisusedforjoint
predictionresults.
We remark on the performance difference between Real2Code and baseline methods: the joint
optimizationofshapesandjointsinPARIS[6]suffersfromacomplexlosslandscapeandproduces
unsatisfactoryreconstructions,especiallywhenthenumberofpartsincreases. Ditto[5]performswell
ontrainingcategories(i.e.,Laptop)butdoesnotgeneralizewelltounseencategories. Incontrast,
wefactorizetheproblemintosegmentationandshapecompletion,aggregate2Dsegmentationfrom
fine-tunedSAMandperformshapecompletioninaper-partfashionleadstobetterresults.
Duetothepartialobservationandnoiseinthesegmentationmasks,simplyextractingmeshesfrom
the grouped point clouds also results in subpar reconstruction results (see column ‘Real2Code-
SegOnly’,where‘N/A’indicatesthemeshextractionsaretoonoisytomatchwithGTmesh). This
furthervalidatestheneedforourshapecompletionmodel. Overall,Real2Codeachievesthebest
reconstructionqualityandelegantlyscalestoalargernumberofparts.
Kinematics-aware2DImageSegmentation.
TodemonstratetheeffectivenessofSAMfine-tuning,weevaluatethefine-tunedmodelonunseen
object images by uniformly sampling a grid of 32×32 query points and compare the predicted
segmentationwithground-truthmasks. WeuseNMSfilteringonthepredictedmasks,thenbysorting
with the model’s predicted confidence score to take the top-K masks that fill the image to more
than95%totalpixels. Weobserveasignificantimprovementoverzero-shotSAM:objectpartsare
segmentedmuchmorecloselyfollowingtheirkinematicsstructure,obtaininga92%meanIoUscore
onthefinalusedmasksand84%matchratetoground-truthmasks.
4.3 ArticulationPredictionExperiments
After completing part-level reconstruction on the test objects, we extract OBBs for each object
partandcomposeatext-promptforourfine-tunedCodeLlamamodel. Weparsethemodel’scode
generationandappenditwithcodeheaderlines(e.g. importpackages)suchthatthepost-processed
code can be directly executed to produce object simulation. We then evaluate the accuracy of
articulationpredictionbymeasuringtheerrorofjointtype,jointaxis,and(forrevolutejointsonly)
jointpositionpredictions.
AsshowninTab.2, weoutperformallbaselinemethodsbyalargemargin. Theeffectivenessof
ourOBBabstractionisfurtheraccentuatedbyReal2Code+gtBB,wherewefeedoracleOBBtothe
codegenerationmoduleandachievehighlyaccuratepredictionsevenonunseenobjectswithalarge
numberofparts.
4.4 QualitativeResults
Forqualitativeresults,weselectobjectswitharangeofvaryingkinematiccomplexities,fromatwo-
partlaptoptoaten-partmulti-drawertable. Wevisualizethefinalreconstructedobjectsfromoursand
8N/A
PARIS
Ditto
Real2Code
w/o Shape
Completion
Real2Code
(ours)
GT
Figure5: QualitativeresultsthatcompareReal2Codetobaselinemethods. Weshowresultsfrom
objectswitharangeofvaryingkinematiccomplexities,fromatwo-partlaptoptoaten-partmulti-
drawer table. Whereas all methods can handle the simpler laptop articulation, baseline methods
struggleasthenumberofobjectpartsincreases,andReal2Codeperformsreconstructionmuchmore
accurately. PARISrunsoutofmemoryandfailstorunontheten-parttable(‘N/A’).
baselinesmethodsin Fig.5. Whereasallmethodscanhandlethesimplerlaptoparticulation,baseline
methodsstruggleasthenumberofobjectpartincreases,andReal2Codeperformsreconstruction
muchmoreaccurately.
4.5 AblationStudies
TofurthervalidateourformulationofusingOBBasreferenceforarticulationprediction,weprovide
additionalablationexperimentsthatusealternativeinputandoutputrepresentation:
• RegressiononJointParameters. InsteadofselectingOBBrotationaxisandedge,wefine-tune
twomoreCodeLlamamodelstotakethesameinputbutoutputscontinuousnumericalvaluesfor
jointparameters: thefirstmodeldirectlypredicts3valuesforeachjointaxisand3foreveryjoint
position(Sec.4.5row‘OBBAbs.’);thesecondmodelpredictsjointaxisthesamewayasReal2Code,
butpredictsjointpositionasarelativepositiontotheOBB’scenter(Sec.4.5column‘OBBRel.’).
• ProvideLLMwithVisualInputs. ToverifywhetherOBBsprovidesufficientspatialinformation,
wefine-tunedanothermodelwithbothRGBandOBBinputs. WeadopttheOpenFlamingo[64,65]
approachforinterleavingimageembeddingswiththeCodeLlamamodelweights,andusesthesame
pre-trainedViT[66]weightsforimageencoder.
92Parts(15) 3Parts(9) 4-5Parts(6) 6-15Parts(6)
Inp. Out rot↓ pos↓ type↑ rot↓ pos↓ type↑ rot↓ pos↓ type↑ rot↓ pos↓ type↑
OBB Abs. 7.5 0.06 0.92 0.0 0.03 1.0 0.0 0.6 0.83 0.0 0.7 0.73
OBB Rot. 0.0 0.18 0.73 0.3 0.23 1.00 0.9 0.19 0.83 5.9 0.06 0.59
+RGB Cls. 0.0 0.06 0.80 5.0 0.03 1.0 0.0 0.03 0.89 0.0 0.02 0.67
OBB Cls. 0.0 0.07 0.93 0.0 0.04 1.0 0.0 0.04 1.00 11.6 0.03 0.94
Table3: JointpredictionresultsfromablationexperimentsonReal2Code. Usingthe‘Regress’output
formulation, the LLM is still able to output reasonable values for two or three part objects, but
generatesmuchlessaccuratejointpositionswhenthenumberofarticulatedpartsincrease. Adding
additionalRGBimageinputyieldsnoclearimprovementsfromthemodel,whichsuggeststheOBB
inputalonecanprovidesufficientinformationforarticulation.
ResultsfromtheablationexperimentsarereportedinTab.3. Wemakethefollowingremarks:
Regression formulation predicts less accurate joint positions. Both predicting absolute joint
positions(column‘OBBAbs.’) andpredictingrelativepositionfromOBBcenter(column‘OBB
Rot.’) yieldahigherpredictionerrorthanselectingOBBedgesasjointposition. Incontrary,the
rotationerrorisstillonareasonablescale: wefoundthisisduetothemodelhaslearnedtocopy
thecorrectaxiscolumnfromtheOBBrotationmatricescontainedintheinputprompt. Thisfurther
validatestheeffectivenessofusingOBBasspatialrepresentation.
RGBinputdoesnotyieldsignificantimprovement. Wedrawthisconclusionfromcomparing
row‘+RGBRel.’ and‘OBBRel.’. ThissuggeststheOBBinputprovidessufficientinformationfor
articulationpredictiontask.
4.6 ExperimentsonRealWorldObjects
Tovalidatethegeneralizationability
of Real2Code, we gather a set of in-
Prompt with OBB Input
the-wild articulated objects and col-
lect multi-view RGB data as inputs.
We run Real2Code with DUSt3R[8].
Due to the achieve reconstruction
Different Joint Output Types Additional RGB Input
from multi-view pose-free RGB im-
ages. Due to the lack of quantita- Absolute Pos. + Axis
tive metrics, we show qualitative re- Relative Pos.
+ OBB Axis
sultsinFig.7thatReal2Codegener-
OBB Edge + Axis
alizeswelltoin-the-wildobjectsand
producesgoodqualityreconstructions
fromRGB-onlyinputs. However,al- Figure6: Qualitativecomparisonofthecodeoutputformatinour
ablationexperiments.Eachpredictionformatoccupiesoneline.In
thoughthelearnedDUSt3R[8]model
‘AbsolutePos.+Axis’,theLLMoutputscontinuouspositionand
performs well on overall shape and
axisvalues;in‘RelativePos.+OBBAxis’,theLLMoutputsone
exterior surface areas of the objects,
indexintotheOBB’srotationaxis,anda2Djointpositionrelative
it predicts less accurate point maps
totheselectedaxis;Real2Code uses‘OBBEdge+Axis‘,where
at areasinside the drawers, which is LLMoutputsindextorotationaxesinanOBB,andtwovalues
likelyduetothelackofsimilardata toindicatetheOBBedge. Bottomrightofthefigureshowsone
in their training dataset. As a result, ePxroammptp wleitho fOBaBd dIniptuiotnalRGBimageinputtotheLLM.
the segmented part point clouds dis-
playnoises(secondrowinFig.7),whichleadstoolowerqualitymeshextractionfromtheshape
completionmodel. SeeappendixA.3formoredetailsonourevaluationsetup.
Regress Additional RGB Input
5 Limitations
Initscurrentform,Real2CodestillhasaCflaeswsifyl imitationsthatpointtointerestingdirectionsforfuture
work: 1)Real2Codeonlyconsiderssingleobjectwithmanyparts,extendingittomultipleobject
sceneswouldrequireadditionalobjectdetectionandpreprocessing. 2)Real2Codeonlypredictsjoint
parametersintermsoftheirtype,position,andaxis. Toinferotherjointparameters,suchasjoint
rangeandfrictionworldrequireadditionalmulti-stepinteractionsandobservations. 3)Wefoundthat
100 3 5, 16, 15
Example
RGB input
Segmented
Part Point
Cloud
Reconstructed
Object
Figure7: WeevaluateReal2Code onrealworldobjectsusingRGBdata. Foreachobject,weuse
10pose-freeRGBimagescapturedin-the-wildandrunReal2CodewithDUSt3R[8]. Weshowone
exampleRGBinput(1strow),segmentedpointclouds(2ndrow)andfullreconstruction(3rdrow)
foreachobject.
thearticulationpredictionaccuracyissensitivetofailuresinthefirst2Dimagesegmentationmodule,
i.e.,OBBsfromwrongsegmentationsdirectlyobstructtheLLMreasoningofobjectstructures;this
canbeimprovedbyprovidinghumancorrectivefeedbackasproposedin[52],i.e.,auserprovides
additionalpointsandpromptthemodeltoadjustitsmaskpredictions.
6 Conclusion
WepresentReal2Code,anovelmethodforreconstructingarticulatedobjectsthatleveragescode
generationcapabilityinpre-trainedLLMs. WeempiricallyshowthatReal2Codeachievesanewstate-
of-the-artinbothgeometryreconstructionandarticulationpredictionandcansuccessfullyreconstruct
objectswithcomplexkinematicstructureswithanarbitrarynumberofparts,whereaspriormethods
fail. Byreliablytranslatingvisualobservationtosimulatablemodels,wehopeReal2Codeunlocks
newopportunitiesinroboticsandmixedrealityapplications.
Acknowledgements
TheauthorswouldliketothankZhenjiaXufortherealworlddatacollection,SamirGadreforhelpful
pointersonLLMfine-tuning,ChengChiforhelpwithtrainingourshapecompletionmodeltraining,
andallmembersofREALab: ZeyiLiu,XiaomengXu,ChuerPan,HuyHa,YihuaiGao,MengdaXu,
AustinPatel,et. al. forvaluablefeedbackanddiscussiononthepapermanuscript. Wealsothank
StanfordEEdepartmentadminsKennyGreen,SteveB.CousinsandMaryK.McMahonfortheir
supportduringrealworlddatacollection. ThisworkwassupportedinpartbytheToyotaResearch
Institute,NSFAward#2143601,SloanFellowship. Theviewsandconclusionscontainedhereinare
thoseoftheauthorsandshouldnotbeinterpretedasnecessarilyrepresentingtheofficialpolicies,
eitherexpressedorimplied,ofthesponsors.
References
[1] Lim,V.,H.Huang,L.Y.Chen,etal. Planarrobotcastingwithreal2sim2realself-supervisedlearning,
2022. 1
[2] Wang,L.,R.Guo,Q.Vuong,etal. Areal2sim2realmethodforrobustobjectgraspingwithneuralsurface
reconstruction,2023.
11[3] Torne,M.,A.Simeonov,Z.Li,etal.Reconcilingrealitythroughsimulation:Areal-to-sim-to-realapproach
forrobustmanipulation,2024. 1
[4] Katara,P.,Z.Xian,K.Fragkiadaki. Gen2sim: Scalinguprobotlearninginsimulationwithgenerative
models,2023. 2
[5] Jiang,Z.,C.-C.Hsu,Y.Zhu. Ditto:Buildingdigitaltwinsofarticulatedobjectsfrominteraction,2022. 2,
3,4,5,7,8
[6] Liu,J.,A.Mahdavi-Amiri,M.Savva. Paris:Part-levelreconstructionandmotionanalysisforarticulated
objects,2023. 2,3,7,8
[7] Tian,Y.,A.Luo,X.Sun,etal. Learningtoinferandexecute3dshapeprograms,2019. 2,3
[8] Wang,S.,V.Leroy,Y.Cabon,etal. Dust3r:Geometric3dvisionmadeeasy,2023. 2,4,5,10,11,16
[9] Mo,K.,S.Zhu,A.X.Chang,etal. PartNet:Alarge-scalebenchmarkforfine-grainedandhierarchical
part-level3Dobjectunderstanding. InTheIEEEConferenceonComputerVisionandPatternRecognition
(CVPR).2019. 2,4,5,6,7,15,16
[10] Zeng,A.,M.Attarian,B.Ichter,etal. Socraticmodels:Composingzero-shotmultimodalreasoningwith
language,2022. 3
[11] Hsu,J.,J.Mao,J.Wu. Ns3d:Neuro-symbolicgroundingof3dobjectsandrelations,2023. 3
[12] Gupta,T.,A.Kembhavi. Visualprogramming:Compositionalvisualreasoningwithouttraining,2022. 3
[13] Surís,D.,S.Menon,C.Vondrick. Vipergpt:Visualinferenceviapythonexecutionforreasoning,2023.
[14] Subramanian,S.,M.Narasimhan,K.Khangaonkar,etal. Modularvisualquestionansweringviacode
generation,2023. 3
[15] Brown,T.B.,B.Mann,N.Ryder,etal. Languagemodelsarefew-shotlearners,2020. 3
[16] OpenAI. Gpt-4technicalreport,2023. 3,7
[17] Jones, R. K., H. Walke, D. Ritchie. Plad: Learning to infer shape programs with pseudo-labels and
approximatedistributions,2022. 3
[18] Liang,Y.-C. Learningtoinfer3dshapeprogramswithdifferentiablerenderer. ArXiv,abs/2206.12675,
2022. 3
[19] Willis,K.D.D.,Y.Pu,J.Luo,etal. Fusion360gallery:Adatasetandenvironmentforprogrammaticcad
constructionfromhumandesignsequences,2021. 3
[20] Tulsiani,S.,H.Su,L.J.Guibas,etal. Learningshapeabstractionsbyassemblingvolumetricprimitives.
2017IEEEConferenceonComputerVisionandPatternRecognition(CVPR),pages1466–1474,2016. 3
[21] Ganin,Y.,S.Bartunov,Y.Li,etal. Computer-aideddesignaslanguage,2021. 3
[22] Avetisyan,A.,C.Xie,H.Howard-Jenkins,etal.Scenescript:Reconstructingsceneswithanautoregressive
structuredlanguagemodel,2024. 3
[23] Todorov,E.,T.Erez,Y.Tassa. Mujoco: Aphysicsengineformodel-basedcontrol. In2012IEEE/RSJ
InternationalConferenceonIntelligentRobotsandSystems,pages5026–5033.2012. 3,6,17
[24] Huang,X.,I.D.Walker,S.Birchfield. Occlusion-awaremulti-viewreconstructionofarticulatedobjects
formanipulation. RoboticsAuton.Syst.,62:497–505,2014. 3
[25] Katz,D.,M.Kazemi,J.A.Bagnell,etal. Interactivesegmentation,tracking,andkinematicmodelingof
unknown3darticulatedobjects. In2013IEEEInternationalConferenceonRoboticsandAutomation,
pages5003–5010.IEEE,2013. 3
[26] Hu,R.,W.Li,O.VanKaick,etal. Learningtopredictpartmobilityfromasinglestaticsnapshot. ACM
TransactionsonGraphics(TOG),36(6):1–13,2017. 3
[27] Yi,L.,H.Huang,D.Liu,etal. Deeppartinductionfromarticulatedobjectpairs. ACMTransactionson
Graphics,37(6):1–15,2018.
[28] Wang,X.,B.Zhou,Y.Shi,etal. Shape2motion: Jointanalysisofmotionpartsandattributesfrom3d
shapes,2019. 3
12[29] Michel,F.,A.Krull,E.Brachmann,etal.Poseestimationofkinematicchaininstancesviaobjectcoordinate
regression. InBritishMachineVisionConference.2015.
[30] Li,X.,H.Wang,L.Yi,etal. Category-levelarticulatedobjectposeestimation,2020.
[31] Zeng,V.,T.E.Lee,J.Liang,etal. Visualidentificationofarticulatedobjectparts. In2021IEEE/RSJ
InternationalConferenceonIntelligentRobotsandSystems(IROS),pages2443–2450.IEEE,2021.
[32] Huang,J.,H.Wang,T.Birdal,etal. Multibodysync:Multi-bodysegmentationandmotionestimationvia
3dscansynchronization. InProceedingsoftheComputerVisionandPatternRecognition(CVPR).2021.
[33] Tseng,W.-C.,H.Liao,Y.-C.Lin,etal. Cla-nerf:Category-levelarticulatedneuralradiancefield. 2022
InternationalConferenceonRoboticsandAutomation(ICRA),pages8454–8460,2022.
[34] Abdul-Rashid,H.,M.Freeman,B.Abbatematteo,etal. Learningtoinferkinematichierarchiesfornovel
objectinstances. In2022InternationalConferenceonRoboticsandAutomation(ICRA),pages8461–8467.
IEEE,2022.
[35] Jiang,H.,Y.Mao,M.Savva,etal. Opd: Single-view3dopenablepartdetection. InComputerVision–
ECCV2022:17thEuropeanConference,TelAviv,Israel,October23–27,2022,Proceedings,PartXXXIX,
pages410–426.Springer,2022.
[36] Liu,S.,S.Gupta,S.Wang. Buildingrearticulablemodelsforarbitrary3dobjectsfrom4dpointclouds. In
ProceedingsoftheComputerVisionandPatternRecognition(CVPR).2023. 3
[37] Buchanan,R.,A.Röfer,J.Moura,etal. Onlineestimationofarticulatedobjectswithfactorgraphsusing
visionandproprioceptivesensing. ArXiv,abs/2309.16343,2023. 3
[38] Heppert,N.,T.Migimatsu,B.Yi,etal.Category-independentarticulatedobjecttrackingwithfactorgraphs.
In2022IEEE/RSJInternationalConferenceonIntelligentRobotsandSystems(IROS).IEEE,2022.
[39] Sun, X., H. Jiang, M. Savva, et al. Opdmulti: Openable part detection for multiple objects. ArXiv,
abs/2303.14087,2023. 3
[40] Mo,K.,L.Guibas,M.Mukadam,etal. Where2act:Frompixelstoactionsforarticulated3dobjects,2021.
3
[41] Gadre,S.Y.,K.Ehsani,S.Song. Actthepart:Learninginteractionstrategiesforarticulatedobjectpart
discovery,2021. 3
[42] Liu,L.,W.Xu,H.Fu,etal. Akb-48:Areal-worldarticulatedobjectknowledgebase,2022. 3
[43] An,B.,Y.Geng,K.Chen,etal. Rgbmanip:Monocularimage-basedroboticmanipulationthroughactive
objectposeestimation. ArXiv,abs/2310.03478,2023.
[44] Geng,H.,H.Xu,C.Zhao,etal. Gapartnet:Cross-categorydomain-generalizableobjectperceptionand
manipulationviageneralizableandactionableparts,2023.
[45] Geng,H.,Z.Li,Y.Geng,etal. Partmanip:Learningcross-categorygeneralizablepartmanipulationpolicy
frompointcloudobservations,2023. 3
[46] Hsu,C.-C.,Z.Jiang,Y.Zhu. Dittointhehouse:Buildingarticulationmodelsofindoorscenesthrough
interactiveperception. 2023IEEEInternationalConferenceonRoboticsandAutomation(ICRA),pages
3933–3939,2023. 3
[47] Nie,N.,S.Y.Gadre,K.Ehsani,etal. Structurefromaction:Learninginteractionsforarticulatedobject3d
structurediscovery,2023.
[48] Mu,J.,W.Qiu,A.Kortylewski,etal.A-sdf:Learningdisentangledsigneddistancefunctionsforarticulated
shaperepresentation,2021. 3,8
[49] Heppert, N., M. Z. Irshad, S. Zakharov, et al. Carto: Category and joint agnostic reconstruction of
articulatedobjects. 2023IEEE/CVFConferenceonComputerVisionandPatternRecognition(CVPR),
pages21201–21210,2023. 3
[50] Kawana,Y.,Y.Mukuta,T.Harada.Unsupervisedpose-awarepartdecompositionforman-madearticulated
objects. InEuropeanConferenceonComputerVision,pages558–575.Springer,2022.
13[51] Wei,F.,R.Chabra,L.Ma,etal. Self-supervisedneuralarticulatedshapeandappearancemodels. In
ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages15816–
15826.2022. 3
[52] Kirillov,A.,E.Mintun,N.Ravi,etal. Segmentanything,2023. 4,5,11,15,16
[53] Rozière,B.,J.Gehring,F.Gloeckle,etal. Codellama:Openfoundationmodelsforcode,2023. 4,6,16
[54] Xiang,F.,Y.Qin,K.Mo,etal. SAPIEN:Asimulatedpart-basedinteractiveenvironment. InTheIEEE
ConferenceonComputerVisionandPatternRecognition(CVPR).2020. 4
[55] Cen,J.,Z.Zhou,J.Fang,etal. Segmentanythingin3dwithnerfs,2023. 5
[56] Mildenhall,B.,P.P.Srinivasan,M.Tancik,etal. Nerf:Representingscenesasneuralradiancefieldsfor
viewsynthesis,2020. 5
[57] Peng,S.,M.Niemeyer,L.Mescheder,etal. Convolutionaloccupancynetworks. InEuropeanConference
onComputerVision(ECCV).2020. 5
[58] Qi,C.,L.Yi,H.Su,etal. Pointnet++:Deephierarchicalfeaturelearningonpointsetsinametricspace.
InNeuralInformationProcessingSystems.2017. 5,16
[59] ÖzgünÇiçek,A.Abdulkadir,S.S.Lienkamp,etal. 3du-net:Learningdensevolumetricsegmentation
fromsparseannotation,2016. 5
[60] Lorensen,W.E.,H.E.Cline. Marchingcubes:Ahighresolution3dsurfaceconstructionalgorithm. In
Proceedingsofthe14thAnnualConferenceonComputerGraphicsandInteractiveTechniques,SIGGRAPH
’87,page163–169.AssociationforComputingMachinery,NewYork,NY,USA,1987. 5,8,16
[61] Tunyasuvunakool,S.,A.Muldal,Y.Doron,etal. dm_control:Softwareandtasksforcontinuouscontrol.
SoftwareImpacts,6:100022,2020. 6,7,15
[62] Community,B.O. Blender-a3Dmodellingandrenderingpackage. BlenderFoundation,Stichting
BlenderFoundation,Amsterdam,2018. 7,15
[63] Denninger,M.,D.Winkelbauer,M.Sundermeyer,etal. Blenderproc2:Aproceduralpipelineforphotore-
alisticrendering. JournalofOpenSourceSoftware,8(82):4901,2023. 7,15
[64] Alayrac,J.-B.,J.Donahue,P.Luc,etal. Flamingo:avisuallanguagemodelforfew-shotlearning,2022. 9
[65] Awadalla, A., I.Gao, J.Gardner, etal. Openflamingo: Anopen-sourceframeworkfortraininglarge
autoregressivevision-languagemodels,2023. 9
[66] Dosovitskiy,A.,L.Beyer,A.Kolesnikov,etal. Animageisworth16x16words:Transformersforimage
recognitionatscale,2020. 9
[67] FujiTsang,C.,M.Shugrina,J.F.Lafleche,etal. Kaolin: Apytorchlibraryforaccelerating3ddeep
learningresearch. https://github.com/NVIDIAGameWorks/kaolin,2022. 15
[68] Lin,T.-Y.,P.Goyal,R.Girshick,etal. Focallossfordenseobjectdetection,2018. 15
[69] Sudre,C.H.,W.Li,T.Vercauteren,etal. GeneralisedDiceOverlapasaDeepLearningLossFunctionfor
HighlyUnbalancedSegmentations,page240–248. SpringerInternationalPublishing,2017. 15
[70] Hu,E.J.,Y.Shen,P.Wallis,etal. Lora:Low-rankadaptationoflargelanguagemodels,2021. 16
[71] 3DScannerApp. 3DScannerApp. https://3dscannerapp.com/,2024. Accessed:2024-03-13. 16
[72] Polycam.Polycam-LiDAR&3DScannerforiPhone&Android.https://poly.cam/,2024.Accessed:
2024-03-07. 17
14A Appendix
A.1 DatasetPreparationDetails
Base: PartNet-MobilityObjectAssets. Weusethesamesetof468trainingand41testingobjects
from4categoriesinPartNet-Mobility[9]. Therawdatasetcontainsarichcollectionofobjectmeshes,
textures,andURDFfilesthatcontainarticulationinformation. Wefurtherprocessthedataasfollows:
RGB-DImageRenderingWerendereachobjectindividuallyusingBlender[62,63]for5loops.
Foreachrenderingloop,theobjectiscenteredatthesceneoriginandtherenderingcameraposesare
randomlysampled;werender12RGB-Dimagesandallthesegmentationmaskscorrespondingto
thealltheobjectparts. Duringrendering,wealsorandomlysamplejointstatesintheobjectsuchthat
allitsdoorsordrawersarepartiallyopen—wemaketheassumptionthatallthepartsourtrainand
testobjectsarepartiallyopentoremoveambiguityandprovidemoreobservationviewintoobject
insides.
Mesh Pre-processing. The original PartNet-Mobility assets contain highly fine-grained meshes,
i.e.,onedrawerpartiscomprisedofmorethantenpanelorbar-shapedmeshes. Topreparedatafor
part-levelshapecompletion,wegroupthesefine-grainedmeshessuchthatmeshesfromthesame
objectpartaremergedintoonesinglemesh. Meshtexturesareignoredduringgrouping,resulting
ingroupedtexture-lesspart-levelmeshes. TheRGB-Dimagesandmasksarethenusedtogenerate
part-levelpointcloudsaspartialobservations. WeuseKaolin[67]tosamplelabeloccupancyvalues
fromobjectpartmeshes.
Code-GenerationData. Topreparedataforfine-tuningcode-generationLLMs,wefirstusethe
renderedRGB-Dimagesandsegmentationmaskstoobtainground-truthpart-levelpoint-clouds,
whichareusedtoextractoriented-boundingboxes(OBBs)foreachpart. Next,wetaketherawobject
URDFfilesandgenerateashortercopywithourgroupedpartmeshes. BecausetherawURDF/XML
syntaxcontainlongunnecessarydetails,wemanuallytranslatethemintoPython-likeMJCF[61]
code,whicharealotmorecompactandfamiliartothepre-trainedLLMs. Finally,foreachofthe5
renderingloopsperobject,were-writetheobjectcodeagaintoreplacetheabsolutejointinformation
withtherelativepositionandrotationofeachjointwithrespecttotheextractedOBBs. Wefurther
augmentthedatabyrandomlyrotatingtheOBBsalongthez-axis,5timesperobject. Thisresultsin
468×5×5=11700trainingsamplesforLLMfine-tuning.
A.2 ModelTrainingDetails
SAMFine-tuning.Thefine-tuningdataconsistsof28,020RGBimages,andeachimagecorresponds
toasetofbinarysegmentationmasks,onepereachobjectpartplusabackgroundmask. Wefine-tune
onlythedecoderlayersofpre-trainedSAM[52]onthiscustomdatasetwhilekeepingtherestofthe
modelweightsfrozen. Eachfine-tuningbatchcontains24RGBimages;foreveryRGBimagein
thebatch,wesample16promptpointsuniformlyacrosseachimage’sground-truthmasks,i.e.,only
samplepointsfromthepositivemaskarea. HenceeachtrainingbatchofsizeB =24contains24
imagesand24×16pairsofpromptpointandground-truthmasks. Followingtheoriginalpaper[52],
we update the model with a weighted average of Focal Loss [68], Dice Loss [69] and MSE IoU
predictionloss.
Input image Zero-shot SAM Fine-tuned SAM Fine-tuning Dataset
Figure8: Kinematics-awareSAMFine-tuning. GivenanRGBinputimage,thepre-trainedzero-
shotSAM[52]producesunnecessarilydetailedsegmentationmasks(column Zero-shotSAM’.We
construct adataset of objects’RGB images andkinematics-aligned ground-truth masks(column
‘Fine-tuningDataset’). Themodelisfine-tunedtotakeoneimageandonesampled2Dquerypoint
andpredictthecorrespondingpartmask. Wecomparetheoutputofthemodelafterfine-tuningon
thesameimage(column‘Fine-tunedSAM’).
15Globally Aligned Dense Point-maps from DUSt3R View-consistent Prompt Points
Figure9: 3DpartsegmentationfromPose-freeRGBimages. IllustrationofhowDUSt3R[8]is
used to achieve 3D part segmentation from unstructured RGB images. For each object, we take
around10pose-freeRGBimagesasinputtothepre-trainedDUSt3R[8]model,whichoutputsaset
ofglobally-aligned2D-to-3Ddensepoint-maps,i.e.,every2Dpixeloneachimageismatchedtoa
pointin3D.Thiscorrespondenceenablescross-viewpixelmatchingviafindingnearest-neighborin
3Dspace. Wecanthereforesampleview-consistent2Dpointsforpromptingourfine-tunedSAM
model,andtheresultingsegmentedmasksaregroupedinto3Dpartsegmentation.
Training Shape Completion Model. We use 6,260 pairs of partial point clouds and size 963
occupancygridsandtrainourPointNet++[58]basedoccupancypredictionmodelfromscratch. Fora
trainingbatchofsizeB,wesampleBpointcloudsofsize2048,andsampleB×12,000querypoints
onthelabeloccupancygrids. Notably,becauseobjectpartsareofdifferentscales,wenormalize
theoccupancygridusingpartialOBBsextractedfromtheinputpointcloudtoavoidunder-fitting
thesmaller-sizedmeshes. Whensamplingtrainingquerypoints,wefoundsampling25%occupied
worksthebestforbalancingbetweenoccupiedareasandemptyspace,andweaddarandomshifting
stepontheoccupiedgridstoimprovemodelaccuracyonthenear-surfaceareas. Attesttime,we
queryona963gridanduseMarchingCubes[60]toextractthecompletedpartmeshes.
Fine-tuningCodeGenerationLLM.Weusethepre-trainedCodellama[53]-7Bmodelonourcode
dataset, whichcontainscodesamplesgeneratedfromPartNet[9]objectsasdescribedabove. We
useLoRA[70],alow-rankweightfine-tuningtechnique,tofine-tunethemodelwiththenext-token
predictionloss. Fortrainingefficiency,wecompressthetrainingsequencesbyremovingunnecessary
emptycharacterspacesandoverheadcodelines(suchaspackageimportstatements). Theresulting
trainingsetcontainsunder800tokenspersequenceforobjectswithupto7parts(i.e.,6articulated
joints). Despitetheshorttrainingdata,wefoundthemodeltobeabletoextrapolatetounseentestset
objectswithupto15parts.
A.3 DetailsonRealWorldEvaluations
DataCollection. Wecollectdatafromasetofcommonfurnitureobjects,includingcabinets,laptops,
nightstands,dressers,rangingfrom1to3movingparts. EachobjectisscannedusingaLiDAR-
equippediPhonecameraand3dScannerApp[71]tocaptureasetofRGBimagesfromthefront180◦
view. Wethenselect10RGBimagesperobject,andcropandresizetheminto512×512images
usedbySAM[52]andDUSt3R[8].
PartSegmentationfromUnstructuredRGBimages. Fig.9visualizestheDUSt3Rmodeloutput
onanexampleobjectin: notably,themodelpredictsdensepoint-mapsontheobject’ssurfacearea
that can be globally-aligned into a object point cloud; but the 3D points are less accurate on the
partiallyoccludedareas,suchastheinsideofthedrawer. Thisislikelyduetotheseareasareless
commoninthemodel’spre-trainingdataset. Alsonoticethat,becausewesampleeach2Dpointfrom
oneRGBimagefirstandusesnearestneighborinthepredicted3Dpoint-maptofinditsmatching2D
pointinanotherimage,itmightfindawrongmatchifthepointisoccludedandnotvisibleinthe
otherimage. Weaddressthisbymanuallysettingadistancethreshold,anddecideamatchcannotbe
foundifits3Dpoint’sdistancetothenearestneighborisabovesetthreshold.
16Jewelry Box Gameboy
Figure10: WedemonstratethatReal2Codecanbeusedforlabelingandanimatingrealworldobjects.
WeevaluateReal2CodeonscannedrealobjectsfromPolycam[72]andexporttheresultingmeshand
jointsinMuJoCo[23].Bluearrowsindicatethesimulatedjointaxisandposition;meshcorresponding
tothemovingpartiscoloredingreen.
B AdditionalResultsonAnimatingScannedRealWorldObjects
InadditiontoobjectreconstructionfromrawRGBimages,weshowReal2Codecanalsobeused
to animate scanned objects. We use real world scanned object meshes uploaded by users of the
Polycam[72]App,anduseourBlenderrenderingpipelinetorenderRGB-Dimages. Weevaluateour
imagesegmentation,shapecompletion,andcodegenerationmodelsontheseimages,anddemonstrate
onlythequalitativeresultsduetothelackofground-truthdata. Weexecutethefinalmodeloutput
codetoshowtheobjectscanbesimulatedinMuJoCo[23]. SeeFig.10forvisualizations. These
realworldobjectsfeaturecomplexvisualappearance–outsideourSAMfine-tuningdistribution–,
butReal2Codeisstillabletosuccessfullysegmentpartsandpredictreasonablejointpositionsand
rotations.
17