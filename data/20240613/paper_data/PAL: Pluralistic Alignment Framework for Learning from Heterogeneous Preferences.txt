PAL: PLURALISTIC ALIGNMENT FRAMEWORK FOR LEARNING
FROM HETEROGENEOUS PREFERENCES
APREPRINT
DaiweiChen YiChen
DepartmentofElectricalandComputerEngineering DepartmentofElectricalandComputerEngineering
UniversityofWisconsin-Madison UniversityofWisconsin-Madison
daiwei.chen@wisc.edu yi.chen@wisc.edu
AniketRege RamyaKorlakaiVinayak
DepartmentofComputerSciences DepartmentofElectricalandComputerEngineering
UniversityofWisconsin-Madison UniversityofWisconsin-Madison
aniketr@cs.wisc.edu ramya@ece.wisc.edu
ABSTRACT
Large foundation models pretrained on raw web-scale data are not readily deployable without
additionalstepofextensivealignmenttohumanpreferences[1]. Suchalignmentistypicallydone
by collecting large amounts of pairwise comparisons from humans (“Do you prefer output A or
B?”)andlearningarewardmodelorapolicywiththeBradley-Terry-Luce(BTL)model[2]asa
proxyforahuman’sunderlyingimplicitpreferences. Thesemethodsgenerallysufferfromassuming
a universal preference shared by all humans, which lacks the flexibility of adapting to plurality
of opinions and preferences [3]. In this work, we propose PAL, a framework to model human
preferencecomplementarytoexistingpretrainingstrategies,whichincorporatespluralityfromthe
groundup. Weproposeusingtheidealpointmodel[4]asalenstoviewalignmentusingpreference
comparisons. Togetherwithournovelreformulationandusingmixturemodeling,ourframework
capturesthepluralityofpopulationpreferenceswhilesimultaneouslylearningacommonpreference
latentspaceacrossdifferentpreferences,whichcanfew-shotgeneralizetonew,unseenusers. Our
approach enables us to use the penultimate-layer representation of large foundation models and
simple MLP layers to learn reward functions that are on-par with the existing large state-of-the-
artrewardmodels, therebyenhancingefficiencyofrewardmodelingsignificantly. Weshowthat
PALachievescompetitiverewardmodelaccuracycomparedtostrongbaselineson1)Language
models with Summary dataset [5] ; 2) Image Generative models with Pick-a-Pic dataset [6] ; 3)
AnewsemisyntheticheterogeneousdatasetgeneratedusingAnthropicPersonas[7]. Finally,our
experimentsalsohighlighttheshortcomingofcurrentpreferencedatasetsthatarecreatedusingrigid
rubricswhichwashawayheterogeneity,andcallformorenuanceddatacollectionapproaches.
Keywords Alignment·PreferenceLearning·Plurality
1 Introduction
Largepre-trained“foundation”models[8],suchaslargelanguagemodels(LLMs)forlanguagegeneration[9,10,11,
12,13,14,15]andtext-to-image(TTI[16])modelsforimagegeneration[17,18,19,20,21,22,23],aretrainedon
massiveamountsofdata,includingdatafromtheinternet. Whilesuchmodelslearnusefulrepresentationsforgeneral
languageorvisiontasks,theyarenotreadilydeployableout-of-the-boxtobeusedintherealworld. Modernmachine
learning(ML)systemsbuiltonlargefoundationmodelsgothroughrigorousfine-tuningoraligningtowardshuman
preferencestomakethemamenabletoreal-worldusage. Thisisusuallyachievedthroughsupervisedfine-tuning(SFT)
withdirecthumaninputonwhatthedesiredoutputsshouldlooklikeforagivencontextandthenfollowedbyalignment
withlargeamountsofhumanpreferencefeedbackusuallyintheformofpairwisecomparisonoftwooutputstoagiven
4202
nuJ
21
]GL.sc[
1v96480.6042:viXraPluralisticAlignmentFrameworkforLearningfromHeterogeneousPreferences APREPRINT
Figure1:IllustrationofPALframeworkforlearningfromdiversepreferences(Section3).Foranyuseri,theprobability
ofpreferringx tox forthecontextx isgivenbyarewardmodelr(i)whichismodeledasausesamixturemodeling
l r c θ
approach to capture diverse user preferences – each user’s preference is modeled as a convex combination of K
prototypes. RewardfunctionformulatedusingPALframeworkcanbeusedflexibly,e.g,withfixedpreferencepoints
(ModelA),withpreferencepointsthatarefunctionsofthecontext/promptx (ModelB).
c
inputcontext[1]. Thisisusuallyachievedeitherby(i)fine-tuningtheSFTmodelwithexplicitlylearnedrewardasdone
inreinforcementlearningwithhumanfeedback(RLHF)methodssuchasproximalpolicyoptimization(PPO)[24]or
implicitlywithmethodssuchasdirectpreferenceoptimization(DPO)[25],or(ii)inference-timepolicyadaptation[26]
withoutfine-tuningtheoriginallargepolicymodel.
While aligning ML/AI models to human preferences, it is imperative to ask ourselves whose preferences are we
aligningtheML/AImodelsto?[27]Thestatusquoofthealignmentphaseistoassumeahomogeneouspreference
shared by all humans and attempt to learn a reward model to learn this preference with the Bradley-Terry-Luce
(BTL)model[2]ofpairedpreferences. Wechallengethesenotionsinanattempttocapturediverse,heterogeneous
preferences[3,28,29,30]. Theimportanceofcapturingthepluralityofpreferencesandvaluesamonghumanshasalso
beenhighlightedrecentlybySorensenetal.. However,themethodssuggestedthereinandotherrecentworksthatlook
atlearningmultiplerewardsasatop-downapproachwherethesystemdesignerdecidesthenumberandaxesthatone
shouldcareabout[1,27,32,33,34],e.g.,helpfulnessvs. harmfulness[35,36,37,38]. Inreality,humanpreferenceis
morecomplexthanthedesigner-specifiedaxes[28],whichleadsustoproposethefollowinggoal.
Goal: Developaframeworkforpluralisticalignmentthatusesdiversehumanpreferencesfromthegroundup.
OurContributions. Towardsthisgoal,wemakethefollowingcontributions,
1. Novel Reformulation: We reframe the problem of alignment from human preferences by introducing the lens
ofidealpointmodel[4]andmetriclearning[39]. Thisre-framingenablesleveragingmodelingandalgorithmic
techniquesfromarichersetoftoolboxes(Sections2and3.1).
2. NewFrameworkforPluralisticAlignment: WeproposePAL,ageneralframeworkforpluralisticalignmentusing
diversehumanpreferencesfromthegroundup. Ourframeworkusesamixturemodelingapproachcombinedwith
theidealpointandmetriclearningreframingthatisinterpretable(Figure1,Section3). Itcanworkwiththeoutput
ofanyfoundationmodelandlearnrewardfunctiontogeneralizetoapopulationofdiversepeople. Ourframeworkis
versatiletobeappliedtoawidevarietyofapplicationdomains.
3. EmpiricalValidationonBenchmarkDatasets: Weevaluateourframeworkthroughextensiveexperimentson
bothsyntheticandrealdatasets(Sections4.1and4.3). Ourexperimentshighlighttheabilityandversatilityofthe
PALframeworktolearnfromdiversepreferenceswhenheterogeneityexistsinbothlanguageandvisiondata. Our
experimentsalsorevealthatevenwhenthedatasetsarecollectedinahomogeneousway,PALcanlearnreward
functions using very simple models, e.g., 2-layer MLP, on top of foundation models and are competitive with
state-of-the-art(SoTA)modelsthatfine-tunelargefoundationmodels. Wediscussbroaderimpacts,limitationsand
areasforfutureworkinSection6.
2PluralisticAlignmentFrameworkforLearningfromHeterogeneousPreferences APREPRINT
2 NotationsandBackground
We begin with a brief discussion of the BTL model and how it is currently used in reward learning from pairwise
preferencecomparisons,followedbymotivatingtheidealpointmodel.
Bradley-Terry-Luce(BTL)model[2]isaparametricmodelforranking.Givenmitemsoralternatives,theassumption
isthatthereisauniversalranking: σ(1)≻σ(2)≻···≻σ(m)whichareareflectionoftheunknowntruescoresor
weightsassociatedwitheachoftheseitemss⋆ >s⋆ >···s⋆ ,whereσ(.)denotespermutationandthescores
σ(1) σ(2) σ(m)
s⋆arepositiverealnumbers. Then,theprobabilitythat“ibeatsj"whencomparingthem,denotedbyi≻j isgivenby,
s⋆ exp(r )
Pr(i≻j)= i = i ,wherethevariablesrre-parameterizes>0. (1)
s⋆+s⋆ exp(r )+exp(r )
i j i j
Notation: Wesetupsomenotationforfurtherdiscussion. LetDdenotethedimensionoftherepresentationspaceof
thefoundationmodels. Letx ∈RD denotetherepresentationofthepromptorthecontext. Letx ∈RD andx ∈RD
c l r
denotetheembeddingsoftwoitemswherethesubscriptsdenoteleftandrightrespectively. Notethatwhilewetakethe
dimensionsoftherepresentationspaceforthepromptandtheoutputtobethesame,itneednotbethesameingeneral.
Intheliteratureonalignmentwithhumanfeedback,thescoresre-parametrizedwithreward,denotedherebyr,are
modeledusinganeuralnetworkdenotedbyr . Moreconcretely,givenacontextorpromptx ,theprobabilitythat
θ c
outputx ispreferredtooutputx undertheBTLmodelisgivenby,
l r
exp(r (x ;x )) 1
Pr(x ≻x |x )= θ l c = . (2)
l r c exp(r (x ;x ))+exp(r (x ;x )) 1+exp(r (x ;x )−r (x ;x ))
θ l c θ r c θ r c θ l c
Thegoalthenistolearnthisrewardfunctionr thattakestheoutputx∈RD foragivencontextx ∈RD,denotedby
θ c
(x;x ),asinputandmapittoareal-valuedrewardscoretoapproximatehumanpreference. Thislearningofr isdone
c θ
usinglotsofpairwisecomparisondataobtainedbyqueryinghumans. Suchalearnedrewardfunctioncanbeusedto
alignthemodel[1,40,41],scorethegenerationsduringinferencetimetooutputmorealignedanswers[42]andtorank
thegenerationsofmultiplemodels[43,44]. RecentworkfromRafailovetal.bypassesthestatusquotwo-stagereward
learning+RLpipelineanddirectlyfinetuneonpairwisepreferences,butstillimplicitlyassumestheBTLmodelfor
ranking.
WhilemostalignmentliteraturefocusesontheBTLmodelingapproach,wewanttodrawattentiontotheidealpoint
model[4]forpreferencelearning.
IdealpointmodelwasproposedbyCoombsforhumanpreferencemodelinginthepsychologyliterature. Thekey
ideabehindthismodelistoexploitthegeometryoftheproblem,assumingthereexistsameaningfulrepresentation
space for the items/alternates being compared. Let X ⊆ RD denote the domain of feature space of the concepts
(items, objects, images, choices, etc.) with a distance associated with it. Preference learning based on ideal point
model[4,45,46,47,48,49,50]assumesthatthereisanunknownidealpreferencepointa ∈ X thatrepresentsthe
referencepointpeopleusefortheirpreferencejudgmentsbasedondistances. So,whenasked“Doyoupreferiorj?”,
theyrespondwithiastheirpreferenceifdist(x ,a) < dist(x ,a)andviceversa,wherex ,x ∈ X arethefeature
i j i j
representationsofiandj respectively. Thatis,itemsthatareclosertotheuser’sidealpreferencepointsarepreferred
bytheuseroverthosethatarefartheraway. Thegoalofpreferencelearningthenistousetheresponsesforpairwise
comparisonqueriesfrompeopleandlearnthepreferencepointa. Oncewelearna,wecanpredictthechoicespeople
makebetweennewunseenpairs. Moreformally,ingeneral,theprobabilitythatibeatsj inpreferenceforuserais
givenby,
Pr(i≻j)∝h(dist2(x ,a)−dist2(x ,a)), (3)
j i
wherehisalinkfunction[51]whichcanbeanymonotonicfunctionsuchthatPr(i≻j)>1/2whendist2(x ,a)−
j
dist2(x ,a)>0andPr(i≻j)=1/2whendist2(x ,a)=dist2(x ,a). Essentially,theideahereisthatthelargerthe
i j i
differenceindistancebetweenthealternatestotheidealpoint,theeasieritistochoosebetweenthem,andhencethe
answerislessnoisy. Incontrast,ifthedifferenceofdistanceisclosetozero,thealternatesseemtobeequallygoodor
badtotheuserandthereforetheprobabilityofi≻j willbeclosetorandom.
3 FrameworkforPluralistAlignment(PAL)
Inthissection,webeginbydescribinghowtoviewexistingapproachesthatusetheBTLmodelforalignmentthrough
thelensoftheidealpointmodel,andthenintroduceourframeworkforpluralisticalignment.
3PluralisticAlignmentFrameworkforLearningfromHeterogeneousPreferences APREPRINT
3.1 Viewingalignmentthroughthelensofidealpointmodelandmetriclearning
Theassumptionintheidealpointmodel(Section2)thattheitemsbeingcomparedhaverepresentationsinavectorspace
ismildone,especiallywhileworkingwithfoundationmodels. However,assumingthattheEuclideandistanceora
knowndistancefunctionintherepresentationspaceofthesefoundationmodelstobethecorrectnotionofsimilarityand
dissimilarityforhumanjudgmentsisastrongone.Were-formulatethegoalofalignment,i.e.learningarewardfunction,
tolearninga(potentiallynon-linear)transformationoftherepresentationoutputbythefoundationmodelwherea
knowndistancefunction,e.g.,Euclideandistanceorcosinesimilarity,isagoodapproximation(inthetransformed
space)tocapturehumanjudgmentsofsimilarityanddissimilarity.
LookingatthecurrentalignmentapproachesusingtheBTLmodelthroughthelensoftheidealpointmodel,wecan
re-interprettheEquation2,
1
Pr(x ≻x |x )= . (4)
l r c 1+exp(r (x ;x )−r (x ;x ))
θ r c θ l c
asanidealpointmodelwherethedifferenceofrewardsisaproxyforthedifferenceofdistances1andthelinkfunction
beingtheSigmoidorlogisticfunction.
ByrelaxingtherequirementoftheSigmoidlinkfunctionusedbytheBTLmodelandtheknowndistancefunctionby
theidealpointmodel,weproposetoviewalignmentfromhumanpreferencesaslearningarewardfunctionthatcan
generalizetothefollowing:2
Pr(x ≻x |x )=h(r (x ;x )−r (x ;x )), (5)
l r c θ r c θ l c
wherehanymonotoniclinkfunctionappropriatelynormalizedtoobtainprobabilitiessuchthatPr(x ≻x |x )>1/2
l r c
when r (x ;x )−r (x ;x ) > 0, i.e., there is a clear winner between items, and Pr(x ≻ x |x ) = 1/2 when
θ r c θ l c l r c
r (x ;x )=r (x ;x ),i.e.,thewinnerisrandom.
θ r c θ l c
Weinstantiatetherewardfunctioninthefollowingways:
1. Withunknownbutfixedidealpoint,unknownrepresentationspaceforjointlyrepresentingthepromptinputx and
c
thecorrespondingoutputxfromthefoundationmodelandEuclideandistance,wemodeltherewardfunctionas,
r (x,x )=||f(x;x )−f(a)||2,wheremappingf :R2D →Rdandidealpointa∈R2D areunknownandlearned
θ c c
frompairwisecomparisonqueries. Thiscorrespondstothefollowingpairwiserankingmodel,
Pr(x ≻x |x )=h(||f(x ;x )−f(a)||2−||f(x ;x )−f(a)||2). (6)
l r c r c 2 l c 2
2. Theuseridealpointisanunknownfunctionofthepromptx anddistanceistheanglebetweentheidealpoint
c
conditioned on the prompt and the unknown representation space for the output x from the foundation model,
r (x,x ) = ⟨f(x),z(x )⟩,wherethemappingsf andz mapRD → Rd andareunknownandarelearnedfrom
θ c c
pairwisecomparisons. Hereweassumethattherangespacesoff andz arenormalizedtousetheangleasthe
distancefunction. Thiscorrespondstothefollowingpairwiserankingmodel,
Pr(x ≻x |x )=h(⟨f(x ),z(x )⟩)−⟨f(x ),z(x )⟩). (7)
l r c r c l c
3.2 Modelingdiversepreferences
Sofarourdiscussionhasfocusedonviewingthecurrentalignmentmethodswhichassumeahomogeneousmodel. That
is,allusers’preferencesareassumedtoarrivefromauniversalmodelwithdisagreementsmodeledasnoise. Anatural
extensiontoindividualizedmodelingcanbewrittenasfollows. Foruseri,Pr (x ≻ x |x ) = h(i)(r(i)(x ;x )−
i l r c θ r c
r(i)(x ;x )),whereh(i)isanymonotoniclinkfunctionthatcanbedependentontheindividualandthequery,andr(i)(.)
θ l c
denotestherewardfunctionforindividuali. Wedonotassumetheknowledgeofthelinkfunctionforourlearning
algorithms(Section3.3.2). Onecouldusethesemodelsatasingle-userleveltolearnapersonalizedmodelusinglotsof
datafromthatspecificuser. However,suchmodelswillnotgeneralizetootherindividuals.
Inreality,differentpeoplecanhavedifferentpreferencesthatarenotjustnoisyperturbationsofauniversalmodel. That
is,peoplecandifferinsystematicallydifferentways. However,therearesharedaspectsacrosssubgroupsofpeople,
e.g.,owingtodemographics,educational,socio-cultural,orothertypesofsimilarities. Weproposeaframeworkto
capturehumanpreferencesbyconsideringthesedifferencesandsimilaritiesbymodelingthepreferencesofindividuals
withalow-rankmodel. Inparticular,weuseamixturemodelingapproachforcapturingdiversepreferenceswherewe
modeleachuserasaconvexcombinationofK prototypes.
1Wenotethathererewardfunctionisaproxyandnotrealdistancefunction.
2Notethatinthisview,therewardislesserwhenitispreferred.So,onecouldratherthinkofthisascostorpenaltyfunctionthan
rewardfunction.
4PluralisticAlignmentFrameworkforLearningfromHeterogeneousPreferences APREPRINT
ModelA:Diversepreferencewithfixedpreferencepoints. Hereeachuser’sidealpointismodeledasaconvex
combinationofK prototypicalidealpoints,{p ,...,p }withp ∈R2D. Thecorrespondingpreferencemodelisgiven
1 K i
asfollows:
ModelA: Pr (x ≻x |x )=h(||f(x ;x )−f(a(i))||2−||f(x ;x )−f(a(i))||2), (8)
i l r c r c 2 l c 2
where a(i) := (cid:80)K w(i)p with the weights w(i) ≥ 0 and (cid:80)K w(i) = 1. Denoting P := [p ,··· ,p ] and
k=1 k k k k=1 k 1 K
w(i) :=[w(i),··· ,w(i)]⊤,a(i) =Pw(i),wherew(i)liesinK-dimensionalsimplexdenotedby∆K.
1 K
ModelB:Diversepreferencewithpreferencepointsasfunctionofinputprompt. Hereeachuser’sidealpointis
modeledasaconvexcombinationofK prototypicalfunctionsthatmapinputpromptstoidealpoints,{g ,...,g }. The
1 K
correspondingpreferencemodelisgivenasfollows:
(cid:16) (cid:17)
ModelB: Pr (x ≻x |x )=h ⟨f(x ),z(i)(x )⟩−⟨f(x ),z(i)(x )⟩ , (9)
i l r c r c l c
wherez(i)(x )=(cid:80)K w(i)g (x )=G(x )w(i)withG(x ):=[g (x ),··· ,g (x )]andw(i) ∈∆K.
c k=1 k k c c c 1 c K c
Wedropthesuperscriptionhforsimplicity, however, wenotethatthelinkfunctionneednotbethesameforall
usersandfurthermore,ourlearningalgorithmdescribedinSection3.3doesnotneedtoknowthelinkfunction(s). We
illustratethePALframeworkinFigure1andFigure6(AppendixA).
3.3 LearningPALmodelsfromDiversePreferences
(cid:110) (cid:111)N
Givenadatasetofanswerstopairwisecomparisonqueries, {(x ,x ;x )(i)}mi ,wherem denotethenumber
l r c j j=1 i
i=1
ofpairsansweredbyuseri,thegoalofthelearningalgorithminthePALframeworkistolearnthemappingsand
prototypessharedacrossthepopulation,andforeachuseritheweightsw(i) := [w(i),...,w(i)]withw(i) ≥ 0and
1 K k
(cid:80)K w(i) =1. FormodelA,mappingf andtheprototypes{p }K areshared,whileformodelB,arethemapping
k=1 k k k=1
f andtheprototypemappings{g }K shared. Withoutlossofgenerality,wehaveassumedthatx ispreferredover
k k=1 l
x . So,thisislearningproblemiscanbelookedatasasupervisedlearningsettingwithbinarylabels.
r
3.3.1 Generalizationoverseenusersversusunseenusers
When learning a reward function from diverse preferences, there are two types of generalization to consider. (1)
Generalizationforunseenpairsforseenusers,i.e.,predictingwellfornewpairsforthepeopleforwhomtheweights
havealreadybeenlearnedfromthetrainingdata. Wecallthisseenaccuracy. (2)Generalizationisforunseenusers,i.e.,
predictingwellforpeoplewhosedatawasnotpartofthetrainingdataatall. Forsuchnewusers,somepartoftheirnew
datawillbeusedtolocalizethemwithinthelearnedmodelbyonlylearningtheweightsforthenewuserbykeeping
thesharedmappingsandprototypesfixed. Wecallthisunseenaccuracy. Wealsonotethatwecanusetheweighted
combinationoftheprototypes,i.e.,anaverageofalltheseenusers,asthezero-shotidealpointfornewusers. However,
weemphasizethatitisimportantforrewardfunctionstogeneralizetounseenusersandourframeworkprovidesa
naturalwaytolocalizethenewuser.
3.3.2 LearningAlgorithm
(cid:110) (cid:111)N
GiventhedatasetD = {(x ,x ;x )(i)}mi ,lossfunctionℓandmodelclassforf ,thelearningalgorithmfor
l r c ji ji=1
i=1
θ
modelAstartsbyrandomlyinitializingtheprototypesP=[p ,...,p ],p ∈RD,userweightsW=[w(1),...,w(N)],
1 K k
wherew(i) ∈∆K. Then,ineachiterationuntilconvergencecriteria,thefollowingstepsarerepeated,
(cid:110) (cid:111)
• Samplearandommini-batch (x ,x ;x )(i) ofcomparisondatafromD.
l r c j
• Computeuseridealpoints: a(i) =P·w(1).
• Computedistances: d(i) =||f (x ;x )−f (a(i))||2,d(i) =||f (x ;x )−f (a(i))||2.
l,j θ l c θ 2 r,j θ r c θ 2
• Lossforeachcomparisonj foruseri: ℓ(i)(x ,x ;x )=ℓ(d(i) −d(i)).
j l r c r,j l,j
• UpdateStep: argmax (cid:80) ℓ(i)(x ,x ;x ).
θ,P,{w(i)∈∆K}N i,j j l r c
i=1
The above steps describe updating the learning algorithm for model A. Similarly, the dataset D =
(cid:110) (cid:111)N
{(x ,x ;x )(i)}mi , loss function ℓ and model class for f , the learning algorithm for model B starts by
l r c ji ji=1
i=1
θ
5PluralisticAlignmentFrameworkforLearningfromHeterogeneousPreferences APREPRINT
randomlyinitializingtheprototypefunctions{g ,...,g },whereeachg isafunctionthatmapsx ∈RD toRd,user
1 K k c
weightsW = [w(1),...,w(N)],wherew(i) ∈ ∆K. Then,ineachiterationuntilconvergencecriteria,thefollowing
stepsarerepeated,
(cid:110) (cid:111)
• Samplearandommini-batch (x ,x ;x )(i) ofcomparisondatafromD.
l r c j
• Computeuseridealpoints: a(i) =[g (x ),...,g (x )]·w(1).
1 c K c
• Computedistances: d(i) =⟨f (x ;x )−f (a(i))⟩,d(i) =⟨f (x ;x )−f (a(i))⟩.
l,j θ l c θ r,j θ r c θ
• Lossforeachcomparisonj foruseri: ℓ(i)(x ,x ;x )=ℓ(d(i) −d(i)).
j l r c r,j l,j
• UpdateStep: argmax (cid:80) ℓ(i)(x ,x ;x ).
θ,{g1,...,gK},{w(i)∈∆K}N
i=1
i,j j l r c
SeeAppendixDforpseudocodedetails.
4 Experiments
We conduct extensive experiments on simulated (Section 4.1), semi-synthetic (Section 4.2), and real (Section 4.3)
preference datasets for both text and image generation tasks to demonstrate that our proposed PAL (Pluralistic
ALignment)frameworkcan: (1)effectivelycapturethediversityofuserpreferences,therebyoutperformingexisting
homogeneousrewardmodels;(2)efficientlyachieveperformancecomparabletotheexistingSoTArewardmodelswith
farfewerparametersandcomputecosts;and(3)beversatileandappliedtodifferentdomains.
Forexperimentsonsemi-syntheticandrealpreferencedatasets,asimpletwo-layerMLPPALrewardmodelcanachieve
orexceedtheperformanceofexistingstatusquorewardmodels,whichoftencontainbillionsofparameters.
ComputeResources. Weconductedmostofourexperimentsusing4RTX4090,eachwith24GBofVRAM.All
of our experiments can be run on a single RTX 4090 with RAM and VRAM usage of less than 16 GB. A typical
experimentcanbefinishedwithin2hours.
4.1 HeterogeneousSyntheticDataset
Dataset. Wesynthesizeasimplepreferencedatasetwiththenormaldistribution(weuseasettingsimilarto[45])and
truef∗ :Rd →RdislinearandtheweightW∼N(0,I). Letx ∼N(0,(1/d)I)denotethei item.
i th
Experiment Setup. Assume K∗ user prototypes {p }K∗, where p ∼ N(0,(1/d)I) with the minimum distance
i i=1 i
constraint∥p −p ∥≥δ, ∀i,j ∈[K∗],i̸=j. Weconsidertwosettings: 1)amixturesetting,whereweassumeeach
i j
userislocatedintheconvexhullofK prototypes;2)asimplerpartitionsetting,whereweassumeN usersareevenly
sampledfromK prototypes,witha ∈{p }K . Eachsampleisgeneratedasfollows: werandomlydrawtwoitems
i k k=1
{x ,x }andoneusera , andlabeltheuser’spreferenceassign(∥f∗(x )−f∗(a )∥ −∥f∗(x )−f∗(a )∥ ). We
l r i l i 2 r i 2
generateatotalofnsamplesperusertolearntheuser’sidealpoint. WeusemodelAwithasingle-layerMLP(without
bias)withhingelossandevaluateontheheld-outtestset.
Figure2: TheperformanceofmodelAonthesimulationdatasetswithd =16,K = {1,2,3,4,5},K∗ = {2,3,4},
N =50∗K∗,andmixtureuseridealpointsetting. Forthefig2(a)visualization,wesetd=2,K =3,K∗ =3.
Results. Wesimulatedatasetswithmultiplesettings(differenttrueK∗,K and,dinbothmixtureandpartitionsettings
–seeAppendixC.1fordetails)andevaluateourmodelAonthesesimulationdatasetswithdifferent#samplesand#
prototypes. Figure2(a)showsthatPALcanaligntheuseridealpointstothetrueuseridealpointsintherepresentation
6PluralisticAlignmentFrameworkforLearningfromHeterogeneousPreferences APREPRINT
K⋆ Personas
2 interestinart,interestinliterature
3 interestinart,interestinliterature,interestinmath
4 interestinart,interestinliterature,interestinmath,interestinmusic
5 interestinart,interestinliterature,interestinmath,interestinmusic,interestinscience
6 interestinart,interestinliterature,interestinmath,interestinmusic,interestinscience,interestinsports
Table1: PersonasusedforeachK⋆inourheteogeneouspersonadataset.
space. SeeAppendixC.1formoredetailedresults. Figure2(b)showsthatthehomogeneousrewardmodel(#prototypes
=1)canonlyachievesub-optimalperformanceonthesimulateddatasetwhendiversepreferencesexist. Whenwelearn
pluralisticpreferencesbysettingmultiplelearnableprototypeswithPAL,wegainasignificant7%accuracyboost.
Figure2(c)showsthatasweincreasethenumberoftrainingsamplesforseenusers,PALachieveshighertestaccuracy,
andisalsomoreaccurateincapturingthetruenumberofprototypesinthedataset(whichweknowfromsimulations).
Remark. Recallthatseenusersaretheuserswhosedataisapartofthetrainingdataandtheaccuracyforthemis
measuredonpredictiononunseenpairsusingtheweightslearnedduringtraining. Noticethatwithoutenoughsamples
peruser,learningdiversepreferencescanharmtheperformance,whichindicatestheimportanceofsamplesizein
pluralisticpreferencelearning. Figure2(d)presentsPAL’spotentialtogeneralizetounseenusers. Withoutanyfurther
fine-tuningofthewell-trainedPALrewardmodel(trainedwith100samplesperseenuser),wecansimplylearnanew
weightfornewunseenuserswithlimitedlabeledsamplestoachievepredictionaccuracysimilartothatofseenusers.
Thatis,wekeeptheprototypesandmappingslearnedfixed,butonlytraintheweightsforthenewuserusingafew
comparisonsamples. Wenotethatforalignmenttobetrulyeffective,itneedstogeneralizetonewusersbeyondtheset
ofuserswhosecomparisondataarepartofthetrainingdataset. Therefore,weclearlydistinguishbetweenaccuracyfor
unseenpairsforseenusersversusaccuracyforunseenusers.
4.2 HeterogeneousSemi-SyntheticDatasets
WeevaluatetheperformanceofPALonsemi-syntheticdatasetswhichweconstructbyinjectingdiversityintoreal
preference datasets for both text generation and image synthesis tasks. Our results show that PAL can achieve or
surpassexistingstate-of-the-art(SoTA)largerewardmodelswithonly2-layerMLPnetworks.
4.2.1 PersonaDataset
Anthropic’sPersonadataset[7]consistsofaseriesofpersonalities(personas),eachcorrespondingwith500statements
thatagreewiththepersonaand500statementsthatdonot. Wedenotethesetofstatementsthatagreeswithapersonaρ
asS(ρ). Weconstructasemi-syntheticdatasetusingAnthropic’sPersonatoevaluatePAL.
Dataset. Letρ={ρ ,...,ρ }denotethesetofpersonasthatexistsinoursemi-syntheticheterogeneousdatasetwith
1 K⋆
K⋆ “true”preferencegroupsi.e. eachperson(user)hasoneoftheK⋆ personalities. Foreachρ ∈ ρ,wegenerate
j
N syntheticseenandunseenusers,whereaseenuserprovidespreferencesamplesinthetrainingandtestsets,while
theunseenuseronlyprovidessamplesinthetestset. Foreachseensyntheticuser,wegeneraten queriesthataskif
p
theuseragreeswithagivenstatementfromthepersonadataset. Foreachunseensyntheticuser,wegeneraten
p,unseen
queries. If the statement aligns with the persona ρ of the user, i.e. the statement belongs to S(ρ ), then the user
j j
answersyes,otherwiseno. Table1liststhepersonasweusedtocreatethedatasetforeachK⋆. Figure3showsa
samplequestion.
ExperimentSetup. WeevaluatetheperformanceofmodelBwithlogisticlossontheheterogeneouspersonadataset
in various settings. We use model B, which accounts for user-specific variance in conditioning (text prompt), as
the prompts in the dataset are the only quantities that differ from question to question. To examine the impact of
varioushyperparameters,weconductexperimentsvaryingthenumberoftrueprototypesinthedatasetK⋆,numberof
prototypicalgroupsusedinthemodelK,queriesperseenusern andlatentdimensiondwithfixednumberofusers
p
pergroupN =10000asseeninFigure4.
(a) VaryingK⋆ ={2,...,6}andK ={1,...,8}whilefixingn =1000,d=16.
p
(b) Varyingn ={75,100,200,500,1000}andK ={1,...,5}whilefixingK⋆ =4,andd=16.
p
(c) Varyingd={4,8,16,32,64}andK ={1,...,5}whilefixingK⋆ =4,andn =1000.
p
(d) Varyingn = {1,10,20,50,100,200,500,1000}andK = {1,...,5}whilefixingK⋆ = 4,n = 1000,
p,unseen p
andd=16.
7PluralisticAlignmentFrameworkforLearningfromHeterogeneousPreferences APREPRINT
Figure3: Anexampleofpairwisecomparisonquerywithapromptfromourheterogeneouspersonadatasetgenerated
usingAnthropic’sPersona. Asyntheticuserwhoisassignedwithapersonaofinterestinartwillhaveagroundtruthof
y =−1byansweringno,whereasauserwhoisassignedwithinterestinmathpairswithagroundtruthofy =+1by
answeringyes.
Figure4: Seenaccuracy(a,b,c)andunseenaccuracy(d)evaluatedontheheterogeneouspersonadatasetacrossthe
numberofprototypicalgroupsusedinthemodelK. Wevary(a)thenumberoftrueprototypesK⋆,(b)thenumber
ofcomparisonsperseenusern ,(c)thesizeoflatentdimensiond,(d)thenumberofcomparisonsperunseenuser
p
n .
p,unseen
Results. Werepeattheseexperimentsfivetimesandreportthemean±onestandarddeviationasbarplotsinFigure4.
Figure4(a,b)illustratesthegeneralizationperformanceofPALontheheterogeneouspersonadataset. Weobserve
thatasK →K⋆,theseenaccuracyincreasesto100%givenasufficientnumberofusersandnumberofcomparisons
peruser. Figure4(b)showsthataswegetmorecomparisonsperuser,weachievereasonableseenuseraccuracy,i.e.
wecangeneralizetounseenpairsforuserswhoareseen(providetrainingsamples)inthedataset. Figure4(c)shows
thatthesizeoflatentdimensionddoesnotaffecttheseenaccuracydramatically. Figure4(d)showstheaccuracyfor
unseenusers,i.e.,userswhodonotprovidetrainingsamples. WhenK = 1,thereisnofurtherlearningneededto
generalizetonewusers. However,whenK >1,werequireweightsovertheK prototypesthatwehavenotyetlearned.
Tolearnthesenewuserweights,asdiscussedinSection3.3.1,wefixtheK prototypesandthemappingf,anduse
onlyafewtestdatasamplestolearntheuserweights(few-shot). Weusetheselearnedweightstomakepredictionson
theremainingtestdata(AlsoseeRemark,Section4.1). FromFigure4(d)weseethatforK =1thenumberofsamples
usedtolearnweightsmakesnodifference,sincetherearenoweightstolearnoverasingleprototype. ForK = 2,
weseethatasweusemoredataforlearningthenewuserweights,theperformanceshowsdiminishingreturnsuntil
saturation. WealsodemonstratethatasthenumberofprototypesK increase,morecomparisonsperuserareneededto
learnthenewuserweights,sincethedimensionoftheweightvectorincreaseswithK.
4.2.2 Pick-a-FilterDataset
Weconstructasemi-syntheticheterogeneouspreferencedatasetwhichwecallPick-a-Filter,andshowthatourPAL
rewardmodelcansignificantlysurpassthehomogeneousrewardmodelwhenpluralisticpreferencesarepresent.
Dataset. ThePick-a-Picdataset[6]isalarge,opendatasetforhumanfeedbackintext-to-imagegeneration,designedto
alignpretrainedmodelswithhumanpreferences. Itcontainsaroundamillionsamplesoftext-to-imagepromptsand
realuserpreferencesovergeneratedimagesfrommultipleopen-sourcepopulardiffusionmodels,withanonymous
userIDs. Motivatedbyanaturalhumancolorpreferencedistribution[52],weconstructthePick-a-Filterdatasetby
addingdifferentcolorfilterstothegeneratedimagestoexplicitly"inject"diverseuserpreferencesintothePick-a-Pic
V1dataset. FurtherdetailsareprovidedinFigure7andAppendixB.Themagnitudeofheterogeneouspreference
injectionisdeterminedbyahyperparametercalledmixtureratio. Themixtureratioβ reflectstheproportionbetween
theoriginalpairsfromthePick-a-Picdatasetandthecolor-filteredpairs. Thelargertheβ,themorecolor-filteredpairs.
8PluralisticAlignmentFrameworkforLearningfromHeterogeneousPreferences APREPRINT
Table2: PALtestaccuracycanmatchSoTAPickscore[6]onthePick-
a-Pic-v1testsetwithafractionofthecompute.
Model TestAccuracy(%)
Pick-a-Picv1test
CLIP-H14 59.23
PickScore 71.85
modelAonCLIP-H 69.29±0.66
modelBonCLIP-H 71.13±0.31
Figure5: PALModelBtestaccuracyon
Pick-a-FiltercomparedtoCLIP-H.
Table3: ResultsforSummaryDataset: SeenaccuracyandunseenaccuracyofourmodelwithK =1,5,10compared
totheindividualusermodelproposedin[53]. Withonly594Kparameters,weachieveon-parperformancecompared
toamethodthatrequiresasupervised-finetuned6Bmodel.
K =1 K =5 K =10 Liet. al. [53]
Seenaccuracy 59.28±0.14 59.66±0.09 59.51±0.12 61.72
Unseenaccuracy(zero-shot) 59.20±0.16 59.45±0.12 59.15±0.11 60.65
Experiment Setup. We train model B with logistic loss on the Pick-a-Filter dataset with different mixture ratios.
DetailedtrainingsetupsaredeferredtoAppendixC.2.
Results. Figure5showsthatPAL-BeffectivelycapturesdiversepreferencesacrossmixtureratiosinPick-a-Filter.
Wecanviewthesemixtureratiosasindicatingtheextenttowhichthetwousergroupsprefertheirrespectivecolor
filters. ThefigureillustratesthatPALenableslearningbeyondauniversalpreference(K >1)toidentifydiverseuser
preferencegroups. PALsignificantlyoutperformsthehomogeneousrewardmodelinpredictinguserpreferences–ata
mixtureratioof1,PALachieves95.2%testaccuracycomparedto75.4%fromthehomogeneousrewardmodel.
4.3 RealDatasets
4.3.1 SummaryDataset
Dataset. RedditTL;DRsummarydatasetcuratedby[5]containsaseriesofpreferencesoversummariesgeneratedby
languagemodels. Foreachpairofsummaries,x andx ,aworkerideterminesifx ispreferredornot. Moreover,
l r l
eachpairisalsoaccompaniedbytheuniqueidentifieroftheworkerwhoprovidesthepreference. Thiswouldallowus
toapplyourmodeltosuchadataset.
ExperimentSetup. WeevaluateourmodelAwithhingelossonatrimmedversionofthesummarydatasetdescribedin
[53]. DetailsregardinghowthedatasetisconstructedandcomparisonstootherbaselinesaredeferredtoAppendixC.3.
Results. Table3comparestheperformanceofourmethodtotheoneproposedin[53]. Weusetheweightedaverageof
prototypeslearnedasthegeneralidealpointfornewuserstoconductzero-shotlearning.Weemphasizethateventhough
ourmodelonlyhas594Kparametersandthesentenceembeddingsweusedaregeneratedfromall-mpnet-base-v2
sentencetransformer[54],whichcontainsaround105Mparameters,westillcanachieveonparperformance,especially
intermsofunseenaccuracy.
4.3.2 Pick-a-PicDataset
Weconductedexperimentson thePick-a-Pic dataset[6]and show two benefitsofour proposedideal pointmodel
comparedwithexistingrewardmodels,includingtheabilitytolearndiverseuserpreferencesandacompetitivereward
model with only 2-layer MLP networks. Recent works on the existing reward models usually require fine-tuning
foundationmodelswithbillionsofparameters[1,6]. Ourmodelcanachievecomparableperformancewithoutany
largemodelfine-tuningstage,whichinturnsavesplentyofcomputingcosts.
Dataset. TherearetwoversionsofPick-a-Picdatasets,v1andv2,wherethePick-a-Picv2datasetextendsv1. To
ensurefairmodel evaluation, wedividethe Pick-a-Picv2 testset[6] into“no-leakage"and “leakage"subsets due
tooverlap(“leakage”)withthev1trainset. Specifically,thePick-a-Picv2testsetcontains18391sampleswithno
9PluralisticAlignmentFrameworkforLearningfromHeterogeneousPreferences APREPRINT
Table4: TestAccuracyofPALcomparedtoCLIP-HandPickScorebaselinesonPick-a-Picv2. Entrieswithasterisk∗
haveinflatedaccuracyduetotheV2testsetoverlapwiththeV1train(SeedatasetdetailsinSection4.3.2).
TestAccuracyonPick-a-Picv2(%)
Model TrainDataset
No-leakage Leakage
CLIP-H14 - 62.57 58.59
PickScore pickapicv1 68.04 74.16∗
modelBonCLIP-H pickapicv1 70.02±0.39 79.32±1.68∗
modelBonCLIP-H pickapicv2 70.51±0.22 68.67±0.51
modelBonPickScore pickapicv2 70.16±0.19 74.79±0.13∗
preferenceties,i.e. onegeneratedimageispreferredtotheother. Outofthese,10587samples(∼58%)overlapwith
thetrainingandvalidationsetsofPick-a-Picv1,whichwasusedtotrainPickscore[6]–wecallthisthev2“leakage"
subset. Theremaining7804testsamples(∼42%)inthev2datasetdonotoverlapwiththev1trainingandvalidation
datasets,ensuringtheyaredistinctforevaluationpurposes–wecallthisthev2“no-leakage"subset.
ExperimentSetup. WetrainedmodelBwithlogisticlossonbothv1andv2datasetsover10epochs,usingCLIP-
H/14orPickScore[6]latentembeddingsasinput. WeadoptthesamehyperparametersusedinearlierPick-a-Filter
experiments,avoidingextensivehyperparametertuning(seeAppendixC.2).
Results. Table2highlightstheeffectivenessofourproposedidealpointmodelframework: whiletrainingonPick-a-Pic
v1,PALexceedsSoTArewardmodelperformanceonno-leakagesubset(i.e. faircomparison)by2%. Additionally,
theperformanceofmodelBtrainedonPickScore(trainedonv1train)latentembeddingsisinferiortothatofmodel
B trained on default CLIP-H/14 embeddings. PAL exceeds SoTA Pickscore performance while training a simple
two-layerMLPnetworkonasingleRTX4090GPU,whereasPickScorerequiresfine-tuningasignificantportionof
CLIP-H/14(∼1Bparameters)with8×A100GPUs–thishighlightsthepotentialofPALforefficientrewardmodeling.
Remark. Sincethedatacollectionprocessforexistingdatasetsinvolvestheusageofstrictrubrics[5,6,55],labeler
performancemonitoring[56]anddisproportionateamountofdatafromsmallfractionofusers,thesedatasetsmay
notbeheterogeneous. Wenotethatastrictrubricleadstouniformityasitessentiallycrowdsourcesthecriteriagiven
undertherubricinsteadofelicitingpreferencesofthepeople. Therefore,evenusingPALwithK =1,wecansurpass
existingSoTAperformance. Theseresultsmotivatetheneedformorenuancedapproachestocollectdatasetsthatelicit
diverseopinions.
5 RelatedWorks
AlignmentStatusQuo. Popularexistingfoundationmodels[1,9,11,15]typicallyuseRLHF[5,40]toalignmodels
after pretraining. Recent foundation models such as Zephyr [57] and the Archangel suite3 have shifted to directly
optimizing on human preferences [25, 58, 59] to avoid the nuances of RL optimization [60]. There has also been
significantrecentworkincollectinglargehumanpreferencedatasetsforrewardmodeltraininginthetext-to-image
(typicallydiffusionmodel[20])space[6,55,56].
RewardModeling. Theseexistingalignmentframeworksgenerallyassumethatallhumansshareasingleunified
preference(e.g. LLM“helpfulness”or“harmlessness”[36])andascribetotheBradley-Terry[2]modelofpairwise
preferences. Consensus-basedmethods[28]aimstofindagreementamonglabelersforspecificgoalslikeharmless-
ness [35, 37], helpfulness [36], or engagement [61]. By design, these methods inherently prioritize the universal
preference(andbiases)inducedbythelabelers[27,32,34]. Inreality,humanshavediverse,heterogeneousprefer-
ences[29,30,31]thatdependonindividualcontexts,andmayevenshareagroupstructure[28]. Rewardedsoups[38]
makeacasetocapturediversitythroughpost-hocweight-spaceinterpolationoveramixtureofexpertsthatlearndiverse
rewards. However,theserewardsarelearnedbypre-definingwhataspectsareimportantwhichisdonebythesystem
designer. Separatedatasetsarecollectedtoelicithumanpreferencesontheseaxesastohowmuchpeoplecareofthem.
DPA[62]modelsrewardsasdirectionsinsteadofscalars,andtrainsamulti-objectiverewardmodelforRLHF.Wu
etal.proposefine-grainedmulti-objectiverewardstoprovidemorefocusedsignalforRLHF.Recently,Lietal.propose
personalizedrewardmodelingbylearningageneraluserembeddingandtreatingeachindividualasaperturbationto
theembedding. Asthispreferenceformulationisstillhomogeneous,theycanonlygeneralizetounseenusersusingthe
fixedgeneraluserembedding.
3https://github.com/ContextualAI/HALOs
10PluralisticAlignmentFrameworkforLearningfromHeterogeneousPreferences APREPRINT
Recentsurveyworksprovideexcellentsummariesofliteratureforalignment[64]andrewardmodeling[65].
HumanPreferenceDatasets. Thepreferenceuniversalityassumptionalsoextendsintothedataannotation/labeling
processing, where labelers are given a rubric to select preferences (e.g. to rank an image pair considering image
aesthetics and image-prompt alignment [6]). Due to this rubric, the current largest scale text-to-image generation
preferencedatasets[6,55,56]showlimiteddiversityamonglabelers. InthePick-a-Pic[6]trainset,thereareonly
701disagreementsamongthe12487imagepairslabeledbydifferentusers(94.38%agreement),andtherearezero
disagreementsinvalidation(1261pairs)andtest(1453pairs)sets. HPS[55]foundthatlabeleragreementoverdiffusion
modelgenerationswashigherformodelsofsimilarqualityorsize,thoughthisdiversitycomeswiththecaveatofthe
labelersbeingprovidedarubrictoprovidetheirpreferences. Imagereward[56]useresearcheragreementasacriteriato
hirelabelers. IntheLLMdomain,thepopularSummarizefromFeedbackdataset[5]isalsocollectedwithrigidrubric,
withlabelerperformancemeasuredviaagreementtothepreferredansweroftheauthors. Duringthedatacollection
period,onlylabelerswithsatisfactoryagreementwereretained,whichledtoasmallnumberofusers,allinagreement
withtheauthors’rubric,beingresponsibleforamajorityoflabeledcomparisons. Statusquopreferencedatasetsusedto
alignfoundationmodelsthussufferfromalackofdiversityduetothenatureoftheirdatacollection.
Preferencelearning. Thereisrichliteratureonpreferencelearningandrankinginvariousdomainsrangingfrom
psychology, marketing, recommendation systems, quantifying social science surveys to crowdsourced democracy,
voting theory and social choice theory. We provide a few relevant works here and direct reader to surveys such
as[66]. Rankingbasedmodels,e.g.,BTL-model[2,67],stochastictransitivitymodels[68]focusonfindingranking
ofmitemsorfindingtop-kitemsbypairwisecomparisons[69,70,71,72,73,74,75]. Rankingmitemsinthese
settings requires O(mlogm) queries. There is also rich literature that stems from ideal point model proposed by
Coombs[4,45,46,47,48,49,50]. Undertheidealpointbasedmodels,thequerycomplexityforrankingmitems
reducestoO(dlogm),wheredisthedimensionofthedomainofrepresentationswhichisusuallymuchsmallerthan
thenumberofitemsbeingranked[48]. Thisisduetothefactthatoncethepreferencepointislearned,itcanthenbe
usedtopredictrankingsofnewitemswithoutneedingmorecomparisons.
Metriclearninghasbeenstudiedquiteextensivelyandwedirectthereadertosurveys[39]andbooks[76].Inparticular,
metriclearningbasedontripletqueryinghasalsobeenquiteextensivelystudied[39,77,78,79,80,81,82,83,84,85]
whichaimstolearntheunderlyingunknownmetricundertheassumptionthatthepeoplebasetheirjudgementfora
triplequerywithconceptsx ,x ,x ∈Dontherelativesimilaritiesbasedonthedistancesbetweentheseconcepts
a b c
undertheunknownmetric.
Simultaneousmetricandpreferencelearning. Morerecentlyafewworkshaveconsideredtheproblemofunknown
metricinpreferencelearningandproposedmethods[45,50,86]andprovidedsamplecomplexityanalysis[45,86]
forsimultaneouslylearninganunknownMahalanobismetricandunknownuserpreference(s). Learningtheunknown
Mahalanobismetriccanbeviewedaslearninglinearlayerontopoftheembeddingsfromafoundationmodel. From
our reframing of alignment, these works can be looked as model A with linear function for f and individual user
preferencesinsteadofhavinganystructureoverthem.
6 Conclusions,Broaderimpacts,LimitationsandFutureWork
Weproposedanovelreformulationoftheproblemofalignmentwithhumanpreferences(Section3.1)andproposed
a new framework for pluralistic alignment with diverse preferences from the ground up (Sections 3.2 and 3.3) by
leveragingsharedstructuresacrossthepopulationwhilelearningtopersonalizeusingamixturemodelingapproach. We
demonstratethePALframeworkisagnostictomodality,showingflexibilityadaptivitytoheterogeneouspreferences
forsyntheticdata(Section4.1),semi-syntheticandrealtextdata(Sections4.2.1and4.3.1)andsemi-syntheticandreal
imagedata(Sections4.2.2and4.3.2). Ourworkaidsinbuildingmuch-neededfoundationstowardspluralityforthe
alignmentofML/AImodels. Ourexperimentsalsohighlightthelimitationsofmanyrealhumanpreferencedatasets
thatarecollectedwithrubricsthatmakethedatasethomogeneous,thuscallingforamorenuancedapproachtodata
collectioninthefuture(Section4.3.2). WhilethemixturemodelingapproachofPALisflexibleandinterpretable,a
limitationofusingitisthatitwillnotgeneralizetouserswhofalloutoftheconvexhullofthelearnedprototypes
(Section4.1). Amorepragmaticandexcitingapproachwouldbeacontinuallearningapproachofaddingprototypesto
adapttonewusersovertime,whichweleaveforfuturework.
7 Acknowledgements
ThisworkwaspartlysupportedbyNSFgrantsNCS-FO2219903andNSFCAREERAwardCCF2238876.
11PluralisticAlignmentFrameworkforLearningfromHeterogeneousPreferences APREPRINT
References
[1] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,
SandhiniAgarwal,KatarinaSlama,AlexRay,etal. Traininglanguagemodelstofollowinstructionswithhuman
feedback. Advancesinneuralinformationprocessingsystems,35:27730–27744,2022.
[2] RalphAllanBradleyandMiltonETerry. Rankanalysisofincompleteblockdesigns: I.themethodofpaired
comparisons. Biometrika,39(3/4):324–345,1952.
[3] EsinDurmus,KarinaNguyen,ThomasI.Liao,NicholasSchiefer,AmandaAskell,AntonBakhtin,CarolChen,
ZacHatfield-Dodds,DannyHernandez,NicholasJoseph,LianeLovitt,SamMcCandlish,OrowaSikder,Alex
Tamkin,JanelThamkul,JaredKaplan,JackClark,andDeepGanguli. Towardsmeasuringtherepresentationof
subjectiveglobalopinionsinlanguagemodels,2024.
[4] ClydeHCoombs. Psychologicalscalingwithoutaunitofmeasurement. Psychologicalreview,57(3):145,1950.
[5] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario
Amodei,andPaulFChristiano. Learningtosummarizewithhumanfeedback. AdvancesinNeuralInformation
ProcessingSystems,33:3008–3021,2020.
[6] YuvalKirstain,AdamPolyak,UrielSinger,ShahbulandMatiana,JoePenna,andOmerLevy. Pick-a-pic: Anopen
datasetofuserpreferencesfortext-to-imagegeneration. AdvancesinNeuralInformationProcessingSystems,36,
2024.
[7] EthanPerez,SamRinger,Kamile˙ Lukošiu¯te˙,KarinaNguyen,EdwinChen,ScottHeiner,CraigPettit,Catherine
Olsson,SandipanKundu,SauravKadavath,AndyJones,AnnaChen,BenMann,BrianIsrael,BryanSeethor,
CameronMcKinnon,ChristopherOlah,DaYan,DanielaAmodei,DarioAmodei,DawnDrain,DustinLi,Eli
Tran-Johnson, Guro Khundadze, Jackson Kernion, James Landis, Jamie Kerr, Jared Mueller, Jeeyoon Hyun,
Joshua Landau, Kamal Ndousse, Landon Goldberg, Liane Lovitt, Martin Lucas, Michael Sellitto, Miranda
Zhang,NeeravKingsland,NelsonElhage,NicholasJoseph,NoemíMercado,NovaDasSarma,OliverRausch,
RobinLarson,SamMcCandlish,ScottJohnston,ShaunaKravec,SheerElShowk,TameraLanham,Timothy
Telleen-Lawton, Tom Brown, Tom Henighan, Tristan Hume, Yuntao Bai, Zac Hatfield-Dodds, Jack Clark,
SamuelR.Bowman,AmandaAskell,RogerGrosse,DannyHernandez,DeepGanguli,EvanHubinger,Nicholas
Schiefer,andJaredKaplan. Discoveringlanguagemodelbehaviorswithmodel-writtenevaluations,2022. URL
https://arxiv.org/abs/2212.09251.
[8] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S
Bernstein,JeannetteBohg,AntoineBosselut,EmmaBrunskill,etal. Ontheopportunitiesandrisksoffoundation
models. arXivpreprintarXiv:2108.07258,2021.
[9] Josh Achiam, Steven Adler, SandhiniAgarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo
Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint
arXiv:2303.08774,2023.
[10] RohanAnil,AndrewMDai,OrhanFirat,MelvinJohnson,DmitryLepikhin,AlexandrePassos,SiamakShakeri,
EmanuelTaropa,PaigeBailey,ZhifengChen,etal. Palm2technicalreport. arXivpreprintarXiv:2305.10403,
2023.
[11] AIAnthropic. Theclaude3modelfamily: Opus,sonnet,haiku. Claude-3ModelCard,2024.
[12] JordanHoffmann,SebastianBorgeaud,ArthurMensch,ElenaBuchatskaya,TrevorCai,ElizaRutherford,Diego
deLasCasas,LisaAnneHendricks,JohannesWelbl,AidanClark,etal. Trainingcompute-optimallargelanguage
models. arXivpreprintarXiv:2203.15556,2022.
[13] JackWRae,SebastianBorgeaud,TrevorCai,KatieMillican,JordanHoffmann,FrancisSong,JohnAslanides,
SarahHenderson,RomanRing,SusannahYoung,etal. Scalinglanguagemodels: Methods,analysis&insights
fromtraininggopher. arXivpreprintarXiv:2112.11446,2021.
[14] MachelReid,NikolaySavinov,DenisTeplyashin,DmitryLepikhin,TimothyLillicrap,Jean-baptisteAlayrac,
RaduSoricut,AngelikiLazaridou,OrhanFirat,JulianSchrittwieser,etal. Gemini1.5: Unlockingmultimodal
understandingacrossmillionsoftokensofcontext. arXivpreprintarXiv:2403.05530,2024.
[15] HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,NikolayBashlykov,
SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,etal. Llama2: Openfoundationandfine-tunedchatmodels.
arXivpreprintarXiv:2307.09288,2023.
[16] AlexandraSashaLuccioni,ChristopherAkiki,MargaretMitchell,andYacineJernite. Stablebias: Analyzing
societalrepresentationsindiffusionmodels. arXivpreprintarXiv:2303.11408,2023.
12PluralisticAlignmentFrameworkforLearningfromHeterogeneousPreferences APREPRINT
[17] MingDing,WendiZheng,WenyiHong,andJieTang. Cogview2: Fasterandbettertext-to-imagegenerationvia
hierarchicaltransformers. AdvancesinNeuralInformationProcessingSystems,35:16890–16902,2022.
[18] MingukKang,Jun-YanZhu,RichardZhang,JaesikPark,EliShechtman,SylvainParis,andTaesungPark. Scaling
upgansfortext-to-imagesynthesis. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition,pages10124–10134,2023.
[19] AdityaRamesh,PrafullaDhariwal,AlexNichol,CaseyChu,andMarkChen. Hierarchicaltext-conditionalimage
generationwithcliplatents. arXivpreprintarXiv:2204.06125,1(2):3,2022.
[20] RobinRombach,AndreasBlattmann,DominikLorenz,PatrickEsser,andBjörnOmmer. High-resolutionimage
synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and
patternrecognition,pages10684–10695,2022.
[21] ChitwanSaharia,WilliamChan,SaurabhSaxena,LalaLi,JayWhang,EmilyLDenton,KamyarGhasemipour,
RaphaelGontijoLopes,BurcuKaragolAyan,TimSalimans,etal. Photorealistictext-to-imagediffusionmodels
withdeeplanguageunderstanding. Advancesinneuralinformationprocessingsystems,35:36479–36494,2022.
[22] AxelSauer,TeroKarras,SamuliLaine,AndreasGeiger,andTimoAila. Stylegan-t: Unlockingthepowerofgans
forfastlarge-scaletext-to-imagesynthesis. arXivpreprintarXiv:2301.09515,2023.
[23] JiahuiYu,YuanzhongXu,JingYuKoh,ThangLuong,GunjanBaid,ZiruiWang,VijayVasudevan,AlexanderKu,
YinfeiYang,BurcuKaragolAyan,etal. Scalingautoregressivemodelsforcontent-richtext-to-imagegeneration.
arXivpreprintarXiv:2206.10789,2(3):5,2022.
[24] JohnSchulman,FilipWolski,PrafullaDhariwal,AlecRadford,andOlegKlimov. Proximalpolicyoptimization
algorithms. arXivpreprintarXiv:1707.06347,2017.
[25] RafaelRafailov,ArchitSharma,EricMitchell,ChristopherDManning,StefanoErmon,andChelseaFinn. Direct
preference optimization: Your language model is secretly a reward model. Advances in Neural Information
ProcessingSystems,36,2024.
[26] Ximing Lu, Faeze Brahman, Peter West, Jaehun Jung, Khyathi Chandu, Abhilasha Ravichander, Prithviraj
Ammanabrolu,LiweiJiang,SahanaRamnath,NouhaDziri,etal. Inference-timepolicyadapters(ipa): Tailoring
extreme-scalelmswithoutfine-tuning. InProceedingsofthe2023ConferenceonEmpiricalMethodsinNatural
LanguageProcessing,pages6863–6883,2023.
[27] Shibani Santurkar, Esin Durmus, Faisal Ladhak, Cinoo Lee, Percy Liang, and Tatsunori Hashimoto. Whose
opinionsdolanguagemodelsreflect? InAndreasKrause,EmmaBrunskill,KyunghyunCho,BarbaraEngelhardt,
Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine
Learning,volume202ofProceedingsofMachineLearningResearch,pages29971–30004.PMLR,23–29Jul
2023. URLhttps://proceedings.mlr.press/v202/santurkar23a.html.
[28] MichielBakker,MartinChadwick,HannahSheahan,MichaelTessler,LucyCampbell-Gillingham,JanBalaguer,
Nat McAleese, Amelia Glaese, John Aslanides, Matt Botvinick, et al. Fine-tuning language models to find
agreementamonghumanswithdiversepreferences. AdvancesinNeuralInformationProcessingSystems,35:
38176–38189,2022.
[29] MarcosNadalandAnjanChatterjee. Neuroaestheticsandart’sdiversityanduniversality. WileyInterdisciplinary
Reviews: CognitiveScience,10(3):e1487,2019.
[30] AaronWildavsky. Choosingpreferencesbyconstructinginstitutions: Aculturaltheoryofpreferenceformation.
Americanpoliticalsciencereview,81(1):3–21,1987.
[31] TaylorSorensen,JaredMoore,JillianFisher,MitchellGordon,NiloofarMireshghallah,ChristopherMichael
Rytting,AndreYe,LiweiJiang,XimingLu,NouhaDziri,etal. Aroadmaptopluralisticalignment. arXivpreprint
arXiv:2402.05070,2024.
[32] MyraCheng,EsinDurmus,andDanJurafsky. Markedpersonas: Usingnaturallanguagepromptstomeasure
stereotypesinlanguagemodels. arXivpreprintarXiv:2305.18189,2023.
[33] HyeongKyuChoiandYixuanLi. Beyondhelpfulnessandharmlessness: Elicitingdiversebehaviorsfromlarge
languagemodelswithpersonain-contextlearning. arXivpreprintarXiv:2405.02501,2024.
[34] GrgurKovacˇ,MasatakaSawayama,RémyPortelas,CédricColas,PeterFordDominey,andPierre-YvesOudeyer.
Largelanguagemodelsassuperpositionsofculturalperspectives. arXivpreprintarXiv:2307.07870,2023.
[35] YuntaoBai,SauravKadavath,SandipanKundu,AmandaAskell,JacksonKernion,AndyJones,AnnaChen,Anna
Goldie,AzaliaMirhoseini,CameronMcKinnon,etal. Constitutionalai: Harmlessnessfromaifeedback. arXiv
preprintarXiv:2212.08073,2022.
13PluralisticAlignmentFrameworkforLearningfromHeterogeneousPreferences APREPRINT
[36] YuntaoBai,AndyJones,KamalNdousse,AmandaAskell,AnnaChen,NovaDasSarma,DawnDrain,Stanislav
Fort,DeepGanguli,TomHenighan,etal. Trainingahelpfulandharmlessassistantwithreinforcementlearning
fromhumanfeedback. arXivpreprintarXiv:2204.05862,2022.
[37] DeepGanguli,LianeLovitt,JacksonKernion,AmandaAskell,YuntaoBai,SauravKadavath,BenMann,Ethan
Perez,NicholasSchiefer,KamalNdousse,etal. Redteaminglanguagemodelstoreduceharms: Methods,scaling
behaviors,andlessonslearned. arXivpreprintarXiv:2209.07858,2022.
[38] AlexandreRame,GuillaumeCouairon,CorentinDancette,Jean-BaptisteGaya,MustafaShukor,LaureSoulier,
andMatthieuCord. Rewardedsoups: towardspareto-optimalalignmentbyinterpolatingweightsfine-tunedon
diverserewards. AdvancesinNeuralInformationProcessingSystems,36,2024.
[39] BrianKulis. Metriclearning: Asurvey. FoundationsandTrends®inMachineLearning,5(4):287–364,2013.
[40] PaulFChristiano,JanLeike,TomBrown,MiljanMartic,ShaneLegg,andDarioAmodei. Deepreinforcement
learningfromhumanpreferences. Advancesinneuralinformationprocessingsystems,30,2017.
[41] JanLeike,DavidKrueger,TomEveritt,MiljanMartic,VishalMaini,andShaneLegg. Scalableagentalignment
viarewardmodeling: aresearchdirection. arXivpreprintarXiv:1811.07871,2018.
[42] XuezhiWang,JasonWei,DaleSchuurmans,QuocLe,EdChi,SharanNarang,AakankshaChowdhery,andDenny
Zhou. Self-consistencyimproveschainofthoughtreasoninginlanguagemodels,2023.
[43] HanzeDong,WeiXiong,DeepanshuGoyal,YihanZhang,WinnieChow,RuiPan,ShizheDiao,JipengZhang,
KashunShum,andTongZhang. Raft: Rewardrankedfinetuningforgenerativefoundationmodelalignment.
arXivpreprintarXiv:2304.06767,2023.
[44] ZhengYuan,HongyiYuan,ChuanqiTan,WeiWang,SongfangHuang,andFeiHuang. Rrhf: Rankresponsesto
alignlanguagemodelswithhumanfeedbackwithouttears. arXivpreprintarXiv:2304.05302,2023.
[45] GregoryCanal,BlakeMason,RamyaKorlakaiVinayak,andRobertNowak. Oneforall: Simultaneousmetric
andpreferencelearningovermultipleusers. arXivpreprintarXiv:2207.03609,2022.
[46] CodyDing. Evaluatingchangeinbehavioralpreferences: Multidimensionalscalingsingle-idealpointmodel.
MeasurementandEvaluationinCounselingandDevelopment,49(1):77–88,2016.
[47] JoelHuber. Idealpointmodelsofpreference. ACRNorthAmericanAdvances,1976.
[48] KevinGJamiesonandRobertNowak.Activerankingusingpairwisecomparisons.Advancesinneuralinformation
processingsystems,24,2011.
[49] AdishSingla,SebastianTschiatschek,andAndreasKrause. Activelylearninghemimetricswithapplicationsto
elicitinguserpreferences. InInternationalConferenceonMachineLearning,pages412–420.PMLR,2016.
[50] AustinXuandMarkDavenport. Simultaneouspreferenceandmetriclearningfrompairedcomparisons. Advances
inNeuralInformationProcessingSystems,33:454–465,2020.
[51] J.A.NelderandR.W.M.Wedderburn. Generalizedlinearmodels. JournaloftheRoyalStatisticalSociety.Series
A(General),135(3):370–384,1972. URLhttp://www.jstor.org/stable/2344614.
[52] StephenEPalmerandKarenBSchloss. Humanpreferenceforindividualcolors. InHumanVisionandElectronic
ImagingXV,volume7527,pages353–364.SPIE,2010.
[53] XinyuLi,ZacharyCLipton,andLiuLeqi. Personalizedlanguagemodelingfrompersonalizedhumanfeedback.
arXivpreprintarXiv:2402.05133,2024.
[54] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. In
Proceedingsofthe2019ConferenceonEmpiricalMethodsinNaturalLanguageProcessing.Associationfor
ComputationalLinguistics,112019. URLhttps://arxiv.org/abs/1908.10084.
[55] Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human
preference score v2: A solid benchmark for evaluating human preferences of text-to-image synthesis. arXiv
preprintarXiv:2306.09341,2023.
[56] JiazhengXu,XiaoLiu,YuchenWu,YuxuanTong,QinkaiLi,MingDing,JieTang,andYuxiaoDong.Imagereward:
Learning and evaluating human preferences for text-to-image generation. Advances in Neural Information
ProcessingSystems,36,2024.
[57] LewisTunstall,EdwardBeeching,NathanLambert,NazneenRajani,KashifRasul,YounesBelkada,Shengyi
Huang,LeandrovonWerra,ClémentineFourrier,NathanHabib,etal. Zephyr: Directdistillationoflmalignment.
arXivpreprintarXiv:2310.16944,2023.
14PluralisticAlignmentFrameworkforLearningfromHeterogeneousPreferences APREPRINT
[58] Mohammad Gheshlaghi Azar, Zhaohan Daniel Guo, Bilal Piot, Remi Munos, Mark Rowland, Michal Valko,
andDanieleCalandriello. Ageneraltheoreticalparadigmtounderstandlearningfromhumanpreferences. In
InternationalConferenceonArtificialIntelligenceandStatistics,pages4447–4455.PMLR,2024.
[59] KawinEthayarajh,WinnieXu,NiklasMuennighoff,DanJurafsky,andDouweKiela. Kto: Modelalignmentas
prospecttheoreticoptimization. arXivpreprintarXiv:2402.01306,2024.
[60] GabrielDulac-Arnold,NirLevine,DanielJMankowitz,JerryLi,CosminPaduraru,SvenGowal,andToddHester.
Challengesofreal-worldreinforcementlearning: definitions,benchmarksandanalysis. MachineLearning,110
(9):2419–2468,2021.
[61] RobertIrvine,DouglasBoubert,VyasRaina,AdianLiusie,ZiyiZhu,VineetMudupalli,AliakseiKorshuk,Zongyi
Liu,FritzCremer,ValentinAssassi,etal. Rewardingchatbotsforreal-worldengagementwithmillionsofusers.
arXivpreprintarXiv:2303.06135,2023.
[62] Haoxiang Wang, Yong Lin, Wei Xiong, Rui Yang, Shizhe Diao, Shuang Qiu, Han Zhao, and Tong Zhang.
Arithmeticcontrolofllmsfordiverseuserpreferences: Directionalpreferencealignmentwithmulti-objective
rewards. arXivpreprintarXiv:2402.18571,2024.
[63] Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu, Noah A Smith, Mari
Ostendorf, and Hannaneh Hajishirzi. Fine-grained human feedback gives better rewards for language model
training. AdvancesinNeuralInformationProcessingSystems,36,2024.
[64] JiamingJi,TianyiQiu,BoyuanChen,BorongZhang,HantaoLou,KaileWang,YawenDuan,ZhonghaoHe,Jiayi
Zhou,ZhaoweiZhang,etal. Aialignment: Acomprehensivesurvey. arXivpreprintarXiv:2310.19852,2023.
[65] Binghai Wang, Rui Zheng, Lu Chen, Yan Liu, Shihan Dou, Caishuang Huang, Wei Shen, Senjie Jin, Enyu
Zhou, Chenyu Shi, et al. Secrets of rlhf in large language models part ii: Reward modeling. arXiv preprint
arXiv:2401.06080,2024.
[66] JohannesFürnkranzandEykeHüllermeier.Preferencelearningandrankingbypairwisecomparison.InPreference
learning,pages65–82.Springer,2010.
[67] RDuncanLuce. Individualchoicebehavior: Atheoreticalanalysis. NewYork: Wiley,1959.
[68] NiharShah,SivaramanBalakrishnan,AdityaGuntuboyina,andMartinWainwright. Stochasticallytransitive
modelsforpairwisecomparisons: Statisticalandcomputationalissues. InInternationalConferenceonMachine
Learning,pages11–20.PMLR,2016.
[69] DavidRHunter. Mmalgorithmsforgeneralizedbradley-terrymodels. Theannalsofstatistics,32(1):384–406,
2004.
[70] ClaireKenyon-MathieuandWarrenSchudy. Howtorankwithfewerrors. InProceedingsofthethirty-ninth
annualACMsymposiumonTheoryofcomputing,pages95–103,2007.
[71] MarkBravermanandElchananMossel. Noisysortingwithoutresampling. arXivpreprintarXiv:0707.1051,2007.
[72] SahandNegahban,SewoongOh,andDevavratShah. Iterativerankingfrompair-wisecomparisons. Advancesin
neuralinformationprocessingsystems,25,2012.
[73] BrianEriksson. Learningtotop-ksearchusingpairwisecomparisons. InArtificialIntelligenceandStatistics,
pages265–273.PMLR,2013.
[74] ArunRajkumarandShivaniAgarwal. Astatisticalconvergenceperspectiveofalgorithmsforrankaggregation
frompairwisedata. InInternationalconferenceonmachinelearning,pages118–126.PMLR,2014.
[75] NiharBShahandMartinJWainwright. Simple,robustandoptimalrankingfrompairwisecomparisons. The
JournalofMachineLearningResearch,18(1):7246–7283,2017.
[76] AurélienBellet,AmauryHabrard,andMarcSebban. Metriclearning. SpringerNature,2022.
[77] RogerNShepard. Theanalysisofproximities: multidimensionalscalingwithanunknowndistancefunction.i.
Psychometrika,27(2):125–140,1962.
[78] RogerNShepard. Theanalysisofproximities: Multidimensionalscalingwithanunknowndistancefunction.ii.
Psychometrika,27(3):219–246,1962.
[79] RogerNShepard. Metricstructuresinordinaldata. JournalofMathematicalPsychology,3(2):287–315,1966.
[80] MatthewSchultzandThorstenJoachims. Learningadistancemetricfromrelativecomparisons. Advancesin
neuralinformationprocessingsystems,16,2003.
[81] OmerTamuz,CeLiu,SergeBelongie,OhadShamir,andAdamTaumanKalai. Adaptivelylearningthecrowd
kernel. arXivpreprintarXiv:1105.1033,2011.
15PluralisticAlignmentFrameworkforLearningfromHeterogeneousPreferences APREPRINT
[82] Matthäus Kleindessner and Ulrike Luxburg. Uniqueness of ordinal embedding. In Conference on Learning
Theory,pages40–67.PMLR,2014.
[83] AurélienBellet,AmauryHabrard,andMarcSebban. Metriclearning. Synthesislecturesonartificialintelligence
andmachinelearning,9(1):1–151,2015.
[84] AurélienBelletandAmauryHabrard. Robustnessandgeneralizationformetriclearning. Neurocomputing,151:
259–267,2015.
[85] BlakeMason,LalitJain,andRobertNowak. Learninglow-dimensionalmetrics. Advancesinneuralinformation
processingsystems,30,2017.
[86] ZhiWang,GeelonSo,andRamyaKorlakaiVinayak.Metriclearningfromlimitedpairwisepreferencecomparisons.
InUAI,2024.
16PluralisticAlignmentFrameworkforLearningfromHeterogeneousPreferences APREPRINT
Figure6:IllustrationofPALframeworkforlearningfromdiversepreferences(Section3).Foranyuseri,theprobability
ofpreferringx tox forthecontextx iscomputedbyarewardmodelr(i)whichusesamixturemodelingapproach
l r c θ
toassignascalarrewardtoasample(e.g. x orx )givencontext(x ). InPAL-A,eachuseri’spreferencea(i) is
l r c
modeledasaconvexcombinationofK prototypicalpreferences,i.e. a(i) =Pw(i). InPAL-B,eachuseri’spreference
z(i)(x )ismodeledasaconvexcombinationofK prototypicalfunctionsg ···g ,i.e. z(i)(x )=. Rewardfunction
c 1 K c
formulatedusingPALframeworkcanbeusedflexibly,e.g.,withfixedpreferencepoints(ModelA),withpreference
pointsthatarefunctionsofthecontext/promptx (ModelB).
c
A ModelDesign
WeillustratethemodelingmechanismofPAL(Section3.2)inslightlymoredetailinFigure6.
B DatasetDesign
Pick-a-Filter : duetothehighlevelof“agreement”amonglabelersoverimagepreferencesonPick-a-PicV1[6],
weconstructasemi-syntheticdatasetbyapplyingfilterstoasubsetofPick-a-PicV1,whichwecallthePick-a-Filter
dataset. Toconstructthedataset,weconsideronlysamplesthathavenoties,i.e. thelabelerdecidesthatoneimage
isdecisivelypreferabletotheother,giventhetextprompt. AsPick-a-PicprovidesuniqueandanonymoususerIDs
forallpreferencepairs,weconsiderasubsetofuserswhoprovidesamplesinboththetrainandtestsets(468/4223
users). Wefurtheronlyconsideruserswhoprovidemorethan50labels(234/468users)andsorttheusersbynumber
ofsamplesprovided. Wesplittheseusersintoequalgroupsof117each,andweassumewithoutlossofgeneralitythat
thefirstgroupofusers(G1)prefers“cold”tones(bluefilter)andthesecondgroup(G2)prefers“warm”tones(red
filter). Lastly,wearbitrarilyconsiderthefirst50users(whoprovidethemostnumberofsamples)as“seen"users,i.e.
usersthatprovidesamplesinboththetrainandtestsetsofPick-a-Filter. Weaddthisseenvs. unseendistinctionto
evaluatehowwellPALcanadapttounseen(i.e. new)usersaftertraining. Currently,ourexperimentsonPick-a-Filter
(Section4.2.2)trainonV1-train-seen(116031samples)andevaluateonV1-test-seen(3693samples). Weshowthe
numberofsamplesineachofthesesplitsinTable5. Afterconstructingsplits,weapplythefollowingfilteringlogic:
1. Apply“winning”and“losing”filterstoappropriateimagesdependingonlabel. ForG1thewinningfilteris
blue,andforG2thewinningfilterisred.
17PluralisticAlignmentFrameworkforLearningfromHeterogeneousPreferences APREPRINT
Figure7: Theconstructiondiagramforthesemi-syntheticPick-a-Picdataset. Itinvolvesrandomlyselectingapproxi-
mately100,000samplesfromthePick-a-PicdatasetanddividingtheuserIDsintotwodisjointgroups. Weassumeone
groupprefersimageswith“cold”(blue)filtersandtheotherwith“warm”(red)filters. Toincorporatediversecolor
filterpreferences,werandomlyselectβ%ofsamplesperuseronwhichtoapplyfilters.
Table5: NumberofsamplesineachsplitofthenewlyconstructedPick-a-Filterdataset.
Category Train Val Test
Seen 58831 628 1597
Group1 Unseen 9527 79 1886
Total 68358 707 3483
Seen 57200 404 2096
Group2 Unseen 9402 52 1812
Total 66602 456 3908
2. Randomlyshortlistβ%ofsamplestoaddfilters. Theremaining(1−β)%ofsampleswillremainunaltered
(defaultimagesfromPick-a-Picv1).
3. Randomly select 50% of above-shortlisted samples to apply a filter to only the winning image, and the
remaining50%toapplyafiltertoonlylosingimage
WeaddthesesourcesofrandomnesstomakelearningpreferencesonPick-a-Filterlesspronetohacking(e.g. themodel
couldtriviallylearntopredictanimagewithafilterasthepreferredimage).
C ExperimentDetails
C.1 HeterogeneousSyntheticDataset
ExperimentSetup. Weintroducethedatasetsimulationprocedureinthesection4.1. Weusethefollowinghyper-
parameters to generate the synthetic dataset d = 16,K = 3,N = 100,n = 100,δ = 1. We generate another 50
comparison pairs per user as the held-out dataset. (Notice, we didn’t simulate the prompt-guided item generation
{x ,x ,x } procedure. Instead, we directly draw the item {x ,x } from a normal distribution for simplicity.) In
c l r l r
the experimental setup, we apply a toy version of the modeling design A, the distance between the synthetic item
andtheuseridealpointismeasuredby∥f(x)−f(u)∥ . Weuseaprojectionmatrix(i.e. one-layerMLPnetwork
2
18PluralisticAlignmentFrameworkforLearningfromHeterogeneousPreferences APREPRINT
withoutbiastermandactivationfunction)asthemodelarchitecture. Werandomlyinitializethelearnableparameters
ofprototypicalusergroupsanduserweights. WeuseAdamastheoptimizer. Thelearningrateoftheprojectorf is
5e−4. Thelearningrateofthelearnableparametersofprototypicalusergroupsanduserweightsis5e−3. The
weight decay of the projection matrix f is 1e−3. To guarantee convergence, we run a total of 1000 epochs for
eachrun. Werunmultipletrialstoexploretheinfluenceofeachfactor: 1)varyingthenumberofsamplesofseen
usersn = {20,40,60,80,100,400,800,1000}, d = {2,16}, K = 5, N = 250, 2)varyingthenumberofsamples
of new users n = {5,10,20,30,40,50,100}, d = {2,16},K = 5, n = 50, 3) varying the number of groups
new
K ={2,3,4,5,6},d={2,16},n=50,N =50∗K.
Figure8: Partitionsetting Figure9: Partitionsetting Figure10: Partitionsetting
#prototypesinmodel=1 #prototypesinmodel=2 #prototypesinmodel=3
Figure11: Mixturesetting Figure12: Mixturesetting Figure13: Mixturesetting
#prototypesinmodel=1 #prototypesinmodel=2 #prototypesinmodel=3
C.2 Pick-a-FilterDataset
ExperimentSetup. Wechoosetwo-layerMLPnetworkswithReLUactivationandresidualconnectionastheprompt
mappingfunctiong andtheoutputmappingfunctionf. Toavoidtheoverfittingissue, wesetthedropoutrateas
k
0.5andweightdecayas1e−2. WeuseAdamoptimizerwitha1e−4learningrate. Whenwemeasurethemodel’s
performance,weloadthebestcheckpointevaluatedonthevalidationset.
Results. TocheckwhetherourmodeltrainedonPick-a-Filterdatasetiscapturingtheusers’preferencefeaturesor
isjustrememberingcolors,weverifythetestaccuracyseparatelyonthecolor-filteredpairsandoriginalpairsinthe
mixture-ratiodataset. Figure15showsthatcomparedtotheCLIP-H14∼65%testaccuracy,ourmodel’sperformance
ontheoriginalno-filterpairsisstillabovethebaseline,whichverifiesthatourmodelutilizesboththeusers’original
preferenceandthe"injected"heterogeneouscolorpreference.
C.3 SummaryDataset
Dataset. RedditTL;DRsummarydatasetcuratedby[5]containsaseriesofpreferencesoversummariesgenerated
bylanguagemodels. High-qualityworkersarehiredbytheauthorstoannotatetheirpreferencesoverthesummaries.
Workershiredfollowedarubricprovidedbytheauthors,whoperiodicallyfiredthoseworkerswhodidnotmeettheir
performancecriteria.
19PluralisticAlignmentFrameworkforLearningfromHeterogeneousPreferences APREPRINT
aTestaccuracy:72.2% bTestaccuracy:83.96% cTestaccuracy:91.26%
Figure14: Normallydistributeditemswithd=2,K =3,N =100,n=100. Thisfigureplotsallitems,thepredicted
useridealpoints,andthetrueuseridealpointsinthefeaturespace. Recallthatinourmodelingdesign,thedistance
betweentheuseridealpointandtheitemreflectstheuser’spreference;hence,thecloserthepredicteduseridealpointis
tothetrueidealpoints,thehighertheperformance. Asshowninthefiguresabove,whenwechoosethehyperparameter
K =3(thecorrectnumberofgroups),ourmodelcanaccuratelycapturethegroupstructureandpredicteachuser’s
idealpoints.
Figure15: Testaccuracyoncolor-filteredororiginalpairsinPick-a-Filterdataset
Foreachpairofsummariesx andx ,aworkerudeterminesifx ispreferredornot. Moreover,eachpairis
left right left
alsoaccompaniedbytheuniqueidentifieroftheworkerwhoprovidesthepreference. Thiswouldallowustoapplyour
modeltosuchadataset.
ExperimentSetupandResults: Comparingto[5] WetrainedourmodelAonthemodifiedsummarydatasetwith
K =1,...,10. Thisisbecausewewanttoevaluatetheperformanceofgeneralizationontheunseenusers. Wesplitthe
giventestingsetintoaseentestingsetandanunseendataset,wheretheseentestingsetcontainsusersinthetraining
set,andtheunseendatasetcontainsonlyusersthatarenotinthetrainingset. Theseentestingsetisusedtovalidatethe
performanceofseenuser,unseencomparisongeneralization. Wearegoingtoconductatrain,testsplitontheunseen
datasettoevaluatetheperformanceofunseenuser,unseencomparisongeneralization.
WeadoptthehyperparametersusedintheexperimentdescribedinC.2inordertosavetimeonhyperparametertuning.
Table6 comparesthe performanceof PALtothe 1.7Breward modelin [5]. The overall accuracy isthe weighted
averageofseenandunseenuseraccuracy. Wewanttoemphasizethatthemainadvantageofourmodelisthatwedo
20PluralisticAlignmentFrameworkforLearningfromHeterogeneousPreferences APREPRINT
Table 6: The performance of our method vs. the 1.3B reward model from [5] on Summary Dataset. Notably,
our approach does not necessitate a supervised fine-tuned model. We leverage the all-mpnet-base-v2 sentence
transformer[54],with105Mparameters,forsummaryembeddings,andtraina2-layerMLP,with592Kparameters.
K =1 K =2 K =3 K =4
Seenuseraccuracy 60.85±0.11 60.95±0.12 60.77±0.10 60.81±0.12
Unseenuseraccuracy 64.13±0.14 64.18±0.19 64.04±0.23 63.99±0.12
Overall 61.36±0.12 61.45±0.13 61.28±0.13 61.30±0.12
K =5 K =6 K =7 K =8
Seenuseraccuracy 60.91±0.10 60.81±0.06 60.71±0.06 60.88±0.13
Unseenuseraccuracy 64.33±0.10 64.11±0.15 64.12±0.17 64.12±0.13
Overall 61.44±0.10 61.32±0.08 61.25±0.09 61.38±0.13
K =9 K =10 Stiennonet. al. (1.3B)
Seenuseraccuracy 60.95±0.10 60.93±0.12 -
Unseenuseraccuracy 64.07±0.20 64.19±0.11 -
Overall 61.43±0.12 61.44±0.12 65.80±2.00
notrequiretheexistenceofasupervisedfine-tunedmodel. Weusedall-mpnet-base-v2sentencetransformer[54],
whichcontainsaround105Mparameters,togeneratetheembeddingforsummariesandtraineda2-layerMLPwith
roughly592Kparameters.
ExperimentSetupandResults: Comparingto[53] WeevaluateourmodelAwithhingelossonatrimmedversion
ofthesummarydatasetdescribedin[53],tocompareourresultswiththeirs. In[53],theoriginaltrainingsetofthe
summarydatasetisfilteredwithsummariesgeneratedbySFTpoliciesandonlythosecomparisonsmadebythetop
10workerswhoconductthemostpairwisecomparisonsarekept. Thetestdatasetissplitinto2foldswherethose
comparisonsmadebythe10workersareusedtoevaluatethegeneralizationperformanceonseenusers,whereasthose
comparisonsmadebyotherworkersareusedtoevaluatethegeneralizationperformanceonunseenusers.
Table3comparestheperformanceofthemethodtotheoneproposedin[53].Weusetheweightedaverageofprototypes
learnedasthegeneralidealpointfornewuserstoconductzero-shotlearning. Weemphasizethateventhoughour
modelonlyhas594Kparametersandthesentenceembeddingsweusedaregeneratedfromall-mpnet-base-v2
sentencetransformer[54],whichcontainsaround105Mparameters,westillcanachieveonparperformance,especially
intermsofunseenaccuracy.
D ModelingDesign
Algorithm1PAL-Aalgorithm
(cid:110) (cid:111)N
Input: Dataset D = {(x ,x ;x )(i)}mi , loss function ℓ, model class for f , prototypes P = [p ,...,p ],
l r c j j=1 θ 1 K
i=1
p ∈Rd,userweightsW=[w(1),...,w(N)],wherew(i) ∈∆K.
k
1: foreachiterationdo
(cid:110) (cid:111)
2: sampleamini-batch (x l,x r;x c)( ji) ▷randompairs,notorderedbyusers
3: UserIdealPoints: a(i) =P·w(i)
4: Distances:
(cid:16) (cid:17) (cid:16) (cid:17)
5: d( l,i j) =||f θ x( l,i j);x( ci ,j) −f θ(a(i))||2 2, d( ri ,j) =||f θ x( ri ,j);x( ci ,j) −f θ(a(i))||2 2
6: Loss: ℓ(i)(x(i),x(i);x(i))=ℓ(d(i) −d(i))
j l,j r,j c,j r,j l,j
7: UpdateStep: argmax (cid:80) l(i)(x(i),x(i);x(i))
θ,P,{w(i)∈∆K}N i,j j l,j r,j c,j
i=1
8: endfor
21PluralisticAlignmentFrameworkforLearningfromHeterogeneousPreferences APREPRINT
Algorithm2PAL-Balgorithm
(cid:110) (cid:111)N
Input: Preference data D = {(x ,x ;x )(i)}mi , loss function ℓ, mapping function f , prototype mapping
l r c j j=1 θ
i=1
functions{g }K ,userweights{w(i) :=[w(i),...,w(i)]}N .
θk k=1 1 K i=1
1: foreachiterationdo
(cid:110) (cid:111)
2: sampleamini-batch (x l,x r;x c)( ji) ▷randompairs,notorderedbyusers
3: UserIdealPoint(conditiononprompts):
(cid:104) (cid:105)⊤
4: a(i) = g θi(x( ci ,j)),...,g θK(x( ci ,j)) ·w(i)
5: Distance:
(cid:16) (cid:17) (cid:16) (cid:17)
6: d( l,i j) =⟨f θ x( l,i j) ,a(i)⟩, d( ri ,j) =⟨f θ x( ri ,j) ,a(i)⟩
7: Loss: ℓ(i)(x(i),x(i);x(i))=ℓ(d(i) −d(i))
j l,j r,j c,j r,j l,j
8: UpdateStep: argmax (cid:80) ℓ(i)(x(i),x(i);x(i))
Θ,P,{w(i)∈∆K}N j l,j r,j c,j
i=1
9: endfor
E BroaderImpacts
This paper presents novel contributions to the field of machine learning towards foundations for learning from
heterogeneouspreferencesaidingthedevelopmentofmodelsandalgorithmstomovetheneedletowardsplurality.
22