Words Worth a Thousand Pictures: Measuring and Understanding
Perceptual Variability in Text-to-Image Generation
RaphaelTang,1,2 XinyuZhang,2 LixinyuXu,1 YaoLu,3 WenyanLi,4
PontusStenetorp,3 JimmyLin,2 FerhanTure1
1ComcastAITechnologies 2UniversityofWaterloo
3UniversityCollegeLondon 4UniversityofCopenhagen
1{firstname_lastname}@comcast.com 2{r33tang, x978zhan, jimmylin}@uwaterloo.ca
Abstract
Diffusionmodelsarethestateoftheartintext-
to-imagegeneration,buttheirperceptualvari-
abilityremainsunderstudied. Inthispaper,we
examine how prompts affect image variabil-
ity in black-box diffusion-based models. We
propose W1KP, a human-calibrated measure
ofvariabilityinasetofimages,bootstrapped
fromexistingimage-pairperceptualdistances.
Currentdatasetsdonotcoverrecentdiffusion Figure1: DALL-E3imagesfortheprompts“amatte
models,thuswecuratethreetestsetsforeval- orange ball in the center against a pure white back-
uation. Our best perceptual distance outper- ground” (top) and “orange ball against white back-
formsninebaselinesbyupto18pointsinaccu- ground” (bottom). Our W1KP score quantifies the
racy,andourcalibrationmatchesgradedhuman perceptualsimilarityforeachsetofimages. Ityields
judgements78%ofthetime. UsingW1KP,we 0.99and0.68forthetopandbottomrows,showingthe
studypromptreusabilityandshowthatImagen greaterimagevariabilityofthelatter.
promptscanbereusedfor10–50randomseeds
before new images become too similar to al-
readygeneratedimages,whileStableDiffusion
Inthispaper,westudytheconnectionbetween
XLandDALL-E3canbereused50–200times.
Lastly, we analyze 56 linguistic features of visual variability and language in black-box text-
realprompts,findingthattheprompt’slength, to-imagemodels,focusingonstate-of-the-artdif-
CLIPembeddingnorm,concreteness,andword fusion models. Previous work tends to study the
sensesinfluencevariabilitymost. Asfaraswe perceptual distance (Zhang et al., 2018) between
areaware,wearethefirsttoanalyzediffusion
pairs of images, while a prompt can generate a
variability from a visuolinguistic perspective.
nearinfinitesetofimages. Furthermore,previous
Ourprojectpageisathttp://w1kp.com.
approacheshavenotbeenexplicitlycalibratedfor
1 Introduction human-friendlygradesofsimilarity. Whatdoesa
score of, for example, 0.2 mean in terms of per-
In text-to-image generation, pictures are worth a
ceivedsimilarity? Suchcalibrationislikelycrucial
thousandwords,butwhichwordsareworthathou-
forrobusthumaninterpretation.
sandpictures? Specifically,howdopromptsaffect
To bridge these gaps in the literature, we first
perceptual variation in generated imagery across
proposeastraightforwardframeworkforconstruct-
randomseeds? Considertheseprompts:
ing human-calibrated perceptual variability mea-
P1: A matte orange ball in the center against a
sures based on existing perceptual distance met-
purewhitebackground.
rics. WecallittheWordsofaThousandPictures
P2: Orangeballagainstwhitebackground.
method, or W1KP ([’wIk.pi:]) for short. On our
AsshowninFigure1,thefirstconveysasinglepar- crowd-sourced dataset of human-judged images
ticularillustration,whilethesecondelicitsmultiple from DALL-E 3, Imagen, and Stable Diffusion
interpretations. Orange could refer to the fruit or XL (SDXL), we validate our choice of Dream-
thecolor,andthescenegeometryisunderspecified. Sim(Fuetal.,2024),arecentdistancetrainedon
But how can we quantify and characterize these Stable Diffusion (Rombach et al., 2022) images.
linguisticintuitions? Our variant of DreamSim outperforms the best
4202
nuJ
21
]VC.sc[
1v28480.6042:viXrabaselineby0.1–0.4pointsintwo-alternativeforced 2 OurW1KPApproach
choiceand0.2–0.4pointsinaccuracy. Toimprove
interpretability,wenormalizeandcalibratescores 2.1 Preliminaries
togradedhumanjudgementsonfourlevelsofper-
Text-to-imagediffusionmodelsareafamilyofde-
ceptualsimilarity,withcutoffpointscorresponding
noising generative models broadly consisting of
tohigh(0.85–1.0),medium(0.4–0.85),low(0.2–
two components: a text encoder that produces la-
0.4),andnosimilarity(<0.2),whichyieldacorrect
tentrepresentationsoflanguage,suchasT5(Raffel
classification78%ofthetime.
etal.,2020)orCLIP(Radfordetal.,2021),anda
Next,togroundouracademicdiscourse,wein- denoisingimagedecoderthattransformsrandom
vestigatethepracticalimplicationsofourapproach. noise into an image conditioned on text, e.g., a
Supposeacomputergraphicspractitionerwishes convolutionalvariationalauto-encoder(VAE;Rom-
togenerateadiversearrayofimagesfromasingle bachetal.,2022). Togenerateanimage,wefeed
prompt,butitisunclearhowmuchitcanbereused a prompt into the text encoder, pass its represen-
withdifferentseedsbeforeadditionalimagescon- tation to the image decoder along with randomly
tributelittletothevariabilityoftheoverallsetof sampled noise, then iteratively denoise the noise
images. Our work provides a quantitative metric intoameaningfulimage. Large-scalemodelsare
forpromptreusability,asweexplorefurtherinSec- generallytrainedusingscorematching(Songetal.,
tion4.1. OnDiffusionDB(Wangetal.,2023),an 2021) on billions of image–caption pairs (Podell
opendatasetofuser-writtentext-to-imageprompts, etal.,2024),suchasthenow-deprecatedLAION-
we find that the same prompt can be reused for 5Bdataset(Schuhmannetal.,2022).
Imagenfor10–20randomseeds,whileSDXLand To conduct a general study, we explore diffu-
DALL-E3aremorereusableat100–200seeds. sion in a black-box manner to be able to gener-
Finally,westudyhow56linguisticfeaturesaf- alize to proprietary models. Formally, let a text-
fectgenerationvariability. Althoughresearchhas to-imagemodelbeG({w i};s,θ)whosecodomain
exploredoptimizingforimagevariabilityindiffu- comprises the sample space of all images I and
sion(Sadatetal.,2024),theyhavenotinvestigated domainthesequenceofwords{w i},randomseed
the contributing linguistic constructs. To under- s ∈ Z to initialize the image noise, and learned
standtheunderlyingstructureofthese56features, parametersθ ∈ Rp. Togeneratemultipleimages
weperformanexploratoryfactoranalysisoverDif- fromasingleprompt,astandardpracticeistorun
fusionDBanduncoverfourfactorsofkeywordpres- multipletrialsfordifferentrandomseedss(Podell
ence(e.g.,“dogwalking,4K,watercolor”),syntac- etal.,2024),whichwefollowinourexperiments.
ticcomplexity(e.g.,Yngvedepth),linguisticunit Ouranalysestargetthreestate-of-the-artmodels,
length, andsemanticrichness. Then, weconduct oneopenandtwoproprietary:
clean-room, single-word generation experiments
1. Stable Diffusion XL (Podell et al., 2024),
over the three strongest features in the semantic
an open model which uses CLIP (Radford
richness factor (concreteness, CLIP embedding
etal.,2021)forencodingtextanda2.6billion-
norm,andnumberofwordsenses)toassesstheir
parameterU-Net(Ronnebergeretal.,2015)for
contribution more precisely. We confirm that all
generatingimages.
three linguistic features significantly (p < 0.01)
correlate with perceptual variability for all three 2. DALL-E3(Betkeretal.,2023),aproprietary
diffusionmodelsstudied. API from OpenAI incorporating a pretrained
Our contributions are as follows: (1) we pro- T5-XXL(Raffeletal.,2020)textencoderand
pose and validate a human-calibrated framework thesameimagedecoderarchitectureasSDXL.
forbuildingperceptualvariabilitymetricsfromex-
3. Imagen(Sahariaetal.,2022),asimilarlypro-
istingperceptualdistancemetrics;(2)weexamine
prietaryAPIfromGoogleusingaT5-XXLen-
anewpracticalapplicationofthemethodinassess-
coderandanefficientvariantofasimilarcon-
ingpromptreusabilityintext-to-imagegeneration;
volutionalU-Netdecoder.
and (3) we provide original insight into the lin-
guistic sources of variability in diffusion models, All models produce images at least 1024×1024
findingthatkeywords,syntacticcomplexity,length, pixelsinresolution. Furtherdetailsaboutthethree
andsemanticrichnessinfluencevariability. modelscanbefoundinAppendixA.2.2.2 OurGeneralFramework
d(h, h) High
1 2
Weaimtomeasurethevisualvariabilityofasetof
E
Medium
syntheticimages. Towardthis,weproposetoaggre-
d(h, h)
1 3
gateperceptualdistances, whicharewellstudied Low
C D
intheliterature,amongallpairsofimagesinaset.
d(h, h)
2 3 None
Toaidhumaninterpretationofthedistances,weap-
A B F
plytwosteps:first,normalization,whichsquashes
potentiallyunboundedand“odd”distributionsinto Figure 2: An illustration of W1KP: image embed-
dings (see A) and pairwise distances (B) computed
the standard uniform distribution U[0,1]. For in-
using a backbone model, fed into the normalization
stance,aperceptualdistancewithatightrangeof
function(C;Eqn.1)producingasinglescorein[0,1].
5.10–5.19 across 1,000 image sets would be dif-
Thecalibrationmodule(D;Eqn.3)alignedtohuman
ficult to comprehend. Second, we calibrate the
judgements(E)thenassignsasimilaritylevel(F).
distancestogradedhumanjudgementsofsimilar-
ityanddeterminethecorrespondingcutoffpoints,
givingmeaningtoscoreranges(seeFigure3). Certainchoicesofhproduceestimatorsofinterest.
Concretely, let I := {I }n ⊆ I be an i.i.d. Weusetwoinourexperiments:
i i=1
sample of images generated by G(·). We seek a • Pairwise mean (η ): let d = d∗, α = 2,
mean
functionη(I)suchthatη(I′) < η(I)ifI′ ismore and h(x,y;d) = d(x,y). This measures the
self-similar than I is. A starting point is percep- expectedsimilarityamongallpairsofimages.
tual distance, a symmetric δ : I ×I (cid:55)→ R+ that
• k-expectedmaximum(η ): letd = d∗,α = k,
assigns larger values to less similar image pairs. k
and h(x ,...,x ) = min{d(x ,x ) : i ̸= j}.
Many metrics (Fu et al., 2024) embed I ,I ∈ I 1 α i j
a b
Thisquantifiestheexpectedmaximumsimilarity
using a feature extractor f : I (cid:55)→ Rℓ, such as
betweenapairofimagesinasetofsizek.
ViT(Dosovitskiyetal.,2021),thencomputeadis-
tanced : Rℓ×Rℓ (cid:55)→ R+ betweenf(I )andf(I ), Wenoteaconnectiontostatisticaldispersion: ifdis
a b
thesquaredEuclideandistanceandhthepairwise
e.g.,Euclideandistance. Tostandardizethesedis-
mean kernel, U is proportional to the trace of
tancestoU[0,1]forbetterinterpretability,weap- d,h
thecovariancematrixoff(I ),...,f(I ),i.e.,the
plythecumulativedistributionfunctiontransform, 1 n
definedasF(x) := P(X ≤ x). Ithastheproperty totalvariance. AproofisinAppendixB.Further-
more, to match the convention of scores in [0,1]
ofF(X)beinguniformlydistributed:
denotingsimilarityratherthandissimilarity(e.g.,
Proposition2.1. IfX isacontinuousrandomvari-
R2),fortherestofthispaperweinvertηandreport
able,F(X)isstandarduniformU[0,1].
η˜:= 1−η instead,callingittheW1KPscore.
Hence,anormalizedd∗ is
Lastly, we find cutoff points for η˜ calibrated
d∗(I ,I ) := F(d(f(I ),f(I ))), (1) to human-judged levels of high, medium, low,
a b a b
and no similarity. For the human judgement
andF isestimatedfromasample{d(I ,I )}m
ai bi i=1 data,wegatheradataset{(I ,I ,z )}N ,where
asFˆ(d(I ,I )) := |{d(I ,I ) ≤ d(I ,I ) : 1 ≤ xi yi i i=1
a b ai bi a b I ,I ∈ I are a pair of generated images from
i ≤ m}|/m. As our sample, we generate 10,000 xi yi
thesameprompt,andz ∈ {none,low,mid,high}
i
imagepairsperdiffusionmodelfor1,000randomly
isthehuman-annotatedlevelofsimilaritybetween
selectedDiffusionDBprompts.
I and I (see Section 3.2 for details). On the
Equippedwithauniformperceptualdistance,we
xi yi
dataset, we optimize the cutoff points β <
nowconstructmeasuresofimagesetvariability(η). low
β < β to maximize the label accuracy of
Anaturalframeworktodothisistodefineafamily mid high
the splits S := [0,β ), S := [β ,β ),
of U-statistics (Li, 2012; Hoeffding, 1948) over none low low low mid
S := [β ,β ),S := [β ,1.0]:
setsofimages: mid mid high high high
Definition2.1. Leth : Rℓ×···×Rℓ (cid:55)→ R+ bean N
1 (cid:88)
α-aritykernelparameterizedbyd. Thenafamily argmax
N
I(η˜({I xi,I yi})∈S zi), (3)
ofU-statisticsformeasuringimagesetvariability βlow,βmid,βhigh i=1
canbedefinedas where I is the indicator function. We illustrate
1 (cid:88) our overall method in Figure 2, and a proof of
U d,h(I):= (cid:0)n(cid:1) h(f(I i1),...,f(I iα);d). (2) Proposition2.1isgiveninAppendixB.
α 1≤i1<···<iα≤nSDXL Imagen DALL-E3 fromCLIP;andlastly,DreamSim(Fuetal.,2024),
Method
which ensembles pretrained transformers trained
2AFC Acc. 2AFC Acc. 2AFC Acc.
onStableDiffusionimagesforfeatureextraction
Oracle 80.0 100 80.7 100 79.3 100
andappliescosinedistanceformeasurement. Since
L2 54.8 55.4 61.0 63.3 58.5 60.1
DreamSim’s domain was closest to ours, we hy-
SSIM 55.2 56.7 59.1 61.7 57.6 59.3
pothesized that it would be most effective. We
LPIPS 64.7 68.6 67.6 72.0 64.8 70.8
ST-LPIPS 60.0 62.4 63.4 67.6 59.6 65.4 also evaluated our variant, DreamSim ℓ2, with L2
DISTS 65.5 69.4 67.5 71.9 63.7 67.5 instead of cosine distance for d, which benefits
SSCD(Large) 63.4 66.7 66.0 69.1 63.3 66.7
frombeingatruemathematicaldistanceandhence
CoPer(CLIP ) 63.2 67.8 64.4 68.9 62.4 67.9
B32
Raw(CLIP ) 67.3 72.4 70.3 76.3 67.3 75.0 allowsformultidimensionalscalinganalyses,asin
L14
AppendixE.
DreamSim(Orig.) 69.2 75.0 71.3 77.3 70.3 77.9
DreamSim ℓ2 (Ours) 69.3 75.2 71.5 77.5 70.7 78.3 We used the standard evaluation metrics of
2AFC score, defined as the mean proportion of
Table1:Qualityofthebackbonesonourevaluationsets,
workersagreeingwiththebackbone’sscores,i.e.,
acrosstheimagegenerationmodel.
1 (cid:80)M I(I ≻ I )yai +I(I ≺ I )(1−yai),
M i=1 ai r bi 5 ai r bi 5
where I ≺ I if η˜({I ,I }) < η˜({I ,I }),
ai r bi ri ai ri bi
3 VeracityAnalyses andmajority-voteaccuracy. Weletη˜= η˜ . See
mean
AppendixA.3forfurthersetupdetails.
Before applying W1KP, we first verify the qual-
ityoftheperceptualdistancebackboneandinter- Results. We present our results in Table 1. As
pretabilityofthescoreonhumanjudgements. an upper bound, we report the maximum possi-
ble 2AFC and accuracy in row one. In line with
3.1 W1KPQuality intuition,ourDreamSimbackbonesattainthehigh-
est quality, surpassing CLIP raw, the second
Setup. Followingpriorworkinperceptualdistance L14
best,by2.0pointsin2AFCand2.8inaccuracyon
evaluation(Zhangetal.,2018),wecrowd-sourced
average. OurvariantDreamSim slightlyoutper-
adatasetoftwo-alternativeforced-choice(2AFC) ℓ2
formstheoriginalDreamSimwithstatisticalsignif-
imagetripletsusingAmazonMTurk(Hauserand
icance(p < 0.05onthepairedt-test)by0.1–0.4in
Schwarz,2016). Fiveuniqueworkerswereshown
2AFCand0.2–0.4inaccuracy,possiblysincethe
three generated images from the same prompt—
embedding norm is informative. Thus, we select
a reference image, image A, and image B—and
DreamSim asthebackboneforW1KP.
instructed to pick whether A or B resembled the ℓ2
referencemore. Thiswasrepeatedthreetimeseach Beyondqualityassurance,anotherpurposethis
for500randompromptsfromDiffusionDB,alarge evaluation serves is to ensure that the backbone
datasetofuser-writtenprompts,foreachofSDXL, does equally well on the three image generators.
Imagen,andDALL-E3,totaling1,500tripletsper Asasanitycheck,theoracle(rowone)hasaspread
model. Formally, let {(I ,I ,I ,y )}M be a of 1.4 points (79.3–80.7) in 2AFC on the three
ri ai bi ai i=1
dataset of M triplets, where I ,I ,I ∈ I are models,indicatingthathumansareunbiased. Our
ri ai bi
imagesandy ai ∈ {0,...,5}thenumberofwork- DreamSim ℓ2 hasaspreadof2.2points(69.3–71.5)
erschoosingI overI . Weusedattentionchecks in2AFC,whichisbelowtheglobalaveragespread
ai bi
throughout the process; for more details, see Ap- of3.3pointsforallthemethods. Weconcludethat
pendixA.3. DreamSim ℓ2 exhibitslessmodel-wisebiasthanits
Forournon-neuralmethods,weevaluatedraw- counterparts,possiblyduetoitsincreasedquality
image Euclidean distance (L2) and the struc- andin-domaintraining.
tural similarity index (SSIM; Wang et al., 2004). A potential issue is that perceptual similarity
For our neural backbones, we tested the popular is inherently subjective and hence challenging to
LPIPS(Zhangetal.,2018),itsshift-tolerantvari- measure. Researchsuggeststoalsoevaluatejust-
ant ST-LPIPS (Ghildyal and Liu, 2022), and an noticeabledifferences(JND),whichisthoughtto
SSIM-inspiredvariantDISTS(Dingetal.,2020), becognitivelyimpenetrableduetoitsviewing-time
all based on VGG-16 (Simonyan and Zisserman, constraint(Acunaetal.,2015). Becauseofthehigh
2015);SSCD(Pizzietal.,2022),amodeltrained correlation between2AFC and JND on synthetic
forimagecopydetection;CoPer(Lietal.,2022), images(r = 0.94;Fuetal.,2024),2AFCappears
anextensionofLPIPStoViT;rawcosinesimilarity tobeaviableproxyforJNDforourstudy.5 10 50 200
Figure3: ImagepairsfromSDXL,orderedrow-wiseby
Number of Images per Prompt
calibratedW1KPscores. Fromtoptobottom,therows
correspondtohigh(0.85–1.0),medium(0.4–0.85),low Figure4: Visualizingtheoverlapbetweenthetwomost
(0.2–0.4),andnosimilarity(0.0–0.2). similarimages(onaverage)aswegeneratemoreimages
forthetwoprompts. Weremovethegreenchannelfor
oneimage(magenta)andkeeponlythegreenforthe
3.2 W1KPMetricInterpretation other,thenstackthetwo. Above,Imagenisreusableup
to10–50images,whileDALL-E3upto50–200.
Setup. ForcalibratingW1KPasdescribedinSec-
tion2.2,wecollectedacrowd-sourceddatasetof
gradedimagepairswithMTurk. For500random
Wepresentqualitativeexamplesofourcutoffs
DiffusionDBprompts,threeuniqueworkerswere
in Figure 3. The levels appear sensible: “high”
presentedwithageneratedpairofimagesfromthe
pairs (top row) match in low-level features (e.g.,
samepromptandaskedtojudgetheirsimilarityon
treesinthesamelocation),high-levelcomposition
afive-pointLikertscalerangingfrom“notsimilar
(e.g.,catsinwashingmachine),artisticstyle(e.g.,
at all” (rating 1) to “the same” (5). Afterwards,
colorphotography);medium(second)incomposi-
we merged the last two categories (“same” and
tionandstyle;low(third)instyle;andnone(last)
“verysimilar”)sincethefifthwasmostlyreserved
mostly differing in all. We also verify that nor-
forattentionchecks,resultinginthefinalfourcat-
malization(Eqn.1)isnecessaryfortherawscores.
egories of high, medium, low, and no similarity. Beforenormalization,rawW1KPscoreshave10th,
We took the median across the three judgements 50th,and90th percentilesof0.4,0.7,and1.1,sig-
andrepeatedtheprocessforSDXL,Imagen,and
nificantly deviating from a uniform distribution
DALL-E3,foratotalof1,500medianjudgements
(p < 0.01accordingtotheKStest).
roughly split into 10%, 30%, 40%, and 20% for
Oneconceivablequestioniswhethercalibration
ratings 1–4. Our evaluation then consisted of ap-
and normalization are essential for downstream
plyingEqn.(3)withfive-foldcrossvalidation. For
analysis. Itcanbearguedthatanalyticconclusions
detailedannotationsettings,seeAppendixA.3.
maystillholdwithoutanormalized,calibratedmet-
Results. Eqn.(3)yieldscutoffpoints(roundedto ric. However, as alluded to in Section 2.2, there
thenearest0.05formemorability)of0.2,0.4,and aretwoclearbenefitstohavingone: first,normal-
0.85 for β , β and β . Overall, we attain izationscalesarbitraryscorestothe0–1range,in
low mid high
macro-andmicro-accuracyscoresof80%and78% linewithothercommonstatisticssuchasF score
1
withDreamSim asthebackbone. Forcomparison, andR2. Ournormalizedscorealsohasthedirect
ℓ2
the average macro-/micro-accuracy scores of hu- interpretationasthepercentileoftherawscoreon
mansare82%/80%. DreamSim alsooutperforms a known ground-truth distribution. Second, cali-
ℓ2
theoriginalDreamSim,whichhasamacro-/micro- brationallowsustointerpretscoresandaidhuman
accuracyof79%/77%. Thus,weconcludethatour understanding. InSection4.1forexample,weuse
calibrationyieldsinterpretablecutoffs. β asacutoffforpromptreusability.
high
hgiH
muideM
woL
enoN
3
E-LLAD
negamIPrompt Reusability by Model 4.2 ExploratoryFactorAnalysis
1.0
0.9 Our next two analyses relate various linguistic
0.8 features of prompts such as syntactic complexity
0.7 Model to perceptual variability. First, to understand the
0.6 DALL-E 3
salient structure of these linguistic features, we
0.5 SDXL
0.4 Imagen conductafactoranalysisoverDiffusionDB.
2 5 10 25 50 100 300 Setup. Our analysis emulates previous work in
Number of Images per Prompt (k)
interpreting linguistic features for speech (Fraser
Figure5: k-expectedmaximum(η˜ )fork =2to300. etal.,2016). Weextracted56featuresforeachof
k
Shaded regions denote 95% confidence intervals and the1,000randomprompts:
theredlineβ .
high • Syntacticcomplexity: 24scalarfeaturesrelated
to syntax comprehension, such as clauses per
4 VisuolinguisticAnalyses T-unit and mean T-unit length, extracted using
L2SCA(Lu,2010). WealsoaddedYngvedepth,
Withthevariabilitymetricestablished,wenowin- ameasureofembeddedness(Yngve,1960). Our
vestigatetheconnectionbetweenvisualvariability motivationwasthatsentenceswithmorequali-
andpromptlanguagefortext-to-imagemodels. fiersandnominalsmaybemorevisuallyprecise.
• Keywords: 20 boolean features indicating the
4.1 PromptReusabilityAnalysis
presence of the top-20 keywords. We had no-
We first ask how many times a prompt can be ticedthatmostpromptscontainedtrailingkey-
reused (under different random seeds) until new word qualifiers after a noun phrase, e.g., “cat
images are too similar to already generated ones. beside road, 4k” (see Appendix C for more);
Thisappliestographicassetcreationinparticular, thus,weextractedthetop20asfeatures.
wherevisualartistsaretaskedwithrenderingmany
• Word order: 3 boolean features denoting the
imagesofthesameconcept. Tostudythisquantita-
presenceofthePTB(Marcinkiewicz,1994)part-
tively,wesampled50randompromptsfromDiffu-
of-speechpatterns“NNVB,”“NNVBRB,”and
sionDB,generated300imagesforeachpromptus-
“JJNN”intheprompt. Ourpurposewastoassess
ingdifferentseedsonSDXL,Imagen,andDALL-E
theeffectsofadjectivesandverbsonnouns.
3,thencomputedthek-expectedmaximumη˜ for
k • Psycholinguistics: 4featuresinmeanconcrete-
k = 1,...,300.
ness judgements (Brysbaert et al., 2014), rich-
AsvisualizedinFigure4andplottedinFigure5,
ness (Honore’s statistic and whether a word
our diffusion models vary in reusability. DALL-
was in a 100k-word dictionary), and word fre-
E 3 on average does not generate highly similar
quency(BrysbaertandNew,2009).
images (η˜ ≥ β ) until k → 200, with our vi-
k high
• Semanticrelations: 3scalarsforthemeannum-
sualization(toptworowsinFigure4,oneprompt
berofhyponyms,hypernyms,andwordsenses,
each)displayingmuchgreen-andmagenta-shifting
from WordNet (Miller, 1995) enhanced with
untilthelastcolumn. Ontheotherhand, Imagen
wordsenseclustering(Snowetal.,2007). Intu-
tendstoproduceduplicateimagesfork → 50. At
itively,wordswithmanysynonyms(e.g.,“saw”)
50images,thetwooverlaidimagesarenearlyin-
or hyponyms (e.g., “animal”) may have more
distinguishablefromthetrue-colorimage;seethe
visualrepresentations.
third column. Figure 5 corroborates these visual
results, with the red line (β ) intersecting Ima- • Embedding norm: 2 scalars for the mean
high
gen’s green line between 5–10 and DALL-E 3’s squareGloVenorm(Penningtonetal.,2014)and
blue line at 50–100. It also suggests that SDXL CLIP embedding norm (Radford et al., 2021).
resemblesDALL-E3inpromptreusability;seethe Wordembeddingnormswerefoundtoencodein-
overlapbetweenthetwo. Weconcludethatdiffu- formationgain(Oyamaetal.,2023),whichmay
sionmodelsdifferinpromptreusability,possibly affectperceptualvariabilitythroughspecificity.
duetodifferentdecoderarchitectures. Forexample, We generated 20 images per prompt for SDXL,
DALL-E3andSDXLsharethesameU-Netarchi- Imagen, and DALL-E 3 and used Stanford
tecture, whereas Imagen’s is sparsified (Saharia CoreNLP(Manningetal.,2014)asourparser(ad-
etal.,2022). ditionaldetailsinAppendixD.1).
)k
( PK1W
xaM
.pxE-k# Name Fac.1 Fac.2 Fac.3 Fac.4 ρ µ OurfeaturecorrelationswithW1KPagreewith
Factor1:StyleKeywordPresence;Mean|ρ|=0.12 intuition. Havinghigherconcreteness(e.g.,house
1 Keyword:cgsociety 0.80 0.09 0.05 vs.dignity)andfewerwordsenses(sawvs.tomato)
2 Keyword:8k 0.75 -0.10 0.12 0.17
3 Keyword:detailed 0.75 0.14 0.05 increasessimilarity(rows20,21),likelysinceab-
4 Keyword:artgerm 0.66 0.15 0.06
stractandpolysemouswordshavemorevisualin-
5 Keyword:cinematic 0.59 0.11 0.04
6 Keyword:digitalart 0.43 0.10 0.04 terpretations. Complexnominals(row11),adjec-
Factor2:SyntacticComplexity;Mean|ρ|=0.09 tivalmodifiers(row18),andkeywords(F1)limit
7 ClausesperT-unit(T) 1.08 -0.13 -0.13 0.07 0.69
variabilitythroughqualification. Semanticrichness
8 Clausespersentence 0.92 -0.13 0.05 0.69
9 NumberofT-units 0.63 -0.37 0.20 0.07 0.92 hasthestrongestcorrelatedfeatures,withhalfhav-
10 Verbphrases/T -0.11 0.47 0.05 0.50
11 Complexnominals/T -0.12 0.46 0.46 0.12 0.19 2.16 ing |ρ|>0.2. CLIP norm is the most predictive
Factor3:LinguisticUnitLength;Mean|ρ|=0.19 of variability (ρ=−0.31), possibly because text
12 MeanT-unitlength 0.49 0.60 0.17 0.18 16.7 embeddingsfromvision-languagemodelsareused
13 Meanclauselength 0.45 0.53 0.19 0.18 15.9
14 Meansentencelength 0.51 0.45 0.27 21.4 to initialize image generation (Sec. 2.1). Larger
15 Coordinatephrases/T 0.15 0.20 0.27 0.13 0.33 normsmayyield morechaoticdecodingtrajecto-
Factor4:SemanticRichness;Mean|ρ|=0.17
ries in the iterative solver, increasing variability.
16 Numberofwords 0.12 0.11 0.75 0.30 24.6
17 CLIPembeddingnorm 0.17 -0.61 -0.31 151 Factor-wise,linguisticunitlengthhasthehighest
18 ADJ NOUN 0.55 0.21 0.82 mean|ρ|of0.19,wheresentencelengthisthethird
19 Percentageofkeywords 0.20 0.11 0.55 0.20 48.8
20 Meanconcreteness 0.47 0.25 2.30 mostpredictivefeature(ρ=0.27). Longerprompts
21 Mean#ofwordsenses -0.11 0.43 -0.18 2.58
presumablyprovidemorevisualinformation. We
22 Honore’sstatistic -0.38 -0.09 7.36
23 Notindictionary 0.29 0.09 0.91 concludethatmanyfeaturesinthelinguisticspace
24 Keyword:elegant 0.21 0.04 0.04
25 Keyword:fantasy 0.15 0.05 0.04 arepredictiveofvariabilityinthevisualspace,es-
peciallyCLIPnorm,length,andconcreteness.
Table2: Linguisticfeaturesgroupedbyinterpretedfac-
tors, with high loadings (≥0.3) in bold and low load-
4.3 ConfirmatoryLexicalAnalysis
ings(<0.1)removed. AllSpearman’sρarestatistically
significant(p<0.05);insignificantfeaturesomitted. Thelastsectionstudieshowpromptsrelatetovari-
abilityintheDiffusionDBcorpus. Whileitbene-
fitsfromrealism,someexperimentalcontrolislost.
Results. WepresentourresultsinTable2. Follow- Thus, to supplement the previous study, this sec-
ingstandardpractice(Fraseretal.,2016),weuse tionusessingle-wordsyntheticprompts,sampled
an oblique promax rotation to enable interfactor andadjustedforwordfrequencyinaclean-room
correlation. Fourfactorscapturesufficientvariance manner. We examine the effects of concreteness,
accordingtoKaiser’scriterion(Kaiser,1958). For CLIPnorm,andpolysemy—threeofthestrongest
eachfeature,wereportitscorrelation(Spearman’s featuresfromSec.4.2.
ρ)withtheper-promptperceptualsimilarity(η˜ )
mean Setup. For our prompts, we sampled 500 words
andcomputethemeanfeaturescoreµ.
fromthe10kmostcommonwordsintheGoogle
Asisconventional,wemanuallyexplainthefour TrillionWordCorpus(BrantsandFranz,2006). We
factors (F1–F4). For F1, “8k,” “detailed,” “cine- notedeachword’sconcretenessrating(x ),num-
conc
matic,”and“digitalart”describetheartstyle,“cg- berofwordsenses(x ),CLIPembeddingnorm
sens
society” pertains to computer graphics, and “art- (x ), and frequency rank (x ) as our explana-
clip freq
germ”isanartistwithaspecificstyle;hence,we toryvariables,mirroringthesetupofSection4.2.
call it “style keyword presence.” F2’s features Words without concreteness ratings were resam-
areclassicmeasuresofsyntacticcomplexity(Lu, pled. Wethengenerated20imagesforeachprompt
2010)andthuslabeledassuch. InF3,meanlength withSDXL,Imagen,andDALL-E3andmeasured
of clauses, sentences, and T-units quantify vari- perceptualvariabilityusingη˜ . Forouranalysis,
mean
ouslengths,sowenameit“linguisticunitlength.” wefitalinearmixedmodelwithx ,x ,x ,
conc sens clip
Lastly,F4primarilydepictssemanticrichness,with andx asthefixedeffects,aninterceptforeach
freq
concreteness,CLIPembeddingnorm(relatedtoin- diffusion model as the random effect, and η˜
mean
formationgain),numberofwordsenses,andADJ as the response variable. Our purpose is to test
NOUNroughlycharacterizingvisual(non)ambiguity whetherconcreteness,polysemy,CLIPnorm,and
and Honore’s statistic, the number of words, and wordfrequencyindependentlyinfluenceperceptual
“notindictionary”portrayinglexicalrichness. variabilityforeachmodel.Similarity by Word Frequency Similarity by CLIP Norm
Model
0.5 0.5
DALL-E 3
=0.05 =-0.04
0.4 SDXL 0.4
Imagen
0.3 0.3
=0.14
0.2 =-0.15
0.2
0.1 =0.29 0.1 =-0.29
0 2500 5000 7500 250 300 350 400 450
Frequency Rank CLIP Norm
Similarity by Concreteness Similarity by # Word Senses
0.5
0.6 0.4 =-0.11
0.4 =0.13 0.3
0.2 =-0.13
0.2 =0.38
=0.40 0.1 =-0.17
1 2 3 4 5 0 10 20 30
Concreteness MOS # Word Senses
Figure6: Aplotofη˜ againstfrequency,CLIPnorm, Figure7: Foursingle-wordImagenpromptswithvary-
mean
concreteness,andwordsensesforsingle-wordprompts. ingconcreteness(“cowboy”vs. “concept”)andnumber
Shadedregionsare95%confidenceintervals. ofwordsenses(“tomato”vs. “saw”).
Results. Our linear mixed model reveals statisti- preciselinguisticfeaturescontributingtovariabil-
cally significant relationships (p<0.01) between ity. One future direction could be to incorporate
η˜ and all the predictors, whose coefficients thesefeaturesintotheoptimizationofvariability.
mean
are 2.4 × 10−3, 4.7 × 10−4, −7.8 × 10−5, and Previous work has analyzed diffusion models
−7.2 × 10−2 for x , x , x , and x , re- usingusingamixtureofcomputationallinguistics
sens clip freq conc
spectively. Inotherwords,polysemy,CLIPnorm, andvisiontechniques. Tangetal.(2023)conducted
wordfrequency,andconcretenessaresignificantin- an attribution analysis over Stable Diffusion and
dependentfactorsforperceptualvariability,where discovered entanglement, to which Rassin et al.
polysemyandCLIPnormarepositivelycorrelated, (2024) proposed to fix using attention alignment.
whilefrequencyandconcretenessnegativelyso. In Separately, Toker et al. (2024) studied the layer-
Figure 6, our feature-wise plots further illustrate intermediaterepresentationsofdiffusion,showing
eachindividualfixedeffect. Thecorrelationscores that rare concepts require more computation. A
areconsistentindirectionacrossthediffusionmod- furtherextensioncouldbetostudylinguisticfea-
els, with similar signs in Spearman’s ρ for each turesresponsibleforincreasedcomputation,asour
feature. They also differ by an additive shift, af- paperalsorelateswordraritytovariability.
firmingourrandom-interceptsmixedmodel. Finally, research has previously scrutinized
Figure7presentspromptsofvaryingconcrete- the(lackof)variabilityinolderarchitecturessuch
nessandsenses. “Cowboy,”aconcreteprompt,is asVAEs(Razavietal.,2019)andgenerativeadver-
lessvariablethan“concept,”anabstractone,since sarialnetworks,e.g.,modecollapse. Inthispaper,
a cowboy is tangible. “Tomato,” a monosemous weextendthisanalysistomoderndiffusionmodels
word,haslessvariabilitythan“saw,”apolysemous whiletakingavisuolinguisticperspective.
word,becauseithasanarrowvisualrepresentation.
Insummary,ourexploratoryfindingsonconcrete- 6 Conclusions
ness,CLIPnorm,andpolysemyfromSection4.2
In conclusion, we examined the connection be-
holdintheclean-roomsingle-wordpromptsetting.
tween visual variability and prompt language for
black-boxdiffusionmodels. Weproposedaframe-
5 RelatedWorkandFutureDirections
work for quantifying and calibrating visual vari-
A related line of work examines boosting image ability,applyingittostudypromptreusabilityand
variability in diffusion models (Zameshina et al., linguisticfeaturesalience. Aftervalidatingitquan-
2023; Sadat et al., 2024; Gu et al., 2024). Com- titatively,wefoundthatlength,embeddingnorm,
plementary to their work, our paper analyzes the andconcretenessinfluencevariabilitythemost.
erocS
PK1W
erocS
PK1W
erocS
PK1W
erocS
PK1W
ssenetercnoC
sesneS
droWLimitations MarcBrysbaertandBorisNew.2009. Movingbeyond
kucˇeraandfrancis: Acriticalevaluationofcurrent
One limitation of our work is that while we ana- wordfrequencynormsandtheintroductionofanew
lyzed the inference-time behavior of various dif- andimprovedwordfrequencymeasureforamerican
fusionmodels, wedidnottracethetraining-time english. Behaviorresearchmethods.
causeofperceptualvariabilityduetothescopeof
Marc Brysbaert, Amy Beth Warriner, and Victor Ku-
ourstudy. Doingsowouldrequirethetrainingof perman.2014. Concretenessratingsfor40thousand
multiplediffusionmodelswhilevaryingthetrain- generally known English word lemmas. Behavior
researchmethods.
ingsets,whichisbeyondourbudget.
Another limitation is that we have not metic- Keyan Ding, Kede Ma, Shiqi Wang, and Eero P. Si-
ulously characterized the precise distribution of moncelli.2020. Imagequalityassessment: Unifying
perceptualvariabilityrelativetovariouslevelsof structureandtexturesimilarity. IEEEtransactions
onpatternanalysisandmachineintelligence.
linguistic features, with our analyses constrained
toaveragesduetothemoderatesamplesize. For Alexey Dosovitskiy, Lucas Beyer, Alexander
instance, does Imagen yield a higher maximum Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
ThomasUnterthiner,etal.2021. Animageisworth
variabilityforcertainlevelsofconcreteness,even
16x16words: Transformersforimagerecognitionat
if on average it is lower? Are there subgroups
scale. InICLR.
withineachfeaturethatbetterexplainvariancesin
perceptual variability? Such questions require a Kathleen C. Fraser, Jed A. Meltzer, and Frank Rudz-
icz.2016. LinguisticfeaturesidentifyAlzheimer’s
largersamplesizetoanswer.
diseaseinnarrativespeech. JournalofAlzheimer’s
To limit the scope of the paper, we also con- disease.
sciously restricted our examination to random
StephanieFu,NetanelTamir,ShobhitaSundaram,Lucy
seedsanddispensedwithcomprehensivelyassess-
Chai,RichardZhang,TaliDekel,andPhillipIsola.
ing other factors possibly influencing perceptual
2024. DreamSim: Learningnewdimensionsofhu-
variability,suchasclassifier-freeguidance(Hoand manvisualsimilarityusingsyntheticdata. NeurIPS.
Salimans, 2021). We briefly vary the guidance
Abhijay Ghildyal and Feng Liu. 2022. Shift-tolerant
scale in Appendix D.2 to confirm that SDXL is
perceptualsimilaritymetric. InECCV.
always more diverse than Imagen regardless of
guidance;nevertheless,afullstudywithadditional Jiatao Gu, Ying Shen, Shuangfei Zhai, Yizhe Zhang,
Navdeep Jaitly, and Joshua M. Susskind. 2024.
factors other than linguistic features and random
Kaleido diffusion: Improving conditional diffu-
seedscouldyieldfurtherinsights.
sion models with autoregressive latent modeling.
Finally, it should be noted that our work inten- arXiv:2405.21048.
tionallydisregardstherelationshipbetweenquality
DavidJ.HauserandNorbertSchwarz.2016. Attentive
andvariability,althoughthetwocanbeconflated.
turkers: MTurkparticipantsperformbetterononline
Forexample,doesincreasedvariabilityreduceim- attention checks than do subject pool participants.
age quality? Is Imagen a better option than, say, Behaviorresearchmethods.
SDXLduetoitshigherquality,evenifitgenerates
JonathanHoandTimSalimans.2021. Classifier-free
lessdiverseimagery? Thus,text-to-imagemodels
diffusionguidance. InNeurIPS2021Workshopon
shouldnotbechosenbasedonthefindingsofour DeepGenerativeModelsandDownstreamApplica-
studyalone. Rather,ourworksupplementsimage tions.
qualitymetricsinmodelselection.
Wassily Hoeffding. 1948. A class of statistics with
asymptotically normal distribution. The Annals of
MathematicalStatistics.
References
HenryF.Kaiser.1958. Thevarimaxcriterionforana-
Daniel E. Acuna, Max Berniker, Hugo L. Fernandes,
lyticrotationinfactoranalysis. Psychometrika.
andKonradP.Kording.2015. Usingpsychophysics
toaskifthebrainsamplesormaximizes. Journalof Hongwei Bran Li, Chinmay Prabhakar, Suprosanna
vision. Shit,JohannesPaetzold,TamazAmiranashvili,Jian-
guo Zhang, Daniel Rueckert, et al. 2022. A
JamesBetker,GabrielGoh,LiJing,TimBrooks,Jian-
domain-specific perceptual metric via contrastive
fengWang,LinjieLi,LongOuyang,JuntangZhuang,
self-supervisedrepresentation: Applicationsonnatu-
JoyceLee,YufeiGuo,etal.2023. Improvingimage
ralandmedicalimages. arXiv:2212.01577.
generationwithbettercaptions. OpenAIBlog.
ThorstenBrantsandAlexFranz.2006. Web1T5-gram
version1. LinguisticDataConsortium.HongzheLi.2012. U-statisticsingeneticassociation OlafRonneberger,PhilippFischer,andThomasBrox.
studies. Humangenetics. 2015. U-Net:Convolutionalnetworksforbiomedical
imagesegmentation. InInternationalConferenceon
XiaofeiLu.2010. Automaticanalysisofsyntacticcom- MedicalImageComputingandComputer-Assisted
plexity in second language writing. International Intervention.
journalofcorpuslinguistics.
SeyedmortezaSadat,JakobBuhmann,DerekBradley,
ChristopherD.Manning,MihaiSurdeanu,JohnBauer, Otmar Hilliges, and Romann M. Weber. 2024.
JennyRoseFinkel,StevenBethard,andDavidMc- CADS:Unleashingthediversityofdiffusionmodels
Closky. 2014. The Stanford CoreNLP natural lan- throughcondition-annealedsampling. InICLR.
guage processing toolkit. In ACL: System Demon-
strations. ChitwanSaharia,WilliamChan,SaurabhSaxena,Lala
Li, Jay Whang, Emily Denton, et al. 2022. Photo-
MaryAnnMarcinkiewicz.1994. Buildingalargeanno- realistic text-to-image diffusion models with deep
tatedcorpusofEnglish: ThePenntreebank. Using languageunderstanding. arXiv:2205.11487.
LargeCorpora.
Christoph Schuhmann, Romain Beaumont, Cade W.
GeorgeA.Miller.1995. WordNet: alexicaldatabase Gordon,RossWightman,TheoCoombes,etal.2022.
forEnglish. CommunicationsoftheACM. LAION-5B:Anopenlarge-scaledatasetfortraining
nextgenerationimage-textmodels.
MomoseOyama,ShoYokoi,andHidetoshiShimodaira.
2023. Normofwordembeddingencodesinformation KarenSimonyanandAndrewZisserman.2015. Very
gain. InEMNLP. deep convolutional networks for large-scale image
recognition. InICLR.
JeffreyPennington,RichardSocher,andChristopherD.
Manning. 2014. GloVe: Global vectors for word Rion Snow, Sushant Prakash, Dan Jurafsky, and An-
representation. In Empirical Methods in Natural drewY.Ng.2007. Learningtomergewordsenses.
LanguageProcessing. InEMNLP-IJCNLP.
Ed Pizzi, Sreya Dutta Roy, Sugosh Nagavara Ravin- YangSong,JaschaSohl-Dickstein,DiederikP.Kingma,
dra,PriyaGoyal,andMatthijsDouze.2022. Aself- Abhishek Kumar, Stefano Ermon, and Ben Poole.
superviseddescriptorforimagecopydetection. In 2021. Score-based generative modeling through
CVPR. stochasticdifferentialequations. InICLR.
Dustin Podell, Zion English, Kyle Lacey, Andreas Raphael Tang, Linqing Liu, Akshat Pandey, Zhiying
Blattmann,TimDockhorn,JonasMüller,JoePenna, Jiang,GefeiYang,KarunKumar,PontusStenetorp,
andRobinRombach.2024. SDXL:Improvinglatent JimmyLin,andFerhanTüre.2023. WhattheDAAM:
diffusionmodelsforhigh-resolutionimagesynthesis. Interpretingstablediffusionusingcrossattention. In
InICLR. ACL.
AlecRadford,JongWookKim,ChrisHallacy,Aditya MichaelToker,HadasOrgad,MorVentura,DanaArad,
Ramesh,GabrielGoh,SandhiniAgarwal,GirishSas- andYonatanBelinkov.2024. Diffusionlens: Inter-
try, Amanda Askell, Pamela Mishkin, Jack Clark, pretingtextencodersintext-to-imagepipelines. In
GretchenKrueger,andIlyaSutskever.2021. Learn- ACL.
ingtransferablevisualmodelsfromnaturallanguage
supervision. InICML. Zhou Wang, Alan C. Bovik, Hamid R Sheikh, and
EeroP.Simoncelli.2004. Imagequalityassessment:
ColinRaffel,NoamShazeer,AdamRoberts,Katherine from error visibility to structural similarity. IEEE
Lee,SharanNarang,MichaelMatena,YanqiZhou, transactionsonimageprocessing.
WeiLi,andPeterJ.Liu.2020. Exploringthelimits
oftransferlearningwithaunifiedtext-to-texttrans- Zijie J. Wang, Evan Montoya, David Munechika,
former. TheJournalofMachineLearningResearch. HaoyangYang,BenjaminHoover,andDuenHorng
Chau. 2023. DiffusionDB: A large-scale prompt
Royi Rassin, Eran Hirsch, Daniel Glickman, Shauli gallerydatasetfortext-to-imagegenerativemodels.
Ravfogel, Yoav Goldberg, and Gal Chechik. 2024. InACL.
Linguisticbindingindiffusionmodels: Enhancing
attributecorrespondencethroughattentionmapalign- Victor H. Yngve. 1960. A model and an hypothesis
ment. NeurIPS. forlanguagestructure. ProceedingsoftheAmerican
philosophicalsociety.
Ali Razavi, Aaron Van den Oord, and Oriol Vinyals.
2019. Generatingdiversehigh-fidelityimageswith Mariia Zameshina, Olivier Teytaud, and Laurent
VQ-VAE-2. NeurIPS. Najman. 2023. Diverse diffusion: Enhanc-
ing image diversity in text-to-image generation.
RobinRombach,AndreasBlattmann,DominikLorenz, arXiv:2310.12583.
Patrick Esser, and Björn Ommer. 2022. High-
resolutionimagesynthesiswithlatentdiffusionmod- Richard Zhang, Phillip Isola, Alexei A. Efros, Eli
els. InCVPR. Shechtman,andOliverWang.2018. Theunreason-
able effectiveness of deep features as a perceptual
metric. InCVPR.Figure8: Interfaceforcollecting2AFCjudgements.
Figure9: Interfaceforcollectinggradedjudgements.
A DetailedExperimentalSettings
A.1 ComputationalEnvironment Forhigherquality,werequiredourworkerstobe
“Masters”forparticipationeligibility.
OurprimarysoftwaretoolkitsincludedHugging-
Face Diffusers 0.25.0, Transformers 4.40.1, Py- W1KPmetricinterpretation. Wepresentouran-
Torch2.1.2,DreamSim0.1.3,andCUDA12.2. We notation interface for gathering graded similarity
ranallexperimentsonamachinewithfourNvidia judgementsinFigure9. Fortheattentionchecks,
A6000GPUsandanAMDEpycMilanCPU. weshowedeachannotatoratleastonepairofim-
ages that were the exact same. If they did not
A.2 DiffusionModelDetails
choose “almost the same,” we discarded all their
SDXL. We downloaded stabilityai/stable- judgements,resultinginanacceptancerateof95%.
diffusion-xl-base-1.0fromHuggingFacezoo.
B DetailedProofs
Weusedthedefaultguidancescaleof7.5and30in-
ferencestepswithouttheadditionalrefinermodule. Proposition 2.1. If X is a continuous random
Each 1024x1024SDXL imagetook 4–5 seconds variable,F(X)isstandarduniformU[0,1].
to generate per card, resulting in a throughput of
Proof. LetX beacontinuousr.v. IfX isU[0,1],
roughly50–60imagesperminute.
then its CDF P(X ≤ x) = x. Since P(F(X) ≤
Imagen. Weselectedtheimagegeneration@006
x) = P(X ≤ F−1(x)) = F(F−1(x)) = x, then
model,thelatestversionasofApril2024,andgen-
F(X)isU[0,1],completingourproof.
erated four square images per call while varying
therandomseed. ThismatchedourSDXLthrough- Proposition 2.2. If d is the squared Euclidean
putof50–60imagesperminute. Eachimagewas distance and h the pairwise mean kernel, U is
d,h
1536x1536inresolution. proportionaltothetraceofthecovariancematrix
off(I ),...,f(I ),i.e.,thetotalvariance.
DALL-E 3. For DALL-E 3, we used the default 1 n
parameters of “hd” resolution (1024x1024) and Proof. Consider the pairwise sum squared Eu-
“vivid” style. To mitigate prompt editing, we fol- clidean distance (cid:80) ||f(I ) − f(I )||2, which
i̸=j i j 2
lowedtheofficialdocumentationandprepended“I
expandsinto
NEEDtotesthowthetoolworkswithextremely
(cid:88) ⊺ ⊺ ⊺
simpleprompts. DONOTaddanydetail,justuse f(I ) f(I )−2f(I ) f(I )+f(I ) f(I ). (4)
i i i j j j
itAS-IS:”totheprompt. Thegenerationspeedof i̸=j
DALL-E3wasconsiderablyslowerthanImagen Thefirstandthirdself-producttermsexpandsas
andSDXLatapproximately10imagesperminute.
n
(cid:88) ⊺
(n−1) f(I ) f(I ) (5)
A.3 AnnotationApparatuses i i
i=1
W1KPquality. Wepresenttheannotationuserin-
and
terfaceforcollecting2AFCjudgementsinFigure8. n
(cid:88) ⊺
(n−1) f(I ) f(I ), (6)
Forourattentionchecks,weshowedeachworker j j
j=1
atleastonetripletwitheitherimageAorBexactly
andthemiddleterm
matchingthereference. Ifthecorrectanswerwas
notchosen,werejectedalltheirlabelsandblocked (cid:88) ⊺ (cid:88)n ⊺
f(I ) f(I )− f(I ) f(I ). (7)
them. Thisresultedinapassrateofaround90%. i j i i
i,j i=1Afteralgebraicmanipulation,wearriveat 7. “giant oversized battle hedgehog with army
pilot uniform and hedgehog babies ,in deep
(cid:32) n (cid:33)
1 (cid:88) ⊺ 1 (cid:88) ⊺ forest hungle , fullbody , Cinematicfocus,
(n−1) f(I ) f(I )− f(I ) f(I ) . (8)
n i i n2 i j
i=1 i,j Polaroidphoto, vintage , neutraldullcolors,
softlights,[...]”
Wearereadytorelatethisquantitytothetraceof
thecovariancematrix,givenby 8. “pizza the hut, akira, gorillaz, poster,
highquality”
n n
tr(Λ)= n1 (cid:88) ||f(I i)− n1 (cid:88) f(I j)||2 2, (9) 9. “tenguspottedinatlanta”
i=1 j=1
10. “underground cinema, realisticarchitecture,
whichsimplifiesas
colorfulllights,octanerender,4k,8k”
(cid:32) n (cid:33)
1 (cid:88) f(I )⊺ f(I )− 1 (cid:88) f(I )⊺ f(I ) . (10) D VisuolinguisticAnalysisDetails
n i i n i j
i=1 i,j
D.1 LinguisticFeatureExtraction
Multiplyingby(n−1),wearriveatthesumofthe
For word sense clustering, we used the “WN
pairwisesquaredEuclideandistance. Dividingby
2.1 -19370 synsets” resource from https://
n(n−1)yieldsthemeanpairwisesquareddistance,
ai.stanford.edu/~rion/swn/, previously pub-
andourproofisfinished.
lished in Snow et al. (2007). Unless other-
C DiffusionDBStatistics wise stated, all our CLIP models were initialized
fromtheopenai/clip-vit-large-patch14-336
Wenowcharacterizethepromptsandkeywordsin
checkpoint from HuggingFace, released by
DiffusionDB.Toextracttrailingkeywords,wesplit
OpenAI. Our GloVe embeddings were the 300-
promptsintoamainpartanditskeywordspartby
dimensional embeddingstrained on 840Btokens
applyingthesesteps:
ofwebtext.
1. Tokenize the prompt by commas, e.g., “cat
D.2 EffectsofClassifier-FreeGuidance
walking,4k”becomes“catwalking”and“4k.”
Webrieflyconfirmedthatincreasingclassifier-free
2. Ifany“token”afterthefirstisshorterthanfour
guidancedidnotworsentheperceptualvariability
words,everythingafterthattokenisconsidered
ofSDXLbelowthatofImagen. ImagenandDALL-
akeyword.
E 3 do not expose classifier-free guidance as an
3. Thefirst“token”isalwaysthemainprompt. inputparameter,hencelimitingustoSDXL.Wein-
creasedtheclassifier-freeguidancefrom5.0to30,
Apreliminaryanalysisshowedthatthiswasmore
muchhigherthanthenormalrangeof5.0–7.5,and
than 95% accurate in identifying keywords. We
regeneratedtheimagesinSection4.1. Wearrived
presenttenexamplesbelow:
atameanW1KPscoreof0.53forSDXL,which
wasbelowImagen’sscoreof0.62,e.g.,SDXLstill
1. “ashtray in the messy desk of the detective,
hadgreatervariability.
smokeanddark,digitalart”
2. “onion very sad crying big tears cartoon,
3drender”
3. “thelostcityofAtlantis,4K,hyperdetailed”
4. “agalleonshipbyDarekZabrocki”
5. “hill overlooking a viking city, fantasy,
forested, largetrees, topdownperspective,
[...]”
6. “photo of an awesome sunny day en-
vironment concept art on a cliff, ar-
chitecture by daniel libeskind with vil-
lage, residentialarea, mixeddevelopment,
highrisemadeupstaircases,[...]”E Dimensionality-ReducingVisualization
Visualization with W1KP Scores (DALL-E 3) Visualization with W1KP Scores (DALL-E 3)
Visualization with W1KP Scores (Imagen) Visualization with W1KP Scores (Imagen)
Visualization with W1KP Scores (SDXL) Visualization with W1KP Scores (SDXL)
Figure 10: Twenty generated images for the prompt Figure11: Generatedimagesforalongerprompt.
“cat,” clustered using multidimensional scaling on
DreamSim . Imagenproducessixdistinctclusters.
ℓ2