Designing Child-Centered Content Exposure and Moderation
Bel´en Sald´ıas
Massachusetts Institute of Technology, USA
ARTICLE HISTORY
2021 – Reviewed by Professor Prof. Dr. Veronica Barassi as part of the General
Exam, in partial fulfillment of the requirements for the degree of Doctor of Philos-
ophy in Media Arts and Sciences at theMassachusetts Instituteof Technology.
2024 –Presentedasextendedabstractandposterpresentationatthe2024Interna-
tional Conference on Computational Social Science (IC2S2).
ABSTRACT
Research on children’s online experience and computer interaction often overlooks
therelationship childrenhavewith hiddenalgorithms thatcontrolthecontentthey
encounter. Furthermore, it is not only about how children interact with targeted
contentbutalsohowtheirdevelopmentandagencyarelargelyaffectedbythese.By
engagingwiththebodyofliteratureattheintersectionofi)human-centereddesign
approaches, ii) exclusion and discrimination in A.I., iii) privacy, transparency, and
accountability, and iv) children’s online citizenship, this article divesinto the ques-
tion of “Howcan weapproach thedesign of achild-centeredmoderation process to
(1)includeaspectsthatfamiliesvaluefortheirchildrenand(2)provideexplanations
for contentappropriateness andremovalsothat wecan scale(according tosystems
and humanneeds) themoderation process assisted by A.I.?”.
This article contributes a sociotechnical highlight of core challenges and op-
portunities of designing child-centered content control tools. The article concludes
by grounding and characterizing design considerations for a child-centered, family-
guidedmoderationsystem.Wehopethisworkservesasasteppingstonefordesigners
and researchers pursuingchildren’ssafety online with an eyeonhidden agentscon-
trollingchildren’sonlineexperiencesand,byextension,thevaluesandopportunities
children are exposed to.
KEYWORDS
Content moderation; children and AI; artificial intelligence; machine learning;
content exposure
1. Introduction
Data and moderation policies vary across platforms, making it difficult to hold de-
signers, architects, and developers of technology accountable to their policies or hold-
ing them to standards across different technologies (Barassi, 2019; Eubanks, 2014;
Nissenbaum, 1996), and even when available, these policies and moderation guidelines
are typically written using concepts that tend to be ambiguous, unclear or lacking
transparency for the majority of the target users, leading them through resignation
to accept these terms (Draper & Turow, 2019). While one can argue that we are free
to opt-out from using certain technologies, the reality is that more and more we are
CONTACTEmail:belen@mit.edu
4202
nuJ
21
]CH.sc[
1v02480.6042:viXrabeingcoerced into using them, and we need to findan actionable path ahead (Barassi,
2019).
Before algorithmic personalization (Biddle, 2021; Misener, 2016), when individuals
were initiated into the online world and learned about their identity and possibilities
online, most often their online interactions were influenced by other online human be-
ings, many times unidentified or with misleading intentions, but human nonetheless.
Technology and media seemed more like an exploration-targeted medium where intel-
ligent engagement entailed social connection, as opposed to an exploitation-targeted
medium where, as of today, social connection is seen as a means to engage users
and capitalize on their engagement (O’neil, 2016); treating people as targetable users,
disregarding their everyday values, autonomy, self-determination, and intentions by
adopting a reductive form of digital citizenship (Third & Collin, 2016). Today the on-
line space is crowded with “intelligent” agents or machine learning agents, which are
granted the ability to profile us, shape, and bound our choices at large scale.
Today it is not only about how children interact with targeted content, algorithms
or automated agents, but also how their development and agency are affected and
conditioned by these interactions. Because of children’s developmental age, algorithms
are increasingly shaping their lives.
Fortunately, intheearlydaysof theinternet,exposuretoitasachild hadnostrings
attached for that child’s future(a.k.a. current) self, even though they may had crossed
some age-appropriateness barriers.
Onlinechildrensafetyisnotonlyaboutdataprivacy,counteractingmalicioushuman
agents, or/and preventing bullying. There are also pressing risks and challenges in
children interacting with “intelligent” agents (through content) that reflect values
that are not necessarily those that children or their families and communities wish for
them as they are growing and developing (Bridle, 2017).
The need for robust, socially-sensitive, and in particular children-centered, artificial
intelligence(A.I.)technologies isbecomingmorepressing.Ourinterestinpursuingthis
area started by questioning how we could make the internet a safer place for youth,
and this work brings a perspective to the ways in which children are unintentionally
or intentionally being exposed to content online—focused on that content decided by
A.I. algorithms—and ways in which we can face primary ethical considerations from a
design+A.I.perspective,specificallyastechnologyisbeinginteractedwithandshaping
our children (Hao, 2020b).
2. Children exposure to online content
Social media platforms, search engines, and web navigation, in general today, rely on
multiple methods to engage users in accessing and interacting with the information
they contain. This engagement can be highly beneficial in exposing children to a
variety of well-rounded learning opportunities and connecting them to resources with
unique facets only available through the internet (e.g., remote friendship and spaces
for identity exploration) (Livingstone et al., 2017). Nevertheless, it is crucial to notice
that a system could push to cocoon children into specific optimization criteria defined
by external agents or policies by exploiting personalized recommendations (Pariser,
2011; van Eeden, 2020).
To better understand the extent to which some of these content exposure and en-
gagement mechanisms are everywhere in today’s online experience, it is essential to
look at how these surfaced and fomented. In the early 2000s, when Netflix intended to
2grow its library to 100,000 titles open for all-you-can-eat consumption, they looked for
a strategy to help their clients find movies more efficiently, which included search and
recommendations (Biddle, 2021). These two features allowed Netflix to start collect-
ing a surplus of data such that they could predict with high confidence those movies
that your friends may enjoy or that you may enjoy based on your viewing history,
movie ratings, demographic information, and your friends’ collected data. The main
aim was to collect data to train algorithms to connect clients to movies they would
enjoy. At the same time, Google Search was starting what is today the primary han-
dler of online search requests, and whose revenue comes almost all from personalized
ads (Google Ads) prompted from users’ search requests (Hosch, 2020; Misener, 2016).
Google’s monetization of data captured through monitoring people’s movements and
behaviors online and in the physical world has had more profound repercussions than
what anyone had anticipated then (Ellenberg, 2019; Zuboff, 2019). Children are not
the exception, and while content moderation tools have been in place for a long time,
children’s exposure to online content deserves attention; children are subject to not
only beingmonitored but also influenced and shaped by these personalization systems
(some examples presented by Bridle (2017); Tan, Ng, Omar, and Karupaiah (2018)).
The unique focus of this work looks at the diverse shapes in which children get
exposed to content online today, varying on the level of endedness (open-ended vs.
closed-ended)andpersonalization (genericvs.targeted vs.restricted).Ofcourse,these
are not unique to children, but they bring particular challenges when interacting with
them.
2.1. Stream of content (open-ended exposure)
Whenever we spend time online, we have access to a stream of content.
When it comes to children, open-ended exposure allows them to discover top-
ics and opportunities that they may not have envisioned for themselves be-
fore (Chiong, Guernsey, Levine, & Stevens, 2014; Coleman, Coleman, & Hagell, 2007;
Livingstone et al., 2017). For example, learning about what others are doing, possible
career paths, seeing Ads about educational tools, and playing video games. While all
these scenarios can be highly beneficial to children, there are still some technical and
ethical challenges to be addressed to enjoy these goodnesses fully:
• Hard to converge: This excess of information can lead to high exposure to irrele-
vant information with a lack of depth in specific knowledge in children pursuing
open-ended media engagement. To address this challenge, technology platforms
now offer, almost by default, search engines or filtering software capabilities.
• Adequacy for minors: Even when children navigate the internet through
a browser with a parental-control system activated, they are likely to en-
counter different kinds of inappropriate content. To comply with COPPA and
GDPR (Stephens, 2020), social media platforms are set to discourage children
from using their services. However, because these measures do not actively pro-
hibit or ban minors from accessing the services–since it is up to the user to
disclaim their age–, children are still an active part of these communities.
2.2. Search engines (closed-ended exposure)
Directed search and navigation is a very effective mechanism to converge to the con-
tent we arelooking for.Search engines and features that allow usto filter content have
3brought enormous advantages and efficiencies to our online experience, allowing us to
categorize those areas of most interest for us. However, there are some challenges that
we as adults encounter and need to learn how to deal with—for example representa-
tional biases and how these can affect children self-image if validated or discriminated
atlarge-scale by an A.I.system /search engine(Noble,2013). Thisunsolved challenge
is technically very challenging to address—and potentially impossible to avoid—but
increasing transparency in search results is key for progress.
The above-described open-ended and closed-ended mechanisms are orthogonal to
the level of personalization, which includes the following exposure mechanisms.
2.3. Content availability (generic exposure)
Backintheday,therewasastandardviewforeveryonenavigatingthroughaweb-page;
just like today, everyone would see the same Wikipedia page if looking for a specific
term in a specific physical region and language (Wikipedia, n.d.). Being exposed to
generic content brings benefits such as preventing a biased machine from deciding
what children should be exposed to; however, it still carries an overwhelming amount
of information online. Search engines and personalized recommendations tackle some
of thesechallenges bylearningabouttheuserandunderstandingwhatthey may enjoy
better (Biddle, 2021; Netflix, n.d.).
2.4. Content recommendation (targeted exposure)
To optimize for a “better” online experience, recommending systems have become the
primarytool for“improving” thisexperience.Personalizing learningrecommendations
andEdTech have becomemoreprominentduringthelastfew years (Deschˆenes,2020).
One of the main benefits of personalized learning is the scalability it can achieve. On
the flip side, these algorithms may be locking children in stereotypes from a very
early age, stereotypes that may be temporary proper or mistaken, and which may
be impacting children’s life opportunities (by reducing the exposure to only what
aligns with their math-based profile) and social mobility as a consequence of reduced
opportunities (Eubanks, 2014; Hillman, 2021; O’neil, 2016).
By profiling children and routing them to specific experiences, algorithms may also
reducetheir agency for self-exploration and development (Barassi, 2020). Childrenare
in a crucial development stage where they also want to be socially accepted and find
their space and uniqueness in society. These potential effects of interacting with A.I.
bring ethical concerns for how much are models allow children to freely choose the
values they want to be associated with (moral autonomy), as well as how much can
they control their narrative in different contexts (contextual integrity) as opposed to
models directing content ranking according to what may be more profitable for the
platform (Barassi, 2019; Livingstone, 2018).
2.5. Content moderation (controlled exposure)
As sometimes parental control tools allow to (Livingstone et al., 2017), systems can
be trained to control (through restricting or enabling) exposure to content. Thanks
to moderation tools, we see big tech companies addressing the spread of fake news
or hate speech. We also see systems such as Youtube kids or search engines designed
for children that are developed to be safe for children of multiple ages, allowing large-
4scaleinformation access foryouth—inpartallowing other beneficialtypesofexposure.
Nevertheless,thereisanassumptionthattheserviceprovidersknowwhatsafetymeans
foreachcommunity,howrisksarepresentedintheirplatforms,andwhatrisksaremore
likely to happen within specific communities.
In their latest work, Haimson, Delmonaco, Nie, and Wegner (2021) show that con-
tentmoderationisnotequitableformarginalizedsocialmediausers.Forexample,they
showthatplatforms—likeFacebook,Instagram,andTwitter—biastheircontentmod-
eration against transgender and Black users by removing their posts involving content
related to expressing their marginalized identities despite following site policies.
Further, content moderation is not necessarily only about restricting content for
children.Inotherwords,contentmoderationdoesnotmeandelegatingallthedecision-
making power to a platform; it can also be designed to allow users to have agency and
control of their experience (Bhargava et al., 2019; Lai et al., 2022; Sald´ıas & Picard,
2019).
Community-values-guided moderation. From a machine-learning perspective,
trainingamodelfor classification tasks (e.g., abinarydecision for content appropriate-
ness) rely on a clear objective or common sense (e.g., differentiating a dog from a cat;
or a negative movie review from a positive movie review). However, there are other
similar tasks where biases influence decisions that can cause direct harm to humans
or when the ground truth / gold standard depends on the specific intended audience.
Content moderation is, arguably, an instance of such a task, where content ap-
propriateness can depend on the target community and their values. In this sce-
nario, each family (or community) should be empowered to define different rules
to explain why a piece of content should be filtered (out or in) in their children’s
news feeds, instead of having a universal (top-down) model that rules what is right
from wrong independent of the communities (Milan & Trer´e, 2019), as these uni-
versal approaches are proven to make mistakes that can lead to even amplification
of hate (CONTENT MODERATION IN UNDER-RESOURCED REGIONS, 2023;
Milan & Trer´e, 2019).
Here we propose to frame moderation as a highly human-controlled and inter-
pretable tool, where families have agency over and visibility of the specific content
their children consume online.
Whenever we browse the internet, we may encounter multiple combinations of the
types of exposure to content described above. For example, an Instagram news feed
shows personalized content (targeted exposure—section 2.4) through a stream of con-
tent (that many believe to be open-ended exposure—section 2.1—while in reality it is
optimizedforengagement). Whenlookingforaspecifichashtag(#)thatcanbecomea
user-narrowed search (closed-ended exposure—section 2.2), wherecontent is not much
moderatedother thanfollowingInstagram universalmoderationrules(Milan & Trer´e,
2019). From our understandingof the content-exposure landscape and ethical A.I., we
envision a world where many worlds fit. Not only those worlds dictated by social-
media platforms, which have the power to shape children—most times—disregarding
the responsibility to allow for their self-determination (Costanza-Chock, 2018).
3. Designing Content Moderation Models With and For Children
In this section, we illustrate opportunities to address core ethical considerations dis-
cussed above, along with design considerations for tools that may be deployed within
5systems that intentionally or unintentionally interact with or affect children. Without
much loss of generality, the specific scenario of analysis is valued-guided moderation
of text-based content designed for and with children and families, which we call child-
centered content moderation. By the end of this section, we delineate opportunities to
intend this system for and with children.
3.1. Child-centered control: enabling and moderating content
Content moderation is one of those tasks where biases influence decisions that can
cause direct harm to humans (e.g., by discouraging, invalidating, or silencing people’s
experiences and opinions shared online) or when the standard depends on the spe-
cific target audience (e.g., language that may seem rude for one community may be
perceived as adequate in another one). We argue that, when content appropriateness
depends on the audience community (e.g., children age-groups or family-level commu-
nities).
Our driving design question is: “How can we help facilitate a child-centered
content curation process to (1) include aspects that families (or communities) value
for their children and (2) provideinterpretable rationales aboutwhy a piece of content
may be appropriate or inappropriate for them, so that we can scale (according to
systems and human needs) the moderation process by being assisted by A.I.?”.
One approach could be to create a classifier to tell us how appropriate a piece
of content is. In fact, there are good models already out there that help detect hate
speech,toxicity, profanity or violence (Jigsaw,2018).Thesemodels,as we have seen in
previous work, extract textual cues and meaning to determine a class (a.k.a., violation
type). However, these have a nearsighted view of what moderation and control imply,
namely only filtering out toxic or violent content—taking a top-down approach where
powerful companies have decided what is to be filtered out. Here we re-frame this
challengeasanopportunity.Whataboutallowingfamiliestoguidecontent-moderation
strategies according to whatthey value? (as opposed to agreeing with parental control
models that may unnecessarily limit children’s exposureto content by activating what
companies think is child-proof or age-appropriate.)
Further, what about pursuing a healthy media diet (Jackson, 2019) by setting the
proportion of values we want our children to be exposed to? E.g., balancing between
content depicting drinking, drugs & smoking, or consumerism (potentially aiming to
reduceit) and content with positive characteristics, suchas positive educational value,
positive messages and positive role models (potentially aiming to increase it)? Can we
empower families and caregivers with this level of granularity? We argue yes, and sub-
stantiated by this research and development companies like CommonSenseMedia.org
(n.d.), in the next section present an approach for this.
3.2. Prompting value scenario
Why are companies in charge of moderating most of the content children and adults
consume online? We argue that each family or caregiver may want to allow their
children to different levels of content characteristics (include more of some or less
of others). Further, different families and communities may perceive differences in
positive messages, role models, or consumerism. Even more so, value definition can
evolve within and across families.
To ground the design of a child-centered moderation tool—and to empha-
6size implications for stakeholders—we follow a value-sensitive-design research strat-
egy (Friedman, Hendry, & Borning, 2017), through imagining the following value sce-
nario:
Laura (girl, 14 years old) uses her smartphone to access social media. By default,
sheseesastreamofcontentthatisalsotargetedto retainherattentionandincrease
her time spent on the platform.
Laura’s parents follow her social media diet through a parental-control app
that allows them to control and moderate the different content Laura is exposed
to. Thanks to a values-aware moderation tool, Laura’s parents realize her news
feed is skewing towards drinking and consumerism. They want to know from
what stance drinking and consumerism are being displayed. Is it preventing drink-
inghabitsinyouth?Isitchallengingconsumerism?orisitencouragingthesehabits?
Laura’sparents have not been trained as computer engineers.Hence, they do not
have the tools to answer this question if not for (1) manually going through all the
content Laura is exposed to or (2) having intelligible access to this feature in their
parent-moderation system view.
Further, as Laura is getting into high school and close to graduating, her
community—school and parents— wants to expose her to diverse skills and
potential role models that reflect some of her interests and other skills she may
beinterestedinacquiring;usingafamily-/community-guidedmoderationapproach.
As designers of this moderation tool, we realize the risk of parents forcing their
children to specific pathways, which is why these family-guided recommendations
influence the content appearing on Laura’s news feed only to some extent, allowing
accidental exposure to opportunities their families may not think for them.
3.3. Design considerations
For this values-aware moderation tool to be empowering for families, it needs to (1)
respect each family’s values and (2) provide intelligible rationales about the process
and moderated content. This human-in-the-loop scenario raises a tension between
large-scale automatic moderation and rapid and fresh access to content. While an
end-to-end machine learning system that assumes all children and families align with
the same values and priorities is not real and/or desirable, we argue that we can still
automate parts of the moderation process without reducing decision makers’ agency
(e.g., caregivers’ and children’s), by creating assistive tools intended to deepen the
understanding and agency of children as digital citizens (Third & Collin, 2016).
To respond to these considerations, we propose a system design targeted to
foster self-identity and community development and counteract oppression on three
levels (Costanza-Chock, 2018):
(1) Personal biography: this moderation tool should not deny a child’s identity
by inexplicably forcing content to them that are unaligned with those of their
family or children themselves.
7(2) Community and cultural context: this moderation tool should prioritize
those values diverse families care about, as our goal is to work with them
to create a better online space for children instead of setting a universal
moderation agenda that foster certain kinds of communities while suppressing
others (Bhargava et al., 2019).
Rapidprototyping,formativeevaluation,andfieldtestingwithfamiliesandchildren
can be an effective means to detect whether this novel values-guided approach to
moderation systems is allowing them to control the content aspects they wish for
their children and to surface and evaluate unintentional biases throughout the design
process (Feuz, Fuller, & Stalder, 2011).
Specific design considerations that will help address the challenges presented above
are as follows.
3.3.1. Access to diverse perspectives
Western-centric values and social understanding can fail to recognize non-mainstream
waysofknowingandunderstandingtheworldthroughindividuallifeexperiences.This
brings a large-scale concern as the most prominent content platforms worldwide are
based in the USA (Google, Facebook, Instagram, Twitter, among others). By deploy-
ing their top-down universal moderation values and policies, we remain in a blind
spot that can be addressed as we delegate decision-making power to communities
and families themselves. Even assuming that our system is successfully implemented
in the USA, theory tends to travel badly, and more often than not, we can fail to
acknowledge the specificities of distinct geographies, cultures, communities, and fam-
ilies (Milan & Trer´e, 2019).
While not easy to address, as we are thinking of building on top of these Western-
centric media platforms, we have opportunities for including diverse perspectives and
values in the moderation schema. First of all, by allowing families to control the
level of each content type they wish for their children, we have the capacity to help
them portray their own values in their children’s content consumption. Secondly,
prototyping and continuously evaluating this system by gathering feedback from
various families can strengthen the inclusion of diverse perspectives.
3.3.2. Accountability through transparency
To pursue accountability, we need more than ethics, whose mission is not to regulate
but establish ethical principles (Resseguier & Rodrigues, 2020). We intend for our
value-sensitive moderation tool to pursue accountability through transparency as it
empowers external agents to audit and challenge this system’s internal functioning.
Yet, this is not enough. We still need clear regulations that guide us to empower
families to advocate for their rights and responsibilities in our system. This additional
structure will allow those affected to trust in the accountability system and neither be
nor feel submissive to our or any A.I. systems.
Major risks that we face in light of accountability include 1) being able to recognize
whenoursystemisnotachievingitspurpose(section3)andactuponthat,2)standing
by our design principles that generate trust in our intended audience (a.k.a., families)
(section 3.3), and 3) working in our mission of increasing transparency and not hiding
8design pitfalls.
Addressing sources of unintentional harm and discrimination and remedying the
correspondingdeficiencies will bedifficult technically, difficult legally, and difficult po-
litically. Yet, thereis alot thatwecan dointernally, like includingacultureof internal
and external audit and evaluation of our systems—technically and in the ways they
affectchildren(Nissenbaum,1996;Resseguier & Rodrigues,2020;Shneiderman,2020).
3.3.3. Social implications of design
Theproposedcontentmoderationsystemismeanttobeaplatformtofacilitatefamily-
guided moderation for their children to improve their safety and experience online.
This system will allow families to request their children’s data to be removed from
the system at any point. While that may reduce the effectiveness of our algorithms
when recommending or removing content to their children’s news feed, we believe in
people making their own choices when it comes to interacting with recommendation
and moderation tools—in the end, their decisions are affecting their family.
Lookingintoeconomics andsocialimplications of design,Eubanks(2014)raises two
critical questions on digital human rights to reflect upon before we push forward with
our systems, adapted to this scenario (namely, replacing “the poor” for “children and
families”):
(1) Doesthetoolincreasetheself-determinationandagencyofchildrenandfamilies?
(2) Would the tool be tolerated if it was targeted at non-poor children and families?
We argue that, as presented here, the proposed child-centered, family-guided mod-
eration system is a decisive step forward in addressing both these questions.
First of all, our system would be designed—and more importantly evaluated–
deliberately to increase families’ agency on their children’s content. Further, it in-
creases transparency and allows children to understand why some content appears
and other is hidden. Also, it will enable children to guide, through their families,
the content characteristics they want to consume more or less frequently, increasing
self-determination by (children) influencingand (parents) controlling their news feeds.
Secondly, this moderation system is not targeted at a specific group of families
but rather aims to learn what different families and communities care about and add
these values as filters they could control for. We acknowledge that the initial set of
values we set for the system will (define a matter of course) represent culturally-
biased decisions. To address this, the design process can aim at reducing these biases
byrunningprototypeswithdifferentcommunitiesbeforedeployingittothem.Asmore
and more families adopt this system, designers and engineers will be better equipped
to understand and serve their needs. It is essential to highlight that this system is not
imposing a set of values that people should consider but rather allowing families to
control for a list of values that the system allows. That list is intended to grow as the
system grows.
A big challenge that needs addressing is understanding the risks associated with
collecting data from children, data that could be later—against to core design
values—used for profiling them further into their lives (Barassi, 2019). Concretely,
as children’s identity is continuously developing—and now recorded through their
social media behavior or their parents’ posting profiles—these data traces can be
used (and are being used) (Angwin, Parris Jr., Mattu, & ProPublica, 2016; Hao,
2020a) to categorize children and predict future behavior and performance potential
9in their daily lives. For example, as part of school or job applications or defining
their insurance policies. Therefore, as a baseline measure towards mitigating this risk,
deployment requires accountability towards not sourcing or joining any children-data
marketplace and having internal policies about anonymization to prevent any children
from being identified.
3.3.4. With and For children
Our guiding principles acknowledge that families have rights over their children’s on-
line experiences. But unfortunately, the amount of control—that parental tools offer
today—failstorecognizethatthesefamiliesandtheircommunitiesmayintenddiverse
and distinct sets of values for their children.
We propose to pursue fairness through awareness and transparency as we aspire
to provide families with tools to act upon content moderation challenges—as op-
posed to hide them and wish we internally (and hiddenly) produce the best re-
sults (Dwork, Hardt, Pitassi, Reingold, & Zemel, 2012). Further, ignoring the fact
that children are shapeable and developing their identity fails to acknowledge their
citizenship status on social life and media (Barassi, 2019).
Fundamentally, evaluation methods for proposed moderation algorithms and
systems need to acknowledge children on their role as humans with values, beliefs,
needs, and as full digital citizens. This is reflected in that the rapid prototyping
process—mentioned earlier in this section—calls to include feedback rounds with
children and families not as a target group for our envisioned system but rather as
rightful digital citizens.
With Children
Children are the ultimate stakeholders of this system. They may be aware of their ex-
periencesbeyondwhattheycanexpressinwordsandareexpertsattheirlives(Zaman,
2020). Predominantly, in systems for children, the decision-making power resides en-
tirely in adults who develop, design, and regulate technologies for young people. How-
ever, we mean “with” children as it is not only designers but also children along
with their families who ought to have faculty to control the key features of this con-
tent moderation system and its design process. Increasing children’s decision-making
power throughout the design process has the beneficial potential to help mitigate
treating children as “others” for whom the product is designed as mere subjects
in need of help. Aligning Zaman (2020)’s youth-centered design opportunities and
riskswith Friedman et al. (2017)’s value-sensitive design approach is apromisingpath
ahead for working with children in a more fruitful content moderation framework.
Furthermore, we account for research with children and parents, which shows
that age-based regulatory approaches that seek to protect children’s data via an age
threshold prove effective primarily among young children compared to teens and older
children (Livingstone, 2018).
Privacy by design
Involving children and their families in privacy by design endeavors helps surface
risks and harms to be detected before these become emergent biases and unnecessary
10risks (Feuz et al., 2011). Furthermore, inviting children to learn and influence the
prototyping and downstream moderation processes can benefit them in the long term.
By empowering children and meaningfully increasing their agency in the design and
decision-making process, they can acquire new competencies, including knowledge,
skills, and critical and constructive attitudes toward emerging technologies (Zaman,
2020). A simple rationale behind this is that children and their families must live with
the effects of the proposed system; hence, we argue that they should have the right
to control their usage and guide design evaluation. Further, in the future, designing
to protect the privacy and rights of all users may work better than trying to identify
children among users so as to treat them differently privacy-wise (Livingstone, 2018).
Note that we need to beware of possible unintended side effects of adult designers
working with children. Adults are still responsible for the decisions, and we cannot
fail to believe that including children relieves adults’ responsibility by delegating it to
children (as per their input).
For Children
Creating a value-guided moderation system “for” children opens doors for adults to
increase control of their own feeds. It can prove more efficient and effective to develop
systems for more vulnerable communities (i.e., children) and then focus on addressing
adult’s needs (as opposed to addressing adult’s needs first and then tackling issues
with children as an afterthought) (Livingstone, 2018).
We acknowledge that, just as in GDPR (Livingstone, 2018), the proposed modera-
tionsystemreliesonfamilies withconscientious parentsanddutifulchildren.However,
the messy world of real families—who may lack time, share devices with one another,
or have internal conflicts— fails to fit the engagement need for family-guided modera-
tion. Delegating responsibility to schools, chosen by caregivers to form their children,
can alleviate the necessity of caregivers’ time to set up and keep track of a system
like this for their families (reason why we talk both about families and communities
as leading this effort).
At the same time, while GDPR acknowledges that children’s data is worthy of
protection, it still fails to address unresolved challenges, like:
(1) Children’s media literacy: how aware are children of the risks and rights of pro-
cessing their personal data? Are parents aware?
(2) Commercial profiling: there is a lack of regulation relating to children’s data.
This implies dealing with stakeholders and grappling with technical capabilities
for safe and guaranteed anonymization.
(3) Nature of family relations: how ready are families to act according to current
regulations? How literate are they in privacy and their responsibility?
Underthesechallenges, assumingthatmanyparentsandcaregivers maynotbeable
to dedicate enough time to control and regulate their children’s online content diet,
the proposed moderation tool should offer a default set of settings that comply with
thelaw ofthecommunities in whichthisis deployed.As exploredby Livingstone et al.
(2017), a moderation tool that enables more content does require more skilled parents
who are aware of the online opportunities available and recognize how to activate
them and control their risks.
114. Conclusion
In this work, we describe a new system aimed at facilitating child-centered, family-
guided moderation through specific design considerations, specifically focused on chil-
dren’s personal biography, community and cultural context, systemic and social insti-
tutions, access to diverse perspectives, transparency through interactivity and human
control, accountability through transparency, social implications of designing this sys-
tem,anddesignandprivacyconsiderationstodesignwithandforchildrenandfamilies.
To prompt these specific considerations, we use a value scenario to help highlight and
reflect on the main stakeholders and how this system can be of help or cause harm.
The presented value scenario and reflection on it offer researchers interested in the
intersection of A.I. and child-development or child-safety systems an anchor to help
ground and reveal challenges and opportunities for their work.
While designing for good can mean different things for different people, the pro-
posed system is intended for good because it provides a baseline platform to increase
families’ and children’s agency and control over the content children consume or are
exposed to on the internet. Through this work, we surface critical ethical consider-
ations, challenges, and opportunities for the presented system. We argue that this
child-centered system works towards increasing socially preferable outcomes, allowing
familiestodecidewhatvaluestheywishfortheirchildren,insteadofhavingauniversal
top-down approach imposed by moderation and recommendation algorithms deployed
by big tech/media companies.
Implementing a successful child-centered, family-guided moderation system will re-
quirecontinuousprototypingandfamilies’participationtohelpalignitssocio-technical
development to the specific design considerations (opportunities and challenges) we
outline in this work. In addition, continuous evaluation should enable us to learn from
the systems’ and development’s successes and opportunities to serve better those fam-
ilies and communities intending to use a system like this one.
We hope this work serves as an example and stepping-stone for designers and re-
searchers whoundertakethemission toincrease children’s safety online byfocusingon
the challenges of youth interacting with intelligent agents that are currently control-
ling the content they get exposed to, and by extension, the values and opportunities
they have access to—and, in turn, contribute in unimposing yet crucial ways to the
fight for children online safety from an A.I. perspective.
References
Angwin, J., Parris Jr., T., Mattu, S., & ProPublica. (2016). What facebook knows about
you. Available: https://www.propublica.org/article/breaking-the-black-box-what-facebook-
knows-about-you.
Barassi, V. (2019). Datafied citizens in the age of coerced digital participation. Sociological
Research Online, 24(3), 414–429.
Barassi, V. (2020). Child data citizen: How tech companies are profiling us from before birth.
MIT Press.
Bhargava, R., Chung, A., Gaikwad, N. S., Hope, A., Jen, D., Rubinovitz, J., ... Zuckerman,
E. (2019). Gobo:Asystemforexploringusercontrolofinvisiblealgorithmsinsocialmedia.
In Conference companion publication of the 2019 on computer supported cooperative work
and social computing (pp. 151–155).
Biddle, G. (2021). A brief history of netflix personalization. Available:
https://gibsonbiddle.medium.com/a-brief-history-of-netflix-personalization-1f2debf010a1.
12Bridle, J. (2017). Something is wrong on the internet (Vol. 6). Nov.
Chiong,C., Guernsey,L., Levine, M., & Stevens,M. (2014). Pioneeringliteracy in the digital
wild west: Empowering parents and educators’. Joan Ganz Cooney.
Coleman, J. C., Coleman, J., & Hagell, A. (2007). Adolescence, risk and resilience: Against
the odds (Vol. 3). John Wiley & Sons.
CommonSenseMedia.org. (n.d.). Common sense media.
Content moderation in under-resourced regions. (2023, Dec). Retrieved from
https://techglobalinstitute.com/announcements/blog/content-moderation-arabic-hebrew-in-under-r
Costanza-Chock, S. (2018). Design justice, ai, and escape from the matrix of domination.
Journal of Design and Science (JoDS).
Deschˆenes,M.(2020).Recommendersystemstosupportlearners’agencyinalearningcontext:
asystematicreview. International Journal of Educational Technology in Higher Education,
17(1), 1–23.
Draper, N. A., & Turow, J. (2019). The corporate cultivation of digital resignation. New
Media & Society, 21(8), 1824–1839.
Dwork,C.,Hardt, M., Pitassi,T., Reingold,O., & Zemel, R. (2012). Fairnessthrough aware-
ness. In Proceedings of the 3rd innovations in theoretical computer science conference (pp.
214–226).
Ellenberg, J. (2019). What’s even creepier than target guessing that you’re pregnant?
Eubanks, V. (2014). Want to predict the future of surveillance? ask poor communities. The
American Prospect, 15.
Feuz, M., Fuller, M., & Stalder, F. (2011). Personal web searching in the age of semantic
capitalism: Diagnosing the mechanisms of personalisation. First Monday.
Friedman,B.,Hendry,D.G.,&Borning,A.(2017).Asurveyofvaluesensitivedesignmethods.
Foundations and Trends in Human-Computer Interaction, 11(2), 63–125.
Haimson, O. L., Delmonaco, D., Nie, P., & Wegner, A. (2021). Disproportionate removals
and differing content moderation experiences for conservative, transgender, and black so-
cial media users: Marginalization and moderation gray areas. Proceedings of the ACM on
Human-Computer Interaction, 5(CSCW2), 1–35.
Hao, K. (2020a). The uk exam debacle reminds us that algorithms can’t fix bro-
ken systems. Available: https://www.technologyreview.com/2020/08/20/1007502/uk-exam-
algorithm-cant-fix-broken-system/.
Hao, K. (2020b). Why kids need special protection from ai’s influence. Available:
https://www.technologyreview.com/2020/09/17/1008549/kids-need-protection-from-ai/.
Hillman, V. (2021). Algorithmic (in)justice in education: Why tech com-
panies should require a license to operate in children’s education. Avail-
able: https://blogs.lse.ac.uk/medialse/2021/10/25/algorithmic-injustice-in-education-why-
tech-companies-should-require-a-license-to-operate-in-childrens-education/.
Hosch, W. L. (2020). Google, american company. Available:
https://www.britannica.com/topic/Google-Inc.
Jackson, J. (2019). You are what you read: Why changing your media diet can change the
world. Unbound Publishing.
Jigsaw. (2018). Perspective api. Available: https://conversationai.github.io/.
Lai, V., Carton, S., Bhatnagar, R., Liao, Q. V., Zhang, Y., & Tan, C. (2022). Human-
ai collaboration via conditional delegation: A case study of content moderation. In Chi
conference on human factors in computing systems (pp. 1–18).
Livingstone, S. (2018). Children: a special case for privacy? Intermedia, 46(2), 18–23.
Livingstone,S.,O´lafsson,K.,Helsper,E.J.,Lupia´n˜ez-Villanueva,F.,Veltri,G.A.,&Folkvord,
F. (2017). Maximizing opportunities and minimizing risks for children online: The role of
digitalskillsinemergingstrategiesofparentalmediation. Journalofcommunication,67(1),
82–105.
Milan,S.,&Trer´e,E.(2019).Bigdatafromthesouth(s):Beyonddatauniversalism.Television
& New Media, 20(4), 319–335.
13Misener,D. (2016). What google’sbig changesto ad personalizationmeanfor you. Available:
https://www.cbc.ca/news/science/google-ads-privacy-1.3664302.
Netflix. (n.d.). How netflix’s recommendations system works. Available:
https://help.netflix.com/en/node/100639.
Nissenbaum, H. (1996). Accountability in a computerized society. Science and engineering
ethics, 2(1), 25–42.
Noble,S.U. (2013). Googlesearch:Hyper-visibilityasameansofrenderingblackwomenand
girls invisible. InVisible Culture, 1(19).
O’neil,C. (2016). Weapons of math destruction: How big data increases inequality and threat-
ens democracy. Crown.
Pariser,E. (2011). The filter bubble: How the new personalized web is changing what we read
and how we think. Penguin.
Resseguier,A., & Rodrigues,R. (2020). Ai ethics should not remaintoothless! a call to bring
back the teeth of ethics. Big Data & Society, 7(2), 2053951720942541.
Sald´ıas,B.,&Picard,R.W. (2019). Tweetmoodifier:Towardsgivingemotionalawarenessto
twitter users. In 2019 8th international conference on affective computing and intelligent
interaction (acii) (pp. 1–7).
Shneiderman, B. (2020). Human-centered artificial intelligence: Reliable, safe & trustworthy.
International Journal of Human–Computer Interaction, 36(6), 495–504.
Stephens, A. (2020). The relationship between coppa and gdpr. Retrieved on Novem-
ber 9, 2020, from https://www.privacycompliancehub.com/gdpr-resources/the-relationship-
between-coppa-and-gdpr-getting-it-right-for-your-business/2020.
Tan, L., Ng, S. H., Omar, A., & Karupaiah, T. (2018). What’s on youtube? a case study
on food and beverage advertising in videos targeted at children on social media. Childhood
Obesity, 14(5), 280–290.
Third,A.,&Collin,P. (2016). Rethinking(children’sandyoungpeople’s)citizenshipthrough
dialogues on digital practice. Negotiating digital citizenship: Control, contest and culture,
41–60.
vanEeden,R. (2020). Teenagers’ appreciation of social media filter bubbles after an interven-
tion that aims to increase knowledge and awareness. (Unpublished master’s thesis).
Wikipedia, A. S. (n.d.). Wikipedia: Strengths, weaknesses, and article quality.
Zaman, B. (2020). Designing technologies with and for youth: Traps of privacy by design.
Media and Communication, 8(4), 229–238.
Zuboff, S. (2019). The age of surveillance capitalism: The fight for a human future at the new
frontier of power: Barack obama’s books of 2019. Profile books.
14