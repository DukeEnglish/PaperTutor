Scaling Laws in Linear Regression:
Compute, Parameters, and Data
LicongLin* JingfengWu† ShamM.Kakade‡ PeterL.Bartlett§ JasonD.Lee¶
June13,2024
Abstract
Empirically, large-scale deep learning models often satisfy a neural scaling law: the test error of the trained
modelimprovespolynomiallyasthemodelsizeanddatasizegrow. However,conventionalwisdomsuggeststhe
testerrorconsistsofapproximation,bias,andvarianceerrors,wherethevarianceerrorincreaseswithmodelsize.
Thisdisagreeswiththegeneralformofneuralscalinglaws,whichpredictthatincreasingmodelsizemonotonically
improvesperformance.
Westudythetheoryofscalinglawsinaninfinitedimensionallinearregressionsetup.Specifically,weconsidera
modelwithM parametersasalinearfunctionofsketchedcovariates. Themodelistrainedbyone-passstochastic
gradient descent (SGD) using N data. Assuming the optimal parameter satisfies a Gaussian prior and the data
covariancematrixhasapower-lawspectrumofdegreea > 1,weshowthatthereduciblepartofthetesterroris
Θ(M−(a−1)+N−(a−1)/a).Thevarianceerror,whichincreaseswithM,isdominatedbytheothererrorsduetothe
implicitregularizationofSGD,thusdisappearingfromthebound.Ourtheoryisconsistentwiththeempiricalneural
scalinglawsandverifiedbynumericalsimulation.
1 Introduction
Deeplearningmodels,particularlythoseonalargescale,arepivotalinadvancingthestate-of-the-artacrossvarious
fields. Recentempiricalstudieshaveshedlightontheso-calledneuralscalinglaws(seeKaplanetal.,2020;Hoffmann
etal.,2022,forexample),whichsuggestthatthegeneralizationperformanceofthesemodelsimprovespolynomiallyas
bothmodelsize,denotedbyM,anddatasize,denotedbyN,increase. Theneuralscalinglawquantitativelydescribes
thepopulationriskas:
c c
R(M,N)≈R∗+ 1 + 2 , (1)
Ma1 Na2
whereR∗isapositiveirreducibleriskandc ,c ,a ,a arepositiveconstantsindependentofM andN. Forinstance,
1 2 1 2
byfittingtheaboveformulawithempiricalmeasurementsinstandardlarge-scalelanguagebenchmarks,Hoffmann
etal.(2022)estimateda ≈0.34anda ≈0.28,whileBesirogluetal.(2024)estimatedthata ≈0.35anda ≈0.37.
1 2 1 2
Thoughtheexactexponentsdependonthetasks,neuralscalinglawsin(1)areobservedconsistentlyinpracticeandare
usedasprincipledguidancetobuildstate-of-the-artmodels,especiallyunderacomputebudget(Hoffmannetal.,2022).
Fromtheperspectiveofstatisticallearningtheory,(1)isratherintriguing. Standardstatisticallearningbounds(see
Mohrietal.,2018;Wainwright,2019,forexample)oftendecomposethepopulationriskintothesumofirreducible
error,approximationerror,biaserror,andvarianceerror(sometheoryreplacesbiasandvarianceerrorsbyoptimization
andgeneralizationerrors,respectively)asintheformof
(cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19)
1 1 c(M)
R(M,N)=R∗+O +O +O , (2)
Ma1 Na2 Na3
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
approximation bias variance
*UCBerkeley.Email:liconglin@berkeley.edu
†UCBerkeley.Email:uuujf@berkeley.edu
‡HarvardUniversity.Email:sham@seas.harvard.edu
§UCBerkeleyandGoogleDeepMind.Email:peter@berkeley.edu
¶PrincetonUniversity.Email:jasonlee@princeton.edu
1
4202
nuJ
21
]GL.sc[
1v66480.6042:viXraa=1.5
a=2.0
103
1.6 1.35
102 1.30
1.5
1.25
1.4
102 1.20
1.3 1.15
1.10
1.2
1.05
101 101
102 103 102 103
Number of effective samples (N eff) Number of effective samples (N eff)
Figure1: Theexpectedrisk(Risk)ofthelastiterateof(SGD)versustheeffectivesamplesizeN andthemodelsizeM for
eff
differentpower-lawdegreesa. Theexpectedriskiscomputedbyaveragingover1000independentsamplesof(w∗,S). Wefit
theexpectedriskusingtheformulaRisk∼σ2+c 1/Ma1 +c 2/Na2 viaminimizingtheHuberlossasinHoffmannetal.(2022).
Parameters:σ=1,γ =0.1.Left:Fora=1.5,d=20000,thefittedexponentsare(a ,a )=(0.54,0.34)≈(0.5,0.33).Right:
1 2
Fora=2,d=2000,thefittedexponentsare(a ,a )=(1.07,0.49)≈(1.0,0.5).Notethatthevaluesof(a ,a )areclosetoour
1 2 1 2
theoreticalpredictions(a−1,1−1/a)inbothcases,verifyingthesharpnessofourriskbounds. Moredetailscanbefoundin
Sections4and5.
wherea ,a ,a arepositiveconstantsandc(M)isameasureofmodelcomplexitythattypicallyincreaseswiththe
1 2 3
modelsizeM. In(2),theapproximationerrorisinducedbythemismatchofthebest-in-classpredictorandthebest
possiblepredictor,hencedecreasingwiththemodelsizeM. Thebiaserrorisinducedbythemismatchoftheexpected
algorithmoutputandthebest-in-classpredictor,hencedecreasingwiththedatasizeN. Thevarianceerrormeasuresthe
uncertaintyofthealgorithmoutput,whichdecreaseswiththedatasizeN butincreaseswiththemodelsizeM (since
themodelcomplexityc(M)increases).
Amystery. Theempiricalneuralscalinglaw(1)isincompatiblewiththetypicalstatisticallearningtheorybound(2).
Whilethetwoerrortermsintheneuralscalinglaw(1)canbeexplainedbytheapproximationandbiaserrorsinthe
theoreticalbound(2)respectively,itisnotclearwhythevarianceerrorisunobservablewhenfittingtheneuralscaling
lawempirically. Thisdifferencemustbereconciled,otherwise,thestatisticallearningtheoryandtheempiricalscaling
lawmakeconflictpredictions: asthemodelsizeM increases,thetheoreticalbound(2)predictsanincreaseofvariance
errorthateventuallycausesanincreaseofthepopulationrisk,buttheneuralscalinglaw(1)predictsadecreaseofthe
populationrisk. Inotherwords,itremainsunclearwhentofollowthepredictionoftheempiricalscalinglaw(1)and
whentofollowthatofthestatisticallearningbound(2).
Our explanation. We investigate this issue in an infinite dimensional linear regression setup. We only assume
accesstoM-dimensionalsketchedcovariatesgivenbyafixedGaussiansketchandtheirresponses. Weconsidera
linearpredictorwithM trainableparameters,whichistrainedbyone-passstochasticgradientdescent(SGD)with
geometricallydecayingstepsizesusingN sketcheddata. Assumingthatthespectrumofthedatacovariancematrix
satisfies a power-law of degree a > 1 and that the optimal model parameters satisfy a Gaussian prior, we derive
matchingupperandlowerboundsonthepopulationriskachievedbytheSGDoutput(seeTheorem4.1). Specifically,
weshowthat
(cid:18)
1
(cid:19) (cid:18)
1
(cid:19) (cid:18) min{M,(Nγ)1/a}(cid:19)
R(M,N)=R∗+ Θ +Θ˜ , Var=Θ˜ ,
Ma−1 (Nγ)(a−1)/a N
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
leadingordergivenbythesumofApproxandBias higherorder,thusunobservable
where γ = O(1) is the initial stepsize used in SGD and Θ˜(·) hides log(N) factors. In our bound, the sum of the
approximationandbiaserrorsdeterminestheorderoftheexcessrisk,whilethevarianceerrorisofastrictlyhigher
orderandisthereforenearlyunobservablewhenfittingR(M,N)asafunctionofM andN empirically. Inaddition,
ouranalysisrevealsthatthesmallvarianceerrorisduetotheimplicitregularizationeffectofone-passSGD(Zouetal.,
2
)M(
ezis
ledoM
ksiR
)M(
ezis
ledoM
ksiR2021). Ourtheorysuggeststhattheempiricalneuralscalinglaw(1)isasimplificationofthestatisticallearningbound
(2)inaspecialregimewhenstrongregularization(eitherimplicitorexplicit)isemployed.
Moreover,wegeneralizetheabovescalinglawto(1)constantstepsizeSGDwithiterateaverage(seeTheoremF.6),(2)
caseswheretheoptimalmodelparametersatisfiesananisotropicprior(seeTheorem4.2),and(3)wherethespectrum
ofthedatacovariancematrixsatisfiesalogarithmicpowerlaw(seeTheorem4.3).
Empricalevidence. Basedonourtheoreticalresults,weconjecturethatthecleanneuralscalinglaw(1)observedin
practiceisduetothedisappearanceofvarianceerrorcausedbystrongregularization. Twopiecesofempiricalevidence
tosupportourunderstanding. First,largelanguagemodelsthatfollowthescalinglaw(1)areoftenunderfitted,asthe
modelsaretrainedoverasinglepassorafewpassesoverthedata(Komatsuzaki,2019;Muennighoffetal.,2023;
Brownetal.,2020;Touvronetal.,2023). Whenmodelsareunderfitted,thevarianceerrortendstobesmaller. Second,
whenlanguagemodelsaretrainedwithmultiplepasses(upto7passes),Muennighoffetal.(2023)foundthattheclean
scalinglawin(1)nolongerholdsandtheyproposedamoresophisticatedscalinglawtoexplaintheirdata. Thiscanbe
explainedbyarelativelylargevarianceerrorcausedbymultiplepasses.
Notation. For two positive-valued functions f(x) and g(x), we write f(x) ≲ g(x) (and f(x) = O(g(x))) or
f(x) ≳ g(x) (and f(x) = Ω(g(x))) if f(x) ≤ cg(x) or f(x) ≥ cg(x) holds for some absolute (if not otherwise
specified)constantc > 0respectively. Wewritef(x) ≂ g(x)(andf(x) = Θ(g(x)))iff(x) ≲ g(x) ≲ f(x). For
twovectorsuandvinaHilbertspace,wedenotetheirinnerproductby⟨u,v⟩oru⊤v. FortwomatricesAandB
ofappropriatedimensions,wedefinetheirinnerproductby⟨A,B⟩:=tr(A⊤B). Weuse∥·∥todenotetheoperator
normformatricesandℓ -normforvectors. Forapositivesemi-definite(PSD)matrixAandavectorvofappropriate
2
dimension,wewrite∥v∥2 :=v⊤Av. ForasymmetricmatrixA,weuseµ (A)torefertothej-theigenvalueofA
A j
andr(A)torefertoitsrank. Finally,log(·)referstologarithmbase2.
2 Related work
Empirical scaling laws. In recent years, the scaling laws ofdeep neural networks in compute, sample size, and
modelsize have beenwidely studiedacrossdifferent modelsanddomains (Hestnesset al.,2017;Rosenfeld etal.,
2019;Kaplanetal.,2020;Henighanetal.,2020;Hoffmannetal.,2022;Zhaietal.,2022;Muennighoffetal.,2023).
The early work by Kaplan et al. (2020) first proposed the neural scaling laws of transformer-based models. They
observedthatthetestlossexhibitsapower-lawdecayinquantitiesincludingtheamountofcompute,samplesize,and
modelsize,andprovidedjointformulasinthesequantitiestopredictthetestloss. Theproposedformulaswerelater
generalizedandrefinedinsubsequentworks(Henighanetal.,2020;Hoffmannetal.,2022;Alabdulmohsinetal.,2022;
Caballeroetal.,2022;Muennighoffetal.,2023). Notably,Hoffmannetal.(2022)proposedtheChinchillalaw,that
is,(1)witha ≈0.34anda ≈0.28. Theempiricalobservationguidedthemtoallocatedataandmodelsizeundera
1 2
givencomputebudget. TheChinchillalawisfurtherrevisedbyBesirogluetal.(2024). MotivatedbytheChinchilla
law,Muennighoffetal.(2023)consideredtheeffectofmultiplepassesovertrainingdataandempiricallyfittedamore
sophisticatedscalinglawthattakesaccountoftheeffectofdatareusing.
Theoryofscalinglaws. Althoughneuralscalinglawshavebeenempiricallyobservedoverabroadspectrumofprob-
lems,thereisarelativelylimitedliteratureonunderstandingthesescalinglawsfromatheoreticalperspective(Sharma
and Kaplan, 2020; Bahri et al., 2021; Maloney et al., 2022; Hutter, 2021; Wei et al., 2022; Michaud et al., 2024;
Jain et al., 2024; Bordelon et al., 2024; Atanasov et al., 2024; Nam et al., 2024; Dohmatob et al., 2024). Among
theseworks,SharmaandKaplan(2020)showedthatthetestlossscalesasN4/dforregressionondatawithintrinsic
dimensiond. Hutter(2021)studiedatoyproblemunderwhichanon-trivialpowerofN arisesinthetestloss. Jain
etal.(2024)consideredscalinglawsindataselection. Bahrietal.(2021)consideredalinearteacher-studentmodel
underapower-lawspectrumassumptiononthecovariates, andtheyshowedthatthetestlossoftheordinaryleast
squareestimatordecreasesfollowingapowerlawinsamplesizeN (resp. modelsizeM)whenthemodelsizeM (resp.
samplesizeN)isinfinite. Bordelonetal.(2024)consideredalinearrandomfeaturemodelandanalyzedthetestlossof
thesolutionfoundby(batch)gradientflow. TheyfocusedonthebottleneckregimeswheretwoofthequantitiesN,M,
T (trainingsteps)areinfiniteandshowedthattheriskhasapower-lawdecayintheremainingquantity. Theproblemin
Bahrietal.(2021);Bordelonetal.(2024)canbeviewedasasketchedlinearregressionmodelsimilartoours. Itshould
3benotedthatbothBahrietal.(2021)andBordelonetal.(2024)onlyderivedthedependenceofpopulationriskonone
ofthedatasize,modelsize,ortrainingstepsintheasymptoticregimewheretheremainingquantitiesgotoinfinity,
andtheirderivationsarebasedonstatisticalphysicsheuristics. Incomparison,weprovematching(ignoringconstant
factors)upperandlowerriskboundsjointlydependingonthefinitemodelsizeM anddatasizeN.
ImplicitregularizationofSGD. One-passSGDinlinearregressionhasbeenextensivelystudiedinboththeclassical
finite-dimensionalsetting(PolyakandJuditsky,1992;BachandMoulines,2013;DéfossezandBach,2015;Dieuleveut
etal.,2017;Jainetal.,2018,2017;Geetal.,2019)andthemodernhigh-dimensionalsetting(DieuleveutandBach,
2015; Berthier etal., 2020; Zou etal., 2023, 2021; Wu etal., 2022a,b; Varre et al.,2021). In particular, Zou et al.
(2021)showedthatSGDinducesanimplicitregularizationeffectthatiscomparableto,andincertaincasesevenmore
preferablethan,theexplicitregularizationeffectinducedbyridgeregression. Thisisoneofthekeymotivationsof
ourscalinglawinterpretation. Fromatechnicalperspective,weutilizethesharpfinite-sampleanddimension-free
analysisofSGDdevelopedbyZouetal.(2023);Wuetal.(2022a,b). Differentfromthem,weconsiderasequenceof
linearregressionmodelswithanincreasingnumberoftrainableparametersgivenbydatasketch. Ourmaintechnical
innovationistosharplycontroltheeffectofdatasketch. Someofourintermediateresults,forexample,tightboundson
thespectrumofthesketcheddatacovarianceunderthepowerlaw(seeLemma6.2),mightbeofindependentinterest.
3 Setup
Weusex∈Htodenoteafeaturevector,whereHisafinited-dimensionalorcountablyinfinitedimensionalHilbert
space,andy ∈Rtodenoteitslabel. Inlinearregression,wemeasurethepopulationriskofaparameterw∈Hbythe
meansquarederror,
R(w):=E(cid:0) ⟨x,w⟩−y(cid:1)2 , w∈H,
wheretheexpectationisover(x,y)∼P forsomedistributionP onH×R.
Definition 1 (Data covariance and optimal parameter). Let H := E[xx⊤] be the data covariance. Assume that
tr(H) and all entries of H are finite. Let (λ ) be the eigenvalues of H sorted in non-increasing order. Let
i i≥0
w∗ ∈argmin R(w)betheoptimalmodelparameter1. Assumethat∥w∗∥2 :=(w∗)⊤Hw∗isfinite.
w H
WeonlyassumeaccesstoM-dimensionalsketchedcovariatesandtheirresponses,thatis,(Sx,y),whereS∈RM ×H
isafixedsketchmatrix. WefocusontheGaussiansketchmatrix2,thatis,entriesofSareindependentlysampledfrom
(cid:0) (cid:1)
N 0,1/M .WethenconsiderlinearpredictorswithM trainableparametersgivenby
f :H→R, x(cid:55)→⟨v,Sx⟩,
v
wherev ∈ RM arethetrainableparameters. VaryingM shouldbeviewedasalinearanalogofvaryingtheneural
networkmodelsize. Oursketchedlinearregressionsettingiscomparabletotheteacher-studentsettingconsideredby
Bahrietal.(2021);Bordelonetal.(2024).
Weconsiderthetrainingoff viaone-passstochasticgradientdescent(SGD),thatis,
v
(cid:0) (cid:1)
v :=v −γ f (x )−y ∇ f (x )
t t−1 t vt−1 t t v vt−1 t
(SGD)
:=v −γ (cid:0) x⊤S⊤v −y (cid:1) Sx , t=1,...,N,
t−1 t t t−1 t t
where(x ,y )N areindependentsamplesfromP and(γ )N arethestepsizes. Weconsiderapopulargeometric
t t t=1 t t=1
decayingstepsizescheduler(Geetal.,2019;Wuetal.,2022a),
fort=1,...,N, γ :=γ/2ℓ, whereℓ=⌊t/(N/log(N))⌋. (3)
t
Here,theinitialstepsizeγ isahyperparameterfortheSGDalgorithm. Withoutlossofgenerality,weassumetheinitial
parameterisv =0. TheoutputoftheSGDalgorithmisthelastiteratev .
0 N
1IfargminR(·)isnotunique,wechoosew∗tobetheminimizerwithminimalH-norm.
2Ourresultscanbeextendedtoothersketchingmethods(seeWoodruffetal.,2014,forexample).
4Conditioning on a sketch matrix S ∈ RM × H, each parameter v ∈ RM induces a sketched predictor through
x(cid:55)→⟨S⊤v,x⟩,andwedenoteitsriskby
R (v):=R(S⊤v)=E(cid:0) ⟨Sx,v⟩−y(cid:1)2 , v∈RM.
M
ByincreasingM andN,wehaveasequenceofdatasetsandtrainableparametersofincreasingsizes,respectively. This
preparesustostudythescalinglaw(1)inthesketchedlinearregressionproblem,thatis,tounderstandR (v )asa
M N
functionofbothM andN.
Riskdecomposition. Inastandardway,wedecomposetheriskachievedbyv ,thelastiterateof(SGD),tothesum
N
ofirreduciblerisk,approximizationerror,andexcessriskasfollows,
R (v )=minR(·)+minR (·)−minR(·)+R (v )−minR (·). (4)
M N M M N M
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
Irreducible Approx Excess
We emphasize that the irreducible risk is independent of M and N and thus can be viewed as a constant; the
approximationerrorisdeterminedbythesketchmatrixS,thusdependsonM butisindependentofN;theexcessrisk
dependsonbothM andN asitisdeterminedbythealgorithm.
4 Scaling laws
Wefirstdemonstrateascaling-lawbehaviorwhenthedataspectrumsatisfiesapowerlaw.
Assumption1(Distributionalconditions). Assumethefollowingaboutthedatadistribution.
A. Gaussiandesign. Assumethatx∼N(0,H).
B. Well-specifiedmodel. AssumethatE[y|x]=x⊤w∗. Defineσ2 :=E(y−x⊤w∗)2.
C. Parameterprior. Assumethatw∗satisfiesapriorsuchthatE(w∗)⊗2 =I.
Assumption2(Power-lawspectrum). Thereexistsa>1suchthattheeigenvaluesofHsatisfyλ ≂i−a,i>0.
i
Theorem4.1(Scalinglaw). SupposethatAssumptions1and2hold. ConsideranM-dimensionalsketchedpredictor
trainedby(SGD)withN samples. LetN :=N/log(N)andrecalltheriskdecompositionin(4). Thenthereexists
eff
somea-dependentconstantc>0suchthatwhentheinitialstepsizeγ ≤c,withprobabilityatleast1−e−Ω(M)over
therandomnessofthesketchmatrixS,wehave
1. Irreducible:=R(w∗)=σ2.
2. E Approx≂M1−a.
w∗
3. Supposeinadditionσ2 ≳1. Theexpectedexcessrisk(Excess)canbedecomposedintoabiaserror(Bias)anda
varianceerror(Var),namely,
EExcess≂Bias+σ2Var,
wheretheexpectationisovertherandomnessofw∗and(x ,y )N . Moreover,BiasandVarsatisfy
i i i=1
Bias≲max(cid:8) M1−a, (N γ)1/a−1(cid:9) ,
eff
Bias≳(N γ)1/a−1when(N γ)1/a ≤M/c forsomeconstantc>0,
eff eff
Var≂min(cid:8)
M, (N
γ)1/a(cid:9)
/N .
eff eff
Inallresults,thehiddenconstantsonlydependonthepower-lawdegreea. Asadirectconsequence,whenσ2 ≂1,it
holdswithprobabilityatleast1−e−Ω(M)overtherandomnessofthesketchmatrixSthat
(cid:18) (cid:19) (cid:18) (cid:19)
1 1
ER (v )=σ2+Θ +Θ ,
M N Ma−1 (N γ)(a−1)/a
eff
wheretheexpectationisovertherandomnessofw∗and(x ,y )N .
i i i=1
5Theorem4.1showsasharp(uptoconstantfactors)scalinglawriskboundunderanisotropticpriorassumptionandthe
power-lawspectrumassumption. WeemphasizethatthescalinglawboundinTheorem4.1holdsforeveryM,N ≥1.
WealsoremarkthatthesumofapproximizationandbiaserrorsdominatesER (v )−σ2,whereasthevarianceerror
M N
isofstricthigherorderintermsofbothM andN,andisthusdisappearedinthepopulationriskbound.
Optimalstepsize. BasedonthetightscalinglawinTheorem4.1,wecancalculatetheoptimalstepsizethatminimizes
therisk. Specifically,theoptimalstepsizeisγ ≂1whenN ≲MaandcanbeanythingsuchthatMa/N ≲γ ≲1
eff eff
whenN ≳Ma. Inbothcases,choosingγ ≂1isoptimal. WhenthesamplesizeislargesuchthatN ≳Ma,the
eff eff
optimalstepsizeisrelativelyrobustandcanbechosenfromarange.
Allocationofdataandmodelsizes. FollowingHoffmannetal.(2022),wemeasurethecomputecomplexitybyMN
as(SGD)queriesM-dimensionalgradientsforN times. GivenatotalcomputebudgetofMN =C,fromTheorem6.1
andN :=N/log(N),weseethatthebestpopulationriskisachievedbysettingγ =Θ(1),M =Θ˜(C1/(a+1)),and
eff
N =Θ˜(Ca/(a+1)). Ourtheorysuggestssettingadatasizeslightlylargerthanthemodelsizewhenthecomputebudget
isthebottleneck.
Comparisonwith(Bordelonetal.,2024). TheworkbyBordelonetal.(2024)consideredthescalinglawofbatch
gradientdescent(orgradientflow)onateacher-studentmodel(seetheirequation(14)). Theirteacher-studentmodel
canbeviewedasoursketchedlinearregressionmodel. However,weconsiderone-passSGD,thereforeinoursetting
thenumberofgradientstepsisequivalenttothedatasize. Whenweequalizethenumberofgradientstepsandthe
datasizeintheirequation(14)andsettheparameterpriorasAssumption1C,theirpredictionisconsistentwithours.
However,ouranalysisshowsthecomputationaladvantageofSGDoverbatchGDsinceeachiterationrequiresonly
1/N thecompute. Bordelonetal.(2024)obtainedthelimitofthepopulationriskastwooutofthedatasize,model
size,andthenumberofgradientstepsgotoinfinitybasedonstatisticalphysicsheuristics. Incomparison,weobtain
upperandlowerriskboundsthatholdforanyfiniteM andN andmatchignoringaconstantfactordependingonlyon
thespectrumpower-lawdegreea.
AverageoftheSGDiterates ResultssimilartoTheorem4.1canalsobeestablishedfortheaverageoftheiteratesof
onlineSGDwithconstantstepsize(PolyakandJuditsky,1992;Dieuleveutetal.,2017;Jainetal.,2018,2017;Zou
etal.,2023). AllresultswillbethesameoncereplacingtheeffectivesamplesizeN inTheorem4.1tothesample
eff
sizeN. FormoredetailsseeTheoremF.6inAppendixF.
4.1 Scalinglawundersourcecondition
Theisotropicparameterpriorcondition(Assumption1C)inTheorem4.1canbegeneralizedtothefollowinganisotropic
version(CaponnettoandDeVito,2007).
Assumption3(Sourcecondition). Let(λ ,v ) betheeigenvaluesandeigenvectorsofH. Assumew∗satisfiesa
i i i>0
priorsuchthat
fori̸=j, E⟨v ,w∗⟩⟨v ,w∗⟩=0; andfori>0, Eλ ⟨v ,w∗⟩2 ≂i−b, forsomeb>1.
i j i i
Alargerexponentbimpliesafasterdecayofsignalw∗andthuscorrespondstoasimplertask(CaponnettoandDeVito,
2007). NotethatAssumption1CsatisfiesAssumption3withb=a.
Theorem4.2(Scalinglawundersourcecondition). InTheorem4.1,supposeAssumption1CisreplacedbyAssumption3
with1<b<a+1. Thenthereexistssomea-dependentconstantc>0suchthatwhenγ ≤c,withprobabilityatleast
1−e−Ω(M)overtherandomnessofthesketchmatrixS,wehave
(cid:18) 1 (cid:19) (cid:18) 1 (cid:19) (cid:18)min(cid:8) M, (N γ)1/a(cid:9)(cid:19)
ER (v )=σ2+Θ +Θ +Θ eff .
M N Mb−1 (N γ)(b−1)/a N
eff eff
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
Approx+Bias Var
wheretheexpectationisovertherandomnessofw∗ and(x ,y )N ,andΘ(·)hidesconstantsthatmaydependon
i i i=1
(a,b).
6When1<b≤a,thetasksarerelativelyhard,andthevarianceerrorisdominatedbythesumofapproximationand
biaserrorsforallchoicesofM,N,andγ ≲ 1. Inthiscase,Theorem4.2givesthesamepredictionaboutoptimal
stepsizeandoptimalallocationofdataandmodelsizesundercomputebudgetasTheorem4.1.
Whena<b<a+1,thetasksarerelativelyeasy,andvarianceremainsdominatedbythesumofapproximationand
biaserrorifthestepsizeisoptimallytuned. Recallthatγ ≲1,thuswecanrewritetheriskboundinTheorem4.2as
1
min(cid:8)
M, (N
γ)1/a(cid:9)
ER (v )−σ2 ≂ + eff
M N min(cid:8) M, (N effγ)1/a(cid:9)b−1 N eff
(cid:40) min(cid:8) M, (N γ)1/a(cid:9) /N M ≳N1/bandNa/b−1 ≲γ ≲1,
≂ eff eff eff eff
min(cid:8) M, (N γ)1/a(cid:9)1−b M ≲N1/borγ ≲Na/b−1.
eff eff eff
Thereforetheoptimalstepsizeandtheriskundertheoptimalstepsizeis
γ ≂Na/b−1ifM ≳N1/b, and Ma/N ≲γ ≲1ifM ≲N1/b.
eff eff eff eff
Undertheoptimallytunnedstepsize,thepopulationriskisintheformof
min ER (v )=σ2+Θ(N(1−b)/b)+Θ(M1−b),
γ M N eff
whichisagaininthescalinglawform(1). Thisisexpectedsinceanoptimallytunedstepsizecontrolsthevarianceerror
byadjustingthestrengthoftheimplicitbiasofSGD.UnderafixedcomputebudgetC =MN,ourtheorysuggeststo
assignM =Θ˜(C1/(b+1))andN =Θ˜(Cb/(b+1)),andsetthestepsizetoγ ≂Θ˜(C(a−b)/(b+1)).
Whenb≥a+1,thetasksareevensimpler. WeprovideupperandlowerboundsinAppendixD.3. However,there
existsagapbetweenthebounds,fixingwhichisleftforfuturework.
4.2 Scalinglawunderlogarithmicpowerlaw
Wealsoderivetheriskformulawhenthedatacovariancehasalogarithmicpower-lawspectrum(Bartlettetal.,2020).
Assumption4(Logarithmicpower-lawspectrum). Thereexistsa > 1suchthattheeigenvaluesofHsatisfyλ ≂
i
i−1log−a(i+1),i>0.
Theorem4.3(Scalinglawunderlogarithmicpowerspectrum). InTheorem4.1,supposeAssumption2isreplacedby
Assumption4. Thenwithprobabilityatleast1−e−Ω(M)overtherandomnessofthesketchmatrixS,wehave
ER (v
)=σ2+Θ(cid:18) 1 (cid:19) +Θ(cid:18) 1 (cid:19)
, Var≂
min(cid:8) M, logaN (e Nff eγ ffγ)(cid:9)
,
M N loga−1(M) loga−1(N γ) N
eff eff
wheretheexpectationisovertherandomnessofw∗and(x ,y )N .
i i i=1
Theorem4.3providesascalinglawunderthelogarithmicpower-lawspectrum. SimilartoTheorem4.1,thevariance
errorisdominatedbytheapproximationandbiaserrorsforallchoicesofM,N,andγ,andthusdisappearedfromthe
riskbound. DifferentfromTheorem4.1,herethepopulationriskisapolynomialoflog(M)andlog(N γ).
eff
5 Experiments
Inthissection,weexaminetherelationbetweentheexpectedriskofthe(SGD)output,thedatasizeN,andthemodel
sizeM whenthecovariatessatisfyapower-lawcovariancespectrum. AlthoughourresultsinSection4holdwithhigh
probabilityoverS,forsimplicity,weassumetheexpectationoftheriskistakenoverbothw∗andSinoursimulations.
WeadoptthemodelinSection3andtrainitusingone-pass (SGD)withgeometricdecayingstepsize(3). Wechoose
the dimension d sufficiently large to approximate the infinite-dimensional case, and the data are generated so that
Assumption1issatisfied. Moreover,wechoosethecovarianceH∈Rd×dtobediagonalwithH ∝iaandtr(H)=1
ii
forsomea>1.FromFigure1,weobservethattheriskindeedfollowsapower-lawformulajointlyinthenumberof
samplesandthenumberofparameters. Inaddition,thefittedexponentsarealignedwithourtheoreticalpredictions
76×101 a=1.5 a=1.5 a=2.0 a=2.0
k=-0.33 4×101 k=-0.49 k=-0.50 k=-0.98
4×101 Risk Risk Risk 101 Risk
3×101 3×101 101
2×101
2×101
101
102 103 104 105 101 102 102 103 104 105 101 102
Number of effective samples (Neff) Model size (M) Number of effective samples (Neff) Model size (M)
(a)a=1.5 (b)a=1.5 (c)a=2 (d)a=2
Figure2: Theexpectedriskofthelastiterateof (SGD)minustheirreducibleriskversustheeffectivesamplesizeandmodel
size.Parametersσ=1,γ =0.1.(a),(b):a=1.5,d=10000;(c),(d):a=2,d=1000.Theerrorbarsdenotethe±1standard
deviationofestimatingtheexpectedriskusing100independentsamplesof(w∗,S).Weuselinearfunctionstofittheexpectedrisk
underthelog-logscaleandreporttheslopeofthefittedlines(denotedbyk).
(a−1,1−1/a)inTheorem4.1. Figure2showsthescalingoftheexpectedriskindatasize(ormodelsize)when
themodelsize(ordatasize)isrelativelylarge. Weseethattheexpectedriskalsosatisfiesapower-lawdecaywith
exponentsmatchingourpredictions. Itisnoteworthythatoursimulationsdemonstratestrongerobservationsthanthe
theoreticalresultsinTheorem4.1,whichonlyestablishesmatchingupperandlowerboundsuptoaconstantfactor.
Additionalsimulationresultsontheriskoftheaverageof(SGD)iteratescanbefoundinAppendixF.
6 Risk bounds under a general spectrum
Inthissection,wepresentsomegeneralresultsontheupperandlowerboundsoftheriskoftheoutputof(SGD). Due
totherotationalinvarianceofthesketchedmatrixS,withoutlossofgenerality,weassumethecovarianceHisdiagonal
withnon-increasingdiagonalentries. OurmainresultsinSection4aredirectlybuiltonthegeneralboundsintroduced
here.
Assumption5(Generaldistributionalconditions). Assumethefollowingaboutthedatadistribution.
A. Hypercontractivity. Thereexistsα≥1suchthatforeveryPSDmatrixAitholdsthat
Exx⊤Axx⊤ ⪯αtr(HA)H.
B. Misspecifiedmodel. Thereexistsσ2 >0suchthatE(y−x⊤w∗)2xx⊤ ⪯σ2H.
ItisclearthatAssumption1impliesAssumption5withα=3.
Excessriskdecomposition. ConditioningonthesketchmatrixS,thetrainingofthesketchedlinearpredictorcanbe
viewedasanM-dimensionallinearregressionproblem. WecanthereforeinvokeexistingSGDanalysis(Wuetal.,
2022a,b)tosharplycontroltheexcessriskbycontrollingthebiasandvarianceerrors. Specifically,letusdefinethe
(w∗-dependent)biaserroras
(cid:13) N (cid:13)2
Bias(w∗):=(cid:13) (cid:13)(cid:89)(cid:0) I−γ tSHS⊤(cid:1) v∗(cid:13) (cid:13) , wherev∗ :=(SHS⊤)−1SHw∗, (5)
(cid:13) (cid:13)
t=1 SHS⊤
andthevarianceerroras
#{λ˜ ≥1/(N γ)}+(N γ)2(cid:80) λ˜2
Var:=
j eff eff λ˜ j<1/(Neffγ) j
, N :=N/log(N), (6)
N eff
eff
where(cid:0) λ˜ (cid:1)M areeigenvaluesofSHS⊤. WealsoletBias:=EBias(w∗),wheretheexpectationisoverthepriorof
j j=1
w∗.Usingtheexistingresultsontheoutputof(SGD)inWuetal.(2022a,b),weshowthattheexcessriskin(4)canbe
exactlydecomposedasthesumofbiasandvarianceerrorsunderweakconditions.
8
)(
nim
)Nv(M
E
)(
nim
)Nv(M
E
)(
nim
)Nv(M
E
)(
nim
)Nv(M
ETheorem6.1(Excessriskdecomposition). ConditioningonthesketchmatrixS,considertheexcessriskin(4)induced
bytheoutputof (SGD). Assumev =0. Thenforanyw∗ ∈H,
0
1. UnderAssumptions5Aand5Bandsupposeγ
≤1/(cid:0) cαtr(SHS⊤)(cid:1)
forsomeconstantc>0,wehave
EExcess≲Bias(w∗)+(cid:0) α∥w∗∥2 +σ2(cid:1) Var.
H
2. UnderthestrongerAssumptions1Aand1Bandsupposeγ ≤1/(cid:0) cαtr(SHS⊤))forsomeconstantc>0,wehave
EExcess≳Bias(w∗)+σ2Var.
Inbothresults,theexpectationsofExcessaretakenover(x ,y )N .
t t t=1
Assumingthatthesignal-to-noiseratioisupperbounded,thatis,∥w∗∥2 /σ2 ≲1,thenthebias-variancedecomposition
H
oftheexcessriskissharpuptoconstantfactors.
Thevarianceerrorisinaniceformandcanbecomputedusingthefollowingimportantlemmaonthespectrumof
SHS⊤. Similarresultsforlogarithmicpower-lawarealsoestablishedinLemmaG.6inAppendixG.
Lemma6.2(Powerlaw). UnderAssumption2,itholdswithprobabilityatleast1−e−Ω(M)that
µ (SHS⊤)≂µ (H)≂j−a, j =1,...,M.
j j
Forany0≤k∗ ≤k† ≤∞,letS ∈RM×(k†−k∗)denotethematrixformedbythek∗+1−k†-thcolumnsofS.
k∗:k†
Wealsoabusethenotationk† :∞fork† :dwhendisfinite. WeletH ∈R(k†−k∗)×(k†−k∗)bethesubmatrixof
k∗:k†
Hformedbythek∗+1−k†-theigenvalues. Fortheapproximationandbiaserror,weusethefollowingupperand
lowerboundstocomputetheirvalues.
Theorem6.3(Ageneralupperbound). SupposeAssumption5holds. Assumev = 0,r(H) ≥ 2M andtheinitial
0
stepsizesatisfiesγ <1/(cαtr(SHS⊤))forsomeconstantc>0.Thenforanyk ,k ≤M/3,withprobabilityatleast
1 2
1−e−Ω(M)
(cid:115)
(cid:18)(cid:80) λ (cid:80) λ2(cid:19)
Approx≲∥w∗ ∥2 + i>k1 i +λ + i>k1 i ∥w∗ ∥2,
k1:∞ Hk1:∞ M k1+1 M 0:k1
(cid:34) (cid:35)2
∥w∗ ∥2 µ (S H S⊤ )
Bias(w∗)≲ 0:k2 2 · M/2 k2:∞ k2:∞ k2:∞ +∥w∗ ∥2 .
N effγ µ M(S k2:∞H k2:∞S⊤ k2:∞) k2:∞ Hk2:∞
Theorem6.4(Agenerallowerbound). SupposeAssumption1holds. Assumev = 0, r(H) ≥ M andtheinitial
0
stepsizeγ
<1/(cid:0) ctr(SHS⊤)(cid:1)
forsomeconstantc>0.Then
E Approx≳
(cid:88)d
λ , E Bias(w∗)≳
(cid:88) µ i(SH2S⊤)
w∗ i w∗ µ (SHS⊤)
i
i=M i:λ˜ i<1/(Neffγ)
almost surely, where (λ )d are eigenvalues of H in non-increasing order, (λ˜ )d are eigenvalues of SHS⊤ in
i i=1 i i=1
non-increasingorder.
7 Conclusion
We analyze neural scaling laws in infinite-dimensional linear regression. We consider a linear predictor with M
trainableparametersonthesketchedcovariates,whichistrainedbyone-passstochasticgradientdescentwithN data.
UnderaGaussianpriorassumptionontheoptimalmodelparameterandapowerlaw(ofdegreea>1)assumption
onthespectrumofthedatacovariance,wederivematchingupperandlowerboundsonthepopulationriskminusthe
irreducibleerror,thatis,Θ(M−(a−1)+N−(a−1)/a). Inparticular,weshowthatthevarianceerror,whichincreases
withM,isofstrictlyhigherordercomparedtotheothererrors,thusdisappearingfromtheriskbound. Weattributethe
niceempiricalformulaoftheneuralscalinglawtothenon-dominationofthevarianceerror,whichultimatelyisan
effectoftheimplicitregularizationofSGD.
9Acknowledgements
WegratefullyacknowledgethesupportoftheNSFforFODSIthroughgrantDMS-2023505,oftheNSFandtheSimons
FoundationfortheCollaborationontheTheoreticalFoundationsofDeepLearningthroughawardsDMS-2031883
and#814639,andoftheONRthroughMURIawardN000142112431. JDLacknowledgessupportoftheNSFCCF
2002272,NSFIIS2107304,andNSFCAREERAward2144994. SMKacknowledgesagiftfromtheChanZuckerberg
InitiativeFoundationtoestablishtheKempnerInstitutefortheStudyofNaturalandArtificialIntelligence;support
fromONRunderawardN000142212377,andNSFunderawardIIS2229881.
Bibliography
IbrahimMAlabdulmohsin,BehnamNeyshabur,andXiaohuaZhai. Revisitingneuralscalinglawsinlanguageand
vision. AdvancesinNeuralInformationProcessingSystems,35:22300–22312,2022.
AlexanderBAtanasov,JacobAZavatone-Veth,andCengizPehlevan. Scalingandrenormalizationinhigh-dimensional
regression. arXivpreprintarXiv:2405.00592,2024.
FrancisBachandEricMoulines. Non-strongly-convexsmoothstochasticapproximationwithconvergencerateo(1/n).
Advancesinneuralinformationprocessingsystems,26:773–781,2013.
YasamanBahri,EthanDyer,JaredKaplan,JaehoonLee,andUtkarshSharma. Explainingneuralscalinglaws. arXiv
preprintarXiv:2102.06701,2021.
Peter L Bartlett, Philip M Long, Gábor Lugosi, and Alexander Tsigler. Benign overfitting in linear regression.
ProceedingsoftheNationalAcademyofSciences,2020.
RaphaëlBerthier,FrancisBach,andPierreGaillard. Tightnonparametricconvergenceratesforstochasticgradient
descentunderthenoiselesslinearmodel. AdvancesinNeuralInformationProcessingSystems,33:2576–2586,2020.
TamayBesiroglu,EgeErdil,MatthewBarnett,andJoshYou. Chinchillascaling: Areplicationattempt. arXivpreprint
arXiv:2404.10102,2024.
BlakeBordelon,AlexanderAtanasov,andCengizPehlevan. Adynamicalmodelofneuralscalinglaws. arXivpreprint
arXiv:2402.01092,2024.
TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhariwal,ArvindNeelakantan,
PranavShyam,GirishSastry,AmandaAskell,etal. Languagemodelsarefew-shotlearners. Advancesinneural
informationprocessingsystems,33:1877–1901,2020.
Ethan Caballero, Kshitij Gupta, Irina Rish, and David Krueger. Broken neural scaling laws. arXiv preprint
arXiv:2210.14891,2022.
AndreaCaponnettoandErnestoDeVito. Optimalratesfortheregularizedleast-squaresalgorithm. Foundationsof
ComputationalMathematics,7:331–368,2007.
AlexandreDéfossezandFrancisBach. Averagedleast-mean-squares: Bias-variancetrade-offsandoptimalsampling
distributions. InArtificialIntelligenceandStatistics,pages205–213,2015.
AymericDieuleveutandFrancisR.Bach. Non-parametricstochasticapproximationwithlargestepsizes. TheAnnals
ofStatistics,2015.
AymericDieuleveut, NicolasFlammarion, andFrancisBach. Harder, better, faster, strongerconvergenceratesfor
least-squaresregression. TheJournalofMachineLearningResearch,18(1):3520–3570,2017.
ElvisDohmatob,YunzhenFeng,PuYang,FrancoisCharton,andJuliaKempe. Ataleoftails: Modelcollapseasa
changeofscalinglaws. arXivpreprintarXiv:2402.07043,2024.
Rong Ge, Sham M Kakade, Rahul Kidambi, and Praneeth Netrapalli. The step decay schedule: A near optimal,
geometricallydecayinglearningrateprocedureforleastsquares. Advancesinneuralinformationprocessingsystems,
32,2019.
10Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun, Tom B
Brown,PrafullaDhariwal,ScottGray,etal. Scalinglawsforautoregressivegenerativemodeling. arXivpreprint
arXiv:2010.14701,2020.
JoelHestness,SharanNarang,NewshaArdalani,GregoryDiamos,HeewooJun,HassanKianinejad,MdMostofaAliPat-
wary,YangYang,andYanqiZhou.Deeplearningscalingispredictable,empirically.arXivpreprintarXiv:1712.00409,
2017.
JordanHoffmann,SebastianBorgeaud,ArthurMensch,ElenaBuchatskaya,TrevorCai,ElizaRutherford,DiegodeLas
Casas,LisaAnneHendricks,JohannesWelbl,AidanClark,etal. Trainingcompute-optimallargelanguagemodels.
arXivpreprintarXiv:2203.15556,2022.
MarcusHutter. Learningcurvetheory. arXivpreprintarXiv:2102.04074,2021.
AyushJain,AndreaMontanari,andErenSasoglu. Scalinglawsforlearningwithrealandsurrogatedata. arXivpreprint
arXiv:2402.04376,2024.
PrateekJain,PraneethNetrapalli,ShamMKakade,RahulKidambi,andAaronSidford.Parallelizingstochasticgradient
descentforleastsquaresregression: mini-batching,averaging,andmodelmisspecification. TheJournalofMachine
LearningResearch,18(1):8258–8299,2017.
PrateekJain,ShamM.Kakade,RahulKidambi,PraneethNetrapalli,VenkataKrishnaPillutla,andAaronSidford. A
MarkovChainTheoryApproachtoCharacterizingtheMinimaxOptimalityofStochasticGradientDescent(forLeast
Squares). In37thIARCSAnnualConferenceonFoundationsofSoftwareTechnologyandTheoreticalComputer
Science(FSTTCS2017),2018.
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec
Radford,JeffreyWu,andDarioAmodei. Scalinglawsforneurallanguagemodels. arXivpreprintarXiv:2001.08361,
2020.
AranKomatsuzaki. Oneepochisallyouneed. arXivpreprintarXiv:1906.06669,2019.
AlexanderMaloney, DanielARoberts, andJamesSully. Asolvablemodelofneuralscalinglaws. arXivpreprint
arXiv:2210.16859,2022.
EricMichaud,ZimingLiu,UzayGirit,andMaxTegmark. Thequantizationmodelofneuralscaling. Advancesin
NeuralInformationProcessingSystems,36,2024.
MehryarMohri,AfshinRostamizadeh,andAmeetTalwalkar. Foundationsofmachinelearning. MITpress,2018.
Niklas Muennighoff, Alexander M Rush, Boaz Barak, Teven Le Scao, Aleksandra Piktus, Nouamane Tazi,
Sampo Pyysalo, Thomas Wolf, and Colin Raffel. Scaling data-constrained language models. arXiv preprint
arXiv:2305.16264,2023.
YoonsooNam,NayaraFonseca,SeokHyeongLee,andArdLouis. Anexactlysolvablemodelforemergenceand
scalinglaws. arXivpreprintarXiv:2404.17563,2024.
BorisTPolyakandAnatoliBJuditsky. Accelerationofstochasticapproximationbyaveraging. SIAMjournalon
controlandoptimization,30(4):838–855,1992.
JonathanSRosenfeld,AmirRosenfeld,YonatanBelinkov,andNirShavit.Aconstructivepredictionofthegeneralization
erroracrossscales. arXivpreprintarXiv:1909.12673,2019.
UtkarshSharmaandJaredKaplan. Aneuralscalinglawfromthedimensionofthedatamanifold. arXivpreprint
arXiv:2004.10802,2020.
WilliamSwartworthandDavidPWoodruff. Optimaleigenvalueapproximationviasketching. InProceedingsofthe
55thAnnualACMSymposiumonTheoryofComputing,pages145–155,2023.
11HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,TimothéeLacroix,Baptiste
Rozière,NamanGoyal,EricHambro,FaisalAzhar,etal. Llama: Openandefficientfoundationlanguagemodels.
arXivpreprintarXiv:2302.13971,2023.
AdityaVardhanVarre,LoucasPillaud-Vivien,andNicolasFlammarion. LastiterateconvergenceofSGDforleast-
squaresintheinterpolationregime. InAdvancesinNeuralInformationProcessingSystems,2021.
MartinJWainwright. High-dimensionalstatistics: Anon-asymptoticviewpoint,volume48. CambridgeUniversity
Press,2019.
AlexanderWei,WeiHu,andJacobSteinhardt. Morethanatoy: Randommatrixmodelspredicthowreal-worldneural
representationsgeneralize. InProceedingsofthe39thInternationalConferenceonMachineLearning,volume162
ofProceedingsofMachineLearningResearch,2022.
DavidPWoodruffetal. Sketchingasatoolfornumericallinearalgebra. FoundationsandTrends®inTheoretical
ComputerScience,10(1–2):1–157,2014.
JingfengWu,DifanZou,VladimirBraverman,QuanquanGu,andShamM.Kakade. Lastiterateriskboundsofsgd
withdecayingstepsizeforoverparameterizedlinearregression. The39thInternationalConferenceonMachine
Learning,2022a.
Jingfeng Wu, Difan Zou, Vladimir Braverman, Quanquan Gu, and Sham M. Kakade. The power and limitation
ofpretraining-finetuningforlinearregressionundercovariateshift. The36thConferenceonNeuralInformation
ProcessingSystems,2022b.
XiaohuaZhai,AlexanderKolesnikov,NeilHoulsby,andLucasBeyer. Scalingvisiontransformers. InProceedingsof
theIEEE/CVFconferenceoncomputervisionandpatternrecognition,pages12104–12113,2022.
Difan Zou, Jingfeng Wu, Vladimir Braverman, Quanquan Gu, Dean P Foster, and Sham Kakade. The benefits of
implicitregularizationfromsgdinleastsquaresproblems. AdvancesinNeuralInformationProcessingSystems,34:
5456–5468,2021.
DifanZou,JingfengWu,VladimirBraverman,QuanquanGu,andShamMKakade. Benignoverfittingofconstant-
stepsizesgdforlinearregression. JournalofMachineLearningResearch,24(326):1–58,2023.
12Appendix
Table of Contents
A Preliminary 13
A.1 Additionalnotationsandcommentsondataassumptions . . . . . . . . . . . . . . . . . . . . . . . . 13
A.2 Approximationerror . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
A.3 Bias-variancedecomposition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
A.4 ProofofTheorem6.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
A.5 ProofsofLemma6.2,Theorem6.3and6.4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
B ProofsinSection4 18
B.1 ProofofTheorem4.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
B.2 ProofofTheorem4.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
B.3 ProofofTheorem4.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
C Approximationerror 19
C.1 Anupperbound . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
C.2 Alowerbound . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
C.3 AlowerboundunderAssumption3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
C.4 ExamplesonmatchingboundsforApprox . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
D Biaserror 26
D.1 Anupperbound . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
D.2 Alowerbound . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
D.3 ExamplesonmatchingboundsforBias(w∗) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
E Varianceerror 32
F Expectedriskoftheaverageof(SGD)iterates 33
F.1 Matchingboundsfortheaverageof(SGD)iteratesunderpower-lawspectrum . . . . . . . . . . . . . 35
F.2 Proofs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
G Concentrationlemmas 39
G.1 Generalconcentrationresults . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
G.2 Concentrationresultsunderpower-lawspectrum . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
G.3 Concentrationresultsunderlogarithmicpower-lawspectrum . . . . . . . . . . . . . . . . . . . . . . 45
A Preliminary
Inthissection,weprovidesomepreliminarydiscussionsandaproofofTheorem6.1. Concretely,inSectionA.1we
discussourdataassumptionsandintroduceadditionalnotations. InSectionA.2,A.3wederiveintermediateresultsthat
contributetotheproofofTheorem6.1. Finally,acompleteproofofTheorem6.1iscontainedinSectionA.4.
A.1 Additionalnotationsandcommentsondataassumptions
Tensors. FormatricesA,B,C,D,andXofappropriateshape,itholdsthat
(B⊤⊗A)◦X=AXB,
13andthat
(D⊤⊗C)◦(B⊤⊗A)◦X=(cid:0) (D⊤B⊤)⊗(CA)(cid:1)
◦X
=CAXBD.
Forsimplicity,wedenote
A⊗2 :=A⊗A.
CommentsonAssumption2,3and4 DuetotherotationalinvarianceoftheGaussiansketchedmatrixS,throughout
theappendix,weassumew.l.o.g. thatthecovarianceoftheinputcovariatesHisdiagonalwiththe(i,i)-thentrybeing
thei-theigenvalue. Specifically,Assumption3canberewrittenas
Assumption6(Sourcecondition). AssumeH=(h ) isadiagonalmatrixwithdiagonalentriesinnon-increasing
ij i,j≥1
order,andw∗satisfiesapriorsuchthat
fori̸=j, Ew∗w∗ =0; andfori>0, Eλ w∗2 ≂i−b, forsomeb>1.
i j i i
NowthatweassumeHisdiagonal. Wemakethefollowingnotations. Define
H :=diag(λ ,...,λ
)∈R(k†−k∗)2
,
k∗:k† k∗+1 k†
where0≤k∗ ≤k†aretwointegers,andweallowk† =∞. Forexample,
H =diag(λ ,...,λ ), H =diag(λ ,...).
0:k 1 i k:∞ k+1
Similarly,foravectorw∈H,wehave
w :=(w ,...,w∗ )⊤ ∈Rk†−k∗ .
k∗:k† k∗+1 k†
A.2 Approximationerror
Recalltheriskdecompositionin(4),
R (v )=minR(·)+minR (·)−minR(·)+R (v )−minR (·).
M N M M N M
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
Irreducible Approx Excess
LemmaA.1(Approximizationerror). ConditionalonthesketchmatrixS,theminimizerofR (v)isgivenby
M
v∗ :=(SHS⊤)−1SHw∗,
andtheapproximationerrorin(4)is
Approx:=minR (·)−minR(·)
M
=(cid:13) (cid:13) (cid:13)(cid:16) I−H21S⊤(cid:0) SHS⊤(cid:1)−1 SH21(cid:17) H1 2w∗(cid:13) (cid:13) (cid:13)2 . (7)
Moreover,Approx≤∥w∗∥2 almostsurelyovertherandomnessofS.
H
ProofofLemmaA.1. Recallthattherisk
R(w):=E(cid:0) ⟨x,w⟩−y(cid:1)2
isaquadraticfunctionandthatw∗istheminimizerofR(·),sowehave
(cid:0)Ex⊗2(cid:1) w∗ =Exy ⇔Hw∗ =Exy,
14and
R(w)=E(cid:0) ⟨x,w⟩−⟨x,w∗⟩(cid:1)2 +R(w∗)
=∥H1 2(w−w∗)∥2+R(w∗).
Recallthattheriskinarestrictedsubspace
R
(v):=R(S⊤v)=E(cid:0) ⟨Sx,v⟩−y(cid:1)2
M
isalsoaquadraticfunction,soitsminimizerisgivenby
v∗ =(cid:0)E(Sx)⊗2(cid:1)−1ESxy
=(cid:0) SHS⊤(cid:1)−1 SHw∗.
Therefore,theapproximationerroris
Approx:=R (v∗)−R(w∗)
M
=R(S⊤v∗)−R(w∗)
=∥H21(S⊤v∗−w∗)∥2
=∥H21(S⊤(cid:0) SHS⊤(cid:1)−1 SHw∗−w∗)∥2
=(cid:13) (cid:13) (cid:13)(cid:16) I−H1 2S⊤(cid:0) SHS⊤(cid:1)−1 SH21(cid:17) H1 2w∗(cid:13) (cid:13) (cid:13)2 .
Finally,since
(cid:16) I−H21S⊤(cid:0) SHS⊤(cid:1)−1 SH1 2(cid:17)2 =I−H1 2S⊤(cid:0) SHS⊤(cid:1)−1 SH21 ⪯I,
itfollowsthatApprox≤∥w∗∥2 .
H
A.3 Bias-variancedecomposition
Theexcessriskin(4)canbeviewedastheSGDexcessriskinanM-dimensional(misspecified)linearregression
problem. WewillutilizeCorollary3.4in(Wuetal.,2022b)togetabias-variancedecompositionoftheexcessrisk. The
followingtwolemmaschecktherelatedassumptionsforCorollary3.4in(Wuetal.,2022b)inoursetup.
LemmaA.2(Hypercontractivityandthemisspecifiednoiseundersketchedfeature). SupposethatAssumptions5A
and5Bhold. ConditioningonthesketchmatrixS,foreveryPSDmatrixA∈RM×M,wehave
E(Sx)⊗2A(Sx)⊗2 ⪯αtr(cid:0) SHS⊤A(cid:1) SHS⊤.
Moreover,fortheminimizerofR (v),thatis,v∗definedinLemmaA.1,wehave
M
E(cid:0) y−⟨v∗,Sx⟩(cid:1)2 (Sx)⊗2 ⪯2(σ2+α∥w∗∥2 )SHS⊤.
H
Theexpectationintheaboveisover(x,y).
ProofofLemmaA.2. ThefirstpartisadirectapplicationofAssumption5A:
E(Sx)⊗2A(Sx)⊗2 =S(cid:0)Exx⊤(S⊤AS)xx⊤(cid:1) S⊤
⪯S(cid:0) αtr(cid:0) HS⊤AS(cid:1) H(cid:1) S⊤
=αtr(cid:0) SHS⊤A(cid:1) SHS⊤.
15Forthesecondpart,wefirstshowthat
E(cid:0) y−⟨v∗,Sx⟩(cid:1)2 x⊗2 ⪯2E(cid:0) y−⟨w∗,x⟩(cid:1)2 x⊗2+2E⟨w∗−S⊤v∗,x⟩2x⊗2
⪯2σ2H+2α⟨H,(w∗−S⊤v∗)⊗2⟩H,
wherethelastinequalityisbyAssumptions5Aand5B.FromtheproofofLemmaA.1,weknowthat
⟨H,(w∗−S⊤v∗)⊗2⟩=Approx≤∥w∗∥2 , almostsurely.
H
Sowehave
E(cid:0) y−⟨v∗,Sx⟩(cid:1)2 x⊗2 ⪯2(σ2+α∥w∗∥2 )H.
H
LeftandrightmultiplyingbothsideswithSandS⊤,weobtainthesecondclaim.
LemmaA.3(Gaussianityandwell-specifiednoiseundersketchedfeatures). SupposethatAssumptions1Aand1Bhold.
ConditionalonthesketchmatrixS,wehave
Sx∼N(0,SHS⊤).
Moreover,fortheminimizerofR (v),thatis,v∗definedinLemmaA.1,wehave
M
E[y|Sx]=⟨Sx,v∗⟩, E(y−⟨Sx,v∗⟩)2 =σ2+Approx≥σ2.
ProofofLemmaA.3. ThefirstclaimisadirectconsequenceofAssumption1A.
Forthesecondclaim,byAssumption1AandLemmaA.1,wehave
E[y|x]=⟨x,w∗⟩
=⟨x,S⊤v∗⟩+⟨x,w∗−S⊤v∗⟩
=⟨x,S⊤v∗⟩+⟨x,(cid:2) I−(SHS⊤)−1SH(cid:3) w∗⟩
=⟨H− 21x,H1 2S⊤v∗⟩+⟨H− 21x,(cid:2) I−H1 2S⊤(SHS⊤)−1SH21(cid:3) H1 2w∗⟩
=⟨SH1 2H−1 2x,v∗⟩+⟨(cid:2) I−H21S⊤(SHS⊤)−1SH21(cid:3) H− 21x,H1 2w∗⟩. (8)
Noticethat
H− 21x∼N(0,I),
byAssumption1Aandthat
SH21(cid:2) I−H21S⊤(SHS⊤)−1SH1 2(cid:3) =0,
therefore
Sx=SH1 2H−1 2xisindependentof(cid:2) I−H21S⊤(SHS⊤)−1SH21(cid:3) H− 21x.
Takingexpectationoverthesecondrandomvectorin(8),wefind
E[y|Sx]=EE[y|x]=⟨SH21H−1 2x,v∗⟩=⟨Sx,v∗⟩.
Itremainstoshow
E(y−⟨Sx,v∗⟩)2 =σ2+Approx.
ThisfollowsfromtheproofofLemmaA.1. Specifically,
E(y−⟨Sx,v∗⟩)2 =R(S⊤v∗)
=Approx+R(w∗)
=Approx+σ2
≥σ2,
wherethesecondequalityisbythedefinitionofApproxandthethirdequalityisbyAssumption1B.Wehavecompleted
theproof.
16A.4 ProofofTheorem6.1
Wenowusetheresultsin(Wuetal.,2022a,b)forSGDtoobtainthefollowingbias-variancedecompositiononthe
excessrisk.
TheoremA.4(Excessriskbounds). Considertheexcessriskin(4)inducedbytheoutputof (SGD). Let
N :=N/log(N), SNR:=(∥w∗∥2 +∥v ∥2 )/σ2.
eff H 0 SHS⊤
ThenconditioningonthesketchmatrixS,foranyw∗ ∈H
1. UnderAssumptions5Aand5B,wehave
EExcess≲(cid:13) (cid:13) (cid:13) (cid:13)(cid:89)N (cid:0) I−γ tSHS⊤(cid:1) (v 0−v∗)(cid:13) (cid:13) (cid:13) (cid:13)2 +(1+αSNR)σ2· D Neff
t=1 SHS⊤ eff
whenγ ≲ 1 forsomeconstantc>0.
cαtr(SHS⊤)
2. UnderAssumptions1Aand1B,wehave
EExcess≳(cid:13) (cid:13) (cid:13) (cid:13)(cid:89)N (cid:0) I−γ tSHS⊤(cid:1) (v 0−v∗)(cid:13) (cid:13) (cid:13) (cid:13)2 +σ2· D Neff
t=1 SHS⊤ eff
whenγ ≲ 1 forsomeconstantc>0.
ctr(SHS⊤)
Inbothresults,theexpectationisover(x ,y )N ,and
t t t=1
D :=#{λ˜ ≥1/(N γ)}+(N γ)2 (cid:88) λ˜2,
eff j eff eff j
λ˜ j<1/(Neffγ)
where(λ˜ )M areeigenvalueofSHS⊤.
j j=1
Theorem6.1followsimmediatelybyLemmaA.1andbysettingv =0andpluggingthedefinitionofBias(w∗)and
0
VarintoTheoremA.4.
ProofofTheoremA.4. This follows from Corollary 3.4 in (Wu et al., 2022b) for a linear regression problem with
populationdatagivenby(Sx,y). NotethatthedatacovariancebecomesSHS⊤ andtheoptimalmodelparameter
becomesv∗.
Fortheupperbound,LemmaA.2verifiesAssumptions1Aand2in(Wuetal.,2022b),withthenoiselevelbeing
σ˜2 =2(σ2+α∥w∗∥2 ).
H
ThenwecanapplytheupperboundinCorollary3.4in(Wuetal.,2022b)(settingtheirindexsetK=∅)toget
EExcess≲(cid:13) (cid:13) (cid:13) (cid:13)(cid:89)N (cid:0) I−γ tSHS⊤(cid:1) (v 0−v∗)(cid:13) (cid:13) (cid:13) (cid:13)2 +(∥v∗−v 0∥2
SHS⊤
+σ˜2)D Neff.
t=1 SHS⊤ eff
Weverifythat
∥v∗−v 0∥2
SHS⊤
≤2∥H1 2S⊤v∗∥2+2∥v 0∥2
SHS⊤
=2∥H21S⊤(SHS⊤)−1SHw∗∥2+2∥v 0∥2
SHS⊤
≤2∥H21w∗∥2+2∥v 0∥2
SHS⊤
=2∥w∗∥2 +2∥v ∥2 ,
H 0 SHS⊤
17whichimpliesthat
(∥v∗−v ∥2 +σ˜2)≤2∥w∗∥2 +2∥v ∥2 +2(σ2+α∥w∗∥2 )
0 SHS⊤ H 0 SHS⊤ H
≲(1+αSNR)σ2.
Substituting,wegettheupperbound.
Forthelowerbound,LemmaA.3showsSxisGaussian,thereforeitsatisfiesAssumption1BinWuetal.(2022b)with
β =1. Besides,LemmaA.3showsthatthelinearregressionproblemiswell-specified,withthenoiselevelbeing
σ˜2 =σ2+Approx≥σ2.
Although the lower bound in Corollary 3.4 in Wu et al. (2022b) is stated for Gaussian additive noise (see their
Assumption2’),itiseasytocheckthatthelowerboundholdsforanywell-specifiednoiseasdescribedbyLemmaA.3.
UsingthelowerboundinCorollary3.4inWuetal.(2022b),weobtain
EExcess≳(cid:13) (cid:13) (cid:13) (cid:13)(cid:89)N (cid:0) I−γ tSHS⊤(cid:1) (v 0−v∗)(cid:13) (cid:13) (cid:13) (cid:13)2 +σ˜2D Neff.
t=1 SHS⊤ eff
Plugginginσ˜2 ≥σ2givesthedesiredlowerbound.
A.5 ProofsofLemma6.2,Theorem6.3and6.4
Lemma6.2isprovedinLemmaG.4. Theorem6.3followsfromLemmaC.1andD.1. Theorem6.4followsfrom
LemmaC.2andD.2.
B Proofs in Section 4
B.1 ProofofTheorem4.1
Proofofpart1. ByAssumption1BandthedefinitionofR(·),wehave
R(w)=E(⟨x,w⟩−y)2 =E(⟨x,w⟩−E[y |x])2+E(y−E[y |x])2
=E(⟨x,w⟩−⟨x,w∗⟩)2+σ2 ≥σ2.
Notethattheequalityholdsifandonlyifw=w∗.ThereforewehaveminR(·)=R(w∗)=σ2.
Proofofpart2. Part2ofTheorem4.1followsimmediatelyfromLemmaC.2.
Proofofpart3. WechooseBias(w∗),Var asdefinedinEq.(5)and(6)andletBias := E Bias(w∗). Part3of
w∗
Theorem4.1followsdirectlyfromthedecompositionoftheexcessriskinTheorem6.1(notethatE∥w∗∥2 /σ2 ≲1),
H
andthematchingboundsinLemmaD.3andE.1.
ItremainstoverifythestepsizeassumptionrequiredinLemmaD.3. SincewehavefromLemmaG.4that
1 1 c c
= ≥ 1 ≥ 2 ≥c
tr(SHS⊤) (cid:80)M λ˜ (cid:80)M λ (cid:80)M i−a 3
i=1 i i=1 i i=1
forsomea-dependentconstantsc ,c ,c > 0withprobabilityatleast1−e−Ω(M),itfollowsthatforanyconstant
1 2 3
c > 1, wecanchooseγ ≤ c forsomea-dependentc suchthatγ ≤ 1 . Therefore, wehaveverifiedthe
0 0 ctr(SHS⊤)
stepsizeassumption.
Finally,thelastclaiminTheorem4.1followsdirectlyfromcombiningthepreviousthreepartsandTheorem6.1,noting
σ2 ≲1. and
min{M, (N γ)1/a} (N γ)1/a
Var≂ eff ≲ eff ≲(N γ)1/a−1 ≲Bias+Approx
N N eff
eff eff
underthestepsizeassumptionγ ≲1.Herethehiddenconstantsmaydependona.
18B.2 ProofofTheorem4.2
SimilartotheproofofTheorem4.1,wehaveminR(·)=σ2underAssumption1B.Moreover,byLemmasC.5,D.4
andE.1,wehavewithprobabilityatleast1−e−Ω(M)that
E Approx≂M1−b,
w∗
Bias≲max(cid:8) M1−b, (N γ)(1−b)/a(cid:9) ,
eff
Bias≳(N γ)(1−b)/awhen(N γ)1/a ≤M/3,
eff eff
Var≂min(cid:8)
M, (N
γ)1/a(cid:9)
/N ,
eff eff
whenthestepsizeγ ≤cforsomea-dependentconstantc>0.Herethehiddenconstantsintheboundsmaydepend
onlyon(a,b). CombiningtheboundsonApprox,Bias,Varandnoting
min{M, (N γ)1/a} (N γ)1/a
Var≂ eff ≲ eff ≲(N γ)(1−b)/a ≲Bias+Approx
N N eff
eff eff
yieldsTheorem4.2. Hereinthesecondinequality,weusetheassumptionb≤a.
B.3 ProofofTheorem4.3
SimilartotheproofofTheorem4.1,wehaveminR(·)=σ2underAssumption1B.Noticethatwehaveγ ≲1implies
γ ≲1/(tr(SHS⊤))withprobabilityatleast1−e−Ω(M)byLemmaG.6. ItfollowsfromLemmaC.6,D.5andE.2that
E Approx≂log1−aM,
w∗
Bias≲max(cid:8) log1−aM, log1−a(N γ)(cid:9) ,
eff
Bias≳log1−a(N γ)when(N γ)1/a ≤Mcforsomesmallconstantc>0,
eff eff
min{M, (N γ)/loga(N γ)} (N γ)/loga(N γ)
Var≂ eff eff ≲ eff eff =log−a(N γ)
N N γ eff
eff eff
withprobabilityatleast1−e−Ω(M)whenthestepsizeγ ≤cforsomea-dependentconstantc>0.SinceVar≲Bias
andlog1−a(N γ)≲log1−aM when(N γ)1/a ≳Mc,puttingtheboundstogethergivesTheorem4.3.
eff eff
C Approximation error
Inthissection,wederiveupperandlowerboundsfortheapproximationerrorin(4)(and7). Wewillalsoshowthatthe
upperandlowerboundsmatchuptoconstantfactorsinseveralexamples.
C.1 Anupperbound
Lemma C.1 (An upper bound on the approximation error). Given any k ≤ d such that r(H) ≥ k + M, the
approximationerrorin(4)(and7)satisfies
Approx≲∥w∗ ∥2 +⟨[H−1 +S⊤ A−1S ]−1,w∗ w∗ ⊤⟩
k:∞ Hk:∞ 0:k 0:k k 0:k 0:k 0:k
almostsurely,whereA :=S H S⊤ . Ifinadditionk ≤M/2,thenwithprobability1−e−Ω(M)
k k:∞ k:∞ k:∞
(cid:115)
(cid:16)(cid:80) λ (cid:80) λ2(cid:17)
Approx≲∥w∗ ∥2 + i>k i +λ + i>k i ∥w∗ ∥2,
k:∞ Hk:∞ M k+1 M 0:k
where(λ )p areeigenvaluesofHinnon-increasingorder.
i i=1
19ProofofLemmaC.1. Write the singular value decomposition H = UΛU⊤, where Λ := diag{λ ,λ ,...} with
1 2
λ ≥ λ ≥ ... ≥ 0andUU⊤ = I. DefineS˜ := SU,w˜∗ := U⊤w∗. ThenbyLemmaA.1theapproximationerror
1 2
Approx=Approx(S,H,w∗)satisfies
Approx(S,H,w∗)=(cid:13) (cid:13) (cid:13)(cid:16) I−H21S⊤(cid:0) SHS⊤(cid:1)−1 SH1 2(cid:17) H1 2w∗(cid:13) (cid:13) (cid:13)2
=(cid:13) (cid:13) (cid:13)(cid:16) I−UΛ1 2S˜⊤(cid:0) S˜ΛS˜⊤(cid:1)−1 SΛ1 2U⊤(cid:17) UΛ21U⊤w∗(cid:13) (cid:13) (cid:13)2
=(cid:13) (cid:13) (cid:13)U(cid:16) I−Λ1 2S˜⊤(cid:0) S˜ΛS˜⊤(cid:1)−1 SΛ1 2(cid:17) Λ1 2U⊤w∗(cid:13) (cid:13) (cid:13)2
=(cid:13) (cid:13) (cid:13)(cid:16) I−Λ1 2S˜⊤(cid:0) S˜ΛS˜⊤(cid:1)−1 SΛ1 2(cid:17) Λ1 2w˜∗(cid:13) (cid:13) (cid:13)2 =Approx(S˜,Λ,w˜∗).
SinceS˜ =d SbyrotationalinvarianceofstandardGaussianvariables,itsufficestoanalyzethecasewhereH=Λisa
diagonalmatrix,astheresultsmaytransfertogeneralHbyreplacingw˜∗withw∗.
Therefore,fromnowonweassumew.l.o.g. thatHisadiagonalmatrixwithnon-increasingdiagonalentries. Define
A:=SHS⊤.
BydefinitionofApprox,wehave
Approx=(cid:13) (cid:13) (cid:13)(cid:16) I−H1 2S⊤(cid:0) SHS⊤(cid:1)−1 SH21(cid:17) H1 2w∗(cid:13) (cid:13) (cid:13)2
=(cid:10) [H1/2S⊤A−1SH1/2−I ]⊗2,H1/2w∗w∗⊤H1/2(cid:11) .
p
Moreover,foranyk ∈[p]
(cid:32) (cid:33)
H1/2S⊤ (cid:16) (cid:17)
H1/2S⊤A−1SH1/2−I p = H10 /: 2k S⊤0:k A−1 S k:∞H1 k/ :∞2 S k:∞H1 k/ :∞2 −I p
k:∞ k:∞
(cid:32) (cid:33)
H1/2S⊤ A−1S H1/2−I H1/2S⊤ A−1S H1/2
= 0:k 0:k 0:k 0:k k 0:k 0:k k:∞ k:∞
H1/2 S⊤ A−1S H1/2 H1/2 S⊤ A−1S H1/2 −I
k:∞ k:∞ 0:k 0:k k:∞ k:∞ k:∞ k:∞ d−k
(cid:18) (cid:19)
U V
=: (9)
V⊤ W
Therefore
(cid:18) U2+VV⊤ UV+VW(cid:19) (cid:18) U2+VV⊤ 0 (cid:19)
[H1/2S⊤A−1SH1/2−I ]⊗2 = ⪯2 ,
p V⊤U+WV⊤ W2+V⊤V 0 W2+V⊤V
andhence
(cid:42)(cid:18) U2+VV⊤ 0 (cid:19) (cid:43)
Approx≤2 ,H1/2w∗w∗⊤H1/2
0 W2+V⊤V
=2(cid:10) U2+VV⊤,H1/2w w⊤ H1/2(cid:11) +2(cid:10) W2+V⊤V,H1/2 w w⊤ H1/2 (cid:11) .
0:k ∗,0:k ∗,0:k 0:k k:∞ ∗,k:∞ ∗,k:∞ k:∞
Weclaimthefollowingresultswhichwewillproveattheendoftheproof.
(cid:10) W2+V⊤V,H1/2 w w⊤ H1/2 (cid:11) ≤∥w∗ ∥2 , (10)
k:∞ ∗,k:∞ ∗,k:∞ k:∞ k:∞ Hk:∞
(cid:10) U2+VV⊤,H1/2w w⊤ H1/2(cid:11) =⟨[H−1 +S⊤ A−1S ]−1,w∗ w∗ ⊤⟩. (11)
0:k ∗,0:k ∗,0:k 0:k 0:k 0:k k 0:k 0:k 0:k
Notethatinclaim(11)theinverseA−1existsalmostsurelysincer(H )≥r(H)−k ≥M byourassumptionand
k k:∞
S ∈ RM×(d−k) isarandomgaussianprojectionontoRM. FirstpartofLemmaC.1followsimmediatelyfrom
k:∞
combiningclaim(10)and(11).
20ToprovethesecondpartofLemmaC.1,firstnotethatwithprobability1−e−Ω(M)wehave
(cid:115)
(cid:16)(cid:80) λ (cid:80) λ2(cid:17)
µ (A−1)=∥A ∥−1 ≥c/ i>k i +λ + i>k i
min k k M k+1 M
forc some constant c > 0 by Lemma G.2. Moreover, by the concentration of the Gaussian variance matrix (see
e.g.,Theorem6.1inWainwright(2019)),wehaveS⊤ S ⪰ I /5withprobability1−e−Ω(M) whenM/k ≥ 2.
0:k 0:k k
Combiningthelasttwoarguments,weobtain
(cid:115)
(cid:16)(cid:80) λ (cid:80) λ2(cid:17)
S⊤ A−1S ⪰cS⊤ S / i>k i +λ + i>k i
0:k k 0:k 0:k 0:k M k+1 M
(cid:115)
(cid:16)(cid:80) λ (cid:80) λ2(cid:17)
≳I / i>k i +λ + i>k i ,
k M k+1 M
andtherefore
⟨[H−1 +S⊤ A−1S ]−1,w∗ w∗ ⊤⟩≤⟨[S⊤ A−1S ]−1,w∗ w∗ ⊤⟩
0:k 0:k k 0:k 0:k 0:k 0:k k 0:k 0:k 0:k
≤∥[H−1 +S⊤ A−1S ]−1∥∥w∗ ∥2
0:k 0:k k 0:k 0:k
(cid:115)
(cid:16)(cid:80) λ (cid:80) λ2(cid:17)
≲ i>k i +λ + i>k i ∥w∗ ∥2 (12)
M k+1 M 0:k
withprobability1−e−Ω(M). CombiningEq.(12)withthefirstpartofLemmaC.1completestheproof.
Proofofclaim(10) Notethat
−I ⪯W=H1/2 S⊤ A−1S H1/2 −I
d−k k:∞ k:∞ k:∞ k:∞ d−k
=H1/2 S⊤ (S H S⊤ +S H S⊤ )−1S H1/2 −I
k:∞ k:∞ 0:k 0:k 0:k k:∞ k:∞ k:∞ k:∞ k:∞ d−k
⪯H1/2 S⊤ (S H S⊤ )−1S H1/2 −I ⪯0 ,
k:∞ k:∞ k:∞ k:∞ k:∞ k:∞ k:∞ d−k d−k
wherethelastinequalityusesthefactthatthenormofprojectionmatricesisnogreaterthanone. Therefore,wehave
∥W∥ ≤1. Now,itremainstoshow
2
W2+V⊤V=−W, (13)
asclaim(10)isadirectconsequenceofEq.(13)andthefactthat∥W∥≤1.
BydefinitionofWinEq.(9),wehave
W2 =(H1/2 S⊤ A−1S H1/2 −I )2
k:∞ k:∞ k:∞ k:∞ d−k
=I −2H1/2 S⊤ A−1S H1/2 +H1/2 S⊤ A−1S H S⊤ A−1S H1/2
d−k k:∞ k:∞ k:∞ k:∞ k:∞ k:∞ k:∞ k:∞ k:∞ k:∞ k:∞
=I −2H1/2 S⊤ A−1S H1/2 +H1/2 S⊤ A−1A A−1S H1/2 .
d−k k:∞ k:∞ k:∞ k:∞ k:∞ k:∞ k k:∞ k:∞
BydefinitionofVinEq.(9),wehave
V⊤V=H1/2 S⊤ A−1(S H S⊤ )A−1S H1/2 .
k:∞ k:∞ 0:k 0:k 0:k k:∞ k:∞
SinceS H S⊤ +A =A,itfollowsthat
0:k 0:k 0:k k
W2+V⊤V=I −2H1/2 S⊤ A−1S H1/2 +H1/2 S⊤ A−1AA−1S H1/2
d−k k:∞ k:∞ k:∞ k:∞ k:∞ k:∞ k:∞ k:∞
=I −H1/2 S⊤ A−1S H1/2 =−W.
d−k k:∞ k:∞ k:∞ k:∞
21Proofofclaim(11) ItsufficestoshowU2+V⊤V=[H−1 +S⊤ A−1S ]−1. UsingthedefinitionofUinEq.(9),
0:k 0:k k 0:k
weobtain
U=H1/2S⊤ A−1S H1/2−I
0:k 0:k 0:k 0:k k
=H1/2S⊤ A−1S H1/2−H1/2S⊤ A−1S [H−1 +S⊤ A−1S ]−1S⊤ A−1S H1/2−I
0:k 0:k k 0:k 0:k 0:k 0:k k 0:k 0:k 0:k k 0:k 0:k k 0:k 0:k k
=H1/2S⊤ A−1S [H−1 +S⊤ A−1S ]−1H−1H1/2−I ,
0:k 0:k k 0:k 0:k 0:k k 0:k 0:k 0:k k
wherethesecondlineusesWoodbury’smatrixidentity,namely
A−1 =[S H S⊤ +A ]−1 =A−1−A−1S [H−1 +S⊤ A−1S ]−1S⊤ A−1.
0:k 0:k 0:k k k k 0:k 0:k 0:k k 0:k 0:k k
ContinuingthecalculationofU,wehave
U=H1/2S⊤ A−1S [H−1 +S⊤ A−1S ]−1H−1/2−I
0:k 0:k k 0:k 0:k 0:k k 0:k 0:k k
=H1/2(S⊤ A−1S [H−1 +S⊤ A−1S ]−1−I )H−1/2
0:k 0:k k 0:k 0:k 0:k k 0:k k 0:k
=−H−1/2[H−1 +S⊤ A−1S ]−1H−1/2.
0:k 0:k 0:k k 0:k 0:k
Therefore,
U2 =H−1/2[H−1 +S⊤ A−1S ]−1H−1[H−1 +S⊤ A−1S ]−1H−1/2. (14)
0:k 0:k 0:k k 0:k 0:k 0:k 0:k k 0:k 0:k
Since
H1/2S⊤ A−1 =H1/2S⊤ A−1−H1/2S⊤ A−1S [H−1 +S⊤ A−1S ]−1S⊤ A−1
0:k 0:k 0:k 0:k k 0:k 0:k k 0:k 0:k 0:k k 0:k 0:k k
=H−1/2[H−1 +S⊤ A−1S ]−1S⊤ A−1
0:k 0:k 0:k k 0:k 0:k k
byWoodbury’smatrixindentity,itfollowsfromthedefinitionofVinEq.(9)that
VV⊤ =H1/2S⊤ A−1S H S⊤ A−1S H1/2
0:k 0:k k:∞ k:∞ k:∞ 0:k 0:k
=H−1/2[H−1 +S⊤ A−1S ]−1S⊤ A−1(S H S⊤ )A−1S [H−1 +S⊤ A−1S ]−1H−1/2
0:k 0:k 0:k k 0:k 0:k k k:∞ k:∞ k:∞ k 0:k 0:k 0:k k 0:k 0:k
=H−1/2[H−1 +S⊤ A−1S ]−1S⊤ A−1S [H−1 +S⊤ A−1S ]−1H−1/2. (15)
0:k 0:k 0:k k 0:k 0:k k 0:k 0:k 0:k k 0:k 0:k
CombiningEq.(14)and(15)yields
U2+VV⊤ =H−1/2[H−1 +S⊤ A−1S ]−1H−1/2, (16)
0:k 0:k 0:k k 0:k 0:k
andtherefore
(cid:10) U2+VV⊤,H1/2w w⊤ H1/2(cid:11) =⟨[H−1 +S⊤ A−1S ]−1,w∗ w∗ ⊤⟩.
0:k ∗,0:k ∗,0:k 0:k 0:k 0:k k 0:k 0:k 0:k
C.2 Alowerbound
FortheapproximationerrorApprox,wehavethefollowingresult.
LemmaC.2(Lowerboundontheapproximationerror). Whenr(H)≥M,underAssumption1C,theapproximation
errorin(4)(and7)satisfies
d
(cid:88)
E Approx≳ λ ,
w∗ i
i=M
where(λ )d areeigenvaluesofHinnon-increasingorder.
i i=1
22ProofofLemmaC.2. Foranyk ≤d,followingtheproofofLemmaC.1,wehave
Approx=(cid:10) [H1/2S⊤A−1SH1/2−I ]⊗2,H1/2w∗(w∗)⊤H1/2(cid:11)
d
and
(cid:32) (cid:33)
H1/2S⊤ A−1S H1/2−I H1/2S⊤ A−1S H1/2
H1/2S⊤A−1SH1/2−I = 0:k 0:k 0:k 0:k k 0:k 0:k k:∞ k:∞
d H1/2 S⊤ A−1S H1/2 H1/2 S⊤ A−1S H1/2 −I
k:∞ k:∞ 0:k 0:k k:∞ k:∞ k:∞ k:∞ d−k
(cid:18) (cid:19)
U V
=: . (17)
V⊤ W
Therefore
(cid:18) U2+VV⊤ UV+VW(cid:19)
[H1/2S⊤A−1SH1/2−I ]⊗2 =
d V⊤U+WV⊤ W2+V⊤V
and
E Approx=E (cid:10) U2+VV⊤,H1/2w∗ w∗ H1/2(cid:11) +E (cid:10) W2+V⊤V,H1/2w∗ w∗ H1/2 (cid:11)
w∗ w∗ 0:k 0:k 0:k 0:k w∗ 0:k k:∞ k:∞ k:∞
+2E (cid:10) UV+VW,H1/2w∗ w∗ H1/2 (cid:11)
w∗ 0:k 0:k k:∞ k:∞
=tr((U2+VV⊤)H )+tr((W2+V⊤V)H ),
0:k k:∞
wherethelastlineusesthefactthatE (w∗)⊗2 =I .UsingEq.(13)and(16)intheproofofLemmaC.1,wefurther
w∗ d
obtain
E Approx=tr(H−1/2[H−1 +S⊤ A−1S ]−1H−1/2H )−tr(WH )
w∗ 0:k 0:k 0:k k 0:k 0:k 0:k k:∞
=tr([H−1 +S⊤ A−1S ]−1)−tr(WH )
0:k 0:k k 0:k k:∞
≥−tr(WH )=:T .
k:∞ 3
whereA :=S H S⊤ .ForT ,wefurtherhave
k k:∞ k:∞ k:∞ 3
T =tr(H1/2 [I −H1/2 S⊤ A−1S H1/2 ]H1/2 )
3 k:∞ d−k k:∞ k:∞ k:∞ k:∞ k:∞
≥tr(H1/2 [I −H1/2 S⊤ A−1S H1/2 ]H1/2 )
k:∞ d−k k:∞ k:∞ k k:∞ k:∞ k:∞
d−k
≥(cid:88) µ (I −H1/2 S⊤ A−1S H1/2 )·µ (H ),
i d−k k:∞ k:∞ k k:∞ k:∞ d+1−k−i k:∞
i=1
wherethesecondlineisduetoA ⪰ A (andhence−A−1 ⪰ −A−1 ),thethirdlinefollowsfromVon-Neuman’s
k k
inequality. SinceM:=I −H1/2 S⊤ A−1S H1/2 isaprojectionmatrixsuchthatM2 =Mandtr(I −
d−k k:∞ k:∞ k k:∞ k:∞ d−k
M)=M,itfollowsthatMhasM eigenvalues0andd−k−M eigenvalues1. Therefore,wefurtherhave
d−k d
(cid:88) (cid:88)
T ≥ µ (M)·µ (H )≥ λ
3 i d+1−k−i k:∞ i
i=1 i=k+M
foranyk ≤d. Lettingk =0maximizesthelowerboundandconcludestheproof.
C.3 AlowerboundunderAssumption3
LemmaC.3(LowerboundontheapproximationerrorunderAssumption3). UnderAssumption3,theapproximation
errorin(4)(and7)satisfies
d
(cid:88)
E Approx≳ λ ia−b,
w∗ i
i=M
where(λ )d areeigenvaluesofHinnon-increasingorderandtheinequalityhidessome(a,b)-dependentconstant.
i i=1
23ProofofLemmaC.3. TheproofisessentiallythesameastheproofofLemmaC.2butweincludeithereforcom-
pleteness. Let Hw := E[w∗w∗⊤] be the covariance of the prior on w∗. Following the proof of Lemma C.2, we
have
E Approx=E (cid:10) U2+VV⊤,H1/2w∗ w∗ H1/2(cid:11) +E (cid:10) W2+V⊤V,H1/2w∗ w∗ H1/2 (cid:11)
w∗ w∗ 0:k 0:k 0:k 0:k w∗ 0:k k:∞ k:∞ k:∞
+2E (cid:10) UV+VW,H1/2w∗ w∗ H1/2 (cid:11)
w∗ 0:k 0:k k:∞ k:∞
=tr((U2+VV⊤)H Hw )+tr((W2+V⊤V)H Hw ),
0:k 0:k k:∞ k:∞
wherethelastlineusesAssumption3andnoticethatH,Hwarebothdiagonal.Next,similartotheproofofLemmaC.2,
usingEq.(13)and(16),wederive
E Approx=tr(H−1/2[H−1 +S⊤ A−1S ]−1H−1/2H Hw )−tr(WH Hw )
w∗ 0:k 0:k 0:k k 0:k 0:k 0:k 0:k k:∞ k:∞
=tr([H−1 +S⊤ A−1S ]−1Hw )−tr(WH Hw )
0:k 0:k k 0:k 0:k k:∞ k:∞
≥−tr(WH Hw )=:T˜
k:∞ k:∞ 3
whereA :=S H S⊤ . ForT˜ ,followingthesameargumentforT intheproofofLemmaC.2,wehave
k k:∞ k:∞ k:∞ 3 3
d−k
T˜ ≥(cid:88) µ (I −H1/2 S⊤ A−1S H1/2 )·µ (H Hw )
3 i d−k k:∞ k:∞ k k:∞ k:∞ d+1−k−i k:∞ k:∞
i=1
d d
(cid:88) (cid:88)
≥ µ (HHw)≳ ia−bλ ,
i i
i=k+M i=k+M
foranyk ≤dwherethelastinequalityusesAssumption3. Settingk =0maximizesthelowerboundandconcludes
theproof.
C.4 ExamplesonmatchingboundsforApprox
In this section, we derive matching upper and lower bounds for Approx (defined in Eq. 4 and 7) in three concrete
examples: power-lawspectrum(LemmaC.4),power-lawspectrumwithsourcecondition(LemmaC.5)andlogarithmic
power-lawspectrum(LemmaD.5).
Lemma C.4 (Bounds on Approx under the power-law spectrum). Suppose Assumption 1C and 2 hold. Then with
probabilityatleast1−e−Ω(M)overtherandomnessofS
M1−a ≲E Approx≲M1−a.
w∗
Here,thehiddenconstantsonlydependonthepower-lawdegreea.
ProofofLemmaC.4. Fortheupperbound,byLemmaC.1andnotingEw∗2 =1foralli,wehavewithprobabilityat
i
least1−e−Ω(M)
(cid:115)
E Approx≲ (cid:88) λ +(cid:18)(cid:80) i>k1λ i +λ + (cid:80) i>k1λ2 i(cid:19) ·k
w∗ i M k1+1 M 1
k>k1
(cid:115)
(cid:18) k1−a k1−2a(cid:19)
≲k1−a+ 1 +k−a+ 1 k
1 M 1 M 1
(cid:18) (cid:19)
k
≲ 1 +1 k1−a
M 1
foranygivenk ≤ M/2. Herethehiddenconstantsdependona. Therefore,lettingk = M/2intheupperbound
1 1
yields
E Approx≲M1−a
w∗
24withprobabilityatleast1−e−Ω(M).
Forthelowerbound,wehavefromLemmaC.2that
∞
(cid:88)
E Approx≳ i−a ≳M1−a.
w∗
i=M
Thiscompletestheproof.
LemmaC.5(BoundsonApproxunderthesourcecondition). SupposeAssumption3hold. Thenwithprobabilityat
least1−e−Ω(M)overtherandomnessofS
M1−b ≲E Approx≲M1−b.
w∗
Here,thehiddenconstantsonlydependonthepower-lawdegreesa,b.
ProofofLemmaC.5. Fortheupperbound,byLemmaC.1andnotingEw∗2 ≂ia−bforalli,wehavewithprobability
i
atleast1−e−Ω(M)
(cid:115)
Approx≲ (cid:88) λ ia−b+(cid:18)(cid:80) i>k1λ i +λ + (cid:80) i>k1λ2 i(cid:19) ·k1+a−b
i M k1+1 M 1
k>k1
(cid:115)
(cid:18) k1−a k1−2a(cid:19)
≲k1−b+ 1 +k−a+ 1 k1+a−b
1 M 1 M 1
(cid:18) (cid:19)
k
≲ 1 +1 k1−b
M 1
foranygivenk ≤M/2. Herethehiddenconstantsdependona,b. Moreover,choosingk =M/2intheupperbound
1 1
gives
E Approx≲M1−b
w∗
withprobabilityatleast1−e−Ω(M).
Forthelowerbound,wehavefromLemmaC.3that
∞
(cid:88)
E Approx≳ i−a·ia−b ≳M1−b.
w∗
i=M
Thiscompletestheproof.
LemmaC.6(BoundsonApproxunderthelogarithmicpower-lawspectrum). SupposeAssumption4hold. Thenwith
probabilityatleast1−e−Ω(M)overtherandomnessofS
log1−aM ≲E Approx≲log1−aM.
w∗
Here,thehiddenconstantsonlydependonthepower-lawdegreea.
ProofofLemmaC.6. Fortheupperbound,byLemmaC.1andnotingEw∗2 =1foralli,wehavewithprobabilityat
i
least1−e−Ω(M)
(cid:115)
Approx≲ (cid:88) λ +(cid:18)(cid:80) i>k1λ i +λ + (cid:80) i>k1λ2 i(cid:19) k
i M k1+1 M 1
k>k1
(cid:115)
(cid:18) log1−ak k1−2a(cid:19)
≲log1−ak + 1 +k−1log−ak + 1 k
1 M 1 1 M 1
(cid:18) (cid:114) (cid:19)
k 1 1 k
≲ 1+ 1 + + 1 log1−ak
M logk logk M 1
1 1
≲log1−ak
1
25foranygivenk ≤M/2,wherethethirdlineuses(cid:80) λ2 ≲1/(k log2ak ). Choosingk =M/2,weobtain
1 i>k1 i 1 1 1
E Approx≲log1−aM
w∗
withprobabilityatleast1−e−Ω(M). Herethehiddenconstantsdependona,b.
Forthelowerbound,wehavefromLemmaC.2that
∞ ∞
(cid:88) (cid:88)
E Approx≳ λ ≳ i−1log−ai≳log1−aM.
w∗ i
i=M i=M
Therefore,wehaveestablishedmatchingupperandlowerboundsforApprox.
D Bias error
Inthissection,wederiveupperandlowerboundsforBias(w∗)definedinEq.(5). Moreover,weshowthattheupper
andlowerboundsmatchuptoconstantfactorsinconcreteexamples.
D.1 Anupperbound
LemmaD.1(Upperboundonthebiasterm). Supposetheinitialstepsizeγ ≤ 1 forsomeconstantc > 1.
ctr(SHS⊤)
Thenforanyw∗ ∈Handk ∈[d]suchthatr(H)≥k+M,thebiastermin(5)satisfies
1
Bias(w∗)≲ ∥v∗∥2.
N γ 2
eff
Moreover,foranyk ≤M/3suchthatr(H)≥k+M,thebiastermsatisfies
(cid:34) (cid:35)2
∥w∗ ∥2 µ (A )
Bias(w∗)≲ 0:k 2 · M/2 k +∥w∗ ∥2
N γ µ (A ) k:∞ Hk:∞
eff M k
with probability 1−e−Ω(M), where A := S H S⊤ , {µ (A )}M denote the eigenvalues of A in non-
k k:∞ k:∞ k:∞ i k i=1 k
increasingorderforsomeconstantc>1.
ProofofLemmaD.1. SimilartotheproofofLemmaC.1,wecanwithoutlossofgeneralityassumethecovariance
matrixH=diag{λ ,λ ,...,λ }whereλ ≥λ foranyi≥j. LetSH1/2 =U˜ (cid:0) Λ˜1/2 0(cid:1) V˜⊤bethesingularvalue
1 2 d i j
decompositionofSHS⊤,whereΛ˜ :=diag{λ˜ ,λ˜ ,...,λ˜ }isadiagonalmatrixdiagonalentriesinnon-increasing
1 2 d
order. DefineA :=S H S⊤ . ThenitfollowsfromsimilarargumentsasinLemmaC.1thatA isinvertible.
k k:∞ k:∞ k:∞ k
Since
λ˜
∥γ SHS⊤∥ =γ λ˜ ≤γλ˜ ≤ 1 ≤1
t 2 t 1 1 c(cid:80)M λ˜
i=1 i
forsomeconstantc>1bythestepsizeassumption,itfollowsthatI −γ SHS⊤ ≻0 forallt∈[N]. Therefore,it
M t M
canbeverifiedthat
N N
(cid:89) (cid:89)
(I
M
−γ tSHS⊤)SHS⊤ (I
M
−γ tSHS⊤)⪯(I
M
−γSHS⊤)NeffSHS⊤(I
M
−γSHS⊤)Neff =:M,
t=1 t=1
andbydefinitionofBias(w∗)inEq.(5),wehave
(cid:13) N (cid:13)2 (cid:13) (cid:13)2
Bias(w∗)≂(cid:13) (cid:13)(cid:89)(cid:0)
I−γ
tSHS⊤(cid:1) v∗(cid:13)
(cid:13)
≤(cid:13) (cid:13)(cid:0) I−γSHS⊤(cid:1)Neffv∗(cid:13)
(cid:13)
(cid:13) (cid:13) (cid:13) (cid:13)
t=1 SHS⊤ SHS⊤
=⟨M,v∗⊗2⟩. (18)
26NotethattheeigenvaluesofMare{λ˜ i(1−γλ˜ i)2Neff}M i=1. Sincethefunctionf(x)=x(1−γx)2Neff ismaximizedat
x =1/[(2N +1)γ]forx∈[0,1/γ]withf(x )≲1/(N γ),itfollowsthat
0 eff 0 eff
∥M∥ ≤c/(N γ) (19)
2 eff
forsomeconstantc>0. ThefirstpartofLemmaD.1followsimmediately.
NowweprovethesecondpartofLemmaD.1. Recallthatv∗ =(cid:0) SHS⊤(cid:1)−1 SHw∗. Substituting
(cid:0) (cid:1)
SH= S H S H
0:k 0:k k:∞ k:∞
intov∗,weobtain
⟨M,v∗⊗2⟩=⟨M,((SHS⊤)−1SHw∗)⊗2⟩
=w∗⊤HS⊤(SHS⊤)−1M(SHS⊤)−1SHw∗
≤2T +2T ,
1 2
where
T :=(w∗ )⊤H S⊤ (SHS⊤)−1M(SHS⊤)−1S H w∗ , (20)
1 0:k 0:k 0:k 0:k 0:k 0:k
T :=(w∗ )⊤H S⊤ (SHS⊤)−1M(SHS⊤)−1S H w∗ . (21)
2 k:∞ k:∞ k:∞ k:∞ k:∞ k:∞
Weclaimthefollowingresultswhichweprovelater. Withprobability1−e−Ω(M)
(cid:34) (cid:35)2
c∥w∗ ∥2 µ (A )
T ≤ 0:k 2 · M/2 k (22a)
1 N γ µ (A )
eff M k
forsomeconstantc>0.
T ≤∥w∗ ∥2 . (22b)
2 k:∞ Hk:∞
CombiningEq.(22a),(22b)givesthesecondpartofLemmaD.1.
Proofofclaim(22a) BydefinitionofT ,wehave
1
T ≤∥H S⊤ (SHS⊤)−1M(SHS⊤)−1S H ∥ ·∥w∗ ∥2.
1 0:k 0:k 0:k 0:k 2 0:k 2
Moreover,
∥H S⊤ (SHS⊤)−1M(SHS⊤)−1S H ∥
0:k 0:k 0:k 0:k 2
≤∥M∥ ·∥(SHS⊤)−1S H ∥2
2 0:k 0:k 2
c
≤ ∥(SHS⊤)−1S H ∥2
N γ 0:k 0:k 2
eff
forsomeconstantc>0,wherethelastlineusesEq.(19).
Itremainstoshow
µ (A )
∥(SHS⊤)−1S H ∥ ≤c· M/2 k (23)
0:k 0:k 2 µ (A )
M k
forsomeconstantc>0withprobability1−e−Ω(M). SinceSHS⊤ =S H S⊤ +A ,wehave
0:k 0:k 0:k k
(SHS⊤)−1S H =(A−1−A−1S [H−1 +S⊤ A−1S ]−1S⊤ A−1)S H
0:k 0:k k k 0:k 0:k 0:k k 0:k 0:k k 0:k 0:k
=A−1S H −A−1S [H−1 +S⊤ A−1S ]−1S⊤ A−1S H
k 0:k 0:k k 0:k 0:k 0:k k 0:k 0:k k 0:k 0:k
=A−1S [H−1 +S⊤ A−1S ]−1H−1H
k 0:k 0:k 0:k k 0:k 0:k 0:k
=A−1S [H−1 +S⊤ A−1S ]−1, (24)
k 0:k 0:k 0:k k 0:k
27wherethesecondlineusesWoodbury’sidentity. Since
H−1 +S⊤ A−1S ⪰S⊤ A−1S ,
0:k 0:k k 0:k 0:k k 0:k
itfollowsthat
∥[H−1 +S⊤ A−1S ]−1∥ ≤∥[S⊤ A−1S ]−1∥ .
0:k 0:k k 0:k 2 0:k k 0:k 2
Therefore,withprobabilityatleast1−e−Ω(M)
∥A−1S [H−1 +S⊤ A−1S ]−1∥ ≤∥A−1∥ ·∥S ∥ ·∥[H−1 +S⊤ A−1S ]−1∥
k 0:k 0:k 0:k k 0:k 2 k 2 0:k 2 0:k 0:k k 0:k 2
≤∥A−1∥ ·∥S ∥ ·∥[S⊤ A−1S ]−1∥
k 2 0:k 2 0:k k 0:k 2
∥A−1∥ ·∥S ∥ ∥A−1∥
≤ k 2 0:k 2 ≲ k 2
µ (S⊤ A−1S ) µ (S⊤ A−1S )
min 0:k k 0:k min 0:k k 0:k
(cid:113)
where the last inequality follows from the fact that ∥S ∥ = ∥S⊤ S ∥ ≤ c for some constant c > 0 when
0:k 2 0:k 0:k 2
k ≤ M/2 with probability at least 1−e−Ω(M). Since S is independent of A and the distribution of S is
0:k k 0:k
rotationally invariant, we may write S⊤ A−1S = (cid:80)M 1 ˜s ˜s⊤, where ˜s i ∼id N(0,I /M) and (λˆ )M are
0:k k 0:k i=1 λˆ
M−i
i i i k i i=1
eigenvaluesofA innon-increasingorder. Therefore,fork ≤M/3
k
M M/2 M/2
S⊤ A−1S =(cid:88) 1 ˜s ˜s⊤ ⪰ (cid:88) 1 ˜s ˜s⊤ ⪰ 1 (cid:88) ˜s ˜s⊤ ⪰ cI k (25)
0:k k 0:k λˆ i i λˆ i i λˆ i i λˆ
i=1 M−i i=1 M−i M/2 i=1 M/2
forsomeconstantc > 0withprobabilityatleast1−e−Ω(M),whereinthelastlineweagainusetheconcentration
propertiesofGaussiancovariancematrices(seee.g.,Theorem6.1inWainwright(2019)). Asadirectconsequence,we
have
µ (A )
∥A−1S [H−1 +S⊤ A−1S ]−1∥ ≤c· M/2 k
k 0:k 0:k 0:k k 0:k 2 µ (A )
M k
withprobabilityatleast1−e−Ω(M)forsomeconstantc>0.Thisconcludestheproof.
Proofofclaim(22b) BydefinitionofT inEq.(21),wehave
2
T
2
=w k∗ :∞⊤H k:∞S⊤ k:∞(SHS⊤)−1/2(I
M
−γSHS⊤)2Neff(SHS⊤)−1/2S k:∞H k:∞w k∗
:∞
≤w∗ ⊤H S⊤ (SHS⊤)−1S H w∗
k:∞ k:∞ k:∞ k:∞ k:∞ k:∞
≤∥H1/2 S⊤ (SHS⊤)−1S H1/2 ∥·∥w∗ ∥2
k:∞ k:∞ k:∞ k:∞ k:∞ Hk:∞
≤∥w∗ ∥2 ,
k:∞ Hk:∞
wherethelastlinefollowsfrom
∥H1/2 S⊤ (SHS⊤)−1S H1/2 ∥ =∥H1/2 S⊤ (S H S⊤ +S H S⊤ )−1S H1/2 ∥
k:∞ k:∞ k:∞ k:∞ 2 k:∞ k:∞ 0:k 0:k 0:k k:∞ k:∞ k:∞ k:∞ k:∞ 2
≤∥H1/2 S⊤ A−1S H1/2 ∥ ≤1.
k:∞ k:∞ k k:∞ k:∞ 2
D.2 Alowerbound
LemmaD.2(Lowerboundonthebiasterm). Supposew∗ followssomepriordistributionandtheinitialstepsize
γ ≤ 1 forsomeconstantc>2. LetHw :=Ew∗w∗⊤. ThenthebiasterminEq.(5)satisfies
ctr(SHS⊤)
E Bias(w∗)≳
(cid:88) µ i(SHHwHS⊤)
w∗ µ (SHS⊤)
i
i:λ˜ i<1/(γNeff)
almostsurely,whereM
:=SHS⊤(I−2γSHS⊤(cid:1)2Neff.
N
28ProofofLemmaD.2. AdoptthenotationsintheproofofLemmaD.1. Bydefinitionofthebiasterm,wehave
(cid:13) N (cid:13)2
Bias(w∗)≂(cid:13) (cid:13)(cid:89)(cid:0)
I−γ
tSHS⊤(cid:1) v∗(cid:13)
(cid:13)
(cid:13) (cid:13)
t=1 SHS⊤
N
=⟨SHS⊤(cid:89)
(I−γ
SHS⊤(cid:1)2Neff,v∗⊗2⟩
t
t=1
N
≥⟨SHS⊤(I−(cid:88)
γ
SHS⊤(cid:1)2Neff,v∗⊗2⟩
t
t=1
≥⟨SHS⊤(I−2γSHS⊤(cid:1)2Neff,v∗⊗2⟩=:⟨M ,v∗⊗2⟩, (26)
N
wherethethirdlineusesI −2γ SHS⊤ ≻0 forallt∈[N]establishedintheproofofLemmaD.1,(cid:80)N γ ≤
M t M i=1 i
2γN ,andthefactthat(1−w)(1−v)≥1−w−vforw,v >0. Substitutingthedefinitionofv∗inEq.(5)intothe
eff
expression,weobtain
E Bias(w∗)≳E ⟨M ,v∗⊗2⟩=E ⟨M ,((SHS⊤)−1SHw∗)⊗2⟩
w∗ w∗ N w∗ N
=tr(HS⊤(SHS⊤)−1M (SHS⊤)−1SHHw)
N
=tr((SHS⊤)−1M (SHS⊤)−1SHHwHS⊤)
N
M
(cid:88)
≥ µ ((SHS⊤)−1M (SHS⊤)−1)·µ (SHHwHS⊤),
M−i+1 N i
i=1
wherethelastlineusesVonNeumann’straceinequality. Continuingthecalculation,wehave
E
Bias(w∗)≳(cid:88)M µ i(SHHwHS⊤)
w∗ µ ((SHS⊤)2M −1)
i=1 i N
=(cid:88)M µ i(SHHwHS⊤)
µ
(cid:16) (SHS⊤)(I−2γSHS⊤(cid:1)−2Neff(cid:17)
i=1 i
≳
(cid:88) µ i(SHHwHS⊤)
,
µ (SHS⊤)
i
i:λ˜ i<1/(γNeff)
wherethefirstinequalityusesµ (A)=µ−1(A−1)foranypositivedefinitematrixA∈RM×M,andthesecond
M+i−1 i
linefollowsfromthedefinitionofM Nandthefactthat(1−λγN eff)−2Neff ≲1whenλ<1/(γN eff).
D.3 ExamplesonmatchingboundsforBias(w∗)
In this section, we derive matching upper and lower bounds for Bias(w∗) in (5) in three scenarios: power-law
spectrum(LemmaD.3),power-lawspectrumwithsourcecondition(LemmaD.4)andlogarithmicpower-lawspectrum
(LemmaD.5). RecallthatwedefineBias:=E Bias(w∗).
w∗
LemmaD.3(BoundsonBiasunderthepower-lawspectrum). SupposeAssumption1Cand2holdandtheinitial
stepsizeγ ≤ 1 forsomeconstantc>2. Thenwithprobabilityatleast1−e−Ω(M)overtherandomnessofS
ctr(SHS⊤)
E Bias(w∗)≲max(cid:8) (N γ)1/a−1, M1−a(cid:9) ,
w∗ eff
and
E Bias(w∗)≳(N γ)1/a−1
w∗ eff
when(N γ)1/a ≤ M/cforsomeconstantc > 0. Here,allthe(hidden)constantsdependonlyonthepower-law
eff
degreea.
29ProofofLemmaD.3. Fortheupperbound,usingLemmaG.5,D.1andtheassumptionthatEw∗2 = 1foralli > 0,
i
withprobabilityatleast1−e−Ω(M),wehave
(cid:34) (cid:35)
∥w∗ ∥2
E Bias(w∗)≲E 0:k2 2 +∥w∗ ∥2
w∗ w∗ N effγ k2:∞ Hk2:∞
≲ k 2 + (cid:88) λ
N γ i
eff
k>k2
k
≂ 2 +k1−a
N γ 2
eff
≲max(cid:8) (N γ)1/a−1, M1−a(cid:9) ,
eff
whereinthelastinequality,wechoosek =[M/3]∧(N γ)1/atominimizetheupperbound.
2 eff
When(N γ)1/a ≤M/3,combiningLemmaD.2andG.4givesthelowerbound
eff
E Bias(w∗)≳
(cid:88) µ i(SHHwHS⊤)
=
(cid:88) µ i(SH2S⊤)
,
w∗ µ (SHS⊤) µ (SHS⊤)
i i
i:λ˜ i<1/(Neffγ) i:λ˜ i<1/(Neffγ)
(cid:88) i−2a (cid:88)
≳ = i−a ≳(N γ)1/a−1
i−a eff
λ˜ i<1/(Neffγ),i≤M λi<1/(Neffγ),i≤M
withprobabilityatleast1−e−Ω(M). Here,thehiddenconstantsdependonlyona.
Lemma D.4 (Bounds on Bias under the source condition). Suppose Assumption 3 hold and the initial stepsize
γ ≤ 1 forsomeconstantc>2. Thenwithprobabilityatleast1−e−Ω(M)overtherandomnessofS
ctr(SHS⊤)
E Bias(w∗)≲max(cid:8) (N γ)(1−b)/a, M1−b(cid:9) ,
w∗ eff
and
E Bias(w∗)≳(N γ)(1−b)/a
w∗ eff
when(N γ)1/a ≤M/cforsomeconstantc>0. Moreover,whenb≥a+1,withprobabilityatleast1−e−Ω(M)
eff
overtherandomnessofS
E Bias(w∗)≲logN ·max(cid:8) (N γ)(1−b)/a, M1−b(cid:9)
w∗ eff eff
Inallresults,thehiddenconstantsdependonlyona,b.
ProofofLemmaD.4. Fortheupperbound,usingLemmaG.5,D.1andtheassumptionthat(w.l.o.g.) Ew∗2 ≂ia−bfor
i
alli>0,withprobabilityatleast1−e−Ω(M),wehave
(cid:34) (cid:35)
∥w∗ ∥2
E Bias(w∗)≲E 0:k2 2 +∥w∗ ∥2
w∗ w∗ N effγ k2:∞ Hk2:∞
k1+a−b
(cid:88)
≲ 2 + λ ·ia−b
N γ i
eff
k>k2
k1+a−b
≲ 2 +k1−b
N γ 2
eff
≲max(cid:8) (N γ)(1−b)/a, M1−b(cid:9)
eff
whenb<a+1,whereinthelastinequality,wechoosek =[M/3]∧(N γ)1/atominimizetheupperbound.
2 eff
30When(N γ)1/a ≤M/cforsomelargeconstantc>0,combiningLemmaD.2andG.4yieldsthelowerbound
eff
E Bias(w∗)≳
(cid:88) µ i(SHHwHS⊤)
≂
(cid:88) µ i(SH(a+b)/aS⊤)
,
w∗ µ (SHS⊤) µ (SHS⊤)
i i
i:λ˜ i<1/(Neffγ) i:λ˜ i<1/(Neffγ)
(cid:88) i−(a+b) (cid:88)
≳ = i−b ≳(N γ)(1−b)/a
i−a eff
λ˜ i<1/(Neffγ),i≤M λi<1/(Neffγ),i≤M
withprobabilityatleast1−e−Ω(M). Here,thehiddenconstantsdependonlyona,b.
Upperboundwhenb ≥ a+1. Followingthepreviousderivations,whenb = a+1,wehavewithprobabilityat
least1−e−Ω(M)
(cid:34) (cid:35)
∥w∗ ∥2
E Bias(w∗)≲E 0:k2 2 +∥w∗ ∥2
w∗ w∗ N effγ k2:∞ Hk2:∞
logk
≲ 2 +k1−b
N γ 2
eff
log(N γ)
≲ eff +M1−b
N γ
eff
wherethelastlinefollowsbysettingk =[M/3]∧(N γ)1/a. Whenb>a+1,wehavewithprobabilityatleast
2 eff
1−e−Ω(M)
(cid:34) (cid:35)
∥w∗ ∥2
E Bias(w∗)≲E 0:k2 2 +∥w∗ ∥2
w∗ w∗ N effγ k2:∞ Hk2:∞
1
≲ +k1−b
N γ 2
eff
1
≲ +M1−b,
N γ
eff
wherethelastfollowsbychooingk =M/3tominimizetheupperbound.
2
Notethatthereexistnon-constantgapsbetweentheupperandlowerboundsonthebiasterm(plustheapproximation
errorterm,LemmaC.5)inthesimpleregimewhereb≥a+1. Weleaveamorepreciseanalysisofthebiastermfor
futurework.
LemmaD.5(BoundsonBiasunderthelogarithmicpower-lawspectrum). SupposeAssumption4holdandtheinitial
stepsizeγ ≤ 1 forsomeconstantc>2. Let¯k:=inf{k :klogak ≥N γ}. Thenwithprobabilityatleast
ctr(SHS⊤) eff
1−e−Ω(M)overtherandomnessofS
E Bias(w∗)≲max(cid:8) log1−a(N γ),log1−aM(cid:9) ,
w∗ eff
and
E Bias(w∗)≳log1−a(N γ)
w∗ eff
when(N γ) ≤ Mc forsomesufficientlysmallconstantc > 0. Here,allconstantsdependonlyonthepower-law
eff
degreea.
31ProofofLemmaD.5. Fortheupperbound,usingLemmaG.7,D.1andtheassumptionthatEw∗2 = 1foralli > 0,
i
withprobabilityatleast1−e−Ω(M),wehave
(cid:34) (cid:35)
∥w∗ ∥2
E Bias(w∗)≲E 0:k2 2 +∥w∗ ∥2
w∗ w∗ N effγ k2:∞ Hk2:∞
≲ k 2 + (cid:88) λ
N γ i
eff
k>k2
k
≂ 2 +log1−ak
N γ 2
eff
≲max(cid:8) log1−a(N γ),log1−aM(cid:9) ,
eff
whereinthelastinequality,wechoosek =[M/3]∧(cid:2) (N γ)/loga(N γ)(cid:3) tominimizetheupperbound.
2 eff eff
Recall k∗ ≂ M/logM (for example we may define k∗ = inf{k : klogk ≥ M}) in Lemma G.6. Combining
LemmaD.2andG.6givesthelowerbound
E Bias(w∗)≳
(cid:88) µ i(SHHwHS⊤)
≂
(cid:88) µ i(SH2S⊤)
,
w∗ µ (SHS⊤) µ (SHS⊤)
i i
i:λ˜ i<1/(Neffγ) i:λ˜ i<1/(Neffγ)
(cid:88)
i−2log−2ai
(cid:88)
≳ = i−1log−ai
i−1log−ai
λ˜ i<1/(Neffγ),i≤k∗ λi<1/(Neffγ),i≤k∗
k∗
(cid:88)
≳ i−1log−ai
i=Neffγ
≳log1−a(N γ)−log1−a(k∗)
eff
≳log1−a(N γ)−c log1−a(M)
eff 1
withprobabilityatleast1−e−Ω(M)forsomeconstantc >0.Here,the(hidden)constantsdependonlyona.Therefore,
1
when(N γ)1/a ≤Mcforsomesufficientlysmallconstantc>0,wehave
eff
E Bias(w∗)≳log1−a(N γ)−c log1−a(M)≳log1−a(N γ).
w∗ eff 1 eff
withprobabilityatleast1−e−Ω(M).
E Variance error
Inthissection,wepresentmatchingupperandlowerboundsonthevariancetermVardefinedin(6)underthepower-law
orlogarithmicpower-lawspectrum.
LemmaE.1(MatchingboundsonVarunderpower-lawspectrum). UnderAssumption2,VardefinedinEq.(6)satisfies
min{M, (N γ)1/a}
Var≂ eff
N
eff
withprobabilityatleast1−e−Ω(M)overtherandomnessofS. Here,thehiddenconstantsonlydependona.
ProofofLemmaE.1. BythedefinitionofVarinEq.(6)andLemmaG.4,wehave
#{λ˜ ≥1/(N γ)}+(N γ)2(cid:80) λ˜2
Var=
j eff eff λ˜ j<1/(Neffγ) j
N
eff
min(cid:8) M, (N γ)1/a+(N γ)2·(N γ)(1−2a)/a(cid:9)
≂ eff eff eff
N
eff
min{M, (N γ)1/a}
≂ eff
N
eff
32withprobabilityatleast1−e−Ω(M)overtherandomnessofS. Herethehiddenconstantsmaydependona.
LemmaE.2(MatchingboundsonVarunderlogarithmicpower-lawspectrum). UnderAssumption4,Vardefinedin
Eq.(6)satisfies
min{M, ¯k} min{M, (N γ)/loga(N γ)}
Var≂ ≂ eff eff
N N
eff eff
withprobabilityatleast1−e−Ω(M)overtherandomnessofS,where¯k:=inf{k :klogak ≥(N γ)}and≂hides
eff
constantsthatonlydependona.
ProofofLemmaE.2. Definek∗ =inf{k :klogk ≥M}andlet
D˜ :=#{λ˜ ≥1/(N γ)}+(N γ)2 (cid:88) λ˜2.
j eff eff j
λ˜ j<1/(Neffγ)
BythedefinitionofVarin(6)andLemmaG.6,wehave
#{λ˜ ≥1/(N γ)}+(N γ)2(cid:80) λ˜2
Var=
j eff eff λ˜ j<1/(Neffγ) j
N
eff
D˜ min{M,¯k}
= ≂
N N
eff eff
withprobabilityatleast1−e−Ω(M)overtherandomnessofS,wherethesecondlinefollowsfrom
N γ
D˜ ≳#{λ˜ ≥1/(N γ)}≂ eff , and
j eff loga(N γ)
eff
D˜ ≲ N effγ + (N effγ)2 · (cid:88) 1 ≲ N effγ
loga(N γ) log2a(N γ) j2 loga(N γ)
eff eff j:λ˜ j<1/(Neffγ) eff
when¯k≲M.
F Expected risk of the average of (SGD) iterates
Inthissection,westudytheexpectedriskoftheaverageof(SGD)iterates. Namely,weconsiderafixedstepsize (SGD)
procedurewhereγ = γ anddefinev¯ :=
(cid:80)N−1v
/N. Ourgoalistoderivematchingupperandlowerbounds
t N i=0 i
R(v¯ )intermsofthesamplesizeN andmodelsizeM.Comparedwiththelastiterateof(SGD)withgeometrically
N
decayingstepsizes,weshowthattheaverageof(SGD)iterateswithafixedstepsizeachievesabetterrisk,inthesense
thattheeffectivesamplesizeN isreplacedbyN inthebounds(c.f. Theorem4.1). Thismaygiveimprovementup
eff
tologarithmicfactors.
WestartwithinvokingthefollowingresultinZouetal.(2023).
TheoremF.1(AvariantofTheorem2.1and2.2in(Zouetal.,2023)). SupposeAssumption1hold. Consideran
M-dimensional sketched predictor trained by fixed stepsize (SGD) with N samples. Let v¯ :=
(cid:80)N−1v
/N be
N i=0 i
the average of the iterations of SGD. Assume v = 0 and σ2 ≳ 1. Conditional on S and suppose the stepsize
0
γ <1/(ctr(SHS⊤))forsomeconstantc>0,thenthereexistApprox,Bias,Varsuchthat
ER (v¯ )−σ2 ≂E Approx+Bias+σ2Var,
M N w∗
wheretheexpectationofR isoverw∗and(x ,y )N and
M i i i=1
Approx:=Eξ2
=(cid:13) (cid:13) (cid:13)(cid:16) I−H21S⊤(cid:0) SHS⊤(cid:1)−1 SH21(cid:17) H1 2w∗(cid:13) (cid:13) (cid:13)2 ,
33E (T +T )≲Bias≲E (T +T ),
w∗ 1 3 w∗ 2 4
D
Var≂ eff,N,
N
and
1 (cid:16)(cid:16) (cid:17)2 (cid:17)
T := tr I−(I−γSHS⊤)N/4 (SHS⊤)−1B , (27a)
1 γ2N2 0
1 (cid:16)(cid:16) (cid:17)2 (cid:17)
T := tr I−(I−γSHS⊤)N (SHS⊤)−1B , (27b)
2 γ2N2 0
T :=
1 tr(cid:16)(cid:16) I−(I−γSHS⊤)N/4(cid:17)
B
(cid:17) ·tr(cid:16)(cid:0) I−(I−γSHS⊤)N/4(cid:1)2(cid:17)
, (27c)
3 γN2 0
T := 1 tr(cid:0) B −(I−γSHS⊤)NB (I−γSHS⊤)N(cid:1) · D eff,N, (27d)
4 γN 0 0 N
B :=v∗v∗⊤, (27e)
0
D :=#{λ˜ ≥1/(Nγ)}+(Nγ)2 (cid:88) λ˜2, (27f)
eff,N j j
λ˜ j<1/(Nγ)
where(λ˜ )M areeigenvalueofSHS⊤.
j j=1
SeeSectionF.2.1fortheproof.
ForT (i=1,2,3,4),wealsohavethefollowingupper(andlower)bounds.
i
LemmaF.2(LowerboundonT ). UndertheassumptionsandnotationsinTheoremF.1,wehave
1
E T ≳
(cid:88) µ i(SH2S⊤)
w∗ 1 µ (SHS⊤)
i
i:λ˜ i<1/(γN)
almostsurely,where(λ˜ )N areeigenvaluesofSHS⊤innon-increasingorder.
i i=1
SeetheproofinSectionF.2.2.
LemmaF.3(UpperboundonT ). UndertheassumptionsandnotationsinTheoremF.1,foranyk ≤M/3suchthat
2
r(H)≥k+M,wehavewithprobabilityatleast1−e−Ω(M)that
1 (cid:104)µ (A )(cid:105)2
T ≲ M/2 k ·∥w∗ ∥2+∥w∗ ∥2 ,
2 Nγ µ (A ) 0:k k:∞ Hk:∞
M k
whereA :=S H S⊤ .
k k:∞ k:∞ k:∞
SeetheproofinSectionF.2.3.
LemmaF.4(LowerboundonT ). UndertheassumptionsandnotationsinTheoremF.1,wehave
3
E T ≳ D eff,N · (cid:88) µ i(SH2S⊤)
w∗ 3 N µ (SHS⊤)
i
i:λ˜ i<1/(γN)
almostsurely,where(λ˜ )M areeigenvaluesofSHS⊤innon-increasingorder.
i i=1
SeetheproofinSectionF.2.4.
LemmaF.5(UpperboundonT ). UndertheassumptionsandnotationsinTheoremF.1andassumer(H)≥M,we
4
have
D
T ≲∥w∗∥2 · eff,N
4 H N
almostsurely,whereA :=S H S⊤ .
k k:∞ k:∞ k:∞
SeetheproofinSectionF.2.5.
Withtheseresultsathand,wearereadytoderiveupperandlowerboundsfortheriskoftheaverageof(SGD)iterates.
34a=1.5
a=2.0
103
1.30
1.5
1.25
102 1.4
1.20
102
1.3 1.15
1.10
1.2
1.05
101 1.1 101
102 103 104 102 103 104
Number of samples (N) Number of samples (N)
Figure 3: The expected risk (Risk) of the average of iterates of (SGD) versus the sample size N and the model
size M for different power-law degrees a. The expected risk is computed by averaging over 1000 independent
samples of (w∗,S). We fit the expected risk using the formula Risk ∼ σ2 +c 1/Ma1 +c 2/Na2 via minimizing
the Huber loss as in Hoffmann et al. (2022). Parameters: σ = 1,γ = 0.1. Left: For a = 1.5, d = 20000, the
fitted exponents are (a ,a ) = (0.59,0.33) ≈ (0.5,0.33). Right: For a = 2, d = 2000, the fitted exponents
1 2
are (a ,a ) = (1.09,0.49) ≈ (1.0,0.5). Note that the values of (a ,a ) are close to our theoretical predictions
1 2 1 2
(a−1,1−1/a)inbothcases.
F.1 Matchingboundsfortheaverageof(SGD)iteratesunderpower-lawspectrum
Inthissection,wederiveupperandlowerboundsfortheexpectedriskunderthepower-lawspectrum. Ourmainresult
(TheoremF.6)followsdirectlyfromTheoremF.1andtheboundsonT (i=1,2,3,4)inLemmasF.2toF.5.
i
TheoremF.6(ScalinglawforaverageiteratesofSGD). SupposeAssumption1and2holdandσ2 ≲1. Thenthere
existssomea-dependentconstantc>0suchthatwhenγ ≤c,withprobabilityatleast1−e−Ω(M)overtherandomness
ofthesketchmatrixS,wehave
ER (v¯ )=σ2+Θ(cid:0) M1−a(cid:1) +Θ(cid:0) (Nγ)1/a−1(cid:1) ,
M N
wheretheexpectationisovertherandomnessofw∗and(x ,y )N ,andΘ(·)hidesconstantsthatmaydependona.
i i i=1
SeetheproofinSectionF.2.6.
ComparedwithTheorem4.1,TheoremF.6suggeststhattheaverageof (SGD)achievesasmallerriskinthesketched
linearmodel—the(N γ)1/aisreplacedby(Nγ)1/aintheboundforthebiasterm. Thisisintuitivesincethesumof
eff
stepsizes(cid:80) γ ≂N γ forthegeometricallydecayingstepsizeschedulerwhile(cid:80) γ ≂Nγ forthefixedstepsize
t t eff t t
scheduler.
WealsoverifytheobservationsinTheoremF.6viasimulations. WeadoptthesamemodelandsetupasinSection5
butusetheaverageofiteratesoffixedstepsize (SGD)(denotedbyv¯ )asthepredictor. FromFigure3and4wesee
N
thattheexpectedriskER(v¯ )alsoscalesfollowingapower-lawrelationinbothsamplesizeN andmodelsizeM.
N
Moreover,thefittedexponentsmatchourtheoreticalpredictionsinTheoremF.6.
F.2 Proofs
F.2.1 ProofofTheoremF.1
SimilartotheproofofTheoremA.4,wehavethedecomposition
R(v¯ )=σ2+Approx+∥v¯ −v∗∥2 .
N N SHS⊤
Notethat(v )N canalsobeviewedastheSGDiteratesonthemodely =⟨Sx,v∗⟩+ξ+ϵ,wherethenoisesatisfies
t t=1
E(ξ+ϵ)2 =R(v∗)=Eξ2+σ2.
35
)M(
ezis
ledoM
ksiR
)M(
ezis
ledoM
ksiRa=1.5 a=1.5 a=2.0 a=2.0
k R= is- k0.32 4×101 k R= is- k0.50 k R= is- k0.49
101
k R= is- k1.00
3×101 101
101 2×101
102
102 103 104 105 101 102 102 103 104 105 101 102
Number of samples (N) Model size (M) Number of samples (N) Model size (M)
(a)a=1.5 (b)a=1.5 (c)a=2 (d)a=2
Figure4: Theexpectedriskoftheaverageofiteratesof (SGD)minustheirreducibleriskversustheeffectivesample
sizeandmodelsize. Parametersσ =1,γ =0.1. (a),(b): a=1.5,d=10000;(c),(d): a=2,d=1000. Theerrobars
denotesthe±1standarddeviationofestimatingtheexpectedriskusing100independentsamplesof(w∗,S). Weuse
linearfunctionstofittheexpectedriskunderlog-logscaleandreporttheslopeofthefittedlines(denotedbyk).
Therefore,theupperandlowerboundsonBias,VarfollowdirectlyfromtheproofofTheorem2.1,2.2andrelated
lemmas(LemmaB.6,B.11,C.3,C.5)inZouetal.(2023).
F.2.2 ProofofLemmaF.2
ForanypositivedefinitematrixA∈RM×M,let
f (A):=(I−(I−γA)N/4)2A−1/γ2/N2.
1
Sinceγ ≤1/(ctr(SHS⊤)),wehavef (SHS⊤)⪰0.BydefinitionofT andrecallingv∗ =(SHS⊤)−1SHw∗,we
1 1
havewithprobabilityatleast1−e−Ω(M)that
E T =E [w∗⊤HS⊤(SHS⊤)−1f (SHS⊤)(SHS⊤)−1SHw∗]
w∗ 1 w∗ 1
=tr(cid:0) [(SHS⊤)−1f (SHS⊤)(SHS⊤)−1](SH2S⊤)(cid:1) .
1
FollowingtheproofofLemmaD.2(byVonNeumann’straceinequality),wehave
E T
≥(cid:88)M µ i(SH2S⊤)
w∗ 1 (cid:16) (cid:17)
µ (SHS⊤)2f (SHS⊤)−1
i=1 i 1
≥
(cid:88) µ i(SH2S⊤)
(cid:16) (cid:17)
µ (SHS⊤)2f (SHS⊤)−1
i:λ˜ i<1/(γN) i 1
≳
(cid:88) µ i(SH2S⊤)
,
µ (SHS⊤)
i
i:λ˜ i<1/(γN)
wherethethirdinequalityisdueto
λ2γ2N2 N2 1
λ/f (λ)≲ ≲ ≲ ≲1
1 (1−(1−γλ)N/4)2 ((cid:80)N/4−1(1−γλ)i)2 (1−γλ)2N
i=0
whenλ<1/(Nγ).
F.2.3 ProofofLemmaF.3
BydefinitionofT ,thefactthat1−xN =(1−x)(cid:80)N−1xi,andrecallingv∗ =(SHS⊤)−1SHw∗,wehave
2 i=0
T =w∗⊤HS⊤f (SHS⊤)SHw∗,
2 2
≤2[w∗ ⊤H S⊤ f (SHS⊤)S H w∗ +w∗ ⊤H S⊤ f (SHS⊤)S H w∗ ],
0:k 0:k 0:k 2 0:k 0:k 0:k k:∞ k:∞ k:∞ 2 k:∞ k:∞ k:∞
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
T21 T22
36
)(
nim
)Nv(M
E
)(
nim
)Nv(M
E
)(
nim
)Nv(M
E
)(
nim
)Nv(M
Ewheref
(A):=[(cid:80)N−1(I−γA)i]2/A/N2foranysymmetricmatrixA∈RM×M.
Moreover,wehave
2 i=0
T =w∗ ⊤H S⊤ f (SHS⊤)S H w∗
21 0:k 0:k 0:k 2 0:k 0:k 0:k
≤∥f (SHS⊤)(SHS⊤)2∥·∥(SHS⊤)−1S H w∗ ∥2.
2 0:k 0:k 0:k
Usingtheassumptiononthestepsizethatγ ≤1/(ctr(SHS⊤)),wehave
N−1
1 (cid:104) (cid:88) (cid:105)2
∥f (SHS⊤)(SHS⊤)2∥≤ max (1−γλ)i λ
2 λ∈[0,1/γ]N2
i=0
N−1
1 (cid:104) (cid:88) (cid:105)
= max (1−γλ)i ·(1−(1−γλ)N)
λ∈[0,1/γ]N2γ
i=0
1 1
≤ ·N ·1= . (28)
N2γ Nγ
CombiningEq.(28)withEq.(23)intheproofofLemmaD.1(notethatweassumek ≤M/3),weobtain
1 (cid:104)µ (A )(cid:105)2
T ≤c M/2 k ·∥w∗ ∥2
21 Nγ µ (A ) 0:k
M k
forsomeconstantc>0withprobabilityatleast1−e−Ω(M). ForT ,wehave
22
T =w∗ ⊤H S⊤ f (SHS⊤)S H w∗
22 k:∞ k:∞ k:∞ 2 k:∞ k:∞ k:∞
≤∥f (SHS⊤)SHS⊤∥·∥(SHS⊤)−1/2(S H1/2 )H1/2 w∗ ∥2
2 k:∞ k:∞ k:∞ k:∞
≤∥f (SHS⊤)SHS⊤∥·∥(SHS⊤)−1/2S H1/2 ∥2·∥w∗ ∥2 .
2 k:∞ k:∞ k:∞ Hk:∞
Since∥f (SHS⊤)SHS⊤∥=∥[(cid:80)N−1(I−γSHS⊤)i]2/N2∥≤1bytheassumptionγ ≤1/(ctr(SHS⊤)),and
2 i=0
∥(SHS⊤)−1/2S H1/2 ∥2 =∥H1/2 S⊤ (SHS⊤)−1S H1/2 ∥
k:∞ k:∞ k:∞ k:∞ k:∞ k:∞
=∥H1/2 S⊤ (S H S⊤ +S H S⊤ )−1S H1/2 ∥
k:∞ k:∞ 0:k 0:k 0:k k:∞ k:∞ k:∞ k:∞ k:∞
≤∥H1/2 S⊤ (S H S⊤ )−1S H1/2 ∥=1,
k:∞ k:∞ k:∞ k:∞ k:∞ k:∞ k:∞
itfollowsthatT ≤∥w∗ ∥2 . CombiningtheboundsonT ,T completestheproof.
22 k:∞ Hk:∞ 21 22
F.2.4 ProofofLemmaF.4
Letf (A):=(I−(I−γA)N/4)/γ/N2foranypositivedefinitematrixA∈RM×M. Followingthesamearguments
3
asintheproofofLemmaF.2,wehavef (SHS⊤)⪰0and
3
(cid:104) 1 (cid:16)(cid:16) (cid:17) (cid:17)(cid:105)
E tr I−(I−γSHS⊤)N/4 B
w∗ γN2 0
=E [w∗⊤HS⊤(SHS⊤)−1f (SHS⊤)(SHS⊤)−1SHw∗]
w∗ 3
=tr((SHS⊤)−1f (SHS⊤)(SHS⊤)−1SH2S⊤).
3
Moreover,
E T
≥(cid:88)M µ i(SH2S⊤)
w∗ 1 (cid:16) (cid:17)
µ (SHS⊤)2f (SHS⊤)−1
i=1 i 3
≥
(cid:88) µ i(SH2S⊤)
(cid:16) (cid:17)
µ (SHS⊤)2f (SHS⊤)−1
i:λ˜ i<1/(γN) i 3
≳
1 (cid:88) µ i(SH2S⊤)
, (29)
N µ (SHS⊤)
i
i:λ˜ i<1/(γN)
37wherethethirdinequalityisdueto
λγN2 N2 N
λ/f (λ)≲ ≲ ≲ ≲N
3 1−(1−γλ)N/4 (cid:80)N/4−1(1−γλ)i (1−γλ)N
i=0
whenλ<1/(Nγ). Notethat
(cid:40)
1−(1−γλ˜ i)N 4 ≥ 1 N− ·γ(1 λ˜− −N1 N) (N 4 N−≥ 4)1 ·− γ2e λ˜− 241 ≥≥ N1 5, ·γλ˜ , λ λ˜ ˜i ≥ < γ1 1N, , ≥ 1 5min{Nγλ˜ i,1}. (30)
4 i 32 i 5 i i γN
Wethushave
M M
tr(cid:16)(cid:0) I−(I−γSHS⊤)N/4(cid:1)2(cid:17) =(cid:88) [1−(1−γλ˜ i)N 4]2 ≳(cid:88) min{(Nγλ˜ i)2,1}
i=1 i=1
=#{λ˜ ≥ 1 }+N2γ2 (cid:88) λ˜2 =D . (31)
i Nγ i eff,N
λ˜ i<1/(Nγ)
CombiningEq.(31)and(29)completestheproof.
F.2.5 ProofofLemmaF.5
Substitutingv∗ =(SHS⊤)−1SHw∗intheexpressionofT andnotingv =0,wehave
4 0
T = 1 tr(cid:0) B −(I−γSHS⊤)NB (I−γSHS⊤)N(cid:1) · D eff,N
4 γN 0 0 N
= 1 tr(cid:0) w∗⊤HS⊤(SHS⊤)−1(cid:2) I−(I−γSHS⊤)2N(cid:3) (SHS⊤)−1SHw∗(cid:1) · D eff,N
γN N
=:tr(cid:0) w∗⊤HS⊤f (SHS⊤)SHw∗(cid:1) · D eff,N, (32)
4 N
wheref (A):=A−1(cid:2) I−(I−γA)2N(cid:3) A−1/(Nγ)foranysymmetricmatrixA∈RM×M. Moreover,
4
tr(cid:0) w∗⊤HS⊤f (SHS⊤)SHw∗(cid:1)
4
≤∥f (SHS⊤)SHS⊤∥·∥(SHS⊤)−1/2SH1/2∥2·∥w ∥2
4 ∗ H
≤∥f (SHS⊤)SHS⊤∥·∥w ∥2 .
4 ∗ H
Since
2N−1
1 (cid:88)
∥f (SHS⊤)SHS⊤∥= ∥ (I−γSHS⊤)i∥≤2
4 N
i=0
byourassumptiononthestepsize,itfollowsthat
tr(cid:0) w∗⊤HS⊤f (SHS⊤)SHw∗(cid:1)≲∥w ∥2 . (33)
4 ∗ H
CombiningEq.(32)and(33)wefind
D
T ≲∥w ∥2 · eff,N.
4 ∗ H N
F.2.6 ProofofTheoremF.6
First,byLemmaG.4wehave1/tr(SHS⊤)≳c forsomea-dependentc >0withprobabilityatleast1−e−Ω(M).
1 1
Therefore we may choose c sufficiently small so that γ ≤ c implies γ ≲ 1/tr(SHS⊤) with probability at least
381−e−Ω(M). Now, suppose we have γ ≲ 1/tr(SHS⊤). Following the notations in Theorem F.1, we claim the
followingboundsonApprox,Bias,Var:
EApprox≂M1−a (34a)
Var≂min(cid:8)
M,
(Nγ)1/a(cid:9)
/N. (34b)
Bias≲max(cid:8) M1−a, (Nγ)1/a−1(cid:9) , (34c)
Bias≳(Nγ)1/a−1when(Nγ)1/a ≤M/cforsomeconstantc>0, (34d)
withprobabilityatleast1−e−Ω(M). PuttingtheboundstogetheryieldsTheoremF.6.
Proof of claim (34a) Note that our definition of Approx in Thereom F.1 is the same as that in Eq. (4) (and 7).
ThereforetheclaimfollowsimmediatelyfromLemmaC.4.
Proofofclaim(34b) ThisfollowsfromtheproofofLemmaE.1withN replacedbyN.
eff
Proofofclaim(34c) ByTheoremF.1,LemmaF.3andF.5,wehave
(cid:34) (cid:35)2
Bias≲E
∥w 0∗ :k2∥2
2 ·
µ M/2(S k2:∞H k2:∞S⊤ k2:∞)
+E ∥w∗ ∥2
+σ2D
eff,N,
w∗ Nγ µ M(S k2:∞H k2:∞S⊤ k2:∞) w∗ k2:∞ Hk2:∞ N
(cid:34) (cid:35)2
≲
k
2
µ M/2(S k2:∞H k2:∞S⊤ k2:∞)
+k1−a+
D
eff,N
Nγ µ (S H S⊤ ) 2 N
M k2:∞ k2:∞ k2:∞
withprobabilityatleast1−e−Ω(M)foranyk ≤M/3. Choosingk =min{M/3,(Nγ)1/a}andusingLemmaG.4
2 2
andclaim(34b),weobtain
min(cid:8)
M,
(Nγ)1/a(cid:9)
Bias≲max(cid:8) M1−a, (Nγ)1/a−1(cid:9) + ≲max(cid:8) M1−a, (Nγ)1/a−1(cid:9) +(Nγ)1/a−1
N
≲max(cid:8) M1−a, (Nγ)1/a−1(cid:9)
withprobabilityatleast1−e−Ω(M).
Proofofclaim(34d) ByTheoremF.1and LemmaF.2,wehave
E Bias≳
(cid:88) µ i(SH2S⊤)
.
w∗ µ (SHS⊤)
i
i:λ˜ i<1/(γN)
When(Nγ)1/a ≤M/cforsomelargeconstantc>0,wehavefromLemmaG.4that
(cid:88) i−2a (cid:88)
E Bias≳ = i−a ≳[(Nγ)1/a]1−a =(Nγ)1/a−1
w∗ i−a
i:λ˜ i<1/(γN) i:λ˜ i<1/(γN)
withprobabilityatleast1−e−Ω(M).
G Concentration lemmas
G.1 Generalconcentrationresults
LemmaG.1. SupposethatS∈RM×dissuchthat3
S ∼N(0,1/M).
ij
3Weallowd=∞.
39Let(λ ) betheeigenvaluesofHinnon-increasingorder. Let(λ˜ )M betheeigenvaluesofSHS⊤innon-increasing
i i≥1 i i=1
order. Then there exists a constant c > 1 such that for every M ≥ 0 and every 0 ≤ k ≤ M, with probability
≥1−e−Ω(M),wehave
foreveryj ≤M, (cid:12) (cid:12) (cid:12) (cid:12)λ˜ j −(cid:18) λ j + (cid:80) i M>kλ i(cid:19)(cid:12) (cid:12) (cid:12) (cid:12)≤c·(cid:32)(cid:114) Mk ·λ j +λ k+1+(cid:115) (cid:80) i M>kλ2 i(cid:33) .
Asadirectconsequence,fork ≤M/c2,wehave
foreveryj ≤M, (cid:12) (cid:12) (cid:12) (cid:12)λ˜ j −(cid:18) λ j + (cid:80) i M>kλ i(cid:19)(cid:12) (cid:12) (cid:12) (cid:12)≤ 21 ·(cid:18) λ j + (cid:80) i M>kλ i(cid:19) +c 1·λ k+1,
wherec =c+2c2.
1
ProofofLemmaG.1. WehavethefollowingdecompositionmotivatedbySwartworthandWoodruff(2023)(seetheir
Section3.4,ProofofTheorem1).
SHS⊤ =S H S⊤ +S H S⊤
0:k 0:k 0:k k:∞ k:∞ k:∞
(cid:80) (cid:80)
λ λ
=S H S⊤ + i>k i ·I +S H S⊤ − i>k i ·I .
0:k 0:k 0:k M M k:∞ k:∞ k:∞ M M
WeremarkthatthisdecompositionideahasbeenimplicitlyusedinBartlettetal.(2020)tocontroltheeigenvaluesofa
Grammatrix. Infact,wewillusetechniquesfromBartlettetal.(2020)toobtainasharperboundthanthatpresentedin
SwartworthandWoodruff(2023).
Fortheupperbound,wehave
µ j(cid:0) SHS⊤(cid:1) ≤µ j(cid:18) S 0:kH 0:kS⊤ 0:k+ (cid:80) i M>kλ i ·I M(cid:19) +(cid:13) (cid:13) (cid:13) (cid:13)S k:∞H k:∞S⊤ k:∞− (cid:80) i M>kλ i ·I M(cid:13) (cid:13) (cid:13)
(cid:13)
2
=µ j(cid:0) S 0:kH 0:kS⊤ 0:k(cid:1) + (cid:80) i M>kλ i ·I M +(cid:13) (cid:13) (cid:13) (cid:13)S k:∞H k:∞S⊤ k:∞− (cid:80) i M>kλ i ·I M(cid:13) (cid:13) (cid:13)
(cid:13)
2
(cid:80)
λ
(cid:32) (cid:115) (cid:80) λ2(cid:33)
≤µ (cid:0) S H S⊤ (cid:1) + i>k i ·I +c · λ + i>k i ,
j 0:k 0:k 0:k M M 1 k+1 M
wherethelastinequalityisbyLemmaG.2. Forj ≤k,usingLemmaG.3,wehave
(cid:114)
µ (cid:0) S H S⊤ (cid:1) ≤λ +c · k ·λ .
j 0:k 0:k 0:k j 2 M j
Fork <j ≤M,wehave
(cid:114)
µ (cid:0) S H S⊤ (cid:1) =0≤λ +c · k ·λ .
j 0:k 0:k 0:k j 2 M j
Puttingthesetogether,wehavethefollowingforeveryj =1,...,M:
(cid:80)
λ
(cid:32) (cid:115) (cid:80) λ2(cid:33)
µ (cid:0) SHS⊤(cid:1) ≤µ (cid:0) S H S⊤ (cid:1) + i>k i ·I +c · λ + i>k i
j j 0:k 0:k 0:k M M 1 k+1 M
(cid:80)
λ
(cid:32)(cid:114)
k
(cid:115) (cid:80) λ2(cid:33)
≤λ + i>k i ·I +c· ·λ +λ + i>k i .
j M M M j k+1 M
40Similarly,wecanshowthelowerbound. Bythedecomposition,wehave
µ j(cid:0) SHS⊤(cid:1) ≥µ j(cid:18) S 0:kH 0:kS⊤ 0:k+ (cid:80) i M>kλ i ·I M(cid:19) −(cid:13) (cid:13) (cid:13) (cid:13)S k:∞H k:∞S⊤ k:∞− (cid:80) i M>kλ i ·I M(cid:13) (cid:13) (cid:13)
(cid:13)
=µ j(cid:0) S 0:kH 0:kS⊤ 0:k(cid:1) + (cid:80) i M>kλ i ·I M −(cid:13) (cid:13) (cid:13) (cid:13)S k:∞H k:∞S⊤ k:∞− (cid:80) i M>kλ i ·I M(cid:13) (cid:13) (cid:13)
(cid:13)
(cid:80)
λ
(cid:32) (cid:115) (cid:80) λ2(cid:33)
≥µ (cid:0) S H S⊤ (cid:1) + i>k i ·I −c · λ + i>k i ,
j 0:k 0:k 0:k M M 1 k+1 M
wherethelastinequalityisbyLemmaG.2. Forj ≤k,usingLemmaG.3,wehave
(cid:114)
µ (cid:0) S H S⊤ (cid:1) ≥λ −c · k ·λ .
j 0:k 0:k 0:k j 2 M j
Fork <j ≤M,wehave
(cid:114)
µ (cid:0) S H S⊤ (cid:1) =0≥λ −λ −c · k ·λ ,
j 0:k 0:k 0:k j k+1 2 M j
wherethelastinequalityisduetoλ ≤λ forj ≥k. Puttingthesetogether,wehave
j k
(cid:80)
λ
(cid:32) (cid:115) (cid:80) λ2(cid:33)
µ (cid:0) SHS⊤(cid:1) ≥µ (cid:0) S H S⊤ (cid:1) + i>k i ·I −c · λ + i>k i
j j 0:k 0:k 0:k M M 1 k+1 M
(cid:80)
λ
(cid:32)(cid:114)
k
(cid:115) (cid:80) λ2(cid:33)
≥λ + i>k i ·I −c· ·λ +λ + i>k i .
j M M M j k+1 M
Sofar,wehaveprovedthefirstclaim. Toshowthesecondclaim,wesimplyapply
(cid:114)
k 1
c· ≤ fork ≤M/c2,
M 2
and
(cid:115)
(cid:80) λ2 (cid:114)(cid:80) λ
c· i>k i ≤c· i>k i ·λ
M M k+1
(cid:80)
1 λ
≤ · i>k i +2c2·λ ,
2 M k+1
inthefirstclaim.
Lemma G.2 (Tail concentration, Lemma 26 in Bartlett et al. (2020)). For any k ≥ 1, with probability at least
1−e−Ω(M),wehave
(cid:115)
(cid:13) (cid:13) (cid:13) (cid:13)S k:∞H k:∞S⊤ k:∞− (cid:80) i M>kλ i ·I M(cid:13) (cid:13) (cid:13)
(cid:13)
≲λ k+1+ (cid:80) i M>kλ2 i.
2
Moreover,theminimumeigenvalueofS H S⊤ satisfies
k:∞ k:∞ k:∞
µ (S H S⊤ )≳λ
min k:∞ k:∞ k:∞ k+2M
withprobabilityatleast1−e−Ω(M).
41ProofofLemmaG.2. ThefirstpartofLemmaG.2isaversionofLemma26in(Bartlettetal.,2020)(seetheirproof).
Weprovideproofhereforcompleteness.
WewriteS∈RM×pas
(cid:18) (cid:19)
(cid:0) (cid:1) 1
S= s ... s , s ∼N 0, ·I , i≥1.
1 p i M M
SinceGaussiandistributionisrotationalinvariance,withoutlossofgenerality,wemayassume
H=diag{λ ,...,λ }.
1 p
Thenwehave
(cid:88)
S H S⊤ = λ s s⊤.
k:∞ k:∞ k:∞ i i i
i>k
Fixingaunitvectorv∈RM,then
v⊤S H S⊤ v=(cid:88) λ (cid:0) s⊤v(cid:1)2 ,
k:∞ k:∞ k:∞ i i
i>k
whereeachs⊤vis(1/M)-subGaussian. ByBernstein’sinequality,wehave,withprobability≥1−δ,
i
(cid:12) (cid:12) (cid:12) (cid:12)(cid:88) λ i(cid:0) s⊤
i
v(cid:1)2 − (cid:80) i M>kλ i(cid:12) (cid:12) (cid:12) (cid:12)≲ M1 ·(cid:18) λ k+1·log1
δ
+(cid:115) (cid:88) λ2
i
·log1 δ(cid:19) .
i>k i>k
ByaunionboundandnetargumentonSM−1,wehave,withprobability≥1−δ,foreveryunitvectorv∈RM,
(cid:12) (cid:12) (cid:12) (cid:12)(cid:88) λ i(cid:0) s⊤
i
v(cid:1)2 − (cid:80) i M>kλ i(cid:12) (cid:12) (cid:12) (cid:12)≲ M1 ·(cid:32) λ k+1·(cid:18) M +log1 δ(cid:19) +(cid:115) (cid:88) λ2
i
·(cid:18) M +log1 δ(cid:19)(cid:33) .
i>k i>k
Sowithprobabilityatleast1−e−Ω(M),wehave
(cid:13) (cid:13) (cid:13) (cid:13)S k:∞H k:∞S⊤ k:∞− (cid:80) i M>kλ i ·I M(cid:13) (cid:13) (cid:13)
(cid:13)
≲ M1 ·(cid:32) λ k+1·M +(cid:115) (cid:88) λ2
i
·M(cid:33)
2 i>k
(cid:115)
(cid:80) λ2
≂λ + i>k i,
k+1 M
whichcompletestheproofofthefirstpartofLemmaG.2.
ToprovethesecondpartofLemmaG.2,itsufficestonotethat
2M+k 2M+k
(cid:88) (cid:88)
S H S⊤ ⪰ λ s s⊤ ⪰λ · s s⊤ ⪰cλ ·I
k:∞ k:∞ k:∞ i i i 2M+k i i 2M+k M
i=k+1 i=k+1
forsomeconstantc>1withprobabilityatleast1−e−Ω(M),wherethelastlinefollowsfromconcentrationproperties
ofGaussiancovariancematrices(seee.g.,Thereom6.1Wainwright(2019)).
LemmaG.3(Headconcentration). Withprobabilityatleast1−e−Ω(M),wehave
(cid:114)
k
foreveryj ≤k, |µ (S H S⊤ )−λ |≲ ·λ .
j 0:k 0:k 0:k j M j
ProofofLemmaG.3. NotethatthespectrumofS H S⊤ isindenticaltothespectrumofH1/2S⊤ S H1/2. We
0:k 0:k 0:k 0:k 0:k 0:k 0:k
willboundthelatter. WestartwithboundingthespectrumofS S⊤ . Tothisend,wewriteS⊤ ∈Rk×M as
0:k 0:k 0:k
(cid:18) (cid:19)
S⊤ =(cid:0) s ... s (cid:1) , s ∼N 0, 1 ·I , i=1,...,M.
0:k 1 M i M k
42ThenrepeatingtheargumentinLemmaG.2,wehave,withprobability≥1−δ,foreveryunitvectorv∈Rk,
(cid:12) M (cid:12)
(cid:12) (cid:12)v⊤S⊤ 0:kS 0:kv−1(cid:12) (cid:12)=(cid:12) (cid:12) (cid:12)(cid:88)(cid:0) s⊤
i
v(cid:1)2 −1(cid:12) (cid:12)
(cid:12)
i=1
(cid:32) (cid:18) (cid:19) (cid:115) (cid:18) (cid:19)(cid:33)
1 1 1
≲ · 1· k+log + M · k+log
M δ δ
(cid:114)
k+log(1/δ))
≲ .
M
Sowehave,withprobability≥1−e−Ω(M),
(cid:114)
(cid:13) (cid:13)S⊤ 0:kS 0:k−I k(cid:13) (cid:13)
2
≲ Mk .
Thisimpliesthat
(cid:114)
µ (cid:0) H1/2S⊤ S H1/2(cid:1) ≤µ (cid:0) H1/2H1/2(cid:1) +c · k ·µ (cid:0) H1/2H1/2(cid:1)
j 0:k 0:k 0:k 0:k j 0:k 0:k 1 M j 0:k 0:k
(cid:114)
k
=λ +c · ·λ ,
j 1 M j
andthat
(cid:114)
µ (cid:0) H1/2S⊤ S H1/2(cid:1) ≥µ (cid:0) H1/2H1/2(cid:1) −c · k ·µ (cid:0) H1/2H1/2(cid:1)
j 0:k 0:k 0:k 0:k j 0:k 0:k 1 M j 0:k 0:k
(cid:114)
k
=λ −c · ·λ .
j 1 M j
Wehavecompletedtheproof.
G.2 Concentrationresultsunderpower-lawspectrum
LemmaG.4(EigenvaluesofSHS⊤underpower-lawspectrum). SupposeAssumption2hold. Thereexista-dependent
constantsc >c >0suchthat
2 1
c j−a ≤µ (SHS⊤)≤c j−a
1 j 2
withprobabilityatleast1−e−Ω(M).
ProofofLemmaG.4. Let(λ˜ )M denotetheeigenvaluesofSHS⊤inannon-increasingorder. UsingLemmaG.1with
i i=1
k =M/cforsomesufficientlylargeconstantcandnotingthat(cid:80) i−a ≂k1−a,wehave
i>k
1 3
·(j−a+c˜ M−a)−c˜ ·M−a ≤λ˜ ≤ ·(j−a+c˜ M−a)+c˜ ·M−a
2 1 2 j 2 1 2
foreveryj ∈[M]forsomeconstantsc˜,i∈[2]withprobabilityatleast1−e−Ω(M). Therefore,forallj ≤M/c˜for
i
somesufficientlylargeconstantc˜>1,wehave
λ˜ ∈[c˜ j−a,c˜ j−a]
j 3 4
with probability at least 1−e−Ω(M) for some constants c˜ ,c˜ > 0. For j ∈ [M/c˜,M], by monotonicity of the
3 4
eigenvalues,wehave
(cid:16)(cid:106)M(cid:107)(cid:17)−a
λ˜ ≤λ˜ ≤c˜ ≤c˜ M−a ≤c˜ j−a
j ⌊M/c˜⌋ 4 c˜ 5 5
43forsomesufficientlylargeconstantc˜ >c˜ withprobabilityatleast1−e−Ω(M). Moreover,usingLemmaG.2with
5 4
k =0,weobtain
λ˜ ≥λ˜ ≥µ (S H S⊤ )≥c˜ λ˜ ≥c˜ (M/c˜)−a ≥c˜ j−a
j M min k:∞ k:∞ k:∞ 6 2M 7 8
with probability at least 1−e−Ω(M) for some constants c˜ ,c˜ ,c˜ > 0. Combining the bounds for j ≤ M/c˜and
6 7 8
j ∈[M/c˜,M]completestheproof.
LemmaG.5(RatioofeigenvaluesofS H S⊤ underpower-lawspectrum). SupposeAssumption2hold. There
k:∞ k:∞ k:∞
existssomea-dependentconstantc>0suchthatforanyk ≥1,theratiobetweentheM/2-thandM-theigenvalues
µ (S H S⊤ )
M/2 k:∞ k:∞ k:∞ ≤c
µ (S H S⊤ )
M k:∞ k:∞ k:∞
withprobabilityatleast1−e−Ω(M).
ProofofLemmaG.5. Weprovethelemmaundertwoscenarioswherekisrelativelysmall(orlarge)comparedwithM.
Letc>0besomesufficientlylargeconstant. ApplyingLemmaG.1withH replacingH,fork =M/c,wehave
k:∞ 0
µ (S H S⊤ )≤
3 ·(cid:18)
λ +
(cid:80) i>k0λ i+k(cid:19)
+c ·λ ,
M/2 k:∞ k:∞ k:∞ 2 M/2+k M 1 k0+1+k
≲(cid:0)M +k(cid:1)−a + (k 0+k)1−a +(k +1+k)−a
2 M 0
≲(k∨M)−a+(k∨M)−a(cid:0) 1∨ k (cid:1) +(k∨M)−a
M
≲(k∨M)−a(cid:0)
1∨
k (cid:1)
(35)
M
withprobabilityatleast1−e−Ω(M)forsomeconstantc >0.
1
Case1: k ≲M FromLemmaG.2,wehave
µ (S H S⊤ )≳λ ≳(k∨M)−a.
min k:∞ k:∞ k:∞ k+2M
withprobabilityatleast1−e−Ω(M).Therefore
µ (S H S⊤ )
M/2 k:∞ k:∞ k:∞ ≲1
µ (S H S⊤ )
M k:∞ k:∞ k:∞
withprobabilityatleast1−e−Ω(M)whenk/M ≲1.
Case2: k ≳M Ontheotherhand,whenkisrelativelylarge,usingLemmaG.1withH replacingHagain,we
k:∞
obtain
µ (S H S⊤ )≥
1 ·(cid:18)
λ +
(cid:80) i>k0λ i+k(cid:19)
−c ·λ ,
M k:∞ k:∞ k:∞ 2 M+k M 1 k0+1+k
≥c (cid:104)(cid:0) M +k(cid:1)−a + (k 0+k)1−a(cid:105) −c ·(k +1+k)−a
2 M 3 0
withprobabilityatleast1−e−Ω(M),wherec ,c ,c > 0aresomeuniversalconstants. Choosingk = M/c2 for
1 2 3 0
somesufficientlylargeconstantc>0,wefurtherobtain
µ (S H S⊤ )≥c (cid:0) M +k(cid:1)−a(cid:104) 1+ k (cid:105) −c (M +k)−a
M k:∞ k:∞ k:∞ 4 M 5
≥c (cid:0) M ∨k(cid:1)−a(cid:104) 1∨ k (cid:105) −c (M ∨k)−a (36)
6 M 7
44withprobabilityatleast1−e−Ω(M),where(c )7 area-dependentconstants. Since
i i=4
c (cid:0) M ∨k(cid:1)−a(cid:104) 1∨ k (cid:105) −c (M ∨k)−a ≥ c 6(k∨M)−a(cid:0) 1∨ k (cid:1)
6 M 7 2 M
whenkislarge,i.e.,k/M >c˜forsomesufficientlylargea-dependentconstantc˜>0thatmaydependon(c )7 ,we
i i=1
havefromEq.(35)and(36)that
µ (S H S⊤ ) (k∨M)−a(cid:0) 1∨ k (cid:1)
M/2 k:∞ k:∞ k:∞ ≲ M ≲1
µ (S H S⊤ ) (k∨M)−a(cid:0) 1∨ k (cid:1)
M k:∞ k:∞ k:∞ M
withprobabilityatleast1−e−Ω(M).
G.3 Concentrationresultsunderlogarithmicpower-lawspectrum
Lemma G.6 (Proof of Theorem 6 in (Bartlett et al., 2020)). Suppose Assumption 4 hold. Then there exist some
a-dependentconstantsc,c˜>0suchthat,withprobabilityatleast1−e−Ω(M)
(cid:40) [c·j−1log−a(j+1),c˜·j−1log−a(j+1)] j ≤k∗,
µ (SHS⊤)∈
j [c·M−1log1−a(M),c˜·M−1log1−a(M)] k∗ <j ≤M,
wherek∗ ≂M/log(M). Also,thereexistssomea-dependentconstantsc ,c >0suchthat
1 2
c c
1 ≤µ (SH2S⊤)≤ 2
jlog2a(j+1) j jlog2a(j+1)
withprobabilityatleast1−e−Ω(M).
ProofofLemmaG.6. TheproofisadaptedfromtheproofofTheorem6in(Bartlettetal.,2020). Weincludeitherefor
completeness.
FirstpartofLemmaG.6. InLemmaG.1,forsomeconstantc>1,choose
(cid:26) (cid:27)
(cid:88)
k∗ :=min k ≥0: λ ≥c·M ·λ .
i k+1
i>k
Thenwithprobability≥1−e−Ω(M),wehave:
1 (cid:18) (cid:80) λ (cid:19) (cid:18) (cid:80) λ (cid:19)
forevery1≤j ≤M, · λ + i>k∗ i ≤λ˜ ≤c · λ + i>k∗ i ,
c j M j 1 j M
1
wherec >1isaconstant.
1
Whenλ ≂j−1log−a(j+1),wehave
j
k∗ ≂M/log(M),
and
(cid:88)
λ ≂log1−a(k∗)≂log1−a(M).
i
i>k∗
Therefore,wehave
(cid:80)
λ
λ˜ ≂λ + i>k∗ i
j j M
(cid:40) j−1log−a(j+1) j ≤k∗,
≂
M−1log1−a(M) k∗ <j ≤M,
wherek∗ ≂M/log(M).
45SecondpartofLemmaG.6. Letλ¯ denotethei-theigenvalueofSH2S⊤ fori ∈ [M]. UsingLemmaG.1with
i
k =M/cforsomesufficientlylargeconstantc andnotingthat(cid:80) λ2 ≂(cid:80) i−2log−2a(i+1)≲k−1log−2ak,
0 i>k i i>k
wehave
1
·j−2log−2a(j+1)−c˜ ·M−2log−2aM
2 2
3
≤λ¯ ≤ ·(j−2log−2a(j+1)+c˜ M−2log−2aM)+c˜ ·M−2log−2aM
j 2 1 2
foreveryj ∈[M]forsomeconstantsc˜,i∈[2]withprobabilityatleast1−e−Ω(M). Therefore,forallj ≤M/c˜for
i
somesufficientlylargeconstantc˜>1,wehave
λ¯ ∈[c˜ ·j−2log−2a(j+1),c˜ ·j−2log−2a(j+1)]
j 3 4
with probability at least 1−e−Ω(M) for some constants c˜ ,c˜ > 0. For j ∈ [M/c˜,M], by monotonicity of the
3 4
eigenvalues,wehave
(cid:16)(cid:106)M(cid:107)(cid:17)−2 (cid:16)(cid:106)M(cid:107)(cid:17)
λ¯ ≤λ¯ ≤c˜ log−2a ≤c˜ M−2log−2aM ≤c˜ ·j−2log−2a(j+1)
j ⌊M/c˜⌋ 4 c˜ c˜ 5 6
forsomeconstantsc ,c >0withprobabilityatleast1−e−Ω(M). Moreover,usingLemmaG.2withk =0,weobtain
5 6
λ¯ ≥λ¯ ≥µ (S H2 S⊤ )≥c˜ λ¯ ≥c˜ ·j−2log−2a(j+1)
j M min k:∞ k:∞ k:∞ 7 2M 8
withprobabilityatleast1−e−Ω(M) forsomeconstantsc˜ ,c˜ > 0whenj ∈ [M/c˜,M]. Combiningtheboundsfor
7 8
j ≤M/c˜andj ∈[M/c˜,M]completestheproof.
LemmaG.7(RatioofeigenvaluesofS H S⊤ underlogarithmicpower-lawspectrum). SupposeAssumption4
k:∞ k:∞ k:∞
hold. Thereexistssomea-dependentconstantc>0suchthatforanyk ≥1,theratiobetweentheM/2-thandM-th
eigenvalues
µ (S H S⊤ )
M/2 k:∞ k:∞ k:∞ ≤c
µ (S H S⊤ )
M k:∞ k:∞ k:∞
withprobabilityatleast1−e−Ω(M).
ProofofLemmaG.7. SimilartotheproofofLemmaG.5,weprovethelemmaundertwoscenarioswherekisrelatively
small(orlarge)comparedwithM.
Letc>0besomesufficientlylargeconstant. ApplyingLemmaG.1withH replacingH,fork =M/c,wehave
k:∞ 0
µ (S H S⊤ )≤
3 ·(cid:18)
λ +
(cid:80) i>k0λ i+k(cid:19)
+c ·λ ,
M/2 k:∞ k:∞ k:∞ 2 M/2+k M 1 k0+1+k
≲(cid:0)M +k(cid:1)−1 log−a(cid:0)M +k(cid:1)
+
log1−a(k 0+k)
+
log−a(k 0+1+k)
2 2 M k +1+k
0
log−a(M +k) log1−a(M +k) log1−a(M +k)
≲ + ≲ (37)
(M +k) M M
withprobabilityatleast1−e−Ω(M)forsomeconstantc >0.
1
Case1: k ≲M. ApplyingLemmaG.1withH replacingH,fork =M/c,wehave
k:∞ 0
µ (S H S⊤ )≳
1 ·(cid:18)
λ +
(cid:80) i>k0λ i+k(cid:19)
−c ·λ ,
M k:∞ k:∞ k:∞ 2 M+k M 1 k0+1+k
≳(cid:0)
M
+k(cid:1)−1 log−a(cid:0)
M
+k(cid:1)
+
log1−a(k 0+k) −clog−a(k 0+1+k)
M k +1+k
0
log−a(M +k) log1−a(M +k) log−a(M)
≳ + −c˜
(M +k) M M
log1−aM
≳
M
46withprobabilityatleast1−e−Ω(M). Therefore,
µ (S H S⊤ ) (cid:104)log1−a(M +k)(cid:105)(cid:46)(cid:104)log1−aM(cid:105)
M/2 k:∞ k:∞ k:∞ ≲ ≲1
µ (S H S⊤ ) M M
M k:∞ k:∞ k:∞
withprobabilityatleast1−e−Ω(M)whenk/M ≲1.
Case 2: k ≳ M. On the other hand, when k is relatively large, using Lemma G.1 with H replacing H and
k:∞
k =M/cagain,weobtain
0
µ (S H S⊤ )≥
1 ·(cid:18)
λ +
(cid:80) i>k0λ i+k(cid:19)
−c ·λ ,
M k:∞ k:∞ k:∞ 2 M+k M 1 k0+1+k
≳(cid:0)
M
+k(cid:1)−1 log−a(cid:0)
M
+k(cid:1)
+
log1−a(k 0+k)
−c
log−a(k 0+1+k)
M 2 k +1+k
0
≳k−1log−a(cid:0) k(cid:1)
+
log1−a(M +k)
−c
log−a(M +k)
M 3 M +k
log1−a(M +k)
≳
M
withprobabilityatleast1−e−Ω(M),wherec ,c ,c >0aresomea-dependentconstants. Therefore,
1 2 3
µ (S H S⊤ ) (cid:104)log1−a(M +k)(cid:105)(cid:46)(cid:104)log1−a(M +k)(cid:105)
M/2 k:∞ k:∞ k:∞ ≲ ≲1
µ (S H S⊤ ) M M
M k:∞ k:∞ k:∞
withprobabilityatleast1−e−Ω(M)whenk/M ≳1.
47