OLMES: A Standard for Language Model Evaluations
YulingGuα OyvindTafjordα BaileyKuehlα DanyHaddadα
JesseDodgeα HannanehHajishirziαβ
αAllenInstituteforArtificialIntelligence βUniversityofWashington
{yulingg, oyvindt}@allenai.org
Abstract
ProgressinAIisoftendemonstratedbynewmodelsclaimingimprovedperfor-
mance on tasks measuring model capabilities. Evaluating language models in
particularischallenging,assmallchangestohowamodelisevaluatedonatask
canleadtolargechangesinmeasuredperformance. Thereisnocommonstan-
dardsetup,sodifferentmodelsareevaluatedonthesametasksindifferentways,
leadingtoclaimsaboutwhichmodelsperformbestnotbeingreproducible. We
proposeOLMES,acompletelydocumented, practical, openstandardforrepro-
ducibleLLMevaluations. Indevelopingthisstandard,weidentifyandreviewthe
varyingfactorsinevaluationpracticesadoptedbythecommunity–suchasdetails
ofpromptformatting,choiceofin-contextexamples,probabilitynormalizations,
and task formulation. In particular, OLMES supports meaningful comparisons
between smaller base models that require the unnatural “cloze” formulation of
multiple-choicequestionsagainstlargermodelsthatcanutilizetheoriginalformu-
lation. OLMESincludeswell-consideredrecommendationsguidedbyresultsfrom
existingliteratureaswellasnewexperimentsinvestigatingopenquestions.1
1 Introduction
ScientificcredibilityinAIrestsonfair,reproduciblecomparisonsbetweenmodels. Manycurrent
AI models, such as pretrained large language models (LLMs), are generalist models capable of
performingdownstreamtaskstheywerenotspecificallytrainedon(OpenAI,2024;Bommasanietal.,
2022). WhenevaluatingLLMsonsuchtasks,therearemanychoicesinhowthetaskispresented
tothemodelandhowthemodeloutputsareinterpretedbeforescoring(Gao,2021;Bidermanetal.,
2024;Liangetal.,2023). Thereiscurrentlynostandardwaytodecideonthesechoices,andtheycan
haveahugeimpactonmodelperformance,withsomerecentpapersclaimingasmuchasan80%
differenceinaccuracyonagiventaskjustfromvaryingformattingandin-contextexamples(Sclar
etal.,2023). Thesechoicesarerarelyreportedwithenoughinformationtoactuallyreproduce,so
whenateamofMLpractitionersreleasesanewmodelitisoftenimpossibleforthemtocompare
againstpreviously-reportedresults.
EffortsliketheHolisticEvaluationofLanguageModels(HELM)benchmark(Liangetal.,2023)
andtheHuggingFaceOpenLLMLeaderboard(Beechingetal.,2023)strivetowardsstandardizing
LMevaluations. Whilethesamesetupisusedtoevaluatemanymodelsensuringconsistencyand
reproducibility,therationalebehindpromptformatting,useofin-contentexamples,normalization
techniques,andtaskformulationarenotalwaysclearlydocumentedandthusnotconsistentlyfollowed
byotherresearchersinsubsequentwork(Touvronetal.,2023b;Bidermanetal.,2023;Jiangetal.,
2023;Groeneveldetal.,2024;AI@Meta,2024).
1Allprompts,examples,andcodeusedforOLMESareavailableathttps://allenai.org/data/olmes
Preprint.Underreview.
4202
nuJ
21
]LC.sc[
1v64480.6042:viXraTable1: ScoresreportedindifferentreferencesforLLMperformanceson ARC-CHALLENGE and
OPENBOOKQA.Scoresindicatedwith† areusingmultiple-choiceformulation(MCF)ratherthan
“cloze”formulation(CF)(seeSection2.1fordefinitions). Entrieswith“?” denoteeitherundocu-
mentedormixedapproachesacrossmodels. Differentreferencesusedifferentevaluationsetups,
some of which are not fully specified, so conclusions about which models perform best are not
reproducible.
ARC-CHALLENGEEvaluations: OPENBOOKQAEvaluations:
Model↓ Ref1Ref2Ref3 Ref4 Ref5 Ref6 OLMES Ref2Ref4Ref5Ref7 Ref8 OLMES
MPT-7B 47.7 42.6 46.5 45.7 51.4 48.6 52.4
RPJ-INCITE-7B 46.3 42.8 45.3 49.4 49.0
Falcon-7B 47.9 42.4 44.5 47.5 49.7 51.6 44.6 53.0 26.0† 55.2
Mistral-7B 60.0 55.5 54.9 78.6† 52.2 77.6† 80.6†
Llama2-7B 53.1 45.9 43.2 45.9 48.5 53.7† 54.2 58.6 58.6 48.4 58.6 54.4† 57.8
Llama2-13B 59.4 49.4 48.8 49.4 67.6† 67.3† 57.0 57.0 57.0 63.4† 65.4†
Llama3-8B 60.2 78.6† 79.3† 76.6† 77.2†
Numshots 25 0 0 0 0 25 5 0 0 0 0 5 5
Curatedshots No No Yes No Yes
Formulation RC RC RC? RC RC MC MC/RC RC RC RC RC MC MC/RC
Normalization char char ? char? pmi none none/pmi pmi pmi? pmi pmi? none none/pmi
Ref Referencecitation Ref Referencecitation
Ref1HFOpenLLMLeaderboard(Beechingetal.,2023) Ref5OLMopaper(Groeneveldetal.,2024)
Ref2Llama2paper(Touvronetal.,2023a) Ref6Llama3modelcard(AI@Meta,2024)
Ref3Mistral7B(Jiangetal.,2023) Ref7Gemmapaper(GemmaTeametal.,2024)
Ref4Falconpaper(Almazroueietal.,2023) Ref8HELMLiteLeaderboard(Liangetal.,2023)
Wehighlighttwoproblemsinthefieldtoday. (1)Releasinganewmodelandcomparingagainst
previouslyreportedresultsisflawedunlessthepreviousworkexplicitlydescribedtheirfullevaluation
setup,andthenthatsetupisfollowedinthenewwork. Unfortunately,suchresultsarenotcomparable
topreviousworkthatusedadifferentformat. Importantly,thekeygoalofcomparingmodelson
thesamedownstreamtaskistomeasuretheirrelativestrengths. Thisisarealproblem,asweshow
inTable1,whichillustrateshowseveralmodels’publishedperformanceontheARC-CHALLENGE
(Clark et al., 2018) and OPENBOOKQA (Mihaylov et al., 2018) tasks varies in the literature. For
instance,lookingatRef1forARC-CHALLENGE,wewouldconcludethatLlama2-13BandLlama3-8B
are performing similarly, but Ref6 reveals there is likely a gap of over 10% between them. For
OPENBOOKQA, on Falcon-7B, there is a discrepancy of more than 25% between Ref2 and Ref8,
whichonlymakessenseifoneconsiderstheverydifferenttaskformulations. (2)Somepapersare
likely reporting over-estimates of performance by optimizing the prompt format for their model,
whileothersarenot. Infact,weseeevidencethatthismightbeacommunity-wideproblemina
recenteffortfromHELM(MaiandLiang,2024). Whenevaluatingasuiteofmodelsusingasingle
formatandcomparingagainsteachmodel’sself-reportedresult,theyfindsignificantdifferencesin
performance. Inparticular,theirfindingssuggestthatmodelswithhigherself-reportedperformance
weremorelikelytohavelowerscoresinthereproduction,suggestingsomeover-estimatingofmodel
performance(seeFigure4inAppendix).
Toaddresstheseproblems, wepresentOLMES(OpenLanguageModelEvaluationStandard), a
standard to improve the transparency and reproducibility of language model evaluation from a
practicalpointofview, removingambiguityinhowafinalperformancemetricisobtainedwhen
evaluatingamodelonadataset. OLMEScanbeappliedtoevaluationduringthemodeldevelopment
process,leaderboards,papers,providingjustifiedrecommendationsondatasampling,howtoformat
instances,thechoiceofin-contextexamples,probabilitynormalization,andtaskformulation.
Importantly,OLMESis:
• Reproducible: OLMESspecifiesalldetailsoftheevaluations,fromprocessingdatasetsto
presentingthetasktomodel,toprocessingmodels’outputs,sotherearenoambiguitiesin
theevaluationprocedure.
2• Practical: OLMES makes practical decisions in use of computation resources for easy
adoptionbythecommunity.
• Documented: Eachdecisioninthestandardisdocumentedwithjustificationsbyapplying
principlesfromexistingstudiesandperformingexperimentstoinvestigateopenquestions.
• Open: Wereleaseallpromptsandcode,alongwiththerationalesbehindthechoicesmade
inOLMES,forsubsequentworktofollowandbuilduponbyextendingthesameprinciples
toanynewtaskandmodel.
SinceOLMESisadocumented,practical,openevaluationstandard,itisstraightforwardtoadoptin
publiclyavailable,well-maintainedevaluationcodebasesliketheEleutherLMEvaluationHarness
(Gao et al., 2023; Biderman et al., 2024) and HELM (Liang et al., 2023). When used by model
developersandotherresearchers,OLMESwillhelpunifyevaluationpracticesinthefield.Allprompts,
examples,andcodeusedforOLMEScanbefoundathttps://allenai.org/data/olmes.
2 Experimentalsetup
2.1 Multiple-choiceQAandLLMevaluation
Multiple-choicequestionanswering(MCQA)taskspresentacompellingwayofevaluatingmodels
andhumansalike,duetotheeaseofscoring(whetherthecorrectanswerischosenoutofthegiven
options)andtheallowedflexibilityinthedomainandcomplexityofthequestions. Suchtasksforma
large,essentialpartofLMevaluationsandarebethefocusinOLMES.Therearegenerallytwoways
toformulatethesetasks.
MCF(Multiple-choiceformulation): presentinganswerchoicesindicatedbylabelsandscoring
predictionofanswerlabels,justlikehowMCQAisposedtohumans. HereisanexampleofMCQA
fromARC-EASY(Clarketal.,2018),adatasetofrealgrade-schoollevelsciencequestions:
Question: Earth’s core is primarily composed of which of the following materials?
(A) basalt (B) iron (C) magma (D) quartz
Answer: (B)
CF (Completion/cloze formulation): scoring each answer choice separately using LM token
probabilities.TheMCFformatisnotcompatiblewiththepurelanguagemodelingnatureofgenerating
the next token. Therefore, the CF format was introduced by when evaluating the GPT-3 model
(Brownetal.,2020). Theyfoundthatitwaspossibletoelicitmuchbetterperformanceusinga“cloze”
completionversionofthetask,wherethemodelisshownapromptlike:
Question: Earth’s core is primarily composed of which of the following materials?
Answer: <answer>,whereeachanswerchoiceisseparatelysubstitutedinfor<answer>.
ThentheLMprobabilityoftheanswerchoicetokensareusedtorankthechoicesandpredictan
answer. Thisformulationhasnaturallimitations,suchasnotbeingabletoproperlyaddresscases
whereoneanswerchoiceis“noneoftheabove”.
2.2 Targetedtasks
We select and implement standards for 10 popular benchmark MCQA tasks, see Table 2 for the
list. Thelistcoverstasksthatarefrequentlyusedinthecommunity’sevaluationpractices,suchas
theHuggingFaceOpenLLMLeaderboard(Beechingetal.,2023),Llamapapers(Touvronetal.,
2023a,b;AI@Meta,2024),HELM(Liangetal.,2023),andtheOLMoevaluationsuite(Groeneveld
etal.,2024). Thisselectionincludesquestionsonscience,varioustypesofcommonsense,factual
knowledge,andcoversarangeoftopics(MMLUalonecovers57subjects),ofvaryingdifficulty.
2.3 Selectionofmodels
WedevelopOLMESbasedonaselectionof15diverse,openlyavailablepretrainedLLMs,coveringa
rangeofsizesfrom1Bto70B–Pythia-1B,Pythia-6.7B(Bidermanetal.,2023),OLMo-1B,OLMo-
7B,OLMo-1.7-7B(Groeneveldetal.,2024),TinyLlama-1.1B(Zhangetal.,2024),StableLM2-1.6B
(Bellagenteetal.,2024),RPJ-INCITE-7B(TogetherComputer,2023),MPT-7b(MosaicML,2023),
3Table2: OLMESdetailsontasks,withourstandardizedchoicesofdatasetsplit,numberofinstances
to use (along with total number if sampling was used), and which CF normalization scheme to
use (see Section 3.3). Column #C shows the number of answer choices (ARC-CHALLENGE and
ARC-EASY†haveafewinstanceswith3or5answerchoices).
task split #C #inst(total) CFnorm reference
ARC-CHALLENGE(ARC_C) Test 4† 1172 pmi (Clarketal.,2018)
ARC-EASY(ARC_C) Test 4† 1000(2376) char (Clarketal.,2018)
BOOLQ Val 2 1000(3270) none (Clarketal.,2019)
COMMONSENSEQA(CSQA) Val 5 1221 pmi (Talmoretal.,2019)
HELLASWAG(HSwag) Val 4 1000(10042) char (Zellersetal.,2019)
MMLU Test 4 14042 char (Hendrycksetal.,2021)
OPENBOOKQA(OBQA) Test 4 500 pmi (Mihaylovetal.,2018)
PIQA Val 2 1000(1838) char (Bisketal.,2020)
SOCIALIQA(SIQA) Val 3 1000(1954) char (Sapetal.,2019)
WINOGRANDE(WinoG) Val 2 1267 none (Sakaguchietal.,2020)
Falcon-7B (Almazrouei et al., 2023), Llama2-7B, Llama2-13B (Touvron et al., 2023b), Mistral-
7B-v0.1(Jiangetal.,2023),Llama3-8B,Llama3-70B(AI@Meta,2024). Thisreflectsourgoalof
providinganevaluationstandardthatsuitsarangeofmodelcapabilities,withtheflexibilitytoapply
thesamemethodologyduringmodeldevelopmentaswellaswhencomparingfinalpowerfulmodels.
3 Standardizingvariationsinevaluation
Toevaluateamodelonadataset,thereareavarietyofdecisionsthathavetobemadetogetafinal
accuracymetricofthatmodelonthatdataset. Theseinclude:
• Howtoformatdatasetinstances? (Section3.1)
• Whichfew-shotexamplestouse? (Section3.2)
• HowtonormalizeLLMprobabilitiesforCF?(Section3.3)
• Whattaskformulationtouse,MCForCF?(Section3.4)
• Otherimplementationchoicesimpactingresults(AppendixA)
Belowweenumeratekeyvariationsinthesesteps,andjustifythechoicesmadeinOLMES(listedin
Table2)tostandardizethesesteps,leavingsomeofthedetailsfortheAppendix.
3.1 Howtoformatdatasetinstances?
EachMCQAdatasetincludesasetoffieldsusedtospecifyaninstance,suchasquestion,answer
choices, and perhaps a context for the question. When formatting an instance as a prompt to an
LLM,manydifferentchoiceshavebeenmadeintheliterature. Thisincludessimplechoiceslike
"Question:"vs"Q:"asquestionprefix(varyingevenwithinapaper,e.g.,Brownetal.(2020)),
or formatting the answer labels (e.g., "A." (Touvron et al., 2023a), "(A)" (Nori et al., 2023),
"<mc>A</mc>"(Anthropic,2024),etc). Thereisalsoachoiceofwhetherornottoprovideageneral
instruction(e.g.,commonforMMLU(Hendrycksetal.,2021),sometimesdoneforOPENBOOKQA
(Almazroueietal.,2023)).
Instanceformatting.OLMESusesaconsistent"Question: <question>"prefixand"Answer:"
suffixinformattingthedatasets. Thisclarifiesthequestion-answeringtaskinanaturalway,without
relyingonverboseinstructionunderstanding. Thethreeexceptionsarelistedandexplainedhere. For
PIQA,weuse"Goal: <goal>"astheprefixinsteadtobeconsistentwiththeoriginalsemanticsof
thedataset.InthecaseofMCF,forHELLASWAG,weskipthequestionprefixandinsteadadd"Choose
the best continuation:"beforepresentingthecontinuationoptions,andforWINOGRANDEwe
usetheprefix"Fill in the blank:"toalignwiththetask. ForHELLASWAGandWINOGRANDE,
wheretheCFanswerstringissimplyalanguagecontinuation,weremovesuchprefixesandsuffixes
fortheCFevaluationsothatthetaskisclosertopurelanguagemodeling.
4MCQAlabelchoice. ForMCFanswerchoices,OLMESusesthecanonicallettersA/B/C/... as
answerlabels,presentingthemultiple-choiceoptionsaftersimpleletterlabels,i.e.," A."format.
Wenotethatmosttokenizerstreataletteratthestartofaline(orstring)asaseparatetokenfromthe
sameletterfollowingaspace. Thereforeweaddaprefixspaceinfrontofeachanswerlabel"\n A.
<choice>"(ratherthan"\nA. <choice>"),toworknaturallywithallcurrenttokenizers(sothat
thefinalanswertokenwillbeidenticaltotheanswerchoicetoken,seeAppendixC.3fordetails). All
theexactOLMESpromptformatsarelistedinAppendixG.
Sampling. FollowingexistingLMevaluationstandardizationefforts(Liangetal.,2023;Beeching
etal.,2023),OLMESusesthetestsplitofadatasetifthelabelsarepubliclyavailable,otherwisethe
validationsplit. Ifthedatasethasmorethan1500instances,wesample1000instancestoevaluate
on,2similartoHELM(Liangetal.,2023)whichcapsevaluationinstancesat1000.3 Notethatthe
potentialextrastatisticalsignalfrommoreinstanceswouldgenerallybedominatedbyothersources
ofscorevariations,likepromptformatting,sothisisapracticalconsiderationtoavoidunnecessary
computationresources. SeeTable2fordetailsonsplitsandsamplingusedinOLMES.
3.2 Whichfew-shotexamplestouse?
PopularizedbyBrownetal.(2020),itiscustomarytoprovideexamplesofthetasktothemodel
throughfew-shotexamples,asthisisaneffectiveanduniversalwaytoconveyatasktoanLLM.
Forexample,theMMLUtask(Hendrycksetal.,2021)originallycamewithafixed5-shotprompt
whichisgenerallyusedinevaluation(Beechingetal.,2023;GemmaTeametal.,2024;Jiangetal.,
2023;Touvronetal.,2023b;AI@Meta,2024)resultinginamorereproducibleresultsthanmany
othertasks.4 Forothertasks,boththenumberofshotsandthewayinwhichtheyaresampledhave
variedindifferentevaluationsetups. Forexample,toevaluateonHELLASWAG,Beechingetal.(2023)
sampled10-shotwhereasHELMLiangetal.(2023)uses0-shot;withinBeechingetal.(2023),a
rangeof25-shot,10-shot,5-shotwassampledforARC-CHALLENGE,HELLASWAGandWINOGRANDE
respectively.
OLMESstandardizesamanuallycurated5-shotspromptforeachtask(fromitstrainingset),ensuring
thattheexamplesareofgoodqualityandcoverthelabelspaceinabalancedway(e.g.,avoiding4
A’sand1Bamongthe5answers).5 Restrictingto5in-contextexampleshelpslimitcomputational
overhead,similartoHELM(Liangetal.,2023).Analysissuggeststhatgoingbeyond5shotsgenerally
doesnotprovidemeaningfuldifferencesinscores(Brownetal.,2020;Barton,2024). Thesemanually
curatedshotsforeachtaskcanbedownloadedfromhttps://allenai.org/data/olmes.
3.3 HowtonormalizeLLMprobabilitiesforCF?
Whenusingthecloze/completionformulation(CF)formultiple-choicequestions,theLLMreturns
P(a |q)todenotetheprobabilityforananswerchoicea givenaquestionpromptq. Rankingsolely
i i
basedontheprobabilitymayheavilyfavorshorteranswerswithfewertokens. Toworkaroundthis
issue,differentnormalizationmethodshavebeenusedintheliterature,whichwecategorizebelow:
• none: ln(P(a |q))
i
• token: ln(P(a |q))/num_tokens(a ),whichnormalizesthelog-probabilitybythenumberof
i i
tokensintheanswer(Brownetal.,2020)
• character: ln(P(a |q))/num_characters(a ), which normalizes the log-probability by the
i i
numberofcharactersintheanswer,usedbyLlamamodels(Touvronetal.,2023a)andEleuther
AILMHarness(Gaoetal.,2023;Bidermanetal.,2024).
• pmi:ln(P(a |q)/P(a |u))whereu="Answer:"istheunconditionalprompt,whichnormalizes
i i
bydividingbyLLMprobabilityofthesameanswerstringwithoutthepresenceofthequestion
(like "Answer: <answer_string>"). This can be considered a form of pointwise-mutual-
information(PMI)andwasexploredfurtherinotherworks(Holtzmanetal.,2021).
2SamplingusesaspecificrandomseedinPython:Random(1234).sample(all_instances, 1000)
3https://crfm-helm.readthedocs.io/en/latest/reproducing_leaderboards/
4SometimessampledexamplesareusedalsoforMMLU(MosaicML,2024).EvenforMMLU,noticeable
discrepancieshavebeenfound,duetootherdifferencesinpromptformatting(MaiandLiang,2024)).
5MoredetailsoncuratingtheexamplescanbefoundinAppendixF.
5Efforts like Liang et al. (2023); Gao et al. (2023); Biderman et al. (2024) compare and support
comparisonsofdifferentnormalizationapproaches,leavingitanopenquestionastohowtomakea
decision. SeeAppendixC.2forfurtherdiscussionsarounddifferentnormalizations.
TochooseanormalizationschemeinOLMES,weevaluatethemodelsoneachdatasetusingCF,
comparing the effect of the 4 normalization techniques. Table 3 shows for each task, how often
eachnormalizationisempiricallythebestacrossthe15models. Detailedscorespermodelarein
AppendixC.2.
OLMESspecifiesthe“pmi”normalizationfor ARC-CHALLENGE, COMMONSENSEQA,and OPEN-
BOOKQA.Theanswerchoicesinthesedatasetstendtocontainunexpectedwordsorphrasesthat
are less likely for models to generate (e.g., “Whirlpool bath” compared to “Bathtub”). The pmi
normalizationadjustsforthisbytakingintoaccounttheapriorilikelihoodoftheanswers. Thisis
consistentwithotherfindings(Holtzmanetal.,2021)andsomeexistingevaluationpractices,e.g.,
Brownetal.(2020)selectivelyusesthisnormalizationforARCandOPENBOOKQA,andTouvronetal.
(2023a,b)forOPENBOOKQA.Computingtheextraunconditionallikelihoodincurssomecomputation
overhead,thusOLMESavoidsthisnormalizationforotherdatasetswherethereisnostrongempirical
ortheoreticalreasontochoosethisapproach.
OLMESspecifiesthe“character”normalizationforARC-EASY,HELLASWAG,PIQA,SOCIALIQA
andMMLU.Basedonourexperiments,itisempiricallythenormalizationtechniquethatgivesthebest
scores6forthesedatasets,andlesscomputationallyexpensivethanthe“pmi”normalization. Italso
hastheadvantage(unlikethe“token”normalization)ofalreadybeingimplemented(asacc_norm)
intheEleutherLMEvaluationHarness,whereitisgenerallyavailableformultiple-choicetasks(Gao
etal.,2023;Gao,2021). ItisalsousedintheHuggingFaceOpenLLMLeaderboard(Beechingetal.,
2023)forARC-CHALLENGEandHELLASWAG,inTouvronetal.(2023a,b)’sevaluationsasthedefault,
(withselectdatasetsasexceptions),aswellasreportedinvariousworkslikeBidermanetal.(2023);
Almazroueietal.(2023).
OLMES specifies the “none” normalization for BOOLQ and WINOGRANDE. In BOOLQ the only
answerchoicesare“yes”or“no”whicharesingletokens,thereforenolengthnormalizationisneeded.
Notethatforsomemodels,the“character”normalizationhasslightlybetterperformanceonBOOLQ
(seeTable9),anaccidentalsideeffectof“yes”havingonemorecharacterthan“no”. Onecouldargue
thatthepminormalizationisappropriateasitcountersanyexistingbiasinthemodelfor“yes”vs
“no”,butwearguethatmodelsshouldbecapableofproducingsuchcommonwords(alsoindicatedin
the5-shotexamples)withoutanysuchcorrections. Finally,WINOGRANDEisaspecialcaseinthatthe
continuationsareidentical(andthepromptsvary),sothechoiceofnormalizationdoesnotmatterand
wesimplyusethe“none”normalization.
Ingeneral,weobservelittledifferencebetweentheOLMESrecommendationandtheempirically
best(“oracle”)normalizationforeachtaskandmodel,see“diffbest”columninTable3(Table8in
AppendixC.2hasmoredetails).
3.4 Whattaskformulationtouse,MCForCF?
AsLLMshavegottenstronger,theMCQAtaskformatshavegraduallychangedfromCFtoMCF.
Forinstance,ARC-CHALLENGEisoftenevaluatedusingtheCFapproach(Touvronetal.,2023a,b;
Almazroueietal.,2023;Beechingetal.,2023),buthasswitchedtoMCFforstrongermodelslike
OpenAI(2024);AI@Meta(2024),appearingwithidenticalnameslike“25-shotARC-CHALLENGE”.
Asanexample,AI@Meta(2024)reportsa25-shotMCFARC-CHALLENGEscorefortheLlama-3
8Bmodelof78.6%vs59.2%forthe25-shotCFontheHuggingFaceOpenLLMLeaderboard. As
performanceonamultiple-choicetaskgetscloserto100%,theCFapproachlagsbehindandgive
significantlylesssignalaboutamodel’sactualperformance. Ontheotherhand,MMLUisalmost
exclusivelyevaluatedusingtheMCFapproach,whichoftenresultsinnear-randomperformancefor
weakermodels(Beechingetal.,2023).
InOLMES,wearguethattheCFformulationprovidesausefulevaluationoftaskknowledgefor
modelsthathavenotyetacquiredtheskillofansweringmultiple-choicequestionsusingMCF.Onthe
otherhand,MCFisamorerealisticformulationformodelsthatcan“understand”thisformat,yielding
6TieforPIQA,andsecond-bestforMMLU.
6Table3:SummaryofCFnormalizationcompar-
MMLUduringtrainingofOLMo-1.7-7B
isons. “winpercentage”showshowofteneach
0.55
MCF normalization was best across the 15 models.
0.50
CF “diffbest”showsthatthereisingeneralminimal
0.45 random differencebetweentheOLMESnormalization
0.40 andtheempiricallyoptimalnormalizationfor
eachtask(differenceoutof100%).
0.35
0.30 winpercentage diff
task none char tok pmi bestOLMES
0.25
0.20 ARC_C 0.0 33.3 0.0 66.7 0.2 pmi
0 500 1000 1500 2000 2500 ARC_E 6.7 86.7 6.7 0.0 0.1 char
Trainingdata(billiontokens) BoolQ 46.7 46.7 0.0 6.7 1.1 none
Figure 1: Performance on MMLU validation CSQA 6.7 33.3 6.7 53.3 0.6 pmi
setduringthetrainingofOLMo-1.7-7Bmodel. HSwag 0.0 100.0 0.0 0.0 0.0 char
MMLU 0.0 46.7 0.0 53.3 0.4 char
Duringearlytraining,thereisgoodsignalfrom
OBQA 0.0 0.0 0.0 100.0 0.0 pmi
CFwhileMCFisrandom.Around400Btokens,
PIQA 6.7 46.7 46.7 0.0 0.2 char
themodelstartsgainingtheabilityontheMCF
SIQA 0.0 86.7 6.7 6.7 0.1 char
format,becomingastrongersignalthanCF.
WinoG 100.0 0.0 0.0 0.0 0.0 none
higherandmorerepresentativescores(Robinsonetal.,2023;OpenAI,2024). SeeAppendixC.1for
furtherdiscussion.
Wecanseeanexplicitexampleofamodelacquiringthe“understanding”ofMCFduringtraining
inFigure1,showingtheOLMo-1.7-7Bmodel(Groeneveldetal.,2024;AI2blog,2024)evaluated
onthe MMLU validationsetinbothCFandMCFvariations. Theplotsuggeststhatmodelstarts
learningtheMCFtaskformatafterabout400billiontrainingtokens,soinearlytrainingCFprovides
abettersignal,whileMCFissignificantlybetterinlatetraining.
Tofurtherstudythisphenomenon,weevaluatetheCFandMCFversionsoneachtaskandmodel.7
Figure2showsforeachtask,theCFandMCFperformanceforthe15modelsorderedalongthex-axis
byoverallperformanceonalltasks. Forinstance,onARC-CHALLENGE,weseeacleardistinction
wheretheweakest8modelshavenear-randomperformanceontheMCFversionofthetask,yetabove
randomwhenusingCFwhichoffersabettersignaltotherelativestrengthofmodels. Forthestronger
models,theMCFversionclearlyoutscorestheCFversion,andisamuchbetterrepresentationof
taskperformancecomparedtotheflattertrendsusingCF(forLlama3-70BtheMCFscoreis93.7%
(6.3%error)whiletheCFscoreisjust69.0%AR(3C1C%hearllreonr)g,eanearly5xdifferenceinerrorrate!).
1.0
ARC-Challenge ARC-Easy BoolQ CommonsenseQA HellaSwag
1.0 1.0 1.0 1.0 1.0
0.8
0.8 0.8 0.8 0.8 0.8
0.6 0.6 0.6 0.6 0.6
0.4 0.40.6 0.4 0.4 0.4
0.2 0.2 0.2 0.2 0.2
0.0 0.0 0.0 0.0 0.0
5 10 15 5 10 15 5 10 15 5 10 15 5 10 15
MMLU 0.4OpenbookQA PIQA SocialIQa WinoGrande
1.0 1.0 1.0 1.0 1.0
0.8 0.8 0.8 0.8 0.8
0.2
0.6 0.6 0.6 0.6 0.6
0.4 0.4 0.4 0.4 0.4
0.2 0.20.0 0.2 0.2 0.2
0.0 0.0 50.0 10 0.0 15 0.0
5 10 15 5 10 15 5 10 15 5 10 15 5 10 15
MCF CF random
Figure2: ComparingmodelperformanceoneachtaskforMCFvsCF.The15modelsareordered
along the x-axis by overall performance across all 10 tasks. In general, CF is needed to elicit a
non-randomsignalfromtheweakermodels,whilestrongermodelscantakeadvantageofMCFfora
moreaccurateassessment.
7MoredetailednumberscanbefoundinTables5and6intheAppendixB
7
ycaruccATable4: ReproducibleperformancescoresacrossmodelsandtasksusingOLMES,providingrobust,
meaningfulcomparisonsacrossawiderangeofmodelsandtasks. †indicatesuseoftheMCFscore.
model ARC_CARC_EBoolQCSQAHSwagMMLUOBQAPIQASIQAWinoGaverage
Pythia-1B 31.4 63.4 56.8† 50.9 48.0 31.1 40.4 68.9 46.4 52.7 49.0
OLMo-1B 38.6 68.3 51.3 62.2 65.2 33.4 47.6 74.1 51.5 59.3 55.1
TinyLlama-1.1B 38.1 69.5 63.6 61.1 60.8 33.6 45.0 71.7 50.4 60.1 55.4
Pythia-6.7B 44.6 72.6 68.7 62.1 66.1 37.7 50.4 74.9 51.7 62.3 59.1
RPJ-INCITE-7B 45.3 78.8 72.0 69.2 72.8 40.1 49.0 75.9 56.6 68.0 62.8
StableLM2-1.6B 50.6† 75.3 82.3 70.4† 70.3 40.4† 56.6† 75.6 64.3† 65.7 65.1
OLMo-7B 46.4 78.9 78.7 70.8 78.1 40.5 55.8 78.5 56.5 68.5 65.3
MPT-7b 45.7 78.0 82.4 70.9 79.6 40.6 52.4 79.2 57.4 70.2 65.6
Falcon-7B 49.7 80.6 78.2 73.4 79.0 42.1 55.2 79.0 60.1 71.3 66.9
Llama2-7B 54.2 84.0 86.1 74.2 78.9 46.2† 57.8 77.5 59.6 71.7 69.0
Llama2-13B 67.3† 85.9 86.7 74.0 83.9 55.8† 65.4† 80.2 65.9† 74.9 74.0
OLMo-1.7-7B 66.9† 83.6† 85.9 85.8† 80.1 54.4† 68.6† 80.3 76.1† 73.6 75.5
Llama3-8B 79.3† 92.4† 87.5 73.9† 81.8 66.6† 77.2† 81.6 70.2† 76.2 78.7
Mistral-7B-v0.1 78.6† 90.8† 89.3 72.4† 83.0 64.0† 80.6† 82.8 71.3† 77.9 79.1
Llama3-70B 93.7† 97.7† 91.7† 83.2† 89.5 79.8† 93.4† 91.6† 78.9† 84.1 88.4
A similar pattern can be seen across other tasks in Figure 2, where the stronger models show
performanceusingMCFeitherexceedingCF(likeARC-EASY,OPENBOOKQA,MMLU,SOCIALIQA,
COMMONSENSEQA,andPIQA)oratleastcatchinguptoit(HELLASWAG,WINOGRANDE,BOOLQ).8
InOLMES,westandardizetoevaluateeachmodelusingboththeMCFandCFformulations,and
thebestperformingoneisused. Thisallowsformeaningfulcomparisonoftaskevaluationnumbers
over a range of models, from the smaller, weaker base models which can only deal with the CF
(whereMCFscoreshoveringaroundrandombaseline),tothestrongermodelswhichcanreportmore
accurateperformanceusingtheMCF(whereCFprovideslessclearsignal).
4 OLMES:Summaryandresults
OLMESincludesthefollowingelements,justifiedindetailabove:
• Usetestsetwhenavailable,otherwisevalidation. Sample1000instancesifmorethan1500
• Usespecified,exactpromptformat(Section3.1)
• Usefixed,curated5-shotexamples(Section3.2)
• UseprescribedprobabilitynormalizationforCF(Section 3.3)
• EvaluatewithbothMCFandCF,usethebestresult(Section3.4)
• Followrecommendationsforallotherevaluationdetails(inAppendixA)
Table4reportsscoresfordifferentmodelsonpopularbenchmarktasks,followingourevaluation
standard,OLMES.
5 Relatedwork
Withmodelreleases,performanceonpopulardatasetsisusedtogaugetheprogressachievede.g.,
OpenAI(2024)tshowingsuperhumanperformanceonbenchmarkslikeMMLU.Suchevaluationalso
guidesthecommunityeffortstowardsunderstandingandsharingfindingsonwhatittakestobuilda
strongmodel(Touvronetal.(2023a,b);Bidermanetal.(2023);Almazroueietal.(2023);MosaicML
(2023);Jiangetal.(2023);GemmaTeametal.(2024);Groeneveldetal.(2024)interalia).
However,givenamodelandadataset,evenforthefrequentlyuseddatasets,therearevariedpractices
inhowaccuracyonthemismeasured. Variousworkhasshownthatmodelevaluationsarevulnerable
todifferencessuchasoptionpositionchangesinmultiple-choicequestions(Zhengetal.,2024;Li
8AppendixC.2.1providesfurtherdiscussion.
8etal.,2024),choicesymbols,re-orderingofansweroptions,changingnumberofansweroptions
(Wangetal.,2024),andtaskformulation(Alzahranietal.,2024;Robinsonetal.,2023;Khatunand
Brown,2024;Wiegreffeetal.,2023). Evenminorformattingchangescancauselarge,generally
arbitrary,scorevariations(Sclaretal.,2023). TheHolisticEvaluationofLanguageModels(HELM)
benchmark(Liangetal.,2023),theHuggingFaceOpenLLMLeaderboard(Beechingetal.,2023),
MosaicEvalGauntlet(Barton,2024)andEleutherAILMHarness(Gaoetal.,2023;Bidermanetal.,
2024)presenteffortstowardgreatertransparencyandreproducibilityofLMevaluations. Liangetal.
(2023);Gaoetal.(2023);Bidermanetal.(2024)describeandprovidesupportforvariabilityfactors,
presentingthemasopenchoicestoresearchersandusers. Evenwhenspecificationsaremade,the
rationaleisnotalwaysdocumentedandthusnotfollowedbyothersinsubsequentwork(seeTable1).
6 Discussion
By identifying and reviewing common evaluation practices in the community, and performing
experimentstoinvestigateopenquestions,wepresentOLMES–anopen,documented,reproducible,
andpracticalevaluationstandard.
Futureworkandlimitations. OureffortsforfutureworkincludeaddingmoretaskstoOLMES,
covering tasks beyond MCQA such as generative tasks and chain-of-thought prompting. This
encompassesstandardizinghowanswersareextractedforevaluation,andforchatmodelshowtosplit
thepromptintomessages. WealsoencouragethecommunitytocontributetoOLMES,extendingthe
principlesofOLMEStonewtasks.
WehopeOLMESwillbeameaningfulsteptowardsstandardizingevaluations, andincorporated
intoevaluationcodebasesforbroadusage. Thiswillfacilitaterobustandsimplifiedcomparisonsof
modelperformances,bothforresearchersduringmodeltraininganddevelopmentandfordevelopers
choosingmodelstobuildupon.
AcknowledgmentsandDisclosureofFunding
In creating this evaluation standard, OLMES, we build on top of the various previous efforts on
languagemodelevaluationinthecommunity–includingpreviousworkonlanguagemodelevaluation
standardization,themanyopenresearchreportsdisclosinghowevaluationonLMshavebeendone,
andthedatasetsthatmadeOLMESpossible,whichweexplicitlyciteandacknowledgeinourpaper.
References
AI2 blog (2024). Olmo 1.7–7b: A 24 point improvement on mmlu. https://blog.allenai.
org/olmo-1-7-7b-a-24-point-improvement-on-mmlu-92b43f7d269d. Accessed: 2024-
06-03.
AI@Meta (2024). Llama 3 model card. https://github.com/meta-llama/llama3/blob/
main/MODEL_CARD.md. Accessed: 2024-05-29.
Almazrouei, E., Alobeidli, H., Alshamsi, A., Cappelli, A., Cojocaru, R., Debbah, M., Étienne
Goffinet,Hesslow,D.,Launay,J.,Malartic,Q.,Mazzotta,D.,Noune,B.,Pannier,B.,andPenedo,
G.(2023). Thefalconseriesofopenlanguagemodels. arXiv:2311.16867.
Alzahrani,N.,Alyahya,H.A.,Alnumay,Y.,Alrashed,S.,Alsubaie,S.,Almushaykeh,Y.,Mirza,F.,
Alotaibi,N.,Altwairesh,N.,Alowisheq,A.,Bari,M.S.,andKhan,H.(2024). Whenbenchmarks
aretargets: Revealingthesensitivityoflargelanguagemodelleaderboards. arXiv:2402.01781.
Anthropic(2024).Theclaude3modelfamily:Opus,sonnet,haiku.https://www-cdn.anthropic.
com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf. Ac-
cessed: 2024-06-03.
Barton,T.(2024). Calibratingthemosaicevaluationgauntlet. https://www.databricks.com/
blog/calibrating-mosaic-evaluation-gauntlet. Accessed: 2024-05-05.
9Beeching, E., Fourrier, C., Habib, N., Han, S., Lambert, N., Rajani, N., Sanseviero, O., Tun-
stall, L., and Wolf, T. (2023). Open llm leaderboard. https://huggingface.co/spaces/
HuggingFaceH4/open_llm_leaderboard.
Bellagente, M., Tow, J., Mahan, D., Phung, D., Zhuravinskyi, M., Adithyan, R., Baicoianu, J.,
Brooks,B.,Cooper,N.,Datta,A.,Lee,M.,Mostaque,E.,Pieler,M.,Pinnaparju,N.,Rocha,P.,
Saini,H.,Teufel,H.,Zanichelli,N.,andRiquelme,C.(2024). StableLM21.6btechnicalreport.
arXiv:2402.17834.
Biderman,S.,Schoelkopf,H.,Anthony,Q.G.,Bradley,H.,O’Brien,K.,Hallahan,E.,Khan,M.A.,
Purohit,S.,Prashanth,U.S.,Raff,E.,etal.(2023). Pythia: Asuiteforanalyzinglargelanguage
models across training and scaling. In International Conference on Machine Learning, pages
2397–2430.PMLR.
Biderman,S.,Schoelkopf,H.,Sutawika,L.,Gao,L.,Tow,J.,Abbasi,B.,Aji,A.F.,Ammanamanchi,
P.S.,Black,S.,Clive,J.,DiPofi,A.,Etxaniz,J.,Fattori,B.,Forde,J.Z.,Foster,C.,Jaiswal,M.,
Lee,W.Y.,Li,H.,Lovering,C.,Muennighoff,N.,Pavlick,E.,Phang,J.,Skowron,A.,Tan,S.,
Tang,X.,Wang,K.A.,Winata,G.I.,Yvon,F.,andZou,A.(2024). Lessonsfromthetrencheson
reproducibleevaluationoflanguagemodels. arXiv:2405.14782.
Bisk, Y., Zellers, R., Lebras, R., Gao, J., andChoi, Y.(2020). PIQA:Reasoningaboutphysical
commonsenseinnaturallanguage. ProceedingsoftheAAAIConferenceonArtificialIntelligence,
34(05):7432–7439.
Bommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., Bernstein, M. S.,
Bohg,J.,Bosselut,A.,Brunskill,E.,Brynjolfsson,E.,Buch,S.,Card,D.,Castellon,R.,Chatterji,
N.,Chen,A.,Creel,K.,Davis,J.Q.,Demszky,D.,Donahue,C.,Doumbouya,M.,Durmus,E.,
Ermon,S.,Etchemendy,J.,Ethayarajh,K.,Fei-Fei,L.,Finn,C.,Gale,T.,Gillespie,L.,Goel,K.,
Goodman,N.,Grossman,S.,Guha,N.,Hashimoto,T.,Henderson,P.,Hewitt,J.,Ho,D.E.,Hong,
J.,Hsu,K.,Huang,J.,Icard,T.,Jain,S.,Jurafsky,D.,Kalluri,P.,Karamcheti,S.,Keeling,G.,
Khani,F.,Khattab,O.,Koh,P.W.,Krass,M.,Krishna,R.,Kuditipudi,R.,Kumar,A.,Ladhak,
F.,Lee,M.,Lee,T.,Leskovec,J.,Levent,I.,Li,X.L.,Li,X.,Ma,T.,Malik,A.,Manning,C.D.,
Mirchandani, S., Mitchell, E., Munyikwa, Z., Nair, S., Narayan, A., Narayanan, D., Newman,
B.,Nie,A.,Niebles,J.C.,Nilforoshan,H.,Nyarko,J.,Ogut,G.,Orr,L.,Papadimitriou,I.,Park,
J.S.,Piech,C.,Portelance,E.,Potts,C.,Raghunathan,A.,Reich,R.,Ren,H.,Rong,F.,Roohani,
Y.,Ruiz,C.,Ryan,J.,Ré,C.,Sadigh,D.,Sagawa,S.,Santhanam,K.,Shih,A.,Srinivasan,K.,
Tamkin,A.,Taori,R.,Thomas,A.W.,Tramèr,F.,Wang,R.E.,Wang,W.,Wu,B.,Wu,J.,Wu,
Y.,Xie,S.M.,Yasunaga,M.,You,J.,Zaharia,M.,Zhang,M.,Zhang,T.,Zhang,X.,Zhang,Y.,
Zheng,L.,Zhou,K.,andLiang,P.(2022). Ontheopportunitiesandrisksoffoundationmodels.
arXiv:2108.07258.
Brown,T.,Mann,B.,Ryder,N.,Subbiah,M.,Kaplan,J.D.,Dhariwal,P.,Neelakantan,A.,Shyam,
P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R.,
Ramesh,A.,Ziegler,D.,Wu,J.,Winter,C.,Hesse,C.,Chen,M.,Sigler,E.,Litwin,M.,Gray,
S.,Chess,B.,Clark,J.,Berner,C.,McCandlish,S.,Radford,A.,Sutskever,I.,andAmodei,D.
(2020). Language models are few-shot learners. In Larochelle, H., Ranzato, M., Hadsell, R.,
Balcan,M.,andLin,H.,editors,AdvancesinNeuralInformationProcessingSystems,volume33,
pages1877–1901.CurranAssociates,Inc.
Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung,
H. W., Sutton, C., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko, S., Maynez, J., Rao, A.,
Barnes, P., Tay, Y., Shazeer, N., Prabhakaran, V., Reif, E., Du, N., Hutchinson, B., Pope, R.,
Bradbury,J.,Austin,J.,Isard,M.,Gur-Ari,G.,Yin,P.,Duke,T.,Levskaya,A.,Ghemawat,S.,
Dev,S.,Michalewski,H.,Garcia,X.,Misra,V.,Robinson,K.,Fedus,L.,Zhou,D.,Ippolito,D.,
Luan,D.,Lim,H.,Zoph,B.,Spiridonov,A.,Sepassi,R.,Dohan,D.,Agrawal,S.,Omernick,M.,
Dai,A.M.,Pillai,T.S.,Pellat,M.,Lewkowycz,A.,Moreira,E.,Child,R.,Polozov,O.,Lee,K.,
Zhou,Z.,Wang,X.,Saeta,B.,Diaz,M.,Firat,O.,Catasta,M.,Wei,J.,Meier-Hellstern,K.,Eck,
D.,Dean,J.,Petrov,S.,andFiedel,N.(2022). PaLM:Scalinglanguagemodelingwithpathways.
arXiv:2204.02311.
Clark,C.,Lee,K.,Chang,M.-W.,Kwiatkowski,T.,Collins,M.,andToutanova,K.(2019). BoolQ:
Exploring the surprising difficulty of natural yes/no questions. In Burstein, J., Doran, C., and
10Solorio,T.,editors,Proceedingsofthe2019ConferenceoftheNorthAmericanChapterofthe
Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long
andShortPapers),pages2924–2936,Minneapolis,Minnesota.AssociationforComputational
Linguistics.
Clark,P.,Cowhey,I.,Etzioni,O.,Khot,T.,Sabharwal,A.,Schoenick,C.,andTafjord,O.(2018).
Think you have solved question answering? Try ARC, the AI2 reasoning challenge. CoRR,
arXiv:1803.05457.
Du, N., Huang, Y., Dai, A.M., Tong, S., Lepikhin, D., Xu, Y., Krikun, M., Zhou, Y., Yu, A.W.,
Firat,O.,etal.(2022). Glam: Efficientscalingoflanguagemodelswithmixture-of-experts. In
InternationalConferenceonMachineLearning,pages5547–5569.PMLR.
Gao,L.(2021). MultiplechoicenormalizationinLMevaluation. https://blog.eleuther.ai/
multiple-choice-normalization/. Accessed: 2024-05-08.
Gao, L., Tow, J., Abbasi, B., Biderman, S., Black, S., DiPofi, A., Foster, C., Golding, L., Hsu,
J.,LeNoac’h,A.,Li,H.,McDonell,K.,Muennighoff,N.,Ociepa,C.,Phang,J.,Reynolds,L.,
Schoelkopf,H.,Skowron,A.,Sutawika,L.,Tang,E.,Thite,A.,Wang,B.,Wang,K.,andZou,A.
(2023). Aframeworkforfew-shotlanguagemodelevaluation. https://zenodo.org/records/
10256836.
GemmaTeam,Mesnard,T.,Hardin,C.,Dadashi,R.,Bhupatiraju,S.,Pathak,S.,Sifre,L.,Rivière,
M.,Kale,M.S.,Love,J.,Tafti,P.,Hussenot,L.,Sessa,P.G.,Chowdhery,A.,Roberts,A.,Barua,
A.,Botev,A.,Castro-Ros,A.,Slone,A.,Héliou,A.,Tacchetti,A.,Bulanova,A.,Paterson,A.,
Tsai,B.,Shahriari,B.,Lan,C.L.,Choquette-Choo,C.A.,Crepy,C.,Cer,D.,Ippolito,D.,Reid,
D.,Buchatskaya,E.,Ni,E.,Noland,E.,Yan,G.,Tucker,G.,Muraru,G.-C.,Rozhdestvenskiy,G.,
Michalewski,H.,Tenney,I.,Grishchenko,I.,Austin,J.,Keeling,J.,Labanowski,J.,Lespiau,J.-B.,
Stanway,J.,Brennan,J.,Chen,J.,Ferret,J.,Chiu,J.,Mao-Jones,J.,Lee,K.,Yu,K.,Millican,K.,
Sjoesund,L.L.,Lee,L.,Dixon,L.,Reid,M.,Mikuła,M.,Wirth,M.,Sharman,M.,Chinaev,N.,
Thain,N.,Bachem,O.,Chang,O.,Wahltinez,O.,Bailey,P.,Michel,P.,Yotov,P.,Chaabouni,R.,
Comanescu,R.,Jana,R.,Anil,R.,McIlroy,R.,Liu,R.,Mullins,R.,Smith,S.L.,Borgeaud,S.,
Girgin,S.,Douglas,S.,Pandya,S.,Shakeri,S.,De,S.,Klimenko,T.,Hennigan,T.,Feinberg,V.,
Stokowiec,W.,huiChen,Y.,Ahmed,Z.,Gong,Z.,Warkentin,T.,Peran,L.,Giang,M.,Farabet,
C., Vinyals, O., Dean, J., Kavukcuoglu, K., Hassabis, D., Ghahramani, Z., Eck, D., Barral, J.,
Pereira, F., Collins, E., Joulin, A., Fiedel, N., Senter, E., Andreev, A., andKenealy, K.(2024).
Gemma: Openmodelsbasedongeminiresearchandtechnology. arXiv:2403.08295.
Groeneveld,D.,Beltagy,I.,Walsh,P.,Bhagia,A.,Kinney,R.,Tafjord,O.,Jha,A.H.,Ivison,H.,
Magnusson,I.,Wang,Y.,Arora,S.,Atkinson,D.,Authur,R.,Chandu,K.R.,Cohan,A.,Dumas,
J.,Elazar,Y.,Gu,Y.,Hessel,J.,Khot,T.,Merrill,W.,Morrison,J.,Muennighoff,N.,Naik,A.,
Nam,C.,Peters,M.E.,Pyatkin,V.,Ravichander,A.,Schwenk,D.,Shah,S.,Smith,W.,Strubell,
E.,Subramani,N.,Wortsman,M.,Dasigi,P.,Lambert,N.,Richardson,K.,Zettlemoyer,L.,Dodge,
J.,Lo,K.,Soldaini,L.,Smith,N.A.,andHajishirzi,H.(2024). OLMo: Acceleratingthescience
oflanguagemodels. arXiv:2402.00838.
Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. (2021).
Measuringmassivemultitasklanguageunderstanding.ProceedingsoftheInternationalConference
onLearningRepresentations(ICLR).
Holtzman,A.,West,P.,Shwartz,V.,Choi,Y.,andZettlemoyer,L.(2021). Surfaceformcompetition:
Whythehighestprobabilityanswerisn’talwaysright. InMoens,M.-F.,Huang,X.,Specia,L.,
andYih,S.W.-t.,editors,Proceedingsofthe2021ConferenceonEmpiricalMethodsinNatural
LanguageProcessing,pages7038–7051,OnlineandPuntaCana,DominicanRepublic.Association
forComputationalLinguistics.
Jiang,A.Q.,Sablayrolles,A.,Mensch,A.,Bamford,C.,Chaplot,D.S.,delasCasas,D.,Bressand,
F.,Lengyel,G.,Lample,G.,Saulnier,L.,Lavaud,L.R.,Lachaux,M.-A.,Stock,P.,Scao,T.L.,
Lavril,T.,Wang,T.,Lacroix,T.,andSayed,W.E.(2023). Mistral7B. arXiv:2310.06825.
Khatun,A.andBrown,D.G.(2024).Astudyonlargelanguagemodels’limitationsinmultiple-choice
questionanswering. arXiv:2401.07955.
11Li,W.,Li,L.,Xiang,T.,Liu,X.,Deng,W.,andGarcia,N.(2024). Canmultiple-choicequestions
reallybeusefulindetectingtheabilitiesofLLMs? InLREC-COLING2024.
Liang, P., Bommasani, R., Lee, T., Tsipras, D., Soylu, D., Yasunaga, M., Zhang, Y., Narayanan,
D.,Wu,Y.,Kumar,A.,Newman,B.,Yuan,B.,Yan,B.,Zhang,C.,Cosgrove,C.A.,Manning,
C.D.,Re,C.,Acosta-Navas,D.,Hudson,D.A.,Zelikman,E.,Durmus,E.,Ladhak,F.,Rong,F.,
Ren,H.,Yao,H.,WANG,J.,Santhanam,K.,Orr,L.,Zheng,L.,Yuksekgonul,M.,Suzgun,M.,
Kim,N.,Guha,N.,Chatterji,N.S.,Khattab,O.,Henderson,P.,Huang,Q.,Chi,R.A.,Xie,S.M.,
Santurkar,S.,Ganguli,S.,Hashimoto,T.,Icard,T.,Zhang,T.,Chaudhary,V.,Wang,W.,Li,X.,
Mai,Y.,Zhang,Y.,andKoreeda,Y.(2023). Holisticevaluationoflanguagemodels. Transactions
onMachineLearningResearch.
Lieber,O.,Sharir,O.,Lenz,B.,andShoham,Y.(2021). Jurassic-1: Technicaldetailsandevaluation.
WhitePaper.AI21Labs.
Mai,Y.andLiang,P.(2024). Massivemultitasklanguageunderstanding(MMLU)onhelm. https:
//crfm.stanford.edu/2024/05/01/helm-mmlu.html. Accessed: 2024-05-29.
Mihaylov,T.,Clark,P.,Khot,T.,andSabharwal,A.(2018). Canasuitofarmorconductelectricity?
anewdatasetforopenbookquestionanswering. InRiloff,E.,Chiang,D.,Hockenmaier,J.,and
Tsujii,J.,editors,Proceedingsofthe2018ConferenceonEmpiricalMethodsinNaturalLanguage
Processing,pages2381–2391,Brussels,Belgium.AssociationforComputationalLinguistics.
MosaicML(2023). IntroducingMPT-7B:Anewstandardforopen-source, commerciallyusable
LLMs. https://www.databricks.com/blog/mpt-7b. Accessed: 2024-05-08.
MosaicML (2024). Mosaic eval gauntlet v0.3.0 - evaluation suite. https://github.com/
mosaicml/llm-foundry/blob/main/scripts/eval/local_data/EVAL_GAUNTLET.md.
Accessed: 2024-05-29.
Nori,H.,King,N.,McKinney,S.M.,Carignan,D.,andHorvitz,E.(2023). Capabilitiesofgpt-4on
medicalchallengeproblems. arXiv:2303.13375.
OpenAI(2024). GPT-4technicalreport. arXiv:2303.08774.
Robinson, J., Rytting, C. M., and Wingate, D. (2023). Leveraging Large Language Models for
MultipleChoiceQuestionAnswering. ProceedingsoftheInternationalConferenceonLearning
Representations(ICLR).
Sakaguchi, K., LeBras, R., Bhagavatula, C., and Choi, Y. (2020). WinoGrande: An adversarial
winogradschemachallengeatscale. ProceedingsoftheAAAIConferenceonArtificialIntelligence,
34(05):8732–8740.
Sap, M., Rashkin, H., Chen, D., Le Bras, R., and Choi, Y. (2019). Social IQa: Commonsense
reasoningaboutsocialinteractions. InInui,K.,Jiang,J.,Ng,V.,andWan,X.,editors,Proceedings
of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th
InternationalJointConferenceonNaturalLanguageProcessing(EMNLP-IJCNLP),pages4463–
4473,HongKong,China.AssociationforComputationalLinguistics.
Sclar,M.,Choi,Y.,Tsvetkov,Y.,andSuhr,A.(2023). Quantifyinglanguagemodels’sensitivityto
spuriousfeaturesinpromptdesignor: Howilearnedtostartworryingaboutpromptformatting.
arXiv:2310.11324.
Smith,S.,Patwary,M.,Norick,B.,LeGresley,P.,Rajbhandari,S.,Casper,J.,Liu,Z.,Prabhumoye,S.,
Zerveas,G.,Korthikanti,V.,etal.(2022). Usingdeepspeedandmegatrontotrainmegatron-turing
nlg530b,alarge-scalegenerativelanguagemodel. arXiv:2201.11990.
Talmor,A.,Herzig,J.,Lourie,N.,andBerant,J.(2019). CommonsenseQA:Aquestionanswering
challengetargetingcommonsenseknowledge. InBurstein,J.,Doran,C.,andSolorio,T.,editors,
Proceedingsofthe2019ConferenceoftheNorthAmericanChapteroftheAssociationforCompu-
tationalLinguistics: HumanLanguageTechnologies,Volume1(LongandShortPapers),pages
4149–4158,Minneapolis,Minnesota.AssociationforComputationalLinguistics.
TogetherComputer(2023). RedPajama: anopendatasetfortraininglargelanguagemodels.
12Touvron,H.,Lavril,T.,Izacard,G.,Martinet,X.,Lachaux,M.-A.,Lacroix,T.,Rozière,B.,Goyal,
N.,Hambro,E.,Azhar,F.,Rodriguez,A.,Joulin,A.,Grave,E.,andLample,G.(2023a). LLaMA:
Openandefficientfoundationlanguagemodels. arXiv:2302.13971.
Touvron,H.,Martin,L.,Stone,K.,Albert,P.,Almahairi,A.,Babaei,Y.,Bashlykov,N.,Batra,S.,
Bhargava,P.,Bhosale,S.,Bikel,D.,Blecher,L.,Ferrer,C.C.,Chen,M.,Cucurull,G.,Esiobu,D.,
Fernandes,J.,Fu,J.,Fu,W.,Fuller,B.,Gao,C.,Goswami,V.,Goyal,N.,Hartshorn,A.,Hosseini,
S.,Hou,R.,Inan,H.,Kardas,M.,Kerkez,V.,Khabsa,M.,Kloumann,I.,Korenev,A.,Koura,P.S.,
Lachaux,M.-A.,Lavril,T.,Lee,J.,Liskovich,D.,Lu,Y.,Mao,Y.,Martinet,X.,Mihaylov,T.,
Mishra,P.,Molybog,I.,Nie,Y.,Poulton,A.,Reizenstein,J.,Rungta,R.,Saladi,K.,Schelten,A.,
Silva,R.,Smith,E.M.,Subramanian,R.,Tan,X.E.,Tang,B.,Taylor,R.,Williams,A.,Kuan,
J.X.,Xu,P.,Yan,Z.,Zarov,I.,Zhang,Y.,Fan,A.,Kambadur,M.,Narang,S.,Rodriguez,A.,
Stojnic,R.,Edunov,S.,andScialom,T.(2023b). Llama2: Openfoundationandfine-tunedchat
models. arXiv:2307.09288.
Wang, H., Zhao, S., Qiang, Z., Qin, B., andLiu, T.(2024). Beyondtheanswers: Reviewingthe
rationality of multiple choice question answering for the evaluation of large language models.
arXiv:2402.01349.
Wiegreffe,S.,Finlayson,M.,Tafjord,O.,Clark,P.,andSabharwal,A.(2023). Increasingprobability
massonanswerchoicesdoesnotalwaysimproveaccuracy. InBouamor,H.,Pino,J.,andBali,
K., editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language
Processing,pages8392–8417,Singapore.AssociationforComputationalLinguistics.
Zellers,R.,Holtzman,A.,Bisk,Y.,Farhadi,A.,andChoi,Y.(2019). HellaSwag: Canamachine
reallyfinishyoursentence? InKorhonen,A.,Traum,D.,andMàrquez,L.,editors,Proceedings
ofthe57thAnnualMeetingoftheAssociationforComputationalLinguistics,pages4791–4800,
Florence,Italy.AssociationforComputationalLinguistics.
Zhang,P.,Zeng,G.,Wang,T.,andLu,W.(2024). Tinyllama: Anopen-sourcesmalllanguagemodel.
arXiv:2401.02385.
Zheng,C.,Zhou,H.,Meng,F.,Zhou,J.,andHuang,M.(2024). LargeLanguageModelsAreNot
Robust Multiple Choice Selectors. Proceedings of the International Conference on Learning
Representations(ICLR).
13Checklist
1. Forallauthors...
(a) Dothemainclaimsmadeintheabstractandintroductionaccuratelyreflectthepaper’s
contributionsandscope? [Yes]
(b) Didyoudescribethelimitationsofyourwork? [Yes] InDiscussionsection.
(c) Didyoudiscussanypotentialnegativesocietalimpactsofyourwork? [N/A]
(d) Haveyoureadtheethicsreviewguidelinesandensuredthatyourpaperconformsto
them? [N/A]
2. Ifyouareincludingtheoreticalresults...
(a) Didyoustatethefullsetofassumptionsofalltheoreticalresults? [N/A]
(b) Didyouincludecompleteproofsofalltheoreticalresults? [N/A]
3. Ifyouranexperiments(e.g. forbenchmarks)...
(a) Didyouincludethecode,data,andinstructionsneededtoreproducethemainexperi-
mentalresults(eitherinthesupplementalmaterialorasaURL)?[Yes] URLatthe
endofIntroductionsection.
(b) Didyouspecifyallthetrainingdetails(e.g.,datasplits,hyperparameters,howthey
werechosen)? [N/A]
(c) Didyoureporterrorbars(e.g.,withrespecttotherandomseedafterrunningexperi-
mentsmultipletimes)? [N/A]
(d) Didyouincludethetotalamountofcomputeandthetypeofresourcesused(e.g.,type
ofGPUs,internalcluster,orcloudprovider)? [Yes] SeeAppendixE.
4. Ifyouareusingexistingassets(e.g.,code,data,models)orcurating/releasingnewassets...
(a) Ifyourworkusesexistingassets,didyoucitethecreators? [Yes] InSection2.
(b) Didyoumentionthelicenseoftheassets? [N/A] Allmodelsanddatasetsusedcanbe
researchforresearchpurposes.
(c) DidyouincludeanynewassetseitherinthesupplementalmaterialorasaURL?[Yes]
(d) Didyoudiscusswhetherandhowconsentwasobtainedfrompeoplewhosedatayou’re
using/curating? [N/A]
(e) Didyoudiscusswhetherthedatayouareusing/curatingcontainspersonallyidentifiable
informationoroffensivecontent? [Yes] InAppendixF.
5. Ifyouusedcrowdsourcingorconductedresearchwithhumansubjects...
(a) Didyouincludethefulltextofinstructionsgiventoparticipantsandscreenshots,if
applicable? [N/A]
(b) Did you describe any potential participant risks, with links to Institutional Review
Board(IRB)approvals,ifapplicable? [N/A]
(c) Didyouincludetheestimatedhourlywagepaidtoparticipantsandthetotalamount
spentonparticipantcompensation? [N/A]
14A OLMES:implementationdetails
Thereareotherimportantdetailsthatgointoafullyspecifiedevaluationresult,andweenumeratethe
choicesmadeinOLMEShere:
• For MMLU: use macro average (over 57 tasks) rather than micro average (over 14042
instances),followingAI@Meta(2024). Thisbetterrepresentsthediversityoffieldsinthe
dataset,althoughinpracticeitdoesnotgenerallymakeabigdifference(seeFigure7).
• Whenamodelrequiresit,makesuretoaddtheappropriate<bos>tokenatstartofprompt
(e.g.,Gemma(GemmaTeametal.,2024)).
• Whenusingthe“character”normalizationforCF,includetheleadingspaceinthecalculation
ofanswerlength.
• Restrictallinputs(withcompletions)to2048tokensforconsistencyacrossmodels9
• Usethedefaultmodelprecisionwhenevaluating(i.e.,avoidoptionslikeload_in_8bit
unlessitproducesidenticalresults)
• OLMESusesthestandardapproachoftwonewlinestoseparateeachin-contextexample.
• OtherthantheoriginalinstructionlineforMMLUHendrycksetal.(2021),wedonotadd
anyextrainstructions. Thisisinviewofpreviousworkfindingthesubjectinformationfrom
instructionsmakeslittlechangestomodelranking(Alzahranietal.,2024),andtoreduce
additionalsourcesofvariationintheprompt.
Note that computational details, like batch size and type/state of GPU, can affect floating point
operationssuchthatanswerchoicedecisionscanflipiftheyareveryclose. Thisishardtoavoid
unlessconsider“ties”whenanswersaresufficientlycloseinconfidence.
B DetailedCFandMCFtaskscores
Tables5and 6presentdetailedscoresacrossalltasks,withbothMCFandCFresults(usingthe
OLMESrecommendationsforCFnormalization).
C Furtherdetailsonvariations
InthisappendixwediscussfurtherdetailsonhowLLMevaluationscanvaryandthechoicesmadein
OLMES.
C.1 Taskformulationdetails
LLMevaluationsstartedoutusingtheCFapproachformanytasks(Brownetal.,2020;Duetal.,
2022;Smithetal.,2022;Chowdheryetal.,2022;Lieberetal.,2021),whichisamorereasonable
optionforweakermodelsthatstrugglewiththemorenaturalMCF(KhatunandBrown,2024). The
taskformulationonlyveryrecentlyandgraduallyswitchedtotheMCFapproachwhenitbecame
clearthatthemodelcouldutilizeit,producinghigherscores(Robinsonetal.,2023;OpenAI,2024;
AI@Meta,2024).
The HELM study (Liang et al., 2023) included comparisons between the MCF (“joint”) and CF
(“separate”) approaches, finding that certain models can really benefit from the MCF approach,
althoughamongthemodelsintheoriginalstudyitwasreallyonlytheAnthropic-LMv4-s3(52B)
modelwhichcouldtakefulladvantageofit.
C.2 CFnormalizationdetails
Tables 9, 10and 11showdetailedcomparisonsofCFnormalizationondifferentmodels,forthe
varioustasks.
UnlikeinMCF,wheretheevaluationmetricinvolvesjustscoringthelog-likelihoodcorrespondingto
theanswerchoicelabel(i.e.,A/B/C/...),thereisachoiceoflog-likelihoodnormalization(“none”,
“pertoken”,“percharacter”or“pmi”)forCFasdetailedinSection3.3.
9ForcurrenttasksthisisonlyexhaustedforafewMMLUinstances.
15Table5: ComparingMCFandCFscoresoneachtask(part1). Weakermodelsatthetopofthetable
havenear-randomMCFscores,whileforstrongermodelsatthebottom,theMCFscoreprovidesa
betterassessmentthantheCFscore.
ARC_C ARC_E BoolQ CSQA HSwag MMLU
model MCF CF MCF CF MCF CF MCF CF MCF CF MCF CF
Pythia-1B 24.1 31.4 24.0 63.4 56.8 56.6 21.0 50.9 23.6 48.0 26.5 31.1
OLMo-1B 25.3 38.6 25.4 68.3 37.9 51.3 20.2 62.2 24.6 65.2 26.6 33.4
TinyLlama-1.1B 26.4 38.1 24.3 69.5 60.7 63.6 17.9 61.1 26.2 60.8 26.2 33.6
Pythia-6.7B 26.6 44.6 24.9 72.6 64.0 68.7 20.5 62.1 24.3 66.1 25.4 37.7
RPJ-INCITE-7B 28.1 45.3 25.1 78.8 64.2 72.0 19.7 69.2 23.5 72.8 29.0 40.1
StableLM2-1.6B 50.6 47.3 69.6 75.3 60.1 82.3 70.4 68.2 52.4 70.3 40.4 37.1
OLMo-7B 27.2 46.4 27.0 78.9 67.5 78.7 20.8 70.8 25.0 78.1 28.3 40.5
MPT-7b 27.9 45.7 27.5 78.0 44.6 82.4 20.9 70.9 26.4 79.6 30.0 40.6
Falcon-7B 27.2 49.7 25.3 80.6 48.1 78.2 20.2 73.4 27.7 79.0 28.0 42.1
Llama2-7B 52.6 54.2 70.6 84.0 57.5 86.1 59.2 74.2 41.4 78.9 46.2 44.4
Llama2-13B 67.3 56.2 85.0 85.9 77.8 86.7 68.1 74.0 62.4 83.9 55.8 47.6
OLMo-1.7-7B 66.9 51.2 83.6 81.5 82.0 85.9 85.8 70.4 50.0 80.1 54.4 42.4
Llama3-8B 79.3 57.1 92.4 86.6 84.8 87.5 73.9 69.9 63.8 81.8 66.6 51.1
Mistral-7B-v0.1 78.6 59.6 90.8 86.8 87.2 89.3 72.4 72.3 71.5 83.0 64.0 50.3
Llama3-70B 93.7 69.0 97.7 89.6 91.7 91.2 83.2 75.8 89.1 89.5 79.8 60.7
WhenevaluatingtheGPT-3model(Brownetal.,2020),theyworkedaroundthisissuebynormalizing
thelog-probabilitybythenumberoftokensintheanswer(similartohowlossiscomputedduring
training). Theyalsonotedthatforafewdatasets,itworkedmarkedlybettertoinstead“normalize”by
dividingbyLLMprobabilityofthesameanswerstringwithoutthepresenceofthequestion(usually
byjusthavingagenericprefixlike"Answer: <answer_string>"). Thiscanbeconsideredaform
ofpointwise-mutual-information(PMI)andwasexploredfurtherinotherworks(Holtzmanetal.,
2021).
TheEleutherLMEvaluationHarness(Gaoetal.,2023;Bidermanetal.,2024)andsomesubsequent
evaluations(e.g.,theLlamamodels(Touvronetal.,2023a))havealsoused“peranswercharacter”
normalization,usingtheargumentation(Gao,2021;Bidermanetal.,2024),thatnormalizingper
tokenisproblematicsinceitdependsonthetokenizer. Sincethepurposeofthenormalizationis
simplytoranktheanswerchoiceswithinthemselves(keepingmodelandtokenizerfixed),thisdoes
notseemlikearelevantargument,andindeedanormalizationwhichchangestheprobabilityof“yes”
vs“no”simplybecausethe“no”tokenhasfewercharactersseemproblematic. Inpractice,fortasks
whereanswerseitherrelativelylongorsimilarinlength,thereareminordifferencesbetweenthese
twolengthnormalization.
TheHELMstudy(Liangetal.,2023)includedcomparisonsbetweenthesenormalizationapproaches
foranumberoftasksandmodels(usingtheterms“separate”and“separatecalibrated”for“token”
and“pmi”respectively),eventuallysettlingonadefaultchoiceforeach,notunlikethechoicesin
theGPT-3report(Brownetal.,2020). TheEleutherLMEvaluationHarnessgenerallyreportstwo
metricsforeachmultiple-choicetask: acc(usingthe“none”normalization)andacc_norm(using
the“character”normalization).
C.2.1 TasksthatgenerallypreferCF
HELLASWAG and WINOGRANDE continuetohaveCFscoreshigherthanMCFscoresevenforthe
strongestmodelsthatcanunderstandtheMCFprompt. Thissomewhatsurprisingtendencyseems
correlatedwiththefactthatthesetasksintheCFformatareexactlylikethelanguagemodelingtask
offindingthemostnaturalcontinuationofarunningpieceoftext. Judgingfromthetrendsinthe
plot,itwouldalsobeinterestingtomonitorifasevenmorecapablemodelsaredeveloped,theMCF
scoreswilleventuallysurpassthatoftheCFscores(givenhowclosetheyalreadygettoeachother).
16Table6: ComparingMCFandCFscoresoneachtask(part2),alongwithoverallaverages. The
“max”averagecorrespondstotheOLMESscore,takingthebestofMCFandCFforeachtask.
OBQA PIQA SIQA WinoG averagescores
model MCF CF MCF CF MCF CF MCF CF MCF CF all max
Pythia-1B 26.0 40.4 52.2 68.9 33.5 46.4 50.4 52.7 33.8 49.0 41.4 49.0
OLMo-1B 28.0 47.6 50.6 74.1 32.8 51.5 51.1 59.3 32.3 55.1 43.7 55.1
TinyLlama-1.1B 25.6 45.0 50.2 71.7 34.9 50.4 50.0 60.1 34.2 55.4 44.8 55.4
Pythia-6.7B 26.2 50.4 51.2 74.9 33.5 51.7 49.6 62.3 34.6 59.1 46.9 59.1
RPJ-INCITE-7B 22.6 49.0 53.5 75.9 33.7 56.6 52.1 68.0 35.1 62.8 49.0 62.8
StableLM2-1.6B 56.6 51.0 62.8 75.6 64.3 61.1 53.5 65.7 58.1 63.4 60.7 65.1
OLMo-7B 27.0 55.8 57.2 78.5 35.1 56.5 50.4 68.5 36.6 65.3 50.9 65.3
MPT-7b 29.6 52.4 53.8 79.2 34.4 57.4 51.1 70.2 34.6 65.6 50.1 65.6
Falcon-7B 27.8 55.2 50.5 79.0 33.9 60.1 49.0 71.3 33.8 66.9 50.3 66.9
Llama2-7B 54.8 57.8 63.2 77.5 58.7 59.6 52.4 71.7 55.7 68.8 62.2 69.0
Llama2-13B 65.4 60.8 74.0 80.2 65.9 63.6 56.1 74.9 67.8 71.4 69.6 74.0
OLMo-1.7-7B 68.6 59.8 65.6 80.3 76.1 54.9 56.2 73.6 68.9 68.0 68.5 75.5
Llama3-8B 77.2 56.2 77.3 81.6 70.2 62.6 61.6 76.2 74.7 71.0 72.9 78.7
Mistral-7B-v0.1 80.6 61.0 79.0 82.8 71.3 63.0 59.8 77.9 75.5 72.6 74.1 79.1
Llama3-70B 93.4 69.0 91.6 83.1 78.9 65.6 79.6 84.1 87.9 77.8 82.8 88.4
Table7: MacrovsmicroaveragescoresonMMLU,wheremacroaverageisoverthe57tasksand
microaverageisoverthe14042individualquestions. Ingeneraltherearesmalldifferencesbetween
thetwo.
model MCF-macro MCF-micro CF-macro CF-micro
Pythia-6.7B 25.4 25.2 37.7 37.5
TinyLlama-1.1B 26.2 25.7 33.6 33.5
Pythia-1B 26.5 26.4 31.1 31.2
OLMo-1B 26.6 26.3 33.4 33.6
Falcon-7B 28.0 27.7 42.1 41.9
OLMo-7B 28.3 28.3 40.5 40.7
RPJ-INCITE-7B 29.0 28.4 40.1 40.1
MPT-7b 30.0 29.3 40.6 40.6
StableLM2-1.6B 40.4 39.6 37.1 37.0
Llama2-7B 46.2 45.5 44.4 44.3
OLMo-1.7-7B 54.4 52.8 42.4 42.4
Llama2-13B 55.8 55.5 47.6 47.1
Mistral-7B-v0.1 64.0 63.0 50.3 49.8
Llama3-8B 66.6 65.4 51.1 50.8
Llama3-70B 79.8 79.2 60.7 60.5
17Table8: Normalizationdetails,showingthatourrecommendationsarenotonlysupportedbyreason-
ingusingprinciplesbehindthenormalizationbutalsoclosetotheempiricallybestnormalizationthat
letsyougetthehighestaccuracyforeachmodeloneachtask(see“diff”columns).
ARC_C ARC_E BoolQ CSQA HSwag MMLU OBQA PIQA SIQA
model pmi diff char diffnonediff pmi diff char diff char diff pmi diff char diff char diff
Pythia-1B 31.4 0.0 63.4 0.0 56.6 4.5 50.9 0.0 48.0 0.0 31.1 1.2 40.4 0.0 68.9 1.4 46.4 0.0
OLMo-1B 38.6 0.0 68.3 0.2 51.3 4.7 62.2 0.0 65.2 0.0 33.4 0.8 47.6 0.0 74.1 0.0 51.5 0.0
TinyLlama-1.1B 38.1 0.0 69.5 0.0 63.6 2.2 61.1 0.0 60.8 0.0 33.6 0.9 45.0 0.0 71.7 0.6 50.4 0.0
Pythia-6.7B 44.6 0.0 72.6 0.0 68.7 0.0 62.1 0.2 66.1 0.0 37.7 0.2 50.4 0.0 74.9 0.0 51.7 1.1
RPJ-INCITE-7B 45.3 0.0 78.8 0.0 72.0 2.5 69.2 0.2 72.8 0.0 40.1 0.8 49.0 0.0 75.9 0.1 56.6 0.0
MPT-7b 45.7 0.6 78.0 0.0 82.4 0.0 70.9 0.0 79.6 0.0 40.6 0.0 52.4 0.0 79.2 0.0 57.4 0.0
Falcon-7B 49.7 0.0 80.6 0.0 78.2 0.6 73.4 0.0 79.0 0.0 42.1 0.0 55.2 0.0 79.0 0.2 60.1 0.0
OLMo-7B 46.4 0.0 78.9 0.0 78.7 0.0 70.8 0.0 78.1 0.0 40.5 0.1 55.8 0.0 78.5 0.8 56.5 0.0
StableLM2-1.6B47.3 0.0 75.3 0.0 82.3 0.0 68.2 0.0 70.3 0.0 37.1 1.5 51.0 0.0 75.6 0.3 61.1 0.0
Llama2-7B 54.2 0.0 84.0 0.0 86.1 0.0 74.2 0.0 78.9 0.0 44.4 0.4 57.8 0.0 77.5 0.2 59.6 0.0
OLMo-1.7-7B 51.2 0.0 81.5 0.0 85.9 0.0 70.4 1.1 80.1 0.0 42.4 0.0 59.8 0.0 80.3 0.0 54.9 0.8
Llama2-13B 56.2 0.9 85.9 0.0 86.7 1.5 74.0 0.0 83.9 0.0 47.6 0.0 60.8 0.0 80.2 0.0 63.6 0.0
Llama3-8B 57.1 1.3 86.6 0.0 87.5 0.3 69.9 4.3 81.8 0.0 51.1 0.0 56.2 0.0 81.6 0.0 62.6 0.0
Mistral-7B-v0.1 59.6 0.6 86.8 0.0 89.3 0.0 72.3 2.1 83.0 0.0 50.3 0.0 61.0 0.0 82.8 0.0 63.0 0.0
Llama3-70B 69.0 0.0 89.6 0.8 91.2 0.5 75.8 1.3 89.5 0.0 60.7 0.0 69.0 0.0 83.1 0.1 65.6 0.0
Table9: ComparingCFnormalizationschemes(part1).
ARC_C ARC_E BoolQ
model none char tok pmi best none char tok pmi best none char tok pmi best
Pythia-1B 26.1 28.4 29.0 31.4 pmi 61.9 63.4 60.9 56.5 char 56.6 61.1 56.6 41.0 char
OLMo-1B 32.9 34.4 34.7 38.6 pmi 68.5 68.3 65.8 60.2 none 51.3 56.0 51.3 42.3 char
TinyLlama-1.1B 31.5 34.1 32.2 38.1 pmi 68.6 69.5 64.4 60.4 char 63.6 65.8 63.6 53.6 char
Pythia-6.7B 36.3 39.5 39.0 44.6 pmi 71.4 72.6 70.0 64.1 char 68.7 66.9 68.7 47.6 none
RPJ-INCITE-7B 40.3 43.5 42.9 45.3 pmi 76.1 78.8 75.9 70.1 char 72.0 74.5 72.0 72.4 char
MPT-7b 41.7 46.3 44.7 45.7 char 76.3 78.0 76.2 68.5 char 82.4 79.9 82.4 76.7 none
Falcon-7B 41.6 47.4 47.6 49.7 pmi 77.0 80.6 78.3 69.8 char 78.2 78.8 78.2 77.6 char
OLMo-7B 41.6 45.5 45.0 46.4 pmi 76.7 78.9 77.4 69.6 char 78.7 77.7 78.7 78.6 none
StableLM2-1.6B 42.2 44.3 44.9 47.3 pmi 73.3 75.3 74.4 70.0 char 82.3 82.0 82.3 76.1 none
Llama2-7B 48.4 52.0 50.2 54.2 pmi 81.4 84.0 81.0 74.7 char 86.1 85.6 86.1 80.5 none
OLMo-1.7-7B 45.5 49.3 48.5 51.2 pmi 79.2 81.5 79.7 71.1 char 85.9 83.8 85.9 85.6 none
Llama2-13B 52.4 57.1 54.2 56.2 char 83.9 85.9 82.8 77.6 char 86.7 88.2 86.7 77.5 char
Llama3-8B 53.6 58.4 56.8 57.1 char 85.8 86.6 85.8 76.6 char 87.5 87.8 87.5 67.0 char
Mistral-7B-v0.1 56.1 60.2 58.9 59.6 char 84.7 86.8 84.6 78.6 char 89.3 89.1 89.3 89.2 none
Llama3-70B 65.7 69.0 67.7 69.0 char 89.7 89.6 90.4 82.6 tok 91.2 90.4 91.2 91.7 pmi
averagescores 43.7 47.3 46.4 49.0 NA 77.0 78.7 76.5 70.0 NA 77.4 77.8 77.4 70.5 NA
winpercentage 0.0 33.3 0.0 66.7 pmi 6.7 86.7 6.7 0.0 char 46.7 46.7 0.0 6.7 none
18Table10: ComparingCFnormalizationschemes(part2)
CSQA HSwag MMLU
model none char tok pmi best none char tok pmi best none char tok pmi best
Pythia-1B 47.7 50.9 47.3 50.9 char 39.2 48.0 47.8 41.0 char 29.5 31.1 30.8 32.3 pmi
OLMo-1B 56.8 60.0 57.6 62.2 pmi 50.9 65.2 64.1 49.8 char 31.7 33.4 33.3 34.2 pmi
TinyLlama-1.1B 58.9 60.5 55.9 61.1 pmi 46.9 60.8 59.7 48.5 char 31.2 33.6 33.0 34.5 pmi
Pythia-6.7B 59.5 62.2 58.9 62.1 char 50.4 66.1 65.9 53.5 char 34.9 37.7 37.0 37.9 pmi
RPJ-INCITE-7B 67.7 69.4 67.2 69.2 char 55.7 72.8 71.8 60.6 char 37.4 40.1 40.0 40.9 pmi
MPT-7b 69.6 70.3 69.1 70.9 pmi 60.5 79.6 76.5 61.5 char 37.8 40.6 40.1 40.4 char
Falcon-7B 70.0 70.3 69.5 73.4 pmi 60.7 79.0 78.4 60.0 char 39.3 42.1 41.9 42.1 char
OLMo-7B 69.0 70.0 67.9 70.8 pmi 59.3 78.1 76.3 64.2 char 37.9 40.5 40.5 40.6 pmi
StableLM2-1.6B 63.6 66.3 65.6 68.2 pmi 54.7 70.3 69.7 56.4 char 35.2 37.1 37.1 38.6 pmi
Llama2-7B 70.5 72.7 68.4 74.2 pmi 61.9 78.9 77.1 64.4 char 42.0 44.4 43.9 44.8 pmi
OLMo-1.7-7B 71.6 63.5 59.0 70.4 none 61.4 80.1 77.7 65.2 char 39.9 42.4 42.2 41.8 char
Llama2-13B 72.2 72.7 68.4 74.0 pmi 63.7 83.9 81.0 70.3 char 44.3 47.6 46.7 47.1 char
Llama3-8B 72.0 74.2 73.5 69.9 char 62.8 81.8 80.3 71.1 char 47.5 51.1 50.8 49.6 char
Mistral-7B-v0.1 73.1 73.8 74.4 72.3 tok 64.5 83.0 81.0 70.3 char 46.9 50.3 50.0 49.0 char
Llama3-70B 77.1 77.1 77.1 75.8 char 70.3 89.5 87.1 80.8 char 57.2 60.7 60.5 59.4 char
averagescores 66.6 67.6 65.3 68.4 NA 57.5 74.5 73.0 61.2 NA 39.5 42.2 41.9 42.2 NA
winpercentage 6.7 33.3 6.7 53.3 pmi 0.0 100.0 0.0 0.0 char 0.0 46.7 0.0 53.3 pmi
Table11: ComparingCFnormalizationschemes(part3).
OBQA PIQA SIQA
model none char tok pmi best none char tok pmi best none char tok pmi best
Pythia-1B 20.2 28.6 30.4 40.4 pmi 70.3 68.9 68.8 60.1 none 42.8 46.4 46.0 44.4 char
OLMo-1B 26.0 33.0 38.4 47.6 pmi 73.2 74.1 73.2 59.9 char 45.3 51.5 49.9 47.3 char
TinyLlama-1.1B 24.4 34.8 35.8 45.0 pmi 72.1 71.7 72.3 62.0 tok 45.6 50.4 48.2 48.4 char
Pythia-6.7B 25.8 37.0 37.4 50.4 pmi 74.8 74.9 74.3 63.6 char 48.0 51.7 52.8 49.2 tok
RPJ-INCITE-7B 31.8 40.0 42.8 49.0 pmi 74.9 75.9 76.0 61.9 tok 50.8 56.6 56.0 52.2 char
MPT-7b 31.6 43.8 43.8 52.4 pmi 77.7 79.2 78.1 63.7 char 51.0 57.4 55.9 52.5 char
Falcon-7B 35.2 45.8 44.4 55.2 pmi 78.3 79.0 79.2 63.2 tok 52.9 60.1 57.5 54.4 char
OLMo-7B 33.2 42.8 45.0 55.8 pmi 78.2 78.5 79.3 65.2 tok 50.3 56.5 56.5 52.8 char
StableLM2-1.6B 34.4 41.6 45.2 51.0 pmi 75.2 75.6 75.9 63.6 tok 52.7 61.1 60.7 56.1 char
Llama2-7B 33.8 44.6 45.0 57.8 pmi 76.7 77.5 77.7 62.9 tok 52.6 59.6 58.3 53.6 char
OLMo-1.7-7B 37.2 48.4 49.6 59.8 pmi 78.5 80.3 79.3 66.3 char 53.5 54.9 54.3 55.7 pmi
Llama2-13B 39.2 46.4 48.4 60.8 pmi 78.9 80.2 79.8 66.4 char 56.7 63.6 60.7 56.8 char
Llama3-8B 37.0 47.6 50.0 56.2 pmi 79.7 81.6 81.1 67.5 char 54.6 62.6 60.1 56.4 char
Mistral-7B-v0.1 38.2 48.4 50.0 61.0 pmi 80.8 82.8 81.3 67.4 char 55.6 63.0 60.9 57.5 char
Llama3-70B 47.0 55.0 56.6 69.0 pmi 82.8 83.1 83.2 68.3 tok 59.7 65.6 64.8 57.3 char
averagescores 33.0 42.5 44.2 54.1 NA 76.8 77.6 77.3 64.1 NA 51.5 57.4 56.2 53.0 NA
winpercentage 0.0 0.0 0.0 100.0 pmi 6.7 46.7 46.7 0.0 char 0.0 86.7 6.7 6.7 char
19C.2.2 Hybridformulation
InCF,overallprobabilityscorecouldbequitemisleadingsinceitmayheavilyfavorshorteranswers
withfewertokens. Notethatthiswouldbedifferentiftheanswerchoicesareactuallylistedbefore
scoringtheanswerstring,thenmosttokens(afterthechoicehasbeendisambiguatedbythefirstfew
tokens)wouldhaveprobabilitynearone. This“hybrid”formulationhasbeenusedinsomecases,
butusuallyscoresinbetweentheCFandMCFapproaches(Wiegreffeetal.,2023). However,this
hybridapproachisnotpopularinevaluationstandardizationeffortsliketheOpenLLMLeaderboard,
HELM,orwhenusedtoevaluatemodelsduringdevelopment,soitisnotafocusinOLMES.
C.3 TokenizationofMCQAchoicelabels
Whenformattingmultiple-choicequestions,OLMESspecifiestheuseofaprefixspaceinfrontof
each answer choice, that is "\n A. <choice>" rather than "\nA. <choice>". Figure 3 shows
explicitexamplesoftokenizerswherethishelpsmaintainacorrespondencebetweenthetokenforthe
answerlabelandthetokeninthefinalanswer(e.g.,"\nAnswer: A"). E.g.,fortheLlamatokenizer,
theconsistenttokenisthen("_A")ratherthantheseparatetoken("A")yougetwithouttheprefix
space.
Figure3: Tokenizerexample,showingtwoexamplesoftokenizerswhichneedaprefixspacebefore
MCQAanswerchoicelabelstorepresentthechoicelabelandthefinalanswerlabelusingthesame
token.
> from transformers import AutoTokenizer
> llama_tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-hf")
> olmo_tokenizer = AutoTokenizer.from_pretrained("allenai/OLMo-1.7-7B-hf")
> test_string = "What is 3+4?\n A. 7\nA. 7\nAnswer: A"
> llama_tokenizer.tokenizer(test_string)
[’_What’, ’_is’, ’_’, ’3’, ’+’, ’4’, ’?’, ’<0x0A>’, ’_A’, ’.’, ’_’,
’7’, ’<0x0A>’, ’A’, ’.’, ’_’, ’7’, ’<0x0A>’, ’Answer’, ’:’, ’_A’] >
olmo_tokenizer.tokenizer(test_string)
[’What’, ’˙Gis’, ’˙G3’, ’+’, ’4’, ’?’, ’˙C’, ’˙GA’, ’.’, ’˙G7’, ’˙C’, ’A’, ’.’, ’˙G7’, ’˙C’,
’Answer’, ’:’, ’˙GA’]
D HELMReproductionofMMLU
InFigure4weseedatatakenfromHELM’sreproductionofMMLUscoresforavarietyofmodels.
E Computeused
TheinferenceonthemodelsevaluatedweredoneonNVIDIARTXA6000GPUs. Atotalofaround
400GPUhourswasused.
F Curationof5-shotexamples: considerations
Procedureformanuallycuratingthefew-shotexamples:
• DownloadthetrainsetfromHuggingFacedatasets
• Startfromthebeginningofthetrainingset,lookingatabatchof10(i.e.,startwithfirst10)
• Skipambiguousinstances
• Skipinstancesthathintatdiscriminationorotherwisedeemedinappropriate
• Skipinstancesifthesamelabelhasappearedfrequently(e.g.,4consecutiveinstanceswith
goldlabel‘C’,keepbetteronesoutofthose)
• Ifinstancesaregrouped/labeledbytopic,chooseinstancestobediverse(e.g. first3areall
aboutacertaintopic,pickfromlateronestoensurediversity)
20Self-reporting overestimates MMLU score compared to reproduction
Trendline for series 1 R² = 0.26
6
4
2
0
-2
30 40 50 60 70 80
Self-Reported MMLU Score
Figure4:Self-reportingoverestimatesMMLUscorecomparedtoreproduction,fromhttps://crfm.
stanford.edu/2024/05/01/helm-mmlu.html. Each point corresponds to a model, the x-axis
showsself-reported MMLU score, andthey-axisshowsthedifferencebetweentheself-reported
scoreandthereproducedscore. Pointsabovethey=0linehavehigherself-reportedperformance
thanthereproduction;thetrendlinehasapositiveslope,indicatingthatonaverage,thehigherthe
self-reportedscorethemoretheyoverestimateperformancecomparedtothereproduction.
• Ifyouendupwithlessthan7instancesthatcoverthelabelspaceorrangeofdifferenttopics,
lookatthenextbatchof10.
• Finally,reorderinstancestoobtainasomewhatbalancedoutputofanswerlabels–thefirst5
shotsshouldcoverthespaceofanswerlabels.
Notethatafewmorethan5shotsperdatasetwerecuratedintheprocess,thoughinpracticeweare
justusingthefirst5.
G OLMESpromptformatsforeachtask
InFigure5weshowanexampleofafull5-shotpromptfromARC-CHALLENGE(CF).Thenweshow
singleinstanceformattingforeachofthe10tasksinFigures6-25. Foreachtask,weshowboththe
MCFandCFformats.
All curated few-shot examples and prompt formatting code are available by accessing https:
//allenai.org/data/olmes.
21
erocS
noitcudorpeR
-
erocS
detropeR-fleSPrompt Question: George wants to warm his hands quickly by rubbing them.
Which skin surface will produce the most heat?
Answer: dry palms
Question: Which of the following statements best explains why magnets
usually stick to a refrigerator door?
Answer: The refrigerator door contains iron.
Question: A fold observed in layers of sedimentary rock most likely
resulted from the
Answer: converging of crustal plates.
Question: Which of these do scientists offer as the most recent
explanation as to why many plants and animals died out at the end
of the Mesozoic era?
Answer: impact of an asteroid created dust that blocked the sunlight
Question: Which of the following is a trait that a dog does NOT
inherit from its parents?
Answer: the size of its appetite
Question: A boat is acted on by a river current flowing north and
by wind blowing on its sails. The boat travels northeast. In which
direction is the wind most likely applying force to the sails of the
boat?
Answer:
Completion east
Figure5: OLMES5-shotpromptexampleforARC-CHALLENGE(CF).
Prompt Question: George wants to warm his hands quickly by rubbing them.
Which skin surface will produce the most heat?
A. dry palms
B. wet palms
C. palms covered with oil
D. palms covered with lotion
Answer:
Completion A
Figure6: OLMESpromptexampleforARC-CHALLENGE(MCF).
Prompt Question: George wants to warm his hands quickly by rubbing them.
Which skin surface will produce the most heat?
Answer:
Completion dry palms
Figure7: OLMESpromptexampleforARC-CHALLENGE(CF).
Prompt Question: Lichens are symbiotic organisms made of green algae and
fungi. What do the green algae supply to the fungi in this symbiotic
relationship?
A. carbon dioxide
B. food
C. protection
D. water
Answer:
Completion B
Figure8: OLMESpromptexampleforARC-EASY(MCF).
22Prompt Question: Lichens are symbiotic organisms made of green algae and
fungi. What do the green algae supply to the fungi in this symbiotic
relationship?
Answer:
Completion food
Figure9: OLMESpromptexampleforARC-EASY(CF).
Prompt Persian language – Persian, also known by its endonym Farsi, is one
of the Western Iranian languages within the Indo-Iranian branch of
the Indo-European language family. It is primarily spoken in Iran,
Afghanistan (officially known as Dari since 1958), and Tajikistan
(officially known as Tajiki since the Soviet era), and some other
regions which historically were Persianate societies and considered
part of Greater Iran. It is written in the Persian alphabet, a
modified variant of the Arabic script, which itself evolved from
the Aramaic alphabet.
Question: do iran and afghanistan speak the same language?
A. yes
B. no
Answer:
Completion A
Figure10: OLMESpromptexampleforBOOLQ(MCF).
Prompt Persian language – Persian, also known by its endonym Farsi, is one
of the Western Iranian languages within the Indo-Iranian branch of
the Indo-European language family. It is primarily spoken in Iran,
Afghanistan (officially known as Dari since 1958), and Tajikistan
(officially known as Tajiki since the Soviet era), and some other
regions which historically were Persianate societies and considered
part of Greater Iran. It is written in the Persian alphabet, a
modified variant of the Arabic script, which itself evolved from
the Aramaic alphabet.
Question: do iran and afghanistan speak the same language?
Answer:
Completion yes
Figure11: OLMESpromptexampleforBOOLQ(CF).
Prompt Question: Sammy wanted to go to where the people were. Where might he
go?
A. race track
B. populated areas
C. the desert
D. apartment
E. roadblock
Answer:
Completion B
Figure12: OLMESpromptexampleforCOMMONSENSEQA(MCF).
Prompt Question: Sammy wanted to go to where the people were. Where might he
go?
Answer:
Completion populated areas
Figure13: OLMESpromptexampleforCOMMONSENSEQA(CF).
23Prompt Health: How to cope with suicidal thoughts. Put off any plans.
Promise yourself that you’ll wait 48 hours before doing anything.
Remember, thoughts don’t have the power to force you to act.
Choose the best continuation:
A. Even when you do, there may be a small image of the future still
lurking around your brain. For instance, don’t tell yourself that you
can’t make it.
B. You’re doing something, and no one can force you to act. It’s
completely natural to feel negative thoughts before you act.
C. Do not panic if people talk to you (even if it’s about quitting
smoking). Have a plan for how you’re going to react to a group of
people who bring on suicidal thoughts.
D. Sometimes extreme pain can distort our perception. Waiting before
taking action will give your mind time to clear.
Answer:
Completion D
Figure14: OLMESpromptexampleforHELLASWAG(MCF).
Prompt Health: How to cope with suicidal thoughts. Put off any plans.
Promise yourself that you’ll wait 48 hours before doing anything.
Remember, thoughts don’t have the power to force you to act.
Completion Sometimes extreme pain can distort our perception. Waiting before
taking action will give your mind time to clear.
Figure15: OLMESpromptexampleforHELLASWAG(CF).
Instruction The following are multiple choice questions (with answers) about
abstract algebra.
Prompt Question: Find all c in Z_3 such that Z_3[x]/(x^2 + c) is a field.
A. 0
B. 1
C. 2
D. 3
Answer:
Completion B
Figure16: OLMESpromptexampleforMMLU(abstract_algebra)(MCF).
Instruction The following are multiple choice questions (with answers) about
abstract algebra.
Prompt Question: Find all c in Z_3 such that Z_3[x]/(x^2 + c) is a field.
Answer:
Completion 1
Figure17: OLMESpromptexampleforMMLU(abstract_algebra)(CF).
Prompt Question: When standing miles away from Mount Rushmore
A. the mountains seem very close
B. the mountains are boring
C. the mountains look the same as from up close
D. the mountains seem smaller than in photographs
Answer:
Completion D
Figure18: OLMESpromptexampleforOPENBOOKQA(MCF).
24Prompt Question: When standing miles away from Mount Rushmore
Answer:
Completion the mountains seem smaller than in photographs
Figure19: OLMESpromptexampleforOPENBOOKQA(CF).
Prompt Goal: how do you stab something?
A. stick a sharp object through it.
B. pin it with a sharp object.
Answer:
Completion A
Figure20: OLMESpromptexampleforPhysicalInteractionQA(MCF).
Prompt Goal: how do you stab something?
Answer:
Completion stick a sharp object through it.
Figure21: OLMESpromptexampleforPhysicalInteractionQA(CF).
Prompt Question: Cameron decided to have a barbecue and gathered her friends
together. How would Others feel as a result?
A. like attending
B. like staying home
C. a good friend to have
Answer:
Completion A
Figure22: OLMESpromptexampleforSOCIALIQA(MCF).
Prompt Question: Cameron decided to have a barbecue and gathered her friends
together. How would Others feel as a result?
Answer:
Completion like attending
Figure23: OLMESpromptexampleforSOCIALIQA(CF).
Prompt Fill in the blank: John moved the couch from the garage to the
backyard to create space. The ___ is small.
A. garage
B. backyard
Answer:
Completion A
Figure24: OLMESpromptexampleforWINOGRANDE(MCF).
Prompt1 John moved the couch from the garage to the backyard to create space.
The garage
Prompt2 John moved the couch from the garage to the backyard to create space.
The backyard
Completion is small.
Figure25: OLMESpromptexampleforWINOGRANDE(CF).Inthiscasethecompletionsarethe
sameforeachanswerchoice,butthepromptisdifferent.
25