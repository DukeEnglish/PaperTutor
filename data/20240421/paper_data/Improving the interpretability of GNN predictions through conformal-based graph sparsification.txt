IMPROVING THE INTERPRETABILITY OF GNN PREDICTIONS
THROUGH CONFORMAL-BASED GRAPH SPARSIFICATION
PabloSánchez-Martín KinaanAamirKhan
MaxPlanckInstituteforIntelligentSystems& McGillUniversity
SaarlandUniversity Montreal,Canada
Tübingen/Saarbrücken,Germany kinaan.khan@mail.mcgill.ca
psanchez@tue.mpg.de
IsabelValera
SaarlandUniversity
MaxPlanckInstituteforSoftwareSystems
Saarbrücken,Germany
ivalera@cs.uni-saarland.de
ABSTRACT
GraphNeuralNetworks(GNNs)haveachievedstate-of-the-artperformanceinsolvinggraphclassifi-
cationtasks. However,mostGNNarchitecturesaggregateinformationfromallnodesandedgesin
agraph,regardlessoftheirrelevancetothetaskathand,thushinderingtheinterpretabilityoftheir
predictions. Incontrasttopriorwork,inthispaperweproposeaGNNtrainingapproachthatjointly
i)findsthemostpredictivesubgraphbyremovingedgesand/ornodes—-withoutmakingassumptions
aboutthesubgraphstructure—whileii)optimizingtheperformanceofthegraphclassificationtask.
Tothatend, werelyonreinforcementlearningtosolvetheresultingbi-leveloptimizationwitha
rewardfunctionbasedonconformalpredictionstoaccountforthecurrentin-traininguncertainty
oftheclassifier. Ourempiricalresultsonninedifferentgraphclassificationdatasetsshowthatour
methodcompetesinperformancewithbaselineswhilerelyingonsignificantlysparsersubgraphs,
leadingtomoreinterpretableGNN-basedpredictions.
Keywords GraphNeuralNetworks·Interpretability·ConformalPrediction·ReinforcementLearning
1 Introduction
GraphNeuralNetworks(GNNs)havebecomeacornerstoneinmodernmachinelearning,excellingindiversedomains
suchassocialnetworkanalysisCaoetal.[2019b,a],recommendersystemsFanetal.[2019],Baietal.[2020]and
bioinformatics Guo et al. [2021], Ramirez et al. [2020], Xiong et al. [2020]. However, the complexity of GNNs
contributestooneoftheirprincipalshortcoming: alackofhuman-interpretablepredictions. Thisopacityhinderstheir
potentialforpractical,real-worldimpact,aspractitionersoftenrequireinterpretablemodelstoinformdecision-making,
ensuretrustworthiness,andcomplywithregulationsDoshi-VelezandKim[2017]. Currentinterpretabilitymethods
forGNNpredictorsaimtoidentifythemostpredictivesubgraphfromanoriginalgraphSunetal.[2021]. Inessence,
interpretabilityinthiscontextiscloselytiedtographsparsity,implyingtheuseofaminimalsetofnodesandedges
fromthegraphforprediction,ratherthantheentiregraphG.
ThisviewpointofinterpretabilityisexemplifiedinFigure1withthesyntheticBA2ShapesdatasetYingetal.[2019]. In
thisdataset,abinaryclassificationtaskcanbesolvedusingonlyspecificmotifsofthegraphs(thehouseorthecycle
motifs,i.e.,yellownodes),whiletheoriginalgraphscontainadditionalirrelevantinformationforthetaskathand,i.e.,
purplenodes. Asparsergraphreducesthevolumeofinformationusedformakingpredictions,renderingiteasierfor
humanstounderstandtheGNNpredictions. Itiscrucialtonotethatinterpretabilityisachievedthroughsparsityonly
whenthesubgraphcompletelyexcludesinformationfromtheomittednodesandedgessinceonlythenthesubgraph,
4202
rpA
81
]LM.tats[
1v65321.4042:viXraFigure1: CORESpipeline. IllustrationofthepipelineofCORESusingthesyntheticBA2Shapes designedforbinarygraph
classification.Ontheleft,wepresenttwoexamplesoforiginalgraphs,denotedasG,correspondingtothepositiveclass(top,cycle
motif)andnegativeclass(bottom,housemotif).ThepolicyϕofCOREStakesGandsparsifiesit,resultinginthepredictivesubgraph
G s,whichretainsonlytherelevantinformationforthetask,i.e.,themotifs. Finally,thegraphclassifierθtakesG sasinputand
producesthepredictiony∈{0,1}.
andthus,theexplanationisfaithfultothemodelprediction. Thissituationdoesnotarise,forexample,ifmessage
passingKipfandWelling[2016]isappliedtoobtainthenodeembeddingsbeforefindingthesubgraph.
While most of explainability methods for GNNs are post-hoc, some existing approaches attempt to identify the
predictive subgraph during training Cangea et al. [2018], Sun et al. [2021]. These methods, however, exhibit two
primarylimitations: i)theyoftenstillrelyontheentiregraphforpredictionsCangeaetal.[2018],resultinginalackof
faithfulnesstothemodelprediction(asdescribedabove),and/orii)theyimposestrongassumptionsconcerningthe
structureofthepredictivesubgraph,suchasapre-definedsize. Inpracticalscenarios,relaxingassumptionsonthe
subgraphstructureisdesirable. Forinstance,incommunitydetectionwithinsocialnetworksContiscianietal.[2020],
ShchurandGünnemann[2018],itiscrucialtoonlyremoveedgesconnectingusersacrossdistinctcommunitiesbutthe
numberofinter-communityconnectionsvariessignificantlyandisunknownapriori. Anotherrelevantexampleisthe
predictionofmutageniceffectsincompounds,whereDebnathetal.[1991b]observedthatdiverse“motifs”ofchemical
elementsarepredictive. WereferthereadertoSection5foraconcreteexampleontheMUTAGdataset.
Inthiswork,weintroduceCORES,anovelalgorithmfortrainingGNNsthataddressesthetwofundamentalchallenges
simultaneously: i)findingtheinformativesubgraphG byremovingedgesand/ornodes,withoutimposingassumptions
s
onthesubgraph’sstructure,andii)optimizingtheperformanceofthegraphclassificationtasksolelyusingtheselected
subgraph G , thereby enhancing the interpretability of the classifier. To this end, we use a bi-level optimization
s
approach. Theoptimizationforperformanceadherestotheconventionalsupervisedproblemparadigm. Wethenapply
reinforcementlearningtoidentifythepredictivesubgraph,usingapolicythatallowsedgeand/ornoderemovalmodes.
Ourcarefullydesignedrewardfunctionallowsustointroduceinductivebiasestowardeithersparseorhigh-performing
solutionsandusesconformalpredictionsAngelopoulosandBates[2021]toaccountforclassifieruncertaintyateach
trainingepoch.
In summary, our work presents the following key contributions: (i) A novel framework, CORES, which enables
graph-levelclassificationusingonlypredictivesubgraphsasinput,leadingtomoreinterpretablesolutions. (ii)The
flexibilitytochoosebetweenachievingsparsitythroughnodeoredgeremoval,whichremovesassumptionsonthe
structureofthepredictivesubgraphandbroadenstherangeofapplicationscenarios. (iii)Thedesignofaversatile
rewardfunctionthattakesintoaccounttheuncertaintyoftheclassifierandallowstointroduceinductivebiasestowards
sparserormorehigh-performingsolutions. (iv)Acomprehensivecomparisonacrossninegraphclassificationdatasets,
showingthatCORESmakespredictionsusingsignificantlysparsergraphswhilekeepingcompetitiveperformance.
2 Preliminaries
ThissectionoutlinesnecessarybackgroundinformationonthekeybuildingblocksofCORES:reinforcementlearning,
message-passing graph neural networks, and conformal predictions. We begin by introducing the notation used
throughoutthiswork.
Notation. AgraphisdenotedasG = (V,E)whereV ∈ {1,...,n} = [n]isthesetofnnodesandE ⊆ V ×V is
thesetofedges. Thesizeofasetisrepresentedas|V|. AsubgraphofG isdenotedasG =(V ,E )⊆G comprising
s s s
2subsetsofnodesV ⊆V andedgesE ⊂E. AlabeleddatasetisrepresentedasD ={G ,y } wherey denotesthe
s s i i i i
targetofG . Here,withoutlossofgenerality,wefocusonKclassesgraphclassification,hencey ∈{1,...,K}=[K].
i i
2.1 Reinforcementlearning
ReinforcementLearning(RL)isalearningparadigminwhichanagentlearnstooptimizedecisionsbyinteractingwith
anenvironmentSuttonandBarto[2005]. Theobjectiveistofindapolicyπthatmaximizestheexpectedcumulative
reward. AnRLproblemistypicallymodeledasaMarkovDecisionProcess(MDP),definedbyatuple(S,A,P,R,γ),
whereSandAdenotethestateandactionspaces,P thetransitionprobability,Rtherewardfunction,andγthediscount
factor.
PolicyGradientMethods. Policygradientmethodsdirectlyoptimizethepolicyusinggradientascentontheexpected
cumulative reward Sutton et al. [1999]. Proximal Policy Optimization (PPO) Schulman et al. [2017b] is a policy
gradientmethodthatintroducesanobjectivefunctionfosteringexplorationwhilemitigatingdrasticpolicyupdates.
PPOaimstosolvetheoptimizationproblem:
(cid:104) (cid:16) (cid:17)(cid:105)
LCLIP(ϕ)=E min r (ϕ)Aˆ ,clip(r (ϕ),1−ϵ,1+ϵ)Aˆ (1)
t t t t t
where r (ϕ) = πϕ(at|st) is the probability ratio, Aˆ an estimator of the advantage function at time t, and ϵ a
t πϕold(at|st) t
hyperparametercontrollingthedeviationfromtheoldpolicy.
2.2 Graphneuralnetworks
MessagepassingGraphNeuralNetworks(GNNs)aredesignedforgraph-structureddataprocessing. Eachnodev ∈V
hasafeaturevectorh,andtheGNNtransformsthesefeaturesusingneighborhoodinformation. Formally,aGNN
performsLupdaterounds,updatingeachnode’sfeaturevectorh(l) atstepl≥1:
i
 
h( il) =f θu(cid:77) m θm(h( il−1) ,h( jl−1)). (2)
j∈Ni
(cid:76)
Here, f and m are differentiable, parameterized functions, N represents node i’s neighbors, and denotes
θu θm i
permutation invariant operations, i.e., sum, mean, or max. After L steps, we obtain the final node representations
h(L)∀i∈V.
Theserepresentationsareusedfortaskslikegraph-levelprediction,employingapermutation-invariant
i
(cid:16)(cid:110) (cid:111)(cid:17)
readoutfunctionyˆ=g h(L) :i∈V thatensurestheoutputisnodeorderindependent.
i
2.3 ConformalPrediction
ConformalpredictionAngelopoulosandBates[2021]isaframeworkthatrigorouslyquantifiesuncertaintyinmachine
learning predictions. Given a labeled dataset {x ,y }N , a heuristic measure of uncertainty of our predictor (e.g.,
i i i=1
softmaxvaluesfromaclassifierf ),andascoringfunctions(x,y)∈Rthatreflectspredictionuncertainty,conformal
θ
predictiongeneratesapredictionsetforanewinputx asfollows:
test
C(x )={y :s(x ,y)≤qˆ}⊆[K]. (3)
test test
Here,qˆrepresentsthe ⌈(n+1)(1−α)⌉ quantileofthecalibrationscores{s } ,whereαisapredefinederrorrate. Inthe
n i n
contextofclassificationtasks,onecommonlyusedconformalprocedureisAdaptivePredictionSets(APS)Angelopoulos
etal.[2020],Romanoetal.[2020],whichdefinesthescoringfunctionas:
k
(cid:88)
s(x,y)= f (x) ,wherey =π (x) (4)
θ πj(x) k
j=1
Here,π(x)representsthepermutationof[K]thatarrangesthesoftmaxvaluesf (x)frommostlikelytoleastlikely.
θ
33 Relatedwork
GNNsextenddeeplearningmodelstoincorporategraph-structureddataBronsteinetal.[2016]. Numerousarchitecture
proposalshaveemerged,tailoredfornode,link,andgraphpredictiontasksScarsellietal.[2009],Gilmeretal.[2017],
Wuetal.[2019,2020],Xuetal.[2018],Corsoetal.[2020],Chenetal.[2020]. However,theseapproachesrelyonthe
completegraphdataandoftendisregardinterpretability,whichisthefocusofourwork. Morerecently,someworksaim
forsparsificationZhengetal.[2020],Hasanzadehetal.[2020],Rongetal.[2019],OonoandSuzuki[2019],Loukas
[2019],Lietal.[2020],Luoetal.[2021],Wickmanetal.[2021],Wangetal.[2021]. However,thesemethodstypically
makestrongassumptionsonthepredictivesubgraph. Moreover,theyoftenfocusontasksotherthangraphprediction.
Here,weexplorerelevantsparsificationtechniquesthatspecificallytargetsupervisedproblems,whicharethemain
focusofourpaper. Wecategorizetheseworksintotwocategories: thoseemployingsoftinformationremoval,wherethe
predictivesubgraphretainsinformationfromthecompletegraph,andthusmaynotbefaithfultothemodelpredictions;
andthoseadoptinghardinformationremoval,whichcloselyalignswithourapproach.
Softremovalofinformation. SeveralarchitecturesJavaloyetal.[2022],Velickovicetal.[2017],Brodyetal.[2021]
assignvaryingimportancetonetworkedges,simulatingsoftedgeremovalwhilstmaintaininginformationflowacross
nodes.DiffPoolYingetal.[2018]hierarchicallyremovespartsoftheoriginalgraph,distillingthegraph’srepresentation
intoasinglevectorusedforprediction. SAGPoolingLeeetal.[2019],Knyazevetal.[2019],usesaGNNtoupdate
nodefeaturesandsubsequentlyselectsthetopkmostrelevantnodesbasedontheirfeatures. Itisworthnotingthat
eventhoughpartofthenetworkisdropped,theremainingsubgraphcontainsinformationfromtheentirenetwork. In
contrast,inourframework,thegraphpredictordoesnothaveaccesstothedroppedinformation.
Hardremovalofinformation. GPoolGaoandJi[2019],Cangeaetal.[2018],Knyazevetal.[2019]selectsthe
topknodesbasedsolelyontheirinformation. However,thenumberofnodesisfixedandcannotdynamicallychange
fordifferentgraphs. SUGARSunetal.[2021]takesadifferentapproachbyidentifyingdiscriminativesubgraphs. It
introducesareinforcementpoolingmoduletrainedwithQ-learningMnihetal.[2015]toadaptivelyselectapooling
ratiowhenrecombiningsubgraphsforclassification. SUGARalsoimposesapredefinedsizeforthesubgraphs. In
contrast,CORESdoesnotmakeanyassumptionsaboutthestructureofthesubgraph,usespolicygradientmethodsto
captureuncertaintyinthesparsityprocess,andallowsfordifferentmodesofremoval.
Finally,thereisextensiveliteraturethataimstounderstandwhichsubgraphoftheinputgraphismostrelevantfor
makingpredictionsinGNNsYingetal.[2019],Luoetal.[2020],Yuanetal.[2020],NumerosoandBacciu[2021],
Yuanetal.[2021],YuandGao[2022],Schlichtkrulletal.[2021]. However,itisimportanttonotethatthesemethods
primarilyfocusonexplainingaGNNpredictortrainedusingtheoriginalgraphs. Incontrast,ourobjectiveistotrain
aGNNthatinherentlyutilizesaminimalportionoftheoriginalgraph. Whilethesemethodsmaycomplementour
approachbyprovidingfurtherinsightsofthepredictions,theyoperateinapost-hocmanner.
4 CORES:Conformal-basedreinforcementlearningforgraphsparsification
In this section, we present CORES, a novel training procedure for GNNs leveraging reinforcement learning with
conformal-basedrewardstoachievegraphsparsification. GivenalabeleddatasetD,themaingoalofCORESisto
identifyacompactpredictivesubgraphG ⊆G thatmaintainshighperformanceforagraphclassificationtask. Next,
s
weprovideadetaileddescriptionofthedifferentpartscomprisingCORES.
4.1 Optimizingforsparsityandperformance
Firstly,weaimtolearnafunctionπ :G (cid:55)→G withparametersϕresponsibleforidentifyingthepredictivesubgraph.
ϕ s
ThisfunctioncorrespondstothepolicyofthereinforcementlearningcomponentofCORES.Secondly,wewanttotrain
agraphclassifierf :G →yˆwithparametersθthattakestheidentifiedpredictivesubgraphasinputandgenerates
θ s
predictions. ThepipelineofourapproachisillustratedwiththesyntheticBA2ShapesdatasetinFigure1. Totacklethis
challengeweintroduceabi-leveliterativeoptimizationapproach:
ϕ∗ =argminL (cid:0) θ⋆(ϕ),ϕ,Dval(cid:1) (5)
spa
ϕ
s.t. θ⋆(ϕ)=argminL (cid:0) θ,ϕ,Dtr(cid:1) (6)
perf
θ
4Algorithm1TrainingofCORES.
1: Input: TrainingDtr andvalidationDval sets;Initialgraphclassifierparametersθandpolicyparametersϕ;Empty
bufferB;Learningratesαandβ;NumberofPPOupdatesK
2: whilenotconvergencedo
3: for{y,G}∈Dtr do ▷Geta(batchof)sample(s)fromthetrainingset
4: π ϕ :G (cid:55)→G s ▷Getthesubgraphusingthepolicy
5: yˆ s =f θ(G s) ▷Gettheprediction
6: θ ←θ−α∇ θL perf(θ,ϕ) ▷Updatetheparametersofthegraphclassifier
7: endfor
8: for{y,G}∈Dval do ▷Geta(batchof)sample(s)fromthevalidationset
9: π ϕ :G (cid:55)→G s ▷Usetheπtosampleasparsegraph
10: yˆ s =f(G s) ▷Getthepredictionofthesparsegraph
11: Addtuple{y,G,yˆ,G s}tothebufferB
12: endfor
13: ComputethequantileqˆusingthecalibrationscoresobtainedwithDtr
14: Computerewardsandpreparemini-batchesfrombufferBforPPOupdates
15: fori={1,2,...,K}do ▷UpdatePPOKtimes
16: {y,G,yˆ,G s,r}∼B ▷Sampleabatchoftuplesfromthebuffer
ˆ
17: ComputeadvantageestimatesA
18: ϕ←ϕ−β∇ ϕL spa(θ,ϕ) ▷UpdatepolicybymaximizingEquation(1)
19: endfor
20: Checkforconvergence,updateconvergenceflag
21: endwhile
22: Output: Optimalpolicyparametersϕandgraphclassifierparametersθ
Thenestedstructureoftheproblemimpliesthatachievinganoptimalpredictivesubgraphrequiresahigh-performing
graphclassifier.Weemploygradientdescentforbothoptimizations,withalargerlearningratefortheinneroptimization
Zhengetal.[2021]. Also,noticethatweusethetrainingDtr andvalidationDval setstosolvetheinnerandouter
proceduresrespectively,whichavoidsoverfitting. Algorithm1summarizesthetrainingprocedure.
Performanceoptimization. ThisobjectivecorrespondstoEquation(6)whichrepresentsastandardgraph-supervised
problem. Here,theobjectiveistominimizealossfunctionL ,e.g.,thecross-entropyloss. Thusthegoalistolearna
perf
functionf :G →yˆwithparametersθthatminimizesthepredictionloss.
θ s
Sparsityoptimization. ThisoptimizationtaskcorrespondsEquation(5). Itpresentsaconsiderablechallengedue
tothecombinatorialnatureofthenodeandedgeremovalprocess. Specifically,wemustdeterminebothwhichedges
((u,v) ∈ E)and/ornodes(v ∈ V)andhowmanyofthem, shouldberemoved. DrawinginspirationfromSparRL
Wickman et al. [2021], we formulate the sparsification task as a Markov Decision Process (MDP) and address it
throughtheframeworkofgraphreinforcementlearningNieetal.[2022]: weparameterizethepolicyπ usingaGNN.
ϕ
Incontrasttopriorapproachesthatrelyonvalue-basedmethodslikeDeepQ-LearningMnihetal.[2015], weopt
forthepolicygradientmethodProximalPolicyOptimization(PPO)Schulmanetal.[2017a]tocapturetheinherent
uncertaintyinthesparsificationprocess. WedenotetheobjectiveofsparsityoptimizationasL ,whichcorrespondsto
spa
solvingthePPOobjectiveinEquation(1). Theoutcomeofthistaskisapolicyπ thatfindsthepredictivesubgraph
ϕ
G ,subsequentlyemployedasinputforthegraphclassificationtask. Next,weprovidemoredetailsonthesparsity
s
optimization.
4.2 Unpackingthesparsityoptimization
Inthissubsection,weprovideadetaileddescriptionofthecomponentsofthePPOmethodwedesignedtoidentify
thepredictivesubgraph. InRLproblems,agentsusuallymakesequentialdecisionsovermultipletimesteps,andthe
rewards they receive at each step may depend on the whole trajectory taken so far. This challenge, known as the
“delayedreward”problem,isacentralchallengeinRLSutton[1992]. Ourobjectiveshiftstowardsidentifyingthe
predictivesubgraphinasinglestep,ratherthanconsideringatrajectoryasintraditionalRL.Thisapproachaimstofind
astrategythatmaximizestherewardacrossthedatasetgraphs. ThissimplifiedscenarioresemblestheMulti-Armed
BanditProblemKuleshovandPrecup[2014].AsourexperimentsinSection5empiricallyshow,thissimplifiedscenario
effectivelyachieveshigh-performancerelyingonsignificantlysparsergraphs. Below,wefocusondescribingtwokey
5components: thepolicyresponsiblefortheremovalprocessandthedesignoftherewardfunctionthatguidesthepolicy
infindingtheoptimalpredictivesubgraph.
4.2.1 Policyformulationforgraphsparsificationvianodeoredgeremoval
Weproposeapolicyπ(a|G;ϕ)wemodelusingaGNNwithparametersϕ. GivenaninputgraphG,thepolicyproduces
anactiona,whereeachelementdetermineswhethertokeep(a =0)orremove(a =1)aspecificnodeoredgein
i i
V orE,respectively. ThisactionresultsinasubgraphG ⊆ G. Recognizingtheinherentuncertaintyinreal-world
s
scenarioswheretheimportanceofanode/edgemaynotbeclear,wedesignthepolicyasaprobabilitydistribution. We
establishtwooperationalmodesdependingontheremovalobjective:
• NodeRemovalPolicy: π (a|G;ϕ),wherea∈{0,1}|V|. Inthisscenario,theresultingsubgraphG ,usedas
n s
inputforthedownstreamtask,satisfiesV ={v ∈V|a =0}andE ={(u,v)∈E|a =0∧a =0}.
s v s u v
• Edge Removal Policy: π (a|G;ϕ), where a ∈ {0,1}|E|. Here, the subgraph G , used as input for the
e s
downstreamtask,hasV =V andE ={(u,v)∈E|a =0}.
s s uv
Notethatnoderemovalimpliestheremovalofalledgesconnectedtothenode. Incontrast,theedgeremovalpolicy
retains all nodes, making it suitable for node classification, which is a direction for future work. In either case,
determining G poses an NP-hard problem for both removal modes. The number of potential subgraphs increases
s
exponentiallywiththenumberofnodes/edgesintheoriginalgraph. Specifically,thenumberofpossibleactionsforπ
n
is(cid:80)|V|−1(cid:0)|V|(cid:1)
,whileforπ
,itis(cid:80)|E| (cid:0)|E|(cid:1)
. RLcanbehelpfultosolvecombinatorialproblems,particularlywhen
i=1 i e i=1 i
therewardfunctioneffectivelyguidestheoptimizationprocessMazyavkinaetal.[2020].
4.2.2 Rewardformulation
TherewardfunctionRplaysapivotalroleinshapingthepolicy’sbehavior. Itshouldprovidepositiverewardsfor
predictiveandsparsesubgraphswhilepenalizingthosethatnegativelyimpacttheperformanceofthegraphclassifier. It
isalsoimportanttoconsiderthattheclassifiermakeserrors,i.e.,itdoesnotachieveperfectperformance. Consequently,
thereareinstanceswhereasubgraphmaybegenuinelypredictive,buttheclassifierproducesanincorrectprediction.
Ourrewarddesignaccountsforthisinherentuncertaintythroughtheuseofconformalpredictions,asdescribedin
a subsequent section. It consists of two primary components: one aimed at enhancing performance and the other
dedicatedtopromotingsparsity.
PerformanceReward(R ). Ourapproachtorewardingperformanceisstraightforward. Wesimplyusethesoftmax
p
scoreassignedbythegraphclassifiertothecorrectclassy,i.e.,R =f (G ) ∈[0,1].
p θ s y
Sparsity Reward (R ). To incentivize a minimal predictive
s
subgraph,weintroduceacomponentthatpenalizesthenodesratio
PR n = | |V Vs || ortheedgesratioPR e = | |E Es || keptfromtheoriginal 0.05 maxim 0.u 2m 5desirednod 0e .s 5/edgesratio( 0d .) 75 0.95
graph, depending on the policy mode. To control the desired
sparsitylevel,weintroduceaparameterd∈[0,1]representingthe
maximumdesirednodes/edgesratio. Then,wedefinethesparsity 1.0
rewardasR (G ) = 1−PRd˜ ,whered˜ isatransformationofd s s 0.8
such that R = 0.95 when PR = d. The variation of R with
s s
respecttoPRfordifferentdvaluesisdepictedintheinlinefigure. 0.6
Eachverticallinerepresentsauniquedvalue,extendingupwards
0.4
untilR =0.95forallinstances.
s
Toseeksparsity,asmallerdvalue(e.g.,blueoryellowcurves)is 0.2
preferred. ThisleadstoaslowerincreaseinR withnode/edge
s 0.0
removal, rewarding substantial removal. Conversely, when d 0.0 0.2 0.4 0.6 0.8 1.0
approaches1(e.g.,pinkcurve),R closeto1areawardedeven Nodes/EdgesRatio(PR)
s
forkeepingmostnodes/edges. Figure2: EvolutionofthesparsityrewardR over
s
thenode/edgeratiofordifferentvaluesofthemaxi-
Note that both reward components fall within the range [0,1],
mumdesirednodes/edgesratiod.
facilitatingcomparison. Now,weproceedtodefinethereward
functionusedinCORES:
6
)sR(draweRytisrapSTable1: Statisticsofthedatasets.
Dataset #Graphs #Features #EdgeFeatures #Classes Undirected #Nodes #Edges
BZR 405 10 0 2 Yes 35.75±7.26 76.72±15.39
COX2 467 10 0 2 Yes 41.22±4.03 86.89±8.53
DD 1178 20 0 2 Yes 284.32±272.00 1431.32±1387.81
ENZYMES 600 21 0 6 Yes 32.63±15.28 124.27±51.00
MUTAG 135 7 4 2 Yes 18.85±4.49 41.67±11.13
NCI1 4110 7 0 2 Yes 29.87±13.56 64.60±29.87
NCI109 4127 8 0 2 Yes 29.68±13.57 64.26±29.92
PROTEINS 1113 4 0 2 Yes 39.06±45.76 145.63±169.20
PTC 349 8 4 2 Yes 14.11±8.44 28.97±18.88

λR p+(1−λ)R s(G s) ify ∈C(G s)∧|C(G s)|=1,
R=
−|C
R(R Gp
s ()|
G )
i if fy
y
∈ ̸∈C C( (G Gs) ).∧|C(G s)|>1, (7)
s s s
Therearetwokeyaspectstohighlight. Firstly,wecomputethequantileqˆusedtoobtainthepredictionsetC usingthe
scoringfunctionintroducedinEquation(4). Secondly,itinvolvesthreedifferentcasesbasedonwhetherthetruelabely
iswithinthepredictionsetandthesizeofthepredictionset.
Thetopscenariocorrespondstocasesinwhichthepredictivesubgraphleadstoahighlycertainandcorrectprediction.
Here,aparameterλ∈[0,1]allowspractitionerstoemphasizeeitherperformance(λ=1)orsparsity(λ=0). Inthis
scenario,therewardisintherangeR∈[0,1]. Themiddlescenarioaddressesinstancesinwhichthesubgraphleadsto
theclassifierbeinguncertainabouttheprediction,i.e.,|C(G )|>1. Insuchcases,weaimtoprovideapositivebut
s
smallerrewardwithintherange(0,0.5). Thisrewardencouragestoincreasetheprobabilityofthetruelabelandto
reducethesizeofthepredictionset,thatismovingtothetopscenario. Intuitively,thisscenarioariseswithchallenging
examples. Thebottomscenarioreferstocasesinwhichthepredictivesubgraphresultsinanincorrectprediction,falling
outsidethepredictionsetC. Asthisbehaviorisundesirable,therewardsRfallwithintherange[−1,0],encouragingto
reducesparsity. Inthisscenario,therewardreaches0onlywhenwerecovertheoriginalgraph.
The use of conformal prediction in the reward function is essential to guide the policy toward discovering a good
predictivesubgraph. Ifthetaskischallengingandtheclassifierexhibitsuncertainty,thefocusshouldinitiallybeon
performance. Onlywhentheclassifierisconfidentaboutthepredictionshouldweencouragesparsity. Inthefollowing
section, we present empirical evidence supporting the reward function’s design, and more generally our approach
CORES,forachievingbothperformanceandcompactpredictivesubgraphs.
5 Experiments
Inthissection,wepresentacomprehensivesetofexperimentstoassesstheperformanceofCORES.First,weconduct
anablationstudytoshowtheimpactofdifferentvaluesofλandmaximumdesiredratio(d)onperformance,aswellas
theeffectofthechoiceofbaseGNNarchitecture. Then,wecompareCORESwithrelevantbaselines.
Proposedapproaches. Weevaluatethetwoinformationremovalmodesoftheproposedframework,describedin
Section4.2.1. WedenotethemodelwiththepolicythatremovesnodesasCORES andthemodelwiththepolicythat
N
exclusivelyremovesedgesasCORES .
E
Datasets. Inthisstudy,weusedsevenBioinformaticsdatasets,whichincludeMUTAGDebnathetal.[1991a],DD
DobsonandDoig[2003],ENZYMESBorgwardtetal.[2005],NCI1Waleetal.[2006],NCI109Waleetal.[2006],
PTCToivonenetal.[2003],andPROTEINSBorgwardtetal.[2005]. Additionally,weincorporatedtwochemical
compounddatasets,BZRFeyandLenssen[2019a]andCOX2FeyandLenssen[2019a]. Table1showsthestatisticsof
thedatasets.
Experimentalsetup. Aimingforcomputationalefficiency,weconductedcross-validationonthehyperparameters
of the vanilla GNN classifier, i.e., the GNN architecture (e.g., GIN) trained with stochastic gradient descent. We
selectedtheoptimalconfigurationofhyperparametersbasedonthevalidationsetandusedittotraintheremaining
models. Weperformed5-foldcross-validationandreportedstandarddeviationvaluesonthetestsetatthelastepoch.
7Model Dataset
CORESN CORESE BZR ENZYMES MUTAG PTC
90 90 90 90
80 80 80 80
70 70 70 70
60 60 60 60
50 50 50 50
0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
d d λ λ
(a)dversusaccuracy (a)λversusaccuracy
80 60 80
60 60 60
40
40 40 40
20
20 20 20
0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
d d λ λ
(b)dversussparsity (b)λversussparsity
Varyingd λ=0.0 Varyingd λ=0.0 Varyingλ d=0.1 Varyingλ d=0.1
100 100 100 100
90 90 90 90
80 80 80 80
70 70 70 70
60 60 60 60
50 50 50 50
0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100
NodeRatio(%) EdgeRatio(%) NodeRatio(%) EdgeRatio(%)
(c)Sparsityversusaccuracy (c)Sparsityversusaccuracy
Figure3: Ablationstudyonthemaximumdesiredratio Figure4: Ablationstudyonλ. WeusetheGINarchitec-
d. WeusetheGINarchitectureandruneachexperiment tureandruneachexperimentfor5differentfolds.
for5differentfolds.
All experiments were conducted on a single CPU with 8GB of RAM. Here, we present results obtained using the
GIN architecture. Additional results using other well-known architectures, namely GAT and GCN, are provided
inAppendixC.Foracomprehensiveoverviewofthebestconfigurationhyperparametersobtainedforeachmodel,
architectureanddataset,pleaserefertoAppendixA.WeimplementedthebaselinesusingtheTorchGeometricpackage
FeyandLenssen[2019b]wheneveritwasavailable. Furthermore,weprovidetheimplementationofCORES,along
withthenecessaryscriptstoreplicatetheexperiments,athttps://github.com/psanch21/CORES.
5.1 Ablationstudy
Inthissection,weprovideanablationstudyontheproposedframeworkanalyzingtheeffectsofkeyhyperparameters
anddifferentchoicesforthebaseGNNarchitecture.
Analysisoftheimpactofλandd. Therewardfunction,presentedinEquation(7),leveragestwohyperparameters:
themaximumdesirednodes/edgesratio(d)andthebalancingparameter(λ)toprioritizeperformanceorsparsity. For
theninedatasetsunderstudy,wepresenttheeffectofvaryingdandλinFigure3andFigure4,respectively,forboth
CORES (solidlines)andCORES (dashedlines). Forthisablationstudy,weuseGINasthebaseGNNarchitecture.
N E
Theresultsarecolor-codedforeachdataset. Thefiguresinthetoprowcontaintheperformanceanalysis,whilethe
middlerowshowsthesparsity,specificallythenoderatio(|Vs|
)forCORES
andtheedgeratio(|Es|
)forCORES .
|V| N |E| E
Thebottomrowshowsperformanceversussparsityintheverticalandhorizontalaxesrespectively.
InFigure3,wefixλ=0.0(onlyrewardingsparsity)whilevaryingdwithintherange[0.2,0.9].Regardingperformance
(top row), we observe that generally, the value of d does not have an impact. The middle row, which focuses on
sparsity,revealsinterestingvariationsamongdatasets. Forinstance,fortheMUTAGdataset,CORESfindsapredictive
subgraphthatutilizesaslittleas20%oftheoriginalnodes/edgeswithd=0.2. Incontrast,forthePTCdataset,we
keepapproximately80%oftheoriginalnodesinCORES and60%oftheoriginaledgesinCORES ,evenwithlowd
N E
values. Thebottomfigureshighlightthatforcertaindatasets,e.g.,BZR,COREScanmaintaincomparableaccuracy
regardlessofsparsitylevels. Conversely,forotherdatasets,decreasingsparsitytendstoenhanceperformance,such
asthecasewiththePTCdatasetandCORES . Thesefindingssuggestthatsomedatasetsexhibitrapidperformance
N
8
)%(ycaruccA
)%(oitaRedoN
)%(ycaruccA
)%(ycaruccA
)%(oitaRegdE
)%(ycaruccA
)%(ycaruccA
)%(oitaRedoN
)%(ycaruccA
)%(ycaruccA
)%(oitaRegdE
)%(ycaruccATable2: AblationstudyonGNNarchitecture. Wecomparetheperformance(accuracy)andsparsity(node/edge
ratio)ofCORESwiththeVanillaapproachfortheGCN(left)andGAT(right)architectures. Weshowthemeanover5
independentrunsandthestandarddeviationasthesubindex. Allmetricsareshowninpercentages.
GCN GAT
Dataset Metric
Vanilla CORES CORES Vanilla CORES CORES
N E N E
Accuracy 86.83 82.44 80.98 80.98 83.90 81.46
4.08 4.69 5.29 4.36 4.43 4.08
BZR NodeRatio - 76.98 100.00 - 37.99 100.00
6.06 0.00 10.27 0.00
EdgeRatio - 61.17 46.16 - 29.92 57.26
11.31 5.68 8.83 21.50
Accuracy 79.15 80.85 81.70 82.13 81.70 80.85
4.85 3.01 4.90 4.15 4.41 2.61
COX2 NodeRatio - 72.49 100.00 - 77.91 100.00
4.24 0.00 2.29 0.00
EdgeRatio - 53.19 31.27 - 62.62 66.18
8.07 23.60 3.24 0.38
Accuracy 78.47 79.32 75.76 76.10 75.42 76.44
3.47 2.58 4.05 3.02 4.19 2.19
DD NodeRatio - 56.44 100.00 - 75.16 100.00
2.50 0.00 1.39 0.00
EdgeRatio - 32.01 51.39 - 56.42 58.62
2.89 1.90 2.16 2.51
Accuracy 66.00 58.31 57.02 71.33 69.84 67.54
5.35 8.47 11.62 10.63 4.43 12.23
ENZYMES NodeRatio - 77.17 100.00 - 80.26 100.00
7.28 0.00 1.03 0.00
EdgeRatio - 62.33 74.20 - 65.92 61.60
9.26 35.99 1.80 9.06
Accuracy 81.43 81.43 87.14 81.43 80.00 81.43
3.91 10.83 6.56 6.39 5.98 6.02
MUTAG NodeRatio - 79.94 100.00 - 75.72 100.00
2.79 0.00 4.04 0.00
EdgeRatio - 63.52 32.66 - 58.74 55.25
4.23 8.79 6.05 14.77
Accuracy 77.33 66.45 65.94 79.42 73.43 75.38
2.09 7.71 6.31 1.34 2.67 1.22
NCI1 NodeRatio - 76.47 100.00 - 63.49 100.00
17.88 0.00 3.58 0.00
EdgeRatio - 64.02 37.98 - 53.33 88.68
18.84 14.34 2.81 6.67
Accuracy 79.32 76.08 77.14 76.90 74.72 72.78
1.57 2.71 2.02 1.34 2.25 2.29
NCI109 NodeRatio - 78.78 100.00 - 71.55 100.00
4.15 0.00 11.30 0.00
EdgeRatio - 67.08 92.02 - 62.37 62.37
4.42 1.93 12.96 13.13
Accuracy 72.86 73.21 75.89 75.36 73.39 73.57
1.02 2.45 1.94 3.00 3.86 1.62
PROTEINS NodeRatio - 74.66 100.00 - 67.94 100.00
6.10 0.00 2.97 0.00
EdgeRatio - 56.12 64.73 - 46.56 52.82
9.77 0.98 4.27 1.86
Accuracy 57.78 62.86 63.43 64.00 62.96 59.43
7.71 2.86 3.13 4.33 4.17 4.69
PTC NodeRatio - 64.10 100.00 - 45.61 100.00
4.19 0.00 6.00 0.00
EdgeRatio - 40.21 44.47 - 21.24 76.36
6.93 5.29 6.53 14.75
degradationwheninformationisremoved. Examiningthedifferentrewardscenariosinourrewarddefinition(referto
Equation(7)),itisintuitivetodeducethatCORESwillprioritizereducingsparsityifthepredictionisnotwithinthe
predictionset(bottomscenario)orenhancingperformancewhentheclassifierexhibitsuncertainty(middlescenario).
InFigure4,wefixd=0.1(favoringhighlevelsofsparsity)andwevaryλwithintherange[0.0,1.0]. Weobservea
similarbehaviorasinFigure3. Thetoprowconsistentlyshowsthatvaryingλhasaminimalimpactonaccuracy,and
onlysometimesleadstoincreasedaccuracyasλincreases(e.g.,BZRdataset). Importantly,fornoneofthedatasetsdoes
performancedegradewithhigherλvalues. Thisbehavioralignswithourobjective,ashighervaluesofλcorrespond
toagreateremphasisonrewardingperformance. Themiddlerowrevealstwodistinctpatterns: i)ahighcorrelation
betweenλandthenumberofnodes/edgeskept,particularlyevidentindatasetslikeMUTAG,andii)minimalsensitivity,
asobservedwiththePTCdataset. Thebottomrowrevealssimilarpatterns: positivecorrelationbetweenthenode/edge
ratioandaccuracy. Itisalsoworthnotingthatvaryingλresultsinawiderangeofsparsitylevelsforsomedatasets,
suchas20-90%forMUTAG,andanarrowerrangeforothers,like40-70%forPTC.
Insummary,weobservethatvaryingdorλleadstooneoftwoscenarios,dependingonthedataset: i)Aminimal
changeinperformanceandlowsparsity,whichsuggeststhatinformationremovalsignificantlydegradesperformance
forthesedatasets. ii)Asmallpositivecorrelationbetweenthehyperparametersandperformanceandahighnegative
correlationbetweenthehyperparametersandsparsity. Basedontheseinsights,werecommendthatpractitionersused
tofine-tunesparsitylevelsandconsiderλ=1.0whenprioritizingperformanceoversparsity. AppendixBcontainsthe
resultsfortherestofthedatasetsunderstudy.
9Table3: Modelcomparisonresults. Weshowthemeanover5independentrunsandthestandarddeviationasthe
subindex. Allmetricsareshowninpercentage. Thelastrowsincludetheaveragerankingofthemodelacrossdatasets.
Bestperformingmodelsonaverageareindicatedinbold.
FullModels SparseModels
Dataset Metric
Vanilla SAGPool DiffPool SUGAR GPool CORES CORES
N E
Accuracy 82.44 85.85 70.24 - 80.98 84.88 81.95
5.00 2.67 5.95 4.01 4.01 5.62
BZR NodeRatio - - - - 96.46 15.91 100.00
0.19 5.30 0.00
EdgeRatio - - - - 96.40 10.84 76.03
0.45 5.80 3.72
Accuracy 82.13 80.43 65.83 - 80.43 82.98 85.11
5.12 3.16 6.65 3.50 3.36 3.01
COX2 NodeRatio - - - - 91.15 86.44 100.00
0.14 3.48 0.00
EdgeRatio - - - - 89.20 77.63 58.30
1.08 6.60 28.00
Accuracy 78.81 74.07 - - 77.46 75.42 75.25
3.44 4.09 3.31 5.22 2.77
DD NodeRatio - - - - 40.23 47.53 100.00
0.03 15.04 0.00
EdgeRatio - - - - 15.41 25.88 51.55
0.26 14.79 2.42
Accuracy 61.67 45.90 31.00 16.67 62.62 62.33 64.92
8.50 5.18 6.66 23.57 5.36 6.08 3.40
ENZYMES NodeRatio - - - - 96.67 80.67 100.00
0.14 1.16 0.00
EdgeRatio - - - - 93.37 66.90 72.71
0.14 1.66 1.00
Accuracy 85.71 78.57 69.00 76.34 81.43 82.86 82.14
8.75 5.05 12.21 3.80 3.91 6.39 5.98
MUTAG NodeRatio - - - - 51.38 52.38 100.00
0.12 7.67 0.00
EdgeRatio - - - - 36.56 36.31 61.09
9.20 11.32 12.39
Accuracy 76.89 69.29 69.93 49.95 77.77 73.34 74.34
2.03 2.12 3.24 35.58 1.93 6.30 7.53
NCI1 NodeRatio - - - - 96.89 89.13 100.00
0.08 13.88 0.00
EdgeRatio - - - - 94.47 86.61 84.06
0.28 20.20 27.86
Accuracy 78.64 74.14 67.49 49.65 75.45 73.03 75.74
2.69 0.85 2.60 35.71 3.30 1.90 2.04
NCI109 NodeRatio - - - - 96.79 66.31 100.00
0.04 5.69 0.00
EdgeRatio - - - - 93.65 61.26 71.34
0.22 7.22 7.62
Accuracy 75.18 70.54 66.42 59.57 72.68 73.75 74.11
2.04 3.63 7.19 43.01 1.85 4.02 2.82
PROTEINS NodeRatio - - - - 41.92 87.01 100.00
0.30 20.04 0.00
EdgeRatio - - - - 19.42 78.55 83.31
0.64 32.05 37.32
Accuracy 63.43 61.14 54.44 56.14 59.43 64.00 63.43
1.28 8.23 11.10 8.95 3.73 3.26 1.28
PTC NodeRatio - - - - 43.52 96.02 100.00
0.67 8.80 0.00
EdgeRatio - - - - 24.80 93.19 88.61
3.48 15.13 15.64
Accuracy 2.11 4.44 6.12 6.67 3.44 2.67 2.67
Avg. Rank
NodeRatio - - - - 1.56 1.44 -
EdgeRatio - - - - 2.22 1.67 2.11
Ablation on the base GNN architecture. Now, we analyze how switching the base architecture affects the per-
formance of CORES. Table 2 summarizes the results for two base architectures: GCN and GAT. We observe that
evenwhenthebasearchitecturechanges,bothCORES andCORES achievecompetitiveaccuracycomparedtothe
N E
respectiveVanillaapproachthatsimplytrainstheGNNarchitectureusingalltheinformationfromtheoriginalgraph.
WereferthereadertoAppendixCforthecompleteresults.
5.2 Performancecomparison
In this section, we conduct a comprehensive evaluation of CORES by comparing it with baselines and competing
methodsacrossninedatasets.
Weassesstheperformanceonthetestsetusingthreekeymetrics: accuracy(thelargerthebetter↑),aswellasthe
percentageofnodes(forCORES )oredges(forCORES )keptinthesubgraph(thelowerthebetter↓). Decreasing
N E
thesepercentagesincreasesgraphsparsity,enhancingmodelinterpretability.
10Atom
Carbon Nitrogen Oxigen Chlorine
Figure5:IllustrationofthepredictivesubgraphobtainedbyCORES foramutagenic(top)andnon-mutagenic(bottom)graphof
N
theMUTAGdataset.
Baselines. Wedividedthebaselinesintotwocategories:Fullmodel(usecompletegraphinformation)andSparse(use
partofthegraphinformation)modelapproaches.IntheFullmodelcategory,wecomparewiththefollowingapproaches:
VanillaapproachrepresentingtrainingthebaseGNNarchitecturewithoutanysparsityconsideration,DiffPoolYing
etal.[2018],andSAGPoolingLeeetal.[2019],Knyazevetal.[2019]—whichusesnodeembeddingscomingfrom
aGNNtoselectthetopk nodes,thusincorporatingglobalinformation. FortheSparsemodels,wecomparewith:
SUGARSunetal.[2021]1(forthedatasetsthattheircodecanrun)andGPoolGaoandJi[2019],Cangeaetal.[2018],
Knyazevetal.[2019]—wherethenodeembeddingsusedtoselectthetopkcomefromamulti-layerperceptron,hence
thesubgraphdoesnotcontainglobalinformation. Here,wepresenttheresultsusingGINXuetal.[2018]asthebase
GNNarchitecture. RefertoAppendixCfortheresultsusingGCNandGAT.
Quantitativeresults. Table3summarizestheresults. Intermsofaccuracy,weobservethatwhileVanillaconsistently
achievesthehighestaccuracyonmostdatasets,CORES andCORES securethesecondandthird-bestpositions,
E N
outperforming all other Full models and all Sparse models, which highlights the competitive performance of the
proposedapproach. Ontheotherhand,SUGARconsistentlyunderperformsacrossalldatasets,anditsresultsexhibit
highvariance. Regardinginterpretability(understoodassparsity),weperformanodeandedgeratioanalysisonthe
threebest-performingSparsemodelsintermsofaccuracy: CORES ,CORES ,andGPool. Itisimportanttonote
N E
thatCORES consistentlymaintainsanoderatioof100%,duetoitsedgeremovalstrategythatpreservesallnodes.
E
Amongthesemodels,CORES achievesthehighestrankingsinbothnodeandedgeratios,indicatingitseffectiveness
N
inremovingalargernumberofnodesfromtheoriginalgraph. Consequently,thegraphclassifieroperatesonareduced,
andthusmoreinterpretable,subgraph. Finally,wefinditinterestingtoanalyzetheresultsforthePTCdataset(bottom
row). WeobservethatCORES,inbothremovalmodes,achievestheworstsparsityresultsbutatthesametimethe
bestperformanceamongallmodels. Intermsofsparsity,CORES keeps96.02%ofthenodes,whileCORES keeps
N E
88.61%oftheedges. Intermsofaccuracy,allthemodelsachievelessthan65%,whichisnotablylowforabinary
classification problem. This indicates that the classifier makes a substantial number of errors. This observation is
importantconsideringthedesignoftheproposedrewardfunctionofCORES(seeEquation(7)). Forthisdataset,the
rewardfunctionplacesCORESinascenariowhereitseekstoenhanceperformance, correspondingtothemiddle
case,orpenalizesparsity,correspondingtothebottomcaseoftherewardfunction. Inessence,CORESbehavesin
accordancewiththerewardfunction.
Qualitativeresults. Figure5presentstwoexamplesofthepredictivesubgraphsdiscoveredbyCORES forthe
N
MUTAGdataset,showcasingpositive(top)andnegative(bottom)instances. Inthisdataset,thepositiveclass(y =1)
representsmutagenicmolecules,whilethenegativeclass(y =0)correspondstonon-mutagenicmolecules. Theleft
sideofthefigureshowstheoriginalgraphs,whilethecenterillustratesthepredictivesubgraphsidentifiedbyCORES .
N
Weobservethatforthemutagenicgraph(top),CORES keepsN0 nitrogendioxidecompoundsandsomecarbon
N 2
1WeusetheofficialimplementationofSUGARtoextracttheresultsontheabovedatasets. Webelieveourevaluationofa
held-outtestsetmayexplainthedisparitybetweentheresultswereportandthoseinSunetal.[2021]. Instead, theavailable
implementationofSUGARevaluatesonthesamevalidationsetusedtoselectthebestmodel.
11atoms,whileforthenon-mutagenicgraph(bottom),itcaptureschlorinecarbonatomsasrelevantfortheclassification
task. ThesefindingsalignwithpriorinthechemicaldomainDebnathetal.[1991b].
Timecomplexity. Trainingandinferencearesignificantlyfasterinmethodsthatemployastraightforwardforward
passcomparedtoRL-basedmethodslikeSUGARandCORES.Notably,ourfindingsindicatethatCORESisatleast
asfastasSUGARduringtrainingandcanbeupto10timesfasterduringinference. Forcomprehensivequantitative
resultsacrossalldatasets,pleaserefertoAppendixD.
6 Conclusions
GraphNeuralNetworks(GNNs)havedemonstratedexceptionalperformanceingraph-leveltasks, butsufferfrom
interpretabilityissuesduetotheircomplexity. ExistinginterpretabilityresearchinGNNsmainlyfocusesonpost-hoc
explanations. Approachesthataimtointroducesparsityduringtrainingoftenprovidepredictivesubgraphsthatare
notfaithfulwiththeclassifierpredictions. Also,thesemethodsoftenrelyoncompletegraphinformationand/ormake
strongassumptionsaboutthesparsergraphs. Inthiswork,weintroducedCORES,anovelGNNtrainingapproach
thatsimultaneouslyidentifiespredictivesubgraphsbyremovingedgesand/ornodes,withoutimposingassumptions
about subgraph structures; and optimizes the performance of the graph classification task. The optimization for
performancereferstotheconventionalsupervisedproblemparadigm. Wethenleveragereinforcementlearningto
identifypredictivesubgraphs,usingapolicythatallowsbothedgeandnoderemovalmodes. OnekeyaspectofCORES
isthedesignoftherewardfunction. Thisfunctionallowsthepractitionertointroduceinductivebiasestowardseither
sparseorhigh-performingpredictivesubgraphsanditincorporatesconformalpredictionsAngelopoulosandBates
[2021]toaccountforclassifieruncertainty. Ourempiricalevaluation,conductedonninegraphclassificationdatasets,
providesevidencethatourapproachnotonlycompetesinperformancewiththebaselinesthatusethecompletegraph
informationbutalsoreliesonsignificantlysparsersubgraphs. Consequently,theresultingGNN-basedpredictionsare
moreinterpretable,addressingtheprimarymotivationbehindourwork.
Practical limitations. The main limitation of our proposed approach lies in the significantly increased training
andinferencetime,introducedbythereinforcementlearningcomponent,comparedtothebaselinesthatdonotuse
reinforcementlearning.
Futurework. Futureresearchcouldfocusonimprovingtheefficiencyofthereinforcementlearningcomponent
toaccelerate thetrainingprocess ofCORES.Another interestingavenueforfuture workcould involve modifying
the current reward function to penalize specific types of errors or undesirable structures, such as isolated nodes.
Additionally,itcouldalsobeinterestingtoexploretheapplicationofCOREStographregressiontasksandCORES to
E
nodeclassificationtasks.
Acknowledgements
PabloSánchezMartínthankstheGermanResearchFoundationthroughtheClusterofExcellence“MachineLearning–
NewPerspectivesforScience”,EXC2064/1,projectnumber390727645forgenerousfundingsupport. Theauthors
thanktheInternationalMaxPlanckResearchSchoolforIntelligentSystems(IMPRS-IS)forsupportingPabloSánchez
Martín.
References
AnastasiosNikolasAngelopoulosandStephenBates. Agentleintroductiontoconformalpredictionanddistribution-
freeuncertaintyquantification. ArXiv,abs/2107.07511,2021. URLhttps://api.semanticscholar.org/
CorpusID:235899036.
AnastasiosNikolasAngelopoulos,StephenBates,JitendraMalik,andMichaelI.Jordan. Uncertaintysetsforimage
classifiersusingconformalprediction. ArXiv,abs/2009.14193,2020. URLhttps://api.semanticscholar.
org/CorpusID:221995507.
TingBai,YoujieZhang,BinWu,andJian-YunNie. Temporalgraphneuralnetworksforsocialrecommendation. 2020
IEEEInternationalConferenceonBigData(BigData),pages898–903,2020.
Karsten M. Borgwardt, Cheng Soon Ong, Stefan Schönauer, S. V. N. Vishwanathan, Alex Smola, and Hans-Peter
Kriegel. Proteinfunctionpredictionviagraphkernels. Bioinformatics,21Suppl1:i47–56,2005.
ShakedBrody,UriAlon,andEranYahav. Howattentivearegraphattentionnetworks? ArXiv,abs/2105.14491,2021.
12MichaelM.Bronstein,JoanBruna,YannLeCun,ArthurD.Szlam,andPierreVandergheynst. Geometricdeeplearning:
Goingbeyondeuclideandata. IEEESignalProcessingMagazine,34:18–42,2016.
Ca˘ta˘lina Cangea, Petar Velickovic, Nikola Jovanovic, Thomas Kipf, and Pietro Lio’. Towards sparse hierarchical
graphclassifiers. ArXiv,abs/1811.01287,2018. URLhttps://api.semanticscholar.org/CorpusID:
53219108.
QiCao,HuaweiShen,JinhuaGao,BingzhengWei,andXueqiCheng. Coupledgraphneuralnetworksforpredicting
thepopularityofonlinecontent. ArXiv,abs/1906.09032,2019a.
QiCao,HuaweiShen,JinhuaGao,BingzhengWei,andXueqiCheng. Popularitypredictiononsocialplatformswith
coupledgraphneuralnetworks. Proceedingsofthe13thInternationalConferenceonWebSearchandDataMining,
2019b.
MingChen,ZheweiWei,ZengfengHuang,BolinDing,andYaliangLi. Simpleanddeepgraphconvolutionalnetworks.
InInternationalConferenceonMachineLearning,2020.
MartinaContisciani,EleanorA.Power,andCaterinaDeBacco. Communitydetectionwithnodeattributesinmultilayer
networks. ScientificReports,10,2020.
GabrieleCorso,LucaCavalleri,D.Beaini,PietroLio’,andPetarVelickovic. Principalneighbourhoodaggregationfor
graphnets. ArXiv,abs/2004.05718,2020.
AsimKumarDebnath,RLLopezdeCompadre,GargiDebnath,AlanJ.Shusterman,andCorwinHansch. Structure-
activityrelationshipofmutagenicaromaticandheteroaromaticnitrocompounds.correlationwithmolecularorbital
energiesandhydrophobicity. Journalofmedicinalchemistry,342:786–97,1991a.
Asim Kumar Debnath, Rosa L. Lopez de Compadre, Gargi Debnath, Alan J. Shusterman, and Corwin Han-
sch. Structure-activity relationship of mutagenic aromatic and heteroaromatic nitro compounds. correlation
with molecular orbital energies and hydrophobicity. Journal of medicinal chemistry, 34 2, 1991b. URL
https://api.semanticscholar.org/CorpusID:19990980.
PaulD.DobsonandAndrewJ.Doig. Distinguishingenzymestructuresfromnon-enzymeswithoutalignments. Journal
ofmolecularbiology,3304:771–83,2003.
FinaleDoshi-VelezandBeenKim. Towardsarigorousscienceofinterpretablemachinelearning. arXiv: Machine
Learning,2017.
WenqiFan,YaoMa,QingLi,YuanHe,YihongEricZhao,JiliangTang,andDaweiYin. Graphneuralnetworksfor
socialrecommendation. TheWorldWideWebConference,2019.
MatthiasFeyandJanE.Lenssen. FastgraphrepresentationlearningwithPyTorchGeometric. InICLRWorkshopon
RepresentationLearningonGraphsandManifolds,2019a.
MatthiasFeyandJanE.Lenssen. FastgraphrepresentationlearningwithPyTorchGeometric. InICLRWorkshopon
RepresentationLearningonGraphsandManifolds,2019b.
HongyangGaoandShuiwangJi. Graphu-nets. IEEETransactionsonPatternAnalysisandMachineIntelligence,44:
4948–4960,2019.
JustinGilmer,SamuelS.Schoenholz,PatrickF.Riley,OriolVinyals,andGeorgeE.Dahl. Neuralmessagepassingfor
quantumchemistry. ArXiv,abs/1704.01212,2017.
ZhichunGuo,ChuxuZhang,W.Yu,JohnE.Herr,O.Wiest,MengJiang,andN.Chawla. Few-shotgraphlearningfor
molecularpropertyprediction. ProceedingsoftheWebConference2021,2021.
ArmanHasanzadeh,EhsanHajiramezanali,ShahinBoluki,MingyuanZhou,NickDuffield,KrishnaNarayanan,and
XiaoningQian. Bayesiangraphneuralnetworkswithadaptiveconnectionsampling. InInternationalconferenceon
machinelearning,pages4094–4104.PMLR,2020.
AdriánJavaloy,PabloSánchez-Martín,AmitLevi,andIsabelValera. Learnablegraphconvolutionalattentionnetworks.
ArXiv,abs/2211.11853,2022.
Thomas Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. ArXiv,
abs/1609.02907,2016. URLhttps://api.semanticscholar.org/CorpusID:3144218.
BorisKnyazev,GrahamW.Taylor,andMohamedR.Amer. Understandingattentionandgeneralizationingraphneural
networks. InNeuralInformationProcessingSystems,2019. URLhttps://api.semanticscholar.org/
CorpusID:195069083.
VolodymyrKuleshovandDoinaPrecup. Algorithmsformulti-armedbanditproblems. arXivpreprintarXiv:1402.6028,
2014.
13Junhyun Lee, Inyeop Lee, and Jaewoo Kang. Self-attention graph pooling. ArXiv, abs/1904.08082, 2019. URL
https://api.semanticscholar.org/CorpusID:119314157.
JiayuLi,TianyunZhang,HaoTian,ShengminJin,MakanFardad,andRezaZafarani. Sgcn: Agraphsparsifierbased
ongraphconvolutionalnetworks. InPacific-AsiaConferenceonKnowledgeDiscoveryandDataMining,pages
275–287.Springer,2020.
AndreasLoukas. Whatgraphneuralnetworkscannotlearn: depthvswidth. arXivpreprintarXiv:1907.03199,2019.
DongshengLuo,WeiCheng,DongkuanXu,WenchaoYu,BoZong,HaifengChen,andXiangZhang. Parameterized
explainerforgraphneuralnetwork. Advancesinneuralinformationprocessingsystems,33:19620–19631,2020.
DongshengLuo,WeiCheng,WenchaoYu,BoZong,JingchaoNi,HaifengChen,andXiangZhang. Learningtodrop:
Robustgraphneuralnetworkviatopologicaldenoising. InProceedingsofthe14thACMInternationalConference
onWebSearchandDataMining,pages779–787,2021.
Nina Mazyavkina, S. V. Sviridov, Sergei Ivanov, and Evgeny Burnaev. Reinforcement learning for combinatorial
optimization: A survey. Comput. Oper. Res., 134:105400, 2020. URL https://api.semanticscholar.
org/CorpusID:212633747.
VolodymyrMnih,KorayKavukcuoglu,DavidSilver,AndreiA.Rusu,JoelVeness,MarcG.Bellemare,AlexGraves,
MartinA.Riedmiller, AndreasKirkebyFidjeland, GeorgOstrovski, StigPetersen, CharlieBeattie, AmirSadik,
IoannisAntonoglou,HelenKing,DharshanKumaran,DaanWierstra,ShaneLegg,andDemisHassabis.Human-level
controlthroughdeepreinforcementlearning. Nature,518:529–533,2015.
MingshuoNie,DongmingChen,andDongqiWang.Reinforcementlearningongraph:Asurvey.ArXiv,abs/2204.06127,
2022.
DaniloNumerosoandDavideBacciu. Meg: Generatingmolecularcounterfactualexplanationsfordeepgraphnetworks.
arXivpreprintarXiv:2104.08060,2021.
KentaOonoandTaijiSuzuki. Graphneuralnetworksexponentiallyloseexpressivepowerfornodeclassification. arXiv
preprintarXiv:1905.10947,2019.
RicardoRamirez,Yu-ChiaoChiu,AllenHererra,MiladMostavi,JoshuaRamirez,YidongChen,YufeiHuang,and
Yu-FangJin. Classificationofcancertypesusinggraphconvolutionalneuralnetworks. InFrontiersofPhysics,2020.
Yaniv Romano, Matteo Sesia, and Emmanuel J. Candès. Classification with valid and adaptive coverage. arXiv:
Methodology,2020. URLhttps://api.semanticscholar.org/CorpusID:219303493.
YuRong,WenbingHuang,TingyangXu,andJunzhouHuang. Dropedge: Towardsdeepgraphconvolutionalnetworks
onnodeclassification. InInternationalConferenceonLearningRepresentations,2019.
FrancoScarselli,MarcoGori,AhChungTsoi,MarkusHagenbuchner,andGabrieleMonfardini. Thegraphneural
networkmodel. IEEETransactionsonNeuralNetworks,20:61–80,2009.
Michael Sejr Schlichtkrull, Nicola De Cao, and Ivan Titov. Interpreting graph neural networks for {nlp} with
differentiable edge masking. In International Conference on Learning Representations, 2021. URL https:
//openreview.net/forum?id=WznmQa42ZAx.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization
algorithms. ArXiv,abs/1707.06347,2017a.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization
algorithms. arXivpreprintarXiv:1707.06347,2017b.
OleksandrShchurandStephanGünnemann. Overlappingcommunitydetectionwithgraphneuralnetworks. ArXiv,
abs/1909.12201,2018.
QingyunSun,JianxinLi,HaoPeng,JiaWu,YuanxingNing,PhilipS.Yu,andLifangHe. Sugar: Subgraphneural
network with reinforcement pooling and self-supervised mutual information mechanism. In Proceedings of the
Web Conference 2021, WWW ’21, page 2081–2091, New York, NY, USA, 2021. Association for Computing
Machinery. ISBN 9781450383127. doi: 10.1145/3442381.3449822. URL https://doi.org/10.1145/
3442381.3449822.
RichardSSutton. Introduction: Thechallengeofreinforcementlearning. Reinforcementlearning,pages1–3,1992.
RichardS.SuttonandAndrewG.Barto. Reinforcementlearning: Anintroduction. IEEETransactionsonNeural
Networks,16:285–286,2005.
RichardS.Sutton,DavidA.McAllester,SatinderSingh,andY.Mansour. Policygradientmethodsforreinforcement
learningwithfunctionapproximation. InNIPS,1999.
14Hannu(TT)Toivonen,AshwinSrinivasan,RossD.King,StefanKramer,andChristophHelma. Statisticalevaluation
ofthepredictivetoxicologychallenge2000-2001. Bioinformatics,1910:1183–93,2003.
PetarVelickovic, GuillemCucurull, ArantxaCasanova, AdrianaRomero, PietroLio’, andYoshuaBengio. Graph
attentionnetworks. ArXiv,abs/1710.10903,2017.
NikilWale,IanA.Watson,andGeorgeKarypis. Comparisonofdescriptorspacesforchemicalcompoundretrievaland
classification. KnowledgeandInformationSystems,14:347–375,2006.
RunzhongWang,ZhigangHua,GanLiu,JiayiZhang,JunchiYan,FengQi,ShuangYang,JunZhou,andXiaokangYang.
Abi-levelframeworkforlearningtosolvecombinatorialoptimizationongraphs. arXivpreprintarXiv:2106.04927,
2021.
RyanWickman,XiaofeiZhang,andWeiziLi. Sparrl: Graphsparsificationviadeepreinforcementlearning. arXiv
preprintarXiv:2112.01565,2021.
ShiwenWu,WentaoZhang,FeiSun,andBinCui. Graphneuralnetworksinrecommendersystems: Asurvey. ACM
ComputingSurveys,55:1–37,2020.
ZonghanWu,ShiruiPan,FengwenChen,GuodongLong,ChengqiZhang,andPhilipS.Yu. Acomprehensivesurvey
ongraphneuralnetworks. IEEETransactionsonNeuralNetworksandLearningSystems,32:4–24,2019.
ZhaopingXiong,DingyanWang,XiaohongLiu,FeishengZhong,XiaozheWan,XutongLi,ZhaojunLi,XiaominLuo,
KaixianChen,HualiangJiang,andMingyueZheng. Pushingtheboundariesofmolecularrepresentationfordrug
discoverywithgraphattentionmechanism. Journalofmedicinalchemistry,2020.
Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? ArXiv,
abs/1810.00826,2018.
RexYing,JiaxuanYou,ChristopherMorris,XiangRen,WilliamL.Hamilton,andJureLeskovec. Hierarchicalgraph
representationlearningwithdifferentiablepooling. InNeuralInformationProcessingSystems,2018.
ZhitaoYing,DylanBourgeois,JiaxuanYou,MarinkaZitnik,andJureLeskovec.Gnnexplainer:Generatingexplanations
forgraphneuralnetworks. Advancesinneuralinformationprocessingsystems,32,2019.
Zhaoning Yu and Hongyang Gao. Motifexplainer: a motif-based graph neural network explainer. arXiv preprint
arXiv:2202.00519,2022.
HaoYuan,JiliangTang,XiaHu,andShuiwangJi. Xgnn: Towardsmodel-levelexplanationsofgraphneuralnetworks.
InProceedingsofthe26thACMSIGKDDInternationalConferenceonKnowledgeDiscovery&DataMining,pages
430–438,2020.
HaoYuan,HaiyangYu,JieWang,KangLi,andShuiwangJi. Onexplainabilityofgraphneuralnetworksviasubgraph
explorations.InProceedingsofthe38thInternationalConferenceonMachineLearning(ICML),pages12241–12252,
2021.
ChengZheng,BoZong,WeiCheng,DongjinSong,JingchaoNi,WenchaoYu,HaifengChen,andWeiWang. Robust
graphrepresentationlearningvianeuralsparsification. InInternationalConferenceonMachineLearning,pages
11458–11468.PMLR,2020.
LiyuanZheng,TannerFiez,ZaneAlumbaugh,BenjaminJ.Chasnov,andLillianJ.Ratliff. Stackelbergactor-critic:
Game-theoreticreinforcementlearningalgorithms. ArXiv,abs/2109.12286,2021.
15Table 4: Best configuration for the Vanilla GNN. Split sizes (#0), Batch size (#1), Dropout rate (#2), Batch
normalizing(#3),Dimensionofhiddenlayers(#4),NumberofGNNlayers(#5),Globalpoolingtype(#6),Classifier
schedulerfactor(#7),Classifierlearningrate(#8),ϵ(#9),Trainableϵ(#10),Numberofheads(#11).
Datasets
Param Archi
BZR COX2 DD ENZYMES MUTAG NCI1 NCI109 PROTEINS PTC
GIN [0.6,0.3,0.1] [0.4,0.5,0.1] [0.6,0.3,0.1] [0.5,0.4,0.1] [0.4,0.5,0.1] [0.5,0.4,0.1] [0.4,0.5,0.1] [0.4,0.5,0.1] [0.6,0.3,0.1]
#0 GAT [0.6,0.3,0.1] [0.5,0.4,0.1] [0.5,0.4,0.1] [0.5,0.4,0.1] [0.5,0.4,0.1] [0.6,0.3,0.1] [0.6,0.3,0.1] [0.4,0.5,0.1] [0.5,0.4,0.1]
GCN [0.6,0.3,0.1] [0.5,0.4,0.1] [0.6,0.3,0.1] [0.5,0.4,0.1] [0.5,0.4,0.1] [0.6,0.3,0.1] [0.6,0.3,0.1] [0.4,0.5,0.1] [0.4,0.5,0.1]
GIN 16 16 32 16 16 16 32 16 32
#1 GAT 32 16 32 16 16 32 32 16 16
GCN 16 16 32 16 32 32 16 32 32
GIN 0.0 0.2 0.5 0.2 0.0 0.0 0.1 0.1 0.4
#2 GAT 0.1 0.2 0.3 0.4 0.3 0.2 0.1 0.1 0.5
GCN 0.3 0.1 0.3 0.0 0.4 0.0 0.1 0.4 0.5
GIN True False True True True True True False False
#3 GAT True True False True True True False False True
GCN True False False True False True True False False
GIN 128 64 32 64 16 128 128 128 16
#4 GAT 16 128 16 128 32 128 128 16 16
GCN 64 128 32 128 16 64 128 32 32
GIN 3 1 1 1 3 1 3 1 3
#5 GAT 1 2 1 4 4 4 3 2 4
GCN 1 4 3 3 4 3 4 1 1
GIN [’mean’,’add’] [’add’] [’mean’,’add’] [’mean’] [’mean’,’add’] [’mean’] [’mean’] [’add’] [’mean’]
#6 GAT [’add’] [’mean’] [’mean’,’add’] [’mean’] [’mean’,’add’] [’add’] [’mean’,’add’] [’add’] [’mean’,’add’]
GCN [’mean’] [’mean’,’add’] [’mean’,’add’] [’mean’] [’mean’,’add’] [’mean’,’add’] [’add’] [’add’] [’mean’]
GIN 0.95 0.99 0.99 0.9 0.95 0.9 0.99 0.9 0.99
#7 GAT 0.95 0.95 0.99 0.95 0.99 0.95 0.99 0.95 0.9
GCN 0.99 0.99 0.99 0.95 0.99 0.99 0.9 0.99 0.99
GIN 0.0001 0.001 0.005 0.0001 0.001 0.01 0.001 0.0005 0.005
#8 GAT 0.01 0.0001 0.0001 0.001 0.005 0.005 0.001 0.005 0.01
GCN 0.01 0.0001 0.001 0.01 0.005 0.005 0.001 0.001 0.01
GIN 0.3 0.2 0.3 0.0 0.2 0.2 0.2 0.0 0.2
#9 GAT - - - - - - - - -
GCN - - - - - - - - -
GIN False True True True True False True False False
#10 GAT - - - - - - - - -
GCN - - - - - - - - -
GIN - - - - - - - - -
#11 GAT 2.0 4.0 4.0 2.0 2.0 4.0 4.0 2.0 4.0
GCN - - - - - - - - -
A Trainingdetails
Inthissection,weprovidefurtherdetailsoftheexperimentalsetupusedtoobtainourresults. Forallexperiments,
we train the models using a fixed seed of 0, and the reported results represent the mean and standard deviation
over five different dataset splits. We run the models up to 1000 epochs, doing early stopping if the accuracy does
not improve for 500 epochs. All experiments were conducted on a single CPU with 8GB of RAM. We carry out
hyperparametertuningusingrandomsampling. Weexplored20differentconfigurationsforeachmodel,data,andGNN
architecture. Completedetailsregardingthecross-validatedhyperparameterscanbefoundinourGitHubrepositoryat
https://github.com/psanch21/CORES. Table4presentsthebestconfigurationsachievedforeachdataset
andarchitecturefortheVanillaGNNbaselines. TheseconfigurationsweresubsequentlyusedfortrainingCORES.
ThebestconfigurationforCORES isdetailedinTable7,whileforCORES ,itcanbefoundinTable8. Thebest
N E
configurationsforSAGPoolingandGPoolareoutlinedinTable5andTable6,respectively.
16Table5: BestconfigurationforSAGPooling. Splitsizes(#0),Batchsize(#1),Earlystoppingclf. patience(#2),
Dropoutrate(#3),Batchnormalizing(#4),Dimensionofhiddenlayers(#5),NumberofGNNlayers(#6),Global
poolingtype(#7),Classifierschedulerfactor(#8),Classifierlearningrate(#9),TopKmultiplier(#10),TopKratio
(#11),Numberofheads(#12),ϵ(#13),Trainbleϵ(#14)
Datasets
Param Archi
BZR COX2 DD ENZYMES MUTAG NCI1 NCI109 PROTEINS PTC
GCN [0.5,0.4,0.1] [0.5,0.4,0.1] [0.4,0.5,0.1] [0.5,0.4,0.1] [0.5,0.4,0.1] [0.5,0.4,0.1] [0.5,0.4,0.1] [0.4,0.5,0.1] [0.6,0.3,0.1]
#0 GAT [0.4,0.5,0.1] [0.4,0.5,0.1] [0.4,0.5,0.1] [0.5,0.4,0.1] [0.4,0.5,0.1] [0.6,0.3,0.1] [0.5,0.4,0.1] [0.4,0.5,0.1] [0.4,0.5,0.1]
GIN [0.5,0.4,0.1] [0.5,0.4,0.1] [0.5,0.4,0.1] [0.6,0.3,0.1] [0.5,0.4,0.1] [0.5,0.4,0.1] [0.6,0.3,0.1] [0.6,0.3,0.1] [0.5,0.4,0.1]
GCN 16 16 16 16 32 16 16 16 32
#1 GAT 16 16 16 16 16 16 32 16 16
GIN 16 16 16 32 32 16 32 32 16
GCN 800 650 500 650 500 650 650 500 650
#2 GAT 800 800 500 800 800 650 500 800 800
GIN 800 500 650 500 800 800 500 500 650
GCN 0.4 0.0 0.5 0.1 0.4 0.0 0.1 0.5 0.4
#3 GAT 0.3 0.3 0.1 0.1 0.4 0.1 0.4 0.3 0.3
GIN 0.2 0.3 0.5 0.0 0.4 0.2 0.0 0.0 0.5
GCN True False False True True False True False False
#4 GAT False False False False True True False False False
GIN True False False True False True True True False
GCN 128 32 32 128 32 32 128 32 64
#5 GAT 16 16 64 128 16 32 16 16 16
GIN 32 128 128 64 16 32 64 64 128
GCN 4 1 4 4 1 1 4 4 3
#6 GAT 1 1 2 2 2 3 2 1 1
GIN 4 4 3 1 2 4 1 1 3
GCN [’add’] [’mean’,’add’] [’add’] [’mean’] [’mean’,’add’] [’mean’,’add’] [’mean’] [’add’] [’mean’]
#7 GAT [’mean’,’add’] [’mean’,’add’] [’mean’,’add’] [’mean’,’add’] [’mean’,’add’] [’mean’] [’mean’,’add’] [’mean’,’add’] [’mean’,’add’]
GIN [’mean’] [’mean’] [’mean’] [’add’] [’mean’,’add’] [’mean’] [’add’] [’add’] [’mean’]
GCN 0.99 0.95 0.99 0.95 0.9 0.95 0.95 0.99 0.9
#8 GAT 0.9 0.9 0.9 0.99 0.99 0.95 0.99 0.9 0.9
GIN 0.9 0.99 0.95 0.95 0.99 0.9 0.95 0.95 0.95
GCN 0.01 0.005 0.0005 0.0001 0.01 0.005 0.0001 0.0005 0.0001
#9 GAT 0.01 0.01 0.01 0.005 0.005 0.0001 0.005 0.01 0.01
GIN 0.001 0.0005 0.0005 0.001 0.005 0.001 0.001 0.001 0.0005
GCN 0.5 2.0 0.5 1.5 1.0 2.0 1.5 0.5 0.5
#10 GAT 1.0 1.0 2.0 1.0 1.5 1.5 0.5 1.0 1.0
GIN 1.5 2.0 0.5 0.5 0.5 1.5 0.5 0.5 0.5
GCN 0.95 0.95 0.9 0.3 0.3 0.95 0.3 0.9 0.2
#11 GAT 0.9 0.9 0.5 0.9 0.2 0.6 0.8 0.9 0.9
GIN 0.3 0.4 0.8 0.3 0.8 0.3 0.3 0.3 0.8
GCN - - - - - - - - -
#12 GAT 1.0 1.0 1.0 1.0 1.0 1.0 4.0 1.0 1.0
GIN - - - - - - - - -
GCN - - - - - - - - -
#13 GAT - - - - - - - - -
GIN 0.4 0.2 0.4 0.4 0.3 0.4 0.4 0.4 0.4
GCN - - - - - - - - -
#14 GAT - - - - - - - - -
GIN False True True True False False True True True
17Table 6: Best configuration for the GPool. Split sizes (#0), Batch size (#1), Early stopping clf. patience (#2),
Dropoutrate(#3),Batchnormalizing(#4),Dimensionofhiddenlayers(#5),NumberofGNNlayers(#6),Global
poolingtype(#7),Classifierschedulerfactor(#8),Classifierlearningrate(#9),TopKmultiplier(#10),TopKratio
(#11),Numberofheads(#12),ϵ(#13),Trainbleϵ(#14).
Datasets
Param Archi
BZR COX2 DD ENZYMES MUTAG NCI1 NCI109 PROTEINS PTC
GAT [0.5,0.4,0.1] [0.6,0.3,0.1] [0.6,0.3,0.1] [0.4,0.5,0.1] [0.4,0.5,0.1] [0.4,0.5,0.1] [0.4,0.5,0.1] [0.4,0.5,0.1] [0.4,0.5,0.1]
#0 GIN [0.4,0.5,0.1] [0.4,0.5,0.1] [0.4,0.5,0.1] [0.4,0.5,0.1] [0.5,0.4,0.1] [0.4,0.5,0.1] [0.4,0.5,0.1] [0.4,0.5,0.1] [0.5,0.4,0.1]
GCN [0.4,0.5,0.1] [0.4,0.5,0.1] [0.4,0.5,0.1] [0.4,0.5,0.1] [0.5,0.4,0.1] [0.4,0.5,0.1] [0.4,0.5,0.1] [0.4,0.5,0.1] [0.6,0.3,0.1]
GAT 16 16 16 32 32 32 32 32 32
#1 GIN 32 16 16 32 16 32 32 16 32
GCN 16 16 32 32 16 32 32 32 16
GAT 500 650 800 500 500 500 500 650 800
#2 GIN 500 800 800 500 800 500 500 800 800
GCN 650 650 500 500 650 500 500 800 650
GAT 0.0 0.3 0.5 0.1 0.1 0.1 0.1 0.1 0.3
#3 GIN 0.1 0.0 0.5 0.1 0.4 0.1 0.1 0.5 0.2
GCN 0.0 0.0 0.4 0.1 0.0 0.3 0.1 0.5 0.1
GAT True True False True True True True False False
#4 GIN True True False True False True True False False
GCN True True False True True True True True False
GAT 16 64 16 64 64 64 64 16 64
#5 GIN 64 32 16 64 32 64 64 16 128
GCN 128 128 128 64 32 128 64 16 16
GAT 2 4 1 2 2 2 2 1 3
#6 GIN 2 1 1 2 1 2 2 1 4
GCN 2 2 2 2 4 3 2 3 3
GAT [’mean’,’add’] [’mean’] [’mean’,’add’] [’mean’] [’mean’] [’mean’] [’mean’] [’add’] [’add’]
#7 GIN [’mean’] [’mean’] [’mean’,’add’] [’mean’] [’mean’,’add’] [’mean’] [’mean’] [’mean’,’add’] [’add’]
GCN [’mean’] [’mean’] [’add’] [’mean’] [’mean’,’add’] [’mean’,’add’] [’mean’] [’mean’,’add’] [’mean’]
GAT 0.9 0.95 0.9 0.9 0.9 0.9 0.9 0.95 0.99
#8 GIN 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9
GCN 0.95 0.95 0.99 0.9 0.99 0.9 0.9 0.99 0.95
GAT 0.005 0.0005 0.001 0.01 0.01 0.01 0.01 0.001 0.0005
#9 GIN 0.01 0.0001 0.001 0.01 0.01 0.01 0.01 0.001 0.0001
GCN 0.0001 0.0001 0.01 0.01 0.01 0.001 0.01 0.001 0.005
GAT 2.0 0.5 1.5 2.0 2.0 2.0 2.0 1.5 0.5
#10 GIN 2.0 2.0 1.5 2.0 1.0 2.0 2.0 1.5 0.5
GCN 1.0 1.0 1.5 2.0 0.5 1.0 2.0 0.5 1.5
GAT 0.4 0.5 0.4 0.95 0.95 0.95 0.95 0.5 0.5
#11 GIN 0.95 0.9 0.4 0.95 0.5 0.95 0.95 0.4 0.4
GCN 0.95 0.95 0.4 0.95 0.7 0.95 0.95 0.5 0.8
GAT 4.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0
#12 GIN - - - - - - - - -
GCN - - - - - - - - -
GAT - - - - - - - - -
#13 GIN 0.0 0.4 0.1 0.0 0.1 0.0 0.0 0.1 0.2
GCN - - - - - - - - -
GAT - - - - - - - - -
#14 GIN True True False True True True True False True
GCN - - - - - - - - -
18Table7: BestconfigurationforCORES . EarlystoppingPPOpatience(#0),Numberofenvironmentsteps(#1),
N
NumberofPPOepochs(#2),Environmentpenaltysize(#3),RLschedulerfactor(#4),Ratioofthecriticlearning
rate(#5),PPOentropycoefficient(#6),PPOMSEcoefficient(#7),PPOclipvalueϵ(#8),Conformalerrorrateα
(#9),d(#10),λ(#11).
Datasets
Param Archi
BZR COX2 DD ENZYMES MUTAG NCI1 NCI109 PROTEINS PTC
GIN 15 5 5 10 10 5 15 5 5
#0 GCN 10 5 5 15 5 15 10 15 15
GAT 15 5 5 10 15 15 10 10 5
GIN 128 128 128 128 128 128 64 64 64
#1 GCN 32 64 64 128 64 32 32 32 128
GAT 64 32 32 32 128 64 32 32 64
GIN 10 15 15 5 15 15 10 5 15
#2 GCN 10 15 15 5 15 3 10 5 5
GAT 10 15 15 10 5 10 10 5 5
GIN 0.5 0.5 0.5 0.5 0.5 0.5 1.5 1.5 1.0
#3 GCN 0.5 0.5 0.5 1.0 0.5 0.5 0.5 1.5 1.5
GAT 1.5 0.5 1.5 0.5 1.5 1.5 0.5 1.0 1.5
GIN 0.99 0.95 0.95 0.99 0.9 0.95 0.9 0.9 0.95
#4 GCN 0.99 0.95 0.95 0.99 0.95 0.99 0.99 0.9 0.9
GAT 0.9 0.95 0.99 0.99 0.9 0.9 0.99 0.95 0.9
GIN 3.0 2.5 2.5 1.0 3.0 2.5 2.5 1.0 3.0
#5 GCN 2.5 1.5 1.5 2.0 1.5 3.0 2.5 2.5 1.5
GAT 2.5 2.0 2.5 2.5 1.5 2.5 2.5 1.0 1.0
GIN 0.001 0.0001 0.0001 0.01 0.001 0.0001 0.0001 0.001 0.0001
#6 GCN 0.0001 0.01 0.01 0.0001 0.01 0.01 0.0001 0.01 0.001
GAT 0.0001 0.001 0.001 0.0001 0.001 0.0001 0.0001 0.001 0.001
GIN 0.5 5.0 5.0 2.0 1.0 5.0 2.0 3.0 0.5
#7 GCN 1.0 0.5 0.5 0.1 0.5 1.0 1.0 5.0 3.0
GAT 2.0 1.0 3.0 1.0 3.0 2.0 1.0 0.1 3.0
GIN 0.2 0.4 0.4 0.1 0.2 0.4 0.2 0.1 0.4
#8 GCN 0.4 0.3 0.3 0.3 0.3 0.1 0.4 0.3 0.1
GAT 0.2 0.2 0.4 0.4 0.1 0.2 0.4 0.4 0.1
GIN 0.2 0.05 0.05 0.1 0.2 0.05 0.2 0.05 0.05
#9 GCN 0.05 0.2 0.2 0.05 0.2 0.1 0.05 0.2 0.05
GAT 0.2 0.2 0.2 0.05 0.05 0.2 0.05 0.05 0.05
GIN 0.2 0.3 0.3 0.7 0.7 0.3 0.4 0.2 0.5
#10 GCN 0.9 0.8 0.8 0.6 0.8 0.4 0.9 0.5 0.2
GAT 0.4 0.6 0.6 0.9 0.2 0.4 0.9 0.6 0.2
GIN 0.2 1.0 1.0 0.9 0.1 1.0 0.4 0.0 0.4
#11 GCN 0.2 0.7 0.7 1.0 0.7 0.8 0.2 0.8 0.7
GAT 0.4 0.3 0.8 0.2 0.7 0.4 0.2 0.5 0.0
19Table8: BestconfigurationforCORES . EarlystoppingPPOpatience(#0),Numberofenvironmentsteps(#1),
E
NumberofPPOepochs(#2),Environmentpenaltysize(#3),RLschedulerfactor(#4),Ratioofthecriticlearning
rate(#5),PPOentropycoefficient(#6),PPOMSEcoefficient(#7),PPOclipvalueϵ(#8),Conformalerrorrateα
(#9),d(#10),λ(#11).
Datasets
Param Archi
BZR COX2 DD ENZYMES MUTAG NCI1 NCI109 PROTEINS PTC
GIN 15 5 5 10 10 5 15 5 5
#0 GCN 10 5 5 15 5 15 10 15 15
GAT 15 5 5 10 15 15 10 10 5
GIN 128 128 128 128 128 128 64 64 64
#1 GCN 32 64 64 128 64 32 32 32 128
GAT 64 32 32 32 128 64 32 32 64
GIN 10 15 15 5 15 15 10 5 15
#2 GCN 10 15 15 5 15 3 10 5 5
GAT 10 15 15 10 5 10 10 5 5
GIN 0.5 0.5 0.5 0.5 0.5 0.5 1.5 1.5 1.0
#3 GCN 0.5 0.5 0.5 1.0 0.5 0.5 0.5 1.5 1.5
GAT 1.5 0.5 1.5 0.5 1.5 1.5 0.5 1.0 1.5
GIN 0.99 0.95 0.95 0.99 0.9 0.95 0.9 0.9 0.95
#4 GCN 0.99 0.95 0.95 0.99 0.95 0.99 0.99 0.9 0.9
GAT 0.9 0.95 0.99 0.99 0.9 0.9 0.99 0.95 0.9
GIN 3.0 2.5 2.5 1.0 3.0 2.5 2.5 1.0 3.0
#5 GCN 2.5 1.5 1.5 2.0 1.5 3.0 2.5 2.5 1.5
GAT 2.5 2.0 2.5 2.5 1.5 2.5 2.5 1.0 1.0
GIN 0.001 0.0001 0.0001 0.01 0.001 0.0001 0.0001 0.001 0.0001
#6 GCN 0.0001 0.01 0.01 0.0001 0.01 0.01 0.0001 0.01 0.001
GAT 0.0001 0.001 0.001 0.0001 0.001 0.0001 0.0001 0.001 0.001
GIN 0.5 5.0 5.0 2.0 1.0 5.0 2.0 3.0 0.5
#7 GCN 1.0 0.5 0.5 0.1 0.5 1.0 1.0 5.0 3.0
GAT 2.0 1.0 3.0 1.0 3.0 2.0 1.0 0.1 3.0
GIN 0.2 0.4 0.4 0.1 0.2 0.4 0.2 0.1 0.4
#8 GCN 0.4 0.3 0.3 0.3 0.3 0.1 0.4 0.3 0.1
GAT 0.2 0.2 0.4 0.4 0.1 0.2 0.4 0.4 0.1
GIN 0.2 0.05 0.05 0.1 0.2 0.05 0.2 0.05 0.05
#9 GCN 0.05 0.2 0.2 0.05 0.2 0.1 0.05 0.2 0.05
GAT 0.2 0.2 0.2 0.05 0.05 0.2 0.05 0.05 0.05
GIN 0.2 0.3 0.3 0.7 0.7 0.3 0.4 0.2 0.5
#10 GCN 0.9 0.8 0.8 0.6 0.8 0.4 0.9 0.5 0.2
GAT 0.4 0.6 0.6 0.9 0.2 0.4 0.9 0.6 0.2
GIN 0.2 1.0 1.0 0.9 0.1 1.0 0.4 0.0 0.4
#11 GCN 0.2 0.7 0.7 1.0 0.7 0.8 0.2 0.8 0.7
GAT 0.4 0.3 0.8 0.2 0.7 0.4 0.2 0.5 0.0
20Model Dataset
CORESN CORESE COX2 DD NCI1 NCI109 PROTEINS
90 90 90 90
80 80 80 80
70 70 70 70
60 60 60 60
50 50 50 50
0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
d d λ λ
(a)dversusaccuracy (a)λversusaccuracy
100 100
80 80 80 80
60 60 60 60
40 40 40 40
20 20 20 20
0
0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
d d λ λ
(b)dversussparsity (b)λversussparsity
Varyingd λ=0.0 Varyingd λ=0.0 Varyingλ d=0.1 Varyingλ d=0.1
100 100 100 100
90 90 90 90
80 80 80 80
70 70 70 70
60 60 60 60
50 50 50 50
0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100
NodeRatio(%) EdgeRatio(%) NodeRatio(%) EdgeRatio(%)
(c)Sparsityversusaccuracy (c)Sparsityversusaccuracy
Figure6: Ablationstudyonthemaximumdesiredratio Figure7: Ablationstudyonλfor5differentrunsforthe
dfor5differentrunsfortheGINarchitecture. GINarchitecture.
B Extraresultsimpactofhyperparameters
Inthissection,wepresentanablationstudyontheimpactoftwokeyhyperparametersofCORES,namelydandλ,for
thedatasetsnotincludedinthemainmanuscript: COX2,DD,NCI1,NCI109,andPROTEINS.Figure6illustratesthe
resultsoftheablationstudyforparameterd,whiletheresultsforparameterλareshowninFigure7.
Thefindingsfromthisablationstudyareconsistentwiththosereportedinthemainmanuscript. Inthetoprowfigures,
weobservethattheaccuracyremainsrelativelystablewhenvaryingeitherdorλ,improvingonlyasthevaluesincrease
forcertaindatasets,suchasNCI1orNCI109. Inthemiddlerowfigures,wefindapositivecorrelationbetweenboth
parametersandthenode/edgeratioformostofthedatasets,andnocorrelationforothers. Remarkably,thecorrelation
isnevernegative. Finally,thebottomrowfiguresrevealapositivecorrelationbetweenthenode/edgeratioandaccuracy
forthemajorityofthedatasets. ThispatternholdsexceptfortheDDdataset,whichpresentanegativecorrelation.
Interestinglythisdatasetpresentsthemostsignificantdisparitiesintermsofthenumberofnodesandedgescompared
totheotherdatasets(seeTable1).
21
)%(ycaruccA
)%(oitaRedoN
)%(ycaruccA
)%(ycaruccA
)%(oitaRegdE
)%(ycaruccA
)%(ycaruccA
)%(oitaRedoN
)%(ycaruccA
)%(ycaruccA
)%(oitaRegdE
)%(ycaruccATable9: ModelcomparisonresultswithGATarchitecture. Weshowthemeanover5independentrunsandthe
standarddeviationasthesubindex. Allmetricsareshowninpercentage. Thelastrowsincludetheaveragerankingof
themodelacrossdatasets.
Models
Dataset Metric
GNN SAGPool GPool CORES CORES
N E
Accuracy 80.984.36 83.903.27 79.516.12 83.904.43 81.464.08
BZR NodeRatio - - 41.110.11 37.9910.27 100.000.00
EdgeRatio - - 20.930.70 29.928.83 57.2621.50
Accuracy 82.134.15 80.004.41 77.873.87 81.704.41 80.852.61
COX2 NodeRatio - - 50.620.09 77.912.29 100.000.00
EdgeRatio - - 20.750.51 62.623.24 66.180.38
Accuracy 76.103.02 75.763.72 77.801.84 75.424.19 76.442.19
DD NodeRatio - - 40.240.03 75.161.39 100.000.00
EdgeRatio - - 15.630.29 56.422.16 58.622.51
Accuracy 71.3310.63 45.007.64 63.936.35 69.844.43 67.5412.23
ENZYMES NodeRatio - - 96.670.14 80.261.03 100.000.00
EdgeRatio - - 93.270.23 65.921.80 61.609.06
Accuracy 81.436.39 88.573.91 78.577.14 80.005.98 81.436.02
MUTAG NodeRatio - - 97.810.36 75.724.04 100.000.00
EdgeRatio - - 95.652.50 58.746.05 55.2514.77
Accuracy 79.421.34 68.931.37 72.432.11 73.432.67 75.381.22
NCI1 NodeRatio - - 96.890.08 63.493.58 100.000.00
EdgeRatio - - 94.070.34 53.332.81 88.686.67
Accuracy 76.901.34 67.464.31 67.223.25 74.722.25 72.782.29
NCI109 NodeRatio - - 96.790.04 71.5511.30 100.000.00
EdgeRatio - - 94.700.98 62.3712.96 61.3713.13
Accuracy 75.363.00 72.683.26 73.213.22 73.393.86 73.571.62
PROTEINS NodeRatio - - 51.150.17 67.942.97 100.000.00
EdgeRatio - - 28.390.42 46.564.27 52.821.86
Accuracy 64.004.33 65.006.39 62.444.12 62.964.17 59.434.69
PTC NodeRatio - - 52.460.41 45.616.00 100.000.00
EdgeRatio - - 36.583.53 21.246.53 76.3614.75
Accuracy 1.89 3.33 3.89 3.00 2.89
Avg.Rank
NodeRatio - - 1.67 1.33 -
EdgeRatio - - 2.33 1.56 2.11
C Extracomparisonresults
Here,weanalyzetheresultsofhowswitchingthebasemodelaffectstheperformance. Table10andTable9summarizes
theresultsfortwobasearchitecturesGCNandGAT.Itcanbeobservedthatevenwhenthebasearchitecturechanges
ourmodelsCORES andCORES achievecompetitiveaccuracycomparedtotherespectivebaselineandachievetop
N E
rankingsintermsofnode/edgeratio.
22Table10: ModelcomparisonresultswithGCNarchitectureWeshowthemeanover5independentrunsandthe
standarddeviationasthesubindex. Allmetricsareshowninpercentage. Thelastrowsincludetheaveragerankingof
themodelacrossdatasets.
Models
Dataset Metric
GNN SAGPool GPool CORES CORES
N E
Accuracy 86.834.08 83.414.69 84.394.08 82.444.69 80.985.29
BZR NodeRatio - - 96.460.19 76.986.06 100.000.00
EdgeRatio - - 92.870.58 61.1711.31 46.165.68
Accuracy 79.154.85 80.433.81 80.001.90 80.853.01 81.704.90
COX2 NodeRatio - - 96.140.18 72.494.24 100.000.00
EdgeRatio - - 92.670.71 53.198.07 31.2723.60
Accuracy 78.473.47 75.934.39 76.105.06 79.322.58 75.764.05
DD NodeRatio - - 40.230.03 56.442.50 100.000.00
EdgeRatio - - 16.480.13 32.012.89 51.391.90
Accuracy 66.005.35 52.005.70 64.594.86 58.318.47 57.0211.62
ENZYMES NodeRatio - - 96.670.14 77.177.28 100.000.00
EdgeRatio - - 93.390.35 62.339.26 74.2035.99
Accuracy 81.433.91 80.003.19 78.575.05 81.4310.83 87.146.56
MUTAG NodeRatio - - 73.130.55 79.942.79 100.000.00
EdgeRatio - - 68.222.47 63.524.23 32.668.79
Accuracy 77.332.09 73.581.64 72.142.64 66.457.71 65.946.31
NCI1 NodeRatio - - 96.890.08 76.4717.88 100.000.00
EdgeRatio - - 96.060.20 64.0218.84 37.9814.34
Accuracy 79.321.57 71.192.03 71.532.12 76.082.71 77.142.02
NCI109 NodeRatio - - 96.790.04 78.784.15 100.000.00
EdgeRatio - - 95.820.11 67.084.42 92.021.93
Accuracy 72.861.02 72.863.13 75.182.75 73.212.45 75.891.94
PROTEINS NodeRatio - - 51.150.17 74.666.10 100.000.00
EdgeRatio - - 28.150.67 56.129.77 64.730.98
Accuracy 57.787.71 64.573.83 63.431.28 62.862.86 63.433.13
PTC NodeRatio - - 83.800.74 64.104.19 100.000.00
EdgeRatio - - 78.213.76 40.216.93 44.475.29
Accuracy 1.89 3.33 3.89 3.00 2.89
Avg.Rank
NodeRatio - - 1.67 1.33 -
EdgeRatio - - 2.56 1.67 1.78
23Table11: Trainingtimecomparison. Weshowtheaveragetrainingtimeinsecondsalongwiththestandarddeviation
ofeachmodeloneachdatasetforoneepochwithabatchsizeofone. WeusetheGINGNNarchitecture.
Dataset FullModels SparseModels
Vanilla DiffPool SUGAR GPool CORES CORES
N E
BZR 2.640.18 4.230.12 - 3.440.27 186.9661.70 121.942.00
COX2 3.070.15 4.830.17 - 10.002.70 411.9385.97 203.931.86
DD 6.130.13 - - 33.314.90 1848.211.12 1857.591.13
ENZYMES 2.400.04 6.260.31 132.863.77 6.080.30 233.786.65 287.952.20
MUTAG 1.090.03 1.630.04 47.823.45 1.710.19 24.971.55 22.940.30
NCI1 15.440.19 41.743.03 1143.8545.27 48.058.12 1596.95107.83 1826.4151.06
NCI109 16.681.15 43.730.44 1195.1047.42 43.463.09 1533.5915.51 1756.8437.12
PROTEINS 5.800.23 11.791.19 511.2122.63 13.582.58 484.9714.74 505.892.70
PTC 1.680.16 3.000.07 95.781.47 3.280.18 64.601.23 67.190.32
D Timecomplexity
Weanalyzethetrainingandinferencetimesofallmodelsoneachdataset. Weruneachmodeloneverydatasetfor
asingleepoch, utilizingabatchsizeofone. Toensurestatisticalsignificanceandreliabilityinourmeasurements,
werepeateachexperimenttentimes. Thisrepetitionenablesustocalculatetheaveragetrainingandinferencetimes
accurately. Suchameticulousapproachguaranteesrobustandpreciseperformanceassessmentforourmodelsacross
thediversesetofdatasetsunderexamination.
Table11andTable12showthecompleteresultsforthetrainingandinferencestimes,respectively. Ifwefocuson
Table11,weobservethemodelsthatonlyrelyonasingleforwardpassaretwoordersofmagnitudefasterthanthe
modelsthatrelyonareinforcementlearning-basedapproach. Still,weobservethatCORESachievescomparablespeed
asSUGAR,andsometimesitisfaster,e.g.,withPTCorMUTAG.Intermsofinferencetime,inTable12,wecan
observethateventhoughCORESisstilloneorderofmagnitudeslowerthantheFullModels,itisconsiderablyfaster
thanSUGAR.
Table 12: Inference time comparison. We show the average inference time in seconds along with the standard
deviationofeachmodeloneachdataset. WeusetheGINGNNarchitecture.
Dataset FullModels SparseModels
Vanilla DiffPool SUGAR GPool CORES CORES
N E
BZR 0.00250.0001 0.14470.0048 - 0.00450.0003 0.05400.0257 0.04250.0011
COX2 0.00260.0001 0.15500.0093 - 0.00620.0008 0.10310.0349 0.05260.0014
DD 0.00280.0004 - - 0.00850.0006 0.22620.0842 0.12060.0016
ENZYMES 0.00240.0000 0.19920.0080 0.13990.0094 0.00630.0007 0.04960.0014 0.06250.0019
MUTAG 0.00290.0001 0.05960.0034 0.01350.0036 0.00650.0017 0.04480.0056 0.04150.0012
NCI1 0.00220.0001 1.37560.1246 1.01870.1040 0.00580.0004 0.05300.0026 0.13010.0602
NCI109 0.00230.0001 1.36850.0988 0.94770.0132 0.00550.0002 0.05000.0006 0.05630.0009
PROTEINS 0.00250.0001 0.38350.0619 3.93750.0347 0.00580.0007 0.04370.0016 0.04390.0008
PTC 0.00280.0001 0.10760.0050 0.03980.0099 0.00580.0005 0.03950.0013 0.04180.0005
24