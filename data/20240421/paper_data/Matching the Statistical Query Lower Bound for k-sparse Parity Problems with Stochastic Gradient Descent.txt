Matching the Statistical Query Lower Bound for k-sparse
Parity Problems with Stochastic Gradient Descent
Yiwen Kou∗† Zixiang Chen∗‡ Quanquan Gu§ Sham M. Kakade¶
Abstract
The k-parity problem is a classical problem in computational complexity and algorithmic
theory, serving as a key benchmark for understanding computational classes. In this paper, we
solve the k-parity problem with stochastic gradient descent (SGD) on two-layer fully-connected
neural networks. We demonstrate that SGD can efficiently solve the k-sparse parity problem
√
on a d-dimensional hypercube (k ≤ O( d)) with a sample complexity of O(cid:101)(dk−1) using 2Θ(k)
neurons,thusmatchingtheestablishedΩ(dk)lowerboundsofStatisticalQuery(SQ)models. Our
theoretical analysis begins by constructing a good neural network capable of correctly solving the
k-parity problem. We then demonstrate how a trained neural network with SGD can effectively
approximate this good network, solving the k-parity problem with small statistical errors. Our
theoretical results and findings are supported by empirical evidence, showcasing the efficiency
and efficacy of our approach.
1 Introduction
The k-parity problem, defined on a binary sequence of length d, is fundamental in the field of
computational complexity and algorithmic theory. This problem involves the analysis of parity
conditions in a subset of cardinality k by assessing if the occurrence of 1’s in this subset is even or
odd. The complexity of the problem escalates as the parameter k increases. Its significance, while
evidently practical, is rooted in its theoretical implications; it serves as a vital benchmark in the
study of computational complexity classes and has profound implications for our understanding of P
versus NP (Vardy, 1997; Downey et al., 1999; Dumer et al., 2003) and other cornerstone questions
in computational theory (Blum, 2005; Klivans et al., 2006). Furthermore, the k-parity problem’s
complexity underpins significant theoretical constructs in error detection and information theory
(Dutta et al., 2008), and is instrumental in delineating the limitations and potential of algorithmic
efficiency (Farhi et al., 1998). This paper tackles the k-sparse parity problem (Daniely and Malach,
2020), where the focus is on the parity of a subset with cardinality k ≪ d.
∗Equal contribution
†Department of Computer Science, University of California, Los Angeles, CA 90095, USA; e-mail:
evankou@cs.ucla.edu
‡Department of Computer Science, University of California, Los Angeles, CA 90095, USA; e-mail:
chenzx19@cs.ucla.edu
§DepartmentofComputerScience,UniversityofCalifornia,LosAngeles,CA90095,USA;e-mail: qgu@cs.ucla.edu
¶Department of Computer Science & Department of Statistics, Harvard University, Cambridge, MA 02138, USA;
e-mail: sham@seas.harvard.edu
1
4202
rpA
81
]GL.sc[
1v67321.4042:viXraRecentprogressincomputationallearningtheoryhasfocusedonimprovingthesamplecomplexity
guarantees for learning k-sparse parity functions using stochastic gradient descent (SGD). Under the
framework of the Statistical Query (SQ) model (Kearns, 1998), it has been established that learning
the k-sparse parity function requires a minimum of Ω(dk) queries (Barak et al., 2022), highlighting
the challenge in efficiently learning these functions. In particular, for a neuron network trained by
SGD algorithm, the total number of scalar queries required is m·d·n, where m is the number
of neurons, d is the dimension, m·d is the total number of scalar queries used for one example,
and n is the total number of fresh examples required in the algorithm. Considerable effort has
been devoted to establishing sample complexity upper bounds for the special XOR case (k = 2),
with notable successes including O(d) sample complexity using infinite-width or exponential-width
(O(2d)) two-layer neural networks trained via gradient flow (Wei et al., 2019; Chizat and Bach, 2020;
Telgarsky, 2022), and O(d2) sample complexity with polynomial-width networks via SGD (Ji and
Telgarsky, 2020; Telgarsky, 2022). A significant advancement in solving the 2-parity problem was
recently introduced by Glasgow (2023). They proved a sample complexity bound of O(cid:101)(d) using a
two-layer ReLU network with logarithmic width trained with SGD, thus matching the SQ lower
bound.
In the general case of k ≥ 2, Barak et al. (2022) has marked significant progress, achieving a
sample complexity of O(cid:101)(dk+1) with a network width requirement of 2Θ(k), which is independent
of the input dimension d. Additionally, Barak et al. (2022) demonstrated that the neural tangent
kernel (NTK)-based method (Jacot et al., 2018) requires a network of polynomial width dΩ(k) to
solve the k-parity problem. In a recent work, Suzuki et al. (2023) attained a sample complexity
of O(d). However, it is important to note that they employed the mean-field Langevin dynamics
(MFLD) (Mei et al., 2018; Hu et al., 2019), which requires neural networks of exponential width
O(ed), and an O(ed) number of iterations to converge. Thus, their method is not computationally
practical and doesn’t match the statistical query lower bound. Abbe et al. (2023a) introduced
the leap-k function for binary and Gaussian sequences, which extended the scope of the k-parity
problem. They also proved Correlational Statistical Query (CSQ) lower bounds for learning leap-k
function with both Gaussian and Boolean input. They proved novel CSQ lower bounds of Ω(dk−1)
for Boolean input in Proposition 30 and Ω(dk/2) for Gaussian input in Proposition 31, which suggests
that learning from Boolean input can be intrinsically harder than learning from Gaussian input.
They also proved SGD can learn low dimensional target functions with Gaussian isotropic data
and 2-layer neural networks using n ≳ dLeap−1 examples. However, their upper bound analysis is
based on the assumption that the input data x follows a Gaussian distribution and relies on Hermite
polynomials, making it unclear how to extend them to analyze Boolean input. This raises a natural
but unresolved question:
Is it possible to match the statistical query lower bound for k-sparse parity problems with stochastic
gradient descent?
In this paper, we give an affirmative answer to the above question. In particular, we consider
the standard k-sparse parity problem, where the input x is drawn from a uniform distribution over
d-dimensional hypercube Unif({−1,1}d). Our approach involves training two-layer fully-connected
neural networks with m = 2Θ(k) width using SGD with a batch size of B = O(dk−1polylog(d)). We
prove that the neural network can achieve a constant-order positive margin with high probability
2after training for T = O(logd) iterations. Therefore, the total number of examples required in
our approach is n = BT = O(cid:101)(dk−1). The total number of scalar queries required in our paper is
m·d·n = 2Θ(k) ·d·(dk−1 ·polylogd) = 2Θ(k)dk ·polylogd, where m is the number of neurons, d
is the dimension, m·d is the total number of scalar queries used for one example, and n is the
total number of fresh examples required in our algorithm. Abbe et al. (2023a) also gave a CSQ
lower bound in Proposition 30 for learning d-dimensional uniform Boolean data, which leads to the
sample complexity lower bound n ≳ dk−1. Thus, we also match the CSQ lower bound in Abbe et al.
(2023a).
1.1 Our Contributions
The Statistical Query (SQ) lower bound demonstrates that, regardless of architecture, gradient
descent requires a query complexity of Ω(dk) for learning k-sparse d-dimensional parities under a
constant noise level. We push the sample complexity frontier of k-sparse party problem to O(cid:101)(dk−1)
via SGD, specifically with online stochastic sign gradient descent. Notably, for the XOR problem
(k = 2) which is a special case of k-parities, Glasgow (2023) achieved a sample complexity of O(cid:101)(d)
via SGD. Nevertheless, the algorithm and analysis used in our paper is quite different from theirs.
Our main result is stated in the following informal theorem:
Theorem 1.1 (Informal) Utilizing sign SGD with a batch size of O(cid:101)(dk−1) on two-layer fully-
connected neural networks of width 2Θ(k), we can find a solution to the k-parity problem with a small
test error in O(klogd) iterations.
The above theorem improves the sample complexity in Barak et al. (2022) from O(cid:101)(dk+1) to O(cid:101)(dk−1).
Moreover, the total number of queries required is O(cid:101)(dk) which matches the SQ/CSQ lower bound
up to logarithmic factors. Additionally, this result matches the best sample complexity when solving
2-parity problem (Glasgow, 2023). It is worth noting that our results only require two-layer fully
connected neural networks with 2Θ(k) width and SGD training with O(klogd) iterations, which
gives an efficient algorithm.
Notation. We use [N] to denote the index set {1,...,N}. We use lowercase letters, lowercase
boldface letters, and uppercase boldface letters to denote scalars, vectors, and matrices, respectively.
For a vector v = (v ,··· ,v )⊤, we denote by ∥v∥ := ((cid:80)d v2)1/2 its L norm. For a vector
1 d 2 j=1 j 2
v = (v ,··· ,v )⊤, we denote by v := (v ,··· ,v )⊤ its truncated vector ranging from the
1 d [i1:i2] i1 i2
i -th coordinate to the i -th coordinate. For two sequence {a } and {b }, we denote a = O(b ) if
1 2 k k k k
|a | ≤ C|b | for some absolute constant C, denote a = Ω(b ) if b = O(a ), and denote a = Θ(b )
k k k k k k k k
if a
k
= O(b k) and a
k
= Ω(b k). We also denote a
k
= o(b k) if lim|a k/b k| = 0. We use O(cid:101)(·) and Ω(cid:101)(·)
to omit logarithmic terms in the notation. Finally, we denote x = poly(y ) if x = O(yD) for some
n n n n
positive constant D, and x = polylog(y ) if x = poly(log(y )).
n n n n
2 Related Work
XOR. The performance of two-layer neural networks in the task of learning 2-parity has been the
subject of extensive research in recent years. Wei et al. (2019); Chizat and Bach (2020); Telgarsky
(2022)employedmargintechniquestoestablishtheconvergencetowardaglobalmarginmaximization
solution, utilizing gradient flow and sample complexity of O(d). Notably, Wei et al. (2019) and
Chizat and Bach (2020) employed infinite-width neural networks, while Telgarsky (2022) employed a
3more relaxed width condition of O(dd). A significant breakthrough in this domain was achieved by
Glasgow (2023), who demonstrated a sample complexity of O(cid:101)(d) by employing SGD in conjunction
with a ReLU network of width polylog(d). Furthermore, several other studies have shown that when
input distribution follows Gaussian distribution, neural networks can be effectively trained to learn
the XOR cluster distribution (Frei et al., 2022; Meng et al., 2023; Xu et al., 2023). A comparison
between the results in this paper and those of related works solving the binary 2-parity problem can
be found in Table 1.
k-parity Problem. The challenge of training neural networks to learn parities has been explored
in previous research from diverse angles. Several papers studied learning the k-parity function by
using two-layer neural networks. Daniely and Malach (2020) studied learning k-parity function by
applying gradient descent on the population risk (infinite samples). Notably, Barak et al. (2022)
presentedbothempiricalandtheoreticalevidencethatthek-parityfunctioncanbeeffectivelylearned
using SGD and a neural network of constant width, demonstrating a sample complexity of O(dk+1)
and query complexity of O(dk+2). Edelman et al. (2024) demonstrated that sparse initialization
and increased network width lead to improvements in sample efficiency. Specifically, Theorem 4 in
Edelman et al. (2024) shows that the sample complexity can be reduced at the cost of increasing
the width. However, the best total number of statistical queries remains the same as the O(dk+2)
result in Barak et al. (2022). Suzuki et al. (2023) reported achieving a sample complexity of O(d)
by employing mean-field Langevin dynamics (MFLD). Furthermore, Abbe et al. (2022) and Abbe
et al. (2023a) introduced a novel complexity measure termed “leap” and established that leap-k
(with k-parity as a special case) functions can be learned through SGD with a sample complexity of
O(cid:101)(dmax(k−1,1)). Additionally,Abbeetal.(2023b)demonstratedthatacurriculum-basednoisy-GD(or
SGD) approach could attain a sample complexity of O(d), provided the data distribution comprises
a mix of sparse and dense inputs. The conditions outlined in this paper are compared to those from
related work involving uniform Boolean data distribution, as presented in Table 2.
3 Problem Setup
In this section, we introduce the k-sparse parity problem and the neural network we consider in this
paper.
Definition 3.1 (k-parity) Each data point (x,y) with x ∈ Rd and y ∈ {−1,1} is generated from
the following distribution D where A is the non-empty set satisfying A ⊆ [n]:
A
1. x ∼ {−1,1} as a uniform random bit for j ∈ [d].
j
2. The label y is generated as Π x .
j∈A j
The k-parity problem with dimension d is then defined as the task of recovering A, where |A| = k,
using samples from D .
A
Without loss of generality, we assume that A = {1,...,k} if |A| = k. Under this assumption,
we denote D by D for simplicity. This k-parity problem in Definition 3.1 is a classical one, which
A
has been studied by Daniely and Malach (2020); Barak et al. (2022) using neural network learning.
When restricted to the 2-parity function, the problem is reduced to the XOR problem (Wei et al.,
2019).
Two-layer Neural Networks. We consider a two-layer fully-connected neural network. The
4Activation Loss Width(m) Sample(n) Iterations(t)
Algorithm
Function Function Requirement Requirement toConverge
Theorem2.1
ReLU logistic WFwithNoise ∞ d/ϵ ∞
(Weietal.,2019)
Theorem8
2-homogenous logistic/hinge WF ∞ d/ϵ ∞
(ChizatandBach,2020)
Theorem3.3
ReLU logistic scalarGF dd d/ϵ d/ϵ
(Telgarsky,2022)
Theorem3.2
ReLU logistic SGD d8 d2/ϵ d2/ϵ
(JiandTelgarsky,2020)
Theorem2.1
ReLU logistic SGD d2 d2/ϵ d2/ϵ
(Telgarsky,2022)
Theorem3.1
ReLU logistic SGD polylog(d) d·polylog(d) polylog(d)
(Glasgow,2023)
Ours x2 correlation SignSGD O(1) d·polylog(d) logd
Table 1: Comparison of related works on the 2-parity problem. We mainly focus on the dimension d
and test error ϵ and treat other arguments as constant. Here WF means Wasserstein flow technique
liesin themeanfield analysis, and GFmeans gradient flow. Thesample requirement andconvergence
iterationinbothGlasgow(2023)andourmethoddonotexplicitlydependonthetesterrorϵ. Instead,
the dependence on ϵ is implicitly incorporated within the condition for d. Specifically, our approach
requiresthatd ≥ Clog2(2m/ϵ)whileGlasgow(2023)requiresd ≥ exp((1/ϵ)C)whereC isaconstant.
.
Activation Loss Width(m) Sample(n) Iterations(t)
Algorithm
Function Function Requirement Requirement toConverge
Theorem4
ReLU hinge SGD 2Θ(k) dk+1·log(d/ϵ)/ϵ2 d/ϵ2
(Baraketal.,2022)
Theorem4
ReLU hinge SGD (d/s)k (s/k)k−1d2log(d)/ϵ2 1/ϵ2
(Edelmanetal.,2024)
Corollary1
VariantofTanh logistic MFLD ed d/ϵ ed
(Suzukietal.,2023)
Ours xk correlation SignSGD 2Θ(k) dk−1·polylog(d) logd
Table 2: Comparison of related works for the general k-parity problem, focusing primarily on the
dimension d and error ϵ, treating other parameters as constants. s in Edelman et al. (2024) is
the sparsity of the initialization that satisfying s > k. The activation function by Suzuki et al.
(2023) is defined as h (x) = R¯[tanh(x⊤w +w )+2tanh(w )]/3, where w = (w ,w ,w )⊤ ∈ Rd+2
w 1 2 3 1 2 3
and R¯ is a hyper-parameter determining the network’s scale. For the sample requirement and
convergence iteration, we focus on the dependency of d,ϵ and omit another terms. Our method’s
sample requirement and convergence iteration are independent of the test error ϵ, instead relying on
a condition for d that implicitly includes ϵ. Specifically, we require d ≥ Clog2(2m/ϵ).
network is defined as follows:
m
(cid:88)
f(W,x) = a σ(⟨w ,x⟩), (3.1)
r r
r=1
5Modified vs. Standard Sign Function
1.00 sign(x)
sign(x)
0.75
0.50
0.25
0.00
0.25
0.50
0.75
1.00
2.0 1.5 1.0 0.5 0.0 0.5 1.0 1.5 2.0
x
Figure 1: The plot above illustrates the comparison between the modified sign function s(cid:103)ign(x)(ρ =
0.5) and the standard sign function sign(x). The s(cid:103)ign(x) function introduces a ‘dead zone’ between
−ρ and ρ where the function value is zero, which is not present in the standard sign function. This
modification effectively creates a threshold effect, only outputting non-zero values when the input x
exceeds the specified bounds of ρ in either direction.
where m is the number of neurons. Here, we employ a polynomial activation function defined by
σ(z) = zk. The term w ∈ Rd represents the weight vector for the r-th neuron, and W denotes the
r
aggregate of all first-layer model weights. The coefficients a , which constitute the fixed second-layer
r
weights, are sampled uniformly from the set {−1,1}.
Algorithm. We train the above neural network model by minimizing the correlation loss function:
L (W) = E ℓ[y·f(W,x)],
D (x,y)∼D
where ℓ(z) = 1−z. We consider binary initialization with w(0) ∼ Unif({±1}d), which is widely used
r
for neural networks solving parity problem (Barak et al., 2022; Abbe and Boix-Adsera, 2022). Our
model then employs a stochastic sign gradient descent with constant step size and weight decay (i.e.,
ℓ norm regularization), formulated as follows:
2
(cid:16)∂L(t)(cid:17)
W(t+1) = (1−λη)W(t)−η·s(cid:103)ign ,
∂W
where λ > 0 is the weight decay parameter, η > 0 is the step size, and the modified sign function,
s(cid:103)ign(x), is defined as

1, for x ≥ ρ,


s(cid:103)ign(x) = sign(x)·1
{|x|≥ρ}
= 0, for −ρ < x < ρ,

 −1, for x ≤ −ρ.
6
yHere, ρ > 0 is a threshold parameter. In this context, L(t) is computed using a randomly sampled
online batch S with batch size |S | = B:
t t
1 (cid:88)
L(t) = ℓ[y·f(W(t),x)].
B
(x,y)∈St
Consequently, the update rule for each w is given by:
r
(cid:18) (cid:19)
1 (cid:88)
w r(t+1) = (1−λη)w r(t)+η·s(cid:103)ign
B
σ′(⟨w r(t),x⟩)·a ryx , (3.2)
(x,y)∈St
where s(cid:103)ign is applied on an elementwise basis.
Remark 3.2 The choice of sign SGD over standard SGD stems primarily from our adoption of the
polynomial activation function σ(z) = zk. As later explained in Section 4, this specific activation
function is pivotal in constructing a neural network that accurately tackles the k-parity problem.
However, it introduces a trade-off: the gradient’s dependency on the weights becomes polynomial
rather than linear. Sign SGD addresses this issue by normalizing the gradient, ensuring that all
neurons progress uniformly towards identifying the parity. Moreover, incorporating a threshold within
the sign function plays a crucial role as it effectively nullifies the gradient of noisy coordinates. This,
combined with regularization, aids in reducing noise, thereby enhancing the overall performance of
the network.
4 Main Results
In this section, we begin by demonstrating the capability of the two-layer fully connected neural
networks (3.1) to classify all samples correctly. Specifically, we construct the following good network:
2k
(cid:88)
f(W∗,x) = a∗σ(⟨w∗,x⟩), (4.1)
r r
r=1
where (cid:8) w r∗ ,[1:k](cid:12) (cid:12)r ∈ [2k](cid:9) = {±1}k, a∗
r
= (cid:81)k j=1sign(w r∗ ,j) and w r∗
,[k+1:d]
= 0
d−k
for any r ∈ [2k].
Notably, leveraging the inherent symmetry within our neural network model, we can formally assert
the following proposition: yf(W∗,x) = y′f(W∗,x′) for any (y,x) and (y′,x′) generated from D .
A
The subsequent proposition demonstrates the precise value of the margin.
Proposition 4.1 For any data point (x,y) generated from the distribution D , it holds that
A
yf(W∗,x) = k!·2k. (4.2)
Proof Given a (y,x) ∈ D , we have that y = Πk x . We divide the neurons into (k+1) groups
A i=1 i
Ω ,i ∈ {0,...,k}. A neuron r ∈ Ω if and only if (cid:80)k 1(w∗ = x ) = i. Then we have that
i i s=j r j
2k
(cid:88)
yf(W∗,x) = (y·a∗)·σ(⟨w∗,x⟩)
r r
r=1
7k
(cid:88) (cid:88)
= (y·a∗)·σ(⟨w∗,x⟩)
r r
i=0r∈Ωi
k (cid:18) k k (cid:19)
(cid:88) (cid:88) (cid:89) (cid:89)
= sign(x )· sign(w∗ ) ·σ(⟨w∗,x⟩)
j r,j r
i=0r∈Ωi j=1 j=1
k (cid:18) (cid:19)
(cid:88) k
= (−1)iσ(k−2i)
i
i=0
= k!·2k,
where the third equality is due to the fact that y = (cid:81)k sign(x ) and a∗ = (cid:81)k sign(w∗ ), the
j=1 j r j=1 r,j
fourthequalityisduetothedefinitionofΩ ,thelastequalityholdsbecauseσ isk-thorderpolynomial
i
activation function and Lemma E.2.
Therefore, we can conclude that for any (x,y), we have yf(W∗,x) = k!·2k > 0. We will demonstrate
in the next section that training using large batch size online SGD, as long as Condition 4.2 is met,
will lead to the trained neural network f(W(T),x) approximating (m/2k+1)·f(W∗,x) effectively
after T = O(klog(d)) iterations. Our main theorem is based on the following conditions on the
training strategy.
Condition 4.2 Suppose there exists a sufficiently large constant C, such that the following conditions
hold:
• Neural network width m satisfies m ≥ C ·5klog(1/δ).
• Dimension is sufficiently large: d ≥ Clog2(2m/ϵ).
• Online SGD batch size B satisfies B ≥ C2k((k−1)!)−2dk−1logk−1(16mdBT/δ)log2(8mdT/δ).
• Learning rate η satisfies η ≤ C−1.
• Regularization parameter λ is taken as λ = 1.
• The threshold ρ for the modified sign function satisfies ρ = 0.1k!.
In the k-parity problem, the label y is determined by a set of k bits. Consequently, the total
count of distinct features is 2k, reflecting all possible combinations of these bits. The condition
of m is established to guarantee a roughly equal number of neurons within the good neuron class,
each correctly aligned with distinct features. The condition of d ensures that the problem is in a
sufficiently high-dimensional setting. The condition of m,d implies that d ≥ Ω(log2m) ≥ Ω(k2),
which is a mild requirement for the parity k. In comparison, Barak et al. (2022) requires d ≥ Ω(k4)
for neural networks solving k-parity problem. By Stirling’s approximation, the condition of B can
be simplified to B ≥ Ω(cid:101)(cid:0) k(2elog(16mdBT/δ)d/k2)k−1(cid:1). Therefore, the conditional batch size B will
√
exponentially increase as parity k ≤ O( d) goes up, which ensures that the stochastic gradient can
sufficiently approximate the population gradient. Finally, the conditions of η,λ ensure that gradient
descent with weight decay can effectively learn the relevant features while simultaneously denoising
the data. Finally, the threshold condition ρ increases as parity k increases to accommodate the
increase of the population gradient. Based on these conditions, we give our main result on solving
the parity problem in the following theorem.
8Theorem 4.3 Under Condition 4.2, we run online SGD for iteration T =
Θ(cid:0) kη−1λ−1logd(cid:1)
itera-
tions. Then with probability at least 1−δ we can find W(T) such that
P(cid:0) yf(W(T),x) ≥ γm(cid:1) ≥ 1−ϵ,
where γ = 0.25k! is a constant.
Theorem 4.3 establishes that, under certain conditions, a neural network is capable of learning to
solve the k-parity problem within Θ(kη−1λ−1logd) iterations, achieving a population error of at
most ϵ. According to Condition 4.2, the total number of samples utilized amounts to BT = O(cid:101)(dk−1)
given a polynomial logarithmic width requirement of m = O(1) with respect to d.
Remark 4.4 While Theorem 4.3 works for fixed second-layer training, we demonstrate that compa-
rable results can be obtained when the second layer of the network is simultaneously trained with a
lower learning rate. Detailed results and further elaboration of this aspect are provided in Appendix D.
Our findings present a sample complexity of O(cid:101)(dk−1), aligning with Conjecture 2 posited by Abbe
et al. (2023a), which suggests a sample complexity lower bound of Ω(cid:101)(dk−1). Besides, our results
for the uniform Boolean distribution match the complexity achieved by Abbe et al. (2023a) under
the isotropic Gaussian scenario. Despite these similarities in outcomes, our methodology diverges
significantly: we employ online sign SGD utilizing a large batch size of O(cid:101)(dk−1) and conduct training
over merely O(cid:101)(1) iterations. In contrast, Abbe et al. (2023a) implement projected online SGD with a
minimal batch size of 1, extending training over O(cid:101)(dk−1) iterations. Abbe et al. (2023a) also requires
a two-phase training process for the first and second layer weights, requiring them to be trained
separately.
5 Overview of Proof Technique
In this section, we discuss the main ideas used in the proof. Based on these main ideas, the proof of
our main Theorem 4.3 will follow naturally. The complete proofs of all the results are given in the
appendix. Section 5.1 serves as a warmup by examining population sign gradient descent. Here,
three pivotal ideas crucial to the proof of stochastic sign gradient descent are introduced:
1. The impact of the initialization’s positivity or negativity on the trajectory of neuron weights.
2. The divergence between feature coordinates and noise coordinates of different neurons.
3. How a trained neural network can effectively approximate the good neural network (4.1).
Moving on to Section 5.2, we delve into the analysis of sign SGD. Contrasting with population GD,
the addition in SGD analysis involves accounting for the approximation error between the population
gradient and the stochastic gradient. This consideration leads to the stipulation of the batch size B
outlined in Condition 4.2.
5.1 Warmup: Population Gradient Descent
For population gradient, we perform the following updates:
w r(t+1) = (1−ηλ)·w r(t)−η·s(cid:103)ign(cid:0) ∇ wrL D(W(t))(cid:1) ,
9where L (W) = E [ℓ(y,f(W,x))] = 1−E [yf(W,x)]. Then, the following coordinate-
D (x,y)∼D (x,y)∼D
wise population gradient update rules hold:
(cid:32) (t) (t) (t)(cid:33)
w w ···w
w r( ,t j+1) = (1−ηλ)w r( ,t j) +η·s(cid:103)ign k!a
r
r,1 r,2
(t)
r,k , j ∈ [k], (5.1)
w
r,j
w(t+1) = (1−ηλ)w(t) , j ∈/ [k]. (5.2)
r,j r,j
In the preceding discussion of the update rule, we have identified that the noise coordinates (j ∈/ [k])
exhibit exponential decay, characterized by a decay constant of 1 − ηλ. To further dissect the
dynamics of this system, we turn our attention to the behavior of feature coordinates. We categorize
neurons into two distinct types based on their initial alignment: a neuron is classified as a good
neuron if a = (cid:81)k sign(w(0) ), and conversely, as a bad neuron if a = −(cid:81)k sign(w(0) ). This
r j=1 r,j r j=1 r,j
distinction is pivotal, as it divides the neuron population into two distinct classes: good and bad.
Neurons in the good class are integral to the functionality of the final trained neural network, playing
a significant role in its test accuracy. Conversely, neurons classified as bad tend to diminish in
influence over the course of training, ultimately contributing minimally to the network’s overall
performance. For good neurons, the update rules for feature coordinates can be reformulated to
(0) (t) (0) (t) (0) (t)
sign(w w )sign(w w )···sign(w w )
(0) (t+1) (0) (t) r,1 r,1 r,2 r,2 r,k r,k
sign(w )w = (1−ηλ)sign(w )w +η· .
r,j r,j r,j r,j (0) (t)
sign(w w )
r,j r,j
(5.3)
For neurons classified as bad, the update rules for feature coordinates can be rewritten as:
(0) (t) (0) (t) (0) (t)
sign(w w )sign(w w )···sign(w w )
(0) (t+1) (0) (t) r,1 r,1 r,2 r,2 r,k r,k
sign(w )w = (1−ηλ)sign(w )w −η· .
r,j r,j r,j r,j (0) (t)
sign(w w )
r,j r,j
(5.4)
Comparing equations (5.3) and (5.4), it becomes apparent that the feature coordinates of good and
bad neurons exhibit divergent behaviors. Consequently, the feature coordinates of good neurons will
significantly outweigh those of bad neurons in the long term. With the regularization parameter λ
set as 1 in Condition 4.2, we derive the following lemma illustrating the divergent trajectories of
population gradient descent for both neuron types:
Lemma 5.1 Under Condition 4.2, for good neurons r ∈ Ω := {r ∈ [m] : a =
(cid:81)k sign(w(0)
)},
g r j=1 r,j
the feature coordinates will remain the same as initialization throughout the training:
(t) (0)
w = w , ∀j ∈ [k],t ≥ 0.
r,j r,j
For bad neurons r ∈ Ω := {r ∈ [m] : a =
−(cid:81)k sign(w(0)
)}, the feature coordinates will decay
b r j=1 r,j
faster than noise coordiantes:
(0) (t+1) (0) (t)
0 < sign(w )w ≤ (1−ηλ)sign(w )w , ∀j ∈ [k],t ≥ 0.
r,j r,j r,j r,j
According to (5.2) and Lemma 5.1, after training T = Θ(kη−1λ−1log(d)) iterations, bad neurons and
10noise coordinates in good neurons diminish to a magnitude of Θ(1/poly(d)), as shown in Lemma 5.2.
In contrast, the feature coordinates of good neurons remain unchanged.
Lemma 5.2 Under Condition 4.2, for T ≥ (k+1)η−1λ−1log(d), it holds that
|w(T) | ≤ d−(k+1), ∀r ∈ Ω ,j ∈ [d]\[k],
r,j g
|w(T) | ≤ d−(k+1), ∀r ∈ Ω ,j ∈ [d].
r,j b
This leads to the following approximation for the trained neural network:
m
(cid:88) (cid:88)
f(W(T),x) = a σ(⟨w(T),x⟩) ≈ a σ(⟨w(T),x⟩)
r r r r
r=1 r∈Ωg
(cid:18) k (cid:19) (cid:18) k (cid:19)
= (cid:88) (cid:89) sign(w(0) ) σ(⟨w(T),x⟩) ≈ (cid:88) (cid:89) sign(w(0) ) σ(⟨w(T) ,x ⟩).
r,j r r,j r,[1:k] [1:k]
r∈Ωg j=1 r∈Ωg j=1
Under Condition 4.2, the condition on m ensures a balanced distribution of neurons across different
initializations, approximately m/2k+1, given 2k+1 kinds of possible initializations. This results in
the trained neural network f(W(T),x) closely approximating (m/2k+1)·f(W∗,x).
5.2 Stochastic Sign Gradient Descent
Transitioning from the trajectory trained by population gradient descent, this section delves into the
dynamics under sign stochastic gradient descent (Sign SGD). We commence by presenting a lemma
that estimates the approximation error between the population gradient and the stochastic gradient.
Lemma 5.3 Under Condition 4.2, with probability at least 1−δ with respect to the online data
generation, the stochastic gradient approximates the population gradient well:
(cid:12) (cid:12) (cid:12) (cid:12)∂L D ∂w(W(t)) − ∂∂ wL(t)(cid:12) (cid:12) (cid:12)
(cid:12)
≤ ϵ 1·∥w r(t)∥k 2−1, ∀t ∈ [0,T],r ∈ [m],j ∈ [d],
r,j r,j
where L(t) is the loss of randomly sampled online batch S and
t
ϵ = O(d−(k−1)/2/polylog(d)).
1
Building upon the approximation delineated in Lemma 5.3, and considering the established order of
ρ,ϵ and ∥w(t) ∥ , we arrive at an important corollary.
1 r 2
Corollary 5.4 Under Condition 4.2, given the same initialization, with probability at least 1−δ,
the stochastic sign gradient is the same as the population sign gradient:
(cid:18)
∂L
(W(t))(cid:19) (cid:18) ∂L(t)(cid:19)
D
s(cid:103)ign = s(cid:103)ign , ∀t ∈ [0,T],r ∈ [m],j ∈ [d].
∂w ∂w
r,j r,j
This corollary suggests that, under identical initialization, the trajectory of a model trained using
population gradient descent will, with high probability, align with the trajectory of a model trained
using stochastic gradient descent.
116 Experiments
In this section, we present a series of experiments designed to empirically validate the theoretical
results established in our main theorem. The primary objectives of these experiments are to (1)
assess the test accuracy of the trained neural network, thereby corroborating the theorem’s results,
and (2) verify the key lemmas concerning the behavior of good and bad neurons. Specifically, we
aim to demonstrate that for good neurons, the feature coordinates remain largely unchanged from
initialization while the noise coordinates decay exponentially. Conversely, for bad neurons, we expect
both feature and noise coordinates to exhibit exponential decay.
Model. We generated synthetic k-parity data based on Definition 3.1. Each data point (x,y) with
x ∈ Rd and y ∈ {±1} is produced from distribution D , where A specifically is taken as [k]. We
A
utilized two-layer fully-connected neural networks with m number of neurons. The network employs
a polynomial activation function σ(z) = zk. The first layer weights for the r-th neuron, w(0) ∈ Rd,
r
were initialized following a binary scheme, where w(0) ∼ Unif({±1}d). The second layer weights for
r
the r-th neuron, a , is randomly initialized as 1 or −1 with equal probability.
r
Training. Our model was trained to minimize the empirical correlation loss function, incorporating
L regularization with a regularization parameter set at λ = 1. The training process utilized online
2
stochastic sign gradient descent (Sign SGD) with a fixed step size η and a predetermined batch
size B. The principal metric for assessment was test accuracy. Our experiments were conducted
considering parity k ∈ {2,3,4}.
For k = 2, the model configuration included a data dimension of d = 8, a hidden layer width of
m = 12, a total of T = 25 epochs, a learning rate η = 0.1, an online batch size of B = 64, and a
threshold for s(cid:103)ign set at ρ = 0.3. In the case of k = 3, we employed a data dimension of d = 16,
increased the hidden layer width to m = 48, extended the training to T = 50 epochs, adjusted the
learning rate to η = 0.05, used an online batch size of B = 256, and set the threshold for s(cid:103)ign at
ρ = 1. For k = 4, the model was further scaled up with a data dimension of d = 20, a hidden layer
width of m = 128, a training epoch of T = 100, a smaller learning rate of η = 0.02, an online batch
size of B = 2048, and a threshold for s(cid:103)ign established at ρ = 3.
Experimental Results. The evaluation of test accuracy across various configurations is presented
in Table 3. Using neural networks with merely 2Θ(k) neurons, we observed high test accuracy for
k-parity problem with k ∈ {2,3,4}, confirming the results of our main theorem (Theorem 4.3).
These empirical results validate the efficacy of our studied model architecture (3.1) and training
methodology in tackling the k-sparse parity problem.
To further validate our theoretical findings, we examined the change of feature and noise
coordinates for the first neuron w(t) over multiple iterations, focusing specifically on the setting
1
k ∈ {2,3,4}. Figures 2, 3, 4, 5, 6, and 7 visually represent these trajectories. Our empirical findings
reveal a consistent pattern: the feature coordinates (w(t) ,...,w(t)) of the neurons identified as good
1,1 1,k
(with initialization satisfying a = (cid:81)k w(0)) exhibit relative stability, while their noise coordinates
r j=1 r,j
(w(t) ,...) show a decreasing trend over time. In contrast, the trajectories for neurons classified as
1,k+1
bad (with initialization satisfying a = −(cid:81)k w(0)) indicate a general reduction in all coordinate
r j=1 r,j
values.
These empirical observations support our theoretical analyses, as outlined in Lemma 5.1 and
12Lemma 5.2, showcasing the consistency between the theoretical foundations of our model and its
practical performance. The disparities between good and bad neurons, as evidenced by their feature
and noise coordinate behaviors, underscore the nuanced dynamics inherent in the learning process of
k-parity problems.
k 2 3 4
Test Accuracy (%) 99.69%±0.29% 97.75%±1.37% 96.89%±0.44%
Table 3: Test accuracy for solving k-sparse parity problem with k ∈ {2,3,4}, averaged over 10 runs.
Change of the First Neuron in the Hidden Layer Change of the First Neuron in the Hidden Layer
1.00 1.00 Feature 1: w1(t ,) 1
0.75 0.75 Feature 2: w1(t ,) 2
0.50 0.50 Noise 1: w1(t ,) 3
0.25 Feature 1: w1(t ,) 1 0.25 Noise 2: w1(t ,) 4
Feature 2: w1(t ,) 2
0.00 Noise 1: w1(t ,) 3 0.00
0.25 Noise 2: w1(t ,) 4 0.25
0.50 0.50
0.75 0.75
1.00 1.00
0 5 10 15 20 25 0 5 10 15 20 25
Epochs Epochs
Figure 2: Illustration of a 2-parity good neuron Figure 3: Illustration of a 2-parity bad neuron
with initial weights w(0) = 1, w(0) = −1, and with initial weights w(0) = −1, w(0) = 1, and
1,1 1,2 1,1 1,2
a = −1. a = 1.
1 1
Change of the First Neuron in the Hidden Layer Change of the First Neuron in the Hidden Layer
1.00 1.00 Feature 1: w1(t ,) 1
0.75 0.75 Feature 2: w1(t ,) 2
0.50 0.50 Feature 3: w1(t ,) 3
Noise 1: w1(t ,) 4
0.25 0.25 Noise 2: w1(t ,) 5 0.00 Feature 1: w1(t ,) 1 0.00 Noise 3: w1(t ,) 6
0.25 Feature 2: w1(t ,) 2 0.25
Feature 3: w1(t ,) 3
0.50 Noise 1: w1(t ,) 4 0.50
0.75 Noise 2: w1(t ,) 5 0.75
1.00 Noise 3: w1(t ,) 6 1.00
0 10 20 30 40 50 0 10 20 30 40 50
Epochs Epochs
Figure 4: Illustration of a 3-parity good neuron Figure 5: Illustration of a 3-parity bad neuron
with initial weights w(0) = 1, w(0) = 1, w(0) = 1, with initial weights w(0) = 1, w(0) = −1, w(0) =
1,1 1,2 1,3 1,1 1,2 1,3
and a = 1. 1, and a = 1.
1 1
7 Conclusion and Future Work
In our study, we have conducted a detailed analysis of the k-parity problem, investigating how
Stochastic Gradient Descent (SGD) can effectively learn intricate features from binary datasets.
13
sthgieW
reyaL
neddiH
sthgieW
reyaL
neddiH
sthgieW
reyaL
neddiH
sthgieW
reyaL
neddiHChange of the First Neuron in the Hidden Layer Change of the First Neuron in the Hidden Layer
1.00 1.00 Feature 1: w1(t ,) 1
0000 00.... ..0257 520505 05 F F F F N N Ne e e e o o oa a a a i i is s st t t t e e eu u u u r r r r 1 2 3e e e e : :
:
1 2 3 4 w w w: : : : 1 1 1( ( (w w w w t t t, , ,) ) )5 6 71 1 1 1( ( ( (t t t t, , , ,) ) ) )1 2 3 4 0000 00.... ..0257 520505 05 F F F N N N Ne e e o o o oa a a i i i is s s st t t e e e eu u u r r r 1 2 3 4e e e : : : : 2 3 4 w w w w: : : 1 1 1 1( ( ( (w w w t t t t, , , ,) ) ) )5 6 7 81 1 1( ( (t t t, , ,) ) )2 3 4
0.75 Noise 4: w1(t ,) 8 0.75
1.00 1.00
0 20 40 60 80 100 0 20 40 60 80 100
Epochs Epochs
Figure 6: Illustration of a 4-parity good neu- Figure 7: Illustration of a 4-parity bad neu-
ron with initial weights w(0) = −1, w(0) = −1, ron with initial weights w(0) = −1, w(0) = −1,
1,1 1,2 1,1 1,2
w(0) = 1, w(0) = 1, and a = 1. w(0) = 1, w(0) = −1, and a = −1.
1,3 1,4 1 1,3 1,4 1
Our findings reveal that SGD, when employed in two-layer fully-connected neural networks solving
k-sparse parity problem, is capable of achieving a sample complexity O(cid:101)(dk−1). Remarkably, this
result matches the theoretical expectations set by the Statistical Query (SQ) model, underscoring
the efficiency and adaptability of SGD. Our empirical evaluations further validate our theoretical
results, establishing their reliability and practical relevance.
Looking ahead, an intriguing direction for future research is to explore the possibility of learning
k-parity using SGD with even smaller queries that surpass the SQ lower bond or studying deeper
neural networks. This potential advancement could pave the way for developing more efficient
algorithms capable of tackling complex problems with weaker data requirements.
A Preliminary Lemmas
During the initialization phase of a neural network, neurons can be categorized into 2k distinct
groups. This classification is based on whether each feature coordinate is positive or negative. We
define these groups as follows:
(0)
Ω = {r ∈ [m]| sign(w ) = b ,∀j ∈ [k]},
b1b2···b k r,j j
where b ,b ,··· ,b ∈ {±1}. To illustrate with specific examples, consider the following special cases:
1 2 k
(0)
Ω = {r ∈ [m]|w > 0,∀j ∈ [k]},
11···1 r,j
(0)
Ω = {r ∈ [m]|w < 0,∀j ∈ [k]}.
−1−1···−1 r,j
In these cases, Ω represents the group of neurons where all initial weights are positive across the
11···1
k features, while Ω consists of neurons with all initial weights being negative. Within each
−1−1···−1
group of neurons, we can further subdivide them into two subgroups based on the value of a . Let’s
r
14
sthgieW reyaL neddiH sthgieW reyaL neddiHdenote
(cid:26) (cid:12) k (cid:27) (cid:26) (cid:12) k (cid:27)
(cid:12) (cid:89) (0) (cid:12) (cid:89) (0)
Ω
g
= r ∈ [m](cid:12) (cid:12)a
r
= sign(w r,j) ,Ω
b
= r ∈ [m](cid:12) (cid:12)a
r
= − sign(w r,j) ,
j=1 j=1
where Ω denotes the good neuron set and Ω denotes the bad neuron set. We will later demonstrate
g b
in the proof and experiments that neurons in Ω and neurons in Ω exhibit distinct behaviors during
g b
the training process. Specifically, for neurons in Ω , the feature coordinates will remain largely
g
unchanged from their initial values throughout training, while the noise coordinates will decrease
to a lower order compared to the feature coordinates. On the other hand, for neurons in Ω , both
b
feature coordinates and noise coordinates will decrease to a lower order compared to their initial
values.
In order to establish the test error result, it is essential to impose a condition on the initialization.
Specifically, the number of neurons for each type of initialization should be approximately equal.
Lemma A.1 With probability at least 1−δ for the randomness in the neural network’s initialization,
the sizes of the sets Ω and Ω are bounded as follows:
g b
|Ω |,|Ω | ∈ [(1−α)m/2,(1+α)m/2].
g b
Besides, the intersections of Ω with both Ω and Ω are also bounded within a specified range:
b1b2···b
k
g b
|Ω ∩Ω |,|Ω ∩Ω | ∈ [(1−α)m/2k+1,(1+α)m/2k+1],
b1b2···b
k
g b1b2···b
k
b
where
(cid:114)
3·2k+1log(2k+2/δ)
α = .
m
Proof Let X = 1(cid:2) w(0) > 0,··· ,w(0) > 0,a = 1(cid:3). Then, by Chernoff bound, we have
r r,1 r,k r
P(cid:32)(cid:12) (cid:12)
(cid:12)
(cid:12)(cid:88)m
X r −
2km +1(cid:12) (cid:12)
(cid:12)
(cid:12)
≥ α·
2km +1(cid:33)
≤
2exp(cid:16)
−
3α ·22m k+1(cid:17)
.
r=1
Then, with probability at least 1−δ, we have
m m
(1−α)· ≤ |Ω ∩Ω | ≤ (1+α)· ,
2k+1 11···1 g 2k+1
where
(cid:114)
3·2k+1log(2/δ)
α = .
m
By applying union bound to all 2k+1 kinds of initialization, with probability at least 1−δ it holds
that for any Ω ∩Ω and Ω ∩Ω (cid:0) (b ,··· ,b ) ∈ {±1}k(cid:1)
b1b2···b
k
g b1b2···b
k
b 1 k
m m
(1−α)· ≤ |Ω ∩Ω | ≤ (1+α)· ,
2k+1 b1b2···b k g 2k+1
m m
(1−α)· ≤ |Ω ∩Ω | ≤ (1+α)· ,
2k+1 b1b2···b k b 2k+1
15where
(cid:114)
3·2k+1log(2k+2/δ)
α = .
m
B Warmup: Population Sign GD
In this section, we train a neural network with gradient descent on the distribution D. Here we
use correlation loss function ℓ(y,y) = 1 − yy. Then, the loss on this attribution is L (W) =
(cid:98) (cid:98) D
E [ℓ(y,f(W,x))], and we perform the following updates:
(x,y)∼D
w r(t+1) = (1−ηλ)w r(t)−η·s(cid:103)ign(cid:0) ∇ wrL D(W(t))(cid:1) .
We assume the network is initialized with a symmetric initialization: for every r ∈ [m], initialize
w(0) ∼ Unif({−1,1}d) and initialize a ∼ Unif({−1,1}).
r r
Lemma B.1 The following coordinate-wise population sign gradient update rules hold:
(cid:32) (t) (t) (t)(cid:33)
w w ···w
(t+1) (t) r,1 r,2 r,k
w
r,j
= (1−ηλ)w
r,j
+η·s(cid:103)ign k!a
r
·
(t)
, j ∈ [k],
w
r,j
(t+1) (t)
w = (1−ηλ)w , j ∈/ [k].
r,j r,j
Proof For population gradient descent, we have
w r(t+1) = (1−ηλ)·w r(t)−η·s(cid:103)ign(cid:0) ∇ wrL D(W)(cid:1)
= (1−ηλ)·w r(t)−η·s(cid:103)ign(cid:0) a
r
·E (x,y)∼D[∇ wrσ(⟨w r(t),x⟩)](cid:1)
= (1−ηλ)·w r(t)−η·s(cid:103)ign(cid:0) ka
r
·E (x,y)∼D[yx(⟨w r(t),x⟩)k−1](cid:1) .
Notice that for j ∈/ [k], we have
(cid:20) (cid:18) (cid:19)(cid:21)
E [yx (⟨w(t),x⟩)k−1] = E x x ···x x (cid:88) w(t) ···w(t) x ···x
(x,y)∼D j r (x,y)∼D 1 2 k j r,j1 r,j k−1 j1 j k−1
j1,···,j
k−1
= (cid:88) w(t) ···w(t) ·E (cid:2) x x ···x x x ···x (cid:3)
r,j1 r,j k−1 (x,y)∼D 1 2 k j j1 j k−1
j1,···,j
k−1
= 0,
where the last equality is because {j ,··· ,j } ⊊ {1,··· ,k,j}. This implies that
1 k−1
(t+1) (t)
w = (1−ηλ)·w .
r,j r,j
16For j ∈ [k], we have
(cid:20) (cid:18) (cid:19)(cid:21)
E [yx (⟨w(t),x⟩)k−1] = E x x ···x x (cid:88) w(t) ···w(t) x ···x
(x,y)∼D j r (x,y)∼D 1 2 k j r,j1 r,j k−1 j1 j k−1
j1,···,j
k−1
= (cid:88) w(t) ···w(t) ·E [x x ···x x x ···x ]
r,j1 r,j k−1 (x,y)∼D 1 2 k j j1 j k−1
j1,···,j
k−1
(t) (t) (t)
w w ···w
r,1 r,2 r,k
= (k−1)!· ,
(t)
w
r,j
wherethelastinequalityisbecauseE [x x ···x x x ···x ] ̸= 0ifandonlyif{j,j ,··· ,j } =
(x,y)∼D 1 2 k j j1 j k−1 1 k−1
{1,2,··· ,k}. It follows that
(cid:32) (t) (t) (t)(cid:33)
w w ···w
(t+1) (t) r,1 r,2 r,k
w
r,j
= (1−ηλ)·w
r,j
+η·s(cid:103)ign k!a
r
·
(t)
.
w
r,j
Given the update rules in Lemma B.1, we observe distinct behaviors for good neurons (Ω ) and
g
bad neurons (Ω ). The following corollary illustrates these differences:
b
Corollary B.2 For any neuron r ∈ Ω ∩Ω , the update rule for feature coordinates (j ∈ [k]) is
b1···b
k
g
given by:
(t) (t) (t) (cid:34) (t) (t) (t) (cid:35)
b w(t+1) = (1−ηλ)b w(t) +η·
sign(b 1w r,1)sign(b 2w r,2)···sign(b kw r,k)
·1
|w r,1w r,2···w r,k|
≥
ρ
.
j r,j j r,j (t) (t) k!
sign(b w ) |w |
j r,j r,j
For any neuron r ∈ Ω ∩Ω , the update rule for feature coordinates (j ∈ [k]) is given by:
b1···b
k
b
(t) (t) (t) (cid:34) (t) (t) (t) (cid:35)
b w(t+1) = (1−ηλ)b w(t) −η·
sign(b 1w r,1)sign(b 2w r,2)···sign(b kw r,k)
·1
|w r,1w r,2···w r,k|
≥
ρ
.
j r,j j r,j (t) (t) k!
sign(b w ) |w |
j r,j r,j
By setting the regularization parameter λ to 1, we observe a noteworthy property of the weight of
feature coordinates in good neurons. This is formalized in the following lemma:
Lemma B.3 Assume λ = 1 and ρ < k!. For a neuron r ∈ Ω ∩Ω , the weight associated with
b1···b
k
g
any feature coordinate remains constant across all time steps t ≥ 0. Specifically, it holds that:
(t)
b w = 1, ∀j ∈ [k], t ≥ 0.
j r,j
Proof We prove this by using induction. The result is obvious at t = 0. Suppose the result holds
17when t =(cid:101)t. Then, according to Corollary B.2, we have
b
w((cid:101)t+1)
= (1−ηλ)b
w((cid:101)t)
+η·
sign(b 1w
r((cid:101) ,t 1)
)sign(b 2w
r((cid:101) ,t 2)
)···sign(b kw
r((cid:101) ,t k)
)
·1(cid:34)
|w
r((cid:101) ,t 1)
w
r((cid:101) ,t 2)
···w
r((cid:101) ,t k)
|
≥
ρ(cid:35)
j r,j j r,j sign(b w((cid:101)t) ) |w(t) | k!
j r,j r,j
= (1−ηλ)+η·1[k! ≥ ρ]
= 1.
The following lemma demonstrates that for bad neurons, the weights of feature coordinates tend to
shrink over time. The dynamics of this shrinking are characterized as follows:
1
Lemma B.4 Assume λ = 1 and η/(1−ηλ) < (ρ/k!)k−1. For a neuron r ∈ Ω
b1b2···b
k
∩Ω b, the
weights of any two feature coordinates j and j′ (where j,j′ ∈ [k]) are equal at any time step, that is,
(t) (t)
b w = b w . Furthermore, the weight of any feature coordinate j ∈ [k] evolves according to the
j r,j j′ r,j′
following inequality for any t ≥ 0:
(t+1) (t)
0 < b w ≤ (1−ηλ)b w .
j r,j j r,j
Proof We prove this by using induction. We prove the following three hypotheses:
b w(t) = b w(t) , ∀j,j′ ∈ [k]. (H )
j r,j j′ r,j′ 1
b w(t) > 0, ∀j ∈ [k]. (H )
j r,j 2
b w(t+1) ≤ (1−ηλ)b w(t) , ∀j ∈ [k]. (H )
j r,j j r,j 3
We will show that H (0) and H (0) are true and that for any t ≥ 0 we have
1 2
• H (t) =⇒H (t),
2 3
• H (t), H (t) =⇒H (t+1),
1 2 1
• H (t), H (t) =⇒H (t+1).
1 2 2
H (0) and H (0) are obviously true since b w(0) = 1 for any j ∈ [k]. Next, we prove that
1 2 j r,j
H (t) =⇒H (t) and H (t), H (t) =⇒H (t+1). According to Corollary B.2, we have
2 3 1 2 1
(t) (t) (t) (cid:34) (t) (t) (t) (cid:35)
b w(t+1) = (1−ηλ)b w(t) −η·
sign(b 1w r,1)sign(b 2w r,2)···sign(b kw r,k)
·1
|w r,1w r,2···w r,k|
≥
ρ
j r,j j r,j (t) (t) k!
sign(b w ) |w |
j r,j r,j
(cid:34) (t) (t) (t) (cid:35)
|w w ···w | ρ
= (1−ηλ)b w(t) −η·1 r,1 r,2 r,k ≥ (B.1)
j r,j (t) k!
|w |
r,j
(t)
≤ (1−ηλ)b w ,
j r,j
18where the second equality is by H (t). This verifies H (t). Besides, given H (t), we have
2 3 1
(t) (t) (t) (t) (t) (t)
|w w ···w | |w w ···w |
r,1 r,2 r,k = r,1 r,2 r,k , ∀j,j′ ∈ [k].
(t) (t)
|w | |w |
r,j r,j′
Plugging this into (B.1), we can get
b w(t+1) = b w(t+1) , ∀j,j′ ∈ [k],
j r,j j′ r,j′
which verifies H (t+1). Finally, we prove that H (t), H (t) =⇒ H (t+1). By (B.1) and H (t), we
1 1 2 2 1
have
(cid:104) ρ(cid:105)
b w(t+1) = (1−ηλ)b w(t) −η·1 |w(t) |k−1 ≥ .
j r,j j r,j r,j k!
If |w(t) | < (ρ/k!)k−1 1, we can get
r,j
(t+1) (t)
b w = (1−ηλ)b w > 0.
j r,j j r,j
If |w(t) | ≥ (ρ/k!)k−1 1, given that
r,j
η 1
< (ρ/k!)k−1,
1−ηλ
we can get
(t+1) (t)
b w = (1−ηλ)b w −η > 0.
j r,j j r,j
Given Lemma B.3 and Lemma B.4, we can directly get the change of neurons of all kinds of
initialization.
1
Corollary B.5 Assume λ = 1, ρ < k! and η/(1−ηλ) < (ρ/k!)k−1. For any fixed (b 1,b 2,··· ,b k) ∈
{±1}k, considering r ∈ Ω , we have the following statements hold.
b1b2···b
k
• For any neuron r ∈ Ω ∩Ω , the weights of feature coordinates remain the same as initializa-
b1b2···b
k
g
(t)
tion: w = b for any t ≥ 0 and j ∈ [k].
r,j j
• For any neuron r ∈ Ω ∩Ω , the weights of feature coordinates will shrink simultaneously
b1b2···b
k
b
over time: b w(t) = b w(t) for any t ≥ 0 and j,j′ ∈ [k] and
j r,j j′ r,j′
(t+1) (t)
0 < b w ≤ (1−ηλ)·b w ,
j r,j j r,j
for any t ≥ 0 and j ∈ [k].
BuildingonCorollaryB.5,wecannowcharacterizethetrajectoryofallneuronsovertime. Specifically,
after a time period T = Θ(kη−1λ−1log(d)), the following observations about neuron weights hold:
19Lemma B.6 Assume λ = 1, ρ < k! and η/(1−ηλ) < (ρ/k!)k−1 1. For T ≥ (k+1)η−1λ−1logd, it
holds that
(T)
w = b , ∀r ∈ Ω ∩Ω ,j ∈ [k],
r,j j b1b1···b k g
|w(T) | ≤ d−(k+1), ∀r ∈ Ω ,j ∈ [d]\[k],
r,j g
|w(T) | ≤ d−(k+1), ∀r ∈ Ω ,j ∈ [d].
r,j b
Proof ThefirstequalityisobviousaccordingtoCorollaryB.5. Weonlyneedtoprovetheinequalities.
According to Lemma B.1, we have for any r ∈ [m] and j ∈ [d]\[k] that
|w(T) | = (1−ηλ)T|w(0) | = (cid:0) (1−ηλ)(ηλ)−1(cid:1)Tηλ ≤ exp(−Tηλ) ≤ d−(k+1),
r,j r,j
where the last inequality is by T ≥ kη−1λ−1log(d). According to Corollary B.5, for any r ∈ Ω and
b
j ∈ [k], we have that
|w(T) | ≤ (1−ηλ)T|w(0) | = (cid:0) (1−ηλ)(ηλ)−1(cid:1)Tηλ ≤ exp(−Tηλ) ≤ d−(k+1).
r,j r,j
Lemma B.7 Under Condition 4.2, with a probability of at least 1−δ with respect to the randomness
in the neural network’s initialization, trained neural network f(W(T),x) approximates accurate
classifier (m/2k+1)·f(W∗,x) well:
(cid:18) f(W(T),x) (cid:19)
P ∈ [0.5,1.5] ≥ 1−ϵ.
x∼Dx (m/2k+1)·f(W∗,x)
Proof First, we can rewrite (4.1) as follows:
(cid:88)
f(W∗,x) = (b ···b )·σ(⟨w∗ ,x⟩),
1 k b1···b
k
(b1,···,b k)∈{±1}k
where w∗ = [b ,b ,··· ,b ,0,··· ,0]⊤. To prove this lemma, we need to estimate the noise part
b1···b
k
1 2 k
of the inner product ⟨w(T) ,x⟩. By Hoeffding’s inequality, we have the following upper bound for the
r
noise part (cid:80)d w(T) x :
j=k+1 r,j j
(cid:32)(cid:12) d (cid:34) d (cid:35)(cid:12) (cid:33) (cid:32)(cid:12) d (cid:12) (cid:33)
P (cid:12) (cid:12) (cid:88) w(T) x −E (cid:88) w(T) x (cid:12) (cid:12) ≥ x = P (cid:12) (cid:12) (cid:88) w(t) x (cid:12) (cid:12) ≥ x
x (cid:12) r,j j r,j j (cid:12) x (cid:12) r,j j(cid:12)
(cid:12) (cid:12) (cid:12) (cid:12)
j=k+1 j=k+1 j=k+1
(cid:18) x2 (cid:19)
≤ 2exp − .
2(cid:80)d [w(t) ]2
j=k+1 r,j
20Then with probability at least 1−ϵ/m we have that for fixed r ∈ [m]
(cid:118)
(cid:12) (cid:12) (cid:12)
(cid:12)
(cid:88)d w r( ,T j) x j(cid:12) (cid:12) (cid:12)
(cid:12)
≤ √ 2log(2m/ϵ)(cid:117) (cid:117) (cid:116) (cid:88)d [w r( ,T j) ]2
(cid:12) (cid:12)
j=k+1 j=k+1
√
(T)
= 2log(2m/ϵ)∥w ∥
r,[k+1:d] 2
√
≤ 2log(2m/ϵ)d−(k+1)(d−k)1/2
≤ d−k,
where the last inequality is by Condition 4.2. By applying union bound to all m neurons, with
probability at least 1−ϵ we have that for any r ∈ [m]
(cid:12) d (cid:12)
(cid:12) (cid:12)
(cid:12)
(cid:88) w r( ,T j) x j(cid:12) (cid:12)
(cid:12)
≤ d−k. (B.2)
j=k+1
By Lemma B.6, we have
(cid:12) m (cid:12)
(cid:12)f(W(T),x)− ·f(W∗,x)(cid:12)
(cid:12) 2k+1 (cid:12)
(cid:12) 2k (cid:12) (cid:12) (cid:12)
(cid:12) (cid:88) m (cid:88) (cid:12) (cid:12) (cid:88) (cid:12)
≤ (cid:12) a σ(⟨w(T),x⟩)− · a∗σ(⟨w∗,x⟩)(cid:12)+(cid:12) a σ(⟨w(T),x⟩)(cid:12)
(cid:12) r r 2k+1 r r (cid:12) (cid:12) r r (cid:12)
(cid:12) (cid:12) (cid:12) (cid:12)
r∈Ωg r=1 r∈Ω
b
(cid:12) (cid:12) (cid:12) (cid:12)
(cid:88) (cid:12) (cid:88) m (cid:12) (cid:12) (cid:88) (cid:12)
≤ (cid:12) σ(⟨w(T),x⟩)− ·σ(⟨w∗ ,x⟩)(cid:12)+(cid:12) a σ(⟨w(T),x⟩)(cid:12)
(cid:12) r 2k+1 b1···b k (cid:12) (cid:12) r r (cid:12)
(cid:12) (cid:12) (cid:12) (cid:12)
(b1,···,b k)∈{±1}k Ω b1···bk∩Ωg r∈Ω b
≤ (cid:88) (cid:32) (cid:16) 2α km +1(cid:17)(cid:12) (cid:12) (cid:12) (cid:12)(cid:88)k b jx j(cid:12) (cid:12) (cid:12) (cid:12)k + (cid:88) (cid:12) (cid:12)σ(⟨w r(T),x⟩)−σ(⟨w b∗
1···b
k,x⟩)(cid:12) (cid:12)(cid:33)
(b1,···,b k)∈{±1}k j=1 Ω b1···bk∩Ωg
(cid:12) (cid:12)
(cid:12) (cid:88) (cid:12)
+(cid:12) a σ(⟨w(T),x⟩)(cid:12)
(cid:12) r r (cid:12)
(cid:12) (cid:12)
r∈Ω
b
≤
(cid:88) (cid:32) (cid:16) 2α km +1(cid:17)(cid:12) (cid:12)
(cid:12)
(cid:12)(cid:88)k
b jx
j(cid:12) (cid:12)
(cid:12)
(cid:12)k
+
(cid:88) kd−k(cid:0) k+d−k(cid:1)k−1(cid:33)
+|Ω
b|(2d−k(cid:1)k
(b1,···,b k)∈{±1}k j=1 Ω b1···bk∩Ωg
≤ (cid:16) αm (cid:17) 2kk(1+e−2)k +|Ω |kd−k(cid:0) k+d−k(cid:1)k−1 +|Ω |(cid:0) 2d−k(cid:1)k
2k+1 g b
(cid:16) (cid:17)
≤ αkk2−k(1+e−2)k +0.5(1+α)kd−k(cid:0) k+d−k(cid:1)k−1 +0.5(1+α)(cid:0) 2d−k(cid:1)k ·m,
where the first three inequalities are by triangle inequality and Lemma A.1; the fourth inequality is
due to
(cid:12) (cid:12)σ(⟨w(T),x⟩)−σ(⟨w∗ ,x⟩)(cid:12)
(cid:12)
r b1···b
k
≤ (cid:12) (cid:12)(⟨w(T),x⟩)k −(⟨w∗ ,x⟩)k(cid:12) (cid:12)
r b1···b
k
≤ |⟨w(T),x⟩−⟨w∗ ,x⟩|·k(max{|⟨w(T),x⟩|,|⟨w∗ ,x⟩|})k−1
r b1···b
k
r b1···b
k
21(cid:12) d (cid:12) (cid:32) (cid:40)(cid:12) k d (cid:12) (cid:12) k (cid:12)(cid:41)(cid:33)k−1
= (cid:12) (cid:12) (cid:88) w(T) x (cid:12) (cid:12)·k max (cid:12) (cid:12)(cid:88) b x + (cid:88) w(T) x (cid:12) (cid:12),(cid:12) (cid:12)(cid:88) b x (cid:12) (cid:12)
(cid:12) r,j j(cid:12) (cid:12) j j r,j j(cid:12) (cid:12) j j(cid:12)
(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)
j=k+1 j=1 j=k+1 j=1
(cid:12)
d
(cid:12) (cid:32)(cid:12)
k
(cid:12) (cid:12)
d
(cid:12)(cid:33)k−1
≤ k(cid:12) (cid:12) (cid:88) w(T) x (cid:12) (cid:12)· (cid:12) (cid:12)(cid:88) b x (cid:12) (cid:12)+(cid:12) (cid:12) (cid:88) w(T) x (cid:12) (cid:12)
(cid:12) r,j j(cid:12) (cid:12) j j(cid:12) (cid:12) r,j j(cid:12)
(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)
j=k+1 j=1 j=k+1
≤
kd−k(cid:0) k+d−k(cid:1)k−1
by (B.2), mean value theorem and Lemma B.6 and for r ∈ Ω
b
|σ(⟨w(T),x⟩)| ≤ |⟨w(T),x⟩|k ≤ (cid:0) kd−(k+1)+d−k(cid:1)k ≤ (cid:0) 2d−k(cid:1)k . (B.3)
r r
Since (m/2k+1)·|f(W∗,x)| = 0.5k!·m, then as long as
√
2πk (cid:16)e+e−1(cid:17)−k
α ≤ · , and α ≤ 1,
8 2
we have
|f(W(T),x)−(m/2k+1)·f(W∗,x)|
(m/2k+1)·|f(W∗,x)|
αkk2−k(1+e−2)k +0.5(1+α)kd−k(cid:0) k+d−k(cid:1)k−1 +0.5(1+α)(cid:0) 2d−k(cid:1)k
≤
0.5k!
2αkk2−k(1+e−2)k +(1+α)kd−k(cid:0) k+d−k(cid:1)k−1 +(1+α)(cid:0) 2d−k(cid:1)k
≤ √
2πk(k/e)k
(cid:114) 2 (cid:16)e+e−1(cid:17)k (cid:114) 2 (cid:18) d−k(cid:19)k−1 (cid:16)e(cid:17)k (cid:114) 2 (cid:16) 2e (cid:17)k
= α + · 1+ · + ·
πk 2 πk k d πk kdk
≤ 0.5,
where the second inequality is by Stirling’s approximation. Then it follows that
f(W(T),x)
∈ [0.5,1.5].
(m/2k+1)·f(W∗,x)
C Stochastic Sign GD
In this section, we consider stochastic sign gradient descent for learning k-parity function. The
primary aim of this section is to demonstrate that the trajectory produced by SGD closely resembles
that of population GD. To begin, let’s recall the update rule of SGD:
(cid:18) (cid:19)
1 (cid:88)
w r(t+1) = (1−λη)w r(t)+η·s(cid:103)ign
|S |
σ′(⟨w r(t),x⟩)·a ryx ,
t
(x,y)∈St
22where |S | = B. Our initial step involves estimating the approximation error between the stochastic
t
gradient and the population gradient, detailed within the lemma that follows.
Lemma C.1 With probability at least 1−δ with respect to the randomness of online data selection,
for all t ≤ T, the following bound holds true for each neuron r ∈ [m] and for each coordinate j ∈ [d]:
(cid:12) (cid:12)
(cid:12) (cid:12) 1 (cid:88) σ′(⟨w(t),x⟩)·a yx−E (cid:2) σ′(⟨w(t),x⟩)·a yx (cid:3)(cid:12) (cid:12) ≤ ϵ ·∥w(t)∥k−1, (C.1)
(cid:12)|S | r r (x,y) r r j (cid:12) 1 r 2
(cid:12) t (cid:12)
(x,y)∈St
where ϵ is defined as
1
2k/2k(log(16mdBT/δ))(k−1)/2log(8mdT/δ) kd(k−3)/2δ
ϵ = √ + .
1
B 8mBT
Proof To prove (C.1), let us introduce the following notations:
g (x,y) = σ′(⟨w(t),x⟩)·a yx = k(⟨w(t),x⟩)k−1·a yx ,
r,j r r j r r j
h (x,y) = g (x,y)·1(cid:2) |⟨w(t),x⟩| ≤ γ(cid:3) ,
r,j r,j r
where g (x,y) represents the gradient at the point (x,y), and h (x,y) denotes the truncated
r,j r,j
version of g (x,y), which is employed for the convenience of applying the Hoeffding’s inequality.
r,j
Firstly, utilizing Hoeffding’s inequality, we can assert the following:
(cid:32)(cid:12) (cid:12) (cid:33) (cid:32) (cid:33)
(cid:12)1 (cid:88) (cid:12) Bx2
P (cid:12) h (x,y)−E h (x,y)(cid:12) ≥ x ≤ 2exp − . (C.2)
(cid:12)B r,j (x,y)∼D r,j (cid:12) 2(kγk−1)2
(cid:12) (cid:12)
(x,y)∈St
Furthermore, wecanestablishanupperboundforthedifferencebetweentheexpectationsofh (x,y)
r,j
and g (x,y):
r,j
(cid:12) (cid:12) (cid:12) (cid:12)
(cid:12)E h (x,y)−E g (x,y)(cid:12) = (cid:12)E g (x,y)·1(cid:2) |⟨w(t),x⟩| > γ(cid:3)(cid:12)
(cid:12) (x,y)∼D r,j (x,y)∼D r,j (cid:12) (cid:12) (x,y)∼D r,j r (cid:12)
≤ kd(k−1)/2∥w r(t)∥ 2k−1·P(cid:0) |⟨w r(t),x⟩| > γ(cid:1) (C.3)
(cid:18) γ2 (cid:19)
≤ kd(k−1)/2∥w(t)∥k−1·2exp − ,
r 2 2∥w(t) ∥2
r 2
where the first inequality is by Cauchy inequality and the second inequality is by Hoeffding’s
23inequality. Additionally, with high probability, the gradient and the truncated gradient are identical:
(cid:32)(cid:12) (cid:12) (cid:33)
(cid:12)1 (cid:88) 1 (cid:88) (cid:12)
P (cid:12) h (x,y)− g (x,y)(cid:12) = 0
(cid:12)B r,j B r,j (cid:12)
(cid:12) (cid:12)
(x,y)∈St (x,y)∈St
(cid:32)(cid:12) (cid:12) (cid:33)
= P (cid:12) (cid:12)1 (cid:88) g (x,y)1(cid:2) |⟨w(t),x⟩| > γ(cid:3)(cid:12) (cid:12) = 0
(cid:12)B r,j r (cid:12)
(cid:12) (cid:12)
(x,y)∈St
(cid:16) (cid:17) (C.4)
≥ P |⟨w(t),x⟩| ≤ γ,∀(x,y) ∈ S
r t
(cid:32)
(cid:18) γ2
(cid:19)(cid:33)B
≥ 1−2exp −
2∥w(t) ∥2
r 2
(cid:18) γ2 (cid:19)
≥ 1−2Bexp − ,
2∥w(t) ∥2
r 2
where the second inequality applies Hoeffding’s inequality. Combining inequalities (C.2), (C.3), and
(C.4), we can assert with probability at least
(cid:18) Bx2 (cid:19) (cid:18) γ2 (cid:19)
1−2exp − −2Bexp − (C.5)
2(kγk−1)2 2∥w(t) ∥2
r 2
that the following inequality holds:
(cid:12) (cid:12)
(cid:12)1 (cid:88) (cid:12) (cid:18) γ2 (cid:19)
(cid:12) g (x,y)−E g (x,y)(cid:12) ≤ x+kd(k−1)/2∥w(t)∥k−1·2exp − . (C.6)
(cid:12) (cid:12)B
(x,y)∈St
r,j (x,y)∼D r,j (cid:12) (cid:12) r 2 2∥w r(t) ∥2
2
√ √
By setting γ = (cid:112) 2log(16B/δ)∥w(t) ∥ and x = 2kγk−1log(8/δ)/ B, we establish that with
r 2
probability at least 1−δ, the following bound is true:
(cid:12) (cid:12)
(cid:12)1 (cid:88) (cid:12)
(cid:12) g (x,y)−E g (x,y)(cid:12)
(cid:12)B r,j (x,y)∼D r,j (cid:12)
(cid:12) (cid:12)
(x,y)∈St
√
2kγk−1log(8/δ) kd(k−1)/2∥w(t) ∥k−1δ
≤ √ + r 2
B 8B
2k/2k(log(16B/δ))(k−1)/2log(8/δ)∥w(t) ∥k−1 kd(k−1)/2∥w(t) ∥k−1δ
= √ r 2 + r 2 .
B 8B
Applying a union bound over all indices r ∈ [m],j ∈ [d], and iterations t ∈ [0,T −1], we conclude
with probability at least 1−δ that
(cid:12) (cid:12)
(cid:12)1 (cid:88) (cid:12)
(cid:12) g (x,y)−E g (x,y)(cid:12)
(cid:12)B r,j (x,y)∼D r,j (cid:12)
(cid:12) (cid:12)
(x,y)∈St
2k/2k(log(16mdBT/δ))(k−1)/2log(8mdT/δ)∥w(t) ∥k−1 kd(k−1)/2∥w(t) ∥k−1δ
= √ r 2 + r 2
B 8mdBT
24(cid:32) (cid:33)
2k/2k(log(16mdBT/δ))(k−1)/2log(8mdT/δ) kd(k−3)/2δ
= √ + ·∥w(t)∥k−1.
B 8mBT r 2
Based on Lemma C.1, we can get the following lemma showing that with high probability, the
stochastic sign gradient follows the same update rule as the population sign gradient.
Lemma C.2 Under Condition 4.2, with probability at least 1−δ with respect to the randomness of
online data selection, the following sign SGD update rule holds:
(cid:32) (t) (t) (t)(cid:33)
w w ···w
(t+1) (t) r,1 r,2 r,k
w
r,j
= (1−ηλ)w
r,j
+η·s(cid:103)ign k!a
r
·
(t)
, j ∈ [k],
w
r,j
(t+1) (t)
w = (1−ηλ)w , j ∈/ [k].
r,j r,j
Proof We prove this by using induction. We prove the following hypotheses:
∥w(t+1)∥ ≤ ∥w(t)∥ , ∀r ∈ [m]. (H )
r 2 r 2 1
(cid:32) (t) (t) (t)(cid:33)
w w ···w
w r( ,t j+1) = (1−ηλ)w r( ,t j) +η·s(cid:103)ign k!a
r
· r,1 r,2
(t)
r,k , ∀r ∈ [m],j ∈ [k]. (H 2)
w
r,j
w(t+1) = (1−ηλ)w(t) , ∀r ∈ [m],j ∈/ [k]. (H )
r,j r,j 3
b w(t) = 1, ∀r ∈ Ω ∩Ω ,∀j ∈ [k]. (H )
j r,j b1···b k g 4
b w(t) = b w(t) , ∀r ∈ Ω ∩Ω ,∀j,j′ ∈ [k]. (H )
j r,j j′ r,j′ b1b2···b k g 5
0 < b w(t+1) ≤ (1−ηλ)b w(t) , ∀r ∈ Ω ∩Ω ,∀j ∈ [k]. (H )
j r,j j r,j b1b2···b k b 6
We will show that H (0), H (0), H (0) and H (0) are true and for any t ≥ 0 we have
2 3 4 5
• H (t), H (t) =⇒ H (t+1). (This can be established by adapting the proof of Lemma B.3; hence,
2 4 4
we omit the proof details here.)
• H (t), H (t) =⇒ H (t+1), H (t). (This can be shown by following the proof of Lemma B.4, so
2 5 5 6
the proof details are omitted here.)
• H (t), H (t), H (t+1), H (t) =⇒H (t).
3 4 4 6 1
• {H (s)}t =⇒ H (t+1), H (t+1).
1 s=0 2 3
H (0) and H (0) are obviously true since w(0) = b for any r ∈ Ω and j ∈ [k]. To prove that
4 5 r,j j b1b2···b k
H (0) and H (0) are true, we only need to verify that
2 3
(cid:32) (cid:33) (cid:32) (0) (0) (0)(cid:33)
s(cid:103)ign |S1
|
(cid:88) σ′(⟨w r(0),x⟩)·a ryx
j
= s(cid:103)ign k!a
r
· w r,1w r,2 (0· )··w r,k , ∀j ∈ [k], (C.7)
0 w
(x,y)∈S0 r,j
25(cid:32) (cid:33)
1 (cid:88)
s(cid:103)ign
|S |
σ′(⟨w r(0),x⟩)·a ryx
j
= 0, ∀j ∈/ [k]. (C.8)
0
(x,y)∈S0
By Lemma C.1, we have for j ∈/ [k]
(cid:12) (cid:12)
(cid:12) (cid:12) (cid:12)|S1
|
(cid:88) σ′(⟨w r(0),x⟩)·a ryx j(cid:12) (cid:12)
(cid:12)
≤ ϵ 1·∥w r(0)∥ 2k−1 = ϵ 1·dk− 21 < ρ,
(cid:12) 0 (cid:12)
(x,y)∈St
leading to (C.8). For j ∈ [k], we have
(cid:12) (0) (0) (0)(cid:12)
(cid:12) (cid:12) (cid:12)|S1
|
(cid:88) σ′(⟨w r(0),x⟩)·a ryx
j
−k!a
r
· w r,1w r,2 (0· )··w r,k(cid:12) (cid:12)
(cid:12)
≤ ϵ 1·∥w r(0)∥ 2k−1 = ϵ 1·dk− 21 ,
(cid:12) 0 w (cid:12)
(x,y)∈S0 r,j
Since k!−ϵ 1·dk− 21 ≥ ρ, we can get
(cid:32) (cid:33) (cid:32) (0) (0) (0)(cid:33)
s(cid:103)ign |S1
|
(cid:88) σ′(⟨w r(0),x⟩)·a ryx
j
= s(cid:103)ign k!a
r
· w r,1w r,2 (0· )··w r,k ,
0 w
(x,y)∈S0 r,j
which verifies (C.7). Next, we verify that H (t), H (t), H (t+1), H (t) =⇒H (t). For r ∈ Ω , given
3 4 4 6 1 g
H (t), H (t) and H (t+1), we can get
3 4 4
∥w(t+1)∥ =
(cid:32) (cid:88)d
(cid:0)
w(t+1)(cid:1)2(cid:33)1 2
=
(cid:32) (cid:88)k
(cid:0) w(t)(cid:1)2 +
(cid:88)d
(cid:0)
(1−ηλ)w(t)(cid:1)2(cid:33) 21
≤ ∥w(t)∥ .
r 2 r,j r,j r,j r 2
j=1 j=1 j=k+1
For r ∈ Ω , given H (t) and H (t), we can get
b 3 6
∥w(t+1)∥ =
(cid:32) (cid:88)d
(cid:0)
w(t+1)(cid:1)2(cid:33) 21
≤
(cid:32) (cid:88)d
(cid:0)
(1−ηλ)w(t)(cid:1)2(cid:33)1 2
≤ ∥w(t)∥ .
r 2 r,j r,j r 2
j=1 j=1
Finally, we verify that {H (s)}t =⇒ H (t+1), H (t+1). Notice that ∥w(t+1) ∥ ≤ ∥w(0) ∥ given
1 s=0 2 3 r 2 r 2
{H (s)}t , we can prove H (t+1) and H (t+1) by following the prove of (C.7) and (C.8) given
1 s=0 3 2
Lemma C.1.
Based on Lemma C.2 and the proof of Lemma B.6, Lemma B.7, we can get the following lemmas
and theorems aligning with the result of population sign GD.
Lemma C.3 Under Condition 4.2, for T = Θ(kη−1λ−1log(d)),with a probability of at least 1−δ
with respect to the randomness of the online data selection, it holds that
(T)
w = b , ∀r ∈ Ω ∩Ω ,j ∈ [k],
r,j j b1b1···b k g
|w(T) | ≤ d−(k+1), ∀r ∈ Ω ,j ∈ [d]\[k],
r,j g
|w(T) | ≤ d−(k+1), ∀r ∈ Ω ,j ∈ [d].
r,j b
26Lemma C.4 Under Condition 4.2, with a probability of at least 1−2δ with respect to the randomness
in the neural network’s initialization and the online data selection, trained neural network f(W(T),x)
approximates accurate classifier (m/2k+1)·f(W∗,x) well:
(cid:18) f(W(T),x) (cid:19)
P ∈ [0.5,1.5] ≥ 1−ϵ.
x∼Dx (m/2k+1)·f(W∗,x)
Based on Lemma C.4, we are now ready to prove our main theorem.
Theorem C.5 Under Condition 4.2, we run mini-batch SGD for T = Θ(kη−1λ−1log(d)) iterations.
Then with probability at least 1−2δ with respect to the randomness of neural network initialization
and the online data selection, it holds that
P (yf(W(T),x) ≥ γm) ≥ 1−ϵ.
(x,y)∼D
where γ = 0.25k! is a constant.
Proof Given Lemma C.4, we have
(cid:18) f(W(T),x) (cid:19)
P ≥ 0.5
x∼Dx (m/2k+1)·f(W∗,x)
(cid:18) f(W(T),x) (cid:19)
≥ P ∈ [0.5,1.5]
x∼Dx (m/2k+1)·f(W∗,x)
≥ 1−ϵ.
According to Proposition 4.1, we can get
(cid:18) f(W(T),x) (cid:19)
P ≥ 0.5
x∼Dx (m/2k+1)·f(W∗,x)
(cid:18) yf(W(T),x) (cid:19)
= P ≥ 0.5
x∼Dx (m/2k+1)·k!·2k
= P (cid:0) yf(W(T),x) ≥ 0.25k!·m(cid:1) ,
x∼Dx
which completes the proof.
D Trainable Second Layer
In this section, we consider sign SGD for training the first and second layers together. In this
scenario, we have the following sign SGD update rule:
(cid:18) (cid:19)
1 (cid:88)
w(t+1) = (1−λη)w(t)+η·s(cid:103)ign σ′(⟨w(t),x⟩)·a(t)yx , (D.1)
r r |S | r r
t
(x,y)∈St
(cid:18) (cid:19)
1 (cid:88)
a( rt+1) = a( rt)+η 2·s(cid:103)ign
|S |
σ(⟨w r(t),x⟩) , (D.2)
t
(x,y)∈St
27where |S | = B. For training the neural network over T = Θ(k(ηλ)−1log(d)) iterations, we adopt a
t
small learning rate for the second layer, adhering to the condition:
(cid:114)
1 πk(cid:16)e+e−1(cid:17)−k
η ≤ .
2
4T 8 2
The network is initialized symmetrically: for every r ∈ [m], initialize w(0) ∼ Unif({−1,1}d) and
r
initialize a(0) ∼ Unif({−1,1}). Under this setting, denote
r
(cid:26) (cid:12) k (cid:27) (cid:26) (cid:12) k (cid:27)
(cid:12) (cid:89) (0) (cid:12) (cid:89) (0)
Ω
g
= r ∈ [m](cid:12) (cid:12)a
r
= sign(w r,j) ,Ω
b
= r ∈ [m](cid:12) (cid:12)a
r
= − sign(w r,j) .
j=1 j=1
Similar to the fix-second-layer case, our initial step involves estimating the approximation error
between the SGD gradient and the population gradient, detailed within the lemma that follows.
Lemma D.1 With probability at least 1−δ with respect to the randomness of online data selection,
for all t ≤ T, we have for any r ∈ [m] and j ∈ [d] that
(cid:12) (cid:12)
(cid:12) (cid:12) 1 (cid:88) σ′(⟨w(t),x⟩)·a(t)yx −E(cid:2) σ′(⟨w(t),x⟩)·a(t)yx (cid:3)(cid:12) (cid:12) ≤ ϵ ·|a(t)|∥w(t)∥k−1, (D.3)
(cid:12)|S | r r j r r j (cid:12) 1 r r 2
(cid:12) t (cid:12)
(x,y)∈St
where
2k/2k(log(16mdBT/δ))(k−1)/2log(8mdT/δ) kd(k−3)/2δ
ϵ = √ + .
1
B 8mBT
Proof To prove (D.3), we denote
g (x,y) = σ′(⟨w(t),x⟩)·a(t)yx = k(⟨w(t),x⟩)k−1·a(t)yx ,
r,j r r j r r j
h (x,y) = g (x,y)·1(cid:2) |⟨w(t),x⟩| ≤ γ(cid:3) .
r,j r,j r
Initially, by invoking Hoeffding’s inequality, we have the following probability bound:
(cid:32)(cid:12) (cid:12) (cid:33) (cid:32) (cid:33)
(cid:12)1 (cid:88) (cid:12) Bx2
P (cid:12) h (x,y)−E h (x,y)(cid:12) ≥ x ≤ 2exp − . (D.4)
(cid:12) (cid:12)B
(x,y)∈St
r,j (x,y)∼D r,j (cid:12) (cid:12) 2(kγk−1a( rt) )2
Next, we establish an upper bound for the difference between the expected values of h (x,y) and
r,j
g (x,y):
r,j
(cid:12) (cid:12) (cid:12) (cid:12)
(cid:12)E h (x,y)−E g (x,y)(cid:12) = (cid:12)E g (x,y)·1(cid:2) |⟨w(t),x⟩| > γ(cid:3)(cid:12)
(cid:12) (x,y)∼D r,j (x,y)∼D r,j (cid:12) (cid:12) (x,y)∼D r,j r (cid:12)
≤ kd(k−1)/2∥w r(t)∥ 2k−1|a( rt)|·P(cid:0) |⟨w r(t),x⟩| > γ(cid:1) (D.5)
(cid:18) γ2 (cid:19)
≤ kd(k−1)/2∥w(t)∥k−1|a(t)|·2exp − ,
r 2 r 2∥w(t) ∥2
r 2
28wherethefirstinequalityfollowsfromtheCauchy-SchwarzinequalityandthesecondfromHoeffding’s
inequality. With high probability, the gradient and the truncated gradient coincide:
(cid:32)(cid:12) (cid:12) (cid:33)
(cid:12)1 (cid:88) 1 (cid:88) (cid:12)
P (cid:12) h (x,y)− g (x,y)(cid:12) = 0
(cid:12)B r,j B r,j (cid:12)
(cid:12) (cid:12)
(x,y)∈St (x,y)∈St
(cid:32)(cid:12) (cid:12) (cid:33)
= P (cid:12) (cid:12)1 (cid:88) g (x,y)1(cid:2) |⟨w(t),x⟩| > γ(cid:3)(cid:12) (cid:12) = 0
(cid:12)B r,j r (cid:12)
(cid:12) (cid:12)
(x,y)∈St
(cid:16) (cid:17) (D.6)
≥ P |⟨w(t),x⟩| ≤ γ,∀(x,y) ∈ S
r t
(cid:32)
(cid:18) γ2
(cid:19)(cid:33)B
≥ 1−2exp −
2∥w(t) ∥2
r 2
(cid:18) γ2 (cid:19)
≥ 1−2Bexp − .
2∥w(t) ∥2
r 2
Combing (D.4), (D.5) and (D.6), it holds with probability at least
(cid:16) Bx2 (cid:17) (cid:18) γ2 (cid:19)
1−2exp − −2Bexp −
2(kγk−1a(t) )2 2∥w(t) ∥2
r r 2
that
(cid:12) (cid:12)
(cid:12)1 (cid:88) (cid:12) (cid:18) γ2 (cid:19)
(cid:12) g (x,y)−E g (x,y)(cid:12) ≤ x+kd(k−1)/2∥w(t)∥k−1|a(t)|·2exp − .
(cid:12) (cid:12)B
(x,y)∈St
r,j (x,y)∼D r,j (cid:12) (cid:12) r 2 r 2∥w r(t) ∥2
2
√ √
By taking γ = (cid:112) 2log(16B/δ)∥w(t) ∥ and x = 2kγk−1|a(t) |log(8/δ)/ B, then with probability at
r 2 r
least 1−δ it holds that
(cid:12) (cid:12)
(cid:12)1 (cid:88) (cid:12)
(cid:12) g (x,y)−E g (x,y)(cid:12)
(cid:12)B r,j (x,y)∼D r,j (cid:12)
(cid:12) (cid:12)
(x,y)∈St
√
2kγk−1|a(t)
|log(8/δ)
kd(k−1)/2∥w(t) ∥k−1|a(t)
|δ
≤ √r + r 2 r
B 8B
2k/2k(log(16B/δ))(k−1)/2log(8/δ)|a(t) |∥w(t) ∥k−1 kd(k−1)/2|a(t) |∥w(t) ∥k−1δ
= √ r r 2 + r r 2 .
B 8B
Then, by applying a union bound to all r ∈ [m],j ∈ [d] and iterations t ∈ [0,T −1], it holds with
probability at least 1−δ that
(cid:12) (cid:12)
(cid:12)1 (cid:88) (cid:12)
(cid:12) g (x,y)−E g (x,y)(cid:12)
(cid:12)B r,j (x,y)∼D r,j (cid:12)
(cid:12) (cid:12)
(x,y)∈St
2k/2k(log(16mdBT/δ))(k−1)/2log(8mdT/δ)∥w(t) ∥k−1|a(t)
|
kd(k−1)/2∥w(t) ∥k−1|a(t)
|δ
= √ r 2 r + r 2 r
B 8mdBT
29(cid:32) (cid:33)
2k/2k(log(16mdBT/δ))(k−1)/2log(8mdT/δ) kd(k−3)/2δ
= √ + ·|a(t)|∥w(t)∥k−1.
B 8mBT r r 2
Lemma D.2 (Stability of Second Layer Weights) For t ≤ T = Θ(k(ηλ)−1log(d)), the magni-
tude of change in the second layer weights from their initial values is bounded as follows:
|a(t)−a(0)| ≤ c,
r r
where
(cid:114)
1 πk(cid:16)e+e−1(cid:17)−k
c = .
4 8 2
Consequently, the sign of each weight remains consistent over time:
sign(a(t)) = sign(a(0)).
r r
Proof Notice that by (D.2) and s(cid:103)ign(·) ∈ {−1,0,1}, we can get for any t ≥ 0 that
a(t)−η ≤ a(t+1) ≤ a(t)+η ,
r 2 r r 2
which implies that
|a(t)−a(0)| ≤ η t ≤ η T ≤ c,
r r 2 2
where the second inequality is by t ≤ T, and the last inequality is by the condition η ≤
2
(cid:113) (cid:16) (cid:17)−k
1 πk e+e−1 .
4T 8 2
Lemma D.3 With probability at least 1−δ with respect to the randomness of online data selection,
the following sign SGD update rule holds:
(cid:32) (t) (t) (t)(cid:33)
w w ···w
w(t+1) = (1−ηλ)w(t) +η·s(cid:103)ign k!a(t)· r,1 r,2 r,k , j ∈ [k],
r,j r,j r (t)
w
r,j
(t+1) (t)
w = (1−ηλ)w , j ∈/ [k].
r,j r,j
Proof We prove this by using induction. We prove the following hypotheses:
∥w(t+1)∥ ≤ ∥w(t)∥ , ∀r ∈ [m]. (H )
r 2 r 2 1
(cid:32) (t) (t) (t)(cid:33)
w w ···w
w r( ,t j+1) = (1−ηλ)w r( ,t j) +η 1·s(cid:103)ign k!a r(t)· r,1 r,2
(t)
r,k , ∀r ∈ [m],j ∈ [k]. (H 2)
w
r,j
30w(t+1) = (1−ηλ)w(t) , ∀r ∈ [m],j ∈/ [k]. (H )
r,j r,j 3
b w(t) = 1, ∀r ∈ Ω ∩Ω ,∀j ∈ [k]. (H )
j r,j b1···b k g 4
b w(t) = b w(t) , ∀r ∈ Ω ∩Ω ,∀j,j′ ∈ [k]. (H )
j r,j j′ r,j′ b1b2···b k g 5
0 < b w(t+1) ≤ (1−ηλ)b w(t) , ∀r ∈ Ω ∩Ω ,∀j ∈ [k]. (H )
j r,j j r,j b1b2···b k b 6
We will show that H (0), H (0), H (0) and H (0) are true and for any t ≥ 0 we have
2 3 4 5
• H (t), H (t) =⇒ H (t+1).
2 4 4
• H (t), H (t) =⇒ H (t+1), H (t).
2 5 5 6
• H (t), H (t), H (t+1), H (t) =⇒H (t).
3 4 4 6 1
• {H (s)}t =⇒ H (t+1), H (t+1).
1 s=0 2 3
H (0) and H (0) are obviously true since w(0) = b for any r ∈ Ω and j ∈ [k]. To prove that
4 5 r,j j b1b2···b k
H (0) and H (0) are true, we can follow the proof of Lemma C.2 by noticing that |a(0) | = 1. Now, we
2 3 r
verify that H (t), H (t) =⇒ H (t+1). By H (t) and sign(a(t) ) = sign(a(0) ) according to Lemma D.2,
2 4 4 2 r r
we have for any neuron r ∈ Ω ∩Ω that
b1b2···b
k
g
(t+1) (t)
b w = (1−ηλ)b w
j r,j j r,j
(t) (t) (t) (cid:34) (0) (t) (t) (t) (cid:35)
+η·
sign(b 1w r,1)sign(b 2w r,2)···sign(b kw r,k)
·1
|a r w r,1w r,2···w r,k|
≥
ρ
(t) (t) k!
sign(b w ) |w |
j r,j r,j
= (1−ηλ)+η·1[k! ≥ ρ]
= 1,
wherethelastequalityisbyρ ≤ k!(1−c) ≤ k!·|a(t) |. H (t), H (t)=⇒H (t+1), H (t)canbeverified
r 2 5 5 6
in the same way as Lemma B.4 by noticing that ρ ≤ k!(1−c) ≤ k!·|a(t) |. H (t), H (t), H (t+1),
r 3 4 4
H (t) =⇒H (t) can be proved by following exactly the same proof as Lemma B.4. {H (s)}t =⇒
6 1 1 s=0
H 2(t+1), H 3(t+1) be verified in the same way as Lemma B.4 by noticing that ϵ 1·(1+c)·dk− 21 < ρ
and k!−ϵ 1·(1+c)·dk− 21 > ρ.
Based on Lemma D.2 and Lemma D.3, we can get the following lemmas and theorems aligning with
the result of the fixed second-layer case.
Lemma D.4 For T = Θ(kη−1λ−1log(d)),with a probability of at least 1−δ with respect to the
randomness of the online data selection, it holds that
(T)
w = b , ∀r ∈ Ω ∩Ω ,j ∈ [k],
r,j j b1b1···b k g
|w(T) | ≤ d−(k+1), ∀r ∈ Ω ,j ∈ [d]\[k],
r,j g
|w(T) | ≤ d−(k+1), ∀r ∈ Ω ,j ∈ [d].
r,j b
31Lemma D.5 With a probability of at least 1 − 2δ with respect to the randomness in the neural
network’s initialization and the online data selection, trained neural network f(W(T),x) approximates
accurate classifier (m/2k+1)·f(W∗,x) well:
(cid:18) f(W(T),x) (cid:19)
P ∈ [0.25,1.75] ≥ 1−ϵ.
x∼Dx (m/2k+1)·f(W∗,x)
Proof Let
m
(cid:88)
f(cid:101)(W(T),x) = a(0)·σ(⟨w(T),x⟩).
r r
r=1
By the proof of Lemma B.7, we can get
|f(cid:101)(W(T),x)−(m/2k+1)·f(W∗,x)|
≤ 0.5.
(m/2k+1)·|f(W∗,x)|
To prove the result, we need to estimate the difference between f(cid:101)(W(T),x) and f(W(T),x):
(cid:12) (cid:12)
m
(cid:12) (cid:12)f(W(T),x)−f(cid:101)(W(T),x)(cid:12)
(cid:12) =
(cid:12) (cid:12)(cid:88) (a(T)−a(0))·σ(⟨w(T),x⟩)(cid:12)
(cid:12)
(cid:12) r r r (cid:12)
(cid:12) (cid:12)
r=1
m
(cid:88)
≤ |a(T)−a(0)|·|⟨w(T),x⟩|k
r r r
r=1
m (D.7)
(cid:88)
≤ c |⟨w(T),x⟩|k
r
r=1
(cid:88) (cid:88)
= c |⟨w(T),x⟩|k+c |⟨w(T),x⟩|k
r r
r∈Ωg r∈Ω
b
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
I1 I2
where the first inequality is by triangle inequality; the second inequality is by Lemma D.2. Then, we
provide upper bounds for terms I and I respectively. For I , we have the following upper bound:
1 2 1
I = (cid:88) |⟨w(T) ,x ⟩+⟨w(T) ,x ⟩|k
1 r,[1:k] [1:k] r,[k+1:d] [k+1:d]
r∈Ωg
≤ (cid:88) |⟨w(T) ,x ⟩|k + (cid:88) |⟨w(T) ,x ⟩|·k(cid:0) |⟨w(T) ,x ⟩|+|⟨w(T) ,x ⟩|(cid:1)k
r,[1:k] [1:k] r,[k+1:d] [k+1:d] r,[1:k] [1:k] r,[k+1:d] [k+1:d]
r∈Ωg r∈Ωg
(cid:12) k (cid:12)k
≤ (cid:88) (cid:88) (cid:12) (cid:12)(cid:88) b jx j(cid:12) (cid:12) + (cid:88) d−k ·k(cid:0) k+d−k(cid:1)k
(cid:12) (cid:12)
(b1,···,b k)∈{±1}kr∈Ω b1b2···bk∩Ωg j=1 r∈Ωg
≤ (cid:88) (cid:16)1 2k+ +α
1
(cid:17) m·(cid:12) (cid:12) (cid:12) (cid:12)(cid:88)k b jx j(cid:12) (cid:12) (cid:12) (cid:12)k +(cid:16)1+
2
α(cid:17) m·d−k ·k(cid:0) k+d−k(cid:1)k
(b1,···,b k)∈{±1}k j=1
≤
(cid:16)1+α(cid:17)
m·2kk(1+e−2)k
+(cid:16)1+α(cid:17)
m·d−k ·k(cid:0) k+d−k(cid:1)k , (D.8)
2k+1 2
32wherethefirstinequalityisbymeanvaluetheorem; thesecondinequalityisby (B.2)andLemmaD.4;
the third inequality is by Lemma A.1; the last inequality is by Lemma E.3. For I , we have the
2
following upper bound
(cid:16)1+α(cid:17)
I ≤ |Ω |·(2d−k)k ≤ m·(2d−k)k. (D.9)
2 b
2
By plugging (D.8) and (D.9) into (D.7), we can get:
(cid:12) (cid:12)f(W(T),x)−f(cid:101)(W(T),x)(cid:12)
(cid:12)
≤ c(cid:0) (1+α)2−km·kk(1+e−2)k +0.5(1+α)m·d−k ·k(cid:0) k+d−k(cid:1)k +0.5(1+α)m·(2d−k)k(cid:1) .
Therefore, we have
(cid:12) (cid:12)
(cid:12)f(W(T),x)−f(cid:101)(W(T),x)(cid:12)
(m/2k+1)·|f(W∗,x)|
(1+α)2−km·kk(1+e−2)k +0.5(1+α)m·d−k ·k(cid:0) k+d−k(cid:1)k +0.5(1+α)m·(2d−k)k
≤ c·
0.5k!·m
2(1+α)2−k ·kk(1+e−2)k +(1+α)·d−k ·k(cid:0) k+d−k(cid:1)k +(1+α)·(2d−k)k
≤ c· √
2πk(k/e)k
(cid:32)(cid:114) 2 (cid:16)e+e−1(cid:17)k (cid:114) 2 (cid:18) d−k(cid:19)k−1 (cid:16)e(cid:17)k (cid:114) 2 (cid:16) 2e (cid:17)k(cid:33)
= c· (1+α) + · 1+ · + ·
πk 2 πk k d πk kdk
(cid:114)
8 (cid:16)e+e−1(cid:17)k
≤ c·
πk 2
1
= ,
4
where the second inequality is by Stirling’s approximation, and the last equality is due to c =
(cid:113)
1 πk(cid:0)e+e−1(cid:1)−k.
4 8 2
E Auxiliary Lemmas
We first introduce a finite difference operator with step h and order n as follows,
n (cid:18) (cid:19)
(cid:88) k
∆n[f](x) = (−1)n−if(x+ih).
h i
i=0
The following Lemma calculates the value finite difference operating on the polynomial function.
Lemma E.1 (Milne-Thomson, 2000) For f(x) = xn, we have that ∆n[f](x) = hnn!.
h
Based on Lemma E.1, we have the following Lemma, which calculates the margin of good NNs
defined in (3.1).
33Lemma E.2 For any integer k, we have
k (cid:18) (cid:19)
(cid:88) k
(−1)i(k−2i)k = 2kk!. (E.1)
i
i=0
Proof Applying Lemma E.1 with f(x) = xk, n = k in Lemma E.1 gives,
k (cid:18) (cid:19)
(cid:88) k
(−1)i(k−2i)k = (−1)k∆k [f](k) = (−1)k(−2)kk! = 2kk!,
i −2
i=0
where the first equality is due to the definition of finite difference operator and the last equality is
due to Lemma E.1.
Lemma E.3 For any positive integer k, it holds that
k (cid:18) (cid:19)
(cid:88) k
|k−2i|k ≤ 2kk(1+e−2)k.
i
i=0
Proof We can establish the following inequality:
(cid:88)k (cid:18) k(cid:19) ⌊ (cid:88)k− 21⌋(cid:18) k(cid:19)
|k−2i|k ≤ 2 |k−2i|k
i i
i=0 i=0
⌊ (cid:88)k− 21⌋(cid:18) k(cid:19)
(cid:16) 2i(cid:17)k
= 2kk 1−
i k
i=0
⌊ (cid:88)k− 21⌋(cid:18) k(cid:19)
≤ 2kk exp(−2i)
i
i=0
k (cid:18) (cid:19)
(cid:88) k
≤ 2kk exp(−2i)
i
i=0
= 2kk(1+e−2)k,
where the first inequality is by (cid:0)k(cid:1) = (cid:0) k (cid:1), the second inequality is by 1−t ≤ exp(−t),∀t ∈ R, the
i k−i
last inequality is by (cid:0)k(cid:1) exp(−2i) > 0.
i
References
Abbe, E., Adsera, E. B. and Misiakiewicz, T. (2022). The merged-staircase property: a
necessary and nearly sufficient condition for sgd learning of sparse functions on two-layer neural
networks. In Conference on Learning Theory. PMLR.
34Abbe, E., Adsera, E. B. and Misiakiewicz, T. (2023a). Sgd learning on neural networks: leap
complexity and saddle-to-saddle dynamics. In The Thirty Sixth Annual Conference on Learning
Theory. PMLR.
Abbe, E. and Boix-Adsera, E. (2022). On the non-universality of deep learning: quantifying the
cost of symmetry. Advances in Neural Information Processing Systems 35 17188–17201.
Abbe, E., Cornacchia, E. and Lotfi, A. (2023b). Provable advantage of curriculum learning on
parity targets with mixed inputs. In Thirty-seventh Conference on Neural Information Processing
Systems.
Barak, B., Edelman, B. L., Goel, S., Kakade, S., Malach, E. and Zhang, C. (2022).
Hidden progress in deep learning: Sgd learns parities near the computational limit. arXiv preprint
arXiv:2207.08799 .
Blum, A. (2005). On-line algorithms in machine learning. Online algorithms: the state of the art
306–325.
Chizat, L.andBach, F.(2020). Implicitbiasofgradientdescentforwidetwo-layerneuralnetworks
trained with the logistic loss. In Conference on Learning Theory. PMLR.
Daniely, A. and Malach, E. (2020). Learning parities with neural networks. Advances in Neural
Information Processing Systems 33 20356–20365.
Downey, R. G., Fellows, M. R., Vardy, A. and Whittle, G. (1999). The parametrized
complexity of some fundamental problems in coding theory. SIAM Journal on Computing 29
545–570.
Dumer, I., Micciancio, D. and Sudan, M. (2003). Hardness of approximating the minimum
distance of a linear code. IEEE Transactions on Information Theory 49 22–37.
Dutta, C., Kanoria, Y., Manjunath, D. and Radhakrishnan, J. (2008). A tight lower bound
for parity in noisy communication networks. In Proceedings of the nineteenth annual ACM-SIAM
symposium on Discrete Algorithms.
Edelman, B., Goel, S., Kakade, S., Malach, E. and Zhang, C. (2024). Pareto frontiers in
deep feature learning: Data, compute, width, and luck. Advances in Neural Information Processing
Systems 36.
Farhi, E., Goldstone, J., Gutmann, S. and Sipser, M. (1998). Limit on the speed of quantum
computation in determining parity. Physical Review Letters 81 5442.
Frei, S., Chatterji, N. S. and Bartlett, P. L. (2022). Random feature amplification: Feature
learning and generalization in neural networks. arXiv preprint arXiv:2202.07626 .
Glasgow, M. (2023). Sgd finds then tunes features in two-layer neural networks with near-optimal
sample complexity: A case study in the xor problem. arXiv preprint arXiv:2309.15111 .
35Hu, K., Ren, Z., Siska, D. and Szpruch, L. (2019). Mean-field langevin dynamics and energy
landscape of neural networks. arXiv preprint arXiv:1905.07769 .
Jacot, A., Gabriel, F. and Hongler, C. (2018). Neural tangent kernel: Convergence and
generalization in neural networks. In Advances in neural information processing systems.
Ji, Z. and Telgarsky, M. (2020). Polylogarithmic width suffices for gradient descent to achieve
arbitrarily small test error with shallow relu networks. In International Conference on Learning
Representations.
Kearns, M. (1998). Efficient noise-tolerant learning from statistical queries. Journal of the ACM
(JACM) 45 983–1006.
Klivans, A. R., Servedio, R. A. and Ron, D. (2006). Toward attribute efficient learning of
decision lists and parities. Journal of Machine Learning Research 7.
Mei, S., Montanari, A. and Nguyen, P.-M. (2018). A mean field view of the landscape of
two-layer neural networks. Proceedings of the National Academy of Sciences 115 E7665–E7671.
Meng, X., Zou, D. and Cao, Y. (2023). Benign overfitting in two-layer relu convolutional neural
networks for xor data.
Milne-Thomson, L. M. (2000). The calculus of finite differences. American Mathematical Soc.
Suzuki, T., Wu, D., Oko, K. and Nitanda, A. (2023). Feature learning via mean-field langevin
dynamics: classifying sparse parities and beyond. In Thirty-seventh Conference on Neural
Information Processing Systems.
Telgarsky, M.(2022). Featureselectionwithgradientdescentontwo-layernetworksinlow-rotation
regimes. arXiv preprint arXiv:2208.02789 .
Vardy, A. (1997). The intractability of computing the minimum distance of a code. IEEE
Transactions on Information Theory 43 1757–1766.
Wei, C., Lee, J. D., Liu, Q. and Ma, T. (2019). Regularization matters: Generalization and
optimization of neural nets v.s. their induced kernel. Advances in Neural Information Processing
Systems .
Xu, Z., Wang, Y., Frei, S., Vardi, G. and Hu, W. (2023). Benign overfitting and grokking in
relu networks for xor cluster data.
36