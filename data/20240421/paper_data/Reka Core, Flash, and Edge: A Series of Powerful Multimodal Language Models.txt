Reka Core, Flash, and Edge: A Series of Powerful
Multimodal Language Models
AitorOrmazabal CheZheng CypriendeMassond’Autume DaniYogatama
DeyuFu DonovanOng EricChen EugenieLamprecht HaiPham IsaacOng
KaloyanAleksiev LeiLi MatthewHenderson MaxBain MikelArtetxe
NishantRelan PiotrPadlewski QiLiu RenChen SamuelPhua
YazhengYang YiTay YuqiWang ZhongkaiZhu ZhihuiXie
Abstract
WeintroduceRekaCore,Flash,andEdge,aseriesofpowerfulmultimodallanguagemodelstrained
fromscratchbyReka.1 Rekamodelsareabletoprocessandreasonwithtext,images,video,andaudio
inputs.Thistechnicalreportdiscussesdetailsoftrainingsomeofthesemodelsandprovidescomprehensive
evaluationresults.WeshowthatRekaEdgeandRekaFlasharenotonlystate-of-the-artbutalsooutperform
manymuchlargermodels,deliveringoutsizedvaluesfortheirrespectivecomputeclass. Meanwhile,our
mostcapableandlargestmodel,RekaCore,approachesthebestfrontiermodels(OpenAI,2023;Google,
2023;Anthropic,2024)onbothautomaticevaluationsandblindhumanevaluations. Onimagequestion
answeringbenchmarks(e.g.,MMMU,VQAv2),CoreperformscompetitivelytoGPT4-V.Meanwhile,on
multimodalchat,Coreranksasthesecondmostpreferredmodelunderablindthird-partyhumanevaluation
setup,outperformingothermodelssuchasClaude3Opus.Ontextbenchmarks,Corenotonlyperforms
competitivelytootherfrontiermodelsonasetofwell-establishedbenchmarks(e.g.,MMLU,GSM8K)but
alsooutperformsGPT4-0613onhumanevaluation.Onvideoquestionanswering(Perception-Test),Core
outperformsGeminiUltra.Modelsareshippedinproductionatchat.reka.ai.Ashowcaseofnoncherry
pickedqualitativeexamplescanalsobefoundatshowcase.reka.ai.
1PleasecitethisreportasauthoredbyRekateam.
1
4202
rpA
81
]LC.sc[
1v78321.4042:viXra1 Introduction
ThistechnicalreportdetailscomprehensiveevaluationsoftheRekamodels(Core,Flash,Edge)onlanguage
andvisiontasksalongwithdiscussionsondevelopment,benchmarkdesign,andthetrainingpipeline.
RekaEdgeandFlasharedensemodelswith7Band21Bparameters,respectively. Ourevaluationshowsthat
thesemodelsarestate-of-the-artfortheircomputeclass,oftensurpassingmodelsmuchlarger. Meanwhile,
thecurrentversionofRekaCoreapproachesmanyofthebestfrontiermodels(OpenAI,2023;Google,2023;
Googleetal.,2023;Anthropic,2024). Itexcelsinbothautomatedbasemodelevaluationsandblindthird-party
humanevaluations. Figure1comparesRekamodelsagainstproprietarylargelanguagemodels(LLM)APIs.
Weplotthepriceagainstperformance,usingMMLUscoreasanapproximateindicatorofmodelquality. All
RekamodelsarepositionedeitheronorbeyondtheParetofrontier.
Figure1: Priceperperformance(MMLUscore)ofdifferentLLMAPIs.
RekaCoreapproachestheperformancelevelsofGPT-4V(OpenAI,2024)onMMMU(Yueetal.,2024),VQAv2,
andthird-partymultimodalchatevaluation. Meanwhile,RekaCoresurpassesallClaude3models(Opus,
Sonnet,Haiku)(Anthropic,2024)onmultimodalchathumanevaluation. Onvideoquestionanswering
(Perception-test(Pătrăuceanetal.,2023)),bothRekaFlashandCoreoutperformGeminiUltra(Google,2023).
Onlanguagebenchmarks,RekaCoreachieves83.2MMLUscoreandcompetitiveGSM8K,HumanEval,and
GPQAscorescomparedtootherfrontiermodels. Ontext-onlychat,blindhumanevaluationshowsthatReka
CoreoutperformsGPT-4(0613)andranksthirdonourinternalELOleaderboard(rightafterGPT-4Turbo
andClaude3Opus).
Meanwhile,ourEdge(7B)modelsurpassesthecurrentstate-of-the-artmodelsofthiscomputeclass,out-
performing both Gemma 7B (Gemma et al., 2024) and Mistral 7B (Jiang et al., 2023). Additionally, the
Flash(21B)model,asidefromoutperformingGPT-3.5Turbo,alsooutperformsmuchlargerstate-of-the-art
modelssuchasGrok-1(xAI,2023),MistralMedium(Touvronetal.,2023)andGeminiPro1.0(Google,
22023). Onmultimodalevaluations,FlashoutperformsbothClaude3OpusandSonnet(Anthropic,2024)on
multimodalchatandmatchestheSonnetmodelonMMMU(Yueetal.,2024). Allinall,theEdge&Flash
modelsareextremelypowerfulmodelsonacompute-classbasis.
Inadditiontocomprehensiveevaluationsandbenchmarkevaluationsonbothlanguageandvision(video+
image)tasks,thisreportalsosharessomeinterestingtechnicaldetailsandbehind-the-scenesoftraininglarge
multimodalmodelsasastartup. Areasdiscussedincludeinfrastructure,datapipeline,compute,annotation
pipelines,andmore. Finally,artifactsofourmodels(playground/chat,developerplatform)canbefoundin
thefollowingresourcetable(Table1).
Table1: ResourcetreeofRekaartifacts.
What Where?
Playground(chatapp) chat.reka.ai
QualitativeExamples(static,non-cherrypicked) showcase.reka.ai
APIplatform(signup,managecredits) platform.reka.ai
Discord(questions) discord
Homepage reka.ai
2 Model
Thissectionbrieflydescribesthetechnicaldetailsbehindthesemodels.
2.1 TrainingData
Thetrainingdatacomprisesamixtureofpubliclyavailableandproprietary/licenseddatasetswithadataset
knowledgecutoffofNovember2023. Thedatasetingestedbyourmodelcomprisesoftext,images,videos,and
audioclips. RekaFlashandRekaEdgeweretrainedonapproximately5trillionand4.5trillionextensively
deduplicatedandfilteredlanguagetokens,respectively. Whiletheclassificationofcorporaisnotstrictly
definedtooneclassorcategory,approximately25%ofourpretrainingdataiscoderelated,and30%are
STEMrelated. Approximately25%ofthedataiswebcrawl. About10%ofourdatahassomerelationto
math. Overallmixtureratesgenerallyfollowaprincipleofprioritizinguniquetokensbutarehand-adjusted
usingsignalfromalimitednumberofsmallscaleablations.
Table2: StatisticsofRekasuiteofmultimodallanguagemodels. Note: RekaCorehasnotfinishedtraining
andisstillimproving.
Model ModelSize Texttokens Context Long-context KnowldgeCutoff
Edge 7Bdense 4.5T 8K 64K Nov2023
Flash 21Bdense 5T 8K 128K Nov2023
Core - - 8K 128K Nov2023
MultilingualData: Approximately15%ofourpretrainingdataisexplicitly(anddeliberately)multilingual,
comprising32diverselanguagestier-weighted(roughlybyfrequencyinthewild). Beyondtheseexplicitly
up-weightedlanguages,wealsotrainontheentiremultilingualWikipediacomprisingof110languagesso
weexpectabaselineperformanceformostlanguages. Itisworthnotingthatthesetiersreflectpretraining
capability and not necessarily downstream post-training induced capabilities of the final model. To be
concrete,thesearemeaningfultoestimatethepotentialofaparticularlanguage,givensuitablesupervised
finetuningdata. Languagesincludedduringpretrainingareshownbelow.
3Table3: Tieredlanguagesinpretrainingmixture.
PretrainingTier Languages
P1languages German, Chinese, Japanese, French, Korean, Spanish, Italian, Arabic,
Hindi
P2languages Indonesian, Vietnamese, Thai, Czech, Dutch, Finnish, Bulgarian, Por-
tuguese,Tamil,Persian,Greek,Russian
Additionallanguages Turkish, Telugu, Burmese, Swahili, Urdu, Estonian, Malay, Basque,
Swedish,Norwegian
MultimodalData: Themultimodaltrainingdatacompriseslargecollectionsofimages,videos,documents,
andwebpages. Thechosendatamixtureiscarefullyoptimizedforquality,diversity,andscale.
2.2 Architecture&Modeling
Figure2: ArchitecturaloverviewforRekaCore,Flash&Edgemodels: amodularencoder-decodertrans-
formersupportingmultimodalinput(image,text,video&audio). Thetextoutputcaninvokefunctioncalls,
suchaswebsearchandcodeexecution,thenreturntheresults.
Transformer
Image
Text Encoder Decoder Text
Video
Code Execution λ
Audio Web Search
...
λ: function calls
λ: function calls
Thissectionintroducestrainingdetails,modelarchitecture,andcontextlengthdetails.
Architecture&Training. Ouroverallarchitecture(Figure2)isamodularencoder-decoderarchitecture
supporting text, image, video, and audio inputs. For now, our model only supports text outputs. The
backbone Transformer model is based on the ’Noam’ architecture, i.e., it uses SwiGLU (Shazeer, 2020),
Grouped Query Attention (Ainslie et al., 2023; Shazeer, 2019), Rotary positional embeddings (Su et al.,
2021)andRMSNorm(ZhangandSennrich,2019). Architecturally,thisissimilartothePaLMarchitecture
(Chowdheryetal.,2022)butwithoutparallellayers. RekaFlashandEdgeusesasentencepiecevocabof
100Kbasedontiktoken(e.g.,GPT-4tokenizer). Weaddsentineltokensformaskingspans,i.e.,<extra_id_0>
andotherspecialusecasessuchastool-usethatarebeyondthescopeofthistechnicalreport. Pretraining
usesacurriculumthatgoesthroughmultiplestageswithdifferentmixturedistributions,contextlengths,
andobjectives. Thecurrentversionofthismodelisadensemodel. Modelsaretrainedwithbfloat16.
ContextLength. Ourstandardmodelshaveacontextlengthof8Kforourregularmodels. RekaFlash
andRekaCorehave128Kforlongcontextmodelsforretrievalandlongdocumenttasks. Allourmodels
passneedle-in-the-haystack(passkeyretrieval)forthecontexttheysupport. Basedonthesetests,our128K
modelsseemtoextrapolateto256Kcontextlength(butnotbeyond). Forlongcontexttraining,inadditionto
instructiontuningdatawecollect,wesyntheticallycreatesupervisedfinetuningdatausingourownsuiteof
4modelsbyconditioningonlongdocumentsfoundinpretrainingcorpususingatechniquewecallreverse
instructiontuningfromlongdocuments.
2.3 Compute&Infrastructure
OurfamilyofRekamodelswastrainedpredominantlyonNvidiaH100susingPytorch(Paszkeetal.,2019).
Oursetupcomprisesofclustersfromamixtureofvendorswithourpeakcomputebeingapproximately
2.5KH100sand2.5KA100s. Ourpeaknumberofclustersis6. Aboutmorethan90%ofourcomputecame
onlineinmid-December2023. RekaFlashandEdgeweretrainedonseveralhundredsofH100sacrossa
periodofseveralweeks. Ourpretrainingprocesswasrelativelysmoothwithveryfewlossspikesdespite
veryaggressivelearningrates2evenformuchlargermodels. Figure3showsthetraininglossforRekaCore.
ToimprovetheI/Oofourclusters,especiallyforscalabletrainingwithmultimodalinputs,weusedtheCeph
filesystemfordistributedandscalabledatastorageacrossnodeswhichimprovedI/Osubstantiallybutcame
withmaintenanceoverheads.
Figure3: TraininglossforRekaCore.
2.3.1 Hardwarelotteryandnodestability.
Generally,wefindgreatunreliabilitywhenitcomestoGPUnodeswhichoftenfailduetohardwareerrors
orconnectionissues. Moreover,reliabilityamongprovidersisgenerallyofhighvariance. Formoredetails,
refertoTay(2024). ToexpanduponTay(2024),wereporttheaveragenumberofnodefailuresacrossfour
anonymizedproviders,asshowninTable4. Sincethelikelihoodofnodefailuresisinfluencedbythenumber
ofnodesconcurrentlyusedfortraining,wereportestimatedfailureratesfordifferentconfigurations.
ChaoticandStablephases Asidefromvariancesacrossclustersandproviders,providerscouldalsohave
highvarianceacrosstimeperiods. Forexample,manycomputeprovidershaveclustersthatbehavevery
differentlyinthefirstfewweeksofhandoverorwhenevertheclusterundergoesabigchange. Hence,we
alsocomparethenodefailureratesduringboththeearlyphaseandstablephase. Moreoftenthannot,aside
fromearlyphaseofhandingoveracluster,provisioningnewnodescanalsointroduceanewchaoticerathat
canlastafewdaysorweeks. Ingeneral,wedeterminedthatakeyfactorinfluencingthedifferencebetween
theearlyandstabilizedphasesiswhethertheclusterwasactivelyusedfordistributedtrainingbyprevious
customers.
2Modelstrainedattheedgeofstabilityturnoutstronger.Seehttps://x.com/m__dehghani/status/1686056450081337344.
5Table4: Averagenumberofnodefailures(onaweeklybasis)acrossfouranonymizedcomputeproviders.
Sincenodefailuresdependonthenumberofnodesusedconcurrently,wereportestimatedfailureratesfor
differentconfigurations. Manycomputeprovidershaveclustersthatbehaveverydifferentlyinthefirstfew
weeksofhandover. Hence,wealsoreportthedifferenceinnodefailurerateinboththeearlyphaseand
stablephase. ChipsrefertoeitherH100sorA100s.
Provider Number of Number of node
chipsused failures(perweek)
ProviderA 2000chips 3
ProviderA(earlyphase) 2000chips 20+
ProviderB 300chips 0.2
ProviderB(earlyphase) 300chips 0.2
ProviderC(stablephase) 300chips 3
ProviderC(stablephase) 100chips 3
ProviderC(earlyphase) 100chips 30+
ProviderD 300chips 2
Inferenceandserving. Webuiltacustominferencestackfortextandmulti-modalityrunningonacombi-
nationofA10sandA100s. WeuseKubernetesastheunderlyingorchestrationengineandmanageseveral
largeclustersacrossdifferentregions.
2.4 Post-Training
Thissectiondescribesthepost-trainingprocesswhichinvolvesaligning,instructiontuningthemodel.
SFTandRLHF. Afterpretraining,ourmodelsaretheninstructiontuned(Weietal.,2021;Ouyangetal.,
2022;Chungetal.,2024)formultipleepochsusingstrongregularization. AsforSFTdata,wetrainona
mixtureofdatasetsthatincludeourproprietaryandpubliclyavailabledata. AfterSFT,modelsarethen
alignedwithRLHF,specificallyPPO(Schulmanetal.,2017),usingthesamefamilyofRekamodelsasthe
rewardmodel. OurmodelsgothroughacoupleofroundsofRLHFintotal. Moreover,ourpost-training
processconsiderstool-use,functioncallingandwebsearch,whichisoutofscopeforthistechnicalreport.
AnnotationPipelinesforDataCollectionandHumanEvaluation. Wecollectdatausingexternaldata
collectioncompaniesandprovidethemwithauserinterfaceforannotatingbothtext-onlyandmultimodal
data. WecreateanannotationUIforbothcollectingdataand/orsendingexamplestohumanratersfor
blindhumanevaluation. Thissoftwarealsosupportsannotatingforindividualpointwisequalityandalso
side-by-side(pairwise)evaluations. Ourannotationsoftwaresupportsimages,videos,andtext-onlyprompts
andresponses. Italsosupportstheannotationofmulti-turndialogues.
3 Evaluation
ThissectiondiscussestheresultsofextensiveevaluationsofRekamodels.
3.1 BaseModelEvaluation
Weconductaseriesoflanguage-onlyandmultimodal(image,videoinput)evaluations.
6LanguageModelEvaluation. Wecompareourmodelsonfourlanguagemodelevaluations: 1)MMLU
(generallanguageunderstandingandquestionanswering)(Hendrycksetal.,2021),2)GSM8K(reasoning
andarithmetic)(Cobbeetal.,2021),HumanEval(codegeneration)(Chenetal.,2021)andGPQA(graduate-
levelquestionanswering)(Reinetal.,2023). Allnumbersfrombaselinesarereportednumbersinother
works. MMLUisevaluatedwith5-shotdirectpromptingforallmodels. ForGSM8K,mostmodelsuse8-shot
chain-of-thought(Weietal.,2022)andmajorityvoting(maj@8). ForHumanEval,thisisevaluatedin0-shot
setup. Allresultsfromothermodelsarereportedfromotherworks.
Multimodal(Image/Video)Evaluation. Wecompareourmodelsusingvisualquestionansweringdatasets,
i.e.,MMMU(Yueetal.,2024),VQAv2(Goyaletal.,2017),andPerception-Test(Pătrăuceanetal.,2023)for
videoquestionanswering. ForRekamodels,allresultsare0-shot.
Table5: ComparisonsofourRekaFlashandRekaCoreagainstotherfrontiermodels. Dashes(−)referto
eithermodelnotsupportingmodalityorunavailablebenchmarkscores.
Model/Eval RekaCore RekaFlash GPT-4 Claude 3 Claude 3 Gemini Gemini
v0.5 v1.5 Opus Sonnet Ultra Pro1.5
MMLU 83.2 75.9 86.4 86.8 79.0 83.7 81.9
(Knowledge)
GSM8K 92.2 85.8 92.0 95.0 92.3 94.4 91.7
(Reasoning)
HumanEval 76.8 72.0 76.5 84.9 73.0 74.4 71.9
(Coding)
GPQA(main) 38.2 34.0 38.1 50.2 39.1 35.7 41.5
(HardQA)
MMMU 56.3 53.3 56.8 59.1 53.1 59.4 58.5
(ImageQA)
VQAv2 78.1 78.4 77.2 − − 77.8 73.2
(ImageQA)
Perception-test 59.3 56.4 − − − 54.7 51.13
(VideoQA)
Results. Table5reportscomparisonsofRekaCoreagainstotherfrontier-classmodels. Overall,RekaCore
performscompetitivelywithotherfrontier-classmodels. Onmostmetrics(withtheexceptionofMMLU),
itiscomparabletoGPT-44. IntermsofoverallperformanceandwithrespecttotheClaude3series,itfalls
somewhereinbetweenOpusandSonnet. WhencomparedtoGeminimodels,RekaCorehasmixedoutcomes,
i.e.,winningsomeandlosingsome. RekaCoreoutperformsGeminiPro1.5onseveralbenchmarks(MMLU,
GSM8K,HumanEval)butisoutperformedonGPQAandMMMU.Notably,RekaCoreandFlashoutperform
GeminiUltra(andPro1.5)onvideoquestionanswering. RekaCoreisstillimprovingsoweexpectbetter
resultsinthenearfuture.
3.2 ChatModelEvaluation
Weconductablindevaluationwithhumanratersfromathirdpartydataprovidercompany. Weconsider
twosetups: 1)multimodalchat,wheretheuserasksaquestionaboutanimage,and2)text-onlychat. We
3WereportPro1.0performanceheresincePro1.5didnotreportperception-test.
4Atleastanolderversion,withtheresultsmostlyreportedfromtherecentClaude3release. HumanEvallookstoolowforthe
Claude3releasesowereferencedtheHumanEvalleaderboardforthisnumber.
7nextdetailourevaluationprotocolandpresentresultsforeachsetting.
3.2.1 EvaluationSetup
Foreachannotationinstance, humanratersaregivenapromptalongwithamaximumof4generations
fromdifferentmodels,andaskedtoratetheanswersaccordingtotheguidelinesprovided. Giventhatthe
numberofmodelsinourevaluationishigherthan4,wecollectmultiplesuchannotationsforeachprompt,
eachwithadifferentsubsetofmodels. Thepairingofmodelsisdecidedrandomlyforeachprompt,withall
combinationsbeingequallylikely. WecomputeELOscoresfollowingAskelletal.(2021),whereweonly
considerpairwisecomparisonswhereannotatorsexpressapreferencestrongerthantheweakestavailable.
Wedesignourevaluationdatasettocoveradiversesetofprompts. Thefollowingtabledetailsthecomposition
ofourtext-onlyevaluationset,whichcomprises1K+prompts:
Table6: Taxonomyofpromptsinourtext-onlyhumanevaluationdataset. Thedatasetisbalancedacross
subcategories.
Category Subcategory
Humanitiesandsocialsciences
Naturalsciences
Knowledge-intensive Engineeringandtechnology
Entertainment
Other
Roleplaying
Brainstorming
Creativewriting Poetry
Literaryprose
Non-literaryprose
Dataprocessing
Readingcomprehension
Classification
Input-based Extraction
Summarization
Rewriting
Translation
Maths
Reasoning Commonsenseandlogicalreasoning
Instructionfollowing
Coding N/A
Similarly,thefollowingtablereportsthecategoriescoveredbyourmultimodalevaluationset:
8Table7: Distributionofpromptsinourmultimodalhumanevaluationdataset.
Category Ratio
Basicimagedescription 23.0%
Advancedimagedescription 20.5%
Codingcapabilitywithvision 7.7%
Multilingualmultimodalunderstanding 7.9%
Multimodalknowledgeandcommonsense 7.7%
Sceneanddocumentreasoning 13.0%
Visualreferringprompting 5.1%
Creativetasks 2.6%
Other 12.5%
3.2.2 MultimodalChatEvaluation
WenextreporttheresultsofourmultimodalchatevaluationincomparsionwithGPT4-V,Claude3,Gemini
Pro,IDEFICS80B,AdeptFuyu8B,andthestrongestLlava1.6Bmodel:
Table8: ELOscoresofallmodelsonourmultimodalhumanevaluation.
Model ELO Winrate
GPT-4V 1201 79.4
RekaCore 1130 72.2
RekaFlash 1082 66.8
Claude3Opus 1073 66.2
Claude3Sonnet 1069 64.1
Llava1.634B 1022 55.9
GeminiPro 1011 54.2
RekaEdge 986 50.5
IDEFICS80B 732 18.8
AdeptFuyu8B 550 6.4
WefindthatRekaCoreoutperformsallmodelsexceptGPT4-Vbyasubstantialmargin. RekaFlashranks
next,performingmarginallybetterthanClaude3Opus. RekaEdgeoutperformsIDEFICS80BandAdept
Fuyu8Bbyalargemargin,approachingtheperformanceofGeminiProandthelargestLlava1.6model.
3.2.3 Text-onlyChatEvaluation
WecompareourmodelsagainstdifferentversionsofGPT,Claude3,Llama2Chat,andGeminiPro(API
version),andreportourresultsnext:
9Table9: ELOscoresofallmodelsonourtext-onlyhumanevaluation.
Model ELO Winrate
GPT-4Turbo(1106-preview) 1227 78.6
Claude3Opus 1185 73.6
RekaCore 1091 60.6
Claude3Sonnet 1074 59.0
GPT-4(0613) 1062 57.0
RekaFlash 1020 49.1
GPT-3.5Turbo(0613) 1012 48.9
Llama2Chat70B 984 43.0
GeminiPro 950 38.3
RekaEdge 903 31.5
Llama2Chat7B 850 24.3
WefindthatRekaCorerankscompetitivelyonourELOleaderboard,outperformingClaude3Sonnetand
GPT-4,anditisonlysurpassedbyGPT-4TurboandClaude3Opus. RekaFlashobtainsstrongresultsforits
size,beatingGPT-3.5Turbo,GeminiProandthemuchlargerLlama2Chat70B.
3.2.4 ModeldevelopmentandautomaticevaluationusingRekaCore
Weleveragethefrontier-classcapabilitiesofRekaCoreformodelselectionanddevelopmentandshowan
exampleofhowweuseitformultimodalchat. WeaskRekaCoretosimulatehumanjudgementbyratingare-
sponsewithrespecttoapromptandareferenceanswer. Inshort,f(prompt,model_output,reference_answer )∈
[1,100]. WefindthatRekaCorerankingsacrossmodelscorrelatetohumanjudgementdespitethegapbe-
tweenpointwiseandpairwise(arenastyle)evaluations. Ourgeneralworkflowisthatweperformlightweight
andsimplepointwiseevaluationsforcontinuoussanitychecksbeforesendingourmodelsforthirdparty
blindhumanevaluations.
10Figure4: ResultsusingRekaCoreasanevaluator. RekaCoreevaluatorscoresalignalmostperfectlywiththe
finalELOscoresweobtainfromhumanraters.
Figure4reportstheRekaCorescoresweobtainrightbeforeproducingTable8. DespiteRekaCoreevaluations
being pointwise, we find that it is ableto accurately approximate the finalrankings. Here, the onlykey
differenceisthatRekaFlashandClaudeOpushaveflippedrankings. Inpractice,thesemodelsmaybevery
similarinperformancethatitcouldgoeitherway. InTable,8,wealsonotethatRekaFlashandClaudeOpus
haveverysimilarwinratesandELOscores,whichiswellreflectedbytheirRekaCorescoresbeingveryclose
aswell. Overall,wefindthatRekaCoreisquiteagoodapproximatoroffinalhumanevaluationoutcomes.
3.3 Cross-lingualEvaluations
Weconductexperimentsonasuiteofgeneralmultilingualbenchmarkssuchasmultilingualcommonsense
(XStoryCloze(Linetal.,2022)),causalreasoning(XCOPA(Pontietal.,2020)),questionanswering(Belebele
(Bandarkaretal.,2023),XQuAD(Artetxeetal.,2019),TydiQA(Clarketal.,2020)). Foralldatasets,we
reportthemeanacrossalllanguages. WecompareourmodelswithLlama270B(Touvronetal.,2023),
GPT-3.5andGPT-4. Allevaluationsarezero-shotgenerativeexceptXStoryClozewhichuseslog-likehood
evaluation.
11Table10: Statisticsofmultilingualdatasets.
Eval Languages Num
Langs
XStoryCloze hi,te,en,zh,ru,my,sw,es,id,eu,ar 11
XCOPA sw,th,tr,et,vi,qu,id,zh,it,ta,ht 11
XQuAD ar,de,el,en,es,hi,ro,ru,th,tr,vi,zh 12
XWinograd fr,en,jp,pt,zh,ru 6
TydiQA ar,bg,en,fi,id,jp,ko,ru,sw,te,th 11
Belebele toomany 150
Table11: ComparisonsofourmodelsonmultilingualtasksagainstGPT-3.5andGPT-4. Alltasksarezero-
shot.
Eval/Model Metric Reka Reka Llama-2 GPT-3.5 GPT-4
Core Flash 70B
v0.5 v1.5
XStoryCloze acc 72.0 70.1 63.2 N/A N/A
XCOPA acc 88.3 68.0 50.6 72.2 86.3
XQuAD EM 65.7 61.4 25.5 34.6 44.2
XWinograd acc 86.8 84.0 65.3 72.2 91.5
TydiQA(wcontext) EM 60.4 64.8 34.9 53.1 58.9
TydiQA(w/ocontext) EM 17.4 15.7 3.9 13.5 21.1
Belebele(alllangs) acc 63.4 57.3 48.0 51.1 N/A
Table11reportsourevals5onmultilingualbenchmarks. GenerallywefindthatRekaCoreoutperformsall
baselinesreliablyonmosttasks(exceptGPT-4whereitismixed). Specifically,RekaCoreoutperformsGPT-4
onXCOPA,XQuAD,TydiQAbutisoutperformedonXWinogradandTydiQA(w/ocontext). Meanwhile,
CoreoutperformsFlashonallbenchmarks. BothFlashandCoreoutperformsLlama-270BandGPT-3.5.
Finally,Figure5showsthelanguagebreakdownsofCorevsGPT-4.
Figure5: ComparisonofRekaCorevsGPT-4. Breakdownoflanguageson0-shotTydiQA(withcontext).
5WedonotrunevalsforGPTmodelsonXStoryClozebecauseweuselogprobs.AsforBelebele,wehitourcreditthresholdjust
evaluatingonthislargeevaluationdatasetsowestopped.
123.4 LongContextQuestionAnswering
Weconductaseriesofevaluationsonlongcontextquestionanswering. Weuseinternalbenchmarksintwo
domains: (1)movieplotsand(2)ToS(terms-of-service)contractwithcontextsintheballparkof100Ktokens.
Bothdatasetsarequestionansweringtaskswherethetaskistoanswerquestionsgivenalongdocument. We
comparewithClaude3(HaikuandSonnet).
Table12: Longcontextquestionansweringevaluationresults.
Model RekaCore RekaFlash Claude3Haiku Claude3Sonnet
MoviePlots 83.6 79.7 76.6 82.2
ToSContract 87.5 90.0 85.0 90.0
Table12reportsresultsonlongcontextquestionansweringusinginternalevaluationdatasets. Overallwe
showthatFlashandCorearebothcompetitivetothelatestClaude3models.
3.5 MedicalReasoning
WecompareourRekamodelsagainststate-of-the-artdomain-specificmedicalmodelssuchasMeditron
(Chenetal.,2023)andMed-PaLM-2(Singhaletal.,2023). WealsocomparewithGPT-4reportedfrom
(Singhaletal.,2023). Wecompareonthreebenchmarks: MedMCQA,PubMedQAandMMLU(Medical).
MMLU medical is a macro-average over clinical knowledge, medical genetics, anatomy, professional medicine,
collegebiologyandcollegemedicine.
Table13: Resultsonmedicalreasoningtaskscomparedtodomainspecializedmodelsandfrontiermodels.
Reka Meditron
Benchmark/Model Edge Flash Core 7B 70B Med-PaLM-2 GPT-4
(7B) (21B)
MedMCQA 52.6 71.3 80.6 28.7 52.0 71.3 72.4
PubMedQA 71.6 69.0 74.6 69.3 79.8 79.2 80.4
MMLU(Medical) 65.7 79.5 88.3 54.2 72.7 87.8 90.3
Avg 63.3 73.2 81.3 50.7 68.2 79.4 81.0
Table13reportsresultsonmedicaltasks. MeditronandMed-PaLM-2arespecializedmodelsformedicine.
OurresultsshowthatRekaCoreiscompetitivewithsomeofthebestfrontiermodelsandspecializedmodels
inmedicine. Firstly,RekaFlashandCoreoutperformstheMeditronseries. Secondly,RekaCoreoutperforms
bothMed-PaLM-2andGPT-4onMedMCQA.However,itisoutperformedonPubMedQA.Finally,onMMLU
(Medical),RekaCoreoutperformsMed-PaLM-2andisslightlybehindGPT-4. Overall,onaverage,Reka
CoreoutperformsMed-PaLM-2andisapproximatelysimilartoGPT-4onmedicaltasks.
3.6 DetailedcomparisonsofEdgeandFlash
WereportdetailedresultsofRekaEdgeandFlashagainstothermodelsofsimilarcomputeclass. Notably,
bothEdgeandFlashhavebeenimprovedquitesubstantiallysincetheinitialreleaseinFeb. Hence,numbers
havebeenupgradedsincetheirfirstappearances.
133.6.1 RekaEdgeresults
WereportresultsofRekaEdgeagainstother7BmodelssuchasLlama2(Touvronetal.,2023),Mistral(Jiang
etal.,2023)andGemma(Gemmaetal.,2024).
Table14: ResultscomparingRekaEdgewithotherleading7Bmodelsintheindustry. Mostbenchmarksare
reportedfromotherworkswiththeexceptionofthosedenotedwith†. Formultilingualbenchmarks,werun
themourselves.
Benchmark metric Llama27B Mistral7B Gemma7B RekaEdge
MMLU 5-shot 45.3 62.5 64.3 65.7
GSM8K maj@1 14.6 35.4 46.4 66.2
MATH 4-shot 2.5 12.7 24.3 23.2
HumanEval 0-shot(pass@1) 12.8 26.2 32.3 54.3
XQuAD† 0-shot 16.6 29.7 21.7 54.2
TydiQA† 0-shot 16.4 31.7 35.8 61.5
TydiQA†(w/ocontext) 0-shot 2.8 5.0 4.7 6.9
Belebele† 0-shot 27.7 32.8 26.8 37.1
Table14reportsresultsofRekaEdgeagainstother7Bmodels(Gemma,Mistral,Llama). Weobservethat
RekaEdgehasanedgeagainstallothermodels(nopunintended). ItoutperformsMistral7BandLlama7B
onall8benchmarks. AsforGemma,itoutperformsGemmaforallbenchmarksexceptMATH.Overall,Reka
Edgeisasuperstrongmodelat7Bscale.
3.6.2 RekaFlashresults
GiventhattherearenotmanygoodmodelsaroundthesamecomputeclassasRekaFlash,wecompareReka
Flashwithmodelsthataremuchlarger. Specifically,Llama270B(Touvronetal.,2023),GeminiPro1.0
(Google,2023),MistralMedium(Touvronetal.,2023)andGrok-1(xAI,2023).
Table15: ResultscomparingRekaFlashwithothermuchlargermodels.
Benchmark metric Llama 2 Gemini Mistral Grok-1 Reka
70B Pro1.0 Medium Flash
MMLU 5-shot 68.9 71.8 75.3 73.0 75.9
GSM8K maj@8 56.8 86.5 − 62.9 85.8
MATH 4-shot 13.5 32.6 − 23.9 29.6
HumanEval 0-shot 29.9 67.7 38.4 63.2 72.0
MMMU(vision) 0-shot N/A 47.9 N/A N/A 53.3
VQAv2 0-shot N/A 77.2 N/A N/A 78.4
Perception-test 0-shot N/A 51.1 N/A N/A 56.4
Table15reportsresultsofFlash(21B)againstothermodelsoflargercomputeclass. Allcompetitorsare
approximatelyaround70BparameterswiththeexceptionofGrok-1whichisasparsemodelwith314billion
parameters. WeseethatFlashoutperforms(oriscompetitiveto)allcompetitorsonmostbenchmarksdespite
beingmuchsmaller.
144 Conclusion
Weintroduceanewseriesofpowerfulmultimodalmodels,namelyRekaCore,Flash,Edge. RekaFlash
andEdgesetsanewstate-of-the-artonacompute-classbasis,oftendeliveringmassiveoutsizedvaluefor
theirscale. OurCoremodelapproachesfrontier-classmodelsonbothhumanevaluationandautomatic
benchmarks. RekaCoreisstillimprovingsoweexpecttoseeevenmoreimprovementsinthemediumterm.
Thefieldoflargelanguagemodels(Radfordetal.,2018;Brownetal.,2020;Devlinetal.,2018;Raffeletal.,
2019;Chowdheryetal.,2022;Hoffmannetal.,2022)isstillnascentbutmovingveryquickly. Withthatcomes
thetrade-offofsignificantamountofnoiseinthelandscape. Wehopethistechnicalreportshowstherigorof
whatittakestobuildfrontier-classmodelsfromscratchgivenlimitedresources.
155 Appendix
5.1 MMMUbreakdown
InTable16,wereportourcategory-levelscoresinMMMU(Yueetal.,2024)forRekaCore.
Table16: BreakdownofcategoriesfromMMMUbenchmark(Yueetal.,2024).
Category Score
Art 86.7
ArtTheory 83.3
Design 86.7
Music 46.7
Accounting 46.7
Economics 56.7
Finance 43.3
Manage 40.0
Marketing 50.0
Biology 56.7
Chemistry 46.7
Math 46.7
Physics 36.7
BasicMedicalScience 56.7
ClinicalMedicine 60.0
DiagnosticsandLaboratoryMedicine 53.3
Pharmacy 63.3
PublicHealth 56.7
History 80.0
Literature 90.0
Sociology 73.3
Agriculture 70.0
ArchitectureandEngineering 40.0
ComputerScience 50.0
Electronics 26.7
EnergyandPower 43.3
Materials 36.7
MechanicalEngineering 43.3
Overall 56.3
5.2 Historicversioning,changelogandtimelineofRekaChatmodels
WeincludetheversionhistoryofRekamodelstoeasilyrefertothemacrossthistechreport.
16Table17: VersionhistoryofallRekaEdge,CoreandFlashmodels.
Model Date Comments
RekaCorev0.5 Q2’24 Aprlaunchversion
RekaFlashv1.5 Q2’24 Aprlaunchversion
RekaFlashv1.0 Q1’24 Febpubliclaunchversion
RekaEdgev1.5 Q2’24 Aprlaunchversion
RekaEdgev1.0 Q4’23 Febpubliclaunchversion
RekaPrototypev0.5 Q3’23 Octoberprivatepreviewversion
17References
JoshuaAinslie,JamesLee-Thorp,MichieldeJong,YuryZemlyanskiy,FedericoLebrón,andSumitSanghai.
Gqa: Traininggeneralizedmulti-querytransformermodelsfrommulti-headcheckpoints,2023.
Anthropic. Theclaude3modelfamily: Opus,sonnet,haiku. 2024.
MikelArtetxe,SebastianRuder,andDaniYogatama. Onthecross-lingualtransferabilityofmonolingual
representations. CoRR,abs/1910.11856,2019.
AmandaAskell,YuntaoBai,AnnaChen,DawnDrain,DeepGanguli,TomHenighan,AndyJones,Nicholas
Joseph, Ben Mann, Nova DasSarma, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Jackson
Kernion,KamalNdousse,CatherineOlsson,DarioAmodei,TomBrown,JackClark,SamMcCandlish,
ChrisOlah,andJaredKaplan. Agenerallanguageassistantasalaboratoryforalignment,2021.
LucasBandarkar,DavisLiang,BenjaminMuller,MikelArtetxe,SatyaNarayanShukla,DonaldHusa,Naman
Goyal,AbhinandanKrishnan,LukeZettlemoyer,andMadianKhabsa. Thebelebelebenchmark: aparallel
readingcomprehensiondatasetin122languagevariants,2023.
Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan,PranavShyam,GirishSastry,AmandaAskell,etal. Languagemodelsarefew-shotlearners.
arXivpreprintarXiv:2005.14165,2020.
MarkChen,JerryTworek,HeewooJun,QimingYuan,HenriquePondedeOliveiraPinto,JaredKaplan,Harri
Edwards,YuriBurda,NicholasJoseph,GregBrockman,AlexRay,RaulPuri,GretchenKrueger,Michael
Petrov,HeidyKhlaaf,GirishSastry,PamelaMishkin,BrookeChan,ScottGray,NickRyder,MikhailPavlov,
AletheaPower,LukaszKaiser,MohammadBavarian,ClemensWinter,PhilippeTillet,FelipePetroskiSuch,
DaveCummings,MatthiasPlappert,FotiosChantzis,ElizabethBarnes,ArielHerbert-Voss,WilliamHebgen
Guss,AlexNichol,AlexPaino,NikolasTezak,JieTang,IgorBabuschkin,SuchirBalaji,ShantanuJain,
William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan
Morikawa,AlecRadford,MatthewKnight,MilesBrundage,MiraMurati,KatieMayer,PeterWelinder,
BobMcGrew,DarioAmodei,SamMcCandlish,IlyaSutskever,andWojciechZaremba. Evaluatinglarge
languagemodelstrainedoncode. 2021.
ZemingChen,AlejandroHernándezCano,AngelikaRomanou,AntoineBonnet,KyleMatoba,Francesco
Salvi,MatteoPagliardini,SiminFan,AndreasKöpf,AmirkeivanMohtashami,AlexandreSallinen,Alireza
Sakhaeirad,VinitraSwamy,IgorKrawczuk,DenizBayazit,AxelMarmet,SyrielleMontariol,Mary-Anne
Hartley,MartinJaggi,andAntoineBosselut. Meditron-70b: Scalingmedicalpretrainingforlargelanguage
models,2023.
AakankshaChowdhery,SharanNarang,JacobDevlin,MaartenBosma,GauravMishra,AdamRoberts,Paul
Barham,HyungWonChung,CharlesSutton,SebastianGehrmann,etal. Palm: Scalinglanguagemodeling
withpathways. arXivpreprintarXiv:2204.02311,2022.
HyungWonChung,LeHou,ShayneLongpre,BarretZoph,YiTay,WilliamFedus,YunxuanLi,XuezhiWang,
MostafaDehghani,SiddharthaBrahma,etal. Scalinginstruction-finetunedlanguagemodels. Journalof
MachineLearningResearch,25(70):1–53,2024.
Jonathan H Clark, Eunsol Choi, Michael Collins, Dan Garrette, Tom Kwiatkowski, Vitaly Nikolaev, and
JennimariaPalomaki. Tydiqa: Abenchmarkforinformation-seekingquestionansweringintypologically
diverselanguages. TransactionsoftheAssociationforComputationalLinguistics,8:454–470,2020.
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias
Plappert,JerryTworek,JacobHilton,ReiichiroNakano,ChristopherHesse,andJohnSchulman. Training
verifierstosolvemathwordproblems,2021.
JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova. Bert: Pre-trainingofdeepbidirectional
transformersforlanguageunderstanding. arXivpreprintarXiv:1810.04805,2018.
18Gemma,ThomasMesnard,CassidyHardin,RobertDadashi,SuryaBhupatiraju,ShreyaPathak,Laurent
Sifre,MorganeRivière,MihirSanjayKale,JulietteLove,PouyaTafti,LéonardHussenot,PierGiuseppe
Sessa, Aakanksha Chowdhery, Adam Roberts, Aditya Barua, Alex Botev, Alex Castro-Ros, Ambrose
Slone,AmélieHéliou,AndreaTacchetti,AnnaBulanova,AntoniaPaterson,BethTsai,BobakShahriari,
CharlineLeLan,ChristopherA.Choquette-Choo,ClémentCrepy,DanielCer,DaphneIppolito,DavidReid,
ElenaBuchatskaya,EricNi,EricNoland,GengYan,GeorgeTucker,George-ChristianMuraru,Grigory
Rozhdestvenskiy,HenrykMichalewski,IanTenney,IvanGrishchenko,JacobAustin,JamesKeeling,Jane
Labanowski,Jean-BaptisteLespiau,JeffStanway,JennyBrennan,JeremyChen,JohanFerret,JustinChiu,
JustinMao-Jones,KatherineLee,KathyYu,KatieMillican,LarsLoweSjoesund,LisaLee,LucasDixon,
MachelReid,MaciejMikuła,MateoWirth,MichaelSharman,NikolaiChinaev,NithumThain,Olivier
Bachem,OscarChang,OscarWahltinez,PaigeBailey,PaulMichel,PetkoYotov,RahmaChaabouni,Ramona
Comanescu,ReenaJana,RohanAnil,RossMcIlroy,RuiboLiu,RyanMullins,SamuelLSmith,Sebastian
Borgeaud, Sertan Girgin, Sholto Douglas, Shree Pandya, Siamak Shakeri, Soham De, Ted Klimenko,
Tom Hennigan, Vlad Feinberg, Wojciech Stokowiec, Yu hui Chen, Zafarali Ahmed, Zhitao Gong, Tris
Warkentin,LudovicPeran,MinhGiang,ClémentFarabet,OriolVinyals,JeffDean,KorayKavukcuoglu,
DemisHassabis,ZoubinGhahramani,DouglasEck,JoelleBarral,FernandoPereira,EliCollins,Armand
Joulin,NoahFiedel,EvanSenter,AlekAndreev,andKathleenKenealy. Gemma: Openmodelsbasedon
geminiresearchandtechnology,2024.
Google,RohanAnil,AndrewM.Dai,OrhanFirat,MelvinJohnson,DmitryLepikhin,AlexandrePassos,
SiamakShakeri,EmanuelTaropa,PaigeBailey,ZhifengChen,EricChu,JonathanH.Clark,LaurentEl
Shafey,YanpingHuang,KathyMeier-Hellstern,GauravMishra,EricaMoreira,MarkOmernick,Kevin
Robinson,SebastianRuder,YiTay,KefanXiao,YuanzhongXu,YujingZhang,GustavoHernandezAbrego,
JunwhanAhn,JacobAustin,PaulBarham,JanBotha,JamesBradbury,SiddharthaBrahma,KevinBrooks,
MicheleCatasta,YongCheng,ColinCherry,ChristopherA.Choquette-Choo,AakankshaChowdhery,
ClémentCrepy,ShachiDave,MostafaDehghani,SunipaDev,JacobDevlin,MarkDíaz,NanDu,Ethan
Dyer,VladFeinberg,FangxiaoyuFeng,VladFienber,MarkusFreitag,XavierGarcia,SebastianGehrmann,
LucasGonzalez,GuyGur-Ari,StevenHand,HadiHashemi,LeHou,JoshuaHowland,AndreaHu,Jeffrey
Hui,JeremyHurwitz,MichaelIsard,AbeIttycheriah,MatthewJagielski,WenhaoJia,KathleenKenealy,
MaximKrikun,SnehaKudugunta,ChangLan,KatherineLee,BenjaminLee,EricLi,MusicLi,WeiLi,
YaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu, Frederick Liu, Marcello Maggioni,
AromaMahendru,JoshuaMaynez,VedantMisra,MaysamMoussalem,ZacharyNado,JohnNham,Eric
Ni,AndrewNystrom,AliciaParrish,MariePellat,MartinPolacek,AlexPolozov,ReinerPope,SiyuanQiao,
EmilyReif,BryanRichter,ParkerRiley,AlexCastroRos,AurkoRoy,BrennanSaeta,RajkumarSamuel,
ReneeShelby,AmbroseSlone,DanielSmilkov,DavidR.So,DanielSohn,SimonTokumine,DashaValter,
VijayVasudevan,KiranVodrahalli,XuezhiWang,PidongWang,ZiruiWang,TaoWang,JohnWieting,
YuhuaiWu,KelvinXu,YunhanXu,LintingXue,PengchengYin,JiahuiYu,QiaoZhang,StevenZheng,
CeZheng,WeikangZhou,DennyZhou,SlavPetrov,andYonghuiWu. Palm2technicalreport,2023.
GeminiTeamGoogle. Gemini: Afamilyofhighlycapablemultimodalmodels,2023.
YashGoyal,TejasKhot,DouglasSummers-Stay,DhruvBatra,andDeviParikh. MakingtheVinVQAmatter:
ElevatingtheroleofimageunderstandinginVisualQuestionAnswering. InConferenceonComputerVision
andPatternRecognition(CVPR),2017.
DanHendrycks,CollinBurns,StevenBasart,AndyZou,MantasMazeika,DawnSong,andJacobSteinhardt.
Measuringmassivemultitasklanguageunderstanding,2021.
JordanHoffmann,SebastianBorgeaud,ArthurMensch,ElenaBuchatskaya,TrevorCai,ElizaRutherford,
DiegodeLasCasas,LisaAnneHendricks,JohannesWelbl,AidanClark,etal. Trainingcompute-optimal
largelanguagemodels. arXivpreprintarXiv:2203.15556,2022.
AlbertQ.Jiang,AlexandreSablayrolles,ArthurMensch,ChrisBamford,DevendraSinghChaplot,Diego
delasCasas,FlorianBressand,GiannaLengyel,GuillaumeLample,LucileSaulnier,LélioRenardLavaud,
Marie-AnneLachaux,PierreStock,TevenLeScao,ThibautLavril,ThomasWang,TimothéeLacroix,and
WilliamElSayed. Mistral7b,2023.
19Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott,
Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura,
VishravChaudhary,BrianO’Horo,JeffWang,LukeZettlemoyer,ZornitsaKozareva,MonaDiab,Veselin
Stoyanov, and Xian Li. Few-shot learning with multilingual generative language models. In Yoav
Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the 2022 Conference on Empirical
Methods in Natural Language Processing, pages 9019–9052, Abu Dhabi, United Arab Emirates, Decem-
ber 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.616. URL
https://aclanthology.org/2022.emnlp-main.616.
OpenAI. Gpt-4technicalreport,2023.
OpenAI. Gpt-4v(ision)systemcard. 2024.
LongOuyang,JeffWu,XuJiang,DiogoAlmeida,CarrollL.Wainwright,PamelaMishkin,ChongZhang,
SandhiniAgarwal,KatarinaSlama,AlexRay,JohnSchulman,JacobHilton,FraserKelton,LukeMiller,
MaddieSimens,AmandaAskell,PeterWelinder,PaulChristiano,JanLeike,andRyanLowe. Training
languagemodelstofollowinstructionswithhumanfeedback,2022.
AdamPaszke,SamGross,FranciscoMassa,AdamLerer,JamesBradbury,GregoryChanan,TrevorKilleen,
ZemingLin,NataliaGimelshein,LucaAntiga,AlbanDesmaison,AndreasKöpf,EdwardZ.Yang,ZachDe-
Vito,MartinRaison,AlykhanTejani,SasankChilamkurthy,BenoitSteiner,LuFang,JunjieBai,andSoumith
Chintala. Pytorch: Animperativestyle,high-performancedeeplearninglibrary. CoRR,abs/1912.01703,
2019. URLhttp://arxiv.org/abs/1912.01703.
EdoardoMariaPonti,GoranGlavaš,OlgaMajewska,QianchuLiu,IvanVulić,andAnnaKorhonen. XCOPA:
Amultilingualdatasetforcausalcommonsensereasoning. InBonnieWebber,TrevorCohn,YulanHe,
andYangLiu,editors,Proceedingsofthe2020ConferenceonEmpiricalMethodsinNaturalLanguageProcessing
(EMNLP), pages 2362–2376, Online, November 2020. Association for Computational Linguistics. doi:
10.18653/v1/2020.emnlp-main.185. URLhttps://aclanthology.org/2020.emnlp-main.185.
Viorica Pătrăucean, Lucas Smaira, Ankush Gupta, Adrià Recasens Continente, Larisa Markeeva, Dylan
Banarse,SkandaKoppula,JosephHeyward,MateuszMalinowski,YiYang,CarlDoersch,TatianaMate-
jovicova,YurySulsky,AntoineMiech,AlexFrechette,HannaKlimczak,RaphaelKoster,JunlinZhang,
StephanieWinkler,YusufAytar,SimonOsindero,DimaDamen,AndrewZisserman,andJoãoCarreira.
Perceptiontest: Adiagnosticbenchmarkformultimodalvideomodels,2023.
AlecRadford,KarthikNarasimhan,TimSalimans,IlyaSutskever,etal. Improvinglanguageunderstanding
bygenerativepre-training. 2018.
ColinRaffel,NoamShazeer,AdamRoberts,KatherineLee,SharanNarang,MichaelMatena,YanqiZhou,
WeiLi,andPeterJLiu. Exploringthelimitsoftransferlearningwithaunifiedtext-to-texttransformer.
arXivpreprintarXiv:1910.10683,2019.
DavidRein,BettyLiHou,AsaCooperStickland,JacksonPetty,RichardYuanzhePang,JulienDirani,Julian
Michael,andSamuelR.Bowman. Gpqa: Agraduate-levelgoogle-proofq&abenchmark,2023.
JohnSchulman,FilipWolski,PrafullaDhariwal,AlecRadford,andOlegKlimov.Proximalpolicyoptimization
algorithms. CoRR,abs/1707.06347,2017. URLhttp://arxiv.org/abs/1707.06347.
NoamShazeer. Fasttransformerdecoding: Onewrite-headisallyouneed. arXivpreprintarXiv:1911.02150,
2019.
NoamShazeer. Gluvariantsimprovetransformer. arXivpreprintarXiv:2002.05202,2020.
KaranSinghal,TaoTu,JurajGottweis,RorySayres,ElleryWulczyn,LeHou,KevinClark,StephenPfohl,
HeatherCole-Lewis, DarleneNeal, MikeSchaekermann, AmyWang, MohamedAmin, SamiLachgar,
Philip Mansfield, Sushant Prakash, Bradley Green, Ewa Dominowska, Blaise Aguera y Arcas, Nenad
Tomasev,YunLiu,ReneeWong,ChristopherSemturs,S.SaraMahdavi,JoelleBarral,DaleWebster,GregS.
Corrado,YossiMatias,ShekoofehAzizi,AlanKarthikesalingam,andVivekNatarajan.Towardsexpert-level
medicalquestionansweringwithlargelanguagemodels,2023.
20JianlinSu,YuLu,ShengfengPan,BoWen,andYunfengLiu. Roformer: Enhancedtransformerwithrotary
positionembedding. arXivpreprintarXiv:2104.09864,2021.
YiTay. Traininggreatllmsentirelyfromgroundupinthewildernessasastartup. 2024.
HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,NikolayBash-
lykov,SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,DanBikel,LukasBlecher,CristianCantonFerrer,
MoyaChen,GuillemCucurull,DavidEsiobu,JudeFernandes,JeremyFu,WenyinFu,BrianFuller,Cynthia
Gao,VedanujGoswami,NamanGoyal,AnthonyHartshorn,SagharHosseini,RuiHou,HakanInan,Marcin
Kardas,ViktorKerkez,MadianKhabsa,IsabelKloumann,ArtemKorenev,PunitSinghKoura,Marie-Anne
Lachaux,ThibautLavril,JenyaLee,DianaLiskovich,YinghaiLu,YuningMao,XavierMartinet,Todor
Mihaylov,PushkarMishra,IgorMolybog,YixinNie,AndrewPoulton,JeremyReizenstein,RashiRungta,
KalyanSaladi,AlanSchelten,RuanSilva,EricMichaelSmith,RanjanSubramanian,XiaoqingEllenTan,
BinhTang,RossTaylor,AdinaWilliams,JianXiangKuan,PuxinXu,ZhengYan,IliyanZarov,YuchenZhang,
AngelaFan,MelanieKambadur,SharanNarang,AurelienRodriguez,RobertStojnic,SergeyEdunov,and
ThomasScialom. Llama2: Openfoundationandfine-tunedchatmodels,2023.
JasonWei,MaartenBosma,VincentYZhao,KelvinGuu,AdamsWeiYu,BrianLester,NanDu,AndrewM
Dai,andQuocVLe. Finetunedlanguagemodelsarezero-shotlearners. arXivpreprintarXiv:2109.01652,
2021.
JasonWei,XuezhiWang,DaleSchuurmans,MaartenBosma,BrianIchter,FeiXia,EdChi,QuocLe,and
DennyZhou. Chainofthoughtpromptingelicitsreasoninginlargelanguagemodels. ConferenceonNeural
InformationProcessingSystems(NeurIPS),2022.
xAI. Announcinggrok. 2023.
XiangYue,YuanshengNi,KaiZhang,TianyuZheng,RuoqiLiu,GeZhang,SamuelStevens,DongfuJiang,
WeimingRen,YuxuanSun,CongWei,BotaoYu,RuibinYuan,RenliangSun,MingYin,BoyuanZheng,
Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mmmu: A massive
multi-disciplinemultimodalunderstandingandreasoningbenchmarkforexpertagi. InProceedingsof
CVPR,2024.
BiaoZhangandRicoSennrich. Rootmeansquarelayernormalization,2019.
21