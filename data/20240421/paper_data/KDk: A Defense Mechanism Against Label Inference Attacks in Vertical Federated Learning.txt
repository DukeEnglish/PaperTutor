ùëò
KD A Defense Mechanism Against Label Inference Attacks in
Vertical Federated Learning
MarcoArazzi SerenaNicolazzo AntoninoNocera
UniversityofPavia UniversityofMilan UniversityofPavia
Pavia,Italy Milan,Italy Pavia,Italy
marco.arazzi01@universitadipavia.it serena.nicolazzo@unimi.it antonino.nocera@unipv.it
ABSTRACT Finally,TransferLearningisapplicableforscenariosinwhichthere
VerticalFederatedLearning(VFL)isacategoryofFederatedLearn- islittleoverlappingindatasamplesandfeatures,andmultiplesub-
inginwhichmodelsaretrainedcollaborativelyamongpartieswith jectswithheterogeneousdistributionsbuildmodelscollaboratively.
verticallypartitioneddata.Typically,inaVFLscenario,thelabels Hence,HFLischaracterizedbyindependenttrainingandnoraw
ofthesamplesarekeptprivatefromallthepartiesexceptforthe dataissharedamongparticipants,onlymodelupdates,reducing
aggregatingserver,thatisthelabelowner.Nevertheless,recent theriskofprivacyconcernsandtheoverheadoftransmitteddata.
worksdiscoveredthatbyexploitinggradientinformationreturned VFL,instead,exploitsdeeperattributedimensionsandleadstomore
bytheservertobottommodels,withtheknowledgeofonlyasmall accuratemodelsbecausedatafeaturesarecomplementaryacross
set of auxiliary labels on a very limited subset of training data differentsources.Thisresultsinahighercomplexity,alsodueto
points,anadversarycaninfertheprivatelabels.Theseattacksare feature-levelcoordination.
knownaslabelinferenceattacksinVFL.Inourwork,weproposea However,evenifrawdataisnotsharedduetothecalculation
novelframeworkcalledKDùëò,thatcombinesKnowledgeDistillation andexchangeoffeatures,combininginformationacrossfeatures
andùëò-anonymitytoprovideadefensemechanismagainstpotential andthepossiblepresenceofacompromisedparticipantmayraise
labelinferenceattacksinaVFLscenario.Throughanexhaustive privacyleakage[24,25].Possibleattacksthatrepresentasignificant
experimentalcampaignwedemonstratethatbyapplyingourap- concerninthiscontextaretheLabelInferenceAttacks,becauseof
proach,theperformanceoftheanalyzedlabelinferenceattacks thehighsensitivityofthelabelsthatmayrevealcrucialclients‚Äô
decreasesconsistently,evenbymorethan60%,maintainingthe information(e.g.,diseases,financialdetails,etc.).Inourwork,we
accuracyofthewholeVFLalmostunaltered. startconsideringthelabelinferenceattackstoVFLdescribedinthe
recentpaperofFuetal.[8].
KEYWORDS Inourstudy,weprovideadefenseagainsttheabove-citedattacks.
OurapplicationscenarioistheclassicalVFLscenarioinwhichtwo
FederatedLearning,VerticalFederatedLearning,VFL,LabelInfer-
enceAttack,KnowledgeDistillation,ùëòanonymity. typologiesofparticipants,aserverandasetofclients,collabora-
tivelytrainanMLholdingdifferentfeaturespaces.Theserveror
1 INTRODUCTION activeparticipantalsostoresthelabelsthatarekeptprivatefrom
theotherpassiveparticipants.Theadversarycontrolssomepas-
FederatedLearning(FL,forshort)hasemergedinthelastyears
siveparticipantsandaimsatdiscoveringtheprivatelabels.The
asakeytechnologyenablingcollaborativemodeltrainingacross
authorsof[8]demonstratethat,inthisscenario,differenttypesof
differententitieswithouttheneedtogatherdatainacentralloca-
labelinferenceattackssucceedinmostcasesreachinggoodaccu-
tion[17].Theapplicationofthisparadigmisadvantageouswhen
racyresults.Specifically,in[8],Fuetal.discoverthatthanksto
organizationsorindividualshavetocooperateonmodeldevelop-
(i)thetrainedlocalmodelheldbythemaliciousparticipant,and
mentwithoutrevealingtheirsensitivedata.Sincemodelupdates
(ii)thereceivedgradientsofthelossthatcontainshiddeninforma-
areperformedlocallyalsocommunicationcostsarereduced,more-
tionaboutlabels,theattackercansucceedinconductingalabel
over,heterogeneousdatacanbeintegratedmoreeasilymaintaining
inferenceattack.Theydescribethreepossiblecategoriesoflabel
thepeculiaritiesofthedifferentparticipantsandbuildingjointand
inferenceattacks.Thefirstattackisapassiveattack,inwhich,with
richerdatapools.Nevertheless,althoughthisapproachisdesigned
thehelpofsomeauxiliarylabeleddata,themaliciousparticipant
tokeeprawdataonlocaldevices,thereisstillariskofindirect
canfine-tunehis/hertrainedbottommodeltoinferthelabelsin
informationleakage,particularlyduringthemodelaggregation
asemi-supervisedmanner.Thesecondtypeisanactiveattack,in
stage.
whichtheattackertriestoscaleupthelearningrateofher/hisbot-
Accordingtothedifferentdatapartitionstrategiesadopted,three
tommodelduringthetrainingphasetoforcethetopmodeltorely
maincategoriesofFLhavebeenformulated,i.e.,Horizontal,Ver-
moreonher/hismodelthusboostingthelabelinferenceaccuracy.
tical,andTransferLearning[29].HorizontalFL(HFL,forshort)
Thethirdtypeisadirectattackthroughwhichtheadversarycan
requiresallpartiestoholdthesameattributespacebutdifferent
inferlabelsbyanalyzingthesignsofgradientsfromtheserver.
samplespaceanditissuitableforscenarioswherevariousregional OurproposalconsistsofdesigninganovelframeworkcalledKDùëò
branchesofthesamebusinesswanttobuildaricherdataset.Ver- (KnowledgeDiscoveryandùëò-anonymity)asadefensemechanism
ticalFL(VFL,hereafter),instead,isbasedonthecollaborations
relyingonanadditionalcomponentfortheserver(oractive)partic-
amongnon-competingentitieswithverticallypartitioneddatathat
ipant.ThisincludesaKnowledgeDistillationstepandanobfuscation
shareoverlappingdatasamplesbutdifferinthefeaturespace(i.e.,
algorithm.Specifically,KnowledgeDistillation(KD,hereafter)[9]
amobilephonecompanyandaTVstreamingserviceprovider).
4202
rpA
81
]GL.sc[
1v96321.4042:viXraisanMLcompressiontechniqueabletotransferknowledgefroma iscrucialtheyarestillanopenchallengeandonlyafewefforts
largerteachermodeltoasmallerstudentone.Theteachernetwork havebeenmade.
producessofterprobabilitydistributionsinsteadofhardlabelsthat Forexample,thearticles[13,14]studylabelinferenceattacksin
canbettercaptureessentialfeaturesandrelationshipsinthedata. VFL,buttheyspecificallyfocusonsplitlearningscenarios.Inpar-
Weincludeintheactiveparticipantateachernetworkwhose ticular,[13]formalizesathreatmodelforlabelleakageintwo-party
outputsaresoftlabels.Thesearethenprocessedbyanalgorithm splitlearninginthecontextofbinaryclassificationandproposes
basedontheconceptofùëò-anonymity[22]toaddafurtherlevel acountermeasurebasedonrandomperturbationtechniquesthat
of uncertainty. This step groups together theùëò labels with the minimizetheamountoflabelleakageofaworst-caseadversary.
higherprobabilitiesmakingithardfortheattackertoinferthe Whereas,theproposalin[14]presentsapassiveclusteringlabel
mostprobableone.Thenthetopmodeloftheservercanbefed inferenceattackforsplitlearning,inwhichtheadversary(thatcan
withthesenewsoftandpartlyanonymizedlabelsandtheVFLtasks beanyclientsortheserver)retrievestheprivatelabelsbycollect-
canbeexecutedcollaboratively. ingtheexchangedgradientsandsmasheddatabothduringand
Our experimental campaign demonstrates that using our ap- afterthetrainingphase.[15]designtheinversionandreplacement
proachtheaccuracyofthethreetypesoflabelinferenceattacks attackstodiscloseprivatelabelsfrombatch-levelmessagesinaVFL
decreasessignificantly. whosecommunicationisprotectedbyaHomomorphicencryption
Insummary,themaincontributionsofthispaperare: mechanismandaconfusionalautoencoder(CoAE)methodasapos-
‚Ä¢ wedesignacountermeasureforthedifferenttypesoflabel siblecountermeasure.Theproposalin[23]dealswiththedesignof
inferenceattacksproposedby[8]. alabelleakageattackfromtheforwardembeddingintwo-party
‚Ä¢ Weconductanexperimentalcampaigntodemonstratethat splitlearningandacorrespondingdefensethatreducesthedis-
the accuracy of all the analyzed types of label inference tancecorrelationbetweencutlayerembeddingandprivatelabels.
attacksconsistentlydecreasesifourcompleteapproachis Kholodetal.,[11]proposeaparallelizationmethodtodecreasedata
applied. transmission,and,consequently,boththelearningcostandprivacy
‚Ä¢ Weprovideacomparisonwithexistingdefensestrategies leakagerisk.AframeworkcalledLabelGuardhasbeendesigned
andshowthehighereffectivenessofoursolution. in[27]todefendagainstlabelinferenceattacksviaacascadeVFL
algorithmthroughaminimizationoftheVFLtasktrainingloss.
Theorganizationofthispaperisoutlinedasfollows.Section2
Inthiswork,westartfromtheproposals[8].Theauthorsof[8]
describesthemainworksrelatedtoourapproach.Section3delves
intothedetailsaboutFL,ùëò-anonimity,andKnowledgeDistillation describethreekindsoflabelinferenceattack,i.e.,passivelabelinfer-
enceattack,activelabelinferenceattack,anddirectlabelinference
that are essential to the understanding of our solution. Section
attack,forVFL.Adversariescouldinferthelabelsoftheactiveparty
4presentsthetypesoflabelinferenceattacksagainstwhichwe
fromboththereceivedplaintextgradientsandobtainedplaintext
provideadefense.Section6discussestheexperimentalcampaign,
finalmodelweights.Althoughtheseattacksareveryeffective,they
includingthesetupandresultsofourdefensemechanisms.Ulti-
makeastrongassumptiononauxiliarylabelsthathavetobeheld
mately,Section7concludestheworkandpresentspossiblefuture
fortheadversary.
directions.
3 BACKGROUND
2 RELATEDWORK
Inthissection,wedescribesomeconceptsusefultounderstand
RecentworkshaveshownthatFLisvulnerabletomultipletypes
ourapproach.Inparticular,weexaminethekeyaspectsanddif-
ofinferenceattacks,suchasmembershipinference,propertyin-
ferentcategoriesofFederatedLearning,werecalltheconceptof
ference,andfeatureinference[18,19,32].Theobjectiveofamem-
ùëò-anonimityandwedelveintotheanalysisofthemainfeaturesof
bershipinferenceistodiscriminatewhetheraspecificrecordisin
KnowledgeDistillation.
aparty‚Äôstrainingdatasetornot.Nevertheless,thistypeofattack
Table1summarizestheacronymsusedinthispaper.
hasnoreasontoexistinVFLaseveryparticipantknowsallthe
trainingsampleIDs.Propertyinferenceaimstoextractsomeprop-
3.1 FederatedLearning
ertiesaboutaparty‚Äôstrainingdataset,whichareuncorrelatedto
thetrainingtask.Inafeatureinferenceattack,instead,apartytries FL is a Machine Learning method designed to train a model in
torecoverthesamplesusedinanotherparty‚Äôstrainingdataset.For adistributedmanneracrossdifferentdevicesholdinglocaldata
instance,Luoetal.proposeafeatureinferenceattackforVFL[16], samples.Thefactthatdataisnottransferredandcentralizedis
inwhichtheactivepartytriestoinferthefeaturesownedbythe advantageousforprivacypreservationreasonsandnetworktraffic
passiveparty.However,theauthorsstronglyassumethattheactive reductionduetolargedatavolumes.
partyknowsthemodelparametersofthepassiveparty,whichis TheactorsofthisprotocolareCdevices(or‚Äúclients‚Äùor‚Äúwork-
difficulttoachieveinreal-worldscenarios.Differentlyfromthe ers‚Äù),runninglocaltrainingandholdingprivatedata;andacentral
above-citedworks,ourproposaldealswithadifferenttypeofinfer- servercalled‚Äúaggregator‚Äù,thatcoordinatesthewholeFLprocess
enceattackinVFL,knownasalabelinferenceattack,conducted aggregatingthelocalupdates.Specifically,FLaimstotrainaglobal
bythepassivepartyandaimingatleakingthelabelsownedby modelwbyuploadingtheweightsoflocalmodels{wùëñ|ùëñ ‚ààC}toa
theactiveparticipant.Sincelabelsoftencontainhighlysensitive parametricserveroptimizingalossfunction:
informationthistypeofattackdeservesmoreandmoreattention. ùëõ
Althoughfindingpossibledefensestrategiesagainsttheseattacks
minùëô(w)=‚àëÔ∏Åùë†
ùëñ ùêø ùëñ(wùëñ ) (1)
w C
ùëñ=1
2Table1:Summaryoftheacronymsusedinthepaper.
Symbol Description
DL DeepLearning
FCNN FullyConnectedNeuralNetwork
FL FederatedLearning
FTL FederatedTransferLearning
GC GradientCompression
HFL HorizontalFederatedLearning
KD KnowledgeDistillation
ML MachineLearning
NG NoiseGradient
OA OriginalArchitecture
PPDL Privacy-PreservingDeepLearning
VFL VerticalFederatedLearning
Figure1:TheFederatedLearningworkflow
whereùêø ùëñ(wi) = ùë†1
ùëñ
(cid:205) ùëó‚ààùêºùëñùëô ùëó(wùëñ,ùë• ùëñ) isthelossfunction,ùë† ùëñ isthe
localdatasizeofthei-thworker,andùêº ùëñ identifiesthesetofdata
indiceswith|ùêº ùëñ|=ùë† ùëñ,andùë• ùëó isadatapoint.
ThebasicFLworkflow,showninFigure1,canbedividedinto
thefollowingsteps[30]:
(1) Modelinitialization,inwhichthecentralserverinitializesall
thenecessaryparametersfortheglobalMLmodelw.This
phasealsoincludestheworkers‚Äôselectionprocess.
(2) Localmodeltrainingandupload,inwhichtheworkersdown-
loadthecurrentglobalmodelandperformlocaltraining
(a)HorizontalFL(HFL) usingtheirprivatedata.Afterthat,eachclientcomputesthe
modelparameterupdatesandtransmitsthemtothecentral
server.Thelocaltrainingtypicallyinvolvesmultipleitera-
tionsofgradientdescent,back-propagation,orotheropti-
mizationmethodstoimprovethelocalmodel‚Äôsperformance.
Specifically,atthet-iteration,eachclientupdatestheglobal
modelbytrainingwiththeirdatasets:wùëñ ùë° ‚Üêwùëñ ùë°‚àíùúÇùúïùêø ùúï(w wùë° ùëñ ùë°,ùëè) ,
whereùúÇ andùëè identify the learning rate and local batch,
respectively.
(3) Globalmodelaggregationandupdate,inwhichthecentral
servercollectsandaggregatesthemodelparameterupdates
(b)VerticalFL(VFL) fromalltheworkers,{wùëñ|ùëñ ‚ààC}.Thecentralservercanem-
ployvariousaggregationmethodslikeaveraging,weighted
averaging,orsecuremulti-partycomputationtoincorporate
thereceivedupdatesfromeachclient,thusimprovingthe
performanceoftheglobalmodel.
FLcanbeclassifiedintothreescenariosaccordingtothediffer-
entdatapartitionstrategiesadopted,i.e.,Horizontal,Vertical,and
TransferLearningtypes[29],asshowninFigure2.
As visible in Figure 2a, Horizontal FL (HFL, hereafter), or
sample-basedFL,isintroducedinthescenariosinwhichthedataset
ofthepartiessharethesamefeaturespace,buthavedifferentspaces
(c)FederatedTransferLearning(FTL) insamples.Forinstance,twobranchesofthesamecompany(two
regionalbanksortwohospitals)holddatareferringtousersof
Figure2:ThethreecategoriesofFLdividedforfeatureand distinctareas,buttheysharethesamefeaturespaces(i.e.,thesame
samplespaces characteristics).Ifthetwopartiesaggregatetheirsamples,they
couldbuildalargerdatasetandthentrainamoreaccuratemodel.
Yet,privacylawforbidsthedirectsharingofsensitiveuserdata.In
suchcases,HFLcanhelptosolvethischallengeandprovidearich
privacy-preservingdataset.
3 VerticalFL(VFL,forshort)orfeature-basedFL,instead,applies
tothecasewherethedatasetsshareoverlappingdatasamplesbut3.2 k-Anonimity
Theconceptofùëò-anonymity,firstdescribedin[22],representsone
foundationalprincipleindatabasetheoryforprivacy-preserving
datapublishing.Itaimstosafeguardtheanonymityoftheindivid-
uals‚Äôdatabyensuringthateachrecordinthedatasetisindistin-
guishablefromatleastùëò‚àí1otherrecords.Severalprocedurescan
beappliedtoattributestoobtainùëò-anonymity,suchas:
‚Ä¢ Suppression,whichimpliesremovingorcleansingcertain
information.
‚Ä¢ Generalizationisreplacingdistinctivevalueswithmoregen-
eralones(e.g.,substitutingexactageswithageranges).
Figure3:Genericarchitectureofknowledgedistillationusing
3.3 KnowledgeDistillation
ateacher-studentmodel
KnowledgeDistillation(KD,forshort)isanMLmodelcompres-
siontechnique,inwhichtheknowledgefromacomplexmodel,
differinthefeaturespace(asshownin2b.Considerthecaseof
or‚Äúteacher‚Äùmodel,istransferredtoasmallerandmoreefficient
twodifferentbusinessessharinginformationaboutoverlapping
model,knownasthe‚Äústudent‚Äùmodelwithoutasignificantdrop
setsofcustomers.Forexample,amobilephonecompanyanda
inaccuracy[9].ThegeneralideawasfirstpresentedbyBucilua
TVstreamingserviceprovidercanhavecommonclients,butthe
etal.in2006[2]andmodeledinitscurrentknownformin2014
typesofdatarelatedtothemareverydifferent.Nevertheless,arich
byHintonetal.[10]whofounditeasiertotrainaclassifierusing
predictionmodelforservicepurchasescanbebuiltleveragingboth
theoutputsofanotherclassifierastargetvaluesthanusingactual
datasetscollaboratively,butdirectlyrevealingpersonalcustomer
ground-truthlabels.Theteachernetworkoutputsarerepresented
informationtothirdpartiesisnotalwayspossiblebecauseofGDPR.
bytheso-calledsoftprobabilitiesthatcontainmoreinformation
Hence,VFLcanrepresentasolutionforthissettingallowingthe
aboutadatapointthanjusttheclasslabel(orhardpredictions)and
companytocollaborativelytrainajoinmodelwitharichdataset.
aretheinputofthestudentnetwork.
VFLcanbewithorwithoutmodelsplitting: Inpractice,givenaninputùë• theteachernetworkproducesa
‚Ä¢ Inthepresenceofmodelsplitting,everyparticipantrunsa vectorofscoresùë† ùë•ùë° = [ùë† 1ùë°,ùë† 2ùë°,...,ùë† ùêæùë° ]thatareconvertedintoproba-
bottom(orlocal)modelwithoutsharingtheentiremodel bilities:
withotherparticipantsandrelyingonfeatureslocallyavail-
ùëùùë° (ùë•)=
ùëíùë† ùëòùë°
(2)
a stb rl ue ca tet dea bc yh ap sa er rt vy e. rT th he atfi cn oa ml bto inp e( so tr hg elo lob ca al l) lym to rd aie nl ei dsr me oco dn el- ùëò (cid:205) ùëóùëíùë†ùë° ùëó
portionstocomputeafinaloutput.Forinstance,oneparty Hintonetal.[10]proposedtomodifytheseprobabilitiesinsoft
may focus on training the model‚Äôs parameters related to probabilitiesasfollowing:
d pe am rao mg er ta ep rh si rc elf ae ta et dur toes d, aw tah ail be oa un to pt uh re cr hp asa er .tymayworkon
ùëùùë° (ùë•)=
ùëíùë† ùëòùë° /ùúè
(3)
‚Ä¢ Intheabsenceofmodelsplitting,themodelremainscentral- ùëò (cid:205) ùëóùëíùë†ùë° ùëó/ùúè
ized,andeachpartycalculatesgradientsofthelossrelying whereùúèisahyperparameter.Astudentnetworkwillproduceasoft-
onitslocaldata,thenitsharesthesegradientswithacentral enedclassprobabilitydistribution,pÀúùë†(ùë•).Thelossforthestudent
server,whichaggregatesthemtoupdatetheglobalmodel. networkisalinearcombinationofthecrossentropyloss,namely
Afterthetrainingprocessisfinished,attheinferencetime,VFL Lùëêùëô andaknowledgedistillationlossLùêæùê∑:
requiresallparticipantstogetinvolved,insteadforHFL,thetrained
globalmodelissharedwitheveryparticipantwhichperformsin- L=ùõºLùëêùëô ‚àí(1‚àíùõº)Lùêæùê∑ (4)
ferenceindividually. whereLùêæùê∑ =‚àíùúè2(cid:205) ùëòùëùÀúùë°(ùë•)logùëùÀúùë†(ùë•)andùõºandùúèarehyperparam-
ThelastcategoryofFLisFederatedTransferLearning(FTL, eters.
hereafter)whichisacombinationofthetwoprevioustypes[26]. Figure 3 shows the generic architecture of the KD using the
Indeed,FTLisapplicableforscenariosinwhichthereislittleover- teacher-studentmodel.Thankstothedistillationalgorithmthestu-
lappinginbothdatasamplesandfeaturesasvisibleinFigure2c.For dentmimicstheteachernetworklearningtherelationshipbetween
instance,thecaseinwhichmultiplesubjectswithheterogeneous different classes discovered by the teacher model that contains
distributionsbuildmodelscollaboratively.Considerthecasewhere informationbeyondthegroundtruthlabels.
anAmericancompanyproducinghealthIoTsensorswantstojoin
itsdatasamplewithaprivatehealthclinicinCanada.Thesetwo 4 LABELINFERENCEATTACKS
entitieshavetofollowlawrestrictions.Moreover,boththeirsets
Inthissection,wedescribethemostcommontypesoflabelinfer-
ofclientsandfeatureshavesmallintersections.Inthiscase,FTL
enceattacksagainstVFL,whichwefocusontodesignourdefense
techniquescanbeappliedtoprovideasolutionandmakethetwo
strategy.Astypicallydoneintherelatedliterature,wemakeexplicit
partiescooperateinthebuildingofonemodel.
referencetothemorecomplexscenarioinwhichVFLiscombined
withmodelsplitting[7](seeSection3).
44.2 ActiveLabelInferenceAttack
Thisattackisclassifiedasactivebecausethemaliciousparticipant
performssomeactionsinthetrainingstage,inparticular,she/he
tries to scale up the learning rate during the training phase of
her/hisbottommodel.Inthisway,she/heaimstoacceleratethe
gradientdescentonher/hisbottommodeltosubmitbetterfeatures
totheserverineachiteration.Consequently,she/hecanforcethe
topmodeltorelymoreonher/hisbottommodelthantheother
participants.
Sinceincreasingthelearningratedoesnotalwaysresultina
moreefficientgradientdescent,theauthorsof[8]performthis
attackbydesigningandexecutingamaliciouslocaloptimizer.This
componentadaptivelyscalesupthegradientofeachparameterin
theadversary‚Äôsbottommodeltoavoidtheoscillationphenomenon
aroundthelocalminimumpoint,thatistypicaloftheuseofan
overlylargelearningrateforgradientdescent.
Usingthemaliciouslocaloptimizer,theattackercangetatrained
bottommodelwithmorehiddeninformationaboutlabels.Inaddi-
tion,she/hecanperformthemodelcompletionstepofthepassive
Figure4:LabelinferenceattackscenarioagainstVFL
attack(seeSection4.1)tofine-tunethebottommodelwithanaddi-
tionalclassificationlayerandobtainthefinallabelinferencemodel.
In this setting, as originally proposed by [8], label inference
4.3 DirectLabelInferenceAttack
attacksarecarriedoutbyadversaries,controllingoneormoreof
thebottommodels,whichaimtoinfertheprivatelabelsforany Tocarryoutthisattacktheadversarydirectlyexploitsthegradients
samplesinthedataset.Recallthat,accordingtothemodelsplitting she/hereceivesfromthetopmodeltoinferthelabelsofthetraining
paradigm,onlytheactiveparty,i.e.,theserver,hastheclassification examples.Thisisbasedontheanalysisofthesignsofthegradients
layer,whoseobjectiveisthepredictionofthecorrectlabelforeach ofthelosses.Theauthorsof[8]demonstratethroughmathematical
datapointininput.Therefore,labelsareavailableonlytothisactive proofthatthismethodworksforlabelinferenceinVFLwithout
party of the FL system and, therefore, are considered sensitive modelsplitting(seeSection3.1fordetailsaboutmodelsplitting).
information.Tocarryoutalabelinferenceattack,adversariescan Sincenogradientsareavailableattheinferencetime,withthis
mainlyexploittwomainaspectsofVFLthat,accordingto[8],may attack,themaliciousparticipantcanonlyinferthelabelsoftraining
generatelabelleakage,namely: examples.Nevertheless,thesediscoveredlabelscanbeusedasthe
‚Ä¢ thetrainedlocalmodelthatisunderthefullcontrolofthe auxiliarydatanecessarytoperformapassivelabelinferenceattack.
maliciousparticipant; Inthisway,theattackercaninferthelabelofanarbitrarysample.
‚Ä¢ thereceivedgradientsofthelossthatcontainhiddeninfor-
5 APPROACHDESCRIPTION
mationaboutlabels.
Inthefollowing,wedescribefourmaintypesoflabelinference Ourapproachaimstoprovideacountermeasureforallthetypes
attacks,namely:(i)PassiveLabelInferenceattack[8];(ii)Active oflabelinferenceattacksdescribedinSection4.
LabelInferenceattack[8];and(iii)DirectLabelInferenceAttack Tobetterpresentourdefensestrategy,asdoneonceagainin
[8]. [8],wewillfocusonabasicVFLattackscenarioshowninFigure
4.Here,twoparticipantsholdingthesamesetofsamplesbutwith
4.1 PassiveLabelInferenceAttack featuresfromdifferentspaceswanttotrainamodelcollaboratively
throughVFL.
Adversaries can perform this attack by exploiting their locally
ownedbottommodel.Itisreferredtoaspassivebecausethema- Thefirstparticipant,thatistheserver,runsboththetopùëá ùëÄ and
liciousparticipantdoesnotperformanyactiveactionduringthe thebottomùêµ ùê¥ models,henceshe/heisthelabelownerùêªùêø and
trainingorinferencephase,butshe/heremainshonestbutcurious.
holdspartoftheverticallypartitioneddataùëã ùê¥.Forthisreason,
she/heisalsoreferredtoasanactiveclient.Her/hisobjectiveis
Thistypeofattackassumesthattheadversarialcanrelyonafew
auxiliarylabeleddata(in[8]onlythe0.08%ofthelabeledtraining toenhancethemodelperformancebycombiningher/hisfeatures
withtheonesofotherparticipantscomingfromdifferentbusiness
samples have been used as auxiliary labels in the experimental
domains.
campaign).Iftheattackercangetthisadditionalknowledge,she/he
Thesecondparticipantinourexampleistheadversarialorpas-
caninferthelabelsbyfine-tuningher/hisbottommodelthrougha
siveclientwhoaimsatinferringthelabelsfromthetrainingprocess
furtherclassificationlayerinasemi-supervisedmanner.Thisstepis
referredasmodelcompletionattack.Oncethetrainingiscompleted, andhasaccessonlytoitsbottommodelùêµ ùëÉ anditspartvertically
themodelcanpredictalabelforeveryitemofthesampleofthe
partitioneddataùëã ùëÉ.Ateachtraininground,thebottommodelout-
adversary.
putsùêª = {ùêª ùê¥,ùêª ùëÉ}aresenttotheserverrunningthetopmodel
ùëá ùëÄ,which,hence,returnsthecorrespondentpartialgradients‚àáùêª
ùê¥
5and‚àáùêª
ùëÉ
ofthelossùëô.Theseareusedtoupdatetheclients‚Äôbottom theoriginaltrainingset.Thefunctionmax(ùêæùê∑(ùêªùêø))returnsthe
modelparametersùëä ùê¥ (ùëä ùëÉ).Thelocalmodelsupdates‚àáùëä ùê¥ and labelswiththehighestprobabilityforeachdatapoint.Whereas,
‚àáùëä ùëÉ arecalculatedasfollows(ùê∂ùê∏=cross-entropy,ùëÜùëÄ =softmax): thefunctionùë°ùëúùëù ùëò(ùêæùê∑(ùêªùêø))returnsthesetoftheùëò‚àí1labelshav-
ùêª =ùê∂ùëÇùëÅùê∂ùê¥ùëá(ùêª ùê¥,ùêª ùëÉ),ùëùùëüùëíùëëùë† =ùëá ùëÄ(ùêª) (5) ingthehighestvaluesfollowingthemaximum(i.e.,onceagain,
thehighestprobabilitiesofbeingthecorrectlabelofthetarget
ùëô =ùê∂ùê∏(ùëÜùëÄ(ùëùùëüùëíùëëùë†), ùëÜùëÄ(ùêªùêø))
datapointasestimatedbytheKDmodel)foreachdatapoint.
‚àáùëä ùê¥ =‚àëÔ∏Å ùúïùêªùúïùëô ¬∑ ùúï ùúïùêª ùêµùê¥. (6)
ùê¥ ùê¥ Algorithm1SoftLabelAlgorithm.
‚àáùëä ùëÉ =‚àëÔ∏Å ùúïùêªùúïùëô
ùëÉ
¬∑ ùúï ùúïùêª ùêµ ùëÉùëÉ. (7) R 1e :q ùêªui ùêør :e s:
etofHardLabels
Althoughtheprivatelabelsùêªùêøneverleavethefirstparticipant‚Äôs 2: ùêæ:setoftop-klabelswithhigherconfidence
storage,theadversarycanexploitthereceivedpartialgradients 3: ùëò:|ùêæ|cardinatilyofùêæ
andthetrainedbottommodeltoconductalabelinferenceattack. 4: ùúñ:smoothingparameter
5: ùëõ:numberofclasses
In particular, to perform the first attack, or Passive Label In-
6: ùëáùëúùëùùêæùêºùëõùëëùëíùë•ùëíùë†,ùëÄùëéùë•ùëâùëéùëôùë¢ùëí‚Üêùëîùëíùë°ùëáùëúùëùùêæùêºùëõùëëùëíùë•ùëíùë†(ùêªùêø,K)
ference Attack, the adversary relies on a small set of auxiliary
7: ùëÜùêø‚Üêùëßùëíùëüùëúùë†(ùëõ)
labels.Ifshe/hemanagestoobtainthissetshe/hecanfine-tune 8: forùëñinùëáùëúùëùùêæùêºùëõùëëùëíùë•ùëíùë†do
her/hisbottommodelthroughafurtherclassificationlayerina 9: ifùêªùêø[ùëñ]==ùëÄùëéùë•ùëâùëéùëôùë¢ùëíthen
semi-supervisedmannertoinferthetraininglabels.Toconduct 10: ùëÜùêø[ùëñ]‚Üê1‚àíùúñ
theotherattacks(i.e.,theActiveandtheDirectLabelInference 11: else
Attacks),instead,themaliciousparticipantexploitsthefactthat, 12: ùëÜùêø[ùëñ]‚Üêùúñ/(ùëò‚àí1)
eventhoughshe/hedoesnothavedirectaccesstothelabel,her/his 13: endif
bottommodelimplicitlyholdsinformationaboutthem,becauseof 14: endfor
thetrainingstep.Withtheselaststrategies,theadversarycannot
obtainalltheprivatelabels,buthe/shecan,then,runasubsequent
passiveattacktoimprovetheattackperformance. 6 EXPERIMENTALRESULTS
Atthispoint,wearereadytopresentourdefensemechanism Inthissection,weillustratetheexperimentscarriedouttoassessthe
againsttheabove-citedtypesoflabelinferenceattacks.Inparticular, performanceofourdefensemechanism.Specifically,inSection6.1,
weincludeintheactiveparticipantarchitectureanadditionalcom- wedescribethedataset,theevaluationmetrics,andtheenvironment
ponentcomprisedofafine-tunedteachernetworkthatperformsa usedforourexperiments.Theremainingsectionsaredevotedto
KnowledgeDistillationùêæùê∑steptooutputsoftlabelsùëÜùêøinsteadof
analyzingtheresultsandtheperformanceofourdefenseapproach
hardonesùêªùêø.Theoutputvectorofagivendatapointcontainsthe
againstthedifferenttypesofanalyzedlabelinferenceattacksand
probabilitiesthatitbelongstoeachclassrepresentedbytheprivate thecomparisonwithotherdefensemechanisms.
labels.Theoutputfromthislayeristhenprocessedbyanalgorithm
basedontheconceptofùëò-anonymity(see3.2fordetail)toadda 6.1 Testbedsdescription
secondlevelofuncertainty.Throughthisfurtherstep,insteadof
Toevaluatetherobustnessofourapproachagainstlabelinference
selectingasinglelabelforeachsample,weselectasetofùëòlabels
attacksweadoptsomeofthedatasetsusedby[8],namely:
inùëÜùêøwiththehighestprobability.Hence,asshowninAlgorithm
‚Ä¢ CIFAR-10dataset[12]consistingof60,00032ùë•32colorim-
1,ifthelabelistheoneassociatedwiththehighestconfidence
itisscaledbyùúñ(whereùúñisasmoothingparameter),otherwise,if ages divided into 10 classes with 6,000 images per class.
thelabelbelongstotheùëò‚àí1highestprobabilitylabels(excluding
Thereare50,000intrainingimagesand10,000intestim-
themaximum)aùúñ/(ùëò‚àí1)factorisappliedtoscaleuptheirfinal ages.
‚Ä¢ CIFAR-100[12]datasetthatissimilartoCIFAR-10,butithas
probabilityvalue.Atthispoint,sincethecorrectlabelforeach
itemisobfuscatedinagroupofùëòlabels,aswewilldemonstratein 100classescontaining600imageseachwith500training
imagesand100testingimagesperclass.
theexperiments,theattackercannolongereasilyinferthemost
‚Ä¢ CINIC-10[5],whichisalargedatasetandanextendedal-
probableoneperforminganyoftheabove-citedattacks.TheVFL
ternativeforCIFAR-10with270,000images,(i.e.,4.5times
processchangesasfollows:
morethatofCIFAR-10).
ùëÜùêø‚Üêùêæùê∑ùëò(ùêªùêø,ùëò,ùúñ) (8) ‚Ä¢ Yahoo!Answerstopicclassificationdataset[31]isformedby
ùêæùê∑ùëò(ùêªùêø,ùëò,ùúñ)=Ô£±Ô£¥Ô£¥Ô£¥Ô£≤1
ùëòùúñ
‚àí‚àí 1ùúñ ii ff ùêøùêø ùëñùëñ ‚àà‚àà ùë°m ùëúa ùëùx ùëò( (ùêæ ùêæùê∑ ùê∑( (ùêª ùêªùêø ùêø) ))
) (9)
‚Ä¢
1
s
Ca0
rm
im
tp
ea oli en
s
[4c
a
]a nt ide sg
6
ao ,r
0
ri e0e a0s l-ta wen sd
oti
rne lda gc dsh
a
amc tl aa
p
ss
l
es
e
tsc r.o en lata tein ds t1 o4 c0 o,0 m0 m0t er ra ci eni fn og
r
Ô£¥Ô£¥Ô£¥0
otherwise
predictingadclick-throughrates.Inthisdataset,composed
Ô£≥
ùëô ùëòùëëùëò =ùê∂ùê∏(ùëÜùëÄ(ùëùùëüùëíùëëùë†),ùëÜùëÄ(ùëÜùêø)) (10) ofonly2classes,bothcategoricalandcontinuousfeatures
‚àáùëä ùëÉ
=‚àëÔ∏Åùúï ùúïùëô
ùêªùëòùëëùëò ¬∑
ùúï ùúïùêª
ùêµùëÉ. (11)
Toaa sr se ese sm tp hl eoy ee ffd e.
ctivenessofourdefenseapproach,weadopt
ùëÉ ùëÉ
Here ùêæùê∑(ùêªùêø) contains the soft labels (a probability vector) re- thefollowingevaluationmetrics:
turnedbytheknowledgedistillationmodelforeachdatapointin
6Table3:TeacherNetworkArchitecturesforKDùëò.
architecturesthathavebeenimplementedforeachdatasetisvisible
inTable3.
TeacherNetwork Allexperimentshavebeenperformedonaworkstationequipped
Dataset
Architecture withaAMD(R)Ryzen(R)7CPU5800x@3.80GHz,32GBRAM,and
CIFAR-10 ResNet-50+FCNN-1 anNVIDIARTX3070TiGPUcard.
CIFAR-100 ResNet-50+FCNN-1
CINIC-10 ResNet-50+FCNN-1 6.2 LabelInferenceAttacksPerformance
Yahoo!Answers Bert+FCNN-1 Comparison
Criteo FCNN-4
Inthissection,wereporttheperformanceresultsofourdefense
mechanismagainstthefourtypesofLabelInferenceattacksde-
‚Ä¢ Top-1Accuracy,thatistheconventionalaccuracyorthe scribedinSection4.
ratioofcorrectlypredictedsamplestothetotalnumberof Foreachanalyzedattack,wechosetheappropriateconfigura-
samples in the dataset. It measures how many times the tionofthetwoanonymizationparametersùúñandùëò,whereùúñisthe
network has predicted the correct label with the highest smoothingparameterandùëò isthenumberofthehighestlabels
probability. consideredforeachdatapointtobuildoursoftlabelsolution.
‚Ä¢ Top-5Accuracy,isametricthatindicateshowmanytimes
6.2.1 PassiveLabelInferenceAttack. WecarriedoutthePassive
thecorrectlabelappearsinthenetwork‚Äôstopfivepredicted LabelInferenceAttackwitha0.08%ofauxiliarylabeleddata(as
classes.Itisusefulforlarge-scaledatasetswithnumerous proposedin[8])andwetesteditagainstourKDùëòframeworkwith
classesandforcasesinwhichadegreeofflexibilityisac- thetwoanonymizationparametersùúñandùëòsetforeachdatasetas
ceptableandtheexactclasscanbenotpredictedwithhigh
arereportedinTable4.Thistablereportsalsotheaccuracyresults
confidence[20].
oftheattackagainsttheoriginalmodelproposedby[8]andagainst
‚Ä¢ Top-1AttackSuccessRate(Top-1ASR)thatisthepercent-
ourdefensemechanism.Thesevaluesshowthattheperformance
ageoflabelscorrectlyextractedbyattacks. oftheattackagainstKDùëòisdrasticallyreducedand,inmostcases,
‚Ä¢ Top-5AttackSuccessRate(Top-5ASR)measureshow
halvedcomparedtotheperformanceagainsttheoriginalmodel
manytimesthelabelcorrectlyextractedbyattacksappears
of[8].Itisworthobservingthat,intheresultsobtainedonthe
inthenetwork‚Äôstopfivepredictedclasses.
Yahoo!Answerdatasetwecanseeasmallerreductionintheattack
Forourexperimentalcampaign,werefertoanOriginalArchi- performance,whichiscausedbythefactthatthebottommodel
tecture(OA,hereafter)thatrepresentsaVFLscenariowithoutany isanalreadypre-trainedBertmodel.Theknowledgeincludedin
defensemechanismaspresentedby[8].Thisarchitecture,shownin theBertmodelisalreadyenoughtoobtainabasicclassificationof
Figure4,employsdifferenttypesofnetworksforeachoftheabove- thetext(i.e.,informationonthelabels),eveniftheattackerdoes
describeddatasets.Inparticular,asvisibleinTable2,thetopmodel notinferadditionalinformationfromthetopmodel.Thereforethe
oftheVFLisimplementedthroughapre-trainedResNet-18(i.e.,an performanceoftheattackdoesnotdecreaseasmuchasintheother
18-layerconvolutionalneuralnetworkpre-trainedongeneraldata cases.
andfine-tunedontheactiveparticipantdata)fortheCIFAR-10,
CIFAR-100,andCINIC-10datasets;afine-tunedBERTmodel[6] 6.2.2 ActiveLabelInferenceAttack. ToperformtheActiveLabel
forYahoo!Answer(thatincludestextualdata);anda3-layerFully Inference Attack, we executed the malicious local optimizer in
ConnectedNeuralNetwork(FCNN-3)toprocesssamplesinthe thetrainingstageofourKDùëò modelandthenweperformedthe
Criteodataset. completion step of the passive inference attack to get the final
label inference model as done in [8]. The configurations of the
Table2:OriginalModelArchitectures. twoanonymizationparametersùúñandùëòchosenforeachdatasetare
reportedinTable5.Similarlytothepreviousexperiment,when
Top Bottom we performed an active label inference attack against our KDùëò
Dataset
ModelArchitecture ModelArchitecture frameworktheASRconsistentlydecreasedcomparedtotheASRof
CIFAR-10 FCNN-4 ResNet-18 theattackedOA.Observethat,theattackstrategyofincreasingthe
CIFAR-100 FCNN-4 ResNet-18 learningrateonthecontrolledclienttopromotemoreinformative
CINIC-10 FCNN-4 ResNet-18 feedbackfromtheserverdoesnotresultinanadvantage,because
Yahoo!Answers FCNN-4 Bert thankstoourdefensethereceivedsignalisheavilyobfuscated.Also
Criteo FCNN-3 FCNN-3 inthiscase,theresultsfromtheYahoo!Answerdatasetpresenta
smallerreductionintheattackperformance,whichis,onceagain,
Moreover,weimplementedourKDùëòsolutionwhosecomponents causedbythefactthatthebottommodelisanalreadypre-trained
Bertmodel.
areillustratedinFigure5.ComparedtotheOriginalArchitecture,
KDùëòincludesapreliminaryprocessingstepexecutedonlybythe 6.2.3 DirectLabelInferenceAttack. WecarriedouttheDirectLabel
activeparticipantfortheanonymizationofthelabels.Thisstepis InferenceAttackdescribedin[8]andwetesteditagainstourKDùëò
realizedthrough:(i)ateachernetworkthatimplementstheKnowl- frameworkwiththetwoanonymizationparametersùúñandùëòsetfor
edgeDistillationand(ii)analgorithmthatobfuscatestheùëòlabels eachdatasetasarereportedinTable6.Inthistable,wereportthe
withhigherconfidencebasedonùëò-anonymity.Theteachernetwork ASRresultsoftheDirectLabelInferenceAttackagainstOAandour
7Figure5:KDùëòmaincomponents
Table4:PassiveLabelInferenceAttackperformanceagainstOAandKDùëò
AttackSuccessRate(ASR)
OA KDùëò
Dataset TypeofASR ùùê ùíå
TrainingSet TestSet TrainingSet TestSet
CIFAR-10 Top-1 0.45 3 80.6% 61.7% 43.9% 35.8%
CIFAR-100 Top-1 0.50 3 31.3% 18.0% 15.9% 10.5%
CIFAR-100 Top-5 0.50 3 62.2% 41.0% 40.3% 29.7%
CINIC-10 Top-1 0.45 3 65.2% 49.0% 32.0% 26.0%
Yahoo!Answers Top-1 0.35 3 63.3% 63.7% 47.5% 47.4%
Criteo Top-1 0.40 2 71.2% 71.9% 50.6% 50.3%
Table5:ActiveLabelInferenceAttackperformanceagainstOAandKDùëò
AttackSuccessRate(ASR)
OA KDùëò
Dataset TypeofASR ùùê ùíå
TrainingSet TestSet TrainingSet TestSet
CIFAR-10 Top-1 0.50 3 84.8% 63.4% 40.5% 35.1%
CIFAR-100 Top-1 0.60 3 39.3% 21.4% 17.6% 12.2%
CIFAR-100 Top-5 0.60 3 72% 47.4% 43.3% 32.7%
CINIC-10 Top-1 0.50 3 73.5% 50.5% 34.5% 29.3%
Yahoo!Answers Top-1 0.40 3 64.2% 64.1% 52.2% 52.1%
Criteo Top-1 0.40 3 71.2% 71.9% 50.0% 50.0%
Table6:DirectLabelInferenceAttackperformanceagainst KDùëòframework.Sincenogradientsareavailableattheinference
OAandKDùëò
time,thisattackcanbeconductedonlyatthetrainingstephence
wereporttheASRvaluesreferredtothetrainingset.Aswecan
AttackSuccessRate(ASR) observe,ingeneral,ourdefensemechanismcanreducetheASRof
OA KDùëò theattackbymorethan60%exceptfortheCriteodatasetbecause
Dataset TypeofASR ùùê ùíå
TrainingSet
CIFAR-10 Top-1 0.45 3 100% 38.5%
CIFAR-100‚àó Top-1 0.5 3 100% 32.6%
CINIC-10 Top-1 0.45 3 100% 38.3%
Yahoo!Answers Top-1 0.35 3 100% 39.6%
Criteo Top-1 0.4 2 100% 80%
8
*Inthiscase,wedonotconsidertheTop-5accuracybecause
theTop-1isalready100%.thenumberofclassesisequalto2andthereforearandomchoice [8,15],forCIFAR-100weconsideronlytheTop-5accuracythat
ofthetargetlabelwouldleadtoanASRresulthigherthan0.5. providesamorenuancedevaluationbecauseofthelargenumber
ofclasses.
6.3 ModelsPerformanceComparison AsvisibleinFigure6,theuseofhigherùúñvaluesresultsinbal-
Themainideabehindourapproach,aspresentedinSection5,is ancingtheprobabilitiesoftheclasses,andthisaffectstheoverall
toobfuscatetheinformationofthereallabeltoadduncertainty performanceofboththeattackandtheKDùëòmodel.Usingdifferent
inthebottommodeloftheattackertoinhibittheeffectivenessof ùëòvalues,instead,doesnotaffectourdefensemechanism.Interest-
theattackspresentedinSection4.Inevitably,thisapproachwill ingly,settingùëò = 10andusingadatasetcomposedof10classes
affecttheperformanceofthemodelontheoriginaltask,though makethedefenseineffective.Thisresultconfirmsourintuition
wetrytominimizeitbyobfuscatingtherealinformationinaset behindthelogicthatmakesourproposalwork.Thereasonwhy
ofhighlyprobablealternatives(and,therefore,possiblyavoiding ourapproachiseffectivereliesontheuncertaintyinstilledinthe
toheavilyimpacttheperformanceofthetopmodel).Theresultsin bottommodelsobtainedbyanonymizingthereallabelbetween
theprevioussectionhavebeenobtainedbysettingtheanonymiza- ùëòadditionalandrelatedones(asindicatedbyourknowledgedis-
tionparameterssotoguaranteethepreservationoftheoriginal tillationcomponent).Inthecaseofùëò =10,wearesettingallthe
globalmodelperformance.TheaccuracyperformanceofourKDùëò secondarylabelstothesameprobability.Thisbreaksthemainlogic
modelcomparedtoOA(theoriginalarchitectureproposedin[8]), behindourapproach.Settingallthesecondarylabelstothesame
isshowninTable7foreachanalyzeddataset.Aswecanseethe valueproducessimilar(withtheadditionofanoffset)lossvalues
performanceoftheoriginalmodelaremostlypreservedwithsmall comparedtothescenariousinghardlabels,directly.Inthiscase,
dropsof4%atmaximum.ObservethatfortheCIFAR-100dataset, theoffsetaddedtothecross-entropylossisnotsufficienttoprop-
theaccuracyresultisevenhigher,becauseoftheeffectofKD.In- erlyobfuscatethelabels,thusresultinginasmalldecreaseinthe
deed,forlargedatasets,ourmodelbenefitsfromthegeneralization accuracyoftheattackinthecaseoftheCINI10datasetorcaneven
capabilitiesoftheteachernetwork[3,28].Aswesaidourapproach beineffectiveinthecaseofCIFAR-10.Tobeeffectivewithùëò =10,
canimpactthemodelaccuracyaccordingtothestrengthlevelof ourapproachmustpushtheùúñvaluetoextremevalues.Thissetting
theparametersùëò andùúñ.InthefollowingSection6.4,wepresent iseffectiveagainsttheattackbutalsopreventsthemodelfrom
adetailedanalysiscombiningparameterswithdifferentlevelsof trainingusingtheprobabilitydistributionbalancedacrossallthe
strengthandrecordingthemodelaccuracyinmodelandtheattack labels,thusresultinginanaccuracyclosetorandomguessing.
successrate.
6.5 ComparisonwithotherDefense
Table7:PerformanceonKDùëòcomparedtoOAfortheused
Mechanisms
datasets
Intheproposalof[8]severaldefensivestrategiesareappliedtothe
ModelAccuracy gradientstopreventinformationleakagefromtheserverandtry
Dataset Accuracy OA KDùëò tomitigatethedifferentlabelinferenceattacks.Inthissectionwe
CIFAR-10 Top-1 81% 79% compareourdefensemechanismswiththefollowingapproaches
CIFAR-100 Top-1 49.1% 49.4% in[8]:
CIFAR-100 Top-5 78.5% 79.9% ‚Ä¢ NoisyGradients(NG).ToperformthisdefenseinVFL,the
CINIC-10 Top-1 66.7% 64.3% serveraddsalaplaciannoisetogradientsbeforesending
Yahoo!Answers Top-1 71.1% 67.5% themtopassiveparticipants.Themetricweanalyzetocom-
Criteo Top-1 71.3% 69.9% parethisapproachwithourKDùëò isthenoisescale,which
representsseveralscalesoftheusedlaplaciannoise.
‚Ä¢ GradientCompression(GC).Thisstrategyusedforcom-
6.4 Performancewithdifferentvaluesofthe munication efficiency and privacy protection consists of
anonymizationparameters sharing fewer gradients with the largest absolute values.
Themetricweconsidertocomparethisapproachwithour
Inthisexperiment,weanalyzehowboththeASRandtheperfor-
KDùëòisthecompressionrate,whichistheratiobetweenthe
manceofthemodel(intermsofaccuracy)changeinrelationto
uncompressedsizeandcompressedsizeofthegradientval-
highervaluesoftheanonymizationparametersùúñ andùëò.Forthis
ues.
study,weconsideredonlythreedatasets,namelyCIFAR-10,CIFAR-
‚Ä¢ Privacy-PreservingDeepLearning(PPDL).Ineachit-
100,andCINIC-10becausetheyhaveatleast10.Criteohasnot
eration,theserver(i)randomlyselectsonegradientvalue
been considered since it is a dataset with binary labels making
andaddsnoisetothisgradient;(ii)setstozerothegradient
itimpossibletotestoursolutionwithùëò higherthan2.Yahooin-
valuessmallerthanathresholdvalueùúè;(iii)repeatsthefirst
steadisnotincludedsincereliesonapre-trainedBertmodeland,
twostepsuntilŒò ùë¢ fractionofgradientvaluesarecollected.
aswealreadystatedinSection6.2.1,itsaccuracyisintrinsecally
Bothùúè andŒò ùë¢ arehyperparameterstobalancethetrade-off
guarateedbytheperformanceofsuchunderlingmodel,henceit
betweenmodelperformanceanddefenseperformance.We
cannotdecreaselowerthanthevaluespresentedinTable4with
evaluatetheperformanceofthistypeofdefensebyanalyzing
anyparametercombination.
Thatsaid,westudiedtheperformancefordifferentvaluesofùëò differentsettingsofthehyperparameterŒò ùë¢.
(i.e.,ùëò =3,ùëò =5,andùëò =10).Astypicallydoneintheliterature
9OA ASR_Train ASR_Test
CIFAR10, k=3 CIFAR10, k=5 CIFAR10, k=10
100 100 100
80 80 80
60 60 60
40 40 40
20 20 20
0 0 0
0.2 0.3 0.4 0.5 0.6 0.2 0.4 0.6 0.8 0.2 0.4 0.6 0.8
CIFAR100, k=3 CIFAR100, k=5 CIFAR100, k=10
100 100 100
80 80 80
60 60 60
40 40 40
20 20 20
0 0 0
0.2 0.3 0.4 0.5 0.6 0.2 0.4 0.6 0.8 0.2 0.4 0.6 0.8
CINIC10, k=3 CINIC10, k=5 CINIC10, k=10
100 100 100
80 80 80
60 60 60
40 40 40
20 20 20
0 0 0
0.2 0.3 0.4 0.5 0.6 0.2 0.4 0.6 0.8 0.2 0.4 0.6 0.8
Figure6:AnalysisoftheperformanceoftheattackandtheperformanceofKDùëòfordifferentùúñandùëòvalues
Table8:ComparisonwithotherDefenseMechanismsagainstdirectlabelinferenceattacksusingCIFARdatasets
CIFAR-10 CIFAR-100
NoiseScale ModelAccuracy AttackAccuracy ModelAccuracy AttackAccuracy
1e-4 81.4% 80.6% 82.4% 12.2%
Noisy 1e-3 81.1% 49.1% 83.1% 2.0%
Gradients 1e-2 71.9% 24.5% 5.1% 2.0%
1e-1 10.0% 12.7% 5.0% 0.6%
CompressionRate ModelAccuracy AttackAccuracy ModelAccuracy AttackAccuracy
75% 80.4% 99.9% 82.4% 100%
Gradient 50% 80.5% 99.3% 83.1% 100%
Compression 25% 78.4% 92.4% 82.4% 99.9%
10% 10.0% 0.1% 73.8% 99.9%
Œòùë¢ ModelAccuracy AttackAccuracy ModelAccuracy AttackAccuracy
0.75 79.8% 39.0% 81.9% 4.6%
Privacy- 0.50 80.5% 38.9% 81.7% 4.5%
preservingDL 0.25 19.9% 0.1% 5.2% 1.1%
0.10 10.0% 0.1% 5.0% 0.9%
N ModelAccuracy AttackAccuracy ModelAccuracy AttackAccuracy
24 81.0% 96.7% 11.2% 99.9%
Discrete 18 80.8% 94.3% 8.8% 99.9%
SGD 12 78.7% 94.7% 7.1% 99.9%
6 74.3% 91.5% 7.3% 99.7%
k ùùê ModelAccuracy AttackAccuracy ModelAccuracy AttackAccuracy
3 0.45 79.0% 38.5% 80.6% 32.6%
KDùëò 5 0.70 71.1% 23.0% 80.5% 19.2%
5 0.75 66.7% 21.7% 79.7% 19.1%
5 0.85 37.5% 14.2% 76.7% 18.8%
‚Ä¢ DiscreteSGDacustomizedversionofsignSGD[1]thought sharedgradients.Followingthethree-sigmarule[21],the
forVFL.Thedefensemechanismproceedsasfollows.(i)In serversetsanintervalas[ùúá‚àí2ùúé,ùúá+2ùúé](whereùúáisthemean
thefirstepoch,theserverobservesthedistributionofthe
10
ycarucca
1-poT
ycarucca
5-poT
ycarucca
1-poT
ycarucca
1-poT
ycarucca
5-poT
ycarucca
1-poT
ycarucca
1-poT
ycarucca
5-poT
ycarucca
1-poTOA ASR_Train ASR_Test
CIFAR10, NG CIFAR10, GC CIFAR10, PPDL CIFAR10, DiscreteSGD CIFAR10, KDk
100 100 100 100 100
80 80 80 80 80
60 60 60 60 60
40 40 40 40 40
20 20 20 20 20
0 0 0 0 0
0 1e-4 1e-3 1e-2 1e-1 0.0 0.2 0.4 0.6 0.8 0.0 0.2 0.4 0.6 0.8 24 18 12 6 0.2 0.3 0.4 0.5 0.6
Noise Scale 1 - Compression Rate 1 - u N
CIFAR100, NG CIFAR100, GC CIFAR100, PPDL CIFAR100, DiscreteSGD CIFAR100, KDk
100 100 100 100 100
80 80 80 80 80
60 60 60 60 60
40 40 40 40 40
20 20 20 20 20
0 0 0 0 0
0 1e-4 1e-3 1e-2 1e-1 0.0 0.2 0.4 0.6 0.8 0.0 0.2 0.4 0.6 0.8 24 18 12 6 0.2 0.3 0.4 0.5 0.6
Noise Scale 1 - Compression Rate 1 - u N
CINIC10, NG CINIC10, GC CINIC10, PPDL CINIC10, DiscreteSGD CINIC10, KDk
100 100 100 100 100
80 80 80 80 80
60 60 60 60 60
40 40 40 40 40
20 20 20 20 20
0 0 0 0 0
0 1e-4 1e-3 1e-2 1e-1 0.0 0.2 0.4 0.6 0.8 0.0 0.2 0.4 0.6 0.8 24 18 12 6 0.2 0.3 0.4 0.5 0.6
Noise Scale 1 - Compression Rate 1 - u N
Figure7:ComparisonwithotherDefenseMechanismsagainstpassiveandactivelabelinferenceattacks
Table9:ComparisonwithotherDefenseMechanismsagainst Forthisexperiment,weperformedbothpassiveandactivela-
directlabelinferenceattacksusingCINIC-10dataset belinferenceattacksonthreedatasets:CIFAR-10,CIFAR-100,and
CINIC-10.Observethat,onceagain,astypicallydoneinthere-
CINIC-10 lated literature, for CIFAR-100 we considered only Top-5 accu-
NoiseScale ModelAccuracy AttackAccuracy
1e-4 70.5% 84.3% racytocopewiththelargenumberofclasses.Asforthesetting
GrN ao di is ey nts 1 1e e- -3
2
6 59 5. .9 5%
%
4 29 4. .7 3%
%
of the different defense mechanisms, we considered the follow-
1e-1 10.3% 12.6% ingparameters:Laplaciannoiselevel‚àà {10‚àí1,10‚àí2,10‚àí3,10‚àí4},
Compre 7s 5s %ionRate Mode 7l 0A .9c %curacy Attack 99A .8c %curacy gradient compression percentage ‚àà {75%,50%,25%,10%}, PPDL
CoG mra pd reie sn sit on 5 2 10 5 0% %
%
6 5 19 4 0. . .1 7 0% %
%
9 9 09 2 .0. .3 5 1% %
%
Œò teùë¢ rvf ar la sct Nion ‚àà‚àà {6{ ,1 10 2% ,1,2 8,5 2% 4, }5 .0 T% h,7 e5% pa} r, aD mis ec tetr re ste oS fG oD urn au pm pb roer aco hf ii nn --
Œòùë¢ ModelAccuracy AttackAccuracy steadaresetasfollows:ùëò = 3andùúñ varyingbetweenthevalues
Privacy- 0 0. .7 55
0
6 69 8. .4 4%
%
3 38 8. .9 6%
%
‚àà{0.25,0.3,0.45,0.5,0.66}.
preservingDL 0.25 20.8% 0.10%
0.10 12.9% 0.04% 6.5.1 PassiveandActiveAttacks. Theresultsofthedefensesagainst
N ModelAccuracy AttackAccuracy
24 63.1% 97.9% themodelcompletionattackarereportedinFigure7.Asvisiblein
Discrete 18 59.6% 95.6% Figures7,fornoisygradients(NG),weexperimentedusingseveral
SGD 12 45.8% 94.3%
6 43.6% 90.3% scalesoflaplaciannoisetoevaluateitsdefenseperformanceagainst
k ùùê ModelAccuracy AttackAccuracy modelcompletioninferenceattack.Obviouslythegreaterthevalue
3 0.45 67.7% 38.3%
KDùëò 5 0.70 62.2% 24.1% ofthenoisescalethemoresuccessfulareallthemitigationtech-
5 0.75 56.7% 22.2% niques.Tobeeffectivethisdefensemustapplytothegradientsan
5 0.85 34.5% 15.3%
extremelyhighlevelofnoisethatdisruptstheperformanceofthe
andùúéisthestandarddeviation).Thegradientsoutsideofthe modelontheoriginaltask.Withlowerlevelsofnoise,itisinterest-
ingtoseehowthisapproachcanevenhelpobtaininghigherattack
intervalareregardedasoutliersandnotconsidered.(ii)The
performance.
serverslicestheintervalintoNsub-intervals.(iii)Before
Inthesecondcolumnofsub-figuresinFigure7,weevaluate
transmittingthegradientstoalltheparticipants,theserver
GradientCompression(GC)techniquesfordifferentcompression
firstroundseachgradientvaluetothenearestendpointof
thesub-intervals.ThehyperparameterùëÅcontrolshowmuch rates.Alsofromthesefigures,wecannoticethatforgreatercom-
pressionratesboththemodelandtheattackperformancedecrease.
magnitudeinformationofthesharedgradientsispreserved.
Wecanseehowbetweentheselecteddefensescomparedtoours,
Weevaluatethefourdefenseapproachesintroducedaboveand
wecomparethemwithourKDùëòapproach. gradientcompressionisthebestpreservingtheoriginalaccuracy
11
ycarucca
1-poT
ycarucca
5-poT
ycarucca
1-poT
ycarucca
1-poT
ycarucca
5-poT
ycarucca
1-poT
ycarucca
1-poT
ycarucca
5-poT
ycarucca
1-poT
ycarucca
1-poT
ycarucca
5-poT
ycarucca
1-poT
ycarucca
1-poT
ycarucca
5-poT
ycarucca
1-poTofthemodelbutonlyslightlyaffectingtheperformanceoftheat- them,whereastheyarekeptsecretfromalltheotherparties(pas-
tack,especiallywithlowervaluesofcompression.AsforthePPDL siveactors).Nevertheless,recentworkshavestartedtodescribe
mechanism, from the sub-figures in the third column in Figure labelleakageissuesinthiscontextproposingstrategiesforlabel
7wecannoticethatforallthreeanalyzeddatasets,thedefense inferenceattacks,namelypassive,active,anddirectattacks.Inthis
canmitigatelabelinferenceattackswiththehyperparameterŒò ùë¢ paper, we analyzed such existing attacks and proposed a novel
setto0.25orlower(i.e.,theaccuracyresultislowerthan40%for defensemechanism,calledKDùëò,abletoprotectVFLfromallthe
1‚àíŒò ùë¢ =0.75).Eveninthiscase,wecannoticehowthedefense knowntypesoflabelinferenceattackswithveryhighperformance.
iseffectiveonlywithahighlevelofmanipulationofthegradient Ourapproachmodifiestheactiveparticipantmodel,integrating
resultinginaheavylossintermsofperformanceontheoriginal bothaKnowledgeDistillationteachernetworkandaùëò-anonymity
taskforbothCIFAR-10andCINIC-10.AsforCIFAR-100,instead, processingsteptoobtainagroupofùëò mostprobablesoftlabels
PPDLrepresentsthebest-performingdefensewearecomparing foreachiteminsteadofasinglehardlabel.Thisaddsalevelof
with. uncertaintythatpreventstheattackerfromperforminglabelin-
Finally,similarlytothepreviousdefenses,wecannoticehow ferencesuccessfully.Wetestedtheperformanceofoursolution
DiscreteSGDisnotcapableofaffectingtheattackpreservingthe withathoroughexperimentalcampaing,whoseobjectivewastode-
functionalityoftheoriginalmodel.Thisdefensecanachieveslightly mostratethatourapproachcaneffectivelyinhibittheattackerfrom
highperformanceonlyforCIFAR-10. beingabletoperformlabelinference(attacksuccessratereduced,
Lookingatoursolutioncomparedtotheotherswecanseehow insomecases,evenmorethan60%withrespecttoitsperformance
wecanpreventtheattackdecreasingitssuccessratetoalmostthe intheabsenceofourdefense),stillmaitaininganalmostunaltered
sameasarandomguessvalueonCIFAR-10andCINIC-10using accuracyofthefederatedglobalmodel(lessthan2%performance
extreme values forùúñ, while preserving most of the accuracy of decreaseonaverage).Finally,wedemonstratedthesuperiorityof
themainmodel.Itisalsointerestingtoseehow,evenwithlower ourproposalwithrespecttothemostrecentandstate-of-the-art
defenseintensityvalues,ourapproachaffectstheattackstillmore existingdefenses,whichprovedtobeeitheruneffectiveagaistthe
thantheothersolutions. attack,or,insomecases,effectiveagainstonlysomeattackvariants
andoftenatthecostofanextremelyhigh,andhencenotacceptable,
6.5.2 DirectLabelInferenceAttack. InTables8and9,weanalyze
impactonthefederatedglobalmodelperformance.
theperformanceofthethreeanalyzeddefensemechanismsand
Theproposalandresultsdescribedinthispapermustnotbe
KDùëòagainstthedirectlabelinferenceattack.Theemployeddatasets
seenasthefinalconclusionofthisreasearch.Asamatteroffact,
are,onceagain,CIFAR-10,CIFAR-100(seeTable8),andCINIC-10
inthefuture,weplantofurtherdevelopourproposedKDùëò de-
(seeTable9).Aswecansee,thebehaviorofthedefenseswearecom-
fensemethodtoprovideenhancedprotectionforotherkindsof
paringwithissimilartotheonewitnessedforthepassiveandactive
FLandattacks,designingamorecompleteprotectionframework.
attacks.Indeed,especiallyforCIFAR-10andCINIC-10,thedefenses
Forinstance,weintendtofocusalsoonHorizontalFL.Duetothe
areeffectiveonlywhenthealterationissuchthattheimpacton
peculiaritiesofthisvariant,athoroughexaminationmustbecon-
themaintaskaccuracyisnotnegligible.Theonlycountermeasure
ductedtocomprehendhowourdefensemechanismcanbeadjusted
capableofmatchingoursolutionintermsofpreservationofthe
accordingtoit.
originalmodelaccuracyanddetrimentoftheattackperformance
istheNoisyGradientsdefense.LookingatCIFAR-100,instead,we
REFERENCES
canseehowalsothePPDLdefenseiscapableofachievinggood
[1] JeremyBernstein,Yu-XiangWang,KamyarAzizzadenesheli,andAnimashree
results.Ourdefensecomparedtotheothersisequallyeffectiveon
Anandkumar.2018.signSGD:Compressedoptimisationfornon-convexproblems.
thethreeconsidereddatasets.Inthiscase,though,amorepowerful InInternationalConferenceonMachineLearning.PMLR,Vienna,Austria,560‚Äì569.
settingisrequiredtocounterthemorepowerfulDirectAttack. [2] CristianBuciluaÀá,RichCaruana,andAlexandruNiculescu-Mizil.2006. Model
compression.InProceedingsofthe12thACMSIGKDDinternationalconference
Insummary,fromtheaboveexperimentswecanconcludethat, onKnowledgediscoveryanddatamining.AssociationforComputingMachinery
ourdefensestrategyistheonlyoneobtaininggoodperformance (ACM),Beijing,China,535‚Äì541.
[3] JangHyunChoandBharathHariharan.2019. Ontheefficacyofknowledge
againstallthedifferentattacksandforalltheanalyzeddatasets.
distillation.InProceedingsoftheIEEE/CVFinternationalconferenceoncomputer
Generally,mostoftheotherdefesesfailedprotectingagainstlabel vision.IEEE,Seoul,SouthKorea,4794‚Äì4802.
inferenceattacks.OnlythePPDLandNoisyGradientssucceeded [4] Criteo.2024.CriteoAILab.https://ailab.criteo.com/ressources.
[5] LukeN.Darlow,ElliotJ.Crowley,AntreasAntoniou,andAmosJ.Storkey.2018.
insomeoftheconsideredattackscenariosbut,asvisibleinour
CINIC-10isnotImageNetorCIFAR-10. arXiv:1810.03505[cs.CV]
results,theycannotbeusedasageneraldefensebecausetheydo [6] JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova.2018.Bert:
notprovideanadequateprotectionagainstallthepossibleattack Pre-trainingofdeepbidirectionaltransformersforlanguageunderstanding.
[7] KaiFan,JingtaoHong,WenjieLi,XingwenZhao,HuiLi,andYintangYang.2023.
settings. FLSG:ANovelDefenseStrategyAgainstInferenceAttacksinVerticalFederated
Learning.IEEEInternetofThingsJournal11(2023),1816‚Äì1826.
7 CONCLUSION [8] ChongFu,XuhongZhang,ShoulingJi,JinyinChen,JingzhengWu,Shanqing
Guo,JunZhou,AlexXLiu,andTingWang.2022.Labelinferenceattacksagainst
FederatedLearning(FL)isanovelparadigmaimingattrainingML verticalfederatedlearning.In31stUSENIXSecuritySymposium(USENIXSecurity
22).USENIXAssociation,Boston,MA,USA,1397‚Äì1414.
modelsinaprivacy-preservingandcollaborativeway.Differently
[9] JianpingGou,BaoshengYu,StephenJMaybank,andDachengTao.2021.Knowl-
fromHorizontalFL,inVerticalFL(VFL)participantssharethesame edgedistillation:Asurvey.InternationalJournalofComputerVision129(2021),
samplespace,buttheirlocalprivatedatadifferinthefeaturespace. 1789‚Äì1819.
[10] GeoffreyHinton,OriolVinyals,andJeffDean.2015.DistillingtheKnowledgein
Moreover,instandardVFL,thelabelsofthesamplesaresensitive
aNeuralNetwork. arXiv:1503.02531[stat.ML]
informationandshouldbeprotectedfromhonest-but-curiouspar-
ties.Hence,onlytheaggregatingserver(oractiveactor)knows
12[11] IvanKholod,AndreyRukavitsyn,AlexeyPaznikov,andSergeiGorlatch.2021. [21] FriedrichPukelsheim.1994.Thethreesigmarule.TheAmericanStatistician48,2
Parallelizationoftheself-organizedmapsalgorithmforfederatedlearningon (1994),88‚Äì91.
distributedsources.TheJournalofSupercomputing77(2021),6197‚Äì6213. [22] PierangelaSamaratiandLatanyaSweeney.1998.Protectingprivacywhendis-
[12] AlexKrizhevsky,GeoffreyHinton,etal.2009.Learningmultiplelayersoffeatures closinginformation:k-anonymityanditsenforcementthroughgeneralization
fromtinyimages.UniversityofToronto,Toronto,ON,Canada. andsuppression.
[13] OscarLi,JiankaiSun,XinYang,WeihaoGao,HongyiZhang,JunyuanXie,Vir- [23] JiankaiSun,XinYang,YuanshunYao,andChongWang.2022. LabelLeak-
giniaSmith,andChongWang.2022.LabelLeakageandProtectioninTwo-party ageandProtectionfromForwardEmbeddinginVerticalFederatedLearning.
SplitLearning. arXiv:2102.08504[cs.LG] arXiv:2203.01451[cs.LG]
[14] JunlinLiuandXinchenLyu.2022. Clusteringlabelinferenceattackagainst [24] SeanVucinichandQiangZhu.2023.TheCurrentStateandChallengesofFairness
practicalsplitlearning. arXiv:2203.05222[cs.LG] inFederatedLearning.IEEEAccess11(2023),80903‚Äì80914.
[15] YangLiu,TianyuanZou,YanKang,WenhanLiu,YuanqinHe,ZhihaoYi,and [25] KangWei,JunLi,ChuanMa,MingDing,ShaWei,FanWu,GuihaiChen,and
QiangYang.2022.BatchLabelInferenceandReplacementAttacksinBlack-Boxed ThilinaRanbaduge.2022.VerticalFederatedLearning:Challenges,Methodologies
VerticalFederatedLearning. arXiv:2112.05409[cs.LG] andExperiments. arXiv:2202.04309[cs.LG]
[16] XinjianLuo,YunchengWu,XiaokuiXiao,andBengChinOoi.2021. Feature [26] KarlWeiss,TaghiMKhoshgoftaar,andDingDingWang.2016. Asurveyof
inferenceattackonmodelpredictionsinverticalfederatedlearning.In2021IEEE transferlearning.JournalofBigdata3,1(2016),1‚Äì40.
37thInternationalConferenceonDataEngineering(ICDE).IEEE,Chania,Greece, [27] WenshengXia,YingLi,LanZhang,ZhonghaiWu,andXiaoyongYuan.2023.
181‚Äì192. CascadeVerticalFederatedLearningTowardsStragglerMitigationandLabel
[17] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and PrivacyoverDistributedLabels.IEEETransactionsonBigData1(2023),1‚Äì14.
BlaiseAguerayArcas.2017. Communication-efficientlearningofdeepnet- [28] ChenglinYang,LingxiXie,SiyuanQiao,andAlanYuille.2018. Knowledge
worksfromdecentralizeddata.InArtificialintelligenceandstatistics.PMLR,Ft. DistillationinGenerations:MoreTolerantTeachersEducateBetterStudents.
Lauderdale,FL,USA,1273‚Äì1282. arXiv:1805.05551[cs.CV]
[18] LucaMelis,CongzhengSong,EmilianoDeCristofaro,andVitalyShmatikov.2019. [29] QiangYang,YangLiu,TianjianChen,andYongxinTong.2019.Federatedmachine
Exploitingunintendedfeatureleakageincollaborativelearning.In2019IEEE learning:Conceptandapplications.ACMTransactionsonIntelligentSystemsand
symposiumonsecurityandprivacy(SP).IEEE,SanFrancisco,CA,USA,691‚Äì706. Technology(TIST)10,2(2019),1‚Äì19.
[19] MiladNasr,RezaShokri,andAmirHoumansadr.2018.Comprehensiveprivacy [30] ChenZhang,YuXie,HangBai,BinYu,WeihongLi,andYuanGao.2021. A
analysisofdeeplearning.InProceedingsofthe2019IEEESymposiumonSecurity surveyonfederatedlearning.Knowledge-BasedSystems216(2021),106775.
andPrivacy(SP).IEEE,SanFrancisco,CA,USA,1‚Äì15. [31] XiangZhang,JunboZhao,andYannLeCun.2015.Character-levelconvolutional
[20] FelixPetersen,HildeKuehne,ChristianBorgelt,andOliverDeussen.2022.Dif- networksfortextclassification.Advancesinneuralinformationprocessingsystems
ferentiabletop-kclassificationlearning.InInternationalConferenceonMachine 28(2015),649‚Äî-657.
Learning.PMLR,Baltimore,MD,17656‚Äì17668. [32] LigengZhu,ZhijianLiu,andSongHan.2019. Deepleakagefromgradients.
Advancesinneuralinformationprocessingsystems32(2019),14747‚Äì14756.
13