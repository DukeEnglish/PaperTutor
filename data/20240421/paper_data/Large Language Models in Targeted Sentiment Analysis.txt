Large Language Models in Targeted Sentiment Analysis for Russian
N. Rusnachenko,1,* A. Golubev,2,** and N. Loukachevitch2,3,***
(Submitted by V. V. Voevodin)
1NewcastleUponTyne,England,UnitedKingdom
2LomonosovMoscowStateUniversity
3ResearchComputingCenterLomonosovMoscowStateUniversity
ReceivedMarch1,2023
Abstract—In this paper we investigate the use of decoder-based generative transformers for ex-
tracting sentiment towards the named entities in Russian news articles. We study sentiment anal-
ysis capabilities of instruction-tuned large language models (LLMs). We consider the dataset of
RuSentNE-2023 in our study. The first group of experiments was aimed at the evaluation of zero-
shot capabilities of LLMs with closed and open transparencies. The second covers the fine-tuning
of Flan-T5 using the "chain-of-thought" (CoT) three-hop reasoning framework (THoR). We found
that the results of the zero-shot approaches are similar to the results achieved by baseline fine-tuned
encoder-based transformers (BERT ). Reasoning capabilities of the fine-tuned Flan-T5 models
base
with THoR achieve at least 5% increment with the base-size model compared to the results of the
zero-shot experiment. The best results of sentiment analysis on RuSentNE-2023 were achieved by
fine-tuned Flan-T5 , which surpassed the results of previous state-of-the-art transformer-based clas-
xl
sifiers. Our CoT application framework is publicly available: https://github.com/nicolay-r/
Reasoning-for-Sentiment-Analysis-Framework
2010MathematicalSubjectClassification:12345, 54321
Keywordsandphrases:SentimentAnalysis,LargeLanguageModels
1. INTRODUCTION
Inrecentyears,largelanguagemodels(LLMs)basedontheTransformerarchitecturehavesignificantly
changedthelandscapeofnaturallanguageprocessing(NLP).Suchmodelsarepre-trainedonlargevolumes
of unlabeled texts. The so-called instruction-tuned language models are further trained on large sets of
instructions(promptsandcorrectanswers). Thispre-trainingmadeitpossibletosolvetaskswithouttraining
(fine-tuning) models on target datasets in the so-called zero-shot or few-shot formats [1, 2]. The zero-shot
format is based solely on the formulation of a special prompt (question) for a model [1, 3]. The few-shot
formatcomprisesapromptandseveralexamplesofcorrectanswers[1].Inaddition,therearespecialprompts
thatcontaininstructionsforreasoning,referredtoasChain-of-Thought(CoT)prompts[4].
Insentimentanalysis,theapplicationofpre-trainedlanguagemodelshasledtoasignificantimprovement
in the performance in various tasks. Sentiment analysis tasks can be divided into two main categories:
generalsentimentanalysis [5],andtargetedsentimentanalysis(TSA)[6]. Generalsentimentanalysisaims
to determine the overall sentiment of a text, while targeted sentiment analysis focuses on identifying the
sentimenttowardsaspecificentity[7–9],itscharacteristics(aspects)[10],orcontroversialissues.
Large language models are primarily trained on English text collections and English datasets. Experi-
mentswiththemodelsarealsotypicallyconductedforEnglish. Whenappliedtootherlanguages,theresults
of largelanguage modelstend to beworse thanfor English. In this paper, we testseveral instruction-tuned
largelanguagemodelsonacomplicatedtaskoftargetedsentimentanalysisofRussiantexts. Weexperiment
with models of different transparency such as «closed models» ChatGPT series (GPT-3.5 and GPT-4) and
«open models» limited by 7 billion (7B) parameters [11–15]. Such "small" open models can be applied
* E-mail:rusnicolay@gmail.com
** E-mail:antongolubev5@yandex.ru
*** E-mail:louk_nat@mail.ru
4202
rpA
81
]LC.sc[
1v24321.4042:viXra2 NICOLAY RUSNACHENKO ET AL.
in NLP tasks with limited computing resources (1 NVidia A100 card), which is important for practical
applications. We use the dataset RuSentNE-2023 of Russian news texts annotated with sentiment towards
thementionednamedentities.
2. RELATED WORK
Evaluatinglanguagemodelsinsentimentanalysis,theauthorsof[16]comparetheperformanceofLLMs
suchasChatGPT(GPT-3.5andGPT-4),PaLM[17],Flan-UL2[18],andLLaMA[19]across13sentiment
analysis tasks on 26 datasets and compare the results against small language models (SLMs) trained on
domain-specific datasets. The tasks under study include document- and sentence-level sentiment analysis,
aspect-basedsentimentanalysis,andalsosuchtasksasimplicitsentiment,irony,hatespeechdetection,etc.
With such a diverse range of tasks and models, the authors created a standardized template for prompts,
containing the task name, definition, and desired output format. The authors conclude that the zero-shot
applicationofLLMsisalreadyeffectiveforsimplersentimentclassificationtasks,suchasbinaryandtrinary
classification. However, for tasks that require structured sentiment outputs, such as aspect-based analysis,
the performance of LLMs lags behind that of small models trained on specific domains: LMs often have a
loweraccuracythanfine-tunedones.
In [20] the authors evaluate large language models GPT-3.5, BLOOMZ, and XGLM in sentiment
classification in 34 languages, including 6 high/medium-resource languages, 25 low-resource languages,
and 3 code-switching datasets. When prompting LLMs, six variants of prompts are used, and the obtained
resultsareaveraged. Forhigh-resourcelanguages,GPT-3.5achieves77.5%byF-measure,forlow-resource
(African)languagesshows38.3%byF-measurein3-wayclassification.
In [21] authors illustrate the application of the CoT concept to extract implicit sentiments from users’
reviews [22, 23]. According to the provided paradigms, the proposed Three-Hop-Reasoning (THoR)
system could be treated as an emerged paradigm: task-agnostic schema of reasoning steps, with one-by-
one components referred to sentiment analysis. With these steps, authors aimed at extraction of «aspects»,
with further «opinion» as atomic components for devising the final answer [21]. The authors conclude
that the application of the fine-tuning process for instruction-tuned Flan-T5 [15] results in models that
surpass few-shot systems across publicly open systems [24] and significantly outperform encoder-based
classifiers [25]. When it comes to limitations, authors conclude their beliefs on unleashing the full LLMs
reasoningcapabilitiesbyapplyingTHoRtowardslargeenoughmodels[21].
For Russian, there are several directions of related investigations: aspect-based (SentiRuEval) [26]
and entity-oriented sentiment analysis (RuSentNE-2023) [9, 27]. The most recent advances in both tasks
show that the highest results are mainly achieved by fine-tuned encoder-based classification language
models[25,27–29]. ThebestresultsonRuSentNE-2023evaluationwereobtainedbyensemblesofBERT-
likeencodermodels[9].
Severalrecentworkexplorestheapplicationofgenerativemodelsinsentimentanalysistasks.Theauthors
of[30]studygenerativemodelsoftheGPTfamilyintheAspect-BasedTripletExtractionTask(ASTE).The
authorscomparethefew-shotstrategiesfortheGPT-3andChatGPT(GPT-3.5)modelswiththefine-tuned
Russian ruGPT-3 and ruGPT-3 models, based on the GPT-2 architecture [31]. They found that a
small large
few-shotapproachonruGPT-3 familymodelsdidnotproduceadequateresults: fine-tunedruGPT-3 models
showedasignificantimprovement.Theauthorsof[32]adopttheT5model[24]andcompareitwithencoder-
based approaches in the RuSentNE-2023 evaluation. Two variants of the Russian-adapted models ruT5
base
andruT5 wereused. However, thebestresultsobtainedbyruT5 are7percentagepointslowerthan
large large
thetopRuSentNE-2023submission[9].
3. RUSENTNE-2023 EVALUATION AND DATASET
The RuSentNE-2023 dataset1 is annotated with sentiment towards named entities in news texts. News
textsposechallengesfortargetedsentimentanalysis[9]. Atfirst,suchtextsmaycontainopinionsconveyed
bydifferentsubjects,includingtheauthor(s)’attitudes,positionsofcitedsources,andrelationsofmentioned
entities to each other. Secondly, some sentences contain several named entities with different sentiments,
complicating the determination of sentiment towards each individual named entity. Thirdly, the majority
of named entities in news texts are mentioned in neutral context, indicating a significant prevalence of the
neutral class. Last but not least, the significant amount of sentiment in news texts is implicit in nature,
primarilyconveyedthroughentityactions.
1Resourcesarepubliclyavailable:https://github.com/dialogue-evaluation/RuSentNE-evaluation
LOBACHEVSKIIJOURNALOFMATHEMATICSLARGE LANGUAGE MODELS IN TARGETED SENTIMENT ANALYSIS 3
ThesourceoftheannotatedsentimentinRuSentNE-2023couldbe:(i)anauthor,(ii)anothercitedsource,
and (iii) another entity mentioned in the text. Sentiment annotation is a three-scale and has the following
labels: positive,negative,andneutral.
For example, in the following sentence there is a negative sentiment to Hungary and neutral sentiment
to German Economy Minister. The source of the negative sentiment to Hungary is the German Economy
Minister:
МинистрэкономикиГерманиикритикуетВенгриюзаналогнаиностранныхинвесторов.
(GermanEconomyMinistercriticizesHungaryfortaxonforeigninvestors.)
Namedentitiesofthefollowingtypesrepresentobjectsofsentiment[33]:
• Person—physicalpersonregardedasanindividual,
• Organization—anorganizedgroupofpeopleorcompany,
• Country—anationorabodyoflandwithonegovernment,
• Profession—jobs,positionsinvariousorganizations,andprofessionaltitles,
• Nationality — nouns denoting country citizens and adjectives corresponding to nations in contexts
differentfromauthority-related.
The distribution of entity types in the training (train), development (dev), and test (test) sets of the
RuSentNE-2023datasetispresentedinTable1.
Table1. Distributionofentitytypesintraining(train),development(dev)andtest(test)setsoftheRuSentNE-2023dataset
train dev test
Entitytype # % # % # %
Person 1934 29 857 30 480 25
Profession 1666 25 533 24 510 26
Organization 1487 23 653 23 484 25
Country 1274 19 686 19 363 19
Nationality 276 4 116 4 110 5
Total 6637 100 2845 100 1947 100
4. EXPERIMENTAL SETUP
We experiment with the initial dataset RuSentNE-2023 and its English translation (RuSentNE-2023en).
Since most LLMs are trained on English data, we adopt GoogleTranslate2 to automatically translate and
composeRuSentNE-2023en. Toevaluatethepredictedresults,wefollowthecompetitionrules[9]andadopt
macroF1-measureranged[0,100]over:(1)positiveandnegativeclassesF1PN,(2)alltaskclassesF1PN0.
We investigate LLMs reasoning capabilities with the following modes: (i) zero-shot, and (ii) fine-
tuning. To conduct the experiment, our computational resources were limited by access to a single
NVIDIAA100GPU(40GB).ThemodeltrainingandinferencecodeareimplementedinPython-3.8.
4.1. LLMsZero-shotExperimentsSetup
Our setup covers a range of recently popular models that fall into two main categories: (i) «closed
models»,and(ii)«openmodels»[11–15].
Inthecaseofclosedmodels,thefollowingchat-baseddialogueassistantswereused:
• GPT-4,version1106preview;
• GPT-3.5,versions1106,0613.
FortheGPTmodels,weutilizedaso-calledsystemprompt,whichisaspecialmessageusedtoassignarole
to the assistant. System prompts prescribe the style and task for the chat-bot communication. We used the
followingsystemprompt:
2https://pypi.org/project/googletrans/
LOBACHEVSKIIJOURNALOFMATHEMATICS4 NICOLAY RUSNACHENKO ET AL.
SystemMessage:YouareanAIassistantskilledinnaturallanguageprocessingandsentimentanalysis.
Yourtaskistoanalyzetextinputstodeterminetheunderlyingsentiment,whetherit’spositive,negative,
or neutral. You should consider the nuances of language, including sarcasm, irony, and context. Your
responses should include not only the sentiment classification but also a brief explanation of why a
particularsentimentwasassigned,highlightingkeywordsorphrasesthatinfluencedthedecision.
In the case of open models, we experiment with instruction-tuned versions. The complete list of the
assessedmodelsispresentedinTable2andincludes:
• Mistral[11]–grouped-queryattention(GQA)[34]forfasterinference,coupledwithslidingwindow
attention [35] to effectively handle sequences of arbitrary length with a reduced inference cost. The
authors adopt byte-fallback BPE tokenization [36]. There are several versions of publicly available
instruction-tunedmodels:v0.1andv0.2. Thev0.1hastheinputcontextwindowsizeof8Ktokens3.
In v0.2 the related size has been increased up to 32K context4. For both versions, information on
trainingdataisnotpubliclyavailable.
• DeciLM [12] – is an auto-regressive language model using an optimized transformer decoder
architecture that includes variable GQA [34]. Information on fine-tuning and utilized datasets is
publicly available. Model has been fine-tuned for instruction with LoRA [37] on the SlimOrca [38]
dataset. Thecontextwindowsizeis8Ktokens.
• Microsoft-Phi-2[14]–trainedusingtheresourceadoptedinPhi-1.5[14];augmentedwithanewdata
source that consists of various NLP synthetic texts and filtered websites (for safety and educational
value). DatasetrepresentscombinationofNLPsyntheticdatacreatedbyAOAI5GPT-3.5andfiltered
webdatafromFalconRefinedWeb[39]andSlimPajama[40],assessedbyAOAIGPT-4. Thecontext
windowsizeofthemodelis2Ktokens. Formodeltraining,authorsutilize96xA100-80Gfor14days.
• Gemma [13] – Built from the same research and technology used to create the Gemini mod-
els [41]. Represent a text-to-text, decoder-only large language models, with architecture similar
to LLaMA [19]. Available in English, with open access to instruction-tuned variants. Represent
well-suited for a variety of text generation tasks, including question answering, summarization, and
reasoning. Thecontextwindowsizeis8Ktokens.
• Flan-T5 [15] – Represent a specialized variant of the T5 [42], initially proposed as the unified text-
to-text transformer. The concept of fine-tuning language models based on instructions (Flan) [15]
highlights the significant improvement across series of tasks, including: MMLU [43], BBH [44],
TyDiQA[45],MGSM[46],open-endedgeneration. Flan-T5trainedwiththeencodercontextlength
of1Ktokens.
Given the sentence (X) with the target entity mentioned in it (t), we utilize prompting techniques of two
types: originalversion(v1)[27]andpreciselyadaptedtherevisedversion,dubbedas(v2):
v1 : Какая оценка тональности в предложении X, по отношению
ru
к t? Выбери из трех вариантов: позитивная, негативная, нейтральная.
v1 : What’stheattitudeofthesentenceX tothetargett? Selectonefrom: positive,negative,neutral.
en
v2 : Каково отношение автора или другого субъекта в предложении
ru
X к t? Выбери из трех вариантов: позитивная, негативная, нейтральная
v2 : What is the attitude of the author or another subject in the sentence X to the target t? Choose
en
from: positive,negative,neutral.
3Technicallyisunlimited,withthresholdoriginatedbythe4Ksizeofslidingwindow[35]
4Ourassumptionthatmodelisnolongeradoptstheslidingwindowattention[35]
5https://github.com/microsoft/sample-app-aoai-chatGPT
LOBACHEVSKIIJOURNALOFMATHEMATICSLARGE LANGUAGE MODELS IN TARGETED SENTIMENT ANALYSIS 5
Table2. ListofLLMsutilizedinZero-shotexperiments,separatedon«closedmodels»withaccessviaOpenAIAPI(GPT)and
«openmodels»[11–15]whichsizedoesnotexceed7billionparameters
Models Versions Params Reference
GPT-4 1106-preview 1.76Ta gpt-4-1106-preview(OpenAIAPI)
GPT-3.5 1106 175Bb gpt-3.5-turbo-1106(OpenAIAPI)
0613 175Bb gpt-3.5-turbo-0613(OpenAIAPI)
Mistral[11] v0.1 7B https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1
v0.2 7B https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2
DeciLM:[12] 7B https://huggingface.co/Deci/DeciLM-7B-instruct
Microsoft- 2.7B https://huggingface.co/microsoft/phi-2
Phi-2[14]
Gemma:[13] Instructive 7Bc https://huggingface.co/google/gemma-7b-it
Instructive 2B https://huggingface.co/google/gemma-2b-it
Flan-T5:[15] XL 3B https://huggingface.co/google/flan-t5-xl
Large 750M https://huggingface.co/google/flan-t5-large
Base 250M https://huggingface.co/google/flan-t5-base
aNon-disclosuredbyOpenAI;accordingtotheothersources,GPT-4yieldsofeightmodels220B-sizedparameterseachconnected
byaMixtureofExperts(MoE)[48](≈1.76Tparams)
bNon-disclosedbyOpenAI,ourassumptionthatthearchitectureisreferredtoGPT-3,sizeof175Bparams[1]
cTheactualandnon-officialamountofhiddenparametersforGemma-7B-ITmodelis9B
Inthecaseofproprietarymodels,weadoptOpenAIAPIservicetoaccesstheOpenAImodels. Toreduce
the charging cost for the tokens, we limit the result output for GPT-3.5 and GPT-4 models by using the
response threshold of 75 tokens. We augment the initial prompt with the suffix that requires short answers
as follows: «Create a very short summary that uses 50 completion_tokens or less.» For LLMs that can
be downloaded for local use, in this paper, we experiment with models whose size does not exceed 7B
parameters. We utilized transformers API [47] to conduct experiments on rented servers. In all models
forzero-shotexperiments,thevalueofthetemperatureparameterwaschosenas0.1.
To assess the inferred textual responses, in this study we adopt a universal annotation answer mapping
strategy that involves the application of class-specific textual lower-cased templates for classes (positive,
negative, and neutral), separately declared for each language. We use these templates during a sequential
occurrence check in lower-cased output in the following order: (1) neutral, (2) positive, (3) negative. In
the case of absence of any class, the «UNK» placeholder (counted as «neutral») was considered. We
use «positive», «negative», «neutral» templates for RuSentNE-2023en, and «позитив» (positive),
«негатив» (negative), «нейтрал» (neutral) templates for experiments on RuSentNE-2023 dataset. We
performfinalchecksinvolvingregularexpressionstoguaranteethatthefinalanswersarenotprefixedwith
theinitialprompt.
4.2. LLMsFine-tuningSetup
To experiment with the fine-tuning, we adopt encoder-decoder style instruction-tuned Flan-T56 as our
backbonelargelanguagemodelfortheproposedmethodologyfortextswritteninEnglish. Inthispaperwe
experimentwiththefollowingfine-tuningtechniques:
• PROMPT–prompt-tuningwiththev1 versionoftheprompt;
en
• THoR – Three-Hop-Reasoning [21] technique, which can be considered as an emerged paradigm:
task-agnosticschemaofreasoningsteps,withone-by-onecomponentsreferredtosentimentanalysis.
Theexperimentswiththree-hopreasoningareresourceandtime-intensive,sowecurrentlystudythis
techniqueonlyforoneLLMsfamily.
Next,wecoverTHoRingreaterdetails. WithC ,i ∈ 1..3wedenotethepromptsthatwrapthecontentin
i
theinputcontext. Giventhesentence(X)withtargetentitymentionedinit(t),Figure1illustratestheinitial
applicationofthethree-stepapproach[21]forinferringthesentiments′. AccordingtoTable1,theresulta′
6
https://huggingface.co/docs/transformers/en/model_doc/flan-t5
LOBACHEVSKIIJOURNALOFMATHEMATICS6 NICOLAY RUSNACHENKO ET AL.
Figure1. Inferringsentiments‘usingCoTthree-hopreasoningframework(THoR),including«finallabelinferring»toanswer
oneofthetaskclasses[21]
couldbeinterpretasa′ = argmaxp(a|X,t),opiniono′aso′ = argmaxp(o|X,t,a′),andthefinalanswer
1
s′ notedas: s′ = argmaxp(e|X,t,s′,o′). Toguaranteethecorrectnessofthefinalanswer(s′),weusethe
followingpromptmessagetoinferringthesentimentlabell(Figure1,bottom).
Weexperimentwith:250M(base),750M(large),and3B(XL)versions.Weconductexperimentsonlyfor
English-translatedRuSentNE-2023endatasetduetothepre-trainingspecificsofFlan-T5model.Formapping
the Flan-T5 output towards task classes, we seek for the exact string from the set of textual task labels:
«positive», «negative», and «neutral». When it comes to fine-tuning parameters setup, we follow
the settings proposed by authors of THoR framework [21]. In particular, we use AdamW [49] optimizer
withlearningrate2·10−4 andbatch-sizeof32. Toinfertheanswers, themaximaltemperatureof1.0was
considered.
5. RESULTS AND DISCUSSION
Table3showsthezero-shotpromptingresultsonthetestsetofRuSentNE-2023acrossvariousLLMs,
separatelyfortheoriginalandtranslatedtexts(RuSentNE-2023en). TheTable3containstwomainmeasures
forevaluation(F1PN,F1PN0),accompaniedbyno-answerrate(N/A %).Ingeneral,allmodelswerecapable
ofhandlinginstructionsgeneratedusingv1 andv2 promptsforentity-orientedsentimentanalysisofinitial
andtranslatedversionsoftheRuSentNE-2023dataset.
It can be seen from Table 3 that such models that support Russian language are tend to perform worse
with texts from RuSentNE-2023 than for its translated variant RuSentNE-2023en. There are a few models
for which the proportion of N/A answers (Table 3) is significantly higher than for other models. The
%
Microsoft-Phi-2modelwithbothversionsofpromptsreturnsthepromptalongwithitsresponse. Sincethe
full response is limited by the max_token value, it is truncated, and we do not receive information about
sentimentclassificationfromthemodel. Theaveragelengthofaresponseforexamplesforwhichthemodel
didnotprovideanansweris465characters,fortherestitequals288. DeciLM-7BandGemma-2Bmodels
withv2promptconfigurationgenerategarbageanswersintheformofdatascrapsfromthepromptorclearly
indicate that they cannot determine the sentiment polarity («...so i cannot answer this question
from the provided context.»).
AccordingtoF1PN,inthecaseofproprietaryOpenAImodelsweseethesimilarperformanceorderofthe
models’resultsirrespectiveofthepromptslanguagesource. Inparticular,thehighestresultsonRuSentNE-
2023en wereachievedbyGPT-4(F1PN=54.53), followedbyGPT-3.5
turbo-0613
(F1PN=49.22)andGPT-
3.5
turbo-1106
(F1PN=47.87).7 SwitchingtotheoriginalRuSentNE-2023resultsin10%and7%decreasein
7WebelievethatthereasonoftheworsebehavioroftheGPT-3.5 againstthepreviousGPT-3.5 iscausedbyahigher
turbo-1106 turbo-0613
toleranceleveloftheresultsresponses.
LOBACHEVSKIIJOURNALOFMATHEMATICSLARGE LANGUAGE MODELS IN TARGETED SENTIMENT ANALYSIS 7
Table3. ResultsoftheLLMsapplicationinzero-shotmodeforthetestpartoftheRuSentNE-2023dataset,separatelyfor
originaltextsandautomaticallytranslatedinEnglish(RuSentNE-2023en);for«N/A %»columnvalues(no-answerrate),«·»
denotescaseswhentheamountofunknownanswersdoesnotexceed1%;topresultspereachversionofthedatasetarebolded
Model F1PN F1PN0 N/A
%
F1PN F1PN0 N/A
%
RuSentNE-2023en[TranslatedTextsintoEnglish]
PromptType v1 v2
en en
GPT-4 1106-preview 54.43 63.44 · 54.59 64.32 ·
GPT-3.5 turbo-0613 49.22 59.51 · 51.79 61.38 ·
GPT-3.5 turbo-1106 47.87 54.62 · 47.04 53.19 ·
Mistral-Instruct-7B v0.1 49.56 58.86 · 49.46 58.51 ·
Mistral-Instruct-7B v0.2 45.69 57.16 · 44.82 56.04 ·
DeciLM-7B 42.73 49.88 · 43.85 53.65 1.44
Microsoft-Phi-2 37.26 31.91 5.55 40.95 42.77 3.13
Gemma-7B-IT 40.58 45.94 · 40.96 44.63 ·
Gemma-2B-IT 18.70 39.51 · 31.75 45.96 2.62
Flan-T5 xl 35.35 31.51 · 48.14 57.33 ·
Flan-T5 large 34.86 23.34 · 36.05 24.27 ·
Flan-T5 base 32.64 21.81 · 31.05 20.84 ·
RuSentNE-2023[OriginaltextswritteninRussian]
PromptType v1 v2
ru ru
GPT-4 1106-preview 49.44 58.74 · 48.04 60.55 ·
GPT-3.5 turbo-0613 45.97 56.10 · 45.85 57.36 ·
GPT-3.5 turbo-1106 38.95 45.93 · 35.07 48.53 ·
Mistral-Instruct-7B v0.2 48.71 57.10 · 42.60 48.05 ·
resultbyF1PN forGPT-4andGPT-3.5 turbo-0613respectively. Inturn,theonlymultilingualMistral-Instruct-
7B
v0.2
illustratesaperformancecomparabletoGPT-3.5
turbo-0613
andoutperformsGPT-3.5
turbo-1106
by≈19-
25%. Also,wefoundMistral-Instruct-7B asmoresensitivetopromptsonoriginalRuSentNE-2023texts,
v0.2
ratherOpenAImodels(seelastrow,Table3).
Most open LLMs are able to follow English instructions. For RuSentNE-2023en, we see that the
best performing models are Mistral [11] models (45.69 ≤ F1PN ≤ 49.56), and DeciLM-7B [12] as the
closest competitor alternative (F1PN= 42.73). The remaining families of Microsoft-Phi-2 and Flan-T5
series demonstrate the gap in results (32.64 ≤ F1PN ≤ 37.26). Through the entire range of open models,
Mistral [11] is the only one capable of handling RuSentNE-2023 in Russian. The official release Mistral-
Instruct-7B wasbettersuitedfornon-Englishlanguages.
v0.2
From experiments with model fine-tuning, we found that training Flan-T5 for 2-3 epochs both for
PROMPT and THoR techniques on RuSentNE-2023en is sufficient. Figure 2 illustrates the evaluation
statistics of Flan-T5 models on dev dataset per each epoch of the training process (6 epochs in total),
separately per each training technique. We found that training for 2-4 epochs is sufficient to prevent the
model from overfitting. For the final evaluation on the test set, checkpoints with the highest results on
devsetwereconsidered. Table4illustratestheresultsofthefine-tuninginstruction-tunedFlan-T5models.
We first compare and discuss the obtained results in comparison with zero-shot approaches, followed by
comparisonofthedifferentfine-tuningtechniques.
Fine-tuning Flan-T5 on RuSentNE-2023en training data results in models that outperform all zero-shot
approaches. Since the Flan-T5 only supports texts written in English, we experiment with a RuSentNE-
2023en dataset. ComparingtheresultswiththoseinTable3,weseethatthefine-tunedversionsoftheFlan-
T5modelsofallsizessignificantlyoutperformedtheircorrespondingzero-shotcounterparts. Theborderline
for the Flan-T5 base version is F1PN= 59.75, which is +9.9% higher than the top result of GPT-4 applied
forRuSentNE-2023en (F1PN= 54.36,seeTable3).
Comparingtheresultsofdifferentfine-tuningtechniques,wefindthatusingTHoRresultsinmorestable
performanceacrossallmodelsizes. Theexceptionalcaseofthexlsizedmodel,withwhichweseesimilar
resultsondevbetweendifferentfine-tuningtechniqueswithhighergapontestset.Accordingtothefindings
in[4],theobtainedresultsillustratethealignmentoftheideasthatwereinvestigatedinTHoRapplicationfor
implicitsentimentanalysis[23]. Analyzingresultsontestpart,thefinetunedmodelwithTHoRtechnique
LOBACHEVSKIIJOURNALOFMATHEMATICS8 NICOLAY RUSNACHENKO ET AL.
70 70
60 60
Flan-T5 base Flan-T5 base
50 Flan-T5 large 50 Flan-T5 large
Flan-T5 Flan-T5
xl xl
40 40
1 2 3 4 5 6 1 2 3 4 5 6
Flan-T5 fine-tuning with prompt technique Flan-T5 fine-tuning with THoR technique
Figure2. AnalysisoftheFlan-T5modelsresultsonRuSentNE-2023endevpereachepoch(horizontalaxis)byF (PN)(vertical
1
axis)duringfine-tuningwithPROMPT(left)andTHoRtechnique(right)perdifferentsizes
on shows +4.2% increment by F1PN once switching from base-size to large-sized, and extra +4.4% by
F1PN with the XL-sized over large. The highest achieved result is F1PN= 68.20 which outperforms the
bestRuSentNE-2023resultsbasedonthetransformerencoderensemble[50].
Table4. ResultsoftheFlan-T5fine-tuningwith(i)PROMPTand(ii)THoRtechniquesforRuSentNE-2023enandresultsofthe
THoRinzero-shotmodeforthecomparison;topresultspereachcolumnarebolded
RuSentNE-2023en
Model Technique
dev test
F1PN F1PN0 F1PN F1PN0
Flan-T5fine-tuningresults
Flan-T5 THoR 68.02 74.82 65.09 72.45
xl
Flan-T5 xl PROMPT(v1 en) 68.62 75.69 68.20 75.29
Flan-T5 THoR 67.31 74.67 62.29 70.70
large
Flan-T5 PROMPT(v1 ) 65.83 73.71 60.80 69.79
large en
Flan-T5 THoR 62.72 70.70 59.75 68.02
base
Flan-T5 PROMPT(v1 ) 62.40 70.68 57.01 66.89
base en
Zero-shotresults
GPT-4 a THoR – – 50.13 55.93
1106-preview
GPT-3.5 THoR 43.41 46.14 44.50 48.17
turbo-0613
GPT-3.5 THoR 40.85 40.04 42.58 42.18
turbo-1106
Flan-T5 THoR 38.30 32.12 38.58 33.55
xl
Flan-T5 THoR 34.66 23.10 34.69 23.13
large
Flan-T5 THoR 33.93 22.79 33.88 23.00
base
BestRuSentNE-2023[50] Ensembleofencoders 70.94 77.63 66.67 74.11
aDuetothehighGPT-4 1106-previewAPIcost,resultswereinferredforthetestpartonly.
6. ERROR ANALYSIS
For the error analysis, we selected examples from the RuSentNE-2023 test set where most models’
predictions did not align with human annotations. The following main types of discrepancies between
models’predictionsandmanualannotationsarefound(denotedas«E1»,«E2»,and«E3»):
E1. Asentencementionsthepositiveauthor’sattitudetothetargetpersonandsomenegativeeventwiththis
person(trauma,death). Theannotatorstreatsuchexamplesaspositiveforthepersonbecauseinmostcases
traumas or death do no influence on existing positive attitude. Models in zero-shot mode answer on such
examples by choosing the negative sentiment to the target person due to “negative effect” context. In turn,
fine-tunedmodelsannotatemostofsuchexamplescorrectly. Forinstance,inthefollowingexamplewecan
seethattheauthorhasapositiveattitudetowardsChuckBerrydespitetheincidentdescribed.
LOBACHEVSKIIJOURNALOFMATHEMATICSLARGE LANGUAGE MODELS IN TARGETED SENTIMENT ANALYSIS 9
ЛегендарныйЧакБеррипотерялсознаниенаконцерте.
(LegendarymusicianChuckBerryfaintedduringaconcertinChicago.)
E2.Asentencementionsseveralentities,whilethenegativesentimentisdirectedonlyatoneoftheseentities.
Modelscannotdistinguishthecorrectobjectofthisnegativeattitude. Forexample:
Юлияжевсвоюочередьобвиняетбывшегосупругавтом,чтоонневыполняетрешениесуда,иуже
объявленвфедеральныйрозыск.
(Yulia,inturn,accusesherex-husbandofnotcomplyingwiththecourtdecisionandhasalreadybeenput
onthefederalwantedlist.)
Inthiscase,themodelspredictanegativeattitudetowardsYulia,butinfactYuliahasanegativeopinionof
herhusband.
E3. SimilartotheE2. Asentencewithevidentnegativesentimentmentionsasingleentity,butsentimentis
directedtosomeout-of-sentenceentity. Almostallmodelsinzero-shotmodepredictanegativesentimentto
thementionedentity. Inthefollowingexample,themodelsinzero-shotmodeanswerbychoosingnegative
sentimenttoAmerica,butinfactthenegativeopinionistowardsthe"situation".
Ситуация,однако,неможетнаснебеспокоить–объемэкспортавАмерикуупална8%.
(Thesituation,however,cannotbutworryus–thevolumeofexportstoAmericafellby8%.)
7. CONCLUSION
In this paper, we investigated the application of large language models (LLM) in targeted sentiment
analysiswithinnewstexts. WefollowtheRuSentNE-2023competitionbothforexperimentswithLLMand
evaluation. TheRuSentNE-2023evaluationwasaimedatextractingsentimenttowardstheobjectsannotated
in Russian-language sentences. In experiments, we used the RuSentNE-2023 dataset as well as its English
automaticallytranslatedversion(RuSentNE-2023en).
LLM-basedsentimentanalysiswasinvestigatedinseveraldirections,whichcovertheinfluenceofaspects
such as: (i) the language of the source texts (Russian or English), (ii) variants of prompts, and (iii) the
effect of fine-tuning, including the application of the Chain-of-Thought technique. We have discovered
thesignificanceofthecontenttranslation,sincemostLLMmodelsdemonstratereasonablecomprehension
capabilities primarily in English. Zero-shot approaches achieve results comparable to fine-tuned BERT-
basedRuSentNE-2023competitionbaselines. ExperimentswithLLMfine-tuning(Flan-T5)haveshownthe
improvement in ≈ 10% performance by F (PN) for all considered Flan-T5 models. The highest results
1
wereobtainedbyfine-tunedxl-sizedFlan-T5
xl
model(3B+parameters),whicharecurrentlythebestresults
achievedontheRuSentNE-2023testset.
In further, we aim to continue experimenting with the Chain-of-Thought in the following directions:
(i)reasoningrevisiontechniquesusingextensiveresourcesofauxiliaryinformation,(ii)parameter-efficient
tuningforlargermodels.
Acknowledgments
TheworkissupportedbytheRussianScienceFoundation(grantNo. 21-71-30003).
REFERENCES
1. T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry,
A.Askell,etal.,LanguageModelsareFew-ShotLearners,AdvancesinNeuralInformationProcessingSystems
(2020).
2. J.Wei,M.Bosma,V.Y.Zhao,K.Guu,A.W.Yu,B.Lester,N.Du,A.M.Dai,andQ.V.Le,Finetunedlanguage
modelsarezero-shotlearners,arXivpreprintarXiv:2109.01652(2021).
3. B.Zhang,D.Ding,andL.Jing, HowwouldStanceDetectionTechniquesEvolveaftertheLaunchofChatGPT?
(2023),arXiv:2212.14548.
4. J.Wei,X.Wang,D.Schuurmans,M.Bosma,F.Xia,E.Chi,Q.V.Le,D.Zhou,etal.,Chain-of-thoughtprompting
elicitsreasoninginlargelanguagemodels,AdvancesinNeuralInformationProcessingSystems(2022).
LOBACHEVSKIIJOURNALOFMATHEMATICS10 NICOLAY RUSNACHENKO ET AL.
5. P. Turney, Thumbs Up or Thumbs Down? Semantic Orientation Applied to Unsupervised Classification of
Reviews,Proceedingsofthe40thAnnualMeetingoftheAssociationforComputationalLinguistics(2002),URL
https://aclanthology.org/P02-1053.
6. O. Toledo-Ronen, M. Orbach, Y. Katz, and N. Slonim, Multi-Domain Targeted Sentiment Analysis, Annual
ConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics(2022).
7. E.Amigó,J.CarrillodeAlbornoz,I.Chugur,A.Corujo,J.Gonzalo,T.Martín,E.Meij,M.DeRijke,andD.Spina,
Overviewofreplab2013:Evaluatingonlinereputationmonitoringsystems,Internationalconferenceofthecross-
languageevaluationforumforeuropeanlanguages(2013).
8. N. Loukachevitch and Y. Rubtsova, Entity-oriented sentiment analysis of tweets: results and problems, Text,
Speech,andDialogue:18thInternationalConference,TSD2015,Pilsen,CzechRepublic,September14-17,2015,
Proceedings18(2015).
9. A. Golubev, N. Rusnachenko, and N. Loukachevitch, RuSentNE-2023: Evaluating Entity-Oriented Sentiment
AnalysisonRussianNewsTexts,ComputationalLinguisticsandIntellectualTechnologies:papersfromtheAnnual
conferenceDialogue(arxiv:2305.17679(2023).
10. M. Pontiki, D. Galanis, H. Papageorgiou, I. Androutsopoulos, S. Manandhar, M. AL-Smadi, M. Al-Ayyoub,
Y. Zhao, B. Qin, O. De Clercq, et al., Semeval-2016 task 5: Aspect based sentiment analysis, ProWorkshop on
SemanticEvaluation(SemEval-2016)(2016).
11. A.Q.Jiang, A.Sablayrolles, A.Mensch, C.Bamford, D.S.Chaplot, D.delasCasas, F.Bressand, G.Lengyel,
G.Lample,L.Saulnier,etal.,Mistral7B(2023),arXiv:2310.06825.
12. D.R.Team,DeciLM-7B(2023),URLhttps://huggingface.co/Deci/DeciLM-7B.
13. T. W. Jeanine Banks, Gemma: Introducing new state-of-the-art open models (2023), URL https://blog.
google/technology/developers/gemma-open-models/.
14. Y. Li, S. Bubeck, R. Eldan, A. D. Giorno, S. Gunasekar, and Y. T. Lee, Textbooks Are All You Need II: phi-1.5
technicalreport(2023),arXiv:2309.05463.
15. H.W.Chung,L.Hou,S.Longpre,B.Zoph,Y.Tay,W.Fedus,Y.Li,X.Wang,M.Dehghani,S.Brahma,etal.,
Scalinginstruction-finetunedlanguagemodels,arXivpreprintarXiv:2210.11416(2022).
16. SentimentAnalysisintheEraofLargeLanguageModels:ARealityCheck,author=WenxuanZhangandYueDeng
andBingLiuandSinnoJialinPanandLidongBing(2023),arXiv:2305.15005.
17. A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton,
S.Gehrmann,etal.,J.Mach.Learn.Res.24(2024),ISSN1532-4435.
18. Y. Tay, M. Dehghani, V. Q. Tran, X. Garcia, J. Wei, X. Wang, H. W. Chung, S. Shakeri, D. Bahri, T. Schuster,
etal.,Ul2:Unifyinglanguagelearningparadigms(2023),2205.05131.
19. H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro,
F.Azhar,etal.,LLaMa:OpenandEfficientFoundationLanguageModels(2023),arXiv:2302.13971.
20. F.Koto,T.Beck,Z.Talat,I.Gurevych,andT.Baldwin,Zero-shotSentimentAnalysisinLow-ResourceLanguages
UsingaMultilingualSentimentLexicon(2024),arXiv:2402.02113.
21. F.Hao,L.Bobo,L.Qian,B.Lidong,L.Fei,andC.Tat-Seng,ReasoningImplicitSentimentwithChain-of-Thought
Prompting,ProceedingsoftheAnnualMeetingoftheAssociationforComputationalLinguistics(2023).
22. M.Pontiki,D.Galanis,J.Pavlopoulos,H.Papageorgiou,I.Androutsopoulos,andS.Manandhar,SemEval-2014
Task4:AspectBasedSentimentAnalysis,Proceedingsofthe8thInternationalWorkshoponSemanticEvaluation
(SemEval2014)(2014),URLhttps://aclanthology.org/S14-2004.
23. Z.Li, Y.Zou, C.Zhang, Q.Zhang, andZ.Wei, Learningimplicitsentimentinaspect-basedsentimentanalysis
withsupervisedcontrastivepre-training(2021).
24. C.Raffel,N.Shazeer,A.Roberts,K.Lee,S.Narang,M.Matena,Y.Zhou,W.Li,andP.J.Liu,Exploringthelimits
oftransferlearningwithaunifiedtext-to-texttransformer,TheJournalofMachineLearningResearch(2020).
25. J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, BERT: Pre-training of deep bidirectional transformers for
languageunderstanding,Proceedingsofthe2019ConferenceoftheNorthAmericanChapteroftheAssociation
forComputationalLinguistics:HumanLanguageTechnologies,Volume1(LongandShortPapers)"(2019),URL
https://aclanthology.org/N19-1423.
26. N.Loukachevitch,P.Blinov,E.Kotelnikov,Y.Rubtsova,V.Ivanov,andE.Tutubalina,SentiRuEval:testingobject-
orientedsentimentanalysissystemsinRussian,ProceedingsofInternationalConferenceDialog(2015).
27. A. Golubev and N. Loukachevitch, Improving results on Russian sentiment datasets, Conference on artificial
intelligenceandnaturallanguage(2020).
28. Y.Liu,M.Ott,N.Goyal,J.Du,M.Joshi,D.Chen,O.Levy,M.Lewis,L.Zettlemoyer,andV.Stoyanov,RoBERTa:
ARobustlyOptimizedBERTPretrainingApproach(2019),arXiv:1907.11692.
29. S. Smetanin and M. Komarov, Deep transfer learning baselines for sentiment analysis in Russian, Information
Processing&Management(2021).
30. S.Chumakov,A.Kovantsev,andA.Surikov,GenerativeapproachtoAspectBasedSentimentAnalysiswithGPT
LanguageModels,ProcediaComputerScience(2023).
31. A.Radford,J.Wu,R.Child,D.Luan,D.Amodei,andI.Sutskever(2019).
32. I.Moloshnikov,M.Skorokhodov,A.Naumov,R.Rybka,andA.Sboev,NamedEntity-OrientedSentimentAnalysis
withtext2textGenerationApproach,ProceedingsoftheInternationalConference“Dialogue(2023).
LOBACHEVSKIIJOURNALOFMATHEMATICSLARGE LANGUAGE MODELS IN TARGETED SENTIMENT ANALYSIS 11
33. N. Loukachevitch, E. Artemova, T. Batura, P. Braslavski, V. Ivanov, S. Manandhar, A. Pugachev, I. Rozhkov,
A.Shelmanov,E.Tutubalina,etal.,Nerel:arussianinformationextractiondatasetwithrichannotationfornested
entities,relations,andwikidataentitylinks(2023).
34. J. Ainslie, J. Lee-Thorp, M. de Jong, Y. Zemlyanskiy, F. Lebron, and S. Sanghai, GQA: Training Generalized
Multi-QueryTransformerModelsfromMulti-HeadCheckpoints,Proceedingsofthe2023ConferenceonEmpirical
MethodsinNaturalLanguageProcessing(2023).
35. I.Beltagy,M.E.Peters,andA.Cohan,Longformer:TheLong-DocumentTransformer(2020),arXiv:2004.05150.
36. R.Sennrich, B.Haddow, andA.Birch, NeuralMachineTranslationofRareWordswithSubwordUnits(2016),
arXiv:1508.07909.
37. E.J.Hu,P.Wallis,Z.Allen-Zhu,Y.Li,S.Wang,L.Wang,W.Chen,etal.,Lora: Low-rankadaptationoflarge
languagemodels(2021).
38. W. Lian, G. Wang, B. Goodson, E. Pentland, A. Cook, C. Vong, and "Teknium", SlimOrca: An Open Dataset
ofGPT-4AugmentedFLANReasoningTraces,withVerification,HuggingFace(2023),URLhttps://https:
//huggingface.co/Open-Orca/SlimOrca.
39. G. Penedo, Q. Malartic, D. Hesslow, R. Cojocaru, A. Cappelli, H. Alobeidli, B. Pannier, E. Almazrouei, and
J.Launay,TheRefinedWebdatasetforFalconLLM:outperformingcuratedcorporawithwebdata,andwebdata
only,arXivpreprintarXiv:2306.01116(2023),URLhttps://arxiv.org/abs/2306.01116.
40. D. Soboleva, F. Al-Khateeb, R. Myers, J. R. Steeves, J. Hestness, and N. Dey, SlimPajama: A
627B token cleaned and deduplicated version of RedPajama, https://www.cerebras.net/blog/
slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama (2023), URL
https://huggingface.co/datasets/cerebras/SlimPajama-627B.
41. G.Team,R.Anil,S.Borgeaud,Y.Wu,J.-B.Alayrac,J.Yu,R.Soricut,J.Schalkwyk,A.M.Dai,A.Hauth,etal.,
Gemini:AFamilyofHighlyCapableMultimodalModels(2023),arXiv:2312.11805.
42. C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu, Exploring the
limitsoftransferlearningwithaunifiedtext-to-texttransformer,J.Mach.Learn.Res.(2020).
43. D.Hendrycks,C.Burns,S.Basart,A.Zou,M.Mazeika,D.Song,andJ.Steinhardt,MeasuringMassiveMultitask
LanguageUnderstanding(2021),arXiv:2009.03300.
44. M. Suzgun, N. Scales, N. Schärli, S. Gehrmann, Y. Tay, H. W. Chung, A. Chowdhery, Q. Le, E. Chi, D. Zhou,
etal.,ChallengingBIG-BenchTasksandWhetherChain-of-ThoughtCanSolveThem,FindingsoftheAssociation
forComputationalLinguistics:ACL2023(2023).
45. J.H.Clark,E.Choi,M.Collins,D.Garrette,T.Kwiatkowski,V.Nikolaev,andJ.Palomaki,TyDiQA:ABenchmark
forInformation-SeekingQuestionAnsweringinTypologicallyDiverseLanguages,TransactionsoftheAssociation
forComputationalLinguistics(2020).
46. F.Shi,M.Suzgun,M.Freitag,X.Wang,S.Srivats,S.Vosoughi,H.W.Chung,Y.Tay,S.Ruder,D.Zhou,etal.,
LanguageModelsareMultilingualChain-of-ThoughtReasoners(2022),arXiv:2210.03057.
47. T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz,
et al., in Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System
Demonstrations (Association for Computational Linguistics, Online, 2020), pp. 38–45, URL https://www.
aclweb.org/anthology/2020.emnlp-demos.6.
48. M.Schreiner,Gpt-4architecture,datasets,costsandmoreleaked,THEDECODER(2023).
49. I.LoshchilovandF.Hutter,Decoupledweightdecayregularization(2018).
50. P.Podberezko,A.Kaznacheev,S.Abdullayeva,andA.Kabaev,HAlf-MAskedModelforNamedEntitySentiment
analysis,ProceedingsoftheInternationalConferenceDialogue(2023).
LOBACHEVSKIIJOURNALOFMATHEMATICS