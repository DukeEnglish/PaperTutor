[
    {
        "title": "Generalized Naive Bayes",
        "authors": "Edith Alice KovácsAnna OrszágDániel PfeiferAndrás Benczúr",
        "links": "http://arxiv.org/abs/2408.15923v1",
        "entry_id": "http://arxiv.org/abs/2408.15923v1",
        "pdf_url": "http://arxiv.org/pdf/2408.15923v1",
        "summary": "In this paper we introduce the so-called Generalized Naive Bayes structure as\nan extension of the Naive Bayes structure. We give a new greedy algorithm that\nfinds a good fitting Generalized Naive Bayes (GNB) probability distribution. We\nprove that this fits the data at least as well as the probability distribution\ndetermined by the classical Naive Bayes (NB). Then, under a not very\nrestrictive condition, we give a second algorithm for which we can prove that\nit finds the optimal GNB probability distribution, i.e. best fitting structure\nin the sense of KL divergence. Both algorithms are constructed to maximize the\ninformation content and aim to minimize redundancy. Based on these algorithms,\nnew methods for feature selection are introduced. We discuss the similarities\nand differences to other related algorithms in terms of structure, methodology,\nand complexity. Experimental results show, that the algorithms introduced\noutperform the related algorithms in many cases.",
        "updated": "2024-08-28 16:36:18 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.15923v1"
    },
    {
        "title": "Implicit Regularization Paths of Weighted Neural Representations",
        "authors": "Jin-Hong DuPratik Patil",
        "links": "http://arxiv.org/abs/2408.15784v1",
        "entry_id": "http://arxiv.org/abs/2408.15784v1",
        "pdf_url": "http://arxiv.org/pdf/2408.15784v1",
        "summary": "We study the implicit regularization effects induced by (observation)\nweighting of pretrained features. For weight and feature matrices of bounded\noperator norms that are infinitesimally free with respect to (normalized) trace\nfunctionals, we derive equivalence paths connecting different weighting\nmatrices and ridge regularization levels. Specifically, we show that ridge\nestimators trained on weighted features along the same path are asymptotically\nequivalent when evaluated against test vectors of bounded norms. These paths\ncan be interpreted as matching the effective degrees of freedom of ridge\nestimators fitted with weighted features. For the special case of subsampling\nwithout replacement, our results apply to independently sampled random features\nand kernel features and confirm recent conjectures (Conjectures 7 and 8) of the\nauthors on the existence of such paths in Patil et al. We also present an\nadditive risk decomposition for ensembles of weighted estimators and show that\nthe risks are equivalent along the paths when the ensemble size goes to\ninfinity. As a practical consequence of the path equivalences, we develop an\nefficient cross-validation method for tuning and apply it to subsampled\npretrained representations across several models (e.g., ResNet-50) and datasets\n(e.g., CIFAR-100).",
        "updated": "2024-08-28 13:26:36 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.15784v1"
    },
    {
        "title": "Remove Symmetries to Control Model Expressivity",
        "authors": "Liu ZiyinYizhou XuIsaac Chuang",
        "links": "http://arxiv.org/abs/2408.15495v1",
        "entry_id": "http://arxiv.org/abs/2408.15495v1",
        "pdf_url": "http://arxiv.org/pdf/2408.15495v1",
        "summary": "When symmetry is present in the loss function, the model is likely to be\ntrapped in a low-capacity state that is sometimes known as a \"collapse.\" Being\ntrapped in these low-capacity states can be a major obstacle to training across\nmany scenarios where deep learning technology is applied. We first prove two\nconcrete mechanisms through which symmetries lead to reduced capacities and\nignored features during training. We then propose a simple and theoretically\njustified algorithm, syre, to remove almost all symmetry-induced low-capacity\nstates in neural networks. The proposed method is shown to improve the training\nof neural networks in scenarios when this type of entrapment is especially a\nconcern. A remarkable merit of the proposed method is that it is model-agnostic\nand does not require any knowledge of the symmetry.",
        "updated": "2024-08-28 02:45:41 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.15495v1"
    },
    {
        "title": "PersonalizedUS: Interpretable Breast Cancer Risk Assessment with Local Coverage Uncertainty Quantification",
        "authors": "Alek FröhlichThiago RamosGustavo CabelloIsabela BuzattoRafael IzbickiDaniel Tiezzi",
        "links": "http://arxiv.org/abs/2408.15458v1",
        "entry_id": "http://arxiv.org/abs/2408.15458v1",
        "pdf_url": "http://arxiv.org/pdf/2408.15458v1",
        "summary": "Correctly assessing the malignancy of breast lesions identified during\nultrasound examinations is crucial for effective clinical decision-making.\nHowever, the current \"golden standard\" relies on manual BI-RADS scoring by\nclinicians, often leading to unnecessary biopsies and a significant mental\nhealth burden on patients and their families. In this paper, we introduce\nPersonalizedUS, an interpretable machine learning system that leverages recent\nadvances in conformal prediction to provide precise and personalized risk\nestimates with local coverage guarantees and sensitivity, specificity, and\npredictive values above 0.9 across various threshold levels. In particular, we\nidentify meaningful lesion subgroups where distribution-free, model-agnostic\nconditional coverage holds, with approximately 90% of our prediction sets\ncontaining only the ground truth in most lesion subgroups, thus explicitly\ncharacterizing for which patients the model is most suitably applied. Moreover,\nwe make available a curated tabular dataset of 1936 biopsied breast lesions\nfrom a recent observational multicenter study and benchmark the performance of\nseveral state-of-the-art learning algorithms. We also report a successful case\nstudy of the deployed system in the same multicenter context. Concrete clinical\nbenefits include up to a 65% reduction in requested biopsies among BI-RADS 4a\nand 4b lesions, with minimal to no missed cancer cases.",
        "updated": "2024-08-28 00:47:55 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.15458v1"
    },
    {
        "title": "Optimal level set estimation for non-parametric tournament and crowdsourcing problems",
        "authors": "Maximilian GrafAlexandra CarpentierNicolas Verzelen",
        "links": "http://arxiv.org/abs/2408.15356v1",
        "entry_id": "http://arxiv.org/abs/2408.15356v1",
        "pdf_url": "http://arxiv.org/pdf/2408.15356v1",
        "summary": "Motivated by crowdsourcing, we consider a problem where we partially observe\nthe correctness of the answers of $n$ experts on $d$ questions. In this paper,\nwe assume that both the experts and the questions can be ordered, namely that\nthe matrix $M$ containing the probability that expert $i$ answers correctly to\nquestion $j$ is bi-isotonic up to a permutation of it rows and columns. When\n$n=d$, this also encompasses the strongly stochastic transitive (SST) model\nfrom the tournament literature. Here, we focus on the relevant problem of\ndeciphering small entries of $M$ from large entries of $M$, which is key in\ncrowdsourcing for efficient allocation of workers to questions. More precisely,\nwe aim at recovering a (or several) level set $p$ of the matrix up to a\nprecision $h$, namely recovering resp. the sets of positions $(i,j)$ in $M$\nsuch that $M_{ij}>p+h$ and $M_{i,j}<p-h$. We consider, as a loss measure, the\nnumber of misclassified entries. As our main result, we construct an efficient\npolynomial-time algorithm that turns out to be minimax optimal for this\nclassification problem. This heavily contrasts with existing literature in the\nSST model where, for the stronger reconstruction loss,\nstatistical-computational gaps have been conjectured. More generally, this\nshades light on the nature of statistical-computational gaps for permutations\nmodels.",
        "updated": "2024-08-27 18:28:31 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.15356v1"
    }
]