[
    {
        "title": "Eagle: Exploring The Design Space for Multimodal LLMs with Mixture of Encoders",
        "authors": "Min ShiFuxiao LiuShihao WangShijia LiaoSubhashree RadhakrishnanDe-An HuangHongxu YinKaran SapraYaser YacoobHumphrey ShiBryan CatanzaroAndrew TaoJan KautzZhiding YuGuilin Liu",
        "links": "http://arxiv.org/abs/2408.15998v1",
        "entry_id": "http://arxiv.org/abs/2408.15998v1",
        "pdf_url": "http://arxiv.org/pdf/2408.15998v1",
        "summary": "The ability to accurately interpret complex visual information is a crucial\ntopic of multimodal large language models (MLLMs). Recent work indicates that\nenhanced visual perception significantly reduces hallucinations and improves\nperformance on resolution-sensitive tasks, such as optical character\nrecognition and document analysis. A number of recent MLLMs achieve this goal\nusing a mixture of vision encoders. Despite their success, there is a lack of\nsystematic comparisons and detailed ablation studies addressing critical\naspects, such as expert selection and the integration of multiple vision\nexperts. This study provides an extensive exploration of the design space for\nMLLMs using a mixture of vision encoders and resolutions. Our findings reveal\nseveral underlying principles common to various existing strategies, leading to\na streamlined yet effective design approach. We discover that simply\nconcatenating visual tokens from a set of complementary vision encoders is as\neffective as more complex mixing architectures or strategies. We additionally\nintroduce Pre-Alignment to bridge the gap between vision-focused encoders and\nlanguage tokens, enhancing model coherence. The resulting family of MLLMs,\nEagle, surpasses other leading open-source models on major MLLM benchmarks.\nModels and code: https://github.com/NVlabs/Eagle",
        "updated": "2024-08-28 17:59:31 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.15998v1"
    },
    {
        "title": "Mamba or Transformer for Time Series Forecasting? Mixture of Universals (MoU) Is All You Need",
        "authors": "Sijia PengYun XiongYangyong ZhuZhiqiang Shen",
        "links": "http://arxiv.org/abs/2408.15997v1",
        "entry_id": "http://arxiv.org/abs/2408.15997v1",
        "pdf_url": "http://arxiv.org/pdf/2408.15997v1",
        "summary": "Time series forecasting requires balancing short-term and long-term\ndependencies for accurate predictions. Existing methods mainly focus on\nlong-term dependency modeling, neglecting the complexities of short-term\ndynamics, which may hinder performance. Transformers are superior in modeling\nlong-term dependencies but are criticized for their quadratic computational\ncost. Mamba provides a near-linear alternative but is reported less effective\nin time series longterm forecasting due to potential information loss. Current\narchitectures fall short in offering both high efficiency and strong\nperformance for long-term dependency modeling. To address these challenges, we\nintroduce Mixture of Universals (MoU), a versatile model to capture both\nshort-term and long-term dependencies for enhancing performance in time series\nforecasting. MoU is composed of two novel designs: Mixture of Feature\nExtractors (MoF), an adaptive method designed to improve time series patch\nrepresentations for short-term dependency, and Mixture of Architectures (MoA),\nwhich hierarchically integrates Mamba, FeedForward, Convolution, and\nSelf-Attention architectures in a specialized order to model long-term\ndependency from a hybrid perspective. The proposed approach achieves\nstate-of-the-art performance while maintaining relatively low computational\ncosts. Extensive experiments on seven real-world datasets demonstrate the\nsuperiority of MoU. Code is available at https://github.com/lunaaa95/mou/.",
        "updated": "2024-08-28 17:59:27 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.15997v1"
    },
    {
        "title": "Spatio-Temporal Context Prompting for Zero-Shot Action Detection",
        "authors": "Wei-Jhe HuangMin-Hung ChenShang-Hong Lai",
        "links": "http://arxiv.org/abs/2408.15996v1",
        "entry_id": "http://arxiv.org/abs/2408.15996v1",
        "pdf_url": "http://arxiv.org/pdf/2408.15996v1",
        "summary": "Spatio-temporal action detection encompasses the tasks of localizing and\nclassifying individual actions within a video. Recent works aim to enhance this\nprocess by incorporating interaction modeling, which captures the relationship\nbetween people and their surrounding context. However, these approaches have\nprimarily focused on fully-supervised learning, and the current limitation lies\nin the lack of generalization capability to recognize unseen action categories.\nIn this paper, we aim to adapt the pretrained image-language models to detect\nunseen actions. To this end, we propose a method which can effectively leverage\nthe rich knowledge of visual-language models to perform Person-Context\nInteraction. Meanwhile, our Context Prompting module will utilize contextual\ninformation to prompt labels, thereby enhancing the generation of more\nrepresentative text features. Moreover, to address the challenge of recognizing\ndistinct actions by multiple people at the same timestamp, we design the\nInterest Token Spotting mechanism which employs pretrained visual knowledge to\nfind each person's interest context tokens, and then these tokens will be used\nfor prompting to generate text features tailored to each individual. To\nevaluate the ability to detect unseen actions, we propose a comprehensive\nbenchmark on J-HMDB, UCF101-24, and AVA datasets. The experiments show that our\nmethod achieves superior results compared to previous approaches and can be\nfurther extended to multi-action videos, bringing it closer to real-world\napplications. The code and data can be found in\nhttps://webber2933.github.io/ST-CLIP-project-page.",
        "updated": "2024-08-28 17:59:05 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.15996v1"
    },
    {
        "title": "CoGen: Learning from Feedback with Coupled Comprehension and Generation",
        "authors": "Mustafa Omer GulYoav Artzi",
        "links": "http://arxiv.org/abs/2408.15992v1",
        "entry_id": "http://arxiv.org/abs/2408.15992v1",
        "pdf_url": "http://arxiv.org/pdf/2408.15992v1",
        "summary": "Systems with both language comprehension and generation capabilities can\nbenefit from the tight connection between the two. This work studies coupling\ncomprehension and generation with focus on continually learning from\ninteraction with users. We propose techniques to tightly integrate the two\ncapabilities for both learning and inference. We situate our studies in\ntwo-player reference games, and deploy various models for thousands of\ninteractions with human users, while learning from interaction feedback\nsignals. We show dramatic improvements in performance over time, with\ncomprehension-generation coupling leading to performance improvements up to 26%\nin absolute terms and up to 17% higher accuracies compared to a non-coupled\nsystem. Our analysis also shows coupling has substantial qualitative impact on\nthe system's language, making it significantly more human-like.",
        "updated": "2024-08-28 17:58:39 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.15992v1"
    },
    {
        "title": "In-Context Imitation Learning via Next-Token Prediction",
        "authors": "Letian FuHuang HuangGaurav DattaLawrence Yunliang ChenWilliam Chung-Ho PanitchFangchen LiuHui LiKen Goldberg",
        "links": "http://arxiv.org/abs/2408.15980v1",
        "entry_id": "http://arxiv.org/abs/2408.15980v1",
        "pdf_url": "http://arxiv.org/pdf/2408.15980v1",
        "summary": "We explore how to enhance next-token prediction models to perform in-context\nimitation learning on a real robot, where the robot executes new tasks by\ninterpreting contextual information provided during the input phase, without\nupdating its underlying policy parameters. We propose In-Context Robot\nTransformer (ICRT), a causal transformer that performs autoregressive\nprediction on sensorimotor trajectories without relying on any linguistic data\nor reward function. This formulation enables flexible and training-free\nexecution of new tasks at test time, achieved by prompting the model with\nsensorimotor trajectories of the new task composing of image observations,\nactions and states tuples, collected through human teleoperation. Experiments\nwith a Franka Emika robot demonstrate that the ICRT can adapt to new tasks\nspecified by prompts, even in environment configurations that differ from both\nthe prompt and the training data. In a multitask environment setup, ICRT\nsignificantly outperforms current state-of-the-art next-token prediction models\nin robotics on generalizing to unseen tasks. Code, checkpoints and data are\navailable on https://icrt.dev/",
        "updated": "2024-08-28 17:50:19 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.15980v1"
    }
]