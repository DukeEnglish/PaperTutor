[
    {
        "title": "Q-MRS: A Deep Learning Framework for Quantitative Magnetic Resonance Spectra Analysis",
        "authors": "Christopher J. WuLawrence S. KegelesJia Guo",
        "links": "http://arxiv.org/abs/2408.15999v1",
        "entry_id": "http://arxiv.org/abs/2408.15999v1",
        "pdf_url": "http://arxiv.org/pdf/2408.15999v1",
        "summary": "Magnetic resonance spectroscopy (MRS) is an established technique for\nstudying tissue metabolism, particularly in central nervous system disorders.\nWhile powerful and versatile, MRS is often limited by challenges associated\nwith data quality, processing, and quantification. Existing MRS quantification\nmethods face difficulties in balancing model complexity and reproducibility\nduring spectral modeling, often falling into the trap of either\noversimplification or over-parameterization. To address these limitations, this\nstudy introduces a deep learning (DL) framework that employs transfer learning,\nin which the model is pre-trained on simulated datasets before it undergoes\nfine-tuning on in vivo data. The proposed framework showed promising\nperformance when applied to the Philips dataset from the BIG GABA repository\nand represents an exciting advancement in MRS data analysis.",
        "updated": "2024-08-28 18:05:53 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.15999v1"
    },
    {
        "title": "Eagle: Exploring The Design Space for Multimodal LLMs with Mixture of Encoders",
        "authors": "Min ShiFuxiao LiuShihao WangShijia LiaoSubhashree RadhakrishnanDe-An HuangHongxu YinKaran SapraYaser YacoobHumphrey ShiBryan CatanzaroAndrew TaoJan KautzZhiding YuGuilin Liu",
        "links": "http://arxiv.org/abs/2408.15998v1",
        "entry_id": "http://arxiv.org/abs/2408.15998v1",
        "pdf_url": "http://arxiv.org/pdf/2408.15998v1",
        "summary": "The ability to accurately interpret complex visual information is a crucial\ntopic of multimodal large language models (MLLMs). Recent work indicates that\nenhanced visual perception significantly reduces hallucinations and improves\nperformance on resolution-sensitive tasks, such as optical character\nrecognition and document analysis. A number of recent MLLMs achieve this goal\nusing a mixture of vision encoders. Despite their success, there is a lack of\nsystematic comparisons and detailed ablation studies addressing critical\naspects, such as expert selection and the integration of multiple vision\nexperts. This study provides an extensive exploration of the design space for\nMLLMs using a mixture of vision encoders and resolutions. Our findings reveal\nseveral underlying principles common to various existing strategies, leading to\na streamlined yet effective design approach. We discover that simply\nconcatenating visual tokens from a set of complementary vision encoders is as\neffective as more complex mixing architectures or strategies. We additionally\nintroduce Pre-Alignment to bridge the gap between vision-focused encoders and\nlanguage tokens, enhancing model coherence. The resulting family of MLLMs,\nEagle, surpasses other leading open-source models on major MLLM benchmarks.\nModels and code: https://github.com/NVlabs/Eagle",
        "updated": "2024-08-28 17:59:31 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.15998v1"
    },
    {
        "title": "Mamba or Transformer for Time Series Forecasting? Mixture of Universals (MoU) Is All You Need",
        "authors": "Sijia PengYun XiongYangyong ZhuZhiqiang Shen",
        "links": "http://arxiv.org/abs/2408.15997v1",
        "entry_id": "http://arxiv.org/abs/2408.15997v1",
        "pdf_url": "http://arxiv.org/pdf/2408.15997v1",
        "summary": "Time series forecasting requires balancing short-term and long-term\ndependencies for accurate predictions. Existing methods mainly focus on\nlong-term dependency modeling, neglecting the complexities of short-term\ndynamics, which may hinder performance. Transformers are superior in modeling\nlong-term dependencies but are criticized for their quadratic computational\ncost. Mamba provides a near-linear alternative but is reported less effective\nin time series longterm forecasting due to potential information loss. Current\narchitectures fall short in offering both high efficiency and strong\nperformance for long-term dependency modeling. To address these challenges, we\nintroduce Mixture of Universals (MoU), a versatile model to capture both\nshort-term and long-term dependencies for enhancing performance in time series\nforecasting. MoU is composed of two novel designs: Mixture of Feature\nExtractors (MoF), an adaptive method designed to improve time series patch\nrepresentations for short-term dependency, and Mixture of Architectures (MoA),\nwhich hierarchically integrates Mamba, FeedForward, Convolution, and\nSelf-Attention architectures in a specialized order to model long-term\ndependency from a hybrid perspective. The proposed approach achieves\nstate-of-the-art performance while maintaining relatively low computational\ncosts. Extensive experiments on seven real-world datasets demonstrate the\nsuperiority of MoU. Code is available at https://github.com/lunaaa95/mou/.",
        "updated": "2024-08-28 17:59:27 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.15997v1"
    },
    {
        "title": "ClimDetect: A Benchmark Dataset for Climate Change Detection and Attribution",
        "authors": "Sungduk YuBrian L. WhiteAnahita BhiwandiwallaMusashi HinckMatthew Lyle OlsonTung NguyenVasudev Lal",
        "links": "http://arxiv.org/abs/2408.15993v1",
        "entry_id": "http://arxiv.org/abs/2408.15993v1",
        "pdf_url": "http://arxiv.org/pdf/2408.15993v1",
        "summary": "Detecting and attributing temperature increases due to climate change is\ncrucial for understanding global warming and guiding adaptation strategies. The\ncomplexity of distinguishing human-induced climate signals from natural\nvariability has challenged traditional detection and attribution (D&A)\napproaches, which seek to identify specific \"fingerprints\" in climate response\nvariables. Deep learning offers potential for discerning these complex patterns\nin expansive spatial datasets. However, lack of standard protocols has hindered\nconsistent comparisons across studies. We introduce ClimDetect, a standardized\ndataset of over 816k daily climate snapshots, designed to enhance model\naccuracy in identifying climate change signals. ClimDetect integrates various\ninput and target variables used in past research, ensuring comparability and\nconsistency. We also explore the application of vision transformers (ViT) to\nclimate data, a novel and modernizing approach in this context. Our open-access\ndata and code serve as a benchmark for advancing climate science through\nimproved model evaluations. ClimDetect is publicly accessible via Huggingface\ndataet respository at: https://huggingface.co/datasets/ClimDetect/ClimDetect.",
        "updated": "2024-08-28 17:58:53 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.15993v1"
    },
    {
        "title": "CoGen: Learning from Feedback with Coupled Comprehension and Generation",
        "authors": "Mustafa Omer GulYoav Artzi",
        "links": "http://arxiv.org/abs/2408.15992v1",
        "entry_id": "http://arxiv.org/abs/2408.15992v1",
        "pdf_url": "http://arxiv.org/pdf/2408.15992v1",
        "summary": "Systems with both language comprehension and generation capabilities can\nbenefit from the tight connection between the two. This work studies coupling\ncomprehension and generation with focus on continually learning from\ninteraction with users. We propose techniques to tightly integrate the two\ncapabilities for both learning and inference. We situate our studies in\ntwo-player reference games, and deploy various models for thousands of\ninteractions with human users, while learning from interaction feedback\nsignals. We show dramatic improvements in performance over time, with\ncomprehension-generation coupling leading to performance improvements up to 26%\nin absolute terms and up to 17% higher accuracies compared to a non-coupled\nsystem. Our analysis also shows coupling has substantial qualitative impact on\nthe system's language, making it significantly more human-like.",
        "updated": "2024-08-28 17:58:39 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.15992v1"
    }
]