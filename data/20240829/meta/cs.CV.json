[
    {
        "title": "Eagle: Exploring The Design Space for Multimodal LLMs with Mixture of Encoders",
        "authors": "Min ShiFuxiao LiuShihao WangShijia LiaoSubhashree RadhakrishnanDe-An HuangHongxu YinKaran SapraYaser YacoobHumphrey ShiBryan CatanzaroAndrew TaoJan KautzZhiding YuGuilin Liu",
        "links": "http://arxiv.org/abs/2408.15998v1",
        "entry_id": "http://arxiv.org/abs/2408.15998v1",
        "pdf_url": "http://arxiv.org/pdf/2408.15998v1",
        "summary": "The ability to accurately interpret complex visual information is a crucial\ntopic of multimodal large language models (MLLMs). Recent work indicates that\nenhanced visual perception significantly reduces hallucinations and improves\nperformance on resolution-sensitive tasks, such as optical character\nrecognition and document analysis. A number of recent MLLMs achieve this goal\nusing a mixture of vision encoders. Despite their success, there is a lack of\nsystematic comparisons and detailed ablation studies addressing critical\naspects, such as expert selection and the integration of multiple vision\nexperts. This study provides an extensive exploration of the design space for\nMLLMs using a mixture of vision encoders and resolutions. Our findings reveal\nseveral underlying principles common to various existing strategies, leading to\na streamlined yet effective design approach. We discover that simply\nconcatenating visual tokens from a set of complementary vision encoders is as\neffective as more complex mixing architectures or strategies. We additionally\nintroduce Pre-Alignment to bridge the gap between vision-focused encoders and\nlanguage tokens, enhancing model coherence. The resulting family of MLLMs,\nEagle, surpasses other leading open-source models on major MLLM benchmarks.\nModels and code: https://github.com/NVlabs/Eagle",
        "updated": "2024-08-28 17:59:31 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.15998v1"
    },
    {
        "title": "Spatio-Temporal Context Prompting for Zero-Shot Action Detection",
        "authors": "Wei-Jhe HuangMin-Hung ChenShang-Hong Lai",
        "links": "http://arxiv.org/abs/2408.15996v1",
        "entry_id": "http://arxiv.org/abs/2408.15996v1",
        "pdf_url": "http://arxiv.org/pdf/2408.15996v1",
        "summary": "Spatio-temporal action detection encompasses the tasks of localizing and\nclassifying individual actions within a video. Recent works aim to enhance this\nprocess by incorporating interaction modeling, which captures the relationship\nbetween people and their surrounding context. However, these approaches have\nprimarily focused on fully-supervised learning, and the current limitation lies\nin the lack of generalization capability to recognize unseen action categories.\nIn this paper, we aim to adapt the pretrained image-language models to detect\nunseen actions. To this end, we propose a method which can effectively leverage\nthe rich knowledge of visual-language models to perform Person-Context\nInteraction. Meanwhile, our Context Prompting module will utilize contextual\ninformation to prompt labels, thereby enhancing the generation of more\nrepresentative text features. Moreover, to address the challenge of recognizing\ndistinct actions by multiple people at the same timestamp, we design the\nInterest Token Spotting mechanism which employs pretrained visual knowledge to\nfind each person's interest context tokens, and then these tokens will be used\nfor prompting to generate text features tailored to each individual. To\nevaluate the ability to detect unseen actions, we propose a comprehensive\nbenchmark on J-HMDB, UCF101-24, and AVA datasets. The experiments show that our\nmethod achieves superior results compared to previous approaches and can be\nfurther extended to multi-action videos, bringing it closer to real-world\napplications. The code and data can be found in\nhttps://webber2933.github.io/ST-CLIP-project-page.",
        "updated": "2024-08-28 17:59:05 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.15996v1"
    },
    {
        "title": "TEDRA: Text-based Editing of Dynamic and Photoreal Actors",
        "authors": "Basavaraj SunagadHeming ZhuMohit MendirattaAdam KortylewskiChristian TheobaltMarc Habermann",
        "links": "http://arxiv.org/abs/2408.15995v1",
        "entry_id": "http://arxiv.org/abs/2408.15995v1",
        "pdf_url": "http://arxiv.org/pdf/2408.15995v1",
        "summary": "Over the past years, significant progress has been made in creating\nphotorealistic and drivable 3D avatars solely from videos of real humans.\nHowever, a core remaining challenge is the fine-grained and user-friendly\nediting of clothing styles by means of textual descriptions. To this end, we\npresent TEDRA, the first method allowing text-based edits of an avatar, which\nmaintains the avatar's high fidelity, space-time coherency, as well as\ndynamics, and enables skeletal pose and view control. We begin by training a\nmodel to create a controllable and high-fidelity digital replica of the real\nactor. Next, we personalize a pretrained generative diffusion model by\nfine-tuning it on various frames of the real character captured from different\ncamera angles, ensuring the digital representation faithfully captures the\ndynamics and movements of the real person. This two-stage process lays the\nfoundation for our approach to dynamic human avatar editing. Utilizing this\npersonalized diffusion model, we modify the dynamic avatar based on a provided\ntext prompt using our Personalized Normal Aligned Score Distillation Sampling\n(PNA-SDS) within a model-based guidance framework. Additionally, we propose a\ntime step annealing strategy to ensure high-quality edits. Our results\ndemonstrate a clear improvement over prior work in functionality and visual\nquality.",
        "updated": "2024-08-28 17:59:02 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.15995v1"
    },
    {
        "title": "Perceive-IR: Learning to Perceive Degradation Better for All-in-One Image Restoration",
        "authors": "Xu ZhangJiaqi MaGuoli WangQian ZhangHuan ZhangLefei Zhang",
        "links": "http://arxiv.org/abs/2408.15994v1",
        "entry_id": "http://arxiv.org/abs/2408.15994v1",
        "pdf_url": "http://arxiv.org/pdf/2408.15994v1",
        "summary": "The limitations of task-specific and general image restoration methods for\nspecific degradation have prompted the development of all-in-one image\nrestoration techniques. However, the diversity of patterns among multiple\ndegradation, along with the significant uncertainties in mapping between\ndegraded images of different severities and their corresponding undistorted\nversions, pose significant challenges to the all-in-one restoration tasks. To\naddress these challenges, we propose Perceive-IR, an all-in-one image restorer\ndesigned to achieve fine-grained quality control that enables restored images\nto more closely resemble their undistorted counterparts, regardless of the type\nor severity of degradation. Specifically, Perceive-IR contains two stages: (1)\nprompt learning stage and (2) restoration stage. In the prompt learning stage,\nwe leverage prompt learning to acquire a fine-grained quality perceiver capable\nof distinguishing three-tier quality levels by constraining the prompt-image\nsimilarity in the CLIP perception space. Subsequently, this quality perceiver\nand difficulty-adaptive perceptual loss are integrated as a quality-aware\nlearning strategy to realize fine-grained quality control in restoration stage.\nFor the restoration stage, a semantic guidance module (SGM) and compact feature\nextraction (CFE) are proposed to further promote the restoration process by\nutilizing the robust semantic information from the pre-trained large scale\nvision models and distinguishing degradation-specific features. Extensive\nexperiments demonstrate that our Perceive-IR outperforms state-of-the-art\nmethods in all-in-one image restoration tasks and exhibit superior\ngeneralization ability when dealing with unseen tasks.",
        "updated": "2024-08-28 17:58:54 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.15994v1"
    },
    {
        "title": "ClimDetect: A Benchmark Dataset for Climate Change Detection and Attribution",
        "authors": "Sungduk YuBrian L. WhiteAnahita BhiwandiwallaMusashi HinckMatthew Lyle OlsonTung NguyenVasudev Lal",
        "links": "http://arxiv.org/abs/2408.15993v1",
        "entry_id": "http://arxiv.org/abs/2408.15993v1",
        "pdf_url": "http://arxiv.org/pdf/2408.15993v1",
        "summary": "Detecting and attributing temperature increases due to climate change is\ncrucial for understanding global warming and guiding adaptation strategies. The\ncomplexity of distinguishing human-induced climate signals from natural\nvariability has challenged traditional detection and attribution (D&A)\napproaches, which seek to identify specific \"fingerprints\" in climate response\nvariables. Deep learning offers potential for discerning these complex patterns\nin expansive spatial datasets. However, lack of standard protocols has hindered\nconsistent comparisons across studies. We introduce ClimDetect, a standardized\ndataset of over 816k daily climate snapshots, designed to enhance model\naccuracy in identifying climate change signals. ClimDetect integrates various\ninput and target variables used in past research, ensuring comparability and\nconsistency. We also explore the application of vision transformers (ViT) to\nclimate data, a novel and modernizing approach in this context. Our open-access\ndata and code serve as a benchmark for advancing climate science through\nimproved model evaluations. ClimDetect is publicly accessible via Huggingface\ndataet respository at: https://huggingface.co/datasets/ClimDetect/ClimDetect.",
        "updated": "2024-08-28 17:58:53 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.15993v1"
    }
]