[
    {
        "title": "CoGen: Learning from Feedback with Coupled Comprehension and Generation",
        "authors": "Mustafa Omer GulYoav Artzi",
        "links": "http://arxiv.org/abs/2408.15992v1",
        "entry_id": "http://arxiv.org/abs/2408.15992v1",
        "pdf_url": "http://arxiv.org/pdf/2408.15992v1",
        "summary": "Systems with both language comprehension and generation capabilities can\nbenefit from the tight connection between the two. This work studies coupling\ncomprehension and generation with focus on continually learning from\ninteraction with users. We propose techniques to tightly integrate the two\ncapabilities for both learning and inference. We situate our studies in\ntwo-player reference games, and deploy various models for thousands of\ninteractions with human users, while learning from interaction feedback\nsignals. We show dramatic improvements in performance over time, with\ncomprehension-generation coupling leading to performance improvements up to 26%\nin absolute terms and up to 17% higher accuracies compared to a non-coupled\nsystem. Our analysis also shows coupling has substantial qualitative impact on\nthe system's language, making it significantly more human-like.",
        "updated": "2024-08-28 17:58:39 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.15992v1"
    },
    {
        "title": "BattleAgentBench: A Benchmark for Evaluating Cooperation and Competition Capabilities of Language Models in Multi-Agent Systems",
        "authors": "Wei WangDan ZhangTao FengBoyan WangJie Tang",
        "links": "http://arxiv.org/abs/2408.15971v1",
        "entry_id": "http://arxiv.org/abs/2408.15971v1",
        "pdf_url": "http://arxiv.org/pdf/2408.15971v1",
        "summary": "Large Language Models (LLMs) are becoming increasingly powerful and capable\nof handling complex tasks, e.g., building single agents and multi-agent\nsystems. Compared to single agents, multi-agent systems have higher\nrequirements for the collaboration capabilities of language models. Many\nbenchmarks are proposed to evaluate their collaborative abilities. However,\nthese benchmarks lack fine-grained evaluations of LLM collaborative\ncapabilities. Additionally, multi-agent collaborative and competitive scenarios\nare ignored in existing works. To address these two problems, we propose a\nbenchmark, called BattleAgentBench, which defines seven sub-stages of three\nvarying difficulty levels and conducts a fine-grained evaluation of language\nmodels in terms of single-agent scenario navigation capabilities, paired-agent\ntask execution abilities, and multi-agent collaboration and competition\ncapabilities. We conducted extensive evaluations on leading four closed-source\nand seven open-source models. Experimental results indicate that API-based\nmodels perform excellently on simple tasks but open-source small models\nstruggle with simple tasks. Regarding difficult tasks that require\ncollaborative and competitive abilities, although API-based models have\ndemonstrated some collaborative capabilities, there is still enormous room for\nimprovement.",
        "updated": "2024-08-28 17:43:55 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.15971v1"
    },
    {
        "title": "More Text, Less Point: Towards 3D Data-Efficient Point-Language Understanding",
        "authors": "Yuan TangXu HanXianzhi LiQiao YuJinfeng XuYixue HaoLong HuMin Chen",
        "links": "http://arxiv.org/abs/2408.15966v1",
        "entry_id": "http://arxiv.org/abs/2408.15966v1",
        "pdf_url": "http://arxiv.org/pdf/2408.15966v1",
        "summary": "Enabling Large Language Models (LLMs) to comprehend the 3D physical world\nremains a significant challenge. Due to the lack of large-scale 3D-text pair\ndatasets, the success of LLMs has yet to be replicated in 3D understanding. In\nthis paper, we rethink this issue and propose a new task: 3D Data-Efficient\nPoint-Language Understanding. The goal is to enable LLMs to achieve robust 3D\nobject understanding with minimal 3D point cloud and text data pairs. To\naddress this task, we introduce GreenPLM, which leverages more text data to\ncompensate for the lack of 3D data. First, inspired by using CLIP to align\nimages and text, we utilize a pre-trained point cloud-text encoder to map the\n3D point cloud space to the text space. This mapping leaves us to seamlessly\nconnect the text space with LLMs. Once the point-text-LLM connection is\nestablished, we further enhance text-LLM alignment by expanding the\nintermediate text space, thereby reducing the reliance on 3D point cloud data.\nSpecifically, we generate 6M free-text descriptions of 3D objects, and design a\nthree-stage training strategy to help LLMs better explore the intrinsic\nconnections between different modalities. To achieve efficient modality\nalignment, we design a zero-parameter cross-attention module for token pooling.\nExtensive experimental results show that GreenPLM requires only 12% of the 3D\ntraining data used by existing state-of-the-art models to achieve superior 3D\nunderstanding. Remarkably, GreenPLM also achieves competitive performance using\ntext-only data. The code and weights are available at:\nhttps://github.com/TangYuan96/GreenPLM.",
        "updated": "2024-08-28 17:38:44 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.15966v1"
    },
    {
        "title": "Leveraging Open Knowledge for Advancing Task Expertise in Large Language Models",
        "authors": "Yuncheng YangYulei QinTong WuZihan XuGang LiPengcheng GuoHang ShaoYucheng ShiKe LiXing SunJie YangYun Gu",
        "links": "http://arxiv.org/abs/2408.15915v1",
        "entry_id": "http://arxiv.org/abs/2408.15915v1",
        "pdf_url": "http://arxiv.org/pdf/2408.15915v1",
        "summary": "The cultivation of expertise for large language models (LLMs) to solve tasks\nof specific areas often requires special-purpose tuning with calibrated\nbehaviors on the expected stable outputs. To avoid huge cost brought by manual\npreparation of instruction datasets and training resources up to hundreds of\nhours, the exploitation of open knowledge including a wealth of low rank\nadaptation (LoRA) models and instruction datasets serves as a good starting\npoint. However, existing methods on model and data selection focus on the\nperformance of general-purpose capabilities while neglecting the knowledge gap\nexposed in domain-specific deployment. In the present study, we propose to\nbridge such gap by introducing few human-annotated samples (i.e., K-shot) for\nadvancing task expertise of LLMs with open knowledge. Specifically, we develop\nan efficient and scalable pipeline to cost-efficiently produce task experts\nwhere K-shot data intervene in selecting the most promising expert candidates\nand the task-relevant instructions. A mixture-of-expert (MoE) system is built\nto make the best use of individual-yet-complementary knowledge between multiple\nexperts. We unveil the two keys to the success of a MoE system, 1) the abidance\nby K-shot, and 2) the insistence on diversity. For the former, we ensure that\nmodels that truly possess problem-solving abilities on K-shot are selected\nrather than those blind guessers. Besides, during data selection, instructions\nthat share task-relevant contexts with K-shot are prioritized. For the latter,\nwe highlight the diversity of constituting experts and that of the fine-tuning\ninstructions throughout the model and data selection process. Extensive\nexperimental results confirm the superiority of our approach over existing\nmethods on utilization of open knowledge across various tasks. Codes and models\nwill be released later.",
        "updated": "2024-08-28 16:28:07 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.15915v1"
    },
    {
        "title": "LLM-Based Multi-Hop Question Answering with Knowledge Graph Integration in Evolving Environments",
        "authors": "Ruirui ChenWeifeng JiangChengwei QinIshaan Singh RawalCheston TanDongkyu ChoiBo XiongBo Ai",
        "links": "http://arxiv.org/abs/2408.15903v1",
        "entry_id": "http://arxiv.org/abs/2408.15903v1",
        "pdf_url": "http://arxiv.org/pdf/2408.15903v1",
        "summary": "The rapid obsolescence of information in Large Language Models (LLMs) has\ndriven the development of various techniques to incorporate new facts. However,\nexisting methods for knowledge editing still face difficulties with multi-hop\nquestions that require accurate fact identification and sequential logical\nreasoning, particularly among numerous fact updates. To tackle these\nchallenges, this paper introduces Graph Memory-based Editing for Large Language\nModels (GMeLLo), a straitforward and effective method that merges the explicit\nknowledge representation of Knowledge Graphs (KGs) with the linguistic\nflexibility of LLMs. Beyond merely leveraging LLMs for question answering,\nGMeLLo employs these models to convert free-form language into structured\nqueries and fact triples, facilitating seamless interaction with KGs for rapid\nupdates and precise multi-hop reasoning. Our results show that GMeLLo\nsignificantly surpasses current state-of-the-art knowledge editing methods in\nthe multi-hop question answering benchmark, MQuAKE, especially in scenarios\nwith extensive knowledge edits.",
        "updated": "2024-08-28 16:15:45 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.15903v1"
    }
]