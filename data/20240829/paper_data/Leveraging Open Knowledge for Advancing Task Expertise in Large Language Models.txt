JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 1
Leveraging Open Knowledge for Advancing Task
Expertise in Large Language Models
Yuncheng Yang, Yulei Qin, Tong Wu, Zihan Xu, Gang Li, Pengcheng Guo, Hang Shao, Yucheng Shi,
Ke Li, Xing Sun, Jie Yang, Yun Gu
Abstract—Thecultivationofexpertiseforlargelanguagemodels(LLMs)tosolvetasksofspecificareasoftenrequires
special-purposetuningwithcalibratedbehaviorsontheexpectedstableoutputs.Toavoidhugecostbroughtbymanualpreparationof
instructiondatasetsandtrainingresourcesuptohundredsofhours,theexploitationofopenknowledgeincludingawealthoflowrank
adaptation(LoRA)modelsandinstructiondatasetsservesasagoodstartingpoint.However,existingmethodsonmodelanddata
selectionfocusontheperformanceofgeneral-purposecapabilitieswhileneglectingtheknowledgegapexposedindomain-specific
deployment.Inthepresentstudy,weproposetobridgesuchgapbyintroducingfewhuman-annotatedsamples(i.e.,K-shot)for
advancingtaskexpertiseofLLMswithopenknowledge.Specifically,wedevelopanefficientandscalablepipelinetocost-efficiently
producetaskexpertswhereK-shotdatainterveneinselectingthemostpromisingexpertcandidatesandthetask-relevantinstructions.
Amixture-of-expert(MoE)systemisbuilttomakethebestuseofindividual-yet-complementaryknowledgebetweenmultipleexperts.
WeunveilthetwokeystothesuccessofaMoEsystem,1)theabidancebyK-shot,and2)theinsistenceondiversity.Fortheformer,
weensurethatmodelsthattrulypossessproblem-solvingabilitiesonK-shotareselectedratherthanthoseblindguessers.Besides,
duringdataselection,instructionsthatsharetask-relevantcontextswithK-shotareprioritized.Forthelatter,wehighlightthediversity
ofconstitutingexpertsandthatofthefine-tuninginstructionsthroughoutthemodelanddataselectionprocess.Extensiveexperimental
resultsconfirmthesuperiorityofourapproachoverexistingmethodsonutilizationofopenknowledgeacrossvarioustasks.Codesand
modelswillbereleasedlater.
IndexTerms—MixtureofExperts,LowRankAdaptation,ModelSelection,DataSelection
✦
1 INTRODUCTION can choose pertinent models from the existing LoRA bank or
library as a good starting point. Accordingly, researchers have
Recentfewyearshavewitnessedsignificantdevelopmentoflarge
developed various approaches to utilize and select the publicly
language models (LLMs) across a broad spectrum of tasks and
released LoRA models [33], [63], [67] either in an off-line or
domains.Theopen-sourcecommunityoffersanarrayofcompeti-
an on-line manner for expeditious task adaptation. Meanwhile,
tivepre-trained/foundationmodelswithgeneral-purposelanguage
greateffortshavebeenputintotheselectionofhigh-qualityopen
understandingandgenerationcapabilities[7],[36],[87],allowing
instruction datasets [74] for improving the instruction following
for the follow-up special-purpose tuning customized for specific
capabilities, where the most challenging [15], [18], [48], [101]
tasksanddomains.Tomatchspecialistcapabilities,onecommon
andinfluential[79],[98],[99],[109]datapointsarepreferred.
explorationtoboostpre-trainedbasemodelswithinspecialtyareas
Despite the tremendous progress made in model and data
is instruction tuning on expert-curated contents [57], [71], [111].
selection, most existing methods target at lifting LLMs for better
However, such a training-intensive approach poses challenges to
alignmentwithhumanpreferenceingeneraldomainsandcommon
themanualcollectionandannotationofinstruction-responsepairs,
tasks. It remains an under-explored problem to advance specific
which impedes agile development and deployment. On the other
task expertise of LLMs by fully exploiting the open knowledge
hand, extravagant computing resources are required to grasp the
such as public LoRA models and datasets. To fulfill such task-
missing domain knowledge from scratch, making it difficult to
oriented knowledge augmentation, in this paper, we propose to
appropriatelycalibratefortheexpectedresponses.
bridgethegapbetweengeneraland“vertical”domainknowledge
Fortunately, a vast amount of fine-tuned and aligned mod-
byresortingtoafewhuman-verifiedinstructionsamples(i.e.,K-
els[95],togetherwithinstructiondatasetsacrossvariousdomains
shot) from the task of interest. Such K-shot data play a steering
and tasks [62], [92], [100], are available online. Such models
role in guiding the selection of the most appropriate candidate
oftenexistintheformofLoRA[32]adapters,whicharederived
modelsandbeneficialdatapointstosolvethetaskathand.
from few strong base models. Given any task of interest, one
Problem Definition Given a small amount of labeled, real-
world instructions from the task of interest (K-shot), we aim at
Y. Yang is with Institute of Image Processing and Pattern Recognition,
Institute of Medical Robotics, Shanghai Jiao Tong University, Shanghai developingauniversallyeffectiveandeasilyscalablepipelinethat
200240, China, and Tencent YouTu Lab, Shanghai 200233, China (Email: leverages publicly available models and datasets to advance the
yaphabates@sjtu.edu.cn).
taskexpertiseofstate-of-the-art(SOTA)LLMs.
Y.Qin,T.Wu,Z.Xu,G.Li,P.Guo,H.Shao,Y.Shi,K.Li,andX.Sunarewith
TencentYouTuLab,Shanghai200233,China(Email:yuleiqin@tencent.com). Inthiscontext,weencounterthreeprimarychallenges:
J. Yang and Y. Gu are with Institute of Image Processing and Pattern Challenge 1 Given a collection of LLMs including one foun-
Recognition, Institute of Medical Robotics, Shanghai Jiao Tong University, dation model and its fine-tuned LoRA variants, how can we take
Shanghai200240,China(Email:yungu@ieee.org).
full advantage of K-shot to efficiently pinpoint the models with
Y.YangandY.Qincontributedequally.
ManuscriptreceivedAug25,2024.Correspondingauthor:YunGu. thehighestpotentialforsolvingtasksofinterest?
4202
guA
82
]VC.sc[
1v51951.8042:viXraJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 2
K-shot Data from Task of Interest Open-Source Datasets
... ...
Step3
Open-Source Models
Similarity-First
Data
Diversity-Aware
Base Models Task Adapters Selection
K-shot & Training Data Augmentation
LLaMA
Step2 Step4
Step1
MoE Continue
Model
Initializa Fine-
Selection
tion Tuning
Mistral Task
Expert Mixture-of-Expert
Expert
Candidates System
Fig.1. Givenfewannotateddatafromanytaskofinterest(K-shot),weaimtoadvanceLLMsintaskexpertisebyleveragingopen-sourcemodels
anddatasets.WeproposeanefficientandscalablepipelinetofullyexploitthesteeringroleofK-shotthroughoutmodelanddataselection.Highly
promisingexpertsarefirstselectedfromthemodelbankbycomprehensiveconsiderationoftheirperplexityandperformanceontheK-shotand
intra-groupdiversity.TheseexpertsareinitializedasoneMoEsystem.Subsequently,weperformdataaugmentationbyselectingdiverseopen
instructions that resemble K-shot the most. Finally, we fine-tune the MoE system with both K-shot and the augmented data, which not only
improvestoken-wisecooperationbetweenexpertsbutalsointegratesbroadknowledgeintothesystem.Theultimatetaskexpertbenefitsfromthe
complementaryskillsandknowledgeofconstitutingexperts.
Challenge 2 Given abundant instructions from open-source uncovers K-shot for model selection where both the reasoning
datasets, how can we identify the ones that share similar task or perplexity and the performance in accuracy provide insights into
domain contexts with K-shot to inject supplementary knowledge the generalized empirical risk of the model on examples from
intoLLMswithoutcausingoverfitting? tasks of interest. We investigate and highlight the role of chain-
Challenge 3 If multiple prospective LLMs are proved to be of-thought (CoT) [93] reformulation of answers with rationales
valid, how can we build an adaptive token-wise gating system in improving the interpretability and robustness of perplexity as
to harness their individual-yet-complementary knowledge with the expertise indicator. To address Challenge 2, we develop a
improvedcooperationbetweenLLMsoverK-shot? similarity-first,diversity-awaredataselectionstrategyforaugmen-
To address these challenges, we propose the following task tation of task expertise, where K-shot intervene in retrieving the
augmentation pipeline (see Fig. 1). Initially, for experimental mostsimilaropen-sourceinstructionsintheembeddingspace.To
controlofcomparabilityandreproducibility,wemaintainourown bridge the knowledge gap between general- and special-purpose
LoRA bank by collecting thirty-eight representative open-source tasks without easily overfitting K-shot, we once again integrate
datasets and preparing a pool of LoRA models. It ensures the diversity into the selection philosophy where semantic duplicates
availabilityanddiversityofinstruction-tunedmodelsonthebasis areremovedtoensureamoredisperseddistributionofdatapoints.
ofLLaMA[87]andMistral[36]pre-trainedmodels.Suchabank ToresolveChallenge3,weconstructamixture-of-experts(MoE)
canbeeasilygeneralizedtoscenarioswhereallpubliclyreleased systemandpayextraattentiontothecompositionchoiceofexperts
models are considered in practice. To tackle Challenge 1, we at initialization and the token-wise coordination between experts
proposetoselectmodelsthattrulyunderstandthetaskofinterest infine-tuning.Fortheformer,wearethefirststudytodemonstrate
rather than guessing randomly. We incorporate K-shot data to the critical role of a diversified selection of expert candidates in
develop a selection mechanism featured by three key indicators: buildingasuccessfulMoEsystemwithopen-sourcemodels.Such
1)reasoningperplexity:theuncertaintyofaLLMinmodelingthe diversity effectively expands the knowledge boundary beyond
reasoningprocesstowardsitsultimateanswertoeachinstruction; a single model. For the latter, traditional methods either use
2)exactmatchaccuracy:theevaluationofthegeneratedresponses linear model merging [34], [37], [106] or detached gating mech-
of the model on K-shot determined by the accuracy metric; anism [63], [67], [84], neglecting the potential conflicts or sub-
3) group diversity: the degree of distinction between multiple optimal allocation among experts. Their lack of interpretability
selectedcandidatesmeasuredinparameterversatility.Toourbest furtherworsensgeneralization[102].Incontrast,ourMoEsystem
knowledge, the proposed method is the pioneering work that is fully optimized on K-shot data augmentation for mitigating
Diversity
Performance
PerplexityJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 3
inter-expertconflictsandgettingfamiliarwithrelevanttasks. theinputquerywiththeaveragedembeddingsofthedatasetsused
Insummary,ourcontributionsarefour-fold: to train each individual expert. Such reliance on the fine-tuning
datasets restricts the practicability of these methods. [28], [52],
• We propose a novel K-shot learning setting in forging
[58]allinvestigateexternalmodelssuchasGPT4[3]andreward
LLMs with task expertise, where few human-verified in-
model [7] for estimation of the routing policy, which increases
structionsfromtasksofinterestareavailableforimproving
theircostofdeploymentandcausesisolationbetweentherouting
real-worldapplicationsinacost-efficientway.
model and the candidate experts. In spite of the development of
• We propose an effective and scalable pipeline that makes
merging techniques, few efforts [53], [69], [84] have been made
the best use of K-shot in leveraging open knowledge,
tocombinecross-domainexpertsformulti-taskproblems.
where the most promising candidates and the most bene-
Very recently, PHATGOOSE [63] proposes a post-hoc adap-
ficialinstructionsarerespectivelyselectedfromthepublic
tivetoken-wisegatingmechanismtorecyclefromacollectionof
LoRAbankandtheopen-sourcedatasets.
specialized experts. It aims at improving the zero-shot general-
• We emphasize diversity throughout the proposed model
ization of the pre-trained base model by constructing a routing
anddataselectionstrategies,whichlaysasolidfoundation
system and performing dynamic token-by-token assignment. De-
for constructing a versatile MoE system and acquiring
spite the shared MoE principal, PHATGOOSE differs from the
task-relevantskillswithoutoverfitting.
proposed method in three key aspects. First, the problem setting
• We demonstrate the advantages of a MoE system max-
ofPHATGOOSEisfundamentallydifferentfromoursinthatthey
imized by proper initialization and fine-tuning with en-
are not targeted at improving specific tasks of interest. There
hanceddomainknowledgeandexpertharmonization.
exists no model and data selection procedures in PHATGOOSE
foracquiringtask-relevantskills.Second,PHATGOOSEassumes
2 RELATED WORKS
that the contributors of the LoRA models provide additional gate
2.1 EfficientFine-tuningofParameters modules that are implemented as sigmoid layers in front of each
LoRA module. However, it is almost impossible to enforce the
Parameter-efficientfine-tuningmethods(PEFTs)areasetoftech-
samegatetrainingpipelineontheentireopen-sourcecommunity.
niques that diverge from traditional full-parameter fine-tuning. In
Inpractice,onecanonlyfindthereleasedLoRAmodulesfromthe
PEFTs,onlyacertainsubsetofamodel’sparametersareadjusted
repositories of Huggingface and Github without gating vectors.
tobettersuitspecifictasksofinterest.
Third, our MoE system adapts to different tasks of interest by
2.1.1 LoRA polishing the routing weights and the constituting experts simul-
taneously, while PHATGOOSE keeps the LoRA modules and
Low-rank adaptation (LoRA) [32] and its variants [25], [54],
gating vectors fixed with lower flexibility. Arrow [67] is another
[60],[110]uselow-rankmatricestoapproximateadditiveweights
contemporary method which maintains the LoRA library and
during training. These methods are beneficial as they do not
proposes a routing mechanism to select the most input-relevant
necessitate additional computational resources during inference,
LoRAs.AlthoughArrowisfeaturedbyitstraining-freerouting,it
where the updated weights can be integrated into the model
does not consider the potential conflicts when the inputs are not
withoutanycomplications.
representativeenoughofthegiventask.ThestatisticsoftheLoRA
2.1.2 PromptTuning parameters (e.g., singular vectors) are directly used as the proxy
for the routing weights, which is prone be biased towards the
Prompt-based methods incorporate randomly initialized soft to-
original datasets used to train LoRA modules. Furthermore, they
kens to the input, usually as a prefix, and train their embeddings
do not introduce the K-shot datapoints from downstream tasks
while maintaining the LLM’s weights constant, as suggested
forcalibratingtheexpertbehavior,andneglecttheexploitationof
in [44], [49], [118]. While these methods perform competitively,
open-sourcedatasetsforoptimizingtheexpertcollaboration.
theydoentailasubstantialcomputationalloadduringinference.
2.1.3 Adapters
2.3 DataSelectionforEfficientTuning
Adapter methods involve the training of extra modules (for in-
stance, fully connected layers) on top of the frozen pre-trained Existingmethodsondataselectionforpre-trainingandinstruction
model [72], [117]. Unfortunately, these adapters are not effort- tuning of LLMs can be categorized into: 1) quality-based, 2)
lessly integrated into the original architecture, thereby reducing diversity-based,and3)importance-based[74].
inferenceefficiency.
2.3.1 Quality
2.2 Mixture-of-ExpertModels [59] explores perplexity, L2-Norm error, and memorization as
TheMoEtechnique[38],[83]combinesseveralspecializedexpert qualityscorestorankandprunepre-trainingcorpora.[48]presents
modelstoefficientlyscaleupmodelsforimprovedgeneralization. anovelself-guidedapproachtoautonomouslyselecthigh-quality
TheprinciplebehindMoEisthateachexpertisadeptathandling samplesfromopen-sourcedatasetsusingtheinstruction-following
a specific region of the input distributional space, and their difficulty (IFD) metric. [18] employs the GPT3.5 to score
combined decision-making outperforms any individual expert. instruction-response pairs and filters out samples with scores
Recent studies [29], [45], [50], [56], [97] focus on utilizing the belowathreshold.[15]introducesinstructminingtoautomatically
MoE as a PEFT technique. Other methods [20], [33], [64], [67], selecthigh-qualityinstruction-followingdataforLLMfine-tuning.
[114]underscoretheuseofexistingLoRAexpertsforconvenient [27] proposes a quality evaluation model to extract high-quality
assembly, where the fine-tuning of their parameters is saved. subsets from the original dataset and designs an algorithm to
Specifically,both[35]and[10]routetooneexpertbycomparing furtherselectaseedinstructiondatasetwithextensivecoverage.JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 4
2.3.2 Diversity
Mixture-of-Expert Model SFT
Geometry-based sampling methods are the most intuitive and Base Model (Low Reasoning Perplexity, Medium Performance)
widely-used ones for maximizing diversity of the selected sam- Base Model (High Reasoning Perplexity, High Performance)
′SFT Model (Low Reasoning Perplexity, Medium Performance)
ples[40],[82],[116].Inparticular,k-centergreedy[80]isfavored
′SFT Model (High Reasoning Perplexity, High Performance)
in diversity sampling on massive pre-training and instruction-
′ ′ Base Model (High Reasoning Perplexity, Low Performance)
tuning corpus [11], [16], [27], [96]. It performs iterative and ′
′
greedy selection on data that exhibit the most dissimilarity with ′ ′
′
thealreadyselectedsetintheembeddingspace. ∆𝑨𝒄𝒄 ′ ′ ′ ∆𝑨𝒄𝒄
+7.9%
+4.6%
2.3.3 Importance
Two kinds of gradient-based methods on importance estimation
have been developed: 1) gradient matching [8], [109], [113], i.e.,
thegradientsoftheentiresetbeingapproximatedbytheweighted
gradients of the selected set, and 2) gradient influence [9], [13],
[73], i.e., the influence of each training datapoint on the testing
set being measured by upweighted gradient multiplication. For Reasoning Perplexity
importance-orientedsampling,[99]adoptsimportanceresampling
to select a subset from a large-scale unlabeled dataset that shares Fig.2.Theperformanceoftask-specificfine-tuningversusthereasoning
similardistributionswiththetargetset. perplexity of candidate models in the bank. Preliminary experiments
demonstratethatmodelsoflowerperformancearenotalwaysinlackof
Althoughthesemethodsstrikeabalancebetweendataquality
domain-specificknowledge.Instead,theirinabilitytofollowinstructions
and quantity, they fail to incorporate the data selection pipeline on the expected output format (e.g., answer choice) causes parsing
into the task-oriented model development. On the contrary, the failure during post-processing on the generated responses, which di-
minishestheirperformanceinevaluationmetrics.Toavoidsuchbiased,
proposed method focuses on improving the downstream per-
partialmeasurementmerelybytheexact-matchaccuracy,wepropose
formance given limited K-shot real-word datapoints under the
tousetheperplexityovertheCoTrationalesofanswersasasuperior,
context of expert fusion. Correspondingly, we simultaneously complementary proxy for model assessment. Accordingly, we evalu-
consider the quality and importance where the resemblance of ate if the model possesses the task-specific knowledge by computing
its perplexity score of modeling the reasoning process. Models that
an open-source instruction to the K-shot set is prioritized during
achievelowerreasoningperplexityareconsideredcompetentandtend
selection. In addition, we treasure the diversity of the selected toachievegreaterperformanceimprovementafterfine-tuningthanthose
datasetasitsvarietyhelpspolishthecoordinationbetweenexperts withhigherreasoningperplexity.
intoken-wiserouting.Tothebestofourknowledge,theproposed
methodisthepioneerthatintegratescomprehensivedataselection
3.1.2 DataPreprocessing
into the advancement of a MoE system for task expertise, where
the concepts of quality, diversity, and importance play a critical All raw datasets are processed into the following format:
rolethroughouttheselectionofbothexpertanddatacandidates. {”instruction”: ⟨instruction⟩, ”input”: ⟨input (can be
empty)⟩, ”output”: ⟨output⟩}. For certain CoT datasets, we
directly follow [75] to use the template from FLAN [57] and
3 METHODOLOGY transformtheoriginaldataintothedataformatabove.
3.1 LoRABankConstruction
3.1.3 LoRAFine-Tuning
The open-source community has witnessed a significant increase
Foreachdataset,wefine-tunepre-trainedbasemodelsandderive
in the number of LoRA models and high-quality datasets. To
theirLoRAweights.TheseLoRAmodelsareusedforsubsequent
ensureexperimentalreproducibility,rationality,andcomparability,
model selection. We follow [90] to set the hyper-parameters for
we have selected thirty-eight representative and widely-used
optimization: a batch size of 2, gradient accumulation steps of
instructiondatasetsfromtheHuggingface[95]toconstructarich
16, and an initial learning rate of 5e-5. Uniformly, all models
andreliableLoRAbank.
underwenttrainingforthreeepochstoensurethateachdatasetis
fullymasteredregardlessofthetaskdifficulty.Acosinedecaying
3.1.1 DataSources
scheduleisadoptedforadjustingthelearningrateoveriterations.
Specifically,thedatatoconstructourLoRABankaresummarized TheLoRAmodulesareappliedonthelinearembeddinglayersof
asfollows:ARC[22],Winogrande[1],GSM8K[23],PiQA[12], theQueryandValuematricesinself-attention.ForallLoRAs,the
CommonSenseQA [85], RACE [42], MBPP [6], MathQA [5], rank was set to 16. To reduce training overhead, sequences with
Esnli [14], ECQA [4], CREAK [66], GPT4Tools [103], lengthexceeding1024weretruncated.
AQuA [51], QASC [39], QED [43], StrategyQA [30],
SensemakingQA [89], Toolformer [78], HellaSwag [107],
3.2 K-shotGuidedExpertModelSelection
SiQA [77], BoolQ [21], Dolly [31], WizardLM [100], We-
bGPT [65], Lima [115], Code-Alpaca [91], ThoughtSource [68], In a bank of LoRA models, it becomes essential to appropriately
CAMEL [46]. We choose these datasets for two reasons: 1) select the most relevant task expert candidates [104], [105].
they provide a comprehensive coverage of mainstream tasks However, few studies have explored model selection options for
and domains, and 2) their quality is confirmed through massive language models, especially in the context of K-shot settings.
downloads worldwide and positive feedback comments from re- Furthermore,K-shotlabeleddataaretypicallysparseanduniform
searchers. in the input embedding space, which means that the evaluation
ecnamrofrePJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 5
results of models on such limited data cannot reflect their true PromptTemplatetoGenerateCoTRationales
expertiselevel.
In this section, we propose to select highly-potential models Read the question and answer. According to the answer,
thatwouldperformwellonanytaskofinterestbeforeconducting presentyourthoughtaboutthesolution.
the task-specific fine-tuning. We hypothesize that two following -Pleasethinkstepbystepandprovidetheanswerfirstthen
withitsexplanation.
perspectivesshouldbeprioritizedduringmodelselection:1)Does
-Yourexplanationshouldnotexceedthreesentences.
themodelpossessthenecessaryknowledgeforthetasksofin-
terest?and2)Canthemodeladequatelyfollowtheinstructions Question:{Question}
under the tasks of interest? Accordingly, we first consider two Answer:{Answer}.
indicators in estimation ofthe empiricalrisk onK-shot samples:
1) the specific evaluation metric for performance measurement,
andtheperplexityofaLLMinmodelingtheanswersinanauto- Fig.3.ExpansionoftheCoTrationalesonK-shotinstructions.
regressivemanner.Specifically,exactmatchaccuracyisadoptedin
thepresentstudyasthedefaultevaluationmetriconthegenerated
responses. If the answer choice precisely matches the model’s theWizardLM2-8x22B[61]forreasoningexplanations(seeFig.3
output,itisconsidered correct;otherwise,itisdeemedincorrect. for detailed prompts). Such rationales improve the transparency
However, the empirical risk merely computed by performance is andinterpretabilityofthedecision-makingprocess,whichbenefits
prone to be affected by poor instruction following capabilities accurate model selection by estimating the uncertainty of any
of LLMs, where models that produce correct but unparsable LLM on the rationales under the context of instruction. Besides,
answers are severely under-estimated. The post-processing tech- answers with reasoning process in advance are more effective in
niques cannot handle all the corner cases of answers that are fine-tuning models, where the justified rationales well calibrate
formattedunexpectedly.Ontheotherhand,modelsthatrandomly the response towards the ultimate correct answers. As illustrated
guess the answer choices in QA tasks might be over-estimated. in Fig. 2, models with higher reasoning perplexity might achieve
Suchmisjudgementoriginatesfromthelowinformativeevaluation better performance before task-specific tuning, but their perfor-
metricthatfailstocomprehensivelyassesswhetherthemodelcan mance gains brought by fine-tuning are not as strong as models
understandandhandlethegiventask.Therefore,itcallsuponthe withlowerreasoningperplexity.
perplexityofthemodelonK-shotsequencesasastraightforward For the K-shot D K = {(x 1,y 1),(x 2,y 2),...,(x K,y K)},
complement. To reduce biased measurement merely from one |D K|=K,thetotalreasoningperplexityisdefinedas:
indicator,wetakeintoconsiderationofthreeaspects:1)perplexity, (cid:88)
PPL (m,D )= PPL(x ,Φ(x ,y ),θ ). (4)
2)performance,and3)diversity. R K i i i m
(xi,yi)∈DK
3.2.1 Perplexity
3.2.2 Performance
The perplexity of a LLM on auto-regressive modeling of the
The evaluation metric measured on K-shot directly reflects a
answers serve as an effective indicator of the model’s capability.
Given a model m parameterized by θ , an input sequence x, model’sabilitytosolvethecorrespondingtaskinan“end-to-end”
m
and its expected output sequence y, the perplexity of language way.Wedenotesuchevaluationresultsastheperformance,which
is defined by any metric calculated by comparing the generated
modelingisdefinedwithcross-entropy:
responsesy˜andtheground-truthanswersy.Inlinewiththetask
requirement, a post-processing step might be involved to extract
|y|
(cid:88) formattedanswersfromresponses.Itcanbeimplementedthrough
PPL(x,y,θ m)=exp(− logP(y (i)|x,y (<i);θ m)), (1) certain function f(·), i.e., y˜′ = f(y˜). Common post-processing
i=1
operations include threshold-based truncation and probability-to-
where P(y |x,y ) is the predicted probability of the i-th categorymapping.Outofsimplicity,theperformanceofamodel
(i) (<i)
tokeny ofy giventhesequencesxandy . monthedatasetD canbedefinedastheaccuracyofmatching
(i) (<i) K
However, as explained in the drawbacks of exact match on thepost-processedy˜′ withtheground-truthy:
multiple-choice problems, the perplexity computed solely on the
ground-truth answer options in many QA tasks still suffers from Acc(m,D ,f)= 1 (cid:88) 1(y˜′ =y ),
K N i i
inaccurateestimationofmodelcapabilities.Toaddressthisissue, (5)
(xi,yi)∈DK
we utilized an advanced open model [61] to expand the answers y˜′ =f(y˜),
with CoT rationales. Subsequently, we calculated the reasoning i i
perplexity of LoRA models by considering both the predicted where 1(·) is the indicator function that equals to 1 when the
answerchoicesandtherationalesbehindthem: conditionholdstrueand0otherwise.y˜ i isthepredictedresponse
byθ giventheinputx .
|yˆ| m i
(cid:88)
PPL(x,yˆ,θ )=exp(− logP(yˆ |x,yˆ ;θ )), (2)
m (i) (<i) m 3.2.3 Diversity
i=1
To fully harness the task-related knowledge inherent in models
yˆ=Φ(x,y), (3)
from the LoRA Bank, our objective is to select all candidate
where Φ(x,y) represents the CoT expansion process given both models that are likely to solve the downstream tasks. To expand
the input x and the output y. For such CoT rationales, we target the knowledge base, it is essential to “deduplicate” the selected
atmultiple-choicedatasetswhoseoutputsareonlyansweroptions models so that each candidate model contributes to the accumu-
(e.g., A and B). We use the CoT-formatted answers rewritten by lation of skills. Therefore, diversity is ensured so that differentJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 6
selected experts possess distinct abilities. Contrary to existing Algorithm1K-shotGuidedExpertModelSelection
studies[41],[47],[84]thatprimarilyfocusoncombiningexperts Input: AmodelbankB,numberofcandidatemodelsM,number
without considering their relationships, we highlight the concept ofselectedexpertsN,K-shotdataD ,andpost-processing
K
of group diversity for retrieving expert candidates. It refers to functionf(·)
the variety of the selected models in a group. A high intra-group Output: SelectedexpertsB
E
diversity lays a solid foundation for exploitation of task-relevant 1: Compute the reasoning perplexity PPL R(m,D K) and the
yetcomplementaryknowledge.GivenagroupofN expertmodels performancebyaccuracyAcc(m,D ,f)for∀m∈B
K
B N = {m 1,m 2,...,m N}, the group diversity Ω BN is defined 2: forallm∈B do
as the inverse of the sum of cosine similarities between the 3: Extract the ranking R P(m) ← rank(PPL R(m,D K)) by
parametersofeachmodelpairE(m )andE(m ): sortingfromsmallesttolargest
i j
4: Extract the ranking R A(m) ← rank(Acc(m,D K,f)) by
Ω
BN
=( N(N2
−1)
(cid:88) (cid:88) ∥EE (( mm i )) ∥· ∥E E( (m mj) )∥)−1,
5:
ens do fr oti rngfromlargesttosmallest
mi∈BNmj̸=mi i j 6: FindthetopM modelsB M withthesmallestsumofrankings
(6)
inbothperplexityandperformance
whereE(m i)denotestheflattenedmatricesoflayersinthemodel B∗ ← argmin (cid:80) (R (m) +
m . The cosine similarity between the paired m and m is M BM⊂B,|BM|=M m∈BM P
i i j R (m))
A
computedmatrixbymatrixandaveragedoveralllayers.
7: Calculate the group diversity Ω BN for all N-tuples ∀B N ⊂
B ,|B |=N
M N
3.2.4 SelectionMechanism
8: forallB N ⊂B M do
TheoverallpipelineofmodelselectionisillustratedinFig.4and 9: Extract the ranking R D(B N) ← rank(Ω BN) by sorting
detailed in Alg. 1. One important principle behind our pipeline fromlargesttosmallest
design is to comprehensively employ indicators from various 10: endfor
aspects,whichreducesthebiasbypartialmeasurement. 11: Find the N-tuple B N ⊂ B M,|B N| = N with the smallest
Wefirstcalculatethereasoningperplexityandtheperformance sum of rankings in perplexity, performance, and group simi-
in accuracy for each model in the bank B based on Eq. 4 and larity
Eq.5.Wesortmodelsbyperplexityandperformancerespectively B∗ ← argmin (cid:80) (R (m) +
fromsmallesttolargestandfromlargesttosmallest.Therankings
RN
(m))+R (B
)BN⊂B M∗ ,|BN|=N m∈BN P
A D N
by perplexity and performance are respectively denoted as R P 12: return B E =B N
and R . Then, we select M candidate models B∗ from the
A M
LoRA Bank B with minimum sum of rankings R and R . In
P A
this way, models in lack of task-relevant knowledge and skills
A more reasonable strategy would be a mixture of expertise.
are filtered out in advance to greatly reduce the computation
Specifically, we train the router in such a manner that the model
overhead on diversity measurement. Subsequently, we calculate
autonomously allocates different tokens to appropriate experts.
the group diversity Ω BN on all combinations of N-tuples B N Given an input x, the output of a N-expert MoE system at the
from B M∗ , and sort these tuples by intra-group diversity from l-th layer gl(x) is computed as the weighted sum of the outputs
largest to smallest for the ranking R . Finally, we sum up all
D fromeachexpert:
the rankings of R , R , and R and choose the top N models
P A D
B N∗ ⊂B M,|B N∗|=N forinitializingourMoEsystemB E. gl(x)=(cid:88)N
Gl(x)·gl(x), (8)
Comparedwithothermethods,ourselectionapproachismore i i
robust to variation of K-shot across tasks due to its all-sided i=1
evaluation in perplexity, performance, and diversity. Besides, it where Gl(x) represents the i-th element of the gating vector
i
consumesfewercomputationalresourcesfortworeasons:1)Low- Gl(x) ∈ RN, controlling the contribution of the i-th expert.
qualityandimpotentmodelsareeliminatedbeforetheexhaustive gl(x) is the output from the i-th expert at the l-th layer. A
i
computation of intra-group diversity. 2) Each model performs simple yet efficient implementation of the gating network Gl(·)
evaluationonlyonceonK-shotdata. is a single fully-connected layer Wl, where the gating vector is
g
obtained by matrix multiplication between Wl and gl(x). Only
g i
3.3 Mixture-of-ExpertsInitialization the top-k activated experts are selected with their gating weights
re-normalizedviasoftmaxfunction:
Upon selecting the most promising experts, our objective is
(cid:16) (cid:16) (cid:17)(cid:17)
to efficiently utilize their potentials at respective domains. The Gl(x)=softmax top-k gl(x)·Wl , (9)
i g
lineararithmeticcompositionoffersastraightforwardapproachto
benefitfromourLoRAbank[20],[33],[108]: where top-k(·) keeps the value of the largest k elements un-
changed and returns the other elements as −∞. We fine-tune all
N
Wˆ =W +(cid:88) w ∆W , (7) LoRAexpertsandrouterssimultaneously(seeFig.5).
i i
i=1
whereW indicatestheoriginalparameterofapre-trainedmodel 3.4 K-shotGuidedSim-DivDataSelection
and ∆W denotes the i-th LoRA variant with
(cid:80)N
w = 1. Insituationswherefewannotateddataareavailableunderthetask
i i=1 i
Nevertheless, such a naive method of assigning weights to dif- ofinterest,itispronetooverfittingifourMoEsystemisdirectly
ferent LoRAs does not promise a dynamic and flexible routing fine-tunedonthesedatapoints.Acommonapproachtoaddressthis
mechanism,restrictingitsquickadaptationtoanytaskofinterest. issueisdataaugmentation[26],[81],[94],[112],whichmitigatesJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 7
Model*LoRA Performance Evaluation on K-shot Data Model*LoRA
A:
1 Model 1: B. Pour it into a jar （ … ） 1
Model 2: A. Pour it onto a plate N-tuple 1
Group Diversity=0.114
2 2
（ … ）
Q: When boiling butter, when it's ready, you can:
3 A. Pour it onto a plate N-tuple 2 3
B. Pour it into a jar Group Diversity=0.004
CoT: Pouring it into a jar is a suitable option （ … ）
because … The Answer is: B. Pour it into a jar.
|B| N-tuple 𝑪𝑵 N
Model 1: Perplexity=3.998 𝑴
Group Diversity=0.251
Model 2: Perplexity=2.787
Bank 𝐵 Reasoning Perplexity on K-shot Data 𝑀 Candidates In-tuple Diversity 𝑁 Models Experts 𝐵
!
Fig.4.TheoverallpipelineofourK-shotguidedmodelselectionstrategy.Acomprehensiveassessmentintermsofperplexity,performance,and
diversityisconductedoneachmodelforexpertselection.GivenK-shotdata,weevaluateamodel’sperformanceviaexactmatchaccuracyon
thedirectlyinferredresults.Thereasoningperplexityisobtainedbycomputingtheperplexityonauto-regressivemodelingoftheCoTrationales
towardsanswersofK-shot.Thetop-M rankedcandidatemodelsarefirstselectedtosavecomputationofthesubsequentintra-groupdiversity,
whereeveryN-tupleoutoftheM candidatesareinvolved.TheN modelsthatsharethelowestsimilarityinparameters(i.e.,thelargestgroup
diversity)contributetotheinitializationofaMoEsystem.
befavoredtoimprovetheperformanceonourtaskofinterest?
× 𝐿
In the present study, we assume that datapoints that share a
Add & Norm … similar distribution with samples in the downstream tasks ought
Expert 0 Expert 1 Expert 2 Expert N tobeprioritized.Accordingly,weproposethesimilarity-firstand
Feed Forward diversity-awareprincipletoguidethedataselectionprocess.
Network Selected … Unselected
Router 3.4.1 DataEncoding
In the first step, we perform encoding of the raw instruc-
tion texts for their projection into the embedding space. Given
Add & Norm the K-shot D and a set of S open-source samples D =
K S
… {(x ,y ),(x ,y ),...,(x ,y )},|D | = S, we aim to find
1 1 2 2 S S S
Expert 0 Expert 1 Expert 2 Expert N the most relevant datapoints in D based on their similarity
Multi-Head S
Attention Selected … Unselected with samples in D K. Given a pre-trained encoding model h
parameterizedbyθ ,weobtaintherepresentationofeachsample
Router h
u withbothinputandoutputvia:
i
Hidden States
Learnable Frozen u =h([x ,y ];θ ), (10)
i i i h
where [·,·] denotes the concatenation operation in python
Fig.5.ThearchitectureofourMoEsystem.ItisimplementedwithLoRA syntax. The embeddings of samples (x i,y i) from D K and
modules, where the selected models from the LoRA bank are trained D are respectively denoted as U and U . In practice,
S K S
withanadditionalroutertolearntoassigndifferenttokenstotherespon- we employ the pre-trained BGE model [17] as h, which is
sibleexperts.Eachtokenisroutedtothetop-k activatedexpertswith
fine-tuned on the XLM-RoBERTa [24] with self-distillation
theirrepresentationsmultipliedbythecorrespondingroutingweightsfor
normalization. on corpus of multiple languages, tasks, and granularities.
Specifically for extraction of u , we use the prefix prompt
i
of query_instruction_for_retrieval: “Represent the
the risk of degeneration via lexical or semantic augmentation
followingsentenceforsimilartaskretrieval:⟨[x ,y ]⟩”.
on the original text inputs. Such manually designed techniques i i
tend to work on traditional discriminative models by imposing 3.4.2 Similarity-First
perturbations to the decision boundary. However, for generative Wecalculatethecross-datasetsimilaritybetweenD andD to
K S
LLMs,vanilladataaugmentationmethodsdonotinherentlyinject select a subset D ⊂ D that resembles D the most. First,
C S K
any new knowledge into the model, nor do they fundamentally we define a distance d(·,·) to measure the similarity between
improvethediversityofthemaintainedinstructiondataset. two encoded samples u and u . Without losing generality, we
i j
In the light of this statement, we propose to leverage open- demonstrate an intuitive implementation of similarity metric via
sourcedatafortask-orientedaugmentation.Ithasthreeadvantages cosinedistance:
including:1)highcost-efficiencyofutilizingthemassiveandfree u ·u
sim(u ,u )=1−d (u ,u )= i j , (11)
open-source datasets, 2) prevention of overfitting by introducing i j cos i j ∥u ∥∥u ∥
i j
diverseandbeneficialinstructions,and3)improvementoftoken-
where each pair of u ∈ U and u ∈ U constructs the entry
wise collaboration between experts via acquiring novel knowl- i K j S
A ofthesimilaritymatrixA∈RK×S:
edge.Whenitcomestotheselectionofspecificdatainstances,a ij
new question is raised: What type of open-source data should A =sim(u ,u ). (12)
ij i j
…
&
ecnamrofreP
yb
gniknaR
ytixelpreP
…
&
ecnamrofreP
yb
gniknaR
ytisreviD
&
ytixelpreP
…JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 8
Algorithm2K-shotGuidedSim-DivDataSelection
Embedding Space K-shot Data
Open-Source Dataset Open-Source Data Input: Asetofopen-sourceinstructionsamplesD ,K-shotdata
S
Q: Give three tips for staying
healthy. D K, encoding model h, pairwise similarity metric sim(·,·),
A: 1. Eat a balanced and Pre- databudgetC,andsimilaritythresholdτ
nutritious diet: Make sure …
trained Output: AugmenteddatasetD
A
K-Shot Data Encoder 1: Encode each (x i,y i) ∈ D S and (x j,y j) ∈ D K by h
Q: Who is likely to use a comb? respectivelyfortheembeddingsetsofU andU
A: A. Medicine cabinet… Similarity-Based Diversity-Based S K
B. Barber… Data Selection Data Selection 2: Compute the cross-dataset similarity matrix between D K
and D as A ∈ RK×S, where A = sim(u ,u ),u ∈
S ij i j i
Fig. 6. The overall pipeline of our K-shot guided sim-div data selec- U K,u j ∈U S
tion.WeextracttheembeddingsofbothK-shotandopen-sourcedata 3: Find the top C datapoints in D S that share the highest
throughpre-trainedmodels.Intheembeddingspace,samplesthatare similaritywithsamplesinK-shotD
close to K-shot are identified and prioritized for task augmentation. D∗ =argmax (cid:80) K (max A )
Simultaneously,toensuredatadiversity,weremovebatchesofdatawith C DC⊂DS,|DC|=C (xj,yj)∈DC i ij
near-duplicatesemantics. 4: Computetheintra-datasetsimilaritymatrixwithinD C asI ∈
RC×C,whereI =sim(u ,u ),u ∈U ,u ∈U ,U =
ij i j i C j C C
{u |(x ,y )∈D }
i i i C
Then, we pinpoint top C samples in D S that share the most 5: forall(x i,y i)∈D C,(x j,y j)∈D C,i̸=j do
similaritywithD
K
bymaximizingalongtherowsofA:
6: ifI ij >τ then
(cid:88) 7: ifmax rA ri >max rA rj then
D C∗ = DC⊂a Drg Sm ,|Da Cx
|=C
(xj,yj)∈DC(m iaxA ij). (13) 8 9:
:
elsD eC ←D C \{(x j,y j)}
The reasons why D is first selected out of D are two-fold: 1) 10: D C ←D C \{(x i,y i)}
C S
It ensures that the samples which enjoy a high level of similarity 11: endif
with D are prioritized while those dissimilar ones would not 12: endif
K
be unexpectedly introduced into the candidate set during the 13: endfor
subsequent selection. 2) It reduces the computation overhead of 14: return D A =D C
diversitymeasurementsincetheentiresetD shrinksintoamuch
S
smallercandidatesetD withC ≪S.
C
4 EXPERIMENTS
3.4.3 Diversity-Aware
4.1 ExperimentsSetup
We remove duplicates in D to improve its overall diversity for
C In this section, we validate the effectiveness of our proposed
the selected dataset D . A greater level of diversity not only
A method through a series of experiments on various tasks of
improves the token-wise cooperation between experts on broader
interest. To begin with, we use six popular open-source datasets,
domains and topics but also reduces the overfitting of the MoE
namelyARC-Challenge,ARC-Easy[22],PiQA[12],BoolQ[21],
systemonK-shotdatapoints.Specifically,weremoveasubsetof
MBPP[6],andGSM8K[23],toserveasourevaluationsetsunder
data with excessively high semantic similarity. We use the same
tasksofinterest.Thesedatasetscoveradiverserangeoffields,in-
distance metric defined in Eq. 11 and compute the intra-dataset
cludingexamination,knowledgequestion-answer(QA),common
similarity matrix I. The pairwise similarity is measured on both
sensereasoning,mathematicalproblems,andcodegeneration.For
u ∈U andu ∈U ,whereU ={u |(x ,y )∈D }:
i C j C C i i i C eachdataset,werandomlysampleKlabeledinstruction-response
pairs from its official training set as K-shot. In consideration
I =sim(u ,u ). (14)
ij i j of the response format, we employ a post-processing approach
customized for each task to standardize the outputs for fair
We follow the SemDeDup [2], [86] to perform semantic
comparison.ForexaminationandQAdatasetsintheLoRAbank
deduplicationbythresholdingwithτ.Ifthesimilaritybetweenany
whose ground-truth answers are multiple-choices, we implement
two instructions exceeds τ, we discard the one whose similarity
therationaleexpansionandtrainthecorrespondingLoRAmodels
withK-shotD islower.Bythismeanswemaintaintheoverall
K
on the CoT-formatted answers for improved task comprehension.
diversity of the final selected dataset D . The entire process of
A
Details about the datasets and the performance of each LoRA
dataselectioniselaboratedinAlg.2.
modelcanbefoundinthesupplementarymaterials.
To enhance the performance of our MoE system, we employ
3.5 Mixture-of-ExpertsFine-Tuning data augmentation by introducing additional datasets for data
We combine the augmented dataset D and the K-shot dataset selectionandMoEfine-tuning(seeSecs.3.4and3.5).Specifically,
A
D for optimizing both the routing weights and the experts of CommonSenseQA[85]andSiQA[77]areinvolvedformultiple-
K
our MoE system θ , where the cross-entropy loss is employed choiceQAtasks(ARC-Challenge,ARC-Easy,PiQA,andBoolQ).
MoE
tosupervisethelanguagemodelingontheoutputsofthelastL-th We choose CommonSenseQA and SiQA in data augmentation
MoElayer(Eq.8): for two reasons: 1) they share the same output formats with
the downstream tasks in the testing sets; 2) they span a wide
|y|
(cid:88) variety of common knowledge and social situations. While for
L(x,y,θ )=− logP(y |x,y ;θ ), (15)
MoE (i) (<i) MoE code generation (MBPP) and mathematical reasoning (GSM8K)
i=1
tasks,wechooseWizardLM[100]becausethedatasetitselfclaims
whereP(y |x,y ;θ )=softmax(gL([x,y ])) . to be effective in improving LLMs in solving coding and maths
(i) (<i) MoE (<i) y(i)JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 9
TABLE1
ComparisonwithbaselinesandSOTAmethodsonsixdownstreamtasks.TheDK,DA,andDT respectivelydenotetheK-shotdata,the
selectedopen-sourcedataforaugmentation,andtheentiretrainingsetofthedownstreamtasks.
Model Method MoE K-shot External ARC-c ARC-e BoolQ PiQA GSM8K MBPP Avg.
BaseModel × × × 33.90% 49.91% 47.86% 53.05% 16.68% 18.20% 36.60%
Random × DK × 40.11% 58.42% 60.12% 55.47% 18.53% 20.00% 42.11%
ExpertLoRA × DT DT 44.07% 62.61% 63.39% 56.58% 22.88% 22.40% 45.32%
SourceBest × × × 49.49% 69.31% 65.81% 61.53% 22.88% 24.60% 48.94%
LLaMA2-7B SourceBestSFT × DK DA 55.13% 72.14% 68.91% 64.32% 23.82% 24.40% 51.45%
MixLoRA[45] ✓ DK × 41.24% 57.92% 60.56% 54.79% 18.53% 20.20% 42.21%
LoRAHub[33] ✓ DK × 43.01% 57.43% 62.91% 53.12% 19.34% 23.80% 43.27%
PEMs[108] ✓ DK DA 50.12% 69.43% 67.12% 63.21% 19.83% 24.00% 48.95%
Ours ✓ DK DA 57.76% 73.60% 69.45% 65.13% 24.83% 24.20% 52.50%
BaseModel × × × 60.68% 73.54% 55.96% 57.67% 45.56% 36.00% 54.90%
Random × DK × 68.81% 81.31% 69.30% 66.97% 47.54% 36.80% 61.79%
ExpertLoRA × DT DT 77.97% 90.30% 80.12% 68.12% 49.73% 37.60% 67.31%
SourceBest × × × 80.00% 90.30% 80.12% 71.76% 51.55% 40.60% 69.06%
Mistral-7B SourceBestSFT × DK DA 78.43% 88.76% 83.29% 76.01% 52.43% 40.00% 69.82%
MixLoRA[45] ✓ DK × 69.15% 83.42% 74.53% 67.93% 48.67% 37.80% 63.58%
LoRAHub[33] ✓ DK × 69.21% 84.14% 80.24% 65.31% 47.94% 39.60% 64.41%
PEMs[108] ✓ DK DA 79.84% 89.87% 87.41% 74.07% 48.43% 39.60% 69.87%
Ours ✓ DK DA 81.43% 92.29% 89.71% 78.89% 52.91% 41.40% 72.77%
problems. The selection of open-source data for augmentation D of each task of interest (Expert LoRA), 4) the expert in the
T
adherestotheprinciplethatno data leakageoccurswithrespect LoRA bank that achieves the best evaluation results respectively
to the testing sets. The availability and quality of these datasets on the testing set of each downstream task (Source Best), and 5)
are guaranteed according to the feedback from developers in the each source best expert fine-tuned on the same augmented open
community. In total, a mixture of K-shot data from downstream datasets with ours (Source Best SFT). It is noted that baselines
tasksandtheselecteddatafromopen-sourcedatasetsareutilized of Random, Expert LoRA, Source Best, and Source Best SFT
forfine-tuningourMoEsystem. are optimized task-by-task, and therefore their results on each
individualtaskarereportedrespectively.NotethatfortheRandom
4.2 ImplementationDetails method,randomsamplingisperformedthreetimesandresultsare
averagedoverthreefine-tunedmodels.
For preparation of 38 models in our LoRA bank, both LLaMA2-
Furthermore,wecomparewithseveralSOTAmethodsinclud-
7B (Base) [87] and Mistral-7B (Base) [36] are investigated in
ing: 1) training a MoE system with randomly initialized LoRA
the present study. We empirically set the number of candidate
models and routers (MixLoRA) [45], 2) combining pre-trained
models M = 8, the number of chosen experts N = 4, and the
LoRA models and fine-tuning only the routers with K-shot data
number of selected experts k = 2. By default, we set K = 50
(LoRAHub)[33],3)performingmodelmergingandfine-tuningon
for K-shot data. Due to the randomness of sampling K-shot as
LoRAmodels(PEMs)[108].Allthesemethodsunearthavailable
theseedinstructions,weperformsamplingthreetimesandreport
LoRA models for maximizing the generalization of LLMs either
the averaged experimental results with three different sets of K-
on unseen domains and tasks or under task-specific supervised
shot to ensure the reliability of testing. For data augmentation,
scenarios. Out of comparability, we implemented SOTA methods
the total number of open-source samples S is respectively 43K
with their officially released codes. We adopted the same hyper-
and143KforthecombinedCommonSenseQAandSiQA,andthe
parametersettingswithoursforthesemethodsexceptLoRAHub,
WizardLM. By default, the data budget C is set to 1K and the
whereitsdefaulthyper-parametersarekeptunchanged.
similarity threshold is τ = 0.9. During experiments, we set the
hyper-parameters following [90] for training the MoE system: a
4.4 ResultsonTasksofInterest
batchsizeof2,gradientaccumulationstepsof16,andalearning
4.4.1 ComparisonwithBaselines
rateof5e-5.Wefine-tuneallMoEmodelsforfiveepochsandtheir
TheexperimentalresultscanbefoundinTable1.Thecomparison
convergenceisguaranteed.Wedonotobservefurtherbenefitswith
between LoRA and MixLoRA is straightforward, as it highlights
longer training. For all LoRA models incorporated in the LoRA
the importance of our model selection. The rational use of open-
Bank, the rank was set to 16. To optimize memory utilization,
source knowledge for specific tasks proves to be superior to the
the employed training setting was the Deepspeed [76] zero stage
single LoRA and the vanilla mixing policy in MixLoRA. Lo-
3. All models were trained with PyTorch [70] Transformers [95]
RAHubisaschemethatalsomaintainsanopen-sourceknowledge
(version4.36.1)onNVIDIAV100GPUswithFloat16mode.
base. PEMs is a strategy that directly merges model parameters,
Explanationsonallthesymbolsandthesettingsofallhyper-
whichisintuitivebutlacksrationality.Inaddition,thesemethods
parameterscanbefoundinthesupplementarymaterials.
rarely consider the impact of the diversity between experts on
theoverallsystem.Ourmethodoutperformsexistingapproaches,
4.3 Baselines
showcasingitseffectiveness.
We compare the proposed method with five vanilla baselines: 1)
thepre-trainedbase(BaseModel),2)therandomlyselectedmodel 4.4.2 VisualizationofExperts
fromtheLoRAbankfine-tunedonlyonK-shotdata(Random)3) The activation patterns of individual experts are illustrated in
the base model respectively fine-tuned on the entire training set Fig. 7. For each layer, we calculate the average activation rateJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 10
TABLE2
AblationstudyonthemodelselectionstrategyforinitializationoftheMoEsystem.TheRP,RA,andRDrespectivelyrepresenttherankingby
reasoningperplexity,therankingbyperformanceinaccuracy,andtherankingbygroupdiversity.
Perp- Perfor- Group
Model Method MoE ARC-c ARC-e BoolQ PiQA GSM8K MBPP Avg.
lexity mance Diversity
BaseModel × × × × 33.90% 49.91% 47.86% 53.05% 16.68% 18.20% 36.60%
Random ✓ × × × 47.81% 63.42% 64.19% 55.26% 17.87% 22.40% 45.16%
LLaMA2-7B Ours ✓ ✓ ✓ R R×P P RR× AA × × × 5 5 54 5 4. . .3 1 62 6 4% % % 6 6 78 9 1. . .5 3 94 2 4% % % 6 6 69 6 7. . .7 1 91 5 9% % % 6 6 63 2 5. . .3 3 39 2 2% % % 2 2 23 3 3. . .7 7 41 1 9% % % 2 2 24 4 3. . .0 0 40 0 0% % % 5 5 50 0 1. . .6 1 11 1 3% % %
✓ RP RA RD 57.76% 73.60% 69.45% 65.13% 24.83% 24.20% 52.50%
BaseModel × × × × 60.68% 73.54% 55.96% 57.67% 45.56% 36.00% 54.90%
Random ✓ × × × 76.51% 84.41% 85.03% 70.99% 47.83% 39.00% 67.29%
Mistral-7B Ours ✓ ✓ ✓ R R×P P RR× AA × × × 7 8 79 0 8. . .3 3 62 6 4% % % 8 9 89 1 8. . .3 1 11 2 2% % % 8 8 86 3 4. . .9 3 11 3 1% % % 7 7 75 6 7. . .1 1 24 9 1% % % 4 4 59 9 2. . .3 2 67 6 5% % % 4 4 40 0 0. . .4 2 20 0 0% % % 7 7 70 0 0. . .0 0 18 8 6% % %
✓ RP RA RD 81.43% 92.29% 89.71% 78.89% 52.91% 41.40% 72.77%
GSM8K ARC-c 4.5.2 The selection of promising models by comprehen-
siveconsiderationofperformance,reasoningperplexity,and
group diversity outperforms the vanilla, simple ones that
solelyemphasizeoneaspect
Firstly,wefoundthattheperformanceofLoRAmodelsrandomly
selectedfromtheLoRAbankfortasksofinterestisincompetent.
This suggests that only the most relevant set of models can
enhance the performance of specific tasks. As shown in Table 2,
Layers Layers ourproposedmodelselectionstrategyoutperformstheapproaches
thatsolelyrelyonK-shotperformanceandreasoningperplexity.
Fig.7.Theactivationpatternsofexpertsacrosslayers.Thedistribution Moreover,usingasinglemodelwitheitherthelowestperplexityor
oftheroutingpolicyisrelativelysparseonboththetasksofGSM8Kand
themaximizedperformanceforidenticalinitializationofourMoE
ARC-c.
system (also known as upcycling) results in lower performance.
Compared with a group of experts with diversity concerns, the
of each expert. Specifically, for each token in a sequence, we MoE system initialized by the same experts only achieve similar
only assign “1” to the chosen experts and “0” to the remaining resultswithSourceBestSFT,indicatingthathighlysimilarmodels
ones.Then,wecalculatetheproportionsoftokensthatarerouted aredetrimentaltothetrainingofMoE.
through each expert, where the number of tokens per expert is
normalized over that of all experts. Such a proportion of expert
activationreflectsthelearningpreferenceandtheeffectivenessof
the MoE system. We observe that on both the GSM8K and the
ARC-c datasets, the distribution of activation is relatively sparse
across layers and each expert is nearly uniformly activated over
theentiredatasets.SuchactivationpatternsconfirmthattheMoE
system does not collapse equivalently into a single model, where
the routing mechanism responds to tokens adaptively and each
Reasoning Perplexity Vanilla Perplexity
expertcontributestotheoverallMoEsystem.
Fig. 8. Comparison between the model selection strategies with the
vanilla perplexity on answer choices and the reasoning perplexity on
4.5 AblationStudyonModelSelection
CoT rationales of the ARC-c dataset. A high negative correlation is
observed between the performance (exact match accuracy) and the
4.5.1 Baselines
reasoningperplexity.Onthecontrary,itisdifficulttoaccuratelyidentify
In terms of our model selection strategy, four baselines are promisingcandidateexpertsfromthevanillaperplexityduetoitsweak
associationwithperformance.
introduced for comparison: 1) random selection of models from
the LoRA Bank (Random), 2) selection of top-ranked models
merely by reasoning perplexity (R ), 3) selection of top-ranked
P
4.5.3 Thereasoningperplexityoutperformsthevanillaper-
modelsmerelybyperformanceinaccuracy(R ),and4)selection
A
plexityinmodelselection
of top-ranked models by perplexity and performance without
consideringintra-groupdiversity(R ).TheselectedN expertsby We compared the reasoning perplexity and the vanilla perplexity
D
each method are used to initialize a MoE system and fine-tuned (withoutCoTrationales)formodelselection.AsshowninFig.8,
following the same data augmentation pipeline on D ∪ D . with the expanded CoT process, the reasoning perplexity of
K A
For the Random method, we also perform sampling three times candidatemodelshasahighercorrelationwiththeirperformance
for initialization and fine-tuning of three MoE systems. Their on testing sets. Consequently, model selection by the reasoning
averagedresultsarereportedhere. perplexityismorerobustthanthatbythevanillaperplexity.
xednI
trepxE
xednI
trepxE
ycaruccA ycaruccAJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 11
TABLE3
Ablationstudyonthedataselectionstrategyforaugmentationinfine-tuningtheMoEsystem.
Model Method Similarity Diversity ARC-c ARC-e BoolQ PiQA GSM8K MBPP Avg.
K-shotOnly × × 79.32% 88.67% 79.23% 73.21% 46.67% 40.00% 67.85%
Random × × 78.42% 84.56% 83.12% 71.21% 48.96% 38.80% 67.51%
Cosine × 82.13% 90.30% 90.01% 75.73% 51.55% 42.40% 72.02%
Cosine ✓ 81.43% 92.29% 89.71% 78.89% 52.91% 41.40% 72.77%
Mistral-7B Ours Convex ✓ 82.02% 91.45% 91.43% 79.53% 52.84% 41.40% 73.11%
KDE(γ=2) ✓ 83.45% 92.34% 88.53% 77.92% 53.43% 41.40% 72.85%
KDE(γ=10) ✓ 80.43% 92.74% 90.55% 79.43% 53.92% 42.00% 73.18%
4.6 AblationStudyonDataSelection
Ablation on Diversity (Mistral-7B) Ablation on Diversity (LLaMA2-7B)
4.6.1 Baselines
To validate the effectiveness of our data selection strategy, we
compare with MoE systems fine-tuned on: 1) K-shot data only
(K-shotOnly),2)randomlyselecteddatafromD (Random),3)
S
dataselectedbycosinesimilaritywithoutconsideringdiversity,4)
data selected by convex hull with the proposed diversity control,
5)dataselectedbykerneldensityestimation(KDE)[19]withthe
Data Budget C Data Budget C
proposed diversity control, 6) data selected by cosine similarity
Ablation on Quantity (Mistral-7B) Ablation on Quantity (LLaMA2-7B)
with Repr Filter-based diversity control [55]. All methods were
performedonthesameinitializedMoEsystembutusingdifferent
D .ExceptforK-shotMoE,wesetthedatabudgetC tobethe
A
same as 1K for all methods. Due to diversity control, Repr Filter
andtheproposedmethoddiscardduplicatesandreducethesizeof
D foraslightlysmallerD .
C A
To mathematically describe the convex hull-based data selec-
tion,wefirstdefinetheconvexhullU KC ofthegivenK-shotdata Data Budget C Data Budget C
D withtheirembeddingsU :
K K
Fig.9.AsthedatabudgetCincreases,theperformanceexhibitsatrend
K n
UC ={u|u=(cid:88) λ u ,u ∈U ,λ ≥0,(cid:88) λ =1}, (16) of initially rising and then declining. It indicates that the augmentation
K i i i K i i bytrulyrelevantdataimprovesperformance.However,thedominance
i=1 i=1 of irrelevant, redundant samples degrades performance as they dilute
the contributions of K-shot similar samples. Diversity also plays an
where λ are the coefficients that determine the convex com-
i important role in balancing distributions, which alleviates overfitting in
bination of samples in U K. Then, the convex-hall-based data fine-tuningtheMoEsystem.
selection is performed by randomly sampling a subset D ⊂
C
D ,|D |=CwhoseembeddingsarelocatedinsideUC,namely
S C K
u ∈UC,u ∈U holdstruefor∀(x ,y )∈D . performanceofourMoEontasksofinterest.However,asshown
j K j C j j C
The kernel density estimation (KDE) is a non-parametric inFig.9,withtheincreasingamountofdataD A,theperformance
geometric technique to estimate the probability density function exhibitsapatternofaninitialgrowthfollowedbyadecline.Sucha
ofarandomvariable.GiventheK-shotD andtheirembeddings trendindicatesthatatthebeginning,theinvolvementofmoretask-
K
U , the estimated density probability of any sample u in the relevantopendataexertsaninstantpositiveeffectonperformance,
K
embeddingspaceispresentedby: which not only brings in knowledge but also stabilizes MoE
optimization. Accompanied by the enlarged dataset, an excessive
p(u)=
1 (cid:88)K Ker(cid:18)u−u i(cid:19)
,u ∈U , (17)
amount of irrelevant data begin to dominate. They dilute the
K·γ γ i K contribution of K-shot related datapoints and easily cause the
i=1
system’sforgettingoftask-specificskillsandknowledge.
where Ker(·) denotes the Gaussian kernel function for distance Additionally, we examined the role of diversity in data se-
measurement and γ is the bandwidth controlling the smoothness lection. Fig. 9 reveals that filtering out data with high semantic
ofthedensityfunction.ForKDE-baseddataselection,wesample similarity leads to a noticeable improvement in the model’s
a subset D C ⊂ D S,|D C| = C according to the probability performance. Especially when the number of data budget C
p(u j),u j ∈ U S. Samples that are closer to K-shot data on increases,deduplicationbecomesanindispensablesteptomitigate
averageareofhigherlikelihoodtobeselected. overfitting.
4.6.2 The open-source data augmentation by similarity- 4.6.3 The implementation of similarity-based selection by
first and diversity-aware selection further improves the MoE cosinedistancecanbereplacedwithothercompetitivegeo-
system metricsamplingtechniques
We investigated the performance of fine-tuning under different Besides, we conducted a comparison between different sampling
data augmentation scenarios. As shown in Table 3, data augmen- schemes,includingthevanillaselectionbyminimizingcosinedis-
tation based on cosine similarity alone effectively enhances the tance(Eq.13),randomsamplingwithintheconvexhullenclosed
ycaruccA
ycaruccA
ycaruccA
ycaruccAJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 12
TABLE4
AblationstudyontheKofK-shot.
Model Method ARC-c ARC-e BoolQ PiQA GSM8K MBPP Avg.
K =1 50.68% 65.08% 62.14% 61.53% 17.29% 20.40% 46.19%
K =5 57.00% 72.64% 68.41% 64.10% 22.98% 23.80% 51.49%
LLaMA2-7B
K =20 57.49% 73.39% 69.01% 64.82% 24.57% 24.00% 52.20%
K =50 57.76% 73.60% 69.45% 65.13% 24.83% 24.20% 52.50%
K =1 76.27% 89.24% 83.73% 75.77% 45.56% 37.80% 68.06%
K =5 80.72% 90.98% 89.12% 78.31% 51.92% 40.80% 71.98%
Mistral-7B
K =20 81.02% 91.27% 89.53% 78.75% 52.85% 41.20% 72.43%
K =50 81.43% 92.29% 89.71% 78.89% 52.91% 41.40% 72.77%
Evaluation Results on Experts Mixed Sample Performance
Cosine Convex Hull All Correct All Incorrect Mixed Case Index
Fig. 11. Evaluation results of each expert (Mistral-7B) in a fine-tuned
MoEsystemontheARC-cdataset.AllCorrectandAllIncorrectrespec-
tivelyrepresentthenumberofcaseswhereallexpertssucceedandfail.
Mixedrepresentsthenumberofcaseswhereatleastoneexpertperform
correctlyandatleastoneincorrectly.Foreachtestingcase,correctand
incorrectexpertsarerespectivelyhighlightedingreenandred.
increase of K from 5 to 50 further improves the performance
KDE (𝛾=2) KDE (𝛾=10) of the overall MoE system. However, the gains are diminishing,
suggesting the trade-off between the efforts of collecting more
Fig.10.T-SNEvisualizationofdifferentsimilarity-basedselectionmeth- K-shotsandtheirultimateprofitsshouldbeconsidered.
odsincludingcosinedistance,convexhull,andKDE.Bluedots,green
dots,andorangedotsrespectivelyrepresenttheentireopen-sourcedat- 4.7.2 The dissimilarity between expert candidates
apoints,theK-shotdata,andtheselectedCsamplesforaugmentation.
is the key to maintaining a task-oriented knowledge-
supplementaryMoEsystem
by K-shot data (Eq. 16), and sampling based on the estimated We explored the diversity among expert models for building a
densityprobability(Eq.17).GiventhesamebudgetC,wepresent successfulMoEsystem.AsshowninFig.11,wedividethetesting
a t-SNE [88] visualization of the selected datapoints to highlight cases of the ARC-c into all correct, all incorrect, and mixed,
thedifferencesbetweenthesemethods(seeFig.10). wheretheconstitutingexpertsrespectivelyallsucceed,allfail,and
Table 3 reports that the convex hull-based sampling achieves perform in between. For all correct cases, all experts are capable
relativelygoodresultsinreasoningtaskssuchasBoolQandPiQA. of independently solving these cases without external assistance.
KDE behaves more robustly as it tend to select samples encom- While for all incorrect cases, case-by-case knowledge injection
passingK-shotasclustercenters.Especiallyonmathandcoding becomes a requisite to master the missing preliminaries. For the
tasks,theKDE-basedsamplingimprovestheinitialMoEthemost mixed cases, it is the judgement by the routing that resolves
becauseitsimultaneouslyintroducessamplesaroundK-shotand disagreement between experts and makes the best use of model
those extrapolated along boundaries, effectively expanding the ensemble for improvement. Table 5 reports the error rate of each
knowledge. Furthermore, we notice that the bandwidth γ affects expertandtheMoEsystemontheARC-c.Itcanbeobservedthat
therateofattenuationindensityestimation.Alargerγresultsina each expert is adept at different testing cases, exhibiting a varied
sparserdistributionoftheselecteddata,allowinggreaterdiversity. distribution across experts. Besides, the error rate of the overall
MoEsystemismuchlowerthanthatofeachexpert,confirmingthe
advantages of knowledge expansion. During model selection, the
4.7 AblationStudyonK,N,andk
heterogeneityofexpertsmeasuredbytheirparametersguarantees
4.7.1 The marginal benefits of increasing K on perfor-
the success of a constructed MoE, where the knowledge of each
mancedecrease
expertsupplementstotheothers.
WeexperimentedfromK =1toK =50(seeTable4)andfound
thatourapproachalreadyachieveshighlycompetitiveresultseven 4.7.3 The amount of data points for fine-tuning the MoE
underK = 5.Itdemonstratesthattheproposedmethodrequires systemshouldbeoptimizedforthetaskofinterest
only a small amount of human-verified data to quickly produce TheamountofdatarequiredtoeffectivelytrainaMoEsystem,es-
expertmodelswithstrongprofessionalcapabilities.Thecontinual peciallythetoken-wiseroutingmechanism,remainsanintriguing
tnuoCJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 13
TABLE5 4.7.5 The optimal number of selected experts is not con-
ErrorratesoftheMoEsystemanditsfourexperts(Mistral-7B)onthe sistentacrossmodelarchitectures
mixedcasesoftheARC-c,whereatleastoneexpertsucceedsand
oneexpertfailsoneachsample. We initialized the MoE system with four experts, and fine-tuned
four MoE models respectively with top-1, top-2, top-3, and all
Model Expert1 Expert2 Expert3 Expert4 MoE selected experts during routing. Table 7 confirms that when only
Error 55.0% 62.5% 37.5% 45.0% 32.5% one expert is selected, the optimization of the MoE system is
sub-optimal. With an increase in the number of selected experts
k, the performance gradually improves. However, we notice that
questiontodiscuss.Inthepresentstudy,weutilizedatotalof1K the optimal number of selected experts is different between the
dataforfine-tuningtheMoEsystembydefault.Comparedwitha LLaMA and Mistral models. Meticulous efforts on choosing the
singleLoRAmodel,theMoEsystemyieldsanincreaseof+1.05% number of selected experts might be needed to achieve the best
on LLaMA and +2.95% on Mistral under the same data volume. performanceofthegivenMoEsystem.
As confirmed in Fig. 7, the activation of routers is adequately
dispersedacrosslayers.Itsuggeststhattheoptimizationofrouting
weights converges even with 1K data. Moreover, we believe that
5 DISCUSSION
thequantityoflearnableparametersandroutingweightsinaMoE 5.1 Non-useofMetaInfofromtheDatasetsandModels
system should be positively correlated with the volume of data.
Inthepresentstudy,wedonotrelyonmetadata(e.g.,descriptions)
Forlargerlanguagemodels(e.g.,70Band110B),thedatabudget
to select datasets and models for the following three reasons.
C should exceed 1K and potentially reaches up to 10K. In our
First,thedescriptionsofvariousopen-sourcemodelsanddatasets
preliminary experiments, we also find that the complexity of the
are often insufficiently detailed. A thorough understanding of the
downstream tasks is worthy of consideration. For simpler tasks
datasetcompositionormodelcharacteristicsdemandsdelvinginto
such as multiple-choice QA, the appropriate data volume hovers
the semantics of datapoints and the representations of the model
around 0.5K-1K. On the contrary, for challenging tasks such as
layers. Second, each dataset may consist of subsets spanning a
math and coding problems, 5K-10K data are encouraged. The
wide range of domains and tasks. Identifying the most relevant
perplexity of each task by measuring the IFD score [48] of K-
datapointsnecessitatesdataselectionintheembeddingspacefor
shotmightbeindicativeinsettingtheoptimalC.
retainingonlysamplesthatcontributetomasteringtask-dependent
skills. Third, it is challenging to keep track of all the constantly
TABLE6 updating metadata on the internet due to privacy and copyrights
AblationstudyonthenumberofexpertsN.Resultsonalldownstream issues.Ontheotherhand,itismorefeasibletoonlyaccessdatasets
tasksareaveraged.
andmodelsaslongastheyareavailableonline.
NumberofExpertsN
Model 5.2 ApplicabilityandAvailability
2 4 6 8
The proposed comprehensive, scalable pipeline begins with the
LLaMA2-7B 52.19% 52.50% 52.43% 50.49%
collectionandpreparationofdatasetsandmodels,followedbythe
Mistral-7B 70.45% 72.77% 72.19% 69.84%
selection of expert candidates and data augmentation for down-
streamtasks,andculminatesattheconstructionandoptimization
of a MoE system for the optimized task expertise. Note that a
majority of open-source LLMs are variants of the LLaMA and
4.7.4 Thenumberofexpertsdoesnotneedtobelargefor
Mistral families that undergo instruction tuning and preference
highcapacity
alignment in different ways. A simple search on Huggingface
In Table 6, we observe that a small number (e.g., four) of high- retrieveshundredsofmodelsoftheidenticalsizeandarchitecture,
qualityexpertscanyieldbetterperformanceinaMoEsystem.One profitfromthecontributionsoftheopen-sourcecommunitytothe
reasonbehindthedegradedperformancewithanincreasingnum- developmentofLLMs.Therefore,theavailabilityofalargemodel
berofexpertsisthatthetrainingofaMoEsystemwithnumerous bankisensuredinpracticalscenarios.Itisnotedthatinthepresent
experts requires more open-source datasets during optimization. study,LoRAmodules,namelytheoffsetstotheparametersofthe
Under the same K-shot settings with data augmentation, the basemodel,areinvestigatedasthemainexistingformsofmodel
optimizationofaneight-expertMoEsystemismorechallenging. parameters. However, there exist numerous open models that are
It becomes more difficult for the routing itself to converge with developed with other PEFT techniques such as adapters. Our
fewerdatapoints. approach necessitates the computation of inter-model similarity
between weights, which may not be compatible across different
TABLE7 PEFTmethods.WeleavetheissueofapplicabilitybeyondLoRA
Ablationstudyonthenumberofselectedexpertskina4-expertMoE. modulestobeaddressedinthefuturework.
Resultsonalldownstreamtasksareaveraged.
6 CONCLUSION
NumberoftheSelectedExpertsk
Model
In this study, we developed an efficient and scalable pipeline
1 2 3 4
to fully utilize K-shot data from downstream tasks of interest
LLaMA2-7B 49.93% 52.50% 51.19% 52.89%
for augmentation of existing LLMs in task expertise. The K-
Mistral-7B 71.82% 72.77% 73.12% 71.64%
shot samples play an important role in both model selection and
data augmentation. Specifically, we comprehensively assess theJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 14
capability of each expert candidate on K-shot data by simulta- [15] Yihan Cao, Yanbin Kang, Chi Wang, and Lichao Sun. Instruction
neously measuring its performance in exact match accuracy and mining: When data mining meets large language model finetuning.
arXivpreprintarXiv,2307,2023.
its reasoning perplexity via auto-regressive language modeling.
[16] Hao Chen, Yiming Zhang, Qi Zhang, Hantao Yang, Xiaomeng Hu,
We bring in diversity for extending the knowledge boundary
XuetaoMa,YifanYanggong,andJunboZhao. Maybeonly0.5%data
enclosedbytheselectedexperts,whichlaysasolidfoundationfor is needed: A preliminary exploration of low training data instruction
initializingaflexibleMoEsystem.Foroptimizingthetoken-wise tuning. arXivpreprintarXiv:2305.09246,2023.
[17] Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and
collaboration between experts, we propose to fine-tune the MoE
Zheng Liu. Bge m3-embedding: Multi-lingual, multi-functionality,
with a data augmentation technique. Under such circumstance,
multi-granularitytextembeddingsthroughself-knowledgedistillation.
a similarity-first, diversity-aware data selection method is devel- arXivpreprintarXiv:2402.03216,2024.
oped where the K-shot data steer the selection of task-relevant [18] LichangChen,ShiyangLi,JunYan,HaiWang,KalpaGunaratna,Vikas
Yadav,ZhengTang,VijaySrinivasan,TianyiZhou,HengHuang,etal.
samples in the embedding space. Diversity is also highlighted by
Alpagasus: Training a better alpaca with fewer data. arXiv preprint
removingthesemanticduplicatesforalleviatingoverfittingunder arXiv:2307.08701,2023.
data scarce scenarios. Experimental results demonstrate that the [19] Yen-Chi Chen. A tutorial on kernel density estimation and recent
proposedmethodoutperformsexistingapproachesinproducinga advances. Biostatistics&Epidemiology,1(1):161–187,2017.
[20] Alexandra Chronopoulou, Matthew E Peters, Alexander Fraser, and
MoEsystemofspecifictaskexpertise.Extensiveablationstudies
JesseDodge.Adaptersoup:Weightaveragingtoimprovegeneralization
confirm the validity of the proposed selection methods in pin-
ofpretrainedlanguagemodels.arXivpreprintarXiv:2302.07027,2023.
pointing the most promising models and appropriate data with [21] ChristopherClark,KentonLee,Ming-WeiChang,TomKwiatkowski,
K-shot, demonstrating a cost-efficient pipeline to excavate open Michael Collins, and Kristina Toutanova. Boolq: Exploring the
surprising difficulty of natural yes/no questions. arXiv preprint
knowledgeforcustomizedskillconsolidation.
arXiv:1905.10044,2019.
[22] PeterClark,IsaacCowhey,OrenEtzioni,TusharKhot,AshishSabhar-
wal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved
REFERENCES questionanswering?tryarc,theai2reasoningchallenge.arXivpreprint
arXiv:1803.05457,2018.
[1] Winogrande:Anadversarialwinogradschemachallengeatscale.2019. [23] KarlCobbe,VineetKosaraju,MohammadBavarian,MarkChen,Hee-
[2] Amro Abbas, Kushal Tirumala, Da´niel Simig, Surya Ganguli, and wooJun,LukaszKaiser,MatthiasPlappert,JerryTworek,JacobHilton,
AriSMorcos. Semdedup:Data-efficientlearningatweb-scalethrough ReiichiroNakano,etal.Trainingverifierstosolvemathwordproblems.
semanticdeduplication. arXivpreprintarXiv:2303.09540,2023. arXivpreprintarXiv:2110.14168,2021.
[3] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge [24] AlexisConneau,KartikayKhandelwal,NamanGoyal,VishravChaud-
Akkaya,FlorenciaLeoniAleman,DiogoAlmeida,JankoAltenschmidt, hary,GuillaumeWenzek,FranciscoGuzma´n,EdouardGrave,MyleOtt,
Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv Luke Zettlemoyer, and Veselin Stoyanov. Unsupervised cross-lingual
preprintarXiv:2303.08774,2023. representationlearningatscale. CoRR,abs/1911.02116,2019.
[4] Shourya Aggarwal, Divyanshu Mandowara, Vishwajeet Agrawal, Di- [25] TimDettmers,ArtidoroPagnoni,AriHoltzman,andLukeZettlemoyer.
nesh Khandelwal, Parag Singla, and Dinesh Garg. Explanations for Qlora: Efficient finetuning of quantized llms. Advances in Neural
commonsenseqa: New dataset and models. In Proceedings of the InformationProcessingSystems,36,2024.
59thAnnualMeetingoftheAssociationforComputationalLinguistics
[26] KaustubhDDhole,VarunGangal,SebastianGehrmann,AadeshGupta,
and the 11th International Joint Conference on Natural Language
Zhenhao Li, Saad Mahamood, Abinaya Mahendiran, Simon Mille,
Processing(Volume1:LongPapers),pages3050–3065,2021.
Ashish Shrivastava, Samson Tan, et al. Nl-augmenter: A frame-
[5] Aida Amini, Saadia Gabriel, Peter Lin, Rik Koncel-Kedziorski, Yejin workfortask-sensitivenaturallanguageaugmentation. arXivpreprint
Choi, and Hannaneh Hajishirzi. Mathqa: Towards interpretable math arXiv:2112.02721,2021.
wordproblemsolvingwithoperation-basedformalisms. arXivpreprint
[27] Qianlong Du, Chengqing Zong, and Jiajun Zhang. Mods: Model-
arXiv:1905.13319,2019.
oriented data selection for instruction tuning. arXiv preprint
[6] JacobAustin,AugustusOdena,MaxwellNye,MaartenBosma,Henryk
arXiv:2311.15653,2023.
Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry,
[28] JonDurbin. Airoboros. https://github.com/jondurbin/airoboros,2024.
QuocLe,etal. Programsynthesiswithlargelanguagemodels. arXiv
[29] WenfengFeng,ChuzhanHao,YueweiZhang,YuHan,andHaoWang.
preprintarXiv:2108.07732,2021.
Mixture-of-loras:Anefficientmultitasktuningforlargelanguagemod-
[7] JinzeBai,ShuaiBai,YunfeiChu,ZeyuCui,KaiDang,XiaodongDeng,
els. arXivpreprintarXiv:2403.03432,2024.
YangFan,WenbinGe,YuHan,FeiHuang,etal.Qwentechnicalreport.
[30] MorGeva,DanielKhashabi,EladSegal,TusharKhot,DanRoth,and
arXivpreprintarXiv:2309.16609,2023.
Jonathan Berant. Did aristotle use a laptop? a question answering
[8] LukasBalles,GiovanniZappella,andCe´dricArchambeau. Gradient-
benchmark with implicit reasoning strategies. Transactions of the
matchingcoresetsforrehearsal-basedcontinuallearning.arXivpreprint
AssociationforComputationalLinguistics,9:346–361,2021.
arXiv:2203.14544,2022.
[31] M Hayes, A Mathur, J Xie, J Wan, S Shah, A Ghodsi, P Wendell,
[9] SamyadeepBasu,PhilipPope,andSoheilFeizi. Influencefunctionsin
deeplearningarefragile. arXivpreprintarXiv:2006.14651,2020. MZaharia,andRXin. Freedolly:Introducingtheworld’sfirsttruly
openinstruction-tunedllm. 2023.
[10] Joshua Belofsky. Token-level adaptation of lora adapters for down-
stream task generalization. In Proceedings of the 2023 6th Artificial [32] EdwardJHu,YelongShen,PhillipWallis,ZeyuanAllen-Zhu,Yuanzhi
IntelligenceandCloudComputingConference,pages168–172,2023. Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank
[11] Gantavya Bhatt, Yifang Chen, Arnav M Das, Jifan Zhang, Sang T adaptationoflargelanguagemodels.arXivpreprintarXiv:2106.09685,
Truong, Stephen Mussmann, Yinglun Zhu, Jeffrey Bilmes, Simon S 2021.
Du, Kevin Jamieson, et al. An experimental design framework for [33] ChengsongHuang,QianLiu,BillYuchenLin,TianyuPang,ChaoDu,
label-efficient supervised finetuning of large language models. arXiv andMinLin. Lorahub:Efficientcross-taskgeneralizationviadynamic
preprintarXiv:2401.06692,2024. loracomposition. arXivpreprintarXiv:2307.13269,2023.
[12] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: [34] Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Suchin
Reasoningaboutphysicalcommonsenseinnaturallanguage. InPro- Gururangan, Ludwig Schmidt, Hannaneh Hajishirzi, and Ali Farhadi.
ceedingsoftheAAAIconferenceonartificialintelligence,volume34, Editingmodelswithtaskarithmetic. arXivpreprintarXiv:2212.04089,
pages7432–7439,2020. 2022.
[13] JonathanBrophy,ZaydHammoudeh,andDanielLowd. Adaptingand [35] JoelJang,SeungoneKim,SeonghyeonYe,DoyoungKim,Lajanugen
evaluatinginfluence-estimationmethodsforgradient-boosteddecision Logeswaran,MoontaeLee,KyungjaeLee,andMinjoonSeo.Exploring
trees. JournalofMachineLearningResearch,24(154):1–48,2023. thebenefitsoftrainingexpertlanguagemodelsoverinstructiontuning.
[14] Oana-MariaCamburu,TimRockta¨schel,ThomasLukasiewicz,andPhil InInternationalConferenceonMachineLearning,pages14702–14729.
Blunsom. e-snli: Natural language inference with natural language PMLR,2023.
explanations. AdvancesinNeuralInformationProcessingSystems,31, [36] AlbertQJiang,AlexandreSablayrolles,ArthurMensch,ChrisBamford,
2018. DevendraSinghChaplot,DiegodelasCasas,FlorianBressand,GiannaJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 15
Lengyel,GuillaumeLample,LucileSaulnier,etal. Mistral7b. arXiv InInternationalConferenceonMachineLearning,pages22631–22648.
preprintarXiv:2310.06825,2023. PMLR,2023.
[37] Xisen Jin, Xiang Ren, Daniel Preotiuc-Pietro, and Pengxiang Cheng. [58] KemingLu,HongyiYuan,RunjiLin,JunyangLin,ZhengYuan,Chang
Dataless knowledge fusion by merging weights of language models. Zhou,andJingrenZhou.Routingtotheexpert:Efficientreward-guided
arXivpreprintarXiv:2212.09849,2022. ensembleoflargelanguagemodels. arXivpreprintarXiv:2311.08692,
[38] Junmo Kang, Leonid Karlinsky, Hongyin Luo, Zhen Wang, Jacob 2023.
Hansen,JamesGlass,DavidCox,RameswarPanda,RogerioFeris,and [59] Max Marion, Ahmet U¨stu¨n, Luiza Pozzobon, Alex Wang, Marzieh
AlanRitter. Self-moe:Towardscompositionallargelanguagemodels Fadaee,andSaraHooker.Whenlessismore:Investigatingdatapruning
withself-specializedexperts. arXivpreprintarXiv:2406.12034,2024. forpretrainingllmsatscale. arXivpreprintarXiv:2309.04564,2023.
[39] TusharKhot,PeterClark,MichalGuerquin,PeterJansen,andAshish [60] FanxuMeng,ZhaohuiWang,andMuhanZhang. Pissa:Principalsin-
Sabharwal. Qasc: A dataset for question answering via sentence gularvaluesandsingularvectorsadaptationoflargelanguagemodels.
composition. In Proceedings of the AAAI Conference on Artificial arXivpreprintarXiv:2404.02948,2024.
Intelligence,volume34,pages8082–8090,2020. [61] Microsoft. Wizardlm-28x22b,2024.
[40] JohnKirchenbauer,GarrettHonke,GowthamiSomepalli,JonasGeip- [62] Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agar-
ing,DaphneIppolito,KatherineLee,TomGoldstein,andDavidAndre. wal, Hamid Palangi, and Ahmed Awadallah. Orca: Progressive
Lmd3: Language model data density dependence. arXiv preprint learning from complex explanation traces of gpt-4. arXiv preprint
arXiv:2405.06331,2024. arXiv:2306.02707,2023.
[41] Aran Komatsuzaki, Joan Puigcerver, James Lee-Thorp, Car- [63] MohammedMuqeeth,HaokunLiu,YufanLiu,andColinRaffel.Learn-
los Riquelme Ruiz, Basil Mustafa, Joshua Ainslie, Yi Tay, Mostafa ing to route among specialized experts for zero-shot generalization.
Dehghani, and Neil Houlsby. Sparse upcycling: Training mixture-of- arXivpreprintarXiv:2402.05859,2024.
experts from dense checkpoints. arXiv preprint arXiv:2212.05055, [64] MohammedMuqeeth,HaokunLiu,andColinRaffel. Softmergingof
2022. expertswithadaptiverouting. arXivpreprintarXiv:2306.03745,2023.
[42] GuokunLai,QizheXie,HanxiaoLiu,YimingYang,andEduardHovy. [65] ReiichiroNakano,JacobHilton,SuchirBalaji,JeffWu,LongOuyang,
Race: Large-scale reading comprehension dataset from examinations. Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju,
arXivpreprintarXiv:1704.04683,2017. WilliamSaunders,etal. Webgpt:Browser-assistedquestion-answering
[43] Matthew Lamm, Jennimaria Palomaki, Chris Alberti, Daniel Andor, withhumanfeedback. arXivpreprintarXiv:2112.09332,2021.
EunsolChoi,LivioBaldiniSoares,andMichaelCollins.Qed:Aframe- [66] Yasumasa Onoe, Michael JQ Zhang, Eunsol Choi, and Greg Durrett.
workanddatasetforexplanationsinquestionanswering. Transactions Creak: A dataset for commonsense reasoning over entity knowledge.
oftheAssociationforcomputationalLinguistics,9:790–806,2021. arXivpreprintarXiv:2109.01653,2021.
[44] BrianLester,RamiAl-Rfou,andNoahConstant.Thepowerofscalefor [67] Oleksiy Ostapenko, Zhan Su, Edoardo Maria Ponti, Laurent Charlin,
parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691, Nicolas Le Roux, Matheus Pereira, Lucas Caccia, and Alessandro
2021. Sordoni. Towards modular llms by building and reusing a library of
[45] DengchunLi,YingziMa,NaizhengWang,ZhiyuanCheng,LeiDuan, loras. arXivpreprintarXiv:2405.11157,2024.
Jie Zuo, Cal Yang, and Mingjie Tang. Mixlora: Enhancing large [68] SimonOtt,KonstantinHebenstreit,ValentinLie´vin,ChristofferEgeberg
languagemodelsfine-tuningwithlorabasedmixtureofexperts. arXiv Hother, Milad Moradi, Maximilian Mayrhauser, Robert Praas, Ole
preprintarXiv:2404.15159,2024. Winther, and Matthias Samwald. Thoughtsource: A central hub for
[46] Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin, and largelanguagemodelreasoningdata. ScientificData,10(1):528,2023.
Bernard Ghanem. Camel: Communicative agents for” mind” explo- [69] Lucas Page-Caccia, Edoardo Maria Ponti, Zhan Su, Matheus Pereira,
rationoflargelanguagemodelsociety.AdvancesinNeuralInformation NicolasLeRoux,andAlessandroSordoni. Multi-headadapterrouting
ProcessingSystems,36:51991–52008,2023. forcross-taskgeneralization. AdvancesinNeuralInformationProcess-
[47] Margaret Li, Suchin Gururangan, Tim Dettmers, Mike Lewis, Tim ingSystems,36,2024.
Althoff, Noah A Smith, and Luke Zettlemoyer. Branch-train-merge: [70] AdamPaszke,SamGross,FranciscoMassa,AdamLerer,JamesBrad-
Embarrassingly parallel training of expert language models. arXiv bury,GregoryChanan,TrevorKilleen,ZemingLin,NataliaGimelshein,
preprintarXiv:2208.03306,2022. LucaAntiga,etal.Pytorch:Animperativestyle,high-performancedeep
[48] Ming Li, Yong Zhang, Zhitao Li, Jiuhai Chen, Lichang Chen, Ning learninglibrary.Advancesinneuralinformationprocessingsystems,32,
Cheng,JianzongWang,TianyiZhou,andJingXiao. Fromquantityto 2019.
quality: Boosting llm performance with self-guided data selection for [71] BaolinPeng,ChunyuanLi,PengchengHe,MichelGalley,andJianfeng
instructiontuning. arXivpreprintarXiv:2308.12032,2023. Gao. Instructiontuningwithgpt-4. arXivpreprintarXiv:2304.03277,
[49] XiangLisaLiandPercyLiang. Prefix-tuning:Optimizingcontinuous 2023.
promptsforgeneration. arXivpreprintarXiv:2101.00190,2021. [72] Jonas Pfeiffer, Andreas Ru¨ckle´, Clifton Poth, Aishwarya Kamath,
[50] YunxinLi,ShenyuanJiang,BaotianHu,LongyueWang,WanqiZhong, Ivan Vulic´, Sebastian Ruder, Kyunghyun Cho, and Iryna Gurevych.
WenhanLuo,LinMa,andMinZhang.Uni-moe:Scalingunifiedmulti- Adapterhub: A framework for adapting transformers. arXiv preprint
modalllmswithmixtureofexperts. arXivpreprintarXiv:2405.11273, arXiv:2007.07779,2020.
2024. [73] Agustin Picard, Lucas Hervier, Thomas Fel, and David Vigouroux.
[51] WangLing,DaniYogatama,ChrisDyer,andPhilBlunsom. Program Influenciæ:Alibraryfortracingtheinfluencebacktothedata-points.
inductionbyrationalegeneration:Learningtosolveandexplainalge- InWorldConferenceonExplainableArtificialIntelligence,pages193–
braicwordproblems. arXivpreprintarXiv:1705.04146,2017. 204.Springer,2024.
[52] JerryLiu. LlamaIndex,112022. [74] Yulei Qin, Yuncheng Yang, Pengcheng Guo, Gang Li, Hang Shao,
[53] QidongLiu,XianWu,XiangyuZhao,YuanshaoZhu,DerongXu,Feng Yuchen Shi, Zihan Xu, Yun Gu, Ke Li, and Xing Sun. Unleashing
Tian,andYefengZheng. Moelora:Anmoe-basedparameterefficient thepowerofdatatsunami:Acomprehensivesurveyondataassessment
fine-tuningmethodformulti-taskmedicalapplications. arXivpreprint andselectionforinstructiontuningoflanguagemodels. 2024.
arXiv:2310.18339,2023. [75] ZhengLinQingyiSi. Alpaca-cot:Aninstructionfine-tuningplatform
[54] Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, with instruction data collection and unified large language models
Yu-Chiang Frank Wang, Kwang-Ting Cheng, and Min-Hung Chen. interface. https://github.com/PhoebusSi/alpaca-CoT,2023.
Dora: Weight-decomposed low-rank adaptation. arXiv preprint [76] JeffRasley,SamyamRajbhandari,OlatunjiRuwase,andYuxiongHe.
arXiv:2402.09353,2024. Deepspeed:Systemoptimizationsenabletrainingdeeplearningmodels
[55] WeiLiu,WeihaoZeng,KeqingHe,YongJiang,andJunxianHe. What with over 100 billion parameters. In Proceedings of the 26th ACM
makes good data for alignment? a comprehensive study of automatic SIGKDD International Conference on Knowledge Discovery & Data
data selection in instruction tuning. In The Twelfth International Mining,pages3505–3506,2020.
ConferenceonLearningRepresentations,2024. [77] MaartenSap,HannahRashkin,DerekChen,RonanLeBras,andYejin
[56] Zefang Liu and Jiahua Luo. Adamole: Fine-tuning large language Choi. Socialiqa: Commonsense reasoning about social interactions.
models with adaptive mixture of low-rank adaptation experts. arXiv arXivpreprintarXiv:1904.09728,2019.
preprintarXiv:2405.00361,2024. [78] TimoSchick,JaneDwivedi-Yu,RobertoDess`ı,RobertaRaileanu,Maria
[57] ShayneLongpre,LeHou,TuVu,AlbertWebson,HyungWonChung, Lomeli,EricHambro,LukeZettlemoyer,NicolaCancedda,andThomas
YiTay,DennyZhou,QuocVLe,BarretZoph,JasonWei,etal.Theflan Scialom. Toolformer: Language models can teach themselves to use
collection:Designingdataandmethodsforeffectiveinstructiontuning. tools. AdvancesinNeuralInformationProcessingSystems,36,2024.JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 16
[79] Stephanie Schoch, Ritwick Mishra, and Yangfeng Ji. Data selection [103] Rui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, Xiu Li, and
forfine-tuninglargelanguagemodelsusingtransferredshapleyvalues. YingShan. Gpt4tools:Teachinglargelanguagemodeltousetoolsvia
arXivpreprintarXiv:2306.10165,2023. self-instruction. AdvancesinNeuralInformationProcessingSystems,
[80] OzanSenerandSilvioSavarese.Activelearningforconvolutionalneu- 36,2024.
ral networks: A core-set approach. arXiv preprint arXiv:1708.00489, [104] YunchengYang,MengWei,JunjunHe,JieYang,JinYe,andYunGu.
2017. Pickthebestpre-trainedmodel:Towardstransferabilityestimationfor
[81] RicoSennrich,BarryHaddow,andAlexandraBirch. Improvingneural medicalimagesegmentation. InInternationalConferenceonMedical
machine translation models with monolingual data. arXiv preprint ImageComputingandComputer-AssistedIntervention,pages674–683.
arXiv:1511.06709,2015. Springer,2023.
[82] Samarth Sinha, Han Zhang, Anirudh Goyal, Yoshua Bengio, Hugo [105] Kaichao You, Yong Liu, Ziyang Zhang, Jianmin Wang, Michael I
Larochelle,andAugustusOdena. Small-gan:Speedingupgantraining Jordan,andMingshengLong.Rankingandtuningpre-trainedmodels:a
using core-sets. In International Conference on Machine Learning, newparadigmforexploitingmodelhubs.JournalofMachineLearning
pages9005–9015.PMLR,2020. Research,23(209):1–47,2022.
[83] ZhanSu,FengranMo,PrayagTiwari,BenyouWang,Jian-YunNie,and [106] LeYu,BowenYu,HaiyangYu,FeiHuang,andYongbinLi. Language
JakobGrueSimonsen. Mixtureofexpertsusingtensorproducts. arXiv modelsaresupermario:Absorbingabilitiesfromhomologousmodels
preprintarXiv:2405.16671,2024. asafreelunch. arXivpreprintarXiv:2311.03099,2023.
[84] SainbayarSukhbaatar,OlgaGolovneva,VasuSharma,HuXu,XiVic- [107] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin
toriaLin,BaptisteRozie`re,JacobKahn,DanielLi,Wen-tauYih,Jason Choi. Hellaswag: Can a machine really finish your sentence? arXiv
Weston,etal. Branch-train-mix:Mixingexpertllmsintoamixture-of- preprintarXiv:1905.07830,2019.
expertsllm. arXivpreprintarXiv:2403.07816,2024. [108] JinghanZhang,JuntengLiu,JunxianHe,etal. Composingparameter-
[85] AlonTalmor,JonathanHerzig,NicholasLourie,andJonathanBerant. efficient modules with arithmetic operation. Advances in Neural
Commonsenseqa:Aquestionansweringchallengetargetingcommon- InformationProcessingSystems,36:12589–12610,2023.
senseknowledge. arXivpreprintarXiv:1811.00937,2018. [109] JipengZhang,YaxuanQin,RenjiePi,WeizhongZhang,RuiPan,and
[86] KushalTirumala,DanielSimig,ArmenAghajanyan,andAriMorcos. TongZhang. Tagcos:Task-agnosticgradientclusteredcoresetselection
D4:Improvingllmpretrainingviadocumentde-duplicationanddiversi- forinstructiontuningdata. arXivpreprintarXiv:2407.15235,2024.
fication.AdvancesinNeuralInformationProcessingSystems,36,2024. [110] Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He,
[87] HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlma- Yu Cheng, Weizhu Chen, and Tuo Zhao. Adaptive budget allocation
hairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal for parameter-efficient fine-tuning. In The Eleventh International
Bhargava,ShrutiBhosale,etal. Llama2:Openfoundationandfine-
ConferenceonLearningRepresentations,2023.
tunedchatmodels. arXivpreprintarXiv:2307.09288,2023. [111] Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun,
Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, et al.
[88] LaurensVanderMaatenandGeoffreyHinton. Visualizingdatausing
Instructiontuningforlargelanguagemodels:Asurvey. arXivpreprint
t-sne. Journalofmachinelearningresearch,9(11),2008.
arXiv:2308.10792,2023.
[89] Cunxiang Wang, Shuailong Liang, Yue Zhang, Xiaonan Li, and Tian
[112] XiangZhang,JunboZhao,andYannLeCun. Character-levelconvolu-
Gao.Doesitmakesense?andwhy?apilotstudyforsensemakingand
tionalnetworksfortextclassification. Advancesinneuralinformation
explanation. arXivpreprintarXiv:1906.00363,2019.
processingsystems,28,2015.
[90] EricJ.Wang. Alpaca-lora. https://github.com/tloen/alpaca-lora,2024.
[113] Bo Zhao and Hakan Bilen. Dataset condensation with distribution
[91] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A
matching. In Proceedings of the IEEE/CVF Winter Conference on
Smith,DanielKhashabi,andHannanehHajishirzi.Self-instruct:Align-
ApplicationsofComputerVision,pages6514–6523,2023.
ing language models with self-generated instructions. arXiv preprint
[114] ZiyuZhao,LeileiGan,GuoyinWang,YuweiHu,TaoShen,Hongxia
arXiv:2212.10560,2022.
Yang, Kun Kuang, and Fei Wu. Retrieval-augmented mixture
[92] ZengzhiWang,RuiXia,andPengfeiLiu. Generativeaiformath:Part
of lora experts for uploadable machine learning. arXiv preprint
i–mathpile: A billion-token-scale pretraining corpus for math. arXiv
arXiv:2406.16989,2024.
preprintarXiv:2312.17120,2023.
[115] Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun,
[93] JasonWei,XuezhiWang,DaleSchuurmans,MaartenBosma,FeiXia,
Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. Lima:
EdChi,QuocVLe,DennyZhou,etal. Chain-of-thoughtprompting
Lessismoreforalignment.AdvancesinNeuralInformationProcessing
elicitsreasoninginlargelanguagemodels. Advancesinneuralinfor-
Systems,36,2024.
mationprocessingsystems,35:24824–24837,2022.
[116] DaquanZhou,KaiWang,JianyangGu,XiangyuPeng,DongzeLian,
[94] Jason Wei and Kai Zou. Eda: Easy data augmentation techniques
Yifan Zhang, Yang You, and Jiashi Feng. Dataset quantization. In
for boosting performance on text classification tasks. arXiv preprint
ProceedingsoftheIEEE/CVFInternationalConferenceonComputer
arXiv:1901.11196,2019.
Vision,pages17205–17216,2023.
[95] ThomasWolf,LysandreDebut,VictorSanh,JulienChaumond,Clement
[117] Han Zhou, Xingchen Wan, Ivan Vulic´, and Anna Korhonen. Au-
Delangue,AnthonyMoi,PierricCistac,TimRault,Re´miLouf,Morgan
topeft: Automatic configuration search for parameter-efficient fine-
Funtowicz, et al. Huggingface’s transformers: State-of-the-art natural tuning. TransactionsoftheAssociationforComputationalLinguistics,
languageprocessing. arXivpreprintarXiv:1910.03771,2019.
12:525–542,2024.
[96] Shengguang Wu, Keming Lu, Benfeng Xu, Junyang Lin, Qi Su, and [118] KaiyangZhou,JingkangYang,ChenChangeLoy,andZiweiLiu.Con-
ChangZhou.Self-evolveddiversedatasamplingforefficientinstruction ditionalpromptlearningforvision-languagemodels.InProceedingsof
tuning. arXivpreprintarXiv:2311.08182,2023. theIEEE/CVFconferenceoncomputervisionandpatternrecognition,
[97] XunWu,ShaohanHuang,andFuruWei.Mole:Mixtureofloraexperts. pages16816–16825,2022.
InTheTwelfthInternationalConferenceonLearningRepresentations,
2023.
[98] Mengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora,
andDanqiChen.Less:Selectinginfluentialdatafortargetedinstruction
tuning. arXivpreprintarXiv:2402.04333,2024.
[99] SangMichaelXie,ShibaniSanturkar,TengyuMa,andPercySLiang.
Data selection for language models via importance resampling. Ad-
vances in Neural Information Processing Systems, 36:34201–34227,
2023.
[100] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan
Feng, Chongyang Tao, and Daxin Jiang. Wizardlm: Empowering
largelanguagemodelstofollowcomplexinstructions. arXivpreprint
arXiv:2304.12244,2023.
[101] YangXu,YongqiangYao,YufanHuang,MengnanQi,MaoquanWang,
BinGu,andNeelSundaresan. Rethinkingtheinstructionquality:Lift
iswhatyouneed. arXivpreprintarXiv:2312.11508,2023.
[102] PrateekYadav,DerekTam,LeshemChoshen,ColinARaffel,andMohit
Bansal. Ties-merging: Resolving interference when merging models.
AdvancesinNeuralInformationProcessingSystems,36,2024.JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 17
APPENDIX
.1 Datasets
Wepresentthestatisticsofpubliclyavailabledatasetsusedinthe
presentstudyforconstructingourLoRAbankinTable8.
.2 EvaluationResultsofModelsintheBank
The evaluation results of LoRA models in the bank are reported
inTables9and10respectivelyforLLaMA2[87]andMistral[36]
families.
Each model is fine-tuned on one high-quality open dataset
under the same experimental settings. Results confirm that all
these LoRA models exhibit different advantages on the down-
stream tasks of interest, implying that each model possesses a
unique skill acquired from the fine-tuned dataset. However, we
also observe that certain models are highly competitive nearly
on all tasks (e.g., RACE for the LLaMA2 and ARC-e for the
Mistral).Suchphenomenonmaybeascribedtothefactthatthese
twodatasetsemphasizethereasoningandreadingcomprehension
capabilities of LLMs, where the acquired skills appear more
general in solving various tasks. Besides, we notice that the two
model families (LLaMA vs. Mistral) behave inconsistently in
the evaluation results even under the same fine-tuning settings.
It suggests that different pre-training corpus are collected and
memorized for language modeling, and thereafter the two base
pre-trainedmodelsalreadydifferintheparameterizedknowledge
before instruction fine-tuning. Consequently, when optimizing
any LLM for the task-of-interest, there exists no golden rule of
selectingonegenerally-applicable,ultimately-superiorinstruction
datasetwhichwouldworkallthetime.Inpractice,oneshoulddy-
namicallyselectpotentialmodelsfollowingtheproposedpipeline
insteadofempiricallysamplingfromthebank.Lastbutnotleast,
it is noted that the diversity of the candidate models in the bank
has to be guaranteed to ensure that each model owns knowledge
thatcansupplementtotheotherone.Suchbankdiversityallowsa
generalizedskill-settobeutilizedviatheconstructedMoEsystem
forsolvingtasksofinterest.
.3 MathmaticalNotations
Inthissection,wepresentthedescriptionofallmathnotationsin
themanuscript(seeTable11).
.4 ImplementationDetails
The detailed settings of all hyper-parameters for optimization of
theLoRAmodelsandtheMoEsystemarelistedinTable12.JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 18
TABLE8
StatisticsofDatasets(Part1).
Dataset Task Train Val Test Example
ARC-c Exam 1.1K 299 1.1K {”instruction”: ”Question: George wants to warm his hands quickly by rubbing
them. Which skin surface will produce the most heat? A. dry palms\nB. wet
palms\nC. palms covered with oil\nD. palms covered with lotion\nAnswer:”,
”input”:””,”output”:”A.drypalms”}
ARC-c
Exam 1.1K 299 1.1K {”instruction”: ”Question: George wants to warm his hands quickly by rubbing
CoT-formatted
them. Which skin surface will produce the most heat? A. dry palms\nB. wet
palms\nC. palms covered with oil\nD. palms covered with lotion\nAnswer:”,
”input”:””,”output”:”Rubbingtwosurfacestogethergeneratesheatduetofriction.
In the case of warming hands, the effectiveness of this process depends on the
coefficient of friction between the surfaces. Dry palms are likely to produce the
mostheatwhenrubbedtogetherbecausetheyhaveahighercoefficientoffriction
comparedtowetpalms,whichwouldslipmoreeasilyandthusgeneratelessheat.
Palms covered with oil or lotion would also have reduced friction due to the
lubricatingeffectofthesesubstances,leadingtolessheatgenerationuponrubbing.
Therefore, dry palms are the most effective choice for generating heat through
friction.TheanswerisA.drypalms”}
ARC-e Exam 2.2K 570 2.3K {”instruction”:”Question:Whichfactorwillmostlikelycauseapersontodevelop
a fever? A. a leg muscle relaxing after exercise\nB. a bacterial population in
the bloodstream\nC. several viral particles on the skin\nD. carbohydrates being
digestedinthestomach\nAnswer:”,”input”:””,”output”:”B.abacterialpopulation
inthebloodstream”}
Coreference
WinoGrande 9.2K 1267 – {”instruction”:”Whichofthefollowingisagoodsentence:\nA.Johnmovedthe
Resolution
couchfromthegaragetothebackyardtocreatespace.Thegarageissmall.\nB.John
movedthecouchfromthegaragetothebackyardtocreatespace.Thebackyardis
small.\nAnswer:”,”input”:””,”output”:”A.Johnmovedthecouchfromthegarage
tothebackyardtocreatespace.Thegarageissmall.”}
GSM8K Math 7.4K 0 1.3K {”instruction”: ”Question: Natalia sold clips to 48 of her friends in April, and
then she sold half as many clips in May. How many clips did Natalia sell
altogetherinAprilandMay?”,”input”:””,”output”:”Answer:Nataliasold48/2=
<<48/2=24>>24clipsinMay.\nNataliasold48+24=<<48+24=72>>72clips
altogetherinAprilandMay.\n”}
Commonsense
PIQA 16.1K 1838 3K {”instruction”:”Question:Whenboilingbutter,whenit’sready,youcan\nA.Pour
Reasoning
itontoaplate\nB.Pouritintoajar\nAnswer:”,”input”:””,”output”:”B.Pourit
intoajar”}
Commonsense
CommonSenseQA 9.7K 1221 1.1K {”instruction”:”Whoislikelytouseacomb?A.medicinecabinet\nB.trashcan\n
Reasoning
C.suitcase\nD.pocket\nE.barbershop”,”input”:””,”output”:”E.barbershop”}
Reading
RACE 87.8K 4887 4.9K {”instruction”:”MrsGreenlivesinasmallvillage.Herhusbandisdead,butshehas
Comprehension
oneson.HisnameisJack.Heistwenty-one.Heworkedinashopinthevillageand
livedwithhismother,butthenhefindsajobinafarawaytownnamedGreenSea.
Hegoesandlivesthere.MrsGreenisnothappyaboutit.\nOnedayMrsGreenis
angry.Shegetsonatrainandgoestoherson’shouseinGreenSea.Thenshesays
tohim,\”Jack,whydoyounevertelephoneme?\”\nJacksmiles,\”But,Mother,
youdon’thaveatelephone.\”\n\”No,\”sheanswers,\”Idon’thave,butyouhave
one!\”\nGreenSeais thesmallvillage.A.in\nB.nextto\nC.near\nD.farfrom”,
”input”:””,”output”:”D.farfrom”}
MBPP Coding 374 90 500 {”instruction”: ”Write a python function to find the first repeated character in a
givenstring.”,”input”:””,”output”:”deffirst repeated char(str1):\r\nforindex,c
in enumerate(str1):\r\n if str1[:index+1].count(c) >1:\r\n return c \r\n return
\”None\””}JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 19
TABLE8
StatisticsofDatasets(Part2).
Dataset Task Train Val Test Example
Mathqa Math 29.4K – – {”instruction”:”6workersshouldfinishajobin8days.after3dayscame4workersjointhem.how
manydaysmdotheyneedtofinishthesamejob?\na)3,b)4,c)5,d)6,e)7”,”input”:””,
”output”:”\”letrateofoneworkerber=>(6*r)*8=1(rate*time=work)=>r=1/48=
>workremainingafter3days1-(3*6)/48=30/48after4ppljoinedin((6+4)*time)/48
=30/48timem=3daystofinishthetaskimoa\””}
Esnli CoT 36K – – {”instruction”: ”Please answer the following question by reasoning step-by-step. Premise: \”Man
scalingwallwithfireinhand.\”\\nHypothesis:\”Amanholdingfireinhishandistryingtoescape
by scaling a wall.\”\\nDo we know that the hypothesis entailed by the premise?. Step-by-step
reasoning:”,”input”:””,”output”:”Justbecauseamanisscalingawalldoesn’timplyheistryingto
escape.Sotheanswerisitisnotpossibletotell.”}
ECQA CoT 7.1K – – {”instruction”: ”What might a person see at the scene of a brutal killing?\\nOptions:\\n- bloody
mess\\n-pleasure\\n-beingimprisoned\\n-feelingofguilt\\n-cake\nLet’sthinkcarefullyfirst.
Step-by-step reasoning process:”, ”input”: ””, ”output”: ”Bloody mess is covered or stained with
blood.Apersonmightseeabloodymessatthesceneofabrutalkilling.Sothefinalanswerisbloody
mess.”}
CREAK CoT 6.9K – – {”instruction”:”Claim:\”OnlypeoplenamedFloydwearingpinkareallowedtoattendPinkFloyd
concerts.\”\\nIstheclaimabovecorrect,andcanitbeverifiedbyhumancommonsenseandwithout
awebsearch?\\nOptions:\\n-yes\\n-noStep-by-stepreasoningprocess:”,”input”:””,”output”:
”Therockgroupwouldnotbeaspopularistheyhadsuchrequirementsfortheirconcerts.Therefore,
thefinalanswerisno.”}
GPT4Tools ToolUse 71.4K – – {”instruction”: ”GPT4Tools can handle various text and visual tasks, such as answering questions
and providing in-depth explanations and discussions. It generates human-like text and uses tools
to indirectly understand images. When referring to images, GPT4Tools follows strict file name
rules.Tocompletevisualtasks,GPT4Toolsusestoolsandstaysloyaltoobservationoutputs.Users
can provide new images to GPT4Tools with a description, but tools must be used for subsequent
tasks.\nTOOLS:\n——\n\nGPT4Tools has access to the following tools:\n\n>Detect the Give
Object:usefulwhenyouonlywanttodetectorfindoutgivenobjectsinthepictureTheinputtothis
tool should be a comma separated string of two, representing the image path, the text description
oftheobjecttobefound\n>PoseDetectionOnImage:usefulwhenyouwanttodetectthehuman
pose of the image. like: generate human poses of this image, or generate a pose image from this
image.Theinputtothistoolshouldbeastring,representingtheimage path\n>RemoveSomething
FromThePhoto:usefulwhenyouwanttoremoveandobjectorsomethingfromthephotofromits
descriptionorlocation.Theinputtothistoolshouldbeacommaseparatedstringoftwo,representing
the image path and the object need to be removed.\n\nTo use a tool, please use the following
format:\n\n“‘\nThought:DoIneedtouseatool?Yes\nAction:theactiontotake,shouldbeoneof
[DetecttheGiveObject,PoseDetectionOnImage,RemoveSomethingFromThePhoto]\nAction
Input:theinputtotheaction\nObservation:theresultoftheaction\n“‘\n\nWhenyouhavearesponse
tosaytotheHuman,orifyoudonotneedtouseatool,youMUSTusetheformat:\n\n“‘\nThought:
Do I need to use a tool? No\nAI: [your response here]\n“‘\n\nFollow file name rules and do
not fake non-existent file names. Remember to provide the image file name loyally from the last
tool observation.\n\nPrevious conversation:\n\nHuman: Provide an image named jmblbwhh.png.
Description:Awomansittingnexttobuildingsmadewithgingerbread.Understandtheimageusing
tools.\nAI:Received.\n\nNewinput:Detectthegingerbreadhouses:\nGPT4Toolsneedstousetools
toobserveimages,notdirectlyimaginethem.Thoughtsandobservationsintheconversationareonly
visibletoGPT4Tools.Whenansweringhumanquestions,repeatimportantinformation.Let’sthink
step by step.\nThought: Do I need to use a tool? Yes\nAction: Detect the Give Object\nAction
Input: jmblbwhh.png, gingerbread houses\nObservation: [output from detection tool]\n”,”input”:
””,”output”:”Thought:DoIneedtouseatool?No\nAI:[output from detection tool]\n\n”}
AQuA CoT 2.7K – – {”instruction”:”Question:Rs.5600isdividedintothreepartsA,BandC.HowmuchAismorethan
Ciftheirratiois1/7:1/7:1/14?\\nOptions:\\n(A)300\\n(B)992\\n(C)1120\\n(D)552\\n(E)312
Let’sthinkfirst.Somerandomreasoning:”,”input”:””,”output”:”1/7:1/7:1/14=2:2:1\\n1/5*5600
=1120\\n2240-1120=1120Thefinalanswer:(C).”}
QASC CoT 1K – – {”instruction”:”Whatistheprocessbywhichlivingthingsgiverisetooffspring?\\nOptions:\\n-
(A)DNA\\n-(B)photosynthesis\\n-(C)bird\\n-(D)sex\\n-(E)subtraction\\n-(F)gametes\\n-
(G)eggs\\n-(H)ovumLet’ssolvethisproblemgradually.\n”,”input”:””,”output”:”Reproduction
istheprocessbywhichlivingthingsgiverisetooffspring.Sexequalsreproduction.Sexistheprocess
bywhichlivingthingsgiverisetooffspring.Sothefinalansweris(D).”}JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 20
TABLE8
StatisticsofDatasets(Part3).
Dataset Task Train Val Test Example
QED CoT 5.1K – – {”instruction”:”AnswerthefollowingQwithstreamofconsciousness.Passage:Webbed
toes is the common name for syndactyly affecting the feet. It is characterised by the
fusionoftwoormoredigitsofthefeet.Thisisnormalinmanybirds,suchasducks;
amphibians,suchasfrogs;andmammals,suchaskangaroos.Inhumansitisconsidered
unusual,occurringinapproximatelyonein2,000to2,500livebirths.\\n\\nQuestion:
Basedonthispassage,whatisthemedicaltermforwebbedtoes?”,”input”:””,”output”:
”Therelevantinformationis:Webbedtoesisthecommonnameforsyndactylyaffecting
thefeet.Therefore,thefinalanswerissyndactylyaffectingthefeet.”}
StrategyQA CoT 2K – – {”instruction”: ”Answer the following question by reasoning step-by-step. Are more
peopletodayrelatedtoGenghisKhanthanJuliusCaesar?”,”input”:””,”output”:”Julius
Caesarhadthreechildren.GenghisKhanhadsixteenchildren.Moderngeneticistshave
determined thatout of every 200 men today has DNA that can be traced to Genghis
Khan.Therefore,thefinalanswerisyes.”}
SensemakingQA CoT 6K – – {”instruction”: ”Of the following two sentences, which one is against common
sense?\\nOptions:\\n- Sentence A: \”He poured orange juice on his cereal.\”\\n-
Sentence B: \”He poured milk on his cereal.\”\\n Let’s be accurate as possible. So
thinkfirst.”,”input”:””,”output”:”Orangejuicedoesnottastegoodoncereal.\nThe
final answer: Sentence A.”}, {”instruction”: ”Let’s think step by step! Which of the
followingtwosentencesdoesnotmakesense?\\nOptions:\\n-SentenceA:\”Hedrinks
apple.\”\\n-SentenceB:\”Hedrinksmilk.\”\\n\n”,”input”:””,”output”:”Anapple
isawholefoodandunabletobedrunkwithoutbeingjuiced.Theanswer:SentenceA.”
Toolformer(th=0.6) ToolUse 2.4K – – {”instruction”:”toolformer:enabled\ntoolformeraccess:wikipedia\nAwrapperaround
Wikipedia. Useful for when you need to display general information about peo-
ple, places, companies, historical events, or other subjects found in an ency-
clopedia, displays a snippet summary of the topic. Input should be a search
query.\nwikipedia(query)\nProvide a summary of a famous historical event based on
theinputeventgiven”,”input”:”FrenchRevolution”,”output”:”SummaryoftheFrench
Revolution:wikipedia(’FrenchRevolution’)”}
Commonsense
HellaSwag 39.9K – 10K {”instruction”: ”[header] How to reset a vizio remote [title] Remove your remote’s
Reasoning
batteries.[step]They’retypicallyfoundinaslotthat’seitheronthebottomorinthe
backoftheremote.[title]Pressandholdtheremote’spowerbutton.\nQuestion:Which
endingmakesthemostsense?A.[step]Thisallowsthedevicetopoweroff.[title]Press
and hold the power button on the remote while you wait to hear a response.B. [step]
It’slocatednearthebottomoftheremote,rightinthe.[title]Releasethepowerbutton
whenyou’resureit’scovered.C.[step]It’susuallyatthetopoftheremote’ssurface.
[title]Releasethepowerbuttonafterfiveseconds.D.[step]Holddownthepowerbutton
asclosetothebatteriesaspossible.Thiswillpowerupyourdeviceandallowthelower
batterytorefillwithpower.”,”input”:””,”output”:”C.[step]It’susuallyatthetopof
theremote’ssurface.[title]Releasethepowerbuttonafterfiveseconds.”}
Commonsense
SiQA 33.4K – 1.9K {”instruction”: ”Addison put Kendall’s cost-cutting ideas into action.\nWhat will
Reasoning
happentoAddison?\nA.kendallwillbethanked\nB.getpromoted\nC.losetheirjob”,
”input”:””,”output”:”B.getpromoted”}
Knowledge
BoolQ Question 2.4K 277 3K {”instruction”: ”Good Samaritan laws offer legal protection to people who give rea-
Answering sonableassistancetothosewhoare,orwhotheybelievetobe,injured,ill,inperil,or
otherwiseincapacitated.Theprotectionisintendedtoreducebystanders’hesitationto
assist,forfearofbeingsuedorprosecutedforunintentionalinjuryorwrongfuldeath.
Anexampleofsuchalawincommon-lawareasofCanada:agoodSamaritandoctrine
isalegalprinciplethatpreventsarescuerwhohasvoluntarilyhelpedavictimindistress
frombeingsuccessfullysuedforwrongdoing.Itspurposeistokeeppeoplefrombeing
reluctanttohelpastrangerinneedforfearoflegalrepercussionsshouldtheymakesome
mistakeintreatment.Bycontrast,adutytorescuelawrequirespeopletoofferassistance
andholdsthosewhofailtodosoliable.\nQuestion:dogoodsamaritanlawsprotectthose
whohelpatanaccident\nA.Yes\nB.No\nAnswer:”,”input”:””,”output”:”A.Yes”}JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 21
TABLE8
StatisticsofDatasets(Part4).
Dataset Task Train Val Test Example
Dolly CoT 15K – – {”instruction”:”WhichAmericanpresidentspokeDutchastheirfirstlanguage?”,”input”:”Van
Buren received a basic education at the village schoolhouse, and briefly studied Latin at
the Kinderhook Academy and at Washington Seminary in Claverack. Van Buren was raised
speakingprimarilyDutchandlearnedEnglishwhileattendingschool;heistheonlypresident
oftheUnitedStateswhosefirstlanguagewasnotEnglish.Alsoduringhischildhood,VanBuren
learnedathisfather’sinnhowtointeractwithpeoplefromvariedethnic,income,andsocietal
groups,whichheusedtohisadvantageasapoliticalorganizer.Hisformaleducationendedin
1796,whenhebeganreadinglawattheofficeofPeterSilvesterandhissonFrancis.”,”output”:
”PresidentVanBurenspokeDutchashisfirstlanguage.HeistheonlypresidentoftheUnited
StatesofAmericawhospokeEnglishasasecondlanguage.”}
WizardLM General 143K – – {”instruction”:”AsanonlineplatformteachernamedAimee,youpossessimpeccablecreden-
tialswhichincludeaBachelorofSciencedegreeinIndustrialandLaborRelationsfromCornell
University,expertiseintheEnglishlanguage,andintermediateproficiencyinbothChineseand
Spanish. Additionally, your professional experience as a STEAM teacher at UN Women in
Singaporehashonedyourskillsinteachingchildrenfromtheagesof6-11andworkingwith
students from all levels of education. Your exceptional teaching abilities in spoken English
andpronunciationpairedwithyourpersonalstrengthsofbeinginformed,patient,andengaging
makeyouanidealteacherforstudentsseekingtoimprovetheirEnglishlanguageskills.Canyou
provideashort,concise,anduniqueEnglishself-introductioninbulletpointformthatwould
attractstudentstoenrollinyourcourse?”,”output”:”Sure,herearesomebulletpointsforyour
Englishself-introduction:\n\n-Hithere!MynameisAimeeandI’mthrilledtobeyouronline
Englishteacher!\n-WithaBachelor’sdegreefromCornellUniversityandexperienceteaching
STEAM at UN Women in Singapore, I know how to help students learn effectively.\n- I’m
passionateaboutlanguageandhaveintermediateproficiencyinbothChineseandSpanish,soI
understandthechallengesoflearninganewlanguage.\n-Inmyclasses,Ifocusonimproving
spokenEnglishandpronunciation,aswellasbuildingvocabularyandgrammarskills.\n-But
learning English doesn’t have to be boring! I’m an engaging and patient teacher who loves
to have fun and make learning enjoyable.\n- So, whether you’re a beginner or an advanced
student,I’mconfidentthatIcanhelpyouimproveyourEnglishskillsandachieveyourgoals.
Let’sgetstarted!”,”input”:””}
ShareGPT General 330K – – {”instruction”:”SummarizethemainideasofJeffWalker’sProductLaunchFormulaintobullet
pointsasitpertainstoagrowthmarketingagencyimplementingthesestrategiesandtacticsfor
theirclients...”,”input”:””,”output”:”HerearethemainideasofJeffWalker’sProductLaunch
Formulathatcanbeappliedbyagrowthmarketingagencyfortheirclients:\n\n1.Identifythe
targetaudienceandtheirneeds:Understandtheidealcustomerfortheproductorservice,and
createamessagingthatresonateswiththem.\n2.Pre-launch:Buildanticipationandexcitement
forthelaunchbycreatingbuzz,gatheringtestimonialsandcasestudies,andusingsocialmedia
tocreateawareness.\n3.Launch:Useawell-craftedlaunchsequencetomaximizesalesand
conversions.Thiscanincludeofferingbonuses,creatingscarcity,andusingadeadlinetocreate
urgency.\n4.Post-launch:Followupwithcustomers,gatherfeedback,andcontinuetoprovide
valuetokeepthemengagedandloyal.\n5.Createaproductsuite:Oncetheinitialproductis
successful,expandtheofferingbycreatingadditionalproductsorservicesthataddressother
needsofthesameaudience.\n6.Continualoptimization:Continuallymonitorandoptimizethe
launchprocessandproductsuitetoimproveresults.\n7.Buildacommunity:Usethelaunch
processtobuildacommunityofcustomerswhoarepassionateabouttheproductandcanhelp
spreadtheword.\n8.Useautomation:Usetechnologyandautomationtostreamlinethelaunch
processandimproveefficiency.”}
GPT-Teacher-Code Coding 4.5K – – {”instruction”: ”Design a C++ function to calculate the factorial of a non-negative inte-
ger.”,”input”:”5”,”output”:”#include<iostream>\n\nunsignedlonglongfactorial(intn){\n
if(n<=1){\nreturn1;\n}\nreturnn*factorial(n-1);\n}\n\nintmain(){\nintnum=
5;\nstd::cout<<’Factorialof’<<num<<’is’<<factorial(num)<<std::endl;\nreturn
0;\n}”}JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 22
TABLE8
StatisticsofDatasets(Part5).
Dataset Task Train Val Test Example
GPT-Teacher-Roleplay Coding 3.1K – – {”instruction”:”Imagineyouareadetectivetryingtosolveamurdermysteryinafictional
Victorian-eratown.Describethesceneofthecrime,themainsuspects,andyourplanto
investigatethecase.”,”input”:””,”output”:”Thesceneofthecrimeisadimlylitstudy
insideagrand,Victorian-eramansion.There’salargeoakdeskatthecenteroftheroom,
andthedeceased,LordBarrington,liesslumpedoveritwithagunshotwoundtothechest.
A shattered windowpane hints at the possible point of entry for the assailant. The three
main suspects are: 1) Lady Barrington, the victim’s wife, who had increasing disputes
with her husband; 2) Randall, the family’s loyal butler, who recently discovered he was
goingtobereplaced;and3)Edgar,adistantcousin,whoexpectedalargeinheritanceif
LordBarringtonpassedaway.Myplantoinvestigatethecaseincludes:interviewingeach
suspectabouttheirwhereaboutsduringthecrime;examiningthebrokenwindowforany
fingerprintsorsignsofforcedentry;andanalyzingthemurderweapon,apistol,totraceits
ownershipandlookforanyadditionalevidencelikepartialfingerprintsorothertracesleft
bytheculprit.”}
GPT-Teacher-Instruct General 17K – – {”instruction”: ”Provide an overview of the process of human evolution.”, ”input”: ””,
”output”:”Humanevolutionbeganmillionsofyearsagowiththeemergenceofcommon
ancestorsofhumans,chimpanzees,andgorillas.Theprocessinvolvedaseriesofdistinct
speciesbranchingoffandevolvingseparately,withsomeleadingtomodernhumans,Homo
sapiens.Keymilestonesinhumanevolutionincludetheappearanceofthefirsthominids
(human-likeprimates),thedevelopmentofbipedalism(walkingupright),andthegrowth
of larger brains. Over time, hominids developed more advanced tools, communication
abilities, and social structures. The timeline of human evolution includes several genera
such as Ardipithecus, Australopithecus, Paranthropus, and Homo. Noteworthy species
include Ardipithecus ramidus, Australopithecus afarensis ’Lucy,’ Homo habilis, Homo
erectus,Homoneanderthalensis(Neanderthals),andfinally,Homosapiens.Theprocessof
humanevolutionwasinfluencedbyfactorssuchasmigration,climatechange,andgenetic
mutations,leadingtothediversepopulationsandculturesthatexisttoday.”}
WebGPT CoT 14K – – {”instruction”:”Whatcausesthathorriblescreechingfeedbacksoundwhenamicrophone
gets too close to a speaker?\n\n”,”input”: ”[TITLE] Why does my microphone make
a loud screeching sound? (www.scienceabc.com)\n[CONTEXT] A typical audio system
(like a Public Address system or the audio system used by bands) consists of three
essential components: a microphone, a speaker and an amplifier. Wherever all three of
these components are present, there it the potential for feedback. The reason is simple;
feedbackoccurswhenthemicrophonepicksuptheoutputsoundfromthespeakersand
then sends it back to the speakers to re-amplify it, kick-starting an endless loop unless
interferedwithexternally.\n[TITLE]Whydoesmymicrophonemakealoudscreeching
sound? (www.scienceabc.com)\n[CONTEXT] However, if the sound discharged from
the speakers somehow reaches back to the mic (which ideally shouldn’t happen), the
process discussed above kick-starts again, i.e., the mic transmits sound to the amplifier,
which then transmits to the speaker, and back to the mic... and then this goes on and
on. The result is that you hear a high-pitched squeal, which gets louder and louder
(due to the reinforced amplitude as a result of multiple rounds of amplification) until
it is corrected.\n[TITLE] What Causes Microphone Feedback and How to Stop It -
Virtuoso Central (virtuosocentral.com)\n[CONTEXT] Audio feedback happens when a
soundcomingoutofaloudspeakerispickedupbythemicrophonereenteringthesound
systemagainandagaininanalwaysincreasingloop.Theresultisaloudrumbleorahigh
pitchringingnoise.\n”,”output”:”Audiofeedbackoccurswhenthemicrophonepicksup
theoutputsoundfromthespeakersandthensendsitbacktothespeakerstore-amplifyit,
kick-startinganendlessloopunlessinterferedwithexternally.[1]Ifthesounddischarged
from the speakers somehow reaches back to the mic, the process discussed above kick-
starts again, i.e., the mic transmits sound to the amplifier, which then transmits to the
speaker,andbacktothemic... andthenthisgoesonandon.Theresultisthatyouhear
ahigh-pitchedsqueal,whichgetslouderandlouder(duetothereinforcedamplitudeasa
resultofmultipleroundsofamplification)untilitiscorrected.[2]”}JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 23
TABLE8
StatisticsofDatasets(Part6).
Dataset Task Train Val Test Example
Lima CoT 1K – – {”instruction”: ”Can brain cells move? By movement I mean long distance migration
(preferablywithinthebrainonly).”,”input”:””,”output”:”Thequestionisrelativelybroadand
oneshouldtakeintoaccountthatthebrainnotonlyconsistsofneurons,butalsoglialcells
(supportivecells)andpre-mitoticneuronalstemcells.Furthermore,ascriticalfellow-scientists
haveindicated,developmentalstageisveryimportant,asthedevelopingembryonicbrainis
very different from the adult brain.\nHowever, after sifting through various publications,
theanswertothequestionisactuallyremarkablysimple:Yes,braincellsmigrate.\nInthe
adultbrainglialcellsmigrateinthebrain(Kl\u00e4mbt,2009).Glialcellsareinvolvedina
myriadoffunctions,butanotableexampleofmigratingglialcellsaretheoligodendrocytes
thatmigraterelativelongdistancestofindtheirtargetaxonsontowhichtheywrapthemselves
toformtheinsulatingmyelinsheath(TsaiandMiller,2002).\nNeuronalstemcellsmigrate
overlongdistancesinresponsetoinjury(Imitolaetal.,2004)andtheymigratefromspecific
stem-cell locations (e.g., hippocampus and subventricular zone) to other regions (Clarke,
2003).\nPost-mitotic,butnon-differentiatedneuronshavebeenshowntomigrateintheadult
braininfish(Scottetal.,2012),andinmammalsandnon-humanprimatesaswell(Sawada
et al., 2011).\nNot surprisingly, glial cells, stem cells and neurons also migrate during
embryonic development. Most notably, post-mitotic neurons destined to fulfill peripheral
functionshavetomigrateoverrelativelylongdistancesfromtheneuralcresttotheirtarget
locations(Neuroscience,2nded,NeuronalMigration).”}
Code-Alpaca Coding 20K – – {”instruction”: ”Create a Python program to accept a string from the user and print out
thenumberofvowelsinthestring.”,”input”:”HelloWorld”,”output”:”defcountVowels(str):
\n\tvowel=0\n\tforiinstr:\n\t\tifi.lower()in[’a’,’e’,’i’,’o’,’u’]:\n\t\t\tvowel+=
1\n\treturnvowel\n\n#Drivercode\nstr=\”HelloWorld\”\nprint(\”Numberofvowels
are:\”,countVowels(str))”}
ThoughtSource CoT 343K – – {”instruction”: ”Two friends plan to walk along a 43-km trail, starting at opposite ends
of the trail at the same time. If Friend P’s rate is 15% faster than Friend Q’s, how
manykilometerswillFriendPhavewalkedwhentheypasseachother?”,”input”:”choices:
21,21.5,22,22.5,23”,”output”: ”If Q complete x kilometers, then P completes 1.15x kilo-
meters.\nx + 1.15x = 43\n2.15x=43\nx = 43/2.15 = 20\nThen P will have have walked
1.15*20=23km.\nTheanswerisE.theansweris23”}
CAMEL-AI-Society Society 107K – – {”instruction”: ”Create a function that can read in a PDF file.”,”input”: ”File path to a
PDFfile.”,”output”:”“‘\nimportPyPDF2\n\ndefread pdf(file path):\nwithopen(file path,
’rb’)aspdf file:\npdf reader=PyPDF2.PdfFileReader(pdf file)\ntext=”\nforpagein
range(pdf reader.getNumPages()):\ntext+=pdf reader.getPage(page).extractText()\nreturn
text\n“‘\n\nThisfunctiontakesinafilepathtoaPDFfileandreturnsthetextcontentofthe
PDFfile.ItusesthePyPDF2librarytoreadinthePDFfileandextractthetextfromeach
page.Theextractedtextisthenconcatenatedandreturnedasasinglestring.”}
CAMEL-Code Coding 367K 0 0 {”Instruction”: ”Implement a method to extract text from a PDF file using Apache
PDFBox library.”,”Input”: ”Path to the PDF file.”,”output”:”Solution: Here’s a method
that uses Apache PDFBox library to extract text from a PDF file:\n\n“‘\nimport
java.io.IOException;\nimport org.apache.pdfbox.pdmodel.PDDocument;\nimport
org.apache.pdfbox.text.PDFTextStripper;\n\npublic String extractTextFromPDF(String
filePath) throws IOException {\n PDDocument document = null;\n try {\n
document = PDDocument.load(new File(filePath));\n PDFTextStripper stripper = new
PDFTextStripper();\n return stripper.getText(document);\n } finally {\n if (document !=
null){\ndocument.close();\n}\n}\n}\n“‘\n\nThismethodloadsthePDFdocumentfrom
thespecifiedfilepath,createsaPDFTextStripperobjecttoextractthetext,andreturnsthe
extractedtextasaString.Italsoensuresthatthedocumentisproperlyclosedafteruse.Next
request.”}
CAMEL-Math Math 46K 0 0 {”Instruction”:”Solveforx:5x+3=28”,”Input”:””,”output”:”Tosolveforx,weneedto
isolatexononesideoftheequation.Step1:Subtract3frombothsidesoftheequation.5x+
3-3=28-35x=25Step2:Dividebothsidesoftheequationby5.5x/5=25/5x=5So,
x=5.”}JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 24
TABLE8
StatisticsofDatasets(Part7).
Dataset Task Train Val Test Example
CAMEL-Physics Physics 20K 0 0 {”Instruction”:”Whatistheenergydifferencebetweenthen=4andn=2energylevelsofa
hydrogenatom,andwhatisthefrequencyofthephotonemittedwhenanelectrontransitions
from the n=4 to n=2 energy level?”,”Input”: ””,”output”:”To find the energy difference
between the n=4 and n=2 energy levels of a hydrogen atom, we can use the formula for
theenergyofanelectroninahydrogenatom:E n=-13.6eV/nˆ2whereE nistheenergy
oftheelectroninthenthenergylevelandnistheprincipalquantumnumber.First,let’sfind
theenergyoftheelectroninthen=4energylevel:E 4=-13.6eV/4ˆ2=-13.6eV/16=
-0.85eVNow,let’sfindtheenergyoftheelectroninthen=2energylevel:E 2=-13.6eV/
2ˆ2=-13.6eV/4=-3.4eVTheenergydifferencebetweenthen=4andn=2energylevels
is:∆E=E 4-E 2=-0.85eV-(-3.4eV)=2.55eVNow,weneedtofindthefrequencyof
thephotonemittedwhenanelectrontransitionsfromthen=4ton=2energylevel.First,we
needtoconverttheenergydifferencefromelectronvolts(eV)tojoules(J):1eV=1.60218
×10ˆ(-19)J∆E=2.55eV×1.60218×10ˆ(-19)J/eV=4.08556×10ˆ(-19)JNext,wecan
usethePlanck’sequationtofindthefrequencyofthephoton:E=h×fwhereEistheenergy
ofthephoton,histhePlanck’sconstant(6.626×10ˆ(-34)Js),andfisthefrequencyofthe
photon.Rearrangingtheequationtosolveforthefrequency:f=E/h=(4.08556×10ˆ(-19)
J)/(6.626×10ˆ(-34)Js)=6.165×10ˆ14HzSo,thefrequencyofthephotonemittedwhen
an electron transitions from the n=4 to n=2 energy level is approximately 6.165 × 10ˆ14
Hz.”}
CAMEL-Chemistry Chemistry 20K 0 0 {”Instruction”: ”What is the IUPAC name for the organic compound with the molecular
formulaC6H12O2?”,”Input”:””,”output”:”Therecanbeseveralisomerswiththemolecular
formula C6H12O2, so it is not possible to provide a specific IUPAC name without more
information about the structure of the compound. If you can provide the structure or any
additionaldetails,IwouldbehappytohelpyoudeterminetheIUPACname.”}
CAMEL-Biology Biology 20K 0 0 {”Instruction”: ”What are the common techniques used in identifying a new species, and
howcanscientistsaccuratelycategorizeitwithintheexistingtaxonomysystem?”,”Input”:
””,”output”:”Identifying a new species and categorizing it within the existing taxonomy
system involves several common techniques. These techniques can be broadly classified
into morphological, molecular, and ecological approaches. Here’s an overview of these
techniques:1.Morphologicalanalysis:Thisisthetraditionalmethodofidentifyinganew
species based on its physical characteristics. Scientists examine the external and internal
features of an organism, such as its size, shape, color, and structure. They also study the
anatomyandphysiologyoftheorganism,includingitsskeletal,muscular,andreproductive
systems.Bycomparingthesefeatureswiththoseofknownspecies,scientistscandetermine
iftheorganismrepresentsanewspecies.2.Molecularanalysis:Withadvancesinmolecular
biology, scientists can now use genetic information to identify and classify new species.
ThisinvolvesanalyzingtheDNAorRNAsequencesoftheorganismandcomparingthem
withthesequencesofknownspecies.TechniquessuchasDNAbarcoding,whichcompares
a short, standardized region of the genome, can help determine if the organism is a new
species. Additionally, phylogenetic analysis can be used to understand the evolutionary
relationshipsbetweenthenewspeciesandotherknownspecies,helpingtoplaceitwithin
theexistingtaxonomysystem.3.Ecologicalanalysis:Studyingtheecologicalnicheofan
organismcanalsoprovidevaluableinformationforspeciesidentificationandclassification.
Thisinvolvesexaminingtheorganism’shabitat,itsinteractionswithotherspecies,andits
roleintheecosystem.Byunderstandingtheecologicalcontextofthenewspecies,scientists
can better determine its taxonomic position and how it relates to other known species. 4.
Behavioralanalysis:Observingthebehaviorofanorganismcanalsohelpinidentifyinga
newspecies.Thisincludesstudyingitsfeedinghabits,matingrituals,andsocialinteractions.
Behavioraldifferencesbetweencloselyrelatedspeciescansometimesbemoreapparentthan
morphologicalormoleculardifferences.5.Biogeography:Thegeographicaldistributionof
anorganismcanprovidecluesaboutitsevolutionaryhistoryandhelpinitsclassification.
Studying the biogeography of a new species can reveal patterns of speciation and help
determine its taxonomic position. Once a new species has been identified using these
techniques, scientists can accurately categorize it within the existing taxonomy system by
followingtheprinciplesofbiologicalclassification.Thisinvolvesplacingthenewspecies
withinahierarchicalstructureoftaxonomicranks,suchaskingdom,phylum,class,order,
family,genus,andspecies.Thenewspeciesisthengivenascientificname,usuallyconsisting
of a genus name and a species epithet, following the rules of binomial nomenclature. In
conclusion, identifying a new species and categorizing it within the existing taxonomy
system involves a combination of morphological, molecular, ecological, behavioral, and
biogeographical analyses. By using these techniques, scientists can accurately classify the
newspeciesandbetterunderstanditsevolutionaryrelationshipswithotherorganisms.”}JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 25
TABLE9
EvaluationresultsofallLLaMA2modelsintheLoRAbank.
Model ARC-c ARC-e GSM8K MBPP BoolQ PiQA Average
Base 33.90% 49.91% 16.68% 18.20% 47.86% 53.05% 29.67%
ARC-c 44.07% 59.44% 15.54% 23.80% 43.33% 56.42% 35.71%
ARC-e 47.12% 62.61% 16.98% 24.60% 48.13% 58.11% 37.83%
WinoGrande 26.61% 53.26% 16.98% 23.60% 62.14% 56.09% 30.11%
GSM8K 20.81% 14.58% 22.88% 21.20% 18.53% 52.61% 19.87%
PiQA 37.97% 53.26% 17.51% 22.40% 42.51% 56.58% 32.79%
CommonSenseQA 37.29% 49.38% 16.38% 23.20% 46.18% 51.80% 31.56%
RACE 49.49% 69.31% 17.29% 21.40% 65.81% 55.55% 39.37%
MBPP 31.19% 46.56% 16.22% 22.40% 43.18% 52.77% 29.09%
MathQA 34.92% 49.03% 15.09% 21.80% 58.07% 53.65% 30.21%
Esnli 20.34% 27.34% 13.57% 21.40% 36.88% 44.45% 20.66%
ECQA 13.22% 13.58% 15.16% 21.80% 0.64% 35.15% 15.94%
CREAK 30.17% 34.22% 15.54% 21.20% 22.35% 56.53% 25.28%
GPT4Tools 25.76% 37.04% 17.29% 20.80% 31.87% 46.84% 25.22%
AQuA 39.32% 53.79% 15.31% 21.20% 64.77% 56.91% 32.41%
QASC 35.25% 42.33% 15.47% 21.60% 27.28% 55.66% 28.66%
QED 10.51% 14.99% 14.56% 21.60% 5.20% 35.91% 15.42%
StrategyQA 35.25% 45.50% 15.24% 20.40% 43.70% 56.37% 29.10%
SensemakingQA 19.66% 20.46% 15.16% 20.00% 34.95% 48.31% 18.82%
Toolformer 32.54% 36.86% 18.35% 19.00% 35.66% 51.80% 26.69%
HellaSwag 35.59% 50.79% 18.50% 23.20% 59.76% 45.27% 32.02%
SiQA 36.95% 49.74% 13.72% 18.20% 59.42% 56.53% 29.65%
BoolQ 36.61% 54.32% 9.48% 24.20% 63.39% 45.43% 31.15%
Dolly 31.19% 40.21% 17.21% 22.40% 15.84% 53.97% 27.75%
WizardLM 38.98% 49.38% 16.38% 16.00% 21.77% 50.98% 30.19%
ShareGPT 41.36% 55.38% 15.62% 18.20% 34.59% 53.97% 32.64%
GPT-Teacher-Code 34.58% 46.56% 18.50% 21.40% 46.88% 54.73% 30.26%
GPT-Teacher-Roleplay 35.25% 46.91% 17.51% 23.80% 40.64% 53.32% 30.87%
GPT-Teacher-Instruct 35.43% 48.93% 17.01% 21.20% 42.02% 55.28% 30.64%
WebGPT 32.88% 44.09% 14.40% 19.80% 30.89% 49.46% 27.79%
Lima 30.52% 39.51% 16.00% 21.20% 15.78% 50.65% 26.81%
Code-Alpaca 33.56% 44.80% 16.07% 21.80% 33.03% 51.69% 29.06%
ThoughtSource 35.59% 51.32% 20.85% 21.20% 54.98% 61.53% 32.24%
CAMEL-AI-Society 31.86% 41.09% 16.53% 19.40% 25.99% 43.04% 27.22%
CAMEL-Code 26.10% 23.99% 1.67% 1.00% 29.42% 40.86% 13.19%
CAMEL-Math 33.90% 48.32% 17.44% 22.80% 45.29% 50.00% 30.62%
CAMEL-Physics 26.78% 39.33% 15.24% 24.40% 32.08% 49.73% 26.44%
CAMEL-Chemistry 34.58% 40.74% 17.66% 24.20% 26.73% 48.37% 29.30%
CAMEL-Biology 31.53% 41.62% 17.59% 22.60% 31.93% 50.44% 28.34%JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 26
TABLE10
EvaluationresultsofallMistralmodelsintheLoRAbank.
Model ARC-c ARC-e GSM8K MBPP BoolQ PiQA Average
Base 60.68% 73.54% 45.56% 36.00% 55.96% 57.67% 54.90%
ARC-c 77.97% 88.36% 47.23% 40.20% 67.55% 64.85% 64.36%
ARC-e 80.00% 90.30% 51.55% 40.60% 59.39% 60.34% 63.70%
WinoGrande 61.69% 78.84% 47.54% 39.80% 69.30% 54.79% 58.66%
GSM8K 49.83% 63.84% 49.73% 36.80% 11.19% 58.27% 44.94%
PiQA 76.27% 87.83% 47.99% 40.60% 78.78% 68.12% 66.60%
CommonSenseQA 75.25% 81.31% 49.20% 40.40% 61.68% 65.02% 62.14%
RACE 74.92% 86.42% 48.67% 39.00% 74.53% 71.76% 65.88%
MBPP 65.08% 60.34% 44.88% 37.60% 48.56% 57.02% 52.25%
MathQA 73.90% 83.42% 43.52% 35.40% 76.79% 63.60% 62.77%
Esnli 43.05% 55.37% 31.92% 34.60% 0.49% 49.56% 35.83%
ECQA 60.00% 65.26% 36.92% 36.00% 5.78% 58.71% 43.78%
CREAK 68.81% 71.43% 42.99% 38.20% 6.06% 66.97% 49.08%
GPT4Tools 69.15% 75.13% 46.70% 38.00% 60.49% 58.49% 57.99%
AQuA 68.74% 77.78% 41.32% 38.00% 45.93% 64.15% 55.99%
QASC 72.20% 78.66% 39.65% 35.80% 16.33% 68.66% 51.88%
QED 71.86% 66.78% 39.27% 37.20% 50.55% 66.76% 55.40%
StrategyQA 72.88% 80.78% 43.06% 38.40% 64.56% 65.56% 60.87%
SensemakingQA 38.31% 46.91% 38.82% 35.20% 1.83% 56.42% 36.25%
Toolformer 69.49% 76.01% 45.64% 38.00% 54.07% 59.58% 57.13%
HellaSwag 77.63% 88.01% 47.23% 40.20% 67.37% 67.79% 64.71%
SiQA 74.24% 81.83% 47.99% 38.20% 54.01% 65.56% 60.31%
BoolQ 73.22% 87.13% 46.78% 39.20% 80.12% 61.04% 64.58%
Dolly 73.56% 83.07% 46.47% 38.80% 48.99% 66.38% 59.55%
WizardLM 73.90% 79.37% 46.63% 36.60% 70.67% 63.60% 61.80%
ShareGPT 69.83% 78.66% 45.64% 37.00% 68.50% 68.61% 61.37%
GPT-Teacher-Code 66.78% 76.37% 47.01% 40.20% 57.22% 59.25% 57.81%
GPT-Teacher-Roleplay 66.78% 74.07% 49.43% 37.60% 67.06% 63.22% 59.69%
GPT-Teacher-Instruct 72.54% 79.72% 47.16% 37.60% 78.20% 64.74% 63.33%
WebGPT 63.39% 76.37% 43.21% 36.40% 52.91% 59.09% 55.23%
Lima 69.83% 77.43% 45.34% 37.80% 30.52% 56.64% 52.93%
Code-Alpaca 71.53% 79.89% 45.72% 40.00% 44.56% 62.40% 57.35%
ThoughtSource 77.97% 83.77% 45.49% 37.00% 78.38% 70.95% 65.59%
CAMEL-AI-Society 68.81% 77.07% 44.66% 38.00% 57.58% 63.49% 58.27%
CAMEL-Code 75.59% 79.54% 45.94% 37.80% 46.67% 65.23% 58.46%
CAMEL-Math 77.97% 84.30% 46.02% 38.80% 58.65% 60.01% 60.96%
CAMEL-Physics 71.86% 79.01% 46.78% 38.20% 51.01% 58.60% 57.58%
CAMEL-Chemistry 70.51% 80.42% 46.70% 38.40% 54.89% 60.72% 58.61%
CAMEL-Biology 63.73% 73.90% 47.16% 37.00% 52.02% 56.86% 55.11%JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 27
TABLE11
Listofsymbols.
Symbol Description
x aninputsequence(i.e.,instruction)consistingofmultipletokensx ,i=1,2,...,|x|
(i)
y anoutputsequence(i.e.,response)consistingofmultipletokensy ,i=1,2,...,|y|
(i)
x thei-thtokenofx
(i)
y thei-thtokenofy
(i)
y thesequenceofy ,j<i,i.e.,[y ,y ,...,y ]
(<i) (j) (1) (2) (i−1)
m aLLMmodelfromtheLoRAbank
θm theparametersofm
P(y (i)|x,y (<i);θm) themodelm’sprobabilityoftheoutputtokeny (i)giventheinputxandtheprecedingoutputtokensy
(<i)
yˆ theexpandedoutputofywithCoTrationales
L(x,y,θ MoE) thecross-entropylanguagemodelinglossoftheMoEsystemonygivenx
PPL(x,y,θm) theperplexityofthemodelmparameterizedbyθmonthesequencesygivenx
Φ(·,·) theCoTexpansionprocessimplementedwithprompts
K thenumberofhuman-annotatedinstructionsfromtasksofinterest
DK K-shotdatafromthetaskofinterest:{(x1,y1),(x2,y2),...,(xK,yK)}
PPL(x,yˆ,θm) thereasoningperplexityofthemodelmparameterizedbyθmontheCoT-formattedyˆgivenx
PPLR thetotalreasoningperplexityofthemodelmonDK
f(·) thepost-processingfunctionforstandardizationofmodelresponses
Acc(m,DK,f) theperformanceofthemodelmonDK inexactmatchaccuracywithapost-processingfunctionf(·)
y˜ theauto-regressivelygeneratedsequencegivenx
y˜′ thepost-processedresponseofy˜
1(·) theindicatorfunction
B themodelbankconsistsofmultiplemodels{m1,m2,...,m |B|}
M thenumberofcandidatemodels
BM asetofcandidatemodelsBM ⊂B
E(mi) theflattenedmatricesofalllayersofmi
N thenumberofexperts
BN asetofchosenexpertsBN ⊂BM
ΩBN theintra-groupdiversityofBN
rank(·) therankingofmodelsbycertainindices
RL(m) therankofthemodelmbyL(m,DK)fromsmallesttolargest
RP(m) therankofthemodelmbyAcc(m,DK,f)fromlargesttosmallest
RD(BN) therankoftheN-tupleBN byintra-groupdiversityΩBN fromthelargesttothesmallest
BE thechosenexpertsforinitializationofaMoEsystem
wi theweightofthei-thLoRAmoduleforlinearcompositionofmodelmerging
W theoriginalparameterofapre-trainedmodel
∆Wi thei-thLoRAvariantwith(cid:80)N i=1wi=1
gl(x) theoutputofaN-expertMoEsystematthel-thlayergiveninputx
Gl(x) thegatingvectoratthel-thlayerwithGl(x)∈RN givenx
Gl(x) thei-thelementofthegatingvectorGl(x)
i
gl(x) theoutputfromthei-thexpertatthel-thlayer
i
Wl thematrixofasinglefully-connectedlayerforthegatingnetwork
g
k thenumberoftheselectedexpertsinaMoEsystem
top-k(·) theoperationthatreturnsthelargestkelementsunchangedandtheotherelementsas−∞
DS asetofSopen-sourcesamples:{(x1,y1),(x2,y2),...,(xS,yS)}
h apre-trainedencodingmodel
θ
h
theparametersofthepre-trainedmodelh
ui theembeddingofbothinputandoutput[xi,yi]
UK theembeddingsofsamplesinDK
US theembeddingsofsamplesinDS
DC thecandidateopen-sourceinstructionsetDC ⊂DS
C thebudgetforthecandidatedatasetDC
d(·,·) thedistancefunction
sim(·,·) thesimilaritymetric
Aij theentryofthecross-datasetsimilaritymatrixbetweeneachpairofui∈UK anduj ∈US,A∈RK×S
Iij theentryoftheintra-datasetsimilaritymatrixbetweenui∈UC anduj ∈UC,UC ={ui|(xi,yi)∈DC}
τ thethresholdforsemanticdeduplication
DA thechosenopen-sourcedatasetfordataaugmentation
DT theofficialtrainingsetfromthedownstreamtaskofinterest(onlyinvolvedinablationstudies)
U KC theconvexhulloftheembeddingsUK fromtheK-shotdataDK
λi thecoefficientsthatdeterminetheconvexcombinationofsamplesinUK
Ker(·) thekernelfunctionfordistancemeasurement
γ thebandwidthcontrollingthesmoothnessofthedensityfunction
p(u) theestimateddensityprobabilityofasampleuintheembeddingspaceJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 28
TABLE12
Listofhyper-parametersfortrainingsettings.
Settings LoRAModels(Single) MoESystem
Optimizer AdamW
Optimizermomentum 0.9
Optimizerweightdecay 1×10−4
Batchsize 2
Gradientaccumulationsteps 16
Trainingscheduler Cosinedecaywithlinearwarm-up
Learningrate 5e-5
TransformersVersion 4.36.1
LocationofPEFT q proj,v proj
Cutofflength 1024
Deepspeed Stage3
Trainingepochsintotal 3 5