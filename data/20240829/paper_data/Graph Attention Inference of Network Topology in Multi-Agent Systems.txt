Graph Attention Inference of Network
⋆
Topology in Multi-Agent Systems
Akshay Kolli∗ Reza Azadeh∗ Kshitij Jerath∗∗
∗Richard Miner School of Computer and Information Sciences,
University of Massachusetts Lowell, MA, USA
(akshay kolli@student.uml.edu, reza@cs.uml.edu).
∗∗Department of Mechanical Engineering, University of Massachusetts
Lowell, MA, USA (kshitij jerath@uml.edu)
Abstract: Accurately identifying the underlying graph structures of multi-agent systems
remains a difficult challenge. Our work introduces a novel machine learning-based solution
that leverages the attention mechanism to predict future states of multi-agent systems by
learning node representations. The graph structure is then inferred from the strength of the
attentionvalues.Thisapproachisappliedtobothlinearconsensusdynamicsandthenon-linear
dynamics of Kuramoto oscillators, resulting in implicit learning the graph by learning good
agent representations. Our results demonstrate that the presented data-driven graph attention
machine learning model can identify the network topology in multi-agent systems, even when
the underlying dynamic model is not known,as evidenced by the F1 scoresachievedin the link
prediction.
Keywords: Neural Networks, Multi-Agent Systems, Attention, Machine Learning, Complex
Systems
1. INTRODUCTION Several previous works have sought to solve the challenge
of network topology inference, often also referred to as
Networks are pivotal in modeling and understanding a graph estimation or graph learning. Both Ortega et al.
wide range of natural and engineered complex systems (2018)andMateosetal.(2019)provideanexcellentsurvey
(Lewis (2011), Newman (2018)) such as technological of these efforts. These workshave largely been focused on
networks (Cai et al. (2016)), social networks (Borgatti identifying the graph structure based on incoming datas-
etal.(2009)),biologicalsystems(Baronchellietal.(2013), treamsfromnodeswithoutfullyleveragingtheunderlying
Sporns (2018)). Within the context of complex multi- agentornode dynamics.Onthe otherhand,the modeling
agent systems (MAS), modeling is enabled significantly and controls researchcommunity has focused a large part
by formulating their dynamics via a network representa- of its attention on control and coordination problems in
tion. These networks define the inter-agent interactions networked systems where the underlying graph structure
as well as system characteristics such as the convergence is either known or some appropriate assumptions can be
properties in consensus dynamics or formation control, made about it (Wen et al. (2021)). This leaves a knowl-
(Olfati-Saberetal.(2007)).Tocontrol,influenceorunder- edge gap which, if addressed, can benefit both research
standsuchsystemswerequireadeeperknowledgeoftheir communities:predictingthedynamicsofindividualagents
underlying network topology. Unfortunately, the network or nodes can assist with the network topology inference
structureisoftennotavailable(orisonlypartiallyknown) challenges, and learning the underling interaction graph
and must be inferred by observing the behavior of the of dynamic agents can assist with control of networked
multi-agent system, which is a challenging problem. multi-agent systems.
Understanding the topology of systems where the system Towards this end, some recent works have attempted to
dynamicsareunknownisusefulacrossamyriadofsystems use graph signal processing and graph attention network
biological networks and social networks. Identification of approaches to address this knowledge gap (Shuman et al.
the emergence of team structures in coordinated tasks, (2013), Batreddy et al. (2023)). For example, Zhu et al.
evaluation of paths and connections in neuron, and graph (2020) use a graph signal processing approach to recover
analysis of gene regulatory networks all provide great theadjacencymatrixusingsnapshotsofconsensusdynam-
insights without requiring the dynamics of the system to ics. Their work estimates the eigenvectors of the graph
be known. Laplacian using the sample covariance of node data and
demonstratesthatthe spectralpropertiesofthe recovered
⋆ graphmatchedthoseofthe originalnetworkstructure.To
© 2024 the authors. This work has been accepted to IFAC for
our knowledge, this work is limited to linear consensus
publication under a Creative Commons Licence CC-BY-NC-ND”.
ThisworkwassupportedbytheStrengtheningTeamworkforRobust problems. Similarly, from a graph attention network per-
OperationsinNovelGroups(STRONG)CRAattheArmyResearch spective,Sebasti´anetal.(2023)presentamachinelearning
LabunderthecontractW911NF20-2-0089.
4202
guA
72
]AM.sc[
1v94451.8042:viXramodel to extract information from node trajectories in a (2019)). We build a machine learning model to obtain
multi-agent system. This approach shows promise when neural network parameters Θ∗ such that:
applied to consensus dynamics and flocking scenarios. Θ∗ =argmin E[loss(x(t),xˆ(t))] (3)
However, despite its innovative approach, this method Θ
requires knowledge of the true adjacency matrix for at where x(t) is the state of the system at time step t loss is
least one example of the problem class, to be able to ac- the mean absolute loss function, and xˆ(t) = fΘ({x} t−τ:t)
curatelypredicttheadjacencymatrix.Inouropinion,this is the prediction of the state made by the learned neural
presents a relatively strongassumption,and the approach network model at time step t. The learned model fΘ
ispotentiallyunsuitableinsettingswherepriorknowledge includes an interpretable attention layer or matrix Aˆ that
of comparable graphs is unavailable.
we demonstrate canmaponto the adjacencymatrix A as
In contrast, the approach we present here is applicable it represents the degree to which agents interact with one
to even those scenarios where no prior information of another. Once the model parameters have been learned,
the network topology is available. Specifically, we pro- we can use the attention matrix Aˆ as an approximate
pose a data-driven graph attention mechanism approach representation of the adjacency matrix A and thus can
(Veliˇckovi´c et al. (2017)) that learns the unknown graph help inform us of the underlying topology that governs
structure (unsupervised) while using full state observa- the dynamics of the multi-agent system.
tions from the unknown dynamics of agents or nodes for
a prediction task (supervised). We demonstrate this work 3. LEARNING THE NETWORK TOPOLOGY
notonlyinthelinearconsensusdynamicssetting,butalso
for nonlinear synchronizationscenarios such as Kuramoto The attention mechanism (Bahdanau et al. (2014)) is a
oscillators (Rodrigues et al. (2016)). Our approachis able powerful mechanism that allows a model to dynamically
to concurrently learn to predict the future states of MAS pay attention to specific parts of its input (Xu et al.
exhibiting collective behavior, as well as learn the under- (2015)). Attention has found its use in Graph Neural
lying topology of the system within the same framework, Networks(Guoetal.(2019))providingawayfornodes to
as we demonstrate below. attendto eachoftheir neighborsandproducingnode rep-
resentations that perform well in downstream tasks. One
2. PROBLEM FORMULATION downstreamtaskthatisespeciallyusefulfornetworkeddy-
namicsystemsisthepredictionoffuturestatetrajectories
We begin by considering a dynamic model of a networked of agents in such systems. This task can find applications
system operating on graph G = (V,E), where V denotes in various fields such as trajectory tracking (Yang and
the set of nodes (or agents) in the system, and E denotes Jerath (2024)), identifying agent influence (Jerath et al.
the edges ofthe network.The number of agents is defined (2024)), model predictive control, and reachability analy-
as |V| = N. Let A be the adjacency matrix of graph G, sis,to name a few. We now discuss the architectureof the
i.e. a ∈ {0,1} where a = 1 indicates the presence of attention-based neural network model.
ij ij
an edge between nodes i and j, and a = 0 denotes the
ij
absence of one. 3.1 Model Architecture
We assume that the networked system has undirected
Our graph attention-based predictive model learns vector
edges,sothattheadjacencymatrixissymmetric,i.e.a =
ij representations for each agent in the multi-agent system.
a
ji
forall(i,j).Thestateoftheith agentattimetisgiven
In the process of learning to predict the states of the
by x i(t) ∈ R1, and the collective state of the networked model,themodelalsolearnstherelationshipbetweeneach
system is given by x(t) = [x1(t),x2(t),...,x N(t)]⊤. The of the agents and encodes it into their representations.
consensus dynamics are given by: Our novel contribution is the formulation and placement
x˙(t)=−Lx(t) (1) of the attention mechanism, such that attention scores
are representative of the adjacency matrix in the multi-
where L represents the graph Laplacian. We also demon-
agent system, while simultaneously contributing to the
strate our work using Kuramoto oscillators,for which the
predictive power of the model.
dynamics are given by:
N Theattentionmechanisminthismodelactslikeaselective
dφ K
i =ω i+ Xsin(φ
j
−φ i) (2) focus, enabling the model to dynamically prioritize and
dt N
j=1 weigh the influence of different agents on each other. It
operates by generating specific vectors—termed ‘key’ and
where φ (t) represents the phase of the ith agent (i = 1, ‘query’vectors—fromtheinitialrepresentationsofagents.
i
2, ... N) at time step t, ω denotes the innate frequency of These vectors help determine the extent of influence or
i
the ith oscillator,K representsthecouplingconstant.The attention one agent should pay to another during the
collective state is x(t) = [φ1(t),φ2(t),...,φ N(t)]⊤. The set learningprocess.Thisisachievedbycomparingallpairsof
ofKuramotooscillatorsexhibitscollectivesynchronization key and query vectors across agents, effectively assessing
behaviorasresultofthe couplingbetweenthem (Strogatz their mutual relevance.
(2000)).
Afterestablishingtherelevancebetweenagents,themodel
The goal of this work is to develop a graph attention- computes a set of attention scores. These scores are akin
based mechanism that can learn the network topology of to the entries of an adjacency matrix in graph theory,
a networked multi-agent system with unknown dynamics depicting the connections between nodes (agents). Higher
by learning to predict the state of the system (Guo et al. scores indicate stronger relationships or influences. These(b)
Fig. 2. The model consists of 4 key components (shown
Fig. 1. Examples of the simulation dynamics. (a) Con-
in blue) that get learned: The agent embeddings, the
sensus dynamics simulation (b) Kuramoto Oscillator
translation layer, the attention projection layer and
simulation. The y-axis on both the plots represents
the head.
the state information for each agent. The x-axis rep-
resents the time steps taken in the simulations. The
plots on the bottom right represent the graph that Agent Embeddings: These embeddings are a collection
the simulations operate on, with yellow signifying a ofvectors,eachvectorisarepresentationofanagentinthe
connection in the adjacency matrix, and purple rep- multi-agent system. The vector embeddings converge to
resenting a zero. being a good representationof each agent during training
such that a high prediction accuracy of trajectories is
scoresthen modulate the interactiondynamics in the net- achieved.
work by focusing the model’s learning on more influential Attention projection layer: This neural network layer
connections, hence enhancing its predictive accuracy. producesthekeyandqueryvectorsfromtheagentembed-
dings.Thekeyandqueryvectorsaresubsequentlyusedto
This attention mechanism is inspired by techniques origi-
compute the attention between each of the agents.
nally developed for machine translation, where the model
Translation Layer: This neuralnetworklayertranslates
needsto decidewhichwordsinasourcesentence aremost
the state information of each agent into a vector in the
relevant when predicting a word in the translation. Here,
model’s latent space.
instead of words, the model evaluates which agents in a
system are most relevant to predict future states effec- Head: This neural network layer converts a vector from
tively. This dynamic, context-based focusing capability is themodel’slatentspaceintoapredictioninthestatespace
what enables the model to perform robustly in complex, of the MAS.
variable multi-agent environments.
Ourmethodincorporatesatechniquefirstusedinmachine
As illustrated in Figure 2, the model consists of several translation (Vaswani et al. (2017)). The first step is to
neural layers, each communicating and passing informa- generate a key and a query vector from each of the agent
tionusingvectorsinthemodel’slatentspace.Themodel’s embeddings using the projection layer. A value vector
latentspaceisad-dimensionalvectorspace.Wesetd=64 is generated from the state x i(t) for each agent using
inallofourexperiments.Thissetupensuresthattheinter- the translation layer. Scaled dot product attention is
nalrepresentationsmaintainaconsistentlevelofcomplex- computed using the keys and the queries, giving us the
ityanddetail.Themodelconsistsoffourmaincomponents attention matrix Aˆ. The prediction is then obtained by
that are learned (indicated by blue boxes in Figure 2), passing the dot product of the attention matrix RN×N
which are listed below. matrix and the values RN×d through the head layer.3.2 Model Training defined as the harmonic mean of precision and recall.
Specifically, the F1 score is defined as:
Themodelistrainedusingdatageneratedfromnumerical precision × recall
F1=2 (4)
simulations of the dynamic models for the multi-agent
precision + recall
systems, i.e. the consensus dynamics and the Kuramoto
where precision is defined as the ratio of the number of
oscillators. In these simulations, the underlying network
correctly predicted links to the total number of correctly
topology for the multi-agent system is a randomly gener-
and falsely predicted links. It measures the accuracy of
ated Erdo˝s-R´enyigraph with edge probability p=0.5.
positivepredictions.Ontheotherhand,recall,alsoknown
The agent states in consensus dynamics are initialized assensitivity,measurestheabilityofamodeltofindallthe
randomly within predetermined bounds, while those of relevant cases within a dataset. It is defined as the ratio
theKuramotooscillatorsarerandomlyinitializedbetween of the number of correctly predicted links to the actual
(−π,π). Our experiments suggested that a larger initial- number of links in the network.
ization bound is helpful in preventing vanishing gradients
during back-propagation. The simulations are run for up 4.1 Network Topology Inference in Consensus Dynamics
to 1000timesteps to ensure that the datasetis dominated
bythetransientdynamicsonly,whileavoidingsteadystate Figure3ashowstheF1scoreperformanceofthemodelfor
conditions which would limit the inference capabilities for systems with varying number of agents in the multi-agent
this or any similar inference approach. system. The green line serves as a baseline, representing
the mean F1 score achieved by adjacency matrices gen-
Morespecifically,the trainingdatasetconsistsofthe state
erated from random graphs in comparison to the target
x(t)atonetimestepastheinput,andthestatex(t+1)at
graph.The model shows a gradualdecline in performance
the next timestep as the target. Our choice of using state
as the number of agents increases. Each training run had
information at a single timestep as the basic constituent
n=100simulationruns.Thedecreaseinperformancecould
of the training data is driven by the need to ensure
be because of the larger number of relationships to learn,
that the neural network learns the network topology. If
causing it to require more than 100 simulation runs to
longer sequences of state information were to be used as
accurately represent the adjacency matrix. Another lim-
inputs, the neural network would learn to extrapolate the
itation could be the use of the softmax functions, which
trajectory rather than infer the network topology.
obscures weaker connections as the number of agents in-
During training, the model uses the current system state creases.
x(t),processesitthroughthegraphattention-basedneural
Figure3bshowstheF1scoreplottedagainstthenumberof
network, and obtains the predicted state xˆ(t + 1). This
simulationsinthetrainingset.Thedifferentlinesrepresent
prediction is compared against the true states x(t + 1)
multi-agentsystems with increasing number of agents.To
knownfrom the numericalsimulations that constitute the
make equally accurate graph topology inference, systems
training dataset.The resultingmean absolute errorloss is
with a small number of agents require considerably fewer
then used to optimize the neural network model weights,
number of simulations in the training data than systems
and an ADAM optimizer is used to perform the back
with larger number of simulations. Though systems with
propagation.
alargenumberofagentsstartofwithasignificantlylower
Through the training process, the model refines its agent F1 score, the score steadily increases with increase in the
embedding vectorsto better capture the characteristicsof number of simulations. The quality of predictions for a
the agents.Asthetrainingprogressesthroughtheepochs, systems of any size are limited by the training resources
the attention between the agent embeddings Aˆ converges available.
towards the true adjacency matrix A. In the next section,
we discuss the efficacy of our approach in inferring the 4.2 Network Topology Inference in Kuramoto Oscillators
network topology as it learns to become proficient at
predicting the future states of a dynamic system. Figure 3c shows the plots for the F1 score performance
of the model in predicting the adjacency matrix for a
4. RESULTS system of Kuramotooscillators.The accuracyfor systems
of all sizes are worse, when compared to their consensus
To examine the performance of our approach, we first dynamics counterparts. The inherent non-linear nature of
applyathresholdfunctiontoconvertthereal-valuedatten- Kuramotooscillatorsmakesitmoredifficultforthetrained
tion matrix Aˆ to a binary-valued approximate adjacency model to provide accurate predictions. Nevertheless, the
matrix. The threshold value was heuristically selected to performance does improve with increase in the number
be −0.4 (based on the observed attention scores). Values of simulations, as evidenced in Figure 3d, making the
intheattentionmatrixthatwerelowerthanthethreshold training resources be the limiting factor for achieving
weresettozero,i.e.,lowattentionscoreswerereplacedby better performance.
0 to indicate the absence of an edge between the agents,
while higher attention scores were set to 1 (to represent 4.3 Learning of attention throughout training
the presence of an edge). Next, we use the F1 score to
comparethe binary-valuedapproximateadjacencymatrix Figure 4 shows the attention that is learned during the
against the true adjacency matrix used for the numerical different stages of training. Each agent pays the most
simulation of the dynamic system. The F1 score is a well- attention to itself, which is intuitive, since each agent’s
known measure for binary classification problems, and is future state is most likely to correspond with its currentFig.3.(a),(c)ProposedmethodyieldshigherF1scores(i.e.,betterperformance)forgraphinferencewithsmallnumber
agents in both Consensus dynamics and Kuramoto oscillators. If data is limited, performance drops for systems
with moreagents.Performancecalculatedwith 100simulationsworthofdata.(b),(d) Using additionalsimulation
data improves inference performance for larger systems as well.
state. This is the easiest relationship to learn, hence thereby enhancing its applicability to weighted directed
the first that is learned. The remaining relationships are graphs.
learned gradually over time, but remain weaker than the
We anticipate extending our research to the dynamic
attention being paid to themselves.
identification and tracking of time-varying graphs. As
the underlying graph evolves, so too should the learned
5. CONCLUDING REMARKS
representations, enabling the continuous detection and
monitoring of these dynamic changes. This progression
In this work, we tackled the problem of learning the
holds significant promise for the advancement of graph-
graphthatamulti-agentsystemoperateson,fromthestate
based learning systems and their application across a
history of the system.
myriad of complex, real-worldscenarios.
Multi-agent systems are often found to be non-linear in
ACKNOWLEDGEMENTS
nature,andstudyingthemodelperformanceonKuramoto
oscillators can extend our understanding of synchroniza-
I’dlike tothankmyparentsfortheirendlesssupport,and
tion phenomena, highlight the importance of network
my advisor Dr. Jerath for his invaluable guidance.
topology in collective dynamics, and offer insights into
designingmoreefficientandrobustdistributedsystemsfor
REFERENCES
various applications.We demonstratedthat pure machine
learningapproachescanbeusedtoconstructflexiblepow- Bahdanau, D., Cho, K., and Bengio, Y. (2014). Neural
erful models for graphprediction on linear and non-linear machine translation by jointly learning to align and
systems. The key contribution of this work is to present translate. arXiv preprint arXiv:1409.0473.
a graph learning or network topology inference approach Baronchelli, A., Ferrer-i Cancho, R., Pastor-Satorras, R.,
for multi-agent systems where both the prior structure Chater, N., and Christiansen, M.H. (2013). Networks
of graphs as well as system dynamics is unknown. They incognitivescience. Trends in cognitive sciences, 17(7),
provideflexibilityinthattheycanbeusedforalargearray 348–360.
ofproblems.The underlyingdynamicsneednotbe known Batreddy,S.,Siripuram,A.,andZhang,J.(2023). Robust
tobeabletopredictthegraph.Possiblefutureapplications graphlearningfor classification. Signal Processing, 211,
can be in measuring trust or reliance on teammates in 109120.
autonomous multi-agent teams, dynamic adjustment of Borgatti, S.P., Mehra, A., Brass, D.J., and Labianca, G.
roles inteams optimizing team performance,coordination (2009). Network analysisin the socialsciences. science,
in decentralized clusters, and improving Human-Robot 323(5916),892–895.
interaction. Cai, Y., Cao, Y., Li, Y., Huang, T., and Zhou, B. (2016).
Cascading failure analysis considering interaction be-
Our findings align with recent studies that emphasize the tween power grids and communication networks. IEEE
importance of prioritizing agents based on their specific Transactions on Smart Grid, 7(1), 530–538.
properties Findik et al. (2023) and incorporating social Findik, Y., Robinette, P., Jerath, K., and Ah-
interactions into multi-agent systems Haeri (2021), rein- madzadeh, S.R. (2023). Impact of relational net-
forcing the potential of relational awareness and network works in multi-agent learning: A value-based fac-
topology in guiding cooperation strategies and shaping torization view. In 2023 62nd IEEE Conference
team behaviors. on Decision and Control (CDC), 4447–4454. doi:
10.1109/CDC49753.2023.10383543.
5.1 Future Work Guo, K., Wang, D., Huang, J., Chen, Y., Zhu, Z., and
Zheng, J. (2019). A graph representationlearning algo-
In our study, we delved into the application of our sys- rithm based on attention mechanism and node similar-
tem to undirected graphs, demonstrating its potential for ity.InComputerSupportedCooperativeWorkandSocial
straightforward extension to directed graphs. The next Computing:14thCCFConference,ChineseCSCW2019,
phase ofour researchshouldaimatrefining the systemto Kunming, China, August 16–18, 2019, Revised Selected
accurately extract edge weights along with the topology, Papers 14, 591–604.Springer.Fig.4.Visualizingattentionvaluesthroughtrainingstages,withthefinalcolumnthresholdingtheattentionvalueinthe
final epoch to give the predicted adjacency matrix. Reults demonstrate that most of the learning is done through
the early epochs, with the later epochs adding to the final details.
Haeri, H. (2021). Reward-sharing relational networks in processing on graphs: Extending high-dimensional data
multi-agent reinforcement learning as a framework for analysistonetworksandotherirregulardomains. IEEE
emergent behavior. In Proceedings of the 20th Inter- signal processing magazine, 30(3), 83–98.
national Conference on Autonomous Agents and Multi- Sporns, O. (2018). Graph theory methods: applications
Agent Systems, AAMAS ’21, 1808–1810. International in brain networks. Dialogues in clinical neuroscience,
Foundation for Autonomous Agents and Multiagent 20(2), 111–121.
Systems, Richland, SC. Strogatz, S.H. (2000). From kuramoto to crawford: ex-
Jerath, K., Gayah, V.V., and Brennan, S.N. (2024). Mit- ploring the onset of synchronization in populations of
igating delay due to capacity drop near freeway bottle- coupled oscillators. Physica D: Nonlinear Phenomena,
necks: Zones of influence of connected vehicles. PLOS 143(1-4),1–20.
ONE, 19(6), 1–30. doi:10.1371/journal.pone.0301188. Vaswani,A.,Shazeer,N.,Parmar,N.,Uszkoreit,J.,Jones,
Lewis,T.G. (2011). Network science: Theory and applica- L., Gomez, A.N., Kaiser, L ., and Polosukhin, I. (2017).
tions. John Wiley & Sons. Attention is all you need. Advances in neural informa-
Mateos, G., Segarra, S., Marques, A.G., and Ribeiro, tion processing systems, 30.
A. (2019). Connecting the dots: Identifying network Veliˇckovi´c, P., Cucurull, G., Casanova, A., Romero, A.,
structure via graph signal processing. IEEE Signal Lio, P., and Bengio, Y. (2017). Graph attention net-
Processing Magazine, 36(3), 16–43. works. arXiv preprint arXiv:1710.10903.
Newman, M. (2018). Networks. Oxford university press. Wen,G.,Yu,X.,Yu,W., andLu,J.(2021). Coordination
Olfati-Saber, R., Fax, J.A., and Murray, R.M. (2007). and control of complex network systems with switching
Consensus and cooperation in networked multi-agent topologies: A survey. IEEE Transactions on Systems,
systems. Proceedings of the IEEE, 95(1), 215–233. Man, andCybernetics: Systems,51(10),6342–6357.doi:
Ortega, A., Frossard, P., Kovaˇcevi´c, J., Moura, J.M.F., 10.1109/TSMC.2019.2961753.
and Vandergheynst, P. (2018). Graph signal pro- Xu,K.,Ba,J.,Kiros,R.,Cho,K.,Courville,A.,Salakhudi-
cessing: Overview, challenges, and applications. Pro- nov, R., Zemel, R., and Bengio, Y. (2015). Show,
ceedings of the IEEE, 106(5), 808–828. doi: attend and tell: Neural image caption generation with
10.1109/JPROC.2018.2820126. visualattention.InInternationalconferenceonmachine
Rodrigues, F.A., Peron, T.K.D., Ji, P., and Kurths, J. learning, 2048–2057.PMLR.
(2016). The kuramoto model in complex networks. Yang, Z. and Jerath, K. (2024). Energy-guideddata sam-
Physics Reports, 610, 1–98. pling for traffic prediction with mini training datasets.
Sebasti´an, E., Duong, T., Atanasov, N., Montijano, E., Zhu, Y., Schaub, M.T., Jadbabaie, A., and Segarra, S.
and Sagu¨´es, C. (2023). Learning to identify graphs (2020). Network inference from consensus dynamics
fromnode trajectoriesinmulti-robotnetworks. In2023 with unknown parameters. IEEE Transactions on Sig-
International Symposium on Multi-Robot and Multi- nal and Information Processing over Networks, 6, 300–
Agent Systems (MRS), 142–148.IEEE. 315.
Shuman, D.I., Narang,S.K., Frossard,P.,Ortega,A., and
Vandergheynst, P. (2013). The emerging field of signal