Highlights
Generalized Naive Bayes
Edith Alice Kovács, Anna Ország, Dániel Pfeifer, András Benczúr
• A Generalized Naive Bayes (GNB) structure is introduced;
• It is proven that the so called GNB probability distribution associated
to the GNB structure gives an approximation at least as good as the
probability distribution associated to the Naive Bayes structure;
• Two new algorithms are introduced: Algorithm GNB-A, which is an ef-
ficientgreedyalgorithmandAlgorithmGNB-Owhichfindstheoptimal
structure given a not very restrictive condition;
• The new algorithms’ complexities are calculated and compared with
the related ones;
• Based on the algorithms a feature selection method and feature impor-
tance score are introduced.
• The algorithms are tested on real, medical datasets, using a "general"
discretization, and compared with the related algorithms;
4202
guA
82
]LM.tats[
1v32951.8042:viXraGeneralized Naive Bayes
Edith Alice Kovácsa, Anna Országb, Dániel Pfeifera, András Benczúrb
aBudapest University of Technology and Economics, Budapest, Hungary,
bInstitute for Computer Science and Control (SZTAKI), Hungarian Research Network
(HUN-REN), Budapest, Hungary,
Abstract
In this paper we introduce the so-called Generalized Naive Bayes structure as
an extension of the Naive Bayes structure. We give a new greedy algorithm
that finds a good fitting Generalized Naive Bayes (GNB) probability distri-
bution. We prove that this fits the data at least as well as the probability
distribution determined by the classical Naive Bayes (NB). Then, under a
not very restrictive condition, we give a second algorithm for which we can
prove that it finds the optimal GNB probability distribution, i.e. best fitting
structure in the sense of KL divergence. Both algorithms are constructed to
maximize the information content and aim to minimize redundancy. Based
on these algorithms, new methods for feature selection are introduced. We
discuss the similarities and differences to other related algorithms in terms
of structure, methodology, and complexity. Experimental results show, that
the algorithms introduced outperform the related algorithms in many cases.
Keywords: Naive Bayes, Generalized Naive Bayes, conditional
independence, classification, feature selection, classification on health data
2000 MSC: 62C12, 62C10, 62-07
1. Introduction
Naive Bayes is one of the most popular machine learning algorithms due
to its simplicity, efficiency and easy interpretation which is appealing for
experts in various domains. Therefore, Naive Bayes (NB) is considered to be
one of the top 10 data mining algorithms [1].
Inordertoshowtheubiquitousinterestinthisalgorithmwegiveaglimpse
of some recent applications of it. A recent work [2] introduces the COVID-
19 Prudential Expectation Strategy (CPES) as a new strategy for predicting
Preprint submitted to Name of Journal August 29, 2024the behavior of a person’s body if he has been infected with COVID-19.
For the classification task, they proposed the Statistical Naive Bayes algo-
rithm. It turned out that this methodology outperforms other competing
methods. Another paper [3] uses Distance Biased Naive Bayes (DBNB) clas-
sificationstrategyforaccuratediagnosisofCovid-19patientswhichcombines
theWeightedNaïveBayesModulewiththeso-calledDistanceReinforcement
Module. Their experimental results showed that the introduced DBNB al-
gorithm outperforms all competitors in maximum diagnosis accuracy and
also minimum error. It is worth interesting to remark that both of these
COVID-related papers started with a feature selection phase.
The paper [4] is concerned with using Naive Bayes in text mining, for au-
tomated literature research to select appropriate gene ontology in the MED-
LINE database.
The paper [5] is related to predicting lung adenocarcinoma (LUAD) from
a selected gene expression. Six key genes were identified based on the tu-
mormicro-environment, whichisusedaspotentialprognosticbiomarkersand
therapeutictargetsofLUAD.Usingthesesixgenes, aNaiveBayesmodelwas
constructed which predicts LUAD accurately. The predictions realized based
on the Naive Bayes model were verified by the receiver operating character-
istic (ROC) curve where the area under the curve (AUC) reached 92.03%,
which shows that the model used could accurately discriminate the tumor
samples from the normal ones.
NB algorithm is based on the assumption that the explanatory variables
are conditionally independent given the classifier. This is not generally true
for real-life problems. Many papers are concerned with the improvement
of this remarkable algorithm. These improvements follow mainly two direc-
tions. Some researches show that selecting some attributes beforehand might
result in better classification accuracy and better generalization. Another
improvement might be achieved by relaxing the conditional independence
assumption.
The aim of this paper is to give a close-optimal relaxation of the condi-
tional independence assumption at a given complexity level.
The remainder of this paper is organized as follows. In Section 2, the
related work is presented. Section 3 introduces the concepts that are used
in our approach. In Section 4, the Generalized Bayes structure is introduced
and discussed, how it is related to other structures which were introduced
earlier. In this section, we also introduce a greedy algorithm for construct-
2ing a Generalised Naive Bayes classifier. We prove here that the introduced
model gives an approximation at least as good as the original NB structure.
We give a second algorithm for constructing a generalized Naive Bayes struc-
ture and prove that this gives the optimal structure in terms of fitting to the
real data. In both cases, the complexity of the algorithms is calculated and
compared with the related algorithms. In Section 5, the classification process
and the new feature selections are presented. In section 6 we apply the al-
gorithm to real datasets and compare the algorithm with related algorithms
like NB and TAN by using various measures like accuracy, precision, recall,
F1 score, AUC. We close the paper with conclusions.
2. Related work
Naive Bayes delivers fast and effective classification with a clear the-
oretical foundation although its drawback is the conditional independence
assumption. Many empirical comparisons between naive Bayes and decision
trees such as C4.5 [6] show that NB predicts equally as well as C4.5. Papers
like Domingos and Pazzani [7] and Zhang [8] try to explain why are Naive
Bayes so effective. Domingos and Pazzani explain the good performance of
NB by the 0-1 loss function, which does not penalize inaccurate probability
estimations. Harry Zhang shows in [8] that the distribution of dependen-
cies among all attributes over classes affects the classification of NB, not the
dependencies themselves. The dependencies between attributes may exist
but they cancel out. This idea is investigated under Gauss distribution. In
[8] a sufficient condition is given under which Naive Bayes is optimal even
though the conditional independence is violated. One of the methods used
to improve the performance of NB is based on attribute selection. This can
be achieved by selecting subsets of explanatory variables or by weighting the
features by their importance based on decision trees [9]. It is shown that
using feature weights in NB improves the quality of the model compared
with the original NB. In [10] the feature weighting method was inspired by
Kullback Leibler divergence.
A recent paper [11] introduces a novel selective Naive Bayes algorithm,
which proposes an efficient method for selecting the attributes for calculating
the class- probability. The models were built in such a way that each one
is an extension of another one. The most predictive one is selected by the
measures of leave-one-out cross-validation. The algorithm uses a pre-ranking
of the attributes based on their mutual information with the classification
3variable. Empirical results show that the introduced method is superior to
NB and it is comparable with Tan [12] as accuracy and running time.
Zhang and Sheng [13] proposed a gain ratio-based feature selection al-
gorithm. The features with a higher gain ratio are preferred. Another way
to improve the accuracy of NB is by weakening the conditional assumption.
The term “semi-Naive Bayes” was introduced by Kononenko [14]. In this
paper, conditional probabilities of joint features were computed based on the
training data. The method was illustrated on medical datasets with slight
improvements over the classical NB.
An important step that extends the structure of naive Bayes is the in-
troduction of the so-called Tree Augmented Naive Bayes (Tan) [12]. The
algorithm assumes that the relationship among the explanatory variables is
only tree-structure-like, in which the class variable directly points to all fea-
ture nodes and each feature has only one parent node from another feature.
Tan algorithm showed significant improvement in accuracy compared to NB.
An improvement for the TAN algorithm is presented in [15]. This result is
given by averaging multiple TAN structures, which turn out to be even more
accurate than the classification using only one TAN structure.
Very recent papers like paper [1] and [16] introduced new algorithms for
classifications. Thealgorithminpaper[1]isbasedonattributeweightingand
instance weighting. They define a new weighting methodology that uses the
correlation of the attributes with the class variables and also with the other
attributes. The weight of an attribute is defined as the difference between
relevance and average redundancy. In paper [16] the author argues that
one-dependence estimator (AODE) performs extremely well against more
sophisticatedmodels,thereforetheyimprovethemodelbyaspecialweighting
based on mutual information.
A very important paper that uses decomposable models to expand the
classical NB was published in Ghofrani et al. in 2018 [17]. This work is also
interesting because, in their comparisons with other models, they use cherry
tree probability distribution defined on the explanatory attribute set. They
concluded that cherry trees were not so efficient because of the overfitting.
The basic idea of their paper related to cherry trees is to replace the tree be-
tween the explanatory variables given by the TAN algorithm, with a cherry
tree. They introduce also their model, which aims to capture interdependen-
cies between the explanatory attributes. Their experimental performance
shows that their algorithm had a better classification performance than, NB
and Cherry tree-based ones. In Section 4 we discuss the similarities and the
4main differences between the algorithms we introduce in this paper and those
introduced earlier in [17], TAN in [12], ATAN [15] and LDMLCS [17].
The new algorithms, we introduce in this paper, are structure-extending
algorithmsthatalleviatetheconditionalindependenceassumptionoftheNB.
Our algorithms’ output can be used as the base for a new feature selection
method. We prove that our so-called Generalized Naive Bayes structure is
at least as good as the NB structure in terms of fitting to the real data.
We give two algorithms. The first one is a good fitting greedy algorithm, the
second one gives as output the optimal structure, under a not very restrictive
condition, which is rigorously proven.
3. Basic concepts
Naive Bayes-related studies approach the classification problem from di-
rected probabilistic networks, called Bayes Networks. In this paper, we
will improve the naive Bayes structure by using the undirected probabilistic
graphical models which are called also Markov networks, and we are con-
cerned with an important subclass of them called cherry junction trees.
Probabilistic graphical models bring together graph theory, probability
theory, and also in some cases information theory, in order to provide a
probabilistic flexible framework for modeling random variables with complex
interactions. However, structure-learning for graphical models remains an
open challenge, since many times one must cope with a combinatorial search
over the space of all possible structures. In this paper we define a special
family as a search space.
Attheheartofprobabilisticgraphicalmodelsstandstheconditionalinde-
pendence concept (CI). In 1950 Loeve [18] defined the concept of CI in terms
of σ−algebras. The properties of CI were formulated in different contexts as
follows. Phi Dawid formulated the formal properties of CI from a statistical
point of view [19]. Later Spohn [20] formulated the properties of CI in a
philosophical logic context and also related CI to the concept of causality.
The following subsection contains the concepts and preliminary results,
which will be used in this paper.
3.1. Probabilistic graphical models used in the GNB structures
First, we define the cherry tree graph structure. This concept was origi-
nally named t-cherry in [21], but since it does not lead to any confusion we
call it simply a cherry tree graph.
5Figure 1: An example construction of a third-order cherry tree.
We define first the cherry tree graph of order k by construction.
Definition 1. We call cherry tree graph structure of order k, a graph
structure obtained by the following two steps:
1. The smallest cherry tree consists of k interconnected vertices.
2. A cherry tree on m + 1 vertices is obtained from a cherry tree on m
(m > k) vertices by connecting a new vertex to k−1 already connected
vertices.
Figure 1 shows how a third-order cherry tree on 6 vertices is built.
Remark 2. A cherry tree is a triangulated (chordal) graph.
The following definition is a consequence of Remark 2.
Definition 3. We call cherry-junction tree of order k the structure as-
signed to a cherry tree of order k in the following way:
(i) To the set of vertices of each k-th order maximum clique a so-called
cluster is assigned ;
(ii) A so-called edge connects two k -element-clusters if the following two
conditions are fulfilled:
- the clusters share k −1 elements
- if a set of k − 1 elements is contained by m clusters, these clusters
will be connected tree-like by m−1 edges, see second row of Figure 2
(iii) The k−1-element set given by the intersection of two connected clusters
is called a separator.
6For an illustration of a 3-rd order cherry-junction tree, corresponding to
a 3-rd order cherry tree graph, see Figure 2.
Remark 4. The junction tree associated with a cherry tree graph structure
is not unique.
Figure 2: Cherry tree structure represented as a chordal graph (first row), represented as
junction tree (second row) and represented in a compact junction tree form (third row)
Remark 5. Any junction tree satisfies the so-called “running intersection
property”, i.e. a vertex is contained in two different clusters it is contained
in any cluster on the path between the two clusters.
For a good overview of the connection between these concepts, see [22].
Let X = (X ,...,X )T a random vector and V = {1,...,d} the set of
1 d
indices, and let us consider a junction tree over V given by the set of clusters
C and the set of separators S.
7Definition 6. The cherry tree probability distribution corresponding to
a cherry-junction tree (V,C,S) is given by the following formula:
(cid:81)
P (X )
C
P (X) = C∈C (1)
cherry (cid:81) P (X )vS−1
S
S∈S
where v is the number of clusters that are connected through the separator
S
S.
Remark 7. It is easy to see that regardless of which graphical representation
of a junction tree we use (see for example Figure 2) the cherry tree probability
distribution formula is the same:
P(X )P(X )P(X )P(X )
123 234 125 126
P (X) = .
cherry (P (X ))2P (X )
12 23
In this paper, the generalization of the NB structure is derived from
probabilistic graphical models. Therefore after we define the probability
distributionassignedtoanNBstructurewewillexpressitintermsofjunction
trees.
Definition 8. The Naive Bayes probability distribution, when the at-
tributes X are supposed to be conditionally independent given the class vari-
i
able Y, has the following expression:
d
(cid:89)
P (X,Y) = P(Y) P(X |Y)
NB i
i=1
Remark 9. It is easy to see that Definition 8 is equivalent to the following
formula:
d
(cid:81)
P(X ,Y)
i
P (X,Y) = i=1 (2)
NB P(Y)d−1
In terms related to junction trees, the clusters consist of two elements
X ,Y, i = 1,...,d and the separators consist of only one element, namely Y.
i
TheNaiveBayesclassifierassignstoanewrealizationxthemostprobable
value y(x) of the class variable defined in the following manner.
y(x) = argmax(p (y ,x))
NB i
yi
8where y , i = 1,...,s denote the classes of the class variable Y. It is clear
i
that this classifier uses conditional independencies between the explanatory
variables given the class variable. These conditional independencies are usu-
ally in real-life problems not satisfied, it seems to be conservative. This fact
motivates our research too.
3.2. Information theoretical concepts
In this subsection, we summarize the main concepts of information theory
(see for example the fundamental book [23]).
We define conditional entropy with an equivalent formulation which we
will use in this paper.
Definition 10. The conditional entropy quantifies the amount of infor-
mation needed to describe the outcome of a random variable Y given that the
value of another random variable X is given by
H(Y|X) = H(X,Y)−H(X).
Definition11. Theinformation contentofarandomvectorX = (X ,...,X )T
i1 i
k
is given by
(cid:88) P (x ,...,x )
I(X ,...,X ) = P (x ,...,x )log
i1 i
k .
i1 i
k
i1 i
k P(x )·...·P(x )
x
i1 i
k
The information content can be seen as a generalization of the mutual
information. There are also other generalizations for the mutual information.
If P(X) is approximated by a probability distribution P (X), a quantifica-
app
tion of the goodness of the approximation is given by Kullback and Leibler
in 1951 [24].
Definition 12. The following expression is called Kullback-Leibler diver-
gence between a given joint probability distribution P(X) and its approxima-
tion P (X):
app
(cid:88) P (x)
KL(P (X),P(X)) = P (x)log .
app
P (x)
app
x
Remark 13. Kullback-Leibler divergence (KL divergence) is a positive num-
ber unless P ≡ P, when KL = 0. The smaller the value of the KL-
app
divergence is the better the approximation is.
9A fundamental theorem we use in this paper expresses the KL divergence
between a probability distribution and a cherry-tree probability distribution
approximation (1).
Theorem 14. [25] The Kullback-Leibler divergence between the cherry tree
approximation (1) and the real distribution P(X) is:
KL(P (X),P(X)) =
Cherry
(cid:34) (cid:35)
n
(cid:88) (cid:88) (cid:88)
−H(X)− I(X )− (ν −1)(I(X )) + H(X ) (3)
C S S i
XC∈C XS∈S i=1
These were the basic information theoretical concepts that we will use in
the next part.
4. Generalized Naive Bayes
This part contains the new results of the paper. First, let us introduce
the Generalized Naive Bayes structure, which is an NB structure augmented
with certain edges.
As we saw earlier the naive Bayes structure can be assigned to a junction
tree with two elements in each cluster see 2.
The new concept introduced in this paper is basically a generalization
of the above idea, i.e., we will use special junction trees, with exactly three
elements in a cluster (two besides Y) and two elements in the separator. This
way we will use a cherry tree structure for the generalization.
4.1. Generalized Naive Bayes graph structure
We introduce the Generalized Naive Bayes graph structure and then de-
fine the probabilistic graphical model associated with it.
Definition 15. A Generalized Bayes graph-structure on V ∪{0} =
= {0,1,...,d} (vertex 0 is assigned to Y), is constructed as follows.
• Step 1. The smallest GNB structure on three vertices is given by the
interconnected triplet (0,i ,i ), where i ,i ,∈ V.
1 2 1 2
10• Step k. Let us suppose that the vertices i ,i ,...,i ∈ V are already in
1 2 k
the GNB structure. We add a new vertex i ∈ V to the existing GNB
k+1
structure by connected it to vertex 0 and an already connected vertex
i ∈ {i ,i ,...,i }. The vertex i is called the mother of vertex i ,
m 1 2 k m k+1
and it is denoted by µ(i ). Vertex 0 is called the father vertex of the
k+1
new (i ) vertex.
k+1
Remark 16. It is easy to see that a GNB is a special type of cherry-junction
tree structure (See Definition 3)
Remark 17. Each vertex i ,k > 1, has one mother vertex µ(i ) and the
k k
same father vertex 0. The first connected vertex i has only the father vertex
1
0.
An important remark, that connects this work to earlier research is the
following.
Theorem 18. A graph structure defined on the vertex set {0,1,...,d} has
a GNB structure (Definition 15) if and only if it has the property that its
restriction to the vertices {1,...,d} form a tree.
Proof. First we prove that a GNB on {0,1,...,d} implies the tree structure
on {1,...,d}.
We do the proof by reductio ad absurdum. Let us suppose that the
vertices in {1,...,d} in a GNB graph structure are not connected tree-
wise. This means that there exists a path {i ,i ,...,i = i }, r > 3, where
1 2 r 1
i ,i ,...,i ∈ V.
1 2 r
We prove this by considering the following two cases:
• if r = 4 then 0,i ,i ,i ,i , are interconnected, which implies that the
1 2 3 4
maximum clique is of size four (see Figure 3), which is in contradiction,
with Definition 15.
• if r > 4 then we have at least three connected triplets (which share
two common vertices): (0,i ,i ), (0,µ(i ),i ), (0,µ(i ),i ), where µ(i )
1 2 3 3 4 4 3
is the mother vertex of i , and µ(i ) is the mother vertex of i , and
3 4 4
µ(i ) ∈ {i ,i }, µ(i ) ∈ {i ,i ,i } Since i = i it belongs to the first
3 1 2 4 1 2 3 1 r
and last triplet, and obviously it does not belong to the intermediate
triplets. Therefore the running intersection property is not fulfilled.
11Figure 3: The maximum clique is of size four, which leads to a contradiction
Figure 4: Adding the vertex 0 to each cluster
Now we will prove the other implication. Let us consider a tree defined on
the vertices {1,...,d}. This tree can be expressed in terms of junction tree,
with two vertices in each cluster, and one vertex in each separator, see the
first row in Figure 4. Then we add to each cluster the vertex 0, because this
is connected by definition with all vertices {1,...,d}. It is easy to see that
the new junction tree will have three vertices in a cluster and two in each
separator (the old vertex and the vertex 0, which is now in all the clusters
(see Figure 4, the second row).
Definition 19. The permutation i ,i ,...,i of the indices {1,...,d}, given
1 2 d
by the steps of Definition 15 is called c-ordering, where the first two indices
in the smallest cherry tree are arranged increasingly.
Without loss of generality, we denote a c-ordering i ,i ,...,i by 1,...,d.
1 2 d
Definition 20. The GNB probability distribution associated to the GNB
12graph structure and a c-ordering 1,...,d is the following:
d
(cid:81) (cid:0) (cid:1)
P Y,X ,X
µ(i) i
P = i=2
GNB
d−1 (cid:16) (cid:17)
(cid:81)
P Y,X
µ(i)
i=3
Now we want to prove that a GNB probability distribution given by Defi-
nition 20 gives a better approximation than the NB probability distribution,
in terms of Kullback- Leibler divergence. For this, we need the Expressions
of the Kullback- Leibler divergence of the NB, and GNB approximations.
Theorem 21. If the joint probability distribution P(x,y) is approximated by
a Naive Bayes distribution (2) the Kullback-Leibler divergence between the
true distribution and the approximation is given by the formula:
d d
(cid:88) (cid:88)
KL(P(x,y),P (x,y)) = H(X )+H(Y)−H(X,Y)− I(X ,Y) (4)
NB i i
i=1 i=1
Proof.
(cid:88) P(x,y)
KL(P(x,y),P (x,y)) = P(x,y)log
NB
P (x,y)
NB
(x,y)
(cid:88) (cid:88)
= P(x,y)logP(x,y)− P(x,y)logP (x,y). (5)
NB
(x,y) (x,y)
The first term in (5) is −H(X,Y). We calculate now the second term by
exploiting the conditional independence relation i.e.:
13d
(cid:81)
P(x ,y)
i
(cid:88) (cid:88)
P(x,y)logP (x,y) = P(x,y)log i=1
NB P(y)d−1
(x,y) (x,y)
d
(cid:88) (cid:89) (cid:88)
= P(x,y)log P(x ,y)− P(x,y)logP(y)d−1
i
(x,y) i=1 (x,y)
d
(cid:88) (cid:89) P(x ,y)
i
= P(x,y)log
P(x )P(y)
i
(x,y) i=1
(cid:88)
+ P(x )logP(x )+P (y)logP(y)
i i
x
d d
(cid:88) (cid:88)
= I(X ,Y)− H(X )−H(Y)
i i
i=1 i=1
Now we substitute this result in the formula (5), and we get the formula
(4).
It is easy to see that this is in fact a consequence of the Chow-Liu tree
method [26].
d
(cid:80)
Remark 22. The goodness of fit depends on the I(X ,Y). As larger this
i
i=1
sum is the better the approximation is.
This result highlights why the Naive Bayes works so well, and why it is
worth choosing the explanatory variable based on their mutual information
with the classifier.
Theorem 23. If P(X) is approximated by a Generalized Naive Bayes distri-
bution P (X) (see Definition 20) then the KL divergence is given by the
GNB
formula:
d
(cid:88)
KL(P(X,Y),P (X,Y)) = H(X )+H(Y)−H(X,Y)
NB i
i=1
(cid:32) (cid:33)
d d
(cid:88) (cid:0) (cid:1) (cid:88) (cid:0) (cid:1)
− I Y,X ,X − I Y,X
µ(i) i µ(i)
i=2 i=3
14Proof. This follows straightforwardly from the formula of KL divergence ap-
plied for an approximation given by a cherry tree probability distribution,
given by formula (3).
We saw in Theorem 21 and Theorem 23 that the corresponding KL di-
vergences depend on two parts, one of them being common to both approx-
imations. Therefore the goodness will depend on the part that differs from
one model to another, from NB to GNB.
Definition 24. Let P (X) be an approximation for P(X). We call the
app
weight of the approximation
d
(cid:88)
W(P(X),P (X)) = H(X )−H(X)−KL(P(X),P (X)).
app i app
i=1
Remark 25. The weight of the P P(X,Y) approximation is:
NB
d
(cid:88)
W(P(X,Y),P P(X,Y)) = I(Y,X ).
NB i
i=1
Remark 26. The weight of the P P(X,Y) approximation is:
GNB
d d
W(P(X,Y),P P(X,Y)) = (cid:88) I(cid:0) Y,X ,X (cid:1) −(cid:88) I(cid:0) Y,X (cid:1) (6)
GNB µ(i) i µ(i)
i=2 i=2
Now we will show that the weight of a GNB approximation is at least as
large as the weight of an NB approximation. To prove this we need first the
following two theorems.
Theorem 27. The weight of the NB approximation can be written as:
d d−1
(cid:88) (cid:88)
W(P,P ) = H(X )− H(X |Y)−H(Y,X ) (7)
NB i i d
i=1 i=1
15Proof.
d
(cid:88)
W(P,P ) = I(Y,X )
NB i
i=1
= H(Y)+H(X )−H(Y,X )
1 1
+H(Y)+H(X )−H(Y,X )
2 2
+H(Y)+H(X )−H(Y,X )
3 3
.
.
.
+H(Y)+H(X )−H(Y,X )
d−1 d−1
+H(Y)+H(X )−H(Y,X )
d d
FirstweaddtheentropiesH(Y),H(X ),H(X ),...,H(X ),thenweapply
1 2 d
the formula H(Y,X )−H(Y) = H(X |Y) starting on from the first row, for
i i
i = 1,...d−1. This way we get the formula (7).
ToproveasimilartheoremregardingtheweightofaGNBapproximation,
we need a result related to the GNB graph structure.
First, we define the concept of a chain of clusters.
Definition 28. A chain of clusters is a special cherry tree in which each
of the clusters is connected to at most two different clusters.
Remark 29. A chain cluster has exactly two simplicial vertices (vertices
connected only to the other two (in the general case, to k −1 ) vertices)
Lemma 30. A GNB graph structure can be represented as a chain of clusters.
Proof. Let us suppose that we have a GNB structure with the property that
there exists a cluster that is connected to three clusters by different sep-
arators. We denote this cluster by C (0,i,j) and its three neighbours by
1
C ,C ,C . This implies that it is possible to connect C with two clusters by
2 3 4 1
two different separators S (0,i) and S (0,j), but we have no other dif-
12 13
ferent subset in C which contains the vertex 0 (which corresponds to class
1
variable Y). Therefore any cluster can be connected to the other clusters by
at most two different separators, which completes the proof.
Since the GNB probability distribution associated to the GNB graph can
be defined as a junction tree associated to a chain of clusters, we will assign
a numbering to the random variables involved.
16Figure 5: The initial construction of the cherry tree
Definition 31. The chain numbering of the attributes based on the chain
structure of GNB is defined as follows. C ,C ,...,C are the clusters
1 2 d−1
of the chain, C = (Y,X ,X ), C \C is denoted by X , ..., C \C is
1 i1 i2 2 1 i3 t t−1
denoted by X , ...,C \C is denoted by X . (Where ”A\B” denotes
it+1 d−1 d−2 i
d
the elements belonging to the set A and not to the set B.)
Remark 32. The chain numbering {X ,...,X } is given by a permutation
i1 i
d
of the indices of the attributes and it is not unique.
For simplicity without loss of generality, we will denote this numbering
{i ,...,i } by {1,...,d} but whenever we use this we mention that this is a
1 d
chain numbering.
Theorem 33. Let {1,...,d} be a chain numbering. The weight of the basic
GNB approximation is given by the formula:
d d−1
(cid:88) (cid:88)
W(d) = H(X )+H(Y)− H(X |Y,X )−H(Y,X ). (8)
GNBbasic i i i+1 d
i=1 i=1
Proof. We do the proof by induction.
Let us consider a chain numbering of the attributes {X ,X ,...,X }.
1 2 d
First we consider the class variable Y and the attributes X ,X ,X , as
1 2 3
illustrated on Figure 5, and we show that the formula is true for d = 3.
As (Y,X ,X ,X ) is a cherry junction tree and the formula of KL diver-
1 2 3
gence is given as in (6) the following formula holds
W(3) = I(Y,X ,X )−I(Y,X )+I(Y,X ,X ) =
GNB 1 2 2 2 3
= H(Y)+H(X )+H(X )−H(Y,X ,X )+
1 2 1 2
+H(Y)+H(X )+H(X )−H(Y,X ,X )−
2 3 2 3
−(H(Y)+H(X )−H(Y,X ))
2 2
17After reduction W is:
GNB
3
(cid:88)
W(3) = H(X )+H(Y)−H(Y,X ,X )−H(Y,X ,X )+H(Y,X )
GNB i 1 2 2 3 2
i=1
We add and substract H(Y,X ) and get the following:
3
3
(cid:88)
W(3) = H(X )+H(Y)+H(Y,X )−H(Y,X ,X )
GNB i 2 1 2
i=1
+H(Y,X )−H(Y,X ,X )−H(Y,X )
3 2 3 3
3
(cid:88)
= H(X )+H(Y)−H(X |Y,X )−H(X |Y,X )−H(Y,X ),
i 1 2 2 3 3
i=1
which is exactly the formula (8) for d = 3.
Figure 6: The inductive construction of the cherry tree in step d
Now, we suppose that the formula (8) is true for d attributes and we
will prove that it is true for d+1 attributes, see Figure 6. Without loss of
generality, we will suppose that X is connected to X and Y.
d+1 d
Since this has also a cherry-junction tree structure with a chain structure,
it’s weight can be written by adding a new cluster (Y,X ,X ) as follows:
d d+1
W(d+1) = W(d) +I(Y,X ,X )−I(Y,X ) =
GNB GNB d d+1 d
W(d) +H(Y)+H(X )+H(X )−H(Y,X ,X )
GNB d d+1 d d+1
−[H(Y)+H(X )−H(Y,X )]
d d
18After reduction we obtain:
W(d+1) = W(d) +H(X )−H(Y,X ,X )+H(Y,X )
GNB GNB d+1 d d+1 d
Now we use the induction hypothesis and get:
d d−1
(cid:88) (cid:88)
W(d+1) = H(X )+H(Y)− H(X |Y,X )−H(Y,X )
GNB i i j+1 d
i=1 i=1
+H(X )−H(Y,X ,X )+H(Y,X )
d+1 d d+1 d
After reduction and adding and subtracting H(Y,X ) we use the formula
d+1
H(Y,X ,X )−H(Y,X ) = H(X |Y,X )
d d+1 d+1 d d+1
and obtain
d+1 d−1
(cid:88) (cid:88)
W(d+1) = H(X )+H(Y)− H(X |Y,X )
GNB i i i+1
i=1 i=1
−H(X |Y,X )−H(Y,X ) =
d d+1 d+1
d+1 d
(cid:88) (cid:88)
= H(X )+H(Y)− H(X |Y,X )−H(Y,X )
i i i+1 d+1
i=1 i=1
which completes the proof.
Theorem 34. The GNB approximation gives an approximation at least as
good as NB approximation.
Proof. First, let us observe that formula (6) is true for any permutation of
the indices, therefore this is particularly true for the case of chain numbering.
We compare the weight of the two approximations.
d d−1
(cid:88) (cid:88)
W(d) = H(X )+H(Y)− H(X |Y)−H(Y,X ) ≤
NB i i d
i=1 i=1
d d−1
(cid:88) (cid:88)
≤ H(X )+H(Y)− H(X |Y,X )−H(Y,X ) = W(d)
i i i+1 d GNB
i=1 i=1
19This inequality takes place because of
H(X |Y) ⩾ H(X |Y,X )
i i i+1
for all i = 1,...,d.
The approximation with a higher weight is a better approximation, there-
fore GNB gives an approximation at least as good as Naive Bayes.
4.2. Greedy algorithm for finding a good fitting GNB structure
The basic idea of this algorithm is to find an approximation with a maxi-
mum weight (6). After calculating the information of all triplets that contain
the class variable Y we choose the first two attributes such that the infor-
mation content of (Y,X ,X ) is maximum. Then we add greedily a new
i1 i2
variable X at a time, such that the maximum of
new
I(X ,X ,Y)−I(Y,X ),
new j j
is achieved, where the variables X are already connected. The pseudo-
j
algorithm is given in Algorithm 1.
Complexity of the GNB-A algorithm
Now we will calculate the runtime of Algorithm 1. We will use O(f(n,d))
to denote the total number of additions, subtractions, multiplications, divi-
sions and comparisons for an input dataset of n entries (rows) and d variables
X ,...,X (not including Y) is less than cf(n,d) for some c ∈ R+.
1 d
Firstly we will show that Python 3 calculates log in O(1) time. The
2
Python math library uses the C programming language’s built-in math pack-
age, an implementation for which can be seen in [27]. It uses an 8-element
Taylor series expansion for log(1+x), with a total of 29 additions, subtrac-
tions or multiplications to calculate log (x) for any input x, meaning that it
2
takes 29 = O(1) steps to calculate.
To calculate P(X = x ) based on a dataset with n entries, we need to
1 1
count how many times X takes on x , which we will denote by #X = x ,
1 1 1 1
then divide by n:
#(X = x )
1 1
P(X = x ) =
1 1
n
20Algorithm 1 GNB-A
procedure GNB-A(D)
Input: Training samples D with d features X ,X ,...,X and class
1 2 d
variable Y
Output: A sorted list of triplets
X := {X ,X ,...,X }
1 2 d
X ,X := argmax I(Y,X ,X )
i1 i2 i j
Xi,Xj∈X
L := [(Y,X ,X )]
3 i1 i2
L := [(Y,X ),(Y,X )]
2 i1 i2
X := X\{X ,X }
i1 i2
while X is not empty do
e∗,X := argmax (I(e,X )−I(e))
new j
e∈L2,Xj∈X
Append (e∗,X ) to L
new 3
Append (Y,X ) to L
new 2
X := X\{X }
new
end while
return L
3
end procedure
which is O(n) comparisons and 1 division, so a total of O(n) operations.
To calculate P(X = x ,X = x ,X = x ) based on a dataset with n
1 1 2 2 3 3
entries and at least 3 columns, we need to evaluate a similar fraction:
#(X = x ,X = x ,X = x )
1 1 2 2 3 3
P(X = x ,X = x ,X = x ) =
1 1 2 2 3 3
n
where the nominator denotes the cases where all three equalities hold at
the same time. This takes 3n comparisons and 1 division, which is again an
O(n) amount.
To calculate the argument of the information content, we need to put all
of these together:
 
O(n)
(cid:122) (cid:125)(cid:124) (cid:123)
 P(x ,x ,x ) 
P(x ,x ,x )log  1 2 3 
1 2 3 2 P(x )P(x )P(x )
(cid:124) (cid:123)(cid:122) (cid:125)(cid:124)(cid:123)(cid:122)(cid:125) 1 2 3 
O(n) O(1) (cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125)
O(n) O(n) O(n)
21Which takes O(n)+O(1)+4O(n) = O(n) steps to evaluate.
Using this, we can show the runtime of calculating an information content
I or I is O(n2). Let us start with I :
3 2 3
(cid:18) (cid:19)
(cid:88) P(x ,x ,x )
1 2 3
I (X ,X ,X ) = P(x ,x ,x )log
3 1 2 3 1 2 3 2 P(x )P(x )P(x )
1 2 3
(x1,x2,x3)
where the index of the summation goes through all the realizations of
(X ,X ,X ). These triplets can be obtained by going through the dataset
1 2 3
of n rows, and storing the unique (x ,x ,x ) values in a dictionary, with
1 2 3
each value of the dictionary being the number of occurrences. Such a dic-
tionary can be constructed in O(n2) steps, since for each row of the dataset
(x ,x ,x ), we will (potentially) need to go through the entire dictionary we
1 2 3
have constructed so far, to see if the triplet (x ,x ,x ) already exists or not.
1 2 3
If it does, then increase its value (occurrence) by one, otherwise, add it to
the dictionary with value 1.
Next, we need to sum up an O(n) amount on an index set of size O(n),
which can also be done in O(n2) steps.
Similarly, I can also be calculated in O(n2) steps. The difference is that
2
the keys of the dictionary will be of size 2 instead of 3.
Now we can move onto the steps of Algorithm 1.
In Step 1, the Algorithm calculates all triplet-information contents where
one of the elements is Y, namely
I (Y,X ,X ) ∀i,j ∈ {1,...,d}
3 i j
There are (cid:0)d(cid:1) choices for (X ,X ), so there are (cid:0)d(cid:1) of such triplets. We
2 i j 2
have already shown that calculating an I information content can be done
3
in O(n2) steps, so calculating (cid:0)d(cid:1) = O(d2) of them takes O(n2d2) operations.
2
In Step 2, the Algorithm calculates all pair-information contents where
one of the elements is Y, namely
I (Y,X ) ∀i ∈ {1,...,d}
2 i
There are d options for X , so similarly to the previous case, this can be
i
done in O(n2d) steps.
In Step 3, the Algorithm calculates all information content differences
I −I where the arguments of I are part of the arguments of I . These are
3 2 2 3
of the form
22I (Y,X ,X )−I (Y,X ) ∀i,j ∈ {1,...,d}
3 i j 2 i
and
I (Y,X ,X )−I (Y,X ) ∀i,j ∈ {1,...,d}
3 i j 2 j
So there are 2(cid:0)d(cid:1) of them, which is an O(d2) amount. Therefore in Step
2
3, the Algorithm calculates O(d2) differences.
Finally in Step 4, for all k ∈ {1,...,d − 1}, the Algorithm chooses the
argmax of the I −I values.
3 2
In a general step k, the algorithm has already selected k argmax’s, so
there are d − k that remain. In order to calculate argmax(I − I ), we
3 2
choose the highest information content nodes (X ,X ) to add, where X ∈
a b a
{X ,...,X } and X ∈ {X ,...,X }. There are k(d−k) such choices.
1 k b k+1 d
The maximum and the argmax of k(d−k) elements can be obtained by
going through each element once, and saving the maximum so far, and the
argmax so far, which takes k(d−k) comparisons.
So in steps k = 1,...,d−1, the total number of comparisons is
d−1 d−1 d−1
(cid:88) (cid:88) (cid:88) (d−1)d (d−1)d(2d−1)
k(d−k) = kd− k2 = d − =
2 6
k=1 k=1 k=1
(cid:18) (cid:19) (cid:18) (cid:19)
d 2d−1 3d−2d+1
= d(d−1) − = d(d−1) =
2 6 6
d(d−1)(d+1)
= = O(d3)
6
So overall in Steps 1 to 4, the Algorithm requires the following number
of additions, subtractions, multiplications, divisions, or comparisons:
O(n2d2)+O(n2d)+O(d2)+O(d3) ≤ O(n2d2 +d3) = O(d2(n2 +d))
In most cases, d (the number of variables or columns) is significantly less
than n2 (the number of entries or rows, squared), so
O(d2(n2 +d)) < O(d2(n2 +n2)) = O(d2n2)
√
So if we have at least d entries in our dataset, then the runtime of the
algorithm is O(d2n2), otherwise it is O(d2(n2 +d)).
234.3. Algorithm for finding the best fitting GNB structure
Algorithm 2 GNB-O
procedure GNB-O(D)
Input: Training samples D with d features X ,X ,...,X and class
1 2 d
variable Y
Output: A list of triplets and a list of pairs.
X := {X ,X ,...,X }
1 2 d
X ,X := argmax I(Y,X ,X )
i1 i2 i j
Xi,Xj∈X
We create a (d+1)×(d+1) score matrix S, where the (i,j)-th cell is
the score for the edge j →− i:
• Row 0 and column 0 belongs to Y, row i and column i belongs to X .
i
• S[i ,0] := I(Y,X )
1 i1
• S[i ,i ] := I(X ,X )
2 1 i1 i2
• S[j ,j ] := I(Y,X ,X )−I(Y,X ),
1 2 j1 j2 j2
∀j ∈ {1,2,...,d}\{i ,i }, ∀j ∈ {1,2,...,d}, j ̸= j
1 1 2 2 1 2
• The remaining elements are zero.
We apply the Chu-Liu-Edmonds algorithm to find the maximum
weighted arborescence in the directed graph belonging to S.
From the maximum weighted arborescence we create the list of triplets
L and the list of pairs L .
3 2
return L ,L
3 2
end procedure
This algorithm aims to find a GNB approximation with the maximum
weight (6). After calculating the information of all triplets that contain the
class variable Y. We choose the first two attributes where the information
content of the triplet (Y,X ,X ) is maximum.
i j
(Y,X ,X ) = argmax(Y,X ,X ). (9)
i1 i2 i j
i,j∈V
NowwedefineanewdirectedandweightedgraphonV∪{0} = {0,1,...,d}
as follows.
24Definition 35. We call auxiliary directed graph structure on the set of
vertices {0,1,...,d}, a directed graph with the following properties:
• vertex 0 has no parent, it is called the root of the graph;
• the parent of vertex i is vertex 0;
1
• the parent of vertex i is vertex i
2 1
• from vertex i and vertex i to all other vertices we have only one-
1 2
directional edge;
• for all vertices i,j different from i and i we have an edge (i,j) and
1 2
an edge (j,i),
• there are no self-pointed edges;
Shortly written the set of edges of the auxiliary graph structure are the
following:
E = {(0,i )} ∪ {(i ,j)|j ∈ V \{i }} ∪ {(i ,j)|j ∈ V \{i ,i }}
1 1 1 2 1 2
∪ {(i,j)|i,j ∈ V \{i ,i },i ̸= j}. (10)
1 2
Now we define the weights of the edges of the auxiliary graph.
Definition 36. The weights of the edges of the auxiliary graph struc-
ture are defined as follows:
• w(0,i ) = I(Y,X )
1 i1
• w(i ,i ) = I(X ,X )
1 2 i1 i2
• For any other edge (i,j) ∈ E\{(0,i ),(i ,i )} the weight of the edge is
1 1 2
defined as follows:
w(i,j) = I(Y,X ,X )−I(Y,X ). (11)
i j i
Definition 37. The weighted graph G(V,E), where the set of edges E is
defined by (10) and the weights of the edges are defined in (11) is called
auxiliary graph of the GNB graph.
25Now we recall the definition of an arborescence in a directed graph.
Definition 38. An arborescence is a directed graph with a root vertex 0
and with all other vertices v having the property that there is exactly one
directed path from 0 to v.
There are some important equivalent definitions. An arborescence can
be defined as a rooted digraph in which the path from the root to any other
vertex is unique.
For finding the maximum weighted arborescence in a directed graph, Chu
Liu Edmond’s algorithm [28] was proposed.
Remark 39. The maximum arborescence of the auxiliary graph contains the
edges (0,i ) and (i ,i ).
1 1 2
Proof. This is true because of Definition 38 and since vertex i ’s single parent
1
is vertex 0, and vertex i ’s single parent is vertex i .
2 1
Theorem 40. Each vertex of an arborescence has only one parent.
Proof. Ifavertexv wouldhavetwoparentsp (v)andp (v)thenbydefinition
1 2
for each of the parents we have a single directed path from the root, then
it is obvious that we would have two directed paths starting from the root
which lead to v, which contradicts Definition 38.
We apply Chu Liu Edmonds algorithm [28] to the auxiliary graph.
The algorithm’s output is a maximum weighted arborescence with the
property that for each vertex i ∈ V a unique directed path from 0 to i is
defined.
Definition41. WedefinetheOptimized Generalized Naive Bayesstruc-
ture based on the maximal weighted arborescence as follows:
• The triplet (Y,X ,X ) defined by formula (9) is the first cluster of the
i1 i2
cherry tree.
• Foranyk ∈ V\{i ,i }thereexistsauniquedirectedpath0,i ,i ...i =
1 2 1 k1 ks
k from 0 to k, such that i ,...i ∈ V. Based on this we will define
k1 ks
the other clusters of the cherry tree as follows (Y,X ,X ), for all
p(ki) ki
k ∈/ {0,i ,i }, k ∈ {i ,...i = k}
i 1 2 i k1 ks
26Now we can state an important result of our paper.
Theorem 42. The Optimized Generalized Naive Bayes given in Definition
41 is the maximum weighted Generalized Naive Bayes distribution if this
structure contains the maximum weighted cherry (Y,X ,X ).
i1 i2
Proof. From Remark 39 we have that the vertices 0,i ,i will be in the tree
1 2
this way the weight of the approximation will contain I(Y,X ,X ).
i1 i2
Any cluster (Y,X ,X ) is connected to the already connected vertex
p(ks) ks
by the separator (Y,X ), and adds to the weight of approximation the
p(ks)
term I(Y,X ,X )−I(Y,X ), the weight of the corresponding edge in
p(ks) ks p(ks)
the auxiliary graph.
IfwewouldhaveaGNBwithalargerweight,thenthemaximumweighted
arborescence would be not optimal, which is in contradiction with Chu Liu
Edmonds algorithm, which guarantees to find the maximum weighted ar-
borescence.
A natural question is how to get the Weight of the Optimized GNB ap-
proximation from the weight of the maximum weighted arborescence (output
of Chu-Liu-Edmonds algorithm).
The weight of the arborescence differs from the weight of the optimal
approximation by the weight of the first two edges, which are present in
the weight of the arborescence, and are not present in the weight of GNB,
therefore we have to subtract them. Although the weight of the GNB ap-
proximation contains the information content of the first triplet, which is not
contained by the weight of arborescence. This is why we have to add it to it.
Complexity of the GNB-O algorithm
The GNB-O algorithm first fills out the S matrix by calculating all pair
and triplet information contents, both of which we have seen can be done in
O(n2d2) time. Then we run the Chu Liu Edmonds algorithm on the directed
graph belonging to S. Its runtime is O(|E|·|V|), where |E| are the number
of edges and |V| are the number of vertices. In our case, |V| = d+1 = O(d)
and |E| = (d − 2)2 + 2 = O(d2), so the runtime of the Chu-Liu-Edmons
algorithm is
O(|E|·|V|) = O(d2 ·d) = O(d3)
Once again, similarly to GNB-A, the total runtime of GNB-O is
O(n2d2 +d3) = O(d2(n2 +d)) ( =∗) O(n2d2)
27where (∗) holds true if d < n2, so the number of attributes (columns) is less
than the number of data (rows) squared; which is true in almost all cases.
4.4. On how the introduced GNB concepts are related to former improve-
ments of NB
The most closely related structures and algorithms are the TAN, ATAN,
and Ghofrani algorithms, and they are given in [12] and [17], while their com-
plexities are shown in [17] (Page 6). This part is dedicated to discussing the
similarities and differences between structures, algorithms, and complexities.
From a structural point of view, we proved in Theorem 18, that the graph
structure defined on the explanatory variables is a tree, which is also the case
in the graph structures used by TAN and ATAN algorithms.
The time complexity of the TAN algorithm is
O(nd2)+O(k(dv)2)+O(d2log(d))
While the time complexity of the ATAN algorithm it is
O(nd2)+O(k(dv)2)+O(d3log(d)),
where n is the number of training data (rows), d is the number of features
(columns), v istheaveragenumberofvaluesperfeature(whichwehavesetto
5 in our evaluation), and k is the number of classes (which in our evaluation
setting is 2, a binary 0−1 value).
The time complexity of the Ghofrani algorithm is
O(ndlc)+O(k(dv)2)+O(d2log(d))+O(d3l3)+O(nkn )
c clq
where n,d,v,k are the same as before, l is the dimensionality of the table
c
containing probability estimates for each class (in our case, 2), and n is the
clq
number of cliques (clusters) generated by the algorithm (which in our case is
d).
To be able to compare the runtime of our algorithm with TAN, ATAN,
and Ghofrani’s algorithm, we will use the substitutions v = 5, k = 2, l = 2
c
and n = d; the runtime of TAN is therefore
cql
O(nd2)+O(d2)+O(d2log(d)) ≤ O(d2(n+log(d)))
The runtime of ATAN is therefore
28O(nd2)+O(d2)+O(d3log(d)) ≤ O(d2(n+dlog(d)))
The runtime of Ghofrani’s algorithm is therefore
O(nd2)+O(d2)+O(d2log(d))+O(d3)+O(nd) ≤ O(nd2 +d3) =
(∗)
= O(d2(n+d)) ≤ O(d2 ·2n) = O(d2n)
where (∗) holds true if there is more training data (rows) than attributes
(columns), which is usually the case.
WehavealreadyseenthattheruntimeofGNB-AisO(d2n2),whichmeans
that TAN is faster as we can assume that log(d) < n, while ATAN is also
slightly faster if n+dlog(d) < n2, which holds true for most datasets (which
have more training data than features), since
d n n2
n+dlog(d) < n+d < n+n = n+ < n2
2 2 2
Furthermore,Ghofrani’sLDMLCSisalsofasterifwesetl = 2,asitsruntime
c
is O(d2n).
The novelty of the LDMLCS algorithm introduced by Ghofrani et.al.in
[17]wasthatitexploitstheconceptofconditionalindependencebetweenran-
dom variables in the context of probabilistic graphical models. The cherry
tree-based algorithms, the GNB-A and GNB-O algorithms exploit condi-
tional independence to maximize the information content of the model and
to reduce redundancy.
The main difference, from an information-theoretical point of view, be-
tween LDMLCS algorithm in [17], the ATAN in [15] and the algorithms
introduced here is that the LDMLCS and ATAN consider only the bivariate
conditional mutual information to weigh the edges between the explanatory
attributes whereas, in our algorithms, we use the fact that any structure
that has a tree-like structure between the explanatory variables has a special
cherry tree probability distribution. Based on this the expression of Kullback
Leibler divergence (23) between this probability distribution and the sample
data depends on the graph structure. By minimizing the KL divergence we
find the graph structure. We emphasize here that in Ghofrani’s paper [17],
as an alternative method they apply the cherry tree algorithm to the set of
attributes, which is different from our methods introduced here, where each
cherry contains the classifier.
295. The classification process and feature selection methods
Let us suppose that based on the training data we fitted a GNB probabil-
ity distribution, by GNB-A or GNB-O. We get as output the set of clusters
and the set of separators of the GNB. Now we present the main steps of the
classification process.
Input: The set of clusters and the set of separators which contain the clas-
sification variable Y:
• C(Y) = {C (Y) = (Y,X ,X ) | (Y,X ,X ) ∈ C} and the cor-
k i j i j
k k k k
responding marginal probability distributions,
• S(Y) = {S (Y) = (Y,X ) | (Y,X ) ∈ S} and the correspond-
k i i
k k
ing marginal probability distributions.
Output: A probability distribution over the classes y ∈ {1,...,m} and the
classification of the element into the most probable class.
Step 1: For y = 1,...,s calculate the approximation of the probability dis-
tribution of the training data by using the cherry tree approximation:
P(cid:101)(Y = y,X = x ,...,X = x ) =
1 1 d d
(cid:81)
P (Y = y,X = x ,X = x )
i i j j
k k k k
=
(Y,Xik,Xjk)∈C
. (12)
(cid:81) P (Y = y,X = x )v k−1
i i
k k
(Y,Xik)∈S
Step 2: • If there exist y ∈ {1,...,s} such that
P(cid:101)(Y = y,X = x ,...,X = x ) ̸= 0,
1 1 d d
then the vector (X = x ,...,X = x ) is classified in the class y
1 1 d d
with probability
P(cid:101)(Y = y|X = x ,...,X = x ) =
1 1 d d
P(cid:101)(Y = y,X = x ,...,X = x )
1 1 d d
= ,
s
(cid:80)
P(cid:101)(Y = y,X = x ,...,X = x )
1 1 d d
y=1
30y∗ = argmaxP(cid:101)(Y = y,X = x ,...,X = x ), y ∈ {1,...,s}.
1 1 d d
y
• Else, go to Step 3.
Step 3: For all k with P (Y = y,X = x ,X = x ) = 0 calculate
i i j j
k k k k
P (Y = y,X = x ) and P (Y = y,X = x )
i i j j
k k k k
• If for any i : P (Y = y,X = x ) = 0, substitute it with respect
k i i
k k
to
P (Y = y,X = x ) := P (Y = y)·P (X = x ), (13)
i i i i
k k k k
then go to Step 4.
• Else, go to Step 4.
Step 4: Calculate
P (Y = y,X = x ,X = x ) :=
i i j j
k k k k
P (Y = y,X = x )P (Y = y,X = x )
i k i k j k j k (14)
P (Y = y)
Go to Step 1.
5.1. Feature selection, feature importance score
The feature selection can be achieved in two stages. The first stage is
during the learning process. The relevant features can be chosen based on
how the information added by joining a variable increases the weight (6) of
the constructed cherry tree. We have to highlight here, that the importance
score of an attribute X will express the amount of information added by it,
i
k
taking into account the information added by all X ...X attributes.
i1 i
k−1
Feature selection can be achieved also in the second stage, based on the
validation set. In this case, the users/ experts can decide on relevant features
(how many triplets respectively attributes they should use for the classifica-
tion) based on the graphs for precision, recall, f1 -score, and AUC-score. We
have to underscore here, that the role of a new attribute X added by a new
i
k
triplet is mirrored by how the given accuracy measure changes by adding the
attribute X to the X ...X attributes already added to the model.
i
k
i1 i
k−1
31In the following, we discuss the feature selection, and feature scores for
both stages, for the two algorithms introduced separately.
Feature selection in stage 1 in the case of Algorithm GNB-A
In this case, in each step, the weight of the cherry tree is increased by
adding the new triplet’s information content and subtracting the bi-variate
separator’s information content (two-node edge) to which the new attribute
is connected. This way after each step, the structure remains a GNB on
the given set of explanatory variables. This method chooses the features in
every step in a greedy way. The algorithm maximizes added information
and minimizes redundancy at the same time, in each step. Based on the
graph wich shows how the added variables influence the cumulation of the
information it is straightforward to decide on which explanatory variable we
chose.
Feature selection in stage 2 in the case of Algorithm GNB-A.
In this case, to the ordered triplets added in each step (in fact by adding
a new feature), the following accuracy scores are assigned: precision-, recall-,
AUC and f1 -score. Based on these scores the expert can choose which and
how many explanatory variables are needed for a given purpose.
Feature selection in stage 1 in the case of Algorithm GNB-O.
In the case of Algorithm 2 is not straightforward to obtain the ordered
clusters of the cherry tree. First, let us recall here, that if the triplet with the
largestweight(information content)iscontainedinthe optimizedGNB, then
the GNB-O Algorithm gives the best-fitting GNB, taking into consideration
alltheattributes. Theoutputofthealgorithmisthesetofalltripletswithout
an ordering of them. To assign an importance score to the attributes first we
have to order them such that any ordered subset of successive clusters has a
cherry tree structure.
The chain numbering of the attributes is a very important concept here.
In this case the attributes X ,...,X , are reordered X ,...,X , by a permu-
1 d i1 i
d
tation. Thisorderingisessential,whenweassigntotheattributesX ,...,X
i3 i
d
an importance score.
Finding this ordering is also at the heart of the accuracy measure rep-
resentations on the validation set, with regard to how many attributes were
taken into consideration.
Now we will give a method to get the chain ordering from the output tree
32representation of the Chu-Liu-Edmonds Algorithm.
We build a score matrix which contains on (i,j) position the weight of
the (i,j) directed edge. The main idea of the numbering algorithm is to find
in each step the "leaf vertices" i.e. those vertices that are not parents, and
choose the one with the smallest weight, then delete it from the columns and
rows. The order of deleting of the vertices gives the descending ordering of
attributes.
Feature selection in stage 2 in the case of Algorithm GNB-O.
In this case, to the ordered triplets (based on the chain ordering proce-
dure), we assign respectively precision-, recall-, and f1 -scores. These will be
the feature importance scores, based on these scores the expert can choose
which and how many explanatory variables are needed for the given purpose.
In the following section, we present how our new methods work on different
datasets, and how can we use the feature scores obtained in the second stage.
6. Numerical Results
The introduced algorithms are designed for the classification task. We
will use evaluation metrics specific to this problem.
6.1. Evaluation metrics
In binary classification, beyond accuracy, precision and recall are often
used metrics to evaluate the models. Let the labels we want to predict be
‘positive’ and ‘negative’, then we can define the confusion matrix, in which
the columns show the actual values and the rows show the predicted values:
Actual
Positive Negative
Positive #{True Positive} #{False Positive}
Predicted
Negative #{False Negative} #{True Negative}
There are four possible outcomes according to the actual and the pre-
dicted label. The elements of the matrix show the occurrence of each out-
come. Based on these accuracy, precision and recall can be defined as follows:
#{True Positive}+#{True Negative}
Accuracy =
#{Total}
33#{True Positive}
Precision =
#{True Positive}+#{False Positive}
#{True Positive}
Recall =
#{True Positive}+#{False Negative}
Precision×Recall
F1 score = 2×
Precision+Recall
6.2. Data preparation
The algorithms introduced in this paper are developed for discrete data.
In general, the datasets contain discrete, categorical, and continuous at-
tributes. The categorical variables can be treated similarly to the discrete
ones. The algorithms introduced in this paper are based on the information
content, which does not depend on the "values" taken on by the variables,
only on the probabilities. This way the algorithm is more flexible in dealing
with different kinds of attributes (categorical, discrete). The continuous at-
tributes are discretized. There are many possibilities for discretization. We
chose here a quite general method, which can be applied to different types
of datasets based on quantiles. However, there are many interesting new
discretization methods, even based on Naive Bayes, such as [29] and [30].
The quantile discretization which we use here will be described shortly in
the following. If a variable X takes on more than 5 distinct values, then it
i
is discretized into at most 5 values. Denoting the individual values of X by
i
X = [x ,x ,...,x ], let us first make an ordered list Xˆ := [x ,...,x ]
i 1 2 n i ϕ(1) ϕ(n)
with x ≤ x ∀j ∈ {1,...,n−1}. Then the quantiles of this attribute
ϕ(j) ϕ(j+1)
are
Q = x
j ⌊ϕ(nj/5)⌋
Forj ∈ {1,...,4}, inourcase(4quantilesalongwiththe0%-quantileand
the 100%-quantile split the interval into 5 parts). If for any i ∈ {1,...,5},
Q = Q , then we threw away Q , and only used Q . Let us denote
i i+1 i+1 i
the number of quantiles remaining after this process by M ≤ 5. And let
Q = minX , Q = maxX .
0 i M+1 i
Then we let the average of all discretized intervals be the representative
value for each interval:
34n
1 (cid:88)
a = x 1(Q ≤ x < Q ) ∀j ∈ {1,...,M}
j i j i j+1
|{i : Q ≤ x < Q }|
j i j+1
i=0
Where 1 is the indicator function. Finally, the discretized elements of
X = [x ,x ,...,x ] are D = [d ,d ,...,d ], where
i 1 2 n i 1 2 n
M
(cid:88)
d = a 1(Q ≤ x < Q ) ∀k ∈ {1,...,n}
k s s k s+1
s=0
6.3. Description of Datasets
Num. of Num. of Num. of Num. of
Dataset attributes continuous discrete entries
(columns) attributes attributes (rows)
Wdbc [ref] 30 30 0 569
Heart Disease [ref] 13 5 8 297
Diabetes [ref] 16 1 15 519
Thyroid Ann Train [ref] 21 15 6 3771
Parkinson [ref] 22 22 0 197
In all datasets we discretized the continuous attributes with the method
explained in Subsection 6.2 and omitted all rows with missing values. It is
important to underscore here that the accuracy results usually may depend
on the discretization methodology. In real-life problems experts may have
a better insight into the data, therefore they can use a better method to
discretize the continuous variables by choosing meaningful intervals. To have
a more general approach here, and to be able to apply the same methodology
to all datasets, we chose the quantile discretization.
All of our results are based on the same type of discretization, therefore
these results should be not compared with other results based on other dis-
cretization. The aim of the paper was to present the GNB structure and to
compare it with other algorithms by using a general discretization method.
The algorithms were run 5 times on each dataset with a random 15% test
set selection, using various numbers of triplets. The average results of these
runs can be seen in this subsection (Figure 7, 8, 9, 10).
356.3.1. Wdbc dataset
IntheDiagnosticWisconsinBreastCancerDatabase(https://archive.
ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic)theaim
is to classify a tumor as benign or malignant. The data is slightly unbalanced
0.63 of the data are benign (B) and 0.37 of the data are malign (M). The
average results of the runs can be seen in Figure 7. In Figure 7 of GNB-A one
can observe that the highest score is achieved with 25 variables whereas in
the case of GNB-O the scores typically improve until it uses all 30 explana-
tory variables. It is also interesting to remark, that by using these methods
on this dataset over-fitting is avoided.
(a) Results of the GNB-A algorithm on the (b) Results of the GNB-O algorithm on the
Wdbcdataset Wdbcdataset
Figure 7
6.3.2. Heart Disease dataset
The Heart Disease database (https://archive.ics.uci.edu/dataset/
45/heart+disease) contains 4 datasets from which we used the Cleveland
dataset as it contained the fewest missing values, 6 rows in all which were
left. The class variable takes on integer values from 0 (no presence of heart
disease) to 4. Still, we concentrated on simply distinguishing the presence
(values 1, 2, 3, 4) from the absence (value 0) of the disease in a patient, so
we joined classes 1, 2, 3 and 4 into one class. The average results of the runs
can be seen on Figure 8. One can observe in Figure 8 that 8 explanatory
variables seem to give the best scores, which cannot be improved further.
36(a) Results of the GNB-A algorithm on the (b) Results of the GNB-O algorithm on the
HeartDiseasedataset HeartDiseasedataset
Figure 8
6.3.3. Diabetes dataset
This dataset contained no missing values and only one continuous at-
tribute (Age), which was discretized with the method explained in Subsec-
tion 6.2. The algorithm was ran 5 times on it with a random 15% test set
selection. The average results of the runs can be seen on Figure 9.
It seems that if a small number of triplets are desired, then using around
8 of them will result in a high accuracy, precision, recall, and F1 score; These
scores remain relatively large by further adding new triplets.
(a)ResultsoftheGNB-AalgorithmontheDi- (b)ResultsoftheGNB-OalgorithmontheDi-
abetesdataset abetesdataset
Figure 9
6.3.4. Thyroid dataset
TheThyroiddatasetcontainedmultipledifferentdataframes,ofwhichwe
have selected the Thyroid Ann Train dataset, as it contained mostly binary
37values and enough rows to reliably calculate the information contents of the
selected triplets.
We have discretized the six continuous attributes with the method ex-
plained in Subsection 6.2, and joined classes 1 and 2 (irregular thryoid level)
into a single class. We have then removed column b14 as it contained 3771
1’s but only a single 0. The algorithm was ran 5 times on it with a random
15% test set selection. The average results of the runs can be seen on Figure
10.
It seems that for this database specifically, using about 5 triplets is op-
timal, as any further triplet introduced does not increase the accuracy, pre-
cision, recall or F1 score significantly. It is interesting to note that even by
adding new triplets over-fitting is avoided.
(a)ResultsoftheGNB-AalgorithmontheThy- (b)ResultsoftheGNB-OalgorithmontheThy-
roiddataset roiddataset
Figure 10
6.3.5. Parkinson dataset
The Parkinsons Disease (PD) Data Set contains biomedical voice mea-
surements from 31 people, 23 with Parkinson’s disease. The aim is to dis-
criminate healthy people from those with PD. The average results of the runs
can be seen in Figure 11. It is worth observing that the accuracy scores were
the highest when almost all features were added.
38(a) Results of the GNB-A algorithm on the (b) Results of the GNB-O algorithm on the
Parkinsondataset Parkinsondataset
Figure 11
6.4. Comparison of GNB-A and GNB-O with other methods
Dataset\Model GNB-A GNB-O NB TAN
Wdbc 0.9349 0.9442 0.8419 0.8884
Heart Disease 0.7956 0.8133 0.7156 0.7511
Diabetes 0.8692 0.8692 0.8692 0.9154
Thyroid Ann Train 0.9212 0.9208 0.9283 0.941
Parkinson dataset 0.8933 0.8867 0.7533 0.76
Table 1: Accuracy
Dataset\Model GNB-A GNB-O NB TAN
Wdbc 0.921 0.9341 0.8688 0.9777
Heart Disease 0.8249 0.828 0.6913 0.7753
Diabetes 0.9419 0.9419 0.8243 0.9605
Thyroid Ann Train 0.9717 0.9717 0.9286 0.971
Parkinson dataset 0.9628 0.9462 0.8158 0.8838
Table 2: Precision
39Dataset\Model GNB-A GNB-O NB TAN
Wdbc 0.9238 0.9339 0.7316 0.7542
Heart Disease 0.7262 0.7621 0.7214 0.6405
Diabetes 0.845 0.845 0.8261 0.9035
Thyroid Ann Train 0.9425 0.9421 0.9996 0.9655
Parkinson dataset 0.8968 0.9045 0.8718 0.7916
Table 3: Recall
Dataset\Model GNB-A GNB-O NB TAN
Wdbc 0.9213 0.9333 0.7928 0.8501
Heart Disease 0.7677 0.7887 0.6989 0.7008
Diabetes 0.8908 0.8908 0.8243 0.9305
Thyroid Ann Train 0.9569 0.9567 0.9628 0.9682
Parkinson dataset 0.9271 0.924 0.8406 0.8315
Table 4: F1-score
Dataset\Model GNB-A GNB-O NB TAN
Wdbc 0.9346 0.9424 0.825 0.8713
Heart Disease 0.7902 0.8077 0.7211 0.7392
Diabetes 0.877 0.8986 0.8598 0.9202
Thyroid Ann Train 0.7922 0.7920 0.4998 0.7852
Parkinson dataset 0.8984 0.8689 0.632 0.7408
Table 5: ROC AUC score
7. Conclusion
This paper introduces the Generalized Naive Bayes structure as a partic-
ular case of junction trees, called third-order cherry trees. We gave in the
paper two new algorithms for the construction of the GNB structure. GNB-
A which is a greedy algorithm, and GNB-O which is the optimal algorithm,
under the assumption that the most informative triplet is contained by this
structure.
We proved that the probability distribution associated with the GNB
structure gives a better approximation to the real data than the probability
distribution associated with the Naive Bayes structure.
40We also proved that our GNB-O algorithm gives the optimal GNB- dis-
tribution in terms of minimization of the Kullback-Leibler divergence if the
triplet with maximum information content is contained by this structure.
Both of our algorithms were tested on real data, and compared with
related algorithms, by using different accuracy measures. It turned out that
our algorithms work many times better than the related algorithms NB and
TAN.
It is very important to highlight here that our algorithms opposed to
the black-box type of algorithms are transparent, overfitting minimization is
encoded in the optimization task and it is easy to interpret the results, and
the relevant features.
While the optimized algorithm GNB-O is optimal over all of the explana-
tory variables considered, the greedy algorithm GNB-A finds in each step
(adding a new variable) the best choice to minimize the Kullback Leibler
divergence. In this way, GNB-A can be stopped at any level, the structure
will be a cherry tree with a high weight, whereas GNB-O is optimal when we
use the whole structure.
Based on the GNB structures, feature importance scores were introduced
which are independent of the "values" taken on by the attributes, because
these are related to the information content concept. Moreover, accuracy
scores (accuracy, precision, recall, f1 score, AUC) were assigned to the in-
creasing number of cherries involved.
We gave a comparison between the algorithms introduced here with the
already-existing related algorithms from structural, algorithmic, and com-
plexity points of view, explaining what they have in common and the dif-
ferences between them. We highlight here that despite the graph of the
GNB structure which as graph coincides with the TAN graph structure, the
GNB structure is optimized to fit the data by minimizing KL divergence.
Moreover, the algorithms introduced, maximize at the same time the added
information and minimize the redundancy in each step.
We hope this work will greatly impact to a large range of classification
problems related to different fields. In medical studies, it can be a great tool
because the results are easy to interpret, and feature importance may have a
very positive impact on different studies. The methods introduced here can
be easily applied to more general classification tasks (not only binary). The
results presented are based on quantile-discretized to keep it as general as
possible. We are sure that all results may be improved by considering other
discretizations, but analyzing this was beyond the scope of this paper.
41References
[1] H. Zhang, L. Jiang, L. Yu, Attribute and instance weighted naive Bayes,
Pattern Recognition 111 (2021).
[2] A. H. Rabie, N. A. Mansour, A. I. Saleh, A. E. Takieldeen, Expecting
individuals’ body reaction to covid-19 based on statistical naïve bayes
technique, Pattern Recognition 128 (2022) 108693.
[3] W. M. Shaban, A. H. Rabie, A. I. Saleh, M. Abo-Elsoud, Accurate de-
tection of covid-19 patients based on distance biased naïve bayes (dbnb)
classification strategy, Pattern Recognition 119 (2021) 108110.
[4] H. Kim, S.-S. Chen, Associative naive bayes classifier: Automated link-
ing of gene ontology to medline documents, Pattern Recognition 42 (9)
(2009) 1777–1785.
[5] Z. Ye, P. Song, D. Zheng, X. Zhang, J. Wu, A naive bayes model on
lung adenocarcinoma projection based on tumor microenvironment and
weighted gene co-expression network analysis, Infectious Disease Mod-
elling 7 (3) (2022) 498–509.
[6] J. R. Quinlan, C4.5: Programs for machine learning, Elsevier, 2014.
[7] P. Domingos, M. Pazzani, Beyond independence: Conditions for the
optimality of the simple Bayesian classifier, in: Proc. 13th Intl. Conf.
Machine Learning, 1996, pp. 105–112.
[8] H. Zhang, The optimality of naive Bayes, in: Proceedings of the Sev-
enteenth International Florida Artificial Intelligence Research Society
Conference, FLAIRS 2004, 2004, p. 3.
[9] M. Hall, A decision tree-based attribute weighting filter for naive Bayes,
in: International conference on innovative techniques and applications
of artificial intelligence, Springer, 2006, pp. 59–70.
[10] C.-H. Lee, F. Gutierrez, D. Dou, Calculating feature weights in naive
Bayes with Kullback-Leibler measure, in: 2011 IEEE 11th International
Conference on data mining, IEEE, 2011, pp. 1146–1151.
[11] S. Chen, G. I. Webb, L. Liu, X. Ma, A novel selective naïve Bayes
algorithm, Knowledge-Based Systems 192 (2020).
42[12] N. Friedman, D. Geiger, M. Goldszmidt, Bayesian network classifiers,
Machine learning 29 (1997) 131–163.
[13] H. Zhang, S. Sheng, Learning weighted naive Bayes with accurate
ranking, in: Fourth IEEE International Conference on Data Mining
(ICDM’04), IEEE, 2004, pp. 567–570.
[14] I. Kononenko, Semi-naive Bayesian classifier, in: Machine
Learning—EWSL-91: European Working Session on Learning Porto,
Portugal, March 6–8, 1991 Proceedings 5, Springer, 1991, pp. 206–219.
[15] L. Jiang, Z. Cai, D. Wang, H. Zhang, Improving tree augmented naive
bayes for class probability estimation, Knowledge-Based Systems 26
(2012) 239–245.
[16] L. Wang, Y. Xie, M. Pang, J. Wei, Alleviating the attribute conditional
independenceandiidassumptionsofaveragedone-dependenceestimator
by double weighting, Knowledge-Based Systems 250 (2022).
[17] F. Ghofrani, A. Keshavarz-Haddad, A. Jamshidi, A new probabilistic
classifier based on decomposable models with application to internet
traffic, Pattern Recognition 77 (2018) 1–11.
[18] M. Loéve, Probability Theory, Foundations, Random Processes, D. van
Nostrand, 1955.
[19] A. P. Dawid, Conditional independence in statistical theory, Journal of
the Royal Statistical Society: Series B (Methodological) 41 (1) (1979)
1–15.
[20] W. Spohn, Stochastic independence, causal independence, and shield-
ability, Journal of Philosophical logic 9 (1) (1980) 73–99.
[21] E. Kovács, T. Szántai, On the approximation of a discrete multivariate
probability distribution using the new concept of t-cherry junction tree,
in: Coping with Uncertainty, Springer, 2010, pp. 39–56.
[22] D. Pfeifer, E. A. Kovács, Vine copula structure representations using
graphs and matrices, Information Sciences (2024) 120151.
[23] T. M. Cover, J. A. Thomas, Elements of information theory, John Wiley
& Sons, 2012.
43[24] S. Kullback, R. A. Leibler, On information and sufficiency, The annals
of mathematical statistics 22 (1) (1951) 79–86.
[25] T. Szántai, E. Kovács, Hypergraphs as a mean of discovering the de-
pendence structure of a discrete multivariate probability distribution,
Annals of Operations Research 193 (1) (2012) 71–90.
[26] C. Chow, C. Liu, Approximating discrete probability distributions with
dependence trees, IEEE transactions on Information Theory 14 (3)
(1968) 462–467.
[27] The implementation of the log base-2 function in C, https://github.
com/sifive/picolibc/blob/master/newlib/libm/common/log2.c,
line 92, an 8-element Taylor series expansion.
[28] J. Edmonds, et al., Optimum branchings, Journal of Research of the
national Bureau of Standards B 71 (4) (1967) 233–240.
[29] H. Zhang, L. Jiang, G. I. Webb, Rigorous non-disjoint discretization for
naive bayes, Pattern Recognition 140 (2023) 109554.
[30] S. Wang, J. Ren, R. Bai, Y. Yao, X. Jiang, A max-relevance-min-
divergence criterion for data discretization with applications on naive
bayes, Pattern Recognition 149 (2024) 110236.
44