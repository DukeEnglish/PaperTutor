In-Context Imitation Learning
via Next-Token Prediction
LetianFu∗1 HuangHuang∗1 GauravDatta∗1 LawrenceYunliangChen1
WillPanitch1 FangchenLiu1 HuiLi2 KenGoldberg1
Abstract: Weexplorehowtoenhancenext-tokenpredictionmodelstoperform
in-contextimitationlearningonarealrobot,wheretherobotexecutesnewtasks
byinterpretingcontextualinformationprovidedduringtheinputphase,without
updatingitsunderlyingpolicyparameters. WeproposeIn-ContextRobotTrans-
former(ICRT),acausaltransformerthatperformsautoregressivepredictionon
sensorimotor trajectories without relying on any linguistic data or reward func-
tion. Thisformulationenablesflexibleandtraining-freeexecutionofnewtasks
attesttime,achievedbypromptingthemodelwithsensorimotortrajectoriesof
thenewtaskcomposingofimageobservations,actionsandstatestuples,collected
throughhumanteleoperation. ExperimentswithaFrankaEmikarobotdemonstrate
thattheICRTcanadapttonewtasksspecifiedbyprompts,eveninenvironment
configurationsthatdifferfromboththepromptandthetrainingdata. Inamulti-
task environment setup, ICRT significantly outperforms current state-of-the-art
next-tokenpredictionmodelsinroboticsongeneralizingtounseentasks. Code,
checkpointsanddataareavailableonhttps://icrt.dev.
Keywords: Multi-TaskLearning,Next-TokenPrediction,In-ContextLearning
1 Introduction
Learning-basedsingleandmulti-taskrobotpolicieshavebecomeincreasinglycapable[1,2,3,4,
5, 6, 7, 8, 9, 10]. This improvement in robot capabilities can largely be attributed to progress in
related fields, particularly in vision and language modeling. Inspired by the recent development
oflargelanguagemodels(LLMs)andlargevisionmodels(LVMs)[11,12,13],whichformulate
natural language processing and vision problems all as next-token-prediction, recent works also
have formulated robot learning as next-token-prediction problems and achieved state-of-the-art
performance [7, 8, 14, 15]. Concurrently, there has been a surge in collecting large-scale robot
datasets[16,17,18,19,20,21,22,23]andpre-trainingmodelsonthesedatasets[24,25,26,27,15].
Despite being pre-trained on large datasets and showing some generalization ability, it is still
challengingtoteachthesemodelstoperformunseentasksindifferentenvironmentswithoutadditional
training.Newhumandemonstrationsviateleoperationornewdatacollectedfromhand-craftedmotion
primitives,aswellasanotherroundofmodel-finetuning,areoftenneededtocompletethenewtasks.
This process adds complexity to the workflow, making it challenging to apply these methods in
real-worldenvironments. Ideally,givenoneorafewdemonstrations,therobotshouldbeableto
perform the task immediately. In their respective domains, LLMs and LVMs [11, 12, 13] have
exhibitedasimilarability, namedin-contextlearning: acapabilityallowingthemodeltorapidly
adapt to and recognize the task corresponding to the prompt provided at inference time without
additionaltraining.
∗EqualContribution
1UniversityofCalifornia,Berkeley
2AutodeskResearch
4202
guA
82
]OR.sc[
1v08951.8042:viXraFigure1:In-ContextRobotTransformer.Whenwewantarobottolearnanewtask,inmanycases,eitherwe
havetoprogramnewprimitivestoperformthetask,orwehavetoprovidemanyhumandemonstrationstotrain
animitationlearningmodel. Canamodellearnthenewtaskwithfewdemonstrationswithouttraining? We
trainanext-tokenpredictionmodeltoperformin-contextimitationlearningonarealrobot.Inparticular,the
modellearnsfromrobottrajectoriestoperformcontinuousactionpredictions.Atinferencetime,wepromptthe
modelwithrobotsensorimotortrajectoriescollectedbyhumanteleoperationandprovidethemodelwiththe
observationofthenewenvironment,androlloutthepolicyonaphysicalrobot.
Isthein-contextlearningcapabilityofnext-tokenpredictionmodelslimitedtovisionandlanguage
domains? In this paper, we introduce In-Context Robot Transformer (ICRT), where we explore
hownext-tokenpredictionmodelscanbeextendedtoperformreal-worldrobotin-contextlearning.
For ICRT, the context is provided as a series of robot trajectories corresponding to a new task.
The model learns from this context to perform the task in a different environment configuration
withoutrequiringadditionaltraining. Arobottrajectoryisasequenceofimageobservations,robot
proprioceptivestates,andactions. Thistrajectoryimplicitlyencodestaskprimitivesandtheobjects
the robot needs to interact with. The model extracts this information from the prompt and then
executesactionsfollowingasimilarpatterninitscurrentenvironment.
Comparedtoexistingoneorfew-shotimitationlearningapproaches,ICRToffersasimpleframe-
workthatavoidscomplicatedlossfunctionsorkey-pointselection,andoperatesdirectlyonrobot
trajectoriesforcontinuouscontrol. Additionally,unlikeexistingnext-tokenpredictionmodelsfor
robotlearning,ICRTfeaturesalongcontextwindow,allowingittotrainonmultiplesensorimotor
trajectories from the same task and use one or more sensorimotor trajectories as prompts during
inference.
Moreover,weobservethatcertainpropertiesofthedatasetarecrucialforenablingeffectivein-context
learningonrealrobots. Specifically,datasetsthatallowmultipletaskstobeperformedfromthesame
initialobservationareparticularlybeneficial. Insuchscenarios,unlikeexistingsingle-taskdatasets
andmanymulti-taskdatasetswhereeachenvironmenthasauniqueobjectfortherobottointeract
with,themodelmustrelyontheprompttocorrectlyidentifythetaskanddeterminetheappropriate
objectforinteraction.
Wemakethefollowingcontributions:
1. WeintroduceICRT,anext-tokenpredictionmodelthatperformsin-contextlearningona
realrobot. ICRTtakesrobot’ssensorimotortrajectoriesonnewtasksascontexttoperform
specifiedtasksinunseenenvironmentconfigurations.
2. Weprovideanewmulti-taskrobotdatasetandatrainingparadigmforfosteringmulti-task
andin-contextcapabilityatinferencetime.
3. WeperformphysicalexperimentsonaFrankaEmikarobotatvariouslevelsoftaskdiffi-
cultiestoevaluatethein-contextlearningabilitiesofICRT.ResultssuggestthatICRTcan
performtheunseentasksspecifiedbytheprompt.
2 RelatedWorks
2.1 ImitationLearningforRobotics
Imitationlearningisapopularandeffectiveparadigmforequippingrobotswithvariousskills. The
simplest algorithm in this domain, behavior cloning, has been successful across a wide range of
tasks[28,29,30]. Inrecentyears,alternativearchitecturessuchasenergy-basedmodels[31]and
2diffusionmodels[1]havealsobeenproposed. Typically,theseapproachesrequiretrainingaseparate
modelforeachtask,althoughmulti-taskpoliciescanbedistilledfromthesetask-specificmodels
aftertraining[32].
Recent advancements have shown that using transformers for next-token prediction in sequence
modelinghasbeenparticularlyeffectiveinbothlanguageandvisiondomains,especiallyformulti-
tasklearning[33,12,34]. Thisapproachhasalsobeenextendedtoroboticlearning,whererobot
actionplanningisframedasanext-tokenpredictiontaskusingtransformer-basedarchitectures[35,
36,7,8,14,15]. Inthesemodels,observationsareusedtopredictthenextrobotactions. Inaddition,
inpursuitofdevelopinggeneralistagentsandrobustrobotpolicies,recentresearchhasdemonstrated
thattrainingpoliciesonlarge,diversedatasetsencompassingmultipletaskscanleadtomorerobust
andgeneralizablemodels[37,38,39,40,15,5,7,36]. Octo[15]andOpenVLA[14]aretrainedon
largeroboticdatasets,andarethestate-of-the-artpoliciesconditioningongoalimagesandlanguage
instructions(Octo)orjustlanguageinstructions(OpenVLA).Octoemploysatransformerwitha
diffusionhead,whichprocessesthegoalconditions—languageinstructionsorgoalimages—andthe
currentimageobservationtopredictrobotactions.OpenVLAfine-tunesapre-trainedvision-language
modeltopredictrobotactionsgivenvisionobservationsandlanguageinstructions.
2.2 In-ContextLearning
Despite the effort of training on large datasets, these policies still struggle with novel tasks or
environmentsandoftenrequirefine-tuning. Severalworkshaveexploredwaystobypasstheneed
formodelfine-tuningortoincreasesampleefficiencywhengeneralizingtonewtasks,leadingto
advancesinzero-shotandfew-shotimitationlearning.Someapproachesinmeta-learning[41,42,43]
enablefew-shotimitationlearningaftertrainingonawiderangeoftasks,butstillrequirefine-tuning
ineachnewdomain. Otherworksdon’trequirefine-tuningmodelparametersforgeneralizingtonew
tasks.Brownetal.[33]referstothisas“in-contextlearning”todifferfromworksthatfine-tunethe
modelparameters.
Manyin-contextlearningmethodsoftenemploycontrastivelearningtotraincontextencoders,which
identifythemostsimilartrainingtaskstothetesttaskinthelatentspace[37,44]. However,how
toeffectivelyintegratethesemethodswithinthenext-token-predictionframeworkremainsunclear.
Valassakisetal.[45]achievedone-shotin-contextlearningbytrainingavisualservoingnetwork
toaligntherobot’send-effectorwiththeobject’srelativeposeduringthedemonstration,butthis
approach requires an additional object segmentation model. Di Palo and Johns [46] introduced
KeypointActionTokens,demonstratingin-contextimitationlearningusingalargelanguagemodel
by representing demonstration trajectories as 3D coordinates with few-shot prompting. Unlike
these approaches, ICRT operates without additional perception modules, processing raw image
observationsdirectly. Additionally,Vid2Robot[47]developedanencoder-decodertransformerthat
usesademonstrationvideoofahumanandthecurrentrobotstateastheprompttogeneraterobot
actions. However,thismethodrequiresmanyauxiliarylosseswhileICRTusesasimplenext-token
predictionloss.
Inthispaper,wefocusonenhancingnext-token-predictionmodelstoperformreal-worldin-context
imitationlearningwithrobots. ICRTbypassestheneedforadditionalcontextencodersbydirectlyus-
ingrobotsensorimotortrajectoriesfromnewtasksaspromptsforthetransformer-basedmodel. ICRT
iscloselyrelatedtotheseminalwork,One-ShotImitationLearning[48]andPromptingDecision
Transformer[49].[48]predictsthenextactionbyapplyingcross-attentionbetweenademonstration
sequenceonanewtaskandthecurrentobservation,while[49]employsashorttrajectoryprompt
toencodetask-specificinformationforguidingpolicygenerationinofflinereinforcementlearning.
However,neitheroftheseapproachesconsidersimageobservationsasinputs,nordotheyextend
beyondtasksinsimulation. Incontrast,ICRTdoesnotmodelrewards,utilizesasignificantlylonger
contextwindow, anddemonstratesin-contextlearningcapabilitiesinphysicalexperimentsusing
imageobservations.
33 ProblemStatement
We investigate in-context imitation learning on a real-robot in a continuous control setting. The
objectiveistotrainamodelwithin-contextlearningcapabilitiesusingamulti-taskdataset. Attest
time,themodelcanperformanunseentaskinanewenvironmentconfigurationbytakingafewnew
human-teleoperatedrobotdemonstrationsasaprompt. Wedefineenvironmentconfigurationasthe
objectsinthesceneandtheirlocations. Importantly,thisisaccomplishedwithoutanyadditional
trainingonthenewdemonstrations.
Wedefinemotionprimitivesasdistinctrobotmotionsusedtocompletedifferenttasks. Eachtask
is characterized by 1) a motion primitive and 2) the set of objects the robot interacts with using
thatprimitive. Byvaryingthetest-timeenvironmentconfigurationfromtheoneintheprompt,we
evaluatethemodel’sabilitytodeterminetheappropriatemotionprimitiveandidentifythecorrect
objecttointeractwith. Inthiswork,weconsidernewtaskstobetasksinvolvingunseenobjectsbut
usingmotionprimitivesfromthetrainingdata(forexample,trainingonpickingupatigertoyand
testingonpickingupacube).
WemakethefollowingassumptionsforICRTexperiments:
1. Themodelistrainedonadatasetconsistingofdiversedemonstrationsofasinglerobot.
EachdemonstrationtrajectorycontainsobservationsfromanRGBcameraatafixedposition
andawrist-mountedRGBcamera,proprioception,action,andtheassociatedtasktype.
2. Thetasktestedontherobotcanbecompletedbyhumanteleoperatingtherobotandisthus
withinthereachableworkspaceoftherobot.
4 Approach
Inthissection,wefirstintroducethedatacompositiontofacilitatein-contextimitationlearning. We
thenintroducethetransformer-basedpolicyanditstrainingformulationtoleveragethedata.
4.1 DataFormulation
For model training, we consider a dataset D of visuomotor trajectories T. Each trajectory of
length t is a sequence of camera images i , proprioceptive robot states s , and actions a : T =
t t t
(i ,s ,a ,...,i ,s ,a ). Weusetheabsoluteend-effectorposeastherobot’sproprioceptivestateand
1 1 1 t t t
thedeltarobotend-effectorposebetweentimestepsastheaction,whichconsistsofdeltatranslation,
delta rotation and the continuous gripper action (see Appendix Section 8.4 for more detail). We
assumeaknowngroupingofthetrajectoriessothatthedatasetcanbepartitionedintodisjointsetsof
tasksD
=(cid:83)K
S ,withS ∩S =∅, k ̸=ℓ,whereS ={T ,...,T }. Inpractice,thisgrouping
k=1 k k ℓ k k1 kn
canberetrievedfromthesemanticlabelsofthedataset. Inthiswork,weutilizetheexistinglarge
roboticdatasetDROID[50]andamulti-taskdatasetmanuallycollectedinourrobotsetup,whichwe
nameICRT-Multi-Task(ICRT-MT).
DROID[50]isajointeffortfromdifferentorganizationsandcontains76kreal-worlddemonstrations.
Werandomlysample10kdemonstrationsfromDROIDafterfilteringoutdemonstrationsshorter
than30stepsandlongerthan450steps. DROIDdatasetlabelsthetaskthroughhuman-specified
languageinstructions,whichmaybedifferentforthesametask. WeorganizedtheDROIDdataby
groupingdemonstrationsbasedontheirlanguageinstructionsCLIPtextembeddingcosinesimilarity.
Specifically,weuseathresholdof0.9forgroupingdemonstrations. Tofurtherfacilitatein-context
learning,wemakesurethateachtaskgroupcontainsatleast4trajectoriessothattherearesufficient
trajectoriestoserveaspromptsforeachother. Thisresultsinroughly2ktrajectoriesthatweusefor
pre-trainingICRT.
ManytrajectoriesintheDROIDdatasetarecollectedinasingle-tasksetup,whereonlyonetaskcan
becompletedinthegivenenvironment. Insuchsetup,themodelcanlearntheshortcutofperforming
thetaskjustconditionedonthecurrentstateandobservationandneverlooksattheprompt,even
4Figure2:MethodOverview:(Left)Weencodetheleftandwristcameraobservationwithapre-trainedvision
transformer.Additionally,weencodeproprioceptionwithamultilayerperceptron(MLP).Weconcatenatethe
visuallatentandtheproprioception’slatentanduseattentionpoolingtoextractafeaturef torepresentthe
s
currentstate.WeuseanotherMLPtoencodetheactiontakenatthecurrentstepastheactionfeaturef .(Right)
a
Weconcatenatemultipletrajectoriesofthesametaskandrandomlysamplethefirstktrajectoriesastheprompt.
Weencodethetrajectoriesviaacausaltransformer,andthemodeldecodesaseriesoftokens.Wedecodethe
tokensthatareatthepositionofthestatefeaturestogeneratethenexth=16actionviaaMLP.
thoughtheprompttrajectoriesaresimilartothecurrenttasktobeperformed. Thereforemulti-task
dataiscrucialforthemodeltolearnfromtheprompt. Wemanuallycollectedamulti-taskdataset
ICRT-Multi-Task(ICRT-MT)usingtheDROIDsetup(Figure.4). Thisdatasethas1098trajectories
intotal,andcontains29taskswith6primitives: picking,pick-and-place,stacking,pushing,poking,
openingandclosingdrawers. Objectsusedinthedatacollectionandexamplesoftheprimitivesare
showninFigure.4. InICRT-MT,eachenvironmentissetsothatthereexistmorethan2possible
tasksforthecurrentobservationsothatthemodelhastodistinguishandlearnthemotionfromthe
prompt.
Duringthetraining,foreachtrajectory,weindependentlyapplyvisionaugmentationontheimage
observationsbyaugmentingthebrightnessandcontrast. Weadditionallyapplyrandomcropsand
scalingtothesidecameraobservation. Wealsoapplyproprioceptionnoisesampledfromanormal
GaussiandistributionN(0,0.005). Foreachepoch,werandomlyshuffletheorderoftrajectories
fromeachtaskandconcatenatethemtoformthetrainingsequence. Foreachbatch,wesubsample
for a subsequence of length L = 512 as the input to the model, where L is the sequence length
definedasthenumberofobservation,state,andactiontuples. Inpractice,512stepsusuallycontain
upto5trajectoriesfromthesametask. Werandomlyselectthefirstk trajectoriesandlabelthem
asthepromptwithinthesequence. Atleastonecompletetrajectoryisincludedintheprompt. This
datagroupingaimstocaptureinter-trajectorypatterns,encouragingthemodeltogenerateaction
conditioned on the prompt trajectories. This approach differs from traditional behavior cloning
methods,whichtypicallyuseshortinputsequencesthatfocusonmodelingintra-trajectorybehaviors.
4.2 ModelArchitecture
Tofacilitatein-contextlearninginaroboticssetting,themodelshouldhaveasufficientlylongcontext
windowtosupportpromptingbyprovidingrobottrajectoriesasdemonstrations. Weconstructthe
ICRT model with three parts: a pre-trained vision encoder, a series of projectors for each input
modality,andacausaltransformerbackbone(Figure2).
VisionEncoderThemodelprocessesmulti-viewimageobservationsthroughapre-trainedvision
transformer.However,mostvisualpre-trainednetworksaretrainedonImageNetorhumanvideos[27,
51,52,24],whichexhibitasignificantdomaingapwhencomparedtotypicalimagesfromrobot
datasets, where the images frequently include robots or grippers. To minimize the domain gap,
we pre-train a vision transformer [53] (ViT-Base) on an equal mix of ImageNet [54] and Open
X-Embodiment [40] data, using CrossMAE as an efficient pre-training method [55]. During the
trainingoftheICRTmodel,wefreezethevisionencoderforefficiency. Thevisionencoderoutputs
the entire feature map for each of the cameras and is then fed into the proprioception projector
(Figure2left).
Modality-SpecificProjectorsToprojectimageobservations,therobot’sproprioceptivestate,and
actionsintoasharedlatentspaceforsequencemodeling,wedesignmodality-specificprojectors.
Ateachtimestep,themodeltakesasinputatokenrepresentingeithertherobot’sstateoranaction.
5Figure3:ExampleinferencepipelineofICRTonthetaskofpickinguptheradishandputtinginthegraybowl.
Ahumanteleoperateddemonstrationtrajectoryconsistingofimageobservations,proprioceptionandactions
areprovidedastheprompt. ICRTtakesthepromptandthecurrentobservationinadifferentenvironmentto
accomplishthetask.
Toproduceasinglestatetokenthatcapturesfine-grainedvisualinformationandtheproprioceptive
stateoftherobot,weuseattentionpooling[56]betweenallvisualtokensfromasinglecamera’s
observation and a proprioception embedding produced by a multi-layer perceptron (MLP). The
resultingembeddingsforeachcameraareconcatenatedtoproduceasinglestatetokenftofdimension
s
equaltothetransformerlatentdimension. Similartoproprioception,theactionisembeddedwithan
MLPintoanactiontokenft. Thisprocessproducesasequenceofstateandactiontokensthatare
a
passedintothetransformer.
Transformer Model The encoded sequence of state and actions is passed into a Transformer
model[57],followingthedesignofLlama2[12]. Thetransformertakesasinputthesequenceof
stateandactionfeatures(f1,f1,··· ,ft,ft)thatareproducedbythemodality-specificprojectors.
s a s a
WeaddMLPdecoderstoproducestateandactionoutputsfromthelastlayerofthetransformerat
theappropriatepositions. Wedenotethetransformerwiththedecoderheadsasg . Therefore,the
θ
desiredoutputsaretheshiftedsequenceofproprioceptivestatesandactions(a1,s2,a2,··· ,at,st+1).
Thisnaturallyformsanexttokenpredictionproblem,asg (f1)predictsa1andg (f1,f1,··· ,fn)
θ s θ s a s
predictsan+1. Inpractice,wefinditbeneficialtopredictthenexthactionsateachtimestep,anduse
temporalensembling[2]toexecutethefinalaction.
InspiredbyOcto[15]andvisiontransformers[53],weconsiderarandomlyinitializedLlama2model
of12layerswithalatentdimensionof768, whichwenameLlama2-Base. Inaddition, multiple
workshaveshownthatmultimodalinputscanbealignedtolarge-languagemodels[34,58,59,8,60].
Multi-modal language model, Palm-E [10] has shown success in enhancing generalization when
beingdirectlyincorporatedintoroboticcontrol[8]. Therefore,wealsoinvestigatetheeffectiveness
ofusingalarge-languagemodelforin-contextrobotlearningbyinitializingthetransformerwitha
pre-trainedLlama2-7B.Duetothelargedomaingapbetweennaturallanguageandrobottrajectories,
a frozen language model may not be sufficient. Therefore, similar to prior work in multimodal
alignment,wefine-tunethelanguagemodelwithLoRA[61],witharankof32. Duetocompute
resourcelimitations,weareunabletofullyfine-tunethemodel.
Loss Function To provide more supervision signals so that the model can better respond to the
trajectory“prompt”weprovideattesttime,wereferenceworksintrainingmulti-turnconversation
chatbots[62,34],wheretheyonlycomputelossontheresponsegeneratedbythechatbot,insteadof
theprompt. RecallthatinSection4.1,werandomlysampledthesubsequenceoftheconcatenated
trajectoriesastheprompttrajectory. Analogously,weonlycomputeactionpredictionwithL1-loss
fortheactionsaftertheprompttrajectories.
InferenceThesimplicityofthenext-tokenpredictionobjectivemakesinferencingwithICRTstraight-
forwardattesttime. AsshowninFigure.3,weprovideoneormorehuman-teleoperateddemon-
6Figure4: Physicalexperimentssetupareshownontheleft,showingtheFrankaEmikarobot,thewristand
side camera and the objects used in training and evaluation. We consider 6 primitives and collect human
demonstrationsfortraining.Weconsiderpickupandplaceandpokeprimitivesforevaluation(darkgreen).
strationsintheformofrobotsensorimotortrajectories(formattedidenticallytothetrainingdata),
alongwiththecurrentimageobservationsandtherobot’sproprioceptivestateasinputs. Themodel
thenpredictsthenextaction,whichisexecutedbytherobot. Aftereachaction,thepolicyreceives
updatedimageobservationsandproprioceptivestate,allowingittoiterativelypredictandexecute
subsequentactions.
Akeyadvantageofthisframeworkisitsuseofthetransformer’ssequentialprocessingcapability.
Insteadofreprocessingtheentiresequencehistoryforeachmodelevaluation,asseeninprevious
works[15,14,7,8],themodelemploysakey-value(KV)cachingmechanism,asdiscussedin[12].
Thismechanismstorespreviousoutputs,allowingthemodeltocomputeonlytheoutputsforthenew
token. Thisapproachsignificantlyreducescomputationaloverhead,loweringthecomplexityfrom
quadratictolinearrelativetothesequencelength.
5 Experiments
Inthissection,wedesignanexperimentalsetuptoevaluatethein-contextlearningcapabilitiesofthe
proposedmodelsforcontinuousrobotcontrolandcomparethemagainstseveralbaselines. Insteadof
focusingonthedifficultyoflearningaspecifictaskprimitive,wedesigntheexperimentstoassessthe
policy’sabilitytoaccomplishunseentasksamongallexecutableoptionsfromtheprovidedprompt
trajectories.
ExperimentDesignWeconsidertwoactionprimitives: apick-and-placeprimitiveandapoking
primitive. For each action primitive, we design six unseen tasks (as defined in Section 3), with
threetasksevaluatingin-domainobjectgeneralizationandthreeevaluatingonobjectsunseenduring
training(selectedfromradish,bluesponge,greydog,andblackdog,seeTable1andTable2).
Eachtaskisdesignedwithfivetiersofdifficulty. Inthepick-and-placeprimitive,themodelistasked
withidentifyingwhichobjecttograspandwheretoplaceitinamulti-objectormulti-placement
setting. Thetiersare: 1)pickandplacetheobjectwithoutanydistractors,2)withonedistractor
object, 3) with two distractor objects, 4) with three distractor objects, and 5) with one distractor
placementposition. Forthepokingprimitive,therobotmustclosethegripper,poketheobject,lift
theend-effector,andopenthegripper. Thefivetiersofdifficultyinvolvethetargetobjectpresented
with0-4distractorsinthescene.
Thepick-and-placeprimitiveisevaluatedbyassigningapartialcreditof0.5iftherobotcorrectly
picksuptheobject. Asuccessfulplacementresultsinatotalscoreof1. Thepokingtaskisevaluated
bywhetherthemodelpokesthecorrectobject;ifanincorrectobjectispoked,thetrialismarkedasa
failure. Themodelisallowedretrieswithinatimelimitof25seconds(or375steps). Eachtierof
difficultyisperformedonce,andwereporttheaveragesuccessratepertask,aswellastheaverage
successrateandstandarddeviationperactionprimitiveacrossthesixtasks.
AlgorithmsThedefaultICRTmodelisarandomlyinitializedLlama2-Basemodelpretrainedon
DROIDandfullyfine-tunedonICRT-MT.Weevaluatetheimpactofmodelinitializationandtraining
datasetsbyintroducingthefollowingthreevariants: 1)ICRT-Llama2,apre-trainedLlama2-7B
languagemodelfine-tunedonICRT-MTwithLoRA;2)ICRT(DROID),arandomlyinitialized
7PickandPlace
PickObject YellowCube YellowCube BlueBear Radish BlackDog BlueSponge
AverageSuccess(±Std.)
PlaceLocation BlackBowl GreyBowl PinkBowl GreyBowl PinkBowl SilverPot
GoalConditioned 40% 30% 20% 40% 40% 30% 33.3%(±7.5%)
Octo 10% 0% 10% 10% 0% 0% 5.0%(±5.0%)
OpenVLA 0% 0% 0% 50% 20% 0% 11.7%(±18.6%)
ICRT-Llama2 40% 40% 40% 60% 40% 40% 43.3%(±7.5%)
ICRT(DROID) 0% 0% 0% 0% 0% 0% 0.0%(±0.0%)
ICRT(MT) 90% 50% 80% 90% 60% 90% 76.7%(±16.0%)
ICRT 60% 50% 80% 50% 60% 90% 65.0%(±15.0%)
Table1:Pickupandplaceprimitiveperformedwithgoalconditionedorbyusingonesequenceastheprompt.
Llama2-BasemodeltrainedonlyontheDROIDdataset;and3)ICRT(MT),arandomlyinitialized
Llama2-BasemodeltrainedonlyontheICRT-MTdataset.
Weconsider3baselinealgorithms. Wetrainagoal-conditionedpolicy, whereineachsampleof
thedataset,thegoalobservationandstatepairarealwaysprependedtothesequence,andineach
sequence, thereexistsonlyonetrajectory. Thisresemblesthenormalgoal-conditionedimitation
learningsetup. Additionally,wefinetuneOcto[15],thestate-of-the-artgoal-imageandlanguage
conditionedpolicy,andOpenVLA[14],thestate-of-the-artlanguageconditionedmulti-taskimitation
learningalgorithm. Octoisfine-tunedusingtheirofficialfine-tuningrecipe. Weincorporateaction
chunking into OpenVLA by asking it to predict the next 16 actions, which performs better than
vanillaOpenVLAwhichpredictsonlythenextstep. Bothofthesemethodsarerepresentativeof
robotpoliciesthatusenext-tokenpredictionobjectives.
PromptGenerationForeachtask,wecollect3demonstrations(withzero,onedistractorobject,a
distractorplacementforpick-and-place,ortwodistractorobjectsforpoking)asthepromptintotal
beforerunningtheexperiment.PleaserefertotheAppendix8.1Figure5foravisualexample.During
testing,arandomdemonstrationisdrawnasaprompttoassessthemodel’sabilitytogeneralizeto
differentprompts. It’simportanttonotethattheenvironmentsetupduringpolicyrolloutdiffersfrom
theprompts’setup,ensuringthattheevaluationmeasuresthemodel’sunderstandingoftask-relevant
informationfromtheprompt,ratherthansimplycopyingactionsfromit.
Poke
PokeObject Radish RedCube GreyDog BlackCube PinkBowl BlueSponge AverageSuccess(±Std.)
GoalConditioned 0% 0% 0% 0% 40% 0% 6.7%(±14.9%)
Octo 20% 0% 60% 0% 0% 0% 13.3%(±22.1%)
OpenVLA 20% 0% 0% 0% 0% 0% 3.3%(±7.4%)
ICRT-Llama2 60% 100% 80% 60% 60% 80% 73.3%(±14.9%)
ICRT(DROID) 0% 0% 0% 0% 0% 0% 0.0%(±0.0%)
ICRT(MT) 100% 100% 40% 60% 60% 60% 70.0%(±22.4%)
ICRT 100% 100% 80% 80% 100% 100% 93.3%(±9.4%)
Table2:Pokingprimitiveperformedwithgoalconditionedorbyusingonesequenceastheprompt.
ResultsWepresenttheresultsinTable1andTable2. Forthepick-and-placeprimitive,weobserve
thatthegoal-conditionedpolicygenerallysucceedsinidentifyingthecorrectobjecttograspwhen
nodistractorobjectsarepresent. However,itsperformancedegradessignificantlyasthenumberof
distractorsincreases. Whenthegoalimageonlyspecifiesthetaskbutnotthespecificwaytoachieve
itinthecurrentenvironment,goal-conditionedpoliciesoftenfailtoexecutethetaskeffectively.
Octostruggleswithdeterminingwhichobjecttointeractwithandwhereitshouldbeplaced,high-
lightingthechallengesposedbyourexperimentalsetupformulti-taskpolicies. OpenVLA,while
oftenmovingtowardsthecorrectobject,frequentlyfailsingraspingtheobjectormistakenlyperforms
thewrongtask(e.g.,graspinginsteadofpoking,andviceversa). ThisindicatesthatOpenVLAmay
requireagreaternumberofdemonstrations(morethan50)pertasktoachievebetterperformance,
andthatrelyingsolelyonlanguageconditioningmaynotbesufficientforgeneralizationtonewtasks.
The results suggest that ICRT outperforms the goal-conditioned policy in identifying the correct
object to pick up and the appropriate placement location. The poking task presents a significant
challengeforthegoalorlanguage-conditionedpolicies,asthegoalpositionoftencloselyresembles
thestartconfiguration. However,afterconditioningontheprompttrajectory,ICRTisabletocorrectly
identify the task as poking, and the results indicate that it consistently reaches the correct target
8objectwhileignoringdistractors. Despitethis,wedoobservesomefailuremodeswithICRT,such
asmissingthegraspofthetargetobject,graspingthewrongobject,orplacingobjectsinincorrect
locations. Specifically, when a distractor object shares the same color but has a different shape,
the model struggles to accurately determine which object to grasp. This implies that additional
fine-tuningofthevisionencodermightberequiredtoenhancemodelperformance,aconclusionalso
reachedbyOpenVLA[14].
6 Ablations
Inthissection,weprovideadditionalexperimentspresentedTable1andTable2thatablateonafew
coredesignchoices. WeprovidemoreablationstudiesinSection8.2.
6.1 ModelInitialization
We conducted ablation studies to examine the impact of using a pretrained Llama2 on language
dataandfine-tuneitforrobotsensorimotorsequencemodeling. Theresults,presentedinTable1
andTable2,showthatalthoughICRT-Llama2-7Bachievesalowertrainingloss,itsperformanceis
worsecomparedtoitssmallercounterparts. Thisdiscrepancymaybeattributedtoalowerinference
frequencyofICRT-Llama2. Wesuggestthatfutureworkshouldfocusonoptimizingtheinference
speedofICRT-Llama2.
6.2 TrainingDataset
WefindthattrainingontheDROIDsubset(seeSection4.1)isinsufficientforsuccessfullycompleting
anyofthetesttasks;thepolicy(ICRT(DROID))showsnoprogressacrossalltasks. Thissuggests
thatalthoughtheDROIDsubsetmayoffergreatervisualdiversity,theuniquestructureofICRT-
MT—wheremultipletasksareperformedfromthesameinitialobservation—isparticularlybeneficial
indevelopingthein-contextlearningcapabilitiesofanext-tokenpredictionrobotmodel.
ICRT (MT) shows similar performance to ICRT that is pretrained on DROID, especially for the
pick-upandplaceprimitive,evensurpassingICRTontheputradishingreybowltask. However,
ICRT(MT)doesnotperformaswellonthepokingprimitive. Theresultssuggestthatitmaybe
beneficialtopre-traintheautoregressivemodelonalargedataset,asadiversedatasetmayhelpthe
transformertoperformbetteralignmentbetweenvisualfeaturesandcontrol.
6.3 NoPromptLoss
Followingthedesignof manymulti-turnconversation largelanguage modelsorvisionlanguage
modelfine-tuningworks[34,62,63,64], wedonotcalculatethelossforthepredictedactionin
theprompttrajectoriesbutonlydosoonthepredictionsaftertheprompttrajectories. Wemarkthe
modelthatcalculateslossonthepromptasICRT+PromptLossandthedefaultmodelasICRT.
TheresultsareshowninTable3andTable4. Wefindthatbylettingthemodelonlypredictthe
trajectoriesafterthedesignatedprompttrajectories,themodel’sperformanceimprovessignificantly.
Wehypothesizethatinthesituationwherethereisalossontheprompttrajectories,themodelis
forcedtodoun-conditionalgenerationbasedonthecurrentenvironmentobservation,especiallywhen
therearemultiplepossibletasksavailable. Thismaycausethemodeltostoppayingattentiontothe
prompt.
PickandPlace
PickObject YellowCube YellowCube BlueBear Radish BlackDog BlueSponge
PlaceLocation BlackBowl GreyBowl PinkBowl GreyBowl PinkBowl SilverPot
ICRT+PromptLoss 20% 10% 20% 40% 30% 10%
ICRT 60% 50% 80% 50% 60% 90%
Table3:Ablationonnotapplyinglossontheprompttrajectoriesforpickandplacetasks.
9Poke
PokeObject Radish RedCube GreyDog BlackCube PinkBowl BlueSponge
ICRT+PromptLoss 0% 20% 20% 80% 0% 20%
ICRT 100% 100% 80% 80% 100% 100%
Table4:Ablationonnotapplyinglossontheprompttrajectoriesforpokingtasks.
7 LimitationsandConclusion
Thismethodhasafewlimitations. WhileresultssuggestthatICRTcangeneralizetheprimitiveto
unseenobjectsandcertainprimitivesthatresembletheonesintraining(seeSection8.2.3),itisstill
unclearhowtogeneralizetounseenprimitives. Futureworksshouldinvestigatehowscalingmodel
capacityandscalingdatasetcanhelpwithprimitive-levelgeneralization. Inaddition,ICRTassumes
afixedrobotmorphologywithafixedimpedancecontroller. Futureworkscanalsoinvestigatehow
tofacilitatetransferbetweendifferentrobotmorphologiesbylearningaunifiedpolicyondifferent
robots. ICRT-Llama2hasalowinferencefrequencywhichmaycontributetoitslowperformance.
WehopetospeedupICRT-Llama2atinferencetimeinthefuture.
Insummary,wepresentICRT,wherewestudyin-contextimitationlearningonarealrobot. We
dosobytrainingacausaltransformermodelonsequencesofrobottrajectories,wheretrajectories
ofthesametaskarecombinedtoserveasthecontextforperformingthetask. Wealsopresenta
correspondingmulti-taskdatasettohelpfacilitatethisin-contextlearning. Wefindthatbyusingrobot
sensorimotortrajectoriesasthecontext,themodelcangeneralizethelearnedprimitivestounseen
objectsanddifferentenvironmentconfigurations,especiallyinenvironmentswheremorethanone
taskispresent.
Acknowledgments
ThisresearchwasperformedattheAUTOLABatUCBerkeleyinaffiliationwiththeBerkeleyAI
Research(BAIR)Lab,andtheCITRIS”PeopleandRobots”(CPAR)Initiative. Intheiracademic
rolesatUCBerkeley,LetianFu,HuangHuang,GauravDatta,LawrenceYunliangChen,William
Chung-Ho Panitch, Fangchen Liu, and Ken Goldberg are supported in part by donations from
Autodesk,Meta,Google,Siemens,ToyotaResearchInstitute,Bosch,andbyequipmentgrantsfrom
PhotoNeo, Nvidia, NSF AI4OPT Centre, and Intuitive Surgical. L.Y. Chen is also supported by
theNationalScienceFoundation(NSF)GraduateResearchFellowshipProgramunderGrantNo.
2146752. WethankXinyangGeng,DantongNiu,andChungMinKimfortheirhelpfuldiscussions
andfeedback.
10References
[1] C. Chi, S. Feng, Y. Du, Z. Xu, E. Cousineau, B. Burchfiel, and S. Song. Diffusion policy:
Visuomotorpolicylearningviaactiondiffusion. arXivpreprintarXiv:2303.04137,2023.
[2] T.Z.Zhao,V.Kumar,S.Levine,andC.Finn. Learningfine-grainedbimanualmanipulation
withlow-costhardware. arXivpreprintarXiv:2304.13705,2023.
[3] C.Lynch,A.Wahid,J.Tompson,T.Ding,J.Betker,R.Baruch,T.Armstrong,andP.Florence.
Interactivelanguage: Talkingtorobotsinrealtime. IEEERoboticsandAutomationLetters,
2023.
[4] S.Reed,K.Zolna,E.Parisotto,S.G.Colmenarejo,A.Novikov,G.Barth-Maron,M.Gimenez,
Y.Sulsky,J.Kay,J.T.Springenberg,etal. Ageneralistagent. arXiv:2205.06175,2022.
[5] D.Shah,A.Sridhar,N.Dashora,K.Stachowicz,K.Black,N.Hirose,andS.Levine. ViNT:A
FoundationModelforVisualNavigation. In7thAnnualConferenceonRobotLearning(CoRL),
2023.
[6] H.Bharadhwaj,J.Vakil,M.Sharma,A.Gupta,S.Tulsiani,andV.Kumar. RoboAgent:Towards
sampleefficientrobotmanipulationwithsemanticaugmentationsandactionchunking. arxiv,
2023.
[7] A.Brohan,N.Brown,J.Carbajal,Y.Chebotar,J.Dabis,C.Finn,K.Gopalakrishnan,K.Haus-
man, A. Herzog, J. Hsu, et al. Rt-1: Robotics transformer for real-world control at scale.
arXiv:2212.06817,2022.
[8] A.Brohan,N.Brown,J.Carbajal,Y.Chebotar,X.Chen,K.Choromanski,T.Ding,D.Driess,
A. Dubey, C. Finn, et al. RT-2: Vision-language-action models transfer web knowledge to
roboticcontrol. arXivpreprintarXiv:2307.15818,2023.
[9] X.Chen,J.Djolonga,P.Padlewski,B.Mustafa,S.Changpinyo,J.Wu,C.R.Ruiz,S.Goodman,
X. Wang, Y. Tay, S. Shakeri, M. Dehghani, D. Salz, M. Lucic, M. Tschannen, A. Nagrani,
H.Hu,M.Joshi,B.Pang,C.Montgomery,P.Pietrzyk,M.Ritter,A.Piergiovanni,M.Minderer,
F.Pavetic,A.Waters,G.Li,I.Alabdulmohsin,L.Beyer,J.Amelot,K.Lee,A.P.Steiner,Y.Li,
D.Keysers,A.Arnab,Y.Xu,K.Rong,A.Kolesnikov,M.Seyedhosseini,A.Angelova,X.Zhai,
N.Houlsby,andR.Soricut. Pali-x: Onscalingupamultilingualvisionandlanguagemodel,
2023.
[10] D.Driess,F.Xia,M.S.Sajjadi,C.Lynch,A.Chowdhery,B.Ichter,A.Wahid,J.Tompson,
Q.Vuong,T.Yu,etal. Palm-e: Anembodiedmultimodallanguagemodel. arXiv:2303.03378,
2023.
[11] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida,
J. Altenschmidt, S. Altman, S. Anadkat, et al. Gpt-4 technical report. arXiv preprint
arXiv:2303.08774,2023.
[12] H.Touvron,L.Martin,K.Stone,P.Albert,A.Almahairi,Y.Babaei,N.Bashlykov,S.Batra,
P.Bhargava,S.Bhosale,etal. Llama2: Openfoundationandfine-tunedchatmodels. arXiv
preprintarXiv:2307.09288,2023.
[13] Y.Bai,X.Geng,K.Mangalam,A.Bar,A.Yuille,T.Darrell,J.Malik,andA.A.Efros. Sequen-
tialmodelingenablesscalablelearningforlargevisionmodels.arXivpreprintarXiv:2312.00785,
2023.
[14] M.J.Kim,K.Pertsch,S.Karamcheti,T.Xiao,A.Balakrishna,S.Nair,R.Rafailov,E.Foster,
G. Lam, P. Sanketi, Q. Vuong, T. Kollar, B. Burchfiel, R. Tedrake, D. Sadigh, S. Levine,
P.Liang,andC.Finn. Openvla: Anopen-sourcevision-language-actionmodel,2024. URL
https://arxiv.org/abs/2406.09246.
11[15] OctoModelTeam,D.Ghosh,H.Walke,K.Pertsch,K.Black,O.Mees,S.Dasari,J.Hejna,
C. Xu, J. Luo, T. Kreiman, Y. Tan, P. Sanketi, Q. Vuong, T. Xiao, D. Sadigh, C. Finn, and
S.Levine. Octo: Anopen-sourcegeneralistrobotpolicy. InProceedingsofRobotics: Science
andSystems,Delft,Netherlands,2024.
[16] A.Depierre, E.Dellandre´a, andL.Chen. Jacquard: Alarge scaledatasetforroboticgrasp
detection. In 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems
(IROS),pages3511–3516.IEEE,2018.
[17] D.Kalashnikov,A.Irpan,P.Pastor,J.Ibarz,A.Herzog,E.Jang,D.Quillen,E.Holly,M.Kalakr-
ishnan, V. Vanhoucke, et al. Scalable deep reinforcement learning for vision-based robotic
manipulation. InCoRL,2018.
[18] S.Levine,P.Pastor,A.Krizhevsky,J.Ibarz,andD.Quillen. Learninghand-eyecoordination
forroboticgraspingwithdeeplearningandlarge-scaledatacollection. IJRR,2018.
[19] C. Eppner, A. Mousavian, and D. Fox. ACRONYM: A large-scale grasp dataset based on
simulation. In2021IEEEInt.Conf.onRoboticsandAutomation,ICRA,2020.
[20] N.M.M.Shafiullah,A.Rai,H.Etukuru,Y.Liu,I.Misra,S.Chintala,andL.Pinto. Onbringing
robotshome,2023.
[21] H.-S.Fang,H.Fang,Z.Tang,J.Liu,J.Wang,H.Zhu,andC.Lu. RH20T:Aroboticdatasetfor
learningdiverseskillsinone-shot. InRSS2023WorkshoponLearningforTaskandMotion
Planning,2023.
[22] F. Ebert, Y. Yang, K. Schmeckpeper, B. Bucher, G. Georgakis, K. Daniilidis, C. Finn, and
S.Levine. Bridgedata: Boostinggeneralizationofroboticskillswithcross-domaindatasets.
arXiv:2109.13396,2021.
[23] H.Walke,K.Black,A.Lee,M.J.Kim,M.Du,C.Zheng,T.Zhao,P.Hansen-Estruch,Q.Vuong,
A.He,V.Myers,K.Fang,C.Finn,andS.Levine. Bridgedatav2: Adatasetforrobotlearning
atscale,2023.
[24] S.Nair,A.Rajeswaran,V.Kumar,C.Finn,andA.Gupta.R3m:Auniversalvisualrepresentation
forrobotmanipulation. arXiv:2203.12601,2022.
[25] T.Xiao,I.Radosavovic,T.Darrell,andJ.Malik. Maskedvisualpre-trainingformotorcontrol.
arXiv:2203.06173,2022.
[26] Y. J. Ma, S. Sodhani, D. Jayaraman, O. Bastani, V. Kumar, and A. Zhang. Vip: Towards
universal visual reward and representation via value-implicit pre-training. arXiv preprint
arXiv:2210.00030,2022.
[27] I.Radosavovic,T.Xiao,S.James,P.Abbeel,J.Malik,andT.Darrell. Real-worldrobotlearning
withmaskedvisualpre-training. arXiv:2210.03109,2022.
[28] D.A.Pomerleau. Alvinn: Anautonomouslandvehicleinaneuralnetwork. InD.Touretzky,
editor,NeurIPS,volume1.Morgan-Kaufmann,1988.
[29] B. D. Argall, S. Chernova, M. Veloso, and B. Browning. A survey of robot learning from
demonstration. Roboticsandautonomoussystems,57(5):469–483,2009.
[30] S.Levine,C.Finn,T.Darrell,andP.Abbeel. End-to-endtrainingofdeepvisuomotorpolicies.
JMLR,2016.
[31] P. R. Florence, C. Lynch, A. Zeng, O. Ramirez, A. Wahid, L. Downs, A. S. Wong, J. Lee,
I.Mordatch,andJ.Tompson. Implicitbehavioralcloning. InCoRL,2021.
12[32] H.Ha,P.Florence,andS.Song. Scalingupanddistillingdown: Language-guidedrobotskill
acquisition. InConferenceonRobotLearning,pages3766–3777.PMLR,2023.
[33] T.B.Brown,B.Mann,N.Ryder,M.Subbiah,J.Kaplan,P.Dhariwal,A.Neelakantan,P.Shyam,
G.Sastry,A.Askell,etal. Languagemodelsarefew-shotlearners. NeurIPS,2020.
[34] H.Liu,C.Li,Q.Wu,andY.J.Lee. Visualinstructiontuning. InNeurIPS,2023.
[35] I. Radosavovic, B. Shi, L. Fu, K. Goldberg, T. Darrell, and J. Malik. Robot learning with
sensorimotorpre-training. arXiv:2306.10007,2023.
[36] I.Radosavovic,B.Zhang,B.Shi,J.Rajasegaran,S.Kamat,T.Darrell,K.Sreenath,andJ.Malik.
Humanoidlocomotionasnexttokenprediction,2024.
[37] E.Jang,A.Irpan,M.Khansari,D.Kappler,F.Ebert,C.Lynch,S.Levine,andC.Finn. Bc-z:
Zero-shottaskgeneralizationwithroboticimitationlearning. InConferenceonRobotLearning,
2022.
[38] Y.Jiang,A.Gupta,Z.Zhang,G.Wang,Y.Dou,Y.Chen,L.Fei-Fei,A.Anandkumar,Y.Zhu,
and L. Fan. VIMA: General robot manipulation with multimodal prompts. International
ConferenceonMachineLearning(ICML),2023.
[39] S.Reed,K.Zolna,E.Parisotto,S.G.Colmenarejo,A.Novikov,G.Barth-Maron,M.Gimenez,
Y.Sulsky,J.Kay,J.T.Springenberg,etal. Ageneralistagent. arXivpreprintarXiv:2205.06175,
2022.
[40] E. Collaboration, A. O’Neill, A. Rehman, A. Maddukuri, A. Gupta, A. Padalkar, A. Lee,
A. Pooley, A. Gupta, A. Mandlekar, A. Jain, A. Tung, A. Bewley, A. Herzog, A. Irpan,
A.Khazatsky,A.Rai,A.Gupta,A.Wang,A.Kolobov,A.Singh,A.Garg,A.Kembhavi,A.Xie,
A.Brohan,A.Raffin,A.Sharma,A.Yavary,A.Jain,A.Balakrishna,A.Wahid,B.Burgess-
Limerick,B.Kim,B.Scho¨lkopf,B.Wulfe,B.Ichter,C.Lu,C.Xu,C.Le,C.Finn,C.Wang,
C. Xu, C. Chi, C. Huang, C. Chan, C. Agia, C. Pan, C. Fu, C. Devin, D. Xu, D. Morton,
D.Driess,D.Chen,D.Pathak,D.Shah,D.Bu¨chler,D.Jayaraman,D.Kalashnikov,D.Sadigh,
E. Johns, E. Foster, F. Liu, F. Ceola, F. Xia, F. Zhao, F. V. Frujeri, F. Stulp, G. Zhou, G. S.
Sukhatme,G.Salhotra,G.Yan,G.Feng,G.Schiavi,G.Berseth,G.Kahn,G.Wang,H.Su,
H.-S.Fang,H.Shi,H.Bao,H.B.Amor,H.I.Christensen,H.Furuta,H.Walke,H.Fang,H.Ha,
I. Mordatch, I. Radosavovic, I. Leal, J. Liang, J. Abou-Chakra, J. Kim, J. Drake, J. Peters,
J.Schneider,J.Hsu,J.Bohg,J.Bingham,J.Wu,J.Gao,J.Hu,J.Wu,J.Wu,J.Sun,J.Luo,
J.Gu,J.Tan,J.Oh,J.Wu,J.Lu,J.Yang,J.Malik,J.Silve´rio,J.Hejna,J.Booher,J.Tompson,
J. Yang, J. Salvador, J. J. Lim, J. Han, K. Wang, K. Rao, K. Pertsch, K. Hausman, K. Go,
K.Gopalakrishnan,K.Goldberg,K.Byrne,K.Oslund,K.Kawaharazuka,K.Black,K.Lin,
K. Zhang, K. Ehsani, K. Lekkala, K. Ellis, K. Rana, K. Srinivasan, K. Fang, K. P. Singh,
K.-H. Zeng, K. Hatch, K. Hsu, L. Itti, L. Y. Chen, L. Pinto, L. Fei-Fei, L. Tan, L. J. Fan,
L.Ott,L.Lee,L.Weihs,M.Chen,M.Lepert,M.Memmel,M.Tomizuka,M.Itkina,M.G.
Castro,M.Spero,M.Du,M.Ahn,M.C.Yip,M.Zhang,M.Ding,M.Heo,M.K.Srirama,
M. Sharma, M. J. Kim, N. Kanazawa, N. Hansen, N. Heess, N. J. Joshi, N. Suenderhauf,
N. Liu, N. D. Palo, N. M. M. Shafiullah, O. Mees, O. Kroemer, O. Bastani, P. R. Sanketi,
P. T. Miller, P. Yin, P. Wohlhart, P. Xu, P. D. Fagan, P. Mitrano, P. Sermanet, P. Abbeel,
P.Sundaresan,Q.Chen,Q.Vuong,R.Rafailov,R.Tian,R.Doshi,R.Mart’in-Mart’in,R.Baijal,
R.Scalise,R.Hendrix,R.Lin,R.Qian,R.Zhang,R.Mendonca,R.Shah,R.Hoque,R.Julian,
S.Bustamante,S.Kirmani,S.Levine,S.Lin,S.Moore,S.Bahl,S.Dass,S.Sonawani,S.Song,
S. Xu, S. Haldar, S. Karamcheti, S. Adebola, S. Guist, S. Nasiriany, S. Schaal, S. Welker,
S. Tian, S. Ramamoorthy, S. Dasari, S. Belkhale, S. Park, S. Nair, S. Mirchandani, T. Osa,
T.Gupta,T.Harada,T.Matsushima,T.Xiao,T.Kollar,T.Yu,T.Ding,T.Davchev,T.Z.Zhao,
T.Armstrong,T.Darrell,T.Chung,V.Jain,V.Vanhoucke,W.Zhan,W.Zhou,W.Burgard,
X.Chen, X.Chen, X.Wang, X.Zhu, X.Geng, X.Liu, X.Liangwei, X.Li, Y.Pang, Y.Lu,
Y. J. Ma, Y. Kim, Y. Chebotar, Y. Zhou, Y. Zhu, Y. Wu, Y. Xu, Y. Wang, Y. Bisk, Y. Cho,
13Y.Lee,Y.Cui,Y.Cao,Y.-H.Wu,Y.Tang,Y.Zhu,Y.Zhang,Y.Jiang,Y.Li,Y.Li,Y.Iwasawa,
Y.Matsuo,Z.Ma,Z.Xu,Z.J.Cui,Z.Zhang,Z.Fu,andZ.Lin. Openx-embodiment: Robotic
learningdatasetsandrt-xmodels,2024.
[41] C.Finn,P.Abbeel,andS.Levine. Model-agnosticmeta-learningforfastadaptationofdeep
networks. InInternationalconferenceonmachinelearning,pages1126–1135.PMLR,2017.
[42] C. Finn, T. Yu, T. Zhang, P. Abbeel, and S. Levine. One-shot visual imitation learning via
meta-learning. InConferenceonrobotlearning,pages357–368.PMLR,2017.
[43] M.Xu,Y.Lu,Y.Shen,S.Zhang,D.Zhao,andC.Gan. Hyper-decisiontransformerforefficient
onlinepolicyadaptation. arXivpreprintarXiv:2304.08487,2023.
[44] Z.Mandi,F.Liu,K.Lee,andP.Abbeel. Towardsmoregeneralizableone-shotvisualimitation
learning,2022. URLhttps://arxiv.org/abs/2110.13423.
[45] E. Valassakis, G. Papagiannis, N. Di Palo, and E. Johns. Demonstrate once, imitate imme-
diately(dome): Learningvisualservoingforone-shotimitationlearning. In2022IEEE/RSJ
InternationalConferenceonIntelligentRobotsandSystems(IROS),pages8614–8621.IEEE,
2022.
[46] N.DiPaloandE.Johns. Keypointactiontokensenablein-contextimitationlearninginrobotics.
arXivpreprintarXiv:2403.19578,2024.
[47] V.Jain,M.Attarian,N.J.Joshi,A.Wahid,D.Driess,Q.Vuong,P.R.Sanketi,P.Sermanet,
S. Welker, C. Chan, et al. Vid2robot: End-to-end video-conditioned policy learning with
cross-attentiontransformers. arXivpreprintarXiv:2403.12943,2024.
[48] Y.Duan,M.Andrychowicz,B.Stadie,O.JonathanHo,J.Schneider,I.Sutskever,P.Abbeel,
and W. Zaremba. One-shot imitation learning. Advances in neural information processing
systems,30,2017.
[49] M.Xu,Y.Shen,S.Zhang,Y.Lu,D.Zhao,J.Tenenbaum,andC.Gan. Promptingdecision
transformerforfew-shotpolicygeneralization. Ininternationalconferenceonmachinelearning,
pages24631–24645.PMLR,2022.
[50] A. Khazatsky, K. Pertsch, S. Nair, A. Balakrishna, S. Dasari, S. Karamcheti, S. Nasiriany,
M. K. Srirama, L. Y. Chen, K. Ellis, P. D. Fagan, J. Hejna, M. Itkina, M. Lepert, Y. J. Ma,
P.T.Miller,J.Wu,S.Belkhale,S.Dass,H.Ha,A.Jain,A.Lee,Y.Lee,M.Memmel,S.Park,
I.Radosavovic, K.Wang, A.Zhan, K.Black, C.Chi, K.B.Hatch, S.Lin, J.Lu, J.Mercat,
A.Rehman,P.R.Sanketi,A.Sharma,C.Simpson,Q.Vuong,H.R.Walke,B.Wulfe,T.Xiao,
J.H.Yang,A.Yavary,T.Z.Zhao,C.Agia,R.Baijal,M.G.Castro,D.Chen,Q.Chen,T.Chung,
J.Drake,E.P.Foster,J.Gao,D.A.Herrera,M.Heo,K.Hsu,J.Hu,D.Jackson,C.Le,Y.Li,
K. Lin, R. Lin, Z. Ma, A. Maddukuri, S. Mirchandani, D. Morton, T. Nguyen, A. O’Neill,
R. Scalise, D. Seale, V. Son, S. Tian, E. Tran, A. E. Wang, Y. Wu, A. Xie, J. Yang, P. Yin,
Y.Zhang,O.Bastani,G.Berseth,J.Bohg,K.Goldberg,A.Gupta,A.Gupta,D.Jayaraman,
J.J.Lim,J.Malik,R.Mart´ın-Mart´ın,S.Ramamoorthy,D.Sadigh,S.Song,J.Wu,M.C.Yip,
Y.Zhu,T.Kollar,S.Levine,andC.Finn. Droid: Alarge-scalein-the-wildrobotmanipulation
dataset,2024.
[51] A.Majumdar,K.Yadav,S.Arnaud,Y.J.Ma,C.Chen,S.Silwal,A.Jain,V.-P.Berges,P.Abbeel,
J.Malik,D.Batra,Y.Lin,O.Maksymets,A.Rajeswaran,andF.Meier. Whereareweinthe
searchforanartificialvisualcortexforembodiedintelligence? arXivpreprintarXiv:2303.18240,
2023.
[52] S.Chen,R.Garcia,I.Laptev,andC.Schmid. Sugar: Pre-training3dvisualrepresentationsfor
robotics. arXivpreprintarXiv:2404.01491,2024.
14[53] A.Dosovitskiy,L.Beyer,A.Kolesnikov,D.Weissenborn,X.Zhai,T.Unterthiner,M.Dehghani,
M.Minderer,G.Heigold,S.Gelly,etal. Animageisworth16x16words: Transformersfor
imagerecognitionatscale. InICLR,2020.
[54] J.Deng,W.Dong,R.Socher,L.-J.Li,K.Li,andL.Fei-Fei.Imagenet:Alarge-scalehierarchical
imagedatabase. InCVPR,2009.
[55] L.Fu,L.Lian,R.Wang,B.Shi,X.Wang,A.Yala,T.Darrell,A.A.Efros,andK.Goldberg.
Rethinking patch dependence for masked autoencoders. arXiv preprint arXiv:2401.14391,
2024.
[56] J.Lee,Y.Lee,J.Kim,A.Kosiorek,S.Choi,andY.W.Teh. Settransformer: Aframeworkfor
attention-basedpermutation-invariantneuralnetworks. InInternationalconferenceonmachine
learning,pages3744–3753.PMLR,2019.
[57] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and
I.Polosukhin. Attentionisallyouneed. InNeurIPS,2017.
[58] J.Han,R.Zhang,W.Shao,P.Gao,P.Xu,H.Xiao,K.Zhang,C.Liu,S.Wen,Z.Guo,X.Lu,
S.Ren,Y.Wen,X.Chen,X.Yue,H.Li,andY.Qiao.Imagebind-llm:Multi-modalityinstruction
tuning,2023.
[59] L.Fu,G.Datta,H.Huang,W.C.-H.Panitch,J.Drake,J.Ortiz,M.Mukadam,M.Lambeta,
R.Calandra,andK.Goldberg. Atouch,vision,andlanguagedatasetformultimodalalignment.
arXivpreprintarXiv:2402.13232,2024.
[60] S.Mirchandani,F.Xia,P.Florence,B.Ichter,D.Driess,M.G.Arenas,K.Rao,D.Sadigh,and
A.Zeng. Largelanguagemodelsasgeneralpatternmachines,2023.
[61] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen. Lora:
Low-rankadaptationoflargelanguagemodels. arXivpreprintarXiv:2106.09685,2021.
[62] W.-L.Chiang,Z.Li,Z.Lin,Y.Sheng,Z.Wu,H.Zhang,L.Zheng,S.Zhuang,Y.Zhuang,J.E.
Gonzalez,etal. Vicuna: Anopen-sourcechatbotimpressinggpt-4with90%*chatgptquality.
Seehttps://vicuna.lmsys.org(accessed14April2023),2(3):6,2023.
[63] H.Liu,C.Li,Y.Li,andY.J.Lee. Improvedbaselineswithvisualinstructiontuning,2023.
[64] W.Dai,J.Li,D.Li,A.M.H.Tiong,J.Zhao,W.Wang,B.Li,P.N.Fung,andS.Hoi.Instructblip:
Towardsgeneral-purposevision-languagemodelswithinstructiontuning. AdvancesinNeural
InformationProcessingSystems,36,2024.
158 SupplementaryMaterial
8.1 SceneIllustrations
Weprovideanillustrationsontheprompttrajectoriesandtestscenesforthepickuptheblackdog
and place in the pink bowl task in Figure 5. As mentioned in Section 5, we collected 3 types of
prompttrajectoriesandtestICRTon5tiersofscenesthataredifferentfromthescenesintheprompt
trajectories.
Figure5:Illustrationsoftheprompttrajectories(top)andtestscenes(bottom)forthepickuptheblackdog
andplaceinthepinkbowltask.Threeprompttrajectoriesofdifferenttypesarecollected.Thetestscenesare
differentfromallprompttrajectoriesand5tiersofsceneswithdifferentnumberofdistractorsareconsidered.
8.2 AblationStudies
Inthissection,weprovideadditionalablationexperimentsonafewcoredesignchoicesanddifferent
promptingstrategies.
8.2.1 RepeatabilityExperiments
WeconductexperimentstoevaluatetherepeatabilityoftheperformanceofICRT.Weconductapick
uptheblackdogandplaceinthepinkbowltaskandapokebluespongetaskfor5rollouts,where
eachrolloutcontains5trialsasinSection5,resultingatotalof25trials. Wecalculatetheaverage
andthestandarddeviationofthesuccessrate. ResultsareshowninTable5. ThelowstdfromTable5
suggeststhattheICRTcanreliablyachievethetask.
Task PickandPlaceBlockDoginPinkBowl PokeBlueSponge
SuccessRateAve.±Std. 60%±0.5% 88%±3.2%
Table5:Repeatabilityexperimentsforapickandplacetaskandapokingtask. Eachtaskisconductedby5
rolloutsandeachrolloutcontains5trials,resultingatotalof25trials.
16PromptType NoDistractor OneDistractor DistractorPlacement TwoPrompts ThreePrompts
SuccessRate 60% 80% 70% 80% 80%
Table6:Experimentsondifferentprompttypesonapickupblackdogandplaceinthepinkbowltask.Thefirst
threecolumnsareresultsforasingleprompttrajectoryofdifferenttypes,whilethelasttwocolumnsarethatfor
usingtwoandthreeprompts.Successratesarecalculatedover5trialsforeachexperiment.
8.2.2 PromptTrajectories
Weconductexperimentsondifferentprompttypestoevaluatetheeffectofdifferentprompttrajectories
ontaskperformance. Weconsiderthetaskofpickingupablackdogandplacinginapinkbowl. We
havethreeprompttrajectoriesofdifferenttypes: onewithnodistractors,onewithonedistractorand
onewithonedistractorplacement,asshowninAppendixFigure5top. Allthreepromptstrajectories
arecollectedbyhumanteleopratingtherobot. Theobjectlocationsandtheplacementlocationsattest
timearedifferentfromthatinallthreeprompts. AsinSection5,foreachprompttype,weconduct
thetaskwith5trialsasshowninAppendixFigure5bottom. Theaveragesuccessratesarereported
inTable6. Weconductexperimentswithoneprompttrajectoryofdifferenttypes(thefirstthree
columnsinTable6),twoprompttrajectoriesandthreeprompttrajectories. Allprompttypesresultin
similarperformance,indicatingICRTisnotsensitivetotheprompttrajectorytypes. Wehypothesize
thisisbecauseduringthetraining,ICRThasseendifferenttypesandnumbersofprompts.
8.2.3 UnseenPrimitives
Task GraspandDroptheToyTiger GraspandDroptheBlueSponge PutBlueSpongetoRightofToyTiger
SuccessRate 40% 80% 80%
Table7:Experimentsonthreetasksusingtwounseenprimitives.Successratesarecalculatedover5trialsfor
eachexperiment.
WeevaluatethegeneralizationcapabilityofICRTonprimitivesthatareunseenduringthetraining
butresemblethetrainingprimitives. Weconsidertwosuchunseenprimitives: graspanddropan
objectandputobjectAtotherightofobjectB.Weconsiderthreetasks: graspanddropatoytiger,
graspanddropabluesponge(unseenobjectsduringtraining)andputthebluespongetotherightof
thetoytiger. AsinSection5,weconduct5tiersforeachtask. Experimentresultsaresummarizedin
Table7,whereICRTshowsdecentsuccessrateonallthreetasks,suggestingthatICRTcangeneralize
tosomeunseenprimitivesthatresemblethetrainingprimitives.
8.2.4 Co-training
FortrainingICRT,weopttoseparatethetrainingintotwostages: apre-trainingphasewherethe
modelispre-trainedontheDROIDdataset[50],andafine-tuningphasewherethemodelistrained
ontheICIL-MTdataset. Inthisablation,weexperimentwithwhetherthesetwocanbecombined
intoasinglestage,wherethepolicyisend-to-endtrainedwithDROIDandICIL-MT.Tobalancethe
twodatasets,wefirstcalculatethemediannumberoftrajectoriespertaskacrossthetwodatasets,
thenforeachepoch,sampleeachtaskwiththemediannumberoftrajectories. Thisallowseachtask
tobeequallyrepresentedineachepoch. Wetrainthemodelforthesamenumberofepochsasfor
ICRTfine-tuningandreporttheresultsinTable8andTable9. Theresultsindicatethatthemodel
doesnotconvergeasquicklyinthecombinedstageandfailstorespondtopromptsandcomplete
taskseffectively. Wehypothesizetworeasonsforthis: firstly,thedatasetisheavilybiasedtowards
DROID,whichcontains200taskscomparedtoonly26tasksinICIL-MT,makingitdifficultforthe
modeltolearnthetasksaseffectivelyasintheseparatestagetraining. Futureworkscananalyzethe
datamixtureandhowtotrainwithlarge-scaledatasetsmoreeffectively.
PickandPlace
PickObject YellowCube YellowCube BlueBear Radish BlackDog BlueSponge
PlaceLocation BlackBowl GreyBowl PinkBowl GreyBowl PinkBowl SilverPot
ICRT(Co-train) 10% 0% 10% 0% 40% 20%
ICRT 60% 50% 80% 50% 60% 90%
Table8:Ablationonco-trainingwithDROID[50]forpickupandplacetasks.
17Poke
PokeObject Radish RedCube GreyDog BlackCube PinkBowl BlueSponge
ICRT(Co-train) 0% 0% 0% 0% 0% 0%
ICRT 100% 100% 80% 80% 100% 100%
Table9:Ablationonco-trainingwithDROID[50]forpokingtasks.
8.3 Hyperparameters
Weprovidethehyperparametersforboththepre-trainingandfine-tuningphaseinTable10andTa-
ble11.
Config Value
optimizer AdamW
baselearningrate 1e-3
learningrateschedule cosinedecay
batchsize 64
weightdecay 0.05
optimizermomentum β ,β =0.9,0.999
1 2
warmupepoch 0.5
totalepochs 4
proprioceptionnoise 0.005
actionnoise 0
sequencelength 512
brightnessaugmentation 0.1
contrastaugmentation 0.2
numactionprediction 16
Table10:Pre-trainingHyperparameters
Config Value
optimizer AdamW
baselearningrate 5e-4
learningrateschedule cosinedecay
batchsize 64
weightdecay 0.01
optimizermomentum β ,β =0.9,0.999
1 2
warmupepoch 1.25
totalepochs 125
proprioceptionnoise 0.005
actionnoise 0
sequencelength 512
brightnessaugmentation 0.1
contrastaugmentation 0.2
numactionprediction 16
Table11:FinetuningHyperparameters
8.4 Parameterization
ProprioceptionTheproprioceptionspaceisparameterizedbytheabsoluteendeffectortranslation
(x, y, z), a 6DoF rotation vector, and a continuous end-effector gripper state. This results in a
10-dimensionalproprioceptionrepresentation. The6DoFrotationvectorisflattenedfromtheSO(3)
rotation’smatrix’sfirsttworows.
Action We use delta end effector pose as the action parameterization. At each prediction step,
themodelpredictstactions. GivenabsoluteendeffectoractiontransformsinT ,T ,··· ,T ina
1 2 t
18trajectoryandthecurrentend-effectorposeT ,wedefinetherelativetransformsthatthemodelneeds
ee
topredictasT−1T ,T−1T ,···T−1T . Wethenappendthecontinuousabsolutegripperpositionto
ee 1 ee 2 ee t
eachdeltaaction. Similartoproprioception,wepresentthedeltaactionbytherelativeendeffector
translationanda6DoFrotation. Thisresultsina10-dimensionalactionrepresentation. Whenrolling
out the predicted actions, in addition to temporal ensembling [2], we also use receding horizon
control[1],andselectanactionhorizonof10steps.
8.5 SystemInformation
Allmodelsaretrainedon4NVIDIAA10080GBGPUs. ICRTpre-trainingonDROIDtakes56
minutesandfine-tuneingonICRT-MTtakes18hours. ICRT-Llama7Btakesroughly28hoursto
finetune. WereporttheinferencespeedofICRTandICRT-Llama2inTable12averagedover100
steps. AlltestsareperformedonaworkstationwithNVIDIARTX3090TiandInteli5-12400Fwith
64GBmemory. Wefindthatusingtheproposedformulation,whichcanleveragetheKVcache,we
canrunICRT-Llama2at10Hznaively.
InferenceFrequency
ICRT 39.6Hz
ICRT-Llama2 10.7Hz
Table12:InferencefrequencyofICRT,averagedover100steps.
19