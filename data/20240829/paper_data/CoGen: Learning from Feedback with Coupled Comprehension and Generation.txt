COGEN: Learning from Feedback
with Coupled Comprehension and Generation
MustafaOmerGul and YoavArtzi
DepartmentofComputerScienceandCornellTech,CornellUniversity
{momergul, yoav}@cs.cornell.edu
Abstract
A B C D E
Systems with both language comprehension
F G H I J
andgenerationcapabilitiescanbenefitfromthe
tightconnectionbetweenthetwo. Thiswork
studies coupling comprehension and genera- Target: C
Person with raised foot
tion with focus on continually learning from
interactionwithusers. Weproposetechniques
totightlyintegratethetwocapabilitiesforboth
Image C
learning and inference. We situate our stud-
iesintwo-playerreferencegames,anddeploy Target: E
Swan facing right
various models for thousands of interactions
with human users, while learning from inter-
action feedback signals. We show dramatic
Image E
improvementsinperformanceovertime,with
comprehension-generationcouplingleadingto
performance improvements up to 26% in ab- Figure1: Illustrationofourreferencegameinteraction
solutetermsandupto17%higheraccuracies scenarioinvolvingaspeakerandlistener. Eachgame
comparedtoanon-coupledsystem. Ouranaly- includesasingleturn. Speakersareassignedatarget
sisalsoshowscouplinghassubstantialqualita- image and write a description such that their partner
tiveimpactonthesystem’slanguage,making can guess the image from the description. The game
itsignificantlymorehuman-like. succeeds if the listener guesses correctly. We deploy
ourmodels(graybot)asspeakertointeractwithhuman
1 Introduction listeners(top)orviceversa(bottom).
Language comprehension and generation are
Westudythedynamicsofthiscouplinginacon-
closely related processes. Indeed, observations
tinual learning1 setting, where trends in learning
such as the ability to finish incomplete partner
andbehaviorcanbeobservedovertime. Wedesign
utterances in dialogue (Clark and Wilkes-Gibbs,
aninteractionscenariowheremodelscantakeboth
1986; Howes et al., 2011), as well as neuroscien-
listener(comprehension)andspeaker(generation)
tificevidence(Pausetal.,1996;Opitzetal.,2003;
roles,andreceivefeedbackwhileinteractingwith
Menenti et al., 2011) have led to integrated ac-
human partners. We couple comprehension and
countsofcomprehensionandgenerationincogni-
generation through several mechanisms, and ob-
tivescience(PickeringandGarrod,2013;Pickering
servetheimpactthiscouplinghasonthelong-term
andGambi,2018),whereprocessesrelatedtogen-
dynamicsofperformanceandlanguage.
erationareactiveduringcomprehensionandvice
Weinstantiatecomprehensionandgenerationas
versa. This suggests a potential for coupling the
thelistenerandspeakerrolesofatwo-playerrefer-
twoincomputationalsystems,andcreatingavirtu-
encegame(KraussandWeinheimer,1964;Clark
ouscycle,wheretheimprovementofonecapability
andWilkes-Gibbs,1986)involvingabstractvisual
driveslearningandperformanceintheother. This
stimuli(Jietal.,2022),whichremainchallenging
is particularly compelling in systems that contin-
ually learn and improve through interaction with
1Continuallearningisattimesusedtodescribescenarios
users,wherethedynamicsbetweenthetwocapa-
wheremodelsareadaptedtonewtasks.Weuseitinthesense
bilitiesplayoutovertime. ofimprovingamodelonitsoriginaltaskovertime.
1
4202
guA
82
]LC.sc[
1v29951.8042:viXraA B C D E F G H I J
Target: C Person with Target: E
raised foot Swan facing right
Image C Multi-task Bandit
Training
Target: E
Person with raised foot Swan facing right
Coupled Interaction
Learning Signals
Image C
Image E
Figure2: Illustrationofourcontinuallearningscenariowithcoupledcomprehensionandgeneration. Theprocess
alternatesbetweeninteractionswithhumanpartnersinareferencegame,andtrainingusinglearningsignalsfrom
the interactions. The model performs both the generation (left) and comprehension (right) tasks, while jointly
reasoningovertheotherrole(thoughtbubbles). Trainingleveragesfeedbackfortherolethemodelperformsaswell
astheopposingrole. Followingeachroundoftraining,were-deploytheupdatedmodelandrepeattheprocess.
for state-of-the-art vision-language models (Fig- forthousandsofinteractionswithhumanpartners
ure1). Wedeployasinglemodelthatcantakeboth inacontrolledstudy. Ourfocusistoobserveboth
roles. The process alternates between the model performance and language trends over time. Our
interacting with human partners, and training to coupledapproachshowsdramaticandfastperfor-
improvebothcomprehensionandgenerationcapa- mance gains, overall improving by 19.48% for
bilitiesbasedonfeedbackfromtheinteractions. comprehensionand26.07%forgeneration,inab-
We couple comprehension and generation soluteterms. Attheconclusionofourdeployment,
through two strategies: (a) at inference-time via thecoupledapproachoutperformsthenon-coupled
ajointinferenceprocessthatincorporatestheop- baselineby14.80%forcomprehensionand17.10%
posingrole,and(b)attrainingtimebygenerating for generation. Furthermore, coupling results in
examplesandrewardsforeachrolefromfeedback greaterdataefficiency,withthefullsystemstillout-
on performance in that role as well as the oppos- performing this baseline with less than one-third
ing role. Figure 2 illustrates the deployment and the number of human interactions. We observe
coupling mechanisms. The combination of these couplingdramaticallyinfluencesthegeneratedlan-
strategiescreatesavirtuouscyclethatevolvesover guage, with the coupled approach exhibiting a
time,asthesystemcontinuallytrainsandimproves. largereffectivevocabularyandgreateralignment
Asonecapabilityimproves(e.g.,comprehension), withhumanlanguageaccordingtobothlinguistic
themodel’sperformanceontheopposingcapabil- measuressuchasutterancelengthandautomated
ity (e.g., generation) also improves via the joint metricssuchasMAUVE(Pillutlaetal.,2021). Our
inferenceprocedure. This,inturn,leadstobetter code, data, and experiment logs are available at
interactions,andthefeedbackthemodelreceives https://github.com/lil-lab/cogen.
changesasitscapabilitiesadvanceanditsfailure
modeschange. Thecouplingoffeedbacksignals 2 InteractionScenarioandOverview
viathetrainingdataqualitativelychangesthetrain-
Westudythecouplingofcomprehensionandgen-
ingbeyondasimpleincreaseintheamountofdata.
eration by training and deploying an agent that
Whereas a generation system that trains on feed-
interactswithhumanusers,andcontinuallylearns
backisonlyexposedtoitsownlanguage,thecou-
fromtheseinteractions. Thisallowsustoobserve
pledsystemiscontinuallyexposedtoastreamof
howtheinterplaybetweencomprehensionandgen-
new human language. This can enable the sys-
erationevolvesovertime,andwhatthelong-term
temtoexpanditsgenerationabilitiesandmakethe
effectsofcouplingthetwoprocessesare.
languagemoresimilartohumans,beyondsimply
refiningittomaximizeinteractionperformance. InteractionScenario Weuseareferencegame
Weconductextensiveexperiments,concurrently asourinteractionscenario(Figure1). Eachgame
deployingabaselineandmultiplemodelvariants involvestwoplayers: aspeakerandlistener. Both
2participantsarepresentedasetofabstracttangram formstheevaluationofthemodelsofar. Weevalu-
imagesascontextI = {I ,...,I }. Eachpartici- atethecomprehensionperformanceofthemodel
1 N
pantobservestheimagesinadifferentorder. The fromitstargetselectionaccuracyasalistener. We
speakerisgivenatargetI ∈ I,andgeneratesan evaluategenerationastheaccuracyofthehuman
t
utteranceu,withthegoalofallowingthelistener listener in selecting the target given the model’s
to pick the target I from the set of the images. generateddescriptionwheninthespeakerrole. We
t
The listener then makes a choice. An interaction alsostudythelinguistictrendsofthemodel’sgener-
succeedsifthelistenerpickstheintendedtarget. ationsovertimetobetterunderstandthedynamics
Reference games have been extensively used createdbycouplingcomprehensionandgeneration.
in research, including in NLP (e.g., Andreas and Thisincludesanalyzingitssimilaritytohumanlan-
Klein, 2016; Ji et al., 2022) and cognitive sci- guageanditslinguisticproperties.
ence(e.g.,KraussandWeinheimer,1964;Rosen-
berg and Cohen, 1964; Clark and Wilkes-Gibbs, 3 ContinualLearning
1986;Hawkinsetal.,2023),andprovideabalance
We combine the continual learning approaches
between complexity and research feasibility: (a)
of Kojima et al. (2021) (generation) and Suhr
theinteractionincludesbothgenerationandcom-
and Artzi (2024) (comprehension). Both ap-
prehension; (b) they are relatively accessible for
proaches map feedback to rewards, treat learn-
crowdsourcing workers; (c) they are well scoped
ing as a contextual bandit problem, and use RE-
so learning is feasible without excessive data re-
INFORCE (Williams, 1992), a relatively simple
quirements; and (d) success is easy to measure.
policygradientalgorithm. Weadoptthesedesign
The tangram shapes we use have been shown to
choices. Akeydifferenceofourprocessisthatwe
elicitrichlinguisticbehavior,bothfromlisteners
combinethecomprehensionandgenerationobjec-
and speakers (Schober and Clark, 1989; Ji et al.,
tivestotrainasinglemodel.
2022). Theyalsoremainchallengingforcontem-
Deploymentandlearningareinterleaved. Each
porarymodels. Ourmodels’initialperformanceis
round ρ starts with deploying the model parame-
atleast33.1%belowhumanaccuracy(Section6).
terized by θ to collect interactions with humans.
ρ
Deployment We deploy our model to interact Werecordfeedbacksignalsfromtheseinteractions.
withhumansinrounds. Eachroundincludesapre- Upon collecting a set of interactions, we re-train
determinednumberofinteractions,eachwiththe themodelgivenalldatacollectedsofartoestimate
modeltakingoneoftheroles(speakerorlistener) newparametersθ . Themodelisthendeployed
ρ+1
and the human participant taking the other. We forthenextround,andtheprocesscontinues.
derivefeedbacksignalsfromtheinteractions,con-
structtrainingexamples,andtrainourmodel. Fol- 3.1 FeedbackCollection
lowingtraining,were-deployforthenextround.
Feedbackcollectionispartofthemodelinteracting
Inference and Learning We use IDEFICS2- withhumanpartners(i.e.,thesystemdeployment),
8B(Laurençonetal.,2024)asourmodel,anauto- anddiffersdependingonthemodel’srole. Asthe
regressiveLLMthatcanalsotakeimagesaspart listener,themodelisgivencontextI andahuman-
ofitsinput. Themodelisparameterizedbyθ. Asa generated utterance u and predicts the index of
speaker(generation),themodelcomputesaproba- thetargetimagetˆ= argmax P (t|I,u;θ ). The
t l ρ
bilitydistributionP (u|I,t;θ)overdescriptionsu game then indicates if the selection was correct
s
ofthetargett ∈ {1,...,N}inthegivencontextI. ornot,andterminates. Wetreatthisindicationas
Asalistener(comprehension),itcomputesadistri- feedback,2 anddirectlymapittoabinaryrewardto
butionP (t|I,u;θ)overtargettselections,given createacomprehensiondatapoint: (I,u,tˆ,r),with
l
thecontextI andadescriptionutteranceu. Both r = 1upongamesuccessandr = −1otherwise.
utterancesandtargetselectionsaregeneratedvia Likewise,asthespeaker,themodelsamplesanut-
aconventionalauto-regressiveprocess. Following teranceuˆ ∼ P (u|I,t;θ )givencontextI andtar-
s ρ
eachdeploymentround,wetrainthemodelusing
2Insingle-turnreferencegames,suchasinourscenario,
allthefeedbackdatacollectedsofarbytreatingthe
tasksuccessandfeedbackarethesame,sowedonotsolicit
feedbackasrewardsforcontextualbanditlearning.
explicitfeedback. However,ourapproachisdesignedtobe
applicabletosettingswherefeedbackandtasksuccessdonot
Evaluation Our main evaluation is conducted
collapse to be the same, such as the setting considered by
throughinteractionwithhumans,whereeachround Kojimaetal.(2021)andSuhrandArtzi(2024).
3getimageindext. Thegameindicationofsuccess 4 CouplingComprehensionand
providesthefeedback,resultinginagenerationdat- Generation
apoint: (I,uˆ,t,r) with r ∈ {−1,1} accordingly.
Eachroundresultsintwodatasets: D andD Wecouplecomprehensionandproductionduring
l,ρ s,ρ
for comprehension and generation. In both, dat- bothlearningandinference. Wealsouseonemodel
apoints constitute model output produced during forbothtasks,creatingacouplingattheparameter
interaction,andarewardforit. Thisisincontrast level,whichiscommonincontemporarymethods,
to supervised learning, where datapoints include partiallyduetohighmemoryneeds.
outputannotations,orhumanfeedbackasusedin
RLHF,wheredatapointsarepairwisepreferences 4.1 LearningwithDataSharing
drawnfromexternalannotators.
We convert comprehension datapoints to genera-
3.2 Learning tiondatapoints,andviceversa,tofullyutilizethe
We estimate the next round’s model parameters data models are exposed to in interactions. For
θ byre-trainingfromtheinitialweights(i.e.,the example,considerthecaseofanagentintherole
ρ+1
originalIDEFICS2weights). Thecomprehension of a listener. If the speaker partner generates the
trainingdatasetisaunionofallcollectedfeedback utterancethetargetisaswanfacingrightandthe
datasofarD = (cid:83)ρ D . Theproductiontask listener correctly guesses the target image (as in
l,≤ρ i=1 l,i
datasetD issimilarlydefined. Figure 2), the listener does not only receive posi-
s,≤ρ
Weframelearningasacontextualbanditprob- tivefeedbackfortheirguess,butalsocanlearnthat
lem with a multi-task additive objective combin- a swan facing right is a valid description for the
ingthecomprehensionandgenerationcomponents. currentcontext-targetpair.
WeoptimizewithaREINFORCE-stylepolicygra- GivendatasetsforcomprehensionD andgen-
l,ρ
dientalgorithm(Williams,1992). Thischoicefol- erationD collectedatroundρ,weexpandboth:
s,ρ
lows prior work (Kojima et al., 2021; Suhr and
Artzi,2024),andismotivatedbythesimplicityof D = D ∪{(I,uˆ,t,r) ∈ D | r = 1}, (3)
l,ρ l,ρ s,ρ
REINFORCE,criticalinasettingwherehumans
D = D ∪{(I,u,tˆ,r) ∈ D | r = 1}. (4)
arepartoftheiterativelearningprocess.3 Thegra- s,ρ s,ρ l,ρ
dient for a comprehension example (I,u,tˆ,r) ∼
D collectedatroundmis: We only convert positively labeled feedback
l,≤ρ
(r = 1), because we generally find positive re-
∆ = c r∇logP (tˆ|I,u;θ) , (1)
l l l wardstobemorereliable. Anegativerewardfora
wherec isthecasedinversepropensityscore(IPS) generatedutterancecouldbebecausetheutterance
l
is incorrect or ambiguous, or the human listener
coefficient introduced by Kojima et al. (2021) to
mitigatetheeffectofnegativeexamples(i.e.,r = madeamistake. Thelistenertaskisessentiallyclas-
−1)allowingforunboundedloss: sification. Creatingacomprehensionexamplewith
negativerewardfromsuchanexampleindicatesto
(cid:40) P(tˆ|I,u;θ)
l ifr = −1 the model the utterance is a valid description for
c
l
= P l(tˆ|I,u;θm) , (2)
anothertarget. Thisisamisleadingsignal,andin
1 else
earlypilotstudieswefounditnottobehelpful,so
where P (tˆ|I,u;θ ) is the probability of the tar- weonlyconvertexampleswithpositivereward.
l m
get tˆwhen it was sampled during the interaction Animportantresultofthisprocessisintroduc-
atroundm. Withoutthiscoefficient,negativeex- ing human language into the training data of the
amples (i.e., r = −1) can dominate the loss and speaker model. Generally, if a generating model
destabilizelearningastheirprobabilitiesdecrease, learnsfromfeedbackonly(Kojimaetal.,2021),it
because lim P(·)→0logP l(·) = −∞. The coeffi- isonlyexposedtolanguageithasgenerated. This
l
cientc l decreasestheimportanceofsuchexamples can lead to its language drifting from human lan-
astheirprobabilitydecreases. Thegradient∆ s for guage,evenifitsaccuracyandlegibilitytohuman
generation datapoints (I,uˆ,t,r) is identical, ex- partnersincrease. Takingadvantageofhumanutter-
ceptusingthegenerationdistributionP s(uˆ|I,t;θ). ancesforthepurposeofgenerationtrainingopens
up this closed system. We further discuss this in
3Moregenerally,Ahmadianetal.(2024)recentlyshowed
REINFORCEcanmatchmoremodernmethods,suchasPPO. ourresultsandanalysis(Section6).
44.2 JointInference fromJietal.(2022)toensurevisualsimilaritybe-
tween images in each context and increase task
WecouplethetwodistributionsP andP during
l s
difficulty. AppendixBprovidesfurtherdetails.
inferencebysamplingfromonedistribution(i.e.,
P inthecaseofcomprehension)andthenre-rank Model and Initialization We fine-tune the
l
withaweightedgeometricmeanofthetwodistribu- instruction-tunedIDEFICS2-8B(Laurençonetal.,
tions. Theweightcontrollingthegeometricmean 2024). The tasks are delineated via prompting.
isahyper-parameter: λ forgenerationandλ for Training hyperparameters are kept fixed through-
s l
comprehension. Inthecaseofcomprehension,the outdifferentcontinuallearningroundsandsystem
jointprobabilitydistributionis: variants. Forsystemswithjointinference,weset
λ = 0.5andλ = 0. AppendixAdetailsprompt
L S
Pj(t|I,u;θ) = (5) design,hyperparameters,andtraining. Beforethe
l
P l(t|I,u;θ)λ lP s(u|I,t;θ)1−λ l firstroundofinteractions,weinitializethemodel
,
(cid:80)N t′=1P l(t′|I,u;θ)λ lP s(u|I,t′;θ)1−λ l b suy cfi cen se s- ft uu ln hin ug mI aD n-E hF uI mC aS n2 gw amith esa
.
Wsm ea all lss oet ado df1 th0 i4
s
whereN isthenumberoftargets. Thejointgenera- datatothelaterroundsofre-training,byassigning
tiondistributionPj(u|I,t;θ)isdefinedinasimilar alltheseexamplesarewardof1. Weuse280suc-
s
fashion,butwiththeλ hyperparameter. Enumer- cessfulhuman-humangamesasavalidationsetfor
s
modelselectionthroughoutourexperiments.
atingallpossibleutterancesforthenormalization
ofthejointgenerationdistributionisintractable,so SystemVariants Werefertoourproposedsys-
wesamplekutterancesfromP (u|I,t;θ)andsum temcouplingcomprehensionandgenerationwith
s
over them to compute the normalization. In the jointinferenceanddatasharingas FULL. Wecom-
caseofcomprehension,wecancomputethejoint pare against three other systems: ablations with-
distributionexactlybecausethenumberofoutputs out data sharing (NO-DS; Section 4.1) or joint
issmall(i.e.,10targets). However,ifthenumber inference(NO-JI;Section4.2),andabaselinethat
oftargetswasintractablylarge,thesameapproxi- usesneither(BASELINE). Weadditionallycollect
mationcouldalsobeperformedforcomprehension. human-human interaction data (HUMAN) to con-
Inpractice,weobservethemultiplicativegenera- textualizeperformanceovertimerelativetohuman
tiondistributiontoskewinferenceheavilytowards performance. Wealsousethishuman-humandata
short utterances when doing joint inference, and forlanguageanalysis.
findλ = 0tobethebestcombinationforthejoint
s Deployment Weconductfourroundsofdeploy-
generationdistribution(Section5). Althoughthis
ment,includinginteractionswithhumanpartners
eliminates the term P from the joint probability,
s and learning. All interactions for each round are
P isstillinfluentialasthesourceofsamples.
s collectedconcurrentlyinarandomizedexperiment.
This joint formulation is similar to a rational
We collect an equal number of interactions for
speech act model (RSA; Goodman and Frank,
thespeakerandlistenerrolesforeachsystemand
2016) with a single level of recursion. RSA is a
round. Wecollect2,000interactionsforeachrole
modelofpragmaticreasoning,andhasbeenevalu-
foreachsysteminthefirstround,andincreasethe
atedextensivelyinreferencegames(Cohn-Gordon
numberby500eachround,asthemarginalbenefit
etal.,2018;McDowellandGoodman,2019). We of more examples decreases as the data grows.4
analyzethispropertyforourspeakermodelinSec-
Becausedatasharingisnotapplicableforthefirst
tion 6.2. Our approximation of the joint speaker
round,the FULL and NO-DS,andthe NO-JI and
distributionisinspiredbysimilarapproachesthat
BASELINEsystemsareidenticalonthefirstround.
wereappliedtoRSA(Friedetal.,2018a).
We deploy our systems to interact with human
workersonMTurk,atatotalcostof$12,980USD.
5 ExperimentalSetup
AppendixEprovidescrowdsourcingdetails.
Game Construction We construct reference Evaluation Ateachround,weevaluatecompre-
game contexts using the KILOGRAM dataset (Ji hension performance from interactions in the lis-
et al., 2022) of 1,016 abstract tangram shapes. tenerroleusingthetargetselectionaccuracy. We
Each context comprises 10 images drawn from
4Comprehensionandgenerationperformanceareidentical
this dataset. We use a CLIP model (Radford
for the HUMAN system, so we collect half the number of
etal.,2021)finetunedonKILOGRAMannotations interactionsforthatsystem.
5FULL NO-JI NO-DS FULL NO-JI NO-DS
BASELINE HUMAN BASELINE HUMAN
Cumulative#Interactions Cumulative#Interactions 90 90
0 2000 4500 7500 0 2000 4500 7500
90 90 80 80
80 80 70 70
70 70 60 60
60 60 50 50
50
θ1
50
θ1
40 40
1 2 3 4 1 2 3 4
40 40
Round Round
1 2 3 4 1 2 3 4
Figure4: Modelcomprehensionandgenerationaccu-
Round Round
racywhenthespeakerutteranceincludes( )anddoes
Figure3: Comprehensionandgenerationperformance notinclude( )wordsforspatialreasoning.
forsystemvariantsacrossfourroundsofdeployment,
with 95% confidence intervals.5 The top x-axis indi-
catesthetotalnumberofinteractionscollectedforarole startswithalreadyhigherperformancecomparedto
uptothedeploymentround. Couplingcomprehension variantswithoutjointinference. Withcomprehen-
andgenerationleadsto FULL outperformingallabla- sion, NO-JI (42.64→66.86%) shows the biggest
tionsthroughout. delta (24.22%). Coupling dramatically increases
learning sample efficiency: FULL at the second
round already performs better than BASELINE at
evaluate a system’s generation performance (i.e.,
theendofstudy,eventhoughittrainedonlessthan
as a speaker) as the accuracy of the human inter-
onethirdofthedataBASELINEhasseenattheend.
locutor’stargetselections. For HUMAN, compre-
Overall,thegapinperformancebetweenFULL
hensionandgenerationperformanceareidentical.
and BASELINE only increases over time. For
6 ResultsandAnalysis comprehension, the gap widens 10.67→14.80%,
but it is much more dramatic for generation with
We focus on two broad questions: (a) does cou- 3.55→17.10%. Bothcouplingstrategiesplayarole
pling influence the rate of improvement on task inthiswideninggapinperformance,butbetween
performance (Section 6.1) and (b) does it lead to the two strategies the relation changes over time.
quantifiabledifferencesinthegeneratedlanguage Although NO-DS startswithhigherperformance
overtime(Section6.2). Overall,wefindtheanswer than NO-JI,theyareessentiallyequivalentatthe
tobothquestionsispositive,withstrongeffects. end,with NO-JIshowingatrendofoutperforming
NO-DS.Thismaybebecause NO-JI isexposedto
6.1 PerformanceAnalysis
moredatafromtheopposingrolewithdatasharing,
Figure3showsmodelperformanceovertime. All compensatingforthelackofjointinference.6
systems show dramatic improvement in perfor- User adaptation is an important potential con-
mance for both comprehension and generation. founder,potentiallyexplaininganyimprovements
Immediately, we observe significant effect from in system performance. During the final round,
joint inference, with FULL and NO-DS outper- wedeploytheinitial FULL modelinaconcurrent
forming NO-JI and BASELINE onthefirstround randomizeddeployment. Weobservethathuman
(53.31% vs. 42.64% comprehension, 52.00% vs. adaptationcannotexplainmodelimprovement,see-
48.45% generation). FULL achieves the highest ing very limited improvement due to adaptation:
performanceattheendofthestudy,withcompre- 0.42%and2.56%forcomprehensionandgenera-
hensionimproving53.31→72.79%(19.48%abso- tion(crossanddashedcurveinFigure3).
luteimprovement)andgeneration52.00→78.07% Duringdeployment,arecurringcomplaintfrom
(26.07% improvement). For generation, FULL workerswasaboutthemodels’inconsistentspatial
showsthebiggestperformancedelta,eventhoughit reasoning, echoing recent evaluations of vision-
5Confidenceintervalsarecomputedusingbootstrapsam- 6Figure8inAppendixD.2depictsdatasharing’simpact
pling,wheren=10,000. ontrainingsetsizeovertime.
6
ycaruccAeloRnoisneherpmoC
ycaruccAeloRnoitareneG
ycaruccAeloRnoisneherpmoC
ycaruccAeloRnoitareneGlanguagemodels(Kamathetal.,2023;Tongetal., pectedthiseffecttobelessstrongorevenreversed
2024a,b). We identified games where utterances once the system is exposed to human utterances,
involveawordrelatingtospatialreasoning.7 Fig- eitherthroughdatasharingorthroughjointinfer-
ure 4 shows a breakdown of performance trends ence with a comprehension model trained on hu-
togamesthatcontainspatialreasoningutterances man utterances. The decrease in the vocabulary
and games that do not. We see a clear difference sizeismuchsmallerforthecoupledvariants,and
between the two sets. Although models improve thesmallestforFULL,butitremainspresent. We
on utterances that contain spatial reasoning, they alsoplot,foreachround,howmanywordsamodel
perform worse on them throughout. During the addedtothecumulativesetofwordsitgenerated
finalround,weobservethatFULL’sperformance untilthatround(thirdpanel). Morenewwordsap-
nearsthatofhumansforgenerationwhennotusing pearforthecoupledvariantsthroughoutthestudy.
wordsforspatialreasoning. Allsystemsdisplayasignificantlylessrichvocab-
Couplingdemonstratesaverystrongeffect,both ulary compared to humans, leaving an important
on performance and language trends. Balancing directionforfuturework.
the utility of further rounds versus the high cost We use MAUVE (Pillutla et al., 2021), a
of each round, we ended the deployment after reference-lessgenerationevaluationmetric,toeval-
four rounds. Appendix D.1 discusses this deci- uate the similarity of each model’s language to
sion,andprovidesanextrapolationofperformance humanlanguage. Foreachroundandsystem,we
foronemoreround,showingacontinuationofthe computethemetricbetweenthemodel-andhuman-
observedtrends. producedutterancesforthatround. WeuseGPT2-
Large as the embedding model (Radford et al.,
6.2 LanguageAnalysis
2019), similar to Pillutla et al. (2021), and keep
We study trends in language use over time. thenumberofclustersfixedat200. Wefindcou-
Throughoutthissection,exceptthepragmaticrea- pling avoids the drift from human language the
soninganalysis,weeliminatefactorsthatcancom- BASELINE displays. The FULL system not only
plicatetheanalysisbygeneratingnewutterances does not stray further from human language, but
onthesamesetofcontext-targetpairsperroundfor actuallymovesclosertoitovertime. Datasharing
all systems. We randomly sample 2,000 context- isparticularlycritical,butthecombinationofjoint
targetpairsfromthehuman-humangamesforeach inferencefurtherhelpstoalignthemodellanguage
round,andgenerateutterancesforthemwitheach withhumanlanguage.
systemusingthesameinferenceprocessasduring Finally,webrieflylookintowhethercouplingaf-
deployment. Figure5plotstheobservedtrends.8 fectsthemodel’spragmaticreasoning. Inreference
We observe a decrease in utterance length for games,thepragmaticinformationistheimagesin
allvariants. Humansalsoshowadownwardtrend thecontextthatarenotthetarget. Aspeakerthat
inlength,likelyreflectingtheparticipantsbecom- employs pragmatic reasoning well will take into
ing experts and therefore more economical in account the other images so to help the speaker
their language. This is a known phenomenon in maketherightselectioninthespecificcontextthey
reference games (Krauss and Weinheimer, 1964; share(i.e.,thespeakerwillrefertopropertiesofthe
ClarkandWilkes-Gibbs,1986),andwasalsoob- target that specifically distinguish from the other
servedinothercollaborativescenarios(Effenberger images). Weoperationalizethisquestionbymea-
et al., 2021). FULL and NO-JI track the human suring the diversity of model descriptions for a
trends best, but generally generate shorter utter- specifictangramwithindifferentcontextsets. We
ancesthroughout. usetheShapeNamingDivergence(SND)metric,
Theeffectivevocabularyofallsystems,thatis introducedby(Jietal.,2022)tomeasurethediver-
thenumberofuniquewordsgeneratedfortheset sityofhumanannotationsforindividualtangrams.9
of context-target pairs, is also decreasing. This Roughlyspeaking, highSNDmeanshighlexical
has been observed in prior studies for generation diversitybetweenthedescriptionsofaspecificim-
systems that are exposed only to their output in age. Foreachsystemandeachround,wegenerate
continual learning (Kojima et al., 2021). We ex- utterances for every context-target pair observed
in all human-human games throughout continual
7AppendixC.1providesthesetofwordsweconsidered.
8ForallanalysisbutMAUVE,utterancesarelowercased
andtokenizedwithspaCy(Honnibaletal.,2020). 9WedescribeSNDinAppendixC.2.
7FULL NO-JI NO-DS BASELINE HUMAN
Mean Effective NewWords
UtteranceLength VocabularySize Added MAUVE MeanSND
1.5k 260
11 0.6 0.6
1.3k 210 0.5 0.5
10
1.1k 160 0.4 0.4
9
0.9k 110 0.3 0.3
8
0.7k 60 0.2 0.2
7 0.5k 10 0.1 0.1
1 2 3 4 1 2 3 4 1 2 3 1 2 3 4 1 2 3 4
Round Round Round Round Round
Figure 5: Language analysis plots, with 95% confidence intervals.11 Trends in utterance length mirror that of
humanswhenusingdatasharing(FULL and NO-JI). FULL possessesthehighesteffectivevocabularysizeand
producesthelargestnumberofnewwordseachround. TheFULLsystemadditionallyshowsanincreaseinMAUVE
scores(↑)overtimeandexhibitsthehighestSND(↑)throughout.
learning.10 We get 10.67 utterances per tangram ing (McDowell and Goodman, 2019) and infer-
on average. Figure 5 (right pane) shows mean ence (White et al., 2020). We use it for different
SNDacrossalltangramsforeachmodelandround. aims, as one of two strategies to couple compre-
Largely,weobserveBASELINE’spragmaticability hensionandgeneration. Liuetal.(2023)studied
tocollapseovertime. Datasharinghelpstosome theincorporationofjointinferenceforgeneration
degree. WhileweseeadecreaseinSNDovertime learning,whichisacomponentofourstudy,with
evenwhenusingjointinference,thistypeofcou- astaticmodellistener. Incontrast,westudylearn-
plingshowsmuchhigherSNDvaluesthroughout, ingdynamicsforbothcomprehensionandgenera-
indicatinggreaterdiversityofutterancesandhence tion,evaluatedatasharingasanadditionalcoupling
agreaterpragmaticeffect. Whilethiseffecttracks mechanism,anddeployforcontinuallearningwith
thevocabularysizetrendsinpractice,itisindepen- humans,whoconstitutenon-staticpartners.
dent, even if a diverse vocabulary is a necessary,
Continuallylearningfrominteractionswithhu-
butinsufficientcondition. Thatsaid,thisanalysis
man users has been studied in the context of in-
ofpragmaticreasoningisrudimentary,andfuture
struction generation (Kojima et al., 2021) and
in-depthanalysisisrequiredtoidentifytheexact
following (Suhr and Artzi, 2024), question an-
qualitiesofthisphenomenaandhowitcorrelates
swering (Gao et al., 2023), and ad-hoc adapta-
withsystemperformance.
tion(Hawkinsetal.,2020). Inourwork,continual
learning enables us to study long-term dynamics
7 RelatedWork
thatarisefromcouplingcomprehensionandgenera-
tion. Ourcontinuallearningsetupisdifferentfrom
Our joint inference strategy (Section 4.2) is tech-
the Reinforcement Learning from Human Feed-
nically based on approximations (Fried et al.,
back framework (RLHF; Ziegler et al., 2019) in
2018a,b) of the Rational Speech Acts frame-
relyingonbinarysignalsderivedfrominteractions
work(RSA;GoodmanandFrank,2016;Yuanetal.,
withusers,whileRLHFrequiresexternalannota-
2018),whichframespragmaticreasoningasare-
torsthatcompareoutputpairs.
cursiveprocessbetweenlistenerandspeakermod-
els. RSA has been studied extensively with the The reference game scenario has been exten-
focusofdevelopingmodelsthatreasonpragmati- sively used in cognitive studies as a prototyp-
cally(e.g.,Monroeetal.,2017;AndreasandKlein, ical, but simple interaction design (Rosenberg
2016), including through incorporation in learn- andCohen,1964;KraussandWeinheimer,1964).
It has been used to study convention forma-
10WecannotcomputeSNDforhumanparticipantsbecause
tion at dyadic (Clark and Wilkes-Gibbs, 1986;
ofinsufficientdataperround.
Wilkes-Gibbs and Clark, 1992) and population-
11Confidence intervals are computed over n = 10,000
randomsamplesof2,000context-targetpairsforeachround. levels(Hawkinsetal.,2023),anddemonstratecom-
8putationaltheoriesofpragmaticreasoning(Good- Thisqualifiesourfindings,bothwithregardtothe
manandFrank,2016;Cohn-Gordonetal.,2019), language choice and the impact of the culture of
among other behaviors. It has also been used to theparticipants. Thesearealsoimportantvariables
developcomputationalmethods,suchastoevalu- forfuturestudies.
atecontrastivecaptioning(Vedantametal.,2017; UnlikehowRLisusuallystudiedintheresearch
Ou et al., 2023) and abstract reasoning of vision- community,ourcontinuallearningprocessinvolves
languagemodels(Jietal.,2022). Thetangramim- humans in the loop. This entails restrictions in
agesweuse,abstractshapescomposedofthesame terms of time and cost. We opt for simplicity
setofsevenprimitives,likewisehaveextensiveuse and choose to train models with a REINFORCE-
asstimuliincognitivescience(ClarkandWilkes- style policy gradient algorithm (Williams, 1992)
Gibbs,1986;SchoberandClark,1989;Hortonand andretrainmodelsfromscratchonthecumulative
Gerrig, 2002). They also remain challenging for setofcollecteddatawitheachroundofcontinual
contemporarymodels(Jietal.,2022),makingthem learning. A more extensive (and costly) search
wellsuitedtodemonstratemodelimprovement. overmethodsmightimpactresults. Weleavethe
study of more complex RL algorithms, such as
8 Conclusion
PPO (Schulman et al., 2017), as well as differ-
entstrategiesforincorporatingdatafromprevious
We study the dynamics of coupling language
roundstofuturework.
comprehension and generation at inference- and
We invested significant effort and resources in
training-timethroughacontinuallearningsetting
running our study for a significant amount of in-
where an agent learns from interactions with hu-
teractionsandrounds. Whileweshowconsistent
mans. Couplinghassignificantimpactovertime,
trends, it is hard to predict trends at much larger
leading to improved agent performance, sample
scale(e.g.,thousandsofroundsormillionsofinter-
efficiency,andsimilaritytohumanlanguage.
actions). Thisisbeyondtheresourceavailablefor
Ourworkpointstomultipledirectionsforfuture
thisresearch. Thatsaid,eveniftrendschangedra-
work,includingcouplingtheprocessesthroughthe
maticallywithsuchalonghorizon,ourapproach
training objective in addition to data at training-
remains useful for faster learning (i.e., reduce re-
time,developingmoreefficientalternativestosam-
gret)intheearlylifeofthesystem.
plingutterancesduringjointinferenceforgenera-
tion,andthestudyofalternativeinteractionscenar-
EthicalConsiderations
ios,includingmulti-turnsettingswheredynamics
betweencomprehensionandgenerationcanaffect Ourworkstudieshowthecouplingofcomprehen-
aninteractionthroughoutitsduration. Scalingup sionandgenerationaffectsthedynamicsofperfor-
our approach and experimental setting to a real- manceandmodellanguage. Throughcouplingat
worlddeploymentfeaturingawiderrangeoftasks trainingtime,ourmodeltrainsonbothitsowngen-
andabroadersetoffeedbacksignals,suchasnat- erationsalongsidegenerationsitshumanpartners
urallanguagefeedback,constitutesaparticularly producedatinteractiontime. Anaiveimplementa-
importantdirection. tionofthisstrategyduringreal-worlddeployment
risks aligning model behavior with the biases of
Limitations its human interlocutors at best and exposing the
modeltoadversarialactorsatworst. Appropriate
Ourworkdoesnottouchonanimportantfactorin
guardrailsorfurtherresearchforselectingwhento
deployedsystems: theadditionofnewparticipants
applydatasharingshouldbeimplementedbefore
into the system. To simplify the crowdsourcing
deploymentisconsideredtoensuresafety.
setup,wekeepthesetofworkersfixedduringour
experiments. Thisdoesnotallowustoobservethe
Acknowledgements
effect of new participants joining the population
andtheimpactofthedatatheycreateinteracting ThisresearchwassupportedbyAROW911NF21-
withouragents. Thisisanimportantdirectionfor 1-0106,NSFundergrantNo. 1750499,agiftfrom
futurework. Whileourmethodsarenotspecifically Open Philanthropy, and a gift from Apple. We
designed for English, our study is only done in thanktheCornellNLPgroupandSiddharthaDatta
English. We restrict the language to English and fordiscussionandcomments;VivianChen,Gloria
recruitworkersfromEnglish-majoritylocalesonly. Geng, and Anne Wu for feedback on the crowd-
9sourcing pipeline; Vivian Chen and Gloria Geng ciationforComputationalLinguistics: HumanLan-
forsharingcodefordatavisualization;RonEliav guageTechnologies,Volume1(LongPapers),pages
1951–1963,NewOrleans,Louisiana.Associationfor
andAnyaJiforallowingustobuildontopoftheir
ComputationalLinguistics.
reference game interface; and the crowdsourcing
workersfortheirparticipation. Daniel Fried, Ronghang Hu, Volkan Cirik, Anna
Rohrbach,JacobAndreas,Louis-PhilippeMorency,
Taylor Berg-Kirkpatrick, Kate Saenko, Dan Klein,
andTrevorDarrell.2018b. Speaker-followermod-
References
elsforvision-and-languagenavigation. Advancesin
Arash Ahmadian, Chris Cremer, Matthias Gallé, NeuralInformationProcessingSystems,31.
MarziehFadaee,JuliaKreutzer,OlivierPietquin,Ah-
metÜstün,andSaraHooker.2024. Backtobasics: GeGao,Hung-TingChen,YoavArtzi,andEunsolChoi.
RevisitingREINFORCE-styleoptimizationforlearn- 2023. ContinuallyimprovingextractiveQAviahu-
ingfromhumanfeedbackinLLMs. InProceedings manfeedback. InProceedingsofthe2023Confer-
of the 62nd Annual Meeting of the Association for ence on Empirical Methods in Natural Language
ComputationalLinguistics(Volume1: LongPapers), Processing,pages406–423,Singapore.Association
pages12248–12267,Bangkok,Thailand.Association forComputationalLinguistics.
forComputationalLinguistics.
NoahDGoodmanandMichaelCFrank.2016. Prag-
Abdullah Almaatouq, Joshua Becker, James P maticlanguageinterpretationasprobabilisticinfer-
Houghton, Nicolas Paton, Duncan J Watts, and ence. Trendsincognitivesciences,20(11):818–829.
MarkEWhiting.2021. Empirica: avirtuallabfor
RobertHawkins,MinaeKwon,DorsaSadigh,andNoah
high-throughputmacro-levelexperiments. Behavior
Goodman.2020. Continualadaptationforefficient
ResearchMethods,53(5):2158–2171.
machinecommunication. InProceedingsofthe24th
JacobAndreasandDanKlein.2016. Reasoningabout Conference on Computational Natural Language
pragmaticswithneurallistenersandspeakers. InPro- Learning, pages 408–419, Online. Association for
ceedingsofthe2016ConferenceonEmpiricalMeth- ComputationalLinguistics.
ods in Natural Language Processing, pages 1173–
1182,Austin,Texas.AssociationforComputational RobertDHawkins,MichaelFranke,MichaelCFrank,
Linguistics. Adele E Goldberg, Kenny Smith, Thomas L Grif-
fiths, andNoahDGoodman.2023. Frompartners
HerbertHClarkandDeannaWilkes-Gibbs.1986. Re- topopulations: Ahierarchicalbayesianaccountof
ferringasacollaborativeprocess. Cognition,22(1):1– coordinationandconvention. PsychologicalReview,
39. 130(4):977.
Reuben Cohn-Gordon, Noah Goodman, and Christo- Matthew Honnibal, Ines Montani, Sofie Van Lan-
pherPotts.2018. Pragmaticallyinformativeimage deghem,andAdrianeBoyd.2020. spaCy: Industrial-
captioning with character-level inference. In Pro- strengthNaturalLanguageProcessinginPython.
ceedingsofthe2018ConferenceoftheNorthAmer-
icanChapteroftheAssociationforComputational WilliamSHortonandRichardJGerrig.2002. Speakers’
Linguistics: Human Language Technologies, Vol- experiences and audience design: Knowing when
ume2(ShortPapers),pages439–443,NewOrleans, andknowinghowtoadjustutterancestoaddressees.
Louisiana.AssociationforComputationalLinguis- JournalofMemoryandLanguage,47(4):589–606.
tics.
ChristineHowes,MatthewPurver,PatrickGTHealey,
Reuben Cohn-Gordon, Noah Goodman, and Christo- GregoryJMills,andEleniGregoromichelaki.2011.
pherPotts.2019. Anincrementaliteratedresponse On incrementality in dialogue: Evidence from
modelofpragmatics. InProceedingsoftheSociety compound contributions. Dialogue & Discourse,
forComputationinLinguistics(SCiL)2019,pages 2(1):279–311.
81–90.
EdwardJHu,yelongshen,PhillipWallis,ZeyuanAllen-
Anna Effenberger, Rhia Singh, Eva Yan, Alane Suhr, Zhu,YuanzhiLi,SheanWang,LuWang,andWeizhu
andYoavArtzi.2021. Analysisoflanguagechange Chen. 2022. LoRA: Low-rank adaptation of large
in collaborative instruction following. In Findings language models. In International Conference on
of the Association for Computational Linguistics: LearningRepresentations.
EMNLP 2021, pages 2803–2811, Punta Cana, Do-
minican Republic. Association for Computational Anya Ji, Noriyuki Kojima, Noah Rush, Alane Suhr,
Linguistics. Wai Keen Vong, Robert Hawkins, and Yoav Artzi.
2022. Abstractvisualreasoningwithtangramshapes.
Daniel Fried, Jacob Andreas, and Dan Klein. 2018a. InProceedingsofthe2022ConferenceonEmpirical
Unifiedpragmaticmodelsforgeneratingandfollow- MethodsinNaturalLanguageProcessing,pages582–
ing instructions. In Proceedings of the 2018 Con- 601,AbuDhabi,UnitedArabEmirates.Association
ferenceoftheNorthAmericanChapteroftheAsso- forComputationalLinguistics.
10AmitaKamath,JackHessel,andKai-WeiChang.2023. JiefuOu,BennoKrojer,andDanielFried.2023. Prag-
What’s“up”withvision-languagemodels? investi- maticinferencewithaCLIPlistenerforcontrastive
gatingtheirstrugglewithspatialreasoning. InPro- captioning. InFindingsoftheAssociationforCom-
ceedingsofthe2023ConferenceonEmpiricalMeth- putationalLinguistics: ACL2023,pages1904–1917,
ods in Natural Language Processing, pages 9161– Toronto,Canada.AssociationforComputationalLin-
9175,Singapore.AssociationforComputationalLin- guistics.
guistics.
TomášPaus,DavidWPerry,RobertJZatorre,KeithJ
Noriyuki Kojima, Alane Suhr, and Yoav Artzi. 2021. Worsley, and Alan C Evans. 1996. Modulation of
Continuallearningforgroundedinstructiongenera- cerebralbloodflowinthehumanauditorycortexdur-
tionbyobservinghumanfollowingbehavior. Trans- ingspeech: Roleofmotor-to-sensorydischarges. Eu-
actionsoftheAssociationforComputationalLinguis- ropeanJournalofNeuroscience,8(11):2236–2246.
tics,9:1303–1319.
MartinJPickeringandChiaraGambi.2018. Predicting
Robert M Krauss and Sidney Weinheimer. 1964. whilecomprehendinglanguage: Atheoryandreview.
Changes in reference phrases as a function of fre- Psychologicalbulletin,144(10):1002.
quencyofusageinsocialinteraction: Apreliminary
study. PsychonomicScience,1:113–114. MartinJPickeringandSimonGarrod.2013. Aninte-
gratedtheoryoflanguageproductionandcomprehen-
HugoLaurençon, LéoTronchon, MatthieuCord, and sion. Behavioralandbrainsciences,36(4):329–347.
Victor Sanh. 2024. What matters when build-
ing vision-language models? arXiv preprint KrishnaPillutla,SwabhaSwayamdipta,RowanZellers,
arXiv:2405.02246. JohnThickstun,SeanWelleck,YejinChoi,andZaid
Harchaoui. 2021. Mauve: Measuring the gap be-
Andy Liu, Hao Zhu, Emmy Liu, Yonatan Bisk, and tweenneuraltextandhumantextusingdivergence
GrahamNeubig.2023. Computationallanguageac- frontiers. AdvancesinNeuralInformationProcess-
quisitionwiththeoryofmind. InTheEleventhInter- ingSystems,34:4816–4828.
nationalConferenceonLearningRepresentations.
AlecRadford,JongWookKim,ChrisHallacy,Aditya
Ilya Loshchilov and Frank Hutter. 2019. Decoupled Ramesh,GabrielGoh,SandhiniAgarwal,GirishSas-
weightdecayregularization. InTheSeventhInterna- try, Amanda Askell, Pamela Mishkin, Jack Clark,
tionalConferenceonLearningRepresentations. etal.2021. Learningtransferablevisualmodelsfrom
naturallanguagesupervision. InInternationalconfer-
Bill McDowell and Noah Goodman. 2019. Learning enceonmachinelearning,pages8748–8763.PMLR.
fromomission. InProceedingsofthe57thAnnual
Meeting of the Association for Computational Lin- AlecRadford,JeffreyWu,RewonChild,DavidLuan,
guistics,pages619–628,Florence,Italy.Association DarioAmodei,IlyaSutskever,etal.2019. Language
forComputationalLinguistics. modelsareunsupervisedmultitasklearners. OpenAI
blog,1(8):9.
Laura Menenti, Sarah ME Gierhan, Katrien Segaert,
andPeterHagoort.2011. Sharedlanguage: overlap Seymour Rosenberg and Bertram D Cohen. 1964.
and segregation of the neuronal infrastructure for Speakers’ and listeners’ processes in a word-
speaking and listening revealed by functional mri. communication task. Science, 145(3637):1201–
Psychologicalscience,22(9):1173–1182. 1203.
Will Monroe, Robert X.D. Hawkins, Noah D. Good- MichaelFSchoberandHerbertHClark.1989. Under-
man,andChristopherPotts.2017. Colorsincontext: standingbyaddresseesandoverhearers. Cognitive
A pragmatic neural model for grounded language psychology,21(2):211–232.
understanding. TransactionsoftheAssociationfor
ComputationalLinguistics,5:325–338. John Schulman, Filip Wolski, Prafulla Dhariwal,
Alec Radford, and Oleg Klimov. 2017. Proxi-
Philipp Moritz, Robert Nishihara, Stephanie Wang, malpolicyoptimizationalgorithms. arXivpreprint
AlexeyTumanov,RichardLiaw,EricLiang,Melih arXiv:1707.06347.
Elibol,ZonghengYang,WilliamPaul,MichaelIJor-
dan,etal.2018. Ray: Adistributedframeworkfor AlaneSuhrandYoavArtzi.2024. Continuallearning
emerging{AI}applications. In13thUSENIXsym- forinstructionfollowingfromrealtimefeedback. Ad-
posiumonoperatingsystemsdesignandimplementa- vances in Neural Information Processing Systems,
tion(OSDI18),pages561–577. 36.
BOpitz,KMüller,ADFriederici,etal.2003. Phono- Shengbang Tong, Erik Jones, and Jacob Steinhardt.
logicalprocessingduringlanguageproduction: fmri 2024a. Mass-producingfailuresofmultimodalsys-
evidenceforasharedproduction-comprehensionnet- tems with language models. Advances in Neural
work. CognitiveBrainResearch,16(2):285–296. InformationProcessingSystems,36.
11ShengbangTong,ZhuangLiu,YuexiangZhai,YiMa, examples in the first epoch, with the proportion
Yann LeCun, and Saining Xie. 2024b. Eyes wide decliningafterwards.
shut? exploring the visual shortcomings of multi-
modalllms. InProceedingsoftheIEEE/CVFCon- A.2 StoppingCriterion
ferenceonComputerVisionandPatternRecognition
(CVPR),pages9568–9578. Eachmodelistrainedforamaximumof15epochs.
An epoch is a complete pass over data for the
RamakrishnaVedantam,SamyBengio,KevinMurphy,
comprehension task. We use patience stopping,
DeviParikh,andGalChechik.2017. Context-aware
captionsfromcontext-agnosticsupervision. InPro- ending training when model validation accuracy
ceedingsoftheIEEEConferenceonComputerVision for the comprehension task does not improve for
andPatternRecognition,pages251–260.
five epochs. For models with joint inference, we
JuliaWhite, JesseMu, andNoahD.Goodman.2020. computevalidationaccuracywiththejointlistener
Learningtoreferinformativelybyamortizingprag-
modelPj(t|I,u;θ),whileformodelswithoutjoint
l
maticreasoning. Preprint,arXiv:2006.00418. inference, we compute it with the base listener
DeannaWilkes-GibbsandHerbertHClark.1992. Coor- P l(t|I,u;θ). We exclusively use comprehension
dinatingbeliefsinconversation. Journalofmemory accuracy. Pilot experiments showed it correlates
andlanguage,31(2):183–194. wellwithdeploymentperformance.
Ronald J Williams. 1992. Simple statistical gradient-
A.3 HyperparameterSearch
followingalgorithmsforconnectionistreinforcement
learning. Machinelearning,8:229–256. Hyperparametersearchisdoneontheseedinitial-
izationdata,usingcomprehensionaccuracyonthe
AriannaYuan,WillMonroe,YuBai,andNateKushman.
2018. Understandingtherationalspeechactmodel. validationsetasthemetric. Wevarylearningrates
InCogSci. {1e − 5,5e − 5,1e − 4,2e − 4}, weight decay
{0,1e−3,1e−1},LoRAα{8,32}andadapter
DanielMZiegler,NisanStiennon,JeffreyWu,TomB
Brown, Alec Radford, Dario Amodei, Paul Chris- placements(onlykey-query-valueprojections;all
tiano, and Geoffrey Irving. 2019. Fine-tuning lan- forward projections; and all forward projections
guage models from human preferences. arXiv
exceptforthelanguagedecoder,whichusedkey-
preprintarXiv:1909.08593.
query-valueprojections)andpromptdesigns. The
A TrainingandInferenceDetails selectionofLoRAadaptersisthemostimportant
hyperparameter,showingastrongimpactonover-
A.1 TrainingHyperparameters
fittingatthedatascalesweworkin.
We use the instruction-tuned IDEFICS2-8B Theλ hyperparameterforthejointcomprehen-
l
model(Laurençonetal.,2024)forallourexperi- siondistributionistunedontheseeddatawiththe
ments,andoptimizewithAdamW(Loshchilovand hyperparametersdescribedonAppendixA.1. We
Hutter, 2019) with a learning rate of 0.0001 and savemodelcheckpointsforeachepochoftraining
aweightdecayof0.1. Eachgradientstepiscom- andinspectcomprehensionaccuracyvaluesonthe
putedoverindependentlysampledminibatchesof validationsetfordifferentsettingsofλ . Wefind
l
size32forcomprehensionandgenerationtasks. λ = 0.5consistentlyperformwell.
l
We use LoRA for finetuning (Hu et al., 2022), Wechooseλ bytrainingmodelswiththejoint
s
where r = 16 and α = 8. We apply adapters to inference strategy with λ = 0.5 and using the
l
all feedforward layers in the vision encoder, the hyperparametersandstoppingcriterionfromAp-
modality projection and the perceiver-resampler pendix A.1 and Appendix A.2. We sample ut-
block, but only to the key, query and value pro- terances on the validation set and inspect the re-
jections of the text decoder. We found applying rankingbehaviorofthejointgenerationdistribution
furtheradaptersforthetextdecodertoexacerbate
Pj(u|I,t;θ)withdifferentλ
values. Weobserve
s s
overfitting. We load and train models with BF16
thattheutterancethejointdistributionPj(u|I,t;θ)
s
precisiontoreducememoryandcomputecosts. rankedasthebestwasoftenequivalenttotheutter-
WeobservetheIPStermfornegativelyrewarded ancethebasegenerationdistributionP (u|I,t;θ)
s
examples(Section3.2)toinfrequentlyattainhigh ranked as the most likely. This skew towards the
valuesinearlyepochsduringpilotexperiments. To basegenerationdistributionisadditionallyexacer-
increasetrainingstability,wecliptheIPStermat batedwithlongertrainingtimes.
5. During our main experiment, clipping is acti- Todetermineλ inlightofthis, weprobehow
s
vatedfor2-3%ofnegativelyrewardedgeneration accurately the joint generation distribution could
12rankutterancesonthevalidationset. Specifically, ilarity scores between the first sampled tangrams
for each context-target pair on the validation set, and all other tangrams. The similarities are com-
wemeasurewhetherthedistributionPj(u|I,t;θ)
putedusingCLIP.
s
assigned higher probabilities to the ground-truth
C ExperimentDetails
utterance for that pair than distractor utterances
collected for other target images in that context.
C.1 SetofSpatialReasoningWords
We vary λ in [0,1] with increments of 0.01 and
s
We curate the set of words relating to spatial
find that λ = 0 achieved the best accuracy at
s
reasoning by parsing the set of all human and
selectingtheground-truthutterance.
model-generatedutterancesusingspaCywiththe
A.4 PromptDesign en_core_web_smpipeline(Honnibaletal.,2020).
We collect the set of all words marked with an
We use the same model (i.e., same architectures
ADP (adposition) part-of-speech tag, which pre-
andsameparameters)forcomprehensionandgen-
dominantlycontainedtermsforspatialreasoning
erationanddesignatewhichtaskthemodelshould
in our task, and manually filtered out the words
performthroughprompting. Figure6andFigure7
suchas“like”thatareirrelevanttospatialreason-
showthepromptsforcomprehensionandgenera-
ing. We then added words relating to notions of
tion.
“left”and“right,”whichwerenotcapturedunder
A.5 GenerationSamplingDetails theADPtag.
The full set of words we used was: ’from’,
Wesampleutterancesautoregressivelyusingatem-
’towards’, ’thru’, ’to’, ’through’, ’until’, ’next’,
peratureofτ = 0.7. Wesamplek = 10utterances
’above’, ’along’, ’about’, ’out’, ’inside’, ’be-
togeneratewiththejointinferenceprocedure. To
hind’, ’outside’, ’forward’, ’back’, ’around’, ’be-
isolatetheinfluenceofrerankingwiththecompre-
neath’,’atop’,’up’,’apart’,’near’,’at’,’below’,
hensionmodel,wealsosamplek = 10utterances
’into’,’onto’,’toward’,’past’,’upwards’,’before’,
whennotperformingjointinferenceandreturnthe
’within’,’against’,’between’,’beside’,’on’,’after’,
utterancewiththehighestprobability.
’by’, ’over’, ’across’, ’down’, ’opposite’, ’under-
A.6 ComputationalResources neath’,’in’,’under’,’left’,’leftward’,’leftwards’,
Each model is trained with a single GPU, RTX ’right’,’rightward’,’rightwards’.
A6000orNVIDIAA100. Hyperparametertuning
C.2 ShapeNamingDivergenceMetric
experimentstook100-200GPUhourstotal,while
WeanalyzepragmaticreasoningusingtheShape
trainingforthemaincontinuallearningexperiment
NamingDivergence(SND)metric(Jietal.,2022),
took approximately 225 GPU hours. For deploy-
whichmeasureshowmuchthenamingofindivid-
ment,ontheotherhand,Modelsaredeployedusing
ual tangrams varies across different annotations.
RTXA6000andV100GPUs,withRayforinfer-
We repurpose it to probe pragmatic reasoning by
enceparallelization(Moritzetal.,2018).
measuring how much a model’s description of a
B ContextConstruction giventangramvariesacrossdifferentcontexts. In-
steadofdescriptionsfromdifferentannotators,we
Eachreferencegameroundinvolvesacontextof
computeSNDoverdescriptionsofthattangramin
size N = 10 comprising 3 blocks (two of size
different contexts. This gives insight into the im-
3 and one of size 4) of visually similar tangrams.
pactofthecontext(i.e.,viapragmaticreasoning)
WeuseaCLIPmodel(Radfordetal.,2021)fine-
onthedescriptionoftheindividualtangram.
tuned by Ji et al. (2022) on annotations from the
KILOGRAM datasettoconstructthesesub-blocks. D AdditionalPerformanceAnalyses
The blocks increase the difficulty of the context,
D.1 EstimatingPerformanceonFuture
becauseelementswithineachblockhavehighvi-
Rounds
sual similarity, making both comprehension and
generationmorechallenging. The decisions of experiment length (i.e., in the
Eachsimilarityblockisconstructedbyrandomly number of rounds) requires to balance costs and
samplingatangram, andsamplingtherestofthe researchutility. Ourmainexperimentincludedfour
blockmembersfromallothertangrams. Thesam- roundsofdeploymentandlearning,whichwassuf-
plingisdoneusingadistributionofnormalizedsim- ficienttoanswerourresearchquestionsgiventhe
13ComprehensionPrompt:
[User]Youwillbepresentedwithasequenceof10imagesandacaptiondescribingexactlyoneofthem.Yourtaskistoguess
whichimagethecaptiondescribes. Image0: <img0>,Image1: <img1>,Image2: <img2>,Image3: <img3>,Image4:
<img4>,Image5: <img5>,Image6: <img6>,Image7: <img7>,Image8: <img8>,Image9: <img9>. Caption: <speaker
caption>.DoesthiscaptiondescribeImage0,1,2,3,4,5,6,7,8or9?
[Assistant]ThecaptiondescribesImage<targetimageindex>
Figure6: IDEFICS2comprehensionprompt. Thetargetimageindexisnotprovidedduringinferencetime.
GenerationPrompt:
[User]Youwillbepresentedwithasequenceof10imagesandbeassignedatargetimage.Yourtaskistoproduceacaption
foryourtargetimagesuchthatanyonecouldguesstheimagefromyourdescription. Image0:<img0>,Image1:<img1>,
Image2: <img2>,Image3: <img3>,Image4: <img4>,Image5: <img5>,Image6: <img6>,Image7: <img7>,Image8:
<img8>,Image9:<img9>.YourtargetisImage<imageindex>.Produceyourcaptionnow.
[Assistant]<caption>
Figure7: IDEFICS2generationprompt. Thecaptionisnotprovidedduringinferencetime.
dramatic differences between the systems. Our FULL NO-JI
datadoesallowustoestimateperformancetrends NO-DS BASELINE
foronemoreround,withoutcollectingadditional
data. Wetrainmodelsforafifthroundgivenallthe 12k 12k
interaction data collected in prior rounds, includ- 10k 10k
ingthelastroundofdeployment,whichprovided 8k 8k
the final performance numbers. We compute of-
6k 6k
flineestimateofcomprehensionperformanceusing
4k 4k
human-modelinteractionscollectedonthefourth
2k 2k
roundbythecontrolsystem(i.e.,theinitial FULL
0 0
model), which come from the same distribution
ofhumanutterancesandareunseenbymodelsin 1 2 3 4 1 2 3 4
Round Round
training.
Thisestimateindicatesthetrendsweobserveare Figure8: Numberoftrainingexamplesforcomprehen-
robust,andcontinueforatleastonemoreroundbe- sionandgenerationtasksacrossfourroundsofdeploy-
yondourexperiment. Comprehensionperformance ment. Theplotsaccountfordatapointsconvertedfrom
continuestoimproveforallmodels(FULL: 72.79 theopposingrolewhendatasharingisapplied.
→ 76.79%; NO-JI: 66.86 → 68.25%; NO-DS:
64.73→65.93%; BASELINE: 58.04→64.25%).
the opposing role increasing as the model perfor-
FULLstilloutperformsallothersystemsbyalarge
manceincreases.
margin. Importantly,theperformanceof FULLon
thesecondroundremainslargerthantheimproved E Crowdsourcing
performance of BASELINE (65.24% > 64.25%),
validatingourobservationthatcouplingboostsdata E.1 WorkerRecruitment
efficiency. Thisindicatesthatthepositiveimpacts
WerecruitworkerswithaminimumHIT(Human
thecouplingofcomprehensionandgenerationhas
IntelligenceTask)approvalrateof98%andatleast
onperformancetrendsarelikelytopersist.
1,000approvedHITs. Werestrictthepooltowork-
ers from English-majority locales (United States,
D.2 ImpactofDataSharingonTrainingSet
Canada,GreatBritain,Ireland,Australia,andNew
Size
Zealand). Workers complete a video tutorial and
Figure8showshowthenumberofdatapointsmod- aqualificationquiztoqualifyforourtasks.11 The
elstrainonforcomprehensionandgenerationtasks quizalsoincludesacceptingaconsentform. The
changeovertime. Couplingwithdatasharingleads
11The quiz may be found in our codebase. The
toastrongdataaugmentationeffectfor FULL and
videotutorialisaccessibleathttps://lil-lab.github.io/
NO-JI,withthenumberofdatapointssharedfrom tangrams-refgame-dev/.
14
selpmaxEgniniarTnoisneherpmoC
selpmaxEgniniarTnoitareneGconsentformdetailshowidentifiableinformation ers in the listener role are not revealed the target.
of workers (i.e.,AMT worker IDs) is encrypted, Wedothistomitigateworkeradaptationtomod-
how the collected data would be published, and elsandconventionformationthroughoutaHIT.If
benefits and risks from participating in the study. neither player makes a decision within the given
We recruit a total of 84 workers. This study was timeframe, the round is considered unsuccessful.
qualifiedasexemptbyCornellUniversity’sInstitu- The HIT terminates if an individual worker does
tionalReviewBoard. nottakeanactionfortwoconsecutiverounds. Fig-
Evenwiththequalificationprocess,workersthat ure9showstheHITintroduction,listenerrole,and
producedlow-effortresponsesorcolludedwithoth- speakerrole.
ers entered the worker pool. We further estimate
thetheeffectivenessofworkersviahuman-human
games. We collected a set of113 pilot games be-
E.4 DeploymentDetails
tweenhumans,whereattheendofeachHIT,play-
ers rated their satisfaction with their partner on a
Ineachdeployment,wegiveeachworkeraccessto
Likertscalefrom1–6. Weremovedworkerswith
anequalnumberofHITstouniformlysamplefrom
an average less than 4 from the pool and manu-
the worker pool. Within a given HIT, a worker
allyreviewedthegamesoftheremainingworkers.
plays40roundsofreferencegames,eitheragainst
Withthisprocess,werestrictedthepoolofwork-
a human or model partner. If playing against a
ers to a set of 50 experts, 41 of whom joined our
model,theworkerplaysagainsteachsystemvari-
finalexperiments. Wecollectedourinitialization
antanequalnumberoftimesandinarandomorder.
andvalidationdata, andperformedourcontinual
Duringthefinalroundofdeployment,weaddition-
learningexperimentwiththissetofexperts.
allyevaluatetheinitialFULLsystem,andtherefore
E.2 PaymentDetails increasethenumberofroundsperHITto50.
The HIT base pay is $0.60USD. For each round Throughout the execution of a HIT, players al-
ofreferencegamesplayedwithinaHIT,workers ternate between roles every 3–4 rounds. In each
receive a bonus of $0.125USD upon success or groupof3–4rounds,theunderlyingcontextiskept
$0.05USDuponfailure. Theestimatedhourlypay fixed,withthetargetschangingeachround. This
was$18.31USDforgamesbetweenhumans,and balances the cognitive load of observing a com-
$20.55USDforgamesbetweenhumansandmod- pletelynewcontextwhilepreventingworkersfrom
els at the final round. We set the base pay and beingabletoguesstargetsbasedonwhathasnot
bonuses through pilot studies among researchers beenmentionedyet. Ifaworkerisplayingagainst
andtunedthevaluesbasedonestimatesofhourly amodel,thesystemtheyareplayingagainstiskept
payduringpilotstudies. fixed within this group of 3–4 rounds. Workers
arenotrevealedwhethertheyareplayingagainsta
E.3 GameInterface
humanoramodel.
ThereferencegameinterfaceisbuiltusingtheEm-
Each HIT additionally includes an attention
pirica framework (Almaatouq et al., 2021). It in-
check round at a random position. The attention
cludesachatboxatthelefthandofthescreenand
checks are randomly sampled from a set of 100
the context tangrams at the center. When in the
manuallyannotatedcontext-targetpairs. Toensure
speakerrole,thetargetisindicatedtothespeaker
simplicity,wesamplethetargetsfromthebottom
with a black square. The speaker has 45 seconds
15th percentile of tangrams in terms of the SND
totypeandsendanutterancethroughthechatbox.
metric (indicating high annotator agreement for
Afterthespeakersendsamessage, thelisteneris
tangram naming within the KILOGRAM dataset)
given 15 additional seconds to make a selection.
andrestricttheremainingtangramsinthecontext
Eachroundlastsatmost60seconds. Thelistener
tothosewithaCLIPcosinesimilaritylessthan0.
makes a selection by clicking on a tangram im-
Therestoftheattentioncheckconstructionfollows
age. Ifsuccessful,thetargetflashesgreenforboth
theprocessoutlinedinAppendixB.
players. Upon failure, the target tangram flashes
redforthespeakerandthechosentangramflashes In practice, we did not disqualify any workers.
red for the listener. Workers in the speaker role Ourmaincontinuallearningexperimentspanned
are not revealed their partners’ choice and work- fromMay3rdtoMay21st.
15Figure 9: Top: the introduction screen shown upon accepting a HIT. Center: worker view in the listener role.
Bottom: workerviewinthespeakerrole.
F DataDetails by500eachround(Section5). Foreachroleand
eachsystem,wecollect2,000interactionsonround
F.1 InteractionsPerRound 1,2,500onround2,3,000onround3,and3,500
onround4.
Wecollect2,000interactionsforeachroleforeach
systeminthefirstround,andincreasethenumber
16F.2 DataRelease
We release all of the data collected during our
experiments alongside the code used to conduct
them. This includes the seed training and valida-
tionsetsof104and280successfulhuman-human
reference games as well as all of the interactions
collected during continual learning, comprising
10,811roundsofhuman-humanreferencegames,
and 43,442 and 43,492 rounds of human-model
referencegameswherethemodelisinthelistener
orspeakerroles. Wedonotincluderoundswhere
thehumanpartneridled.
Duringdatacollection,allworkerIDswereen-
cryptedwithMD5hashes. Workerinformationis
furtheranonymizedduringthereleasebymapping
eachIDhashtoanumericindex.
G LicensesofScientificArtifactsUsed
Our chosen model architecture, IDEFICS2-
8B (Laurençon et al., 2024), and the Ray library
haveopenlicenses(Apache2.0);therepositoryfor
MAUVE(Pillutlaetal.,2021)hasaGNUGeneral
PublicLicense;andspaCy(Honnibaletal.,2020)
hasanMITlicense.
17