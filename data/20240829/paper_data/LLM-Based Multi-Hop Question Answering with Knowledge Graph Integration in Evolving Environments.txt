LLM-Based Multi-Hop Question Answering with Knowledge Graph
Integration in Evolving Environments
♦ † † ♦
RuiruiChen ,WeifengJiang ,ChengweiQin ,IshaanSinghRawal ,
♦ ♦ ♣ ♦
ChestonTan ,DongkyuChoi ,BoXiong ,BoAi
♦
InstituteofHighPerformanceComputing(IHPC),
AgencyforScience,TechnologyandResearch(A*STAR)
1FusionopolisWay,#16-16Connexis,Singapore138632,RepublicofSingapore
† ♣
NanyangTechnologicalUniversity UniversityofStuttgart
Abstract Information evolves over time
Boris Rishi
TherapidobsolescenceofinformationinLarge British Prime Minister
Johnson Sunak
Language Models (LLMs) has driven the de-
velopment of various techniques to incorpo-
Who is married to the
rate new facts. However, existing methods 2-hop QA British Prime Minister?
for knowledge editing still face difficulties
withmulti-hopquestionsthatrequireaccurate
fact identification and sequential logical rea- Boris Carrie
Johnson Symonds head of
soning,particularlyamongnumerousfactup-  government
dates. To tackle these challenges, this paper UK
introduces Graph Memory-based Editing for ✔ spouse
Large Language Models (GMeLLo), a strait- Rishi Akshata
Sunak Murty forwardandeffectivemethodthatmergesthe
explicit knowledge representation of Knowl-
edge Graphs (KGs) with the linguistic flex- Figure 1: Multi-hop question answering in dynamic
ibility of LLMs. Beyond merely leveraging domains (Zhong et al., 2023). Dynamic nature of in-
LLMs for question answering, GMeLLo em- formation: Changesovertimemaytriggersubsequent
ploys these models to convert free-form lan- modifications. Forinstance,atransitionintheBritish
guageintostructuredqueriesandfacttriples, Prime Minister, such as from Boris Johnson to Rishi
facilitatingseamlessinteractionwithKGsfor Sunak,necessitatescorrespondingadjustments,likethe
rapidupdatesandprecisemulti-hopreasoning. changeintheBritishPrimeMinister’sspouse.
Our results show that GMeLLo significantly
surpasses current state-of-the-art knowledge
editingmethodsinthemulti-hopquestionan- As LLMs operate as black boxes, modifying
sweringbenchmark,MQuAKE,especiallyin one fact might inadvertently alter another, mak-
scenarioswithextensiveknowledgeedits. ingitchallengingtoguaranteeaccuraterevisions.
Inthispaper,weintroduceGMeLLo,aneffective
1 Introduction
approach designed to synergize the strengths of
AsthewidespreaddeploymentofLLMscontinues, LLMsandKGsinaddressingthemulti-hopques-
the imperative to keep their knowledge accurate tionansweringtaskafterknowledgeediting(Zhong
andup-to-date,withoutincurringextensiveretrain- etal.,2023). Anillustrativeexampleofourfocusis
ing costs, becomes increasingly evident (Sinitsin presentedinFigure 1. Followinganupdateregard-
et al., 2020). Several approaches have been pro- ingtheinformationoftheBritishPrimeMinister,
posedinpriorworkstoaddressthischallenge,with itbecomesevidentthatthecorrespondingspouse
somefocusingontheincrementalinjectionofnew informationshouldalsobemodified.
facts into language models (Rawat et al., 2020; AsdepictedinFigure 2,ourGMeLLocomprises
De Cao et al., 2021; Meng et al., 2022; Mitchell thefollowingkeysteps:
etal.,2022a). Interestingly,certainmethodologies
• WeutilizeLLMstotranslateeditedfactsen-
intheliteraturedivergefromtheconventionalpath
tencesintotriples,employingthesetriplesto
of updating model weights, opting instead for an
updatetheKGandensureitsinformationre-
innovative strategy involving the use of external
mainsuptodate.
memorytostoretheedits(Mitchelletal.,2022b;
Zhongetal.,2023). • Givenaquestion,weutilizeLLMstoextract
4202
guA
82
]LC.sc[
1v30951.8042:viXraitsrelationchain,encompassingtheprimary (Yinetal.,2016),free-formtext(Yangetal.,2018;
entityanditsconnectionswithotherunknown Welbl et al., 2018), or a heterogeneous combina-
entities. Afterpopulatingatemplate,wecon- tionofthesesources(Chenetal.,2020;Mavietal.,
verttherelationchainintoaformalqueryand 2022; Lei et al., 2023). With the development of
useittosearchtheupdatedKG. LLMs,prompt-basedmethodscombinedwithan
optionalretrievalmodulehavebecomeapopular
• In addition, we retrieve the most pertinent
approachforhandlingmulti-hopquestionanswer-
editedfactsbasedonthequestionandprompt
ing(Khattabetal.,2022;Pressetal.,2023;Zhong
LLMs to generate an answer in accordance
etal.,2023). Whilemostpreviousworksfocuson
withthesefacts.
a static information base, our approach targets a
dynamicdomain,accommodatingchangesinfacts.
• In instances where the answer provided by
theLLMconflictswiththatfromtheKG,we
2.2 KnowledgeEditing
prioritizetheanswerfromtheKGasthefinal
AshighlightedinYaoetal.(2023),twoparadigms
response.
existforeditingknowledge: modifyingmodelpa-
LLMs, trained on extensive sentence corpora rametersandpreservingmodelparameters.
(Brown et al., 2020; Rae et al., 2022; Chowdh-
2.2.1 ModifyingModelParameters
ery et al., 2023), are expected to encapsulate a
widerangeofcommonlyusedsentencestructures. Inthecaseofmodifyingmodelparameters,thiscan
As a result, they are invaluable tools for analyz- befurthercategorizedintometa-learningorlocate-
ingsentencesandextractingentitiesandrelations. and-edit approaches. Meta-learning methods, as
Once the correct relation chain and edited triples discussed in (De Cao et al., 2021; Mitchell et al.,
are obtained, using a formal query to interrogate 2022a), utilize a hyper network to learn the nec-
the KG in a Knowledge-based Question Answer- essaryadjustmentsforeditingLLMs. Thelocate-
ing(KBQA)(Cuietal.,2017)mannerensurespre- then-editparadigm,asdemonstratedin(Daietal.,
cision in the searching process. In cases where 2022; Meng et al., 2022, 2023; Li et al., 2023a;
KBQA fails, we still have LLMs for question Gupta et al., 2023; Zhang et al., 2024), involves
answering (QA) to ensure comprehensive cover- initially identifying parameters corresponding to
age. GMeLLo outperforms current state-of-the- specific knowledge and subsequently modifying
art (SOTA) methods on two datasets from the themthroughdirectupdatestothetargetparame-
MQuAKEbenchmark,affirmingitseffectiveness ters.
inmulti-hopquestionansweringwithinanevolving
2.2.2 PreservingModelParameters
environment.
In the case of preserving model parameters, the
2 RelatedWork introduction of additional parameters or external
memorybecomesnecessary. Theparadigmofad-
ThisworkutilizesbothKGsandLLMstoaddress
ditional parameters, as presented in (Dong et al.,
the challenge of multi-hop question answering,
2022;Hartvigsenetal.,2022;Huangetal.,2022),
withaparticularfocusonscenariosinvolvingevolv-
incorporatesextratrainableparametersintothelan-
ing factual knowledge. Therefore, we review ex-
guage model. These parameters are trained on
istingliteratureonmulti-hopquestionanswering,
a modified knowledge dataset, while the original
knowledgeediting,andtheaugmentationofLLMs
modelparametersremainstatic. Ontheotherhand,
1
withknowledgegraphs .
memory-based models (Mitchell et al., 2022b;
Zhongetal.,2023)explicitlystorealleditedexam-
2.1 Multi-HopQuestionAnswering
ples in memory and employ a retriever to extract
Multi-hopquestionansweringismorechallenging
therelevanteditfactsforeachnewinput,guiding
becauseitrequiresnotonlyrecallingfactsbutalso
themodelingeneratingtheeditedoutput.
appropriatelyaggregatingandchainingthem. Facts
While previous evaluation paradigms have pri-
canbesourcedfromaknowledgegraph(Linetal.,
marily focused on validating the recall of edited
2018;Chengetal.,2023;Zhongetal.,2023),tables
facts, Zhong et al. (2023) introduced MQuAKE,
1Duetospaceconstraints,someoftheliteratureislocated abenchmarkthatincludesmulti-hopquestionsin-
inAppendixB. volvingcounterfactualortemporaledits. Thetwodatasets within MQuAKE assess whether meth- progressesataslowerpace,theextensivetraining
odscanaccuratelyanswerquestionswherethere- data of LLMs should enable them to effectively
sponseshouldchangeduetoeditedfacts. comprehendmostsentencepatterns. Inthispaper,
weemployLLMstoextracttherelationchainfrom
3 GMeLLo: GraphMemory-based asentence,encompassingthementionedentityin
EditingforLargeLanguageModels the question and its relations with other uniden-
tified entities. Similar to the fact triple exaction
Inthissection,weintroduceourmethodGMeLLo
3.1,wetaskLLMswithselectingarelationfroma
formulti-hopquestionansweringwithknowledge
predefinedlisttomitigatevariedrepresentationsof
editing(Figure2).
thesamerelation. Takeaquestionsentencefrom
theMQuAKE-CF(Zhongetal.,2023)datasetas
3.1 ExtractingFactTriplesfromEdited
anexample,
InformationUsingLLMs
Question
KGs play a pivotal role in enhancing the capabil-
Whatisthecapitalofthecountryofcitizenship
itiesofLLMsbyofferingexternalknowledgefor
ofthechildofthecreatorofEeyore?
improvedinferenceandinterpretability,asdemon-
RelationChain
stratedbyrecentstudies(Panetal.,2023;Rawte
Eeyore->creator->?x->child->?y
et al., 2023). Apart from merely storing updated
->country of citizenship
informationinanexternalmemory,suchasalist
->?z->capital->?m
ofseparatesentencestatementsasseeninconven-
tionalapproaches(Zhongetal.,2023),weutilize Thepresentedquestionnecessitatesa4-hoprea-
the KG to maintain inherent connections and en- soningprocess. With"Eeyore"astheknownentity
suretheintegrationofthelatestinformation. in focus, the journey to the final answer involves
In our approach, we leverage Wikidata (Vran- identifying its creator ’?x’, moving on to the cre-
decˇic´ and Krötzsch, 2014), a widely recognized ator’s child ’?y’, obtaining the child’s country of
KG, as the foundational knowledge base. When citizenship’?z’,andculminatingwiththeretrieval
updatedfactsarereceived,weutilizeLLMstoex- ofthecountry’scapital’?m’. Alltherelations,such
tractentitiesfromthesentencesanddeterminetheir as ’creator,’ ’child,’ ’country of citizenship,’ and
relationships(selectingarelationfromtheprede- ’capital,’arechosenfromapredefinedlistofrela-
finedlist). Thisprocessgenerateseditedfacttriples, tions. Therelationchainencapsulatesallessential
whicharethenusedtoupdatetheKG(seeFigure informationforderivingtheanswer.
2). UpdatingtheKGwithaneditedfacttriplein- To enable LLMs to extract relation chains and
volvesidentifyingtheconnectionsintheKGbased generateoutputsinastructuredtemplate,wepro-
on the subject entity and relation, breaking these videseveralexamplesofrelationchainextraction
connections, and establishing a new connection inthepromptandutilizein-contextlearning(Dong
basedonthetriple. etal.,2023),asdetailedinAppendixA.4.
Weincorporatein-contextlearning(Dongetal.,
3.3 ConvertingRelationChainintoaFormal
2023) to ensure the LLMs have thorough under-
Query
standingofthetask. Furthermore,giventhepossi-
bilitythatLLMsmaygeneraterelationsnotpresent Oncethe relationchain isobtained, the next step
in the predefined relation list (Chen et al., 2024), involvesintegratingtheknownentityandtherela-
weusearetrievalmodeltoidentifythemostsimilar tionsintoaformalquerytemplate. ConsideraKG
2
relation(i.e.,theclosestrelationintheembedding representedinRDF formatandacorresponding
3
space)fromthepredefinedrelationlist. Theintegra- SPARQL query, the relation chain elucidated in
tionofretrievalmodelmakesthetripleextraction Section3.2shouldberepresentedasfollows,
processmorerobust.
PREFIX ent: <http://www.kg/entity/>
3.2 ExtractingRelationChainfromQuestions PREFIX rel: <http://www.kg/relation/>
UsingLLMs SELECT DISTINCT ?id ?label WHERE {
ent:E0 rel:R0 ?x.
Astheworldevolvesrapidly,thetrainingdatafor
LLMs can quickly become outdated. However, 2https://www.w3.org/RDF/
sincetheevolutionoflinguisticpatternstypically 3https://www.w3.org/TR/sparql11-query/GMeLLo Formal Query
SELECT DISTINCT ?id ?label
WHERE {
Relation Chain
ent:E0 rel:R0 ?x.
Questions & Edited Facts Extract E >e chyo ildre ->-> ?c yr -e >a ct oo ur n-> tr? yx - Template ?x rel:R1 ?y.
Filling ?y rel:R2 ?z.
Multi-hop question LLM 3.2 o >cf ac pit ii tz ae ln ->s ?h mip->?z- 3.3 ?z rel:R3 ?id.
• What is the capital of the ?id rdfs:label ?label.
country of citizenship of the }
child of the creator of Eeyore? QA LIMIT 1
LLM 3.4
Final
Relevant Edited Facts KBQA
Retrieve Answer
• A. A. Milne’s child is Cosette 3.4
Edited Facts 3.4
• The headquarters of Yamaha
Corporation is located in the Edited Fact Triples
city of Naka-ku.
• <Yamaha Corporation, headquarters
• The author of David
location, Naka-ku>
Copperfield is Thomas Mann.
Extract • <David Copperfield, author, Thomas
• Star Trek was created by KG
Mann>
Stephen King. LLM 3.1
• <Star Trek, creator, Stephen King>
• ……
……
Figure2: Theillustrationdepictsourproposedmethod,GMeLLo. WebeginbyutilizingLLMstoextractentities
andrelationsfromeditedfacts,resultinginalistofeditedfacttriples. ThesetriplesarethenusedtoupdateaKG.
Similarly,weemployLLMstoextractrelationchainsfromagivenquestion. Bypopulatingthisinformationintoa
template,wegenerateaformalquerysuitableforuseinKBQA(Lanetal.,2022). Simultaneously,weutilizeLLMs
forquestionanswering,providingananswerbasedontherelevanteditedfactsretrieved. IncaseswheretheLLM’s
answercontradictsthatoftheKG,wedefertotheKG’sanswerasthefinalresponse.
?x rel:R1 ?y. pectedtobesimplerandyieldmoreaccurateresults
?y rel:R2 ?z. whenthefactsareprovidedaccurately.
?z rel:R3 ?id. However,addressingmulti-hopquestions,espe-
?id rdfs:label ?label. ciallythosewheretheeditedfactspertaintointer-
} mediary hops, presents a challenge in accurately
LIMIT 1 retrieving the relevant information and perform-
ing correct multi-hop question answering. This
Inthiscontext,"ent"and"rel"serveasprefixes
challengeisparticularlypronouncedwhendealing
forentityandrelation,respectively. Theidentifier
with a large volume of edited facts. For instance,
"E0"uniquelyrepresents"Eeyore"withintheKG,
accurately identifying the relevant fact given the
whiletheidentifiersfor"creator,""child,""country
question in Figure 2 and producing the correct
ofcitizenship,"and"capital"aredenotedas"R0",
finalanswerisdifficult.
"R1", "R2", and "R3" respectively. After identi-
KBQA.ToaddressthechallengesofLLM-based
fying the entity "?id", we retrieve its string label
question answering, we integrate responses from
"?label"asthefinalanswer.
KBQAtorefinetheoutputsfromtheLLMs,asde-
tailedinSections3.1-3.3. Whentherelationchain
3.4 IntegratingLLM-basedQAandKBQA
andfacttriplesareaccuratelyderived,theKBQA
Thissubsectionoutlinestheintegrationofthepro-
systemprovidesthecorrectanswer. However,ifthe
posed KBQA module with the LLM-based QA
relation chain is incorrectly extracted, the search
modulewithintheGMeLLoframework.
path in the KG may become invalid, leading the
LLM-basedquestionanswering. Whenaques-
KBQAsystemtoyieldnooutput. Insuchinstances,
tionarises,weretrievethetop-xrelevantfactsus-
weaccepttheresponsefromtheLLMsasthefinal
ingthepre-trainedContriever(Izacardetal.,2022)
answer.
modelfromalistofeditedfactsentences. Wethen
prompttheLLMstogenerateanswersbasedonthe
4 Experiment
questionandthesepertinentfacts. Comparedtothe
"split-answer-check" pipeline in MeLLo (Zhong In the upcoming section, we will conduct experi-
et al., 2023), this LLM-based QA method is ex- mentstodemonstratetheeffectivenessofemploy-ingourGMeLLomethodology. • MeLLo (Zhong et al., 2023). It employs a
memory-basedapproachformulti-hopques-
4.1 ExperimentSetup
tionanswering,storingallupdatedfactsinan
4.1.1 Dataset externalmemory.
Ourexperimentfocusesonthemulti-hopquestion-
Giventhesubstantialcostsassociatedwithtrain-
answering benchmark, MQuAKE (Zhong et al.,
ing,deploying,andmaintaininglargerLLMs(Li
2023),whichcomprisestwodatasets: MQuAKE-
4 et al., 2023b), and the challenges of scaling up
CF , designed for counterfactual edits, and
knowledgeeditingmethodsthatrequiremodelpa-
MQuAKE-T, specifically tailored for updates in
rametermodifications,thispaperprimarilyfocuses
temporalknowledge.
onsmallerLLMs,specificallyGPT-J(6B)(Wang
The MQuAKE-CF dataset comprises 3,000 N-
andKomatsuzaki,2021)andVicuna(7B)(Chiang
hop questions (N ∈ {2,3,4}), each linked toone
et al., 2023). However, to showcase GMeLLo’s
ormoreedits. Thisdatasetfunctionsasadiagnos-
effectivenesswithlargerLLMsinpracticalscenar-
tictoolforexaminingtheeffectivenessofknowl-
ios,wealsoreporttheperformanceofbothMeLLo
edge editing methods in handling counterfactual
andGMeLLoontheMQuAKE-CFdatasetwhen
edits. The MQuAKE-T dataset consists of 1,868
k = 3000.
instances, each associated with a real-world fact
change. Its purpose is to evaluate the efficacy of 4.1.4 KnowledgeGraphSetting
knowledge editing methods in updating obsolete
ConsideringWikidata’scommunity-drivennature,
information with contemporary, factual data. A
guaranteeingadynamicandcomprehensivedataset
tableofstatisticsisavailableinAppendixA.1.
acrossaspectrumofknowledgedomains,weuse
4.1.2 EvaluationSettings Wikidata (Vrandecˇic´ and Krötzsch, 2014) as the
foundationalKGforthisexperiment. Toalignthe
To evaluate our models, we adhere to the testing
relations in the question and fact sentences with
settings outlined by Zhong et al. (2023). Specif-
thoseinWikiData(Vrandecˇic´ andKrötzsch,2014),
ically, instances are batched in groups of size k,
withk ∈ 1,100,1000,3000forMQuAKE-CF,and wefollowthefollowingsteps:
k ∈ 1,100,500,1868forMQuAKE-T.Forexam-
5
• Weselectthefirst500itemproperties from
ple, in the MQuAKE-CF dataset, when k = 100,
WikiData as the base relations. Items repre-
the3000instancesaresplitinto30groups,andwe
senteitherconcreteorabstractentities,such
reporttheaverageperformanceasthefinalresult.
asaperson(PiscopoandSimperl,2019).
Foreachtestinstance,thedatasetincludesthree
multi-hopquestionsthatconveythesamemeaning. • Next,weemployGPT-3.5-Turbotoexamine
InalignmentwithZhongetal.(2023),ifthemodel each multi-hop question in the test samples
correctlyanswersanyoneofthesequestions, we to determine if it contains any of the base
considertheinstancetobeaccuratelyresolved. relations.
4.1.3 Baselines
• Afterward, we rank the frequencies of each
Todemonstratetheeffectivenessofourapproach, relation and choose the top 50 relations as
weconductcomparisonswiththefollowingSOTA candidatesforuseinrelationchainextraction
knowledgeeditingmethods. andeditedfacttripleextraction.
• MEND (Mitchell et al., 2022a). It trains a
To stay updated with the latest information on
hyper-networktogenerateweightupdatesby
6
WikiData, we utilize the WikiData API service
transformingrawfine-tuninggradientsbased
7
andtheWikiDataQueryService . Thecorrectness
onaneditedfact.
ofourKBQAresulthingesontheaccurateextrac-
• MEMIT(Mengetal.,2023). Itupdatesfeed- tionofbotheditedfacttriplesandrelationchains.
forwardnetworksacrossvariouslayerstoin- If the relation chain is found to be incorrect, we
corporateallrelevantfacts.
5https://www.wikidata.org/w/index.php?title=
4Following Zhong et al. (2023), our experiments on Special:ListProperties/wikibase-item&limit=500&
offset=0
MQuAKE-CF are carried out on a randomly sampled sub-
set of the complete dataset, comprising 3000 instances in
6https://www.wikidata.org/w/api.php
total(1000instancesforeachof2,3,4-hopquestions). 7https://query.wikidata.org/sparqlMQuAKE-CF MQuAKE-T
BaseModel Method
k=1 k=100 k=1000 k=3000 k=1 k=100 k=500 k=1868
MEMIT 12.3 9.8 8.1 1.8 4.8 1.0 0.2 0.0
MEND 11.5 9.1 4.3 3.5 38.2 17.4 12.7 4.6
GPT-J-6B
MeLLo 20.3 12.5 10.4 9.8 85.9 45.7 33.8 30.7
GMeLLo 76.3 53.4 49.5 49.0 86.9 82.1 81.5 81.5
MeLLo 20.3 11.9 11.0 10.2 84.4 56.3 52.6 51.3
Vicuna-7B
GMeLLo 71.3 46.5 42.5 41.9 97.1 86.3 85.4 85.1
Table1: PerformancecomparisonofGMeLLoandotherapproachesontheMQuAKE-CFandMQuAKE-Tdatasets
usingGPT-J-6BorVicuna-7Basthebaselanguagemodels. AdheringtothemethodologyoutlinedbyZhongetal.
(2023),instancesaregroupedintobatchesofsizek. FortheMQuAKE-CFdataset,kvariesfrom1to3000,andfor
theMQuAKE-Tdataset,itrangesfrom1to1868. Forexample,intheMQuAKE-CFdataset,whenk =100,the
3000instancesareorganizedinto30groups,andtheaverageperformancereportedasthefinalresult. Themetric
usedisaccuracy.
conductanonlinesearchonWikiDatatodetermine ingitwell-suitedforreal-worldquestionanswering
iftherelationchainleadstoanentitythatcouldpo- applicationsthatrequiremanaginglargevolumes
tentiallyyieldanincorrectanswerforthespecific ofrapidlychanginginformation.
question,whichtakesabout1second. Inaddition,weevaluatedMeLLoandGMeLLo
usingtwolargermodels,GPT-3.5-Turbo-Instruct
4.1.5 StrategiesforManagingUnforeseen 8
andGPT-3.5-Turbo ,ontheMQuAKE-CFdataset
Relationships 9
with k=3000 . The accuracy rates achieved by
Aspreviouslynoted,sinceLLMsmayproducere- MeLLoandGMeLLowithGPT-3.5-Turbo-Instruct
lationsthataresimilarinmeaningbutnotidentical, were 30.7% and 51.4%, respectively. While
weemploythepretrainedContrievermodel(Izac- GMeLLoachievedanaccuracyof66.4%withGPT-
ardetal.,2022)toretrievethemostsimilarrelation 3.5-Turbo, the same model consistently returned
(i.e., the closest relation in the embedding space) errors when tested with MeLLo, suggesting that
from the base list of relations. This replacement thepromptsmayrequiremodificationforcompati-
isperformedwhenundefinedrelationsareencoun- bilitywithchatcompletionmodels. Theseresults
teredduringbotheditedfacttripleextractionand indicate that GMeLLo performs well even when
relationchainextraction. scaledtolargerLLMs.
4.2 MainResults 4.3 AblationStudy
As shown in Table 1, our GMeLLo significantly Togainacomprehensiveunderstandingoftheper-
outperforms all existing methods on the both the formanceofvariouscomponents,i.e.,LLM-based
MQuAKE-CFdatasetandtheMQuAKE-Tdataset QAandKBQA,weconductanexperimenttoillus-
(Zhongetal.,2023),particularlywhenhandlinga tratetheimpactofLLM-basedQAandKBQAas
largenumberofedits. thenumberofeditsincreases.
TheperformancedegradationinMeLLoispri- As demonstrated in Table 2, the performance
marilyduetoitschallengesinidentifyingrelevant of KBQA remains consistent, as all edited facts
factsasthenumberofeditsincreases. Whenk=1, areconvertedtotriplesandallrelationchainsare
the model utilizes only the facts directly related extractedfromthetestquestions,regardlessofthe
to the input question for context. However, as k value of ’k’. However, as the parameter ’k’ in-
increases,themodelfacesthechallengeofdiscern- creases,moreeditedfactsarestoredintheexternal
ing relevant facts from a broader memory. Our memory. Consequently,selectingtherelevanted-
proposed GMeLLo model mitigates this by em- itstoaccuratelyansweringthequestionsbecomes
ployinganexplicitsymbolicgraphrepresentation, increasinglychallengingforLLM-basedQA.
whichenhancesthesystem’sabilitytoupdateand
8https://platform.openai.com/docs/models/gpt-3-5-turbo
retrieverelevantfactseffectively. Thisfeaturesig- 9The model text-davinci-003 used in Zhong et al.
nificantlybooststhescalabilityofGMeLLo,mak- (2023)wasdeprecatedonJanuary4,2024.MQuAKE-CF MQuAKE-T
BaseModel Method
k=1 100 1000 3000 k=1 100 500 1868
QA 71.0 24.2 14.3 12.2 32.3 18.0 15.7 15.5
GPT-J-6B KBQA 43.3 43.3 43.3 43.3 80.2 80.2 80.2 80.2
GMeLLo 76.3 53.4 49.5 49.0 86.9 82.1 81.5 81.5
QA 72.6 27.0 16.5 13.5 96.9 63.0 59.2 58.2
Vicuna-7B KBQA 35.9 35.9 35.9 35.9 73.6 73.6 73.6 73.6
GMeLLo 71.3 46.5 42.5 41.9 97.1 86.3 85.4 85.1
Table 2: Ablation study of GMeLLo. QA involves directly using LLM for answering the multi-hop questions.
KBQA involves using LLM to transform edited fact sentences into triples, update WikiData, convert question
sentencesintorelationchains,andgenerateformalKGqueriesforquestionanswering. GMeLLocombinesthese
methodsbyusingKBQAtocorrectanswersfromLLM-basedQA.
When k=1 and all relevant facts are provided 4.4.1 InferiorPerformanceofGPT-JinQA
to the LLMs for question answering, the LLM-
based QA proves to be quite effective. However, Table2showsthattheperformanceofGPT-Jand
a more realistic scenario involves multiple edits VicunainconductingQAtasksiscomparableon
occurringsimultaneously,whereeachquestionis the MQuAKE-CF dataset when k=1. However,
askedseparately(i.e.,k>1). Theperformanceshow- GPT-Jexhibitsnotablylowerperformanceonthe
casedinTable2demonstratestheeffectivenessof MQuAKE-Tdataset. Furtheranalysisrevealedthat
our GMeLLo, highlighting that KBQA serves as GPT-Jstrugglesinansweringquestionswithonly
avaluableenhancementtoLLM-basedQAwithin aneditedfactpertainingtoitsintermediaryinfor-
evolvingenvironments. mation,suchas:
SamplefromMQuAKE-CF
4.3.1 FurtherAnalysis
Facts: Midfielderisassociatedwiththesport
To evaluate the impact of KBQA on LLM-based ofGaelicfootball
QAwithintheGMeLLoframework,weconducted Question: Whatisthecapitalofthecountry
an analysis comparing the responses from LLMs wherethesportassociatedwithKieronDyer’s
to those from the KG. We consider the KG’s re- specialtywasfirstplayed?
sponseasthefinalanswer. Therefore,comparing PredictedAnswer: BondiJunction
toonlyusingLLM-basedQA,iftheanswerfrom Answer: Dublin
LLMs is correct but the answer from the KG is
incorrect, this leads to a decline in performance.
SamplefromMQuAKE-T
Conversely,iftheanswerfromLLMsisincorrect
Facts: The name of the current head of the
buttheanswerfromtheKGiscorrect,performance
PhilippinesgovernmentisBongbongMarcos
improves. IftheKBQAprovidesnoresponse,per-
Question: Whoistheheadofgovernmentof
formanceremainsunchanged. AsillustratedinTa-
thecountrythatJoeydeLeonisacitizenof?
ble3,whentherearediscrepanciesbetweenKBQA
PredictedAnswer: BenignoAquinoIII
and LLM-based QA responses, the likelihood of
Answer: BongbongMarcos
KBQAprovidingthecorrectanswerincreasesas
theparameterkincreases.
However, it can achieve the correct answer in
KBQAbecauseitaccuratelyextractsthefacttriple
4.4 QualitativeAnalysis
and relation chain of the question. Given that
Table 2 illustrates that Vicuna exhibits superior all test samples in MQuAKE-T contain only one
performance in directly handling the QA task, editedfact,whileapproximately63.6%oftestsam-
particularly when provided with the exact edited ples in MQuAKE-CF consist of more than two
facts. Conversely,GPT-Jexcelsinsentenceanaly- editedfacts,GPT-Jisabletoconnectmostofthe
sistasks, showcasing itshigh performancein the informationtogether. Therefore,itachievesbetter
KBQAtask. performanceintheMQuAKE-CFdataset.Scenario MQuAKE-CF MQuAKE-T
BaseModel
LLM KG Performance k=1 100 1000 3000 k=1 100 500 1868
✖ ✔ ↑ 8.1 22.9 24.9 25.0 44.0 47.2 47.9 48.0
✔ ✖ ↓ 12.5 2.4 1.2 0.7 0.7 0.4 0.3 0.3
GPT-J-6B
✔ ◯ - 34.2 7.0 4.0 3.7 7.1 2.8 2.4 2.3
✖ ✔ ↑ 7.7 17.8 19.6 20.0 4.2 19.7 21.4 21.7
✔ ✖ ↓ 21.8 3.9 2.0 1.2 7.2 4.2 4.0 3.9
Vicuna-7B
✔ ◯ - 32.7 7.4 4.0 3.4 35.7 19.8 18.1 17.7
Table3: FurtheranalysisforscenarioswheretheanswersfromLLMandKGcontradicteachother. Thevaluesare
expressedaspercentages. Itisimportanttonotethatthetotalnumberoftestquestionsisthreetimesthenumberof
testinstances. Forinstance,inMQuAKE-CF,eachtestinstancecomprisesthreedistinctquestionswiththesame
meaning, totaling9,000testquestions. Symbolsused: ↑indicatesimprovedperformance, ↓indicatesreduced
performance,and◯denotesnoresponsefromKBQA,resultinginnoimpactonthefinaloutput(-).
4.4.2 InferiorPerformanceofVicunain 4.5 FurtherDiscussion
KBQA
KG offers a clearer representation of multi-hop
Compared to GPT-J, Vicuna performs less effec-
informationanditsupdates. InGMeLLo,wehar-
tively in the KBQA task. Aside from misunder-
nessthestrengthsofbothKBQAandLLM-based
standings,themainreasonsareasfollows:
QA, benefiting from KBQA’s high precision and
LLM-basedQA’sextensivecoverage. Ourexperi-
• Itoftenmakeserrorsinthesequence. Forex- mentsreveal thatGPT-Jexcelsin extractingrela-
ample, given the fact "The author of Misery tionchainsandfacttriples,whereasVicunademon-
isRichardDawkins",itsoutputfacttripleis strates superior performance in LLM-based QA.
"RichardDawkins->author->Misery". How- GiventhatKBQAandLLM-basedQAoperateas
ever,thecorrectsequenceis"Misery->author- separate modules in GMeLLo, we can optimize
>RichardDawkins". their use by employing different LLMs in each
module,maximizingtheireffectivenessinpractical
• Itfrequentlymakeserrorsinselectingarela- applications.
tion from the list. For example, it often out-
puts a relation chain as "Mike->citizenship-
5 Conclusion
>country->head of state", instead of "Mike-
>countryofcitizenship->headofstate".
In this paper, we present GMeLLo, a method de-
signed for multi-hop question answering in dy-
Itisimportanttonotethateveniftherelationchain
namicenvironments. ExceptleveragingLLMsfor
isincorrect,theKBQAsystemmaystillprovidethe
questionanswering,wealsoleveragethecapabili-
correctanswerbecauseofsomeloopsinWikiData,
tiesofLLMstoextractthetriplesfromeditedfact
suchasthecountryoftheUSAistheUSA.
sentencetoupdateKG,andusethecapabilitiesof
AlthoughVicunaisnotaseffectiveoverall,we
LLMstoanalyzequestionsentencesandgenerate
stillfindthatinsomecasesitcancorrectlyextract
a relation chain, and finally get the formal query
relations, but cannot provide the correct answer
byfillinginaformalquerytemplate. Finally,we
directly. Anexampleisgivenasfollows:
combineKBQAandLLM-basedQAtobolsterthe
SamplefromMQuAKE-CF multi-hopquestionansweringcapabilitywithina
Facts: Pointguardisassociatedwiththesport dynamic environment. This approach capitalizes
ofcricket onthestrengthsofbothLLMsandKGs. Byutiliz-
Question: Whatisthecapitalofthecountry ingLLMsforanalyzingquestionsentencesandQA
fromwhichErikSpoelstra’ssportcomes? toensurethecoverage,andKBQAtoprovideac-
PredictedAnswer: Miami curateresults,weachieveasynergybetweenthese
Answer: London twomethodologies.Limitations WenhuChen,HanwenZha,ZhiyuChen,WenhanXiong,
Hong Wang, and William Yang Wang. 2020. Hy-
Despitethepromisingresults,itisimportanttoac- bridQA:Adatasetofmulti-hopquestionanswering
knowledgethatthisinvestigationisstillinitsearly overtabularandtextualdata. InFindingsoftheAsso-
ciationforComputationalLinguistics:EMNLP2020,
stages. Althoughourperformancesignificantlysur-
pages1026–1036,Online.AssociationforComputa-
passesbaselineapproachesinmulti-hopquestions
tionalLinguistics.
indynamicdomains,particularlyforlargeknowl-
edge bases and complex questions, there is still Zhen Cheng, Jianwei Niu, Shasha Mo, and Jia Chen.
2023. Genboost: Generativemodelingandboosted
roomforfurtherimprovement. Ourfutureresearch
learning for multi-hop question answering over in-
includes
complete knowledge graphs. In 2023 IEEE 29th
InternationalConferenceonParallelandDistributed
• Leveraging more sophisticated prompting Systems(ICPADS),pages1131–1138.
techniques,suchasChainofThought(CoT)
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,
(Wei et al., 2022), to enable more accurate
ZhanghaoWu,HaoZhang,LianminZheng,Siyuan
multi-hopreasoning.
Zhuang,YonghaoZhuang,JosephE.Gonzalez,Ion
Stoica, and Eric P. Xing. 2023. Vicuna: An open-
• Refining the predefined relation list to en- sourcechatbotimpressinggpt-4with90%*chatgpt
hanceitsaccuracy. quality.
AakankshaChowdhery,SharanNarang,JacobDevlin,
• EnhancingtheKGtosupportmorecomplex
MaartenBosma,GauravMishra,AdamRoberts,Paul
questionanswering,suchasinquiriesinvolv-
Barham,HyungWonChung,CharlesSutton,Sebas-
inghistoricalinformation. tianGehrmann,etal.2023. Palm: Scalinglanguage
modelingwithpathways. JournalofMachineLearn-
Webelievetheseimprovementscanfurtherenhance ingResearch,24(240):1–113.
theperformanceandscalabilityofthesystem,en-
Wanyun Cui, Yanghua Xiao, Haixun Wang, Yangqiu
ablingittohandlemorecomplexanddiversereal-
Song, Seung-won Hwang, and Wei Wang. 2017.
worldapplications.
Kbqa: learning question answering over qa cor-
pora and knowledge bases. Proc. VLDB Endow.,
10(5):565–576.
References
DamaiDai,LiDong,YaruHao,ZhifangSui,Baobao
JinheonBaek,AlhamFikriAji,andAmirSaffari.2023. Chang,andFuruWei.2022. Knowledgeneuronsin
Knowledge-augmentedlanguagemodelprompting pretrainedtransformers. InProceedingsofthe60th
forzero-shotknowledgegraphquestionanswering. AnnualMeetingoftheAssociationforComputational
InProceedingsoftheFirstWorkshoponMatching Linguistics (Volume 1: Long Papers), pages 8493–
From Unstructured and Structured Data (MATCH- 8502,Dublin,Ireland.AssociationforComputational
ING2023),pages70–98,Toronto,ON,Canada.As- Linguistics.
sociationforComputationalLinguistics.
NicolaDeCao,WilkerAziz,andIvanTitov.2021. Edit-
Tom Brown, Benjamin Mann, Nick Ryder, Melanie ingfactualknowledgeinlanguagemodels. InPro-
Subbiah,JaredDKaplan,PrafullaDhariwal,Arvind ceedingsofthe2021ConferenceonEmpiricalMeth-
Neelakantan,PranavShyam,GirishSastry,Amanda ods in Natural Language Processing, pages 6491–
Askell, Sandhini Agarwal, Ariel Herbert-Voss, 6506,OnlineandPuntaCana,DominicanRepublic.
Gretchen Krueger, Tom Henighan, Rewon Child, AssociationforComputationalLinguistics.
AdityaRamesh,DanielZiegler,JeffreyWu,Clemens
Winter, Chris Hesse, Mark Chen, Eric Sigler, Ma- Qingxiu Dong, Damai Dai, Yifan Song, Jingjing Xu,
teusz Litwin, Scott Gray, Benjamin Chess, Jack ZhifangSui, andLeiLi.2022. Calibratingfactual
Clark, ChristopherBerner, SamMcCandlish, Alec knowledgeinpretrainedlanguagemodels. InFind-
Radford, Ilya Sutskever, and Dario Amodei. 2020. ingsoftheAssociationforComputationalLinguistics:
Language models are few-shot learners. In Ad- EMNLP2022,pages5937–5947,AbuDhabi,United
vances in Neural Information Processing Systems, ArabEmirates.AssociationforComputationalLin-
volume 33, pages 1877–1901. Curran Associates, guistics.
Inc.
QingxiuDong,LeiLi,DamaiDai,CeZheng,Zhiyong
Ruirui Chen, Chengwei Qin, Weifeng Jiang, and Wu,BaobaoChang,XuSun,JingjingXu,LeiLi,and
Dongkyu Choi. 2024. Is a large language model ZhifangSui.2023. Asurveyonin-contextlearning.
agoodannotatorforeventextraction? InProceed-
ingsoftheAAAIConferenceonArtificialIntelligence, AnshitaGupta,DebanjanMondal,AkshaySheshadri,
volume38,pages17772–17780. WenlongZhao,XiangLi,SarahWiegreffe,andNiketTandon.2023. Editingcommonsenseintransform- KevinMeng,DavidBau,AlexAndonian,andYonatan
ers. InProceedingsofthe2023ConferenceonEmpir- Belinkov.2022. Locatingandeditingfactualasso-
icalMethodsinNaturalLanguageProcessing,pages ciationsingpt. InAdvancesinNeuralInformation
8214–8232. ProcessingSystems,volume35,pages17359–17372.
CurranAssociates,Inc.
ThomasHartvigsen,SwamiSankaranarayanan,Hamid
Palangi, YoonKim, andMarzyehGhassemi.2022. Kevin Meng, Arnab Sen Sharma, Alex Andonian,
Agingwithgrace: Lifelongmodeleditingwithdis- YonatanBelinkov,andDavidBau.2023. Massedit-
cretekey-valueadaptors. InNeurIPS2022Workshop ing memory in a transformer. The Eleventh Inter-
onRobustnessinSequenceModeling. national Conference on Learning Representations
(ICLR).
ZeyuHuang,YikangShen,XiaofengZhang,JieZhou,
EricMitchell,CharlesLin,AntoineBosselut,Chelsea
WengeRong,andZhangXiong.2022. Transformer-
Finn,andChristopherDManning.2022a. Fastmodel
patcher: One mistake worth one neuron. In The
editing at scale. In International Conference on
EleventhInternationalConferenceonLearningRep-
LearningRepresentations.
resentations.
EricMitchell,CharlesLin,AntoineBosselut,Chelsea
GautierIzacard,MathildeCaron,LucasHosseini,Sebas-
Finn,andChristopherD.Manning.2022b. Memory-
tianRiedel,PiotrBojanowski,ArmandJoulin,and
basedmodeleditingatscale. InInternationalCon-
EdouardGrave.2022. Unsuperviseddenseinforma-
ferenceonMachineLearning.
tionretrievalwithcontrastivelearning. Transactions
onMachineLearningResearch. Zhijie Nie, Richong Zhang, Zhongyuan Wang, and
XudongLiu.2024. Code-stylein-contextlearningfor
Omar Khattab, Keshav Santhanam, Xiang Lisa knowledge-basedquestionanswering. InProceed-
Li, David Hall, Percy Liang, Christopher Potts, ingsoftheAAAIConferenceonArtificialIntelligence,
and Matei Zaharia. 2022. Demonstrate-search- volume38,pages18833–18841.
predict: Composing retrieval and language mod-
els for knowledge-intensive nlp. arXiv preprint Shirui Pan, Linhao Luo, Yufei Wang, Chen Chen, Ji-
arXiv:2212.14024. apuWang,andXindongWu.2023. Unifyinglarge
languagemodelsandknowledgegraphs: Aroadmap.
Yunshi Lan, Gaole He, Jinhao Jiang, Jing Jiang, arXivpreprintarXiv:2306.08302.
WayneXinZhao,andJi-RongWen.2022. Complex
knowledgebasequestionanswering:Asurvey. IEEE AlessandroPiscopoandElenaSimperl.2019. Whatwe
TransactionsonKnowledgeandDataEngineering. talk about when we talk about wikidata quality: a
literaturesurvey. InProceedingsofthe15thInterna-
tionalSymposiumonOpenCollaboration,OpenSym
FangyuLei,XiangLi,YifanWei,ShizhuHe,Yiming
’19,NewYork,NY,USA.AssociationforComputing
Huang,JunZhao,andKangLiu.2023. S3HQA:A
Machinery.
three-stageapproachformulti-hoptext-tablehybrid
questionanswering. InProceedingsofthe61stAn-
OfirPress,MuruZhang,SewonMin,LudwigSchmidt,
nualMeetingoftheAssociationforComputational
NoahSmith,andMikeLewis.2023. Measuringand
Linguistics(Volume2: ShortPapers),pages1731–
narrowingthecompositionalitygapinlanguagemod-
1740, Toronto, Canada. Association for Computa-
els. InFindingsoftheAssociationforComputational
tionalLinguistics.
Linguistics: EMNLP2023,pages5687–5711,Singa-
pore.AssociationforComputationalLinguistics.
XiaopengLi,ShashaLi,ShezhengSong,JingYang,Jun
Ma,andJieYu.2023a. Pmet: Precisemodelediting
Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie
inatransformer.
Millican, Jordan Hoffmann, Francis Song, John
Aslanides, Sarah Henderson, Roman Ring, Susan-
YuanzhiLi,SébastienBubeck,RonenEldan,AllieDel nah Young, Eliza Rutherford, Tom Hennigan, Ja-
Giorno,SuriyaGunasekar,andYinTatLee.2023b. cobMenick,AlbinCassirer,RichardPowell,George
Textbooksareallyouneedii: phi-1.5technicalre- van den Driessche, Lisa Anne Hendricks, Mari-
port. beth Rauh, Po-Sen Huang, Amelia Glaese, Jo-
hannes Welbl, Sumanth Dathathri, Saffron Huang,
XiVictoriaLin,RichardSocher,andCaimingXiong. JonathanUesato,JohnMellor,IrinaHiggins,Anto-
2018. Multi-hop knowledge graph reasoning with niaCreswell,NatMcAleese,AmyWu,ErichElsen,
reward shaping. In Proceedings of the 2018 Con- SiddhantJayakumar,ElenaBuchatskaya,DavidBud-
ferenceonEmpiricalMethodsinNaturalLanguage den,EsmeSutherland,KarenSimonyan,MichelaPa-
Processing, pages 3243–3253, Brussels, Belgium. ganini,LaurentSifre,LenaMartens,XiangLorraine
AssociationforComputationalLinguistics. Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena
Gribovskaya,DomenicDonato,AngelikiLazaridou,
VaibhavMavi,AnubhavJangra,andAdamJatowt.2022. ArthurMensch,Jean-BaptisteLespiau,MariaTsim-
Asurveyonmulti-hopquestionansweringandgen- poukelli,NikolaiGrigorev,DougFritz,ThibaultSot-
eration. arXivpreprintarXiv:2204.09140. tiaux,MantasPajarskas,TobyPohlen,ZhitaoGong,DanielToyama,CypriendeMassond’Autume,Yujia YunzhiYao,PengWang,BozhongTian,SiyuanCheng,
Li,TayfunTerzi,VladimirMikulik,IgorBabuschkin, ZhouboLi,ShuminDeng,HuajunChen,andNingyu
Aidan Clark, Diego de Las Casas, Aurelia Guy, Zhang.2023. Editinglargelanguagemodels: Prob-
Chris Jones, James Bradbury, Matthew Johnson, lems, methods, and opportunities. In Proceedings
Blake Hechtman, Laura Weidinger, Iason Gabriel, of the 2023 Conference on Empirical Methods in
WilliamIsaac,EdLockhart,SimonOsindero,Laura NaturalLanguageProcessing,pages10222–10240,
Rimell,ChrisDyer,OriolVinyals,KareemAyoub, Singapore.AssociationforComputationalLinguis-
JeffStanway,LorrayneBennett,DemisHassabis,Ko- tics.
rayKavukcuoglu,andGeoffreyIrving.2022. Scaling
languagemodels: Methods,analysis&insightsfrom PengchengYin,ZhengdongLu,HangLi,andKaoBen.
traininggopher. 2016. Neuralenquirer: Learningtoquerytablesin
natural language. In Proceedings of the Workshop
Ankit Singh Rawat, Chen Zhu, Daliang Li, Felix Yu, onHuman-ComputerQuestionAnswering,pages29–
ManzilZaheer,SanjivKumar,andSrinadhBhojana- 35,SanDiego,California.AssociationforComputa-
palli. 2020. Modifying memories in transformer tionalLinguistics.
models. In International Conference on Machine
Learning(ICML)2021. MengqiZhang,XiaotianYe,QiangLiu,PengjieRen,
Shu Wu, and Zhumin Chen. 2024. Knowledge
VipulaRawte,AmitSheth,andAmitavaDas.2023. A graphenhancedlargelanguagemodelediting. CoRR,
surveyofhallucinationinlargefoundationmodels. abs/2402.13593.
arXivpreprintarXiv:2309.05922.
ZexuanZhong,ZhengxuanWu,ChristopherManning,
PriyankaSen,SandeepMavadia,andAmirSaffari.2023. ChristopherPotts,andDanqiChen.2023. MQuAKE:
Knowledgegraph-augmentedlanguagemodelsfor Assessingknowledgeeditinginlanguagemodelsvia
complex question answering. In Proceedings of multi-hop questions. In Proceedings of the 2023
the1stWorkshoponNaturalLanguageReasoning Conference on Empirical Methods in Natural Lan-
and Structured Explanations (NLRSE), pages 1–8, guageProcessing, pages15686–15702, Singapore.
Toronto,Canada.AssociationforComputationalLin- AssociationforComputationalLinguistics.
guistics.
AntonSinitsin,VsevolodPlokhotnyuk,DmitryPyrkin,
SergeiPopov,andArtemBabenko.2020. Editable
neural networks. In International Conference on
LearningRepresentations.
DennyVrandecˇic´ andMarkusKrötzsch.2014. Wiki-
data: afreecollaborativeknowledgebase. Communi-
cationsoftheACM,57(10):78–85.
Ben Wang and Aran Komatsuzaki. 2021. GPT-J-
6B: A 6 Billion Parameter Autoregressive Lan-
guageModel. https://github.com/kingoflolz/
mesh-transformer-jax.
JasonWei,XuezhiWang,DaleSchuurmans,Maarten
Bosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le,
andDennyZhou. 2022. Chain-of-thoughtprompt-
ing elicits reasoning in large language models. In
AdvancesinNeuralInformationProcessingSystems,
volume35,pages24824–24837.CurranAssociates,
Inc.
JohannesWelbl,PontusStenetorp,andSebastianRiedel.
2018. Constructing datasets for multi-hop reading
comprehensionacrossdocuments. Transactionsof
theAssociationforComputationalLinguistics,6:287–
302.
ZhilinYang,PengQi,SaizhengZhang,YoshuaBengio,
WilliamCohen,RuslanSalakhutdinov,andChristo-
pher D. Manning. 2018. HotpotQA: A dataset for
diverse, explainablemulti-hopquestionanswering.
In Proceedings of the 2018 Conference on Empiri-
calMethodsinNaturalLanguageProcessing,pages
2369–2380,Brussels,Belgium.AssociationforCom-
putationalLinguistics.A Implementation
k=100 k=1000 k=3000 Average
A.1 DatasetStatistics Top-4 15.6 9.1 7.2 10.63
Top-5 16.8 8.3 6.9 10.67
Table4providesasummaryofthestatisticsforthe
Top-6 16.6 8.5 7.4 10.83
MQuAKE-CFandMQuAKE-Tdatasets.
Top-10 15.3 9.0 8.0 10.77
Top-100 8.2 4.7 3.7 5.53
#Edits 2-hop 3-hop 4-hop Total
1 513 356 224 1,093 Table 5: Hyperparameter search for top-x in Vicuna-
2 487 334 246 1,067 basedQAsystemsontheMQuAKE-CFdataset.
MQuaKE-CF 3 - 310 262 572
4 - - 268 268
der’,’publisher’,’originallanguageoffilmorTV
All 1,000 1,000 1,000 3,000
show’,’ethnicgroup’,’militarybranch’].
MQuaKE-T 1(All) 1,421 445 2 1,868 AfterGPT-3.5-Turbofiltering,theMQuAKE-T
datasetincludesatotalof35relations. Therelation
Table4: StatisticsofMQuAKEdataset(Zhongetal., list is [’head of government’, ’country of citizen-
2023). ship’,’headofstate’,’countryoforigin’,’country’,
’headquarters location’, ’location’, ’sport’, ’per-
former’,’genre’,’developer’,’employer’,’manu-
A.2 HyperparmersSettings
facturer’,’placeofdeath’,’placeofbirth’,’author’,
Toensurereproducibility, wesetthetemperature ’member of’, ’capital’, ’member of sports team’,
to zero in all experiments. Table 5 shows that re- ’chiefexecutiveofficer’,’notablework’,’director/
trievingthetop-6editedfactsfromexternalmem- manager’,’originalbroadcaster’,’creator’,’work
oryprovidesthebestaverageperformanceonthe location’,’educatedat’,’locatedintheadministra-
MQuAKE-CFdatasetfork > 1. Consequently,we tiveterritorialentity’,’headcoach’,’placeofpub-
include top-6 edited facts in the prompt for sub- lication’,’locationofformation’,’director’,’pro-
sequent experiments on this dataset when k > 1. ducer’,’transportnetwork’,’continent’,’child’]
Similarly,fortheMQuAKE-Tdatasetwhenk > 1,
A.4 PromptSetupandPost-Processing
weoptedtoincorporatethetop-1editedfactinthe
prompt. Thepromptsusedforeditedfacttripleextraction,
relationchainextraction,andLLM-basedQAare
A.3 PredefinedRelationsUtilizedinthe depictedinFigures 3, 4,and 5. Theeditedtriple
PromptsforRelationChainandFact canberegardedasaspecializedrelationchain,with
TripleExtraction onlyonerelationbetweenentitiesandallentities
known. All samples in the prompt are selected
After filtering by GPT-3.5-Turbo, the first 50
from the complete MQuAKE-CF dataset, ensur-
relations utilized in MQuAKE-CF dataset are:
ing they are distinct from the test samples. To
[’country of origin’,’sport’, ’country of citizen-
ship’, ’capital’, ’continent’, ’official language’,
Prompt for Transforming the Edited Sentences to Triples
’head of state’, ’head of government’, ’creator’,
Sentence: The headquarters of University of Cambridge is located in
’country’,’author’,’headquarterslocation’,’place
the city of Washington, D.C.
of birth’,’spouse’, ’director / manager’,’religion Relation Chain: University of Cambridge->headquarters location-
>Washington, D.C.
or worldview’, ’genre’, ’work location’, ’per-
......
former’,’manufacturer’, ’developer’, ’place of Given the above samples, please help me analyze the relation chain
of the following sentence. All the relations should be selected from
death’,’employer’,’educatedat’,’memberofsports
['country of origin','sport', ...].
team’, ’head coach’, ’languages spoken, writ- Sentence: The chief executive officer of Boeing is Marc Benioff
ten or signed’, ’notable work’, ’child’, ’founded Relation Chain:
by’,’location’,’chiefexecutiveofficer’,’original Figure3: Thepromptusedfortransformingeditedfact
broadcaster’,’chairperson’,’occupation’,’position sentencestotriples.
played on team / speciality’,’member of’, ’lan-
guageofworkorname’,’director’,’league’,’home improve the performance of LLMs in extracting
venue’, ’native language’, ’composer’, ’place of relationchainsandensurethatoutputsconformto
origin(Switzerland)’,’officeholder’,’religiousor- aspecifiedformat,weemploya4-shotlearningap-Prompt for Transforming the Question Sentences to Relation Chains ment,’and’headofgovernment’—itoftenmakes
Question: What is the birthplace of the author of "The Little Match errorsintheirsequencing. Toaddressthis,weem-
Girl"? 10
ploySpacy todetectinstanceswheretheobject
Relation Chain: The Little Match Girl->author->?x->place of birth-
>?y of an edited triple is not a person. If it is not, we
......
adjustthesequenceoftheobjectandsubjectinthe
Given the above samples, please help me analyze the relation chain
of the following sentence. All the relations should be selected from tripleaccordingly.
['country of origin','sport', ...].
Question: What is the continent where the CEO responsible for
B TheDistinctionsBetweenOur
developing Windows 8.1 was born?
Relation Chain: GMeLLoandOtherMethods
Figure4: Thepromptusedfortransformingquestion While both GMeLLo and MeLLo (Zhong et al.,
sentencestorelationchains.
2023) are memory-based models targeting multi-
hop question answering in an evolving environ-
Prompt for LLM-based QA
ment,theydifferinthefollowingaspects:
Facts: Hans Christian Andersen was born in the city of Brittany
Question: What is the birthplace of the author of "The Little Match
• MeLLo employs in-context learning to di-
Girl"?
Answer: Brittany rectLLMsinsplittingthequestionintosub-
......
questions, answering each, and verifying
Facts: Windows 8.1 was developed by Boeing; The chief executive
officer of Boeing is Marc Benioff; California is located in the against relevant edited facts for contradic-
continent of Europe; Marc Benioff was born in the city of California
tions. In contrast, GMeLLo retrieves perti-
Question: What is the continent where the CEO responsible for
developing Windows 8.1 was born? nent edited facts for the multi-hop question
Answer:
and presents them alongside the question to
Figure5: ThepromptusedinLLM-basedQA. LLMsforanswering.
• Except storing edited facts as isolated sen-
proachfortheMQuAKE-CFdatasetanda3-shot tences in an external memory, we leverage
learningapproachfortheMQuAKE-Tdataset. For LLMstotranslatethesesentencesintotriples
MQuAKE-CF, the approach involves presenting and update the KG. In addition to obtaining
themodelwithsamplesofone2-hopquestion,one an answer from LLMs, we utilize KBQA to
3-hopquestion,andtwo4-hopquestions. Incon- enhancetheprecisionofmulti-hopquestion
trast,forMQuAKE-T,themodelispresentedwith answeringwithinanevolvingenvironment.
one2-hopquestion, one3-hopquestion, and one
Recently, the advent of LLMs has spurred the
4-hopquestion.
developmentofLLM-basedKBQAsystems(Baek
ToaddressthelimitationsofGPT-JandVicuna
et al., 2023; Sen et al., 2023; Nie et al., 2024).
in conforming to the desired output format, we
However, our GMeLLo are different from these
establish a heuristic rule for extracting essential
worksinthefollowingaspects:
informationfromtheiroutputs. Forinstance,inthe
contextofrelationchainextraction,thisheuristic • Firstly, we consider question answering in
isoutlinedasfollows: a dynamic environment, where changes in
the knowledge graph need to accounted for,
• Narrow the attention to the output sentence
whereastheydonot.
containingthe"->"indicator.
• Secondly, we focus on multi-hop questions,
• Dividethesentencebasedonthe"->"delim-
whereastheydealwithstandardKBQAtasks,
iter.
including intersection and difference ques-
• Regard the initial segment as the predicted tionsetc.
entity. Subsequently, process the following
• Thirdly, theKBQAandLLM-basedQAare
segmentssequentiallyasrelations,provided
handledseparately,usingtheKBQAanswer
theydonotbeginwith"?".
asthefinalanswer. Incontrast,theyretrieve
A.5 StrategiesforManagingSequenceErrors triplesfromtheknowledgegraphandincorpo-
inExtractingFactTriples ratethemintotheprompttoguideLLM-based
QA.
WhileLLMsconsistentlyidentifiesrelationsaccu-
rately—such as ’head of state,’ ’chief of depart- 10https://spacy.io/NumberofHops
Model Method
2 3 4 Avg
MEND 13.9 11.3 9.5 11.5
MEMIT 22.5 6.0 8.4 12.3
GPT-J-6B
MeLLo - - - 20.3
GMeLLo 89.5 73.7 65.6 76.3
Table6: ThebreakdownperformanceontheMQuAKE-
CF dataset with respect to the number of hops when
k =1.
C Multi-HopPerformanceAnalysis
We study the breakdown of performance on the
MQuAKE-CF dataset with respect to the num-
ber of hops when k = 1. Table 6 provides the
hop-specificperformanceofdifferentmethods. Al-
thoughMQuAKEdidnotprovidethehopperfor-
mance for MeLLo, it can be inferred that the av-
eragehopperformanceshouldnotexceed65.6%,
giventhattheoverallperformanceis20.3%.