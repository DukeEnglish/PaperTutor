Optimal level set estimation for non-parametric tournament
and crowdsourcing problems
Maximilian Graf1, Alexandra Carpentier1, and Nicolas Verzelen2
1
Institutfu¨rMathematik,Universit¨atPotsdam,Potsdam,Germany.
2
INRAE,MISTEA,Univ. Montpellier,Montpellier,France
Abstract
Motivated by crowdsourcing, we consider a problem where we partially observe the cor-
rectness of the answers of n experts on d questions. In this paper, we assume that both the
experts and the questions can be ordered, namely that the matrix M containing the prob-
ability that expert i answers correctly to question j is bi-isotonic up to a permutation of it
rowsandcolumns. Whenn=d,thisalsoencompassesthestronglystochastictransitive(SST)
modelfromthetournamentliterature. Here,wefocusontherelevantproblemofdeciphering
small entries of M from large entries of M, which is key in crowdsourcing for efficient alloca-
tion of workers to questions. More precisely, we aim at recovering a (or several) level set p of
the matrix up to a precision h, namely recovering resp. the sets of positions (i,j) in M such
that M > p+h and M < p−h. We consider, as a loss measure, the number of misclas-
ij i,j
sified entries. As our main result, we construct an efficient polynomial-time algorithm that
turns out to be minimax optimal for this classification problem. This heavily contrasts with
existing literature in the SST model where, for the stronger reconstruction loss, statistical-
computationalgapshavebeenconjectured. Moregenerally,thisshadeslightonthenatureof
statistical-computational gaps for permutations models.
1 Introduction
Ranking problems have spurred a lot of interest both in the statistics and machine learning com-
munities. Applications of these problems include a variety of things ranging from tournament
problems [1], pairwise comparisons [2], to crowdlabeling problems [3].
In tournament problems with n players, we observe results of games between players and the
general objective is to gain knowledge on the ranking between the players. More formally, this
amounts to have noisy partial observations from an unknown matrix M ∈ [0,1]n×n where M
ij
corresponds to the probability that player i beats player j. In crowdsourcing problems, n experts
(or workers) are faced to d types of questions (or tasks). Here, the n×d matrix M encodes the
fact that M is the probability that expert i correctly answers the question j. Based on noisy
ij
observations of the matrix M, the objective is also to rank the experts or/and the questions. In
this manuscript, we consider both crowdsourcing and tournament data, the tournament problem
being a specific case where n=d and M =1−M (skew symmetry).
ij ji
Earliermodelsfortournamentproblemsareparametricinnature. Amongothers, theBradley-
Luce-Terry model [4, 5] has prompted a lot of works, even recently [6–8]. In particular, com-
putationally efficient and statistically optimal parameter estimation methods have been intro-
duced. Other parametric models such as the noisy sorting1 are also well understood [9, 10].
However, it has been observed that these simple parametric models are often unrealistic [11, 12]
and do not tend to fit the data well. This has lead to a recent line of literature in tourna-
ment and crowdsourcing where strong parametric assumptions are replaced by shape-constrained
non-parametric assumptions on the matrix M [13–22]. Arguably, the most popular model in
this field are the strong stochastically transitive (SST) [14] model for tournament and the bi-
isotonic model for crowdsourcing problems. The SST model presumes that the square matrix
1thismodelispreciselydefinedinSection6
1
4202
guA
72
]LM.tats[
1v65351.8042:viXraM is, up to a common permutation π of the rows and of the columns, bi-isotonic, that is
M π(−1)(i)π(−1)(j) ≥ max(M π(−1)(i+1)π(−1)(j),M π(−1)(i)π(−1)(j+1)). From a modeling perspective, this
corresponds to assuming that, if player i is better than j, then it has a larger probability than
player j of beating a third player k. Similarly, the bi-isotonic models in crowdsourcing data, sub-
sumes that, up to a permutation π of the rows and η of the columns, the matrix M is bi-isotonic.
Most works in the recent literature [13–19, 23] have focused on estimating the matrix M in
Frobenius distance. As recalled in Section 2, obtaining a good estimator of M mostly boils down
to estimating the permutation π (and η for crowdsourcing problems) with respect to some l
2
type distance. Although optimal rates for estimation of M have been pinpointed in the earlier
paper of Shah et al. [14], there remains a large gap between these optimal rates and the best
known performances of polynomial time algorithms. This has led to conjecture the existence of a
statistical-computational gap [17, 19].
1.1 Localizing large entries of M
In this manuscript, we move aside from the problem of estimating the matrix M in Frobenius
distance to the related problem of deciphering small entries of M from large entries of M. Given
M ∈ [0,1]n×d, some threshold p ∈ [0,1] and some tolerance h ∈ [0,1], we define the classification
matrix R∗ by (R∗ ) =1 if M ≥p+h, (R∗ ) =0 if M ≤p+h, and (R∗ ) =NA otherwise.
p,h p,h ij ij p,h ij ij p,h ij
In tournament problems, the matrix R∗ encodes the set of games (i,j) such that the probability
p,h
M that i beats j either exceeds p+h or are below p−h. In crowdsourcing problems, R∗
ij p,h
encodes the sets of experts/question such that the probability of obtaining the right answer is
above (resp. below) p+h (resp. p−h). For instance, in tournament problems, finding R∗ is
p,h
∗
relevant for betting purposes. In crowdlabeling applications, the knowledge of R is important
p,h
to assign experts/workers to given tasks/questions. Our main objective in this paper is to recover
∗
the matrix R from noisy and partial observations of M. In the sequel, we refer to this problem
p,h
∗
as the classification problem R .
p,h
Given an estimator Rˆ of R∗ we quantify its error by
p,h p,h
L [Rˆ ]= ∑ ∣R∗ −Rˆ ∣ . (1)
0,1,NA p,h p,h p,h
(i,j)∶(R p∗ ,h) ij∈{0,1}
This loss simply counts the number of classification error among the entries which are outside the
∗
region of tolerance. Estimating the classification matrix R is, in some sense, a weaker problem
p,h
than estimating the full matrix M in Frobenius norms. Indeed, given an estimator Mˆ we can
define the plug-in estimator R (Mˆ) by [R (Mˆ)]=1 . Then, one can easily deduce that
p p Mˆ ij≥p
1
L [R (Mˆ)]≤ ∥Mˆ −M∥2 . (2)
0,1,NA p 4h2 F
If M is an SST matrix, it is well known that the optimal rate of convergence for ∥Mˆ −M∥2 with
F
full observations is of the order of n [13], so that from an information-theoretical point of view,
∗
it is possible to classify the large entries, that is to estimate R , with a loss of the order of
p,h
n/h2. However, as explained above, no polynomial time-estimator of M achieves the n rate in
Frobenius norm and, as of today, the best achievable rate is of the order of
n7/6
[19, 24]. The
above observations raise the two following important questions:
(a) In crowdsourcing and tournaments problems, what is the optimal error for classification? In
particular, for tournament problems with full observation, is this optimal rate of the order
of n/h2 as suggested by (2)?
(b) Is there a polynomial time algorithm achieving this rate?
1.2 Our contribution
In this manuscript, we answer by the affirmative to both questions by characterizing the optimal
∗
rate for estimating the classification R and by introducing a new computationally efficient and
p,h
2statistically optimal estimator. In contrast, relying on available state-of-the-art polynomial time
ranking estimators such as those in [19, 23] would lead to a larger loss by a multiplicative factor
at
(n∨d)1/6.
As a notable consequence of our results, we establish the absence of a computational gap for
reconstructionM inFrobeniusdistancewhenthematrixM isrestrictedtotakeafinitenumberof
values. This entails that the conjectured computation gap for SST matrix estimation, if it exists,
only arises for multi-scale matrices M.
From a technical perspective, we introduce a novel procedure to estimate the permutations π
(and possibly η). The general idea is to iteratively localize the level set at height p of the matrix
M. Intuitively,ateachstep,weconsidergroupsE ofrowsthatremaintobecomparedandranked,
and based on our partial knowledge of the ranking, we select an envelope Q of columns, that is a
small set Q, such that, for the rows E, the threshold p is achieved on the set Q. Then, by looking
at the noisy observations of M on E×Q, we are able to gain additional knowledge on the mutual
ordering of the rows in E. While the idea of iteratively refining a partial ordering on the rows by
further localizing the columns of interests is not new [19, 24], there are some important differences
both in the algorithm and its see analysis. See Section 6 for further discussion.
1.3 Organization and notation
In Section 2, we formally introduce the observation model, and we also reduce the problem of
∗
estimating R to that of estimating the permutations π and η with respect to a suitable loss
p,h
L . In Section 3, we describe our main results, whereas Section 4 and Section 5 are dedicated
p,h
to the description of our polynomial time estimator procedure. Finally, we discuss our results and
furthercomparethemtotheliteratureinSection6. Alltheproofsarepostponedtotheappendix.
Inthesequel,wewrite[n]fortheset{1,...n}. WewriteS forthesetofpermutationsof[n].
n
Besides, id[n] stands for the identity permutation. For a permutation π on [n], we say i is below j
if π(i)<π(j). Since our work is motivated by crowdsourcing problems, expert i henceforth refers
to the i-th question of M and question j to the j-th column of M. Given σ > 0, a mean-zero
random variable is said to be σ2-subGaussian if it satisfies E[exp(tW)] ≤ exp(t2σ2)/2 for t ∈ R.
We write SG(σ2) for the class of centered σ2-subGaussian distributions. Given a matrix M, we
write∥M∥ foritsFrobeniusnorm. Fortwoquantitiesxandy,x∨y andx∧y respectivelyreferto
F
max(x,y) and min(x,y). We write ⌊⋅⌋ and ⌈⋅⌉ for the lower and upper integer parts. Also, log (x)
a
refers to log(x)/log(a). Finally, x ≲ y, means that there exists a numerical constant c such that
x≤cy.
2 Preliminaries
2.1 Problem formulation
We consider, without loss of generality, the general crowdsourcing setting where M
∈[0,1]n×d,
the
tournamentsettingbeingaspecificcasewheren=dandwehaveskew-symmetry. Inthesequel,we
write C Biso(id[n],id[d])⊂[0,1]n×d the collection of bi-isotonic matrices, that is matrices satisfying
the inequality M
ij
≥max(M i+1j,M ij+1). Given some unknown permutation π∈S
n
and η∈S d, we
assume henceforth that the matrix M π−1,η−1 defined by (M π−1,η−1) ij =M π−1(i)η−1(j) is bi-isotonic.
Forfixedπandη,wewriteC (π,η)forthecorrespondingcollectionofmatrices. Finally,wewrite
Biso
C ∶=∪ C (π,η) for the collection of bi-isotonic matrices up to two permutation. Similarly,
Biso π,η Biso
we define C′ (π) the set of matrices M such that, up to the permutation π, M is non-increasing
Biso
on each row and non-decreasing on each column. Equipped with this definition, we can define the
collectionofstronglystochastictransitive(SST)matricesasC =⋃C′ (π)∩{M ∶M+MT =1}.
SST Biso
As usual in the literature –e.g. [17]–, we use the Poissonization trick to model the partial
observations on the matrix M. Given some λ > 0, which is henceforth referred as the sampling
0
effort, and N =λ nd, we have N′∼Poi(N) observations of the form
0
Y′=M +W′
, (3)
t itjt t
3for t ∈ {1,...,N′}, where the sequences I = (i ) ,J = (j ) are independent and uniformly dis-
t t t t
tributed on [n] and [d], and where (W′) is an independent σ2-subGaussian noise for σ>0. The
t t
data in this model is therefore of the form (N′ ,I,J,Y′), where Y′ = (Y′) . In particular, the
t t
′
observation model (3) allows for binary observations where Y is a Bernoulli random variable with
t
parameter M in which case we have σ2=1/4.
itjt
With this observation scheme, each entry (i,j) of M is observed in a noisy way in expectation
λ times. If λ is much smaller than 1, λ is to be interpreted as the probability of observing any
0 0 0
given entry and we do not have any observation on most entries, making the ranking task more
′
challenging. The assumption that the total sample size N is distributed as a Poisson random
variable is questionable, but as usual for this type of problems, this can be leveraged quite easily.
We mostly keep it here for the sake of simplicity.
Recallthat, forp, h∈[0,1], werefertopasthethreshold andhasthetolerance. Inthesequel,
given some observations
(N′ ,I,J,Y′)
sampled from (3), our objective is to infer the classification
∗
matrix R .
p,h
2.2 Permutation loss and reduction
∗
ForestimatingR ,themainchallengeistosuitablyestimatetheranking,thatisthepermutations
p,h
π and η such that M ∈C (π,η). For any estimators πˆ and ηˆ, we define the loss L (πˆ,ηˆ) as
Biso p,h
L p,h(πˆ,ηˆ)∶=∣{(i,j)∈[n]×[d]∶ M π−1(i)η−1(j)≤p−h, M πˆ−1(i)ηˆ−1(j)≥p+h}∣
+∣{(i,j)∈[n]×[d]∶ M π−1(i)η−1(j)≥p+h, M πˆ−1(i)ηˆ−1(j)≤p−h}∣ . (4)
Note thatthis loss depends on thetrue value of thematrix M. It countsthe number of timesthat
entries smaller than p−h get confused with entries larger than p+h, if we sort M by πˆ and ηˆ
instead of the oracle permutations π and η.
In previous works where the focus was to estimate M in Frobenius norm, the overall challenge
was to estimate the permutations π and η [17, 19] with respect to the stronger loss L defined by
F
L F(πˆ,ηˆ)∶=∥M πˆ−1(.),ηˆ−1(.)−M π−1(.),η−1(.)∥2
F
, (5)
which measures how close the matrix re-ordered by the estimated permutations is to the perfectly
re-ordered matrix. Obviously, we have L (πˆ,ηˆ)≤(4h)−2L ((π,η),(πˆ,ηˆ)), so that it suffices to
p,h F
bound the latter to control the former loss. However, as alluded in the introduction, we are able
in the next section to craft polynomial time algorithms whose performances with respect to L
p,h
are much better than what is suggested by the previous bound.
3 Main results
Ourmaincontributionistheconstructionofapolynomialtimeestimatorofthepermutation(πˆ,ηˆ),
that turns out to be optimal in the minimax sense with respect to loss L . In turn, this allows
p,h
∗
us to easily derive an optimal estimator classification matrix R . To ease the reading, we mainly
p,h
state risk bounds in this section and we postpone the definition of our procedures to the next two
sections.
3.1 Minimax lower bound
We first state a minimax lower bound both for the permutation estimation problem with the loss
L and for the classification matrix R∗ , with respect to the loss L . For the purpose of
p,h p,h 0,1,NA
the following lower bound, we write E for the expectation with respect to the data (N′ ,I,J,Y′)
M
sampled with a given λ and M, and where the noise in (3) is normally distributed with variance
0
σ2.
4Theorem 3.1. There exist universal constants c,c′ ,c′′ >0, such that the following holds for any
σ>0, λ , p∈[0,1], and h∈(0,min(p,1−p)), and n, d such that n∨d≥2. If λ h2≤cσ2, then
0 0
inf sup E [L [Rˆ ]]≥[c′
σ2
(n∨d)]∧(nd) .
Rˆ
p,h
M∈C
Biso
M 0,1,NA p,h λ 0h2
inf sup E [L (πˆ,ηˆ)]≥[c′′
σ2
(n∨d)]∧(nd) .
πˆ,ηˆ M∈C
Biso
M p,h λ 0h2
The condition λ h2 ≤ cσ2 is really mild, because in the most relevant setting, we have λ < 1
0 0
(lessthan1observationinexpectationperentry)andσ oftheorderofaconstant,asforBernoulli
observations. In the regime where λ <1, Mao et al. [17] introduced exponential-time least-square
0
type estimators Mˆ and (π˜,η˜) that satisfy
E[∥Mˆ
−M∥2]≤(cσ2log3/2(nd)n∨d
)∧nd ,
F λ
0
E[∥M π˜−1,η˜−1−M π−1,η−1∥ F]≤(c′
σ2log3/2(nd)n∨d
)∧nd ,
λ
0
for some universal constants c,c′ > 0. We have explained in the previous subsection how we can
deduce bounds with respect to the losses L and L from the above equations. This implies
0,1,NA p,h
that Theorem 3.1 is tight, up to polylogarithmic terms, and characterizes the minimax risk for
reconstructing the classification matrix.
3.2 Permutation and classification matrix estimation
In order to estimate π and η, we introduce in the next section a polynomial time estimator πˆ ,
S
whichdependsonp,handσ2 andatuningparameterδ∈(0,1),whichcorrespondstoaprobability
of error.
Theorem 3.2. There exist two universal constants c,c′ >0 such that the following holds for any
δ>0, σ>0, λ ∈(0,log(nd)], p∈[0,1], h∈[0,1], π∈S , η∈S , M ∈C (π,η). With probability
0 n d Biso
higher than 1−δ, the estimator (πˆ ,ηˆ ) –defined in (13)– with tuning parameter δ satisfies
S S
L (πˆ ,ηˆ
)≤c(σ2∨1)log5/2(nd/δ)n∨d
,
p,h S S λ h2
0
If we fix δ=1/(nd), we also have
E[L (πˆ ,ηˆ
)]≤(c′(σ2∨1)log5/2(nd)n∨d
)∧(nd) . (6)
p,h S S λ h2
0
Intheabovetheorem, weareonlyassumingthatthesamplingeffortλ ≤log(nd)sothatthere
0
are in expectation less than log(nd) observations per entry. As the sparse case λ <1 is arguably
0
the most relevant, this is not really restrictive. In fact, we would need to use a variant of our
procedure to better handle the case of very large sampling effort (λ ≥ log(nd)); we restricted
0
ourselves to the sparser case for the sake of conciseness.
Comparing(6)withTheorem3.1,weobservethattheestimator(πˆ,ηˆ)is,uptoapolylogfactor,
minimax optimal for any n, d, λ ≤ log(nd) and for any noise level σ bounded away from zero.
0
Also,Theorem3.2handlesthecaseofsparseobservations,theconvergenceratesbeingoptimaland
non-trivial for λ as small Polylog(nd)/[h2(n∧d)], which corresponds to the challenging situation
0
where there are a logarithmic number of observations on each row (resp. column) if n ≥ d (resp.
n≤d).
Wehavestatedtheprevioustheoremforasinglethreshold/tolerance(p,h),butinfact,wecan
construct our estimator for a collections (p,h)=(p ,h ),...,(p ,h ).
1 1 m m
Corollary 3.3. Thereexistsanuniversalconstantc>0suchthatthefollowingholdsforanyδ>0,
σ > 0, λ ∈ (0,log(nd)], (p,h) = (p ,h ),...,(p ,h ), M ∈ C . With probability higher than
0 1 1 m m Biso
51−δ, the estimator (πˆ ,ηˆ ) defined in (13) with (p,h)=(p ,h ),...,(p ,h ) and δ/m satisfies,
S S 1 1 m m
simultaneously for all l=1,...,m.
L (πˆ ,ηˆ
)≤[c(σ2∨1)log5/2(ndm )n∨d
]∧(nd) .
pl,hl S S δ λ h2
0 l
In comparison to the single choice of threshold, tolerance (p,h) (Theorem 3.2), we only pay a
mildlogarithmicpricewithrespecttothenumbermofthresholds.
For(p,h)and(p′ ,h′)suchthat
[p−h,p+h]⊂[p′−h′ ,p+h′] and for any (π′ ,η′) we have, by definition, L p′,h′(π′ ,η′)≤L p,h(π′ ,η′).
This allows us to get a simultaneous control over all losses L as explained in the following
p,h
remark.
Remark 3.4. By building a regular grid of thresholds p of width 1/(nd), and for each threshold,
a dyadic grid (1/2,1/4,1/8,...,1/2⌈log 2(nd)⌉) of tolerance, we deduce from the above corollary that
(πˆ ,ηˆ ) defined in (13) with this choice of (p,h) and δ/(ndlog (nd)), satisfies, with probability
S S 2
higher than 1−δ,
L (πˆ ,ηˆ
)≤[c′(σ2∨1)log5/2(nd )n∨d
]∧(nd) ,
p,h S S δ λ h2
0
simultaneously for all (p,h)∈[0,1], where c′>0 is a universal constant.
As a consequence, the above estimator of the permutations turns out to be simultaneously
optimal over all thresholds and tolerances. Finally, we describe in Section 5 how to deduce a
∗
polynomial time estimator of the level set R .
p,h
Theorem 3.5. There exists a universal constant c>0 such that the following holds for any σ>0,
λ ∈[0,log(nd)], p∈[0,1], h∈[0,1], and M ∈C . The estimator Rˆ defined in (19) satisfies
0 Biso p,h
E[L [Rˆ
]]≤c(σ2∨1)log7/2(nd)n∨d
,
0,1,NA p,h λ h2
0
InlightofTheorem3.1thepolynomialtimeclassificationestimatorRˆ is,uptopolylogarith-
p,h
mic terms, minimax optimal for the bi-isotonic classification problem.
4 Description of the ranking algorithm
The procedure that we present in this section is aiming at reconstructing a specific level set of the
matrix. Afundamentalstepinitistoestimatearankingoftheexperts/questions. Thisisdonein
Algorithm SoHLoB, which is our main algorithmic contribution. Then based on the permutations
outputtedbySoHLoB,were-orderthematrixandperforminferenceofthelevelsetofthereordered
matrix. In what follows, we first present the intuition behind SoHLoB, then describe in details the
procedure itself.
4.1 Intuition behind SoHLoB
SoHLoB outputs an estimator of the true permutation of the experts. The algorithm itself is
involved and makes recursive calls to several routines. We describe here the intuition behind
this algorithm and behind the optimal error bound σ2(n∨d)/(h2λ ). For the purpose of this
0
subsection, we assume that the matrix M only takes two values, namely {p−h,p+h}. Also,
we focus on the specific problem of estimating the permutation π of the set [n] of experts. The
intuitionspresentedherewillhoweverremainvalidinthegeneralsetting. Fromabroadperspective,
Algorithm SoHLoB recursively refines an ordered partition of the experts, and transforms it in the
end into a permutation. This refinement is done by a subroutine which, given a set E ⊂[n] of the
partition, splits it into an ordered partition.
6Q (E )
∗ 1
E
1
Q (E )
∗ 2
Q (E)
∗
Q ∗(E 3) E 2
Q ∗(E 4) E 3
E
E
4
(a) Some set of questions E ⊂ [n] and the (b) After several halving steps, we end up
corresponding set of questions of interest with an ordered partition (E ) of [n]
i i=1,2,3,4
Q∗(E), defined in Equation (7). suchthatthecorrespondingquestionsofinterest
(Q∗(E )) are small.
i i=1,2,3,4
Figure 1: Illustration of two bi-isotonic matrices M ∈ C Biso(id[n],id[d]) so that π = id[n] and
η=id[d]. These matrices take two values p+h (red) and p−h (blue).
Active set Q∗(E) of questions for a set E of experts. ForpartitioningthesetE ofexperts,
one needs to compare the corresponding rows of the matrix M on suitable questions. In our case,
the only relevant subset of questions to compare experts in E is
Q∗(E)∶={j ∈[d]∶ maxM ≥p+h, minM ≤p−h} , (7)
ij ij
i∈E i∈E
i.e. the questions where the experts within E differ around the threshold p - see Figure 1a. In the
absence of noise, one would like to split E, using
Q∗(E),
into two parts, one containing the best
half of the experts, and the other containing the worst half. Recursively adding this bisection to
the ordered partition, one would ultimately end up with the true permutation π. Note however
that since we observe noisy versions of M, we will only be able to estimate - albeit imperfectly -
Q∗(E)
and subsequently split E if both E and
Q∗(E)
are large enough compared to the size of h,
namely if
∣Q∗(E)∣≳
σ2
. (8)
h2λ
0
Indeed, if
∣Q∗(E)∣≲ σ2
, then it is impossible to compare any two experts i and
i′
in E, because
corresponding rows
oh f2λ M0
differ by 2h on at most
∣Q∗(E)∣
entries. Hence, the signal differs by 2h
on atmost σ2/h2 observations, whichmakes it impossible to compare i and i′ when thenoise level
is σ.
Intuitionbehindtherateσ2(n∨d)/(λ h2). Consideranidealizedsituation,where,foreachE,
0
an oracle gives us
Q∗(E)
and, if
Q∗(E)
satisfies (7), the oracle also provides us a perfect bisection
of E into two sets O and I such that, for any i∈O and i′∈I, we have π(i)<π(i′). This idealized
procedure would end up in providing an ordered partition of the experts (E ,...,E ) such that
1 H
• the ordered sets E l and E l+1 are such that, for i∈E l and i′∈E l+1, we have π(i)<π(i′).
• each set E has the property that either ∣E ∣=1 or ∣Q∗(E )∣≲σ2/(h2λ ).
l l l 0
SeeFigure1b. Theestimatedpermutationπˆisthentakenasbeinganypartitionwhichisconsistent
′
withtheorderedpartition. Notethatforasuchpermutationπ ,theonlyentriesonwhichitmight
induce an error with respect to L (πˆ,η) are those contained in the union of E ×Q∗(E ). Hence,
p,h i i
the error L (πˆ,η) is bounded by
p,h
∑∣E ∣∣Q∗(E )∣≲σ2 n .
i i h2λ
i 0
7Bysymmetry,wewouldgetaboundoftheorderoftheorderσ2 d forη. Notethatthismatches
h2λ0
the minimax error bound, see Theorem 3.1, as well as the error bound of our procedure, see
Corollary 3.3.
The above description streamlines the structure of Algorithm SoHLoB, as well as its analysis.
However, since there is noise, the algorithm as well as its analysis are much more complicated, in
the three following directions.
• We need to estimate Q∗(E) based on noisy data. This will lead to errors on the set of
questions we use for comparing experts in E.
• Even if we were equipped with oracle knowledge of Q∗(E), we would need to select relevant
subsetsofQ∗(E)inordertocompareagivenpairofexpertsinE.
Indeed,twogivenexperts
in E may not significantly differ on all the questions
Q∗(E),
but just on a small subset of it.
• Even if we were equipped with an oracle knowledge on which questions are most useful
for comparing two given experts in E: we would still end up performing a noisy comparison
betweenthesetwoexperts. WewillthereforenotbeabletoperfectlydivideE intwoordered
sets, and need to take into account these mistakes in the final partition.
Dealing with each of these problems presents a challenge of its own, which we describe in more
details in the full description of the algorithm below. We emphasize in particular the second
challenge in Subsection 6.2, which is the one that demanded the most innovative algorithmic and
analytical innovations, and is a main highlight of this paper.
4.2 Definition of the estimators and preliminaries
In this section, we introduce our new ranking procedure SoHLoB and we describe the estimators
πˆ and ηˆ . Let
S S
k∗=3⌈log 2(n∨d))⌉ . (9)
The procedure SoHLoB takes as inputs as sequence (p ,h ),...,(p ,h ) of thresholds and tol-
1 1 m m
erance, some quantity δ ∈ (0,1) which corresponds to some probability of error, and k∗ noisy
observations of the matrix M. For that purpose, we use a simple subsampling strategy to build
these k∗ matrices from the sample.
Subsampling and definition of πˆ and ηˆ . Recall that we have at our disposal the data
S S
(N′ ,I,J,Y′)
which stands for a noisy sample of entries of the matrix M (see Section 2.1). Using
the Poissonization trick, we easily define k∗ estimators of M as follows. For
t=1,...,N′
, we first
sample independently a random variable U
t
uniformly in [k∗]. Then, for each i ∈ [n], j ∈ [d],
k∈[k∗], we define
N(k) ∶=∣{t≤N′∶ I =i, J =j, U =k}∣ , (10)
ij t t t
as the number of observation in the k-th sample that fall within (i,j). Then, the observation
matrix Y(k)∈Rn×d is simply defined, for any (i,j), by
Y(k) ∶= 1
∑N′
1{I =i, J =j, U =k}⋅Y′ , (11)
ij
N
i( jk)
t=1
t t t t
with the convention 0/0 = 0. By the Poissonization trick, the matrices Y(k) are i.i.d. and the
N(k) are distributed as independent Poisson random variables with parameters λ− = λ0 .
ij 0 3⌈log (n∨d)⌉
2
Besides, if
N(k)
=0, then
Y(k)
=0. Conditionally to
N(k)
>0, we have
ij ij ij
Y(k)
=M
+W(k)
, (12)
ij ij ij
where the
W(k)
are i.i.d. in k, independent in i,j, and σ2-subGaussian. The probability that
ij
N
i( jk)
>0 is equal to λ
1=1−e−λ−
0.
8Then, the ranking estimator πˆ ∶=πˆ (N′ ,I,J,Y′ ;δ) is defined by
S S
πˆ (N′ ,I,J,Y′ ;δ)∶=SoHLoB[(Y(1) ,...,Y(k∗));p,h,λ ,δ] . (13)
S 0
To order the columns of M, we simply apply the same procedure to the noisy observation of MT.
This leads us to ηˆ (N′ ,I,J,Y′)∶=SoHLoB[(Y(1)T,...,Y(k∗)T);p,h,λ ,δ].
S 0
Preliminaries. Given the parameters p, h, σ2, and δ, we define the quantities ρ and γ.
√
ρ∶=(1∨σ)e 8log(24nd(n∨d)1/2⌈log (n∨d)⌉/δ) ; (14)
2
γ ∶=2log(24nd(n∨d)1/2⌈log (n∨d)⌉/δ)/λ e2 . (15)
2 1
where e=exp(1). Here, we have
ρ≍(1∨σ)log1/2(nd/δ)
and γ ≍log(nd/δ)/λ . In the sequel, we
1
say that E =(E ,...,E ) form an ordered partition of [n] if the collection (E ,...,E ) of subsets
1 r 1 r
is a partition of the set [n] of rows. We say that a permutation π is compatible with E, if for any
i∈E and j ∈E with r<s, we have π(i)<π(j). Given an ordered partition E, Permutation(E)
r s
standsforanypermutationπ (possiblyrandom)thatiscompatiblewithE. Notethatconstructing
such π from a given E can be done in linear time.
4.3 The hierarchical sorting tree
From a broad perspective, the general structure of SoHLoB (in Algorithm 1) is that of iteratively
building a hierarchical sorting tree. We start with the trivial ordered partition E =([n]). In the
first step, we build in Line 20 a Trisection of [n] into three sets (O,P,I) where O ⊂ [n] (resp.
I) is, with high-probability, made of rows i, such π(i) is below (resp. above) the median of [n],
that is π(i)≤n/2 (resp. π(i)≥n/2) and P contains the remaining rows for which we cannot draw
any significant decision. This leads us to E = (O,P,I). Then, at the later steps, we recursively
construct the ordered partition. Together with E, we maintain a vector v ∈{0,1}∣E∣ where v =0
k
encodesthatthesetE isnottoberefinedanymore. ThisiseitherthecasebecauseE arisesfrom
k k
asetP ofindecisiverowsinprevioussubsection,orbecause∣E ∣issmallcomparedlog(nd)/(h2λ )
k 1
–see Line 21 in Algorithm 1 or the explanation below (8)– or if the sets Q of questions associated
to E are too small for (p ,h ) –see Line 17 and the explanations below (8). See Figure 3 in
l l
Appendix C.5 for a visual representation of the hierarchical sorting tree. The algorithm stop after
less than ⌈log (n)⌉ iterations when none of the sets are to be refined. At the end (Line 27), we
2
simply compute any permutation π which is compatible with the final ordered partition E (see
Section B.5 for a formal definition). Using hierarchical sorting trees has already been proposed
in [19, 23] and, more generally, recursive approaches for ranking problems have already been put
forward in [17].
The procedure SoHLoB mostly differs from those earlier works [10, 17, 19] in three ways. First,
aside from the hierarchical sorting tree, SoHLoB records a comparison graph G ∈ {0,1}n×n. The
procedure initializes with G=0 n×n. Within the subroutine ScanAndUpdate, we update the com-
parison G ii′ to 1, if we find statistical evidence that
π(i)<π(i′).
In the main algorithm, the steps
in Lines 10–16 amounts to find as many statistically significant comparisons as possible within E.
Then, in Line 20, we trisect E into (O,P,I) using the graph G –see Algorithm 2. If, according to
′
G, the rank of i is below (resp. above) at least half of the experts i in E –and is therefore below
(resp. above) the median of E– we put i in O (resp. I). Then remaining experts are put in the
indecisive set P. Hence, the statistical efficiency of the procedure mostly depends on the way we
construct G. For that, there are two main ingredients. First, the subroutine Envelope (Line 11)
that selects a set Q of questions which, with high probability, contains Q∗ (E), which is defined
pl,hl
in (7). As explained in Subsection 4.1, this set is instrumental towards comparing experts in E.
Second, the subroutine ScanAndUpdate compares experts based on partial row sums on subsets
Q. This subroutine differs from the idealized examples in Section 4.1 in that, it does not simply
compare the row sums restricted to Q on the set E, but also selects many relevant subsets of Q.
These two subroutines are described in the next subsections.
In the proof of Theorem 3.2 –more specifically in that of Theorem A.2– we will show that, on
aneventofprobabilityhigherthan1−δ/2,atallthestepsofthealgorithms,theenvelopesetQwill
9Algorithm 1 SoHLoB
Require: samples (Y(k)) , p=(p ,...,p ), h=(h ,...,h ), λ , δ.
k=1,...3⌈log (n)⌉ 1 m 1 m 0
2
Ensure: permutation π of [n] and directed graph G
1: initialize G←0 n×n, k←0, E ←([n]), v←(1{λ 1n>4ρ2/h2})
2: while v≠0 do
3: k←k+1, E˜←(), v˜←()
4: for t from 1 to ∣E∣ do
5: E ←tth set in E
6: if v t=0 then ▷ means that the algorithm decided in an earlier step not to refine E
7: E˜←(E˜,E), v˜←(v˜,0)
8: else
9: u←0
10: for l from 1 to m do
11: Q← Envelope(t,E,v,Y(3k−2) ,p l,h l,λ 0,δ) ▷ estimates questions of interest
w.r.t. E
12: if λ 1∣Q∣>4ρ2/h2 then ▷ Q large enough, further refinement of E possible
13: u←1
14: ScanAndUpdate(Y(3k−1) ,Y(3k) ,E,Q,G,λ 0,δ) ▷ update of G
15: end if
16: end for
17: if u=0 then ▷ all envelopes are small, no further improvement needed
18: E˜←(E˜,E), v˜←(v˜,0)
19: else
20: (O,P,I)←GraphTrisect(E,G)
21: E˜←(E˜,O,P,I), v˜←(v˜,1{λ 1∣O∣>4ρ2/h2},0,1{λ 1∣I∣>4ρ2/h2)} ▷ no further
refinement of P or I if those are small enough
22: end if
23: end if
24: end for
25: E ←E˜, v←v˜
26: end while
27: Return (Permutation(E))
Algorithm 2 GraphTrisect
Require: E ⊆[n], graph G∈{0,1}n×n
Ensure: trisection O,P,I of E
O←{i∈E ∶ ∑ i′∈EG ii′ >∣E∣/2} ▷ π(i)<π(i′) detected for the majority of i′∈E
I ←{i∈E ∶ ∑ i′∈EG i′i>∣E∣/2} ▷ π(i)>π(i′) detected for the majority of i′∈E
P ←E∖(O∪I)
contain the oracle envelopes Q∗ (E) and that a trisection (O,P,I) computed by GraphTrisect
will be such that, for any i
inp Ol,hl
and i′ ∈ I we have π(i) < π(i′). However, it does not preclude
′
some expert i in P to have a true rank below some of O or above some of I. In summary, the
estimatedpermutationπˆ makeserrorsbecauseinthefinalorderedpermutationE =(E ,...,E ),
S 1 r
(i) within any set E , the E ’s are ordered arbitrarily and (ii) because, for some (i,i′) with i∈E
s s s
and i′ ∈ E s′ with s < s′ we have π(i′) < π(i). This second problem arises because of the sets P
in the trisection steps. The cornerstone of the proof amounts to proving the loss of arising from
these errors is small.
4.4 The ScanAndUpdate routine
As indicated above, the purpose of the ScanAndUpdate routine is to update the comparison graph
GontheexpertsinE. ScanAndUpdatetakesasaparameterasubsetQofquestions–thathasbeen
′
computed using the Envelope routine– and builds many possibly relevant subsets Q of questions
(to be discussed below). Given such a subset
Q′
of questions, and some matrix
Y˜(b)
sampled
accordingto(12), itcallstheroutineUpdateGraph(Algorithm4)thatcomparesinLine3, forany
i,
i′∈E, thepartialrowsumsofY˜(b) restrictedtothecolumnsinQ′
.
SinceY˜(b)
isclosetoM and
10sinceM ∈C biso,whenthepartialrowsumofiissignificantlylargerthanthatofi′ ,weca√nconclude
that π(i) < π(i′) and we update the comparison graph to G ii′ = 1. The thresholds 2ρ λ 1/Q′ in
Line 3 has been chosen in such a way that we draw any false conclusion with extremely small
probability. ScanAndUpdate performs all possible ∣E∣(∣E∣−1) comparisons. Note that the partial
′
row sums are computed only if Q is large enough (Condition in Line 1), otherwise we may not
have enough observations on
Y˜(b)
to draw statistically significant conclusions.
′
Let us now further discuss the construction of all the sets Q in Lines 2, 5, and 7. We do not
simply use Q to compare experts in E, as Q - which estimates
Q∗(E)
- aims at selecting any
questions on which two experts in E disagree. But in order to compare a given expert i∈E with
other experts in E, it is important to refine Q and adapt it to questions that are most relevant to
′
i. The choice of sets Q reflect this - see Subsection 6.2 for a more in depth explanation.
For each j ∈Q, the algorithm uses partial columns sums of Y˜(a) with respect to E to construct
sets Q′ ⊆Q that contain j′ ∈Q for which we can detect based on data that either η(j′)<η(j) or
η(j′) > η(j); or for which such a detection based on these partial columns sums is not possible.
In our construction, we restrict the deviation of the partial columns sums by different choices of a
′
parameter c. This gives us an implicit control of the size of the Q.
Algorithm 3 ScanAndUpdate
Require: E ⊆[n], Q⊆[d], Y˜(a) ,Y˜(b)∈Rn×d, graph G∈{0,1}n×d, λ , δ
0
Ensure: Update of G
1: for j ∈Q do √
2: Q′←{j′∈Q∶ ∣ ∣E1 ∣∑ i∈EY˜ i( ja) −Y˜ i( ja ′) ∣≤2ρ λ 1/∣E∣} ▷ questions j′ with η(j′) close to η(j)
3: UpdateGraph(G, √E,
Q′
,
Y˜(b)
,λ 0,δ)
4: for c=2,3,...,⌈ λ 1√∣E∣/(2ρ)+1⌉ do
√
▷ try different margins
5: Q′←{j′∈Q∶ 2ρ λ 1/∣E∣≤ ∣E1 ∣∑ i∈E[Y˜ i( ja ′) −Y˜ i( ja) ]≤2cρ λ 1/∣E∣} ▷ questions j′ with
η(j′)<η(j)
within the given margin
6: UpdateGraph(G, √E,
Q′
,
Y˜(b)
,λ 0,δ)
√
7: Q′←{j′∈Q∶ 2ρ λ 1/∣E∣≤ ∣E1 ∣∑ i∈E[Y˜ i( ja) −Y˜ i( ja ′) ]≤2cρ λ 1/∣E∣} ▷ questions j′ with
η(j′)>η(j)
within the given margin
8: UpdateGraph(G, E,
Q′
,
Y˜(b)
, λ 0, δ)
9: end for
10: end for
Algorithm 4 UpdateGraph
Require: graph G∈{0,1}n×n, E ⊆[n], Q′⊆[d], Y˜(b)∈Rn×d, λ , δ
0
Ensure: update of G
1: if ∣Q′∣>γ then ▷ required for detection based on partial row sums
2: for i,i′∈E do
√
3: if ∣Q1 ′∣∑ j∈Q′[Y˜ i( jb) −Y˜ i( ′jb) ]>2ρ λ 1/∣Q′∣ then ▷ criterion for detecting π(i)<π(i′)
4: G ii′ ←1
5: end if
6: end for
7: end if
4.5 Selecting relevant questions with Envelope
We emphasized in Subsection 4.1 the importance of reducing the set of questions from [d] to
Q∗ (E) defined in (7). The purpose of Envelope is to build a set Q, whose size is as small
p,h
as possible, and that contains Q∗ (E) with high probability. We can decompose Q∗ (E) =
p,h p,h
Q∗ (E)∩Q∗ (E).
p,h p,h
Q∗ (E)={j ∈[d]∶ maxM ≥p+h} , Q∗ (E)={j ∈[d]∶ minM ≤p−h} .
p,h i∈E ij p,h i∈E ij
11In Algorithm 5, the goal is to build a superset Q of Q∗ (E ) and a superset Q of Q∗ (E ). We
p,h s p,h s
focus on the latter. In Line 4, we define s as largest index t < s such that v = 1. The reason
t
why we take s and note simply s−1 is that, with high probability2, (i) the collection ∣E ∣ is large
s
enough so that partial column averages on E are statistically significant and (ii) it holds that all
s
′
i inE
s
arebelowthoseinE
s
–notethatthisisnotnecessarilythecaseforE s−1. Under(ii),forall
j ∈[d], max i′∈EsM
ij
≤min i∈ESM
ij
so that Q∗ p,h(E)⊂{j ∈[d]∶ ∣E1 s∣∑ i∈EsM
ij
≤p−h}. In Line 5,
we build Q which, with high probability, is a superset of the latter by accounting for the noise of
the observations. Finally, the routine Envelope returns the intersection of Q and Q.
Algorithm 5 Envelope
Require: index s∈[r], E =(E ,E ,...,E ), v∈{0,1}r, Y˜ ∈Rn×d, p, h, λ , δ
1 2 r 0
Ensure: Q⊆[d]
1: if s=min{t=1,2,...,r∶ v t=1} then
2: Q←[d] ▷ use all questions as “left envelope”
3: else
4: s←max{t<s∶ v t=1}
√
5: Q={j ∈[d]∶ ∣E1 s∣∑ i∈EsY˜ ij ≥λ 1(p+h)−ρ λ 1/∣E s∣} ▷ use partial column sums on E s to
detect questions uniformly ≥p+h on E , detected questions j as “left envelope”
s
6: end if
7: if s=max{t=1,2,...,r∶ v t=1} then
8: Q←[d] ▷ use all questions as “right envelope”
9: else
10: s←max{t>s∶ v t=1}
√
11: Q={j ∈[d]∶ ∣E1 s∣∑ i∈EsY˜ ij ≤λ 1(p−h)+ρ λ 1/∣E s∣} ▷ use partial column sums on E s to
detect questions uniformly ≤p−h on E , detected questions j as “right envelope”
s
12: end if
13: Q←Q∩Q
Computationalcomplexityof SoHLoB. Withasuitableimplementationthatkeepsinmemory
values of partial row sums, the total computational complexity is (up to polylogarithmic terms)
of the order of mnd(d+n), where we recall that m in the number threshold/tolerance under
consideration. Thus, SoHLoB is less than quadratic with respect to the size of the data set.
5 Classification matrix estimation
In this section, we explain how to deduce an estimator Rˆ of the classification matrix R∗ from
p,h p,h
some permutation estimator.
Given suitable estimators πˆ and ηˆ, and an independent sample
(N′ ,I,J,Y′)
it is easy to build
an estimator Rˆ of R∗ using a block averaging strategy. Define the window sizes
p,h p,h
√ √
512log(nd)n 512log(nd)d
k ∶=⌈(σ∨1) ⌉ , l ∶=⌈(σ∨1) ⌉ . (16)
h dλ h2 h nλ h2
0 0
If k ≥ n or l ≥ d, we simply define Rˆ as the constant matrix equal to 1. Else, we define
h h p,h
B as the regular grid of [n]×[d] with blocks of size k ×l . More specifically, for all (r,s) ∈
h h
[(⌊n/k ⌋)∨1]×[(⌊n/l ⌋)∨1], we define the blocks B ∶=[(r−1)k +1,rk ]×[(s−1)l +1,sl ],
h h r,s h h h h
except for the blocks such that r=⌊n/k ⌋ or s=⌊d/l ⌋ for which the blocks go up to n or d.
h h
πˆ,ηˆ
Then, the block constant matrix YB is defined by first ordering the data according to πˆ and
ηˆ and averaging over all blocks in B. More precisely, for any block B ∈ B and any (i,j) ∈ B, we
2seeAppendixC.4forprecisestatementsandproofs
12have
[YBπˆ,ηˆ ]
ij
∶=
N
B1
πˆ,ηˆ
t∑N =1′ (k∑ ,l)∈B1{((I t,J t)=(πˆ−1(k),ηˆ−1(l))}Y t′ ,
with the convention 0/0=0 and where N Bπˆ,ηˆ ∶= ∑N t=′ 1∑ (k,l)∈B1{((I t,J t) = (πˆ−1(k),ηˆ−1(l))} is the
number of observations in B. Finally, we define the estimator Rˆ by
p,h
[Rˆ p,h]
ij
=1{[Yπ Bˆ,ηˆ ] πˆ(i),ηˆ(j)≥p} . (17)
Proposition5.1. Foranyπ,η,anymatrixM ∈C (π,η),andanyestimator(πˆ,ηˆ)independent
Biso
of the sample (N′ ,I,J,Y′) of intensity λ , we have
0
⌈log 2(1/h)⌉ 3
E[L 0,1,NA[Rˆ p,h]∣πˆ,ηˆ]≤c[ ∑ ∑ 2sL p+th2s−2,h2s−2(πˆ,ηˆ)+∑ ∑ L p+3th2−s,h2−s(πˆ,ηˆ)
s=1 t∈{−1,1} s=2t∈{−1,1}
n∨d
+1+(σ2∨1) log2(nd)]∧nd . (18)
λ h2
0
Inlightoftheaboveresult,itsufficestosplitthedataintotwoindependentsamplesandusethe
first subsamples to estimate π and η in such a way that a polynomial number of ranking losses of
theformL aresmallandusethesmallsampletocontrolestimatethematrixM tohaveasmall
p,h
classification risk. More specifically, we divide the full sample
(N′ ,I,J,Y′)
into two subsamples
of intensity λ /2. We use the first subsample to build the permutations (πˆ ,ηˆ ) defined in (13)
0 S S
with the vector (p,h) of (thresholds, tolerance) arising in (18), and δ =(nd)−2. Then, we use the
second subsample to build the block constant matrix
Yπ BˆS,ηˆS.
Thus, we obtain the final estimator
[R˜ p,h]
ij
=1{[Yπ BˆS,ηˆS] πˆ−1(i),ηˆ−1(j)≥p} . (19)
S S
In Section 3, we have stated that this simple estimator is minimax optimal.
6 Extensions and discussion
6.1 Connection to the noisy-sorting literature
The noisy sorting model [9] is a specific case of SST tournament model. In its most specific form,
it subsumes the existence of permutation π of [n] such that M = 1/2+h if π(i) < π(j) and
ij
M = 1/2−h if π(i) > π(j), the diagonal of M being equal to 1/2. We write C for the
ij Noisy,h
collection of such matrices with a given h > 0. In its most general form, it is assumed that the
matrix M satisfies M ≥1/2+h if π(i)<π(j) and M ≤1/2−h if π(i)>π(j) and write C′
ij ij Noisy,h
for the collection of such matrices.
With these models, authors generally express the quality of their ranking procedure using a
distance on the space of the permutations such as the Kendall tau distance defined by
n n
d (π′ ,π)=∑∑1{π(i)>π(j), π′(i)<π′(j)} ,
KT
i=1j=1
which, is some way, measures the level of disagreement when comparing any two players with
respect to π and
π′
. Interestingly, one readily checks that, for any matrix M
∈C′
,
Noisy,h
L 1/2,h(π′ ,π′)=2d KT(π′ ,π) , (20)
wherewerecallthatL isdefinedin(4). GeneralizingtheworkofBravermanandMossel[9],Mao
p,h
etal.[10]haveshownthattheminimaxoptimalKTerroroverC′ isoftheorderofn/(λ h2).
Noisy,h 0
Conversely, they introduced a simple multi-sorting estimator πˆ which, up to polylogarithmic
MWR
terms, achieves the optimal rate, but only in the regime where h is bounded away from zero and
for restricted noisy matrices. More specifically, their estimator achieves
sup E[d (πˆ ,π)]≤c n ∧n2 , for h∈[c′ ,1/2) ,
KT MWR
M∈C Noisy,h λ 0
13′
where c and c are universal constants. Braverman and Mossel [9] –see also Shah et al. [14]–
have focused on the full observation model which roughly corresponds to λ = 1 in our context.
0
Relying on the disagreement-minimizing permutation πˆ , which can be efficiently computed,
FAS
they manage to handle the generalized noisy sorting model, but still with a margin h which is
bounded away from zero. More specifically, they obtain
sup E[d (πˆ ,π)]≤c nlog(n) for h∈(0,1/2) ,
KT FAS h
M∈C′
Noisy,h
wheretheconstantc dependsinanon-explicitwayonthevalueofh. Besides, thecomputational
h
complexity of their algorithm is exponential with respect to 1/h.
In light of (20), we can build3 an estimator πˆ from SoHLoB that achieves
S
sup E[d (π˜ ,πˆ)]≤c′ log3/2(n) n ∧n2 ,
M∈C′ KT S λ 0h2
Noisy,h
′
according to Theorem 3.2. Here, the constant c is universal. Hence, our procedure, is up to our
knowledge, the first one to achieve, in polynomial time, the optimal convergence rate with respect
tothemarginhandthesamplingrateλ ,thisonthewholeclassofgeneralizedsortingmodels. We
0
thereby answer an open question in [10]. Although the construction of πˆ requires the knowledge
S
of h, is also possible to be adaptive with respect to an unknown value h by relying on multiple
tolerance in SoHLoB –see Remark 3.4 for a more detailed discussion.
We would like to emphasize that the noisy sorting problem with margin h has unique features
whichmakeitsimplerthanotherbi-isotonicmodels. Inparticular,recoveringthepermutationπ is
equivalent to recovering the level set {(i,j),M >1/2}, this level being (up to the permutation π)
ij
completelytriangular. Itismuchsimplertorecoversuchalevelsetbecauseofallthesymmetriesin
the model: a procedure based on trisections like ours would not need to pay attention to potential
asymmetriesandimbalancesaroundthelevelsetp,whichmakeinparticularthechoiceofquestions
based on which one trisects much more complicated - see Subsection 6.2 for a discussion on this.
Inourwork,wehaveintroducedapolynomialtimeestimator,forrecoveringanylevelset(p,h)
in both the tournament and crowdsourcing model. In particular, our procedure does not require
symmetries - as in the classical noisy sorting model - nor the existence of a margin in the matrix
M as for noisy sorting problems.
6.2 Discussion of the choices of the set Q′ of questions in ScanAndUpdate
We now further discuss the ScanAndUpdate routine, as the algorithmic idea behind as well as its
analysis is one of the most innovative one in comparison to the available literature. Recall that we
construct iteratively a partition E˜of the experts [n], and we update iteratively a graph G, which
bothencodeorderinformationontheexperts. Inordertodothat,weworkateachsteponallsets
E
∈E˜,andweconstructseveralsetsofquestionsQ′
thatweuseinordertocomparetheexpertson
thesequestions-seemostly ScanAndUpdateinAlgorithm3, andalsoEnvelopeinAlgorithm5for
′
the creation of the envelope set Q based on which the Q are constructed. Note first that, for any
E ∈E˜, a natural oracle choice of the set Q′ would be to consider Q∗(E), which contains precisely
questions on which experts in E display substantial performance gaps around the threshold p, see
Equation (7). However, two distinct challenges occur:
• (i) first, Q∗(E) is not available and needs to be estimated, and
• (ii) second, while Q∗(E) is the set of questions that is most effective for distinguishing the
bestexpertinE fromtheworstexpertinE, manyquestionsinsideitmightbeirrelevantfor
′
distinguishing between two given experts i,i of E, e.g. if they lie more toward the median
of E - see below for a detailed discussion and an illustration of this important point. So
that in some situations there exist subsets Q of
Q∗(E)
that are much more informative to
0
′
distinguish between experts i and i.
3Asexplainedearlier,aSSTmatrixM isbi-isotonicuptothepermutationsπ andπ− andwhereπ− isdefined
byπ−(i)=n+1−π(i).
14Q (E)
∗
E
Q
(a)WeestimateQ∗(E)usingsomeexpertsi′<i∀i∈Eandsomeexperts
i′>i∀i∈E (depictedasredandbluerows,resp.,E correspondstothe
orrangerowsandQ∗(E)tothegraycolumns). InSection4.5,weexplain
why the corresponding dashed areas must contain Q∗(E) which yields
Q as intersection to be the output of the Envelope procedure.
E
j
(b) We explain in Section 4.4, how, for each j, nested intervals left and
right of j are constructed as candidate sets Q′. These intervals are
depicted in gray with different shades.
Figure 2: Illustration of some bi-isotonic matrix M ∈ C Biso(id[n],id[d]) so that π = id[n] and
η=id[d]. Thetwopurplecurvesdividethematricesintothreeareas: values, thatareatleastp+h
(light red background); values, that are at most p−h (light blue background); and values that are
between p+h and p−h (violet background). We construct sets Q′ , first using Envelope (2a) and
then in ScanAndUpdate (2b).
′
To handle this, we have to estimate a collection of proxies Q of Q in such a way that:
0
′
(a) atleastoneproxyQ containsa”significant”numberofelementsofQ , whileitscardinality
0
should not be too large (compared to ∣Q ∣),
0
(b) the number of elements in this collection of proxies is not too large (i.e. polynomial with
n,d).
′
The purpose of Property (a) is that comparing experts based on such Q will be as effective as a
comparison based on Q . Properties (b) also enforces that the computational complexity of the
0
procedure remains controlled.
We would like to emphasize that in classical noisy sorting - tournament setting where n = d,
wherep=1/2andwherethematrixM takesvaluesin{1/2−h,1/2+h}arounditsdiagonal-wehave
Q∗(E)=E
for any set E of experts, by symmetry. So that the problem of estimating
Q∗(E)
does
not exist in classical noisy sorting. Besides, in this noisy sorting model, computing averages based
15on
Q∗(E),
is mostly relevant for comparisons of experts in E, because of symmetries mentioned
earlier. In our setting however - i.e. either in a tournament setting where we possibly consider a
different threshold than p=1/2, or in a ranking setting - we do not have the same symmetries as
innoisysorting.
SothatQ∗(E)cannotbestraightforwardlyconstructedbasedonE
anymoreand
some subsets Q of
Q∗(E)
are perhaps more informative.
0
Inoursetting, ourmainalgorithmicandconceptualinnovation-comparedtonoisysorting[10]
and other ranking algorithms [17, 23–25] - is the construction of the sets Q′ and their analy-
sis, which turned out to be surprisingly challenging. We already explained the intuition behind
Envelope in Section 4.5. This is however insufficient in very asymmetric situations. Indeed, even
in a noiseless case where our estimation of the questions in Q would be perfect, one might have
that only a small subset of Q obtained Envelope is actually composing
Q∗(E)
- see Figure 2a
and even worse, only a small subset of
Q∗(E)
is more informative. For this reason, we go one
′
step further in the refinement of our question sets and construct several subsets Q of Q which are
candidate proxies for
Q∗(E)
- see ScanAndUpdate in Algorithm 3. The idea is to consider each
question j ∈Q. Then, based on the averages of the observations on E, we either select the subset
′ ′
ofquestionj whoseaveragesareclosetothatofj -Line2-orquestionsj whoseaveragesbelong
′
tosomenestedcollectionofbinsbelowthatofj -Line5-orquestionsj whoseaveragesbelongto
some bins above that of j - Line 7. The size of these bins has been chosen in such a way that the
error due to the noise is small compared to the respective size of these bins. See Figure 2b for an
example. In this way, we target the actual set of questions where there are most variations among
experts within E, ensuring that we are not impacted by the case where this set is actually smaller
than Q - see Figure 2a. In the theoretical analysis of our algorithm, we prove that whenever the
′
variation within E is large enough, one of these sets Q will be a relevant set of questions, namely
thatitwillbeofsmallenoughsize,yetcontainsufficientlymanyquestionswheresufficientlymany
of the experts disagree so that it can be used to effectively discriminate between the experts. This
is done by carefully crafted volumetric arguments.
6.3 Two (or several) values problem
Tosimplifythediscussion, weassumeinthissubsectionthatn≥d. Letusfirstfocusonaspecific,
yet emblematic case, where the matrix M only takes two values, say p−h and p+h - note though
that here we do not necessarily consider the noisy sorting setting. In that setting, our estimators
πˆ and ηˆ (13) satisfy the optimal risk bound from Theorem 3.2
S S
E[L (πˆ ,ηˆ )]≲log5/2(n)(σ2∨1) n .
p,h S S λ h2
0
This bound interprets as the fact that, on average, the estimated permutation πˆ wrongly ranks
S
rows i and j only if those differ by less 1/(λ h2) entries. Regarding the matrix M, in this setting,
0
it is equivalent, up to a factor h2, to reconstruct the matrix M in square Frobenius norm and to
estimate the level set R p∗ ,h. Indeed, upon defining Mˆ =p−h1 n×d+2hR˜
p,h
where R˜
p,h
is defined in
Equation 17, we deduce from Theorem 3.5 that
E[∥Mˆ −M∥2]≲log7/2(n)(σ2∨1) n . (21)
F λ
0
We know e.g. from [17] that this Frobenius risk bound is, up to logarithmic term, rate-optimal.
In fact, the above results easily extend to the case where the matrix M takes K ≥ 2 values.
Now assume that the square matrix M only takes K values (p ,...,p ) with p < p < ...p .
1 K 1 2 K
Using MultSoHLoB, we can construct (πˆ ,ηˆ ) and using block-constant estimation of the matrix
S S
in the spirit of Equation 19 we can build an estimator Mˆ achieving
E[∥Mˆ −M∥2]≲log7/2(n)K(σ2∨1) n , (22)
F λ
0
which only differs from (22) by the multiplicative factor K. In summary, our polynomial time
procedure, achieves the optimal reconstruction risk when the number of values is considered as
constant.
166.4 Narrowing the computational barrier for bi-isotonic problems
For a sake of clarity, we focus in this subsection on the square case where n=d, λ =1, and σ≤1.
0
As explained in the introduction, it is known [10, 13, 14] that the minimax risk for estimating M
in Frobenius norm is of the order n, i.e.
cn≤inf sup E [∥Mˆ −M∥2]≤c′ log2(n)n .
M F
Mˆ M∈C
biso
However,thebestavailablepolynomialtimeprocedureonlyachieves,uptopolylogterms,therisk
n7/6
[19, 24] despite a long line of research on this topic [13, 14, 26], which leads to the conjecture
of a computational barrier for this problem [17]. As already pointed out in [17], the problem of
estimating M mostly boils down to that of estimating the permutation π and η. More specifically,
if we have at hand estimators πˆ and ηˆ, we can simply estimate M using the least-square estimator
on the space of bi-isotonic matrices based on a permuted matrix of observations Y πˆ−1,ηˆ−1 - where
we do sample splitting and consider a sample Y independent of πˆ and ηˆ. Then, it is proved in [17]
that
E[∥Mˆ −M∥2 F]≤E[∥M πˆ−1,ηˆ−1−M π−1,η−1∥2 F]+cnlog2(n) .
AsimilarresultholdsifwerestrictourattentiontoSSTmatrices. Asaconsequence,theexistence
of a computational barrier for Frobenius estimation of bi-isotonic matrices up to permutations
is equivalent to the existence of the barrier for the problem of permutation estimation - namely
estimation of π and η - with respect to the Frobenius-type losses L defined in (5).
F
In this work and more specifically in Remark 3.4, we have introduced a polynomial time-
estimator (πˆ ,ηˆ ) achieving, for all (p,h)∈(0,1), the risk bound
S S
E[L (πˆ ,ηˆ )]≤c′ log5/2(n) n , (23)
p,h S S h2
thereby showing the absence of computation-information gap for simultaneous inference with re-
spect to all the weaker losses L . This has an important consequence. As underlined in Sub-
p,h
section 6.3, the statistical-computational gap disappears when the matrix M only takes a (small)
finite number of values. This implies that the conjectured computation barrier for estimation of
SST and, up to permutations, bi-isotonic matrices can only arise for multi-scale matrices taking a
polynomialnumberofdifferentvalues. Areasonforthedifferencebetweenthegeneralsetting,and
with the finite number of values for setting, is as follows. In general, one should not consider the
reconstruction of each level set separately - as we do in our procedure SoHLoB. This is sufficient if
M takesasmallnumberofdifferentvalues. Ingeneral,oneshouldaggregateinformationcontained
in several relevant level sets. In order to obtain the optimal rate of reconstruction. Doing this is
very challenging, and we believe that it is not possible in polynomial time, leading back to the
conjectured statistical-computational gap in the general case.
Acknowledgements
TheworkofN.VerzelenhasbeenpartiallysupportedbygrantANR-21-CE23-0035(ASCAI,ANR).
TheworkofA.CarpentierispartiallysupportedbytheDeutscheForschungsgemeinschaft(DFG)-
Project-ID318763901-SFB1294”DataAssimilation”,ProjectA03. TheworkofA.Carpentierand
M.GrafisalsopartiallysupportedbytheDFGontheForschungsgruppeFOR5381”Mathematical
Statistics in the Information Age - Statistical Efficiency and Computational Tractability”, Project
TP 02 (Project-ID 460867398), by the Agence Nationale de la Recherche (ANR) and the DFG
on the French-German PRCI ANR-DFG ASCAI CA1488/4-1 ”Aktive und Batch-Segmentierung,
Clustering und Seriation: Grundlagen der KI” (Project-ID 490860858).
References
1. Buhlmann, H. & Huber, P. J. Pairwise comparison and ranking in tournaments. The Annals
of Mathematical Statistics 34, 501–510 (1963).
2. Fu¨rnkranz, J. & Hu¨llermeier, E. in Preference learning 65–82 (Springer, 2010).
173. Raykar, V. C. & Yu, S. Ranking annotators for crowdsourced labeling tasks. Advances in
neural information processing systems 24 (2011).
4. Bradley, R. A. & Terry, M. E. Rank analysis of incomplete block designs: I. The method of
paired comparisons. Biometrika 39, 324–345 (1952).
5. Luce, R. D. Individual choice behavior: A theoretical analysis (Courier Corporation, 2012).
6. Hunter, D. R. MM algorithms for generalized Bradley-Terry models. The annals of statistics
32, 384–406 (2004).
7. Gao, C., Shen, Y. & Zhang, A. Y. Uncertainty quantification in the Bradley–Terry–Luce
model. Information and Inference: A Journal of the IMA 12, 1073–1140 (2023).
8. Bong, H. & Rinaldo, A. Generalized results for the existence and consistency of the MLE
in the Bradley-Terry-Luce model in International Conference on Machine Learning (2022),
2160–2177.
9. Braverman,M.&Mossel,E.Noisysortingwithoutresampling inProceedingsofthenineteenth
annual ACM-SIAM symposium on Discrete algorithms (2008), 268–276.
10. Mao, C., Weed, J. & Rigollet, P. Minimax rates and efficient algorithms for noisy sorting in
Algorithmic Learning Theory (2018), 821–847.
11. McLaughlin, D. H. & Luce, R. D. Stochastic transitivity and cancellation of preferences
between bitter-sweet solutions. Psychonomic Science 2, 89–90 (1965).
12. Ballinger, T. P. & Wilcox, N. T. Decisions, error and heterogeneity. The Economic Journal
107, 1090–1105 (1997).
13. Shah, N. et al. Estimation from pairwise comparisons: Sharp minimax bounds with topology
dependence in Artificial intelligence and statistics (2015), 856–865.
14. Shah,N.B.,Balakrishnan,S.,Guntuboyina,A.&Wainwright,M.J.StochasticallyTransitive
Models for Pairwise Comparisons: Statistical and Computational Issues. IEEE Transactions
on Information Theory 63, 934–959 (2016).
15. Shah,N.B.,Balakrishnan,S.&Wainwright,M.J.FeelingtheBern:Adaptiveestimatorsfor
Bernoulli probabilities of pairwise comparisons. IEEE Transactions on Information Theory
65, 4854–4874 (2019).
16. Shah, N. B., Balakrishnan, S. & Wainwright, M. J. A permutation-based model for crowd
labeling: Optimal estimation and robustness. IEEE Transactions on Information Theory 67,
4162–4184 (2020).
17. Mao,C.,Pananjady,A.&Wainwright,M.J.Towardsoptimalestimationofbivariateisotonic
matrices with unknown permutations. The Annals of Statistics 48, 3183–3205 (2020).
√
18. Mao, C., Pananjady, A. & Wainwright, M. J. Breaking the 1/ n Barrier: Faster Rates for
Permutation-based Models in Polynomial Time in Conference On Learning Theory (2018),
2037–2042.
19. Liu, A. & Moitra, A. Better algorithms for estimating non-parametric models in crowd-
sourcing and rank aggregation in Conference on Learning Theory (2020), 2780–2829.
20. Flammarion, N., Mao, C. & Rigollet, P. Optimal rates of statistical seriation. Bernoulli 25,
623–653 (2019).
21. Bengs, V., Busa-Fekete, R., El Mesaoudi-Paul, A. & Hu¨llermeier, E. Preference-based online
learningwithduelingbandits:Asurvey.The Journal of Machine Learning Research 22,278–
385 (2021).
22. Saad, E. M., Verzelen, N. & Carpentier, A. Active ranking of experts based on their per-
formances in many tasks in International Conference on Machine Learning (2023), 29490–
29513.
23. Pilliat,E.,Carpentier,A.&Verzelen,N.OptimalPermutationEstimationinCrowdSourcing
problems. The Annals of Statistics 51, 935–961 (2023).
24. Pilliat,E.,Carpentier,A.&Verzelen,N.Optimalratesforrankingapermutedisotonicmatrix
in polynomial time in Proceedings of the 2024 Annual ACM-SIAM Symposium on Discrete
Algorithms (SODA) (2024), 3236–3273.
25. Liu, H., Gao, C. & Samworth, R. J. Minimax rates in sparse, high-dimensional change point
detection. The Annals of Statistics 49, 1081–1112 (2021).
1826. Pananjady,A.& Samworth, R.J.Isotonic regressionwithunknownpermutations:Statistics,
computation and adaptation. The Annals of Statistics 50, 324–350 (2022).
27. Wainwright, M. J. High-Dimensional Statistics: A Non-Asymptotic Viewpoint (Cambridge
University Press, Cambridge, 2019).
28. Massart, P. Concentration Inequalities and Model Selection (ed Picard, J.) (Springer, Berlin,
Heidelberg, 2007).
19A Reduction to rows and columns sorting
Recall that our goal is the estimation of unknown permutations π∈S and η∈S . For estimators
n n
πˆ and ηˆ, we have defined a loss function L (πˆ,ηˆ) which depends on π, η and the unknown
p,h
matrix M ∈ C (π,η). Let us define analog losses that consider only consider row and column
Biso
permutations respectively, namely
R p,h(πˆ)∶=∣{(i,j)∈[n]×[d]∶ M π−1(i)j ≤p−h, M πˆ−1(i)j ≥p+h}∣
+∣{(i,j)∈[n]×[d]∶ M π−1(i)j ≥p+h, M πˆ−1(i)j ≤p−h}∣ ,
and
C p,h(ηˆ)∶=∣{(i,j)∈[n]×[d]∶ M iη−1(j)≤p−h, M iηˆ−1(j)≥p+h}∣
+∣{(i,j)∈[n]×[d]∶ M iη−1(j)≥p+h, M iηˆ−1(j)≤p−h}∣ .
The following lemma, proved in section C.1, states that the global loss L can be bounded by
shifts of the losses R,C for respectively πˆ and ηˆ.
Lemma A.1. It holds
L p,2h(πˆ,ηˆ)≤2(R p−h,h(πˆ)+C p+h,h(ηˆ))∧2(C p−h,h(ηˆ)+R p+h,h(πˆ))
and
L (πˆ,ηˆ)≥R (πˆ)∨C (ηˆ) .
p,h p,h p,h
In what follows, we will therefore mostly focus on the estimation of π through πˆ and the loss
R (πˆ) - which by symmetry will allow us to bound C (ηˆ) and then L (πˆ,ηˆ).
p,h p,h p,h
Theorem A.2. Let π∈S , η∈S , M ∈C (π,η) and (N′ ,I,J,Y′) an observation that satisfies
n d Biso
Definition 2.1 under some distribution P with subGaussian constant σ2. Given p,h∈[0,1], there
exists a universal constant c>0 such that for the permutation estimator πˆ defined in (13) holds
S
with probability at least 1−δ/2
R (πˆ
)≤c(σ2∨1)log(nd/δ)5/2n∨d
∧nd .
p,h S λ h2
0
The main challenge in this paper will be to establish this theorem and this is the purpose of
the two next subsections –see in particular Section B.5. Before this, we show how Theorem 3.2 is
easily obtained from this last result.
Proof of Theorem 3.2. Consider πˆ –defined in (13)– for the threshold p−h/2, tolerance h/2 and
S
tuning parameter δ/2. By exchanging the roles of rows and columns, we also consider ηˆ for the
S
threshold p+h/2, tolerance h/2 and tuning parameter δ/2. By Theorem A.2 we know for some
universal constant c>0 that
R p−h/2,h/2(πˆ
S)≤c(σ2∨1)log(nd/δ)5/2n λ∨ hd
2
∧nd ,
0
C p+h/2,h/2(ηˆ
S)≤c(σ2∨1)log(nd/δ)5/2n λ∨ hd
2
∧nd ,
0
both with probability at least 1−δ/2. Together with the first part of Lemma A.1, a union bound
yields
L p,h(πˆ S,ηˆ S)≤2R p−h/2,h/2(πˆ S)+2C p+h/2,h/2(ηˆ S)
≤4c(σ2∨1)log(nd/δ)5/2n∨d
∧nd ,
λ h2
0
with probability at least 1−δ.
20B General Analysis of SoHLoB (Proof of Theorem A.2)
In this section, we will give an overview of the analysis of each of the building blocks of the proof
of Theorem A.2, where the row loss R is considered for the output πˆ of SoHLoB with only one
p,h S
threshold p and tolerance parameter h as input. Respective proofs are postponed to later sections
of this appendix.
In Section B.1, we show, that with high probability, all the trisection of a set E into three sets
(O,P,I) satisfy suitable properties with respect to the ordering. Then, Section B.2, which is the
keystonepartofourproof,controlsthepermutationerrorwhichisduestoindecisivesetsP –infact
we look with a superset P of P. Section B.3 is dedicated to the analysis of the Envelope routine.
In Section B.4, we gather all these results to study the properties of the sorting tree. Finally, in
Section B.5, we control the error R (πˆ ) of the ordering and thereby we prove Theorem A.2
p,h S
B.1 Validity of the trisection procedure
We start by analyzing the procedures GraphTrisect, UpdateGraph and ScanAndUpdate (Algo-
rithms 2, 3 and 4) for a generic input first. More precisely, we assume here that these procedures
are fed with data
Y˜(a) ,Y˜(b)
, set E of experts, set Q of questions and comparison graph G that
satisfy the following property.
Property 1 for Y˜(a), Y˜(b), E, Q, and G.
• Y˜(a) and Y˜(b) are independent and distributed as described in Equation (12),
• Q∗(E)={j ∈[d]∶ max i∈EM
ij
≥p+h, min i∈EM
ij
≤p−h}⊆Q,
• λ (∣E∣∧∣Q∣)≥4ρ2/h2,
1
• For all i,i′∈E, G ii′ =1⇒π(i)<π(i′).
We will later show that, outside an event of small probability, this property is satisfied each
time, we apply GraphTrisect, UpdateGraph or ScanAndUpdate. The last point of Property 1
states that G only contains correct information about π.
The next lemma states that, when ScanAndUpdate is fed with
Y˜(a)
,
Y˜(b)
, E, Q, and G that
satisfy Property 1, then the updated graph G˜ still satisfies the last point of Property 1.
Lemma B.1. Under Property 1 for Y˜(a), Y˜(b), E, Q and G, on an event of probability at least
1− ∣Q∣δ , the updated graph G˜ obtained from ScanAndUpdate satisfies
12⌈log (n∨d)⌉d
2
G˜
ii′
=1⇒π(i)<π(i′)
for all i,i′∈E on the updated graph G˜.
With the updated graph G˜, GraphTrisect produces what we call trisection (O,P,I) of E in
the following manner:
O∶={i∈E ∶ ∑ G˜ ii′ >∣E∣/2} ; I ∶={i∈E ∶ ∑ G˜ i′i>∣E∣/2} ; P ∶=E∖(O∪I) . (24)
i′∈E i′∈E
We further define i –the median of E with respect to π– by the property
∣{i′∈E ∶ π(i′)≤π(i)}∣=⌈∣E∣/2⌉ , (25)
and prove the following intuitive statement about the trisection of E.
Lemma B.2. Under Property 1 for Y˜(a), Y˜(b), E, Q and G, it holds with a probability of at least
1−
∣Q∣δ
that
12⌈log (n∨d)⌉d
2
π(i )<π(i)<π(i ) ∀i ∈O, i ∈I . (26)
O I O I
Inequality (26) shows, that (O,P,I) is a partition of E with i ∈ P, all the elements i of O
O
satisfying π(i )<π(i), and all elements i of I satisfying π(i )>π(i). This implies that elements
O I I
of O are well ordered in comparison to I.
21B.2 An error bound for the trisection
When we trisect a set E into (O,P,I), the sets O and I are well separated in the sense of
Lemma B.2. P can be seen as a set of experts that are indistinguishable from i, and it is possible
thati∈P exists,suchthateitherπ(i)<π(i )forsomei ∈O orπ(i)>π(i )forsomei ∈I. This
O O I I
overlap is an important error source, since we want to build a sorting tree based on trisections.
We describe here briefly how we control this error and postpone the proofs of these results to
Section C.3.
ScanAndUpdate not only compares experts by considering partial row sums with respect to Q,
but also by looking at suitable subsets Q′ ⊆Q. We construct them in Section C.2, more precisely
′
in (39). The collection of the sets Q on that we compare experts in E during ScanAndUpdate is
defined as Q. Instead of the unobservable median i defined in (25), let us consider some empirical
counterparts: For
Q′∈Q
we define the empirical median
ι(Q′)
via the property
∣{i∈E ∶ y i(Q′)>y ι(Q′)(Q′)}∣<∣E∣/2 and ∣{i∈E ∶ y i(Q′)<y ι(Q′)(Q′)}∣≤∣E∣/2 .
Note that for the definition of the empirical median
ι(Q′),
multiple choices might be possible. In
that case, one can choose an arbitrary expert with the defining properties without effecting any
proofs.
Our aim is to control the error inside an extension of P (see Lemma C.3), namely
√
P ∶={i∈E ∶ ∣y i(Q′)−y ι(Q′)(Q′)∣≤8ρ λ 1/∣Q′∣ ∀Q′∈Q} . (27)
For doing that, we will consider a slightly different error metric. For given p,h>0, E′ ⊆[n] and
Q′ ⊆[d], it relates to the maximal number of entries in E′×Q′ above p+h that can be confused
with entries below p−h by exchanging experts in E′ and is given as
R˜ p,h,E′,Q′ ∶= ∑ ∣{i∈E′∶ M ij ≤p−h}∣∧∣{i∈E′∶ M ij ≥p+h}∣ . (28)
j∈Q′
We can prove the following bound.
Lemma B.3. Under Property 1 for Y˜(a), Y˜(b), E, Q and G, with a probability of at least
1−
∣Q∣δ
, it holds
12⌈log (n∨d)⌉d
2
R˜ = ∑ ∣{i∈P ∶ M ≤p−h}∣∧∣{i∈P ∶ M ≥p+h}∣
p,h,P,[d] ij ij
j∈[d]
= ∑∣{i∈P ∶ M ≤p−h}∣∧∣{i∈P ∶ M ≥p+h}∣≤3744ρ(∣E∣∨∣Q∣)/λ h2 .
ij ij 1
j∈Q
Thislemmaisoneofthekeypartsofourproof. Whiletheaboveboundwouldhavebeenquite
simple to prove e.g.in a toy models where Q=Q∗(E), p=1/2, M corresponds to a noisy sorting
modelwithvalues1/2−hand1/2+h, thisturnsouttomuchmorechallenginginthegeneralcase.
The proof of Lemma B.3 can be found in Section C.3 –see in particular Figure 4.
B.3 Analysis of the envelope procedure
As in the analysis of the trisection procedure, we will first analyze the procedure Envelope (Algo-
rithm 5) for some generic input. We consider Y˜, E =(E ,E ,...,E ) and v =(v ,v ,...,v ) that
1 2 r 1 2 r
satisfies the following:
Property 2 for Y˜, E and G.
• Y˜ is distributed as described in Equation (12),
• E is a partition of [n],
• v =1⇒λ ∣E ∣>4ρ2/h2,
s 1 s
• v s=v s′ =1 for s<s′ implies π(i)<π(i′) for i∈E s and i′∈E s′.
22The two last points mean that v indicates whether groups of experts are large enough and
that these large groups are ordered with respect to π. If we obtain the set Q as an output of
s
Envelope(s, E, v, Y˜, p, h, λ , δ) for all s with v =1, we show the following.
0 s
Lemma B.4. Under Property 2 for Y˜, E and v, with probability at least 1− δ ,
12⌈log (n∨d)⌉(n∨d)1/2
2
it holds that
r
∑ ∣Q ∣≤3d , and Q∗(E )⊆Q ∀s=1,2,...,r with v =1 .
s s s s
s=1, vs=1
This enforces that the sets Q obtained through Envelope all contain the relevant sets of
s
questions Q∗(E s) and that the total size of the estimated questions, ∑r s=1, vs=1∣Q s∣, is controlled.
B.4 Analysis of the sorting tree
Equipped with the previous results, we establish some properties of the sorting tree that we have
built. In order so state these properties, we need to introduce further notation to formalize of the
intermediaryobjectsbuiltbythealgorithm. WebrieflydescribedSoHLoB(Algorithm1)inSection
4.3. Let us now define
E0∶=([n]), v0∶=(1{λ 1n>4ρ2/h2}), r 0=1 and G0=0 n×n ,
and assume we are given independent observations
Y(1) ,Y(2) ,...,Y(3⌈log 2(n)⌉)
from the Poisson
observation model (12), as described in Section 4.
SoHLoB operates in main rounds, indexed by k, starting with k = 1, which correspond to
successive refinement of the partition - and sometimes also in sub-rounds indexed by t, which
correspond to successive trisections. In Algorithm 1, notations corresponding to the rounds are
not indexed by the rounds in order to alleviate notation. We introduce below the sequential
notations corresponding to all rounds, in order to be able to analyze the algorithm. For k ≥ 1,
given an ordered partition
Ek−1, vk−1∈{0,1}rk−1,
r k−1 and a graph
Gk−1∈{0,1}n×n,
let us define:
• Qk−1 for vk−1 = 1; output of the routine Envelope (Algorithm 5) with parameters t, Ek−1,
t t
vk−1, Y(3k−2)
, computed in line 11 of Algorithm 1
• wk−1∈{0,1}rk−1 defined by
t
⎧
⎪⎪1{λ ∣Qk−1∣>4ρ2/h2}, if vk−1=1 ,
wk−1=⎨ 1 t t
t ⎪⎪0 if vk−1=0 ,
⎩ t
corresponds to the indexes of the sets of the partition
Ek−1
to be further refined.
• r
k
∶=∣{t=1,2,...,r k−1∶ w tk−1=0}∣+3⋅∣{t=1,2,...,r k−1∶ w tk−1=1}∣. Here, r
k
will stand for
the size of the partition Ek.
• Gk ∶=Gk−1.
0
Consider for t = 1,2,...,r k−1 the index s = ∣{t′ < t ∶ w tk ′−1 = 0}∣+3⋅∣{t′ < t ∶ w tk ′−1 = 1}∣+1. Then,
if w tk−1 =0, we do not refine E tk−1 anymore and we define E sk ∶=E tk−1, v sk ∶=0, and Gk
t
∶=Gk t−1. If
wk−1=1,
we denote
t
• (O(Ek−1),P(Ek−1),I(Ek−1)); output of GraphTrisect (Algorithm 2) with parameters
t t t
Y(3k−1) , Y(3k) , E tk−1, Qk t−1 and Gk t−1, computed in line 20 of Algorithm 1
• Ek ∶=O(Ek−1), vk ∶=1{λ ∣Ek∣>4ρ2/h2},
s t s 1 s
• E sk +1∶=P(E tk−1), v sk +1∶=0,
• E sk +2∶=I(E tk−1), v sk +2∶=1{λ 1∣E sk +2∣>4ρ2/h2},
23• Gk t; update of Gk t−1; every time we run ScanAndUpdate (Algorithm 3) in line 14, the routine
UpdateGraph(Algorithm4)withparametersGk t−1,E tk−1,Qk t−1,Y(3k) leadstoanewdirected
graph.
• Qk−1; defined as in (39), collection of subsets Q′ ⊆ Qk−1 on which we compare experts in
t t
Ek−1
during ScanAndUpdate.
t
At the end of the r k−1 iterations, one obtains Ek = (E 1k,E 2k,...,E rk k), vk = (v 1k,v 2k,...,v rk k) and
Gk =Gk .
rk−1
The algorithm goes through every Ek−1 ∈ Ek. If vk−1 = 0, we say that Ek−1 is a passive set.
t t t
We account this by setting wk−1 = 0. Passive sets are carried over to Ek, and we also set the
t
corresponding entry of vk to 0. If vk−1 = 1, we first compute Qk−1. If this set is small, we will
t t
change the status of Ek−1 to be a passive set. So again, wk =0, Ek−1 is carried over to Ek and the
t t t
corresponding entry of vk is set to 0. If vk−1 =1 and Qk−1 is large enough, we call Ek−1 an active
t t t
set, so wk =1. We will apply GraphTrisect and obtain a trisection of Ek−1 of the form (O,P,I).
t t
This trisection now replaces Ek−1 in the new Ek. O and I will be considered as passive, if they
t
are too small, which is taken into account in the definition of the corresponding entries of vk. P is
always considered a passive set, so the corresponding entry of vk is set to 0. We refer to Figure 3
for a graphical illustration.
0
E
[n]
1
E
O P I
2
E
O P I O P I
3
E
O P I
Figure 3: Illustration of a sorting tree with K =3 iterations. The red nodes correspond to active
sets, on which we apply Algorithm GraphTrisect, the trisection. The blue nodes correspond to
passive sets, which are carried over unchanged into all further levels of the tree.
In Section C.5, we will prove the following statement which is mainly a consequence of the
results above for generic input arguments. In the next theorem, the first point states that each of
theenvelopescontainstherelevantsetofquestionsandthattheirsizesisbounded. Thesecondand
the third points state that each of the trisections satisfies the properties outlined in the previous
subsections. Finally, the two last points emphasize that so-called active sets are perfectly ordered
with each other.
Theorem B.5. There exist events ξ1⊃ξ2⊃⋅⋅⋅⊃ξK with
kδ
P(ξk)≥1− for k=1,2,...,K ,
2⌈log (n∨d)⌉
2
such that on ξk, the following holds true after the kth iteration step of Algorithm 1:
1. For t∈{1,2,...,r k−1} with v tk−1=1 we have
Qk−1⊇Q∗(Ek−1)
t t
24and moreover it holds that
∑
∣Qk−1∣≤3d
.
t
t≥1, wk−1=1
t
2. Let t∈{1,2,...,r k−1}, w tk−1=1 and E sk =O(E tk−1), E sk +1=P(E tk−1), E sk +2=I(E tk−1).
2.a. (E sk,E sk +1,E sk +2) is a partition of E tk−1,
2.b. If ik t−1 is the median of E tk−1 in the sense of Equation (25), it holds ik t−1 ∈E sk +1 and
π(i O)<π(i
tk−1
)<π(i I) ∀i O ∈E sk, i I ∈E sk +2 .
2.c. ∣E sk∣∨∣E sk +2∣≤2−kn.
3. Gk =1⇒π(i)<π(i′) ∀i,i′∈[n].
i,i′
4. For i∈Ek and i′∈Ek with vk =vk =1 holds
s s′ s s′
s<s′ ⇒ π(i)<π(i′) ,
5. Ek is a partition of [n].
Moreover, with a probability of at least 1−δ/2, Algorithm 1 terminates after at most ⌈log (n)⌉
2
iterations.
While the above theorem states that the partition is in some way coherent with the true
permutation π this will not be sufficient to control the error of the estimated permutation πˆ. For
that we need to define auxiliary objects. First, for any
Ek−1,
we define
O˜(Ek−1), I˜(Ek−1),
and
t t t
P˜(Ek−1)
as some completions of the sets
O(Ek−1), I(Ek−1),
and
P(Ek−1).
t t t t
O˜(Ek−1)∶={i∈[n]∶
min
π(i′)≤π(i)≤
max
π(i′)}
,
t i′∈O(Ek−1) i′∈O(Ek−1)
t t
P˜(Ek−1)∶={i∈[n]∶
min
π(i′)≤π(i)≤
max
π(i′)}
, (29)
t i′∈P(Ek−1) i′∈P(Ek−1)
t t
I˜(Ek−1)∶={i∈[n]∶
min
π(i′)≤π(i)≤
max
π(i′)}
. (30)
t i′∈I(Ek−1) i′∈I(Ek−1)
t t
InotherwordsO˜(Ek−1)⊃O(Ek−1)containsalltheexpertsin[n]whosepositionπ(i)issandwiched
t t
by two experts in
O(Ek−1).
t
Recall also that we defined median
ik−1 ∈Ek−1
in Equation (25) by the property
t t
∣{i′∈Ek−1∶ π(i′)≤π(ik−1 )}∣=⌈∣Ek−1∣/2⌉
.
t t t
Given a subset Q′ ⊂ [d] of question, we now define its empirical counterpart ιk−1(Q′) by the
t
property
RRRRRRRRRRR⎧
⎪⎪
⎨
⎪⎪
⎩i∈E tk−1∶ ∣Q1
′∣
j∑ ∈Q′Y i( j3k) > ∣Q1
′∣
j∑ ∈Q′Y ι(
k
t3 −k 1)
(Q′)j⎫
⎪⎪
⎬
⎪⎪
⎭RRRRRRRRRRR<∣E
tk−1∣/2 ,
RRRRRRRRRRR⎧
⎪⎪
⎨
⎪⎪
⎩i∈E tk−1∶ ∣Q1
′∣
j∑ ∈Q′Y i( j3k) < ∣Q1
′∣
j∑ ∈Q′Y ι(
k
t3 −k 1)
(Q′)j⎫
⎪⎪
⎬
⎪⎪
⎭RRRRRRRRRRR≤∣E
tk−1∣/2 .
In other words,
ιk−1(Q′)
corresponds to the median of
Ek−1
according to the empirical average of
t t
′
the data on the set Q of questions.
Equipped with this new notation and recalling the definition of
Qk−1
above, we define
t
P(E
tk−1)=⎧
⎪⎪
⎨
⎪⎪
⎩i∈E tk−1∶ ∣Q1
′∣RRRRRRRRRRRj∑
∈Q′Y i( j3k) −Y ι(
k
t3 −k 1)
(Q′)jRRRRRRRRRRR≤8ρ√
λ 1/∣Q′∣ ∀Q′∈Qk
t−1⎫
⎪⎪
⎬
⎪⎪
⎭
. (31)
25In fact, under an event of large probability, the set
P(Ek−1)
contains
P(Ek−1);
this set will be
t t
important to control the permutation error in the next subsection. We would like to point out a
small abuse of notation in the definition of
P(Ek−1),
which not only depends on the set
Ek−1,
but
t t
also on the indices t and k−1, since we consider sets Q′∈Qk−1.
t
With this notation, the following result, whose proof is an immediate consequence of that of
Theorem B.5.
Corollary B.6. On the event ξK, it holds that
√
1. (∃Q′∈Qk t−1 s.t. ∣Q1 ′∣∑ j∈Q′Y i( j3k) −Y ι( k3 −k 1)
(Q′)j
>2ρ λ 1/∣Q′∣) implies i∈O(E tk−1),
t
√
2. (∃Q′∈Qk t−1 s.t. ∣Q1 ′∣∑ j∈Q′Y ι( k3 −k 1) (Q′)j−Y i( j3k) >2ρ λ 1/∣Q′∣) implies i∈I(E tk−1),
t
√
3. i∈P(E tk−1) implies ∣Q1 ′∣∣∑ j∈Q′Y i( j3k) −Y ι( k3 −k 1) (Q′)j∣≤2ρ λ 1/∣Q′∣ ∀Q′∈Qk t−1,
t
4. O(Ek−1)⊆O˜(Ek−1), P(Ek−1)⊆P˜(Ek−1) and I(Ek−1)⊆I˜(Ek−1),
t t t t t t
5. Ek−1∩P˜(Ek−1)⊆P(Ek−1) and
t t t
6. R˜ ≤3744ρ(∣Ek−1∣∨∣Qk−1∣)/λ h2.
p,h,P(Ek−1),[d] t t 1
t
Thethreefirstpointsareconsequenceofthedefinitionoftheprocedure. Thefourthpointstates
the
O˜(Ek−1)
is a superset of
O(Ek−1),
the fifth point
P(Ek−1)
is large. Finally, the last point
t t t
controls the modified error R˜ defined in (28) for the larger set P(Ek−1) –this bound corresponds
t
to Lemma B.3.
B.5 Analysis of the permutation estimator
We have seen in Theorem B.5 that Algorithm 1 terminates with a probability of at least 1−δ/2
after K ≤⌈log (n)⌉ iterations and returns a partition EK =(EK,EK,...,EK) of [n]. Recall that
we define
the2
estimator πˆ as permutation drawn uniformly
am1 ong2
all
permrk
utations π′ ∈ S with
n
the property
i∈E sK ⇔ ∑ ∣E sK ′∣<π′(i)≤ ∑ ∣E sK ′∣ .
s′<s s′≤s
Throughout this subsection write πˆ for πˆ to simplify the notation. On ξK, the property that πˆ is
S
coherentwithpartitionEK impliesthatitisalsocoherentwithalllessrefinedpartitionsEk,k≤K
as stated in the following lemma.
Lemma B.7. On ξK, it holds for all k∈{0,1,...,K} that
i∈E sk ⇔ ∑ ∣E sk ′∣<πˆ(i)≤ ∑ ∣E sk ′∣ .
s′<s s′≤s
The proof can be found in Section C.6.
B.5.1 Simple error bound in an idealized situation
At first, consider the following idealized situation. If we had at each step, for sets
Ek−1
with
t
wk−1 = 1, a perfect separation of the form (O(Ek−1),{ik−1 },I(Ek−1)), this would lead to a final
t t t t
partition EK with π(i)<π(i′) if i∈EK, i′∈EK for s<s′ . If we apply τ ∶=π−1○πˆ on some i∈EK,
s s′ s
the definition of πˆ therefore impliesˆi=π−1(πˆ(i))∈E sK. Note that M i⋅ and M ˆi⋅ differ on at most
∣Q∗(EK)∣ entries. All EK are passive sets, meaning they are either of the form {ik−1 } (no error
s s t
26arises from exchanging an element with itself); or EK =O(Ek−1) or EK =I(Ek−1), where either
s t s t
∣EK∣≤4ρ2/λ h2 or ∣Q∗(EK)∣≤4ρ2/λ h2. It is easy to see that for each EK we have
s 1 s 1 s
∑ ∣{j ∈[d]∶M ≥p+h, M ≤p−h}∣+∣{j ∈[d]∶M ≤p−h, M ≥p+h}∣
ij ˆij ij ˆij
i∈EK
s
≤∣EK∣⋅∣Q∗(EK)∣≤
4ρ2
(∣EK∣∨∣Q∗(EK)∣)
s s λ h2 s s
1
and in total this sums up to an error
R (πˆ)≤ ∑∣EK∣⋅∣Q∗(EK)∣≲
ρ2
(n∨d) . (32)
p,h s≥1 s s λ 1h2
B.5.2 Valid control of the error
Ingeneral,P(Ek−1)doesnotsimplycontainthemedianik−1
. Weindicatedearlier,thatanoverlap
t t
of
P(Ek−1)
with
O(Ek−1)
or
I(Ek−1)
might be a source of error. In fact, the arguments for the
t t t
error bounds we just stated cannot be formalized if i ∈ O(Ek−1) but ˆi = π−1(πˆ(i)) ∉ O˜(Ek−1) or
t t
i∈I(Ek−1) butˆi∉I˜(Ek−1).
The following two statements are quite technical, but take up exactly
t t
this idea. Morally they lead to a proof of Theorem A.2, where every error from confusing i with
ˆi, that we cannot handle as in (32), can be dealt with by controlling the sets P(El), with l≤K,
u
u∈{1,...,r }. We postpone the proofs of those lemmas to Section C.6.
l
Lemma B.8. Consider k ≤ K, s ∈ {1,2,...,r } such that Ek is of the form Ek = O(Ek−1) or
k s s t
E sk =I(E tk−1) for some t∈{1,2,...,r k−1}. Consider i∈E sk such thatˆi∶=π−1(πˆ(i))∉E˜ sk. Then on
ξK, there exist l<k and u∈{1,2,...,r } such that i,ˆi∈P(El).
l u
In other words, these lemmas entails that an error in some Ek corresponds to elements that
s
belong to a super set P(El). This suggests that the size of P(El) is important to quantify the
u u
permutation loss.
Lemma B.9. Consider k,s such that E sk is of the form P(E tk−1) for some t ∈ {1,2,...,r k−1}
with wk−1 = 1. Consider i ∈ Ek and write ˆi ∶= π−1(πˆ(i)). On the event ξK, there exist l < k and
t s
u∈{1,2,...,r } such that i,ˆi∈P(El).
l u
Similarly, this lemma states that, if i andˆi are mixed and if i belongs to some P(Ek−1), then
t
both i andˆi belong to some P(El).
u
We now have the main ingredients to analyze the error
R p,h(πˆ)=∣{(i,j)∈[n]×[d]∶ M π−1(i)η−1(j)≤p−h, M πˆ−1(i)η−1(j)≥p+h}∣
+∣{(i,j)∈[n]×[d]∶ M π−1(i)η−1(j)≥p+h, M πˆ−1(i)η−1(j)≤p−h}∣ .
Also define for 1≤k<K the sets
Ok ∶={E tk ∶ ∃t′∈{1,2,...,r k−1} with E tk =O(E tk ′−1) and w tk =0} ,
Ik ∶={E tk ∶ ∃t′∈{1,2,...,r k−1} with E tk =I(E tk ′−1) and w tk =0} .
Then for s ∈ {1,2,...,r K}, each set E sK is either in ⋃K k=1Ok ∪Ik or has the form P(E tk ′−1) for
some k ∈ {1,2,...,K} and t′ ∈ {1,2,...,r k−1} with w tk ′−1 = 1. By combining Theorem B.5 with
Lemmas B.8 and B.9, we arrive, with some work, at the following control for the loss.
Lemma B.10. On the event ξK, it holds that
R (πˆ)≤4
ρ2 ∑K ∑rk ∣Ek∣∨∣Q∗(Ek)∣+2∑K r ∑l−1
R˜ . (33)
p,h λ 1h2 k=1 t=1 t t l=1 u=1 p,h,P(E ul−1),[d]
Ek∈Ok∪Ik wl−1=1
t u
The above bound contains two terms. The first one accounts for the size of groups and cor-
responding questions of the form O(Ek) or I(Ek) that are not to be cut anymore. The second
t t
27accounts for the error R induced by the set of the form P = P(El−1), such an error being
p,h,P,h u
introduced and bounded in Section B.2. We respectively call (I) and (II) the two terms in the
rhs of (33). We first focus on (I). First, we claim that all Ek in Ok∪Ik with k = 1,...,K are
t
disjoint and that all corresponding
∣Q∗(Ek)∣
are also disjoint. It follows from this claim that
t
ρ2
(I)≤4 (n+d) . (34)
λ h2
1
Let us show the above claim. Consider any two such sets Ek1 and Ek2. Since these sets are not
t1 t2
cut anymore in the sorting tree, there exists l<k ∧k and u∈[r ] with wl =1 such that
1 2 l u
Ek1 ⊆O(El), Ek2 ⊆I(El) or Ek2 ⊆O(El), Ek1 ⊆I(El) .
t1 u t2 u t2 u t1 u
In fact, El corresponds to the closest common ancestor of Ek1 and Ek2 in the sorting tree. This
u t1 t2
provesthatEk1 andEk2 aredisjoint. ThisshowsthatEk1 andEk2 aredisjointandwecanassume
t1 t2 t1 t2
w.l.o.g. Ek1 ⊆O(El), Ek2 ⊆I(El),soinparticularwehavebyTheoremB.5.2.bthatπ(i )<π(i )
t1 u t2 u 1 2
for all i ∈ Ek1 and i ∈ Ek2. By the bi-isotonicity, it therefore holds for all j ∈ Q∗(Ek1),
1 t1 2 t2 1 t1
j
∈Q∗(Ek2)
2 t2
min M ≤p−h<p+h≤ maxM ≤ min M ,
i∈Ek1
ij1
i∈Ek2
ij2
i∈Ek1
ij2
t1 t2 t1
which implies η(j )>η(j ). Therefore, also Q∗(Ek1) and Q∗(Ek2) are disjoint. We have proved
1 2 t1 t2
the claim.
Let us turn to the second term (II) in the rhs of (33). It follows from Corollary B.6.6 that
2R˜ ≤ 7488ρ (∣El−1∣∨∣Ql−1∣) .
p,h,P(E ul−1),[d] λ 1h2 u u
Then, for a fixed l, we know that the El−1 are disjoint by construction so that ∑ ∣El−1∣ ≤ n.
u u u
Besides, Theorem B.5.1 implies that ∑
∣Ql−1∣≤3d.
Overall, we arrive at
u u
(II)≤44928Kρ(n∨d)/λ h2 . (35)
1
Hence, gathering (34) and (35) in (33), we conclude that, on ξK, we have
n∨d
R (πˆ)≤(44928Kρ+8ρ2) . (36)
p,h λ h2
1
We are almost done with the proof of Theorem A.2. We only need to recall that λ
1=1−e−λ−
0 with
λ− = λ0 . So consider the function g(x) = 1−e−x for x > 0. g is monotonously increasing
0 3⌈log (nd)⌉
2
and we have λ =g( λ0 ). One can show that ⌈log (nd)⌉≤2log(nd) and that for x∈(0,1]
1 3⌈log (nd)⌉ 2
2
it holds g(x)≥x/e. Since we assumed λ ∈(0,log(nd)], we obtain λ ≥ λ0 which, together
0 1 6elog(nd)
with (36) conclude the proof of Theorem A.2.
C Proofs of the intermediary results
C.1 Proof of Lemma A.1
Consider (i,j)∈[n]×[d] such that
M π(i)η(j)≤p−2h and M πˆ(i)ηˆ(j)≥p+2h .
Then it holds either that
M π(i)η(j)≤p−2h, M πˆ(i)η(j)≥p or M πˆ(i)η(j)≤p, M πˆ(i)ηˆ(j)≥p+2h .
Since η and πˆ are both permutations and hence bijective, this proves
∣{(i,j)∈[n]×[d]∶ M π(i)η(j)≤p−2h, M πˆ(i)ηˆ(j)≥p+2h}∣≤R p−h,h(πˆ)+C p+h,h(ηˆ) .
28In the same way, one can prove
∣{(i,j)∈[n]×[d]∶ M π(i)η(j)≤p−2h, M πˆ(i)ηˆ(j)≥p+2h}∣≤R p+h,h(πˆ)+C p−h,h(ηˆ),
∣{(i,j)∈[n]×[d]∶ M π(i)η(j)≥p+2h, M πˆ(i)ηˆ(j)≤p−2h}∣≤R p−h,h(πˆ)+C p+h,h(ηˆ) ,
and
∣{(i,j)∈[n]×[d]∶ M π(i)η(j)≥p+2h, M πˆ(i)ηˆ(j)≤p−2h}∣≤R p+h,h(πˆ)+C p−h,h(ηˆ) .
This concludes the first part of the statement.
For the lower bound, we will only prove
L (πˆ,ηˆ)≥R (πˆ) .
p,h p,h
Note that we can write
n
L p,h(πˆ,ηˆ)=∑∣{j ∈[d]∶ M π−1(i)η−1(j)≤p−h, M πˆ−1(i)ηˆ−1(j)≥p+h}∣
i=1
+∣{j ∈[d]∶ M π−1(i)η−1(j)≥p+h, M πˆ−1(i)ηˆ−1(j)≤p−h}∣
and
n
R p,h(πˆ)=∑∣{j ∈[d]∶ M π−1(i)η−1(j)≤p−h, M πˆ−1(i)η−1(j)≥p+h}∣
i=1
+∣{j ∈[d]∶ M π−1(i)η−1(j)≥p+h, M πˆ−1(i)η−1(j)≤p−h}∣ .
Wewillboundtherespectivesummands.
W.l.o.g.,consideri∈[n]withπ(πˆ−1(i))≥i.
Thisimplies
M π−1(i)η−1(j)≥M πˆ−1(i)η−1(j) for all j ∈[d] by the bi-isotonicity assumption. To prove our claim, it
is therefore sufficient to show
∣{j ∈[d]∶ M π−1(i)η−1(j)≤p−h, M πˆ−1(i)ηˆ−1(j)≥p+h}∣
≥∣{j ∈[d]∶ M π−1(i)η−1(j)≤p−h, M πˆ−1(i)η−1(j)≥p+h}∣ .
To this end, define
j l∶=min{j ∈[d]∶ M π−1(i)η−1(j)≤p−h},
j
r
∶=max{j ∈[d]∶ M πˆ−1(i)η−1(j)≥p+h} .
It holds
∣{j ∈[d]∶ M π−1(i)η−1(j)≤p−h, M πˆ−1(i)η−1(j)≥p+h}∣
=∣{j ∈[d]∶ j ≤j ≤j }∣
l r
=∣{j ∈[d]∶ j ≤j ≤j , η(ηˆ−1(j))≤j }∣+∣{j ∈[d]∶ j ≤j ≤j , η(ηˆ−1(j))>j }∣
l r r l r r
≤∣{j ∈[d]∶ j ≤j ≤j , η(ηˆ−1(j))≤j }∣+∣{j ∈[d]∶ j ≤j , η(ηˆ−1(j))>j }∣
l r r r r
=∣{j ∈[d]∶ j ≤j ≤j , η(ηˆ−1(j))≤j }∣+∣{j ∈[d]∶ j >j , η(ηˆ−1(j))≤j }∣
l r r r r
=∣{j ∈[d]∶ j ≤j, η(ηˆ−1(j))≤j }∣
l r
=∣{j ∈[d]∶ M π−1(i)η−1(j)≤p−h, M πˆ−1(i)ηˆ−1(j)≥p+h}∣ ,
where we used, that
η○ηˆ−1
is a permutation and that
η(ηˆ−1(j))≤j
implies
r
M πˆ−1ηˆ−1(j)=M πˆ−1(i)η−1(η(ηˆ−1(j)))≥M πˆ−1(i)η−1(jr)≥p+h .
This concludes the proof.
C.2 Proofs of Lemmas B.1 and B.2
We will derive properties of the trisection under Property 1 for two observations
Y˜(a)
and
Y˜(b)
,
some set E ⊂ [n] of experts, some set Q ⊂ [d] of questions and a directed graph G ∈ {0,1}n×n.
First, the average quantities
yj(E)∶= 1 ∑Y˜(a) and mj(E)∶= 1 ∑M for j ∈Q . (37)
∣E∣ ij ∣E∣ ij
i∈E i∈E
29We will prove that with high probability
√
∣yj(E)−λ mj(E)∣≤ρ λ /∣E∣ ∀j ∈Q . (38)
1 1
The algorithm now uses the yj(E) to construct sets of questions of the following form: for j ∈Q
√
and c∈{2,3,...,⌈ λ ∣E∣/2ρ+1⌉}, let
1
√ √
Ql(c)∶={j′∈Q∶2ρ λ /∣E∣<yj′ (E)−yj(E)≤2cρ λ /∣E∣} ,
j √ 1 √ 1
Qr(c)∶={j′∈Q∶2ρ λ /∣E∣<yj(E)−yj′ (E)≤2cρ λ /∣E∣} , (39)
j 1 1
and
√
A ∶={j′∈Q∶∣yj′ (E)−yj(E)∣≤2ρ λ /∣E∣} .
j 1
The collection of potential sets of interest is defined as
⎛ √
Q∶= {Ql(c)∶j ∈Q, c=2,3,...,⌈ λ ∣E∣/2ρ+1⌉}
⎝ j 1
√ ⎞
∪{Qr(c)∶j ∈Q, c=2,3,...,⌈ λ ∣E∣/2ρ+1⌉}∪{A ∶j ∈Q}∪{Q} ∩{Q′⊆Q∶ ∣Q′∣≥γ} .
j 1 j ⎠
By Property 1, we have λ ∣Q∣ > 4ρ2/h2 by implies Q ∈ Q. Since the collection Q depends on the
1
realization of
Y˜(a)
, we use the independent observation
Y˜(b)
to define
y (Q′)∶= 1 ∑ Y˜(b) and m (Q′)∶= 1 ∑ M ∀Q′∈Q, ∀i∈E .
i ∣Q′∣ ij i ∣Q′∣ ij
j∈Q′ j∈Q′
For our analysis, we will assume that we are on the event ξ where the concentration inequality in
Equation (38) holds and where also
√
∣y (Q′)−λ m (Q′)∣≤ρ λ /∣Q′∣ ∀Q′∈Q, ∀i∈E , (40)
i 1 i 1
namely
ξ={Equation (38) holds}∩{Equation (40) holds}.
ξ is an event of high probability, as proven below.
Lemma C.1. Under Property 1 for Y˜(a), Y˜(b), E, Q and G, it holds that P(ξ)≥1− ∣Q∣δ .
12⌈log (n∨d)⌉d
2
Proof of Lemma C.1. ConsiderQ′∈Q. Since∣Q′∣≥γ =2log(24⌈log (n∨d)⌉nd(n∨d)1/2/δ)/λ e2,
2 1
Lemma G.1 gives us for any i∈E, with probability of at least 1− δ
12⌈log (n∨d)⌉nd(n∨d)1/2
2
√
∣y (Q′)−λ m (Q′)∣≤ 2(1∨σ2)e2log(24⌈log (n∨d)⌉nd(n∨d)1/2/δ)λ /∣Q′∣
i 1 i 2 1
+2(1∨σ)log(24⌈log (n∨d)⌉nd(n∨d)1/2/δ)/∣Q′∣
√ 2
≤ρ λ /∣Q′∣ ,
1
√
since ρ=(1∨σ)e 8log(24⌈log (n∨d)⌉nd(n∨d)1/2/δ).
2
NotethatbyProperty1wealsoknowλ ∣E∣>4ρ2/h2 whichagainimpliesthat∣E∣≥γ. Forany
1
j ∈Q, this implies by Lemma G.1 that with probability of at least 1− δ
12⌈log (n∨d)⌉nd(n∨d)1/2
2
√
∣yj(E)−λ mj(E)∣≤ 2(1∨σ2)e2log(24⌈log (n∨d)⌉nd(n∨d)1/2/δ)λ /∣E∣
1 2 1
+2(1∨σ)log(24⌈log (n∨d)⌉nd(n∨d)1/2/δ)/∣E∣
√ 2
≤ρ λ /∣E∣ .
1
30Recall that the collection Q contains the set Q and sets of the form A and Ql(c), Qr(c) with
√ j j j
j ∈Q and c∈{2,3,...,⌈ λ ∣E∣/2ρ+1⌉}. Therefore, the cardinality of Q is bounded by
1
√ √
∣Q∣⋅(2⌈ λ ∣E∣/2ρ⌉+1)+1≤∣Q∣( λ ∣E∣/ρ+3)+1
1 1
√
≤3∣Q∣ λ ∣E∣/ρ
1
≤∣Q∣∣E∣1/2/2
,
√ √
where we used 1 ≤ 1/h < λ ∣E∣/2ρ, λ < 1 and ρ ≥ e 8log(24) > 6. At the same time, we have
1 1
again ∣E∣>4ρ2/h2λ >4, so ∣Q∣≤∣Q∣∣E∣3/2/2. In the end, the total number of inequalities in (38)
1
and (40) is bounded by
∣E∣⋅∣Q∣+∣Q∣≤∣Q∣∣E∣3/2≤∣Q∣n(n∨d)1/2
.
Consequently, using the union bound, we have
∣Q∣δ
P(ξ)≥1− ,
12⌈log (n∨d)⌉d
2
which concludes the proof.
Proof of Lemma B.1. Consider i,i′∈E with π(i)≥π(i′). Under Property 1, we have G ii′ ≠1. For
every Q′ ∈Q, the bi-isotonicity assumption gives us m i(Q′)≤m i′(Q′) and on ξ, in particular by
(40), it holds that
√
y i(Q′)−y i′(Q′)≤y i(Q′)−λ 1m i(Q′)+λ 1m i′(Q′)−y i′(Q′)≤2ρ λ 1/∣Q′∣ .
So it remains G˜ ii′ =G ii′ ≠1 also after applying Algorithm 2.
Proof of Lemma B.2. Consideri∈E withπ(i)≤π(i). UnderProperty1wecanapplyLemmaB.1
and obtain on ξ that
⌊∣E∣/2⌋=∣{i′∈E ∶ π(i)<π(i′)}∣≥∣{i′∈E ∶ π(i)<π(i′)}∣≥∣{i′∈E ∶ G˜ ii′ =1}∣ .
By the definition in (24), this implies i ∉ O. Conversely, this means that if i ∈ O, we have
O
π(i )<π(i). The second part of (26) follows in the same manner.
O
C.3 Proof of Lemma B.3
We start with a few lemmas and notation. Recall that we defined for each Q′ ∈ Q the empirical
median
ι(Q′)
via the property
∣{i∈E ∶ y i(Q′)>y ι(Q′)(Q′)}∣<∣E∣/2 and ∣{i∈E ∶ y i(Q′)<y ι(Q′)(Q′)}∣≤∣E∣/2
as an empirical counterpart to i defined in (25).
The following lemma is a direct consequence of the definition of our trisection scheme in Equa-
tion (24).
Lemma C.2. Under Property 1 for Y˜(a), Y˜(b), E, Q and G, on the event ξ, it holds for any i∈E:
√
• If there exists Q′∈Q s.t. y i(Q′)−y ι(Q′)(Q′)>2ρ λ 1/∣Q′∣, then i∈O.
√
• If there exists Q′∈Q s.t. y ι(Q′)(Q′)−y i(Q′)>2ρ λ 1/∣Q′∣, then i∈I.
So that:
√
• If i∈P, then ∣y i(Q′)−y ι(Q′)(Q′)∣≤2ρ λ 1/∣Q′∣ ∀Q′∈Q.
31Proof of Lemma C.2. Wewillfirstprovethefirstclaim. AssumethatthereexistsQ′∈Qsuchthat
√
y i(Q′)−y ι(Q′)(Q′)>2ρ λ 1/∣Q′∣ .
Note that by definition, the number of all i′ such that y i′(Q′) ≤ y i(ι(Q′)) is at least ∣E∣/2. For
′
each such i it holds
√
y i(Q′)−y i′(Q′)>2ρ λ 1/∣Q′∣ ,
and consequently G˜ ii′ =1. This proves i∈O. The second claim follows analogously and the third
claim is just the contraposition of the first two statements.
Let us introduce the sets O˜, P˜, and I˜which are to be interpreted as completions of O, P, and
I.
O˜ ∶={i∈[n]∶ minπ(i′)≤π(i)≤maxπ(i′)} , P˜ ∶={i∈[n]∶ minπ(i′)≤π(i)≤maxπ(i′)} ,
i′∈O i′∈O i′∈P i′∈P
I˜∶={i∈[n]∶ minπ(i′)≤π(i)≤maxπ(i′)} .
i′∈I i′∈I
We note the following relationships between the defined sets.
Lemma C.3. Under Property 1 for Y˜(a), Y˜(b), E, Q and G, on the event ξ, it holds that O⊆O˜,
P ⊆P˜ and I ⊆I˜. Besides, we have P˜∩E ⊆P.
Proof of Lemma C.3. The first statement (three first inclusions) follows trivially from the re-
spective definitions. We therefore focus on the second statement. Consider i ∈ P˜ ∩E and let
i max∶=argmax i′∈Pπ(i′) and i min∶=argmin i′∈Pπ(i′). By definition it holds that
π(i )≤π(i)≤π(i ) .
min max
From Lemma C.2 we know that
√
∣y imax(Q′)−y ι(Q′)(Q′)∣≤2ρ λ 1/∣Q′∣ ∀Q′∈Q , (41)
and
√
∣y imin(Q′)−y ι(Q′)(Q′)∣≤2ρ λ 1/∣Q′∣ ∀Q′∈Q .
Because of the bi-isotonicity assumption, we have
m
(Q′)≤m (Q′)≤m (Q′) ∀Q′∈Q
.
imax i imin
Consequently, by (40), it holds that
√
y (Q′)−y (Q′)≤y (Q′)−λ m (Q′)+λ m (Q′)−y (Q′)≤2ρ λ /∣Q′∣ ∀Q′∈Q ,
imax i imax 1 imax 1 i i 1
and
√
y (Q′)−y (Q′)≤y (Q′)−λ m (Q′)+λ m (Q′)−y (Q′)≤2ρ λ /∣Q′∣ ∀Q′∈Q .
i imin i 1 i 1 imin imin 1
Combined with (41), this yields
√
∣y i(Q′)−y ι(Q′)(Q′)∣≤4ρ λ 1/∣Q′∣ ∀Q′∈Q ,
which concludes the proof by definition of P¯.
Next, we prove a lemma, that relates the empirical medians
ι(Q′)
to the actual median i.
LemmaC.4. LetN ∈N∗,a ≥a ≥⋅⋅⋅≥a andb ,b ,...,b suchthat∣a −b ∣≤Rfori=1,...,N.
1 2 N 1 2 N i i
Consider ι such that ∣{i∶ b i>b ι}∣<N/2 and ∣{i∶ b i<b ι}∣≤N/2. Then ∣a⌈N/2⌉−a ι∣≤2R.
32Proof of Lemma C.4. Weprovethestatementbycontradiction. Assumefirstthata⌈N/2⌉−a ι>2R.
Then, for i≤⌈N/2⌉, we have
b i−b ι≥a i−a ι−2R≥a⌈N/2⌉−a ι−2R>0 ,
so b
i
>b ι, which is a contradiction to ∣{i∶ b
i
>b ι}∣<N/2. So that a⌈N/2⌉−a
ι
≤2R. Similarly, we
prove that a ι−a⌈N/2⌉≤2R and conclude the proof.
Proof of Lemma B.3. For the entire proof, assume that Property 1 is satisfied, and that we are on
the event ξ. To simplify notation, we assume in what follows w.l.o.g. that π=id[n] and η=id[d].
Property 1 states that
Q∗(E)={j ∈[d]∶ maxM ≥p+h, minM ≤p−h}⊆Q .
ij ij
i∈E i∈E
Hence, for any j ∈[d]∖Q, it holds that
maxM ≤maxM <p+h or minM ≥minM >p+h ,
ij ij ij ij
i∈P i∈E i∈P i∈E
and consequently
∣{i∈P ∶ M ≤p−h}∣∧∣{i∈P ∶ M ≥p+h}∣=0 .
ij ij
This proves the equality of the sums in the statement of the lemma.
Throughout this proof, we will consider the quantities j ∶=max{j ∈Q∶ M ≥p} and
ij
√
O∶={i∈E ∶ ∃Q′∈Q s.t. y i(Q′)−y ι(Q′)(Q′)>8ρ λ 1/∣Q′∣ } . (42)
Recall that we have defined yj(E) in (37), which we have used to define in (39) the set
√ √
A = {j′ ∈ Q ∶ ∣yj′ (E)−yj(E)∣ ≤ 2ρ λ /∣E∣} and for c ∈ {2,3,...,⌈ λ ∣E∣/2ρ+1⌉} the sets
j √ 1 √ 1 √
Ql(c) = {j′ ∈ Q ∶ 2ρ λ /∣E∣ < yj′ (E)−yj(E) ≤ 2cρ λ /∣E∣} and Qr(c) = {j′ ∈ Q ∶ 2ρ λ /∣E∣ <
j √1 1 j 1
yj(E)−yj′
(E)≤2cρ λ /∣E∣}. Let us define further
1
√
c∗∶=inf{c∈{1,2,...,⌈ λ ∣E∣/2ρ⌉}∶λ ∣Qr(c+1)∣>576ρ2/h2} ,
r √ 1 1 j
c∗∶=inf{c∈{1,2,...,⌈ λ ∣E∣/2ρ⌉}∶λ ∣Ql(c+1)∣>576ρ2/h2} ,
l 1 1 j
√
with inf∅=∞, ∞+1=∞ and the conventions Ql(∞)={j′ ∈Q∶2ρ λ /∣E∣<yj′ (E)−yj(E)},
√ j 1
Qr(∞)={j′∈Q∶2ρ λ /∣E∣<yj(E)−yj′ (E)} and Qr(1)=Ql(1)=∅.
j 1 j j
We will provide an error decomposition on the sets of questions
R∶=Qr(c∗) , R∶=Qr(c∗+1) , R∶=Qr(c∗+2) , ∆ ∶=R∖R , (43)
j r j r j r R
L∶=Ql(c∗) , L∶=Ql(c∗+1) , L∶=Ql(c∗+2) , ∆ ∶=L∖L ,
j l j l j l L
A∶=A ,
j
We will show following upper bounds:
• R˜ ≤144ρ2(∣E∣∨∣Q∣)/λ h2, R˜ ≤144ρ2(∣E∣∨∣Q∣)/λ h2,
p,h,P,∆R 1 p,h,P,∆L 1
• R˜ ≤576ρ2(∣E∣∨∣Q∣)/λ h2, R˜ ≤576ρ2(∣E∣∨∣Q∣)/λ h2,
p,h,P,R 1 p,h,P,L 1
• R˜ ≤2304ρ2(∣E∣∨∣Q∣)/λ h2 and
p,h,P,A 1
• R˜ =0.
p,h,P,Q∖(L∪A∪R)
33j j R
R R
\
O
i
A R ∆R
Figure 4: Illustration of a part of M . The dotted curves separate two areas of the matrix:
E,Q
That where the entries are at least p+h (top left) and that where they are at most p−h (bottom
right). The curve in between separates values that are at least p from values that are smaller
than p. Question j is the “last” question for which the median expert i has value at least p. Our
algorithm relieson two detection steps: First, weuse
Y˜(a)
and thecorresponding columnaverages
y (j), to detect areas of interest left and right of j (see (43) for a definition of these sets). Then,
E
we detect from
Y˜(b)
, whether an expert is above or below i and for that purpose, we focus on
the following sets of questions: First, A corresponds to the questions for which we cannot detect
from our observation, whether they are left or right of j. Second, R contains questions that are
provablyrightj,butthesizeofRistoosmallforreliablydetectingexpertsaboveifromthegiven
observation
Y˜(b)
. Though, we can detect those relying on averages on the larger sets of questions
R and R. As a consequence, experts that differ from i on this whole areas (in particular have all
values at least p+h) are assigned to O by (42). Following this, experts that remained in P cannot
“perform better” than p+h on every question in R∖R and consequently there exists j ∈R with
R
M <p+h for all i∈P. By the bi-isotonicity, only questions j <j can contribute to the error
ijR R
we want to bound, which is why we extend our analysis to ∆ =R∖R.
R
Summing them up concludes the proof. We want to point out, that in the case c∗ = 1 or c∗ = 1,
r l
parts of the analysis become trivial, this is even more true if c∗ =∞ or c∗ =∞. In the case c∗ =1
r l r
for example, we have R = ∅ and therefore R˜ = 0. In the case c∗ = ∞, we obtain ∆ = ∅
p,h,P,R r R
which implies R˜ =0. Further, note that by (38) and M taking values in [0,1], it holds
p,h,P,∆R
√
yj(E)−yj(E)≤2ρ λ /∣E∣+λ mj(E)−λ mj(E)
1 1 1
√ √
≤(2+ λ ∣E∣/ρ)ρ λ /∣E∣
1 1
√ √
≤2⌈ λ ∣E∣/2ρ+1⌉ρ λ /∣E∣ ,
1 1
√
For c∗ = ∞, this implies R = Qr(∞) = Qr(⌈ λ ∣E∣/2ρ+1⌉) and therefore λ ∣R∣ ≤ 576ρ2/h2 (oth-
r √j j 1 1
erwise, this would mean c∗ ≤ ⌈ λ ∣E∣/2ρ⌉), which is sufficient for the analysis on R we provide
r 1
below.
Beforeprovidingtheproofs,letusgivesomeintuitionfortheanalysis. DuringScandAndUpdate,
we construct sets Q′ ⊆Q in order to have meaningful comparisons of experts i∈E, in particular
with i. Assume we want to detect i<i. We will see, that given a set Q′ ⊆Q with ∣Q′∣≳ ρ2 and
M ≤p for all j ∈Q′ , we are able to detect all i∈E with M ≥p+h for all j ∈Q′
whicλ h1h b2
y the
ij ij
34bi-isotonicity of M would be sufficient for π(i)<π(i) and will lead to i∈O. We will see that the
constructed sets R and R have such properties, see Figure 4 for an illustration. The bi-isotonicity
assumption will then yield that there exist j ∈R with M <p+h for all i∈P. Informally, this
R ij
means that questions j ≥j don’t contribute to the error in the sense that
R
∣{i∈P ∶ M ≤p−h}∣∧∣{i∈P ∶ M ≥p+h}∣=0.
ij ij
In the same way, we can look at L and construct j ∈L with j ≤j not contributing to the error.
L L
By looking at R∪A∪L, we capture all j with j <j <j , so all j that might contribute to the
L R
error R˜ .
p,h,P,Q
We further decompose R = R∪∆ . The set R was considered too small for a meaningful
R
comparison of experts i∈E, but this will turn out to lead also to a simple bound of R˜ .
p,h,P,R
By construction, the questions in ∆ cannot be much easier than j , this means that we have
R R
a good control of the number of i ∈ P with M ≥ p+h for j ∈ ∆ . At the same times, P is by
ij R
definition a set of experts i∈P for which we have a control over the deviation from i on R. Since
M ≤p for all j ∈∆ , we can also bound the number of questions j such that M ≥p+h. We will
ij R ij
see that combining these bounds will allow us to bound R˜ , and we can bound R˜
p,h,P,∆R p,h,P,∆L
in the same manner.
A contains all questions j for which we could not detect whether j ≥j or j <j. Still, we can
decomposeAintoquestionsj ∈Awithj ≥j andquestionsj <j. Wewillseethatdependingonthe
respective sizes of the sets, either the ideas of the analysis on R and L or concepts of the analysis
on ∆ and ∆ carry over.
R L
Analysis on ∆ and ∆ . We will only prove the upper bound for ∆ , since it can be easily
R L R
adapted for ∆ . More precisely, we will use
L
R˜ ≤∣{(i,j)∈P ×∆ ∶ M ≥p+h}∣ .
p,h,P,∆R R ij
If i ∶=minP and j ∶=min∆ , the bi-isotonicity of M yields
min min R
{(i,j)∈P ×∆ ∶ M ≥p+h}⊆{i∈P ∶ M ≥p+h}×{j ∈∆ ∶ M ≥p+h} ,
R ij ijmin R iminj
and we will bound
R˜ ≤∣{i∈P ∶ M ≥p+h}∣⋅∣{j ∈∆ ∶ M ≥p+h}∣ .
p,h,P,∆R ijmin R iminj
This will be done in four steps:
a1) M <p for all j ∈R,
ij
a2) M ≤p+h/2 for j ∶=maxR and all i∈P,
ijmax max
√ √
a3) ∣{i∈P ∶ M ≥p+h}∣≤12ρ ∣E∣/ λ h,
ijmin 1
√
√
a4) ∣{j ∈∆ ∶ M ≥p+h}∣≤12ρ ∣R∣/ λ h.
R iminj 1
From a3) and a4), it follows directly the desired bound
√
R˜ ≤144ρ2 ∣E∣⋅∣Q∣/λ h2≤144ρ2(∣E∣∨∣Q∣)/λ h2 .
p,h,P,∆R 1 1
Proof of a1): By definition of R in Equation (39) with Equation (43) and the concentration
inequality in Equation (38), we know for every j ∈R, that
√
λ (mj(E)−mj(E))≥yj(E)−yj(E)−2ρ λ /∣E∣>0 .
1 1
The bi-isotonicity of M implies j >j and also M <p by definition of j.
ij
35Proof of a2): Let i∈P. From the concentration equality in Equation (40), Lemma C.4 and
the definition of P in Equation (27) we know that
√
λ (m (R)−m (R))≤λ (m (R)−m (R))+2ρ λ /∣R∣
1 i i 1 i ι(R) 1
√ √
≤y (R)−y (R)+4ρ λ /∣R∣≤12ρ λ /∣R∣ . (44)
i ι(R) 1 1
√
This together with a1) implies m (R) ≤ p+12ρ/ λ /∣R∣ ≤ p+h/2, since λ ∣R∣ ≥ 576ρ2/h2 by
i 1 1
definition. Finally, by bi-isotonicity of M, we conclude that M ≤m (R)≤p+h/2.
ijmax i
Proof of a3): Recall that j = min∆ while j = maxR, so j < j . Note that
min R max min max
because of a2)
∣E∣⋅(mjmin(E)−mjmax(E))= ∑M −M
ijmin ijmax
i∈E
≥ ∑ M −M
ijmin ijmax
i∈P, Mijmin≥p+h
≥(h/2)⋅∣{i∈P ∶ M ≥p+h}∣ . (45)
ijmin
If yjmax(E)≥yjmin(E), concentration inequality (38) yields
√ √
λ (mjmin(E)−mjmax(E))≤yjmin(E)−yjmax(E)+2ρ λ /∣E∣≤2ρ λ /∣E∣ .
1 1 1
Otherwise, by definition of ∆ and R in (39) and (43),
R
√ √
yj(E)−2(c∗+2)ρ λ /∣E∣≤yjmax(E)<yjmin(E)<yj(E)−2c∗ ρ λ /∣E∣ ,
r 1 r 1
which yields
√ √
λ (mjmin(E)−mjmax(E))≤yjmin(E)−yjmax(E)+2ρ λ /∣E∣≤6ρ λ /∣E∣ .
1 1 1
√ √
This together with (45) gives us ∣{i∈P ∶ M ≥p+h}∣≤12ρ ∣E∣/ λ h.
ijmin 1
Proof of a4): In a1) we have seen M < p for all j ∈ R. So we can argue like in (45) and
ij
obtain
∣{j ∈∆ ∶ M ≥p+h}≤∣R∣⋅(m (R)−m (R))/h .
R imaxj imin i
From (44) we know
√
λ (m (R)−m (R))≤12ρ λ /∣R∣ ,
1 imin i 1
√ √
and end up with ∣{j ∈∆ ∶ M ≥p+h}∣≤12ρ ∣R∣/ λ h .
R iminj 1
Analysis on R and L. We will only consider R as the proof is similar for L. From the con-
struction of R in Equation (43), it follows that λ ∣R∣ ≤ 576ρ2/h2. We obtain the trivial upper
1
bound
R˜ ≤∣P ×R∣≤576ρ2∣P∣/λ h2≤576ρ2(∣E∣∨∣Q∣)/λ h2 , (46)
p,h,P,R 1 1
where for the last step we just used the rough inequality ∣P∣≤∣E∣≤∣E∣∨∣Q∣.
Analysis on A. Recall that j ∈A was the last question j such that M ≥p holds. We will split
ij
up A into Al ∶={j ∈A∶ j ≤j}={j ∈A∶ M ≥p} and Ar ∶={j ∈A∶ j >j}={j ∈A∶ M <p}. If
ij ij
λ ∣A∣≤2304ρ2/h2, we can conclude just like in (46) that
1
R˜ ≤2304ρ2(∣E∣∨∣Q∣)/λ h2 .
p,h,P,A 1
So assume w.l.o.g. that λ ∣A∣ > 2304ρ2/h2 and ∣Ar∣ ≥ ∣Al∣, so in particular λ ∣Ar∣ ≥ λ ∣A∣/2 >
1 1 1
1152ρ2/h2. Again, we will derive the upper bound in several steps:
36√ √
b1) ∣{j ∈Ar ∶ M ≥p+h}∣≤12ρ ∣A∣/ λ h for i =minP,
iminj 1 min
b2) M <p+h/2 for j =maxA and all i∈P,
ijmax max
√ √
b3) ∣{i ∈ P ∶ M ≥ p+h}∣ ≤ 8ρ ∣E∣/λ h2 and ∣{i ∈ P ∶ M ≥ p+h}∣ ≤ 12ρ ∣E∣/λ h2 for
ij 1 ijmin 1
j =minA,
min
√
b4) ∣{j ∈Al∶ M ≤p−h}∣≤12ρ ∣A∣/λ h2 for i =maxP.
imaxj 1 max
Once this is proven, one can bound R˜ =R˜ +R˜ . It holds
p,h,P,A p,h,P,Ar p,h,P,Al
R˜ ≤∣{(i,j)∈P ×Ar ∶ M ≥p+h}∣
p,h,P,Ar ij
≤∣{i∈P ∶ M ≥p+h}∣⋅∣{j ∈Ar ∶ M ≥p+h}∣≤96ρ2(∣E∣∨∣Q∣)/λ h2 ,
ij iminj 1
by b1) and b3) and
R˜ = ∑ ∣{i∈P ∶ M ≤p−h}∣∧∣{i∈P ∶ M ≥p+h}∣
p,h,P,Al ij ij
j∈Al
≤ ∑ ∣{i∈P ∶ M ≥p+h}∣
ij
j∈Al, Mimaxj≤p−h
≤∣{j ∈Al∶ M ≤p−h}∣⋅∣{i∈P ∶ M ≥p+h}∣≤144ρ2(∣E∣∨∣Q∣)/λ h2 ,
imaxj ijmin 1
by b4) and b3), so in total, we would end up with R˜ ≤240ρ2(∣E∣∨∣Q∣)/λ h2.
p,h,P,A 1
Proof of b1) and b2): Let i =minP. Like in (44) one can prove
min
√
λ (m (A)−m (A))≤12ρ λ /∣A∣ .
1 imin i 1
This implies
∣Ar∣(m (Ar)−m (Ar))= ∑ M −M
imin i iminj ij
j∈Ar
≤ ∑M −M
iminj ij
j∈A
√ √
=∣A∣(m (A)−m (A))≤12ρ ∣A∣/λ ≤ρ 288∣Ar∣/λ .
imin i 1 1
By definition of Ar, we know that M < p for all j ∈ Ar, so we can show b1) by arguing like in
ij
(45) that
√ √
∣{j ∈Ar ∶ M ≥p+h}∣≤∣Ar∣(m (Ar)−m (Ar))/h≤12ρ ∣A∣/( λ h) .
iminj imin i 1
For showing b2), note that it holds
∑ M −M >∣Ar∣(m (Ar)−p) ,
iminj ij imin
j∈Ar
which implies
√
m (Ar)<p+ρ 288/(λ ∣Ar∣)<p+h/2
imin 1
by the assumption λ ∣Ar∣>1152ρ2/h2. For any i∈P we conclude by the bi-isotonicity of M
1
M ≤M ≤m (Ar)<p+h/2 .
ijmax iminjmax imin
Proof of b3): By the definition of A in (39) and (43) we know that ∣yj(E)−yjmax(E)∣ ≤
√ √
2ρ λ /∣E∣ and ∣yjmin(E)−yjmax(E)∣≤4ρ λ /∣E∣. With concentration inequality (38) this yields
1 1
√ √
λ (mj(E)−mjmax(E))≤4ρ λ /∣E∣ and λ (mjmin(E)−mjmax(E))≤6ρ λ /∣E∣ .
1 1 1 1
Like in (45), using M ≤p+h/2 for i∈P from b2), we obtain
ijmax
√
∣{i∈P ∶ M ≥p+h}∣≤2∣E∣(mj(E)−mjmax(E))/h≤8ρ ∣E∣/λ h2
ij 1
and
√
∣{i∈P ∶ M ≥p+h}∣≤2∣E∣(mjmin(E)−mjmax(E))/h≤12ρ ∣E∣/λ h2 .
ijmin 1
37Proof of b4): This follows by adapting the proof of b1).
Analysis on Q∖(L∪A∪R). If j ∈Q but j ∉L∪A∪R, then we have either
√ √
yj(E)−yj(E)>2(c∗+2) λ /∣E∣ or yj(E)−yj(E)>2(c∗+2) λ /∣E∣ .
r 1 l 1
Assume w.l.o.g. that the first inequality holds. For j =maxR, we know
max
√
yj(E)−yjmax(E)≤2(c∗+1) λ /∣E∣
r 1
and therefore
√
yjmax(E)−yj(E)≤2 λ /∣E∣ .
1
The concentration inequality (38) implies λ (mjmax(E)−mj(E))≤0 and the bi-isotonicity of M
1
yields j >j . Like in a2), we deduce M ≤p+h/2 for all i∈P, so ∣{i∈P ∶ M ≥p+h}∣=0.
max ij ij
C.4 Proof of Lemma B.4
Consider Y˜ sampled accordingly to observation model in Equation (12), E =(E ,E ,...,E ) and
1 2 r
a vector v∈{0,1}r. Let us redefine
1 1
yj(E )= ∑ Y and mj(E )= ∑ M ∀s=1,2,...,r, ∀j ∈[d] .
s ∣E ∣ ij s ∣E ∣ ij
s i∈Es s i∈Es
Consider the event ξ where the inequalities
env
√
∣yj(E )−λ mj(E )∣≤ρ λ /∣E ∣ ∀s=1,2,...,r with v =1, ∀j ∈[d] , (47)
s 1 s 1 s s
hold true.
Lemma C.5. Under Property 2 for Y˜, E and v, it holds that
δ
P(ξ )≥1− .
env 12⌈log (n∨d)⌉(n∨d)1/2
2
Proof of Lemma C.5. Recall Property 2 for Y˜, E and v from Section 4.5. Then for each s =
1,2,...,r with v =1, we know that λ ∣E ∣>4ρ2/h2. Lemma G.1 yields that each inequality
s 1 s
√
∣yj(E)−λ mj(E)∣≤ρ λ /∣E ∣ ,
1 1 s
holds with a probability of at least 1− δ , just like in the proof of Lemma C.1.
12⌈log (n∨d)⌉nd(n∨d)(1/2)
2
For each s we are considering d such inequalities. Because E is a partition of [n] by Property 2,
we can roughly upper bound ∣{s = 1,2,...,r ∶ v = 1}∣ ≤ n, so the total number of inequalities
s
consideredisboundedbynd. AunionboundargumentyieldsP(ξ )≥1− δ .
env 12⌈log (n∨d)⌉(n∨d)1/2
2
For s=1,2,...,r we can define
√
Q(E )∶={j ∈[d]∶ yj(E )≥λ (p+h)−ρ λ /∣E ∣} ,
s s 1 1 s
√
Q(E )∶={j ∈[d]∶ yj(E )≤λ (p−h)+ρ λ /∣E ∣} .
s s 1 1 s
If v = 1, Algorithm 5 checks whether s = max{t < s ∶ v = 1} exists. If this is the case, it sets
s t
Q = Q(E ), otherwise Q = [d]. In the same way Q = Q(E ) or Q = [d], depending on the
s s s s s s
existence of s=min{t>s∶ v =1}. The algorithm then returns Q =Q ∩Q . See Figure 5 for an
t s s s
illustration of the construction of Q .
s
38E
s
E
s
E
s
Q
s
Figure 5: Illustration of some bi-isotonic matrix M ∈ C Biso(id[n],id[d]). The part in the top left
(light red background) corresponds to matrix values ≥ p+h, the part on the bottom right (light
blue background) to matrix values ≤p−h and the area in between (purple background) to values
in (p−h,p+h). Assume we have E , E , E ∈E such that each of the sets is larger than 4ρ2/λ h2
s s s 1
and i<i<i for i∈E , i∈E and i∈E . We are interested in estimating the questions Q∗(E ),
s s s s
which correspond to the gray area. To do so, we detect sets that contain all questions on which
the success-probabilities are at least p+h for all experts in E (red dashed area) and at most p−h
s
for all experts in E (blue dashed area). Intersecting these sets yields Q .
s s
Proof of Lemma B.4. Consider s∈{1,2,...,r} with v =1 and j ∈Q∗(E ). Then
s s
maxM ≥p+h and minM ≤p−h .
ij ij
i∈Es i∈Es
Assume w.l.o.g. that s = max{t < s ∶ v = 1} exists; otherwise j ∈ [d] = Q would be trivial. By
t s
Property 2, π(i) < π(i′) for all i ∈ E and i′ ∈ E . The bi-isotonicity assumption therefore yields
s s
M ≥p+h for all i∈E , which implies mj(E )≥p+h. Since we assume that we are in the regime
ij s s
of event ξ , it follows from (47)
env
√ √
yj(E )≥λ mj(E )−ρ λ /∣E ∣≥λ (p+h)−ρ λ /∣E ∣
s 1 s 1 s 1 1 s
and consequently j ∈Q . In the same manner, one can show j ∈Q . This proves
s s
Q∗(E )⊆Q ∩Q =Q
s s s s
For the second claim it suffices to show that for each j ∈ [d] there are at most three s ∈
{1,2,...,r} with v =1 such that j ∈Q . We show this by contradiction. So assume 1≤s<t<u<
s s
w≤r exist with v =v =v =v =1 and j ∈Q ∩Q ∩Q ∩Q .
s t u w s t u w
We obtain j ∈Q ∩Q , so
s w
√ √
yj(E )≤λ (p−h)+ρ λ /∣E ∣ and yj(E )≥λ (p+h)−ρ λ /∣E ∣ .
s 1 1 s w 1 1 w
It holds s≤t<u≤w and v =v =1, and Property 2 implies
s w
λ ∣E ∣>4ρ2/h2 and λ ∣E ∣>4ρ2/h2 ,
1 s 1 w
which is equivalent to
√ √
λ h>2ρ λ /∣E ∣ and λ h>2ρ λ /∣E ∣ .
1 1 s 1 1 w
All this together with (47) yields
√ √
λ mj(E )≤yj(E )+ρ λ /∣E ∣≤λ (p−h)+2ρ λ /∣E ∣<λ p ,
1 s s 1 s 1 1 s 1
and
√ √
λ mj(E )≥yj(E )−ρ λ /∣E ∣≥λ (p+h)−2ρ λ /∣E ∣>λ p .
1 w w 1 s 1 1 s 1
So in particular mj(E ) < mj(E ). This means that there exist i ∈ E and i ∈ E with M <
s w s w ij
M . The bi-isotonicity assumption implies π(i) > π(i), and together with s < w, this yields a
ij
contradiction to Property 2.
39C.5 Proofs of Theorem B.5 and Corollary B.6
We will see, that for each active set Ek−1, i.e. wk−1 =1, our algorithm relies on inequalities of the
t t
form
∣E
tk1
−1∣RRRRRRRRRRRRRi∈E∑
tk−1Y i( j3k−1) −λ 1M
ijRRRRRRRRRRRRR≤ρ√
λ 1/∣E tk−1∣ ∀j ∈Qk t−1 ,
and
∣Q1
′∣RRRRRRRRRRRj∑
∈Q′Y i( j3k) −λ 1M
ijRRRRRRRRRRR≤ρ√
λ 1/∣Q′∣ ∀Q′∈Qk t−1, i∈E tk−1 , (48)
where
Qk−1
is defined by sets of the form given in Equation (39).
t
Proof of Theorem B.5. We will prove the claims via induction over k.
The case k = 1: If λ n ≤ 4ρ2/h2, there is nothing to prove, so assume λ n > 4ρ2/h2. By
1 1
definition, Algorithm 5 returns Q0 = [d] which shows B.5.1. If λ d ≤ 4ρ2/h2 it sets w0 = (0),
1 1
E1=([n]), v1=(0) and G1=G0. One can easily check that in this case, all claims are fulfilled. If
λ d>4ρ2/h2, let us consider the concentration inequalities
1
1 ∣∑n Y(2)
−λ M
∣≤ρ√
λ /n ∀j ∈[d] ,
n i=1 ij 1 ij 1
and
∣Q1
′∣RRRRRRRRRRRj∑
∈Q′Y i( j3) −λ 1M
ijRRRRRRRRRRR≤ρ√
λ 1/∣Q′∣ ∀Q′∈Q0
1
, i∈[n] .
Here, Q0 is defined as Q in Section C.2 by sets of the form (39) with respect to E =[n]. We call
1
ξ1 the event, on which these inequalities hold. Since Property 1 for Y(2) , Y(3) , [n], [d] and G is
obviously fulfilled, Lemma C.1 yields
δ
P(ξ1)≥1− .
12⌈log (n∨d)⌉
2
The claims B.5.2.a (and therefore B.5.5) and B.5.2.b follow directly from Lemma B.2. By
0
B.5.2.b and the definition of i in (25) we obtain ∣O([n])∣ ≤ n/2 and ∣I([n])∣ ≤ n/2, which yields
1
B.5.2.c. Property B.5.3 for the graph Gk is consequence of Lemma B.1. In order to prove part
B.5.4, note that v1 =0. If v1 =v1 =1, the claim follows from B.5.2.b. Otherwise, there is nothing
2 1 3
to prove.
From k−1 to k: Assume (induction assumption) we have constructed events ξ1⊂ξ2⊂⋅⋅⋅⊂ξk−1
that satisfy the theorem (up to step k−1). From now on we work on the event ξk−1. Algorithm 5
is applied on
• Y(3k−2) , an unused sample from observation model (12),
• Ek−1, which by induction hypothesis (i.e. B.5.5) is a partition of [n],
• vk−1, for which it holds that
– vk−1=1 ⇒ λ ∣Ek−1∣>4ρ2/h2 by definition,
t 1 t
– vk−1 = vk−1 = 1 for s < s′ implies π(i) < π(i′) for i ∈ Ek−1, i′ ∈ Ek−1 by induction
t t′ t t′
hypothesis (i.e. B.5.4).
40This implies that, conditionally to
ξk−1,
the input
Y(3k−2)
,
Ek−1
and
vk−1
satisfies Property 2. So,
as in Section 4.5, we can construct an event
Ξk−1
with
0
P(Ξk−1 ∣ ξk−1)≥1− δ
0 12⌈log (n∨d)⌉(n∨d)1/2
2
by Lemma C.5 and such that, thanks to Lemma B.4, it holds on
ξk−1∶=ξk−1∩Ξk−1
that
0 0
r
Q∗(E tk−1)⊆Qk t−1, ∀t∈{1,2,...,r k−1} with v tk−1=1, and ∑ ∣Q s∣<3d . (49)
t=1, vk−1=1
t
This proves part B.5.1 on
ξk−1
. In order to prove part B.5.2, consider t to be the smallest
0 1
t∈{1,2,...,r k−1} with w tk−1=1. Conditionally to ξ 0k−1, we apply Algorithm 2 to
• Y(3k−1) and Y(3k) , which are independent samples from observation model (12),
• Q∗(Ek−1)⊆Qk−1 by (49),
t1 t1
• λ (∣Ek−1∣∧∣Qk−1∣)>4ρ2/h2 by the induction assumption (i.e. B.5.1),
1 t1 t1
• Gk t1 = Gk−1 with G i,i′ = 1 ⇒ π(i) < π(i′) for all i,i′ ∈ [n] by the induction assumption
(i.e. B.5.3).
Then, conditionally to ξ 0k−1, Property 1 is again satisfied by Y(3k−1) , Y(3k) ,E tk 1−1, Qk t1−1 and Gk t1−1.
Consider the concentration inequalities of the form
∣E
tk1
1−1∣RRRRRRRRRRRRRRi∈E∑
tk
1−1Y i( j3k−1) −λ 1M
ijRRRRRRRRRRRRRR≤ρ√
λ 1/∣E tk 1−1∣ ∀j ∈Qk t1−1 ,
and
∣Q1
′∣RRRRRRRRRRRj∑
∈Q′Y i( j3k) −λ 1M
ijRRRRRRRRRRR≤ρ√
λ 1/∣Q′∣ ∀Q′∈Qk t1−1, i∈E tk 1−1 .
Again, Qk−1 is defined as Q in Section C.2 by sets as in Equation (39), this time with respect to
E
=Ek−1.t1
Lemma C.1 again tells us that given ξk the event Ξk−1 on which these inequalities hold
t1 0 t1
satisfies
∣Qk−1∣δ
P(Ξk−1 ∣ ξk−1)≥1− t1 .
t1 0 12⌈log (n)⌉d
2
We define
ξk−1∶=ξk−1∩Ξk−1
and obtain that conditional to
ξk−1,
Lemma B.2 implies B.5.2.a and
B.5.2.b. Sint c1 e w 0 =1, wet k1 now that on ξk−1, the set Ek−1 itset l1 f results from a trisection such that
t1 t1
by induction hypothesis (i.e. B.5.2.c)
∣Ek−1∣≤2−(k−1)
n. With B.5.2.b we obtain again that
t1
∣O(Ek−1)∣∨∣I(Ek−1)∣≤ 1 ∣Ek−1∣≤2−kn
,
t1 t1 2 t1
which yields B.5.2.c. The properties B.5.3 hold for
Gk−1
by Lemma B.1.
t1
We then move on to the smallest t >t with w =1. It is worth mentioning, that although we
use again
Y(3k−1)
and
Y(k)
for the
tri2 secti1
on,
sincet2
Ek−1
and
Ek−1
are disjoint, these observations
canstillbeconsideredasunused.
Then,togetherwitht2 LemmaBt1
.1,conditionaltoξk−1,Property1
t1
a ag sa ii nn th ho eld ss tef por bY ef( o3 rk e−1 a) , ndY( c3 hk e) c, kE ttk 2 h− a1, tQ alk t l2− p1 ra on pd erG tik t e2 s−1 o. fW B.e 5.t 2he hn old defi wn he enΞk t w2−1 ea trn id seξ ctk t2− E1∶ k= −1ξ ,tk 1− a1 n∩ dΞ ak t l2− so1
B.5.3 for the updated graph
Gk−1
follows in the same manner.
t2
t2
Byiteratingoveralltwithwk−1=1untilthelargestsuchindext ,weobtainaneventξk =ξk−1
t α tα
such that B.5.2 holds uniformly and the graph Gk ∶= Gk that satisfies by iteration B.5.3. Note
tα
that indeed, with B.5.1, it holds
P(ξk)=P(ξk−1 ∩Ξk−1)=P(ξk−1)⋅P(Ξk−1 ∣ ξk−1)≥P(ξk−1)−P((Ξk−1)C ∣ ξk−1)
tα−1 tα tα−1 tα tα−1 tα−1 tα tα−1
α
=⋯≥P(ξk−1)−P((Ξk−1)C ∣ ξk−1)−P((Ξk−1)C ∣ ξk−1)−∑P((Ξk−1)C ∣ ξk−1)
0 t1 0 ta ta−1
a=2
≥1−
(k−1)δ
−
δ
−
∑α a=1∣Qk ta−1∣δ
2⌈log (n∨d)⌉ 12⌈log (n∨d)⌉(n∨d)1/2 12⌈log (n∨d)⌉d
2 2 2
(k−1)δ δ 3dδ kδ
≥1− − − ≥1−
2⌈log (n∨d)⌉ 4⌈log (n∨d)⌉ 12⌈log (n∨d)⌉d 2⌈log (n∨d)⌉
2 2 2 2
41and that, on ξk, the graph Gk satisfies B.5.3.
In order to prove claim B.5.4, consider t, t′ ∈{1,2,...,r k−1} such that i∈E tk−1 and i′ ∈E tk ′−1.
By definition, i ∈ Ek and i′ ∈ Ek with s < s′ and vk = vk = 1 imply t ≤ t′ and vk−1 = vk−1 = 1. If
s s′ s s′ t t′
t<t′
, the claim
π(i)<π(i′)
follows directly by assumption. Otherwise, we know
i∈O(Ek−1)
and
t
i′∈I(Ek−1)
and B.5.2.b proves the claim.
t
Claim B.5.5 follows from the fact that we assumed that Ek−1 is a partition of [n]. By B.5.2.a,
Ek is obtained by replacing all active sets in Ek−1 by a trisection, which in total yields again a
partition of [n].
Number of iterations: Assume that for some k the vector vk is non-zero. By B.5.2.c we know
that for t with vk = 1 it holds ∣Ek∣ ≤ 2−kn ≤ 1. By definition, ∣Ek∣ > 4ρ2/λ h2 > 1. Therefore,
t t t 1
k ≤log 2(n). This implies, that K ≤⌈log 2(n)⌉ with v K−1 ≠0 and v K =0 exists. So the algorithm
terminates after round K on the event ξK, which holds with probability at least 1−δ/2.
Proof of Corollary B.6. On ξK, we assume that concentration inequalities of the form (48) hold.
Therefore, Corollary B.6.1–B.6.3 follow like in Lemma C.2. Corollary B.6.4 and B.6.5 are an
adaptation of Lemma C.3. The last point Corollary B.6.6 follows like Lemma B.3.
C.6 Proofs of Lemmas B.7–B.10
Proof of Lemma B.7. For K, the statement follows immediately from the definition of πˆ. Now
consider any k<K, and t<s. By the hierarchical construction of the sorting tree (see Figure 3),
it follows that Ek (resp. Ek) is the disjoint union of EK for indices l ∈ L (resp. l ∈ L ) with
t s l t t
max{l∶l∈L }<min{l∶l∈L }. For any i∈Ek and j ∈Ek, we have πˆ(i)<πˆ(j). It follows that πˆ
t s l s
is coherence with the partition Ek and the result follows.
Recall that, for E ⊆[n], we denote E˜ as its completion with respect to π.
E˜ ∶={i∈[n]∶ minπ(i′)≤π(i)≤maxπ(i′)} . (50)
i′∈E i′∈E
Proof of Lemma B.8. We consider in the entire proof that we are on the event ξK. Note that
ˆi∉E˜k impliesbydefinition(50)eitherthatπ(ˆi)>π(i′) ∀i′∈Ek orthatπ(ˆi)<π(i′) ∀i′∈Ek .
s s s
Assume w.l.o.g. that
π(ˆi)>π(i′) ∀i′∈Ek
. (51)
s
holds. Lemma B.7 implies that π(ˆi) = πˆ(i) ≤ ∑ s′≤s∣E sk ′∣. Since π is bijective on [n] (as it is
a permutation), it follows that there exists i′′ ∈ Ek with s′ < s such that π(i′′) > π(ˆi) holds.
s′
Moreover, (51) implies
π(i)<π(ˆi)<π(i′′)
. (52)
By definition of the algorithm, there must be a maximal number l <k such that u∈{1,2,...,r }
l
exists with i,i′′∈El. By this maximality assumption, El must be an active set and i and i′′ must
u u
end up in different sets of the partition after step l (in terms of the sets O(El),P(El),I(El)) of
u u u
the corresponding trisection.
Since by assumption E sk has the form O(E tk−1) or I(E tk−1) for some t ∈ {1,2,...,r k−1}, the
definition of the algorithm does not allow i∈P(El). Therefore, because s′ <s, only i∈I(El) is
u u
possible. Since we stated in Equation (52) that π(i)<π(i′′), and since by definition of El, i and
u
′′
i must end up in different sets after step l, by Theorem B.5.2.b, it must then hold that
i′′∈P(El)
. (53)
u
Again with Theorem B.5.2.b, we can extend the inequalities (52) to
π(il )<π(i)<π(ˆi)<π(i′′)
,
u
42which implies that i,ˆi ∈ P˜(El) since both il and i′′ belong to P(El) (Theorem B.5.2.b) and
u u u
by definition of P˜(El) in (29). If ˆi ∈ El, then Corollary B.6.5 implies that i,ˆi ∈ P(El), which
u u u
completes the proof.
It remains to consider the case where ˆi ∉ El. Consider the maximal l′ < l such that u ∈
u
{1,2,...,r l′} exists with i,ˆi∈E ul′ ′. Again E ul′
′
must be an active set and after the trisection i (and
therefore i′′ ) must be contained in an active set. So in particular i,i′′ ∈ O(El′ ) or i,i′′ ∈ I(El′ ).
u′ u′
′
Notethatinbothcases,themaximalityassumptiononl andTheoremB.5.2.b,togetherwith(52)
implyˆi∈P(El′
).
u′
The proof is completed if we can show i ∈
P(El′
). In the case i ∈
I(El′
), note that, by
u′ u′
Theorem B.5.2.b together with Equation (52), it follows that
π(il u′ ′)<π(i)<π(ˆi) ,
and therefore
i∈P˜(El′
). Since
i∈El′
, the statement follows from Corollary B.6.5.
u′ u′
Hence, it only remains to consider the case where i ∈
O(El′
). To show that i ∈
P(El′
), by
u′ u′
definition of
P(El′
), we only need to establish that
u′
∣Q1
′∣RRRRRRRRRRRj∑
∈Q′[Y i( j3l′) −Y ι(
l
u′3 ′l (′ Q)
′)j]RRRRRRRRRRR≤8ρ√
λ 1/∣Q′∣ ∀Q′∈Ql u′ ′ . (54)
Consider such a set
Q′∈Q(El′
). Note that
u′
√
1
∑
[Y(3l′) −Y(3l′)
]>2ρ λ /∣Q′∣ ,
∣Q′∣
j∈Q′
ιl u′ ′(Q′)j ij 1
leads to
i∈I(El′
) by Corollary B.6.2, a contradiction. So we know that
u′
√
1
∑
[Y(3l′) −Y(3l′)
]≤2ρ λ /∣Q′∣ ,
∣Q′∣
j∈Q′
ιl u′ ′(Q′)j ij 1
so it remains to be shown that
√
1
∑
[Y(3l′) −Y(3l′)
]≤8ρ λ /∣Q′∣ .
∣Q′∣
j∈Q′
ij ιl u′ ′(Q′)j 1
We have seen earlier in the proof that also i ∈ I(El) holds. We can adapt the proof of
u
Lemma C.2: if for some
Q′∈Q(El′
) we have
u′
√
1
∑
[Y(3l′) −Y(3l′)
]>2ρ λ /∣Q′∣ ,
∣Q′∣
j∈Q′
ij ιl u(Q′)j 1
this would mean Gl′ =1 for at least half of the i′∈El ⊂El′ . Since the algorithm does not change
ii′ u u′
entries of the graph that are already set to 1, this would imply i∈O(El), a contradiction. So we
u
also have
√
1
∑
[Y(3l′) −Y(3l′)
]≤2ρ λ /∣Q′∣ . (55)
∣Q′∣
j∈Q′
ij ιl u(Q′)j 1
In the same spirit, we can show that
∣Q1
′∣RRRRRRRRRRRj∑
∈Q′Y ι(
l
u3 (l Q′) ′)j−Y i( ′′3
jl′)RRRRRRRRRRR≤2ρ√
λ 1/∣Q′∣ , (56)
becauseotherwisewewouldendupwithacontradictiontoi′′∈P(El)from(53)byCorollaryB.6.3.
u
Recall from Equation (52) that
π(ˆi)<π(i′′).
The bi-isotonicity assumption and (48) yield
√
1
∑
[Y(3l′) −Y(3l′)
]≤
1
∑
[Y(3l′)
−λ
M(3l′)
]+
1
∑ [λ
M(3l′) −Y(3l′)
]≤2ρ λ /∣Q′∣ .
∣Q′∣ i′′j ˆij ∣Q′∣ i′′j 1 i′′j ∣Q′∣ 1 ˆij ˆij 1
j∈Q′ j∈Q′ j∈Q′
(57)
43Finally, from Lemma C.2, since we
haveˆi∈P(El′
), it follows
u′
∣Q1
′∣RRRRRRRRRRRj∑
∈Q′[Y ˆi( j3l′) −Y ι(
l
u′3 ′l (′ Q)
′)j]RRRRRRRRRRR≤2ρ√
λ 1/∣Q′∣ . (58)
Hence, we derive from (55–58) that
∣Q1
′∣RRRRRRRRRRRj∑
∈Q′[Y i( j3l′) −Y ι(
l
u′3 ′l (′ Q)
′)j]RRRRRRRRRRR≤8ρ√
λ 1/∣Q′∣ .
Then consider all
Q′∈Ql′
, we obtain (54), which concludes the proof.
u′
Proof of Lemma B.9. Ifˆi∈Ek =P(Ek−1),byCorollaryB.6.4thereisnothingtoprove. Soassume
s t
w.l.o.g. thatˆi∈Ek with s <s. We will consider the cases π(i)<π(ˆi) and π(i)>π(ˆi) separately.
s1 1
Case 1: π(i)<π(ˆi). Considerlmaximalsuchthatu∈{1,2,...,rl}existswithi,ˆi∈El. Assume
u u
first, that l=k−1. Since i∈P(E tk−1) and we just assumedˆi∈E tl∩⋃ s′<sE sk ′, onlyˆi∈O(E tk−1) is
possible. But then, by Theorem B.5.2.b, we have π(i) < π(ˆi) < π(ik−1 ), soˆi ∈ P˜(Ek−1)∩Ek−1 ⊆
t t t
P(Ek−1)
by Corollary B.6.5.
t
If l<k−1, then Theorem B.5.2.b does not allow thatˆi∈O(El) and i∈I(El). Furthermore,
u u
i∈P(El) would lead to a contradiction to wk−1=1. So from s <s, onlyˆi∈P(El) and i∈I(El)
u t 1 u u
is possible. From Theorem B.5.2.b it follows that π(il ) < π(i) < π(ˆi). Hence, we conclude that
u
i∈P˜(El)∩El ⊆P(El) by Corollary B.6.5.
u u u
Case 2: π(i) > π(ˆi). Note that we assumed ˆi ∈ Ek with s < s, but that from Lemma B.7 it
s1 1
follows
π(ˆi)=πˆ(i)> ∑ ∣E sk ′∣ .
s′<s
By the bijectivity of π and the Pigeonhole principle, this means that there must be s ≥ s and
2
i′∈Ek
such that
s2
π(i′)≤ ∑ ∣E sk ′∣<π(ˆi) .
s′<s
Consider now l maximal such that u∈{1,2,...,rl} exists with i′ ,ˆi∈El.
u u
If l = k−1, s < s ≤ s implies i′ ,ˆi ∈ Ek−1 and ˆi ∈ O(Ek−1). By Theorem B.5.2.b this means
1 2 t t
i′∈P(Ek−1).
Recall that
t
π(i′)<π(ˆi)<π(i)
,
soˆi∈P˜(Ek−1)∩Ek−1⊆P(Ek−1)
by Corollary B.6.5.
t t t
If l<k−1, note that again, by Theorem B.5.2.b,ˆi∈O(El) and i′ ∈I(El) is impossible, since
u u
we have π(i′) < π(ˆi). This leaves us with the possible casesˆi ∈ O(El), i′ ∈ P(El) orˆi ∈ P(El),
u u u
i′∈I(El). Note that s <s≤s implies i∈El.
u 1 2 u
Ifˆi∈O(El) and i′∈P(El), then by our assumptions and Theorem B.5.2.b it follows
u u
π(i′)<π(ˆi)<π(i)<π(il
)
u
and therefore i∈P˜(El)∩El ⊆P(El) with Corollary B.6.5.
u u u
Hence, it remains to consider the subcase where ˆi ∈ P(El) and i′ ∈ I(El). Since s < s, we
u u 1
know that i∈I(El). Consider Q′∈Ql. We will prove that
u u
∣Q1
′∣RRRRRRRRRRRj∑
∈Q′Y ι(
l
u3 (l Q) ′)j−Y i(
j3l)RRRRRRRRRRR≤8ρ√
λ 1/∣Q′∣ .
44Indeed, this property will imply that i∈P(El) and conclude the proof.
u
We have seen that i∉O(El), so Corollary B.6.1 implies
u
√
1 ∑ [Y(3l) −Y(3l) ]≤2ρ λ /∣Q′∣ ,
∣Q′∣
j∈Q′
ij ιl u(Q′)j 1
and we are left with the proof of
√
1 ∑ [Y(3l) −Y(3l) ]≤8ρ λ /∣Q′∣ . (59)
∣Q′∣
j∈Q′
ιl u(Q′)j ij 1
Sinceˆi∈P(El), Corollary B.6.2 yields
u
∣Q1
′∣RRRRRRRRRRRj∑
∈Q′Y ι(
l
u3 (l Q) ′)j−Y ˆi(
j3l)RRRRRRRRRRR≤2ρ√
λ 1/∣Q′∣ .
Since
π(i′)<π(ˆi),
the bi-isotonicity assumption and Equation (48) imply that
√
1 ∑ [Y(3l) −Y(3l) ]≤2ρ λ /∣Q′∣ .
∣Q′∣ ˆij i′j 1
j∈Q′
Hence, we have shown (59) if we can prove that
√
1 ∑ [Y(3l) −Y(3l) ]≤4ρ λ /∣Q′∣ . (60)
∣Q′∣ i′j ij 1
j∈Q′
Observe that
√
1 ∑ [Y(3l) −Y(3l) ]≤2ρ λ /∣Q′∣
∣Q′∣
j∈Q′
ιk t−1(Q′)j ij 1
musthold.
Otherwise,onecouldarguelikeintheproofofLemmaC.2andwouldobtaini∈I(Ek−1).
t
In the case i′ ∈Ek−1, note that s ≥s implies i′ ∉O(Ek−1). By an adaptation of the proof of
t 2 t
Lemma C.2, this implies
√
1 ∑ [Y(3l) −Y(3l) ]≤2ρ λ /∣Q′∣
∣Q∣′
j∈Q′
i′j ιk t−1(Q′)j 1
and we obtain Equation (60).
In the case i′ ∉ E tk−1, consider l′ maximal such that u′ ∈ {1,2,...,r l′} exists with i,i′ ∈ E ul′ ′.
Again, by Theorem B.5.2.b and
π(i′)<π(ˆi)<π(i)
,
it cannot hold i∈O(E ul′ ′) and i′∈I(E u′). Since w tk−1=1 we can also exclude i∈P(E ul′ ′). So s<s
2
only permits
i∈O(Ek′
) and
i′∈P(Ek′
).
u′ u′
Like before, one can show
√
1 ∑ [Y(3l) −Y(3l) ]≤2ρ λ /∣Q′∣ ,
∣Q′∣
j∈Q′
ιl u′ ′(Q′)j ij 1
otherwise i would be in
I(El′
). Similarly, we have
u′
√
1 ∑ [Y(3l) −Y(3l) ]≤2ρ λ /∣Q′∣
∣Q′∣
j∈Q′
ιl u′ ′(Q′)j ij 1
otherwise, i′ would be in O(El′ )). This shows Equation (60) and completes the proof.
u′
45Proof of Lemma B.10. First, for i ,i ∈[n], let us define
1 2
R (i ,i )∶=∣{j ∈[d]∶ M ≥p+h, M ≤p−h}∣+∣{j ∈[d]∶ M ≤p−h, M ≥p+h}∣ .
p,h 1 2 i1j i2j i1j i2j
Also, consider τ
∶=π−1○πˆ
and note that τ as permutation is bijective, as well as η. Therefore,
it holds that
n
R p,h(πˆ)=∑[∣{j ∈[d]∶ M π−1(i)η−1(j)≤p−h, M πˆ−1(i)η−1(j)≥p+h}∣
i=1
+ ∣{j ∈[d]∶ M π−1(i)η−1(j)≥p+h, M πˆ−1(i)η−1(j)≤p−h}∣]
n
=∑[∣{j ∈[d]∶ M ij ≤p−h, M τ(i)j ≥p+h}∣ + ∣{j ∈[d]∶ M ij ≥p+h, M iτ(j)≤p−h}∣]
i=1
rK
= ∑ ∑ R (i,τ(i))
p,h
s=1i∈EK
s
K rk K rk−1
= ∑ ∑ ∑ R (i,τ(i))+∑ ∑ ∑ R (i,τ(i)) , (61)
p,h p,h
k=1
E
tk∈t O= k1 ∪Iki∈E tk k=1
w
tkt ′′ −= 11 =1i∈P(E tk ′−1)
where, in the last line, we used the property that any set EK is either belongs to some Ok∪Ik or
s
is of the form P(Ek−1) for some t and k≤K. We will first provide upper bounds for
t′
∑ R (i,τ(i)) with Ek ∈Ok∪Ik (62)
p,h t
i∈Ek
t
and
∑ R p,h(i,τ(i)) with w tk ′−1=1 . (63)
i∈P(E tk ′−1)
Upper Bound for (62). W.l.o.g., let us consider Ek =O(Ek−1) with wk =0. Then,
t t′ t
∑ R (i,τ(i))= ∑ R (i,τ(i))+ ∑ R (i,τ(i)) .
p,h p,h p,h
i∈E tk i∈O(E tk ′−1), τ(i)∈O˜(E tk ′−1) i∈O(E tk ′−1), τ(i)∉O˜(E tk ′−1)
Recall that we defined
Q∗(Ek)={j ∈[d]∶ maxM ≥p+h, minM ≤p−h} ,
t ij ij
i∈Ek i∈Ek
t t
and consider j ∈[d], i ∈O(Ek−1)=Ek and i ∈O˜(Ek−1). By the bi-isotonicity assumption,
1 t′ t 2 t′
M ≤p−h⇒minM ≤p−h and M ≥p+h⇒maxM ≥p+h .
i2j
i∈Ek
ij i2j
i∈Ek
ij
t t
So if
M ≥p+h, M ≤p−h or M ≤p−h, M ≥p+h ,
i1j i2j i1j i2j
we deduce that j ∈Q∗(Ek). This implies that R (i ,i )≤∣Q∗(Ek)∣ and consequently
t p,h 1 2 t
∑ R
(i,τ(i))≤∣Ek∣⋅∣Q∗(Ek)∣
,
p,h t t
i∈O(E tk ′−1), τ(i)∈O˜(E tk ′−1)
Recall that we assumed wk =0 because Ek is not to be refined anymore. If ∣Ek∣>4ρ2/λ h2, the
t t t 1
algorithmcomputesQkbutitmusthold∣Qk∣≤4ρ2/λ h2. ByTheoremB.5.1,itholdsQk ⊆Q∗(Ek).
t t 1 t t
We conclude that
∑ R (i,τ(i))≤4
ρ2
⋅(∣Ek∣∨∣Q∗(Ek)∣) .
p,h λ h2 t t
i∈O(E tk ′−1), τ(i)∈O˜(E tk ′−1) 1
46Next, note that i ∈ O(Ek−1) but τ(i) ∉ O˜(Ek−1) implies, by Lemma B.8, that it exists l ≤ k
t′ t′
and u∈{1,2,...,r l−1} such that i,τ(i)∈P(E ul−1). We can therefore upper bound
K rl−1
∑ R (i,τ(i))≤∑ ∑ ∑ R (i,τ(i)) .
p,h p,h
i∈O(E tk ′−1), τ(i)∉O˜(E tk ′−1) l=1 wu ul−= 11 =1i τ∈E (itk )∈∩ PP (( EE lul −− 11 ))
u
Gathering the two previous inequalities, we conclude that
∑ R (i,τ(i))≤4
ρ2 ⋅(∣Ek∣∨∣Q∗(Ek)∣)+∑K r ∑l−1
∑ R (i,τ(i)) . (64)
i∈E tk p,h λ 1h2 t t l=1 wu ul−= 11 =1i τ∈E (itk )∈∩ PP (( EE lul −− 11 )) p,h
u
Upper Bound for (63). In the case Ek =P(Ek−1), we can apply Lemma B.9 and obtain again
t t′
some l≤k and u∈{1,2,...,r l−1} such that i,τ(i)∈P(E ul−1). This yields
K rl−1
∑ R (i,τ(i))≤∑ ∑ ∑ R (i,τ(i)) . (65)
p,h p,h
i∈E tk l=1 wu ul−= 11 =1i τ∈E (itk )∈∩ PP (( EE lul −− 11 ))
u
Consider El−1 with wl−1 =1 and an arbitrary bijection τ˜∶P(El−1)→P(El−1) with τ˜(i)=τ(i) if
t t u u
τ(i)∈P(El−1).
Then
u
∑ R (i,τ(i))≤ ∑ R (i,τ˜(i))
p,h p,h
i∈P(El−1) i∈P(El−1)
u u
τ(i)∈P(El−1)
u
= ∑ ∑ 1{M ij ≥p+h, M τ˜(i)j ≤p−h}
j∈[d] i∈P(Ek−1)
t
+1{M ij ≤p−h, M τ˜(i)j ≥p+h} .
Note that
∑ 1{M ij ≥p+h, M τ˜(i)j ≤p−h}≤∣{i∈P(E tk−1)∶ M ij ≥p+h}∣∧∣{i∈P(E tk−1)∶ M τ˜(i)j ≤p+h}∣
i∈P(Ek−1)
t
=∣{i∈P(Ek−1)∶ M ≥p+h}∣∧∣{i∈P(Ek−1)∶ M ≤p+h}∣ .
t ij t ij
Recall the definition (28) of R˜ . We obtain
p,h,P(El−1)[d]
u
∑ R (i,τ(i))≤2R˜ . (66)
p,h p,h,P(El−1)[d]
i∈P(El−1) u
u
τ(i)∈P(El−1)
u
Combining the upper bounds. Combining (64), (65), and (66) with (61) gives us
R (πˆ)≤4
ρ2 ∑K ∑rk ∣Ek∣∨∣Q∗(Ek)∣+2∑K
∑ R˜ .
p,h λ 1h2 k=1 t=1 t t l=1 u=1 p,h,P(E ul−1)[d]
Ek∈Ok∪Ik wl−1=1
t u
D Proof of Corollary 3.3
In this section, we explain how the proof of Theorem A.2 easily extends to the case of multiples
thresholds and tolerances up to a minor change. Consider some l ∈ {1,...,m}. Note that the
only difference for Algorithm SoHLoB with multiple thresholds instead of one threshold is that we
consider multiple envelope sets, constructed in line 11. To this end, define
Q∗ (E)∶={j ∈[d]∶ maxM ≥p +h , minM ≤p −h } ,
pl,hl i∈E ij l l i∈E ij l l
47and consider the envelope with respect to p ,h for
Ek−1
with
vk−1
as
Qk−1.
Again, we only use
l l t t t,l
thissetfortheScanAndUpdateprocedureifitisthecasethatProperty1isfulfilled,andconstruct
each time a new
Qk−1
as in (39). Therefore, Lemma C.1, Lemma B.1 and Lemma B.2 are not
t,l
effected. It is not hard to see that Lemma C.2 also holds, if our trisection is based on running
ScanAndUpdateformultipleenvelopesets. Onewayofthinkingis,thattheresultinggraphismore
informative than a graph being updated with respect to a single envelope set only. The first three
points of Lemma C.3 are again an immediate consequence, and Corollary B.6.5 can be adapted.
The proof of Lemma B.3 can be adapted, such that we obtain the error bound stated in (67).
For each l, we can construct a good event exactly as in the case where only one threshold p and
tolerance h is considered, so Lemma C.5 and Lemma B.4 can be applied to all sets
Q∗ (Ek−1)
pl,hl t
and
Qk−1
with
vk−1
as well.
t,l t
So in the proof of Theorem B.5, what changes is the number of concentration inequalities
considered,leadingtoafinaleventofprobabilityatleast1−mδ/2insteadof1−δ/2duetoaunion
bound, and a slightly changed statement of Theorem B.5.1, such that
Qk−1⊆Q∗ (Ek−1).
t,l pl,hl t
and again
∑
∣Qk−1∣≤3d
t,l
t≥1, wk−1=1
t
for l = 1,...,m. On this newly constructed event, Corollary B.6 changes slightly. For the first
three points, our results can be reformulated with respect to sets Q′ ∈⋃m l=1Qk t,− l1. Corollary B.6.5
then holds if we consider
P l(E
tk−1)∶=⎧
⎪⎪
⎨
⎪⎪
⎩i∈E tk−1∶ ∣Q1
′∣RRRRRRRRRRRj∑
∈Q′Y i( j3k) −Y ι(
k
t3 −k 1)
(Q′)jRRRRRRRRRRR≤8ρ√
λ 1/∣Q′∣ ∀Q′∈Qk t,−
l1⎫
⎪⎪
⎬
⎪⎪
⎭
as replacement for P, and for B.6.6 we have
R˜ ≤3744ρ(∣Ek−1∣∨∣Qk−1∣)/λ h2 , (67)
pl,hl,Pl(E tk−1)[d] t t,l 1 l
both jointly for all l=1,...,m.
Lemma B.8 and Lemma B.9 can also be stated with respect to sets of the form P (El), and
l u
with the adaptations in Corollary B.6 just stated we can also adapt the proofs of the lemmas.
TheproofofCorollary3.3canbeconcludedjustliketheproofofTheoremA.2andTheorem3.2
for each l=1,...,m, and substituting δ by δ/m finally yields the claimed bound with probability
≥1−δ.
E Proof for the classification matrix
Proof of Theorem 3.5. This theorem is a straightforward corollary of Corollary 3.3 and Proposi-
tion 5.1.
Proof of Proposition 5.1. We consider three cases depending on the values of (k ,l ). First, we
h h
assume that k ∨l >1 and that k ≤n and l ≤d. The simple cases where k =l =1 or k ≥n or
h h h h h h h
l ≥d are postponed to the end of the proof.
h
For the sake of simplicity we simply write L for L (πˆ,ηˆ) Let us upper bound the recon-
p,h p,h
struction error of Rˆ .
p,h
L [Rˆ ]= ∑ 1{(Rˆ ) =1}+ ∑ 1{(Rˆ ) =0}=(I)+(II) . (68)
0,1,NA p,h p,h ij p,h ij
(i,j)∶Mij≤p−h (i,j)∶Mij≥p+h
Bysymmetry,wefocusonthesecondterm(II)oftherhs. DefineU 0∶={(i,j)∶M πˆ−1(i)ηˆ−1(j)≥p+h}
asthelevelsetofthematrixM πˆ−1,ηˆ−1 orderedaccordingtotheestimatedrankings. Wealsodefine
V 1∶={(i,j)∶M π−1(i)η−1(j)≥p+h/2}asthelevelsetoftheoracleorderedmatrixM π−1,η−1 atp+h/2.
48By definition of the loss function, we know that ∣U 0 ∖V 1∣ ≤ L p+3h/4,h/4. Since M π−1,η−1 is a
bi-isotonic matrix, as long as V is non-empty, V is a connected subset of [n]×[d] that contains
1 1
(1,1). We call B the collection of k ×l blocks that are fully included in V and V′ the subset
0 h h 1 1
of V which does not belong to any of the block in B . In fact, V′ corresponds to the boundary of
1 0 1
the level set V so that
1
∣V′∣≤k l (⌈n/k ⌉+⌈d/l
⌉)≤c(σ2∨1)n∨d
log(nd) ,
1 h h h h λ h2
0
for some universal constant c since we assume that k ∨l > 1. Since (II) corresponds to the
h h
πˆ,ηˆ
number of entries in U of the block constant matrix Y that are below p, we arrive for the error
0 B
(II) at the following bound in (68)
(II)≤ ∑ ∣B∣1{Yπˆ,ηˆ ≤p}+∣V′∣+∣U ∖V ∣
B 1 0 1
B∈B
0
≤ ∑ ∣B∣1{Y Bπˆ,ηˆ ≤p}+L p+3h/4,h/4+c(σ2∨1)n λ∨ hd
2
log(nd) . (69)
B∈B
0
0
Let us now define the level sets W 0 ∶= {(i,j) ∈ V 1 ∶ M πˆ−1(i)ηˆ−1(j) ∈ (p−h,p+h/4]} and, for
s=1,...,s max where s max ∶=⌈log 2(1/h)⌉, we define W s ∶={(i,j)∈V 1 ∶(M πˆ−1(i)ηˆ−1(j) ∈[p−h2s,p−
h2s−1)}.
Again by definition of the loss functions and of V , we have
1
∣W 0∩V 1∣≤L p+3h/8,h/8; ∣W s∩V 1∣≤L p−h2s−2,h2s−2 , (70)
for s = 1,...,s . Recall that the observations are σ2-subGaussians. Conditionally to πˆ and ηˆ,
max
we have, with probability higher than 1/(nd)2, for any B∈B ,
0
¿
Yπˆ,ηˆ −p≥h/4−σ` `(cid:192)4log(nd) − 3 s ∑max Nπˆ,ηˆ h2s ,
B N Bπˆ,ηˆ 2N Bπˆ,ηˆ s=0 Ws∩B
where N Wπˆ, sηˆ
∩B
= ∑N t=′ 1∑ (k,l)∈Ws∩B1{(I t,J t) = (πˆ−1(k),ηˆ−1(l))} is the number of observations in
W ∩B. TherandomvariablesN followindependentPoissondistributionwithparameterλ k l .
s B 0 h h
By Bennett’s inequality, P[N ≤ λ /2k l ] ≤ exp[−0.15λ k l ]. Hence, with probability higher
B 0 h h 0 h h
than 1/(nd)2, we have N ≥λ /2k l for all B∈B . Hence, it follows that
B 0 h h 0
√
Yπˆ,ηˆ −p≥h/4−σ 8log(nd) − 3h ∑Nπˆ,ηˆ 2s
B λ 0k hl h λ 0k hl h s=1 Ws∩B
3h
≥h/8+ ∑Nπˆ,ηˆ 2s ,
λ 0k hl h s=1
Ws∩B
by definition of k and l . Coming back to (68) and applying Markov inequality, we deduce that
h h
24smax n∨d
(II)≤
λ 0
s∑
=0
N Wπˆ, sηˆ2s+c(σ2∨1)
λ 0h2
log(nd)+L p+3h/4,h/4 .
The random variables Nπˆ,ηˆ follow a Poisson distribution with parameters λ ∣W ∣. By Bennett
Ws 0 s
inequality, with probability higher than 1−1/(nd)2, we have, for any s = 0,...,s , Nπˆ,ηˆ ≤
c′ λ ∣W ∣+c′′ log(nd), where c′ and c′′ are absolute constants. Gathering this bound wm ia thx (70W ),s we
0 s
conclude that, with probability higher than 1−3/(nd)2, we have
smax n∨d
(II)≤c[L p+3h/4,h/4+L p+3h/8,h/8+ s∑
=1
2sL p−h2s−2,h2s−2+(σ2∨1)
λ 0h2
log2(nd)] .
By handling analogously (I), we conclude that, with probability higher than 1−6/(nd)2, we have
smax
L 0,1,NA[Rˆ p,h]≤c[ ∑ 2s(L p+h2s−2,h2s−2+L p−h2s−2,h2s−2)
s=1
n∨d
+L p+3h/4,h/4+L p+3h/8,h/8+L p−3h/4,h/4+L p−3h/8,h/8+(σ2∨1)
λ h2
log2(nd)] .
0
The result follows since, on the remaining event, the loss is smaller than nd.
49Finally,weconsidertheextremecases. First,assumek =l =1. Inthiscase,eachoftheblock
h h
has size 1. Hence, for any entry (i,j), (Rˆ ) is simply 1{Y ≥p}. By Bennett’s inequality, with
p,h ij ij
probability higher than 1−1/(nd)2, we have N′ ≥λ /2. Then, by standard deviation inequality
ij 0
for subGaussian variables, we deduce, that with probability higher than 1−3/(nd)2, we have
√
σ
∣Y −M ∣≤4 log(nd)<1/h .
ij ij
λ
0
Hence, under this event, we have L [Rˆ ]=0. This concludes the proof for this case. Finally,
0,1,NA p,h
we assume that either k ≥n or l ≥d. Then, we use the trivial bound L [Rˆ ]≤nd, since we
h h 0,1,NA p,h
have here nd≲(σ2∨1)n∨d log2(nd).
λ0h2
F Proof of the lower bound
Proof of Theorem 3.1. ThisproofisbasedonFano’smethod,seee.g. [27],thatwerecallhere. Let
(S,d S) be a pseudometric space, θ 1,θ 2,...,θM ∈S such that d S(θ k,θ l)≥2D for some D >0 and
k ≠l. Consider a family of probability measures P ,P ,...,P with KL(P ∣P )≥κ for k ≠l.
θ1 θ2 θM θk θl
Then for any estimator θˆ∈S, it holds
κ+log2
max E [d (θˆ,θ)]≥D(1− ) . (71)
k=1,...,M θk S log∣M∣
We recall the definition of our observation model from Section 2.1. Consider λ >0 to be the
0
expected number of observations we make for every entry (i,j) ∈ [n]×[d], so that in total we
expect to have N = λ nd observations. In our setting, P is a distribution such that given the
0 M
matrix M we have observations
(N′ ,I,J,Y′)
of the following form:
• N′∼Pois(N) is the number of observations we have,
• (I,J)∣(N′=m)∼U([n]m×[d]m) is a vector of entries we observe
• Y∣(N′=m,(I,J)=((i 1,...,i m),(j 1,...,j m)))∼⊗m k=1N(M ikjk,σ2)
Finally,wewanttoboundtheKullback-LeiblerdivergenceoftwodistributionsP M andP M′ relating
to samples from the described observation model:
Lemma F.1. Consider the random vector (N′ ,I,J,Z) with distributions P
M
and P M′. Then
KL(P M∣P M′)= 2λ σ0 2∥M −M′∥2 F.
Let us consider S = {v ∈ {0,1}n ∶ ∑n i=1v
i
= n/2} (where we assume w.l.o.g. that n is an even
number), equipped with the Hamming distance d (v,v′) = ∣{i ∈ [n] ∶ v ≠ v′}∣. We define the
H i i
surjective map
v∶ S →S, v(π) =1{π(i)≤n/2} .
n i
For some l>0 to be chosen later, we also define the map
M ∶ S → ⋃ C Biso(π,id[d]), M(v)
ij
=p−h+2h⋅1{v i=1, j ≤l} .
π∈S
n
Then, for any π∈S and any estimator (πˆ,ηˆ), with vˆ=v(πˆ), we have
n
E M(v(π))[L p,h(πˆ,ηˆ)]≥l⋅E M(v(π))[d H(v(π),vˆ)] . (72)
Similarly, for any estimator Rˆ p,h, if we define v˜ by v˜ i=1 if ∑d j=1(Rˆ p,h)
ij
>l/2.
l
E M(v(π))[L 0,1,NA[Rˆ p,h]]≥ ⋅E M(v(π))[d H(v(π),v˜)] . (73)
2
50Next, we want to construct a packing of S. An adaptation of the Varshamov-Gilbert bound
like in [28] yields, that a collection of vectors v 1, v 2,..., vM exists with d H(v k,v l)≥n/4 for k ≠l
and logM≥c⋅n (where c can be chosen as 0.08).
Lemma F.1 implies for k≠l, that
λ lh2d (v ,v ) λ lh2n
KL(P M(vk)∣P M(vl))= 0 2H
σ2
k l ≥ 0
8σ2
.
We now have all ingredients to prove Theorem 3.1. Indeed, it follows from Fano’s method that,
for any vˆ, we have
n λ lh2n/8σ2+log(2)
k=m 1,.a ..x ,ME M(vk)[d H(v k,vˆ)]≥
8
(1− 0
0.08⋅n
) .
Let us choose l = ⌊0.16σ2/(λ h2)⌋∧d. Assuming that λ h2 ≤ 0.16σ2, we deduce that l ≥ 1. If
0 0
n≥35, we have log(2)≤0.08n/4 and it follows that
n
in vˆf k=m 1,.a ..x ,ME M(vk)[d H(v k,vˆ)]≥
16
.
In light of (72) and (73), this implies that
inf sup E [L [Rˆ ]]≥c′(
nσ2
∧nd) , (74)
Rˆ
p,h
M∈C
Biso
M 0,1,NA p,h λ 0h2
inf sup E [L (πˆ,ηˆ)]≥c′(
nσ2
∧nd) , (75)
πˆ,ηˆ M∈C
Biso
M p,h λ 0h2
wherec′ isapositiveuniversalconstant. Wehandlethecasewheren∈[2,35]slightlydifferentlyby
simply considering two vector v and v such that d (v ,v )=n/2 and using Le Cam’s approach
1 2 H 1 2
together with Pinsker’s inequality. Hence, as long as, n≥2, we have proven (74) and (75)
inf sup E [L [Rˆ ]]≥c′(
nσ2
∧nd) ,
Rˆ
p,h
M∈C
Biso
M 0,1,NA p,h λ 0h2
inf sup E [L (πˆ,ηˆ)]≥c′(
nσ2
∧nd) ,
πˆ,ηˆ M∈C
Biso
M p,h λ 0h2
Exchanging the role of n and d concludes the proof.
Proof of Lemma F.1. Consider the likelihoods
L
(D)=e−NNm
⋅
1 ⋅∏m √1 exp(−(y k−M ikjk)2
) ,
M m! (nd)m k=1 2πσ 2σ2
L
M′(D)=e−NN mm
!
⋅
(nd1
)m
⋅
k∏m
=1√
21
πσ
exp(−(y k− 2M σ2′ ikjk)2
) .
with D = (m,(i ,...,i ),(j ,...,j ),(y ,...,y )) being a possible realization of (N′ ,I,J,Y′).
1 m 1 m 1 m
Then we obtain as log-likelihood-ratio the term
log(L M(N′ ,I,J,Y′) )= ∑N′ ⎛M IkJk −M I′ kJkY − M I2 kJk −M′2 IkJk⎞ .
L M′(N′,I,J,Y′) k=1⎝ σ2 k 2σ2 ⎠
51For the Kullback-Leibler divergence, this means
L
(N′ ,I,J,Y′)
KL(P M∣P M′)=E M[log(
L
MM ′(N′,I,J,Y′))]
⎡ ⎢N′ ⎛M −M′ M2 −M′2 ⎞⎤ ⎥
=E M⎢ ⎢ ⎢∑
⎝
IkJk
σ2
IkJkY k− IkJk
2σ2
IkJk ⎠⎥ ⎥
⎥
⎣k=1 ⎦
⎡ ⎢ ⎡ ⎢N′ ⎛M −M′ M2 −M′2 ⎞ ⎤ ⎥⎤ ⎥
=E M⎢ ⎢ ⎢E M⎢ ⎢ ⎢∑
⎝
IkJk
σ2
IkJkY k− IkJk
2σ2
IkJk ⎠∣N′ ,I,J⎥ ⎥ ⎥⎥ ⎥
⎥
⎣ ⎣k=1 ⎦⎦
⎡ ⎢N′ (M −M′ )2⎤ ⎥
=E M⎢ ⎢ ⎢∑ IkJk
2σ2
IkJk ⎥ ⎥
⎥
⎣k=1 ⎦
= m∑ ≥0e−NN mm
!
⋅E M⎡ ⎢ ⎢ ⎢
⎢
⎣k∑N =′ 1(M IkJk 2− σM
2
I′ kJk)2 ∣N′=m⎤ ⎥ ⎥ ⎥
⎥
⎦
= ∑
e−NNm
⋅
1 ∑m
∑ ∑
(M ikjk −M i′ kjk)2
m≥0 m! (nd)m k=1(i1,...,im)∈[n]m(j1,...,jm)∈[d]m 2σ2
= ∑
e−NNm
⋅
m ∥M −M′∥2
F
m≥0 m! nd 2σ2
N ∥M −M′∥2
= F ,
nd 2σ2
where we used in the second last step the identity
m (M −M′ )2 n d (M −M′ )2
∑ ∑ ∑ ikjk ikjk =m(nd)m−1∑∑ ij ij
k=1(i1,...,im)∈[n]m(j1,...,jm)∈[d]m
2σ2
i=1j=1
2σ2
which can be proven with an induction over m.
G Concentration of partial sums
In this section, we state and prove a deviation bound for local averages of the observation matrix.
In what follows, we will have to control the deviations of partial row and column sums of an
observed matrix Y˜ to their mean.
Lemma G.1. Consider Y˜ ∶=Y(1) as described in Equation (12). Let T ⊆[n]×[d]. For any δ>0,
∣T1
∣RRRRRRRRRRRR(i,∑
j)∈TY˜ ij−λ 1M
ijRRRRRRRRRRRR≤√
2(1∨σ2)e2log(2/δ)λ 1/∣T∣+2(1∨σ)log(2/δ)/∣T∣
holds with probability ≥1−δ, where λ
1=1−e−λ−
0.
Proof of Lemma G.1. Write Y˜ for the k-th observation of coordinate i,j that is used to con-
i,j,k
struct Y˜ as described in Equation (12) - i.e. Y˜
ij
= N(1
i,j)
∑N k=( 1i,j) Y˜ i,j,k. We use the decomposition
Y˜ −λ M =α +β with
ij 1 ij ij ij
α
∶=B(i,j)⋅⎛ 1 N ∑(i,j)
[Y˜ −M
]⎞
,
ij ⎝N(i,j) i,j,k ij ⎠
k=1
and
β ∶=M ⋅(B(i,j)−λ ) ,
ij ij 1
where the N(i,j) are i.i.d. Pois(λ−) distributed and B(i,j) = 1{N(i,j) > 0} ∼ Ber(λ ) Bernoulli
0 1
distributed with P(B(i,j) = 1) = 1−P(B(i,j) = 0) = λ . By Markov’s inequality and Cauchy–
1
52Schwarz, we obtain for t>0
⎛ 1 ⎞ ⎛ ⎞
P ∑ Y˜ −λ M >t =P ∑ α +β >∣T∣t
⎝∣T∣ ij 1 ij ⎠ ⎝ ij ij ⎠
(i,j)∈T (i,j)∈T
⎡ ⎧ ⎫ ⎤
⎢ ⎛ ⎪⎪ ⎪⎪⎞⎥
≤e−x∣T∣tE⎢ ⎢ ⎢exp ⎝x⋅⎨
⎪⎪
∑ α ij+β ij⎬ ⎪⎪⎠⎥ ⎥
⎥
⎣ ⎩(i,j)∈T ⎭ ⎦
⎧ ⎡ ⎤ ⎡ ⎤⎫1/2
⎪⎪ ⎢ ⎛ ⎞⎥ ⎢ ⎛ ⎞⎥⎪⎪
≤e−x∣T∣t⎨ ⎪⎪E⎢ ⎢ ⎢exp ⎝2x⋅ ∑ α ij⎠⎥ ⎥ ⎥⋅E⎢ ⎢ ⎢exp ⎝2x⋅ ∑ β ij⎠⎥ ⎥ ⎥⎬
⎪⎪
,
⎩ ⎣ (i,j)∈T ⎦ ⎣ (i,j)∈T ⎦⎭
(76)
where we will choose a suitable x>0.
Now note that
⎡ ⎤ ⎡ ⎤
E⎢ ⎢ ⎢ ⎢exp⎛ ⎝2x⋅ ∑ α ij⎞ ⎠⎥ ⎥ ⎥ ⎥= ∏ E⎢ ⎢ ⎢ ⎢exp⎛ ⎝2x⋅B(i,j)⋅⎛ ⎝N(1
i,j)
N ∑(i,j) Y˜ i,j,k−M ij⎞ ⎠⎞ ⎠⎥ ⎥ ⎥
⎥
,
⎣ (i,j)∈T ⎦ (i,j)∈T ⎣ k=1 ⎦
and for (i,j)∈T
⎡ ⎤
E⎢ ⎢ ⎢ ⎢exp⎛ ⎝2x⋅B(i,j)⋅⎛ ⎝N(1
i,j)
N ∑(i,j) Y˜ i,j,k−M ij⎞ ⎠⎞ ⎠⎥ ⎥ ⎥
⎥
⎣ k=1 ⎦
=(1−λ )+ ∑ P(N(i,j)=K)E[exp(2x( 1 ∑K Y˜ −M ))]
1 i,j,k ij
K
K≥1 k=1
≤(1−λ )+ ∑ P(N(i,j)=K)exp(2x2σ2/K)≤λ (exp(2x2σ2)−1)+1
1 1
K≥1
≤λ (σx)2e2+1≤exp(λ e2(σx)2) ,
1 1
for x∈[−1/σ,1/σ], where we used e2s2−1≤s2e2 for ∣s∣≤1. So in total, we obtain
⎡ ⎤
⎢ ⎛ ⎞⎥
E⎢ ⎢ ⎢exp ⎝2x⋅ ∑ α ij⎠⎥ ⎥ ⎥≤exp(λ 1∣T∣e2(σx)2) for x∈[−1/σ,1/σ] . (77)
⎣ (i,j)∈T ⎦
Forthesecondfactor,notethatforarandomvariableB∼Ber(p)themomentgeneratingfunction
can be bounded via
E[exp(2y⋅(B−p))]=p⋅e2y⋅(1−p)+(1−p)⋅e−2yp
=e−2yp(p(e2y−1)+1)
≤exp(−2yp+pe2y−1)
≤exp(p(e2y−2y−1))
≤exp(pe2y2) for ∣y∣≤1 ,
where we used that e2y−2y−1≤e2y2 for ∣y∣≤1. This implies
⎡ ⎤
⎢ ⎛ ⎞⎥
E⎢ ⎢ ⎢exp ⎝2x⋅ ∑ β ij⎠⎥ ⎥ ⎥= ∏ E[exp(2xM ij⋅(B(i,j)−λ 1))]
⎣ (i,j)∈T ⎦ (i,j)∈T
≤ ∏ exp(λ e2x2M2) (for ∣x∣≤1)
1 ij
(i,j)∈T
≤exp(λ e2x2∣T∣) (for ∣x∣≤1) . (78)
1
Let κ∶=1∨σ. Then plugging (77) and (78) into (76) yields
⎛ 1 ⎞
P ∑ Y˜ −λ M >t ≤exp(λ e2x2(σ2+1)∣T∣/2−x∣T∣t) for x∈[−1/κ,1/κ]
⎝∣T∣ ij 1 ij ⎠ 1
(i,j)∈T
≤exp(λ e2x2κ2∣T∣−x∣T∣t) for x∈[−1/κ,1/κ]
1
∣T∣ t2
≤exp(− ( ∧t)) .
2κ λ κe2
1
53We conclude the proof by claiming that the right hand side is bounded by δ/2 for the value
√
t= 2κ2e2log(2/δ)λ /∣T∣+2κlog(2/δ)/∣T∣ and that in the same way, we have shown
1
⎛ 1 ⎞
P ∑ Y˜ −λ M >t ≤δ/2
⎝∣T∣ ij 1 ij ⎠
(i,j)∈T
one can show
⎛ 1 ⎞
P ∑ λ M −Y˜ >t ≤δ/2 .
⎝∣T∣ 1 ij ij ⎠
(i,j)∈T
54