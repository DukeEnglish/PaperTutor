Implicit Regularization Paths of Weighted Neural Representations
Jin-Hong Du†‡ Pratik Patil§
jinhongd@andrew.cmu.edu pratikpatil@berkeley.edu
Abstract
We study the implicit regularization effects induced by (observation) weighting of pre-
trained features. For weight and feature matrices of bounded operator norms that are in-
finitesimally free with respect to (normalized) trace functionals, we derive equivalence paths
connecting different weighting matrices and ridge regularization levels. Specifically, we show
that ridge estimators trained on weighted features along the same path are asymptotically
equivalent when evaluated against test vectors of bounded norms. These paths can be inter-
preted as matching the effective degrees of freedom of ridge estimators fitted with weighted
features. For the special case of subsampling without replacement, our results apply to in-
dependently sampled random features and kernel features and confirm recent conjectures
(Conjectures 7 and 8) of the authors on the existence of such paths in [50]. We also present
an additive risk decomposition for ensembles of weighted estimators and show that the risks
are equivalent along the paths when the ensemble size goes to infinity. As a practical conse-
quence of the path equivalences, we develop an efficient cross-validation method for tuning
andapplyittosubsampledpretrainedrepresentationsacrossseveralmodels(e.g.,ResNet-50)
and datasets (e.g., CIFAR-100).
1 Introduction
Inrecentyears,neuralnetworkshavebecomestate-of-the-artmodelsfortasksincomputervision
and natural language processing by learning rich representations from large datasets. Pretrained
neural networks, such as ResNet, which are trained on massive datasets like ImageNet, serve as
valuable resources for new, smaller datasets [32]. These pretrained models reduce computational
burden and generalize well in tasks such as image classification and object detection due to their
rich feature space [32, 69]. Furthermore, pretrained features or neural embeddings, such as the
neural tangent kernel, extracted from these models, serve as valuable representations of diverse
data [33, 66].
However, despite their usefulness, fitting models based on pretrained features on large datasets
can be challenging due to computational and memory constraints. When dealing with high-
dimensional pretrained features and large sample sizes, direct application of even simple linear
regression may be computationally infeasible or memory-prohibitive [23, 44]. To address this
issue, subsampling has emerged as a practical solution that reduces the dataset size, thereby al-
leviatingthecomputationalandmemoryburden.Subsamplinginvolvescreatingsmallerdatasets
byrandomlyselectingasubsetoftheoriginaldatapoints.Beyondthesecomputationalandmem-
oryadvantages,subaggingcanalsogreatlyimprovepredictiveperformanceinoverparameterized
†Department of Statistics and Data Science, Carnegie Mellon University, Pittsburgh, PA 15213, USA.
‡Machine Learning Department, Carnegie Mellon University, Pittsburgh, PA 15213, USA.
§Department of Statistics, University of California, Berkeley, CA 94720, USA.
1
4202
guA
82
]GL.sc[
1v48751.8042:viXraTable 1: Overview of related work on the equivalence of implicit regularization and explicit ridge regu-
larization.
Main analysis Feature structure Weight structure Reference
Gaussian subsampling [37]
Risk characterization linear subsampling [51]
Gaussian bootstrapping [5, 16, 17]
linear subsampling [50]
general general Theorem 1
Estimator equivalence
general subsampling Theorem 2
linear, random, kernel subsampling Propositions 3–5
linear subsampling [50]
Risk equivalence
general general Theorem 6
regimes,especiallynearmodelinterpolationthresholds[53].Moreover,throughdistributedlearn-
ing, models fitted on multiple subsampled datasets can be aggregated as an ensemble to provide
more stable predictions [20, 21, 51].
There has been growing interest in understanding the effects of subsampling (without replace-
ment) [16, 25, 37, 50, 51]. These works relate subsampling to explicit ridge regularization, as-
suming either Gaussian features ϕ ∼ N(0 ,Σ) or linearly decomposable features (referred to
p
as linear features in this paper) ϕ = Σ1/2z, where Σ ∈ Rp×p is the covariance matrix and
z ∈ Rp contains i.i.d. entries with zero means and bounded 4 + δ moments for some δ > 0.
Specifically, [50] establish a connection between implicit regularization induced by subsampling
and explicit ridge regularization through a path defined by the tuple (k/n,λ), where k and n
are the subsample size and the full sample size, respectively, and λ is the ridge regularization
level. Along this path, any subsample estimator with the corresponding ridge regularization
exhibits the same first-order (or estimator equivalence) and second-order (or risk equivalence)
asymptotic limits. Moreover, the endpoints of all such paths along the two axes of k = n (no
subsampling) and λ = 0 (no regularization) span the same range. Although these results have
been demonstrated for linear features, [50] also numerically observe similar equivalence behavior
in more realistic contexts and propose conjectures for random features and kernel features based
on heuristic “universality” justifications. However, extending these results to encompass more
general feature structures and other sampling schemes remains an open question.
Towards answering this question, in this paper, we view subsampling as a weighted regression
problem [67]. This perspective allows us to study the equivalence in its most general form, con-
sidering arbitrary feature structures and weight structures. The general weight matrix approach
used in this study encompasses various applications, including subsampling, bootstrapping,
variance-adaptive weighting, survey, and importance weighting, among others. By interpret-
ing subsampling as a weighted regression problem, we leverage recent tools from free probability
theory, which have been developed to analyze feature sketching [39, 42, 54]. Building on these
theoretical tools, we establish implicit regularization paths for general weighting and feature
structures. We summarize our main results below and provide an overview of our results in the
context of recent related work in Table 1.
21.1 Summary of results and paper outline
We summarize our main results and provide an outline for the paper below.
• Paths of weighted representations. In Section 3, we demonstrate that general weighted mod-
els exhibit first-order equivalence along a path (Theorem 1) when the weight matrices are
asymptotically independent of the data matrices. This path of equivalence can be computed
directly from the data using the formula provided in Equation (2). Furthermore, we pro-
vide a novel interpretation of this path in terms of matching effective degrees of freedom of
models along the path for general feature structures when the weights correspond to those
arising from subsampling (Theorem 2).
• Paths of subsampled representations. We further specialize our general result in Theorem 2
for the weights induced by subsampling without replacement to structured features in Sec-
tion 3.2. These include results for linear random features, nonlinear random features, and
kernelfeatures,asshowninPropositions3–5,respectively.Thelattertworesultsalsoresolve
Conjectures 7 and 8 raised by [50] regarding subsampling regularization paths for random
and kernel features, respectively.
• Risk equivalences and tuning. In Section 4, we demonstrate that an ensemble of weighted
modelshasgeneralquadraticriskequivalenceonthepath,withanerrortermthatdecreases
inversely as 1/M as the number of ensemble size M increases (Theorem 6). The risk equiva-
lenceholdsforbothin-distributionandout-of-distributionsettings.Forsubsamplinggeneral
features, we derive an upper bound for the optimal subsample size (Proposition 7) and pro-
pose a cross-validation method to tune the subsample and ensemble sizes (Algorithm 1),
validated on real datasets in Section 4.3.
This level of generality is achievable because we do not analyze the risk of either the full model
or the weighted models in isolation. Instead, we relate these two sets of models, allowing us to
maintain weak assumptions about the features. The key assumption underlying our results is
the asymptotic freeness of weight matrices with respect to the data matrices. While directly
testing this assumption is generally challenging, we verify its validity through its consequences
on real datasets in Section 4.3.
1.2 Related literature
We provide a brief account of other related work below to place our work in a better context.
Linear features. Despite being overparameterized, neural networks generalize well in practice
[70, 71]. Recent work has used high-dimensional “linearized” networks to investigate the various
phenomena that arise in deep learning, such as double descent [12, 46, 48], benign overfitting
[10,35,45],andscalinglaws[7,19,66].Thisliteratureanalyzeslinearregressionusingstatistical
physics [14, 60] and random matrix theory [22, 30]. Risk approximations hold under random
matrix theory assumptions [6, 30, 66] in theory and apply empirically on a variety of natural
data distributions [43, 60, 66].
Random and kernel features. Random feature regression, initially introduced in [56] as a way
to scale kernel methods, has recently been used for theoretical analysis of neural networks and
trendsofdoubledescentindeepnetworks[1,46].Thegeneralizationofkernelridgeregressionhas
been studied in [11, 40, 57]. The risks of kernel ridge regression are also analyzed in [9, 19, 29].
The neural representations we study are motivated by the neural tangent kernel (NTK) and
3related theoretical work on ultra-wide neural networks and their relationships to NTKs [34, 68].
Resampling analysis. Resampling and weighted models are popular in distributed learning to
provide more stable predictions and handle large datasets [20, 21, 51]. Historically, for ridge
ensembles, [36, 61] derived risk asymptotics under Gaussian features. Recently, there has been
growing interest in analyzing the effect of subsampling in high-dimensional settings. [37] consid-
ered least squares ensembles obtained by subsampling, where the final subsampled dataset has
more observations than the number of features. For linear models in the underparameterized
regime, [59] also provide certain equivalences between subsampling and iterative least squares
approaches. The asymptotic risk characterization for general data models has been derived by
[51]. [25, 50] extended the scope of these results by characterizing risk equivalences for both
optimal and suboptimal risks and for arbitrary feature covariance and signal structures. Very
recently, different resampling strategies for high-dimensional supervised regression tasks have
been analyzed by [17] under isotropic Gaussian features. Cross-validation methods for tuning
the ensemble of ridge estimators and other penalized estimators are discussed in [13, 25, 26].
Our work adds to this literature by considering ensembles of models with general weighting and
feature structures.
2 Preliminaries
In this section, we formally define our weighted estimator and state the main assumption on
the weight operator. Let f : Rd → Rp be a pretrained model. Let {(x ,y ): i = 1,...,n} in
nn i i
Rd×R be the given dataset. Applying f to the raw dataset, we obtain the pretrained features
nn
ϕ = f (x ) for i = 1,...,n as the resulting neural representations or neural embeddings. In
i nn i
matrix notation, we denote the pretrained feature matrix by Φ = [ϕ ,...,ϕ ]⊤ ∈ Rn×p. Let
1 n
W ∈ Rn×n be a general weight matrix used for weighting the observations. The weight matrix
W is allowed to be asymmetric, in general.
We consider fitting ridge regression on the weighted dataset (WΦ,Wy). Given a ridge penalty
λ, the ridge estimator fitted on the weighted dataset is given by:
(cid:18) ∥Wy−WΦβ∥2 (cid:19)
β(cid:98)W,λ := ar βg ∈m Rpin
n
2 +λ∥β∥2
2
= (Φ⊤W⊤WΦ+nλI p)†Φ⊤W⊤Wy. (1)
Inthedefinitionabove,weallowforλ = 0,inwhichcasethecorrespondingridgelessestimatoris
defined as the limit λ → 0+. For λ < 0, we use the Moore-Penrose pseudoinverse. An important
special case is where W is a diagonal matrix, in which case the above estimator reduces to
weighted ridge regression. This type of weight matrix encompasses various applications, such as
resampling, bootstrapping, and variance weighting. Our main application in this paper will be
subsampling.
For our theoretical results, we assume that the weight matrix W preserves some spectral struc-
tureofthefeaturematrixΦ.Thisassumptioniscapturedbytheconditionofasymptotic freeness
between W⊤W and the feature Gram matrix ΦΦ⊤. Asymptotic freeness is a concept from free
probability theory [64].
AssumptionA(Weightstructure). LetW⊤W andΦΦ⊤/nconvergealmostsurelytobounded
operators that are infinitesimally free with respect to (tr[·],tr[C(·)]) for any C independent of
W with ∥C∥ uniformly bounded. Additionally, let W⊤W have a limiting S-transform that is
tr
analytic on the lower half of the complex plane.
4At a high level, Assumption A captures the notion of independence but is adapted for non-
commutative random variables of matrices. We provide background on free probability theory
and asymptotic freeness in Appendix A.3. Here, we briefly list a series of invertible transforma-
tions from free probability to help define the S-transform [47]. The Cauchy transform is given
by G (z) = tr[(zI−A)−1]. The moment generating series is given by M (z) = z−1G (z−1)−1.
A A A
TheS-transformisgivenbyS (w) =
(1+w−1)M⟨−1⟩
(w).ThesearetheCauchytransform(neg-
A A
ative of the Stieltjes transform), moment generating series, and S-transform of A, respectively.
⟨−1⟩
Here, M denotes the inverse under the composition of M . The notation tr[A] denotes the
A A
average trace tr[A]/p of A ∈ Rp×p.
The freeness of a pair of operators A and B means that the eigenvectors of one are completely
unaligned or incoherent with those of the other. For example, if A = URU⊤ for a uniformly
random unitary matrix U drawn independently of the positive semidefinite B and R, then
A and B are almost surely asymptotically infinitesimally free [15]. Other well-known examples
includeWignermatrices,whichareasymptoticallyfreewithrespecttodeterministicmatrices[4,
Theorem5.4.5].Gaussianmatrices,wheretheGrammatrixG = ΦΦ⊤/n = U(VV⊤/n)U⊤ and
any deterministic S, are almost surely asymptotically free [47, Chapter 4, Theorem 9]. Although
not proven in full generality, it is expected that diagonal matrices are asymptotically free from
data Gram matrices constructed using i.i.d. data. In Section 3.2, we will provide additional
examples of feature matrices, such as random and kernel features from machine learning, for
which our results apply.
Our results involve the notion of degrees of freedom from statistical optimism theory [27, 28].
Degrees of freedom in statistics count the number of dimensions in which a statistical model
may vary, which is simply the number of variables for ordinary linear regression. To account for
regularization, this notion has been extended to effective degrees of freedom (Chapter 3 of [31]).
Undersomeregularityconditions,fromStein’srelation[63],thedegreesoffreedomofapredictor
f(cid:98)are measured by the trace of the operators y (cid:55)→ (∂/∂y)f(cid:98)(Φ). For the ridge estimator β(cid:98)I,µ
fitted on (Φ,y) with penalty µ, the degrees of freedom is consequently the trace of its prediction
operator y (cid:55)→ Φ(Φ⊤Φ+µI )†Φ⊤y, which is also referred to as the ridge smoother operator.
p
That is, df(β(cid:98)I,µ) = tr[Φ⊤Φ(Φ⊤Φ+µI p)†]. We denote the normalized degrees of freedom by
df = df/n. Note that df(β(cid:98)I,µ) ≤ min{n,p}/n ≤ 1.
Finally, we express our asymptotic results using the asymptotic equivalence relation. Consider
sequences {A } and {B } of (random or deterministic) matrices (which includes vectors
n n≥1 n n≥1
and scalars). We say that A and B are equivalent and write A ≃ B if lim |tr[C (A −
n n n n p→∞ n n
B )]| = 0 almost surely for any sequence C of matrices with bounded trace norm such that
n n
limsup∥C ∥ < ∞asn → ∞.Ourforthcomingresultsapplytoasequenceofproblemsindexed
n tr
by n. For notational simplicity, we omit the explicit dependence on n in our statements.
3 Implicit regularization paths
We begin by characterizing the implicit regularization induced by weighted pretrained features.
Wewillshowthatthedegreesoffreedomoftheunweightedestimatorβ(cid:98)I,µ onthefulldata(Φ,y)
with regularization parameter µ are equal to the degrees of freedom of the weighted estimator
β(cid:98)W,λ for some regularization parameter λ. For estimator equivalence, our data-dependent set
of weighted ridge estimators (W,λ) that connect to the unweighted ridge estimator (I,µ) is
defined in terms of “matching” effective degrees of freedom of component estimators in the set.
5Tostatetheupcomingresult,denotetheGrammatrixoftheweighteddataasG = WΦΦ⊤W⊤/n
W
andtheGrammatrixoftheunweighteddataasG = ΦΦ⊤/n.Furthermore,letλ+ (A)denote
I min
the minimum positive eigenvalue of a symmetric matrix A.
Theorem 1 (Implicit regularization of weighted representations). For G ∈ Rn×n, suppose
I
that the subsampling operator W ∈ Rn×n satisfies Assumption A and limsup∥y∥2/n < ∞
2
as n → ∞. For any µ > −liminf λ+ (G ), let λ > −λ+ (G ) be given by the
n→∞ min I min W
following equation:
λ = µ/S W⊤W(−df(β(cid:98)I,µ)), (2)
where S is the S-transform of the operator W⊤W. Then, as n → ∞, it holds that:
W⊤W
df(β(cid:98)W,λ) ≃ df(β(cid:98)I,µ) and β(cid:98)W,λ ≃ β(cid:98)I,µ. (3)
In other words, to achieve a target regularization of µ on the unweighted data, Theorem 1
provides a method to compute the regularization penalty λ with given weights W from the
available data using (2). The weighted estimator then has asymptotically the same degrees of
freedom as the unweighted estimator. This means that the level of effective regularization of
the two estimators is the same. Moreover, the estimators themselves are structurally equivalent;
that is, c⊤(β(cid:98)W,λ −β(cid:98)I,µ) −a −. →s. 0 for every constant vector c with bounded norm. The estimator
equivalence in Theorem 1 is a “first-order” result, while we will also characterize the “second-
order” effects in Section 4.
The notable aspect of Theorem 1 is its generality. The equivalence results hold for a wide range
of weight matrices and allow for negative values for the regularization levels. Furthermore, we
have not made any direct assumptions about the feature matrix Φ, the weight matrix W, and
the response vector y (other than mild bounded norms). The main underlying ingredient is the
asymptotic freeness between W and Φ, which we then exploit using tools developed in [39]
in the context of feature sketching. We discuss special cases of interest for W and Φ in the
upcoming Sections 3.1 and 3.2.
3.1 Examples of weight matrices
There are two classes of weighting matrices that are of practical interest:
• Non-diagonal weighting matrices. One can consider observation sketching, which in-
volves some random linear combinations of the rows of the data matrix. Such observation
sketching is beneficial for privacy, as it scrambles the rows of the data matrix, which may
contain identifiable information about individuals. It also helps in reducing the effect of
non-i.i.d. data that arise in time series or spatial data, where one wants to smooth away the
impact of irregularities or non-stationarity.
• Diagonal weighting matrices.Whenobservationsareindividuallyweighted,W isadiag-
onal matrix, which includes scenarios such as resampling, bootstrapping, and subsampling.
Note that even with subsampling, one can have a non-binary diagonal weighting matrix.
For example, one can consider sampling with replacement or sampling with a particular
distribution, which yields non-binary diagonal weighting matrices. Other examples of non-
6Degrees of freedom Gaussian
0.0
0.040
800 0.035
0.030
0.1
600 0.025
0.020
400
1
0.015
0.010
200
0.005
10
0.0 0.1 1 0.0 0.1 1
Subsample ratio k/n Subsample ratio k/n
Figure 1: Equivalence under subsampling. The left panel shows the heatmap of degrees of freedom, and
the right panel shows the random projection E W[a⊤β(cid:98)W,λ] where a ∼ N(0 p,I p/p). In both heatmaps,
the red color lines indicate the predicted paths using Equation (4), and the black dashed lines indicate
the empirical paths by matching empirical degrees of freedom. The data is generated according to Ap-
pendix F.1 with n = 10000 and p = 1000, and the results are averaged over M = 100 random weight
matrices W.
binary diagonal weighting matrices include inverse-variance weighting sampling to mitigate
the effects of heterogeneous variations if the responses have different variances for different
units.
In general, the set of equivalent weighted estimators depends on the corresponding S-transform
as in (2), and it can be numerically evaluated. When focusing on subsampling without re-
placement, the data-dependent path for equivalent estimators with associated subsampling and
regularization levels can be explicitly characterized in the following result by analyzing the
S-transform of subsampling operators.
Theorem 2 (Regularization paths due to subsampling). For a subsampling operator W(k)
consisting of k unit diagonal entries, the path (2) in terms of (k,λ) simplifies to:
(1−df(β(cid:98)I,µ))·(1−λ/µ) = (1−k/n). (4)
The relation (4) is remarkably simple, yet quite general! It provides a clear interplay between
normalizedtargetcomplexitydf(β(cid:98)I,µ),regularizationinflationλ/µ,andsubsamplefractionk/n:
(1−target complexity)·(1−regularization inflation) = (1−subsample fraction). (5)
Since the normalized target complexity and subsample fraction are no greater than one, (5) also
implies that the regularization level λ for the subsample estimator is always lower than the reg-
ularization level µ for the full estimator. In other words, subsampling induces (positive) implicit
regularization, reducing the need for explicit ridge regularization. This is verified numerically in
Figure 1.
Forafixedtargetregularizationamountµ,thedegreesoffreedomdf(β(cid:98)I,µ)oftheridgeestimator
on full data is fixed. Thus, we can observe that the path in the (k/n,λ)-plane is a line. There
7
ytlanep
egdiRare two extreme cases: (1) when the subsample size k is close to n, we have µ ≈ λ; and (2) when
the subsample size is near 0, we have µ ≈ ∞. When λ = 0, the effective regularization level λ is
such that df(β(cid:98) W(k),λ) = df(β(cid:98)I,µ) = k, which we find to be a neat relation!
Beyond subsampling without replacement, one can also consider other subsample operators.
For example, for bootstrapping k entries, we observe a similar equivalent path in Figure 5.
Additionally, for random sample reweighting, as shown in Figure 6, we also observe certain
equivalence behaviors of degrees of freedom. This indicates that Theorem 1 also applies to more
general weighting schemes.
3.2 Examples of feature matrices
As mentioned in Section 2, when the feature matrix Φ consists of i.i.d. Gaussian features, any
deterministic matrix W satisfies the condition stated in Assumption A. However, our results
are not limited to Gaussian features. In this section, we will consider more general families of
featurescommonlyanalyzedinmachinelearninganddemonstratetheapplicabilityofourresults
to them.
(1) Linear features.Asafirstexample,weconsiderlinearfeaturescomposedof(multiplicatively)
transformed i.i.d. entries with sufficiently bounded moments by a deterministic covariance ma-
trix.
Proposition 3 (Regularization paths with linear features). Suppose the feature ϕ can be
decomposed as ϕ = Σ1/2z, where z ∈ Rp contains i.i.d. entries z for i = 1,...,p with
i
mean 0, variance 1, and satisfies E[|z |4+µ] ≤ M < ∞ for some µ > 0 and a constant M ,
i µ µ
and Σ ∈ Rp×p is a deterministic symmetric matrix with eigenvalues uniformly bounded
between constants r > 0 and r < ∞. Then, as n,p → ∞ such that p/n → γ > 0, the
min max
equivalences in (3) hold along the path (4).
Featuresofthistypearecommoninrandommatrixtheory[8]andinawiderangeofapplications,
including statistical physics [14, 60], high-dimensional statistics [22, 55, 58], machine learning
[18], among others. The generalized path (2) in Theorem 2 recovers the path in Proposition 4 of
[50]. Although the technique in this paper is quite different and more general than that of [50].
(2) Kernel features. As the second example, Theorem 2 also applies to kernel features. Kernel
features are a generalization of linear features and lift the input feature space to a high- or
infinite-dimensional feature space by applying a feature map x (cid:55)→ ϕ(x). Kernel methods use the
kernel function K(x ,x ) = ⟨ϕ(x ),ϕ(x )⟩ to compute the inner product in the lifted space.
i j i j
Proposition 4 (Regularizationpathswithkernelfeatures). Supposethesameconditionsas
inProposition3andthekernelfunctionisoftheformK(x ,x ) = g(∥x ∥2/p,⟨x ,x ⟩/p,∥x ∥2/p),
i j i 2 i j j 2
where g is C1 around (τ,τ,τ) and C3 around (τ,0,τ) and τ := lim tr[Σ]/d. Then, as
p→∞
n → ∞, the equivalences in (3) hold in probability along the path (4).
The assumption in Proposition 4 is commonly used in the risk analysis of kernel ridge regression
[9, 19, 29, 57], among others. It includes neural tangent kernels (NTKs) as a special case.
Proposition 4 confirms Conjecture 8 of [50] for these types of kernel functions.
(3) Random features. Finally, we consider random features that were introduced by [56] as a
8Linear features Kernel features Random features
0.0 500 500
400
400 400
0.1
300 300 300
1 200 200 200
100 100 100
10
0.0 0.1 1 0.0 0.1 1 0.0 0.1 1
Subsample ratio k/n Subsample ratio k/n Subsample ratio k/n
Figure 2: Equivalence of degrees of freedom for various feature structures under subsampling. The three
panelscorrespondtolinearfeatures,randomfeatureswithReLUactivationfunction(2-layer),andkernel
features (polynomial kernel with degree 3 and without intercept), respectively. In all heatmaps, the
red color lines indicate the predicted paths using Equation (4), and the black dashed lines indicate
the empirical paths by matching the empirical degrees of freedom. The data is generated according to
Appendix F.1 with n = 5000 and p = 500, and the results are averaged over M = 100 random weight
matrices W.
way to scale kernel methods to large datasets. Linked closely to two-layer neural networks [46],
the random feature model has f (x) = σ(Fx), where F ∈ Rd×p is some randomly initialized
nn
weight matrix, and σ : R → R is a nonlinear activation function applied element-wise to Fx.
Proposition 5 (Regularization paths with random features). Suppose x ∼ N(0,Σ) and
i
theactivationfunctionσ : R → Risdifferentiablealmosteverywhereandthereareconstants
c
0
and c
1
such that |σ(x)|,|σ′(x)| ≤ c 0ec1x, whenever σ′(x) exists. Then, as n,p,d → ∞ such
that p/n → γ > 0 and d/n → ξ > 0, the equivalences in (3) hold in probability along the
path (4).
As mentioned in the related work, random feature models have recently been used as a standard
model to study various generalization phenomena observed in neural networks theoretically
[1, 46]. Proposition 5 resolves Conjecture 7 of [50] under mild regularity conditions on the
activation function.
It is worth noting that the prior works mentioned above, including [50], have focused on first
characterizing the risk asymptotics in terms of various population quantities for each of the
cases above. In contrast, our work in this paper deviates from these approaches by not express-
ing the risk in population quantities but rather by directly relating the estimators at different
regularization levels. In the next section, we will explore the relationship between their squared
prediction risks.
4 Prediction risk asymptotics and risk estimation
The results in the previous section provide first-order equivalences of the estimators, which are
related to the bias of the estimators. In practice, we are also interested in the predictive perfor-
manceoftheestimators.Inthissection,weinvestigatethesecond-orderequivalenceofweighting
and ridge regularization through ensembling. Specifically, we show that aggregating estimators
fitted on different weighted datasets also reduces the additional variance. Furthermore, the pre-
diction risks of the full-ensemble weighted estimator and the unweighted estimator also match
9
ytlanep
egdiRalong the path.
Beforepresentingourriskequivalenceresult,wefirstintroducesomeadditionalnotation.Assume
there are M i.i.d. weight matrices W ,...,W ∈ Rn×n. The M-ensemble estimator is defined
1 M
as:
M
(cid:88)
β(cid:98)W1:M,λ = M−1 β(cid:98)Wm,λ, (6)
m=1
and its performance is quantified by the conditional squared prediction risk, given by:
R(β(cid:98)W1:M,λ) = E x0,y0[(y 0−ϕ⊤ 0β(cid:98)W1:M,λ)2 | Φ,y,{W m}M m=1], (7)
where (x ,y ) is a test point sampled independently from some distribution P that may be
0 0 x0,y0
different from the training distribution P , and ϕ = f (x ) is the pretrained feature at the
x,y 0 nn 0
test point. The covariance matrix of the test features ϕ is denoted by Σ . When P = P ,
0 0 x0,y0 x,y
we refer to it as the in-distribution risk. On the other hand, when P differs from P , we
x0,y0 x,y
refer to it as the out-of-distribution risk. Note that the conditional risk R is a scalar random
M
variable that depends on both the dataset (Φ,y) and the weight matrix W for m ∈ [M].
m
Our goal in this section is to analyze the prediction risk of the ensemble estimator (6) for any
ensemble size M.
Theorem 6 (Risk equivalence along the path). Under the setting of Theorem 1, assume
that the operator norm of Σ is uniformly bounded in p and that each response variable y
0 i
for i = 1,...,n has mean 0 and satisfies E[|y |4+µ] ≤ M < ∞ for some µ,M > 0. Then,
i µ µ
along the path (4),
C
R(β(cid:98)W1:M,λ) ≃ R(β(cid:98)I,µ)+
M
tr[(G
I
+µI)†yy⊤(G
I
+µI n)†], (8)
where the constant C is given by:
C = −∂µ/∂λ·λ2S W′ ⊤W(−df(β(cid:98)I,µ))tr[(G
I
+µI)†(ΦΣ 0Φ⊤/n)(G
I
+µI)†]. (9)
At a high level, Theorem 6 provides a bias-variance-like risk decomposition for both the squared
risks of weighted ensembles. The risk of the weighted predictor is equal to the risk of the un-
weighted equivalent implicit ridge regressor (bias) plus a term due to the randomness due to
weighting(variance).TheinflationfactorC controlsthemagnitudeofthisterm,anditdecreases
at a rate of 1/M as the ensemble size M increases (see Figure 7 for a numerical verification of
thisrate).Therefore,byusingaresampleensemblewithasufficientlylargesizeM,wecanretain
the statistical properties of the full ridge regression while reducing memory usage and increasing
parallelization.
Theorem 6 extends the risk equivalence results in [50, 52]. Compared to previous results, Theo-
rem 6 provides a broader risk equivalence that holds for general weight and feature matrices, as
well as an arbitrary ensemble size M. It is important to note that Theorem 6 holds even when
the test distribution differs from the training data, making it applicable to out-of-distribution
risks. Furthermore, our results do not rely on any specific distributional assumptions for the
response vector, making them applicable in a model-free setting. The key idea behind this result
istoexploitasymptoticfreenessbetweenthesubsampleanddatamatrices.Next,wewilladdress
the question of optimal tuning.
104.1 Optimal oracle tuning
AsinTheorem2,wenextanalyzevariouspropertiesrelatedtooptimalsubsamplingweightsand
their implications for the risk of optimal ridge regression. Recall that the subsampling operator
W(k) is a diagonal matrix with k ∈ {1,...,n} nonzero diagonal entries, which is parameterized
by the subsample size k. Note that the optimal regularization parameter µ∗ for the full data
(W(k) = I or k = n) is a function of the distribution of pretrained data and the test point.
Based on the risk equivalence in Theorem 6, there exists an optimal path of (k,λ) with the
corresponding full-ensemble estimator β(cid:98)
W(k),λ
:= lim M→∞β(cid:98)
W(k),λ
that achieves the optimal
1:∞ 1:M
predictive performance at (n,µ∗). In particular, the ridgeless ensemble with λ∗ = 0 happens to
be on the path. From previous work [25, 50], the optimal subsample size k∗ for λ∗ = 0 has the
property that k∗ ≤ p under linear features. We show in the following that this property can be
extended to include general features.
Proposition 7 (Optimalsubsampleratio). AssumethesubsamplingoperatorW asdefined
in Theorem 2. Let µ∗ = argmin R(β(cid:98) ). Then the corresponding subsample size
µ≥0 W(k),µ
1:∞
satisfies:
k∗ = df(β(cid:98) W(k),µ∗) ≤ rank(G I). (10)
1:∞
The optimal subsample size k∗ obtained from Proposition 7 is asymptotically optimal. For
linear features, in the underparameterized regime where n > p, [25, 50] show that the optimal
subsample size k∗ is asymptotically no larger than p. This result is covered by Proposition 7
by noting that rank(G ) ≤ p under linear features. It is interesting and somewhat surprising
I
to note that in the underparameterized regime (when p ≤ n), we do not need more than p
observations to achieve the optimal risk. In this sense, the optimal subsampled dataset is always
overparameterized.
When the limiting risk profiles R(γ,ψ,µ) := lim R(β(cid:98) ) exist for subsample
p/n→γ,p/k→ψ W(k),µ
1:∞
ensembles, the limiting risk of the optimal ridge predictor inf R(γ,γ,µ) is monotonically
µ≥0
decreasing in the limiting sample aspect ratio γ [50]. This also (provably) confirms the sample-
wisemonotonicityofoptimally-tunedriskforgeneralfeaturesinanasymptoticsense[48].Dueto
theriskequivalenceinTheorem6,foranyµ > 0,thereexistsψsuchthatR(γ,γ,µ) = R(γ,ψ,0).
This implies that inf R(γ,γ,µ) = inf R(γ,ψ,0). In other words, tuning over subsample
µ≥0 ψ≥γ
sizes with sufficiently large ensembles is equivalent to tuning over the ridge penalty on the full
data.
4.2 Data-dependent tuning
As suggested by Proposition 7, the optimal subsample size is smaller than the rank of the
Gram matrix. This result has important implications for real-world datasets where the number
of observations (n) is much larger than the number of features (p). In such cases, instead of
using the entire dataset, we can efficiently build small ensembles with a subsample size k ≤ p.
This approach is particularly beneficial when n is significantly higher than p, for example, when
n = 1000p. By fitting ensembles with only M = 100 base predictors, we can potentially reduce
the computational burden while still achieving optimal predictive performance. Furthermore,
this technique can be especially valuable in scenarios where computational resources are limited
11Degrees of freedom Training error Prediction error
0.0 0.00050
1200
0.017
1000 0.00045 0.016
800 0.00040 0.015
0.1 0.014
600
0.00035 0.013
400
1 0.012
0.00030
200 0.011
10
Subsample ratio k/n Subsample ratio k/n Subsample ratio k/n
Figure 3: Equivalence in pretrained features of pretrained ResNet-50 on Flowers-102 datasets.
or when dealing with massive datasets that cannot be easily processed in their entirety.
In the following, we propose a method to determine the optimal values of the regularization
parameter µ∗ for the full ridge regression, as well as the corresponding subsample size k∗ and
the optimal ensemble size M∗. According to Theorem 6, the optimal value of M∗ is theoretically
infinite.However,inpractice,thepredictionriskoftheM-ensemblepredictordecreasesatarate
of 1/M as M increases. Therefore, it is important to select a suitable value of M that achieves
the desired level of performance while considering computational constraints and the specified
error budget. By carefully choosing an appropriate M, we can strike a balance between model
accuracy and efficiency, ensuring that the subsampled neural representations are effectively used
in downstream tasks.
Consider a grid of subsample size K ⊆ {1,...,n}; for instance, K = {0,k ,2k ,...,n} where
n n 0 0
k is a subsample size unit. For a prespecified subsample size k ∈ K and ensemble size M ∈
0 n 0
N, suppose we have multiple risk estimates R(cid:98)m of R
m
for m = 1,...,M 0. The squared risk
decomposition [51, Eq (7)] along with the equivalence path (8) implies that R = m−1R +
m 1
(cid:0) 1−m−1(cid:1) R , for m = 1,...,M . Summing these equations yields (cid:80)M0 R = (cid:80)M0 1R +
∞ 0 m=1 m m=1 m 1
(cid:80)M0 (cid:0) 1−m−1(cid:1)
R . Thus, we can estimate R by:
m=1 ∞ ∞
R(cid:98)∞ =
(cid:16)
(cid:88)M0
R(cid:98)m−
(cid:88)M0
m−1R(cid:98)1(cid:17)
/
(cid:88)M0
(cid:0) 1−m−1(cid:1)
. (11)
m=1 m=1 m=1
Then, the extrapolated risk estimates R(cid:98)m (with m > M 0) are defined as:
R(cid:98)m :=
m−1R(cid:98)1+(cid:0) 1−m−1(cid:1)
R(cid:98)∞ for m > M 0. (12)
The meta-algorithm that implements the above cross-validation procedure is provided in Algo-
rithm 1. To efficiently tune the parameters of ridge ensembles, we use and combine the corrected
generalized cross-validation (CGCV) method [13] and the extrapolated cross-validation (ECV)
method [26]. The improved CV method is implemented in the Python library [24].
4.3 Validation on real-world datasets
Inthissection,wepresentnumericalexperimentstovalidateourtheoreticalresultsonreal-world
datasets. Figure 3 provides evidence supporting Assumption A on pretrained features extracted
fromcommonlyusedneuralnetworksappliedtoreal-worlddatasets.Thefirstpanelofthe figure
demonstrates the equivalence of degrees of freedom for these pretrained features. Furthermore,
12
ytlanep
egdiRAlgorithm 1 Meta-algorithm for tuning of ensemble size and subsample operator.
Input: A dataset D = {(x ,y ) ∈ Rp×R : 1 ≤ i ≤ n}, a regularization parameter λ, a class of
n i i
subsampleoperatordistributionP = {P } ,aensemblesizeM ≥ 2forriskestimation,
n k k∈Kn 0
and optimality tolerance parameter δ.
(k) (k) i.i.d.
1: Build ensembles β(cid:98) W(k) ,λ with M 0 base estimators, where W 1 ,...,W M0 ∼ P k for each
1:M0
k ∈ K .
n
2: Estimate the prediction risk of β(cid:98)
W(k) ,λ
with R(cid:98)m,k by CV methods such as CGCV [13], for
1:M0
k ∈ K and m = 1,...,M .
n 0
3: Extrapolate the risk estimations R(cid:98)m,k for m > M 0 using (11) and (12).
4: Select a subsample size (cid:98)k ∈ argmin k∈KnR(cid:98)∞,k. that minimizes the extrapolated estimates.
5: Select an ensemble size M(cid:99)∈ argmin m∈N1{R(cid:98)
m,(cid:98)k
> R(cid:98)
∞,(cid:98)k
+δ} for the δ-optimal risk.
6: If M(cid:99)> M 0, fit a M(cid:99)-ensemble estimator β(cid:98) W(k(cid:98)),λ.
1:M(cid:99)
Output: Return the tuned estimator β(cid:98) W(k(cid:98)),λ, and the risk estimators R(cid:98)M,k for all M,k.
1:M(cid:99)
Prediction risk M=1 M=2 M=5 M=10 M=100
Estimated risk M=1 M=2 M=5 M=10 M=100
Fashion-MNIST / ResNet-18 init. CIFAR-10 / ResNet-18 pretr. CIFAR-100 / ResNet-50 pretr. Food-101 / ResNet-101 pretr.
0.09 0.09 0.020 0.020
0.08 0.08 0.018 0.018
0.016
0.07 0.07 0.016
0.014
0.014
0.06 0.06
0.012
0.012
0.05 0.05 0.010
0.010
0.008
0.04 0.04
0.006 0.008
0.03 0.03
103 102 101 100 103 102 101 100 103 102 101 100 103 102 101 100
Subsample ratio k/n Subsample ratio k/n Subsample ratio k/n Subsample ratio k/n
Figure 4: Risk estimation by corrected and extrapolated generalized cross-validation. The risk estimates
are computed based on M =25 base estimators using Algorithm 1 with λ=10−3.
0
we also observe consistent behavior across different neural network architectures and different
datasets (see Figures 8 and 9). Remarkably, the path of equivalence can be accurately predicted,
offeringvaluableinsightintotheunderlyingdynamicsofthesemodels.Thisobservationsuggests
that the pretrained features from widely used neural networks exhibit similar properties when
appliedtoreal-worlddata,regardlessofthespecificarchitectureemployed.Theabilitytopredict
the equivalence path opens up new possibilities for optimizing the performance of these models
in practical applications.
OneimplicationoftheequivalenceresultsexploredinTheorems1and6isthatinsteadoftuning
for the full ridge penalty µ on the large datasets, we can fix a small value of the ridge penalty λ,
fitsubsampleridgeensembles,andtuneforanoptimalsubsamplesizek.Toillustratethevalidity
of the tuning procedure described in Algorithm 1, we present both the actual prediction errors
andtheirestimatesbyAlgorithm1inFigure4.Weobservethattheriskestimatescloselymatch
thepredictionrisksatdifferentensemblesizesacrossdifferentdatasets.Evenwithasubsampling
ratio k/n of 0.01 and a sufficiently large M, the risk estimate is close to the optimal risk. A
smaller subsample size could also yield even smaller prediction risk in certain datasets.
13
eulaV5 Limitations and outlook
While our results are quite general in terms of applying to a wide variety of pretrained features,
they are limited in that they only apply to ridge regression fitted on the pretrained features.
The key challenge for extending the analysis based on Assumption A to general estimators be-
yond ridge regression is the characterization of the effect of subsampling general resolvents as
additional ridge regularization. To extend to generalized linear models, one approach is to view
the optimization as iteratively reweighted least squares [38] in combination with the current
results. Another approach is to combine our results with the techniques in [41] to obtain deter-
ministic equivalents for the Hessian, enabling an understanding of implicit regularization due to
subsampling beyond linear models.
Beyond implicit regularization due to subsampling, there are other forms of implicit regular-
ization, such as algorithmic regularization due to early stopping in gradient descent [2, 3, 49],
dropout regularization [62, 65], among others. In some applications, multiple forms of implicit
regularization are present simultaneously. For instance, during a mini-batch gradient step, im-
plicit regularization arises fromboth iterative methods and mini-batch subsampling. The results
presentedinthispapermayhelptomakeexplicitthecombinedeffectofvariousformsofimplicit
regularization.
Acknowledgments
We thank Benson Au, Daniel LeJeune, Ryan Tibshirani, and Alex Wei for the helpful conversa-
tions surrounding this work. We also thank the anonymous reviewers for their valuable feedback
and suggestions.
We acknowledge the computing support the ACCESS allocation MTH230020 provided for some
oftheexperimentsperformedontheBridges2systematthePittsburghSupercomputingCenter.
The code for reproducing the results of this paper can be found at https://jaydu1.github.
io/overparameterized-ensembling/weighted-neural.
References
[1] Adlam, B., Levinson, J. A., and Pennington, J. (2022). A random matrix perspective on
mixtures of nonlinearities in high dimensions. In International Conference on Artificial
Intelligence and Statistics.
[2] Ali, A., Dobriban, E., and Tibshirani, R. J. (2020). The implicit regularization of stochastic
gradient flow for least squares. In International conference on machine learning.
[3] Ali, A., Kolter, J. Z., and Tibshirani, R. J. (2019). A continuous-time view of early stop-
ping for least squares regression. In International Conference on Artificial Intelligence and
Statistics.
[4] Anderson, G. W., Guionnet, A., and Zeitouni, O. (2010). An Introduction to Random Ma-
trices. Cambridge University Press.
[5] Ando, R. and Komaki, F. (2023). On high-dimensional asymptotic properties of model
averaging estimators. arXiv preprint arXiv:2308.09476.
14[6] Bach, F. (2023). High-dimensional analysis of double descent for linear regression with
random projections. arXiv:2303.01372.
[7] Bahri, Y., Dyer, E., Kaplan, J., Lee, J., and Sharma, U. (2021). Explaining neural scaling
laws. arXiv:2102.06701.
[8] Bai, Z. and Silverstein, J. W. (2010). Spectral Analysis of Large Dimensional Random Ma-
trices. Springer. Second edition.
[9] Barthelm´e, S., Amblard, P.-O., Tremblay, N., and Usevich, K. (2023). Gaussian process
regression in the flat limit. The Annals of Statistics, 51(6):2471–2505.
[10] Bartlett, P. L., Long, P. M., Lugosi, G., and Tsigler, A. (2020). Benign overfitting in linear
regression. Proceedings of the National Academy of Sciences, 117(48):30063–30070.
[11] Bartlett, P. L., Montanari, A., and Rakhlin, A. (2021). Deep learning: A statistical view-
point. Acta Numerica, 30:87–201.
[12] Belkin, M., Hsu, D., and Xu, J. (2020). Two models of double descent for weak features.
SIAM Journal on Mathematics of Data Science, 2(4):1167–1180.
[13] Bellec, P., Du, J.-H., Koriyama, T., Patil, P., and Tan, K. (2023). Corrected gen-
eralized cross-validation for finite ensembles of penalized estimators. arXiv preprint
arXiv:2310.01374.
[14] Bordelon, B., Canatar, A., and Pehlevan, C. (2020). Spectrum dependent learning curves
in kernel regression and wide neural networks. In International Conference on Machine
Learning.
[15] C´ebron, G., Dahlqvist, A., and Gabriel, F. (2022). Freeness of type B and conditional
freeness for random matrices. arXiv preprint arXiv:2205.01926.
[16] Chen, X., Zeng, Y., Yang, S., and Sun, Q. (2023). Sketched ridgeless linear regression:
The role of downsampling. In Proceedings of the 40th International Conference on Machine
Learning.
[17] Clart´e, L., Vandenbroucque, A., Dalle, G., Loureiro, B., Krzakala, F., and Zdeborov´a, L.
(2024). Analysis of bootstrap and subsampling in high-dimensional regularized regression.
arXiv preprint arXiv:2402.13622.
[18] Couillet,R.andLiao,Z.(2022). RandomMatrixMethodsforMachineLearning. Cambridge
University Press.
[19] Cui, H., Loureiro, B., Krzakala, F., and Zdeborov´a, L. (2021). Generalization error rates
in kernel regression: The crossover from the noiseless to noisy regime. Advances in Neural
Information Processing Systems, 34:10131–10143.
[20] Dobriban,E.andSheng,Y.(2020). Wonder:Weightedone-shotdistributedridgeregression
in high dimensions. Journal of Machine Learning Research, 21(66):1–52.
15[21] Dobriban, E. and Sheng, Y. (2021). Distributed linear regression by averaging. The Annals
of Statistics, 49(2):918–943.
[22] Dobriban, E. and Wager, S. (2018). High-dimensional asymptotics of prediction: Ridge
regression and classification. The Annals of Statistics, 46(1):247–279.
[23] Drineas, P., Mahoney, M. W., and Muthukrishnan, S. (2006). Sampling algorithms for ℓ
2
regression and applications. Proceedings of the Seventeenth Annual ACM-SIAM Symposium
on Discrete Algorithm.
[24] Du, J.-H. and Patil, P. (2023). Python package sklearn ensemble cv v0.2.1. PyPI.
[25] Du, J.-H., Patil, P., and Kuchibhotla, A. K. (2023). Subsample ridge ensembles: Equiva-
lences and generalized cross-validation. In International Conference on Machine Learning.
[26] Du, J.-H., Patil, P., Roeder, K., and Kuchibhotla, A. K. (2024). Extrapolated cross-
validation for randomized ensembles. Journal of Computational and Graphical Statistics.
[27] Efron, B. (1983). Estimating the error rate of a prediction rule: improvement on cross-
validation. Journal of the American Statistical Association, 78(382):316–331.
[28] Efron, B. (1986). How biased is the apparent error rate of a prediction rule? Journal of the
American Statistical Association, 81(394):461–470.
[29] Ghorbani,B.,Mei,S.,Misiakiewicz,T.,andMontanari,A.(2020).Whendoneuralnetworks
outperform kernel methods? Advances in Neural Information Processing Systems.
[30] Hastie, T., Montanari, A., Rosset, S., and Tibshirani, R. J. (2022). Surprises in high-
dimensional ridgeless least squares interpolation. The Annals of Statistics, 50(2):949–986.
[31] Hastie, T. and Tibshirani, R. (1990). Generalized Additive Models. Chapman & Hall.
[32] He,K.,Zhang,X.,Ren,S.,andSun,J.(2016). Deepresiduallearningforimagerecognition.
In Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR).
[33] Jacot, A., Gabriel, F., and Hongler, C. (2018). Neural tangent kernel: Convergence and
generalization in neural networks. Advances in neural information processing systems.
[34] Jacot, A., Simsek, B., Spadaro, F., Hongler, C., and Gabriel, F. (2020). Kernel alignment
risk estimator: Risk prediction from training data. Advances in Neural Information Process-
ing Systems.
[35] Koehler, F., Zhou, L., Sutherland, D. J., and Srebro, N. (2021). Uniform convergence of
interpolators: Gaussian width, norm bounds and benign overfitting. Advances in Neural
Information Processing Systems.
[36] Krogh, A. and Sollich, P. (1997). Statistical mechanics of ensemble learning. Physical
Review E, 55(6):811.
16[37] LeJeune, D., Javadi, H., and Baraniuk, R. (2020). The implicit regularization of ordinary
least squares ensembles. In International Conference on Artificial Intelligence and Statistics.
[38] LeJeune, D., Javadi, H., and Baraniuk, R. G. (2021). The flip side of the reweighted
coin: Duality of adaptive dropout and regularization. In Advances in Neural Information
Processing Systems.
[39] LeJeune, D., Patil, P., Javadi, H., Baraniuk, R. G., and Tibshirani, R. J. (2024). Asymp-
totics of the sketched pseudoinverse. SIAM Journal on Mathematics of Data Science,
6(1):199–225.
[40] Liang, T. and Rakhlin, A. (2020). Just interpolate: Kernel “ridgeless” regression can gen-
eralize. The Annals of Statistics.
[41] Liao, Z. and Mahoney, M. W. (2021). Hessian eigenspectra of more realistic nonlinear
models. In Advances in Neural Information Processing Systems, volume 34.
[42] Liu,S.andDobriban,E.(2020).Ridgeregression:Structure,cross-validation,andsketching.
In International Conference on Learning Representations.
[43] Loureiro, B., Gerbelot, C., Cui, H., Goldt, S., Krzakala, F., Mezard, M., and Zdeborova, L.
(2021). Learning curves of generic features maps for realistic datasets with a teacher-student
model. In Advances in Neural Information Processing Systems.
[44] Mahoney, M. W. (2011). Randomized algorithms for matrices and data. Foundations and
Trends in Machine Learning, 3(2):123–224.
[45] Mallinar, N., Simon, J. B., Abedsoltan, A., Pandit, P., Belkin, M., and Nakkiran, P. (2022).
Benign, tempered, or catastrophic: A taxonomy of overfitting. arXiv:2207.06569.
[46] Mei, S. and Montanari, A. (2022). The generalization error of random features regression:
Precise asymptotics and the double descent curve. Communications on Pure and Applied
Mathematics, 75(4):667–766.
[47] Mingo, J. A. and Speicher, R. (2017). Free Probability and Random Matrices, volume 35.
Springer.
[48] Nakkiran, P., Venkat, P., Kakade, S. M., and Ma, T. (2021). Optimal regularization can
mitigate double descent. In International Conference on Learning Representations.
[49] Neu, G. and Rosasco, L. (2018). Iterate averaging as regularization for stochastic gradient
descent. In Conference On Learning Theory.
[50] Patil, P. and Du, J.-H. (2023). Generalized equivalences between subsampling and ridge
regularization. Advances in Neural Information Processing Systems.
[51] Patil, P., Du, J.-H., and Kuchibhotla, A. K. (2023). Bagging in overparameterized learn-
ing: Risk characterization and risk monotonization. Journal of Machine Learning Research,
24(319):1–113.
17[52] Patil, P., Du, J.-H., and Tibshirani, R. J. (2024). Optimal ridge regularization for out-of-
distribution prediction. arXiv preprint arXiv:2404.01233.
[53] Patil,P.,Kuchibhotla,A.K.,Wei,Y.,andRinaldo,A.(2022). Mitigatingmultipledescents:
A model-agnostic framework for risk monotonization. arXiv preprint arXiv:2205.12937.
[54] Patil, P. and LeJeune, D. (2024). Asymptotically free sketched ridge ensembles: Risks,
cross-validation, and tuning. In International Conference on Learning Representations.
[55] Paul, D. and Aue, A. (2014). Random matrix theory in statistics: A review. Journal of
Statistical Planning and Inference, 150:1–29.
[56] Rahimi,A.andRecht,B.(2007). Randomfeaturesforlarge-scalekernelmachines. Advances
in neural information processing systems, 20.
[57] Sahraee-Ardakan, M., Emami, M., Pandit, P., Rangan, S., and Fletcher, A. K. (2022).
Kernel methods and multi-layer perceptrons learn linear models in high dimensions. arXiv
preprint arXiv:2201.08082.
[58] Serdobolskii, V. I. (2007). Multiparametric Statistics. Elsevier.
[59] Slagel, J. T., Chung, J., Chung, M., Kozak, D., and Tenorio, L. (2019). Sampled tikhonov
regularization for large linear inverse problems. Inverse Problems, 35(11):114008.
[60] Sollich,P.(2001). Gaussianprocessregressionwithmismatchedmodels. AdvancesinNeural
Information Processing Systems, 14.
[61] Sollich, P. and Krogh, A. (1995). Learning with ensembles: How overfitting can be useful.
In Advances in Neural Information Processing Systems.
[62] Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. (2014).
Dropout: A simple way to prevent neural networks from overfitting. The Journal of Machine
Learning Research, 15(1):1929–1958.
[63] Stein, C. M. (1981). Estimation of the mean of a multivariate normal distribution. The
Annals of Statistics, pages 1135–1151.
[64] Voiculescu, D. V. (1997). Free Probability Theory. American Mathematical Society.
[65] Wager, S., Wang, S., and Liang, P. S. (2013). Dropout training as adaptive regularization.
In Advances in Neural Information Processing Systems.
[66] Wei, A., Hu, W., and Steinhardt, J. (2022). More than a toy: Random matrix models
predict how real-world neural representations generalize. In International Conference on
Machine Learning.
[67] Weisberg, S. (2005). Applied Linear Regression. John Wiley & Sons.
18[68] Yang, G. (2019). Scaling limits of wide neural networks with weight sharing: Gaussian
processbehavior,gradientindependence,andneuraltangentkernelderivation.arXivpreprint
arXiv:1902.04760.
[69] Yosinski, J., Clune, J., Bengio, Y., and Lipson, H. (2014). How transferable are features in
deep neural networks? In Advances in Neural Information Processing Systems.
[70] Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O. (2017). Understanding deep
learning requires rethinking generalization. In International Conference on Learning Repre-
sentations.
[71] Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O. (2021). Understanding deep
learning (still) requires rethinking generalization. Communications of the ACM, 64(3):107–
115.
19Appendix
This serves as an appendix to the paper “Implicit Regularization Paths of Weighted Neural
Representations.” The beginning (unlabeled) section of the appendix provides an organization
for the appendix, followed by a summary of the general notation used in both the paper and
the appendix. Any other specific notation is explained inline where it is first used.
Organization
• In Appendix A, we provide a brief technical background on free probability theory and
various transforms that we need and collect known asymptotic ridge equivalents that we use
in our proofs.
• In Appendix B, we present proofs of the theoretical results in Section 3 (Theorems 1 and 2
and Propositions 3–5).
• In Appendix C, we present proofs of the theoretical results in Section 4 (Theorem 6 and
Proposition 7).
• In Appendix D, we provide additional illustrations for the results in Section 3 (Figures 5
and 6).
• In Appendix E, we provide additional illustrations for the results in Section 4 (Figures 7–9),
including our meta-algorithm for tuning (Algorithm 1) that is not included in the main text
due to space constraints.
• In Appendix F, we provide additional details on the experiments in both Section 3 and
Section 4.
Notation
We use blackboard letters to denote some special sets: N denotes the set of natural numbers, R
denotes the set of real numbers, R denotes the set of positive real numbers, C denotes the set
+
of complex numbers, C+ denotes the set of complex numbers with positive imaginary part, and
C− denotes the set of complex numbers with negative imaginary part. We use [n] to denote the
index set {1,2,...,n}.
We denote scalars and vectors using lower-case letters and matrices using upper-case letters.
For a vector β, β⊤ denotes its transpose, and ∥β∥ denotes its ℓ norm. For a pair of vectors
2 2
u and v, ⟨u,v⟩ denotes their inner product. For a matrix X ∈ Rn×p, X⊤ ∈ Rp×n denotes its
transpose, and X† ∈ Rp×n denotes its Moore-Penrose inverse. For a square matrix A ∈ Rp×p,
tr[A] denotes its trace, tr[A] denotes its average trace tr[A]/p, and A−1 denotes its inverse,
provided that A is invertible. For a symmetric matrix A, λ+ (A) denotes its minimum nonzero
min
eigenvalue. For a positive semidefinite matrix G, G1/2 denotes its principal square root. For a
matrix X, we denote by ∥X∥ its operator norm with respect to the ℓ vector norm. It is also
op 2
the spectral norm of X. For a matrix X, we denote by ∥X∥ its trace norm. It is given by
tr
tr[(X⊤X)1/2], and is also the nuclear norm X. We denote p×p identity matrix by I , or simply
p
by I when it is clear from the context.
For symmetric matrices A and B, we use A ⪯ B to denote the Loewner ordering to mean
that A−B is a positive semidefinite matrix. For two sequences of matrices A and B , we use
p p
A ≃ B to denote a certain asymptotic equivalence; see Appendix A.3 for a precise definition.
p p
20A Technical background
A.1 Basics of free probability theory
In this section, we briefly review definitions from free probability theory and its applications to
random matrices. This review will help set the stage by introducing the various mathematical
structures and spaces we are working with. It will also introduce some of the notation used
throughout the text.
Freeprobabilityisamathematicalframeworkthatdealswithnon-commutativerandomvariables
[19].Theuseoffreeprobabilitytheoryhasappearedinvariousrecentworksinstatisticalmachine
learning, including [1, 2, 11, 12, 17]. Good references on free probability theory include [3, 13],
from which we borrow some basic definitions in the following. All the material in this section is
standard in free probability theory and mainly serves to keep the definitions self-contained.
Definition 8 (Non-commutative algebra). A set A is called a (complex) algebra (over the field
of complex numbers C) if it is a vector space (over C with addition +), equipped with a bilinear
multiplication ·, such that for all x,y,z ∈ A and α ∈ C,
(1) x·(y·z) = (x·y)·z,
(2) (x+y)·z = x·z+y·z,
(3) x·(y+z) = x·y+x·z,
(4) α(x·y) = (αx)·y = x·(αy).
In addition, an algebra is called unital if a multiplicative identity element exists. We will use 1
A
to denote this identity element. We will drop the “·” symbol to denote multiplication over the
algebra.
Definition 9 (Non-commutative probability space). Let A over C be a unital algebra with
identity 1 . Let φ : A → C be a linear functional which is unital (that is, φ(1 ) = 1). Then
A A
(A,φ) is called a non-commutative probability space, and φ is called a state. A state φ is said
to be tracial if φ(xy) = φ(yx) for all x,y ∈ A.
Definition 10 (Moments). Let (A,φ) be a non-commutative probability space. The numbers
{φ(xk)}∞ are called the moments of the variable x ∈ A.
k=1
Definition 11 (∗-algebra). An algebra A is called a ∗-algebra if there exists a mapping x → x∗
from A → A such that, for all x,y ∈ A and α ∈ C,
(1) (x+y)∗ = x∗+y∗,
(2) (αx)∗ = αx∗,
(3) (xy)∗ = y∗x∗,
(4) (x∗)∗ = x.
A variable x of a ∗-algebra is called self-adjoint if x = x∗. A unital linear functional φ on a
∗-algebra is said to be positive if φ(x∗x) ≥ 0 for all x ∈ A.
Definition 12 (∗-probability space). Let A be a unital ∗-algebra with a positive state φ. Then
(A,φ) is called a ∗-probability space.
Example 1. Denote by M (C) the collection of all p×p matrices with complex entries. Let
p
21the multiplication and addition operations be defined in the usual way. The ∗-operation is the
same as taking the conjugate transpose. Let tr : M (C) → C be the normalized trace defined
p
by:
1
tr(A) = tr[A].
p
The state tr is tracial and positive.
Definition 13 (Free independence). Suppose (A,φ) is a ∗-probability space. Then, the ∗-sub-
algebras {A } of A are said to be ∗-freely independent (or simply ∗-free) if, for all n ≥ 2
i i∈I
and all x ,x ,··· ,x from {A } , κ (x ,x ,··· ,x ) = 0 whenever at least two of the x are
1 2 n i i∈I n 1 2 n i
from different A . In particular, any collection of variables is said to be ∗-free if the sub-algebras
i
generated by these variables are ∗-free.
Lemma 14. Suppose (A,φ) is a ∗-probability space. If x and y are free in (A,φ), then for all
non-negative integers n and m,
φ(xnym) = φ(xn)φ(ym) = φ(ymxn).
In other words, elements of the algebra are considered free if any alternating product of centered
polynomials is also centered.
In this work, we will consider φ to be the normalized trace. The normalized trace is the gener-
alization of 1 tr[A] for A ∈ Cp×p to elements of a C∗-algebra A. Specifically, for any self-adjoint
p
a ∈ A and any polynomial p, we have
(cid:90)
φ(p(a)) = p(z)dµ (z),
a
where µ is the probability measure that characterizes the spectral distribution of a.
a
Definition 15 (Convergenceinspectraldistribution). Let(A,φ)beaC∗-probabilityspace.We
say that A ,...,A ∈ Cp×p converge in spectral distribution to elements a ,...,a ∈ A if, for
1 m 1 m
all 1 ≤ ℓ < ∞ and 1 ≤ i ≤ m for 1 ≤ j ≤ ℓ, we have
j
1
tr[A ···A ] → φ(a ···a ).
p
i1 i
ℓ
i1 i
ℓ
Then, with slight abuse of notation, two matrices A,B ∈ Rp×p are said to be free if
(cid:34) L (cid:35)
1 (cid:89)
tr polyA(A)polyB(B) = 0,
p ℓ ℓ
ℓ=1
for all L ≥ 1 and all centered polynomials, that is, tr[polyA(A)] = 0. This notation is an abuse
ℓ
of notation because finite matrices cannot satisfy this condition. However, they can satisfy it
asymptotically as p → ∞, and in this case, we say that A and B are asymptotically free.
Note:Withsomeabuseofnotation,wewillletmatricesinboldfacedenoteboththefinitematrix
and the limiting element in the free probability space. The limiting element can be understood,
for example, as a bounded linear operator on a Hilbert space. We also remark that all notions
we need are well-defined in this limit as well, as long as they are appropriately normalized.
22A.2 Useful transforms and their relationships
In this section, we review the key transforms used in free probability theory and their interrela-
tionships.
Definition 16 (Cauchytransform). Letabeanelementofa∗-probabilityspace(A,φ).Suppose
there exists some C > 0 such that |φ(an)| ≤ Cn for all n ∈ N. Then the Cauchy transform of a
is defined as:
(cid:88)∞ φ(an)
G (z) =
a zn+1
n=0
for all z ∈ C with |z| > C.
Note that the Cauchy transform is the negative of the Stieltjes transform. In this paper, we will
focus only on the Cauchy transform. Recall that for a probability measure ν on R and for z ∈/ R,
the Cauchy transform of ν is defined as:
(cid:90)
1
G(z) = dν(x).
z−x
R
The definition above is motivated by the following property of the Cauchy transform of a mea-
sure. Suppose ν is a probability measure whose support is contained in [−C,C] for some C > 0
and which has moments {m (ν)}∞ . Then the Cauchy transform of ν is defined for z ∈ C with
k k=0
|z| > C as:
∞
(cid:88) m k(ν)
G (z) = .
ν zk+1
k=0
Definition 17 (Moment generating function). Let a be an element of a ∗-probability space
(A,φ). The moment generating function of a is defined as:
∞
(cid:88)
M (z) = 1+ φ(ak)zk
a
k=1
for z ∈ C such that |z| < r . Here, r is the radius of convergence of the series.
a a
For a probability measure ν, the moment generating function is defined analogously. (Note: The
definitionabove isnot tobeconfused withthemomentgeneratingfunction ofarandom variable
in probability theory.) The Cauchy transform is related to the moment series via:
(cid:18) (cid:19)
1 1
G (z) = M . (13)
a a
z z
In the other direction, we have:
(cid:18) (cid:19)
1 1
M (z) = G −1. (14)
a a
z z
Definition 18 (S-transform). For
∞
(cid:88)
M (z) = φ(am)zm,
a
m=0
23we define the S-transform of a by:
1+w
S (w) = M⟨−1⟩(w), (15)
a w a
where M⟨−1⟩ denotes the inverse under composition of M.
Finally, in terms of operator A, we summarize the series of invertible transformations between
the various transforms introduced in this section.
• Cauchy transform:
G (z) = tr[(zI −A)−1].
A
• Moment generating series:
(cid:18) (cid:19)
1 1
M (z) = G −1.
A A
z z
• S-transform:
1+w
⟨−1⟩
S (w) = M (w).
A w A
Here:
• M (z) = (cid:80)∞ tr[Ak]zk is the moment generating series.
A k=1
• M⟨−1⟩ denotes the inverse under composition of M .
A A
• tr[A] denotes the average trace tr[A]/p of a matrix A ∈ Rp×p.
A.3 Asymptotic ridge resolvents
In this section, we provide a brief background on the language of asymptotic equivalents used
in the proofs throughout the paper. We will state the definition of asymptotic equivalents and
point to useful calculus rules. For more details, see [16, Appendix S.7].
To concisely present our results, we will use the framework of asymptotic equivalence [5, 6, 16],
defined as follows. Let A and B be sequences of matrices of arbitrary dimensions (including
p p
vectorsandscalars).WesaythatA andB areasymptotically equivalent,denotedasA ≃ B ,
p p p p
if lim |tr[C (A −B )]| = 0 almost surely for any sequence of random matrices C with
p→∞ p p p p
boundedtracenormthatareindependentofA andB .Notethatforsequencesofscalarrandom
p p
variables, the definition simply reduces to the typical almost sure convergence of sequences of
random variables involved.
The notion of deterministic equivalents obeys various calculus rules such as sum, product, dif-
ferentiation, conditioning, and substitution. We refer the reader to [16] for a comprehensive list
of these calculus rules, their proofs, and other related details.
Next,wecollectfirst-andsecond-orderasymptoticequivalentsforsketchedridgeresolventsfrom
[11, 17], which will be useful for our extensions to weighted ridge resolvents.
Assumption B (Sketch structure). Let S ∈ Rp×q be the feature sketching matrix and X ∈
Rn×p be the data matrix. Let SS⊤ and 1X⊤X converge almost surely to bounded operators
n
that are infinitesimally free with respect to (1 tr[·],tr[Θ(·)]) for any Θ independent of S with
p
∥Θ∥ uniformly bounded. Additionally, let SS⊤ have a limiting S-transform that is analytic
tr
on the lower half of the complex plane.
24For the statement to follow, let us define Σ(cid:98) := n1X⊤X. Let λ(cid:101)0 := −liminf p→∞λ+ min(S⊤Σ(cid:98)S).
Here, recall that λ+ (A) represents the minimum nonzero eigenvalue of a symmetric matrix A.
min
Theorem 19 (Free sketching equivalence; [11], Theorem 7.2). Under Assumption B, for all
λ > λ(cid:101)0,
S(S⊤Σ(cid:98)S +λI q)†S⊤ ≃ (Σ(cid:98) +νI p)†, (16)
where ν > −λ+ min(Σ(cid:98)) is increasing in λ > λ(cid:101)0 and satisfies:
ν ≃ λS SS⊤(−tr[Σ(cid:98)S(S⊤Σ(cid:98)S +λI q)†S⊤]) ≃ λS SS⊤(−tr[Σ(cid:98)(Σ(cid:98) +νI p)†]). (17)
Lemma 20 (Second-order equivalence for sketched ridge resolvents; [17], Lemma 15). Under
the settings of Lemma 21, for any positive semidefinite Ψ with uniformly bounded operator
norm, for all λ > λ(cid:101)0,
S(S⊤Σ(cid:98)S +λI q)†S⊤ΨS(S⊤Σ(cid:98)S +λI q)†S⊤ ≃ (Σ(cid:98) +νI p)†(Ψ+ν Ψ′ I p)(Σ(cid:98) +νI p)†, (18)
where ν′ ≥ 0 is given by:
Ψ
∂ν
ν Ψ′ = − ∂λλ2S S′ S⊤(−tr[Σ(cid:98)(Σ(cid:98) +νI p)†])tr[(Σ(cid:98) +νI p)†Ψ(Σ(cid:98) +νI p)†]. (19)
B Proofs in Section 3
B.1 Proof of Theorem 1
Our main ingredient in the proof is Lemma 21. We will first show estimator equivalence and
then show degrees of freedom equivalence.
Estimator equivalence. Recall from (1) the ridge estimator on the weighted data is:
β(cid:98)W,λ = (Φ⊤W⊤WΦ/n+λI p)†Φ⊤W⊤Wy/n.
This is the “primal” form of the ridge estimator. Using the Woodbury matrix identity, we first
write the estimator into its “dual” form.
β(cid:98)W,λ = Φ⊤W⊤(WΦΦ⊤W⊤/n+λI n)†Wy/n
= Φ⊤W⊤(G +λI )†Wy/n.
W n
Now, we can apply the first part of Lemma 21 to the matrix W⊤(G +λI )†W. From (31),
W n
we then have the following equivalence:
β(cid:98)W,λ ≃ Φ⊤(G
I
+µI n)†y/n
= Φ⊤(ΦΦ⊤+µI )†y/n
n
= (Φ⊤Φ/n+µI n)†Φ⊤y/n = β(cid:98)I,µ,
where µ satisfies the following equation:
(cid:18)
tr[G (G +µI
)†](cid:19)
I I n
µ = λS
W⊤W
−
n
= λS W⊤W(−df(β(cid:98)I,µ)).
25Note that in the simplification, we used the Woodbury identity again to go back from the
dual form into the primal form for the ridge estimator based on the full data. Rearranging, we
obtain the desired estimator equivalence. We next move on to showing the degrees of freedom
equivalence.
Degrees of freedom equivalence. For the subsampled estimator β(cid:98)W,λ, the effective degrees of
freedom is given by:
df(β(cid:98)W,λ) = tr[Φ⊤Φ/n(Φ⊤Φ/n+λI p)†]
= tr[Φ(Φ⊤Φ/n+λI )†Φ⊤/n]
p
= tr[(ΦΦ⊤/n+λI )†ΦΦ⊤/n].
n
The second equality above follows from the push-through identity Φ(Φ⊤Φ/n + λI )†Φ⊤ =
p
(ΦΦ⊤+λI )†ΦΦ⊤. Recognizing the quantity inside the trace as the degrees of freedom of the
n
full ridge estimator, we have
µ = λS W⊤W(−df(β(cid:98)I,µ)).
We can equivalently write the equation above as
(cid:16)µ(cid:17) µ
−S W−1
⊤W λ
= df(β(cid:98)I,µ) or
λ
= S W⊤W(−df(β(cid:98)I,µ)).
Rearranging the display above provides the desired degrees of freedom equivalence and finishes
the proof.
B.2 Proof of Theorem 2
We will apply Theorem 1 to the subsampling weight operator W. The main ingredient that we
need is the S-transform of the spectrum of the matrix W⊤W. As summarized in Appendix A.2,
one approach to compute the S-transform is to go through the following chain of transforms.
First, we apply the Cauchy transform, then the moment-generating series, and finally, take the
inverse to obtain the S-transform. We will do this in the following steps.
Cauchy transform. Recall that the Cauchy transform from Definition 16 can be computed as:
G (z) = tr[(zI −W⊤W)−1].
W⊤W n
Moment generating series. We can then compute the moment series from Definition 17 using
(14) as follows:
(cid:34) (cid:35)
1
(cid:18)
1
(cid:19)−1
M (z) = tr I −W⊤W −1
W⊤W
z z
n
= tr[(I −zW⊤W)−1]−tr[I ]
n n
= −tr[I ]+tr[(I −zW⊤W)−1]
n n
= tr[(zW⊤W −I +I )(I −zW⊤W)−1]
n n n
= tr[zW⊤W(I −zW⊤W)−1].
n
We now note that the operator W⊤W has k eigenvalues of 1 and n − k eigenvalues of 0.
Therefore, we have
M (z) = tr[zW⊤W(I −zW⊤W)−1]
W⊤W n
26(cid:32) n (cid:33)
1 (cid:88) zd i
=
n 1−zd
i
i=1
k z
= · . (20)
n 1−z
S-transform. The inverse of the moment generating series map z (cid:55)→ M (z) from (20) is:
W⊤W
w
M⟨−1⟩(w) = . (21)
w+k/n
Therefore, from Definition 18 and using (21), we have
1+w w 1+w
S(w) = · = . (22)
w w+k/n w+k/n
Now, we are ready to apply Theorem 1 to the subsampling operator W.
Substituting (22) into (2), we get
µ 1−df(β(cid:98)I,µ)
= S(−df(β(cid:98)I,µ)) = .
λ −df(β(cid:98)I,µ)+k/n
Rearranging, we obtain
df(β(cid:98)I,µ)·(µ−λ) = µ·(k/n)−λ.
Thus, we get
λ−µ·(k/n)
df(β(cid:98)I,µ) = − .
µ−λ
In other words, we have
(cid:18) (cid:19) (cid:18) (cid:19)
µ k
1−df(β(cid:98)I,µ) = · 1− .
µ−λ n
Multiplying (1−λ/µ) on both sides, we arrive at the desired relation. This completes the proof.
B.3 Proof of Proposition 3
We prove this by matching the path (4) with the one in [15]. Let γ = p/n, ψ = p/k, H be
p
the spectral distribution of Σ(cid:98) = X⊤X/n. The path from Equation (5) of [15] is given by the
following equation:
(cid:90)
r
µ = (ψ−γ) dH (r), (23)
p
1+v(µ,γ)r
where v(µ,γ) is the unique solution to the following fixed-point equation:
(cid:90) (cid:90)
1 r r
= µ+γ dH (r) = ψ dH (r). (24)
p p
v(µ,γ) 1+v(µ,γ)r 1+v(µ,γ)r
For given γ and µ, we will show that ψ that solves (23) gives rise k = p/ψ that also solves (4)
with λ = 0:
−1 tr(cid:2)1XX⊤(cid:0)1XX⊤+µI (cid:1)† ] = −k .
n n n n n
27Rearranging the above equation yields:
k
=
1−µtr(cid:2)(cid:0)1XX⊤+µI (cid:1)†
] = 1−µv(µ,γ),
n n n
where the second equality is from Lemma B.2 of [15]. This implies that
(cid:18) (cid:19)
k 1
µ = 1−
n v(µ,γ)
(cid:18) (cid:19) (cid:90)
k r
= 1− ψ dH (r)
p
n 1+v(µ,γ)r
(cid:90)
r
= (ψ−γ) dH (r),
p
1+v(µ,γ)r
where the second equality follows from (24). The above is the same as the path (23) in [15]. This
finishes the proof.
B.4 Proof of Proposition 4
We first describe the setup for the kernel ridge regression formulation and then show the desired
equivalence.
Setup. Let K(·,·) : Rd × Rd → R be a kernel function. Let H denote the reproducing kernel
Hilbert space associated with kernel K. Kernel ridge regression with the subsampling operator
W solves the following problem with tuning parameter λ ≥ 0:
f(cid:98)W,λ = argmin∥Wy−Wf(X)∥2 2/n+λ∥f∥2 H,
f∈H
where f(X) = [f(x ),...,f(x )]⊤. Kernel ridge regression predictions have a closed-form ex-
1 n
pression:
f(cid:98)W,λ(x) = K(x,X)⊤W⊤(WK(X,X)W⊤+λI n)†Wy.
Here, K(x,X) ∈ Rn with i-th entry K(x,x ), and K(X,X) ∈ Rn×n with the ij-th entry
i
K(x ,x ).
i j
The predicted values on the training data X are given by
f(cid:98)W,λ(X) = K(X,X)⊤W⊤(WK(X,X)W⊤+λI n)†Wy.
Here, the matrix K(X,X)⊤W⊤(WK(X,X)W⊤+λI )†W is the smoothing matrix.
n
Define G = K(X,X) and G = WK(X,X)W⊤. Leveraging the kernel trick, the preceding
I W
optimization problem translates into solving the following problem (in the dual domain):
α = argminα⊤(G +λI )α+2α⊤Wy,
(cid:98)W,λ W n
α∈Rn
where the dual solution is given by α = (G +λI )†Wy. The correspondence between the
(cid:98)W,λ W n
dualandprimalsolutionsissimplygivenby:β(cid:98)W,λ = Φ⊤W⊤α (cid:98)W,λwhereΦ = [ϕ(x 1),...,ϕ(x n)]⊤
is the feature matrix and ϕ: Rd (cid:55)→ H is the feature map of the Hilbert space H with kernel K.
Thus, f(cid:98)W,λ(X) = WΦβ(cid:98)W,λ = WΦΦ⊤W⊤α
(cid:98)W,λ
= G W(G
W
+λI n)†Wy and the degrees of
freedom is given by df(β(cid:98)I,µ) = tr[G W(G
W
+λI n)†].
28Next, we show that (3) holds. Alternatively, one can also show that
α
(cid:98)W,λ
≃ α (cid:98)I,µ, and f(cid:98)W,λ(x 0) ≃ f(cid:98)I,µ(x 0),
which we omit due to similarity. Our proof strategy consists of two steps. We first show that
it suffices to establish the desired result for the linearized version. We then show that we can
suitably adapt our result for the linearized version.
Linearization of kernels. In the below, we will show that for µ ≥ 0,
W⊤(G +λI )†W ≃ (G +µI )†,
W n I n
tr[λ(G +λI )†] ≃ tr[µ(G +µI )†],
W n I n
where W and λ ≥ 0 satisfy that
µ = λS (−1 tr[G (G +λI )†]) = λS (−1 tr[G (G +µI )†]). (25)
W⊤W n I I n W⊤W n W W n
Using assumptions of Proposition 3 and the assumption in Proposition 4, by [18, Proposition
5.1],1
∥G −Glin∥ −→p 0,
I I op
where
Glin = c I +c 1 1⊤+c XX⊤
I 0 n 1 n n 2
and (c ,c ,c ) associated with function g in Proposition 4 and τ = lim tr[Σ]/p are defined
0 1 2 p→∞
as
tr[Σ]
c = g(τ,τ,τ)−g(τ,0,τ)−c , (26)
0 2
p
tr[Σ2]
c = g(τ,0,τ)+g′′(τ,0,τ) , (27)
1 2p2
c = g′(τ,0,τ). (28)
2
Assume C is a sequence of random matrices with bounded trace norm. Note that
n
tr[C ((G +µI )†−(Glin+µI )†)]
p I n I n
≤ tr[C ]∥(G +µI )†−(Glin+µI )†∥
p I n I n op
≤ tr[C ]∥(G +µI )†∥ ∥(Glin+µI )†∥ ∥G −Glin∥ −a −. →s. 0.
p I n op I n op I I op
where in the last inequality, we use a matrix identity A−1 −B−1 = A−1(B −A)B−1 for two
invertible matrices A and B. Thus, we have
(G +µI )† ≃ (Glin+µI )†. (29)
I n p I n
1AssumptionA1of[18]requiresfinite5+δ-moments,whichcanberelaxedtoonlyfinite4+δ-momentsasin
the assumption of Proposition 3, by a truncation argument as in the proof of Theorem 6 of [7, Appendix A.4].
29Hence, combining (29) and the transition property of asymptotic equivalence [15, Lemma S.7.4
(1)], it suffices to show
W⊤(Glin +λI )†W ≃ (Glin+µI )†,
W n I n
where Glin = WGlinW⊤, and λ and µ satisfy (25). Similarly, we can also show that the path
W I
(25) is asymptotically equivalent to
µ = λS (−1 tr[Glin(Glin+λI )†]) = λS (−1 tr[Glin(Glin +µI )†]). (30)
W⊤W n I I n W⊤W n W W n
Equivalence for linearized kernels. We next show that the resolvent equivalence result holds for
Klin. This follows from additional manipulations building on Lemma 21.
B.5 Proof of Proposition 5
In the below, we will show that for µ ≥ 0,
W⊤(WΦΦ⊤W⊤/n+λI )†W ≃ (ΦΦ⊤/n+µI )†,
n n
tr[λ(WΦΦ⊤W⊤/n+λI )†] ≃ tr[µ(ΦΦ⊤/n+µI )†].
n n
Under assumptions in Proposition 5, the linearized features take the form
(cid:114)
ρ √
Φlin = s FX + ρ ω U,
s s
d
where the constants ρ and ω are given in Proposition 5 and U ∈ Rn×p has i.i.d. standard
s s
normal entries. From Claim A.13 of [10], the linear functionals of the estimators β(cid:98)D,λ and β(cid:98)I,µ
with random features Φ and Φlin are asymptotically equivalent. Now, following the proof of
Proposition 4, we apply Lemma 21 on Φlin to yield the desired result.
B.6 Technical lemmas
In preparation for the forthcoming statement, define λ = −liminf λ+ (G ). Recall the
0 n→∞ min W
Gram matrices G = ΦΦ⊤/n and G = WΦΦ⊤W⊤/n.
W
Lemma 21 (General first-order equivalence for freely subsampled ridge resolvents). For W ∈
Rn×n, suppose Assumption A holds for WW⊤. Then, for all λ > λ ,
0
W⊤(G +λI)†W ≃ (G+µI)†, (31)
W
tr[λ(G +λI )†] ≃ tr[µ(G+µI )†], (32)
W n n
where µ > −λ+ (G) solves the equation:
min
µ = λS (−tr[G(G+µV)†]) ≃ λS (−tr[G (G +λV)†]). (33)
WW⊤ WW⊤ W W
Proof of Lemma 21. The first result follows from using Theorem 19 by suitably changing the
roles of X and Φ. In particular, we set Φ to be X⊤ and W to be S and apply Theorem 19 to
obtain
W⊤(WΦΦ⊤W⊤/n+λI )†W ≃ (ΦΦ⊤/n+µI )†. (34)
n n
Writing in terms of G and G , this proves the first part (31).
W
30For the second part, we use the result (34) in the first part and multiply both sides by ΦΦ⊤/n
to get
(ΦΦ⊤/n)·W⊤(WΦΦ⊤W⊤/n+λI )†W ≃ (ΦΦ⊤/n)·(ΦΦ⊤/n+µI )†.
n n
Using the trace property of asymptotic equivalence [15, Lemma S.7.4 (4)], we have
tr[(ΦΦ⊤/n)·W⊤(WΦΦ⊤W⊤/n+λI )†W] ≃ tr[(ΦΦ⊤/n)·(ΦΦ⊤/n+µI )†].
n n
Using the cyclic property of the trace operator yields
tr[(WΦΦ⊤/n)·W⊤(WΦΦ⊤W⊤/n+λI )†] ≃ tr[(ΦΦ⊤/n)·(ΦΦ⊤/n+µI )†].
n n
In terms of G and G , this is the same as
W
tr[G (G +λI )†] ≃ tr[G(G+µI )†].
W W n n
Adding and subtracting λI and µI on the left- and right-hand resolvents, we arrive at the
n n
second part (32). This completes the proof.
C Proofs in Section 4
C.1 Proof of Theorem 6
ThemainingredientsoftheproofareLemmas21and22.Webeginbydecomposingtheunknown
response y into its linear predictor and residual. Specifically, let β be the optimal projection
0 0
parameter given by β = Σ−1E[ϕ y ]. Then, we can express the response as the sum of its best
0 0 0 0
linear predictor, ϕ⊤β , and the residual, y − ϕ⊤β . Denote the variance of this residual by
0 0 0 0 0
σ2 = E[(y −ϕ⊤β )2]. It is easy to see that the risk decomposes as follows:
0 0 0 0
R(β(cid:98)W1:M,λ) = E(cid:2) (y 0−ϕ⊤ 0β(cid:98)W1:M,λ)2 | Φ,y,{W m}M m=1(cid:3)
= (β(cid:98)W1:M,λ−β 0)⊤Σ(β(cid:98)W1:M,λ−β 0)+σ 02.
Here, we used the fact that (y −ϕ⊤β ) is uncorrelated with ϕ , that is, E[ϕ (y −ϕ⊤β )] = 0 .
0 0 0 0 0 0 0 0 p
We note that ∥β ∥ < ∞ and Σ has uniformly bounded operator norm.
0 2 0
Observe that
R(β(cid:98)W1:M,λ) = (β(cid:98)W1:M,λ−β 0)⊤Σ 0(β(cid:98)W1:M,λ−β 0)+σ 02
(cid:18)
1
(cid:88)M (cid:19)⊤ (cid:18)
1
(cid:88)M (cid:19)
=
M
β(cid:98)Wm,λ−β
0
Σ
0
M
β(cid:98)Wm,λ−β
0
+σ 02
m=1 m=1
M M
1 (cid:88) 2 (cid:88)
=
M2
β(cid:98) W⊤ k,λΣ 0β(cid:98)W ℓ,λ−
M
β 0⊤Σ 0β(cid:98)Wm,λ+β 0⊤Σ 0β 0+σ 02
k,ℓ=1 m=1
M
1 (cid:88)
=
M2
(β(cid:98) W⊤ k,λΣ 0β(cid:98)W ℓ,λ−β(cid:98) I⊤ ,µΣ 0β(cid:98)I,µ)+β(cid:98) I⊤ ,µΣ 0β(cid:98)I,µ
k,ℓ=1
M
2 (cid:88)
−
M
β 0⊤Σ 0β(cid:98)Wm,λ+β 0⊤Σ 0β 0+σ 02.
m=1
31By Lemma 21, note that
M
1 (cid:88)
M
β(cid:98)Wm,λ ≃ β(cid:98)I,µ.
k=1
Thus, we have
M
R(β(cid:98)W1:M,λ) ≃ M1
2
(cid:88) (cid:0) β(cid:98) W⊤ k,λΣ 0β(cid:98)W ℓ,λ−β(cid:98) I⊤ ,µΣ 0β(cid:98)I,µ(cid:1)
k,ℓ=1
M
2 (cid:88)
+β(cid:98) I⊤ ,µΣ 0β(cid:98)I,µ−
M
β 0⊤Σ 0β(cid:98)I,µ+β 0⊤Σ 0β 0+σ 02.
m=1
Now, by two applications of Lemma 21, we know that β(cid:98) W⊤ k,λΣ 0β(cid:98)W ℓ,λ−β(cid:98)I,µΣ 0β(cid:98)I,µ −a −. →s. 0 when
k ̸= ℓ since W and W are independent. Hence, we have
k ℓ
M
R(β(cid:98)W1:M,λ) ≃ M1
2
(cid:88) (cid:0) β(cid:98) W⊤ m,λΣ 0β(cid:98)Wm,λ−β(cid:98) I⊤ ,µΣ 0β(cid:98)I,µ(cid:1) +(β(cid:98)I,µ−β 0)⊤Σ 0(β(cid:98)I,µ−β 0)+σ 02
m=1
≃ M1 (cid:0) β(cid:98) W⊤ ,λΣ 0β(cid:98)W,λ−β(cid:98) I⊤ ,µΣ 0β(cid:98)I,µ(cid:1) +(β(cid:98)I,µ−β 0)⊤Σ 0(β(cid:98)I,µ−β 0)+σ 02
= M1 (cid:0) β(cid:98) W⊤ ,λΣ 0β(cid:98)W,λ−β(cid:98) I⊤ ,µΣ 0β(cid:98)I,µ(cid:1) +R(β(cid:98)I,µ) (35)
where we used the fact that the M terms where k = ℓ converge identically in the second to last
line and a risk decomposition similar to that for β(cid:98)W1:M,λ in the last line. Thus, it suffices to
evaluate the difference β(cid:98) λ⊤Σ 0β(cid:98)λ−β(cid:98) µ⊤Σ 0β(cid:98)µ to finish the proof.
We have
β(cid:98) W⊤ ,λΣ 0β(cid:98)W,λ−β(cid:98) I⊤ ,µΣ 0β(cid:98)I,µ
= (y⊤W⊤/n)WΦ(Φ⊤W⊤WΦ/n+λI )†Σ (Φ⊤W⊤WΦ/n+λI )†Φ⊤W⊤(Wy/n)
p 0 p
−(y⊤Φ/n)(Φ⊤Φ/n+µI )†Σ (Φ⊤Φ/n+µI )†(Φ⊤y/n)
p 0 p
= tr[W⊤WΦ(1Φ⊤W⊤WΦ+λI )†Σ (1Φ⊤W⊤WΦ+λI )†Φ⊤W⊤W/n·(yy⊤)]
n p 0 n p
−tr[Φ(Φ⊤Φ/n+µI )†Σ (Φ⊤Φ/n+µI )†Φ⊤/n·(yy⊤)]
p 0 p
≃ tr[(ΦΦ⊤/n+µI )†(ΦΣ Φ⊤/n+µ′ I )(ΦΦ⊤/n+µI )†(yy⊤)]
n 0 Σ0 n n
−tr[Φ(Φ⊤Φ/n+µI )†Σ (Φ⊤Φ/n+µI )†Φ⊤/n·(yy⊤)]
p 0 p
= tr[(ΦΦ⊤/n+µI )†(ΦΣ Φ⊤/n)(ΦΦ⊤/n+µI )†(yy⊤)]
n 0 n
+µ′ tr[(ΦΦ⊤+µI )†(yy⊤)(ΦΦ⊤+µI )†]
Σ0 n n
−tr[(ΦΦ⊤/n+µI )†(ΦΣ Φ⊤/n)(ΦΦ⊤/n+µI )†(yy⊤)]
n 0 n
= µ′ tr[(ΦΦ⊤+µI )†(yy⊤)(ΦΦ⊤+µI )†], (36)
Σ0 n n
where in the third line, we used the second-order equivalence for freely weighted ridge resolvents
from Lemma 22; in the fourth line, we employed the push-through identity multiple times.
Substituting for µ′ from Lemma 22 in (36) and substituting this back into (35), we arrive at
Σ0
the desired decomposition. This completes the proof.
32C.2 Proof of Proposition 7
We use the path (4) with k∗ and µ∗, and setting λ∗ = 0:
(cid:32) (cid:33)
df(β(cid:98)I,µ∗)
(cid:18) k∗(cid:19)
1− = 1− .
n n
This suggests that
k∗ = df(β(cid:98)I,µ∗).
Note that r := rank(X⊤X) = rank(G ). By the definition of degrees of freedom, it follows that
I
df(β(cid:98)I,µ∗) = tr[X⊤X(X⊤X +µ∗I p)†]
r
(cid:88) s i
= ≤ r = rank(G ),
s +µ∗ I
i
i=1
where s ,...,s are non-zero eigenvalues of X⊤X. This finishes the proof.
1 r
C.3 Technical lemmas
Recall from Appendix B.6 that we define λ = −liminf λ+ (WΦΦ⊤W⊤/n).
0 n→∞ min
Lemma 22 (General second-order equivalence for freely weighted ridge resolvents). Under the
settings of Lemma 21, for any positive semidefinite Σ with uniformly bounded operator norm,
0
for all λ > λ ,
0
1W⊤WΦ(1Φ⊤W⊤WΦ+λI )†Σ (1Φ⊤W⊤WΦ+λI )†Φ⊤W⊤W
n n p 0 n p
≃ (1ΦΦ⊤+µI )†(1ΦΣ Φ⊤+µ′ I )(1ΦΦ⊤+µI )†, (37)
n n n 0 Σ0 n n n
where µ′ ≥ 0 is given by:
Σ0
µ′ = −∂µ λ2S′ (cid:0) − 1 tr(cid:2)1ΦΦ⊤(1ΦΦ⊤+µI )†(cid:3)(cid:1)
Σ0 ∂λ WW⊤ n n n n
· 1 tr(cid:2) (1ΦΦ⊤+µI )†(1ΦΣ Φ⊤)(1ΦΦ⊤+µI )†(cid:3) . (38)
p n n n 0 n n
Proof. We use the Woodbury matrix identity to write
1WW⊤Φ(1Φ⊤WW⊤Φ+λI )†Σ (1Φ⊤WW⊤Φ+λI )†Φ⊤WW⊤
n n p 0 n p
= 1W(1W⊤ΦΦ⊤W +λI )†W⊤ΦΣ Φ⊤W(1W⊤ΦΦ⊤W +λI )†W⊤.
n n m 0 n m
The equivalence in (37) and the inflation parameter in (38) now follow from the second-order
result for feature sketch by substituting W for S, Φ for Φ⊤, and 1ΦΣ Φ⊤ for Σ in (18).
n 0 0
33D Additional illustrations for Section 3
D.1 Implicit regularization paths for bootstrapping
Degrees of freedom Gaussian
0.0
0.040
0.035
800
0.030
0.1
600 0.025
0.020
400
1
0.015
0.010
200
0.005
10
0.0 0.1 1 0.0 0.1 1
Subsample ratio k/n Subsample ratio k/n
Figure5: Equivalence underbootstrapping. Theleft panelshows theheatmap ofdegrees offreedom,and
therightpanelshowstherandomprojectionE W[a⊤β(cid:98)W,λ]wherea∼N(0 p,I p/p).Inbothheatmaps,the
redlinesindicatethepredictedpathsusingEquation(4),andtheblackdashedlinesindicatetheempirical
paths obtained by matching empirical degrees of freedom. Despite the complexity of the theoretical path
forbootstrapping,weobservethattheempiricalpathscloselyresembleit.Therefore,thetheoreticalpath
for sampling without replacement from (4) serves as a good approximation.
D.2 Implicit regularization paths with non-uniform weights
Degrees of freedom Gaussian
0.0
35 0.012
30 0.010
0.1
25
0.008
20
0.006
1 15
0.004
10
0.002
5
10 0.000
0.0 0.1 1 0.0 0.1 1
Subsample ratio k/n Subsample ratio k/n
Figure 6: Equivalence under non-uniform weighting. The left panel shows the heatmap of degrees of
freedom, and the right panel shows the random projection E W[a⊤β(cid:98)W,λ], where a ∼ N(0 p,I p/p). The
weights (diag(W)) for observations are initially generated as (9/10)i for i = 0,...,n−1, subsample k
entries from {1,...,n}, zero out the other n−k entries, and then normalized to have norm k. The black
dashed lines indicate the empirical paths obtained by matching the empirical degrees of freedom.
34
ytlanep
egdiR
ytlanep
egdiRE Additional illustrations for Section 4
E.1 Rate illustration for ensemble risk against ensemble size
=0 =0.01 =0.1
10 Prediction risk Prediction risk Prediction risk
Estimate Estimate Estimate
8
6
4
2
M=1 M=20 M=100 M=1 M=20 M=100 M=1 M=20 M=100
1 1/M
Figure 7: Risk equivalence for random feature structures when sampling without replacement. The solid
lines represent the prediction risks and their estimates of the subsample ridge ensemble, and the red
dashed lines indicate the prediction error of the full ridge predictor. The data and random features with
the ReLU activation function are generated according to Appendix F.1 with n=5000 and p=500. The
regularization level for the full ridge is set as µ = 1, and each subsampled ridge ensemble is fitted with
M =100randomlysampledsubsamplingmatrices.Foreachvalueofλ,thesubsampleratioisdetermined
by solving Equation (4).
E.2 Real data illustrations for implicit regularization paths
Degrees of freedom Training error Prediction error
0.0 500 0.450
0.0018
0.425
400 0.0016
0.400
300 0.0014 0.375
0.1
0.350
0.0012 200
0.325
1 0.0010
100 0.300
0.0008 0.275
10
0.001 0.01 0.1 1 0.001 0.01 0.1 1 0.001 0.01 0.1 1
Subsample ratio k/n Subsample ratio k/n Subsample ratio k/n
Figure 8: Equivalence in pretrained features of pretrained ResNet-18 on CIFAR-10 dataset.
Degrees of freedom Training error Prediction error
0.0
140 0.40
120 0.39 0.08
100
0.38 0.07
80
0.1
0.37
60 0.06
1 40 0.36 0.05
20 0.35
10 0.04
0.001 0.01 0.1 1 0.001 0.01 0.1 1 0.001 0.01 0.1 1
Subsample ratio k/n Subsample ratio k/n Subsample ratio k/n
Figure 9: Equivalence in features of randomly initialized ResNet-18 on Fashion-MNIST dataset.
35
eulaV
ytlanep
egdiR
ytlanep
egdiRF Details of experiments
F.1 Simulation details
The simulation settings are as follows.
• Covariance model. The covariance matrix of an auto-regressive process of order 1 (AR(1))
is given by Σ ∈ Rd×d, where (Σ ) = ρ|i−j| for some parameter ρ ∈ (0,1). For the
ar1 ar1 ij ar1 ar1
simulations, we set ρ = 0.25.
ar1
• Signal model. Define β = 1 (cid:80)5 w where w is the eigenvector of Σ associated with
0 5 j=1 (j) (j) ar1
the top jth eigenvalue r .
(j)
• Response model. We generated data (x ,y ) ∈ Rd × R for i = 1,...,n from a nonlinear
i i
model:
y = x⊤β + 1 (∥x ∥2−tr[Σ ])+ε , x = Σ1 2 z , z i ∼id t 5 , ε ∼ t 5 , (M-AR1)
i i 0 p i 2 ar1 i i ar1 i ij σ i σ
5 5
(cid:112)
where σ = 5/3 is the standard deviation of t distribution.
5 5
The benefit of using the above nonlinear model is that we can clearly separate the linear and
the nonlinear components and compute the quantities of interest because β happens to be the
0
best linear projection.
The linear, random, and kernel features are generated as follows.
• Linear features. For a given feature dimension p, we use d = p raw features from (M-AR1)
as linear features.
• Random features.Forgeneratingrandomfeatures,weused = 2prawfeaturesfrom(M-AR1)
and sample a randomly initialized weight matrix F ∈ Rp×d whose entries are i.i.d. samples
from N(0,d−1/2). Then the transform feature is given by x = φ(Fx ) ∈ Rp, where φ is a
(cid:101)i i
nonlinear transformation and set to be ReLU function in our experiment.
• Kernel features. For kernel features, we use d = p raw features from (M-AR1) to construct
the kernel matrix.
Inthesimulations,theestimatesareaveragedacross20simulationswithdifferentrandomseeds.
F.2 Experimental details in Section 4.3
Following the similar experimental setup in [20], we use residual networks to extract features
on several computer vision datasets, both at random initialization and after pretraining. More
specifically, we consider ResNet-{18, 34, 50, 101} applied to the CIFAR-{10,100} [9], Fashion-
MNIST [21], Flowers-102 [14], and Food-101 [4] datasets. All random initialization was done
following [8]; pretrained networks (obtained from PyTorch) were pretrained on ImageNet, and
the outputs of the last pretrained layer on each dataset mentioned above were used as the
embedding feature Φ.
After obtaining the embedding features from the last layer of the neural network model, we
further normalize each row of the pretrained feature to have a norm of p, and center the one-hot
labels to have zero means. To reduce the computational burden, we only consider the first 10
one-hot labels of all datasets. For datasets with different data aspect ratios, we stratify 10% of
36the training samples as the training set for the CIFAR-100 dataset. The training and predicting
errors are the mean square errors on the training and test sets, respectively, aggregated over all
the labels.
Table 2: Summary of pretrained features from different real datasets.
Number of
Number of Number of
Dataset Model pretrained
train samples test samples
features
ResNet-18
Fashion-MNIST 60000 10000 512
init.
ResNet-18
CIFAR-10 50000 10000 512
pretr.
ResNet-50
CIFAR-100 (subset) 5000 10000 2048
pretr.
ResNet-50
Flowers-102 2040 6149 2048
pretr.
ResNet-101
Food-101 75750 25250 2048
pretr.
References
[1] Adlam, B., Levinson, J. A., and Pennington, J. (2022). A random matrix perspective on
mixtures of nonlinearities in high dimensions. In International Conference on Artificial
Intelligence and Statistics.
[2] Adlam, B. and Pennington, J. (2020). The neural tangent kernel in high dimensions: Triple
descent and a multi-scale theory of generalization. In International Conference on Machine
Learning.
[3] Bose, A. (2021). Random Matrices and Non-Commutative Probability. CRC Press.
[4] Bossard, L., Guillaumin, M., and Van Gool, L. (2014). Food-101–mining discriminative
components with random forests. In Computer Vision–ECCV 2014: 13th European Con-
ference. Springer.
[5] Dobriban,E.andSheng,Y.(2020). Wonder:Weightedone-shotdistributedridgeregression
in high dimensions. Journal of Machine Learning Research, 21(66):1–52.
[6] Dobriban, E. and Sheng, Y. (2021). Distributed linear regression by averaging. The Annals
of Statistics, 49(2):918–943.
[7] Hastie, T., Montanari, A., Rosset, S., and Tibshirani, R. J. (2022). Surprises in high-
dimensional ridgeless least squares interpolation. The Annals of Statistics, 50(2):949–986.
[8] He, K., Zhang, X., Ren, S., and Sun, J. (2015). Delving deep into rectifiers: Surpassing
human-level performance on imagenet classification. In International Conference on Com-
puter Vision.
37[9] Krizhevsky,A.andHinton,G.(2009). Learningmultiplelayersoffeaturesfromtinyimages.
[10] Lee, D., Moniri, B., Huang, X., Dobriban, E., and Hassani, H. (2023). Demystifying
disagreement-on-the-line in high dimensions. In International Conference on Machine
Learning.
[11] LeJeune, D., Patil, P., Javadi, H., Baraniuk, R. G., and Tibshirani, R. J. (2024). Asymp-
totics of the sketched pseudoinverse. SIAM Journal on Mathematics of Data Science,
6(1):199–225.
[12] Mel, G. and Pennington, J. (2021). Anisotropic random feature regression in high dimen-
sions. In International Conference on Learning Representations.
[13] Mingo, J. A. and Speicher, R. (2017). Free Probability and Random Matrices, volume 35.
Springer.
[14] Nilsback, M.-E. and Zisserman, A. (2008). Automated flower classification over a large
number of classes. In Indian Conference on Computer Vision, Graphics and Image Pro-
cessing.
[15] Patil, P. and Du, J.-H. (2023). Generalized equivalences between subsampling and ridge
regularization. Advances in Neural Information Processing Systems.
[16] Patil, P., Du, J.-H., and Kuchibhotla, A. K. (2023). Bagging in overparameterized learning:
Risk characterization and risk monotonization. Journal of Machine Learning Research,
24(319):1–113.
[17] Patil, P. and LeJeune, D. (2024). Asymptotically free sketched ridge ensembles: Risks,
cross-validation, and tuning. In International Conference on Learning Representations.
[18] Sahraee-Ardakan, M., Emami, M., Pandit, P., Rangan, S., and Fletcher, A. K. (2022).
Kernel methods and multi-layer perceptrons learn linear models in high dimensions. arXiv
preprint arXiv:2201.08082.
[19] Voiculescu, D. V. (1997). Free Probability Theory. American Mathematical Society.
[20] Wei, A., Hu, W., and Steinhardt, J. (2022). More than a toy: Random matrix models
predict how real-world neural representations generalize. In International Conference on
Machine Learning.
[21] Xiao, H., Rasul, K., and Vollgraf, R. (2017). Fashion-mnist: a novel image dataset for
benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747.
38