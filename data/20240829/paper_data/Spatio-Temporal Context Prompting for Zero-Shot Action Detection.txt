Spatio-Temporal Context Prompting for Zero-Shot Action Detection
Wei-JheHuang1 Min-HungChen2 Shang-HongLai1
1NationalTsingHuaUniversity,Taiwan 2NVIDIA
Abstract
Person token Context token
Spatio-temporalactiondetectionencompassesthetasks
of localizing and classifying individual actions within a
video. Recent works aim to enhance this process by in-
corporating interaction modeling, which captures the re-
lationship between people and their surrounding context.
However,theseapproacheshaveprimarilyfocusedonfully-
supervised learning, and the current limitation lies in the
Person-Context Interest Token
lack of generalization capability to recognize unseen ac-
Interaction Spotting
tion categories. In this paper, we aim to adapt the pre-
trained image-language models to detect unseen actions.
To this end, we propose a method which can effectively
leverage the rich knowledge of visual-language models to talk to
talk to
performPerson-ContextInteraction. Meanwhile,ourCon-
hold
text Prompting module will utilize contextual information stand push
topromptlabels,therebyenhancingthegenerationofmore
Context Prompting
representativetextfeatures. Moreover,toaddressthechal- Person-Context
lenge of recognizing distinct actions by multiple people at relational token
the same timestamp, we design the Interest Token Spotting
mechanism which employs pretrained visual knowledge to
find each person’s interest context tokens, and then these
Figure1.Overviewofourmethod.Weaimtotransfertheknowl-
tokens will be used for prompting to generate text features
edgeofCLIPtodetectunseenactions.Weleveragethepretrained
tailoredtoeachindividual. Toevaluatetheabilitytodetect
knowledgetomodeltheinteractionbetweenpeopleandtheirsur-
unseenactions,weproposeacomprehensivebenchmarkon
roundingcontext.Besides,theInterestTokenSpottingmechanism
J-HMDB,UCF101-24,andAVAdatasets. Theexperiments utilizestheknowledgetofindthetokensmostrelevanttoaperson,
showthatourmethodachievessuperiorresultscomparedto then the Context Prompting uses these visual tokens to augment
previousapproachesandcanbefurtherextendedtomulti- thetextcontents,whichcanmakethemeasiertodistinguish.
actionvideos,bringingitclosertoreal-worldapplications.
ThecodeanddatacanbefoundinST-CLIP.
rating attention-based relation modeling [4,19,24]. These
approachesaimtomodeltherelationshipbetweenindividu-
1.Introduction alsandtheirsurroundingenvironment,includingotherpeo-
ple,objects,andthecontextualscene. Byintegratingmore
The task of spatial-temporal action detection is to de- interactioninformationintothepersonfeature,amorecom-
tect people and recognize their respective actions in both prehensiverepresentationoftheiractionsisachieved, con-
space and time, which holds broad applications in vari- sequently enhancing the accuracy of action classification.
ous fields, including self-driving cars, sports analysis, and However, these methods primarily center around fully su-
surveillance. Recently, the rise of 3D CNN backbones pervisedlearning,limitingtheircapabilitytodetectonlythe
[5,25,26]hasstrengthenedthecapabilitiesofrepresentation actionclassesincludedinthetrainingphase. Inreal-world
learninginspatial-temporalcontext,whichhasgreatlyim- applications, numerousactionsbeyondthetrainingclasses
proved the performance of action detection. Furthermore, are bound to occur. Therefore, our approach aims to push
some recent studies have extended their focus by incorpo- theboundariesfurtherbydetectingunseenactionsinzero-
1
4202
guA
82
]VC.sc[
1v69951.8042:viXrashot scenarios, alleviating the considerable labor-intensive ofdetectingtheseunseenactions. Theexperimentalresults
effortsassociatedwiththeannotationprocess. demonstrate that, in comparison to other zero-shot video
Inrecentyears,visual-languagemodels[11,20,33]have classification methods, our approach exhibits superior per-
gradually become the models of choice in zero-shot video formanceonJ-HMDBandachievescompetitiveresultson
understandingduetotheirstronggeneralizationcapability. UCF101-24. Furthermore, experiments on AVA demon-
Nevertheless, thepredominantfocusinthisdomainiscur- stratethatourmethodcandetectvariousunseenactionsin-
rentlyonvideoclassification[12,18,27,28,30]andtemporal dividuallywithinthesamevideo,affirmingitspotentialex-
action detection [12,17], where the entire video or a short tensiontoreal-worldapplications. Tosummarize,ourcon-
clipisconsideredforactionclassification.[9]ismostsimi- tributionsareasfollows:
lartoourgoal,whichistodetectindividualunseenactions.
• WeproposeanovelmethodST-CLIPthatfullylever-
However,thescenariotheyhandleistoosimple. Theyonly
agesthevisual-languagemodeltocapturetherelation-
process videos with single actions and target specific un-
shipbetweenpeopleandthespatial-temporalcontext,
seenlabels,limitingtheirabilitytoeffectivelyevaluatethe
withouttrainingextrainteractionmodules.
robustnessofthemethod. Incontrast, ourgoalistodetect
avarietyofunseenactionsandextendthemethodtovideos
• We devise a multi-layer Context Prompting module
containingmultipleactions.
thatemploysbothlow-levelandhigh-levelcontextin-
Towards the aforementioned goal, we propose a novel
formationtopromptclassnames,enrichingtheseman-
framework called ST-CLIP, which adapts CLIP [20] to
ticcontent.Inaddition,weintroduceanInterestToken
Zero-ShotSpatio-TemporalActionDetectioninbothvisual
Spottingmechanismtoidentifytokensmostrelevantto
and textual aspects, as shown in Figure 1. In terms of vi-
individualsforprompting,therebygeneratingtextfea-
sion,weproposetoutilizethevisualknowledgeembedded
turesthatareuniquetoeachperson.
in CLIP to perform Person-Context Interaction. This ap-
proach enables us to grasp the relationship between indi-
• We propose a complete benchmark on J-HMDB,
vidualsandtheirsurroundingcontextwithoutthenecessity
UCF101-24andAVAdatasetstoevaluateperformance
for additional interaction modules, thereby preserving the
onZero-ShotSpatio-TemporalActionDetection. The
generalizationcapabilitiesofCLIPandstreamliningthein-
experimentsdemonstratethestronggeneralizationca-
teraction modeling process. In the textual domain, given
pabilitiesofourmethod, andshowtheabilitytoindi-
that the class names in the dataset offer limited semantic
viduallydetectunseenactionswithinthesamevideo.
information, the ambiguity between different labels may
degrade the quality of the classification results. Our ob-
2.RelatedWork
jective is to enhance the textual content through effective
prompting. Inspiredby[18],wedesignamulti-layerCon- Spatio-Temporal Action Detection. Typical action de-
textPromptingmodule,whichincrementallyutilizesvisual tection methods mostly use the two-stage pipeline, which
cluesfromspatio-temporalcontexttoaugmenttextdescrip- meansfirstlocalizingpeopleinavideo,andthenperform-
tions,therebyincreasingthediscriminationcapability. Fur- ingactionclassificationbasedonthefeaturesofthesepeo-
thermore,giventhatreal-worldscenariosofteninvolvemul- ple. Most of these methods utilize additional person de-
tipleindividualsconcurrentlyperformingdifferentactions, tectors like Faster R-CNN [21] to generate actor bound-
wefurtherintroduceInterestTokenSpotting,whichaimsto ing boxes, which are used to perform RoIAlign [8] on the
identify context tokens most relevant to each person’s ac- video features generated by the 3D CNN backbone to ob-
tions.Subsequently,thesetokensareutilizedintheprompt- tain the person features. While [5] directly utilizes naive
ingprocesstogenerateadescriptionthataptlycaptureseach actor features to classify actions, [4,19,24] further exploit
individual’ssituation. relation modeling to combine more information about hu-
In order to assess the effectiveness of our method, we man and environmental interactions. Besides, some meth-
referto[9]andproposeamorecompletebenchmarkonJ- ods[1,6,23,31]optimizethetwo-stagenetworksbyajoint
HMDB, UCF101-24 and AVA datasets. For the first two lossinanend-to-endframework. Recently,therearequery-
datasets,weconductcross-validationwithvaryingtrain/test based detectors that simultaneously predict the actor posi-
label combinations. This approach, as opposed to [9], tionsandtheircorrespondingactionsinasingle-stageman-
which exclusively experiments with a specific label split, ner.[35]utilizesasetoftubelet-queriestosamplefeatures
providesamorecomprehensiveassessmentofthemethod’s from video representation, while [29] samples discrimina-
robustness. ForexperimentsonAVA,whereasinglevideo tive features based on the multi-scale feature map. Al-
may involve multiple actions, we randomly select certain though the above works have made significant progress in
videosthatlackcommonclassesfortraining,thenthesub- traditional fully-supervised scenarios, our goal is to better
sequentevaluationwillfocusonassessingtheperformance recognizeunseenactionsthroughzero-shotlearning.
2Video Understanding with Visual-Language Models. 3.2.OverallArchitecture
Recently,large-scalevisual-languagemodelssuchasCLIP
[20], ALIGN [11], and Florence [33] have demonstrated TheoverviewofourST-CLIPisshowninFigure2. Asa
their usability to different visual-language tasks including two-stage framework, our model takes the person detected
image captioning [16], video-text retrieval [3] and scene from video frames by a human detector as input and out-
textdetection[32]. Duetoasharedfeaturespacethateffec- puts the corresponding action classification results. Given
tivelyalignsthevisualandtextdomains,anincreasingnum- an image with the person bounding boxes, we first extract
berofmethods[12,18,27,28,30]choosetoperformzero- theseportionsandobtainperson-specifictokensthroughthe
shotvideoclassificationbasedonthesefoundationmodels. image encoder. In this process, we utilize the adapter to
Themainfocusoftheseworksistodesigntemporalmod- makethesepersontokensmoresuitableforsubsequentin-
eling to adapt the image encoder to the video domain and teraction modeling, which will be discussed later. Given
todevelopwaystopromptthetext. Onthisbasis,[9]pro- thecontinuousnatureofactions,inadditiontoconsidering
cesseszero-shotspatio-temporalactiondetectiontofurther a person’s information, we sample neighboring frames to
subdivide the scope of action classification to the individ- construct a spatial-temporal context, thereby capturing in-
ual level. They proposed extracting people and objects in formationacrossdifferentspacesandtimes.Toobtainthese
the image and using different interaction blocks to model contexttokens,weconducttemporalmodelingonthepatch
therelationshipbetweenthem. Besides,theywilluseeach tokensofdifferentframes,enablingthetokenstoaggregate
person’s interaction feature to prompt labels. To evaluate informationoverthisperiodoftime.
their performance, they selected specific unseen actions to
detectonthetwodatasets,J-HMDBandUCF101-24.How- Subsequently, to fully leverage CLIP’s visual knowl-
ever,thisbenchmarkisstillnotcloseenoughtoreal-world edge, we jointly input person and context tokens into the
scenarios.First,theydidnotextensivelytestavarietyofun- image encoder. The Multi-Head Self Attention (MHSA)
seenlabels.Second,thevideosinbothdatasetscontainonly ineachencoderlayerisemployedtoguideusinachieving
single actions. Considering this, we propose a more com- the following three objectives: (i) performing further spa-
plete benchmark to evaluate performance on multi-action tialmodelingontheinputcontexttokenstoobtainspatial-
videos,aimingformorepracticalapplications. temporal tokens. (ii) modeling person-person and person-
context interaction through the mutual influence between
tokens. (iii) identifying the interest tokens most relevant
3.ProposedMethod to each person’s actions through attention weight. On the
textual side, we initially utilize the CLIP text encoder to
3.1.Preliminary: Visual-LanguageModel generate the original text features for class names. Then,
followedbyeachimageencoderlayer,theContextPrompt-
Asourmethodexploitsthepretrainedknowledgeofthe ing layer will use context tokens to prompt each label. In
CLIP model, we briefly review the formulation of image this process, considering that the videos in J-HMDB and
andtextencodersinthissection. Fortheimageencoderof UCF101-24onlycontainsingleaction,wecantreatallcon-
ViT architecture [2], given an image I ∈ RH×W×3 with texttokensasrelevanttothisaction. Hence,weuseallcon-
height H and width W, it will be split into N = H P × W P texttokensforprompting,resultingineveryoneinthesame
patches,wherethepatchsizeisP×P,theneachpatchwill
frame sharing the same text features. However, in AVA,
obtainitstokenthroughpatchembedding. Inthisprocess, todiscerndifferentactionsbymultipleindividuals,weuti-
theConv2DwithkernelsizeP×P andoutputchannelsize lize context tokens that each person deems important (i.e.,
D willbeusedtogeneratepatchtokensx ∈ RN×D. After interesttokens)toprompttheirrespectivetextfeatures. Fi-
that,anextralearnabletokenx
cls
∈RDisconcatenatedfor
nally,weusethepersontokensandlabelfeaturesoutputby
classification. Theinputtokensforthetransformerencoder the last image encoder layer and Context Prompting layer
layerisgivenby: to calculate the cosine similarities, which are used as the
actionclassificationscores.
X =[x ,x ,x ,...,x ]+e (1)
cls 1 2 N Inthetrainingprocess,inordertoretainthegeneraliza-
tion capability of CLIP for zero-shot tasks, we freeze the
whereeisthepositionalencoding. Theclassificationtoken pretrainedweightsintheimageandtextencoders,andonly
x outputfromthelastencoderlayerisoftenregardedas train our additional learnable modules. Besides, we insert
cls
animagefeature. Similarly,thetextencoderisalsoatrans- the LoRA trainable matrices into the Feed-Forward Net-
formerarchitecture,whileitfirsttokenizesthetextintoem- work (FFN) of each image encoder layer, which can fur-
beddings, and then uses the EOS token of the last encoder ther adapt CLIP model to detect actions without affecting
layeroutputasthetextfeature. itswell-alignedvisual-languagefeatures.
3current frame
crop Image adapter person Frozen
Encoder tokens
Tuned
neighboring frames
Target person
temporal context
modeling tokens Interest tokens
... ...
sit hold write
Text Encoder
context person
tokens tokens ... ...
query
context tokens person tokens Kc h too ko es ne s Linear vk ae luy/ e Context Prompting
projector 1 layer 1
Image Encoder layer 1
query
context tokens person tokens choose key/
K tokens Linear value Context Prompting
projector 2 layer 2
Image Encoder layer 2
context tokens person tokens choose key/
K tokens Linear value Context Prompting
projector L layer L
Image Encoder layer L
person-context Cos Similarity ... ... enhanced
text representations
relational tokens
Figure2. ST-CLIPframework. Wefirstextractthepersontokensforthepersonboundingboxesdetectedfromeachframe. Then,we
performtemporalmodelingontheneighboringframestoobtainthecontexttokens. Afterthat,weleveragetheCLIP’svisualknowledge
toperformperson-contextinteractiononthesetokens.Inaddition,weutilizetheattentionweightineachencoderlayertofindtheinterest
tokens for each person, then the Context Prompting layer will use these visual tokens to prompt the class names. Finally, the cosine
similaritiesbetweenperson-contextrelationaltokensandthelabelpromptingfeaturesdeterminetheclassificationscoresfortheactions.
3.3.Person-ContextInteraction notes the frame index, N is the number of patch tokens,
and D is the token dimension. Then we gather the tokens
As mentioned earlier, to utilize spatial-temporal con- of each frame into [X 1,X 2,...,X T] ∈ RT×N×D. After
text and facilitate the recognition of continuous actions, that, we utilize MHSA to model the relationship between
we sample T neighboring frames before and after the cur- patchtokensatthesamepositionindifferentframesZ i =
rent frame. We first conduct temporal modeling on these [x 1,i,x 2,i,...,x T,i] ∈ RT×D, where i ∈ {1,...,N}, as
frames to consolidate information at different times. Sub- follows:
Z¯ =Z +etemp
sequently, we leverage the spatial modeling capability of i i
the image encoder to further fuse these visual contents in Zˆ =Z¯ +MHSA(LN(Z¯)) (2)
i i i
both space and time, which results in the generation of
Z˜ =AvgPool(Zˆ),
spatial-temporal tokens. Our temporal modeling is shown i i
in Figure 3. First, we use CLIP’s pretrained patch em- where etemp is the temporal encoding and LN stands
bedding to obtain the patch tokens of each frame X = for layer normalization. After temporal modeling, we
t
[x ,x ,...,x ] ∈ RN×D, where t ∈ {1,...,T} de- can obtain context tokens Z˜ ∈ RD at each position
t,1 t,2 t,N i
4ken i. Based on this, we use a person’s token as the row
frame 1 frame 2 frame T
index,andselectthetopKhighestamongallCimportance
average pooling scores. These selected K indexes are the token positions
MHSA
thatthepersonisinterestedin. Afterthat,wepasseachto-
kenthroughtheMHSAandFFNofthisencoderlayer,and
patch
embedding usetheselectedK indexestoobtaintheinteresttokens.
3.5.ContextPrompting
Figure3. Temporalmodeling. Weapplyself-attentionalongthe
The Context Prompting layer primarily consists of
temporaldimensiontofusetheinformation.
Cross-Attention (CA), where text features serve as the
query, and the context tokens act as key and value. This
i∈{1,...,N}thathaveaggregatedtemporalinformation, setup facilitates the gradual absorption of visual informa-
whichwillbeusedtomodeltheinteractionwithperson. tionintothetextcontent. InAVA,wefurthernarrowdown
Regardingthepersontokens,weinitiallyutilizetheim- the scope of the extracted information for each person to
ageencodertoobtainfeaturesforeachperson. Considering their personal interest tokens. Given an image with B de-
that these person features have undergone multiple trans- tectedindividuals,weneedtoclassifytheiractionsintoN
L
former encoder layers, they are relatively high-level com- possible labels. First, we assign the same set of original
paredtotheaforementionedcontexttokens,whichareonly text features which are obtained by the CLIP text encoder
atthepatch-embeddinglevel. Toenhancetheutilizationof to these B people. Subsequently, we employ a linear pro-
self-attention in modeling relationships among all tokens, jector to project each person’s interest tokens to the same
weemployanadaptertoadjustpersonfeaturestothesame dimensionasthetextfeatures. Followingthis,boththein-
level as other context tokens. The adapter is a straightfor- terest tokens and the text features are sent to the Context
wardtwo-layerFFNcommonlyusedinthetransformeren- Prompting layer for prompting. The following equations
coderlayer,andwehavefoundittobeeffectiveinadapting describehowitworks:
these person features. Specifically, we generate person to-
kensinthefollowingways: F¯ T =F T +CA(F T,F I),
Fˆ =F¯ +FFN(F¯ ), (4)
P˜ =P +FFN(LN(P )) (3) T T T
i i i
F˜ =F +ρFˆ ,
T T T
where P is the person feature output from the image en-
i
coderandP˜ iisthepersontokenwewilluseforsubsequent whereF
T
∈ RB×NL×D consistsofthetextfeatures,F
I
∈
interactionmodeling. RB×K×D are the interest tokens, ρ ∈ RD is a learnable
weight vector, and F˜ will be input to the next prompting
3.4.InterestTokenSpotting T
layer. ForJ-HMDBandUCF101-24,westraightforwardly
In a multi-action dataset like AVA, the same timestamp useallcontexttokensforprompting,makingallBindividu-
may encompass various actions performed by multiple in- alsinanimageutilizethesametextfeaturesF
T
∈RNL×D
dividuals, and all the context tokens will contain informa- forsimilaritycalculation.
tionaboutdifferentactions. Therefore,weintroduceInter-
est Token Spotting, a mechanism that employs a personal 4.DatasetsandBenchmarks
perspective to extract context tokens most relevant to each
individual’s actions. Subsequently, we utilize these tokens Weestablishbenchmarksforzero-shotspatial-temporal
forpromptingtogeneratepersonalizedtextfeatures. Inthis action detection on three popular datasets: J-HMDB,
process, we also leverage CLIP’s pretrained visual knowl- UCF101-24,andAVA.Forthefirsttwodatasets,wefurther
edge to find each person’s interest tokens. To be more extendthesettingsof[9]toincludemorediverseunseenac-
specific, we exploit the attention weight calculated by the tions. Besides, we also build benchmark on AVA which is
MHSA in each image encoder layer as an indicator of the morerepresentativeofreal-worldscenarios. Weuseframe
tokenimportance. TheMulti-HeadSelfAttentionwillfirst mAPwith0.5IoUthresholdinallthebenchmarksforeval-
calculateanattentionmapM ∈RC×C basedonthequery uation. Moredetailsaboutthelabelsplitsaredescribedin
i
andkeyofeachhead,wherei∈{1,...,num heads},and thesupplementarymaterials.
C isthenumberofinputtokens. Then, weaveragetheat-
4.1.ZS-JHMDB
tention maps of each head to obtain an importance score
matrix M ∈ RC×C. In this matrix, each row represents J-HMDB dataset [10] is a subset of the HMDB51
the importance of every token to a certain token. For in- dataset. It has 21 classes and 928 videos. The videos are
stance, M(i,j) represents how important token j is to to- trimmed and there are totally 31,838 annotated frames in
5these videos. To assess the generalization capability of an images have lower resolution, we further use the ground-
action detection method, the zero-shot evaluation necessi- truth of training classes to finetune the person detector for
tates that the model has not seen samples related to test 10 epochs in each label split. Besides, to remove the false
classesduringthetrainingprocess, whichmeansthetrain- positives,wekeepthedetectedboxwiththehighestconfi-
ingandtestinglabelsaredisjoint. Inthisscenario,werefer dent score S in each frame, then we select the boxes with
to the evaluation settings proposed by [9], which exploits scoreshigherthanS−T fromtherest,whereT is0.001for
random sampling to take 75% action classes for training, J-HMDBand0.7forbothUCF101-24andAVA.
andtheremaining25%fortesting. However,theyonlyem- Hyperparameters: For all the experiments on J-HMDB
ployaspecificlabelsplitforevaluation,whichisinadequate and UCF101-24, we employ ViT-B/16 as our CLIP back-
forfullymeasuringtheeffectivenessofthemethod.Instead, boneandusethesamehyper-parametersasfollows: Train-
weperformcross-validationonmultiplelabelsplitsasfol- ingfor3Kiterationswithbatchsizeof8. WeuseSGDas
lows: wesplitJ-HMDBinto4labelsplits,eachsplithas15 ouroptimizerandthebaselearningrateissetto2.5e-4. As
trainingclassesand6testingclasses,andthetestingclasses forAVA,weuseViT-L/14backboneandtrainthemodelfor
in each split are disjoint (part of split 4 will overlap with 20Kiterationswithabaselearningrateof4e-4.
split1). Thesplit1isthesameasthesplitusedin[9].
5.2.Zero-ShotSpatial-TemporalActionDetection
4.2.ZS-UCF
UCF101-24datasetisasubsetofUCF101[22]. Itcon- FramemAP@0.5
sistsof3207videosfrom24actionclasses,andeachvideo Method
split1 split2 split3 avg avg*
contains a single action. In this benchmark, we employ
the same setting as in ZS-JHMDB, which also divides all baseline 8.67 8.57 2.77 6.67 10.09
classes into 75% for training and 25% for evaluation. The baseline(personcrop) 8.22 5.05 3.04 5.44 7.55
labelsplit1isalsothesameasusedin[9]. iCLIP[9] 4.04 9.08 1.91 5.01 6.91
Vita-CLIP[28] 4.25 4.29 0.64 3.06 10.45
4.3.ZS-AVA ST-CLIP(Ours) 12.85 10.17 4.01 9.01 11.76
AVA [7] is a large-scale action detection dataset which
Table1.EvaluationonZS-AVA.*denotesusingtheground-truth
containsmultiple actionswithin asingle video. It consists
bounding boxes of the test data. All methods employ ViT-L/14
of235videosfortrainingand64videosforvalidation.Each
backbone. IntheinferencestageofVita-CLIP[28], weusetwo
videolastsfor15minutesandisannotatedbysamplingone
different tracklets: (1) tracklets obtained by associating detected
keyframepersecond. ForAVA,sincethesamevideocon-
boxes with ByteTrack [34], and (2) tracklets with ground-truth
tains multiple actions, and some action categories exist in boxesprovidedbyAVAofficial.
multiplevideos,itbecomeschallengingtofindasufficient
amountoftrainingdataandtestingdatawithdisjointlabels. In addition to using detected person boxes to measure
Instead, we randomly select some training videos, ensur- theactualresultsonzero-shotspatial-temporalactiondetec-
ing that they all lack samples of the same classes. These tion, we also provide results using ground-truth bounding
missingclassesarethentreatedasunseenclassesforevalu- boxesofthetestdata. Thisallowsustoanalyzetheperfor-
ation. Duringtheevaluationphase,wetestallclassesinthe mance of all methods without the influence of localization
validation videos, but the focus is solely on evaluating the errors. The results of each label split using ground-truth
performanceofunseenclasses. Underthissetting,wepro- boundingboxesareprovidedinthesupplementary.
posethreesplits. Whentheamountsoftrainingandtesting Table 1 shows our results on detecting unseen actions
dataforthesesplitsarenearlythesame,weselectdifferent inAVA.Themaindistinctionbetweenourmethodandthe
combinationsofthreeactiontypes—poseaction,objectin- video classification approaches is our capability to detect
teraction,andpersoninteractionasunseenclasses,allowing different unseen actions within the same video, which are
foramorecomprehensiveevaluation. difficulttoachieve. Firstly,inthetrainingprocessofthose
methods, providing a fixed video label is challenging due
5.Experiments
tothepresenceofnumerousdifferentactions. Additionally,
duringtheinferencestage,thesemethodstendtodetectthat
5.1.ExperimentalSetup
everyoneinthevideohasthesameaction. Tostudythefea-
PersonDetector:Inthefollowingexperiments,weemploy sibility of the video classification method, we further nar-
Faster R-CNN [13] with a ResNet-50-FPN [14] backbone rowed the scope of classification from the entire video to
pretrained on MSCOCO [15] for person detection. For J- tracklets to avoid misclassifying different actions into the
HMDBandAVA,wedirectlyinferenceonthetestdatawith same category. Besides, considering the limited prior re-
pretrainedpersondetector. WhileforUCF101-24,sincethe searchinthisarea,wefollow[9]toimplementanaivebase-
6lineforcomparisonwithourmethod. Foraframewithde- formance to [18]. With ground-truth bounding boxes, our
tectedindividuals,thebaselineutilizesthepretrainedimage methoddemonstratesthesecond-bestaverageperformance,
encoderofCLIPtoextracttheimagefeatureofthisframe. surpassing [18] and achieving results comparable to [28].
Subsequently, it calculates the cosine similarities with the TheresultsonZS-JHMDBandZS-UCFshowthatourST-
text features of each class name, which are then consid- CLIP can perform competitively with other video classifi-
eredastheactionclassificationscoresfortheseindividuals. cationmethodswhenprocessingsingle-actionvideos.
Sincethebaselineregardspeopleinthesameframeashav-
ing the same actions, we also implement the person crop FramemAP@0.5
method. This involves cropping out parts of each person Method
split1 split2 split3 split4 avg avg*
to obtain their respective image features for classification.
Nevertheless, this cropping method may result in reduced Withouttheassumptionofsingle-actionvideo
performanceforthebaselineinsplit1andsplit2,asitcap- baseline 64.63 71.04 82.13 75.88 73.42 81.21
turesinsufficientinformation.TheresultsshowthatourST- iCLIP[9] 66.53 69.99 82.88 71.84 72.81 79.01
CLIPachievesthebestaverageperformance,whetherusing ST-CLIP(Ours) 74.55 74.97 83.59 83.22 79.08 85.53
detectedboxesorground-truthboxes,demonstratingitsef-
Withtheassumptionofsingle-actionvideo
fectivenessindetectingindividualunseenactions.
ActionCLIP[27] 69.18 75.28 77.11 76.55 74.53 82.07
WepresenttheexperimentalresultsonZS-JHMDBand A5[12] 50.92 66.06 69.07 60.21 61.57 67.39
ZS-UCF in Tabs. 2 and 3. Firstly, since each video in X-CLIP[18] 72.91 72.62 80.02 77.78 75.83 83.11
both datasets contains only a single action, a straightfor- Vita-CLIP[28] 68.60 82.35 84.95 80.19 79.02 86.02
wardapproachistoconductactiondetectionthroughzero- ST-CLIP(Ours) 79.62 78.70 85.84 87.19 82.84 90.12
shotvideoclassification. Thesemethodscaninitiallyclas-
sify the entire video into an action class and then consider Table2.EvaluationonZS-JHMDB.*denotesusingtheground-
all detected individuals in the video as performing this ac- truthboxesofthetestdata.AllmethodsuseViT-B/16backbone.
tion. However,sinceourmethodconcentratesondetecting
different actions for each person, individuals in the same
videomaybeclassifiedintodifferentactions. Thisgeneral FramemAP@0.5
settingcanresultinothervideoclassificationmethodshav- Method
split1 split2 split3 split4 avg avg*
ing an advantage over us in these two datasets. For a fair
comparison with the other methods, we further adopt the Withouttheassumptionofsingle-actionvideo
assumption that a video contains only one action. In this baseline 48.37 52.84 39.76 51.92 48.22 90.49
context,weperformsoftvotingoneachperson’sclassifica- iCLIP[9] 50.34 52.75 39.73 47.95 47.69 85.47
tionscore,extendingourmethodtosuitthisscenario. ST-CLIP(Ours) 49.09 54.95 42.05 53.92 50.00 91.80
We first present the results on ZS-JHMDB in Table 2. Withtheassumptionofsingle-actionvideo
Regardless of whether the assumption is applied, our ST- ActionCLIP[27] 52.64 55.01 38.04 51.84 49.38 89.72
CLIP outperforms others in most label splits, and thus A5[12] 46.78 47.36 41.23 53.18 47.14 85.60
achieves the best average performance, along with 3.82 X-CLIP[18] 50.10 56.94 44.52 55.35 51.73 94.12
mAP higher than the state-of-the-art method [28]. Addi- Vita-CLIP[28] 52.52 57.22 45.12 57.32 53.05 96.57
tionally,byusingground-truthboundingboxestoeliminate ST-CLIP(Ours) 51.36 56.30 43.12 54.52 51.33 95.02
localization errors, our average performance improves to
90.12mAP,whichis4.1mAPhigherthan[28]. Table3.EvaluationonZS-UCF.*denotesusingtheground-truth
Table 3 presents the results on ZS-UCF. Firstly, with- boxesofthetestdata.AllmethodsuseViT-B/16backbone.
out the assumption, our method exhibits a 2.31 mAP im-
provement over [9], which also focuses on detecting in-
5.3.AblationStudy
dividual actions. It is worth mentioning that the localiza-
tionerrorshaveanoticeableimpactonourperformancein We present an ablation study in Table 4 to investigate
thisdataset. Sincethereareinstanceswhereirrelevantpeo- differentdesignchoicesinourmethod. Theexperimentin
ple who are not performing actions are detected, these in- Table 4b is performed on the split 1 of ZS-AVA. Tabs. 4c
dividuals should be considered as part of the background. to4gareconductedonthelabelsplit1ofZS-JHMDB,and
However, these false positive cases will also contribute to Table4aisperformedonboth.
thesoftvotingprocess,leadingtoourclassificationresults Proposedcomponents:Wefirstinvestigatetheimportance
being slightly inferior to other methods that solely rely on ofeachcomponentinTable4a. OnJ-HMDB,ourproposed
sampledframestodeterminevideolabels. Inthiscase,our Person-ContextInteractioneffectivelymodelstherelation-
methodstilloutperforms[12,27],andexhibitssimilarper- ship between individuals and their surroundings, resulting
7Components J-HMDB AVA Number mAP Method +prompt
Baseline 64.63 8.67 80 11.77
iCLIP[9]
66.02−+ −0 −.5 →1
66.53
+Person-ContextInteraction 66.98 10.41 100 12.85 +7.57
Ours 66.98−−−→74.55
+ContextPrompting 74.55 12.07 150 12.46
+InterestTokenSpotting - 12.85 200 11.46 (c)ComparisonwithiCLIP
(a)Proposedcomponents (b)Interesttokens
Rank mAP Persontokens mAP Temporalmodeling mAP Prompting mAP
w/oLoRA 72.80 Withouttheadapter w/otemporal 68.03 w/oprompting 66.98
r=2 64.52 Patch-embedding 70.92 averagepooling 68.63 onlyinlastlayer 66.02
r=4 71.70 Imageencoder 70.29 1-layerMHSA 74.55 ineverylayer 74.55
r=8 74.55 2-layerMHSA 72.06
Withtheadapter (g)Contextprompting
r=16 68.39
1-layerFC 67.37 (f)Contexttokens
(d)LoRAinFFN 2-layerFFN 74.55
(e)Persontokens
Table4.AblationStudy.WereportframemAPwith0.5IoUthresholdonthelabelsplit1ofZS-JHMDBandZS-AVA.
ina2.35mAPimprovementcomparedtothebaseline. Fur- equippingadaptercanperformbetterthanusingpersonfea-
thermore,theContextPromptingmoduleleveragescontext turesattheimageencoderlevel. Thisdemonstratesthatour
informationtoenhancetextcontent,leadingtoanadditional adapter can effectively adapt person tokens, making them
improvementof7.57mAP.OnAVA,theabovetwocompo- moresuitableforthesubsequentperson-contextinteraction.
nentsalsodemonstratetheireffectiveness. Additionally,in Contexttokens: Table4fshowstheimportanceoftempo-
multi-action videos, our Interest Token Spotting can iden- ral modeling. Using only the current frame to obtain con-
tify context tokens most relevant to individual actions for texttokensresultsintheworstperformance,indicatingthat
prompting,furtherenhancingperformance. aggregating temporal information is beneficial when iden-
Interesttokens:Intable4b,weconductexperimentsusing tifying continuous actions. Additionally, equipping only a
differentnumbersofinteresttokenstoobservetheirimpact 1-layerMHSAcansignificantlyimproveperformancecom-
on the results. Our findings indicate that in multi-action paredtosimpleaveragepooling.
videos,introducingtoomanytokenscanpotentiallysample Context prompting: The results in Table 4g show the ef-
backgroundnoiseunrelatedtotheaction,therebyimpacting fectiveness of our prompting strategy. The results demon-
theeffectivenessofprompting. strate that our prompting method, which utilizes tokens
from low-level to high-level, yields better outcomes com-
Comparison with iCLIP [9]: We demonstrate the advan-
paredtousingonlyhigh-leveltokensfromthelastlayer.
tages of our method compared to [9] in Table 4c. Without
prompting, our method performs slightly better than [9],
6.Conclusion
and we do not need to use additional object detectors and
interaction blocks in the interaction modeling process. In
In this paper, we explore zero-shot spatio-temporal ac-
addition,ourpromptingstrategyusesmultiplelevelsofcon-
tion detection. We propose a complete benchmark on
texttokenstoaugmenttextcontent,resultinginanimprove-
J-HMDB, UCF101-24 and AVA. Besides, we propose a
ment of 7.57 mAP. However, the prompting method of [9]
method to adapt the visual-language model for this task.
reliesheavilyontheresultsofinteractionmodeling,which
ThePerson-ContextInteractionemployspretrainedknowl-
limitstheirperformanceimprovement.
edge to model the relationship between people and their
LoRA in FFN: In Table 4d, we investigate the impact of surroundings, and the Context Prompting module utilizes
LoRA ranks. The results show that when we additionally the visual information to augment the text content. To ad-
trainlearnablematriceswithrank8,wecanperformbetter dress multi-action videos, we further introduce the Inter-
thanrelyingsolelyonCLIP’spretrainedweight. estTokenSpottingmechanismtoidentifythevisualtokens
Person tokens: Table 4e explores different ways of gen- most relevant to each individual action. The experiments
erating person tokens. Initially, the simple approach of demonstrate that our method achieves competitive perfor-
pooling over the embeddings of all patches in the person mancecomparedtoothervideoclassificationmethodsand
cropfailstodeliversatisfactoryperformance. Furthermore, canalsohandlemulti-actionvideos.
8SupplementaryMaterial ofbrushingteeth,whileanotherpersonundertakingdistinct
actionsishighlightedwithinthecircle. Inthisscenario,our
Inthesupplementarymaterial,wepresentadditionalexper-
InterestTokenSpottingmechanismcanidentifythecontext
imental results to substantiate the efficacy of our method
tokens most relevant to the action of brushing teeth. This
andprovidemoredetailsontheexperimentalsettings. Ini-
ensuresthatthepromptingprocessselectivelyincorporates
tially,weelaborateourproposedbenchmarkforZero-Shot
thesecrucialvisualclues,enhancingthefocusonpertinent
Spatio-TemporalActionDetectioninSec1. Subsequently,
information. Likewise, in Figure 4b, the individual high-
we showcase a visualization depicting interest tokens on
lightedasthetargetpersonisengagedinfishing,whilethe
ZS-AVAinSec. 2. Then,inSec. 3,weillustratethedistri-
personwithinthecircledareaisswimming.Inthisinstance,
butionoftextfeaturesonbothZS-JHMDBandZS-UCFto
theidentifiedinteresttokensexcludeinformationassociated
assesstheimpactofprompting. Wethenprovidethecom-
withswimming,consequentlyenhancingthequalityofthe
plexityanalysisofourmethodandothersinSec. 4,andthe
prompting process. Furthermore, in comparison to utiliz-
results using ground-truth bounding boxes on each bench-
ingallcontexttokensforprompting, theuseofonlyinter-
markinSec. 5. Finally,wediscusssomelimitationsofour
est tokens can elevate the confidence score for the ”brush
approachinSec. 6,andgivetheimplementationdetailsof
teeth” action from 0.34 to 0.45, and the score of ”fishing”
othermethodsinSec. 7.
canalsobeimprovedfrom0.30to0.34.Theresultsdemon-
stratethatevenwhendealingwithanunseenactionnoten-
1.LabelSplitsDetails
counteredduringthetrainingprocess,ourmethodexcelsin
Inthissection,weprovidedetailsofeachlabelsplitused identifyinginformationmostrelevanttothisactionwithina
inourbenchmarks. ForZS-JHMDBandZS-UCF,weper- multi-personenvironment.
formcross-validationon4labelsplitstoassesstheefficacy We provide more visualization results in Figure 5. In
ofourmethod. EachlabelsplitofZS-JHMDBhas15train- Figure5aand5b, whentheactionperformedbythetarget
ingclassesand6testingclasses,andeachsplitofZS-UCF person has limited relevance to others, the identified inter-
has18trainingclassesand6testingclasses. Morespecifi- esttokenswillexhibitreducedemphasisonareasinvolving
cally,withineachlabelsplit,thereare6classesfortesting, other people. Conversely, in Figure 5c and 5d, where the
and all the remaining classes in the dataset are designated person’s actions interact with others, our method adeptly
as training classes. In each label split, we follow the offi- recognizes context tokens within other people’s areas as
cial split 1 of the two datasets, J-HMDB and UCF101-24, interest tokens, effectively extracting relevant information
obtainingthetrainingvideosofthetrainingclassesandthe fromthoseregions. Moreover,ourInterestTokenSpotting
testingvideosofthetestclasses. possessthecapabilitytoidentifycrucialobjects,enhancing
For ZS-AVA, to ensure an adequate volume of training our ability to recognize actions, such as the chair and the
data,werefrainfromutilizingclassesthatfrequentlyappear computerinFigure5a,andthecellphoneinFigure5d.
in most training videos as unseen classes. Our objective
is to diversify each split by incorporating various types of 3.TextFeaturesDistribution
unseen classes, including pose actions, object interactions,
As class names inherently carry limited semantic infor-
andpersoninteractions. Thesplit1contains5poseactions
mation,relyingsolelyonthetextfeaturesofthesewordsto
and 13 object interactions, split 2 contains 2 pose actions,
calculate similarity with person tokens may introduce am-
6objectinteractionsand4personinteractions,andthesplit
biguity, potentially impacting the accuracy of action clas-
3 contains 5 object interactions and 1 person interactions.
sification. To examine the distribution of text features in
ThetestingclassesineachlabelsplitareshowninTable5.
thefeaturespace,weemployPrincipalComponentAnaly-
sis (PCA) to reduce each feature to two dimensions. Sub-
2.InterestTokens
sequently, we illustrate the text features distribution of the
We first explore the effects of utilizing only interest to- CLIPtextencoderandvariousContextPromptinglayerout-
kenstopromptlabels,asopposedtoincorporatingallcon- putsinFigure6.
text tokens. In Figure 4, we showcase the current frame Initially, the results reveal that regardless of the label
alongwithaboundingbox, indicatingourobjectiveofde- group, the original text features derived solely from class
tectingtheperson’saction. Additionally,weincludeneigh- namesarerelativelyclose,potentiallyresultinginmisclas-
boringframestofacilitatetheobservationofchangesinthe sificationofactions.However,ourContextPromptingmod-
video over this period. Since the context tokens have not ulewillutilizedifferentvisualinformationineachlayerto
undergonespatialmodelinginthefirstimageencoderlayer, augment the text content, thereby gradually increasing the
we choose to visualize the results obtained from this layer discriminabilitybetweeneachclassname. Forexample,in
to improve the interpretability of the identified interest to- Figure 6c, if we rely solely on the original text features to
kens.InFigure4a,thetargetpersonisengagedintheaction categorizetheaction”Skateboarding,”itmayresultinmis-
9ZS-JHMDB
split1 clap sit wave throw pullup catch
kick climb shoot
split2 run stand pick
ball stairs gun
brush shoot shoot
split3 push pour jump
hair bow ball
swing
split4 golf walk clap sit wave
baseball
ZS-UCF
Ice Floor Salsa Skate Soccer Volleyball
split1
Dancing Gymnastics Spin Boarding Juggling Spiking
Golf Cliff
split2 Basketball Skiing Biking Diving
Swing Diving
Horse Long Pole Basketball
split3 Fencing Surfing
Riding Jump Vault Dunk
Walking
Rope Tennis Trampoline Cricket
split4 Skijet With
Climbing Swing Jumping Bowling
Dog
ZS-AVA
brush chop cook crawl play
dance
teeth boardgame
split1 extract fishing jump/ kick dig
martialart
leap (anobject)
row sail take
swim
boat boat shovel stir aphoto
hand lift play swim shovel take
clap (aperson) boardgame aphoto
split2
kick martial play row play workon
(aperson) art withkids boat withpets acomputer
text on/look at turn
hug
split3 press shovel stir a (e.g.,a
(aperson)
cellphone screwdriver)
Table5.TestingclassesineachlabelsplitonZS-JHMDB,ZS-UCFandZS-AVA.
classification due to the ambiguity with ”VolleyballSpik- onallthevideosinthetrain/testsplit. Comparedto[9]and
ing” in the feature space. However, as these text features our method, which classifies actions for individuals, these
extractvisualinformationacrossmultiplepromptinglayers, video classification methods only need to classify the en-
the distinction between them becomes more pronounced, tire video once to infer the actions of all detected people
whichaidsineasierdifferentiationbetweenvariousactions. in it, potentially resulting in lower GFLOPs and inference
time.Asfortrainingtime,unlike[9]andourmethod,which
optimize on an individual basis during the training phase,
4.ComplexityAnalysis
thesemethodsoptimizeonanentirevideounit,thusrequir-
Wereportthecomplexityanalysisonthelabelsplit1of ingfewertrainingiterations. However,itisworthmention-
ZS-JHMDBinTable6. WecalculatetheGFLOPsrequired ingthatourmethodcanachieve79.62framemAPwithsoft
to infer a video and the training/inference time is reported voting,whichisasubstantialimprovementoverothermeth-
10neighboring frames neighboring frames current frame interest tokens
(a)Unseenaction: brushteeth. Comparedwithutilizingallcontexttokensforprompting,theuseofonlyinteresttokensincreasestheconfidencescore
from0.34to0.45.
neighboring frames neighboring frames current frame interest tokens
(b)Unseenaction:fishing. Comparedwithutilizingallcontexttokensforprompting,theuseofonlyinteresttokensincreasestheconfidencescorefrom
0.30to0.34.
Figure4.Theimpactofinteresttokens.
(a)sit,workonacomputer (b)bend/bow(atthewaist),carry/hold(anobject),shovel
(c)bend/bow(atthewaist),watch(aperson),lift(aperson),playwithkids (d)walk,listento(aperson),texton/lookatacellphone
Figure5.Morevisualizationofinteresttokens.Boldtextindicateunseenactions.
ods. Besides,thesevideoclassificationmethodsonlywork 5.ResultswithGroundtruthBoundingBoxes
on single-action videos, while we can handle videos with
Inthissection,wepresenttheresultsusingground-truth
multiple actions. Compared to [9], which also uses indi-
bounding boxes of the test data on ZS-AVA, ZS-JHMDB,
viduals as the classification unit, our method requires less
andZS-UCFinTabs.7to9respectively,toanalyzetheper-
trainingtimeandachieves74.55mAPwithoutsoftvoting,
formanceofallmethodswithoutlocalizationerrors.
surpassing[9]by8.02mAP.
TheresultsinTable7showthatourperformanceisbet-
terthanmostothermethods,exceptforsplits2and3,where
it is slightly inferior to [28]. However, the performance
of video classification methods (e.g., [28]) on multi-action
11original text features Context Prompting layer 4 Context Prompting layer 8 Context Prompting layer 12
(a)Textfeaturesdistributionpromptedbythe”kickball”visualtokens.
original text features Context Prompting layer 4 Context Prompting layer 8 Context Prompting layer 12
(b)Textfeaturesdistributionpromptedbythe”push”visualtokens.
original text features Context Prompting layer 4 Context Prompting layer 8 Context Prompting layer 12
(c)Textfeaturesdistributionpromptedbythe”SkateBoarding”visualtokens.
original text features Context Prompting layer 4 Context Prompting layer 8 Context Prompting layer 12
(d)Textfeaturesdistributionpromptedbythe”HorseRiding”visualtokens.
Figure6.TextFeaturesDistribution.
videos largely depends on the quality of tracklets. In the trast, ourST-CLIPisnotconstrainedbythetracker, sothe
case of using detected boxes to comply with the zero-shot performancewillnotbesignificantlyreducedifweswitch
scenario, these methods require additional trackers instead tousingdetectedboxes.
ofusingground-truthtracklets,whichsignificantlyimpacts
Tabs. 8 and 9 present the results on ZS-JHMDB and
their performance (avg drops from 10.45 to 3.06). In con-
ZS-UCF.OnZS-JHMDB,ourmethodhasthebestaverage
12Training Inference Throughput Tuning
Method GFLOPs mAP
time(s) time(s) (FPS) parameters(M)
baseline 721.11 - 3.95 671.14 - 64.63
ActionCLIP
409.7 199.06 9.04 293.25 18.92 69.18
[27]
A5[12] 264.15 239.34 10.33 256.63 6.35 50.92
X-CLIP[18] 159.91 185.36 6.61 401.06 57.93 72.91
Vita-CLIP[28] 145.26 259.52 5.88 450.85 35.61 68.60
iCLIP[9] 2410.07 2493.1 58.11 45.62 11.6 66.53
ST-CLIP
2431.51 806.76 31.77 83.44 55.11 79.62
(Ours)
Table6.Complexityanalysisonthelabelsplit1ofZS-JHMDB.Wetrain/testallthemethodson8TeslaV100GPUs.
FramemAP@0.5 FramemAP@0.5
Method Method
split1 split2 split3 avg split1 split2 split3 split4 avg
baseline 13.49 13.23 3.55 10.09 Withouttheassumptionofsingle-actionvideo
baseline(personcrop) 11.88 7.06 3.72 7.55 baseline 85.70 95.11 88.17 92.99 90.49
iCLIP[9] 5.58 12.59 2.56 6.91 iCLIP[9] 91.30 89.91 81.56 79.12 85.47
Vita-CLIP[28] 10.88 14.69 5.78 10.45 ST-CLIP(Ours) 87.11 96.73 91.11 92.23 91.80
ST-CLIP(Ours) 15.80 14.32 5.16 11.76
Withtheassumptionofsingle-actionvideo
ActionCLIP[27] 91.33 94.39 85.26 87.90 89.72
Table7. EvaluationonZS-AVA.Wereporttheresultsusingthe
A5[12] 84.74 79.67 90.00 87.98 85.60
ground-truthboundingboxesofthetestdata.
X-CLIP[18] 89.85 98.28 93.69 94.64 94.12
Vita-CLIP[28] 94.31 98.80 96.56 96.59 96.57
FramemAP@0.5 ST-CLIP(Ours) 92.90 98.75 93.97 94.44 95.02
Method
split1 split2 split3 split4 avg
Table9. EvaluationonZS-UCF.Wereporttheresultsusingthe
Withouttheassumptionofsingle-actionvideo
ground-truthboundingboxesofthetestdata.
baseline 70.17 79.91 95.51 79.26 81.21
iCLIP[9] 71.06 76.57 93.44 74.98 79.01
ST-CLIP(Ours) 79.02 82.99 94.83 85.28 85.53 avoid false positives affecting the soft voting results, our
Withtheassumptionofsingle-actionvideo method can achieve the second-best performance on most
ActionCLIP[27] 74.92 83.33 89.46 80.58 82.07 splitsandonaverage.Itisworthnotingthattheimagequal-
A5[12] 54.81 74.36 78.20 62.19 67.39 ityoftheUCF101-24datasetisrelativelyblurry,withmost
X-CLIP[18] 77.85 80.21 92.71 81.68 83.11 of the characters occupying only a small part of the pic-
Vita-CLIP[28] 73.13 89.83 97.05 84.07 86.02 ture. Thisaffectstheaccuracyofourindividualclassifica-
ST-CLIP(Ours) 85.02 87.87 97.33 90.27 90.12 tion, making it slightly disadvantageous compared to [28],
whichreliesontheentiresampledframe.
Table8. EvaluationonZS-JHMDB.Wereporttheresultsusing
theground-truthboundingboxesofthetestdata. 6.Limitations
In certain scenarios, there may be unrelated individuals
performancewhetherusingdetectedboxesorground-truth whoarenotengagedinanyactions. Insuchcases,thesein-
boxes. On ZS-UCF, when we use ground-truth boxes to dividualsshouldbetreatedasbackgroundelements. How-
138 frames from each video, then utilize 6 transformer en-
coder layers to perform temporal modeling. Besides, we
adoptthehandcraftedpromptstheyproposedtopromptla-
bels. For Efficient-Prompting [12], we sample 16 frames
fromeachvideo,andemploytheA5modeltheyproposed,
which uses 2 encoder layers for temporal modeling and
prepends/appends 16 vectors with the textual embeddings.
ForX-CLIP[18],wesample8framesfromeachvideo,and
use the proposed Cross-frame Communication and 1-layer
Multi-frameIntegrationTransformertogeneratevideofea-
tures. Besides,wealsoleveragetheVideo-specificPrompt-
ing in their method to prompt labels. For Vita-CLIP [28],
we sample 8 frames from each video, and utilize 8 Global
Video-Level Prompts following their method. As for the
training iterations, for iCLIP, we train the network for 7K
iterationsonZS-JHMDBand10KiterationsonZS-UCFas
intheirmethod.Wetrainothervideoclassificationmethods
for1KiterationsonZS-JHMDBandZS-UCF,andmoreit-
Figure 7. Visualization of wrong detected person bounding erations will lead to lower performance due to overfitting.
box. The imageis fromthe testdata ofclass ”SoccerJuggling”, For all the aforementioned methods, we utilize the CLIP
and the boxes are detected by Faster R-CNN. The red box is backbone and freeze the image and text encoder, consis-
countedasafalsepositiveaccordingtothelabelinginthisdataset
tent with our approach. We provide the details of training
sincethepeopleinsidearenotdoingthe”SoccerJuggling”action.
hyperparametersforallmethodsonZS-AVA,ZS-JHMDB,
andZS-UCFinTabs.10to12.
ever,achievingthisnecessitatesthehumandetectortolearn
fromsamplesofaspecificactionclasstoeffectivelydetect
only the person executing this action. In a zero-shot set-
ting, the person detector is prone to experiencing more lo-
calizationerrorsasithasnotbeenexposedtosamplesofthe
testing classes. Taking Figure 7 as an example, the Faster
R-CNN may detect two person bounding boxes with high
confidence scores, although only one of them is genuinely
performingtheaction”SoccerJuggling”. Consequently,the
detection of the other box will be considered a false posi-
tive.
Givenourmethod’stwo-stagepipelinenature,theafore-
mentionedlocalizationerrorwillinfluenceourperformance
in two aspects: (1) In processing single-action videos,
these false positive instances will contribute to soft vot-
ing, thereby compromising the classification performance
tosomeextent. (2)Whenhandlingmulti-actionvideos,the
persontokensfromtheseincorrectlydetectedboxeswillbe
employedforinteractionmodeling,whichwillinfluencethe
effectivenessoftheperson-personinteractioninvolvingthe
targetindividual.
7.ImplementationDetails
Inthissection,weprovidetheimplementationdetailsfor
eachcomparedmethodinourexperiments. ForiCLIP[9],
weemploythebest-performingmodelfeaturingfourtypes
of interaction modules and Interaction-Aware Prompting.
ForActionCLIP[27], tofollowtheirapproach, wesample
14Method Iterations Learningrate Warmupiterations Warmupfactor Optimizer Batchsize
iCLIP[9] 30000 2000
Vita-CLIP[28] 10000 0.0004 1000 0.25 SGD 8
ST-CLIP(Ours) 20000 2000
Table10.hyperparametersonZS-AVA.AllmethodsemployViT-L/14backbone.
Method Iterations Learningrate Warmupiterations Warmupfactor Optimizer Batchsize
ActionCLIP[27]
A5[12]
1000 0.0002 700 0.25 SGD 8
X-CLIP[18]
Vita-CLIP[28]
iCLIP[9] 7000 0.0002 700
0.25 SGD 8
ST-CLIP(Ours) 3000 0.00025 800
Table11.hyperparametersonZS-JHMDB.AllmethodsemployViT-B/16backbone.
Method Iterations Learningrate Warmupiterations Warmupfactor Optimizer Batchsize
ActionCLIP[27]
A5[12]
1000 0.0002 1000 0.25 SGD 8
X-CLIP[18]
Vita-CLIP[28]
iCLIP[9] 10000 0.0002 1000
0.25 SGD 8
ST-CLIP(Ours) 3000 0.00025 800
Table12.hyperparametersonZS-UCF.AllmethodsemployViT-B/16backbone.
15References [14] Tsung-Yi Lin, Piotr Dolla´r, Ross Girshick, Kaiming He,
Bharath Hariharan, and Serge Belongie. Feature pyra-
[1] Shoufa Chen, Peize Sun, Enze Xie, Chongjian Ge, Jian-
mid networks for object detection. In Proceedings of the
nan Wu, Lan Ma, Jiajun Shen, and Ping Luo. Watch only
IEEE conference on computer vision and pattern recogni-
once: Anend-to-endvideoactiondetectionframework. In
tion,pages2117–2125,2017. 6
ProceedingsoftheIEEE/CVFInternationalConferenceon
[15] Tsung-YiLin,MichaelMaire,SergeBelongie,JamesHays,
ComputerVision,pages8178–8187,2021. 2
PietroPerona,DevaRamanan,PiotrDolla´r,andCLawrence
[2] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Zitnick. Microsoft coco: Common objects in context. In
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
ComputerVision–ECCV2014: 13thEuropeanConference,
MostafaDehghani,MatthiasMinderer,GeorgHeigold,Syl-
Zurich, Switzerland, September 6-12, 2014, Proceedings,
vain Gelly, et al. An image is worth 16x16 words: Trans-
PartV13,pages740–755.Springer,2014. 6
formers for image recognition at scale. arXiv preprint
[16] Ron Mokady, Amir Hertz, and Amit H Bermano. Clip-
arXiv:2010.11929,2020. 3
cap: Clip prefix for image captioning. arXiv preprint
[3] Han Fang, Pengfei Xiong, Luhui Xu, and Yu Chen.
arXiv:2111.09734,2021. 3
Clip2video: Mastering video-text retrieval via image clip.
[17] Sauradip Nag, Xiatian Zhu, Yi-Zhe Song, and Tao Xi-
arXivpreprintarXiv:2106.11097,2021. 3
ang.Zero-shottemporalactiondetectionviavision-language
[4] GueterJosmyFaure,Min-HungChen,andShang-HongLai.
prompting. In European Conference on Computer Vision,
Holisticinteractiontransformernetworkforactiondetection.
pages681–697.Springer,2022. 2
InProceedingsoftheIEEE/CVFWinterConferenceonAp-
[18] BolinNi,HouwenPeng,MinghaoChen,SongyangZhang,
plicationsofComputerVision, pages3340–3350, 2023. 1,
Gaofeng Meng, Jianlong Fu, Shiming Xiang, and Haibin
2
Ling.Expandinglanguage-imagepretrainedmodelsforgen-
[5] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and
eral video recognition. In European Conference on Com-
Kaiming He. Slowfast networks for video recognition. In
puter Vision, pages 1–18. Springer, 2022. 2, 3, 7, 13, 14,
ProceedingsoftheIEEE/CVFInternationalConferenceon
15
ComputerVision(ICCV),October2019. 1,2
[19] Junting Pan, Siyu Chen, Mike Zheng Shou, Yu Liu, Jing
[6] RohitGirdhar,JoaoCarreira,CarlDoersch,andAndrewZis-
Shao, andHongshengLi. Actor-context-actorrelationnet-
serman. Videoactiontransformernetwork. InProceedings
workforspatio-temporalactionlocalization.InProceedings
oftheIEEE/CVFconferenceoncomputervisionandpattern
oftheIEEE/CVFConferenceonComputerVisionandPat-
recognition,pages244–253,2019. 2
ternRecognition,pages464–474,2021. 1,2
[7] ChunhuiGu,ChenSun,DavidARoss,CarlVondrick,Car-
[20] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
oline Pantofaru, Yeqing Li, Sudheendra Vijayanarasimhan,
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
George Toderici, Susanna Ricco, Rahul Sukthankar, et al.
AmandaAskell,PamelaMishkin,JackClark,etal.Learning
Ava: Avideodatasetofspatio-temporallylocalizedatomic
transferable visual models from natural language supervi-
visual actions. In Proceedings of the IEEE conference on
sion.InInternationalconferenceonmachinelearning,pages
computervisionandpatternrecognition,pages6047–6056,
8748–8763.PMLR,2021. 2,3
2018. 6
[21] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
[8] KaimingHe,GeorgiaGkioxari,PiotrDolla´r,andRossGir-
Fasterr-cnn: Towardsreal-timeobjectdetectionwithregion
shick.Maskr-cnn.InProceedingsoftheIEEEinternational
proposalnetworks.Advancesinneuralinformationprocess-
conferenceoncomputervision,pages2961–2969,2017. 2
ingsystems,28,2015. 2
[9] Wei-Jhe Huang, Jheng-Hsien Yeh, Min-Hung Chen,
[22] KhurramSoomro,AmirRoshanZamir,andMubarakShah.
GueterJosmyFaure,andShang-HongLai.Interaction-aware
Ucf101:Adatasetof101humanactionsclassesfromvideos
promptingforzero-shotspatio-temporalactiondetection. In
inthewild. arXivpreprintarXiv:1212.0402,2012. 6
ProceedingsoftheIEEE/CVFInternationalConferenceon
[23] ChenSun,AbhinavShrivastava,CarlVondrick,KevinMur-
ComputerVision,pages284–293,2023. 2,3,5,6,7,8,10,
phy,RahulSukthankar,andCordeliaSchmid. Actor-centric
11,13,14,15
relation network. In Proceedings of the European Confer-
[10] Hueihan Jhuang, Juergen Gall, Silvia Zuffi, Cordelia
enceonComputerVision(ECCV),pages318–334,2018. 2
Schmid, and Michael J Black. Towards understanding ac-
[24] Jiajun Tang, Jin Xia, Xinzhi Mu, Bo Pang, and Cewu Lu.
tionrecognition. InProceedingsoftheIEEEinternational
Asynchronous interaction aggregation for action detection.
conferenceoncomputervision,pages3192–3199,2013. 5
In Computer Vision–ECCV 2020: 16th European Confer-
[11] ChaoJia,YinfeiYang,YeXia,Yi-TingChen,ZaranaParekh,
ence,Glasgow,UK,August23–28,2020,Proceedings,Part
HieuPham, QuocLe, Yun-HsuanSung, ZhenLi, andTom
XV16,pages71–87.Springer,2020. 1,2
Duerig. Scaling up visual and vision-language representa-
tion learning with noisy text supervision. In International [25] DuTran,LubomirBourdev,RobFergus,LorenzoTorresani,
conferenceonmachinelearning,pages4904–4916.PMLR, andManoharPaluri. Learningspatiotemporalfeatureswith
2021. 2,3
3dconvolutionalnetworks.InProceedingsoftheIEEEinter-
[12] ChenJu,TengdaHan,KunhaoZheng,YaZhang,andWeidi nationalconferenceoncomputervision, pages4489–4497,
Xie. Prompting visual-language models for efficient video 2015. 1
understanding. In European Conference on Computer Vi- [26] DuTran,HengWang,LorenzoTorresani,JamieRay,Yann
sion,pages105–124.Springer,2022. 2,3,7,13,14,15 LeCun,andManoharPaluri.Acloserlookatspatiotemporal
[13] YanghaoLi,SainingXie,XinleiChen,PiotrDollar,Kaim-
ing He, and Ross Girshick. Benchmarking detection
16
transfer learning with vision transformers. arXiv preprint
arXiv:2111.11429,2021. 6convolutions for action recognition. In Proceedings of the
IEEEconferenceonComputerVisionandPatternRecogni-
tion,pages6450–6459,2018. 1
[27] MengmengWang,JiazhengXing,andYongLiu.Actionclip:
Anewparadigmforvideoactionrecognition.arXivpreprint
arXiv:2109.08472,2021. 2,3,7,13,14,15
[28] Syed Talal Wasim, Muzammal Naseer, Salman Khan, Fa-
hadShahbazKhan,andMubarakShah.Vita-clip:Videoand
textadaptiveclipviamultimodalprompting.InProceedings
oftheIEEE/CVFConferenceonComputerVisionandPat-
ternRecognition,pages23034–23044,2023. 2,3,6,7,11,
13,14,15
[29] TaoWu,MengqiCao,ZitengGao,GangshanWu,andLimin
Wang. Stmixer: Aone-stagesparseactiondetector. InPro-
ceedingsoftheIEEE/CVFConferenceonComputerVision
andPatternRecognition,pages14720–14729,2023. 2
[30] WenhaoWu,XiaohanWang,HaipengLuo,JingdongWang,
Yi Yang, and Wanli Ouyang. Bidirectional cross-modal
knowledge exploration for video recognition with pre-
trained vision-language models. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition,pages6620–6630,2023. 2,3
[31] Xitong Yang, Xiaodong Yang, Ming-Yu Liu, Fanyi Xiao,
Larry S Davis, and Jan Kautz. Step: Spatio-temporal pro-
gressivelearningforvideoactiondetection. InProceedings
oftheIEEE/CVFConferenceonComputerVisionandPat-
ternRecognition,pages264–272,2019. 2
[32] WenwenYu,YuliangLiu,WeiHua,DeqiangJiang,BoRen,
andXiangBai. Turningaclipmodelintoascenetextdetec-
tor. InProceedingsoftheIEEE/CVFConferenceonCom-
puter Vision and Pattern Recognition, pages 6978–6988,
2023. 3
[33] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella,
Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang,
Boxin Li, Chunyuan Li, et al. Florence: A new
foundation model for computer vision. arXiv preprint
arXiv:2111.11432,2021. 2,3
[34] Yifu Zhang, Peize Sun, Yi Jiang, Dongdong Yu, Fucheng
Weng, ZehuanYuan, PingLuo, WenyuLiu, andXinggang
Wang.Bytetrack:Multi-objecttrackingbyassociatingevery
detectionbox. InEuropeanconferenceoncomputervision,
pages1–21.Springer,2022. 6
[35] Jiaojiao Zhao, Yanyi Zhang, Xinyu Li, Hao Chen, Bing
Shuai, Mingze Xu, Chunhui Liu, Kaustav Kundu, Yuanjun
Xiong, Davide Modolo, et al. Tuber: Tubelet transformer
forvideoactiondetection. InProceedingsoftheIEEE/CVF
Conference on Computer Vision and Pattern Recognition,
pages13598–13607,2022. 2
17