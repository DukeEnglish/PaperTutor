JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 1
Perceive-IR: Learning to Perceive Degradation
Better for All-in-One Image Restoration
Xu Zhang , Jiaqi Ma , Guoli Wang , Qian Zhang , Huan Zhang , Member, IEEE, and
Lefei Zhang , Senior Member, IEEE
Abstract—The limitations of task-specific and general image
restoration methods for specific degradation have prompted the
developmentofall-in-oneimagerestorationtechniques.However,
the diversity of patterns among multiple degradation, along
with the significant uncertainties in mapping between degraded
imagesofdifferentseveritiesandtheircorrespondingundistorted
versions, pose significant challenges to the all-in-one restoration
tasks. To address these challenges, we propose Perceive-IR, an
all-in-oneimagerestorerdesignedtoachievefine-grainedquality
control that enables restored images to more closely resemble
theirundistortedcounterparts,regardlessofthetypeorseverity
of degradation. Specifically, Perceive-IR contains two stages: (1)
prompt learning stage and (2) restoration stage. In the prompt
learning stage, we leverage prompt learning to acquire a fine-
grained quality perceiver capable of distinguishing three-tier
qualitylevelsbyconstrainingtheprompt-imagesimilarityinthe
CLIP perception space. Subsequently, this quality perceiver and
difficulty-adaptive perceptual loss are integrated as a quality-
aware learning strategy to realize fine-grained quality control
in restoration stage. For the restoration stage, a semantic
guidance module (SGM) and compact feature extraction (CFE)
are proposed to further promote the restoration process by
utilizing the robust semantic information from the pre-trained
large scale vision models and distinguishing degradation-specific
features. Extensive experiments demonstrate that our Perceive-
Fig.1. PSNRcomparisonswithstate-of-the-artall-in-oneandgeneralmeth-
IR outperforms state-of-the-art methods in all-in-one image
odsacrosstwocommonall-in-oneimagerestorationscenarios.*denotesre-
restorationtasksandexhibitsuperiorgeneralizationabilitywhen
sultsobtainedunderAll-in-One(“Noise+Haze+Rain+Blur+Low-light”)train-
dealing with unseen tasks. ingsetting,whileunmarkedresultsarefromAll-in-One(“Noise+Haze+Rain”)
trainingsetting.Bestviewedincolor.
Index Terms—All-in-one image restoration, Prompt learning,
Large vision model. to address specific types of degradation. Once switched to
I. INTRODUCTION othertypesofdegradationscenarios,theeffectivenessofthese
IMAGE restoration, the process of which is to recover a methods diminishes significantly. To overcome these limi-
tations, general image restoration methods [16]–[22] which
clearimagefromitsdegradedversion,hasseenremarkable
could handle various types of degradation, have been in-
advancements with the emergence of deep learning. Tradi-
troduced. However, this approach typically requires training
tionally, this challenge has been addressed by task-specific
a separate network for each type of degradation. During
networks, each specifically designed and trained to handle a
the inference stage, model parameters need to be switched
uniquetypeofdegradation.Thistargetedapproachhasyielded
between different degradation scenarios to achieve optimal
significant success across a spectrum of restoration tasks,
e.g., denoising [1]–[3], dehazing [4]–[6], deraining [7]–[9], restoration performance. This process is not only resource-
intensive but also impractical.
deblurring [10]–[12], and low-light enhancement [13]–[15].
Although task-specific methods have proven effective, their Recently, all-in-one image restoration methods [23]–[29]
applicabilityis inherentlyrestrictedbecausethey aredesigned have surfaced as a potential solution to the aforementioned
challenges. These methods are typically capable of address-
Thisworkwasinpartsupportedby.(XuZhangandJiaqiMacontributed ing multiple degradation concurrently by employing various
equallytothiswork.)(Correspondingauthor:LefeiZhang.)
mechanisms. Early works, such as AirNet [23] and ADMS
Xu Zhang, Jiaqi Ma, and Lefei Zhang are with the Institute of Artifi-
cial Intelligence, School of Computer Science, Wuhan University, Wuhan [24], obtained discriminative representations of degraded fea-
430072, China. Lefei Zhang is also with the Hubei Luojia Labora- tures by explicitly constructing degradation encoder or dis-
tory, Wuhan, China (e-mail: zx12220802@163.com; jiaqima@whu.edu.cn;
criminator. Later, ProRes [25] and PromptIR [26] further
zhanglefei@whu.edu.cn).
Guoli Wang and Qian Zhang are with the Horizon Robotics, Beijing improved the all-in-one restoration performance by injecting
100083,China(e-mail:guoli.wang@horizon.cc;qian01.zhang@horizon.ai). visual prompt information. Recently, some studies aim to
Huan Zhang is with the School of Information Engineering, Guang-
leverage the powerful feature representation capabilities of
dong University of Technology, Guangzhou 510006, China (e-mail:
huanzhang2021@gdut.edu.cn). pre-trained large-scale visual models such as CLIP [30] and
4202
guA
82
]VC.sc[
1v49951.8042:viXraJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 2
DINO [31] to promote texture information reconstruction and practical applications. The main challenge lies in using a
structural consistency maintenance. For instance, Tan et al. single set of model parameters to handle various types of
[32] explored the application of CLIP for adverse weather degradation and accurately restore the corresponding compo-
removal, demonstrating their effectiveness in restoring clear nents.Toachievethis,AirNet[23]proposedlearningdiscrim-
image under challenging weather conditions. Lin et al. [27] inative degradation representation using contrastive learning.
proposed a multi-task image restoration method guided by IDR [34] took a different approach by utilizing a two-stage
robust DINO features, which effectively enhances restoration ingredients-oriented restoration network. PromptIR [26] and
quality under various degradation conditions. ProRes [25] further enhanced the network’s ability to handle
Nevertheless, the substantial challenges in accurately map- multiple degradation through vision prompts. More recently,
ping degraded images, which exhibit varying degrees of dis- CLIP-AWR[32],DA-CLIP[29],andDINO-IR[27]leveraged
tortion, to their corresponding ground truths remain largely pre-trained large-scale vision models to excel in all-in-one
unresolved.Basedonthis,wedevelopanovelall-in-oneimage restoration tasks.
restoration method, which integrates quality-aware learning However, in practice, degraded images often suffer from
strategy alongside degradation-aware loss, leveraging the ro- varying levels of corruption. The above methods may fall into
bust capabilities of large-scale vision models, enabling ef- the trap of processing these images with the same restoration
fective restoration of diverse degraded images with varying effort, resulting in sub-optimal performance. Based on this
levelsofdegradation.Fisrt,weconstructafine-grainedquality observation,Chenetal.[36]designedablindall-in-oneimage
perceiver to discern three-tier image quality levels by con- restoration method namely UtilityIR with learning an image
straining the prompt-image similarity in the CLIP perception quality ranker. However, it depends on the simple image
space. Subsequently, this quality perceiver is integrated as quality metric, i.e., PSNR, and is susceptible to variations
a quality-aware learning strategy together with difficulty- in certain parameters. In comparison, our method focuses on
adaptive perceptual loss to realize fine-grained quality control leveraging CLIP’s zero-shot capability to construct a more
in restoration stage. For the restoration stage, a semantic robustqualityperceivertoachievefine-grainedqualitycontrol
guidancemodule(SGM)andcompactfeatureextraction(CFE) of the restored images.
are proposed to promote the restoration process. The SGM
B. Prompt Learning
leverages robust semantic insights from large-scale vision
In recent years, prompt learning as an emerging learning
models, while the CFE discerns and isolates degradation-
paradigm has seen significant advancements in the field of
specificfeatures.AsshowninFig.1,ourproposedPerceive-IR
naturallanguageprocessing[41]–[43].Itseffectivenesshasled
demonstratessuperiorperformancethanstate-of-the-artall-in-
to its widespread application in vision-related tasks [44]–[48].
one and general methods under All-in-One training setting.
The core idea is to enable pre-trained models to better under-
The contributions of this work can be summarized as
stand and perform downstream tasks by constructing specific
follows:
prompts. In natural language processing, these prompts are
1) We develop Perceive-IR, an all-in-one image restoration
usually given in the form of text, whereas in computer vision,
method that integrates quality-aware learning strategy along-
they may involve images, text, or other forms of data.
side degradation-aware loss, leveraging the robust capabilities
In the image restoration, PromptIR [26] and ProRes [25]
of large-scale vision models.
firstly introduced prompt learning to all-in-one image restora-
2) The proposed quality-aware learning strategy, which is
tion, further enhancing the restoration capability of the model
groundedinpromptlearninganddifficulty-adaptiveperceptual
with vision prompts. Recently, CLIP-LIT [49] has shown that
losstorealizefine-grainedqualitycontrol,pushingtherestored
initializingtextualpromptsinCLIPcanaidinextractingmore
image towards the ground truth while pulling it away from
accurate low-level image representations. However, relying
samples under bad and medium quality levels.
solelyondistinguishingbetweenpositiveandnegativeprompts
3) We proposed a semantic guidance module which com-
may fail to effectively guide the restoration process towards
prising a pre-trained DINO-v2 and the prompt guidance mod-
achieving undistorted images. This is because the vast spec-
ule to produce feature representations enriched with semantic
trum of possibilities between the two prompts can complicate
and degradation priors.
the accurate mapping of degraded images to their pristine
4) Extensive experimental results across various restoration
counterparts. To address this, we propose an effective prompt
tasks demonstrate the superior performance of our Perceive-
learning approach by further differentiating among three-tier
IR, including image denoising, dehazing, deraining, deblur-
prompt pairs to enable more nuanced and precise control of
ring, and low-light enhancement tasks.
the restoration process towards the undistorted counterparts.
II. RELATEDWORK
C. Large-scale Vision Models
A. All-in-one restoration Large-scale vision models (LVMs) have shown powerful
All-in-one restoration [23]–[29], [32]–[37] aims to recover robustfeaturerepresentationandzero-shottransfercapabilities
cleanimagesfrommultipledegradedimagesthroughaunified in various tasks. For example, CLIP [30] has been success-
model, has grown to be a promising field of low-level vision fully applied on various downstream tasks [50]–[52] due to
tasks. Compared to task-specific [1]–[15] and general [16]– its amazing semantic alignment ability between vision and
[22], [38]–[40] image restoration, all-in-one restoration is language. As the self-supervised ViT model, DINO [31] and
more advantageous in terms of model storage efficiency and DINO-v2[53]havedemonstratedeffectivenessacrossmultipleJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 3
I I I cross entropy loss
! " #
“Excellent” T
$
“Mediocre”T
"
bad quality medium quality good quality “Terrible” T
%
degraded restored ground truth
CLIP CLIP Learned prompts
Image Encoder Text Encoder
Frozen Learnable
restoration model
Fig. 2. In the proposed prompt learning stage, we initialize and train textual prompts using image-text pairs categorized into three tiers of quality. These
promptsaretrainedwithcross-entropylossintheCLIPmodel.Oncetrained,thelearnedpromptsarefixedandusedtoguidetherestorationofhigh-quality
imagesduringthesubsequentrestorationstage.
tasks [54], [55] and eliminate the need for labeled input data. a“mediocre”textualpromptT ∈RN×512,anda“excellent”
m
Since these LVMs can learn useful prior knowledge from textual prompt T ∈ RN×512. N represents the number of
e
external hyper-scale datasets, a similar pipeline that draws embedded tokens in each prompt. Then, we feed the bad,
on pre-trained LVMs has become increasingly popular in medium, and good quality images I , I , and I to the CLIP
d m g
the low-level community [27], [29], [32]. Inspired by the image encoder to obtain their latent encoding. Meanwhile, we
aforementioned studies, in this paper, we utilize the semantic extractthelatentencodingsofthe“excellent”,“mediocre”,and
prior knowledge and structural information mined by the “terrible” textual prompts by feeding them to the CLIP text
proposed DINO-v2-based semantic guidance module to guide encoder.Basedonthetext-imagesimilarityintheCLIPlatent
the restoration process. space,weusethecrossentropylossL ofclassifyingthebad,
ce
medium,andgoodqualityimagestolearntheseprompts.The
III. METHOD
L can be described as
ce
A. Overview Pipeline
1 (cid:88) (cid:88)
Our Perceive-IR contains two stages: a prompt learning L ce =− 3 Y ijlog
stage and a restoration stage. In the prompt learning stage, i∈{d,m,g}j∈{e,m,t}
we initialize and train textual prompts using image-text pairs (cid:32) exp(cid:0) S(cid:0) Φ (I ),Φ (T )(cid:1)(cid:1) (cid:33) (1)
I i T j
categorized into three-tier of quality by constraining the text- (cid:80) (cid:0) (cid:0) (cid:1)(cid:1) ,
exp S Φ (I ),Φ (T )
image similarity in the CLIP perception space. The learned k∈{e,m,t} I i T k
prompts are fixed and then used to guide the restoration pro- where S(·,·) denotes cosine similarity and Y is the label of
ij
cessing during the subsequent stage. In the restoration stage, the current image I ; Φ (·) and Φ (·) denote CLIP image
i I T
we use compact feature extraction (CFE) to learn distinct encoder and CLIP text encoder, respectively.
degradation representations. These representations are then
concatenated with semantic priors extracted by pre-trained C. Restoration Stage
DINO-v2 encoder. These concatenated features modulate the Restoration Branch. As shown in Fig. 3, given a degraded
output features of the decoder within the prompt guidance input I ∈ RH×W×3, we first applies a 3×3 convolution to
d
module(PGM).Thecalibratedfeaturesarefurtheremphasized extract shallow embeddings X ∈ RH×W×C, where H×W
s
by the prior guidance cross attention (PGCA) to extract and denotesthespatialdimensionandC isthenumberofchannels.
convey structural semantic information. Moreover, we utilize Next,theshallowfeaturesX aregraduallyandhierarchically
s
quality-aware learning strategy which contains CLIP-aware encoded into deep features Xe l,d ∈ R 2lH −1× 2lW −1×2l−1C. Af-
loss and difficulty-adaptive perceptual loss to realize fine- ter encoding the degraded input into latent features X ∈
4
grained quality control and guide the restoration of high- RH 8×W 8 ×8C, the decoder progressively recovers the high-
quality images. resolution representations. Finally, a 3 × 3 convolution is
applied to reconstruct restored image I ∈RH×W×3.
B. Prompt Learning Stage r
In the restoration branch, we choose Restormer [20] as
TheprocessofpromptlearningstageisshowninFig.2.We
backbone. Specifically, in the encoder layer, we integrate
use the CLIP model to learn the three types of prompts, i.e.,
Multi-Dconv Head Transposed Attention (MDTA) [20] and
“terrible”, “mediocre”, and “excellent”, corresponding to the
Gated-Dconv Feed-Forward Network (GDFN) [20] to jointly
three-tier quality levels of images to the CLIP image encoder,
construct the Transformer Block (TB). In the latent and
i.e., bad, medium, and good quality images. Specifically, we
decoder layers, we introduce the Prior Guidance Cross At-
divide the degraded inputs equally into two subsets. Then, we
tention (PGCA) and GDFN to jointly construct the Enhanced
train a restoration model, i.e., Restormer [20] to obtain the
Transformer Block (ETB). The process of restoration branch
restored images I via cross-validation strategy. The restored
m can be described as
imagesareregardedasthemediumqualityimages.Afterthat,
given a bad quality image I d ∈ RH×W×3, a medium quality F tb(Xe l)=F gdfn(cid:0) F mdta(Xe l)(cid:1) , (2)
imageI ∈RH×W×3,andagoodqualityI ∈RH×W×3,we
m g
randomly initialize a “ terrible” textual prompt T ∈RN×512, F (Xd,Y )=F (cid:0) F (Xd,Y )(cid:1) , (3)
t etb l l gdfn pgca l l
Similarity
ScoreJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 4
Restoration Branch (RB) degraded
Skip connection
degraded TB ´L 1 ´L 2 ´L ´L ´L 2 ETB ´L 1 Restored
3 ´L 3 4
ℒ
!"#
Compact Feature Extraction (CFE) Y Y Y Y
Z Xd 4 Xd 3 Xd 2 Xd 1
4 3 2 1
PGM PGM PGM PGM
GT
ℒ Z Z Z Z
!" F F F F
4 3 2 1
Sec. III-D
Quality-aware
Semantic Guidance Module (SGM)
Learning Strategy
F F F F
1 2 3 4
PromptGuidanceModule(PGM) PriorGuidanceCross Attention(PGCA) ℒ =ℒ +𝜆 ℒ +𝜆 ℒ +𝜆 ℒ
$%$&’ !"# ( #’ ) #’*+ , -+’
X Xd
l 𝛾 𝛽 Y l l V Sec. III-E Sec. III-D
Conv Conv
K Element-wise Multiplication
DINO-v2
Encoder Z Linear Linear Y Q X lm Element-wise Addition
C l
F C Concatenation Frozen
l
Fig.3. Theproposedrestorationstageconsistsof:(a)RestorationBranch(RB):A4-levelU-shapedencoder-decoderstructurethatincorporatesTransformer
Block (TB) [20] in the encoder and Enhanced Transformer Block (ETB) in the decoder. (b) Compact Feature Extraction (CFE): A module designed to
generate distinctive degradation representation. (c) Semantic Guidance Module (SGM): Comprising a pre-trained DINO-v2 [53] and the Prompt Guidance
Module(PGM)toproducefeaturerepresentationsenrichedwithsemanticanddegradationpriors.
I
r
=Conv(cid:18) F e4 t, b3,2,1(cid:16) F t1 b,2,3(cid:0) Conv(I d)(cid:1) ↓ ×2,Y l(cid:17) ↑ ×2(cid:19) +I d, shown in Fig. 3, given a degraded image I d, it goes through
the pre-trained DINO-v2 encoder and outputs four different
(4)
levels of semantic features F ∈R1×768 (l = 1, 2, 3, 4). This
where F (·), F (·), F (·), F (·), and F (·) in- l
gdfn mdta pgca tb etb process can be described as
dicate GDFN, MDTA, PGCA, TB, and ETB processes, re-
spectively; Xe l denotes output of l-th encoder layer and Xd l F 1,F 2,F 3,F 4 =D i(I d), i=1,4,8,12, (5)
represents input of l-th decoder layer, respectively; Conv(·)
whereD (·)denotesi-thlayerofDINO-v2encoder.Then,the
i
indicates 3×3 convolution operation and Y denotes output
l semantic prior and learned degradation representation Z are
of prompt guidance module; ↓ and ↑ indicate down-sampling
exploited to generate reliable content to guidance the restora-
and up-sampling, respectively.
tion by PGM. The modulated feature Y is transmitted to
l
Compact Feature Extraction. In the restoration stage, we priorguidancecrossattention(PGCA)toguidetherestoration
propose Compact Feature Extraction (CFE) to extract a com- branch.Specifically,wetakeY asquerytoperformthecross-
l
pact degradation representation Z∈R1×128. Then, we utilize attention. The above process can be expressed as
thedegradationlossL (asdetailedinSec.III-E)tooptimize
cl Y =F (F ,Z,Xd), (6)
CFE by leveraging the consistency of images with the same l pgm l l
degradationandtheinconsistencyacrossdifferentdegradation. CrossAtt(Y ,Xd,Xd)=Softmax(Qˆ ⊗Kˆ/α(cid:1) ⊗Vˆ, (7)
l l l
Afterthat,Zwillbeconcatenatedwiththemulti-scalefeatures Xm =F (cid:0) CrossAtt(Y ,Xd,Xd)(cid:1) +Y , (8)
F extracted by pre-trained large vision model. The deep l mdta l l l l
l
feature Xd and the concatenated feature will perform affine where F indicates PGM process; ⊗ denotes matrix multi-
l pgm
transformation in Prompt Guidance Module (PGM) to obtain plication.
Y . Subsequently, the Y is employed to execute cross- D. Quality-aware Learning Strategy
l l
attention within the Enhanced Transformer Block (ETB).
CLIP-aware Loss. As illustrated in Fig. 4, given the learned
Semantic Guidance Module. Recently, large-scale vision textpromptsobtainedfromthepromptlearningstage,wefirst
models (e.g., DINO family [31], [53] ) have demonstrated its fix these prompts and then train the restoration model using
potential in a series of visual downstream tasks [54], [55] in the CLIP-aware loss L . The L can be described as
clip clip
a self-supervised manner. DINO-v2, as an improved version (cid:16) (cid:0) (cid:1)(cid:17)
exp S Φ (I ),Φ (T )
of DINO, can provide more powerful feature representation I r T e
L =1− , (9)
thanks to pre-training on more than one million data. To clip (cid:80) (cid:16) (cid:0) (cid:1)(cid:17)
exp S Φ (I ),Φ (T )
this end, we employ the DINO-v2 to extract useful semantic k∈{e,m,t} I r T k
feature priors from degraded images. Additional, a prompt where S(·,·) denotes cosine similarity. The L is to min-
clip
guidance module (PGM) is designed to help the network imize the gap between the restored images I and the “ex-
r
bettercaptureandpreservestructuralsemanticinformation.As cellent” prompt T , while maximizing the gap between I
e r
Extraction
Resblock
MDTA
Conv LReLU
GDFN
Conv
Avg
Pool
Down
Linear
TB
LReLU
Down
Linear
TB Down ETB Up ETB Up
MDTA
ETB Up PGCA GDFN
ReconstructionJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 5
degraded (a) CLIP-aware loss the easy negative and set λ = 2. {ξ i} represents the set of
hyperparameters, we set ξ (i = 3, 7, 11, 15) to 1 , 1, 1, and
i 12 6 3
“Excellent”T" 1, respectively.
“Mediocre”T$ E. Training Loss
Restoration Stage “Terrible” T% In the prompt learning stage, we use L
ce
to learn the initial
CLIP CLIP Fixed prompts text prompt pairs. In the restoration stage, we first utilize
Restored Image Encoder Text Encoder
the CFE and introduce a degradation-aware loss L to learn
cl
(b) Difficulty-adaptive perceptual loss degradation representations. The L can be written as
… cl
ℒ !"# degraded Easy neg. exp(S(Z,Z+)/τ)
GT
AuR ge ms eto nr tGe eTd
d
… VGG-16 Positivepush
Ha pushrd n Ve eg
pr
us.
hy-hard neg.
wL
hc el
r=
e
n− alo ng
de τxp in( dS ic( aZ te,Z th+ e)/ nτ u) m+ be(cid:80)
r
on i= f1 ne ex gp at(cid:0) ivS e(Z sa, mZ p− i le)/ sτ ( a1(cid:1)
n2,
d)
Proxy Models Augm… ented
pull
Anchor thetemperature,respectively.Givenseveralimagescontaining
different degradation types, we randomly crop two patches
Fig. 4. The proposed quality-aware learning strategy contains two compo- of the same resolution on an image and let them be positive
nents.(a)TheCLIP-awarelossand(b)Thedifficulty-adaptiveperceptualloss. samples, while letting patches of different degradation types
with the “mediocre” prompt T m and “terrible” prompt T t, benegativesamples.Then,inputthemintoCFEtogetanchor
respectively. It emphasizes pulling the restored results closer feature Z, positive feature Z+, and negative feature Z−.
to ground truth, guiding the model to generate high-quality The total loss of the restoration stage can be described as
images.
L =L +λ L +λ L +λ L , (13)
Difficulty-adaptive Perceptual Loss. Using contrastive reg- total rec 1 cl 2 clip 3 dpl
ularization has been demonstrated as an effective way of
where L is L loss; λ , λ , and λ are hyperparameters to
rec 1 1 2 3
improving image restoration performance [56], [57]. Based
make a balance among the total loss.
on this, we introduce the difficulty-adaptive perceptual loss to
IV. EXPERIMENTS
dynamically adjust its behavior based on the difficulty level
A. Experimental Setup
of the restoration process. It effectively handles both easy and
hard samples, ensuring that the network focuses appropriately Datasets. Following [23], [26], [34], we differentiate
onchallengingsampleswhileefficientlyprocessingeasycases. the training setups into “All-in-One” and “One-by-One”
As shown in Fig. 4, the image predicted I by the network based on whether datasets are combined for mixed
r
servesastheanchor,thereferenceimageI aspositivesample, training. In our study, we summarise two common
g
the degraded I as easy negative sample, and the augmented mixed degradation: “Noise+Haze+Rain (N+H+R)” and
d
imagesI fromproxyrestorationmodels(e.g.,InstructIR[28], “Noise+Haze+Rain+Blur+Low-light (N+H+R+B+L)”. Under
q
PromptIR [26], FSNet [21], and MambaIR [40]) as hard or the “One-by-One” setting, the model is trained and tested
very-hard negative samples. using datasets from a single image restoration task at a time.
Specifically,atthebeginningofthek-thepoch,weevaluate Underthe“All-in-One”setting,themodelistrainedandtested
the performance of network by calculating the average PSNR using mixed datasets from multiple image restoration tasks.
score and categorize the non-easy negative sample as a very- In Tab. I, we detail the training and testing datasets used in
hard sample if its PSNR surpasses the current network per- our experiments. For single-task image restoration, i) Image
formance, otherwise, it is classified as hard negative sample. denoising: we conduct training using a merged dataset of
Then,theweightO ofthek-thepochassignedtoanon-easy BSD400[59]andWED[60]with400and4,744clearimages,
k
negative sample N , are defined as respectively. Noisy images are generated with Gaussian noise
q
(cid:26) (σ ∈ {15,25,50}). Testing is performed on CBSD68 [61],
1+γ, avgPSNR(I ,Θ )≥PSNR(N ),
O (N )= r k−1 q Urban100[62],andKodak24[63]datasets.ii)Imagedehazing:
k q 1−γ, otherwise,
we use the OTS dataset of RESIDE-β [64] with 72,135 pairs
(10)
for training and 500 images from SOTS-Outdoor [64] dataset
where q = 1, 2, ···, z; z is the number of non-easy negatives,
for testing. iii) Image deraining: we use the Rain100L [65]
and γ is a hyperparameter set to 0.25; Θ represents the
k−1
datasetwith200pairsofimagesfortrainingand100pairsfor
modelweightsfromthe(k−1)-thepoch.Theweightsofhard
testing.iv)Imagedeblurring:wetrainthemodelontheGoPro
andvery-hardnegativesaresetto1+γ and1−γ,respectively.
[11]dataset,whichcontains2,103pairsfortrainingand1,111
Based on this, the proposed L can be written as
dpl
pairs for testing. v) Low-light enhancement: we use the LOL
L =
(cid:88)
ξ
M i(I g,I r)
,
[13]dataset,whichcontains485pairsfortrainingand15pairs
dpl
i=3,7,11,15
i
λM i(I d,I
r)+(cid:80)z
q=1O k(N q)M i(N q,I r) for testing. For multi-task image restoration, we trained on a
(11) mixed dataset containing multiple degradation and tested one
where M (·,·) = ∥V (·)−V (·)∥ ; V (·) represents the i-th by one on the dataset containing a single type of degradation.
i i i 1 i
latentfeatureextractedfromthepre-trainedVGG-16[58].We Evaluation Metrics. For evaluation, we present the values
use the latent features of 3-rd, 7-th, 11-th, 15-th layers from of the Peak Signal to Noise Ratio (PSNR) and Structural
VGG-16 to calculate M (·,·). We assign a fixed weight λ to Similarity (SSIM). In addition, for the underwater image
i
Similarity
ScoreJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 6
TABLEI
DATASETSUMMARYUNDERTWOTRAININGSETTINGS.
Setting Degrdation Trainingdataset(Number) Testingdataset(Number)
Noise(N) N:BSD400[59]+WED[60](400+4744) N:CBSD68[61]+Urban100[62]+Kodak24[63](68+100+24)
Haze(H) H:RESIDE-β-OTS[64](72135) H:SOTS-Outdoor[64](500)
Rain(R) R:Rain100L[65](200) R:Rain100L[65](100)
Blur(B) B:GoPro[11](2103) B:GoPro[11](1111)
Low-light(L) L:LOL[13](485) L:LOL[13](15)
BSD400[59]+WED[60]+RESIDE-β-OTS[64]+Rain100L[65] N:CBSD68[61](68)
N+H+R Number:400+4744+72135+200 H:SOTS-Outdoor[64](500)
Total:77479 R:Rain100L[65](100)
N:CBSD68[61](68)
BSD400[59]+WED[60]+RESIDE-β-OTS[64]+Rain100L[65]
H:SOTS-Outdoor[64](500)
+GoPro[11]+LOL[13]
N+H+R+B+L R:Rain100L[65](100)
Number:400+4744+72135+200+2103+485
Total:80067 B:GoPro[11](1111)
L:LOL[13](15)
enhancement task, we extend our report to include Learnt AdamW optimizer with β =0.9 and β =0.999 to optimize
1 2
Perceptual Image Patch Similarity (LPIPS) [66], Underwater the network. The learning rate is set to 2×10−4 with a total
Colour Image Quality Evaluation Metric (UCIQE) [67], and batch size of 6 for 400K iterations. The weighting parameters
Underwater Image Quality Measure (UIQM) [68] scores. In for L are: λ = 0.1, λ = 0.05, and λ = 0.1.
total 1 2 3
tables,thebestandsecond-bestscoresarehighlightedinbold Allexperimentsareconductedon8NVIDIAGeForceRTX
and underline, respectively. PSNR/SSIM metrics are reported 3090GPUs.Duringtraining,weutilizecroppedpatchesofsize
on the RGB images. 128 × 128 as input, and random horizontal and vertical flips
Baselines. In the All-in-One (“N+H+R”) setting, we select are applied to augment the training data.
four task-specific methods: LPNet [69], ADFNet [70], De- B. All-in-One Restoration Results
hazeFormer [6], and DRSformer [9]; five general methods:
Tab. II presents the overall performance of Perceive-
MPRNet [16], Restormer [20], NAFNet [19], FSNet [21],
IR and other state-of-the-art methods under the All-in-One
and MambaIR [40]. and senven all-in-one methods: DL [71],
“Noise+Haze+Rain” training setting. It can be observed that
AirNet [23], IDR [34], NDR [35], PromptIR [26], Gridformer
the all-in-one methods basically perform better than general
[37] and InstructIR [28]. In the All-in-One (“N+H+R+B+L”)
methods and task-specific methods. Particularly, in terms of
setting, we add task-specific methods like HI-Diff [72] and
overall performance, our Perceive-IR exhibits superior results
Retinexformer [15]; general methods such as DGUNet [38],
compared with PromptIR [26] by over 0.57 dB/0.004 on
MIRNet-v2 [18], and SwinIR [17], as well as all-in-one
average in PSNR/SSIM while utilizing the same backbone
methods like Transweather [73], and TAPE [33].
of Restormer [20]. Compared to the latest methods, Grid-
In the One-by-One setting, we focus on adjustments to
former [37] and InstructIR [28], Perceive-IR also demon-
task-specific methods. For denoising, we employ DnCNN [1],
stratesaPSNR/SSIMimprovementof0.44dB/0.005and0.20
FFDNet [2], BRDNet [3], and ADFNet [70]. For dehazing,
dB/0.004, respectively.
we utilize DehazeNet [4], AODNet [74], FDGAN [5], and
As indicated in Tab. III, when extending to the more
DehazeFormer[6].Forderaining,weapplyUMR[7],MSPFN
challenging “Noise+Haze+Rain+Blur+Low-light” setting, our
[8], LPNet [69], and DRSformer [9]. For image deblurring,
Perceive-IR still exhibit superior results compared with
we leverage DeblurGAN [10], Stripformer [12], and HI-Diff
PromptIR, Gridformer, and InstructIR by over 0.69 dB/0.005,
[72].Lastly,forlow-lightimageenhancement,weincorporate
0.51dB/0.005,and0.29dB/0.002onaverageinPSNR/SSIM,
Retinex-Net [13], URetinex [14], and Retinexformer [15]. In
respectively. It can be observed that as the number of degra-
additiontothethesemethods,wealsoreportresultsfromsome
dation types increases, the advantages of the comparative all-
representative general and all-in-one methods trained in the
in-one methods become faint over general image restoration
One-by-One setting.
methods. By contrast, by incorporating both type-aware and
Implementation Details. For the prompt learning stage, we
quality-awareperception,andthedelicateincorporationofthe
employ the CLIP [30] model with ViT-B/32 as the backbone.
DINO-v2-based semantic prior guidance module, Perceive-IR
We train Restormer [20] for 100K iterations with a learning
still maintains good performance.
rate of 2×10−4. The number of embedded tokens N in each
C. One-by-One Restoration Results
learnable prompt is set to 16, and the prompt initialization
is set to 100K iterations with a learning rate of 4 × 10−5 In this part, we evaluate Perceive-IR under One-by-One
and the batch size is set to 32. For the restoration stage, setting. As shown in Tab. IV, compared to the state-of-
We chose Restormer as the restoration backbone of Perceive- the-art (SOTA) task-specific denoising method ADFNet [70]
IR and DINO-v2 [53] base version as the semantic guidance and the SOTA general image restoration method FSNet [21],
module. We adopt a similar setting to the original Restormer. Perceive-IRsurpassesthemby0.17dBand0.29dBinPSNR,
Fromlevel-1tolevel-4,thenumbersofTBandETBare[4,6, respectively, at a noise level of 15 on the CBSD68 and
6, 8], attention heads in MDTA and PGCA are both [1, 2, 4, Urban100 datasets. As shown in Tab. V, while Perceive-IR
8],andthechannelnumbersare[48,96,192,384].Weutilize demonstrateshighlycompetitiveperformanceindehazingand
enO-yb-enO
enO-ni-llA
)ksat-elgniS(
)ksat-itluM(JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 7
TABLEII
PERFORMANCECOMPARISONINALL-IN-ONE(“N+H+R”)SETTINGWITHTASK-SPECIFIC,GENERAL,ANDALL-IN-ONEIMAGERESTORATION
METHODS.THEREPORTEDRESULTSAREPARTIALLYBASEDONNDR[35]ANDPROMPTIR[26].*DENOTESAMODELTHATHASBEENRETRAINED.
Denoising(CBSD68[61]) Dehazing Deraining
Type Method Average Params (M)
σ=15 σ=25 σ=50 SOTS[64] Rain100L[65]
(CVPR’19)LPNet[69] 26.47/0.778 24.77/0.748 21.26/0.552 20.84/0.828 24.88/0.784 23.64/0.738 2.84
(AAAI’23)ADFNet*[70] 33.76/0.929 30.83/0.871 27.75/0.793 28.13/0.961 34.24/0.965 30.94/0.904 7.65
(TIP’23)DehazeFormer*[6] 33.01/0.914 30.14/0.858 27.37/0.779 29.58/0.970 35.37/0.969 31.09/0.898 25.44
(CVPR’23)DRSformer*[9] 33.28/0.921 30.55/0.862 27.58/0.786 29.02/0.968 35.89/0.970 31.26/0.902 33.72
(CVPR’21)MPRNet[16] 33.27/0.920 30.76/0.871 27.29/0.761 28.00/0.958 33.86/0.958 30.63/0.894 15.74
(CVPR’22)Restormer[20] 33.72/0.930 30.67/0.865 27.63/0.792 27.78/0.958 33.78/0.958 30.75/0.901 26.13
(ECCV’22)NAFNet[19] 33.03/0.918 30.47/0.865 27.12/0.754 24.11/0.928 33.64/0.956 29.67/0.844 17.11
(TPAMI’23)FSNet*[21] 33.81/0.930 30.84/0.872 27.69/0.792 29.14/0.968 35.61/0.969 31.42/0.906 13.28
(ECCV’24)MambaIR*[40] 33.88/0.931 30.95/0.874 27.74/0.793 29.57/0.970 35.42/0.969 31.51/0.907 26.78
(TPAMI’19)DL[71] 33.05/0.914 30.41/0.861 26.90/0.740 26.92/0.391 32.62/0.931 29.98/0.875 2.09
(CVPR’22)AirNet[23] 33.92/0.932 31.26/0.888 28.00/0.797 27.94/0.962 34.90/0.967 31.20/0.910 8.93
(CVPR’23)IDR*[34] 33.89/0.931 31.32/0.884 28.04/0.798 29.87/0.970 36.03/0.971 31.83/0.911 15.34
(ArXiv’23)ProRes[25] 32.10/0.907 30.18/0.863 27.58/0.779 28.38/0.938 33.68/0.954 30.38/0.888 370.63
(ArXiv’23)NDR[35] 34.01/0.932 31.36/0.887 28.10/0.798 28.64/0.962 35.42/0.969 31.51/0.910 -
(NeurIPS’23)PromptIR[26] 33.98/0.933 31.31/0.888 28.06/0.799 30.58/0.974 36.37/0.972 32.06/0.913 32.96
(IJCV’24)Gridformer*[37] 33.93/0.931 31.37/0.887 28.11/0.801 30.37/0.970 37.15/0.972 32.19/0.912 34.07
(ECCV’24)InstructIR[28] 34.15/0.933 31.52/0.890 28.30/0.804 30.22/0.959 37.98/0.978 32.43/0.913 15.80
Perceive-IR (Ours) 34.13/0.934 31.53/0.890 28.31/0.804 30.87/0.975 38.29/0.980 32.63/0.917 42.02
Input AirNet PromptIR InstructIR Ours GT
Fig.5. VisualcomparisonsofPerceive-IRwithstate-of-the-artall-in-onemethodsforAll-in-One(“N+H+R”)setting.Zoom-inforbestview.
deraining tasks, it does not perform as well as the best task- may be due to the fact that our method does not explicitly
specific methods (i.e., HI-Diff [72] and Retinexformer [15]) explore frequency domain information and illumination mod-
and the best general methods (i.e., FSNet [21] and MIRNet els, making it less effective in capturing critical degradation
[76]) on deblurring and low-light enhancement tasks. This features, as well as handling a wide range of light intensities
gnisioneD
gnizaheD
gniniareD
cfiicepS
lareneG
enO-ni-llAJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 8
TABLEIII
PERFORMANCECOMPARISONINALL-IN-ONE(“N+H+R+B+L”)SETTINGWITHSTATE-OF-THE-ARTSTASK-SPECIFIC,GENERAL,ANDALL-IN-ONE
IMAGERESTORATIONMETHODS.FOLLOW[34],DENOISINGRESULTSAREREPORTEDFORTHENOISELEVELσ=25.THEREPORTEDRESULTSARE
PARTIALLYBASEDONIDR[34].*DENOTESAMODELTHATHASBEENRETRAINED.
Denoising Dehazing Deraining Deblurring Low-light
Type Method Average Params(M)
CBSD68[61] SOTS[64] Rain100L[65] GoPro[11] LOL[13]
(AAAI’23)ADFNet*[70] 31.15/0.882 24.18/0.928 32.97/0.943 25.79/0.781 21.15/0.823 27.05/0.871 7.65
(TIP’23)DehazeFormer*[6] 30.89/0.880 25.31/0.937 33.68/0.954 25.93/0.785 21.31/0.819 27.42/0.875 25.44
(CVPR’23)DRSformer*[9] 30.97/0.881 24.66/0.931 33.45/0.953 25.56/0.780 21.77/0.821 27.28/0.873 33.72
(NeurIPS’23)HI-Diff*[72] 30.61/0.878 25.09/0.935 33.26/0.951 26.48/0.800 22.01/0.822 27.49/0.877 23.99
(ICCV’23)Retinexformer*[15] 30.84/0.880 24.81/0.933 32.68/0.940 25.09/0.779 22.76/0.834 27.24/0.873 1.61
(ICCVW’21)SwinIR[17] 30.59/0.868 21.50/0.891 30.78/0.923 24.52/0.773 17.81/0.723 25.04/0.835 0.91
(TPAMI’22)MIRNet-v2[18] 30.97/0.881 24.03/0.927 33.89/0.954 26.30/0.799 21.52/0.815 27.34/0.875 5.86
(CVPR’22)DGUNet[38] 31.10/0.883 24.78/0.940 36.62/0.971 27.25/0.837 21.87/0.823 28.32/0.891 17.33
(CVPR’22)Restormer[20] 31.49/0.884 24.09/0.927 34.81/0.962 27.22/0.829 20.41/0.806 27.60/0.881 26.13
(ECCV’22)NAFNet[19] 31.02/0.883 25.23/0.939 35.56/0.967 26.53/0.808 20.49/0.809 27.76/0.881 17.11
(TPAMI’23)FSNet*[21] 31.33/0.883 25.53/0.943 36.07/0.968 28.32/0.869 22.29/0.829 28.71/0.898 13.28
(ECCV’24)MambaIR*[40] 31.41/0.884 25.81/0.944 36.55/0.971 28.61/0.875 22.49/0.832 28.97/0.901 26.78
(TPAMI’19)DL[71] 23.09/0.745 20.54/0.826 21.96/0.762 19.86/0.672 19.83/0.712 21.05/0.743 2.09
(ECCV’22)TAPE[33] 30.18/0.855 22.16/0.861 29.67/0.904 24.47/0.763 18.97/0.621 25.09/0.801 1.07
(CVPR’22)Transweather[73] 29.00/0.841 21.32/0.885 29.43/0.905 25.12/0.757 21.21/0.792 25.22/0.836 37.93
(CVPR’22)AirNet[23] 30.91/0.882 21.04/0.884 32.98/0.951 24.35/0.781 18.18/0.735 25.49/0.846 8.93
(CVPR’23)IDR[34] 31.60/0.887 25.24/0.943 35.63/0.965 27.87/0.846 21.34/0.826 28.34/0.893 15.34
(NeurIPS’23)PromptIR*[26] 31.47/0.886 26.54/0.949 36.37/0.970 28.71/0.881 22.68/0.832 29.15/0.904 32.96
(IJCV’24)Gridformer*[37] 31.45/0.885 26.79/0.951 36.61/0.971 29.22/0.884 22.59/0.831 29.33/0.904 34.07
(ECCV’24)InstructIR[28] 31.40/0.887 27.10/0.956 36.84/0.973 29.40/0.886 23.00/0.836 29.55/0.907 15.80
Perceive-IR(Ours) 31.44/0.887 28.19/0.964 37.25/0.977 29.46/0.886 22.88/0.833 29.84/0.909 42.02
TABLEIV TABLEV
DENOISINGPERFORMANCECOMPARISONINTHEONE-BY-ONESETTING PERFORMANCECOMPARISONINTHEONE-BY-ONESETTINGON
ONDENOISINGTASK.⋄,⋄,AND⋄DENOTETASK-SPECIFIC,GENERAL, DEHAZING,DERAINING,DEBLURRING,ANDLOW-LIGHTENHANCEMENT
ANDALL-IN-ONEIMAGERESTORATIONMETHODS,RESPECTIVELY.THE TASKS.⋄,⋄,AND⋄DENOTETASK-SPECIFIC,GENERAL,ANDALL-IN-ONE
RESULTSINTHETABLEAREMAINLYBASEDONIDR[34]. IMAGERESTORATIONMETHODS,RESPECTIVELY.THEBESTOVERALL
RESULTSAREMARKEDWITHBOLDANDTHEBESTRESULTSFROMEACH
Method
CBSD68[61] Urban100[62] Kodak24[63] METHODCATEGORYAREINBOLD.
15 25 50 15 25 50 15 25 50
⋄DnCNN[1] 33.90 31.24 27.95 32.98 30.81 27.59 34.60 32.14 28.95 Dehazing Deraining
⋄FFDNet[2] 33.87 31.21 27.96 33.83 31.40 28.05 34.63 32.13 28.98 Method SOTS[64] Method Rain100L[65]
⋄BRDNet[3] 34.10 31.43 28.16 34.42 31.99 28.56 34.70 32.17 28.99
⋄DehazeNet[4] 22.46/0.851 ⋄UMR[7] 32.39/0.921
⋄ADFNet*[70] 34.21 31.60 28.19 34.50 32.13 28.71 34.77 32.22 29.06
⋄AODNet[74] 20.29/0.877 ⋄MSPFN[8] 33.50/0.948
⋄SwinIR[17] 33.31 30.59 27.13 32.79 30.18 26.52 33.89 31.32 27.93
⋄MIRNet-v2[18] 33.66 30.97 27.66 33.30 30.75 27.22 34.29 31.81 28.55 ⋄FDGAN[5] 23.15/0.921 ⋄LPNet[69] 33.61/0.958
⋄DGUNet[38] 33.85 31.10 27.92 33.67 31.27 27.94 34.56 32.10 28.91 ⋄DehazeFormer*[6] 31.78/0.977 ⋄DRSformer*[9] 38.14/0.983
⋄Restormer[20] 34.03 31.49 28.11 33.72 31.26 28.03 34.78 32.37 29.08
⋄Restormer[20] 30.87/0.969 ⋄Restormer[20] 36.74/0.978
⋄NAFNet[19] 33.67 31.02 27.73 33.14 30.64 27.20 34.27 31.80 28.62
⋄FSNet*[21] 34.09 31.55 28.12 33.88 31.31 28.07 34.75 32.38 29.10 ⋄NAFNet*[19] 30.98/0.970 ⋄NAFNet*[19] 36.63/0.977
⋄TAPE[33] 32.86 30.18 26.63 32.19 29.65 25.87 33.24 30.70 27.19 ⋄FSNet*[21] 31.11/0.971 ⋄FSNet*[21] 37.27/0.980
⋄AirNet[23] 34.14 31.48 28.23 34.40 32.10 28.88 34.81 32.44 29.10 ⋄AirNet[23] 23.18/0.900 ⋄AirNet[23] 34.90/0.977
⋄IDR[34] 34.11 31.60 28.14 33.82 31.29 28.07 34.78 32.42 29.13
⋄PromptIR[26] 31.31/0.973 ⋄PromptIR[26] 37.04/0.979
⋄PromptIR[26] 34.34 31.71 28.49 34.77 32.49 29.39 - - -
⋄Perceive-IR 34.38 31.74 28.53 34.86 32.55 29.42 34.84 32.50 29.16 ⋄Perceive-IR 31.65/0.977 ⋄Perceive-IR 38.41/0.984
andilluminationconditions.Additionally,theuseofRestormer Deblurring Low-light
Method Method
as a generator of “medium” quality images during the prompt GoPro[11] LOL[13]
learningstagemightinterferewiththelearningprocessonthe ⋄Nahetal.[11] 29.08/0.914 ⋄Retinex-Net[13] 16.77/0.560
⋄DeblurGAN[10] 28.70/0.858 ⋄URetinex[14] 21.33/0.835
deblurring and low-light enhancement tasks. Despite produc-
⋄Stripformer[12] 33.08/0.962 ⋄SMG[75] 23.81/0.809
ing images of decent quality, they do not match the mediocre
⋄HI-Diff[72] 33.33/0.964 ⋄Retinexformer[15] 25.16/0.845
quality expected, which affects the restoration performance
⋄MPRNet[16] 32.66/0.959 ⋄MIRNet[76] 24.14/0.835
in these tasks. Despite these limitations, our Perceive-IR
⋄Restormer[20] 32.92/0.961 ⋄Restormer[20] 22.43/0.823
consistently outperforms SOTA all-in-one methods across five
⋄FSNet[21] 33.29/0.963 ⋄DiffIR[22] 23.15/0.828
tasks under One-by-One setting.
⋄AirNet*[23] 31.64/0.945 ⋄AirNet*[23] 21.07/0.811
D. Visual Results ⋄PromptIR*[26] 32.41/0.956 ⋄PromptIR*[26] 22.55/0.826
Figs.5-6presenttherestorationresultsobtainedbystate-of- ⋄Perceive-IR 32.83/0.960 ⋄Perceive-IR 23.24/0.838
the-artall-in-onemethodsforbothAll-in-One(“N+H+R”)and
One-by-One setting. In noisy scenarios, Perceive-IR produces Fig. 5). In challenging hazy scenes, as seen in row 3, AirNet
clear and sharp denoised outputs, preserving fine textures and PromptIR generate results with low color fidelity, while
compared to AirNet [23], PromptIR [26], and InstructIR [28] InstructIRproducesblurrierresults.Inrow4,thesecomparison
(e.g., the head and stomach texture parts in rows 1 and 2 of methods generate results with additional offsets. Conversely,
cfiicepS
lareneG
enO-ni-llAJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 9
Input AirNet PromptIR Ours GT
Fig.6. VisualcomparisonsofPerceive-IRwithstate-of-the-artall-in-onemethodsforOne-by-Onesetting.Zoom-inforbestview.
TABLEVI TABLEVII
EFFECTIVENESSOFDIFFERENTCOMPONENTSONTHERAIN100L[65] EFFECTIVENESSOFTHEDIFFERENTLOSSFUNCTIONSONTHERAIN100L
DATASET. [65]DATASET.
Index Prompt DINO-v2 PGM CFE PSNR↑ SSIM↑ Index L clip L dpl L cl PSNR↑ SSIM↑
(a) ✗ ✗ ✗ ✗ 37.15 0.975 (a) ✗ ✗ ✗ 37.30 0.978
(b) ✓ ✗ ✗ ✗ 37.37 0.979 (b) ✓ ✗ ✗ 37.51 0.980
(c) ✗ ✓ ✗ ✗ 37.32 0.979 (c) ✗ ✓ ✗ 37.42 0.979
(d) ✗ ✗ ✓ ✗ 37.28 0.978 (d) ✗ ✗ ✓ 37.37 0.979
(e) ✗ ✗ ✗ ✓ 37.23 0.977 (e) ✗ ✓ ✓ 37.46 0.979
(f) ✓ ✓ ✗ ✗ 37.52 0.981 (f) ✓ ✓ ✗ 37.61 0.981
(g) ✗ ✗ ✓ ✓ 37.38 0.979 (g) ✓ ✗ ✓ 37.55 0.981
(h) ✗ ✓ ✓ ✓ 37.45 0.979 (h) ✓ ✓ ✓ 37.76 0.982
(i) ✓ ✗ ✓ ✓ 37.50 0.980 Allablationexperimentsareperformedontheimagederaining
(j) ✓ ✓ ✓ ✓ 37.76 0.982
task (Rain100L [65]) by training models for 100K iterations
the results generated by our method maintain good color and except for special instructions.
structural integrity. In deraining task, as shown in the iron- Effects of Different Components. As shown in Tab. VI, we
frame region in row 6, both AirNet, PromptIR, and InstructIR evaluate the effectiveness of various components through a
introduced additional artifacts. In contrast, our approach pro- comparison with baseline method (index a) that excludes our
duced more realistic result. The same trend is illustrated in modules. The baseline adopts Restormer [20] as its backbone.
Fig. 6. These findings demonstrate the effectiveness of our “Prompt” denotes the prompt learning stage. Specifically,
method. without prompt learning and DINO-v2 (index g), the average
E. Ablation Study PSNR performance is reduced by 0.38 dB. Similarly, without
Weconductseveralablationexperimentstodemonstratethe PGM and CFE (index f), the performance is reduced by
effectiveness of each component in the proposed Perceive-IR. 0.24 dB. When compared to other modules, the improvement
gnisioneD
gnizaheD
gniniareDJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 10
0.05, and 0.1, respectively.
Effects of Prompt Number and Initialization Schemes In
thisstudy,weexaminetheimpactsofdifferentpromptnumber
andpromptinitializationsettings.Weconsiderthreescenarios:
two, three, and four pairs of prompt-image. Specifically, we
Input explorethreeconfigurationsforpromptinitialization,i)Fixed:
ℒ1 ℒ1+ℒ𝑐𝑐𝑐𝑐 The prompts are fixed throughout the entire training process.
ii)Randominitialization:Thepromptsareinitiallyrandomized
and then learned from there. iii) Partial random initialization:
Part of the token is initialized randomly while one token is
fixed as a specific word. For example, a five-token prompt
GT
is initialized as “×, ×, ×, ×, excellent”, where “×” denotes
Fig.7. ℒV1i +su ℒa𝑐𝑐𝑐𝑐l +co ℒm𝑐𝑐𝑐𝑐𝑖𝑖𝑖𝑖parisonofth ℒe1+re ℒs𝑐𝑐t𝑐𝑐o +re ℒd𝑐𝑐𝑐𝑐𝑖𝑖i𝑖𝑖m +a ℒg𝑑𝑑e𝑖𝑖𝑐𝑐sobtainedusingtheindividual
lossschemesandtheproposedscheme. randominitialization.Andtheentirepromptisupdatedduring
prompt learning.
37.90 0.982 PSNR 0.982
SSIM As shown in Table VIII, using three prompts in the CLIP
0.981 0.981
37.80 0.981 model,withthesameinitializationstrategy,outperformsusing
0.980 0.980 37.76 0.980 two or four prompts. The use of two prompt-image pairs
37.70 0.980
37.70 0.979 0.979 0.979 37.68 may result in inadequate model discrimination, while four
37.60 37.63 0.979 pairs could offer too much detail, negatively affecting the
37.60
37.58 37.55 37.57 portrayal of image quality. This suggests that an appropriate
37.50 37.52 0.978
number of prompts is beneficial in providing a more precise
37.40 0.977 guidefortherestorationprocess.Inallpromptconfigurations,
“Partial random initialization” consistently outperforms both
37.30 0.976
“Fixed” and “Random initialization”. The use of fixed or
random prompts can lead to performance degradation due to
the domain gap between the pre-training data of the CLIP
Fig. 8. Comparison of PSNR and SSIM with different weight between
different loss function on the Rain100L [65] dataset. red, blue, and green model and the specifics of our task. Random prompts slightly
aretheweightsλ1,λ2,andλ3 inEq.13,respectively. enhanceperformancebyeasinglearningconstraintscompared
brought by prompt learning is more significant, as shown withfixedprompts.Therefore,thepartialrandominitialization
in (indexs b-e). This suggests that the proposed CLIP-based strategy achieves optimal results, likely because it provides
promptlearningstrategyiscrucialtotheperformance.Overall, more effective guidance and ensuring ample learning space
our model (index j) achieves a significant average PSNR across various prompt scenarios. We additionally consider
improvement of 0.61 dB than baseline, which is attributed to the formulation of the specified words. As seen in (indexs
the effectiveness of each proposed component. g, i) and (indexs l, m), such vague definitions are prone to
Effects of Different Loss Functions. As shown in Tab. appear in the data of the pre-trained model, which reduces
VII, we determine the effectiveness of Perceive-IR under the performance. When the prompt and image quality do not
different loss functions. The baseline (index a) use only align, as in (indexs h, i), it disrupts the prompt learning stage,
pixel-level reconstruction loss L . Specifically, compared leadingtoasignificantdropinperformance.Inall,ourscheme
rec
to the baseline, the approach incorporating L , L , and (index i) achieves the best results and proves its superiority.
clip dpl
L simultaneously achieves a PSNR improvement of 0.46 Effects of Different Combinations of Degradation. In this
cl
dB. Furthermore, by integrating the perceptual capabilities study,weassesstheperformanceofPerceive-IRundervarious
of CLIP, the utilization of L (index b) results in PSNR combinations of degradation types. As demonstrated in Tab.
clip
improvement of 0.09 dB and 0.14 dB, respectively, over IX, the performance of the model is optimal on a single
the use of L (index c) and L (index d) alone. These task. With the number of tasks incremented, the performance
dpl cl
findings demonstrate that combining multiple loss functions of each task oscillates below the optimal value. In addition,
and leveraging CLIP’s perceptual properties can significantly there may be some interesting phenomena under different
elevate the quality of image restoration. Fig. 7 illustrates the combinations of degradation. For example, models trained in
visualresultsofvariousschemesusingindividuallossmodules the “N+H+L” setting show superior performance in denoising
and our proposed scheme. As the proposed loss modules are and dehazing compared to those trained in the “N+H+B”
successivelyadded,therestoredimagemorecloselyresembles setting. Similarly, models trained in the “N+H+R+L” setting
the ground truth. hasbetterperformanceondenoisinganddehazingthanmodels
In addition, we further explored the effect of weights trained in the “N+H+R+B” setting, but worse performance on
between different loss functions. As shown in Fig. 8, peak thederainingtask.Thismaybeduetothedistributionoflow-
performance is observed around (λ , λ , λ ) = (0.1, 0.05, light images may contain more noise and haze-like artifacts,
1 2 3
0.1). Similarly, sub-optimal performance is obtained when λ which can enhance the model’s ability to handle these types
1
and λ are larger than λ , e.g., (0.05, 0.025, 0.05) and (0.2, of degradation. On the other hand, deraining requires specific
3 2
0.1, 0.2). In other cases, the performance degradation is more featuresrelatedtorainstreaks,whichmightnotbeasprevalent
significant.Thus,inourmethod,λ ,λ ,andλ aresetas0.1, in low-light conditions.
1 2 3
RNSP MISSJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 11
TABLEVIII
PERFORMANCECOMPARISONSONTHERAIN100L[65]DATASETAMONGDIFFERENTPROMPTINITIALIZATIONSETTINGSFORTHECLIPMODEL.
Type Index PromptSetting PSNR↑ SSIM↑
(a) Fixed(“excellent”/“terrible”) 37.36 0.979
Twopairsofprompt-image (b) Randominitialization 37.49 0.980
(c) Partialrandominitialization(“excellent”/“terrible”) 37.56 0.981
(d) Fixed(“excellent”/“mediocre”/“terrible”) 37.45 0.979
(e) Fixed(“good”/“moderate”/“bad”) 37.50 0.980
(f) Randominitialization 37.52 0.981
Threepairsofprompt-image
(g) Partialrandominitialization(“good”/“moderate”/“bad”) 37.68 0.982
(h) Partialrandominitialization(“terrible”/“moderate”/“excellent”) 37.43 0.979
(i) Partialrandominitialization(“excellent”/“mediocre”/“terrible”)(Ours) 37.76 0.982
(j) Fixed(“excellent”/“slightlybetter”/“slightlyworse”/“terrible”) 37.44 0.979
(k) Randominitialization 37.56 0.981
Fourpairsofprompt-image
(l) Partialrandominitialization(“excellent”/“good”/“bad”/“terrible”) 37.63 0.982
(m) Partialrandominitialization(“excellent”/“slightlybetter”/“slightlyworse”/“terrible”) 37.67 0.982
TABLEIX
PERFORMANCEOFTHEPROPOSEDPERCEIVE-IR,WHENCONDUCTEDONVARIOUSCOMBINATIONSOFDEGRADATIONTYPES.NUMBEROF
COMBINATIONSOFTASKSFROM1TO5.
Degradation Denoising (CBSD68 [61]) Dehazing Deraining Deblurring Low-light
N H R B L σ=15 σ=25 σ=50 SOTS [64] Rain100L [65] GoPro [11] LOL [13]
✓ ✗ ✗ ✗ ✗ 34.38/0.939 31.74/0.898 28.53/0.813 - - - -
✗ ✓ ✗ ✗ ✗ - - - 31.65/0.977 - - -
✗ ✗ ✓ ✗ ✗ - - - - 38.41/0.984 - -
✗ ✗ ✗ ✓ ✗ - - - - - 32.83/0.960 -
✗ ✗ ✗ ✗ ✓ - - - - - - 23.24/0.838
✓ ✓ ✗ ✗ ✗ 34.35/0.939 31.70/0.897 28.46/0.812 30.79/0.975 - - -
✓ ✗ ✓ ✗ ✗ 34.31/0.937 31.66/0.895 28.41/0.810 - 38.11/0.980 - -
✗ ✓ ✓ ✗ ✗ - - - 30.92/0.976 38.23/0.981 - -
✗ ✗ ✗ ✓ ✓ - - - - - 32.33/0.953 22.97/0.835
✓ ✓ ✓ ✗ ✗ 34.13/0.934 31.53/0.890 28.31/0.804 30.87/0.975 38.29/0.980 - -
✓ ✓ ✗ ✓ ✗ 34.16/0.934 31.55/0.890 28.32/0.803 30.11/0.972 - 31.29/0.934 -
✓ ✓ ✗ ✗ ✓ 34.20/0.935 31.57/0.891 28.33/0.804 30.36/0.973 - - 22.90/0.834
✓ ✓ ✓ ✓ ✗ 34.11/0.932 31.50/0.888 28.26/0.802 28.87/0.967 37.83/0.979 30.31/0.911 -
✓ ✓ ✓ ✗ ✓ 34.13/0.933 31.52/0.889 28.27/0.802 29.62/0.970 37.49/0.978 - 22.76/0.831
✓ ✓ ✗ ✓ ✓ 34.10/0.932 31.48/0.888 28.25/0.801 29.23/0.968 - 29.89/0.897 22.81/0.834
✓ ✓ ✓ ✓ ✓ 34.04/0.931 31.44/0.887 28.19/0.801 28.19/0.964 37.25/0.977 29.46/0.886 22.88/0.833
EffectsofRestorationModelsforPromptLearning.Inthis TABLEX
part, we explore the effects of different restoration methods PERFORMANCECOMPARISONSONTHERAIN100L[65]DATASETAMONG
DIFFERENTRESTORATIONMODELSFORTHEPROMPTLEARNINGSTAGE.
during the prompt learning stage. As shown in Tab. X,
the better all-in-one restoration model like PromptIR [26] Method PSNR↑ SSIM↑
yields poorer results than AirNet [23], and superior general Perceive-IR(MPRNet[16]) 37.68 0.982
Perceive-IR(AirNet[23]) 37.64 0.981
restoration model like Restormer [20] perform better than
Perceive-IR(PromptIR[26]) 37.57 0.981
MPRNet [16]. These observations may be attributed to two Perceive-IR(Restormer[20])(Ours) 37.76 0.982
factors. Firstly, the quality of images restored by PromptIR
TABLEXI
surpasses the “mediocre” standard, which lead to confusion PERFORMANCECOMPARISONSONUNSEENNOISELEVELOFσ=60,100.
during prompt learning and potentially undermine perfor-
CBSD68[61] Urban100[62]
mance.Secondly,astheserestorationmethodsgenerateimages Method
σ=60 σ=100 σ=60 σ=100
of notably high quality, the prompt “mediocre” may converge
AirNet[23] 26.01 14.29 25.11 14.23
towards the prompt “excellent”. This convergence could di- PromptIR[26] 26.71 20.23 27.24 20.94
minish the effectiveness of the “mediocre” prompt, reducing Perceive-IR 27.11 20.67 27.59 21.52
the distinctiveness of the three prompts to essentially two.
F. Generalization
When comparing Restormer and MPRNet, the opposite effect
isobservedontherelativelypoorerMPRNet.Insummary,the Follow[26],weutilizetheall-in-onemethodstrainedunder
“medium” quality image restored by appropriate restoration thenoiselevelsofσ ∈{15,25,50}totestontheunseennoise
method are more beneficial for the prompt learning stage. levels of σ ∈ {60,100}. In addition, we evaluate the perfor-
manceofthesemethodsunder“N+H+R+B+L”setting,specifi-
callyfocusingonthetraining-unseenunderwaterenhancementJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 12
TABLEXII
PERFORMANCECOMPARISONSONTRAINING-UNSEENUNDERWATER
leverage pre-trained large vision models (e.g., DINO-v2) to
IMAGEENHANCEMENTTASK.WEUSETHEALL-IN-ONEMETHODS
(PRE-TRAINEDUNDER“N+H+R+B+L”TRAININGSETTING)TO construct semantic guidance module to perceive the reliable
DIRECTLYTESTONTHEUIEB[77]DATASET.THEPROMPTUSEDIN content to promote the restoration process. Finally, we intro-
INSTUCTIR[28]IS“Thisunderwaterimageispoor,pleaseenhanceit.”.
duce a quality-aware learning strategy along with difficulty-
UIEB[77] C60[77] adaptiveperceptuallosstorealizefine-grainedqualitycontrol.
Method
PSNR↑ SSIM↑ LPIPS↓ UCIQE↑ UIQM↑ ExtensiveexperimentshavedemonstratedthatourPerceive-IR
PromptIR[26] 20.19 0.827 0.256 0.527 2.305
achievesstate-of-the-artresultsonall-in-oneimagerestoration
Gridformer[37] 20.48 0.846 0.243 0.540 2.517
tasksanddemonstratesitssuperiorgeneralisationabilitywhen
InstructIR[28] 21.07 0.872 0.189 0.552 2.489
Perceive-IR 21.75 0.891 0.168 0.561 2.558 dealing with unseen tasks.
REFERENCES
TABLEXIII
PERFORMANCECOMPARISONSOFPERCEIVE-IRWITHDIFFERENT
[1] K.Zhang,W.Zuo,Y.Chen,D.Meng,andL.Zhang,“Beyondagaussian
RESTORATIONBACKBONESUNDERTHE“N+H+R”AND“N+H+R+B+L” denoiser: Residual learning of deep cnn for image denoising,” TIP,
TRAININGSETTINGSVERSUSSTATE-OF-THE-ARTALL-IN-ONEIMAGE
vol.26,no.7,pp.3142–3155,2017.
RESTORATIONMETHODS.
[2] K. Zhang, W. Zuo, and L. Zhang, “Ffdnet: Toward a fast and flexible
N+H+R N+H+R+B+L
solutionforcnn-basedimagedenoising,”TIP,vol.27,no.9,pp.4608–
Method PSNR↑ SSIM↑ PSNR↑ SSIM↑ Params(M) 4622,2018.
[3] C. Tian, Y. Xu, and W. Zuo, “Image denoising using deep cnn with
AirNet[23] 31.20 0.910 25.49 0.846 8.93
batchrenormalization,”NN,vol.121,pp.461–473,2020.
PromptIR[26] 32.06 0.913 29.15 0.904 32.96
[4] B. Cai, X. Xu, K. Jia, C. Qing, and D. Tao, “Dehazenet: An end-to-
InstructIR[28] 32.43 0.913 29.55 0.907 15.80
end system for single image haze removal,” TIP, vol. 25, no. 11, pp.
Perceive-IRNAFNet 32.52 0.915 29.71 0.908 21.73
5187–5198,2016.
Perceive-IRRestormer 32.63 0.917 29.84 0.909 42.02
[5] Y.Dong,Y.Liu,H.Zhang,S.Chen,andY.Qiao,“Fd-gan:Generative
adversarialnetworkswithfusion-discriminatorforsingleimagedehaz-
task. Tabs. XI and XII show that our Perceive-IR outperforms
ing,”inAAAI,vol.34,no.07,2020,pp.10729–10736.
other all-in-one methods on both unseen noise levels and the [6] Y. Song, Z. He, H. Qian, and X. Du, “Vision transformers for single
unseen task, demonstrates superior generalization capability. imagedehazing,”TIP,vol.32,pp.1927–1941,2023.
[7] R. Yasarla and V. M. Patel, “Uncertainty guided multi-scale residual
G. Discussion and Limitation learning-using a cycle spinning cnn for single image de-raining,” in
CVPR,2019,pp.8405–8414.
To validate the adaptation of our method with different
[8] K.Jiang,Z.Wang,P.Yi,C.Chen,B.Huang,Y.Luo,J.Ma,andJ.Jiang,
restorationbackbone,weusingNAFNetasthebackboneinthe “Multi-scaleprogressivefusionnetworkforsingleimagederaining,”in
restoration stage. As demonstrated in the Tab. XIII, Perceive- CVPR,2020,pp.8346–8355.
[9] X. Chen, H. Li, M. Li, and J. Pan, “Learning a sparse transformer
IR outperforms the state-of-the-art all-in-one methods
NAFNet networkforeffectiveimagederaining,”inCVPR,June2023,pp.5896–
such as PromptIR [26] and InstrutIR [28], by achieving an 5905.
average PSNR that is 0.46/0.56 dB and 0.09/0.16 dB higher [10] O. Kupyn, V. Budzan, M. Mykhailych, D. Mishkin, and J. Matas,
“Deblurgan:Blindmotiondeblurringusingconditionaladversarialnet-
under “N+H+R” and “N+H+R+B+L” settings, respectively.
works,”inCVPR,2018,pp.8183–8192.
Note that the number of parameters in Perceive-IR depends [11] S.Nah,T.HyunKim,andK.MuLee,“Deepmulti-scaleconvolutional
heavily on the chosen backbone model, as shown in Perceive- neural network for dynamic scene deblurring,” in CVPR, 2017, pp.
3883–3891.
IR and Perceive-IR . This suggests that our ap-
NAFNet Restormer [12] F.-J.Tsai,Y.-T.Peng,Y.-Y.Lin,C.-C.Tsai,andC.-W.Lin,“Stripformer:
proachhasthepotentialtoserveasamorelightweightsolution Striptransformerforfastimagedeblurring,”inECCV,2022,pp.146–
and could be integrated as a plug-and-play module into more 162.
[13] C. Wei, W. Wang, W. Yang, and J. Liu, “Deep retinex decomposition
advanced restoration models in the future.
forlow-lightenhancement,”inBMVC,2018.
AlthoughourPerceive-IRhasachievedsuperiorgeneraliza- [14] W.Wu,J.Weng,P.Zhang,X.Wang,W.Yang,andJ.Jiang,“Uretinex-
tion ability and scalability in addressing multiple degradation, net:Retinex-baseddeepunfoldingnetworkforlow-lightimageenhance-
ment,”inCVPR,2022,pp.5901–5910.
its flexibility in blind real-world conditions may be limited.
[15] Y.Cai,H.Bian,J.Lin,H.Wang,R.Timofte,andY.Zhang,“Retinex-
Moreover, while Perceive-IR demonstrates superiority over former: One-stage retinex-based transformer for low-light image en-
the state-of-the-art all-in-one and general image restoration hancement,”inICCV,2023.
[16] S.W.Zamir,A.Arora,S.Khan,M.Hayat,F.S.Khan,M.-H.Yang,and
methods, it still falls short when compared to the most
L. Shao, “Multi-stage progressive image restoration,” in CVPR, 2021,
advancedstate-of-the-arttask-specificmethods,particularlyin pp.14816–14826.
tasks such as deblurring, and low-light enhancement. Lastly, [17] J. Liang, J. Cao, G. Sun, K. Zhang, L. Van Gool, and R. Timofte,
“Swinir:Imagerestorationusingswintransformer,”inICCV,2021,pp.
the interrelationships among the various degradation are not
1833–1844.
sufficientlyclear,forexample,asshowninTab.IX,themodel [18] S. W. Zamir, A. Arora, S. Khan, M. Hayat, F. S. Khan, M.-H. Yang,
performs better on the dehazing task in the “N+H+R+B” andL.Shao,“Learningenrichedfeaturesforfastimagerestorationand
enhancement,”TPAMI,vol.45,no.2,pp.1934–1948,2022.
settingcomparedtothe“N+H+R+L”setting,yetthederaining
[19] L. Chen, X. Chu, X. Zhang, and J. Sun, “Simple baselines for image
performance is degraded. restoration,”inECCV,2022,pp.17–33.
[20] S.W.Zamir,A.Arora,S.Khan,M.Hayat,F.S.Khan,andM.-H.Yang,
V. CONCLUSION
“Restormer:Efficienttransformerforhigh-resolutionimagerestoration,”
This paper proposes Perceice-IR, an approach for learning inCVPR,2022,pp.5728–5739.
[21] Y.Cui,W.Ren,X.Cao,andA.Knoll,“Imagerestorationviafrequency
toperceivedegradationbetterforall-in-oneimagerestoration.
selection,”TPAMI,2023.
Specifically, we leverage prompt learning technique to ac- [22] B. Xia, Y. Zhang, S. Wang, Y. Wang, X. Wu, Y. Tian, W. Yang, and
quireafine-grainedqualityperceivercapableofdistinguishing L. Van Gool, “Diffir: Efficient diffusion model for image restoration,”
inICCV,2023,pp.13095–13105.
three-tier quality levels by constraining the prompt-image
[23] B. Li, X. Liu, P. Hu, Z. Wu, J. Lv, and X. Peng, “All-in-one image
similarity in the CLIP perception space. In addition, we fully restorationforunknowncorruption,”inCVPR,2022,pp.17452–17462.JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 13
[24] D. Park, B. H. Lee, and S. Y. Chun, “All-in-one image restoration for [51] Z. Wang, Y. Lu, Q. Li, X. Tao, Y. Guo, M. Gong, and T. Liu, “Cris:
unknown degradations using adaptive discriminative filters for specific Clip-drivenreferringimagesegmentation,”inCVPR,2022,pp.11686–
degradations,”inCVPR,2023,pp.5815–5824. 11695.
[25] J. Ma, T. Cheng, G. Wang, Q. Zhang, X. Wang, and L. Zhang, [52] Y. Rao, W. Zhao, G. Chen, Y. Tang, Z. Zhu, G. Huang, J. Zhou, and
“Prores:Exploringdegradation-awarevisualpromptforuniversalimage J.Lu,“Denseclip:Language-guideddensepredictionwithcontext-aware
restoration,”arXivpreprintarXiv:2306.13653,2023. prompting,”inCVPR,2022,pp.18082–18091.
[26] V. Potlapalli, S. W. Zamir, S. Khan, and F. S. Khan, “Promptir: [53] M.Oquab,T.Darcet,T.Moutakanni,H.Vo,M.Szafraniec,V.Khalidov,
Prompting for all-in-one blind image restoration,” in NeurIPS, 2023, P. Fernandez, D. Haziza, F. Massa, A. El-Nouby et al., “Dinov2:
pp.71275–71293. Learning robust visual features without supervision,” arXiv preprint
[27] X. Lin, C. Ren, K. C. Chan, L. Qi, J. Pan, and M.-H. Yang, “Multi- arXiv:2304.07193,2023.
task image restoration guided by robust dino features,” arXiv preprint [54] C. Beilei, I. Mobarakol, B. Long, and R. Hongliang, “Surgical-dino:
arXiv:2312.01677,2023. Adapterlearningoffoundationmodelfordepthestimationinendoscopic
[28] M.V.Conde,G.Geigle,andR.Timofte,“High-qualityimagerestoration surgery,”arXivpreprintarXiv:2401.06013,2024.
followinghumaninstructions,”inECCV,2024. [55] J.Zhang,C.Herrmann,J.Hur,L.PolaniaCabrera,V.Jampani,D.Sun,
[29] Z. Luo, F. K. Gustafsson, Z. Zhao, J. Sjo¨lund, and T. B. Scho¨n, andM.-H.Yang,“Ataleoftwofeatures:Stablediffusioncomplements
“Controlling vision-language models for universal image restoration,” dinoforzero-shotsemanticcorrespondence,”inNeurIPS,vol.36,2024.
arXivpreprintarXiv:2310.01018,2023. [56] H.Wu,Y.Qu,S.Lin,J.Zhou,R.Qiao,Z.Zhang,Y.Xie,andL.Ma,
[30] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, “Contrastive learning for compact single image dehazing,” in CVPR,
G.Sastry,A.Askell,P.Mishkin,J.Clarketal.,“Learningtransferable 2021,pp.10551–10560.
visual models from natural language supervision,” in ICLR, 2021, pp. [57] Y. Zheng, J. Zhan, S. He, J. Dong, and Y. Du, “Curricular contrastive
8748–8763. regularizationforphysics-awaresingleimagedehazing,”inCVPR,2023,
[31] M.Caron,H.Touvron,I.Misra,H.Je´gou,J.Mairal,P.Bojanowski,and pp.5785–5794.
A.Joulin,“Emergingpropertiesinself-supervisedvisiontransformers,” [58] K.SimonyanandA.Zisserman,“Verydeepconvolutionalnetworksfor
inICCV,2021,pp.9650–9660. large-scaleimagerecognition,”arXivpreprintarXiv:1409.1556,2014.
[32] Z. Tan, Y. Wu, Q. Liu, Q. Chu, L. Lu, J. Ye, and N. Yu, “Exploring [59] P.Arbelaez,M.Maire,C.Fowlkes,andJ.Malik,“Contourdetectionand
the application of large-scale pre-trained models on adverse weather hierarchicalimagesegmentation,”TPAMI,vol.33,no.5,pp.898–916,
removal,”TIP,vol.33,pp.1683–1698,2024. 2010.
[33] L.Liu,L.Xie,X.Zhang,S.Yuan,X.Chen,W.Zhou,H.Li,andQ.Tian, [60] K. Ma, Z. Duanmu, Q. Wu, Z. Wang, H. Yong, H. Li, and L. Zhang,
“Tape:Task-agnosticpriorembeddingforimagerestoration,”inECCV, “Waterloo exploration database: New challenges for image quality
2022,pp.447–464. assessmentmodels,”TIP,vol.26,no.2,pp.1004–1016,2016.
[34] J. Zhang, J. Huang, M. Yao, Z. Yang, H. Yu, M. Zhou, and F. Zhao, [61] D. Martin, C. Fowlkes, D. Tal, and J. Malik, “A database of human
“Ingredient-oriented multi-degradation learning for image restoration,” segmentednaturalimagesanditsapplicationtoevaluatingsegmentation
inCVPR,2023,pp.5825–5835. algorithmsandmeasuringecologicalstatistics,”inICCV,vol.2,2001,
[35] M.Yao,R.Xu,Y.Guan,J.Huang,andZ.Xiong,“Neuraldegradation pp.416–423.
representationlearningforall-in-oneimagerestoration,”arXivpreprint [62] J.-B. Huang, A. Singh, and N. Ahuja, “Single image super-resolution
arXiv:2310.12848,2023. fromtransformedself-exemplars,”inCVPR,2015,pp.5197–5206.
[36] Y.-W. Chen and S.-C. Pei, “Always clear days: Degradation type and [63] R. Franzen, “Kodak lossless true color image suite,” http://r0k.us/
severity aware all-in-one adverse weather removal,” arXiv preprint graphics/kodak/,1999.
arXiv:2310.18293,2023. [64] B. Li, W. Ren, D. Fu, D. Tao, D. Feng, W. Zeng, and Z. Wang,
[37] G. R. dense transformer with grid structure for image restoration in “Benchmarkingsingle-imagedehazingandbeyond,”TIP,vol.28,no.1,
adverse weather conditions, “Gridformer: Residual dense transformer pp.492–505,2018.
withgridstructureforimagerestorationinadverseweatherconditions,” [65] W. Yang, R. T. Tan, J. Feng, J. Liu, Z. Guo, and S. Yan, “Deep joint
IJCV,pp.1–23,2024. rain detection and removal from a single image,” in CVPR, 2017, pp.
[38] C.Mou,Q.Wang,andJ.Zhang,“Deepgeneralizedunfoldingnetworks 1357–1366.
forimagerestoration,”inCVPR,2022,pp.17399–17410. [66] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang, “The
[39] C. Wang, J. Pan, W. Wang, J. Dong, M. Wang, Y. Ju, and J. Chen, unreasonable effectiveness of deep features as a perceptual metric,” in
“Promptrestorer:Apromptingimagerestorationmethodwithdegrada- CVPR,2018,pp.586–595.
tionperception,”inNeurIPS,2023,pp.8898–8912. [67] M.YangandA.Sowmya,“Anunderwatercolorimagequalityevaluation
[40] H.Guo,J.Li,T.Dai,Z.Ouyang,X.Ren,andS.-T.Xia,“Mambair:A metric,”TIP,vol.24,no.12,pp.6062–6071,2015.
simplebaselineforimagerestorationwithstate-spacemodel,”inECCV, [68] K. Panetta, C. Gao, and S. Agaian, “Human-visual-system-inspired
2024. underwaterimagequalitymeasures,”JOE,vol.41,no.3,pp.541–551,
[41] T.Brown,B.Mann,N.Ryder,M.Subbiah,J.D.Kaplan,P.Dhariwal, 2015.
A.Neelakantan,P.Shyam,G.Sastry,A.Askelletal.,“Languagemodels [69] H. Gao, X. Tao, X. Shen, and J. Jia, “Dynamic scene deblurring with
arefew-shotlearners,”inNeurIPS,2020,pp.1877–1901. parameter selective sharing and nested skip connections,” in CVPR,
[42] X.Liu,K.Ji,Y.Fu,W.L.Tam,Z.Du,Z.Yang,andJ.Tang,“P-tuning 2019,pp.3848–3856.
v2:Prompttuningcanbecomparabletofine-tuninguniversallyacross [70] H. Shen, Z.-Q. Zhao, and W. Zhang, “Adaptive dynamic filtering
scalesandtasks,”arXivpreprintarXiv:2110.07602,2021. networkforimagedenoising,”inAAAI,vol.37,no.2,2023,pp.2227–
[43] Y. Lu, J. Liu, Y. Zhang, Y. Liu, and X. Tian, “Prompt distribution 2235.
learning,”inCVPR,2022,pp.5206–5215. [71] Q. Fan, D. Chen, L. Yuan, G. Hua, N. Yu, and B. Chen, “A general
[44] M.Jia,L.Tang,B.-C.Chen,C.Cardie,S.Belongie,B.Hariharan,and decoupled learning framework for parameterized image operators,”
S.-N.Lim,“Visualprompttuning,”inECCV,2022,pp.709–727. TPAMI,vol.43,no.1,pp.33–47,2019.
[45] Q.Huang,X.Dong,D.Chen,W.Zhang,F.Wang,G.Hua,andN.Yu, [72] Z. Chen, Y. Zhang, L. Ding, X. Bin, J. Gu, L. Kong, and X. Yuan,
“Diversity-aware meta visual prompting,” in CVPR, 2023, pp. 10878– “Hierarchicalintegrationdiffusionmodelforrealisticimagedeblurring,”
10887. inNeurIPS,2023.
[46] W.Liu,X.Shen,C.-M.Pun,andX.Cun,“Explicitvisualpromptingfor [73] J. M. J. Valanarasu, R. Yasarla, and V. M. Patel, “Transweather:
low-levelstructuresegmentations,”inCVPR,2023,pp.19434–19445. Transformer-based restoration of images degraded by adverse weather
[47] A.Bar,Y.Gandelsman,T.Darrell,A.Globerson,andA.Efros,“Visual conditions,”inCVPR,2022,pp.2353–2363.
promptingviaimageinpainting,”inNeurIPS,vol.35,2022,pp.25005– [74] B. Li, X. Peng, Z. Wang, J. Xu, and D. Feng, “Aod-net: All-in-one
25017. dehazingnetwork,”inICCV,2017,pp.4770–4778.
[48] A.Hojel,Y.Bai,T.Darrell,A.Globerson,andA.Bar,“Findingvisual [75] X.Xu,R.Wang,andJ.Lu,“Low-lightimageenhancementviastructure
taskvectors,”arXivpreprintarXiv:2404.05729,2024. modelingandguidance,”inCVPR,2023,pp.9893–9903.
[49] Z. Liang, C. Li, S. Zhou, R. Feng, and C. C. Loy, “Iterative prompt [76] S. W. Zamir, A. Arora, S. Khan, M. Hayat, F. S. Khan, M.-H. Yang,
learningforunsupervisedbacklitimageenhancement,”inICCV,2023, andL.Shao,“Learningenrichedfeaturesforrealimagerestorationand
pp.8094–8103. enhancement,”inECCV,2020,pp.492–511.
[50] T. Wei, D. Chen, W. Zhou, J. Liao, Z. Tan, L. Yuan, W. Zhang, and [77] C.Li,C.Guo,W.Ren,R.Cong,J.Hou,S.Kwong,andD.Tao,“An
N. Yu, “Hairclip: Design your hair by text and reference image,” in underwater image enhancement benchmark dataset and beyond,” TIP,
CVPR,2022,pp.18072–18081. vol.29,pp.4376–4389,2020.