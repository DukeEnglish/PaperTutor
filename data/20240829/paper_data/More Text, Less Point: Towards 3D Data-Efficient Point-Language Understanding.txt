More Text, Less Point: Towards 3D Data-Efficient Point-Language Understanding
YuanTang1∗,XuHan1∗,XianzhiLi1†,QiaoYu1,JinfengXu1,YixueHao1,LongHu1,MinChen2
1HuazhongUniversityofScienceandTechnology
2SouthChinaUniversityofTechnology
{yuan tang,xhanxu,xzli,qiaoyu epic,yixuehao,hulong}@hust.edu.cn,minchen@ieee.org,jinfengxu.edu@gmail.com
Abstract GreenPLM-0 (Ours) GPT-4o mini GreenPLM (Ours) PointLLM
54.57 Point-Bind LLM 60.08 MiniGPT-3D
E 3Dna pb hli yn sg icL aa lr wge orL ldan rg emua ag ie nsM ao sd ige nls ifi(L caL nM tcs h) ato llec no gm ep .r De uh een tod tt hh ee 42.16
+3.46
+ 491 .0 1.9 35 4+ 91 .01 2.06 49.40 4+ 62 .. 49 82 4+ 62 .6.7 82
lackoflarge-scale3D-textpairdatasets,thesuccessofLLMs +24.79 38.70
29.78 +27.89 +14.87
hasyettobereplicatedin3Dunderstanding.Inthispaper,we 26.68 27.29
rethinkthisissueandproposeanewtask:3DData-Efficient
Point-LanguageUnderstanding.ThegoalistoenableLLMsto Cls. (Avg) Cap. (S-Bert) Cls. (Avg) Cap. (S-Bert)
original mesh original mesh
achieverobust3Dobjectunderstandingwithminimal3Dpoint point cloud input point cloud input
(only for reference) (only for reference)
cloudandtextdatapairs.Toaddressthistask,weintroduce
GreenPLM,whichleveragesmoretextdatatocompensate
forthelackof3Ddata.First,inspiredbyusingCLIPtoalign What is the function of its spoiler ? Does it require electricity ?
imagesandtext,weutilizeapre-trainedpointcloud-texten- T anh de ss tp ao bi il le itr y i s a td he is gig hn se pd e eto d sim , ep nr ho av ne c a ine gro td hy en ca am r'i sc s No, it does not require electricity as it is a
codertomapthe3Dpointcloudspacetothetextspace.This performance. manual bicycle.
mappingleavesustoseamlesslyconnectthetextspacewith point cloud input (only fo ori rg ri en fa el r em ne cs eh ) point cloud input (only fo ori rg ri en fa el r em ne cs eh )
LLMs.Oncethepoint-text-LLMconnectionisestablished,
we further enhance text-LLM alignment by expanding the
intermediatetextspace,therebyreducingtherelianceon3D
pointclouddata.Specifically,wegenerate6Mfree-textde-
scriptions of 3D objects, and design a three-stage training I ht o d wo e cs an n' t I h ca ov ne t rn ou l m ite ?ric buttons, Why does it have 4 propellers ?
strategytohelpLLMsbetterexploretheintrinsicconnections C suo rn fatr co el , i as l lm owan ina gg e fd o rt h inro tuu ig tih v ea nto au vc ih g- as te ion ns i ati nv de T beh te te f ro su tr a bp ir lo itp ye all ne drs c a ol nlo trw o lt h de u rd inro gn fe l it go h h t.ave
betweendifferentmodalities.Toachieveefficientmodality interaction.
Based on its flight stability, which
alignment,wedesignazero-parametercross-attentionmodule W useh o p hw ya ss ic t ah le b f uir ts tot nto s ?propose not to industries can it be applied to ?
fortokenpooling.Extensiveexperimentalresultsshowthat The concept of not using physical buttons I vt as r f ioli ug sh t i ns dta ub si tl ri it ey s m sua ck he s a sit rs eu ai lt a eb stl ae t efo , r
GreenPLMrequiresonly12%ofthe3Dtrainingdatausedby was proposed by Steve Jobs, the co-founder agriculture, and media .
of Apple Inc.
existingstate-of-the-artmodelstoachievesuperior3Dunder- Besides the touch phone, what else How to use drones to real estate?
standing.Remarkably,GreenPLMalsoachievescompetitive does he propose ?
performanceusingtext-onlydata.Thecodeandweightsare S tat be lv ee t, J wob hs ic a hl s wo op ur lo dp oo fs fe ed r t ah e la c rgre ea r t sio cn re o enf a a nd I an e rr ie aa l l v e iest wat se o, fd r po rn oe ps e rc ta ien s ,b pe ru os ve id d it no g c pa op tt eu nre ti al
availableat:https://github.com/TangYuan96/GreenPLM. m trao dr ie t iv oe nr as la pti hle o nfu en s.ctionality compared to b thu ey per rs o pw ei rt th y 'a s c lao ym op ur t e ah ne dn s si uv re ro u un nd de ir ns gta sn .ding of
GreenPLM-0 (Text-only) GreenPLM (Limited 3D Data 90K)
Figure 1: We propose GreenPLM, which expands the text
Introduction
spacetoreducetheneedfor3Ddata.GreenPLMachieves
Recentadvancementsinlargelanguagemodels(LLMs)have strong3Dunderstandingusingjust12%ofthe3Ddataor
revolutionizednaturallanguageprocessing,demonstrating evenwithtext-onlydata.
emergent intelligence and exceptional capabilities in lan-
guage understanding and generation (OpenAI 2023; Yang
etal.2024a;Dubeyetal.2024;Teametal.2023).However, putstoatextspaceclosertoLLMsusingpre-trainedmulti-
LLMsareblindtothe3Dphysicalworldbecausetheylack modalencoders,enablingLLMstounderstanddatabeyond
the ability to capture and understand 3D objects. Solving justlanguage.Existing3Dpoint-languagemodelsfollowa
thischallengingmultimodal3D-languageunderstandingtask similar approach, applying LLMs to 3D understanding by
couldbenefitmanyapplications,suchasautonomousdriving, learningfrom3Dpoint-textdatapairs(Luoetal.2024;Qi
roboticsandembodiedAI(Driessetal.2023;Fuetal.2024; etal.2024b).Forexample,PointLLM(Xuetal.2023)and
Brohanetal.2023). ShapeLLM(Qietal.2024a)employpre-trainedmultimodal
pointcloudencoders(Xueetal.2024;Qietal.2024a),map-
InspiredbyCLIP(Radfordetal.2021),multimodallarge
pingthepointcloudspacetothetextspace.Thisleavesthe
languagemodels(MLLMs)canmapdifferentmodalityin-
alignmentofpointcloudwithLLMstoonlyalignthetext
∗Equalcontribution;†correspondingauthor. spacewithLLMs,whichisrelativelyeasierforLLMs.Fi-
4202
guA
82
]VC.sc[
1v66951.8042:viXraexpensive3Ddata.(2)Weproposea3-stagetrainingstrategy
730K3D-text data pairs
designedtohelpLLMsbetteruncovertheintrinsicconnec-
Existing tionsbetweendifferentmodalities.Specifically,wepropose
Methods: Point Encoder Text space LLM
acoarse-to-finetrainingapproach,progressingfromdatato
model.Thefirsttwostagesfine-tunetheLLMswithtext-only
data,whilethefinalstageusesminimal3Ddataforfurther
90K3D-text data pairs Freetext descriptions point-LLMsalignment.(3)Fromthearchitecture’sperspec-
Ours: tive,wedesignaparameter-freecross-attentionmodulefor
Point / Text
Encoder Text space LLM tokenpooling,namely0M-Pooling,whichbetterutilizesthe
encoder’soutputtokens,therebyaligningpointcloudswith
LLMsmoreeffectively.This,wecanachieveexcellentper-
Figure2:ExistingmethodslikePointLLMusemassive3D- formance with only an efficient LLM (Abdin et al. 2024).
textdata(∼730K)toenhancethepoint-textmapping,there- Together,wecancompletetraininginjust26.6hoursusinga
forerealizepoint-languageunderstanding,whilewecanalso single3090GPU(24GB),leavingopportunitiesforefficient
achieve this with only a small number of 3D data (∼90K) end-sidedeployment.
andfree-textdescriptionsforbetterpoint-LLMalignment. Tofairlyandreasonablyevaluatethemodels,weintroduce
a new metric to measure the efficiency of 3D data usage,
nally,theyproposetofine-tunethewholemodelwithlarge and establish a new evaluation benchmark based on open-
amountof3D-textdatapairs,thusenhancingtheLLMs’3D sourceLLMs.ExperimentalresultsshowthatourGreenPLM
understandingcapabilities.However,thisfieldremainsunder- outperformspreviousmodelsusingonly12%ofthe3Ddata.
explored.TheprimaryreasonisthattrainingLLMsrequires ItevensurpassesGPT4Point(Qietal.2024b)withoutany3D
billionsoftextdatafromtheinternet,while3D-textpairdata data,maintainingextremely3Ddata-efficientpoint-language
isscarcesince3Ddataitselfishardtoacquireandrequires understanding,whichdemonstratestheeffectivenessofour
expensive annotations. Consequently, the scaling law that approach.Thecontributionsofthispaperareasfollows:
drives LLMs success are difficult to achieve in the 3D do-
• We introduce a new task of 3D data-efficient point-
main, directly limiting the development of 3D foundation
language understanding, aiming to enable LLMs to
models.
achieverobust3Dunderstandingwithminimal3Ddata.
Inthispaper,werevisitthe3Ddatabottleneckandpose
• WeproposeGreenPLMtotacklethis3Ddata-limitedtask
aquestion:Canweachieverobust3Dunderstandingwith
from a novel perspective, enhancing point-LLM align-
minimal 3D data? To answer this question, we propose a
mentwithmorefree-textdata.Specifically,weintroduce
newtask:3DData-EfficientPoint-LanguageUnderstanding
a6MT3Ddataset,designa3-stagetrainingstrategy,and
(3DEPL).ThegoalistoenableLLMstoachieverobust3D
presenta0M-Poolingmodulefortokenpooling.
understandingusingaslittle3Dpointcloud-textdatapairs
aspossible.Thisrequiresthemodeltoexploretheintrinsic • WeintroducetheAccuracy-to-3D-DataRatio(A3DR)to
connections between different modalities, and effectively measuretheefficiencyof3Ddatausageandestablishan
leveragethepowerfullanguagecomprehensioncapabilities evaluationbenchmarkbasedonopen-sourceLLMs.
ofLLMstoachievedata-efficient3Dunderstanding. • GreenPLMoutperformspreviousmodelsusingonly12%
Toaddressthisdata-limitedmultimodalalignmentprob- of3DdataandevensurpassesGPT4Point(660K3Ddata)
lem,weproposeGreenPLM.Intuitively,asshowninFig.2, usingonlytext,demonstratingsuperior3Ddataefficiency.
weobservethatafterestablishingthepoint-text-LLMconnec-
tion,insteadofincreasingpoint-textdatapairstooptimize RelatedWork
the point-text mapping like in existing methods (Xu et al.
3DPoint-LanguageUnderstanding
2023; Qi et al. 2024a), we can also enhance the text-LLM
alignmentbysimplyaddingmoretextdata.Thisapproach ToenableLLMstounderstandthe3Dphysicalworld,early
canalsoimprovethepoint-LLMalignmentand,moreimpor- approaches project 3D point clouds into 2D images, rely-
tantly,reducetherelianceonpoint-textdatapairs,shifting ing on 2D-LLMs for comprehension (Hong et al. 2023).
thedatabottleneckfromexpensiveandscarce3D-textdata Point-Bind LLM (Guo et al. 2023) establishes a 3D-2D-
toabundantandcheaptextdata.Thatis,thetext-LLMalign- LLM connection for better performance. However, these
mentapproachfitsperfectlywiththegoalof3Ddata-efficient 2D-basedmethodslosecrucial3Dinformation,leadingto
point-languageunderstanding,alsoprovidesanalternativeso- issueslikeocclusion,ambiguity,andhallucination.Recently,
lutionforaligningpointcloudswithLLMs,enablingGreen- with the availability of large-scale 3D-text data (Luo et al.
PLMtoachieverobust3Dunderstandingevenwithlimited 2024;Qietal.2024b)andmultimodalencoders,methodslike
3Ddata. PointLLM(Xuetal.2023)andShapeLLM(Qietal.2024a)
Indetail,GreenPLMsolvesthe3DEPLtaskwithkeytech- connectpointencoderswithLLMsandfine-tunethe3DPoint
niquesacrossthreeperspectives:data,trainingstrategy,and Cloud-LLMs(3D-LLMs)usingvastamountsof3D-textdata.
model architecture. (1) We bring T3D dataset, a 6M text Unfortunately,comparingtoimages,3D-textdataremains
datasetof3Dobjectdescriptionsandconversationsforfree, extremely scarce (Objaverse-1M vs. LAION-5B) (Deitke
the largest to our knowledge, to expand the text space for etal.2023;Schuhmannetal.2022)andexpensive,letalone
bettertext-LLMalignmentandcompensateforthescarcityof the near infinite and free text data, making it challengingof3Dobjectdescriptionsandconversations.Then,tobetter
uncoverconnectionsbetweendifferentmodalities,wedesign
a3-stagetrainingstrategy.Finally,weintroduceaparameter-
(cid:3)(cid:8)(cid:9)(cid:15)(cid:2)
(cid:9)(cid:15)(cid:15)(cid:2)(cid:9)(cid:5)(cid:9)(cid:11)(cid:4)(cid:2) freetokenpoolingmoduletoefficientlyutilizeinformation
(cid:4)(cid:18)(cid:17)(cid:18)(cid:5)
(cid:3)(cid:4)(cid:8)(cid:2)(cid:21)(cid:2)
(cid:13)(cid:2)(cid:9)(cid:12)(cid:19)(cid:5)(cid:2)(cid:8)(cid:9)(cid:10)(cid:5)(cid:2) (cid:2)(cid:15)(cid:0) (cid:5)(cid:2) (cid:2)(cid:3) (cid:3)(cid:4) (cid:2)(cid:5) (cid:11)(cid:6) (cid:12)(cid:7) (cid:12)(cid:14)(cid:2) (cid:15)(cid:2)(cid:3)(cid:2) (cid:6)(cid:5) (cid:0)(cid:10) (cid:2)(cid:2)
(cid:11)(cid:12)
(cid:18)(cid:6)(cid:2) (cid:7)(cid:13)(cid:16) (cid:24)(cid:2)(cid:14)(cid:15)
(cid:2)
(cid:4)(cid:17)
(cid:17)
(cid:12)(cid:0)(cid:9) (cid:2)(cid:6) (cid:6) (cid:21)(cid:2)(cid:11) (cid:11)(cid:15)(cid:4) (cid:2)(cid:18) (cid:6) (cid:11)(cid:4)(cid:5)(cid:15) (cid:12)(cid:12) (cid:4)(cid:6)(cid:18)(cid:11) (cid:18)(cid:5)(cid:4)(cid:9) (cid:17)(cid:17)(cid:12) (cid:18)(cid:19)(cid:2)(cid:20) (cid:5)(cid:0)(cid:2)(cid:2)(cid:5)(cid:2) (cid:9)(cid:21)(cid:3) (cid:5)(cid:2)(cid:9)(cid:21)(cid:22) (cid:13)(cid:2)(cid:7)(cid:17) (cid:2)(cid:3) (cid:2)(cid:19)(cid:23)(cid:19)(cid:23)(cid:3) (cid:9)(cid:2)(cid:2)(cid:3)(cid:12) (cid:12)(cid:19)(cid:5)(cid:2)(cid:15)(cid:19)(cid:5)(cid:15)(cid:18)(cid:13)(cid:3)(cid:0)(cid:18)
(cid:19) (cid:2)(cid:11)(cid:4) (cid:12)(cid:12) (cid:14)(cid:6)(cid:18) (cid:15)(cid:11)
(cid:2)(cid:3)(cid:6)(cid:2)(cid:23)(cid:11)(cid:22) (cid:22)(cid:17)(cid:6)(cid:2)(cid:13)(cid:6)(cid:6) (cid:6)(cid:15)(cid:4)(cid:11)(cid:21) (cid:11)(cid:9)(cid:19)(cid:11)(cid:0)(cid:5) (cid:0)(cid:15)(cid:4)(cid:2)(cid:18)(cid:2)(cid:11)(cid:3)(cid:13)(cid:2)(cid:2)(cid:9)(cid:13) (cid:12)(cid:12)(cid:2)(cid:19)(cid:12)(cid:5)(cid:14)(cid:9)(cid:2)(cid:15)(cid:12)(cid:21) (cid:2)(cid:19)(cid:9) (cid:5)(cid:2)(cid:4)(cid:9)(cid:17) (cid:2)(cid:4)(cid:2)(cid:2)(cid:21)(cid:12)(cid:3)(cid:13)(cid:18)(cid:2)(cid:3)(cid:2)(cid:2)(cid:3)(cid:7)(cid:18)(cid:11)(cid:5)(cid:12) (cid:5)(cid:9)(cid:24)(cid:5) (cid:12)(cid:22) (cid:6)(cid:19) (cid:14)(cid:2)(cid:6)(cid:12)(cid:12) (cid:9)(cid:4)(cid:11)(cid:4)(cid:14)(cid:19)(cid:12)(cid:0)(cid:15)(cid:12)(cid:19)(cid:22) (cid:17)(cid:2)(cid:5)(cid:5)(cid:6)(cid:2)(cid:2)(cid:11)(cid:19)(cid:0)(cid:3)(cid:2) f t 3hr Do em se Ott bhh jre e ee cen tpc Dao rt ed s se a cr’ r rs e ipo a tsu it f op o nu llt o awt no s dk .e Cn os neq vu ee rn sace t. ioT nh De ade tata si els
t
of
Fi(cid:22)(cid:6)(cid:11)(cid:0)g(cid:4)(cid:18)(cid:17)(cid:18)u(cid:5) re3:T3Ddatasetdistribution. Leveraging multimodal pre-trained encoders, we propose
usinglargeamountsoftextdatatocompensateforthelackof
Table1:3DobjectdescriptionandconversationsofT3D.
3Ddatapairs.Specifically,wefirstalignthetextencoderwith
DataType Size Sample theLLMusingextensivetextdata.Sincethetextencoderis
Caption - A3Dmodelofafriendlywhitedog... alreadyalignedwiththepointencoder,wethenonlyneeda
Brief Q:Summarizethe3Dpointcloudobjectbriefly. smallamountof3Ddataforpointencoder-LLMalignment.
1M
Desc. A:The3Dobjectisawhitedogmodel...
To achieve this, we bring T3D, a 6M text dataset of
Detail Q:Offeradetaileddescriptionofthispointcloud.
1M 3D object descriptions and conversations. Fig. 3 shows
Desc. A:This3Dobjectdepictsawhitedogsitting...
theverb-noundistributionandavisualizedwordcloud.In-
Single Q:Whatisthepostureofthedog?
Conv. 3M A:Thedogissittingupright.... steadofusingtheclosed-sourceGPT-4(OpenAI2023),we
Q1:Whattypeofanimaldoesthe3Dmodelrepresent? use the equally powerful open-source model Qwen2-72B-
Multi A1:The3Dmodelrepresentsadog. Instruct (Yang et al. 2024a) to construct this dataset. We
1M Q2:Canyoudescribethedog’sposture?
Conv. A2:Thedogissittingupright.... selectobjectcategoriesfromCap3D(Luoetal.2024)and
... DiffuRank (Luo, Johnson, and Lee 2024), and we design
promptstogenerate4typesofdata:1Mbriefdescriptions,
to build powerful 3D foundation models according to the 1M detailed descriptions, 3M single-round conversations,
scalinglaw.Also,training3D-LLMsisresource-intensive, and1Mmulti-roundconversations.Theobjectdescriptions
often requiring 8xA100 GPUs for hundreds of hours. Al- help the LLMs learn rich semantic knowledge, while the
thoughMiniGPT-3D(Tangetal.2024)reducestrainingtime conversations enable the LLMs to extract useful informa-
to 26.8h on a single GPU, the 3D data bottleneck persists. tionfromthecontexttoimprove3Dunderstanding.Notably,
Our GreenPLM proposes to solve this 3D data bottleneck this dataset is constructed without any manual annotation
byleveragingextensivetextdatatocompensateforthelack orpost-processing,requiringonlyminimalmodelinference
of 3D data, and introducing a 3-stage training strategy for cost.EachtypeofdatafollowstheCaption-Question-Answer
effectiveandefficientalignment. format,asshowninTab.1.Duringtraining,weinputtheCap-
tionintothetextencoder,passtheencodedtokensthrough
MultimodalEncodersin3D-LLM a projector, and then input them along with the Question
intotheLLM,whichoutputsaresponsetocalculatetheloss
Theencodermapsrawdataintoamorecompactembedding
againsttheAnswer.Moredetailedpromptsanddistributions
space,whichcanthenbealignedwithLLMs.Toreducethe
areinAppendix.
trainingcost,onecanintuitivelyemployamultimodalpre-
trainedencoder,suchasCLIP(Radfordetal.2021),which
3-StageTrainingStrategy
hasbeentrainedontext-imagepairs,foraligning2Dimages
withLLMs.Thismakesiteasiertoaligndatafromdifferent Forbettermultimodalencoder-LLMalignmentandminimiz-
modalitieswithLLMs.Similarly,someexisting3D-LLMs ingtheuseof3Dpoint-textdatapairs,weproposea3-stage
usemultimodalpre-trainedencoders(Huangetal.2023;Xue training strategy, as shown in Fig. 4. Our design principle
etal.2023;Qietal.2023;Gaoetal.2024;Leietal.2023; is to first use a large amount of text data to align the text
Chen et al. 2024a; Han et al. 2024) to map point clouds encoderwiththeLLMviaaMLPprojector(StageIandII).
intoembeddingspace,followedbyfine-tuningthe3D-LLM. Then,usingonlyasmallamountof3Dpoint-textdatapairs,
However,evenwithouttrainingtheencoder,constructingthe wealignthepointcloudencoderwiththeLLMviathesame
3D-LLMstillrequiresavastamountofpoint-textdata(Xu projector (Stage III). Specifically, for each stage, we will
et al. 2023; Zhou et al. 2023; Qi et al. 2024a; Tang et al. introducethepipeline,trainablelayers,anddataaspectsas
2024). We observe that existing methods underutilize the follows.
potentialofthetextencoder,onlyfocusingonaligningpoint
StageI isshowninFig.4(a).First,weinputatextcaption
encoderwithLLM.Incontrast,weproposeleveragingthe
D of a 3D object into the pre-trained text encoder f ,
cost-efficienttextspaceandthetextencodertoreducethe text
obtainingtheglobaltextembeddingC astheencoderoutput.
dependencyon3Ddata. t
C isthenpassedthroughalearnableMLPprojectorf
t proj
to connect with the LLM f . The LLM input consists
Method LLM
oftheprojectoroutputf (C ),andthetexttokensofan
proj t
To enable LLMs to achieve robust 3D understanding with instruction prompt I such as “What is this?”. Finally, the
minimal3Ddata,weproposeusingmoretextdatatoreduce LLM outputs a brief description R of the 3D object,
brief
reliance on 3D data. First, we generate a 6M text dataset whichcanbeusedtocalculatethelosswiththeground-truthTrainable Frozen
Detailed Description Detailed Description
Brief Description &
Brief Description & Conversation & Conversation
LLM LLM LoRA LLM LoRA
MLP MLP MLP
Copy Copy
Instruction Weight Instruction Weight Mix pooling Instruction
tokens
Text Text Class 0M-Pooling Point
Embedding Embedding Token Tokens
CLIP CLIP CLIP
Text Encoder Text Encoder Point Cloud Encoder
A black and yellow samurai sword, Text Caption
3D Point Cloud
featuring a long, curved blade …… for 3D Object
(a) Stage I (b) Stage II (c) Stage III
Figure4:Illustrationof3-StageTrainingStrategy.WeexpandthetextspacebyfeedingmoretextdatainStageI&II,thus
reduce the demand of 3D data in Stage III. We input the text/point cloud to the encoders, then align with LLM via a MLP
projector.Additionally,wedesigna0M-Poolingmoduletoefficientlycompressthetokensequenceoutputbypointencoder.
description.Theformulasareasfollows: StageIII isshowninFig.4(c),weuse3Dpointcloudas
input.The3DpointcloudP isfedintothepointclouden-
C t =f text(D), (1) coderf pctooutputatokensequence.Unlikepreviousstages
R =f (f (C ),h(I)), (2) that use only the global text embedding (corresponding to
brief LLM proj t
the class token in the point encoder) for the projector, in
where,histheLLM’stokenizer. this stage, we extract representations from all tokens T
pc
TrainableLayers&Data:Notethat,onlytheprojector tomoreeffectivelyleverageinformationfromthepointen-
f is a trainable MLP, while the rest, including the text coder.Toreducethetokensequencelengthforefficiency,we
proj
encoderf andLLMf ,havefrozenweights.Wetrain introduceaparameter-freetokenpoolingmodulebasedon
text LLM
the model using a large dataset of brief descriptions (1M) cross-attention,namely0M-Pooling,whichcompressesthe
fromourT3Ddataset,asshowninTab.1. tokenlengthfrom512to32.ThepooledpointtokensTp,
pc
alongwiththreetokensfromMix-poolingandtheclassto-
StageII isshowninFig.4(b),StageIIissimilartoStage kenC ,areinputtotheprojector.Thus,theprojectorf
pc proj
I.Wealsofirstinputacaptionofa3Dobjectintothetext receives 32+3+1=36 tokens. We then feed the projector’s
encoder f , then extract the global text embedding and output,alongwiththeinstructionI,intof togenerate
text LLM
pass it to the projector f . The projector output, along thepredictresponsesR ofdescriptionsorconversations.
proj pred
withacomplexinstruction,isthenfedtotheLLMf .Fi- Theresponseswillbeusedtocomputelosswiththeground
LLM
nally,theLLMoutputsdetaileddescriptionandconversation truth.Thisstagecanbeformulatedas:
results,whicharethenusedtocalculatetheloss. [C ,T ]=f (P), Tp =0M-Pooling(T ), (3)
pc pc pc pc pc
TrainableLayers&Data:ThedifferencesfromStageI
R =f
(cid:0)
f
(cid:0)
C ,Mix(T
),Tp(cid:1) ,h(I)(cid:1)
,
are as follows: (1) The weights of the projector f proj are pred LLM proj pc pc pc
copiedfromStageIforinitializationandremaintrainable. (4)
(2)WeuseLoRA(Huetal.2021)totraintheLLMf LLM in whereMixrepresentsMix-poolingofmax,mean,andsum.
thisstagetoachievebettermultimodalalignment.Thetext TrainableLayers&Data:SimilartoStageII,theweights
encoder f text remains frozen. We use only 210K detailed of projector f proj here are copied from the previous stage
descriptionsandconversationdatafor3Dobjectsfromour andthenstillkepttrainable.WecontinueusingLoRA(Hu
T3Ddataset,suchasdescribinganobjectin∼50wordsand etal.2021)totrainf forefficientpoint-LLMalignment.
LLM
engaginginmulti-turnconversations,asshowninTab.1. Thepointcloudencoderf remainfrozen.Inthisstage,we
pc
Notably,toenhancetheperceptionrobustnessoftheLLM, trainusingonlyasmallamountof3D-textpairs(90K).
weaddGaussiannoisetotheencoder’soutputfeaturestosim-
LossFunction Foralltrainingstages,givenapairofLLM
ulatethesemanticdiscrepanciesbetweendifferentmodalities,
outputRandtextgroundtruthy,GreenPLMisoptimized
inspiredbyChenetal.(2024b).Aftertwostagesofpuretext
underacausallanguagemodelingobjective(Liuetal.2018):
training,ourGreenPLMacquirestheabilitytocomprehend
raw3Dpointcloudsbydirectlyreplacingthetextencoder
L=CrossEntropyLoss(R,h(y)), (5)
f withapointencoderf withoutweighttuning.
text pcBaselines. Tovalidateour3Ddata-freecapability,wecom-
Output paredGreenPLM-0withtheSOTA2D-LLMs,InstructBLIP
andLLaVA,aswellasthe3D-2D-LLMmodelPoint-Bind
LLM(Guoetal.2023).ToevaluateGreenPLMwithlimited
3Ddata,wechoosetheSoTAmodelsPointLLM(Xuetal.
2023)andMiniGPT-3D(Tangetal.2024).Forfairness,we
trainbothusingthesame90Klimited3Dpoint-textdatas.
EvaluationSettings. Anefficientandaccuratemodeleval-
uation method is a shared goal in the MLLM community.
Weobservethatexistingevaluationapproachesoftenrelyon
Input
GPT-4andGPT-3.5toassessthesimilaritybetweengener-
atedresultsandgroundtruthsentences.Whilethismethod
Figure5:Illustrationof0M-Pooling,whichcompressesN
providesaccurateevaluations,ithastwomajordrawbacks:
tokenstoM tokens(M <<N).
inconsistentAPIversionsandhighevaluationcosts.Forin-
stance, the GPT-3.5-turbo-0613 model used in PointLLM
whereCrossEntropyLossisthecross-entropyloss,andhde-
andMiniGPT-3Disnolongermaintained,makingitdifficult
notestheLLM’stokenizer.
toreplicatetheresults.Toaddresstheseissues,wepropose
a new benchmark based on open-source models and intro-
0M-Pooling
duceanewmetrictoevaluatedataefficiency.Specifically,
Tofullyleveragetheoutputofthepointcloudencoder,we weusetwopromptsfortheclassificationtask:anInstruction-
extractinformationfromalloutputtokensT pc,notjustthe type (I) prompt, “What is this?”, and a Completion-type
classtoken,whilereducingcomputationalload.Asshown (C)prompt,“Thisisanobjectof.”.Forthecaptioningtask,
inFig.5,wedesignazero-parametertokenpoolingmodule weuseasingleprompt:“Captionthis3Dmodelindetail.”.
basedoncross-attention,namely0M-Pooling,whichcom- WethenreplaceGPT-4andGPT-3.5withtheopen-source
presses the 512 output tokens down to 32 tokens, without Qwen2-72B-Instruct(Yangetal.2024a)(Qwen2forshort)to
introducinganylearnableparameters,definedas: evaluatethemodel’soutput.WeintroducetheAccuracy-to-
3D-DataRatio(A3DR)metrictoassessamodel’sefficiency
T =FPS(T ), T =KNN(T ,T ),
c pc p c pc inutilizing3Ddata,definedasfollows:
(6)
T =MaxPool(T ), Tp =SoftMax(T TT)T ,
m p pc p m p 1
A3DR(Acc)= , (7)
where T ∈ RN×1×C is the output point token sequence 1+exp(−2×Acc)
pc Size+ϵ
of the point cloud encoder (N = 512), T ∈ RM×1×C is
c whereSizeisthesizeof3Ddata(K),Accistheaccuracy,
the central token gained via farthest point sampling (FPS)
ϵ=1e−5toavoidzerodivision.
fromT (M =32),andT ∈RM×K×C representstheK-
pc p
NearestNeighborhood(KNN)tokensofT withinT (K =
c pc Generative3DObjectClassification
8).Then,wepassT toMaxPoolingontheK dimension
p
to get T ∈ RM×1×C. Finally, we use cross-attention in Wevalidatethemodel’srecognitionabilitybyperformingthe
Equ.(6)tm oaggregateinformationfromT ∈R512×1×C → generative3DobjectclassificationtaskontheModelNet40
pc dataset(Wuetal.2015)andObjaversedataset(Deitkeetal.
Tp ∈ R32×1×C. Finally, we obtain the compressed token
pc 2023),usingI-typeandC-typeprompts,withresultsshownin
Tp usingzerotrainableparameters.Notely,theT inputto
pc pc Tab.2.Forclose-setzero-shotclassificationonModelNet40,
0M-Poolingisfromthepointencoder’ssecond-to-lastlayer.
weletQwen2selecttheclosestmatchingcategoryinthe40
classesasthemodel’soutput.Foropen-vocabularyclassifica-
Experiment
tiononObjaverse,weuseQwen2toevaluateifthemodel’s
Implementationdetails.WeusePhi-3(Abdinetal.2024)as outputdescribesthecategoryofgroundtruthsentence.
theLLMbackbone,withEVA-CLIP-E(Sunetal.2023)and As shown in Tab. 2, our GreenPLM-0 achieves an av-
ViT(Dosovitskiyetal.2020)bothtrainedbyUni3D(Zhou erageclassificationaccuracy(AvgAcc)of54.57%without
etal.2023)asthetextencoderandpointencoder,respectively. using any 3D data, outperforming all 2D-based models. It
The point encoder outputs 512+1 tokens, each with C = surpassesLLaVA-1.5-13Bby+21.95andPoint-BindLLM
1024.TheMLPprojectorconsistsoftwolinearlayersand by+27.89inAvgAcc.Remarkably,ourmodelalsoexceeds
aGeLUactivation,mappingtheencoder’soutputtokensto GPT4Point(660K),whichistrainedwith660K3Ddata,by
tokenswith3072dimensionsofPhi-3.OurGreenPLMhas +20.08andperformsonparwithPointLLM-7B(730K).With
63.3Mtrainableparametersandrequiresonly26.6hoursof onlyasmallamountof3Ddata(90K),GreenPLMachieves
trainingonasingle3090GPU.Besidesthestandard3-stage anaverageaccuracyof60.08%,surpassingPointLLMand
trainingofGreenPLM,wealsotrainGreenPLM-0withtext- MiniGPT by +10.95 and +11.06 in AvgAcc, respectively.
only data, utilizing only Stages I and II. During inference, GreenPLMevenoutperformsPointLLM-13B(730K)while
we simply replace the text encoder in GreenPLM-0 with using a smaller LLM, and obtains results comparable to
thepointencoderfromUni3Dwithoutweighttuning.More SOTAmodelMiniGPT-3D(730K).Additionally,GreenPLM
detailedtrainingsettingsareincludedinAppendix. (90K) outperforms MiniGPT-3D (90K) and MiniGPT-3DTable2:Generative3DobjectclassificationresultsontheModelNet40testsplitandObjaverse.Theaccuracy(%)underthe
Instruction-typed(I)prompt“Whatisthis?”andtheCompletion-type(C)prompt“Thisisanobjectof”arereported.
LLM 3DData ModelNet40 Objaverse A3DR
Model Reference Input Average
Size Size (I) (C) (I) (C) (Avg)
Text-onlyDatainTraining
InstructBLIP-7B(Daietal.2024) NIPS23 7B 0K Single-V.Img. 17.67 22.81 21.50 26.00 22.00 1.000
InstructBLIP-13B(Daietal.2024) NIPS23 13B 0K Single-V.Img. 21.56 21.92 21.50 21.50 21.62 1.000
LLaVA-1.5-7B(Liuetal.2024) CVPR24 7B 0K Single-V.Img. 27.11 21.68 37.50 30.00 29.07 1.000
LLaVA-1.5-13B(Liuetal.2024) CVPR24 13B 0K Single-V.Img. 27.71 27.76 39.50 35.50 32.62 1.000
GPT-4omini(Jacobetal.2024) OpenAI - 0K Single-V.Img. 22.00 23.10 39.00 35.00 29.78 1.000
Point-BindLLM(Guoetal.2023) arXiv23 7B 0K PointCloud 46.60 45.02 7.50 7.58 26.68 1.000
GreenPLM-0(Ours) - 3.8B 0K PointCloud 62.60(+16.00) 62.68(+17.66) 48.00(+40.50) 45.00(+37.42) 54.57(+27.89) 1.000
Limited3DDatainTraining
PointLLM-7B(Xuetal.2023) ECCV24 7B 90K PointCloud 45.22 39.30 59.00 53.00 49.13 0.749
MiniGPT-3D(Tangetal.2024) MM24 2.7B 90K PointCloud 43.56 43.03 54.50 55.00 49.02 0.748
GreenPLM(Ours) - 3.8B 90K PointCloud 58.95(+13.73) 62.36(+19.33) 60.50(+1.50) 58.50(+3.50) 60.08(+10.95) 0.792
Extensive3DDatainTraining
GPT4Point(Qietal.2024b) CVPR24 2.7B 660K PointCloud 21.39 21.07 49.00 46.50 34.49 0.526
PointLLM-7B(Xuetal.2023) ECCV24 7B 730K PointCloud 51.34 50.36 62.00 63.00 56.68 0.539
PointLLM-13B(Xuetal.2023) ECCV24 13B 730K PointCloud 51.70 52.67 61.50 63.00 57.22 0.539
MiniGPT-3D(Tangetal.2024) MM24 2.7B 730K PointCloud 61.99 60.49 65.00 68.50 64.00 0.544
Table3:3DobjectcaptioningresultsonObjaverse.TheresultsarefromQwen2evaluation,andtraditionalmetrics.
LLM 3DData
Model Reference Input Qwen2 Sentence-BERT SimCSE
Size Size
Text-onlyDatainTraining
InstructBLIP-7B(Daietal.2024) NIPS23 7B 0K Single-V.Img. 16.10 35.79 36.67
InstructBLIP-13B(Daietal.2024) NIPS23 13B 0K Single-V.Img. 13.79 33.52 35.60
LLaVA-1.5-7B(Liuetal.2024) CVPR24 7B 0K Single-V.Img. 17.80 39.32 41.08
LLaVA-1.5-13B(Liuetal.2024) CVPR24 13B 0K Single-V.Img. 16.00 39.64 40.90
GPT-4omini(Jacobetal.2024) OpenAI - 0K Single-V.Img. 26.00 38.70 39.13
Point-BindLLM(Guoetal.2023) arXiv23 7B 0K PointCloud 1.93 27.29 25.35
GreenPLM-0(Ours) - 3.8B 0K PointCloud 15.93(+14.00) 42.16(+14.87) 40.90(+15.55)
Limited3DDatainTraining
PointLLM-7B(Xuetal.2023) ECCV24 7B 90K PointCloud 35.77 46.48 47.01
MiniGPT-3D(Tangetal.2024) MM24 2.7B 90K PointCloud 35.05 46.68 47.75
GreenPLM(Ours) - 3.8B 90K PointCloud 42.55(+6.78) 49.40 (+2.72) 49.36(+1.61)
Extensive3DDatainTraining
GPT4Point(Qietal.2024b) CVPR24 2.7B 660K PointCloud 21.75 41.10 41.24
PointLLM-7B(Xuetal.2023) ECCV24 7B 730K PointCloud 42.20 48.50 48.92
PointLLM-13B(Xuetal.2023) ECCV24 13B 730K PointCloud 40.40 49.07 48.41
MiniGPT-3D(Tangetal.2024) MM24 2.7B 730K PointCloud 48.17 49.54 51.39
(730K) on the A3DR (average accuracy) by +5.9% and Table4:Ablationon3-StageTrainingandTokenFusion.
45.6%,respectively.Theseresultsdemonstratethehigh3D
data-efficiencyofourmodel. #No. StageI StageII StageIII Acc.
1 ✓ 53.85
2 ✓ 47.03
3DObjectCaptioning 3 ✓ 45.29
4 ✓ ✓ 58.25
Weevaluatetheabilitytounderstand3Dcontextthrougha 5 ✓ ✓ 42.78
3Dobjectcaptioningtask,asshowninTab.3.Followingpre- 6 ✓ ✓ 54.57
7 ✓ ✓ ✓ 60.08
viousworks(Xuetal.2023;Tangetal.2024),weassessthe
#No. ClassToken GlobalTokens PooledPointTokens Acc.
similaritybetweenthemodel’sresponseandthegroundtruth
8 ✓ 38.36
caption using an LLM, and also evaluate embedding simi- 9 ✓ ✓ 45.42
larityusingSentence-BERT(ReimersandGurevych2019) 10 ✓ ✓ ✓ 60.08
(S-BERT)andSimCSE(Gao,Yao,andChen2021).
Itisevidentthatallmodelswithout3Ddataunderperform CSEscorescomparabletoMiniGPT-3D(730K)whileusing
comparedtothosetrainedwith3Ddata,astheylosesignif- only12%of3Ddata.TheseresultsagaindemonstrateGreen-
icant3Dinformation.However,ourGreenPLM-0canstill PLM’sabilitytoefficientlyextract3Dinformationfromeven
outperformsPoint-BindLLMandachievescomparablere- smallamountsof3Ddataorpurelytextdata.
sultstopowerful2D-LLMsbyalargemargin.Whenusinga
QualitativeResults
smallamountof3Ddata(90K),ourQwen2scoresurpasses
MiniGPT-3D (90K) by +7.50, with S-BERT and SimCSE Fig.1andTab.5presentthequalitativeresults.Asshownin
scoresalsoexceedingby+2.72and+1.61,respectively.Sim- Fig.1,whethertrainedontext-onlyorwithminimal3Ddata,
ilarly,GreenPLM(90K)achievesaQwen2scorehigherthan GreenPLM provides accurate, context-aware responses in
PointLLM-13B (730K) by +2.15, with S-BERT and Sim- multi-turnconversations.Tab.5showsthatourGreenPLM-Table5: Qualitativecomparisons.Conversationexample
                     ofaguitarinModelNet40.OurGreenPLMgeneratesmore           
detailedandinsightfulresponsescomparedtoothers.         
        
             
Sample        6 W D J H  ,  6 W D J H  , ,
  
   N     N     N      N   N    N     N     N
Prompt Whatisthis?
Label Guitar Figure8:AblationonTextdatasizeinStageI&II.
Ins-BLIP telescope
     
G mP inT i-4o T orhe obim jeca tg ,e pa op sp sie ba lr ys rto esb ee ma bls ii nm gp tl he eb sl hac ak peou ot fli ane kno if fea oto ro al " mT ai"
n
t:
e
" nT ah ne
c
ef e en vc ee
r
yn e ye ed as
r. ",
             
     
     
similarimplement.      
"Q": "What do we need to   
P-Bind Thisisabirdflyinginthesky.
do for the wooden fence?",
Ours TheobjectisaGibsonLesPaulelectricguitar,predomi-   
(Text- nantlyblackwithaglossyfinish.Ithasamaplefretboard, "A": "We should check and      
only) asingle-coilpickup,andavibratobridge.Theguitar’s replace any rotten posts."   
                                  
bodyisadornedwiththeGibsonlogo,anditincludesa
strapbuttonforeasycarrying.
Figure9:Dailytextdata. Figure10:NoiseStd.
PointLLM This3Dmodelrepresentsablackelectricguitarequipped
withadistinctiveheadstock.
cloudinformationfromthetokensequence,enhancingGreen-
Ours Thisisa3Dmodelofacartoon-styleelectricguitar.The
PLM’s3Dunderstandingability.
(Limited guitarispredominantlyblack,givingitasleekandmodern
3Ddata) appearance.Thedesignissimplifiedandstylized,typical
ofcartoonaesthetics,makingitsuitableforuseinanimated T3D dataset. To test the impact of captions in our T3D
films,videogames,orotherdigitalmedia.Despiteits dataset,whichserveasinputtothetextencoder,wereplace
cartoonishappearance,itretainstherecognizablefeatures
captionswithlow-informationsentencesinStageI,andgen-
ofanelectricguitar,suchasthefretboardandstrings.
erate a 1M daily conversation dataset (example in Fig. 9).
            Using daily conversation data causes a significant perfor-
      mancedropinFig.7,indicatingthatcaptionsprovidemore
                        effectivesemanticinformationforthemodel.Moreover,we
   assesstheimpactoftextdatasizeinStagesIandII.Asshown
   inFig.8,withmoretextdata,themodellearnsfromalarger
  0  3 R R O L Q J  0 H D Q  3 R R O L Q J  0 D [  3 R R O L Q J  7  '   ' D L O \  & R Q Y 
Figure6:Ablationon Figure7:Ablationon textspace,leadingtoastrongerpoint-text-LLMconnection.
0M-Pooling. T3Dcaption. Thisconfirmstheeffectivenessofthetextspace,reducing
theneedfor3Ddataandaddressingthe3DEPLtask.
0effectivelyidentifiesobjectsandunderstandsdetailslike
colorandcomponentswithtext-onlydata.2D-basedmeth- Token Fusion before MLP projector. In Stage III, the
odslikeInstruct-BLIP(Ins-BLIP)andGPT-4ominilose3D tokens input into the MLP projector consist of three parts:
information,sufferingfromocclusion,ambiguityandsevere theClasstoken,theMix-Pooledtoken,andthe0M-Pooled
hallucinations.Point-BindLLM(P-BLLM)lacksaccurate token. We conduct ablation experiments on these three to-
3Dperceptionduetoitsnon-robust3D-2D-LLMconnection. kens,asshowninTab.4.Theresultsdemonstratethatboth
While using few 3D data (90K), GreenPLM offers signifi- Mix-Poolingand0M-Poolingenhancethemodel’sabilityto
cantlymoredetaileddescriptionsandbettercaptureslocal extractinformationfromthetokensequence.
details in point clouds, such as guitar strings and octopus
NoiselevelinStageI&II. AddingGaussiannoisetothe
suctioncups,comparedtoPointLLM.
tokensequenceoutputbythetextencoderforcestheLLMto
AblationStudy learnusefulinformationfromnoisydata,therebyimproving
themodel’srobustness.AsshowninFig.10,weexperiment
Weconductablationexperimentsonthegenerative3Dobject
with different noise levels. As the standard deviation (std)
classificationtaskandreporttheaverageaccuracy.
of the noise increases from 0 to 0.06, GreenPLM’s accu-
Trainingstages. AsshowninTab.4,removinganystage racyinitiallyincreasesandthendecreases,reachingitspeak
reducesperformance,withthebiggestdropwhenStageIis at 0.05. The results demonstrate that appropriately adding
removed.ThisisbecauseStageItrainstheMLPprojectorto noisecanenhancethemodel’sabilitytoextractcross-modal
aligntheencoderwiththeLLM.Comparingrows#4and#7, information,thereforeimprovingits3Dunderstanding.
weobservethatStageIIhelpstheLLMbetteralignwiththe
semanticspace.Theresultsofrows#6and#7indicatethat Conclusion
StageIIIinjects3DinformationintotheLLM,significantly
To enable LLMs to achieve strong 3D understanding with
enhancingthemodel’s3Dunderstanding.
minimal3Ddata,weintroduceanewtask:3DData-Efficient
0M-Pooling. As shown in Fig. 6, when we replace 0M- Point-Language Understanding. We propose GreenPLM,
Pooling with Max Pooling or Mean Pooling, the accuracy whichemploysa3-stagetrainingstrategythatincreasestext
dropsby1.96and1.58,respectively,eventhoughthelearn- datainStageI&IItoreducetheneedfor3DdatainStage
ableparametersremainzero.Thisdemonstratesthatour0M- III.Wecreatea6MT3Ddatasetandanunifiedbenchmark.
Pooling module effectively and efficiently captures point Results show that GreenPLM achieves performance com-
     \ F D U X F F $      \ F D U X F F $
     \ F D U X F F $
     \ F D U X F F $
     \ F D U X F F $parabletostate-of-the-artsusingonly12%ofthe3Ddata. Gao,T.;Yao,X.;andChen,D.2021. Simcse:Simplecon-
Remarkably,ourmodelperformswellevenwithout3Ddata. trastive learning of sentence embeddings. arXiv preprint
Limitations.Ourapproachhaslimitations.Duetotime arXiv:2104.08821.
andresourceconstraints,wecouldn’texplorealltextand3D Gao, Y.; Wang, Z.; Zheng, W.-S.; Xie, C.; and Zhou, Y.
datacombinations.Webelievescalingupeithercouldfurther 2024. SculptingHolistic3DRepresentationinContrastive
improveperformance.Additionally,weonlytestfeasibility Language-Image-3D Pre-training. In Proceedings of the
onsmallobjects,andwillexploreGreenPLM’spotentialfor IEEE/CVF Conference on Computer Vision and Pattern
largerscenesinfuturework. Recognition,22998–23008.
Guo, Z.; Zhang, R.; Zhu, X.; Tang, Y.; Ma, X.; Han, J.;
References
Chen, K.; Gao, P.; Li, X.; Li, H.; et al. 2023. Point-bind
Abdin,M.;Jacobs,S.A.;Awan,A.A.;Aneja,J.;Awadal- &point-llm:Aligningpointcloudwithmulti-modalityfor3d
lah,A.;Awadalla,H.;Bach,N.;Bahree,A.;Bakhtiari,A.; understanding,generation,andinstructionfollowing. arXiv
Behl,H.;etal.2024. Phi-3technicalreport:Ahighlycapa- preprintarXiv:2309.00615.
blelanguagemodellocallyonyourphone. arXivpreprint Han,J.;Gong,K.;Zhang,Y.;Wang,J.;Zhang,K.;Lin,D.;
arXiv:2404.14219. Qiao,Y.;Gao,P.;andYue,X.2024. Onellm:Oneframework
Brohan, A.; Brown, N.; Carbajal, J.; Chebotar, Y.; Chen, to align all modalities with language. In Proceedings of
X.; Choromanski, K.; Ding, T.; Driess, D.; Dubey, A.; theIEEE/CVFConferenceonComputerVisionandPattern
Finn, C.; et al. 2023. Rt-2: Vision-language-action mod- Recognition,26584–26595.
elstransferwebknowledgetoroboticcontrol. arXivpreprint Hong,Y.;Zhen,H.;Chen,P.;Zheng,S.;Du,Y.;Chen,Z.;and
arXiv:2307.15818. Gan,C.2023. 3d-llm:Injectingthe3dworldintolargelan-
Chen,S.;Chen,X.;Zhang,C.;Li,M.;Yu,G.;Fei,H.;Zhu, guagemodels. AdvancesinNeuralInformationProcessing
H.;Fan,J.;andChen,T.2024a. LL3DA:VisualInteractive Systems,36:20482–20494.
InstructionTuningforOmni-3DUnderstandingReasoning Hu,E.J.;Shen,Y.;Wallis,P.;Allen-Zhu,Z.;Li,Y.;Wang,
andPlanning. InProceedingsoftheIEEE/CVFConference S.;Wang,L.;andChen,W.2021. Lora:Low-rankadaptation
onComputerVisionandPatternRecognition,26428–26438. oflargelanguagemodels. arXivpreprintarXiv:2106.09685.
Chen, Y.; Wang, Q.; Wu, S.; Gao, Y.; Xu, T.; and Hu, Y. Huang, T.; Dong, B.; Yang, Y.; Huang, X.; Lau, R. W.;
2024b.Tomgpt:Reliabletext-onlytrainingapproachforcost- Ouyang,W.;andZuo,W.2023. Clip2point:Transferclipto
effectivemulti-modallargelanguagemodel. ACMTransac- pointcloudclassificationwithimage-depthpre-training. In
tionsonKnowledgeDiscoveryfromData. ProceedingsoftheIEEE/CVFInternationalConferenceon
Dai,W.;Li,J.;Li,D.;Tiong,A.M.H.;Zhao,J.;Wang,W.; ComputerVision,22157–22167.
Li,B.;Fung,P.N.;andHoi,S.2024. Instructblip:Towards Jacob, M.; Kevin, L.; Shengjia, Z.; Eric, W.; Hongyu, R.;
general-purposevision-languagemodelswithinstructiontun- Haitang,H.;Nick,S.;andFelipe,P.S.2024. GPT-4omini:
ing. Advances in Neural Information Processing Systems, advancingcost-efficientintelligence. [Online;accessed16-
36. August-2024].
Deitke, M.; Schwenk, D.; Salvador, J.; Weihs, L.; Michel, Lei,W.;Ge,Y.;Yi,K.;Zhang,J.;Gao,D.;Sun,D.;Ge,Y.;
O.;VanderBilt,E.;Schmidt,L.;Ehsani,K.;Kembhavi,A.; Shan, Y.; and Shou, M. Z. 2023. Vit-lens-2: Gateway to
andFarhadi,A.2023. Objaverse:Auniverseofannotated omni-modalintelligence. arXivpreprintarXiv:2311.16081.
3dobjects. InProceedingsoftheIEEE/CVFConferenceon Liu, H.; Li, C.; Li, Y.; and Lee, Y. J. 2024. Improved
ComputerVisionandPatternRecognition,13142–13153. baselineswithvisualinstructiontuning. InProceedingsof
Dosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn, theIEEE/CVFConferenceonComputerVisionandPattern
D.;Zhai,X.;Unterthiner,T.;Dehghani,M.;Minderer,M.; Recognition,26296–26306.
Heigold,G.;Gelly,S.;etal.2020. Animageisworth16x16 Liu,P.J.;Saleh,M.;Pot,E.;Goodrich,B.;Sepassi,R.;Kaiser,
words:Transformersforimagerecognitionatscale. arXiv L.;andShazeer,N.2018. Generatingwikipediabysumma-
preprintarXiv:2010.11929. rizinglongsequences. arXivpreprintarXiv:1801.10198.
Driess,D.;Xia,F.;Sajjadi,M.S.;Lynch,C.;Chowdhery,A.; Luo, T.; Johnson, J.; and Lee, H. 2024. View selection
Ichter,B.;Wahid,A.;Tompson,J.;Vuong,Q.;Yu,T.;etal. for 3d captioning via diffusion ranking. arXiv preprint
2023. Palm-e: An embodied multimodal language model. arXiv:2404.07984.
arXivpreprintarXiv:2303.03378. Luo,T.;Rockwell,C.;Lee,H.;andJohnson,J.2024. Scal-
Dubey,A.;Jauhri,A.;Pandey,A.;Kadian,A.;Al-Dahle,A.; able 3d captioning with pretrained models. Advances in
Letman, A.; Mathur, A.; Schelten, A.; Yang, A.; Fan, A.; NeuralInformationProcessingSystems,36.
et al. 2024. The llama 3 herd of models. arXiv preprint OpenAI,R.2023. Gpt-4technicalreport.arxiv2303.08774.
arXiv:2407.21783. ViewinArticle,2(5).
Fu,D.;Li,X.;Wen,L.;Dou,M.;Cai,P.;Shi,B.;andQiao, Qi,Z.;Dong,R.; Fan,G.;Ge,Z.;Zhang, X.;Ma,K.;and
Y.2024. Drivelikeahuman:Rethinkingautonomousdriving Yi,L.2023. Contrastwithreconstruct:Contrastive3drep-
withlargelanguagemodels.InProceedingsoftheIEEE/CVF resentationlearningguidedbygenerativepretraining. InIn-
WinterConferenceonApplicationsofComputerVision,910– ternationalConferenceonMachineLearning,28223–28243.
919. PMLR.Qi, Z.; Dong, R.; Zhang, S.; Geng, H.; Han, C.; Ge, Z.; Yang,A.;Yang,B.;Hui,B.;Zheng,B.;Yu,B.;Zhou,C.;Li,
Yi, L.; and Ma, K. 2024a. Shapellm: Universal 3d ob- C.;Li,C.;Liu,D.;Huang,F.;Dong,G.;Wei,H.;Lin,H.;
jectunderstandingforembodiedinteraction. arXivpreprint Tang,J.;Wang,J.;Yang,J.;Tu,J.;Zhang,J.;Ma,J.;Xu,J.;
arXiv:2402.17766. Zhou,J.;Bai,J.;He,J.;Lin,J.;Dang,K.;Lu,K.;Chen,K.;
Qi,Z.;Fang,Y.;Sun,Z.;Wu,X.;Wu,T.;Wang,J.;Lin,D.; Yang,K.;Li,M.;Xue,M.;Ni,N.;Zhang,P.;Wang,P.;Peng,
and Zhao, H. 2024b. Gpt4point: A unified framework for R.; Men, R.; Gao, R.; Lin, R.; Wang, S.; Bai, S.; Tan, S.;
point-languageunderstandingandgeneration.InProceedings Zhu,T.;Li,T.;Liu,T.;Ge,W.;Deng,X.;Zhou,X.;Ren,X.;
oftheIEEE/CVFConferenceonComputerVisionandPattern Zhang,X.;Wei,X.;Ren,X.;Fan,Y.;Yao,Y.;Zhang,Y.;Wan,
Recognition,26417–26427. Y.;Chu,Y.;Liu,Y.;Cui,Z.;Zhang,Z.;andFan,Z.2024a.
Qwen2TechnicalReport. arXivpreprintarXiv:2407.10671.
Radford,A.;Kim,J.W.;Hallacy,C.;Ramesh,A.;Goh,G.;
Agarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.; Yang,A.;Yang,B.;Hui,B.;Zheng,B.;Yu,B.;etal.2024b.
etal. 2021. Learningtransferable visualmodelsfrom nat- Qwen2TechnicalReport. arXivpreprintarXiv:2407.10671.
urallanguagesupervision. InInternationalconferenceon Zhou,J.;Wang,J.;Ma,B.;Liu,Y.-S.;Huang,T.;andWang,
machinelearning,8748–8763.PMLR. X.2023. Uni3d:Exploringunified3drepresentationatscale.
Reimers,N.;andGurevych,I.2019. Sentence-bert:Sentence arXivpreprintarXiv:2310.06773.
embeddings using siamese bert-networks. arXiv preprint
arXiv:1908.10084.
Schuhmann,C.;Beaumont,R.;Vencu,R.;Gordon,C.;Wight-
man, R.; Cherti, M.; Coombes, T.; Katta, A.; Mullis, C.;
Wortsman,M.;etal.2022. Laion-5b:Anopenlarge-scale
datasetfortrainingnextgenerationimage-textmodels. Ad-
vancesinNeuralInformationProcessingSystems,35:25278–
25294.
Sun,Q.;Fang,Y.;Wu,L.;Wang,X.;andCao,Y.2023. Eva-
clip: Improved training techniques for clip at scale. arXiv
preprintarXiv:2303.15389.
Tang,Y.;Han,X.;Li,X.;Yu,Q.;Hao,Y.;Hu,L.;andChen,
M.2024.MiniGPT-3D:EfficientlyAligning3DPointClouds
withLargeLanguageModelsusing2DPriors. arXivpreprint
arXiv:2405.01413.
Team,G.;Anil,R.;Borgeaud,S.;Wu,Y.;Alayrac,J.-B.;Yu,
J.;Soricut,R.;Schalkwyk,J.;Dai,A.M.;Hauth,A.;etal.
2023.Gemini:afamilyofhighlycapablemultimodalmodels.
arXivpreprintarXiv:2312.11805.
Wang, Y.; Kordi, Y.; Mishra, S.; Liu, A.; Smith, N. A.;
Khashabi,D.;andHajishirzi,H.2022. Self-instruct:Align-
inglanguagemodelswithself-generatedinstructions. arXiv
preprintarXiv:2212.10560.
Wu, Z.; Song, S.; Khosla, A.; Yu, F.; Zhang, L.; Tang,X.;
andXiao,J.2015. 3dshapenets:Adeeprepresentationfor
volumetricshapes. InProceedingsoftheIEEEconference
oncomputervisionandpatternrecognition,1912–1920.
Xu, R.; Wang, X.; Wang, T.; Chen, Y.; Pang, J.; and Lin,
D.2023. Pointllm:Empoweringlargelanguagemodelsto
understandpointclouds. arXivpreprintarXiv:2308.16911.
Xue,L.;Gao,M.;Xing,C.;Mart´ın-Mart´ın,R.;Wu,J.;Xiong,
C.;Xu,R.;Niebles,J.C.;andSavarese,S.2023. Ulip:Learn-
ingaunifiedrepresentationoflanguage,images,andpoint
cloudsfor3dunderstanding.InProceedingsoftheIEEE/CVF
conferenceoncomputervisionandpatternrecognition,1179–
1189.
Xue,L.;Yu,N.;Zhang,S.;Panagopoulou,A.;Li,J.;Mart´ın-
Mart´ın,R.;Wu,J.;Xiong,C.;Xu,R.;Niebles,J.C.;etal.
2024. Ulip-2:Towardsscalablemultimodalpre-trainingfor
3dunderstanding. InProceedingsoftheIEEE/CVFConfer-
enceonComputerVisionandPatternRecognition,27091–
27101.Appendix parameters ranging from 6.2M to 1016.5M, as shown in
HereintheAppendix,wepresentthedetaileddistribution Tab. 6. The results indicate that as the point encoder size
oftheT3Ddataset,alongwiththepromptsandinstructions increases,themodel’sperformancefirstimprovesandthen
usedtocreateit,andprovideseveraldataexamples.Wealso declines,achievingthebestaccuracywith22.6Mparameters.
showcasemorevisualresultcomparisons.Additionally,we Notably, even with just 6.2M parameters, GreenPLM still
provideadditionalablationresultsandmoredetailedtraining demonstratesstrong3Dunderstanding,furtherprovingthe
parameters. Finally, we include illustrations of the model efficiencyofourmodel.
architectureusedduringtrainingandinference.
TrainingandInferenceArchitecture
Our6MT3DDataset We show the difference of architectures between training
Distributions Weshowthedetaileddistributionsofour6M andinferenceinFig.20.Notethateachstageofour3-stage
T3DdatasetinFig.11andFig.12.Specifically,inFig.11, strategycanbeusedforinference.Specifically,ifusingthe
weshowwordcloudsforcaptionsandresponses.Following Stage I or Stage II for inference, simply replace the text
Wangetal.(2022),wealsopresentthedistributionofverb- encoderwiththealignedpointencoder.
nounpairsinthedataset,highlightingitsdiverseattributes.
Additionally,inFig.12,wedisplaythelengthdistribution
fordifferentdatatypes;forinstance,mostbriefanddetailed
descriptionsrangefromabout18to42words.
PromptsandInstructions Here,weshowoneexampleof
thedatagenerationpipelineusingQwen2-72B-Instruct(Yang
etal.2024b)inFig.13,alsotheinstructionlistfordescription
datainFig.7andFig.8.
Data Samples We give several samples of four types of
datafromourT3Ddataset,showninFig.14-17.Pleasesee
supplementarymaterialformoredatasamplesofourT3D
dataset.
QualitativeResults
We show more qualitive results of our GreenPLM-0 and
GreenPLM in Fig. 18 and Fig. 19. Trained with only text
data,ourGreenPLM-0accuratelyidentifiestheshape,color,
andusageof3Dobjects.Forexample,inthebottomrightof
Fig.18,themodelnotonlycorrectlyrecognizestheshoe’s
category and purpose but also accurately distinguishes be-
tween the left and right shoe. When using only a small
amountof3Ddata,ourGreenPLMcanaccuratelyandthor-
oughly describe 3D objects, as shown in the first row of
Fig.19.
DetailedTrainingSettings
WereportmoredetailedtrainingsettingsinTab.9.
Ablationonthesizeofpointencoder
Table6:Ablationonpointencodersize.
PointEncoder ParamSize Avg.Acc.
Giant 1016.5M 48.36
Large 306.7M 54.50
Base 88.4M 52.74
Small 22.6M 54.57
Tiny 6.2M 54.34
We conduct ablation experiments on the generative 3D
object classification task and report the average accuracy.
Weexperimentwith5differentsizesofpointencoders,with(a) Textcaptionfor3Dobject.
(b) Briefdescriptionresponse. (c) Detaileddescriptionresponse.
(cid:14)(cid:12)(cid:12)(cid:2)(cid:14)(cid:5)(cid:14)(cid:8)(cid:4)(cid:2)
(cid:3)(cid:13)(cid:14)(cid:12)(cid:2) (cid:0)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:2)
(cid:19)(cid:6)(cid:15)(cid:9)(cid:2)(cid:2)(cid:0)(cid:6) (cid:17)(cid:3)
(cid:17)(cid:4)(cid:20)(cid:3)(cid:2)(cid:3)(cid:21)(cid:2)(cid:3)(cid:2)(cid:16)(cid:14)(cid:2)(cid:22)(cid:2)(cid:9)(cid:16) (cid:5)(cid:13)(cid:14)(cid:0)(cid:7)(cid:6)(cid:18)(cid:23)(cid:7)(cid:6)(cid:16)(cid:23)(cid:8) (cid:2)(cid:4)(cid:6)(cid:0)(cid:6)(cid:8)(cid:17)(cid:20)(cid:9)(cid:14)(cid:11)(cid:0)(cid:9)(cid:2)(cid:2)
(cid:16)(cid:14)(cid:6)(cid:16)(cid:9)(cid:12)(cid:2)(cid:5)(cid:0) (cid:2)(cid:18)(cid:6) (cid:3)(cid:5)(cid:16) (cid:3)(cid:6)(cid:6)(cid:2) (cid:6)(cid:23)(cid:9)(cid:8)(cid:2)(cid:9)(cid:8)(cid:3)(cid:8)(cid:22) (cid:14)(cid:11)(cid:10)(cid:9)(cid:3)(cid:6)(cid:6)(cid:20)(cid:0)(cid:8)(cid:25)(cid:6)(cid:5)(cid:16) (cid:6)(cid:12)(cid:23)(cid:3) (cid:2)(cid:3)(cid:2)(cid:9) (cid:17)(cid:14) (cid:13)(cid:13) (cid:8)(cid:9)(cid:18)(cid:11)
(cid:2)(cid:2)(cid:5)(cid:14)(cid:6)(cid:6)(cid:12)(cid:14)(cid:8)(cid:17)(cid:12)(cid:3)(cid:2) (cid:2)(cid:9)(cid:3)(cid:17) (cid:0)(cid:23)(cid:2)(cid:11)(cid:9) (cid:2)(cid:7)(cid:16)(cid:18)(cid:11)(cid:24)(cid:17)(cid:6)(cid:2)(cid:17)(cid:2)(cid:8)(cid:2)(cid:2)(cid:4)(cid:8)(cid:0)(cid:9)(cid:9)
(cid:6)(cid:0)(cid:2)(cid:8)(cid:9)(cid:13) (cid:6)(cid:10)(cid:5) (cid:14)(cid:2) (cid:11)(cid:15)(cid:3) (cid:2)(cid:2) (cid:5)(cid:16) (cid:2)(cid:12)(cid:7) (cid:5)(cid:16) (cid:17) (cid:2)(cid:2) (cid:3)(cid:14)(cid:0) (cid:2)(cid:18)(cid:2) (cid:8)(cid:2)(cid:12) (cid:9)(cid:6)(cid:4)(cid:9) (cid:16)(cid:14)(cid:9)(cid:2)(cid:4)(cid:5)(cid:13)(cid:14)(cid:6)(cid:5)(cid:14)(cid:14)(cid:4)(cid:9)(cid:17)(cid:2)(cid:5)(cid:23)(cid:9)(cid:11)(cid:3)(cid:9)(cid:7)(cid:11)(cid:12)(cid:17)(cid:2)(cid:3)(cid:2)(cid:9)(cid:24)(cid:23)(cid:14) (cid:2)(cid:5)(cid:11)(cid:12) (cid:4)(cid:18)(cid:12) (cid:9)(cid:6)(cid:3)(cid:2) (cid:8)(cid:9)(cid:18)(cid:5)(cid:14)(cid:20)(cid:6)(cid:0)(cid:8)(cid:4)(cid:5) (cid:0)(cid:9)(cid:2) (cid:20)(cid:14)(cid:22) (cid:5)(cid:9)(cid:2) (cid:2)(cid:8)(cid:5)(cid:0)(cid:9)(cid:9)(cid:6)(cid:11)(cid:2)(cid:12)(cid:23)(cid:11)(cid:4)(cid:2)(cid:3)(cid:5)(cid:6)(cid:4) (cid:12)(cid:19) (cid:2)(cid:8)(cid:23) (cid:2)(cid:17)(cid:23)(cid:5)
(cid:10)(cid:2)(cid:14)(cid:9)(cid:20)(cid:5)(cid:2)(cid:18)(cid:6)(cid:8)(cid:0)
(cid:3)(cid:4)(cid:13)(cid:2)(cid:16)(cid:2) (cid:16)(cid:14)(cid:9)(cid:2)(cid:5)(cid:6)(cid:14)(cid:17) (cid:9)(cid:11)(cid:12)(cid:2)(cid:18)(cid:6)(cid:8)(cid:0)
(cid:4)(cid:23)(cid:17)(cid:23)(cid:5) (cid:4)(cid:23)(cid:17)(cid:23)(cid:5)
(d) Single-roundinstruc(cid:9)(cid:11)(cid:12)(cid:2)t(cid:23)(cid:7)i(cid:24)(cid:2)(cid:4)o(cid:9)n. (e) Single-roundresponse.
(cid:4)(cid:18)(cid:17)(cid:18)(cid:5)
(cid:9)(cid:16)(cid:16)(cid:2)(cid:9)(cid:5)(cid:9)(cid:11)(cid:4)(cid:2)
(cid:13)(cid:2)(cid:9)(cid:12)(cid:19)(cid:5)(cid:2)
(cid:3)(cid:4)(cid:8)(cid:2)(cid:22)(cid:2)
(cid:13)(cid:2)(cid:9)(cid:12)(cid:19)(cid:5)(cid:2)
(cid:8)(cid:9)(cid:10)(cid:2)
(cid:0)(cid:2)(cid:3) (cid:2)(cid:4) (cid:15)(cid:16)(cid:5) (cid:17)(cid:9)(cid:6) (cid:6)(cid:7) (cid:11)(cid:2)
(cid:6)(cid:0)(cid:5) (cid:2)(cid:2)(cid:16) (cid:11)(cid:5)(cid:2) (cid:12)(cid:3) (cid:6)(cid:6) (cid:2) (cid:13)(cid:11) (cid:11) (cid:14)(cid:4) (cid:12)(cid:18)(cid:5)(cid:16)(cid:18) (cid:3)(cid:5) (cid:2)(cid:9) (cid:5)(cid:12)(cid:2) (cid:10)(cid:6)(cid:11) (cid:2)(cid:4)(cid:17)(cid:19)(cid:0)(cid:2)(cid:20)(cid:2)(cid:9)(cid:5)(cid:3) (cid:19)(cid:19) (cid:3)(cid:21) (cid:2)(cid:21)(cid:2)(cid:3)(cid:12)(cid:17)(cid:6) (cid:0)(cid:2) (cid:3)(cid:11) (cid:12)(cid:0)(cid:0) (cid:18)(cid:3)(cid:2)(cid:18) (cid:19)(cid:2) (cid:16)(cid:22) (cid:5)(cid:11) (cid:13)(cid:2)(cid:6)(cid:6)(cid:8)(cid:6) (cid:4)(cid:11)(cid:11)(cid:9)
(cid:12)(cid:22)(cid:9)(cid:0)(cid:11)(cid:5) (cid:12)(cid:2) (cid:9)(cid:4)(cid:2)(cid:3)(cid:2) (cid:23)(cid:2)(cid:22) (cid:2)(cid:7)(cid:17)(cid:2)
(cid:13)(cid:2)(cid:9)(cid:12)(cid:19)(cid:3)(cid:12)(cid:14)(cid:2) (cid:5)(cid:17)(cid:2)(cid:2)(cid:17)(cid:2)(cid:9) (cid:22)(cid:4)(cid:4) (cid:2)(cid:2)(cid:13) (cid:11)(cid:2)(cid:3)(cid:9)
(cid:12)(cid:12)(cid:19)(cid:3)(cid:5)(cid:2)(cid:18)(cid:5)(cid:4)(cid:14)(cid:18)(cid:13) (cid:17)(cid:18)(cid:2)
(cid:5)(cid:9)(cid:12)(cid:19)(cid:5)(cid:23)(cid:23)(cid:2)(cid:6)(cid:6)(cid:11)(cid:11)(cid:0)(cid:12)(cid:14)(cid:0)(cid:16)(cid:23) (cid:2)(cid:6)(cid:12)(cid:14)(cid:11)(cid:0) (cid:16)(cid:13) (cid:6) (cid:2)(cid:19) (cid:0)(cid:3)(cid:16)(cid:11) (cid:4)(cid:18)(cid:17)(cid:17)(cid:4) (cid:18)(cid:9)(cid:19)(cid:5)(cid:12) (cid:14)(cid:6) (cid:3)(cid:19)(cid:18) (cid:3)(cid:2)(cid:11) (cid:2)(cid:9)(cid:13) (cid:17)(cid:4)(cid:2)(cid:3) (cid:6)(cid:18)(cid:9)(cid:12) (cid:12)(cid:17)(cid:4)(cid:13) (cid:9) (cid:18)(cid:12) (cid:14)(cid:18)(cid:19) (cid:19)(cid:5)(cid:7)(cid:22)(cid:11) (cid:5)(cid:6)(cid:16) (cid:2)(cid:17)(cid:4) (cid:2)(cid:18)(cid:6)(cid:17)(cid:11)(cid:12) (cid:2)(cid:3) (cid:12)(cid:23)(cid:6) (cid:22)(cid:2)(cid:9) (cid:14)(cid:18) (cid:11)(cid:18) (cid:13)(cid:6)(cid:12)(cid:2)(cid:22) (cid:12)(cid:2)(cid:12)(cid:11)(cid:14)(cid:11)(cid:11)(cid:7)(cid:9) (cid:0) (cid:14)(cid:12)(cid:12)(cid:16) (cid:0)(cid:16)(cid:22) (cid:14)(cid:24)(cid:9)(cid:18)(cid:2)(cid:3) (cid:16)(cid:16) (cid:2)(cid:18) (cid:2)(cid:12) (cid:3)(cid:0)(cid:2) (cid:17)(cid:9)(cid:5) (cid:2)(cid:4)(cid:7)(cid:9) (cid:6)(cid:6)(cid:19) (cid:2)(cid:12) (cid:21)(cid:5) (cid:12)(cid:12)(cid:24)(cid:2)(cid:4)(cid:9) (cid:17)(cid:14)(cid:2) (cid:11)(cid:12)(cid:11)(cid:5)(cid:19)(cid:4)(cid:6)(cid:4)(cid:5)(cid:9)(cid:2)(cid:12)(cid:2)(cid:17)
(cid:2)(cid:17)(cid:2)(cid:22)(cid:2)(cid:11)(cid:12) (cid:18)(cid:7)(cid:24)(cid:2)(cid:4)(cid:12)(cid:13) (cid:12)(cid:14)(cid:19) (cid:16)(cid:2)(cid:11)(cid:4)(cid:12)(cid:6)(cid:18)(cid:11)(cid:16)(cid:19)(cid:5)(cid:16)(cid:3) (cid:18)(cid:6)(cid:21) (cid:3)(cid:2)(cid:11)(cid:6)(cid:13)(cid:6)(cid:4)(cid:9)(cid:11)(cid:4)(cid:2)(cid:13)(cid:2)(cid:9)(cid:12)(cid:19)(cid:5)(cid:2) (cid:2)(cid:17)(cid:2)(cid:22)(cid:2)(cid:11)(cid:12)(cid:23)(cid:6)(cid:11)(cid:0) (cid:12)(cid:14)(cid:16)(cid:2)(cid:2)(cid:17)(cid:2)(cid:22)(cid:2)(cid:11)(cid:12)
(f) Multi-roundinstruction. (g) Multi-roundresponse.
(cid:23)(cid:6)(cid:11)(cid:0) (cid:16)(cid:19)(cid:5)(cid:16)(cid:18)(cid:3)(cid:2)
(cid:4)(cid:18)(cid:17)(cid:18)(cid:5) (cid:12)(cid:14)(cid:16)(cid:2)(cid:13)(cid:2)(cid:9)(cid:12)(cid:19)(cid:5)(cid:2)
Figure11:Wordcloudsandverb-nounpairdistributionsofalltypesofdatainourT3Ddataset.      
   
  
   
  
   
  
  
       
                                         
 / H Q J W K  / H Q J W K
(a) Briefdescriptioninstruction. (b) Briefdescriptionresponse.
  
   
   
  
   
      
   
  
  
       
                                       
 / H Q J W K  / H Q J W K
(c) Detaileddescriptioninstruction. (d) Detaileddescriptionresponse.
       
       
       
      
      
      
       
                                                   
 / H Q J W K  / H Q J W K
(e) Single-roundinstruction. (f) Single-roundresponse.
   
   
   
   
   
   
   
  
   
  
      
       
                                                   
 / H Q J W K  / H Q J W K
(g) Multi-roundinstruction. (h) Multi-roundresponse.
Figure12:SentencelengthofalltypesofdatainourT3Ddataset.
  .   U H E P X 1
  .   U H E P X 1
  .   U H E P X 1
  .   U H E P X 1
  .   U H E P X 1
  .   U H E P X 1
  .   U H E P X 1
  .   U H E P X 1System Prompt Based on the sentence above, further imagine and design a 3D object
1. Write a detailed caption by describing it in 40-50 words, including its type, color, appearance, and any other related information as
functionalities, usage, etc. that are obtainable by only looking at this 3D object. The info of caption must be accurate.
2. Design 1 single round Q&A, ask "Summarize the 3D point cloud object briefly".
3. Design 1 single round Q&A, ask "Offer a detailed description of this point cloud."，the answer is the same as the info of the caption
in "1", but the expression is different
4. Generate 3 single round Q&As registering diverse aspects of the object based on the caption in "1", Questions should be different
from those in "2" ,"3".
5. Construct 1 set of 3-round Q&As based on the caption in "1" , ensuring logical reliability between the questions and answers.
Questions should be different from those in "2", "3", "4".
Only pose questions with definite answers that can be confidently answered by examining the content in the caption. The answers
should be in a tone that a visual Al assistant. Provide detailed answers when answering complex questions. You should try to avoid
asking “Q: Is there xxx? A: Yes, xxx.”.
Format your response as: JSON
{
"caption": "description",
"brief": [{"Q": "Q", "A": "A"} x 1],
"detail": [{"Q": "Q", "A": "A"} x 1],
"single conversation": [{"Q": "Q", "A": "A"},{"Q": "Q", "A": "A"},{"Q": "Q", "A": "A"}],
"multi conversation": [{"Q1": "Q", "A1": "A", "Q2": "Q", "A2": "A", "Q3": "Q", "A3": "A"} x 1]
}
Please only output the content of JSON.
Input A white dog.
Output {
"caption": "A 3D model of a friendly white dog, sitting upright on its hind legs, with floppy ears and a pink nose. The dog's coat issmooth
and shiny, and it has a black collar around its neck. This model could be used for animation, games, or educational purposes.",
"brief": [
{
"Q": "Summarize the 3D point cloud object briefly.", "A": "The 3D object is a white dog model with a black collar, suitable for
various digital applications."
}],
"detail": [
{
"Q": "Offer a detailed description of this point cloud.", "A": "This 3D object depicts a white dog in a sitting position, characterized
by its smooth coat, black collar, and a pink nose, ideal for digital content creation."
} ],
"single conversation": [
{
"Q": "What is the primary color of the 3D object?", "A": "The primary color of the 3D object is white, representing the dog's coat
color."
},
{
"Q": "What is the posture of the dog in the 3D model?", "A": "The dog in the 3D model is sitting upright on its hind legs."
},
{
"Q": "What accessory does the dog wear in the 3D model?", "A": "The dog wears a black collar around its neck in the 3D model."
}],
"multi conversation": [
{
"Q1": "What type of animal does the 3D model represent?", "A1": "The 3D model represents a dog.",
"Q2": "Can you describe the dog's posture in the model?", "A2": "The dog is depicted sitting upright on its hind legs.",
"Q3": "What additional detail does the model include regarding the dog's appearance?", "A3": "The dog has a black collar and a
pink nose, with floppy ears and a shiny, smooth coat."
} ]
}
Figure13:AnexampleofdatagenerationpipelineusingQwen2-72B-Instruct.Givenanyobjectcategory,theLLMgenerates
5typesofdatabasedonourdesignedprompttemplates.TheoutputisinJSONformat,includingacaption,briefdescription,
detaileddescription,threeroundsofsingle-turnconversation,andoneroundofmulti-turnconversation.Table7:Theinstructionlistforbriefdescriptions.WefollowPointLLM(Xuetal.2023)byusingmorediverseinstructions,
replacingthegeneratedsimplerquestionsinbriefdescriptions,asthefinalinstructions.
• Summarizethe3Dpointcloudobjectbriefly.
• Whatkindofobjectisdepictedbythispointcloud?
• Provideashortexplanationofthis3Dstructure.
• Whatdoesthiscollectionofpointsrepresent?
• Offerasuccinctsummaryofthis3Dobject.
• Canyougiveabriefoverviewofthispointcloud?
• Characterizetheobjectthispointcloudisillustrating.
• Shareabriefinterpretationofthis3Dpointcloud.
• Provideanoutlineofthis3Dshape’scharacteristics.
• Whatobjectisthispointcloudrendering?
• Deliveraquickdescriptionoftheobjectrepresentedhere.
• Howwouldyoudescribethe3Dformshowninthispointcloud?
• Whatisthenatureoftheobjectthispointcloudisrepresenting?
• Presentacompactaccountofthis3Dobject’skeyfeatures.
• Whatcanyouinferabouttheobjectfromthispointcloud?
• Offeraclearandconcisedescriptionofthispointcloudobject.
• Howwouldyousummarizethis3Ddataset?
• Giveabriefexplanationoftheobjectthatthiscloudofpointsforms.
• Whatkindofstructuredoesthis3Dpointclouddepict?
• Couldyoudelineatetheformindicatedbythispointcloud?
• Expressinbrief,whatthispointcloudisrepresenting.
• Giveaquickoverviewoftheobjectrepresentedbythis3Dcloud.
• Conveyasummaryofthe3Dstructurerepresentedinthispointcloud.
• Whatkindofobjectisillustratedbythiscollectionofpoints?
• Describetheobjectthatthispointcloudforms.
• Howwouldyouinterpretthis3Dpointcloud?
• Canyoubrieflyoutlinetheshaperepresentedbythesepoints?
• Giveaconciseinterpretationofthe3Ddatapresentedhere.
• Explaintheobjectthispointclouddepictssuccinctly.
• Offerasummaryofthe3Dobjectillustratedbythiscloud.Table8:Theinstructionlistfordetaileddescriptions.WefollowPointLLM(Xuetal.2023)byusingmorediverseinstructions,
replacingthegeneratedsimplerquestionsindetaileddescriptions,asthefinalinstructions.
• Canyoutellmemoreaboutthis?
• Whatdoesthisrepresent?
• Canyoudescribethisinmoredetail?
• I’minterestedinthis,canyouexplain?
• Whatisthisobjectmadeof?
• Couldyouprovidemoreinfoaboutthis?
• WhatexactlyamIlookingathere?
• Whatisthis?
• Couldyoudescribethedetailedstructureofthis?
• Thislooksinteresting,canyouexpandonit?
• Canyouexplainmoreaboutthisform?
• Whatcanyoutellmeabouttheshapeofthisobject?
• Couldyoudelvedeeperintothis?
• Iwanttoknowmoreaboutthis,canyouhelp?
• Canyouwalkmethroughthedetailsofthisobject?
• Canyouprovideacomprehensiveaccountofthisobject?
• Offeradetailedinterpretationofthispointcloud.
• Pleaseelucidateonthecharacteristicsofthisform.
• Couldyouprovideanin-depthdescriptionofthisstructure?
• Whatdoesthiscloudrepresentinitsentirety?
• Elaborateonthedetailsofthispointcloud,please.
• Kindlyfurnishmewithmoreinformationaboutthisobject.
• Pleaseexpandontheintricatestructureofthisform.
• Provideameticulousexplanationofwhatthesepointsrepresent.
• Irequestadetailedbreakdownofthisstructure.
• Giveathoroughrundownofthispointcloud.
• Canyouofferacompleteanalysisofthisobject?
• Iwouldlikeacomprehensiveexplanationofthisform.
• Pleasedetailthespecificfeaturesofthispointcloud.
• Couldyouelaborateextensivelyonwhatthisrepresents?{
"caption": "A 3D model of a fantasy humanoid figure, cast in matte gray, showcases a muscular build with an aggressive
stance. Adorned with a short, textured skirt, extended forearms protected by bracers, and a distinctive horned helmet, the
figure exudes strength and dynamism.",
"conversations": [
{
"from": "human",
"value": "<point>\nWhat is the nature of the object this point cloud is representing? "
Sample 1 },
{
"from": "gpt",
"value": "The 3D object is a matte gray fantasy humanoid model with a muscular physique, wearing a textured skirt,
bracers, and a horned helmet, standing in an aggressive posture."
}
]
}
{
"caption": "A black metal garment rack features a sturdy rectangular base, two tall vertical poles, and an adjustable
horizontal bar designed for hanging clothes. The rack's minimalist design allows for easy mobility and space-saving
storage solutions.",
"conversations": [
{
"from": "human",
Sample 2 "value": "<point>\nDeliver a quick description of the object represented here. "
},
{
"from": "gpt",
"value": "It's a portable black metal garment rack with an adjustable hanging bar."
}
]
}
{
"caption": "A 3D model of a stylized ninja character depicted in a fluid running motion. The figure is entirely black,
featuring a sleek hooded outfit that contours to its athletic build. A quiver full of arrows is strapped to its back, signifying
the ninja's proficiency in ranged combat.",
"conversations": [
{
"from": "human",
Sample 3 "value": "<point>\nGive a brief explanation of the object that this cloud of points forms. "
},
{
"from": "gpt",
"value": "The object is a 3D model of a black-clad ninja in a running pose, equipped with a quiver of arrows."
}
]
}
Figure14:BriefdescriptionsamplesofourT3Ddataset.{
"caption": "A set of cylindrical brown wax candles with a melted top and a visible wick, typically used for decorative
purposes or to create a cozy atmosphere.",
"conversations": [
{
"from": "human",
"value": "<point>\nIwant to know more about this, can you help?"
},
Sample 1
{
"from": "gpt",
"value": "The 3D model represents a collection of cylindrical candles made of brown wax. Each candle has a melted
top, indicating recent use, and a visible wick suggesting they're ready to be lit again. They are commonly used for
decoration or to enhance ambiance."
}
]
}
{
"caption": "A tiered amphitheater-like structure features multiple levels of stair-step seating platforms, outlined
prominently in contrasting blue and yellow hues. This open-air venue is designed for events, gatherings, or performances,
offering clear sightlines from every seat.",
"conversations": [
{
"from": "human",
"value": "<point>\nProvidea meticulous explanation of what these points represent."
Sample 2 },
{
"from": "gpt",
"value": "Visualized is an amphitheater-inspired design composed of several tiers, each serving as a seating platform.
The structure's outlines are highlighted in blue and yellow, making it visually striking. It's an open-space venue intended
for facilitating events, assemblies, or performances, ensuring unobstructed views for all attendees."
}
]
}
{
"caption": "A red office chair with a five-point star base and a smooth, glossy finish. It features adjustable height, swivel
capabilities, and ergonomic design for comfortable seating during long work hours.",
"conversations": [
{
"from": "human",
"value": "<point>\nOffera detailed interpretation of this point cloud."
},
Sample 3
{
"from": "gpt",
"value": "This 3D object is an office chair in vibrant red, equipped with a five-pronged star base providing stability.
Its glossy surface adds a touch of elegance, while adjustable height and swivel functions enhance usability. Ergonomically
designed for optimal comfort during extended periods of sitting."
}
]
}
Figure15:DetaileddescriptionsamplesofourT3Ddataset.{
"caption": "A 3D model of a fantasy humanoid figure, cast in matte gray, showcases a muscular build with an aggressive
stance. Adorned with a short, textured skirt, extended forearms protected by bracers, and a distinctive horned helmet, the
figure exudes strength and dynamism.",
"conversations": [
{
"from": "human",
Sample 1 "value": "What is the color of the 3D model?"
},
{
"from": "gpt",
"value": "The 3D model is rendered in a matte gray color."
}
]
}
{
"caption": "A minimalist abstract sculpture of a red humanoid figure, characterized by geometric shapes and clean lines,
standing approximately 1 meter tall with a smooth matte finish, ideal for modern art collections or interior design accents.",
"conversations": [
{
"from": "human",
"value": "Can you describe the style of the sculpture?"
Sample 2
},
{
"from": "gpt",
"value": "The style is minimalist and abstract, using basic geometric shapes to form a humanoid figure."
}
]
}
{
"caption": "A purple rectangular prism, resembling a folded yoga mat with an attached smaller flap, designed for compact
storage and easy carrying. Its vibrant color and foldable design make it ideal for fitness enthusiasts.",
"conversations": [
{
"from": "human",
"value": "What is the primary function of the 3D object?"
Sample 3 },
{
"from": "gpt",
"value": "The primary function of the object is to serve as a foldable yoga or gym mat, offering convenience for users
on the go."
}
]
}
Figure16:Single-roundconversationsamplesofourT3Ddataset.{
"caption": "A monochromatic 3D sculpture depicting a human figure in a suit with an outstretched arm. The figure stands
approximately 1 meter tall, crafted entirely in a uniform shade of matte black. The suit is tailored, with sharp creases and
folds, indicating high-quality material. The outstretched arm suggests a gesture of greeting or presentation.",
"conversations": [
{
"from": "human",
"value": "What is the primary color of the sculpture?"
},
{
"from": "gpt",
"value": "The sculpture is monochromatic, with a primary color of matte black."
},
{
"from": "human",
Sample 1
"value": "Can you describe the pose of the figure?"
},
{
"from": "gpt",
"value": "The figure has an outstretched arm, which typically signifies a gesture of greeting or presentation."
},
{
"from": "human",
"value": "How detailed is the suit in the sculpture?"
},
{
"from": "gpt",
"value": "The suit is highly detailed, featuring sharp creases and folds that emulate the texture of real fabric,
suggesting a high level of craftsmanship and attention to detail."
}
]
}
{
"caption": "A red office chair with a five-point star base and a smooth, glossy finish. It features adjustable height, swivel
capabilities, and ergonomic design for comfortable seating during long work hours.",
"conversations": [
{
"from": "human",
"value": "Can you tell me about the material of the chair?"
},
{
"from": "gpt",
"value": "The chair has a smooth, glossy finish, suggesting a hard, polished material likely plastic or metal for the
exterior."
},
{
"from": "human",
Sample 2
"value": "Is the chair suitable for long working hours?"
},
{
"from": "gpt",
"value": "Absolutely, the chair is ergonomically designed to ensure comfort even during extended periods of sitting."
},
{
"from": "human",
"value": "What makes this chair different from others?"
},
{
"from": "gpt",
"value": "This chair stands out due to its vibrant red color, glossy finish, and a five-point star base, combined with
functional features like adjustability and swivel."
}
]
}
Figure17:Multi-roundconversationsamplesofourT3Ddataset.original mesh original mesh
point cloud input point cloud input
(only for reference) (only for reference)
What is this?
What is the function of the
flight control motherboard?
This 3D object is a meticulously crafted
sculpture of a blue pony with a white mane
The flight control motherboard is designed
and tail, featuring a pair of wings that are a
to interface with the drone's propellers and
mix of blue and white, enhancing its
sensors, allowing for precise control and
magical appeal. The pony's body is
navigation.
Text only adorned with a soft, plush texture, making
it appear inviting and comforting.
Can humans control this
without flight control?
Why do horses have wings?
Yes, the drone includes a manual
In this context, the wings are symbolic, control mechanism, enabling users to
representing the pony's ability to fly, operate it even without the flight
adding an element of fantasy and control system.
adventure to the sculpture.
So does this creature rely on flight What are the uses of drones?
or walking?
While the wings suggest flight, the pony's Drones are typically used for aerial
hooves indicate it is also capable of photography, surveillance, and delivery
walking, making it versatile for various services, thanks to their ability to fly and
environments and activities. carry payloads.
original mesh original mesh
point cloud input point cloud input
(only for reference) (only for reference)
Why can it run so fast?
What is the main color of
the surface?
The car's aerodynamic design, including its
low profile and streamlined body,
contributes to its high-speed capabilities.
The main color of the surface is black.
What brand may this car be？
What kind of shoes, hiking or
Given its design and color scheme, it sports?
could potentially be a Lamborghini or a
Ferrari. The shoes are designed for sports,
specifically for running and agility.
How many people can sit?
Is it the left or right shoe?
This car is designed for two passengers,
offering a comfortable and exclusive
driving experience. It is the right shoe.
Figure18:ConversationexamplesofGreenPLM-0(text-onlydata).original mesh original mesh
point cloud input point cloud input
(only for reference) (only for reference)
What is this? Please provide a detailed
description of this.
This is a 3D model of a football, which is
The 3D object model is a cartoon-style
a spherical object with a pattern of black
representation of a red apple. The apple is
and white pentagons. The football is
vividly red, with a glossy finish that
designed with a classic design, with the
suggests it's fresh and ripe. The apple's
black pentagonsand white pentagons
cartoonish design includes exaggerated
alternating in a pattern around the sphere.
features, such as a large, round shape and
This design is typical of a traditional
a slightly bulbous bottom. This model
football, and it is used in various sports,
could be used in various digital
most notably soccer. The football's surface
applications, such as animations, video
is smooth and it is inflated, ready for a
games, or educational materials for
game.
children.
In which country did football
originate? How tall is an apple tree?
Football originated in England. The height of an apple tree can vary
greatly, but on average, they can grow
anywhere from 10 to 30 feet tall.
How many substitute players can
a team have at most during a game?
How long can picked apples last?
Stage
A team can have up to 3 substitute
3 players during a game. Picked apples can last for several weeks
if stored properly.
original mesh
original mesh point cloud input
point cloud input (only for reference)
(only for reference)
What role does it play in
human development?
What is it? The spacecraft plays a significant role in
human development as it represents the
technological advancements and
It's a 3D model of a cartoon-style elephant.
exploratory spirit of humankind.
Why does it launch vertically
What is the lifespan of an instead of horizontally?
elephant?
The vertical launch is likely to provide
Elephants have a long lifespan, a more efficient and powerful thrust,
typically living up to 70 years. which is crucial for space travel.
What is the difference between a
What is the function of their noses? rocket and a space shuttle?
A rocket is a vehicle that travels through
space by burning fuel and expelling
Elephants use their large noses, or trunks,
exhaust gases, while a space shuttle is a
for various tasks such as smelling,
reusable spacecraft that can carry crew
drinking, and eating.
and cargo to andfrom space.
Figure19:ConversationexamplesofGreenPLM(limited3Ddata).Table9:Detailedtrainingsettings.
Setting StageI StageII StageIII
DatasetSource T3D(Ours) Point-textInstructionDataset(Xuetal.2023)
Dataset
BriefDescription
DetailDescription
DatasetType BriefDescription &DetailDescription
&Conversation
&Conversation
DatasetScale 1M 210K 90K
TrainingTime 12.6Hours 5.9Hours 8.1Hours
TrainableParameters 12.6M 62.9M 63.3M
BatchSize 16 14 25
Epoch 1 1 3
LearningRate 1e-3 2e-4 5e-5
NoiseStd 0.05 0.05 -
Parameters 5B 5B -
HiddenSize 1280 1280 -
TextEncoder
HeadofAttention 20 20 -
NumberofLayer 32 32 -
Parameters - - 22.6M
PointNumber - - 8192
PointGroupSize - - 64
PointCloudEncoder PointPatchNumber - - 512
HiddenSize - - 384
HeadofAttention - - 6
NumberofLayer - - 12
FPSTokenNumber - - 32
0M-Pooling
KNNTokenNumber - - 8
NumberofLayer 2 2 2
ProjectorMLP
1024->3072 1024->3072 1024->3072
Dimension
3072->3072 3072->3072 3072->3072
Parameters 3.8B 3.8B 3.8B
RankofLoRA - 32 32
AlphaofLoRA - 64 64
LargeLanuguageModelBackbone
NumberofLayer 32 32 32
HeadofAttention 32 32 32
HiddenSize 3072 3072 3072Trainable Frozen
Detailed Description Detailed Description
Brief Description &
Brief Description & Conversation & Conversation
LLM LLM LoRA LLM LoRA
MLP MLP MLP
Copy Copy
Train Instruction Weight Instruction Weight Mix pooling Instruction
tokens
Text Text Class 0M-Pooling Point
Embedding Embedding Token Tokens
CLIP CLIP CLIP
Text Encoder Text Encoder Point Cloud Encoder
A black and yellow samurai sword, Text Caption
3D Point Cloud
featuring a long, curved blade …… for 3D Object
Response Response Response
LLM LLM LoRA LLM LoRA
MLP MLP MLP
Instruction Instruction Mix pooling Instruction
Inference tokens
Class Class Class 0M-Pooling Point
Token Token Token Tokens
CLIP CLIP CLIP
Point Cloud Encoder Point Cloud Encoder Point Cloud Encoder
3D Point Cloud 3D Point Cloud 3D Point Cloud
(a) Stage I (b) Stage II (c) Stage III
Figure20:Architecturesoftrainingandinferencinginthreestages.