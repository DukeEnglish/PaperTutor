TrafficGamer: Reliable and Flexible Traffic Simulation for
Safety-Critical Scenarios with Game-Theoretic Oracles
Guanren Qiao1, Guorui Quan2, Jiawei Yu1, Shujun Jia3, Guiliang Liu1∗
1The Chinese University of Hong Kong, Shenzhen
2The University of Manchester
3Shenyang MXNavi Co.,Ltd.
∗Corresponding Author, liuguiliang@cuhk.edu.cn
Abstract
While modern Autonomous Vehicle (AV) systems can develop reliable driving poli-
cies under regular traffic conditions, they frequently struggle with safety-critical traffic
scenarios. This difficulty primarily arises from the rarity of such scenarios in driving
datasets and the complexities associated with predictive modeling among multiple ve-
hicles. To support the testing and refinement of AV policies, simulating safety-critical
traffic events is an essential challenge to be addressed. In this work, we introduce Traf-
ficGamer, which facilitates game-theoretic traffic simulation by viewing common road
driving as a multi-agent game.In evaluating the empirical performance across various
real-world datasets, TrafficGamer ensures both fidelity and exploitability of the simu-
lated scenarios, guaranteeing that they not only statically align with real-world traffic
distribution but also efficiently capture equilibriums for representing safety-critical sce-
narios involving multiple agents. Additionally, the results demonstrate that TrafficGamer
exhibits highly flexible simulation across various contexts. Specifically, we demonstrate
that the generated scenarios can dynamically adapt to equilibriums of varying tight-
ness by configuring risk-sensitive constraints during optimization. To the best of our
knowledge, TrafficGamer is the first simulator capable of generating diverse traffic sce-
narios involving multiple agents. We have provided a demo webpage for the project at:
https://qiaoguanren.github.io/trafficgamer-demo/.
Introduction
As a cutting-edge technology, autonomous driving has been a critical component of future
transportation systems. The development of Autonomous Vehicles (AV) requires extensive
testing and calibration of their control systems. Given the significant risks of conducting
these tasks on real roads, the industry commonly relies on traffic simulation systems to ensure
the safe development of AVs. Within these simulation systems, the commonly studied goals
include traffic flow simulation (e.g., SUMO [1] and CityFlow[2]), sensory data simulation (e.g.,
1
4202
guA
82
]IA.sc[
1v83551.8042:viXraAirsim [3] and Unisim [4]), driving policy simulation (e.g., Highway-Env [5], SUMMIT [6],
Metadrive [7], Commonroad [8], Intersim [9], Tbsim [10], Waymax [11], TorchDriveEnv [12]
and CarDreamer [13]), vehicle dynamics simulation (e.g., Carsim [14] and Matlab [15]) and
multi-task simulation (e.g., Carla [16], NVIDIA’s Drive Sim [17] and VI Worldsim [18]).
The major advancements in traffic simulation have primarily focused on improving fidelity
by replicating observed vehicle trajectories. While these methods can guarantee the expected
accuracy of reproducing real-world traffic flows, they often fail to capture rare events that
occur at the long-tail end of the data distribution. Conversely, a significant challenge for
modern AVs is their difficulty in managing these safety-critical ”tail” events, rather than the
more commonly observed traffic scenarios. This situation reveals the importance of actively
simulating safety-critical but infrequent events, which are crucial for thoroughly testing the
reliability and robustness of AV control systems.
Beyond replicating realistic traffic, some recent studies have specifically focused on modeling
safety-critical events. These event generating strategies can be categorized into [19]: 1)
Data-driven generation detects and reproduces safety-critical scenarios recorded in real-world
datasets. Some recent advancements, including Simnet [20], TrafficGen [21], Trafficsim [22],
VBD [23], NeuralNDE [24], Goal-LBP [25] and SceneGen [26], model the distribution of traffic
scenariosbymaximizingthelikelihoodofobservedvehicletrajectories. 2)Adversarial generation
intentionally creates risky scenarios by manipulating the generation process of autonomous
vehicle (AV) systems [27]. Under this setting, Advsim [28] and STRIVE [29] manipulated the
scenario’s initial conditions or provided the complete trajectory upfront. The Reinforcement
Learning (RL)-based methods constructed an adversarial policy network [30, 31, 32] to control
autonomous vehicles 3) Knowledge-based Generation leverages external domain knowledge
to facilitate the generation of safety-critical events. To achieve this goal, RobustTraj [33],
ChatScene[34], CGT[35], RTR[36]andGCRL[37]incorporatedlatentembeddingsorconstraint
signals of traffic rules into the trajectory prediction model.
The aforementioned approaches primarily depend on imitating human demonstrations and
manually-crafted heuristics. However, as shown in Figure 1a, safety-critical traffic scenarios
involve complex systems with multiple agents. These high-risk events are sparsely represented in
real-worlddemonstrationdata,leadingtohigherpredictiveerrorsandcrashratesintheimitation
model. Such discrepancies impact the effectiveness of data imitation and human experience.
More importantly, the agents’ behaviors in these systems are intricately interconnected and
interdependent. The reactions of one agent are largely dependent on the movements of others.
Moreover, these reactions are shaped by the agents’ specific objectives and shared safety
concerns. Figure 1b shows that referring only to log-reply is insufficient. T here is a lack of a
mechanism that can effectively model the strategic behaviors of AV agents and actively discover
safety-critical events conditioning on different environments.
To this end, a critical step toward strategic traffic simulation is to develop a game-theoretic
traffic simulation algorithm designed to model the complex interactions among multiple agents
in traffic scenarios. Moreover, such a system must be reactive, enabling vehicles to adapt to
the movements of other traffic participants. From the perspective of game theory, whenever we
modify the policy of a vehicle, the multi-agent system can converge to a new equilibrium where
no agent has an incentive to deviate. These characteristics distinguish our proposal from other
simulators primarily relying on imitation learning.
2To ensure the practical application of these simulated strategic behaviors, it is essential to
resolve the following challenges shown in Figure 1c: 1) Distributional Fidelity: The generated
vehicle trajectories should closely replicate the behaviors of actual drivers to provide realistic
driving scenarios. Unlike imitation-based methods that focus on point-wise regression, the
simulated scenarios should maintain a minimal distribution distance from the realistic dataset
to ensure their alignment in naturalistic driving behaviors. 2) Efficient Exploitability: The
algorithm simulates the competitive interactions among multiple agents. This algorithm should
efficiently identify and converge to an equilibrium, where no agent has an incentive to deviate.
Furthermore, this process must be scalable to accommodate a large number and diverse types
of vehicles. 3) Flexible Simulation: While an equilibrium typically assumes that agents aim to
maximize their expected returns, human drivers’ behaviors are inherently diverse and sensitive
to various safety risks (i.e., bounded rationality). Maintaining an overly tight equilibrium
may result in an inaccurate representation of drivers’ behaviors and limit the opportunities
for controllers to develop safe policies. Therefore, it is crucial to control the tightness of
the equilibrium. This advancement can ensure safety and flexibility for policy improvement,
accommodating the nuanced and variable nature of human driving behaviors.
In response to the outlined challenges, we propose TrafficGamer, a game-theoretic algorithm
designed to facilitate distributional fidelity, efficient exploitability, and flexible simulation in
multi-agent traffic simulations within complex environments. Figure 2a shows the overall
framework of TrafficGamer. Specifically, TrafficGamer pre-trains a generative world model
using large-scale traffic data collected from a variety of traffic scenarios. The pre-training
is implemented through an end-to-end motion prediction task, which forecasts the future
position of a vehicle based on historical observations of dynamic features (e.g., surrounding
vehicles) and state features (e.g., map attributes). This approach ensures that the traffic
model accurately reflects a high fidelity to drivers’ preferences under realistic conditions.
Subsequently, TrafficGamer fine-tunes this model by following a game-theoretic oracle. This
refinement process involves minimizing the distributional distance to the pre-trained model,
thereby maintaining the fidelity of the realistic data distribution while capturing its competitive
behaviors among different AV agents. The AVs’ policies derived from our model can be
mathematically characterized by a Coarse Correlated Equilibrium (CCE). As is shown in
Figure 2b, we develop a multi-agent CCE-Solver algorithm to efficiently approximate this
equilibrium. The theoretical foundation for the optimality of our algorithm is grounded by
recent advancements in multi-agent CCE [38, 39, 40]. Additionally, the empirical performance
of the algorithm draws upon principles from Magnetic Mirror Descent (MMD) [41, 42, 43, 44].
This combination of theoretical and empirical insights ensures that our approach is robust and
effective in complex traffic simulation scenarios. To dynamically adapt the tightness of the
equilibrium, TrafficGamer incorporates a configurable safety constraint into CCE policies (See
Figure 2c). For example, by maintaining a larger distance between cars, we derive a softer CCE
by implementing risk-preventing constraints. In this setting, agents are inclined to cooperate
to meet joint safety constraints, such as maintaining appropriate distances between vehicles.
This strategy ensures both safety and flexibility in traffic simulation.
As a scalable algorithm, TrafficGamer can be adapted to various datasets. To demonstrate
the scalability of TrafficGamer, we evaluated its empirical performance using well-known
traffic datasets such as Argoverse 2 [45] and Waymo Open Motion Dataset [46], as well as a
3commonly used simulator. In specific, we mainly conduct three sections of experiments and
illustrate the results under a variety of driving scenarios (twelve in total): 1) Fidelity Validation
for the Generated Motion Trajectories: We demonstrate that the trajectories generated by
TrafficGamer can accurately characterize real-world traffic scenarios. Specifically, by quantifying
the distributional divergence between generated traffic features (including distance between
vehicles, speed, and collision rate) and those observed in the dataset, we show that the
scenarios from TrafficGamer can statistically match the data distribution in the real world.
2) Efficient Exploitability under a Variety of Scenarios: We illustrate that TrafficGamer can
most effectively close the CCE-Gap (Coordinated Compliance and Enforcement Gap), thereby
efficiently capturing the equilibrium necessary for representing safety-critical scenarios under a
variety of traffic conditions. 3) Simulation of Diverse Safety-Critical Scenarios: To demonstrate
the flexibility of our traffic simulation, TrafficGamer supports both 2D and 3D visualizations of
each driving scenario from multiple perspectives. These visualizations show that TrafficGamer
can generate a variety of intriguing and infrequent safety-critical traffic scenarios. Additionally,
it can effectively configure the degree of competition and collaboration among vehicles within
these scenarios. Illustrative videos are provided in the supplementary materials.
Results
Dataset
While these scenarios are crucial for the development and evaluation of autonomous vehicle
(AV) systems, real-world datasets for safety-critical scenarios are rarely available. Therefore,
we assess the performance of our model using the publicly available dataset Argoverse 2[45],
which contains 250,000 scenarios extracted from six distinct urban driving environments across
the United States. Argoverse 2 provides diverse and intriguing vehicle trajectories and
environments for constructing autonomous driving systems. It records 10 various object types
between dynamic and static categories including cars, buses, pedestrians, bicycles, etc. Each
scenario is accompanied by a local vector map and 11 seconds (at a rate of 10 Hz) of trajectory
data, detailing the 2D position, velocity, and orientation for all observed tracks relative to the
ego vehicle’s perspective within the local environment. More details about the dataset can be
found in Supplementary Section 1a.
Additionally, to demonstrate the robustness and generalization of our approach, we further
evaluate our method using the Waymo Open Motion Dataset v1.2.0 [46]. This dataset
features over 570 hours of unique data, covering 1,750 kilometers of roadways and over 100,000
scenes, each lasting 20 seconds and recorded at 10 Hz. It includes detailed interactions between
vehicles, pedestrians, and cyclists across six cities in the United States. More detailed results
about this dataset are available in the Supplementary Section 1a.
Experiment Settings
Based on the trajectory prediction task setting, our generative world model predicts the agent’s
future states by observing the agent’s historical information, map features, and the surroundings
of neighboring agents. By iteratively performing this prediction process, TrafficGamer learns a
4CCE solver to generate traffic congestion scenarios with varying degrees of competition. To
verify the generative performance of our algorithm, we select six representative scenarios in
the validation dataset of Argoverse 2 and Waymo respectively, including 1) Merge where two
separate lanes of traffic join into a single lane, 2) Dual-lane intersection where five cars with
different destinations are driving through a two-way intersection, 3) T-Junction where one
road ends at a perpendicular junction with another road, forming a ”T” shape, 4) Dense-lane
intersection where cars enter a four-way intersection with dense traffic. 5). Roundabout where
traffic flows counterclockwise around a central circle, and 6) Y-Junction where three directions
of traffic flow converge at a single intersection. Figure 3 showcases the road map of our
experiments in the last time step of historical trajectories. In the training procedure, we
utilize all training datasets to learn the generative world model. During validation, we leverage
validation datasets to evaluate our world model’s performance. For the Fine-tuning stage,
TrafficGamer controls 5-7 agents within the 6 scenarios selected from the validation datasets to
obtain various safety-critical events by modeling different traffic congestion levels. More details
about experiment settings can be found in Supplementary Section 1b.
Evaluation Metrics
We leverage a comprehensive set of statistical metrics to evaluate the fidelity and effectiveness
of the proposed TrafficGamer. The metrics include:
Fidelity. The fidelity metrics measure how well the simulated traffic distribution (i.e., the
distribution of vehicles’ temporal and spatial features) matches the observed data distribution.
We implement these fidelity metrics by f-Divergence D (P∥Q) which measures the divergence
f
between two probability distributions P and Q over the space X, so that:
(cid:90) (cid:18) (cid:19)
dP
D (P∥Q) ≡ f dQ (1)
fα
dQ
X
where f : [0,+∞) → (−∞,+∞] denotes a convex function where f(t) is finite for all t > 0,
α
f(1) = 0, and f(0) = lim f(t). In this work, we study the following implementation of D :
t→0+ fα
√
• By setting f(t) = 1( t−1)2, f-Divergence becomes Hellinger distance D such that:
2 H
(cid:90)
1 (cid:16)(cid:112) (cid:112) (cid:17)2
D (P∥Q) = P(dx)− Q(dx) (2)
H
2
X
• By implementing f(t) = tlnt, f-Divergence can be transformed into Kullback-Leibler
divergence (KL). We can calculate KL-divergence D as:
KL
(cid:90) (cid:18) (cid:19)
P(dx)
D (P∥Q) = P(dx)log (3)
KL
Q(dx)
X
Additionally, since f-Divergence remains undefined when the support of compared distribu-
tions are non-overlapping, our experiment includes Wasserstein Distance of two probability
5distributions to avoid the appearance of inaccurate results. For a finite moment p ∈ [1,+∞],
the Wasserstein p-distance between two probability distributions P and Q is defined by:
W (P,Q) = inf
(cid:0)
E
d(µ,ν)p(cid:1)1/p
, (4)
p (µ,ν)∼γ
γ∈Γ(P,Q)
where Γ(P,Q) is the set of couplings for distribution P and Q. Aiming at computational
tractability, in our experiments, we set p = 1.
We demonstrate the fidelity of our method in modeling safety-critical events. A crucial
metric that reflects the safety of simulated traffic is the crash rate. Supplementary Section 3c
outlines the relevant details.
Exploitability. An exploitability metric reflects the optimality of the joint policies under a
General Sum Markov Game (GS-MG)[47]. Traffic simulation reflects the optimality of simulated
vehicles in quickly reaching their destinations under traffic rules and other constraints. This
study characterizes this optimality by the Coarse Correlated Equilibrium (CCE):
Definition 1. (ϵ-approximate CCE). A General Correlated policy π [48] is an ϵ-approximate
Coarse Correlated Equilibrium (ϵ-CCE) if
(cid:16) (cid:17)
max V†,π−i(s)−Vπ(s) ≤ ϵ (5)
0,i 0,i
i∈[I]
where V†,π−i(s) = sup Vπ i′,π−i(s) denotes the best response for the ith agent against π . We
0,i π′ 0,i −i
i
say π is an (exact) CCE if the above is satisfied with ϵ = 0.
Under this definition, a CCE-gap(i) = V†,π−i(s)−Vπ(s) quantifies the deviation between
0,i 0,i
the learned policies of each agent and the performance of the best equilibrium policy. In our
study, Vπ(s) = Vπ,r(s)+λVπ,c(s), where r is denoted as the reward and c is the cost. Unlike
0,i 0,i 0,i
Nash Equilibrium (NE) [49] which enforces the independence of each agent during optimization,
a CCE permits interdependencies among agents’ policies, allowing each agent’s strategy to be
informed by the strategies of others. We study the CCE-gap since it better aligns with the
decision-making processes of human drivers, who typically base their actions on the behavior of
nearby vehicles. Additionally, it simplifies the challenge of convergence, as CCEs are inherently
less restrictive and more prevalent than NEs.
We compare the proposed method with QCNet [50], MAPPO (Multi-Agent Proximal Policy
Optimization)[51], and GameFormer[52]. QCNet jointly predicts the trajectory of multiple
agents under a supervised learning framework. MAPPO is a policy gradient algorithm designed
for Multi-agent reinforcement learning (MARL). GameFormer proposes a game-theoretic model
and learning framework for interactive prediction and planning using Transformers. More
details for the baselines can be found in Supplementary Section 1c.
Fidelity Validation of Generated Motion Trajectories
A crucial prerequisite for simulated safety-critical traffic is that it aligns with realistic traffic
scenarios. We characterize this alignment with distributional fidelity (Section Evaluation
Metrics), which quantifies the divergence between realistic traffic distributions and those
6simulated by our TrafficGamer. Among the spatial and temporal traffic features, vehicle
speed, and inter-vehicle distance are the most commonly studied features for examining the
performance of AV simulators [24]. We adapt f-divergence to qualify how well the simulated
distribution of vehicle speed and car distance match the realistic ones from the large-scale
traffic datasets (Argoverse 2 and Waymo).
In this experiment, we assess fidelity by quantifying distributional divergence using several
divergence metrics, including Kullback-Leibler divergence, Hellinger distance, and Wasserstein
distance [53]. Figure 4 shows the experiment results, and we find the simulated scenarios
generated by our TrafficGamer can properly imitate the real-world distribution of instantaneous
vehicle speeds and vehicle distances. In specific, for methods based on multi-agent fine-tuning
(TrafficGamer and MAPPO), we observe that the divergence between realistic traffic data
and the simulated traffic features generated by TrafficGamer is smaller compared to those
produced by MAPPO. This is because the MMD objective in our TrafficGamer (Formulate 12)
explicitly minimizes the divergence to the observed policy during optimization. The reduction
in divergence results in a more accurate replication of vehicle speed and distance distributions,
indicating that the learned multi-agent policies are more closely aligned with actual human
driving behaviors. To have more evidence, the performance of TrafficGamer is comparable to
the imitation learning-based methods (QCNet and GameFormer) in terms of ensuring fidelity
to realistic driving behaviors.
To better evaluate the effectiveness of our method in replicating the driving styles of human
drivers across various scenarios, we categorize the scenarios into specific types, including fork
road and cross road. Figure 4 (rows from 3 to 6) compares the learned and actual distributions
of distance and speed. In the crossroad scenario (rows 5 and 6 in Figure 4), TrafficGamer
achieves performance comparable to, albeit slightly less robust than QCNet in maintaining
fidelity. This result is anticipated since QCNet is designed to closely mimic the observed
behaviors of drivers, resulting in trajectories that heavily overlap with those in the source
dataset. Another key observation is that our method, TrafficGamer, significantly outperforms
standard MARL approaches such as MAPPO. This demonstrates TrafficGamer’s capability to
accurately replicate a wide range of human driving behaviors and generate realistic driving
scenarios. We also check the fidelity in the Waymo motion dataset, and the results closely
match the ground-truth distribution. More details can be found in supplementary Section 3d.
Efficient Exploitability under a Variety of Scenarios
Unlike standard RL algorithms that focus on reward maximization, in the multi-agent au-
tonomous driving environment, we primarily consider exploitability [43], which measures the
extent to which a vehicle’s policy can exploit the current traffic policies. An ideal driving equi-
librium should have zero exploitability, meaning no single vehicle can achieve greater benefits by
continually improving the policy. In this work, we follow the methodology outlined in [47] and
utilize the CCE-gap (see definition 1) to measure exploitability. Unlike the common-studied
toy environment (e.g., Bargaining, TradeComm, and Battleship [44]), the traffic scenarios
involve complex game contexts, multiple agents, and continuous action space, which makes the
best response π† computationally intractable, and accurately calculating the exact CCE-gap
i
becomes challenging. Therefore, we estimate the CCE-gap by empirical approximating π† with
i
7the following methods:
• Breaking the Equilibrium. Upon the studied algorithm converges, we estimate each agent’s
best response under the current equilibrium, denoted as π†, by fixing the policies of the other
i
I −1 agents and continuously encouraging this agent to maximize its current reward. This
process assesses the agent’s ability to disrupt the existing equilibrium. If none of the policies
can achieve significantly higher rewards, it indicates that the experimental algorithm has
successfully identified a reliable CCE.
• Restricting Action Space. To overcome the computational intractability caused by complex
driving behaviors, we constrain the decision-making domain to choose actions from a pre-
defined candidate action set. This set is generated by a pre-trained action predictor (see
objective 9), which identifies and ranks the top K actions most similar to those observed in
the dataset. Within this tractable action space, we compute π† by selecting the actions that
i
optimally maximize rewards at each step.
WecomparetheperformanceofTrafficGamerinallArgoverse2scenarioswithotherbaselines
based on the aforementioned CCE-Gap. Figure 5 and Figure 6 present the CCE-gap of each
agent during training. The results show that the training curves of QCNet and GameFormer
perform unstable and struggle to converge to the CCE. It illustrates that end-to-end imitation
learning methods can not model competitiveness between agents, which makes it difficult to
optimize policies for capturing CCE. MAPPO performs better than other baselines, but it
still falls short of the results obtained by TrafficGamer. MAPPO faces challenges in efficiently
exploring the entire policy space of the Multi-agent game environment during the optimization
process, making it difficult to capture the underlying CCE. As a solver defined for CCE,
TrafficGamer ensures that the agents’ policies are distributionally aligned with human-driven
policies and supports stable exploration. This allows each agent to learn the optimal policy
and gradually converge to an approximate CCE.
Table 1 and Table 2 present the CCE-gap of each agent at the end of training. TrafficGamer
achieves a smaller CCE-gap compared to other methods, demonstrating its superior exploitabil-
ity across various scenarios. While TrafficGamer achieves a smaller CCE-gap compared to other
methods, it exhibits slightly reduced performance for certain agents in some scenarios. An
in-depth analysis reveals two main reasons for this phenomenon: 1) TrafficGamer may struggle
to effectively adapt to rapid changes in opponents’ strategies under specific conditions. For
example, when surrounding vehicles accelerate, a particular vehicle may need to decelerate to
maintain safe spacing. 2) The presence of other vehicles significantly influences the decision-
making processes of algorithm-controlled vehicles. For example, if a car on the other hand
begins to merge into the current lane, the algorithm might adopt a more conservative driving
strategy to prioritize safety, thereby influencing exploiting. Overall, in a multi-agent game
environment, it is essential to evaluate the collective performance of all agents. TrafficGamer
is capable of learning a CCE that approximates the optimal solution, resulting in a more
comprehensive modeling of vehicle driving behaviors.
8Simulation of Diverse Safety-Critical Scenarios
To demonstrate the ability of TrafficGamer to generate diverse traffic scenarios, we visualize
the generated scenarios featuring varying degrees of competitive behavior. However, automated
driving scenarios involve complex elements including road structures, traffic regulations, and
vehicle behaviors, which can be challenging to interpret directly from raw data. To address
this, we simulate the actual behavior of vehicles in various safety-critical scenarios and traffic
conditions using 2D and 3D visualizations. These are captured in third-person and first-person
views, to provide a clearer understanding of the dynamics at play. To facilitate 2D visualization,
TrafficGamer supports the display of lines, markers, the road network, vehicle positions, and
movement trajectories on a unified 2D plane by following the formats established by Argoverse
2 [45] and Waymo [46]. This approach uses concise symbols to represent various traffic elements,
enhancing the intuitiveness of scene analysis. To capture the real-world complexity of traffic
behaviors more accurately, TrafficGamer enables large-scale 3D traffic scenario modeling and
simulation through ScenarioNet [54], based on the MetaDrive simulator [7]. The scenarios
generated can be replayed and explored interactively, ranging from Bird’s Eye View layouts to
realistic 3D renderings in ScenarioNet.
In specific, this experiment explores how well TrafficGamer can facilitate the generation of
different safety-critical scenarios under twelve distinct traffic scenarios. To generate diverse
traffic scenarios, we design the constrained and risk-sensitive policy optimization [55, 56] for
capturing the equilibrium subject to different levels of tightness. Specifically, by adjusting the
inter-vehicle distance constraints and risk coefficients, we derive a diverse number of traffic
scenarios under different game contexts. Figure 8 illustrates the details of our results.
By comparing the scenarios generated with different levels of inter-vehicle distance con-
straints (from top to bottom in Figure 8 and Figure 9) , we find as the inter-vehicle distance
increases, our model adapts, leading to traffic scenarios characterized by less competitive behav-
ior and safer navigation across all examined situations. This outcome arises because maintaining
distance constraints requires the cooperation of multiple vehicles. Imposing more restrictive
constraints (i.e., increasing the distance) significantly enhances the impact of this cooperation
on the optimization objective 16 (dynamically controlled by the Lagrange parameter λ). Figure
7 displays the variation of the Lagrangian penalty factor during the training process. As the
constraints become tighter, the Lagrangian penalty term also increases, indicating that it has
a larger impact on the objective 16. On the other hand, if we reduce the required distance,
agents begin to prioritize their interests, which significantly increases the likelihood of traffic
congestion where no agent can further optimize their policy. All these dynamic scenarios are
characterized by the learned CCEs.
Similarly, by comparing the scenarios generated with different levels of risk sensitivity
(from left to right in Figure 8), we find imposing a higher confidence level leads to more
conservative and cautious driving behaviors. A higher confidence level forces the agent to
satisfy the constraint with greater probability, resulting in driving strategies that feature lower
speeds, increased spacing between vehicles, and more careful navigation through complex traffic
scenarios. On the other hand, setting a lower confidence level results in more aggressive and
risk-seeking driving behaviors, characterized by faster vehicle speeds and shorter following
distances. The system’s tolerance for some aggressive behaviors (such as overtaking etc.) has
9increased. This approach can lead to a higher risk of collisions and increase the likelihood of
critical safety scenarios in traffic.
These diverse scenarios are crucial for investigating the trade-offs between aggressive
and conservative driving behaviors. By analyzing AV policies within these scenarios, we
can more effectively evaluate the optimality of driving strategies across different levels of
competition and congestion. Furthermore, the comprehensive 2D and 3D simulations provided
by our TrafficGamer from both third-person and first-person perspectives offer a detailed
understanding of traffic dynamics. These simulations demonstrate how variations in risk
sensitivity and distance constraints impact vehicle behavior in real-world driving scenarios.
Discussion
We demonstrate that our model, TrafficGamer, can effectively represent various safe-critical
traffic scenarios by capturing a range of CCEs. To the best of our knowledge, this is the
first algorithm that fine-tunes generative world models to accurately model competitive and
collaborative behaviors among multiple agents across varying degrees of constraints and
dynamic environments. Most importantly, TrafficGamer can accurately characterize traffic
congestion scenarios frequently observed in reality but are underrepresented in datasets, such
as roundabouts, intersections, and merge points. This capability ensures the high fidelity of
TrafficGamer in real-world applications.
To facilitate the reliable generation of our safety-critical scenarios, we resolve three critical
challenges, including 1) how to guarantee the fidelity of generated trajectories, 2) how to
efficiently capture the CCE of each scenario by modeling the competition behaviors of vehicles,
and 3) how to dynamically adapt the strength of the equilibrium. These safety-critical scenarios
can serve as important testbeds for AV controlling algorithms evaluating the reliability and
robustness of AV control algorithms before their practical deployment. Traditional simulation
systems have relied heavily on manually designed rules or data-driven trajectory imitation,
often resulting in scenarios that lack fidelity and diversity. Our model, TrafficGamer, addresses
these limitations by generating a variety of realistic, safety-critical scenarios. It is important to
note that our method currently models vehicle behaviors on a static map. Future enhancements
for TrafficGamer may include integrating multi-modal large models to generate scenarios that
account for varying vehicle behaviors in response to environmental factors, such as weather
conditions, time of day, and topography.
Methods
Problem Formulation
We formulate the task of multi-vehicle motion prediction in the traffic scenarios as a Decentral-
ized Partially Observable Constrained Markov Decision Process (Dec-POCMDP).
(S,{Ω ,A ,O ,r ,c }I ,T ,γ,p ) where:
i i i i i i=1 0
• i denotes the number of agents from 1 to I.
10• Ω andA denotethespacesofobservationsandactionsforaspecificagenti. Theobservations
i i
include the position, velocity, heading, and partial map features in a neighbor region of the
agent i. The actions consist of relative heading and acceleration, as described in [11].
• S denotes the state space that comprises all agents’ historical trajectory information and
map features.
• O : S → Ω denotestheobservationfunctionthatmapsstatesandactionstolocalobservation
i i
for the ith agent. O = {O ,...,O } denotes the function set.
1 I
• r : {Ω × A }I → R denotes the agent-specific reward function that maps actions and
i i i i=1
observations from all agents to the reward of ith agent. We consider reward factors such as
collision avoidance, lane deviation, and reaching the destination.
• c : {Ω × A }I → R denotes the agent-specific cost function that maps actions and
i i i i=1
observations from all agents to the cost of ith agent. We adapt vehicle distance as a constraint
condition, additionally, the expectation of cumulative constraint functions c must not exceed
i
associated thresholds episodic constraint threshold δ.
• T : S ×A → ∆S 1denotes the transition function.
• γ ∈ [0,1] and p ∈ ∆S denote the discount factor and the initial distribution.
0
More details of reward and cost functions can be found in Supplementary Section 2a.
In the framework of Dec-POCMDP, each agent is assigned an individual reward function
and cost function, denoted as r (·) and c (·). This aligns a general-sum game structure [57],
i i
more complex and less explored than zero-sum and cooperative games [58, 59]. Figure 10
displays the differences between these three settings. Despite challenges, modeling a general-
sum game better aligns with real-world driving scenarios since human drivers often prioritize
their objects. These objects can be applied to satisfy conflicting interests among different AV
agents. During the optimization process, human drivers’ behaviors inevitably affect others’
decisions. Accordingly, we consider the General Sum Markov Games (GS-MGs) under
the Dec-POCMDP. For the agent i, the value function Vπ,r : S → R , action-value function
i,t
Qπ,r : S ×A → R and advantage function Aπ,r : S ×A → R are represented by:
i,t i i,t i
(cid:34) (cid:35)
T
(cid:88)
Vπ,r(s) = E γtr (o ,a )|o = O(s ) (6)
i,0 p0,T,π i t t 0 t+1
t=0
(cid:34) (cid:35)
T
(cid:88)
Qπ,r(s,a ,−a ) = E γtr (o ,a )|o = O(s ),a = a (7)
i,0 i i p0,T,π i t t 0 t+1 i,0 i
t=0
Aπ,r(s,a ,−a ) = Qπ,r(s,a ,−a )−Vπ,r(s) (8)
i,0 i i i,0 i i i,0
where −a = {1 a′}I 2 denotes the joint action performed by I −1 players (without i’th
i i′̸=i i i′=1
player) and π = {π }I denotes the product policy.
i i=1
Under a GS-MG, the goal of policy optimization is capturing a Coarse Correlated
Equilibrium (CCE) (Definition 1), which allows agents’ policies to be interdependent,
1∆S denotes the probability simplex over the space S.
2Throughout this work, the bold symbols (e.g., a) indicate a vector of variables while the unbold ones (e.g.,
a) represent a single variable.
11contrasting with NE where each agent optimizes independently. Unlike previous GS-MG policies
[38, 40] that use the Markov policy π (a|s), our method incorporates historical information,
i
including the actions and observations of neighboring agents, to better characterize the decision-
making process of AVs.
Unlike previous GS-MG solvers [60, 39, 40, 61, 57] that rely on an interactive environment,
traffic simulation presents additional challenges since our algorithm can only utilize an offline
database that records the behaviors of multiple drivers on open roads. Additionally, we
incorporate constraints into the optimization process to learn the agents’ behavior under traffic
rules. This problem can be formulated as Offline Multi-agent Constrained Reinforcement
Learning (Offline MA-CRL). Using the offline database that records the behaviors of
multiple drivers on open roads, our algorithm aims to learn the CCE policies of multiple AVs
under various constraints that accurately reflect the human drivers’ behaviors in the dataset.
Specifically, the problem can be summarized as follows:
Definition 2. (Offline MA-CRL in GS-MGs.) let D = {M ,τ ,...,τ }N defines the
o n n,1 n,I n=1
offline dataset, where n = [N] defines the number of scenario, M presents the game context
n
in the nth scenario, c = {c ,c ,...,c }I presents constraints and τ = {o ,a ,...,o ,a }
0 1 i i=1 n,i i,0 i,0 i,T i,T
denotes the trajectory of ith agent in the nth scenario. Given D , the goal of our algorithm is
o
to learn a πˆ that satisfies the constraints with the following properties: 1) Exploitability: πˆ
c c
satisfies the ϵ-approximate CCE in definition 1, and 2) Fidelity: πˆ must be consistent with the
c
real driver’s policies such that D (πˆ ,πo) ≤ ξ where D and ξ denote the divergence metric
f c f
and a threshold.
In this work, to solve the offline MA-CRL problem in GS-MGs under the Dec-POCMDP
(definition 2), we consider a model-based MA-CRL approach that 1) trains a generative
world model to acquire AV environment features in a data-driven way, 2) converges to
CCEs based on the predicted environment dynamics and predefined action space and 3) adjusts
the level of competition for capturing diverse degrees of CCEs.
Modelling Traffic Dynamics based on Offline Data
To represent the environmental dynamics, we introduce the generative world model, action
predictor, and observation model based on the offline datasets D .
l
Generative World Model. For computation efficiency, we follow the decoder-only archi-
tecture in GPT [62] and implement the world model as a Auto-regressive Trajectory Decoder.
As a sequential prediction model, our world model maps the previous state (e.g., s ) and
t−1
the action of each agent (e.g., a ,...,a ) into the next state s . Unlike QCNet [50], which
1,t i,t t
generates trajectories for future vehicle motions over a fixed period, our world model adopts an
auto-regressive approach, predicting vehicles’ step-wise motion based on its previous predictions.
This approach enables modeling how past movements influence future decisions.
Under the context of Dec-POCMDP, when t = 0, s captures the static game map M and
0
initial features of all agents. when t > 0,s = {zA }t,I captures the spatial-temporal
t i,w w=t−W,i=0
information of all agents under the game map M in the previous t − 1 time steps, and
a ,...,a denotes the acceleration and heading of agents in the current time step. Under this
1,t i,t
12setting, our decoder is implemented by 1) Agent-to-Map Cross Attention, 2) Agent-to-Temporal
Cross Attention, 3) Agent-to-Neighbor Cross Attention and 4) Self-Agent Attention incorporates
map information , historical information , the spatial-temporal features of the surrounding
agents , and the features of the agent itself in the temporal dimension thereby mapping s and
t
a to s . The details of implementation are illustrated in Figure 11. In addition, our
1,...,I,t t+1
world model also includes two other important modules:
Action Predictor. The actor model predicts the actions (acceleration and heading) based
on the state s = {zs }t,I to satisfy latent traffic rules in the realistic driving scenarios.
t i,w w=t−W,i=0
The actor model π (a |o ) denotes the probability of the i’s agent generates an action a
i,t i,t i,t t
such that max ωkp(a |µk ,bk ) = π (a |o ) where 1) ωk denotes a learnable coefficient and 2)
i t i,t i,t i,t i,t i,t i
k
p denotes the k-th mixture component’s Laplace density. We constrain the output actions
k
within a reasonable range to prevent irrational driving behaviors such as sudden acceleration
or deceleration, sharp turns, etc. In the subsequent RL fine-tuning stage, we sample the i-th
agent’s future trajectory as a weighted mixture of Laplace distributions by following [50, 63]:
T T K
(cid:89) (cid:89)(cid:88) (cid:0) (cid:1)
πl(τˆ) = πl(aˆ | o ) = ωkp aˆ | µk ,bk (9)
i i,t i,t i i,t i,t i,t
t=1 t=1 k=1
where ωk can effectively act as the weighting coefficients and µk and bk characterize the mean
i i,t i,t
position and the level of uncertainty of the i-th agent at the time step t.
Observation Model For each agent, our observation model maps s and a into agent-
t i,t
specific observations {o }I. The observation model is implemented by o = fMLP(zA), where
i,t i i,t i i,t
MLP is the Multilayer Perceptron. Besides, this module considers the historical actions and
observation of all agents such that h = {(o ,a )}w,I .
i,ι i,ι ι=0,i=1
Recognizing CCEs in the General Sum Markov Games
As our environment is structured as a multi-player competitive game with rewards and costs
specific to each agent, inspired by [40, 43], we consider a decentralized update of each agent’s
policy where we fix the rest I−1 agents’ policy π and train policy π to get the best response
−i i
of agent i. In this work, we update π by iteratively optimizing the following objective:
i
1
πj = argmaxE [V¯πi,π−i,j−1,r(s)]−η B (π ,πl)− B (π ,πj−1) (10)
i πi,µ0 i,t 1 ψ i i η ψ i i
2
where πl denotes the imitation policy learned by the world model (Equation 9), and the πj−1
i i
denotes the policy learned from the previous iteration. This objective contains several key
components that can efficiently facilitate convergence to a CCE by utilizing:
Optimistic V-learning. Inspired by [57], our optimistic value function is defined by:
(cid:34) (cid:35)
T
(cid:88)
V¯πi,π−i(s) = E γι[r (o ,a )+β (o )]|o = O(s ) (11)
i,t µ0,T,πi,π−i i ι ι i ι 0 ι+1
ι=t
13where β (o) = c serves as an exploration bonus to less visited state. c is a hyper-parameter
i ρi(o)
and ρ (o) denotes the density of visited observation o [64], representing the probability of o
i
occurrences at time t [65]. Such an optimistic V-learning objective served as an extension of the
CCE-V-Learning algorithm [40, 38], which had been proven to converge to CCE under discrete
environments, and we extend this algorithm to solve continuous decision-making problems.
Magnetic Mirror Descent (MMD). We follow [43] and incorporate Bregman divergence
B (·,·) with respect to the mirror map ψ such that B (x,y) = ψ(x)−ψ(y)−⟨∇ψ(y),x−y⟩
ψ ψ
into the objective with convergence guarantees. Recent studies [44, 43, 41, 42] justified that
the mirror decent approaches can solve different kinds of games in multi-player settings. To
derive a more intuitive objective, we implement the mirror map as the negative entropy such
(cid:80)
that ψ(x) = p(x)logp(x), and the objective (12) becomes:
1
πj = argmaxE [V¯πi,π−i,j−1,r(s)]−η D (π ∥πl)− D (π ∥πj−1) (12)
i,t πi,µ0 i,t 1 kl i i η kl i i
2
where D is the KL-divergence of two variables. Intuitively, by punishing the distance between
kl
current policy π and imitation policy πm, this objective ensures the fidelity in the offline
i i
MARL problem (Definition 2). By constraining the scale of updates between current policy π
i
and previous policy πj−1, the training process becomes more stable. By default our objective
i
considers the trajectory-generating τ probability:
i
T−1
π (τ ) = µ (s
)(cid:89)
[T (s |s ,a )π (a |o )π (a |o
)]γt
(13)
i i 0 0 t+1 t t i,t i,t i,t −i,t −i,t −i,t
t=0
However, both the transition function T and policy of other players π are not subject to
−i,t
optimize in the objective (12), and thus recent studies [66, 67] often consider the discounted
causal entropy [68] (cid:80)T γtH[π(a |o )]. Similarly, instead of utilizing the computationally
t=0 i,t i,t
intractable trajectory-level KL-divergence D (π ∥πj−1), we consider the time-wise causal KL-
kl i i
divergence (cid:80)T γtD [π (·)∥πj−1(·)], and by substituting it and the equation (11) into the
t=0 kl i,t i,t
objective (12), we have:
(14)
T
(cid:104)(cid:88) (cid:16) 1 (cid:17)(cid:105)
maxE γt r (o ,a )+β (o )−η D [π (·)∥πl (·)]− D [π (·)∥πj−1(·)]
i t t i t 1 kl i,t i,t η kl i,t i,t
2
t=0
where, for brevity, we denote π (a |o ) as π (·). This objective maximizes the rewards
i,t i,t i,t i,t
under the guarantee of fidelity and stability, which aligns well with the RL paradigm. Since
D (x,y) = H(x,y)−H(x), objective (14) can be further derived as:
kl
T
(cid:104)(cid:88) (cid:16) (cid:17)(cid:105)
maxE γt r∗(o ,a )+ηH[π (·)] (15)
i t t i,t
t=0
whereforbrevity,wedenoteη = 1+ ηη 21η2 andr i∗(o t,a t) = r i(o t,a t)+β i(o t)+E πi,t[log(π io ,t)η1(π ij ,− t1)η1 2].
This objective maximizes the entropy of learned policy π , which aligns well with the RL
i,t
paradigm.
14Algorithm 1 TrafficGamer for capturing CCE
Input: Offline dataset Dl, the number of total agents I, constraint threshold ϵ, Lagrange
multiplier λ, rollout rounds B, update rounds K, loss parameters λ and λ , clipping
1 2
parameter ω, value functions
V¯πi,π−i,j−1,
risk measure ρ, GAE lambda λ , distributional cost
ϕr g
i,t
value critic {Zc}I , the policies {π }I
i i=1 θi i=1
Initialize the world model Ml, observation o from Dec-POCMDP and the roll-out dataset
θ 0
D ;
roll
for n = 1,2,...,N do
Retrieve the nth scenario (including trajectories {τ }I and map ζ ) from Dl;
n,i i=1 n
K T
(cid:104)(cid:89)(cid:16)(cid:88) (cid:16) (cid:17) (cid:16) (cid:17)(cid:17)(cid:105)
UpdatetheworldmodelMl: L = −E log ωkp(al | µk ,bk ) +log ωk(s )
θ D l i i,t i,t i,t i T
k=0 t=0 (cid:124) (cid:123)(cid:122) (cid:125)
(cid:124) (cid:123)(cid:122) (cid:125) classification loss
regression loss
end for
for n = 1,2,...,N do
Retrieve the nth scenario (including trajectories {τ }I and map ζ ) from Dl;
n,i i=1 n
for b = 1,2,...,B do
For each agent i:
Perform roll-out with the policy π in the nth scenario;
θ
Collect trajectories τ = [o ,a ,r ,c ,..,o ,a ,r ,c ];
i,b i,0 i,0 i,0 i,0 i,T i,T i,T i,T
Calculate reward advantages Ar and total rewards R from the trajectory;
i,t i,t
Calculate cost advantages Ac = (cid:80)T (γλ )ι[c +γρ(Zc(o ))−ρ(Zc(o ))]
i,t ι=t g i,ι i,ι+1 i,ι
Add samples to the dataset D = D ∪{o ,a ,r ,c ,Ar ,R ,Ac }T ;
roll roll i,t i,t i,t i,t i,t i,t i,t t=1
end for
for i = 1,2,...,I do
for k = 1,2,...,K do
Sample a data point o ,a ,r ,c ,Aπ,r,R ,Aπ,c;
i,k i,k i,k i,k i,k i,k i,k
(cid:104)
π (·) π (·)
Calculate the clipping loss: LCLIP(θ ) = min ( θi,k Ar ,clip( θi,k ,1 − ω,1 +
i,k π (·) i,k π (·)
θold θold
(cid:105) i,k i,k
ω)Ar )+ηH[π (·)]−λ(Ac −ε)
i,k θ i,k i,k
Calculate the value function loss: LVF = ∥V¯πi,π−i,j−1(s )−R∥2
ϕr k 2
i,k
Update policy parameters by minimizing the loss: −LCLIP +λ LVF −λ H(π )
1 2 i
Update the cost distribution Zc by distributional Bellman operator with the equation
i
18
end for
Update the Lagrange multiplier by minimizing the loss: Lλ: λ[E (Aˆc)−ϵ]
D roll i
end for
end for
15Adjusting the Level of Competition among Heterogeneous Agents
To effectively simulate complex traffic scenarios that include various vehicle types—such as
cars, buses, and trucks—and diverse driving styles, such as aggressive and conservative driving,
it is crucial to tailor the behavior of each agent and regulate the level of competition in the
scenarios created. By subjecting the AV control system to these varied scenarios, we can more
thoroughly assess the system’s robustness. This comprehensive evaluation helps in developing
trustworthy AV vehicles capable of performing reliably in realistic traffic conditions.
To accurately represent diverse levels of scenarios characterized by different CCEs, we
incorporate the constrained and risk-sensitive policy optimization into the multi-agent traffic
simulation system.
Constrained Traffic Simulation. To dynamically adjust the intensity of CCEs, we request
the agents to AV agents varying levels of driving constraints, thereby modulating the severity
and nature of the driving conditions. Specifically, we expand the objective (15) by formulating
the trade-off between rewards and costs under a constrained policy optimization objective:
T T
(cid:104)(cid:88) (cid:16) (cid:17)(cid:105) (cid:104)(cid:88) (cid:105)
argmaxE γt r∗(o ,a )+ηH[π (·)] s.t. E γtc(o ,a ) ≤ ϵ (16)
π
π,T,µ0 i,t i,t i,t i,t i,t
t=0 t=0
where c represents the cost function aligning to different constraints. In this study, we
mainly explore how the distance constraint influences the resulting CCE from our algorithm.
Additionally, we can set different vehicle distance constraints to achieve varying intensities of
CCE. As the distance between vehicles increases, the competitiveness among agents decreases.
Risk-sensitive Traffic Simulation. This strategy explicitly manages the risk sensitivity
of driving behaviors, thereby deriving risk-seeking or risk-averse policies for each AV agent.
This approach effectively promotes either aggressive or conservative driving policies to enhance
the realism and variability of the scenarios. To drive risk-sensitive and constraints-satisfying
policies for multiple agents, inspired by [55], we develop a risk-sensitive constraint to capture
the uncertainty induced by environmental dynamics and extend the objective 16 as follows:
T T
(cid:104)(cid:88) (cid:16) (cid:17)(cid:105) (cid:104)(cid:88) (cid:105)
argmaxE γt r∗(o ,a )+ηH[π (·)] s.t. ρ γtC(O (s ),A ) ≤ ϵ (17)
π
π,T,µ0 i,t i,t i,t α i,t t i,t
t=0 t=0
where C is the cost variable and α represents confidence. To specify the risk measure, we define
thecorrespondingriskenvelopeUπi = {ζ : Γ → [0, 1]|(cid:80) ζ(τ )π (τ ) = 1},characterizedasa
α α α τi∈Γ i i i
compact, convex, and bounded set. This envelope guides the risk measure, which is induced by a
distorted probability distribution for each agent πζ(τ ) = ζ·π (τ ). For example, the Conditional
i i i i
Value-at-Risk (CVaR) can be defined as ρπ αi[(cid:80)T t=0γtc i,t] = sup
ζα∈Uαπi
E τi∼pπi[ζ α(τ i)(cid:80)T t=0γtc i,t].
Constructingthedistortedprobability-basedriskmeasurereliesontheestimateddistribution
ofdiscountedcumulativecosts. Toestimatethisdistribution,wedefinethevariableofdiscounted
cumulative costs as Zc(o ) = (cid:80)T−tγιC |O = o [69]. During fine-tuning, the stochastic
i,t ι=0 ι 0 i,t
POCMDP process can be captured by the distributional Bellman equation [70, 56]:
Zc(o ) : =∆ C(o ,a )+γZc(O (s )) where s ∼ T (·|s ,a ) and a ∼ π (·|o )
i,t i,t i,t i,t+1 t+1 t+1 t i,t i,t i,t i,t
(18)
16By following [71], we parameterize the distribution with N supporting quantiles and update
these function via quantile regression [56], which acts as an asymmetric squared loss in an
interval [−κ,κ] around zero:
(cid:40)
1u2, if|u| ≤ κ
ρκ (u) = |τ −δ |L (u) where L (u) = 2 (19)
τq q {u<0} κ κ κ(|u|− 1κ), otherwise
2
τ is the quantile, δ denotes a Dirac and L (u) is a Huber loss. Under these formulations,
q κ
the risk-sensitive advantage function Ac can be computed with 1-step TD updates such that
i,t
Ac = c +γρ(Zc(o ))−ρ(Zc(o )). To effectively optimize (17) by updating the Lagrange
i,t i,t i,t+1 i,t
multipliers, we design a multi-agent constrained policy gradient algorithm to update the policy
π (a |o ) under the CTDE framework [72, 51] (see Algorithm 1). Implementation details of
i,t i,t i,t
the specific actor and critic networks can be found in Supplementary Section 1e. By utilizing
the risk-sensitivity optimization with the CVaR method, we can adjust the convince α to
control whether the policy exhibits risk-seeking or risk-avoidance behavior. The larger α is, the
more the policy tends to accept risk-seeking behavior.
Data availability
The Argoverse 2 dataset we used to train TrafficGamer is publicly available at https://www.ar
goverse.org/ for non-commercial usage. The Waymo dataset we used to train TrafficGamer
is publicly available at https://waymo.com/open/ for non-commercial usage. The background
image of the simulated Argoverse 2 and Waymo environment is from the Argoverse 2 and
Waymo dataset. Source data for figures are provided in this paper.
Code availability
The Scenarionet simulation platform is publicly available at https://github.com/metadri
verse/scenarionet. The av2-api is available at https://github.com/argoverse/av2-api.
The waymo-api is available at https://github.com/waymo-research/waymo-open-dataset.
The source code used to analyze experiment results is publicly available at https://github.c
om/qiaoguanren/TrafficGamer.
17References
´
[1] Lo´pez, P. A. et al. Microscopic traffic simulation using SUMO. In International Conference
on Intelligent Transportation Systems, (ITSC) (2018).
[2] Zhang, H. et al. Cityflow: A multi-agent reinforcement learning environment for large
scale city traffic scenario. In The World Wide Web Conference, (WWW) (2019).
[3] Shah, S., Dey, D., Lovett, C. & Kapoor, A. Airsim: High-fidelity visual and physical
simulation for autonomous vehicles. In Field and Service Robotics (2017).
[4] Yang, Z. et al. Unisim: A neural closed-loop sensor simulator. In IEEE/CVF Conference
on Computer Vision and Pattern Recognition, (CVPR) (2023).
[5] Leurent, E. An environment for autonomous driving decision-making (2018).
[6] Cai, P., Lee, Y., Luo, Y. & Hsu, D. SUMMIT: A simulator for urban driving in massive
mixed traffic. In International Conference on Robotics and Automation, (ICRA) (2020).
[7] Li, Q. et al. Metadrive: Composing diverse driving scenarios for generalizable reinforcement
learning. IEEE Trans. Pattern Anal. Mach. Intell., (TPAMI) (2023).
[8] Wang, X., Krasowski, H. & Althoff, M. Commonroad-rl: A configurable reinforcement
learning environment for motion planning of autonomous vehicles. In IEEE International
Intelligent Transportation Systems Conference, (ITSC) (2021).
[9] Sun, Q., Huang, X., Williams, B. C. & Zhao, H. Intersim: Interactive traffic simulation via
explicit relation modeling. In IEEE/RSJ International Conference on Intelligent Robots
and Systems, (IROS) (2022).
[10] Xu, D., Chen, Y., Ivanovic, B. & Pavone, M. Bits: Bi-level imitation for traffic simulation.
In IEEE International Conference on Robotics and Automation, (ICRA) (2023).
[11] Gulino,C.et al. Waymax: Anaccelerated,data-drivensimulatorforlarge-scaleautonomous
driving research. CoRR abs/2310.08710 (2023).
[12] Lavington, J.W.et al. Torchdriveenv: Areinforcementlearningbenchmarkforautonomous
driving with reactive, realistic, and diverse non-playable characters. arXiv preprint
arXiv:2405.04491 (2024).
[13] Gao, D. et al. Cardreamer: Open-source learning platform for world model based au-
tonomous driving. arXiv (2024).
[14] Simulation, M. Carsim (2024). URL https://www.carsim.com.
[15] MathWorks, T. Vehicle dynamics blockset: Model and simulate vehicle dynamics in a
virtual 3d environment (2024). URL https://www.mathworks.com/products/vehicle
-dynamics.html.
18[16] Dosovitskiy, A., Ros, G., Codevilla, F., Lopez, A. & Koltun, V. CARLA: An open urban
driving simulator. In Conference on Robot Learning, (CoRL) (2017).
[17] Nvidia. Nvidia drive end-to-end platform for software-defined vehicles (2024). URL
https://www.nvidia.com/en-us/self-driving-cars/.
[18] VI-grade. Driving simulators: An invaluable set of tools to successfully accelerate your
develop ment process (2023). URL https://www.vi-grade.com/en/solutions/drivin
gsimulators.
[19] Ding, W. et al. A survey on safety-critical driving scenario generation - A methodological
perspective. IEEE Trans. Intell. Transp. Syst., (TIPS) (2023).
[20] Bergamini, L. et al. Simnet: Learning reactive self-driving simulations from real-world
observations. In IEEE International Conference on Robotics and Automation, (ICRA)
(2021).
[21] Feng, L., Li, Q., Peng, Z., Tan, S. & Zhou, B. Trafficgen: Learning to generate diverse and
realistic traffic scenarios. In IEEE International Conference on Robotics and Automation,
(ICRA) (2023).
[22] Suo, S., Regalado, S., Casas, S. & Urtasun, R. Trafficsim: Learning to simulate realistic
multi-agent behaviors. In IEEE/CVF Conference on Computer Vision and Pattern
Recognition, (CVPR) (2021).
[23] Huang, Z. et al. Versatile scene-consistent traffic scenario generation as optimization with
diffusion. CoRR abs/2404.02524 (2024).
[24] Yan, X. et al. Learning naturalistic driving environment with statistical realism. Nature
Communications 14, 2037 (2023).
[25] Yao, Z., Li, X., Lang, B. & Chuah, M. C. C. Goal-lbp: Goal-based local behavior
guided trajectory prediction for autonomous driving. IEEE Transactions on Intelligent
Transportation Systems, (TIPS) 25, 6770–6779 (2024).
[26] Tan, S. et al. Scenegen: Learning to generate realistic traffic scenes. In IEEE Conference
on Computer Vision and Pattern Recognition, (CVPR) (2021).
[27] Zheng, Z., Ying, X., Yao, Z. & Chuah, M. C. C. Robustness of trajectory prediction
models under map-based attacks. In IEEE/CVF Winter Conference on Applications of
Computer Vision, (WACV) (2023).
[28] Wang, J. et al. Advsim: Generating safety-critical scenarios for self-driving vehicles. In
IEEE Conference on Computer Vision and Pattern Recognition, (CVPR) (2021).
[29] Rempe, D., Philion, J., Guibas, L. J., Fidler, S. & Litany, O. Generating useful accident-
prone driving scenarios via a learned traffic prior. In IEEE/CVF Conference on Computer
Vision and Pattern Recognition, (CVPR) (2022).
19[30] Feng, S., Yan, X., Sun, H., Feng, Y. & Liu, H. X. Intelligent driving intelligence test for au-
tonomous vehicles with naturalistic and adversarial environment. Nature Communications
(2021).
[31] Hanselmann, N., Renz, K., Chitta, K., Bhattacharyya, A. & Geiger, A. KING: generating
safety-critical driving scenarios for robust imitation via kinematics gradients. In European
Conference on Computer Vision, (ECCV) (2022).
[32] Suo, S. et al. Mixsim: A hierarchical framework for mixed reality traffic simulation. In
IEEE/CVF Conference on Computer Vision and Pattern Recognition, (CVPR) (2023).
[33] Cao, Y. et al. Robust trajectory prediction against adversarial attacks. In Conference on
Robot Learning, (CoRL) (2022).
[34] Zhang,J.,Xu,C.&Li,B. Chatscene: Knowledge-enabledsafety-criticalscenariogeneration
for autonomous vehicles. In IEEE/CVF Conference on Computer Vision and Pattern
Recognition, (CVPR) (2024).
[35] Wang, Z., Li, X., Wei, D., Wang, L. & Huang, Y. Efficient generation of safety-critical
scenarios combining dynamic and static scenario parameters. IEEE Transactions on
Intelligent Vehicles, (TIV) (2024).
[36] Zhang, C. et al. Learning realistic traffic agents in closed-loop. In Conference on Robot
Learning, (CoRL) (2023).
[37] Ding, W., Lin, H., Li, B. & Zhao, D. Generalizing goal-conditioned reinforcement learning
with variational causal reasoning. In Advances in Neural Information Processing Systems,
(NeurIPS) (2022).
[38] Bai, Y., Jin, C. & Yu, T. Near-optimal reinforcement learning with self-play. In Advances
in Neural Information Processing Systems, (NeurIPS) (2020).
[39] Bai, Y., Jin, C., Wang, H. & Xiong, C. Sample-efficient learning of stackelberg equilibria
in general-sum games. In Advances in Neural Information Processing Systems, (NeurIPS)
(2021).
[40] Song, Z., Mei, S. & Bai, Y. When can we learn general-sum markov games with a
large number of players sample-efficiently? In International Conference on Learning
Representations, (ICLR) (2022).
[41] Liu, M., Ozdaglar, A. E., Yu, T. & Zhang, K. The power of regularization in solving
extensive-form games. In International Conference on Learning Representations, (ICLR)
(2023).
[42] Cen, S., Chi, Y., Du, S. S. & Xiao, L. Faster last-iterate convergence of policy optimization
in zero-sum markov games. In International Conference on Learning Representations,
(ICLR) (2023).
20[43] Sokota, S. et al. A unified approach to reinforcement learning, quantal response equilibria,
and two-player zero-sum games. In International Conference on Learning Representations,
(ICLR) (2023).
[44] Li, P. et al. Configurable mirror descent: Towards a unification of decision making. In
International Conference on Machine Learning, (ICML) (2024).
[45] Wilson, B. et al. Argoverse 2: Next generation datasets for self-driving perception and
forecasting. In Advances in Neural Information Processing Systems Track on Datasets and
Benchmarks 1, (NeurIPS) (2021).
[46] Ettinger, S. et al. Large scale interactive motion forecasting for autonomous driving :
The waymo open motion dataset. In IEEE/CVF International Conference on Computer
Vision, (ICCV) (2021).
[47] Zhang, Y., Zhang, R., Gu, Y. & Li, N. Multi-agent reinforcement learning with reward
delays. In Learning for Dynamics and Control Conference, (L4DC) (2023).
[48] Guanren, Q., Guorui, Q., Rongxiao, Q. & Guiliang, L. Modelling competitive behaviors in
autonomous driving under generative world model. In European Conference on Computer
Vision, (ECCV) (2024).
[49] Nash, J. Non-cooperative games. Annals of mathematics 286–295 (1951).
[50] Zhou,Z.,Wang,J.,Li,Y.&Huang,Y. Query-centrictrajectoryprediction. InInternational
Conference on Computer Vision and Pattern Recognition, (CVPR) (2023).
[51] Yu, C. et al. The surprising effectiveness of PPO in cooperative multi-agent games. In
Advances in Neural Information Processing Systems, (NeurIPS) (2022).
[52] Huang, Z., Liu, H. & Lv, C. Gameformer: Game-theoretic modeling and learning
of transformer-based interactive prediction and planning for autonomous driving. In
International Conference on Computer Vision, (ICCV) (2023).
[53] Rubner, Y., Tomasi, C. & Guibas, L. The earth mover’s distance as a metric for image
retrieval. International Journal of Computer Vision, (IJCV) 40, 99–121 (2000).
[54] Li, Q. et al. Scenarionet: Open-source platform for large-scale traffic scenario simulation
and modeling. In Advances in Neural Information Processing Systems, (NeurIPS) (2023).
[55] Xu,S.&Liu,G. Uncertainty-awareconstraintinferenceininverseconstrainedreinforcement
learning. In International Conference on Learning Representations, (ICLR) (2024).
[56] Dabney, W., Rowland, M., Bellemare, M. G. & Munos, R. Distributional reinforcement
learning with quantile regression. In AAAI Conference on Artificial Intelligence, (AAAI)
(2017).
[57] Mao, W. & Basar, T. Provably efficient reinforcement learning in decentralized general-sum
markov games. Dynamic Games And Applications 13, 165–186 (2023).
21[58] Hwang, K.-S., Chiou, J.-Y. & Chen, T.-Y. Cooperative reinforcement learning based on
zero-sum games. SICE Annual Conference 2973–2976 (2008).
[59] Yang, Y. & Wang, J. An overview of multi-agent reinforcement learning from game
theoretical perspective. CoRR abs/2011.00583 (2020).
[60] Hu, J. & Wellman, M. P. Nash q-learning for general-sum stochastic games. Journal of
Machine Learning Research, (JMLR) 4, 1039–1069 (2003).
[61] Hambly, B. M., Xu, R. & Yang, H. Policy gradient methods find the nash equilibrium
in n-player general-sum linear-quadratic games. Journal of Machine Learning Research,
(JMLR) 24, 139:1–139:56 (2023).
[62] Brown, T. B. et al. Language models are few-shot learners. In Advances in Neural
Information Processing Systems, (NeurIPS) (2020).
[63] Zhou, Z., Wen, Z., Wang, J., Li, Y.-H. & Huang, Y.-K. Qcnext: A next-generation
framework for joint multi-agent trajectory prediction. arXiv preprint arXiv:2306.10508
(2023).
[64] Qiao, G., Liu, G., Poupart, P. & Xu, Z. Multi-modal inverse constrained reinforcement
learning from a mixture of demonstrations. In Advances in Neural Information Processing
Systems, (NeurIPS) (2023).
[65] Sun, Y. & Yang, S. Manifold-constrained gaussian process inference for time-varying
parameters in dynamic systems. Statistics and Computing 33, 142 (2023).
[66] Haarnoja, T., Tang, H., Abbeel, P. & Levine, S. Reinforcement learning with deep
energy-based policies. In International Conference on Machine Learning, (ICML) (2017).
[67] Haarnoja, T., Zhou, A., Abbeel, P. & Levine, S. Soft actor-critic: Off-policy maximum
entropy deep reinforcement learning with a stochastic actor. In International Conference
on Machine Learning, (ICML) (2018).
[68] Ziebart, B. D., Bagnell, J. A. & Dey, A. K. Modeling interaction via the principle of
maximum causal entropy. In International Conference on Machine Learning, (ICML),
1255–1262 (2010).
[69] Luo, Y., Liu, G., Duan, H., Schulte, O. & Poupart, P. Distributional reinforcement learning
with monotonic splines. In International Conference on Learning Representations, (ICLR)
(2022).
[70] Gerstenberg, J., Neininger, R. & Spiegel, D. On solutions of the distributional bellman
equation. CoRR abs/2202.00081 (2022).
[71] Dabney, W., Rowland, M., Bellemare, M. G. & Munos, R. Distributional reinforcement
learning with quantile regression. In AAAI Conference on Artificial Intelligence, (AAAI),
2892–2901 (2018).
22[72] Schulman, J., Wolski, F., Dhariwal, P., Radford, A. & Klimov, O. Proximal policy
optimization algorithms. CoRR abs/1707.06347 (2017).
Acknowledgment
Author contributions
G.Q.1 and G.L. conceived and led the research project. G.Q.1, G.Q.2, and G.L. developed the
framework and algorithm. G.Q.1 and G.L. wrote the paper. G.Q.1 and G.L. developed the
experimental metrics for the algorithm. G.Q.1 and G.Q.2 implemented the algorithms and
world model. G.Q.1 and J.Y. pre-trained the world model. G.Q.1 prepared the exploitability
experiment results and fidelity experiment results. G.Q.1, G.Q.2, and J.Y. implemented the
baselines. G.Q.1 and J.Y. prepared the scenario visualization experiment results. All authors
provided feedback during the manuscript revision and discussion of results. G.L. approved the
submission and accepted responsibility for the overall integrity of the paper.
Competing Interests
The authors declare no competing interests.
23Figure 1: Crafting flexible and reliable competitive driving scenarios with game-
theoretic oracles. a The complex autonomous driving competitive environment exists in
reality, but the probability of this scenario occurring is low. We lack such safety-critical
instances to train a robust AV system. we find that with the increase in congestion level and
OOD data, the performance of the AV system will decrease. This will greatly hinder the
development of AV systems. b Autonomous driving scenarios encompass valuable information
such as historical trajectories, map features, and interactive characteristics of surrounding
vehicles. TrafficGamer can effectively capture the dynamics of competitive environments to
predict future trajectories and efficiently assist in learning optimal policies for each vehicle and
solving Coarse Correlated Equilibriums to generate safety-critical scenarios. c Major challenges
for generating safety-critical scenarios. The challenges include the ”curse of fidelity” for future
trajectory prediction, the ”Finding Equilibrium” for solving CCE during the algorithm update
process, and the ”controlled Equilibrium” for how to control the intensity of CCE.
24Figure 2: Structure of TrafficGamer. The definition of the symbols in the figure can be
found in the Problem Formulation section of the Method. a The TrafficGamer framework
consists of two parts: pre-training and fine-tuning. Initially, we utilize the training dataset to
train a generative world model. Subsequently, MARL algorithms are employed to fine-tune this
world model. b We treat the world model as the environment providing observations (Obs),
with throttle and brake actions for vehicles, and design reward and cost functions to establish a
Dec-POCMDP (Decentralized Partially Observable Markov Decision Process). Our Multi-agent
CCE-Solver, combined with optimistic V-learning update and magnet mirror descent, is utilized
to learn optimal policies for each agent. This process controls and captures CCE during the
learning phase. c We adjust distance constraint and risk coefficient to control the intensity
of CCE, thereby influencing the competitiveness of vehicles in scenarios. By extension, we
combine a Lagrangian-based optimization algorithm, which adopts constraints on different
vehicle distances to control CCE and a risk-sensitive algorithm, which employs CVaR with
different confidence levels to affect the vehicles’ risk sensitivity.
25Figure 3: Visualization of road map. The experimental scenarios we selected from Argoverse
2 and Waymo. yellow cars are controlled by algorithms, green and gray cars represent
environmental vehicles, black lines depict vehicle trajectories, and blue arrows indicate travel
direction.
26Figure 4: Statistical realism of driving behavior. From top to bottom are all methods’
vehicle distance and speed distributions. We use various distance metrics to measure the
distance between the generated and the real data distribution in three settings: all scenarios,
fork road, and cross road.
27Figure 5: CCE-gap obtained from the first calculation approach, where each row
represents a scenario and scenarios 1-6 correspond to Y-Junction, Dense-lane intersection,
Roundabout, Dual-lane intersection, T-Junction, and Merge respectively. Each column corre-
sponds to one of the agents in the multi-agent environment. Because of the space limit, the
CCE-gap results of other agents can be seen in Supplementary Section 3a.
28Figure 6: CCE-gap obtained from the second calculation approach, where each row
represents a scenario and scenarios 1-6 correspond to Y-Junction, Dense-lane intersection,
Roundabout, Dual-lane intersection, T-Junction, and Merge respectively. Each column corre-
sponds to one of the agents in the multi-agent environment. Because of the space limit, the
CCE-gap results of other agents can be seen in Supplementary Section 3a.
29Figure 7: Variation of Lagrangian Parameters under varying constraints, where each
row represents a scenario and scenarios 1-6 correspond to Y-Junction, Dense-lane intersection,
Roundabout, Dual-lane intersection, T-Junction, and Merge respectively. Each column corre-
sponds to one of the agents in the multi-agent environment. Because of the space limit, the
lagrangian parameter results of other agents can be seen in Supplementary Section 3b.
3031Figure 8: Visualization of generated trajectories with high diversity (Argoverse2).
We visualize all Argoverse2 scenarios in a 3x3 grid layout with 2D and 3D simulations. In
each grid, the distance constraint relaxes from top to bottom, and the risk level strengthens
from left to right. In the 2D scenes, yellow cars are algorithm-controlled, green cars and gray
cars are environmental vehicles, black lines are vehicle trajectories, and blue arrows indicate
travel direction. In the 3D scenes, red cars are algorithm-controlled, while blue cars represent
environmental vehicles.
3233Figure 9: Visualization of generated trajectories with high diversity (Waymo). We
visualize all Waymo scenarios in a 3x3 grid layout with 2D and 3D simulations. In each grid,
the distance constraint relaxes from top to bottom, and the risk level strengthens from left
to right. In the 2D scenes, yellow cars are algorithm-controlled, green cars and gray cars are
environmental vehicles, black lines are vehicle trajectories, and blue arrows indicate travel
direction. In the 3D scenes, red cars are algorithm-controlled, while other cars represent
34
environmental vehicles.Figure 10: Presentation of the game type. In cooperative games, agents share a common
interest and work together to optimize a global objective, demonstrating fully collaborative
behavior. In contrast, zero-sum games are defined by strict competition, where one player’s gain
is exactly equal to another’s loss, ensuring that the total rewards among all participants remain
constant—typically zero. General-sum games, however, present a more complex scenario where
the sum of all agents’ rewards can vary significantly. In these games, while agents may initially
act competitively due to self-interest, they can also maximize their benefits by collaborating
with others, blending competitive and cooperative dynamics.
35Figure 11: Illustration of the decoder network module. We have N attention lay-
ers to decode accurate trajectories. 1) Agent-to-Map Cross Attention incorporates the
map information zm = ξ(M) (ξ is a map encoder) including all kinds of lane mark-
ings and types of polygons into the most recent agents’ features such that e.g., zs =
i,t
CrossAttn(τ ,...,τ ,zm). 2) Agent-to-Temporal Cross Attention that embeds the histor-
1,t I,t
ical information into the agents’ trajectories τ ,...,τ where τ consists of a and
1,t+1 I,t+1 1,t+1 1,t
s , e.g., zW = CrossAttn({zs ,...,zs },zs ), where H indicates the length of encoding
1,t i,t i,t−1 i,t−W i,t
historical steps. 3) Agent-to-Neighbor Cross Attention that embeds the spatial-temporal features
of the surrounding agents such that zN = CrossAttn(zW,{zW} ) and 4) Self-Agent Attention
i,t i,t j,t j∈Ni
considers the agent’s entire predicted trajectory at each time step, capturing dependencies
between distant elements, e.g., zA = SelfAttn({zN}T ). The final state s is represented by
i,t i,t t=0 t+1
concatenating s = {zA }t,I with the predicted zA ,...,zA .
t i,w w=t−W,i=0 1,t+1 I,t+1
36Table 1: The mean ± std of the CCE-gap, calculated using the first method, is based on
an average of over 300 episodes. The best average performance is highlighted in bold. N/A
indicates this agent does not exist in this scenario.
Method Agent 1 Agent 2 Agent 3 Agent 4 Agent 5 Agent 6 Agent 7
Scenario 1
QCNet 8.98±4.18 4.51±0.99 13.98±1.95 39.58±9.26 17.90±0.45 N/A N/A
GameFormer 19.63±6.26 9.93±7.54 21.73±6.14 100.78±18.41 36.17±10.70 N/A N/A
MAPPO 7.85±0.47 4.20±0.71 1.64±0.42 7.45±0.76 7.54±0.80 N/A N/A
TrafficGamer 2.32±0.60 1.05±0.23 2.09±1.22 0.99±0.41 2.13±0.79 N/A N/A
Scenario 2
QCNet 3.36±0.50 47.48±7.13 22.10±0.16 25.51±0.31 26.94±0.32 35.58±4.74 6.33±2.81
GameFormer 11.81±2.59 34.69±0.78 27.75±1.29 25.95±0.95 29.90±0.99 57.80±18.25 4.04±1.62
MAPPO 10.22±0.58 11.12±0.49 11.50±0.45 9.33±0.32 13.43±0.33 12.34±0.44 11.24±0.69
TrafficGamer 2.99±0.25 2.02±0.48 2.76±0.52 1.13±0.52 2.83±0.68 2.87±0.51 1.42±1.26
Scenario 3
QCNet 24.21±2.27 13.61±0.69 3.72±2.26 16.47±2.52 8.65±0.72 3.10±0.39 9.77±0.76
GameFormer 56.98±9.77 24.98±4.20 15.82±7.25 57.54±8.56 25.16±5.18 23.03±8.74 27.16±5.97
MAPPO 4.28±0.58 4.90±0.80 2.85±0.55 6.27±0.50 3.01±0.47 0.59±0.41 3.22±0.78
TrafficGamer 1.66±0.46 2.98±0.74 1.71±0.44 1.06±0.44 1.19±0.64 2.38±0.75 2.44±0.45
Scenario 4
QCNet 15.08±2.04 52.62±7.41 11.54±1.89 13.83±0.56 27.96±4.47 N/A N/A
GameFormer 10.39±3.44 21.58±2.71 16.74±1.21 19.92±0.88 25.81±6.66 N/A N/A
MAPPO 4.36±0.58 22.02±0.67 5.33±0.65 6.05±0.49 5.38±0.59 N/A N/A
TrafficGamer 2.78±0.34 2.98±0.55 1.48±0.37 2.22±0.50 1.21±0.61 N/A N/A
Scenario 5
QCNet 5.59±1.35 16.46±3.66 11.50±0.38 6.43±0.66 18.16±2.57 20.58±0.48 13.53±0.30
GameFormer 15.88±4.28 51.62±14.47 20.96±3.78 29.89±9.07 13.76±1.73 25.27±0.79 14.93±1.11
MAPPO 6.21±0.60 8.60±0.50 9.85±0.78 9.69±1.01 18.37±0.91 8.46±1.08 6.01±1.14
TrafficGamer 2.10±0.40 2.71±0.79 3.03±0.31 2.27±0.69 1.97±0.30 1.30±0.40 2.14±0.28
Scenario 6
QCNet 33.65±4.52 27.86±10.28 90.52±15.58 48.55±12.74 11.91±2.35 N/A N/A
GameFormer 57.82±18.64 70.98±34.60 65.00±22.43 22.35±6.29 17.52±2.06 N/A N/A
MAPPO 7.01±1.21 5.69±0.79 27.21±0.82 5.30±0.82 3.13±0.61 N/A N/A
TrafficGamer 0.72±0.53 1.09±0.56 1.39±1.33 1.49±0.66 0.63±0.34 N/A N/A
37Table 2: The mean ± std of the CCE-gap, calculated using the second method, is based on
an average of over 300 episodes. The best average performance is highlighted in bold. N/A
indicates this agent does not exist in this scenario.
Method Agent 1 Agent 2 Agent 3 Agent 4 Agent 5 Agent 6 Agent 7
Scenario 1
QCNet 16.00±5.13 5.32±2.50 9.67±3.23 22.31±3.70 13.92±0.51 N/A N/A
GameFormer 21.85±2.51 11.27±6.19 15.48±5.21 85.46±15.52 28.83±7.31 N/A N/A
MAPPO 14.70±1.68 4.95±0.55 8.99±1.11 7.16±0.96 10.22±1.18 N/A N/A
TrafficGamer 13.17±1.30 3.56±1.06 6.33±1.41 8.34±0.88 6.77±0.96 N/A N/A
Scenario 2
QCNet 5.69±1.02 37.63±10.62 16.45±0.24 19.88±0.67 14.47±0.51 29.02±4.78 21.75±2.74
GameFormer 14.83±5.49 25.50±1.32 22.31±1.93 20.99±1.54 18.09±2.10 51.29±18.34 19.48±1.65
MAPPO 6.63±0.90 15.97±1.42 6.76±0.53 13.40±0.56 9.92±0.81 17.06±0.66 16.10±0.61
TrafficGamer 2.52±0.77 11.56±2.01 6.77±0.62 12.13±0.62 8.95±0.81 15.25±0.83 14.77±0.52
Scenario 3
QCNet 18.84±3.83 9.31±1.40 8.47±5.26 12.96±3.71 4.36±0.83 1.55±0.49 6.40±0.77
GameFormer 51.65±16.59 18.75±5.53 18.14±10.20 53.29±16.05 19.67±9.20 21.49±8.76 23.66±5.94
MAPPO 13.22±0.84 3.58±1.51 8.50±0.51 15.78±0.73 0.65±0.16 1.20±0.78 7.58±0.82
TrafficGamer 11.73±1.13 3.68±1.40 9.04±0.87 4.20±1.58 0.86±0.26 1.65±0.56 5.01±0.63
Scenario 4
QCNet 14.67±2.76 42.88±11.14 3.97±3.31 9.46±1.12 14.47±7.29 N/A N/A
GameFormer 9.78±3.45 13.18±4.03 8.77±1.38 15.22±0.66 7.75±4.98 N/A N/A
MAPPO 8.00±0.53 28.46±1.37 5.66±1.46 7.97±0.82 16.74±1.44 N/A N/A
TrafficGamer 5.09±0.68 26.37±1.80 3.51±3.13 7.80±0.91 13.06±0.99 N/A N/A
Scenario 5
QCNet 5.22±1.85 14.60±8.33 7.94±0.48 2.79±0.91 13.64±4.75 21.03±0.60 10.02±0.43
GameFormer 13.92±5.96 48.58±17.36 17.50±5.18 23.06±10.27 8.42±2.09 25.75±0.81 11.48±1.09
MAPPO 2.67±0.46 20.33±2.47 3.02±0.44 3.42±0.51 6.07±0.54 15.39±0.36 2.83±0.36
TrafficGamer 3.33±0.62 13.09±1.73 2.85±0.50 2.03±0.65 4.49±1.16 11.83±0.37 0.94±0.38
Scenario 6
QCNet 20.46±4.64 16.94±10.80 63.95±16.86 44.22±10.87 6.96±2.39 N/A N/A
GameFormer 44.60±9.95 73.87±15.71 25.60±10.73 20.37±4.16 12.70±1.15 N/A N/A
MAPPO 21.46±1.51 7.07±0.83 13.60±1.79 7.06±1.03 5.64±0.79 N/A N/A
TrafficGamer 18.79±1.40 5.13±0.94 7.89±1.08 4.27±1.19 2.21±0.92 N/A N/A
38