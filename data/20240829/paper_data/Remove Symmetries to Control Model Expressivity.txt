Remove Symmetries to Control Model Expressivity
Liu Ziyin1,2,∗, Yizhou Xu3,∗, Isaac Chuang1,4
1Research Laboratory of Electronics, Massachusetts Institute of Technology
2Physics & Informatics Laboratories, NTT Research
3School of Computer and Communication Sciences, E´cole Polytechnique F´ed´erale de Lausanne
4Department of Physics, Massachusetts Institute of Technology
August 29, 2024
Abstract
When symmetry is present in the loss function, the model is likely to be trapped in a low-capacity
statethatissometimesknownasa“collapse.”Beingtrappedintheselow-capacitystatescanbeamajor
obstacletotrainingacrossmanyscenarioswheredeeplearningtechnologyisapplied. Wefirstprovetwo
concrete mechanisms through which symmetries lead to reduced capacities and ignored features during
training. We then propose a simple and theoretically justified algorithm, syre, to remove almost all
symmetry-induced low-capacity states in neural networks. The proposed method is shown to improve
the training of neural networks in scenarios when this type of entrapment is especially a concern. A
remarkablemeritoftheproposedmethodisthatitismodel-agnosticanddoesnotrequireanyknowledge
of the symmetry.
1 Introduction
The unprecedented scale and complexity of modern neural networks, which incorporate a vast number of
neurons and connections, inherently introduce a high degree of redundancy in model parameters. This com-
plexity in the architecture and the design of loss functions often implies that the loss functions are invariant
to various hidden, nonlinear, and nonlocal transformations of the model parameters. These invariant trans-
formations, or “symmetries,” in the loss function have been extensively documented in the literature, with
commonexamplesincludingpermutation,rescaling,scale,androtationsymmetries[1,2,3,4,5,6,7,8,9,10].
A unifying perspective tounderstand how these symmetries affect learning is the framework of reflection
symmetries [11].1 A per-sample loss function ℓ possesses a P-reflection symmetry if for all θ, and all data
points x,
ℓ((I−2P)θ,x)=ℓ(θ,x). (1)
The solutions where θ = (I −P)θ are the symmetric solutions with respect to P. It has been shown that
almost all contemporary models contain quite a few reflection symmetries. Permutation, rescaling, scale,
and rotation symmetries all imply the existence of one or multiple reflection symmetries in the loss.
Recent literature has shown that symmetries in the loss function of neural networks often lead to the
formationoflow-capacitysaddlepointswithinthelosslandscape[12,13,14]. Thesesaddlepointsarelocated
at the symmetric solutions and often possess a lower capacity than the minimizers of the loss [11]. When
a model encounters these saddle points during training, the model parameters are not only slow to escape
thembutalsoattracted to thesesolutionsbecausethesethegradient noisealsovanishclose tothesesaddles
[14, 15]. Essentially, the model’s learning process stagnates, and it fails to achieve optimal performance due
to reduced capacity. However, while many works have characterized the dynamical properties of training
algorithms close to symmetric solutions, no methods are known to enable full escape from them.
*Equalcontribution.
1Essentially,thisisbecausethecommonsymmetriesindeeplearningallcontainZ 2 asasubgroup.
1
4202
guA
82
]GL.sc[
1v59451.8042:viXraBecausetheselow-capacitysaddlesarecreatedbysymmetries, weproposeamethodtoexplicitlyremove
thesesymmetriesfromthelossfunctionsofneuralnetworks. Themethodweproposeistheoreticallyjustified
and only takes one line of code to implement. By removing these symmetries, our method allows neural
networks to explore a more diverse set of parameter spaces and access more expressive solutions. The main
contributions of this work are:
1. We show how discrete symmetries in the model can severely limit the expressivity of neural networks in
the form commonly known as “collapses” (Section 4);
2. We propose a simple method that provably removes almost all symmetries in neural networks, without
having any knowledge of the symmetry (Section 5);
3. We apply the method to solve a broad range of practical problems where symmetry-impaired training
can be a major concern (Section 6).
We introduce the notations and problem setting for our work in the next section. Closely related works are
discussed in Section 3. All the proofs and experimental details are deferred to the appendix.
2 Discrete Symmetries in Neural Networks
Notation. For a matrix A, we use A+ to denote the pseudo-inverse of A. For groups U and G, U ⊲ G
denotes that U is a subgroup of G. For a vector w and matrix D, ∥θ∥2 ∶= wTDw is the norm of w with
D
respect to D. ⊙ denotes the element-wise product between vectors.
Let f(θ,x) be a function of the model parameters θ and input data point x. For example, f could either
be a sample-wise loss function ℓ or the model itself. Whenever f satisfies the following condition, we say
that f has the P-reflection symmetry (general symmetry groups are dealt with in Theorem 5 in Section 5).
Definition 1. Let P be a projection matrix and θ′ be a point. f is said to have the (θ′,P)-reflection
symmetry if for all x and θ, (1) f(θ+θ′,x)=f((I−2P)θ+θ′,x), and (2) Pθ′=θ′.
The second condition is due to the fact that there is a redundancy in the choice of θ′ when θ′ ≠ 0.
Requiring Pθ′ = θ′ removes this redundancy and makes the choice of θ′ unique. Since every projection
matrix can be written as a product of a (full-rank or low-rank) matrix O with orthonormal columns, one
can write P =OOT and refer to this symmetry as an O symmetry. In common deep learning scenarios, it
is almost always the case that θ′=0 (for example, this holds for the common cases of rescaling symmetries,
(double) rotation symmetries, and permutation symmetries, see Theorem 2-4 of Ref. [11]). A consequence
of θ′=0 is that the symmetric projection Pθ of any θ always has a smaller norm than θ: thus, a symmetric
solution is coupled to the solutions of weight decay, which also favors small-norm solutions. As an example
ofreflectionsymmetry,considerasimpletanhnetworkf(θ,x)=θ tanh(θ x). Themodeloutputisinvariant
1 2
to a simultaneous sign flip of θ and θ . This corresponds to a reflection symmetry whose projection matrix
1 2
is the identity P =((1,0),(0,1)). The symmetric solutions correspond to the trivial state where θ =θ =0.
1 2
Tobemoregeneral,weallowθ′ tobenonzerotogeneralizethetheoryandmethodtogenerichyperplanes
since the purpose of the method is to remove all reflection symmetries that may be hidden or difficult to
enumerate. Let us first establish some basic properties of a loss function with P-reflection, to gain some
intuition (again, proofs are in the appendix).
Proposition 1. Let f have the (θ′,P)-symmetry and Let f′(θ)=f(θ+θ†). Then, (1) for any θ† such that
Pθ†=θ′, f(θ+θ†)=f((I−2P)θ+θ†), and (2) f′ has the (θ′−θ†,P) symmetry.
Therefore, requiring Pθ′ = θ′ reduces different manifestations of the symmetry to a unique one and
simplyshiftingthefunctionwillnotremoveanysymmetry. Thispropositionemphasizesthesubtledifficulty
in removing a symmetry.
3 Related Works
One closely related work is Ref. [11], which shows that every reflection symmetry in the model leads to a
low-capacity solution that is energetically favored when weight decay is used. This is because the minimizer
of the weight decay is coupled with stationary points of the reflection symmetries – the projection of any
parameter to a symmetric subspace always decreases the norm of the parameter, and is thus energetically
preferred by weight decay. Our work develops a method to decouple symmetries and weight decay, thus
2avoidingcollapsingintolow-capacitystatesduringtraining. Besidesweightdecay,analternativemechanism
for this type of capacity loss is gradient-noise induced collapse, which happens when the learning rate -
batchsize ratio is high [14, 15].
Contemporarily, Ref. [16] empirically explores how removing symmetries can benefit neural network
training and suggests a heuristic for removing symmetries by hold a fraction of the weights unchanged
duringtraining. However,theproposedmethodcanonlyremovepermutationsymmetriesinfullyconnected
layers. This is particularly a problem because most of the symmetries in nonlinear systems are unknown
and hidden [17, 18]. In sharp contrast, the technique proposed in our work is both architecture-independent
and symmetry-agnostic and provably removes all known and unknown P-reflection symmetries in the loss.
4 Symmetry Impairs Model Capacity
We first show that reflection symmetry directly affects the model capacity. For simplicity, we let θ′ =0 for
all symmetries. Let f(x,θ) ∈ R be a Taylor-expandable model that contains a P-reflection symmetry. Let
∆=θ−θ . Then, closetoanysymmetricpointθ (anyθ forwhichPθ =0), forallx, Ref.[11]showedthat
0 0 0 0
1
f(x,θ)−f(x,θ )=∇ f(x,θ )(I−P)∆+O(∥P∆∥)2+ ∆TPH(x)P∆+O(∥∆∥3), (2)
0 θ 0
·„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„‚„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„¶ 2
·„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„‚„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„¶
Symmetrysubspace
Symmetry-brokensubspace
where H(x) is the Hessian matrix of f. An important feature is that the symmetry subspace is a generic
expansion where both odd and even terms are present, and the first order term does not vanish in general.
In contrast, in the symmetry-broken subspace, all odd-order terms in the expansion vanish, and the leading
order term is the second order. This implies that close to a symmetric solution, escaping from it will be
slow, and if at the symmetric solution, it is impossible for gradient descent to leave it. The effect of this
entrapment can be quantified by the following two propositions.
Proposition 2. (Symmetry removes feature.) Let f have the P-symmetry, and θ be intialized at θ such
0
that Pθ =0. Then, the kernalized model, g(x,θ)=lim (λ−1f(x,λθ+θ )−f(x,θ )), converges to
0 λ→0 0 0
θ∗=A+∑∇ f(x,θ )Ty(x) (3)
θ 0
x
under GD for a sufficiently small learning rate. Here A∶=(I−P)∑ ∇ f(x,θ )T∇ f(x,θ )(I−P) and A+
x θ 0 θ 0
denotes the Moore–Penrose inverse of A.
This means that in the kernel regime2, being at a symmet-
ric solution implies that the feature kernel features are being
masked by the projection matrix:
∇ f(x,θ )→(I−P)∇ f(x,θ ), (4)
θ 0 θ 0
and learning can only happen given these masks. This im-
plies that the model is not using the full feature space that is
available to it.
Proposition 3. (Symmetry reduces parameter dimension.)
Let f have the P-symmetry, and θ ∈ Rd be intialized at θ Figure 1: Training loss ℓ as a function of
0
such that Pθ = 0. Then, for all time steps t under GD or the iteration t for a fully connected network
0
SGD, there exists a model f′(x,θ′) and sequence of parameters onMNIST.Startingfromalow-capacitystate,
θ′ such that for all x, vanillaneuralnetworksaretrappedunderSGD
t
or Adam training (left). Blacker lines corre-
f′(x,θ′)=f(x,θ ), (5) spond to higher-capacity initializations, where
t t
more neurons are away from the permutation-
where dim(θ′)=d−rank(P). symmetric state. When the symmetries are
removed, the capacity of the initialization no
longer affects the solution found at the end of
2Technically,thisisthelazytraininglimit[19].
the training (right).
3The existence of this type of dimension reduction when symmetry is present has also been noticed by
previous works in case of permutation symmetry [1]. Intuitively, the above two results follow from the fact
that the symmetric subspaces of reflection symmetries (and any general discrete symmetries) are a linear
subspace, and so gradient descent (and thus gradient flow) cannot take the model away from it, despite the
discretization error. This means that, essentially, the model is identical to a model with a strictly smaller
dimension throughout its training, no matter whether the training is through GD or SGD. When there are
multiple symmetries, there is a compounding effect. An alternative perspective to this problem is through
the classical framework of singular learning theories, where each symmetry corresponds to a singularity and
directly affects the asymptotic performance of the model [20].
See Figure 1 for an illustration. We initialize a two-layer ReLU neural network on a low-capacity state
whereafractionofthehiddenneuronsareidentical(correspondingtothesymmetricstatesofthepermutation
symmetry [11]) and train with and without removing the symmetries. We see that when the symmetries
are removed (with the method proposed in the next section), the model is no longer stuck at these neuron-
collapsed solutions.
5 Removing Symmetry with Static Bias
Next, we prove that a simple algorithm that involves almost no modification to any deep learning training
pipeline can remove almost all such symmetries from the loss without creating new ones. From this section
onward, we will consider the case where the function under consideration is the loss function (a per-batch
loss or its expectation): f =ℓ.
5.1 Countable Symmetries
Weseekanalgorithmthateliminatesthereflectionsymmetriesfromthelossfunctionℓ. Weshowthatwhen
the number of reflection symmetries in the loss function is finite, one can completely remove them using
a simple technique. The symmetries are required to have the following property and the loss function is
assumed to obey assumption 1.
Property 1. (Enumeratability) There exists a countable set of pairs of projection matrices and biases
S ={(θ† ,P )}N such that ℓ(θ,x) has the θ† -centric P -reflection symmetry for all i. In addition, ℓ does not
i i i i i
have any (θ†,P) symmetry for (θ,P)∉S.
Assumption 1. There only exists countably many pairs (c ,θ˜) such that g(x) = ℓ(θ,x)−c θ contains a
0 0
θ˜-centric P symmetry, where we require Pc =c and Pθ˜=θ˜.
0 0
Thisassumptionissatisfiedbycommonneuralnetworkswithstandardactivations. Themainpurposeof
this assumption is to rule out the pathological of a linear or quadratic deterministic objective, which never
appears in practice or for which symmetry is not a concern.3
For symmetry removal, we propose to utilize the following alternative loss function. Let θ be drawn
0
from a Gaussian distribution with variance σ and ℓ be the original loss function:
0
ℓ (θ,x)=ℓ(θ+θ )+γ∥θ∥2. (6)
r 0
γ is nothing but the standard weight decay. We will see that using a static bias along with weight decay
is essential for the method to work. We find that with unit probability, the loss function ℓ contains no
r
reflection symmetry:
Theorem 1. Let ℓ satisfy Property 1 and Assumption 1. Then, with probability 1 (over the sampling of θ ),
0
there exists no projection matrix P and reflection point θ′ such that ℓ has the (θ′,P)-symmetry.
r
The core mechanism of this theorem is decoupling of the solutions of the symmetries from the solutions
of the weight decay. With weight decay, a solution with a small norm is favored, whereas with a random
bias, the symmetric solutions are shifted by a small constant and no longer overlap with solutions that have
3An example that violates this assumption is when ℓ(θ,x)=cTθ. ℓ(θ,x)−cTθ has infinitely many reflection symmetries
0 0
everywhereandforeverydatapointbecauseitisaconstantfunction.
4a small norm. Equivalently, this method can be implemented as a random bias in the L regularization:
2
ℓ (θ,x)=ℓ(θ)+γ∥θ+θ ∥2. This is a result of the fact that simple translation does not change the nature of
r 0
thefunction. Therearetworemarkablepartsofthetheorem: (1)itnotonlyremovesallexistingsymmetries
but also guarantees that there are no remaining ones; (2) the proof works as long as θ is nonzero. This
0
hints at the possibility of using a small and random θ , which removes all symmetries in principle and also
0
does not essentially affect the solutions of the original objective. In this work, we will refer to the method
in Eq. (6) as syre, an abbreviation for “symmetry removal.”
5.2 Uncountably Many Symmetries
Indeeplearning,itispossibleforthemodeltosimultaneouslycontaininfinitelymanyreflectionsymmetries.
This happens, for example, when the model parameters have the rotation symmetry or the double rotation
symmetry (common in self-supervised learning problems or transformers). It turns out that simply adding
a static bias and weight decay is not sufficient to remove all symmetries in this case.
We propose to train on the following alternative loss instead, where ar stands for “advanced removal”:
ℓ (θ)=ℓ(θ+θ )+γ∥θ∥2 , (7)
ar 0 D
where D is a positive diagonal matrix in which all diagonal elements of D are different. The simplest way
to achieve such a D is to set D ∼Uniform(1−ϵ,1+ϵ), where ϵ is a small quantity.
ii
Theorem 2. Any (θ′,P)-symmetry that ℓ satisfies obeys: (1) Pθ =θ′ (2) and PD=DP.
ar 0
Conditions(1)and(2)impliesthatthereareatmostfinitelymany(θ′,P)-symmetryℓ canhave. When
ar
there does not exist any symmetry that satisfies this condition, we have removed all the symmetries. In
the worst case where ℓ is a constant function, there are 2N symmetries where N is the number of reflection
symmetries. If we further assume that every P is associated with at most finitely many θ′, then we, again,
remove all symmetries with probability 1. The easiest way to determine this D matrix is through sampling
from a uniform distribution with a variance σ ≪1.
D
5.3 Strength of Symmetry Removal
While any level of θ and σ are sufficient to remove the symmetries, one might want to quantify the
0 D
degree to which the symmetries are broken. This is especially relevant when the model is located close to a
symmetric solution and requires a large gradient to escape from it. Also, a related question that may arise
in practice is how large one should choose σ and σ , which are the variances of θ and D. The following
0 D 0
theorem gives a quantitative characterization of the degree of symmetry breaking.4
Theorem 3. Let the original loss satisfy a (θ∗,P)-symmetry, where θ∗∈Rd. Then, for any local minimum
θ∈Θ(1), if σ =o(σ ) and Pθ≠0,
D 0
1
∆∶= [ℓ (θ+θ∗)−ℓ ((I−2P)θ+θ∗)]=Ω(γσ ). (8)
∥Pθ∥ ar ar 0
This theorem essentially shows a “super-Lipschitz” property of the difference between the loss function
values between parameters and their reflections. This means that with a random bias, the symmetry will
be quite strongly removed, as long as we ensure σ ≪ σ , which is certainly ensured when σ = 0. As
D 0 D
a corollary, it also shows that after applying a static bias, no symmetric solution where Pθ = 0 can still
be a stationary point because as Pθ → 0, the quantity ∆ converges to the projection of the gradient onto
symmetry breaking subspace.
Corollary 1. For any θ such that Pθ=0, P∇ ℓ =Ω(γσ ).
θ ar 0
Now, the more advanced question is the case when there are multiple reflection symmetries, and one
wants to significantly remove every one of them.
4Becauseθ0 andDarerandomlysampled,thebig-Onotationx∈O(z)isusedtomeanthatasthescalingparametertends
toinfinity,∃c0 suchthatPr(∥x∥<c0z)→1.
5Theorem 4. Let ℓ contain N reflection symmetries: {(P ,θ∗)}N . Let
i i i=1
ℓ (θ+θ∗)−ℓ ((I−2P )θ+θ∗)
∆ ∶= ar i ar i i . (9)
i ∥P θ∥
i
Then, for any local minimum θ∈Θ(1), letting γσ =Ω(2ϵN) guarantees that Pr(min ∣∆ ∣>ϵ)>δ for any ϵ
0 1−δ i i
and δ<1.
In the theorem, the probability is taken over the random sampling of the static bias. Namely, the
achievable strengths of symmetry-breaking scales inversely linearly in N, the size of the minimal set of the
entire group generated by N reflections. In general, without further assumptions there is no way to improve
this scaling because, for example, the smallest of N independent bounded variables roughly scales as 1/N
towards its lower boundary.
General Groups. Lastly, one can generalize the theory to prove that the proposed method removes
symmetries from a generic group. Let G be the linear representation of a generic finite group, possibly with
many nontrivial subgroups. If the loss function ℓ is invariant under transformation by the group G, then
∀g, ℓ(θ)=ℓ(gθ). (10)
Because G is finite, it follows that the representations g must be full-rank and unipotent.
The following theorem shows that at every symmetric solution, there exists an escape direction with a
strong gradient that pulls the parameters from every subgroup of the related symmetry. In other words, it
is no longer possible to be trapped in a symmetric solution. While general groups appear less common in
deep learning scenarios, they exist in a lot of application scenarios with equivariant networks [21], where it
is common to incorporate generic group structures into the model.
Letting U be a subgroup of G, we denote with the overbar the following matrix:
1
U = ∑ u. (11)
∣U∣
u∈U
NotethatU isaprojectionmatrix: UU =U. ThismeansthatI−U isalsoaprojectionmatrix. Importantly,
U projectsavectorintothesymmetricsubspace. Foranyu∈U,uU =U. Likewise,I−U projectsanyvector
into the symmetry-broken subspace, a well-known result in the theory of finite groups [22]. We denote by
∆ =∥(I−V)∇ ℓ∥ (12)
V θ
the strength of the symmetry removal for the subgroup V.
Theorem 5. Let Γ(G) denote the smallest minimal generating set for the group G. Z denotes the number
of minimal subgroups of G. Let ℓ be invariant under the group transformation G and let θ be in the invariant
subspace of a subgroup U ⊲G. Then, for every subgroup V ⊲U ⊲G,
1. ∆ =Ω(γσ rank(I−V));
V 0
2. min ∆ =Ω(γσ rank(I−V)Z−1);
V⊲U V 0
3. if G is abelian, min ∆ =Ω(γσ rank(I−V)∣Γ(U)∣−1);5
V⊲U V 0
4. additionally, for any ϵ>0 and δ<1, Pr(min ∆ >ϵ)>δ, if γσ =Ω(2ϵ∣Γ(U)∣).
V⊲U V 0 1−δ
Item (2) of the theorem is essentially due to the fact that removing symmetries from a larger group can
be reduced to removing them from one of its subgroups. Conceptually, this result has the same root as the
classical theory of combinatorial designs, where averaging over a subset of groups has the same effect as
averaging over the whole group [23]. In general, 1 ≤ Z ≤ ∣G∣ (and sometimes ≪ ∣G∣), and so this scaling is
not bad. For the part (3) of the theorem, the term ∣Γ(U)∣ is especially meaningful. It is well-known that
∣Γ(U)∣ ≤ log∣U∣, and so the worst-case symmetry-breaking strength is only of order 1/log∣U∣, which is far
slower than what one would expect. In fact, for a finite group with size N, the number of subgroups can
grow as fast as NlogN [24], and thus, one might naively think that the minimal breaking strength decreases
as N−logN. This theorem shows that the proposed method is highly effective at breaking the symmetries in
the loss function or the model.
5This result can be generalized to the case where all projectors V¯ commute with each other, even if G is nonabelian.
Essentially,thisresultisaconsequenceofthepropertiesoftherepresentationsofGandnotduetoGitself.
6A numerical example is shown in Figure 2, which validates a major
prediction of the theorem: a symmetry is easier to remove if it is high-
dimensional. Wetrainatwo-layerReLUnetinateacher-studentscenario
and change the input dimension. This experiment holds the number of
(permutation)symmetriesfixedanddirectlycontrolsrank(I−V). Asthe
input dimension increases, the symmetry of the learned model becomes
lower. In comparison, without a static bias, having a high dimension is
not so helpful.
Role of Noise. Acoreconceptinourtheoryisthatrandomnoise,even
Figure 2: With the proposed
if static, can help remove the symmetries. A related question is whether
method, the robustness of the fea-
simplyhavingnoiseintheinputorlabelissufficient,andsonostaticbias
ture increases with the input di-
isrequired. Theanswerisno. InAppendixB.1,wefindthataddinginput
mension. The same may happen
noise does help alleviate collapse, whereas adding label noise exacerbates under vanilla training, but the ef-
it. fect is not strong. The covariance
matrix is measured from the first-
layer output with batch size 1000,
5.4 Hyperparameter and Implementation Remark
and we set eigenvalues larger than
As discussed, there are two ways to implement the method (Eq. (6) or 10−4 to zero.
Eq. (7)). In our experiments, we stick to the definition of Eq. (6), where the model parameters are biased,
andweightdecayisthesameasthestandardimplementation. Forthechoiceofhyperparameters,wealways
setσ =0aswefindonlyintroducingσ tobesufficientformosttasks. Experimentswithstandardtraining
D 0
settings (see the next section for the Resnet18 experiment on CIFAR-10) show that choosi√ng σ
0
to be at
least an order of magnitude smaller than the standard initialization scale (usu√ally of order 1/ d for a width
of d) works the best. We thus recommend a default value of σ to be 0.01/ d, where 1/d is the common
0 √
−1
initialization variance. For the rest of the paper, we state σ in relative units of d for this reason. That
0
being said, we stress that σ is a hyperparameter worth tuning, as it directly controls the tradeoff between
0
optimization and symmetry removal.
6 Experiment
First, we show that the proposed method is compatible with standard training methods. We then apply
the method to a few settings where symmetry is known to be a major problem in training. We see that the
algorithm leads to improved model performance on these problems.
6.1 Compatibility with Standard Training
Ridgelinearregression. Letusfirstconsidertheclassicalproblemoflinearregressionwithd-dimensional
data, where one wants to find min ∑ (wTx −y )2. Here, the use of weight decay has a well-known effect
w i i i
of preventing the divergence of generalization loss at a critical dataset size N =d [25, 26]. This is due to the
fact that the Hessian matrix of the loss becomes singular exactly at N =d (at infinite N and d). The use of
weightdecayshiftsalltheeigenvaluesoftheHessianbyγ andremovesthissingularity. Inthiscase, onecan
show that the proposed method is essentially identical to the simple ridge regression. The ridge solution is
w∗=E[γI+A]−1E[xy], where A=E[xxT], and the solution to the biased model is
w∗=E[γI+A]−1(E[xy]+γθ ). (13)
0
The difference is negligible with the original solution if either γ and θ are small. See Figure 3-left.
0
Reparametrized Linear Regression. A minimal model with emergent interest in the problem of com-
pressing neural networks is the reparametrized version of linear regression [28], the loss function of which is
ℓ(u,w)=∥(u⊙w)Tx−y∥2, where we let u, w, x∈R200 and y∈R. Due to the rescaling symmetry between
every parameter u and w , the solutions where u =w =0 is a low-capacity state where the i-th neuron is
i i i i
“dead.” For this problem, we compare the training with standard SGD and syre. We also compare with a
7Figure3: Trainingwithsyreinstandardsettings. Theresultshowsthatbiasingthemodelsbyasmallstatic
bias does not change the performance of standard training settings. Left: Application of the method to a
linear regression problem. Here, α−1 =d/N is the degree of parameterization. A well-known use of weight
decay is to prevent double descent when α = 1. Here, we see that the proposed method works as well as
vanilla weight decay. Because there is no reflection symmetry in the problem, the proposed method should
approximatevanillaweightdecay. Mid: TestaccuracyofResnet18ontheCIFAR-10datasets. Theblueline
denotes the performance of Resnet, and the shadowed area denotes its standard deviation estimated over
10 trials. For σ <0.2, there is no significant difference between the performance of the vanilla Resnet and
0
syre Resnet. Right: linear regression with a redundant parametrization [27, 28]. The loss function takes
the form ℓ(u,w)=∥(u⊙w)Tx−y∥2. Due to symmetry, the point (u ,w )=0 is a low-capacity state where
i i
the i-th neuron is “dead”. Training with style, the model stayed away from any trapping low-capacity state
during training. In comparison, training with vanilla SGD or a heuristic for fixing the weights does not fix
the problem of collapsing to a low-capacity state.
Figure 4: The degree of symmetry versus the objective value for two choices of B and various training
methods with different hyperparameters. The proposed method is the only method to smoothly interpolate
between optimized solutions and solutions with low symmetry. syre performs well in both cases. Left:
objective with unstructured symmetry Right: structured symmetry.
heuristicmethod(W-fix),whereafractionϕ=0.3ofweightsofeverylayerisheldfixedwithafixedvariance
κ = 0.01 at initialization. This method has been suggested in [16] as a heuristic for removing symmetries
and is found to work well when there is permutation symmetry. We see that both the vanilla training and
the W-fix collapse to low-capacity states during training, whereas the proposed method stayed away from
them throughout. The reason is that the proposed method is model-independent and symmetry-agnostic,
working effectively for any type of possibly unknown symmetry in an arbitrary architecture.
ResNet. We also benchmark the performance of the proposed method for ResNet18 with different σ on
0
the CIFAR-10 datasets in Figure 3. When σ =0, the syre model is equivalent to the vanilla model. Figure
0
3 shows that the difference in performance between the vanilla Resnet and syre Res√net is very small and
becomes neglectable when σ <0.2. We thus recommend a default value of σ ≤0.01/ d.
0 0
8Figure5: Performanceofa4-layerFCNfordatasetswithvariousweightdecayγ anddatadistributionswith
varyings strengths of linear correlation α. As the theory predicts, the covariance of the vanilla model has a
low-rankstructureandperformssignificantlyworse. Inthemainfigures,solidlinesdenotetrainingaccuracy
and dashed lines denote test accuracy. The dashed black line corresponds to random guess. Subfigures
show the rank of the covariance matrix of the first layer output before (solid lines) and after (dashed lines)
activation with batch size 1000 scaled by α2. We set eigenvalues smaller than 10−4 to 0. Left: α = 1 and
different γ. Right: γ =0.01 and different α.
6.2 Benchmarking Symmetry Removal
In this section, we benchmark the effect of symmetry control of the proposed method for two controlled
experiments. To compare the influence of syre and other training methods on the degree of symmetry, we
consider minimizing the following objective function
d
(wTw)2−wTBw∶=(wTw)2−∑λ (vTw)2,
i i
i=1
Benchmarking where w ∈ Rd is the optimization parameter and B ∈ Rd×d is a given symmetric matrix
with eigenvalues λ and eigenvectors v (vTv = 1). The objective function has n reflection symmetries
i i i i
P w ∶= w−2(vTw)v . Hence, we define the degree of symmetry as ∑d 1{vTw < c }, where c is a given
i i i i=1 i th th
threshold. Depending on the spectrum of B, the nature of the task is different. We thus consider two types
of spectra: (1) an unstructured spectrum where B =G+GT for a Gaussian matrix G, and (2) a structured
spectrum where B = diag(v) where v is a random Gaussian vector. Conceptually, the first type is more
similar to rotation and double rotation symmetries in neural networks where the basis can be arbitrary,
while the second is a good model for common discrete symmetries where the basis is often diagonal or
sparse. For the first case we choose c =10−3 and for the second case we choose c =10−1.
th th
In Figure 4, we compare syre, W-fix, drop out, weight decay, and the standard training methods in this
setting for d=1000 and two choices of B. In both cases, we use Gaussian initialization and gradient descent
with a learning rate of 10−4. For syre and weight decay, we choose weight decay from 0.1 to 10. For W-fix,
we choose ϕ from 0.001 to 0.1. For dropout, we choose a dropout rate from 0.01 to 0.6. Figure 4 shows that
forbothcases,syre istheonlymethodthateffectivelyandsmoothlyinterpolatesbetweensolutionswithlow
symmetry and best optimization. This is a strong piece of evidence that the proposed method can control
the degree of symmetries in the model.
6.3 Feature and Neuron Collapses in Supervised Learning
See Figure 5, where we train the vanilla and syre four-layer networks with various levels of weight decay
γ and various levels of input-output covariance α. The dataset is constructed by rescaling the input by a
factor of α for the MNIST dataset. The theory predicts that the syre model can remove the permutation
symmetry in the hidden layer. This is supported by subfigures in Figure 5, where vanilla training results
in a low-rank solution. Meanwhile, the accuracy of the low-rank solution is significantly lower for a large
γ or a small α, which corresponds to the so-called neural collapses. Also, we observe that syre shifts the
eigenvalues of the representation by a magnitude proportional to σ , thus explaining the robustness of the
0
method against collapses in the latent representation (See Figure 11).
9Figure6: RankandreconstructionlossforaVAEistrainedontheFashionMNISTdataset. Thecovariance
of the vanilla model has a low-rank structure and larger reconstruction loss. More importantly, posterior
collapse happens at β = 5 but is mitigated with weight decay. Left: the rank of the encoder output with
batch size 1000. We set eigenvalues smaller than 10−6 to 0. Right: reconstruction loss of vanilla and syre
models.
Figure7: ExamplesofFashionMNIST reconstructionwith syre and β=10. Left: No weight decay. Right:
γ =1000. Clearly, the posterior collapse is mitigated by imposing syre with weight decay.
Hyperparameter Low-rankness Penult. Layer Acc. Last Layer Acc.
vanilla [34] - 70% 46.8% 22.2%
syre σ =0.1 0% 46.8% 31.7%
0
σ =0.01 0% 46.2% 32.5%
0
σ =0.1, all layers 0% 44.6% 30.7%
0
σ =0.01, all layers 0% 45.4% 32.4%
0
Table1: PerformanceofthelinearlyevaluatedResnet18onCIFAR100fortheunsupervisedself-constrastive
learning task. Here, the low-rankness measures the proportion of eigenvalues smaller than 10−5. Our
experiment indicates that symmetry-induced reduction in model capacity can explain about 50% of the
performance difference between the representation of the two layers.
6.4 Posterior Collapse in Bayesian learning
Ref.[29]pointsoutthatatypeofposteriorcollapseinBayesianlearning[30,31]iscausedbythelow-rankness
of the solutions. We show that training with syre could overcome this kind of posterior collapse. In Figure
6, we train a β-VAE [32, 33] on the Fashion MNIST dataset. Following Ref. [29], we use β to weigh the
KL loss, which can be regarded as the strength of prior matching. Both the encoder and the decoder are a
two-layer network with SiLU activation. The hidden dimension and the latent dimension are 200. Only the
encoder has weight decay because the low-rank problem is caused by the encoder rather than the decoder.
We also choose the prior variance of the latent variable to be η = 0.01. Other settings are the same as
enc
[29]. Posteriorcollapsehappensatβ=10,signalizedbyalargereconstructionlossintherightsideofFigure
6. However, the reconstruction loss decreases, and the rank of the encoder output increases (according to
the left side of Figure 6) after we use weight decay and syre. This is further verified by the generated image
in Figure 7. Therefore, we successfully remove the permutation symmetry of the encoder.
10Figure 9: Loss of plasticity in continual learning in an RL setting. We use the PPO algorithm [38] to solve the
Slippery-Ant problem [39]. The rank and the performance of the vanilla PPO decrease quickly, while the rank and
theperformanceofPPOwithsyre remainthesame,beyondthatofPPOwithweightdecay. Left: theeffectiverank
of the policy network as defined in [39]. Right: returns. Each trajectory is averaged over 5 different random seeds.
6.5 Low-Capacity Trap in Self-supervised Learning
Acommonbutbizarrepracticeinself-supervisedlearning(SSL)istothrowawaythelastlayerofthetrained
model and use the penultimate learning representation, which is found to have much better expressiveness
thanthelastlayerrepresentation. Fromtheperspectiveofsymmetry,thisproblemiscausedbytherotation
symmetry of the last weight matrix in the SimCLR loss [8]. We train a Resnet-18 together with a two-layer
projection head over the CIFAR-100 dataset according to the setting for training SimCLR in [34]. Then,
a linear classifier is trained using the learned representations. Our implementation reproduces the typical
accuracy of SimCLR over the CIFAR-100 dataset [35]. As in [34], the hidden layer before the projection
headisfoundtobeabetterrepresentationthanthelayerafter. Therefore, weapplyoursyre methodtothe
projection head or to all layers. According to Table 1, syre removes the low-rankness of the learned features
and increases the accuracy trained with the features after projection while not changing the representation
before projection. Thus, symmetry-induced reduction in model capacity can explain about 50% of the
performance difference between the representation of the two layers. Also, an interesting observation is that
just improving the expressivity of the last layer is insufficient to close the gap between the performance of
the last layer and the penultimate layer. This helps us gain a new insight: symmetry is not the only reason
why the last layer representation is defective.
6.6 Loss of Plasticity in Continual Learning
A form of low-capacity collapse also happens during continual
learning, i.e., theplasticityofthenetworkgraduallydecreasesas
the model is trained on more and more tasks. This problem is
common in both supervised and reinforcement learning settings
andmayalsoberelevanttothefinetuningoflargelanguagemod-
els [40, 41, 42, 43].
In Figure 8, we train a CNN with two convolution layers (10
channels and 20 channels) and two fully connected layers (320
Figure 8: Performance and accuracy of a
units and 50 units) over the MNIST datasets. For the data, we
CNN trained on a continual learning task
randomly permute the pixels of the training and test sets for 9
(permutedMNIST[36,37]). Themainfigure
times,forming10differenttasks(includingtheoriginalMNIST).
shows the test accuracy, and the subfigure
We then train a vanilla CNN and a syre CNN over the 10 tasks
showstherankoftheconvolutionlayersout-
continuallywithSGDandweightdecay0.01. TheinsetofFigure putwithbatchsize1000,whereweseteigen-
8 shows that the rank of the original model gradually decreases, values smaller than 10−4 to 0. The results
but the syre model remains close to full rank. Correspondingly, suggest that the rank of the vanilla model
in the right side of Figure 8, the accuracy over the test set drops gradually decreases, and the model com-
while the rank of the original model collapses, but the accuracy pletely collapses after the sixth task, while
of the syre model remains similar. the syre model remains unaffected.
11Reinforcement Learning. In Figure 9, we use the PPO algorithm for the Slippery-Ant problem [39], a
continual variant of the Pybullet’s Ant problem [44] with friction that changes every 5M steps. Hyperpa-
rameters for the PPO algorithm are borrowed from [39], and we use a weight decay of 0.002 for both PPO
with weight decay and with syre. Figure 9 suggests that syre is effective in maintaining the rank of the
model during continual training and obtains better performance than pure weight decay.
7 Conclusion
Symmetry-induced neural-network training problems exist extensively in machine learning. We have shown
thattheexistenceofsymmetriesinthemodelorlossfunctionmayseverelylimittheexpressivityofthetrained
model. We then developed a theory that leverages the power of representation theory to show that adding
random static biases to the model, along with weight decay, is sufficient to remove almost all symmetries,
explicitorhidden. Wehavedemonstratedtherelevanceofthemethodtoabroadrangeofapplicationsindeep
learning, and a possible future direction is to deploy the method in large language models, which naturally
containmanysymmetries. Lastly, itisworthnotingthatonitsown, symmetryisneithergoodnorbad. For
example, practitioners may be interested in introducing symmetries to the model architecture in order to
control the capacity of the model [28]. However, with too much symmetry, the training of models becomes
slow and likely to contain many low-capacity traps. Meanwhile, a model completely without symmetry may
have undesirably high capacity and be more prone to overfitting. Having the right degree of symmetry
might thus be crucial for achieving both smooth optimization and good generalization. With our proposed
method, it becomes increasingly possible to deliberately fine-grain engineer symmetries in the loss function,
introducing desired symmetries and removing undesirable ones.
Acknowledgements
The authors acknowledge support from NTT Research, and from the Institute for Artificial Intelligence and
Fundamental Interactions (IAIFI) through NSF Grant No. PHY-2019786.
References
[1] Berfin Simsek, Franc¸ois Ged, Arthur Jacot, Francesco Spadaro, Cl´ement Hongler, Wulfram Gerstner,
and Johanni Brea. Geometry of the loss landscape in overparameterized neural networks: Symmetries
and invariances. In International Conference on Machine Learning, pages 9722–9732. PMLR, 2021.
[2] RahimEntezari,HanieSedghi,OlgaSaukh,andBehnamNeyshabur.Theroleofpermutationinvariance
in linear mode connectivity of neural networks. arXiv preprint arXiv:2110.06296, 2021.
[3] L. Dinh, R. Pascanu, S. Bengio, and Y. Bengio. Sharp Minima Can Generalize For Deep Nets. ArXiv
e-prints, March 2017.
[4] Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics
of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120, 2013.
[5] Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On the
role of implicit regularization in deep learning. arXiv preprint arXiv:1412.6614, 2014.
[6] Ryan J Tibshirani. Equivalences between sparse models and neural networks. Working Notes. URL
https://www. stat. cmu. edu/˜ ryantibs/papers/sparsitynn. pdf, 2021.
[7] SergeyIoffeandChristianSzegedy. Batchnormalization: Acceleratingdeepnetworktrainingbyreduc-
ing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
[8] LiuZiyin,EkdeepSinghLubana,MasahitoUeda,andHidenoriTanaka. Whatshapesthelosslandscape
of self supervised learning? In The Eleventh International Conference on Learning Representations,
2023.
12[9] Bo Zhao, Robert M Gower, Robin Walters, and Rose Yu. Improving convergence and generalization
using parameter symmetries. arXiv preprint arXiv:2305.13404, 2023.
[10] Charles Godfrey, Davis Brown, Tegan Emerson, and Henry Kvinge. On the symmetries of deep learn-
ing models and their internal representations. Advances in Neural Information Processing Systems,
35:11893–11905, 2022.
[11] LiuZiyin. Symmetryleadstostructuredconstraintoflearning. arXiv preprint arXiv:2309.16932,2023.
[12] Kenji Fukumizu. A regularity condition of the information matrix of a multilayer perceptron network.
Neural networks, 9(5):871–879, 1996.
[13] XingguoLi,JunweiLu,RamanArora,JarvisHaupt,HanLiu,ZhaoranWang,andTuoZhao.Symmetry,
saddlepoints,andglobaloptimizationlandscapeofnonconvexmatrixfactorization. IEEE Transactions
on Information Theory, 65(6):3489–3514, 2019.
[14] Liu Ziyin, Botao Li, Tomer Galanti, and Masahito Ueda. The probabilistic stability of stochastic
gradient descent, 2023.
[15] Feng Chen, Daniel Kunin, Atsushi Yamamura, and Surya Ganguli. Stochastic collapse: How gradient
noise attracts sgd dynamics towards simpler subnetworks. arXiv preprint arXiv:2306.04251, 2023.
[16] DerekLim,MoePutterman,RobinWalters,HaggaiMaron,andStefanieJegelka. Theempiricalimpact
of neural parameter symmetries, or lack thereof. arXiv preprint arXiv:2405.20231, 2024.
[17] Marco Cariglia. Hidden symmetries of dynamics in classical and quantum physics. Reviews of Modern
Physics, 86(4):1283, 2014.
[18] Valeri P Frolov, Pavel Krtouˇs, and David Kubiznˇ´ak. Black holes, hidden symmetries, and complete
integrability. Living reviews in relativity, 20:1–221, 2017.
[19] Lenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable programming.
arXiv preprint arXiv:1812.07956, 2018.
[20] Sumio Watanabe and Manfred Opper. Asymptotic equivalence of bayes cross validation and widely
applicable information criterion in singular learning theory. Journal of machine learning research,
11(12), 2010.
[21] Taco Cohen and Max Welling. Group equivariant convolutional networks. In International conference
on machine learning, pages 2990–2999. PMLR, 2016.
[22] Daniel Gorenstein. Finite groups, volume 301. American Mathematical Soc., 2007.
[23] Charles C Lindner and Christopher A Rodger. Design theory. Chapman and Hall/CRC, 2017.
[24] Alexandre Borovik, Laszlo Pyber, and Aner Shalev. Maximal subgroups in finite and profinite groups.
Transactions of the American Mathematical Society, 348(9):3745–3761, 1996.
[25] Anders Krogh and John A Hertz. A simple weight decay can improve generalization. In Advances in
neural information processing systems, pages 950–957, 1992.
[26] TrevorHastie,AndreaMontanari,SaharonRosset,andRyanJTibshirani.Surprisesinhigh-dimensional
ridgeless least squares interpolation. arXiv preprint arXiv:1903.08560, 2019.
[27] Clarice Poon and Gabriel Peyr´e. Smooth bilevel programming for sparse regularization. Advances in
Neural Information Processing Systems, 34:1543–1555, 2021.
[28] Liu Ziyin and Zihao Wang. spred: Solving L1 Penalty with SGD. In International Conference on
Machine Learning, 2023.
[29] Zihao Wang and Liu Ziyin. Posterior collapse of a linear latent variable model. Advances in Neural
Information Processing Systems, 35:37537–37548, 2022.
13[30] James Lucas, George Tucker, Roger Grosse, and Mohammad Norouzi. Don’t blame the elbo! a linear
vae perspective on posterior collapse, 2019.
[31] Yixin Wang, David Blei, and John P Cunningham. Posterior collapse and latent variable non-
identifiability. Advances in neural information processing systems, 34:5443–5455, 2021.
[32] DiederikPKingmaandMaxWelling. Auto-encodingvariationalbayes. arXivpreprintarXiv:1312.6114,
2013.
[33] Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir
Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a constrained vari-
ational framework. 2016.
[34] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. In International conference on machine learning, pages
1597–1607. PMLR, 2020.
[35] Massimiliano Patacchiola and Amos J Storkey. Self-supervised relational reasoning for representation
learning. Advances in Neural Information Processing Systems, 33:4003–4014, 2020.
[36] Ian J Goodfellow, Mehdi Mirza, Da Xiao, Aaron Courville, and Yoshua Bengio. An empirical inves-
tigation of catastrophic forgetting in gradient-based neural networks. arXiv preprint arXiv:1312.6211,
2013.
[37] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A
Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcom-
ing catastrophic forgetting in neural networks. Proceedings of the national academy of sciences,
114(13):3521–3526, 2017.
[38] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
[39] Shibhansh Dohare, J Fernando Hernandez-Garcia, Parash Rahman, Richard S Sutton, and A Rupam
Mahmood. Maintaining plasticity in deep continual learning. arXiv preprint arXiv:2306.13812, 2023.
[40] MichaelMcCloskeyandNealJCohen. Catastrophicinterferenceinconnectionistnetworks: Thesequen-
tial learning problem. In Psychology of learning and motivation, volume 24, pages 109–165. Elsevier,
1989.
[41] Jordan Ash and Ryan P Adams. On warm-starting neural network training. Advances in neural
information processing systems, 33:3884–3894, 2020.
[42] Shibhansh Dohare, Richard S Sutton, and A Rupam Mahmood. Continual backprop: Stochastic gradi-
ent descent with persistent randomness. arXiv preprint arXiv:2108.06325, 2021.
[43] Zaheer Abbas, Rosie Zhao, Joseph Modayil, Adam White, and Marlos C Machado. Loss of plasticity
in continual deep reinforcement learning. arXiv preprint arXiv:2303.07507, 2023.
[44] Erwin Coumans and Yunfei Bai. Pybullet, a python module for physics simulation for games, robotics
and machine learning, 2016.
14A Theoretical Concerns
A.1 Proof of Proposition 1
Proof. Part (1). Note that we have
θ†=Pθ†+(I−P)θ†=θ′+(I−P)θ†
. (14)
Thus,
(I−2P)(I−P)θ†=(I−P)θ†
. (15)
Therefore, we have
f(θ+θ†)=f((θ+(I−P)θ†)+θ′)=f((I−2P)(θ+(I−P)θ†)+θ′)=f((I−2P)θ+θ†).
(16)
This proves part (1).
Part (2). By definition,
f′(θ−θ†+θ′)=f(θ+θ′)
(17)
=f((I−2P)θ+θ′) (18)
=f′((I−2P)θ−θ†+θ′).
(19)
This completes the proof.
A.2 Proof of Proposition 2
Proof. By (2), g(x,θ) simplifies to a kernel model
g(x,θ)=∇ f(x,θ )(I−P)θ. (20)
θ0 0
Letusconsiderthesquaredlossℓ(θ)=∑ ∣∣y(x)−g(x,θ)∣∣2anddenoteA∶=∑ (I−P)∇ f(x,θ )T∇ f(x,θ )(I−
x x θ0 0 θ0 0
P), b∶=(I−P)∑ ∇ f(x,θ )Ty(x). Assuming the learning rate to be η, the GD reads
x θ0 0
θt+1=θt−2η(Aθt−b), (21)
where θ0=0. If
1
η< , (22)
2λ (A)
max
GD converges to
t
θ∗= lim ∑(I−2ηA)k∗2ηb
t→∞ (23)
k=0
=A+b,
which is the well-known least square solution.
A.3 Proof of Proposition 3
Proof. According to Ref. [11, Theorem 4], we have
P∇ ℓ(x,θ )=0. (24)
θ 0
Therefore, after one step of GD or SGD, we still have Pθ =0. By induction, we have Pθ =0.
1 t
Finally, suppose that {a }d−rank(P) forms a basis of kerP, and define f′(x,θ′)∶=f(x,∑d−rank(P)θ′a ) for
i i=1 i=1 i i
dim(θ′)=d−rank(P). By choosing θ′ =θTa , we have f′(x,θ′)=f(x,θ ).
i i t t
15A.4 Lemmas
Lemma 1. Let x∈Rd and P be a projection matrix. Let f(x) be a scalar function that satisfies
f(x+x′)=f((I−2P)x+x′)+cTPx, (25)
where c is a constant vector. Then, there exists a unique function g(x) such that
1. g(x) has the x′-centric P-symmetry,
2. and f(x)=g(x)+ 1cTPx.
2 0
Proof. (a) Existence. f(x)=g(x)+ 1cTx. Let us suppose g(x) is not x′-centric P-symmetry. Then, there
2 0
exists x such that
g(x+x′)−g((I−2P)x+x′)=∆≠0. (26)
Then, by definition, we have that
cTPx=f(x+x′)+f((I−2P)x+x′) (27)
0
1
=g(x+x′)− cTP(x+x′)−g((I−2P)x+x′)−cTP(x+x′) (28)
2 0 0
1 1
=∆+ cTP(x+x′)− cTP((I−2P)x+x′) (29)
2 0 2 0
=∆+cTPx. (30)
0
This is a contradiction. Therefore, there must exist g(x) that satisfies the lemma statement.
(b) Uniqueness. Simply note that the expression of g is uniquely given by6
g(x)=f(x+x′)−f((I−2P)x+x′). (31)
A.5 Proof of Theorem 1
Proof. Weprovebycontradiction. Letussupposethereexistssuchpair,(θ′,P). Bydefinition,wehavethat
ℓ (θ+θ′)=ℓ(θ+θ′+θ )+γ∥θ+θ′∥2. (32)
r 0
By assumption, we have that
ℓ((I−2P)θ+θ′+θ )+γ∥(I−2P)θ+θ′∥2=ℓ(θ+θ′+θ )+γ∥θ+θ′∥2, (33)
0 0
and, so, for all θ,
ℓ((I−2P)θ+θ′+θ )=ℓ(θ+θ′+θ )+4γθTPθ′. (34)
0 0
There are two cases: (1) Pθ′=0 and (2) Pθ′≠0.
For case (1), we have that ℓ((I−2P)θ+θ′+θ )=ℓ(θ+θ′+θ ), but this can only happen if the original
0 0
loss ℓ has the (θ′+θ )-centric P-symmetry. By Property 1, this implies that
0
θ′+θ =θ† (35)
0 i
for some i. Applying P on both sides, we obtain that
Pθ
=Pθ†
. (36)
0 i
But, θ is a random variable with a full-rank covariance while the set
{Pθ†}
has measure zero in the real
0 i
space, and so this equality holds with probability zero.
6Alternatively, note that cTx is odd and that g(x) can be shifted by a constant to be an even function. The uniqueness
followsdirectlyfromthefactthateveryfunctioncanbeuniquelyfactorizedintoanoddfunctionandanevenfunction.
16For case (2), it follows from Lemma 1 that for a fixed x and θ , ℓ(θ) can be uniquely decomposed in the
0
following form
ℓ(θ)=g(θ)−2γθTPθ′, (37)
where g(θ) has the θ′+θ -centric P-symmetry.
0
Letc =2γPθ′andθ˜=P(θ′+θ ). Thenℓ(θ)+c θhastheθ˜-centricP symmetry. Wealsohaveθ˜−c0 =Pθ .
0 0 0 2γ 0
According to Assumption 1, there are only countable many such {c ,θ˜} pairs. However, Pθ is a standard
0 0
Gaussian random variable, which leads to a contradiction with probability 1.
Remark. It is easy to see that Assumption 1 could be slightly relaxed. We only require {θ˜− c0} to have a
2γ
zero measure, which is also a necessary and sufficient condition.
A.6 Proof of Theorem 2
Proof. Weprovebycontradiction. Letussupposethereexistssuchpair,(θ′,P). Bydefinition,wehavethat
ℓ (θ+θ′)=ℓ(θ+θ′+θ )+γ∥θ+θ′∥2 . (38)
ar 0 D
By assumption, we have that
ℓ((I−2P)θ+θ′+θ )+γ∥(I−2P)θ+θ′∥2 =ℓ(θ+θ′+θ )+γ∥θ+θ′∥2 , (39)
0 D 0 D
and, so, for all θ,
ℓ((I−2P)θ+θ′+θ )=ℓ(θ+θ′+θ )+4θTPD((I−P)θ+θ′). (40)
0 0
There are two cases: (1) PD((I−P)θ+θ′)=0 and (2) PD((I−P)θ+θ′)≠0.
Like before, there are two cases. For case (2), the proof is identical, and so we omit it. For case (1), it
must be the case that for some P, and θ′
ℓ((I−2P)θ+θ′+θ )=ℓ(θ+θ′+θ ). (41)
0 0
This is possible if and only if P(θ′+θ )=θ† for some i and P =P for the corresponding projection matrix.
0 i i
However, because Pθ′=0, this requires that
P(θ′+θ )=θ† . (42)
0 i
By the definition of the reflection symmetry, we have that
Pθ′=θ′. (43)
This means that
θ′=θ†−Pθ
. (44)
i 0
At the same time, we have
PD((I−P)θ+θ′)=0, (45)
which implies that
PD(I−P)θ=−PDθ′. (46)
Because the right hand side is a constant that only depends on θ. This can only happen if both sides are
zero, which is achieved if:
PD(I−P)=0, (47)
and
θ†=Pθ
. (48)
i 0
The first condition implies that
PD=PDP =DTPT =DP, (49)
which implies that P and D must share the eigenvectors because they commute. Noting that
Pθ†=θ†
, (50)
i i
† †
we obtain that θ is an eigenvector of P and so θ is an eigenvector of D, but D is diagonal and with
i i
†
nonidentical diagonal entries, θ much then be a one-hot vector, and P must also be diagonal and consists
i
of values of 1 and 0 in the diagonal entries.
17A.7 Proof of Theorem 3
Proof. First of all, note that shifting parameters of ℓ is the same as shifting the parameters of the weight
decay. Therefore, for any local minimum θ′,
ℓ (θ′)=ℓ(θ)+γ∥θ−θ ∥2, (51)
ar 0
where θ=θ′+θ .
0
Therefore,
ℓ (θ′+θ∗)−ℓ ((I−2P)θ′+θ∗) (52)
ar ar
=ℓ(θ+θ∗)+γ∥θ+θ∗−θ ∥2 −ℓ((I−2P)θ+θ∗)−γ∥(I−2P)θ+θ∗−θ ∥2 (53)
0 D 0 D
=γ∥θ+θ∗−θ ∥2 −γ∥(I−2P)θ+θ∗−θ ∥2 (54)
0 D 0 D
=γ(zT(D−I)z +zTD(θ∗−θ )), (55)
⊥ ∥ ⊥ 0
wherewehaveusedthedefinitionofreflectionsymmetryinthethirdline. Inthefourthline,wehavedefined
z =Pθ and z =(I−P)θ. Thus,
⊥ ∥
∥θ∥2=∥z ∥2+∥z ∥2=Θ(1). (56)
⊥ ∥
Thus, we have that
Θ(ℓ (θ′+θ∗)−ℓ ((I−2P)θ′+θ∗))=γΘ(zT(D−I)z )+Θ(zTD(θ∗−θ )) (57)
ar ar ⊥ ∥ ⊥ 0
=γΘ(σ ∥z ∥)+Ω(σ ∥z ∥), (58)
D ⊥ 0 ⊥
where we have used the fact that each element of θ∗−θ is Ω(σ ) because θ∗ is an arbitrary constant and
0 0
θ ∼N(0,σ2). By the assumption that σ =o(σ ), we obtain the desired relation
0 D 0
ℓ (θ′+θ∗)−ℓ ((I−2P)θ′+θ∗)=Ω(γσ )∥z ∥. (59)
ar ar 0 ⊥
This finishes the proof.
A.8 Proof of Theorem 4
Proof. First of all,
Pr(min∣∆ ∣>ϵ)=Pr(∣∆ ∣>ϵ∧...∧∣∆ ∣>ϵ) (60)
i 1 N
i
N
≥max(0,∑Pr(∣∆ ∣>ϵ)−N +1), (61)
i
i
where we have applied the Frechet inequality in the second line.
The sum ∑NPr(∣∆ ∣>ϵ) can also be lower bounded if each ∆ is a Gaussian variable with variance σ :
i i i i
N N 2ϵ
∑Pr(∣∆ ∣>ϵ)≥∑(1− √ ) (62)
i
2πσ2
i i i
2ϵN
≥N − √ . (63)
min 2πσ2
i i
Thus, for Pr(min ∣∆ ∣>ϵ) to be larger than 1−δ, we must have
i i
2ϵN
minσ ≥ √ . (64)
i i 2π(1−δ)
Now, we show that ∆ -s are indeed Gaussian variables. From the previous proof, it is easy to see that for a
i
unit vector n ,
i
∆ =γnT(θ∗−θ )+o(γσ ). (65)
i i 0 0
Therefore, ∆ is a Gaussian variable with standard deviation γσ . Thus, min σ = γσ . Thus, we have
i 0 i i 0
obtained the desired result
2ϵN
γσ ≥ √ . (66)
0 2π(1−δ)
18A.9 Proof of Theorem 5
Before we start proving Theorem 5, we introduce a definition that facilitates the proof.
Definition 2. (Symmetryreduction.) WesaythatremovingasymmetryfromgroupG reducestoremoving
1
the symmetry due to group G if for any vector n,
2
∥(I−G )n∥≤∥(I−G )n∥. (67)
2 1
Now, we are ready to prove the main theorem.
Proof. We first show (I−V)T∇ ℓ(θ)=0. For any g∈V and z∈R, we have
θ
ℓ(θ+zn)=ℓ(g(θ+zn)), (68)
where n is an arbitrary unit vector. Taking the derivative with respect to z, and recalling that gθ = θ, we
have
(gn)T∇ ℓ(θ)=nT∇ ℓ(θ). (69)
θ θ
Accordingly, we have
1
(Vn)T∇ ℓ(θ)∶= ∑(gn)T∇ ℓ(θ)=nT∇ ℓ(θ). (70)
θ ∣V∣ θ θ
g∈V
Due to the arbitrary choice of n, we have (I−V)T∇ ℓ(θ)=0.
θ
Therefore,
(I−V)T∇ ℓ (θ)=γ(I−V)T∇ ∥θ−θ ∥2 (71)
θ ar θ 0 D
=2γ(I−V)TD(θ−θ ) (72)
0
=2γ(I−V)Tθ +o(γσ (1+σ )). (73)
0 0 D
As θ
0
is a Gaussian vector with mean 0 and variance σ 02, ∣∣(I−V)Tθ 0∣∣ is a Gau√ssian variable with mean 0
and variance ∣∣I−V∣∣2σ2=Ω(rank(I−V)σ2), which gives ∥(I−V)∇ ℓ∥=Ω(γσ rank(I−V)).
0 0 θ 0
Now, we prove part (2) of the theorem. Note that if V ⊲U and if θ ∈kerV, then θ ∈kerU. This means
0 0
that for any group U such that V ⊲U and any vector θ
0
∥(I−V)θ ∥<∥(I−U)θ ∥. (74)
0 0
This means that to remove the symmetry from a large group U, it suffices to remove the symmetry from
one of its minimal subgroups. Thus, let M denote the set of minimal subgroups of the group G, we have
G
min∥(I−V)θ ∥≥ min ∥(I−V)θ ∥. (75)
0 0
V⊲G V⊲MG
Thenumberofminimalsubgroupsisstrictlyupperboundedbythenumberofelementsofthegroupbecause
allminimalsubgroupsareonlytriviallyintersecteachother. Thisfollowsfromthefactthattheintersection
ofgroupsmustbeasubgroup,whichcanonlybetheidentityfortwodifferentminimalsubgroups. Therefore,
the number of minimal subgroups cannot exceed the number of elements of the group. This finishes the
second part of the theorem.
For the third part, we show that the symmetry broken subspace of any subgroup contains the symmetry
brokensubspaceofagroupgeneratedbyoneofthegeneratorsandsoitsufficestoonlyremovethesymmetries
due to the subgroups generated by each generator. Let us introduce the following notation for a matrix
representation z of a group element:
1 ord(z)
z= ∑ zi, (76)
ord(z)
i=1
where ord(z) denotes the order of z. This is equivalent to the symmetry projection matrix of the subgroup
generated by z.
19Now, let G be abelian. Then, both U and V are abelian. Let us denote by Γ(U) = {z } the mininal
i
generating set of U. Suppose that for all n≠0 such that n∈im(I−V), we must have n∉im(I−z ) for all
j
j. This means that
n∉⋃im(I−z ). (77)
j
j
Or, equivalently,
n∈⋂im(z ). (78)
j
j
However, the space ⋂ im(z ) ⊆ im(V) because V is a subgroup of U, which is generated by z ,⋯,z . To
j j 1 m
see this, let n∈im(z ) for all j, then,
j
z n=n (79)
j
for all j. Now, let v=∏ zdi(v)∈V, we have
i i
(I−V)n=(I−∑∏zdi(v))n=0 (80)
i
v i
Thismeansthatnisinbothim(V)andim(I−V),whichispossibleonlyifn=0–acontradiction. Therefore,
as long as I −V¯ is not rank 0, it must share a common subspace with one of the I −z , and so removing
j
the symmetry from any subgroup V of U can be reduced to removing the symmetry from the cyclic group
generated by one of its generators from the minimal generating set.7
Therefore, we have proved that removing symmetries due to any subgroup of U can be reduced to
removingthesymmetryfroma(properortrivial)subgroupofeachofthecyclicdecompositionsofthegroup
U, each of which is generated by a minimal generator of U. By the fundamental theorem of finite abelian
groups, each of these groups is of order pk for some prime number p. Because each of these groups is cyclic,
it contains exactly k nontrivial subgroups. Taken together, this means that if ∣U∣=pk1...pkm, we only have
1 m
to remove symmetries from at most
m
∑k =log∣U∣ (81)
i
i
many subgroups. This completes part (3).
Forpart(4),wedenote∆ ∶=(I−V )T∇ ℓ (θ)fori=1,⋯,∣Γ(U)∣. Accordingto(73),∆ isapproximately
i i θ ar i
a Gaussian variable with zero mean and variance γ2rank(I−V)σ2. Therefore,
0
∣Γ(U)∣ 2ϵ∣Γ(U)∣
∑ Pr(∣∆ ∣>ϵ)≥∣Γ(U)∣− √ . (82)
i
i min 2πγ2rank(I−V)σ2
i 0
For Pr(min ∣∆ ∣>ϵ) to be larger than 1−δ, we must have
i i
2ϵ∣Γ(U)∣
γσ ≥ √ . (83)
0
2π rank(I−V)(1−δ)
7ThisholdstrueevenifV isasubsetof⟨zj⟩.
20Figure 10: For the same setting as Figure 2, the rank of the covariance matrix decreases with the label noise and
increases with the input noise. We set eigenvalues smaller than 10−3 to zero.
Figure 11: The spectrum of the covariance matrix of the vanilla model (Left) and the syre model (Right) for
γ=0.01 and α=1. Clearly, the vanilla model learns a low-rank solution.
B Additional Experiments and Experimental Detail
B.1 Teacher-student Scenario
This section gives some additional details and additional experiments in the teacher-student scenario in
Figure 2. Specifically, we implement a two-layer network with tanh activation, 300 hidden units, and
different input units. The network outputs a ten-dimensional vector corresponding to ten different classes.
We then randomly generate such a network as the teacher model, 10000 standard Gaussian samples as the
training set, and 1000 standard Gaussian samples. For both the syre and the vanilla model, we choose the
Adam optimizer, learning rate 0.01, and weight decay 0.01.
As additional experiments, we also measure the influence of noisy labels and noisy input on the rank
of the model in Figure 10. For the label noise, we randomly change 0% to 80% of the labels, and for the
input noise, we add a Gaussian noise to the input with standard deviation 0 to 1.6. Figure 10 suggests that
the rank of the vanilla model decreases in the face of noisy labels and increases in the face of noisy input,
perhaps because the latter can be regarded as data augmentation. The syre model, however, is not affected.
B.2 Supervised Learning
ThissectionpresentssomeadditionalexperimentsforSection6.3. Figure11givestheeigenvaluedistribution
of the networks in Fig.5, which further supports the claim that the vanilla network leads to a low-rank
solution. In all the experiments above and in Section 6.3, we use a four-layer FCN with 300 neurons in each
layer trained on the MNIST dataset with batch size 64.
21