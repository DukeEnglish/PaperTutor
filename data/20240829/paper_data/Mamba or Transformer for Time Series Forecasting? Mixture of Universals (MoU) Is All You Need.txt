Mamba or Transformer for Time Series Forecasting?
Mixture of Universals (MoU) Is All You Need
SijiaPeng1,YunXiong1*,YangyongZhu1,ZhiqiangShen2
1ShanghaiKeyLaboratoryofDataScience,SchoolofComputerScience,FudanUniversity
2MohamedbinZayedUniversityofArtificialIntelligence
sjpeng21@m.fudan.edu.cn,{yunx,yyzhu}@fudan.edu.cn,zhiqiang.shen@mbzuai.ac.ae
Abstract
S-Mamba
Time series forecasting requires balancing short-term and 39MB, 20.24s 50MB100MB300MB 600MB
long-term dependencies for accurate predictions. Existing DLinear,
33MB, 8.93s
methods mainly focus on long-term dependency modeling,
neglecting the complexities of short-term dynamics, which Memory Footprint
mayhinderperformance.Transformersaresuperiorinmod-
eling long-term dependencies but are criticized for their
PatchTST
quadraticcomputationalcost.Mambaprovidesanear-linear 408MB, 37.64s
alternative but is reported less effective in time series long-
ModernTCN
term forecasting due to potential information loss. Current MoU (Ours), 611MB, 105.62s
architectures fall short in offering both high efficiency and 372MB, 30.48s
strongperformanceforlong-termdependencymodeling.To
address these challenges, we introduce Mixture of Univer-
sals(MoU),aversatilemodeltocapturebothshort-termand Figure 1: Model efficiency comparison. The results are on
long-term dependencies for enhancing performance in time ETTm2withforecastinglengthof720byaunifiedtesting.
series forecasting. MoU is composed of two novel designs:
Mixture of Feature Extractors (MoF), an adaptive method
designed to improve time series patch representations for multi-scaledpatchingstrategytosummarizeshort-termde-
short-termdependency,andMixtureofArchitectures(MoA), pendencyinvariousresolutions.Althoughtheseapproaches
which hierarchically integrates Mamba, FeedForward, Con- enhanceshort-terminformationbygeneratingpatchtokens,
volution,andSelf-Attentionarchitecturesinaspecializedor- they still rely on uniform linear transformations for patch
der to model long-term dependency from a hybrid perspec- embedding. This approach neglects the divergence in fea-
tive.Theproposedapproachachievesstate-of-the-artperfor-
tureaffinitywithindifferentpatches,whicharisesfromtheir
mancewhilemaintainingrelativelylowcomputationalcosts.
varyingsemanticcontexts.Asaresult,importantcontextual
Extensiveexperimentsonsevenreal-worlddatasetsdemon-
informationmaybelost,limitingtheaccuraterepresentation
strate the superiority of MoU. Code is available at https:
ofshort-termdetails.
//github.com/lunaaa95/mou/.
Thismotivatesustoexploreadaptivemethodsthattailor
the embedding process for different patch tokens, generat-
1 Introduction ingmoreinformativerepresentations.Suchapproacheshave
shown success in computer vision (Chen et al. 2020; Yang
Time series forecasting is crucial in various fields, such
etal.2019).Forinstance,DynamicConvolution(Chenetal.
as climate prediction (Murat et al. 2018; Scher 2020; Haq
2020)andConditionalConvolution(Yangetal.2019)adapt
2022; Neumann et al. 2024), financial investment (Sezer,
convolution kernel parameters for different input patches.
Gudelek, and Ozbayoglu 2020; Liu et al. 2023; Bie-
However, we find that these methods perform poorly for
ganowskiandSlepaczuk2024),andhouseholdpowerman-
timeseriespatches,evenworsethanuniformlineartransfor-
agement(Bilaletal.2022;Kim,Park,andKim2023;Cas-
mation(fordetailedexplanation,pleaserefertoSection3.4).
cone et al. 2023). However, achieving accurate forecasts
Weattributethisfailuretotheincreasedtunableparameters
is challenging as both short-term and long-term dynamics
relativetothesmallsizeofpatchdata,whichmayhinderthe
jointlyinfluencefuturevalues(Chenetal.2023).
featureextractor’sabilitytolearnrobustrepresentations.
To model short-term dependency, the patching strategy
has gained attention for its ability to preserve contextual Giventhelackofsuitableadaptivemethodsfortimeseries
semantics. For instance, PatchTST (Nie et al. 2023) pro- patches, we propose Mixture of Feature Extractors (MoF).
posesgroupingdatapointsintopatchtokenstoretainlocal InspiredbytheMixtureofExpertsapproach(Shazeeretal.
semantics, while Pathformer (Chen et al. 2017) employs a 2017),whichusessparseactivationtoflexiblyadjustmodel
structureforvariousdownstreamtasks,MoFcomprisesmul-
*Correspondingauthor. tiple sub-extractors designed to handle divergent contexts
1
4202
guA
82
]GL.sc[
1v79951.8042:viXra×𝐍 ×𝐍 ×𝐍 ×𝐍 Attention ×𝐍
Attention Mamba ConvFFN Mamba Layer
Layer Layer Layer
Conv
MoA
Layer
Attention Mamba ConvFFN Attention
Layer Layer Layer Mamba
Layer
Attention
Layer Mamba DWConv
Layer Mamba MoF
Layer … Positional
encoding
Positional encoding
Tokenembedding emT bo ek de dn ing eT me bm ep do dr inal g emT bo ek de dn ing eT me bm ep do dr inal g emT bo ek de dn in g eT me bm ep do dr inal g Tokenembedding
(1)PatchTST/Transformer (2)Mamba (3)ModernTCN (4)Mambaformer (5)MoU(Ours)
Figure2:Illustrationofdifferentarchitecturesforlong-termtimeseriesforecasting.FromlefttorightarePatchTST/Trans-
former (Nie et al. 2023), Mamba (Gu and Dao 2023; Wang et al. 2024b), ModernTCN (Donghao and Xue 2024), Mam-
baformer(Xuetal.2024),andourproposedMoU.Feed-forwardlayerisomittedforsimplicityinTransformerandourmodel.
within patches. With sparse activation, MoF selectively ac- term dependency modeling of MoA. It pioneers the de-
tivates the most appropriate sub-extractor based on the in- composition of long-term dependencies from a partial-
put patch, ensuring both the learning of diverse contexts to-globalviewandappliessuchadiversemixturetotime
and minimal parameter increase. By effectively capturing series forecasting. Architecture comparison of our MoU
thevaryingcontextualinformationofdifferentpatches,MoF isprovidedinFigure2.
standsasapromisingfeatureextractorfortimeseriesdata. • We conduct extensive experiments on seven real-world
For capturing long-term dynamics, Transformers are ef- datasets to evaluate the performance of MoU in time se-
fectiveduetotheirglobalawarenessviatheattentionmech- rieslong-termforecastingtask,theresultsshowthatMoU
anism (Vaswani et al. 2017). However, they suffer from consistently achieves state-of-the-art results on the ma-
quadratic computational costs in each Self-Attention layer, jorityofthedatasets.
which are often stacked multiple times (Nie et al. 2023;
Chen et al. 2024). Mamba is recently proposed for long- 2 Approach
termsequencemodelingwithnear-linearcomputationalcost
2.1 ProblemSettingandModelStructure
(GuandDao2023),butithasshownlimitedperformancein
long-termtimeseriesforecasting,likelyduetoinformation Theaimofmultivariatetimeseriesforecastingistopredict
loss from its compression and selection mechanism (Wang future values over T time steps, given historical data from
et al. 2024b). To address both effectiveness and efficiency, the past L time steps. Specifically, given the historical val-
weintroduceMixtureofArchitectures (MoA),anovelhy- uesofamultivariatetimeseriesconsistingofM variables,
bridencoderfortimeserieslong-termforecasting.MoAfea- X input = (cid:2) X1,X2,...,XM(cid:3) ∈ RM×L,whereeachXi isa
tures a hierarchical structure starting with a Mamba layer vectoroflengthL,Xi =[x 1,x 2,...,x L]∈RL,ourtaskis
(cid:104) (cid:105)
that selects and learns key dependencies using a Selective topredictthefuturevaluesXˆ = Xˆ1,Xˆ2,...,XˆM ∈
output
State-Space Model (SSM). This is followed by a FeedFor-
ward transition layer and a Convolution-layer that broad- RM×T,whereeachXˆi isavectoroflengthT,representing
ens the receptive field to capture longer dependencies. Fi- thepredictedvaluesforthei-thvariablefromL+1toL+T.
nally,aSelf-Attentionlayerintegratesinformationglobally Inthiswork,weadoptthevariateindependencesettingasin
tofullycapturelong-termdependencies.Byinitiallyfocus- PatchTST(Nieetal.2023),simplifyingourgoaltolearning
ingonpartialdependenciesandprogressivelyexpandingto afunctionF:X → Xˆ,whichmapshistoricaltimeseriesto
a global view, MoA achieves better long-term dependency predictedtimeseries.Inourwork,F isourproposedMoU.
learning with minimal cost. Additionally, using the Self- First,wepreprocessthetimeseriesusingapatchingstrat-
Attentionlayeronlyoncereducesthecomputationalburden. egy, which applies a sliding window of fixed size P with
Efficiency and model size comparisons are shown in Fig- strideS togenerateasequenceofN patchtokens:
ure1.
X =Patch(X) (1)
p
Ourkeycontributionsofthisworkareasfollows:
where X ∈ RL is the raw time series, and X ∈ RN×P
• We propose Mixture of Feature Extractors (MoF) as an p
istheresultingpatchedsequence,consistingofN patchto-
adaptivefeatureextractorthatcapturesthecontextualse-
kens,eachcontainingP datapoints.
manticsofpatchestoenhanceshort-termrepresentation.
Tocaptureshort-termdependencieswithintokens,X is
To our knowledge, MoF is the first adaptive method for p
fed into our MoF module. The process of generating adap-
embedding time series patches. We also introduce Mix-
tiverepresentationsisdescribedas:
ture of Architectures (MoA) for hierarchical long-term
dependencymodeling. X rep =MoF(X p), (2)
• We introduce MoU, integrating the adaptive feature ex- where MoF(·) is our proposed Mixture of Feature Extrac-
tractioncapabilitiesofMoFwithacomprehensivelong- tors, specifically designed for time series patch tokens. It
2Router NetworkNoise～Normal Distribution 𝑊$ 𝐵×𝑁×𝑁 Add &
𝐵×𝑁×𝑃 … Top 2 expert Selection 𝐵×𝑁×𝐷 Flatten+Linear Head 𝑊# DotProduct MatMulN Fo Frm N 𝐵×𝑁×𝐷
𝐵 𝑁 𝐵×𝑁×𝐷 𝑊% C 𝜪om 𝒏p /u 𝒅ta /ti 𝒏on − C 𝟏ost: Self-Attention
𝐷 Add & Norm Layer
Ext ①ractor Ext ②ractor Ext ③ractor Ext ④ractor… Ext ⑤ractor
Weighted Sum
Add & Norm
Self-Attention Layer 𝐵×𝑁×𝐷 𝐵×𝐷×𝑁 𝐵×𝐷×𝑁 𝐵× 𝑁×𝐷
Instances
Time
Dimension
RevINorm
Instances
Time
Dimension
Patching
Instances
Token
Dimension
CoA nvd od
l
u&
ti
oN no Lrm
ayer
Conv-Lay
𝑇
er W ① ③e K Pk eae rde ndp ei lo n _u sg it = zp e1u = t 3 d i m
𝑇
e n ② ④sio Sn Kt r eu i rn d nc e eh = la _1 nn ug med = b y D:
Figure 3: Illustration of the proposed Mixture of Feature 𝐵×𝑁×𝐷 𝐵×𝑁×𝐷! 𝐵× 𝑁×𝐷!
ReLU Extractors (MoF) structure. MoF contains multiple Sub- 𝜎 Add & Norm
Extractors,eachistailoredtolearndifferentcontextswithin 𝐵× 𝑁×𝐷
Feed Forward
individual patches. Sub-Extractors are selectively activated
by Router in a sparse manner, thereby ensuring both adap- Linear Projection
Add & Norm
tivityandhighefficiency.
Mamba Layer SiLu
adjusts network parameters for different patches based on × 𝐍 Co n𝜎 v1d C A" △" SiL 𝜎u
their divergent context information while maintaining low
B"
Adaptive Feature extractor Linear B×1×D Linear
computationalcost.ThedetaileddescriptionofMoF(·)will Projection Project Projection
bepresentedinSection2.2.AfterapplyingMoF(·),weob- Preprocess 𝐵×𝑁×𝐷
Selective SSM
tainX ∈RN×D,whereDdenotesthefeaturedimension
rep
ofthepatchtokens. Figure 4: Illustration of the proposed Mixture of Architec-
To capture long-term dependencies among tokens, X tures(MoA)structure.MoAinitiallyconcentratesonpartof
rep
isprocessedbyourproposedMoA.MoAusesmultiplelay- dependencies selected by SSM, and progressively expands
ersofdiversearchitecturestomodellong-termdependencies thereceptivefieldintoacomprehensiveglobalview.
fromwithahybridperspective:
H(X ) = [H(X ) ,H(X ) ,...H(X ) ], where H(X )
p p 1 p 2 p c p i
X =MoA(X ), (3) denotesthescoreofi-thsub-extractor:
rep’ rep
(cid:0) (cid:1)
whereMoA(·)denotesourlong-termencoderbasedonthe H(X p) i=(X p·W g) i+SN·Softplus (X p·W noise)
i
(7)
Mixture of Architectures. Its detailed description will be whereW containstheparametersofalinearfunction,and
providedinSection2.3.ThisstepyieldsX rep’ ∈RN×D. thesecong dterminjectstunablenoiseforloadbalancing,fol-
Finally, we flatten X rep’ and apply a linear projector to lowingtheapproachinMoE(Shazeeretal.2017).Here,SN
obtainthefinalprediction: representsthestandardnormaldistribution.
Xˆ =P(Flatten(X )). (4) This mechanism effectively partitions the patch tokens
rep’ into combinations of c different patterns, where c corre-
sponds to the number of sub-extractors. Since the informa-
2.2 MixtureofFeatureExtractors tion of each pattern is processed by corresponding optimal
To account for the diverse feature affinities present within featureextractor,MoFiscapableofgeneratingmostrepre-
variouspatches,weintroduceMoFinFigure3asanadap- sentativeembeddingforpatcheswithdivergentcontexts.
tive feature extractor specifically designed for time series
2.3 MixtureofArchitectures
patches.Differentfromotheradaptivemethods,MoFkeeps
minimum increment in activated net parameters by sparse We propose MoA to capture comprehensive long-term de-
activation. This facilitates a robust generation of adaptive pendencies. As shown in Figure 4, MoA is structured hier-
representations,especiallyfortimeseriespatcheswithsuch archically with four layers, Mamba, FeedForward, Convo-
small data size. Specifically, MoF consists of a set of sub- lution, and Self-Attention layers, each captures a different
extractors {F ,F ,...,F }, each representing an indepen- aspectoflong-termdependencies.Benefitingfromitsgrad-
1 2 c
dentlinearmapping.Therepresentationforapatchtokenis ually expanding perspective, MoA is capable of modeling
generatedbyMoFasfollows: long-termdependencieswitheffectivenessandefficiency.
(cid:88)n Mamba-layerinTimeSeriesisfirsttoselectrelevantdata
X =MoF(X )= R (X )F (X ) (5)
rep p i p i p and learns time-variant dependencies with selective SSM.
i=1 LetxdenotestheX ,theprocesscanbedescribedas:
rep
where R i(·) is an input-relevant router that generates a x′ =σ(Conv1D(Linear(x)))
sparse vector with most elements set to zero, enabling the (8)
z =σ(Linear(x))
sparseactivationofsub-extractorsandensuringminimalpa-
rameterincrease.TherouterfunctionR(X ) iscalculated: whereσ isactivationfunctionSiLU.Then,x,x′ andz are
p i
usedtocalculateoutputyby:
R(X ) =Softmax(Top (H(X ) ,k)) (6)
p i k p i
where Softmax(·) normalizes the top k scores kept by
y′ =Linear(SelectiveSSM(x′)⊗z)
(9)
Top k(·,k). H(X p) is a vector of scores for sub-extractors: y =LayerNorm(y′+x)
3
FFN-Layer
Mamba-Layer
Add
&
Norm
Add
&
Norm
Add
& Norm
Add
& Normwhere ⊗ denotes element-wise multiplication, and the 2.4 ComputationalComplexityandModel
SelectiveSSMcanbefurtherexpandedas: Parameter
SelectiveSSM(x′)=y GivenasequencewithT tokens,thecomputationalcostof
t t
(10) anMoUblockthatselectsthetop-kexpertsis:
y =Ch , h =A¯h +B¯x′
t t t t−1 t C =kT×d2+T×d2+T×d2+kTd2+T2×d+T×d2
MoU
Specifically,h tisthelatentstateupdatedattimestept,while (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124)(cid:123)(cid:122)(cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
y is the output representation. The discrete matrices A, B MoF Mamba FFN Conv Transformer
t (14)
andC areinput-relevantandupdatedbythetime-variantre-
wherekisthekernelsizeintheconvolutionallayer,disthe
currentruleoverthetime:
dimensionofthevectorrepresentations.IntheTransformer
B t =S B(x′ t), C t =S C(x′ t), block,weaccountforthecomplexityofboththelineartrans-
(11)
∆ =softplus(S (x′)) formation used to compute query (Q), key (K), and value
t ∆ t
(V)matrices,aswellasthecomplexityoftheSelf-Attention
whereS ,S ,andS arelinearprojectionlayers.Weuse
B C ∆ layer.Forcomparison,thecomputationalcostrequiredbya
followingequationtoobtaindiscreteparametersA¯ andB¯ :
t t three-layerMulti-HeadSelf-Attention(MHSA)is:
f A(∆ t,A)=exp(∆ tA) C
MHSA
=3×(T2d+Td2) (15)
f B(∆ t,A,B t)=(∆ tA)−1(exp(∆ tA)−I)·∆B t (12) As shown, except for the Transformer block, the complex-
A¯ =f (∆ ,A), B¯ =f (∆ ,A,B ) ityofourstructureislinear,resultinginsignificantlylower
t A t t B t t
computational requirements compared to pure Transformer
wheref andf arediscretizationfunctions,whileA,B,C
A B models like PatchTST (Nie et al. 2023). Considering the
and∆areparameters.
model parameters, the linear layer primarily involves pa-
FeedForward-layer serves as the second transition layer, rameters of size d2 + bias for MoF and FFN layers, the
following the Transformer architecture’s convention of en-
convolutionallayerismainlydeterminedbythekernelsize
hancingnon-linearity:x ffn =FeedForward(y t ;w 1,σ,w 2), of k × d2, and the Self-Attention layer’s parameters are
wherew andw areparameters,σisactivationfunction.
1 2 mainlythemappingmatricesforQ,K,V,eachheadofsize
Convolution-layer, the third layer, expands MoA’s recep- d2. As shown in Figure 1, the total size of our model is
tive field. It facilitates information exchange among to-
372MB,comparedtoPatchTSTof408MBandModernTCN
kens encapsulating partial long-term dependencies, allow-
of611MB.
ingmorecomprehensivelearningofdependencies:x =
conv
Conv(x ffn;k,s,p,c out), where k is the kernel size, s is the 3 Experiments
stride, p is the padding, and c is the number of output
out
3.1 Datasets
channels,withtheoutputdimensionkeptunchanged.
Self-Attention-layer is the final layer, capturing compre- Weevaluatelong-termforecastingperformanceofourpro-
hensivelong-termdependencieswithitsglobalperspective: posed MoU on 7 commonly used datasets (Wu et al. 2021),
including Weather, ILI and four ETT datasets (ETTh1,
ETTh2,ETTm1andETTm2).DetailsareinAppendixA.
x =FeedForward(Attention(Q,K,V))
att
(cid:18) QKT(cid:19)
3.2 BaselinesandSetup
Attention(Q,K,V)=Softmax √ V (13)
d k To evaluate the overall performance of MoU in time se-
Q=x W , K =x W , V =x W rieslong-termforecastingtask,weconductedacomparative
conv Q conv K conv V
analysisagainstfourstate-of-artmodels,eachrepresentative
whereW ,W ,W areparameters,andself-attentionisim-
Q K V ofadistinctarchitecturalparadigm.Theincludedbaselines
plementedusingascaleddotproduct.
areasfollows:
Partial-to-globalDesignforTimeSeries.Tocapturelong-
• Mamba-based Models (S-Mamba): S-Mamba model,
termdependenciesintimeseries,ourMoAinitiallyfocusing
known for reducing computational cost from quadratic
onpartofdependenciesandgraduallyexpandingintoacom-
tolinearwhilemaintainingcompetitiveperformance,has
prehensive global perspective. It begins with the Mamba-
beeneffectivelyappliedtotimeseriesforecasting,mak-
layer, which selectively processes time-variant dependen-
ingitarelevantbaselineforourstudy.
cies using SSM, focusing on partial dependencies to iden-
tify essential features. The FeedForward-layer adds non- • Linear-based Models (D-Linear): The D-Linear model,
linearityandtransitionsthesepartialdependenciesintomore which challenges the dominance of Transformer-based
complexrepresentations.TheConvolution-layerthenbroad- modelsbyachievingsuperiorperformanceintimeseries
ensthereceptivefield,facilitatinginformationexchangebe- forecasting, represents our chosen baseline for Linear-
tween tokens to enhance understanding of broader tempo- basedmodelsduetoitssimplicityandefficiency.
ral relationships. Finally, the Self-Attention-layer provides • Convolution-based Models (ModernTCN): by expand-
aglobalperspective,integratinglocalizedinformationintoa ingreceptivefield,demonstratesthatconvolutionalmod-
completeunderstandingoflong-termdependencies.Thishi- elscanachievestate-of-the-artperformanceinlong-term
erarchicaldesignensuresthethecapturingofintricatetime time series forecasting, making it our selected baseline
seriespatternswhileremainingcomputationallyefficient. forConvolution-basedmodels.
4Table 1: Multivariate long-term forecasting results with MoU on seven real-world datasets. We use prediction length T ∈
{24,36,48,60}forILIdatasetandT ∈ {96,192,336,720}forothers.Bestresultsareshowninbold,secondbestresultsare
showninunderline.
Model MoU (Ours) ModernTCN PatchTST DLinear S-Mamba
Metric MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE
96 0.358 0.393 0.368 0.394 0.370 0.400 0.375 0.399 0.398 0.417
192 0.402 0.418 0.406 0.414 0.413 0.429 0.405 0.416 0.438 0.444
336 0.389 0.418 0.392 0.412 0.422 0.440 0.439 0.443 0.453 0.455
720 0.440 0.462 0.450 0.461 0.448 0.468 0.472 0.490 0.510 0.508
96 0.266 0.332 0.264 0.333 0.274 0.336 0.289 0.353 0.301 0.358
192 0.319 0.369 0.318 0.373 0.339 0.379 0.383 0.418 0.366 0.398
336 0.305 0.369 0.314 0.376 0.331 0.380 0.448 0.465 0.391 0.419
720 0.379 0.422 0.394 0.432 0.379 0.422 0.605 0.551 0.417 0.444
96 0.292 0.346 0.297 0.348 0.293 0.343 0.299 0.343 0.303 0.358
192 0.329 0.371 0.334 0.370 0.330 0.368 0.335 0.365 0.345 0.383
336 0.358 0.391 0.370 0.392 0.365 0.392 0.369 0.386 0.378 0.403
720 0.412 0.420 0.413 0.416 0.419 0.424 0.425 0.421 0.440 0.440
96 0.166 0.257 0.169 0.256 0.166 0.256 0.167 0.260 0.177 0.270
192 0.220 0.294 0.227 0.299 0.223 0.296 0.224 0.303 0.229 0.305
336 0.270 0.329 0.276 0.329 0.274 0.330 0.281 0.342 0.281 0.338
720 0.351 0.380 0.351 0.381 0.362 0.385 0.397 0.421 0.371 0.392
96 0.149 0.199 0.150 0.204 0.149 0.198 0.152 0.237 0.158 0.210
192 0.193 0.241 0.196 0.247 0.194 0.242 0.220 0.282 0.202 0.251
336 0.234 0.279 0.237 0.283 0.245 0.282 0.265 0.319 0.256 0.291
720 0.308 0.329 0.315 0.335 0.314 0.333 0.323 0.362 0.329 0.341
24 1.468 0.774 1.367 0.720 1.301 0.734 2.215 1.081 1.995 0.873
36 1.269 0.692 1.345 0.760 1.658 0.898 1.963 0.963 1.963 0.864
48 1.613 0.827 1.526 0.837 1.657 0.879 2.130 1.024 1.773 0.863
60 1.650 0.841 1.838 0.878 1.436 0.790 2.368 1.096 2.176 0.960
96 0.127 0.222 0.131 0.226 0.129 0.222 0.153 0.237 0.135 0.231
192 0.145 0.238 0.144 0.239 0.147 0.240 0.152 0.249 0.157 0.252
336 0.163 0.262 0.161 0.259 0.163 0.259 0.169 0.267 0.175 0.271
720 0.193 0.290 0.191 0.286 0.197 0.290 0.233 0.344 0.196 0.293
Wincount 35 17 14 2 0
• Transformer-based Models (PatchTST): PatchTST 3.4 AblationStudy
leverages the superiority of Transformer architecture
Ablation for feature extractor design. In this section,
meanwhileaddressingitsquadraticcomplexitybypatch-
we investigate the efficacy of MoF in capturing short-
ing, making it our chosen representative for handling
term dependencies within patch tokens. We compare four
long-termdependenciesintimeseriesforecasting.
types of encoders: one using a uniform transformation ap-
In our study, all models follow the same experimental proach (Linear) and three employing adaptive transforma-
setup with prediction length T ∈ {24,36,48,60} for ILI tion methods, including our proposed MoF, SE-M1, and
datasetandT ∈ {96,192,336,720}forotherdatasetsasin Dyconv (Chen et al. 2020). All methods take input X ∈
p
PatchTST((Nieetal.2023)).Wererunthebaselinemodels RN×P andproduceoutputX ∈RN×D.
rep
forfourdifferentlook-backwindowL∈{48,60,104,144}
FromTable2,wemakethefollowingobservations:
forILIandL ∈ {96,192,336,720}forothers,andalways
choosethebestresultstocreatestrongbaselines.Morede- • MoF outperforms the uniform transformation method
tailscanbefoundinAppendixB. (Linear), demonstrating that adaptive feature extraction
leadstobetterrepresentationofpatchtokens.
3.3 MainResults
• Dyconv, another adaptive method, fails to outperform
Multivariate long-term forecasting results are shown in Ta- Linear. We attribute Dyconv’s poor performance to the
ble 1. Overall, MoU consistently outperforms other base- huge increase in network parameters, making it unsuit-
lines.ComparedtoModernTCN,MoUachievesa17.2%re- ableforsmalldatasetsliketimeseriespatches.Thishigh-
ductioninMSEanda9.5%reductioninMAE.Whencom- lights our MoF’s advantage in maintaining a small set
pared to PatchTST, MoU yields a 22.6% reduction in MSE
anda15.6%reductioninMAE.Additionally,MoUdemon- 1SE-MisamodifiedmethodbasedonSqueeze-and-Excitation
stratessignificantimprovementsacrossalldatasetsoverthe (Huetal.2019),whichappliesanelement-wisemultiplicationon
Linear-basedDLinearandMamba-basedS-Mambamodels. thelinearlytransformedrepresentationbyagatingvector.
5
1hTTE
2hTTE
1mTTE
2mTTE
rehtaeW
ssenllI
yticirtcelETable2:Ablationexperimentsforfeatureextractorsinmodelingshort-termdependencies.Wecomparethreetypesofadaptive
featureextractorsandoneuniformfeatureextractor.Bestresultsareshowninbold.
Model MoF(inMoU) SE-M Linear Dyconv
Metric MSE MAE MSE MAE MSE MAE MSE MAE
96 0.358 0.393 0.363 0.397 0.361 0.395 0.468 0.460
192 0.402 0.418 0.408 0.423 0.405 0.421 0.497 0.475
336 0.389 0.418 0.394 0.424 0.394 0.424 0.467 0.474
720 0.440 0.462 0.486 0.465 0.441 0.466 0.504 0.506
96 0.166 0.257 0.165 0.256 0.166 0.257 0.206 0.294
192 0.220 0.294 0.223 0.297 0.220 0.295 0.254 0.324
336 0.270 0.329 0.268 0.330 0.273 0.332 0.301 0.350
720 0.351 0.380 0.356 0.381 0.353 0.381 0.383 0.400
96 0.149 0.199 0.155 0.208 0.153 0.207 0.194 0.256
192 0.193 0.241 0.194 0.242 0.194 0.244 0.230 0.283
336 0.234 0.279 0.244 0.283 0.247 0.288 0.257 0.305
720 0.308 0.329 0.307 0.330 0.307 0.330 0.326 0.350
ofactiveparameters,achievingbothstrongperformance SuSbub--EExxttrraactcotro 1r 1 SuSbu-bE-Exxttrraaccttoor r2 2 s SuSbub--EExxttrraacctotro 3r 3 SuSbub-E-Exxttrraacctotro 4r 4
andhighefficiency. 4 4 2
3 0 3 2 0
• MoF also outperforms SE-M on most datasets. MoF’s 2 -1 1 -2
diversesub-extractorsgeneratevariedrepresentationsin 1 0 -2 -0 1 -4
different contexts, whereas SE-M’s calibration strategy, -1 0 5 10 15-3 0 5 10 15- s2 0 5 10 15-6 0 5 10 15
whichmultipliestheoriginalrepresentationbyanormal- Figure 5: The patches categorized by their activated sub-
ized gating vector, limits its ability of handling highly extractorsautomatically.
diversecontextualinformation. 0 0 0 0
In summary, MoF shows superior performance across all 20 20 20 20
datasets,whichdemonstratestheeffectivenessofourdesign
40 40 40 40
incapturingshort-termdependencies.
60 60 60 60
Ablation for long-term encoders design. In this sec- 0 20 40 60 0 20 40 60 0 20 40 60 0 20 40 60
(a) (b) (c) (d)
tion, we focalize our ablation study on the MoA design.
We design 10 baselines: AA, MM, MFA, AAA, MMA, Figure 6: Visualization of hidden attention maps of Self-
AMM, MAM, AMA, AFM, and AFCM for comprehen- AttentionLayer(a-b)andMamba-Layer(c-d)inMoA.
sive comparison. Wherein, A, M, F, and C represent Self-
Attention, Mamba, FeedForward, and Convolution-layers, performs the F transition (MFA), demonstrating the effec-
respectively,withtheletterorderindicatingthesequenceof tivenessofF-C.TheConvolution-layerfurtherexpandsthe
layers.TheexperimentalresultsarepresentedinTable3. receptivefieldoftheMamba-layer,providinganintermedi-
First,weexaminetheimpactoftheorderbetweenMamba ateviewthatbridgestheMamba-layer’spartialviewandthe
(M) and Self-Attention (A) layers on model performance. Self-Attentionlayer’sglobalview.
Models with the M-A order (MAM, AMA, MMA) consis-
tently outperform those without it (AMM), suggesting that 3.5 ModelAnalysis
placing Mamba before Self-Attention is more effective for
Does MoF actually learn contexts within patches? To
capturing long-term dependencies. Conversely, the model
assess whether MoF effectively learns distinct contexts
withouttheA-Morder,MMA,stillperformswellcompared
within patches, we analyze the sub-extractor activations.
to those with A-M (MAM, AMA, AMM), indicating that
After training MoF, we input a set of patches and record
A-MislesseffectivethanM-Aforenhancingperformance.
the activated sub-extractor for each. In cases where multi-
These findings highlight the significance of the Mamba
ple sub-extractors are activated, we noted the one with the
and Self-Attention layer order, with the M-A order prov-
highest score, as detailed in Section 2.2. We then catego-
ing superior. We attribute this to the gradual expansion of
rize the patches into C classes based on their sub-extractor
long-term dependency awareness, from the partial view in
activations. As shown in Figure 5, patches associated with
the Mamba-layer to the global view in the Self-Attention
the same sub-extractor exhibit similar wave patterns, while
layer, which enhances comprehensive long-term depen-
those linked to different sub-extractors show divergent dy-
dency learning. In Section 3.5, we study deeper into what
namics. This confirms that MoF effectively learns distinct
theMambaandSelf-Attentionlayersspecificallycapture.
contextswithinpatches,leadingtomorerepresentativeem-
Second,weexplorethedesignofthetransitionlayerbe-
beddings.Moredetailsandmorevisualizationscanbefound
tween Mamba and Self-Attention layers. We compare two
inAppendixE.
structures: a single FeedForward-layer (F) and a combina-
tionofFeedForwardandConvolution-layers(F-C).Results What is learned by the layers of MoA? As aforemen-
show that the F-C transition (our MoA) significantly out- tioned,MoAisdesignedtocapturelong-termdependencies
6
1hTTE
2mTTE
rehtaeWTable3:Ablationexperimentsforthedesignoflong-termencoderswithvariousarchitectures.Itisnotedthattheorderoflayers
significantlyinfluencetheperformance.Bestresultsareshowninbold.
Model MoA(inMoU) AA MM MFA AAA MMA
Metric MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE
96 0.358 0.393 0.370 0.396 0.372 0.394 0.367 0.393 0.376 0.401 0.369 0.395
192 0.402 0.418 0.411 0.419 0.413 0.416 0.407 0.415 0.414 0.421 0.414 0.419
336 0.389 0.418 0.390 0.412 0.413 0.430 0.404 0.420 0.416 0.432 0.422 0.435
720 0.440 0.462 0.456 0.464 0.468 0.473 0.471 0.463 0.479 0.479 0.456 0.457
96 0.166 0.257 0.168 0.257 0.169 0.259 0.169 0.258 0.166 0.256 0.167 0.257
192 0.220 0.294 0.222 0.294 0.222 0.295 0.223 0.296 0.223 0.296 0.224 0.297
336 0.270 0.329 0.281 0.333 0.281 0.335 0.281 0.334 0.275 0.330 0.275 0.331
720 0.351 0.380 0.365 0.386 0.355 0.381 0.356 0.384 0.359 0.383 0.356 0.383
96 0.149 0.199 0.152 0.203 0.156 0.206 0.150 0.201 0.151 0.202 0.152 0.202
192 0.193 0.241 0.194 0.243 0.201 0.248 0.195 0.244 0.195 0.245 0.193 0.242
336 0.234 0.279 0.242 0.283 0.243 0.286 0.245 0.284 0.244 0.284 0.244 0.283
720 0.308 0.329 0.308 0.331 0.316 0.334 0.310 0.333 0.309 0.331 0.309 0.332
Model MoA(inMoU) AMM MAM AMA AFM AFCM
Metric MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE
96 0.358 0.393 0.372 0.394 0.365 0.394 0.368 0.395 0.369 0.394 0.367 0.393
192 0.402 0.418 0.420 0.422 0.411 0.418 0.413 0.420 0.410 0.417 0.410 0.417
336 0.389 0.418 0.429 0.440 0.423 0.438 0.426 0.438 0.407 0.423 0.397 0.417
720 0.440 0.462 0.474 0.475 0.470 0.464 0.587 0.515 0.465 0.467 0.465 0.470
96 0.166 0.257 0.168 0.257 0.167 0.257 0.169 0.257 0.168 0.257 0.168 0.257
192 0.220 0.294 0.226 0.298 0.223 0.295 0.220 0.294 0.225 0.296 0.224 0.296
336 0.270 0.329 0.278 0.331 0.277 0.331 0.276 0.330 0.281 0.333 0.277 0.330
720 0.351 0.380 0.356 0.381 0.365 0.386 0.358 0.385 0.356 0.380 0.357 0.380
96 0.149 0.199 0.157 0.208 0.148 0.199 0.152 0.203 0.151 0.202 0.150 0.200
192 0.193 0.241 0.195 0.244 0.193 0.242 0.192 0.243 0.196 0.245 0.193 0.242
336 0.234 0.279 0.251 0.287 0.247 0.284 0.242 0.280 0.236 0.280 0.236 0.282
720 0.308 0.329 0.315 0.333 0.318 0.334 0.307 0.331 0.309 0.330 0.313 0.332
byinitiallyfocusingonpartialdependenciesintheMamba- ernTCN (Donghao and Xue 2024) optimizes the use
layer and gradually expanding to a global perspective in of convolution in time series by proposing a DWConv
the Self-Attention layer. To validate this design, we ana- andConvFFN-basedstructure.TimeMachine(Ahamedand
lyze the attention maps of both the Mamba-layer and the Cheng 2024) leverages Mamba, a state-space model, to
Self-Attentionlayer.SincetheMamba-layerdoesnotnatu- capture long-term dependencies in multivariate time series
rally generate attention maps, we use a method referenced whilemaintaininglinearscalabilityandlowmemoryusage.
in (Ali,Zimerman,andWolf2024)tocreateone.Figure6 Mambaformer (Xu et al. 2024) combines Transformer and
showstheseattentionmaps,withpanels(a)and(b)forSelf- Mamba architectures for enhanced time series forecasting.
Attentionlayerandpanels(c)and(d)toMamba-layer. Additionally, Time-LLM (Wang et al. 2024a) repurposes
Inpanels(c)and(d),thediagonalbrightnessgradientin- large language models (LLMs) of LLaMA (Touvron et al.
dicatesthattheMamba-layerprimarilycapturesrecentlong- 2023)forgeneraltimeseriesforecastingbyusingLLMsasa
termdependencies.Thesemi-brightgridsinpanel(d),start- reprogrammingframeworkwhilekeepingthebackbonelan-
ing from the diagonal and gradually fading, suggest that guagemodelsintact.
the Mamba-layer detects periodic long-term dependencies
within a limited view. Conversely, panels (a) and (b) show 5 Conclusion
scattered bright pixels, indicating that the Self-Attention
layer captures long-term dependencies from a global per-
WehavepresentedMixtureofUniversals(MoU),apioneer-
spective. These findings align with our initial design, con- ing and advanced model designed for efficient and effec-
firmingtherationaleandeffectivenessofourlayerarrange- tive time series forecasting. MoU consists of two key com-
mentinMoA. ponents: Mixture of Feature Extractors (MoF), an adap-
tive method specifically designed to enhance time series
patchrepresentationsforcapturingshort-termdependencies,
4 RelatedWork
and Mixture of Architectures (MoA), which hierarchically
Research into architectures for long-term time series fore- integrates multiple architectures in a structured sequence
casting recently has gained significant attention. For in- to model long-term dependencies from a hybrid perspec-
stance, ETSformer (Woo et al. 2022) proposes time se- tive. The proposed approach achieves state-of-the-art per-
ries Transformers by integrating the principle of expo- formanceacrossvariousreal-worldbenchmarkswhilemain-
nential smoothing. PatchTST (Nie et al. 2023) introduces taininglowcomputationalcosts.Wehopethatourworkcan
a Transformer-based model using patching and channel- bringnewideasandinsightstothemodelstructuredesignin
independent structures for time series forecasting. Mod- thefieldoftimeseriesforecasting.
7
1hTTE
2mTTE
rehtaeW
1hTTE
2mTTE
rehtaeWReferences Kim,H.;Park,S.;andKim,S.2023. Time-seriesclustering
and forecasting household electricity demand using smart
Ahamed,M.A.;andCheng,Q.2024. Timemachine:Atime
meterdata. EnergyReports,9:4111–4121. 1
series is worth 4 mambas for long-term forecasting. arXiv
preprintarXiv:2403.09898. 7 Lepikhin, D.; Lee, H.; Xu, Y.; Chen, D.; Firat, O.; Huang,
Y.; Krikun, M.; Shazeer, N.; and Chen, Z. 2020. Gshard:
Ali,A.;Zimerman,I.;andWolf,L.2024. Thehiddenatten-
Scalinggiantmodelswithconditionalcomputationandau-
tion of mamba models. arXiv preprint arXiv:2403.01590.
tomaticsharding. arXivpreprintarXiv:2006.16668. 12
7
Liu,S.;Wu,K.;Jiang,C.;Huang,B.;andMa,D.2023. Fi-
Bieganowski, B.; and Slepaczuk, R. 2024. Supervised Au-
nancialtime-seriesforecasting:Towardssynergizingperfor-
toencoderMLPforFinancialTimeSeriesForecasting.arXiv
manceandinterpretabilitywithinahybridmachinelearning
preprintarXiv:2404.01866. 1
approach. arXivpreprintarXiv:2401.00534. 1
Bilal,M.;Kim,H.;Fayaz,M.;andPawar,P.2022.Compara- Murat, M.; Malinowska, I.; Gos, M.; and Krzyszczak, J.
tiveanalysisoftimeseriesforecastingapproachesforhouse- 2018. Forecasting daily meteorological time series using
hold electricity consumption prediction. arXiv preprint ARIMAandregressionmodels. Internationalagrophysics,
arXiv:2207.01019. 1 32(2). 1
Cascone, L.;Sadiq, S.;Ullah, S.; Mirjalili,S.; Siddiqui, H. Neumann, O.; Beichter, M.; Heidrich, B.; Friederich, N.;
U. R.; and Umer, M. 2023. Predicting household electric Hagenmeyer,V.;andMikut,R.2024. IntrinsicExplainable
power consumption using multi-step time series with con- Artificial Intelligence Using Trainable Spatial Weights on
volutionalLSTM. BigDataResearch,31:100360. 1 NumericalWeatherPredictions. InProceedingsofthe15th
ACM International Conference on Future and Sustainable
Chen,L.;Chen,D.;Shang,Z.;Wu,B.;Zheng,C.;Wen,B.;
EnergySystems,551–559. 1
and Zhang, W. 2023. Multi-Scale Adaptive Graph Neural
Network for Multivariate Time Series Forecasting. IEEE Nie, Y.; Nguyen, N. H.; Sinthong, P.; and Kalagnanam, J.
TransactionsonKnowledgeandDataEngineering,35(10): 2023. A Time Series is Worth 64 Words: Long-term Fore-
10748–10761. 1 casting with Transformers. In International Conference on
LearningRepresentations. 1,2,4,5,7,10,12
Chen, P.; ZHANG, Y.; Cheng, Y.; Shu, Y.; Wang, Y.; Wen,
Q.; Yang, B.; and Guo, C. 2017. Pathformer: Multi-scale Scher,S.2020.Artificialintelligenceinweatherandclimate
TransformerswithAdaptivePathwaysforTimeSeriesFore- prediction: Learning atmospheric dynamics. Ph.D. thesis,
casting. InTheTwelfthInternationalConferenceonLearn- DepartmentofMeteorology,StockholmUniversity. 1
ingRepresentations. 1 Sezer,O.B.;Gudelek,M.U.;andOzbayoglu,A.M.2020.
Financialtimeseriesforecastingwithdeeplearning:Asys-
Chen,P.;Zhang,Y.;Cheng,Y.;Shu,Y.;Wang,Y.;Wen,Q.;
tematicliteraturereview:2005–2019. Appliedsoftcomput-
Yang,B.;andGuo,C.2024. Multi-scaletransformerswith
ing,90:106181. 1
adaptive pathways for time series forecasting. In Interna-
tionalConferenceonLearningRepresentations. 2 Shazeer,N.;Mirhoseini,A.;Maziarz,K.;Davis,A.;Le,Q.;
Hinton, G.; and Dean, J. 2017. Outrageously large neu-
Chen,Y.;Dai,X.;Liu,M.;Chen,D.;Yuan,L.;andLiu,Z.
ral networks: The sparsely-gated mixture-of-experts layer.
2020.Dynamicconvolution:Attentionoverconvolutionker-
arXivpreprintarXiv:1701.06538. 1,3,12
nels. InProceedingsoftheIEEE/CVFconferenceoncom-
Touvron,H.;Lavril,T.;Izacard,G.;Martinet,X.;Lachaux,
puter vision and pattern recognition, 11030–11039. 1, 5,
M.-A.; Lacroix, T.; Rozie`re, B.; Goyal, N.; Hambro, E.;
11
Azhar, F.; et al. 2023. Llama: Open and efficient founda-
Donghao, L.; and Xue, W. 2024. ModernTCN: A Modern
tionlanguagemodels. arXivpreprintarXiv:2302.13971. 7
Pure Convolution Structure for General Time Series Anal-
Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,
ysis. InInternationalConferenceonLearningRepresenta-
L.;Gomez,A.N.;Kaiser,Ł.;andPolosukhin,I.2017. At-
tions. 2,7,10
tentionisallyouneed. Advancesinneuralinformationpro-
Gu, A.; and Dao, T. 2023. Mamba: Linear-time se- cessingsystems,30. 2
quencemodelingwithselectivestatespaces. arXivpreprint
Wang,S.;Wu,H.;Shi,X.;Hu,T.;Luo,H.;Ma,L.;Zhang,
arXiv:2312.00752. 2
J.Y.;andZHOU,J.2024a.TimeMixer:DecomposableMul-
Haq, M. A. 2022. CDLSTM: A novel model for climate tiscaleMixingforTimeSeriesForecasting. InInternational
change forecasting. Computers, Materials & Continua, ConferenceonLearningRepresentations(ICLR). 7
71(2). 1
Wang,Z.;Kong,F.;Feng,S.;Wang,M.;Zhao,H.;Wang,D.;
Hu, J.; Shen, L.; Albanie, S.; Sun, G.; and Wu, E. 2019. and Zhang, Y. 2024b. Is Mamba Effective for Time Series
Squeeze-and-Excitation Networks. IEEE Transactions on Forecasting? arXivpreprintarXiv:2403.11144. 2,10
Pattern Analysis and Machine Intelligence, 42(8): 2011– Woo,G.;Liu,C.;Sahoo,D.;Kumar,A.;andHoi,S.2022.
2023. 5,11 Etsformer: Exponential smoothing transformers for time-
Jacobs,R.A.;Jordan,M.I.;Nowlan,S.J.;andHinton,G.E. seriesforecasting. arXivpreprintarXiv:2202.01381. 7
1991. Adaptivemixturesoflocalexperts. Neuralcomputa- Wu, H.; Xu, J.; Wang, J.; and Long, M. 2021. Auto-
tion,3(1):79–87. 12 former:DecompositionTransformerswithAuto-Correlation
8for Long-Term Series Forecasting. In Advances in Neural
InformationProcessingSystems. 4,10
Xu, X.; Liang, Y.; Huang, B.; Lan, Z.; and Shu, K. 2024.
IntegratingMambaandTransformerforLong-ShortRange
TimeSeriesForecasting. arXivpreprintarXiv:2404.14757.
2,7
Yang,B.;Bender,G.;Le,Q.V.;andNgiam,J.2019. Cond-
conv:Conditionallyparameterizedconvolutionsforefficient
inference. Advances in neural information processing sys-
tems,32. 1
Zeng,A.;Chen,M.;Zhang,L.;andXu,Q.2023. Aretrans-
formerseffectivefortimeseriesforecasting?InProceedings
oftheAAAIconferenceonartificialintelligence,volume37,
11121–11128. 10
9Appendix
datasetsand7:1:2fortheothers.Thebestparametersarese-
lected based on the lowest validation loss and then applied
In this Appendix, we provide details which are omitted in
tothetestsetforperformanceevaluation.
themaintext.Theoutlinesareasfollows:
• SectionA:Anintroductionofdatasetsandtheirtraining B ImplementationDetails
setsplitconfigurations(in“Experiments”ofmaintext).
Devices All the deep learning networks are implemented
• Section B: A description of training configurations, in-
inPyTorchandconductedonNVIDIAV10032GBGPU.
cluding devices, optimization, training parameter set-
tingsandthevisualizationofpredictionscomparedwith Optimization Our training target is minimizing l 2 loss,
baselinemodels(in“Experiments”ofmaintext). with optimizer of Adam by default. Then initial training
rateissetto2.5×10−3 onILIdatasetforquicksearching,
• SectionC:Thedetailsaboutmodelparametersconfigu-
2×10−4 forETTm2forbetterconvergence,and1×10−3
rationinourMoU(in“Experiments”ofmaintext).
forotherdatasetsbydefault.
• Section D: An introduction to the baseline models and
an elucidation of their respective calculation processes Parameter Setting Instead of using a fixed look-back
(in“AblationStudy”ofmaintext). window, we rerun PatchTST, ModernTCN, and S-Mamba
with varying look-back windows: L ∈ {48,60,104,144}
• SectionE:MoredetailsforbehaviorsofMoF,andadis-
for the ILI dataset and L ∈ {192,336,512,720} for the
cussionaboutdifferencesbetweenMoFandothersimilar
other datasets. The best results are selected to generate
existingworks(in“ModelAnalysis”ofmaintext).
strong baselines. For DLinear, we directly use the results
• SectionF:Anillustrationofmetricsusedtoevaluateall
fromPatchTST(Nieetal.2023),wherethebestresultswith
models(in“Experiments”ofmaintext).
varyinglook-backwindowsarealreadyselected.
• SectionG:TheperformanceofMoUforunivariatelong-
Visualizationofprediction Wepresentavisualizationof
termforecasting,andanalysisforimpactofvariouslook-
thefuturevaluespredictedbyMoUwiththegroundtruthval-
backwindows(in“MainResults”ofmaintext).
uesfortheETTm2dataset.Thelook-backwindowissetto
• Our code as well as the running scripts are available at
512andpredictedlengthissetto336.Theresultsareshown
https://github.com/lunaaa95/mou/.
in Figure 7). We observe that the predicted values (orange
line)arehighlyconsistentwithground-truth(blueline),in-
A DatasetsDetails dicating our model is capable of making accurate forecast-
WeevaluatetheperformanceofMoUonsevenwidelyused ing. Besides, we also notice that the prediction repeats the
periodicwaveswhich haveshowninhistoricalseries, indi-
datasets (Wu et al. 2021), including Weather, ILI, Electric-
catingalong-termdynamicscapturedbymodel.
ity and four ETT datasets (ETTh1, ETTh2, ETTm1 and
ETTm2).Alldatasetsarepubliclyavailable.
0.8
• ETT datasets2 contain two-year records of Electricity
0.6
TransformerTemperaturefromtwodistinctregionsin a
Chinese province, labeled with suffixes 1 and 2. Each 0.4
datasetincludessevenvariableswithtimestamps:HUFL,
HULL, MUFL, LUFL, LULL, and OT. For univariate 0.2
forecasting, OT (oil temperature) is the primary focus.
0.0
Thedatasetsareprovidedintwotimeresolutions:hourly
(‘h’)and15-minuteintervals(‘m’). 0.2
• Weather3 comprises 21 meteorological indicators from
0.4
Germanyfortheyear2020,includingvariableslikehu-
midity,airtemperature,recordedat10-minuteintervals. 0.6
• ILI4 is a nationaldataset tracking influenza-like illness, 0.8 GroundTruth
includingpatientcountsandtheillnessratio.Itcontains Prediction
7variables,withdatacollectedweekly. 0 200 400 600 800
• Electricity5 dataset records the hourly electricity con- Figure 7: Visualization of MoU’s predicted values with
sumptionof321customers. groundtruth.
WefurthercollectthepredictionsofMoU,PatchTST(Nie
Dataset details are presented in Table 4. Following the
etal.2023),Dlinear(Zengetal.2023),ModernTCN(Dong-
setup in (Nie et al. 2023), we split each dataset into train-
hao and Xue 2024), S-Mamba (Wang et al. 2024b) on ILI
ing,validation,andtestsets.Thesplitratiois6:2:2forETT
datasetwithaforecasthorizonsetto36.Thecomparisonre-
2https://github.com/zhouhaoyi/ETDataset sultsaredisplayedon8.Theactualfuturevaluesaredenoted
3https://www.bgc-jena.mpg.de/wetter/ bytheblueline,representingthegroundtruth.Ourproposed
4https://gis.cdc.gov/grasp/fluview/fluportaldashboard.html model,MoU,isdepictedbytheorangelineandisobservedto
5https://archive.ics.uci.edu/ml/datasets/- mostcloselyalignwiththegroundtruth,indicatingitsbest
ElectricityLoadDiagrams20112014 accuracyincomparisonwithotherbaselinemodels.
10Table4:Descriptionofsevencommonlyuseddatasetsfortimeseriesforecasting.
Datasets ETTh1 ETTh2 ETTm1 ETTm2 Weather ILI Electricity
NumberofVariables 7 7 7 7 21 7 321
NumberofTimesteps 17420 17420 69680 69680 52696 966 26304
GroundTruth where σ 1 and σ 2 are respectively ReLU function and
MoU (Ours) Sigmoid function. Z is a average pooled vector which
2.5 S-Mamba squeezestheinformation:
ModernTCN
PatchTST Z=AvgPool(W X ) (18)
se p
2.0 Dlinear
To avoid underestimating the ability of original Squeeze-
and-Excitation method, we apply Squeeze-and-Excitation
1.5 onW X insteadofvanillaX togenerateenrichedrepre-
se p p
sentation.Thismodificationisthedifferencebetweenorigi-
nalSqueeze-and-ExcitationandourbaselinemethodSE-M.
1.0
Overall,theparametersparticipatedinoncecalculationare
W ∈ RP×D, W ∈ RD×(D/r) and W ∈ R(D/r)×D,
se 1 2
0.5 whereristhereductionrate.
Wisalineartransformation.Theprocesscanbesumma-
rizedas:
0 5 10 15 20 25 30 35
X =Linear(X )=W X (19)
Figure 8: Visualization of MoU’s predicted values in com- rep p l p
parisonwithotherbaselinemodels. DyconvisthemethodofDynamicConvolution(Chenetal.
2020).Theprocesscanbedescribedas:
C ModelArchitectureandParameterDetails
X =Dyconv(X )=Conv(X ;K) (20)
rep p p
By default, our MoU model includes a single MoA block. whereKdenotestheparametersofadaptivekernel,whichis
Our experimental setup uses a uniform patch length of 16 aggregatedbyasetofkernels{K }withattentionweightof
i
andapatchstrideof8forgeneratingpatchtokens.Thenum- π (X ),thisprocesscanbedescribedas:
i p
berofSub-Extractorsissettofour,withthetoptwoselected
N
intheMoFforgeneratingpatchrepresentations.TheMamba (cid:88)
K= π (X )K (21)
layer employs an SSM state expansion factor of 21. In the i p i
FeedForward layer, the hidden size expansion rate is set to i=1
2.TheConvolutionlayerparametersarefixedwithakernel Wecalculatetheweightofeachkernelπ (X )by:
i p
sizeof3,astrideof1,andpaddingof1.IntheSelf-Attention
π (X )=Softmax(W (σ(W (AvgPool(X )))) (22)
layer,thenumberofheadsissetto4fortheETTh1,ETTh2, i p 2 1 p
andILIdatasets,and16forallotherdatasets. where σ is activation function of ReLU. The size of pa-
Thedimensionalityofbothshort-termandlong-termrep- rameters in Dyconv is biggest among all listed methods.
resentationsissetto64fortheETTh1andETTh2datasets Overall,theparametersparticipatedinoncecalculationare
and increased to 128 for the remaining datasets for better W ∈RP×D,W ∈RD×(D/r)and{K }.
1 2 i
performance. Within the MoA block, the input and output
dimensionsarekeptconsistentacrosslayers,butcanbead- E MoreModelAnalysis
justedbymodifyingtheFeedForwardlayer’soutputdimen-
Behaviors of Sub-Extractors in MoF We conducted an
sionortheConvolutionlayer’skernelsizeandstride.
experiment to extract the contextual information learned
by the Sub-Extractors in MoF. The method is descried in
D BaselineModelsDetails Section 3.5 (main text), with the experimental outcomes
presented in an integrated format in Figure 5 (main text).
We present the detailed calculations of SE-M, W, and Dy-
Specifically,weselectedthetop10patcheswiththehighest
conv,whicharebaselinemodelsinSection3.4(maintext).
scores to serve as representative examples for their respec-
SE-MisamodifiedSqueeze-and-Excitation(Huetal.2019)
tiveSub-Extractors.
method.TheprocessofSE-Mcanbedescribedas:
Toenhanceclarity,eachpatchisindividuallydisplayedin
Figure9.Withineachrowofthefigure,thepatchesarepro-
X =SE-M(X )=(W X )⊗Y (16)
rep p se p
cessed by the same Sub-Extractor. It is observable that the
where parameter W ∈ RP×D, and Y denotes the gating patches exhibit a consistent shape within rows, while there
se
vector,whichcanbedescribesas: isasignificantdivergenceinshapesacrossrows.Thisobser-
vationdemonstratestheproficiencyoftheMoFincapturing
Y =Expand(σ (W σ (W Z))) (17) contextualinformation.
2 2 1 1
11Sub-Extractor 1
Sub-Extractor 2
Sub-Extractor 3
Sub-Extractor 4
Figure9:ThepatchescategorizedbytheiractivatedSub-ExtractorsinMoF.Separatelydisplayed.
Table 5: Univariate long-term forecasting results of MoU on ETTh1, ETTh2 and ETTm1. Following setup of PatchTST (Nie
etal.2023),look-backwindowisfixedto336withpredictedlengthsT ∈{96,192,336,720}.Bestresultsareshowninbold.
Model MoU(Ours) ModernTCN PatchTST DLinear
Metric MSE MAE MSE MAE MSE MAE MSE MAE
96 0.052 0.176 0.055 0.179 0.055 0.179 0.056 0.180
192 0.065 0.199 0.070 0.205 0.071 0.205 0.071 0.204
336 0.073 0.214 0.074 0.214 0.081 0.225 0.098 0.244
720 0.085 0.230 0.086 0.232 0.087 0.232 0.189 0.359
96 0.126 0.278 0.124 0.274 0.129 0.282 0.131 0.279
192 0.155 0.316 0.164 0.321 0.168 0.328 0.176 0.329
336 0.173 0.341 0.171 0.336 0.185 0.351 0.209 0.367
720 0.198 0.361 0.228 0.384 0.224 0.383 0.276 0.426
96 0.026 0.123 0.026 0.121 0.026 0.121 0.028 0.123
192 0.039 0.150 0.040 0.152 0.039 0.150 0.045 0.156
336 0.052 0.173 0.053 0.173 0.053 0.173 0.061 0.182
720 0.073 0.209 0.073 0.206 0.074 0.207 0.080 0.210
Wincount 18 10 5 0
Sub-Extractor 1 Sub-Extractor 2 Sub-Extractor 3 Sub-Extractor 4
MoF vs. MoE MoE has various applications. Originally,
MoE is used to process data which naturally can be par- 0.2 0.2 0.2 0.2
titioned into multiple subsets by topic or domains (Jacobs
etal.1991).Then,(Shazeeretal.2017)reportsthattheap- 0.2 0.2 0.2 0.2
plication of MoE in Recurrent Neural Networks (RNN) to
enlarge model capacity for very large datasets while main- -0.2 -0.2 -0.2 -0.2
tainingefficiency.Further,(Lepikhinetal.2020)extendthe Figure10:Visualizationofsub-extractorparameters.
studyofMoEtoTransformers.OurMoFisbasedonMoE,
and has similar structure with (Shazeer et al. 2017). But
We also extract the parameters in Sub-Extractors on
there are still some differences. First, MoF is not designed
datasetETTh1.Thevisualizationoftheirlinearmatricesare
forenlargingcapacityforlargedatasets,butfocusonsmall
displayed in Figure 10. Axis-0 denotes input dimension of
datasets with very divergent contexts (such as time series
patch,whichisthesamevaluewithpatchsize,whileaxis-1
patches).Second,MoFisaimedtoconsidervariousseman-
denotesoutputdimensionsetto64onETTh1.
ticcontexthiddenwithintimeseriespatches,whichhaveim-
plicitmeanings,whileotherapplicationsofMoEaremainly
designed for language modeling, which have explicit dif-
F MetricsIllustration
ferentmeanings.Therefore,despitesimilarstructures,MoF
worksfordifferentpurposeandprocessesdifferentdatatype
comparedtoexistingMoEmodels. We use mean square error (MSE) and mean absolute error
(MAE)asourmetricsforevaluationofallforecastingmod-
12
1hTTE
2hTTE
1mTTETable6:Impactoflook-backwindowstoperformanceofMoU formultivariatelong-termforecasting.Wetestfourlook-back
windowsL∈{192,336,512,720}.Bestresultsareshowninbold.
L 192 336 512 720
Metric MSE MAE MSE MAE MSE MAE MSE MAE
96 0.375 0.400 0.358 0.393 0.366 0.402 0.381 0.416
192 0.418 0.423 0.402 0.418 0.435 0.446 0.429 0.445
336 0.410 0.428 0.389 0.418 0.422 0.447 0.420 0.449
720 0.488 0.488 0.440 0.462 0.461 0.479 0.477 0.494
96 0.275 0.337 0.266 0.332 0.257 0.329 0.264 0.334
192 0.338 0.378 0.319 0.369 0.312 0.367 0.323 0.375
336 0.322 0.376 0.305 0.369 0.303 0.367 0.310 0.375
720 0.395 0.428 0.379 0.422 0.382 0.427 0.389 0.433
96 0.170 0.255 0.167 0.256 0.166 0.257 0.164 0.256
192 0.228 0.294 0.220 0.294 0.220 0.294 0.218 0.294
336 0.286 0.334 0.278 0.331 0.270 0.329 0.268 0.327
720 0.368 0.383 0.359 0.384 0.351 0.380 0.346 0.377
els.ThencalculationofMSEandMAEcanbedescribedas:
L+T
MSE= 1 (cid:88) (cid:16) Xˆ −X (cid:17)2 (23)
T i i
i=L+1
MAE=
1 L (cid:88)+T (cid:12)
(cid:12)Xˆ −X
(cid:12)
(cid:12) (24)
T (cid:12) i i(cid:12)
i=L+1
whereXˆ ispredictedvectorwithT futurevalues,whileX
isthegroundtruth.
G MoreExperiments
Univariate long-term forecasting. We also conduct uni-
variate long-term forecasting experiments on four ETT
datasets, focusing on oil temperature as the primary vari-
able of interest. The results, presented in Table 5, demon-
strate that our proposed MoU outperforms baseline models,
includingModernTCN,PatchTST,andDLinear.
Impact of input length The length of look-back win-
dowhassignificantimpactontheperformanceofmodel,as
longerlook-backwindowindicatesmorehistoricalinforma-
tion.However,informationfromtoodistantpastmayharm
theaccuracyofcurrentpredictions.Thus,weconductexper-
iments to evaluate MoU’s performance with four look-back
windowsL∈{192,336,512,720},asshowninTable6.
13
1hTTE
2hTTE
2mTTE