TEDRA: Text-based Editing of Dynamic and Photoreal Actors
BasavarajSunagad1 HemingZhu1† MohitMendiratta1† AdamKortylewski1,3
ChristianTheobalt1,2 MarcHabermann1,2∗
1MaxPlanckInstituteforInformatics,SaarlandInformaticsCampus,
2Saarbru¨ckenResearchCenterforVisualComputing,InteractionandAI
3UniversityofFreiburg
{bsunagad, hezhu, mmendira, akortyle, theobalt, mhaberma}@mpi-inf.mpg.de
Abstract highly realistic and easily animatable avatars presents sig-
nificantchallengesduetotheintricateanddiversenatureof
Overthepastyears,significantprogresshasbeenmade humangeometryandappearance. Whilecreatingandedit-
in creating photorealistic and drivable 3D avatars solely ing highly realistic avatars is possible, it remains a time-
from videos of real humans. However, a core remaining intensive and manual process requiring substantial exper-
challenge is the fine-grained and user-friendly editing of tise. Achievingaspecificindividual’slikenessfurtheradds
clothing styles by means of textual descriptions. To this complexitytothis.
end, we present TEDRA the first method allowing text- In the past years, text-driven image synthesis attracted
basededitsofanavatar,whichmaintainstheavatar’shigh the attention of the research community, as text is one of
fidelity,space-timecoherency,aswellasdynamics,anden- themostuser-friendlydatamodalitiesthatcanbeeasilyde-
ables skeletal pose and view control. We begin by train- ployedwithoutanyexpertknowledge. Thankstothewide
ing a model to create a controllable and high-fidelity dig- development and adaptation of transformers [47, 59] and
ital replica of the real actor. Next, we personalize a pre- diffusion models [49], several works have shown the abil-
trainedgenerativediffusionmodelbyfine-tuningitonvar- itytoeditimagesin2D[2,50]and3D[14,23,46], given
ious frames of the real character captured from different a text prompt as input. While 2D-based methods produce
cameraangles,ensuringthedigitalrepresentationfaithfully visually convincing results, they most likely can not pro-
captures the dynamics and movements of the real person. duce edits that are 3D-consistent. In contrast, 3D-based
Thistwo-stageprocesslaysthefoundationforourapproach methods[14,23,46]showresultsona3Dvolumethatcan
to dynamic human avatar editing. Utilizing this personal- be rendered faithfully from an arbitrary camera viewpoint.
ized diffusion model, we modify the dynamic avatar based However,suchmethodsaremostlylimitedtostaticscenes.
on a provided text prompt using our Personalized Normal Several methods have been proposed to generate 3D
Aligned Score Distillation Sampling (PNA-SDS) within a avatars from textual descriptions by distilling the 2D prior
model-based guidance framework. Additionally, we pro- ofgenerativemodelsinto3Davatarrepresentations[18,21,
pose a time step annealing strategy to ensure high-quality 28]. However, despite the promising results, these meth-
edits. Our results demonstrate a clear improvement over ods often fail to adequately capture the dynamic and fine-
priorworkinfunctionalityandvisualquality. grained details such as clothing movement. These aspects
arecrucialforinteractiveanddynamicapplications.
Recent efforts [37, 52] have focused on modifying dy-
1.Introduction namic avatars while preserving their underlying motion.
However,theseapproachesarerestrictedtotheupperbody
Digital avatars of real humans play a vital role in vari- [37, 52] and struggle to generalize to novel poses [37]. A
ous applications, including augmented and virtual reality, significantandopenchallengepersistsinseamlesslyapply-
gaming, movie production, and synthetic data generation ingtext-basededitstohighlyrealisticandcontrollablefull-
[10, 11, 13, 29, 30, 35, 45, 66, 72]. However, creating bodyavatarsofrealhumans.Thecriticalrequirementisthat
these edits must maintain spatio-temporal consistency, dy-
†Equalcontribution.
namics,andthehighfidelityoftheoriginalavatar,allwhile
*Correspondingauthor.
Projectpage:vcai.mpi-inf.mpg.de/projects/Tedra adheringtouser-specifiedmodifications.
1
4202
guA
82
]VC.sc[
1v59951.8042:viXraFigure1. Weproposeamethodfortext-basededitingofdynamicandphotorealactors(TEDRA).Ourapproacheditsapre-trainedneural
3Dhumanavataraccordingtoauser-definedtextprompt. Importantly, wepreservetheoriginaldynamicsandviewconsistencyofthe
digitalavatarwhilealsosatisfyingthedesirededit.
In this work, we propose TEDRA, the first text-based enables high-frequency edits. In summary, our contribu-
method for editing the appearance of a dynamic full-body tionsare:
avatarwhilepreservingintricatedetails(seeFig.1).Ourap- • WeintroduceTEDRA,amethodforeditingdynamic3D
proachassumesafull-bodyavatarasinput,whichistrained full-body avatars based on textual input. Our approach
frommulti-viewvideo. Inparticular,ourworkbuildsupon combines neural volumetric scene representations with
TriHuman [74], where the avatar representation is mod- text-driven diffusion models. This allows for precisely
eled as a signed distance and radiance field anchored to editingdynamicdigitalavatarswhilepreservingdetailed
anexplicitanddeformablemeshtemplate,allowingfastin- wrinklepatternsandensuringseamlessanimatability.
ference of photorealistic human appearance and geometry. • We propose a novel technique, termed as Personal-
Throughapre-trainingstage,adrivableandphotorealdigi- izedNormalAlignedScoreDistillationSampling(PNA-
talavataroftherealactorisobtained. SDS),facilitatinghigh-qualitypersonalizededitingwhile
Our method enables the text-based editing of such a maintainingtheintegrityofdynamics.
dynamic volumetric avatar, ensuring spatial and temporal • We present windowed time-step annealing for score dis-
coherence. More precisely, we achieve editing through tillationfromtext-to-imagediffusionmodels, preventing
text-based conditional image generation utilizing a diffu- over-saturationartifactsandachievinghigh-qualityedits.
sionmodel. Wecontributeseveraltechnicaladvancements We conduct a comprehensive evaluation of our method,
to ensure that the editing of TEDRA is authentic, person- employing, both, subjective and numerical assessments
alized, and maintains visually convincing spatiotemporal through a user study and comparisons with related tech-
consistency. Initially, we subsample frames from multi- niques. Theresultsdemonstratethatourapproachnotonly
view videos to capture the avatar’s identity and dynam- generatesawiderangeoftext-basededitsbutalsomaintains
ics. We then fine-tune a pre-trained diffusion model with the integrity of the initial identity. Additionally, we show-
theseframesandauniquetextidentifiertocreateaperson- case animations of the edited avatars, further highlighting
alized generative model that captures the avatar’s detailed the temporal coherency and superior performance of our
characteristics. Buildingonthis,weintroducePersonalized methodcomparedtootherrelevantapproaches.
Normal-Aligned Score Distillation Sampling (PNA-SDS),
amodel-based,classifier-freemethodinspiredbyZhanget 2.RelatedWork
al. [71]. The method employs two latent diffusion mod-
els—onepersonalizedandonepre-trained—thatperformit- Diffusion-based Text-to-3D Generation and Editing. In
erativeeditsonthedynamicavatar. Thesediffusionmodels therealmofcomputergraphics, thesynthesisof3Dassets
are conditioned on rendered normals of the avatar to pre- fromtextualdescriptionshasemergedasacaptivatingarea
serve the dynamics while enhancing localized edits. Ad- ofresearch. TheseassetsaregeneratedusingScoreDistil-
ditionally, noise estimates from both the personalized and lation Sampling (SDS), a technique introduced in Dream-
pre-trained diffusion models are strategically combined at Fusion [46], which enables the generation of 3D content
specific timesteps to optimally balance the original avatar fromtextualinputsbyliftingtext-to-imagediffusionmod-
characteristicswiththeintendedmodifications. els to 3D domain. Fantasia3d [7] proposes a text-to-3D
Furthermore, topreventover-saturationartifacts, wein- contentcreationmethodbydisentanglingthemodelingand
troduceawindowedannealingstrategy,whichgraduallyre- learningprocessofgeometryandappearance. Meanwhile,
ducestheinfluenceofthepersonalizeddiffusionmodeland Hi-FA[75]introducesanoveltimestepannealingapproach
2Figure2. Anoverviewofourapproachfortext-driveneditingofdynamicandphotorealavatars(TEDRA).Ourapproachstartswitha
pre-trainedTriHumanmodelasthebasehumanrepresentation. Then,weleverageafine-tuneddiffusionmodelinconjunctionwithour
proposedPersonalizedNormalAlignedScoreDistillationSampling(PNA-SDS).ThePNA-SDStechniquethencomputesanormalaligned
model-basedscoredistillationsamplinglosstooptimizethehumanrepresentationtowardstheeditpromptwhilepreservingthesubject’s
characteristics.Thisprocessisfurtherenhancedbyincorporatinganannealingmechanism,whichgraduallyrefinestheeditingprocess.
thatprogressivelyreducesthesampledtimestepthroughout ever,alltheaboveworksrelyonaparametricbodymodelto
asingle-stageoptimizationprocess. InstructNerf2Nerf[14] createa3Davatar. Whendealingwithdiverseavatarsthat
employs an image-conditioned diffusion model for editing deviatesignificantlyfromtheparametricmodel,employing
NeRF scenes with text instructions. Given a Neural Ra- the original skinning weights in such instances will result
diance Field (NeRF) [39] representation of a scene and inanimationsperceivedasunrealistic. Secondly,andmost
respective multi-view images, the method uses an image- importantly,theydonotmodelthedynamicsofthesurface
conditioned diffusion model [3] to iteratively edit the in- andappearance,i.e. wrinklesandcastshadows.
putimageswhileoptimizingtheunderlyingscene. Incon- Recent works such as DynVideo-E [33] and Con-
trast, Re-PaintNerf [73] starts with the semantic selection trol4D [52] focus on 4D editing, ensuring temporal and
oftheobjecttobemodified,followedbyguidingtheNeRF spatialconsistencybyutilizinganunderlyingradiancefield
modelusingapre-traineddiffusionmodel. Alltheseworks representation. However, these methods do not prioritize
haveincommonthattheycreatestaticscenes,whichneither preservingthedetailsandfidelityoftheunderlyingavatar,
support direct animation nor modeling of piece-wise rigid nordotheyadequatelycapturedeformationssuchaswrin-
articulated objects and respective dynamics, e.g. motion- kles. Acloselyrelatedwork, AvatarStudio[37]introduces
inducedclothingdeformations. View-TimeSDSformaintainingconsistencyineditingthe
Diffusion-based Text-to-3D Avatar Generation. In the underlying facial avatar across multiple views. Neverthe-
fieldoftext-driven3Davatargeneration,aseriesofmethod- less, this method encounters challenges in generalizing to
ologies have been proposed recently, each tackling a spe- facialmotionsthatithasnotencounteredbeforeandfaces
cificchallenge. Forinstance,AvatarVerse[68]andAvatar- limitations in real-time rendering. Additionally, the use of
Craft[24]focusongeneratingavatarsbasedsolelyontex- view-specificpromptsinAvatarStudioleadstoanentangle-
tual input. In contrast, DreamAvatar [4] prioritizes creat- ment of camera and identity views, restricting its applica-
ing avatars with controllable poses and body types. Hu- bilitytodatasetswheretheperson’smovementislimited.
manNorm[20]takesauniqueapproach,enhancingtheper- Drivable 3D Avatars. Modeling high-fidelity, dynamic
ceivedrealismof3Davatarsbyfocusingonhow2Dinfor- 3D-clothed human avatars has been an emerging topic in
mationtranslatesto3Dgeometry. Severalmethods,includ- recent years. Here, we focus on the works related to the
ingDreamWaltz[21],DreamHuman[28],andTADA[32], modeling of drivable 3D avatars, which take only skeletal
combinetext-drivengenerationwithpre-builtbodymodels posesandvirtualcameraviewsasinputatinferencetimeas
tocreateanimatableavatars. Additionally,ZeroAvatar[65] theyaremostcloselyrelated.
andTeCH[22]aimtoimprovetheoverallqualityanddetail Previousresearchondrivableavatarscanbedividedinto
ofgeneratedavatars,whileHaveFun[67]tacklestheprob- two streams: mesh-based and hybrid approaches. Mesh-
lemofreconstructingavatarsfromjustafewphotos. How- based methods [5, 12, 53, 60] represent the shape and
3appearance of dynamic characters with drivable template 3.1.Preliminaries
meshes with static (dynamic) textures. However, the ren-
AvatarRepresentation. Amongtheexistingmethods, we
deringqualityisboundedbytheresolutionoftheunderly-
choose TriHuman [74] as the human representation for its
ingmeshtemplate.
abilitytogeneratehigh-quality,motion-aware,andcoherent
To improve the quality of both the generated geom- geometryandappearanceofdynamichumansinreal-time.
etry and rendering, hybrid approaches articulate implicit Asahybridrepresentation, TriHuman firstlyadoptsan
fields[15,54,55]orradiancefields[38,69]withtheexplicit explicit,skeleton-drivenhumantemplatemeshtodepictthe
shape proxies, i.e., 3D skeletons, parametric human body human character’s coarse geometry. To generate the tem-
models[26,36,42,44],orperson-specifictemplatemeshes platemesh,itlearnsthenon-rigiddeformationofthehuman
[13,29,34,74]. Aprevalentresearchtrend[1,6,9,19,25, template mesh in the canonical space in a graph-to-graph
31, 41, 56, 57, 62, 64] focuses on modeling dynamic hu- translationmanner[12]. Thenon-rigiddeformedcanonical
mansbymappingtheposedspacetoapose-agnosticcanon- template meshes are then transformed to the posed space
icalspace. Tobettermodelthepose-dependentappearance viaDualQuaternionSkinning[27].
ofhumans,recentstudies [10,13,29,35,45,66,72]incor-
Whilethetemplatemeshcapturesthemotion-awaredy-
porate motion-aware residual deformations in the canoni-
namicsofthehumancharacters, thefidelityoftheappear-
calized space. Among them, Neural Actor [35] and HD-
ance and geometry is bounded by the resolution of the
Humans[13]leveragethetexturespaceofthehumanbody
template mesh. To this end, TriHuman introduced a de-
mesh as local features to model dynamic human appear-
formablevolume,parameterizedwithaTri-plane,anchored
ances. Nevertheless, both methods require approximately
onthedeformablecharacter’stexturespace. Givenaspatial
5secondstorenderasingleframe. TriHuman[74]achieves
samplex alongmarchingraysintheposedspace, TriHu-
j
real-time rendering and geometry generation through a
man non-rigidlymapsthesamplex tothetexturevolume
j
deformable tri-plane anchored on the motion-controllable
bridged by the pose-deformed template mesh through in-
template mesh. The rendering and geometry quality is on
verseskinning. Therespectivepositionofx inthetexture
j
par with, or even better than, the previous offline methods
volumeisdenotedasu . ThesampledTri-plane’sfeatures,
j
and significantly excels the real-time methods. We, there-
denotedasF atpositionu arefurtherfedintotwoshal-
j,f j
fore, take it as our underlying drivable avatar representa-
low MLPs, i.e., the shape MLP H , and the color MLP
sdf
tion. Nevertheless,alltheseapproachesexhibitalimitation
H , to produce the corresponding SDF s and color
col j,f
as they do not allow for text edits and solely animate the
valuec .
j,f
clothingpresentedinthevideo.
H (F ,p(u )=s ,q (1)
sdf j,f j j,f j,f
H (q ,n ,p(d))=c . (2)
3.Method col j,f j,f j,f
where q denotes the motion-aware local shape features,
Weaimtoeditapre-trained3Dhumanavatar,learnedfrom j,f
n is the surface normal, p indicates positional encod-
multi-view video data, using textual prompts that spec- j,f
ing[38],f denotestheframeindex,anddistheraydirec-
ify desired changes, such as ”Man wearing a hoodie” (see
tion. Lastly, unbiasedvolumerendering[61]isadoptedto
Fig.2). Themainchallengeistomaintaintheavatar’sover-
integrate the ray samples and generate the final rendering.
all characteristics, transfer the original clothing dynamics,
We refer to the original work [74] and our supplemental
andachievethespecifiededits.
documentformoredetails.
We leverage the recent state-of-the-art neural 3D avatar Latent Diffusion Models. In Latent Diffusion Mod-
representation, TriHuman [74], due to its high geometric els (LDMs) [49], an encoder function E maps a high-
and visual quality as well as its real-time performance. dimensional data point x, e.g., an image, into a lower-
The challenge lies in preserving intricate dynamics, such dimensional latent representation z = E(x). This latent
as wrinkles and other details, from the pre-trained human space representation is then subject to a diffusion process
avatarduringtheeditingprocess. Toachievethis, wepro- [17,40],aMarkovchainofT steps. Eachstepaddsasmall
pose a novel score distillation-based approach, which ef- amountofGaussiannoise,graduallytransformingthedata
fectivelyleveragestheeditcapabilitiesoflargetext-guided into a noise distribution. Mathematically, this process can
latentdiffusionmodels(LDM)[8,49]tocoherentlymodify bedescribedas:
the motion-aware geometry and appearance generated by
√ √
TriHuman. Next, before we introduce TEDRA (Sec. 3.2), z = α z+ 1−α ϵ,ϵ∼N(0,I). (3)
t t t
we first discuss the necessary foundations, i.e., the avatar
representation, LDMs, and Score Distillation Sampling where the diffusion timestep t ranges from 1 ≤ t ≤ T.
(Sec.3.1). The reverse diffusion process in LDMs aims to gradually
4denoisethelatentrepresentationtogeneratenewdatasam- fewimages(4-5)ofthesubject. Whileeffectiveforgener-
ples. The reverse process is modeled by a neural network ating novel images, it is not suitable for consistently edit-
ϵ , whereϕrepresentsthetrainableparametersofthenet- ing articulated and animatable avatars. Unlike AvatarStu-
ϕ
work. This network learns to predict the noise ϵ added at dio’s[37]view-timefine-tuningschemeforshort-sequence
each step, enabling the model to reverse the diffusion pro- facialeditswith8-10images,editinganimatablefull-body
cess. Diffusion models, akin to various other generative avatars is more complex and requires the LDM to manage
models,areinherentlycapableofcapturingconditionaldis- novelposesandextensiveview-dependentappearances,ne-
tributions denoted as p(x|c), where c represents a condi- cessitating long sequences with a large number of frames
tioning variable. The learning process for the conditional for training. Furthermore, the fine-tuning strategy strug-
latentdiffusionmodelsthenminimizes gles with large datasets to generate accurate view-time to-
kensamples.
E ϵ∼N(0,1),t(cid:2) ∥ϵ−ϵ ϕ(z t,t,c)∥2 2(cid:3) . (4) Toaddressthis,weproposeamorecomprehensivefine-
tuningstrategythatencompassesallconceivableposesand
Score Distillation Sampling (SDS). Score Distillation
viewpoints, aiming to accurately represent the full spec-
Sampling[46]optimizesanimplicit3Drepresentationfrom
trumofhumansurfacedynamicsandappearances. Weren-
textualdescriptionstogenerateaview-consistentsceneus-
der multi-view and multi-pose images denoted as [x ;i ∈
i
ing pre-trained 2D text-to-image diffusion models. The
{1,...,n}] from the pre-trained TriHuman model for fine-
approach constructs the scene through a differentiable im-
tuning the UNet and the text encoder of the LDM. The
age parameterization, employing a differentiable genera-
fine-tuning process is further refined by incorporating an
tor G that produces 2D images x from 3D scene parame-
identity-specific token alongside a class noun within the
ters θ. Themethodutilizes acombinationofa pre-trained
prompt. This results in a structured prompt of the form
and personalized diffusion model to derive a score func-
’aphotoofasksman/woman’, where’sks’istheidentity-
tionΨ (x ,y,t). Thisscorefunctionestimatesthenoiseϵ,
ϕ t specifictoken.
giventhenoisyimagex t,textembeddingy,andnoiselevel We then fine-tune the UNet, denoted as ζˆ and text en-
ϕ
t. Thescorefunctioniscrucialfordeterminingthegradient
coder Γ, so the LDM can regenerate an image x from an
i
directionforthesceneparameterupdates θ. Thegradient,
initial noise map ϵ ∼ N(0,I) and a conditioning vector
essentialforupdatingtheseparameters,iscomputedas
s = Γ(P), derived using a text encoder, conditioned on
(cid:20) ∂x(cid:21) a text prompt P. The fine-tuning of ζ θ and Γ is super-
∇ L (ϕ,x)=E µ(t)(Ψ (x ;y,t)−ϵ) . (5) visedwithasquarederrorlossfunctionandaclass-specific
θ SDS t,ϵ ϕ t ∂θ
priorpreservationloss[50]. Thesquarederrorlossforde-
noising a variably-noised image or latent code, given by
Here,µ(t)isaweightingbasedonthediffusiontimestept.
z =α E(x )+β ϵ,isexpressedas
t,i t i t
3.2.ProposedMethod
(cid:104) (cid:105)
E w ∥ζˆ (z ,s)−E(x )∥2 , (6)
Westartbyemployingthepre-trainedTriHumanmodelto xi,si,ζ,t t ϕ t,i i 2
generate images displaying diverse views and poses. Sub-
whereα ,β ,andw controlthenoiseschedule.
sequently, we fine-tune the pre-trained Stable Diffusion t t t
model[48]onthesegeneratedimages,utilizinganidentity- Afterfine-tuningtheLDM,weuseapre-trainednormal-
specificprompt(Sec.3.2.1). Toensuretheaccurateediting aligned ControlNet [70] encoder conditioned with a null
ofthepre-trainedavatar,weintroduceournovelPNA-SDS prompttocontrolthevirtualcameraviewandskeletalpose
loss (Sec. 3.2.2). This loss is computed through model- inthegeneratedimages.
based classifier-free guidance [16], guided by the normal-
alignedControlNet[70].Finally,weintroduceourwindow-
3.2.2 PersonalizedNormal-alignedModel-basedScore
root timestep annealing strategy (Sec. 3.2.3), specifically
Distillation(PNA-SDS)
designedfordiffusion-guidedtext-to-3dediting.
Toenablesubject-consistent,view,andpose-dependented-
its,wecomputetheeditscorewithmodel-basedclassifier-
3.2.1 Fine-tuningtheLatentDiffusionModel
free guidance [16] for updating the pretrained TriHuman
Indynamicfull-bodyediting,themainchallengeispreserv- model. This is done by interpolating the noise estimates
ingthebody’soriginalcharacteristics,suchasidentity,cloth from the fine-tuned model ζˆ and pre-trained stable diffu-
ϕ
deformationdetails,andmotions,ratherthancompletelyal- sion model ζ , respectively, at specific timesteps. Our 3D
ϕ
teringitsappearance. DreamBooth[50]addressesidentity- human representation can generate surface normal maps,
specific image generation by fine-tuning the Latent Diffu- which guide the diffusion models via our normal-aligned
sion Model (LDM) with an identity-specific prompt and a pre-trained ControlNet. We adopt a parameter k based on
5the diffusion timestep t to determine whether or not to in- Eq. 7. When t is deterministically chosen, as in HiFA, we
terpolatebetweenthescores,whicheffectivelybalancesbe- facethefollowinglimitations:
tweeneditsandidentity. • With a fixed threshold k = 600, model-based guidance
Givenarenderedimagex,weobtainitslatentrepresen- ceasesaftert>k,leadingtoalossofidentityduetothe
tationzusingtheVAEofthestablediffusion:z=E(x).We lackofinfluencefromthepersonalizedmodelinlaterit-
then uniformly sample a diffusion timestep t ∼ U(t ,t ), erations(refertoPNA-SDS+HiFAannealinginSec.4.3).
1 2
wheret andt representtheupperandlowerlimitsofthe • Constant model-based guidance (k = 0) restricts edit
1 2
diffusion noise timestep t. These limits are introduced in flexibilityandresultsinblurryartifacts.
thenextparagraph,wherewediscusstheannealingprocess. Thus, it is important to stochastically sample t to balance
Weapplythissampledtimesteptointroducenoisetothein- identitypreservationandeditflexibility.
putlatent,resultinginanoisedlatentz . To address this, we introduce a windowed square root
t
Letcrepresenttheembeddingoftheeditingtextprompt annealingstrategyspecificallydesignedtomodulatethean-
(e.g.,’aphotoofamanwearingahoodie’),ˆcrepresentthe nealingoftimestepswhileallowingrandomsampling. This
textembeddingofthepromptfortheidentity(e.g.,’aphoto approachensuresamorecontrolledandgradualprogression
of a sks man’), and n (Eq. 2) represents the normal map oftimestepsduringthetrainingprocess.
rendered with TriHuman. Then, we can obtain the editing Given the total number of iterations N and the current
scoreasfollows: iteration τ, along with a specified window size w, our an-
nealingvaluesare:
Ψ(zt,t,c,cˆ,n)=w((1−v)ζ ϕ(zt,c,n)+vζˆ ϕ(zt,cˆ,n)) (cid:114) τ w
(cid:40) 0.3 ift>k, (7) t 1 =t max−(t max−t min)× N, k =t 1− 2, t 2 =t 1−w.
+(1−w)ζ ϕ(zt,n),v=
0 otherwise. (8)
Here, t and t represent the maximum and minimum
wherewistheoverallguidanceweightandvstandsforthe max min
diffusiontimesteps,respectively. Thewindow[t ,t ]isthe
1 2
modelguidanceweight.WeuseEq.7forthenoiseestimate
rangewithinwhichtisrandomlysampled. Theparameter
andk representsathresholdtimestepforusingapersonal-
ksignifiesthetimestepforusingthepersonalizeddiffusion
izeddiffusionmodel. Itisimportanttonotethatincorporat-
model. This dynamic window adapts throughout the an-
ingsurfacenormalsasaconditionforbothtext-guidedand
nealingprocess,facilitatingabalancebetweenestablishing
null prompts is essential to maintain spatial relationships
editsemanticsathighertimestepsandaddingfinedetailsat
and surface orientations within the estimated score. Our
lowertimesteps,therebymitigatingissuesofoversaturation.
PersonalizedNormalAligned-SDS,combinedwithacom-
Moreover, the weight v of the personalized diffusion
prehensivefine-tuningstrategy,excelsingeneralizingedits
model is annealed to enhance the faithfulness of the edits
tonovelviewsandposesnotseeninthefine-tuningdataset.
to the input prompt. This approach not only improves the
InFig.5,weshowtheeffectivenessofPersonalizedNormal
semantic integrity during initial higher timesteps but also
Aligned-SDScomparedwithotherSDSvariants.
ensures the preservation of fine details as the process pro-
gressestolowertimesteps.
3.2.3 WindowedRootTimestepAnnealing
4.Experiments
Similartopreviousstudies[7,63,75],ourexperimentalre-
sults indicate that Score Distillation Sampling (SDS) en- Our evaluation focuses on three key aspects: 1) Ensuring
counters notable challenges related to over-saturation and alignmentwiththetargettextpromptwhilemaintainingthe
loss of fine details, particularly when a large timestep t subject’s inherent characteristics; 2) The capability to pro-
is randomly selected. In diffusion models, the higher duce3Dconsistenteditsforhigh-qualityfree-viewrender-
timesteps correspond to the semantics of the image, while ing; 3) The temporal coherency of the generated edits, en-
the lower timesteps correspond to finer details [43]. Thus, ablingdynamicreplayandskeletonanimations.
foraneditingtask,itiscrucialtoestablishtheeditseman-
ticsearlyonandthenmovetoaddfinerdetails. Dataset. We conduct experiments on one subject (wear-
Several annealing strategies have been proposed [7, 63, ing shorts and a shirt) of the DynaCap dataset [12], which
75], but none of them are suitable for an editing task. is recorded in a calibrated multi-camera studio. In addi-
HiFA [75] proposed a square root annealing strategy for tion,wecapturedthreemoresubjectsinvariousclothingin
selecting the diffusion timestep t based on the iteration a similar studio setup. For training TriHuman, we obtain
step,directlycorrelatingthediffusionprocess’progression skeletal motion using markerless motion capture [58], and
with the training iteration. However, random sampling of foregroundmasksusingbackgroundmatting[51].Formore
t is crucial for model-based classifier-free guidance as per details,werefertothesupplementaldocument.
6Figure 3. Qualitative Results. We present the text-based visual editing results and the underlying geometry. Our method generates
compellingtext-drivenvisualedits,ensuring3Dandtemporalconsistencywhilealteringappearanceandgeometry. Werecommendthe
readerstozoominforbetterviewingofthedetails.
InsN2N[14] AvatarStudio[37] Ours
Q1 9.6 12.8 77.6
Q2 12 24 64
Q3 14.4 8 77.6
Q4 11.2 10.4 78.4
Table1.UserStudy.Resultsfromourstudywith25participants.
OurapproachoutperformsAvatarStudioandInstructNeRF2NeRF
(InsN2N)byalargemargin.
in Fig. 3 (”Women wearing a suit”), TEDRA can mod-
ify not only the appearance but also the style of the out-
fits. The edited outfits significantly differ from the origi-
Figure4. QualitativeComparison. Ourapproachpreservesthe
nals in, both, style and appearance. (3) 3D and tempo-
subject’s characteristics and produces visually compelling edits
ral coherent: TEDRA edits, both, the appearance and the
thatalignwiththeprompts.
underlying geometry of the clothed human avatar to align
withthegiventextprompt,asevidentintherepresentations
4.1.QualitativeResults of Fig. 3 (”Women wearing military uniform”). The edits
seamlesslyintegratewiththedrivableavatarandremainco-
Fig. 3 presents the text-based edits upon a single identity
herent across various poses. Please see the supplemental
withvariousposesgeneratedbyTEDRA.Theresultsyield
materialsformoreresultsonnovelposesandviews.
by TEDRA shine in the following aspects: (1) Localized
andprecise:AsillustratedinFig.3(”Womenwearingsun-
4.2.QuantitativeEvaluation
glasses”),TEDRAyieldsfine-grainededitsonthetargetre-
gionwhilepreservingtheclothingwrinkledetailsfromthe Wecompareourmethodwithtworecent3Dtext-basededit-
original avatar. (2) Versatile and expressive: As shown ing approaches: InstructNeRF2NeRF [14] and AvatarStu-
7InsN2N[14] AvatarStudio[37] Ours
CLIPSimilarity↑ 0.1656 0.1412 0.2098
FIDScore↓ 98.932 116.732 80.74
Table2. WeadoptCLIPtext-imageDirectionSimilaritytoassess
the alignment of the edits with the text within the CLIP space.
Additionally,wereporttheFIDscorestoevaluatethepreservation
Figure5.AblationStudy.Ourfullmethodachievesvisuallyplau- ofidentityandgeometricaccuracyoftheavatars.
sibleeditsandpreservesthedynamicsoftheoriginalcharacter.
SDS NA-SDS P-SDS PNA-SDS PNA+HiFA Ours
dio [37]. To ensure a fair comparison, we use the same
trainingdataforInstructNeRF2NeRFandapplyAvatarStu- 0.1153 0.2256 0.1844 0.2005 0.1159 0.2409
dio’s full-head editing strategy to our full-body avatar.
Table 3. CLIP-Similarity scores corresponding to the ablations
Fig.4showstheoutcomesunderdifferenttextprompts.
presentedinFig.5. Notethateachofourdesignchoicesleadsto
InstructNeRF2NeRFexhibitspooralignmentwithtarget
animprovementcomparedtothebaselines.
prompts and may alter the original identity by editing in-
correct regions. AvatarStudio produces blurry results that
lack fine details like wrinkles and clothing dynamics. In
contrast,ourmethodretainsfinedetailsanddynamics,pro- SDS. As shown in Fig. 5, aligning the score with normals
ducingmorecoherenteditsalignedwiththetext. crucially helps to preserve geometric details that are lost
Given the difficulty of quantitative comparison against withstandardSDS.
ground truth in this setting, we adopted a user study ap-
Next, we highlight the importance of adopting the per-
proachfromAvatarStudio[37]. Eachsessionfeaturedfour
sonalized model to preserve the identity. Here, we com-
side-by-sidevideos: theoriginalinputandrandomlyshuf-
putethescoreusingourproposedloss, butweexcludethe
fledoutputsfromthreetext-drivenmethods. Weevaluated
normals to highlight the impact of the personalized model
three identities with three different prompts, totaling nine
(Eq. 7 with n = ⊘) termed as P-SDS. We observe an im-
videos, each 10-15 seconds long. Participants were asked
provementintheappearanceandoverallstructureoftheed-
torespondtothefollowingquestions:
itscomparedtovanillaSDS.
• Q1: Which method better preserves the identity of the
AlthoughthemodelwithPersonalized-SDS,i.e,P-SDS,
inputsequence(subjectconsistency)?
demonstrates significant improvements compared to stan-
• Q2: Whichmethodbetteradherestotheprovidedtextual
dard SDS, the absence of geometric guidance from nor-
prompt(promptpreservation)?
mals results in broken avatars. To this end, we use nor-
• Q3: Which method better maintains the animations and
mal guidance in our PNA-SDS formulation Eq. 7, which
dynamicsoftheoriginalmotion(temporalconsistency)?
furtherhelpspreservethegeometryaswellasthekeyfea-
• Q4: Which method performs better overall, considering
turesintheoriginalavatar. Yet,therandomsamplingofthe
thethreeaspectsabove?
timestep t leads to a diminished resolution of fine details,
Tab. 1 presents the user study results from 25 partici-
underscoringthenecessityforanannealingprocess.
pants, with our method consistently receiving the highest
ratings. Foroverallquality(Q4),ourmethodwaspreferred Additionally, weillustratetheineffectivenessofthean-
78.4%ofthetime, comparedtoAvatarStudio’s10.4%and nealing strategy proposed by HiFA [75] (termed as PNA-
InstructNeRF2NeRF’s11.2%. WealsoevaluateCLIPText- SDS+HiFAannealing)whenappliedtoourmethod.Here,
Image Direction Similarity and FID scores. Tab. 2 shows we set a constant value for k and deterministically select
that our method outperforms prior works in these metrics thevalueofdiffusiontimesteptbasedontheiterationstep.
byalargemargin. Thisisineffectiveforourapproachasoncet < k,theper-
sonalized model does not contribute to the score, resulting
4.3.Ablations inacompletebreakdownoftheavatar.
In Fig. 5 and Tab. 3, we conduct ablative studies to assess Instrikingcontrast,ourfullmethod,termedOurs,deliv-
theeffectivenessofourdesignchoices. ershigh-qualityeditswhilepreservingthecrucialdetailsof
Initially,weestablishthenecessityofincorporatingnor- thepre-trainedhumanavatar. TheCLIPscoresfortheabla-
malsasaconditionfor,both,theedit-promptandthenull- tionstudy(Tab.3),furtherprovethatourcompletemethod
prompt,weuseapre-traineddiffusionmodel[48]andCon- outperforms all design alternatives in terms of alignment
trolNet [70] (Eq. 7 with v = 0), which we term as NA- withthetextualpromptsandoverallvisualquality.
85.Conclusion Learningageneralizedanimatableneuralrepresentationfor
humanactors. IEEETVCG,2023. 1,4
In this work, we tackled the problem of intuitive editing
[11] Marc Habermann, Weipeng Xu, Michael Zollhofer, Ger-
of 3D neural avatars where a user can specify a desired
ard Pons-Moll, and Christian Theobalt. Deepcap: Monoc-
edit via text prompts. Our method then automatically ad- ular human performance capture using weak supervision.
juststheshapeandappearanceoftheneuralavatartofulfill In Proceedings of the IEEE/CVF Conference on Computer
the user’s demands while maintaining the subject’s visual VisionandPatternRecognition,pages5052–5063,2020. 1
coherence. At the technical core, we propose a Person- [12] MarcHabermann, LingjieLiu, WeipengXu, MichaelZoll-
alized Normal-Aligned Score Distillation Sampling and a hoefer,GerardPons-Moll,andChristianTheobalt.Real-time
windowedtimestepannealingtoensurespace-timeconsis- deepdynamiccharacters.ACMTOG,40(4),2021.3,4,6,1
tency in edits and high visual fidelity. Our results demon- [13] Marc Habermann, Lingjie Liu, Weipeng Xu, Gerard Pons-
strate a clear step towards more intuitive, high-fidelity 3D Moll, Michael Zollhoefer, and Christian Theobalt. Hd-
humans: A hybrid approach for high-fidelity digital hu-
neural avatar editing and outperform respective competing
mans. ProceedingsoftheACMonComputerGraphicsand
methods. While TEDRA facilitates the intuitive editing of
InteractiveTechniques,6(3):1–23,2023. 1,4
3D neural human avatars with space-time consistency and
[14] Ayaan Haque, Matthew Tancik, Alexei Efros, Aleksander
superior visual quality, it faces challenges with long train-
Holynski,andAngjooKanazawa. Instruct-nerf2nerf: Edit-
ingtimes.Futureworkwillfocusonenablingmoredetailed
ing 3d scenes with instructions. In Proceedings of the
editswhilereducingresourcedemands.
IEEE/CVF International Conference on Computer Vision,
2023. 1,3,7,8,4
References [15] PhilippHenzler,NiloyJMitra,andTobiasRitschel. Escap-
ing plato’s cave: 3d shape from adversarial rendering. In
[1] Alexander Bergman, Petr Kellnhofer, Wang Yifan, Eric
CVPR,pages9984–9993,2019. 4
Chan,DavidLindell,andGordonWetzstein.Generativeneu-
[16] Jonathan Ho and Tim Salimans. Classifier-free diffusion
ral articulated radiance fields. NeurIPS, 35:19900–19916,
guidance,2022. 5
2022. 4
[17] JonathanHo,AjayJain,andPieterAbbeel. Denoisingdiffu-
[2] TimBrooks,AleksanderHolynski,andAlexeiA.Efros. In-
sionprobabilisticmodels,2020. 4
structpix2pix:Learningtofollowimageeditinginstructions.
[18] Fangzhou Hong, Mingyuan Zhang, Liang Pan, Zhongang
InCVPR,2023. 1
Cai, Lei Yang, and Ziwei Liu. Avatarclip: Zero-shot
[3] TimBrooks,AleksanderHolynski,andAlexeiAEfros. In-
text-driven generation and animation of 3d avatars. ACM
structpix2pix:Learningtofollowimageeditinginstructions.
TransactionsonGraphics(TOG),41(4):1–19,2022. 1
In Proceedings of the IEEE/CVF Conference on Computer
[19] Shoukang Hu, Fangzhou Hong, Liang Pan, Haiyi Mei, Lei
VisionandPatternRecognition,pages18392–18402,2023.
Yang,andZiweiLiu. Sherf:Generalizablehumannerffrom
3
asingleimage. arXivpreprint,2023. 4
[4] YukangCao,Yan-PeiCao,KaiHan,YingShan,andKwan-
[20] XinHuang,RuizhiShao,QiZhang,HongwenZhang,Ying
YeeKWong. Dreamavatar: Text-and-shapeguided3dhu-
Feng, Yebin Liu, and Qing Wang. Humannorm: Learning
manavatargenerationviadiffusionmodels. arXivpreprint
normaldiffusionmodelforhigh-qualityandrealistic3dhu-
arXiv:2304.00916,2023. 3
mangeneration. arXiv,2023. 3
[5] Dan Casas, Marco Volino, John Collomosse, and Adrian
[21] YukunHuang,JiananWang,AilingZeng,HeCao,Xianbiao
Hilton. 4d video textures for interactive character appear-
Qi,YukaiShi,Zheng-JunZha,andLeiZhang. Dreamwaltz:
ance. Comput.Graph.Forum,33(2):371–380,2014. 3
Makeascenewithcomplex3danimatableavatars. 2023. 1,
[6] Jianchuan Chen, Ying Zhang, Di Kang, Xuefei Zhe, Lin- 3
chao Bao, Xu Jia, and Huchuan Lu. Animatable neural [22] YangyiHuang,HongweiYi,YuliangXiu,TingtingLiao,Ji-
radiance fields from monocular rgb videos. arXiv preprint axiangTang,DengCai,andJustusThies.Tech:Text-guided
arXiv:2106.13629,2021. 4 reconstructionoflifelikeclothedhumans,2023. 3
[7] RuiChen,YongweiChen,NingxinJiao,andKuiJia. Fan- [23] Ajay Jain, Ben Mildenhall, Jonathan T. Barron, Pieter
tasia3d: Disentangling geometry and appearance for high- Abbeel, and Ben Poole. Zero-shot text-guided object gen-
quality text-to-3d content creation. In Proceedings of the erationwithdreamfields. CVPR,2022. 1
IEEE/CVF International Conference on Computer Vision [24] Ruixiang Jiang, Can Wang, Jingbo Zhang, Menglei Chai,
(ICCV),2023. 2,6,1 MingmingHe,DongdongChen,andJingLiao. Avatarcraft:
[8] CompVis. Stablediffusion,2022. 4 Transformingtextintoneuralhumanavatarswithparameter-
[9] YaoFeng,JinlongYang,MarcPollefeys,MichaelJ.Black, izedshapeandposecontrol,2023. 3
and Timo Bolkart. Capturing and animation of body and [25] T.Jiang,X.Chen,J.Song,andO.Hilliges. InCVPR,pages
clothingfrommonocularvideo. InSIGGRAPHAsia2022 16922–16932,2023. 4
ConferencePapers,2022. 4 [26] Hanbyul Joo, Tomas Simon, and Yaser Sheikh. Total cap-
[10] QingzheGao,YimingWang,LibinLiu,LingjieLiu,Chris- ture:A3ddeformationmodelfortrackingfaces,hands,and
tian Theobalt, and Baoquan Chen. Neural novel actor: bodies. InCVPR,pages8320–8329,2018. 4
9[27] Ladislav Kavan, Steven Collins, Jiˇr´ı Zˇa´ra, and Carol [42] AhmedA.A.Osman,TimoBolkart,andMichaelJ.Black.
O’Sullivan. Skinningwithdualquaternions. InProceedings Star: Sparse trained articulated human body regressor. In
of the 2007 symposium on Interactive 3D graphics and ECCV,pages598–613,2020. 4
games,pages39–46,2007. 4 [43] Or Patashnik, Daniel Garibi, Idan Azuri, Hadar Averbuch-
[28] Nikos Kolotouros, Thiemo Alldieck, Andrei Zanfir, Ed- Elor, and Daniel Cohen-Or. Localizing object-level
uardGabrielBazavan,MihaiFieraru,andCristianSminchis- shape variations with text-to-image diffusion models. In
escu. Dreamhuman:Animatable3davatarsfromtext. 2023. Proceedings of the IEEE/CVF International Conference on
1,3 ComputerVision,pages23051–23061,2023. 6
[29] YoungjoongKwon,LingjieLiu,HenryFuchs,MarcHaber- [44] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani,
mann, and Christian Theobalt. Deliffas: Deformable light TimoBolkart,AhmedA.A.Osman,DimitriosTzionas,and
fieldsforfastavatarsynthesis. NeurIPS,2023. 1,4 MichaelJ.Black. Expressivebodycapture: 3dhands,face,
[30] Ruilong Li, Kyle Olszewski, Yuliang Xiu, Shunsuke Saito, and body from a single image. In CVPR, pages 10975–
ZengHuang,andHaoLi.Volumetrichumanteleportation.In 10985,2019. 4
ACMSIGGRAPH2020Real-TimeLive!,pages1–1.2020. [45] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan
1 Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Ani-
[31] RuilongLi,JulianTanke,MinhVo,MichaelZollhofer,Jur- matableneuralradiancefieldsformodelingdynamichuman
genGall,AngjooKanazawa,andChristophLassner. Tava: bodies. InICCV,pages14314–14323,2021. 1,4
Template-freeanimatablevolumetricactors. 2022. 4
[46] BenPoole,AjayJain,JonathanT.Barron,andBenMilden-
[32] Tingting Liao, Hongwei Yi, Yuliang Xiu, Jiaxaing Tang,
hall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv,
Yangyi Huang, Justus Thies, and Michael J Black.
2022. 1,2,5
Tada! text to animatable digital avatars. arXiv preprint
[47] AlecRadfordandKarthikNarasimhan. Improvinglanguage
arXiv:2308.10899,2023. 3
understandingbygenerativepre-training. 2018. 1
[33] Jia-Wei Liu, Yan-Pei Cao, Jay Zhangjie Wu, Weijia Mao,
[48] Robin Rombach, A. Blattmann, Dominik Lorenz, Patrick
Yuchao Gu, Rui Zhao, Jussi Keppo, Ying Shan, and
Esser, and Bjo¨rn Ommer. High-resolution image synthesis
MikeZhengShou.Dynvideo-e:Harnessingdynamicnerffor
with latent diffusion models. 2022 IEEE/CVF Conference
large-scale motion- and view-change human-centric video
onComputerVisionandPatternRecognition(CVPR),pages
editing,2023. 3
10674–10685,2021. 5,8
[34] LingjieLiu,JiataoGu,KyawZawLin,Tat-SengChua,and
[49] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
ChristianTheobalt.Neuralsparsevoxelfields.NeurIPS,33:
PatrickEsser,andBjo¨rnOmmer.High-resolutionimagesyn-
15651–15663,2020. 4
thesiswithlatentdiffusionmodels. InComputerVisionand
[35] LingjieLiu,MarcHabermann,ViktorRudnev,Kripasindhu
PatternRecognition(CVPR),2022. 1,4
Sarkar, Jiatao Gu, and Christian Theobalt. Neural actor:
[50] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,
Neuralfree-viewsynthesisofhumanactorswithposecon-
MichaelRubinstein,andKfirAberman. Dreambooth: Fine
trol. ACMTrans.Graph.(ACMSIGGRAPHAsia),2021. 1,
tuning text-to-image diffusion models for subject-driven
4
generation. 2022. 1,5
[36] Matthew Loper, Naureen Mahmood, Javier Romero, Ger-
[51] SoumyadipSengupta, VivekJayaram, BrianCurless, Steve
ard Pons-Moll, and Michael J. Black. SMPL: A skinned
Seitz, andIraKemelmacher-Shlizerman. Backgroundmat-
multi-person linear model. ACM Trans. Graphics (Proc.
ting: The world is your green screen. In CVPR, 2020. 6,
SIGGRAPHAsia),34(6):248:1–248:16,2015. 4
1
[37] MohitMendiratta,XingangPan,MohamedElgharib,Kartik
Teotia,MallikarjunBR,AyushTewari,VladislavGolyanik, [52] Ruizhi Shao, Jingxiang Sun, Cheng Peng, Zerong Zheng,
Adam Kortylewski, and Christian Theobalt. Avatarstudio: Boyao Zhou, Hongwen Zhang, and Yebin Liu. Con-
Text-driveneditingof3ddynamichumanheadavatars.ACM trol4d: Dynamic portrait editing by learning 4d gan from
Trans.Graph.,42(6),2023. 1,3,5,7,8,4 2ddiffusion-basededitor. arXivpreprintarXiv:2305.20082,
[38] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, 2023. 1,3
JonathanT.Barron,RaviRamamoorthi,andRenNg. Nerf: [53] Aliaksandra Shysheya, Egor Zakharov, Kara-Ali Aliev,
Representingscenesasneuralradiancefieldsforviewsyn- Renat Bashirov, Egor Burkov, Karim Iskakov, Aleksei
thesis. InECCV,2020. 4 Ivakhnenko,YuryMalkov,IgorPasechnik,DmitryUlyanov,
[39] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, etal. Texturedneuralavatars. InCVPR,pages2387–2397,
JonathanTBarron,RaviRamamoorthi,andRenNg. Nerf: 2019. 3
Representingscenesasneuralradiancefieldsforviewsyn- [54] Vincent Sitzmann, Justus Thies, Felix Heide, Matthias
thesis. CommunicationsoftheACM,65(1):99–106, 2021. Nießner, GordonWetzstein, andMichaelZollho¨fer. Deep-
3 voxels: Learning persistent 3d feature embeddings. In
[40] AlexNicholandPrafullaDhariwal.Improveddenoisingdif- CVPR,2019. 4
fusionprobabilisticmodels,2021. 4 [55] Vincent Sitzmann, Michael Zollho¨fer, and Gordon Wet-
[41] Atsuhiro Noguchi, Xiao Sun, Stephen Lin, and Tatsuya zstein. Scene representation networks: Continuous 3d-
Harada. Neuralarticulatedradiancefield. InICCV,2021. structure-aware neural scene representations. In NeurIPS,
4 2019. 4
10[56] Shih-Yang Su, Frank Yu, Michael Zollhofer, and Helge onComputerVisionandPatternRecognition(CVPR),pages
Rhodin. A-nerf:Articulatedneuralradiancefieldsforlearn- 6027–6037,2022. 2
ing human shape, appearance, and pose. NeurIPS, 34: [72] Zerong Zheng, Xiaochen Zhao, Hongwen Zhang, Boning
12278–12291,2021. 4 Liu, and Yebin Liu. Avatarrex: Real-time expressive full-
[57] Shih-Yang Su, Timur Bagautdinov, and Helge Rhodin. bodyavatars. ACMTOG,42(4),2023. 1,4
Danbo:Disentangledarticulatedneuralbodyrepresentations [73] XingchenZhou,YingHe,F.RichardYu,JianqiangLi,and
viagraphneuralnetworks. InECCV,2022. 4 YouLi. Repaint-nerf: Nerfedittingviasemanticmasksand
[58] TheCaptury.TheCaptury.http://www.thecaptury. diffusionmodels,2023. 3
com/,2020. 6,1 [74] HemingZhu,FangnengZhan,ChristianTheobalt,andMarc
[59] AshishVaswani,NoamShazeer,NikiParmar,JakobUszko- Habermann. Trihuman : A real-time and controllable tri-
reit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Il- plane representation for detailed human geometry and ap-
lia Polosukhin. Attention is all you need. In Advances in pearancesynthesis,2023. 2,4,1
NeuralInformationProcessingSystems.CurranAssociates, [75] JunzheZhuandPeiyeZhuang.Hifa:High-fidelitytext-to-3d
Inc.,2017. 1 generationwithadvanceddiffusionguidance,2023. 2,6,8,
[60] Marco Volino, Dan Casas, John Collomosse, and Adrian 3
Hilton. Optimal representation of multiple view video. In
BMVC.BMVAPress,2014. 3
[61] PengWang,LingjieLiu,YuanLiu,ChristianTheobalt,Taku
Komura,andWenpingWang.Neus:Learningneuralimplicit
surfacesbyvolumerenderingformulti-viewreconstruction.
Advances in Neural Information Processing Systems, 34:
27171–27183,2021. 4
[62] Shaofei Wang, Katja Schwarz, Andreas Geiger, and Siyu
Tang.Arah:Animatablevolumerenderingofarticulatedhu-
mansdfs. InECCV,2022. 4
[63] ZhengyiWang,ChengLu,YikaiWang,FanBao,Chongxuan
Li,HangSu,andJunZhu.Prolificdreamer:High-fidelityand
diversetext-to-3dgenerationwithvariationalscoredistilla-
tion.InAdvancesinNeuralInformationProcessingSystems
(NeurIPS),2023. 6
[64] Chung-Yi Weng, Brian Curless, and Ira Kemelmacher-
Shlizerman. Vid2actor: Free-viewpoint animatable per-
son synthesis from video in the wild. arXiv preprint
arXiv:2012.12884,2020. 4
[65] ZhenzhenWeng,ZeyuWang,andSerenaYeung.Zeroavatar:
Zero-shot3davatargenerationfromasingleimage,2023. 3
[66] Hongyi Xu, Thiemo Alldieck, and Cristian Sminchisescu.
H-nerf: Neural radiance fields for rendering and temporal
reconstruction of humans in motion. NeurIPS, 34:14955–
14966,2021. 1,4
[67] Xihe Yang, Xingyu Chen, Daiheng Gao, Xiaoguang Han,
andBaoyuanWang.Have-fun:Humanavatarreconstruction
from few-shot unconstrained images. arXiv:2311.15672,
2023. 3
[68] Huichao Zhang, Bowen Chen, Hao Yang, Liao Qu, Xu
Wang,LiChen,ChaoLong,FeidaZhu,KangDu,andMin
Zheng.Avatarverse:High-quality&stable3davatarcreation
fromtextandpose,2023. 3
[69] Kai Zhang, Gernot Riegler, Noah Snavely, and Vladlen
Koltun. Nerf++: Analyzingandimprovingneuralradiance
fields. arXivpreprintarXiv:2010.07492,2020. 4
[70] LvminZhang, AnyiRao, andManeeshAgrawala. Adding
conditionalcontroltotext-to-imagediffusionmodels,2023.
5,8,1
[71] Zhixing Zhang, Ligong Han, Arna Ghosh, Dimitris N.
Metaxas, and Jian Ren. Sine: Single image editing with
text-to-imagediffusionmodels.2023IEEE/CVFConference
11TEDRA: Text-based Editing of Dynamic and Photoreal Actors
Supplementary Material
This supplemental document provides further informa- Diffusion Model (LDM) [49]. We follow the fine-tuning
tion about the implementation details (Sec. A). The addi- strategyproposedbyDreamBooth[50]:
tionalresults(Sec.B)showcasetestingonvarioussubjects, (cid:104) (cid:105)
E w ∥ζˆ (z ,s)−E(x )∥2 , (9)
free-viewpoint rendering capabilities, animation transfer xi,si,ζ,t t ϕ t,i i 2
demonstratingposeadaptabilitytomultipleavatars,further
where z = α E(x )+β ϵ, ϵ ∼ N(0,I), x is the ren-
t,i t i t i
ablation studies, and qualitative comparisons, highlighting
deredimageandα ,β controlthenoiseschedule. Weuse
√ t t
the robustness and versatility of our approach. Finally, we
w =σ2 1−σ2asproposedinFantasia3D[7]forappear-
t
address the limitations of our study and suggest potential
ancemodeling. Oncetrained,thismodelcannowgenerate
directionsforfutureresearch(Sec.C).
imagesgiventheprompt’aphotoofasksman/woman’in
randomposesandviewpoints.
A.ImplementationDetails
Toachieveposeandview-pointcontrolofthegenerated
images we employ a pre-trained ControlNet [70] which is
We first provide more details about the dataset (Sec. A.1),
conditionedonnormal-maps. TriHumaniscapableofgen-
followed by more details concerning fine-tuning the diffu-
erating images of surface normals which are computed by
sionmodel(Sec.A.2),editingtheavatar(Sec.A.3),andour
positionalderivativesoftheSDFfield.Alongwiththecom-
annealingstrategy(Sec.A.4).
putednormalsandanemptystringasinput,theControlNet
A.1.Dataset now acts as an encoder to provide pose and view control
overgeneratedimagesofthefine-tunedLDM.
The dataset adopted to train the deformable avatar repre-
Using this strategy, our fine-tuned model can also gen-
sentation consists of two parts: the DynaCap [12] dataset
eralize avatar editing to novel views and poses. The fine-
and the newly recorded sequences. The DynaCap dataset
tuning is performed for 20,000 iterations with a batch size
consists of 5 subjects wearing different types of apparel,
of30,andthelearningrateissetto1e-6.
performingdiversifiedeverydaymotions. Inthispaper,we
takeonerepresentativesequencefromtheDynaCapdataset
fortrainingthedeformableavatarmodel. Notably, wefol-
lowtheprotocolsmentionedintheTriHuman[74]andtrain
thedeformableavatarusingthetrainingsplitsprovidedby
theDynaCapdataset. ApartfromtheDynaCapdataset,we
captured3newsequencestodemonstratetheeffectiveness
ofourmodel.Thesequencefeatures3Subjectswearingev-
eryday clothing and engaging in various activities, includ-
ing running, jumping-jack, boxing, and dancing. The se-
quences are recorded in a multi-view studio with 120 4K
camerasataframerateof25fps. Inspiredbytheprotocol
proposedbyDynaCapDataset,werecordedseparatetrain-
ing and testing sequences with 27,000 and 7,000 frames.
Specifically,weholdout4camerasfromdifferentviewing
directionsastestingcameraviews. Additionally,weanno-
tate all the captured frames with 3D skeletal poses (gen-
erated with markerless motion capture software [58]), and
foreground segmentation masks (produced by the state-of-
the-artbackgroundmattingmethod[51]).Wewillmakethe
dataandtheannotations,publiclyavailableforresearchuse Figure 1. More ablative results on the ControlNet conditioning
uponacceptance. scale, andthewindowedrootannealingmethod, pleasezoom-in
toseethedetails.
A.2.Fine-tuningDetails
A.3.EditingDetails
Westartbyrenderingimagesat1fpsand50viewsfromthe
pre-trainedavatar,therenderedimagesarethenusedtofine- During the editing phase, both the latent diffusion models
tune the U-net (ζˆ ) and the text-encoder (Γ) of the Latent andControlNetsarefrozen,andonlytheTriHumanmodel
ϕ
1is optimized using the proposed score distillation termed ingfinedetailswithoutlosingthem,ariskpresentathigher
PNA-SDS (Personalized Normal-Aligned Score Distilla- timesteps.
tionSampling)asdefinedinEq.7.
B.AdditionalResults
Since we use pre-computed normals for both the pre-
trainedandpersonalizedLDM,wesettheControlNetcon-
The results section provides a comprehensive overview of
ditioningscaleto0.5and1.0respectively. Thisallowsthe
thestudy’sfindings.Itdemonstratestheeffectivenessofthe
pre-trained LDM to facilitate geometrical changes toward
methodologythroughenhancededitingresultsacrossadi-
thetargetededit. Fig.1(a)showstheimpactofControlNet
verserangeofsubjects(Sec.B.1),underscoringitsversatil-
conditioningscaleonsamplesfrompre-trainedLDM.
ity. Additionally, theexplorationoffree-viewpointrender-
Thehyperparametersv = 0.3andk = 750areempiri-
ing enriches visual representation (Sec. B.2), offering new
callychosentooptimizethebalancebetweenpreservingthe
perspectives on edited subjects, and the animation section
originalidentityoftheavatarandachievingthedesireded-
(Sec.B.3)showcasesposetransferadaptabilitytomultiple
its. TheimpactofthesevaluescanbefoundinSection4.3
avatars, highlighting the robustness of our approach. Fur-
ofSINE[71].
ther ablations reveal insights into variable impacts on the
Forasequenceoflength1kframes,weoptimizetheTri-
editing process (Sec. B.4). Finally, we present additional
Humanmodelfor50kiterationswithalearningrateof1e-4
comparisons of our method with state-of-the-art methods
onanNVIDIAA100GPU.Weutilizeclassifier-freeguid-
(seeSec.B.5),highlightingadvancementsandlimitations.
ancewithw =20.
B.1.MoreSubjects
A.4.Time-stepAnnealingDetails
Fig. 3-5 provide more qualitative results on multiple sub-
WithreferencetoEq.8wesetthemaximumandminimum jects. Ourapproachintroducesanovelframeworkforgen-
diffusion timesteps as t = 980 and t = 20, with erating visually pleasing edits guided by textual prompts
max min
a window size of w = 500. Initially, this configuration across various contexts. The second row of Fig. 3 show-
yields t = 980, t = 480, and a blending threshold of cases how our system adeptly adjusts the appearance of
1 2
k =730. Theannealingprocessceasesoncet reaches500 gloves based on the provided textual guidance. Moreover,
1
to prevent further increases in blurriness. Fig. 2 shows a ourmethoddemonstratesaremarkableabilitytotargetand
graphicalrepresentationoftheproposedannealingstrategy. modifyspecificregionsasdirectedbytheinputtext.Forin-
stance,thethirdrowofFig.3illustratestheversatilityofour
method,showcasinggeometricalterationspromptedbyin-
structionssuchas”womanwearingabicyclehelmet”. This
demonstrates the proficiency of our system in interpreting
complex textual instructions and creating visually appeal-
ing edits as a result. Critical to our approach is the main-
tenance of subject consistency and coherence across both
three-dimensionalstructureandtemporalprogression. This
issubstantiatedbytheconsistencyobservedinoursupple-
mentary video evidence, reinforcing the reliability and ef-
ficacyofourmethodingeneratingvisuallyconsistentout-
comes.
Insummary,ourmethodexcelsingeneratingcaptivating
Figure2. Thefigureshowstheannealingoftimestepsusingthe visual edits driven by textual prompts, offering a flexible
proposedwindow-roottimestepannealingstrategyfor10kitera- andintuitiveapproachtomanipulatingimagesacrossvari-
tions. Thetimesteptisrandomlysampledwithintheshownwin- ousscenarios.
dow. AsperEq. 7ift>kThenthescoresfrombothpre-trained
B.2.Free-viewpointRendering
LDM and personalized LDM are used else only the scores from
pre-trainedLDMareused.
Fig.6-8presentsthefree-viewpointrenderingsoftheedited
avatars. These results affirm that our approach maintains
Thewindowedrootannealingprioritizeslargertimesteps consistency across different viewpoints and time frames
tearlyinthetrainingprocess,which,akintodiffusionmod- duringtheeditingprocess.
els, establishes the target semantics quickly. In Fig. 1 (b),
B.3.Animation
the windowed root annealing method shows the formation
of a ’cap’ by prioritizing larger timesteps t early in train- Wedemonstratetheabilitytotransferposesfromonechar-
ing. As training progresses, t gradually decreases, refin- actertomultipleavatars,asshowninFig.9. Theresultsnot
2Figure3.QualitativeResults.Wepresentthetext-basededitingresults.Werecommendthereaderstozoomintobetterviewthedetails.
onlyconfirmthemethod’sabilitytoperformversatileedits
but also its adaptability to novel poses. This adaptability
isparticularlychallengingwithinthedomainofphotoreal-
istic 3D human avatars, highlighting the robustness of our
approach.
B.4.AdditionalAblations
Weconductqualitativeablationsononemoreidentitywith
adifferentpromptasshowninFig.11. Thetermsusedfor
differentsettingsareasfollows:
SDS: Score Distillation Sampling using only the pre-
trainedlatentdiffusion.
NA-SDS:NormalAlignedSDSusingpre-treinedLDM
andControlNet.
P-SDS:PersonalizedSDSusingfine-tuned/personalized
andpre-trainedLDMs.
PNA-SDS:Personalized Normal Aligned SDS using
fine-tuned/personalized and pre-trained LDMs with pre-
trainedControlNetconditioning(ours).
Figure10. QualitativeComparisons. Weofferfurthercompar-
PNA-SDS + HiFA Annealing: PNA SDS with the dif-
isonsinvolvingAvatarStudio[37]andInstructNeRF2NeRF[14].
fusiontimestepannealingstrategyproposedbyHiFA[75].
Our findings indicate that the outcomes generated by alternative
PNA-SDS+ourAnnealing: PNASDSalongwithour methods,specificallyincolumns2and3,exhibitsmoothersurface
windowroottimestepannealing(ourfullmethod). detailsandlackconsistencyinmaintainingsubjectcoherence.
3Figure 4. Qualitative Results. Our method demonstrates the capability to execute diversified contextual edits on photo-real avatars.
Theseeditsencompassvariousalterationssuchasadjustinglengthofthebeard,aswellasmorelocalizedchangesthattargetspecific.We
recommendthereaderstozoominforbetterviewingofthedetails.
The results clearly indicate the effectiveness of our C.LimitationsandFutureWork
comprehensive method, achieving high-quality edits while
TEDRA significantly advances text-driven 3D avatar edit-
maintaining the essential details and dynamics of the pre-
ing, providing compelling and coherent modifications.
trainedhumanavatar.
However, it struggles to recover fine facial details, like
eyes, particularly because latent diffusion models struggle
tosamplefull-bodyimageswithhigh-qualityfacialdetails.
B.5.AdditionalQualitativeComparisons
Ourmethod’smask-basedraysamplingrestrictssignificant
deviations in clothing from the pre-trained avatar model.
In this section, we conducted more qualitative compar-
Additionally,ourmethod’sdependencyonper-promptopti-
isonsagainstcompetingapproaches. Fig.10illustratesthat
mizationanditsintensiveGPUrequirementshighlightareas
our method maintains subject consistency while preserv-
forefficiencyimprovements.
ing clothing deformations. In contrast, Avatar Studio [37]
Further, TEDRA needs data from a multi-view studio,
produces over-saturated and excessively smoothed results
limiting its accessibility. Exploring monocular setups for
duetoitslimitedsubjectinformation. Conversely,Instruct
multi-vieweditsordevelopingdynamic,implicitrepresen-
Nerf2Nerf [14] exhibits lower visual quality and reduced
tationsofnovelhumansfromtextpromptsofferspromising
temporal consistency. Please refer to the supplementary
directionsforfutureresearch.
videoformoredynamicresults.
4Figure5. QualitativeResults. Ourapproachgeneratescaptivatingvisualeditsguidedbytextualpromptsacrossvariouscontexts. We
recommendthereaderstozoominforbetterviewingofthedetails.
Figure6.QualitativeResults.Thefree-viewpointrenderingresults.Werecommendthereaderstozoomintobetterviewthedetails.
5Figure7.QualitativeResults.Thefree-viewpointrenderingresults.Werecommendthereaderstozoomintobetterviewthedetails.
Figure8.QualitativeResults.Thefree-viewpointrenderingresults.Werecommendthereaderstozoomintobetterviewthedetails.
6Figure9.QualitativeResults.Wepresenttheresultsofavataranimationusingnovelposes.Thefirstrowshowsthedrivingpose,followed
byeditedavatarsdrivenbythesamepose. Theresultsindicatethattheeditsaregeneralizabletonovelposes. Werecommendthereaders
tozoomintobetterviewthedetails.
Figure 11. Ablation Study. We conduct a qualitative analysis, comparing our comprehensive approach to various design alternatives
usingthetextprompt”aphotoofawomanwearingabicyclehelmet”. Ourcompletemethodsuccessfullygeneratesvisuallyconvincing
modificationsthatmaintainthecrucialaspectsoftheoriginalavatar.
7