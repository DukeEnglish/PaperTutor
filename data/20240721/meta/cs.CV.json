[
    {
        "title": "GroupMamba: Parameter-Efficient and Accurate Group Visual State Space Model",
        "authors": "Abdelrahman ShakerSyed Talal WasimSalman KhanJuergen GallFahad Shahbaz Khan",
        "links": "http://arxiv.org/abs/2407.13772v1",
        "entry_id": "http://arxiv.org/abs/2407.13772v1",
        "pdf_url": "http://arxiv.org/pdf/2407.13772v1",
        "summary": "Recent advancements in state-space models (SSMs) have showcased effective\nperformance in modeling long-range dependencies with subquadratic complexity.\nHowever, pure SSM-based models still face challenges related to stability and\nachieving optimal performance on computer vision tasks. Our paper addresses the\nchallenges of scaling SSM-based models for computer vision, particularly the\ninstability and inefficiency of large model sizes. To address this, we\nintroduce a Modulated Group Mamba layer which divides the input channels into\nfour groups and applies our proposed SSM-based efficient Visual Single\nSelective Scanning (VSSS) block independently to each group, with each VSSS\nblock scanning in one of the four spatial directions. The Modulated Group Mamba\nlayer also wraps the four VSSS blocks into a channel modulation operator to\nimprove cross-channel communication. Furthermore, we introduce a\ndistillation-based training objective to stabilize the training of large\nmodels, leading to consistent performance gains. Our comprehensive experiments\ndemonstrate the merits of the proposed contributions, leading to superior\nperformance over existing methods for image classification on ImageNet-1K,\nobject detection, instance segmentation on MS-COCO, and semantic segmentation\non ADE20K. Our tiny variant with 23M parameters achieves state-of-the-art\nperformance with a classification top-1 accuracy of 83.3% on ImageNet-1K, while\nbeing 26% efficient in terms of parameters, compared to the best existing Mamba\ndesign of same model size. Our code and models are available at:\nhttps://github.com/Amshaker/GroupMamba.",
        "updated": "2024-07-18 17:59:58 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.13772v1"
    },
    {
        "title": "Training-Free Model Merging for Multi-target Domain Adaptation",
        "authors": "Wenyi LiHuan-ang GaoMingju GaoBeiwen TianRong ZhiHao Zhao",
        "links": "http://arxiv.org/abs/2407.13771v1",
        "entry_id": "http://arxiv.org/abs/2407.13771v1",
        "pdf_url": "http://arxiv.org/pdf/2407.13771v1",
        "summary": "In this paper, we study multi-target domain adaptation of scene understanding\nmodels. While previous methods achieved commendable results through\ninter-domain consistency losses, they often assumed unrealistic simultaneous\naccess to images from all target domains, overlooking constraints such as data\ntransfer bandwidth limitations and data privacy concerns. Given these\nchallenges, we pose the question: How to merge models adapted independently on\ndistinct domains while bypassing the need for direct access to training data?\nOur solution to this problem involves two components, merging model parameters\nand merging model buffers (i.e., normalization layer statistics). For merging\nmodel parameters, empirical analyses of mode connectivity surprisingly reveal\nthat linear merging suffices when employing the same pretrained backbone\nweights for adapting separate models. For merging model buffers, we model the\nreal-world distribution with a Gaussian prior and estimate new statistics from\nthe buffers of separately trained models. Our method is simple yet effective,\nachieving comparable performance with data combination training baselines,\nwhile eliminating the need for accessing training data. Project page:\nhttps://air-discover.github.io/ModelMerging",
        "updated": "2024-07-18 17:59:57 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.13771v1"
    },
    {
        "title": "Addressing Imbalance for Class Incremental Learning in Medical Image Classification",
        "authors": "Xuze HaoWenqian NiXuhao JiangWeimin TanBo Yan",
        "links": "http://arxiv.org/abs/2407.13768v1",
        "entry_id": "http://arxiv.org/abs/2407.13768v1",
        "pdf_url": "http://arxiv.org/pdf/2407.13768v1",
        "summary": "Deep convolutional neural networks have made significant breakthroughs in\nmedical image classification, under the assumption that training samples from\nall classes are simultaneously available. However, in real-world medical\nscenarios, there's a common need to continuously learn about new diseases,\nleading to the emerging field of class incremental learning (CIL) in the\nmedical domain. Typically, CIL suffers from catastrophic forgetting when\ntrained on new classes. This phenomenon is mainly caused by the imbalance\nbetween old and new classes, and it becomes even more challenging with\nimbalanced medical datasets. In this work, we introduce two simple yet\neffective plug-in methods to mitigate the adverse effects of the imbalance.\nFirst, we propose a CIL-balanced classification loss to mitigate the classifier\nbias toward majority classes via logit adjustment. Second, we propose a\ndistribution margin loss that not only alleviates the inter-class overlap in\nembedding space but also enforces the intra-class compactness. We evaluate the\neffectiveness of our method with extensive experiments on three benchmark\ndatasets (CCH5000, HAM10000, and EyePACS). The results demonstrate that our\napproach outperforms state-of-the-art methods.",
        "updated": "2024-07-18 17:59:44 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.13768v1"
    },
    {
        "title": "Visual Haystacks: Answering Harder Questions About Sets of Images",
        "authors": "Tsung-Han WuGiscard BiambyJerome QuenumRitwik GuptaJoseph E. GonzalezTrevor DarrellDavid M. Chan",
        "links": "http://arxiv.org/abs/2407.13766v1",
        "entry_id": "http://arxiv.org/abs/2407.13766v1",
        "pdf_url": "http://arxiv.org/pdf/2407.13766v1",
        "summary": "Recent advancements in Large Multimodal Models (LMMs) have made significant\nprogress in the field of single-image visual question answering. However, these\nmodels face substantial challenges when tasked with queries that span extensive\ncollections of images, similar to real-world scenarios like searching through\nlarge photo albums, finding specific information across the internet, or\nmonitoring environmental changes through satellite imagery. This paper explores\nthe task of Multi-Image Visual Question Answering (MIQA): given a large set of\nimages and a natural language query, the task is to generate a relevant and\ngrounded response. We propose a new public benchmark, dubbed \"Visual Haystacks\n(VHs),\" specifically designed to evaluate LMMs' capabilities in visual\nretrieval and reasoning over sets of unrelated images, where we perform\ncomprehensive evaluations demonstrating that even robust closed-source models\nstruggle significantly. Towards addressing these shortcomings, we introduce\nMIRAGE (Multi-Image Retrieval Augmented Generation), a novel retrieval/QA\nframework tailored for LMMs that confronts the challenges of MIQA with marked\nefficiency and accuracy improvements over baseline methods. Our evaluation\nshows that MIRAGE surpasses closed-source GPT-4o models by up to 11% on the VHs\nbenchmark and offers up to 3.4x improvements in efficiency over text-focused\nmulti-stage approaches.",
        "updated": "2024-07-18 17:59:30 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.13766v1"
    },
    {
        "title": "Shape of Motion: 4D Reconstruction from a Single Video",
        "authors": "Qianqian WangVickie YeHang GaoJake AustinZhengqi LiAngjoo Kanazawa",
        "links": "http://arxiv.org/abs/2407.13764v1",
        "entry_id": "http://arxiv.org/abs/2407.13764v1",
        "pdf_url": "http://arxiv.org/pdf/2407.13764v1",
        "summary": "Monocular dynamic reconstruction is a challenging and long-standing vision\nproblem due to the highly ill-posed nature of the task. Existing approaches are\nlimited in that they either depend on templates, are effective only in\nquasi-static scenes, or fail to model 3D motion explicitly. In this work, we\nintroduce a method capable of reconstructing generic dynamic scenes, featuring\nexplicit, full-sequence-long 3D motion, from casually captured monocular\nvideos. We tackle the under-constrained nature of the problem with two key\ninsights: First, we exploit the low-dimensional structure of 3D motion by\nrepresenting scene motion with a compact set of SE3 motion bases. Each point's\nmotion is expressed as a linear combination of these bases, facilitating soft\ndecomposition of the scene into multiple rigidly-moving groups. Second, we\nutilize a comprehensive set of data-driven priors, including monocular depth\nmaps and long-range 2D tracks, and devise a method to effectively consolidate\nthese noisy supervisory signals, resulting in a globally consistent\nrepresentation of the dynamic scene. Experiments show that our method achieves\nstate-of-the-art performance for both long-range 3D/2D motion estimation and\nnovel view synthesis on dynamic scenes. Project Page:\nhttps://shape-of-motion.github.io/",
        "updated": "2024-07-18 17:59:08 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.13764v1"
    }
]