[
    {
        "title": "Multi-Label Learning with Stronger Consistency Guarantees",
        "authors": "Anqi MaoMehryar MohriYutao Zhong",
        "links": "http://arxiv.org/abs/2407.13746v1",
        "entry_id": "http://arxiv.org/abs/2407.13746v1",
        "pdf_url": "http://arxiv.org/pdf/2407.13746v1",
        "summary": "We present a detailed study of surrogate losses and algorithms for\nmulti-label learning, supported by $H$-consistency bounds. We first show that,\nfor the simplest form of multi-label loss (the popular Hamming loss), the\nwell-known consistent binary relevance surrogate suffers from a sub-optimal\ndependency on the number of labels in terms of $H$-consistency bounds, when\nusing smooth losses such as logistic losses. Furthermore, this loss function\nfails to account for label correlations. To address these drawbacks, we\nintroduce a novel surrogate loss, multi-label logistic loss, that accounts for\nlabel correlations and benefits from label-independent $H$-consistency bounds.\nWe then broaden our analysis to cover a more extensive family of multi-label\nlosses, including all common ones and a new extension defined based on\nlinear-fractional functions with respect to the confusion matrix. We also\nextend our multi-label logistic losses to more comprehensive multi-label\ncomp-sum losses, adapting comp-sum losses from standard classification to the\nmulti-label learning. We prove that this family of surrogate losses benefits\nfrom $H$-consistency bounds, and thus Bayes-consistency, across any general\nmulti-label loss. Our work thus proposes a unified surrogate loss framework\nbenefiting from strong consistency guarantees for any multi-label loss,\nsignificantly expanding upon previous work which only established\nBayes-consistency and for specific loss functions. Additionally, we adapt\nconstrained losses from standard classification to multi-label constrained\nlosses in a similar way, which also benefit from $H$-consistency bounds and\nthus Bayes-consistency for any multi-label loss. We further describe efficient\ngradient computation algorithms for minimizing the multi-label logistic loss.",
        "updated": "2024-07-18 17:51:02 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.13746v1"
    },
    {
        "title": "Optimistic Q-learning for average reward and episodic reinforcement learning",
        "authors": "Priyank AgrawalShipra Agrawal",
        "links": "http://arxiv.org/abs/2407.13743v1",
        "entry_id": "http://arxiv.org/abs/2407.13743v1",
        "pdf_url": "http://arxiv.org/pdf/2407.13743v1",
        "summary": "We present an optimistic Q-learning algorithm for regret minimization in\naverage reward reinforcement learning under an additional assumption on the\nunderlying MDP that for all policies, the expected time to visit some frequent\nstate $s_0$ is finite and upper bounded by $H$. Our setting strictly\ngeneralizes the episodic setting and is significantly less restrictive than the\nassumption of bounded hitting time {\\it for all states} made by most previous\nliterature on model-free algorithms in average reward settings. We demonstrate\na regret bound of $\\tilde{O}(H^5 S\\sqrt{AT})$, where $S$ and $A$ are the\nnumbers of states and actions, and $T$ is the horizon. A key technical novelty\nof our work is to introduce an $\\overline{L}$ operator defined as $\\overline{L}\nv = \\frac{1}{H} \\sum_{h=1}^H L^h v$ where $L$ denotes the Bellman operator. We\nshow that under the given assumption, the $\\overline{L}$ operator has a strict\ncontraction (in span) even in the average reward setting. Our algorithm design\nthen uses ideas from episodic Q-learning to estimate and apply this operator\niteratively. Therefore, we provide a unified view of regret minimization in\nepisodic and non-episodic settings that may be of independent interest.",
        "updated": "2024-07-18 17:49:09 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.13743v1"
    },
    {
        "title": "Understanding Reinforcement Learning-Based Fine-Tuning of Diffusion Models: A Tutorial and Review",
        "authors": "Masatoshi UeharaYulai ZhaoTommaso BiancalaniSergey Levine",
        "links": "http://arxiv.org/abs/2407.13734v1",
        "entry_id": "http://arxiv.org/abs/2407.13734v1",
        "pdf_url": "http://arxiv.org/pdf/2407.13734v1",
        "summary": "This tutorial provides a comprehensive survey of methods for fine-tuning\ndiffusion models to optimize downstream reward functions. While diffusion\nmodels are widely known to provide excellent generative modeling capability,\npractical applications in domains such as biology require generating samples\nthat maximize some desired metric (e.g., translation efficiency in RNA, docking\nscore in molecules, stability in protein). In these cases, the diffusion model\ncan be optimized not only to generate realistic samples but also to explicitly\nmaximize the measure of interest. Such methods are based on concepts from\nreinforcement learning (RL). We explain the application of various RL\nalgorithms, including PPO, differentiable optimization, reward-weighted MLE,\nvalue-weighted sampling, and path consistency learning, tailored specifically\nfor fine-tuning diffusion models. We aim to explore fundamental aspects such as\nthe strengths and limitations of different RL-based fine-tuning algorithms\nacross various scenarios, the benefits of RL-based fine-tuning compared to\nnon-RL-based approaches, and the formal objectives of RL-based fine-tuning\n(target distributions). Additionally, we aim to examine their connections with\nrelated topics such as classifier guidance, Gflownets, flow-based diffusion\nmodels, path integral control theory, and sampling from unnormalized\ndistributions such as MCMC. The code of this tutorial is available at\nhttps://github.com/masa-ue/RLfinetuning_Diffusion_Bioseq",
        "updated": "2024-07-18 17:35:32 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.13734v1"
    },
    {
        "title": "Realizable $H$-Consistent and Bayes-Consistent Loss Functions for Learning to Defer",
        "authors": "Anqi MaoMehryar MohriYutao Zhong",
        "links": "http://arxiv.org/abs/2407.13732v1",
        "entry_id": "http://arxiv.org/abs/2407.13732v1",
        "pdf_url": "http://arxiv.org/pdf/2407.13732v1",
        "summary": "We present a comprehensive study of surrogate loss functions for learning to\ndefer. We introduce a broad family of surrogate losses, parameterized by a\nnon-increasing function $\\Psi$, and establish their realizable $H$-consistency\nunder mild conditions. For cost functions based on classification error, we\nfurther show that these losses admit $H$-consistency bounds when the hypothesis\nset is symmetric and complete, a property satisfied by common neural network\nand linear function hypothesis sets. Our results also resolve an open question\nraised in previous work (Mozannar et al., 2023) by proving the realizable\n$H$-consistency and Bayes-consistency of a specific surrogate loss.\nFurthermore, we identify choices of $\\Psi$ that lead to $H$-consistent\nsurrogate losses for any general cost function, thus achieving\nBayes-consistency, realizable $H$-consistency, and $H$-consistency bounds\nsimultaneously. We also investigate the relationship between $H$-consistency\nbounds and realizable $H$-consistency in learning to defer, highlighting key\ndifferences from standard classification. Finally, we empirically evaluate our\nproposed surrogate losses and compare them with existing baselines.",
        "updated": "2024-07-18 17:35:03 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.13732v1"
    },
    {
        "title": "Predictive Low Rank Matrix Learning under Partial Observations: Mixed-Projection ADMM",
        "authors": "Dimitris BertsimasNicholas A. G. Johnson",
        "links": "http://arxiv.org/abs/2407.13731v1",
        "entry_id": "http://arxiv.org/abs/2407.13731v1",
        "pdf_url": "http://arxiv.org/pdf/2407.13731v1",
        "summary": "We study the problem of learning a partially observed matrix under the low\nrank assumption in the presence of fully observed side information that depends\nlinearly on the true underlying matrix. This problem consists of an important\ngeneralization of the Matrix Completion problem, a central problem in\nStatistics, Operations Research and Machine Learning, that arises in\napplications such as recommendation systems, signal processing, system\nidentification and image denoising. We formalize this problem as an\noptimization problem with an objective that balances the strength of the fit of\nthe reconstruction to the observed entries with the ability of the\nreconstruction to be predictive of the side information. We derive a\nmixed-projection reformulation of the resulting optimization problem and\npresent a strong semidefinite cone relaxation. We design an efficient, scalable\nalternating direction method of multipliers algorithm that produces high\nquality feasible solutions to the problem of interest. Our numerical results\ndemonstrate that in the small rank regime ($k \\leq 15$), our algorithm outputs\nsolutions that achieve on average $79\\%$ lower objective value and $90.1\\%$\nlower $\\ell_2$ reconstruction error than the solutions returned by the\nexperiment-wise best performing benchmark method. The runtime of our algorithm\nis competitive with and often superior to that of the benchmark methods. Our\nalgorithm is able to solve problems with $n = 10000$ rows and $m = 10000$\ncolumns in less than a minute.",
        "updated": "2024-07-18 17:33:14 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.13731v1"
    }
]