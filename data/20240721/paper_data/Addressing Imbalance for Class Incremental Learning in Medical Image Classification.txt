Addressing Imbalance for Class Incremental Learning in Medical
Image Classification
XuzeHao,WenqianNi,XuhaoJiang,WeiminTan*,BoYan*
ShanghaiKeyLaboratoryofIntelligentInformationProcessing,SchoolofComputerScience,FudanUniversity
Shanghai,China
{wmtan,byan}@fudan.edu.cn
ABSTRACT
Step 1
Training Data
Deepconvolutionalneuralnetworkshavemadesignificantbreak-
Train Predict
throughsinmedicalimageclassification,undertheassumption Model 1
thattrainingsamplesfromallclassesaresimultaneouslyavailable.
However,inreal-worldmedicalscenarios,there’sacommonneed No DR Mild Moderate
tocontinuouslylearnaboutnewdiseases,leadingtotheemerging
field of class incremental learning (CIL) in the medical domain.
Step 2
Training Data
Typically,CILsuffersfromcatastrophicforgettingwhentrainedon
newclasses.Thisphenomenonismainlycausedbytheimbalance Train Predict
betweenoldandnewclasses,anditbecomesevenmorechalleng- Model 2
ingwithimbalancedmedicaldatasets.Inthiswork,weintroduce
Severe
twosimpleyeteffectiveplug-inmethodstomitigatetheadverse
effectsoftheimbalance.First,weproposeaCIL-balancedclassifi-
cationlosstomitigatetheclassifierbiastowardmajorityclasses Training Data Step 3
vialogitadjustment.Second,weproposeadistributionmarginloss Train Predict
thatnotonlyalleviatestheinter-classoverlapinembeddingspace Model 3
butalsoenforcestheintra-classcompactness.Weevaluatetheef-
fectivenessofourmethodwithextensiveexperimentsonthree Proliferative
benchmarkdatasets(CCH5000,HAM10000,andEyePACS).The
resultsdemonstratethatourapproachoutperformsstate-of-the-art : old class : new class
methods.
Figure1:Overviewoftheclassincrementallearningsetting
CCSCONCEPTS inmedicalimageclassification.Duringtheincrementalpro-
cess,thetrainingdataisonlyprovidedforthecurrentclasses,
•Computingmethodologies→Computervision.
whilethedatafrompreviousstepsisnotaccessible.Ateach
step,themodelisrequiredtoperformclassificationforall
KEYWORDS
theclassesseensofar.
Classincrementallearning,medicalimageclassification,classim-
balance
modelswouldbefixedoncedeveloped,whileinrealclinicalprac-
1 INTRODUCTION
tice,thedistributionofmedicaldatafrequentlyundergoesshifts
Nowadays,deeplearninghasemergedasapowerfultoolacross overtime,primarilyduetothecontinuousemergenceofnewdis-
variousfields[7,18,23,24],includingthemedicaldomain[2,33,36]. eases,treatmentprotocols,andpatientdata[6,41].Undersuch
However,traditionaldeeplearningmethodsoftenmakeassump- circumstances,themodelneedstoincorporatenewclassknowl-
tionsaboutstationaryandindependentdatadistributions,which edgeincrementallyinsteadofretrainingthemodelwithalldata
maybeimpracticalinreal-worldscenarios.Mosttraineddiagnosis available[28].Therefore,inthiswork,wefocusonclassincremen-
tallearninginthemedicaldomain.
∗Correspondingauthors:WeiminTan,BoYan.
Fig.1illustratesthesettingofclassincrementallearninginmed-
ical Image classification. Taking the EyePACS dataset [8] as an
Permissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalor
example,themodelisinitiallytrainedtoclassifythreeclasses(i.e.,
classroomuseisgrantedwithoutfeeprovidedthatcopiesarenotmadeordistributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation NoDR,Mild,andModerate).Subsequently,incrementalclasses(e.g.,
onthefirstpage.Copyrightsforcomponentsofthisworkownedbyothersthanthe Severe,andProliferativeDR)arriveinsequentialstepstoupdate
author(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or
themodel.Theclassesintroducedindifferentstepsaredisjoint,and
republish,topostonserversortoredistributetolists,requirespriorspecificpermission
and/orafee.Requestpermissionsfrompermissions@acm.org. themodelmustbeabletopredictallclassesseenovertime.How-
ACMMM,2024,Melbourne,Australia ever,whenupdatingthemodelwithonlynewclasses,newdata
©2024Copyrightheldbytheowner/author(s).PublicationrightslicensedtoACM.
tendstoerasepreviousknowledge.Thisphenomenonisknownas
ACMISBN978-x-xxxx-xxxx-x/YY/MM
https://doi.org/10.1145/nnnnnnn.nnnnnnn catastrophicforgetting[16,26].
4202
luJ
81
]VC.sc[
1v86731.7042:viXraACMMM,2024,Melbourne,Australia XuzeHao,WenqianNi,XuhaoJiang,WeiminTan*,BoYan*
Forclassincrementallearning,imbalanceddatabetweenoldand outputsofthefinallayerbutalsofortheintermediatefeaturesto
newclassesisoneoftheprimaryreasonsforcatastrophicforget- mitigaterepresentationforgetting.However,regularization-based
ting[27,54].Tothisend,numerousapproacheshavebeenproposed methodsstillsufferfromfeaturedegradationofoldknowledgedue
tostoreasmallproportionofprevioustrainingdatainmemoryand tothelimitedaccesstoolddata[50].
rehearsethemwhenlearningnewclasses[1,20,49].However,the Structure-basedmethods[15,21,31,44,50]aimtopreservethe
limitedsizeofmemorycanalsoleadtoanimbalancebetweenold learnedparametersassociatedwith oldclasseswhileincremen-
andnewclasses[32,38].Underthiscircumstance,theclassimbal- tallycreatingmodulestoenhancethemodel’scapacitytoacquire
ancewillleadto(i)aclassifierbiasedtowardsthenewandmajority new knowledge. Recently, DER [50] adds a new feature extrac-
classes;and(ii)theembeddingsofnewclassesinevitablyoverlap torateachstepandthenconcatenatestheextractedfeaturesfor
withtheoldonesinthefeaturespace(i.e.theambiguitiesproblem). classification.DyTox[15]appliestransformer[12]toincremental
Inadditiontotheclassincrementallearningimbalance,manyreal- learninganddynamicallyexpandstasktokenswhenlearningnew
lifemedicaldatasetsexhibitsignificantclassimbalance[36],with classes.Nevertheless,dynamicallyaddingnewmoduleswilllead
someclasseshavingnotablyhigherinstancesintrainingsamples toanexplosioninthenumberofparametersandanincreaseinthe
thanothers,e.g.,HAM10000[42],andEyePACS[8],whichfurther independencebetweeneachfeatureextractortoharmperformance
aggravatethecatastrophicforgetting.Therefore,addressingdata innewclasses[44].
imbalanceiscrucialforclassincrementallearninginmedicalimage Memory-basedmethods[3,4,38,46,49,51]addressthechallenge
classification. offorgettingbystoringalimitednumberofrepresentativesamples
Inthispaper,weproposetwosimpleyeteffectiveplug-inloss fromoldclassesinamemorybuffer.iCaRL[38]learnstheexemplar-
objectivestotackletwochallengescausedbyimbalanceinclass baseddatarepresentationandmakespredictionsusinganearest-
incrementallearning.First,weproposeaCIL-balancedclassifica- mean-of-exemplarsclassifier.GEM[4]usesexemplarsforgradient
tionlossinsteadofthetraditionalcross-entropy(CE)losstoavoid projectiontoovercomeforgetting.Additionally,someapproaches
theissueofclassifierbias.Specifically,wefirstadjustthelogits employgenerativemodelstosynthesizeoldclasssamplesfordata
basedonthecategoryfrequencytoplacemoreemphasisonrare rehearsal[35,39,45]whileotherworksconsidersavingfeature
classesandthenintroduceascalefactortofurtherachieveabalance embeddingsinsteadofrawimages[22].Inourwork,wefollowthe
betweenoldandnewclasses.Second,toalleviatetheoverlapof memory-basedapproachtodirectlystoreasmallsubsetofoldclass
classesinthefeaturespace,weproposeadistributionmarginloss, dataforrehearsal.
anovelimprovedmarginloss,whichnotonlyfacilitatestopush
awaythedistributionsofoldandnewclassesbutalsoobtainsthe
compactintra-classclustering.Extensiveexperimentsonbench-
2.2 ClassImbalance
markdatasetsundervarioussettingsverifythesuperiorityofour
method. Class imbalance is a key challenge for class incremental learn-
Tosummarize,themaincontributionsofthispaperare: ing[20].Duetotheonlyaccesstotheclassesofthecurrentstep,
• Toreducetheclassifierbiastowardsnewandmajorityclasses, theclassifierisseverelybiased,andthereisaninevitableoverlap
andconfusionbetweenthefeaturespacerepresentationsofold
weproposeaCIL-balancedclassificationlossthatempha-
andnewclasses[27].Evenwiththelimitedsizeofthememory
sizesrareonesvialogitadjustment.
• Weintroduceanoveldistributionmarginlossthatcanef- buffer,thebiasedoptimizationbyimbalanceddatabetweenold
andnewclassesisstillacrucialproblemthatcausescatastrophic
fectivelyseparatethedistributionsofoldandnewclassesto
forgetting[32,38].Tocopewithit,SS-IL[1]isolatesthecompu-
avoidambiguitiesandrealizetheoptimizationoftheintra-
tationofthesoftmaxprobabilitiesonoldandnewclassesforbias
classcompactness.
• Extensiveexperimentsdemonstratethatourmethodcanef- compensation.BiC[49]introducesabiascorrectionlayertoaddress
thebiasinthelastfullyconnectedlayer.
fectivelyaddresstheissueofdataimbalancewiththestate-of-
Inreal-worldmedicalscenarios,mostexistingdatasetscontain
the-artperformanceachievedonthreebenchmarkdatasets:
highlyimbalancednumbersofsamples[36],whichleadstoamore
CCH5000,HAM10000,andEyePACS.
severeforgetting.Tothebestofourknowledge,LO2LN[6]isthe
2 RELATEDWORK firstattempttoaddresstheproblemofclassincrementallearningin
medicalimageclassification.First,theyutilizetheclass-balancedfo-
2.1 ClassIncrementalLearning
calloss[9]toavoidtheclassifierbias.However,theclass-balanced
Classincrementallearningaimstotrainamodelfromasequence focallossisnotspecializedandefficientforincrementallearning.
ofclasses,ensuringitsperformanceacrossalltheclasses.Existing Second,theyintroducethemarginrankingloss[20]toseparate
classincrementallearningmethodscanberoughlydividedinto oldandnewclasses.Wearguethatthisconstraintmaynotbesuffi-
threegroups:regularization-based,structure-based,andmemory- cientlyrobust,resultinginlargeclusterswithinclasses(intra-class)
based. andpotentialoverlapsbetweenclasses(inter-class).Bycontrast,in
Regularization-basedmethods[11,14,17,20,40,52]applyad- thispaper,weproposetwosimpleyeteffectiveplug-inlossobjec-
ditionalconstraintstopreventtheexistingmodelfromforgetting tives:(i)aCIL-balancedclassificationlosstoalleviateprediction
previousknowledge.LUCIR[20]constrainstheorientationofthe biasbyadjustingthelogits,and(ii)adistributionmarginlossthat
features to preserve the geometric configuration of old classes. canpushthedistributionsofoldandnewclassesawayandprovide
PODNet[14]introducesanovelspatialdistillationnotonlyforthe morecompactintra-classclusteringsimultaneously.AddressingImbalanceforClassIncrementalLearninginMedicalImageClassification ACMMM,2024,Melbourne,Australia
3 METHOD follows:
In this section, we first outline the setting of class incremental 𝑒𝑥𝑝(cid:16) 𝑝𝑡 (cid:17)
l aea dr en ti an ig lei dn dm ee sd cric ipal tii om nag oe fc tl ha ess ti wfic oat pio ron p( oSe sec. d3 l.1 o) s. sT oh be jn e, cw tie vep sr :ov Ci Id Le
-
L𝑙𝑏𝑐 =− (cid:12)
(cid:12)
(cid:12)Dˆ1 𝑡(cid:12)
(cid:12)
(cid:12)𝑖∑︁
∈Dˆ
𝑡𝑙𝑜𝑔
(cid:205) 𝑗∈Y
1:𝑡𝑒𝑥𝑝(cid:16) 𝑝𝑖, 𝑖𝑡𝑦 ,𝑗𝑖
+𝑣
𝑦𝑖,𝑗(cid:17), (4)
balancedclassificationloss(Sec.3.2)anddistributionmarginloss
where:
(Sec.3.3).
𝑥3
C
d
rD
e
𝑖a
𝑡.
l
pa1
t ∈a
=s rs
ei
Xsni {n
e
𝑡cS
D
nc
r
ie
er
t
s1e
smt
,
amt
D
tei
she
nn
2
an
et
,
mg
at
D
ta
l
prlla
y
3
a
ll
e,.
in
e n.Sa
a.d
.
ipr
n,
nn
e
D
dgN
cin
i
𝑇
𝑦fi
sgo
𝑖𝑡}
ecat
,
ta
∈ia
wlm
fl
Yrt
y
h
osi
,
𝑡eo
mwt
r
ion
seet
s
tr
d
D
t
ha
ee
ei p𝑡nn
co
𝑡=a
ote
rm
w
r(t
eXo
ih
std
𝑡e
h
p,e
os
Yl
𝑛
nef
𝑡
𝑡qr d)o
u
ii
nm
ne
=
gn
sta
c
(cid:8)
lae
as
(
n
b𝑥e
o
c
eq
𝑖𝑡f
e
l,u
.st
𝑦e
a
.
T𝑖𝑡n
s
H
h)kc
(cid:9)
ese
e𝑛 𝑖 r=
lao
𝑡
ae1sf
-, forIt this ek
a𝑣
n
c𝑦
o
c𝑖,
w
u𝑗 rn=
att


eh𝑙
𝑙
𝑙
𝑙
a
c𝑜
𝑜
𝑜
𝑜
t
l𝑔
𝑔
𝑔
𝑔
atr
s𝑚
𝑚𝑞
𝑞
𝑞𝑚
a
s𝑞𝑚 𝑦
𝑦𝑗
id𝑗
fi𝑖
𝑖
it,
c[[
[
i=
ao>
<
tn
i00 oa0]]
l]
n,
s,
,
oof
ftii
ii
m
sff
ff
a𝑦𝑦
𝑦𝑦
a
m𝑖𝑖
𝑖𝑖
x
p∈∈
∈∈
l lo
eYY
YY
ss
𝑥𝑡𝑡11 ,,:: 𝑖n𝑡𝑡 .𝑗𝑗−−
e
Ic11
∈∈
ne,, sYY𝑗𝑗
os
r𝑡1∈∈
i
dt: .𝑡
a
eYY
−
t
re1𝑡
1
s,
,: t𝑡 o−
𝑝
𝑖𝑡1 p,,
𝑦 r𝑖 io> rit𝑝
i(
𝑖
z𝑡5
,
e𝑗)
belspaceofthemodelisallseenclassesY𝑖:𝑡 = (cid:208) 𝑖𝑡 =1Y𝑖,where thelearningofoldandrareclasses,weemploythefollowinglogit
Y𝑡 ∩Y𝑡′ = ∅ for all 𝑡 ≠ 𝑡′. Inspired by memory-based meth- adjustmentstrategy.Specifically,when𝑦 𝑖 ∈Y 1:𝑡−1and𝑗 ∈Y𝑡 (the
ods[3,38,49],ourmethodconsistentlysamples𝑚representative firstlineinEq.5),weinsteadrequire𝑝 𝑖𝑡
,𝑦𝑖
>𝑝 𝑖𝑡 ,𝑗+log(cid:0)𝑞 𝑗/𝑚(cid:1)[>0].
win hst ica hnc ie ss uf pro dm atee dac ah fto el rd tc hl eas ts raa in nd ins gto sr te epth 𝑡em iscin omam ple em teo dr .y Itb su hff oe ur ldM b𝑡 e, H trae in nc ine, gi pt ri os cc el se sa pr lt ah ca et mw oe rer ee mqu pi hre asa isl oar ng oe lr d𝑝 c𝑖𝑡 l, a𝑦 s𝑖 s, 𝑦w 𝑖h thic ah nm pra ek ve ios ut sh lye
.
mentionedthatonlydatafromDˆ 𝑡 = D𝑡 ∪M𝑡−1isavailablefor However,ifboth𝑦 𝑖and𝑗arewithinY 1:𝑡−1(thesecondlineinEq.5),
trainingduringthe𝑡-thstep.
thelogitremainsunchanged,sincetheyarebotholdclasseswith
Classically,themodelatsteptcanbewrittenasthecomposition
thesamememorysize.
oftwofunctions: 𝑓𝑡 = 𝑓 𝜃𝑡 ◦𝑓 𝜙𝑡(·),where 𝑓 𝜙𝑡 representsafeature Fortheothertwocaseswhen𝑦 𝑖 ∈Y𝑡.If𝑗 ∈Y 1:𝑡−1(thethirdline
extractor,and𝑓 𝜃𝑡 representsaclassifier.Foraninputsample𝑥 𝑖,its inEq.5),thetermlog(cid:0)𝑚/𝑞 𝑦𝑖(cid:1) <0suggeststhatoldclass𝑗receives
featurerepresentationisdenotedasℎ 𝑖𝑡 =𝑓 𝜙𝑡(𝑥 𝑖).Weemploycosine moreemphasis.If𝑗 ∈Y𝑡 (thefourthlineinEq.5),moreemphasisis
normalization[20]astheclassifier𝑓𝑡
.Consequently,thepredicted
placedontheclass𝑦 𝑖 whenithasfewerinstances,andconversely,
logit𝑝 𝑖𝑡 ,𝑐 forclass𝑐atstep𝑡 canbe𝜃 calculatedfromℎ 𝑖𝑡 as: t loh ge itf -o bc au ls anis ceo dn cc ll aa ss ss ifi𝑗 cw ath ioen nt lh oe sssi cz ae n𝑞 e𝑗 ffis ecs tm iva el ll yer r. eT dh ue cr eef to hr ee, bt ih ae s
𝑝 𝑖𝑡 ,𝑐 =𝜂(cid:10)ℎ 𝑖𝑡,𝑤 𝑐(cid:11), (1) towardsnewandfrequentclasses.
Tofurthercontrolthebalancebetweentheoldandnewclasses,
where𝑤 𝑐 aretheweightsforclass𝑐 intheclassifierlayer,𝜂 isa weintroduceascalefactor𝛾:
learnablescalar,and ⟨·,·⟩ denotesthecosinesimilaritybetween
(cid:40)
twovectors.
𝛾 𝑐 =
𝛼, if𝑐 ∈Y 1:𝑡−1,
(6)
1, if𝑐 ∈Y𝑡,
3.2 CIL-BalancedClassificationLoss
where𝛼 ∈ [0,1]isatrade-offcoefficientforeachdataset.Withthe
Asclaimedinpreviousworks[32,36],theinherentimbalancein
helpofthisscalefactor,theCIL-balancedclassificationlosscanbe
medicaldatasetsandtheimbalanceinclassincrementallearning
writtenas:
canleadtoabiasedclassifier.Inspiredby[34],weaimtomitigate
t H oh noi ls w yi es tv hsu e ere , dfb aoy tr aaa fd m rj ou e ms mti Don ˆrg 𝑡y-t ib sh ae as vl eo adg ilmi at bs e lta ehc o ac d to sr ind tei cn plg a 𝑡s ,t so winc hca ir ct ee hmg co e or n ny t saf ilr se l te sq au ore n fn i tn hc gy e,. L𝑐𝑏𝑐 =− (cid:12)
(cid:12)
(cid:12)Dˆ1 𝑡(cid:12)
(cid:12)
(cid:12)𝑖∑︁
∈Dˆ
𝑡𝑙𝑜𝑔
(cid:205)
𝑗∈Y
1𝛾 :𝑡𝑦 𝛾𝑖 𝑗· ·𝑒 𝑒𝑥 𝑥𝑝 𝑝(cid:16) (cid:16)𝑝 𝑝𝑖𝑡 𝑖𝑡, ,𝑦 𝑗𝑖 +(cid:17)
𝑣
𝑦𝑖,𝑗(cid:17), (7)
memorybufferM𝑡−1 andthetrainingsetD𝑡.Hence,wedefine whichreducestheoutputvaluesfortheoldclasseswhilemaintain-
thecategoryfrequencyasfollows: ingtheoutputsforthenewclassesunchanged,therebyencouraging
𝑟 𝑐
=

(cid:12)
(cid:12) (cid:12)
(cid:12)
(cid:12)
(cid:12)D
D𝑞𝑚
ˆ ˆ𝑐𝑡
𝑡(cid:12)
(cid:12) (cid:12)
(cid:12)
(cid:12)
(cid:12),
,
i if f𝑐
𝑐
∈ ∈Y Y𝑡1 ,:𝑡−1,
(2)
𝛼t
t
ch
h
la
ie
i ms
sm
s
ps
ico
rna
od
cl
ve
ri
enl
e
st mgo
te
hsp
nt
er
r
to
a
sad
it
l
geu
l
ng
ec iye
a
firl
cf
na
u
ar
i
nr
ng
t
cge
h
er
.e
I
ol
r
no fg
m
oti
h
lt
i
ds
t
isif
g
co
c
la
aor
t
snet sh
s
t
eee
st
xs ,he
t
ie
,
to
a
mil lsd
t
as
ho
u
yon
e
u
ae ffgos
h
e.
f
cC
i
a
tmo tdn
hb
es
a
ece
l
r
mq
a
eu
n
a
oe
c
s
dn
e
e
et
li
il ’ny
n
s,
where𝑞
𝑐
isthenumberoftrainingsamplesforclass𝑐,and|·|isthe learningofnewones.Thus,determiningtheoptimal𝛼 becomes
cardinalityofagivenset.Afterthat,weadd𝑙𝑜𝑔𝑟 𝑐 totheoutput crucialforachievingabettertrade-off(seeSec.4.4).Notably,when
𝛼 isassignedavalueof1,thecurrentCIL-balancedclassification
logitsduringtraining.Thus,thelogit-balancedclassificationloss
lossdegradestothelogit-balancedclassificationloss(Eq.4).
canbeformulatedas:
L𝑙𝑏𝑐 =− (cid:12) (cid:12)
(cid:12)Dˆ1
𝑡(cid:12) (cid:12)
(cid:12)𝑖∑︁
∈Dˆ
𝑡𝑙𝑜𝑔 (cid:205)
𝑗𝑒 ∈𝑥 Y𝑝 1:𝑡(cid:16) 𝑒𝑝 𝑥𝑖𝑡 , 𝑝𝑦𝑖
(cid:16)
𝑝+
𝑖𝑡
,𝑙 𝑗𝑜𝑔 +𝑟 𝑙𝑜𝑦 𝑔𝑖(cid:17)
𝑟 𝑗(cid:17). (3)
3
I cn
l. a3
c sl sa es
ssD wini ocs
r
ut
e
lr
m
di
e
bb
n
eu
t
eat ali slo
e
ilan
yrn
oM
i vn
ega
r,
lr
t
ahg pei pn
r ee
dpL
r
ino ess
te
hs
n eta dt eio en ps fo ef at th ue reol sd paan ced [n 5e 3w
].
Toexplainhowourmethodworks,wereformulateEq.3into Toaddressthisissue,marginloss[5]isintroducedtoavoidthe
Eq.4byintroducing𝑣 𝑦𝑖,𝑗 :=𝑙𝑜𝑔𝑟 𝑗 −𝑙𝑜𝑔𝑟 𝑦𝑖,whicharedefinedas ambiguities between old and new classes. In detail, the vanillaACMMM,2024,Melbourne,Australia XuzeHao,WenqianNi,XuhaoJiang,WeiminTan*,BoYan*
: Anchor Embedding : Positive Distribution Algorithm1Classincrementallearningwithourmethod.
: Positive Embedding Input:IncrementaltaskdataD𝑡,Memoryexemplars:M𝑡−1
: Negative Distribution
: Negative Embedding Output:Updatedcurrentmodel
1: //Trainingprocessinincrementalsteps(𝑡 ≥2)
2: Dˆ 𝑡 =D𝑡 ∪M𝑡−1; ⊲Rehearsal
∟
∟ 3: repeat
�� �� ��,�� ��
��
�� ��,� ∟�
��
4 5:
:
L L𝑑𝑐𝑏 𝑚𝑐 ←← EE qq .. 97 ;; ⊲CIL-B ⊲al Dan isc te rd ibC utla ios nsifi Mc aa rti go in nL Lo os ss
s
∟ ��,�� 6: L𝑘𝑑 ←Eq.11; ⊲KnowledgeDistillationLoss
��,�� 7: //UpdatethecurrentmodelviaoptimizingL𝑎𝑙𝑙
8: L𝑎𝑙𝑙 ←Eq.12; ⊲OverallLoss
9: untilreachespredefinedepoch
(a) (b)
twolossterms.Thefirsttermoptimizesthetriplesbyensuring
��
thatthedistancefromtheanchortothepositiveembeddingisless
∟∟
∟ �� thanitsdistancetothenegativeclassdistributionsbythemargin
��,�� �� ��,�� �� 𝑚,ratherthanmerelytothenegativeembeddings.Byoptimizing
�� ∟ ��
thisterm,thedistributionmarginlosscanpushthesamplesofold
��,��
��,�� classesawayfromthenewclassdistributionstofacilitatetheinter-
classseparation(showninFig.2b).Thesecondtermattemptsto
(c) (d) maintaintheanchorembeddingwithinthedistributionrangeofits
correspondingclass,thusobtainingcompactintra-classclustering
Figure2:(a)Thevanillamarginlossforcesthecosinesim- (showninFig.2d).Accordingly,thedistributionmarginlosscanbe
ilarity betweenℎ 𝑎 and𝑤 𝑝 to be larger than that between formulatedas:
ℎ (b𝑎 )a On ud r𝑤 d𝑛 isw tri it bh uo tiu ot nc mon as ri gd ie nri ln osg st ah ie md sis totr pib uu st hio ℎn 𝑎s ae wp aa yra ft ri oo mn.
L𝑑𝑚 =
∑︁ ∑︁ 𝑚𝑎𝑥(cid:8) 0,(cid:10)ℎ 𝑖𝑡,𝑤ˆ𝑐(cid:11) −(cid:10)ℎ 𝑖𝑡,𝑤 𝑦𝑖(cid:11) +𝑚(cid:9)
thedistributionofthenegativeclassinsteadofjust𝑤 𝑛,thus 𝑖∈M𝑡−1𝑐∈Y𝑡 (9)
mitigatingfeaturespaceoverlap.(c)Thevanillamarginloss + ∑︁ 𝑚𝑎𝑥(cid:8) 0,(cid:10)𝑤ˆ𝑦𝑖,𝑤 𝑦𝑖(cid:11) −(cid:10)ℎ 𝑖𝑡,𝑤 𝑦𝑖(cid:11)(cid:9),
f
m
tra ui al tys hrto
e
clsm
au
sli stn .ii (nm d)ℎi T𝑎ze hbt eeh
i
dne
ig
sin
td
rt
i
ir
s
ba
t
ua-c
tn
il
t
oas nfs
ro
md mi as rt
t
ga
h
in
e
nc ce
le
oa
n
sd ste
e
erq nu
o
sa
f
ut
i
re
t
el
s
sy g, trw
ho
ah
u
ti nc ℎdh
𝑎-
where𝑤ˆ𝑐
represent𝑖 s∈ tM he𝑡− d1
istributionrangeofclass𝑐.Specifically,
wemodelthedatadistributionofeachclassinthefeaturespaceby
remainswithinitscorrespondingclassdistribution,enhanc-
applyingaGaussiandistributionaroundtheircentroids.However,
ingintra-classcompactness.
duetotheimbalancednumberofsamplesacrossdifferentclasses,
thefeaturesofclasseswithlimitedinstancesmaygetsqueezedinto
marginlossaimstoensurethatthedistancefromtheanchortothe anarrowareainthefeaturespace[47].Asaresult,weassignalarger
positive(embeddingoftheground-trutholdclass)islessthanthe distributionrangetothemajorityclassesandamorerestricted
distanceoftheanchorfromthenegative(embeddingofthenew rangetotheminorityclasses:
class)tomeetapredefinedmargin𝑚,whichcanbecomputedas: 𝑞 𝑐
𝑤ˆ𝑐 =𝑤 𝑐 +𝜂∗𝑟ˆ𝑐, 𝑟ˆ𝑐 = (cid:205) 𝑞 , (10)
L𝑚 = ∑︁ ∑︁ 𝑚𝑎𝑥(cid:8) 0,(cid:10)ℎ 𝑖𝑡,𝑤 𝑐(cid:11) −(cid:10)ℎ 𝑖𝑡,𝑤 𝑦𝑖(cid:11) +𝑚(cid:9), (8) 𝑖∈Y 1:𝑡 𝑖
𝑖∈M𝑡−1𝑐∈Y𝑡 where𝑟ˆ𝑐 representstheinherentratioofclass𝑐 amongallseen
where⟨·,·⟩denotesthecosinesimilarityandthemargin𝑚issetto
classes,and𝜂 ∼N(0,1)isaGaussiannoisewhichhasthesame
dimensionastheclassifierweight.
0.4forallexperiments.
Topreventforgettingandmaintainthediscriminationability,
However,thevanillamarginlossexhibitstwolimitations.First,
wealsoapplyknowledgedistillationloss[19]tobuildamapping
itonlyfocusesonthetriplet:anchor,positive,andnegativeembed-
betweentheoldandthecurrentmodel:
dings.Evenifthedistancefromtheanchortothenegativeexceeds
t ch loa st eto orth ee vep no osi vt eiv re lab py ,ta hem rea br ygi in n𝑚 tro, dth ue ci ir ngdi pst or ti eb nu tt ii ao ln as mm ba igy ur ie tim esai in n L𝑘𝑑 = (cid:12) 1 (cid:12) ∑︁ ∑︁ (cid:13) (cid:13) (cid:13)𝑝 𝑖𝑡 ,𝑐 −𝑝 𝑖𝑡 ,− 𝑐1(cid:13) (cid:13) (cid:13). (11)
classification(showninFig.2a).Second,whilethevanillamargin (cid:12) (cid:12)Dˆ 𝑡(cid:12) (cid:12)𝑖∈Dˆ 𝑡𝑐∈Y 1:𝑡−1
lossaimstoseparatetheground-trutholdclassfromnewclasses Therefore,theoveralllossisdefinedas:
(maximizinginter-classdistance),itfailstoadequatelyaddressthe
L𝑎𝑙𝑙 =L𝑐𝑏𝑐 +𝜆 𝑑L𝑑𝑚+𝜆 𝑘L𝑘𝑑, (12)
minimizationofintra-classdistance,oftenleadingtolargeintra-
classclustering(showninFig.2c). where𝜆 𝑑 and𝜆 𝑘 arethehyper-parametersforbalancingtheim-
Toaddresstheabovelimitations,wetrytorestoretheclassdis- portanceofeachloss.Weshowtheguidelineofourmethodat
tributionanddesignanoveldistributionmarginlossthatcontains incrementalsteptinAlg.1.AddressingImbalanceforClassIncrementalLearninginMedicalImageClassification ACMMM,2024,Melbourne,Australia
4 EXPERIMENTS 4-2(3steps) 4-1(5steps)
Method
4.1 ExperimentalSetups Acc Fgt Acc Fgt
Datasets.Followingthebenchmarksetting[6],weevaluatethe iCaRL[38] 93.0±0.2 6.8±1.0 91.1±1.8 9.0±3.3
performanceonCCH5000[25],HAM10000[42],andEyePACS[8]. UCIR[20] 93.9±0.3 4.4±0.9 92.0±1.0 5.5±2.6
• CCH5000:consistsofhistologicalimagesinhumancolorec- PODNET[14] 92.0±0.3 5.2±0.4 89.2±0.5 6.0±1.2
talcancer.Thisdatasetcontains8differentclasseswith625 DER[50] 93.0±0.5 6.4±1.4 91.0±1.7 5.6±1.9
imagesperclass:tumor,stroma,complex,lympho,debris, LO2LN[6] 94.6±0.4 4.0±0.8 94.5±0.8 3.9±2.0
mucosa,adipose,andempty. Ours 95.5±0.2 2.3±0.7 95.2±0.2 2.1±1.5
• HAM10000:consistsof10,015skincancerimages,including
Table1:ExperimentalresultsonCCH5000underthreedif-
seventypesofskinlesions:melanoma,melanocyticnevus,
ferentclassorders.Numbersinbolddenotethebestresults.
basalcellcarcinoma,actinickeratosis,benignkeratosis,der-
matofibroma,andvascularlesions.Thedistributionratiosfor
eachtypeareasfollows:3.27%,5.13%,10.97%,1.15%,11.11%,
66.95%,and1.42%,whichindicatesasevereclassimbalance.
3-2(3steps) 3-1(5steps)
• EyePACS:iscommonlyusedforthetaskofdiabeticretinopa- Method
Acc Fgt Acc Fgt
thy (DR) classification. EyePACS dataset contains 35,126
retinaimagesfortraining,whicharecategorizedintofive iCaRL[38] 76.3±3.1 20.1±12.6 68.3±2.8 25.3±4.5
stagesofDR.Specifically,thereare25,810imageslabeled UCIR[20] 79.1±1.4 16.8±9.5 74.1±3.1 16.3±9.1
asnoDR,2,443asmildDR,5,292asmoderateDR,873as PODNET[14] 75.6±2.2 20.5±2.1 66.3±2.3 17.3±4.8
severeDR,and708asproliferativeDRimages.Itisworth DER[50] 76.2±2.8 24.8±10.9 66.9±4.5 24.7±4.8
notingthatthisdatasetisalsohighlyimbalanced. LO2LN[6] 82.0±1.3 12.8±3.3 78.1±3.4 10.1±3.9
Evaluationprotocols.Followingtheexperimentalprotocolsin[6], Ours 85.0±3.1 8.0±3.5 80.9±2.9 5.2±2.5
weevaluateourmethodfordifferentscenarios,suchas4-1,4-2, Table2:ExperimentalresultsonHAM10000underthreedif-
3-1,and3-2.Ineachscenario,thenumbersindicatethenumberof ferentclassorders.Numbersinbolddenotethebestresults.
baseandnewclasses,respectively.Forexample,consideringthe
HAM10000datasetwith7classes,thescenarioof3-2corresponds
tolearning3classesattheinitialstepandsubsequentlyadding2
newclassesateachincrementalstep,requiringatotalof3training 3-1(3steps)
Method
steps. Acc Fgt
Metrics.Followingpreviouswork[6],weevaluateourmethod
iCaRL[38] 64.4±3.3 17.8±4.6
basedontwostandardmetrics:AverageAccuracy(Acc)andAverage
UCIR[20] 70.2±7.6 15.4±11.4
Forgetting(Fgt).Let𝑎 𝑡,𝑖 betheaccuracyofthemodelevaluatedon
PODNET[14] 63.3±5.4 22.8±4.9
thetestsetofclassesinY𝑖 aftertrainingonthefirst𝑡 steps.The
DER[50] 58.7±9.4 30.2±6.9
AverageAccuracyisdefinedas:
LO2LN[6] 81.9±2.5 -0.2±0.8
(cid:18) (cid:19)
Acc= 𝑇1 (cid:205)𝑇 𝑡=1 1 𝑡(cid:205) 𝑖𝑡 =1𝑎 𝑡,𝑖 , (13) Ours 82.8±2.8 -0.5±0.7
Table3:ExperimentalresultsonEyePACSunderthreedif-
whichmeasurestheaverageclassificationaccuracyofthemodel ferentclassorders.Numbersinbolddenotethebestresults.
untilstep𝑇.TheAverageForgettingisdefinedas:
(cid:20) (cid:21)
Fgt= 𝑇1 (cid:205)𝑇 𝑡=1 𝑡−1 1(cid:205) 𝑖𝑡 =− 11 𝑗∈m [1,a 𝑡x −1](cid:0)𝑎 𝑗,𝑖 −𝑎 𝑡,𝑖(cid:1) , (14)
usethesamememorysettingforeverycomparedmethod,i.e.,a
whichmeasuresanestimateofhowmuchthemodelforgetsby fixednumberof20trainingexamplesperclassareselectedviathe
averagingthedeclineinaccuracyfromthepeakperformancetoits
herdingselectionstrategy[48]andstoredinmemoryM.Further-
currentperformance. more,weconductallexperimentsonthreedifferentclassorders
Comparedmethods.Todemonstratethesuperiorityofourmethod,
andreportthemeans±standarddeviationsoverthreeruns.
wefirstcompareittoclassicalincrementallearningapproaches:
4.2 PerformanceComparison
iCaRL[38],UCIR[20],PODNet[14],andDER[50].Besides,we
alsocomparetothecurrentstate-of-the-artmethod:LO2LN[6]. AsshowninTab.1,2,and3,wereporttheexperimentalresults
Implementationdetails.Asin[6],weadoptacosinenormaliza- onthreebenchmarkdatasets:CCH5000,HAM10000,andEyePACS,
tionclassifierwithaResNet-18[18]backbonepre-trainedonthe respectively.
ImageNet[10].OurmethodisimplementedinPyTorch[37],and CCH5000.Wecanseethatourmethodachievesstate-of-the-art
weemploySGDwithamomentumvalueof0.9andweightdecayof performanceintermsofAccandFgtonbothsettings.Specifically,
0.0005foroptimization.Duringtraining,thebatchsizeissetto32 ourmethodsurpassesLO2LNby1.7%onthe4-2settingand1.8%
fortheCCH5000andHAM10000datasetsand128fortheEyePACS onthe4-1settingintermsofFgt,indicatingtheeffectivenessof
datasetineachlearningstep.Notethat,forafaircomparison,we ourmethodinovercomingforgetting.ACMMM,2024,Melbourne,Australia XuzeHao,WenqianNi,XuhaoJiang,WeiminTan*,BoYan*
90
98 90
96 80
94 80
70
92
90 iCaRL 70 60
UCIR
88
PODNet
86 DER 60 50
84 LO2LN 40
Ours
82 50
0 1 2 3 4 0 1 2 3 4 0 1 2
Incremental Step Incremental Step Incremental Step
(a)4-1onCCH5000 (b)3-1onHAM10000 (c)3-1onEyePACS
Figure3:AccuracyateachsteponCCH5000,HAM10000,andEyePACS.
50
14
40
12 40
10 30
30
8
6 20 20
4
10 10
2
0
0 0
0 1 2 3 4 0 1 2 3 4 0 1 2
Incremental Step Incremental Step Incremental Step
(a)4-1onCCH5000 (b)3-1onHAM10000 (c)3-1onEyePACS
Figure4:ForgettingateachsteponCCH5000,HAM10000,andEyePACS.
HAM10000.DifferentfromtheCCH5000dataset,theHAM10000 L𝑐𝑏𝑐 L𝑑𝑚 Acc Fgt
datasetisahighlyimbalanceddermoscopydataset.Experimental
resultsdemonstratethatourmethodsignificantlyimprovesthe (cid:37) (cid:37) 67.6±1.6 30.4±11.9
performanceontheHAM10000dataset,benefitingfromthestrong (cid:37) (cid:33) 80.2±2.8 8.6±4.0
abilitytoaddressclassimbalance.Tobemorespecific,compared (cid:33) (cid:37) 83.6±2.9 13.1±6.0
withtheSOTAmethod,weimprovetheaccuracyfrom82.0%to
(cid:33) (cid:33) 85.0±3.1 8.0±3.5
85.0%onthe3-2setting.Onthe3-1setting,weachieveanoverall
performanceof80.9%,whichis2.8%higherthanLO2LN’s78.1%. Table4:Performancecontributionofeachcomponentonthe
Moreover,theaverageforgettingisalsoreducedby4.8%(3-2setting) HAM100003-2setting.
and4.9%(3-1setting).
EyePACS.Furthermore,wepresentacomparisonofdifferentmeth-
odsonthechallengingEyePACSdataset.Ourproposedmethod
notonlydemonstratessignificantlyhigheraverageaccuracybut and our method over time. This demonstrates that our method
alsoachievesloweraverageforgettingthantheotherbaselines. benefitsclassincrementallearninginmedicalimageclassification
Notably,itsurpassesLO2LNby0.9%intermsofAccandoutper- andoutperformspriorworks.
formsPODNetandDERbysubstantialmarginsof19.5%and24.1%, Forgetting.Fig.4depictstheaverageforgettingacrosseachincre-
respectively. mentalstepforthreedatasets.Theforgettingofmostmethodsin-
creasesrapidlyasnewclassesarrive,whileourmethodconsistently
4.3 AnalysisofIncrementalPerformance outperformstheSOTAmethods,indicatingimprovedresilienceto
catastrophicforgetting.
Accuracy.AsshowninFig.3,wedisplaytheaverageincremental
performanceofeachstepforthreedatasets.Accordingtothese
4.4 AblationStudy
curves,itisevidentthattheperformancesofallmethodsaresimilar
inthefirststep,butthebaselinessufferfromasignificantdropas Impactofeachcomponent.InTab.4,wepresentanablation
thelearningstepsincrease.Incontrast,ourmethodeffectivelyslows analysisontheHAM100003-2settingtoevaluatetheeffectofeach
downthedrop,leadingtoanincreasinggapbetweenthebaselines proposedcomponent.Thefirstrowreferstothebaseline,whichis
)%(
ycaruccA
)%(
gnittegroF
)%(
ycaruccA
)%(
gnittegroF
)%(
ycaruccA
)%(
gnittegroFAddressingImbalanceforClassIncrementalLearninginMedicalImageClassification ACMMM,2024,Melbourne,Australia
Figure5:Thet-SNEvisualizationoffeaturedistributionsofw/omarginloss(left),marginrankingloss(middle),andour
distributionmarginloss(right)ontheCCH5000dataset.
Classificationloss Acc Fgt Method Acc Fgt
CE 80.2±2.8 8.6±4.0 iCARL 68.3±2.8 25.3±4.5
Focal[29] 81.5±2.4 12.6±5.2 +L𝑑𝑚 69.2±3.1 +0.9 23.7±8.1 +1.6
CBFocal[9] 82.9±3.1 14.5±6.3 +L𝑐𝑏𝑐 70.8±2.8 +2.5 21.9±6.5 +3.4
logit-balanced(Eq.4) 83.1±3.0 14.2±6.0
+L𝑐𝑏𝑐 +L𝑑𝑚 71.6±2.4 +3.3 19.6±7.3 +5.7
CIL-balanced(Eq.7) 85.0±3.1 8.0±3.5 UCIR 74.1±3.1 16.3±9.1
Table5:Performanceofdifferentclassificationlossesonthe
+L𝑑𝑚 75.7±4.6 +1.6 13.9±8.2 +2.4
HAM100003-2setting. +L𝑐𝑏𝑐 76.4±3.4 +2.3 2.1±4.0 +14.2
+L𝑐𝑏𝑐 +L𝑑𝑚 77.1±4.6 +3.0 1.5±6.0 +14.8
Table7:ImpactofintegratingtheCIL-balancedclassification
Marginloss Acc Fgt lossL𝑐𝑏𝑐 andthedistributionmarginlossL𝑑𝑚withexisting
methodsontheHAM10003-1setting.Theredhighlightsthe
w/omarginloss 83.6±2.9 13.1±6.0
relativeimprovement.
Marginrankingloss[20] 83.9±2.4 12.6±5.1
Distributionmarginloss(Eq.9) 85.0±3.1 8.0±3.5
Table 6: Performance of different margin losses on the
HAM100003-2setting. proposedmethodsconsistentlyoutperformtheotherclassification
lossobjectives,indicatingtheeffectivenessofthemtoaddressthe
imbalanceissue.Furthermore,theCIL-balancedclassificationloss
(Eq.7)achievesanadditional1.9%improvementcomparedtothe
trainedwiththecross-entropyloss(CE)andtheknowledgedistilla- logit-balancedclassificationloss(Eq.4),benefitingfromthescale
tionlossL𝑘𝑑.Firstly,weobservethatthedistributionmarginloss factor𝛾 tostrengthenthelearningofoldclasses.
L𝑑𝑚 bringsasignificantcontributionwhenappliedalone,improv- EffectofDistributionMarginLoss.Toverifytheeffectiveness
ingtheperformanceby12.6%intermsofAcc.Secondly,whenwe ofourdistributionmarginlossobjectives,weconductexperiments
replaceCEwiththeCIL-balancedclassificationlossL𝑐𝑏𝑐,theaver- ontheHAM100003-2settingcombinedwiththeknowledgedis-
ageaccuracyisimprovedfrom67.6%toanotable83.6%.Finally,the tillationloss L𝑘𝑑 andourCIL-balancedclassificationloss L𝑐𝑏𝑐.
combinationofL𝑑𝑚 andL𝑐𝑏𝑐 furtherimprovestheperformance, TheresultspresentedinTab.6demonstratethatourdistribution
demonstratingtheeffectofbothproposedcomponents. marginlossbringssignificantimprovementscomparedtocases
EffectofCIL-BalancedClassificationLoss.Weinvestigatethe withoutthemarginlossandwiththemarginrankingloss[20].
impactofdifferentclassificationlossesontheHAM100003-2setting To further illustrate the advantages of our method, we present
whencombinedwiththeknowledgedistillationlossL𝑘𝑑 andour t-SNE[43]visualizationsoffeaturedistributionswithdifferentmar-
distribution margin loss L𝑑𝑚. As shown in Tab. 5, we present ginlossobjectivesfortheCCH5000dataset,asshowninFig.5.In
resultsofusingcross-entropyloss(CE),focalloss(Focal)[29],class- theabsenceofmarginloss,weobservelargeintra-classclusters
balancedfocalloss(CBFocal)[9],andourproposedmethods(logit- (bluerectangle)andsignificantinter-classoverlapinfeaturespace
balancedandCIL-balanced).Itcanbeobservedthatbothofour (redcircle).Whenemployingthemarginrankingloss,theissueofACMMM,2024,Melbourne,Australia XuzeHao,WenqianNi,XuhaoJiang,WeiminTan*,BoYan*
0.1 84.1 84.3 84.4 84.2 83.0 85.0 99 56 CCH5000
Method
50-10(6steps) 50-5(11steps)
84.5 94 Conv LT Conv LT
0.25 84.3 84.3 84.4 84.8 83.7 0.1 0.3 0.5 0.7 0.9
86
HAM10000 UCIR[20] 61.2 42.7 58.7 42.2 0.5 84.2 84.3 84.6 84.8 84.3 84.0 84
82 PODNET[14] 63.2 44.1 61.2 44.0
0.75 84.2 84.3 84.6 85.0 84.2 0.1 0.3 0.5 0.7 0.9
83.5 84 LWS[30] 64.6 44.4 62.6 44.4
EyePACS
1.0 84.1 84.5 84.5 84.3 83.8 83 Ours 65.8 48.2 63.9 47.5
0.1 0.2 0.3 0.4 0.5 83.0 82 0.1 0.3 0.5 0.7 0.9
d Table8:AverageaccuracyonCIFAR100intheconventional
(a) (b) (Conv)andlong-tailed(LT)scenarios.Numbersinboldde-
notethebestresults.
Figure6:Sensitivitystudyofhyper-parameters.(a)𝜆
𝑑
and𝜆
𝑘
ontheHAM100003-2setting.(b)𝛼 onthreedatasets.
60
75
overlapismitigatedtosomeextent(redcircle)comparedtothe 55
70
methodwithoutmarginloss.Finally,byoptimizingourdistribution 50
marginloss,weachieveamorepronouncedseparationbetween 65 45
thedistributionsofoldandnewclasses(redcircle),whilesimul- 60 40
UCIR
taneouslyensuringthattherepresentationsofoldclassesbecome 55 PODNet 35
LWS
morecompact(bluerectangle). 50 Ours 30
Abilitytointegratewithotherexistingmethods.Ourproposed 50 60 70 80 90 100 50 60 70 80 90 100
Incremental Step Incremental Step
methodscanbeeasilyintegratedwithotherexistingCILmethods.
(a)50-5(Conv)onCIFAR100 (b)50-5(LT)onCIFAR100
Todemonstratethis,weconductexperimentsutilizingiCaRL[38]
andUCIR[20]ontheHAM100003-1setting.Specifically,were-
Figure7:IncrementalaccuracyonCIFAR10050-5settingfor
placetheclassificationlossineachbaselinewithourCIL-balanced
bothconventional(Conv)andlong-tailed(LT)scenarios.
classificationlossandincorporateourdistributionmarginloss.As
showninTab.7,theaccuracy(Acc)forbothbaselinescanbeim-
provedbyabout3%withtheintegrationofourmethods.More
notably,ourapproacheseffectivelyreduceforgetting(Fgt)by5.7%
in[30],weconductexperimentsunderbothconventional(Conv)
and14.8%foriCaRLandUCIR,respectively.
and long-tail (LT)scenarios. In theconventionalscenario, each
Sensitivitystudyofhyper-parameters.Inthispaper,thereare
classhas500trainingsamplesfortraining.Conversely,thelong-
threehyper-parametersduringtraining:theweightofthedistribu-
tailedscenariofollowsanexponentialdecayinsamplesizesacross
tionmarginloss𝜆 𝑑,theweightoftheknowledgedistillationloss
classes,wheretheratiobetweentheleastandthemostfrequent
𝜆 𝑘,andthetrade-offcoefficient𝛼.Wefirstconductexperiments
classis0.01.AsillustratedinTab.8,ourmethodachievessuperior
toexploretheimpactsofdifferent𝜆 𝑑 and𝜆 𝑘 ontheHAM10000
resultsinallsettings.Specifically,weobserveamoresignificant
3-2setting.AsshowninFig.6a,wevary𝜆 𝑑 withintherangeof
improvementinthelong-tailscenario,furthervalidatingtheeffec-
{0.1,0.2,0.3,0.4,0.5},and𝜆 𝑘withintherangeof{0.1,0.25,0.5,0.75,
tivenessofourmethodinaddressingtheclassimbalanceproblem
1.0},resultinginatotalof25comparedresults.Fromtheresults,
inclassincrementallearning.Furthermore,wepresentthedynamic
weconsistentlyobservesatisfactoryperformancefromourmodel,
performancechangesduringtheincrementallearningprocessin
demonstratingitsrobustnesstotheselectionof𝜆 𝑑 and𝜆 𝑘.
Fig.7.Itisevidentthatwithmorelearningsteps,thegapbetween
Toinvestigatetheimpactofdifferentvaluesof𝛼 onaddressing
thebaselinesandourmethodwidens,andourmethod’sperfor-
theimbalancebetweentheoldandnewclasses,weevaluatethe
manceremainssuperioracrossdifferentscenarios(conventional
accuracybyvarying𝛼 from{0.1,0.3,0.5,0.7,0.9}onthreebench-
andlong-tailed)throughoutmostofthelearningsteps.
markdatasets.AsshowninFig.6b,theresultsindicatethatthe
accuracygraduallyimprovesas𝛼 growslargerinitially,whileit
startstodeclinewhen𝛼iscloseto1.Sincethedatadistributiondif- 5 CONCLUSION
fersacrossdatasets,theselectionofthetrade-offcoefficient𝛼 also Inthispaper,weproposetwosimpleyeteffectiveplug-inlossfunc-
varies.Specifically,theoptimalvaluesof𝛼 forthethreedatasets tionsforclassincrementallearninginmedicalimageclassification.
are0.7,0.5,and0.3,respectively. First,toaddressthechallengeofclassifierbiascausedbyclassim-
Longerincrementallearning.Inclassincrementallearning,a balance,weintroduceaCIL-balancedclassificationlossvialogit
keychallengeiscatastrophicforgetting,whichbecomesmorepro- adjustment.Second,weproposeanoveldistributionmarginloss
nouncedasthenumberoflearningclassesincreases[13,20,38]. thataimstoenforceinter-classdiscrepancyandintra-classcom-
To quantify the robustness of our method in overcoming cata- pactnesssimultaneously.Ourextensiveexperimentalevaluation
strophicforgetting,weevaluateitontwolonger-stepprotocols: demonstratesthestate-of-the-artperformanceofourmethodacross
50-10(6steps)and50-5(11steps),employingthemorechallenging variousscenariosonmedicalimagedatasets:CCH5000,HAM10000,
CIFAR100dataset.Followingtheexperimentalprotocoloutlined andEyePACS.
k
ccA
ccA
ccA
)%(
ycaruccA
)%(
ycaruccAAddressingImbalanceforClassIncrementalLearninginMedicalImageClassification ACMMM,2024,Melbourne,Australia
ACKNOWLEDGMENTS
[22] AhmetIscen,JeffreyZhang,SvetlanaLazebnik,andCordeliaSchmid.2020.
Memory-efficientincrementallearningthroughfeatureadaptation.InComputer
ThisworkwassupportedinpartbyNSFC(No.U2001209,62372117).
Vision–ECCV2020:16thEuropeanConference,Glasgow,UK,August23–28,2020,
ThecomputationsinthisresearchwereperformedusingtheCFFF Proceedings,PartXVI16.Springer,699–715.
platformofFudanUniversity. [23] XuhaoJiang,WeiminTan,RiCheng,ShiliZhou,andBoYan.2022.Learningpar-
allaxtransformernetworkforstereoimagejpegartifactsremoval.InProceedings
ofthe30thACMInternationalConferenceonMultimedia.6072–6082.
[24] XuhaoJiang,WeiminTan,TianTan,BoYan,andLiquanShen.2023. Multi-
REFERENCES modalitydeepnetworkforextremelearnedimagecompression.InProceedings
oftheAAAIConferenceonArtificialIntelligence,Vol.37.1033–1041.
[1] HongjoonAhn,JihwanKwak,SubinLim,HyeonsuBang,HyojunKim,andTaesup [25] JakobNikolasKather,Cleo-AronWeis,FrancescoBianconi,SusanneMMelchers,
Moon.2021.Ss-il:Separatedsoftmaxforincrementallearning.InProceedingsof LotharRSchad,TimoGaiser,AlexanderMarx,andFrankGerritZöllner.2016.
theIEEE/CVFInternationalconferenceoncomputervision.844–853. Multi-classtextureanalysisincolorectalcancerhistology.Scientificreports6,1
[2] DiegoArdila,AtillaPKiraly,SujeethBharadwaj,BokyungChoi,JoshuaJReicher, (2016),1–11.
LilyPeng,DanielTse,MozziyarEtemadi,WenxingYe,GregCorrado,etal.2019. [26] JamesKirkpatrick,RazvanPascanu,NeilRabinowitz,JoelVeness,Guillaume
End-to-endlungcancerscreeningwiththree-dimensionaldeeplearningonlow- Desjardins,AndreiARusu,KieranMilan,JohnQuan,TiagoRamalho,Agnieszka
dosechestcomputedtomography.Naturemedicine25,6(2019),954–961. Grabska-Barwinska,etal.2017.Overcomingcatastrophicforgettinginneural
[3] EdenBelouadahandAdrianPopescu.2019. Il2m:Classincrementallearning networks.Proceedingsofthenationalacademyofsciences114,13(2017),3521–
withdualmemory.InProceedingsoftheIEEE/CVFinternationalconferenceon 3526.
computervision.583–592. [27] KibokLee,KiminLee,JinwooShin,andHonglakLee.2019.Overcomingcata-
[4] ArslanChaudhry,Marc’AurelioRanzato,MarcusRohrbach,andMohamedElho- strophicforgettingwithunlabeleddatainthewild.InProceedingsoftheIEEE/CVF
seiny.2018.Efficientlifelonglearningwitha-gem.arXivpreprintarXiv:1812.00420 InternationalConferenceonComputerVision.312–321.
(2018). [28] ZhizhongLiandDerekHoiem.2017.Learningwithoutforgetting.IEEEtransac-
[5] GalChechik,VarunSharma,UriShalit,andSamyBengio.2010. Largescale tionsonpatternanalysisandmachineintelligence40,12(2017),2935–2947.
onlinelearningofimagesimilaritythroughranking.JournalofMachineLearning [29] Tsung-YiLin,PriyaGoyal,RossGirshick,KaimingHe,andPiotrDollár.2017.
Research11,3(2010). Focallossfordenseobjectdetection.InProceedingsoftheIEEEinternational
[6] EvelynChee,MongLiLee,andWynneHsu.2023.Leveragingoldknowledge conferenceoncomputervision.2980–2988.
tocontinuallylearnnewclassesinmedicalimages.InProceedingsoftheAAAI [30] XialeiLiu,Yu-SongHu,Xu-ShengCao,AndrewDBagdanov,KeLi,andMing-
ConferenceonArtificialIntelligence,Vol.37.14178–14186. MingCheng.2022.Long-tailedclassincrementallearning.InEuropeanConference
[7] Liang-ChiehChen,GeorgePapandreou,FlorianSchroff,andHartwigAdam.2017. onComputerVision.Springer,495–512.
Rethinkingatrousconvolutionforsemanticimagesegmentation.arXivpreprint [31] YaoyaoLiu,BerntSchiele,andQianruSun.2021.Adaptiveaggregationnetworks
arXiv:1706.05587(2017). forclass-incrementallearning.InProceedingsoftheIEEE/CVFconferenceon
[8] JorgeCuadrosandGeorgeBresnick.2009.EyePACS:anadaptabletelemedicine ComputerVisionandPatternRecognition.2544–2553.
systemfordiabeticretinopathyscreening.Journalofdiabetesscienceandtech- [32] YaoyaoLiu,YutingSu,An-AnLiu,BerntSchiele,andQianruSun.2020.Mnemon-
nology3,3(2009),509–516. icstraining:Multi-classincrementallearningwithoutforgetting.InProceedings
[9] YinCui,MenglinJia,Tsung-YiLin,YangSong,andSergeBelongie.2019.Class- oftheIEEE/CVFconferenceonComputerVisionandPatternRecognition.12245–
balancedlossbasedoneffectivenumberofsamples.InProceedingsoftheIEEE/CVF 12254.
conferenceoncomputervisionandpatternrecognition.9268–9277. [33] ScottMayerMcKinney,MarcinSieniek,VarunGodbole,JonathanGodwin,
[10] JiaDeng,WeiDong,RichardSocher,Li-JiaLi,KaiLi,andLiFei-Fei.2009.Imagenet: NatashaAntropova,HutanAshrafian,TrevorBack,MaryChesus,GregSCor-
Alarge-scalehierarchicalimagedatabase.In2009IEEEconferenceoncomputer rado,AraDarzi,etal.2020.InternationalevaluationofanAIsystemforbreast
visionandpatternrecognition.Ieee,248–255. cancerscreening.Nature577,7788(2020),89–94.
[11] SonglinDong,XiaopengHong,XiaoyuTao,XinyuanChang,XingWei,and [34] AdityaKrishnaMenon,SadeepJayasumana,AnkitSinghRawat,HimanshuJain,
YihongGong.2021.Few-shotclass-incrementallearningviarelationknowledge AndreasVeit,andSanjivKumar.2020.Long-taillearningvialogitadjustment.
distillation.InProceedingsoftheAAAIConferenceonArtificialIntelligence,Vol.35. arXivpreprintarXiv:2007.07314(2020).
1255–1263. [35] OleksiyOstapenko,MihaiPuscas,TassiloKlein,PatrickJahnichen,andMoin
[12] AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,DirkWeissenborn,Xi- Nabi.2019.Learningtoremember:Asynapticplasticitydrivenframeworkfor
aohuaZhai,ThomasUnterthiner,MostafaDehghani,MatthiasMinderer,Georg continuallearning.InProceedingsoftheIEEE/CVFconferenceoncomputervision
Heigold,SylvainGelly,etal.2020.Animageisworth16x16words:Transformers andpatternrecognition.11321–11329.
forimagerecognitionatscale.arXivpreprintarXiv:2010.11929(2020). [36] ŞabanÖztürkandTolgaÇukur.2022.Deepclusteringviacenter-orientedmargin
[13] ArthurDouillard,YifuChen,ArnaudDapogny,andMatthieuCord.2021.Plop: free-tripletlossforskinlesiondetectioninhighlyimbalanceddatasets. IEEE
Learningwithoutforgettingforcontinualsemanticsegmentation.InProceedings JournalofBiomedicalandHealthInformatics26,9(2022),4679–4690.
oftheIEEE/CVFconferenceoncomputervisionandpatternrecognition.4040–4050. [37] AdamPaszke,SamGross,FranciscoMassa,AdamLerer,JamesBradbury,Gregory
[14] ArthurDouillard,MatthieuCord,CharlesOllion,ThomasRobert,andEduardo Chanan,TrevorKilleen,ZemingLin,NataliaGimelshein,LucaAntiga,etal.2019.
Valle.2020. Podnet:Pooledoutputsdistillationforsmall-tasksincremental Pytorch:Animperativestyle,high-performancedeeplearninglibrary.Advances
learning.InComputerVision–ECCV2020:16thEuropeanConference,Glasgow,UK, inneuralinformationprocessingsystems32(2019).
August23–28,2020,Proceedings,PartXX16.Springer,86–102. [38] Sylvestre-AlviseRebuffi,AlexanderKolesnikov,GeorgSperl,andChristophH
[15] ArthurDouillard,AlexandreRamé,GuillaumeCouairon,andMatthieuCord.2022. Lampert.2017.icarl:Incrementalclassifierandrepresentationlearning.InPro-
Dytox:Transformersforcontinuallearningwithdynamictokenexpansion.In ceedingsoftheIEEEconferenceonComputerVisionandPatternRecognition.2001–
ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition. 2010.
9285–9295. [39] HanulShin,JungKwonLee,JaehongKim,andJiwonKim.2017. Continual
[16] RobertMFrench.1999.Catastrophicforgettinginconnectionistnetworks.Trends learningwithdeepgenerativereplay.Advancesinneuralinformationprocessing
incognitivesciences3,4(1999),128–135. systems30(2017).
[17] XuzeHao,XuhaoJiang,WenqianNi,WeiminTan,andBoYan.2024.Prompt- [40] JamesSmith,Yen-ChangHsu,JonathanBalloch,YilinShen,HongxiaJin,and
GuidedSemantic-awareDistillationforWeaklySupervisedIncrementalSemantic ZsoltKira.2021. Alwaysbedreaming:Anewapproachfordata-freeclass-
Segmentation.IEEETransactionsonCircuitsandSystemsforVideoTechnology incrementallearning.InProceedingsoftheIEEE/CVFInternationalConferenceon
(2024). ComputerVision.9374–9384.
[18] KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun.2016.Deepresidual [41] LeSun,MingyangZhang,BenyouWang,andPrayagTiwari.2023. Few-Shot
learningforimagerecognition.InProceedingsoftheIEEEconferenceoncomputer Class-IncrementalLearningforMedicalTimeSeriesClassification.IEEEJournal
visionandpatternrecognition.770–778. ofBiomedicalandHealthInformatics(2023).
[19] GeoffreyHinton,OriolVinyals,andJeffDean.2015.Distillingtheknowledgein [42] PhilippTschandl,CliffRosendahl,andHaraldKittler.2018. TheHAM10000
aneuralnetwork.arXivpreprintarXiv:1503.02531(2015). dataset,alargecollectionofmulti-sourcedermatoscopicimagesofcommon
[20] SaihuiHou,XinyuPan,ChenChangeLoy,ZileiWang,andDahuaLin.2019. pigmentedskinlesions.Scientificdata5,1(2018),1–9.
Learningaunifiedclassifierincrementallyviarebalancing.InProceedingsofthe [43] LaurensVanderMaatenandGeoffreyHinton.2008.Visualizingdatausingt-SNE.
IEEE/CVFconferenceonComputerVisionandPatternRecognition.831–839. Journalofmachinelearningresearch9,11(2008).
[21] BingchenHuang,ZhinengChen,PengZhou,JiayinChen,andZuxuanWu.2023. [44] Fu-YunWang,Da-WeiZhou,Han-JiaYe,andDe-ChuanZhan.2022. Foster:
Resolvingtaskconfusionindynamicexpansionarchitecturesforclassincre- Featureboostingandcompressionforclass-incrementallearning.InComputer
mentallearning.InProceedingsoftheAAAIConferenceonArtificialIntelligence, Vision–ECCV2022:17thEuropeanConference,TelAviv,Israel,October23–27,2022,
Vol.37.908–916. Proceedings,PartXXV.Springer,398–414.ACMMM,2024,Melbourne,Australia XuzeHao,WenqianNi,XuhaoJiang,WeiminTan*,BoYan*
[45] LiyuanWang,KuoYang,ChongxuanLi,LanqingHong,ZhenguoLi,andJun [50] ShipengYan,JiangweiXie,andXumingHe.2021.Der:Dynamicallyexpandable
Zhu.2021.Ordisco:Effectiveandefficientusageofincrementalunlabeleddata representationforclassincrementallearning.InProceedingsoftheIEEE/CVF
forsemi-supervisedcontinuallearning.InProceedingsoftheIEEE/CVFConference ConferenceonComputerVisionandPatternRecognition.3014–3023.
onComputerVisionandPatternRecognition.5383–5392. [51] Da-WeiZhou,Qi-WeiWang,Han-JiaYe,andDe-ChuanZhan.2022. Amodel
[46] LiyuanWang,XingxingZhang,KuoYang,LonghuiYu,ChongxuanLi,Lanqing or603exemplars:Towardsmemory-efficientclass-incrementallearning.arXiv
Hong,ShifengZhang,ZhenguoLi,YiZhong,andJunZhu.2022.Memoryreplay preprintarXiv:2205.13218(2022).
withdatacompressionforcontinuallearning.arXivpreprintarXiv:2202.06592 [52] Da-WeiZhou,Han-JiaYe,andDe-ChuanZhan.2021. Co-transportforclass-
(2022). incrementallearning.InProceedingsofthe29thACMInternationalConferenceon
[47] YuchaoWang,JingjingFei,HaochenWang,WeiLi,TianpengBao,LiweiWu,Rui Multimedia.1645–1654.
Zhao,andYujunShen.2023.BalancingLogitVariationforLong-tailedSemantic [53] FeiZhu,ZhenCheng,Xu-YaoZhang,andCheng-linLiu.2021.Class-incremental
Segmentation.InProceedingsoftheIEEE/CVFConferenceonComputerVisionand learningviadualaugmentation. AdvancesinNeuralInformationProcessing
PatternRecognition.19561–19573. Systems34(2021),14306–14318.
[48] MaxWelling.2009.Herdingdynamicalweightstolearn.InProceedingsofthe [54] FeiZhu,Xu-YaoZhang,ChuangWang,FeiYin,andCheng-LinLiu.2021.Proto-
26thAnnualInternationalConferenceonMachineLearning.1121–1128. typeaugmentationandself-supervisionforincrementallearning.InProceedings
[49] YueWu,YinpengChen,LijuanWang,YuanchengYe,ZichengLiu,Yandong oftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.5871–
Guo,andYunFu.2019.Largescaleincrementallearning.InProceedingsofthe 5880.
IEEE/CVFConferenceonComputerVisionandPatternRecognition.374–382.