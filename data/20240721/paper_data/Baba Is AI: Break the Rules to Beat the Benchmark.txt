Baba Is AI: Break the Rules to Beat the Benchmark
NathanCloos1 MeaganJens1 MichelangeloNaim1 Yen-LingKuo2 IgnacioCases1
AndreiBarbu*1 ChristopherJ.Cueva*1
Abstract 1)Theabilitytoidentifyandmanipulaterelevantstimuliin
Humans solve problems by following existing theenvironmentwhileignoringdistractorobjectsandrules.
rulesandprocedures,andalsobyleapsofcreativ-
2) The ability to combine previously seen rules in novel
itytoredefinethoserulesandobjectives.Toprobe
ways.
these abilities, we developed a new benchmark
basedonthegameBabaIsYouwhereanagent The ability to study how an agent explicitly learns rules,
manipulatesbothobjectsintheenvironmentand composesthem,andcrucially,makesorbreakstheserules
rules, represented by movable tiles with words toalterhowtheenvironmentandagentbehaves,prompted
written on them, to reach a specified goal and ustodevelopanewbenchmarkenvironmentbasedonthe
winthegame. Wetestthreestate-of-the-artmulti- puzzlegameBabaIsYou.Inthisgame,theplayeroftencon-
modallargelanguagemodels(OpenAIGPT-4o, trolsacharacternamed“Baba”andmustnavigatethrough
Google Gemini-1.5-Pro and Gemini-1.5-Flash) thegrid-basedworldfilledwithblocks,objects,andtextual
andfindthattheyfaildramaticallywhengeneral- rules. Wecanthinkofthisgameasadynamicenvironment
izationrequiresthattherulesofthegamemustbe where the player interacts with various objects and rules
manipulatedandcombined. toachievespecificgoals. AremarkableaspectofBabaIs
Youisthattherulesofthegamecanbemanipulatedand
rearrangedbytheplayer.
1.Introduction
Rule:{noun}is{property}
refersto
{noun} {object}
Example:
{noun}= ,{object}= ,{property}=
modifies
{property} {object}
theagentcontrols{object}
Plan:break[wallisstop],make[dooriswin],goto[door] theagentwinswhenon{object}
Figure1.EnvironmentbasedonthepuzzlegameBabaIsYou.
theagentisblockedby{object}
Humansdemonstrateremarkableabilitiesinrapidlearning
andadaptivebehaviorwhenfacedwithnovelenvironments Figure2.Activerulesintheenvironmentmodifytheproperties
-notonlylearningandfollowingrulesdictatedbytheen- oftheobjects.Aruleisactivewhenitishorizontallyalignedand
vironmentbutalteringtheserulestoenablenewoutcomes.
hastheform{noun}is{property}.
Theseabilitiesleveragetwokeycomponentsthatweexplore
Figure 1 shows an example game environment. The text
inthispaper:
blocks[babaisyou]indicatetheplayeriscontrollingthe
*Equal senior authors 1MIT 2Department of Computer Sci- whitetriangle,i.e. the[baba]object,andcannowmovethis
ence,UniversityofVirginia,USA.Correspondenceto: Nathan objectthroughtheenvironment. Nowlet’slookforthetext
Cloos<nacloos@mit.edu>,AndreiBarbu<abarbu@mit.edu>, blocksthatspecifyhowtowinthegame. The[iswin]text
ChristopherJ.Cueva<ccueva@gmail.com>.
blocksintheupperrightoftheenvironmentareincomplete
andsotheagentmustrecognizethatthereiscurrentlyno
Proceedings of the 41st International Conference on Machine
Learning,Vienna,Austria.PMLR235,2024.Copyright2024by waytowinthegameuntilthewinningconditionisspecified.
theauthor(s). Thisisaccomplishedbymovingoneoftheavailabletext
1
4202
luJ
81
]LC.sc[
1v92731.7042:viXraBabaIsAI:BreaktheRulestoBeattheBenchmark
Single-roomEnvironment
100 Nodistractor(in-context) Objectdistractor(test) Ruledistractor(test) Object&R (teu sle t)distractor Winruledistractor(test)
75
50
gemini-1.5-pro
25
gemini-1.5-flash
gpt-4o
0 Plan:goto[ball] Plan:goto[door] Plan:goto[key] Plan:goto[ball] Plan:goto[ball]
Nodistractor Object Rule Object&RuleWinrule
(in-context) distractor distractor distractor distractor
(test) (test) (test) (test)
Figure3.AccuracyofLLMsacross5environmentstestingtheabilitytogeneralizeinthepresenceofdistractors.Thetaskistogo
tothewinningobjectspecifiedbythetextboxintheactivewinrule.Accuracydropssubstantiallyonthefinaltaskwherebothanobject
andanactiveruledistractorarepresent.Inthisfinaltasktheirrelevantwinruledoesnotrefertoanyoftheobjectsintheenvironment.
blocksuchas[door]or[ball]tocreatearuleforwinningthe abilityofthesemodelstoevaluatethemdirectlyonvisual
game. Withthisspecificenvironmentallayout,awinning inputsofthegame.
strategyistopushthe[door]blocktocreatetherule,[door
iswin],andthenmovetheagentontothedoorblock,shown
2.Method
ingreen,towinthegame. However,thetextblocks[wall
isstop]arealignedandsothisruleisactiveandtheplayer WefirstpromptLLMswithgeneraltextinstructionstoplay
cannotmovebabathroughtheverticalwallofgraysquares thegame. Thisincludesadescriptionofthepossibleobjects
tocarryoutthisplan. Theplayermustfirstpushoneofthe andtextualruleblocksintheenvironment,andhowactive
blocks in this rule out of alignment to deactivate the rule rulescanchangeobjectproperties(asillustratedinFigure
[wallisstop]. Thefinalplantowinthegameistofirstbreak 2,withtheexactpromptinAppendixA).Importantly,we
therule[wallisstop],thenmaketherule[dooriswin],and specifythataruleisactiveonlyifitfollowstheform“object
finallymoveontothedoorobject. isproperty”andthatthethreeruleblocksmustbealigned
horizontallyintheenvironment.
Asthisexampleillustrates,thisisadynamicenvironment
wheretheagentmustidentifytherelevantobjectsandrules FollowingpreviousworkonLLM-basedagentsandplan-
in the environment and then manipulate the environment ners (Ichter et al., 2022; Huang et al., 2022; Park et al.,
tochangeorcreaterulesforsuccess(Figure2). Weimple- 2023; Valmeekam et al., 2023b; Song et al., 2023; Wang
mented a simplified version of Baba Is You (Baba Is AI) etal.,2024),weaskLLMstooperateatahigherlevelthan
basedontheGymnasiumMinigridenvironment(Chevalier- thelow-levelcontrolofactionsintheenvironment. Specif-
Boisvertetal.,2023). ically, we ask LLMs to produce high-level textual plans
consistingofthefollowingprimitives: breakinganactive
ThegoaloftheBabaIsAIbenchmarkistoevaluatetherole
rule, making a rule active, or moving to a specific object
ofsystematiccompositionalityinrule-basedgeneralization.
intheenvironment(seeanexampleplaninFigure1). We
Thecorecomponentofthisbenchmarkisthatthewritten
instruct LLMs that these actions can only be taken if the
commands are not only grounded in an environment, but
relevantobjectsandruleblocksarepresentinthecurrent
thegroundingitselfcanbemanipulatedviachangingthe
environment. Togeneratetheirplan,LLMsreceiveasvi-
rulesoftheenvironment. Thisdynamicdesignallowsusto
sualinputastaticimageoftheinitialconfigurationofthe
exploreabroadernotionofgeneralizationcomparedtothe
environment.
currentbenchmarks.
After providing the game instructions, we present LLMs
Weshowresultsforthreelargelanguagemodels(LLMs):
with10exampleimagesandcorrespondingwinningplans
GPT-4o,Gemini-1.5-Pro(May2024),andGemini-1.5-Flash
forin-contextlearning(Brownetal.,2020). Foreachexam-
(May2024)(Team,2024). WechoseGPT-4oandGemini-
ple,LLMsareaskedtogeneratereasoningstepstoderivethe
1.5-Pro as these models occupy the top two spots on the
targetplanfromthegivenimage. Followingthein-context
Chatbot Arena Leaderboard (May 2024) (Chiang et al.,
examples,LLMsarepromptedtodescribeageneralalgo-
2024). We also include Gemini-1.5-Flash as this model
rithmtosolvetheenvironmentsandtoapplyittounseentest
occupiesanintriguingspotintheLLMecosystemwithboth
environments. Thetestenvironmentsarespecificallycho-
excellent performance and affordable price, making it an
sentoassessdifferenttypeofgeneralization. Wemeasure
attractiveoptionformanyapplications. Previousworkoften
accuracyastheexactmatchbetweenthefinalresponseof
convertvisualinputsintotextbeforeevaluatingLLMs(Yao
LLMsandthewinningplanofthetestenvironment. LLMs
et al., 2023; Carta et al., 2023; Momennejad et al., 2023;
areevaluatedon5samplesforeachtestenvironment.
Valmeekametal.,2023a).Hereweleveragethemulti-modal
2
ycaruccABabaIsAI:BreaktheRulestoBeattheBenchmark
Nodistractor(in-context) Objectdistractor(test)
Two-roomEnvironment
gemini-1.5-pro
100
gemini-1.5-flash
gpt-4o
75
50 Plan:goto[ball] Plan:goto[ball]
Ruledistractor(test) Object&Ruledistractor(test) Winruledistractor(test)
25
0
Nodistractor Object Rule Object&RuleWinrule
(in-context) distractor distractor distractor distractor
(test) (test) (test) (test)
Plan:goto[key] Plan:goto[ball] Plan:goto[door]
Figure4.Themeanaccuracyforallthreemodelsislowerwhenaskedtogeneralizetodistractorsinamorecomplexenvironment.
Thisenvironmentintroducesacentralverticalwall.However,therule[wallisstop]isinitiallyalwaysinactiveandsothewallhasno
practicalimpactonthemovementoftheagent.Thetaskistogototheobjectreferredtobytheactivewinrule(sameasinFigure3).
This entire process is repeated for 5 random seeds, each distractor.
correspondingtodifferentin-contextandtestexamples.
The sequence of environments used to test the LLMs in
Figure4includesthesamedistractorsasinFigure3,butnow
3.Results alltheenvironmentsincludeagrayverticalwallthatruns
downthecenteroftheenvironment. Theenvironmentsare
OurfirsttestsassesstheLLMs’abilitytoextractthemost
alwaysinitializedwiththerule[wallisstop]inactive,asthe
basicruleofthegamefromin-contextexamples,namely,
threeblocksthatformthisrulearenothorizontallyaligned,
gotothewinningobject,andthenapplythisruleinnovel
andsothewallhasnopracticalimpactonthemovement
environmentswheredistractorsarepresent. Complexenvi-
oftheagent. However,theseenvironmentsnowallcontain
ronmentscontainnotonlyrelevantstimulibutalsoirrelevant
theextradistractorblocksthatcomposetheinactivewall,
objectsorrules;identifyingtherelevantfromirrelevantisa
andblocksaboutthewallrule. Themeanaccuracyforall
crucialabilitythatweprobeinthissetofexperiments.
threemodelsislowerunderthisincreaseddistractorload
Figure3showstheaccuracyoftheLLMsinfivedifferent (compareFigures3and4).
environments: 1) Environments without a distractor, i.e.
Compositionalgeneralizationhasbeenstudiedinmanycon-
newrandomvariationsoftheenvironmentusedduringin-
texts(Lakeetal.,2017;Lake&Baroni,2017;Ruisetal.,
context learning. 2) Environments where there are now
2020),forexample,ifanagenthaslearnedtosolveatask
two objects but one of them is a distractor. In order to
withredcirclesandgreenkeysthenitshouldgeneralizeto
winthegame,theagentmustgototheobjectspecifiedin
redkeysandgreencircles. IntheBabaIsAIenvironment
thetextboxwiththewinrule,e.g. [dooriswin]requires
wecannotonlystudythesetraditionalformsofgeneraliza-
the agent to go to the door. 3) Environments contain a
tionbutprobemodelsunderscenarioswheretheveryrules
nounblockthatisdistractingfromtheactivewinrule. 4)
ofthegamemustbemanipulatedandcombined. Figure5
Environments contain both a distractor object and noun
shows one example scenario where the LLMs are shown
block. 5)Environmentscontainbothadistractorobjectand
environmentsthateachhighlightthreewinningstrategies
anounblockthatispartofanactiverule. Thedistractor
andthenareaskedtosolveanewsetofenvironmentsthat
rule is not relevant for the environment and so should be
require a novel composition of these previously learned
ignored.Forexample,therightmostpanelinFigure3shows
rules.
thedistractorrule[dooriswin]butthereisnodoorobject 
goto{object}
intheenvironmentandsothewinningstrategyistofollow 
In-context: make{rule},goto{object}
theotherrule[balliswin]andnavigatetotheball.
 break{rule},goto{object}
Impressively, GPT-4o performs with perfect accuracy on
thefirstfourenvironments,andasareminder,thisiswhile Test: break{rule},make{rule},goto{object}
receivingvisualandnottextualinputsaboutthegame. Sur-
The accuracy for all three LLMs is low. We have also
prisingly, Gemini-1.5-FlashoutperformsGemini-1.5-Pro,
alternatedthefourstrategiesshowninFigure5soadifferent
withallmodelsshowingthesametrenddownwardsinaccu-
threeareusedforin-contexttrainingandtheremainingis
racyonthefinaltaskthatincludesbothanobjectandarule
usedfortesting(notshown),andaccuracyremainslow.
3
ycaruccABabaIsAI:BreaktheRulestoBeattheBenchmark
goto make-goto
In-context:goto,make-goto,break-goto
Test:break-make-goto+distractors
100
gemini-1.5-pro
gemini-1.5-flash 75 gpt-4o Plan:goto[ball] Plan:make[dooriswin],goto[door]
50 break-goto break-make-goto
25
0
Nodistractor Object Rule Object&Rule
(test) distractor distractor distractor
(test) (test) (test)
Plan:break[wallisstop],goto[key] Plan:break[wallisstop],make[balliswin],goto[ball]
Figure5.LLMs generalize poorly under scenarios where the rules of the game must be manipulated and combined. LLMs
areshownenvironmentsthateachhighlightthreewinningstrategies: goto{object};make{rule}thengoto{object};break{rule}then
goto{object}.Thentheyareaskedtosolveanewsetofenvironmentsthatrequireanovelcompositionofthesepreviouslylearnedrules:
break{rule}thenmake{rule}thengoto{object}.
Plan: Plan: Plan:
break[wallisstop],make[dooriswin],goto[door] break[wallisstop],make[walliswin],goto[wall] make[keyisyou],make[dooriswin],goto[door]
Figure6.Despitesuperficialsimilaritiesandidenticalobjects,eachenvironmentrequiresdistinctwinningsolutions,illustrating
furtherchallengesinrulemanipulationandcompositionalgeneralization.
Theseaspectsofcompositionalgeneralizationacrossrules environment. Figure6illustratessomeofthefurtherchal-
areparticularlyuniquetotheBabaIsAIbenchmark,and lengesintheseenvironments. Allthreeenvironmentsare
thepoorperformanceindicatesthatthisbenchmarkcreates superficiallysimilarandcontainthesameobjects,yetthe
meaningfulgeneralizationchallengesforLLMs. winningsolutionsaredifferentineachcase(seetextatthe
top of the figures). For example, the center environment
Model Accuracy(mean±std) requirestheagenttobreakthe[wallisstop]rule,thenmove
gemini-1.5-flash 20.0±29.28 the[wall]blocktocreatetherule[walliswin],andfinally
gemini-1.5-pro 14.67±20.66 gotooneofthewallblockstowinthegame. Asasecond
gpt-4o 17.33±28.15
example,intheenvironmentshownintherightmostpanel
ofFigure6therule[wallisstop]islocatedinthecornerof
Table1.ModelaccuraciesfortheenvironmentsshowninFigure6. theenvironmentandsothereisnowaytopushtheseblocks
outofalignmentandbreakthisrule; theagentisinitially
4.Discussion
trappedintheleftmostroomoftheenvironment. Theagent
Inorderforagentstohavehuman-likeinteractionswiththe mustbreakthecurrentlyactiverule[babaisyou]andcreate
world,theyshouldnotonlybeabletointeractwithobjects [keyisyou]inordertocontrolthekeyontheothersideof
but also have the capacity to understand and manipulate thewall. Thentheagentcanusethekeytocreatetherule
therulesoftheirenvironment. Bydefiningastaticsetof [dooriswin]andmovetothedoor. Theaccuracyonthese
rules that an agent must follow, many games and bench- challengingenvironmentsislowasshowninTable1.
markshaveoverlookedacriticalcapability: theabilityto
TheerrorsthatLLMsmakeinsolvingtheBabaIsAIen-
understandrulesviarulemanipulation. Therefore,theBaba
vironments are instructive about future opportunities for
IsAIbenchmarkexplorescompositionalgeneralizationun-
improvements(seeAppendixB).LLMsmakegrounding
derconditionsinwhichagentscanmodifytherulesofthe
4
ycaruccABabaIsAI:BreaktheRulestoBeattheBenchmark
mistakes: theLLMreferstoanobjectthatdoesnotexistin
theenvironment. LLMsmakepathplanningmistakes: the
LLMincorrectlyassertsthatthepathtoaspecificobjectis
blockedbyanotherobject,despitethepathbeingclearin
theenvironment.
5BabaIsAI:BreaktheRulestoBeattheBenchmark
References Lake,B.M.,Ullman,T.D.,Tenenbaum,J.B.,andGersh-
man, S.J. Buildingmachinesthatlearnandthinklike
Brown,T.,Mann,B.,Ryder,N.,Subbiah,M.,Kaplan,J.D.,
people. BehavioralandBrainSciences,40:e253,2017.
Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,
doi: 10.1017/S0140525X16001837.
Askell,A.,Agarwal,S.,Herbert-Voss,A.,Krueger,G.,
Henighan,T.,Child,R.,Ramesh,A.,Ziegler,D.,Wu,J., Momennejad,I.,Hasanbeig,H.,VieiraFrujeri,F.,Sharma,
Winter,C.,Hesse,C.,Chen,M.,Sigler,E.,Litwin,M., H.,Jojic,N.,Palangi,H.,Ness,R.,andLarson,J.Evaluat-
Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, ingcognitivemapsandplanninginlargelanguagemodels
S.,Radford,A.,Sutskever,I.,andAmodei,D. Language with cogeval. In Oh, A., Naumann, T., Globerson, A.,
modelsarefew-shotlearners. InLarochelle,H.,Ranzato, Saenko,K.,Hardt,M.,andLevine,S.(eds.),Advancesin
M.,Hadsell,R.,Balcan,M.,andLin,H.(eds.),Advances NeuralInformationProcessingSystems,volume36,pp.
in Neural Information Processing Systems, volume 33, 69736–69751.CurranAssociates,Inc.,2023.
pp.1877–1901.CurranAssociates,Inc.,2020.
Park, J. S., O’Brien, J., Cai, C. J., Morris, M. R., Liang,
Carta, T., Romac, C., Wolf, T., Lamprier, S., Sigaud, O., P.,andBernstein,M.S. Generativeagents: Interactive
andOudeyer,P.-Y. Groundinglargelanguagemodelsin simulacraofhumanbehavior. InProceedingsofthe36th
interactiveenvironmentswithonlinereinforcementlearn- AnnualACMSymposiumonUserInterfaceSoftwareand
ing. InKrause,A.,Brunskill,E.,Cho,K.,Engelhardt,B., Technology.AssociationforComputingMachinery,2023.
Sabato,S.,andScarlett,J.(eds.),Proceedingsofthe40th
Ruis, L., Andreas, J., Baroni, M., Bouchacourt, D., and
InternationalConferenceonMachineLearning,volume
Lake,B.M. Abenchmarkforsystematicgeneralization
202ofProceedingsofMachineLearningResearch,pp.
ingroundedlanguageunderstanding. InProceedingsof
3676–3713.PMLR,23–29Jul2023.
the34thInternationalConferenceonNeuralInformation
Chevalier-Boisvert,M.,Dai,B.,Towers,M.,deLazcano, ProcessingSystems,2020.
R., Willems, L., Lahlou, S., Pal, S., Castro, P. S., and
Song,C.,Sadler,B.M.,Wu,J.,Chao,W.,Washington,C.,
Terry,J.Minigrid&miniworld:Modular&customizable
andSu,Y. Llm-planner: Few-shotgroundedplanningfor
reinforcement learning environments for goal-oriented
embodiedagentswithlargelanguagemodels. In2023
tasks. CoRR,abs/2306.13831,2023.
IEEE/CVFInternationalConferenceonComputerVision
(ICCV).IEEEComputerSociety,2023.
Chiang,W.-L.,Zheng,L.,Sheng,Y.,Angelopoulos,A.N.,
Li, T., Li, D., Zhang, H., Zhu, B., Jordan, M., Gonza-
Team,G.Gemini1.5:Unlockingmultimodalunderstanding
lez, J. E., and Stoica, I. Chatbot arena: An open plat-
acrossmillionsoftokensofcontext,2024.
form for evaluating llms by human preference. ArXiv,
abs/2403.04132,2024. Valmeekam, K., Marquez, M., Olmo, A., Sreedharan, S.,
andKambhampati,S. Planbench: Anextensiblebench-
Huang,W.,Abbeel,P.,Pathak,D.,andMordatch,I. Lan-
markforevaluatinglargelanguagemodelsonplanning
guagemodelsaszero-shotplanners: Extractingaction- andreasoningaboutchange. InThirty-seventhConfer-
ableknowledgeforembodiedagents,2022. enceonNeuralInformationProcessingSystemsDatasets
andBenchmarksTrack,2023a.
Ichter, B., Brohan, A., Chebotar, Y., Finn, C., Hausman,
K., Herzog, A., Ho, D., Ibarz, J., Irpan, A., Jang, E., Valmeekam,K.,Marquez,M.,Sreedharan,S.,andKamb-
Julian, R., Kalashnikov, D., Levine, S., Lu, Y., Parada, hampati,S. Ontheplanningabilitiesoflargelanguage
C., Rao, K., Sermanet, P., Toshev, A. T., Vanhoucke, models-acriticalinvestigation.InThirty-seventhConfer-
V., Xia, F., Xiao, T., Xu, P., Yan, M., Brown, N., Ahn, enceonNeuralInformationProcessingSystems,2023b.
M., Cortes, O., Sievers, N., Tan, C., Xu, S., Reyes, D.,
Rettinghouse,J.,Quiambao,J.,Pastor,P.,Luu,L.,Lee, Wang,G.,Xie,Y.,Jiang,Y.,Mandlekar,A.,Xiao,C.,Zhu,
K.-H.,Kuang,Y.,Jesmonth,S.,Jeffrey,K.,Ruano,R.J., Y.,Fan,L.,andAnandkumar,A.Voyager:Anopen-ended
Hsu,J.,Gopalakrishnan,K.,David,B.,Zeng,A.,andFu, embodied agent with large language models. Transac-
C.K. Doasican,notasisay: Groundinglanguagein tionsonMachineLearningResearch,2024. ISSN2835-
roboticaffordances. In6thAnnualConferenceonRobot 8856.
Learning,2022.
Yao,S.,Zhao,J.,Yu,D.,Du,N.,Shafran,I.,Narasimhan,
K.,andCao,Y. React: Synergizingreasoningandacting
Lake, B. M. and Baroni, M. Generalization without sys-
inlanguagemodels,2023.
tematicity: Onthecompositionalskillsofsequence-to-
sequencerecurrentnetworks.InInternationalConference
onMachineLearning,2017.
6BabaIsAI:BreaktheRulestoBeattheBenchmark
A.Prompt
Youwillbeplayingagridworldgame.Inthisgameyoucanchangetherulesofthegamebymovingblocksaround.Blocksin
thegridcanbeobjectsorrules.
Aruleisactiveiftheruleblocksarealignedhorizontallyandtheyformavalidrule.Avalidrulehastheform"<object>is
<prop>",where<object>istheobjecttowhichtheruleapplies,and<prop>isthepropertythattheobjectgains.Activatedrules
modifytherulesoftheobjectsinthegrid.
-Objects:thepossibleobjectsare"baba"(whitetriangle),"wall","key","door","ball".Objectsarerecognizablebytheirshape
andcolor.
-Ruleblocks:theruleblocksare"is","win","you","stop",andruleblockswiththenamesoftheobjects.Ruleblocksare
representedbywhiteboxeswithwhitetextinside.
Ruleproperties:
-"is":theruleblockthatconnectstheobjectandtheproperty.
-"win":theruleblockthatindicatestheobjectthattheagentneedstoreachtowinthegame.
-"you":theruleblockthatindicatestheobjectthattheagentcontrols.
-"stop":theruleblockthatindicatestheobjectthatblockstheagentfrommovingthroughit.
Bydefaultobjectsarenotblocking,butiftherule"objectisstop"isactive,theobjectwillblocktheagentfrommovingthroughit.
Ruleblocksarealwayspushablebytheagents.
Thegameisalwayssolvableandthereisalwaysauniquewinningplantosolvethegame.
Awinningplaniscomposedofoneormoreactions(commaseparated).Anactioncanbeoneofthefollowing:
-goto[<obj>]:movetheagenttowardstheobject<obj>.
-make[<obj>is<prop>]:createarulethatmakestheobject<obj>havetheproperty<prop>.Thisactioncanonlybetakenif
boththeobjectruleblockandthepropertyruleblockareinthegridandtheruleisnotalreadyactive.
-break[<obj>is<prop>]:removetherulethatmakestheobject<obj>havetheproperty<prop>.Thisactioncanonlybetakenif
theruleisactiveintheenvironment(i.e.theruleblocks<obj>,is,<prop>arehorizontallyaligned).
Nx
Example:
<image>
Winningplan:goto[door]
Explainwhatarethereasoningstepstofindthewinningplanbasedonthegivengridworld.Beasconciseaspossible
andavoidunnecessarydetails.
Model:[...]
Basedontheexamplesandthereasoningstepsyouhavegiven,describeanalgorithmtofindthewinningplanforagiven
gridworld.It'sreallyimportantthatanotherpersoncanunderstandyouralgorithmandapplyittofindthewinningplan.Beas
conciseaspossibleandavoidunnecessarydetails.
Model:[...]
Applythealgorithmstepsbystepstothefollowinggridworld.
<image>
Model:[...]
Giveyourfinalanswerusingexactlythisformat:'Winningplan:<yourplan>'.
Model:[...]
Figure7.Prompttemplate.ThefirstpartofthepromptincludesgeneralinstructionsdescribingthegameandthetaskfortheLLMs.
Afterprovidingthegameinstructions,wepresentLLMswithN=10examplegridimagesandcorrespondingwinningplans,andask
LLMstogeneratereasoningstepstoderivetheplanfromthegridimage. WethenaskLLMstodescribeanalgorithmtosolvethe
environmentsandtoapplytonewenvironmentsthatsystematicallydifferfromthein-contextenvironments.
7BabaIsAI:BreaktheRulestoBeattheBenchmark
B.Errorcases
Prompt:
[...]
Applythealgorithmstepsbystepstothefollowinggridworld.
Model:
Let'sapplythealgorithmstepbysteptothegivengridworld.
###Step1:IdentifytheAgent
-Scanthegridfortherule"babaisyou".
-Therule"babaisyou"isactive,indicatingthatthewhitetriangle(baba)istheagent
controlledbytheplayer.
-Positionofbaba:(5,4).
###Step2:IdentifytheWinningCondition
-Scanthegridforanyactiveruleoftheform"<object>iswin".
-Therearetwoactiverules:"dooriswin"and"balliswin".
-Positions:
-Door(redsquare):(0,0).
-Ball(bluecircle):(3,2).
###Step3:PlantheMovement
-Checkifthereareanyobstaclesbetweentheagentandthewinningobjects.
-Thepathtothedoor(redsquare)isblockedbythekey(bluekey)at(1,1).
-Thepathtotheball(bluecircle)isclear.
###Step4:ExecutethePlan
-Sincethepathtotheballisclear,planadirectpathtotheball.
###GeneratethePlan
-Theactiontoachievethewinningconditionistomovetheagenttowardstheball.
###OutputthePlan
-Winningplan:`goto[ball]`.
Correctplan:goto[door]
Figure8.TwocommontypesofmistakesobservedinthereasoningofLLMs,illustratedhereforGPT-4owhentestedonthesingle-
roomenvironmentwithanadditionalwinruledistractor.(i)Groundingmistake:theLLMreferstoanobjectthatdoesnotexistinthe
environment.Inthisexample,theLLMmentionsaball(Step3,thirdbulletpointinthemodel’sanswer),specifyingthatisitabluecircle
inparenthesis,eventhoughnosuchballispresent. (ii)Pathplanningmistake: theLLMincorrectlyassertsthatthepathtoaspecific
objectisblockedbyanotherobject,despitethepathbeingclearintheenvironment.Inthisinstance,theLLMclaimsthatthepathtothe
doorisblockedbythekey,eventhoughitisnotthecase.
8