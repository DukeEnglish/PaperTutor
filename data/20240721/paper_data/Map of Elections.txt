FIELD OF SCIENCE: Natural sciences
SCIENTIFIC DISCIPLINE: Computer and information sciences
DOCTORAL THESIS
Map of Elections
Author: Stanisław Andrzej Szufa
Supervisor: prof. dr hab. inz˙. Piotr Faliszewski
Completed in: AGH University, Faculty of Computer Science
Krakow, 2024
4202
luJ
61
]AM.sc[
1v98811.7042:viXraStreszczenie
W niniejszej rozprawie skupiam sie˛ na badaniu zagadnien´ zwia˛zanych z obli-
czeniowa˛teoria˛wyboru społecznego (ang. computational social choice). Dyscy-
plina ta koncentruje sie˛ na analizie zbiorowego podejmowania decyzji – w szcze-
gólnos´cinajejobliczeniowychaspektach.Zezbiorowympodejmowaniemdecyzji
mamydoczynieniamie˛dzyinnymiwkonteks´ciewyborów.Przykładowomoga˛to
byc´ wyboryprezydenckieba˛dz´ wyboryparlamentarne.Wyboramimoz˙narówniez˙
nazwac´ proces wyłaniania zwycie˛zcy w konkursie Chopinowskim. Formalnie,
przez wybory rozumiemy zbiór kandydatów oraz zbiór wyborców posiadaja˛cych
pewne preferencje wzgle˛dem tychz˙e kandydatów. Teoria wyborów mierzy sie˛ z
szeregiem problemów, takich jak stwierdzanie kto wygrywa dane wybory, oce-
nianie marginesów zwycie˛stwa, analiza róz˙nego rodzaju manipulacji wynikiem,
badaniewłasnos´ciaksjomatycznych(np.kryteriumCondorceta)iwieleinnych.
W swoich badaniach przede wszystkim skupiam sie˛ na analizie róz˙nych sta-
tystycznych modeli preferencji, czyli modeli pozwalaja˛cych generowac´ wybory
(zestawy głosów). Analizowane przeze mnie modele preferencji sa˛ powszech-
niewykorzystywaneprzezspołecznos´c´ zajmuja˛ca˛sie˛ algorytmicznymiaspektami
wyborów.Ichlepszezrozumieniepozwoliwprzyszłos´cinatrafniejszedobieranie
modeli zalez˙nie od sytuacji (przykładowo do symulacji obliczeniowych) oraz na
bardziejracjonalneplanowanieeksperymentówobliczeniowych.
Kaz˙de wybory, zarówno te prawdziwe, jak i te wygenerowane przez modele
statystyczne, moz˙emy utoz˙samic´ z punktem w pewnej wielowymiarowej prze-
strzeni.Pojawiasie˛ pytanie,jakporównywac´ zesoba˛róz˙newybory?Wszczegól-
nos´cijakmierzyc´ odległos´cipomie˛dzynimi?
Próbuja˛c odpowiedziec´ na powyz˙sze pytania wprowadzam narze˛dzie nazy-
wane mapa˛ wyborów – graficzna˛ reprezentacje˛ ułatwiaja˛ca˛ zrozumienie prze-
strzeni wyborów. Na pocza˛tku przygotowujemy zestaw wyborów. Naste˛pnie,
zgodnie z zadana˛metryka˛, obliczamy odległos´ci pomie˛dzy kaz˙da˛para˛wyborów.
Na koniec, bazuja˛c na obliczonych odległos´ciach, osadzamy wszystkie wybory
(punkty) w dwuwymiarowej przestrzeni euklidesowej, tak aby odległos´ci eukli-
desowe, jak najlepiej odzwierciedlały, te obliczone przy pomocy metryki. Mapa
wyborów – to nie pojedyncza mapa, a narze˛dzie pozwalaja˛ce tworzyc´ róz˙ne wa-
riantymapydlaróz˙nychmodeliiparametrów.Mapawyborówpozwalalepiejzro-
zumiec´ zarówno istnieja˛ce modele, jak i prawdziwe wybory. Dzie˛ki mapie udało
sie˛ dokonac´ wieluistotnychspostrzez˙en´.Abstract
In the following thesis, we study the topics related to computational social
choice theory. This discipline focuses on the analysis of collective decision-
making, in particular, on its computational aspects. We deal with collective
decision-making, for example, in the context of elections. For instance, it can
be a presidential or a parliamentary election. By an election we can also call the
process of selecting the winner in the Chopin Competition. Formally, by an elec-
tion we mean a set of candidates and a set of voters who have certain preferences
over these candidates. Election theory faces a number of problems, such as de-
termining the winner or winners in an election, calculating margins of victory,
analyzing various types of manipulation, studying axiomatic properties (e.g., the
Condorcetwinnercriterion),andmanyothers.
In our research, we mainly focus on the analysis of various statistical models
of preferences, i.e., models for generating elections (votes). The preference mod-
elswehaveanalyzedarewidelyusedbythecommunitydealingwiththealgorith-
mic aspects of elections. Their better understanding will allow for more accurate
selection of models depending on the situation (for example, for computational
simulations)andformorerationalplanningofcomputationalexperiments.
Each election, both real and generated from statistical cultures, can be as-
sociated with a point in a multidimensional space. The question arises, how to
compare different elections with each other? In particular, how do we measure
thedistancesbetweenthem?
In an attempt to answer these questions, we introduce a framework called a
mapofelections,agraphicalrepresentationthatmakesiteasiertounderstandthe
election space. First, we prepare a set of elections. Then, according to a given
metric, we calculate the distances between each pair of elections. Finally, based
on the calculated distances, we embed all elections (points) in a two-dimensional
EuclideanspacesothattheEuclideandistancesreflectascloselyaspossiblethose
computed with the metric. The map of elections is not a single map, but a frame-
work that allows us to create different map variants for different models and pa-
rameters. Moreover, the map of elections allows us to better understand both ex-
istingstatisticalculturesandreal-lifeelections. Thankstothemap,itwaspossible
tomakemanyintriguingobservations.Contents
1 Introduction 4
1.1 MapofElections . . . . . . . . . . . . . . . . . . . . . . . . . . 5
1.2 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
1.3 Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
1.4 ConferencePublications . . . . . . . . . . . . . . . . . . . . . . 12
2 Preliminaries 15
2.1 Elections . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
2.2 Distances . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
2.3 Correlation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
2.4 IntegerLinearProgramming . . . . . . . . . . . . . . . . . . . . 18
2.5 Embeddings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
2.6 MapofObjects . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
3 StatisticalCultures 22
3.1 GeneralModels . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
3.2 StructuredDomains . . . . . . . . . . . . . . . . . . . . . . . . . 24
3.2.1 Single-PeakedElections . . . . . . . . . . . . . . . . . . 24
3.2.2 Single-CrossingElections . . . . . . . . . . . . . . . . . 25
3.2.3 Group-SeparableElections . . . . . . . . . . . . . . . . . 27
3.2.4 EuclideanElections . . . . . . . . . . . . . . . . . . . . . 28
3.3 CompassElections . . . . . . . . . . . . . . . . . . . . . . . . . 29
3.4 MapofPreferences . . . . . . . . . . . . . . . . . . . . . . . . . 30
3.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
4 DistancesAmongElections 35
4.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
4.2 ElectionIsomorphism . . . . . . . . . . . . . . . . . . . . . . . . 36
14.3 IsomorphicDistances . . . . . . . . . . . . . . . . . . . . . . . . 38
4.3.1 ComputationalComplexity . . . . . . . . . . . . . . . . . 39
4.3.2 ILP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42
4.3.3 VisualizationoftheDistances . . . . . . . . . . . . . . . 44
4.3.4 IsomorphicMapsofElections . . . . . . . . . . . . . . . 49
4.4 NonisomorphicDistances . . . . . . . . . . . . . . . . . . . . . . 53
4.4.1 AggregateRepresentations . . . . . . . . . . . . . . . . . 53
4.4.2 PositionwiseDistance . . . . . . . . . . . . . . . . . . . 55
4.4.3 PairwiseDistance . . . . . . . . . . . . . . . . . . . . . . 57
4.4.4 BordawiseDistance . . . . . . . . . . . . . . . . . . . . 59
4.4.5 MapsofElectionsUsingNonisomorphicDistances . . . . 62
4.5 Comparison . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63
4.5.1 AnalysisoftheCompass . . . . . . . . . . . . . . . . . . 65
4.5.2 EquivalenceClasses . . . . . . . . . . . . . . . . . . . . 73
4.5.3 Correlation . . . . . . . . . . . . . . . . . . . . . . . . . 74
4.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76
5 Applications 79
5.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79
5.2 Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80
5.3 Embedding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83
5.3.1 Monotonicity . . . . . . . . . . . . . . . . . . . . . . . . 86
5.3.2 Distortion . . . . . . . . . . . . . . . . . . . . . . . . . . 88
5.3.3 Scalability . . . . . . . . . . . . . . . . . . . . . . . . . 90
5.4 VotingRules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94
5.4.1 Score . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94
5.4.2 RunningTime . . . . . . . . . . . . . . . . . . . . . . . 100
5.4.3 Approximation . . . . . . . . . . . . . . . . . . . . . . . 104
5.5 Real-LifeInstances . . . . . . . . . . . . . . . . . . . . . . . . . 109
5.5.1 Real-LifeElectionsontheMap . . . . . . . . . . . . . . 112
5.5.2 CapturingReal-LifeElections . . . . . . . . . . . . . . . 112
5.6 SkeletonMap . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114
5.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117
6 Subelections 119
6.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119
6.2 VariantsoftheIsomorphismProblem . . . . . . . . . . . . . . . 121
6.3 ComputationalComplexityAnalysis . . . . . . . . . . . . . . . . 124
26.3.1 IntractabilityofSubelectionIsomorphism . . . . . . . . . 125
6.3.2 IntractabilityofMax. CommonSubelection . . . . . . . . 131
6.4 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135
6.4.1 ResultsandAnalysis . . . . . . . . . . . . . . . . . . . . 137
6.4.2 Real-lifeSubelections . . . . . . . . . . . . . . . . . . . 139
6.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144
7 ApprovalElections 145
7.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145
7.2 Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148
7.3 StatisticalCulturesforApprovalElections . . . . . . . . . . . . . 149
7.4 MapsofApprovalPreferences . . . . . . . . . . . . . . . . . . . 152
7.5 Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 156
7.6 AGridofApprovalElections . . . . . . . . . . . . . . . . . . . . 157
7.7 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159
7.7.1 ExperimentalDesign . . . . . . . . . . . . . . . . . . . . 159
7.7.2 ExperimentalResults . . . . . . . . . . . . . . . . . . . . 161
7.7.3 Correlation . . . . . . . . . . . . . . . . . . . . . . . . . 165
7.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167
8 Discussion&FutureWork 169
A DistancesBetweentheCompassElections 171
B MapsofApprovalCandidates 179
3Chapter 1
Introduction
Whentalkingaboutelections,mostpeoplefocusontheelection’soutcome,or,in
other words, on the winners. However, a raw election without a voting rule and
without a winner is a very interesting object in itself. It is surprising how much
we can say about an election without electing anyone at all. For example, if in an
election each voter votes differently, we would say that such an election is very
diverse. On the other hand, if in an election all voters vote in exactly the same
way (i.e., all votes are identical), then we would say that the voters are in perfect
agreement.
Usuallybyanelectionwewillrefertoanobjectthatconsistsofasetofcandi-
dates and a collection of voters thathave some preferences over these candidates.
Inmostcases,weassumethateachvoterstrictlyranksallthecandidates,fromthe
mosttotheleastappealingone.
The most popular examples of elections are political elections, such as pres-
idential and parliamentary ones. But, in fact, we do not only deal with elections
in politics. Many surveys or sport competitions can be seen as elections as well.
However, perhaps in a less intuitive way. To clarify how one can treat a sport
competition as an election, let us give an example. Given Formula 1 races from
a certain year, we can treat each single race as a single vote, where the position
of each candidate in the ballot is the place he or she won in the race. The driver
whofinishestheracefirstwillberankedinthefirstplaceintheballot,thesecond
driver will be ranked second, and so on. The total number of votes will be equal
to the number of races that took place in a given year. We can understand various
othercompetitions,suchasTourdeFranceorGirod’Italiaaselectionsinasimilar
manner.
41.1 Map of Elections
One of the questions that arises naturally is when are two elections similar? Or
stated the other way around, when are two elections different? And if they are
different, we might want to know how different they are. For instance, are pres-
idential elections in Poland and France very different? Or maybe the structure
of the elections is similar and only the names on the ballots differ? Let us have a
lookatthefollowingtoyexample. Wehavetwoelections,eachconsistingofthree
voters and three candidates. In the first election, children give their preferences
aboutanimals,andinthesecondone,adultsgivetheirpreferencesaboutfood:
Veronica: Pig ≻ Snail ≻ Rabbit,
John: Snail ≻ Pig ≻ Rabbit,
Nicholas: Rabbit ≻ Snail ≻ Pig.
Newman: Risotto ≻ Salad ≻ Pizza,
Veil: Pizza ≻ Salad ≻ Risotto,
Johnson: Salad ≻ Pizza ≻ Risotto.
Are these two elections similar? At first glance, probably not that much. But
ifweforgetaboutthenamesofthevotersandthenamesofthealternatives,these
two elections become identical. To see this, let us assume that Veronica is Veil,
John is Johnson, Nicholas is Newman, Pig is Pizza, Snail is Salad and Rabbit is
Risotto. Mathematically speaking, we simply have three preference orders, each
ofthemappearingexactlyonce. Theorderofvotesisirrelevant:
(p ≻ s ≻ r, s ≻ p ≻ r, r ≻ s ≻ p)
To speakmore generally, given two elections, the firstproblem which wewill
faceisverifyingwhethertheseelectionsareisomorphic,thatis,ifitispossibleto
rename the candidates and the voters in such a way that these elections become
identical. Verifying whether two elections are isomorphic can be done in poly-
nomial time. However, if two elections are not isomorphic, the second problem
arises,thatis,howtodefineandcomputethedistancebetweenthem.
Anefficientwayofcomputingdistancesbetweenelectionsisimportant. How-
ever, even if we knew that the distance between two elections is equal to five, we
stillwouldnotknowmuchabouttheseelections. Isfivealotornot?
5To solve this problem, we introduce another crucial component of this dis-
sertation, the concept called the map of elections, to which this work owes its
title. The idea is as follows. First, we generate numerous elections from vari-
ous statistical cultures (that is, models that serve for generating random instances
of elections). Second, we compute the distances between each pair of elections.
Third, we embed these distances in a two-dimensional Euclidean space using an
embedding algorithm. Finally, we obtain a map. Map-representation of elections
makesiteasiertounderstandtheirnumerousproperties.
To make it even easier, we mark four characteristic points on our map. First,
we have an identity election, where all voters agree on a single preference order.
Then, we have a uniformity election, where the votes are as diverse as possible.
Finally, we have stratification and antagonism elections, the description of which
we will omit in the introduction for simplicity (all four points will be described
in detail in Chapter 3). We call these points the compass because they help us
navigate through the map; so when a given point (an election) lands in a certain
part of the map, we can say something meaningful about this election. Identity
and uniformity are the two most extreme points, representing order versus chaos,
respectively. Inallofourmetrics,thedistancebetweenidentityanduniformityis
the largest possible in the whole space of elections. For example, if the distance
between two particular elections is five, but the distance between identity and
uniformity is six, then these two elections are far away. However, if the distances
betweenidentity anduniformity werefifty, then wecan arguethat theseelections
arequitesimilar.
Togivethereadertheflavorofwhatthisthesisisabout,wepresentanexample
ofamapofelectionsinFigure1.1. Eachdotcorrespondstoasingleelection. The
closer two particular dots are on the map, the more similar are elections that they
represent, and if two dots are of the same color, it means that they come from the
same distribution, i.e., statistical culture. How to generate elections from a given
model will be described in detail in Chapter 3. Nonetheless, without going into
the technical details of particular models, we can see that for most of the models,
the elections generated from that model are very similar to each other. However,
it is not entirely true for, for example, blue points, which represent the Mallows
model—apopularmodelwhichwewillnowbrieflydescribe. TheMallowsmodel
is parametrized by a dispersion parameter, which defines the correlation between
the votes within an election. The larger is the parameter, the less correlated are
the votes. If this parameter is equal to zero, we have an extreme correlation and
allvotesareidentical. Whenthisparameterisequaltoone,wewitnessfullchaos
and no correlation at all. Going back to our map in Figure 1.1, depending on
6Figure1.1: AnexampleofMapofElections.
the dispersion parameter, elections generated from the Mallows model (the blue
points)canoccupyquitedifferentplaces. IfwesampleelectionsfromtheMallows
modelwithnumerousdifferentvaluesofthedispersionparameter,weobtainwhat
we call a path from one of the extreme points, identity, to another extreme point,
uniformity. Toconclude,dependingonthedispersionparameter,wecangenerate
drastically different elections. Nevertheless, for a fixed parameter, all generated
electionswillbesimilartoeachother.
Our analyzes of distances between elections started a new line of research
within computational social choice, resulting in numerous papers and, hopefully,
many more to come. In this dissertation, we focus on ordinal elections, but the
map of elections framework can be easily generalized to map of instances, which
canbeusedformanyothertypesofobjectsthatarestudiedwithincomputational
socialchoice,suchasapprovalelections(whichwediscussindetailinChapter7),
stable roommates instances, stable marriages instances, participatory budgeting
instances,orfairdivisionones.
71.2 Motivation
Althoughmanypapersoncomputationalsocialchoicearetheoretical,thenumber
of experimental works is rapidly growing. And there are many questions that can
only be answered by experimentation. We start by giving an example related to
theCondorcetwinner. WecallacandidateaCondorcetwinnerifsuchacandidate
is preferred by more than half of the voters when compared one-to-one with any
other candidate. In some elections none of the candidates is a Condorcet win-
ner; however, if such a candidate exists, many people claim that he or she should
become an overall winner of the election. We say that a voting rule satisfies the
Condorcet winner criterion if whenever a Condorcet winner exists, this rule se-
lects him or her as the winner. From a theoretical point of view, we can divide
rulesintotwogroups,thosethatsatisfytheCondorcetwinnercriterion,andthose
that do not. Unfortunately, the real world is not black and white. It might be the
case that some of the rules that do not satisfy the Condorcet winner criterion, but
do not satisfy it due to very few unrealistic instances, on which they fail to select
the Condorcet winner. This moves us to the second problem—what does it mean
that an instance is unrealistic? It is hard to answer this question in general. But
if we speak about particular types of elections, we can try to give an answer. For
example, in the context of political elections, we usually have many more voters
thancandidates,soaninstancewith1000candidatesand10votersprobablyisnot
veryrealistic. Anotherwayofverifyingwhetheragiveninstanceisrealisticisby
comparingitwithreal-lifedatafromagivencontext,e.g.,political. Goingbackto
ourCondorcetwinnercriterion,insteadoftwogroups,weratherhaveaspectrum
of rules. And to distinguish between rules that almost always select a Condorcet
winner (if such a candidate exists) and those that fail it more frequently, we need
experiments.
Another thing that we can only partially describe with raw theory is the time
needed to perform particular tasks. For example, the time needed to compute
a winner or a winning committee under a certain voting rule. In Figure 1.2 we
present an introductory example of a map of elections where each point’s color
depicts the time needed to compute the winning committee under the Harmonic-
Borda multiwinner voting rule. As we can see, the longest time is needed for
instancessimilartothosefromimpartialculture,whiletheshortesttimeisneeded
forthosesimilartoidentity. Formostoftherules,weknowtheirtime-complexity,
however, usually it relates only to the worst case. So again, it might be the case
that the rule in practice is fast, but due to some unfortunate instances, the worst-
case complexity is far from polynomial. It is also interesting to know whether, if
8Figure1.2: ILPsruntime(inseconds)forHarmonic-Bordavotingrule.
twoinstancesofelectionsaresimilar,ittakesthesameamountoftimetocompute
thewinnersoftheseelectionsundertheconsideredvotingrule.
Thenext potentialbenefitfrom thisthesisis ageneral improvementonexper-
iments done across the computational social choice. In numerous experiments,
people use different models with different parameters that seem to be selected
quite arbitrarily. A better understanding of statistical cultures and the nature of
elections is crucial for conducting better experiments. We believe that the map
of elections framework, proposed by us, can help in choosing synthetically gen-
erated elections to use in experiments when evaluating a given voting rule or a
socialchoicephenomenon.
There are many statistical cultures, for example, the Mallows model, the urn
model, or, the impartial culture and there are many questions worth asking here.
First, it would be valuable to know how different from each other the elections
generated from a given model are. Next, how different are the statistical models
from each other. For parameterized models, it would also be important to know
how theirparameters influencethem, and whichranges ofparameters correspond
torealisticinstances.
9Another motivation regards real-life elections. So, one way of getting data is
bygeneratingitaccordingtoacertainstatisticalmodel. Thisgivesusflexibilityin
selectingarbitrarilythenumberofcandidatesandthenumberofvoters. However,
itisalsoveryinterestingtoanalyzereal-lifedata. Howdoreal-lifeelectionsrelate
tosyntheticdata? Arereal-lifeelectionssimilartoeachother?
Finally, our analysis will help us better understand the space of elections in
itself and will tell us how different two elections can be. In the following, we
brieflydescribethestructureofthedissertation.
1.3 Structure
We start by describing statistical cultures for sampling ordinal elections and pro-
vide some insight into the inner structure of such elections. Next, we focus on
numerous distances between elections. First, we introduce isomorphic distances
(that is, distances under which only isomorphic elections are at distances zero).
Then, we move on to nonisomorphic distances. Later, we evaluate the map of
elections framework, and study its potential applications. After that, we discuss
the distances between elections of different sizes. Finally, we focus on elections
with approval ballots and present maps of approval elections. Below we briefly
describethecontentofeachchapteronebyone.
Preliminaries. Introductionofbasicdefinitionsandnotation.
StatisticalCultures. Weprovideadescriptionofstatisticalculturesknowninthe
literature,andhowtosampleelectionsfromthesecultures. Next,wepresent
maps of preferences, where we look at relations between votes within a
singleelection. Informallyspeaking,itisamicroscopeviewofanelection,
givingusinsightintothestructureofthevotes.
Distances. In the first part of this chapter, we define the ELECTION ISOMOR-
PHISM problem and introduce three isomorphic distance. We say that a
distance is isomorphic if, for any two elections that are not isomorphic, the
distancebetweenthemislargerthanzero. ThesethreemetricsaretheSwap
distance, the Spearman distance, and the discrete distance. Unfortunately,
only the discrete distance can be computed in polynomial time. As for the
Swap and Spearman distances, the complexity mostly comes from the fact
that we have to find optimal matchings of voters and of candidates at the
10same time. What is surprising is that, for Swap distance, even if the voters’
matchingisgiven,theproblemremainsNP-hard.
In the second part, we introduce the nonisomorphic positionwise distance,
which can be computed in polynomial time. We argue that this particular
distance is very practical. Although it is a pseudometric, and it is losing
someprecisionwhencomparedto,e.g.,theSwapdistance,itismuchfaster
tocomputeandstillcarriesalotofinformation. Withinthischapter,wealso
discuss two other nonisomorphic distances, the pairwise distance and the
Bordawisedistance. Finally,wecompareallisomorphicandnonisomorphic
distances.
Moreover, throughout the chapter we present various maps of elections—
oneforeachmetric.
Applications. We focus on practical applications of the map of elections frame-
work. We consider several embedding algorithms (i.e., ways of putting a
set of points in a low-dimensional Euclidean space) and evaluate their per-
formance. We focus on Fruchterman-Reingold force-directed algorithm,
a novel variant of Kamada-Kawai algorithm, Multi Dimensional Scaling
and a few others. Next, we evaluate popular voting rules for ordinal elec-
tions. First,wefocusonsingle-winnervotingrulessuchasPlurality,Borda,
Copeland,andDodgson. Alltheserulesassignacertainscoretoeachcandi-
dateandthecandidatewiththehighestscoreisdeclaredawinner. Foreach
election, we can compute such highest score, and then color the map pro-
portionallytothatscore,thatis,coloreachpointonthemapproportionally
to the highest score in the election that that point depicts. We also ana-
lyze multiwinner voting rules such as Chamberlin–Courant and Harmonic-
Borda. Both these rules assign a certain score to each committee, and the
committee with the highest score is declared as winning. So, again, we can
color the map, however, this time we color it proportionally to the score of
the best committee. Besides coloring the map by a score, we also color it
by the runtime of the algorithm for a given voting rule. Such time-focused
coloring gives an insight into which types of elections are harder (i.e., take
moretime)andwhichonesareeasier(i.e.,takelesstime)tocompute. Then,
we analyze real-life instances of elections. In particular, we focus on the
data provided within PrefLib—a popular preference library that contains
various real-life datasets. We study such instances as political elections in
Dublin, Glasgow, and Aspen; voting of Electoral Reform Society; surveys
11aboutdifferenttypesofsushiandaboutpicturesonT-Shirts;numeroussport
competitionsandmanyothers. Finally,weconsideraskeletonmap—aspe-
cialtypeofamapofelections,whichcanbecomputedanalytically.
Subelections. For the classical ELECTION ISOMORPHISM problem, we always
consider elections of the same size, that is, with the same number of voters
and the same number of candidates in both elections. In this chapter, we
introduce SUBELECTION ISOMORPHISM, where we relax the assumption
about the sizes of elections. In the SUBELECTION ISOMORPHISM problem
we are given two elections, a smaller and a larger one, and we ask if it is
possible to remove some candidates and voters from the larger election so
thatitbecomesisomorphictothesmallerone.
Moreover, we consider a family of MAXIMUM COMMON SUBELECTION
problems, where given two elections we ask for the largest election, which
is a subelection of both given elections at the same time. First, we provide
the computational complexity for all variants, and later we present several
experimentalresultsonbothsyntheticandreal-lifedata.
ApprovalElections. We consider approval elections, where instead of ranking
all the candidates, voters approve subsets of them. In other words, each
voterpartitionallthecandidatesintotwosets,thosethatheorsheapproves
andthosethatheorshedoesnot. Asforordinalelections,weintroducedis-
tances between such elections (the isomorphic Hamming distance and the
nonisomorphic approvalwise distance). We present several novel statistical
cultures, and argue why we recommend using them. Again, we show maps
of preferences. Finally, we present maps of approval elections and conduct
experimentsuchas,forexample,analysisofcohesivenesslevelorbehavior
ofvotingrules.
Summary. In the last chapter, we recapitulate the main contributions of this dis-
sertation,andshowdirectionsforpossibleextensionsandfuturework.
1.4 Conference Publications
Most of the results presented in this dissertation have already been presented at
various conferences. In the following, we attach the list of publications chrono-
logically, by the date of publication, on which this thesis is based. All the results
thatareincludedinthethesisareduetoStanisławSzufa.
121. HowSimilarAreTwoElections?
PiotrFaliszewski,PiotrSkowron,ArkadiiSlinko,StanisławSzufa,Nimrod
Talmon(AAAI-2019).
2. DrawingaMapofElectionsintheSpaceofStatisticalCultures
StanisławSzufa,PiotrFaliszewski,PiotrSkowron,ArkadiiSlinko,Nimrod
Talmon(AAMAS-2020).
3. PuttingaCompassontheMapofElections
Niclas Boehmer, Robert Bredereck, Piotr Faliszewski, Rolf Niedermeier,
StanisławSzufa(IJCAI-2021).
4. TheComplexityofSubelectionIsomorphismProblems
PiotrFaliszewski,KrzysztofSornat,StanisławSzufa(AAAI-2022).
5. UnderstandingDistanceMeasuresAmongElections
Niclas Boehmer, Piotr Faliszewski, Rolf Niedermeier, Stanisław Szufa,
TomaszWa¸s(IJCAI-2022).
6. HowtoSampleApprovalElections?
Stanisław Szufa, Piotr Faliszewski, Łukasz Janeczko, Martin Lackner,
ArkadiiSlinko,KrzysztofSornat,NimrodTalmon(IJCAI-2022).
7. Expected Frequency Matrices of Elections: Computation, Geometry, and
PreferenceLearning
Niclas Boehmer, Robert Bredereck, Edith Elkind, Piotr Faliszewski,
StanisławSzufa(NeurIPS-2022).
Below we list the results from the thesis that are not included in any of the
publicationsdescribedabove.
• MapsofOrdinalPreferences(Section3.4).
• Evaluation of different embedding algorithms (i.e., analysis of distortion
andmonotonicity;Section5.3).
• Comparisonoftheperformerceofdifferentvotingrules(Section5.4).1
• Experimentsonreal-lifedatainthecontextofsubeletions(Section6.4.2)
1MinorresultswerealsopublishedintheworkofSzufaetal.[2020]
13• MapsofApprovalPreferences(Section7.4).
Astotheconnectionsbetweenthechaptersandthepapers,theyareasfollows.
• Chapter 4 is based on the papers How Similar Are Two Elections? [Fal-
iszewski et al., 2019], Drawing a Map of Elections in the Space of Sta-
tistical Cultures [Szufa et al., 2020], Putting a Compass on the Map of
Elections[Boehmeretal.,2021],UnderstandingDistanceMeasuresAmong
Elections[Boehmeretal.,2022c].
• Chapter 5 is based on the papers Drawing a Map of Elections in the Space
ofStatisticalCultures[Szufaetal.,2020]andExpectedFrequencyMatrices
of Elections: Computation, Geometry, and Preference Learning [Boehmer
etal.,2022a].
• Chapter6isbasedonthepaperTheComplexityofSubelectionIsomorphism
Problems[Faliszewskietal.,2022].
• Chapter7isbasedonthepaperHowtoSampleApprovalElections?[Szufa
etal.,2022].
Acknowledgements
TheresearchpresentedinthisdissertationwassupportedbytheNationalScience
Centre,Poland(NCN)grantNo2018/29/N/ST6/01303andbyEuropeanResearch
Council(ERC)undertheEuropeanUnion’sHorizon2020researchandinnovation
programme(grantagreementNo101002854).
14Chapter 2
Preliminaries
In this chapter, we describe the basic concepts and notation. We define basic
metrics between vectors and basic metrics between votes, which will serve us for
computingdistancesbetweenelections,tobeintroducedinChapter4. Webriefly
describe six different embedding algorithms. Finally, we explain the concept of
amap.
For a given positive integer t, we write [t] to denote the set {1,2,...,t}, and
we write [t] as an abbreviation for [t] ∪ {0}. By R we denote the set of non-
0 +
negativerealnumbers. ByS wemeanthesetofallpermutationsover[n]. Given
n
two equal-sized sets A and B, by Π(A,B) we denote the set of all one-to-one
mappings from A to B. For a vector x, x denotes the arithmetic average of the
valuesfromx.
2.1 Elections
An election E = (C,V) consists of a set of candidates C = {c ,...,c } and
1 m
a collection of voters V = (v ,...,v ), where each voter v has a preference or-
1 n
der (sometimes referred to as a vote), also denoted as v (the exact meaning will
always be clear from the context, and this convention will simplify our discus-
sions). We write L(C) to denote the set of all preference orders over C. Every
subset D of L(C) is called a domain (of preference orders over C) and, in partic-
ular, L(C) itself is the general domain. The preference orders always come from
some domain D (the general domain, unless stated otherwise). Given two candi-
dates c ,c ∈ C, we write c ≻ c (or, equivalently, v: c ≻ c ) to denote that
i j i v j i j
voter v prefers c to c . We extend this notation to more than two candidates in a
i j
15naturalway. Forexample,wewritev: c ≻ c ≻ ··· ≻ c toindicatethatvoterv
1 2 m
likes c best, then c , and so on, until c . If we put some set S of candidates
1 2 m
in such a description of a preference order, then we mean listing its members in
←−
some arbitrary (but fixed, global) order. Including S means listing the members
ofS inthereverseorder.
Consider two sets of candidates, C and D, of the same cardinality. Let σ be a
bijection from C to D. We extend σ to act on preference orders v in L(C) in the
natural way: σ(v) ∈ L(D) is the preference order such that for each c,c′ ∈ C it
holdsthatv: c ≻ c′ ⇐⇒ σ(v): σ(c) ≻ σ(c′).
For an election E = (C,V), where V = (v ,...,v ), a candidate set D,
1 n
and a bijection σ from C to D, by σ(E) we mean election with candidate set D
and voter collection (σ(v ),...,σ(v )). Similarly, given a permutation π ∈ S ,
1 n n
byπ(V)wemean(v ,...,v ).
π(1) π(n)
2.2 Distances
Formally, for a set X a function d: X × X → R ∪ {0} is a metric if for
+
eachx,y,z ∈ X itholdsthat:
1. d(x,y) = 0ifandonlyifx = y,
2. d(x,y) = d(y,x),
3. d(x,z) ≤ d(x,y)+d(y,z).
A pseudometric relaxes the first condition to the requirement that d(x,x) = 0 for
each x ∈ X. In particular, for a pseudometric d it is possible that d(x,y) = 0
whenx ̸= y.
DistancesBetweenVectors
For some metrics, as an intermediate step, we will be computing the distances
betweenvectorsofrealnumbers.
Let x = (x ,...,x ) and y = (y ,...,y ) be two real-valued vectors. Then
1 n 1 n
foragivenp ∈ R,theirℓ -distanceisℓ (x,y) = (|x −y |p+···+|x −y |p)1/p.
p p 1 1 n n
Giventworeal-valuedvectorsxandy,wewriteemd(x,y)todenotetheearth
mover’s distance (EMD) between them. Intuitively, this is the minimal cost of
turningxintoy,wherethecostofmovingavalue∆frompositionitopositionj
16inthevectoris∆·|i−j|. OurEMDdistancecanbecomputedusingawell-known
greedypolynomial-timealgorithm.
Given a real-valued vector z = (z ,...,z ), we write zˆ to denote its prefix-
1 n
sum variant, i.e., an n-dimensional vector such that for each i ∈ [n], its i-th entry
is zˆ = z +z +···+z . If the entries of x and y sum up to the same value and
i 1 2 i
contain only nonnegative entries, then their earth mover’s distance alternatively,
canbedefinedas:
emd(x,y) = ℓ (xˆ,yˆ).
1
Bothdefinitions,presentedabove,areequivalent(Rubneretal.[2000]).
DistancesBetweenVotes
Wefocusonthefollowingthreedistancesbetweenpreferenceorders(below,letC
beasetofcandidatesandletuandv betwopreferenceordersfromL(C)):
DiscreteDistance. Thediscretedistancebetweenuandv,d (u,v),is0whenu
disc
andv coincideandis1otherwise.
SwapDistance. The swap distance between u and v (also known as Kendall’s
Tau distance in statistics), denoted d (u,v), is the smallest number of
swap
swapsofconsecutivecandidatesthatneedtobeperformedwithinutotrans-
formitintov.
SpearmanDistance. The Spearman’s distance (also known as the Spearman’s
footrule or the displacement distance) measures the total displacement of
candidatesinurelativetotheirpositionsinv. Formally,itisdefinedas:
(cid:88)
d (u,v) = |pos (c)−pos (c)|.
Spear v u
c∈C
Example 2.1. Consider an election E = (C,V), where C = {a,b,c,d,e}, V =
(u,v),andthevotesare:
u: a ≻ b ≻ c ≻ d ≻ e,
v: b ≻ a ≻ e ≻ c ≻ d.
Then, d (u,v) = 1, because the votes are not identical; d (u,v) = 3, as
disc swap
to transform u into v we can first swap (a,b), then (d,e), and finally (c,e);
andd (u,v) = 1+1+1+1+2 = 6;1fromcandidateabecause|pos (a)−
Spear u
pos (a)| = 1,also1fromcandidatebbecause|pos (b)−pos (b)| = 1,similarly1
v u v
fromc,and1fromd,andfinally2fromebecause|pos (e)−pos (e)| = 2. △
u v
17(a)PCC=−1 (b)PCC=−0.5 (c)PCC=0 (d)PCC=0.5 (e)PCC=1
Figure 2.1: Examples of PCC values. In each picture we visualize two vectors
(x ,...,x ) and (y ,...,y ) as points (p ,...,p ), where each point p has co-
1 n 1 n 1 n i
ordinatesx ,y .
i i
We only consider distances over preference orders that are defined for all sets
of candidates (as is the case for d , d , and d ). For an in-depth discus-
disc swap Spear
sion regarding distances between elections, we point to the literature on distance
rationalizabilityofvotingrules[Nitzan,1981,MeskanenandNurmi,2008,Elkind
etal.,2015]and,inparticular,tothesurveyofElkindandSlinko[2016].
2.3 Correlation
ThePearsonCorrelationCoefficient(PCC)measurestheleveloflinearcorrelation
between two random variables and takes values between −1 and 1. Its absolute
value gives the level of correlation, and the sign indicates positive or negative
correlation. For two vectors x = (x ,...,x ) and y = (y ,...,y ), their PCC is
1 t 1 t
definedas:
PCC(x,y) = (cid:80)t i=1(xi−x)(yi−y) .
((cid:80)t i=1(xi−x)2(cid:80)t i=1(yi−y)2)1/2
InFigure2.1wepresenttheexamplesofthePCCvalues. Forvaluescloseto0we
do not have correlation at all. For values close to 1 (−1) we have strong positive
(negative)correlation.
We use PCC as it is one of the standard, well-known ways of calculating the
correlation.
2.4 Integer Linear Programming
IntegerLinearProgrammingisawayofsolvingoptimizationproblems,wherethe
original problem is being represented as a linear objective function and a set of
18constraints expressed as linear inequalities, where all variables are integers. ILP
algorithmsareparticularlyusefulwhentheconsideredproblemisNP-hard.
Inourexperiments,weusetwopopular(andfreeunderacademiclicense)ILP
solvers. One provided by Gurobi Optimiztion and the other one provided by IBM
ILOGCPLEXOptimizationStudio.
In some experiments, we will focus on the running time of certain algorithms
(e.g., the running time needed to computing the winning committee under given
multi-winner voting rule). Whenever we discuss the running time of a particular
algorithm, we assume that the computation for a single instance was run with
CPLEX on a single thread (Intel(R) Xeon(R) Platinum 8280 CPU @ 2.70GH) of
a 448 thread machine with 6TB of RAM, with exception for experiments done
in 6, which were performed on a single thread on Apple MacBook Air with M1
processorand8GBRAM.
2.5 Embeddings
Sometimes,givenasetofpointsandtheirdistancematrix(i.e.,squarematrixcon-
taining all pairwise distances between points), we want to embed these points in
alow(i.e.,twoorthree)dimensionalspace. Todothis,wecanuseawidevariety
of techniques. We use the following six methods (which we briefly describe be-
low): Principal Component Analysis1 [Minka, 2000], (metric) Multidimensional
Scaling2 [Kruskal, 1964, de Leeuw, 2005], t-Distributed Stochastic Neighbor
Embedding3 [der Maaten and Hinton, 2008, der Maaten, 2010], Locally Linear
Embedding4 [Donoho and Grimes, 2003, Zhang and Wang, 2006], Fruchterman-
Reingold5 [FruchtermanandReingold,1991],andKamada-Kawai6 [Kamadaand
Kawai,1989,Sapała,2022]:
PrincipalComponentAnalysis(PCA) is a linear dimensionality reduction al-
gorithmthataimsatextractingcrucialinformationfromahigh-dimensional
space. Itisbasedoneigenvaluesandeigenvectorsofthedistancematrix.
1WeusePythonimplementationfromsklearn.decomposition.PCApackage.
2WeusePythonimplementationfromsklearn.manifold.MDSpackage.
3WeusePythonimplementationfromsklearn.manifold.TNSEpackage.
4WeusePythonimplementationfromsklearn.manifold.LocallyLinearEmbeddingpackage.
5WeusePythonimplementationfromnetworkx.spring_layoutpackage.
6WeusePythonimplementationfrommapel.corepackage
19(metric)MultidimensionalScaling(MDS) unlike PCA, is a nonlinear dimen-
sionality reduction method. It mainly focuses on maintaining the original
distances,byminimizingthestressfunction,wherethestressfunctionisthe
square root of the normalized squared misrepresentations (i.e., differences
betweenoriginaldistancesandEuclideandistancesaftertheembedding).
t-DistributedStochasticNeighborEmbedding(t-SNE) is a statistical method
based on Kullback–Leibler divergence. It may not properly preserve densi-
tiesordistances. LikeMDS,itisnonlinear.
LocallyLinearEmbedding(LLE) is also a nonlinear dimensionality reduction
method. Whileembeddingthepoints,insteadoftryingtomaintainproperly
the distances between all of them, it is only focusing on maintaining the
distances between points that are close (i.e., the original distance between
themissmall)toeachother.
Fruchterman-Reingold(FR) is a force-directed graph drawing algorithm (i.e.,
it aims at drawing the graphs in a pleasant and appealing way). It works in
analogy to physical springs as edges between points. It uses both attracting
and repulsing forces between points. The aim of the method is to draw
an appealing graph. It tries to distribute all the points more or less evenly
acrossthegivenspace.
Kamada-Kawai(KK), like FR, is a force-directed graph drawing algorithm.
The difference between FR and KK is that FR focuses more on producing
apleasantpicture,whileKKfocusesmoreonmaintainingproperdistances.
We use a variant of KK proposed by Sapała [2022] that lowers the proba-
bilityoftheresultbeingstuckinthelocalminima.
Oneofthemaindisadvantagesofforce-directedalgorithmsisthattheyareslower
compared to the other methods described above. We provide a detailed compari-
sonoftheembeddingalgorithmsinSection5.3.
2.6 Map of Objects
Givenasetofobjects(forexample,asetofelectionsorasetofvotes),byamapof
these objects, we refer to a two-dimensional graphical representation of that set.
Tocreatesuchamap,wefirstcomputedistancesbetweeneachpairofobjects,and
then, based on these distances, we create a two-dimensional embedding, where
20a b c d e
a − 2 2 4 4
b 2 − 2 4 4
c 2 2 − 3 3
d 4 4 3 − 1
e 4 4 3 1 −
(a)DistanceMatrix (b)Map
Figure2.2: Toyexamplewithmatrixdistances(left)andthemap(right).
each point depicts a single object. We expect similar objects to be embedded
close to one another. Note that objects might be located in a high-dimensional
non-Euclidean space, hence, it will not always be possible to embed properly all
thepoints,i.e.,maintainallthedistances.
We briefly discuss a toy example of such a map. Let us assume that we have
five items named a,b,c,d, and e, and the distance matrix shown in Figure 2.2a.
Looking at the matrix, we expect a,b, and c to be located relatively close to each
otherandtoformmoreorlessatriangle,andweexpectdandetobelocatedeven
closer toeach other(because thedistance between themis the smallest onein the
wholematrix). Moreover,weexpectthattheabctrianglewouldberotatedinsuch
awaythatcwouldbepointingtowardsde. Thatisexactlywhatweobserveinthe
embeddingpresentedinFigure2.2b,andthatisourmap.
21Chapter 3
Statistical Cultures
In this chapter we describe the statistical cultures that we use for generating in-
stances of elections. First, we describe general models, and then we move on to
structured domains. At the end of the chapter, we present maps of preferences,
a simple yet interesting visualization of instances generated from various models
described below. Maps of preferences help in understanding the structure of the
votesandshow(dis)similaritiesbetweendifferentmodels.
3.1 General Models
Below, we define the most popular general1 statistical cultures that we will use
in this dissertation. When discussing elections, by m we denote the number of
candidates,andbynwedenotethenumberofvoters.
ImpartialCultureandRelatedModels
Under the impartial culture (IC) model, every preference order appears with the
same probability. That is, to generate a vote, we choose a preference order uni-
formlyatrandom.
Under the impartial anonymous culture (IAC) model, we require that each
voting situation appears with the same probability [Kuga and Nagatani, 1974,
Fishburn and Gehrlein, 1978]. A voting situation specifies how many votes with
a given preference order are present in a profile; thus, IAC generates anonymized
preferenceprofilesuniformlyatrandom.
1Bygeneralwemeanthatanyelectioncanbesampledfromsuchstatisticalculture.
22The impartial anonymous neutral culture (IANC) additionally abstracts away
fromthenamesofthecandidates[Eg˘eciog˘luandGiritligil,2013]. Thismeansthat
foragivennumbersofcandidatesandvoters,thenumberofdifferentIANCelec-
tionsisequaltothenumberofequivalenceclassesunderanyisomorphicdistance,
suchas,forexample,theswapdistance.
Pólya-EggenbergerUrnModel
The Pólya-Eggenberger urn model [Berg, 1985, McCabe-Dansted and Slinko,
2006] is parametrized with a nonnegative number α, the level of contagion, and
proceeds as follows: Initially, we have an urn with one copy of each of the m!
possible preference orders. To generate a vote, we draw a preference order from
the urn uniformly at random (this is the generated vote) and return it to the urn
together with additional αm! copies. The larger α is, the more correlated are the
generated votes. For α = 0, the model is equivalent to IC, for α = 1/ m!, it is
equivalenttoIAC,andforα = ∞allvotesareidentical.
MallowsModel
The Mallows model [Mallows, 1957] is parameterized by a dispersion parame-
ter ϕ ∈ [0,1] and a center preference order v (we choose it uniformly at random
and then use it for all generated votes). We generate each vote independently at
random, where the probability of generating vote u is proportional to ϕdswap(v,u).
For ϕ = 1, the model is equivalent to IC, while for ϕ = 0 all generated votes are
identical to the center vote v. See the work of Lu and Boutilier for an effective
algorithmforsamplingfromtheMallowsmodel[LuandBoutilier,2014].
In our experiments, we consider a new parameterization introduced by
Boehmeretal.[2021]. Itusesanormalizeddispersionparameter norm-ϕ,which
isconvertedtoavalueofϕsothattheexpectedswapdistancebetweenthecentral
votev∗ andasampledvotev is
norm-ϕ
timesthemaximumswapdistancebetween
2
two votes. We refer to Mallows model with normalized dispersion parameter as
NormalizedMallows(Norm-Mallows)model.
Besides the basic Norm-Mallows model, we consider a combination of two
Norm-Mallows models, where ω ∈ (0,0.5] fraction of the votes are reversed,
i.e., after sampling all the votes from the basic Norm-Mallows model, we reverse
the first ⌊ωn⌋ of them2. We refer to this variant as the weighted Norm-Mallows
2Itisequivalenttosampling⌊ωn⌋votesfromthebasicNorm-Mallowsmodel,andthensam-
plingtherestofthevotesfromthesameNorm-Mallowsmodelbutwithareversedcentralballot.
23model.
3.2 Structured Domains
In this section we focus on structured domains. We describe several properties of
elections such as single-peakedness, single-crossingness, and group-separability,
and discuss how to sample elections having such properties. Moreover, we study
theEuclidean-basedmodels.
3.2.1 Single-Peaked Elections
Single-peaked preferences, introduced by Black [1958], capture settings where it
ispossibletoorderthecandidatesinsuchawaythataswemovealongthisorder,
eachvoter’sappreciationofthecandidatesfirstincreasesandthendecreases. One
typical example of such an order is the classic left-to-right spectrum of political
opinions.
Definition 3.1. Let v be a vote over C and let ◁ be the societal axis over C. We
say that v is single-peaked with respect to ◁ if for every t ∈ [|C|] its t top-ranked
candidates form an interval within ◁. An election is single-peaked with respect
to ◁ if all its votes are. An election is single-peaked (SP) if it is single-peaked
withrespecttosomeaxis.
Example 3.1. Consider an election with the set of candidates C = {a,b,c,d,e}
andvotes:
v : a ≻ b ≻ c ≻ d ≻ e,
1
v : e ≻ d ≻ c ≻ b ≻ a,
2
v : b ≻ c ≻ a ≻ d ≻ e.
3
This election is single-peaking with respect to axis a,b,c,d,e. Moreover, it is a
uniqueaxiswithrespecttowhichthiselectionissingle-peaked. △
Wealsoconsiderthesingle-peakedonacircledomain(SPOC),introducedby
Peters and Lackner [2020]. A vote is SPOC with respect to an axis c ◁···◁c
1 m
ifitissingle-peakedwithrespecttosomeaxisoftheform:
c ◁c ◁···◁c ◁c ◁···◁c ;
i i+1 m 1 i−1
24An election is SPOC with respect to an axis if all of its votes are (the value of i
may differ from one vote to another). SPOC votes may capture, for example,
preferencesregardingmeetingtimeswhenpeopleareindifferenttimezones.
Sampling
We consider two ways of generating single-peaked elections, one studied
by Walsh [2015] and one studied by Conitzer [2009]; hence, we refer to them
as the Walsh model and the Conitzer model. In both models, we first choose the
axis(uniformlyatrandom). Togenerateavote,weproceedasfollows:
1. UndertheWalshmodel,wechooseasingle-peakedpreferenceorder(under
the given axis) uniformly at random. Walsh [2015] provided a sampling al-
gorithm for this task. This model is also sometimes referred to as impartial
cultureoversingle-peakedvotes.
2. TogenerateavoteundertheConitzermodelfortheaxisc ◁c ◁···◁c ,
1 2 m
wefirstchoosesomecandidatec (uniformlyatrandom)toberankedontop
i
(so, at this point, c is the only ranked candidate). Then, we perform m−1
i
steps as follows: Let {c ,c ,...,c } be the set of the currently ranked
j j+1 k
candidates. We choose the next-ranked candidate from the set {c ,c }
j−1 k+1
uniformly at random. This model is also sometimes referred to as the ran-
dompeak model.
To generate a single-peaked on a circle vote, we use the Conitzer model, except
that we take into account that the axis is cyclical (note that this process generates
each possible SPOC vote with equal probability, so, in fact, we can say that we
useimpartialcultureoverSPOCvotes).
3.2.2 Single-Crossing Elections
We also consider single-crossing elections, introduced by Mirrlees [1971] and
Roberts[1977]inthecontextoftaxation.
Definition 3.2 (Mirrlees [1971], Roberts [1977]). An election E = (C,V) is
single crossing if it is possible to order the voters in such a way that for each pair
of candidates a,b ∈ C, the set of voters that prefer a to b either forms a prefix or
asuffixofthisorder.
25Example 3.2. Consider election with the set of candidates C = {a,b,c,d}, and
votes:
v : a ≻ b ≻ c ≻ d,
1
v : a ≻ b ≻ d ≻ c,
2
v : d ≻ a ≻ c ≻ b,
3
v : d ≻ c ≻ b ≻ a.
4
This election is single-crossing because each pair of candidates is crossing at
most once. In particular, pair {c,d} is crossing between votes v and v ,
1 2
pairs {a,d},{b,c}, and {b,d} are crossing between votes v and v , and
2 3
pairs{a,b}and{a,c}arecrossingbetweenvotesv andv . △
3 4
We say that a set of preference orders D is a single-crossing domain if every
election where each voter has a preference order from D is single-crossing. For
a recent discussion of single-crossing domains, see, e.g., the work of Puppe and
Slinko[2019].
Sampling
We would like to generate single-crossing elections uniformly at random, but we
are not aware of an efficient sampling algorithm for this task. Thus, to generate
a single-crossing election, we first generate a single-crossing domain D and then
drawnvotesfromituniformlyatrandom. Togeneratethisdomainforacandidate
setC = {c ,...,c },weusethefollowingprocedure:
1 m
1. We let v be a preference order c ≻ c ≻ ··· ≻ c and we output v as the
1 2 m
firstmemberofourdomain.
2. Werepeatthefollowingstepsuntilweoutputc ≻ c ≻ ··· ≻ c :
m m−1 1
(a) We draw candidate c uniformly at random and we let c be the can-
j i
didate ranked right ahead of c in v (if c is ranked on top, then we
i j
repeat);
(b) If i < j then we swap c and c in v and output the new preference
i j
order.
3. Werandomlypermutethenamesofthecandidates.
Ourdomainsalwayshavecardinality(1/ 2)m(m−1)+1.
26x
1
y a x
1 2
z 1 y 2 y 3 b x 3
a b c d a b c d c d
(a)T (b)T (c)T
1 2 3
Figure3.1: Threeexamplesofclonedecompositiontrees.
3.2.3 Group-Separable Elections
Next, we consider group-separable elections, introduced by Inada [1964, 1969].
Anelectionisgroup-separableifeachsetAofatleasttwocandidatescanbepar-
titionedintotwononemptysubsets,A′ andA′′,suchthateachvotereitherprefers
allmembersofA′ toallmembersofA′′ ortheotherwayround. Forourpurposes,
it will be convenient to use the tree-based definition of Karpov [2019] which is
equivalent to the previous one. Let C = {c ,...,c } be a set of candidates and
1 m
consider a rooted, ordered tree T whose leaves are elements of C. The frontier
of this tree is the preference order that ranks the candidates in the order in which
they appear in the tree from left to right. A preference order is consistent with a
given tree if it can be obtained as its frontier by reversing the order in which the
childrenofsomenodesappear.
Definition 3.3. An election E = (C,V) is group-separable if there is a rooted,
ordered tree T whose leaves are members of C, such that each vote in V is con-
sistentwithT .
ThetreesfromDefinition3.3formasubclassofclonedecompositiontrees,which
areexamplesofPQ-trees[Elkindetal.,2012,BoothandLueker,1976].
27Example3.3. ConsiderthesetofcandidatesC = {a,b,c,d},treesT ,T ,andT
1 2 3
fromFigure3.1,andvotes:
v : a ≻ b ≻ c ≻ d,
1
v : c ≻ d ≻ b ≻ a,
2
v : b ≻ d ≻ c ≻ a.
3
Vote v is consistent with each of the trees, v is consistent with T (reverse the
1 2 2
children of y and y ) and with T (reverse the children of x and x ) , and v is
1 2 3 1 2 3
consistentwithT (reversethechildrenofx andx ). △
3 1 3
In many cases, we will be interested in two particularly characteristic trees,
i.e.,balancedandcaterpillarones. Balancedtreeisacomplete,fullbinarytree(if
the number of candidates/leaves is equal to a power of two, then this is a perfect
tree). Caterpillar tree is a binary tree where each inner node’s left child is a leaf,
and the right child is either an inner node or a leaf. T from Figure 3.1 is an
2
exampleofabalancedtree,whereasT isanexampleofacaterpillartree.
3
Sampling
Givenacertaintree,togenerateavote,wesimplyreverseeachinternalnodewith
probability 0.5 and then take the frontier as our vote. We repeat this procedure
independentlytogenerateasmanyvotesasrequiredintheelection.
3.2.4 Euclidean Elections
Finally, Euclidean preferences, discussed in detail, e.g., by Enelow and Hinich
[1984, 1990], are based on a similar idea as the single-peaked ones, but are de-
fined geometrically: Each candidate and each voter corresponds to a point in a
Euclideanspaceandvotersformtheir preferencesbyrankingthecandidateswith
respect to their distance. That is, if the point of voter v is closer to the point of
candidatecthantothatofcandidatedthenv prefersctod.
Definition3.4. Lettbeapositiveinteger. AnelectionE = (C,V)ist-Euclidean
if it is possible to associate each candidate and each voter with his or her ideal
point in a t-dimensional Euclidean space Rt in such a way that the following
holds: For each voter v and each two candidates a,b ∈ C, v prefers a to b if and
onlyifv’spointisclosertothepointofathantothepointofb.
28Sampling
To generate the Euclidean election, we simply sample ideal points of candidates
and voters from a given space and then, based on these ideal points, we create
the votes. Given a certain space, we sample from it uniformly at random. In
particular,weconsiderthefollowingmodels:
• Interval – points are sampled uniformly at random from a 1-dimensional
interval.
• Disc–pointsaresampleduniformlyatrandomfroma2-dimensionaldisc.
• Square – points are sampled uniformly at random from a 2-dimensional
square.
• Cube–pointsaresampleduniformlyatrandomfroma3-dimensionalcube.
• n-Cube – points are sampled uniformly at random from an n-dimensional
hypercube.
• Circle–pointsaresampleduniformlyatrandomfromacircle.
• Sphere – points are sampled uniformly at random from an ordinary sphere
ina3-dimensionalEuclideanspace.
• n-Sphere–pointsaresampleduniformlyatrandomfromann-sphereinan
(n+1)-dimensionalEuclideanspace.
It is well known that Interval elections are both single-peaked and single-
crossing. WealsonotethatinCircleelections,thevotershaveSPOCpreferences.
3.3 Compass Elections
Next,weprovidefourcharacteristicelections,towhichwereferascompasselec-
tions. We believe that they capture some notions of “extremes” and are qualita-
tivelydifferentfromeachother. Thesefourcompasspointsareasfollows.
Identity. In the identity elections, denoted ID, all voters have the same, fixed
preferenceorder—whichwesampleuniformlyatrandom.
29Antagonism. In the antagonism elections, denoted AN, half of the voters rank
the candidates in one way and half of the voters rank them in the opposite
way.
Uniformity. Intheuniformityelections,denotedUN,eachpossiblevoteappears
thesamenumberoftimes.
Stratification. In the stratification elections, denoted ST, the candidates are par-
titioned into two equal-sized sets A and B. Each possible preference order
whereallmembersofAarerankedaheadofB appearsthesamenumberof
times.
In practice, to generate the identity election, we sample one vote uniformly
at random, and all votes are its copies. To generate the antagonism election, we
sample one vote uniformly at random, and half of the votes are its copies, while
theotherhalfarecopiesofthereversevote. Togetidealuniformityandstratifica-
tion,wewouldneedexponentiallymanyvotes(i.e.,withrespecttothenumberof
candidates), so due to limited number of votes, for uniformity we just sample an
election from impartial culture—as an approximation of the uniformity, and for
stratification,togenerateavotewesamplethefirsthalfofthevotefromimpartial
culture (based on the first half of the candidates), and then we sample the second
half of the vote also from impartial culture (but based on the second half of the
candidates).
3.4 Map of Preferences
To get a better understanding of our statistical cultures, in this section we present
a map of preferences3. For a given election, to generate its map of preferences,
we proceedas follows. First, we compute theswap distance between eachpair of
votes. Then, based on these distances, we create a two-dimensional embedding
using the MDS algorithm (see Section 2.5). Each dot corresponds to a single
vote. The closer two dots are on the map, the more similar are the votes that they
represent(or,moreprecisely,thesmalleristheirswapdistance).
3Inprinciple,themapofpreferencesisverysimilartothemapofelections,whereeachpoint
on the map, instead of depicting a single election, is depicting a single vote. Historically, we
introducedthemapsofelectionspriortothemapsofpreferences
30Figure3.2: MapsofPreferences(10candidates,1000voters).
We generated elections with 10 candidates and 1000 voters from 25 different
models4, described before. These models include impartial culture, urn model
with α ∈ {0.05,0.2,1}, Walsh and Conitzer models, SPOC, single-crossing
model, balanced and caterpillar group-separable models, Interval, Square, Cube,
10-dimensional Hypercube, Circle, and Sphere Euclidean models, and Norm-
Mallows model with ϕ ∈ {0.05,0.2,0.5}, and ω ∈ {0,0.25,0.5} (with each
possible combination of ϕ and ω). Moreover, we added three compass elections,
i.e., ID, AN, and ST; we skipped UN because the result is almost identical to the
ICmap. TheresultsarepresentedinFigure3.2. Forclarity,iftherearemorethan
30 copies of the same vote, we denote it by adding a purple disc—the larger the
disc,themorecopiesthereare.
We start our analysis by looking at the impartial culture election. Votes are
more or less uniformly spread, with slightly higher density near the edge. In
multidimensionalspacethevoteswouldformapermutohedron;however,herewe
4Wetakeatmostoneelectionfromagivenmodel,theonlyexceptionsareparametrizedmodels
such as the Norm-Mallows and urn models, from which we take several elections with different
parameters.
31are limited to an embedding in two-dimensional space, so proportionally more
voteslandontheedge.
Then, we have ID followed by AN and ST. As expected, for ID we have
a single point in the center because all votes are identical, and for AN we have
twopointslocatedatthelargestpossibledistancebecausewehaveonlytwotypes
of votes (i.e., 500 times vote v, and 500 times its reversed copy). For ST, we
observe a similar picture to the one for the IC election, however, the diameter is
much smaller. This is because in an ST election all the voters agree that half of
the candidates are better than the other half, hence, the largest possible distance
between two votes is equal to half of the largest possible distance between two
votesfromIC.
Next, we have three elections from the urn model. The larger is the α param-
eter, the smaller is the number of different votes, leading to fewer points on the
map. To be more precise, below we provide the formula for the (upper bound
on the) expected number of different votes under the urn model, with assumption
thatn ≤ m!.
Proposition 3.1. Given parameter of contagion α and number of voters n
the expected number of different votes under the urn model is upper-bounded
by(cid:80)n 1 .
i=1 1+(i−1)α
Proof. Theprobabilityofhavinganewvoteinthefirstiterationis1;inthesecond
iteration it is at most 1 ; in the third iteration it is at most 1 and so on. In
1+α 1+2α
general,intheithiterationwehaveatmostprobability 1 ofsamplingavote
1+(i−1)α
from the original urn, and probability (i−1)α of repeating one of the previous
1+(i−1)α
votes. Therefore, the expected number of different votes in n iterations is upper-
boundedby(cid:80)n 1 . Itisanupper-boundbecauseweignorethecasewhere
i=1 1+(i−1)α
whilesamplingavotefromtheoriginalurn,wesampleavotethatwehavealready
sampledbefore.
For the normalized Mallows model, the shorthand captions in the pictures are
oftheformω-N-Mal.ϕ. ForstandardNormalizedMallows,asexpected,wehave
acentralpoint(correspondingtothecentralorder)andthefurtherawaywemove
from that point, the fewer votes we have. On the other hand, for the weighted
variant with ω ∈ {0.25,0.5} we observe two antagonistic groups. The central
ranking and its reverse are at the largest possible distance. Any noise on one of
themisshiftingagivenvoteclosertotheothergroup.
Next, we move on to structured domains. We start with the single-crossing
model. The map for the single-crossing is one straight line. It is because the
32single-crossing domain is defined by a sequence of swaps, so for each vote the
sum of its distances to the two most extreme votes is constant. Moreover, there
cannot be two different votes that are at the same distances from the extremes,
because it would contradict the fact that the domain is defined by a sequence of
swaps. ThemapfortheIntervalmodellooksverysimilar. Notethateveryelection
from the Interval model is also single-crossing. Interestingly, the votes from the
Interval election look less evenly distributed than those from the single-crossing
election. It is so, because in the Interval election candidates’ points are sampled
randomly, so since there are only ten of them, by chance they can be distributed
unevenly over the interval, which leads to an uneven distribution of preference
orders. Forthesingle-crossingmodelsuchathingcannotoccur.
When we shift from the Interval model to the Square, Cube, and finally the
10-dimensionalHypercubeones,themapsbecomegraduallymoreandmoresim-
ilar to that for impartial culture. The same is true when we shift from Circle to
Sphere,etc. However,hypersphereelectionsconvergefastertowardimpartialcul-
turemodelthanhypercubeones. Forexample,even10-dimensionalHypercubeis
stillsomethinginbetweenN-Mal.0.5andimpartialculture.
Inasingle-peakedelectiontherearetwopossibleextremevotes,i.e.,oneiden-
ticalwiththesocietalaxis,andthesecondone,identicalwiththereversedsocietal
axis. For single-peaked models, we observe an interesting difference between the
Walsh and Conitzer approaches. For the Walsh model, the points are more uni-
formly spread, while for the Conitzer model, we obtain somewhat antagonistic
single-peaked elections. In fact, for Conitzer model, the probability of sampling
an extreme vote is 21 (for m > 1). Therefore, for the presented example ∼ 200
m
votes will be extreme ones (∼ 100 per each extreme). On the other hand, for the
Walshmodel,theprobabilityofsamplinganextremevoteis2−(m−2) (form > 1),
soforthepresentedexample∼ 0.4votewillbeanextremeone.
Although, the voters in an election from the Circle model have SPOC pref-
erences, votes from the Circle model are less evenly distributed than those from
the SPOC model. This is a similar case to that of Interval and single-crossing
elections.
For the balanced and caterpillar group-separable models, we see the divisions
of points into subgroups, which corresponds to the inner nodes of the trees. For
the group-separable caterpillar variant, they are spread across a larger space than
forthegroup-separablebalancedvariant.
333.5 Summary
In this chapter, we introduced some of the most popular statistical cultures that
are used in experiments in computational social choice. Next, we described four
compass elections: identity, uniformity, antagonism, and stratification. Finally,
using the map of preferences framework, we gave the reader the intuition about
howelectionsfromdifferentmodelslooklike.
34Chapter 4
Distances Among Elections
4.1 Introduction
How similar are two elections? In this chapter we suggest how one can go about
answering this question. We introduce the ELECTION ISOMORPHISM problem
and a family of its approximate variants, which measure the degree of similarity
betweentwoelectionsbyusingdistancesoverpreferenceorders.
In the ELECTION ISOMORPHISM problem we are given two elections, E
1
and E , both with the same numbers of candidates and the same numbers of
2
voters, and we ask if it is possible to transform one into the other by renaming
the candidates and reordering the voters. While this problem is similar in spirit
tothefamous GRAPH ISOMORPHISM problem(whosecomplexitystatusremains
elusive; see the report of Babai et al. [2015] and further discussion on Babai’s
home page for recent progress on the problem), the structure of elections with
ordinal ballots is such that it is very easy to provide a polynomial-time algorithm
for ELECTION ISOMORPHISM. On the other hand, for approval-based elections,
ELECTION ISOMORPHISMisatleastashardasGRAPH ISOMORPHISM—agraph
can be encoded as an approval election in a simple way. However, more details
abouttheapproval-basedelectionswillbegiveninChapter7.
We are also interested in approximate variants of the ELECTION ISOMOR-
PHISM problem, which turn out to define distances over elections. We extend the
distance between preference orders to whole elections in a way that respects both
anonymity and neutrality. Namely, we ask if, via appropriate renaming of the
candidatesandreorderingthevoters,itispossibletobringagivenelectionwithin
somesmalldistanceofanothergivenone. WenotethatapproximateGRAPH ISO-
35MORPHISM problemsarealsostudiedintheliterature[Arvindetal.,2012,Grohe
etal.,2018]. Although,inspirit,theyareverysimilartoourproblems,theydiffer
onthetechnicallevel.
We focus on three isomorphic distances (i.e., distances under which only iso-
morphicelectionsareatdistanceszero),thatis,theswap,Spearman,anddiscrete
distances. Unfortunately, both the swap and Spearman distances are quite com-
plex and take a lot of time to compute even for relatively small instances of elec-
tions. On the other hand, the discrete distance is faster, yet not very informative.
So, in one way or another, all three distances are of limited practical value when
comparing elections with, for example, 100 candidates and 100 voters. This con-
clusion leads to the development of various “nonisomorphic” distances. We call
them nonisomorphic because sometimes, even if two elections are not isomor-
phic, these distances might return zero. All our nonisomorphic distances instead
of operating on complete elections, work on their aggregate representations—
compressed forms of elections. It can be seen as a tradeoff, when we accept los-
ing some information about elections in exchange for a better performance with
regard to the running time. However, as we will show in Section 4.4, not for all
nonisomorphicdistancesthistradeoffpaysoff.
The structure of this chapter is as follows. First, we focus on ELECTION
ISOMORPHISM, and isomorphic distances. Second, we move to the aggregate
representations of elections and nonisomorphic distances based on these repre-
sentations. In these parts we largely focus on the complexity of computing our
distances. Then, we compare both isomorphic and nonisomorphic distances alto-
gether: We discuss the relation between compass elections (i.e., the four charac-
teristic elections which were initially presented in Chapter 3). Finally, we study
correlationbetweendistances,numbersofequivalenceclassesundereachofthem,
and(whatismostinteresting)wecomparethemapsthatourdistancesproduce.
4.2 Election Isomorphism
In this section we define the notion of election isomorphism, illustrate its useful-
ness, and show that testing if two elections are isomorphic is a polynomial-time
computabletask. Westartwithaformaldefinition.
Definition4.1. WesaythatelectionsE = (C,V)andE′ = (C′,V′),where|C| =
|C′|, V = (v ,...,v ), and V′ = (v′,...,v′), are isomorphic if there is a bijec-
1 n 1 n
tionσ: C → C′ andapermutationν ∈ S suchthatσ(v ) = v′ foralli ∈ [n].
n i ν(i)
36Example 4.1. Consider elections E = (C,V) and E′ = (C′,V′), such that C =
{a,b,c}, C′ = {x,y,z}, V = (v ,v ,v ), V′ = (v′,v′,v′), with the following
1 2 3 1 2 3
preferenceorders:
v : a ≻ b ≻ c, v′ : y ≻ x ≻ z,
1 1
v : b ≻ a ≻ c, v′ : x ≻ y ≻ z,
2 2
v : c ≻ a ≻ b, v′ : z ≻ x ≻ y.
3 3
E and E′ are isomorphic, by mapping candidates a to x, b to y, and c to z, and
votersv tov′,v tov′,andv tov′. △
1 2 2 1 3 3
The idea of election isomorphism has already appeared in the literature,
thoughwithoutusingthisnameandusuallyasatooltoachievesomespecificgoal.
For example, Eg˘eciog˘lu and Giritligil [2013] refer to two isomorphic elections as
membersofthesameanonymousandneutralequivalenceclass(ANEC)andstudy
theproblemofsamplingrepresentativesofANECsuniformlyatrandom.Hashemi
and Endriss [2014] use the election isomorphism idea in their analysis of prefer-
encediversityindices.
In the ELECTION ISOMORPHISM problem we are given two elections and we
askiftheyareisomorphic. Surprisingly,theproblemhasaneasypolynomial-time
algorithm.
Proposition4.1. ELECTION ISOMORPHISM isinP.
Proof. Let E = (C,V) and E′ = (C′,V′) be two input elections where C =
{c ,...,c }, C′ = {c′,...,c′ }, V = (v ,...,v ) and V = (v′,...,v′). With-
1 m 1 m 1 n 1 n
out loss of generality, let us assume that v ’s preference order is v : c ≻ c ≻
1 1 1 2
··· ≻ c . For each v′ there is a bijection σ from C to C′ such that for the pref-
m j j
erence order of v′ we have pos (σ (c )) = i. For each σ , we build a bipartite
j v′ j i j
j
graph where v ,...,v are the vertices on the left, v′,...,v′ are the vertices on
1 n 1 n
the right, and there is an edge between v and v if σ (v ) = v ; we accept if this
i ℓ j i ℓ
graphhasaperfectmatchingforsomeσ andwerejectotherwise.
j
The algorithm runs in polynomial time because there are n σ ’s to try, and
j
computing perfect matchings is a polynomial-time computable task. The correct-
ness follows from the fact that we need to map v to some vote in E′ and we try
1
allpossibilities.
Beforemovingtoisomorphicdistances,foramomentwewillstopanddiscuss
thesingle-peakedandsingle-crossingdomainsandtheirrelationtoisomorphism.
37MaximalDomains
As an extended example of the usefulness of the isomorphism idea, we consider
thesingle-peakedandsingle-crossingdomains. Theyreceivedextensiveattention
within (computational) social choice; we point the reader to the survey of Elkind
etal.[2022]formoredetails.
A single-peaked (single-crossing) domain is maximal if it is not contained in
any other single-peaked (single-crossing) domain. Each maximal single-peaked
domainD ⊆ L(C)contains2|C|−1 preferenceorders(Monjardet[2009]attributes
thisfacttoa1962workofKreweras). Sincewecanviewadomainasanelection
thatincludesasingle copyofeverypreferenceorderfromthe domain,ournotion
of isomorphism directly translates to the case of domains, and we can formalize
thefundamentaldifferencebetweensingle-peakednessandsingle-crossingness.
Proposition 4.2. Each two maximal single-peaked domains over candidate sets
ofthesamesizeareisomorphic.
Proof. ItsufficestonotethatifDandD′ aretwomaximalsingle-peakeddomains
(over candidate sets {x ,...,x } and {y ,...,y }, respectively), with axes >
1 m 1 m 1
and> ,suchthat:
2
x > ··· > x and y > ··· > y ,
1 1 1 m 1 2 2 m
then a bijection that maps each x to y witnesses that the two domains are iso-
i i
morphic.
According to Slinko et al. [2021], the number of maximal nonisomorphic
single-crossingdomainsisequivalenttothenumberofweakBruhatorders1.
Corollary 4.1. There are (m 2)!/ 1n−1·3n−2·····(2n−3)1 maximal single-crossing do-
mainsoverthesamesetofcandidatesthatarenotisomorphic.
Thismeansthatthereisasignificantdifferencebetweenthesingle-peakedand
single-crossingdomains.
4.3 Isomorphic Distances
We use the isomorphism idea to build distances between elections that respect
voter anonymity (so the order of the voters in an election is irrelevant) and candi-
date neutrality (so the names of the candidates are nothing more than temporary
identifiers).
1https://oeis.org/A005118
38We focus on the following three distances, swap, Spearman, and discrete,
whichweredescribedindetailinSection2.2.
Asareminder,byS ,wemeanthesetofallpermutationsover[n]. Moreover,
n
for two sets A, B of the same cardinality, by Π(A,B) we denote the set of all
one-to-onemappingsfromAtoB. Belowwegiveourmaindefinition.
Definition 4.2. Let d be a distance between preference orders. Let E = (C,V)
andE′ = (C′,V′)betwoelections,where|C| = |C′|,V = (v ,...,v )andV′ =
1 n
(v′,...,v′). Wedefinethed-isomorphismdistancebetweenE andE′ as:
1 n
n
(cid:88)
d(E,E′) = min min d(σ(v ),v′ )
i ν(i)
ν∈Snσ∈Π(C,C′)
i=1
We sometimes refer to the bijection σ as the candidate matching and to the
permutation ν as the voter matching, and sometimes instead of ν, we use bi-
jection τ ∈ Π(V,V′) (depending on what is more convenient). The name, d-
isomorphism distance, is justified by the fact that if d(E,E′) = 0 for some two
elections (and d is a metric over preference orders), then these elections are iso-
morphic.
Note that in the above definition, we view elections as both anonymous and
neutral. This is why we apply the minimum operator over all permutations of the
votersandoverallbijectionsbetweenthecandidates.
4.3.1 Computational Complexity
We now turn to the complexity of computing isomorphism distances. Formally,
ourproblemisdefinedasfollows.
Definition 4.3. Let d be a distance over preference orders. In the d-
ISOMORPHISM DISTANCE problem (the d-ID problem) we are given two elec-
tions, E = (C,V) and E′ = (C′,V′) such that |C| = |C′| and |V| = |V′|, and an
integerk. Weaskifd(E,E′) ≤ k.
We are also interested in two variants of this problem, the d-ID WITH CAN-
DIDATE MATCHING problem, where the bijection σ between the candidate sets
is given (and fixed), and the d-ID WITH VOTER MATCHING problem, where
the voter permutation ν is given (and fixed). The former problem is in P for
polynomial-timecomputabledistances,but,aswewillseelater,thisisnotalways
trueforthelatter.
39WITH VOTER WITH CANDIDATE
d d-ID MATCHING MATCHING
d P P P
disc
d NP-complete† P P
Spear
d NP-complete NP-complete P
swap
Table 4.1: The complexity of computing isomorphic distances. † this result is not
acontributionofthisdissertation.
The summary of results is presented in Table 4.1. Now, we will move on to
analyzingallninevariants.
Proposition 4.3. For a polynomial-time computable d, the problem d-ID WITH
CANDIDATE MATCHING isinP.
Proof. Let E and E′ be our input elections and let σ be the input matching be-
tween candidates from E and E′. To compute the distance between elections,
it suffices to do the following. First, compute a distance between every pair of
votes(onefromσ(E)andanotherfromE′),Then,buildacorrespondingbipartite
graph,whereverticesontheleftarethevotersfromσ(E),theverticesontheright
arethevotersfromE′,andallpossibleedgesexist,weightedbythedistancesbe-
tween the votes they connect. Finally, find the smallest-weight matching. The
weight of the matching gives the value of the distance, and the matching itself
givesthepermutationν).
Using an argument very similar to that in the proof of Proposition 4.1, we
showthatd -IDproblemisinP.
disc
Proposition4.4. Thed -IDproblemisinP.
disc
Proof. Given two elections E = (C,V) and E′ = (C′,V′), where |C| =
|C′|, V = (v ,...,v ) and V′ = (v′,...,v′), for each pair of votes (v ,v′) we
1 n 1 n i j
constructamappingσ : C → C′sothatpos (c) = pos (σ (c))foreachc ∈ C.
ij vi v j′ ij
Wechooseσ thatleadstothesmallestd distance(wecomputethesedistances
ij disc
usingthed disc-ID WITH CANDIDATE MATCHING problem).
The correctness of the algorithm follows from the observation that the largest
possible value of d (E,E′) is n − 1; we can always ensure that at least one
disc
vote from E matches perfectly a vote from E′. Thus, there must be two votes for
whichσ istheoptimalcandidatematching.
ij
40Usingthesamereasoningasabove,wecanalsoeasilyshowthefollowing.
Corollary4.2. Thed disc-ID WITH VOTER MATCHING problemisinP.
The elections for which the d distance is small are, in fact, nearly identical
disc
(up to renaming of the candidates and reordering the voters). In consequence,
we do not expect such elections to frequently appear in real-life (for example, for
twoelectionswithnvotersandarelativelylargenumberofcandidates,generated
according to the impartial culture model, we would expect their d distance to
disc
typically be n − 1). Thus, we need more fine-grained distances, such as d
swap
andd . Unfortunately,theyareNP-hardtocomputeand,indeed,ford we
Spear swap
inheritthisresultfromtheKemenyrule.
The d -ID problem generalizes the problem of finding a Kemeny ranking
swap
(roughly speaking, to find a Kemeny ranking for a given election, it suffices to
find the smallest swap-based isomorphism distance between this election and a
“constant”one,whereallthevotersreportidenticalpreferenceorders).
Proposition 4.5. The d -ID problem is NP-complete, even for elections with
swap
fourvoters.
Proof. MembershipinNPiseasytosee. Wegiveareductionfromthe KEMENY
SCORE problem. In the KEMENY SCORE problem we are given an election E =
(C,V) and an integer k, and we ask if there exists a preference order p over C
(cid:80)
such that d (v,p) ≤ k. The problem is NP-complete [Bartholdi et al.,
v∈V swap
1989] and remains NP-complete even for the case of four voters [Dwork et al.,
2001]. We reduce it to the d -ID problem in a straightforward way: Given
swap
electionE = (C,V)andk,ourreductionoutputselectionE,anewlyconstructed
election E′ = (C′,V′), and an integer k, where C′ = {c′,...,c′ } and every
1 |C|
voterinV′ hasidenticalpreferenceorderv′ : c′ ≻ ··· ≻ c′ .
1 |C|
The reduction runs in polynomial time. Let us now argue that it
is correct. Let V = (v ,...,v ) and let V′ consist of n copies
1 n
of v′. We note that d (E,E′) = min (cid:80)n d (σ(v ),v′) =
swap σ∈Π(C,C′) i=1 swap i
min (cid:80)n d (v ,σ′(v′)), which is at most k if and only if there ex-
σ′∈Π(C′,C) i=1 swap i
(cid:80)
istsapreferenceorderp ∈ L(C)suchthat d (v,p) ≤ k.
v∈V swap
Since the above reduction works even for elections with four voters, having a
matching between the voters cannot make the problem simpler (this also follows
fromthefactthatinourreductiononeelectionconsistsofidenticalvotes).
Corollary4.3. d swap-ID WITH VOTER MATCHING isNP-complete.
41The situation for d -ID is somewhat different. In this case Litvak’s rule
Spear
[Litvak, 1983], defined analogously to the Kemeny rule, but for the Spearman
distance, is polynomial-time computable [Dwork et al., 2001] and we can lift
this result to the case of d Spear-ID WITH VOTER MATCHING. Without the voter
matching,d -IDisNP-complete.
Spear
Proposition4.6. d Spear-ID WITH VOTER MATCHING isinP.
Proof. Let E = (C,V) and E′ = (C′,V′) be two elections, where |C| =
|C′|, V = (v ,...,v ), and V′ = (v′,...,v′), and let ν ∈ S be the given
1 n 1 n n
voter matching. For a bijection σ: C → C′, the Spearman distance between E
andE′ is(cid:80)n d (σ(v ),v′ ),whichis:
i=1 Spear i ν(i)
n
(cid:88) (cid:88)
|pos (σ−1(c′))−pos (c′)|.
vi v ν′
(i)
c′∈C′ i=1
In consequence, the cost induced by matching candidates c ∈ C and c′ ∈ C′
is cost(c,c′) = (cid:80)n |pos (c) − pos (c′)|. To solve our problem, it suffices
i=1 vi v ν′
(i)
to find a minimum cost perfect matching in a bipartite graph where candidates
fromC aretheverticesontheleft,candidatefromC′ aretheverticesontheright,
and for each c ∈ C and c′ ∈ C′ we have an edge from c to c′ with cost cost(c,c′).
ThefinalmissingresultwasprovedbyFaliszewskietal.[2019]
Theorem4.1(Faliszewskietal.[2019]). Thed -ID problemisNP-complete.
Spear
4.3.2 ILP
Weprovideintegerlinearprograms(ILPs)forcomputingd andd .
Spear swap
Proposition4.7. ThereisanILPford .
Spear
Proof. Let E = (C,V) and E′ = (C′,V′) be the elections we wish to compute
the distance for, with C = {c ,...,c }, C′ = {c′,...,c′ }, V = {v ,...,v },
1 m 1 m 1 n
and V′ = {v′,...,v′}. For each k,k′ ∈ [n], we define a binary variable N
1 n k,k′
with the intention that value 1 indicates that voter v is matched to voter v′ .
k k′
Similarly, for each i,i′ ∈ [m], we define a binary variable M with the intention
i,i′
that value 1 means that candidate c is matched to candidate c′ . For each k,k′ ∈
i i′
42[n] and each i,i′ ∈ [m], we define a binary variable P with the intention
k,k′,i,i′
thatP = N ·M . Weintroducethefollowingconstraints:
k,k′,i,i′ k,k′ i,i′
(cid:80) N = 1,∀k ∈ [n]; (cid:80) N = 1,∀k′ ∈ [n] (4.1)
k′∈[n] k,k′ k∈[n] k,k′
(cid:80) M = 1,∀i ∈ [m]; (cid:80) M = 1,∀i′ ∈ [m] (4.2)
i′∈[m] i,i′ i∈[m] i,i′
(cid:80)
P = 1, ∀i ∈ [m],k ∈ [n] (4.3)
k′∈[n],i′∈[m] k,k′,i,i′
(cid:80) P = 1, ∀i′ ∈ [m],k′ ∈ [n] (4.4)
k∈[n],i∈[m] k,k′,i,i′
P ≤ N , ∀i,i′ ∈ [m],k,k′ ∈ [n] (4.5)
k,k′,i,i′ k,k′
P ≤ M , ∀i,i′ ∈ [m],k,k′ ∈ [n] (4.6)
k,k′,i,i′ i,i′
Constraints (4.1) and (4.2) ensure that variables N and M describe match-
k,k′ i,i′
ings between voters and candidates, respectively. Constraints (4.3)–(4.6) imple-
mentthesemanticsoftheP variables(theformertwoensurethatforagiven
k,k′,i,i′
vote/candidate pair, there is exactly one vote/candidate pair in the other election
to which they are matched; the latter two ensure connection between the P
k,k′,i,i′
variables and the N and M variables). The optimization goal is to mini-
k,k′ i,i′
mize(cid:80) P ·|pos (c )−pos (c′ )|(which,forvaluesP
k,k′∈[n],i,i′∈[m] k,k′,i,i′ v
k
i v k′
′
i′ k,k′,i,i′
that satisfy the constraints of the program, defines the Spearman distance for the
givenmatchings). Values|pos (c )−pos (c′ )|areprecomputed.
v
k
i v k′
′
i′
Proposition4.8. ThereisanILPford .
swap
Proof. Theproofford isverysimilartotheoneford . Ford weneed
swap Spear swap
all the constraints presented for d and three more, so, we focus only on the
Spear
additional ones. For each k,k′ ∈ [n] and each i,i′,j,j′ ∈ [m], we define a binary
variableR withtheintentionthatR = N ·M ·M . Note
k,k′,i,i′,j,j′ k,k′,i,i′,j,j′ k,k′ i,i′ j,j′
that, we assume that i < j, i′ ̸= j′. For d it suffices to have four indices (two
Spear
for voters and two for candidates), because to compute the Spearman distance
between two matched votes we only need to iterate over each pair of matched
candidates. However, for the d we need six indices (two for voters and four
swap
for candidates), because to compute the swap distances between two matched
votesweneedtoiterateovereachpairofpairsofcandidates.
Weintroducethefollowingconstraints:
R ≤ P , ∀i′,j′∈[m] ∈ [m],k,k′ ∈ [n] (4.7)
k,k′,i,i′,j,j′ k,k′,i,i′ i<j,i′̸=j′
R ≤ P , ∀i′,j′∈[m] ∈ [m],k,k′ ∈ [n] (4.8)
k,k′,i,i′,j,j′ k,k′,j,j′ i<j,i′̸=j′
(cid:80)
R =
n·(cid:0)m(cid:1)
(4.9)
k,k′∈[n],i′,j′∈[m] k,k′,i,i′,j,j′ 2
i<j,i′̸=j′
43Constraints (4.7) and (4.8) ensure that R can be true only if P
k,k′,i,i′,j,j′ k,k′,i,i′
and P are true. And constraints (4.9) ensure that proper number of R vari-
k,k′,j,j′
ablesareequaltoone.
Theoptimizationgoalistominimize:

1ifpos (c ) > pos (c′)
 

v k i v k′ ′ j
(cid:80)
  

andpos v k(c i′) < pos v k′ ′(c′ j′)
R × 1ifpos (c ) < pos (c′)
k,k′∈[n], ii <′,j j′ ,i∈ ′̸=[m j]
′
k,k′,i,i′,j,j′
   


andpov k
s v
ki
(c i′) >
pv ok′ s′
v k′
′j
(c′ j′)
0otherwise
whereposvaluesareprecomputed.
WhiletheILPsdescribedabovefindoptimalsolutions,theycanbequiteslow
to solve for any but the smallest instances. Thus, in practice when we want to
compute particular distances, instead of ILPs, we have to use a brute-force (BF)
algorithm.
Comparison
To compare ILP and BF approaches, we conducted a simple experiment in which
we computed the Spearman and swap distances for small numbers of candidates
and voters and compared the time needed to find the optimal solution. For the
Spearman distance, we use elections with 3,...,8 candidates and 8 voters, while
for the swap distances, we use elections with 3,4,5 candidates and 5 voters. The
results are presented in Table 4.2 (for Spearman) and Table 4.3 (for swap). In
each cell, we have the average time (in seconds) needed to compute a single dis-
tancebetweentworandomimpartialcultureelectionsusingILPandBF.Thepre-
sentedvaluesareaveragesover1000iterations. Thedifferencesareextreme,with
BF approach being, literally speaking, thousands times faster. When computing
the swap distance with 5 candidates and 5 voters, the BF approach was more
than300000timesfasterthantheILPapproach.
4.3.3 Visualization of the Distances
In this section we present "cross maps" of preferences, a similar experiment to
the one presented in Section 3.4. The main difference is that, in Section 3.4 on
each single picture we presented one election, while now on each single picture
wepresenttwoelectionsembeddedjointly.
44Method 3 4 5 6 7 8
ILP 0.12s 0.61s 1.69s 5.49s 15.86s 42.82s
BF < 0.01s < 0.01s < 0.01s < 0.01s 0.01s 0.05s
Table 4.2: Time needed to compute the Spearman distance between two random
impartial culture instances. All the values are presented in seconds. We consider
electionswith3,...,8candidatesand8voters.
Method 3 4 5
ILP 0.55s 12.15s 97.54s
BF < 0.01s < 0.01s < 0.01s
Table 4.3: Time needed to compute the swap distance between two random im-
partial culture instances. All the values are presented in seconds. We consider
electionswith3,4,5candidatesand5voters.
Given two elections, we compute the mapping between the candidates from
these elections, such that it minimizes the swap distance between them. Next,
given the mapping, we proceed as for previous maps of preferences and simply
computetheswapdistancebetweeneachpairofvotesfrombothelections.
In Figure 4.1 we present cross maps for the eight following models: impartial
culture, antagonism, the Norm-Mallows model with norm-ϕ = 0.2, the 0.25-
Norm-Mallows model with norm-ϕ = 0.2, the urn model with α = 0.2, and
SPOC. We generated 16 elections (two from each model). Eight of them are as
columns (red ones), and the other eight of them are as rows (blue ones). When
presented jointly, all red points represent the column election and all blue points
representtherowone.
We analyze the results row by row. The votes from impartial culture occupy
the whole space, and hence other models, when combined with it, should look
similartohowtheylookalone. Weobservethisforallinstanceswiththeexception
forthosefrom0.25-Norm-Mallowsmodel,whichareshiftedtowardstheedge.
In the next row we have antagonism, which is slightly “squeezing” all other
instances. As expected, when combined with 0.25-Norm-Mallows, two extreme
AN votes match the centers of two mallows groups. In the following discussion,
werefertothesegroupsasthesmallergroupandthelargergroup.
At the diagonal, we have pairs of elections from the same model embedded
45Figure4.1: Crossmapsofpreferences(10candidates,2×500voters).
46Figure4.2: Crossmapsofpreferences(10candidates,2×500voters).
47jointly. Interestingly, most of the elections, when embedded jointly with another
election from the same model, produce a very similar picture to those, when they
areembeddedseparately. Moreover,manypointsfrombothelectionsoverlap. We
canseethismostclearlyfortheANelections. WhenweembedtwoANelections,
they fully overlap (which should not be surprising because they are isomorphic).
Similarly, if we look, for example, at two SPOC elections embedded jointly, they
also strongly overlap. However, it is not the case for all the models, for example,
two elections from the urn model are relatively independent of each other, and
pointsfrombothelectionsoccupyquitedifferentplacesinthepicture.
Then we have SPOC, which is maintaining its circular shape in all pictures.
However,sometimeswhencombinedwithNorm-Mallows,itisgettinglesssharp.
WhenNorm-Mallowsiscombinedwith0.25-Norm-Mallows,itisplacingitscen-
teroverthelargergroupof0.25-Norm-Mallows.
In Figure 4.2 we present another set of cross maps, for the eight following
models: impartial culture, 1D Interval, single-crossing, single-peaked by Walsh,
single-peakedbyConitzer,andGSCaterpillar.
The single-crossing, Walsh, Conitzer, and 1D Interval models all have the
sameoblongshape. Interestingly,whenembeddingjointlyWalshandConitzeror
1D Interval elections, the oblong shapes are put together nicely, i.e., one over the
other. Nonetheless, when single-crossing elections were embedded jointly with
otherlongitudinalinstances,theyformacrossing-overshape.
Caterpillar group-separable elections are changing a lot depending on what
other instances they are embedded with. When combined with impartial culture,
they present a very similar shape to the one when embedded alone. However,
when embedded jointly with an oblong-shaped instance, they disperse signifi-
cantly.
MainConclusions
• Presented maps of preferences confirm the general intuition behind
different statistical cultures and the relations between them. Later
on, when we see the map of elections in Figure 4.3, if two elections
are similar (i.e., their red and blue points are close to covering each
otherinFigures4.1and4.2)then,indeed,theyarenexttoeachother
onthemap.
48Model NumberofElections
ImpartialCulture 20
Urn 60
Mallows 60
Group-Separable(Balanced) 20
Group-Separable(Caterpillar) 20
Single-Peaked(Conitzer) 20
Single-Peaked(Walsh) 20
SPOC(Conitzer) 20
Single-Crossing 20
Interval 20
Disc 20
Cube 20
Circle 20
Compass(ID,AN,UN,ST) 4
Table4.4: Listofselectedstatisticalculturesandnumbersofelectionsfromthese
culturesaccordingly.
4.3.4 Isomorphic Maps of Elections
Next we present our first maps of elections. We start with the description of the
concept, and later we present the technical details. To build a map, we proceed
as follows. First, we generate a number of instances of elections. Second, we
computeacertaindistancebetweeneachpairofthem. Third,weembedthesedis-
tancesinatwo-dimensionalEuclideanspace. Voilà,weobtainamapofelections.
Now,wewillgooverthesethreestepswithmoretechnicaldetails.
We assembled a number of elections generated using statistical cultures
from Chapter 3 and four compass elections that capture four different types of
(dis)agreement among voters, identity, uniformity, antagonism, and stratification
(seeChapter3). Weexpectgoodmetricstoputthesecompasselectionsfarapart.
We list the exact distributions, and numbers of generated elections used in
the map in Table 4.4. All in all, we generated 340 + 4 elections, each with 10
candidates and 50 voters, some from very popular statistical cultures, and some
fromlesstypicalones,suchastheSPOC;plusfourcompasselections.
49(a)Swap (b)Spearman (c)Discrete
Figure4.3: Theaveragedistancesbetweenelectionsfromgivencultures(normal-
izedbythelargestdistance).
Regarding the parameters, for the Norm-Mallows model we choose norm-ϕ
uniformly at random. For the urn model, we choose α according to the Gamma
distribution2 with shape parameter k = 0.8 and scale parameter θ = 1 (we will
discussthisindetailinChapter5). ForallEuclideanmodels,wesampletheideal
pointsofcandidatesandvotersuniformlyatrandomfrom:
• [0,1]intervalfortheIntervalmodel;
• discwithradiusr = 0.5andcenterin(0,0)fortheDiscmodel;
• [0,1]3 cubefortheCubemodel;
• circlewithradiusr = 0.5andcenterin(0,0)fortheCirclemodel.
As a second step, we computed the swap (Spearman/discrete) distance be-
tweeneachpairofthegeneratedelections. Foreachsetofelections,wegivetheir
average distance to the elections from the other sets (or to the elections within
the set, on the diagonal), normalized by the largest distance. We show statistics
regarding(someof)thesedistancesinFigure4.3.
With the concrete values of the swap (Spearman/discrete) distances in hand,
we computed a mapping of the generated elections to a 2D space, so that the
Euclidean distances between the points in this mapping reflect the original dis-
tancesbetweentheelections. Tocomputetheembedding,weusedavariantofthe
2Popularprobabilitydistributionparametrizedbyshapeandscaleparameters.
50(a)Swap (b)Spearman
(c)Discrete
Figure4.4: Mapsofelectionsbasedonisomorphicdistances.
Kamada-Kawaialgorithm, recentlyproposed bySapała[2022], whichis basedon
the work of Kamada and Kawai [1989]. More details about the different embed-
dingalgorithmsarepresentedinChapter5.
We present the visualization we obtained for this embeddings in Figure 4.4
and refer to them as our maps of elections. We first focus on the left map, which
is based on the swap distances. Three compass elections, i.e., identity, unifor-
mity,andantagonism,formatrianglethatisalmostequilateralandroughlylimits
the space. At the bottom, we see elections from the Norm-Mallows model3 that
form a path from the identity to the uniformity elections. The higher the norm-ϕ
3FortheNorm-Mallowselections,thelargeristhetransparencyofagivenpoint,thesmalleris
thenorm-ϕvalue.
51parameter, the closer we get to the impartial culture. Elections from the Pólya-
Eggenberger urn model4 are distributed over a large area, in comparison to elec-
tions from other models; in principle, the larger the α parameter, the closer they
are to the identity, however, unlike for the Norm-Mallows model, two urn elec-
tionswiththesameα parametercanbeverydifferentfromoneanother.
What is surprising is that the way of sampling single-peaked elections is
stronglyinfluencingtheirlocationonthemap. ElectionsfromtheConitzermodel
arenotthatclosetoelectionsfromtheWalshone. However,theyareverycloseto
elections from the Interval model. Similarly, the elections from the SPOC model
lie next to those from the Circle model. For Euclidean elections, the higher the
dimension,theclosertheyaretotheimpartialcultureones.
Almost the whole left upper part, that is, the part between the identity and
antagonism, is occupied by elections where, on average, all candidates have very
similar Borda scores5. In other words, all the candidates perform similarly. We
callthisparttheBordabalancearea,andwewillreturntoitinSection5.4.1.
Now, if we look at the map that is based on the Spearman distances, we see
that it is very similar to the one based on the swap distances. At the same time,
the map created based on the discrete distance is clearly different. We will not
exaggerateifwesaythatthediscretemapisoflimitedusefulness.
In Figure 4.5 we present the correlation plots for our isomorphic distances.
Eachpurpledotrepresentsthedistancebetweenapairofelections. Aswecansee,
the swap and Spearman distances are very strongly correlated (having Pearson
correlation coefficient 0.99), while the swap and discrete distances are vaguely
correlated(havingPearsoncorrelationcoefficientequalto0.33).
While the map based on the discrete distance is not very appealing, the maps
based on the swap and Spearman distances give us an interesting insight into the
space of statistical cultures. Unfortunately, the computations of these distances,
even for instances with only 10 candidates and 50 voters, are quite demanding.
Due to this fact, if we wanted a map with a larger number of candidates, such
as 20 or 100, we would need a new distance, which could be computed faster.
Andthatiswhatwearegoingtodiscussinthenextsection. Moreover,duetothe
very strong similarity between the swap and Spearman distances, in the later part
ofthisdissertationwefocusonlyontheswapdistance.
4We mark the urn elections with small, medium, and large α parameters by yellow, orange,
andredcolors,respectively
5Inparticular,standarddeviationforthecaterpillargroup-separable,balancedgroup-separable,
SPOC, IC and Sphere elections on average equals 18.45, while for other elections on average it
equals76.71,i.e.,fourtimesmore.
52(a)SwapvsSpearman (b)SwapvsDiscrete
Figure4.5: Correlationbetweentheisomorphicdistances.
MainConclusions
• TheSpearmandistanceisverystronglycorrelatedwiththeswapdis-
tance.
• Elections from the same statistical culture tend to lie next to each
otheronthemaps.
4.4 Nonisomorphic Distances
Next, we introduce several nonisomorphic distances. For each of these distances,
westartbygivingitsformaldefinition,then,weshowthatitisapsuedometric,and
finally we discuss its computational complexity. Before that, we present several
aggregaterepresentationsofelections,whicharenothingelsebutsimplifiedforms
ofelections,andwhichwillbeusefulfordefiningourdistances.
4.4.1 Aggregate Representations
Let E = (C,V) be an election with C = {c ,...,c } and V = (v ,...,v ).
1 m 1 n
Below,wepresentthefollowingaggregaterepresentationsofE:
WeightedMajorityRelation. For each two candidates c,d ∈ C, M (c,d) is
E
thenumberofvotersthatpreferctodinelectionE. Wecallittheweighted
53majority relation and represent it as an m × m matrix where rows and
columns correspond to the candidates (the diagonal is undefined). A rel-
ativeweightedmajorityrelationisaweightedmajorityrelationfromwhose
entrieswesubtractn/ 2.
PositionMatrix. For a candidate c ∈ C and a position i ∈ [m], P (c,i)
E
is the number of voters from E that rank c on position i; P (c) =
E
(P (c,1),...,P (c,m))isa(column)positionvectorofc. WeviewP as
E E E
amatrixwithcolumnsP (c ),...,P (c )andcallitapositionmatrix.
E 1 E m
BordaScoreVector. For a candidate c ∈ C, B (c) =
(cid:80)n (cid:0)
m − pos
(c)(cid:1)
is
E i=1 vi
the Borda score of c in E. Then B = (B (c ),...,B (c )) is the Borda
E E 1 E m
scorevector,whoseentriescorrespondtothecandidates.
Note that in each of these aggregate representations, we are losing certain
information about the election, i.e., there may be two distinct elections that have
thesameaggregaterepresentation. Next,weprovideasimpleexamplethatshows
howtheseaggregaterepresentationslookinpractice.
Example 4.2. Consider an election E = (C,V), where C = {a,b,c}, V =
(v ,v ,v ,v ),andthevotesare:
1 2 3 4
v : a ≻ b ≻ c,
1
v : b ≻ c ≻ a,
2
v : b ≻ a ≻ c,
3
v : c ≻ a ≻ b.
4
AggregaterepresentationsM ,P ,andB forelectionE areasfollows:
E E E
a b c a b c
   
a − 2 2 1 1 2 1 a b c
(cid:2) (cid:3)
M E = b 2 − 3 , P E = 2 2 1 1 , B E = 4 5 3 .
c 2 1 − 3 1 1 2
△
By a realization of an aggregated representation, we refer to an election that
hasagivenaggregatedrepresentation.
544.4.2 Positionwise Distance
The first nonisomorphic distance that we will discuss is based on analyzing how
frequently the candidates are ranked at particular positions, and we call it the po-
sitionwise distance. (This distance is based on the earth mover’s distance (EMD)
introducedinChapter2,andonpositionmatrices.) Thedefinitionisasfollows.
Definition 4.4. Let E = (C ,V ) and E = (C ,V ) be two elections such
1 1 1 2 2 2
that |C | = |C |. For a bijection δ: C → C , we define δ-d (E ,E ) =
1 2 1 2 pos 1 2
(cid:80)
emd(P (c ),P (c )). The positionwise distance between elections E
c∈C1 E 1 E 2 1
andE ,d (E ,E ),istheminimumoftheδ-d (E ,E )values,takenoverδ.
2 pos 1 2 pos 1 2
We use earth mover’s distance in Definition 4.4 because it captures the idea
thatbeingrankedonthetoppositionismoresimilartobeingrankedonthesecond
position than to being ranked on the bottom one. Alternately, instead of using
EMD, one can use, e.g., the ℓ distance. By d we refer to EMD-positionwise
1 pos
distances (which we treat as the default variant) and by dℓ1 we refer to the ℓ -
pos 1
positionwisedistance,thevariantofthedistancewherewereplaceEMDwithℓ .
1
Example 4.3. Consider two elections, E and E , over candidate sets C =
1 2 1
{a,b,c}andC = {x,y,z}. ElectionE containsvotersv ,v ,v andelectionE
2 1 1 2 3 2
containsvotersu ,u ,u :
1 2 3
v : a ≻ b ≻ c, v : b ≻ a ≻ c, v : b ≻ c ≻ a,
1 2 3
u : x ≻ y ≻ z, u : z ≻ x ≻ y, u : y ≻ x ≻ z.
1 2 3
The vectors (i.e., columns in the position matrix) associated with each of our
candidatesareasfollows:
P (a) = (1,1,1), P (b) = (2,1,0), P (c) = (0,1,2),
E1 E1 E1
P (x) = (1,2,0), P (y) = (1,1,1), P (z) = (1,0,2).
E2 E2 E2
We see that emd(P (a),P (y)) = 0, emd(P (b),P (x)) = 1 because to
E1 E2 E1 E2
transform P (b) into P (x), we need to move value 1 from the first position
E1 E2
to the second one (so we multiply 1 by 1), and emd(P (c),P (z)) = 1. Thus
E1 E2
for δ(a) = y, δ(b) = x, and δ(c) = z we have δ-d (E ,E ) = 2 and, in
pos 1 2
fact,d (E ,E ) = 2. △
pos 1 2
The positionwise distance is not a metric, because the distance between two
nonisomorphicelectionscanbezero. However,itisapseudometric.
55Proposition4.9. Thepositionwisedistanceisapseudometric.
Proof. We show that the positionwise distance satisfies the triangle inequality
(the other requirements for being a pseudometric are easy to verify). Consider
three elections with candidate sets of equal size, E = (C ,V ), E = (C ,V ),
1 1 1 2 2 2
and E = (C ,V ). Let δ and σ be the permutations of the candidates that mini-
3 3 3
mize the distances between E and E and between E and E , respectively. We
1 2 2 3
havethat:
(cid:80)
d (E ,E ) = emd(P (c),P (σ(δ(c)))
pos 1 3 c∈C1 E1 E3
(cid:80)
≤ emd(P (c),P (δ(c))
c∈C1 E1 E2
(cid:80)
+ emd(P (δ(c)),P (σ(δ(c)))
c∈C2 E1 E2
= d (E ,E )+d (E ,E ).
pos 1 2 pos 2 3
The first inequality follows from the definition of the positionwise distance, the
secondone—fromthefactthatEMDisametric.
One of the advantages of the positionwise distance is the fact that it can be
computedinpolynomial-time.
Proposition 4.10. There exists a polynomial-time algorithm for computing the
positionwisedistance.
Proof. Let E = (C ,V ) and E = (C ,V ) be two elections where |C | =
1 1 1 2 2 2 1
|C |. The value of d (E ,E ) is equal to the minimum-cost matching in the
2 pos 1 2
bipartite graph whose vertex set is C ∪ C and which has the following edges:
1 2
For each c ∈ C and each c ∈ C there is an edge with the cost equal to
1 1 2 2
the EMD between c ’s and c ’s candidate distribution vectors (these weights can
1 2
be computed independently for each pair of candidates). Such minimum-cost
matchingscanbecomputedinpolynomialtime.
While computing a position matrix of an election is straightforward, the re-
verse direction is less clear. We observe that each m × m position matrix has
a corresponding m-candidate election with at most m2 − 2m + 2 distinct pref-
erence orders. This was shown by Leep and Myerson [1999, Theorem 7] (they
speak of “semi-magic squares” and not “position matrices” and show a decom-
position of a matrix into permutation matrices, which correspond to votes in our
setting). Inotherwords,givenapositionmatrixwecancomputeitsrealizationin
polynomial-time.
Observation4.1. GivenapositionmatrixX,onecancomputeinO(m4.5)timean
electionE thatcontainsatmostm2−2m+2differentvotessuchthatP(E) = X.
564.4.3 Pairwise Distance
Next,wedefinethepairwisedistance,whichisinspiredbytheclassofCondorcet-
consistentvotingrulesandreliesonanalyzingtheresultsofhead-to-headmajority
contestsbetweenthecandidates.
Definition 4.5. Let E = (C ,V ) and E = (C ,V ) be two elections such
1 1 1 2 2 2
that |C | = |C |. For a bijection δ: C → C , we define δ-d (E ,E ) =
1 2 1 2 pair 1 2
(cid:80) (cid:12) (cid:12)
(cid:12)M (c,d) − M (δ(c),δ(d))(cid:12). The pairwise distance between
(c,d)∈C1×C1 E1 E2
elections E and E , d (E ,E ), is the minimum value of the δ-d (E ,E )
1 2 pair 1 2 pair 1 2
values,takenoverδ.
Example 4.4. Let us consider the two elections from Example 4.3. Weighted
majorityrelationslookasfollows:
a b c x y z
   
− 1 2 − 2 2
a x
M E1 = b  2 − 3  M E2 = y  1 − 2 
1 0 − 1 1 −
c z
Forδ(a) = y,δ(b) = x,andδ(c) = z,theδ-d (E ,E ) = 2,andthisisalsothe
pair 1 2
valueofd (E ,E ). △
pair 1 2
Note that using EMD for the pairwise distance would not be very useful, be-
cause each value in the matrix is in some sense independent of the values sur-
roundingit.
Similarlytothepositionwisedistance,pairwisedistanceisapseudometric.
Proposition4.11. Thepairwisedistanceisapseudometric.
Proof. Clearly, the pairwise distance is symmetric, and for each election E it
holds that d (E,E) = 0. The triangle inequality follows by the same reason-
pair
ing as in the case of the positionwise distance. In particular, we define δ and σ
analogouslyasinthatproof. Then:
57(cid:88) (cid:12) (cid:12)
d (E ,E ) ≤ (cid:12)M (c,d)−M (σ(δ(c)),σ(δ(d))(cid:12)
pair 1 3 E1 E3
(c,d)∈C1×C1
(cid:88) (cid:12) (cid:12)
≤ (cid:12)M (δ(c),δ(d))−M (σ(δ(c)),σ(δ(d))(cid:12)
E2 E3
(c,d)∈C1×C1
(cid:88) (cid:12) (cid:12)
+ (cid:12)M (c,d)−M (δ(c),δ(d)(cid:12)
E1 E2
(c,d)∈C1×C1
= d (E ,E )+d (E ,E ).
pair 1 2 pair 2 3
Thiscompletestheproof.
Both the positionwise distance and the pairwise distance satisfy our minimal
requirements; they both are pseudometrics defined to be neutral/anonymous. Yet,
we can compute the positionwise distances in polynomial-time, but the pairwise
distance is intractable (indeed, it is similar to the NP-complete APPROXIMATE
GRAPH ISOMORPHISM problem[Arvindetal.,2012,Groheetal.,2018]).
Proposition 4.12 (Szufa et al. [2020]). The decision variant of the problem of
computingthepairwisedistanceisNP-complete.
Nonetheless, we can compute the pairwise distance by formulating it as an
integer linear program. In practice, this allows us to compute distances between
electionsofuptoaround20candidates.
Proposition4.13. ThereisanILPford .
pair
Proof. Let E = (C,V) and E′ = (C′,V′) be the elections we wish to compute
the distance for, with C = {c ,...,c }, C′ = {c′,...,c′ }, V = {v ,...,v },
1 m 1 m 1 n
andV′ = {v′,...,v′}. Foreachi,i′ ∈ [m],wedefineabinaryvariableM with
1 n i,i′
the intention that value 1 indicates that candidate c is matched to candidate c′ .
i i′
For each i,i′,j,j′ ∈ [m],i ̸= j,i′ ̸= j′, we define a binary variable P with
i,i′,j,j′
58theintentionthatP = M ·M . Weintroducethefollowingconstraints:
i,i′,j,j′ i,i′ j,j′
(cid:80)
M = 1,∀i ∈ [m]; (4.10)
i′∈[m] i,i′
(cid:80) M = 1,∀i′ ∈ [m]; (4.11)
i∈[m] i,i′
(cid:80)
P = 1,∀i,j ∈ [m]; (4.12)
i′,j′∈[m] i,i′,j,j′
i̸=j,i′̸=j′
(cid:80) P = 1,∀i′,j′ ∈ [m]; (4.13)
i′,j′∈[m] i,i′,j,j′
i̸=j,i′̸=j′
P ≤ M
,∀i,i′,j,j′∈[m];
(4.14)
i,i′,j,j′ i,j i̸=j,i′̸=j′
P ≤ M
,∀i,i′,j,j′∈[m].
(4.15)
i,i′,j,j′ k,l i̸=j,i′̸=j′
Constraints (4.10) and (4.11) ensure that variables M describe matchings be-
i,j
tween the candidates. Constraints (4.12)–(4.15) implement the semantics of
the P variables (the former two ensure that there is one-to-one matching
i,i′,j,j′
between pairs of candidates; the latter two ensure connection between the P
i,i′,j,j′
variablesandtheM andM variables).
i,i′ j,j′
Theoptimizationgoalistominimize:
(cid:80) P ·|M (i,j)−M (i′,j′)|.
i,i′,j,j′∈[m] i,i′,j,j′ E E′
i̸=j,i′̸=j′
ValuesM (i,j)andM (i′,j′)areprecomputed.
E E′
Unlike for the positionwise distance, for the pairwise distance it is hard to
recoveranelectionwithagivenweightedmajorityrelation.
Theorem 4.2 (Boehmer et al. [2022c]). Given an m × m matrix M, it is NP-
completetodecideifthereisanelectionE withM = M.
E
4.4.4 Bordawise Distance
We introduce one more metric, similar in spirit to the positionwise and pairwise
ones, but defined on top of the election’s Borda score vectors. Given two equal-
sizedelectionsE andE′,theirBordawisedistanceis:
d (E,E′) = emd(sort(B ),sort(B )),
Borda E E′
where for a vector x, sort(x) means the vector obtained from x by sorting it in
the nonincreasing order. The Bordawise metric is defined to be as simple as pos-
sible, while trying to still be meaningful. For example, sorting the score vectors
ensures that two isomorphic elections are at distance zero and removes the use of
anexplicitmatchingbetweenthecandidates.
59Example4.5. TheBordascorevectorsoftheelectionsfromExample4.3are
a b c
(cid:2) (cid:3)
B = 3 5 1 ,
E1
x y z
(cid:2) (cid:3)
B = 4 3 2 ,
E2
(cid:0) (cid:1)
andthedistancebetweenthemisemd (5,3,1),(4,3,2) = 2. △
Observation4.2. TheBordawisedistanceisapseudometric.
EMD is a distance itself, and the Bordawise distance simply computes the
EMD between two Borda score vectors, so it must satisfy the triangle inequality
and symmetry as well, and the distance between two identical vectors is zero.
However,itmightbethecasethatdifferentelectionswillproducethesameBorda
score vector, so there will be two different elections at distance zero. Therefore,
theBordawisedistanceisapseudometric.
Observation 4.3. There is a polynomial-time algorithm for computing the Bor-
dawisedistance.
Converting an election into a Borda score vector requires polynomial time,
andcomputingEMDbetweentwovectorsusespolynomialtimeaswell.
Unfortunately, for Borda score vectors (as for weighted majority relations) it
ishardtodecidewhetherthereexistsarealization.
Theorem 4.3. Given a vector x of nonnegative integers, it is NP-complete to
decideifthereisanelectionE withB = x.
E
Proof. Yu et al. [2004] showed that given a sequence of positive inte-
gers a ,...,a such that a ≥ a ≥ ··· ≥ a ,
(cid:80)m
a = m(m + 1), and
1 m 1 2 m i=1 i
such that for each i ∈ [m] we have 2 ≤ a ≤ 2m, it is NP-complete to de-
i
cide if there are two permutations ϕ,ϕ′ ∈ S such that for all i ∈ [m] it holds
m
that ϕ(i) + ϕ′(i) = a . We reduce this problem to the one from the statement of
i
thetheorembyformingavectorx = (a −2,...,a −2).
1 m
If there are two permutations ϕ and ϕ′ that satisfy the conditions of Yu et al.’s
problem, then we form a two-voter election E = (C,V) as follows: We let C =
[m] and we form two votes, v and v′. For each candidate i ∈ C, the first (the
second) voter ranks i on position m − ϕ(i) + 1 (m − ϕ′(i) + 1); note that the
60(a)EMD-positionwise (b)ℓ -positionwise
1
(c)Swap
Figure4.6: Comparisonofmapsofelections.
produced votes rank exactly one candidate in each position because ϕ and ϕ′ are
permutations. Then,theBordascoreofeachi ∈ C isϕ(i)−1+ϕ′(i)−1 = a −2.
i
For the other direction, assume that there is an election E = (C,V) with
Bordascorevectorx. Then,E mustcontainexactlytwovotersbecauseotherwise
the sum of the candidates’ scores would either be too large or too small. W.l.o.g.,
we assume that C = [m] and that each candidate i ∈ C has Borda score a − 2.
i
Letvandv′ bethetwovotesinE. Weformapermutationϕsothatforeachi ∈ C
we have ϕ(i) = m − pos (i) + 1, We form ϕ′ analogously, but using v′ instead
v
of v. It follows that for each i ∈ [m] we have ϕ(i)+ϕ′(i) = (a −2)+2 = a .
i i
Thiscompletestheproof.
614.4.5 Maps of Elections Using Nonisomorphic Distances
This section is analogous to Section 4.3.4, but this time we focus on the maps
based on nonisomorphic distances and compare them with those for the swap
distance.
We use the same elections as before. (Details of the dataset were described in
Table4.4). Justasareminder,allelectionsconsistof10candidatesand50voters.
However, the embedding algorithm differs from the one used in Section 4.3. In
thischapterwedecidedtousethealgorithmofFruchtermanandReingold[1991]
toplacethepoints6.
We will start by focusing on maps based on the positionwise distance. In
Figure4.6wepresenttwomapsfortheEMD-andℓ -variantsofthepositionwise
1
distance, and one map for the isomorphic swap distance, which will serve as a
reference point. As we can see, EMD- and ℓ - variants are quite similar, and at
1
firstglanceitishardtosaywhichoneisbetter. Bybeingbetter,wemeanthatthe
mapismoresimilartotheoneproducedbasedontheswapdistance.
There are three significant differences between the positionwise variants and
the swap one. First, let us have a look at group-separable elections. Under the
swapdistance,balancedgroup-separableelectionsareclosertoANthanthecater-
pillar group-separable elections, while for the positionwise variants, the caterpil-
lar elections are closer to AN than the balanced ones. Second, for positionwise
maps, ST appears to be one of the extreme points, while for the swap distance
map,thespaceseemstospanbetweenAN,ID,andUN,whileSTisnotthatcru-
cial. Third, the swap distance is far better at distinguishing between SPOC and
impNaretixatl,cwueltumreoveeletcotiotwnso.more maps, i.e., the maps based on the pairwise and
Bordawise distances. The results are presented in Figure 4.7. Both the Borda
scorevectorsandtheweightedmajorityrelationsdonotdistinguishbetweenuni-
formity and antagonism elections (i.e., under both the Bordawise and pairwise
distances, the distance between UN and AN is zero). Unfortunately, for the Bor-
dawise distance, the situation is even more drastic. If the Borda score of all the
candidatesismoreorlessequal,thensuchelectionswillbealmostidenticalunder
the Bordawise distance. Note that in the maps based on the swap or positionwise
distances, in all elections that lie in the upper left part of the map (somewhere
between UN and AN), all the candidates (on average) have very similar Borda
scores. As to the pairwise distance, in spite of the fact that the whole Borda bal-
anceareaiscollapsingontoUN,therestofthemaplooksrelativelyfine,thatis,it
6MoredetailsaboutthedifferencesbetweenembeddingswillbepresentedinChapter5
62(a)ℓ -pairwise (b)EMD-Bordawise
1
Figure4.7: Comparisonofmapsofelections.
roughly resembles the map based on the swap distance. In Figure 4.8 we present
the average distances between elections from each pair of statistical cultures (we
omittedtheurnandMallowselectionsbecausetheyareparametrizedandcompar-
ingtheaveragevaluewouldbemeaningless). Eachcellgivestheaveragedistance
(according to a given metric) between the elections generated from respective
models. Allvaluesarenormalizedbythelargestpossibledistanceunderthegiven
metric, i.e., the distance between ID and UN (we will return to the problem of
calculatingthelargestpossibledistance,foragivenmetric,inSection4.5.1).
MainConclusions
The main conclusion of Section 4.4 is the following. Maps based on the
positionwise distances show a lot of similarities to the map based on the
swap distance, while, at the same time, being much easier to generate, due
tocomputationalcomplexityofrespectivedistances.
4.5 Comparison
In this section we compare nonisomorphic and isomorphic distances with each
other. We start with an analysis of the compass. The relation between compass
elections differs depending on the distance chosen. Next, we focus on the maps
of elections and compare maps based on nonisomorphic distances with the map
based on the swap distance—which we treat as an ideal one. Finally, we discuss
the correlation between metrics, and conclude by discussing equivalence classes
63(a)EMD-positionwise (b)ℓ -positionwise
1
(c)ℓ -pairwise (d)EMD-Bordawise
1
Figure4.8: Theaveragedistancesbetweentheelectionsfromgivencultures.
64ofourdistances.
4.5.1 Analysis of the Compass
For isomorphic distances, we can easily create instances of identity and antago-
nism elections. For the uniformity and stratification ones, we need exponentially
manyvoterswithrespecttothenumberofcandidates. Henceweusuallyusetheir
approximations, as we described it in Chapter 3. Luckily, for nonisomorphic dis-
tances such as the positionwise, pairwise, and Bordawise ones, we can represent
the compass perfectly, using the aggregate representations. We are going to de-
scribe all four characteristic points, and their aggregate representations, for each
ofthemetricsdescribedintheprevioussection. Moreover,wearegoingtopresent
thedistancesbetweenthesecharacteristicpoints. Wefocusonthefollowingvari-
ants: EMD-positionwise, ℓ -positionwise, ℓ -pairwise, and EMD-Bordawise. All
1 1
our nonisomorphic distances are independent of the number of voters, and can
be computed between elections with different numbers of voters. From isomor-
phic distances we study the swap and discrete ones—for them, to compare two
elections, we need exactly the same numbers of voters and candidates in both
elections.
TheproofsofallthepropositionsfromthissectionareintheAppendixA,due
totheirtediouslytechnicalcharacterandlimitedinterest.
EMD-positionwise
We start with the EMD-positionwise distance. Sometimes instead of using posi-
tion matrix, it is more convenient to use its normalized variant, which we define
asfollows.
Consider an election E = (C,V) with C = {c ,...,c } and V =
1 m
(v ,...,v ). For each candidate c and position i ∈ [m], we define #freq (c ,i)
1 n j E j
to be the fraction of the votes from V that rank c in position i. We define
j
the column vector #freq (c ) to be (#freq (c ,1),...,#freq (c ,m)) and ma-
E j E j E j
trix #freq(E) to consist of vectors #freq (c ),...,#freq (c ). We refer to
E 1 E m
#freq(E) as the frequency matrix of election E. Frequency matrices are bis-
tochastic, i.e., their entries are nonnegative and each of their rows and columns
sums up to one. Note that if we take position matrix and divide all its entries
by the number of voters we immediately obtain the frequency matrix of the same
65election.7
Two most important matrices are the identity matrix ID and the uniformity
matrixUN. Theidentitymatrixcorrespondstoelectionswhereeachvoterhasthe
samepreferenceorder,i.e.,thereisacommonorderingofthecandidatesfromthe
most to the least desirable one. For ID, we have ones on the diagonal and zeros
elsewhere,aspresentedbelow.
 
1 0 ··· 0
0 1 ··· 0
ID m =  . . . . . . . . .
. . . .
0 0 ··· 1
In contrast, the uniformity matrix captures elections where each candidate is
rankedoneachpositionequallyoften,i.e.,where,inaggregate,allthecandidates
are viewed as equally good. Uniformity elections are quite similar to the IC ones
and, in the limit, indistinguishable from them. Yet, for a fixed number of voters,
typically IC elections are at some (small) positionwise distance from uniformity.
ForUN,eachentryisequalto1/ m.
 
1/
m
1/
m
··· 1/
m
1/ m 1/ m ··· 1/ m
UN m =   . . . . . . . .  .
 . . . . 
1/
m
1/
m
··· 1/
m
The next matrix, stratification, is defined as follows (we assume that m is
even):
(cid:20) (cid:21)
UN 0
ST = m/2 .
m 0 UN
m/2
Stratification matrices correspond to elections where the voters agree that half of
thecandidatesaremoredesirablethantheotherhalf,but,inaggregate,areunable
todistinguishbetweenthequalitiesofthecandidatesineachgroup.
For the final matrix, we need one more piece of notation. Let rID be the
m
matrixobtainedbyreversingtheorderofthecolumnsoftheidentitymatrixID .
m
Wedefinetheantagonismmatrix,AN m,tobe1/ 2ID
m
+1/ 2rID m.
7Whenweusefrequencymatricesinsteadofpositionmatricesallthedistancesbetweensuch
matricesarescaledbythefactorof1/n.
66   
1 0 ··· 0 0 0 ··· 1
0 1 ··· 0 . . . . .. . .
AN m = 1 2   . . . . . . . . . . . .  + 1 2   0. 1. ·. ·· 0.  .
0 0 ··· 1 1 0 ··· 0
Suchmatricesaregenerated,forexample,byelectionswherehalfofthevoters
rank the candidates in one way, and half of the voters rank them in the opposite
one, so there is a clear conflict. In some sense, stratification and antagonism
are based on similar premises. Under stratification, the group of candidates is
partitioned into halves with different properties, whereas in antagonism (for the
casewherehalfofthevotersrankthecandidatesinthesameorder)thevotersare
partitioned. However,thenatureofthepartitioningis,naturally,quitedifferent.
Proposition4.14. Ifmisdivisibleby4,thenitholdsthat:
1. d (ID ,UN ) = 1(m2 −1),
pos m m 3
2. d (ID ,AN ) = d (UN ,ST ) = m2,
pos m m pos m m 4
3. d (ID ,ST ) = d (UN ,AN ) = 2(m2 −1),
pos m m pos m m 3 4
4. d (AN ,ST ) = 13m2 − 1.
pos m m 48 3
Whatisworthemphasizingisthefactthatd (ID,UN)isthelargestpossible
pos
distance in the whole space; more precisely, there do not exist any other pair of
electionsthatareatlargerdistancethanIDandUN.
Thesameistrueforallotherdistancesthatwewillanalyzewithinthissection.
Formoredetails,seetheworkofBoehmeretal.[2022c]).
Theorem 4.4 (Boehmer et al. [2022c]). For each two elections X and Y, each
overmcandidates,itholdsthatd (X,Y) ≤ d (ID ,UN ).
pos pos m m
TonormalizethedistancesfromProposition4.14,wedividethembyD(m) =
d (ID ,UN ). ForeachtwomatricesX andY amongourfourcompassmatri-
pos m m
ces, we let d pos(X,Y) = lim m→∞dpos(Xm,Ym)/ D(m). A simple computation shows
thefollowing(seealsothedrawingontherightside;wesometimesomitthesub-
scriptmforsimplicity):
67AN
1 3
d (ID,UN) = 1, 2 4
pos UN 13 ID
d pos(ID,AN) = d pos(UN,ST) = 3/ 4, 1 16
d pos(AN,ST) = 13/ 16,
3 1
d pos(ID,ST) = d pos(UN,AN) = 1/ 2.
4 ST 2
ℓ -positionwise
1
For the ℓ -positionwise variant, all compass matrices are exactly the same as for
1
theemd-positionwise,sowemovedirectlytocomputingdistancesbetweenthem.
Proposition4.15. Ifmisdivisibleby4,thenitholdsthat:
1. dℓ1 (ID ,UN ) = 2(m−1)
pos m m
2. dℓ1 (UN ,AN ) = dℓ1 (AN ,ST ) = dℓ1 (ID ,ST ) = 2(m−2)
pos m m pos m m pos m m
3. dℓ1 (UN ,ST ) = dℓ1 (ID ,AN ) = m
pos m m pos m m
As before, we normalize these distances by dividing them by the largest pos-
sibledistance,dℓ1 (ID ,UN ),andthencomputethelimits.
pos m m
AN
dℓ1 (ID,UN) = dℓ1 (AN,ST) 1
pos pos 1
2
= dℓ1 (ID,ST) UN 1 1 ID
pos
= dℓ1 (UN,AN) = 1.
pos
dℓ p1 os(ID,AN) = dℓ p1 os(UN,ST) = 1/ 2, 1
2 ST
1
If we compare the EMD and ℓ variants, we will see that, for
1
EMD,d (ID,UN)isdominatingallotherdistances,whileforℓ ,d (AN,ST),
pos 1 pos
d (ID,ST),andd (UN,AN)arealmostaslargeasd (ID,UN).
pos pos pos
ℓ -pairwise
1
Forthepairwisedistanceweonlyconsidertheℓ variant,sousuallyinsteadofℓ -
1 1
pairwisewesimplywritepairwise.
68As before, we start by defining weighted majority relations for our compass
elections, normalized by the number of voters.8 For the identity, we simply have
amatrixwithonesabovethediagonalandzerosbelow.
− 1 ··· 1 1
0 − ··· 1 1
ID
m
=  . .
.
. .
.
... . .
.
. . . ,
0 0 ··· − 1
0 0 ··· 0 −
Now,weobservesomethinginteresting. Bothuniformityandantagonismpro-
duce exactly the same weighted majority relation with undefined values on the
diagonal and 0.5 values everywhere else. In head-to-head comparisons between
anytwocandidatesthereisalwaysatie.
− 0.5 ··· 0.5 0.5
0.5 − ··· 0.5 0.5
UN
m
= AN
m
= 

. .
.
. .
.
... . .
.
. .
.
 ,
0.5 0.5 ··· − 0.5
0.5 0.5 ··· 0.5 −
Finally, we present the matrix for the stratification election. It consists of
four squares. The upper-right square is filled with ones, the lower-left square is
filledwithzeros,whiletheupper-leftandlower-rightsquaresareundefinedonthe
diagonalandhave0.5valueselsewhere.
 
− 0.5 ··· 0.5 1 ··· 1 1
0.5 − ··· 0.5 1 ··· 1 1
  . . . . . . ... . . . . . . ... . . . . . .  
ST =  0.5 0.5 ··· − 1 ··· 1 1  
m  0 0 ··· 0 − ··· 0.5 0.5


. .
.
. .
.
... . .
.
. .
.
... . .
.
. .
.


0 0 ··· 0 0.5 ··· − 0.5
0 0 ··· 0 0.5 ··· 0.5 −
Next, we compute the pairwise distances of these matrices. Because UN and AN
areidentical,weomitdistancesbetweenANandotherpointsfromthecompass.
Proposition4.16. Itholdsthat:
1. d (ID ,UN ) = 1m(m−1)
pair m m 2
8When we use normalized weighted majority relations instead of unnormalized ones, all the
distancesarescaledbythefactorof1/n.
692. d (UN ,ST ) = 1m2
pair m m 4
3. d (ID ,ST ) = 1m(m−2)
pair m m 4
As before, we normalize these distances by dividing them by the largest pos-
sibledistance,d (ID ,UN ),andthencomputethelimits.
pair m m
1
d (ID,UN) = 1 UN ID
pair
1
d (UN,ST) = d (ID,ST) =
pair pair 2 AN 1 ST 1
2 2
EMD-Bordawise
FortheBordawisedistanceweonlyconsidertheEMDvariant,sousuallyinstead
of EMD-Bordawise we simply write Bordawise. Moreover, we normalize all the
valuesinBordascorevectorsbyn.9
TheBordascorevectoroftheidentityelectionisasfollows:
ID = [ ]
m (m−1),(m−2),...,1,0
As was the case for the pairwise distance, here again uniformity and antagonism
areindistinguishableandproducethesameBordascorevector.
UN
m
= AN
m
= [m 2−1,...,m 2−1]
Finally,wehavethevectorforthestratificationelection.
ST
m
= [3(m 4−1),...,3(m 4−1),m 4−1,...,m 4−1]
NextwecomputetheBordawisedistancesofthesevectors:
Proposition4.17. Ifmiseven,itholdsthat:
1. d (ID ,UN ) = 1 ·m(m2 −1)
Borda m m 12
2. d (UN ,ST )) = 1 ·m2(m−1)
Borda m m 16
3. d (ID ,ST ) = 1 ·m(m2 +3m−4)
Borda m m 48
9Itmeansthatallthedistancesbetweensuchnormalizedvectorsarescaledbythefactorof1/n.
70As before, we normalize these distances by dividing them by the largest pos-
sibledistance,d (ID ,UN ),andthencomputethelimits.
Borda m m
d (ID,UN) = 1
Borda 1
3
UN ID
d (UN,ST) =
Borda
4
1 AN
3 ST 1
d (ID,ST) =
Borda 4 4 4
Thedistances(andthewholepicture)fortheBordawisedistanceareverysim-
ilar to those of the pairwise distance. The only major difference is the placement
of the stratification election. While under the pairwise distance it is located in
themiddlebetweenidentityanduniformity,fortheBordawisedistanceitismuch
closertoidentity.
Swap
Unlike for the nonisomorphic distances, for the swap distances we do not have
any aggregate form of elections and compute the swap distances on the original
elections. Unfortunately, not all compass elections we can easily generate with
any m and n. For the identity election we have the simplest scenario, because
for any n we can easily generate ID . For the antagonism election it is also
m,n
simple: To generate AN , it suffices to assume that n is even. However, for the
m,n
stratificationanduniformityelectionsthesituationisgettingcomplicated,because
forST weneed m!|n,andforUN weneedm!|n.
m,n 2 m,n
Proposition4.18. Ifm!|nitholdsthat:
1. d (ID ,UN ) = d (ID ,AN ) = 1n(m2 −m)
swap m,n m,n swap m,n m,n 4
2. d (ID ,ST ) = 1n(m2 −2m)
swap m,n m,n 8
3. d (UN ,AN ) = Θ(nm2)(seeRemark1below)
swap m,n m,n
4. d (UN ,ST ) = 1nm2
swap m,n m,n 8
5. d (AN ,ST ) = Θ(nm2)(seeRemark1below)
swap m,n m,n
Remark 4.1. Unfortunately for d (UN ,AN ) and d (AN ,ST )
swap m,n m,n swap m,n m,n
wedonothaveclosedformformulas(andwearenotsureiftheyexist). However,
it holds that 1/ 8 n(m2 −3m+2) ≤ d swap(UN m,n,AN m,n) ≤ 1/ 4 n(m2 −m) and
also1/ 8n(m2 −2m) ≤ d swap(AN m,n,ST m,n) ≤ 1/ 4n(m2 −m).
71As before, we normalize these distances by dividing them by the largest pos-
sibledistance,d (ID ,UN ),andthencomputethelimits.
swap m,n m,n
AN
? 1
d (ID,UN) = d (ID,AN) = 1
swap swap ?
1
d (ID,ST) = d (UN,ST) =
swap swap
2
1 1
2 2
UN ST ID
1
Discrete
For the discrete distance, the situation is analogous to the case of the swap dis-
tance.
Proposition4.19. Ifm!|nitholdsthat:
1. d (ID ,UN ) = nm!−1
disc m,n m,n m!
2. d (ID ,AN ) = 1n
disc m,n m,n 2
3. d (UN ,AN ) = nm!−2
disc m,n m,n m!
4. d (UN ,ST ) =
nm!−((m/2)!)2
disc m,n m,n m!
5. d (ID ,ST ) = d (AN ,ST ) =
n((m/2)!)2−1
disc m,n m,n disc m,n m,n ((m/2)!)2
As before, we normalize these distances by dividing them by the largest pos-
sibledistance,d (ID ,UN ),andthencomputethelimits.
disc m,n m,n
d (ID,UN) = d (ID,ST) = d (UN,AN)
disc disc disc
= d (UN,ST) = d (AN,ST) = 1
disc disc
1
d (ID,AN) =
disc
2
72|C|×|V| ANECs Positionwise Pairwise Bordawise
3×3 10 10 8 8
3×4 24 23 17 13
3×5 42 40 25 18
4×3 111 93 50 37
4×4 762 465 200 76
4×5 4095 1746 513 131
Table4.5: Numberofequivalenceclassesunderourmetrics.
Unlike for the other distances, for the discrete distance we do not present
graphicalrepresentationduetoitsobscurity.
4.5.2 Equivalence Classes
Given a distance, two elections are in the same equivalence class if their distance
iszero. Ananonymous,neutralequivalenceclass(ANEC)consistsofallelections
that are isomorphic to each other [Eg˘eciog˘lu and Giritligil, 2013]. While ANECs
are the equivalence classes of the isomorphic distances (e.g., the swap one), the
other distances are less precise and their equivalence classes are unions of some
ANECs.
To get a feeling as to how much precision is lost due to various aggregate
representations,inTable4.5wecomparethenumbersofANECsandthenumbers
of equivalence classes of the positionwise, pairwise, and Bordawise metrics, for
smallelections;wecomputedthetableusingexhaustivesearch10 (notethatEMD-
andℓ -positionwisemetricshavethesameequivalenceclasses).
1
Among these metrics, the positionwise ones perform best and Bordawise per-
forms worst. Next, we provide a partial theoretical explanation for this obser-
vation. We say that a metric d is at least as fine as a metric d′ if for each two
elections A and B, d(A,B) = 0 implies that d′(A,B) = 0 (i.e., each equivalence
class of d is a subset of some equivalence class of d′). Metric d is finer than d′ if
itisatleastasfineasd′ butd′ isnotatleastasfineasd.
Proposition 4.20 (Boehmer et al. [2022c]). The swap and discrete isomorphic
distances are finer than the EMD/ℓ -positionwise and pairwise ones, which both
1
10ThereareexactformulasforsomecolumnsinTable4.5,butnotforall. See,e.g.,theworkof
Eg˘eciog˘luandGiritligil[2013].
73ANECs
PairwiseECs PositionwiseECs
BordawiseECs
Figure 4.9: Relationship between different metrics. An arc from metric d to d′
meansthatd′ isatleastasexpressiveasd.
are finer than the Bordawise distance. Neither the EMD/ℓ -positionwise distance
1
isfinerthanthepairwisedistancenortheotherwayround.
In Figure 4.9 we present a scheme that illustrates the relationship between
differentdistances(i.e.,theimplicationsofProposition4.20).
4.5.3 Correlation
In Figure 4.10 we present the correlation plots for the nonisomorphic distances
and the swap distance. (Recall that the synthetic dataset that we use consists
of elections with 10 candidates and 50 voters sampled from 13 models; for 11
of them we generated 20 elections and for the Norm-Mallows and urn models
we sampled 60 elections.) As a complement to the correlation plots, we present
twoadditionaltables. First,wehaveTable4.6,wherewehavecomputedPearson
correlationcoefficientbetweentheswapdistancesandotherdistancesSecond,we
haveTable4.7,wherewehavecomputedthePCCbetweentheswapdistancesand
thoseprovidedbytheothermetricsforeachstatisticalcultureindependently(i.e.,
for each statistical culture we give the correlation coefficients between the swap
distances of all pairs of elections from this culture and their distances according
toourothermetrics).
Aswecansee,thestrongestcorrelationiswitnessedbytheEMD-positionwise
distance (having PCC equal 0.745, see Table 4.6) followed by the EMD-
Bordawise distance (having PCC equal 0.713). Then we have the ℓ -pairwise
1
distance (having PCC equal 0.708), and ℓ -positionwise distance (having PCC
1
equal 0.563), and, finally, the worst correlation is witnessed by discrete dis-
tance (having PCC equal 0.342). The surprisingly high correlation for the EMD-
Bordawiseandℓ -pairwisedistancesapparentlycomesfromthefactthatthisdis-
1
tanceworkswellforelectionsfromtheNormalizedMallowsandurnmodels,and
74(a)SwapvsEMD-positionwise (b)Swapvsℓ -positionwise
1
(c)Swapvsℓ -pairwise (d)SwapvsEMD-Bordawise
1
Figure 4.10: Correlation between the nonisomorphic distances and the swap dis-
tancesbasedonthesyntheticdatasetdescribedinTable4.4.
75|C|×|V| EMD-Pos. ℓ -Pos. ℓ -Pair. EMD-Bordawise Discrete
1 1
3×3 0.942 0.748 0.860 0.817 0.614
3×4 0.900 0.697 0.860 0.737 0.636
3×5 0.920 0.759 0.843 0.747 0.680
4×3 0.850 0.577 0.735 0.675 0.402
4×4 0.782 0.561 0.689 0.610 0.434
4×5 0.772 0.567 0.672 0.606 0.432
10×50 0.745 0.563 0.708 0.713 0.342
(340elections)
Table 4.6: Pearson correlation coefficients between the swap distance and the
otheronescomputedforourdatasets.
in our dataset we had a lot of elections from these two models. In Table 4.7, we
can see that for almost all other models (with the exception of group-separable
andimpartialculture)thecorrelationisinsignificant.
MainConclusions
Among studied distances (i.e., EMD-positionwise, ℓ -positionwise, EMD-
1
Bordawise, ℓ -pairwise) the EMD-positionwise is most strongly correlated
1
with the swap distance, with a clear advantage over the other metrics.
Hence, we recommend it for using in practice, especially when dealing
withlargeelections(i.e.,withmanyvotersandcandidates).
4.6 Summary
The main objective of this chapter was to find meaningful ways of calculating
the distances between elections with ordinal ballots. We believe that we have
succeeded in fulfilling this task, or at least we have shown the direction in which
togo.
We proposed three isomorphic distances, i.e., swap, Spearman, and discrete
distances. Both the swap and Spearman distances are very precise, but slow to
compute. Without surprise, the discrete distance proves to be quite useless, with
mostelectionsbeingatmaximal(oralmostmaximal)distancesfromeachother.
Wealsointroducedseveralnonisomorphicdistances. Wehavetwovariantsof
76Name EMD-Pos. ℓ -Pos. ℓ -Pair. EMD-Borda. Discrete
1 1
ImpartialCulture 0.481 0.114 0.525 0.471 -0.039
SPbyConitzer 0.471 0.727 -0.142 -0.015 0.976
SPbyWalsh 0.377 0.467 -0.119 0.111 0.7
SPOC 0.297 0.409 -0.074 0.079 0.622
Single-Crossing 0.252 0.248 0.123 0.098 0.625
Interval 0.242 0.219 0.101 0.088 0.606
Disc 0.337 0.317 0.203 0.149 0.636
Cube 0.406 0.347 0.311 0.286 0.67
Circle 0.406 0.329 0.335 0.287 0.651
Urn 0.84 0.86 0.803 0.772 0.102
Norm-Mallows 0.86 0.784 0.839 0.82 0.255
GSBalanced 0.863 0.793 0.844 0.822 0.259
GSCaterpillar 0.864 0.795 0.845 0.824 0.252
Table 4.7: Pearson correlation coefficients between the swap distance and the
otheronescomputedforeachstatisticalcultureusedinourmaps.
thepositionwisedistance,oneusingEMDandtheotherusingℓ astheunderlying
1
norms. Although both variants are quite similar, we favor EMD over ℓ due to its
1
stronger correlation with the swap distance. Then we have pairwise and Borda-
wise distances. Neither of them is convincing because they collapse the whole
Bordabalancedareaintoasinglepoint(inparticular,uniformity,andantagonism
elections become indistinguishable). Nonetheless, both are doing well enough
at placing elections between the uniformity and identity. Regarding the amount
of time needed to compute the distances, pairwise distance is relatively slow to
compute,whileBordawiseisextremelyfast—butitisitsonlyadvantage.
As a major conclusion, we can say that if the number of candidates is limited
(e.g., not larger than 10) then we recommend using the swap distance, as it is the
most precise one. For elections with more candidates, we recommend the EMD-
positionwiseasitachievesthebesttrade-offbetweenprecisionandtime.
77MainContributions
• Introductionofvariousdistancesthatserveformeasuringsimilarities
betweenordinalelections.
• Introductionofthemapofelectionsframework.
• Detailedcomparisonbetweendifferentdistances,concludingthatfor
smallelectionswesuggestusingswapdistance,whileforlargerelec-
tionswesuggestusingtheEMD-positionwiseone.
78Chapter 5
Applications
5.1 Introduction
Creating a map of elections consists of the three following steps. First, we have
to prepare the elections—we can either sample them from statistical models or
select some real-life ones. Second, we compute the distances between each pair
ofelections—thisgivesusadistancematrix. Third,weembedthedistancematrix
inatwo-dimensionalEuclideanspace. Eachofthesestepscanbedoneinnumer-
ous ways, that is, there are many ways of generating elections, there are several
distancestochoosefrom,andfinallywehavetodecideonaparticularembedding
algorithm.
In the first part of this chapter, we argue that the way we design the map is
reasonable. We present results for several different embeddings, and explain why
werecommendusingoneovertheother. Inparticular,weanalyzetheconceptsof
monotonicity and distortion of embeddings, which test the quality of a given em-
bedding. We also discuss how changing the number of candidates is influencing
themap,inotherwords,weanswerthequestionofscalabilityofthemap.
Later,inthesecondpart,weprovidenumerouspracticalexamplesofapplica-
tions of the map. We study single-winner voting rules such as plurality, Borda,
Copeland, and Dodgson, and multiwinner voting rules such as Chamberlin–
Courant and Harmonic-Borda. We use the map to show the relationship between
various voting rules and statistical cultures. In particular, we are curious if elec-
tionslyingnexttooneanotheronthemapbehaveinasimilarmanner—forexam-
ple, the winning candidate/committee has a similar score, or computing the win-
ning candidate or committee is taking similar amount of time. For example, for
79Dodgsonrulethelongestrunningtimewaswitnessedbygroup-separablecaterpil-
lar elections, while for Harmonic-Borda it was 4-Sphere elections. Moreover, for
the Chamberlin–Courant and Harmonic-Borda rules, we compare the effective-
ness of their approximation algorithms. Then, we analyze a number of real-life
instances of elections, and see where they land on the map. We study political
elections,surveys,andsportcompetitions. Finally,webrieflydiscusstheconcept
of a skeleton map, where instead of sampling numerous elections from a given
distribution, we present only one frequency matrix that captures that statistical
culture.
All the maps presented in this chapter are based on the positionwise distance.
This means that we operate on matrices rather than elections. We have chosen
the positionwise distance in order to be able to draw maps with large (up to 100)
numbersofcandidates.
5.2 Setup
We start by outlining the basic setup for our experiments, i.e., the set of elections
onwhichthemapisbased.
In Table 5.1 we list all the models that we use in our maps, and also the num-
bers of elections sampled from each model. The exact number of candidates and
voterswillbespecifiedforeachexperimentindependently.
Wealsointroducenewartificialfamiliesofelections,calledpaths,whichserve
for making the map more stable and easier to interpret. Briefly speaking, we take
convex combinations of the compass matrices and create paths between them.
Below,wedescribethisconceptinmoredetail.
PathsbetweenElectionMatrices
We consider convex combinations of frequency matrices. Given two such matri-
ces, X and Y, and α ∈ [0,1], one might expect that matrix Z = αX +(1−α)Y
wouldlieatpositionwisedistance(1−α)·d (X,Y)fromX andatpositionwise
pos
distanceα·d (X,Y)fromY,sothatwewouldhave:
pos
d (X,Y) = d (X,Z)+d (Z,Y).
pos pos pos
However, without further assumptions, this is not necessarily the case. Indeed, if
wetakeX = ID andY = rID ,then0.5X+0.5Y = AN andd (X,Y) = 0,
m m m pos
but d (X,0.5X +0.5Y) = d (ID,AN) > 0. However, if we arrange the two
pos pos
80Model NumberofElections
ImpartialCulture 20
Single-Peaked(Conitzer) 20
Single-Peaked(Walsh) 20
SPOC 20
Single-Crossing 20
Interval 20
Square 20
Cube 20
5-Cube 20
10-Cube 20
20-Cube 20
Circle 20
Sphere 20
4-Sphere 20
Group-Separable(Balanced) 20
Group-Separable(Caterpillar) 20
Urn 80
Mallows 80
Compass(ID,AN,UN,ST) 4
Paths 20×4
Table5.1: Setup
81Figure5.1: Pathsbetweencompassmatrices.
matrices X and Y so that their positionwise distance is achieved by the identity
permutation of their column vectors, then their convex combination lies exactly
betweenthem.
Proposition 5.1. Let X = (x ,...,x ) and Y = (y ,...y ) be two m × m
1 m 1 m
frequencymatricessuchthatd (X,Y) =
(cid:80)m
emd(x ,y ). Then,foreachα ∈
pos i=1 i i
[0,1]itholdsthatd (X,Y) = d (X,αX+(1−α)Y)+d (αX+(1−α)Y,Y).
pos pos pos
Proof. LetZ = (z ,...,z ) = αX +(1−α)Y beourconvexcombinationofX
1 m
and Y. We note two properties of the earth mover’s distance. Let a, b, and c be
threevectorsthatconsistofnonnegativenumbers,wheretheentriesinbandcsum
up to the same value. Then, it holds that emd(a+b,a+c) = emd(b,c). Further,
foranonnegativenumberλ,wehavethatemd(λb,λc) = λemd(b,c). Usingthese
observationsandthedefinitionoftheearthmover’sdistance,wenotethat:
d (X,Z) ≤
(cid:80)m
emd(x ,z )
pos i=1 i i
=
(cid:80)m
emd(x ,αx +(1−α)y )
i=1 i i i
=
(cid:80)m
emd((1−α)x ,(1−α)y )
i=1 i i
=
(1−α)(cid:80)m
emd(x ,y ) = (1−α)d (X,Y).
i=1 i i pos
ThelastequalityfollowsbyourassumptionregardingX andY. Byananalogous
reasoning, we also have that d (Z,Y) ≤ αd (X,Y). By putting these two
pos pos
inequalitiestogether,wehavethat:
d (X,Z)+d (Z,Y) ≤ d (X,Y).
pos pos pos
By the triangle inequality, we have that d (X,Y) ≤ d (X,Z) + d (Z,Y)
pos pos pos
and,so,wehavethatd (X,Z)+d (Z,Y) = d (X,Y).
pos pos pos
82Using Proposition 5.1, for any two compass matrices, we can generate a se-
quence of matrices that form a path between them. For example, matrix 0.5ID+
0.5UNisexactlyatthesamedistancefromIDandfromUN.
In Figure 5.1 we show a map of elections that contains our four compass ma-
trices and, for each two of them, i.e., for each two X,Y ∈ {ID,UN,AN,ST},
a set of 20 matrices obtained as their convex combinations with values of α uni-
formly spread over [0,1]. The map was created using the MDS embedding. Even
though each path consisted of the same number of matrices, we see that propor-
tions of the distances between the compass matrices are maintained. Recall that,
if d pos(ID,UN) = 1, then d pos(ID,AN) = d pos(UN,ST) = 3/ 4, d pos(AN,ST) =
13/ 16,andd pos(ID,ST) = d pos(UN,AN) = 1/ 2.
5.3 Embedding
In this section, we compare various different embedding methods. In particu-
lar, we consider the following six algorithms: multidimensional scaling (MDS),
t-distributed stochastic neighbor embedding (t-SNE), locally linear embedding
(LLE), a variant of the Kamada and Kawai algorithm (KK), principal component
analysis(PCA),andtheFruchtermanandReingoldalgorithm(FR).Technicalas-
pects of these methods were described in Chapter 2. Sometimes we use the term
MDSmap,asanabbreviatedformforthemapthatwascreatedusingtheMDSem-
bedding. WheneverwewriteoriginaldistancewerefertotheEMD-positionwise
distancebetweentheelections,andwheneverwewriteembeddeddistancewere-
fer to the Euclidean distance between the points on the plane (which correspond
to these elections) after embedding. Whenever we write normalized distance, we
refer to the distance divided by the distance between the identity and uniformity
becauseitisthelargestpossibleone.
How to compare two different embeddings? To answer this question, first we
have to explain what the main purpose of the map is. We want to make it easier
andmoreintuitivetoseecertainfeaturesandpropertiesofelections. Althoughwe
have the table with the original distances, it is hard to analyze the data solely by
lookingatthevaluesinthetable. Iftwoelectionsaresimilar,wewouldlikethem
to lie next to each other on the map. However, in most cases we are interested
especiallyinthelocalcorrectnessofthemap(i.e.,iftwoelectionsarefarawayon
themap,wedonotgivethatmuchattentiontodistinguishingwhethertheyarefar
orveryfar). Nonetheless,itisimportanttoknowwhichembeddingsarefocusing
onlocalcorrectness,andwhicharetryingtocorrectlyembedallthedistances.
83(a)FR (b)LLE
(c)KK (d)t-SNE
(e)MDS (f)PCA
Figure5.2: Comparisonofembeddingsalgorithms.
84In Figure 5.2 we present the results for several embeddings. As we can see,
the maps are quite diverse. For t-SNE and PCA, we clearly see that they will not
be very useful for us. The LLE embedding is slightly better, and we can see the
main shape, yet it is still far from what we want (we would like most elections
to lie between the four paths, and, if possible, to be more spread over the space).
As to the MDS, KK and FR, they produce more or less the same shape but with
different levels of compactness—with the MDS being the most compact and the
FR being the least. From now on, we focus only on these three embeddings and
discusstheminmoredetail.
For the MDS map we witness a flaw, that is, some elections are questionably
placed. Forexample,severalelectionsfromthegroup-separablecaterpillarmodel
are far away from the rest—which is not the case when we look closely at the
original distances. We usually observe such flaws in maps with a high number of
candidates(forexample100). Foramoment,letusforgetaboutthisflawbecause
fornumerousmapswithsmallernumbersofcandidatesitisnotoccurring.
Which map is the best? One approach would be to verify the correlation be-
tweentheembeddeddistancesandtheoriginalones. Hereiswhatweget: Forthe
KK method the PCC is the highest and is equal to 0.9805, for the MDS method it
isequalto0.9748andfortheFRmethoditisequal0.9364.
Ifourgoalweretolocalizewhereagivenelectionpreciselylies,wewouldrec-
ommend KK—we give arguments for this in the following sections about mono-
tonicity and distortion of the embeddings. However, if we would later color the
map according to certain features, for example, the highest Borda score in each
election or the time needed to compute the winning committee under a particular
rule, it is useful to have a less compact map—as long as it maintains the proper
shape and is not giving us misleading impressions. Therefore, for the maps col-
ored by features, we recommend using the FR embedding. Nevertheless, we still
find the MDS algorithm useful for some other tasks, such as, for example, the
maps of preferences (for ordinal preferences recall Section 3.4, and for approval
preferencesseeSection7.4).
Whenever we write m×n elections, we refer to elections with m candidates
and n voters. To simplify the discussion of the concepts of monotonicity and
distortionweintroducethenotionofanexperiment.
Experiment
By an experiment Q = (E,d ,d ) we refer to a triple that consists of a set of
M Euc
electionsE,originaldistancesd betweentheseelectionsaccordingtometricM,
M
85(a)FR (b)KK
(c)MDS
Figure5.3: Monotonicitywithϵ = 0(100×100).
andEuclideandistancesd betweentheseelectionsaftertheembedding. Inour
Euc
case,ford weselecttheEMD-positionwisedistance.
M
5.3.1 Monotonicity
One of the tools that we use to evaluate the quality of different embeddings is
what we call monotonicity. The intuition is that if the original distance between
elections A and B is larger than the original distance between elections A and C,
then we expect that the same will hold for the embedded distances, that is, the
embedded distance between elections A and B will be larger than the embedded
distancebetweenelectionsAandC. Now,wemovetoformaldefinition.
For a given experiment Q = (E,d ,d ) and a given election X ∈ E, we
M Euc
86definethetotalmonotonicityofthiselectioninthisexperimenttobe:
(cid:88)
µ (X) = ∆ (Y,Z),
Q X
Y,Z∈E
where∆ (Y,Z)isequalto1,if
X
sgn(d (X,Y)−d (X,Z)) = sgn(d (X,Y)−d (X,Z)),
Euc Euc M M
and is equal to 0 otherwise. Positive (negative) signs mean that both the original
and the embedded distances between X and Y were larger (smaller) than the
distances between X and Z. In principle, the larger the total monotonicity the
better. We also consider a relaxed variant of the monotonicity notion, where in
thecaseofdifferentsignsweallowforasmallerror. Formally,foragivenϵ ∈ R,
maximalerror∆ϵ (Y,Z)isequalto1if
X
sgn(d (X,Y)−d (X,Z)) = sgn(d (X,Y)−d (X,Z)),
Euc Euc M M
or
|d (X,Y)−d (X,Z)| ≤ ϵ·min(d (X,Y),d (X,Z)).
Euc Euc Euc Euc
This means that, given target point A and two other points B and C, if originally
point B was closer to A than point C, and after the embedding point B is further
from A than point C, but the difference between embedded distances between
points A and B, and A and C is relatively small, than we can argue that the
embeddingofAinrelationtoB,C isnotperfectbutstilluseful,becausetheerror
issmall.
In Figure 5.3 we present the maps (created using the FR, KK, and MDS em-
beddings), where each point (election) is colored accordingly to its monotonicity
(with ϵ = 0). The larger (the closer to green) the value, the better, and the lower
(the closer to black) the value, the worse. Monotonicity equal to 1 means that
all inequalities are maintained after the embedding. For all three maps, the main
message is the same, elections from the IC, SPOC, Mallows, Walsh, and mul-
tidimensional Euclidean models are nicely embedded. Then, elections from the
single-crossing and group-separable models are still fine, but on average worse
than the previously mentioned models. Finally, we have elections from the In-
terval, Conitzer, and urn models — which are the worst embedded (not count-
ing some elections from group-separable caterpillar group for MDS embedding,
whichareobviouslywrong).
87Algorithm ϵ = 0 ϵ = 0.05 ϵ = 0.1
FR 0.887 0.912 0.929
KK 0.928 0.951 0.964
MDS 0.925 0.947 0.947
Table5.2: Monotonicity(100×100).
Moreover,inTable5.2weenclosetheaveragemonotonicityforthepresented
maps, also for two other ϵ values, 0.05, and 0.1. We see that with respect to
monotonicity, the KK embedding performs best, with MDS right behind it, and
followedbyFR.
5.3.2 Distortion
In addition to monotonicity, we also consider distortion. In spirit, it is similar
to monotonicity but instead of triplets, it analyzes pairs. The intuition is that
the normalized embedded distance should be similar to the normalized original
distance. Formally,foragivenpairofelectionsX andY thedistortionisdefined
as:
¯ ¯
max(d (X,Y),d (X,Y))
Euc M
MR(X,Y) = ,
¯ ¯
min(d (X,Y),d (X,Y))
Euc M
¯
where d(X,Y) means that the distance between X and Y is normalized by the
distance between ID and UN. For a given experiment Q and a given election X,
wedefinethetotaldistortionofthiselectioninthisexperimenttobe:
(cid:88)
TMR (X) = MR(X,Y).
Q
Y∈Q
ThecloseristheTMRvaluetoone,thebetter—thismeansthattheembedded
distancedareproportionaltotheoriginalones.
In Figure 5.4, we present the maps colored according to their distortion. The
best distortion is witnessed by the KK embedding, with elections from the urn
model having the worst distortion. For the FR embedding, the situation is very
similar, however on average we have slightly worse distortion. For the MDS em-
bedding the situation differs. Besides the misplaced group-separable caterpillar
elections and some urn elections in the lower part of the map, we can see dark
88(a)FR (b)KK
(c)MDS
Figure5.4: Distortion(100×100).
points in the middle—in the impartial culture cluster, and among highly dimen-
sional Euclidean cluster—which was not the case for the previous two embed-
dings.
Moreover, in Table 5.3 we present the average values for the discussed meth-
ods and different numbers of candidates. We observe two patterns. The first one
isrelatedtotheembeddingmethods: KKisalwaysthebest,followedbyFR,with
MDS being the worst. The second pattern is related to the number of candidates:
Forallthreeembeddingalgorithmsthehigherthenumberofcandidates,thelower
the distortion. Nonetheless, for each method the distances in the embedding are,
onaverage,offby20−30%. Thismeansthatwecangetintuitionsfromthemaps,
butwealwaysneedtocarefullyverifythem.
89Algorithm 4×100 10×100 20×100 100×100
FR 1.322 1.282 1.272 1.255
KK 1.258 1.248 1.236 1.200
MDS 1.333 1.331 1.320 1.315
Table5.3: Distortion
5.3.3 Scalability
Inthissectionwecomparetheembeddingresultsforelectionswithdifferentnum-
bers of candidates. In Figure 5.5, we present four maps with 4, 10, 20, and 100
candidates, created using the FR embedding. As we can see, the maps for 10, 20,
and100candidatesaresurprisinglysimilar. Thelargestdifferencewecanobserve
isforthebalancedgroup-separableelections—forthecaseof100candidatesthey
are clearly separated, while for 10 and 20 candidates they are mingling with the
Circleelections. Inaddition,multidimensionalEuclideanelectionsarebettersep-
arated for 100 candidates than for 10 or 20 candidates. Another interesting thing
is that the Walsh elections are shifting toward the right side of the embedding as
we increase the number of candidates. At the same time, the caterpillar group-
separableelectionsareshiftingtowardAN.
Only the map with four candidates is different. Yet, the main shape is still
maintained. Note that for 4 candidates, there are only 24 possible different votes,
which likely explains why the map is not as meaningful. In principle, elections
with only four candidates are very similar to each other (we confirm this in Sec-
tion6.4).
ParametrizedModels
Some of the statistical cultures that we study are parametrized. We are especially
interested in the urn and Mallows models, because for both these models, de-
pendingonthevaluesoftheirparameterswecangenerateelectionsthatareeither
similartoIDortoUNorliesomewhereinbetween. Whatwewouldliketoverify
is whether the urn or Mallows elections with a given parameter occupy the same
partofthemapregardlessofthenumberofcandidates.
Urn Model Interestingly, given certain parameters α, the urn model behaves
the same no matter how many candidates we have. In Figure 5.6 we present the
90(a)4candidates&100voters (b)10candidates&100voters
(c)20candidates&100voters (d)100candidates&100voters
Figure5.5: Mapsofelectionswithdifferentnumberofcandidates.
comparison of the urn model elections with 100 voters and 10, 20, and 40 candi-
dates. Wegeneratedsixpathsbetweenthecompasspoints,20pointseach,and80
elections from the urn model, where α was sampled from the Gamma(0.8,1)
distribution.1 Aswecansee,allthreemapsaresimilartoeachother.
1Itisfarfromobvioushowtosensiblysampletheαparameter.Firstly,itsdomainisunbounded
ononeside.Secondly,thelargeraretheαvalues,thesmallerarethedifferencesbetweenelections
generatedusingthem. Forexample,changingαfrom0.1to0.3isinfluencingtheresultfarmore
thanchangingitfrom2.1to2.3. Thismeansthatwewouldliketohaveadecreasingprobability
densityfunction. Takingtheaboveintoconsideration,wesuggestusingtheGammadistribution.
Aparticularselectionofparameters(thatis,shapeequalto0.8,andscaleequalto1)producesthe
outcomewheremoreorlesshalfoftheurnelectionslieclosertoIDandtheotherhalfliecloser
toUN.
91(a)10×100 (b)20×100 (c)40×100
Figure5.6: Scalabilityoftheurnmodel;α followstheGammadistribution.
Mallows Model Unfortunately, for the standard Mallows model (i.e., not nor-
malizedone),givenacertainϕparameter,themorecandidateswehave,thecloser
our elections are to the identity ones. It means that when comparing the results
ofsomeexperimentwithafixedϕanddifferentnumbersofcandidates,wemight
get a false impression of some phenomena. Fortunately, Boehmer et al. [2021]
propose a useful way of normalizing the ϕ parameter by the number of swaps—
whereby, if we use the normalized parameter, we maintain the same position be-
tween uniformity and identity even if we change the number of candidates. In
Figure 5.7 we present a comparison of the standard Mallows model and its nor-
malized variant, for elections with 100 voters and 10, 20, and 40 candidates. As
before,wegeneratedsixpathsbetweenthecompasspoints,20pointseach,and80
electionsfromtheMallows(Norm-Mallows)model,whereϕ(norm-ϕ)wassam-
pled from the uniform distribution. In the upper row, we show the results for
the Mallows model, and in the lower row for the Norm-Mallows model. For the
Norm-Mallows model, all three pictures look more or less the same, as was the
casefortheurnmodel. However,forelectionsfromthestandardMallowsmodel,
we see that the more candidates we have, the more are the points shifted toward
theidentity.
92(a)10×100 (b)20×100 (c)40×100
Figure 5.7: Scalability of the Mallows model. Teal points in the upper row rep-
resent elections from the Mallows model, while blue points in the lower row rep-
resent elections from the Normalized Mallows model; ϕ and norm-ϕ follow the
uniformdistribution.
MainConclusions
• Depending on the context, all three embedding algorithms (i.e., FR,
KK,andMDS)mightbeareasonablechoice,withFRgeneratingthe
mostscatteredmaps,andMDStheleast. Nonetheless,withregardto
monotonicityanddistortioncriteria,KKperformsbest.
• Map of elections framework is not recommended for maps with few
candidates.
• One has to be careful when comparing elections of different sizes
(in particular, when comparing elections with different numbers of
candidates).
935.4 Voting Rules
To demonstrate the usefulness of our map framework, we show several practical
applications related to analysis of voting rules. We use the same elections as de-
scribed in Table 5.1 with 100 candidates and 100 voters. First we focus on scores
obtained by winning candidates for single-winner rules, and winning committees
formultiwinnerrules. ThenwelookmorecloselyattherunningtimeofILP-based
algorithmsforselectedNP-hardrules. Finally,wefocusonseveralapproximation
methods used to approximate the highest score for the Chamberlin–Courant and
Harmonic-Bordavotingrules.
5.4.1 Score
Inthissectionwepresentthebehaviorofvariousvotingrulesonthemap. Westart
withsingle-winnervotingrulessuchasthePlurality,Borda,Copeland,andDodg-
son. Next, we consider the following two multiwinner voting rules: Chamberlin–
CourantandHarmonic-Borda.
Single-winnerVotingRules
Allrulesthatwediscussworksimilarly,thatis,wecomputeascoreforeachcan-
didateandthenthecandidatewiththehighest(orlowest)scorewinstheelection:
Plurality. Each voter assigns one point to his or her favorite candidate. The can-
didatewiththehighestscorewins.
Borda. Each voter assigns m − 1 points to his or her favorite candidate, m − 2
pointstohisorhersecondfavoritecandidate,andsoon. Thecandidatewith
thehighestscorewins.
Copeland. We examine all pairs of candidates. In each pair, the candidate who
is preferred by more than half of the voters gets a point. In case of a draw,
both candidates receive half a point. The candidate with the highest score
wins.
Dodgson. Condorcetwinnerisacandidatewho,whencomparedone-to-onewith
everyothercandidate,ispreferredbymorethanhalfofthevoters. Foreach
candidate, we check what is the minimum number of swaps of adjacent
candidates in the votes needed to make him or her the Condorcet winner.
Thecandidateforwhomthevalueisthelowestwins.
94Note that if a Condorcet winner exists, he or she will always be selected by
the Copeland and Dodgson rules. A rule that always elects the Condorcet winner
whenoneexistsiscalledaCondorcetExtension.
Example 5.1. Consider an election E = (C,V), where C = {a,b,c,d,e}, V =
(v ,v ,v ,v ,v ),andthevotesareasfollows:
1 2 3 4 5
v : a ≻ c ≻ b ≻ d ≻ e,
1
v : a ≻ c ≻ b ≻ e ≻ d,
2
v : d ≻ e ≻ b ≻ c ≻ a,
3
v : b ≻ e ≻ d ≻ c ≻ a,
4
v : c ≻ b ≻ e ≻ d ≻ a.
5
Accordingtothepluralityrule,aisthewinnerhavingscoreof2,whileallthe
othercandidateshavescoreof0or1. AccordingtotheBordarule,bisthewinner
having 13 points, followed by c with 12 points. Then we have e with 9 points,
and in the end there are a and d having 8 points each. Under the Copeland rule,a
has 0 points (losing all duels), c has 4 points (winning all duels), b has 3 points, d
has 1 point, and e has 2 points. Clearly, c is selected as the winner. Moreover,
notethatcisaCondorcetwinneraswell. Hence,heorshewillalsowinunderthe
Dodgson rule, having the lowest score of 0 (i.e., no swap is needed to make him
orheraCondorcetwinner). △
For each of the elections in our map, we computed a winning candidate and
his or her score. For Plurality, Borda, and Copeland, the higher, the better, while
for Dodgson the lower, the better. Computing the Plurality, Borda, and Copeland
scores is straightforward. To compute the Dodgson score, we used the ILP pro-
posed by Bartholdi et al. [1989]. We present the results in Figure 5.8, where the
colorofeachpointcorrespondstothehighest(lowestforDodgson)scoreobtained
bythewinningcandidate.
ForPlurality,halfoftheelectionshavethehighestscoreofsixorless—which
weregardasverylow. Notmanymodelswitnesshighervalues;nonetheless,letus
have a closer look at them. For caterpillar group-separable elections, the highest
score is around 48 (with more voters, or more elections, the average should con-
verge to 50); for elections from the Walsh model, the highest score is around 12;
for single-crossing elections it is around 35. For the Norm-Mallows model it is
strongly correlated with the norm-ϕ parameter, hence we see shading from UN
to ID, but still most of the points are dark. Only for the urn elections we witness
95(a)HighestPluralityscore (b)HighestBordascore
(c)HighestCopelandscore (d)LowestDodgsonscore
Figure5.8: Mapscoloredaccordingtothescoreobtainedbythewinner.
96Figure5.9: DifferencebetweenthehighestandthelowestBordascore.
numerous elections with high score values, which is not surprising, because for
electionsfromthe urnmodelthehighest plurality scorewillbesimilar tothesize
of the largest group of (identical) voters. Sometimes it might be slightly higher if
twoormoregroupshavethesamefavoritecandidate.
AstotheBordacoloring,weobservemuchsmoothershadingthanforPlural-
ity, because, for Plurality we ignore all the voters’ preferences, but first choices,
while for Borda we care about each position in the votes. Moreover, Borda score
nicely correlates with position on the map. If we move closer to ID the highest
scoreisincreasing,andifwemovetowardstheUNthescoreisdecreasing.
NotethatforPluralityandBordamaps,thepathsarecoloredaswell,because
to compute Plurality or Borda score it suffices to have the position matrix (or
frequency matrix and the number of voters). As to the Copeland and Dodgson
scores,wehaveleftthepaths’pointsuncolored,becausethepositionmatrixisnot
sufficienttodeterminethescore.
Last observation regarding the Borda score is about the Borda balance area
(mentioned previously in Section 4.3.4). We observe that in the left upper part
of the map most elections have very low highest Borda score (close to the lowest
possible). However, if the highest Borda score is close to the lowest possible
Borda score, it means that most candidates need to have very similar scores. To
emphasize this, we present one additional map in Figure 5.9 where the color of
eachpointcorrespondstothedifferencebetweenthehighestandthelowestBorda
score in a given election. As expected, the closer we are to the upper left part of
themap,thesmallerthedifference.
97(a)HighestCCscore (b)HighestHBscore
Figure5.10: Mapscoloredaccordingtothescoreofthewinningcommittee.
For the Copeland and Dodgson maps, with crosses, we mark elections that
haveaCondorcetwinner. Inourmap55%(264outof480)ofgeneratedelections
have a Condorcet winner.2 In particular, almost all the single-crossing, Walsh,
caterpillar group-separable, 3-Cube, 5-Cube, 10-Cube, 20-Cube, and around half
of Interval, Square, Conitzer, balanced group-separable elections have a Con-
dorcet winner. Even one IC elections has a Condorcet winner. In terms of the
urn and Norm-Mallows elections, having a Condorcet winner is strongly related
totheirparameters.
The lowest Copeland score is witnessed by a balanced group-separable elec-
tion. In general, the vast majority of lowest values are obtained by Circle, Sphere
and 4-Sphere and some SPOC and balanced group-separable elections. Note that
all values are much larger than the lowest possible value, which is 49.5 with ev-
eryonehavingexactlythesameCopelandscore. Suchanelectioncanbeachieved
by,forexample,takingtheidentityelectionandreversinghalfofthevotes.
Thehighest(i.e.,theworst)Dodgsonscoreiswitnessedbyanurnelection. In
general, the highest scores are obtained by the urn elections and some balanced
group-separable,Circle,Sphereand4-Sphere,SPOCelections.
2Elections that are single-peaked, single-crossing, or group-separable, for an uneven number
ofvoters,alwayshaveaCondorcetwinner,andforanevennumberofvoters,alwayshaveaweak
Condorcetwinner(orwinners).
98MultiwinnerRules
We consider two multiwinner voting rules, Harmonic-Borda (HB) and
Chamberlin–Courant (CC). We start by defining HB. Given an election E =
(C,V) and committee size k, the rule outputs a set of k candidates, referred to as
thewinningcommittee. Itchoosesthiscommitteeasfollows: Consideracommit-
teeS,avoterv,anddenotebyp ,...,p thepositionsofthemembersofS,sorted
1 k
from the smallest (most preferred) to the largest (e.g., for a vote v: c ≻ c ≻ c
2 3 1
and committee S = {c ,c }, we would have p = 2, p = 3). Then the satisfac-
1 3 1 2
(cid:80)
tion of v is (m−pi)/ i; it captures the notion of how a given voter is satisfied
i∈[k]
with a given committee. HB selects a committee S that maximizes the sum of
the voters’ satisfaction values. CC is similar to HB, but simpler. Under CC, each
voter gives points only to his or her favorite candidate. Formally, the satisfaction
ofv ism−p .
1
Example 5.2. Consider an election E = (C,V), where C = {a,b,c,d}, V =
(v ,v ,v ,v ),andthevotesare:
1 2 3 4
v : a ≻ b ≻ c ≻ d
1
v : a ≻ b ≻ c ≻ d,
2
v : a ≻ b ≻ c ≻ d,
3
v : d ≻ c ≻ b ≻ a.
4
AccordingtotheCCrule,{a,d}isthewinningcommitteewithscoreof12(which
is the largest possible score for elections with 4 candidates and 4 voters). Under
the HB rule, {a,b} is the winning committee with score of 13, with first three
voters giving 4 points each (3·1 for a and 2· 1 for b), so 12 in total, and the last
2
votergiving1point(1·1forband0· 1 fora)
2
△
RulessuchasCCorHBhavereceivedquitesomeattentionfromtheresearch
community (for more details, see, e.g., the chapter of Faliszewski et al. [2017a]).
Both CC and HB are OWA-based [Lang and Skowron, 2018] committee scoring
rules[Faliszewskietal.,2016]).
Unfortunately, identifying a winning committee under CC is NP-hard [Pro-
caccia et al., 2012, Lu and Boutilier, 2011, Betzler et al., 2013] and the same is
true for HB [Faliszewski et al., 2017b], but we can try to overcome this issue, for
example,byformulatingtheproblemasanintegerlinearprogram(ILP)andsolv-
ing it with an off-the-shelf ILP solver, or by designing efficient polynomial-time
99approximation algorithms. We show how our map can be helpful in establish-
ing how feasible the ILP approach is (i.e., how quickly can we compute winning
committees).
For each of the elections on our map, we computed a winning committee of
size10usinganILPsolver(CPLEX;weusedtheILPformulationforOWA-based
rulesofSkowronetal.[2016],appliedtothecaseofHBandCC).
InFigure5.10,wepresentthescoresobtainedbythewinningcommitteeunder
CC (left) and HB (right). In both pictures, we can see nice shading from ID to
UN for the Norm-Mallows elections. However, when we look at urn elections,
we see a huge difference; for HB the shading of urn elections is similar to that
of Norm-Mallows ones, but for CC, the vast majority of urn elections witness
a very high score. It is because in the urn model, we have groups of identical
voters,soacommitteeselectedbytheCCrulewillusuallysatisfyk largestgroup
of voters, by selecting their top candidates. Another striking difference is the
behavior of group-separable caterpillar elections. Under CC, all of them get the
highestpossiblescore,whileunderHBthescoresarerelativelysmall.
It is also interesting to consider elections where our rules have the lowest
scores of the winning committees. For CC, most of them are witnessed by the IC
elections, while for HB the lowest scores are witnessed by the Circle, Sphere, 4-
Sphere,andSPOCmodels,withtheICelectionshavingnoticeablyhigherscores.
5.4.2 Running Time
In this part, we analyze the time that is needed to compute the outcomes of the
votingrulesdescribedabove. Fourofthem,thatis,Plurality,Borda,Copeland,the
runningtimereliesonlyontheinputsize(i.e.,numbersofcandidatesandvoters);
hence, it is hard to conclude anything interesting. We focus only on Dodgson
(single-winnerrule),andCCandHB(multiwinnerrules).
We report the achieved running times in Figure 5.11, where the colors give
the running times (the darker the color, the longer the computation time; for the
CC picture we set a limit3 at 60 seconds, and for HB and Dodgson ones we set a
limitat600seconds),andallinstancesthatneedlongertimehavethesamecolor.
Moreover,thescalefortheHBpictureisquadratic.4
3Allinstancesthattooklongertocomputethanthelimitarecoloredwiththesamecolor.
4We would prefer to have the same scale for all three pictures, however if all three pictures
wouldhavelinearscaleitwouldhavebeenhardtoseeanythinginterestingforCCandHB,andif
allthreepictureswouldhavequadraticscaleitwouldhavebeenhardtoseeanythinginteresting
forDodgson.
100(a)CCrunningtime (b)HBrunningtime
(c)Dodgsonrunningtime
Figure5.11: ILPsrunningtime(inseconds)forCC,HB,andDodgson.
101(a)CC (b)HB
(c)Dodgson
Figure 5.12: ILPs running time (in seconds) vs distance from ID. Note that, for
theDodgsonpicture(right)weusethelinearscale,whilefortheHBpicture(left)
weuselogarithmicone.
We start our analysis with the CC rule by looking at the running time of par-
ticularinstances. Theworstcase(i.e.,thelongestrunningtime)tooktwominutes
tosolve,whilethesimplestone(i.e.,theshortestrunningtime)tooklessthanone
second. ThetwentyworstcaseswereduetoimpartialcultureandNorm-Mallows
(with norm-ϕ parameter having close to 1 value) instances. On the other hand,
thesimplestoneswerethosefromtheurnmodel.
As to the HB rule, the worst case took five hours to solve, while the sim-
plestonetooklessthantwoseconds. Tenworstcaseswerewitnessedby4-Sphere
elections—whichsuggeststhatitisparticularlyhardtofindoptimalwinningcom-
mitteeunderHBruleforelectionsfromthe4-Spheremodel.
PerhapsthemostvisiblephenomenonisthattheILPsolverneedsmosttimeon
the elections similar to those from the impartial culture, and the farther elections
weconsider,thelesstimeisneeded.
102Culture CC HB Dodgson
avg. time std. dev. avg. time std. dev. avg. time std. dev.
ImpartialCulture 43.8s 22.8 155.7s 85.5 251.4s 17.2
ConitzerSP 0.8s 0.0 12.2s 2.2 252.7s 14.9
WalshSP 0.7s 0.0 3.6s 0.2 281.1s 5.1
SPOC 1.1s 0.0 22.3s 4.1 230.0s 8.0
Single-Crossing 0.9s 0.0 7.1s 0.6 235.6s 4.4
Interval 0.8s 0.0 11.8s 1.4 213.2s 7.8
Square 1.4s 0.3 20.6s 9.1 249.9s 7.3
Cube 2.7s 1.0 11.4s 1.8 254.1s 10.6
5-Cube 3.7s 1.0 10.8s 2.0 251.0s 6.7
10-Cube 3.5s 0.9 12.0s 3.4 249.3s 5.2
20-Cube 2.8s 1.0 12.1s 3.0 249.7s 6.0
Circle 1.0s 0.0 22.9s 3.7 222.3s 2.9
Sphere 1.9s 0.6 126.8s 64.9 230.5s 5.0
4-Sphere 6.5s 2.5 1614.1s 3912.1 232.3s 6.0
BalancedGS 1.2s 0.2 22.2s 6.9 223.0s 2.1
CaterpillarGS 1.2s 0.0 44.7s 14.3 513.0s 106.1
Urn 1.1s 2.8 11.1s 13.1 30.0s 35.1
Norm-Mallows 8.6s 17.9 17.3s 24.1 264.2s 19.7
Table5.4: AnalysisofILPsrunningtimeforCC,HBandDodgsonrules.
103For the Dodgson rule the situation is quite different. Most instances need
the same amount of time (i.e., around four minutes on average). Two exceptions
are the urn elections (which needed half a minute per election on average) and
caterpillar group-separable elections (which needed eight and a half minutes per
election on average). In Table 5.4 we present average values and standard devia-
tion for each statistical culture that we used. Moreover, in Figure 5.12 we show
thecorrelationbetweentherunningtimeanddistancefromID. Aswecansee,for
the CC and HB rules all the hardest instances were at almost the largest possible
distancefromID,whilefortheDodgsonruleitisnotthecase. Although,usually
thelessstructureintheelectionthelongerittakestocomputeagivenvotingrule,
sometimes(like,forexample,inthecaseoftheDodgsonrule)addingstructureto
theelectionmightincreasetherunningtimeofaparticularalgorithm.
5.4.3 Approximation
We compare four approximation algorithms for the CC multiwinner voting rule.
We refer to them as SeqCC (sequential, sometimes also referred to as the greedy
variant), RemovalCC, RangingCC, and BanzhafCC. At the second part of this
section, we also compare two approximation algorithms for the HB multiwinner
votingrule.
Let k ∈ [m] be the size of the committee we want to select, where m is the
totalnumberofcandidates. Belowwedescribethesefouralgorithms.
SeqCC starts with an empty committee and works in k iterations, where in
each of them it adds to the committee a single candidate, so that the resulting
committee has as large total satisfaction as possible. RemovalCC proceeds simi-
larly, but it starts with a committee containing all candidates and works in m−k
iterations, in each of them removing a single candidate, so the resulting commit-
tee has as large total satisfaction as possible. Both algorithms are well-known in
the literature and are used for computing approximate winning committees under
various voting rules [Skowron et al., 2016, 2017, Faliszewski et al., 2018], for
example,CCorHB.
We also consider RangingCC algorithm [Skowron et al., 2015, Elkind et al.,
2017],whichwasdesignedespeciallyfortheCCvotingrule.
The last algorithm we discuss is BanzhafCC [Faliszewski et al., 2018], which
is a special variant of SeqCC method. When deciding which candidate should
be added to the committee in each step, it is using the concept of the Banzhaf
index. Briefly put, SeqCC always adds the candidate that currently leads to the
highest score. BanzhafCC, on the other hand, adds the candidate that is expected
104(a)SequentialCC (b)RemovalCC
(c)BanzhafCC (d)RangingCC
Figure5.13: ComparisonofapproximationalgorithmsforCC.
105Figure5.14: SeqCCversusRemovalCC.
tomaximizethecommitteescoreifallbutonemissingcommitteememberswere
chosen randomly. For details, we point the readers to the work of Faliszewski
et al. [2018] and Munagala et al. [2021]. (Both RangingCC and BanzhafCC can
serveasabaseforpolynomial-timeapproximationschemesforCC).
We evaluate the approximation algorithms by computing the approximation
ratio, that is, the score of the winning committee selected by the approximation
algorithmsdividedbythescoreofthewinningcommitteeselectedbytheoptimal
method. Thelargertheapproximationratio,thebetter.
In Figure 5.13, we present the results. For all four approximation algorithms,
the closer we are to UN, the worse is the approximation ratio. Moreover, the
results for SeqCC and RemovalCC are significantly better than for BanzhafCC
and RangingCC. Finally, RangingCC is clearly the worst one. Nonetheless, all
fouralgorithmsonaveragehaveveryhighapproximationratios(closetoone).
What is interesting for Banzhaf, is the fact that even though Walsh elections
are relatively close to ID, the approximation ratio is quite bad in comparison to
theelectionslyingnexttothem(e.g.,theNorm-Mallowelections).
In Figure 5.14, we present a comparison of SeqCC and RemovalCC, where
thebluepointsrefertoplaceswhereSeqCCisbetteratapproximatingCC,thered
pointsmarktheelectionswhereRemovalCCisbetter,andfinallythegreenpoints
depictelectionswherethereisadraw.
On most single-crossing, around half of the urn and Norm-Mallows elections
106(a)SequentialHB (b)RemovalHB
Figure5.15: ComparisonofapproximationalgorithmsforHB.
(those with low norm-ϕ), some caterpillar group-separable and three Walsh elec-
tionsthereisadraw. Foralltherest,oneortheotherofthealgorithmswasbetter.
When there was no tie, for the majority of the instances, RemovalCC was
better. The only three models for which for more than half of the instances Se-
qCC was better are the IC, 10-Cube, and Norm-Mallows. This means that for
low dimensional Euclidean elections RemovalCC was much better, and for high
dimensionalEuclideanelections,like10-Cubeelections,SeqCCwasbetter.
Next, we move on to a similar experiment, but for the HB rule. We study
two approximation algorithms, to which we refer to as SeqHB and RemovalHB.
ThesetwomethodsaredefinedanalogouslytotheSeqCCandRemovalCC.InFig-
ure 5.15 we present results individually for each of the algorithms, and in Fig-
ure 5.16 we present them jointly. The performance of both algorithms is almost
excellent. Inapproximatelyhalfoftheinstances(239outof480)anoptimalsolu-
tion was found by both methods. In 153 instances solution found by RemovalHB
wasbetter,andin82instancestheonefoundbySeqHBwasbetter.
For the Walsh and caterpillar group separable elections almost always an op-
timal solution was found, however, sometimes it was only found by RemovalHB,
while the solution found by SeqHB was suboptimal. In most elections from the
Interval,Conitzer,andbalancedgroup-separable,RemovalHBwasbetterthanSe-
qHB.
While SeqCC and SeqHB have approximation guarantees of 1 − 1, Re-
e
movalCCandRemovalHBdonothaveanysuchguarantees.
107Figure5.16: SeqCCversusRemovalCC.
MainConclusions
TheconclusionsfromSection5.4areasfollows.
• Electionsthataresimilar(i.e.,areatsmalldistancefromoneanother)
behave similarly under different voting rules. For instance, similar
electionsobtainsimilarscoresunderscoringvotingrulesandrequire
similaramountoftimetocomputewinnersunderagivenvotingrule.
Forinstance,usuallythecloserelectionsaretoIC,thelongerittakes
tocomputetheiroutcomes. However,therearesomeexceptions. For
example, for the Dodgson voting rule, the longest time was witness
by elections from group-separable statistical culture, which are far
awayfromIC.
• While SeqCC and SeqHB have approximation guarantees of 1 − 1
e
(and even a stronger one for SeqCC), in practice we observe that
they are outperformed by RemovalCC and RemovalHB, which do
nothavesuchguarantees.
1085.5 Real-Life Instances
Next, we focus on real-life elections. We have several sources of our datasets,
where the most prominent two are PrefLib [Mattei and Walsh, 2013] and the
work of Boehmer and Schaar [2023]. There are two main problems with real-
life elections. One is the fact that usually there are few candidates participating.
The second one is that in many cases, numerous votes are incomplete. In the be-
ginning, we will describe how to preprocess the data in general, for example, to
have complete preference orders. Then we will describe our datasets one by one,
andhowwepreprocessedeachofthemspecifically—someofthedatasetsneeded
some special treatment. Then, we present where these real-life elections land on
our maps of elections. Finally, we will try approximating real life elections with
theNorm-Mallowsmodel,i.e.,wewillsearchforsuchnorm-ϕparameterssothat
electionssampledfromtheNorm-Mallowsmodelwiththisparameterareasclose
tothereal-lifeelectionsaspossible.
Whenever we speak of real-life elections in this section, we mean elections
fromourdatasets.
SelectionofDatasets
In Table 5.5, we present a detailed description of the selected datasets. All of
themareavailableatPrefLib. Wechoseelevenreal-lifedatasetsofdifferenttypes,
andwedividedthemintothreecategories. Thefirstgroupcontainspoliticalelec-
tions: citycouncilelectionsinGlasgowandAspen[O’Neill,2013],electionsfrom
Dublin North and Meath constituencies (Irish), and elections held by non-profit
organizations, trade unions, and professional organizations (ERS). The second
group consists of sport elections: Tour de France (TDF) [Boehmer and Schaar,
2023], Giro d’Italia (GDI) [Boehmer and Schaar, 2023], speed skating [Boehmer
et al., 2021], and figure skating. The last group consists of surveys: preferences
over Sushi [Kamishima, 2003], T-Shirt designs, and costs of living and popula-
tion in different cities [Caragiannis et al., 2019]. For TDF and GDI, each race is
a vote, and each season is an election. For speed skating, each lap is a vote, and
eachcompetitionisanelection. Forfigureskating,eachjudge’sopinionisavote,
andeachcompetitionisanelection.
109Category Name #ValidElections Avg.m Avg.n Description
Political Irish 2 13 ∼54011 ElectionsfromDublinNorthandMeath
Political Glasgow 13 ∼11 ∼8758 Citycouncilelections
Political Aspen 1 11 2459 Citycouncilelections
Political ERS 13 ∼12 ∼988 Variouselectionsheldbynon-profitorganizations,
tradeunions,andprofessionalorganizations
Sport FigureSkating 40 ∼23 9 Figureskating
Sport SpeedSkating 13 ∼14 196 Speedskating
Sport TDF 12 ∼55 ∼22 TourdeFrance
Sport GDI 23 ∼152 20 Girod’Italia
Survey T-Shirt 1 11 30 PreferencesoverT-Shirtlogo
Survey Sushi 1 10 5000 PreferencesoverSushi
Survey Cities 2 42 392 Preferencesovercities
Table 5.5: Each row contains a description of one of the real-life datasets we
consider. In the column # Selected Elections, we denote the number of elections
wefinallyselectfromtherespectivedataset.
We are only interested in elections that have at least 10 candidates5. As our
model only allows us to consider complete votes without ties, we are interested
in instances where votes are as complete as possible and contain only a few ties.
In some datasets, only parts of the data meet our criteria (i.e., complete votes
without ties over at least 10 candidates). For example, in the dataset containing
Irishelections,wehavethreedifferentelections,butoneofthem(anelectionfrom
the Dublin West constituency) contains only nine candidates. We delete all such
elections. After doing so, we finally arrive at eleven real-life datasets containing
electionsmeetingourcriteria.
Aswecannotincludeallelectionsfromeachdatasetonthemapofelections6,
we further reduce the number of elections by considering only selected elections.
In Table 5.5, we include in the column # Valid Elections the number of elections
weselectedfromeachdatasetintheend. Webasedourdecisiononthenumberof
voters and candidates. That is, for ERS, we only take elections with at least 500
voters, for Speed Skating with at least 80 voters, for TDF with at least 20 voters,
andfor FigureSkatingwithat least9. Inaddition tothat,forTDF, weonlyselect
electionswithnomorethan75candidates.
5Themorecandidates,themoreinterestingisthedata. Ontheotherhand,ifwehadrequired
too many candidates, we would end up having very few instances. In that sense, 10 candidates
isatradeoffbetweenthenumberofcandidates,andthenumberofinstancesthathaveatleast10
candidates.
6If there are too many elections embedded jointly on a single map, the picture is becoming
unclearanddifficulttointerpret.
110PreprocessingofDatasets
Therearetwotypesofproblemsthatweencounterinselecteddatasets. First,ties
(i.e.,pairsorlargersetsofcandidatesthatarereportedasequallygood). Anyties
thatappearwebreakrandomly. Second,incompletevotes(i.e.,voteswheresome
ofthetopcandidatesarerankedandtheremainingcandidatesarenot). Sometimes
bothproblemshappenatthesametime.
For all elections from our selected datasets that contain incomplete votes, we
need to fill-in all the missing data. For the decision how to complete each vote,
we use the other votes as references, assuming that voters that rank the same
candidatesontopalsocontinuetorankcandidatessimilarlytowardthebottom.
For each incomplete vote v, we proceed as follows. Let us assume that vote v
isoverm′ candidates. LetV bethesetofalloriginalvotesofwhichv isaprefix.
P
We uniformly at random select one vote v from V and then at the end of vote v
p P
we add candidate which is at position m′ +1 in vote v . We repeat the procedure
p
until vote v is complete. If the set V is empty, then we choose c uniformly at
P
random(fromthosecandidatesthatarenotpartofv yet).
After applying these preprocessing steps, we arrive at a collection of datasets
containingelectionswithtenormorecandidatesandcompletevoteswithoutties.
As we focus on ten candidates, we need to select a subset of ten candidates for
each election. In a given election we compute the Borda score of each candidate,
and select ten candidates with the highest ones. In case there is a tie, we break it
randomly.
Werefertotheresultingdatasetsasintermediatedatasets.
SamplingElectionsfromtheIntermediateDatasets
Wetreateachofourintermediatedatasetsasaseparateelectionmodelfromwhich
we sample 15 elections to create the final datasets that we use. For each interme-
diatedataset,wesampleelectionsasfollows. First,werandomlyselectoneofthe
elections present internally in it (for example, the election held in Dublin North
constituencyfromtheIrishdataset). Second,wesample100votesfromthiselec-
tion uniformly at random (this implies that for elections with less than 100 votes,
we select some votes multiple times, and for elections with more than 100 votes,
wedonotselectsomevotesatall). Wedosotomakefulluseofelectionswithfar
more than 100 votes. For instance, our Sushi intermediate dataset contains only
one election consisting of 5000 votes. Sampling an election from the Sushi inter-
mediatedatasetthuscorrespondstodrawing100votesuniformlyatrandomfrom
111the set of 5000 votes. On the other hand, for intermediate datasets containing a
highernumberof elections,e.g.,the TourdeFranceintermediatedataset, mostof
thesampledelectionscomefromdifferentoriginalelections.
After executing this procedure, we arrive at eleven sets, each containing 15
elections consisting of 100 complete and strict votes over 10 candidates, which
weuseforourexperiments.
5.5.1 Real-Life Elections on the Map
In Figure 5.17, we show a map of our real-life elections along with the compass,
Mallows,andurnelections. Forreadability,wepresenttheMallowsandurnelec-
tions as large, pale-colored areas. Not all real-life elections form clear clusters,
hencethelabelsrefertothelargestcompactgroupings.
While the map is not a perfect representation of distances among elections,
nevertheless, analyzing it leads to many conclusions. Most strikingly, real-life
electionsoccupyaverylimitedareaofthemap;thisisespeciallytrueforpolitical
elections and surveys. Except for several sport elections, all elections are closer
to UN than to ID, and none of the real-life elections falls in the top-right part of
the map. Another observation is that Mallows elections go right through the real-
lifeelections,whileurnelectionsareonaveragefurtheraway. Thismeansthatfor
most real-life elections there exists a parameter ϕ such that elections generated
according to the Mallows model with that parameter are relatively close (see the
nextsectionforspecificrecommendations).
Most of the political elections lie close to each other and are located next to
the Mallows elections and high-dimensional hypercube ones. At the same time,
sport elections are spread over a larger part of the map and, with the exception
of GDI, are shifted toward ID. Regarding the surveys, the Cities survey is very
similar to a sample from IC.7 The Sushi survey is surprisingly similar to political
elections. The T-Shirt survey is shifted toward stratification (apparently, people
oftenagreewhichdesignsarebetterandwhichareworse).
5.5.2 Capturing Real-Life Elections
Let us now analyze how to choose the norm-ϕ parameter so that elections gener-
atedusingtheMallowsmodelwithournormalizationresemblethereal-lifeones.
7In the survey people were casting votes in the form of truncated ballots, ranking only their
sixfavoriteoptions. ThisispartlythereasonwhyitissosimilartoIC.Nevertheless,wehavenot
observedanyparticularstructurewithinthesevotes,henceitssimilaritytoICisnotaccidental.
112Figure5.17: Mapofreal-lifeinstances.
We consider four different datasets, each consisting of elections with 10 candi-
datesand100voters(createdasdescribedinSection5.5.1): thesetofallpolitical
elections, the set of all sport elections, the set of all survey elections, and the
combined set, i.e., the union of the three preceding ones. For each of these four
datasets, to find the value of norm-ϕ that produces elections that are as similar
as possible to the respective real-life elections, we conducted the following ex-
periment. For each norm-ϕ ∈ {0,0.001,0.002,...,0.999,1}, we generate 100
elections with 10 candidates and 100 voters from the Mallows model with the
given norm-ϕ parameter. Subsequently, we compute the average distance be-
tween these elections and the elections from the respective dataset. Finally, we
select the value of norm-ϕ that minimizes this distance. We present the results of
thisexperimentinTable5.6.
Recall that in the previous section we have observed that a majority of real-
lifeelectionsareclosetosomeelectionsgeneratedfromtheMallowsmodelwitha
certaindispersionparameter. However,wehavealsoseenthatthereal-lifedatasets
consist of elections that differ to a certain extent from one another (in particular,
this is very visible for the sports elections). Thus, it is to be expected that elec-
tionsdrawnfromtheMallowsmodelforafixeddispersionparameterareatsome
nonzero (average) distance from the real-life ones. Indeed, this is the case here.
However, the more homogeneous political elections and survey elections can be
captured quite well using the Mallows model with parameter norm-ϕ = 0.750
and norm-ϕ = 0.730, respectively. Generally speaking, if one wants to generate
113Typeofelections Valueof Avg. Norm. Norm. Std. Num. of
norm-ϕ Distance Dev. Elections
Politicalelections 0.750 0.15 0.036 60
Sportelections 0.534 0.27 0.080 60
Surveyelections 0.730 0.20 0.034 45
Allreal-lifeelections 0.700 0.22 0.106 165
Table5.6: Valuesofnorm-ϕsuchthatelectionsgeneratedwiththeMallowsmodel
for m = 10 are, on average, as close as possible to elections from the respective
dataset. We include the average distance of the elections generated with Mallows
model for this parameter norm-ϕ from the elections from the dataset as well as
the standard deviation, both normalized by distance between the uniformity and
identity. The last column gives the number of elections in the respective real-life
dataset.
elections that should be particularly close to elections from the real world, then
choosing a norm-ϕ value between 0.7 and 0.78 seems like a good strategy. If,
however, one wants to capture the full spectrum of real-life elections, then we
recommend using the Mallows model with different values of norm-ϕ from the
interval[0.5,0.8].
MainConclusions
TheconclusionsfromSection5.5areasfollows.
• Real-life elections that we study witness surprisingly little antago-
nism.
• Mostofthepoliticalelectionsarequitesimilartoeachother,andcan
beapproximatedbytheNorm-Mallowsmodelwithnorm-ϕ = 0.75.
5.6 Skeleton Map
Ourfinalgoalinthischapteristoformwhatwecallaskeletonmapofvotedistri-
butions(skeletonmap,forshort),evaluateitsqualityandrobustness,andcompare
ittothepreviousmaps,thusgettingsomeinsightsregardingthequalityandcredi-
bilityofthelatter. AtrivialversionofaskeletonmapwaspresentedinFigure5.1,
114whereweshowedamapwithonlycompassmatricesandpathsbetweenthem.
Boehmer et al. [2022a] proved that for some statistical cultures, it is possible
to create frequency matrices of distributions. In other words, instead of doing
it empirically (i.e., sampling numerous elections from a given model, computing
frequencymatricesforeachofthem,andthencreatingafinalmatrixastheaverage
over these matrices), we can analytically calculate the expected frequency matrix
of a given distribution—i.e., if we had sampled infinitely many elections, and
took the average over their frequency matrices, we would have obtained such an
expectedfrequencymatrix.
We start with the Mallows model. Let Φ = {0,0.05,0.1,...,1} be a set of
normalized dispersion parameters that we will use for Mallows-based distribu-
tions.
For a given number of candidates, we consider the four compass matrices
(UN, ID, AN, ST) and paths between each matrix pair consisting of their convex
combinations (denoted by gray dots on the map, which will be shown later), the
frequency matrices of the Norm-Mallows distribution with its normalized disper-
sion parameters from Φ (denoted by blue triangles), and the frequency matrices
of Conitzer (CON), Walsh (WAL), and group-separable caterpillar (CAT). More-
over, we add the frequency matrices of the following vote distributions (we again
usethedispersionparametersfromΦ):
1. The distribution for Norm-Mallows with ω = 0.5 and ω = 0.25 (denoted
byredandgreentriangles,respectively),
2. The ϕ-Conitzer and ϕ-Walsh distributions where first we sample a vote v
from the Walsh and Conitzer distributions, and then we sample the final
vote from the Mallows distribution with dispersion parameter ϕ and with v
asthecentralvote(denotedbymagentaandorangecrosses,respectively).
In Figure 5.18 we show our map for the case of 10 candidates. The lines be-
tweensomepoints/matricesshowtheirpositionwisedistances(tomaintainclarity,
weprovideonlysomeofthem). ThemapwascreatedusingtheMDSembedding.
We now verify the credibility of the skeleton map. As the map does not have
many points, we expect its embedding to truly reflect the positionwise distances
betweenthematrices. This,indeed,seemstobethecase,althoughsomedistances
arerepresented(much)moreaccuratelythanothers.
In Figure 5.19 we provide the positionwise distances between the several se-
lected matrices (for m = 10; matrix M2W is the Mallows matrix in our data set
that is closest to the Walsh matrix), and in Figure 5.20 for each pair of selected
115AN
0.19
0.38
CAT
0.33
0.32 MID
0.33 0.66 0.38
UN 0.33 0.77 ID
CON
0.44
0.39
0.65
WAL
0.34
ST
Figure 5.18: The skeleton map with 10 candidates. We have MID = 1/ 2AN +
1/ 2ID.
matrices we report the distortion (the smaller the value, the more accurate the
embedding, recall Section 5.3.2). Most of the distortions are below 1.5, with the
majority being below 1.2, and all but one are below 2.58. Thus, in most cases,
themapisquiteaccurateandoffersgoodintuitionabouttherelationsbetweenthe
matrices. Yet, some distances are particularly badly represented. As an extreme
example, the Euclidean distance between the Walsh matrix and the closest Mal-
lowsmatrix,M2W,isoffbyalmostafactorof8(thesematricesareclose,butnot
as close as the map suggests). While one always has to verify claims suggested
by the skeleton map, we view it as quite credible. This conclusion is particularly
valuable when we compare the skeleton map and the previous maps. The two
maps are similar, and analogous points (mostly) appear in analogous positions.
Perhaps the biggest difference is the location of the Conitzer matrix on the skele-
ton map and Conitzer elections in the previous maps, but even this difference is
not huge. We remark that the Conitzer matrix is closer to UN and AN than to ID
and ST, whereas for the Walsh matrix the opposite is true. We made a similar
observationbefore;ourresultsallowustomakethisclaimformal.
8NotethattheaveragedistortionforMDSmappresentedinFigure5.2was1.315
116Figure 5.19: Normalized positionwise Figure5.20: DistortionoftheMDSem-
distancesbetweenselectedmatrices. beddingoftheskeletonmap.
SkeletonMapforDifferentNumbersofCandidates
A natural question to raise is whether the map would look similarly if we had
taken different numbers of candidates. In Figure 5.21 we present three additional
skeleton maps with 5, 25, and 50 candidates. We see that the maps in general
are quite similar, however some of the models are “moving”. In particular, we
observe that when we increase the number of candidates, Walsh model is shifting
towardsID,andgroup-separablecaterpillarmodelisshiftingtowardsAN. Atthe
sametime,theConitzermodelisstayingalmostinthesameplace.
MainConclusions
The "skeleton" map witnesses lover distortion than the standard map of
elections. It confirms general intuition about the positions of particular
statisticalculturesonthemap.
5.7 Summary
Themainreasonbehindtheanalysisgiveninthischapterwastojustifytheuseof
particular parameters, embeddings, etc., and showing how selecting one embed-
ding or another can influence the outcome. The second reason was to prove the
practicality of the map, as well as its usefulness. We presented how the map of
1170.31AN
C0. A16 TAN
0.39
C0 A.1 TAN
0.38
UN0.28 C 0.A 3CT O0. 03 N .3 50 3.4
0.2
W0 3.4 A80 L.44 MID 0.78 0.590.44 ID UN0.39 0.30 3. C33 ON0.35 0.7 0.5M 2ID 0.76 00 .2.3 99 ID UN0.42 0.330 C.3 O3 N0.33 0.840.74 0M .0 5. 9I 7D 6 0.3 08
.21
ID
0.77 WAL
0.53 WAL
0.34 0.37
ST
ST ST
(a)5candidates (b)25candidates (c)50candidates
Figure5.21: Skeletonmapfordifferentnumberofcandidates.
elections framework can be used to study the behavior of voting rules. We also
showedtherelationshipbetweensyntheticelectionsandreal-lifedata.
Someofthemostinterestingfindingsarethefollowing. Formanyvotingrules
(e.g., Borda) there is a correlation between the position of a given election on the
map and the result (e.g., the highest Borda score in that election). While Urn
model is easily scalable, with Mallows model we should be more cautious and
depending on what the goal is, we should decide on using the normalized version
or not. Most of political real-life elections lie in the particular (lower-left) part of
themapandcanbeapproximatedbytheMallowsmodel.
MainContributions
• Evaluationoftherobustnessofthemapofelectionsframework.
• Examplesofusefulnessofthemapofelectionsframework:
– Analysisofvotingrules,i.e.,showingthatdifferentvotingrules
behavedifferentlyondifferenttypesofelections(differentparts
ofthemap).
– Analysis of real-life elections, i.e., showing where they land on
themap,andhowtoapproximatethemwiththeNorm-Mallows
model.
• Introductionofthe"skeleton"mapapproach,whichconfirmsgeneral
intuition about the positions of particular statistical cultures on the
map.
118Chapter 6
Subelections
6.1 Introduction
In this chapter we study the computational complexity of several extensions of
the ELECTION ISOMORPHISM problem, which was introduced in Chapter 4 as
an analogue of GRAPH ISOMORPHISM. While in the latter we are given two
graphs and we ask if they can be made identical by renaming the vertices, in
the former we are given two ordinal elections and we ask if they can be made
identical by renaming the candidates and reordering the voters. As we mentioned
in Chapter 4, even though the exact complexity of GRAPH ISOMORPHISM, as
well as of many related problems, remains elusive, ELECTION ISOMORPHISM
has a simple polynomial-time algorithm. Yet, in many practical settings, perfect
isomorphismistoostringentandapproximatevariantsarenecessary. Forthecase
of GRAPH ISOMORPHISM,researchersconsideredtwotypesofrelaxation: Either
they focused on making a small number of modifications to the input graphs that
make them isomorphic (see, e.g., the works of Arvind et al. [2012] and Grohe
et al. [2018]), or they sought (maximum) isomorphic subgraphs of the input ones
(see,e.g.,theclassicpaperofCook[1971]andthetextbookofGareyandJohnson
[1979]; for an overview focused on applications in cheminformatics we point to
theworkofRaymondandWillett[2002]).
While in Chapter 4 we focused on comparing different elections by measur-
ing distances between them, which is analogous to the first type of relaxation of
GRAPH ISOMORPHISM, in this chapter we consider the second type. In partic-
ular, we consider the SUBELECTION ISOMORPHISM and MAXIMUM COMMON
SUBELECTION families ofproblems. Inthe former, weare giventwo elections, a
119smaller and a larger one, and we ask if it is possible to remove some candidates
and voters from the larger election so that it becomes isomorphic to the smaller
one. Put differently, we ask if the smaller election occurs as a minor in the larger
one. One reason why this problem is interesting is its connection to restricted
preference domains. For example, single-peaked and single-crossing elections
arecharacterizedasthosethatdonothavecertainforbiddenminors[Ballesterand
Haeringer,2011,Brederecketal.,2013]. Weshowthat SUBELECTION ISOMOR-
PHISM is NP-complete and W[1]-hard to parameterize by the size of the smaller
election, which suggests that there are no fast algorithms for the problem. For-
tunately, the characterizations of single-peaked and single-crossing elections use
minors of constant size and such elections can be recognized efficiently; indeed,
thereareveryfastalgorithmsforthesetasks[BartholdiandTrick,1986,Escoffier
et al., 2008, Elkind et al., 2012]. Our results show that characterizations with
nonconstantminorsmightleadtoNP-hardrecognitionproblems.
Inoursecondproblem, MAXIMUM COMMON SUBELECTION,weaskforthe
largest isomorphic subelections of the two input ones. Although we find that
many of our problems are NP-hard, we also find polynomial algorithms, also for
practicallyusefulcases.
For both of our problems, we consider their candidate and voter variants. For
example, in CANDIDATE SUBELECTION ISOMORPHISM we ask if it is possible
toremovecandidatesfromthelargerelection(butwithoutdeletinganyvoters)so
that it becomes isomorphic with the smaller one. Similarly, in MAXIMUM COM-
MON VOTER-SUBELECTION weaskifwecanensuretheisomorphismofthetwo
inputelectionsbyonlydeletingvoters(sothatatleastagivennumberofvotersre-
mains). InSection6.4weusethislatterproblemtoevaluatethesimilaritybetween
elections generated from various statistical cultures and some real-life elections.
These results confirm some findings observed in previous chapters and provide a
newperspectiveonsomeofthesestatisticalculturesandreal-lifeelections.
In the most general variants of our problems, we assume that both input elec-
tionsareoverdifferentcandidatesetsandincludedifferentvoters. Yet,sometimes
it is natural to assume that the candidates or the voters are the same (for exam-
ple, in a presidential election votes collected in two different districts would have
thesamecandidatesets,butdifferentvoters,whereastwoconsecutivepresidential
electionswouldlargelyinvolvethesamevoters,butnotnecessarilythesamecan-
didates). Wemodelsuchscenariosbyvariantsofourproblemsinwhicheitherthe
matchings between the candidates or the voters of the input elections are given.
Althoughonewouldexpectthathavingsuchmatchingswouldmakeourproblems
easier,therearecaseswheretheyremainNP-hardevenwithbothmatchings. This
120no voter candidate both
Problem matching matching matching matchings
ElectionIsomorphism P P P P
SubelectionIsomorphism W[1]-hard NP-com. P P
Cand.-SubelectionIsomorphism NP-com. NP-com. P P
Voter-SubelectionIsomorphism P P P P
Max. CommonSubelection W[1]-hard NP-com. NP-com. NP-com.
Max. CommonCand.-Subelection NP-com. NP-com. NP-com. W[1]-com.
Max. CommonVoter-Subelection P P P P
Table 6.1: An overview of our results; those for ELECTION ISOMORPHISM are
takenfromChapter4. W[1]-hardnessholdswithrespecttothesizeofthesmaller
electionoracommonsubelection. TheW[1]-hardproblemsarealsoNP-hard.
contrasts sharply with the results from Table 4.1 from Chapter 4. For a summary
ofourresults,seeTable6.1.
The approach taken in this chapter is significantly different from the one pre-
sented in Chapter 4 but, as before, the main aim is to get a better understanding
of the nature of statistical culture models as well as of real-life elections. Results
presented in this chapter are complementary to the previous ones, and give us a
betterunderstandofthemapofelections.
6.2 Variants of the Isomorphism Problem
Given elections E = (C,V) and E′ = (C′,V′), we say that E′ is a subelection
ofE ifC′ isasubsetofC andV′ canbeobtainedfromV bydeletingsomevoters
and restricting the remaining ones to the candidates from C′. We say that E′
is a voter subelection of E if we can obtain it by only deleting voters from E,
and that E′ is a candidate subelection of E if we can obtain it from E by only
deletingcandidates. Bythesizeofanelection,wemeanthenumberofcandidates
multipliedbythenumberofvoters.
As a reminder, two elections are isomorphic if it is possible to rename their
candidates and reorder their voters so that they become identical. Formally,
elections (C ,V ) and (C ,V ), are isomorphic if |C | = |C |, |V | = |V |,
1 1 2 2 1 2 1 2
and there is a bijection σ: C → C and a permutation π ∈ S such
1 2 |V1|
that (σ(C ),σ(π(V ))) = (C ,V ). We refer to σ as the candidate matching and
1 1 2 2
toπ asthevotermatching.
121Given a graph G, we write V(G) to refer to its set of vertices and E(G) to
refer to its set of edges. Most of our intractability proofs follow by reductions
from the CLIQUE problem. An instance of CLIQUE consists of a graph G and a
nonnegativeintegerk,andweaskifGcontainskverticesthatareallconnectedto
each other. CLIQUE is well-known to be both NP-complete and W[1]-complete,
for the parameterization by k [Downey and Fellows, 1995]. As all the problems
that westudy can easilybe seen tobelong toNP, in ourNP-completeness proofs
weonlygivehardnessarguments.
Now we are ready to introduce two extensions of the ELECTION ISOMOR-
PHISM problem, called SUBELECTION ISOMORPHISM and MAXIMUM COM-
MON SUBELECTION. In the former, we are given two elections, and we ask if
the smaller one is isomorphic to a subelection of the larger one. That is, we ask
ifwecanremovesomecandidatesandvotersfromthelargerelectiontomakethe
twoelectionsisomorphic.
Definition 6.1. An instance of SUBELECTION ISOMORPHISM consists of two
elections,E = (C ,V )andE = (C ,V ),suchthat|C | ≤ |C |and|V | ≤ |V |.
1 1 1 2 2 2 1 2 1 2
WeaskifthereisasubelectionE′ ofE isomorphictoE .
2 1
The VOTER-SUBELECTION ISOMORPHISM problem is defined in the same
way, except that we require E′ to be a voter subelection of E . Similarly, in
2
CANDIDATE-SUBELECTION ISOMORPHISM we require E′ being a candidate
subelection. We often abbreviate the name of the latter problem to CAND.-
SUBELECTION ISOMORPHISM.
Example 6.1. Consider elections E = (C,V) and F = (D,U), where C =
{a,b,c}, D = {x,y,z,w}, V = (v ,v ,v ) and U = (u ,u ,u ), with prefer-
1 2 3 1 2 3
enceorders:
v : a ≻ b ≻ c, u : w ≻ x ≻ y ≻ z,
1 1
v : b ≻ a ≻ c, u : y ≻ w ≻ x ≻ z,
2 2
v : c ≻ b ≻ a, u : z ≻ w ≻ y ≻ x.
3 3
Ifweremovecandidatewfrom(D,U),thenwefindthattheresultingelectionsare
isomorphic(toseethis,itsufficestomatchvotersv ,v ,v withu ,u ,u ,respec-
1 2 3 1 2 3
tively, and candidates a,b,c with x,y,z). Thus E is isomorphic to a (candidate)
subelection of F and, so, (E,F) is a yes-instance of (CAND.-)SUBELECTION
ISOMORPHISM. △
122In the MAXIMUM COMMON SUBELECTION problem, we seek the largest
isomorphic subelections of two given ones. We often abbreviate MAXIMUM as
MAX.
Definition 6.2. An instance of MAX. COMMON SUBELECTION consists of two
elections, E = (C ,V ) and E = (C ,V ), and a positive integer t. We ask if
1 1 1 2 2 2
there is a subelection E′ of E and a subelection E′ of E such that E′ and E′
1 1 2 2 1 2
areisomorphicandthesizeofE′ (orequivalently,thesizeofE′)isatleastt.
1 2
Analogously to the case of SUBELECTION ISOMORPHISM, we also consider
the MAX. COMMON CAND.-SUBELECTION and MAX. COMMON VOTER-
SUBELECTION problems. In the former, E′ and E′ must be candidate subelec-
1 2
tionsandinthelattertheyneedtobevotersubelections(thus,intheformerprob-
lemE andE musthavethesamenumbersofvoters,andinthelatterE andE
1 2 1 2
musthavethesamenumbersofcandidates).
For each of the above-defined problems, we consider its variant with or with-
out the candidate or voter matching. Specifically, the variants defined above are
with no matchings. Variants with candidate matching include a bijection σ that
matches (some of) the candidates in one election to (some of) those in the other
(in the case of SUBELECTION ISOMORPHISM and its variants, all candidates in
the smaller election must be matched to those in the larger one; in case of MAX.
COMMON SUBELECTION there are no such requirements). Then we ask for an
isomorphism between respective subelections that agrees with σ. In particular,
this means that none of the unmatched candidates remains in the considered sub-
elections (another interpretation is to assume that both input elections have the
samecandidatesets).
Example 6.2. Consider elections (C,V) and (D,U) from Example 6.1, and a
matchingσ suchthatσ(a) = x,σ(b) = w,wherec,y,andz areunmatched. After
applying it and dropping the unmatched candidates, the votes in the first election
become
v : x ≻ w, v : w ≻ x, v : w ≻ x,
1 2 3
whereas all the voters in the second election have preference order w ≻ x. Thus,
thisinstanceof MAX. COMMON SUBELECTION WITH CANDIDATE MATCHING
hasisomorphicsubelections,respectingthematchingσ,ofsize2·2 = 4. △
Thevariantswithvotermatchingaredefinedsimilarly: Wearegivenamatch-
ingbetween(someof)thevotersfromoneelectionand(someof)thevotersfrom
123the other (and, again, for SUBELECTION ISOMORPHISM and its variants, each
voter in the smaller election is matched to some voter in the larger one). The
sought-after isomorphism must respect this matching (again, this means that we
candisregardtheunmatchedvoters).
The variants with both matchings include both the matching between the can-
didates and the matching between the voters (note that these variants are not triv-
ial because we still need to decide who to remove). By writing all four matching
caseswemeanthefourjustdescribedvariantsofagivenproblem.
Finally, we note that each variant of MAX. COMMON SUBELECTION is at
least as computationally difficult as its corresponding variant of SUBELECTION
ISOMORPHISM.
Proposition6.1. LetM beavariantofMAX. COMMON SUBELECTIONandletS
be a corresponding variant of SUBELECTION ISOMORPHISM. We have that S
reducestoM inpolynomialtime.
Proof. We are given a problem M which is a variant of MAX. COMMON SUB-
ELECTION, and problem S, which is an analogous variant of SUBELECTION
ISOMORPHISM (so if the former only allows deleting candidates, then so does
the latter, etc.). We want to show that S reduces to M in polynomial time.
Let I = (E ,E ) be an instance of S, where E is the smaller election. We
S 1 2 1
form an instance I = (E ,E ,t) of M, which uses the very same elections and
M 1 2
where t is set to be the size of E . This means that in I we cannot perform any
1 M
operations on election E , because that would decrease its size below t. There-
1
fore,wecanonlyperformoperationsonE ,sothesituationisthesameasintheS
2
problemandintheI instance.
S
6.3 Computational Complexity Analysis
In this section, we present our complexity results. Although in most cases we
obtain intractability (recall Table 6.1 for a summary of our results), we find that
all our problems focused on voter subelections are solvable in polynomial time,
and having candidate matchings leads to the polynomial-time algorithm for all
variantsof SUBELECTION ISOMORPHISM.
All our polynomial-time results are based on the trick used for the case of
ELECTION ISOMORPHISM in Chapter 4. The idea is to guess a pair of (matched)
votersandusethemtoderivethecandidatematching.
124Theorem 6.1. VOTER-SUBELECTION ISOMORPHISM and MAX. COMMON
VOTER-SUBELECTION areinPforallfourmatchingcases. SUBELECTION ISO-
MORPHISM, CAND.-SUBELECTION ISOMORPHISM are in P for cases with can-
didatematchings.
Proof. We first give an algorithm for MAX. COMMON VOTER-SUBELECTION.
Let E = (C,V) and F = (D,U) be our input elections and let t be the desired
sizeoftheirisomorphicsubelections. Sincewearelookingforavotersubelection,
without loss of generality we may assume that |C| = |D| (and we write m to
denote the number of candidates in each set). For each voter v ∈ V and each
voteru ∈ U weperformthefollowingalgorithm:
1. Denoting the preference orders of v and u as c ≻ c ≻ ··· ≻ c
1 v 2 v v m
and d ≻ d ≻ ··· ≻ d , respectively, we form a bijection σ: C → D
1 u 2 u u m
suchthatforeachc ∈ C wehaveσ(c ) = d .
i i i
2. WeformabipartitegraphwherethevotersfromV formonesetofvertices,
thevotersfromU formtheothersetofvertices,andthereisanedgebetween
votersv′ ∈ V andu′ ∈ U ifσ(v′) = u′.
3. Wecomputethemaximumcardinalitymatchinginthisgraphandformsub-
elections that consist of the matched voters. We accept if their size is at
leastt.
Ifthealgorithmdoesnotacceptforanychoiceofv andu,wereject.
Very similar algorithms also work for the variants of MAX. COMMON
VOTER-SUBELECTION with either one or both of the matchings: If we are given
the candidate matching, then we can omit the first step in the enumerated algo-
rithm above, and if we are given a voter matching then instead of trying all pairs
of voters v and u it suffices to try all voters from the first election and obtain
the other one via the matching. Analogous algorithms also work for VOTER-
SUBELECTION ISOMORPHISM (for all four matching cases) and for all the other
variants of SUBELECTION ISOMORPHISM, provided that the candidate matching
isgiven.
6.3.1 Intractability of Subelection Isomorphism
Next, we show the computational hardness of all the remaining variants of our
problems. Inthissectionweconsider SUBELECTION ISOMORPHISM.
125Theorem 6.2. SUBELECTION ISOMORPHISM is NP-complete and W[1]-hard
withrespecttothesizeofthesmallerelection.
Proof. Before we describe our reduction, we first provide a method for trans-
forming a graph into an election: For a graph H, we let E be an election
H
whose candidate set consists of the vertices of H and two special candidates, α
H
and β , and whose voters correspond to the edges of H. Specifically, for each
H
edgee = {x,y} ∈ E(H)wehavefourvoters,v1,...v4,withpreferenceorders:
e e
v1: x ≻ y ≻ α ≻ β ≻ V(H)\{x,y},
e H H
v2: x ≻ y ≻ β ≻ α ≻ V(H)\{x,y},
e H H
v3: y ≻ x ≻ α ≻ β ≻ V(H)\{x,y},
e H H
v4: y ≻ x ≻ β ≻ α ≻ V(H)\{x,y}.
e H H
Note that elements from the set V(H)\{x,y} are always in the same order. We
give a reduction from CLIQUE. Given an instance (G,k) of CLIQUE, where G
has at least k vertices and
(cid:0)k(cid:1)
edges, we let K be a size-k complete graph and
2
we form an instance (E K,E G) of SUBELECTION ISOMORPHISM. The reduction
runsinpolynomialtimeanditremainstoshowitscorrectness.
First, let us assume that G has a size-k clique. Let X be the set of its vertices
andletY bethesetofitsedges. WeformasubelectionE′ ofE byremovingall
G
the candidates that are not in X ∪ {α ,β } and removing all the voters that do
G G
not correspond to the edges from Y. One can verify that E′ and E are, indeed,
K
isomorphic.
Second, let us assume that E is isomorphic to some subelection E′ of E .
K G
We will show that this implies that G has a size-k clique. First, we claim that E′
includesbothα andβ . Toseewhythisisso,considerthefollowingtwocases:
G G
1. If E′ contained exactly one of α ,β , then this candidate would appear
G G
in every vote in E′ among the top three positions. Yet, in E there is no
K
candidatewiththisproperty,soE′ andE wouldnotbeisomorphic.
K
2. If E′ contained neither α nor β then every vote in E′ would rank
G G
some vertex candidates z and w on positions three and four (to be able
to match α and β to them). However, by the construction of E , either
K K G
in every vote from E′ we would have z ≻ w or in every vote from E′ we
would have w ≻ z. Since in E half of the voters rank the candidates
K
from positions three and four in the opposite way, E′ and E would not be
K
isomorphic.
126Thus α and β are included in E′. Moreover α and β are matched with α
G G G G K
and β because they are the only candidates from E that can appear on po-
K G
sitions three and four in every vote in E′ but possibly in different order. As a
consequence, for each vote v from E that appears in E′, the candidate set of E′
G
must include the two candidates fromV(G) that v ranks on top (if it were not the
case, then E′ would contain a candidate—either α or β —that appeared in all
G G
the votes within the top four positions and in some vote within top two positions;
yetE doesnothavesuchacandidate). Thismeansthatforeachedgee ∈ E(G),
K
if E′ contains some voter vi for i ∈ [4], then it also contains the other voters cor-
e
responding to e (otherwise, E′ and E would not be isomorphic). The number
K
of voters in E is
4(cid:0)k(cid:1)
, and the number of distinct corresponding edges from G
K 2
is
(cid:0)k(cid:1)
. As said before, for each such chosen edge, we also choose two corre-
2
sponding vertices as candidates. It means that the number of chosen candidates
(except α and β ) is between k and
2(cid:0)k(cid:1)
. However, the number of candidates
G G 2
inE exceptα andβ isk,thereforeweconcludethatchosenvertex-candidates
K K K
formasize-k cliqueinG. ThiscompletestheproofofNP-hardness.
To show W[1]-hardness, note that the number of candidates and voters in the
smaller election equals k +2 and
4(cid:0)k(cid:1)
respectively, hence the size of the smaller
2
electionisafunctionofparameterk forwhich CLIQUE isW[1]-hard.
Next, we consider CAND.-SUBELECTION ISOMORPHISM. In this problem
both elections have the same number of voters, and we ask if we can delete can-
didates from the one that has more, so that they become isomorphic. We first
show that this problem is NP-complete for the case where the voter matching is
given(whichalsoprovesthesameresultforSUBELECTION ISOMORPHISM WITH
VOTER MATCHING) and next we describe how this proof can be adapted to the
variant without any matchings (the variant with candidate matching is in P and
wasconsideredintheprecedingsection).
Theorem 6.3. SUBELECTION ISOMORPHISM WITH VOTER MATCHING and
CAND.-SUBELECTION ISOMORPHISM WITH VOTER MATCHING are NP-
complete.
Proof. It suffices to consider CAND.-SUBELECTION ISOMORPHISM WITH
VOTER MATCHING. We give a reduction from the EXACT COVER BY 3-SETS
problem (X3C). An instance of X3C consists of a set X = {x ,...,x } of ele-
1 m
mentsandafamilyS = {S ,...,S }ofthree-elementsubsetsofX. WeaskifS
1 n
containsasubfamilyS′ suchthateachelementfromX belongstoexactlyoneset
fromS′. Givensuchaninstance,weformtwoelections,E andE ,asfollows.
1 2
127Election E will be our smaller election and E will be the larger one. We
1 2
letX bethecandidatesetforelectionE ,whereastoformthecandidatesetofE
1 2
we proceed as follows. For each set S = {x ,x ,x } ∈ S, we introduce candi-
t i j k
datess ,s ,ands . Intuitively,ifsomecandidates remainsinasubelection
i,t j,t k,t u,t
isomorphictoE ,thenwewillinterpretthisfactassayingthatelementx iscov-
1 u
ered by set S ; this will, of course, require introducing appropriate consistency
t
gadgets to ensure that S also covers its other members. For each i ∈ [m], we
t
let P be the set of all the candidates of the form s , where t belongs to [n] (in
i i,t
other words, each P contains the candidates from E that are associated with
i 2
elementx ).
i
Example6.3. LetX = {x ,x ,x ,x ,x ,x }andS = {S ,...,S },where
1 2 3 4 5 6 1 4
S = {x ,x ,x },S = {x ,x ,x },S = {x ,x ,x },S = {x ,x ,x }}.
1 1 2 5 2 1 3 6 3 2 3 5 4 3 4 6
Then
P = {s ,s }, P = {s ,s }, P = {s ,s ,s },
1 1,1 1,2 2 2,1 2,3 3 3,2 3,3 3,4
P = {s }, P = {s ,s }, P = {s ,s }.
4 4,4 5 5,1 5,3 6 6,2 6,4
△
WedenotethevotercollectionofelectionE asV = (v,v′,v ,v′,...,v ,v′)
1 1 1 n n
and the voter collection of E as U = (u,u′,u ,u′,...,u ,u′ ). Voters v and v′
2 1 1 n n
are matched to voters u and u′, respectively, and for each i ∈ [2n], v is matched
i
tou andv′ ismatchedtou′. Thepreferenceordersofthefirsttwopairsofvoters
i i i
are:
v: x ≻ x ≻ ··· ≻ x , u: P ≻ P ≻ ··· ≻ P ,
1 2 m 1 2 m
←− ←− ←−
v′: x ≻ x ≻ ··· ≻ x , u′: P ≻ P ≻ ··· ≻ P .
1 2 m 1 2 m
Next, for each t ∈ [n] such that S = {x ,x ,x }, i < j < k, we let the
t i j k
preferenceordersofv ,v′ andtheircounterpartsfromU beasfollows(bywriting
t t
“···” in the votes from E we mean listing the candidates from X \{x ,x ,x }
1 i j k
in the order of increasing indices, and for the voters from E by “···” we mean
2
orderP ≻ P ≻ ··· ≻ P withP ,P ,andP removed):
1 2 m i j k
v : x ≻ x ≻ x ≻ ··· ,
t i j k
u : s ≻ s ≻ s ≻ P \{s } ≻ P \{s } ≻ P \{s } ≻ ··· ,
t i,t j,t k,t i i,t j j,t k k,t
v′: x ≻ x ≻ x ≻ ··· ,
t k j i
u′: s ≻ s ≻ s ≻ P \{s } ≻ P \{s } ≻ P \{s } ≻ ··· .
t k,t j,t i,t k k,t j j,t i i,t
128This finishes our construction. It is clear that it is polynomial-time computable
anditremainstoshowthatitiscorrect.
Letusassumethatwehaveayes-instanceof X3C,thatis,thereisafamilyS′
ofsetsfromS suchthateachelementfromX belongstoexactlyonesetfromS′.
WeformasubelectionE′ ofE bydeletingallthecandidatess exceptforthose
2 i,t
forwhomsetS belongstoS′. Then,letσ beafunctionsuchthatforeachx ∈ X
t i
wehaveσ(x ) = s ,whereS isasetfromS′ thatcontainsx . Togetherwithour
i i,t t i
votermatching,σ witnessesthatE andE′ areisomorphic.
1
For the other direction, let us assume that E has a subelection E′ that is
2
isomorphic to E and let C′ be its candidate set. First, we claim that for each i ∈
1
[m] exactly one candidate from P is included in C′. Indeed, if it were not the
i
case, then u and u′ would not have identical preference orders, as required by the
fact that they are matched to v and v′. Second, we note that for each i ∈ [m] the
candidatematchingthatwitnessesourisomorphismmustmatchx withthesingle
i
candidateinP ∩C′. Finally,weclaimthatifsomecandidates isincludedinC′,
i i,t
where S = {x ,x ,x }, i < j < k, then candidates s and s are included
t i j k j,t k,t
in C′ as well. Indeed, if s were not included in C′, then u and u′ would rank
j,t t t
themembersofP ∩C′ andthemembersofP ∩C′ inthesameorder,whereasv
i j j
andv′ rankx andx inoppositeorders(and,bythesecondobservation,x andx
j i j i j
are matched to the member of P ∩ C′ and P ∩ C′, respectively). The same
i j
argumentappliestox . WesaythatasetS = {x ,x ,x } ∈ S isselectedbyC′
k,t t i j k
if the candidates s , s , and s belong to C′. By the above reasoning, we see
i,t j,t k,t
thatexactlyn/3setsareselectedandthattheyformanexactcoverofX.
CAND.-SUBELECTION ISOMORPHISM remains NP-complete also without
the voter matching. By doubling the voters and using a few extra candidates,
weensurethatonlytheintendedvotermatchingispossible.
Proposition6.2. CAND.-SUBELECTION ISOMORPHISM isNP-complete.
Proof. We give a reduction from CAND. SUBELECTION ISOMORPHISM WITH
VOTER MATCHING. Let the input instance be (E 1,E 2), where the smaller elec-
tion, E , has voter collection (v ,...,v ) and the larger election, E , has voter
1 1 n 2
collection (u ,...,u ). Additionally, for each i ∈ [n] voter v is matched with
1 n i
voter u . We form elections E′ and E′ in the following way. The candidate set
i 1 2
of E′ is the same as that of E except that it also includes candidates from the
1 1
set D = {d ,...,d }. Similarly, E′ contains the same candidates as E plus the
1 2n 2 2
candidates from the set F = {f ,...,f }. The voter collections of E′ and E′
1 2n 1 2
are, respectively, (v′,v′′,...,v′,v′′) and (u′,u′′,...,u′ ,u′′). For each i ∈ [n]
1 1 n n 1 1 n n
129thesevotershavethefollowingpreferenceorders(bywriting[v ]or[u ]inapref-
i i
erence order we mean inserting the preference order of voter v or u in a given
i i
place:
v′: d ≻ d ≻ ··· ≻ d ≻ [v ],
i 2i−1 1 2n i
u′: f ≻ f ≻ ··· ≻ f ≻ [u ],
i 2i−1 1 2n i
v′′: d ≻ d ≻ ··· ≻ d ≻ [v ],
i 2i 1 2n i
u′′: f ≻ f ≻ ··· ≻ f ≻ [u ].
i 2i 1 2n i
WeclaimthatE′ isisomorphictoacandidatesubelectionofE′ ifandonlyifE is
1 2 1
isomorphictoacandidatesubelectionofE withthegivenvotermatching. Inone
2
directionthisisclear: IfE isisomorphictoasubelectionofE withagivenvoter
1 2
matching,thenitsufficestousethesamevotermatching(extendedintheobvious
way) for the case of E′ and E′, and the same candidate matching, extended with
1 2
matchingeachcandidated tof .
i i
Next, let us assume that E′ is isomorphic to some subelection E′ of E′. By
1 2
a simple counting argument, we note that E′ must contain some candidates not
in F. Further, we also note that it must contain all members of F. Indeed, each
voterinE′ hasadifferentcandidateontop,andthiswouldnotbethecaseinE′ if
1
it did not include all members of F (if E′ did not include any members of F then
this would hold for each two votes u′ and u′′, and if E′ contained some members
i i
ofF butnotallofthem,thenthiswouldholdbecauseeachvoterinE′ wouldrank
somememberofF ontop,buttherewouldbefewermembersofF thanvotersin
theelection).
As a consequence, every candidate matching σ that witnesses isomorphism
between E′ and E′ matches some member of D to some member of F. Further-
1
more, we claim that for each i ∈ [2n], σ(d ) = f . For the sake of contradiction,
i i
let us assume that this is not the case and consider some i ∈ [2n − 1] for which
there are j and k such that σ(d ) = f , σ(d ) = f and j > k (such i, j, k
i j i+1 k
mustexistunderourassumption). However,inE′,allbutonevoterrankd ahead
1 i
of d , while in E′ all but one voter rank σ(d ) ahead of σ(d ). Thus σ cannot
i+1 i+1 i
witnessisomorphismbetweenE′ andE′.
1
Finally, since for each i ∈ [2n] we have that d is matched to f , it also must
i i
be the case that for each j ∈ [n] voters v′ and v′′ are matched to u′ and u′′,
j j j j
respectively (indeed, v′ is the only voter who ranks d on top, and u′ is the
j 2j−1 j
only voter who ranks f on top; the same argument works for the other pair of
2j−1
voters). As a consequence, we have that E is isomorphic to a subelection of E
1 2
underthevotermatchingthatforeachi ∈ [n]matchesv tou .
i i
1306.3.2 Intractability of Max. Common Subelection
Perhaps the most surprising result regarding MAX. COMMON SUBELECTION is
that it is NP-complete even when both matchings are given. The surprise stems
fromthefactthatISOMORPHISM DISTANCEproblems(i.e.,computingSpearman
or Swap distances between elections) are solvable in polynomial-time given both
matchings. Wefirstshowthisresultforcandidatesubelections.
Theorem 6.4. MAX. COMMON CAND.-SUBELECTION WITH BOTH MATCH-
INGS isNP-completeandW[1]-completewithrespecttothecandidatesetsizeof
isomorphiccandidatesubelections.
Proof. WegiveareductionfromtheCLIQUEproblem,wheretheideaistoencode
the adjacency matrix of a given graph by a pair of elections with both matchings
defined. Missingedgesinthegraphweencodeasaconflictoncandidateordering
withinmatchedvoters.
Formally, given an instance (G,k) of CLIQUE, we form two elections, E
1
=
(C,V ) and E = (C,V ), where C = V(G). Since we need to provide an
1 2 2
instancewithcandidatematching,wesimplyspecifybothelectionsoverthesame
candidateset. Withoutlossofgenerality,weassumethatV(G) = {1,...,n}. For
each x ∈ V(G) we define the neighborhood of x in G as N(x) = {y ∈ V(G) :
{x,y} ∈ E(G)}andthesetofnon-neighborsasM(x) = V(G)\{N(x)∪{x}}.
For each vertex x ∈ V(G) we define two matched voters, v1 in E and v2
x 1 x
inE ,definedasfollows:
2
v1: M(x) ≻ x ≻ N(x),
x
v2: x ≻ M(x) ≻ N(x).
x
WeaskifE andE haveisomorphiccandidatesubelectionsthatcontainatleastk
1 2
candidateseach. Intuitively,inasolutiontotheproblem,foreachvertexxonehas
to remove either x or all vertices from M(x). It is a direct definition of a clique:
Either x is not in a clique or all its nonneighbors are not in a clique. It is clear
that the reduction can be computed in polynomial time and it remains to show its
correctness.
First,letusassumethatGhasasize-k clique. LetK bethesetofthisclique’s
vertices. We form elections E′ and E′ by restricting E and E to the candidates
1 2 1 2
from K. To verify that E′ and E′ are isomorphic via the given matchings, let us
1 2
consider an arbitrary pair of matched voters v1 and v2. If x is not included in K
x x
thenpreferenceordersofv1 andv2 restrictedtoK areidentical. Indeed,removing
x x
131even only x from the set of candidates makes v1 and v2 identical. Otherwise, if x
x x
isinK thenK∩M(x) = ∅asK isaclique. Therefore,removingM(x)fromthe
setofcandidatesmakesv1 andv2 identical.
x x
For the other direction, let us assume that there are subelections E′ and E′
1 2
of E and E , respectively, each with candidate set K, such that |K| ≥ k and E′
1 2 1
andE′ areisomorphicviathegivenmatchings. Itmustbethecasethatthevertices
2
from K form a clique because if K contained two vertices x and y that were
not connected by an edge, then votes v1 and v2 would not be identical. Indeed,
x x
we would have y ≻ x in v1 and x ≻ y in v2, respectively, when restricted to
x x
candidates from K. This completes the proof of NP-hardness. To show W[1]-
hardness, note that the required number of candidates in isomorphic candidate
subelectionsisequaltotheparameterk forwhich CLIQUE isW[1]-hard.
Let us now give a reduction in the opposite direction. Let E = (C,V )
1 1
and E = (C,V ) be our input elections and let k be the number of candidates
2 2
in maximum isomorphic candidate subelections (since we are in the “with candi-
datematching”regime,wetakethecandidatesetstobeequal). Letm = |C|,n =
|V | = |V |(sincewecannotremovethevoters).
1 2
We create an instance (G,k) of CLIQUE as follows. We define G as having
vertices corresponding to candidates, i.e., V(G) = C. We construct the set of
edges by starting from a complete graph and removing some of them as follows.
For every two matched voters v and u and every two candidates x and y such
that x ≻ y and y ≻ x, we remove edge {x,y} from the graph. It is clear that
v u
the reduction can be computed in polynomial time and both parameters have the
samevalue. Itremainstoshowitscorrectness.
First,letusassumethattherearesubelectionsE′ andE′ ofE andE ,respec-
1 2 1 2
tively,eachwithcandidatesetK,suchthat|K| ≥ kandE′ andE′ areisomorphic
1 2
viathegivenmatchings. ItmustbethecasethattheverticesfromK formaclique.
Indeed, if K contained two vertices x and y that were not connected by an edge,
then edge {x,y} had to be removed by some two matched voters v and u such
that x ≻ y and y ≻ x. Since both voters belong to subelections E′ and E′, we
v u 1 2
obtainacontradictionthattheyareisomorphicviathegivenmatchings.
For the other direction, let us assume that G has a size-k clique. Let K be
the set of this clique’s vertices. We form elections E′ and E′ by restricting E
1 2 1
and E to the candidates from K. To verify that E′ and E′ are isomorphic via
2 1 2
the given matchings, let us consider an arbitrary pair of matched voters v and u
and arbitrary pair of candidates x,y ∈ K. It follows that x ≻ y and x ≻ y.
v u
Otherwise edge {x,y} would have been removed during the reduction, hence K
wouldnotbeaclique. Acontradiction.
132All the remaining variants of MAX. COMMON CAND.-SUBELECTION also
are NP-complete. The proofs follow either by applying Proposition 6.1 or by
introducing candidates that implement a required voter matching. In the latter
case, W[1]-hardness does not follow from this reduction as we introduce dummy
candidatesthathavetobeincludedinasolution,buttheirnumberisnotafunction
ofthe CLIQUE parameter(cliquesize).
Proposition 6.3. MAX. COMMON CAND.-SUBELECTION is NP-complete and
so are its variants with a given candidate matching and with a given voter match-
ing.
Proof. Below we give the reductions for all the three cases, i.e., the case with a
given candidate matching, with a given voter matching, and without any match-
ings.
The case with a given candidate matching. We give a reduction from MAX.
COMMON CAND.-SUBELECTION WITH BOTH MATCHINGS. Let E 1 = (C,V)
and E = (C,U) be our input elections, where V = (v ,...,v ) and U =
2 1 n
(u ,...,u ), and let t be the desired size of the isomorphic subelection (since
1 n
we are in the setting with both matchings, we can assume that both elections are
over the same candidate set). We assume that for each i ∈ [n] voter v is matched
i
tou . Letm = |C|andletk = t/n. Wenotethatk ≤ m.
i
Our construction proceeds as follows. First, we form m + 1
sets, A, D ,...,D , each containing m + 1 new candidates. Let D = A ∪
1 m
D ∪···∪D . Note that |D| = (m+1)2. We form elections E′ = (C ∪D,V′)
1 m 1
and E′ = (C ∪ D,U′), where V′ = (v′,...,v′) and U′ = (u′,...,u′ ). For
2 1 n 1 n
each i ∈ [n], we set their preference orders as follows (by writing [v ] or [u ] we
i i
meancopyingthepreferenceorderoftherespectivevoter):
v′: D ≻ ··· ≻ D ≻ A ≻ D ≻ ··· ≻ D ≻ [v ],
i 1 i−1 i m i
u′: D ≻ ··· ≻ D ≻ A ≻ D ≻ ··· ≻ D ≻ [u ].
i 1 i−1 i m i
Finally, we set the desired size of the isomorphic subelections to be t′ = n·(k +
(m+1)2) = t+n(m+1)2.
WeclaimthatE′ andE′ haveisomorphiccandidatesubelectionsofsizet′ for
1 2
thegivencandidatematchingifandonlyifE andE haveisomorphiccandidate
1 2
subelectionsofsizetforgivencandidateandvotermatchings.
Let us assume that E′ and E′ have the desired candidate subelections, E′′
1 2 1
and E′′. We claim that their isomorphism is witnessed by such a matching that
2
133for each i voter v′ is matched to u′. If it were not the case, then to maintain
i i
the isomorphism these subelections would have to lose at least m−1 candidates
from D (e.g., the candidates from A) and their sizes would be at most n(m +
m(m + 1)) = n((m + 1)2 − 1) < t′. Thus the isomorphism of E′′ and E′′ is
1 2
witnessed by the same voter matching as the one required by our input instance.
A simple counting argument shows that after dropping candidates from D from
subelections E′′ and E′′, we obtain elections witnessing that (E ,E ) is a yes-
1 2 1 2
instance of MAX. COMMON CAND.-SUBELECTION WITH BOTH MATCHINGS.
Thereversedirectionisimmediate.
The case with a given voter matching. This case follows by Proposition 6.1
and the fact that CAND.-SUBELECTION WITH VOTER MATCHING is NP-
complete.
The case without any given matchings. This case follows by Proposition 6.1
andthefactthatCAND.-SUBELECTION ISOMORPHISMproblemisNP-complete.
Similarly to all four matching cases of the MAX. COMMON CAND.-
SUBELECTION, all four matching cases of the MAX. COMMON SUBELECTION
alsoareNP-complete.
Proposition 6.4. All four matching cases of MAX. COMMON SUBELECTION
areNP-complete.
Proof. For the case without any matchings and the case with the voter match-
ing, we use Proposition 6.1 to reduce from the corresponding variant of SUB-
ELECTION ISOMORPHISM. For the variants that include the candidate matching
(for which SUBELECTION ISOMORPHISM is in P), we reduce from the corre-
sponding variants of MAX. COMMON CAND.-SUBELECTION. Let E
1
= (C,V 1)
and E = (C,V ) be our input elections and let t be the desired size of their iso-
2 2
morphic candidate subelections (since we are in the “with candidate matching”
regime, we take the candidate sets to be equal). Without loss of generality, we
can assume that |V 1| = |V 2|; our NP-completeness proofs for MAX. COMMON
CAND.-SUBELECTION givesuchinstances.
Let m = |C|, n = |V | = |V |, and let D be a set of (n − 1)m dummy
1 2
candidates. We form elections E′ and E′ to be identical to E and E , re-
1 2 1 2
spectively, except that they also include the candidates from D, who are always
134ranked on the bottom, in the same order. Therefore, the number of candidates
in E′ and E′ equals nm. We ask if E′ and E′ have isomorphic subelections of
1 2 1 2
sizet′ = t+n(n−1)m.
If E and E have isomorphic candidate subelections of size t, then cer-
1 2
tainly E′ and E′ have isomorphic subelections of size t′ (it suffices to take the
1 2
samesubelectionsasforE andE andincludethecandidatesfromD).
1 2
On the other hand, if E′ and E′ have isomorphic subelections of size t′,
1 2
then E and E have size-t isomorphic candidate subelections. In fact, the sub-
1 2
elections of E′ and E′ must include all the n voters. Otherwise their sizes would
1 2
beatmost(n−1)mn < t+(n−1)mn ≤ t′. ThusthesubelectionsofE′ andE′
1 2
are candidate subelections. As we can also assume that the subelections of E′
1
andE′ includeallthecandidatesfromD,byomittingthesecandidateswegetthe
2
desiredcandidatesubelectionsofE andE .
1 2
MainConclusions
• Allproblemsrelatedto VOTER-SUBELECTIONareinP.Ontheother
hand,generalSubelectionproblemsand CAND-SUBELECTION ones
tend to be NP-hard. The isomorphic variants, given the candidate
matching, become significantly simpler (shifting to P). However, the
most interesting (or surprising) is the fact that the MAX. COMMON
SUBELECTION problemwithbothmatchingsremainsNP-complete.
6.4 Experiments
Next we use the MAX. COMMON VOTER-SUBELECTION problem to analyze
similarity between elections generated from various statistical models. While
MAX. COMMON VOTER-SUBELECTION has a polynomial-time algorithm, it is
too slow for our purposes. Thus we have expressed it as an integer linear pro-
gram (ILP) and we were solving it using the CPLEX ILP solver. A formal ILP
formulationisasfollows.
1. For each pair of voters v ∈ V and u ∈ U, we have a binary variable N .
v,u
If it is set to 1, then we interpret it as saying that voter v is included in the
subelection of E, voter u is included in the subelection of F, and the two
voters are matched. Value 0 means that the preceding statement does not
hold.
1352. For each pair of candidates c ∈ C and d ∈ D, we have a binary vari-
able M . If it is set to 1 then we interpret it as saying that c is matched
c,d
to d in isomorphic subelections (note that, since we are looking for voter
subelections, every candidate from C has to be matched to some candidate
fromD,andtheotherwayround).
ToensurethatvariablesN andM describetherespectivematchings,wehave
v,u c,d
thefollowingbasicconstraints:
(cid:80) (cid:80)
N ≤ 1, ∀v ∈ V, M = 1, ∀c ∈ C,
u∈U v,u d∈D c,d
(cid:80) (cid:80)
N ≤ 1, ∀u ∈ U, M = 1, ∀d ∈ D.
v∈V v,u c∈C c,d
For each pair of voters v ∈ V, u ∈ U and each pair of candidates c ∈ C
and d ∈ D, we introduce constant w which is set to 1 if v ranks c on the
v,u,c,d
same position as u ranks d, and which is set to 0 otherwise. We use these con-
stants to ensure that the matchings specified by variables N and M indeed
v,u c,d
describe isomorphic subelections. Specifically, we have the following constraints
(letm = |C| = |D|):
(cid:80) (cid:80)
w ·M ≥ m·N , ∀v ∈ V,u ∈ U.
c∈C d∈D v,u,c,d c,d v,u
For each v ∈ V and u ∈ U, they ensure that if v is matched to u then each
candidatecappearsinvonthesamepositionasthecandidatematchedtocappears
inu.
We stress that we could have used other problems from the MAX. COM-
MON SUBELECTION family in this section. We chose MAX. COMMON VOTER-
SUBELECTION because its outcomes are particularly easy to interpret, which is
not always the case for MAX. COMMON SUBELECTION. For example, in MAX.
COMMONSUBELECTIONproblemiftheresultingvalueisk,thenwedonotknow
ifitisduetoanelectionwithonevoteoverk candidatesoranelectionwithk vot-
ers voting for a single candidate, or (if k is not a prime number) something in
between.
Our findings are similar to those presented in the previous chapters, but our
claims of similarity between statistical cultures are stronger, whereas our dissim-
ilarity claims are weaker. Further, our results are most appealing for very small
numbers of candidates, whereas in the preceding chapters we focused on larger
candidatessets.
1366.4.1 Results and Analysis
We study the following nine models: IC, 1D-Interval, Conitzer model, Walsh
model, urn (with α ∈ {0.1,0.5}), Norm-Mallows (with norm-ϕ ∈ {1/ 3,2/ 3}), and
identity. Weconsiderelectionswith4,6,8,and10candidatesandwith50voters.
For each scenario and each two of the selected models, we have generated 1000
pairs of elections. For each pair of models, we recorded the average number of
voters in the maximum common voter subelections (normalized by fifty, i.e., the
number of voters in the original elections), as well as the standard deviation of
thisvalue.
WeshowournumericalresultsinFigure6.1(eachcellcorrespondstoapairof
models;thenumberinthetop-leftcorneristheaverage,andtheoneinthebottom-
right corner is the standard deviation). Note that the matrices in Figure 6.1 are
symmetric(theresultsformodelsAandB arethesameasformodelsB andA).
For the case with four candidates, we see that the level of similarity between
elections from various models is quite high and drops sharply as the number of
candidatesincreases. Thisshowsthatforexperimentswithveryfewcandidatesit
is not as relevant to consider very different election models, but for more candi-
datesusingdiversemodelsisjustified.
Despite the above, some models remain similar even for 6, 8, and sometimes
even10candidates. Thisisparticularlyvisibleforthecaseofsingle-peakedelec-
tions. The1D-IntervalmodelremainsverysimilartotheConitzermodel,andthe
Walsh model is quite similar to these two for up to 6 candidates, but for 8 and 10
candidatesitstartstostandout.
We also note that the urn models remain relatively similar to each other (and
to the 1D-Interval and Conitzer models) for all numbers of candidates, but this is
not the case for the Norm-Mallows models. One explanation for this is that the
urn model proceeds by copying some of the votes already present in the election,
whereas the Norm-Mallows model generates votes by perturbing the central one.
The former leads to more identical votes in an election. Indeed, to verify this,
it suffices to consider the “ID” column (or row) of the matrix: The similarity to
the identity elections simply shows how often the most frequent vote appears in
electionsfromagivenmodel. For10candidates,urnelectionswithα ∈ {0.1,0.5}
have,onaverage,21±8%and49±17%identicalvotes,respectively. ForNorm-
Mallows elections, this value drops to around 2% (in our setting, this means 1 or
2voters,onaverage).
Finally, we consider the diagonals of the matrices in Figure 6.1, which show
the self-similarity of our models. Intuitively, the larger these values, the fewer
137(a)4candidates&50voters (b)6candidates&50voters
(c)8candidates&50voters (d)10candidates&50voters
Figure 6.1: The numbers typeset in large font denote the rounded % of matched
votes for MAX. COMMON VOTER-SUBELECTION. The numbers typeset in
small font denote the rounded standard deviation. There are results for elections
with4,6,8,and10candidatesand50voters.
138elections of a given type one needs in an experiment. Single-peaked elections
stand out here for all numbers of candidates, whereas urn models become more
prominentforlargercandidatesets.
We have also analyzed the average running time that CPLEX needed to find
themaximumcommonvotersubelections. WefocusonIC,identity,Walshmodel,
Conitzer model, Norm-Mallows model with norm-ϕ = 0.5, and 1D-Interval.
First, we generate 1000 pairs of elections from each model with 10 candidates
and5,10,...,45,50voters(ineachpairbothelectionsarefromthesamemodel),
and calculate the average time needed to find the maximum common voter sub-
elections. Second, we fixed the number of voters to 50 and generated elections
with3,4,...,9,10candidates,and,likebefore,calculatetheaveragetimeneeded
tofindthemaximumcommonvotersubelections.
The results are presented in Figure 6.2. As we increase the number of vot-
ers, the time seems to increase exponentially. We observe large differences be-
tween the models, with the IC being by far the slowest. Conitzer model and
Walsh model are significantly different from each other, even though both gener-
ate single-peaked elections. Moreover, the fact that the 1D-Interval and Conitzer
modelsneedonaveragethesameamountoftimeconfirmstheirsimilarity.
MainConclusions
Most elections with few candidates are very similar to each other – this
explainswhythemapswithfewcandidatesarenotthatinformative.
6.4.2 Real-life Subelections
Wealsoconductedanalogousexperiments,butinsteadofusingstatisticalcultures,
weusedreal-lifedata. Weusedthesame11modelsasinSection5.5andwealso
added impartial culture as a reference point. We selected one election from each
model,andthentreatingeachelectionasadistribution,1 wesampled10instances
fromit,intotalhaving120elections.
We consider elections with 4, 6, 8, and 10 candidates, and 50 voters. In Ta-
ble 6.2 we present the total number of votes and the number of distinct votes for
eachdistributionthatweuse. Notethatforseveralmodels,thenumberofvotesis
smallerthan50,andthesmalleristhenumberofvotes,thelargeristheprobability
1TotreatelectionEasadistributionmeansthateachvoteissampledwithprobability a,where
n
aisthenumberofcopiesofagivenvote,andnisthenumberofallvotesinelectionE.
139Figure 6.2: Average time needed to find the maximum common voter subelec-
tions with the fixed number of candidates (upper), and fixed number of voters
(lower). Theshadedpartsdepictthestandarddeviation.
140Category Name #Votes #DistinctVotes
Political Irish 43942 29908
Political Glasgow 10376 5790
Political Aspen 2459 2018
Political ERS 380 336
Sport FigureSkating 9 9
Sport SpeedSkating 12 12
Sport TDF 15 15
Sport GDI 17 17
Survey T-Shirt 30 30
Survey Sushi 5000 4926
Survey Cities 392 392
Table 6.2: Number of votes in the real-life elections used as distributions for
sampling.
thatsomevoteswillbeselectedmultipletimes.
In Figure 6.3 we show the results. For the experiment with only four can-
didates (upper left matrix), we observe that there is a correlation between the
similarity with the impartial culture elections and their position in Figure 5.17.
The same is true (but on a smaller scale) for the cases of 6, 8 and 10 candidates.
Without surprise, we observe that the smaller is the number of distinct votes in a
givendistribution,themoresimilararetheelectionsfromthatdistributiontoeach
other (values displayed on the diameter). For elections with 8 and 10 candidates,
all values, except those on the diameter, are very small. The only part of the ma-
trixwithslightlylargervaluesisthelowerrightcorner,butthesimilaritybetween
thesemodelsisduetoasmallernumberofdifferentvotesinthedistributionfrom
whichtheseelectionsweresampled.
Interestingly, for elections with only four candidates, sport elections are less
similar to each other than the rest of elections, even though they have fewer dis-
tinct votes. The exception is the similarity between Tour de France (TDF) and
Giro d’Italia (GDI) – two cycling competitions, which (among sport elections)
seem to be very similar. Moreover, note that for GDI and TDF there is a very
largedifferencebetweenthecasefor4and6candidates,whencomparedwiththe
differencesforotherpairsofsportinstances.
Regarding impartial culture, political elections, and surveys with four candi-
141dates, when comparing any two elections, usually we can match two thirds of the
votes, which is around 33 votes out of 50. This number seems to be quite large
andimpliesahighlevelofchaosintheseinstances.
Another thing worth pointing out is the relative similarity between two polit-
ical elections: Irish and Glasgow. For elections with four candidates, Irish elec-
tions are the most similar to Glasgow ones; however, the opposite is not true. But
for elections with six candidates, both Glasgow and Irish are each other’s closest
instancesandmoresimilartoeachotherthananyotherpair(exceptfortheSpeed
SkatingandFigureSkating).
MainConclusions
The experimental results based on real-life elections from Preflib confirm
our previous observations (drawn from the map of real-life elections). For
example, we see that sport elections are similar to each other and quite
similar to identity. Also, as was the case for statistical cultures, analyzing
elections with very few candidates is not particularly meaningful because
mostsuchelectionsareverysimilartoeachother.
142(a)4candidates&50voters (b)6candidates&50voters
(c)8candidates&50voters (d)10candidates&50voters
Figure 6.3: The numbers denote the rounded % of matched votes for MAX.
COMMON VOTER-SUBELECTION. There are results for elections with 4, 6, 8,
and10candidatesand50voters.
1436.5 Summary
Wehaveshownthatvariantsof ELECTION ISOMORPHISM thatarebasedoncon-
sidering subelections are largely intractable but, nevertheless, some of them can
be solved in polynomial-time. In fact, we have used the polynomial-time solv-
able MAX. COMMON VOTER-SUBELECTION problem to analyze the similarity
betweenvariousdifferentmodelsofgeneratingrandomelections.
In Section 6.3 we classified variants of the problem as either belonging to P
orbeingNP-complete(andsomebeingW[1]-hard).
Finally, in Section 6.4 we presented some more experimental results based
on synthetic and real-life data, showing that computing the MAX. COMMON
VOTER-SUBELECTION can serve as a measure of similarity between elections.
Forexample,ithelpednoticingthedifferencebetweenWalshandConitzermodel,
with elections from Conitzer model being more similar to each other, than those
from Walsh model. Experimental results for four candidates confirms observa-
tionsfromChapter5,i.e.,mapsforelectionswithsmallnumberofcandidatesare
abitchaoticbecausemostoftheelectionsarequitesimilartoeachother.
MainContributions
The main contribution of this chapter is theoretical analysis of a family of
subelection problems. Moreover, we provide experimental results both on
synthetic and real-life data, showing that majority of small elections are
verysimilartoeachother.
144Chapter 7
Approval Elections
7.1 Introduction
So far we were focusing only on ordinal elections, where each voter ranks all the
candidatesfromthemosttotheleastappreciatedone. Inthischapterweconsider
approval elections [Brams and Fishburn, 1983]. In an approval election, each
voterindicateswhichcandidatesheorshefindsacceptableforacertaintask(e.g.,
to be a president, to join the parliament, or to enter the final round of a competi-
tion), and a voting rule is used to aggregate these preferences and determine the
winner or the winning committee. In the single-winner setting (e.g., when choos-
ing the president), the most popular rule is to pick the candidate with the highest
number of approvals. In the multiwinner setting (e.g., in parliamentary elections
or when choosing finalists in a competition), there is a rich spectrum of rules to
selectfrom,eachwithdifferentpropertiesandadvantages(see,e.g.,theoverview
of Lackner and Skowron [2023]. Approval voting is particularly attractive due to
its simplicity and low cognitive load imposed on the voters. In fact, its practical
applicability has already been tested in several field experiments, including those
in France [Laslier and der Straeten, 2008, Baujard and Igersheim, 2011, Baujard
et al., 2014, Bouveret et al., 2019] and Germany [Alós-Ferrer and Granic´, 2012].
Overtherecentyears,therewasalsoatremendousprogressregardingitstheoret-
icalproperties(see,e.g.,theoverviewofLaslierandSanver[2010].
In spite of all these achievements, numerical experiments regarding approval
votingarestillchallengingtodesign. Oneofthemaindifficultiesiscausedbythe
lack of consensus about which statistical cultures to use. To answer this problem,
in particular, we introduced various new statistical cultures. Moreoever, we eval-
145uated them using experiments – showing their usefullness, and at the same time
showing drawbacks of previously used models. Below we list a few cultures that
wererecentlyused:
1. In the impartial culture setting, we assume that each vote is equally likely.
Taken literally, this means that each voter approves each candidate with
probability 1/ 2 [Barrot et al., 2017]. As this is quite unrealistic, several
authorstreattheapprovalprobabilityasaparameter[Brederecketal.,2019,
Faliszewski et al., 2020] or require that all voters approve the same (small)
number of candidates [Lackner and Skowron, 2020]. A further refinement
is to choose an individual approval probability for each candidate [Lackner
andMaly,2021].
2. In Euclidean models, each candidate and voter is a point in Rd, where d is
a parameter, and a voter approves a candidate if they are sufficiently near.
Such models are used, e.g., by Bredereck et al. [2019] and Godziszewski
et al. [2021]. Naturally, the distribution of the candidate and voter points
stronglyaffectstheoutcomes.
3. Some authors consider statistical cultures designed for the ordinal setting
(where the voters rank the candidates from the most to the least desirable
one) and let the voters approve some top-ranked candidates (e.g., a fixed
number of them). This approach is taken, e.g., by Lackner and Skowron
[2020]ontopoftheordinalMallowsmodel(lateron,Alloucheetal.[2022]
and Caragiannis et al. [2022] provided approval-based analogues of the
Mallowsmodel).
Furthermore, even if two papers use the same model, they often choose its pa-
rameters differently. Since it is not clear how the parameters affect the models,
comparingtheresultsfromdifferentpapersisnoteasy.
Our goal is to initiate a systematic study of approval-based statistical cultures
and to attempt to rectify at least some of the above issues. We do so by applying
ourmapofelectionsframework.
Tocreateamapforapprovalelections,westartbyidentifyingtwometricsbe-
tweenapprovalelections,theisomorphicHammingdistanceandtheapprovalwise
distance. The first one is accurate, but difficult to compute, whereas the second
oneislessprecise,buteasilycomputable. Fortunately,inourelectiondatasetsthe
twometricsarestronglycorrelated;thus,weusemostlythelatterone.
146Next, we analyze the space of approval elections with a given number of can-
didates and voters. For each p ∈ [0,1], by p-identity (p-ID) elections we mean
those where all the votes are identical and approve the same p-fraction of candi-
dates. By p-impartial culture (p-IC) elections we mean those where each voter
chooses to approve each candidate with probability p. We view p-ID and p-IC
elections as two extremes on the spectrum of agreement between the voters and,
intuitively, we expect that every election (where each voter approves on average
apfractionofcandidates)islocatedsomewherebetweenthesetwo. Inparticular,
for p,ϕ ∈ [0,1], we introduce the(p,ϕ)-resampling model, which generates elec-
tions whose expected approvalwise distance from p-ID is exactly the ϕ fraction
of the distance between p-ID and p-IC (and the expected distance from p-IC is
the1−ϕfraction).
Armed with these tools, we proceed to draw maps of elections. First, we
consider p-ID, p-IC, and (p,ϕ)-resampling elections, where the p and ϕ values
arechosentoformagrid,andcomputetheapprovalwisedistancesbetweenthem.
We find that, for a fixed value of p, the (p,ϕ)-resampling elections indeed form
lines between the p-ID and p-IC ones, whereas for fixed ϕ values they form lines
between 0-ID and 1-ID ones (which we refer to as the empty and full elections).
We obtain more maps by adding elections generated according to other statistical
cultures; the presence of the (p,ϕ)-resampling grid helps in understanding the
locations of these new elections. For each of our elections we compute several
parameters,suchas,e.g,thehighestnumberofapprovalsthatacandidatereceives,
the time required to compute the results of a certain multiwinner voting rule, or
thecohesivenesslevel(seeSection7.2foradefinition). Foreachofthestatistical
cultures,wepresentmapswherewecolortheelectionsaccordingtothesevalues.
This gives further insight into the nature of the elections they generate. Finally,
we compare the results for randomly generated elections with those appearing in
real-life,inthecontextofparticipatorybudgeting.
We also provide maps of approval preferences (similar to those for ordinal
preferences shown in Section 3.4). We present maps from both voters’ and can-
didates’ perspectives (in the former ones each point depicts a voter, while in the
latteroneseachpointdepictsacandidate). TocreatethesemapsweusetheHam-
mingdistanceandtheJaccarddistance(whichisanormalizedvariantoftheHam-
mingdistancethatisputtingmoreemphasisonapprovalsthanondisapprovals).
The structure of this chapter is different from that of the previous ones. Since
wemovefromtheordinaltotheapprovalworldofelections,weneednewprelim-
inarieswherewedefineseveralthings,suchas,forinstance,avoteoranelection.
In a way, within this chapter we repeat the work that for ordinal elections was
147divided into several parts. That is, this chapter is an application of all the contri-
butionsfromthepreviousonesandshowshowthemapframeworkcanbeapplied
to new types of objects (see also our work on the maps of stable roommates in-
stances[Boehmeretal.,2023b])
7.2 Preliminaries
Elections. A (simple) approval election E = (C,V) consists of a set of can-
didates C = {c ,...,c } and a collection of voters V = (v ,...,v ). Each
1 m 1 n
voter v ∈ V casts an approval ballot, i.e., he or she selects a subset of candidates
thatheorsheapproves. Givenavoterv,wedenotethissubsetbyA(v). Occasion-
ally, we refer to the voters or their approval ballots as votes; the exact meaning
will always be clear from the context. An approval-based committee election (an
ABC election) is a triple (C,V,k), where (C,V) is a simple approval election
andk isthesizeofthedesiredcommittee. Weusesimpleelectionswhenthegoal
istochooseasingleindividualandABCelectionswhenweseekacommittee.
Given an approval election E (be it a simple election or an ABC one) and a
candidate c, we write score (c) to denote the number of voters that approve c.
av
We refer to this value as the approval score of c. The single-winner approval rule
(called AV) returns the candidate with the highest approval score (or the set of
suchcandidates,incaseofatie).
Distances Between Votes. For two voters v and u, their Hamming distance is
ham(v,u) = |A(v)△A(u)| = |A(v) \ A(u)| + |A(u) \ A(v)|, i.e., the number
of candidates approved by exactly one of them. Other distances include, e.g.,
the Jaccard one, defined as jac(v,u) = ham(v,u) . For other examples of such
|A(v)∪A(u)|
distances,wepointtotheworkofCaragiannisetal.[2022].
Approval-BasedCommitteeVotingRules. Anapproval-basedcommitteevot-
ing rule (an ABC rule) is a function that maps an ABC election (C,V,k) to a
nonempty set of committees of size k. If an ABC rule returns more than one
committee,thenweconsiderthemtied.
We introduce two prominent ABC rules. Multiwinner Approval Voting (AV)
selects the k candidates with the highest approval scores. Given a commit-
tee W, its approval score is the sum of the scores of its members; score (W) =
av
(cid:80)
score (w). Ifthereismorethanonecommitteethatachievesamaximum
w∈W av
148score, AV returns all tied committees. The second rule is Proportional Approval
Voting(PAV).PAVoutputsallcommitteeswiththemaximumPAV-score:
(cid:80)
score (W) = h(|A(v)∩W|),
pav v∈V
where h(x) =
(cid:80)x
1/ j is the harmonic function. Intuitively, AV selects commit-
j=1
teesthatcontainthe“best”candidates(inthesenseofhavingthemostapprovals)
and PAV selects committees that are in a strong sense proportional [Aziz et al.,
2017,Brilletal.,2018]. IncontrasttoAV,whichispolynomial-timecomputable,
PAV is NP-hard to compute [Aziz et al., 2015, Skowron et al., 2016]. In practice,
PAV can be computed by solving an integer linear program [Peters and Lackner,
2020]orbyanapproximationalgorithm[Dudyczetal.,2020].
Cohesive Groups. Intuitively, a proportional committee should represent all
groups of voters in a way that (roughly) corresponds to their size. To speak of
proportional committees in ABC elections, Aziz et al. [2017] introduced the con-
ceptofcohesivegroups.
Definition 7.1. Consider an ABC election (C,V,k) with n voters and some non-
negative integer ℓ. A group of voters V′ ⊆ V is ℓ-cohesive if (i) |V′| ≥ ℓ· n and
(cid:12)(cid:84) (cid:12) k
(ii)(cid:12) A(v)(cid:12) ≥ ℓ.
v∈V′
An ℓ-cohesive group is large enough to deserve ℓ representatives in the com-
mittee and is cohesive in the sense that there are ℓ candidates that can represent
it. A number of proportionality notions have been proposed based on cohesive
groups, such as (extended) justified representation [Aziz et al., 2017], propor-
tional justified representation [Sánchez-Fernández et al., 2017], proportionality
degree [Skowron, 2021], and others. For our purposes, it is sufficient to note that
all these concepts guarantee cohesive groups different types and levels of repre-
sentations (see also the survey of Lackner and Skowron [2023] for a comprehen-
siveoverview).
7.3 Statistical Cultures for Approval Elections
In the following, we present several statistical cultures (probabilistic models) for
generatingapprovalelections. Ourinputconsistsofthedesirednumberofvotersn
and a set of candidates C = {c ,...,c }. For models that already exist in the
1 m
literature,weprovideexamplesofpapersthatusethem.
149Resampling, IC, and ID Models. Let p and ϕ be two numbers in [0,1]. In the
(p,ϕ)-resampling model, we first draw a central ballot u, by choosing ⌊p · m⌋
approved candidates uniformly at random. Then, we generate each new vote v
by initially setting A(v) = A(u) and executing the following procedure for ev-
ery candidate c ∈ C: With probability 1 − ϕ, we leave c ’s approval intact and
i i
with probability ϕ we resample its value (i.e., we let c be approved with proba-
i
bility p). The resampling model is our contribution and is one of our basic tools
for analyzing approval elections. By fixing ϕ = 1, we get the p-impartial culture
model (p-IC) where each candidate in each vote is approved with probability p;
it was used, e.g., by Bredereck et al. [2019] and Faliszewski et al. [2020]. By
fixingϕ = 0,weensurethatallvotesinanelectionareidentical(i.e.,approvethe
samepfractionofthecandidates). Werefertothismodelasp-identity(p-ID).
Moving Model. The (p,ϕ)-moving model is a variant of the (p,ϕ)-resampling
one, where each time a new vote is generated, the new vote replaces the central
one. Occasionally, we also consider (p,ϕ,g)-moving model where we add one
moreparameterg,whichdenotesthenumberofgroups. Itworksasfollows. After
each ⌊n⌋ votes are generated, we set the central vote back to the original central
g
ballotinsteadofsettingittothelastvote. Notethatifthevalueofg isequaltothe
number of voters then (p,ϕ,g)-moving model and (p,ϕ)-resampling model are
equivalent, because after sampling each vote we are setting central ballot back to
theoriginalone.
Disjoint Model. The (p,ϕ,g)-disjoint model, where p and ϕ are numbers
in [0,1] and g is a non-negative integer, works as follows: We draw a random
partition of C into ⌊p · m⌋-sized g sets, C ,...,C (note that, if p · g < 1 then
1 g
some candidates will not be members of any group, and if p·g > 1 the model is
not well-defined), and, to generate a vote, we choose i ∈ [g] uniformly at random
and sample the vote from a (p,ϕ)-resampling model with the central vote that
approvesexactlythecandidatesfromC .
i
Noise Models. Let p and ϕ be two numbers from [0,1] and let d be a distance
betweenapprovalvotes(suchastheHammingorJaccardones). Werequirethatd
is polynomial-time computable and, for each two approval votes u and v, d(u,v)
depends only on |A(u)|, |A(v)|, and |A(u) ∩ A(v)|; both Hamming and Jaccard
distances have this property. In the (p,ϕ,d)-noise model we first generate a cen-
tralvoteuasintheresamplingmodeland,then,eachnewvotevisgeneratedwith
150probability proportional to ϕd(u,v). Such noise models are analogous to the Mal-
lows model for ordinal elections and were studied, e.g., by Allouche et al. [2022]
and Caragiannis et al. [2022]. In particular, Caragiannis et al. [2022] gave a sam-
plingprocedurefortheHammingdistance. Weextendittoarbitrarydistances.
Proposition7.1. Thereisapolynomial-timesamplingprocedureforthe (p,ϕ,d)-
noisemodels(asdefinedabove).
Proof. Let u be the central vote and let z = |A(u)|. Consider non-negative inte-
gers x and y such that x ≤ z and y ≤ m − z. The probability of generating a
vote v that contains x candidates from A(u) and y candidates from C \ A(u) is
proportionaltothefollowingvalue(abusingnotation,wewrited(x,y,z)tomean
thevalued(u,v);indeed,d(u,v)dependsonlyonx,y,andz):
f(x,y) =
(cid:0)z(cid:1)(cid:0)m−z(cid:1)
ϕd(x,y,z).
x y
(cid:80)
Next, let Z = f(x,y). To sample a vote, we draw values x ∈ [z]
x∈[z]0,y∈[m−z]0
and y ∈ [m−z] with probability f(x,y) and form the vote as approving x random
Z
membersofA(u)andy randommembersofC \A(u).
In the reminder, we only use the noise model with the Hamming distance and we
refertoitasthe(p,ϕ)-noisemodel. Notethattherolesofpandϕinthismodelare
similarbutnotthesameasinthe(p,ϕ)-resamplingmodel(forexample,forϕ = 0
wegetthep-IDmodel,butforϕ = 1wegetthe0.5-ICone).
Euclidean Models. In the t-dimensional Euclidean model, each candidate and
each voter is a point from Rt and a voter v approves candidate c if the distance
between their points is at most r (this value is called the radius); such models
werediscussed,e.g.,intheclassicalworksofEnelowandHinich[1984,1990],and
morerecentlybyElkindandLackner[2015],Elkindetal.[2017],Brederecketal.
[2019],andGodziszewskietal.[2021]. Weconsidert-dimensionalmodelsfort ∈
{1,2},wheretheagents’pointsaredistributeduniformlyatrandomon[0,1]t. We
refer to them as Interval and Square models (note that to fully specify each of
them,wealsoneedtoindicatetheradiusvalue).
TruncatedUrnModels. Letpbeanumberin[0,1]andletαbeanon-negative
realnumber(theparameterofcontagion). TruncatedurnmodelisbasedonPólya-
Eggenberger urn model (see Chapter 3), however, after sampling an ordinal vote
weconvertittoanapprovalone. Westartwithanurnthatcontainsallm!possible
151linear orders over the candidate set. To generate a vote, we (1) draw a random
order r from the urn, (2) produce an approval vote that consists of ⌈p · m⌉ top
candidates according to r (this is the generated vote), and (3) return αm! copies
of r to the urn. For α = 0, all votes with ⌈p·m⌉ approved candidates are equally
likely, whereas for large values of α all votes are likely to be identical (so the
modelbecomessimilartop-ID).
MainConclusions
Introduction of new statistical culture models, including: resampling, dis-
joint,andmovingmodels.
7.4 Maps of Approval Preferences
Now we will have a closer look at instances generated according to the statistical
cultures described above. We will conduct an analogous experiment to the one
describedinSection3.4,however,thistimewefocusonapprovalelections.
As a metric between two approval votes, we use the Hamming and Jaccard
distances. All generated instances consist of 100 candidates and 1000 voters. As
formapsofordinalpreferences,weusetheMDSembeddingandwithpurplediscs
wedepictthecaseswheremorethan10voteswereidentical(thelargerthecircle,
the more votes were identical). We present the results in Figures 7.1 and 7.2.
The names and the parameters of each instance are presented above each picture.
Now we discuss the results (more or less moving from the upper rows toward the
bottomones).
In the first row, we have the impartial culture elections. We see that for the
Hamming distance the closer we are to p = 0.5, the larger is the circle. For
small values of p the whole map has smaller diameter (i.e., the largest distance
between any two votes in a p-IC election is smaller) because all votes are similar
to one another just by not accepting numerous candidates. (For instance, if we
have 100 candidates and two disjoint votes each approving 10 candidates, then
their Hamming distance is 20, while if we have two disjoint votes each approv-
ing50candidates,thentheirHammingdistancesis100;hence,weobservealarge
difference between these values, even though in both cases the sets of approved
candidates are disjoint). The Hamming distance is symmetric with regard to ap-
provalsanddisapprovals,soifwereplacepwith1−pintheICmodel,weshould
have the same result. For example, we observe that the picture for p = 0.25 is
152Hamming Jaccard
Figure7.1: Mapsof(Approval)Preferences(100candidates,1000voters). Onthe
left (teal) based on the Hamming distance, and on the right (navy) based on the
Jaccarddistance.
153Hamming Jaccard
Figure7.2: Mapsof(Approval)Preferences(100candidates,1000voters). Onthe
left (teal) based on the Hamming distance, and on the right (navy) based on the
Jaccarddistance.
154alike to the one for p = 3. However, this is far from true for the Jaccard distance,
4
which is not symmetric with regard to approvals and disapprovals, and, in some
sense, favors the approvals. For the Jaccard distance when we increase the aver-
age number of approvals, the votes from the impartial culture will be on average
atsmallerdistancesfromeachother.
In the next four rows, we show results for the disjoint model (note that the
resamplingmodelisequivalenttothedisjointmodelwithonlyonegroup). Inthe
first three rows (i.e., for ϕ ∈ { 1 , 1 , 1 } we observe clear division into groups.
100 40 20
For the last row (i.e., ϕ = 0.75) the boundaries between the groups are fading
away.
In the last two rows, we show the moving model. In the first row, we fix ϕ
valueto 1 andincreasethenumberofgroupsfromonetofour. Notethathaving
200
two groups is equivalent to having one group. In the second row, we consider
larger numbers of groups, i.e., 10,25,50,100 and proportionally increased ϕ val-
ues, i.e., 1 , 1 , 1 , 1 , respectively. (We increase the ϕ value because otherwise,
100 40 20 10
for large numbers of groups, like 50 or 100, we would end up having many votes
extremelysimilartooneanother). Moreover,notethat,theresamplingmodelcan
be seen as an extreme case of the moving model, where the number of groups is
equaltothenumberofvotes.
In the second set of maps (Figure 7.2), we start with the noise model. When
weincreasetheϕvalue,wemoveclosertowardIC(inparticular,closerto0.5-IC).
Unlike for the resampling model, for noise model when we increase the noise we
also increase the average number of approvals (or decrease if the initial p value
was above 0.5). Then we have three rows of the Euclidean elections. Keep in
mindthatthelargertheradius,themoreapprovalswehaveonaverage.
Next, we have two rows for the urn elections. With α parameter increasing
from 1 upto1,andpequal 1 intheupperrow,and 1 inthelowerrow. Againwe
10 4 10
canseethatundertheHammingdistance,forsmallerpthediameterofthewhole
map is smaller. At the same time the Jaccard distance is proportionally stretching
themapsinthelowerrowsoforbothvaluesofptheylooksimilar.
Finally, we have four real-life instances based on the participatory budgeting
elections held in Warsaw in 2022. One Municipal, where citizens could approve
up to 10 projects, and three district ones, where citizens were allowed to approve
upto15projects. FortheHammingdistance,weobservedensecentersinallfour
instances. These centers depict the voters that selected only one project, hence,
are at most at distance 2 from each other. The further a given point is from the
center, the more projects were approved by the voter which that point represents.
For the Jaccard distance, we see that the votes were very diverse; however, some
155ofthemhadsomecopies. Inprinciple,wedonotobserveanyparticularstructure.
We also conducted a very similar experiment, but from the candidates’ per-
spective. Due to the fact, that results for candidates, in essence, were not signif-
icantly different from those for voters, we decided to shift detailed description of
theseresultstoAppendixB.
MainConclusions
The maps of approval preferences (similarly to the maps of ordinal prefer-
ences)confirmourintuitionaboutthebehaviorofstatisticalcultures. They
also help us get a better understanding of how particular parameters influ-
encethemodels.
7.5 Metrics
Next, we describe two (pseudo)metrics used to measure distances between ap-
provalelections. Sinceweareinterestedindistancesbetweenrandomlygenerated
elections,ourmetricsareindependentofrenamingthecandidatesandvoters.
Considertwoequally-sizedcandidatesetsC andD,andavoterv withaballot
over C. For a bijection σ: C → D, by σ(v) we mean a voter with an approval
ballot A(σ(v)) = {σ(c) | c ∈ C}. In other words, σ(v) is the same as v, but with
the candidates renamed by σ. Next, we define the isomorphic Hamming distance
(inspiredbytheisomorphicswapandSpearmandistancesSection2.2).
Definition 7.2. Let E = (C,V) and F = (D,U) be two elections, where |C| =
|D|, V = (v ,...,v )andU = (u ,...,u ). TheisomorphicHammingdistance
1 n 1 n
betweenE andF,denotedd (E,F),isdefinedas:
H
min min
(cid:0)(cid:80)n
ham(σ(v ),u
)(cid:1)
.
σ∈Π(C,D) ρ∈Sn i=1 i ρ(i)
Intuitively, under the isomorphic Hamming distance we unify the names of the
candidates in both elections and match their voters to minimize the sum of the
resulting Hamming distances. We call this distance isomorphic because its value
is zero exactly if the two elections are identical, up to renaming the candidates
and voters. Computing this distance is NP-hard (see also the related results for
approximategraphisomorphism[Arvindetal.,2012,Groheetal.,2018]).
Proposition 7.2. [Szufa et al. [2022]] Computing the isomorphic Hamming dis-
tancebetweentwoapprovalelectionsisNP-hard.
156Consequently, we compute this distance using a brute-force algorithm (which
is faster than using, e.g., ILP formulations). Since this limits the size of elections
that we can deal with, we also introduce a simple, polynomial-time computable
metric.
Definition 7.3. Let E be an election with candidate set {c ,...,c } and n
1 m
voters. Its approvalwise vector, denoted av(E), is obtained by sorting the
vector (score (c )/n, ...,score (c )/n) in the non-increasing order. Then,
av 1 av m
the approvalwise distance between elections E and F with approvalwise vec-
torsav(E) = (x ,...,x )andav(F) = (y ,...,y )isdefinedas:
1 m 1 m
d (E,F) = |x −y |+···+|x −y |.
app 1 1 m m
Inotherwords,theapprovalwisevectorofanelectionisasortedvectorofthenor-
malized approval scores of its candidates, and an approvalwise distance between
two elections is the ℓ distance between their approvalwise vectors. We sort the
1
vectors to avoid the explicit use of candidate matching, as is needed in the Ham-
ming distance. Occasionally we will speak of approvalwise distances between
approvalwisevectors,withoutreferringtotheelectionsthatprovidethem.
It is easy to see that the approvalwise distance is computable in polynomial
time. In fact, its definition is so simplistic that it is natural to even question its
usefulness. Initsspirit,theapprovalwisedistanceisverysimilartotheBordawise
distance (used for the ordinal elections); both distances convert elections to vec-
tors of length m and compare them. While the Bordawise distance seems not to
beveryuseful,surprisingly,theapprovalwisedistanceisquiteeffective.
In Section 7.7.3 we will see that in our election datasets the approvalwise
distanceisstronglycorrelatedwiththeHammingdistance. Thus,inthefollowing
discussion,wefocusonapprovalwisedistances.
7.6 A Grid of Approval Elections
To better understand the approvalwise metric space of elections, next we analyze
expecteddistancesbetweenelectionsgeneratedaccordingtothe(p,ϕ)-resampling
model.
Fixsomenumbermofcandidatesandparametersp,ϕ ∈ [0,1],suchthatpmis
aninteger,andconsidertheprocessofgeneratingvotesfromthe(p,ϕ)-resampling
model. Inthelimit,theapprovalwisevectoroftheresultingelectionis:
((1−ϕ)+(ϕ·p),...,(1−ϕ)+(ϕ·p),ϕ·p,...,ϕ·p).
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
p·m (1−p)·m
157full
m/
2
m/
2
d m
a=2mp(1−p)ϕ
m/
2 b=2mp(1−p)(1−ϕ)
0.5-IC 0.5-ID c=mp
d=m(1−p)
p-IC b a p-ID
(p,ϕ)
m/ 2 c m/ 2
empty
Figure7.3: Distancesbetweenresamplingelections.
Indeed, each of the p · m candidates approved in the central ballot ei-
ther stays approved (with probability 1 − ϕ) or is resampled (with prob-
ability ϕ, and then gets an approval with probability p). Analogous rea-
soning applies to the remaining (1 − p) · m candidates. With a slight
abuse of notation, we call the above vector av(p,ϕ). Furthermore, we refer
to av(p,0) as the p-ID vector, to av(p,1) as the p-IC vector, and to 0-ID and
1-ID vectors as the empty and full ones, respectively (note that 0-ID = 0-IC and
1-ID=1-IC).
Now, consider two additional numbers, p′,ϕ′ ∈ [0,1], such that p′m is an
integer. Simplecalculationsshowthat:
d (empty,full) = m,
app
d (p-IC,p-ID) = 2mp(1−p),
app
d (av(p,ϕ),av(p′,ϕ)) = m·|p−p′|,
app
d (av(p,ϕ),av(p,ϕ′)) = 2mp(1−p)·|ϕ−ϕ′|.
app
Thus d (av(p,ϕ),empty) = mp is a p fraction of the distance between empty
app
and full, and d (av(p,ϕ),p-ID) = 2mp(1−p)ϕ is a ϕ fraction of the distance
app
betweenp-ICandp-ID(seealsoFigure7.3). Furthermore,d (empty,full) = m
app
isthelargestpossibleapprovalwisedistance.
Intuitively, (p,ϕ)-resampling elections form a grid that spans the space be-
tween the extreme points of our election space; the larger the ϕ parameter, the
more “chaotic” an election becomes (formally, the closer it is to the p-IC elec-
tions), and the larger the p parameter, the more approvals it contains (the closer it
158istothefullelection). Weuse(p,ϕ)-resamplingelectionsasabackgrounddataset,
which consists of 241 elections with 100 candidates and 1000 voters each, with
thefollowingpandϕparameters:
1. p is chosen from {0,0.1,0.2,...,0.9,1} and ϕ is chosen from the inter-
val(0,1),1
2. ϕ is chosen from {0,0.25,0.5,0.75,1} and p is chosen from the inter-
val(0,1).
For each of these elections, we compute a point in R2, so that the Euclidean dis-
tances between these points are as similar to the approvalwise distances between
the respective elections as possible. For this purpose, we use the Fruchterman-
Reingold force-directed algorithm (see Section 2.5). For the resulting map, we
see the clear grid-like shape on the left side of Figure 7.4.2 Whenever we present
maps of elections later in the paper, we compute them in the same way as de-
scribedabove(butfordatasetsthatincludeotherelectionsinadditiontotheback-
groundones).
7.7 Experiments
In this section, we use the map of elections approach to analyze the quantitative
properties of approval elections generated according to our models. In particular,
wewillseehowanelection’spositioninthegridinfluenceseachoftheproperties,
and what parameters to use to generate elections with the quantitative property in
adesiredrange.
7.7.1 Experimental Design
We use the map framework to visualize information about the following four
statistics:
MaximalApprovalScore. The highest approval score among all the candidates
in a given election, normalized by the maximum possible score, i.e., the
numberofvoters.
1Bygeneratingtelectionswithaparameterfrominterval(a,b),wemeangeneratingoneelec-
tionforeachvaluea+ib−a,fori∈[t].
t+1
2While our visualizations fit nicely into the two-dimensional embedding, our election space
hasamuchhigherdimension.
159Figure7.4: Mapsfortheresamplingmodel.
CohesivenessLevel. The largest integer ℓ such that there exists an ℓ-cohesive
group(forcommitteesize10). Tocomputethis,weusethealgorithmbased
ontheoneprovidedbyJaneczkoandFaliszewski[2022].
VotersinCohesiveGroups. Fraction of voters that belong to at least one 1-
cohesivegroup(forcommitteesize10).
PAVRuntime. Runtime (in seconds) required to compute a winning committee
under the PAV rule, by solving an integer linear program provided by the
abcvotinglibrary[Lackneretal.,2021],usingtheGurobiILPsolver.
We use the background dataset and six new datasets. Five of them are gener-
ated using our statistical cultures and consist of 100 candidates and 1000 voters
(except for the experiments related to the cohesiveness level, where we have 50
candidatesand100voters,duetocomputationtime). Wehave:
• 225 elections from the noise model with Hamming distance (25 for each
p ∈ {0.1,0.2,...,0.9}withϕ ∈ (0,1));
• 250 elections from the disjoint model (50 for each g ∈ {2,3,4,5,6}
withϕ ∈ (0.05,1/ g));
160• 225 elections from the moving model (25 for each p ∈ {0.1,0.2,...,0.9}
withϕ ∈ (0,1/ 100),andg = 1);
• 200 elections from Euclidean model (100 for Interval, with radius in
(0.0025,0.25), and 100 for Square, with radius in (0.005,0.5)); these pa-
rametersareasusedbyBrederecketal.[2019].
• 225 elections from the truncated urn model (25 for each p ∈ {0.1,
0.2,...,0.9}withα ∈ (0,1));
The last dataset uses real-life participatory budgeting data and contains 44
elections from Pabulib [Stolicki et al., 2020], where for each (large enough) elec-
tion we randomly selected a subset of 50 candidates and 1000 voters (other real-
lifedatasetsweconsideredhadmuchfewercandidates).
7.7.2 Experimental Results
Our visualizations are shown in Figures 7.4, 7.5, 7.6, and 7.7. We use the grid
structure of the background dataset for comparison with other datasets. Notably,
some of them do not fill this grid: the disjoint model (Figure 7.5b) is restricted
to the lower half (i.e., the disjoint model does not yield elections with very many
approvals), the Euclidean model (Figure 7.6d) is restricted to the left half (due
to the uniform distribution of points, its elections are rather “chaotic”), and the
real-worlddatasetPabulib(Figure7.7f)isplacedverydistinctlyinthebottomleft
part.
Togetanintuitiveunderstandingofthefourstatistics,letusconsidertheback-
grounddatasetinFigure7.4. Weseethatthehighestapprovalscorevalueislowest
in the lower left side and increases toward up and right. This is sensible: If the
average number of approved candidates increases, so does this statistic. Further-
more, if voters become more homogeneous, high-scoring candidates are likely to
exist. Moreover, regarding voters in cohesive groups, it turns out that in most
elections almost all voters belong to some 1-cohesive groups, with the left lower
part as an exception (where there are not enough approvals to form 1-cohesive
groups). The time needed to find a winning committee under PAV is correlated
withthedistancefrom0.5-IC.Weseethatittakesthelongesttofindwinningcom-
mitteesiftheelectionisunstructured. Similarlytothehighestapprovalscore,the
cohesiveness level increases when moving up or right in the diagram. Cohesive
groups with levels close to the committee size only exist in very homogeneous
elections(rightmostpath)andelectionswithmanyapprovals(toppart).
161Figure 7.5: Maps for (a) the noise model and (b) the disjoint model. The darker a
dotinthemainplotis,thelargeristhevalueoftheϕparameter.
162Figure 7.6: Maps for (c) the moving model, (d) the Euclidean model. The darker
a dot in the main plot is, the larger is the value of ϕ parameter for (c), and the
largeristhelengthoftheradiusfor(d).
163Figure7.7: Mapsfor(e)thetruncatedurnmodeland(f)Pabulib. Thedarkeradot
inthemainplotis,thelargeristhevalueofα parameterfor(e).
164Wemoveontotheresultsforthesixotherdatasets. Notethateachfigurealso
contains the background dataset (gray dots) for reference. These results help to
understandthedifferencesbetweenourstatisticalcultures.
The maximum approval score statistic provides insight into whether there is
a candidate that is universally supported. Instances with a value close to 1 pos-
sess such a candidate. In a single-winner election, this candidate is likely to be a
clear winner. This is undesirable when simulating, for example, contested elec-
tions. Also note that in the real-world data set (Pabulib) we do not observe such
candidates.
WhenlookingatthePAVruntime,wefindsomestatisticalculturesthatgener-
ate computationally difficult elections, such as, e.g., the (p,ϕ)-resampling model
with parameter values close to p = 0.5 and ϕ = 1 (0.5-IC), the noise model with
parameters p ∈ [0.5,0.9] and ϕ > 0.5, and the disjoint model with g = 2. Yet,
instancesfromthereal-worlddataset,aswellasfromtheEuclideanandurnones,
canbecomputedveryquickly.3
Concerningvotersincohesivegroups,wheneverthisstatisticiscloseto1,itis
easytosatisfymostvoterswithatleastoneapprovedcandidateinthecommittee;
such committees are easy to find [Aziz et al., 2017]. Since many proportional
rules take special care of voters who belong to cohesive groups, in such elections
therearenovotersthatareatasystematicdisadvantage. Inmanyofourgenerated
elections (almost) all voters belong to 1-cohesive groups, but this is not the case
for the real-world, Pabulib data. Indeed, to simulate Pabulib data well, we would
likelyneedtoprovidesomenewstatisticalculture(s).
For the cohesiveness level, we see that all models generate a full spectrum
(i.e., [0,10]) of cohesiveness levels. However, we expect realistic elections to
appearinthe“lowerleft”partofourgrid(withfewapprovals),andsuchelections
tend to have low cohesiveness levels. Indeed, this is also the case for the Pabulib
elections. Hence, it is important how proportional rules treat ℓ-cohesive groups
withsmallℓ.
7.7.3 Correlation
Figures 7.4 to 7.7 are based on the approvalwise distance. We argue that they
would not change much if we used the (computationally intractable) isomorphic
3Lessthan1secondonasinglecore(IntelXeonPlatinum8280CPU@2.70GH)ofa224core
machinewith6TBRAM.Incontrast,theworst-caseinstance(0.3-IC)required25minuteson13
cores.
165Figure7.8: CorrelationbetweenisomorphicHammingandapprovalwisemetrics.
Hamming distance. To this end, we generated 413 elections with 10 candidates
and 50 voters from the statistical cultures used in the previous experiment. The
dataset we use for comparing metrics consists of: 40 elections from the disjoint
models, 45 elections from the noise models with Hamming distance, 50 elections
from moving model, 50 elections from the truncated urn models, 50 elections
fromEuclideanmodels,134electionsfromresamplingmodels, 20electionsfrom
IC, 20 elections from ID, and four extreme elections (i.e., 0.5-IC, 0.5-ID, Empty,
Full).
We compare Hamming and approvalwise distances. The results are presented
inFigure7.8. Eachdotthererepresentsapairofelections,anditscoordinatesare
thedistancesbetweenthem,accordingtotheHammingandapprovalwisemetrics.
The Pearson Correlation Coefficient is 0.9899, and for 67% of pairs of elections
the distances are identical. In Table 7.1 we take a more fine-grained view of dif-
ferent models, presenting PCC individually for each of them. In each row, we
present a correlation based on distances between elections, where at least one
of the elections is from a given model. As we can see, when computing Ham-
ming and approvalwise distances from ID we have a perfect correlation, while
for distances from impartial culture we have the worst correlation – which is still
extremelyhighandequals0.966.
166StatisticalCulture PCC %equal
Identity 1.0 1.0
Disjoint 0.997 0.777
Moving 0.995 0.652
2DEuclidean 0.994 0.682
Resampling 0.992 0.713
TruncatedUrn 0.985 0.490
Noise 0.974 0.579
1DEuclidean 0.971 0.494
ImpartialCulture 0.966 0.556
Table 7.1: Pearson correlation coefficients between the Hamming distance and
the approvalwise distance for each statistical culture used in our maps. The last
column contains the percentage of pairs of elections for which both distances are
equal.
MainConclusions
Theconclusionsofourexperimentsareasfollows.
• The approvalwise distance is surprisingly (given how simple it is)
strongly correlated with the Hamming distance, which we treat as
theidealone.
• Resampling model seems to be quite powerful. We can easily inter-
pretitsparametersandgeneratealargevarietyofelections.
7.8 Summary
Weintroducedseveralmodelsforgeneratingsyntheticapprovalelections. Webe-
lievethatthesemodels(inparticular,theresamplingmodel)willmakeiteasierto
perform future experiments that involve approval elections. We also introduced
twodistancesbetweenapprovalelections;oneisomorphicidealone,whichispre-
cise but slow (it takes a lot of time to compute it), and other that is less precise,
butfast(canbecomputedimmediatelyevenforinstanceswiththousandsofvoters
and candidates), and strongly correlated with the ideal one. We presented the ap-
plications of the map of approval elections showing how different models behave
167under different circumstances. Among others, we analyzed the running time of
PAV rule, exhibiting regions of the map in which the time needed to compute the
winningcommitteeisthelongest. Moreover,weshowwheresomeofthereal-life
elections lie on the map, however, a good direction and an important task for fu-
tureworkistobroadlystudymorereal-lifedatasetswiththemethodsproposedin
thischapter.
MainContributions
• Introductionofnewmodelsforgeneratingapprovalelections. Inpar-
ticular, introduction of the resampling model, which turned out to
be very practical and is already used by other researchers (Brill and
Peters[2023],Lackneretal.[2023]).
• Adaptationofthemapofelectionsframeworkforapprovalballots.
• Experimental analysis of the introduced models, showing their
strengthsandweaknesses.
168Chapter 8
Discussion & Future Work
We would like to emphasize that the main contribution of this thesis is a frame-
work that can be used for numerous novel applications. We started a new line
of research and to the date of submission of this thesis, there are already several
papersusingthecontentprovidedwithinthisdissertation.
There exist many possible applications and extensions of the presented re-
search. One possibility is to use the framework to study new types of instances.
A good example of direct application of the map framework is a recent paper
dedicated to the Stable Roommates and Stable Marriage instances. (This paper
receivedBestStudentPaperAwardatAAMAS-2023).
• AMapofDiverseSyntheticStableRoommatesInstances
NiclasBoehmer,KlausHeeger,andStanisławSzufa;AAMAS-2023b
Another approach is to use the map concept to visualize the election data, what
wasdonein:
• Collecting,Classifying,Analyzing,andUsingReal-WorldRankingData
NiclasBoehmerandNathanSchaar;AAMAS-2023
Moreover, one can study more deeply proposed distances and aggregate repre-
sentation of election associated with them. For example, in the following paper,
authorsfocusontheanalysisofpositionmatrices.
• PropertiesofPositionMatricesandTheirElections
Niclas Boehmer, Jin-Yi Cai, Piotr Faliszewski, Austen Z. Fan, Łukasz
Janeczko,AndrzejKaczmarczyk,andTomaszWa¸s;AAAI-2023a
169Below,wepresentalistofotherpapersthatalsostudysimilarproblems. Note
that,inthischapter,wementiononlythoseworkswhichareeithercoauthoredby
StanisławSzufaorbyhisclosecoworkers.
• Diversity,Agreement,andPolarizationinElections
Tomasz Wa¸s, Piotr Faliszewski, Andrzej Kaczmarczyk, Krzysztof Sornat,
andStanisławSzufa;IJCAI-2023b
• An Experimental Comparison of Multiwinner Voting Rules on Approval
Elections
PiotrFaliszewski,MartinLackner,KrzysztofSornat,andStanisławSzufa;
IJCAI-2023c
• ParticipatoryBudgeting: Data,Tools,andAnalysis
PiotrFaliszewski,JarosławFlis,DominikPeters,GrzegorzPierczyn´ski,Pi-
otrSkowron,DariuszStolicki,StanisławSzufa,NimrodTalmon;
IJCAI-2023a
• A Quantitative and Qualitative Analysis of the Robustness of (Real-World)
ElectionWinners1
Niclas Boehmer, Robert Bredereck, Piotr Faliszewski, and Rolf Nieder-
meier;EAAMO-2022b
• DiscoveringConsistentSubelections
Łukasz Janeczko, Jérôme Lang, Grzegorz Lisowski, and Stanisław Szufa;
ToappearatAAMAS-2024
There are also some related problems that have not been given enough atten-
tionyet. Forexample,givenasetofordinalelectionscalledB,findanewelection,
such that its distance to the closest election from B is the largest possible. This
will allow us to fill in the potential gaps in our map. Another problem is how to
reasonablycompareapprovalandordinalelections,and,moregenerally,elections
ofdifferentsizesandelectionswithpartialpreferencedata.
We see this thesis as an invitation to a deeper study of different elections,
statisticalcultures,andtheirrelations.
1PreviouslythepaperwascalledOntheRobustnessofWinners: CountingBriberiesinElec-
tions,andinitsfullarXivversionusedthemapofelectionsframework.
170Appendix A
Distances Between the Compass
Elections
Here,weprovidemissingproofsfromSection4.5.1.
EMD-Positionwise
Proposition4.14. Ifmisdivisibleby4,thenitholdsthat:
1. d (ID ,UN ) = 1(m2 −1),
pos m m 3
2. d (ID ,AN ) = d (UN ,ST ) = m2,
pos m m pos m m 4
3. d (ID ,ST ) = d (UN ,AN ) = 2(m2 −1),
pos m m pos m m 3 4
4. d (AN ,ST ) = 13m2 − 1.
pos m m 48 3
Proof. ID and UN . We start by computing the distance between ID and
m m m
UN . Note that UN always remains the same matrix regardless of how its
m m
columns are ordered. Thus, we can compute the distance between these two ma-
tricesusingtheidentitypermutationbetweenthecolumnsofthetwomatrices:
171m
(cid:88)
dpos(IDm,UNm)= emd((IDm)i,(UNm)i)
i=1
=(cid:80)m ((cid:80)i−1 j +(cid:80)m−i j )
i=1 j=1 m j=1 m
= 1 (cid:80)m (1+(i−1)(i−1)+ 1+(m−i)(m−i))
m i=1 2 2
= 1 (cid:80)m (2i2−2i−2mi+m2+m)
2m i=1
1 m(m+1)(2m+1)
= (2 −m(m+1)−m2(m+1)+m(m2+m))
2m 6
1 (m2+m)(2m+1)
= ( −(m+1)(m+m2)+m(m2+m))
2m 3
m+1 (2m+1)
= ( −(m+1)+m)
2 3
(m+1)(m−1)
=
3
1
= (m2−1).
3
In the following, we use (∗) when we omit some calculations analogous to the
calculationsford (ID ,UN ).
pos m m
UN and ST : Similarly, we can also directly compute the distance between
m m
UN and ST using the identity permutation between the columns of the two
m m
matrices. Inthiscase,allcolumnvectorsofthetwomatriceshaveinfactthesame
emddistancefromeachother:
d (UN ,ST ) = m·(1 +2·(cid:80)m 2−1 i ) = m + m(m −1) = m2.
pos m m 2 i=1 m 2 2 2 4
UN and AN : Next, we compute the distance between UN and AN using
m m m m
the identity permutation between the columns of the two matrices. Recall that
AN canbewrittenas:
m
(cid:20) (cid:21)
ID rID
AN = 0.5 m/2 m/2 .
m rID ID
m/2 m/2
Thus, it is possible to reuse our ideas from computing the distance between iden-
tityanduniformity:
d (UN ,AN ) = 4(cid:80)m 2 ((cid:80)i−1 j +(cid:80)m 2−i j ) = (∗) = 2(m2 −1).
pos m m i=1 j=1 m j=1 m 3 4
ID and ST : There exist only two different types of column vectors in ST ,
m m m
i.e., m columnsstartingwith m entriesofvalue 2 followedby m zero-entriesand
2 2 m 2
m columnsstartingwith m zeroentriesfollowedby m entriesofvalue 2. InID ,
2 2 2 m m
m columns have a one entry in the first m rows and m columns have a one entry
2 2 2
172in the last m rows. Thus, again, the identity permutation between the columns of
2
thetwomatricesminimizestheemddistance:
d (ID ,ST ) = 2·d (ID ,UN ) = 2(m2 −1)
pos m m pos m 2 m 2 3 4
AN and ST : We now turn to computing the distance between AN =
m m m
(an ,...,an ) and ST = (st ,...,st ). As all column vectors of AN are
1 m m 1 m m
palindromes, each column vector of AN has the same emd distance to all col-
m
umn vectors of ST , i.e., for i ∈ [m] it holds that emd(an ,st ) = emd(an ,st )
m i j i j′
for all j,j′ ∈ [m]. Thus, the distance between AN and ST is the same for all
m m
permutation between the columns of the two matrices. Thus, we again use the
identity permutation. We start by computing emd(an ,st ) for different i ∈ [m]
i i
separately distinguishing two cases. Let i ∈ [m]. Recall that an has a 0.5 at
4 i
position i and position m−i+1 and that st has a 2 at entries j ∈ [m]. We now
i m 2
analyzehowtotransforman tost . Forallj ∈ [i−1],itisclearthatitisoptimal
i i
that the value 2 moved to position j comes from position i. The overall cost of
m
thisis(cid:80)i−1 2j. Moreover,theremainingsurplusvalueatpositioni(thatis, 1−2i)
j=1 m 2 m
needs to be moved toward the end. Thus, for j ∈ [i + 1, m], we move value 2
4 m
frompositionitopositionj. Theoverallcostofthisis(cid:80)m 4−i 2j. Lastly,weneed
j=1 m
to move value 2 to positions j ∈ [m +1, m]. This needs to come from position
m 4 2
m−i+1. Thus,foreachj ∈ [m+1, m],wemovevalue 2 frompositionm−i+1
4 2 m
topositionj. Theoverallcostofthisis
1·(m−i)+(cid:80)m
4 2j = 1(m−i)+ m +1
2 2 j=1 m 2 2 16 4
Now,leti ∈ [m +1, m]. Forj ∈ [m],weneedtomovevalue 2 fromposition
4 2 4 m
itopositionj. Theoverallcostofthisis 1 ·(i− m
−1)+(cid:80)m
4 2j = 1 ·(i− m −
2 4 j=1 m 2 4
1)+ m +1. Forj ∈ [m+1, m],weneedtomovevalue 2 frompositionm−i+1
16 4 4 2 m
topositionj. Theoverallcostofthisis
1·(m−i)+(cid:80)m
4 2j = 1·(m−i)+m+1.
2 2 j=1 m 2 2 16 4
Observing that the case i ∈ [3m +1,m] is symmetric to i ∈ [m] and the case
4 4
i ∈ [m +1, 3m] is symmetric to i ∈ [m +1, m] the emd distance between AN
2 4 4 2 m
andST canbecomputedasfollows:
m
173m m
1 (cid:88)4 m m m 1 1 (cid:88)2 m m m 1
dpos(ANm,STm)=2·(A+ ·( −i)+ ·( + )+ ·( ·(i− −1))+ ·( + )
2 2 4 16 4 2 4 4 16 4
i=1 i=m+1
4
m
1 (cid:88)2 m m m 1
+ ·( −i)+ ·( + )
2 2 4 16 4
i=m+1
4
m2 1 3m2−4m m m 1 m2−4m m m 1
= − + + ·( + )+ + ·( + )
48 3 32 2 16 4 32 2 16 4
m2−4m m m 1
+ + ·( + )
32 2 16 4
m2 1 3m2−4m 3m m 1 m2−4m
= − + + ( + )+
48 3 32 2 16 4 16
1 3 3 1 4 3 4 1
=( + + + )m2+(− + − )m
48 32 32 16 32 8 16 3
13 1
= m2−
48 3
with
A = (cid:80)m 4 ((cid:80)i−1 2j +(cid:80)m 4−i 2j) = (∗) = 1(m2 −1) = 1(m2 − 1)
i=1 j=1 m j=1 m 6 16 2 48 3
ID and AN : Lastly, we consider ID = (id ,...,id ) and AN =
m m m 1 m m
(an ,...,an ). Note that, for i ∈ [m], id contains a 1 at position i and an
1 m i i
contains a 0.5 at position i and position m − i. Note further that for i ∈ [m] it
2
holds that an = an . Fix some i ∈ [m]. For all j ∈ [i,m − i + 1] it holds
i m−i+1 2
thatemd(an ,id ) = m−2i+1 andforallj ∈ [1,i−1]∪[m−i+2,m]itholdsthat
i j 2
emd(an ,id ) > m−2i+1. That is, for every i ∈ [m], an has the same distance to
i j 2 i
allcolumnvectorsofID wheretheoneentryliesinbetweenthetwo0.5entries
m
of an but a larger distance to all column vectors of ID where the one entry is
i m
above the top 0.5 entry of an or below the bottom 0.5 entry of an . Thus, it is
i i
optimal to choose a mapping of the column vectors such that for all i ∈ [m] it
holds that an is mapped to a vector id where the one entry of id lies between
i j j
the two 0.5 in an . This is, among others, achieved by the identity permutation,
i
whichweusetocompute:
d (ID ,AN ) = 2(cid:80)m 2 (1(m−2i+1)) = mm− m(m +1)+ m = m2
pos m m i=1 2 2 2 2 2 4
174ℓ -Positionwise
1
Proposition4.15. Ifmisdivisibleby4,thenitholdsthat:
1. dℓ1 (ID ,UN ) = 2(m−1)
pos m m
2. dℓ1 (UN ,AN ) = dℓ1 (AN ,ST ) = dℓ1 (ID ,ST ) = 2(m−2)
pos m m pos m m pos m m
3. dℓ1 (UN ,ST ) = dℓ1 (ID ,AN ) = m
pos m m pos m m
Proof. ID and UN : Whenever computing the distances between UN and
m m m
anyothermatrix,wecanassumetheidentitypermutationbetweenthecolumnsof
the both matrices; any other permutation will produce exactly the same distance
becauseintheUN matrixallcolumnsareidentical. Onthediagonal,wehavem
m
elementscontributing|1− 1|tothetotaldistanceeach,andalltheotherm(m−1)
m
elements are contributing |0 − 1| each. Hence, the total distance is 1 · m(m −
m m
1)+ m−1 ·m = 2(m−1).
m
UN and ST : Similarly, we can also directly compute the distance between
m m
UN andST usingtheidentitypermutation. Eachelementcontributes 1 (either
m m m
|1 − 2|,or|1 −0|),hencethetotaldistanceis 1 ·m2 = m.
m m m m
UN and AN : Again, we can directly compute the distance between UN
m m m
and AN using the identity permutation. Each element on the diagonal and anti-
m
diagonal contributes |1 − 1| to the total distance, while all the other m(m − 2)
2 m
elementscontributes 1 each. Therefore,thetotaldistanceis m−2·2m+ 1 ·m(m−
m 2m m
2) = 2(m−2).
ID andST : Letusassumetheidentitypermutation. Bothmatriceshavezeros
m m
intheupper-rightandlower-leftquarter,hencewefocusonlyontheupper-leftand
bottom-right quarters. However, note that each of these two parts is equivalent to
ID forID ,andUN forST . Therefore,thetotaldistanceis2·2(m−1) =
m/2 m m/2 m 2
2(m − 2). If we use any another permutation than identity permutation, then
each candidate can contribute to the total distance either the same as for identity
permutationormore(i.e.,2insteadof2·(1− 2)).
m
AN andST : Letusassumetheidentitypermutation. Allelementsfromupper-
m m
left and lower-right quarters (but not on a diagonal) contribute 2 to the total dis-
m
tance. All elements from upper-right and lower-left quarters (but not on an anti-
diagonal) contribute 0 because they are equal in both matrices. All elements on
thediagonalcontribute 1 − 2,andallelementsontheanti-diagonalcontribute 1.
2 m 2
175Therefore, the total distance is 2 ·(1m2 −m)+ m−4m+ 1m = 2(m−2). Any
m 2 2m 2
otherpermutationwillproduceexactlythesamedistance.
ID and AN : Again, let us assume the identity permutation. The elements on
m m
the diagonal and anti-diagonal contributes to the total distance 1 (either |1 − 1|
2 2
or |0 − 1|) each. All the other elements in both matrices are zeros, hence the
2
total distances is 1 · 2m = m. If we use any another permutation than identity
2
permutation, then each candidate can contribute to the total distance either the
sameasforidentitypermutationormore(i.e.,2insteadof1).
176ℓ -Pairwise
1
Proposition4.16. Itholdsthat:
1. d (ID ,UN ) = 1m(m−1)
pair m m 2
2. d (UN ,ST ) = 1m2
pair m m 4
3. d (ID ,ST ) = 1m(m−2)
pair m m 4
Proof. Given the fact that each matrix is not defined on the diagonal, we omit it
inourreasoning,andfocusonlyontheotherm(m−1)elements.
ID andUN : InUN matrixallcolumnvectorsareidentical,hencewedonot
m m m
needtoworryaboutthecandidatepermutation. Therefore,calculatingthedistance
is straightforward. If we use the identity permutation (or any other permutation),
then the distances is as follows. Each element contributes 1 (either |1 − 1| or
2 2
|0− 1|)tothetotaldistance. Hence,thetotaldistanceis 1m(m−1).
2 2
UN and ST : Like for the previous distance, we do not need to worry about
m m
the permutation, and can simply assume that we use identity permutation. The
values in the upper-left and lower-right quarters of both matrices are identical, so
the distance between the elements in these parts is zero. As for the upper-right
and lower-left quarters, each element contributes 1 (either |1− 1| or |0− 1|) like
2 2 2
forthedistancesbetweenID andUN . Thereare 1m2 suchelements,hencethe
m m 2
totaldistanceis 1(m2).
4
ID and ST : Let us assume the identity permutation. The values in the upper-
m m
rightandlower-leftquartersofbothmatricesareidentical,sothedistancebetween
the elements in these parts is zero. As for the upper-right and lower-left quarters,
for ID each of these parts is equivalent to ID , and for ST each of these part
m m/2 m
is equivalent to UN . Hence, the total distance is twice the distances between
m/2
ID
m/2
andUN
m/2
whichis2·1/ 2· m 2(m
2
−1) = 1 4m(m−2). Ifweuseanyanother
permutation than identity permutation, then each candidate can contribute to the
totaldistanceeitherthesameasforidentitypermutationormore.
177EMD-Bordawise
Proposition4.17. Ifmiseven,itholdsthat:
1. d (ID ,UN ) = 1 ·m(m2 −1)
Borda m m 12
2. d (UN ,ST )) = 1 ·m2(m−1)
Borda m m 16
3. d (ID ,ST ) = 1 ·m(m2 +3m−4)
Borda m m 48
Proof. Thecalculationsareasfollows.
m/2 1 m m/2−1 m
d (ID ,UN+m) = n Σ(i− ) +n Σ i( −i)
Borda m
2 2 2
i=1 i=1
(cid:20) (cid:21)
1 1 1
= n m3 + (m−2)m2 − (m−2)(m−1)m
16 16 24
1
= n·m(m2 −1)(forevenm),
12
mmm−1 1
d (UN ,ST ) = n = n·m2(m−1),
Borda m m
2 2 4 16
d (ID ,ST ) = d (UN ,ID )−d (UN ,ST )
Borda m m Borda m m Borda m m
1
= n·m(m2 +3m−4).
48
178Appendix B
Maps of Approval Candidates
Let S(c) denote the set of supporters of candidate c (i.e., those voters that ap-
prove c). Then, for two candidates c and d, their (candidate) Hamming distance
is |S(c)△S(d)|, i.e., the number of voters that approve exactly one of them. The
(candidate)Jaccarddistanceis |S(c)△S(d)|.
|S(c)∪S(d)|
In Figures B.1 and B.2 we present the candidate maps. As before, on the
left side are the results for the Hamming distance, and on the right side, are the
resultsfortheJaccardone. Weusedexactlythesameelectionsasbefore(i.e.,100
candidates and 1000 voters). With the purple discs, we depict the cases where
more than five candidates are identical. Numerous things which were true for the
maps of votes are also true for the maps of candidates; hence, we mainly focus
on the differences. The first difference is related to the disjoint model. Here, we
observe one more cloud of points than the number of groups (with an exception
for the map with g = 4). It is because the additional group consists of candidates
that were not approved in any of the initial ballots. For g = 4 we do not witness
suchagroupbecauseeachcandidateisamemberofoneofthegroups. Regarding
the maps for the resampling model with ϕ = 3, for the maps of preferences, all
4
four maps were almost indistinguishable. However, for the maps of candidates,
weseethedifferencesbetweenmaps(especiallybetweenthefirsttwo).
For the noise model (as for the resampling model) we see a crucial difference
between the candidates’ and the voters’ perspectives. Candidates are divided into
two groups, those that are approved in the central ballot, and those that are not.
What is interesting, although justified, is the fact that the maps for the Euclidean
electionsforcandidatesareverysimilartotheanalogousmapsforthevoters. The
picturesfortheurnelectionsarerelativelychaotic. Intheurnelections,thelarger
theα,thesmallerthenumberofdifferentvotes.
179Forreal-lifeelections,fortheHammingdistance,theconcentrationinthemid-
dle of all four instances is due to numerous weak projects that are similar to each
other because they were disapproved by most voters. So, the farther a project is
from the center, the more approval it is likely to get. If we look at the Jaccard
maps, we observe tiny clustering of points in the outskirts, which means there
were some groups of similar projects; however, there were no groups of projects
thatwereapprovedbyalargefractionofthesociety. FortheJaccarddistance,not
much information can be gained from these pictures, which is an information in
itself. Itmeansthatthereisnotmuchstructureinreal-lifeelections.
180Hamming Jaccard
FigureB.1: Mapsof(Approval)Candidates(100candidates,1000voters). Onthe
left (teal) based on the Hamming distance, and on the right (navy) based on the
Jaccarddistance.
181Hamming Jaccard
FigureB.2: Mapsof(Approval)Candidates(100candidates,1000voters). Onthe
left (teal) based on the Hamming distance, and on the right (navy) based on the
Jaccarddistance.
182Bibliography
T. Allouche, J. Lang, and F. Yger. Truth-tracking via approval voting: Size mat-
ters. InProceedingsoftheAAAI-2022,volume36,pages4768–4775,2022.
C. Alós-Ferrer and D. Granic´. Two field experiments on approval voting in Ger-
many. SocialChoiceandWelfare,39(1):171–205,2012.
V. Arvind, J. Köbler, S. Kuhnert, and Y. Vasudev. Approximate graph isomor-
phism. InProceedingsofMFCS-2012,pages100–111,2012.
H. Aziz, S. Gaspers, J. Gudmundsson, S. Mackenzie, N. Mattei, and T. Walsh.
Computational aspects of multi-winner approval voting. In Proceedings of
AAMAS-2015,pages107–115,2015.
H. Aziz, M. Brill, V. Conitzer, E. Elkind, R. Freeman, and T. Walsh. Justified
representationinapproval-basedcommitteevoting. SocialChoiceandWelfare,
48(2):461–485,2017.
L.Babai,A.Dawar,P.Schweitzer,andJ.Torán. Thegraphisomorphismproblem
(dagstuhlseminar15511). DagstuhlReports,5(12):1–17,2015.
M. Ballester and G. Haeringer. A characterization of the single-peaked domain.
SocialChoiceandWelfare,36(2):305–322,2011.
N.Barrot,J.Lang,andM.Yokoo. ManipulationofHamming-basedapprovalvot-
ingformultiplereferendaandcommitteeelections. InProceedingsofAAMAS-
2017,pages597–605,2017.
J. Bartholdi, III and M. Trick. Stable matching with preferences derived from a
psychologicalmodel. OperationsResearchLetters,5(4):165–169,1986.
183J. Bartholdi, III, C. Tovey, and M. Trick. Voting schemes for which it can be
difficult to tell who won the election. Social Choice and Welfare, 6(2):157–
165,1989.
A. Baujard and H. Igersheim. Framed-field experiment on approval voting and
evaluation voting. Some teachings to reform the French presidential electoral
system. In B. Dolez, B. Grofman, and A. Laurent, editors, In Situ and Labo-
ratory Experiments on Electoral Law Reform, Studies in Public Choice, pages
69–89.Springer,2011. doi: 10.1007/978-1-4419-7539-3.
A.Baujard,H.Igersheim,I.Lebon,F.Gavrel,andJ-F.Laslier. Who’sfavoredby
evaluative voting? An experiment conducted during the 2012 French presiden-
tialelection. ElectoralStudies,34:131–145,2014.
S.Berg. Paradoxofvotingunderanurnmodel: Theeffectofhomogeneity. Public
Choice,47(2):377–387,1985.
N. Betzler, A. Slinko, and J. Uhlmann. On the computation of fully proportional
representation. JournalofArtificialIntelligenceResearch,47:475–519,2013.
D.Black. TheTheoryofCommitteesandElections. CambridgeUniversityPress,
1958.
N. Boehmer and N. Schaar. Collecting, classifying, analyzing, and using real-
world ranking data. In Proceedings of AAMAS-2023, pages 1706–1715. ACM,
2023.
N.Boehmer,R.Bredereck,P.Faliszewski,R.Niedermeier,andS.Szufa. Puttinga
compass on the map of elections. In Proceedings of IJCAI-2021, pages 59–65,
2021.
N.Boehmer,R.Bredereck,E.Elkind,P.Faliszewski,andS.Szufa. Expectedfre-
quencymatricesofelections: Computation,geometry,andpreferencelearning.
InProceedingsofNeurIPS-2022,2022a.
N.Boehmer,R.Bredereck,P.Faliszewski,andR.Niedermeier. Aquantitativeand
qualitativeanalysisoftherobustnessof(real-world)electionwinners. 2022b.
N.Boehmer,P.Faliszewski,R.Niedermeier,S.Szufa,andT.Wa˛s. Understanding
distancemeasuresamongelections. InProceedingsofIJCAI-2022,pages102–
108,2022c.
184N. Boehmer, J. Cai, P. Faliszewski, and A. Kaczmarczyk T. Wa˛s Z. Fan,
Ł.Janeczko. Propertiesofpositionmatricesandtheirelections. InProceedings
ofAAAI-2023,pages5507–5514,2023a.
N. Boehmer, K. Heeger, and S. Szufa. A map of diverse synthetic stable room-
matesinstances. InProceedingsofAAMAS-2023,pages1003–1011,2023b.
K. Booth and G. Lueker. Testing for the consecutive ones property, interval
graphs, and graph planarity using PQ-tree algorithms. Journal of Computer
andSystemSciences,13(3):335–379,1976.
S.Bouveret,R.Blanch,A.Baujard,F.Durand,H.Igersheim,J.Lang,A.Laruelle,
J. Laslier, I. Lebon, and V. Merlin. Voter autrement 2017 for the French presi-
dential election - the data of the in situ experiment. Technical report, Zenodo,
2019. http://doi.org/10.5281/zenodo.3548574.
S.BramsandP.Fishburn. ApprovalVoting. Birkhäuser,Boston,1983.
R. Bredereck, J. Chen, and G. Woeginger. A characterization of the single-
crossingdomain. SocialChoiceandWelfare,41(4):989–998,2013.
R. Bredereck, P. Faliszewski, A. Kaczmarczyk, and R. Niedermeier. An experi-
mental view on committees providing justified representation. In Proceedings
ofIJCAI-2019,pages109–115,2019.
M.BrillandJ.Peters. Robustandverifiableproportionalityaxiomsformultiwin-
nervoting. arXivpreprintarXiv:2302.01989,2023.
M. Brill, J-F. Laslier, and P. Skowron. Multiwinner approval rules as apportion-
mentmethods. JournalofTheoreticalPolitics,30(3):358–382,2018.
I. Caragiannis, X. Chatzigeorgiou, G. Krimpas, and A. Voudouris. Optimizing
positional scoring rules for rank aggregation. Artificial Intelligence, 267:58–
77,2019.
I. Caragiannis, C. Kaklamanis, N. Karanikolas, and G. A. Krimpas. Evaluating
approval-basedmultiwinnervotingintermsofrobustnesstonoise. Autonomous
AgentsandMultiagentSystems,36(1):1–22,2022.
V. Conitzer. Eliciting single-peaked preferences using comparison queries. Jour-
nalofArtificialIntelligenceResearch,35:161–191,2009.
185S. Cook. The complexity of theorem-proving procedures. pages 151–158. ACM
Press,May1971.
J.deLeeuw. Modernmultidimensionalscaling: Theoryandapplications. Journal
ofStatisticalSoftware,14:1–2,2005.
L. Van der Maaten. Fast optimization for t-SNE. In NeurIPS-2010 Workshop on
ChallengesinDataVisualization,volume100,2010.
L. Van der Maaten and G. Hinton. Visualizing data using t-SNE. Journal of
machinelearningresearch,9(11),2008.
D. Donoho and C. Grimes. Hessian eigenmaps: Locally linear embedding tech-
niques for high-dimensional data. Proceedings of the National Academy of
Sciences,100(10):5591–5596,2003.
R. Downey and M. Fellows. Fixed-parameter tractability and completeness II:
On completeness for W[1]. Theoretical Computer Science, 141(1-2):109–131,
1995.
S. Dudycz, P. Manurangsi, J. Marcinkowski, and K. Sornat. Tight approximation
for proportional approval voting. In Proceedings of IJCAI-2020, pages 276–
282,2020.
C.Dwork,R.Kumar,M.Naor,andD.Sivakumar. Rankaggregationmethodsfor
theweb. InProceedingsofWWW-2001,pages613–622,March2001.
E.ElkindandM.Lackner. Structureindichotomouspreferences. InProceedings
ofIJCAI-2015,pages2019–2025,2015.
E.ElkindandA.Slinko.Rationalizationsofvotingrules.InF.Brandt,V.Conitzer,
U. Endriss, J. Lang, and A. Procaccia, editors, Handbook of Computational
SocialChoice,chapter8,pages169–196.CambridgeUniversityPress,2016.
E. Elkind, P. Faliszewski, and A. Slinko. Clone structures in voters’ preferences.
InProceedingsofEC-2012,pages496–513,June2012.
E. Elkind, P. Faliszewski, and A. Slinko. Distance rationalization of voting rules.
SocialChoiceandWelfare,45(2):345–377,2015.
186E.Elkind,P.Faliszewski,J.Laslier,P.Skowron,A.Slinko,andN.Talmon. What
do multiwinner voting rules do? An experiment over the two-dimensional eu-
clideandomain. InProceedingsofAAAI-2017,pages494–501,2017.
E. Elkind, M. Lackner, and D. Peters. Preference restrictions in computational
socialchoice: Asurvey. arXivpreprintarXiv:2205.09092,2022.
J. Enelow and M. Hinich. The Spatial Theory of Voting: An Introduction. Cam-
bridgeUniversityPress,1984.
J. Enelow and M. Hinich. Advances in the Spatial Theory of Voting. Cambridge
UniversityPress,1990.
B.Escoffier,J.Lang,andM.Öztürk. Single-peakedconsistencyanditscomplex-
ity. InProceedingsofECAI-2008,pages366–370.IOSPress,July2008.
Ö. Eg˘eciog˘lu and A. Giritligil. The impartial, anonymous, and neutral culture
model: Aprobabilitymodelforsamplingpublicpreferencestructures. Journal
ofMathematicalSociology,37(4):203–222,2013.
P. Faliszewski, P. Skowron, A. Slinko, and N. Talmon. Committee scoring rules:
Axiomatic classification and hierarchy. In Proceedings of IJCAI-2016, pages
250–256,2016.
P. Faliszewski, P. Skowron, A. Slinko, and N. Talmon. Multiwinner voting: A
newchallengeforsocialchoicetheory. InU.Endriss,editor,TrendsinCompu-
tationalSocialChoice.AIAccessFoundation,2017a.
P.Faliszewski,P.Skowron,A.Slinko,andN.Talmon. Multiwinnerrulesonpaths
from k-Borda to Chamberlin–Courant. In Proceedings of IJCAI-2017, pages
192–198,2017b.
P. Faliszewski, M. Lackner, D. Peters, and N. Talmon. Effective heuristics for
committee scoring rules. In Proceedings of AAAI-2018, pages 1023–1030,
2018.
P. Faliszewski, P. Skowron, A. Slinko, S. Szufa, and N. Talmon. How similar are
twoelections? InProceedingsofAAAI-2019,pages1909–1916,2019.
P.Faliszewski,A.Slinko,andN.Talmon. Multiwinnerruleswithvariablenumber
ofwinners. InProceedingsofECAI-2020,pages67–74,2020.
187P. Faliszewski, K. Sornat, and S. Szufa. The complexity of subelection isomor-
phism problems. In Proceedings of AAAI-2022, volume 36, pages 4991–4998,
2022.
P.Faliszewski,J.Flis,D.Peters,G.Pierczyn´ski,P.Skowron,D.Stolicki,S.Szufa,
and N. Talmon. Participatory budgeting: Data, tools and analysis. In Proceed-
ingsofIJCAI-2023,pages2667–2674,82023a.
P. Faliszewski, A. Kaczmarczyk, K. Sornat, S. Szufa, and T. Wa˛s. Diversity,
agreement, and polarization in elections. In Proceedings of IJCAI-2023, pages
2684–2692,2023b.
P.Faliszewski,M.Lackner,K.Sornat,andS.Szufa. Anexperimentalcomparison
of multiwinner voting rules on approval elections. In Proceedings of IJCAI-
2023,pages2675–2683,2023c.
P. Fishburn and W. Gehrlein. Condorcet’s paradox and anonymous preference
profiles. PublicChoice,26:1–18,1978.
T. Fruchterman and E. Reingold. Graph drawing by force-directed placement.
Software: PracticeandExperience,21(11):1129–1164,1991.
M. Garey and D. Johnson. Computers and Intractability: A Guide to the Theory
ofNP-Completeness. W.H.FreemanandCompany,1979.
M. Godziszewski, P. Batko, P. Skowron, and P. Faliszewski. An analysis of
approval-based committee rules for 2D-Euclidean elections. In Proceedings
ofAAAI-2021,pages5448–5455,2021.
M. Grohe, G. Rattan, and G. Woeginger. Graph similarity and approximate iso-
morphism. InProceedingsofMFCS-2018,pages20:1–20:16,2018.
V. Hashemi and U. Endriss. Measuring diversity of preferences in a group. In
ProceedingsofECAI-2014,pages423–428,2014.
K. Inada. A note on the simple majority decision rule. Econometrica, 32(32):
525–531,1964.
K.Inada. Thesimplemajoritydecisionrule. Econometrica,37(3):490–506,1969.
Ł.JaneczkoandP.Faliszewski. Thecomplexityofproportionalitydegreeincom-
mitteeelections. ProceedingsofAAAI-2022,2022.
188T. Kamada and S. Kawai. An algorithm for drawing general undirected graphs.
Informationprocessingletters,31(1):7–15,1989.
T.Kamishima. Nantonaccollaborativefiltering: Recommendationbasedonorder
responses. InProceedingsofKDD-2003,pages583–588,2003.
A.Karpov.Onthenumberofgroup-separablepreferenceprofiles.GroupDecision
andNegotiation,28(3):501–517,2019.
J.Kruskal. Multidimensionalscalingbyoptimizinggoodnessoffittoanonmetric
hypothesis. Psychometrika,29(1):1–27,1964.
K. Kuga and H. Nagatani. Voter antagonism and the paradox of voting. Econo-
metrica: JournaloftheEconometricSociety,pages1045–1067,1974.
M.LacknerandJ.Maly. Approval-basedshortlisting. InProceedingsofAAMAS-
2021,pages737–745,2021.
M. Lackner and P. Skowron. Utilitarian welfare and representation guarantees of
approval-basedmultiwinnerrules. ArtificialIntelligence,288:103366,2020.
M. Lackner and P. Skowron. Approval-based committee voting. In Multi-Winner
VotingwithApprovalPreferences,pages1–7.Springer,2023.
M. Lackner, P. Regner, B. Krenn, and S. Forster. abcvoting: A Python li-
brary of approval-based committee voting rules, 2021. URL https://
doi.org/10.5281/zenodo.3904466. Current version: https://
github.com/martinlackner/abcvoting.
M.Lackner,P.Regner,andB.Krenn. abcvoting: Apythonpackageforapproval-
basedmulti-winnervotingrules. JournalofOpenSourceSoftware,8(81):4880,
2023.
J. Lang and P. Skowron. Multi-attribute proportional representation. Artificial
Intelligence,263:74–106,2018.
J.LaslierandK.VanderStraeten. Aliveexperimentonapprovalvoting. Experi-
mentalEconomics,11(1):97–105,2008.
J.LaslierandR.Sanver,editors. HandbookonApprovalVoting. Springer,2010.
189D. Leep and G. Myerson. Marriage, magic, and solitaire. The American Mathe-
maticalMonthly,106(5):419–429,1999.
B. Litvak. Distances and consensus rankings. Cybernetics and systems analysis,
19(1):71–81, 1983. Translated from Kibernetika, No. 1, pp. 57–63, January–
February,1983.
T. Lu and C. Boutilier. Budgeted social choice: From consensus to personalized
decisionmaking. InProceedingsofIJCAI-2011,2011.
T. Lu and C. Boutilier. Effective sampling and learning for Mallows models with
pairwise-preferencedata. JournalofMachineLearningResearch,15(1):3783–
3829,2014.
C.Mallows. Non-nullrankingmodels. Biometrica,44:114–130,1957.
N. Mattei and T. Walsh. Preflib: A library for preferences. In Proceedings of
ADT-2013,pages259–270,2013.
J. McCabe-Dansted and A. Slinko. Exploratory analysis of similarities between
socialchoicerules. GroupDecisionandNegotiation,15:77–107,2006.
T.MeskanenandH.Nurmi. Closenesscountsinsocialchoice. InM.Brahamand
F.Steffen,editors,Power,Freedom,andVoting.Springer-Verlag,2008.
T.Minka. AutomaticchoiceofdimensionalityforPCA. ProceedingsofNeurIPS-
2000,13,2000.
J. Mirrlees. An exploration in the theory of optimal income taxation. Review of
EconomicStudies,38:175–208,1971.
B. Monjardet. Acyclic domains of linear orders: A survey. In S. Brams,
W. Gehrlein, and F. Roberts, editors, The Mathematics of Preference, Choice
and Order, Studies in Choice and Welfare, pages 139–160. Springer Berlin
Heidelberg,2009.
K. Munagala, Z. Shen, and K. Wang. Optimal algorithms for multiwinner elec-
tionsandtheChamberlin-Courantrule. InProceedingsofEC-2021,pages697–
717,2021.
S. Nitzan. Some measures of closeness to unanimity and their implications. The-
oryandDecision,13(2):129–138,1981.
190J.O’Neill. OpenSTV,www.openstv.org. 2013.
D. Peters and M. Lackner. Preferences single-peaked on a circle. Journal of
ArtificialIntelligenceResearch,68:463–502,2020.
A. Procaccia, S. Reddi, and N. Shah. A maximum likelihood approach for se-
lectingsetsofalternatives. InProceedingsoftheTwenty-EighthConferenceon
UncertaintyinArtificialIntelligence,pages695–704,2012.
C. Puppe and A. Slinko. Condorcet domains, median graphs and the single-
crossingproperty. EconomicTheory,67(1):285–318,2019.
J.RaymondandP.Willett. Maximumcommonsubgraphisomorphismalgorithms
forthematchingofchemicalstructures. JournalofComputer-AidedMolecular
Design,16(7):521–533,2002.
K. Roberts. Voting over income tax schedules. Journal of Public Economics, 8
(3):329–340,1977.
Y. Rubner, C. Tomasi, and L. Guibas. The earth mover’s distance as a metric for
imageretrieval. InternationalJournalofComputerVision,40(2):99–121,2000.
L.Sánchez-Fernández,E.Elkind,M.Lackner,N.Fernández,J.A.Fisteus,P.Bas-
antaVal,andP.Skowron. Proportionaljustifiedrepresentation. InProceedings
ofAAAI-2017,pages670–676,2017.
K.Sapała.Algorithmsforembeddingmetricsineuclideanspaces.Master’sthesis,
AGHUniversityofScienceandTechnology,2022.
P. Skowron. Proportionality degree of multiwinner rules. In Proceedings of EC-
2021,pages820–840,2021.
P.Skowron,P.Faliszewski,andA.Slinko. Achievingfullyproportionalrepresen-
tation: Approximabilityresult. ArtificialIntelligence,222:67–103,2015.
P. Skowron, P. Faliszewski, and J. Lang. Finding a collective set of items: From
proportional multirepresentation to group recommendation. Artificial Intelli-
gence,241:191–216,2016.
P.Skowron,M.Lackner,M.Brill,D.Peters,andE.Elkind. Proportionalrankings.
InProceedingsofIJCAI-2017,pages409–415,2017.
191A. Slinko, Q. Wu, and X. Wu. A characterization of preference domains that are
single-crossingandmaximalcondorcet. EconomicsLetters,204:109918,2021.
D. Stolicki, S. Szufa, and N. Talmon. Pabulib: A participatory budgeting library.
arXivpreprintarXiv:2012.06539,2020.
S. Szufa, P. Faliszewski, P. Skowron, A. Slinko, and N. Talmon. Drawing a map
ofelectionsinthespaceofstatisticalcultures. InProceedingsofAAMAS-2020,
pages1341–1349,2020.
S. Szufa, P. Faliszewski, Ł. Janeczko, M. Lackner, A. Slinko, K. Sornat, and
N.Talmon. Howtosampleapprovalelections? InProceedingsofIJCAI-2022,
pages496–502,2022.
T. Walsh. Generating single peaked votes. Technical Report arXiv:1503.02766
[cs.GT],arXiv.org,March2015.
W.Yu,H.Hoogeveen,andJ.K.Lenstra. Minimizingmakespaninatwo-machine
flowshopwithdelaysandunit-timeoperationsisNP-hard. JournalofSchedul-
ing,7(5):333–348,2004.
Z.ZhangandJ.Wang. MLLE:Modifiedlocallylinearembeddingusingmultiple
weights. ProceedingsofNeurIPS-2006,19,2006.
192