Random Latent Exploration for Deep Reinforcement Learning
SrinathMahankali12 Zhang-WeiHong12 AyushSekhari2 AlexanderRakhlin2 PulkitAgrawal12
Abstract ϵ-greedy,Boltzmannsampling,posteriorsampling(Osband
et al., 2016a; 2019; Fortunato et al., 2017; Ishfaq et al.,
Theabilitytoefficientlyexplorehigh-dimensional
2021; 2023; Plappert et al., 2017)) and (ii) Bonus-based
statespacesisessentialforthepracticalsuccessof
exploration (Bellemare et al., 2016; Pathak et al., 2017;
deepReinforcementLearning(RL).Thispaperin-
Burdaetal.,2018;Oudeyer&Kaplan,2009;Pathaketal.,
troducesanewexplorationtechniquecalledRan-
2019;Hongetal.,2018). Unfortunately,neitherfamilyof
domLatentExploration(RLE),thatcombinesthe
approaches consistently outperforms the other when per-
strengths of bonus-based and noise-based (two
formance is measured across a range of tasks with either
popular approaches for effective exploration in
discrete(Chenetal.,2022)orcontinuous(Schwarkeetal.,
deepRL)explorationstrategies. RLEleverages
2023)actionspaces. Unsurprisingly, thechoiceofexplo-
the idea of perturbing rewards by adding struc-
rationstrategyforanewtaskisintimatelytiedtothetask
turedrandomrewardstotheoriginaltaskrewards
characteristics that are difficult to determine in advance.
incertain(random)statesoftheenvironment,to
Therefore,thecurrentcommonpracticeisfindingthebest
encouragetheagenttoexploretheenvironment
explorationschemeusingthetrial-and-errorprocessoftry-
duringtraining. RLEisstraightforwardtoimple-
ingdifferentstrategies.
mentandperformswellinpractice. Todemon-
stratethepracticaleffectivenessofRLE,weeval- Noise-basedexplorationtypicallyinvolvesperturbingthe
uateitonthechallengingATARIandISAACGYM policy’sparameters(suchastheweightsofthepolicynet-
benchmarksandshowthatRLEexhibitshigher work(Fortunatoetal.,2017;Plappertetal.,2017)),orthe
overallscoresacrossallthetasksthanotherap- actionoutputofthepolicy(e.g.ε-greedy). Theaddednoise
proaches. preventstheagentfromgeneratingthesametrajectoryre-
peatedly,therebyencouragingtheexplorationofdifferent
trajectories. Noise-based exploration is the go-to explo-
1.Introduction ration scheme in deep RL due to its simplicity of imple-
mentation. However, such strategies are less effective in
Reinforcementlearning(RL)(Sutton&Barto,2018)trains tasks requiring deep exploration than bonus-based explo-
agentstomaximizerewardsthroughinteractionswiththeen- rationstrategies(Osbandetal.,2016a). Onepossiblereason
vironment. Sincerewardscanbedelayed,focusingonlyon revealedbyourexperiments(seeSection4.1)isthatcom-
immediaterewardsoftenleadstosub-optimallong-horizon monly used perturbation strategies only affect the policy
strategies. Often,agentsmustsacrificeshort-termrewards locally and, therefore, do not explore states far from the
todiscoverhigherrewards. Identifyingactionsthateven- initialstates.
tually result in higher rewards, known as the exploration
Bonus-based strategies augment the task reward with an
problem,isacorechallengeinRL.
incentive(orabonus)(e.g.,predictionerror(Pathaketal.,
Explorationischallengingbecausethecurrentaction’sef- 2017),visitationcount(Bellemareetal.,2016),information
fect may often be revealed only after many interactions gain(Houthooftetal.,2016))thatencouragestheagenttoex-
with the environment. Exploration is well-studied(Amin plorefarawaystatesandtherebyachievedeepexploration.
etal.,2021)andthemajorapproachescanbebroadlycat- Unfortunately, computing the bonus requires training an
egorizedintotwotypes: (i)Noise-basedexploration(e.g., additionaldeepneuralnetwork. Furthermore,whilebonus-
basedexplorationoutperformsnoise-basedexplorationin
Project website: https://srinathm1359.github.
afewhard-explorationtasks,whenaverageperformanceis
io/random-latent-exploration
1ImprobableAILab2MassachusettsInstituteofTechnology. measuredacrossarangeoftasks(i.e.,botheasyandhardex-
Correspondenceto:SrinathMahankali<srinathm@mit.edu>. plorationproblems),bothstrategiesperformsimilarly(Taïga
etal.,2019;Chenetal.,2022).
Proceedings of the 41st International Conference on Machine
Learning,Vienna,Austria.PMLR235,2024.Copyright2024by Instead,wehypothesizethatitiseasiertoexplorebytraining
theauthor(s).
1
4202
luJ
81
]GL.sc[
1v55731.7042:viXraRandomLatentExplorationforDeepReinforcementLearning
theagenttoachieveavarietyofdiversegoals,i.e. exploring 2.Preliminaries
thespaceofgoals. Followingthisintuition,inthiswork,we
ReinforcementLearning(RL). RLisapopularparadigm
introduceadifferentexplorationstrategywhereinsteadof
for solving sequential decision-making problems (Sutton
injectingnoiseoraddingbonusestoencourageexploration,
& Barto, 2018) where an agent operates in an unknown
we train the agent to achieve various goals from a goal
environment(Sutton&Barto,2018)andaimstoimprove
spaceZ. Ourkeyintuitionisthatifthegoalsaredesigned
itsperformancethroughrepeatedinteractionswiththeen-
such that different goals incentivize the agent to explore
vironment. At each round of interaction, the agent starts
diversepartsoftheenvironment,thentraininganagentto
from an initial state s of the environment and collects a
achievethesegoalswillleadittoexploreavarietyofstates, 0
trajectory. At each timestep t within that trajectory, the
eventuallyleadingittofindandexploithightaskrewards.
agent perceives the state s , takes an action a ∼ π(.|s )
Unfortunately,representingandselecting"therightgoals" t t t
withitspolicyπ,receivesataskrewardr =r(s ,a ),and
ischallenginginpractice,astheymaybedependentonthe t t t
transitionstoanextstates untilreachingterminalstates,
underlying task in complex ways that the learner cannot t+1
afterwhichanewtrajectoryisinitializedfroms andthe
anticipateinadvance. 0
above repeats. The goal of the agent is to learn a policy
(cid:104) (cid:105)
Tocircumventtheissueoffindingtherightgoalstotarget, π that maximizes expected return E (cid:80)∞ γtr(s ,a )
weproposeRandomLatentExploration(RLE),wherethe π t=0 t t
inatrajectory. Astraightforwardapproachistoestimate
agent’spolicyisconditionedonrandomvectorssampled
the expected return of a policy by rolling out trajectories
fromafixeddistributionasgoals,whichdefineadditional
(s ,a ,s ,··· ,)throughMonteCarlosampling(Konda&
rewardbonusesdifferingateachstatebasedonthechosen 0 0 1
Tsitsiklis,1999),andthenoptimizingthistofindtheoptimal
vectors. Inparticular,therandomvectorsactaslatentgoals,
policy,butunfortunately,thecorrespondingestimatesare
eachcreatingdifferentrewardfunctionsthatencouragethe
ofhighvarianceandthusoftenrequireahugenumberof
agenttoreachdifferentpartsoftheenvironment. Bysam-
data. Thus,inpractice,variousRLalgorithmslearnavalue
plingenoughrandomvectorsduringtraining,theagentis
function (or value network) Vπ from the interaction that
trainedtopursuemanydifferentgoals,thusresultingindeep
approximates
exploration. OurexperimentsshowthatRLEleadstosignif-
icantlymorediverseanddeepertrajectoriesthantraditional ∞
(cid:104)(cid:88) (cid:105)
noise-basedmethods,withouttheneedforcomplexbonus Vπ(s 0)≈E π γtr(s t,a t) , (1)
computation. ThismakesRLEbotheasytoimplementand t=0
effectiveinpractice. andtrainthepolicyπ tomaximizethevalueVπ(s )(e.g.
0
To show the effectiveness of our approach, we evaluated usingpolicygradient).
RLEin ATARI—apopulardiscreteactionspacedeepRL
Exploration. Astherewardmaybedelayedandnotimme-
benchmark (Bellemare et al., 2013), and ISAACGYM—a
diatelypresentedtotheagent,theagentmayneedtotake
popularcontinuouscontroldeepRLbenchmark(Makoviy-
manyactionsandvisitasequenceofstateswithoutrewards
chuketal.,2021),eachconsistingofmanydifferenttasks
before it receives any learning signal (reward). As such,
with varying degrees of exploration difficulty. We imple-
takinggreedyactiona ateachstepthatmaximizesimme-
t
mentourmethodontopofthepopularRLalgorithm,Prox-
diate reward r(s ,a ) does not necessarily lead to a high
t t
imal Policy Optimization (PPO) (Schulman et al., 2017)
return. Thus,RLalgorithmsrequire“exploring”statesand
and compare it with PPO in other exploration strategies.
actionsthatmayleadtolowimmediaterewardsbutcould
OurexperimentalresultsdemonstratethatRLEimproves
potentiallyendupwithhighreturninthelongrun. Werefer
over standard PPO in ATARI and ISAACGYM. Further-
tothisprocessasexplorationthroughoutthispaper.
more,RLEalsoexhibitsahigheraggregatedscoreacrossall
tasksinATARIthanotherexplorationmethods,including
3.OurMethod: RandomLatentExploration
RND(Burdaetal.,2019)andrandomizedvaluefunction
strategies (Fortunato et al., 2017). Importantly, these im-
Problem statement. Our goal in this paper is to create
provements were obtained by simply adding RLE on the
an exploration strategy that improves the standard action
topofthebasePPOimplementationwhileonlychanging
noiseexplorationmethodoverallonthemajorityoftasks
the learning rate (and discount rate for ATARI). We use
and domains (both discrete and continuous control) and
thesamehyperparametersacrossall57ATARIgamesand
yet is easy to implement. Toward that end, we provide a
the same hyperparameters across all 9 ISAACGYM tasks,
simpleexplorationmethodthatcombinesthebestofboth
thushighlightingthegeneralityofourapproachasaplug-in
worldsandbridgestheapproachofusingrewardbonuses
utility.
(Bellemareetal.,2016;Burdaetal.,2019)withnoisesor
posteriorsamplingforexploration(Osbandetal.,2016a;b;
Fortunatoetal.,2017).
2RandomLatentExplorationforDeepReinforcementLearning
Challenge:Ensuringdiversity.Thekeychallengeinnoise- functionmustbefactorizedtoallowboththepolicyandthe
basedexplorationmethodsliesintheirabilitytogenerate valuefunctiontoconditionthefactorsoftherandomized
diversebehaviorswithintheenvironment. Specifically,if rewardfunction. Thisavoidstheperceptionofthereward
noise-perturbedpoliciesbeartoomuchsimilarity,theywill aspartiallyobservablebythepolicy. Tofulfillthesecriteria,
yieldnearlyidenticaltrajectories(s ,a ,r ,s ,···),con- weimplementtherandomizedrewardfunction(denotedas
0 0 0 1
sequently limiting exploration to a narrow region of the F)asthedotproductofthestatefeatureandarandomly
environment. Theprimarytechnicalchallenge,then,isde- chosenlatentvector:
visingamethodthatensuressuchdiversityinthegenerated
trajectories. F(s,z)=ϕ(s)·z, (2)
Approach: Randomizedrewards. Toovercometheabove
challenge,weproposeenhancingtrajectorydiversitybyal- whereϕ : S → Rd isafeatureextractorthattransformsa
teringtheagent’srewards,inspiredbyskilldiscoverymeth- state into a d-dimensional vector, and z ∈ Rd represents
ods(Eysenbachetal.,2018)thatusevariedrewardfunctions a latent vector. Randomized rewards for each state are
todevelopdiverseskillsresultingindifferentstatevisitation generatedbysamplingzfromagivendistributionP z,and
distribution. Inparticular,ineveryepisodeoftraining,we thensettingrewardsasF(s,z).
randomlyperturbthegiventaskrewardfunctionbyadding
Latentconditionedpolicyandvaluenetwork. Recallthat
arandomizedrewardtocertain(random)partsofthestate
thepolicyandthevaluefunctionhavetobeawareofthe
space. Then,wetrainpoliciestomaximizethecompositeof
stateandtherandomvariablethatfactorizestherandomized
randomizedandtaskrewards. Thekeyideaisthatinevery
rewardfunctionF. Toachievethis,weaugmenttheinput
roundofinteraction,therandomrewardsaddedtocertain
tothepolicyandthevaluefunctionswiththelatentvector
randompartsofthestatespacewillincentivizetheagent
z. Theresultingpolicyandthevaluenetworksareπ(.|s,z)
tovisitthoseareas,andiftheserandomareasarediverse andVπ(s,z).Wetrainthelatent-conditionedvaluenetwork
enough,wewillgetdiversebehaviorsintheenvironment
toapproximatetheexpectedsumoftheoriginalrewardand
duringtraining—thusincentivizingexploration. However,
therandomizedrewardsasbelow
since the random rewards are repeatedly resampled and
thuskeeponchangingduringtrainingtoensurestableand (cid:34) ∞ (cid:35)
(cid:88)
effectivelearning,bothpoliciesandvaluefunctionsmust Vπ(s,z)≈E γt(R(s ,a )+F(s ,z)) , (3)
π t t t+1
be aware of the specific random reward function in use; t=0
otherwise,thechangingrewardfunctionswillcomprisea
partiallyobservableMDP(Kaelblingetal.,1998). Wetake and train the latent-conditioned policy π to maximize
inspirationfromtheUniversalValueFunctionApproximator Vπ(s,z)ateverystatesandlatentvectorz.Bothvalueand
(UVFA)(Schauletal.,2015),whichtrainsnetworksbased policy networks can be trained withany off-the-shelf RL
onvaryinggoals. Weadopttheirapproachbyequatingtheir algorithms,e.g. PPO(Schulmanetal.,2017),DQN(Mnih
goalstodifferentrewardfunctions,thusmakingthepolicyπ etal.,2015),A3C(Mnihetal.,2016),SAC(Haarnojaetal.,
andthevaluefunctionVπ conditiononthesampledreward 2018).
function. Thisensuresthattherandomrewardsnolonger
Latent vector sampling. To randomize the latent vector
appearasnoisestothepolicy. Theremainingquestionsare:
z,ratherthanresamplingitateachtimestep,whichcould
causeditheringactionsduetofast-changinglatentvectors
• Howtoimplementtherandomizedrewardfunctions? fedtothepolicy,itisresampledatthestartofeachtrajec-
tory. This ensures each trajectory is rolled out under the
• Howtomakethepolicyandthevaluefunctioncondition
samepolicyandlatentvectorz,maintainingtemporalcon-
onthesampledrewardfunctions?
sistencycrucialfordeepexploration,asindicatedbyprior
studies(Osbandetal.,2016a;Fortunatoetal.,2017). The
Weoutlineourimplementationoftheaboveideainthenext distributionofzwillbeelaboratedinSection4.
sectionanddeferfullimplementationdetailstoAppendixB.
Aswetrainthepolicyconditionedonthelatentfactorof
therandomlysampledrewardfunction,wetermourmethod
3.1.AlgorithmicImplementation
as Random Latent Exploration (RLE). We outline the al-
Randomized reward functions. An effective approach gorithminAlgorithm1andpresentthedetailedversionin
toimplementingrandomizedrewardfunctionsnecessitates Algorithm2. Notethatatline6inAlgorithm1,wecom-
adherence to two fundamental principles. First, the ran- putetherandomizedrewardatthenextstates sincethe
t+1
domizedrewardsrequirecorrelatingwithstates;otherwise, nextstatereflectsthecreditoftakingactiona andstates .
t t
they would appear as white noises, which does not help This choice is also common in prior works that compute
explorationasshownbyFortunatoetal.(2017).Second,the explorationbonusesateachstep(Burdaetal.,2019).
3RandomLatentExplorationforDeepReinforcementLearning
s
Algorithm1RandomLatentExploration(RLE) Theagentperceivesthe(x,y)co-
1: Input: LatentdistributionP z ordinates as the state input, and
2: repeat can take an action to move left,
3: Sampleafreshlatentvector: z ∼P z right, up, and down (if not in-
4: fort=0,...,T do terrupted by a wall). At the be-
5: Takeactiona t ∼π(.|s t,z)andtransitiontos t+1 ginning of each trajectory, the
6: Receivereward: r t =R(s t,a t)+F(s t+1,z) agentalwaysstartsfromthetop-
7: endfor rightcorneroftheroom(denoted
8: UpdatepolicynetworkπandvaluenetworkVπ with by the letter “S”). In this study,
Figure1.FOURROOMen-
thecollectedtrajectory(z,s ,a ,r ,s ,··· ,s ) we always give zero rewards to
0 0 0 1 T vironment. The agent
9: untilconvergence theagentsinceweareinterested startsatthetop-rightstate
inunderstandingandcomparing (denoted by red ’S’) and
howeachexplorationstrategybe- can move left, right, up,
haves in the presence of sparse anddown.Theblackbars
4.Experiments
rewards,orevenzeroeverywhere. denotewallsthatblockthe
We aim to show that our RLE improves over the action Thisisalsoknownasreward-free agent’smovement.
noiseexplorationmethodtypicallyusedinmanyRLalgo- exploration.
rithms (Schulman et al., 2017; Mnih et al., 2015) on the
Wecomparedtheagentstrainedwithdifferentexploration
majority of tasks in both discrete and continuous control
strategies: PPO, NOISYNET, RND, and RLE. The cor-
domains. Throughouttheexperiments,wetraintheagent
responding policy in each of these exploration strategies
foreachtaskusingPPO(Schulmanetal.,2017)sinceitisa
istrainedwithProximalPolicyOptimization(PPO)algo-
popularRLalgorithmusedinbothdiscreteandcontinuous
rithm(Schulmanetal.,2017)for2.5milliontimesteps. For
control tasks. Standard PPO implementation (Schulman
RLE,thefeatureextractordefinedinEquation(2)issetto
etal.,2017)exploresbysamplingactionsfromthepolicy
bearandomlyinitializedneuralnetworkwithonehidden
(i.e.,aBoltzmanndistributionoveractions)learned.
layer,theoutputlayerofwhichhasthesamedimensionas
WealsocompareRLEwiththefollowingexplorationstrate- z ∼ P z. Further implementation details are available in
giesasbaselines: AppendixB.2. Weremarkthat, becauseofthewalls, the
FOURROOMenvironmentrequiresdeepexplorationtogo
tostatesdistantfromtheinitialstates.
• NOISYNET (Fortunato et al., 2017): We chose it to be
therepresentativebaselinefromthefamilyofnoise-based Howdoesthelatentvectorinfluencethegeneratedtra-
exploration(Osbandetal.,2016a;Fortunatoetal.,2017; jectories? TheexplorationofRLEisdrivenbysampling
Plappert et al., 2017) because it has been used in prior thelatentvectorztochangethebehaviorofthepolicynet-
worksonbenchmarkingexplorationstrategies(Chenetal., workπandtheobjectiveofthevaluenetworkVπ. Onemay
2022;Taïgaetal.,2019). wonderwhatistheimpactofzontheinducedpolicy’sbe-
haviors. Toevaluatethis,wesampledifferentlatentvector
• RND (Burda et al., 2019): We choose RND to be the z ∼P z androllouttrajectorieswiththepolicyconditioning
representativebaselinefromthefamilyofbonus-based onthosez,plottingeachtrajectoryinadifferentcolorin
explorationmethodssinceitshowsconsiderableimprove- Figure2.Forthisplot,wechosethecheckpointofthepolicy
mentsoveractionnoisesandnoise-basedapproachesin networkstoredinthemiddleoftraining(i.e., 1.5million
hard-explorationtasksinATARI. timesteps)sincewewanttostudythebehaviorsofthepol-
icynetworkbeforeitconverges. Figure2demonstratesthat
thetrajectoriesgeneratedbydifferentlatentvectorszcan
4.1.IllustrativeExperimentsonFOURROOM
spanacrossallfourrooms. Thisshowsthatalteringthela-
Totestwhetherourmethodcanperformdeepexploration tentvectorzcanproducediversetrajectoriesandthatlatent
that is essential in many tasks requiring exploration, we vectorscaninfluencethebehaviorsofthepolicynetwork.
start by running toy experiments on the the FOURROOM Weprovidetherolloutsoftrajectoriesforrandomseedsfor
environment(Suttonetal.,1999). Weexplainthesettingof RLE,PPO,RNDandNOISYNETinAppendixC.
FOURROOMenvironmentandtheresultsasfollows.
Explainingtheobservedtrajectorydiversity. Toexplain
Setup. Figure1illustratesFOURROOMenvironmentwith whythetrajectoriesgeneratedbyanRLEpolicyarediverse,
50×50 states. The environment consists of four rooms
WealsoperformexperimentsonFOURROOMwithansparse
separatedbysolidwalls(whichtheagentcan’tcross)and taskrewardof1atthebottom-leftcorner.Theresultsandvisual-
connectedwithsmallopeningsofasinglestateeach(which izationofvisitationcountsaredeferredtoAppendixC.
the agent needs to go through to travel across rooms).
4RandomLatentExplorationforDeepReinforcementLearning
RLE (Ours) PPO 6
s s 10
3
10
RND NoisyNet
s s
Figure2.Rolloutofmultipletrajectoriesfromapolicytrainedwith
RLEinthemiddleofthetraining(1.5milliontimesteps),where
eachcolordenotesadistincttrajectory.Asthefiguredemonstrates,
changingthelatentvectorzinRLEleadstodiversetrajectories 0
acrossallfourrooms.
Figure3.Statevisitationcountsofallthemethodsaftertrainingfor
2.5Mtimestepswithoutanytaskreward(reward-freeexploration).
weplotseveraloftherewardfunctionsinducedbysampling
Thestartlocationisrepresentedbythered‘S’atthetopright.RLE
different z in Figure 10. These plots demonstrate the di-
achievesmuchwiderstatevisitationcoverageoverthecourseof
versityoftherandomrewards,eachofwhichcanguidethe
trainingcomparedtootherbaselines,confirmingthatthediverse
policytoadifferentpartofthestatespace.
trajectoriesgeneratedbythepolicyareusefulforexploration.
State Visitations in the environment. We plot the state
visitationcountsofeachexplorationstrategiesandpresent
theresultsinFigure3. TheresultsshowsthatPPO’sstate
PPO
2.5
visitation centers around the initial room (i.e, top-right),
RND
indicatingthatactionnoisesarenotabletobringtheagent NoisyNet
2.0
farawayfromtheinitialstate. Incontrast,weseethatRLE, RLE (Ours)
RND,andNOISYNETareallabletofrequentlyreachthe
1.5
roomsbeyondtheinitialroom,withRLEvisitationcount
spreadoutacrossthefourrooms. ThissuggeststhatRLE
1.0
iscapableofdoingdeepexplorationsimilartopriordeep
explorationalgorithmsforthisenvironment.
0.5
4.2.BenchmarkingResultsonATARI
0.0
0.0 0.2 0.4 0.6 0.8 1.0
Having performed illustrative experiments on the FOUR- Training Progress
ROOM toyenvironment, wenowevaluateourmethodon
morerealisticandchallengingtasks. Weaimtoshowthat
Figure4.Aggregatedhumannormalizedscoreacrossall57ATARI
RLEcanimprovePPO’soverallperformanceonmosttasks.
games. RLEexhibitsahigherinterquartilemean(IQM)ofnor-
Setup. Weevaluateourmethodinthewell-knownATARI malizedscorethanPPOacross57ATARIgames,showingthat
RLEimprovesoverPPOinthemajorityoftasks.
benchmark(Bellemareetal.,2013). Followingthecommon
practiceinATARI(Mnihetal.,2015),theagentperceivesa
stackofthemostrecentfour84×84grayscaleframesas
inputsandtakesdiscreteactionsavailableinthegamepad Thehyperparametersandimplementationdetailsofallthe
ofATARI(seeBellemareetal.(2013)forfurtherenviron- algorithmsandPPOaredeferredtoAppendixB.Foreach
ment details). For RLE, wechose thefeature learned by ATARIgame(i.e.,environment),wetraineachagentwith5
thevaluenetworkfollowedbyarandomlyinitializedlinear differentrandomseedsfor40millionframes,aspriorwork
layer as ϕ (used in Equation 2) and set the dimension of (Chenetal.,2022;Bellemareetal.,2016)suggested. How-
thelatentvectorzas8. Notethattherandomlyinitialized ever,wetrainedMONTEZUMA’SREVENGEfor200million
linearlayerisfrozenafterinitialization. Weusethestan- framessinceitsexplorationdifficultyismuchharderthan
dardPPOhyperparametersprovidedin(Burdaetal.,2019). otherATARIgames(Burdaetal.,2019).
5
erocS
dezilamroNRandomLatentExplorationforDeepReinforcementLearning
DoesRLEimprovetheoverallperformance? Weanswer
Pr(RLE > Algorithm)
thisquestionbycalculatingtheinterquartilemean(IQM)
NoisyNet 0.72
(Agarwal et al., 2021) and its 95% confidence interval,
whichwasestimatedusingthebootstrappingmethod(DiCi- RND 0.71
ccio&Efron,1996)ontheaggregatedhuman-normalized PPO 0.67
scoresfrom57games. Unlikeempiricalmeanscores,IQM 0.3 0.4 0.5 0.6 0.7 0.8
mitigatestheinfluenceofoutliersontheaggregatedmetric. (a) Atari Games
Figure 4 demonstrates that RLE achieves a higher IQM
human-normalizedscorecomparedtoallbaselines,indicat- Pr(RLE > Algorithm)
ingthatRLEenhancesperformanceoverotherexploration PPO (Norm. Rew.) 0.83
strategiesinthemajorityofATARItasks. Besidestheaggre-
RND 0.65
gateresults,wepresentthelearningcurvesforallmethods
across the 57 ATARI games in Figure 18. Additionally, PPO 0.66
thefinalmeanscoreofeachmethodacrossfiveseedsfor 0.4 0.6 0.8 1.0
each ATARI gameisprovidedinTable5intheappendix. (b) IsaacGym
FromFigure18,wenoticethatRLEdoesnotperformwell
onMONTEZUMA’SREVENGE,indicatingthatwhileRLE
assists in producing diverse trajectories, MONTEZUMA’S Figure5.(a) Probability of improvement (POI) of our method,
REVENGE still presents a challenge when not relying on RLE,overthebaselinesNOISYNET,RNDandPPOacrossall
bonus-basedexploration.
57ATARIgames(higherisbetter).Thelowerconfidencebound
ofRLE’sPOIovertheotheralgorithmsareallgreaterthan0.5.
Does RLE improveoverthebaselinesconsistently? In ThismeansthatRLEstatisticallyimprovesoverotheralgorithms
additiontothemarginofperformanceimprovementonthe (Agarwaletal.,2021). (b)ProbabilityofimprovementofRLE
aggregated score across all games, we demonstrate that overthebaselinesRND,PPO,andPPOwithrewardnormalization
RLEresultsinperformanceimprovementoverthebaselines acrossall9ISAACGYMtasks. Inthisdomainaswell,thelower
confidenceboundofRLE’sPOIovertheotheralgorithmsareall
with high probability. Following the evaluation protocol
greaterthan0.5.ThismeansRLEstatisticallyimprovesoverthe
suggestedinAgarwaletal.(2021),wemeasuretheproba-
otheralgorithms.
bilityofimprovement(POI)betweenalgorithmsandtheir
95%confidenceintervals,estimatedusingthebootstrapping
method(DiCiccio&Efron,1996),andpresenttheresults
inFigure5andFigure23. Figure5(a)showsthatthelower tasks. WecomparetheperformanceofRLEtothatofPPO
confidenceboundofPOIforRLEovereachalgorithmis andRNDasbaselines. AsRLEandRNDusetworeward
above0.5,indicatingthatRLEstatisticallyoutperformsthe streams with potentially different scales, we use reward
otherbaselines(Agarwaletal.,2021). Thismeansthatfora normalizationforthesemethods. Thus,wecompareagainst
randomlychosentaskinATARI,runningRLEislikelyto PPOwithrewardnormalizationasanadditionalbaseline.
yieldahigherscorethantheotherbaselines,implyingthat
Weimplemented RLE ontopof PPO andtraineditwith
RLE’sperformanceimprovementsareconsistentandnot
standard PPO hyperparameters in ISAACGYM provided
limitedtoafewgames. Conversely, Figure23(a)reveals
inCleanRL,withimplementationdetailsprovidedinAp-
that the POI over PPO for both NOISYNET and RND is
pendixB.4. Weselectedbothmanipulationandlocomotion
below 0.5, suggesting that NOISYNET and RND do not
tasksforevaluationandpresentedthelearningcurvesinFig-
consistentlyimproveover PPO despitehavingbetterper-
ure6,withlearningcurvesonallenvironmentspresentedin
formanceinafewgames(seeFigure18). WeuseaCNN
Figure22.
torepresentthepolicyandvaluefunctioninallATARIex-
periments, rather than an LSTM as done in (Chen et al., DoesRLEimproveoverbaselinesincontinuouscontrol?
2022) as the CNN-based architectures were also used in TheresultsshowthatRLEachievesahigheraveragereturn
priorwork(Burdaetal.,2019)andduetoitssimplicityin thanPPOinmosttasks,withparticularlylargeperformance
implementation. ThiscouldexplainwhyPPOoutperforms gainsinALLEGROHANDandSHADOWHAND,indicating
RNDoverall57ATARIgamesinourexperiments. thatRLEimprovesuponPPOincontinuouscontroltasks.
InALLEGROHANDandSHADOWHAND,theobjectiveis
4.3.BenchmarkingResultsonISAACGYM tocontrolaanthropomorphichandtoreorientobjectstoa
targetpose. Thesetasksrequiremoreexplorationthanother
TodemonstratethatRLEcanimproveuponPPOinboth continuouscontroltaskssinceittakesmanystepstoachieve
discrete and continuous control tasks, we also conducted thetargetpose. Additionally,inCARTPOLE, PPOperfor-
experimentsinISAACGYM(Makoviychuketal.,2021),a mancedegradesabruptlyinthemiddleoftraining, while
benchmarksuitecontainingnumerouscontinuouscontrol RLEmaintainshighperformancethroughout,suggesting
6
mhtiroglA
mhtiroglARandomLatentExplorationforDeepReinforcementLearning
that RLE prevents the learning process from collapsing Thisdesignchoice’simportanceisunderscoredbycompar-
duringtraining. ingRLEmodelswithandwithoutlatent-conditionedpolicy
and value networks, as shown in Figure 9. RLE without
Does RLE achieve higher overall performance than
latentvectorconditioningexhibitedaperformancedropin
PPO?Tostudytheoverallperformance, wemeasurethe
the VENTURE task, a hard-exploration game with sparse
probabilityofimprovementof RLE over PPO and RND
rewards. Wehypothesizethattheabsenceoflatentvector
across 9 different ISAACGYM tasks and also present the
conditioningresultsinlimitedbehavioralvariabilityinthe
resultsinFigure5andFigure23. Figure5(b)showsthat
policynetwork,asitsoutputsremainunchangedbydifferent
RLEhasastatisticallysignificantchanceofimprovingover
latentvectorsamples. Thislimitationlikelyleadstofailures
bothPPOandRNDinISAACGYMtasksasthelowerconfi-
inchallengingexplorationtasksthatnecessitateabroader
denceboundofthePOIforRLEovereachbaselinemethod
diversityintrajectorygeneration.
isgreaterthan0.5. Furthermore,Figure23(b)showsthat
outofallconsideredmethods,RLEistheonlyonewitha
Featurevectorsobtainedfromrandomneuralnetwork.
statisticallysignificantPOIoverPPO.
In the ATARI experiments described in Section 4.2, we
WealsomeasuretheaggregatednormalizedreturnofRLE, usedaslow-movingestimateoftheCNNfeatures(seeAp-
PPO,andRNDacrossall9ISAACGYMtasks.Asthereturn pendix B.3.1) learned by the value network to compute
ispositiveforPPOineachenvironment,wenormalizeruns RLErewards. Thischoiceoffeaturesslightlycontributes
bydividingbythemeanscoreofPPOinthatenvironment toimprovedperformance. Figure20presentstheIQMof
(i.e.,normalizePPOtohaveascoreof1).Forfurtherdetails thenormalizedscoreandthePOIofRLEoverPPOwith
ofthismetric,seeAppendixB.4.2. Wepresenttheresults andwithoutusingtheCNNfeatureslearnedbythevalue
inFigure24andFigure25,whichshowthatRLEachieves network. Theresultsdemonstratethatincorporatingvalue
higheraggregateperformancethanPPOandmatchesRND network features leads to a higher POI, while not signif-
inthismetric. icantly affecting the IQM. We plot the learning curve in
eachgameforbothvariantsinFigure21,findingthatwhile
4.4.AblationStudies performanceisbroadlysimilar,thereareafewgameswhere
thetwovariantshavelargedifferencesinperformance.
WeranvariousablationstudiesonATARIandISAACGYM
environmentstoexplorehowdifferenthyperparametersand
Choicesofnetworkarchitecturefortherandomreward
designchoicesaffectRLEperformance.
network. SinceRLEreliesonneuralnetworkstoextract
Latent vector distribution. We investigated the impact features for computing random rewards F(s,z), it’s im-
of different latent vector distributions on RLE’s perfor- portanttoexaminehowthechoiceofnetworkarchitecture
mance. OurstudyinvolvestrainingRLEwithvariousdis- affectsperformance. Weinvestigatedtheimpactofdiffer-
tributions, including Uniform([−0.5,0.5]d) and isotropic ent network architectures for extracting features ϕ(s) on
normalN(0,I d)distributions,withinad-dimensionalspace the computation of RLE rewards F(s,z) in ISAACGYM
whered=8. Thedetailedimplementationisdescribedin (Makoviychuketal.,2021),withtheIQMofthenormalized
AppendixB.TheresultspresentedinFigure7indicatethat scoreinFigure27andPOIoverPPOshowninFigure26.
RLEperformsbetterthanPPOacrossdifferentlatentvector InouroriginalISAACGYMexperiments,weusedthevalue
distributions. ThissuggeststhatRLE’sefficacyisnotsig- network’sarchitectureforRLE.Inthisablationstudy,we
nificantlyaffectedbythechoiceoflatentvectordistribution. testedashallowerneuralnetworkarchitecture. Theresults
indicatethatRLEwithashallowernetworkstillperforms
Latentvectordimension. Thisstudyexploreshowrobust
well, suggesting that RLE is not highly sensitive to the
isRLEtodifferentchoicesofthedimensiondofthelatent
choiceofnetworkarchitecture.
vector z. We trained RLE for d ∈ {2,8,32,128}, where
d = 8 is the dimension used in the results presented in
Whitenoiserandomrewards. InSection3.1,weempha-
Section4.2. Theoutcomes, depictedinFigure8, demon-
sizedtheimportanceofensuringthatRLE’srandomreward
strate that RLE is capable of surpassing PPO across all
functionF iscorrelatedwithstates. Iftherandomrewards
testeddimensions. Althoughthereareslightperformance
arenotstate-dependent,theywillactaswhitenoiseandwill
variationsbetweendifferentdvalues,thesedifferencesare
notenhanceperformance. Inthisablationstudy,weempiri-
subtle,suggestingthatRLE’sperformanceisinsensitiveto
callyinvestigatethesignificanceofmakingrandomrewards
thechoiceoflatentvectordimensiond.
dependentonstates. Wecomparethestate-dependentRLE
Latentvectorconditioning. InSection3.1,weemphasized rewardF(s,z)withwhitenoiserewardssampledfroma
thenecessityforthepolicytobeconditionedonthelatent normaldistributionwithzeromeanandunitvariance. This
vectortopreventrandomizedrewardsF(s,z)frombeing study was conducted in ISAACGYM environments, with
perceivedasnoisebyboththepolicyandthevaluenetwork. theresultspresentedinFigure28. Theresultsdemonstrate
7RandomLatentExplorationforDeepReinforcementLearning
AllegroHand ShadowHand Humanoid AnymalTerrain
1500 10
1000
4000
1000
5
500
500 2000
0 0 0 0
0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0
Training Progress Training Progress Training Progress Training Progress
PPO RND PPO + Reward Norm. RLE (Ours)
Figure6.ComparisonofperformancebetweenRLE,PPO,andRNDinfourISAACGYMtasks(higherisbetter).RLEachieveshigher
returnthanPPOandRNDinthemajorityoftasks,especiallyintaskslikeALLEGROHANDandSHADOWHANDthatrequiremore
exploration.ThissuggestsRLEimprovesoverPPOandRNDincontinuouscontroldomainsaswell.
Qbert Seaquest Qbert Seaquest
15000 1500 15000 1500
10000 1000 10000 1000
5000 500 5000 500
0 0 0 0
0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0
SpaceInvaders Venture SpaceInvaders Venture
2000 2000
1000 1000
1000 1000
500 500
0 0 0 0
0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0
Training Progress Training Progress Training Progress Training Progress
PPO RLE (Normal) PPO RLE (d=8) RLE (d=128)
RLE (Uniform) RLE (Sphere) RLE (d=2) RLE (d=32)
Figure7.PerformanceofRLEwithvaryinglatentvectordistribu- Figure8.PerformanceofRLEwithvaryinglatentvectordimen-
tionP (seeSection3.1),whereRLE(Sphere)istheoneusedin siond(seeSection3.1),whereRLE(d = 8)istheoneusedin
z
Section4.2.ThefigureshowsthatRLEwiththethreedistributions Section4.2.ThefigureshowsthatRLEcanoutperformPPOinall
canalloutperformPPO.ThisshowsthatRLEisnotsensitiveto thefourchosendimensions.ThisshowsthatRLEisnotsensitive
thechoiceoflatentvectordistribution. tothechoiceoflatentvectordimension.
thatwhitenoiserewardssignificantlydegradeperformance,
indicatingthatRLErewardsarenotmerelywhitenoise. presentsadistinctapproachfromRameshetal.(2022)both
in terms of motivation and implementation. Contrary to
5.RelatedWorks Ramesh et al. (2022), which aligns with previous studies
(Burdaetal.,2019;Pathaketal.,2017)byemployingpre-
Random reward prediction was used as an auxiliary task dictionerrorsasexplorationbonuses,ourRLEalgorithm
Jaderbergetal.(2016)forimprovingrepresentationlearning directlytrainsthepolicywithrandomrewards,demonstrat-
inpriorworks(Dabneyetal.,2021;Lyleetal.,2021). A ing superior performance. This finding underscores that
closelyrelatedworkisRameshetal.(2022),whichemploys RLE providesanewangletodesignexplorationstrategy
arandomgeneralvaluefunction(GVF)(Suttonetal.,2011) beyondusingpredictionerrorsasexplorationbonuses. Ad-
for exploration by initializing a random reward function ditionally,ourRLEalgorithmoffersamorestraightforward
andusinganensembleofnetworkstopredictpolicy-based implementationbyeliminatingtheneedforensembletrain-
randomrewardsums. Thedifferenceinpredictionandthe ingandMonteCarloreturnestimationofrandomrewards.
Monte Carlo estimate of random rewards, multiplied by The detailed discussion on the relevant literature can be
predictionvariance,enhancestheagent’sreward. Ourwork foundinAppendixA.
8
serocS
serocS
snruteR
serocS
serocSRandomLatentExplorationforDeepReinforcementLearning
concentratearoundthetruemodel,andthusthealgorithm
Qbert Seaquest
willexecutetheoptimalpolicyfortheunderlyingenviron-
15000 1500 ment. Whereas,inRLE,sincethetaskrewardsareconstant
throughoutlearningwhereasrandomrewardschange,inthe
10000 1000
laterstageofthelearning,thetrainedpolicyπ shouldfocus
z
5000 500
onoptimizingjustthetaskrewards,andwebelievethatthe
0 0 randomrewardswillsimplyactasaregularization.
0.0 0.5 1.0 0.0 0.5 1.0
SpaceInvaders Venture Benefits from parallelization. Note that, by design, our
algorithmsamplesindependentz ineveryroundandcan
2000
1000 thusbenefitfromparallelizationbyrunningthealgorithm
onmultiplecopiesofthesameenvironment(whenpossible,
1000
500 e.g. usingasimulator). Sincedifferentz producediverse
trajectories(seeFigure2orFigure14forillustrations),mul-
0 0 tipleparallelcopiesofthesameagentwillsimplyproduce
0.0 0.5 1.0 0.0 0.5 1.0
morediversedatawhichwouldaccelerateexploration.
Training Progress Training Progress
PPO RLE (w/o z-cond) RLE (w z-cond) Ontheinductivebiasofϕ. RLE ismodularasonecan
choose any feature extractor ϕ(s) e.g. Transformer net-
works(Vaswanietal.,2017),MLPs,orevennonparametric
Figure9.PerformanceofRLEwith(wz-cond)andwithout(w/o
modelssuchaskernels. InourATARIexperimentsweuse
z-cond)latentvectorconditioninthepolicyandvaluenetworks
a CNN for ϕ, but it would be interesting to explore how
(see Section 3.1). The figure displays that RLE without latent
otherchoicesofϕaffectthediversityoftheinducedreward
vectorconditionsuffersperformancedropinVENTURE,ahard-
explorationtaskwithsparserewards. functions,andhencethegeneratedtrajectories.
z-sampling. AteverytimestepinAlgorithm1thelatent
variablez issampledindependentlyfromthefixeddistri-
6.DiscussionandConclusion
butionP whichischosenatinitialization. However,itis
z
alsointuitivetoexpectthatP shouldchangeaswelearn
Inthispaper,weproposedanewexplorationmethodcalled z
moreabouttheunderlyingenvironment. Lookingforward
RLEthatisstraightforwardtoimplementinpractice,and
itwouldbeinterestingtoexplorealgorithmsthatchangeP
effectiveinchallengingdeepRLbenchmarkslike ATARI. z
whiletraining, e.g. tosamplemorefromthesetoflatent
Weconcludewithdiscussionsandfutureworkdirections:
variableswhichhavenotbeenexploredyetorforwhichthe
Simpleplug-infordeepRLalgorithms. RLEsimplyre- correspondingpoliciesπ havehistoricallyperformedwell
z
quiresaddingrandomizedrewardstotherewardsreceived inthegivenenvironment.
by the agent and augmenting the agent’s input with addi-
Other limitations. Currently, we limit our study to on-
tional latent variables that correlate to these randomized
policyalgorithms. Lookingforwarditwouldbeinteresting
rewards. Asaresult,RLEisagnostictothebaseRLalgo-
toextendRLEtooff-policyalgorithmssuchasDQN(Mnih
rithmandcanbeintegratedwithanyRLalgorithm. Given
et al., 2015) and SAC (Haarnoja et al., 2018); A practi-
itssimplicity, generality,andtheoverallperformanceim-
cal way to do so would be to condition the Q-function
provement it provides, we recommend using RLE as the
on z in addition to its usual inputs. Separately, an im-
defaultexplorationstrategyindeepRLimplementations.
portantdirectionforfutureworkistoexplorethemethod
Connectiontoposteriorsampling. Atahighlevel,while in more continuous control and real-world robotics do-
RLE seems similar to the posterior sampling-based ap- mains. While it is clear that our approach scales to high-
proaches (Thompson, 1933; 1935; Russo et al., 2018) in dimensionalstatespacesinATARIandcontinuouscontrol
the sense that both utilize randomization for exploration, tasks in ISAACGYM, it would also be interesting to see
there are important differences: Firstly, the two methods how it would generalize for real-world RL applications,
exploreviadifferentmechanisms. Posteriorsamplingran- e.g.inrobotics. Finally,whileRLEleadstoperformance
domizesoverdifferentmodelsoftheenvironment,whereas improvementsingeneral,wenotethatitdoesnotimprove
RLE perturbstherewardfunctionusingrandomrewards. performance in the famous hard-exploration game MON-
Secondly,thesamplingdistributionP z isfixedthroughout TEZUMA’S REVENGE. Thus, in the future, it would be
learninginRLE,whereastheposteriordistributioninposte- interestingtoextendRLEtoenvironmentsandtaskswith
riorsamplingchangeswithtimeandneedstobecomputed evensparserrewards.
for every round (which is often challenging in practice).
Thirdly,inposteriorsampling,theposteriorwilleventually
9
serocS
serocSRandomLatentExplorationforDeepReinforcementLearning
Acknowledgement References
WethankmembersoftheImprobableAILabforhelpful Agarwal, A., Henaff, M., Kakade, S., and Sun, W. Pc-
discussionsandfeedback. WearegratefultoMITSuper- pg: Policycoverdirectedexplorationforprovablepolicy
cloudandtheLincolnLaboratorySupercomputingCenter gradientlearning. Advancesinneuralinformationpro-
forprovidingHPCresources. Thisresearchwassupported cessingsystems,33:13399–13412,2020.
inpartbyHyundaiMotorCompany,QuantaComputerInc.,
MIT-IBMWatsonAILab,anAWSMLRAresearchgrant, Agarwal,R.,Schwarzer,M.,Castro,P.S.,Courville,A.C.,
ARO MURI under Grant Number W911NF-23-1-0277, andBellemare,M. Deepreinforcementlearningatthe
AROMURIunderGrantNumberW911NF-21-1-0328,and edge of the statistical precipice. Advances in Neural
ONRMURIunderGrantNumberN00014-22-1-2740,NSF InformationProcessingSystems,34,2021.
through award DMS-2031883, DOE through award DE-
Amin,S.,Gomrokchi,M.,Satija,H.,VanHoof,H.,andPre-
SC0022199, MIT UROP through the John Reed UROP
cup,D.Asurveyofexplorationmethodsinreinforcement
FundandthePaulE.Gray(1954)UROPFund,andSimons
learning. arXivpreprintarXiv:2109.00157,2021.
Foundation. Theviewsandconclusionscontainedinthis
documentarethoseoftheauthorsandshouldnotbeinter-
Auer, P., Jaksch, T., and Ortner, R. Near-optimal regret
pretedasrepresentingtheofficialpolicies,eitherexpressed
boundsforreinforcementlearning. Advancesinneural
orimplied,oftheArmyResearchOfficeortheUnitedStates
informationprocessingsystems,21,2008.
AirForceortheU.S.Government. TheU.S.Government
isauthorizedtoreproduceanddistributereprintsforGov-
Ayoub,A.,Jia,Z.,Szepesvari,C.,Wang,M.,andYang,L.
ernmentpurposes,notwithstandinganycopyrightnotation
Model-basedreinforcementlearningwithvalue-targeted
herein.
regression. In International Conference on Machine
Learning,pp.463–474.PMLR,2020.
AuthorContributions
• SrinathMahankaliraninitialexperimentstoinvesti- Badia, A. P., Piot, B., Kapturowski, S., Sprechmann, P.,
gatethebenefitofrandomrewardsthatinformedthe Vitvitskyi, A., Guo, Z. D., and Blundell, C. Agent57:
eventualformulationofRLE,whichhethencompared Outperformingtheatarihumanbenchmark. InInterna-
againstbaselinemethodsonAtariandIsaacGymenvi- tionalConferenceonMachineLearning.PMLR,2020.
ronmentsandhelpedwithpaperwriting.
Bellemare,M.,Srinivasan,S.,Ostrovski,G.,Schaul,T.,Sax-
• Zhang-WeiHongconceivedthepossibilityofusing ton,D.,andMunos,R. Unifyingcount-basedexploration
randomrewardsforexploration. Hewasinvolvedin andintrinsicmotivation. InNIPS,2016.
researchdiscussions,helpedscaleexperiments,played
asignificantroleinpaperwriting,andadvisedSrinath. Bellemare,M.G.,Naddaf,Y.,Veness,J.,andBowling,M.
The arcade learning environment: An evaluation plat-
• AyushSekhariwasinvolvedinresearchdiscussions formforgeneralagents. JournalofArtificialIntelligence
and helped set the overall formulation of RLE. He Research,47:253–279,2013.
playedasignificantroleinpaperwriting,andadvised
SrinathandZhang-Wei. Burda,Y.,Edwards,H.,Pathak,D.,Storkey,A.,Darrell,T.,
andEfros, A.A. Large-scalestudyofcuriosity-driven
• AlexanderRakhlinwasinvolvedinresearchdiscus- learning. arXivpreprintarXiv:1808.04355,2018.
sionsandadvising.
Burda, Y., Edwards, H., Storkey, A., and Klimov, O.
• PulkitAgrawalwasinvolvedinresearchdiscussions, Exploration by random network distillation. In In-
overalladvising,paperwriting,andpositioningofthe ternational Conference on Learning Representations,
work. 2019. URLhttps://openreview.net/forum?
id=H1lJJnR5Ym.
ImpactStatement
Cesa-Bianchi, N., Gentile, C., Lugosi, G., and Neu, G.
Thispaperpresentsworkwhosegoalistoadvancethefield Boltzmannexplorationdoneright. Advancesinneural
ofMachineLearning. OurapproachaimstoaccelerateRL informationprocessingsystems,30,2017.
in real-world domains, and depending on the domain to
which it will be applied, we see many potential societal Chen,E.,Hong,Z.-W.,Pajarinen,J.,andAgrawal,P. Re-
consequencesofourwork,however,noneofthemwefeel deemingintrinsicrewardsviaconstrainedoptimization.
mustbespecificallyhighlightedhere. arXivpreprintarXiv:2211.07627,2022.
10RandomLatentExplorationforDeepReinforcementLearning
Dabney, W., Barreto, A., Rowland, M., Dadashi, R., Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S. Soft
Quan, J., Bellemare, M. G., and Silver, D. The value- actor-critic: Off-policymaximumentropydeepreinforce-
improvement path: Towards better representations for ment learning with a stochastic actor. arXiv preprint
reinforcementlearning. InProceedingsoftheAAAICon- arXiv:1801.01290,2018.
ferenceonArtificialIntelligence,volume35,pp.7160–
7168,2021. Hazan,E.,Kakade,S.,Singh,K.,andVanSoest,A. Prov-
ablyefficientmaximumentropyexploration. InInterna-
Dai,Y.,Luo,H.,andChen,L. Follow-the-perturbed-leader tionalConferenceonMachineLearning,pp.2681–2691.
for adversarial markov decision processes with bandit PMLR,2019.
feedback. AdvancesinNeuralInformationProcessing
Systems,35:11437–11449,2022. Hong,Z.-W.,Shann,T.-Y.,Su,S.-Y.,Chang,Y.-H.,Fu,T.-
J.,andLee,C.-Y. Diversity-drivenexplorationstrategy
Dann,C.,Mansour,Y.,Mohri,M.,Sekhari,A.,andSrid-
for deep reinforcement learning. Advances in neural
haran,K. Guaranteesforepsilon-greedyreinforcement
informationprocessingsystems,31,2018.
learningwithfunctionapproximation. InInternational
conferenceonmachinelearning,pp.4666–4689.PMLR, Houthooft,R.,Chen,X.,Duan,Y.,Schulman,J.,DeTurck,
2022. F.,andAbbeel,P. Vime: Variationalinformationmaxi-
mizingexploration. InNIPS,2016.
DiCiccio,T.J.andEfron,B. Bootstrapconfidenceintervals.
Statisticalscience,11(3):189–228,1996.
Huang,S.,Dossa,R.F.J.,Ye,C.,Braga,J.,Chakraborty,D.,
Mehta,K.,andAraújo,J.G.Cleanrl:High-qualitysingle-
Du,S.,Kakade,S.,Lee,J.,Lovett,S.,Mahajan,G.,Sun,W.,
fileimplementationsofdeepreinforcementlearningalgo-
andWang,R.Bilinearclasses:Astructuralframeworkfor
rithms. JournalofMachineLearningResearch,23(274):
provablegeneralizationinrl. InInternationalConference
1–18, 2022. URL http://jmlr.org/papers/
onMachineLearning,pp.2826–2836.PMLR,2021.
v23/21-1342.html.
Dwaracherla,V.,Lu,X.,Ibrahimi,M.,Osband,I.,Wen,Z.,
Ishfaq,H.,Cui,Q.,Nguyen,V.,Ayoub,A.,Yang,Z.,Wang,
and Van Roy, B. Hypermodels for exploration. arXiv
Z., Precup, D., and Yang, L. Randomized exploration
preprintarXiv:2006.07464,2020.
in reinforcement learning with general value function
Eysenbach,B.andLevine,S. Maximumentropyrl(prov- approximation. InInternationalConferenceonMachine
ably) solves some robust rl problems. arXiv preprint Learning,pp.4607–4616.PMLR,2021.
arXiv:2103.06257,2021.
Ishfaq, H., Lan, Q., Xu, P., Mahmood, A. R., Precup,
Eysenbach,B.,Gupta,A.,Ibarz,J.,andLevine,S. Diversity D., Anandkumar, A., and Azizzadenesheli, K. Prov-
isallyouneed: Learningskillswithoutarewardfunction. able and practical: Efficient exploration in reinforce-
arXivpreprintarXiv:1802.06070,2018. mentlearningvialangevinmontecarlo. arXivpreprint
arXiv:2305.18246,2023.
Fortunato,M.,Azar,M.G.,Piot,B.,Menick,J.,Osband,I.,
Graves,A.,Mnih,V.,Munos,R.,Hassabis,D.,Pietquin,
Jaderberg, M., Mnih, V., Czarnecki, W. M., Schaul, T.,
O.,etal. Noisynetworksforexploration. arXivpreprint
Leibo,J.Z.,Silver,D.,andKavukcuoglu,K. Reinforce-
arXiv:1706.10295,2017.
mentlearningwithunsupervisedauxiliarytasks. arXiv
preprintarXiv:1611.05397,2016.
Foster, D. J., Kakade, S. M., Qian, J., and Rakhlin, A.
The statistical complexity of interactive decision mak-
Jia, Z., Li, G., Rakhlin, A., Sekhari, A., and Srebro, N.
ing. arXivpreprintarXiv:2112.13487,2021.
When is agnostic reinforcement learning statistically
tractable? arXivpreprintarXiv:2310.06113,2023.
Fu, J., Co-Reyes, J.D., andLevine, S. Ex2: Exploration
withexemplarmodelsfordeepreinforcementlearning.
Jin, C., Yang, Z., Wang, Z., and Jordan, M. I. Provably
arXiv:1703.01260,2017.
efficientreinforcementlearningwithlinearfunctionap-
Garg, D., Hejna, J., Geist, M., and Ermon, S. Extreme proximation. In Conference on Learning Theory, pp.
q-learning: Maxent rl without entropy. arXiv preprint 2137–2143.PMLR,2020.
arXiv:2301.02328,2023.
Jin, C., Liu, Q., and Miryoosefi, S. Bellman eluder di-
Gopalan,A.andMannor,S. Thompsonsamplingforlearn- mension: Newrichclassesofrlproblems,andsample-
ingparameterizedmarkovdecisionprocesses. InConfer- efficient algorithms. Advances in neural information
enceonLearningTheory,pp.861–898.PMLR,2015. processingsystems,34:13406–13418,2021.
11RandomLatentExplorationforDeepReinforcementLearning
Kaelbling,L.P.,Littman,M.L.,andCassandra,A.R. Plan- Mnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap,
ningandactinginpartiallyobservablestochasticdomains. T.P.,Harley,T.,Silver,D.,andKavukcuoglu,K. Asyn-
Artificialintelligence,101(1-2):99–134,1998. chronousmethodsfordeepreinforcementlearning. In
ICML,2016.
Konda, V. and Tsitsiklis, J. Actor-critic algorithms. Ad-
vances in neural information processing systems, 12, Osband, I., Russo, D., and Van Roy, B. (more) efficient
1999. reinforcementlearningviaposteriorsampling. Advances
inNeuralInformationProcessingSystems,26,2013.
Kveton,B.,Szepesvari,C.,Ghavamzadeh,M.,andBoutilier,
Osband,I.,Blundell,C.,Pritzel,A.,andVanRoy,B. Deep
C. Perturbed-historyexplorationinstochasticlinearban-
explorationviabootstrappeddqn. InNIPS,2016a.
dits. arXivpreprintarXiv:1903.09132,2019a.
Osband,I.,VanRoy,B.,andWen,Z. Generalizationand
Kveton, B., Szepesvari, C., Vaswani, S., Wen, Z., Latti-
explorationviarandomizedvaluefunctions. InInterna-
more, T., and Ghavamzadeh, M. Garbage in, reward
tionalConferenceonMachineLearning,pp.2377–2386.
out: Bootstrapping exploration in multi-armed bandits.
PMLR,2016b.
In International Conference on Machine Learning, pp.
3601–3610.PMLR,2019b. Osband,I.,VanRoy,B.,Russo,D.J.,Wen,Z.,etal. Deep
exploration via randomized value functions. J. Mach.
Kveton, B., Zaheer, M., Szepesvari, C., Li, L., Learn.Res.,20(124):1–62,2019.
Ghavamzadeh, M., and Boutilier, C. Randomized ex-
plorationingeneralizedlinearbandits. InInternational Oudeyer,P.-Y.andKaplan,F. Whatisintrinsicmotivation?
Conference on Artificial Intelligence and Statistics, pp. a typology of computational approaches. Frontiers in
2066–2076.PMLR,2020. neurorobotics,2009.
Pathak, D., Agrawal, P., Efros, A. A., and Darrell, T.
Kveton,B.,Konobeev,M.,Zaheer,M.,Hsu,C.-w.,Mlade-
Curiosity-driven exploration by self-supervised predic-
nov,M.,Boutilier,C.,andSzepesvari,C. Meta-thompson
tion. InProceedingsofthe34thInternationalConference
sampling.InInternationalConferenceonMachineLearn-
onMachineLearning,pp.2778–2787,2017.
ing,pp.5884–5893.PMLR,2021.
Pathak,D.,Gandhi,D.,andGupta,A. Self-supervisedex-
Li, Z., Li, Y., Zhang, Y., Zhang, T., and Luo, Z.-Q. Hy-
plorationviadisagreement. InInternationalConference
perdqn: Arandomizedexplorationmethodfordeepre-
onMachineLearning,pp.5062–5071.PMLR,2019.
inforcement learning. In International Conference on
LearningRepresentations,2021. Plappert,M.,Houthooft,R.,Dhariwal,P.,Sidor,S.,Chen,
R.Y.,Chen,X.,Asfour,T.,Abbeel,P.,andAndrychow-
Lyle,C.,Rowland,M.,Ostrovski,G.,andDabney,W. On
icz, M. Parameter space noise for exploration. arXiv
theeffectofauxiliarytasksonrepresentationdynamics.
preprintarXiv:1706.01905,2017.
InInternationalConferenceonArtificialIntelligenceand
Statistics,pp.1–9.PMLR,2021. Rakhlin, A. and Sridharan, K. Bistro: An efficient
relaxation-based method for contextual bandits. In In-
Makoviychuk,V.,Wawrzyniak,L.,Guo,Y.,Lu,M.,Storey, ternationalConferenceonMachineLearning,pp.1977–
K., Macklin, M., Hoeller, D., Rudin, N., Allshire, A., 1985.PMLR,2016.
Handa,A.,etal.Isaacgym:Highperformancegpu-based
physics simulation for robot learning. arXiv preprint Ramesh,A.,Kirsch,L.,vanSteenkiste,S.,andSchmidhu-
arXiv:2108.10470,2021. ber,J. Exploringthroughrandomcuriositywithgeneral
valuefunctions. arXivpreprintarXiv:2211.10282,2022.
Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A.,
Russo,D.J.,VanRoy,B.,Kazerouni,A.,Osband,I.,Wen,
Antonoglou,I.,Wierstra,D.,andRiedmiller,M. Playing
Z.,etal. Atutorialonthompsonsampling. Foundations
atari with deep reinforcement learning. arXiv preprint
andTrends®inMachineLearning,11(1):1–96,2018.
arXiv:1312.5602,2013.
Schaul,T.,Horgan,D.,Gregor,K.,andSilver,D. Universal
Mnih,V.,Kavukcuoglu,K.,Silver,D.,Rusu,A.A.,Veness,
valuefunctionapproximators.InInternationalconference
J.,Bellemare,M.G.,Graves,A.,Riedmiller,M.,Fidje-
onmachinelearning,pp.1312–1320.PMLR,2015.
land,A.K.,Ostrovski,G.,Petersen,S.,Beattie,C.,Sadik,
A.,Antonoglou,I.,King,H.,Kumaran,D.,Wierstra,D., Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and
Legg,S.,andHassabis,D. Human-levelcontrolthrough Klimov, O. Proximal policy optimization algorithms.
deepreinforcementlearning. Nature,2015. arXivpreprintarXiv:1707.06347,2017.
12RandomLatentExplorationforDeepReinforcementLearning
Schwarke, C., Klemm, V., Van der Boon, M., Bjelonic, least-squaresvalueiteration. InInternationalConference
M., and Hutter, M. Curiosity-driven learning of joint onArtificialIntelligenceandStatistics,pp.1954–1964.
locomotionandmanipulationtasks. InProceedingsof PMLR,2020.
The7thConferenceonRobotLearning,volume229,pp.
Zhang, T. Feel-good thompson sampling for contextual
2594–2610.PMLR,2023.
bandits and reinforcement learning. SIAM Journal on
Sun, W., Jiang, N., Krishnamurthy, A., Agarwal, A., and MathematicsofDataScience,4(2):834–857,2022.
Langford,J. Model-basedrlincontextualdecisionpro-
Zhang,T.,Ren,T.,Yang,M.,Gonzalez,J.,Schuurmans,D.,
cesses: Pacboundsandexponentialimprovementsover
andDai,B. Makinglinearmdpspracticalviacontrastive
model-freeapproaches.InConferenceonlearningtheory,
representationlearning. InInternationalConferenceon
pp.2898–2933.PMLR,2019.
MachineLearning,pp.26447–26466.PMLR,2022.
Sutton,R.S.andBarto,A.G. Reinforcementlearning: An
introduction. MITpress,2018.
Sutton, R. S., Precup, D., and Singh, S. Between mdps
andsemi-mdps: Aframeworkfortemporalabstractionin
reinforcementlearning. Artificialintelligence,112(1-2):
181–211,1999.
Sutton, R. S., Modayil, J., Delp, M., Degris, T., Pilarski,
P. M., White, A., and Precup, D. Horde: A scalable
real-timearchitectureforlearningknowledgefromunsu-
pervisedsensorimotorinteraction. InThe10thInterna-
tionalConferenceonAutonomousAgentsandMultiagent
Systems-Volume2,pp.761–768,2011.
Taïga, A. A., Fedus, W., Machado, M. C., Courville, A.,
and Bellemare, M. G. Benchmarking bonus-based ex-
ploration methods on the arcade learning environment.
arXivpreprintarXiv:1908.02388,2019.
Thompson, W. R. On the likelihood that one unknown
probability exceeds another in view of the evidence of
twosamples. Biometrika,25(3-4):285–294,1933.
Thompson,W.R. Onthetheoryofapportionment. Ameri-
canJournalofMathematics,57(2):450–456,1935.
Vaswani,A.,Shazeer,N.,Parmar,N.,Uszkoreit,J.,Jones,
L.,Gomez,A.N.,Kaiser,Ł.,andPolosukhin,I. Atten-
tionisallyouneed. InAdvancesinneuralinformation
processingsystems,pp.5998–6008,2017.
Vaswani, S., Mehrabian, A., Durand, A., and Kveton, B.
Olddoglearnsnewtricks: Randomizeducbforbandit
problems. arXivpreprintarXiv:1910.04928,2019.
Williams,R.J.andPeng,J. Functionoptimizationusing
connectionist reinforcement learning algorithms. Con-
nectionScience,3(3):241–268,1991.
Wu, R. and Sun, W. Making rl with preference-based
feedback efficient via randomization. arXiv preprint
arXiv:2310.14554,2023.
Zanette,A.,Brandfonbrener,D.,Brunskill,E.,Pirotta,M.,
andLazaric,A. Frequentistregretboundsforrandomized
13RandomLatentExplorationforDeepReinforcementLearning
A.AdditionalRelatedWorks
TherearetwomainapproachesinRLforexploration,(a)Randomizetheagentbyinjectingnoisethusinducingdiverse
behavior,and(b)Provideexplicitrewardbonusesthatincentivizetheagenttogotonovelstates. RLE,bridgingthetwo
approaches,injectsnoiseintotheagentbyaddingrandomlatentrewardbonusesduringtraining.
RandomnessisthekeytoolinmanyexplorationstrategiesinRL.Perhapsthemostpopularexampleisϵ-greedyorBoltzmann
sampling-basedexploration(Mnihetal.,2013;Dannetal.,2022;Cesa-Bianchietal.,2017;Eysenbachetal.,2018),which
explores by playing random actions. Entropy regularization (Williams & Peng, 1991; Mnih et al., 2016), and MaxEnt
RL(Haarnojaetal.,2018;Eysenbach&Levine,2021;Gargetal.,2023;Hazanetal.,2019)areotherinstancesofexploration
algorithms that utilize randomness, as they explicitly bias towards learning policies that have a high entropy. Another
explorationapproachistodirectlyinjectnoiseintotheparametersofthepolicyorvaluenetworks,e.g. inoff-policymethods
(Fortunatoetal.,2017),RLHFwithlinearMDPs(Wu&Sun,2023),onlineRLintabular(Osbandetal.,2016b),andlinear
MDPs(Zanetteetal.,2020). AnotherlineofworkincludesusingThompsonsampling(Thompson,1933;1935;Russo
etal.,2018),orposteriorsamplingforexploration(Osbandetal.,2013;Gopalan&Mannor,2015;Kvetonetal.,2021;
Zhang,2022),whichmaintainsaposteriordistributionoverthegroundtruthmodelandreliesontheuncertaintyinthe
posteriordistributionforexploration. Posteriorsampling,however,isintractableinpracticalRLsettingsduetotheneedto
samplefromtheextremelycomplexposteriordistribution;Variousempiricalapproachesaimtosamplefromanapproximate
posteriorinstead(Lietal.,2021;Dwaracherlaetal.,2020),butareunfortunatelymemoryintensive. WenotethatRLEis
differentfromtheseotherworksasitexploresbyaddingrandomrewardsinsteadofrandomizingovertheactions,policies,
ormodelsoftheenvironment.
ExplorationbyaddingexplicitrewardbonusesisalsowellstudiedinboththeoreticalandappliedRLliterature. Apopular
techniqueistoaddnovelty-basedexplorationbonusesthatareconstructedusingpredictionerrorsintransitiondynamics
(Pathak et al., 2017; 2019; Ramesh et al., 2022) or outputs of randomly initialized target network RND (Burda et al.,
2019),etc. Otherapproachesconstructrewardbonusesusingupperconfidenceboundsontheuncertaintyestimatesfor
theunderlyingmodel(Aueretal.,2008;Vaswanietal.,2019),usingdiscriminativelytrainedexemplarmodelstoestimate
novelty(Fuetal.,2017),orusingellipticalpotentialswhentheMDPhasalinearparameterization(Jinetal.,2020;Zhang
etal.,2022;Agarwaletal.,2020). Unfortunately,thesemethodsoftenintroduceadditionalcomponentsintothelearning
setups,e.g. additionalneuralnetworksforgeneratingbonusesandtheassociatedhyperparameters,whichcanmakelearning
unstable. Incontrast,RLEismuchsimplertodeployasitaddsrandomrewardfunctionsthatarecomputedusingfeatures
fromthepolicynetwork.
RLEcloselyresemblestheideaofFollowThePerturbedLeader(FTPL)developedinRLtheoryliterature(Kvetonetal.,
2019b;a;2020;Rakhlin&Sridharan,2016;Daietal.,2022). FTPL-basedmethodsexplorebyaddingcarefullydesigned
perturbationstotherewardfunctionthatcanguaranteeoptimism;sincetheperturbationsarecloselytiedtotheunderlying
modelingassumptions,FTPL-basedmethodsarecurrentlylimitedtorestrictedsettingslikelinearbandits,tabularMDPs,
etc. Incontrast,RLEsimplyaddsarandomrewardfunctionsampledfromafixeddistributionP ,andisthusapplicable
z
inmorepracticalRLsettings. Anothermajordifferenceisthatourmethodalsoutilizesz-conditionedpoliciesandvalue
functions,andthustherandomnessissharedamongsttherewardfunction,policynetworkandvaluenetwork.
Finally,thereisalonglineofworkinthetheoreticalRLliteratureondevelopingexplorationalgorithmsthatcanoptimally
exploitthestructureoftheunderlyingtaskinmodel-basedRL(Ayoubetal.,2020;Sunetal.,2019;Fosteretal.,2021),
model-freeRL(Jinetal.,2021;Duetal.,2021)andagnosticRLsetting(Jiaetal.,2023),however,thefocusintheseworks
isonstatisticalefficiencyandtheprovidedapproachesarenotcomputationallytractable.
14RandomLatentExplorationforDeepReinforcementLearning
B.ExperimentalImplementationDetails
Inthissection,weprovidethehyperparametersandimplementationdetailsofouralgorithm(RLE)alongwiththebaseline
methods(PPO,RND,NOISYNET)fortheFOURROOMandATARIEnvironments. Wealsoprovidehyperparametersand
implementationdetailsforallISAACGYMexperiments.
B.1.RLEPseudocode
Below,weprovidethepseudocodeforRLEinAlgorithm2.
Algorithm2DetailedPseudocodeforRandomLatentExploration(RLE)
1: Input: LatentdistributionP z,N parallelworkers,T stepsperupdate,S stepspersampling,featurenetworkupdate
rateτ
2: Randomlyinitializeafeaturenetworkϕwiththesamebackbonearchitectureasthepolicyandvaluenetworks
3: Initializerunningmeanµ=0andstandarddeviationσ =1estimatesofϕ(s)overthestatespace
4: Sampleaninitiallatentvectorforeachparallelworker: z ∼P z
5: repeat
6: Sampleinitialstates 0.
7: fort=0,...,T do
8: Takeactiona t ∼π(.|s t,z)andtransitiontos t+1
9: Computefeaturef(s t+1)=(ϕ(s t+1)−µ)/σ
10: Computerandomreward: F(s t+1,z)= ∥f f( (s st t+ +1 1)
)∥
·z
11: Receivereward: r t =R(s t,a t)+F(s t+1,z)
12: fori=0,1,...,N −1do
13: ifworkeriterminatedorS timestepspassedwithoutresamplingthen
14: Resamplesamplez ∼P z forworkeri
15: endif
16: endfor
17: endfor
18: UpdatepolicynetworkπandvaluenetworkVπ withthecollectedtrajectory(z,s 0,a 0,r 0,s 1,··· ,s T)
19: Updatefeaturenetworkϕusingthevaluenetwork’sparameters: ϕ←τ ·π+(1−τ)·ϕ
20: Updateµandσusingthebatchofcollectedexperience.
21: untilconvergence
B.2.FOURROOMEnvironment
WeprovidethehyperparametersusedforexperimentsintheFOURROOMenvironmentinTable1.
B.2.1.RLEIMPLEMENTATIONINFOURROOMENVIROMENT
Inourimplementationof RLE for FOURROOM environment,weensurethattherandomrewardfunctionsF(s,z)take
valuesin[−1,1].Tocomputetherewardgivenastatesandlatentvariablez,wenormalizetheoutputofϕ(s)tohaveunit
norm. Specifically,wedefinetherewardas:
ϕ(s)
F(s,z)= ·z,
∥ϕ(s)∥
whereϕistherandomlyinitializedfeaturenetworkthattransformsthestatestoavectorwiththesamedimensionasz. In
theFOURROOMenvironment,wesamplezfromtheunitsphereateverytrainingstep,whichoccursevery128timesteps.
Weperformthesamplingindependentlyforeachofthe32parallelworkers.
B.3.ATARI
WedisplaythehyperparametersusedforexperimentsinATARIgamesinTable2. ForPPOandRND,weusethedefault
hyperparametersbasedonthecleanrlcodebase(Huangetal.,2022),whichweretunedforATARIgames.ForNOISYNET,
weusethesamehyperparametersasPPOwiththeexceptionoftheentropylossweight,whichissetto0asrecommended
15RandomLatentExplorationforDeepReinforcementLearning
by(Fortunatoetal.,2017). WegiveadetaileddescriptionoftheATARIimplementationofRLEbelow.
B.3.1.RLEIMPLEMENTATIONDETAILSINATARI
Featurenetworkarchitectureandupdate. Westartwitharandomlyinitializedneuralnetworkϕwhichtakesastatesas
inputandoutputsavectorinRd,whichhasthesamedimensionasz. Inourimplementation,ϕcontainsaCNNbackbone
withanidenticalarchitecturetothe(shared)policyandvaluebackbone,alongwithafinallinearlayerontoptoconvertitto
alowdimensionRd. Inourimplementation,wechoosed=8. Toupdateϕ,wefollowtherule:
CNN ←τ ·CNN +(1−τ)·CNN
ϕ V ϕ
forasmallvalueofτ,andwechooseτ =0.005forourexperiments. Thisnetworkupdateisinspiredbythetargetnetwork
updateinDQN(Mnihetal.,2015)anddoesnotrequireanygradientsteps.
Computationofrandomrewardbonus. Whentheagentexperiencesatransition(s,a,s′),weobtainrandomreward
bonus from ϕ as follows: Obtain the low-dimensional vector output ϕ(s′). We standardize the output of ϕ(s′) using a
runningmeanandstandarddeviationestimatesothattheoutputisanormaldistributiononRd. Meanwhile,sampleavector
z ∼Sd−1,andcomputethefollowingvalue
ϕ(s′)·z
F(s′,z)= .
∥ϕ(s′)∥
Policyinput. Thepolicyobservestheobservationreturnedbytheenvironment,whichis4stackedgrayscaleframesof
dimension(84,84). Inaddition,thepolicyobserveszaswellastherandomrewardF(s ,z)fromtheprevioustimestep.
t
Resamplingofthelatentvariablez. InourAtariexperiments,thereare128parallelworkers.Wesamplezindependently
acrossallworkersfromthed-dimensionalunitsphereSd−1,andresampleuponeitherofthefollowingsignals:
1. Anenvironmenthasreachedthe‘DONE’flag,or
2. Anenvironmenthassurvivedwiththiszfor1280timesteps.
Policy training. We use PPO with the augmented observation space train on the combined reward as usual. As we
resamplezduringanepisode,wealsotreattheproblemofmaximizingrandomizedrewardasepisodic. Specifically,weset
the‘done’signaltoTruewheneverweresamplez. Thus,wedonotusereturnsfromfuturezwithinthesameepisodeto
estimatethereturnunderthecurrentz.
B.3.2.EVALUATIONDETAILSINATARI
Humannormalizedscore Tocomputeaggregateperformance,wefirstcomputethehumannormalizedscoreforeach
seedineachenvironmentas Agent score−Randomscore . Afterthis,wecomputetheIQMtomeasureaggregateperformanceas
Humanscore−Randomscore
recommendedin(Agarwaletal.,2021)asitisrobusttooutliers.
Cappedhumannormalizedscore Weusethecappedhumannormalizedscore(CHNS)(Badiaetal.,2020)tomeasurethe
aggregateperformanceofRLEandbaselinesinFigure19. TocomputetheCHNS,wefirstcomputethehumannormalized
score(HNS)oftheagent,asdonein(Badiaetal.,2020),as Agent score−Randomscore ,afterwhichitisclippedtobebetween
Humanscore−Randomscore
0and1. Inadditiontoaggregatemetrics,weprovideindividualmeanscoresofallmethodsinall57gamesinTable5along
withthecorrespondinglearningcurvesinFigure18.
Probabilityofimprovement Weusetheprobabilityofimprovement(POI),recommendedin(Agarwaletal.,2021),to
measuretherelativeperformancebetweenalgorithmsacrossall57ATARIgames.
Bootstrappedconfidenceintervals Weusethebootstrappingmethod(DiCiccio&Efron,1996;Agarwaletal.,2021)
toestimatetheconfidenceintervalsforallaggregatedmetricswereport,andmeanperformanceforanalgorithminone
environment.
16RandomLatentExplorationforDeepReinforcementLearning
B.4.ISAACGYM
WedisplaythehyperparametersusedforexperimentsinIsaacGyminTable3. ForPPO,weusethedefaulthyperparameters
recommendedbythecleanrlcodebase(Huangetal.,2022),whichweretunedforIsaacGymtasksandcanvaryacross
tasks(specifically,usingdifferenthyperparametersfortheSHADOWHANDandALLEGROHANDtasks). ForRLE,weuse
thesamehyperparametersforeachtask. Wealsodisplaytheenvironment-specifichyperparametersinTable4,whichare
sharedforeachtrainingalgorithmweconsiderinourexperiments.
B.4.1.RLEIMPLEMENTATIONDETAILSINISAACGYM
Featurenetworkarchitectureandupdate SimilartoourimplementationofRLEintheATARIdomain,westartwitha
randomlyinitializedneuralnetworkϕthathasanMLPbackbonewiththesamearchitectureasthebackboneofthevalue
function. WeupdatethebackboneparamatersusingthesameslowmovingaverageasinAppendixB.3.1withτ =0.005:
MLP ←τ ·MLP +(1−τ)·MLP .
ϕ V ϕ
Computationofrandomrewardbonus Westandardizetheoutputofϕ(s′)usingarunningmeanandstandarddeviation
estimatesotheoutputapproximatesanormaldistributiononRd. Wesampleavectorz ∼Sd−1andcomputetherewardas:
F(s′,z)=ϕ(s′)·z.
Note that this is slightly different from the implementation in ATARI, where we divide by ∥ϕ(s′)∥. We use reward
normalizationforRLEinbothdomainstoscaletherandomizedreward,sobothtypeshaveasimilareffect.
B.4.2.EVALUATIONDETAILSINISAACGYM
PPOnormalizedscore WeusetheIQMofthePPOnormalizedscoretocomputeaggregateperformanceacross9different
environmentsinIsaacGym. WecomputethePPOnormalizedscoreoftheagentasAgent /PPO . Forexample,the
score mean
meanperformanceofPPOinasingleenvironmentunderthePPOnormalizedscorewillbe1. WecomputetheIQMofthis
metricfor5seedsacross9games(or45totalruns)toaggregateperformance.
Otherevaluationdetails SimilartoourexperimentsintheATARIdomain,weusethebootstrappingmethodtoestimate
confidenceintervalsandusetheprobabilityofimprovementtomeasurerelativeperformancebetweendifferentalgorithms.
C.Visualizationson FOURROOM
Inthissection,weprovidefurtherresultsandvisualizationsfortheFOURROOMenvironment:
RLwithtaskreward. Inadditiontothereward-freesetting,wetrainallmethodsintheFOURROOMenvironmentina
sparse-rewardsettingfor2.5Mtimesteps. Thereisarewardof1inthebottom-leftcorner,andtherewardis0atallother
states. Weplotthestatevisitationcountsofallmethodsafter500Kand2.5MtimestepsinFigure11. Inaddition,wetrain
fiveseedsinthisenvironmentforeachmethod,andfindthattheaveragescoreforRLEandNOISYNETis0.6,whilethe
averagescoreforRNDandPPOis0. ThissuggeststhattheFOURROOMenvironmentisataskthatrequiresexplorationas
itisdifficultformethodsthatrelyonactionnoiselikePPOtoachieveanyreward.
Statevisitations.
• Figure11showsstatevisitationcountsforallalgorithmstrainedwithasparsetaskrewardwhichis1atthebottom-left
state(red’*’)and0everywhereelse.
• Figure12showsstatevisitationcountsforallalgorithmstrainedfor500Kand2.5Mstepswithoutanytaskreward.
• Figure 13 shows state visitation counts for RLE trained in a modified version of the environment with stochastic
observationswithina2x2squareregionoftheenvironment.Throughthis,wetestifRLEissusceptibletothe“NoisyTV”
problem(Burdaetal.,2019).
17RandomLatentExplorationforDeepReinforcementLearning
1
0
1
Figure10.VisualizationoftherewardfunctionF(s;z)for10differentrandomchoicesofzinFOURROOMenvironment.Therewardis
isgivenbyF(s,z)=z·ϕ(s)/∥ϕ(s)∥.Theaboveimagedemonstratesthediversityandcoverageofrandomrewardfunctionsinthe
FOURROOMenvironment.
RLE (Ours) PPO 5 RLE (Ours) PPO 6
s s 10 s s 10
3
* * * * 10
2
10
RND NoisyNet RND NoisyNet
s s s s
* * 0 * * 0
(a)After500Ktimesteps. (b)After2.5Mtimesteps.
Figure11.StatevisitationcountsfordifferentmethodsonFOURROOMSenvironmenttrainedfor500Kand2.5Mtimestepswithtask
reward.Thestartlocationisthetop-rightstateofthegrid(representedbythered‘S’).Theagentgetsataskrewardof1atthebottom-left
state(representedbyred‘*’).
Visualizationsoftrajectorydiversityacrossalgorithms.
• Figure14shows5trajectoriessampledfrompoliciestrainedwithRLEacross5differentseedsatthreedifferentpoints
intraining: after500Ksteps,1.5Msteps,and2.5Msteps.
• Figure15showsthesameforNOISYNET.
18
draweRRandomLatentExplorationforDeepReinforcementLearning
RLE (Ours) PPO 5 RLE (Ours) PPO 6
s s 10 s s 10
3
10
2
10
RND NoisyNet RND NoisyNet
s s s s
0 0
(a)After500Ksteps (b)After2.5Msteps
Figure12.StatevisitationcountsfordifferentalgorithmsonFOURROOMSenvironmentaftertrainingfor500Ktimestepsand2.5M
timesteps.Allalgorithmsweretrainedwithouttaskreward(reward-freeexploration).
500K 2.5M 10M
6
s s s 10
3
10
0
Figure13.StatevisitationcountsforRLEwhentrainedinanenvironmentwithstochasticityintheobservationspace.Theobservation
isonlystochasticwithintheredsquareandisdeterministiceverywhereelse. Evenafterdiscoveringtheredsquare,theagentisable
todiscoverstatesoutsideofthoseregionsandcontinuestoexplorethroughouttraining.ThissuggeststhatRLEislessaffectedbythe
NoisyTVproblemcomparedtonovelty-basedexplorationmethods.
• Figure16showsthesameforRND.
• Figure17showsthesameforPPO.
Fromvisualevaluation,theaboveplotssuggestthatRLEinducesmorediversetrajectoriesascomparedtootherbaselines
(PPO,RND,andNOISYNET)ontheFOURROOMenvironment.
19RandomLatentExplorationforDeepReinforcementLearning
500K Timesteps 1.5M Timesteps 2.5M Timesteps
Figure14.VisualizationoftrajectoriesgeneratedbysamplingfromapolicytrainedwithRLEfor2.5Mtimestepsinareward-freesetting
across5seedsatdifferentpointsintraining.Wesample5trajectoriesforeachseed.
20
0
deeS
1
deeS
2
deeS
3
deeS
4
deeSRandomLatentExplorationforDeepReinforcementLearning
500K Timesteps 1.5M Timesteps 2.5M Timesteps
Figure15.VisualizationoftrajectoriesgeneratedbysamplingfromapolicytrainedwithNOISYNETfor2.5Mtimestepsinareward-free
settingacross5seedsatdifferentpointsintraining.Wesample5trajectoriesforeachseed.
21
0
deeS
1
deeS
2
deeS
3
deeS
4
deeSRandomLatentExplorationforDeepReinforcementLearning
500K Timesteps 1.5M Timesteps 2.5M Timesteps
Figure16.VisualizationoftrajectoriesgeneratedbysamplingfromapolicytrainedwithRNDfor2.5Mtimestepsinareward-freesetting
across5seedsatdifferentpointsintraining.Wesample5trajectoriesforeachseed.
22
0
deeS
1
deeS
2
deeS
3
deeS
4
deeSRandomLatentExplorationforDeepReinforcementLearning
500K Timesteps 1.5M Timesteps 2.5M Timesteps
Figure17.VisualizationoftrajectoriesgeneratedbysamplingfromapolicytrainedwithPPOfor2.5Mtimestepsinareward-freesetting
across5seedsatdifferentpointsintraining.Wesample5trajectoriesforeachseed.
23
0
deeS
1
deeS
2
deeS
3
deeS
4
deeSRandomLatentExplorationforDeepReinforcementLearning
Parameter Value
PPO
TotalTimesteps 2,500,000
Optimizer Adam
LearningRate 0.001
AdamEpsilon 0.00001
ParallelWorkers 32
StepsperBatch 128
DiscountRate 0.99
GeneralizedAdvantageEstimationλ 0.95
MinibatchesperEpoch 4
EpochsperTrainingStep 4
ClippingCoefficient 0.2
EntropyLossWeight 0.01
DiscountRate 0.99
ValueLossWeight 0.5
GradientNormBound 0.5
UseAdvantageNormalization True
UseClippedValueLoss True
PolicyNetworkArchitecture MLP(64,64,4)
ValueNetworkArchitectures MLP(64,64,1)
NetworkActivation Tanh
NOISYNET
Initialσ 0.017
RND
IntrinsicRewardCoefficient 1.0
DropProbability 0.25
PredictorNetworkArchitecture MLP(256,256,256,256,256)
TargetNetworkArchitecture MLP(64,256)
NetworkActivation ReLU
RLE
IntrinsicRewardCoefficient 0.1
LatentVectorDimension 4
FeatureNetworkArchitecture MLP(64,64,64,4)
NetworkActivation ReLU
Table1. HyperparametersandnetworkarchitecturesforFOURROOMexperiments.
24RandomLatentExplorationforDeepReinforcementLearning
Parameter Value
PPO
TotalTimesteps 40,000,000
Optimizer Adam
LearningRate 0.0001
AdamEpsilon 0.00001
ParallelWorkers 128
StepsperBatch 128
DiscountRate 0.99
GeneralizedAdvantageEstimationλ 0.95
MinibatchesperEpoch 4
EpochsperTrainingStep 4
ClippingCoefficient 0.1
EntropyLossWeight 0.01
DiscountRate 0.99
ValueLossWeight 0.5
GradientNormBound 0.5
UseAdvantageNormalization True
UseClippedValueLoss True
PolicyNetworkArchitecture CNN+MLP(256,448,448,18)
ValueNetworkArchitectures CNN+MLP(256,448,448,1)
NetworkActivation ReLU
NOISYNET
Initialσ 0.017
EntropyLossWeight 0
RND
IntrinsicRewardCoefficient 1.0
ExtrinsicRewardCoefficient 2.0
UpdateProportion 0.25
ObservationNormalizationIterations 50
DiscountRate 0.999
EntropyLossWeight 0.001
IntrinsicDiscountRate 0.99
PredictorNetworkArchitecture CNN+MLP(512,512,512)
TargetNetworkArchitecture CNN+MLP(512)
NetworkActivation LeakyReLU
RLE
IntrinsicRewardCoefficient 0.01
LatentVectorDimension 16
LatentVectorResampleFrequency 1280
LearningRate 0.0003
DiscountRate 0.999
IntrinsicDiscountRate 0.99
FeatureNetworkUpdateRateτ 0.005
FeatureNetworkArchitecture CNN+MLP(256,448,16)
NetworkActivation ReLU
Table2. HyperparametersandnetworkarchitecturesforATARIexperiments.
25RandomLatentExplorationforDeepReinforcementLearning
Parameter Value
PPO
Optimizer Adam
LearningRate 0.0026
AdamEpsilon 0.00001
StepsperBatch 16
DiscountRate 0.99
GeneralizedAdvantageEstimationλ 0.95
MinibatchesperEpoch 2
EpochsperTrainingStep 4
ClippingCoefficient 0.2
EntropyLossWeight 0.0
DiscountRate 0.99
ValueLossWeight 2.0
GradientNormBound 1.0
UseAdvantageNormalization True
UseClippedValueLoss False
PolicyNetworkArchitecture MLP(256,256,256)
ValueNetworkArchitecture MLP(256,256,256,1)
NetworkActivation Tanh
RewardScale 1.0
RND
IntrinsicValueLossWeight 2.0
IntrinsicRewardCoefficient 0.5
UpdateProportion 0.0625
ObservationNormalizationIterations 50
PredictorNetworkArchitecture MLP(256,256,256,256)
TargetNetworkArchitecture MLP(64,64,256)
NetworkActivation ReLU
RLE
IntrinsicValueLossWeight 0.5
IntrinsicRewardCoefficient 0.01
LatentVectorDimension 32
LatentVectorResampleFrequency 16
LearningRate 0.001
FeatureNetworkUpdateRateτ 0.005
PolicyNetworkArchitecture MLP(256,256,256)
ValueNetworkArchitecture MLP(512,512,256,256,1)
FeatureNetworkArchitecture MLP(512,512,256)
NetworkActivation Tanh
PPO(ALLEGROHANDandSHADOWHAND)
StepsperBatch 8
MinibatchesperEpoch 4
EpochsperTrainingStep 5
RewardScale 0.01
Table3.HyperparametersandnetworkarchitecturesforIsaacGymexperiments. Thenumberoftrainingstepsandparallelworkers
dependsontheenvironment,butaresharedacrossdifferentmethods.
26RandomLatentExplorationforDeepReinforcementLearning
Parameter Value
ALLEGROHAND
NumberofTimesteps 600,000,000
NumberofParallelEnvironments 8,192
SHADOWHAND
NumberofTimesteps 600,000,000
NumberofParallelEnvironments 8,192
BALLBALANCE
NumberofTimesteps 200,000,000
NumberofParallelEnvironments 4,096
HUMANOID
NumberofTimesteps 200,000,000
NumberofParallelEnvironments 4,096
ANT
NumberofTimesteps 100,000,000
NumberofParallelEnvironments 4,096
CARTPOLE
NumberofTimesteps 100,000,000
NumberofParallelEnvironments 4,096
FRANKACABINET
NumberofTimesteps 100,000,000
NumberofParallelEnvironments 4,096
ANYMAL
NumberofTimesteps 100,000,000
NumberofParallelEnvironments 4,096
ANYMALTERRAIN
NumberofTimesteps 100,000,000
NumberofParallelEnvironments 4,096
Table4. Environment-specificparametersandtheirvalues.Theseparametersaresharedacrossallalgorithms.
27RandomLatentExplorationforDeepReinforcementLearning
D.DetailedResultsandLearningCurvesonall ATARI Games
Weprovide:
• Thescoresforeachofthealgorithms(RLE(Ours), PPO,RNDandNOISYNET)onall57ATARIgamesinTable
5. Eachofthealgorithmswastrainedfor40MstepsonallAtarigamesexceptfortheresultsfor MONTEZUMA’S
REVENGEwherewetrainedfor400Msteps. ThereportedHumanperformanceisobtainedfromMnihetal.(2013);
Badiaetal.(2020).
• Learningcurvesforallthealgorithms(RLE(Ours),PPO,RNDandNOISYNET)forall57ATARIgamesinFigure18.
• Aggregatedcappedhumannormalizedscore(describedinAppendixB.3.2)foreachofthealgorithms(RLE(Ours),
PPO,RNDandNOISYNET)overall57Atarigames.
• AnablationstudyofhowthesoftupdateruleforthefeaturenetworkaffectsperformanceonATARIgamestakingthree
metricsintoaccount(scoresonindividualgames,IQMofhumannormalizedscore,andprobabilityofimprovement
overPPO).
E.DetailedResultsandLearningCurvesonall ISAACGYM Tasks
Weprovide:
• ThelearningcurvesforPPOandRLEinall9ISAACGYMtasksthatweconsiderinFigure22.
• ThePOIofallmethodsoverPPOoverall9ISAACGYMtasksinFigure23.
• TheIQMofPPOnormalizedscoreofRLE,PPO,andRNDaggregatedacrossall9ISAACGYMtasksinFigure24.
• ThemeanofPPOnormalizedscoreofRLE,PPO,andRNDaggregatedacrossall9ISAACGYMtasksinFigure25.
• AnablationstudyofhowdifferentnetworkarchitecturesforϕaffectperformanceonISAACGYMtasksIQMofPPO
normalizedscore,andprobabilityofimprovementoverPPO).WeplottheIQMofPPOnormalizedscoreinFigure27,
andprobabilityofimprovementoverPPOinFigure26.
• Anablationstudyofhowusingwhitenoiseforrandomizingrewardsaffectsperformance,showninFigure28.
28RandomLatentExplorationforDeepReinforcementLearning
Alien Amidar Assault Asterix Asteroids 1e6 Atlantis
2000 6000 1.0
6000
1000 500 5000 4000 4000 0.5
2000 2000
0 0 0 0 0 0.0
0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0
BankHeist BattleZone BeamRider Berzerk Bowling Boxing
1000 40000 7500 100
1500 75
5000 500 20000 1000 50 50
2500 500 25
0 0 0 0 0 0
0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0
Breakout Centipede ChopperCommand CrazyClimber Defender DemonAttack
300 10000 100000 40000
40000 200 5000
100 5000 50000 20000 20000
0 0 0 0 0 0
0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0
DoubleDunk Enduro FishingDerby Freeway Frostbite Gopher
0 1000 30 6000 15000
0
10 20 4000 10000
500
50 10 2000 5000
20
0 100 0 0 0
0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0
Gravitar Hero IceHockey Jamesbond Kangaroo Krull
1500 30000 0 15000 10000
1000 20000 10 10000 5000 5000
500 10000 5000
0 0 20 0 0 0
0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0
KungFuMaster MontezumaRevenge MsPacman NameThisGame Phoenix Pitfall
3000 0
6000 20000
40000 4000 2000 10000 100
20000 2000 1000 5000 10000 200
0 0 0 0 0 300
0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0
Pong PrivateEye Qbert Riverraid RoadRunner Robotank
60000 40
15000
500 10000 40000 0 10000 20
0 5000 5000 20000
20 500 0 0 0 0
0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0
Seaquest Skiing Solaris SpaceInvaders StarGunner Surround
0 0
4000 2000 60000
2000
40000
10000 2000 1000 5
1000 20000
0 20000 0 0 0 10
0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0
Tennis TimePilot Tutankham UpNDown Venture VideoPinball
0
10000 200 200000 1000 100000
10
5000 100 100000 500 50000
20
0 0 0 0 0
0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0
Training Progress Training Progress Training Progress
WizardOfWor YarsRevenge Zaxxon
75000
10000
50000 10000
5000
25000
0 0 0
0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0
Training Progress Training Progress Training Progress
PPO RND NoisyNet RLE (Ours)
Figure18. Gamescoresfordifferentalgorithmsforall57ATARIgames.
29
erocS
erocS
erocS
erocS
erocS
erocS
erocS
erocS
erocS
erocSRandomLatentExplorationforDeepReinforcementLearning
PPO RND NoisyNet RLE(Ours)
Alien-v5 1409.22 1010.94 1112.59 1680.13
Amidar-v5 595.53 304.01 210.88 836.01
Assault-v5 1886.68 7045.97 1605.63 8368.57
Asterix-v5 3392.84 2439.56 2892.50 5350.79
Asteroids-v5 2913.15 3232.00 3513.02 1798.64
Atlantis-v5 915703.86 957537.41 972286.70 979023.58
BankHeist-v5 1017.55 250.91 678.44 1036.61
BattleZone-v5 35832.79 32536.65 30021.64 34793.65
BeamRider-v5 5200.10 6915.42 5755.02 5895.93
Berzerk-v5 845.95 931.95 812.45 1445.49
Bowling-v5 46.84 67.17 56.17 35.88
Boxing-v5 97.99 67.44 97.06 93.94
Breakout-v5 227.34 139.41 127.66 337.07
Centipede-v5 4634.04 7972.15 4419.47 4151.10
ChopperCommand-v5 4342.80 2140.08 4787.63 9710.00
CrazyClimber-v5 112278.70 107602.05 116637.17 115593.01
Defender-v5 46957.65 43387.18 34448.07 47872.91
DemonAttack-v5 30714.39 44342.27 28287.09 45217.82
DoubleDunk-v5 -1.57 -5.04 -1.43 -1.51
Enduro-v5 387.89 595.65 150.38 990.95
FishingDerby-v5 31.13 -57.88 14.14 30.68
Freeway-v5 25.83 21.19 32.40 32.49
Frostbite-v5 949.08 2944.37 1747.85 4658.90
Gopher-v5 1020.40 11822.26 1055.82 13290.12
Gravitar-v5 920.19 597.42 674.99 1381.69
Hero-v5 25495.80 14695.30 11433.06 9668.68
IceHockey-v5 -2.09 -16.70 -1.34 -2.39
Jamesbond-v5 3157.81 9347.30 4633.37 2452.21
Kangaroo-v5 6504.67 5474.45 1596.99 6992.13
Krull-v5 8731.23 7264.60 9063.52 8981.43
KungFuMaster-v5 26131.84 30902.44 43341.34 27813.32
MontezumaRevenge-v5 2077.03 4406.79 0.00 79.48
MsPacman-v5 2417.82 1446.32 2127.62 2676.20
NameThisGame-v5 9392.45 6078.34 7818.62 13701.36
Phoenix-v5 7137.14 19195.54 4786.92 11272.80
Pitfall-v5 -0.67 -3.41 -0.05 -57.65
Pong-v5 14.52 -10.34 7.10 17.17
PrivateEye-v5 98.34 87.24 95.55 97.79
Qbert-v5 12168.36 4300.73 3381.40 16261.59
Riverraid-v5 9268.85 4267.51 5642.73 12009.63
RoadRunner-v5 30354.36 19452.68 27037.68 53920.12
Robotank-v5 28.68 22.11 26.34 36.71
Seaquest-v5 1172.22 2463.42 920.71 1724.96
Skiing-v5 -16370.14 -10644.07 -16398.72 -13887.77
Solaris-v5 2203.41 1206.94 2584.66 2203.76
SpaceInvaders-v5 938.00 878.91 981.24 1981.37
StarGunner-v5 52219.39 23174.16 42645.43 64011.13
Surround-v5 -3.41 -7.28 -2.41 -3.91
Tennis-v5 -2.03 -19.44 -1.06 -4.49
TimePilot-v5 8319.51 9695.60 10888.94 11636.24
Tutankham-v5 204.57 140.97 142.70 209.23
UpNDown-v5 212171.29 251442.66 219951.36 151036.48
Venture-v5 401.20 969.00 0.06 782.98
VideoPinball-v5 32654.24 35275.58 28236.75 84825.64
WizardOfWor-v5 8355.05 10151.69 6306.86 9942.29
YarsRevenge-v5 74833.17 71789.37 65902.61 58507.98
Zaxxon-v5 17354.21 6273.86 6104.86 17403.15
Table5.Performanceonall57ATARIgames. Eachalgorithmwastrainedfor40Mtimesteps,exceptforMONTEZUMA’SREVENGE
wherewetrainedfor400Mtimesteps.ThereportedHumanperformanceisobtainedfromMnihetal.(2013);Badiaetal.(2020).
30RandomLatentExplorationforDeepReinforcementLearning
0.8
0.7
0.6
0.5
0.4
0.3
PPO
RND
0.2
NoisyNet
0.1
RLE (Ours)
0.0
0.0 0.2 0.4 0.6 0.8 1.0
Training Progress
Figure19.Cappedhumannormalizedscoreacrossall57Atarigames.RLEoutperformsallothermethodsinthismetricandrequireshalf
thetrainingtimetoreachthesamescoreasthenextbestmethod(PPO).
2.5 RLE
RLE (No Update)
2.0
1.5
Pr. of Improvement Over PPO 1.0
RLE 0.68
0.5
RLE (No Update) 0.65
0.0
0.0 0.2 0.4 0.6 0.8 1.0
0.3 0.4 0.5 0.6 0.7 0.8 Training Progress
Pr(Algorithm > PPO)
(b)IQMofhumannormalizedscoreofRLE,bothwithand
(a)ProbabilityofimprovementoverPPOwithandwithouta withoutaslowvaluefeatureupdaterule.Withrespecttothis
slowvaluefeatureupdaterule.Usingthevaluefeaturesleads metric,bothversionsofthemethodperformverysimilarly
toaslightincreaseinperformance. overall.
Figure20. ComparisonofRLEperformancewithandwithoutaslowvaluefeatureupdaterule.
31
mhtiroglA
erocS
dezilamroN
erocS
dezilamroNRandomLatentExplorationforDeepReinforcementLearning
Alien 1000 Amidar Assault 6000 Asterix Asteroids 1.01e6 Atlantis
2000 2000
4000
500 5000 0.5
1000 2000 1000
0 0 0 0.0
0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0
BankHeist BattleZone BeamRider Berzerk Bowling Boxing
1000 6000 100
1500
4000 40
500 20000 1000 50
2000
500
20
0 0 0 0
0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0
Breakout Centipede ChopperCommand CrazyClimber Defender DemonAttack
300 4000 10000 100000 60000 60000
200 40000 40000
100 2000 5000 50000 20000 20000
0 0 0 0 0
0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0
DoubleDunk Enduro FishingDerby Freeway Frostbite Gopher
1000 30 6000
0
0 500 20 4000 10000
50 10 2000
20 0 100 0 0 0
0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0
Gravitar Hero IceHockey Jamesbond Kangaroo Krull
2000
10000 2.5 4000 10000
1000 5000 5.0 2000 5000 5000
7.5
0 0 0 0 0
0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0
KungFuMaster MontezumaRevenge MsPacman NameThisGame Phoenix Pitfall
30000 300 3000 15000 0
20000 200 2000 10000 10000 50
10000 100 1000 5000 5000
100
0 0 0 0 0
0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0
Pong PrivateEye Qbert Riverraid RoadRunner Robotank
15000 60000 40
200 15000
0 0 10000 10000 40000 20
200 5000 5000 20000
20 400 0 0 0 0
0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0
Seaquest Skiing Solaris SpaceInvaders StarGunner Surround
1500 12000 3000 2000 60000 4
1000 14000 2000 40000 6
1000
500 16000 1000 20000 8
0 0 0 10
0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0
Tennis TimePilot Tutankham UpNDown Venture VideoPinball
200 200000 100000
10000 1000
10
5000 100 100000 500 50000
20
0 0 0 0 0
0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0
Training Progress Training Progress Training Progress
WizardOfWor YarsRevenge Zaxxon
20000
10000 75000
50000 10000
5000
25000
0 0 0
0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0
Training Progress Training Progress Training Progress
RLE RLE (No Update)
Figure21.Gamescoresforourmethodwithandwithoutaslowvaluefeatureupdaterule.Performanceisusuallysimilar,butnoticeably
differentinahandfulofgames(forexampleAlien,Frostbite,Skiing,Tutankham,YarsRevenge).
32
erocS
erocS
erocS
erocS
erocS
erocS
erocS
erocS
erocS
erocSRandomLatentExplorationforDeepReinforcementLearning
AllegroHand ShadowHand BallBalance
1250 1500 300
1000
1000 200 750
500
500 100
250
0 0 0
Ant Humanoid Cartpole
500
6000
400
4000
4000 300
200
2000
2000
100
0 0 0
FrankaCabinet AnymalTerrain Anymal
3000
10 50
8 40
2000
6 30
1000 4 20
2 10
0 0 0
0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00
Training Progress Training Progress Training Progress
PPO PPO + Reward Norm. RND RLE (Ours)
Figure22.ComparisonofachievedreturnsbetweenRLEandstandardPPO(higherisbetter). RLEachievesreturngreaterthanorequal
tothatofstandardPPOinthemajorityoftasks.WealsocompareRLEtoanablationofPPOthatusesrewardnormalizationandfind
thatRLEimprovesoveritaswell.Finally,wecompareRLEtoRND,findingthatwhileRNDsurprisinglyimprovesperformancein
thesetaskscomparedtoPPO,RLEimprovesoverRNDinthemajorityoftasks.
33
snruteR
snruteR
snruteRRandomLatentExplorationforDeepReinforcementLearning
Pr(Algorithm > PPO)
NoisyNet 0.35
RND 0.38
RLE 0.67
0.3 0.4 0.5 0.6 0.7 0.8
(a) Atari Games
Pr(Algorithm > PPO)
PPO (Rew. Norm.) 0.42
RND 0.54
RLE 0.66
0.3 0.4 0.5 0.6 0.7 0.8
(b) IsaacGym
Figure23.(a)ProbabilityofimprovementofRLE,RND,andNOISYNEToverPPOacrossall57ATARIgames.POIoverPPOofboth
NOISYNETandRNDarebelow0.5,implyingthatneitherNOISYNETnorRNDstatisticallyimproveoverPPOoverallacross57ATARI
games.(b)ProbabilityofimprovementofRLE,RND,andPPOwithrewardnormalizationoverPPOacrossall9ISAACGYMtasks.POI
overPPOofbothRNDandPPOwithrewardnormalizationarebelow0.5,demonstratingthatneitherbaselinestatisticallyimprovesover
PPOoverallacrosstheISAACGYMtasks.
PPO
2.0 RND
PPO + Reward Norm.
RLE (Ours)
1.5
1.0
0.5
0.0
0.0 0.2 0.4 0.6 0.8 1.0
Training Progress
Figure24.Normalizedscoreacrossall9ISAACGYMtasksthatweconsider,aggregatedusingtheIQM.RLEachievesahigherinterquartile
meanofnormalizedscorecomparedtobothvariantsofPPO,indicatingthatitcanimproveoverPPOincontinuouscontroldomainsas
well.Meanwhile,RLEperformssimilarlytoRNDintermsofaggregatedperformance.
34
mhtiroglA
erocS
dezilamroN
mhtiroglARandomLatentExplorationforDeepReinforcementLearning
2.0
1.5
1.0
PPO
0.5 RND
PPO + Reward Norm.
RLE (Ours)
0.0
0.0 0.2 0.4 0.6 0.8 1.0
Training Progress
Figure25.Meannormalizedscoreacrossall9ISAACGYMtasks.Aggregatingusingthemeanyieldssimilarresults: RLEoutperforms
bothvariantsofPPO,andslightlyoutperformsRNDoverall.
Probability of Improvement over PPO
0.69
0.66
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0
RLE RLE (Diff. Arch.)
Algorithm
Figure26.ProbabilityofimprovementoverPPOofRLEandRLEwithadifferentarchitectureforϕ.Theprobabilityofimprovement
forbothRLEvariantsiscloseandtheconfidenceintervalsfortheprobabilityofimprovementmetricheavilyoverlap.Thissuggeststhat
RLEisrobusttothechoiceofarchitectureforϕ.
35
)OPP
>
mhtiroglA(rP
erocS
dezilamroNRandomLatentExplorationforDeepReinforcementLearning
RLE (Diff. Arch.)
2.0
RLE
1.5
1.0
0.5
0.0
0.0 0.2 0.4 0.6 0.8 1.0
Training Progress
Figure27.IQMofPPOnormalizedscoreofRLEandRLEwithadifferentarchitectureforϕ.Thedifferentarchitectureusedinthis
experimenthaslesswidthandusesonelesslayer.TheIQMofnormalizedscoreissimilarforbothmethods,suggestingthatRLEdoes
nothighlydependonthearchitectureofthenetworkϕ.
AllegroHand ShadowHand BallBalance
300
1250 1500
1000
1000 200 750
500
500 100
250
0 0 0
Ant Humanoid Cartpole
500
6000
400
4000
4000 300
2000 200
2000
100
0 0 0
FrankaCabinet AnymalTerrain Anymal
1500 10 50
8 40
1000
6 30
4 20
500
2 10
0 0 0
0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00
Training Progress Training Progress Training Progress
PPO + Random Reward RLE (Ours)
Figure28.ComparisonofachievedreturnsbetweenRLEandPPOwithrandomnormalnoisesampledi.i.dfromastandardnormal
distributionaddedtotherewardateachtimestep.Theintrinsicrewardcoefficientis0.01.RLEoutperformsthisvariantofPPOinalarge
majorityofgames,suggestingthatRLEbenefitsfromusingstate-dependentrandomrewards.
36
snruteR
snruteR
snruteR
erocS
dezilamroN