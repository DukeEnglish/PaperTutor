[
    {
        "title": "Phase-Bounded Broadcast Networks over Topologies of Communication",
        "authors": "Lucie GuillouArnaud SangnierNathalie Sznajder",
        "links": "http://arxiv.org/abs/2406.15202v1",
        "entry_id": "http://arxiv.org/abs/2406.15202v1",
        "pdf_url": "http://arxiv.org/pdf/2406.15202v1",
        "summary": "We study networks of processes that all execute the same finite state\nprotocol and that communicate through broadcasts. The processes are organized\nin a graph (a topology) and only the neighbors of a process in this graph can\nreceive its broadcasts. The coverability problem asks, given a protocol and a\nstate of the protocol, whether there is a topology for the processes such that\none of them (at least) reaches the given state. This problem is undecidable. We\nstudy here an under-approximation of the problem where processes alternate a\nbounded number of times $k$ between phases of broadcasting and phases of\nreceiving messages. We show that, if the problem remains undecidable when $k$\nis greater than 6, it becomes decidable for $k=2$, and EXPSPACE-complete for\n$k=1$. Furthermore, we show that if we restrict ourselves to line topologies,\nthe problem is in $P$ for $k=1$ and $k=2$.",
        "updated": "2024-06-21 14:43:23 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.15202v1"
    },
    {
        "title": "Towards General Negotiation Strategies with End-to-End Reinforcement Learning",
        "authors": "Bram M. RentingThomas M. MoerlandHolger H. HoosCatholijn M. Jonker",
        "links": "http://arxiv.org/abs/2406.15096v1",
        "entry_id": "http://arxiv.org/abs/2406.15096v1",
        "pdf_url": "http://arxiv.org/pdf/2406.15096v1",
        "summary": "The research field of automated negotiation has a long history of designing\nagents that can negotiate with other agents. Such negotiation strategies are\ntraditionally based on manual design and heuristics. More recently,\nreinforcement learning approaches have also been used to train agents to\nnegotiate. However, negotiation problems are diverse, causing observation and\naction dimensions to change, which cannot be handled by default linear policy\nnetworks. Previous work on this topic has circumvented this issue either by\nfixing the negotiation problem, causing policies to be non-transferable between\nnegotiation problems or by abstracting the observations and actions into\nfixed-size representations, causing loss of information and expressiveness due\nto feature design. We developed an end-to-end reinforcement learning method for\ndiverse negotiation problems by representing observations and actions as a\ngraph and applying graph neural networks in the policy. With empirical\nevaluations, we show that our method is effective and that we can learn to\nnegotiate with other agents on never-before-seen negotiation problems. Our\nresult opens up new opportunities for reinforcement learning in negotiation\nagents.",
        "updated": "2024-06-21 12:24:36 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.15096v1"
    },
    {
        "title": "Effects of non-uniform number of actions by Hawkes process on spatial cooperation",
        "authors": "Daiki MiyagawaGenki Ichinose",
        "links": "http://arxiv.org/abs/2406.15036v1",
        "entry_id": "http://arxiv.org/abs/2406.15036v1",
        "pdf_url": "http://arxiv.org/pdf/2406.15036v1",
        "summary": "The emergence of cooperative behavior, despite natural selection favoring\nrational self-interest, presents a significant evolutionary puzzle.\nEvolutionary game theory elucidates why cooperative behavior can be\nadvantageous for survival. However, the impact of non-uniformity in the\nfrequency of actions, particularly when actions are altered in the short term,\nhas received little scholarly attention. To demonstrate the relationship\nbetween the non-uniformity in the frequency of actions and the evolution of\ncooperation, we conducted multi-agent simulations of evolutionary games. In our\nmodel, each agent performs actions in a chain-reaction, resulting in a\nnon-uniform distribution of the number of actions. To achieve a variety of\nnon-uniform action frequency, we introduced two types of chain-reaction rules:\none where an agent's actions trigger subsequent actions, and another where an\nagent's actions depend on the actions of others. Our results revealed that\ncooperation evolves more effectively in scenarios with even slight\nnon-uniformity in action frequency compared to completely uniform cases. In\naddition, scenarios where agents' actions are primarily triggered by their own\nprevious actions more effectively support cooperation, whereas those triggered\nby others' actions are less effective. This implies that a few highly active\nindividuals contribute positively to cooperation, while the tendency to follow\nothers' actions can hinder it.",
        "updated": "2024-06-21 10:33:12 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.15036v1"
    },
    {
        "title": "Autonomous Agents for Collaborative Task under Information Asymmetry",
        "authors": "Wei LiuChenxi WangYifei WangZihao XieRennai QiuYufan DangZhuoyun DuWeize ChenCheng YangChen Qian",
        "links": "http://arxiv.org/abs/2406.14928v1",
        "entry_id": "http://arxiv.org/abs/2406.14928v1",
        "pdf_url": "http://arxiv.org/pdf/2406.14928v1",
        "summary": "Large Language Model Multi-Agent Systems (LLM-MAS) have achieved great\nprogress in solving complex tasks. It performs communication among agents\nwithin the system to collaboratively solve tasks, under the premise of shared\ninformation. However, when agents' communication is leveraged to enhance human\ncooperation, a new challenge arises due to information asymmetry, since each\nagent can only access the information of its human user. Previous MAS struggle\nto complete tasks under this condition. To address this, we propose a new MAS\nparadigm termed iAgents, which denotes Informative Multi-Agent Systems. In\niAgents, the human social network is mirrored in the agent network, where\nagents proactively exchange human information necessary for task resolution,\nthereby overcoming information asymmetry. iAgents employs a novel agent\nreasoning mechanism, InfoNav, to navigate agents' communication towards\neffective information exchange. Together with InfoNav, iAgents organizes human\ninformation in a mixed memory to provide agents with accurate and comprehensive\ninformation for exchange. Additionally, we introduce InformativeBench, the\nfirst benchmark tailored for evaluating LLM agents' task-solving ability under\ninformation asymmetry. Experimental results show that iAgents can collaborate\nwithin a social network of 140 individuals and 588 relationships, autonomously\ncommunicate over 30 turns, and retrieve information from nearly 70,000 messages\nto complete tasks within 3 minutes.",
        "updated": "2024-06-21 07:37:19 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.14928v1"
    },
    {
        "title": "Social learning with complex contagion",
        "authors": "Hiroaki Chiba-OkabeJoshua B. Plotkin",
        "links": "http://arxiv.org/abs/2406.14922v1",
        "entry_id": "http://arxiv.org/abs/2406.14922v1",
        "pdf_url": "http://arxiv.org/pdf/2406.14922v1",
        "summary": "We introduce a mathematical model that combines the concepts of complex\ncontagion with payoff-biased imitation, to describe how social behaviors spread\nthrough a population. Traditional models of social learning by imitation are\nbased on simple contagion -- where an individual may imitate a more successful\nneighbor following a single interaction. Our framework generalizes this process\nto incorporate complex contagion, which requires multiple exposures before an\nindividual considers adopting a different behavior. We formulate this as a\ndiscrete time and state stochastic process in a finite population, and we\nderive its continuum limit as an ordinary differential equation that\ngeneralizes the replicator equation, the most widely used dynamical model in\nevolutionary game theory. When applied to linear frequency-dependent games, our\nsocial learning with complex contagion produces qualitatively different\noutcomes than traditional imitation dynamics: it can shift the Prisoner's\nDilemma from a unique all-defector equilibrium to either a stable mixture of\ncooperators and defectors in the population, or a bistable system; it changes\nthe Snowdrift game from a single to a bistable equilibrium; and it can alter\nthe Coordination game from bistability at the boundaries to two internal\nequilibria. The long-term outcome depends on the balance between the complexity\nof the contagion process and the strength of selection that biases imitation\ntowards more successful types. Our analysis intercalates the fields of\nevolutionary game theory with complex contagions, and it provides a synthetic\nframework that describes more realistic forms of behavioral change in social\nsystems.",
        "updated": "2024-06-21 07:32:58 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.14922v1"
    }
]