[
    {
        "title": "NAVSIM: Data-Driven Non-Reactive Autonomous Vehicle Simulation and Benchmarking",
        "authors": "Daniel DaunerMarcel HallgartenTianyu LiXinshuo WengZhiyu HuangZetong YangHongyang LiIgor GilitschenskiBoris IvanovicMarco PavoneAndreas GeigerKashyap Chitta",
        "links": "http://arxiv.org/abs/2406.15349v1",
        "entry_id": "http://arxiv.org/abs/2406.15349v1",
        "pdf_url": "http://arxiv.org/pdf/2406.15349v1",
        "summary": "Benchmarking vision-based driving policies is challenging. On one hand,\nopen-loop evaluation with real data is easy, but these results do not reflect\nclosed-loop performance. On the other, closed-loop evaluation is possible in\nsimulation, but is hard to scale due to its significant computational demands.\nFurther, the simulators available today exhibit a large domain gap to real\ndata. This has resulted in an inability to draw clear conclusions from the\nrapidly growing body of research on end-to-end autonomous driving. In this\npaper, we present NAVSIM, a middle ground between these evaluation paradigms,\nwhere we use large datasets in combination with a non-reactive simulator to\nenable large-scale real-world benchmarking. Specifically, we gather\nsimulation-based metrics, such as progress and time to collision, by unrolling\nbird's eye view abstractions of the test scenes for a short simulation horizon.\nOur simulation is non-reactive, i.e., the evaluated policy and environment do\nnot influence each other. As we demonstrate empirically, this decoupling allows\nopen-loop metric computation while being better aligned with closed-loop\nevaluations than traditional displacement errors. NAVSIM enabled a new\ncompetition held at CVPR 2024, where 143 teams submitted 463 entries, resulting\nin several new insights. On a large set of challenging scenarios, we observe\nthat simple methods with moderate compute requirements such as TransFuser can\nmatch recent large-scale end-to-end driving architectures such as UniAD. Our\nmodular framework can potentially be extended with new datasets, data curation\nstrategies, and metrics, and will be continually maintained to host future\nchallenges. Our code is available at\nhttps://github.com/autonomousvision/navsim.",
        "updated": "2024-06-21 17:59:02 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.15349v1"
    },
    {
        "title": "Privacy Preserved Blood Glucose Level Cross-Prediction: An Asynchronous Decentralized Federated Learning Approach",
        "authors": "Chengzhe PiaoTaiyu ZhuYu WangStephanie E BaldewegPaul TaylorPantelis GeorgiouJiahao SunJun WangKezhi Li",
        "links": "http://arxiv.org/abs/2406.15346v1",
        "entry_id": "http://arxiv.org/abs/2406.15346v1",
        "pdf_url": "http://arxiv.org/pdf/2406.15346v1",
        "summary": "Newly diagnosed Type 1 Diabetes (T1D) patients often struggle to obtain\neffective Blood Glucose (BG) prediction models due to the lack of sufficient BG\ndata from Continuous Glucose Monitoring (CGM), presenting a significant \"cold\nstart\" problem in patient care. Utilizing population models to address this\nchallenge is a potential solution, but collecting patient data for training\npopulation models in a privacy-conscious manner is challenging, especially\ngiven that such data is often stored on personal devices. Considering the\nprivacy protection and addressing the \"cold start\" problem in diabetes care, we\npropose \"GluADFL\", blood Glucose prediction by Asynchronous Decentralized\nFederated Learning. We compared GluADFL with eight baseline methods using four\ndistinct T1D datasets, comprising 298 participants, which demonstrated its\nsuperior performance in accurately predicting BG levels for cross-patient\nanalysis. Furthermore, patients' data might be stored and shared across various\ncommunication networks in GluADFL, ranging from highly interconnected (e.g.,\nrandom, performs the best among others) to more structured topologies (e.g.,\ncluster and ring), suitable for various social networks. The asynchronous\ntraining framework supports flexible participation. By adjusting the ratios of\ninactive participants, we found it remains stable if less than 70% are\ninactive. Our results confirm that GluADFL offers a practical,\nprivacy-preserving solution for BG prediction in T1D, significantly enhancing\nthe quality of diabetes management.",
        "updated": "2024-06-21 17:57:39 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.15346v1"
    },
    {
        "title": "GenoTEX: A Benchmark for Evaluating LLM-Based Exploration of Gene Expression Data in Alignment with Bioinformaticians",
        "authors": "Haoyang LiuHaohan Wang",
        "links": "http://arxiv.org/abs/2406.15341v1",
        "entry_id": "http://arxiv.org/abs/2406.15341v1",
        "pdf_url": "http://arxiv.org/pdf/2406.15341v1",
        "summary": "Recent advancements in machine learning have significantly improved the\nidentification of disease-associated genes from gene expression datasets.\nHowever, these processes often require extensive expertise and manual effort,\nlimiting their scalability. Large Language Model (LLM)-based agents have shown\npromise in automating these tasks due to their increasing problem-solving\nabilities. To support the evaluation and development of such methods, we\nintroduce GenoTEX, a benchmark dataset for the automatic exploration of gene\nexpression data, involving the tasks of dataset selection, preprocessing, and\nstatistical analysis. GenoTEX provides annotated code and results for solving a\nwide range of gene identification problems, in a full analysis pipeline that\nfollows the standard of computational genomics. These annotations are curated\nby human bioinformaticians who carefully analyze the datasets to ensure\naccuracy and reliability. To provide baselines for these tasks, we present\nGenoAgents, a team of LLM-based agents designed with context-aware planning,\niterative correction, and domain expert consultation to collaboratively explore\ngene datasets. Our experiments with GenoAgents demonstrate the potential of\nLLM-based approaches in genomics data analysis, while error analysis highlights\nthe challenges and areas for future improvement. We propose GenoTEX as a\npromising resource for benchmarking and enhancing AI-driven methods for\ngenomics data analysis. We make our benchmark publicly available at\n\\url{https://github.com/Liu-Hy/GenoTex}.",
        "updated": "2024-06-21 17:55:24 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.15341v1"
    },
    {
        "title": "Image Conductor: Precision Control for Interactive Video Synthesis",
        "authors": "Yaowei LiXintao WangZhaoyang ZhangZhouxia WangZiyang YuanLiangbin XieYuexian ZouYing Shan",
        "links": "http://arxiv.org/abs/2406.15339v1",
        "entry_id": "http://arxiv.org/abs/2406.15339v1",
        "pdf_url": "http://arxiv.org/pdf/2406.15339v1",
        "summary": "Filmmaking and animation production often require sophisticated techniques\nfor coordinating camera transitions and object movements, typically involving\nlabor-intensive real-world capturing. Despite advancements in generative AI for\nvideo creation, achieving precise control over motion for interactive video\nasset generation remains challenging. To this end, we propose Image Conductor,\na method for precise control of camera transitions and object movements to\ngenerate video assets from a single image. An well-cultivated training strategy\nis proposed to separate distinct camera and object motion by camera LoRA\nweights and object LoRA weights. To further address cinematographic variations\nfrom ill-posed trajectories, we introduce a camera-free guidance technique\nduring inference, enhancing object movements while eliminating camera\ntransitions. Additionally, we develop a trajectory-oriented video motion data\ncuration pipeline for training. Quantitative and qualitative experiments\ndemonstrate our method's precision and fine-grained control in generating\nmotion-controllable videos from images, advancing the practical application of\ninteractive video synthesis. Project webpage available at\nhttps://liyaowei-stu.github.io/project/ImageConductor/",
        "updated": "2024-06-21 17:55:05 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.15339v1"
    },
    {
        "title": "Multimodal Task Vectors Enable Many-Shot Multimodal In-Context Learning",
        "authors": "Brandon HuangChancharik MitraAssaf ArbelleLeonid KarlinskyTrevor DarrellRoei Herzig",
        "links": "http://arxiv.org/abs/2406.15334v1",
        "entry_id": "http://arxiv.org/abs/2406.15334v1",
        "pdf_url": "http://arxiv.org/pdf/2406.15334v1",
        "summary": "The recent success of interleaved Large Multimodal Models (LMMs) in few-shot\nlearning suggests that in-context learning (ICL) with many examples can be\npromising for learning new tasks. However, this many-shot multimodal ICL\nsetting has one crucial problem: it is fundamentally limited by the model's\ncontext length set at pretraining. The problem is especially prominent in the\nmultimodal domain, which processes both text and images, requiring additional\ntokens. This motivates the need for a multimodal method to compress many shots\ninto fewer tokens without finetuning. In this work, we enable LMMs to perform\nmultimodal, many-shot in-context learning by leveraging Multimodal Task Vectors\n(MTV)--compact implicit representations of in-context examples compressed in\nthe model's attention heads. Specifically, we first demonstrate the existence\nof such MTV in LMMs and then leverage these extracted MTV to enable many-shot\nin-context learning for various vision-and-language tasks. Our experiments\nsuggest that MTV can scale in performance with the number of compressed shots\nand generalize to similar out-of-domain tasks without additional context length\nfor inference.",
        "updated": "2024-06-21 17:50:02 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.15334v1"
    }
]