[
    {
        "title": "A SMART Mnemonic Sounds like \"Glue Tonic\": Mixing LLMs with Student Feedback to Make Mnemonic Learning Stick",
        "authors": "Nishant BalepurMatthew ShuAlexander HoyleAlison RobeyShi FengSeraphina Goldfarb-TarrantJordan Boyd-Graber",
        "links": "http://arxiv.org/abs/2406.15352v1",
        "entry_id": "http://arxiv.org/abs/2406.15352v1",
        "pdf_url": "http://arxiv.org/pdf/2406.15352v1",
        "summary": "Keyword mnemonics are memorable explanations that link new terms to simpler\nkeywords. Prior works generate mnemonics for students, but they do not guide\nmodels toward mnemonics students prefer and aid learning. We build SMART, a\nmnemonic generator trained on feedback from real students learning new terms.\nTo train SMART, we first fine-tune LLaMA-2 on a curated set of user-written\nmnemonics. We then use LLM alignment to enhance SMART: we deploy mnemonics\ngenerated by SMART in a flashcard app to find preferences on mnemonics students\nfavor. We gather 2684 preferences from 45 students across two types: expressed\n(inferred from ratings) and observed (inferred from student learning), yielding\nthree key findings. First, expressed and observed preferences disagree; what\nstudents think is helpful does not fully capture what is truly helpful. Second,\nBayesian models can synthesize complementary data from multiple preference\ntypes into a single effectiveness signal. SMART is tuned via Direct Preference\nOptimization on this signal, which we show resolves ties and missing labels in\nthe typical method of pairwise comparisons, augmenting data for LLM output\nquality gains. Third, mnemonic experts assess SMART as matching GPT-4, at much\nlower deployment costs, showing the utility of capturing diverse student\nfeedback to align LLMs in education.",
        "updated": "2024-06-21 17:59:51 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.15352v1"
    },
    {
        "title": "Multimodal Task Vectors Enable Many-Shot Multimodal In-Context Learning",
        "authors": "Brandon HuangChancharik MitraAssaf ArbelleLeonid KarlinskyTrevor DarrellRoei Herzig",
        "links": "http://arxiv.org/abs/2406.15334v1",
        "entry_id": "http://arxiv.org/abs/2406.15334v1",
        "pdf_url": "http://arxiv.org/pdf/2406.15334v1",
        "summary": "The recent success of interleaved Large Multimodal Models (LMMs) in few-shot\nlearning suggests that in-context learning (ICL) with many examples can be\npromising for learning new tasks. However, this many-shot multimodal ICL\nsetting has one crucial problem: it is fundamentally limited by the model's\ncontext length set at pretraining. The problem is especially prominent in the\nmultimodal domain, which processes both text and images, requiring additional\ntokens. This motivates the need for a multimodal method to compress many shots\ninto fewer tokens without finetuning. In this work, we enable LMMs to perform\nmultimodal, many-shot in-context learning by leveraging Multimodal Task Vectors\n(MTV)--compact implicit representations of in-context examples compressed in\nthe model's attention heads. Specifically, we first demonstrate the existence\nof such MTV in LMMs and then leverage these extracted MTV to enable many-shot\nin-context learning for various vision-and-language tasks. Our experiments\nsuggest that MTV can scale in performance with the number of compressed shots\nand generalize to similar out-of-domain tasks without additional context length\nfor inference.",
        "updated": "2024-06-21 17:50:02 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.15334v1"
    },
    {
        "title": "Gradient-Mask Tuning Elevates the Upper Limits of LLM Performance",
        "authors": "Haoling LiXin ZhangXiao LiuYeyun GongYifan WangYujiu YangQi ChenPeng Cheng",
        "links": "http://arxiv.org/abs/2406.15330v1",
        "entry_id": "http://arxiv.org/abs/2406.15330v1",
        "pdf_url": "http://arxiv.org/pdf/2406.15330v1",
        "summary": "Large language models (LLMs) have revolutionized lots of fields of research.\nAlthough it is well-known that fine-tuning is essential for enhancing the\ncapabilities of LLMs, existing research suggests that there is potential\nredundancy in the fine-tuning process and therefore proposes to update only a\nsubset of parameters. However, these methods fail to leverage the task-specific\ninformation to identify important parameters during training. Based on the\ninsight that gradients inherently contain information on task-specific data, we\npropose Gradient-Mask Tuning (GMT), a method that selectively updates\nparameters during training based on their gradient information. Specifically,\nwe compute the absolute values of the gradients and apply masking to those with\nrelatively smaller magnitudes. Our empirical results across various tasks\ndemonstrate that GMT not only outperforms traditional fine-tuning methods but\nalso elevates the upper limits of LLM performance. Further analysis indicates\nthat GMT exhibits insensitivity to mask ratio and possesses computational\nefficiency comparable to vanilla SFT.",
        "updated": "2024-06-21 17:42:52 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.15330v1"
    },
    {
        "title": "LongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs",
        "authors": "Ziyan JiangXueguang MaWenhu Chen",
        "links": "http://arxiv.org/abs/2406.15319v1",
        "entry_id": "http://arxiv.org/abs/2406.15319v1",
        "pdf_url": "http://arxiv.org/pdf/2406.15319v1",
        "summary": "In traditional RAG framework, the basic retrieval units are normally short.\nThe common retrievers like DPR normally work with 100-word Wikipedia\nparagraphs. Such a design forces the retriever to search over a large corpus to\nfind the `needle' unit. In contrast, the readers only need to extract answers\nfrom the short retrieved units. Such an imbalanced `heavy' retriever and\n`light' reader design can lead to sub-optimal performance. In order to\nalleviate the imbalance, we propose a new framework LongRAG, consisting of a\n`long retriever' and a `long reader'. LongRAG processes the entire Wikipedia\ninto 4K-token units, which is 30x longer than before. By increasing the unit\nsize, we significantly reduce the total units from 22M to 700K. This\nsignificantly lowers the burden of retriever, which leads to a remarkable\nretrieval score: answer recall@1=71% on NQ (previously 52%) and answer\nrecall@2=72% (previously 47%) on HotpotQA (full-wiki). Then we feed the top-k\nretrieved units ($\\approx$ 30K tokens) to an existing long-context LLM to\nperform zero-shot answer extraction. Without requiring any training, LongRAG\nachieves an EM of 62.7% on NQ, which is the best known result. LongRAG also\nachieves 64.3% on HotpotQA (full-wiki), which is on par of the SoTA model. Our\nstudy offers insights into the future roadmap for combining RAG with\nlong-context LLMs.",
        "updated": "2024-06-21 17:23:21 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.15319v1"
    },
    {
        "title": "STARD: A Chinese Statute Retrieval Dataset with Real Queries Issued by Non-professionals",
        "authors": "Weihang SuYiran HuAnzhe XieQingyao AiZibing QueNing ZhengYun LiuWeixing ShenYiqun Liu",
        "links": "http://arxiv.org/abs/2406.15313v1",
        "entry_id": "http://arxiv.org/abs/2406.15313v1",
        "pdf_url": "http://arxiv.org/pdf/2406.15313v1",
        "summary": "Statute retrieval aims to find relevant statutory articles for specific\nqueries. This process is the basis of a wide range of legal applications such\nas legal advice, automated judicial decisions, legal document drafting, etc.\nExisting statute retrieval benchmarks focus on formal and professional queries\nfrom sources like bar exams and legal case documents, thereby neglecting\nnon-professional queries from the general public, which often lack precise\nlegal terminology and references. To address this gap, we introduce the STAtute\nRetrieval Dataset (STARD), a Chinese dataset comprising 1,543 query cases\ncollected from real-world legal consultations and 55,348 candidate statutory\narticles. Unlike existing statute retrieval datasets, which primarily focus on\nprofessional legal queries, STARD captures the complexity and diversity of real\nqueries from the general public. Through a comprehensive evaluation of various\nretrieval baselines, we reveal that existing retrieval approaches all fall\nshort of these real queries issued by non-professional users. The best method\nonly achieves a Recall@100 of 0.907, suggesting the necessity for further\nexploration and additional research in this area.\n  All the codes and datasets are available at:\nhttps://github.com/oneal2000/STARD/tree/main",
        "updated": "2024-06-21 17:10:09 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.15313v1"
    }
]