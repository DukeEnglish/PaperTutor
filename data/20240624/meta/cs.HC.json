[
    {
        "title": "V-RECS, a Low-Cost LLM4VIS Recommender with Explanations, Captioning and Suggestions",
        "authors": "Luca PodoMarco AngeliniPaola Velardi",
        "links": "http://arxiv.org/abs/2406.15259v1",
        "entry_id": "http://arxiv.org/abs/2406.15259v1",
        "pdf_url": "http://arxiv.org/pdf/2406.15259v1",
        "summary": "NL2VIS (natural language to visualization) is a promising and recent research\narea that involves interpreting natural language queries and translating them\ninto visualizations that accurately represent the underlying data. As we\nnavigate the era of big data, NL2VIS holds considerable application potential\nsince it greatly facilitates data exploration by non-expert users. Following\nthe increasingly widespread usage of generative AI in NL2VIS applications, in\nthis paper we present V-RECS, the first LLM-based Visual Recommender augmented\nwith explanations(E), captioning(C), and suggestions(S) for further data\nexploration. V-RECS' visualization narratives facilitate both response\nverification and data exploration by non-expert users. Furthermore, our\nproposed solution mitigates computational, controllability, and cost issues\nassociated with using powerful LLMs by leveraging a methodology to effectively\nfine-tune small models. To generate insightful visualization narratives, we use\nChain-of-Thoughts (CoT), a prompt engineering technique to help LLM identify\nand generate the logical steps to produce a correct answer. Since CoT is\nreported to perform poorly with small LLMs, we adopted a strategy in which a\nlarge LLM (GPT-4), acting as a Teacher, generates CoT-based instructions to\nfine-tune a small model, Llama-2-7B, which plays the role of a Student.\nExtensive experiments-based on a framework for the quantitative evaluation of\nAI-based visualizations and on manual assessment by a group of\nparticipants-show that V-RECS achieves performance scores comparable to GPT-4,\nat a much lower cost. The efficacy of the V-RECS teacher-student paradigm is\nalso demonstrated by the fact that the un-tuned Llama fails to perform the task\nin the vast majority of test cases. We release V-RECS for the visualization\ncommunity to assist visualization designers throughout the entire visualization\ngeneration process.",
        "updated": "2024-06-21 15:50:10 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.15259v1"
    },
    {
        "title": "Exploring the Efficacy of Robotic Assistants with ChatGPT and Claude in Enhancing ADHD Therapy: Innovating Treatment Paradigms",
        "authors": "Santiago Berrezueta-GuzmanMohanad KandilMaría-Luisa Martín-RuizIván Pau-de-la-CruzStephan Krusche",
        "links": "http://arxiv.org/abs/2406.15198v1",
        "entry_id": "http://arxiv.org/abs/2406.15198v1",
        "pdf_url": "http://arxiv.org/pdf/2406.15198v1",
        "summary": "Attention Deficit Hyperactivity Disorder (ADHD) is a neurodevelopmental\ncondition characterized by inattention, hyperactivity, and impulsivity, which\ncan significantly impact an individual's daily functioning and quality of life.\nOccupational therapy plays a crucial role in managing ADHD by fostering the\ndevelopment of skills needed for daily living and enhancing an individual's\nability to participate fully in school, home, and social situations. Recent\nstudies highlight the potential of integrating Large Language Models (LLMs)\nlike ChatGPT and Socially Assistive Robots (SAR) to improve psychological\ntreatments. This integration aims to overcome existing limitations in mental\nhealth therapy by providing tailored support and adapting to the unique needs\nof this sensitive group. However, there remains a significant gap in research\nexploring the combined use of these advanced technologies in ADHD therapy,\nsuggesting an opportunity for novel therapeutic approaches.\n  Thus, we integrated two advanced language models, ChatGPT-4 Turbo and\nClaude-3 Opus, into a robotic assistant to explore how well each model performs\nin robot-assisted interactions. Additionally, we have compared their\nperformance in a simulated therapy scenario to gauge their effectiveness\nagainst a clinically validated customized model. The results of this study show\nthat ChatGPT-4 Turbo excelled in performance and responsiveness, making it\nsuitable for time-sensitive applications. Claude-3 Opus, on the other hand,\nshowed strengths in understanding, coherence, and ethical considerations,\nprioritizing safe and engaging interactions. Both models demonstrated\ninnovation and adaptability, but ChatGPT-4 Turbo offered greater ease of\nintegration and broader language support. The selection between them hinges on\nthe specific demands of ADHD therapy.",
        "updated": "2024-06-21 14:38:25 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.15198v1"
    },
    {
        "title": "Balancing The Perception of Cheating Detection, Privacy and Fairness: A Mixed-Methods Study of Visual Data Obfuscation in Remote Proctoring",
        "authors": "Suvadeep MukherjeeVerena DistlerGabriele LenziniPedro Cardoso-Leite",
        "links": "http://arxiv.org/abs/2406.15074v1",
        "entry_id": "http://arxiv.org/abs/2406.15074v1",
        "pdf_url": "http://arxiv.org/pdf/2406.15074v1",
        "summary": "Remote proctoring technology, a cheating-preventive measure, often raises\nprivacy and fairness concerns that may affect test-takers' experiences and the\nvalidity of test results. Our study explores how selectively obfuscating\ninformation in video recordings can protect test-takers' privacy while ensuring\neffective and fair cheating detection. Interviews with experts (N=9) identified\nfour key video regions indicative of potential cheating behaviors: the\ntest-taker's face, body, background and the presence of individuals in the\nbackground. Experts recommended specific obfuscation methods for each region\nbased on privacy significance and cheating behavior frequency, ranging from\nconventional blurring to advanced methods like replacement with deepfake, 3D\navatars and silhouetting. We then conducted a vignette experiment with\npotential test-takers (N=259, non-experts) to evaluate their perceptions of\ncheating detection, visual privacy and fairness, using descriptions and\nexamples of still images for each expert-recommended combination of video\nregions and obfuscation methods. Our results indicate that the effectiveness of\nobfuscation methods varies by region. Tailoring remote proctoring with\nregion-specific advanced obfuscation methods can improve the perceptions of\nprivacy and fairness compared to the conventional methods, though it may\ndecrease perceived information sufficiency for detecting cheating. However,\nnon-experts preferred conventional blurring for videos they were more willing\nto share, highlighting a gap between the perceived effectiveness of the\nadvanced obfuscation methods and their practical acceptance. This study\ncontributes to the field of user-centered privacy by suggesting promising\ndirections to address current remote proctoring challenges and guiding future\nresearch.",
        "updated": "2024-06-21 11:40:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.15074v1"
    },
    {
        "title": "Real-Time Hand Gesture Recognition: Integrating Skeleton-Based Data Fusion and Multi-Stream CNN",
        "authors": "Oluwaleke YusufMaki HabibMohamed Moustafa",
        "links": "http://arxiv.org/abs/2406.15003v1",
        "entry_id": "http://arxiv.org/abs/2406.15003v1",
        "pdf_url": "http://arxiv.org/pdf/2406.15003v1",
        "summary": "This study focuses on Hand Gesture Recognition (HGR), which is vital for\nperceptual computing across various real-world contexts. The primary challenge\nin the HGR domain lies in dealing with the individual variations inherent in\nhuman hand morphology. To tackle this challenge, we introduce an innovative HGR\nframework that combines data-level fusion and an Ensemble Tuner Multi-stream\nCNN architecture. This approach effectively encodes spatiotemporal gesture\ninformation from the skeleton modality into RGB images, thereby minimizing\nnoise while improving semantic gesture comprehension. Our framework operates in\nreal-time, significantly reducing hardware requirements and computational\ncomplexity while maintaining competitive performance on benchmark datasets such\nas SHREC2017, DHG1428, FPHA, LMDHG and CNR. This improvement in HGR\ndemonstrates robustness and paves the way for practical, real-time applications\nthat leverage resource-limited devices for human-machine interaction and\nambient intelligence.",
        "updated": "2024-06-21 09:30:59 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.15003v1"
    },
    {
        "title": "Human-AI collectives produce the most accurate differential diagnoses",
        "authors": "N. ZöllerJ. BergerI. LinN. FuJ. KomarneniG. BarabucciK. LaskowskiV. ShiaB. HarackE. A. ChuV. TrianniR. H. J. M. KurversS. M. Herzog",
        "links": "http://arxiv.org/abs/2406.14981v1",
        "entry_id": "http://arxiv.org/abs/2406.14981v1",
        "pdf_url": "http://arxiv.org/pdf/2406.14981v1",
        "summary": "Artificial intelligence systems, particularly large language models (LLMs),\nare increasingly being employed in high-stakes decisions that impact both\nindividuals and society at large, often without adequate safeguards to ensure\nsafety, quality, and equity. Yet LLMs hallucinate, lack common sense, and are\nbiased - shortcomings that may reflect LLMs' inherent limitations and thus may\nnot be remedied by more sophisticated architectures, more data, or more human\nfeedback. Relying solely on LLMs for complex, high-stakes decisions is\ntherefore problematic. Here we present a hybrid collective intelligence system\nthat mitigates these risks by leveraging the complementary strengths of human\nexperience and the vast information processed by LLMs. We apply our method to\nopen-ended medical diagnostics, combining 40,762 differential diagnoses made by\nphysicians with the diagnoses of five state-of-the art LLMs across 2,133\nmedical cases. We show that hybrid collectives of physicians and LLMs\noutperform both single physicians and physician collectives, as well as single\nLLMs and LLM ensembles. This result holds across a range of medical specialties\nand professional experience, and can be attributed to humans' and LLMs'\ncomplementary contributions that lead to different kinds of errors. Our\napproach highlights the potential for collective human and machine intelligence\nto improve accuracy in complex, open-ended domains like medical diagnostics.",
        "updated": "2024-06-21 08:46:30 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.14981v1"
    }
]