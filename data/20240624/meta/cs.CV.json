[
    {
        "title": "NAVSIM: Data-Driven Non-Reactive Autonomous Vehicle Simulation and Benchmarking",
        "authors": "Daniel DaunerMarcel HallgartenTianyu LiXinshuo WengZhiyu HuangZetong YangHongyang LiIgor GilitschenskiBoris IvanovicMarco PavoneAndreas GeigerKashyap Chitta",
        "links": "http://arxiv.org/abs/2406.15349v1",
        "entry_id": "http://arxiv.org/abs/2406.15349v1",
        "pdf_url": "http://arxiv.org/pdf/2406.15349v1",
        "summary": "Benchmarking vision-based driving policies is challenging. On one hand,\nopen-loop evaluation with real data is easy, but these results do not reflect\nclosed-loop performance. On the other, closed-loop evaluation is possible in\nsimulation, but is hard to scale due to its significant computational demands.\nFurther, the simulators available today exhibit a large domain gap to real\ndata. This has resulted in an inability to draw clear conclusions from the\nrapidly growing body of research on end-to-end autonomous driving. In this\npaper, we present NAVSIM, a middle ground between these evaluation paradigms,\nwhere we use large datasets in combination with a non-reactive simulator to\nenable large-scale real-world benchmarking. Specifically, we gather\nsimulation-based metrics, such as progress and time to collision, by unrolling\nbird's eye view abstractions of the test scenes for a short simulation horizon.\nOur simulation is non-reactive, i.e., the evaluated policy and environment do\nnot influence each other. As we demonstrate empirically, this decoupling allows\nopen-loop metric computation while being better aligned with closed-loop\nevaluations than traditional displacement errors. NAVSIM enabled a new\ncompetition held at CVPR 2024, where 143 teams submitted 463 entries, resulting\nin several new insights. On a large set of challenging scenarios, we observe\nthat simple methods with moderate compute requirements such as TransFuser can\nmatch recent large-scale end-to-end driving architectures such as UniAD. Our\nmodular framework can potentially be extended with new datasets, data curation\nstrategies, and metrics, and will be continually maintained to host future\nchallenges. Our code is available at\nhttps://github.com/autonomousvision/navsim.",
        "updated": "2024-06-21 17:59:02 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.15349v1"
    },
    {
        "title": "Full-Scale Indexing and Semantic Annotation of CT Imaging: Boosting FAIRness",
        "authors": "Hannes UlrichRobin HendelSantiago PazminoBjörn BerghBjörn Schreiweis",
        "links": "http://arxiv.org/abs/2406.15340v1",
        "entry_id": "http://arxiv.org/abs/2406.15340v1",
        "pdf_url": "http://arxiv.org/pdf/2406.15340v1",
        "summary": "Background: The integration of artificial intelligence into medicine has led\nto significant advances, particularly in diagnostics and treatment planning.\nHowever, the reliability of AI models is highly dependent on the quality of the\ntraining data, especially in medical imaging, where varying patient data and\nevolving medical knowledge pose a challenge to the accuracy and\ngeneralizability of given datasets. Results: The proposed approach focuses on\nthe integration and enhancement of clinical computed tomography (CT) image\nseries for better findability, accessibility, interoperability, and\nreusability. Through an automated indexing process, CT image series are\nsemantically enhanced using the TotalSegmentator framework for segmentation and\nresulting SNOMED CT annotations. The metadata is standardized with HL7 FHIR\nresources to enable efficient data recognition and data exchange between\nresearch projects. Conclusions: The study successfully integrates a robust\nprocess within the UKSH MeDIC, leading to the semantic enrichment of over\n230,000 CT image series and over 8 million SNOMED CT annotations. The\nstandardized representation using HL7 FHIR resources improves discoverability\nand facilitates interoperability, providing a foundation for the FAIRness of\nmedical imaging data. However, developing automated annotation methods that can\nkeep pace with growing clinical datasets remains a challenge to ensure\ncontinued progress in large-scale integration and indexing of medical imaging\nfor advanced healthcare AI applications.",
        "updated": "2024-06-21 17:55:22 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.15340v1"
    },
    {
        "title": "Image Conductor: Precision Control for Interactive Video Synthesis",
        "authors": "Yaowei LiXintao WangZhaoyang ZhangZhouxia WangZiyang YuanLiangbin XieYuexian ZouYing Shan",
        "links": "http://arxiv.org/abs/2406.15339v1",
        "entry_id": "http://arxiv.org/abs/2406.15339v1",
        "pdf_url": "http://arxiv.org/pdf/2406.15339v1",
        "summary": "Filmmaking and animation production often require sophisticated techniques\nfor coordinating camera transitions and object movements, typically involving\nlabor-intensive real-world capturing. Despite advancements in generative AI for\nvideo creation, achieving precise control over motion for interactive video\nasset generation remains challenging. To this end, we propose Image Conductor,\na method for precise control of camera transitions and object movements to\ngenerate video assets from a single image. An well-cultivated training strategy\nis proposed to separate distinct camera and object motion by camera LoRA\nweights and object LoRA weights. To further address cinematographic variations\nfrom ill-posed trajectories, we introduce a camera-free guidance technique\nduring inference, enhancing object movements while eliminating camera\ntransitions. Additionally, we develop a trajectory-oriented video motion data\ncuration pipeline for training. Quantitative and qualitative experiments\ndemonstrate our method's precision and fine-grained control in generating\nmotion-controllable videos from images, advancing the practical application of\ninteractive video synthesis. Project webpage available at\nhttps://liyaowei-stu.github.io/project/ImageConductor/",
        "updated": "2024-06-21 17:55:05 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.15339v1"
    },
    {
        "title": "Keystroke Dynamics Against Academic Dishonesty in the Age of LLMs",
        "authors": "Debnath KunduAtharva MehtaRajesh KumarNaman LalAvinash AnandApoorv SinghRajiv Ratn Shah",
        "links": "http://arxiv.org/abs/2406.15335v1",
        "entry_id": "http://arxiv.org/abs/2406.15335v1",
        "pdf_url": "http://arxiv.org/pdf/2406.15335v1",
        "summary": "The transition to online examinations and assignments raises significant\nconcerns about academic integrity. Traditional plagiarism detection systems\noften struggle to identify instances of intelligent cheating, particularly when\nstudents utilize advanced generative AI tools to craft their responses. This\nstudy proposes a keystroke dynamics-based method to differentiate between bona\nfide and assisted writing within academic contexts. To facilitate this, a\ndataset was developed to capture the keystroke patterns of individuals engaged\nin writing tasks, both with and without the assistance of generative AI. The\ndetector, trained using a modified TypeNet architecture, achieved accuracies\nranging from 74.98% to 85.72% in condition-specific scenarios and from 52.24%\nto 80.54% in condition-agnostic scenarios. The findings highlight significant\ndifferences in keystroke dynamics between genuine and assisted writing. The\noutcomes of this study enhance our understanding of how users interact with\ngenerative AI and have implications for improving the reliability of digital\neducational platforms.",
        "updated": "2024-06-21 17:51:26 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.15335v1"
    },
    {
        "title": "Multimodal Task Vectors Enable Many-Shot Multimodal In-Context Learning",
        "authors": "Brandon HuangChancharik MitraAssaf ArbelleLeonid KarlinskyTrevor DarrellRoei Herzig",
        "links": "http://arxiv.org/abs/2406.15334v1",
        "entry_id": "http://arxiv.org/abs/2406.15334v1",
        "pdf_url": "http://arxiv.org/pdf/2406.15334v1",
        "summary": "The recent success of interleaved Large Multimodal Models (LMMs) in few-shot\nlearning suggests that in-context learning (ICL) with many examples can be\npromising for learning new tasks. However, this many-shot multimodal ICL\nsetting has one crucial problem: it is fundamentally limited by the model's\ncontext length set at pretraining. The problem is especially prominent in the\nmultimodal domain, which processes both text and images, requiring additional\ntokens. This motivates the need for a multimodal method to compress many shots\ninto fewer tokens without finetuning. In this work, we enable LMMs to perform\nmultimodal, many-shot in-context learning by leveraging Multimodal Task Vectors\n(MTV)--compact implicit representations of in-context examples compressed in\nthe model's attention heads. Specifically, we first demonstrate the existence\nof such MTV in LMMs and then leverage these extracted MTV to enable many-shot\nin-context learning for various vision-and-language tasks. Our experiments\nsuggest that MTV can scale in performance with the number of compressed shots\nand generalize to similar out-of-domain tasks without additional context length\nfor inference.",
        "updated": "2024-06-21 17:50:02 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.15334v1"
    }
]