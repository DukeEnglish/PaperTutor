[
    {
        "title": "ExDAG: Exact learning of DAGs",
        "authors": "Pavel RytířAleš WodeckiJakub Mareček",
        "links": "http://arxiv.org/abs/2406.15229v1",
        "entry_id": "http://arxiv.org/abs/2406.15229v1",
        "pdf_url": "http://arxiv.org/pdf/2406.15229v1",
        "summary": "There has been a growing interest in causal learning in recent years.\nCommonly used representations of causal structures, including Bayesian networks\nand structural equation models (SEM), take the form of directed acyclic graphs\n(DAGs). We provide a novel mixed-integer quadratic programming formulation and\nassociated algorithm that identifies DAGs on up to 50 vertices, where these are\nidentifiable. We call this method ExDAG, which stands for Exact learning of\nDAGs. Although there is a superexponential number of constraints that prevent\nthe formation of cycles, the algorithm adds constraints violated by solutions\nfound, rather than imposing all constraints in each continuous-valued\nrelaxation. Our empirical results show that ExDAG outperforms local\nstate-of-the-art solvers in terms of precision and outperforms state-of-the-art\nglobal solvers with respect to scaling, when considering Gaussian noise. We\nalso provide validation with respect to other noise distributions.",
        "updated": "2024-06-21 15:15:38 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.15229v1"
    },
    {
        "title": "Generative Topological Networks",
        "authors": "Alona Levy-JurgensonZohar Yakhini",
        "links": "http://arxiv.org/abs/2406.15152v1",
        "entry_id": "http://arxiv.org/abs/2406.15152v1",
        "pdf_url": "http://arxiv.org/pdf/2406.15152v1",
        "summary": "Generative models have seen significant advancements in recent years, yet\noften remain challenging and costly to train and use. We introduce Generative\nTopological Networks (GTNs) -- a new class of generative models that addresses\nthese shortcomings. GTNs are trained deterministically using a simple\nsupervised learning approach grounded in topology theory. GTNs are fast to\ntrain, and require only a single forward pass in a standard feedforward neural\nnetwork to generate samples. We demonstrate the strengths of GTNs in several\ndatasets, including MNIST, celebA and the Hands and Palm Images dataset.\nFinally, the theory behind GTNs offers insights into how to train generative\nmodels for improved performance.",
        "updated": "2024-06-21 13:55:34 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.15152v1"
    },
    {
        "title": "KalMamba: Towards Efficient Probabilistic State Space Models for RL under Uncertainty",
        "authors": "Philipp BeckerNiklas FreymuthGerhard Neumann",
        "links": "http://arxiv.org/abs/2406.15131v1",
        "entry_id": "http://arxiv.org/abs/2406.15131v1",
        "pdf_url": "http://arxiv.org/pdf/2406.15131v1",
        "summary": "Probabilistic State Space Models (SSMs) are essential for Reinforcement\nLearning (RL) from high-dimensional, partial information as they provide\nconcise representations for control. Yet, they lack the computational\nefficiency of their recent deterministic counterparts such as S4 or Mamba. We\npropose KalMamba, an efficient architecture to learn representations for RL\nthat combines the strengths of probabilistic SSMs with the scalability of\ndeterministic SSMs. KalMamba leverages Mamba to learn the dynamics parameters\nof a linear Gaussian SSM in a latent space. Inference in this latent space\namounts to standard Kalman filtering and smoothing. We realize these operations\nusing parallel associative scanning, similar to Mamba, to obtain a principled,\nhighly efficient, and scalable probabilistic SSM. Our experiments show that\nKalMamba competes with state-of-the-art SSM approaches in RL while\nsignificantly improving computational efficiency, especially on longer\ninteraction sequences.",
        "updated": "2024-06-21 13:27:36 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.15131v1"
    },
    {
        "title": "Exact discovery is polynomial for sparse causal Bayesian networks",
        "authors": "Felix L. RiosGiusi MoffaJack Kuipers",
        "links": "http://arxiv.org/abs/2406.15012v1",
        "entry_id": "http://arxiv.org/abs/2406.15012v1",
        "pdf_url": "http://arxiv.org/pdf/2406.15012v1",
        "summary": "Causal Bayesian networks are widely used tools for summarising the\ndependencies between variables and elucidating their putative causal\nrelationships. Learning networks from data is computationally hard in general.\nThe current state-of-the-art approaches for exact causal discovery are integer\nlinear programming over the underlying space of directed acyclic graphs,\ndynamic programming and shortest-path searches over the space of topological\norders, and constraint programming combining both. For dynamic programming over\norders, the computational complexity is known to be exponential base 2 in the\nnumber of variables in the network. We demonstrate how to use properties of\nBayesian networks to prune the search space and lower the computational cost,\nwhile still guaranteeing exact discovery. When including new path-search and\ndivide-and-conquer criteria, we prove optimality in quadratic time for\nmatchings, and polynomial time for any network class with logarithmically-bound\nlargest connected components. In simulation studies we observe the polynomial\ndependence for sparse networks and that, beyond some critical value, the\nlogarithm of the base grows with the network density. Our approach then\nout-competes the state-of-the-art at lower densities. These results therefore\npave the way for faster exact causal discovery in larger and sparser networks.",
        "updated": "2024-06-21 09:41:30 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.15012v1"
    },
    {
        "title": "Probabilistic and Differentiable Wireless Simulation with Geometric Transformers",
        "authors": "Thomas HehnMarkus PeschlTribhuvanesh OrekondyArash BehboodiJohann Brehmer",
        "links": "http://arxiv.org/abs/2406.14995v1",
        "entry_id": "http://arxiv.org/abs/2406.14995v1",
        "pdf_url": "http://arxiv.org/pdf/2406.14995v1",
        "summary": "Modelling the propagation of electromagnetic signals is critical for\ndesigning modern communication systems. While there are precise simulators\nbased on ray tracing, they do not lend themselves to solving inverse problems\nor the integration in an automated design loop. We propose to address these\nchallenges through differentiable neural surrogates that exploit the geometric\naspects of the problem. We first introduce the Wireless Geometric Algebra\nTransformer (Wi-GATr), a generic backbone architecture for simulating wireless\npropagation in a 3D environment. It uses versatile representations based on\ngeometric algebra and is equivariant with respect to E(3), the symmetry group\nof the underlying physics. Second, we study two algorithmic approaches to\nsignal prediction and inverse problems based on differentiable predictive\nmodelling and diffusion models. We show how these let us predict received\npower, localize receivers, and reconstruct the 3D environment from the received\nsignal. Finally, we introduce two large, geometry-focused datasets of wireless\nsignal propagation in indoor scenes. In experiments, we show that our\ngeometry-forward approach achieves higher-fidelity predictions with less data\nthan various baselines.",
        "updated": "2024-06-21 09:14:11 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.14995v1"
    }
]