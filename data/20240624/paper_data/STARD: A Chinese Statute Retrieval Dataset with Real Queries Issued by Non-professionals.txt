STARD: A Chinese Statute Retrieval Dataset with Real Queries Issued by
Non-professionals
WeihangSu*1,YiranHu†2,AnzheXie3,QingyaoAi‡1,ZibingQue2,NingZheng4,YunLiu2,
WeixingShen2,andYiqunLiu1
1DepartmentofComputerScienceandTechnology,TsinghuaUniversity
2SchoolofLaw,TsinghuaUniversity
3SchoolofElectronicsEngineeringandComputerScience,PekingUniversity
4DepartmentofAutomation,TsinghuaUniversity
Abstract Question:Isdisclosingthemedicalcaseinformationof
apatientconsideredaninvasionofprivacy?
Statuteretrievalaimstofindrelevantstatutory
RelevantStatuteArticles
articlesforspecificqueries. Thisprocessisthe
PersonalInformationProtectionLaw,Article28:Sen-
basisofawiderangeoflegalapplicationssuch sitivepersonalinformationreferstoinformationthat,if
as legal advice, automated judicial decisions, leaked orillegally used, couldeasily harman individ-
legaldocumentdrafting,etc. Existingstatute ual’sdignityorendangertheirpersonalorpropertysafety.
Thisincludesbiometricdata,religiousbeliefs,specific
retrievalbenchmarksfocusonformalandpro-
identities, medical health, financial accounts, tracking
fessionalqueriesfromsourceslikebarexams
information,andpersonalinformationofminorsunder
andlegalcasedocuments,therebyneglecting theageoffourteen.
non-professionalqueriesfromthegeneralpub-
CivilCode,Article1032:Individualshavetherightto
lic,whichoftenlackpreciselegalterminology
privacy.Noorganizationorindividualmayinfringeupon
andreferences. Toaddressthisgap,weintro- another’sprivacyrightsthroughsnooping,harassment,
ducetheSTAtuteRetrievalDataset(STARD),a disclosure, or publicization. Privacy encompasses the
Chinesedatasetcomprising1,543querycases tranquilityofanindividual’sprivatelifeandtheprivate
spaces,activities,andinformationtheywishtokeepun-
collected from real-world legal consultations
knowntoothers.
and55,348candidatestatutoryarticles1.Unlike
existingstatuteretrievaldatasets,whichprimar- CivilCode,Article1226:Medicalinstitutionsandtheir
medicalpersonnelmustkeeppatients’privacyandper-
ilyfocusonprofessionallegalqueries,STARD
sonalinformationconfidential. Thosewhodisclosepa-
captures the complexity and diversity of real
tients’privateandpersonalinformationorpublishtheir
queries from the general public. Through a medicalrecordswithoutthepatient’sconsentmustbear
comprehensiveevaluationofvariousretrieval infringementliability.
baselines,werevealthatexistingretrievalap-
Table1: Anexampleofthequeryandrelevantstatute
proaches all fall short of these real queries
articlesintheSTARDdataset.
issued by non-professional users. The best
methodonlyachievesaRecall@100of0.907, Thisprocessisvitalinthelegalfieldandsupportsa
suggestingthenecessityforfurtherexploration widerangeofapplications,includinglegaladvice
andadditionalresearchinthisarea.
services, automated judicial decisions, and logi-
1 Introduction callegalanalysis. Thistaskischallengingforthe
followingreasons: (1)Statutesusecomplexlegal
Statutesarewrittenlawsformallycreatedandap-
terminologyandlinguisticstructuresrarelyfound
provedbyalegislativebody,suchasaparliament
inopen-domaincorpus. Asaresult,traditionalre-
orcongress(Livingston,1990). Theysetoutspe-
trievalmodelsthatlackdomain-specificknowledge
cific rules and guidelines within a certain area or
maystruggletoaccuratelycapturethemeaningsof
jurisdiction. Therefore, statutes are the primary
thesespecializedterms. (2)Thecriteriaforassess-
sourceoflegalauthorityincivillawcountriesand
inginformationrelevanceinthelegaldomaindif-
alsoplayasignificantroleincommonlawjurisdic-
fergreatlyfromthoseusedinopen-domainsearch
tions.
tasks. Generalsearchtasksfocusmainlyontextual
Statuteretrievalinvolvesfindingrelevantstatu-
similarity,whilelegaltasksinvolvelegalreasoning
toryarticlesorsectionsoflawsforaspecificquery.
thatrequirestheunderstandingofdifferentareasof
*swh22@mails.tsinghua.edu.cn law,therelationsbetweenthem,aswellastherele-
†EqualContribution. vanceofspecificlegalprinciplesandtheirpractical
‡CorrespondingAuthor:aiqy@tsinghua.edu.cn
applications.
1All the codes and datasets are available at:
https://github.com/oneal2000/STARD/tree/main Duetothechallengingnatureofstatuteretrieval
4202
nuJ
12
]RI.sc[
1v31351.6042:viXraanditsparamountimportanceincivillawsystems, edgesourceforRetrieval-AugmentedGeneration
significantprogresshasbeenmadeinthisfield. For (RAG)significantlyenhancestheperformanceof
example,theannualCOLIEEcompetitionsintro- large generative language models (LLMs) on le-
duce a series of statute retrieval tasks using the galtasks. ThisindicatesthatSTARDisusefulfor
questions extracted from the Japanese legal bar developingmoreaccessibleandefficientlegalsys-
exams(Goebeletal.,2023;Kimetal.,2022;Ra- tems.
beloetal.,2022). Thesetasksaimtoretrieverel- Inconclusion,thecontributionsofthispaperare
evant statute law from the Japanese Civil Code asfollows:
Articleaccordingtothequestionfrombarexams.
AILA(Bhattacharyaetal.,2019)competitionsalso • WeproposeSTARD,astatuteretrievaldatasetde-
introduceaseriesofstatuteretrievaldatasets. The rivedfromreal-worldlegalconsultationposedby
queriesfromAILAarecasedocumentsthatwere non-professionals,with1,543queriesandtheir
judged by the Supreme Court of India. The can- correspondingrelevantstatutes.
didate statutes are part of the set of statutes from
Indianlaw. • Weproposeacomprehensiveannotationframe-
Despite these advancements, a significant gap workspecificallydesignedforthestatuteretrieval
persists in addressing real queries from non- task based on non-professional queries, which
professional people, who represent a large pop- providesreferencesandinsightsforfutureanno-
ulationoflegaladviceserviceusers. Thecurrent tationinthelegalfield.
statuteretrievalbenchmarksareprimarilybasedon
queriesfromformallegaldocuments,suchasbar • Weconductexperimentsonawiderangeofre-
examquestionsorSupremeCourtcasedocuments, trieval baselines and find that statute retrieval
which differ significantly from the everyday lan- withqueriesissuedbynon-professionalsisstilla
guageusedbythegeneralpublic. However,queries difficulttaskthatrequiresfurtherinvestigation.
fromnon-professionalsoftenlackpreciselegalter-
minologyandmayincludeambiguousreferences • WepresentexperimentsonLLMssolvinglegal
to legal concepts, which significantly complicate taskswithandwithouttheSTARDdataset. Ex-
thetaskofstatuteretrieval. perimentsshowthatSTARDcannotablyenhance
To address the limitations of existing bench- theperformanceofLLMsinlegaltasks.
marks, we propose STAtute Retrieval Dataset
(STARD)i.e. STARD,aChinesestatuteretrieval 2 ProblemFormulation
datasetbasedonreallegalconsultationquestions
2.1 StatuteofCivilLawSystem
fromthegeneralpublic. TheSTARDdatasetcom-
prises1,543querycasescollectedfromreal-world Civillawisalegalsystemprimarilybasedoncodi-
legalconsultationsand55,348candidatestatutory fiedlawsratherthancaseprecedents,makingwrit-
articlesextractedfromallofficialChineselegalreg- tenstatutesthemainsourceoflegalauthority. This
ulationsandjudicialinterpretations. Table1shows contrastswithcommonlawsystems,whereprevi-
anexampleofourdataset. Tothebestofourknowl- ous judicial decisions also play a central role. In
edge, STARD is the first statute retrieval dataset civillaw,statutesarecreatedandenactedbylegisla-
wherequeriesarefromreal-worldlegalconsulting tivebodies,suchasparliaments,andareorganized
proposedbythegeneralpublic. intosystematiccollectionsknownascodes,which
Weconductexperimentsonawiderangeofin- covervariousareasoflawlikecontracts,torts,and
formation retrieval (IR) baselines on the STARD property. Astatuteisaformalwrittenlawthatpro-
dataset,includingtraditionallexicalmatchingmod- videsspecificrulesandguidelinestobefollowed
els,open-domainneuralretrievalmodels,legaldo- withinajurisdiction. Withinstatutes,therearesec-
mainneuralretrievalmodels,andadenseretriever tionsknownasstatutoryarticles,whichdetailindi-
trainedwithdataannotatedbyGPT-4. Theexperi- vidualprovisionsorclausesofthelaw,addressing
mentalresultsshowthatallexistingbaselinesfall particularaspectsorrequirements. Thesestatutes
shortofaccuratelyandcomprehensivelyretrieving andtheirarticlesarefundamentalincivillawsys-
therelevantstatutes, leavingsignificantroomfor tems to ensure that the legal framework is clear,
futurework. Additionally,ourexperimentalresults predictable,andaccessible,therebyfacilitatingor-
showthatemployingSTARDasanexternalknowl- deranddefiningsocietalrightsandresponsibilities.Step1:Recall Step2:Query Decomposition Step3:Filter
Queries from General Public
CivilLaw-Tort Liability
What should l doif l am What should l doif l am …
bitten by someoneelse's pet? bitten by someoneelse's pet? Article1247:If damage is caused to others by dangerous animals
such as dogs that are prohibited from being kept, the keeper or
Life issues manager of the animal should bear the tort liability.
Criminal Law Marriage Causing Domestic Article1245:If damage is caused by a domesticated animal, the
Legal facts keeper or manager of the animal should bear the tort liability.
Civil Law … Damage Animals However, if it can be proven that the damage was caused by ...
Procedure Law Tort Liability Article1249:If damage is caused to others by an animal that has
been abandoned or has escaped during the period of abandonment or
… escape the original keeper or manager...
…
Fields of Law Subfields of Law
Figure1: Aschematicofourannotationframeworkwithgreenboxeshighlightingquery-relevantelements.
2.2 DefinitionofStatuteRetrieval broaddepartmentalcategoriesoflaw,suchascivil,
criminallaw,andadministrativelaw. Uponencoun-
Thestatuteretrievaltaskaimstoaccuratelyretrieve
tering a specific issue, annotators first determine
relevantstatutoryarticlesinresponsetoaquery. To
whichcategoryofdepartmentallawitfallsunder,
bespecific, givenaqueryq thatdescribesalegal
thenprogressivelyrefinetheissuetomorespecific
issueorsituationandacorpusofstatutoryarticles
aspectsofthelaw. Forinstance,iftheissuepertains
S = {s ,s ,...,s }, n ∈ N+. For each statute
1 2 n
tocivillaw,theannotatorassesseswhetheritrelates
s in the corpus, there is a Bernoulli variable r
i i
tocontractlawortortlaw. Ifitisamatterofcon-
indicating whether s is relevant2 to the query q.
i
tractlaw,afurtherdeterminationismaderegarding
Thegoalofthestatuteretrievaltaskistoretrieve
thespecifictypeofcontractinvolved. Similarly,for
a set of statutes R = {s |r = 1}, including all
j j
tortlaw,thespecifictypeoftortisidentified. This
statutesrelevanttothequery.
stepeffectivelynarrowsthescopeoflegalstatute
retrievaltoparticularchapterswithintherelevant
3 AnnotationFramework
departmentallaw.
This section explains how annotators transform
generalquestionsintoprofessionallegalquestions 3.2 Step2: QueryDecomposition
submittedbynon-professionalsandthenidentify
Given the specialized nature of legal knowledge,
the most relevant legal statutes to support these
individualswithoutaformaleducationinlawoften
questions. To be specific, annotators use a three-
frametheirquerieswithinformallanguagerather
stepmethod: recall,querydecomposition,andfil-
thanprofessionallegalterminology. Thesequeries
tering(illustratedinFigure1). Thismethodmirrors
typically consist of straightforward semantic ex-
the structured approach commonly used in legal
pressionsthatdonotdirectlycorrespondtoestab-
reasoning, which involves three logical steps: es-
lishedlegalnorms. Forinstance,considertheques-
tablishingabroadlegalprinciple(majorpremise),
tion “What should I do if I am bitten by some-
applying it to the specific facts of a case (minor
one’spet?”. Here,“petbite”representsacommon,
premise),andthenconcluding. Thissectionisor-
non-technical description of an incident. Search-
ganizedintothreesubsections,eachdetailingapart
ing for legal norms based solely on such descrip-
oftheannotationprocessthatisdesignedtomirror
tionsmightleadtoirrelevantorimpreciseresults.
theselogicalstepsinlegalreasoning.
Therefore, when annotators perform legal statute
retrieval, they should transform the informal fact
3.1 Step1: Recall
descriptions written by the questioner into legal
Wheninitiatingtheannotationoflegalstatutesper-
factsthroughinterpretation. Thisisthesteptofind
tinenttoaquery,ourannotatorsfirstnarrowdown
theminorpremiseinthelegallogicsyllogism. In
thescopeoftherelevantstatutes. Specifically,they
thistransformationprocess,theannotatorevaluates
startbyidentifyingthemostpertinentareasoflaw
thelifefactsaccordingtotheprovisionsofthelaw
withintheentirelegalsystem. Theprocessusesa
andselectsthelegalnormscorrespondingtothese
top-downrefiningmethod. Annotatorsbeginwith
life facts. For example, for the aforementioned
issue of a pet biting a person, the annotators will
2Thedefinitionof“relevant”isdiscussedindetailinSec-
tion3. transform“petbitesaperson”intothelegalfactof“causingdamagetoother”and“domesticanimals” oflocallawsmustbeconsidered,makingstraight-
accordingtotheprovisionsofChapter9oftheTort forward translation inadequate for cross-national
LiabilityCompilationoftheCivilCode. applications. For RQ2, our proposed Three-Step
AnnotationFrameworkispotentiallygeneralizable
3.3 Step3: Filter
toothercountriesunderthecivillawsystem. Coun-
Thefilteringprocessisacriticalstepwhereanno- tries with civil law systems, such as Germany,
tators refine and finalize the selection of relevant France, and Japan, typically share a similar pro-
legal statutes. This is accomplished by employ- cess for retrieving law statutes. This process can
inga“subsumption”method,integraltothesyllo- generally be structured into three steps: Recall,
gistic reasoning in law. In this method, the legal Query Decomposition, and Filtering. Therefore,
facts,whichhavebeeninterpretedandtransformed ourframeworkcouldbeadaptedtotheseenviron-
from real-life scenarios in the previous steps, are ments, supporting the construction of statute re-
matchedagainstthesmallestpossiblesubsetofle- trievaldatasetsandtheapplicationoflegalstatutes
galstatutesthatadequatelyaddressthequery. acrossvariouscivillawjurisdictions.
To be specific, consider a set of legal statutes
S = {S ,S ,S } recalled in the first step. 4 DatasetConstruction
1 2 3
Through the transformation process, the query is
4.1 DataSources
deconstructed into distinct legal facts F ,F ,F .
1 2 3
Each fact corresponds to a subset of statutes that All queries in our dataset derive from real legal
itimplies,denotedasS = {S ,S ,S },S = consultations. Specifically,ourlegalteamcreates
F1 1 4 5 F2
{S ,S }, and S = {S ,S }. The objective in legalquestionsfromthe12348ChinaLegalService
1 5 F3 3 6
thefilteringstageistointersectthesesubsetswith
Website3,followedbyamanualanonymizationof
the initially recalled set S to determine the most eachquestion,whichinvolvedremovinganypoten-
relevantstatutes. ThisisrepresentedasS = tialidentifiersassociatedwithentities,corporations,
Golden
(S ∪S ∪S )∩S,yieldingS = {S ,S }. orindividuals.
F1 F2 F3 Golden 1 3
These statutes in S are considered the Toobtainthe55,348candidatestatutoryarticles,
Golden
“golden”legalstatutesforthedataset, astheyen- our legal team conducted extensive research and
compassallthelegalimplicationsdrawnfromthe discussionstocompileacomprehensivelistofcur-
facts of the query. This step ensures that the se-
rentlyvalidChinesestatutorylawsandregulations4.
lectedstatutesarenotonlyrelevantbutalsocom- Wethenmanuallydownloadedthemostup-to-date
prehensiveincoveringthelegalissuespresentedin versionsoftheselawsfromthegovernment’soffi-
thequery,therebyprovidingasolidlegalfounda- cialwebsite. Theselawsweresubsequentlydivided
tiontosupporttheresolutionofthequery. intothesmallestsearchableunitsbasedonarticles
usingautomatedscripts.
3.4 GeneralizabilityofOurFramework
4.2 RecruitmentandPaymentofAnnotators
In this section, we discuss the generalizability of
theSTARDdatasetandourannotationframework, Forrecruitment,wesourcedannotatorsfrompromi-
discussing the following two research questions nent law schools5. The annotation team initially
(RQs): consistedof16members. Althoughthreemembers
departed during the project, their positions were
• RQ1: CantheSTARDdatasetbeappliedtothe
quicklyfilledtomaintaintheteamsize. Oursalary
legal systems of other countries through direct
planremuneratesparticipantsbasedonthenumber
translationofourdataset?
ofannotationstheycomplete,withafixedrateof
approximately10CNYperannotation. Onaverage,
• RQ2: Can our Three-Step Annotation Frame-
annotatorsprocessedfourqueriesperhour,result-
workbeappliedtootherlegalsystems?
ing in an average hourly wage of 40 CNY. This
ForRQ1,directlytranslatingtheSTARDdataset
3ThisistheChinesegovernment’sofficialwebsiteforon-
intootherlanguagesdoesnotguaranteeitsapplica-
linelegalservices:http://www.12348.gov.cn/
bilityinforeignlegalsystems. Eachcountrypos- 4Theentirelistofstatutesweselectedcanbefoundonour
sessesuniquelegalstatutes;articlesselectedfrom officialGitHub.
5Manyexistinglegaldatasetschooselawschoolstudents
onejurisdictionmaynotexistormayhaveentirely
asannotators(Lietal.,2023e;Zongyueetal.,2023;Liuetal.,
differentimplicationsinanother. Thus,thenuances 2023).payratesignificantlyexceedstheminimumhourly andexpanduponourwork,advancingthefield
wagemandatedinBeijing. oflegalinformatics.
4.3 AnnotationProcess • (3) Accountability: Recognizing the dynamic
Annotatorsaretaskedwithidentifyingrelevantar- natureoflegalstatutes,wecommittoregularly
ticlesofstatutesinresponsetoactuallegalqueries updatingtheSTARDdatasettoreflectthelatest
posed by the general public. The specifics of changesinlaw. Thisensuresthedatasetremains
the annotation framework are detailed in Section accurate and reliable for ongoing research and
3. Additionally, annotators are instructed not to application.
use generative models, such as ChatGPT, for as-
• (4)Accessibility: TheSTARDdatasetisfreely
sistance. The annotation process starts with the
availablefordownloadfromtheofficialwebsite
manualanonymizationofeachquestionwithinthe
under the MIT license, facilitating easy access
STARDdataset,involvingtheremovalofanypo-
forresearchersandpractitionersalike. Thispro-
tentialidentifiersassociatedwithentities,corpora-
motes broader usage and supports innovation
tions,orindividuals. Subsequently,annotatorsare
acrossvariousfields.
required to locate relevant statutes for each ques-
tion,followingthethree-stepprincipleintroduced
5 DatasetStatisticsandAnalysis
inSection3.
The basic statistics of our proposed dataset are
4.4 AnnotationConsistency
shown in Table 2. STARD comprises a total of
For each question, two annotators were assigned.
1,543 queries and a large-scale corpus of 55,348
The final gold standard for each question was
candidate statutory articles. Among these candi-
established only when both annotators agreed
datestatutoryarticles,1,445articlesarerelevantto
onthesamelegalprovisions6.
atleastoneofthequeriesinthedataset. Theaver-
Toevaluatethereliabilityofagreementamong
agelengthofaqueryis27.3words,andtheaverage
humanannotators,weutilizedCohen’sKappa(Co-
lengthofastatutearticleisnearly120words.
hen,1960)Kcoefficientinabinaryclassification
Figure 2 presents the distribution of queries
context. Eachquery-statutearticlepaircorresponds
across the number of relevant statutory articles,
to a binary classification task, where annotators
highlighting the varied complexity within the
judgewhetherthequeryisrelatedorunrelatedto
dataset. A substantial majority of the queries,
thestatute. Thisanalysis,conductedonadataset
843 out of 1,543, correspond to just one relevant
comprising 1,543 annotated instances, yielded a
statutoryarticle,indicatingasignificantnumberof
Kvalueof0.5312. Thisindicatesmoderateagree-
queriescanbeaddressedwithasingle,specificle-
ment. Achieving such a K value is considered
galreference. Thiscouldsuggestthatmanyofthe
satisfactoryforacomplextaskinvolvingfiftythou-
non-professional queries are focused and pertain
sandclassificationswithmultiplepossiblecorrect
tospecificlegalissuesthatrequirestraightforward
labels.
statuteretrieval. However,45%ofqueriesrequire
multiplestatutoryarticleswhichindicatessomeof
4.5 EthicsDiscussion
thequestionsaremorecomplex,involvingmultiple
Wehavethoroughlyaddressedthefollowingethical
references of law. This diversity in query com-
considerations:
plexitydemonstratesthatourdatasetiscapableof
accommodating a wide range of legal questions,
• (1)PrivacyandAnonymity: Giventhesensitive
fromstraightforwardtohighlyintricate.
natureoflegalconsultations,wehaverigorously
anonymizedallqueriesintheSTARDdataset.
6 StatuteRetrievalExperiment
• (2) Transparency: To promote reproducibility
6.1 SelectedRetrievalBaselines
andtransparency,wehavemadethedataset,asso-
ciatedmodels,andthecodespubliclyavailable7. We consider four types of baselines for compari-
Thisallowsotherresearcherstoverify,replicate, son,includingtraditionalIRmethods,pre-trained
Languagemodelsongeneraldomaindata,PLMs
6In cases where annotators had differing opinions, the
tailored for IR, and pre-trained language models
questionwouldnotbeincludedinthefinaldataset.
7https://github.com/oneal2000/STARD/tree/main builtwithlegaldocuments. TheimplementationTable2:BasicstatisticsofourproposedSTARDdataset. – SAILER(Lietal.,2023a)isastructure-aware
pre-trainedlanguagemodelfortailoredlegal
Statistic #Number
documentrepresentation. Itutilizesthelogical
TotalQueries 1,543
connectionsbetweendifferentsectionswithin
TotalCandidateStatutoryArticles 55,348
alegaldocument.
TotalNumofRelevantStatutoryArticles 1,445
OccurrencesofRelevantArticles 2,717
Avg.RelevantArticlesperQuery 1.76
• Fine-tunedDenseRetrievalModel
Avg.QueryLength 27.30
Avg.ArticleLength 119.93
– Dense-CAILisadenseretrievalmodeltrained
on the CAIL2018 dataset (Xiao et al., 2018).
800 Wechoosethisbaselinetoverifywhetherthe
existingdatasetbasedonformalprofessional
600
questionsissufficientforaddressingstatutere-
400 trievaltasksbasedonnon-professionalqueries.
– Dense-STARD employs a five-fold cross-
200
validationtechniqueontheSTARDdataset.
0
1 2 3 4 5 5+
WeinitializetheabovetwomodelswithChinese-
Number of Relevant Statutory Articles
Roberta-WWM(Cuietal.,2021). Forthesetting
Figure2: Distributionofrelevantstatutoryarticlenum-
of cross-validation, the dataset is randomly di-
bersforeachquery.
videdintofivesubsets,whereonesubsetserves
as the test set and the remaining four are used
details of these baselines are provided in Ap- as training sets. The details of our fine-tuning
pendixB processareintroducedinAppendixG.
• TraditionalIRMethods • Dense-GPT4: Wedistilladenseretrievalmodel
fromGPT-4. TheprocessinvolvedusingGPT-4
– QL (Ponte and Croft, 2017) is a language
to generate legal questions based on statute ar-
modelbasedonDirichletsmoothingandhas
ticles within a given corpus. Specifically, we
goodperformanceonretrievaltasks.
promptedGPT-4tocreatealegalquestionq that
– BM25(Robertsonetal.,2009)isaneffective iscloselyrelatedtoaspecificstatutearticlea+,
i
retrievalmodelbasedonlexicalmatchingthat resulting in a query-statute pair (q,a+). Then,
i
achievesgoodperformanceinretrievaltasks.
we employ a contrastive learning approach uti-
lizingthesequery-statutepairstotrainthedense
• GeneralDomainPre-trainedModels
retriever. DetailsareprovidedinAppendixH.
– Chinese-RoBERTa-WWM(Cuietal.,2021)
is a language model pre-trained with the • LSI-STARD is a Transformer based classifier
WholeWordMaskingstrategy. fine-tunedonSTARD.IntheLegalStatuteIdenti-
fication(LSI)field(Zhongetal.,2018;Pauletal.,
– SEED (Lu et al., 2021) is a pre-trained text
2022;Chalkidisetal.,2021),thestatuteretrieval
encoderfordenseretrievalthatachievesstate-
task is approached as a classification problem,
of-the-artperformance.
where each statute is treated as a unique label.
– coCondenser (Gao and Callan, 2021b) is
Thismethodtransformsthetaskintoclassifying
an enhanced version of Condenser(Gao and
legaldocumentsorqueriesagainstasetoflabels,
Callan, 2021a) that adds an unsupervised
each representing a different statute. Follow-
corpus-level contrastive loss to warm up the
ingthismethodology,wefinetuneatransformer-
passageembeddingspace.
basedclassificationmodelontheSTARDdataset,
employingthesamefive-foldcross-validationset-
• LegalDomainPre-trainedModels
ting. Weinitializethetransformer-basedmodel
– Lawformer (Xiao et al., 2021) apply Long- withChinese-Roberta-WWM(Cuietal.,2021)
former(Beltagy et al., 2020) to initialize and andrandomlyinitializetheoutermostMLPLayer.
trainwiththeMLMtaskonthelegaldomain. DetailsareprovidedinAppendixI.
seireuQ
fo
rebmuNTable3: TheoverallexperimentalresultsofmultiplebaselinesonSTARD.Thebestresultsareinbold,andthe
second-bestresultsareunderlined. “R”standsforRecall,and“M”standsforMRR.GeneralPLMandLegalPLM
areallinthezero-shotsetting. NotethatLSI-STARDisaclassificationmodelwhereeachstatuteistreatedasa
uniquelabel;wereportitsrankingperformancebasedontheprobabilityforeachstatute.
R@5 R@10 R@20 R@30 R@50 R@100 R@200 M@3 M@5 M@10
QL 0.3363 0.4020 0.4651 0.4839 0.5537 0.6515 0.7224 0.3052 0.3167 0.3304
LexicalMatching
BM25 0.3349 0.3943 0.4504 0.4773 0.5240 0.6493 0.7035 0.3176 0.3251 0.3369
Roberta 0.3216 0.3908 0.4646 0.5042 0.5715 0.6633 0.7351 0.2766 0.2905 0.3010
Open-DomainPLM SEED 0.2897 0.3555 0.4264 0.4589 0.4975 0.5626 0.6260 0.2607 0.2708 0.2816
coCondenser 0.1120 0.1598 0.2223 0.2659 0.3288 0.4292 0.5246 0.0847 0.0922 0.1004
SAILER 0.2330 0.3050 0.3790 0.4286 0.4885 0.5674 0.6463 0.2006 0.2115 0.2234
LegalPLM
Lawformer 0.2411 0.2989 0.3720 0.4137 0.4733 0.5478 0.6309 0.2205 0.2313 0.2412
Dense-STARD 0.5206 0.6061 0.7064 0.7485 0.8107 0.9065 0.9531 0.4372 0.4543 0.4724
Dense-GPT4 0.4382 0.5174 0.5961 0.6471 0.6810 0.7984 0.8521 0.3842 0.3948 0.4106
Fine-tunedPLM
Dense-CAIL 0.0887 0.1272 0.1832 0.2341 0.2712 0.3281 0.3819 0.0660 0.0719 0.0842
LSI-STARD 0.1861 0.2069 0.2386 0.2564 0.3004 0.3410 0.3956 0.2062 0.2093 0.2156
6.2 EvaluationMetrics suggests significant differences between the non-
professionalqueriesinSTARDandtheformalle-
WeuseMeanReciprocalRankandRecallaseval-
gal queries in existing datasets. Consequently, it
uation metrics. By using both MRR and Recall,
underscores the unique nature of STARD, neces-
wecangaininsightsintoboththeaccuracyofthe
sitatingspecializedmodelsforeffectivestatutere-
top-ranked results and the comprehensiveness of
trieval. (5)WhiletheLSIclassifierperformswell
the relevant statutory articles retrieved by the re-
in existing studies for tasks involving the classi-
trievalmodel. Detaileddefinitionsofthesemetrics
fication of a few dozen statutes, it struggles with
areprovidedinAppendixC.
the STARD dataset, which contains over 50,000
6.3 ExperimentalResults labels,resultinginsuboptimalperformance. Asa
result,retrievalmethodsaremoreeffectivethanthe
Inthissubsection,weprovideadetailedanalysis
LSIapproachforlarge-scalestatuteretrievaltasks.
of the performance of various retrieval baselines
(6)Theperformanceofboththelexicalmatching
evaluated on our proposed STARD dataset. We
methodandthenon-finetunedmodelsislesseffec-
have the following insights into the effectiveness
tive than that of the Dense-STARD model. This
ofdifferentretrievalmethods:
arises because the former models lack the capac-
(1)Underthezero-shotsetting,traditionallexi-
itytointerpretlifeissuesaslegalfacts,acapabil-
calmatchingtechniquessurpassbothgeneraland
itythatDense-STARDhasacquiredthroughfine-
legal-domainpre-trainedlanguagemodels(PLMs).
tuning. It has been trained to associate the life
Thisdemonstratesthatlexicalmatchingmethods
issuespresentedinquerieswithrelevantlegalarti-
arestillverystrongbaselinesinretrievaltasks. (2)
cles. However,Dense-STARD’strainingsetiscon-
Amongallthemethodsthatdonotusehumananno-
finedtojustoveronethousandquery-articlepairs.
tation,theperformanceofDense-GPT4standsout,
Consequently, its recall rates remain suboptimal,
exceedingthatofallunsupervisedmethodstested.
withRecall@100atonly90.65%. Thesefindings
This indicates that distilling GPT4 to train task-
underscorethenecessityforfurtherexplorationin
specificmodelsisagoodchoiceinscenarioswith-
thisfield.
outhumanannotations. (3)Domain-specificmod-
elslikeSAILERareoptimizedforparticulartasks,
7 RetrievalAugmentedGeneration
thus resulting in underperformance compared to
Experiment
generaldomainmodels. Specifically, SAILER is
tailoredforlegalcaseretrievalinvolvinglongdoc- 7.1 SelectedBenchmark
umentsasqueries. Consequently,itstruggleswith
We select two datasets encompassing three tasks
tasksthatinvolveshortqueriesandmedium-length
forourRAGexperiment:
articles,unlikethemodelSTARD.(4)Theretrieval
model fine-tuned on the CAIL2018 dataset per- • JecQA (Zhong et al., 2020) is the most ex-
formedsub-optimallyontheSTARDdataset. This tensive multiple-choice dataset within the Chi-Table4:TheoverallexperimentalresultsofthreeLLMs the value of our proposed dataset in improving
on the JecQA benchmark. We report accuracy as the
theeffectivenessofLLMsonlegaltasks. There-
evaluationmetric. Thebestresultsareinboldandthe
sultsalsorevealthatdifferentLLMshaveunique
secondbestresultsareunderlined.
preferences for retrievers. For the Baichuan and
ChatGLMmodels,afine-tuneddenseretrieversur-
Retriever JQA-CA JQA-KD CAIL
passesBM25,indicatingthatthesemodelsbenefit
w/oRAG 0.231 0.266 0.850
Baichuan BM25 0.233 0.288 0.766 fromdenseretrievers’highrecallrates. However,
Dense-STARD 0.238 0.291 0.816 thisadvantageisnotobservedwiththeChatGPT
w/oRAG 0.185 0.194 0.636 model, where BM25 outperforms the fine-tuned
chatGLM BM25 0.189 0.224 0.646 denseretriever. Thissuggeststhattheperformance
Dense-STARD 0.200 0.237 0.684
ofRAGishighlydependentonthepreferencesof
w/oRAG 0.187 0.206 0.496
theLLMregardingtheretriever. Theexperimental
chatGPT BM25 0.233 0.293 0.528
Dense-STARD 0.193 0.252 0.503 resultsontheCAIL2018datasetalignwiththose
observed for JecQA, with one notable exception:
the performance of the Baichuan model without
nese legal field. This dataset includes two dis- RAG. In this setting, Baichuan’s performance is
tinct tasks: Knowledge-Driven Questions (KD- markedly superior to that of chatGLM, chatGPT,
questions) and Case-Analysis Questions (CA- andBaichuanwithRAG.Wehypothesizethatthis
questions),encompassingatotalof26,365ques- exception arises from the Baichuan model’s uti-
tions. Allthequestionsaremulti-select,meaning lization of the CAIL 2018 dataset during its pre-
thatmorethanoneoptioncanbecorrect. trainingphase,leadingtoadirectansweraccuracy
ratethatiseven81%higherthanthatofchatGPT.
• CAIL 2018 (Xiao et al., 2018) is a large-scale
Chineselegaldatasetdesignedforjudgmentpre-
8 RelatedWork
dictionwithover2.6millioncriminalcases. This
datasetcontainsdetailedannotationsofjudgment
CAIL2018(Xiaoetal.,2018;Zhongetal.,2018)
results,includingapplicablelawarticles,specific
competitionsconductlawstatuteretrievalworkus-
charges,andprescribedprisonterms. Weselect
ingformallegaljudgmentdocuments. Thequeries
theChargePredictiontaskofCAIL2018anduse
in the dataset originate from the “Court’s Find-
predictionAccuracyastheevaluationmetric.
ings” part of the judgments, and the candidates
arestatutearticlesofChineseCriminalLaw. The
7.2 SelectedLLMsandSettings
annual COLIEE competitions introduce a series
OurselectedLLMsareintroducedinAppendixD. ofstatuteretrievaldatasetsusingthequestionsex-
The generation configuration is detailed in Ap- tractedfromtheJapaneselegalbarexams(Goebel
pendix E. The prompt template for LLMs is de- etal.,2023;Kimetal.,2022;Rabeloetal.,2022;Li
tailedinAppendixF. etal.,2023c,d). Thesetasksaimtoretrieverelevant
statutelawfromtheJapaneseCivilCodeArticleac-
7.3 ExperimentalResults
cordingtoquestionsfrombarexams. AILA(Bhat-
Table 4 presents the results of the LLM’s per- tacharyaetal.,2019)competitionsalsointroduce
formance with and without the use of Retrieval- a series of statute retrieval datasets. The queries
Augmented Generation (RAG). In the scenario fromAILAarelegaljudgmentdocumentsfromthe
withoutRAG,theLLMdirectlyoutputsthecorrect SupremeCourtofIndia. Thecandidatestatutesare
optionsbasedonthequestion. IntheRAGscenario, part of the set of statute articles from Indian law.
theretrievalmodel(BM25orDense-STARD)re- BSARD(LouisandSpanakis,2021)isastatutory
callsthetop10relevantstatutoryarticlesfromthe article retrieval dataset in French with candidate
corpusbasedonthequestion. Theretrievedstatu- articlesfroma22,600+Belgianlawarticlescorpus.
toryarticlesarethenintegratedintoameticulously In the studies of the Legal Statute Identifi-
designedprompttemplate(detailedinAppendixF). cation (LSI) (Zhong et al., 2018; Paul et al.,
The experimental results reveal that using the 2022; Chalkidis et al., 2021), finding the rele-
STARDcorpusastheexternalknowledgebasefor vantstatuteisapproachedasaclassificationprob-
the RAG significantly enhances the performance lem, where each statute is treated as a unique la-
oflargelanguagemodels(LLMs)andunderscores bel. LADAN (Xu et al., 2020) is an LSI methodthat uses a graph neural network and attention 11 EthicsStatement
mechanism to distinguish confusing law articles.
Intheframeworkofthisresearch,ethicalconsider-
LeSICiN (Paul et al., 2022) utilizes both textual
ationshavebeenparamountfromtheinitialstages,
contentandlegalcitationnetworkstoidentifyrele-
underscoring our commitment to the responsible
vantlegalstatutes.
advancement and application of artificial intelli-
Legal QA tasks also aim to fulfill the public’s
gence technologies. Our adherence to the princi-
demand for legal information (Do et al., 2017).
plesofopenresearchandthecriticalimportanceof
LLeQA (Louis et al., 2024) is a French long-
reproducibilityhavecompelledustomakeallas-
form legal QA dataset comprising 1,868 expert-
sociatedmodels,datasets,andcodebasespublicly
annotatedlegalquestions. GerLayQA(Büttnerand
availableonGitHub.
Habernal, 2024) is a question-answering dataset
Moreover,inthedevelopmentofourdataset,we
comprising 21k laymen’s legal questions paired
have paid scrupulous attention to privacy and re-
with answers from lawyers and grounded in con-
spectforindividuals’rights. Giventheinherently
cretelawbookparagraphs.
sensitivenatureoflegalconsultations,wehavedili-
gentlyanonymizedeveryquerywithintheSTARD
9 Conclusion
dataset. Thisprocessinvolvedtheremovalofany
potentialidentifiersrelatedtoentities,corporations,
WepresentSTARD,anewbenchmarkconsisting
or individuals, thereby safeguarding privacy and
of 1,543 questions from the general public. To
preemptingthepossibilityofdatamisuse.
thebestofourknowledge,STARDisthefirstChi-
nesestatutesretrievaldatasettailoredforthegen-
eral public. Moreover, we propose an annotation
References
frameworktoimprovetheaccuracyandrelevance
of statute retrieval annotation, which offers valu- Qingyao Ai, Keping Bi, Jiafeng Guo, and W Bruce
Croft.2018. Learningadeeplistwisecontextmodel
able guidelines for future legal annotations. Our
for ranking refinement. In The 41st international
experimentsacrossvariousretrievalmodelshigh-
ACMSIGIRconferenceonresearch&development
lightedthecomplexitiesofnon-professionalstatute ininformationretrieval,pages135–144.
retrieval,indicatingthenecessityforfurtherexplo-
IzBeltagy,MatthewEPeters,andArmanCohan.2020.
ration. Additionally, we demonstrated that inte-
Longformer: Thelong-documenttransformer. arXiv
gratingtheSTARDdatasetsignificantlybooststhe
preprintarXiv:2004.05150.
performance of LLMs in legal tasks, showcasing
itspotentialtoenhancelegalAIapplications. Paheli Bhattacharya, Kripabandhu Ghosh, Saptarshi
Ghosh, Arindam Pal, Parth Mehta, Arnab Bhat-
tacharya,andPrasenjitMajumder.2019. Fire2019
10 Limitations ailatrack: Artificialintelligenceforlegalassistance.
InProceedingsofthe11thannualmeetingofthefo-
Weacknowledgethelimitationsofthispaper. One rumforinformationretrievalevaluation,pages4–6.
of the primary limitations is that our dataset is
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
specificallydesignedaroundtheChineselegalsys-
Subbiah,JaredDKaplan,PrafullaDhariwal,Arvind
tem, inherently limiting its direct applicability to Neelakantan,PranavShyam,GirishSastry,Amanda
legalsystemsoutsideofthiscontext. Despiteour Askell,etal.2020. Languagemodelsarefew-shot
discussionsonpotentialmethodologiesforadapt- learners. Advancesinneuralinformationprocessing
systems,33:1877–1901.
ing STARD to other civil law systems, such an
expansionnecessitatescreatingandannotatingnew MariusBüttnerandIvanHabernal.2024. Answering
datasets tailored to those systems’ distinct legal legalquestionsfromlaymeningermancivillawsys-
frameworks and statutes. Thus, our future work tem. InProceedingsofthe18thConferenceofthe
EuropeanChapteroftheAssociationforComputa-
willbededicatedtodevelopingadditionaldatasets
tional Linguistics (Volume 1: Long Papers), pages
thatencompassabroaderrangeofcivillawsystems.
2015–2027.
Thisendeavoraimstoextendtheutilityofourwork
andfosterfurtherresearchanddevelopmentinthe ZheCao,TaoQin,Tie-YanLiu,Ming-FengTsai,and
Hang Li. 2007. Learning to rank: from pairwise
domainoflegalstatuteretrieval,ensuringbroader
approachtolistwiseapproach. InProceedingsofthe
applicability and relevance across different legal
24thinternationalconferenceonMachinelearning,
landscapes. pages129–136.Ilias Chalkidis, Abhik Jana, Dirk Hartung, Michael VladimirKarpukhin,BarlasOg˘uz,SewonMin,Patrick
Bommarito, Ion Androutsopoulos, Daniel Martin Lewis,LedellWu,SergeyEdunov,DanqiChen,and
Katz,andNikolaosAletras.2021. Lexglue:Abench- Wen-tau Yih. 2020. Dense passage retrieval for
markdatasetforlegallanguageunderstandinginen- open-domain question answering. arXiv preprint
glish. arXivpreprintarXiv:2110.00976. arXiv:2004.04906.
Jia Chen, Haitao Li, Weihang Su, Qingyao Ai, and Mi-YoungKim,JulianoRabelo,RandyGoebel,Masa-
Yiqun Liu. 2023. Thuir at wsdm cup 2023 task haru Yoshioka, Yoshinobu Kano, and Ken Satoh.
1: Unbiased learning to rank. arXiv preprint 2022. Coliee2022summary: Methodsforlegaldoc-
arXiv:2304.12650. umentretrievalandentailment. InJSAIInternational
Symposium on Artificial Intelligence, pages 51–67.
XuesongChen, ZiyiYe, XiaohuiXie, YiqunLiu, Xi- Springer.
aorongGao,WeihangSu,ShuqiZhu,YikeSun,Min
Zhang,andShaopingMa.2022. Websearchviaan PatrickLewis,EthanPerez,AleksandraPiktus,Fabio
efficient and effective brain-machine interface. In Petroni,VladimirKarpukhin,NamanGoyal,Hein-
ProceedingsoftheFifteenthACMInternationalCon- richKüttler, MikeLewis, Wen-tauYih, TimRock-
ferenceonWebSearchandDataMining,pages1569– täschel,etal.2020. Retrieval-augmentedgeneration
1572. forknowledge-intensivenlptasks. AdvancesinNeu-
ralInformationProcessingSystems,33:9459–9474.
Jacob Cohen. 1960. A coefficient of agreement for
nominalscales. Educationalandpsychologicalmea- HaitaoLi,QingyaoAi,JiaChen,QianDong,Yueyue
surement,20(1):37–46. Wu, Yiqun Liu, Chong Chen, and Qi Tian. 2023a.
Sailer: structure-awarepre-trainedlanguagemodel
YimingCui,WanxiangChe,TingLiu,BingQin,and
forlegalcaseretrieval. InProceedingsofthe46th
Ziqing Yang. 2021. Pre-training with whole word
InternationalACMSIGIRConferenceonResearch
masking for chinese bert. IEEE/ACM Transac-
and Development in Information Retrieval, pages
tionsonAudio, Speech, andLanguageProcessing,
1035–1044.
29:3504–3514.
Haitao Li, Jia Chen, Weihang Su, Qingyao Ai, and
Phong-KhacDo,Huy-TienNguyen,Chien-XuanTran,
YiqunLiu.2023b. Towardsbetterwebsearchperfor-
Minh-Tien Nguyen, and Minh-Le Nguyen. 2017.
mance: Pre-training,fine-tuningandlearningtorank.
Legal question answering using ranking svm and
arXivpreprintarXiv:2303.04710.
deepconvolutionalneuralnetwork. arXivpreprint
arXiv:1703.05320.
HaitaoLi,WeihangSu,ChangyueWang,YueyueWu,
Qingyao Ai, and Yiqun Liu. 2023c. Thuir@ col-
Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding,
iee 2023: Incorporating structural knowledge into
JiezhongQiu,ZhilinYang,andJieTang.2022. Glm:
pre-trainedlanguagemodelsforlegalcaseretrieval.
Generallanguagemodelpretrainingwithautoregres-
arXivpreprintarXiv:2305.06812.
siveblankinfilling. InProceedingsofthe60thAn-
nualMeetingoftheAssociationforComputational
HaitaoLi,ChangyueWang,WeihangSu,YueyueWu,
Linguistics(Volume1: LongPapers),pages320–335.
QingyaoAi,andYiqunLiu.2023d. Thuir@coliee
2023:Moreparametersandlegalknowledgeforlegal
Yan Fang, Jingtao Zhan, Qingyao Ai, Jiaxin Mao,
caseentailment. arXivpreprintarXiv:2305.06817.
Weihang Su, Jia Chen, and Yiqun Liu. 2024.
Scaling laws for dense retrieval. arXiv preprint
Qingquan Li, Yiran Hu, Feng Yao, Chaojun Xiao,
arXiv:2403.18684.
Zhiyuan Liu, Maosong Sun, and Weixing Shen.
Luyu Gao and Jamie Callan. 2021a. Condenser: a 2023e. Muser: Amulti-viewsimilarcaseretrieval
pre-training architecture for dense retrieval. arXiv dataset. In Proceedings of the 32nd ACM Interna-
preprintarXiv:2104.08253. tional Conference on Information and Knowledge
Management,pages5336–5340.
LuyuGaoandJamieCallan.2021b. Unsupervisedcor-
pusawarelanguagemodelpre-trainingfordensepas- BulouLiu,YiranHu,QingyaoAi,YiqunLiu,Yueyue
sageretrieval. arXivpreprintarXiv:2108.05540. Wu,ChenliangLi,andWeixingShen.2023. Lever-
aging event schema to ask clarifying questions for
LuyuGao,ZhuyunDai,andJamieCallan.2021. Re- conversationallegalcaseretrieval. InProceedingsof
thinktrainingofbertrerankersinmulti-stageretrieval the32ndACMInternationalConferenceonInforma-
pipeline. In European Conference on Information tionandKnowledgeManagement,pages1513–1522.
Retrieval,pages280–286.Springer.
Michael Livingston. 1990. Congress, the courts, and
Randy Goebel, Yoshinobu Kano, Mi-Young Kim, Ju- thecode: Legislativehistoryandtheinterpretationof
liano Rabelo, Ken Satoh, and Masaharu Yoshioka. taxstatutes. Tex.L.Rev.,69:819.
2023. Summaryofthecompetitiononlegalinforma-
tion,extraction/entailment(coliee)2023. InProceed- AntoineLouisandGerasimosSpanakis.2021. Astatu-
ingsoftheNineteenthInternationalConferenceon toryarticleretrievaldatasetinfrench. arXivpreprint
ArtificialIntelligenceandLaw,pages472–480. arXiv:2108.11792.AntoineLouis,GijsvanDijck,andGerasimosSpanakis. WeihangSu,ChangyueWang,QingyaoAi,YiranHu,
2024. Interpretablelong-formlegalquestionanswer- ZhijingWu,YujiaZhou,andYiqunLiu.2024c. Un-
ingwithretrieval-augmentedlargelanguagemodels. supervisedreal-timehallucinationdetectionbasedon
InProceedingsoftheAAAIConferenceonArtificial theinternalstatesoflargelanguagemodels. arXiv
Intelligence,volume38,pages22266–22275. preprintarXiv:2403.06448.
ShuqiLu,DiHe,ChenyanXiong,GuolinKe,Waleed ChaojunXiao, XueyuHu, ZhiyuanLiu, CunchaoTu,
Malik, Zhicheng Dou, Paul Bennett, Tie-Yan Liu, andMaosongSun.2021. Lawformer: Apre-trained
andArnoldOverwijk.2021. Lessismore: Pretraina languagemodelforchineselegallongdocuments. AI
strongsiameseencoderfordensetextretrievalusing Open,2:79–84.
a weak decoder. In Proceedings of the 2021 Con-
ferenceonEmpiricalMethodsinNaturalLanguage ChaojunXiao,HaoxiZhong,ZhipengGuo,CunchaoTu,
Processing,pages2780–2791. ZhiyuanLiu,MaosongSun,YansongFeng,Xianpei
Han,ZhenHu,HengWang,etal.2018. Cail2018:
Alarge-scalelegaldatasetforjudgmentprediction.
Yixiao Ma, Yueyue Wu, Weihang Su, Qingyao Ai,
arXivpreprintarXiv:1807.02478.
andYiqunLiu.2023. Caseencoder: Aknowledge-
enhancedpre-trainedmodelforlegalcaseencoding.
NuoXu,PinghuiWang,LongChen,LiPan,Xiaoyan
arXivpreprintarXiv:2305.05393.
Wang,andJunzhouZhao.2020. Distinguishconfus-
inglawarticlesforlegaljudgmentprediction. arXiv
ShounakPaul,PawanGoyal,andSaptarshiGhosh.2022.
preprintarXiv:2004.02557.
Lesicin: Aheterogeneousgraph-basedapproachfor
automaticlegalstatuteidentificationfromindianle-
AiyuanYang,BinXiao,BingningWang,BorongZhang,
gal documents. In Proceedings of the AAAI Con-
CeBian,ChaoYin,ChenxuLv,DaPan,DianWang,
ferenceonArtificialIntelligence,volume36,pages
DongYan,etal.2023. Baichuan2: Openlarge-scale
11139–11146.
languagemodels. arXivpreprintarXiv:2309.10305.
Jay M Ponte and W Bruce Croft. 2017. A language ZiyiYe,XiaohuiXie,QingyaoAi,YiqunLiu,Zhihong
modelingapproachtoinformationretrieval. InACM Wang,WeihangSu,andMinZhang.2024. Relevance
SIGIRForum,volume51,pages202–208.ACMNew feedbackwithbrainsignals. ACMTransactionson
York,NY,USA. InformationSystems,42(4):1–37.
JulianoRabelo,RandyGoebel,Mi-YoungKim,Yoshi- YueZhang,YafuLi,LeyangCui,DengCai,LemaoLiu,
nobu Kano, Masaharu Yoshioka, and Ken Satoh. TingchenFu,XintingHuang,EnboZhao,YuZhang,
2022. Overviewanddiscussionofthecompetitionon YulongChen,etal.2023. Siren’ssongintheaiocean:
legalinformationextraction/entailment(coliee)2021. asurveyonhallucinationinlargelanguagemodels.
TheReviewofSocionetworkStrategies,16(1):111– arXivpreprintarXiv:2309.01219.
133.
HaoxiZhong,ChaojunXiao,ZhipengGuo,CunchaoTu,
StephenRobertson,HugoZaragoza,etal.2009. The ZhiyuanLiu,MaosongSun,YansongFeng,Xianpei
probabilistic relevance framework: Bm25 and be- Han,ZhenHu,HengWang,etal.2018. Overview
yond. FoundationsandTrends®inInformationRe- ofcail2018: Legaljudgmentpredictioncompetition.
trieval,3(4):333–389. arXivpreprintarXiv:1810.05851.
Weihang Su, Qingyao Ai, Xiangsheng Li, Jia Chen, Haoxi Zhong, Chaojun Xiao, Cunchao Tu, Tianyang
YiqunLiu,XiaolongWu,andShengluanHou.2024a. Zhang,ZhiyuanLiu,andMaosongSun.2020. Jec-
Wikiformer: Pre-trainingwithstructuredinformation qa: a legal-domain question answering dataset. In
of wikipedia for ad-hoc retrieval. In Proceedings ProceedingsoftheAAAIconferenceonartificialin-
of the AAAI Conference on Artificial Intelligence, telligence,volume34,pages9701–9708.
volume38,pages19026–19034.
XueZongyue,LiuHuanghai,HuYiran,KongKangle,
Wang Chenlu, Liu Yun, and Shen Weixing. 2023.
Weihang Su, Qingyao Ai, Yueyue Wu, Yixiao Ma,
Leec: Alegalelementextractiondatasetwithanex-
Haitao Li, and Yiqun Liu. 2023a. Caseformer:
tensivedomain-specificlabelsystem. arXivpreprint
Pre-trainingforlegalcaseretrieval. arXivpreprint
arXiv:2310.01271.
arXiv:2311.00333.
Weihang Su, Xiangsheng Li, Yiqun Liu, Min Zhang, A LicenseandPermissions
andShaopingMa.2023b. Thuir2atntcir-16session
search(ss)task. arXivpreprintarXiv:2307.00250. STARDisavailableundertheMITLicense. This
permissive license was chosen to encourage the
Weihang Su, Yichen Tang, Qingyao Ai, Zhijing Wu, widespread use and adaptation of our resources,
and Yiqun Liu. 2024b. Dragin: Dynamic retrieval
allowing for both academic and commercial ap-
augmentedgenerationbasedonthereal-timeinforma-
plicationswithoutsignificantrestrictions. Forde-
tionneedsoflargelanguagemodels. arXivpreprint
arXiv:2403.10081. tailed terms and conditions, including how thedataset, code, and models can be used, modified, retrieved to the total number of relevant items in
andshared,pleaserefertothedocumentationpro- thedatabase, whichiscriticalinscenarioswhere
videdinourGitHubrepository8. missinganyrelevantitemcouldbecostly:
B ImplementationDetailsofRetrieval
Baselines Numberofrelevantitemsretrieved
Recall =
Totalnumberofrelevantitems
• For the implementation of traditional IR meth- (2)
odsQLandBM25,weusethePyserinitoolkit:
https://github.com/castorini/pyserini. D SelectedLLMs
• For the implementation of Chinese-RoBERTa-
OurselectedLLMsarelistedasfollows:
WWM,wedirectlyusetheirmodelsreleasedon
Huggingface9. AsSEEDandCondenserhaveno
• Baichuan(Yangetal.,2023)isaseriesoflarge-
available Chinese versions, we reproduce their
scalemultilinguallanguagemodels,trainedfrom
work on the Chinese Wikipedia based on their
scratch on 2.6 trillion tokens. We choose the
open-sourcetrainingcodeandfollowallsettings
Baichuan-2-Base-13B model which is widely
providedintheirpaper(Luetal.,2021;Gaoand
usedinbilingualChinese-Englishscenarios.
Callan,2021a).
• For the implementation of Lawformer (Xiao • ChatGLM (Du et al., 2022) is a series of gen-
etal.,2021)andSAILER(Lietal.,2023a),wedi- erativelanguagemodelsoptimizedforChinese
rectlyusethecheckpointsreleasedontheofficial question answering and dialogue. We choose
GitHub10. ChatGLM3-6Bwith6.2billionparameters.
C EvaluationMetrics • ChatGPT (Brown et al., 2020) is a series of
large language models developed by OpenAI,
Followingthesettingofpreviousworks(Lietal.,
including several versions. Among these, we
2023b;Chenetal.,2023;Suetal.,2023b;Maetal.,
chooseGPT-3.5-turbo,whichisidentifiedasthe
2023; Chen et al., 2022; Ye et al., 2024), we use
mostadvancedGPT-3.5model.
Mean Reciprocal Rank and Recall as evaluation
metrics.
E GenerationConfiguration
TheMeanReciprocalRankisastatisticalmea-
sureusedtoevaluatetheperformanceofaquery-
We obtain responses from chatGPT by accessing
basedsystem,wheretheprimarygoalistoretrieve
its official API 11. For Baichuan and chatGLM,
thehighest-rankeditem. MRRcalculatestheaver-
wedirectlydownloadmodelparametersfromeach
ageofthereciprocalranksofresultsforasample
model’sofficialHuggingFacerepositoriesanduse
ofqueries. Thereciprocalrankofaqueryresponse
theofficialPythoncodeprovidedbyHuggingFace
isthemultiplicativeinverseoftherankofthefirst
toobtaintheresponse. Weusetheofficialdefault
correctanswer:
configurationsprovidedbyeachmodelforthegen-
erationconfiguration.
Q
1 (cid:88) 1
MRR = (1)
Q rank i F PromptTemplateforRAG
i=1
whereQisthenumberofqueries,andrank isthe
i PreviousstudieshaveshownthatRAGcansignifi-
rankpositionofthefirstrelevantdocumentforthe
cantlyimprovetheperformanceofLLMs(Lewis
i-thquery.
etal.,2020;Suetal.,2024b)andmitigatethehallu-
Recallmeasurestheabilityofamodeltoretrieve
cinationphenomenoninLLMs(Zhangetal.,2023;
allrelevantinstancesinadataset. Itisdefinedas
Su et al., 2024c). In our RAG experiments, we
theratioofthenumberofrelevantitemscorrectly
employed the following prompt template for the
8https://github.com/oneal2000/STARD/tree/main LLM:
9https://huggingface.co/hfl/chinese-roberta-wwm-ext
10https://github.com/CSHaitao/SAILER/, 11https://platform.openai.com/docs/guides/text-
https://github.com/thunlp/LegalPLMs generation/chat-completions-apiPrompt1 H TrainingProcessoftheDenseRetrieval
ModelDistilledfromGPT-4
Please answer the question based on the
followingstatutearticles: We introduce an approach utilizing GPT-4 to
Article1: [Content] generate labels for question-article pairs. Our
...... methodologyleveragesGPT-4’scapabilitiestoau-
Article10: [Content] tonomouslygeneratenon-professionallegalques-
Pleaseanswerthefollowingquestionbased tionsfromstatutoryarticles,thusenablingthepair-
on the provided articles and your knowl- ing of these questions with their corresponding
edge,prioritizingtheprovidedknowledge. articleswithouttheneedforhumansupervision.
Note that the provided articles might not The process begins by selecting statutory arti-
includethoserelevanttothequestion. cles from the corpus of STARD. GPT-4 is then
Question: xxx tasked with generating a legal question based on
thecontentofeacharticle. Thisisachievedbypro-
viding GPT-4 with a specific prompt designed to
G Fine-tuningProcess
simulateascenarioinwhichanindividualwithout
We initialize the model with Chinese-Roberta- prior legal knowledge seeks advice. The prompt
WWM(Cuietal.,2021). Weusethedual-encoder instructsGPT-4toformulateaquestionthatsuch
architecture (Karpukhin et al., 2020; Fang et al., anindividualmightask,ensuringthatthequestion
2024;Suetal.,2023a)tocomputethedotproduct isdirectlyrelatedtoandexplainablebythecontent
between two embedding vectors as the relevance ofthestatutoryarticleprovided. Thepromptused
score: inthisstudyisstructuredasfollows:
X(c)=[CLS]q[SEP], (3) Prompt1
Giventhefollowingknownstatutoryarticle:
X(s)=[CLS]s[SEP], (4)
[Contentofthestatutoryarticle]
Emb(X)=transformer [CLS](X), (5) Imagine a scenario in which a person
without legal knowledge is seeking legal
advice. Pleasegenerateaquestionthatthis
S(q,s)=Emb(X(q))⊤·Emb(X(s)), (6) partymightask.
where q is the query, s is the statute,
Note: The question must be fully explain-
transformer (·) outputs a contextual-
[CLS] able using the statutory article mentioned
ized vector for each token and we select the
above, andrememberthatthepersonwho
"[CLS]" vector as the embedding vector of the
proposes this question has never read the
input. InEquation 6,weregardtheinnerproducts
legalarticlesmentionedbefore.
ofembeddingsastherelevancescoreS.
Forthelossfunction,weusetheSoftmaxCross
EachinteractionwithGPT-4resultsinthecreation
Entropy Loss (Cao et al., 2007; Ai et al., 2018; ofaquery-statutepair(q,a+),whereq isthegen-
i
Gaoetal.,2021;Suetal.,2024a)tooptimizethe eratedquestionanda+isthepositivestatutearticle
i
retrievalmodel,whichisdefinedas:
towhichthequestionisrelevant.
Followingthegenerationofquery-statutepairs,
we employ a contrastive learning framework to
L(Q,s+,N)
train a dense retriever model. We use the same
exp(S(Q,s+)) (7)
=−log exp(S(Q,s+)+(cid:80) exp(S(Q,s−)), relevancescoringfunctionS,asdetailedinEqua-
s−∈N
tion6,whichassessestherelevanceofarticlesto
where S is the relevance score function which is thequestions.
definedinEquation6. Qisthequery,s+ istherel- In the training phase, for each query Q paired
evantstatuteandN isthesetofirrelevantstatutes withapositivearticlea+,wealsosample8nega-
i
randomlysampledfromthecorpus. tivearticlesfromthecorpus. Thesenegativesam-plesarenotrelevanttothequeryandserveasthe
negative set. The loss function employed, repre-
sentedbyEquation8,isdesignedtomaximizethe
scoreofthepositivearticlerelativetothescoresof
thenegativesamples,effectivelytrainingthemodel
to distinguish between relevant and non-relevant
articlesaccurately.
L(Q,a+,N)
i
exp(S(Q,a+)) (8)
=−log i ,
exp(S(Q,a+)+(cid:80) exp(S(Q,a−))
i a−∈N
whereS istherelevancescorefunction,whichis
definedinEquation6,andN isthesetofirrelevant
statutesrandomlysampledfromthecorpus.
I TrainingProcessoftheLSIClassifier
Weapplyafine-tunedclassifierapproachtoeval-
uate the performance of Legal Statute Identi-
fication (LSI) methods, as defined in previous
works(Zhongetal.,2018;Pauletal.,2022). LSI
isframedasaclassificationtaskwhereeachlegal
statuteistreatedasadistinctlabel. Thistransforma-
tionallowsfortheclassificationoflegaldocuments
or queries by associating them with the relevant
statutorylabels.
Our methodology utilizes a transformer-based
classificationmodel,specificallyfine-tunedonthe
STARDdatasetwithinafive-foldcross-validation
framework. We initiate our model using the
Chinese-Roberta-WWM(Cuietal.,2021)forthe
transformer’sparameters,whiletheparametersfor
theoutermostMulti-LayerPerceptron(MLP)layer
areinitializedrandomly. Theprocessofinputtrans-
formationandsubsequentclassificationisdefined
bythefollowingequations:
X(q)=[CLS]q[SEP], (9)
L(q)=MLP(transformer (X(q))), (10)
[CLS]
where q represents a query from the STARD
dataset. The function transformer (·) first
[CLS]
encodes the input using the transformer architec-
ture, focusingontheoutputofthe[CLS]token’s
embedding vector. The MLP(·) then maps this
embeddingontothespaceofstatutorylabelsL.