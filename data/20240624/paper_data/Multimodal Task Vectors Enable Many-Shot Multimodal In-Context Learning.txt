Multimodal Task Vectors Enable Many-Shot
Multimodal In-Context Learning
BrandonHuang1* ChancharikMitra1* AssafArbelle2 LeonidKarlinsky3
TrevorDarrell1 RoeiHerzig1,2
1UniversityofCalifornia,Berkeley 2IBMResearch 3MIT-IBMWatsonAILab
Abstract
The recent success of interleaved Large Multimodal Models (LMMs) in few-
shot learning suggests that in-context learning (ICL) with many examples can
bepromisingforlearningnewtasks. However,thismany-shot multimodalICL
settinghasonecrucialproblem: itisfundamentallylimitedbythemodel’scontext
lengthsetatpretraining. Theproblemisespeciallyprominentinthemultimodal
domain,whichprocessesbothtextandimages,requiringadditionaltokens. This
motivatestheneedforamultimodalmethodtocompressmanyshotsintofewer
tokenswithoutfinetuning. Inthiswork,weenableLMMstoperformmultimodal,
many-shotin-contextlearningbyleveragingMultimodalTaskVectors(MTV)—
compactimplicitrepresentationsofin-contextexamplescompressedinthemodel’s
attentionheads. Specifically,wefirstdemonstratetheexistenceofsuchMTVin
LMMs and then leverage these extracted MTV to enable many-shot in-context
learningforvariousvision-and-languagetasks. OurexperimentssuggestthatMTV
canscaleinperformancewiththenumberofcompressedshotsandgeneralizeto
similarout-of-domaintaskswithoutadditionalcontextlengthforinference.
1 Introduction
LargeMultimodalModels(LMMs)suchasGPT-4V[59],LLaVA[49,50],andtheBLIP[13,43]
familyofmodelsdemonstratestate-of-the-artperformanceonavarietyofvisionandlanguage(VL)
tasksduetotheirstrongreasoningcapabilitiesoverbothtextandimages. Recentworksshowthat
LMMspre-trainedoninterleavedtext-imagedatacandomultimodalin-contextlearning[6,39]. In
particular,few-shot,in-contextlearning(ICL)intext-onlyLLMshasbeenscaledwithanincreasing
numberofexamplesinlong-contextlanguagemodels—asettingcalledmany-shotlearning[1]. A
naturalquestionarisesonhowtoperformmany-shotlearninginthemultimodaldomain.
Thefirstissuewithdirectlyapplyingamany-shotlearningregimentoLMMsistheintrinsiclimitation
ofcontextlength. Thisisespeciallytrueinthemultimodaldomain,asLMMsmustencodebothtext
andimages,whoseembeddingsaretoken-expensive.Moreover,long-contextlanguagemodels,which
LMMsleverageforreasoning,struggletousetheirentirecontextlengtheffectivelyforICL[45,51].
Secondly,perhapsduetothemisalignmentofpretrainingtaskswithICL,manyinstruction-tuned
LMMsunderperformontasksintheICLsetting[16],suggestingtheimportanceofinterleavedLMMs.
Finally,thereisalsothechallengeoftheincreasingmemoryandrun-timerequiredforprocessinglong
contextsforeveryinferencecall. Thesechallengesmotivateamethodforcompressingmultimodal
in-contextexamplesintocompact, implicitrepresentations. Therefore, inthispaper, wepropose
*DenotesEqualContribution
Preprint.Underreview.
4202
nuJ
12
]VC.sc[
1v43351.6042:viXraFigure1: OverviewofMultimodalTaskVectors(MTV).Inthestandardmultimodalin-context
learning(ICL)paradigm(top),thenumberofshotsislimitedbyanLMM’scontextlength. Wesolve
thisissuebyfirstcalculatingthemeanactivationsofthelasttoken’sattentionheads(greysquaresin
theLMM)givenasetofmultimodalICLexamplesandthenfindingasetofattentionheadlocations
(yellowsquaresintheLMM)thatbestalignwiththedownstreamtask. Thesemeanactivationsare
thenreplaceddirectlyintheseattentionheadslocations,enablingmany-shotmultimodalICL.
MultimodalTaskVectors(MTV)—compactrepresentationsofmultimodalin-contexttasks—within
theattentionheadsofLMMstoenablemany-shotICL.Inparticular,weshowtheexistenceofMTV
ininterleavedLMMs,andweusethemtocompresslargenumbersofmultimodalICLexamples.
Recent research in explainability has demonstrated the existence of task vectors in both the lan-
guage[25,79]andvision[27]domains. Thesetaskvectorsareimplicitrepresentationsofin-context
tasksrepresentedbysetsofactivationsinthemodel. Theseactivationscompactlysummarizethe
informationinICLexamples. Inourwork,wegobeyondprovingtheexistenceofthesetaskvectors
inthemultimodaldomainbydemonstratingtheirabilitytocompressexamplesformany-shotICLin
LMMswithouttheneedforfinetuning.
Ourmethodcanbedescribedinthreesteps. First,givenasetofmany-shotmultimodalICLexamples,
wecalculatethemeanactivationsoftheattentionheadsacrossmultipleinferenceiterations. Second,
toavoidthecontextlengthconstraint,weselectasetofattentionheadsinthemodeltostorethe
meanactivationsoftheICLexamples. However,sincethedownstreamtaskmaybezero-shotoruse
adifferentnumberofICLexamples,weselectasetofexamplesalignedwithitsform. Wethenuse
theseexamplestofindanoptimalsetofLMMheadlocationswherethemany-shotexampleswillbe
encoded. WerefertothesemeanactivationsandlocationsasMTV,whichimplicitlyencodesthe
many-shotmultimodalexamplesforuseinthedownstreamtask. Finally,fordownstreaminference,
wereplacethemeanactivationsfromStep1withtheattentionheadlocationsfoundinStep2. Since
weinputexamplestotheLMMacrossdifferentiterationsinStep1,MultimodalTaskVectorscan
implicitlyencodemoreexamplesthanareallowablebythecontextlimit. Wefindthatutilizingmany
examplesforextractingMTVsurpassesperformanceonzero-shotandmoststandardfew-shotICL
2settings,suggestingtheeffectivenessofourmethod. Anotherkeybenefitofourmethodisthatitfrees
uptokensforthemodelduringdownstreaminferencecomparedtostandardfew-shotICLmethods.
Wesummarizeourmaincontributionsasfollows: (i)WeshowtheexistenceofMultimodalTask
Vectors,compactimplicitrepresentationsofin-contextfunctionsinLMMs. (ii)MTVcanencode
more examples than allowed by an LMM’s context length, enabling both runtime and memory-
efficientmultimodalmany-shotin-contextlearning. (iii)MTVsurpasseszero-shotandfew-shotICL
settingsonvariousVLbenchmarkswithoutfinetuning. (iv)MTVcanscaletolargernumbersof
examplesandcangeneralizetosimilarout-of-domaintasks.
2 RelatedWorks
Many-ShotIn-ContextLearning. Few-shotin-contextlearning(ICL)isasignificantareaofstudy
intext-onlyLLMs[9,87]. Anaturalquestionarisesaboutthepossibilityofusingalargernumberof
shots(e.g.,hundreds)tofurtherimproveperformanceorlearnmorecomplextasks. Indeed,some
earlyworkintext-onlymany-shot,in-contextlearningsuggestsperformanceondifferenttaskscan
scalewithalargernumberofexamples[1,7,44,45].
However, scaling ICL in text-only LLMs is a challenge due to the intrinsic context length. One
methodtoincreasecontextlengthinthesemodelsistoapplypositionalinterpolationmethods[10,62].
However,researchontheselonger-contextmodelsfindsthattheystruggletousetheentirecontextfor
ICL[45,51]. Moreover,asinferenceonlongcontextsofinputsisalsotimeandmemory-expensive,
itisunclearwhethersimplyscalingthecontextofmodelsispracticalforenablingmultimodalmany-
shotICL.Thishasledtoworkthatlookstocompressexplicitinputtokens[11,20,35,57,71,75].
Butcrucially,manyofthesemethodsrequirefinetuningandonlytrytopreserveperformance. Our
workisdifferentinthatitisthefirsttoenablemultimodalmodelswithmany-shotICLcapabilities,
whilealsohavingthebenefitofimprovingoncomplexVLtaskswithoutfinetuning.
TaskVectors. Ourworkbuildsoffofresearchintext-onlyandvision-onlydomainsshowingthat
internalrepresentationsofthesemodelscalledtaskvectors[25,27,79](orfunctionvectors)can
encapsulate tasks outlined by ICL examples. Our is the first demonstration of Multimodal Task
Vectors(MTV)inLMMs. Goingbeyondpreviouswork,however,weshowthatMTVenableLMMs
notonlytousemany-shot, multimodalICLexamplesbutalsoscalewithmoresamples, beused
alongsideexplicitICLshots,andevengeneralizetounseenclassesorsimilartasks.
ModelDomainAdaptationMethods. AsLLMandLMMmodelarchitectureshaveadvanced,so
havemethodstoallowthesemodelstogeneralizebeyondtheirpretrainingdistributions. Methodslike
instructiontuning[5,50,67,86]haveshownstrongzero-shotgeneralizationtosomeout-of-domain
tasks,butforgettingremainsanissue. OnepopularsolutiontothisissueinvolvesParameterEfficient
Fine-tuning(PEFT)[28]: finetuningeitherasetofsoftpromptinputtokens[41,46],low-rankmodel
weights[14,30,96],oraseparateadapterfromthemainmodel[18,31,97].
Promptingmethodsareawell-exploredareaforadaptingmodelswithoutfinetuning. LLMprompt-
ing includes zero-shot methods [36, 81, 83], few-shot and ICL methods [9, 15, 53, 55], expert
prompting[91],andChain-of-Thought(CoT)[88,98],withextensionslikeself-consistency[84],
Tree-of-Thought(ToT)[92],andGraph-of-Thought(GoT)[8,40,93]formorecomplexstructures.
SimilarmultimodalpromptingmethodsexistforLMMsaswell[56,82,85,99,101].
LargeMultimodalModels(LMMs). Thestate-of-the-artperformanceofLMMs[2,6,13,17,21,
43,49,50,94,95,102]onmultimodaltasksstemsfromcombiningLLMs’reasoningcapabilities
[3,12,26,65,69,76]withtheperceptionabilitiesofvisionmodels.LMMs’generativereasoningalso
makesthemmoreapplicabletocomplextasksthanpreviouscontrastivemethods[42,43,64]. Such
tasksincludevisualquestion-answering[4,23,24,32,33,54,66]aswellasobjectidentificationand
localization[37,48,58,80]. VisualProgrammaticModels(VPMs)areanotherclassofmultimodal
methods that makes use of in-context APIs code generation [19, 22, 52, 63, 68, 70, 72, 74, 90].
However,contextlengthlimitsbothLMMs’andVPMs’abilitytousemultimodalpromptingmethods
suchasICL[9]. AnotherkeychallengeisthatmanyLMMsarepre-trainedonsingletext-imagepair
data. Recently,manyLMMmodelsnowpretrainoninterleavedtext-imagedata[2,6,16,34,39,73,
100],makingeffectivemultimodalICLpossible. Inourwork,MTVgoesbeyondsimplefew-shot
multimodalICLandscalestomany-shotmultimodalICL.
33 MultimodalTaskVectors
Toaddressthechallengeofperformingmany-shotmultimodalin-contextlearning,wedemonstrate
theexistenceofMTVinLMMsandthenleveragethemformany-shotmultimodalICL.Webeginby
describingsomebackgroundonmultimodalICLandtaskvectors(Section3.1). Wethenintroduce
ourthree-stepapproach: (i)Wecalculatethemeanactivationsoftheattentionheadsfromthemany-
shotmultimodalICLexamples(Section3.2);(ii)WethenextractthesetofLMMattentionheads
locationsthatbestaligntothedownstreamtaskusinganadaptedversionoftheREINFORCE[89]
algorithm(Section3.3);and(iii)WereplacethecalculatedmeanactivationvaluesintotheLMMfor
adownstreamtask(Section3.4). ThemethodoverviewisalsoillustratedFigure1.
3.1 Preliminaries
In the multimodal in-context learning setting, an LMM learns a new task outlined by a set of
multimodalexamples. TheinputtotheLMMwouldbeoutlinedasfollows:
I =[(x :y ),(x :y ),...,(x :y ),Q] (1)
few 1 1 2 2 n n
wherethemodelispromptedtoansweraqueryQgivenasetofinput-outputexamples(eachx
i
beingamultimodalinputandeachy atextoutput).
i
We note that in-context examples are commonly passed sequentially to the LMM, necessarily
restrictingmultimodalICLtobeingsmallnumbersofshotsduetolimitedcontextlength.Furthermore,
the images require more tokens to embed, which means enabling many-shot ICL is even more
challenging in the multimodal domain. To solve this, we utilize our method MTV—which are
implicitrepresentationsinthemodel’sattentionheadsthatencodeamany-shotmultimodalICLtask.
Westartwithabackgroundontaskvectorsforsometaskj. GivenamodelF,wedenotethesetof
attention-headlocationsasλ={l|∀l∈F}whereeachlocationlisindexedasl=(h,m)forthe
hthlayerandmthattentionhead. Wedefinethetaskvectorsasfollows: (1)thetaskvectorvaluesµ
j
areasubsetofmeanactivationsproducedbytheattentionheadsofF givenexamplesofatask,and
(2)thetaskvectorlocationsλ ,whichdenotesasubsetoftheattentionheadindicespertask. Thus,
j
thetaskvectoris(µ ,λ ). Forinference,µ replacestheactivationvaluesoftheheadsinλ .
j j j j
In prior work [25, 27, 79], the calculation of the mean activations µ and the extraction of the
j
attention-headlocationsλ areusedtogethertoextractthetaskvector. Interestingly,wefindthat
j
thesetwostepsshouldbedecoupledinordertobetteralignwiththedownstreamtask. Inourwork,
wecalculatethemeanactivationsµ correspondingtothelasttokenspecificallytoencodeadataset
j
ofmany-shotmultimodalICLexamplesbyaveragingthemacrossmultipleinferencecalls. However,
thedownstreamtaskmaynotalwaysbeinthesameICLformatasthemany-shotexamples(e.g.,the
downstreamtaskusesadifferentnumberofshotsoriszero-shot). Tosolvethis,weuseaseparateset
ofexamplesthatareoftheexactformatofthedownstreamtasktoaligntheextractedattention-head
locationsλ withtheinferencetask. Thisseparationofresponsibilities, whereinµ capturesthe
j j
essential information from the many-shot examples and λ identifies the specific attention head
j
locationsforthedownstreamtask,optimizestheutilizationoftheencodedinformationatrelevant
locationswithinthemodel.
OurapproachtofindingMultimodalTaskVectors(MTV)(µMTV,λMTV)allowsLMMstoactually
j j
leverage many-shot multimodal ICL examples for complex vision-language tasks without being
limitedbycontextlength. Weproceedbyfirstdescribinghowtocalculatethemeanactivations.
3.2 Step1: CalculateMTVMeanActivations
Theultimateobjectiveofmany-shotmultimodalICListousealargenumberofinput-outputexamples
whensolvingataskj.However,itisnottrivialtogettheLMMtoseemoreexamplesduringinference
timethanitscontextlengthallows.
To address this issue, we pass a few-shot input I for each inference call t for a total of T > 1
t
inferencecalls. EachI consistsofN shots(whereN >1)ofmultimodalin-contextexamplesinthe
t
formofrandomly-selectedinput-outputresponsepairs(x : y ),andQ ,whichisthequerytobe
t t t
answeredbytheLMMinthatiteration.
I =[(x :y ),(x :y ),...,(x :y ),Q ] (2)
t 1 1 2 2 N N t
4Thus,overT LMMinferencecalls,wehaveamany-shotmultimodaldataset(ofN ×T examples):
I =[I ,I ,...,I ] (3)
many 1 2 T
However,thisdatasetisstilljustadisconnectedsetoffew-shotexamples. Next,wewouldliketo
connecttheseparateexamplesintooneunifiedmany-shotmultimodalICLrepresentation.
For each inference call, the LMM is given N-shot ICL examples. We calculate the mean of the
activations corresponding to the last token of the input z for each attention head index ∀l ∈ λ
l,j
(Section3.1)acrossT inferencecalls,yielding:
T T
1 (cid:88) 1 (cid:88)
∀l∈λ: µ = E[z |I ]= E[z |(x :y ),(x :y ),...,(x :y ),Q ] (4)
l,j T l,j t T l,j 1 1 2 2 N N t
t=1 t=1
Inthisstep,wehavefoundthemeanactivationsµ ,whichencodeaninternalLMMrepresentation
l,j
ofmanyshotsofmultimodalICLexamples. Inthenextsubsection,wedescribeourmethodologyfor
selectingthesetofattentionheadswherethesemeanactivationswillbeused.
3.3 Step2: ExtractMTVAttentionHeadLocations
After Step 1, we now have mean activations for the attention heads of the last token in a given
many-shotmultimodaltask. Yet,westillneedtofindwhichsetofattentionheadsλMTV shouldbe
j
chosentoencodeourtask.
Tochoosethesetofattentionheads,wefirstprepareaseparatesetofS examplesspecificallyaligned
totheformatofthedownstreamtask. Forinstance,ifthedownstreamsettingisa2-way,one-shot
classificationtask,thentheS examplesshouldconformtothisparadigm. Forourexplanation,let’s
consideradownstreamtaskthatiszero-shotsuchthatthereisasinglequeryQ andcorresponding
s
responseR foralls∈[1,2,...,S].
s
Fromtheseexamples,weutilizeanadaptedversionoftheREINFORCE[89]algorithm—aniterative
policyoptimizationmethodthatcanbeusedtofindtaskvectorlocations. GivenanLMMF,we
firstselectaproposedsetofattentionheadlocationsbysamplingaBernoullidistributionoverthe
locationsmultipletimes. Next,wedirectlyreplacethevaluesoftheselectedattentionheadswiththe
correspondingmeanactivationsµ . Then,afterpromptingthemodelwiththequeryQ ,weuse
l,j s
thenegativecross-entropylossbetweentheLMM’soutputlogitsandthelogitsoftheground-truth
responseR tooptimizetheBernoullidistribution. ByoptimizingtheBernoullidistributionacrossS
s
iterations,wearefindingthebestattentionheadlocationsλMTV forpatchinginourmeanactivations.
j
Finally,wecanextractλMTV,theoptimizedindicesofattentionheads,bysamplingouroptimized
j
Bernoullidistribution.
λMTV =MTV_EXTRACT(F,[Q ,Q ,...,Q )],[R ,R ,...,R ]) (5)
j 1 2 S 1 2 S
It is important to note that MTV_EXTRACT does not require finetuning of the LMM parame-
ters, but rather only inference calls. We describe further the underlying details of our adapted
MTV_EXTRACTalgorithminSectionBoftheSupplementary. HavingfoundλMTV andµ ,we
j l,j
describeinwhatfollows,thefinalproceduretouseMTVforinference.
3.4 Step3: MultimodalTaskVectorApplication
After we have identified a set of attention heads λMTV, it is straightforward to apply MTV for
j
inference. WedenotethesetofmeanactivationsµMTV asfollowsµMTV ={µ |∀l∈λMTV}.
j j l,j j
TorundownstreaminferenceonanewqueryQ withourmodelF,wedirectlyreplacethevalues
new
ofattentionheadsλMTV withµMTV andproducethefollowingresponseR :
j j new
R =F(Q |λMTV,µMTV) (6)
new new j j
R isthusaresponsegeneratedusingmanyshotsofmultimodalexamplesasimplicitcontextvia
new
MTV.ThekeyinsightofourmethodistheimportanceofN (thenumberofmultimodalexamples)
andmanyT (thenumberofiterations)duringthecalculationofMTV.ThisenablesanLMMtogo
5beyonditscontextlengthtolearnmorenuancedpropertiesofthetaskfromseeingmanyexamples.
Additionally,insertionofMTVdirectlyintotheLMMalsoobviatestheneedforanycontextlength
duringdownstreaminference,actuallyfreeingadditionalcontextforotheruse(e.g.,anadditional
prompt,moreICLexamples,etc.). Finally,becausewealigntheattention-headlocationswiththe
downstreamtask,MTVcanbeeffectivelyappliedtozero-shotanddifferentICLsettings.
4 Evaluation
InorderforLMMstoperformmultimodalICL,itisimportantforinterleaveddatatobeincludedin
pretraining. WeapplyourMTVapproachtoQwen-VL[6],Idefics2-8B[38],andViLA-1.5-8B[47]
threepopularinterleavedLMMs. Foreachmodel,wecompareourmethodtousingfew-shotICL
acrossdifferentvision-and-languagetaskslikeVQAandobjectidentification.
4.1 ImplementationDetails
WeimplementedMTVusingPyTorch[60]. Weusedeachmodel’srespectiveofficialimplementation.
Whilethecomputeandmemoryrequirementsdifferslightlybetweenmodels,allourexperimentscan
berunonasingleNVIDIAA6000GPU.Foradditionalinformation,refertoSupplementarySectionC.
Ourmodelandweightswillbereleaseduponacceptance,andourcodeisinSupplementary.
4.2 Models
In this work, we apply MTV to the following interleaved LMMs as they are better-suited for
multimodalICLasshownby[16]: (1)QwenVL[6]isaLLaMA-basedmodelthathastheabilityto
processhigh-resolutionimages,anditstwo-stagepre-trainingmethodology,whichincludesmulti-task
finetuningandinterleavedtext-imagedata. (2)Idefics2-8B[39]isaMistral-basedmodelthatbenefits
fromitspre-trainingontheexpansiveOBELICSdataset,whichcomprisesaweb-scalecollection
ofinterleavedimage-textdocuments. Weutilizethebaseversionofthemodel. Thisdemonstrates
multimodal in-context learning abilities. (3) ViLA-1.5-8B. ViLA-1.5-8B [47] is an architecture
thatleveragesLLaMA-3astheLLMbackbone. Asinothers,asignificantportionofthemodel’s
pretrainingdataisinterleavedtext-imagedata.
4.3 Datasets
Webrieflydescribethetasksanddatasetsweevaluateourmethodon. Moredetailsaboutthedatasets
andtheirsetupcanbefoundinSectionC.
VQADatasets.Weusethefollowingcommonly-evaluateddatasetswhichemphasizedifferentaspects
ofmultimodalreasoning, includingvisualfeatures(VizWiz)andoutsideknowledge(OK-VQA):
(1) VizWiz [23] consists of images taken by visually impaired individuals paired with questions
theyposeabouttheseimages,makingitcrucialfordevelopingAIsystemsthatassistinreal-world,
accessibility-focusedvisualunderstandingtasks. (2)OK-VQAdataset[54]isdesignedtopushthe
boundariesofVisualQuestionAnswering(VQA)byfocusingonknowledge-basedquestions,where
answersrequireexternalknowledgebeyondtheimagecontent. (3)
ObjectClassification. Weusethefollowingdatasets,whicharecommonlyusedforobjectclas-
sificationinmultimodalICL:(1)TheFlowersdataset[58],commonlyknownastheOxford102
Flowersdataset,isacollectionspecificallydesignedforimage-basedflowerspeciesrecognitionfor
fine-grainedclassificationof102differentcategoriesofflowers. (2)Caltech’sCUBDataseton
Birds[80]isawell-knownresourceforevaluatingalgorithmsonthetaskofobjectidentification,
specificallyfocusedonbirdspecies. Itfeatures200birdspecieswithroughly30imageseach,anno-
tatedwithkeyattributesandboundingboxes. BothFlowersandBirdsareformattedas2-way,1-shot
classificationepisodes,withmodelinputsbeingapositiveandnegativeimagefortheclasstobe
identifiedinthequeryimage. Theresponseformatisashorttextresponse.
5 Results
OurmainresultsareshowninTable1. ForVQA,weshowtheresultsofMTVwith4shotsper100
iterationstocalculatethemeanactivationsand100examplesfortaskvectorlocations(500examples
6Table 1: Results. (Left) MTV evaluated on VQA datasets. (Right) MTV evaluated on object
classificationdatasets. Thebaselinesareshowningray.
(a)MTVonVQABenchmarks (b)MTVonObjectClassification
Model VizWiz OK-VQA Model Flowers CUB
Flamingo9B 28.8 44.7 LLaVA-1.5-13B
+4-shotICL 34.9 49.3 +1-shotICL 58.60 58.24
+8-shotICL 39.4 50.0 LLaVA-1.6-13B
Blip3 21.2 26.5 +1-shotICL 65.58 67.90
+4-shotICL 38.4 49.2 Flamingo9B
+8-shotICL 44.3 49.1 +1-shotICL9B 48.78 51.2
Qwen-VL-7B 35.2 58.6 IDEFICS-9B
+4-shotICL 42.0 62.0 +1-shotICL 55.29 62.0
+8-shotICL 44.3 61.5 Emu37B
+MTV 45.6 62.0 +1-shotICL 52.76 53.56
Idefics2 31.3 52.4 Qwen-VL-7B
+4-shotICL 40.8 51.5 +1-shotICL 55.0 56.5
+8-shotICL 43.8 52.3 +MTV+1-shotICL 78.1 80.0
+MTV 52.5 53.0 Idefics2
Llama3- +1-shotICL 82.8 88.7
28.0 32.8
VILA-1.5-8B +MTV+1-shotICL 83.8 89.8
+4-shotICL 39.3 35.6 Llama3-VILA-1.5-8B
+8-shotICL 44.2 36.5 +1-shotICL 87.4 88.4
+MTV 55.2 40.6 +MTV+1-shotICL 89.3 89.7
total). Thetaskvectorisextractedusingexamplesfromthetrainsetofthedatasetandevaluatedon
thevalidationset. Forobjectclassification,weextractMTVbasedona2-way,one-shotregimen
per100iterationsforbothmeanactivationsandtaskvectorlocations(200examplestotal). Thetask
vectorisextractedusingatrainsetof30%oftheobjectclassesandevaluatedontheremaining70%
ofunseenclasses. WedemonstratehowMultimodalTaskVectorsoutperformszero-shotandfew-shot
ICLsettingsonthreedifferentmodelsonVLtasks,highlightingtheeffectivenessofourmethod.
Next,wedescribetheuniquecapabilitiesofourmethod,suchasscalingtomoresamplesandshowing
somegeneralizationstoothertasks. MoreresultscanbefoundinSectionA.1ofSupplementary.
5.1 MTVscaleswithmoreexamples
Figure2: ScalingofQwen-MTVonVizWiz: (Left)Weshowtheeffectofvaryingthenumberof
shotsperiterationforafixed100iterations. (Right)Wealsoshowtheeffectofvaryingnumbersof
iterationsfixing4shotsperiteration.
7Weareinterestedinevaluating(i)theeffectofdifferentnumbersofshotsusedperiterationtoextract
MTVand(ii)theeffectofdifferentnumbersofiterationsused. Wetesttheimpactonaccuracywhen
increasingbothoftheseparametersforQwenVLontheVizWizvalidationset. InFigure2,weshow
ontheleftthattheoptimalnumberofmultimodalICLshotsis16shotsperiteration. Further,we
showontherightsideofthefigurethat1000examplesyieldthebestperformance. Theseresults
illustratethatMTVcaneffectivelyscalebyutilizinglargernumbersofICLexamplesperiteration
andalsoinaggregate.
5.2 MTVworkswithexplicitfew-shotexamples
OneofthebenefitsofMTVoverafew-shotICListhecontextlengththatissavedduringinference.
Thisisbecausethemany-shotexamplesareencodeddirectlyintheactivationspaceratherthaninthe
inputtokenspace. Thus,weaskwhethertheLMMcanusethefreedcontextforadditionalfew-shot
examples. Forobjectclassification,weformulatebothFlowersandCUBasa1-shotcomparison
between a positive and negative sample to identify the correct class (i.e., 2-way, 1-shot ICL by
construction). Wereportresultson1-shotICLandMTVwith1-shotclassificationduringinference.
MTV+1-shotICLsurpasses1-shotICLaccuracyonthesetasks,showingthatMTVcanbeutilized
alongsidefew-shotexamples.Furthermore,itisvitaltonotethattheevaluationclassesarecompletely
unseenbyMTV.Thus,withjusta1-shotICLexample,MTVisabletogeneralizetounseenclasses.
5.3 MTVheadsgeneralizetoothertasks
Table2: Generalization&MethodComparison(Left)MTV-VizWizevaluatedonOK-VQA.
(Right)MTVcomparedtoVizWizfinetuning,functionvectors[79],andtaskvectors[27].
(a)AttentionHeadGeneralization (b)ComparisontoOtherMethods
Model VizWiz OK-VQA Model VizWiz OK-VQA
Qwen-VL-7B 35.2 58.6
ViLA-1.5-8B 28.0 32.8
+VizWizLoRA 62.0 25.1
+4-shot-ICL 39.3 35.6
+FV 36.4 59.0
+8-shot-ICL 44.2 36.5
+VTV 37.0 58.9
+MTV-Vizwiz 55.2 38.3 +MTV 45.6 62.0
In this experiment, we further ask whether the MTV heads λMTV extracted on one task j can
j
generalizetoaseparate,butsimilartaskk. Totestthis,weusetheattentionheadsextractedfrom
ViLA-1.5-8BonVizWizforuseonOK-VQA.OurresultsontheleftofTable2demonstratethatthe
extractedheadsfromonetaskcanimproveaccuracyonanothersimilartask. Thisgeneralizabilityof
theheadsissignificantbecauseitsuggeststhattheheadsfromMTVmayonlyhavetobeextracted
oncetobeappliedtomanyothersimilartasks. Theonlycalculationnecessarythenwouldbethe
meanactivationsofthemany-shotexamplesusedforthetargetdataset,makingtheapplicationof
many-shotmultimodalICLevenmoreefficientforsimilartasks.
5.4 LoRAFinetuningasanupperbound
InTable2b,wecompareourmethodtoLoRAfinetuning[29]. Todothis,weuseLoRAonthesame
numberofexamplesasMTVusesfromthetrainsetandevaluatenotonlyonthevalidationsetbut
alsoonthevalidationsetofanothersimilardataset. Inparticular,foraViLA-1.5-8Bmodelfinetuned
onVizWiz,wereportaccuracyonbothVizWizandOK-VQAvalidationsets. Itcanbeseenthat
LoRAfinetuningisindeedanupperboundonthedatasetthemodelwasfinetunedon. However,we
showthatLoRAleadstooverfittingonthefinetuneddatasetandforgettingthezero-shotcapabilities.
Incontrast,wealsoshowthatMTVnotonlyimproveszero-shotcapabilitiesbutcangeneralizeto
similartaskswithonlyafewinferenceexamplesTable1bandTable2a.
5.5 Comparisontoothermethods
Wecompareourmethodtotwodifferentmethodsthatcanfindtaskvectors: VisualTaskVectors
(VTV)[27]andFunctionVectors(FV)[79]. Originally,theseworkscouldnotbeappliedas-isto
8supportmultimodalICL,buthere,wehaveimplementedaversionthatfollowstheoriginalexactly
withonlyminormodificationstoallowperformingourevaluatedmultimodaltasks. Moredetails
aboutthemethodscanbefoundinSectionBintheSupplementary. InourexperimentsTable2b,we
findthatMTVsurpassesbothmethodsonVizWizandOK-VQA.VTVareimage-onlytaskvectors
that use only one-shot image examples for fixed small T iterations, and they calculate the mean
activations and the locations together without aligning to the downstream task. FV are text-only
task vectors that use Causal Mediation Analysis [61] to extract task vector locations. But then a
vectorofmeanactivationsattheselocationsissimplyaddedaftertheoutputofanarbitrarylayer.
Theresultssuggesttheimportanceoffindingthetaskvectorsbydecouplingthecalculationofthe
meanactivationsandlocationsintwoseparatestepstoperformmany-shotmultimodalICLmore
effectivelyforcomplexmultimodaltasks.
5.6 Computeandruntimeefficiency
Metric 0-shot 4-shot 8-shot 16-shot MTV(400-shot)
MaxGPUMemory(GB) 17.4 18.3 19.0 20.6 19.8
Runtimeper100iterations(min) 1.1 2.7 3.1 3.3 1.9
Table3: Efficiency: WeshowthateventhoughMTVencodes400multimodalICLexamplesinthe
meanactivations,itstillrequireslessruntimeandmemorythan8-shotand16-shotmultimodalICL.
AnimportantfeatureofourworkisthatmultimodalICLexamplesdonotrequireexplicittokens
duringinference. Becauseofthis,weareinterestedintheefficiencygainsofourmethod. Intuitively,
thelongerMTVextractiontimeisamortizedduringdownstreaminference,wheretheruntimewould
beequivalenttothezero-shotcase. Similarly,thememoryrequirementsaremaximalduringtheMTV
extractionprocessbutrequirethesamememoryasthezero-shotcaseafterward. Incontrast,theICL
taskshaveaslowerruntimeandlargermemoryrequirementthroughoutduetorunninginferenceon
N examplesforeveryiteration. Todemonstratethis,wecalculatethemaximummemoryrequirement
ingigabytes(GB)forViLA-1.5-8BonVizWizusingdifferentICL-shotcountsandMTVwith400
examples. AsshowninTable3,MTVrequireslessruntimethan16-shot,8-shot,and4-shotICL
methodsandalsorequireslessmemorythan16-shotICL.TheseresultsdemonstratethatMTVcan
encodemanymultimodalICLexampleswithgreaterefficiencythanfew-shotmethods.
6 Conclusion
In this work, we present Multimodal Task Vectors a compact, implicit representation that can
efficientlyencodemany-shotmultimodalICLexamplesforuseincomplexvision-languagetasks.
WedemonstratethisimplicitmodelrepresentationnotonlyencodesamultimodalICLtaskbutcan
alsoenablemany-shotmultimodalICLtosurpasszero-shotandfew-shotperformanceonavarietyof
VLtasks. Ourmethodstandsoutfrompreviousworkinitsabilitytoscale,useadditionalexplicit
multimodalICLexamples,andgeneralizetoothersimilarVLtasks. Ourworkisaviablewayto
surpassthelimitofcontextlengthofanLMMformultimodalICLanddemonstratesclearlythatthese
additionalexamplesaidinmultimodalreasoning. Finally,wedonotanticipateaspecificnegative
impact,but,aswithanyMachineLearningmethod,werecommendexercisingcaution.
7 Limitations
WhileMultimodalTaskVectorsofferssubstantialbenefitsforhandlingcomplexvision-language
taskscomparedtofinetuningorfew-shotICL,itisimportanttorecognizecertainlimitationsthat
accompanyourapproach. MTVrequiresaccesstotheinternalarchitectureofanLMM,sowhileitis
aneffectivesolutionforallopen-sourcemodels,itsapplicationisrestrictedfromproprietarymodels,
suchasGPT-4[59]andGemini[77,78]. Furthermore,whilemany-shotICLisincrediblyattractive
formanyapplications,itmaynotbepracticalforlow-datascenarioswheresyntheticdata[1]orthe
transferofMTVextractedfromanotherdatasetmayberequired. Wefeelthesechallengesrepresent
greatopportunitiesforfutureworkinthemany-shotmultimodalin-contextlearningdomain.
9Acknowledgements.
We would like to thank Suzie Petryk and Grace Luo for helpful feedback and discussions. This
projectwassupportedinpartbyDoD,includingPTGand/orLwLLprograms,aswellasBAIR’s
industrialallianceprograms.
References
[1] RishabhAgarwal,AviSingh,LeiM.Zhang,BerndBohnet,StephanieChan,AnkeshAnand,Zaheer
Abbas,AzadeNova,JohnD.Co-Reyes,EricChu,FeryalM.P.Behbahani,AleksandraFaust,andHugo
Larochelle. Many-shotin-contextlearning. 2024.
[2] Jean-BaptisteAlayrac,JeffDonahue,PaulineLuc,AntoineMiech,IainBarr,YanaHasson,KarelLenc,
ArthurMensch,KatieMillican,MalcolmReynolds,RomanRing,ElizaRutherford,SerkanCabi,Tengda
Han,ZhitaoGong,SinaSamangooei,MarianneMonteiro,JacobMenick,SebastianBorgeaud,Andy
Brock,AidaNematzadeh,SahandSharifzadeh,MikolajBinkowski,RicardoBarreira,OriolVinyals,
AndrewZisserman,andKarenSimonyan. Flamingo: avisuallanguagemodelforfew-shotlearning.
ArXiv,abs/2204.14198,2022.
[3] AmitAlfassy,AssafArbelle,OshriHalimi,SivanHarary,RoeiHerzig,EliSchwartz,RameswarPanda,
MicheleDolfi,ChristophAuer,PeterW.J.Staar,KateSaenko,RogerioFeris,andLeonidKarlinsky.
FETA:Towardsspecializingfoundationalmodelsforexperttaskapplications. InThirty-sixthConference
onNeuralInformationProcessingSystemsDatasetsandBenchmarksTrack,2022.
[4] StanislawAntol,AishwaryaAgrawal,JiasenLu,MargaretMitchell,DhruvBatra,CLawrenceZitnick,
andDeviParikh. Vqa:Visualquestionanswering. InProceedingsoftheIEEEinternationalconference
oncomputervision,pages2425–2433,2015.
[5] EladBenAvraham,RoeiHerzig,KarttikeyaMangalam,AmirBar,AnnaRohrbach,LeonidKarlinsky,
TrevorDarrell,andAmirGloberson. Bringingimagescenestructuretovideoviaframe-clipconsistency
ofobjecttokens. InThirty-SixthConferenceonNeuralInformationProcessingSystems,2022.
[6] JinzeBai,ShuaiBai,ShushengYang,ShijieWang,SinanTan,PengWang,JunyangLin,ChangZhou,
andJingrenZhou. Qwen-vl: Afrontierlargevision-languagemodelwithversatileabilities. ArXiv,
abs/2308.12966,2023.
[7] AmandaBertsch,MaorIvgi,UriAlon,JonathanBerant,MatthewR.Gormley,andGrahamNeubig.
In-contextlearningwithlong-contextmodels:Anin-depthexploration. 2024.
[8] MaciejBesta,NilsBlach,AlesKubicek,RobertGerstenberger,LukasGianinazzi,JoannaGajda,Tomasz
Lehmann, MichalPodstawski, HubertNiewiadomski, PiotrNyczyk, andTorstenHoefler. Graphof
thoughts:Solvingelaborateproblemswithlargelanguagemodels. ArXiv,abs/2308.09687,2023.
[9] TomB.Brown,BenjaminMann,NickRyder,MelanieSubbiah,JaredKaplan,PrafullaDhariwal,Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss,
GretchenKrueger,TomHenighan,RewonChild,AdityaRamesh,DanielM.Ziegler,JeffWu,Clemens
Winter,ChristopherHesse,MarkChen,EricSigler,MateuszLitwin,ScottGray,BenjaminChess,Jack
Clark,ChristopherBerner,SamMcCandlish,AlecRadford,IlyaSutskever,andDarioAmodei. Language
modelsarefew-shotlearners. ArXiv,abs/2005.14165,2020.
[10] ShouyuanChen,ShermanWong,LiangjianChen,andYuandongTian. Extendingcontextwindowof
largelanguagemodelsviapositionalinterpolation. ArXiv,abs/2306.15595,2023.
[11] AlexisChevalier, AlexanderWettig, AnirudhAjith, andDanqiChen. Adaptinglanguagemodelsto
compresscontexts. ArXiv,abs/2305.14788,2023.
[12] AakankshaChowdhery,SharanNarang,JacobDevlin,MaartenBosma,GauravMishra,AdamRoberts,
PaulBarham,HyungWonChung,CharlesSutton,SebastianGehrmann,ParkerSchuh,KensenShi,Sasha
Tsvyashchenko,JoshuaMaynez,AbhishekRao,ParkerBarnes,YiTay,NoamM.Shazeer,Vinodkumar
Prabhakaran,EmilyReif,NanDu,BentonC.Hutchinson,ReinerPope,JamesBradbury,JacobAustin,
MichaelIsard,GuyGur-Ari,PengchengYin,TojuDuke,AnselmLevskaya,SanjayGhemawat,Sunipa
Dev,HenrykMichalewski,XavierGarcía,VedantMisra,KevinRobinson,LiamFedus,DennyZhou,
DaphneIppolito,DavidLuan,HyeontaekLim,BarretZoph,AlexanderSpiridonov,RyanSepassi,David
Dohan,ShivaniAgrawal,MarkOmernick,AndrewM.Dai,ThanumalayanSankaranarayanaPillai,Marie
Pellat,AitorLewkowycz,EricaMoreira,RewonChild,OleksandrPolozov,KatherineLee,Zongwei
Zhou,XuezhiWang,BrennanSaeta,MarkDíaz,OrhanFirat,MicheleCatasta,JasonWei,KathleenS.
Meier-Hellstern,DouglasEck,JeffDean,SlavPetrov,andNoahFiedel.Palm:Scalinglanguagemodeling
withpathways. J.Mach.Learn.Res.,24:240:1–240:113,2022.
10[13] WenliangDai,JunnanLi,DongxuLi,AnthonyMengHuatTiong,JunqiZhao,WeishengWang,Boyang
Li,PascaleFung,andStevenHoi. Instructblip:Towardsgeneral-purposevision-languagemodelswith
instructiontuning,2023.
[14] TimDettmers,ArtidoroPagnoni,AriHoltzman,andLukeZettlemoyer. Qlora:Efficientfinetuningof
quantizedllms. ArXiv,abs/2305.14314,2023.
[15] QingxiuDong,LeiLi,DamaiDai,CeZheng,ZhiyongWu,BaobaoChang,XuSun,JingjingXu,and
ZhifangSui. Asurveyonin-contextlearning. 2022.
[16] SivanDoveh,ShakedPerek,MuhammadJehanzebMirza,AmitAlfassy,AssafArbelle,ShimonUllman,
andLeonidKarlinsky. Towardsmultimodalin-contextlearningforvision&languagemodels. ArXiv,
abs/2403.12736,2024.
[17] DannyDriess,F.Xia,MehdiS.M.Sajjadi,CoreyLynch,AakankshaChowdhery,BrianIchter,Ayzaan
Wahid, Jonathan Tompson, Quan Ho Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre
Sermanet,DanielDuckworth,SergeyLevine,VincentVanhoucke,KarolHausman,MarcToussaint,Klaus
Greff,AndyZeng,IgorMordatch,andPeterR.Florence. Palm-e:Anembodiedmultimodallanguage
model. InInternationalConferenceonMachineLearning,2023.
[18] PengGao,JiamingHan,RenruiZhang,ZiyiLin,ShijieGeng,AojunZhou,W.Zhang,PanLu,Conghui
He, Xiangyu Yue, Hongsheng Li, and Yu Jiao Qiao. Llama-adapter v2: Parameter-efficient visual
instructionmodel. ArXiv,abs/2304.15010,2023.
[19] JiaxinGe,SanjaySubramanian,BaifengShi,RoeiHerzig,andTrevorDarrell. Recursivevisualprogram-
ming. ArXiv,abs/2312.02249,2023.
[20] TaoGe,JingHu,XunWang,Si-QingChen,andFuruWei.In-contextautoencoderforcontextcompression
inalargelanguagemodel. ArXiv,abs/2307.06945,2023.
[21] TaoGong,ChengqiLyu,ShilongZhang,YudongWang,MiaoZheng,QianmengkeZhao,KuikunLiu,
WenweiZhang,PingLuo,andKaiChen. Multimodal-gpt:Avisionandlanguagemodelfordialogue
withhumans. ArXiv,abs/2305.04790,2023.
[22] TanmayGuptaandAniruddhaKembhavi. Visualprogramming:Compositionalvisualreasoningwithout
training. 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages
14953–14962,2022.
[23] DannaGurari,QingLi,AbigaleStangl,AnhongGuo,ChiLin,KristenGrauman,JieboLuo,andJeffreyP.
Bigham. Vizwizgrandchallenge: Answeringvisualquestionsfromblindpeople. 2018IEEE/CVF
ConferenceonComputerVisionandPatternRecognition,pages3608–3617,2018.
[24] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross B. Girshick. Momentum contrast for
unsupervisedvisualrepresentationlearning.2020IEEE/CVFConferenceonComputerVisionandPattern
Recognition(CVPR),pages9726–9735,2019.
[25] Roee Hendel, Mor Geva, and Amir Globerson. In-context learning creates task vectors. ArXiv,
abs/2310.15916,2023.
[26] RoeiHerzig,AlonMendelson,LeonidKarlinsky,AssafArbelle,RogerioFeris,TrevorDarrell,andAmir
Globerson. Incorporatingstructuredrepresentationsintopretrainedvision\&languagemodelsusing
scenegraphs. InThe2023ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,2023.
[27] AlbertoHojel,YutongBai,TrevorDarrell,AmirGloberson,andAmirBar. Findingvisualtaskvectors.
2024.
[28] NeilHoulsby,AndreiGiurgiu,StanislawJastrzebski,BrunaMorrone,QuentinDeLaroussilhe,Andrea
Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for NLP. In
Proceedingsofthe36thInternationalConferenceonMachineLearning,pages2790–2799,2019.
[29] EdwardJHu,YelongShen,PhillipWallis,ZeyuanAllen-Zhu,YuanzhiLi,SheanWang,LuWang,and
WeizhuChen. Lora:Low-rankadaptationoflargelanguagemodels. arXivpreprintarXiv:2106.09685,
2021.
[30] J.EdwardHu,YelongShen,PhillipWallis,ZeyuanAllen-Zhu,YuanzhiLi,SheanWang,andWeizhu
Chen. Lora:Low-rankadaptationoflargelanguagemodels. ArXiv,abs/2106.09685,2021.
[31] ZhiqiangHu,YihuaiLan,LeiWang,WanyuXu,Ee-PengLim,RoyKa-WeiLee,LidongBing,and
SoujanyaPoria. Llm-adapters:Anadapterfamilyforparameter-efficientfine-tuningoflargelanguage
models. ArXiv,abs/2304.01933,2023.
11[32] DrewA.HudsonandChristopherD.Manning. Gqa: Anewdatasetforreal-worldvisualreasoning
andcompositionalquestionanswering. 2019IEEE/CVFConferenceonComputerVisionandPattern
Recognition(CVPR),pages6693–6702,2019.
[33] ChaoJia,YinfeiYang,YeXia,Yi-TingChen,ZaranaParekh,HieuPham,QuocV.Le,Yun-HsuanSung,
ZhenLi,andTomDuerig. Scalingupvisualandvision-languagerepresentationlearningwithnoisytext
supervision. InInternationalConferenceonMachineLearning,2021.
[34] DongfuJiang,XuanHe,HuayeZeng,CongWei,MaxW.F.Ku,QianLiu,andWenhuChen. Mantis:
Interleavedmulti-imageinstructiontuning. arXiv2405.01483,2024.
[35] HuiqiangJiang, QianhuiWu, Chin-YewLin, YuqingYang, andLiliQiu. Llmlingua: Compressing
promptsforacceleratedinferenceoflargelanguagemodels. InConferenceonEmpiricalMethodsin
NaturalLanguageProcessing,2023.
[36] TakeshiKojima,ShixiangShaneGu,MachelReid,YutakaMatsuo,andYusukeIwasawa. Largelanguage
modelsarezero-shotreasoners. ArXiv,abs/2205.11916,2022.
[37] RanjayKrishna,YukeZhu,OliverGroth,JustinJohnson,KenjiHata,JoshuaKravitz,StephanieChen,
YannisKalantidis,Li-JiaLi,DavidAShamma,etal. Visualgenome:Connectinglanguageandvision
usingcrowdsourceddenseimageannotations. InternationalJournalofComputerVision,123(1):32–73,
2017.
[38] HugoLaurenccon,LucileSaulnier,LéoTronchon,StasBekman,AmanpreetSingh,AntonLozhkov,
ThomasWang,SiddharthKaramcheti,AlexanderM.Rush,DouweKiela,MatthieuCord,andVictorSanh.
Obelisc:Anopenweb-scalefiltereddatasetofinterleavedimage-textdocuments. ArXiv,abs/2306.16527,
2023.
[39] HugoLaurenccon,LéoTronchon,MatthieuCord,andVictorSanh. Whatmatterswhenbuildingvision-
languagemodels? 2024.
[40] BinLei,Pei-HungLin,ChunhuaLiao,andCaiwenDing. Boostinglogicalreasoninginlargelanguage
modelsthroughanewframework:Thegraphofthought. ArXiv,abs/2308.08614,2023.
[41] BrianLester,RamiAl-Rfou,andNoahConstant. Thepowerofscaleforparameter-efficientprompt
tuning. InConferenceonEmpiricalMethodsinNaturalLanguageProcessing,2021.
[42] JunnanLi,DongxuLi,CaimingXiong,andStevenHoi. Blip:Bootstrappinglanguage-imagepre-training
forunifiedvision-languageunderstandingandgeneration. arXivpreprintarXiv:2201.12086,2022.
[43] JunnanLi,DongxuLi,SilvioSavarese,andStevenHoi. BLIP-2: bootstrappinglanguage-imagepre-
trainingwithfrozenimageencodersandlargelanguagemodels. InICML,2023.
[44] MukaiLi,ShansanGong,JiangtaoFeng,YihengXu,JinchaoZhang,ZhiyongWu,andLingpengKong.
In-contextlearningwithmanydemonstrationexamples. ArXiv,abs/2302.04931,2023.
[45] TianleLi,GeZhang,QuyDucDo,XiangYue,andWenhuChen. Long-contextllmsstrugglewithlong
in-contextlearning. 2024.
[46] XiangLisaLiandPercyLiang.Prefix-tuning:Optimizingcontinuouspromptsforgeneration.Proceedings
ofthe59thAnnualMeetingoftheAssociationforComputationalLinguisticsandthe11thInternational
JointConferenceonNaturalLanguageProcessing(Volume1:LongPapers),abs/2101.00190,2021.
[47] JiLin,HongxuYin,WeiPing,YaoLu,PavloMolchanov,AndrewTao,HuiziMao,JanKautz,Mohammad
Shoeybi,andSongHan. Vila:Onpre-trainingforvisuallanguagemodels,2023.
[48] Tsung-YiLin,M.Maire,SergeJ.Belongie,JamesHays,P.Perona,D.Ramanan,PiotrDollár,andC.L.
Zitnick. Microsoftcoco:Commonobjectsincontext. InECCV,2014.
[49] HaotianLiu,ChunyuanLi,YuhengLi,andYongJaeLee. Improvedbaselineswithvisualinstruction
tuning,2023.
[50] HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee.Visualinstructiontuning. InNeurIPS,2023.
[51] NelsonF.Liu,KevinLin,JohnHewitt,AshwinParanjape,MicheleBevilacqua,FabioPetroni,andPercy
Liang. Lostinthemiddle:Howlanguagemodelsuselongcontexts. TransactionsoftheAssociationfor
ComputationalLinguistics,12:157–173,2023.
12[52] PanLu,BaolinPeng,HaoCheng,MichelGalley,Kai-WeiChang,YingNianWu,Song-ChunZhu,and
JianfengGao. Chameleon:Plug-and-playcompositionalreasoningwithlargelanguagemodels. ArXiv,
abs/2304.09842,2023.
[53] HuanMa,ChangqingZhang,YataoBian,LemaoLiu,ZhiruiZhang,PeilinZhao,ShuZhang,H.Fu,
QinghuaHu, andBingWu. Fairness-guidedfew-shotpromptingforlargelanguagemodels. ArXiv,
abs/2303.13217,2023.
[54] KennethMarino,MohammadRastegari,AliFarhadi,andRoozbehMottaghi. Ok-vqa:Avisualquestion
answeringbenchmarkrequiringexternalknowledge. 2019IEEE/CVFConferenceonComputerVision
andPatternRecognition(CVPR),pages3190–3199,2019.
[55] SewonMin, XinxiLyu, AriHoltzman, MikelArtetxe, MikeLewis, HannanehHajishirzi, andLuke
Zettlemoyer. Rethinkingtheroleofdemonstrations: Whatmakesin-contextlearningwork? ArXiv,
abs/2202.12837,2022.
[56] ChancharikMitra,BrandonHuang,TrevorDarrell,andRoeiHerzig. Compositionalchainofthought
promptingforlargemultimodalmodels. InProceedingsoftheIEEE/CVFConferenceonComputer
VisionandPatternRecognition(CVPR),2024.
[57] JesseMu,XiangLisaLi,andNoahD.Goodman. Learningtocompresspromptswithgisttokens. ArXiv,
abs/2304.08467,2023.
[58] Maria-ElenaNilsbackandAndrewZisserman. Automatedflowerclassificationoveralargenumber
ofclasses. 2008SixthIndianConferenceonComputerVision,Graphics&ImageProcessing,pages
722–729,2008.
[59] OpenAI. Gpt-4technicalreport. ArXiv,abs/2303.08774,2023.
[60] AdamPaszke,SamGross,FranciscoMassa,AdamLerer,JamesBradbury,GregoryChanan,Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-
performancedeeplearninglibrary. Advancesinneuralinformationprocessingsystems,32,2019.
[61] JudeaPearl. Directandindirecteffects. ProbabilisticandCausalInference,2001.
[62] BowenPeng, JeffreyQuesnelle, HongluFan, andEnricoShippole. Yarn: Efficientcontextwindow
extensionoflargelanguagemodels. ArXiv,abs/2309.00071,2023.
[63] YujiaQin,ShiLiang,YiningYe,KunlunZhu,LanYan,Ya-TingLu,YankaiLin,XinCong,Xiangru
Tang,BillQian,SihanZhao,RunchuTian,RuobingXie,JieZhou,MarcH.Gerstein,DahaiLi,Zhiyuan
Liu,andMaosongSun. Toolllm:Facilitatinglargelanguagemodelstomaster16000+real-worldapis.
ArXiv,abs/2307.16789,2023.
[64] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,SandhiniAgarwal,Girish
Sastry,AmandaAskell,PamelaMishkin,JackClark,etal. Learningtransferablevisualmodelsfrom
naturallanguagesupervision. InInternationalConferenceonMachineLearning, pages8748–8763.
PMLR,2021.
[65] ColinRaffel,NoamM.Shazeer,AdamRoberts,KatherineLee,SharanNarang,MichaelMatena,Yanqi
Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text
transformer. J.Mach.Learn.Res.,21:140:1–140:67,2019.
[66] TanikSaikh,TirthankarGhosal,AmishMittal,AsifEkbal,andPushpakBhattacharyya. Scienceqa:a
novelresourceforquestionansweringonscholarlyarticles. InternationalJournalonDigitalLibraries,
23:289–301,2022.
[67] VictorSanh,AlbertWebson,ColinRaffel,StephenBach,LintangSutawika,ZaidAlyafeai,AntoineChaf-
fin,ArnaudStiegler,ArunRaja,MananDey,MSaifulBari,CanwenXu,UrmishThakker,ShanyaSharma
Sharma,ElizaSzczechla,TaewoonKim,GunjanChhablani,NihalNayak,DebajyotiDatta,Jonathan
Chang,MikeTian-JianJiang,HanWang,MatteoManica,ShengShen,ZhengXinYong,HarshitPandey,
RachelBawden,ThomasWang,TrishalaNeeraj,JosRozen,AbheeshtSharma,AndreaSantilli,Thibault
Fevry,JasonAlanFries,RyanTeehan,TevenLeScao,StellaBiderman,LeoGao,ThomasWolf,and
AlexanderMRush. Multitaskpromptedtrainingenableszero-shottaskgeneralization. InInternational
ConferenceonLearningRepresentations,2022.
[68] TimoSchick,JaneDwivedi-Yu,RobertoDessì,RobertaRaileanu,MariaLomeli,LukeZettlemoyer,
NicolaCancedda,andThomasScialom. Toolformer:Languagemodelscanteachthemselvestousetools.
ArXiv,abs/2302.04761,2023.
13[69] ChuyiShang,AmosYou,SanjaySubramanian,TrevorDarrell,andRoeiHerzig. Traveler:Amulti-lmm
agentframeworkforvideoquestion-answering. ArXiv,abs/2404.01476,2024.
[70] YongliangShen,KaitaoSong,XuTan,DongShengLi,WeimingLu,andYueTingZhuang. Hugginggpt:
Solvingaitaskswithchatgptanditsfriendsinhuggingface. ArXiv,abs/2303.17580,2023.
[71] CharlesBurtonSnell,DanKlein,andRuiqiZhong.Learningbydistillingcontext.ArXiv,abs/2209.15189,
2022.
[72] Sanjay Subramanian, Medhini G. Narasimhan, Kushal Khangaonkar, Kevin Yang, Arsha Nagrani,
Cordelia Schmid, Andy Zeng, Trevor Darrell, and Dan Klein. Modular visual question answering
viacodegeneration. ArXiv,abs/2306.05392,2023.
[73] QuanSun,YufengCui,XiaosongZhang,FanZhang,QiyingYu,ZhengxiongLuo,YuezeWang,Yongming
Rao,JingjingLiu,TiejunHuang,andXinlongWang. Generativemultimodalmodelsarein-context
learners. ArXiv,abs/2312.13286,2023.
[74] D’idacSur’is,SachitMenon,andCarlVondrick. Vipergpt: Visualinferenceviapythonexecutionfor
reasoning. ArXiv,abs/2303.08128,2023.
[75] SijunTan,XiuyuLi,ShishirG.Patil,ZiyangWu,TianjunZhang,KurtKeutzer,JosephE.Gonzalez,and
RalucaA.Popa. Lloco:Learninglongcontextsoffline. 2024.
[76] YiTay,MostafaDehghani,VinhQ.Tran,XavierGarcía,JasonWei,XuezhiWang,HyungWonChung,
DaraBahri,TalSchuster,HuaixiuStevenZheng,DennyZhou,NeilHoulsby,andDonaldMetzler. Ul2:
Unifyinglanguagelearningparadigms. InInternationalConferenceonLearningRepresentations,2022.
[77] GeminiTeam. Gemini1.5:Unlockingmultimodalunderstandingacrossmillionsoftokensofcontext.
ArXiv,abs/2403.05530,2024.
[78] GeminiTeam,RohanAnil,SebastianBorgeaud,YonghuiWu,Jean-BaptisteAlayrac,JiahuiYu,Radu
Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable
multimodalmodels. arXivpreprintarXiv:2312.11805,2023.
[79] EricTodd,MillicentLi,ArnabSenSharma,AaronMueller,ByronC.Wallace,andDavidBau. Function
vectorsinlargelanguagemodels. ArXiv,abs/2310.15213,2023.
[80] CatherineWah,SteveBranson,PeterWelinder,PietroPerona,andSergeJ.Belongie. Thecaltech-ucsd
birds-200-2011dataset. 2011.
[81] XingchenWan,RuoxiSun,HanjunDai,SercanÖ.Arik,andTomasPfister. Betterzero-shotreasoning
withself-adaptiveprompting. InAnnualMeetingoftheAssociationforComputationalLinguistics,2023.
[82] LeiWang,YilangHu,JiabangHe,XingdongXu,NingLiu,HuijuanLiu,andHengtaoShen. T-sciq:
Teachingmultimodalchain-of-thoughtreasoningvialargelanguagemodelsignalsforsciencequestion
answering. ArXiv,abs/2305.03453,2023.
[83] Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim.
Plan-and-solveprompting:Improvingzero-shotchain-of-thoughtreasoningbylargelanguagemodels. In
AnnualMeetingoftheAssociationforComputationalLinguistics,2023.
[84] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Huai hsin Chi, and Denny Zhou. Self-
consistencyimproveschainofthoughtreasoninginlanguagemodels. ArXiv,abs/2203.11171,2022.
[85] ZhenhailongWang,ManlingLi,RuochenXu,LuoweiZhou,JieLei,XudongLin,ShuohangWang,Ziyi
Yang,ChenguangZhu,DerekHoiem,Shih-FuChang,MohitBansal,andHengJi. Languagemodelswith
imagedescriptorsarestrongfew-shotvideo-languagelearners. ArXiv,abs/2205.10747,2022.
[86] JasonWei,MaartenBosma,VincentZhao,KelvinGuu,AdamsWeiYu,BrianLester,NanDu,AndrewM.
Dai,andQuocV.Le. Finetunedlanguagemodelsarezero-shotlearners. ArXiv,abs/2109.01652,2021.
[87] JasonWei,YiTay,RishiBommasani,ColinRaffel,BarretZoph,SebastianBorgeaud,DaniYogatama,
MaartenBosma,DennyZhou,DonaldMetzler,EdHuaihsinChi,TatsunoriHashimoto,OriolVinyals,
PercyLiang,JeffDean,andWilliamFedus. Emergentabilitiesoflargelanguagemodels. Trans.Mach.
Learn.Res.,2022,2022.
[88] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Huai hsin Chi, F. Xia, Quoc Le,
and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. ArXiv,
abs/2201.11903,2022.
14[89] RonaldJ.Williams. Simplestatisticalgradient-followingalgorithmsforconnectionistreinforcement
learning. MachineLearning,8:229–256,2004.
[90] ChenfeiWu,Sheng-KaiYin,WeizhenQi,XiaodongWang,ZechengTang,andNanDuan.Visualchatgpt:
Talking,drawingandeditingwithvisualfoundationmodels. ArXiv,abs/2303.04671,2023.
[91] BenfengXu,AnYang,JunyangLin,QuangWang,ChangZhou,YongdongZhang,andZhendongMao.
Expertprompting:Instructinglargelanguagemodelstobedistinguishedexperts. ArXiv,abs/2305.14688,
2023.
[92] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik
Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. ArXiv,
abs/2305.10601,2023.
[93] YaoYao,Z.Li,andHaiZhao. Beyondchain-of-thought,effectivegraph-of-thoughtreasoninginlarge
languagemodels. ArXiv,abs/2305.16582,2023.
[94] QinghaoYe,HaiyangXu,GuohaiXu,JiaboYe,MingYan,YiZhou,JunyanWang,AnwenHu,Pengcheng
Shi,YayaShi,ChenliangLi,YuanhongXu,HehongChen,JunfengTian,QiangQi,JiZhang,andFeiyan
Huang. mplug-owl: Modularization empowers large language models with multimodality. ArXiv,
abs/2304.14178,2023.
[95] QinghaoYe, HaiyangXu, JiaboYe, MingshiYan, AnwenHu, HaoweiLiu, QiQian, JiZhang, Fei
Huang,andJingrenZhou.mplug-owl2:Revolutionizingmulti-modallargelanguagemodelwithmodality
collaboration. ArXiv,abs/2311.04257,2023.
[96] QingruZhang,MinshuoChen,AlexanderW.Bukharin,PengchengHe,YuCheng,WeizhuChen,and
TuoZhao. Adaptivebudgetallocationforparameter-efficientfine-tuning. ArXiv,abs/2303.10512,2023.
[97] RenruiZhang,JiamingHan,AojunZhou,XiangfeiHu,ShilinYan,PanLu,HongshengLi,PengGao,
andYuJiaoQiao.Llama-adapter:Efficientfine-tuningoflanguagemodelswithzero-initattention.ArXiv,
abs/2303.16199,2023.
[98] ZhuoshengZhang,AstonZhang,MuLi,andAlexanderJ.Smola. Automaticchainofthoughtprompting
inlargelanguagemodels. ArXiv,abs/2210.03493,2022.
[99] ZhuoshengZhang,AstonZhang,MuLi,HaiZhao,GeorgeKarypis,andAlexanderJ.Smola.Multimodal
chain-of-thoughtreasoninginlanguagemodels. ArXiv,abs/2302.00923,2023.
[100] HaozheZhao,ZefanCai,ShuzhengSi,XiaojianMa,KaikaiAn,LiangChen,ZixuanLiu,ShengWang,
Wenjuan Han, and Baobao Chang. Mmicl: Empowering vision-language model with multi-modal
in-contextlearning. ArXiv,abs/2309.07915,2023.
[101] GeZheng,BinYang,JiajinTang,Hong-YuZhou,andSibeiYang. Ddcot:Duty-distinctchain-of-thought
promptingformultimodalreasoninginlanguagemodels. ArXiv,abs/2310.16436,2023.
[102] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing
vision-languageunderstandingwithadvancedlargelanguagemodels. arXivpreprintarXiv:2304.10592,
2023.
15Multimodal Task Vectors Enable Many-Shot Multimodal
In-Context Learning
SupplementaryMaterial
Here,weprovideadditionalinformationaboutourexperimentalresults,qualitativeexamples,imple-
mentationdetails,anddatasets. Specifically,SectionAprovidesmoreexperimentresults,SectionB
providesadditionalmethoddetails,SectionCprovidesadditionalimplementationdetails,andSec-
tionDprovidesqualitativevisualizationstoillustrateourapproach.
A AdditionalExperimentResults
WepresentseveraladditionalexperimentsthatfurtherdemonstratethebenefitsofourMTVapproach.
A.1 AdditionalExperiments
HereweprovideadditionalablationsthatfurtherillustratedifferentcharacteristicsofMTV.
AttentionheadgeneralizationonobjectclassificationtasksTable4a. Wealsotestgeneralization
forobjectclassificationtasksidenticaltotheformulationdescribedinSection5.3. Forclarity,MTV
showsanotherkindofgeneralizationwhenitisleveragedalongsideadditionalexplicitICLsamples.
ThiscapabilityisdescribedinSection5.2. Tosummarizeourexperiment,wecalculateMTVusing
theFlowersdatasetusing1-shotICLexamplefor100iterationsforboththemeanactivationsµMTV
j
andtheattentionheadlocationsλMTV. Then,weapplyMTVtotheCUBtaskusingthesamesetof
j
attentionheadlocationsfromFlowers. WejustcalculatethemeanactivationsfortheCUBdataset
usinga1-shotfor100iterations(halvingourdatarequirementforthisspecificscenario). Onceagain,
wefindthattheheadsofMTVcanindeedgeneralizebetweensimilarclasses.
Table4: Generalization&DirectICLComparison(Left)MTV-FlowersevaluatedonOK-VQA.
(Right)DirectcomparisonofMTVextractedfrom4-shots,1-iteration(MTV_4shot_1it)comparedto
4-shotICL
(a)AttentionHeadGeneralization (b)ComparisontoOtherMethods
Model Flowers CUB Model VizWiz OK-VQA
ViLA-1.5-8B ViLA-1.5-8B 28.0 32.8
+1-shot-ICL 87.4 88.4 +4-shot-ICL 39.3 35.6
+MTV-Flowers+1-shot-ICL 89.3 89.9 +MTV_4shot_1it 57.4 40.0
MTVone-to-onecomparisonwithICLTable4b. Althoughnotdirectlycomparable,weconsider
an extreme case of MTV where we encode only 4-shots of ICL examples for 1 iteration. This
matchestheexactsettingusedinstandard4-shotICL.Interestingly,MTVappliedtobothVizWiz
andOK-VQAexceedsperformanceonthe4-shot-ICLcaseandevenMTVformulatedon4-shotsper
100iterationsforcalculatingthemeanactivations. Thisresultsuggeststhattheremaybescopefor
MTVtobeeffectiveinbothhighandlow-dataregimens. Moreresearchneedstobedonetoexplore
thisidea.
Effectofpermutationorderofexamples. Weconsiderapplyingfiverandomseedstoboth4-shot-
ICLandMTVextractedon4-shotsper100iterationsonVizWiz. Wefindthe4-shot-ICLaverageand
standarddeviationtobe41.3%(±.8%)andtheMTVaverageandstandarddeviationtobe45.2%
(±.7%). ThissuggeststhatMTVisstableacrossdifferentpermutationsofthegivenICLexamples.
ScalingonFlowersDataset. WeprovideadditionalresultsonthescalingpropertyofMTVonthe
Flowersdataset. Weagainnotethattheexamplesare2-way,one-shotexampleswith2examples
(onepositiveandonenegative)foreachsample. Asinthemainpaper,wefix1shotperiterationto
calculatethemeanactivations,scalingupto500totalexamplesused. Ourresultsshowthatthereis
asaturationofMTVat100examples(i.e.,1exampleper100iterations). Whilethisstillindicates
somescalingastheresultisanimprovementover20examples,theresultsshowthatthetaskvector
1Figure3: Efficiency. WeshowthatforFlowers,MTVdoesscaletobutonlyupto100examplesin
ourexperiments.
canreachitsbestaccuracywithfewershotsdependingonthecomplexityofthetask. Futureworkto
probemoredeeplyintothescalingnatureofMTVacrossdifferenttaskswouldbevaluable.
B AdditionalMethodDetails
HereweprovidesomeadditionalmethoddetailsaboutMTV,VisualTaskVectors(VTV)[27],and
FunctionVectors[79](FV).
B.1 MTV-EXTRACT
WedescribetheparticularsofourMTV-EXTRACTalgorithmforfindingthesetofattentionhead
locationsthatbestalignwiththedownstreamtaskasfollows(Q andR areformattedidenticallyto
s s
thedownstreamtask):
Algorithm1MTV-EXTRACTforfindingtaskvectorlocations
Require: F (LMM),S (examples),µ (meanactivations),Q ,R (queriesandresponses)
j s s
Ensure: λMTV (optimizedattentionheadlocations)
j
1: Initializeθrandomly
2: fors←1toS do
3: fori←1to32do ▷Samplingheads32times
4: Sampleλ i ∼Bernoulli(σ(θ))
5: Replaceactivationsforλ iinF withµ l,j
6: ComputeoutputlogitsO s ←F(Q s) ▷PassQ stoLMMF
7: L i ←NegativeCross-Entropy(O s,R s)
8: endfor
9: θ ←Adam(θ,∇ θ31 2(cid:80)3 i=2 1L i) ▷Updaterule
10: endfor
11: SamplefinalλMTV ∼Bernoulli(σ(θ)) ▷Finalsetofheadlocations
j
12: returnλMTV
j
We point out a few important factors. It is important to note that none of the parameters of F
are being finetuned through any gradient update. We take the negative cross-entropy (negative
as MTV_EXTRACT draws inspiration from REINFORCE [89], which is a policy optimization
algorithm)betweentheoutputlogitsO andthefirsttokenofthetargetresponseR forasimple
s s
updatescheme. Thisalongwiththechoiceof32samplesoftheBernoullidistributionareoneswe
encouragemoreexperimentationwithinfuturework.
2B.2 VisualTaskVectors(VTV)AdaptationforMultimodalICL
VisualTaskVectors(VTV)[27]wereoriginallydesignedtobeappliedtolargevision-transformer-
basedmodels. Wemakeasfewchangesaspossibletoapplythismethodformultimodaltasks. We
preserveVTVsdistinctfactorslikeatheusageof1-shotexamplesforbothcalculationofthemean
activationsandattentionheadlocationsregardlessoftheformatofthedownstreamtask. Furthermore,
wefixthenumberofiterationsforbothmeanactivationandattentionheadcalculationat10. Finally,
wereplacetheproposedMSElosswithacross-entropylossthatismoresuitedforanLMMtask.
B.3 FunctionVectors(FV)
BecauseFunctionVectorsdescribetext-onlytaskvectors,wefollowtheimplementationofFunction
Vectors[79]almostexactlyasLLMsandLMMsaresimilar. Theonlymajorchangemadeistheuse
ofmany-shotmultimodalICLexamplesformeanactivationcalculation. Wepreservethelackofan
optimizationmethodforthelayerusedtoreplacethemeanactivations. Ratherthanperforminga
standardgridsearchoverthesetoflayers,wesetthelayernumberto20asrecommendedforLLaMA
andLLaMA-basedmodelsbythepaper. Theonlyotherdifferenceistheencodingofmultimodal
ICLexamples. Again,duethethesimilaritybetweenLMMsandtext-onlyLLMs,thesetestscanbe
usedasneededaslongasthemultimodalinputsareproperlyprocessedbytheLMM.
C AdditionalImplementationDetails
Torunallofourexperiments, weuse1NVIDIARTX6000GPU.Importantly, thisincludesthe
runtimeandefficiencyablations,whichwereevaluatedonthesameGPUforconsistency. Please
refer to the respective model’s paper for their specific implementation details of the architecture.
Besidestheoutputtokengenerationlength,whichvariesdependingonthestandardsettingforeach
task,weusethedefaultgenerationparameters(e.g. temperatureandno. ofbeamsinbeamsearch)
recommendedforeachmodel. Inthefollowingsections,wedescribesomeofthefinernuancesof
ourMTV-EXTRACTprocessaswellasourimplementationsoftheVisualTaskVectors(VTV)and
FunctionVectors(FV)implementations.
C.1 VizWiz
Dataset. TheVizWizdatasetisdesignedtochallengeandevaluatethecapabilitiesofLargeMul-
timodal Models (LMMs) in understanding and responding to real-world visual questions. This
dataset is comprised of images accompanied by spoken questions, which have been transcribed
andpairedwithanswers. Eachimageinthisdatasetissourcedfromvisuallyimpairedindividuals
seekingassistance,therebyincorporatingawidearrayofeverydaychallengestheyface. Thissetup
isinherentlydiverseandoftenrequireshigh-levelvisualunderstandingcombinedwithcontextual
reasoning,makingthemarobustbenchmarkforassessingthepracticalutilityofLMMsinassistive
technologies. Theformatofthedatasetsamplesisanimagepairedwithatextquestion. TheLMM
isrequiredtoprovideashortresponselimitedto10tokensorrespondwith“unanswerable"ifthe
questionisnotanswerablegivetheimage.
Forthisresearchpaper,wespecificallyutilizetheVizWizdatasettobenchmarktheperformanceof
ourproposedtaskvectorsinmultimodalin-contextlearning(MM-ICL)onadatasetthatchallenges
visual scene understanding of LMMs. We extract MTV on the training set and evaluate on the
evaluationsetcontaining4,319validationimage/questionpairs.
Inference details. We use the standard VQA question-answer response format that is outlined
in the QwenVL repository https://github.com/QwenLM/Qwen-VL. Put simply, the LMM is
presented with an image and a corresponding text question. The response is then expected in a
short text format of no more than 10 tokens (set as the “max_tokens” parameter in the LMM).
One nuance is the special answer “unanswerable". We handle this by providing MTV and all
baselines with the following prompt for every question: “First carefully understand the given
examples. Thenusethegivenimageandanswerthequestioninthesamewayastheexamples. If
thequestioncannotbeanswered,respondunanswerable. "Theofficialdatasetcanbedownloaded
athttps://vizwiz.org/tasks-and-datasets/vqa/.
3C.2 OK-VQA
Dataset. TheOK-VQAdataset,differsfromtraditionalVQAdatasetsinitsfocusonnecessitating
knowledgebeyondwhatispresentedinthegivenimages. Thisdatasetencompassesover14,000
questionsthatarenotmerelyreliantonvisualcuesbutrequireassociativereasoningwithexternaldata
sources,makingitauniquetoolforevaluatingAI’scapabilityinhandlingcomplex,knowledge-driven
queries. Thus,weevaluateonthisdatasettotestwhetherMTVcanbebeneficialforthistypeof
reasoning.
WeonceagainextractMTVonthetrainsetandevaluateonthevalidationset. OK-VQAisformatted
asanimagewithacorrespondingtextquestion. However,itisimportanttonotethatthetextquestion
heavilyreliesonexternalknowledgetoanswer. ExamplesofquestionscanbefoundinSectionD.
Inferencedetails. WeusethestandardVQAquestion-answerresponseformatthatisoutlinedinthe
QwenVLrepositoryhttps://github.com/QwenLM/Qwen-VL. Putsimply,theLMMispresented
withanimageandacorrespondingtextquestion. Theresponseisthenexpectedinashorttextformat
ofnomorethan10tokens(setasthe“max_tokens”parameterintheLMM).Wedonotaddany
additionalpromptsorspecialtokensapartfrompromptformatorimagetokensrequiredbythemodel
beingevaluated. Theofficialdatasetcanbedownloadedathttps://okvqa.allenai.org/.
C.3 Flowers
Dataset. Flowers[58]isanobjectclassificationdatasetthatrequiresfine-grainedclassificationof
102differentflowerspecies. TheFlowersdatasetisformulatedasa2-way,1-shottaskwhereone
exampleisthepositivesampleandtheotheristhenegativesample. Inthisway,thedataposesa
uniquechallengeforMTVhavingtostoreexampleswithtwoassociatedimages. Thus,giventhe
2-wayexamplesandthequeryimage,theLMMistaskedwithselectingthecorrectclassfromthe
giventwooptions. ExamplescanbefoundinSectionD
Implementation Details. We use the official data released by the authors which is available at
https://www.robots.ox.ac.uk/~vgg/data/flowers/. We provide a Python code snippet
belowshowingtheFlowersdataformat:
def format_flower(cur_data):
pos = cur_data["pos"]
neg = cur_data["neg"]
pos_label = cur_data["pos_label"]
neg_label = cur_data["neg_label"]
query = cur_data["query"]
rand_num = random.randint(0,1)
if rand_num == 0:
pos_example = f"<img>{pos}</img>What is the type of flower in the image? A.{
pos_label} B.{neg_label}\nAnswer with the option’s letter from the given
choice directly. Answer: A\n"
neg_example = f"<img>{neg}</img>What is the type of flower in the image? A.{
pos_label} B.{neg_label}\nAnswer with the option’s letter from the given
choice directly. Answer: B\n"
cur_query = f"<img>{query}</img>What is the type of flower in the image? A.{
pos_label} B.{neg_label}\nAnswer with the option’s letter from the given
choice directly. Answer:"
query_label = "A"
return pos_example + neg_example + cur_query, query_label, -1
else:
pos_example = f"<img>{pos}</img>What is the type of flower in the image? A.{
neg_label} B.{pos_label}\nAnswer with the option’s letter from the given
choice directly. Answer: B\n"
neg_example = f"<img>{neg}</img>What is the type of flower in the image? A.{
neg_label} B.{pos_label}\nAnswer with the option’s letter from the given
choice directly. Answer: A\n"
4cur_query = f"<img>{query}</img>What is the type of flower in the image? A.{
neg_label} B.{pos_label}\nAnswer with the option’s letter from the given
choice directly. Answer:"
query_label = "B"
return neg_example + pos_example + cur_query, query_label, -1
C.4 CUB
Dataset. CUB[80]orCUB-200-2011isanobjectclassificationdatasetthatteststhefine-grained
classificationof200classesofbirds. SimilartotheFlowersdataset,CUBisformulatedasa2-way,
1-shottaskwhereoneexampleisthepositivesampleandtheotheristhenegativesample. Inthisway,
thedataposesauniquechallengeforMTVhavingtostoreexampleswithtwoassociatedimages.
Thus,giventhe2-wayexamplesandthequeryimage,theLMMistaskedwithselectingthecorrect
classfromthegiventwooptions.
Implementation Details. We use the official data released by the authors which is available at
https://www.vision.caltech.edu/datasets/cub_200_2011/. We provide a Python code
snippetbelowshowingtheFlowersdataformat:
def format_cub(cur_data):
pos = cur_data["pos"]
neg = cur_data["neg"]
pos_label = cur_data["pos_label"]
neg_label = cur_data["neg_label"]
query = cur_data["query"]
rand_num = random.randint(0,1)
if rand_num == 0:
pos_example = f"<img>{pos}</img>What is the type of bird in the image? A.{
pos_label} B.{neg_label}\nAnswer with the option’s letter from the given
choice directly. Answer: A\n"
neg_example = f"<img>{neg}</img>What is the type of bird in the image? A.{
pos_label} B.{neg_label}\nAnswer with the option’s letter from the given
choice directly. Answer: B\n"
cur_query = f"<img>{query}</img>What is the type of bird in the image? A.{
pos_label} B.{neg_label}\nAnswer with the option’s letter from the given
choice directly. Answer:"
query_label = "A"
return pos_example + neg_example + cur_query, query_label, -1
else:
pos_example = f"<img>{pos}</img>What is the type of bird in the image? A.{
neg_label} B.{pos_label}\nAnswer with the option’s letter from the given
choice directly. Answer: B\n"
neg_example = f"<img>{neg}</img>What is the type of bird in the image? A.{
neg_label} B.{pos_label}\nAnswer with the option’s letter from the given
choice directly. Answer: A\n"
cur_query = f"<img>{query}</img>What is the type of bird in the image? A.{
neg_label} B.{pos_label}\nAnswer with the option’s letter from the given
choice directly. Answer:"
query_label = "B"
return neg_example + pos_example + cur_query, query_label, -1
D QualitativeVisualizations
WepresentfurtherqualitativesuccessandfailurecasesofQwenVL-MTVinFigure4onOK-VQA
andFlowers.
5Figure4: Examples. Hereweshowexampleoutputsofourmethodcomparedtozero-shotoutputs.
E LicensesandPrivacy
Thelicense,PII,andconsentdetailsofeachdatasetareintherespectivepapers. Inaddition,wewish
toemphasizethatthedatasetsweusedonotcontainanyharmfuloroffensivecontent,asmanyother
papersinthefieldalsousethem. Thus,wedonotanticipateaspecificnegativeimpact,but,aswith
anymachinelearningmethod,werecommendexercisingcaution.
6