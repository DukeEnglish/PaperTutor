LongRAG: Enhancing Retrieval-Augmented Generation
with Long-context LLMs
ZiyanJiang XueguangMa WenhuChen
UniversityofWaterloo UniversityofWaterloo UniversityofWaterloo
ziyanjiang528@gmail.com x93ma@uwaterloo.ca wenhuchen@uwaterloo.ca
https://tiger-ai-lab.github.io/LongRAG/
Abstract
Traditional RAG
Retrieval Ranker
In traditional RAG framework, the basic re-
trieval units are normally short. The com- Reader
monretrieverslikeDPRnormallyworkwith
100-word Wikipedia paragraphs. Such a de- LongRAG Long Retrieval
signforcestheretrievertosearchoveralarge
corpus to find the ‘needle’ unit. In contrast, Long Reader
thereadersonlyneedtoextractanswersfrom
theshortretrievedunits. Suchanimbalanced
‘heavy’retrieverand‘light’readerdesigncan
lead to sub-optimal performance. In order
toalleviatetheimbalance, weproposeanew
framework LongRAG, consisting of a ‘long
retriever’anda‘longreader’. LongRAGpro-
cessestheentireWikipediainto4K-tokenunits,
which is 30x longer than before. By increas-
ing the unit size, we significantly reduce the
total units from 22M to 600K. This signifi-
cantly lowers the burden of retriever, which
leadstoaremarkableretrievalscore: answer
recall@1=71% on NQ (previously 52%) and
answer recall@2=72% (previously 47%) on
HotpotQA(full-wiki). Thenwefeedthetop-k
retrievedunits(≈30Ktokens)toanexisting
long-contextLLMtoperformzero-shotanswer
extraction.Withoutrequiringanytraining,Lon-
gRAG achieves an EM of 62.7% on NQ and
Figure 1: Traditional RAG vs. LongRAG. (Up) Tra-
64.3% on HotpotQA (full-wiki), which is on
ditional RAG operates on short retrieval units, where
parwiththeSoTAmodel. Ourstudyoffersin-
retriever needs to scan over massive amount of units
sightsintothefutureroadmapforcombining
to find the relevant piece. In contrast, LongRAG op-
RAGwithlong-contextLLMs.
erates on long retrieval units (30x longer). Retriever
1 Introduction hasamuchlessworkload, whichsignificantlyboosts
the recall score. LongRAG fully exploits the ability
Retrieval-Augmented Generation (RAG) meth- of long-context language models (reader) to achieve
ods have long been employed to enhance large strongperformance. (Middle)LongRAGvs. otherRAG
language models (LLMs) (Mialon et al., 2023). methods on NQ. Blue model names indicate models
without fine-tuning, while red model names indicate
Knowledge in the form of natural language can
modelswithfine-tuning. (Down)LongRAGvs. other
be entirely offloaded from the parametric knowl-
RAGmethodsonHotpotQA.
edgeofLLMsbyleveragingastandaloneretrieval
componentfromanexternalcorpus. Theexisting
RAGframeworktendstouseshortretrievalunits, tiny retrieval unit) from the “haystack” (i.e. the
suchas100-wordpassagesinpopularopen-domain massivecorpuswithtensofmillionsofinformation
questionansweringtasks(Chenetal.,2017;Lewis units). Subsequently,theretrievedunitsarepassed
etal.,2020;Karpukhinetal.,2020). Theretriever tothereadertogeneratethefinalresponse. Onthe
istaskedwithfindingthe“needle”(i.e. theprecise contrary,thereaderonlyneedstoextractanswers
1
4202
nuJ
12
]LC.sc[
1v91351.6042:viXrafrom these retrievals, which is a fairly easy task. et al., 2019) and HotpotQA (Yang et al., 2018).
This kind of imbalanced design, with a “heavy” LongRAG has several advantages: 1) It does not
retrieveranda“light”reader,putstoomuchpres- requireadditionalre-rankers, andthebestresults
sureontheretriever. Therefore,thestate-of-the-art can be attained by only considering the top 4-8
RAGmodels(IzacardandGrave,2020b)needto retrieved units. 2) The long retrieval unit amal-
recall huge amount of units, such as the top-100 gamatescomprehensiveinformationfromrelated
orevenmore,combinedwithadditionalcomplex documents,whichcanbeuseddirectlytoanswer
re-rankertoachievegreatperformance. Moreover, multi-hopquestionswithoutiterativeretrieval.
short retrieval units can lead to semantic incom- In our experiments, we adopt off-the-shelf re-
pleteness due to document truncation. This can triever like BGE (Xiao et al., 2023) and reader
leadtoinformationloss,ultimatelyrestrictingthe like Gemini-1.5-Pro (Reid et al., 2024) or GPT-
end performance. This traditional design choice 4o (OpenAI, 2024) without any tuning on NQ or
oftheRAGframeworkwasmadeinanerawhen HotpotQA.Inourexperiments,wereducetheNQ
NLPmodelswereheavilyrestrictedbytheirability corpus size from 22M to 600K document units,
tohandlelongcontexts. Withtherecentadvances which improves the answer recall@1 from 52%
in long-context language models, the reader can (DPR)to71%. Similarly,wereducetheHotpotQA
potentiallyhandleupto128Korevenmillionsof corpussizefrom5Mto500K,whichimprovesthe
tokens as input (Reid et al., 2024; Achiam et al., recall@2from47%(DPR)to72%. Theimprove-
2023). Inthispaper,weproposetorevisitthisde- mentinretrievercansignificantlybenefitthereader
sign choice for open-domain question answering model. Byexploitingthelong-contextunderstand-
andproposeLongRAGframeworkasasolutionto ing ability of GPT-4o, LongRAG can achieve an
balancetheworkloadbetweentheretrieverandthe EMof62%onNQand64%onHotpotQA.These
reader, as illustrated in Figure 1. There are three results could be comparable to the strongest fine-
importantdesignsinournovelframework: tunedRAGmodelslikeAtlas(Izacardetal.,2022)
and MDR (Xiong et al., 2020b). Through these
1. Long Retrieval Unit: By using entire
experiments, we have verified the effectiveness
Wikipedia documents or grouping multiple
of LongRAG in handling open domain question
relateddocuments,wecanconstructlongre-
answering. We believe LongRAG could provide
trievalunitswithmorethan4Ktokens. This
strong insights into how we build modern RAG
designcouldalsosignificantlyreducethecor-
system. Meanwhile,thereisstillroomforimprove-
pussize(numberofretrievalunitsinthecor-
ment in our framework, particularly the need for
pus). Then,theretriever’staskbecomesmuch
strongerlongembeddingmodels,asshowninTable
easier. Additionally, the long retrieval unit
3. Additionally, more general methods to formu-
will also improve the information complete-
latelongretrievalunitsbeyondhyperlinkswillbe
nesstoavoidambiguityorconfusion.
helpful.
2. LongRetriever: Thelongretrieverwilliden-
2 RelatedWork
tifycoarserelevantinformationforthegiven
query by searching through all the long re- 2.1 Retrieval-AugmentedGeneration.
trievalunitsinthecorpus. Thetop4to8re-
Augmentinglanguagemodelswithinformationre-
trievalunitsareconcatenatedastheretrieved
trieved from large corpora has become a popular
longcontextforthenextstep.
and effective approach for knowledge-intensive
tasks, particularly open-domain question answer-
3. Long Reader: The long reader will further
ing. The predominant architecture follows a
extractanswersfromtheconcatenationofre-
retriever-readerstyle(Chenetal.,2017;Guuetal.,
trievals,whichisnormallyaround30Ktokens.
2020), where the input query retrieves informa-
We simply prompt an existing long-context
tionfromacorpus,andalanguagemodelusesthis
LM(likeGeminiorGPT4)withthequestion
information as additional context to make a final
toproducetheanswers.
prediction. Recent work has focused on improv-
These three novel designs significantly boost ing the retriever (Karpukhin et al., 2020; Xiong
theoverallperformanceofRAGonopen-domain etal.,2020a;Quetal.,2020;Xiongetal.,2020b;
question answering tasks like NQ (Kwiatkowski Khalifaetal.,2023),enhancingthereader(Izacard
2and Grave, 2020b; Cheng et al., 2021; Yu et al., LLM content window extension methods on em-
2021; Borgeaud et al., 2022), fine-tuning the re- beddingmodels(Zhuetal.,2024a;PengandQues-
trieverandreaderjointly(Yu,2022;Izacardetal., nelle,2023),orbyemployingstate-spaceencoder
2022;Singhetal.,2021;IzacardandGrave,2020a), models(Saad-Falconetal.,2024).
and integrating the retriever with the black-box
3 LongRAG
languagemodel(Yuetal.,2023;Shietal.,2023;
Trivedietal.,2022). However,theimpactofdocu-
OurproposedLongRAGframeworkiscomprised
mentgranularityontheeffectivenessandefficiency
of two components: the Long Retriever and the
oftheretrieval-augmentedgenerationpipelinere-
LongReader. Anillustrativeexampleofthesetwo
mainsunderexplored.
componentsaredepictedinFigure2.
2.2 LongContextLargeLanguageModels.
3.1 LongRetriever
TheeffectivenessofTransformer-basedmodelsis
ThetraditionalRAGframeworkemployssmaller
hinderedbythequadraticincreaseincomputational
retrieval units and prioritizes retrieving the exact
cost relative to sequence length, especially when
fine-grained short context containing the answer.
dealingwithlongcontextinputs. Inordertosolve
In contrast, our proposed LongRAG framework
thisissue,differentapproacheshavebeenproposed
placesgreateremphasisonrecall,aimingtoretrieve
tomitigatecomputationalissues,includingsliding
relevantcontextwithmuchcoarsegranularity. This
memory window and chunk segmentation (Hao
designchoiceshiftsmoreburdenfromtheretriever
etal.,2022;Ratneretal.,2023;Zhuetal.,2024b).
tothereadertoextracttheexactanswersfromthe
FlashAttention(Daoetal.,2022)hasalsobeena
relevantcontext.
pivotalstrategytosignificantlyreducethememory
We denote our corpus for retrieval as C =
footprinttoalmostlinearw.r.tsequencelength.
{d ,d ,...,d }, which is a collection of D doc-
1 2 D
Toenablelengthextrapolation,RoPE(Suetal.,
uments. Formally speaking, the long context re-
2021) and AliBI (Press et al., 2021) position en-
triever is a function: F : (q,C) → C that takes
F
codingshaveshownpotentialtoenablelengthex-
asinputaquestionq andacorpusC andreturnsa
trapolation, which have been widely used in the
filtered set of texts C ⊂ C. In traditional RAG,
F
literature. Recentendeavorshaveexploreddiverse
C isusuallysmallwhichcontainsabouthundred
F
strategiestotacklethischallenge,whichismainly
oftokens,whichshouldcontainexactinformation
Positionreorganization(Jinetal.,2024;Anetal.,
related to the question q. In our framework, C
F
2024),Positioninterpolation(Chenetal.,2023a;
is usually more than 4K tokens, which contains
Peng et al., 2023; Liu et al., 2024). Furthermore,
relavant but not exact information related to the
alternative architectures beyond the Transformer
questionq. ThelongretrieverfunctionF : (q,C)
havebeenexploredtohandlelonginputsmorenat-
isthendividedintothreesteps:
urally. These diverse approaches claim that they
canenhancethecapabilitiesofLLMsinprocessing Formulate long retrieval units A function is
longcontextinputsmoreefficiently. applied to the corpus to form M retrieval units:
G(C) = {g ,g ,...,g }. IntraditionalRAG,the
1 2 M
2.3 LongContextEmbedding
retrievalunitg istypicallyashortspanofpassage
Recent efforts also increased the context length which is split from the documents d, containing
for embedding models, extending the supported hundredsoftokens. Inourframework,g couldbe
text snippet length from a limit of 512 tokens to as long as the whole document or even a group
32k tokens. Typically, the development of long- of documents, resulting in much longer retrieval
context embedding models involves first obtain- units. Wegroupthedocumentsbasedontheirrela-
ing a long-context backbone model. This can tionships,usinghyperlinksembeddedwithineach
be achieved either by pre-training with long in- document. ThegroupingalgorithmisshowninAl-
putsfromscratch(Güntheretal.,2023;Nussbaum gorithm1. Theoutputgroupisalistofdocuments
et al., 2024; Chen et al., 2024) or by utilizing ex- thatarerelatedtoeachother. Byhavingalonger
isting large language models that support longer retrievalunit,therearetwoadvantages: First,iten-
context (Wang et al., 2023). Additionally, some suresthesemanticintegrityofeachretrievalunit;
works extend the capabilities of existing embed- Second,itprovidesmuchrichercontextfortasks
ding models to handle long contexts by applying thatrequireinformationfrommultipledocuments.
3Query Corpus
Document 1: [Kiss and Tell (play)]
… A film version written by Herbert
and based on his play was released by Documents What government position
Columbia Pictures on October 4, 1945 was held by the woman who
with Shirley Temple in the role of portrayed Corliss Archer in
Corliss Archer … the film Kiss and Tell?
Document 2: [Kiss and Tell (1945
film)]
Grouping
Kiss and Tell is a 1945 American
comedy film starring then 17-year-old
Long Retriever
Shirley Temple as Corliss Archer …
Long Reader
Document 3: [Shirley Temple]
… Later, she was named United States
Ambassador to Ghana and
Czechoslovakia, and also served as
Chief of
Chief of Protocol of the United States Answer
Group Documents Protocol
…
Figure 2: LongRAG example. On the left side, it shows that the long retrieval unit is grouped by Wikipedia
documentsthroughhyperlinks. Eachretrievalunitcontainsanaverageof4Ktokens,correspondingtomultiple
relateddocuments. Ontherightside,itshowsamulti-hopquestionanswertestcasefromHotpotQA.Thefinal
resultcanbeachievedbyusingonlyafewretrievalunits,whichisthenfedintoalongreader.
Algorithm1GroupDocumentsAlgorithm Similarity search We utilize an encoder, de-
Input: S (max group size), D (documents), noted as E Q(·), to map the input question to a
adj[d] (related documents for each d), deg(d) d-dimensionalvector. Additionally,weemploya
(numberofrelateddocumentsforeachd) differentencoder,E C(·),tomaptheretrievalunit
Output: G (setofgroups) toad-dimensionalvector. Wedefinethesimilarity
SortD fromlowdegree(deg(d))tohighdegree between the question and the retrieval unit using
InitializeanemptysetofgroupsG thedotproductoftheirvectors:
foreachdocumentdinD do sim(q,g) = E (q)TE (g)
Q C
related_groups ← ∅
foreachrelateddocumentr inadj[d]do In LongRAG settings, E C(g) is challenging
foreachgroupg inG do giventhelengthofg,soweresorttoanapproxima-
ifr ∈ g then tionasbelow.
related_groups ← sim(q,g) = E (q)TE (g)
Q C
related_groups∪{g}
≈ max(E (q)TE (g′))
endif Q C
g′⊆g
endfor
Weapproximateitbymaximizingthescoresofall
endfor
chunksg′ withintheretrievalunitg. Weconsider
Createanewgroupg = {d}
new different levels of granularity, including passage
Sortrelated_groupsbytheirsize
level, document level, and the complete grouped
foreachgroupg inrelated_groupsdo
document. Theempiricalstudyaboutthissettings
if|g |+|g| ≤ S then
new isinTable3. Withthissimilarityscoresetup,we
g ← g ∪g
new new willretrievethetopk retrievalunitsclosesttothe
Removeg fromG
givenquery. Forefficientretrieval,weprecompute
endif
the embedding of each retrieval unit g′ and pre-
endfor
dicttheexactinnerproductsearchindexinFAISS
Addg toG
new (Johnsonetal.,2019).
endfor
returnG Aggregate retrieval result We will concate-
nate the top k retrieval units into the long con-
text as the retrieval result, denoted by C =
F
4Concat(g1,g2,...,gk). Depending on the selec- articlesidentifiedbyannotators. Thisdatasetcon-
tion of retrieval units, a larger retrieval unit size tains3,610questions.
willresultinasmallervalueofk beingused. For
HotpotQA (Yang et al., 2018) consists of two-
instance,iftheretrievalunitisapassage,k isap-
hopquestionsoverdiversetopics. Wefocusonthe
proximately above 100; if it’s a document, k is
fullwikisettinginwhichtwoWikipediapassages
around10;andforgroupeddocumentsasretrieval
arerequiredtoanswerthequestions. Sincethegold
units,wetypicallysetk to4to8.
passagesforthetestsetarenotavailable,wefollow
3.2 LongReader priorwork(Xiongetal.,2020b)andevaluateonthe
developmentset,whichhas7,405questions. There
The long reader operates straightforwardly. We
aretwomainquestiontypesinHotpotQA:(1)com-
feed the related instruction i, the question q, and
parisonquestionsusuallyrequirecontrastingtwo
thelongretrievalresultC intoanLLM,enabling
F entitiesand(2)bridgequestionscanbeanswered
ittoreasonoverthelongcontextandgeneratethe
byfollowingaconnectingentitythatlinksonedoc-
finaloutput. It’simportantthattheLLMusedinthe
umenttoanother.
longreadercanhandlelongcontextsanddoesnot
exhibitexcessivepositionbias. WeselectGemini- Wikipedia(KnowledgeSource) Weusediffer-
1.5-Pro (Reid et al., 2024) and GPT-4o (OpenAI, ent versions of English Wikipedia for different
2024)asourlongreadergiventheirstrongability datasets following previous works (Lewis et al.,
tohandlelongcontextinput. 2020; Yang et al., 2018). For NQ, we use the
We utilize different approaches for short and WikipediadumpsfromDecember20,2018,which
longcontexts. Forshortcontexts,typicallycontain- containapproximately3milliondocumentsand22
ingfewerthan1Ktokens,weinstructthereaderto million passages. For HotpotQA, we use the ab-
directlyextracttheanswerfromtheprovidedcon- stractparagraphsfromtheOctober1,2017dump,
textretrievedfromthecorpus. Forlongcontexts, which contain around 5 million documents. For
typically longer than 4K tokens, we empirically eachpage, onlytheplaintextisextractedandall
find that using a similar prompt as for short con- structured data sections such as lists, tables and
texts, where the model extracts the final answer figuresarestrippedfromthedocument.
directly from the long context, often leads to de-
4.2 RetrievalPerformance
creased performance. Instead, the most effective
approach is to utilize the LLM as a chat model. Metrics Retrieval performance is measured us-
Initially, it outputs a long answer, typically span- ingAnswerRecall(AR)andRecall(R).ForNQ,
ningafewwordstoafewsentences. Subsequently, we use only answer recall, while for HotpotQA,
wepromptittogenerateashortanswerbyfurther we use both metrics. Answer Recall is the recall
extractingitfromthelonganswer. Thepromptis oftheanswerstringinalltheretrieveddocuments
providedintheAppendix6.1. that we plan to use in the reader. For example,
if the retrieval unit is at the “passage” level and
4 Experiments thenumberofretrievalunitsis100,answerrecall
measures whether the answer string is present in
In this section, we will first detail the dataset we
these 100 passages. For HotpotQA, we compute
adopt, and then demonstrate the retriever perfor-
ARonlyforquestionswithspananswers,specifi-
mance. Finally, we will show the end question-
cally the “bridge” type questions, while ignoring
answeringperformance.
yes/no and comparison questions, following pre-
viouswork(Khalifaetal.,2022). Recallusedfor
4.1 Data
HotpotQA measures whether the two gold docu-
OurproposedmethodsaretestedontwoWikipedia- ments are present in all the retrieved results. For
relatedquestionansweringdatasets: NaturalQues- example,iftheretrievalunitisatthe“document”
tionsandHotpotQA. levelandthenumberofretrievalunitsis10,recall
measureswhetherbothgolddocumentsarepresent
Natural Question (Kwiatkowski et al., 2019)
amongthe10retrieveddocuments.
was designed for end-to-end question answering.
ThequestionswereminedfromrealGooglesearch Experiment Setup We leverage open-sourced
queriesandtheanswerswerespansinWikipedia denseretrievaltoolkit,Tevatron(Gaoetal.,2022),
5AverageNumofTokens
RetrievalUnit CorpusSize NumofRetrievalUnits AnswerRecall(AR)
Corpus TestSet
1 120 130 52.24
Passage 22M 100 12K 14K 89.92
200 24K 28K 91.30
1 820 4K 69.45
Document 3M 5 4K 18K 85.37
10 8K 34K 88.12
1 4K 6K 71.69
GroupedDocuments 600K 4 16K 25K 86.30
8 32K 50K 88.53
Table1: ThetableillustratestheretrievalperformanceonNQ.Employingalong-contextretriever(withanaverage
numberoftokensforeachretrievalunitupto6K)compressesthecorpussizebyupto30times(from22Mto
600K),enhancingtop-1answerrecallbyapproximately20points(from52.24to71.69). Furthermore,long-context
retrieval requires significantly fewer retrieval units (10 times fewer) to achieve comparable results. Therefore,
integratinglong-contextretrievalsignificantlyalleviatestheburdenontheretrievermodel.
forallourretrievalexperiments. Thebaseembed- notabledifferenceisthatinHotpotQA,theretrieval
dingmodelweusedisbge-large-en-v1.5,ageneral- units areonly at the documentlevel and grouped
purposeembeddingsmodelthatisn’tspecifically document level, as HotpotQA uses only abstract
trainedonourtestdata. paragraphsfromeachWikipediapage.
Table 1 and Table 2 have shown the retrieval Encode the long retrieval unit As discussed
results on NQ and HotpotQA. In the NQ dataset, in Section 3.2, it’s very challenging to employ
we utilize three different retrieval units, ranging an encoder, E C(·), to map the retrieval unit g
from shorter to longer: passage, document, and to a d-dimensional vector when g is very long.
grouped documents. In the table, we have men- Therefore, we use an approximation in our pro-
tioned two kinds of average number of tokens in posed system. Table 3 demonstrates that our
each retrieval unit: one for the entire corpus and approximation, sim(q,g) = E Q(q)TE C(g) ≈
one for each test set. The retrieval units for each max g′⊆g(E Q(q)TE C(g′)),ismuchmoreeffective
test case can sometimes be much longer than the thanencodingtheentirelongcontextdirectly. We
average size across the whole corpus, as the cor- comparethreemethods: 1)Usingthegeneralem-
pusmightincludesomeWikipediapageswithvery bedding model “bge-large-en-v1.5” (Xiao et al.,
few words, while the test cases may focus more 2023), with g′ selected as text of 512-token size.
onlongerdocuments. Generally,ourlong-context 2)Usinglongembeddingmodel“E5-Mistral-7B”
retriever(atthedocumentlevelandgroupeddocu- (Zhu et al., 2024a), with g′ selected as the whole
mentlevel)usesretrievalunitscontaininganaver- document,whichhasanaveragesizeof4Ktokens.
ageof6Ktokens. Byusinglongerretrievalunits, 3)Usinglongembeddingsmodel“E5-Mistral-7B”,
thereareseveraladvantages: 1)Itwillsignificantly with no approximation, encoding the entire g di-
alleviatetheburdenontheretrieverbycompressing rectly, where g has an average size of 6K tokens.
the corpus size by approximately 30 times, from Wecannoticefromthetablethatourapproxima-
22M to 600K. The top-1 answer recall improves tion by taking the maximum score between the
byabout20points,from52.24to71.69. Wecould query and each text piece from the long context
use significantly fewer retrieval units to achieve producesmuchbetterresultsthanencodingthem
comparableretrievalperformance. Forinstance,8 directlyusingthelongembeddingmodel. Webe-
retrieval units at the grouped document level can lievefutureimprovementsintheresearchdirection
achievesimilarrecallas100retrievalunitsatthe oflongembeddingmodelswillfurtherenhanceour
passage level. 2) It could provide more compre- frameworktoreducememoryconsumption.
hensive information to the reader. In the original
4.3 FullQAPerformance
passage-level RAG setup, information might be
incompleteduetothechunkingoperation. Inthe We leverage Gemini-1.5-Pro and GPT-4o as the
HotpotQAdataset,weobservesimilarresults. One reader in our LongRAG framework. The prompt
6AverageNumofTokens Recall AnswerRecall
RetrievalUnit CorpusSize NumofRetrievalUnits
(R) (AR)
Corpus TestSet
2 130 200 30.01 47.75
Document 5.2M 100 6.5K 10K 74.84 84.67
200 13K 20K 79.68 88.34
2 1K 8K 56.30 72.49
GroupedDocuments 500K 8 4K 29K 74.71 84.40
Table2: ThetableillustratestheretrievalperformanceonHotpotQA.SimilartothefindingsonNQ,along-context
retrievalcouldsignificantlyalleviatetheburdenontheretrievercomponentwithintheentireRAGframework.
Model Granularity AR@1 Method EM
BGE-Large 512-tokenspassage 71.7% Closed-Book
E5-Mistral-7B document(Avg4Ktokens) 54.2% Claude-3-Opus(Anthropic,2024) 32.8
E5-Mistral-7B groupeddocuments(Avg6Ktokens) 19.6% Gemini-1.5-Pro(Reidetal.,2024) 33.9
GPT-4-Turbo(Achiametal.,2023) 42.4
Table3: Differentmethodstoencodethelongretrieval
Fully-supervisedRAG
unitinthelongretriever. Usingageneralembedding
CogQA(Dingetal.,2019) 37.1
modelandapproximatingbymaximizingthesimilarity DrKIT(Dhingraetal.,2020) 42.1
scores between the query and all chunks within the Transformer-XH(Zhaoetal.,2019) 51.6
retrievalunit isbetter than usingthe longembedding QAMAT+(Chenetal.,2023b) 57.6
HGN(Fangetal.,2019) 59.7
modeltoencodetheentirecontext.
PathRetriever(Asaietal.,2019) 60.0
HopRetrieve(Lietal.,2021) 62.1
MDR(Xiongetal.,2020b) 62.3
Method EM
HopRetrieve-plus(Lietal.,2021) 66.5
Closed-Book AISO(Zhuetal.,2021) 68.1
GPT-4-Turbo(Achiametal.,2023) 41.2 COS(Maetal.,2023) 68.2
Gemini-1.5-Pro(Reidetal.,2024) 47.8
NoFine-tuningRAG
Claude-3-Opus(Anthropic,2024) 49.2
DSP(Khattabetal.,2022) 51.4
Fully-supervisedRAG PromptRank(Khalifaetal.,2023) 55.7
REALM(Guuetal.,2020) 40.4 LongRAG(Gemini-1.5-Pro;Recall8units) 57.5
DPR(Karpukhinetal.,2020) 41.5 LongRAG(GPT-4o;Recall8units) 64.3
RAG(Lewisetal.,2020) 44.5
RETRO(Borgeaudetal.,2022) 45.5
Table5: ThetableshowstheQAresultsontheHotpot-
RePAQ(Lewisetal.,2021) 47.8
QAdevset.Wecomparetheresultswiththreegroupsof
Fusion-in-Decoder(IzacardandGrave,2020b) 51.4
EMDR2(Singhetal.,2021) 52.5 baselines: closed-book,whichinvolvesdirectlyprompt-
Atlas(Izacardetal.,2022) 64.0 ingstate-of-the-artLLMswith16-shotin-contextexam-
ples;fully-supervisedRAG,wheretheRAGframework
NoFine-tuningRAG
REPLUG(Shietal.,2023) 45.5 is used and the model is fully supervised and trained
LongRAG(Gemini-1.5-Pro;Recall4units) 58.6 onthetrainingdata;andNoFine-tuningRAG,which
LongRAG(GPT-4o;Recall4units) 62.7 employstheRAGframeworkwithoutanytuning.
Table 4: The table shows the QA results on the NQ
group is “Closed-Book”: These baselines mean
dataset. We compare the results with three groups of
baselines: closed-book,whichinvolvesdirectlyprompt- thatnoretrievalcomponentisused;instead,state-
ingstate-of-the-artLLMswith16-shotin-contextexam- of-the-artLLMsareemployedtodirectlyobtainthe
ples;fully-supervisedRAG,wheretheRAGframework finalresult. WeevaluateourresultsonGemini-1.5-
is used and the model is fully supervised and trained pro(Reidetal.,2024),Claude-3-Opus(Anthropic,
onthetrainingdata;andNoFine-tuningRAG,which
2024)andGPT-4-Turbo(Achiametal.,2023). All
employstheRAGframeworkwithoutanytuning.
modelsareevaluatedon16-shotin-contextlearn-
ing with direct prompting; The second group is
“Fully-supervisedRAG”,andthesebaselinesin-
we use for our experiments are in Table 6. We
volve full-supervised fine-tuning on the training
alsorefinethestandardexactmatchratedefinition
dataset. Thethirdgroupis“NoFine-tuningRAG”,
to more fairly evaluate LongRAG’s performance.
andthesebaselinesdoesn’tinvolveanysupervised
MoredetailscanbefoundinSection6.2.
fine-tuningonthetrainingdataset.
We compare our model with several groups of
strong previous models as baselines. The first TheQAresultsonNQarepresentedinTable4,
770 65
Passage Document Grouped Documents
64
60 63
62
61
50
60
59
40
58
57
30 1 100200 1 5 10 1 4 8 56
Num of Retrieval Units 55
Gemini-1.5-pro GPT-4-Turbo GPT-4o
Reader Model
Figure 3: This figure compares different settings of
LongRAGontheNQdataset. Thistableleverages200 Figure5: ThisfigurecomparesdifferentreadersofLon-
test cases from the test set to help compare different gRAGontheNQdataset. Thistableleverages200test
retrievalunitselectionsandoptimalnumberofretrieval cases from the test set to help compare performance
unitsfedintothereader(Gemini-based). usingdifferentreaders.
relevant information from the long context. For
Document Grouped Documents
65 passage-level retrieval units, the turning point is
between100and200;fordocument-levelretrieval
55
units, the turning point is between 5 and 10; and
forgroupeddocumentslevel,theturningpointis
45
between4and8. Ingeneral,themostsuitablecon-
35 textlengthfedintothereaderisaround30Ktokens.
Second,thesemanticintegrityisimportantwhen
25
2 100 200 2 8 16
Num of Retrieval Units comparing the performance of passage-level re-
trievalunitswithdocumentorgroupeddocuments
Figure4: ThistablecomparesdifferentsettingsofLon-
levelretrievalunits,highlightingtheadvantageof
gRAGontheHotpotQAdataset. Thistableleverages
usinglongerandmorecompleteretrievalunits.
200testcasesfromthetestsettohelpcomparediffer-
entretrievalunitselectionsandtheoptimalnumberof
ReaderModel InFigure5,wecomparetheper-
retrievalunitsfedintothereader(Gemini-based).
formance of three different readers: Gemini-1.5-
pro, GPT-4-Turbo, andGPT-4o. Theresultsindi-
andtheQAresultsonHotpotQAarepresentedin catethatGPT-4oachievesthehighestexactmatch
Table 5. On the NQ dataset, LongRAG achieves scoreonthe200testquestionsoftheNQdataset
a 62.7 exact match rate, which is on par of the amongthethreemodels. ThissuggeststhatGPT-
strongest fine-tuned RAG model like Atlas. On 4oisthemosteffectiveintheroleofalongreader
theHotpotQAdataset,LongRAGachievesa64.3 intheLongRAGframework. Theenhancedperfor-
exactmatchrate,whichisalsoclosetotheSoTA manceofGPT-4ocanbeattributedtoitssuperior
fully-supervisedRAGframeworks. abilitytoprocessandcomprehendlengthycontexts,
ensuringthatcrucialinformationisaccuratelyex-
RetrievalUnitSelection Figure3andFigure4
tracted. Therefore, we mainly report the GPT-4o
comparedifferentsettingsofLongRAG.Thistable
resultsinourmaintable.
leverages 200 random test cases from the test set
to help compare different retrieval unit granular-
5 Conclusion
ity selection and the optimal number of retrieval
units used in the reader. On the NQ dataset, we Inthispaper,weproposeanewframework,Lon-
havetwoobservations: First,regardlessofwhich gRAG,toalleviatetheimbalancebetweenthebur-
retrieval unit is selected, there will be a turning den of the retriever. The LongRAG framework
point where feeding more retrieval units into the consists of a “long retriever” and a “long reader”
reader becomes detrimental. This is due to the componentontopofthe4K-tokenretrievalunits.
excessiveburdenplacedonthereader,preventing Ourproposedframeworkcansignificantlyreduce
it from effectively understanding and extracting the corpus size by 10 to 30 times, which greatly
8
)%(
hctaM
tcaxE
)%(
hctaM
tcaxE
)%(
hctaM
tcaxEimproves the recall of the retriever. On the other Lespiau, BogdanDamoc, AidanClark, etal.2022.
hand,thelongretrievalunitpreservesthesemantic Improvinglanguagemodelsbyretrievingfromtril-
lionsoftokens. InInternationalconferenceonma-
integrity of each document. We test our frame-
chinelearning,pages2206–2240.PMLR.
workonend-to-endquestionansweringtasksand
demonstrateitssuperiorperformancewithoutany DanqiChen,AdamFisch,JasonWeston,andAntoine
Bordes. 2017. Reading wikipedia to answer open-
training. WebelieveLongRAGcanpavetheroad
domainquestions. InProceedingsofthe55thAnnual
forthemodernRAGsystemdesign.
Meeting of the Association for Computational Lin-
guistics(Volume1: LongPapers),pages1870–1879.
Limitation
JianlvChen,ShitaoXiao,PeitianZhang,KunLuo,Defu
Therearethreemajorlimitationsofourproposed Lian, and Zheng Liu. 2024. Bge m3-embedding:
Multi-lingual,multi-functionality,multi-granularity
framework. First, it relies on the long embed-
textembeddingsthroughself-knowledgedistillation.
ding model. Although recent studies have made
arXivpreprintarXiv:2402.03216.
progress in this direction, there is still a need for
ShouyuanChen,ShermanWong,LiangjianChen,and
stronger long embedding models. In our work,
YuandongTian.2023a. Extendingcontextwindow
we use an approximation to calculate the seman-
oflargelanguagemodelsviapositionalinterpolation.
ticscorewitharegularembeddingmodel,which arXivpreprintarXiv:2306.15595.
provesmoreeffectivethanusingalongembedding
WenhuChen,PatVerga,MichieldeJong,JohnWieting,
model. Future improvements in long embedding
andWilliamCohen.2023b. Augmentingpre-trained
models could help us further enhance the perfor- languagemodelswithqa-memoryforopen-domain
mance of our system and reduce the storage size questionanswering. InProceedingsofthe17thCon-
of corpus embeddings if the entire long context ferenceoftheEuropeanChapteroftheAssociation
forComputationalLinguistics,pages1597–1610.
couldbeencodeddirectly. Thesecondlimitation
isthatweonlyuseablack-boxLLMasthereader. HaoCheng,YelongShen,XiaodongLiu,PengchengHe,
A reader that supports long input and is less af- WeizhuChen,andJianfengGao.2021. Unitedqa: A
hybridapproachforopendomainquestionanswering.
fected by position bias is necessary. Currently,
arXivpreprintarXiv:2101.00178.
mostopen-sourceLLMsdonotmeettheserequire-
ments. The third limitation is that our grouping Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and
Christopher Ré. 2022. Flashattention: Fast and
methods are based on hyperlinks, which are spe-
memory-efficientexactattentionwithio-awareness.
cific to the Wikipedia corpus. A more general
AdvancesinNeuralInformationProcessingSystems,
groupingmethodshouldbeconsidered. 35:16344–16359.
BhuwanDhingra,ManzilZaheer,VidhishaBalachan-
dran, Graham Neubig, Ruslan Salakhutdinov, and
References
William W Cohen. 2020. Differentiable reason-
ing over a virtual knowledge base. arXiv preprint
JoshAchiam,StevenAdler,SandhiniAgarwal,Lama
arXiv:2002.10640.
Ahmad, Ilge Akkaya, Florencia Leoni Aleman,
DiogoAlmeida,JankoAltenschmidt,SamAltman,
MingDing,ChangZhou,QibinChen,HongxiaYang,
ShyamalAnadkat,etal.2023. Gpt-4technicalreport.
andJieTang.2019. Cognitivegraphformulti-hop
arXivpreprintarXiv:2303.08774.
readingcomprehensionatscale. InProceedingsof
the57thAnnualMeetingoftheAssociationforCom-
Chenxin An, Fei Huang, Jun Zhang, Shansan Gong,
putationalLinguistics,pages2694–2703.
XipengQiu,ChangZhou,andLingpengKong.2024.
Training-freelong-contextscalingoflargelanguage YuweiFang,SiqiSun,ZheGan,RohitPillai,Shuohang
models. arXivpreprintarXiv:2402.17463. Wang, and Jingjing Liu. 2019. Hierarchical graph
network for multi-hop question answering. arXiv
Anthropic. 2024. Introducing the next generation of
preprintarXiv:1911.03631.
claude.
Luyu Gao, Xueguang Ma, Jimmy J. Lin, and Jamie
AkariAsai,KazumaHashimoto,HannanehHajishirzi, Callan. 2022. Tevatron: An efficient and flexible
RichardSocher,andCaimingXiong.2019. Learn- toolkitfordenseretrieval. ArXiv,abs/2203.05765.
ing to retrieve reasoning paths over wikipedia
graph for question answering. arXiv preprint MichaelGünther,JackminOng,IsabelleMohr,Alaed-
arXiv:1911.10470. dineAbdessalem,TanguyAbel,MohammadKalim
Akram,SusanaGuzman,GeorgiosMastrapas,Saba
Sebastian Borgeaud, Arthur Mensch, Jordan Hoff- Sturua, Bo Wang, et al. 2023. Jina embeddings 2:
mann, Trevor Cai, Eliza Rutherford, Katie Milli- 8192-tokengeneral-purposetextembeddingsforlong
can,GeorgeBmVanDenDriessche,Jean-Baptiste documents. arXivpreprintarXiv:2310.19923.
9KelvinGuu,KentonLee,ZoraTung,PanupongPasu- PatrickLewis,EthanPerez,AleksandaraPiktus,Fabio
pat,andMingweiChang.2020. Retrievalaugmented Petroni,VladimirKarpukhin,NamanGoyal,Hein-
languagemodelpre-training. InInternationalconfer- rich Kuttler, Mike Lewis, Wen tau Yih, Tim Rock-
enceonmachinelearning,pages3929–3938.PMLR. täschel, Sebastian Riedel, and Douwe Kiela. 2020.
Retrieval-augmented generation for knowledge-
YaruHao,YutaoSun,LiDong,ZhixiongHan,Yuxian intensivenlptasks. ArXiv,abs/2005.11401.
Gu,andFuruWei.2022. Structuredprompting:Scal-
ing in-context learning to 1, 000 examples. ArXiv, PatrickLewis,YuxiangWu,LinqingLiu,PasqualeMin-
abs/2212.06713. ervini,HeinrichKüttler,AleksandraPiktus,Pontus
Stenetorp,andSebastianRiedel.2021. Paq: 65mil-
GautierIzacardandEdouardGrave.2020a. Distilling
lionprobably-askedquestionsandwhatyoucando
knowledgefromreadertoretrieverforquestionan- withthem. TransactionsoftheAssociationforCom-
swering. arXivpreprintarXiv:2012.04584. putationalLinguistics,9:1098–1115.
Gautier Izacard and Edouard Grave. 2020b. Lever-
ShaoboLi,XiaoguangLi,LifengShang,XinJiang,Qun
aging passage retrieval with generative models for
Liu,ChengjieSun,ZhenzhouJi,andBingquanLiu.
open domain question answering. arXiv preprint
2021. Hopretriever: Retrieve hops over wikipedia
arXiv:2007.01282.
toanswercomplexquestions. InProceedingsofthe
AAAIconferenceonartificialintelligence,volume35,
Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas
pages13279–13287.
Hosseini, Fabio Petroni, Timo Schick, Jane A. Yu,
ArmandJoulin,SebastianRiedel,andEdouardGrave.
Jiaheng Liu, Zhiqi Bai, Yuanxing Zhang, Chenchen
2022. Few-shot learning with retrieval augmented
Zhang,YuZhang,GeZhang,JiakaiWang,Haoran
languagemodels. ArXiv,abs/2208.03299.
Que, Yukang Chen, Wenbo Su, et al. 2024. Eˆ 2-
Hongye Jin, Xiaotian Han, Jingfeng Yang, Zhimeng llm: Efficientandextremelengthextensionoflarge
Jiang,ZiruiLiu,Chia-YuanChang,HuiyuanChen, languagemodels. FindingsofACL2024.
andXiaHu.2024. Llmmaybelonglm: Self-extend
llmcontextwindowwithouttuning. arXivpreprint KaixinMa,HaoCheng,YuZhang,XiaodongLiu,Eric
arXiv:2401.01325. Nyberg, and Jianfeng Gao. 2023. Chain-of-skills:
Aconfigurablemodelforopen-domainquestionan-
JeffJohnson,MatthijsDouze,andHervéJégou.2019. swering. InThe61stAnnualMeetingOfTheAssoci-
Billion-scale similarity search with gpus. IEEE ationForComputationalLinguistics.
TransactionsonBigData,7(3):535–547.
GrégoireMialon,RobertoDessì,MariaLomeli,Christo-
VladimirKarpukhin,BarlasOg˘uz,SewonMin,Patrick forosNalmpantis,RamPasunuru,RobertaRaileanu,
Lewis,LedellWu,SergeyEdunov,DanqiChen,and Baptiste Rozière, Timo Schick, Jane Dwivedi-Yu,
Wen-tau Yih. 2020. Dense passage retrieval for AsliCelikyilmaz,etal.2023. Augmentedlanguage
open-domain question answering. arXiv preprint models: asurvey. arXivpreprintarXiv:2302.07842.
arXiv:2004.04906.
ZachNussbaum,JohnXMorris,BrandonDuderstadt,
Muhammad Khalifa, Lajanugen Logeswaran, Moon-
andAndriyMulyar.2024. Nomicembed: Training
tae Lee, Honglak Lee, and Lu Wang. 2022. Few-
a reproducible long context text embedder. arXiv
shotrerankingformulti-hopqavialanguagemodel
preprintarXiv:2402.01613.
prompting. arXivpreprintarXiv:2205.12650.
OpenAI.2024. Hellogpt4-o.
Muhammad Khalifa, Lajanugen Logeswaran, Moon-
tae Lee, Honglak Lee, and Lu Wang. 2023. Few-
Bowen Peng and Jeffrey Quesnelle. 2023. Ntk-
shotrerankingformulti-hopqavialanguagemodel
aware scaled rope allows llama models to
prompting. arXivpreprintarXiv:2205.12650.
have extended (8k+) context size without any
fine-tuning and minimal perplexity degrada-
Omar Khattab, Keshav Santhanam, Xiang Lisa
tion. https://www.reddit.com/r/LocalLLaMA/
Li, David Hall, Percy Liang, Christopher Potts,
comments/14lz7j5/ntkaware_scaled_rope_
and Matei Zaharia. 2022. Demonstrate-search-
allows_llama_models_to_have.
predict: Composing retrieval and language mod-
els for knowledge-intensive nlp. arXiv preprint
arXiv:2212.14024. BowenPeng,JeffreyQuesnelle,HongluFan,andEn-
ricoShippole.2023. Yarn: Efficientcontextwindow
TomKwiatkowski, JennimariaPalomaki, OliviaRed- extensionoflargelanguagemodels. arXivpreprint
field,MichaelCollins,AnkurParikh,ChrisAlberti, arXiv:2309.00071.
DanielleEpstein,IlliaPolosukhin,JacobDevlin,Ken-
tonLee,etal.2019. Naturalquestions: abenchmark OfirPress,NoahSmith,andMikeLewis.2021. Train
forquestionansweringresearch. Transactionsofthe short,testlong: Attentionwithlinearbiasesenables
Association for Computational Linguistics, 7:453– inputlengthextrapolation. InInternationalConfer-
466. enceonLearningRepresentations.
10YingqiQu,YuchenDing,JingLiu,KaiLiu,Ruiyang WenhanXiong,XiangLorraineLi,SriniIyer,Jingfei
Ren, Wayne Xin Zhao, Daxiang Dong, Hua Wu, Du, Patrick Lewis, William Yang Wang, Yashar
and Haifeng Wang. 2020. Rocketqa: An opti- Mehdad, Wen-tau Yih, Sebastian Riedel, Douwe
mizedtrainingapproachtodensepassageretrieval Kiela, et al. 2020b. Answering complex open-
foropen-domainquestionanswering. arXivpreprint domain questions with multi-hop dense retrieval.
arXiv:2010.08191. arXivpreprintarXiv:2009.12756.
NirRatner,YoavLevine,YonatanBelinkov,OriRam, Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-
Inbal Magar, Omri Abend, Ehud Karpas, Amnon gio,WilliamWCohen,RuslanSalakhutdinov,and
Shashua, KevinLeyton-Brown, andYoavShoham. ChristopherDManning.2018. Hotpotqa: Adataset
2023. Parallelcontextwindowsforlargelanguage fordiverse,explainablemulti-hopquestionanswer-
models. In Proceedings of the 61st Annual Meet- ing. arXivpreprintarXiv:1809.09600.
ingoftheAssociationforComputationalLinguistics
(Volume1: LongPapers),pages6383–6402,Toronto, Donghan Yu, Chenguang Zhu, Yuwei Fang, Wenhao
Canada.AssociationforComputationalLinguistics. Yu,ShuohangWang,YichongXu,XiangRen,Yim-
ing Yang, and Michael Zeng. 2021. Kg-fid: In-
Machel Reid, Nikolay Savinov, Denis Teplyashin, fusing knowledge graph in fusion-in-decoder for
Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste open-domain question answering. arXiv preprint
Alayrac,RaduSoricut,AngelikiLazaridou,OrhanFi- arXiv:2110.04330.
rat,JulianSchrittwieser,etal.2024. Gemini1.5: Un-
lockingmultimodalunderstandingacrossmillionsof Wenhao Yu. 2022. Retrieval-augmented generation
tokensofcontext. arXivpreprintarXiv:2403.05530. across heterogeneous knowledge. In Proceedings
ofthe2022ConferenceoftheNorthAmericanChap-
Jon Saad-Falcon, Daniel Y Fu, Simran Arora, Neel teroftheAssociationforComputationalLinguistics:
Guha,andChristopherRé.2024. Benchmarkingand Human Language Technologies: Student Research
buildinglong-contextretrievalmodelswithlocoand Workshop,pages52–58.
m2-bert. arXivpreprintarXiv:2402.07440.
Wenhao Yu, Zhihan Zhang, Zhenwen Liang, Meng
Weijia Shi, Sewon Min, Michihiro Yasunaga, Min- Jiang,andAshishSabharwal.2023. Improvinglan-
joon Seo, Rich James, Mike Lewis, Luke Zettle- guagemodelsviaplug-and-playretrievalfeedback.
moyer,andWen-tauYih.2023. Replug: Retrieval- arXivpreprintarXiv:2305.14002.
augmented black-box language models. arXiv
preprintarXiv:2301.12652. Chen Zhao, Chenyan Xiong, Corby Rosset, Xia
Song, Paul Bennett, and Saurabh Tiwary. 2019.
Devendra Singh, Siva Reddy, Will Hamilton, Chris Transformer-xh: Multi-evidencereasoningwithex-
Dyer,andDaniYogatama.2021. End-to-endtrain- tra hop attention. In International Conference on
ingofmulti-documentreaderandretrieverforopen- LearningRepresentations.
domain question answering. Advances in Neural
InformationProcessingSystems,34:25968–25981. DaweiZhu,LiangWang,NanYang,YifanSong,Wen-
haoWu,FuruWei,andSujianLi.2024a. Longem-
JianlinSu, YuLu, ShengfengPan, AhmedMurtadha, bed: Extendingembeddingmodelsforlongcontext
Bo Wen, and Yunfeng Liu. 2021. Roformer: En- retrieval. arXivpreprintarXiv:2404.12096.
hancedtransformerwithrotarypositionembedding.
arXivpreprintarXiv:2104.09864. DaweiZhu,NanYang,LiangWang,YifanSong,Wen-
hao Wu, Furu Wei, and Sujian Li. 2024b. PoSE:
Harsh Trivedi, Niranjan Balasubramanian, Tushar EfficientcontextwindowextensionofLLMsviapo-
Khot, and Ashish Sabharwal. 2022. Interleav- sitionalskip-wisetraining. InTheTwelfthInterna-
ing retrieval with chain-of-thought reasoning for tionalConferenceonLearningRepresentations.
knowledge-intensive multi-step questions. arXiv
preprintarXiv:2212.10509. YunchangZhu,LiangPang,YanyanLan,HuaweiShen,
andXueqiCheng.2021. Adaptiveinformationseek-
LiangWang,NanYang,XiaolongHuang,LinjunYang, ingforopen-domainquestionanswering. InProceed-
RanganMajumder,andFuruWei.2023. Improving ingsofthe2021ConferenceonEmpiricalMethods
textembeddingswithlargelanguagemodels. arXiv inNaturalLanguageProcessing,pages3615–3626.
preprintarXiv:2401.00368.
Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas 6 Appendix
Muennighoff. 2023. C-pack: Packaged re-
sources to advance general chinese embedding. 6.1 PromptsTemplateforLongContext
arXiv:2309.07597.
Reader
LeeXiong,ChenyanXiong,YeLi,Kwok-FungTang, Wehaveputoutpromptsusedfortheexperiments
JialinLiu,PaulBennett,JunaidAhmed,andArnold
in Table 6. For the closed-book method, we use
Overwijk. 2020a. Approximate nearest neighbor
16-shot in-context examples. For LongRAG, we
negativecontrastivelearningfordensetextretrieval.
arXivpreprintarXiv:2007.00808. useatwo-turnapproachtoextractthefinalanswer.
11Inthefirstturn,thelongretrievedcontextandthe
questionareconcatenatedasinput,andwedonot
useanyin-contextexampleshereduetothecontext
beingaround30Ktokens. Empirically,wefoundit
beneficialtoletthereadergeneratealongeranswer
initially, typically ranging from a few words to
a few sentences. In the second turn, we use 8-
shot in-context examples to guide the reader in
further extracting the most important part of the
longanswerastheshortanswer,whichistypically
justafewwords.
6.2 RefinedMetric
The most standard metric used in open-domain
question answering tasks is EM (Exact Match),
sincethecorrectanswermustbeasubstringwithin
the corpus. In our framework, since the long re-
trieved context, which contains multiple highly-
related documents to the given query, is fed into
thereader, thereisamuchhigherpossibilitythat
an alias of the ground truth exists in the context
and can be extracted by the reader. As shown in
Table 7, although LongRAG’s prediction doesn’t
exactly match the ground truth, it’s obvious that
LongRAG’s prediction is correct. To better and
morefairlyevaluateLongRAG’sperformance,we
haverefinedtheEMmetricslightly. Werecognize
it as an exact match if the prediction is less than
fivetokens(indicatingthattheshortanswerissuc-
cessfullyextractedasdescribedinSection6.1)and
thegroundtruthisasubstringofthepredictionor
vice versa. We have also manually verified that
thisrefinedmetricindeedcapturesaliasesorother
formsofthegroundtruth. Forthefully-supervised
RAGbaselinesusedinourpaper, giventhatthey
arefine-tunedonthetrainingdataandtheretrieval
unit is a small snippet, we believe that the differ-
ence won’t be significant when using the refined
EM.
6.3 DatasetLicenses
• NQ:ApacheLicense2.0
• HotpotQA:CCBY-SA4.0License
12Method Prompt
CLOSED- Here are some examples of questions and their corresponding answer, each
BOOK witha“Question”fieldandan“Answer”field. Answerthequestiondirectly
anddon’toutputotherthing.
“Question”: ...“Answer”: ...
“Question”: ...“Answer”: ...
“Question”: ...“Answer”: ...
...
“Question”: ...“Answer”: ...
Answerthefollowingquestion.
“Question”: whoistheownerofreadingfootballclub“Answer”:
LONGRAG Turn1: Gothroughthefollowingcontextandthenanswerthequestion. The
contextisalistofWikipediadocuments,orderedbytitle: ....
EachWikipediadocumentcontainsatitlefieldandatextfield. Thecontextis:
“Title”: ...“Text”: ...
“Title”: ...“Text”: ......
“Title”: ...“Text”: ...
Find the useful documents from the context, then answer the question: ....
Answerthequestiondirectly. Yourresponseshouldbeveryconcise.
Turn2: Youhavebeenprovidedwithaquestionanditslonganswer. Yourtask
istoderiveaveryconciseshortanswer,extractingasubstringfromthegiven
longanswer. Shortansweristypicallyanentitywithoutanyotherredundant
words. It’simportanttoensurethattheoutputshortanswerremainsassimple
aspossible. Hereafewexamples:
“Question”: ...“LongAnswer”: ...“ShortAnswer”: ...
“Question”: ...“LongAnswer”: ...“ShortAnswer”: ...
“Question”: ...“LongAnswer”: ...“ShortAnswer”: ...
Extracttheshortanswerofthefollowingquestionandlonganswer:
“Question”: whendidthephiladelphiaeaglesplayinthesuperbowllast“Long
Answer”: ThePhiladelphiaEagleslastplayedintheSuperBowlonFebruary4,
2018,inSuperBowlLII.“ShortAnswer”:
Table 6: Here are the prompts we used for all the experiments. For the closed-book method, we use 16-shot
in-contextexamples. ForLongRAG,weuseatwo-turnapproachtoextractthefinalanswer. Thefirstturndoesn’t
requireanyin-contextexamplesandgeneratealongeranswer,typicallyrangingfromafewwordstoafewsentences.
Inthesecondturn, weuse8-shotin-contextexamplestocalibrateandextracttheexactshortanswer, whichis
typicallyjustafewwords.
Question Groundtruth LongRAGprediction
wheredoesthebobandtomshowbroadcastfrom Indianapolis,Indiana Indianapolis
whohasgiventhetheoryofunbalancedeconomicgrowth Hirschman AlbertO.Hirschman
whendoesseason6ofthenextstepstart 2018 September29,2018
whatwastheprecursortothepresentdayinternet theARPANETproject ARPANET
Table7: SomeexamplesdemonstratethatLongRAGhasextractedaliasesordifferentformsofthegroundtruth.
13