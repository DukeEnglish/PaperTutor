IMAGE CONDUCTOR: PRECISION CONTROL FOR INTER-
ACTIVE VIDEO SYNTHESIS
YaoweiLi1,2, XintaoWang2, ZhaoyangZhang2:, ZhouxiaWang2,3,
ZiyangYuan2,4, LiangbinXie2,5,6, YuexianZou1(cid:0), YingShan2
1PekingUniversity2ARCLab,TencentPCG 3NanyangTechnologicalUniversity
4TsinghuaUniversity 5UniversityofMacau 6ShenzhenInstituteofAdvancedTechnology
ProjectPage: https://liyaowei-stu.github.io/project/ImageConductor/
🪄
(a) Camera Transitions
A corgi dog.
A girlwithwavy hair.
lush greenery.
🪄
(b) Object Movements
A couple.
A burningrose.
A jellyfish.
Figure1: OrchestratedResultsofImageConductor. ImageConductorenables
fine-grainedandaccurateimage-to-videomotioncontrol,includingbothcamera
transitionsandobjectmovements. Colorfullinesdenotemotiontrajectories.
ABSTRACT
Filmmakingandanimationproductionoftenrequiresophisticatedtechniquesfor
coordinatingcameratransitionsandobjectmovements,typicallyinvolvinglabor-
intensivereal-worldcapturing. DespiteadvancementsingenerativeAIforvideo
creation,achievingprecisecontrolovermotionforinteractivevideoassetgener-
ationremainschallenging. Tothisend,weproposeImageConductor,amethod
forprecisecontrolofcameratransitionsandobjectmovementstogeneratevideo
assets from a single image. An well-cultivated training strategy is proposed to
separatedistinctcameraandobjectmotionbycameraLoRAweightsandobject
LoRAweights. Tofurtheraddresscinematographicvariationsfromill-posedtrajec-
tories,weintroduceacamera-freeguidancetechniqueduringinference,enhancing
objectmovementswhileeliminatingcameratransitions. Additionally,wedevelop
atrajectory-orientedvideomotiondatacurationpipelinefortraining. Quantitative
andqualitativeexperimentsdemonstrateourmethod’sprecisionandfine-grained
controlingeneratingmotion-controllablevideosfromimages,advancingtheprac-
ticalapplicationofinteractivevideosynthesis.
(cid:0)Correspondingauthor.:Projectlead.
1
4202
nuJ
12
]VC.sc[
1v93351.6042:viXra1 INTRODUCTION
Filmmakingandanimationproductionareessentialformsofvisualart. Duringthecreativeprocessof
videomedia,professionaldirectorsoftenrequireadvancedcinematographytechniquestometiculously
plan and coordinate camera transitions and object movements, ensuring storyline coherence and
refinedvisualeffects. Toachieveprecisecreativeexpression,thecurrentworkflowforvideomedia
orchestrationandproductionheavilyreliesonreal-worldcapturingand3Dscanmodeling,whichare
labor-intensiveandcostly.
Recentwork(Hoetal.,2022;Blattmannetal.,2023b;Girdharetal.,2023;Xingetal.,2023;Chen
etal.,2023;Blattmannetal.,2023a;Bar-Taletal.,2024;Brooksetal.,2024)exploresanAIGC-
basedfilmmakingpipelinethatleveragesthepowerfulgenerativecapabilitiesofdiffusionmodelsto
generatevideoclipassets. Despitetheseadvancements,generatingdynamicvideoassetsallowing
creators precisely express their ideas remains unusable, for: (1) Lacking of efficient generating
controlinterface. (2)Lackingoffine-grainedandaccuratecontrolovercameratransitionsandobject
movements.
Although several works have attempted to introduce motion control signals to guide the video
generationprocess(Yinetal.,2023;Wangetal.,2023b;2024;Wuetal.,2024),noneoftheexisting
methodssupportaccurateandfine-grainedcontroloverbothcameratransitionsandobjectmovements
(seeFig.1).
Infact, dataavailableontheinternetoftenmixesbothcameratransitionsandobjectmovements,
leadingtoambiguitiesbetweenthetwotypesofmotion. AlthoughMotionCtrl(Wangetal.,2023b)
usesadata-drivenapproachtodecouplecameratransitionsfromobjectmotion,itstilllacksprecision
andeffectiveness. Cameraparametersareneitherintuitivenorstraightforwardtoobtainforcinemato-
graphicvariations. Forobjectmovements,MotionCtrlusesParticleSfM(Zhaoetal.,2022),amotion
segmentationnetworkbasedonopticalflowestimation,whichintroducessignificanterrors. Addi-
tionally,groundtruthvideosannotatedbasedonmotionsegmentationnetworksstillcontaincamera
transitions,causinggeneratedvideostoexhibitunintendedcinematographicvariations. Decoupling
cinematographicvariationsfromobjectmovementsthroughdatacurationisinherentlychallenging.
Obtainingvideodatafromafixedcameraviewpoint,i.e.,videoswithonlyobjectmovements,is
difficult. Opticalflow-basedmotionsegmentationmethods(Teed&Deng,2020;Xuetal.,2022;
Zhaoetal.,2022;Yinetal.,2023;Wangetal.,2023b)struggletoaccuratelytrackmovingobjects
withouterrorsandfailtoeliminateintrinsiccameratransitionsinrealisticvideos. Overall,existing
methodsareeithernotfine-grainedornotsufficientlyeffective.
Inthispaper,weproposeImageConductor,aninteractivemethodforfine-grainedobjectmotion
andcameracontroltogenerateaccuratevideoassetsfromasingleimage. Effectivefine-grained
motioncontrolrequiresrobustmotionrepresentation. Trajectories,beingintuitiveanduser-friendly,
allowuserstocontrolmotioninvideocontentbydrawingpaths. However,alarge-scale,high-quality
open-sourcetrajectory-basedtrackingvideodatasetiscurrentlylacking. Toaddressthis, weuse
CoTracker(Karaevetal.,2023)toannotateexistingvideodataanddesignadatafilteringworkflow,
resultinginhigh-qualitytrajectory-orientedvideomotiondata.
Toaddressthecouplingofcinematographicvariationsandobjectmovementsinreal-worlddata,we
firsttrainavideoControlNet(Zhangetal.,2023)usingannotateddatatoconveymotioninformationto
theUNetbackboneofthediffusionmodel. Wethenproposeacollaborativeoptimizationmethodthat
appliesdistinctsetsofLow-RankAdaptation(LoRA)weights(Huetal.,2021a)ontheControlNetto
distinguishvarioustypesofmotion. Inadditiontothedenoisinglosscommonlyusedindiffusion
models, we introduce an orthogonal loss to ensure the independence of different LoRA weights,
enablingaccuratemotiondisentanglement.
Toflexiblyeliminatecinematographicvariationscausedbyill-posedtrajectories,whicharedifficult
to distinguish in LoRA, and to enhance object movement, we also introduce a new camera-free
guidance. Thistechniqueiterativelyexecutesanextrapolationfusionbetweendifferentlatentsduring
thesamplingprocessofdiffusionmodels,similartotheclassifier-freeguidancetechnique(Ho&
Salimans,2022).
Inbrief,ourmaincontributionsareasfollows:
• Weconstructahigh-qualityvideomotiondatasetwithprecisetrajectoryannotations,ad-
dressingthelackofsuchdataintheopen-sourcecommunity.
2𝐼𝑚𝑎𝑔𝑒 𝐶𝑜𝑛𝑡𝑟𝑜𝑙𝑁𝑒𝑡
elegant Conv Layers Sum 1. Video Collection
koi fish
gliding CLIP Transformers Trainable ···
through (Spatial + Temporal) Freeze mixed camera
the dark.
2. CutsDetection & Selection
···
avoid cuts
3. MotionEstimation & Filtering
motion
··· ··· ·st·re·ngth
𝑍! 4. Cropping & Tracking
𝑇𝑟𝑎𝑗𝑠 ···
LoRA 5. Sampling & Gaussian Filter
LoRA
𝑀𝑜𝑡𝑖𝑜𝑛−𝐶𝑜𝑛𝑡𝑟𝑜𝑙𝑙𝑎𝑏𝑙𝑒
𝐶𝑜𝑛𝑡𝑟𝑜𝑙𝑁𝑒𝑡 ··· sd ·· pe ·· an r··s se
e
(a) (b)
Figure2: a)FrameworkofImageConductor. 3DUNetservesasthediffusionbackbone,while
imageControlNetandmotion-controllableControlNet(anditsLoRAweights)conveyappearance
andmotioninformation,respectively. Weprogressivelyfine-tunedifferentmodulesduringtrarning
phase (see Sec 2.4). b) Trajectory-oriented video motion data construction workflow. We
carefullycuratethedatatoensuredynamicandconsistentvideocontent,aswellasprecisetrajectory
annotations(seeSec2.2).
• WeintroduceamethodtocollaborativelyoptimizeLoRAweightsinmotionControlNet,
effectivelyseparatingandcontrollingcameratransitionsandobjectmovements
• Weproposecamera-freeguidancetoheuristicallyeliminatecameratransitionscausedby
multipletrajectoriesthatarechallengingtoseparatewithLoRAweights.
• Extensiveexperimentsdemonstratethesuperiorityofourmethodinpreciselyandfinely
motioncontrol,enablingthegenerationofvideosfromimagesthatalignwithuserdesires.
2 APPROACH
2.1 OVERVIEW
ImageConductoraimstoanimateastaticimagebypreciselydirectingcameratransitionsandobject
movementsaccordingtouserspecifications,producingcoherentvideoassets. Ourworkflowincludes
trajectory-orientedvideodataconstruction(Sec.2.2),amotion-awareimage-to-videoarchitecture
(Sec.2.3),controllablemotionseparation(Sec.2.4),andcamera-freeguidance(Sec.2.5).
Weuseuser-friendlytrajectoriestodefinetheintensityanddirectionofcameratransitionsandobject
movements. Toaddressthelackoflarge-scaleannotatedvideodata,wedesignadataconstruction
pipelinetocreateaconsistentvideodatasetwithappropriatemotion.
Usingthisdata,wetrainVideoControNet(Zhangetal.,2023)tosynthesizemotion-controllablevideo
content. Toeliminateambiguitiesbetweencameratransitionsandobjectmovements,weemploy
separatesetsofLoRAweights. First,wetrainwithcamera-onlyLoRAweightstocontrolcamera
transitions. Then,weloadtheseweightsanduseanewsetofobjectLoRAweightstodecoupleobject
movement,ensuringprecisecontrol. Wealsointroducealossfunctionwithorthogonalconstraintsto
maintainindependencebetweendifferentLoRAweights.
Toseamlesslyblendcameratransitionsandobjectmovements,weproposeacamera-freeguidance
techniquethatiterativelyextrapolatesbetweencameraandobjectmotionlatentsduringinference.
Fig.2(a)showsourframework,Fig.2(b)illustratesourdatacurationpipeline,andFig.3presents
thecoreideaofImageConductor.
32.2 TRAJECTORY-ORIENTEDVIDEOMOTIONDATACONSTRUCTION.
Since Image Conductor relies on trajectories to guide motion, we need a dataset with trajectory
annotationstotrackdynamicinformationinvideos. Existinglarge-scalevideodatasetstypically
lack such annotations. While some methods use motion estimators to annotate video data, these
approachesoftensufferfrominaccuracies(Yinetal.,2023;Wangetal.,2023b;Wuetal.,2024)or
lackgenerality(Wuetal.,2024). Moreover,almostallannotateddatasetswithtrajectoryannotations
arenotpubliclyavailable. Toaddressthis,weintroduceacomprehensiveandgeneralpipelinefor
generatinghigh-qualityvideodatawithappropriatemotionandconsistentscenes,asillustratedin
Fig.2(b).
VideoCollection. WeutilizetheWebViddataset(Bainetal.,2021),alarge-scalemixeddatasetwith
textualdescriptions,andtheRealestate10Kdataset(Zhouetal.,2018),acamera-onlydataset,forour
research. TheImageConductoraimstodecoupleobjectmovementsfrommixeddata,requiringscene
consistencyandhighmotionquality. Toensuretemporalquality,weprocesstheWebViddataset
bydetectingcutsandfilteringmotion. FortheRealestate10Kdataset,wefocusonthediversityof
cameratransitionsandgeneratevideocaptionsusingBLIP2(Lietal.,2023)byextractingframesat
specificintervalsandconcatenatingtheirdescriptions.
Cuts Detection and Selection. In videos, cuts refer to transitions between different shots, and
generativevideomodelsaresensitivetosuchmotioninconsistencies(Blattmannetal.,2023a). To
avoidcutsandabruptscenechanges,whichcancausethemodeltooverfitthesephenomena,we
firstuseacutdetectiontool1 toidentifycutswithinthevideodataset. Wethenselectthelongest
consistentscenesasourvideoclips,ensuringsceneconsistency.
Motion Estimation and Filtering. o ensure the dataset exhibits good dynamics, we use
RAFT(Teed&Deng,2020)tocomputetheopticalflowbetweenadjacentframesandcalculatethe
Frobeniusnormasamotionscore. Wefilteroutthelowest25%ofvideosamplesbasedonthisscore.
Toreducecomputationalcost,weresizetheshortersideofthevideosto256pixelsandrandomly
samplea32-framesequencewithatemporalintervalof1to16frames. These32framesareusedas
thetrainingdataset,andtheirmotionscoresarecomputedforsamplefiltering.
CroppingandTracking. Tostandardizethedimensionsofthetrainingdata,weperformcenter
croppingonthepreviouslyobtaineddata, resultinginvideoframesofsize 384ˆ256ˆ32. We
then employ CoTracker (Karaev et al., 2023), a tracking method towards dense point, to record
motionwithinthevideousinga16ˆ16grid. Comparedtoopticalflow-basedpointcorrespondence
methods(Teed&Deng,2020;Xuetal.,2022),trackingavoidsdrift-inducederroraccumulation,
providingamoreaccuraterepresentationofmotion. Aftertracking,weaccumulatepointtrajectories
bycalculatingthedifferencesbetweenadjacentpointswithinthesametrajectory. Thisresultsin
stackedflowmapscompatiblewiththeinputformatofControlNet(Zhangetal.,2023).
SamplingandGaussianFilter. oenhanceuserinteractionandusability,weusesparsetrajectories
formotionguidance. WeheuristicallysamplenPr1,8strajectoriesfromthedenseset,with8being
theupperlimit. Thevalueofnisrandomlyselected,andthenormalizedmotionintensityofeach
trajectoryisusedasthesamplingprobability. Theaccumulatedflowmapfromthesetrajectories
formsasparsematrix. Toavoidtraininginstabilitycausedbythesparsematrix,weapplyaGaussian
filtertothetrajectories,similartopreviousmethods(Yinetal.,2023;Wangetal.,2023b;Wuetal.,
2024). Throughthisdataprocessingpipeline, weconstructedatrajectory-orientedvideomotion
datasetcontaining130kmixedvideoswithcameratransitionsandobjectmovements,and62kvideos
withonlycameratransitions.
2.3 MOTION-AWAREIMAGE-TO-VIDEOARCHITECTURE
Image-to-VideoBackbone. AsillustratedinFig.2(a),weutilizeAnimatediff(Guoetal.,2023b)
equippedwithSparseCtrl(Guoetal.,2023a)forimagesasourpre-trainedimage-to-videofounda-
tionalmodel. ThismodelusestheCLIP(Radfordetal.,2021)textencodertoextracttextembeddings
1https://github.com/Breakthrough/PySceneDetect.
4Training Workflow GT Inference Workflow
Update Camera LoRA
Camera LoRA
Loss
Camra−𝑜𝑛𝑙𝑦 𝑀𝑜𝑡𝑖𝑜𝑛−𝐶𝑜𝑛𝑡𝑟𝑜𝑙𝑙𝑎𝑏𝑙𝑒 𝐶𝑜𝑛𝑡𝑟𝑜𝑙𝑁𝑒𝑡 Pan Left Camera LoRA
A sunflower. 𝑍𝑜𝑜𝑚 𝑂𝑢𝑡
GT
Rocky coastline with
Camera LoRA Loss crashing waves.
Object LoRA
Camra&𝑂𝑏𝑗𝑒𝑐𝑡 𝑀𝑜𝑡𝑖𝑜𝑛−𝐶𝑜𝑛𝑡𝑟𝑜𝑙𝑙𝑎𝑏𝑙𝑒 𝐶𝑜𝑛𝑡𝑟𝑜𝑙𝑁𝑒𝑡
Pan Left & ObjectMoveRight
Object LoRA
A herd of alpacas. Update Object LoRA ObjectMove
Camera Transitions Object Movements Camera Pose Freeze Freeze and Stopgrad Trainable
Figure3: Fine-grainedMotionSeparationMethod. a)Thetrainingprocessisdividedintotwo
stages. Initially,camera-onlydataisusedtoempowerthecameraLoRAwiththeabilitytocontrol
cameratransitions. Afterloadingthewell-trainedcameraLoRA,mixedmotiondataisusedtotrain
theobjectLoRA,refiningobjectmotioninformation. b)Duringinference,loadingdifferentLoRAs
providesthemodelwithvariouscontrolcapabilities.
c P R1ˆd, which are then passed to the UNet (Ronneberger et al., 2015) backbone via cross-
txt
attentionmechanism. Theinputimage,servingasthefirstframe,isconcatenatedwithanall-zero
frame matrix and a mask identifier channel-wise to form c P RTˆ4ˆHˆW. Next, the video
img
SparseCtrl,avariantoftheControlNet(Zhang&Agrawala,2023)thatremovestheskip-connections
betweentheControlNet’sandtheUNetencoder’sinputlatents,isusedtoextractsimageinformation
fromc .
img
Motion-ControllableControlNet. Toextractmotioninformationfromtheannotatedtrajectory
inputc PRTˆ2ˆHˆW forcompositionofcameratransitionsandobjectmovementsinvideos,
trajs
we use ControlNet as the motion encoder to capture multi-level motion representations. This
ControlNetincorporatesdifferenttypesofLoRAweightstoguidetheimage-to-videogeneration
with user-desired camera transitions and object movements. Consistent with the observations of
SparseCtrl(Guoetal.,2023a),wefindthatremovingtheskipconnectionsbetweenthemainbranch’s
andtheconditionalbranch’sinputlatentsspeedsupconvergenceduringtraining.
2.4 CONTROLLABLEMOTIONSEPARATION
Theaimofourapproachistopreciselyseparatecameratransitionsandobjectmovementsinvideos,
enablingfine-grainedcontroloverthegenerationofvideoclipassertsthatmeetsuserexpectations.
Tothisend,weintroducedcameraLoRAθ andobjectLoRAθ intothemotionControlNetto
cam obj
guidethesynthesisofdifferenttypesofmotion. AsshownintheFig.3,duringthetrainingprocess,
weemployedacollaborativeoptimizationstrategy. First,weoptimizedthecameraLoRA,andthen,
weoptimizedtheobjectLoRAbasedontheloadedcameraLoRA.Duringtheinferencestage,the
modelloadsdifferentLoRAtocontrolcameratransitions(e.g.,zoomingout)andobjectmovements
(e.g.,twowavesadvancinginaspecifieddirection).
CameraTransitions. Sinceitisavailabletoobtaindatawithcamera-onlytransition,westraight-
forwardlytraincameraLoRAθ “θ `∆θ usingourourcarefullycultivatedcameramotion
cam 0 cam
dataset,endowingtheControNetwiththeabilitytodirectcinematographicvariations. Thestandard
diffusiondenoisingtrainingobjectiveisutilized:
“ ‰
L “E ∥ϵ´ϵ ,t,c ,c ,c q∥2 , (1)
cam z0,cam,ctxt,cimg,ctrajs,ϵ„Np0,Iq,t θcampzt,cam txt img trajs 2
whereθ isthedenoiserwhereControlNetwithcameraLoRAloaded,z isthenoisylatentof
cam t,cam
videoswithonlycameratransitionattimestept,c ,c andc refertothetextprompt,image
txt img trajs
prompt,andconditionaltrajectory,respectively.
ObjectMovements. Duetothescarcityoffixed-camera-viewvideodatawithoutcinematographic
variations,weneedtodecoupleobjectmotionfrommixeddatawherebothcameratransitionsand
objectmovementsareexsist.Observingthatdistincttypesofmotionsharethesametrajectory,wecan
5furthertraintheobjectLoRAθ “θ `∆θ afterloadingthewell-trainedcameraLoRAweights,
obj 0 obj
i.e.,targetingthereconstructionofcameratransitionsandobjectmovementsintheoriginalvideo
contentfrommixeddata. Formally,weloadboththecameraLoRAandobjectLoRAsimultaneously
duringtrainingphase,andpreventgradientflowtothecameraLoRAviastopgradsgr¨s:
θ “θ `sgr∆θ s`∆θ . (2)
mixed 0 cam obj
Similarly,weoptimizetheobjectLoRAusingthestandarddiffusiondenoisingobjective:
“ ‰
L “E ∥ϵ´ϵ ,t,c ,c ,c q∥2 , (3)
mixed z0,mixed,ctxt,cimg,ctrajs,ϵ„Np0,Iq,t θmixedpzt,mixed txt img trajs 2
whereθ isthedenoiserwhereControlNetwithallLoRAloadedasinEq.2,z isthenoisy
mixed t,cam
latentofvideoswithcameratransitionandobjectmovementsattimestept.
OrthogonalLoss. ToencouragetheobjectLoRAtolearnconceptsdistinctfromthecameraLoRA
andtoacceleratetheconvergenceofthemodel,weproposeanorthogonallossasajointoptimization
objective. Specifically,weextractalllinearlayerweightsW andW fromthedifferentLoRAs
cam traj
andimposeanorthogonalityconstraintonthem:
“ ‰
L “E ∥I´W WT ∥2 (4)
ortho Wi,camPWcam,Wi,trajPWtraj i,cam i,traj 2
whereI representstheidentitymatrix,W andW refertotheweightsofthei-thlinearlayer
i,cam i,traj
ofthecameraLoRAandobjectLoRA,respectively.
Inall,theoptimizationprocessisincremental. WefirstoptimizethecameraLoRAusingL ,and
cam
thenoptimizetheobjectLoRAusingL andL .
mixed ortho
2.5 CAMERA-FREEGUIDANCE
Whenusersaimtocontrolmultipleobjects,multipletrajectoriesoftenintroducecameratransitions.
Inspired by classifier-free guidance (Ho & Salimans, 2022), we propose a camera-free guidance
techniquetoflexiblyandseamlesslyenhancemotionintensitywhileeliminatingcameratransitions.
ϵˆ px ,cq“ϵ px ,∅q
θ0,θtrajs t θ0 t
`λ pϵ px ,cq´ϵ px ,∅qq (5)
cfg θ0 t θ0 t
`λ pϵ px ,cq´ϵ px ,cqq,
trajs θtrajs t θ0 t
where θ refer to the model with object LoRA and θ is the model with pre-trained motion
trajs 0
ControlNet. Thefinaloutputlatentisderivedbyextrapolatingtheoutputsofthesetwocomponents.
3 EXPERIMENTS
3.1 COMPARISONSWITHSTATE-OF-THE-ARTMETHODS
We compare Image Conductor with exsisting state-of-the-art image-based or text-based motion
controllablevideogenerationmethods,namelyDragNUWA(Yinetal.,2023),DragAnything(Wu
etal.,2024)andMotionCtrl(Wangetal.,2023b).
Evaluationdatasets. Toindependentlyevaluatecameratransitionsandobjectmovements,weuse
twodistinctdatasets: 1)Camera-OnlyMotionEvaluationDataset: Weselect10cameratrajectories,
e.g.panleft,panright,panup,pandown,zoomin,zoomout,toevaluatecontrolovercinematographic
variations. 2)Object-OnlyMotionEvaluationDataset: Wedesign10variedtrajectories,including
straightlines,curves,shakinglines,andtheircombinations.
Qualitative Evaluation Fig. 4 displays some of our qualitative results. Compared to previous
methods(Yinetal.,2023;Wuetal.,2024;Wangetal.,2023b),ourapproachcaneffectivelycontrol
camera transitions and object movements. In terms of camera transitions, both DragNUWA and
DragAnythingfailtoachievethecameratransitionofpanningdownandthenupinthegenerated
video. Although Motionctrl-SVD is capable of generating the specified camera movement, it is
unabletodefinenaturalcontentchangesviatextprompts. Additionally,itcannotaccuratelydefine
theintensityofcamerachanges,andsometimesintroducesdistortionartifact.
6🪄
(a) Camera Transitions
Pan Up then Down
Sunflowers.
🪄
(b) Object Movements
Twocars.
Figure4: QualitativeComparisonsoftheproposedImageConductor. (a)CameraTransitions.
Our method can simultaneously utilize text, image, and trajectory prompts as control signals to
achievemorenaturalcontentandcameratransitions. (b)ObjectMovements. Apartfromourmethod,
otherapproachesincorrectlyconfuseobjectmovementswithcameratransitions.
Intermsofobjectmovements,bothDragNUWAandDragAnythingincorrectlyinterpretobjectmove-
mentascameratransition,resultingingeneratedvideosthatdonotmeetuserintentions. Inaddition,
themotiontrajectoriesoftheirgeneratedvideosareoftenpoorlymatchedtothedesiredtrajectories
preciselyduetotheerrorsintroducedbythelabeleddataset. Astrajectory-basedMotionCtrlrelies
onthetext-to-videomodel,wedirectlyusetextandtrajectorypromptstocontrolthegenerationof
thevideounderdifferentseed. Theresultsdemonstratethatitlacksfine-grainedcontroloverthe
generatedcontentduetoitsinabilitytouseimagesasconditions. Additionally, itstillexhibitsa
significantamountofcameratransitionratherthanobjectmovement. Inall,ourmethodiscapableof
accuratelyandfinelycontrollingvarioustypesofmotionutilizingtheseparatedLoRA.
Quantitative Evaluation As shown in the Tab. 1, compared to other methods, our proposed
Image Conductor achieves state-of-the-art quantitative performance. We measure our alignment
with the given trajectoies via the CamMC and ObjMC metrics, surpassing the baseline models
and demonstrating our precise motion control capabilities. At the same time, the FID and FVD
metricsillustratethatourgenerationqualitysurpassesothermodels,capableofproducingrealistic
videos. Furthermore,weinvite31participantstoassesstheresultsofDragNUWA,DragAnything
andImageConductor. Theassessmentincludesvideoquality,motionsimilarity. Participantsare
7
AWUNgarD
gnihtynAgarD
lrtCnoitoM
sruO
AWUNgarD
gnihtynAgarD
lrtcnoitoM
sruO🪄
(a) Personalized and Controllable Video Synthesis
A cat.
A mountain.
A tank34.
A jellyfish.
Figure5: ResultsofPersonalizedandControllableVideoSynthesis. Thepre-trainedbasemodel
andLoRAweightsaresourcedfromTuSun2,HelloObject3,andCardosAnime4checkpoint.
Table1: QuantitativeComparisonswithSOTAMethods. Weutilizeautomaticmetrics(i.e.,FID,
FVD,CamMC,ObjMC)andhumanevaluation(i.e.,overallperformance,samplequality,motion
similarity) to evaluate the performance. DN and DA denotes DragNUWA (Wu et al., 2024) and
DragAnything(Yinetal.,2023),respectively.
AutomaticMetrics HumanEvaluation
Method
FIDÓ FVDÓ CamMCÓ ObjMCÓ OverallÒ QualityÒ MotionÒ
DN(Yinetal.,2023) 237.26 1283.85 48.72 51.24 31.8% 37.1% 27.7%
DA(Wuetal.,2024) 243.17 1287.15 66.54 60.97 6.5% 8.1% 6.3%
ImageConductor 209.74 1116.17 33.49 42.38 61.7% 54.8% 66.0%
alsoaskedtogiveanoverallpreferenceforeachcomparedpair. Thestatisticalresultsconfirmthat
ourgeneratedvideosnotonlyappearmorerealisticandvisuallyappealingbutalsoexhibitsuperior
motionadherencecomparedtothoseproducedbyothermodels.
3.2 PERSONALIZEDANDCONTROLLABLEVIDEOSYNTHESIS
SincethebaseT2Vmodelisnotfine-tuned,ourmethodnaturallypossessestheabilityforpersonalized
generationwhilemaintainingcontrollability.InFig.5,weloadedsomepersonalizedmodelstosample
videosusingtheprovidedprompt,guidancescaleanduser-specifiedtrajectories.Theresultsshowthat
ourmethodcanseamlesslyintegratewithopen-sourcecustomizationcommunities(e.g.,CIVITAI5)
andhaspowerfulcapabilitiesforgeneratingcontrollablevideocontentassets.
3.3 ABLATIONSTUDIES
EffectofDistinctLoRAWeights Tovalidatethatourcarefullydesignedinteractiveoptimization
strategycanseparatecameratransitionsandobjectmovementsthroughdistinctLoRAweights,we
usethesametrajectoryasinputtoguidedifferentLoRAtogeneratevideos. AsshowninFig.6,
loadingvariousLoRAweightsendowsthemodelwithdifferentcapabilities. Forinstance,avertically
2https://civitai.com/models/33194/leosams-pallass-catmanul-lora.
3https://civitai.com/models/121716/helloobjects.
4https://civitai.com/models/25399/cardos-anime.
5https://civitai.com/.
8Rocks and coastline.
Figure6: EffectofdistinctLoRAweights. Imageconductorenablesuserstoindependentlycontrol
cameraandobjectmotioninteractively.
Figure7: EffectofCamera-freeGuidance. Thecamera-freeguidanceapproachflexiblyenhances
objectmovementsduringinference.
upwardtrajectorycausesthevideotopanupwhenthecameraLoRAisloaded,anditcreatesupward
waveswhentheobjectLoRAisloaded.
EffectofCamra-freeGuidance AsshowninFig.7,usingcamera-freeguidancecanfacilitate
theseparationofobjectmovementsfromcameratransitionsinseveralchallengingexamples. When
camera-freeguidanceλ issetto1,i.e.,camera-freeguidanceisnotyetused,thegeneratedvideo
trajs
exhibits a unexpected pan left transformation. When the λ is set to 1.1, the generated videos
trajs
exhibitreasonableobjectmovements,yetsomeartifactsstillremain. Astheguidanceincreases,the
movementsoftheobjectbecomesmoreapparentandclear.
4 CONCLUSION
Inconclusion,thispaperintroducesImageConductor,anovelapproachforpreciseandfine-grained
control of camera transitions and object movements in interactive video synthesis. We design
a training strategy and utilized distinct LoRA weights to decouple camera transition and object
movements. Additionally,weproposeacamera-freeguidancetechniquetoenhanceobjectmovement
control. Extensiveexperimentsdemonstratetheeffectivenessofourmethod,markingasignificant
steptowardspracticalapplicationsinvideo-centriccreativeexpression.
9
ARoL
aremaC
ARoL
tcejbO
ARoL
aremaC
ARoL
tcejbO
1=%$#"!𝜆
1.1=%$#"!𝜆
0.2=%$#"!𝜆REFERENCES
MaxBain,ArshaNagrani,GülVarol,andAndrewZisserman. Frozenintime: Ajointvideoand
imageencoderforend-to-endretrieval. InProceedingsoftheIEEE/CVFInternationalConference
onComputerVision,pp.1728–1738,2021.
OmerBar-Tal,HilaChefer,OmerTov,CharlesHerrmann,RoniPaiss,ShiranZada,ArielEphrat,
JunhwaHur, YuanzhenLi, TomerMichaeli, etal. Lumiere: Aspace-timediffusionmodelfor
videogeneration. arXivpreprintarXiv:2401.12945,2024.
Georgios Batzolis, Jan Stanczuk, Carola-Bibiane Schönlieb, and Christian Etmann. Conditional
imagegenerationwithscore-baseddiffusionmodels. arXivpreprintarXiv:2111.13606,2021.
AndreasBlattmann,TimDockhorn,SumithKulal,DanielMendelevitch,MaciejKilian,Dominik
Lorenz,YamLevi,ZionEnglish,VikramVoleti,AdamLetts,etal. Stablevideodiffusion: Scaling
latentvideodiffusionmodelstolargedatasets. arXivpreprintarXiv:2311.15127,2023a.
AndreasBlattmann,RobinRombach,HuanLing,TimDockhorn,SeungWookKim,SanjaFidler,and
KarstenKreis. Alignyourlatents: High-resolutionvideosynthesiswithlatentdiffusionmodels.
InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pp.
22563–22575,2023b.
Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe
Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video
generation models as world simulators. 2024. URL https://openai.com/research/
video-generation-models-as-world-simulators.
Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo
Xing,YaofangLiu,QifengChen,XintaoWang,etal. Videocrafter1: Opendiffusionmodelsfor
high-qualityvideogeneration. arXivpreprintarXiv:2310.19512,2023.
Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao,
ByungEunJeon,YuweiFang,Hsin-YingLee,JianRen,Ming-HsuanYang,etal. Panda-70m:
Captioning70mvideoswithmultiplecross-modalityteachers. arXivpreprintarXiv:2402.19479,
2024.
RinonGal,YuvalAlaluf,YuvalAtzmon,OrPatashnik,AmitHBermano,GalChechik,andDaniel
Cohen-Or. Animageisworthoneword: Personalizingtext-to-imagegenerationusingtextual
inversion. arXivpreprintarXiv:2208.01618,2022.
RohitGirdhar,MannatSingh,AndrewBrown,QuentinDuval,SamanehAzadi,SaiSakethRambhatla,
AkbarShah,XiYin,DeviParikh,andIshanMisra.Emuvideo:Factorizingtext-to-videogeneration
byexplicitimageconditioning. arXivpreprintarXiv:2311.10709,2023.
YuweiGuo,CeyuanYang,AnyiRao,ManeeshAgrawala,DahuaLin,andBoDai.Sparsectrl:Adding
sparsecontrolstotext-to-videodiffusionmodels. arXivpreprintarXiv:2311.16933,2023a.
YuweiGuo,CeyuanYang,AnyiRao,YaohuiWang,YuQiao,DahuaLin,andBoDai. Animatediff:
Animateyourpersonalizedtext-to-imagediffusionmodelswithoutspecifictuning. arXivpreprint
arXiv:2307.04725,2023b.
HaoHe,YinghaoXu,YuweiGuo,GordonWetzstein,BoDai,HongshengLi,andCeyuanYang.Cam-
eractrl: Enablingcameracontrolfortext-to-videogeneration. arXivpreprintarXiv:2404.02101,
2024.
JonathanHoandTimSalimans. Classifier-freediffusionguidance. arXivpreprintarXiv:2207.12598,
2022.
JonathanHo,AjayJain,andPieterAbbeel. Denoisingdiffusionprobabilisticmodels. Advancesin
neuralinformationprocessingsystems,33:6840–6851,2020.
JonathanHo,WilliamChan,ChitwanSaharia,JayWhang,RuiqiGao,AlexeyGritsenko,DiederikP
Kingma,BenPoole,MohammadNorouzi,DavidJFleet,etal. Imagenvideo: Highdefinition
videogenerationwithdiffusionmodels. arXivpreprintarXiv:2210.02303,2022.
10EdwardJHu,YelongShen,PhillipWallis,ZeyuanAllen-Zhu,YuanzhiLi,SheanWang,LuWang,
and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint
arXiv:2106.09685,2021a.
EdwardJHu,YelongShen,PhillipWallis,ZeyuanAllen-Zhu,YuanzhiLi,SheanWang,LuWang,
and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint
arXiv:2106.09685,2021b.
NikitaKaraev,IgnacioRocco,BenjaminGraham,NataliaNeverova,AndreaVedaldi,andChristian
Rupprecht. Cotracker: Itisbettertotracktogether. arXivpreprintarXiv:2307.07635,2023.
DiederikPKingmaandJimmyBa. Adam: Amethodforstochasticoptimization. arXivpreprint
arXiv:1412.6980,2014.
JunnanLi,DongxuLi,SilvioSavarese,andStevenHoi. Blip-2: Bootstrappinglanguage-imagepre-
trainingwithfrozenimageencodersandlargelanguagemodels. arXivpreprintarXiv:2301.12597,
2023.
AlexanderQuinnNicholandPrafullaDhariwal. Improveddenoisingdiffusionprobabilisticmodels.
InInternationalConferenceonMachineLearning,pp.8162–8171.PMLR,2021.
AlecRadford, JongWookKim, ChrisHallacy, AdityaRamesh, GabrielGoh, SandhiniAgarwal,
GirishSastry,AmandaAskell,PamelaMishkin,JackClark,etal. Learningtransferablevisual
modelsfromnaturallanguagesupervision. InICML,2021.
Weijieying Ren, Xinlong Li, Lei Wang, Tianxiang Zhao, and Wei Qin. Analyzing and reducing
catastrophicforgettinginparameterefficienttuning. arXivpreprintarXiv:2402.18865,2024.
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-
resolutionimagesynthesiswithlatentdiffusionmodels. InCVPR,2022.
OlafRonneberger,PhilippFischer,andThomasBrox. U-net: Convolutionalnetworksforbiomedical
imagesegmentation. InMICCAI,2015.
TimSalimansandJonathanHo. Progressivedistillationforfastsamplingofdiffusionmodels,2022.
MaximilianSeitzer. pytorch-fid: FIDScoreforPyTorch. https://github.com/mseitzer/
pytorch-fid,2020.
JaschaSohl-Dickstein,EricWeiss,NiruMaheswaranathan,andSuryaGanguli. Deepunsupervised
learningusingnonequilibriumthermodynamics. InInternationalconferenceonmachinelearning,
pp.2256–2265.PMLR,2015.
Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv
preprintarXiv:2010.02502,2020a.
YangSongandStefanoErmon. Generativemodelingbyestimatinggradientsofthedatadistribution.
Advancesinneuralinformationprocessingsystems,32,2019.
YangSong,JaschaSohl-Dickstein,DiederikPKingma,AbhishekKumar,StefanoErmon,andBen
Poole. Score-basedgenerativemodelingthroughstochasticdifferentialequations. arXivpreprint
arXiv:2011.13456,2020b.
YangSong,JaschaSohl-Dickstein,DiederikPKingma,AbhishekKumar,StefanoErmon,andBen
Poole. Score-basedgenerativemodelingthroughstochasticdifferentialequations. arXivpreprint
arXiv:2011.13456,2020c.
ZacharyTeedandJiaDeng. Raft: Recurrentall-pairsfieldtransformsforopticalflow. InComputer
Vision–ECCV2020: 16thEuropeanConference,Glasgow,UK,August23–28,2020,Proceedings,
PartII16,pp.402–419.Springer,2020.
ThomasUnterthiner,SjoerdVanSteenkiste,KarolKurach,RaphaelMarinier,MarcinMichalski,and
SylvainGelly. Towardsaccurategenerativemodelsofvideo: Anewmetric&challenges. arXiv
preprintarXiv:1812.01717,2018.
11Jiawei Wang, Yuchen Zhang, Jiaxin Zou, Yan Zeng, Guoqiang Wei, Liping Yuan, and Hang
Li. Boximator: Generating rich and controllable motions for video synthesis. arXiv preprint
arXiv:2402.01566,2024.
XiangWang,HangjieYuan,ShiweiZhang,DayouChen,JiuniuWang,YingyaZhang,YujunShen,
Deli Zhao, and Jingren Zhou. Videocomposer: Compositional video synthesis with motion
controllability. arXivpreprintarXiv:2306.02018,2023a.
Zhouxia Wang, Ziyang Yuan, Xintao Wang, Tianshui Chen, Menghan Xia, Ping Luo, and Ying
Shan. Motionctrl: Aunifiedandflexiblemotioncontrollerforvideogeneration. arXivpreprint
arXiv:2312.03641,2023b.
WejiaWu,ZhuangLi,YuchaoGu,RuiZhao,YefeiHe,DavidJunhaoZhang,MikeZhengShou,
YanLi, TingtingGao, andDiZhang. Draganything: Motioncontrolforanythingusingentity
representation. arXivpreprintarXiv:2403.07420,2024.
JinboXing,MenghanXia,YongZhang,HaoxinChen,XintaoWang,Tien-TsinWong,andYing
Shan. Dynamicrafter: Animatingopen-domainimageswithvideodiffusionpriors. arXivpreprint
arXiv:2310.12190,2023.
HaofeiXu,JingZhang,JianfeiCai,HamidRezatofighi,andDachengTao. Gmflow: Learningoptical
flowviaglobalmatching. InProceedingsoftheIEEE/CVFconferenceoncomputervisionand
patternrecognition,pp.8121–8130,2022.
ShengmingYin,ChenfeiWu,JianLiang,JieShi,HouqiangLi,GongMing,andNanDuan.Dragnuwa:
Fine-grainedcontrolinvideogenerationbyintegratingtext,image,andtrajectory. arXivpreprint
arXiv:2308.08089,2023.
LvminZhangandManeeshAgrawala. Addingconditionalcontroltotext-to-imagediffusionmodels.
arXivpreprintarXiv:2302.05543,2023.
Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image
diffusionmodels. InProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision,
pp.3836–3847,2023.
WangZhao,ShaohuiLiu,HengkaiGuo,WenpingWang,andYong-JinLiu. Particlesfm: Exploiting
densepointtrajectoriesforlocalizingmovingcamerasinthewild. InECCV,2022.
TinghuiZhou,RichardTucker,JohnFlynn,GrahamFyffe,andNoahSnavely. Stereomagnification:
Learningviewsynthesisusingmultiplaneimages. arXivpreprintarXiv:1805.09817,2018.
12A RELATED WORKS
VideoSynthesis. Withtheemergenceofmassivedata(Bainetal.,2021;Chenetal.,2024)and
thegradualperfectionofdiffusionmodeltheory(Hoetal.,2020;Songetal.,2020c;Batzolisetal.,
2021),deepgenerativemodelshavemaderemarkableprogress(Rombachetal.,2022;Galetal.,
2022;Zhang&Agrawala,2023;Wangetal.,2023a;Chenetal.,2023;Brooksetal.,2024). Despite
the significant achievements, current video generation methods (Wang et al., 2023a; Chen et al.,
2023;Brooksetal.,2024;Blattmannetal.,2023a;Girdharetal.,2023)stillexhibitrandomnessand
facechallengesingeneratinghigh-qualityvideoswithcontrollability,whichhindersthepractical
applicationofAIGC-basedvideogenerationmethods.
MotionControlinVideos. Recently,somestudieshaveintroducedadditionalcontrolsignals,such
astrajectories(Yinetal.,2023;Wuetal.,2024;Wangetal.,2023b),cameraparameters(Wangetal.,
2023b;Heetal.,2024),andboundingboxes(Wangetal.,2024),tocontrolvisualelementsinvideos,
i.e.,cameratransitionsandobjectmovements,thusachievinginteractivevideoassertsgeneration.
However,theylackthecapabilitytopreciselyandfinelymanipulatevisualelements,especiallywhen
itcomestoobjectmovements(Yinetal.,2023;Wangetal.,2023b). Inthispaper,wemeticulously
designatrainingstrategythatutilizesexistingdatatoachieveflexibleandprecisemotionseparation
andcontrol.
B PRELIMINARY
B.1 CONDITIONALVIDEODIFFUSIONMODEL
Formally, diffusion models consists of a forward process and a reverse process (Sohl-Dickstein
etal.,2015;Hoetal.,2020;Songetal.,2020c). TheforwardprocessisdefinedasaMarkovchain
thatprogressivelyaddsdistinctlevelsofGaussiannoisetothesignalx overaseriesoftimesteps
0
tPr0,Ts,untilthex iscompletelycorruptedtox „Np0,Iq:
0 T
? ?
qpx |x q“Np α x ,p1´α qIq, and qpx |x q“Np αs x ,p1´αs qIq, (6)
t t´1 t t´1 t t 0 t 0 t
Here wśe consider the variance-preserving setting (Song et al., 2020b) with 0 ă α
t
ă 1 and
αs “ t α whereα isadecreasingsequence. Thereverseprocessisaparameter-containing
t i“1 i t
processdesignedtoiterativelydenoisethecorruptedsequencex :
T
ppx |x q“Npµ px q,σ2Iq. (7)
t´1 t t t t
Themeanandvarianceofthereverseprocesscanbedefinedas:
? ?
α p1´αs q αs p1´α q
µ px ,x q“ t t´1 x ` t´1 t x , (8)
t t 0 1´αs t 1´αs 0
ˆt ˙ t
1´αs
σ2 “p1´α q t´1 . (9)
t t 1´αs
t
Hereweconsiderσ2isanuntrainedtimedependentconstants(Hoetal.,2020;Nichol&Dhariwal,
t
2021),andx canbereparameterizedusingEq.6andestimatedusingv-prediction(Salimans&Ho,
0
2022)orϵ-predictiontechniques(Hoetal.,2020).
Givenaninputconditionc,thegoaloftheconditionalvideodiffusionmodelistosampleavideo
sequence x “ tx1,x2,¨¨¨ ,xLu with L frames from the conditional probability distribution
0 0 0 0
ppx |cq. Specifically,µ px ,t,cqcanbecalculatedusingtheϵ-prediction:
0 θ t
ˆ ˙
1 1´α
µ θpx t,t,cq“ ?
α
x t´ ? 1´αst ϵ θpx t,t,cq , (10)
t t
whereϵ isadenoisingUNetnetwork. Inthiscase,theϵ isoptimizedviadenoisingscorematch-
θ θ
ing(Song&Ermon,2019):
“ ? ? ‰
minE }ϵ´ϵ p α x ` 1´α ϵ,t,cq}2 . (11)
θ
px0,cq„qpx0,cq,ϵ„Np0,Iq,t θ t 0 t 2
13Figure8: Inherentvideocontentinconsistencybetweenthefirstframeandsubsequentframesinthe
basemodel.
B.2 LOW-RANKADAPTATION
Low-RankAdaptation(LoRA)(Huetal.,2021b)isaparameter-efficienttuningapproachusedto
acceleratemodelfine-tuningonincomingdata,whichcanpreventcatastrophicforgetting(Renetal.,
2024). Unliketrainingtheentiremodel,LoRAaddsapairofrank-decompositionmatricestothe
linearlayerweights, whichoptimizesonlythenewlyintroducedparametersandensuresthatthe
otherparametersarefixed. Mathematically,thenewweightsW1 PRmˆncanbedefinedas:
W1 “W `∆W “W `ABT, (12)
whereAPRmˆr andB PRnˆr areapairoflearnablematricesandr !minpm,nqistherankto
reducethecostoffine-tuning.
C EXPERIMENTAL DETAILS
C.1 IMPLEMENTATIONDETAILS.
WeuseAnimatediffv3(Guoetal.,2023b)combinedwithRGBSparseCtrl(Guoetal.,2023a)as
ourbasemodelforimage-to-videogeneration. WetrainonlythemotionControlNetwhilekeeping
theUNetbackboneweightsfrozen. ThemotionControlNetistrainedonourcultivatedsampled
16-framevideosequenceswitharesolutionof384ˆ256(Section2.2). BothcameraLoRAand
objectLoRAisoptimizedwithAdam(Kingma&Ba,2014)on8NVIDIATeslaV100GPUsfora
weekwithabatchsizeof64andalearningrateof1ˆ10´4. WeinitiallytrainthemotionControlNet
usingmixeddata. Subsequently,weutilizecamera-onlydataandmixeddatatoextractthecamera
LoRAandobjectLoRAweightsrespectively. Tofacilitateuserinput,wefollowastrategyoftraining
ondensetrajectoriesfirst,andthenfine-tuningthemodelonsparsetrajectories. Duringtheinference
phase,weuse25stepsofDDIMsampler(Songetal.,2020a). Unlessotherwisenoted,thescaleof
classifier-freeguidance(Ho&Salimans,2022)issetto8.5.
C.2 EVALUATIONMETRICS.
Tothoroughlyevaluatetheeffectivenessofourmethod,wefollowingMotionCtrl(Wangetal.,2023b)
toassessedtwotypesofmetrics: 1)Videocontentqualityevaluation. WeemployFréchetInception
Distance(FID)(Seitzer,2020),FréchetVideoDistance(FVD)(Unterthineretal.,2018)tomeasure
the visual quality and temporal coherenceand. The reference videos of FID and FVD are 5000
videosrandomlyselectedfromWebVid(Bainetal.,2021). 2)Videomotionqualityevaluation. The
Euclideandistancebetweenthepredictedandgroundtruthtrajectories,i.e.,CamMCandObjMC,
is used to evaluate the motion control. Unlike MotionCtrl, which uses particleSFM (Zhao et al.,
2022)toestimatethecameraposesofthepredictedvideoforcalculatingCamMC,wedirectlyextract
pixel-levelmovementtrajectoriestocomputeCamMCsimilartoObjMC.
14
1emarF
2
emarFD LIMITATIONS
Despiteourmodelcanfaithfullyproducemotioninformationbasedonuser-inputtrajectories,the
generatedqualityofcontentisconstrainedbythebasemodel. Forexample,asshowninFig.8,we
observethatalthoughAnimatediff(Guoetal.,2023b)withimageSparseCtrl(Guoetal.,2023a)
imposes strong constraints on the first frame, subsequent frames exhibit some inconsistencies in
coloranddetailcomparedtothefirstframe. Onepossiblesolutionistoconcatenatenoisyimage
latentstotheinputnoiseinadditiontousingtheimageconditioninginjectionmechanism,similarto
SVD(Blattmannetal.,2023a)andDynamiCrafter(Xingetal.,2023).
Anotherlimitationisthatdespitetextandimagepromptsgenerallycomplementingeachotherin
mostscenariosduringthevideogenerationprocess,iftheyconveydifferentmeanings,thequalityof
theoutputmaybecompromised.
15