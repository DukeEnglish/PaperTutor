V-RECS, a Low-Cost LLM4VIS Recommender with Explanations,
Captioning and Suggestions.
LucaPodo ,MarcoAngelini andPaolaVelardi
TRAINING PHASE INFERENCE PHASE
A - Task T1 T2 T3
description explain describe suggest Datasets
+ Dataset User’s

query
B - GPT-4 Column name & Return the average price of
as a teacher User’s
 types extraction ep aro cd hu cc ats te t gh oa rt y h ca ov de e
query
resC po - n C so eT
 s R1 R2 R3 + ('produc( t_'p cr ao td eu gc ot_ ryid _ .' .c, . o'n du em ', e 'cr aic t' e), g orical'),
VegaZero

specifications
D - Dataset

Visualization

augmentation narrative
The user is interested in understanding the average
+ price of products in each category. Therefore, the E 'product_category_code' is selected as the x-axis to
represent different categories, [...]


E - Llama as

student Llama-2-7B FinS eu -p tue nrv inis ge (d S F) dT ih ffi es r eis n a t pb ra or d c uh ca t r ct a w teh ge or re y t ch oe d x e- sa x ai ns dr e thp ere ys -e an xt is s C represents the mean price of [... 
Which product category has the highest average
price?
 S
[...]
V-RECS V-R
Fig.1: Duringthetrainingphase,alargeLLM,GPT-4,actingasaTeacher(B),receivesasinputtriples(D,Q,V)extractedfroman
availabledataset,whereDistabulardata,Qisanaturallanguagequery,andVistheVegaZerospecificationforachartmatchingthe
user’squeryQ.ThemodelisaskedtoperformthreeChainofThought(CoT)tasks(A):T1,toexplainthereasoningstepsthatjustify
theanswerVgiventhequeryQ;T2,togenerateacaptionthatdescribesthechart,andT3,tosuggestadditionalusefulqueries.The
responsesofthemodelarecombinedtocreateavisualizationnarrative,VN(C,D).Next,asmallmodel,Llama-2-7B(theStudent),is
fine-tuned(E)withtheenricheddatasetofquadruples(D,Q,C,VN).Atinferencetime,theresultingspecializedmodel,namedV-RECS,
receivesasinputapair(D,Q)andgenerates,alongwithV,anarrativemadeofanexplanationE,acaptionC,andasuggestion
Sofadditionalqueries. Adoptingateacher-studentmetaphor, theesteemedprofessorGPT-4, leveragingitstrillion-parameters
knowledgeofdatavisualizationandnaturallanguageunderstanding,generatesexamplesofvisualizationnarrativesfortheyoung
studentLlama-2-7B.Llamausesthissupplementarymaterialtodiligentlylearnthetaskofproducingsimilarnarrativesalongsidethe
recommendedvisualization,andfinallygetsitsdoctorateinVisualRecommendationwithNarratives,becomingDr.V-RECS.During
theexerciseofitsprofession,willDr.V-RECSbeabletomatchProfessorGPT-4’sabilities?Certainly,itsfeesaremuchcheaper.
Abstract—NL2VIS(naturallanguagetovisualization)isapromisingandrecentresearchareathatinvolvesinterpretingnatural
languagequeriesandtranslatingthemintovisualizationsthataccuratelyrepresenttheunderlyingdata. Aswenavigatetheeraof
bigdata,NL2VISholdsconsiderableapplicationpotentialsinceitgreatlyfacilitatesdataexplorationbynon-expertusers.Following
theincreasinglywidespreadusageofgenerativeAIinNL2VISapplications,inthispaperwepresentV-RECS,thefirstLLM-based
VisualRecommenderaugmentedwithexplanations(E),captioning(C),andsuggestions(S)forfurtherdataexploration. V-RECS’
visualizationnarrativesfacilitatebothresponseverificationanddataexplorationbynon-expertusers. Furthermore,ourproposed
solutionmitigatescomputational,controllability,andcostissuesassociatedwithusingpowerfulLLMsbyleveragingamethodologyto
effectivelyfine-tunesmallmodels,likeLLama-2-7B.Togenerateinsightfulvisualizationnarratives,weuseChain-of-Thoughts(CoT),a
promptengineeringtechniquetohelpLLMidentifyandgeneratethelogicalstepstoproduceacorrectanswer.SinceCoTisreported
toperformpoorlywithsmallLLMs,weadoptedastrategyinwhichalargeLLM(GPT-4),actingasaTeacher,generatesCoT-based
instructionstofine-tuneasmallmodel,Llama-2-7B,whichplaystheroleofaStudent.Extensiveexperiments-basedonaframework
forthequantitativeevaluationofAI-basedvisualizationsandonmanualassessmentbyagroupofparticipants-showthatV-RECS
achievesperformancescorescomparabletoGPT-4,atamuchlowercost.TheefficacyoftheV-RECSteacher-studentparadigmis
alsodemonstratedbythefactthattheun-tunedLlama-2-7Bfailstoperformthetaskinthevastmajorityoftestcases. Werelease
V-RECSforthevisualizationcommunitytoassistvisualizationdesignersthroughouttheentirevisualizationgenerationprocess.
IndexTerms—Visualizationrecommendation,LLM,MachineLearningforVisualization,ChainofThoughts,Low-costGenerativeAI
1 INTRODUCTION
• LucaPodoiswithSapienzaUniversity.E-mail:podo@di.uniroma1.it AIandBigDatadevelopmenthasreshapedhowcompaniesmakede-
• MarcoAngeliniiswithLinkCampusUniversityofRome,Sapienza cisions in recent years. A more data-driven approach has become
University.E-mail:angelini@diag.uniroma1.it
• PaolaVelardiiswithSapienzaUniversity.E-mail:velardi@di.uniroma1.it
4202
nuJ
12
]CH.sc[
1v95251.6042:viXraincreasinglycrucialforcompaniestomakemoreinformeddecisions, whichcanbeverifiedbycompilingandthenvalidatingtheexpected
frombigcorporationstosmallandmediumenterprises[49].Frommar- output,evaluatingthequalityofarecommendedvisualizationismore
keting[40]tobusinessintelligence,thedata-centricstrategyrepresents challenging. If users do not know what to expect, or they are not
aterrificopportunitytokeepupwithaspecificfield’sevolutionand domainexperts,theymayperceivethemodel’soutputasaccurate,even
discoverpatternsthatotherwisewouldbehidden. Notwithstanding, whenincorrect.
thisintroducesnewchallengesandcomplexity.Thedataanalysisand Tomitigatesomeoftheoutlinedproblems,inthispaperwepropose
transformationintoactionableinsightsdemandspecificdomainexperts V-RECS,aLLM-basedVisualizationRecommenderenhancedwith
andeconomicresources.Often,especiallyforSMEs,thesefactorsrep- Explanations (E), Captioning (C), and Suggestions for further data
resentchallengingaspectsduetotheneedformorefinancialresources exploration(S).ThedatavisualizationnarrativesEandCmitigatethe
torecruitthesespecializedprofiles.Furthermore,thesupplyofthese problemofresponseverification,whileSsupportnon-expertusersto
specializeddatascientistsoftenfallsshortofmeetingtheburgeoning furtherexploretheavailabledata.Thegeneratedmodel,basedonafine-
needsoftheindustry1. tuned”small"LLM,Llama-2-7B,alsomitigatescostandcontrollability
VisualizationRecommendationSystems(VRSs)trytofillthegap issues6.
between non-expert users and data analysis [36], by providing au- V-RECS’ training methodology is based on Chain of Thoughts
tomatictoolstosupportvisualdiscoveryfromdataacrossdifferent (CoT),astrategytoinstructalargeLLMtobreakdownitsresponses
sectors and applications. Many contributions have been developed intoreasoningsteps[55].SinceCoThasbeenshownnottoworkwell
in this field (among others, [15,26,29,38]), some of which gained withsmallLLM[53],weadoptateacher-studentparadigminwhich
widespreadadoptioninwell-knowntoolssuchasTableau2. Todate, GPT-4,theteacher,isaskedtoenrichwithvisualizationnarrativesan
state-of-the-artVRSsdonotachievefullautomation,relyingonbuilt-in existingdatasetofusers’queriesandcorrespondingcharts,leveraging
knowledge(suchasperceptualrules)oruserinputtoguidethesystem theCoTstrategy.Next,theenricheddatasetisusedtofine-tuneasmall
togeneratevisualizations,forexamplebyenteringnaturallanguage open-sourceLLM,Llama-2-7B,toperformvisualizationrecommen-
queries. ThislattercategoryofVRSisreferredtointheliteratureas dationsalongwithtextualnarrativestoexplainandfurtherexplorethe
NL2VIS(NaturalLanguagetoVisualization). ManyNL2VISsolu- inputdata.V-RECSexploitsthesynergiesbetweenLLMandvisualiza-
tionsrelyonclassicNLP(NaturalLanguageProcessing)orrule-based tiontoimprovethegenerationofinsightfulvisualrepresentationsfrom
techniques[13,32,42]totranslatetheuser’squery,calledNaturalLan- data,whileensuringthattheuserhasfullcontroloverthereliabilityof
guageQuery(NLQ),intoinsightfulvisualizations.However,reliance whatissuggestedbytheVisualRecommender.Themaincontributions
onthesetechniquesposesasignificantlimitation,asitconstrainsusers ofthisworkare:
toadheretospecificformats(suchastheneedtoclearlyspecifyin
• V-RECS,thefirstLLMfine-tunedforthevisualizationfieldthat
theNLQthetypeofchartandthevariablestobeplottedontheaxes),
performsvisualizationrecommendationalongwiththeproduction
therebyrestrictingtheflexibilityofexpressionintheirqueries.Further-
ofinsightfulvisualizationnarratives;
more,theseapproachesstruggletomanageambiguoustexts,failingto
produceaconsistentandinsightfulvisualizationinmanycases. • Low-costandcontrollablesolution: V-RECSisbasedonfine-
Following the disruptive use of generative artificial intelligence tuning the open source Llama-2-7B, which was estimated to
acrossapplicationdomains, LLM4VIS(LargeLanguageModelfor be100timeslessexpensivethanGPT-4, alongwiththeother
Visualizations)[53]isbecomingoneofthemostpromisingresearch previouslysummarizedadvantagesoflow-parameterOpen-AI
areastoadvanceexistingNL2VISapproaches.LLM4VISaimstouse models7. DespiteleveragingasmallLLM,V-RECSachieves
cutting-edgeLLMtechnologytoquerydatasourcesusingonlynatural performancescomparabletoGPT-4,asshowninSection6.
languagequeriesandtranslatethemintovisualizationsthataccurately
• LLM4VISmethodology:wepresentanend-to-endsolutionfor
represent the underlying data. Although LLM like Llama [51] or
LLM4VIS,contributinganovelapproachthatbridgesthegap
GPT[34], haveshownamoreadvancedsemanticunderstandingof
betweengenerativeartificialintelligence,explainablemachine
freetext[7],theysufferfromthelackofinterpretabilityofthemodel’s
learning,andthevisualizationfield.
responses.Furthermore”Large"LLM,thoseinvolvingcomputationally
intensiveandexpensivearchitecturescontainingbillions,andrecently • Acomprehensivecomparativequantitativeandqualitativeevalua-
trillions3 of parameters, also have other contraindications, such as tionexperiment,tostudyhowtheproposedV-RECSmodeland
hallucinations[16],thelackofcontrollability[47]andinterpretability GPT-4performthevisualizationtask.
[54], and the high computational cost4, hindering their large-scale
We release V-RECS for the visualization community on a GitHub
adoptioninapplications. Inparticular,controllabilityhasanotable, repository8.
andoftenneglected,impactonapplications:ifacertainmodelisbased
Thepaperisorganizedasfollows:inSection2wesummarizethe
ontheadaptationofaproprietaryLLM,suchasGPT,theevolutionof
mainconceptsofLLM.Section3isdedicatedtothestate-of-the-artin
theoriginalLLM5couldsignificantlyimpacttheadaptedmodel.This
NL2VISandLLM4VIS.Section4presentsaformaldescriptionofthe
meansthatdevelopersofamodelbasedonaproprietaryLLMdonot
LLM4VISproblem.Section5describestheproposedV-RECSmodel
havefullcontroloftheirsystem.
andtheLLM-basedmethodologytodevelopit.Section6reportsthe
Finally,althoughLLMshaveprovengreatcapabilitiesontaskslike
resultsofquantitativeandqualitativeevaluationactivities.Finally,we
text,images,orcodegeneration,ithasbeenobservedthattheyaremuch
presentadiscussionandourconcludingremarksinSection7.
lessproficientonothertasks,suchasdatavisualizationandanalysis.In
thisregard,Podoetal.[35]observedhow,eventhoughGPT-3.5-turbo
2 BACKGROUND
canalreadyperformdatavisualizationtasks,itstillproducesahigh
rateoferrorsandposesaproblemofresponseverification,especially LLMsarepowerfulmachinelearningmodelsbasedontheconceptof
whentheuserisnotsufficientlyexperttoevaluatetheresult. Unlike pre-trainingfollowedbyfine-tuning. Duringpre-training,themodel
text and images, which can be easily verified by any user, or code, learnsrepresentationsoflanguagethroughunsupervisedlearningon
extremelyvastcorporaoftextualdata. Thisphaseiscrucialforthe
1https://shorturl.at/ePUXY acquisitionofgenerallinguisticknowledge. Pre-trainingisbasedon
2www.tableau.com,whichintegratesafunctionalitynamedSuggestMe Transformer-basedarchitectures[23],capableofcapturinglong-range
[30]
3aspostulatedbyseveralsources,amongwhichhttps://the-decoder. 6Llama-2-7Bhassevenbillionparameters,while,e.g.,GPT-4hasonetrillion.
com/gpt-4-has-a-trillion-parameters/ Furthermore,Llama-2-7Bisavailablefreeofchargeforresearchandcommercial
4inhttps://shorturl.at/puz24-amongotherpublishedexperiments- use;incontrast,GPT-nmodelsareproprietary,andtheirinnerworkingsare
itisshownthatLlama2obtainssimilarperformancetoGPT-4forafractionof hiddenfromthepublichttps://shorturl.at/bdoOZ
thecostonanumberoftasks 7amongstthemanystudies,seee.g.,https://tinyurl.com/56kpjd7r
5Forexample,therearenotabledifferencesbetweenGPT-3andGTP-4. 8https://github.com/lucapodo/V-RECS.gitdependenciesandcontexts,essentialforunderstandingthecomplex- variablestobeconsideredinthechart,avoidingsemanticallyambigu-
itiesofhumanlanguage. Followingpre-training,fine-tuningtailors oussentences,etc.). Adeparturefromthesesyntacticapproachesis
theLLMtospecificdownstreamtasksbyexposingittolabeleddata NcNet,proposedby [28].Theauthorsproposetouseaseq2seqneural
andadjustingitsparametersthroughsupervisedlearning.Thisprocess networkmodeltotranslateuserqueriesintovisualizations,leveraging
adaptsthemodel’slearnedrepresentationstothespecifictaskathand, anovelvisualizationgrammarnamedVegaZero.VegaZerostreamlines
enhancingitsperformance.Recently,LLMshavebeenadaptedtopro- thecomplexVega-Litespecificationintoamoreintuitivesequence-
cessandgeneratedifferenttypesofunstructureddatabeyondtext,such basedkeywordformat.Generatingaplaintextsequenceprovestobe
asimagesandvideos,music,climatedata,code,andvisualizations, moremanageableforaseq2seqmodel,incontrasttothechallenges
whichisthemainfocusofthispaper. associatedwithhierarchicaloutputs,suchasforVega-Lite.
AnotherrelevantconceptinLLMsisPromptengineering[39],a Despite the advancements introduced, classic NLP systems en-
techniqueinvolvingthedesignofeffectiveinstructions(prompts)or counter various challenges. Primarily, they depend on the well-
inputexamplestoguidethebehavioroflanguagemodelsduringfine- structuredsyntaxofuserqueriesforeffectiveprocessing.Furthermore,
tuningorinference. ItaimstodesigninstructionsfortheLLMthat failuretounderstandthesemanticsofthequeryfailstounderstand
elicitthedesiredresponsesfromthemodelwhileminimizingambiguity whattheuserwantstoexplore.
andbias. Promptengineeringisakeyareaofresearchtoextendthe
capabilitiesofstate-of-the-artLLM.Wecandistinguishbetweentwo 3.2 LargeLanguageModelsforvisualization
mainpromptingtechniques:zeroandfew-shotprompting.Zero-shot
WithinthefieldofNL2VIS,LargeLanguageModelsforVisualiza-
[20]representsaparadigmwherethemodelistaskedwithperforming
tion(LLM4Vis)isemergingasthenewfrontierfortranslatinguser
agivenobjectivesolelybasedonataskdescriptionwithoutexplicit
queries into consistent visualizations. LLM4VIS seeks to integrate
examplesillustratinghowtoaccomplishit.Thismethodologyleverages
LLMsintodatavisualizationpipelinestoachievefullsupportforvi-
themodel’sembeddedknowledgetogenerateappropriateresponses.
sualizationautomation.Theyhaveshownanincreasingproficiencyin
Conversely,few-shotlearning[37]enhancestheLLMcapabilitiesby
comprehendingtheunderlyingsemanticsofhumaninstructions[14],
augmentingthetaskdescriptionwithasmallsetofexampleinputs
representingacompellingopportunitytoovercomethelimitationsin-
pairedwiththeircorrespondingoutputs.
heritedbythe”classic"approachesillustratedintheprevioussection.
Whilethistechnologyisstillinitsearlystagesandrequiresfurther
3 RELATEDWORK
explorationandstudy,itnonethelesspresentsanemergingopportunity
AVisualizationRecommendationSystem(VRS)isasystemdesigned forthevisualizationfield,asdiscussedbySchetingeretal.[41]. We
tosuggestinsightfulvisualizationsgivenadataset.AsoutlinedbyPodo cancategorizetheLLM4VISapproachesintotwomainclasses.The
etal.[36],therearetwoclassesofVRS:task-agnosticandtask-aware. firstclassencompassesallthemethodsthatinvolvethemodelinits
Thetask-agnostics[11,15]requiresolelythedatasetasinputwithout standardconfiguration(theso-called”plainfoundationmodels")and
additionalguidancetogenerateinsightfulvisualizations.Mostofthe usepromptengineeringtechniquestoimprovethemodel’sperformance.
approachesrelyonclassicalmachineordeeplearningmethods.Recent Conversely,thesecondclassincludesalltheapproachesthatinvolvea
advancementshaveexploredleveragingLLMs[10,53]toperformthis "fine-tuning"phasetospecializethefoundationmodeltowardaspecific
task. Inthiscase,thevisualizationrecommendationprocessissplit setoftasks.
intomultiplesteps(e.g.,datasetfiltering,datasetdescription,column LLM4VISprompttuningapproachesAnovelmethodfortranslating
selection),withtheLLMinvolvedinallorsomeofthesesteps. naturallanguagequeriesintovisualizationisproposedbyChenetal.[4].
Whileattainingfullautomationinvisualizationrecommendation TheauthorsinvolveCodex-apre-trainedLLMdevelopedbyOpenAI
pipelineswouldsignifyaremarkablemilestone,itisstillintheearly
fornaturallanguagetocodetranslation9-configuredwithinafew-shot
stages,facingseveralchallengesthathavenotyetbeenexplored[36]. learningpromptsetup.Theyproposeatwo-stepstrategytorecommend
Conversely,task-awareVRSrequiresuserinputorguidance,typically a visualization. Firstly, the input is shaped as examples of natural
asuserqueries,togeneratevisualizations. Task-awareVRSsareex- languagetoSQLpairsalongwiththeuser’squery,lettingthemodel
tensivelystudiedintheliteratureandfallwithinthedomainofNatural generatethemostappropriateSQL.Theseexamplesserveasreference
LanguagetoVisualization(NL2VIS).InNL2VIS,thesystemisguided points,guidingthemodelinaddressingnewuserquerieseffectively.
byanaturallanguagequerythattellsthemodelwhattoexplorein Then,theresponsegeneratedbythemodelistranslatedintoVega-Lite
adataset(e.g.: Whatarethepaymentdateofthepaymentwiththe specificationsusingarule-basedapproach[5]. Maddiganetal.[31]
amountpaidhigherthan300orwithpaymenttypeischeck,andcount presentacomparablestudyinvolvingCodex,GPT-3,andChatGPT,
thembyabarchart,orderbytheyaxisfromlowtohigh?).Recently, assessingzero-shotpromptingtoevaluatethequalityofthesemodels
thefieldhasbeensignificantlyinfluencedbyLLMs,whichdemonstrate ongeneratingvisualizationsasPythoncode. Anotablelimitationof
considerableabilityinunderstandinghumanlanguage,allowingthem theseworksisthelackofcomprehensivediscussionregardingresults.
toaddressthechallengeofambiguityinherentinuserexpressionsthat Instead,theyrelyprimarilyonasmallnumberofqualitativeevaluation
currentlylimitexistingapproaches. experiments,merelyprovidinganoverviewofpotentialimplementa-
tions.Lietal.[22]introduceSheetcopilot,anLLM-basedsystemthat
3.1 NL2VISapproaches usesnaturallanguagequeriestodriveuserinteractionwithinspread-
sheetsoftware,suchastablefilteringorchartgeneration.Sheetcopilot
ThedevelopmentofNL2VIShaswitnessedsignificantadvancements, involvesin-contextlearningpromptingtechniquetointeractwithall
primarilythroughclassicalNLtechniques. spreadsheetfunctions,employingGPTasmodel.Itutilizestheuser’s
AsdiscussedbySetluretal.[42],theauthorsintroduceaninteractive query alongside a prompt encompassing the model task, a compre-
visualizationtoolthatemploysahybridapproachintegratingNLPand hensivelistofsoftwarefunctions(i.e.,atomicactions),andanoutput
decisionrules.Thissystemcombinesaprobabilisticgrammarapproach format.Withtheseinputs,Sheetcopilottranslatestheuserrequestinto
withpredefinedrulesthatcanbedynamicallyupdatedbasedonthe asystematicplan, outliningstep-by-stepactionstofulfilltheuser’s
underlyingdata.AnotherexampleisproposedbySrinivasanetal.[45], queryeffectively.AsimilarapproachisDatacopilotintroducedbyLin
wheretheauthorspresentBOLT,asystembasedontraditionalNLP etal. [24].Datacopilotisamethodtogenerateautomatedinterfaces
techniquestomapuserqueriestoprevalentdashboardobjectivesand andautonomousworkflowsforhuman-tableinteraction,solvingdiffer-
generateappropriatevisualizations.Incontrast,thesolutionproposed entdata-relatedactivities,suchasacquisition,processing,forecasting,
byNarechaniaetal.[33]focusesongeneratingasinglevisualization tablemanipulation,andvisualization. ItinvolvesGPTinin-context
ratherthansuggestingacompletedashboard. learningtogenerateanddeployadataprocessingworkflowusingsmall
Whiletheseapproachesprovidereliablesolutions,theyimposeinput inputguidance. However,bothSheetcopilotandDatacopilot,while
constraintsbyaskinguserstoadheretospecificinputtemplatesrather
thanfreelyusingnaturallanguage(forexample,explicitlynamingthe 9https://openai.com/blog/openai-codexcapable of generating visualization, are not optimized for this task. theoperationsduringthereasoningprocessandprovideaverification
Theirprimaryfocusliesingeneratinginstructionsorcommandsto involvingthevisualizationnarrative.V-RECSprovidesamethodology
fulfilluserrequestswithinsoftwareenvironmentslikespreadsheetsor toadaptsmallermodelstoperformasthelargerones, asprovedin
toolsetsratherthanoptimizingforinsightfulvisualization. theevaluation. Remarkably,V-RECSleveragesa”small"andopen-
Overall,whileexistingprompttuningapproachesoffercost-effective sourceLLMmodel, Llama-2-7B,thusmitigatingcost, privacy, and
deploymentatinferencetime, theycomewithlimitations. Relying controllabilityissuesmentionedinSection1.
solely on prompt engineering results in a lack of control over the
model’sbehaviorintaskexecution.Additionally,themajorityofthese 4 PROBLEMFORMULATION
methodsdependonproprietarymodelslikeGPT.Anyupdatesmadeto WecanformulatethegoalofLLM4VISsystemsasmaximizingthe
thesemodelsmayintroducenewbehaviors,potentiallyundermining probabilityofsuggestingthebestvisualizationtoauser,givenadata
theefficacyoftheproposedmethodologies. tableT andauserqueryQ:
LLM4VISfinetuningapproaches.Adistinctapproachtoaligning
aLargeLanguageModel(LLM)withthevisualizationtaskinvolves f i(Q i,T)→argmax ViP(V i|T,Q i) where V i:{v i,j|0< j≤n} (1)
fine-tuning the model. Zha et al. [56] introduce TableGPT, a fine-
tunedmodeldesignedforqueryingdatatablessolelythroughnatural whereQ
i
istheuserqueryattimei,Tisthetablerepresentation
languagequeries. Tomitigateinputtokenlengthconstraints,theau- passedtothemodelandV i istheoutputvisualizationsset(asingle
thorsemployanencodertopreprocessthetable,extractingitsvector visualizationoradashboard).Notethat,commonly,V isnotdirectlya
representation.Subsequently,theembeddedrepresentationalongside chart,butrather,thespecificationofachartinavisualrepresentation
theuserqueryisfedintotheLLM,whichgeneratesacommandset language, typically, a JSON-based language like Vega-Lite and the
and textual reply. Unlike the previous approach, which focuses on simplerVegaZero[28].
table interactions workflow, Tian et al. [50] propose ChartGPT, an ThepreviousisageneralformulationthatalsoappliestoNL2VIS
LLMstrictlytailoredforvisualizationrecommendation.Theauthors systems. LLM4VISsystemsintroduceaseriesofintermediatetasks,
fine-tunedtheopen-sourcemodelFLAN-T5-XL[8],bysplittingthe illustratedinFigure2. First,itiscrucialtonotethatthemodeldoes
datavisualizationgenerationproblemintotwobroadtasks(i.e.,data not directly receive the original data table T as input, but rather T,
transformationandvisualizationrecommendation),eachconsistingof whichresultsfromatabletransformationoperation.GiventheLLM’s
threesub-tasks.ChartGPTfocusessolelyonthevisualizationrecom- limitedinputsize,dependingonthenumberofinputtokensamodel
mendationtask,offeringanovelLLMadaptationstrategy. However, canaccept,itbecomesessentialtopre-processtheoriginaltableinto
itlacksacomprehensivevisualizationnarrative.Whileitcansuggest aformatsuitableformodelinput.Tofacethisproblem,forexample,
optimal visualizations based on user queries, it fails to explain the Zhaetal.[56]transformthetableusinganencoder;differently,Tian
reasoningbehindtherecommendations. etal. [50]proposetouseonlythetabledescriptionwithsomerows
toprovidethemodelabettercontextofthedata.Followingthetable
3.3 Generatingvisualizationnarratives transformation,theinputisshapedbyapromptfunctionP(u i,T)(see
Section2)thatmapsthequeryandtablerepresentationtoadesigned
Onceamodelproducesavisualizationorprovidesaresponsetoauser
prompttemplate. Afterthat,theLLMshouldperformthefollowing
query,endusersfaceconsiderabledifficultyindeterminingtheaccu-
taskstoproduceV (asshowninFigure2,left):
racyoftheprovidedoutput.Comparedtotextualresponses,verifying
visualizationsposesamoredifficultchallenge. Whiletaskssuchas • columnsselection.Oncethemodelreceivesthepromptasinput,
textsummarization,contentgeneration,orcodegenerationallowfor themodelhastodeterminethemostmeaningfulandprominent
relativelystraightforwardoutputverificationbyend-users,thesame columnsfromthedatatablebasedontheuserquery.
cannotbesaidforvisualizationsbecausenon-expertusersmayinadver-
• datatransformation. Afterthemodelhaschosenthecolumns
tentlyconsidermisleadingrepresentationsasaccurate.Consequently,
basedontheuserqueryandthecolumn’sdatatype, themost
theproductionofa”visualizationnarrative"(adescriptionofwhyarec-
appropriatedatatransformationfunctionsshouldbeselected.For
ommendedvisualizationfitsauser’sintentionsimpliedbythenatural
instance,themodelshoulddeterminewhethertoaggregate(e.g.,
languagequery)emergesascrucial.Thetaskofgeneratingavisualiza-
min,average,max)differentdatacolumnsorvalues.
tionnarrativedeliversatextualelucidationoftheunderlyingprocess
behindthevisualizationgenerationprocessperformedbythemodel. • visualizationrecommendation.Basedontheoutputoftheprevi-
Thisnotonlyenhancestransparencybutalsoreducesthepotentialfor oussteps,themodelmustchoosethebestvisualizationelements
misinterpretationorerroneousacceptanceofvisualizeddata. (axes,marks,presentation,etc.)tomapthedataandgeneratethe
AfirstapproachinthisdirectionhasbeenproposedbySultanum visualizationascode(e.g.,matplotpythoncodetoproduceabar
etal. [46]. TheauthorsproposeDatatales, amethodtogeneratea chart),image(e.g.,barchartaspixelrepresentation),orgrammar
visualizationnarrativewhengivenavisualizationandanannotation (e.g.,avisualizationexpressedusingVegaLitespecifications).
asinput. Oncethesystemreceivestheinput,itispassedtoGPT-3,
Inadditiontotheabovetasks,inthispaperweproposethattheLLM
whichprovidesatextualnarrativebasedonasimpleprompt.Another
modelshouldalsoperformthreeadditionaltasks(seethegreenboxes
similarapproachisproposedbyKoetal.[19]. Theauthorspresent
inFigure2):
aVL2NLframeworktogenerateavisualizationnarrativegivenaVe-
• visualization caption. The model should generate descriptive
gaLitespecification. Itacceptsasinputsolelythespecificationand
captionsfortheproducedvisualizations.Thesecaptionselucidate
producesavisualizationdescriptionandadatadescriptionasoutput.
thevisualrepresentation’scontentandmeaning,helpingtheuser
Additionally,itprovidesasecondoutput,representingapossiblequery
understandthedatashown.
thatagenericusercouldusetogeneratethevisualization. Notethat
bothDatatalesandVL2NLperformadifferenttaskfromNL2VIS,since • visualization explanation.Subsequently, the model should ex-
thevisualizationisaninput,ratherthananoutput. plainwhyspecificcolumnshavebeenselectedfromtheavailable
Unlike all the methods in the literature, we propose V-RECS, a dataset. Thistransparencyisessentialforuserstocomprehend
model capable of not only generating a consistent visualization, as thedecisionsbehindthemodel’schoices,enablingthemtoverify
proposedbyTianetal.[50],butalsoexplainingthegeneratedoutput therelevanceandaccuracyofthegeneratedvisualizations.
byintegratingvisualizationnarrativecapabilities,assuggestedbyKo
• queriessuggestion.Finally,themodelshouldoffersuggestions
et al. [19]. V-RECS aims to combine the strengths of the existing
foradditionaldataqueries,guidingusersonwheretodelvedeeper
strategiestodeliverasystemcapableofgeneratingandexplaininga
intothedatasetforfurtherinsights.
visualizationinafully-flaggedautomaticway.Furthermore,V-RECS
isdesignedtobeindependentofadditionalsoftwareortoolstoproduce WenameV-RECSthespecializedLLM4VISmodelabletogenerate
the expected output, as seen in [4,22,56]. Instead, we perform all visualizationrecommendationsalongwiththeabovetasks,referredData Source Input Data processing Visualization narrative
Data-utterance pair Prompt design LLM sub-actions Explain the visualization
u: User

i utterance
Prompt
 Columns
 Data
 Visualization
 Visualization
 Visualization
 Suggestions of
template selection transformation recommendation caption explanation further queries
T:
Table
transformation
Fig.2:V-RECSmodeltaskworkflow.ThethreegreenboxesrepresentV-RECS’specifictasks,whiletheothersarecommontoallLLM4VISmodels.
toasgenerationofvisualizationnarratives. Accordingly,theformal Thefew-shotconfigurationnecessitatesasetofquestion-answerpair
specificationoftheV-RECSmodeltaskisasfollows: examplesinadditiontothenewone.Inthiscase,theprompttypically
comprisesmultiplequestion-reasoningsteps-answertriplets,guiding
f i(Q i,T)→argmax ViP(V i,N i|T,Q i) where V i:{v i,j|0< j≤n} themodelthroughproblem-solving.Conversely,thezero-shotconfig-
(2) urationrequiresaspecifictextformulation,’Let’sthinkstepbystep,’
whereN idenotestherichnarrativeaccompanyingthegeneratedvisual- withintheprompt,tellingthemodeltosplittheproblemintosequential
ization. sub-tasks.Forinstance,aquerylike’Q:howmuchis2+2?Let’sthink
stepbystep’drivesthemodeltobreakthequestionandsolveeachpart
5 V-RECSMODEL individually,culminatinginafinalresponse.Unlikethefew-shotvari-
ant,zero-shotCoTdoesnotprovideexamplesoftheproblem-solving
Inthissection,wedescribethemethodologytoimplementV-RECS.
process but only relies on the model’s embedded knowledge. CoT
Theproposedmethodologyisbasedonfine-tuningasmallmodelina
hasdemonstratedsubstantialadvantages,particularlyininferringnew
supervisedmanner,usinganexistingdatasetenrichedwithvisualization
taskswithminimaleffort,withoutnecessitatingstructuralchangesto
narrativesgeneratedbyamorepowerfulmodel.Themethodologyis
theunderlyingmodelarchitecture[55]. However, itsapplicationto
depictedinFigure1(theteaser).First(boxA,BofFigure1),GPT-4,
modelswithfewerthan100billionparametershasbeenassociatedwith
actingasaTeacher,isinstructedtogeneratevisualizationnarratives
logicalinconsistenciesduringthereasoningprocess[55].Toaddress
leveraging the Chain-of-Toughts technique proposed in [19]. The
thesediscrepancies,fine-tuningsmallermodelsemergesasaviable
Teacherreceivesasinputtriplesfromanexistingdataset,Ncnet[28],
solutiontoenhancetheproficiencyofsmallmodelsinsolvingcomplex
composed by a dataset D, a user’s query Q, and a visualization V
tasks,suchasdatavisualization.Nonetheless,thisapproachpresents
matchingtheuser’sneedsexpressedbyQ.BasedontheCoTapproach,
itslimitations. Fine-tuningsmallermodelsoncomplextasks,using
wehavedefinedthreetasks,eachwithadifferentgoal.Thefirsttask,
question-answerpairsalone,devoidofreasoningsteps,oftenyields
T1,aimstoexplainwhyavisualizationVisacorrectanswergiventhe
incorrectresponses. Toovercometheselimitationsandaugmentthe
user’squeryQ,whiletheT2goalinvolvesdescribingthevisualization.
capabilitiesofsmallermodelsindatavisualizationtasks,wepropose
Differently,thegoalofT3istosuggestpotentialareasofinterestfor
leveraginganovelapproachknownasFine-tuningbasedonCoT[19].
furtherexplorationtotheuser.Next,theteacher-generatedresponses
ThismethodcombinesbothCoTandfine-tuningbyenrichingquestion-
forthethreetasksarecombinedusingacustomprompttemplatefor
answerpairswithoutreasoningstepswiththesub-tasksneededtosolve
thesubsequentfine-tuningphase(boxesCandDinFigure1).
the problem posed, improving the reliability of a smaller model in
Then,theresponsesproducedbythelargermodelareusedas”ex-
producing correct outputs [1]. The process involves exploiting the
pertise"toteachasmallerLLMmodel(theStudentLLama-2-7B)to
embeddedknowledgeofalarger’Teacher’model,suchasGPT-4,ina
behavesimilarly(boxE).Atinferencetime(rightsideofFigure1),the
zero-shotCoTconfiguration,toprovideresponsesenrichedwithreason-
fine-tunedmodel,namedV-RECS(VisualizationRecommenderwith
ingsteps.Subsequently,asmaller’Student’modelisfine-tunedbased
Explanations,Captions,andSuggestions),receivesasinputa(D,Q)
ontheseresponses, effectivelyleveragingtheTeacher’sknowledge.
pairandrecommendsavisualizationValongwithavisualizationnar-
Forexample,forthequestion:’Ajugglercanjuggle16balls.Halfof
rativecomposedbyanexplanation(E),acaption(C),andsuggestions
theballsaregolfballs,andhalfofthegolfballsareblue.Howmany
forfurtherexplorations(S).Toinstructthemodelsweusethezero-shot
bluegolfballsarethere?’,theTeacheristaskedwithazero-shotCoT,
promptingstrategy(seeSection2)fortheTeacherandfully-supervised
providinglogicalstepstoproducetheexpectedsolutionsuchas: ’Step
trainingfortheStudent.Atinferencetimeinstead,V-RECSworksin
1:Thereare16ballsintotal.Step2:Halfofthemaregolfballs.Step
zero-shot.Inwhatfollowsweexplainthemethodologyinmoredetail.
3:Therefore,thereareeightgolfballs.Step5:etc.’.Then,theresponse
generatedandtheoriginalquestionareusedtofine-tunetheStudent
5.1 Modeltrainingmethodology
model,therebyleveragingtheTeacher’sknowledgetoenhancethefinal
AsdescribedinSection4,thegoalofV-RECSistorecommendvisual- performanceofthegeneratedmodel.
izations,alongwithrichnarrativestoexplainandextendrecommended
visualizations.Furthermore,wealsoaimtobuildoursolutionbylever- Whilefine-tuningbasedonCoThasbeenusedtoaugmentdatasets
agingasmallandopen-sourceLLMmodel,theadvantagesofwhich for solving scientific problems [25], its application to visualization
wehavealreadydescribedinSections1and3. Smallermodelsspe- tasksremainsunexplored. BuildinguponthemethodologybyKoet
cializedinaspecifictask,withcomparableperformancetolargerones, al.[19],wehavedevisedacustomCoTstrategyfortheV-RECSprob-
representaterrificopportunitytoreachthesameresultsataminor lemdomain,involvingthree’Teachers,’eachtaskedwithaddressing
cost,whileleveraginganopensourcesoftware. Intheliterature,ap- specificchallengesrelatedtotheV-RECStasks,includingvisualization
proachesaimedatenhancingthecapabilitiesofLLMscanbebroadly recommendationandcaptioning,explanationoftheprocessofcolumn
categorizedintothoseleveragingthemodelinitsexistingform,pri- selectioninTandfiltering,andsuggestionofadditionalquestionsfor
marilythroughprompttuning,suchasChain-of-Thoughts(CoT)[55], dataexploration. TheTeachers’tasksaredelineatedinTable1,with
andmoreadvancedtechniquesinvolvingupdatingthemodelweights, variationsbasedontheinputandintendedgoal.
likethefine-tuning(seeSection3.2).CoTisoneofthemostpopular
methodologiesamongtheformerapproaches.Itpushesthemodelto Finally,theresponsesgeneratedbythethreeTeachers,alongwith
emulatehuman-likecognitiveprocessesbytellingittobreakdowncom- theoriginalqueryQandtheexpectedvisualizationVarecombined
plexproblemsintomoremanageablesub-problems,enhancingresponse accordingtoacustompromptingstrategyusedtofine-tunetheStudent,
accuracy. CoTcanbeclassifiedaszero-shot[20]andfew-shot[55]. asdetailednext.Table1:DescriptionoftheTeacher’stasks
A Let's generate an explanation step by step.

Task Input Goal Description
Consider the input template to be the following:
Query, Explaintotheuserwhythefeaturesusedin
mark [T] encoding x [X] y aggregate [AggFunction][Y] color
T1 VegaZero, Explain VegaZeroarethemostappropriategivenall
[Z] transform filter [F] group [G] bin [B] sort [S] topk [K]


Dataset thedatasetfeaturesandtheuser’squery
T2 VegaZero Describe Describethecontentofthevisualization
Step 1. Extract the input [X] and [Y] values based on the
Discoverthemostinsightfulquestionsto
T3 VegaZero Suggest provided template. Refer to this: {VEGAZERO}

suggesttotheuserforexploringthedata B
Step 2. Based on their request, generate a description of
what the user is looking for in the dataset. Refer to this user
request: {QUERY}

5.2 Implementationdetails Step 3. Explain why the features selected are the best
among all the others from the dataset based on the user
Inthissectionweprovidetherelevantimplementationdetailstorepli- request. Refer to this dataset: {DATASET}


cateourmethodology.Additionaldetailscanbefoundintheannexed
SupplementaryMaterialandattheV-RECSGitHub10. ##

Step 1. Columns selections:

VisualizationrepresentationAchallengingaspectconcernsthechoice
- X-axes [X]:
ofavisualizationdescriptionmethod. Avisualizationcanberepre-
C - Y-axes [Y]:

sentedasadeclarativespecification,abitmapimage,orthroughits - Transform filter [F] and [G]:

code.Thefirsttypeisbasedonvisualizationgrammars,likethepopular Step 2. Description:

Vega-Lite. Thesecondtypeconsidersthevisualizationasanimage, Step 3. Explain:
andthethirdtyperepresentsthevisualizationasasetofinstructions,
forexample,usingMatplotlibinPython. Pixel-basedvisualizations
wouldaddlimitationsandcomplexitytoourapproach,asLLMsare Fig.3:StructureoftheT1prompt.TherelatedtaskisdescribedinTable
1. Ithasthreeparts: AencouragestheLLMtoreason”step-by-step"
primarilytrainedusingtextualdata,whilelimitinginteractivityand
as per the Chain of Thoughts method, B includes the description of
deployability.Similarly,employingacode-basedvisualizationmethod
sub-tasks(steps)toguidethemodel,andCistheresponsetemplate.
wouldhinderthecreationofapureLLMapproach,requiringaspecific
codeenvironmenttorenderandgeneratevisualizations.Differently,a
grammar-basedvisualizationcanbeconsideredagnostictotheused
renderingenvironment,makingitmoregeneralthancode,whilestill 1. Theheaderincludestheexplicittaskthemodelissupposedto
formattedasstructuredtext.Forthisreason,wechosegrammar-based pursue.Itdrivesthemodeltounderstandwhatithastoachieve.
visualizationsasthemostsuitableforourscenario. Specifically,we Alongwiththetask,thispartalsoincludestheVegaZerotemplate
haveoptedforVegaZero,proposedin[28]. structureand,finally,the"Let’sthinkstepbystep"formula[20]
SelectionofthetrainingdatasetVariousdatasetshavebeenproposed whichisthebasisoftheChainofThoughtpromptingmethod.
intheliteratureforNaturalLanguagetoVisualization(NL2VIS)tasks,
2. Theinputincludestheuserqueryandthedatasetdescription.
withNvBench[27]emergingasthemostprominentandwidelyadopted.
Thedatasetisembeddedusingonlythedatafeaturenamesand
However, itexhibitslimitationssuchasalackofcomplexityinthe
relatedtypes.
visualizationtypes,overlyexplicitnaturallanguagequeries(exceptfor
someinstancespre-classifiedas”hard"),andtheabsenceofsupport 3. The body consists of the intermediate or reasoning steps the
forvisualizationnarrativesorreasoningsteps. Whileotherdatasets model should follow to select proper visualization properties,
likeNLVCorpus[44]orQuda[12]providealternatives,theyalsocome suchastheaggregationfunction.Thispartiscrucialtoprovide
withtheirownconstraints. Inparticular, NLVCorpusincludesonly additional information to the end-user so that they can under-
893 visualization-querypairs, and Quda doesnot provide the chart standthemodel’sdesignchoicesandgeneratethefinalresponse.
associated with the queries. In the absence of a dataset capable of Addingthispartenablestheexplainabilityofthegeneratedre-
supportingallourrequirements,weoptedtochooseNVBenchasthe sponse.
best fit for our purpose. We chose the NvBench version provided
4. Theresponseisthelastpartoftheprompt,whereweprovidethe
byNcNet,asitalignscloselywithourtaskrequirements,providing
structureoftheanswerthatmustbepresentedbythemodeltothe
thequeries,visualization,anddatasources.Specifically,thisversion
end-user.ItincludestheVegaZerospecificationVtogeneratea
excludesvisualizationsgeneratedfrommultipledatatablesandadds
chart,andtheE,C,Striplethatmakesupthenarrativeexplaining
VegaZerorepresentationstailoredtoeachquery.Otherdatasets,such
therecommendedvisualization.
asVisText[48]orVIS30k[3],lackthequery-vis-datatriplesandfocus
solelyoncaptiongeneration. TheStudentprompttemplate,alongwithafilledexampleoftraining
Teacher and Student prompting GPT-4 has been selected as the instance,isdepictedinFigure4.Theleft-handsideofFigure4shows
Teacherandpromptedwiththreedistincttasks(T1,T2,andT3),de- howthedifferentcomponentsoftheTeachers’responsescontributeto
scribedatahigh-levelinTable1,basedontheinputandtaskdelivered. fillingthestudentprompt.
Furthermore,leveragingtheworkofKoetal.[19],weprovideeach Fine-tuning The last step is fine-tuning the model. As a student,
promptwithin-contextreferencetotheVegaZerotemplatetohelpthe wehaveusedLlama-2-7B[51],thesmallestfromtheLlama2family.
modelgenerateconsistentvisualization.EachtaskistailoredforZero- To fine-tune Llama-2-7B, we generated a dataset comprising 2938
shotCoTsupplementedwithstrategicallyembeddedsub-tasks,denoted triples,eachconsistingofadatasetD,auser’squeryQ,andaVegaZero
hereas”steps". Thesestepsactas”guidingprinciples",offeringthe specificationV,followingthesamedistributionofquerycomplexity
Teachersubtlecuesonapproachingandunravelingthetask.Thesteps oftheoriginaldataset. DrawingfromtheQLoratechnique[9],our
varyforeachteacher,basedontheirtask. trainingapproachwentbeyondattentionmodules[52]toincorporate
Figure3showsthedetaileddescriptionoftheprompttemplatefor linearcomponents. QLorahasshownverypromisingresultsduring
T1(Explain),splitintothreepartsandrelatedsteps.Theotherprompt training.Withreferenceto[9],whenconfiguringLora,weoptedforan
templates,alongwithamoredetaileddescription,canbeinspectedin
rvalueof64andanαvalueof128.Duringfine-tuning,weemployed
theSupplementaryMaterial. a batch size of 4, a learning rate of 1e−4, and utilized the AdamW
optimizer.
Next, all the teachers’ responses R i are combined into a prompt
templatetoinstructtheStudent,consistingoftheheader,theinput,the Theresultingfine-tunedmodelisV-RECS.TheV-RECSmodelwas
body,andtheresponse,asfollows: deployedonAWSSagemakerwithanA10GNvidiaGPU,fedusing
onlytheuser’squeryandthedataset.Differently,GPT-4operatedina
10https://github.com/lucapodo/V-RECS.git zero-shotCoT,withpromptmirroringusedduringV-RECStraining.Example
Fine-tuning prompt template
Your task is to recommend and explain the best visualization,
Header using the VegaZero template, for a user query on a dataset:
Your task is to recommend and explain best mark [T] encoding x [X] y aggregate [AggFunction][Y] color [Z]
visualization, using the VegaZero template, for transform filter [F] group [G] bin [B] sort [S] topk [K]


a user query on a dataset:
mark [T] encoding x [X] y aggregate Let's think step by step.


[AggFunction][Y] color [Z] transform filter [F]
Explain group [G] bin [B] sort [S] topk [K]

 ## Query:

S1 Return the average price of products that have each category code

R1 S2 Let's think step by step.

 ## Dataset:

S3 Input [('product_id', 'numeric'), ('parent_product_id', 'numeric'), ...]


## Query:

{USER QUERY}
 ## Reasoning process:

Describe
S1 ## Dataset:
 Step 1. Columns selection:
{DATASET DESCRIPTION}

 - X-axes [X]: product_category_code 
R1 S2
Body - Y-axes [Y]: mean product_price ... 
S3 ## Reasoning process:
 Step 2. Chart selection:

Step 1. Columns selection: S1
 - Aggregation function [AggFunction]: mean 
Suggest Step 2. Chart selection: S2 - Color [Z]: Not specified 
S1 Step 3. Columns math operation selection: S1

 - ... 
R1 S2 Response Step 3. Columns math operation selection: Mean


S3 ## Response:

Step 1. Vegazero visualization: {VEGAZERO}
 ## Response:

Step 2. Visualization explanation:
 Step 1. Vegazero visualization: {VEGAZERO}

S3
 Step 2. Visualization explanation:
S3
 The user is interested in understanding the average price of ...

Step 3. Insights suggestions: S3 The visualization is a bar chart where the x-axis represents ...

Step 3. Queries suggestion: Which product category has the
highest average price? ...
Fig.4:Fine-tuningprompttemplate(Studentprompt).TheleftmostboxshowshowtheresponsesR1,R2,andR3contributetofillingthedifferent
partsofthetemplate.Therightmostboxisa(partly)filledtemplateforaspecifictraining/testinginstance.
Figure5showsanexampleofV-RECSresponseatinferencetime, □Syntaxcorrectness:Forthislevel,weemployedtheAltairPython
whengivenadatasetDandqueryQ. library’srendertovalidateVegaLitespecificationsgeneratedbyGPT
andaparserdevelopedbyVegaZeroauthorsforV-RECS.Theeval-
6 EVALUATION uationmetricusedtocomparecorrectspecificationsagainstthetotal
evaluatedexampleswasaccuracy.
ThissectionpresentsacomprehensiveevaluationofV-RECS,encom-
□Datamapping:Thislevelaimstoassessthemodel’sabilitytoselect
passing quantitative and qualitative assessments. In both types of
appropriatecolumnsfromdatasetsbasedonuserqueries.Asevaluation
evaluation,wecomparetheresultsgeneratedbyV-RECSwiththeones
metrics,weemployeddatamappingaccuracy,computedbycomparing
generatedbyGPT-4.Whileabaselineevaluationwasinitiallyplanned
thecolumnsselectedbyeachmodelagainsttheground-truthcolumns
withLlama-2-7B(theun-tunedmodel),extensiveexperimentationwith
inNcNet.
prompttemplatesandincrementalpromptconstructionyieldedunsatis-
□MarkcorrectnessandAxesquality:Theselevelswereevaluated
factoryresults,leadingtoitsexclusionfromfurtherevaluation.
based on accuracy, comparing predicted marks and axis mappings
The evaluation was carried out using EvaLLM [35]. Similar to againstgroundtruth.
thepopularISO-OSIstack,EvaLLMintroducesalayeredevaluation □Captionquality:Captionqualityisanadditionallevelthatwasnot
frameworkwhereeachlayer’soutputservesastheinputforthelayer present in the original EvaLLM framework. To assess whether the
aboveit,includingfivelayers.Thelowerlayersfocusonthestructural visualizationnarrativewasmeaningful,wefirstembeddedthenarra-
aspectsofthevisualization, whiletheupperlayersdelveintomore tivegeneratedfrombothcomparedmodels(GTP-4andLLama-2-7B)
semanticandinsightfulproperties. andthegroundtruthnarrativeusingaword2vecprovidedbytheSpacy
1. □ The code layer serves as the foundational level, evaluating Pythonlibrary11.Then,wecomputedthecosinesimilaritybetweenthe
fundamentalstructuralproperties. groundtruthembeddedresponseandeachmodel’sembeddedresponse.
Finally,wecomputedthemeanofallcosinesimilarities,returningfor
2. □Therepresentationlayeraddressescorevisualizationproperties eachmodelthemeancosinesimilarityasameasureofcaptionquality.
basedonestablishedrepresentationrules. □Significance:Thiswasthemostchallengingevaluationtasksinceit
focusesonthecorrelationbetweenuserqueries,datasetcharacteristics,
3. □Thepresentationlayerassessestheperceptualqualityofdata
generatedvisualizations,andcaptions.Unliketheotherlevelsthatcan
presentation.
beimplementedusingautomaticstrategies,forthislevel,weoptedfor
4. □Theinformativenesslayergaugestheintrinsicqualityofvisu- aqualitativeexperimentintheformofacomparativestudybasedona
alizationsintermsofinsightsandadherencetobestpracticesin customwebapp,asdetailedlaterinSection6.1.
visualizationliteracy. □LLMeffort:Finally,thislevelgaugedthecomplexityofthestrategy
employed to achieve the desired outcomes, ranging from zero-shot
5. □Lastly,theLLMlayerevaluatesthecomputationalandstrategic prompting(fortheteacher)tofine-tuning(forthestudent).
costsofgeneratingspecificvisualizations.
Notethatalltheabovelevelscanbeevaluatedquantitatively,except
Furthermore,asdetailedbyPodoetal.[35],theEvaLLMstackintro- forSignificance,whichweassessedwithaqualitativeevaluationby
ducestwelvedifferentlevelswithineachlayer,eachevaluatingdistinct humanjudges.
properties.
SinceEvaLLMisaframework,itmustbespecializedforspecific 6.1 Quantitativeevaluation
experiments.ForthepurposeofevaluatingV-RECSwechosetofocus Forthequantitativeevaluation,weused1800samplesdrawnfromthe
onlyonsevenofthetwelvelevelsproposedinEvaLLM.Additionally, originalncNetdataset[28],notusedinthetrainingphase,augmented
weintroducedanewlevel,”Captionquality". Weimplementedthe
sevenselectedlevelsasfollows: 11https://spacy.io/Provide me a dataset Students.csv
Stuid
 Lname
 Age
 Gender
 Major

Type: numeric Type: categorical Type: numeric Type: categorical Type: numeric
Ask me something
Show all ages and corresponding number of students in a scatter chart
(Explain) The 'age' column is chosen for both axes because the user wants to see the distribution of
students across different ages. The 'count' aggregate function is used on the y-axis to count the number of
students for each age. The data is then grouped by 'age' to show the count of students for each age. Other
columns like'stuid', 'lname', 'fname','sex','major', 'advisor', and 'city_code' are not selected as they do not
contribute to the user's request.

(Describe) The visualization is a point chart where the x-axis represents the 'age' and the y-axis represents
the count of each 'age'. The data is grouped by 'age'.
(Suggest) Other instructions to generate other data visualizations, based on the generated one, could
include 
Which age group has the highest number of students 
Are there any noticeable trends in the number of students across different age groups?
Fig.5:ExampleofV-RECSresponsegivenadatasetandauser’squery
withthegeneratedvisualizationnarratives,asdescribedinSection5. differentparticipants,therebymitigatingindividualbiases. Tomini-
ThisexperimentaimedtopromptbothGPT-4andV-RECSmodels mizetheimpactofanyoutliersorinconsistenciesintheevaluations,we
withuserqueriesanddatasets,subsequentlyevaluatingtheresponses enforcedaminimumthresholdofthreeuniqueevaluationspersample.
generatedbyeachmodel. Theseresponsesencompassedtherecom- Figure6,intheSignificancelayer,showsaslightadvantageofV-
mendedvisualizationsVandtheaccompanyingnarrative,allowingfor RECS.ThisisbettershowninFigure7,whereresultsaredistributed
acomprehensiveperformanceassessment.Indetail,theexperimental accordingtothecomplexityofthequery, andcategorizedintofour
setupinvolvedpromptingthemodelswithauserquery,dataset,and levelsasintheoriginaldataset. Asexpected,thequalityofanswers
taskdescription.Conversely,thepromptforGPT-4wasmoreelaborate, worsenswiththecomplexityofnaturallanguagequeriesforbothsys-
includingthetask,zero-shotCoTformulation,asetoftasksguiding tems,withV-RECSperformingslightlybetterexceptfortheextrahard
itsresponsegeneration,andthedesiredresponseformat12.Asshown sentences.
inFigure6,thetwomodelsexhibitedcomparableperformanceacross Uponasubsequentmodels’evaluationbytheauthors,wenoticed
theselevels,withGPT-4showcasingaslightadvantagefrom+3%to thattheexplanationsandcaptionsproducedbythetwosystemsare
+4%accuracyinwhatiscomparableperformancesbetweenthetwo slightlydifferentbutequallysatisfactory,althoughV-RECSprovides
models. However,anotablediscrepancyemergedinassessingaxes amorecomprehensiveexplanationofthereasoningsteps.Thisresult
quality,whereGPT-4appearedtosignificantlyunderperformcompared mitigatessomeoftheconcernsreportedintheuserstudybyKimet
toV-RECS(75%against93%accuracy).Specifically,GPT-4frequently al.[18]intermsoflackofdepthandcriticalthinkingcapabilitieswhile
exhibited errors such as axis inversions, a phenomenon previously explainingavisualizationdesign. Contrarily,GPT-4oftensurpasses
discussedinliterature[35]. Despitethis,bothmodelsdemonstrated V-RECSinsuggestingadditionalquestions,presentinggreaterdiversity
comparableaccuracyacrossthefirstsixlevels,indicatingcomparable andserendipity.Webelievethisdifferencecanbereducedinourfuture
efficacyingeneratingmeaningfulvisualizationsandnarratives.Further- researchbyimprovingthefine-tuningpromptstrategyfortheT3task.
more,itisessentialtoconsidertheefficiencyaspectofourevaluation. WealsonotedacoupleofhallucinationsinGPT-4,whileintwocases
Efficiency,depictedforalldimensionsinFigure6(right),entailscom- V-RECSwasabletoproduceagoodnarrativebut,duetosomeproblem
paringtheaccuracyachievedbyeachmodelateachlevel,whilealso inhandlingdatavalues,couldnotgenerateavisualization.
consideringthemodelcomplexity.Notably,ourfindingssuggestthat
resultscomparabletoGPT-4canbeachievedwithamuchsmallerand 7 DISCUSSIONANDFUTUREWORK
moremanageablemodel,whichisarelevantadvantageinpractical
ThispaperhasproposedthefirstLLM-basedvisualrecommender,V-
deploymentscenarios.
RECS,capableofprovidingexplanations,captioning,andsuggestions
forfurtheranalyses,andhasdemonstrateditscapabilities. Remark-
6.2 Qualitativeevaluation
ably,V-RECSisbasedonfine-tuninga”small",open-sourcemodel,
FortheSignificancelayer,wehavedesignedaqualitativeexperimentin- Llama-2-7B,whileachievingcomparableperformancetoamuchlarger,
volving50randomlyselectedsamples(notusedfortraining)evaluated proprietarymodel,GPT-4.
by15humanannotators.Importantly,wemaintainedtheoriginalquery Atthesametime,inthissection,welistsomelimitationsthatV-
difficultydistribution(ascategorizedinthesourcedataset),ensuring RECSpresentstoencouragefurtherresearchonthetopicandoppor-
thatourevaluationwasrepresentativeofreal-worldscenarios.Tofacil- tunitiesthattheavailabilityofV-RECSenablesforvisualizationand
itatetheevaluationprocess,wedevelopedawebapplicationdesigned visualanalyticsresearchersandpractitioners.
specificallyforconductingacomparativestudy13. Thisapplication
allowedustopresenteachvolunteerwithpairsofresponsesfromthe 7.1 Limitationsandchallenges
twomodelsforthesamequeryanddataset.Theidentityofthemodels’
HerewereportlimitationsconcerningV-RECSandthemethodologyit
responseswashiddenfromparticipants,ensuringunbiasedevaluations.
isbuilton.Morespecificallywereporton:
EachparticipantevaluatedtheresponsesusingaLikertscale,ranging
VisualizationLiteracyCoverage: Thefine-tuningofV-RECSand
from1(CompletelyMeaningless)to5(CompletelyMeaningful).This
quantitativeexperimentationareatthismomentbasedontheNCnet
scaleprovidedastructuredframeworkforcapturingthenuanceddif-
dataset,aderivationfromthewell-knownandstate-of-the-artNVBench
ferencesinthequalityandrelevanceoftheresponses. Firstly,each
dataset.Thechoicewasmadeasthisistheonlydatasetcapableofsup-
participantwasassignedasubsetof20samplesfromthetotalpoolof
portingthegenerationandevaluationofthevisualization,explanation
50.Thisensuredthateachsamplereceivedmultipleevaluationsfrom
(E),Captioning(C),andSuggestionsforfurtherdataexploration(S).
12i.e.,weuseatemplateformatsimilartothoseusedforthethreeteachers, Atthesametime,additionaldatasetsaremadeavailable,bothinterms
seeSection5.2 ofchartcollectionmethodsdiscussedbyChenandLiuintheirrecent
13https://vrecseval.streamlit.app/ state-of-the-artreportoncorporaforautomatingchartgeneration[2]EvaLLM Levels VRECS GPT-4 Accuracy Efficiency
Zero-shot CoT
LLM effort
Fine tuning based on CoT
67%
Significance 68% GPT
V-RECS
Caption 95%
quality 97% Comparable
Axes 75% Almost
quality 93% Worse
Mark 98%
correctness 95%
Data 98%
mapping 92%
Syntaxt 99%
correctness 96%
Fig.6:ResultsoftheEvaLLMevaluationexperiment
7.2 Opportunities
VRECS GPT-4
5 Alongwithlimitations,wealsoreportopportunitiesthatV-RECSen-
ablesforthevisualizationcommunity:
SupporttodemocratizationofpowerfulLLMsThemethodology
4 followedforthecreationofV-RECScanbereappliedtofine-tuneanew
modelfordifferenttasksinthevisualizationfield,suchasvisualization
modificationsorgenerationofabitmapversionofavisualization,ex-
3 ploitingalargefoundationmodelastheTeachermodel(e.g.,ChatGPT
fortext-to-text,orDALL-Efortext-to-image)atarelativelylow-cost.
Inthisway, thecapabilitiesoftheselargermodelscanbeinherited
2 (evaluatingthedegreeofinheritancecorrectly)andused,analyzed,or
modifiedbyresearchersandpractitioners,fosteringtheirspreadand
bettercomprehension.
1
SupporttovisualizationandvisualanalyticspipelinesV-RECS,the
resultingmodelfromtheapplicationoftheproposedmethodology,can
beexploitedforusageinvisualizationpipelinesandvisualanalytics
0
Easy Medium Hard Extra Hard pipelinesasabuildingblockoftheproposedsystem: lookingatthe
formeropportunity,tasksdescribedbySchlesingeretal.[41]suchas
ChartrecommendationandRapidprototypingarealreadysupported
Fig.7:Resultsofthequalitativeevaluationbycomplexityofthenatural byV-RECSasis,whileotherssuchasthegenerationofMoodboards
languagequery representaneasyextension. Interestingly,byconstructionV-RECS
canalsosupporttaskslikevisualizingtheeffectoftrainingdataon
theoutcomesofthemodelprovidingoverallmorecontrollabilityin
thevisualizationrecommendationprocess. Focusingonthelatter,a
veryrecentworkbyZhaoetal.[58],LEVA,systematizedtheusageof
ortherecentlyavailableVisTextdatasetforbenchmarkingcaptioning LLMforVisualAnalyticsthroughthedefinitionofthreemainstages
quality [48]. Although, as discussed in Section 5.2, these datasets ofintervention:onboarding,exploration,andsummarization. Inthis
arenotreadilyusableforourpurposes,weforeseetheneedtocreate context, V-RECSfitsthesummarizationstageforgeneratingvisual
anintegratedresourcecapableofevaluatingallthefeaturesproduced summarizationandexplanations,eventuallyproposingdirectionsto
byV-RECSinacombinedway,possiblyincludingalsodatasetslike proceedwiththeanalysisthroughitssuggestions(S).Itcouldevenbe
VIS30k[3]andOldOnlineVis[57]. adaptedtotheothertwostagesthroughtheapplicationofthemethodol-
User Evaluation: In its current form, the qualitative evaluation of ogyforgeneratingapathofanalysis(checkthepreviousopportunity)
V-RECShasbeenconductedthroughacomparativeexperimentwith15 byresearchersorpractitioners.
expertusers.Weforeseetheneedtoexpandthisstudytoabroaderset SupportfortrainingactivitiesStillreferringtotheworkbyZhaoet
ofparticipantstoexplorethesubtledifferencesthatcouldexistinthe al.[58],weforeseetheopportunitytoexploitV-RECSfortrainingactiv-
presentationandsignificancelayersoftheEvaLLMevaluationstack ities(referredtoastheonboardingstageforVisualAnalyticssystems)
andbetterevaluatethedifferenceinourmethodbetweentheproprietary onhowtodevelopavisualizationforaspecificanalysistask,through
GTPmodelandtheopen-sourcefine-tunedmodel.Giventherichness thesupportofexplanations,oraworkflow,exploitingachaininwhich
ofthisactivity,weforeseeitasadedicatedefforttobeconductedin ateachrecommendationstageanewsuggestionisselectedandgivenas
futurework. inputforthenextrecommendationstep.Thisactivitywouldbeinline
FormofExplanation:Initscurrentform,theexplanation(E)provided withtherecenttrendofusingLLMforeducationliketheexperimentby
toillustratethegenerationprocessisreportedonlyintextualform.As Chenetal.[6]testingChatGPT-3.5and4tocompleteassignmentsof
muchasitisthestandardforLLMs,andV-RECSalignswithit,wesee avisualizationcourse(CS171)andthepreliminaryworkproposedby
asalimitationthelackofvisualsupporttothisexplanationorlackof Joshietal.[17]onleveragingChatGPT-3.5andBard2.0forteachinga
coordinationbetweensub-partsoftheexplanationandvisualelements visualizationtechnique.
ofthegeneratedchart.Inthissense,theexplorationofvisualmethods
tosupporttheexplanation,astheonesreportedbyLaRosaetal.[21]or
recentlybySpinneretal.[43]forLLMs,shouldbetheareaforfurther
enhancementofthisapproach.
e
vitatilauQ
e
vitatitnauQ
noitaulave
noitaulaveACKNOWLEDGMENTS [19] H.-K.Ko,H.Jeon,G.Park,D.H.Kim,N.W.Kim,J.Kim,andJ.Seo.
Naturallanguagedatasetgenerationframeworkforvisualizationspowered
The authors wish to thank the participants who contributed to the
bylargelanguagemodels.arXivpreprintarXiv:2309.10245,2023.4,5,6
manualassessmentofV-RECS.Thisworkwasinpartsupportedbya
[20] T.Kojima,S.S.Gu,M.Reid,Y.Matsuo,andY.Iwasawa.Largelanguage
grantfromLazioRegion,FESRLazio2021-2027,project@HOME modelsarezero-shotreasoners.Advancesinneuralinformationprocessing
(#F89J23001050007). Thisresearchhasalsobeensupportedbythe systems,35:22199–22213,2022.3,5,6
AWSCloudCreditforResearchProgram,andtheOpenAI’sResearcher [21] B.LaRosa,G.Blasilli,R.Bourqui,D.Auber,G.Santucci,R.Capobianco,
AccessProgram. E.Bertini,R.Giot,andM.Angelini. Stateoftheartofvisualanalytics
forexplainabledeeplearning.ComputerGraphicsForum,42(1):319–355,
2023.doi:10.1111/cgf.147339
REFERENCES
[22] H.Li,J.Su,Y.Chen,Q.Li,andZ.-X.ZHANG.Sheetcopilot:Bringing
[1] V.S.Bursztyn,D.Demeter,D.Downey,andL.Birnbaum. Learningto softwareproductivitytothenextlevelthroughlargelanguagemodels.
performcomplextasksthroughcompositionalfine-tuningoflanguage AdvancesinNeuralInformationProcessingSystems,36,2024.3,4
models.arXivpreprintarXiv:2210.12607,2022.5 [23] T.Lin,Y.Wang,X.Liu,andX.Qiu.Asurveyoftransformers,2021.2
[2] C.ChenandZ.Liu.Thestateoftheartincreatingvisualizationcorpora [24] X. Lin, W. Wang, Y. Li, S. Yang, F. Feng, Y. Wei, and T.-S. Chua.
forautomatedchartanalysis.ComputerGraphicsForum,42(3):449–470, Data-efficientfine-tuningforllm-basedrecommendation.arXivpreprint
2023.doi:10.1111/cgf.148558 arXiv:2401.17197,2024.3
[3] J.Chen,M.Ling,R.Li,P.Isenberg,T.Isenberg,M.Sedlmair,T.Möller, [25] P.Lu,S.Mishra,T.Xia,L.Qiu,K.-W.Chang,S.-C.Zhu,O.Tafjord,
R.S.Laramee, H.-W.Shen, K.Wünsche, andQ.Wang. Vis30k: A P.Clark,andA.Kalyan. Learntoexplain: Multimodalreasoningvia
collectionoffiguresandtablesfromieeevisualizationconferencepub- thoughtchainsforsciencequestionanswering.AdvancesinNeuralInfor-
lications. IEEETransactionsonVisualizationandComputerGraphics, mationProcessingSystems,35:2507–2521,2022.5
27(9):3826–3833,2021.doi:10.1109/TVCG.2021.30549166,9 [26] Y.Luo,X.Qin,N.Tang,andG.Li.Deepeye:Towardsautomaticdatavisu-
[4] Y.Chen,R.Li,A.Mac,T.Xie,T.Yu,andE.Wu.Nl2interface:Interactive alization.In2018IEEE34thinternationalconferenceondataengineering
visualizationinterfacegenerationfromnaturallanguagequeries. arXiv (ICDE),pp.101–112.IEEE,2018.2
preprintarXiv:2209.08834,2022.3,4 [27] Y.Luo,N.Tang,G.Li,C.Chai,W.Li,andX.Qin.Synthesizingnatural
[5] Y.ChenandE.Wu.Pi2:End-to-endinteractivevisualizationinterfacegen- languagetovisualization(nl2vis)benchmarksfromnl2sqlbenchmarks.
erationfromqueries.InProceedingsofthe2022InternationalConference InProceedingsofthe2021InternationalConferenceonManagementof
onManagementofData,pp.1711–1725,2022.3 Data,SIGMODConference2021,June20–25,2021,VirtualEvent,China.
[6] Z.Chen,C.Zhang,Q.Wang,J.Troidl,S.Warchol,J.Beyer,N.Gehlen- ACM,2021.6
borg,andH.Pfister. Beyondgeneratingcode:Evaluatinggptonadata [28] Y.Luo,N.Tang,G.Li,J.Tang,C.Chai,andX.Qin. Naturallanguage
visualizationcourse.In2023IEEEVISWorkshoponVisualizationEdu- tovisualizationbyneuralmachinetranslation. IEEETransactionson
cation,Literacy,andActivities(EduVis),pp.16–21,2023.doi:10.1109/ VisualizationandComputerGraphics,28(1):217–226,2021.3,4,5,6,7
EduVis60792.2023.000099 [29] J.Mackinlay. Automatingthedesignofgraphicalpresentationsofrela-
[7] Y. Cheng, C. Zhang, Z. Zhang, X. Meng, S. Hong, W. Li, Z. Wang, tionalinformation.AcmTransactionsOnGraphics(Tog),5(2):110–141,
Z.Wang,F.Yin,J.Zhao,etal. Exploringlargelanguagemodelbased 1986.2
intelligentagents: Definitions,methods,andprospects. arXivpreprint [30] J.Mackinlay,P.Hanrahan,andC.Stolte.Showme:Automaticpresenta-
arXiv:2401.03428,2024.2 tionforvisualanalysis.IEEEtransactionsonvisualizationandcomputer
[8] H.W.Chung,L.Hou,S.Longpre,B.Zoph,Y.Tay,W.Fedus,Y.Li, graphics,13(6):1137–1144,2007.2
X.Wang,M.Dehghani,S.Brahma,etal. Scalinginstruction-finetuned [31] P.MaddiganandT.Susnjak.Chat2vis:Generatingdatavisualisationsvia
languagemodels.arXivpreprintarXiv:2210.11416,2022.4 naturallanguageusingchatgpt,codexandgpt-3largelanguagemodels.
[9] T.Dettmers,A.Pagnoni,A.Holtzman,andL.Zettlemoyer. Qlora: Ef- arXivpreprintarXiv:2302.02094,2023.3
ficientfinetuningofquantizedllms. AdvancesinNeuralInformation [32] C.D.Manning, M.Surdeanu, J.Bauer, J.R.Finkel, S.Bethard, and
ProcessingSystems,36,2024.6 D.McClosky.Thestanfordcorenlpnaturallanguageprocessingtoolkit.In
[10] V.Dibia.Lida:Atoolforautomaticgenerationofgrammar-agnosticvisu- Proceedingsof52ndannualmeetingoftheassociationforcomputational
alizationsandinfographicsusinglargelanguagemodels.arXivpreprint linguistics:systemdemonstrations,pp.55–60,2014.2
arXiv:2303.02927,2023.3 [33] A. Narechania, A. Srinivasan, and J.Stasko. NL4DV: AToolkit for
[11] V.DibiaandÇ.Demiralp.Data2vis:Automaticgenerationofdatavisu- generatingAnalyticSpecificationsforDataVisualizationfromNatural
alizationsusingsequence-to-sequencerecurrentneuralnetworks.IEEE Languagequeries. IEEETransactionsonVisualizationandComputer
computergraphicsandapplications,39(5):33–46,2019.3 Graphics(TVCG),2020.doi:10.1109/TVCG.2020.30303783
[12] S.Fu,K.Xiong,X.Ge,S.Tang,W.Chen,andY.Wu.Quda:Naturallan- [34] OpenAI. Gpt-4: Ahighlyadvancedlanguagemodel,2024. Accessed:
guagequeriesforvisualdataanalytics.arXivpreprintarXiv:2005.03257, February5,2024.2
2020.6 [35] L.Podo,M.Ishmal,andM.Angelini. Vi(e)vallm!aconceptualstack
[13] T.Gao,M.Dontcheva,E.Adar,Z.Liu,andK.G.Karahalios.Datatone: forevaluatingandinterpretinggenerativeai-basedvisualizations.arXiv
Managingambiguityinnaturallanguageinterfacesfordatavisualization. preprintarXiv:2402.02167,2024.2,7,8
In Proceedings of the 28th annual acm symposium on user interface [36] L.Podo,B.Prenkaj,andP.Velardi. Agnosticvisualrecommendation
software&technology,pp.489–500,2015.2 systems:Openchallengesandfuturedirections. IEEETransactionson
[14] M.U.Hadi,q.a.tashi,R.Qureshi,A.Shah,a.muneer,M.Irfan,A.Zafar, VisualizationandComputerGraphics,2024.2,3
M.B.Shaikh,N.Akhtar,J.Wu,andS.Mirjalili.Largelanguagemodels: [37] L.ReynoldsandK.McDonell.Promptprogrammingforlargelanguage
Acomprehensivesurveyofitsapplications,challenges,limitations,and models:Beyondthefew-shotparadigm.InExtendedAbstractsofthe2021
futureprospects.Nov.2023.doi:10.36227/techrxiv.23589741.v43 CHIConferenceonHumanFactorsinComputingSystems,pp.1–7,2021.
[15] K.Hu,M.A.Bakker,S.Li,T.Kraska,andC.Hidalgo.Vizml:Amachine 3
learningapproachtovisualizationrecommendation.InProceedingsofthe [38] S.F.Roth,J.Kolojejchick,J.Mattis,andJ.Goldstein.Interactivegraphic
2019CHIConferenceonHumanFactorsinComputingSystems,pp.1–12, designusingautomaticpresentationknowledge. InProceedingsofthe
2019.2,3 SIGCHIconferenceonHumanfactorsincomputingsystems,pp.112–117,
[16] S.Jha,S.K.Jha,P.Lincoln,N.D.Bastian,A.Velasquez,andS.Neema. 1994.2
Dehallucinating large language models using formal methods guided [39] P. Sahoo, A. K. Singh, S. Saha, V. Jain, S. Mondal, and A. Chadha.
iterativeprompting.In2023IEEEInternationalConferenceonAssured Asystematicsurveyofpromptengineeringinlargelanguagemodels:
Autonomy(ICAA),pp.149–152.IEEE,2023.2 Techniquesandapplications,2024.3
[17] A.Joshi, C.Srinivas, E.E.Firat, andR.S.Laramee. Evaluatingthe [40] J.R.Saura,D.Palacios-Marqués,andD.Ribeiro-Soriano. Digitalmar-
recommendationsofllmstoteachavisualiza-tiontechniqueusingbloom’s ketinginsmesviadata-drivenstrategies:Reviewingthecurrentstateof
taxonomy.9 research.JournalofSmallBusinessManagement,61(3):1278–1313,2023.
[18] N.W.Kim,G.Myers,andB.Bach.Howgoodischatgptingivingadvice 2
onyourvisualizationdesign?,2024.8 [41] V.Schetinger,S.D.Bartolomeo,M.El-Assady,A.McNutt,M.Miller,J.P.A.Passos,andJ.L.Adams. DoomorDeliciousness: Challenges
andOpportunitiesforVisualizationintheAgeofGenerativeModels.
ComputerGraphicsForum,2023.doi:10.1111/cgf.148413,9
[42] V.Setlur,S.E.Battersby,M.Tory,R.Gossweiler,andA.X.Chang.Eviza:
Anaturallanguageinterfaceforvisualanalysis. InProceedingsofthe
29thannualsymposiumonuserinterfacesoftwareandtechnology,pp.
365–377,2016.2,3
[43] T. Spinner, R. Kehlbeck, R. Sevastjanova, T. Stähle, D. A. Keim,
O.Deussen,andM.El-Assady. generaitor: Tree-in-the-looptextgen-
erationforlanguagemodelexplainabilityandadaptation. ACMTrans.
Interact.Intell.Syst.,mar2024.JustAccepted.doi:10.1145/36520289
[44] A.Srinivasan,N.Nyapathy,B.Lee,S.M.Drucker,andJ.Stasko. Col-
lectingandcharacterizingnaturallanguageutterancesforspecifyingdata
visualizations. InProceedingsofthe2021CHIConferenceonHuman
FactorsinComputingSystems,pp.1–10,2021.6
[45] A.SrinivasanandV.Setlur.Bolt:Anaturallanguageinterfacefordash-
boardauthoring.InEuroVis,2023.3
[46] N.SultanumandA.Srinivasan.Datatales:Investigatingtheuseoflarge
languagemodelsforauthoringdata-drivenarticles.In2023IEEEVisual-
izationandVisualAnalytics(VIS),pp.231–235.IEEE,2023.4
[47] J.Sun,Y.Tian,W.Zhou,N.Xu,Q.Hu,R.Gupta,J.Wieting,N.Peng,
andX.Ma. Evaluatinglargelanguagemodelsoncontrolledgeneration
tasks.InH.Bouamor,J.Pino,andK.Bali,eds.,Proceedingsofthe2023
ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pp.
3155–3168.AssociationforComputationalLinguistics,Singapore,Dec.
2023.2
[48] B.J.Tang,A.Boggust,andA.Satyanarayan. VisText: ABenchmark
forSemanticallyRichChartCaptioning. InTheAnnualMeetingofthe
AssociationforComputationalLinguistics(ACL),2023.6,9
[49] A.-R. Tawil, M. Mohamed, X. Schmoor, K. Vlachos, and D. Haidar.
Trendsandchallengestowardsaneffectivedata-drivendecisionmaking
inuksmes:Casestudiesandlessonslearntfromtheanalysisof85smes.
arXivpreprintarXiv:2305.15454,2023.2
[50] Y.Tian,W.Cui,D.Deng,X.Yi,Y.Yang,H.Zhang,andY.Wu.Chartgpt:
Leveragingllmstogeneratechartsfromabstractnaturallanguage.arXiv
preprintarXiv:2311.01920,2023.4
[51] H.Touvron,T.Lavril,G.Izacard,X.Martinet,M.-A.Lachaux,T.Lacroix,
B.Rozière,N.Goyal,E.Hambro,F.Azhar,etal. Llama: Openand
efficientfoundationlanguagemodels.arXivpreprintarXiv:2302.13971,
2023.2,6
[52] A.Vaswani,N.Shazeer,N.Parmar,J.Uszkoreit,L.Jones,A.N.Gomez,
Ł.Kaiser,andI.Polosukhin.Attentionisallyouneed.Advancesinneural
informationprocessingsystems,30,2017.6
[53] L.Wang,S.Zhang,Y.Wang,E.-P.Lim,andY.Wang. Llm4vis: Ex-
plainablevisualizationrecommendationusingchatgpt. arXivpreprint
arXiv:2310.07652,2023.2,3
[54] Y.Wang. Decipheringtheenigma:Adeepdiveintounderstandingand
interpretingllmoutputs.AuthoreaPreprints,2023.2
[55] J.Wei,X.Wang,D.Schuurmans,M.Bosma,F.Xia,E.Chi,Q.V.Le,
D.Zhou,etal. Chain-of-thoughtpromptingelicitsreasoninginlarge
languagemodels. AdvancesinNeuralInformationProcessingSystems,
35:24824–24837,2022.2,5
[56] L.Zha,J.Zhou,L.Li,R.Wang,Q.Huang,S.Yang,J.Yuan,C.Su,X.Li,
A.Su,etal. Tablegpt: Towardsunifyingtables,naturelanguageand
commandsintoonegpt.arXivpreprintarXiv:2307.08674,2023.4
[57] Y. Zhang, R. Jiang, L. Xie, Y. Zhao, C. Liu, T. Ding, S. Chen, and
X.Yuan. Oldvisonline: Curatingadatasetofhistoricalvisualizations.
IEEETransactionsonVisualizationandComputerGraphics,30(1):551–
561,2024.doi:10.1109/TVCG.2023.33269089
[58] Y.Zhao,Y.Zhang,Y.Zhang,X.Zhao,J.Wang,Z.Shao,C.Turkay,and
S.Chen.Leva:Usinglargelanguagemodelstoenhancevisualanalytics.
IEEETransactionsonVisualizationandComputerGraphics,pp.1–17,
2024.doi:10.1109/TVCG.2024.33680609