1
Real-Time Hand Gesture Recognition:
Integrating Skeleton-Based Data Fusion and
Multi-Stream CNN
Oluwaleke Yusuf, Maki Habib, Mohamed Moustafa
Abstract—This study focuses on Hand Gesture Recognition Consequently, a sequence of hand poses must be interpreted
(HGR), which is vital for perceptual computing across various to understand the meaning of a gesture. This paper presents
real-world contexts. The primary challenge in the HGR domain
a new approach which combines data-level fusion techniques
lies in dealing with the individual variations inherent in human
with a specialized multi-stream CNN architecture, setting our
hand morphology. To tackle this challenge, we introduce an
innovative HGR framework that combines data-level fusion method apart in addressing the challenges of dynamic gesture
and an Ensemble Tuner Multi-stream CNN architecture. This recognition.
approach effectively encodes spatiotemporal gesture information Within the HGR domain, a variety of frameworks have
from the skeleton modality into RGB images, thereby mini-
been investigated, each focusing on different combinations
mizing noise while improving semantic gesture comprehension.
of network architectures and input modalities to overcome
Our framework operates in real-time, significantly reducing
hardware requirements and computational complexity while specific challenges with dynamic gesture recognition. Certain
maintaining competitive performance on benchmark datasets frameworks enhance their performance by merging various
suchasSHREC2017,DHG1428,FPHA,LMDHGandCNR.This input types, including RGB, skeleton data, optical flow, and
improvementinHGRdemonstratesrobustnessandpavestheway
depth maps, with different network architectures [1]–[8].
forpractical,real-timeapplicationsthatleverageresource-limited
These multi-stream networks consist of various sub-networks
devicesforhuman-machineinteractionandambientintelligence.
designed with distinct input channels operating concurrently,
with the features extracted from each sub-network merged to
IndexTerms—Data-LevelFusion,Multi-StreamConvolutional
produce the final output of the overall network. On the other Neural Network, Skeleton Modality, Ensemble Learning, Hand
Gesture Recognition. hand, multimodal networks process different input modalities
through either distinct inputs in a multi-stream network or
a combined input in a single-stream network. Also, sev-
I. INTRODUCTION
eral frameworks employ deep, data-driven neural network
Hand Gesture Recognition (HGR) plays a vital role in per- architectures, such as Recurrent Neural Networks (RNN) [1],
ceptual computing by enabling computers to capture and un- [2], [9], [10], 3D Convolutional Neural Networks (3DCNN)
derstandhumanhandgesturesusingmathematicalalgorithms. [3], [6], [7], Attention Networks [2], [11], [12], and Long-
This technology enables computational devices to facilitate Term Recurrent Convolutional Networks (LRCN) [13]–[15]
advanced applications such as human-machine interactions, to extract temporal information from dynamic hand gestures.
human behavior analysis, active and assisted living, virtual When developing real-world applications beyond the scope
and augmented reality, as well as ambient intelligence. How- ofacademicresearch,it’simportanttofocusoncreatingprac-
ever, recognizing hand gestures poses significant challenges tical HGR-based systems. The developed frameworks often
due to the intricate nature of the human hand, which can prioritize maximizing performance, which can lead to addi-
assume countless poses and the wide variations in size and tionalhardwaredemands,increasedcomputationalcomplexity
color across individuals. Furthermore, HGR applications fre- and specialized, often expensive, sensors for input modalities.
quentlyfunctioninchallengingsettingsmarkedbyocclusions, Thesefactors,particularlywithmultimodalnetworks,canlead
changing backgrounds, noisy data, and demands for real-time tohighercosts,reduceduser-friendliness,andlongerinference
processing. timesinreal-timeapplications.Additionally,theseframeworks
A successful HGR framework needs to navigate the com- require significant training data with data augmentation to
plexitiesintroducedbythehumanhandandtheapplicationset- attain acceptable levels of gesture recognition performance.
tingtosatisfybothdevelopersandend-users.Keyperformance Skeleton-based HGR frameworks address these challenges
indicators encompass ease of use, computational demand, by using data-level fusion to transform dynamic gesture data
hardware needs, response time, and accuracy. Hand gestures, intoRGBimages.Thismethod,whichredefinesgesturerecog-
inherently dynamic with poses and positions changing over nition as a standard image classification issue, has its own
time, introduce a temporal dimension to the recognition task. set of limitations. The implementation of data-level fusion
in [16] results in noisy and visually similar RGB images
Oluwaleke Yusuf and Maki Habib are with The American University in which complicates gesture recognition and classification dur-
Cairo(AUC),Egypt.(e-mail:oluwaleke.umar,maki@aucegypt.edu)
ing model training and inference with their vanilla CNN
Mohamed Moustafa was with The American University in Cairo (AUC),
Egypt.HeisnowwithAmazon,Seattle,USA.(e-mail:moustafa@ieee.org) architecture.Inaddition,theeffectivenessoftheselectedview
4202
nuJ
12
]VC.sc[
1v30051.6042:viXra2
orientationsisheavilydependentonhowtheskeletondatawas principles of our proposed framework. These works can be
initially captured. It is also worth noting that the framework categorizedintothreeapproaches:recognitionofhandgestures
has not been benchmarked against widely recognized HGR via skeleton data, data-level fusion of temporal information
datasets,raisingdoubtsaboutitsperformancerelativetoother at the data level, and architectures based on multi-stream
frameworks. networks.
This paper presents an effective data-level fusion and spe-
cialized CNN architecture for a robust, lightweight, skeleton-
A. Skeleton-based Hand Gesture Recognition
basedHGRframework.Thecontributionsofthispaperinclude
the following: Historically, HGR frameworks primarily relied on RGB
1) We enhance the conversion of 3D skeleton data to videosordepthmaps.However,withtheevolutionoftechnol-
spatiotemporal 2D representational images for effective ogy and the emergence of a deeper understanding of gesture
dynamic gesture recognition. This outcome is achieved complexities, recent frameworks have shifted towards using
using data-level fusion, incorporating denoising, and skeleton poses as an input modality. This modality, usually
sequence fitting. derived from raw color and depth (RGB-D) data, provides
2) We introduce a simplified, fully-trainable Ensemble a robust representation of hand positions and is effective
Tuner multi-stream CNN architecture for establishing at addressing issues like occlusions, variable backgrounds,
robust semantic connections between multiple represen- and differences in individual hand morphologies. However,
tations of the same input data during image classifica- skeleton-based methods have the drawback of needing offline
tion. preprocessing to extract skeleton data from RGB-D sources,
3) WeintroduceadynamicHGRframeworkaimedatmin- with a susceptibility to errors in this inferred data. Various
imizing hardware needs and computational complexity methods for processing skeleton data have been developed
for HGR tasks, achieving parity with state-of-the-art within HGR frameworks.
(SOTA) benchmarks across various datasets. Some significant contributions in this domain include: [22]
4) We showcase our framework’s real-world applicability utilized a Temporal Convolutional Network with a motion
bydevelopingaHGRapplicationthatsatisfiesreal-time summarization module for processing skeleton pose features.
constraints and is compatible with standard consumer [12] proposed a spatial-temporal attention network that relies
PC hardware. on attention mechanisms to model inter-joint spatiotemporal
dependencies, without requiring knowledge of joint posi-
Extensive experiments were conducted on five publicly
tions or connections. [4] tackled skeleton data challenges by
available datasets: 3D Hand Gesture Recognition Using a
streamlining input features and the network design, using a
Depth and Skeleton Dataset (SHREC2017) [17], Dynamic
network with Joint Collection Distances and global motion
Hand Gesture 14/28 Dataset (DHG1428) [18], First-Person
features at two scales. In [10], a Deep Convolutional LSTM
HandActionBenchmark(FHPA)[19],LeapMotionDynamic
(ConvLSTM) model was introduced, designed to inherently
Hand Gesture Benchmark (LMDHG) [20], and Consiglio
learndiscriminativespatiotemporalfeaturesfromskeletondata
Nazionale delle Ricerche Hand Gestures Dataset (CNR) [16].
by capturing gestures across different scales.
The outcomes indicate that our framework achieved perfor-
Unimodal skeleton-based HGR frameworks consistently
mancecloselyalignedwith,andinsomeinstancessurpassing,
yield the best results [2], [12], [22]. Said frameworks not
the state-of-the-art, with accuracies ranging from -4.10% to
only show better performance with respect to computational
+6.86%comparedtothehighestbenchmarks.Furthermore,the
efficiency metrics (like trainable parameters and FLOPS) but
lowlatencyoftheHGRapplicationsuccessfullydemonstrated
alsonetworkarchitecturalcomplexity[2],[4],[12],[23]–[25].
the viability of data-level fusion for practical, real-time HGR
Our proposed framework capitalizes on the robust encoding
applications.Wealsoconductedanexploratorystudyusingthe
capabilitiesoftheskeletonmodality,towardsachievingSOTA
SBU Kinect Interaction Dataset (SBUKID) [21] to investigate
performance. Concurrently, we devised strategies to bypass
potential extensions of our framework for Human Action
the necessity for specialized hardware and high computa-
Recognition (HAR). The findings provide strong motivation
tional demands often associated with other skeleton-based
for further development and experimentation.
approaches.Additionally,itisworthnotingthatskeleton-based
The rest of this paper is structured as follows: Section II
HGR applications inherently reduce ethical concerns related
reviewstherelevantHGRliteratureonskeletonmodality,data-
to surveillance and privacy breaches, given the limited user-
levelfusion,andmulti-streamarchitectures;SectionIIIdetails
identifiable information in skeleton data [26], [27].
the proposed HGR framework and associated methodologies;
Section IV reports experimental findings and analyses using
benchmark datasets; Section V describes the implementation B. Data-Level Fusion
of the HGR application; and Section VI concludes the paper
The challenges associated with recognizing dynamic hand
and outlines avenues for future research.
gesturesarisefromtheirtemporalattributes.Handgesturesare
composedofasequenceofhandposesovertime,necessitating
II. RELATEDWORK
an HGR framework capable of understanding the semantic
This section provides an overview of significant research in connectionsbetweenthesesuccessiveposesforprecisegesture
the HGR domain, crucial for understanding the foundational identification. This complexity increases in multi-stream and3
multimodal networks, which require precise pixel-level corre- effectiveness of this approach is evident as it provides the
spondence across various streams and modalities to recognize model with several ‘views.’ Such views enhance the model’s
thatthedifferentinputscorrespondtodifferentrepresentations abilitytodistinguishbetweengesturesthatmayappearsimilar
of the same gesture sequence. and reduces the likelihood of overfitting. Nevertheless, multi-
The effectiveness of the HGR framework is heavily influ- stream networks depend on the integration of features at two
enced by the selected method for integrating input modalities specific levels: the combination of features from different
and network streams. Deep Learning (DL) models typically convolutional layers (feature-level fusion), and the merging
implementfusionatthedata-,feature-,ordecision-level,each of classification probabilities from the network’s dense layers
with distinct approaches [28]. HGR frameworks commonly (decision-level fusion). These fusion techniques are crucial
utilize online decision-level and feature-level fusion, which for compiling and balancing the contributions from each sub-
affectsperformancedifferently.Suchframeworksalsodemand network, thereby improving the overall network’s functional-
complex network architectures, substantial training datasets ity.
(including data augmentation), and tailored loss functions to To identify and categorize continuous hand gestures, [6]
automatically align semantic and pixel-level details during employed a hierarchical architecture comprising two models
training. of3D-CNNs.Initially,adetectormodelspotsagesturewithin
Temporal Information Condensation is a preprocessing the video frames and subsequently triggers a deeper classifier
technique used offline to convert spatiotemporal data from model which further hones the detector’s findings to classify
gesture sequences into static images before model training. the gestures accurately. Diverging from approaches that blend
This method of data-level fusion simultaneously captures the multimodal data, [30] employed unique I3D sub-networks
spatiotemporal variation of dynamic gestures, helping to re- to process information from different modalities. These sub-
solve challenges related to spatiotemporal semantic coherence networksmaintainastablecorrelationthroughouttheirdeeper
withinmulti-streamnetworksduringtraining.Bytransforming levels,usingspatiotemporalsemanticalignmentlosstoensure
the dynamic HGR task into a static image classification featurecoherenceacrossthenetworks.[31]developedamulti-
problem,thisstrategyenablestheuseoftransferlearningwith channelCNNconsistingofseparatefeatureextractionmodules
pre-trained models from well-known CNN architectures [16], for high-resolution and low-resolution channels, coupled with
[28]. a dedicated residual branch per channel. Skeleton sequences
In [28], Motion Fused Frames were introduced, where areinitiallyresizedandthenbrokendownintounidimensional
framesderivedfromopticalflowserveassupplementarychan- jointsequences,whicharetheninputtedintothecorresponding
nelstostaticRGBimages.Saidopticalflowframes,calculated channel of the model.
from frames preceding the selected static image, are then Frameworks that utilize data-level fusion tend to adopt
processed using a Temporal Segment Network that employs either a straightforward single-stream CNN architecture [16]
a pretrained Inception CNN architecture. [29] describes a or a two-stream CNN architecture with features concatenated
techniquecalledstarRGB,whichtransformsdynamicgestures atthefeaturelevel[28].Conversely,[29]implementedanovel
fromvideosintosingleRGBimagesbysummingtheintensity soft-attention mechanism for feature-level fusion, leading to
differencesofconsecutivevideoframes.Theseimagesarethen a slight boost in classification accuracy over other fusion
classifiedbyadynamicgestureclassifierthatusesanensemble methods. This indicates a knowledge gap concerning network
of ResNet CNNs fused via a soft-attention mechanism. architectures specially engineered for precessing spatiotempo-
In [16], the authors exclusively used skeleton data to ral images generated via data-level fusion. Furthermore, the
transform gesture information into RGB images, showcasing fusiontechniquesemployedatbothfeatureanddecisionlevels
variations in hand joint positions during gestures on a plane. inmulti-streamframeworkscanberefinedtoenhancesemantic
Temporal details are depicted through the color intensity of alignment and minimize computational overhead.
theseprojectedpoints.Saidimagesaresubsequentlyclassified
using a pretrained ResNet-50 CNN, demonstrating effective III. HANDGESTURERECOGNITIONFRAMEWORK
performance on the balanced CNR dataset also developed by
This section elaborates on the essential elements of the
the authors. While promising, this approach could be further
proposed skeleton-based hand gesture recognition framework
enhanced by refining the temporal information condensation
shown in Figure 1. Initially, the technique of temporal infor-
method and network architecture for application to more
mationcondensationforgeneratingstatic,spatiotemporalRGB
challenging datasets
imagesisexplored.Subsequently,thenetworkarchitecturethat
takesadvantageofthisdata-levelfusionstrategyisintroduced.
C. Multi-Stream Network Architectures Finally, the data-level fusion and network architecture are
mergedtopresentanHGRframeworkadaptabletoanysource
In developing advanced HGR frame, numerous high-
ofhandskeletondata—real-timeinferenceorcollateddatasets.
performing frameworks adopt multi-stream network architec-
tures that integrate decoupled spatiotemporal streams, oper-
A. Data-Level Fusion
ate on multiple temporal scales, and handle various input
modalities. The goal of this strategy is to capture a wide Data-level fusion serves as an initial processing stage, con-
rangeof contextualdetailsfrom gesture sequencesthroughout ductedeitherofflineoronline,wheredatafromthespatialand
the model’s training phase [6], [12], [22], [30]–[32]. The temporal channels of a dynamic gesture sequence are merged4
Fig. 1. Diagrammatic Overview of the Proposed HGR Framework [33], Highlighting Key Components for Recognizing and Classifying Dynamic Hand
Gestures.
into a static 2D spatiotemporal image, preserving a square • It minimizes minor variances in motion paths and se-
aspect ratio. This transformation involves condensing the quence durations that result from the unique way each
temporal information present in the original sequence while person performs a gesture.
retaining pertinent semantic information about the gesture.
In combination, the resampling adjustment underscores the
Similartotheapproachin[33],theproblemoftransforminga
commonalitiesbetweengesturesequencesg ϵC inS,thereby
i h
dynamic gesture sequence into a static spatiotemporal image
improving the condensation of temporal information, con-
can be defined as follows:
sequently improving the model’s accuracy when classifying
Let g i denote a dynamic gesture, and let S = C hN h=1 gestures.Eachresampledgestureg i isthensplitintoitsspatial
represent the set of gesture sequences across N classes. The and temporal components as follows:
temporal variation of g is defined in Equation 1 as follows:
i • Spatial: This component encodes changes in hand pose
across each frame Gτ as a 3D model of the hand’s
i
G ={Gτ}Ti (1) skeleton. Each finger is shown in a distinct CSS color,
i i τ=1
selected to provide strong contrast with the other colors
in the and the dark background.
In the context of a temporal window sized T , τϵ[1,T ]
i i • Temporal: In this channel, hand movement is encoded
specifies a precise moment τ, and Gτ indicates the frame of
i through 3D visualizations of ”temporal trails” of the five
g i atthatmoment.Thechallengeofclassifyingdynamichand fingertips, extending from the start of the gesture G1 to
i
gestures involves assigning the correct class C h for sequence itsendGT.Thesetrailscompriseasequenceofmarkers,
i
g .
i with distinct CSS colors correlating to their respective
Gesture sequences in the set S present varying lengths fingers. The transparency of these markers, indicated by
of time windows based on the gesture category and the thealphachannel,variesovertime,makingmarkersfrom
performer’s execution. Each sequence g i in S is resampled earlier in the sequence (τ ≈ 1) more transparent, and
to a uniform temporal window T i, set to be longer than any those closer to the end (τ ≈ T) more opaque, thus
existing window T i in S. This adjustment has the following capturing the temporal dynamics.
effects on each gesture sequence g :
i Temporalinformationcondensationcreates,foreachresam-
• It smoothens out inaccuracies in individual frames Gτ i pled gesture g i, a 3D spatiotemporal image, which merges
arising during pose estimation. the hand pose at last frame GT with the temporal path it has
i5
Fig. 2. Workflow of Data-Level Fusion: Illustrating the Transformation of 3D Skeleton Data into 2D Spatiotemporal RGB Representations for Image
Classification.
Fig. 3. Illustration of View Orientations Used for Spatiotemporal Gesture Representations. Left-to-Right: Top-Down, Front-To, Front-Away, Side-Right,
Side-Left,andAxonometric.
followedGτT−1,asillustratedinFigure2.The3Dspatiotem- position are essential during visualization to ensure that the
iτ=1
poral image can be viewed from any arbitrary perspective spatiotemporalrepresentationisfullyenclosedwithinthestatic
(view orientation). This paper considers six (6) specific view image without being cropped off or any truncation. These
orientations (VOs)—axonometric, front-away, side-left, top- settings cannot be manually set or fixed, as they fluctuate for
down, front-to, and side-right. These VOs are defined by the everygestureg ,andsetofsequencesS.Instead,thesequence
i
elevation and azimuth angles set for the virtual camera during fitting for each gesture g is estimated as follows:
i
the visualization process, detailed in Figure 3. The choice of
angles differs across datasets, influenced by the methods used g =g −mean(g )−(L/2) (2)
i i i
to collect and derive skeleton data from the actual gestures by
participants.
The outcome of the data-level fusion for any given gesture P =(L/2) (3)
g is a single image of the 3D spatiotemporal representa-
i
tion from one of the chosen view orientations VO where
j
j ∈[1,6]. Adjustments to the virtual camera’s zoom level and Z =max(g )−min(g )+γ (4)
i i i6
Equation 2 represents the adjustment made to each gesture
g to fit it within the static image. By subtracting the mean
i
of the gesture from itself and subtracting half of the length
L of the static image, the gesture is shifted to the center of
the image. The adjustment made in Equation 2 ensures that
the virtual camera can be fixed to position P in 3D space,
which coincides with the center of the static image for all
g ϵS. As outlined in Equation 3, the value of P is determined
i
Fig. 4. Comparison of Classifiers: The Original ImageNet Classifier (left)
by the length L of the static image. Equation 4 calculates
versusOurTailoredHGRClassifier(right).
the zoom level Z of the virtual camera for each gesture g .
i i
The zoom level is estimated from the bounds of the gesture’s
representation, with an optional padding value γ for tweaking
the estimated zoom level for all g ϵS. With the full extent of
i
the gesture taken into account, the zoom level ensures that
the entire spatiotemporal representation remains visible in the
image without being cropped off.
In conclusion, the data-level fusion transforms the dynamic
hand gesture classification task into a static image classifica-
tion problem task. Thus, by employing a mapping function
Φ, the 3D skeleton information of a dynamic gesture g from
i
any view orientation VO is transformed into a single 2D
j
spatiotemporal image I = Φ(G ). This image becomes the
ij i
inputfortheclassificationtask,wherethegoalistodetermine
the correct class C for I . This approach simplifies the
h ij
classification problem by leveraging established techniques
in image classification, enabling the utilization of existing Fig.5. IllustrationShowingtheProcessingofStaticSpatiotemporalImages
research and algorithms in this domain. bytheEnsembleTunerMulti-StreamCNNArchitecture.
B. Ensemble Tuner Multi-Stream CNN Architecture to process spatiotemporal images generated from any view
orientation VO . For each gesture g ∈ S, data-level fusion
WiththesetS,consistingofdynamichandgesturesg ,con- j i
i producesasetofI 6 spatiotemporalimages,corresponding
verted into static spatiotemporal images I ij, traditional CNN ijj=1
to j different view orientations.
architectures become suitable for classifying these gestures.
Each image I is sequentially input into the multi-stream
The HGR framework also take advantage of transfer learning ij
encoder for feature extraction. The derived feature maps are
by using models from deep learning CNN architectures that
then processed by the multi-stream classifier, as illustrated
were originally developed for image classification. Transfer
in Figure 4 (right), producing a series of class probabilities
learning enables swift convergence during model training and
expedites prototype development within the framework. To {CP}h ijN (h=1). The multi-stream encoder and classifier are
choosethebasearchitectureforournetwork,weevaluatedsev- shared across all images, effectively lowering the compu-
eral CNN models pretrained on the ImageNet dataset, which tational demands of the network architecture. However, a
have demonstrated state-of-the-art (SOTA) performance. limitation of this approach is that the ordering of images
During transfer learning, the pre-trained model’s fully- (view orientations) is not random, resulting in an expanded
connected (FC) layers are customized to fit the new task at searchspacefortheidealsequence.Themulti-streamencoder
hand. We substituted the original FC layers with a tailored and classifier sub-network jointly learn the best weights for
classifier, maintaining the primary convolutional layers (along merging the view orientations VO j guided by loss calculated
with their pre-trained weights) to serve as an encoder. As for each image I ij. Given that each image yields a unique
depicted in Figure 4, this new classifier includes additional loss value, the sequence (order and combination) of the input
pooling, batch normalization, dropout, linear, and non-linear spatiotemporal images affects the performance of the multi-
layers, all trained from scratch. This approach repurposes the stream sub-network [33].
feature maps extracted by the encoder more effectively for The class probabilities
{CP}hN
obtained from the
ij(h=1)
the new classification task. The classifier produces a set of multi-stream classifier are transformed into a single RGB
probabilities, predicting the likelihood that the input image pseudo-image via online decision-level fusion. This pseudo-
I belongs to any class C ,hϵ[1,N] within the set S. image is then passed through into an ensemble tuner sub-
ij h
To leverage the diverse view orientations available during network,producinganothersetofclassprobabilities.Thetuner
data-level fusion for generating the spatiotemporal images, utilizes a comparatively simple, pre-trained CNN architecture
we employ the specialized multi-stream CNN architecture as an encoder, whereas its classifier is unchanged as shown
illustratedinFigure5.Thisarchitecture,whichutilizestransfer in Figure 4 (right). The ensemble tuner multi-stream CNN
learning and ensemble training, is specifically engineered architecture undergoes end-to-end training, yielding (j + 1)7
classprobabilitiesandlossesforeachgestureg .Onlytheclass joints and employs a 70:30 random-split protocol for
i
probabilities—andclassificationaccuracy—obtainedfromthe thetraining(1960gestures)andvalidation(840gestures)
ensemble tuner sub-network are reported in this paper. To subsets.
tackle the multi-task nature of the problem, a specialized • DHG1428:FollowingastructuresimilartoSHREC2017,
loss function inspired by [34] is employed. Similar to the the DHG1428 dataset [18] comprises 2800 sequences
approach outlined in [33], this loss function accounts for the performance by 20 subjects for the 14G and 28G bench-
homoscedastic uncertainties related to the view orientations marks. It provides equivalent skeleton date and follows
and the ensemble tuner by appropriately weighing the (j+1) the same 70:30 split for training and validation.
cross-entropy losses. • SBUKID: This smaller HAR dataset [21] contains 282
Implementing a multi-stream network that accepts inputs action sequences across eight classes involving two-
from diverse view orientations helps the model distinguish person interaction. SBUKID provides skeleton data for
between gesture classes that look similar when visualized 15 joints per subject and uses a five-fold cross-validation
from a single view orientation, thus improving the classifi- evaluation protocol, with average accuracies reported
cation accuracy. Additionally, using of RGB pseudo-images across all folds.
for decision-level fusion facilitates transfer learning and aids
in preserving semantic alignment among the class probabili- B. Generalized HGR Framework
ties {CP} generated for each view orientation {VO} by
j j Our proposed HGR framework, in its generalized form, in-
the multi-stream sub-network. Integrating the multi-stream
corporates data-level fusion to create static 2D spatiotemporal
and ensemble tuner sub-networks into a single architecture
images combined with the ensemble tuner multi-stream CNN
circumvents the need to train multiple models separately,
architectureforclassifyingsaidimages.TableIshowsthatthe
addressing a common limitation of conventional ensemble
elevation and azimuth angles of the virtual camera required
training approaches.
for the view orientations during data-level fusion are dataset-
specific. The same padding value γ =0.125 was used for all
IV. EXPERIMENTS&RESULTS
datasets during sequence fitting. Note that the aforementioned
A. Datasets settings do not apply to the CNR dataset as it only provides
static images.
In this section, we review the five benchmark datasets used
For the custom network architecture, the performance of
in the experiments to build and test the main elements of
26 distinct CNN architectures from the ResNet, Inception,
our proposed HGR framework, along with its comparison
EfficientNet,ResNeXt,SE-ResNeXt,SE-ResNet,andxResNet
to the current state-of-the-art (SOTA). Each dataset presents
families were evaluated as shown in Table II. Said architec-
distinct challenges and opportunities for the development and
tures were pre-trained on ImageNet and trained with a dataset
evaluation of HGR frameworks.
ofstatic,spatiotemporalimagesgeneratedfromtheDHG1428
• CNR: The CNR dataset [16] includes spatiotemporal
dataset and front-to view orientation. From the empirical
images captured from a top-down view, covering 1925
results for two distinct training setups (TS1 and TS2), the
gesture sequences across 16 classes.This dataset is split
ResNet variants ResNet-50 and ResNet-18 were selected as
into a training set with 1348 images and a validation
thebaseencodersforourcustomnetworkarchitecture’smulti-
set with 577 images. The absence of raw skeleton data
stream and ensemble tuner sub-networks.
presentsalimitationasourframeworkcanonlybetrained
The optimal sequence of VOs for feeding spatiotemporal
from one viewpoint.
inputs into the multi-stream sub-network is specific to each
• LMDHG: Comprising 608 gesture sequences over 13
dataset, depending on the specifics of how the raw gestures
classes, the LMDHG dataset [20] is divided into training
were collected and processed. Our experiments showed that
(414 gestures) and validation (194 gestures) sets. The
combinations of three unique VOs are enough for good clas-
dataset exhibits minimal overlap in subjects across both
sification performance. We employed an iterative approach to
subsets, featuring comprehensive 3D skeleton data com-
determinetheoptimalsequenceofVOs:initially,eachVOwas
prising a total of 46 hand joints for each hand.
trained individually in a single-stream network. Then, paired
• FPHA: This dataset [19], featuring a wide range of
combinations of the highest-performing VOs were trained in
styles, viewpoints, and scenarios, includes 1175 gesture
a two-stream network. Following this, triple combinations of
sequences across 45 classes. Its primary challenges stem
the top-performing VO pairs were trained in a three-stream
from the similarity in motion patterns, the diverse range
network. This process identifies the optimal sequence of three
of objects involved, and a low ratio of gesture sequences
VOs tailored to each dataset.
to classes. The dataset is divided into 600 gestures for
training and 575 for validation, providing 3D skeleton
C. Implementation and Training
data for 21 hand joints per subject.
• SHREC2017: With 2800 sequences performed by 28 The developed framework was developed using Python
subjects, the SHREC2017 dataset [17] is designed for 3.8.5, and tested on a Linux Ubuntu 18.04 server with four
both coarse and fine-grained gesture classification, di- NVIDIA GeForce GTX TITAN X graphics cards. For data-
videdinto14-gesture(14G)and28-gesture(28G)bench- level fusion, the Vispy visualization library was utilized,
marks.Thedatasetprovides3Dskeletondatafor22hand whereas OpenCV was employed to generate pseudo-images8
TABLEI
VIRTUALCAMERA(ELEVATION,AZIMUTH)ANGLES(INDEGREES)FOREACHVIEWORIENTATION.
ViewOrientation DHG1428 SHREC2017 FHPA LMDHG
top-down (0.0,0.0) (0.0,0.0) (90.0,0.0) (0.0,0.0)
front-to (90.0,180.0) (90.0,180.0) (0.0,180.0) (-90.0,-180.0)
front-away (-90.0,0.0) (-90.0,0.0) (0.0,0.0) (90.0,0.0)
side-right (0.0,-90.0) (0.0,-90.0) (0.0,90.0) (0.0,90.0)
side-left (0.0,90.0) (0.0,90.0) (0.0,-90.0) (0.0,-90.0)
custom (30.0,-132.5) (30.0,-132.5) (25.0,115.0) (-15.0,-135.0)
TABLEII the learning rate, and the initial learning rate for each stage
COMPARATIVEANALYSISOFVARIOUSPRE-TRAINEDCNN was automatically determined using the FastAI learner.lr find
ARCHITECTURES.
method. Furthermore, various data augmentation techniques
were applied during the training process, including random
CNNArchitecture ClassificationAccuracies(%)
horizontal flips, affine transformations, perspective warping,
Family Variants TS1 TS2 Average
random zooms, random rotations, and adjustments in color
ResNet18 0.8083 0.7762
space.
ResNet34 0.8190 0.7952
ResNet ResNet50 0.8417 0.8012 0.8149
ResNet101 0.8226 0.8286
D. Results on the CNR Dataset
ResNet152 0.8310 0.8250
Inception-v3 0.8155 0.7762 Table III presents a comparative evaluation of our proposed
Inception-v4 0.8310 0.7964 framework with other HGR frameworks on the CNR dataset.
Inception 0.8124
Inception-ResNet-v1 0.8179 0.8262
Ourproposedframeworkexhibitedslightlylowerperformance
Inception-ResNet-v2 0.8167 0.8190
compared to the SOTA [16] by -1.73%. This drop in perfor-
EfficientNet-B0 0.8143 0.7964
mance can be attributed to the absence of raw skeleton data
EfficientNet-B3 0.8107 0.8107
EfficientNet EfficientNet-B5* 0.8143 0.8131 0.8083 for data-level fusion. Thus, our enhanced data-level fusion—
EfficientNet-B7* 0.7893 0.8179 which incorporates denoising, sequence fitting, and multiple
ResNeXt26 0.8048 0.7726 view orientations—was not available during model training.
ResNeXt ResNeXt50 0.8298 0.7952 0.8042
ResNeXt101 0.8226 0.8000
TABLEIII
SE-ResNeXt SE-ResNeXt50 0.8143 0.7595 0.8033 COMPARISONOFVALIDATIONACCURACYWITHSOTAONTHECNR
SE-ResNeXt101 0.8405 0.7988 DATASET
SE-ResNet18 0.7881 0.7774
SE-ResNet26 0.8048 0.7726 Method ClassificationAccuracy(%)
SE-ResNet SE-ResNet50 0.8179 0.8024 0.8032
ProposedFramework 97.05
SE-ResNet101 0.8310 0.8131
Lupinettietal.[16] 98.78
SE-ResNet152 0.8060 0.8190
xResNet50 0.7440 0.7333
xResNet xResNet50-Deep 0.7226 0.7357 0.7379
xResNet50-Deeper 0.7405 0.7512
E. Results on the LMDHG Dataset
Table IV provides a comparative performance analysis be-
tween our proposed framework and other HGR frameworks
for decision-level fusion. Furthermore, the entire machine
on the LMDHG dataset. Our framework surpassed the SOTA
learning workflow, including the design of network architec-
results presented in [16] by +6.86%, attributed to the utiliza-
turesandthetrainingofmodels,wascarriedoutusingPyTorch
tionofourimproveddata-levelfusionapproachforgenerating
and FastAI. The code for this paper can be found at this
spatiotemporal images. These evaluation results highlight the
GitHub repository.
effectiveness of our data-level fusion enhancements and the
During data-level fusion, each gesture sequence was re-
subsequent shift from dynamic hand gesture recognition to
sampled to a unified temporal window T = 250 frames.
static image classification. In addition, our transformation
The static images and pseudo-images were generated with a
process effectively preserved crucial semantic information by
square aspect ratio at 960px and 224px respectively. During
utilizing an optimal sequence of view orientations—[custom,
the training phase, a batch size of 16 was utilized, the Adam
front-away, top-down].
optimizer was employed, and a custom loss function incor-
poratingcross-entropyandhomoscedasticitywasapplied.The
F. Results on the FPHA Dataset
trainingwasconductedinmultiplestages,withthedimensions
of the static input images progressively adjusted to 224px, As previously mentioned, the FPHA dataset is quite chal-
276px, 328px, and 380px (as needed) to enhance model lenging for HGR evaluation, and this difficulty is further
performance. The cosine annealing schedule was utilized for exacerbated by the adoption of a 1:1 evaluation protocol,9
TABLEIV H. Results on the DHG1428 Dataset
COMPARISONOFVALIDATIONACCURACYWITHSOTAONTHELMDHG
Table VII showcases an extensive evaluation of our
DATASET
proposed framework with other HGR frameworks on the
Method ClassificationAccuracy(%) DHG1428 dataset. Employing the optimal sequence of view
orientations—[custom, top-down, front-away]—the 14G and
Boulahiaetal.[20] 84.78
Lupinettietal.[16] 92.11 28G validation accuracies were 95.83% and 92.38%, respec-
Mohammedetal.[10] 93.81 tively. Thus, our framework exhibited a marginally lower
ProposedFramework 98.97
performance than the SOTA [2] by -0.48% and -1.67%,
respectively.
resulting in closely balanced training and validation pro- TABLEVII
portions. Table Vpresents our proposed framework’s perfor-
COMPARISONOFVALIDATIONACCURACYWITHSOTAONTHE
DHG1428DATASET
mance compared to other HGR frameworks evaluated on
the FPHA dataset. Employing an optimal sequence of view ClassificationAccuracy(%)
Method
orientations—[front-away, custom, top-down]—our proposed 14G 28G Average
framework fell short of the SOTA [22] by -4.10%. Laietal.[38] 85.46 74.19 79.83
Chenetal.[39] 84.68 80.32 82.50
Wengetal.[40] 85.80 80.20 83.00
TABLEV Devineauetal.[31] 91.28 84.35 87.82
COMPARISONOFVALIDATIONACCURACYWITHSOTAONTHEFPHA Nguyenetal.[36] 92.38 86.31 89.35
DATASET Chenetal.[37] 91.90 88.00 89.95
Mohammedetal.[10] 91.64 89.46 90.55
Method ClassificationAccuracy(%) Liuetal.[3] 92.54 88.86 90.70
Liuetal.[11] 92.71 89.15 90.93
Sahbi[35] 86.78 Nguyenetal.[41] 94.29 89.40 91.85
Liuetal.[11] 89.04 Shietal.[12] 93.80 90.90 92.35
Lietal.[2] 90.26 ProposedFramework 95.83 92.38 94.11
Liuetal.[3] 90.96 Lietal.[2] 96.31 94.05 95.18
ProposedFramework 91.83
Nguyenetal.[36] 93.22
Rehanetal.[32] 93.91
It is important to acknowledge that while the SHREC2017
Sabateretal.[22] 95.93
and DHG1428 datasets share similarities, DHG1428 offers
a more equitable distribution of subjects across all classes.
Consequently, the DHG1428 14G and 28G classification ac-
curacies reported by HGR frameworks tend to be consistently
G. Results on the SHREC2017 Dataset lower than those reported for the SHREC2017 dataset [3],
[11],[12],[37].Inthisregard,ourproposedframeworkdidnot
Table VI presents a comparative analysis between our
deviate from this trend, with DHG1428 14G and 28G results
proposed framework and other HGR frameworks on the
-2.03% and -2.98% lower than their SHREC2017 equivalents.
SHREC2017 dataset. The empirically established ideal se-
Figure 6 presents the confusion matrix of our framework’s
quence of view orientations, [front-away, custom, front-to],
performance on the DHG1428 28G dataset. As evident from
resulted in 14G and 28G validation accuracies of 97.86%
the robust validation accuracy of 92.38%, a significant align-
and 95.36%, respectively. These results show improvements
ment exists between the actual gesture classes and their
of +0.86% and +1.46% in comparison to the SOTA results
corresponding predictions. Note that class labels in the figure
reported in [12].
areaugmentedwithnumericalprefixes,differentiatingbetween
the DHG1428 performance modes,. The prefixes specifically
TABLEVI denote whether the gesture was executed with one finger (01-
COMPARISONOFVALIDATIONACCURACYWITHSOTAONTHE
14) or the entire hand (15-28).
SHREC2017DATASET
Consistentwiththeobservationsinpreviousresearchobser-
ClassificationAccuracy(%) vations [42], our analysis underscores a significant challenge
Method
14G 28G Average in differentiating between the “Grab” and “Pinch” gesture
Sabateretal.[22] 93.57 91.43 92.50 classes.Thischallengepersistsacrossbothperformancemodes
Chenetal.[37] 94.40 90.70 92.55
and becomes apparent when visually inspecting the images
Yangetal.[4] 94.60 91.90 93.25
Liuetal.[3] 94.88 92.26 93.57 generated for these two gesture classes. The visual similarity
Liuetal.[11] 95.00 92.86 93.93 between them is significant enough to cause confusion even
Rehanetal.[32] 95.60 92.74 94.17
for human observers.
Mohammedetal.[10] 95.60 93.10 94.35
Dengetal.[23] 96.40 93.30 94.85
Minetal.[9] 95.90 94.70 95.30 I. Ablation Study on SBUKID Dataset
Shietal.[12] 97.00 93.90 95.45
ProposedFramework 97.86 95.36 96.61 Due to the design of the data-level fusion process, we be-
lieve our proposed framework is domain-agnostic and is simi-
larlyapplicabletootherdomainsinvolvingtheclassificationof10
Fig.6. ConfusionMatrixfortheProposedFrameworkontheDHG142828GDataset.
temporal dynamic data in the form of 2D/3D coordinates. To data from various domains for image classification.
testthishypothesis,weadaptedourframeworktotheskeleton-
based Human Action Recognition (HAR) domain, with slight
alterations in the data-level fusion process. HAR, similar to
HGR, enables computers to recognize and interpret dynamic TABLEVIII
human actions by using the entire human body as input. COMPARISONOFVALIDATIONACCURACYWITHSOTAONTHESBUKID
DATASET
We utilized the SBUKID dataset to create spatiotemporal
datasetsvisualizedfromallsixvieworientations.Eachdataset AverageCross-Validation
Method
was then independently used to train a custom single-stream ClassificationAccuracy(%)
CNN architecture based on a pre-trained ResNet-50 encoder. Songetal.[43] 91.50
Liuetal.[44] 93.50
From the experimental results in Table VIII, our proposed
Kacemetal.[45] 93.70
framework compares favorably with other specialized HAR ProposedFramework 93.96
frameworks, achieving an average cross-validation classifica- Keetal.[46] 94.17
Liuetal.[47] 94.90
tion accuracy of 93.96% with the single [front-away] view
Maghoumietal.[48] 95.70
orientation. Without utilizing the specialized e2eET multi- Zhangetal.[49] 98.30
stream CNN architecture, said performance is only -4.34%
lowerthantheSOTA,thushighlightingtheeffectivenessofour
data-level fusion design for transforming temporal dynamic11
Fig.7. DemonstrationoftheReal-TimeHGRApplication,BasedonOurProposedDynamicHGRFramework.
V. REAL-TIMEHGRAPPLICATION inference. Compared with other applications running on the
PC, the total CPU and RAM usage was deemed acceptable
To demonstrate the practical utility of our proposed frame-
and had a negligible impact on the PC’s functionality and
work in reducing hardware and computational demands for
other applications running simultaneously.
HGR-based applications, we developed a real-time HGR ap-
plication[33]basedonourgeneralizedHGRframework.This
application’s underlying model was trained specifically on
VI. CONCLUSIONSANDFUTUREWORK
the ”Swipe” gestures from the DHG1428 dataset, including A. Conclusions
gestures like ”Up”, ”Down”, ”Right”, ”Left”, ”+”, ”V”, and
This paper first explores various frameworks for dynamic
”X”. The operational pipeline of the application is depicted in
HGR and then introduces a robust framework for skeleton-
Figure 7 and can be summarized as follows:
basedhandgesturerecognition(HGR)thatefficientlyconverts
1) OpenCV captures the raw gesture data as RGB videos the task of recognizing dynamic hand gestures into that of
from the inbuilt PC webcam. static image classification while preserving essential semantic
2) The video frames are processed for hand detection and information. The framework incorporates an improved data-
pose estimation using MediaPipe Hands. level fusion technique to generate static RGB spatiotemporal
3) Data-level fusion is performed on the skeleton data, images from hand pose information. It also introduces an en-
resulting in three spatiotemporal images from [custom, semble tuner multi-stream CNN architecture, which leverages
top-down, front-away] view orientations. multiple view orientations for accurate classification of the
4) These spatiotemporal images are then processed by the static images while maintaining computational efficiency.
trained DHG1428 model, which produces four class
Extensive experiments were conducted on five benchmark
predictions: three from the multi-stream sub-network
datasets to evaluate the effectiveness and generalization of
and one from the ensemble tuner sub-network.
the proposed HGR framework. The results demonstrate the
5) An output graphical user interface (GUI) presents the
framework’s competitiveness, achieving accuracies within a
class predictions, information about the input gesture
rangeof-4.10%to+6.86%comparedtoreportedstate-of-the-
sequence, and the framework’s latency.
artvalidationaccuracies.Ablationstudiesrelatedtothehuman
Thereal-timeapplicationeffectivelyshowedthatastandard action recognition domain also yielded competitive results,
PC webcam can adequately collect raw dynamic gesture data falling -4.34% short of the reported state-of-the-art validation
from a user, negating the necessity for extra specialized and accuracy.These findingshighlight theframework’s efficacyin
costlysensors.OnaPCequippedwithanIntelCorei7-9750H handling temporal dynamic data across various domains.
CPUand16GBofRAM,theapplicationmaintainedalatency Additionally, the framework’s practical usefulness was il-
of 2-3 seconds with a capture rate of 15 frames per second lustrated by creating a real-time HGR application. This appli-
from the execution of a gesture to the completion of model cation functioned using a standard built-in PC webcam and12
showed low CPU and RAM resource consumption, highlight- [4] F. Yang, S. Sakti, Y. Wu, and S. Nakamura, “Make Skeleton-
ing its efficiency. This underscores the viability of data-level based Action Recognition Model Smaller, Faster and Better,”
arXiv:1907.09658 [cs], Mar. 2020, arXiv: 1907.09658 version: 8.
fusion in reducing hardware requirements and computational
[Online].Available:http://arxiv.org/abs/1907.09658
complexity without compromising latency and frames-per- [5] H. Gammulle, S. Denman, S. Sridharan, and C. Fookes, “TMMF:
second performance. Temporal Multi-Modal Fusion for Single-Stage Continuous Gesture
Recognition,” IEEE Transactions on Image Processing, vol. 30, pp.
7689–7701, 2021, conference Name: IEEE Transactions on Image
B. Future Work Processing.
[6] G. Benitez-Garcia, J. Olivares-Mercado, G. Sanchez-Perez, and
Inadditiontotheachievementsoutlinedinthiswork,several K. Yanai, “IPN Hand: A Video Dataset and Benchmark for
potential enhancements and expansions could be explored in Real-Time Continuous Hand Gesture Recognition,” arXiv:2005.02134
[cs], Oct. 2020, arXiv: 2005.02134. [Online]. Available: http:
futureresearch.Extendingtheapproachtoskeleton-basedHu-
//arxiv.org/abs/2005.02134
manActionRecognition(HAR)isanaturalprogression.HGR [7] Z. Wang, Q. She, T. Chalasani, and A. Smolic, “CatNet: Class
andHARexhibitsufficientsimilaritiestojustifyleveragingthe Incremental3DConvNetsforLifelongEgocentricGestureRecognition,”
arXiv:2004.09215 [cs, eess], Apr. 2020, arXiv: 2004.09215. [Online].
existingframeworktorecognizeandcategorizehumanactions
Available:http://arxiv.org/abs/2004.09215
based on skeletal data. [8] C. Zhang, Y. Zou, G. Chen, and L. Gan, “PAN: Towards
Anotherareaworthexploringisenhancingthemulti-stream Fast Action Recognition via Learning Persistence of Appearance,”
arXiv:2008.03462 [cs], Aug. 2020, arXiv: 2008.03462. [Online].
networkarchitecture.Integratingtransformersorattentionnet-
Available:http://arxiv.org/abs/2008.03462
works into the architecture has shown promise in improving [9] Y. Min, Y. Zhang, X. Chai, and X. Chen, “An Efficient PointLSTM
performance.Giventheadvancementsinattentionmechanisms for Point Clouds Based Gesture Recognition,” 2020, pp. 5761–5770.
[Online]. Available: https://openaccess.thecvf.com/content CVPR
in deep learning, further exploration of their application to
2020/html/Min An Efficient PointLSTM for Point Clouds Based
enhance hand gesture recognition and other tasks would be Gesture Recognition CVPR 2020 paper.html
valuable. [10] A. Mohammed, Y. Gao, Z. Ji, J. Lv, S. Islam, and Y. Sang,
“Automatic 3D Skeleton-based Dynamic Hand Gesture Recognition
Testing the implementation of the developed framework
Using Multi-Layer Convolutional LSTM,” in Proceedings of the
in real-world scenarios is crucial to validate its practicality. 7th International Conference on Robotics and Artificial Intelligence,
Deployingtheframeworkindomainslikehealthcareorvirtual ser. ICRAI ’21. New York, NY, USA: Association for Computing
Machinery, Apr. 2022, pp. 8–14. [Online]. Available: https://dl.acm.
reality can provide valuable insights into its effectiveness,
org/doi/10.1145/3505688.3505690
limitations, and areas that need refinement. Expanding on the
[11] J. Liu, Y. Wang, S. Xiang, and C. Pan, “HAN: An Efficient
existing real-time hand gesture recognition application and Hierarchical Self-Attention Network for Skeleton-Based Gesture
exploring its deployment in these domains would be a logical Recognition,” arXiv:2106.13391 [cs], Jun. 2021, arXiv: 2106.13391.
[Online].Available:http://arxiv.org/abs/2106.13391
next step.
[12] L. Shi, Y. Zhang, J. Cheng, and H. Lu, “Decoupled Spatial-
Considering this paper’s emphasis on computational ef- Temporal Attention Network for Skeleton-Based Action Recognition,”
ficiency in this paper, it would be highly relevant to in- arXiv:2007.03263 [cs], Jul. 2020, arXiv: 2007.03263. [Online].
Available:http://arxiv.org/abs/2007.03263
vestigate further advanced optimization methods to improve
[13] X.Yang,P.Molchanov,andJ.Kautz,“MakingConvolutionalNetworks
the framework’s performance. Researching and implementing Recurrent for Visual Sequence Learning,” 2018, pp. 6469–6478.
optimization techniques tailored to the framework’s specific [Online]. Available: https://openaccess.thecvf.com/content cvpr 2018/
html/Yang Making Convolutional Networks CVPR 2018 paper.html
requirements can maximize its efficiency and effectiveness.
[14] P. Molchanov, X. Yang, S. Gupta, K. Kim, S. Tyree, and J. Kautz,
Lastly, conducting a comprehensive evaluation of the user “Online Detection and Classification of Dynamic Hand Gestures
experience (UX) would offer valuable feedback on the real- with Recurrent 3D Convolutional Neural Networks,” in 2016 IEEE
ConferenceonComputerVisionandPatternRecognition(CVPR). Las
time application’s usability and shape the direction of subse-
Vegas,NV,USA:IEEE,Jun.2016,pp.4207–4215.[Online].Available:
quent iterations and refinements. A dedicated UX study can http://ieeexplore.ieee.org/document/7780825/
provide insights from users into potential areas for refinement [15] Z.Yu,B.Zhou,J.Wan,P.Wang,H.Chen,X.Liu,S.Z.Li,andG.Zhao,
“SearchingMulti-RateandMulti-ModalTemporalEnhancedNetworks
andenhancement.Exploringthesefutureresearchavenueswill
for Gesture Recognition,” IEEE Transactions on Image Processing,
advance dynamic hand gesture recognition and provide valu- vol. 30, pp. 5626–5640, 2021, conference Name: IEEE Transactions
able insights into the practicality, performance optimization, onImageProcessing.
[16] K.Lupinetti,A.Ranieri,F.Giannini,andM.Monti,“3Ddynamichand
and user-centric aspects of the developed framework.
gestures recognition using the Leap Motion sensor and convolutional
neural networks,” arXiv:2003.01450 [cs, eess], Sep. 2020, arXiv:
REFERENCES 2003.01450.[Online].Available:http://arxiv.org/abs/2003.01450
[17] Q. De Smedt, H. Wannous, J.-P. Vandeborre, J. Guerry, B. Le Saux,
[1] K. Lai and S. Yanushkevich, “An Ensemble of Knowledge Sharing and D. Filliat, “SHREC’17 Track: 3D Hand Gesture Recognition
Models for Dynamic Hand Gesture Recognition,” arXiv:2008.05732 Using a Depth and Skeletal Dataset,” in 3DOR - 10th Eurographics
[cs], Aug. 2020, arXiv: 2008.05732. [Online]. Available: http: Workshop on 3D Object Retrieval, I. Pratikakis, F. Dupont, and
//arxiv.org/abs/2008.05732 M. Ovsjanikov, Eds., Lyon, France, Apr. 2017, pp. 1–6. [Online].
[2] C. Li, S. Li, Y. Gao, X. Zhang, and W. Li, “A Two-stream Neural Available:https://hal.archives-ouvertes.fr/hal-01563505
NetworkforPose-basedHandGestureRecognition,”arXiv:2101.08926 [18] Q. De Smedt, H. Wannous, and J.-P. Vandeborre, “Skeleton-Based
[cs], Jan. 2021, arXiv: 2101.08926. [Online]. Available: http: Dynamic Hand Gesture Recognition,” in 2016 IEEE Conference on
//arxiv.org/abs/2101.08926 Computer Vision and Pattern Recognition Workshops (CVPRW). Las
[3] J. Liu, Y. Liu, Y. Wang, V. Prinet, S. Xiang, and Vegas,NV,USA:IEEE,Jun.2016,pp.1206–1214.[Online].Available:
C. Pan, “Decoupled Representation Learning for Skeleton- http://ieeexplore.ieee.org/document/7789643/
Based Gesture Recognition,” 2020, pp. 5751–5760. [Online]. [19] G. Garcia-Hernando, S. Yuan, S. Baek, and T.-K. Kim, “First-Person
Available: https://openaccess.thecvf.com/content CVPR 2020/ Hand Action Benchmark with RGB-D Videos and 3D Hand Pose
html/Liu Decoupled Representation Learning for Skeleton-Based Annotations,” arXiv:1704.02463 [cs], Apr. 2018, arXiv: 1704.02463.
Gesture Recognition CVPR 2020 paper.html [Online].Available:http://arxiv.org/abs/1704.0246313
[20] S.Y.Boulahia,E.Anquetil,F.Multon,andR.Kulpa,“Dynamichand [38] K. Lai and S. N. Yanushkevich, “CNN+RNN Depth and Skeleton
gesturerecognitionbasedon3Dpatternassembledtrajectories,”in2017 basedDynamicHandGestureRecognition,”in201824thInternational
Seventh International Conference on Image Processing Theory, Tools ConferenceonPatternRecognition(ICPR),Aug.2018,pp.3451–3456,
andApplications(IPTA),Nov.2017,pp.1–6,iSSN:2154-512X. arXiv:2007.11983 [cs]. [Online]. Available: http://arxiv.org/abs/2007.
[21] K. Yun, J. Honorio, D. Chattopadhyay, T. L. Berg, and D. Samaras, 11983
“Two-personInteractionDetectionUsingBody-PoseFeaturesandMul- [39] X. Chen, H. Guo, G. Wang, and L. Zhang, “Motion Feature
tiple Instance Learning,” in 2012 IEEE Computer Society Conference Augmented Recurrent Neural Network for Skeleton-based Dynamic
onComputerVisionandPatternRecognitionWorkshops,Jun.2012,pp. Hand Gesture Recognition,” 2017 IEEE International Conference
28–35,iSSN:2160-7516. on Image Processing (ICIP), pp. 2881–2885, Sep. 2017, arXiv:
[22] A. Sabater, I. Alonso, L. Montesano, and A. C. Murillo, “Domain 1708.03278.[Online].Available:http://arxiv.org/abs/1708.03278
andView-pointAgnosticHandActionRecognition,”arXiv:2103.02303 [40] J. Weng, M. Liu, X. Jiang, and J. Yuan, “De-
[cs], Oct. 2021, arXiv: 2103.02303. [Online]. Available: http: formable Pose Traversal Convolution for 3D Action and
//arxiv.org/abs/2103.02303 Gesture Recognition,” 2018, pp. 136–152. [Online]. Avail-
[23] Z.Deng,Q.Gao,Z.Ju,andX.Yu,“Skeleton-BasedMultifeaturesand able: https://openaccess.thecvf.com/content ECCV 2018/html/Junwu
MultistreamNetworkforReal-TimeActionRecognition,”IEEESensors Weng Deformable Pose Traversal ECCV 2018 paper.html
Journal, vol. 23, no. 7, pp. 7397–7409, Apr. 2023, conference Name: [41] X. Nguyen, L. Brun, O. Lezoray, and S. Bougleux, “Skeleton-Based
IEEESensorsJournal. Hand Gesture Recognition by Learning SPD Matrices with Neural
[24] A.S.M.Miah,M.A.M.Hasan,andJ.Shin,“DynamicHandGesture Networks,” May 2019, arXiv:1905.07917 [cs]. [Online]. Available:
Recognition Using Multi-Branch Attention Based Graph and General http://arxiv.org/abs/1905.07917
Deep Learning Model,” IEEE Access, vol. 11, pp. 4703–4716, 2023, [42] J. Chen, C. Zhao, Q. Wang, and H. Meng, “HMANet: Hyperbolic
conferenceName:IEEEAccess. Manifold Aware Network for Skeleton-Based Action Recognition,”
[25] J.-H.Song,K.Kong,andS.-J.Kang,“DynamicHandGestureRecogni- IEEE Transactions on Cognitive and Developmental Systems, vol. 15,
tion Using Improved Spatio-Temporal Graph Convolutional Network,” no.2,pp.602–614,Jun.2023,conferenceName:IEEETransactionson
IEEE Transactions on Circuits and Systems for Video Technology, CognitiveandDevelopmentalSystems.
vol. 32, no. 9, pp. 6227–6239, Sep. 2022, conference Name: IEEE [43] S. Song, C. Lan, J. Xing, W. Zeng, and J. Liu, “Spatio-Temporal
TransactionsonCircuitsandSystemsforVideoTechnology. Attention-Based LSTM Networks for 3D Action Recognition and De-
[26] M. R. Morris, “AI and Accessibility,” Communications of the tection,” IEEE Transactions on Image Processing, vol. 27, no. 7, pp.
ACM, vol. 63, no. 6, pp. 35–37, May 2020. [Online]. Available: 3459–3471,Jul.2018,conferenceName:IEEETransactionsonImage
https://dl.acm.org/doi/10.1145/3356727 Processing.
[27] W.MuchaandM.Kampel,“BeyondPrivacyofDepthSensorsinActive [44] X.Liu,H.Shi,X.Hong,H.Chen,D.Tao,andG.Zhao,“3DSkeletal
and Assisted Living Devices,” in The15th International Conference GestureRecognitionviaHiddenStatesExploration,”IEEETransactions
on PErvasive Technologies Related to Assistive Environments. Corfu onImageProcessing,vol.29,pp.4583–4597,2020,conferenceName:
Greece: ACM, Jun. 2022, pp. 425–429. [Online]. Available: https: IEEETransactionsonImageProcessing.
//dl.acm.org/doi/10.1145/3529190.3534764 [45] A. Kacem, M. Daoudi, B. B. Amor, S. Berretti, and J. C. Alvarez-
[28] O.Ko¨pu¨klu¨,N.Ko¨se,andG.Rigoll,“MotionFusedFrames:DataLevel Paiva, “A Novel Geometric Framework on Gram Matrix Trajectories
Fusion Strategy for Hand Gesture Recognition,” arXiv:1804.07187 for Human Behavior Understanding,” IEEE Transactions on Pattern
[cs], Apr. 2018, arXiv: 1804.07187. [Online]. Available: http: AnalysisandMachineIntelligence,vol.42,no.1,pp.1–14,Jan.2020,
//arxiv.org/abs/1804.07187 conferenceName:IEEETransactionsonPatternAnalysisandMachine
[29] C. C. d. Santos, J. L. A. Samatelo, and R. F. Vassallo, “Dynamic Intelligence.
Gesture Recognition by Using CNNs and Star RGB: a Temporal [46] Q. Ke, M. Bennamoun, S. An, F. Sohel, and F. Boussaid, “Learning
Information Condensation,” arXiv:1904.08505 [cs], Sep. 2019, arXiv: ClipRepresentationsforSkeleton-Based3DActionRecognition,”IEEE
1904.08505.[Online].Available:http://arxiv.org/abs/1904.08505 TransactionsonImageProcessing,vol.27,no.6,pp.2842–2855,Jun.
[30] M. Abavisani, H. R. V. Joze, and V. M. Patel, “Improving the 2018,conferenceName:IEEETransactionsonImageProcessing.
Performance of Unimodal Dynamic Hand-Gesture Recognition With [47] J. Liu, G. Wang, L.-Y. Duan, K. Abdiyeva, and A. C. Kot, “Skeleton-
Multimodal Training,” 2019, pp. 1165–1174. [Online]. Available: BasedHumanActionRecognitionWithGlobalContext-AwareAttention
https://openaccess.thecvf.com/content CVPR 2019/html/Abavisani LSTM Networks,” IEEE Transactions on Image Processing, vol. 27,
Improving the Performance of Unimodal Dynamic Hand-Gesture no.4,pp.1586–1599,Apr.2018,conferenceName:IEEETransactions
Recognition With Multimodal CVPR 2019 paper.html onImageProcessing.
[31] G.Devineau,W.Xi,F.Moutarde,andJ.Yang,“DeepLearningforHand [48] M. Maghoumi and J. J. LaViola Jr, “DeepGRU: Deep Gesture
GestureRecognitiononSkeletalData,”May2018.[Online].Available: Recognition Utility,” arXiv:1810.12514 [cs], Oct. 2019, arXiv:
https://hal-mines-paristech.archives-ouvertes.fr/hal-01737771 1810.12514.[Online].Available:http://arxiv.org/abs/1810.12514
[32] M. Rehan, H. Wannous, J. Alkheir, and K. Aboukassem, “Learning [49] P. Zhang, C. Lan, J. Xing, W. Zeng, J. Xue, and N. Zheng, “View
Co-occurrence Features Across Spatial and Temporal Domains for Adaptive Neural Networks for High Performance Skeleton-Based Hu-
Hand Gesture Recognition,” in Proceedings of the 19th International man Action Recognition,” IEEE Transactions on Pattern Analysis and
Conference on Content-based Multimedia Indexing, ser. CBMI ’22. Machine Intelligence, vol. 41, no. 8, pp. 1963–1978, Aug. 2019,
New York, NY, USA: Association for Computing Machinery, Oct. conferenceName:IEEETransactionsonPatternAnalysisandMachine
2022, pp. 36–42. [Online]. Available: https://dl.acm.org/doi/10.1145/ Intelligence.
3549555.3549591
[33] O. Yusuf and M. Habib, “Development of a Lightweight Real-Time
Application for Dynamic Hand Gesture Recognition,” in 2023 IEEE
International Conference on Mechatronics and Automation (ICMA),
Aug.2023,pp.543–548,iSSN:2152-744X.
[34] A. Kendall, Y. Gal, and R. Cipolla, “Multi-Task Learning Using
UncertaintytoWeighLossesforSceneGeometryandSemantics,”Apr.
2018, arXiv:1705.07115 [cs]. [Online]. Available: http://arxiv.org/abs/
1705.07115
[35] H.Sahbi,“Skeleton-basedHand-GestureRecognitionwithLightweight
Graph Convolutional Networks,” arXiv:2104.04255 [cs], Apr. 2021,
arXiv:2104.04255.[Online].Available:http://arxiv.org/abs/2104.04255
[36] X. S. Nguyen, L. Brun, O. Le´zoray, and S. Bougleux, “A
neural network based on SPD manifold learning for skeleton-based
hand gesture recognition,” arXiv:1904.12970 [cs], Apr. 2019, arXiv:
1904.12970.[Online].Available:http://arxiv.org/abs/1904.12970
[37] Y. Chen, L. Zhao, X. Peng, J. Yuan, and D. N. Metaxas, “Construct
Dynamic Graphs for Hand Gesture Recognition via Spatial-Temporal
Attention,” arXiv:1907.08871 [cs], Jul. 2019, arXiv: 1907.08871.
[Online].Available:http://arxiv.org/abs/1907.08871