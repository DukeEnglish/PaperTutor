Generative Topological Networks
AlonaLevy-Jurgenson1,ZoharYakhini1
1ReichmanUniversity
Correspondence:levyalona@gmail.com
Abstract from a Gaussian distribution to the data distribution, per-
formedovermultipletime-stepsknownasthereversediffu-
Generative models have seen significant advancements in
sion process. The complexity of this process also makes it
recent years, yet often remain challenging and costly to
challengingtoproduceinterpolations–desirableinthecon-
trainanduse.WeintroduceGenerativeTopologicalNetworks
(GTNs) – a new class of generative models that addresses text of continuous data like audio and video. Additionally,
these shortcomings. GTNs are trained deterministically us- diffusion-based methods often generate samples that fall
ingasimplesupervisedlearningapproachgroundedintopol- outsidethedatadistribution(e.g.thehorizontally-flipped3s
ogy theory. GTNs are fast to train, and require only a sin- in (Sohl-Dickstein et al. 2015b) Figure App.1.). Although
gle forward pass in a standard feedforward neural network recent advancements have made significant steps towards
togeneratesamples.WedemonstratethestrengthsofGTNs enhancing diffusion models, the complex diffusion process
inseveraldatasets,includingMNIST,celebAandtheHands
isstillakeychallengingcomponent(Manduchietal.2024).
andPalmImagesdataset.Finally,thetheorybehindGTNsof-
In this work we introduce a particularly efficient, stable
fersinsightsintohowtotraingenerativemodelsforimproved
andsimpleclassofgenerativemodels–GenerativeTopolog-
performance.Codewillbepublisheduponacceptance.
icalNetworks(GTNs).GTNslearnacontinuousandinvert-
iblemappingbetweentwodistributionsinasupervisedman-
1 Introduction
ner that facilitates efficient training and sampling. Specif-
DeepgenerativemodelssuchasGenerativeAdversarialNet- ically, given a training set of samples (e.g. images) and a
works (GANs) (Goodfellow et al. 2020), Variational Au- tractable source distribution (e.g. Gaussian), GTNs learn a
toencoders (VAEs) (Kingma and Welling 2013), Energy- functionhˆ suchthat,givenaysampledfromthesourcedis-
Based Models (EBMs) (LeCun et al. 2006; Ngiam et al. tribution,hˆ(y)isasamplerepresentingthetrainingdatadis-
2011), autoregressive models (Bengio and Bengio 2000;
tribution.GTNsarereminiscentofNFs,whichaimtotrans-
Larochelle and Murray 2011), normalizing flows (NFs)
form one distribution into another using a sequence of in-
(RezendeandMohamed2015),anddiffusionmodels(Sohl-
vertible and differentiable maps. GTNs, however, are con-
Dickstein et al. 2015a), have demonstrated remarkable ca-
structedentirelydifferently,andoffermanyadvantagesfrom
pabilitiesforgeneratingsamplesbasedontrainingdatadis-
bothpracticalandtheoreticalperspectives.
tributions (Kang et al. 2023; Ho, Jain, and Abbeel 2020;
Fromapracticalperspective,GTNsareextremelysimple
Ramesh et al. 2021; Ho et al. 2019; Kingma and Dhariwal
andefficient,requiringasingle,vanilla,feedforwardarchi-
2018;Kingmaetal.2016;Reedetal.2017;VanDenOord,
tecture trained using supervised learning. This means that
Vinyalsetal.2017;Rameshetal.2021).
GTNs avoid the complexities of introducing stochasticity
Diffusionmodels,inparticular,haveseengreatadvance-
during training which is present in VAE and diffusion; cir-
ments over the last few years in a variety of settings (Ho,
cumvent the intricacies of training more complex architec-
Jain,andAbbeel2020;Luoetal.2023;NicholandDhariwal
turessuchasthoseemployedbydiffusionandGANs;arenot
2021;DhariwalandNichol2021;Hoetal.2022;Rombach
alikelihood-basedapproach,asareVAE,diffusionandNFs;
et al. 2022), and possess several advantages compared to
anddonotposeconstraintsonthestructureoftheneuralnet-
othermethods.Forexample,comparedtoGANsandVAEs
work,asdoNFs.ThesupervisedmannerinwhichGTNsare
diffusionmodelshavemoretrainingstabilityastheydonot
trainedalsoformsastronginductivebiasthatspeedsupcon-
suffer from mode collapse like GANs (Zhang, Li, and Yu
vergence.GTNsalsooperateonlatentrepresentationsofthe
2018),orposteriorcollapselikeVAEs(Lucasetal.2019).
data, providing even more enhanced training and sampling
However,diffusionmodelshaveseveraldownsides.Most
efficiency. These advantages manifest in our experiments –
prominent is their inferior time-efficiency during training
realistic samples are obtained at early training epochs, and
and sampling when compared to VAEs and GANs. This
generatingasingleimagetakesaround4millisecondsona
is because they employ a gradual and stochastic transition
standardPCand0.003millisecondsonaT4GPU.
Copyright©2025,AlonaLevy-JurgensonandZoharYakhini.All Fromatheoreticalperspective,GTNsprovideguarantees
rightsreserved. and properties that are desirable in the context of genera-
4202
nuJ
12
]GL.sc[
1v25151.6042:viXraFigure1:SamplesgeneratedfromastandardnormaldistributionusingaGTNtrainedonCelebA64×64withalatentdimension
of100.
tive models. These include: learnability (via the universal distribution.Onesolutionwouldbetosamplefromaknown
approximationtheorem),continuity(forcontinuousinterpo- andtractabledistribution,suchasthestandardnormaldistri-
lations – Figure 5), bijectivity (for diversity and coverage bution,andthentoapplyafunctionthatmapsthissampleto
ofthedatadistribution–Figure3)andpropertiesthatserve a corresponding sample from X. Diffusion models attempt
asguidingtopologicalprinciplesonhowtotraingenerative to approximate such a mapping through gradual stochastic
modelsforbetterperformance(seetheMethodsectionand manipulationsofthestandardnormalsamplebacktoasam-
theswiss-rollexample). ple from X. We will show that, under certain general con-
Theremainderofthepaperisstructuredasfollows:Inthe ditions,suchamappingcanbeexplicitlydefined,providing
MethodsectionwefirstdevelopthetheorybehindGTNsfor a simple deterministic function which we will denote as h
the1-dimensionalcaseanduseittogeneratesamplesfrom (andwhichweillustrateinFigure2).Wewillshowthathis
the swiss-roll. We then extend this theory to higher dimen- in fact a homeomorphism – it is continuous, invertible and
sions and use it to generate samples from the multivariate has a continuous inverse (see Definition 2.1). This has sig-
uniform distribution. In the Experiments section we apply nificantimplications,aswewillsoonexplain.
GTNstorealdatasets,alsoincludingdemonstrationsofcon-
Definingh. Inthissectionwewilldefineafunctionhthat
tinuousinterpolation.WefollowwiththeRelatedWorksec-
transformsonedistributionintoanother(illustratedinFigure
tionandconcludewiththeDiscussionsection.
2).Wewillalsoprovethath,underfairlygeneralconditions,
possessescertainpropertiesthataredesirableinthecontext
2 Method
ofgenerativemodelsbyprovingthatitisahomeomorphism
We first develop the method in the simple case of a 1- (Theorem2.1).Forexample,wewillseethathisbijective,
dimensional space, and proceed to generalize to higher di- so that each sample y is mapped precisely to one sample
mensions. x =h(y),generatingdifferentsamplesfordifferenty,and
y
guaranteeing that each sample x has a sample y that gen-
2.1 1-Dimension
eratesit.Wewillalsodiscussotherusefulconsequencesof
ConsideracontinuousrandomvariableX withvaluesinR. hbeingahomeomorphism.Webeginbydefiningahomeo-
We wish to generate samples from X without knowing its morphisminourcontextandproceedtodefinehinTheoremNormal
Uniform
Figure 2: Illustration of the mapping produced by h and of the labeling process for training its approximation hˆ, where Y is
normallydistributedandX isuniformlydistributed.Apointyfromthenormalsampleislabeledwiththeuniquepointx from
y
theuniformsamplethathasthesameempiricalCDFvalueasy.
2.1. mationtheoremtheycanbeapproximatedtoarbitraryaccu-
Definition2.1 (Homeomorphism for Rn) Let S,T be two racybyaneuralnetwork(Hornik,Stinchcombe,andWhite
subsetsofRn.Afunctionh : S → T isahomeomorphism 1989).ThisisdemonstratedinFigure3(A).
if: (1) h is continuous; (2) h is bijective; (3) h−1 is con- 2. Coverage and diversity – The bijectivity of a home-
omorphism means that there is a one-to-one and onto cor-
tinuous. When such an h exists, then S and T are called
respondencebetweenthesupportsS andS .Thismeans
homeomorphic. X Y
thatwecanuseY tocoverallsamplesthatcanbeobtained
Theorem2.1 LetX andY berandomvariablesthathave fromXandthatnotwosamplesinY willgeneratethesame
continuous probability density functions (pdfs) f X, f Y and sampleinX.ThisisdemonstratedinFigure3(A)and(B).
supports S , S that are open intervals in R. Denote the
X Y 3. Continuous interpolation – The fact that h is con-
correspondingcumulativedistributionfunctions(CDFs)as
tinuous is significant for purposes of interpolation. For ex-
F X andF Y.Define: ample, given a generative model g : R → S , two points
X
h:S Y →S X
(1)
y ϕ1, :y [02 ,∈ 1]R →,an Rd )th we ef wun oc ut li don l: ikϕ e(λ (g)= ◦ϕλ )y (1 λ+ )t( o1− beλ c) oy n2 t( iw nuh oe ure
s
h(y)=F |−1(F | (y))
X SX Y SY (e.g. for video generation). If g is stochastic, for example,
this cannot be guaranteed. However, using h as g, we have
Then:(1)hiswell-defined;(2)hisahomeomorphism.
thatg◦ϕiscontinuousasacompositionofcontinuousfunc-
NotethattherequirementthatS X,S Y areopenintervals tions.Thisprovidesthedesiredcontinuousinterpolationbe-
canbeadaptedtoaunionofopenintervals–seeAppendix tweenpointsfromX.ThisisdemonstratedinFigure5.
A.3.TheproofofTheorem2.1isinAppendixA.1.
4. Guiding topological properties – There are useful
Thesimplespecialcaseoff ,f >0inRillustratesthe
X Y propertiesthatareinvariantunderhomeomorphisms(”topo-
keyideasofTheorem2.1.Inthisspecialcasewehavethat:
logical properties”) that can guide us in designing better
generativemodels.Forexample,onepropertyisthathome-
h:R→R omorphic manifolds must have the same dimension. The
(2) useofthispropertyisdemonstratedunderExample:Swiss-
h(y)=F−1(F (y))
X Y RollParameter,whereitwillleadustotheconclusionthat
Forsimplicityofexposition,wecontinuewiththisspecial we are better off generating swiss-roll samples from a 1-
casewheref ,f >0onallofR,butnotethatadaptingthe dimensional standard normal distribution, rather than from
X Y
resultstothegeneralcaseisamatteroftechnicality.Namely a2-dimensionalone.Anotherexampleforausefulproperty
– f ,f > 0 on S ,S respectively. Combined with S isinAppendixA.3.
X Y X Y X
andS beingopenintervals,weconcludethattherestricted
CDFsY F | ,F | are continuous and strictly monoton- Learning h Using a Neural Network hˆ and Generating
icallyinX creS aX singY onSY S ,S ,whichisthemainobservation Sampleswithhˆ. Asexplainedintheprevioussubsection,
X Y
neededinordertogeneralize. oneconsequenceofhbeingahomeomorphismisthatitcan
be approximated by a feedforward neural network hˆ. If we
The Significance of h Being a Homeomorphism. The
hadknownbothF−1andF thenwewouldbeabletoeasily
factthathisahomeomorphismissignificantinthecontext X Y
generatelabelsforeachy ∈Y:denotingthelabelofagiven
ofgenerativemodelsforseveralreasons:
y ∈Y asx ,wewouldsetx =h(y),meaning:
1. Learnability – Since h being a homeomorphism im- y y
plies that both h and h−1 are continuous real-valued func-
tionsoversomesubsetofR,thenbytheuniversalapproxi- x =h(y)=F−1(F (y)) (3)
y X Yĥ
ĥ
A B
ĥ ĥ
Normal Swiss-Roll Normal Uniform
Figure3:(A)TestresultsforaGTNhˆ trainedtomapfromY ∼N(0,1)totheswiss-rollparameter.Thecolorindicateswhich
point in the normal sample was mapped to which point in the swiss-roll. (B) Test results for a GTN hˆ trained to map from
Y ∼ N(0,I)toX ∼ U(cid:0) (0,1)×(0,1)(cid:1) .Thecolorisbasedonthenormalsample,i.e.foreachy inthenormalsample,hˆ(y)
has the same color as y so that the figure on the right shows how the normal sample was stretched to a uniform distribution
(whichisinlinewithEquation4).
AlthoughwedonotknowF ,F ,wedohaveaccessto 1-dimensional standard normal distribution to obtain D .
X Y Y
samples from X and Y which we can use to approximate Welabeledeachy ∈ D withitsx ∈ D asdefinedin
i Y yi X
x . From Eq. 3 we see that x satisfies F (x ) = F (y), theprevioussection.Weusedastandardfeedforwardneural
y y X y Y
meaningx y istheuniquex∈X thatmatchesthepercentile networkashˆ (3layersofwidth5withReLUactivationand
ofy.Wecanusethisobservationtoapproximatex y byre- batchnormalization)andMSEasthelossfunction.Figure3
placingF
X
andF
Y
withtheempiricalCDFsasfollows. (A)showstheresultoftestingthetrainedmodelhˆonasetof
Let D := {x ,...,x } and D := {y ,...y } be n
X 1 n Y 1 n new samples y ,...,y drawn from N(0,1) (each point is
1 k
observed values of X and Y, correspondingly. Labels are obtainedbypredictingθˆ :=hˆ(y )andapplyingtheformula
obtainedeasilybysortingD andD toobtainDsortedand i i
X Y X fortheswissrolltoθˆ).
Dsortedandlabelingeachy ∈Dsortedwiththecorresponding i
Y Y Forcomparison,wereferthereadertoFigure1(middle)
x∈Dsorted(i.e.assumingD ,D aresorted,y isassigned
X X Y i in (Sohl-Dickstein et al. 2015a) where a trained diffusion
x ).ThisisillustratedinFigure2.
i model was applied to points sampled from a standard 2-
ThelossfunctionistheMSE:
dimensional normal distribution. The points are gradually
n1 (cid:88)n
||hˆ(y i)−x yi||2
a ifn od lds ,to bc uh ta ts ht eic ra el sly ulm
t
co ov ne td aib na sc pk ot io nw tsa tr hd as tth fae llsw ouis tss i- dro ell ofm ta hn is-
i=1 manifold. With hˆ, the generated points lie entirely within
GeneratingsamplesfromX isnowstraightforward–we thedatamanifold.Infact,thisholdstrueearlyinthetrain-
sampley ∈Y andcomputehˆ(y). ingprocess,asshowninAppendixFigure6.
Besides demonstrating that hˆ can serve as an accurate
Example: Swiss-Roll Parameter. In this example, we
generative model, this example also emphasizes an impor-
willseethatsamplesfromtheswiss-rollcanbeeasilygen-
tantpoint–namely,thatifwewanttolearnsuchahomeo-
eratedwithvisiblynear-perfectaccuracy(Figure3(A))us- morphismh,wemightneedtoreducethedimensionalityof
ing this method. This would not be as easy using a 2-
thedataifitisnotalreadyinitsintrinsicdimension(forthe
dimensionalnormaldistribution(theapproachtakenbypre- swiss-rollthiswas1).Thisobservationwillguideusinlater
viousmethodssuchas(Sohl-Dicksteinetal.2015a)),aswe
sections.
willnowexplain.
The swiss-roll is a 1-dimensional manifold in a 2- 2.2 HigherDimensions
dimensional space R2 (generated using a single, 1-
Togeneralizetomorethanonedimension,wereducetothe
dimensional, continuous random variable θ). Being a 1-
1-dimensional case by considering lines that pass through
dimensional manifold, it cannot be homermorphic to the
support of a 2-dimensional normal distribution (R2) since theorigin.Briefly,wetaketherandomvariablesobtainedby
restrictingX andY toeachline,andapplythehomeomor-
homeomorphic manifolds must have the same dimension.
phism defined for the 1-dimensional case to these random
Therefore,therewouldbenohopeoflearningahomeomor-
variables.Webeginwithaformalsetupthatisverysimilar
phismthatmapsa2-dimensionalGaussianrandomvariable
tothe1-dimensionalcase.
to the swiss-roll. However, the swiss roll is homeomorphic
toR–thesupportofa1-dimensionalstandardnormaldis-
Definingh. LetX =(X ,...,X ),Y =(Y ,...,Y )be
1 d 1 d
tribution. In fact, since θ is a random variable that satisfies
multivariate random-variables with continuous joint prob-
theconditionsinTheorem2.1,itishomeomorphictoRvia
ability density functions f , f , and with supports S ,
h,whichwecanlearnusinghˆ. S , each of which is a prX oducY t of d open intervals inX R
Y
To train hˆ, we created a dataset of n = 50,000 sam- (see Appendix A.3. to accommodate for other supports).
ples from θ, denoted D . We sampled n samples from a Forexample,X couldbeuniformlydistributedwithsupport
XS = (0,1)×(0,1) and Y could be normally distributed Algorithm1:Labeling
X
withsupportS Y =R×R. Input:
Fordistributionsthataresymmetricabouttheorigin(e.g. D ={x ,...,x }
X 1 n
standard normal), we can define the following homeomor- D ={y ,...,y }sampledfromN(0,I)
Y 1 n
phism(weassumeforsimplicitythatS Y,S X =Rd): Output:res
h:Rd →Rd 1: res←[]
h(y)=(cid:40)
h 1(||y||) ||y y||, y ̸=0 (4)
2 3:
:
D whXso ir lt eed D,D YsoYs ro ter dted ̸=← ∅:sortD X,D Y ascendingby||·|| 2
0, y =0 4: y ←Dsorted[0]
Y
5: x y ←argmax x∈Dsortedcosine sim(x,y)
whereh : (0,∞) → (0,∞)isthe1-dimensionalhome- X
1 6: Dsorted ←Dsorted[1...n]
omorphismappliedtotherandomvariables||Y||,||X||. Y Y
We prove that this is indeed a homeomorphism in Ap-
7: D Xsorted ←D Xsorted\{x y}
(cid:0) (cid:1)
pendix A.5. Note that this generalizes the 1-dimensional 8: res.append (y,x y)
casesinceford=1wegety (cid:55)→h (y).Intuitively,hcanbe 9: returnres
1
seenasshrinkingy totheunitvectorinthesamedirection,
and stretching(or shrinking it to reach the resulting x. This Algorithm2:Sampling
stretchingandshrinkingoccursonthesamelinethroughthe
Input:hˆtrainedonres(seeAlg.1)
origin,sharedbybothxandy,whereh guaranteesthatthe
1
resultingxhasthesamequantileasyonthisline(i.e.when 1: y ← samplefromN(0,I)
measured withrespect to therandom variables obtained by 2: returnhˆ(y)
restrictingY,X tothisline).Anotherwayofthinkingabout
thisisprovidedinAppendixA.6.
Learning h. We now turn to our labeling approach. Let tinuous interpolations of the form described in the Method
D andD beobservedvaluesfromX andY.Ideally,we section.ThisadvantageislaterdemonstratedinFigure5.
X Y
would use the 1-dimensional labeling scheme on each line
Example: 2-Dimensional Uniform Distribution. Figure
that passes through the origin by using h on both sides of
1
3(B)showsasamplegeneratedusingAlgorithm2afterap-
theline.Inpractice,however,theprobabilitythatanygiven
plying the multivariate method to the multivariate uniform
linecontainspointsfromeitherD orD isnegligent.We
X Y
distribution X ∼ U(0,1) × U(0,1). Specifically, we cre-
therefore approximate this labeling approach using cosine-
ated D by sampling n = 100,000 points from X, and
similarity. Specifically, we match y with the x that is clos- X
created D by sampling the same number of points from
estintermsofbothcosine-similarity(sothatbothy,x are Y
y
N(0,I). We then applied Algorithm 1 to D ,D to gen-
as close as possible to being on the same line and in the X Y
eratelabeleddata.Wetrainedastandardfeedforwardneural
same direction from the origin) and distance from the ori-
networkhˆ (5hiddenlayersofwidth6withReLUactivation
gin(toproducetheeffectofh (||y||)).Thisisformallyde-
1
andbatchnormalization(IoffeandSzegedy2015))andused
scribed in Algorithm 1. To see why Algorithm 1 approxi-
it to generate new samples according to Algorithm 2. The
mates h, consider a line with an infinite sample from both
modelwastraineduntilconvergenceonaseparatelygener-
X and Y in both directions from the origin. The maximal
atedvalidationsetwithn=10,000.Thecolorsinbothim-
cosine-similarity over all x ∈ D and a given y on this
X
agesinFigure3(B)arebasedonthedistanceofthepoints
line is 1. Since the algorithm returns as x an x with max-
y
from the origin in the Gaussian sample (the left image in
imum cosine-similarity, then x will indeed be on the line
y
(B)),sothattherightimagereflectswhereeachpointinthe
and in the same direction as y. Since there are infinitely
manysuchcandidatesxwithcosine-similarity1,itremains
Gaussianwaspredictedto’move’tobyhˆ.
to choose the particular x whose distance from the origin
best approximates h (||y||). Since both D and D are 3 Experiments
1 X Y
sorted in ascending order by distance from the origin, and
Intheprevioussection,wedemonstratedhowtolearnhˆ on
since matched pairs (x ,y) are always removed, the algo-
y
syntheticdata–theswiss-rollandthemultivariateuniform
rithm will indeed return the particular x from this pool of
distribution.Inthissection,wewilldemonstratehowtoap-
candidateswhosedistancefromtheoriginwillhaveasimi-
larempiricalquantiletothatof||y||.Wefurtherelaborateon plyhˆ toimages.
Algorithm1inAppendixA.2. In light of the discussion in Section 2.1 on using the in-
UsingthelabeledsamplesfromAlgorithm1,wetrainhˆ– trinsicdimensionofthedata(orlower),weuseautoencoders
torepresenteachdatasetinalowerdimensionallatentspace
afeedforwardneuralnetwork–usingMSEasthelossfunc-
tion. We then use hˆ to generate new X instances just as in
beforetraininghˆ.Eachdatasetwastrainedseparately(with
the 1-dimensional case and as is formally described in Al- its own autoencoder and hˆ). In all experiments, we use the
gorithm2. same autoencoder architecture – a simple vanilla autoen-
Notethattherequirementofrespectingthe1-dimensional coder (the encoder consisted of 2 convolution layers with
homeomorphisms on each line facilitates the desired con- ReLUactivationfollowedbyafullyconnectedlayer).ForhˆEpoch 1 Epoch 5 Epoch 11 Epoch 21 (last) REAL
Figure4:RandomlychosensamplesgeneratedbyGTNduringthetrainingprocessonMNISTatfourpointsofimprovement
onthevalidationset.Eachepochshows100images,eachofwhichisthedecodedhˆ(r)forsomerandomr ∼ N(0,I)).The
rightmostgridcontains100imageseachofwhichisthedecodedlatentvectorofarandomrealimage.
weuseasimplefeedforwardneuralnetworkwiththewidth age) on a T4 GPU. We used a latent dimension of 100 in
and depth depending on the data (as reported below) and the autoencoder. For hˆ we used width 1200 and depth 25.
using batch normalization and ReLU activation after each Weusedacentercropof148×148andresizedtheimages
layerexceptfortheoutputlayer. to64×64.
For hˆ, we set X to be the latent vectors of the train-
Interpolation. WenextusedtheCelebAdatasetaswellas
ing set (the encoded images), and set Y to be the stan-
the Hands and Palm Images (HaP) dataset (Kaggle. 2024)
dard multivariate normal distribution of the same dimen-
to demonstrate that GTNs indeed provide continuous in-
sion as the latent space (e.g., in CelebA we used a latent
terpolations. The HaP dataset is a small dataset contain-
spaceof 100 inthe autoencoder,so Y hasdimension 100).
ing11,000imagesofupward-facinganddownward-facing
Images were generated using Algorithm 2 by computing:
autoencoder.decoder(cid:0) hˆ(r)(cid:1)
where r ∼ N(0,I) (genera-
hands. We chose the HaP dataset since generating hands
is known to be notoriously difficult for generative models
tionoccursinthelatentspace).
(TheNewYorker.2023;Vox.2023),letaloneinterpolating
Figure 4 shows random sets of generated images for
betweenthem.
MNISTatseveralepochsofimprovementonthevalidation
Figure5showsthatGTNindeedgeneratescontinuousin-
set,aswellasasampleofrandomrealimagesforcompar-
terpolations for both CelebA and HaP. Each grid contains
ison. These results show that the generated samples were
generatedinterpolationsbetweentwogeneratedimages(top
already realistic even after one epoch (a similar observa-
left and bottom right in each grid). More precisely, each
tion was also made in the swiss-roll example). We use a
grid contains the results of decoding
hˆ(cid:0)
λy + (1 −
latent dimension of 5 to allow for comparison with VAE topleft
(cid:1)
for the same latent dimension (Figure 5(b) in (Kingma and λ)y bottomright for100linearlyspacedλ ∈ [0,1]whereboth
Welling2013)).Furthercomparisonscanbemadewithdif- y topleftandy bottomrightweresampledfromN(0,I).
fusion(FigureApp.1.in(Sohl-Dicksteinetal.2015b))anda It is worth noting an interesting observation for the HaP
recentcontinuousnormalflow(CNF)(Figure3(a)(right)in data results – the interpolations in Figure 5 are between
(HuangandYeh2021)).Forhˆ weusedwidth50anddepth handsofthesameorientation(downwardfacing).However,
generated interpolations between hands (downward facing)
5. Training was stopped when no improvement in the vali-
and palms (upward-facing) do not seem as natural (Ap-
dationMSEwasobtainedfor3epochs.
pendix Figure 9). This is likely because the dataset does
NextweappliedthemethodtoCelebA(Figure1andAp-
not contain in-between positions for the latter (transitions
pendixFigure7).AppendixFigure7alsoshowstheprogress
between downward and upward), but it does contain in-
in image generation during training, with randomly gen-
betweenpositionsfortheformer(closed/openfingerstovar-
erated images at epochs of improvement in the Inception
iousdegrees).Thisdemonstratesthat,regardlessofmethod-
Score (IS) (Salimans et al. 2016). We used IS as the stop-
ology,thecompletenessofthedatasetisimportantforaccu-
pingcriteria,asexplainedinAppendixA.4.Oneepochtook
rateimagegeneration,andparticularlyforinterpolation.
just under 1 minute (50.6 seconds on average) on a single
T4 GPU, reflecting 9 hours until the last improvement in
4 Relatedwork
ISatepoch640.ObservingAppendixFigure7,wecansee
thatrealisticimageswerealreadyobtainedathalfthistime GTNs are related in concept to NFs which seek to map
(see epoch 276). Appendix Figure 8 contains a plot of the a source distribution into a target distribution using a se-
IS by epoch showing that the true IS (the IS score for a quenceofbijectivetransformations.Thesearetypicallyim-
random set of real decoded samples) is nearly achieved by plementedbyaneuralnetwork,oftenintheformofatleast
theGTN.Thetimetogenerate1,000samples(i.e.tocom- tensofneuralnetworkblocks(XuandCampbell2024)and
pute:
autoencoder.decoder(cid:0) hˆ(r)(cid:1)
) on a standard PC was sometimesmore(Xu,Chen,andCampbell2023).Thecore
4.07 seconds (reflecting 4.07 milliseconds per image) and of NFs is based on each of these transformations being a
0.00367 seconds (reflecting 0.00367 milliseconds per im- diffeomorphism–anotionsimilartoahomeomorphismbutFigure5:InterpolationsproducedbyatrainedGTNforCelebA(top)andtheHaPdataset(bottom).
more constrained as it is defined between smooth mani- in NFs. While CNFs have shown improvements over NFs
folds (as opposed to any topological spaces, including any in certain cases , they are extremely time-intensive since
manifolds) and requires that the function and its inverse theyrequiresolvingaseriesofODEs,equivalenttorunning
are differentiable (as opposed to just continuous for home- a neural network with hundreds of layers (Huang and Yeh
omorphisms). NFs also require that the log-determinant of 2021).ImprovingboththespeedandperformanceofCNFs
the Jacobian of these transformations is tractable, posing isanactivefieldofresearch.Tocomparebetweenarecently
a constraint on the model architecture. GTNs do not pose suggestedCNFandGTNseetheExperimentssection.
any limitations on the model architecture. NFs also differ
fromGTNsintheiroptimizationmethodsinceNFsemploy 5 Discussion
a maximum likelihood objective – typically the Kullback-
Leibler(KL)divergencewhileGTNsaretrainedusingMSE. In this work we introduced a new class of generative mod-
It was observed that optimizing the KL-divergence may be els – GTNs – that learn a homeomorphism h between the
difficultformorecomplexnormalizingflowstructures(Xu, supportsoftwocontinuousrandomvariablesusingsamples
Chen,andCampbell2023).NFsmayalsosufferfromlow- from both. The samples are used in an efficient algorithm
qualitysamplegeneration(Behrmannetal.2021),partially to create a training dataset for the GTN which is trained in
becausetheconstraintontheJacobian,besideslimitingthe a simple supervised approach, with no restrictions on the
model structure, may also lead to issues such as spurious model architecture. The freedom to use a standard neural
localminimaandnumericalinstability(Golinski,Lezcano- networkarchitecture,andthefactthathsatisfiesthecondi-
Casado,andRainforth2019). tionsoftheuniversalapproximationtheorem,leadGTNsto
quickly converge towards h, providing an efficient method
Continuous normalizing flow (CNF) are a relatively re- togeneratesamplesfromatargetrandomvariable.Thiswas
centtypeofNFsthatweredevelopedtoavoidthemaincon- demonstratedexperimentallyinmultiplecases,particularly
straints posed by NFs – namely the requirements of invert- inthatgeneratedsamplesreachedthedatamanifoldatearly
ibility and a tractable Jacobian determinant. To avoid these epochs and in the total time to convergence. Other positive
constraints,CNFsuseODEstodescribethetransformations implications of h being a homeomorphism – namely, cov-erage and diversity, continuous interpolations and guiding modelfordrugdesignfromproteintargetsequence.Journal
topological properties – were also discussed and demon- ofCheminformatics,15(1):38.
stratedexperimentally. Corso,G.;Sta¨rk,H.;Jing,B.;Barzilay,R.;andJaakkola,T.
One downside of our method is that it assumes that the 2022.Diffdock:Diffusionsteps,twists,andturnsformolec-
data is in its intrinsic dimension (or lower). This may re- ulardocking. arXivpreprintarXiv:2210.01776.
quire use of dimensionality reduction methods, such as the
Dhariwal, P.; and Nichol, A. 2021. Diffusion models beat
vanillaautoencodersemployedinthiswork.Futureworkfo-
gans on image synthesis. Advances in neural information
cusedonhighresolutionimagesmaybeinterestedinexper-
processingsystems,34:8780–8794.
imenting with stronger autoencoders. Additionally, the as-
Golinski,A.;Lezcano-Casado,M.;andRainforth,T.2019.
sumptionsonthesupportofthedistributionsmeanthatour
Improving normalizing flows via better orthogonal param-
methoddoesnotimmediatelyapplytodiscretedistributions,
eterizations. In ICML Workshop on Invertible Neural Net-
andthatdatasetsmayneedtobesplitandtrainedonindivid-
worksandNormalizingFlows.
ually, as explained in Appendix A.3. This also means that
largerdatasetsmayberequiredforGTNstoperformwell. Goodfellow, I.; Pouget-Abadie, J.; Mirza, M.; Xu, B.;
Warde-Farley, D.; Ozair, S.; Courville, A.; and Bengio, Y.
6 BroaderImpact 2020. Generativeadversarialnetworks. Communicationsof
theACM,63(11):139–144.
Our work falls under the category of deep generative mod-
els, and is similar in impact to other studies seeking to im- Ho,J.;Chen,X.;Srinivas,A.;Duan,Y.;andAbbeel,P.2019.
prove these methods. Since our work provides an efficient Flow++:Improvingflow-basedgenerativemodelswithvari-
method to generate realistic samples, it may contribute to ational dequantization and architecture design. In Interna-
malicioususessuchasgeneratingfakenews,videosofhigh tionalconferenceonmachinelearning,2722–2730.PMLR.
profile figures etc. Fortunately, there are methods to detect Ho, J.; Jain, A.; and Abbeel, P. 2020. Denoising diffusion
suchgenerateddata,andthereisongoingworktocontinue probabilistic models. Advances in neural information pro-
to improve in doing so (Wang et al. 2020). There are also cessingsystems,33:6840–6851.
concernsofperpetuatingbiasesinherenttothedataset,par- Ho,J.;Salimans,T.;Gritsenko,A.;Chan,W.;Norouzi,M.;
ticularlysinceourmethodessentiallylearnsthedatadistri- andFleet,D.J.2022. Videodiffusionmodels. Advancesin
butionwithhighfidelity.Thereisagrowinglineofresearch NeuralInformationProcessingSystems,35:8633–8646.
ondetectingandhandlingbiasesindatasets,andwebelieve
Hoogeboom, E.; Satorras, V. G.; Vignac, C.; and Welling,
there is an increasing awareness among data curators and
M. 2022. Equivariant diffusion for molecule generation in
usersofthisproblem.Moreresearchwouldbenecessaryin
3d. InInternationalconferenceonmachinelearning,8867–
ordertoalleviatethisproblem.
8887.PMLR.
Although malicious uses are indeed a concern, genera-
Hornik, K.; Stinchcombe, M.; and White, H. 1989. Mul-
tivemodelscouldhavefarreachingpositiveimplicationson
tilayer feedforward networks are universal approximators.
our lives. Perhaps the most prominent example is the field
Neuralnetworks,2(5):359–366.
of healthcare, where generative models have found use in
generatingdigitaltwinsforimprovedpersonalizedmedicine Huang,H.-H.;andYeh,M.-Y.2021. Acceleratingcontinu-
(Bordukova et al. 2024; Tsialiamanis et al. 2021), medi- ousnormalizingflowwithtrajectorypolynomialregulariza-
calimaging(Kazerounietal.2023;Moghadametal.2023; tion. In Proceedings of the AAAI Conference on Artificial
Pinayaetal.2022)anddrugdesign(Korablyovetal.2024; Intelligence,volume35,7832–7839.
Chen et al. 2023; Hoogeboom et al. 2022; Schneuing et al. Ioffe,S.;andSzegedy,C.2015.Batchnormalization:Accel-
2022;Corsoetal.2022). eratingdeepnetworktrainingbyreducinginternalcovariate
shift.InInternationalconferenceonmachinelearning,448–
References 456.pmlr.
Behrmann, J.; Vicol, P.; Wang, K.-C.; Grosse, R.; and Ja- Kaggle. 2024. Hands and Palm Images Dataset.
cobsen,J.-H.2021.Understandingandmitigatingexploding https://www.kaggle.com/datasets/shyambhu/hands-and-
inversesininvertibleneuralnetworks. InInternationalCon- palm-images-dataset. Accessed:2024-05-15.
ferenceonArtificialIntelligenceandStatistics,1792–1800. Kang, M.; Zhu, J.-Y.; Zhang, R.; Park, J.; Shechtman, E.;
PMLR. Paris, S.; and Park, T. 2023. Scaling up gans for text-to-
Bengio, S.; and Bengio, Y. 2000. Taking on the curse of image synthesis. In Proceedings of the IEEE/CVF Confer-
dimensionality in joint distributions using neural networks. ence on Computer Vision and Pattern Recognition, 10124–
IEEETransactionsonNeuralNetworks,11(3):550–557. 10134.
Bordukova, M.; Makarov, N.; Rodriguez-Esteban, R.; Kazerouni, A.; Aghdam, E. K.; Heidari, M.; Azad, R.;
Schmich,F.;andMenden,M.P.2024. Generativeartificial Fayyaz,M.;Hacihaliloglu,I.;andMerhof,D.2023. Diffu-
intelligence empowers digital twins in drug discovery and sion models in medical imaging: A comprehensive survey.
clinical trials. Expert Opinion on Drug Discovery, 19(1): MedicalImageAnalysis,102846.
33–42. Kingma, D. P.; and Dhariwal, P. 2018. Glow: Generative
Chen, Y.; Wang, Z.; Wang, L.; Wang, J.; Li, P.; Cao, D.; flow with invertible 1x1 convolutions. Advances in neural
Zeng, X.; Ye, X.; and Sakurai, T. 2023. Deep generative informationprocessingsystems,31.Kingma, D. P.; Salimans, T.; Jozefowicz, R.; Chen, X.; Rezende,D.;andMohamed,S.2015. Variationalinference
Sutskever, I.; and Welling, M. 2016. Improved variational withnormalizingflows. InInternationalconferenceonma-
inferencewithinverseautoregressiveflow.Advancesinneu- chinelearning,1530–1538.PMLR.
ralinformationprocessingsystems,29. Rombach,R.;Blattmann,A.;Lorenz,D.;Esser,P.;andOm-
Kingma,D.P.;andWelling,M.2013. Auto-encodingvaria- mer, B. 2022. High-resolution image synthesis with latent
tionalbayes. arXivpreprintarXiv:1312.6114. diffusionmodels. InProceedingsoftheIEEE/CVFconfer-
ence on computer vision and pattern recognition, 10684–
Korablyov, M.; Liu, C.-H.; Jain, M.; van der Sloot, A. M.;
10695.
Jolicoeur, E.; Ruediger, E.; Nica, A. C.; Bengio, E.;
Lapchevskyi,K.;St-Cyr,D.;etal.2024. GenerativeActive Salimans,T.;Goodfellow,I.;Zaremba,W.;Cheung,V.;Rad-
LearningfortheSearchofSmall-moleculeProteinBinders. ford,A.;andChen,X.2016. Improvedtechniquesfortrain-
arXivpreprintarXiv:2405.01616. ing gans. Advances in neural information processing sys-
tems,29.
Larochelle,H.;andMurray,I.2011. Theneuralautoregres-
sivedistributionestimator. InProceedingsofthefourteenth Schneuing, A.; Du, Y.; Harris, C.; Jamasb, A.; Igashov, I.;
internationalconferenceonartificialintelligenceandstatis- Du,W.;Blundell,T.;Lio´,P.;Gomes,C.;Welling,M.;etal.
tics,29–37.JMLRWorkshopandConferenceProceedings. 2022. Structure-based drug design with equivariant diffu-
sionmodels. arXivpreprintarXiv:2210.13695.
LeCun,Y.;Chopra,S.;Hadsell,R.;Ranzato,M.;andHuang,
F. 2006. A tutorial on energy-based learning. Predicting Sohl-Dickstein, J.; Weiss, E.; Maheswaranathan, N.; and
structureddata,1(0). Ganguli, S. 2015a. Deep unsupervised learning using
nonequilibrium thermodynamics. In International confer-
Lucas,J.;Tucker,G.;Grosse,R.;andNorouzi,M.2019.Un-
enceonmachinelearning,2256–2265.PMLR.
derstanding posterior collapse in generative latent variable
Sohl-Dickstein, J.; Weiss, E. A.; Maheswaranathan, N.;
models.
and Ganguli, S. 2015b. Deep Unsupervised Learning
Luo,Z.;Chen,D.;Zhang,Y.;Huang,Y.;Wang,L.;Shen,Y.;
using Nonequilibrium Thermodynamics. arXiv preprint
Zhao,D.;Zhou,J.;andTan,T.2023. Videofusion:Decom-
arXiv:1503.03585.
poseddiffusionmodelsforhigh-qualityvideogeneration.In
TheNewYorker.2023. TheUncannyFailureofA.I.-Gen-
ProceedingsoftheIEEE/CVFConferenceonComputerVi-
erated Hands. https://www.newyorker.com/culture/rabbit-
sionandPatternRecognition,10209–10218.
holes/the-uncanny-failures-of-ai-generated-hands. Ac-
Manduchi, L.; Pandey, K.; Bamler, R.; Cotterell, R.;
cessed:2024-05-15.
Da¨ubener,S.;Fellenz,S.;Fischer,A.;Ga¨rtner,T.;Kirchler,
Tsialiamanis,G.;Wagg,D.J.;Dervilis,N.;andWorden,K.
M.;Kloft,M.;etal.2024. OntheChallengesandOpportu-
2021. On generative models as the basis for digital twins.
nitiesinGenerativeAI. arXivpreprintarXiv:2403.00025.
Data-CentricEngineering,2:e11.
Moghadam, P. A.; Van Dalen, S.; Martin, K. C.; Lennerz,
Van Den Oord, A.; Vinyals, O.; et al. 2017. Neural dis-
J.; Yip, S.; Farahani, H.; and Bashashati, A. 2023. A mor-
crete representation learning. Advances in neural informa-
phologyfocuseddiffusionprobabilisticmodelforsynthesis
tionprocessingsystems,30.
ofhistopathologyimages. InProceedingsoftheIEEE/CVF
Vox.2023. WhyAIArtStruggleswithHands. https://www.
winterconferenceonapplicationsofcomputervision,2000–
youtube.com/watch?v=24yjRbBah3w. Accessed: 2024-05-
2009.
15.
Ngiam,J.;Chen,Z.;Koh,P.W.;andNg,A.Y.2011. Learn-
Wang, S.-Y.; Wang, O.; Zhang, R.; Owens, A.; and Efros,
ing deep energy models. In Proceedings of the 28th inter-
A.A.2020. CNN-generatedimagesaresurprisinglyeasyto
nationalconferenceonmachinelearning(ICML-11),1105–
spot...fornow. InProceedingsoftheIEEE/CVFconference
1112.
oncomputervisionandpatternrecognition,8695–8704.
Nichol, A. Q.; and Dhariwal, P. 2021. Improved denoising
Xu,Z.;andCampbell,T.2024. Embracingthechaos:anal-
diffusion probabilistic models. In International conference
ysis and diagnosis of numerical instability in variational
onmachinelearning,8162–8171.PMLR.
flows. AdvancesinNeuralInformationProcessingSystems,
Pinaya, W. H.; Tudosiu, P.-D.; Dafflon, J.; Da Costa, P. F.; 36.
Fernandez,V.;Nachev,P.;Ourselin,S.;andCardoso,M.J.
Xu,Z.;Chen,N.;andCampbell,T.2023.MixFlows:princi-
2022. Brainimaginggenerationwithlatentdiffusionmod-
pledvariationalinferenceviamixedflows. InInternational
els.InMICCAIWorkshoponDeepGenerativeModels,117–
ConferenceonMachineLearning,38342–38376.PMLR.
126.Springer.
Zhang,Z.;Li,M.;andYu,J.2018. Ontheconvergenceand
Ramesh, A.; Pavlov, M.; Goh, G.; Gray, S.; Voss, C.; Rad-
modecollapseofGAN. InSIGGRAPHAsia2018Technical
ford,A.;Chen,M.;andSutskever,I.2021.Zero-shottext-to-
Briefs,1–4.
image generation. In International conference on machine
learning,8821–8831.Pmlr.
A Appendix
Reed, S.; Oord, A.; Kalchbrenner, N.; Colmenarejo, S. G.;
A.1
Wang,Z.;Chen,Y.;Belov,D.;andFreitas,N.2017. Paral-
lelmultiscaleautoregressivedensityestimation. InInterna- 1. h is well-defined: we need to show that: (a) F | is
X SX
tionalconferenceonmachinelearning,2912–2921.PMLR. invertible,and(b)thattheimageofF | isinthedo-
Y SYmainofF | .For(a),observethatF | iscontinu- theISscorewasobtainedfor300epochs,butobservingthe
X SX X SX
ous,strictlymonotonicallyincreasing(theCDFisstrictly generatedsamplesbyepochabovedemonstratesthatearlier
monotonicallyincreasingonthesupport)andonto(0,1). stoppingmaysuffice.
A symmetric argument can be made for F | , which
wewilluselatertodefinetheinverseofh.
Y SY
A.5
For(b),weknowfrom(a)thattheimageofF | isthe h is injective as can be observed by taking the norm when
Y SY
domainofF | sohiswelldefined. equatingfortwodifferentpointsy ,y :
X SX 1 2
2. h is a homeomorphism since: a. it is continuous as a y y
compositionofcontinuousfunctions(F X−1|
SX
isthein- h 1(||y 1||)
||y
11
||
=h 1(||y 2||) ||y2
2||
verse of a continuous and strictly monotonous function
on an open interval in R); b. it is bijective with inverse which yields h (||y ||) = h (||y ||). Since h is injective,
1 1 1 2 1
F−1| (F | (x)); c. its inverse is continuous for the thismeansthat||y ||=||y ||.Pluggingthisintotheequality
Y SY X SX 1 2
samereasonsstatedina.Also,itisaknownresultthata andcancellingoutequalterms(y ,y ̸= 0)yieldsy = y .
1 2 1 2
continuous and bijective function between two open in- The function is also onto since h is onto. Particularly, for
1
tervalshasacontinuousinverse. any x ̸= 0 there is a value v in (0,∞) with h (v) = ||x||.
1
SincethesupportofY isRd thereisaythatisonthesame
A.2 lineasxfromtheoriginandthatsatisfies||y|| = v sothat:
To understand the rationale behind Algorithm 1, imagine x=h 1(||y||) ||yy ||.
four points from each of X and Y lying on the x-axis in h is continuous since for y ̸= 0 it is a composition of
R2 suchthatbothX andY havetwopointsoneachsideof continuous functions, and for y = 0 we can observe that
theorigin(weassumeD X,D
Y
arecentralized).Thecosine lim y→0h 1(||y||) ||y
y||
= 0 ( ||y
y||
is the unit vector in the di-
similarity of each pair of points {a,b} with a ∈ D X,b ∈ rectionofyandh 1(||y||)→0asy →0bydefinitionofthe
D Y is1iftheyareonthesamesideor−1oftheyareonop- CDF).
positesides.Takethefirsty ∈Dsorted (theclosestonetothe Theinverseisalsocontinuoussinceacontinuousandbi-
Y
originwhichreflects∼50thpercentilesinceweassumedan jectivefunctionsbetweenopensubsetsofRd iscontinuous
equal number of points from Y on both sides). The maxi- (bytheinvarianceofdomaintheorem).
mum cosine similarity out of all x ∈ Dsorted is 1, meaning
X
y willbematchedwithanxonthesameside.Thefactthat A.6
D issorted,meansthatweareusingthedistanceasatie
X Another way of thinking about this is as follows: assume
breaker – out of all x ∈ D X that are on the same side as for simplicity that f ,f > 0 (so that S ,S = Rd and
X Y X Y
y,thexthatisclosesttotheoriginwillbechosen(∼ 50th
therefore l ∩S and l ∩S are simply l). Since l is a 1-
X Y
percentile). Likewise, the leftmost y will be matched with
dimensionalmanifold,X andY induce1-dimensionalran-
theleftmostx(both0thpercentile),therightmostywiththe dom variables Xl and Yl on the line l, with pdfs f ,f
rightmostx(both100thpercentile)etc. Xl Yl
andCDFsF ,F .WecanapplyTheorem2.1totheseran-
Xl Yl
domvariablestoobtainahomeomorphismhl foreachline.
A.3
Asmentionedearlier,undercertainconditionsonX,Y,this
Oneexampleofatopologicalpropertyistheconnectedness processyieldsamultivariatehomeomorphism.
property – being a ”single piece”, like R, as opposed to
”morethanonepiece”,like(−∞,−1)∪(1,∞),ispreserved
betweenhomeomorphicspaces(so,inparticular,thesetwo
are not homeomorphic, but R is homeomorphic to each of
the two intervals, separately). Therefore, if we suspect that
ourdataiscomposedofseparateclasseslikeimagesofboth
hands and faces, and we wish hˆ as a generative model, we
could split our data to its different components (learning a
separate h for each). This relates to the conditions of The-
orem 2.1 which assume that X’s support is an open inter-
val. For example, if we suspect that our data can be sepa-
rated into two disjoint uniform distributions, then the theo-
rem doesn’t apply. However, it does apply to each separate
uniformlydistributedcomponent.
A.4
We experimented with two stopping criteria. The first was
basedonthevalidationset,asintheMNISTcase.Thesec-
ond used IS to assess the quality of the generated images
using a small random set of 200 images. IS yielded bet-
ter results. Training was stopped when no improvement inEpoch 1 Epoch 5 Epoch 15 Epoch 31 (last)
Figure6:Swiss-rollsamplesgeneratedbyGTNduringthetrainingprocess.Epoch 1 Epoch 18 Epoch 65 Epoch 127
Epoch 250 Epoch 276 Epoch 640 REAL
Figure7:RandomlychosensamplesgeneratedbyGTNduringthetrainingprocessoncelebAatseveralepochsofimprovement
in the IS score, and randomly chosen real images (bottom-right grid). Each epoch shows 200 images, each of which is the
decodedhˆ(r)forsomerandomr ∼ N(0,I)).Thebottomrightgridcontains200imageseachofwhichisthedecodedlatent
vectorofarandomrealimage.Epoch
Figure8:Inceptionscore(IS)for200randomlygeneratedimages(decodedlatentvectorsgeneratedbyhˆ fromrandomnormal
vectors)plottedbyepoch(atepochsofimprovementinthescore)(blue);andISofarandomsetof200realimages(decoded
latentvectorsofrealimages)(green).
Figure9:Interpolationsbetweenahand(downwardfacing)andapalm(upwardfacing)(left)andviceversa(right).