NAVSIM: Data-Driven Non-Reactive
Autonomous Vehicle Simulation and Benchmarking
DanielDauner1,2 MarcelHallgarten1,5 TianyuLi3 XinshuoWeng4 ZhiyuHuang4,6
ZetongYang3 HongyangLi3 IgorGilitschenski7,8 BorisIvanovic4 MarcoPavone4,9
AndreasGeiger1,2 KashyapChitta1,2
1UniversityofTübingen 2TübingenAICenter 3OpenDriveLabatShanghaiAILab
4NVIDIAResearch 5RobertBoschGmbH 6NanyangTechnologicalUniversity
7UniversityofToronto 8VectorInstitute 9StanfordUniversity
NAV
IM
On Road On Road On Road
Human No Collision No Collision No Collision
ADE: 0.0m ADE: 2.3m ADE: 1.1m ADE: 1.0m
Figure1: NAVSIM.Traditionalmetricssuchastheaveragedisplacementerror(ADE)donotaccount
fortheinteractiveandmulti-modalnatureofdriving. Ourbenchmarkusessimulation-basedmetrics
toprovidemoremeaningfulevaluationsoftrajectoryoutputsfromsensor-baseddrivingpolicies.
Abstract
Benchmarkingvision-baseddrivingpoliciesischallenging. Ononehand,open-
loop evaluation with real data is easy, but these results do not reflect closed-
loopperformance. Ontheother,closed-loopevaluationispossibleinsimulation,
but is hard to scale due to its significant computational demands. Further, the
simulators available today exhibit a large domain gap to real data. This has
resultedinaninabilitytodrawclearconclusionsfromtherapidlygrowingbodyof
researchonend-to-endautonomousdriving. Inthispaper,wepresentNAVSIM,a
middlegroundbetweentheseevaluationparadigms,whereweuselargedatasets
in combination with a non-reactive simulator to enable large-scale real-world
benchmarking. Specifically,wegathersimulation-basedmetrics,suchasprogress
andtimetocollision,byunrollingbird’seyeviewabstractionsofthetestscenesfor
ashortsimulationhorizon. Oursimulationisnon-reactive,i.e.,theevaluatedpolicy
andenvironmentdonotinfluenceeachother. Aswedemonstrateempirically,this
decouplingallowsopen-loopmetriccomputationwhilebeingbetteralignedwith
closed-loopevaluationsthantraditionaldisplacementerrors. NAVSIMenabled
anewcompetitionheldatCVPR2024,where143teamssubmitted463entries,
resultinginseveralnewinsights.Onalargesetofchallengingscenarios,weobserve
thatsimplemethodswithmoderatecomputerequirementssuchasTransFusercan
match recent large-scale end-to-end driving architectures such as UniAD. Our
modularframeworkcanpotentiallybeextendedwithnewdatasets,datacuration
strategies,andmetrics,andwillbecontinuallymaintainedtohostfuturechallenges.
Ourcodeisavailableathttps://github.com/autonomousvision/navsim.
4202
nuJ
12
]VC.sc[
1v94351.6042:viXra1 Introduction
Autonomousvehicles(AVs)havegainedimmenseresearchinterestduetotheirpotentialtochange
transportationandimprovetrafficsafety[23,9]. Thishascreatedalargecommunityworkingonthe
developmentofAValgorithms,whichmaphigh-dimensionalsensordatatodesiredvehiclecontrol
outputs. Therefore,measuringandcomparingtheperformanceofAValgorithmsisacrucialtask.
Unfortunately,itisextremelychallengingtoevaluatedrivingperformance,andthemostwidely-used
benchmarkstodayfallshortinseveralrespects: (1)thedatasetsused,suchasnuScenes[5],were
createdforperceptiontaskssuchasobjectdetection. Assuch,theyfocusonvisualdiversityand
labelqualityinsteadoftherelevanceofthedataforresearchonplanning. Often,mostframeshavea
trivialsolutionofextrapolatingthehistoricaldrivingbehavior,leadingto“blind”drivingpolicies
thatobserveonlythevehicle’spasttrajectoryobtainingstate-of-the-artperformance [56,32,16].
(2)Duetothefactthatdrivingisaninherentlymultifacetedtaskwherethealgorithmmustbalance
severaldesiredpropertiessuchassafety,comfort,andprogress,theevaluationmustalsoinvolve
multiplecomplementarymetrics. However,asshowninFig.1,existingmetricssuchastheaverage
displacementerror(ADE)betweenapredictedandrecordedhumantrajectoryoftenmisrepresent
therelativeaccuracyoftrajectories. (3)Sincedrivinginvolvesinteractionsamongmultipleagents,
evaluationmustideallybeinteractive,e.g.,insimulation. Unfortunately,existingsimulatorswith
syntheticsensordataexhibitasignificantdomaingaptoreal-worlddriving. (4)Besides,thelackof
astandardizedevaluationsetuphasledtosubtleinconsistenciesbetweenmetricsinexistingwork,
leading to unfair comparisons and inaccurate conclusions [50, 32]. Collectively, these problems
hinderprogressinthedevelopmentofAVs,emphasizingtheneedformoreprincipledbenchmarks.
Inthiswork,wetakestepstowardsalleviatingtheseissues. First,weproposeastrategyforsampling
interestingdrivingscenariosandapplyittothelargestpublicly-availabledrivingdataset[26]. We
obtain,forthefirsttime,over100kchallengingreal-worlddrivingscenariosfortrainingandevaluating
sensor-baseddrivingpolicies.Weshowthatinthesescenarios,“blind”drivingpoliciesfailtocompete
withmoreprincipledsensor-basedpolicies. Second,wedrawinspirationfromtheliteratureofrule-
basedplanningforAVs[41,18,39,16]toidentifyasetofdiverse,efficient,andprincipledmetrics
that cover multiple facets of the autonomous driving task. Third, we circumvent the need for
inaccuratesensorsimulationwithdomaingapsbysimplifyingoursimulationtoanon-reactiveone.
Givenanobservedreal-worldsensorinput, theagentundertestcommitstoasetofactionsfora
specifictimehorizon. Further,theseactionsareassumedtonotaffectthefuturebehaviorofother
agentsinthescene. Underthissetting,itispossibletosimulatetheexpectedmotionofallagents
overthistimehorizoninasimplifiedbird’s-eye-view(BEV)abstractionofthescene,andincorporate
metrics that involve interactions, as we observe in Fig. 1. Empirically, we demonstrate that our
selectedmetricsarewell-correlatedtotheoutcomesofclosed-loopsimulations. Finally,weestablish
an official evaluation server on the open-source HuggingFace platform, which is free, has a low
maintenanceoverhead,andenablesfuturescalingtomorechallengingdatasetsandmetrics.
WecombinetheseideastoproposeNAVSIM,acomprehensivetoolforAVdatacuration,simulation,
and benchmarking. We instantiate standardized training and evaluation splits for NAVSIM with
theOpenScenedataset[15],thoughourframeworkcanbeextendedtootherdatasets. Withthese
splits,wepresentadetailedanalysisofpopularend-to-enddrivingmodelspreviouslybenchmarked
eitherexclusivelyonCARLA[17]ornuScenes[5],providingthefirstdirectcomparisonbetween
these families of approaches in an independent evaluation setting. Interestingly, we find that the
performancesofthebestmethodsdevelopedinbothsettingsaresimilar,despiteavastdifferencein
computationalrequirementsfortheirtraining. Finally,wereviewtheinsightsgainedthroughthe2024
NAVSIMchallenge1,hostedinconjunctionwiththeCVPR2024WorkshoponFoundationModels
forAutonomousSystems. Forthechallenge,143teamsfrom13countriesdevelopeddiversemethods
thatcompetedontheproposedbenchmark. Thetopmethodsrangedfrommulti-billionparameter
visionlanguagemodels[45,33,53,58]tomoreefficientandrecentlyoverlookedapproachesbased
1https://opendrivelab.com/challenge2024/#end_to_end_driving_at_scale
2ontrajectorysamplingandscoring[40,20,10],demonstratingtheremarkableabilityofthebroader
communitytoadvanceAVresearchwhenprovidedwiththerighttools.
Contributions. (1)WebuildNAVSIM,aframeworkfornon-reactiveAVsimulation,withstandard-
izedprotocolsfortrainingandtesting,datacurationtoolsensuringbroadaccessibility,andanofficial
publicevaluationserverusedfortheinauguralNAVSIMchallenge. (2)Wedevelopconfigurable
simulation-basedmetricsthatarewell-suitedforevaluatingsensor-basedmotionplanning. (3)We
reimplementacollectionofend-to-endapproachesforNAVSIMincludingTransFuser,UniAD,and
PARA-Drive,showcasingthesurprisingpotentialofsimplemodelsinourchallengingscenarios.
2 RelatedWork
End-to-EndDriving. End-to-enddrivingstreamlinestheentirestackfromperceptiontoplanning
intoasingleoptimizablenetwork. Thiseliminatestheneedformanuallydesigningintermediate
representations. Followingpioneeringwork[35,4,27],adiverselandscapeofend-to-endmodels
has emerged. For instance, an extensive body of end-to-end approaches focuses on closed-loop
simulators,utilizingsingle-framecameras,LiDARpointclouds,oracombinationofbothforexpert
imitation [7, 11, 36, 8, 52, 43, 44, 12, 24, 57, 22]. More recently, developing end-to-end models
onopen-loopbenchmarkshasgainedtraction[20,21,25,54,32,50]. Ourworkintroducesanew
evaluationschemewithwhichwecompareend-to-endmodelsfrombothcommunities.
Closed-LoopBenchmarkingwithSimulation. Drivingsimulatorsallowustoevaluateautonomous
systemsinaclosed-loopmannerandcollectdownstreamdrivingstatistics,includingcollisionrates,
traffic-rulecompliance,orcomfort. Abroadbodyofresearchconductsevaluationsinsimulators,
suchasCARLA[17]orMetadrive[29]withsensorsimulation,ornuPlan[26]andWaymax[19]
for data-driven simulation. Unfortunately, ensuring realism when simulating traffic behavior or
sensordataremainsachallengingtask. TosimulatecameraorLiDARsensors, mostestablished
simulatorsrelyongraphics-basedrenderingmethods,leadingtoaninherentdomaingapinterms
ofvisualfidelityandsensorcharacteristics. Data-drivensimulatorsformotionplanningincorporate
traffic recordings but do not support image or LiDAR-based methods [26, 19, 13]. Data-driven
sensorsimulationleveragesandadaptsreal-worldsensordatatocreatenewsimulationswherethe
vehiclemaymovedifferently,buttherenderingqualityofexistingtoolsissubpar[1,2,49]. Further,
whilepromisingimage[47]orLiDAR[34]synthesisapproachesexist,efficientlysimulatingsensors
entirelyfromdataremainsanopenproblem. Inthiswork,weprovideanapproachfortheevaluation
ofrealsensordatawithsimulation-basedmetricsbymakingasimplifyingassumptionthattheagent
andenvironmentdonotinfluenceeachotheroverashortsimulationhorizon. Despitethisstrong
assumption,whenbenchmarkingonrealdata,NAVSIMbetterreflectsplanningperformancethan
establishedevaluationprotocols,asdemonstratedthroughoursystematicexperimentalanalysis.
Open-LoopBenchmarkingwithDisplacementErrors. Open-loopevaluationprotocolscommonly
measure displacement errors between trajectories of a recorded expert (i.e., of a human driver)
and a motion planner. However, several issues concerning evaluation with displacement errors
have surfaced recently, particularly on the nuScenes dataset [5]. Given that nuScenes does not
providestandardizedplanningmetrics,priorworkreliedonindependentimplementations,which
ledtoinconsistencieswhenreportingorcomparingresults[50,32]. Next,mostplanningmodelsin
nuScenesreceivethehumantrajectoryendpointasadiscretedirectioncommand[20,21,25,32,50],
thereby leaking ground-truth information into inputs. Moreover, about 75% of the scenarios in
nuScenesinvolvestraightdriving[32],leadingtosimplesolutionswhenextrapolatingtheego-motion.
Forinstance,AD-MLPdemonstratesthatanMLPonthekinematicegostatus(ignoringperception
completely)canachievestate-of-the-artdisplacementerrors[56]. Suchblindagentsareundeniably
dangerous,whichhighlightsabroaderconcern: displacementmetricsarenotcorrelatedtoclosed-
loopdriving[14,17,3,16]. Inthiswork,weaddressprevalentissuesofnuScenesandproposea
standardizeddrivingbenchmarkwithchallengingscenariosandanofficialevaluationserver. We
deriveanavigationgoalfromthelanegraphinsteadofthehumantrajectorytopreventlabelleakage,
andproposeprincipledsimulation-basedmetricsasanalternativetodisplacementerrors.
33 NAVSIM:Non-ReactiveAutonomousVehicleSimulation
NAVSIMcombinestheeaseofuseofopen-loopbenchmarkssuchasnuScenes[5]withmetricsbased
onclosed-loopsimulatorssuchasnuPlan[26]. Inthefollowing,wegiveadetailedintroductionto
thetaskandmetricsthatdrivingagentsarechallengedwithinNAVSIM.Subsequently,weproposea
filteringmethodtoobtainstandardizedtrainandtestsplitscoveringchallengingscenes.
Taskdescription. DrivingagentsinNAVSIMmustplanatrajectory,definedasasequenceoffuture
poses,overahorizonoffourseconds. Theirinputcontainsstreamsofpastframesfromonboard
sensors,suchascameras,LiDAR,aswellasthevehicle’scurrentspeed,acceleration,andnavigation
goal,jointlytermedtheegostatus. Forcompatibilitywithpriorwork[20,21,25,50],weprovidethe
navigationgoalasaone-hotvectorwiththreecategories: left,straight,orright.
Non-Reactive Simulation. Traditional closed-loop benchmarks normally infer planners at high
frequencies(e.g.,10Hz)[17,26]. However,thisrequiresefficientsimulationofallinputmodalitiesof
thedrivingagent,includinghigh-dimensionalsensorstreamsinthecaseofsensor-basedapproaches.
Tosidestepthis,thecoreideaofNAVSIMistoevaluatedrivingagentsusinganon-reactivesimulation.
Thismeansdrivingagentsareonlyemployedintheinitialframeofeachscene. Afterwards, the
planned trajectory is kept fixed for the entire trajectory duration. Over this short horizon, no
environmental feedback is provided to the driving agent, and the NAVSIM evaluation is purely
basedontheinitialreal-worldsensorsample. Thismakestheagent’staskmorechallenging,asit
requiresasafeplanuptotheentirefour-secondhorizon. Therefore,itisnotpossibletoscalethis
kindofsimulationtoarbitrarilylongtimehorizons. Despitethislimitation,non-reactivesimulation
offersakeyadvantage: unliketraditionalopen-loopbenchmarks,whichmainlycomparetheplanned
trajectory to the human driver’s trajectory in a similar setting, it enables the use of simulation
outcomestocomputemetricsreflectingsafety,comfort,andprogress. AnLQRcontroller[28]is
appliedateachsimulationiterationtocalculatesteeringandaccelerationvalues,andakinematic
bicyclemodel[37]propagatestheegovehicle. Weexecutethispipelineat10Hzoverthe4strajectory
horizon. InSec.4.1,weshowthatdespiteoursimplifyingassumption,ourevaluationresultsina
muchbetteralignmentwithclosed-loopmetricsthantraditionalopen-loopmetricsachieve.
PDMScore.NAVSIMscoresdrivingagentsintwosteps.First,subscoresinrange[0,1]arecomputed
aftersimulation. Second,thesesubscoresareaggregatedintothePDMScore(PDMS)∈[0,1]. Itis
namedafterthePredictiveDriverModel(PDM)[16],astate-of-the-artrule-basedplannerwhichuses
thisscoringfunctiontoevaluatetrajectoryproposalsduringclosed-loopsimulationinnuPlan. The
metricisalsoanefficientreimplementationofthenuPlanclosed-loopscoremetric[26]. InNAVSIM,
thePDMScanbeadaptedbyaddingorremovingsubscores,changingaggregationparameters,or
makingsubscoresmorechallenging,e.g.,byadaptingtheirinternalthresholds. Itiscalculatedper
frameandaveragedacrossframes. Inthiswork,weusethefollowingaggregationofsubscores:
(cid:32) (cid:33) (cid:32)(cid:80) (cid:33)
PDMS= (cid:89) score
m
× w ∈{(cid:80)EP,TTC,C }weigh wt ew ig× htscore w . (1)
(cid:124) m ∈{NC,DA (cid:123)C } (cid:122) (cid:125) (cid:124) w ∈{EP,TTC (cid:123), (cid:122)C } w (cid:125)
penalties weightedaverage
Subscoresarecategorizedbytheirimportanceaspenaltiesortermsinaweightedaverage. Apenalty
punishesinadmissiblebehaviorsuchascollisionswithafactor<1. Theweightedaverageaggregates
subscoresforotherobjectivessuchasprogressandcomfort. Inthefollowing,webrieflydescribe
eachsubscore. Moredetailscanbefoundinthesupplementarymaterial.
Penalties. Avoiding collisions and staying on the road is imperative for motion planning as it
ensurestrafficrulecomplianceandthesafetyofpedestriansandroadusers. Thus,failingtodrive
with no collisions (NC) with road users (vehicles, pedestrians, and bicycles) or infractions with
regardtodrivableareacompliance(DAC)resultinhardpenaltiesofscore =0orscore =0
NC DAC
respectively. ThisresultsinaPDMSof0forthecurrentscene. Weignorecertaincollisionsthatare
notconsidered"at-fault"inthenon-reactiveenvironment,e.g.whentheegovehicleisstatic. For
collisionswithstaticobjects,weapplyasofterpenaltyofscore =0.5.
NC
4(a)PDMS (b)LongitudinalFrequency (c)LateralFrequency
100%
non-filtered
1.00 filtered
20%
0.75
10%
0.50 10%
0.25 1%
0.00 0%
Human ConstantVel. 0 20 40 4 2 0 2 4
− −
LongitudinalEndpoint[m] LateralEndpoint[m]
Figure2: Filtering. (a)Weconsiderchallengingsceneswheremaintainingaconstantvelocityand
headingfailscomparedtothehumandriver. (b)Ourfilteringprimarilyremovessceneswithstaticor
fastlongitudinalmovementand(c)leadstomorediversityinlateralmovement(log-scale).
WeightedAverage. Theweightedaverageaccountsforegoprogress(EP),time-to-collision(TTC),
andcomfort(C).Theegoprogresssubscorescore representstheagentprogressalongtheroute
EP
centerasaratiotoanapproximatedsafeupperboundfromthePDM-Closedplanner[16]. PDM-
Closedobtainsapossibleprogressvaluewithoutcollisionsoroff-roaddrivingwithasearch-based
strategybasedontrajectoryproposals. Thefinalratioisclippedto[0,1]whilediscardinglowor
negativeprogressscoresiftheupperboundisbelow5meters. Next,theTTCsubscoreensuresthat
drivingagentsrespectthesafetymarginstoothervehicles. Defaultingtoavalueof1,thissubscore
issetto0ifforanysimulationstepwithinthe4shorizon,theego-vehicle’stime-to-collison,when
projectedforwardwithaconstantvelocityandheading,islessthanacertainthreshold. Finally,the
comfortsubscoreisobtainedbycomparingtheaccelerationandjerkofthetrajectorytopredetermined
thresholds. FollowingthecostweightsusedbythePDM-Closedplanner,wesetthecoefficientsof
theweightedaverageasweight =5,weight =5,andweight =2.
EP TTC C
3.1 GeneratingStandardizedandChallengingTrainandTestSplits
Dataset. TheNAVSIMframeworkisagnostictothechoiceofdrivingdataset. WechooseOpen-
Scene[15],aredistributionofnuPlan[26],thelargestannotatedpublicdrivingdataset. OpenScene
includes 120 hours of driving at a reduced frequency of 2Hz typically considered by end-to-end
planningalgorithms,resultingina10×reductionofdatastoragerequirementscomparedtonuPlan
fromover20TBto2TB.Ouragentinput,basedonOpenScene,compriseseightcameras,eachwith
aresolutionof1920×1080pixels,andamergedLiDARpointcloudfromfivesensors. Theinput
includesthecurrenttime-stepandoptionally3pastframes,totaling1.5sat2Hz. Inprinciple,any
drivingdatasetthatprovidesannotatedHDmaps,objectboundingboxes,andsensordatacanbe
convertedintothisformatandthusbeusedwithNAVSIM.
Filteringforchallengingscenes. Amajorityofhumandrivingdatainvolvestrivialsituationssuch
asbeingstationaryorstraightdrivingatanearconstantspeed. Thesecanbesolvedefficientlyby
simple heuristics, e.g., as depicted in Fig. 2 (a), the baseline of maintaining a constant velocity
andheadingachievesaPDMSof79%ontheOpenScenedataset,wherehuman-levelperformance
correspondsto91%. InNAVSIM,weproposetheuseofafiltereddatasettoremoveframeswith
(1)near-trivialsolutionsand(2)significantannotationerrors. Weremovehighlysimplisticscenes
bydetectingifthepreviouslymentionedconstantvelocityagentexceedsaPDMSof0.8. Similarly,
weremovescenesinwhichthehumantrajectoryresultsinaPDMSoflessthan0.8. Thisensures
thatanacceptablesolutionexiststothesedifficultscenariosandfiltersoutnoisyannotationssuchas
inaccurateboundingboxes. Thesethresholdscanbeadjustedbasedonthedesiredfiltereddataset
size. Theresultingscenariosarechallenging,whichisunderlinedbythescoreoftheconstantvelocity
agent dropping to 22%, whereas the human expert achieves a score of 95%. The higher ratio of
non-trivialscenarios,suchasturning,alsoresultsinendpointsbeinglessdistantlongitudinallywhen
nonzero,andmoreevenlydistributedlaterally,asseeninFig.2(b-c).Weemploythisfilteringstrategy
toprovidestandardizedsplitsfortrainingandtesting, callednavtrainandnavtest, with103k
51.0
(b) (c)
1.0 (a) PDMSRank
0.8 PDMSLinear
OLSRank
0.6 OLSLinear
0.8
0.4
0.2
0.6
0.0
d=4s d=8s d=15s f=2Hz f=10Hz
1.0
0.4 (d) (e)
0.8
0.2 PDMSlearned 0.6
PDMSrule-based 0.4
OLSlearned
OLSrule-based 0.2
0.0
0.0
0.0 0.2 0.4 0.6 0.8 1.0
h=2s h=4s h=6s h=8s reactive non-reactive
CLS(f=10Hz,d=15s)
Figure3: Closed-LoopAlignment. (a)Foreachplanner,weshowopen-loopmetrics(OLS,PDMS)
togetherwiththecorrespondingclosed-loopscore(CLS).Thetrendlinesdepictingcorrelationsarefit
linearlytoall(learnedandrule-based)planners. Moreover,weanalyzedifferent(b)CLSdurationsd,
(c)planningfrequenciesf,(d)PDMShorizonsh,and(e)backgroundagentbehaviors.
and12ksamplesrespectively. Thiscurateddataservesasabenchmarkaccessibleasastandalone
downloadoptionwithamoderatestoragedemandgivenitslargescaleanddiversity(450GB).
4 Experiments
Inthissection,wepresenttheresultsofourexperimentsaimedatansweringthefollowingquestions:
(1)Cannon-reactiveopen-loopsimulationprovidesufficientcorrelationtoclosed-loopmetrics? (2)
WhatnewconclusionsdoexperimentsonNAVSIMprovidecomparedtopriorbenchmarks?
4.1 AlignmentBetweenOpen-LoopandClosed-LoopEvaluation
Open-loopmetricsshouldideallybealignedwithclosed-loopmetricsintheirevaluationofdifferent
drivingalgorithms. Inthissection,webenchmarkalargesetofplannerstoanalyzethealignmentof
closed-loopmetricswithtraditionaldistance-basedopen-loopmetricsandtheproposedPDMS.
Benchmark. Studyingtherelationofclosed-loopandopen-loopmetricsnecessitatesaccesstoa
fullyreactivesimulator. Tostaycompatiblewiththedataset,weusethenuPlansimulator[26],which
enablessimulationforprivilegedplannerswithaccesstoground-truthperceptionandHDmapinputs.
SimilartoPDMS,nuPlancombinesweightedaveragesandmultipliedpenaltiesintwoofficialscores:
theopen-loopscore(OLS)aggregatesdisplacementandheadingerrorswithamultipliedmiss-rate,
andtheclosed-loopscore(CLS)implementssimilarmetricsfromSection3. IncludingPDMS,all
metricsarein[0,1]withhigherscoresindicatingbetterperformance.
Duetotheheavycomputationalrequirementsofclosed-loopsimulation,weevaluateonthenavmini
split. Thisisanewsplitwecreateforrapidtesting,with396scenariosintotalthatareindependentof
bothnavtrainandnavtestbutfilteredusingthesamestrategy(Section3.1)andhencesimilarly
distributed. WenotethatnuPlanofferstwokindsofbackgroundagents: reactiveagentsalonglane
centersbasedontheIntelligentDriverModel(IDM)[48],andnon-reactiveagentsreplayedfrom
thedataset,whichweemployunlessotherwisestated. Wealsodefaulttoaclosed-loopsimulation
durationofd=15s,andaplanningfrequencyoff =10Hz,whicharestandardfornuPlan.
MotionPlanners. Open-loopmetricsfavorlearnedplannerswhilerule-basedapproachesperform
wellinclosed-loopevaluationinnuPlan[16]. Weuseacombinationofbothplannertypesinthis
experimenttocoverdifferentperformancelevels. Intotal,weinclude37rule-basedplannerswith2
6
erocS
noitalerroC
noitalerroCMethod EgoStat. Image LiDAR Video NC↑ DAC↑ TTC↑ Comf.↑ EP↑ PDMS↑
ConstantVelocity ✓ 69.9 58.8 49.3 100 49.3 21.6
EgoStatusMLP ✓ 93.0 77.3 83.6 100 62.8 65.6
LTF[12] ✓ ✓ 97.4 92.8 92.4 100 79.0 83.8
TransFuser[12] ✓ ✓ ✓ 97.7 92.8 92.8 100 79.2 84.0
UniAD[21] ✓ ✓ ✓ 97.8 91.9 92.9 100 78.8 83.4
PARA-Drive[50] ✓ ✓ ✓ 97.9 92.4 93.0 99.8 79.3 84.0
Human 100 100 100 99.9 87.5 94.8
Table1: NavtestBenchmark. Weshowthenoat-faultcollision(NC),drivableareacompliance
(DAC),time-to-collision(TTC),comfort(Comf.),andegoprogress(EP)subscores,andthePDM
Score(PDMS),aspercentages. Relyingontheegostatusisinsufficientforcompetitiveresults. While
sensoragentsimprove,thegaptohumanperformancehighlightsourbenchmark’schallenges.
constantvelocityand8constantaccelerationmodels,15IDMplanners[48],and12PDM-Closed
variants[16]whichdifferinhyperparametersfortrajectorygeneration. Forlearnedplanning,we
evaluateUrbanDrivermodels[42]of2modelsizesand2traininglengths,andPlanCNN[38]models
with15inputcombinationsoftheBEVraster,egostatus,centerline,andnavigationgoal. Wetrain
allmodelson{25%,50%,100%}ofnavtrainandanequallysizeduniformlysampledsubsetof
OpenScene,giving114learnedplanners. Seethesupplementarymaterialforadditionaldetails.
Results. The alignment between metrics is presented in Fig. 3 (a-e). Compared to OLS, we
consistentlyobservebetterclosed-loopcorrelationforPDMS,intermsofSpearman’s(rank)and
Pearson’s (linear) correlation coefficients. As shown in (a), PDMS can capture the closed-loop
propertiesofbothlearnedandrule-basedplanners,whereasdistance-basedopen-loopmetricsshow
aclearmisalignment. DecreasingtheCLSdurationin(b)fromd = 15stod = 4sfurtherraises
thecorrelationofPDMSandOLS,asthesimulationhorizonmorecloselymatchestheopen-loop
counterparts. Interestingly,weobserveahighercorrelationofopen-loopmetricsin(c)whenreducing
theplanningfrequencyto2Hz. Weexpectalowerplanningfrequencytomitigatecumulativeerrors
and enhance the controller’s stability in simulation, leading to more precise trajectory execution.
Moreover, we observe an increase in correlation for longer PDMS horizons in (d), ranging from
h=2stoh=8s. Whilepredictingthefuturemotionover8sischallenginginuncertainscenarios,
ourresultsindicatethevalueoflonghorizonswhenevaluatingmotionplanners. Lastly,replacingthe
non-reactivebackgroundagentswithreactiveIDMvehiclesin(e)haslittleeffectonthecorrelation,
possiblyduetothesimilardifficultyofbothtasks[16].
4.2 AnalysisoftheStateoftheArtinEnd-to-EndAutonomousDriving
Inthissection,webenchmarkacollectionofend-to-endarchitectures,whichpreviouslyachieved
state-of-the-artperformanceonexistingopen-orclosed-loopbenchmarks.
Methods. Asalowerbound,weconsiderthe(1)ConstantVelocitybaselinedetailedinSection3.1.
Weincludean(2)EgoStatusMLPasasecond"blind"agent,whichleveragesanMLPfortrajectory
predictiongivenonlytheegovelocity,accelerationandnavigationgoal.Asanestablishedarchitecture
onCARLA,weevaluateourreimplementationof(3)TransFuser[12],whichusesthreecropped
anddownscaledforward-facingcameras,concatenatedintoa1024×256image,andarasterized
BEVLiDARinputforpredictingwaypoints. Itperforms3DobjectdetectionandBEVsemantic
segmentationasauxiliarytasks. Wethenconsider(4)LatentTransFuser(LTF)[12],whichshares
thesamearchitectureasTransFuserbutreplacestheLiDARinputwithalearnedembedding,hence
requiringonlycamerainputs. Moreover,weprovidetwostate-of-the-artend-to-endarchitectures
for open-loop trajectory prediction on nuScenes. (5) UniAD [21] incorporates a wide range of
tasks,suchasmapping,tracking,motion,andoccupancypredictioninasemi-sequentialarchitecture,
which processes feature representations through several transformer decoders culminating in a
trajectoryplanningmodule. (6)PARA-Drive[50]usesthesameauxiliarytasks,butparallelizes
thenetworkarchitecture,andtheauxiliarytaskheadsaretrainedinparallelwithasharedencoder.
BothUniADandPARA-DriveuseaBEVFormerbackbone[31],whichencodestheeightsurround-
7Config Parameter Setting NC↑ DAC↑ TTC↑ Comf.↑ EP↑ PDMS↑
A1 Seed1 98.0 91.3 94.2 100 78.1 83.3
A2 Defaultconfig Seed2 97.7 92.8 92.8 100 79.2 84.0
A3 Seed3 97.9 93.0 93.1 100 79.3 84.4
B1 Goalonly 96.8 91.9 91.3 98.6 77.3 81.8
Egostatus
B2 Goalandvelocityonly 96.7 92.3 91.0 100 77.8 82.3
C1 60 ◦(1camera) 96.7 90.2 90.9 100 75.8 80.3
C2 CameraFOV 160 ◦(3cameras) 97.6 91.4 92.7 100 78.1 82.8
C3 240 ◦(5cameras) 97.8 92.5 93.0 100 79.2 84.1
D1 F:16,B:16,L:16,R:16 96.9 88.3 91.2 100 74.6 79.1
D2 LiDARrange F:64,B:32,L:32,R:32 97.8 92.7 93.4 100 79.3 84.3
D3 F:64,B:64,L:64,R:64 96.8 90.3 91.5 100 76.5 81.0
E1 NoBEVsegmentation 97.4 90.5 92.2 100 77.1 81.6
Supervision
E2 No3Ddetection 97.8 92.7 92.9 100 79.2 84.0
Table2: TransFuserAblations. Thedefaultconfiguration,whichobtainsthebestresults,usesthe
navigationgoal,velocity,andaccelerationasegostatusinputs. ItscameraFOVisaround140 and
◦
LiDARrangeis32mtothefront(F),back(B),left(L),andright(R).Itusesbothauxiliarytasks.
view 1920×1080 camera images over four temporal frames into a BEV feature representation.
Implementationdetailsforallmethodsareprovidedinthesupplementarymaterial.
Results. We show our results on navtest in Table 1. The Constant Velocity model is a lower
bound,astheagentisusedtoidentifytrivialdrivingscenesexcludedfromthebenchmark. TheEgo
StatusMLPachievesaPDMSof65.6,showingthevalueoftheaccelerationandnavigationgoalfor
avoidingcollisionsanddrivingoff-road. However,weobserveacleargapbetweenagentsrelying
solelyontheegostatusandthoseconsideringsensordata,incontrasttoresultsonnuScenes[32]. All
sensoragentsachieveaPDMSofover83,whereTransFuserandPARA-Drivemarginallyperform
best,withaPDMSof84.0.Surprisingly,thecamera-onlyLTFachievessimilarresults(83.8).UniAD
reaches a PDMS of 83.4, which, together with PARA-Drive, do not surpass the performance of
TransFuserandLTF,despitetheneedformoredemandingtraining,e.g.,80GPUsfor3daystotrain
PARA-Driveversus1GPUfor1dayforTransFuseronthenavtrainsplit. Duetothedefinition
of at-fault collisions, which discard certain rear-collisions into the ego vehicle, we suspect that
surround-viewcamerasusedbyUniADandPARA-Drive,andLiDARinputofTransFuser,areless
importantthanthewide-anglefrontcamerawhichistheonlyinputofLTF.The10PDMSdiscrepancy
tothehumanoperatordemonstratesthatnavtestposeschallengeseventowell-studiedend-to-end
architectures. Specifically, thedrivableareacompliance(DAC)andegoprogress(EP)subscores
remainthemostchallenging. Notably,EPcannotbesolvedpurelybyhumanimitation,giventhatthe
maximumprogressestimateusedfornormalizationisbasedonaprivilegedrule-basedmotionplanner.
Interestingly,allagentsachievenear-perfectcomfortscores,indicatingthatsmoothaccelerationand
jerkprofilesarelearnednaturallyfromhumanimitation.
Analyzing TransFuser. In Table 2, we compare several training settings for TransFuser. For
the three training seeds in configs A1-A3, we observe a standard deviation of ± 0.56 in PDMS,
whichisrelativelysmallcomparedtovarianceamongtrainingseedsforclosed-loopsimulationsin
CARLA[17]. Further,unlikeCARLA,NAVSIMisdeterministic,andweobtainidenticalscores
whenrepeatingevaluationsofadeterministicdrivingagent. Discardingvelocityandacceleration
(B1)lowersPDMSby1.5−2.6,whereasonlyremovingtheacceleration(B2)lowersthescoreby
1.0−2.1. WeconcludethatwhileTransFuserbenefitsfromtheegostatus,itisnotpurelyrelying
onthekinematicstateforplanning. Next,onlyconsideringthefrontcamera(C1)witha60 FOV
◦
leadstoasmalldropinalmostallsubscores,comparedtoourdefaultsettingofthreecroppedand
concatenatedimageswithaFOVof140 . However,expandingtheFOVwithadditionalcameras
◦
doesnotresultinsubstantiallyimprovedscores. Interestingly,restrictingtheLiDARrangeto16m
inalldirections(D1),resultsinascoreof79,whichislowerthandroppingLiDARaltogether(see
LTFinTable1). ExpandingtheLiDARrangeto64mintheforwarddirection(D2)oralldirections
(D3)doesnotprovidesignificantimprovements. WesuspectthatchangesintheLiDARrangeoverly
simplifyorcomplicatetheauxiliary3DobjectdetectionandBEVsemanticsegmentationtasks,which
8operateintheLiDARcoordinateframe,hinderingeffectiveimitationlearning. Wechecktheimpact
oftheauxiliarytasksbyexcludingthem,whereperformancedropswithoutBEVSegmentation(E1).
CVPR2024NAVSIMChallenge. Weorganizedtheinaugu-
ral NAVSIM challenge which ran from March - May 2024.
Toensureintegrity,weusedaprivatedatasetandonlygave
20
participantsaccesstosensorinputs, withholdingallannota-
tions. Competitorscouldsubmittheiragent’strajectoriesto
ourleaderboard,wheretheyweresimulatedandscoredtoob- 10
tainthePDMS.Wereceived463submissionsfrom143teams,
ofwhich78submissionsweremadepubliclyvisible. Wesum-
0
marizetheirscoresinFig.4,relativetotheconstantvelocity 0.25 0.50 0.75 1.00
and TransFuser baselines from Table 1. The winning entry PDMScore
extendedTransFuserandlearnedtopredictproxysubscores Figure4: NAVSIMChallenge.
fortrajectorysamples[30],withasamplingstrategyinspired
byVADv2[10].Thesepredictedsubscoreswereweightedalongsideahumanimitationscoretoselect
theoutputplan. Whiletheideaofsamplingandscoringtrajectoriesiswell-known[46,51,55,6,16],
ithasrecentlybeenoverlookedinfavorofapproacheswhichpredictasingletrajectory. Thisresult
promptsareassessmentofsuchmethods. Theteamthatplacedsecondemployedavisionlanguage
model(VLM)fordriving,whichisrapidlyemergingasasub-fieldintheAVliterature[45,33,53,58].
SeveralsubmissionsattemptedtoreimplementorextendpriorworkonnuScenessuchasUniAD[21]
andVAD[25],butwereunabletooutperformtheTransFuserbaselinebythechallengesubmission
deadline,giventhesignificantengineeringchallengeandcomputerequirements. Thediversityofthe
solutionsontheleaderboardshowsthepotentialofNAVSIMasaframeworkforpushingthefrontiers
ofautonomousdrivingresearch. Weaimtoholdfuturecompetitionswithmorechallengingdataand
metrics. Detailedcompetitionresultsandstatisticsareprovidedinthesupplementarymaterial.
5 Discussion
WepresentNAVSIM,aframeworkfornon-reactiveAVsimulation. Weaddressseveralshortcomings
ofexistingdrivingbenchmarksandproposestandardizedbutconfigurablesimulation-basedmetrics
forbenchmarkingdrivingpolicies.WeimproveaccessibilityforconductingAVresearchbyproviding
downloadablechallengingscenariosplitsandsimpledatacurationmethods. Wedemonstratethat
our evaluation protocol is better aligned to closed-loop driving, benchmark an established set of
end-to-endplanningbaselines,andpresenttheresultsofourinauguralcompetition. Wehopethat
NAVSIMcanserveasanaccessibletoolkitforAVresearchersthatbridgesthegapbetweensimulated
andreal-worlddriving. Weaimtosupportmoredatasetsinthefuture,andadvocateformoreopen
datasetreleasesbythecommunityforacceleratingprogressinautonomousdriving.
Limitations. Whileweshowimprovementsoverdisplacementerror-basedbenchmarking,several
aspectsofdrivingremainunaddressedbyevaluationinNAVSIM.AhighPDMSdoesnotalways
implyahighCLS,sinceourframeworkdoesnotconsiderreactivenessorcompoundingaccumulation
oferrorsinclosed-loopsimulation. Moreover,closed-loopmetricsalsofaceproblems,i.e.,PDMS
inheritsseveralweaknessesofnuPlan’sCLS.Bothscoresdonotregardcertaintrafficrules(e.g.,
stop-signortrafficlightcompliance)orconceptssuchastransitandfuelefficiency. Moreover,asin
CLS,rear-endcollisionsintotheegovehiclearecurrentlynotclassifiedas"at-fault",resultingin
littleimportancegiventothescenebehindthevehicleinNAVSIM.Further,certainlimitationsof
thenuPlandatasetpersistinNAVSIM,suchasminorerrorsincameraparametersornoiseinposes
and3Dannotations. Ouranalysismightfavormethodsthatarerobusttosuchinconsistencies. Inthe
future,weaimtoimprovethesubscoredefinitions(e.g.theat-faultcollisionlogic)andaddmore
subscoresduringaggregation. Additionally,wewouldliketoexploreposeorrouteaugmentations
toaccountforcontroldriftsinclosed-loopdrivingandimproveourevaluation’sdiversity. Given
theselimitations,westronglyencouragetheuseofgraphics-basedclosed-loopsimulators,suchas
CARLA[17],ascomplementarybenchmarkstoNAVSIMwhendevelopingplanningalgorithms.
9
smaeT#
yticoleVtnatsnoC
resuFsnarTAcknowledgments
ThisworkwassupportedbytheERCStartingGrantLEGO-3D(850533),theDFGEXCnumber
2064/1 - project number 390727645, the German Federal Ministry of Education and Research:
Tübingen AI Center, FKZ: 01IS18039A and the German Federal Ministry for Economic Affairs
andClimateActionwithintheprojectNXTGENAIMETHODS.WethanktheInternationalMax
Planck Research School for Intelligent Systems (IMPRS-IS) for supporting Daniel Dauner and
KashyapChitta. WealsothankHuggingFaceforhostingourevaluationservers,theteammembers
ofOpenDriveLabfortheirorganizationalsupport,aswellasNapatKarnchanachariandhisteam
fromMotionalforopen-sourcingtheirdatasetandprovidingustheprivatetestsplitusedinthe2024
NAVSIMChallenge.
References
[1] AlexanderAmini,IgorGilitschenski,JacobPhillips,JuliaMoseyko,RohanBanerjee,Sertac
Karaman,andDanielaRus. Learningrobustcontrolpoliciesforend-to-endautonomousdriving
fromdata-drivensimulation. IEEERoboticsandAutomationLetters(RA-L),2020.
[2] Alexander Amini, Tsun-Hsuan Wang, Igor Gilitschenski, Wilko Schwarting, Zhijian Liu,
SongHan,SertacKaraman,andDanielaRus. Vista2.0: Anopen,data-drivensimulatorfor
multimodalsensingandpolicylearningforautonomousvehicles. InProc.IEEEInternational
Conf.onRoboticsandAutomation(ICRA),2022.
[3] MayankBansal,AlexKrizhevsky,andAbhijitS.Ogale. Chauffeurnet: Learningtodriveby
imitatingthebestandsynthesizingtheworst. InProc.Robotics: ScienceandSystems(RSS),
2019.
[4] MariuszBojarski,DavideDelTesta,DanielDworakowski,BernhardFirner,BeatFlepp,Prasoon
Goyal,LawrenceD.Jackel,MathewMonfort,UrsMuller,JiakaiZhang,XinZhang,JakeZhao,
andKarolZieba. Endtoendlearningforself-drivingcars. arXiv.org,1604.07316,2016.
[5] HolgerCaesar,VarunBankiti,AlexHLang,SourabhVora,VeniceErinLiong,QiangXu,Anush
Krishnan,YuPan,GiancarloBaldan,andOscarBeijbom. nuscenes: Amultimodaldatasetfor
autonomousdriving. InProc.IEEEConf.onComputerVisionandPatternRecognition(CVPR),
2020.
[6] Sergio Casas, Abbas Sadat, and Raquel Urtasun. Mp3: A unified model to map, perceive,
predictandplan. InProc.IEEEConf.onComputerVisionandPatternRecognition(CVPR),
2021.
[7] DianChen,BradyZhou,VladlenKoltun,andPhilippKrähenbühl. Learningbycheating. In
Proc.Conf.onRobotLearning(CoRL),2019.
[8] DianChen,VladlenKoltun,andPhilippKrähenbühl. Learningtodrivefromaworldonrails.
InProc.oftheIEEEInternationalConf.onComputerVision(ICCV),2021.
[9] LiChen,PenghaoWu,KashyapChitta,BernhardJaeger,AndreasGeiger,andHongyangLi.
End-to-endautonomousdriving: Challengesandfrontiers. arXiv.org,2306.16927,2023.
[10] ShaoyuChen,BoJiang,HaoGao,BenchengLiao,QingXu,QianZhang,ChangHuang,Wenyu
Liu,andXinggangWang. VADv2: End-to-endvectorizedautonomousdrivingviaprobabilistic
planning. arXiv.org,2402.13243,2024.
[11] KashyapChitta,AdityaPrakash,andAndreasGeiger. Neat: Neuralattentionfieldsforend-to-
endautonomousdriving. InProc.oftheIEEEInternationalConf.onComputerVision(ICCV),
2021.
10[12] KashyapChitta,AdityaPrakash,BernhardJaeger,ZehaoYu,KatrinRenz,andAndreasGeiger.
TransFuser: Imitation with transformer-based sensor fusion for autonomous driving. IEEE
Trans.onPatternAnalysisandMachineIntelligence(PAMI),2023.
[13] KashyapChitta,DanielDauner,andAndreasGeiger. Sledge: Synthesizingsimulationenviron-
mentsfordrivingagentswithgenerativemodels. arXiv.org,2403.17933,2024.
[14] Felipe Codevilla, Antonio M. Lopez, Vladlen Koltun, and Alexey Dosovitskiy. On offline
evaluationofvision-baseddrivingmodels. InProc.oftheEuropeanConf.onComputerVision
(ECCV),2018.
[15] OpenSceneContributors. Openscene: Thelargestup-to-date3doccupancypredictionbench-
markinautonomousdriving. https://github.com/OpenDriveLab/OpenScene,2023.
[16] Daniel Dauner, Marcel Hallgarten, Andreas Geiger, and Kashyap Chitta. Parting with mis-
conceptionsaboutlearning-basedvehiclemotionplanning. InProc.Conf.onRobotLearning
(CoRL),2023.
[17] Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen Koltun.
CARLA:Anopenurbandrivingsimulator. InProc.Conf.onRobotLearning(CoRL),2017.
[18] HaoyangFan,FanZhu,ChangchunLiu,LiangliangZhang,LiZhuang,DongLi,Weicheng
Zhu, Jiangtao Hu, Hongye Li, and Qi Kong. Baidu apollo EM motion planner. arXiv.org,
1807.08048,2018.
[19] ColeGulino,JustinFu,WenjieLuo,GeorgeTucker,EliBronstein,YirenLu,JeanHarb,Xinlei
Pan,YanWang,XiangyuChen,JohnD.Co-Reyes,RishabhAgarwal,RebeccaRoelofs,YaoLu,
NicoMontali,PaulMougin,ZoeyYang,BrandynWhite,AleksandraFaust,RowanMcAllister,
Dragomir Anguelov, and Benjamin Sapp. Waymax: An accelerated, data-driven simulator
forlarge-scaleautonomousdrivingresearch. InAdvancesinNeuralInformationProcessing
Systems(NeurIPS),2023.
[20] ShengchaoHu,LiChen,PenghaoWu,HongyangLi,JunchiYan,andDachengTao. ST-P3:
End-to-endvision-basedautonomousdrivingviaspatial-temporalfeaturelearning. InProc.of
theEuropeanConf.onComputerVision(ECCV),2022.
[21] YihanHu,JiazhiYang,LiChen,KeyuLi,ChonghaoSima,XizhouZhu,SiqiChai,Senyao
Du,TianweiLin,WenhaiWang,LeweiLu,XiaosongJia,QiangLiu,JifengDai,YuQiao,and
HongyangLi. Planning-orientedautonomousdriving. InProc.IEEEConf.onComputerVision
andPatternRecognition(CVPR),2023.
[22] BernhardJaeger,KashyapChitta,andAndreasGeiger. Hiddenbiasesofend-to-enddriving
models. InProc.oftheIEEEInternationalConf.onComputerVision(ICCV),2023.
[23] JoelJanai,FatmaGüney,AseemBehl,andAndreasGeiger. ComputerVisionforAutonomous
Vehicles: Problems, Datasets and State of the Art, volume 12. Foundations and Trends in
ComputerGraphicsandVision,2020.
[24] XiaosongJia,PenghaoWu,LiChen,JiangweiXie,ConghuiHe,JunchiYan,andHongyangLi.
Thinktwicebeforedriving: Towardsscalabledecodersforend-to-endautonomousdriving. In
Proc.IEEEConf.onComputerVisionandPatternRecognition(CVPR),2023.
[25] Bo Jiang, Shaoyu Chen, Qing Xu, Bencheng Liao, Jiajie Chen, Helong Zhou, Qian Zhang,
WenyuLiu, ChangHuang, andXinggangWang. VAD:Vectorizedscenerepresentationfor
efficientautonomousdriving. InProc.oftheIEEEInternationalConf.onComputerVision
(ICCV),2023.
11[26] Napat Karnchanachari, Dimitris Geromichalos, Kok Seang Tan, Nanxiang Li, Christopher
Eriksen, Shakiba Yaghoubi, Noushin Mehdipour, Gianmarco Bernasconi, Whye Kit Fong,
YiluanGuo,andHolgerCaesar. Towardslearning-basedplanning: ThenuPlanbenchmarkfor
real-worldautonomousdriving. InProc.IEEEInternationalConf.onRoboticsandAutomation
(ICRA),2024.
[27] Alex Kendall, Jeffrey Hawke, David Janz, Przemyslaw Mazur, Daniele Reda, John Mark
Allen,VinhDieuLam,AlexBewley,andAmarShah. Learningtodriveinaday. arXiv.org,
abs/1807.00412,2018.
[28] NormanLehtomaki,NilsSandell,andMichaelAthans. Robustnessresultsinlinear-quadratic
gaussianbasedmultivariablecontroldesigns. IEEETrans.onAutomaticControl(TAC),1981.
[29] Quanyi Li, Zhenghao Peng, Lan Feng, Qihang Zhang, Zhenghai Xue, and Bolei Zhou.
Metadrive: Composing diverse driving scenarios for generalizable reinforcement learning.
IEEETrans.onPatternAnalysisandMachineIntelligence(PAMI),45(3):3461–3475,2022.
[30] ZhenxinLi,KailinLi,ShihaoWang,ShiyiLan,ZhidingYu,YishenJi,ZhiqiLi,ZiyueZhu,Jan
Kautz,ZuxuanWu,Yu-GangJiang,andJoseM.Alvarez. Hydra-mdp: End-to-endmultimodal
planningwithmulti-targethydra-distillation. arXiv.org,2024.
[31] Zhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie, Chonghao Sima, Tong Lu, Yu Qiao, and
JifengDai. BEVFormer: Learningbird’s-eye-viewrepresentationfrommulti-cameraimages
viaspatiotemporaltransformers. InProc.oftheEuropeanConf.onComputerVision(ECCV),
2022.
[32] ZhiqiLi,ZhidingYu,ShiyiLan,JiahanLi,JanKautz,TongLu,andJoseM.Alvarez. Isego
statusallyouneedforopen-loopend-to-endautonomousdriving? InProc.IEEEConf.on
ComputerVisionandPatternRecognition(CVPR),2024.
[33] YingziMa,YulongCao,JiachenSun,MarcoPavone,andChaoweiXiao. Dolphins:Multimodal
languagemodelfordriving. arXiv.org,2312.00438,2023.
[34] SivabalanManivasagam,IoanAndreiBârsan,JingkangWang,ZeYang,andRaquelUrtasun.
Towardszerodomaingap: Acomprehensivestudyofrealisticlidarsimulationforautonomy
testing. InProc.oftheIEEEInternationalConf.onComputerVision(ICCV),pages8272–8282,
2023.
[35] DeanPomerleau. ALVINN:anautonomouslandvehicleinaneuralnetwork. InAdvancesin
NeuralInformationProcessingSystems(NeurIPS),1988.
[36] AdityaPrakash,KashyapChitta,andAndreasGeiger. Multi-modalfusiontransformerforend-
to-endautonomousdriving. InProc.IEEEConf.onComputerVisionandPatternRecognition
(CVPR),2021.
[37] RajeshRajamani. Vehicledynamicsandcontrol. SpringerScience&BusinessMedia,2011.
[38] Katrin Renz, Kashyap Chitta, Otniel-Bogdan Mercea, Sophia Koepke, Zeynep Akata, and
AndreasGeiger. Plant: Explainableplanningtransformersviaobject-levelrepresentations. In
Proc.Conf.onRobotLearning(CoRL),2022.
[39] AbbasSadat,MengyeRen,AndreiPokrovsky,Yen-ChenLin,ErsinYumer,andRaquelUrtasun.
Jointly learnable behavior and trajectory planning for self-driving vehicles. In Proc. IEEE
InternationalConf.onIntelligentRobotsandSystems(IROS),2019.
[40] AbbasSadat,SergioCasas,MengyeRen,XinyuWu,PranaabDhawan,andRaquelUrtasun.
Perceive,predict,andplan:Safemotionplanningthroughinterpretablesemanticrepresentations.
InProc.oftheEuropeanConf.onComputerVision(ECCV),2020.
12[41] AxelSauer,NikolaySavinov,andAndreasGeiger. Conditionalaffordancelearningfordriving
inurbanenvironments. InProc.Conf.onRobotLearning(CoRL),2018.
[42] OliverScheel,LucaBergamini,MaciejWolczyk,Błaz˙ejOsin´ski,andPeterOndruska. Urban
driver: Learningtodrivefromreal-worlddemonstrationsusingpolicygradients. InProc.Conf.
onRobotLearning(CoRL),2021.
[43] Hao Shao, Letian Wang, RuoBing Chen, Hongsheng Li, and Yu Liu. Safety-enhanced au-
tonomous driving using interpretable sensor fusion transformer. In Proc. Conf. on Robot
Learning(CoRL),2022.
[44] Hao Shao, Letian Wang, Ruobing Chen, Steven L. Waslander, Hongsheng Li, and Yu Liu.
Reasonnet: End-to-enddrivingwithtemporalandglobalreasoning. InProc.IEEEConf.on
ComputerVisionandPatternRecognition(CVPR),2023.
[45] ChonghaoSima,KatrinRenz,KashyapChitta,LiChen,HanxueZhang,ChengenXie,PingLuo,
AndreasGeiger,andHongyangLi. DriveLM:Drivingwithgraphvisualquestionanswering.
arXiv.org,2312.14150,2023.
[46] SebastianThrun,MichaelMontemerlo,HendrikDahlkamp,DavidStavens,AndreiAron,James
Diebel,PhilipFong,JohnGale,MorganHalpenny,GabrielHoffmann,KennyLau,CeliaM.
Oakley, Mark Palatucci, Vaughan R. Pratt, Pascal Stang, Sven Strohband, Cedric Dupont,
Lars-ErikJendrossek,ChristianKoelen,CharlesMarkey,CarloRummel,JoevanNiekerk,Eric
Jensen,PhilippeAlessandrini,GaryR.Bradski,BobDavies,ScottEttinger,AdrianKaehler,
AraV.Nefian,andPamelaMahoney. Stanley: TherobotthatwontheDARPAgrandchallenge.
JournalofFieldRobotics(JFR),23(9):661–692,2006.
[47] AdamTonderski,CarlLindström,GeorgHess,WilliamLjungbergh,LennartSvensson,and
ChristofferPetersson. NeuRAD:Neuralrenderingforautonomousdriving. InProc.IEEEConf.
onComputerVisionandPatternRecognition(CVPR),2024.
[48] Martin Treiber, Ansgar Hennecke, and Dirk Helbing. Congested traffic states in empirical
observationsandmicroscopicsimulations. PhysicalreviewE,2000.
[49] Tsun-HsuanWang,AlexanderAmini,WilkoSchwarting,IgorGilitschenski,SertacKaraman,
and Daniela Rus. Learning interactive driving policies via data-driven simulation. In Proc.
IEEEInternationalConf.onRoboticsandAutomation(ICRA),2022.
[50] XinshuoWeng,BorisIvanovic,YanWang,YueWang,andMarcoPavone. Para-drive: Paral-
lelizedarchitectureforreal-timeautonomousdriving. InProc.IEEEConf.onComputerVision
andPatternRecognition(CVPR),2024.
[51] Moritz Werling, Julius Ziegler, Sören Kammel, and Sebastian Thrun. Optimal trajectory
generationfordynamicstreetscenariosinafrenetframe. InProc.IEEEInternationalConf.on
RoboticsandAutomation(ICRA),2010.
[52] Penghao Wu, Xiaosong Jia, Li Chen, Junchi Yan, Hongyang Li, and Yu Qiao. Trajectory-
guidedcontrolpredictionforend-to-endautonomousdriving: Asimpleyetstrongbaseline. In
AdvancesinNeuralInformationProcessingSystems(NeurIPS),2022.
[53] ZhenjieYang,XiaosongJia,HongyangLi,andJunchiYan. LLM4Drive: Asurveyoflarge
languagemodelsforautonomousdriving. arXiv.org,2311.01043,2023.
[54] TengjuYe,WeiJing,ChunyongHu,ShikunHuang,LingpingGao,FangzhenLi,JingkeWang,
KeGuo,WencongXiao,WeiboMao,etal. Fusionad: Multi-modalityfusionforpredictionand
planningtasksofautonomousdriving. arXiv.org,2023.
[55] WenyuanZeng,WenjieLuo,SimonSuo,AbbasSadat,BinYang,SergioCasas,andRaquel
Urtasun. End-to-endinterpretableneuralmotionplanner. InProc.IEEEConf.onComputer
VisionandPatternRecognition(CVPR),2019.
13[56] Jiang-Tian Zhai, Ze Feng, Jinhao Du, Yongqiang Mao, Jiang-Jiang Liu, Zichang Tan, Yifu
Zhang,XiaoqingYe,andJingdongWang. Rethinkingtheopen-loopevaluationofend-to-end
autonomousdrivinginnuscenes. arXiv.org,2305.10430,2023.
[57] JimuyangZhang,ZanmingHuang,andEshedOhn-Bar. Coachingateachablestudent. InProc.
IEEEConf.onComputerVisionandPatternRecognition(CVPR),2023.
[58] YunsongZhou,LinyanHuang,QingwenBu,JiaZeng,TianyuLi,HangQiu,HongziZhu,Minyi
Guo,YuQiao,andHongyangLi. Embodiedunderstandingofdrivingscenarios. arXiv.org,
2403.04593,2024.
14