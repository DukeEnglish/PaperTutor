Gradient-Mask Tuning Elevates the Upper Limits of LLM Performance
HaolingLi1,2*,XinZhang2†,XiaoLiu2,YeyunGong2,YifanWang1,
YujiuYang1†,QiChen2,PengCheng2
1TsinghuaUniversity,2MicrosoftResearch
li-hl23@mails.tsinghua.edu.cn xinzhang3@microsoft.com
Abstract Considering the above concerns, delta tuning
(Ding et al., 2023), a fine-tuning framework that
Large language models (LLMs) have revolu- diminishes redundancy by updating only a por-
tionizedlotsoffieldsofresearch. Althoughit tion of the model parameters, has been proposed.
iswell-knownthatfine-tuningisessentialfor
Compared to vanilla fine-tuning, existing delta
enhancing the capabilities of LLMs, existing
tuning methods can be classified into two main
researchsuggeststhatthereispotentialredun-
categories based on implementation. The first is
dancyinthefine-tuningprocessandtherefore
proposes to update only a subset of parame- to randomly reset some of the parameters in the
ters. However, these methods fail to lever- fine-tunedmodelcheckpointstothoseintheorigi-
age the task-specific information to identify nalmodelcheckpointsafterfine-tuning(Yuetal.,
important parameters during training. Based 2023a),whichisalsocalledone-offdrop. Thesec-
ontheinsightthatgradientsinherentlycontain
ond is to selectively update only a subset of the
informationontask-specificdata,wepropose
whole parameters during the model training pro-
Gradient-MaskTuning(GMT),amethodthat
cessbasedoneitherrandomselection(Leeetal.,
selectivelyupdatesparametersduringtraining
2019; Hui et al., 2024) or predefined rules (Za-
based on their gradient information. Specifi-
cally, we compute the absolute values of the ken et al., 2022; Hu et al., 2023, 2021; Fu et al.,
gradients and apply masking to those with 2023). Althoughthesemethodsshowthepotential
relativelysmallermagnitudes. Ourempirical ofachievingbetterresultsthanvanillafine-tuning
results across various tasks demonstrate that and involve fewer changes to the original param-
GMT not only outperforms traditional fine-
eters, these is a problem that cannot be ignored
tuningmethodsbutalsoelevatestheupperlim-
thatthesemethodstunethefixedsetofparameters
itsofLLMperformance. Furtheranalysisindi-
withoutleveraginginformationfromtask-specific
catesthatGMTexhibitsinsensitivitytomask
ratio and possesses computational efficiency data,leadingtosub-optimaloptimizationtarget.
comparabletovanillaSFT. To mitigate this problem, an appropriate pa-
rameterselectionmethodduringtrainingthatcan
1 Introduction fullyexploittheinformationoftask-specificdata
isdesired. Sincethefactthatthedeltaparameters
Largelanguagemodels(LLMs)havepivotedthe
(i.e., the disparity between fine-tuned and pre-
centerpieceofvarioustaskssuchastextualunder-
trainedparameters)aretheoverallaccumulationof
standing,naturallanguageplanningandinstruction
gradientsduringtraining,itisintuitivethatusing
following(Achiametal.,2023;Chowdheryetal.,
the magnitude of gradient will lead to an finer-
2023;Touvronetal.,2023;Jiangetal.,2023). Al-
graineddroppingeffect. Inthiswork,wepresent
though it is well-known that fine-tuning is essen-
theGradient-MaskTuning(GMT)toselecttheset
tialtoenhancetheabilityofLLMsondownstream
ofparameterstobeupdatedduringtrainingbased
tasks,existingworks(Liuetal.,2024a;Yadavetal.,
on the gradient information associated with the
2024;Wangetal.,2024a)havedemonstratedthat
task-specific data. In a nutshell, after calculating
potentialredundancywillbeleveragedduringfine-
thegradientsinthebackwardprocess,wemeasure
tuning,suggestingthatnotallparametersneedto
the absolute values of gradients and mask those
beupdatedforLLMs.
withsmallvalues. Thereasonswechoosethemag-
nanimityofgradientaretwofolds. First,gradient
* WorkdoneduringhisinternshipatMicrosoft.
† Correspondingauthor. naturallyconveystheinformationoftask-specific
4202
nuJ
12
]IA.sc[
1v03351.6042:viXradata and is a by-product of without additional 2.1 Preliminary
computationsduringtraining. Second,webelieve
GivenamodelparameterizedbyΘ = {θ },where
ij
that different network parameters vary in their
iandj denotetheindexofasingleparameter,we
importancefordifferenttrainingdata. Thegradient
defineabinarymaskmatrixM = {M ∈ {0,1}}
ij
is a well-suited criterion for determining the im-
withthesamesizeasΘ. Deltatuningfocuseson
portanceoftheseparameters,whichleadstofiner-
updating an elaborately selected sparse subset of
grainedcontroloftheentirefine-tuningprocess.
themodel’sparameters,denotedasΘ = {θ ∈
∆ ij
Toevaluatetheeffectivenessof GMT,wepro- Θ|M = 1}, where M = 1 indicates selection
ij ij
videtheoreticalanalysisandconductexperiments for update and M = 0 indicates the parameter
ij
withvariousbasemodelsonseveralwidely-used remains unchanged. The loss function L(Θ) is
benchmarks on math reasoning, code generation, computedbasedontheentiresetofparametersΘ.
andgeneralinstructionfollowing. Theexperimen- In this way, the parameter update at iteration t
talresultsdemonstratethatGMTexploitsthegra- canbestatedas:
dient information to identify more important net-
workparametersforsparseupdatingwithaccept-
(cid:40)
θ(t)−η·∇ L(Θ), ifM =1
ableextratimespent,whichisbettercomparedto θ(t+1) = ij θij ij (1)
ij θ(t), else
several representative baselines including vanilla ij
fine-tuning,one-offdrop,randommaskandsoon.
whereηdenotesthelearningrate,and∇ L(Θ)is
Furthermore, it illustrates that GMT is not sensi-
θij
thegradientofthelossfunctionwithrespecttothe
tive to mask ratio selection and is more effective
parameter θ . This update is selectively applied
ij
thanrandomparameterupdating. Wefurtherana-
onlytotheparametersincludedinΘ ,asindicated
∆
lyzethecomputationalFLOPsandtimeefficiency
bythemaskmatrixM.
ofGMT,demonstratingthatitenablesaplug-and-
playreplacementofvanillaSFTwithoutdestroying 2.2 Gradient-MaskTuning
thenetworkarchitecture.
Toincorporatetheinformationoftask-specificdata
Ourcontributionscanbesummarizedas:
duringtraining,weproposeGMTtoselectimpor-
• We propose GMT, which masks the gradi- tantparameters,leadingtoafiner-grainedlevelof
ents with small absolute values and discrim- parameterselection.
inates the importance of parameters during Foreachbatchoftrainingdata,wecomputethe
training,naturallyutilizingtheinformationof gradientofthelossfunctionL(Θ)withrespectto
task-specificdata. the parameters. To facilitate gradient accumula-
tion,wedefineanaccumulationintervalN,which
• Weconductboththeoreticalanalysisandex- denotes the number of mini-batches over which
haustiveexperimentswithdifferentbasemod- gradientsareaccumulatedbeforeaparameterup-
els on several benchmarks comparing with dateisperformed. TheaccumulatedgradientΓ
ij
representativebaselines. Theresultsconfirm foreachparameterθ isdefinedas:
ij
theeffectivenessof GMT.
N
1 (cid:88)
Γ = ∇ L(Θ,B ) (2)
• We further demonstrate the adaptability, ro- ij N θij n
n=1
bustness and efficiency of the GMT by ana-
where B represents the n-th mini-batch of data
lyzingthedropstrategy,maskratioandtime n
and∇ L(Θ,B )isthegradientofthelossfunc-
efficiency. θij n
tionwithrespecttoθ forthemini-batch.
ij
UponaccumulatingthegradientsoverN mini-
2 Methodology batches,weemploygradientinformationasasig-
nal to identify the importance of parameters at
Inthissection,weextendthedescriptionofdelta the element-wise level in a fine-grained manner.
tuningtofacilitatefurtherderivationandthende- Thisoperationinvolvessortingthecomponentsof
tailthemathematicaldescriptionofGMT.Figure1 each accumulated gradient Γ by their absolute
ij
shows the framework of GMT, including a com- valuesandselectingapredefinedtoppercentilek
parisonbetweentheone-offdeltaparameterdrop forupdatingtheparameters. Themaskedgradient
strategyandtheproposed GMT. M(Γ ,k)iscalculatedas:
ijFigure1:IllustrationofourproposedmethodGMT,comparedwiththeone-offdropdeltaparametersapproach.The
figureontheleftdelineatesthedistinctionbetweenatrivialdropandarandomdrop,whereinthetrivialdropserves
todiminishtheparameterredundancythatarisesduringthefine-tuningprocess. Buildinguponthisinsight,we
refinethetrainingprocedurebypreferentiallyupdatingmoresignificantparameters,asdeterminedbythegradient
informationpertinenttothetask-specificdata. Thisselectiveupdatingisoperationalizedthroughtheimplementation
ofamaskingstrategythatfiltersoutgradientswithsmallerabsolutevalues.
Algorithm1Gradient-MaskTuning
M(Γ ij,k)={g ij |g ij ∈Γ ij, |g ij|≥T k} (3) Hyperparameters: TheaccumulationintervalN,
thetrainingstepT andthelearningrateη.
where g represents the ij-th component of the
ij Input: TheinitialmodelparametersΘ(0) andthe
accumulatedgradientforparameterθ . Thethresh-
ij
n-thmini-batchofdataB .
old T is the value such that the absolute values n
k Output: TheupdatedmodelparametersΘ(T).
of the components of Γ that are greater than or
ij
equal to T fall within the top k percentile of all 1: fortin0 → T −1do
k
componentsbymagnitude. 2: Γ ← 0
3: fornin1 → N do
The subsequent parameter update step utilizes
themaskedgradientM(Γ ,k):
4: Γ ← Γ+∇ ΘL(Θ(t),B n)
ij
5: endfor
θ(t+1) =θ(t)−η·M(Γ ,k) (4) 6: Γ ← 1Γ
ij ij ij N
7: T k ← PercentilethresholdbasedonΓ
whereη isthelearningrateandtindexesthecur-
8: M(Γ,k) ← {g ij |g ij ∈ Γ ij, |g ij| ≥ T k}
renttrainingstep. Thedetailedalgorithmictraining
9: forθ ij ∈ Θdo
procedureisgiveninAlgorithm1.
10: if|Γ ij| ≥ T k then
Compared to the existing work on Parameter-
(t+1) (t)
EfficientFine-Tuning(PEFT)(Xuetal.,2023;Han 11: θ ij ← θ ij −η·Γ ij
12: endif
etal.,2024),whichfocusesonreducingcomputa-
13: endfor
tionandimprovingefficiency,GMTaimstoiden-
14: endfor
tifyimportantneuronsinamodelandelevatethe
upper limits of its performance through selective
parameterupdating.
functioninAppendixA.Then,wediscussthethe-
oreticalanalysisofselectingtheabsolutevalueof
3 TheoreticalAnalysis
thegradientasusedin GMT.
We present the theoretical analysis in two parts. Inordertoconsidertheimpactoftask-specific
First,weexplainwhysparsityisfeasiblefroman data on the importance of each parameter in a
optimizationperspective(Fuetal.,2023;Huietal., model,weidentifyandupdateonlyasparsesubset
2024)bydemonstratingthatdeltatuningwithspar- of parameters based on their impact on the loss
sityoptimizesanupperboundontheoriginalloss function. WedefinethelossfunctionL(Θ;D)overthedatasetD as:
(cid:12) (cid:12)
n
s
ij
=(cid:12)∇ θijL(Θ;D)(cid:12) (8)
1 (cid:88)
L(Θ;D)=
n
ℓ(Θ;(x i,y i)) (5) Usingthissaliencymeasure,weconstructthebi-
i=1
nary mask matrix M such that M = 1 if the
ij
whereℓrepresentsthelossforasingledatapoint,
parameterθ isdeemedimportantbasedonapre-
ij
withx andy denotingtheinputfeaturesandcor-
i i definedsparsitylevelκ:
respondinglabel,respectively. Thefullparameter
setisdenotedbyΘ. M ij =I[s ij ≥s˜ (κ)] (9)
Todiscerntheimpactofindividualparameters
wheres˜ isthesaliencythresholdselectedtoen-
(κ)
θ onthelossfunctionL(Θ;D),weconsiderthe
ij surethatonlythetopκinfluentialparametershave
impact of their removal while keeping all other
their corresponding entries in M set to one. The
parametersconstant. Thedifferentialeffectofex- indicator function I[·] yields one if the condition
cludingaparameterθ isquantifiedbythechange
ij withinthebracketsistrueandzerootherwise.
inloss∆L (Θ;D),whichisformulatedas:
ij
4 Experiments
∆L ij(Θ;D)=L(I⊙Θ;D)−L((I−E ij)⊙Θ;D) (6) 4.1 ExperimentalSetup
Wethoroughlyevaluatethetask-specificcapabili-
whereIrepresentstheidentitymatrixandE de-
ij
tiesofmodelsthroughvarioustasksincodegener-
notesanindicatormatrixthathasthesamedimen-
ation,mathreasoningandthegeneraldomain.
sionsasΘ. InE ,allelementsarezeroexceptfor
ij
the (i,j) element, which is one. The Hadamard
TrainingandEvaluation. Forcodegeneration,
product,indicatedby⊙,performsanelement-wise
we employ the Magicoder-Evol-Instruct-110K
multiplication,isolatingtheeffectofthesinglepa-
(Wei et al., 2023) as the training data, which is a
rameterθ ij. decontaminated version of evol-codealpaca-v12.
Given the computational infeasibility of eval-
We utilize MISTRAL-7B (Jiang et al., 2023)
uating ∆L for each parameter, we invoke the
ij and DEEPSEEK-CODER-BASE-6.7B (Guo
first-orderTaylorseriesexpansionaroundthecur-
et al., 2024) as the base models. The trained
rent parameter vector Θ, which provides a linear
modelsareevaluatedusingtheHumanEval(Chen
approximation of the loss function’s behavior in
et al., 2021) and MBPP (Austin et al., 2021)
the vicinity of Θ. The first-order approximation
benchmarks, which are widely recognized for
isrepresentedbythegradientofthelossfunction
their effectiveness in measuring the proficiency
withrespecttoθ :
ij of Python text-to-code generation. To enable a
morecomprehensiveevaluation,wealsointroduce
∆L (Θ;D)≈∇ L(Θ;D)·(−θ ) (7)
ij θij ij
HumanEval+ and MBPP+, both of which are
where∇ L(Θ;D)denotesthepartialderivative
θij providedbyEvalPlus(Liuetal.,2024b).
ofthelossfunctionconcerningtheparameterθ .
ij Formathreasoning,theMetaMathQA (Yuetal.,
Thenegativesignarisesfromthefactthatweare
2023b) dataset is employed to fine-tune on the
considering the removal of the parameter, which MISTRAL-7B and LLAMA3-8B3 models. The
correspondstoanegativeperturbationinitsvalue.
evaluationisconductedusingtheGSM8k(Cobbe
Ourobjectiveistodiscerntheimportantparam-
et al., 2021) and MATH (Hendrycks et al., 2021)
eterswithinthenetworkarchitecturethatarerele-
benchmarks,whicharespecificallyconstructedto
vanttotask-specificdata. Tothisend,weemploy
testthemodel’scapacityformathematicalreason-
themagnitudeofthegradient∇ asthecriterion
θij ingandproblem-solving.
forsaliency. Itisimperativetorecognizethatahigh
For the general domain, the TÜLU V2 (Wang
magnitude of the gradient (regardless of its sign)
et al., 2024b) dataset is utilized in training on
typicallydenotesthattheparameterθ exertsasub-
ij the LLAMA2-7B (Touvron et al., 2023) and
stantialinfluenceonthelossfunction,whetherthe
LLAMA2-13Bmodel. FollowingHFT(Huietal.,
effectispositiveornegative. Consequently,such
2024),weevaluatemodelonMMLU(Hendrycks
parametersmustbepreservedtofacilitatelearning
1https://evalplus.github.io/leaderboard.html
onthecorrespondingweights. Theabsolutevalue
2https://huggingface.co/datasets/
ofthegradient∇ isthenusedasasaliencymea-
θij theblackcat102/evol-codealpaca-v1
suretodeterminetheimportanceoftheparameter: 3https://github.com/meta-llama/llama3Method HumanEval HumanEval+ MBPP MBPP+ Average GSM8k MATH
MISTRAL-7B MISTRAL-7B
Pre-trained† 28.7 23.8 51.9 42.1 36.6 - -
SFT 68.3 64.0 56.1 46.9 58.8 75.3 27.0
One-offDrop 68.3 64.0 54.6 46.1 58.3 74.6 27.6
HFT 67.1 61.6 57.1 48.1 58.5 77.8 27.3
RandomMask 70.7 64.6 55.1 45.4 59.0 74.6 25.1
GMT 69.5(+1.2) 62.2(-1.8) 59.6(+3.5) 48.6(+1.7) 60.0(+1.2) 78.6(+3.3) 28.5(+1.5)
DEEPSEEK-CODER-BASE-6.7B LLAMA3-8B
Pre-trained† 47.6 39.6 72.0 58.7 54.5 - -
SFT 76.8 73.8 74.9 62.4 72.0 78.1 29.2
One-offDrop 76.2 72.6 74.7 62.4 71.5 78.1 29.4
HFT 74.4 70.1 75.2 62.9 70.7 74.2 28.5
RandomMask 72.6 67.1 78.8 67.5 71.5 80.3 31.3
GMT 78.0(+1.2) 74.4(+0.6) 75.7(+0.8) 63.7(+1.3) 73.0(+1.0) 82.0(+3.9) 32.0(+2.8)
Table 1: Experimental results for a single task in a specific domain. All models are evaluated with zero-shot
prompting. †WeobtaintheresultsofthecodebenchmarkfromEvalPlus1andtheresultsofthemathbenchmark
fromthecorrespondingoriginalpapers. “-”denotesthatnozero-shotresultswereofficiallyreported. Boldtext
indicatesthebestresultsforthefine-tunedmodeloneachbenchmark. Anindicationofthedecreaseandincrease
comparedtovanillaSFTisshowningreenandred,respectively.
et al., 2020), GSM8k (Cobbe et al., 2021), datedatafine-grainedelement-wiselevel.
BBH(Suzgunetal.,2023),TyDiQA(Clarketal.,
Forfairness,wedeploythesamemaskratiofor
2020), TruthfulQA (Lin et al., 2022) and Hu-
One-off Drop, Random Mask and GMT, whose
manEval(Chenetal.,2021).
detailsareshowninAppendixB.
Implementation Details. We choose different
4.2 MainResults
base models for different tasks. All training ex-
perimentsweredoneonNvidiaA100andNvidia CodeGeneration. Theexperimentalresultsfor
H100machines. Inaddition,weutilizeBFloat16 thecodegenerationtaskarepresentedinTable1.
precisionandsettheweightdecayto0. Weusethe By integrating task-specific gradient information
cosinelearningrateschedulerafteralinearwarm- with avoidance of random parameter selection,
upstagewitharatioof0.03. GMT achievesanaverageperformanceimprove-
mentof1.2%onthe MISTRAL-7B modeland1%
Baselines. In order to thoroughly evaluate the ontheDEEPSEEK-CODER-BASE-6.7Bmodel. In
effectivenessofourmethod,wecompare GMT to contrast, HFT freezes half of the parameters to
thefollowingbaselines: maintaintheoriginalcapabilitiesofthemodel,and
• SFT:Vanillasupervisedfine-tuning. thelearningprocessishamperedbythelackofsuf-
ficientparameterupdatesforaspecifictaskdomain.
• One-offDrop: Asanextensionof(Yuetal.,
Furthermore, the random mask approach shows
2023a),droppingapresetratiooftrivialdelta
somebenefitsofusingsparsityonthe MISTRAL-
parameters after the vanilla supervised fine-
7B.However,itfailstomaintainthisperformance
tuning.
onanothermodel,suggestingthatitlacksrobust-
• HFT4 (Hui et al., 2024): Half of the param- nessduetoitsinabilitytoutilizetask-specificdata
eters are selected for learning the new task, information. ComparedtoSFT,aOne-offDropdi-
andtheotherhalfisfrozentoretainprevious rectlyonafine-tunedmodeldoesnotimproveper-
knowledge. formanceduetoitssensitivitytotheoptimaldrop
• RandomMask: Duringtraining,apresetra- ratio. Inthehighlyprofessionalfieldofcodegen-
tio of the delta parameters is randomly up- eration,thisimprovementhighlightstheefficacyof
GMT in optimizing model performance through
4Theresultsofdomain-specificHFTexperimentsformath
judiciouslyfine-grainedselectionofparameters.
reasoningandcodegenerationarereproducedbyus. Since
itisnotobviouswhichparameterselectionstrategyisbetter
MathReasoning. Table1presentsacomparative
forHFT,weapplyHFTinallparametersofthemodelusinga
category-levelparameterselectionstrategy. analysisoftheGMTmethodagainstvariousbase-lineapproaches. GMToutperformsothermethods
byleveraginggradientinformationtocapitalizeon
sparsityindependentoftheinconsistenciesassoci-
atedwithrandommethods. Particularly, GMTper-
formsexcellentlyontheGSM8kbenchmark,out-
performingSFTby3.3%and3.9%onbothmodels.
In contrast, the HFT method shows an improve-
mentoverthebaselineon MISTRAL-7B.However,
itperformsterriblyon LLAMA3-8B,showingthe
Figure 2: Fine-tuning performance of the proposed
instabilityofthemethod,whichisattributedtothe
GMTwithrespecttovariousmaskratiosduringtraining
strategyofrandomlychosenparameters. One-off
inthreedomains. ThetrainingLLMsutilizedforcode
Drop is slightly improved on the MATH bench-
generationtask, mathreasoningtask, andgeneraldo-
markbutnotaseffectiveastheGMTmethodthat mainareDEEPSEEK-CODER-BASE-6.7B,MISTRAL-
performsthedropoperationthroughoutthetraining 7B,andLLAMA2-7B,respectively. Theexperimental
process. Insummary,thisstrategicuseofgradient resultsfortheMATHbenchmarkarepresentedonthe
signalassociatedwiththetask-specificdataensures secondaryy-axislocatedontherightsideofthefigure.
stableandprogressiveperformanceimprovements
throughout the learning process, as evidenced by
For extreme cases, experiments were performed
itssuccessacrossdifferentmodels.
with mask ratios of 95% and 99%. As shown in
Figure 2, the experimental results indicate that
GeneralDomain. Table2showstheexperimen-
despite fluctuations in the number of parameter
tal comparison of the proposed GMT with the
updates, the proposed method maintains a rela-
baseline. In the experiments with the fine-tuned
tivelyconsistentperformancelevelwithnomore
LLAMA2-7B model, the GMT method outper-
than 90% mask ratio. Our GMT reveals that the
formstheSFTby3.0%onaverageacrossalltasks,
optimal performance in fields like mathematics
and in particular, it significantly leads by 13.0%
andcoding,whichrequirespecializedknowledge,
onGSM8k. GMTlikewisedemonstratedsuperior
isachievedwithmaskratiosof20%to40%. Even
performance than HFT in multitasking scenarios
underextrememaskratiosettings,GMTmaintains
in the general domain, obtaining an average lead
impressive performance in task-specific domains
of1.1%. AftertheexpansionoftheLLMsize,the
such as math reasoning and code generation.
GMT maintainsitsperformance,witha3.3%im-
However,itseffectivenesssignificantlydiminishes
provementinfine-tuningperformanceofLLAMA2-
inthegeneraldomainwithinmulti-taskscenarios.
13B compared to the vanilla SFT. Notably, both
This suggests a stronger adaptability of GMT to
RandomMaskandHFTperformbetterthanSFT,
single-task scenarios, where the entire learning
suggestingthatthegeneraldomainofmulti-tasking
processcanbecompletedwithmerelya1%param-
benefitsfromappropriatesparsity,evenwhenthe
eterupdate. Conversely,inmulti-tasksettings,the
parameterselectionstrategyiscompletelyrandom.
potentiallackofsufficientparameterupdatesmay
Withthesameamountofparameterupdates,GMT
lead to an inadequate learning process. Further
shows further improvement over Random Mask,
analysisdemonstratesthattheperformanceofthe
suggestingthatthestrategyofselectingparameters
proposed GMTexhibitsrobustnesstovariationsin
based on gradient information derived from task-
themaskratio,despitethelatterbeinganadjustable
specificdatainourmethodispracticallyeffective.
hyperparameterwithinthemodel’sconfiguration.
4.3 FurtherDiscussions
AnalysisofDropStrategy. Ouranalysisreveals
Analysis of Mask Ratio. To comprehensively that not all delta parameters contribute equally
ascertain the sensitivity of the hyperparameter tothemodel’sperformance. Specifically,wefind
mask ratio selection on GMT, we analyze the thatparameterswithsalientvaluesarecriticalfor
influence of the amount of parameter updates maintaining the efficacy of the LLM, and their
during training. Comparative experiments were removalleadstoanotableandrapiddegradationin
conducted across three domains. We iterated modelfunctionality. Tosystematicallyexplorethis
through all experiments, applying the range of phenomenon,wedesignedaseriesofexperiments
parameterstobefine-tunedwith10%granularity. using the MISTRAL-7B model, focusing onMMLU GSM8k BBH TyDiQA TruthfulQA HumanEval
Model Method Average
(0-shot,EM) (8-shotCoT,EM) (3-shotCoT,EM) (1-shot,F1) (0-shot,MC2) (0-shot,Pass@10)
Pre-trained 41.6 12.0 39.9 48.4 38.5 26.2 34.4
SFT 48.5 25.0 42.2 51.2 41.7 36.9 41.0
LLAMA2-7B HFT 50.8 30.5 43.6 52.3 45.4 34.6 42.9
RandomMask 47.4 34.5 44.4 52.9 47.9 33.7 43.4
GMT 47.6(-0.9) 38.0(+13.0) 43.3(+1.1) 53.1(+1.9) 47.5(+5.8) 34.6(-2.3) 44.0(+3.0)
Pre-trained 52.2 34.5 50.7 50.3 49.8 32.7 45.0
SFT 50.6 45.0 47.8 55.0 42.6 42.4 47.2
LLAMA2-13B HFT 54.5 46.5 53.7 56.7 45.7 43.5 50.1
RandomMask 54.6 50.5 52.8 56.5 45.2 41.4 50.2
GMT 54.6(+4.0) 54.0(+9.0) 51.5(+3.7) 57.1(+2.1) 46.0(+3.4) 39.5(-2.9) 50.5(+3.3)
Table2: Resultsof LLAMA2-7B and LLAMA2-13B modelsfinetunedonthe TÜLU V2dataset. Theresultsof
Pre-trained,SFT,andHFTaretakenfromHuietal.(2024). Boldtextindicatesthebestresultsforthefine-tuned
modeloneachbenchmark. AnindicationofthedecreaseandincreasecomparedtovanillaSFTisshowningreen
andred,respectively.
ofthegradientinformationtotrivialdropduringthe
trainingprocesscanelevatetheupperlimitofLLM
performanceandisnotconstrainedbydroprate.
Suchexperimentsdemonstratethattheparame-
terupdatesduringthefine-tuningprocessofLLM
are redundant, and that the use of a reasonable
sparse updating strategy enables the model to
learn better. Comparison with the experiments
of random drop indicates that our strategy of
Figure3: Weemployedthreedistinctstrategiesforse-
considering task-specific data during training,
lectivelydroppingdeltaparametersatvariousratesde-
using gradient information as a signal, and
pendingonthemagnitude(absolutevalue)ofthedelta
retainingneuronswithlargeupdatemagnitudesfor
parameters: 1)preferentiallydroppingsalientparame-
ters,2)preferentiallydroppingtrivialparameters,and completingtheupdateiseffective.
3)droppingparametersrandomly.
TraningSpeed FLOPs
Model Method
(samples/s) (1e18)
domain-specific applications in mathematics VanillaSFT 15.32 1.481
MISTRAL-7B
RandomMask 13.37 1.481
(GSM8kandMATH).Theexperimentalresultsare CodeGeneration
GMT 13.43 1.480
depictedinFigure3.
VanillaSFT 13.53 1.706
LLAMA2-13B
Our results indicate substantial differences in RandomMask 12.54 1.707
GeneralDomain
theimpactofeachstrategyonmodelperformance. GMT 12.74 1.707
Thestrategyofdroppingaportionofthedeltapa-
Table 3: Comparison of training speed and computa-
rameters with small absolute values after vanilla
tionalFLOPs,thenumberoftrainsamplespersecond
fine-tuning demonstrates a relatively robust per- isanindicatoroftrainingspeed.
formance, only showing significant degradation
whenthedropoutratereaches80%. Thissuggests
AnalysisofEfficiency. Todemonstratethetime-
potentialbenefitsfromsparsification, whichmay
efficiencyofproposedGMT,wecomparethetrain-
enhanceperformancebyreducingoverfittingorre-
ingspeedandcomputedFLOPs5 of GMT,vanilla
movingnoise. Incontrast,therandomdropstrategy
SFT,andRandomMask,withthemetricofnumber
leadstoanapparentperformancedeclineatamuch
of training samples per second responding to the
lowersparsitylevelof40%. Thestrategyofdrop-
trainingspeed,andtheresultsareshowninTable3.
pingparameterswithsalientmagnitudesresultsin
TheFLOPscomputedbythethreetuningstrategies
a precipitous decline in model capabilities. This
duringtrainingarequitecomparableduetothefact
findingunderscorestheimportanceofthesesignif-
thatthegradientinformationutilizedby GMT is
icantdeltaparametersinsustainingthefunctional
integrityofthemodel. Expectedly,theutilization 5Alldataloggingisfromhttps://wandb.ai.a by-product of model training and does not im- In-trainingStrategy Thein-trainingStrategyis
poseadditionalderivationoperations. Intermsof to select a subset of parameters or additional ex-
trainingspeed,therandommaskisclosetoGMT, tensionsinsteadoffullparameterfine-tuning. The
being14%and6%slowerthanvanillaSFTinthe Low-Rank Adaptation (LoRA) (Hu et al., 2021)
codegenerationandgeneraldomain,respectively. leveragestrainablerank-decompositionmatricesto
SinceGMTneedstocalculatethethresholdofthe facilitatethefine-tuningprocess. Prefix-tuning(Li
gradient being masked based on a preset ratio, a andLiang,2021)adjuststhePLMbyupdatingaset
process that requires a TopK operation after the ofprefixedparametersineverytransformerlayer
modelbackpropagatesthegradient,andthendrops thatarecloselyrelatedtothetask,significantlyre-
valueswithsmallerabsolutevaluesofthegradient ducingtrainingcostsandenhancingperformance
basedonthethreshold. Nonetheless,thistrade-off across multiple tasks. Adapter (Houlsby et al.,
betweenasmallminortimeoverheadandperfor- 2019; Zhang et al., 2023; Hu et al., 2023) mod-
manceimprovementiscompletelyacceptable. ules add small, trainable networks to each LLM
layer, allowing specific fine-tuning while main-
taining existing parameters’ stability. Moreover,
5 RelatedWorks
methodssuchasMasking(Zhaoetal.,2020),Diff-
Pruning (Guo et al., 2021), BitFit (Zaken et al.,
Delta Tuning (DT) (Hu et al., 2022; Ding et al.,
2022)andAdaLoRA(Zhangetal.) havebeenpro-
2023)standsoutforitsefficacyintuningasubsetof
posed,whichidentifyasubsetoftrainableparame-
theLLMparameters,conservingtheintegrityofthe
terswithintheLLMtoachieveefficientfine-tuning.
predominantpre-trainedweights. Wecategorized
(Pfeiffer et al., 2023) implements computational
delta tuning into post-processing and in-training
unitstypicallyasautonomousparameter-efficient
basedonthestageofimplementation. Thissection
modules. Informationisroutedtoasubsetofthe
describesthepropertiesofthetypicalmethodsof
modulesandthenaggregated.
bothstrategiesaswellastherespectivedrawbacks.
Thesemethodologiesnotonlyexpeditetheadap-
tation process of the model to new tasks but also
Post-processing Strategy The post-processing substantially reduce the consumption of compu-
strategy,althoughrequiringtuningwithfullparam- tational resources and training time. In contrast,
eter participation, achieves the same purpose as our approach is focused on the task-specific data
in-trainingthroughaseriesofdeltaparameterdrop informationandfine-grainedtuningofparameters,
and reset strategies. Inspired by dropout, mixout whichnotonlypreservesthemodel’sgeneralizabil-
(Leeetal.,2019)stochasticallymixessourceand itybutalsosignificantlyimprovesitsadaptability
targetparametersasaregularizationtechniqueand andperformanceonspecifictasks.
extendstofine-tunedownstreamtasks. DARE(Yu
etal.,2023a)findsthatsettingmost(90%oreven 6 Conclusion
99%) of the delta parameters to zero does not di-
minish the ability to fine-tune the LLM, and ex- Inthispaper,weproposetheGradient-MaskTun-
tends to model fusion accordingly. Ties-merging ing(GMT),apragmaticapproachtodeltatuning
(Yadav et al., 2024) describes the delta parame- forLLMs. Byleveraginggradientmagnitudesasso-
ter as a type of task vector that trims it, and then ciatedwithtask-specificdatatoselectivelyupdate
averagestheparametersthatareidenticalformulti- parameters, GMT strikes an optimal balance be-
plemodelaggregationsymbolstoachievemodel tween maintaining computational simplicity and
merging. ExPO(Zhengetal.,2024)isinspiredby achievingfine-grainedparametertuning. Addition-
modelinterpolation(Wortsmanetal.,2022)andob- ally, GMT identifiesmoresignificantparameters
tainsbetteralignedmodelsbydirectextrapolation andeliminatestheredundancyintroducedbytrivial
fromtheparametersofthealignedmodelobtained delta parameters in the fine-tuning. We evaluate
by DPO or RLHF and the initial SFT model. Al- the performance of GMT on multiple models in
thoughthesemethodscanbeimplementeddirectly thecodegeneration,mathreasoning,andgeneral
onfine-tunedmodelsthatareopenaccessandre- domain. Experimentsdemonstratethat GMT not
quirelittleadditionalcomputationaloverhead,they onlyelevatestheupperlimitsofLLMperformance
neverachieveoptimalperformanceduetothelack acrossdiversetasksbutalsoexhibitsrobustnessto
ofrefinementinthetrainingprocess. variousfine-tuningsettings. WefurtheranalyzethecomputationalefficiencyofGMT toconfirmthat Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei,
noadditionalcomputationalFLOPsareneeded,as Zonghan Yang, Yusheng Su, Shengding Hu, Yulin
Chen, Chi-Min Chan, Weize Chen, et al. 2023.
wellasacceptableextratimeconsumption. More-
Parameter-efficient fine-tuning of large-scale pre-
over,GMTallowsfordeltatuningwithoutdestroy-
trained language models. Nature Machine Intelli-
ingthenetworkstructure,andthuscanreadilyre- gence,5(3):220–235.
placeSFTtoaccomplishtheoptimizationprocess.
Zihao Fu, Haoran Yang, Anthony Man-Cho So, Wai
Lam,LidongBing,andNigelCollier.2023. Onthe
7 Limitations
effectivenessofparameter-efficientfine-tuning. In
Proceedings of the AAAI Conference on Artificial
This work has several limitations. First, the ex-
Intelligence,volume37,pages12799–12807.
periments in this paper could have tried more
datasets and more base models. Furthermore, a DayaGuo,QihaoZhu,DejianYang,ZhendaXie,Kai
Dong, Wentao Zhang, Guanting Chen, Xiao Bi,
well-performingLLMrequiresasequenceoftrain-
YWu,YKLi,etal.2024. Deepseek-coder:Whenthe
ing paradigms, and we have not yet considered
largelanguagemodelmeetsprogramming–theriseof
theroleof GMT inthepre-trainingandalignment codeintelligence. arXivpreprintarXiv:2401.14196.
training(e.g.,viaDPOorRLHF).Finally,thispa-
DemiGuo,AlexanderMRush,andYoonKim.2021.
per has not analyzed the effect of the amount of
Parameter-efficienttransferlearningwithdiffprun-
data on the results, i.e., the comparison of the re- ing. InProceedingsofthe59thAnnualMeetingofthe
sultsof GMT andSFTinthecaseoflimiteddata. Association for Computational Linguistics and the
11thInternationalJointConferenceonNaturalLan-
guageProcessing(Volume1: LongPapers),pages
4884–4896.
References
Zeyu Han, Chao Gao, Jinyang Liu, Sai Qian Zhang,
JoshAchiam,StevenAdler,SandhiniAgarwal,Lama
etal.2024. Parameter-efficientfine-tuningforlarge
Ahmad, Ilge Akkaya, Florencia Leoni Aleman,
models: A comprehensive survey. arXiv preprint
DiogoAlmeida,JankoAltenschmidt,SamAltman,
arXiv:2403.14608.
ShyamalAnadkat,etal.2023. Gpt-4technicalreport.
arXivpreprintarXiv:2303.08774.
DanHendrycks,CollinBurns,StevenBasart,AndyZou,
MantasMazeika,DawnSong,andJacobSteinhardt.
JacobAustin,AugustusOdena,MaxwellNye,Maarten
2020. Measuringmassivemultitasklanguageunder-
Bosma, Henryk Michalewski, David Dohan, Ellen
standing. InInternationalConferenceonLearning
Jiang,CarrieCai,MichaelTerry,QuocLe,etal.2021.
Representations.
Programsynthesiswithlargelanguagemodels. arXiv
preprintarXiv:2108.07732.
DanHendrycks,CollinBurns,SauravKadavath,Akul
Arora,StevenBasart,EricTang,DawnSong,andJa-
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming
cobSteinhardt.2021. Measuringmathematicalprob-
Yuan,HenriquePondedeOliveiraPinto,JaredKa-
lem solving with the math dataset. arXiv preprint
plan, HarriEdwards, YuriBurda, NicholasJoseph,
arXiv:2103.03874.
Greg Brockman, et al. 2021. Evaluating large
language models trained on code. arXiv preprint
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,
arXiv:2107.03374.
Bruna Morrone, Quentin De Laroussilhe, Andrea
Gesmundo,MonaAttariyan,andSylvainGelly.2019.
AakankshaChowdhery,SharanNarang,JacobDevlin,
Parameter-efficienttransferlearningfornlp. InIn-
MaartenBosma,GauravMishra,AdamRoberts,Paul
ternationalconferenceonmachinelearning,pages
Barham,HyungWonChung,CharlesSutton,Sebas-
2790–2799.PMLR.
tianGehrmann,etal.2023. Palm: Scalinglanguage
modelingwithpathways. JournalofMachineLearn- Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan
ingResearch,24(240):1–113. Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,
and Weizhu Chen. 2021. Lora: Low-rank adap-
JonathanHClark,EunsolChoi,MichaelCollins,Dan tation of large language models. arXiv preprint
Garrette, Tom Kwiatkowski, Vitaly Nikolaev, and arXiv:2106.09685.
JennimariaPalomaki.2020. Tydiqa: Abenchmark
forinformation-seekingquestionansweringintypo- ShengdingHu,ZhenZhang,NingDing,YadaoWang,
logicallydiverselanguages. TransactionsoftheAs- Yasheng Wang, Zhiyuan Liu, and Maosong Sun.
sociationforComputationalLinguistics,8:454–470. 2022. Sparsestructuresearchfordeltatuning. Ad-
vances in Neural Information Processing Systems,
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, 35:9853–9865.
MarkChen,HeewooJun,LukaszKaiser,Matthias
Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Zhiqiang Hu, Lei Wang, Yihuai Lan, Wanyu Xu, Ee-
Nakano,etal.2021. Trainingverifierstosolvemath PengLim,LidongBing,XingXu,SoujanyaPoria,
wordproblems. arXivpreprintarXiv:2110.14168. andRoyLee.2023. Llm-adapters:Anadapterfamilyforparameter-efficientfine-tuningoflargelanguage YifanWang,YafeiLiu,ChufanShi,HaolingLi,Chen
models. InProceedingsofthe2023Conferenceon Chen, Haonan Lu, and Yujiu Yang. 2024a. In-
EmpiricalMethodsinNaturalLanguageProcessing, scl: Adata-efficientcontinuallearningparadigmfor
pages5254–5276. fine-tuninglargelanguagemodelswithinstructions.
arXivpreprintarXiv:2403.11435.
TingfengHui,ZhenyuZhang,ShuohuanWang,Weiran
Xu, Yu Sun, and Hua Wu. 2024. Hft: Half fine- Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack
tuning for large language models. arXiv preprint Hessel,TusharKhot,KhyathiChandu,DavidWad-
arXiv:2404.18466. den,KelseyMacMillan,NoahASmith,IzBeltagy,
etal.2024b. Howfarcancamelsgo? exploringthe
Albert Q Jiang, Alexandre Sablayrolles, Arthur Men- state of instruction tuning on open resources. Ad-
sch,ChrisBamford,DevendraSinghChaplot,Diego vances in Neural Information Processing Systems,
delasCasas,FlorianBressand,GiannaLengyel,Guil- 36.
laumeLample,LucileSaulnier,etal.2023. Mistral
7b. arXivpreprintarXiv:2310.06825. YuxiangWei,ZheWang,JiaweiLiu,YifengDing,and
LingmingZhang.2023. Magicoder: Sourcecodeis
CheolhyoungLee,KyunghyunCho,andWanmoKang. allyouneed. arXivpreprintarXiv:2312.02120.
2019. Mixout: Effective regularization to fine-
tunelarge-scalepretrainedlanguagemodels. arXiv
MitchellWortsman,GabrielIlharco,SamirYaGadre,
preprintarXiv:1909.11299.
RebeccaRoelofs,RaphaelGontijo-Lopes,AriSMor-
cos,HongseokNamkoong,AliFarhadi,YairCarmon,
Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning:
SimonKornblith,etal.2022. Modelsoups: averag-
Optimizing continuous prompts for generation. In
ingweightsofmultiplefine-tunedmodelsimproves
Proceedingsofthe59thAnnualMeetingoftheAsso-
accuracy without increasing inference time. In In-
ciationforComputationalLinguisticsandthe11th
ternationalconferenceonmachinelearning,pages
InternationalJointConferenceonNaturalLanguage
23965–23998.PMLR.
Processing (Volume 1: Long Papers), pages 4582–
4597.
Lingling Xu, Haoran Xie, Si-Zhao Joe Qin, Xiaohui
Tao, and Fu Lee Wang. 2023. Parameter-efficient
StephanieLin,JacobHilton,andOwainEvans.2022.
fine-tuningmethodsforpretrainedlanguagemodels:
Truthfulqa: Measuring how models mimic human
A critical review and assessment. arXiv preprint
falsehoods. InProceedingsofthe60thAnnualMeet-
arXiv:2312.12148.
ingoftheAssociationforComputationalLinguistics
(Volume1: LongPapers),pages3214–3252.
PrateekYadav,DerekTam,LeshemChoshen,ColinA
Raffel,andMohitBansal.2024. Ties-merging: Re-
JamesLiu,GuangxuanXiao,KaiLi,JasonDLee,Song
solving interference when merging models. Ad-
Han,TriDao,andTianleCai.2024a. Bitdelta: Your
vances in Neural Information Processing Systems,
fine-tunemayonlybeworthonebit. arXivpreprint
36.
arXiv:2402.10193.
LeYu,BowenYu,HaiyangYu,FeiHuang,andYongbin
Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and
Li.2023a. Languagemodelsaresupermario: Ab-
LingmingZhang.2024b. Isyourcodegeneratedby
sorbingabilitiesfromhomologousmodelsasafree
chatgptreallycorrect? rigorousevaluationoflarge
lunch. arXivpreprintarXiv:2311.03099.
languagemodelsforcodegeneration. Advancesin
NeuralInformationProcessingSystems,36.
Longhui Yu, Weisen Jiang, Han Shi, YU Jincheng,
Jonas Pfeiffer, Sebastian Ruder, Ivan Vulic´, and ZhengyingLiu,YuZhang,JamesKwok,ZhenguoLi,
EdoardoMariaPonti.2023. Modulardeeplearning. AdrianWeller,andWeiyangLiu.2023b. Metamath:
arXivpreprintarXiv:2302.11529. Bootstrapyourownmathematicalquestionsforlarge
languagemodels. InTheTwelfthInternationalCon-
Mirac Suzgun, Nathan Scales, Nathanael Schärli, Se- ferenceonLearningRepresentations.
bastian Gehrmann, Yi Tay, Hyung Won Chung,
Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny EladBenZaken,YoavGoldberg,andShauliRavfogel.
Zhou,etal.2023. Challengingbig-benchtasksand 2022. Bitfit: Simpleparameter-efficientfine-tuning
whetherchain-of-thoughtcansolvethem. InFind- fortransformer-basedmaskedlanguage-models. In
ingsoftheAssociationforComputationalLinguistics: Proceedings of the60th Annual Meeting of the As-
ACL2023,pages13003–13051. sociationforComputationalLinguistics(Volume2:
ShortPapers),pages1–9.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay Qingru Zhang, Minshuo Chen, Alexander Bukharin,
Bashlykov,SoumyaBatra,PrajjwalBhargava,Shruti Pengcheng He, Yu Cheng, Weizhu Chen, and Tuo
Bhosale, et al. 2023. Llama 2: Open founda- Zhao. Adaptive budget allocation for parameter-
tion and fine-tuned chat models. arXiv preprint efficientfine-tuning. InTheEleventhInternational
arXiv:2307.09288. ConferenceonLearningRepresentations.RenruiZhang,JiamingHan,ChrisLiu,PengGao,Ao-
junZhou, XiangfeiHu, ShilinYan, PanLu, Hong-
shengLi,andYuQiao.2023. Llama-adapter: Effi- minL(Θ) ≤ maxminL (Θ,λ) = L¯. (14)
d
cientfine-tuningoflanguagemodelswithzero-init Θ λ Θ
attention. arXivpreprintarXiv:2303.16199.
Given that the regularization term ∥(I −
MengjieZhao,TaoLin,FeiMi,MartinJaggi,andHin- M)Θ ∥2 is non-negative, the dual formulation
∆
richSchütze.2020. Maskingasanefficientalterna-
doesnotexceedthevalueoftheoriginallossfunc-
tivetofinetuningforpretrainedlanguagemodels. In
tionL(Θ),therebyestablishingthedesiredupper
Proceedings of the 2020 Conference on Empirical
MethodsinNaturalLanguageProcessing(EMNLP), bound. Thisensuresthatthefine-tuningadheresto
pages2226–2241. theimposedsparsityconstraint,selectivelyupdat-
ingparametersasdictatedbythemaskM,while
Chujie Zheng, Ziqi Wang, Heng Ji, Minlie Huang,
maintainingthestabilityandintegrityoftheLLM
and Nanyun Peng. 2024. Weak-to-strong ex-
trapolation expedites alignment. arXiv preprint throughouttheadaptationprocess.
arXiv:2404.16792.
B Detailsofthetrainingmaskratio
A EffectivenessofSparsityinFine-tuning
Table4describesthemaskratiodetailsofthepro-
Weconsiderthetaskofminimizingthelossfunc- posed GMTmethodforallmodelsandalldomain
tionL(Θ)foralargelanguagemodel(LLM)with experiments. To be fair in comparison, One-off
parameters Θ, subject to a sparsity constraint on Drop, Random Mask and GMT utilize the same
theparameterupdates. Thiscanbeformalizedas maskratio. Allexperimentsreachoptimalperfor-
thefollowingoptimizationproblem: manceat10%-40%sparsityupdates.
minL(Θ) Model Domain MaskRatio(%)
Θ (10) MISTRAL-7B MathReasoning 20
subjectto ∥(I −M)Θ ∥2 = 0, LLAMA3-8B MathReasoning 30
∆
MISTRAL-7B CodeGeneration 10
where I represents the identity matrix, ensuring DEEPSEEK-CODER-BASE-6.7B CodeGeneration 30
LLAMA2-7B General 40
thattheconstraintappliesacrossalldimensionsof LLAMA2-13B General 10
Θ.
Table4: Detailsofmaskratioforallexperimentsinthis
By introducing a vector of Lagrange multipli-
paper.
ers λ, we construct the Lagrangian function L
d
associatedwithouroptimizationproblem(10):
L (Θ,λ) = L(Θ)+λ⊤∥(I−M)Θ ∥2. (11)
d ∆
Thedualproblemseekstofindthesaddlepoint
ofL throughamin-maxstrategy:
d
L¯= maxminL (Θ,λ). (12)
d
λ Θ
Themin-maxinequalityallowsustoestablishan
upperboundforourprimalproblem. Forthedual
problem(12),themin-maxinequalitysuggests:
minL(Θ)+∥(I−M)Θ ∥2
∆
Θ
≤ maxminL d(Θ,λ) (13)
λ Θ
≤ minmaxL (Θ,λ).
d
Θ λ
Consequently,theright-handsideofinequality
(13)providesanupperboundfortheprimalopti-
mizationproblem: