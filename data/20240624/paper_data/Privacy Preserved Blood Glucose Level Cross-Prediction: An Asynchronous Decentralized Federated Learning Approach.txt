Privacy Preserved Blood Glucose Level Cross-Prediction: An
Asynchronous Decentralized Federated Learning Approach
ChengzhePiaoa, TaiyuZhub, YuWanga, StephanieEBaldewegc,d, PaulTaylora, PantelisGeorgioue,
JiahaoSunf, JunWangg and KezhiLia,∗
aInstituteofHealthInformatics,UniversityCollegeLondon,London,NW12DA,UK
bDepartmentofPsychiatry,UniversityofOxford,Oxford,OX37JX,UK
cDepartmentofDiabetes&Endocrinology,UniversityCollegeLondonHospitals,London,NW12PG,UK
dCentreforObesity&Metabolism,DeptofExperimental&TranslationalMedicine,UniversityCollegeLondon,London,WC1E6JF,UK
eCentreforBio-InspiredTechnology,DepartmentofElectricalandElectronicEngineering,ImperialCollegeLondon,London,SW72AZ,UK
fFLock.io,London,WC2H9JQ,UK
gDepartmentofComputerScience,UniversityCollegeLondon,London,WC1E6EA,UK
ARTICLE INFO ABSTRACT
Keywords: NewlydiagnosedType1Diabetes(T1D)patientsoftenstruggletoobtaineffectiveBloodGlucose
FederatedLearning (BG)predictionmodelsduetothelackofsufficientBGdatafromContinuousGlucoseMonitoring
BloodGlucosePrediction (CGM),presentingasignificant“coldstart”probleminpatientcare.Utilizingpopulationmodels
Type1Diabetes toaddressthischallengeisapotentialsolution,butcollectingpatientdatafortrainingpopulation
Cross-PatientAnalysis modelsinaprivacy-consciousmannerischallenging,especiallygiventhatsuchdataisoftenstored
onpersonaldevices.Consideringtheprivacyprotectionandaddressingthe“coldstart”problemin
diabetescare,wepropose“GluADFL”,bloodGlucosepredictionbyAsynchronousDecentralized
FederatedLearning.WecomparedGluADFLwitheightbaselinemethodsusingfourdistinctT1D
datasets, comprising 298 participants, which demonstrated its superior performance in accurately
predictingBGlevelsforcross-patientanalysis.Furthermore,patients’datamightbestoredandshared
across various communication networks in GluADFL, ranging from highly interconnected (e.g.,
random,performsthebestamongothers)tomorestructuredtopologies(e.g.,clusterandring),suitable
forvarioussocialnetworks.Theasynchronoustrainingframeworksupportsflexibleparticipation.By
adjustingtheratiosofinactiveparticipants,wefounditremainsstableiflessthan70%areinactive.
OurresultsconfirmthatGluADFLoffersapractical,privacy-preservingsolutionforBGprediction
inT1D,significantlyenhancingthequalityofdiabetesmanagement.
utilization of patient data on mobile devices or databases
1. Introduction
in hospitals in a privacy-preserving manner. By directly
Blood Glucose (BG) prediction [1, 2, 3] is indispens-
leveraging the data in the place where it is generated, FL
able for individuals with Type 1 Diabetes (T1D), as it
allows for the development of robust prediction models
enables proactive management of BG levels [4, 5], thus
without compromising patient privacy. However, applying
preventing potential hyperglycemia and hypoglycemia and
FLinthecoldstartproblemtriggerssomeconcerns:
mitigating complications. A common challenge faced by
many,especiallythosenewlydiagnosedorrecentlyinitiated • Target Differences: In a setting similar to zero-shot
on Continuous Glucose Monitoring (CGM) devices, is the learning [10], patients can be categorized as seen or
lack of sufficient BG trajectory data, leading to a “cold unseenbasedonwhethertheirdatahasbeenusedto
start” issue in developing accurate prediction models. This train the model. Patients who have been previously
problem is particularly acute for critically ill patients in observed (seen patients) contribute their data to FL
intensivecare,wheretimelyandeffectiveBGmanagement training with the expectation of obtaining personal-
iscrucial[6].Giventhatpopulationmodelscanaddressthe izedmodels.Thesepersonalizedmodelsaredesigned
coldstartproblem[7],theemergenceofFederatedLearning toretaintheuniquebehavioralpatternsofindividual
(FL) might offer a promising solution to this challenge, patients.Conversely,patientswhohavenotbeenpre-
especially in gathering population features while regarding viouslyobserved(unseenpatients)seekapopulation
dataprivacy[8,9]. modelbyleveragingseenpatients’data.Thepopula-
Specifically,employingtheintegrationofdatafrommul- tionmodelprovidesthemwithafoundationalstarting
tiple individuals into population models, FL enables the point, and it is defined as models that encapsulate
the general patterns observed across a broad patient
∗Correspondingauthor
cohort(refertoFigure1).
chengzhe.piao.21@ucl.ac.uk(C.Piao);taiyu.zhu@psych.ox.ac.uk
( BT a. ldZ eh wu) e; gy );u. pw .. t2 a3 y@ lu oc rl @. ua cc l. .u ak c.( uY k. (W P.a Tn ag y); los r. )b ;a pl ad ne tw ee lg i@ su @c il m. pa ec r. iu ak l.(S ac.E .u.
k(P.
• OverheadandLatency:Learningpopulationmodels
Georgiou);sun@flock.io(J.Sun);junwang@cs.ucl.ac.uk(J.Wang); inFLnecessitatescollectingextensivedatafrompa-
ken.li@ucl.ac.uk(K.Li) tients.However,scalabilityandefficiencychallenges
ORCID(s):0000-0003-0494-5098(C.Piao) inmanaginglarge-scaledata,especiallyincentralized
Chengzhe Piao et al. : PreprintsubmittedtoElsevier Page1of14
4202
nuJ
12
]GL.sc[
1v64351.6042:viXraPrivacy Preserved Blood Glucose Level Cross-Prediction: An Asynchronous Decentralized Federated Learning Approach
Server Server
Population
Population Model
Model
Global Aggregating/Dispatching
Data Training Models
Model Updates
Glucose
Trajectories Data Data Personal The latest The latest
Neural Smart Devices The latest model model
Networks model
LD aa teta
st
Lap Pt ao dp PShmoanret
Model
UM pdo ad te el
s
Local Model Updates
CGM CGM Training fr so em rv t eh re Store
CGM
Collect
Client 2 Client 3
CGM
Glucose
Participant 3
Participant 1 Trajectories
Participant
Participant 2
Client 1
a. Traditional Supervised Learning b. Centralized Federated Learning (FedAvg)
Figure 1: Population models for glucose prediction: a) Traditional Supervised Learning, which centralizes patients’ data on a
servertocreatemodelsbycombiningdata,andb)CentralizedFederatedLearning(FedAvg,[9]),whichutilizesdataonpersonal
devices.WithFedAvg,onlymodelupdates,notrawdata,aresenttotheserver,whichthenaggregatestheseupdatestoimprove
thepopulationmodel.Thisapproachmaintainsdataprivacybykeepingpatientdatalocalizedwhilestillbenefitingfromcollective
insights. Nevertheless, it has scalability and efficiency challenges in managing large-scale data.
FL(seeFigure1b),arenotableduetoincreasingover- Meanwhile,unseenpatientscaninitiallyusethepopulation
headandlatencywithmoreparticipantdevices.These modelwitha“warm”start.Theycanevensubsequentlyfine-
issues are crucial to resolve for practical healthcare tune it as they collect more personal data. Throughout this
applications. process, the privacy of personal data is safeguarded, as it
remainsconfinedtotheindividual’sdevice.
• DeviceLimitations:Patientswithbasicsmartphones
Then, given other concerns, a centralized FL structure
mightfacelongertrainingtimesduetolimitedcompu-
fails to offer viable solutions. Therefore, we propose “Glu-
tationalresources.Additionally,frequentphoneusage
ADFL”,aGlucosemanagementsystemusingAsynchronous
can hinder continuous participation in synchronous
Decentralized FL. Removing the central server allows our
FLtraining,reducingoverallefficiency.
methodtobescalableforlarge-scaledataapplications.This
system accommodates various communication topologies
• SocialPreferences:Somepatientsmaywillinglyen-
(ring, cluster, random), catering to diverse social needs.
gage in FL training and allow their devices to com-
By enabling asynchronous training, it mitigates the impact
municate broadly, while others may prefer limiting
of “slow” and “busy” devices, allowing broader and more
interactionstotheirsocialcircles.
convenientparticipationinthetrainingprocess.Meanwhile,
Inordertoaddressthecoldstarttowardsunseenpatients, as demonstrated in the BG level prediction challenge 2020
theprimaryobjectiveofFListodevelopapopulationmodel [11], Long Short-Term Memory (LSTM) models [12, 13]
that leverages data from seen patients while safeguarding have proven their efficacy. Besides, well-developed APIs
theirprivacy(Figure1).Thispopulationmodelisintended for LSTM models, such as TensorFlow or PyTorch, are
to be applicable to both seen and unseen patients, which widely supported across a broad range of mobile devices.
sets it apart from most existing work that focuses on per- Conversely, other models, such as Neural Basis Expansion
sonalized BG prediction. However, in this study, we also Analysis for interpretable Time Series (N-BEATS, [14]),
introduce a “personalized from population” approach that Neural Hierarchical interpolation for Time Series (NHiTS,
further fine-tunes the population model to better suit each [15]) or customized transformers [16], have yet to receive
patient’s unique characteristics. Specifically, seen patients similarsupport.Therefore,wechooseLSTMasthepredic-
can leverage their mobile devices such as smart phones tion method for GluADFL due to its reliable performance
to engage in FL training. Such devices are equipped with andwelldevelopedAPIsonmobiledevices.
storageforCGMsensordata,andtoolslikeTensorFlowLite To evaluate GluADFL with LSTM, our experiments
supporton-devicetraining1.Afterdevelopingthepopulation consider four diverse datasets, involving 298 participants.
model, seen patients can further optionally personalize the Ourfindingsaresummarizedasfollows:
modelwiththeirdata,i.e.,“personalizedfrompopulation”.
• The LSTM-based population model, trained through
1https://www.tensorflow.org/lite/examples/on_device_training/overview GluADFL, possesses the capability to cross-predict,
Chengzhe Piao et al. : PreprintsubmittedtoElsevier Page 2 of 14Privacy Preserved Blood Glucose Level Cross-Prediction: An Asynchronous Decentralized Federated Learning Approach
Personal Model Population Collecting Local Models
Neural Smart Devices Model Model after Training
Networks
Data Laptop PShmoanret Model Updates Node 2 Model Future
Local frU o Nmp od dOa et te h ss e r Pad Model Updates Node 3 PG relu dc ico ts ioe n
Training Store Model Input
CGM
Collect
Glucose
Model UpdateU spdate Ns
ode 5
TeN mo pd oe
ra
4
rily
M Mo od de el
l Collect CGM
Trajectories A Glucose
Participant Inactive Trajectory
Unseen People
Node 1 𝑡 =𝑇
Asynchronous Randomized Topology
Temporarily Temporarily
Node 2 Inactive Inactive Node 2 Node 2
Node 3 Node 3 Node 3
Node 1 Node 1 Node 1
…
Temporarily Node 4 Node 4 Node 4
Inactive
Node 5 Node 5 Node 5 Temporarily
Inactive
𝑡 = 1 𝑡 =2 𝑡 =𝑇
a. GluADFL (Random and Decentralized)
Node 2 Cluster 2
Node 2 Cluster 1
Node 3 Node 3 Node 2 Node 3
Node 1 Server Node 1
Node 4
Node 4 Node 4 Node 1
Node 5
Node 5 Node 5
Star Topology (Centralized) Ring Topology (Decentralized) Cluster Topology (Decentralized)
b. Other Topologies
Figure 2: A Glucose management system using Asynchronous Decentralized Federated Learning (GluADFL), tailored for Type
1 Diabetes (T1D) management. Figure a showcases the GluADFL employing randomized communication graphs, highlighting
the dynamic nature of node interactions for distributed learning. Nodes can mirror the participants (seen patients) attending
the training. In these nodes, through CGM devices, glucose data is collected and processed on local devices for model training.
Engaging in the GluADFL process, these devices collaboratively develop a population model to enhance BG management for
both seen patients with ample CGM data and unseen patients lacking sufficient data. Figure b presents other three distinct
communication graph topologies: star, ring, and cluster. Star topology is only for centralized FL (FedAvg [9]). Random, ring
and cluster can be utilized in GluADFL, each catering to different operational efficiencies and privacy considerations within the
network. These topologies reflect diverse participant preferences for data sharing and communication.
meaningitcanaccuratelypredictBGlevelsforunseen thetrainingframework’seffectivenessissignificantly
patients. Additionally, fine-tuning this model with impactedonlywhentheproportionofinactivenodes
data from seen patients results in personalized mod- exceeds70%.
els that significantly outperform those created from
• Wecomparethecross-patientBGpredictioncapabil-
random initialization. Along with the safe privacy,
itiesofGluADFLwitheightdistinctmethods.When
this improvement in BG prediction is an attractive
training population models using seen patients’ data
incentiveforseenpatientstoparticipateinFLtraining
via traditional supervised learning, the LSTM-based
torefinetheirpersonalizedmodels.
approachoutperformsothermethodssuchaslinearre-
• We evaluate the performance of GluADFL across gression,eXtremeGradientBoosting(XGBoost[17]),
differentcommunicationtopologies,i.e.,ring,cluster, N-BEATS, [14], and NHiTS [15], validating LSTM
and random, to cater to various social needs. The as a practical choice for population modeling. Fur-
random topology emerges as relatively the most ef- thermore, the GluADFL-based models achieve com-
fective,showingatendencytoconvergewiththebest parableperformancetosupervisedlearningandcen-
performance. To assess the robustness of GluADFL tralized FL [9], and they are superior to the meta-
in asynchronous environments, we experiment with learningframework[18,19]withoutfine-tuning.This
varyinginactivenoderatios.Thisapproachsimulates highlights the robust cross-predictive capability of
theimpactofdifferentlevelsofparticipantinactivity GluADFL-basedLSTMpopulationmodels.
on the training efficiency of the population model.
Weobservethatinarandom-basedGluADFLsetting,
Chengzhe Piao et al. : PreprintsubmittedtoElsevier Page 3 of 14Privacy Preserved Blood Glucose Level Cross-Prediction: An Asynchronous Decentralized Federated Learning Approach
trainingframeworksfallshortinaddressingdiverse“social
2. RelatedWork
preferences” and “device limitations”. To the best of our
FL has significantly evolved since its initial concept
knowledge, our proposed approach, GluADFL, is the first
with FedAvg [9], a centralized model involving a server
work that overcomes these limitations by employing asyn-
coordinating multiple clients. FedAvg’s core principle is
chronousdecentralizedFLaidedbyvariouscommunication
to train models directly on devices where data originates,
graphs for learning population models, thus enhancing the
avoidingtheneedforcentralizeddatagatheringandenhanc-
practicalityandapplicabilityofFLinhealthcare.
ingprivacyprotection.
DecentralizedFLhasfurtherevolvedfromthisconcept
byeliminatingthecentralserver,enablingbroaderparticipa- 3. ProposedMethod
tion. Various researchers have proposed innovative decen-
3.1. ProblemDefinition
tralized FL frameworks. For instance, He et al. [20] intro-
Our goal is to develop an asynchronous decentralized
ducedSpreadGNN,amulti-taskfederatedframeworkadept
FL framework, aimed at enhancing diabetes management
athandlingpartiallabelswithoutacentralserver.Shietal.
for both seen and unseen patients. While this support ini-
[21]developedDFedSAMandDFedSAM-MGS,theformer
tially focuses on Blood Glucose Level Prediction (BGLP),
utilizing gradient perturbation for local model flattening,
it’s designed to be adaptable for broader applications such
andthelatterenhancingthiswithMultipleGossipStepsfor
as insulin guidance, dietary recommendations, and other
improved model consistency and communication balance.
criticalaspectsoflife-cyclemanagement.Inthispaper,we
Liuetal.[22]implementedamethodbalancingcommunica-
concentratespecificallyonBGLP.
tionefficiencyandmodelconsensusthroughperiodiclocal
BGLP (BG only): We define BGLP as the task of
updates and inter-node communications. Chen et al. [23]
predicting future glucose levels 𝑥 , based on a series
optimizedresourceefficiencybylimitinggradientpushesto 𝐿+𝐻
ofhistoricalglucoserecords𝑥 = {𝑥 ,...,𝑥 },collected
a neighbor subset. Dai et al. [24] and Bornstein et al. [25] 1∶𝐿 1 𝐿
throughCGMatregularintervals𝛿𝑙(e.g.,every5minutes).
explored various decentralized communication topologies,
The prediction horizon, denoted as 𝐻, is the future time
suchasrandom,ring,andcluster,offeringpotentialsolutions
when the prediction is made (e.g., 𝐻 = 6 equals to 30
fordiversesocialneeds.
minutesahead).
AsynchronousFLrepresentsasignificantadvancement
Ourapproachfocusesonusingunivariateglucoseseries,
in FL methodologies. This variation has been explored in
denoted as 𝑥 . This approach stems from the practical
various studies. Hagos et al. [26] ventured beyond central- 1∶𝐿
challengesincollectinghigh-qualitydataviaafullsuiteof
ized FL by investigating scalable asynchronous FL tech-
wearable devices from all patients [38] and ensuring their
niques for privacy-preserving real-time surveillance sys-
comfort with wearing multiple sensors. Self-reported data
tems, demonstrating its practical applications. Jang et al.
alsopresentstheriskofartifactsanderrorsinlogging[39]
[27] introduced AsyncFL, a system where clients upload
and could lead to an unpleasant user experience due to the
modelsbasedontheircapabilities,andtheFLserverasyn-
frequentneedformanualentry.Thus,relyingsolelyonCGM
chronouslyupdatesandbroadcaststheglobalmodel,show-
dataemergesasamoreconvenientandaccuratemethodfor
casing the flexibility of asynchronous operations. Yang et
futureBGpredictions.Thisdecisionalignswithresearchun-
al.[28]developedAnarchicFL,allowingworkerstodecide
derscoringthesubstantialimpactofglucosetrajectorieson
their participation schedule and the extent of local compu-
predictionaccuracy[40,41,42].Forinstance,ourprevious
tations in each round, emphasizing the autonomy in cen-
work[40]hasdemonstratedthatCGMdataandtimestamps
tralized FL settings. Bornstein et al. [25] proposed SWIFT
togethercontributesignificantly(93.9%)tofutureBGlevel
for decentralized FL, enabling participants to join training
predictions,withthemajorcontributionattributedtoCGM.
asynchronously and broadcast model parameters at their
Meanwhile, there is substantial variability in self-reported
convenience, highlighting the adaptability in decentralized
events among individuals, which can impair the predictive
environments.
performanceofpopulationmodels.Thisissueisparticularly
FLplaysapivotalroleinensuringdatasecurityandpri-
crucialforFLthataimstogeneralizemodelsacrossdiverse
vacy,adheringtostrictregulationsliketheEU/UKGeneral
populations.
DataProtectionRegulation,therebybecomingincreasingly
Anadditionalbenefitofadoptingaunivariateapproach
relevant in healthcare [29]. Its application across various
istheoptimizationofcomputationalresourcesanditseffec-
healthcare scenarios [30, 31, 32, 33], especially including
tiveperformance,especiallyinLSTMmodels[12,43].This
diabetesdetection[34]andprediction[35,36],andglucose
notonlyenhancesperformancebutalsosignificantlyreduces
prediction [37], underscores its significance in protecting
the training load on mobile devices, making the process
healthcare data. Notably, Falco et al. [37] introduced a
moreefficientandfeasibleforwidespreaduseinpatientcare.
FL framework using an evolutionary algorithm in diabetes
management marks a significant stride in privacy-focused
3.2. ProblemFormulation
glucose prediction. However, existing studies in healthcare
As outlined in section 1, our objective is to utilize FL
often overlook the full potential of FL in terms of gen-
fordevelopingapopulationmodelthataddressesthe“cold
eralization, scalability, and asynchronous updates, essen-
tial for real-world healthcare implementation. Centralized
Chengzhe Piao et al. : PreprintsubmittedtoElsevier Page 4 of 14Privacy Preserved Blood Glucose Level Cross-Prediction: An Asynchronous Decentralized Federated Learning Approach
start”issueforunseenpatientswhilesimultaneouslyenhanc- 3.3. ProposedAsynchronousDecentralizedFLfor
ingthepredictionaccuracyofpersonalizedmodelsforseen LearningaPopulationModelinBGLP
patients.Thepopulationmodelischaracterizedasfollows: Figure 2a depicts participants (seen patients with T1D
Population Modeling: Given a set of patients  = whopossesssufficientCGMdata)asnodesinthecommu-
{1,...,𝑛,...,𝑁}, where each patient 𝑛 possesses a private nicationgraph,representinganetworkthatcanadoptring,
dataset𝑛,ourgoalistoderiveapopulationmodel𝑥̂𝑛 = cluster, or random topologies as shown in Figure 2b. This
𝐿+𝐻
𝑓(𝑥𝑛 ;𝐰)forall𝑛in,with𝐰representingthemodel’s topologychoiceisbasedontheparticipants’communication
1∶𝐿
learnableparameters,and𝐻 isthepredictionhorizon. preferences.CGMdevicescollectglucosedataforprocess-
Incontrasttothepersonalizedmodel𝑓(𝑥𝑛 ;𝐰𝑛),which ing and training on local devices. These devices engage in
1∶𝐿
tailors to individual data of the patient 𝑛, the population GluADFL, an asynchronous decentralized FL process for
model 𝑓(𝑥𝑛 ;𝐰) utilizes non-personalized parameters 𝐰 BGLP, working collectively to create a population model
1∶𝐿
to capture universal patient patterns [12]. This approach thatsupportsBGmanagementfornewusers(unseenpatients
effectively tackles the “cold start” issue for patients not withT1DwhodonothaveenoughCGMdata).
previously encountered in the model [7]. In this work, we TheGluADFLprocessentails:
adopttheLSTM[44]forthepopulationmodel.TheLSTM
framework captures long-term dependencies via its self- • Step 1 - Initial Setup: Node models are initialized
withrandomparameters(Line3).
gated mechanism, which includes input, forget, and output
gates,allowingforeffectiveinformationflowmanagement.
• Step 2 - Broadcasting: Active nodes share their
Unlike convolutional methods that lack memory for long-
model parameters with nearby active nodes, influ-
term sequences, LSTM excels in recognizing patterns over
encedby’stopology(Line5).
extended periods. Moreover, this technology strikes an op-
timal balance by offering the capacity to learn complex • Step3-LocalModelAggregation:Eachactivenode
temporal patterns with minimal need for extensive hyper- updateitsmodelbyaveragingitsownmodelandthe
parameter tuning, ensuring both resource conservation and receivedmodelsfromneighbors(Lines7-9).
effectiveperformanceinconstrainedenvironments.Itsblend
ofcomputationalefficiency,effectiveness,andthepresence • Step 4 - Local Training: Nodes refine their mod-
els using their data based on aggregated parameters
of well-developed APIs renders it particularly suitable for
(Lines11-13).
mobiledevices.
Furthermore, we aim to adopt decentralized FL to fa-
• Step 5 - Iterative Learning: Steps 2-4 are repeated
cilitate increased participation without the scalability con-
asynchronouslyuntilachievingconvergenceormeet-
straints associated with a central server. This approach en-
ingastopcondition.
couragesabroaderinclusionofparticipantdatainthelearn-
ingprocess. • Step6-PopulationModelFormation:Finalmodel
Decentralized FL: In a decentralized FL setup, nodes parametersareaggregatedfromallnodestoformthe
 = {1,...,𝑛,...,𝑁} are interconnected via a commu- populationmodel(Lines15-16).
nication graph , mirroring the patient set . Each pa-
Convergence(instep5)withinthecontextofGluADFL
tient 𝑛 corresponds to a node in , with the graph’s edges
canbedefinedasreachingastatewheretheobjectivefunc-
representingthecommunicationlinksbetweenpatients.As
tion, as detailed in Equation (2), cannot be minimized any
for node 𝑛, this topology allows it to direct communicate
further.Thisindicatesthatthemodelhasreacheditsoptimal
withitsneighboringnodes𝑛connectedbyone-hoplinks.
performance, and further iterations do not yield significant
ThedecentralizedFLframeworkaimstooptimizeaglobal
improvements in prediction accuracy. Additional stopping
function:
criteriaforthealgorithmcanincludespecificthresholdsfor
min∑ 𝐽(𝑥𝑛 ,𝐰), (1) the objective function. This allows for flexibility in deter-
𝐰 𝐿+𝐻 miningwhenthelearningprocesshasreachedasatisfactory
𝑛
level of performance or optimization, providing a practical
where the loss function for the BGLP at each node 𝑛 is mechanism for stopping the algorithm under predefined
definedas:
conditions.
Thealgorithmenablesthedirectapplicationofthepopu-
𝐽(𝑥𝑛 𝐿+𝐻,𝐰)=𝔼 𝑥𝑛∼𝑛[(𝑥𝑛
𝐿+𝐻
−𝑓(𝑥𝑛 1∶𝐿;𝐰))2], (2)
lationmodelforunseenpatients,offeringimmediatebenefits
in BG management. Seen patients, who have contributed
aiming to minimize the mean square error of all nodes
their data, have the flexibility to further refine this pop-
between predicted and actual BG levels, thus improving
ulation model with their own data, utilizing Equation (2)
predictionaccuracyinBGLPthroughcollaborativelearning
withoutcentralizedcontrol. and adjusting the learning rate 𝛾 for personalization. This
customizationprocessenhancesthemodel’srelevancetothe
individual’s unique data patterns. Moreover, seen patients
might also directly use the population model, as it often
Chengzhe Piao et al. : PreprintsubmittedtoElsevier Page 5 of 14Privacy Preserved Blood Glucose Level Cross-Prediction: An Asynchronous Decentralized Federated Learning Approach
Algorithm1:GluADFL fullyconnectedinternallyandlinkedtoothersthrough
specific nodes. This setup, shown in Figure 2b, pro-
1
Input:Trainingsteps𝑇,setofnodes,max
motes efficient intra- and inter-cluster communica-
communicationbatchsize𝐵,initialmodel
tion,withthestructureremainingconstantthroughout
parameters{𝐰𝑛}𝑁,learningrate𝛾,andtraining
0 1 training.
data{𝑛}𝑁
1
2
Output:Populationmodel𝐰 • Ring [25]: Nodes are arranged in a circular pattern,
enablingsequentialparametersharingalongthering.
3
Initializenodemodelsin with{𝐰𝑛}𝑁
This ensures a consistent, cyclic flow of information
0 1
among all active participants, depicted in Figure 2b,
4 for𝑡=1to𝑇 do
5
Broadcastmodelparametersfromactivenodes withthecommunicationpatternfixedduringthetrain-
totheirneighbors,dependingonthecurrent ing.
communicationgraph
The choice of communication graphs in GluADFL can
6 #Modelupdatespost-broadcast cater to diverse social preferences and privacy concerns
7 for𝑛=1to𝑁 do of participants. The random topology fits scenarios where
8 if node𝑛isactivethen participantsareopentobroaderinteractionbeyondtheirim-
9 Updatenode𝑛’smodelwiththe mediatesocialcircles.Theclustertopologyisidealforthose
neighbornodeset𝑛,ensuring preferring interactions within known groups, resembling
𝑡
|𝑛|≤𝐵:𝐰̂𝑛 ← socialcircles.Forutmostprivacy,theringtopologyallows
| 𝑡 1| ( ∑ 𝑡−1 𝐰𝑛′ +𝐰𝑛 ) communicationexclusivelybetweentwocloselyconnected
| 𝑡𝑛 |+1 𝑛′∈ 𝑡𝑛 𝑡−1 𝑡−1 individuals, mirroring the preference to limit data (model
parameters) sharing to a minimum. Thus, these topologies
#Nodemodelupdateswithlocaldata
10 offer customizable communication settings to meet varied
11 for𝑛=1to𝑁 do socialandprivacyneeds.
12 if node𝑛isactivethen These communication graphs enhance the distributed
Updateusinglocaldata:
13 nature of communication and decrease dependency on a
𝐰𝑛 𝑡 ←𝐰̂𝑛 𝑡−1−𝛾∇𝐽(𝑥𝑛 𝐿+𝐻,𝐰𝑛 𝑡−1) single master node, characteristic of the star topology in
FedAvg [9], as illustrated in Figure 2b. By facilitating ef-
14
#Finalizethepopulationmodel𝐰 ficient, decentralized exchanges among participants, these
15
Aggregatemodelparametersfromallnodes structures foster collaboration and accelerate the learning
16 𝐰← 𝑁1 ∑ 𝑛𝐰𝑛
𝑇
p apro pc re os as chfo tor FB LG .LP, offering a more resilient and scalable
surpasses solely personalized models that lack population- 4. ExperimentsandResults
derived insights, providing a robust starting point for indi-
4.1. Datasets
vidualadaptations.
All the results are based on the prediction of future
TheGluADFLframeworkfacilitatesasynchronouscom-
BG levels for patients with T1D using OhioT1DM [11],
munication [25], allowing nodes to update their models or
ABC4D ([45, 3], NCT02053051), CTR3 (NCT02137512)
pause without waiting for others, enhancing flexibility and
and REPLACE-BG ([46], NCT02258373). The statistics
efficiency.Nodescanrejointhelearningprocesswhenready,
for these four datasets are presented in Table 1, highlight-
ensuringthateachparticipantcontributesattheirownpace.
ing their diversity. For example, the ABC4D dataset deliv-
Thiswait-freemechanismsupportscontinuouslocalmodel
ered insulin exclusively using pens (Novo Nordisk Echo),
improvements and collective learning without the need for
whereas other datasets leveraged pumps. This potentially
synchronous updates, making the learning process more
makes the BG variability the greatest in ABC4D, result-
adaptabletoindividualandnetworkvariabilities.
ing in a challenging BG prediction. Each dataset, exclud-
In the GluADFL framework, we explore three distinct
ing OhioT1DM, is divided into training (60%), validation
communication graphs to facilitate model training and pa-
(20%) and testing data (20%) by time per person. As for
rametersharing:
OhioT1DM,ithasbeenoriginallydividedintotrainingand
• Random [24]: As illustrated in Figure 2a, active testing data. We make the last 20% of its training data as
nodes randomly establish connections with up to 𝐵 the validation data. As mentioned in section 3.1, only BG
otheractivenodesforexchangingmodelparameters. timeseriesisconsidered.Hence,weleveragedZ-Scorenor-
This topology introduces a flexible and dynamically malizationtostandardizetheBGlevelswithineachdataset
changingnetworkstructureateachtrainingstep. usingthemeanandstandarddeviationoftrainingdata,then
allmissingvaluesarereplacedwithzero[12].
• Cluster[25]:Initially,nodesareorganizedintoclus-
ters,creatingaring-likestructurewhereeachclusteris
Chengzhe Piao et al. : PreprintsubmittedtoElsevier Page 6 of 14Privacy Preserved Blood Glucose Level Cross-Prediction: An Asynchronous Decentralized Federated Learning Approach
Table 1
The statistics of four datasets
Demographic OhioT1DM ABC4D CTR3 REPLACE-BG
CGM MedtronicEnlite DexcomG5 DexcomG4 DexcomG4
InsulinPumporPen MiniMed530G/630G NovoNordiskEcho RocheAccu-Chek MiniMed530G,OmniPod,etc.
No.ofParticipants 12 25 30 226
No.ofDays 54(2) 168(14) 163(67) 251(39)
No.ofCGMRecordsperParticipant 13871(1015) 43259(5460) 43421(18309) 66153(10701)
MeanofCGMData(mg/dL) 159.35(16.34) 156.66(24.24) 151.37(13.34) 160.69(21.18)
SDofCGMData(mg/dL) 58.11(6.15) 60.52(14.47) 55.29(8.24) 60.33(11.65)
TimeInRinge(%) 63.54(9.70) 62.54(15.58) 69.92(7.95) 63.10(12.18)
TimeBelowRinge(%) 3.30(2.25) 6.01(4.13) 3.53(2.11) 3.78(2.51)
TimeAboveRinge(%) 33.15(10.71) 31.45(15.65) 26.54(8.57) 33.13(12.93)
CoefficientofVariation(%) 36.63(3.70) 38.40(6.22) 36.44(3.90) 37.45(4.70)
LowBloodGlucoseIndex 0.88(0.48) 1.73(1.02) 0.97(0.48) 1.00(0.58)
HighBloodGlucoseIndex 7.15(2.45) 7.26(3.85) 5.89(1.93) 7.57(3.42)
4.2. EthicsandDataavailability (e.g., hypoglycemia, hyperglycemia) have different clinical
AlldatasetscanbeaccessedpubliclyapartfromABC4D, implications.
which can be accessed via authorised procedures by con-
√
tacting the project manager and the corresponding au- 1 ∑
𝑔𝑅𝑀𝑆𝐸 = 𝑃(𝑥 ,𝑥̂ )(𝑥 −𝑥̂ )2,
thor. The ABC4D studies were conducted under protocol 𝐼 𝐿+𝐻,𝑖 𝐿+𝐻,𝑖 𝐿+𝐻,𝑖 𝐿+𝐻,𝑖
𝑖
(13/LO/0264) approved by the London - Chelsea Research
(6)
EthicsCommitteein2013.
where𝑃(𝑥 ,𝑥̂ )penalizesoverestimationinhypo-
𝐿+𝐻,𝑖 𝐿+𝐻,𝑖
4.3. Metrics glycemia and underestimation in hyperglycemia, and more
In this study, the metrics used to evaluate the per- detailsarein[47].
formance of the BGLP includes root mean square error In BGLP, time lag (minutes) quantifies the temporal
(RMSE),meanabsoluterelativedifference(MARD),mean discrepancy between actual BG level changes and when
absolute error (MAE), glucose-specific RMSE (gRMSE, thesechangesaredetectedbythemodel’spredictions.This
[47,48])andtimelag.RMSE(mg/dL)measuresthesquare concept,asdetailedbyCohen[49]throughcross-correlation
rootoftheaverageofthesquaresoftheerrors,asfollows(𝐼 analysis, and further applied in BGLP contexts [50, 51],
isthetotalnumberoftestingexamples): highlights the critical challenge of timely and accurate BG
prediction, emphasizing the importance of minimizing lag
√
1 ∑ foreffectivediabetesmanagement.
𝑅𝑀𝑆𝐸 =
𝐼
(𝑥 𝐿+𝐻,𝑖−𝑥̂ 𝐿+𝐻,𝑖)2. (3) Insummary,wepursuelowervaluesofallthesemetrics,
𝑖 meaningbetterpredictionmodelsforBGLP.
MARD (%) averages the absolute differences between
4.4. Baselines
predictedandactualvalues,expressedasapercentageofthe
In our experiments, several baselines were compared
actualvalues.Itisusedtoassesstherelativeaccuracyofthe
with the proposed model. First of all, five kinds of popu-
predictions.
lationmodelsweregottenbytraditionalsupervisedlearning
throughmixingthetrainingdataofallpatientswithineach
𝑀𝐴𝑅𝐷=
1 ∑|𝑥 𝐿+𝐻,𝑖−𝑥̂ 𝐿+𝐻,𝑖|
×100%. (4) datasetasfollows:
𝐼 𝑥
𝑖 𝐿+𝐻,𝑖
• LR:itisaLinearRegression(LR)modelthatpredicts
MAE(mg/dL)measurestheaverageoftheabsolutedif- values based on linear relationships between input
ferencesbetweenthepredictedvaluesandtheactualvalues. featuresandtargetvariable.
Withoutsquaringtheerrorsbeforeaveragingallowsittobe
• XGBoost[17]:itisbasedongradientboosting,lever-
lesssensitivetolargeerrorsthanRMSE.
aging decision trees and optimizing model perfor-
mancethroughsequentiallearning.
1 ∑
𝑀𝐴𝐸 = |𝑥 −𝑥̂ |. (5)
𝐼
𝑖
| 𝐿+𝐻,𝑖 𝐿+𝐻,𝑖| • LSTM: a single layer of LSTM whose self-gated
mechanisms can model long short-term patterns of
gRMSE(mg/dL)isavariationofRMSE,evaluatingthe
timeseries.
prediction error specifically within certain ranges of BG
levels. It allows for a more detailed assessment of model • N-BEATS[14]:itisaNeuralBasisExpansionAnal-
performance across different glucose level ranges, which ysis for interpretable Time Series (N-BEATS) for-
is crucial in diabetes management where different ranges castingthroughleveragingastackoffully-connected
layersandbackcasting.
Chengzhe Piao et al. : PreprintsubmittedtoElsevier Page 7 of 14Privacy Preserved Blood Glucose Level Cross-Prediction: An Asynchronous Decentralized Federated Learning Approach
• NHiTS [15]: it is a Neural Hierarchical Interpola-
Table 2
tion for Time Series (NHiTS), leveraging hierarchi-
Generalization of population models trained by GluADFL
cal interpolation and pooling techniques. Compared
TestingData
with N-BEATS, it improves time series predictions TrainingData Metric OhioT1DM ABC4D CTR3 REPLACE-BG
RMSE 19.66(2.52) 23.61(4.44) 20.49(3.23) 20.73(3.45)
by specializing in different frequencies and reducing
MARD 9.43(1.62) 12.97(3.73) 10.38(1.50) 10.71(1.95)
computationalcomplexity. OhioT1DM MAE 13.80(1.61) 16.25(2.42) 14.41(2.24) 14.75(2.45)
gRMSE 24.61(3.35) 30.65(7.00) 25.15(4.27) 25.98(4.67)
TimeLag 5.22(4.48) 6.54(5.05) 10.31(3.97) 9.41(3.99)
Furthermore,weintroducedmeta-learningmethodsand RMSE 19.74(2.56) 22.28(4.03) 20.08(3.21) 20.21(3.22)
MARD 9.45(1.61) 12.16(3.22) 10.13(1.51) 10.43(1.85)
centralizedFLtoourbaselinesforcomparisons. ABC4D MAE 13.88(1.59) 15.41(2.05) 14.11(2.23) 14.43(2.34)
gRMSE 24.69(3.26) 28.60(6.34) 24.57(4.20) 25.28(4.32)
TimeLag 5.36(4.17) 6.69(4.90) 10.37(4.01) 9.51(3.87)
• MAML [18]: an LSTM model trained by Model- RMSE 19.76(2.61) 22.79(4.19) 20.00(3.13) 20.28(3.27)
AgnosticMeta-Learning(MAML).MAMListofind MARD 9.45(1.62) 12.55(3.54) 10.09(1.47) 10.49(1.87)
CTR3 MAE 13.83(1.57) 15.73(2.19) 14.03(2.17) 14.44(2.34)
asetofinitialparametersthatcanquicklyadapttonew gRMSE 24.71(3.42) 29.46(6.66) 24.42(4.07) 25.36(4.41)
TimeLag 5.01(3.76) 6.35(4.81) 9.76(3.79) 9.02(3.79)
tasksthroughfewgradientupdates. RMSE 19.75(2.61) 22.97(4.26) 20.17(3.20) 20.34(3.32)
MARD 9.38(1.61) 12.54(3.46) 10.17(1.50) 10.45(1.88)
REPLACE-BG MAE 13.80(1.59) 15.77(2.19) 14.14(2.21) 14.44(2.36)
• MetaSGD [19]: an LSTM model trained by Meta- gRMSE 24.62(3.40) 29.68(6.74) 24.61(4.15) 25.38(4.47)
learneractinglikeStochasticGradientDescent(MetaSGD). TimeLag 5.56(4.46) 6.64(4.95) 10.38(4.10) 9.46(3.93)
Bold:resultsforseenpatients(diagonal);
MetaSGDextendsMAMLbyintroducingalearnable Un-Bold:resultsforunseenpatients(non-diagonal);
Red:differencesbetweenseenpatientsandunseenpatientsarebelow0.30foreachcolumn;
learningrateforeachparameterinthemodel,provid- Blue:differencesbetweenseenpatientsandunseenpatientsarebetween(0.3,0.5]foreachcolumn.
ingmoreflexibilityandpotentiallytovarioustasks.
minor discrepancies (≤ 0.50) compared to seen patients.
• FedAvg [9]: an LSTM model trained by using cen-
ExcludingmodelstrainedwiththeOhioT1DMdataset,this
tralizedFederatedAveraging(FedAvg)method.Ithas
percentagerisesto87%,suggestingahighlevelofpredictive
a centralized server to periodically broadcast latest
consistency across different patient groups. This indicates
model parameters to local devices and average the
that models trained through GluADFL generally maintain
local model parameters which are trained separately
uniformaccuracy.However,modelstrainedspecificallywith
bylocaldata.
OhioT1DMdatadisplayweakergeneralization,mainlydue
Then, our proposed method with three different com- to the dataset’s smaller size. Similarly, in Table 3, 80% of
municationgraphsaredenotedas“GluADFL(Ring)”,“Glu- metrics for unseen patients are closely matched (≤ 0.50)
ADFL(Cluster)”and“GluADFL(Random)”. withthoseofseenpatients,increasingto88%whenexclud-
We set the historical BG series length as 𝐿 = 12, i.e., ing OhioT1DM-trained models, showcasing the inherent
2 hours, predicting the BG level in 30 minutes (𝐻 = 6). generalizationcapabilityofthepopulationmodelsbytradi-
Except for LR, each method was trained four times with tion supervised learning across patient data. Incorporating
differentrandomseeds.Weleveragegridsearchforselecting FLpreservesthiscapabilitywithonlyaslightreduction.
thehyperparametersofXGBoost.Weimplementedalldeep InFigure3,weexamine“PersonalizedModel”,“Popula-
learning methods using PyTorch 1.11.0, based on [52, 53] tionModel”,and“PersonalizedfromPopulation”acrossfour
andPyTorchForcasting2,andexecutedthemonanNVIDIA datasets,focusingontheadvantageforseenpatientspartic-
RTX3090Ti.Fordeeplearningmethods,thelearningrate ipatinginFLtraining.A“PersonalizedModel”startswitha
was found in {10−3,10−4,10−5} and the hidden state size randomlyinitializedmodelforeachparticipant,individually
was found in {128,256,512}. The best hyperparameters tailoredwiththeirdata.The“PopulationModel”isderived
were selected based on validation data performance. All from “GluADFL”. “Personalized from Population” allows
experimentsandresultscanbeaccessibleandreproducible: further refining the population model with personal data,
“https://github.com/ChengzhePiao/coldstartbglp”. enhancingitintoapersonalizedmodel.IntermsofRMSE,
“PersonalizedfromPopulation”outperforms“Personalized
4.5. Results Model” by 0.83, 0.75, 0.40, and 0.44 mg/dL, and in terms
Toassessthecross-predictioncapabilitiesofpopulation of gRMSE, by 1.07, 1.07, 0.62, and 0.57 mg/dL across
models for unseen patients, we compare the performance thedatasets,indicatingsignificantbenefitsforseenpatients
of population LSTM models trained via the “GluADFL fromincorporatingpopulationfeaturesthroughFLtraining.
(Random)” approach (Table 2) and traditional supervised In our analysis, we investigate the impact of different
learning(Table3)acrossfourdistinctdatasets.Performance typologiesofGluADFLonthetrainingprocess,asdepicted
isevaluatedusingtestingdatafromalldatasets,withspecific inFigure4.Weobservethat,towardstheendofthecommu-
interest in the off-diagonal cells (non-bold) in Tables 2 nicationrounds,theperformanceof“GluADFL(Random)”
and 3, which represent predictions for unseen patients. In surpassesthatof“GluADFL(Ring)”acrossvariousdatasets.
the analysis, differences in performance between seen and Specifically,improvementsinRMSEfor“GluADFL(Random)”
unseenpatientsarecategorizedbycolor:differencesbelow compared to “GluADFL(Ring)” are noted as 0.06, 0.48,
0.30orbetween(0.3,0.5]aremarkedinredorblue,respec- 0.28,and0.53mg/dLforOhioT1DM,ABC4D,CTR3,and
tively.InTable2,78%ofmetricsforunseenpatientsshow REPLACE-BG datasets, respectively. The performance of
2https://github.com/jdb78/pytorch-forecasting
Chengzhe Piao et al. : PreprintsubmittedtoElsevier Page 8 of 14Privacy Preserved Blood Glucose Level Cross-Prediction: An Asynchronous Decentralized Federated Learning Approach
30
Reduced 0.75 mg/dL Personalized Model Reduced 1.07 mg/dL Personalized Model
23 22.76 Population Model 29.31 Population Model
29
22.28 Personalized from Population 28.6 Personalized from Population
22 22.01 28.24
28
21 Reduced 0.44 mg/dL
Reduced 0.83 mg/dL Reduced 0.40 mg/dL 20.51 27 20.31 20.25 20.34
20 19.66 20 19.85 20.07 26 Reduced 1.07 mg/dL Reduced 0.57 mg/dL
19.48 25.45 25.64
Reduced 0.62 mg/dL 25.38
19 25 25 25.08
24.61
24.38 24.42 24.38
18 24
OhioT1DM ABC4D CTR3 REPLACE-BG OhioT1DM ABC4D CTR3 REPLACE-BG
Training and Testing Datasets Training and Testing Datasets
a. RMSE (mg/dL) results b. gRMSE (mg/dL) results
Figure3:Evaluationof“PersonalizedModel”,“PopulationModel”,and“PersonalizedfromPopulation” acrossfourdatasets.The
“Personalized Model” involves individual models for each patient, originally randomly initialized and trained with the patient’s
own data. The “Population Model” is derived from the GluADFL framework using a random topology and trained by mixing
patients’ data. “Personalized from Population” approach refines the GluADFL-based population model by integrating individual
patientdatatocreateacustomizedmodel,offeringahybridapproachthatleveragesbothbroadpopulationinsightsandspecific
patient data.
GluADFL(Ring) GluADFL(Ring)
24.5 GluADFL(Cluster) 28 GluADFL(Cluster)
GluADFL(Random) GluADFL(Random)
24.0
27
Reduced 0.49 mg/dL
23.5
26
23.0 Reduced 0.72 mg/dL
25
22.5 Reducing 0.48 mg/dL
Reducing 0.06 mg/dL
24
22.0
0 10 20 30 40 50 0 10 20 30 40 50
Communication Rounds Communication Rounds
a. Trained and Validated by OhioT1DM b. Trained and Validated by ABC4D
23.0 GluADFL(Ring) GluADFL(Ring)
GluADFL(Cluster) 23.0 GluADFL(Cluster)
22.5 GluADFL(Random) GluADFL(Random)
22.5
22.0
22.0
21.5
Reduced 0.50 mg/dL Reduced 0.63 mg/dL
21.0 21.5
Reducing 0.53 mg/dL
20.5 21.0
Reducing 0.28 mg/dL
20.0 20.5
0 10 20 30 40 50 0 10 20 30 40 50
Communication Rounds Communication Rounds
c. Trained and Validated by CTR3 d. Trained and Validated by REPLACE-BG
Figure 4: The convergence of population models trained by GluADFL with different communication graphs on four datasets.
“GluADFL(Cluster)” is found to be intermediate between reduction in RMSE by 1.40, 1.28, and 1.09 mg/dL on
thetwo. the OhioT1DM, ABC4D, and CTR3 datasets, respectively,
Furthermore,weassesstheinfluenceofvaryinginactive when compared with “GluADFL(Ring)”. Additionally, at
participant proportions during the training phases on the a 70% inactive node ratio, “GluADFL(Random)” shows a
overall performance across different topologies, as shown reduction in RMSE by 1.19 mg/dL on the REPLACE-BG
in Figure 5. The evaluation highlight that with a 90% dataset compared with “GluADFL(Ring)”. Overall, “Glu-
inactive node ratio, “GluADFL(Random)” demonstrates a ADFL(Random)” achieve an average reduction in RMSE
Chengzhe Piao et al. : PreprintsubmittedtoElsevier Page 9 of 14
)Ld/gm(
ESMR
)Ld/gm(
ESMR
)Ld/gm(
ESMR
)Ld/gm(
ESMRg
)Ld/gm(
ESMR
)Ld/gm(
ESMRPrivacy Preserved Blood Glucose Level Cross-Prediction: An Asynchronous Decentralized Federated Learning Approach
24.0 Reduced 1.40 mg/dL
GluADFL(Ring) GluADFL(Ring) Reduced 1.28 mg/dL
23.5 GluADFL(Cluster) 22.5 GluADFL(Cluster)
GluADFL(Random) GluADFL(Random)
23.0
22.0
22.5
21.5
22.0
21.0
21.5 Reduced 0.30 mg/dL
Reduced 0.09 mg/dL
21.0 20.5
0.0 10.0 30.0 50.0 70.0 90.0 0.0 10.0 30.0 50.0 70.0 90.0
Inactive Node Ratio in Each Communication (%) Inactive Node Ratio in Each Communication (%)
a. Trained by OhioT1DM and Tested by All Datasets b. Trained by ABC4D and Tested by All Datasets
Reduced 1.09 mg/dL 23.5
23.0 GluADFL(Ring) GluADFL(Ring)
GluADFL(Cluster) GluADFL(Cluster)
23.0
GluADFL(Random) GluADFL(Random)
22.5
22.5
Reduced 1.19 mg/dL
22.0
22.0
21.5
21.5
Reduced 0.56 mg/dL
21.0
Reduced 0.25 mg/dL 21.0
20.5
20.5
0.0 10.0 30.0 50.0 70.0 90.0 0.0 10.0 30.0 50.0 70.0 90.0
Inactive Node Ratio in Each Communication (%) Inactive Node Ratio in Each Communication (%)
c. Trained by CTR3 and Tested by All Datasets d. Trained by REPLACE-BG and Tested by All Datasets
Figure 5: The performance of GluADFL with different communication topology when changing inactive node ratio in each
communication.
onaspecifieddatasetandsubsequentlytestedonbothseen
Table 3
(identical dataset) and unseen (alternative datasets) patient
Generalization of population models trained by mixing data
data.Thiscomparisonaimstoevaluatethegeneralizability
TestingData
TrainingData Metric OhioT1DM ABC4D CTR3 REPLACE-BG of each model comprehensively. The performance of the
RMSE 19.65(2.48) 23.59(4.41) 20.56(3.28) 20.76(3.45)
LSTMmodel,trainedthroughsupervisedlearning,ishigh-
MARD 9.53(1.65) 12.93(3.58) 10.46(1.51) 10.81(1.95)
OhioT1DM MAE 13.89(1.60) 16.28(2.38) 14.49(2.25) 14.86(2.46) lighted in bold, establishing a benchmark for comparison.
gRMSE 24.65(3.26) 30.60(6.96) 25.28(4.34) 26.05(4.68)
TimeLag 5.11(4.38) 6.60(5.13) 10.34(4.07) 9.43(3.99) Discrepanciesfromthisbenchmarkareindicatedvisuallyin
RMSE 19.87(2.58) 22.42(4.06) 20.22(3.13) 20.44(3.23)
MARD 9.79(1.71) 12.51(3.33) 10.44(1.51) 10.85(1.94) red for differences ≤ 30 and in blue for differences within
ABC4D gRM MA SE
E
21 44 .. 81 35 (( 31 .. 26 94 )) 1 25 8. .6 79 1( (2 6. .1 30 7)
)
1 24 4. .3 78 2( (2 4. .2 01 7)
)
1 24 5. .7 56 5( (2 4. .3 37 2)
)
the(30,50]range.
TimeLag 5.44(4.20) 6.77(4.97) 10.29(3.85) 9.48(3.87)
RMSE 20.06(2.57) 22.87(4.20) 20.02(3.06) 20.54(3.28) • Among the evaluated population models LR, XG-
MARD 9.79(1.65) 12.83(3.59) 10.24(1.46) 10.85(1.93)
CTR3 MAE 14.24(1.56) 16.01(2.27 14.17(2.14) 14.83(2.40) Boost, LSTM, N-BEATS and NHiTS, the LSTM
gRMSE 25.39(3.37) 29.72(6.65) 24.70(4.01) 25.97(4.46)
TimeLag 4.85(3.92) 6.14(4.71) 9.21(3.66) 8.55(3.65) modeloutperformsintermsofaccuracyforbothseen
RMSE 19.75(2.59) 22.70(4.14) 20.17(3.19) 20.31(3.27) andunseenpatientdata.LRandXGBoostaretheleast
MARD 9.47(1.65) 12.45(3.33) 10.22(1.50) 10.50(1.88)
REPLACE-BG MAE 13.88(1.62) 15.69(2.12) 14.21(2.22) 14.49(2.35) effective, while N-BEATS and NHiTS lag slightly
gRMSE 24.56(3.35) 29.15(6.54) 24.55(4.14) 25.24(4.37)
TimeLag 5.45(4.27) 6.71(4.97) 10.38(3.98) 9.50(3.93) behind LSTM, with 80% and 75% of their results
UB no -ld B: or ldes :u rl et ss uf lo tsr fs oe ren unp sa et eie nn pts at( id enia tg so (n na ol n);
-diagonal);
showingonlya0.3disparitywithLSTMforseenand
R Be lud e: :d diff iffe er re en nc ce es sb be et tw we ee en ns se ee en np pa at tie ien nt ts sa an nd du un ns se ee en np pa at tie ien nt ts sa ar re eb be elo tww ee0 n.3 (0 0.f 3o ,r 0e .a 5c ]h foc ro elu am chn;
column.
unseenpatients,respectively.
• In the context of meta-learning and FL versus tra-
of0.50,0.64,0.59,and0.87mg/dLacrosstheOhioT1DM, ditional supervised learning, meta-learning methods
ABC4D, CTR3, and REPLACE-BG datasets, respectively, donotsignificantlysurpassthebenchmark.FL-based
indicatingasuperiorperformancerelativeto“GluADFL(Ring)”, methodologies,particularlyFedAVGand“GluADFL(Random)”,
with“GluADFL(Cluster)”maintainingaperformancelevel achieve results comparable to supervised learning,
thatisbetweenthetwo. with 95% of their outcomes differing from LSTM’s
We detail the comparative results of prediction perfor- by 0.3 across both seen and unseen patient data
manceacrossdiversemethods,ascapturedinTable4.This scenarios. “GluADFL(Ring)” shows relatively lower
table illustrates the efficacy of each method when trained effectiveness,with60%and65%ofitsresultsshowing
Chengzhe Piao et al. : PreprintsubmittedtoElsevier Page 10 of 14
)Ld/gm(
ESMR
)Ld/gm(
ESMR
)Ld/gm(
ESMR
)Ld/gm(
ESMRPrivacy Preserved Blood Glucose Level Cross-Prediction: An Asynchronous Decentralized Federated Learning Approach
a 0.3 difference from LSTM for seen and unseen suggested in [24, 25]. In Figure 4, with a communication
patients, respectively. “GluADFL(Cluster)” is posi- batch size of 𝐵 = 7, we observe some differences in the
tioned intermediate between “GluADFL(Random)” training efficacy of decentralized FL based on the commu-
and“GluADFL(Ring)”intermsofperformance. nicationgraphtopology.Therandomtopologydemonstrates
thebestconvergence,i.e.,convergingatthelowestRMSE,
followed by the cluster topology, with the ring topology
5. Discussions
showingrelativelytheworstconvergence.
5.1. Crosspredictionofpopulationmodels Thispatterncanbeattributedtothenumberofconnec-
towardsunseenpatientsusingfederated tions in each topology; more connections typically result
inbetterconvergence,i.e.,convergingatlowerRMSE.The
learning
Recent advancements in deep learning have embraced clustertopology,withitsmoderatenumberofconnections,
methodologies such as transfer learning [54, 55, 48] and converges better than the ring topology but worse than the
meta-learning [56, 57] to enhance the prediction of BG randomtopology.Therandomtopology’stime-varyingcon-
levels.Theseapproacheshaveshownpromisebyoptimizing nectionsfacilitatewiderbroadcastingofmodelparameters,
populationmodelsthroughtheapplicationofinsightsgained leadingtoitsbestconvergence.
acrosspatientgroups. Moreover,asthepopulationsizeincreases(fromOhioT1DM
However, a notable drawback of transfer learning and to REPLACE-BG), the differences in convergence per-
meta-learning lies in their implications for patient privacy, formance among these topologies become more notable.
necessitatingthecollectionandaggregationofpatientdata. Specifically,topologieswithfewerconnections,likethering
Additionally, both transfer learning and meta-learning re- topology,tendtotransmitmodelparametersmoreslowlyand
quire data from unseen patients for individual adaptation, limitedduringcommunications.
conflicting with the preference for adaptation-free popula- Additionally, we implement a wait-free mechanism, as
tionmodels. discussed in [25], where only active nodes participate in
In the literature, population-based LSTM models have communication and local model updates. Figure 5 reveals
been recognized for their ability to accurately model the the impact of inactive node ratios on the prediction perfor-
dynamicsofBGlevelsacrossdiversepatientgroups,thereby mance across different communication topologies. In line
delivering commendable outcomes in BGLP tasks [12]. with our earlier observations, the random topology main-
Thesemodelsachievethisbydiscerningandapplyinggen- tains stability even as the inactive node ratio increases.
eralpatternsacrossdatasetswithouttheneedforindividual- Notably, larger datasets amplify the advantages of the ran-
ized transfer adaptations. This characteristic enables them dom topology, showing more significant performance gaps
to effectively predict BG levels in patients not previously comparedtoothertopologies.Figure5alsoindicatesasharp
seen during training [7]. In our experiments, we also uti- declineintheeffectivenessofGluADFLwhentheinactive
lizedfourdistinctdatasetsofvaryingpopulationsizes,aim noderatioexceeds70%.
to validate the cross-prediction capabilities of population TheseobservationsrevealthatourGluADFLframework
LSTMs for unseen patients, as detailed in Table 3. The re- adapts well to varying network topologies, with the ran-
sultsdemonstratethatthesemodels,whentrainedandtested dom topology showing particular resilience in unstable or
across different datasets, exhibit comparable performance. dynamic network conditions. Its robust performance, even
This consistency in performance extends to experiments withahighinactivenoderatio,demonstratesitspotentialfor
involving“GluADFL(Random)”,reinforcingthenotionthat real-worldapplicationswherenetworkscenariosarevariable
population LSTMs, when trained through FL, retain their for different social preferences. The observed scalability
predictiveaccuracyinBGLPforunseenpatients,asshown with larger datasets reinforces GluADFL’s suitability for
inTable2. extensivedecentralizedlearningenvironments.
Additionally, seen patients whose data are included in
thetrainingsetcanfurtherrefineandpersonalizethepopu- 5.3. Comparisonofbloodglucoseprediction
lationmodelpostFLtraining,asillustratedinFigure3.Such performance
customizationisachievedbyadjustingthemodelparameters IncomparingGluADFLwithbaselinesonfourdatasets
tobetterfittheindividual’suniquedata,therebyenhancing forbothseenandunseenpatients(seeSection4.4,Tables4),
themodel’saccuracyandeffectivenessfortheirspecificBG weobservedthefollowing:
level predictions. However, the distinction between “Popu- 1)LSTMmodelsoutperformtraditionalalgorithmslike
lationModel”and“PersonalizedfromPopulation”isslight, LRandXGBoostinBGLP.DespitetheeffectivenessofLR
highlightingpopulationfeaturesaremoreimportant.There- and XGBoost in BGLP as documented in [58, 59], they
fore,weoptnottofocusonpersonalizedFLincurrentwork. fall short in comparison to LSTM in all metrics for both
seen and unseen patients. Advanced deep learning models
5.2. Theimpactofdifferentcommunication like N-BEATS and NHiTS, though excellent in healthcare
applications[60,61],donotsurpassLSTMwhenusingonly
typologiesofGLuADFLonBGLP
We explore three sparse communication topologies for BGdataasinput,similartofindingsin[62].Thiscouldbe
decentralized FL, specifically ring, cluster, and random, as
Chengzhe Piao et al. : PreprintsubmittedtoElsevier Page 11 of 14Privacy Preserved Blood Glucose Level Cross-Prediction: An Asynchronous Decentralized Federated Learning Approach
Table 4
Blood Glucose Prediction for Seen/Unseen Patients by Different Population Methods on Four Datasets
MethodsTestedbySeenPatients
TrainingData Metrics
LR XGBoost LSTM N-BEATS NHiTS MAML MetaSGD FedAvg GluADFL(Ring) GluADFL(Cluster) GluADFL(Random)
RMSE 21.48(5.38) 20.64(3.72) 19.65(3.59) 19.94(3.82) 19.89(3.75) 20.68(3.81) 20.44(3.87) 19.65(3.60) 19.74(3.63) 19.65(3.58) 19.66(3.59)
MARD 10.52(2.67) 10.18(2.32) 9.53(2.19) 9.64(2.31) 9.66(2.28) 9.97(2.21) 10.23(2.42) 9.43(2.22) 9.52(2.24) 9.47(2.24) 9.43(2.23)
OhioT1DM MAE 15.28(3.08) 14.76(2.60) 13.89(2.45) 14.08(2.58) 14.06(2.54) 14.77(2.61) 14.74(2.65) 13.78(2.44) 13.87(2.46) 13.81(2.44) 13.80(2.44)
gRMSE 26.94(7.44) 26.07(5.16) 24.65(5.02) 25.00(5.34) 24.83(5.23) 25.81(5.37) 25.10(5.32) 24.45(5.02) 24.57(5.07) 24.46(4.99) 24.61(5.03)
TimeLag 7.17(4.90) 6.63(4.86) 5.11(4.29) 5.67(4.37) 5.76(4.42) 5.88(4.39) 5.75(4.33) 5.34(4.29) 5.40(4.29) 5.41(4.30) 5.22(4.28)
RMSE 27.14(4.40) 23.56(3.60) 22.42(3.33) 22.92(3.45) 22.67(3.41) 22.99(3.43) 23.23(3.49) 22.27(3.33) 22.73(3.43) 22.42(3.36) 22.28(3.33)
MARD 14.70(2.60) 13.16(2.31) 12.51(2.12) 12.64(2.17) 12.30(2.08) 12.75(2.15) 12.54(2.06) 12.26(2.07) 12.57(2.14) 12.43(2.12) 12.16(2.04)
ABC4D MAE 18.68(3.06) 16.45(2.56) 15.69(2.33) 15.85(2.39) 15.64(2.34) 16.23(2.45) 16.34(2.51) 15.44(2.30) 15.77(2.37) 15.57(2.33) 15.41(2.30)
gRMSE 35.54(6.15) 30.75(5.00) 28.71(4.58) 29.74(4.82) 29.08(4.70) 29.50(4.74) 30.06(4.92) 28.57(4.58) 29.03(4.69) 28.77(4.62) 28.60(4.59)
TimeLag 9.59(5.61) 7.93(4.75) 6.77(4.14) 6.76(4.25) 6.85(4.25) 6.60(4.20) 6.29(4.21) 6.74(4.20) 7.32(4.42) 6.87(4.27) 6.69(4.16)
RMSE 22.95(4.67) 20.83(3.59) 20.02(3.39) 20.09(3.49) 20.13(3.44) 20.54(3.52) 20.55(3.54) 20.00(3.40) 20.30(3.49) 20.07(3.42) 20.00(3.40)
MARD 11.67(2.58) 10.70(2.35) 10.24(2.17) 10.25(2.26) 10.29(2.21) 10.71(2.37) 10.79(2.43) 10.08(2.12) 10.28(2.17) 10.16(2.17) 10.09(2.12)
CTR3 MAE 16.08(2.96) 14.77(2.52) 14.17(2.37) 14.17(2.39) 14.22(2.37) 14.65(2.46) 14.69(2.47) 14.04(2.33) 14.27(2.37) 14.11(2.35) 14.03(2.32)
gRMSE 28.53(6.54) 25.81(5.01) 24.70(4.75) 24.72(4.90) 24.60(4.79) 25.00(4.89) 24.90(4.89) 24.47(4.75) 24.70(4.85) 24.56(4.79) 24.42(4.74)
TimeLag 13.13(4.99) 11.69(4.59) 9.21(3.89) 9.43(3.97) 9.60(4.02) 10.19(4.18) 9.96(4.11) 9.72(4.02) 10.48(4.23) 9.86(4.07) 9.76(4.04)
RMSE 23.31(4.73) 21.21(3.59) 20.31(3.39) 20.43(3.51) 20.42(3.48) 21.82(3.68) 21.44(3.61) 20.33(3.45) 20.87(3.64) 20.70(3.58) 20.34(3.46)
MARD 11.85(2.54) 11.18(2.27) 10.50(2.09) 10.50(2.16) 10.43(2.09) 12.58(2.77) 11.27(2.26) 10.43(2.11) 10.75(2.23) 10.69(2.21) 10.45(2.12)
REPLACE-BG MAE 16.34(2.95) 15.24(2.51) 14.49(2.32) 14.50(2.37) 14.49(2.36) 16.32(2.73) 15.60(2.58) 14.42(2.34) 14.80(2.46) 14.70(2.43) 14.44(2.34)
gRMSE 29.39(6.61) 26.75(4.96) 25.24(4.68) 25.46(4.88) 25.44(4.82) 27.99(5.21) 27.40(5.17) 25.36(4.80) 25.98(5.05) 25.85(4.98) 25.38(4.81)
TimeLag 12.19(5.02) 11.08(4.69) 9.50(4.21) 9.63(4.24) 9.87(4.28) 8.40(4.05) 8.88(4.12) 9.45(4.22) 10.49(4.48) 10.14(4.42) 9.46(4.22)
MethodsTestedbyALLOtherUnseenPatients
TrainingData Metrics
LR XGBoost LSTM N-BEATS NHiTS MAML MetaSGD FedAvg GluADFL(Ring) GluADFL(Cluster) GluADFL(Random)
RMSE 24.50(5.38) 21.72(3.72) 20.99(3.59) 21.52(3.82) 21.41(3.75) 22.23(3.81) 22.17(3.87) 20.97(3.60) 21.08(3.63) 20.94(3.58) 20.96(3.59)
MARD 12.16(2.67) 11.51(2.32) 10.96(2.19) 11.15(2.31) 11.14(2.28) 11.38(2.21) 11.84(2.42) 10.89(2.22) 10.95(2.24) 10.91(2.24) 10.88(2.23)
OhioT1DM MAE 16.60(3.08) 15.57(2.60) 14.94(2.45) 15.23(2.58) 15.17(2.54) 15.86(2.61) 15.98(2.65) 14.85(2.44) 14.92(2.46) 14.85(2.44) 14.85(2.44)
gRMSE 30.99(7.44) 27.52(5.16) 26.37(5.02) 27.04(5.34) 26.82(5.23) 27.75(5.37) 27.36(5.32) 26.19(5.02) 26.35(5.07) 26.15(4.99) 26.30(5.03)
TimeLag 11.60(4.90) 11.25(4.86) 9.28(4.29) 9.67(4.37) 9.93(4.42) 9.72(4.39) 9.62(4.33) 9.36(4.29) 9.33(4.29) 9.37(4.30) 9.25(4.28)
RMSE 23.69(4.40) 21.35(3.60) 20.39(3.33) 20.51(3.45) 20.43(3.41) 20.95(3.43) 21.24(3.49) 20.17(3.33) 20.46(3.43) 20.26(3.36) 20.18(3.33)
MARD 12.53(2.60) 11.38(2.31) 10.75(2.12) 10.67(2.17) 10.51(2.08) 11.09(2.15) 10.99(2.06) 10.43(2.07) 10.63(2.14) 10.52(2.12) 10.36(2.04)
ABC4D MAE 17.21(3.06) 15.46(2.56) 14.69(2.33) 14.66(2.39) 14.56(2.34) 15.28(2.45) 15.45(2.51) 14.40(2.30) 14.64(2.37) 14.48(2.33) 14.37(2.30)
gRMSE 30.12(6.15) 27.14(5.00) 25.43(4.58) 25.77(4.82) 25.39(4.70) 26.19(4.74) 26.85(4.92) 25.11(4.58) 25.31(4.69) 25.21(4.62) 25.18(4.59)
TimeLag 14.01(5.61) 11.15(4.75) 9.39(4.14) 9.62(4.25) 9.70(4.25) 9.18(4.20) 9.10(4.21) 9.47(4.20) 10.28(4.42) 9.69(4.27) 9.42(4.16)
RMSE 23.72(4.67) 21.39(3.59) 20.74(3.39) 20.78(3.49) 20.71(3.44) 21.16(3.52) 21.20(3.54) 20.52(3.40) 20.73(3.49) 20.61(3.42) 20.49(3.40)
MARD 12.23(2.58) 11.47(2.35) 10.99(2.17) 10.97(2.26) 10.94(2.21) 11.51(2.37) 11.65(2.43) 10.65(2.12) 10.79(2.17) 10.79(2.17) 10.64(2.12)
CTR3 MAE 16.67(2.96) 15.40(2.52) 14.91(2.37) 14.84(2.39) 14.81(2.37) 15.29(2.46) 15.37(2.47) 14.56(2.33) 14.71(2.37) 14.67(2.35) 14.54(2.32)
gRMSE 30.17(6.54) 27.26(5.01) 26.30(4.75) 26.27(4.90) 26.00(4.79) 26.47(4.89) 26.41(4.89) 25.80(4.75) 25.92(4.85) 25.92(4.79) 25.72(4.74)
TimeLag 11.48(4.99) 10.10(4.59) 8.15(3.89) 8.23(3.97) 8.41(4.02) 8.90(4.18) 8.69(4.11) 8.53(4.02) 9.13(4.23) 8.64(4.07) 8.59(4.04)
RMSE 24.69(4.73) 21.93(3.59) 21.04(3.39) 21.24(3.51) 21.20(3.48) 22.17(3.68) 21.97(3.61) 21.13(3.45) 21.78(3.64) 21.56(3.58) 21.14(3.46)
MARD 12.45(2.54) 11.52(2.27) 10.92(2.09) 10.96(2.16) 10.85(2.09) 12.70(2.77) 11.58(2.26) 10.89(2.11) 11.27(2.23) 11.18(2.21) 10.91(2.12)
REPLACE-BG MAE 16.75(2.95) 15.40(2.51) 14.70(2.32) 14.75(2.37) 14.72(2.36) 16.17(2.73) 15.61(2.58) 14.68(2.34) 15.14(2.46) 15.00(2.43) 14.69(2.34)
gRMSE 31.27(6.61) 27.69(4.96) 26.27(4.68) 26.62(4.88) 26.53(4.82) 28.39(5.21) 28.09(5.17) 26.48(4.80) 27.23(5.05) 27.03(4.98) 26.50(4.81)
TimeLag 10.52(5.02) 9.56(4.69) 8.13(4.21) 8.18(4.24) 8.44(4.28) 7.18(4.05) 7.60(4.12) 8.09(4.22) 9.00(4.48) 8.68(4.42) 8.12(4.22)
Bold:resultsforLSTMtrainedbysupervisedlearning;
Red:differencesbetweenLSTMtrainedbysupervisedlearningandothermethodsarebelow0.30foreachrow;
Blue:differencesbetweenLSTMtrainedbysupervisedlearningandothermethodsarebetween(0.3,0.5]foreachrow.
becauseN-BEATSandNHiTSaremoresuitedforcomplex
6. LimitationsandFuturework
timeseriesforecastingthansinglepointpredictions.
This study, while contributing valuable insights into
2) Population FL-based LSTMs have inherent general-
BGLP using FL and population LSTM models, acknowl-
izationandcross-predictivecapabilitiesforunseenpatients,
edgescertainlimitationsthatpresentareasforfutureexplo-
as seen in section 5.1. We test meta-learning approaches
ration.
like MAML and MetaSGD, known for quick adaptation
Our current investigation is confined to single-horizon
to new tasks, in our baseline models without fine-tuning
BGlevelpredictions,anddoesnotextendtomulti-horizon
for unseen patients. However, these meta-learning models
forecasting. Multi-horizon prediction involves estimating
cannotoutperformLSTM,FedAvg,orGluADFLforunseen
BG levels at multiple future time points, offering a more
patients. Consequently, we decide against incorporating a
comprehensiveviewofglucosefluctuationsovertime.This
meta-learningmoduleinGluADFLanddidnotpursuemeta-
limitation highlights a significant area for further research,
learning-basedFL[63]ordomaingeneration-basedFL[64]
where expanding the scope to include multi-horizon pre-
further.
dictions could enhance the utility of our models for more
3)GluADFLshowscomparableperformancetoFedAvg,
dynamicandanticipatorydiabetesmanagement.
indicating that the FL and decentralized structure do not
Theexplorationofourproposedmethodisfocusedpri-
compromise LSTM’s modeling and cross-predictive abili-
marily on BGLP and has yet to encompass other critical
ties. GluADFL also addresses the “cold start” problem in
areas of diabetes management, such as insulin dose rec-
BGLP, enabling cross-prediction for unseen patients while
ommendation, dietary advice, or physical activity impact
protectingprivacy.Therefore,wechoosenottoaddcomplex
analysis. The complexity and multifaceted nature of dia-
modules to GluADFL, such as masked model-parameter
betes care suggest a wealth of opportunities for applying
averaging [24] or client-communication weight selection
ourmethodologytootheraspectsofdiabetesmanagement,
[25].
potentiallyofferingamoreholisticapproachtomanagingthe
condition.
The focus of this work is on designing an FL training
framework. Therefore, we adopted LSTM as a basic deep
learning-based approach in BGLP and will more advanced
Chengzhe Piao et al. : PreprintsubmittedtoElsevier Page 12 of 14Privacy Preserved Blood Glucose Level Cross-Prediction: An Asynchronous Decentralized Federated Learning Approach
modelslikethosebasedontransformers[16,40,65,66,67], healthcare, IEEEJ.Biomed.HealthInformatics27(2023)664–672.
inthefuture. doi:10.1109/JBHI.2022.3165945.
[9] B.McMahan,E.Moore,D.Ramage,S.Hampson,B.A.yArcas,
Communication-efficientlearningofdeepnetworksfromdecentral-
7. Conclusions izeddata,in:AISTATS’17,volume54,2017,pp.1273–1282.
[10] Y.Xian,C.H.Lampert,B.Schiele,Z.Akata, Zero-shotlearning-
To address the “cold start” problem in BGLP while
Acomprehensiveevaluationofthegood,thebadandtheugly,IEEE
ensuring privacy, we propose GluADFL, an asynchronous Trans.PatternAnal.Mach.Intell.41(2019)2251–2265.doi:10.1109/
decentralized FL framework. We employed LSTMs as a TPAMI.2018.2857768.
practical and efficient solution for individual patient BG [11] C.Marling,R.C.Bunescu, Theohiot1dmdatasetforbloodglucose
levelpredictions.UsingfourT1Ddatasets,wedemonstrated levelprediction:Update2020, in:KDH@ECAI’20,volume2675,
2020,pp.71–74.
GluADFL’sabilitytomaintainLSTM’scross-predictiveca-
[12] R.Bevan,F.Coenen, Experimentsinnon-personalizedfutureblood
pacity, showing scalability with larger populations and ro- glucoselevelprediction, in:KDH@ECAI’20,volume2675,2020,
bustnessacrossvariouscommunicationtopologiesandasyn- pp.100–104.
chronoussettings.Hence,wesuccessfullyestablishedaro- [13] T.Yang,R.Wu,R.Tao,S.Wen,N.Ma,Y.Zhao,X.Yu,H.Li,Multi-
bust and privacy-preserving approach to BGLP, offering scalelongshort-termmemorynetworkwithmulti-lagstructurefor
bloodglucoseprediction, in:KDH@ECAI’20,volume2675,2020,
bothhighaccuracyandadherencetoprivacyconsiderations.
pp.136–140.
[14] B.N.Oreshkin,D.Carpov,N.Chapados,Y.Bengio,N-BEATS:neu-
ralbasisexpansionanalysisforinterpretabletimeseriesforecasting,
8. Acknowledgement
in:ICLR’20,2020.
We thank the support from UKRI Center for Doctoral [15] C. Challu, K. G. Olivares, B. N. Oreshkin, F. G. Ramírez, M. M.
Canseco, A. Dubrawski, NHITS: neural hierarchical interpolation
TraininginAI-enabledhealthcaresystems[EP/S021612/1]
for time series forecasting, in: AAAI’23, 2023, pp. 6989–6997.
andUniversityCollegeLondonOverseasResearchScholar-
doi:10.1609/AAAI.V37I6.25854.
ships.WealsoappreciatethesupportfromRosetreesTrust [16] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.
(Grant number: UCL-IHE-2020\102) and Great Ormond Gomez, L. Kaiser, I. Polosukhin, Attention is all you need, in:
Street Hospital (Charity ref.X12018). The study sponsors NeurIPS’17,2017,pp.5998–6008.
[17] T.Chen,C.Guestrin, Xgboost:Ascalabletreeboostingsystem, in:
hadnostudyinvolvement.
KDD’16,2016,pp.785–794.doi:10.1145/2939672.2939785.
[18] C.Finn,P.Abbeel,S.Levine, Model-agnosticmeta-learningforfast
adaptation of deep networks, in: ICML’17, volume 70, 2017, pp.
References
1126–1135.
[1] I.Contreras,A.Bertachi,L.Biagi,J.Vehí,S.Oviedo, Usinggram- [19] Z.Li,F.Zhou,F.Chen,H.Li, Meta-sgd:Learningtolearnquickly
matical evolution to generate short-term blood glucose prediction forfewshotlearning,CoRRabs/1707.09835(2017).
models,in:IJCAI-ECAI’18,volume2148,2018,pp.91–96. [20] C. He, E. Ceyani, K. Balasubramanian, M. Annavaram, S. Aves-
[2] E.Montaser,J.Díez,J.Bondia, Glucosepredictionundervariable- timehr, Spreadgnn:Decentralizedmulti-taskfederatedlearningfor
lengthtime-stampeddailyevents:Aseasonalstochasticlocalmodel- graphneuralnetworksonmoleculardata, in:AAAI’22,2022,pp.
ingframework,Sensors21(2021)3188.doi:10.3390/S21093188. 6865–6873.doi:10.1609/AAAI.V36I6.20643.
[3] T.Zhu,L.Kuang,J.Daniels,P.Herrero,K.Li,P.Georgiou, Iomt- [21] Y.Shi,L.Shen,K.Wei,Y.Sun,B.Yuan,X.Wang,D.Tao,Improv-
enabledreal-timebloodglucosepredictionwithdeeplearningand ingthemodelconsistencyofdecentralizedfederatedlearning, in:
edge computing, IEEE Internet Things J. 10 (2023) 3706–3719. ICML’23,volume202,2023,pp.31269–31291.
doi:10.1109/JIOT.2022.3143375. [22] W.Liu,L.Chen,W.Wang,Generaldecentralizedfederatedlearning
[4] P.G.Jacobs,P.Herrero,A.Facchinetti,J.Vehi,B.Kovatchev,M.D. forcommunication-computationtradeoff, in:IEEEINFOCOM’22,
Breton,A.Cinar,K.S.Nikita,F.J.Doyle,J.Bondia,T.Battelino,J.R. 2022,pp.1–6.doi:10.1109/INFOCOMWKSHPS54753.2022.9797891.
Castle,K.Zarkogianni,R.Narayan,C.Mosquera-Lopez, Artificial [23] M.Chen,Y.Xu,H.Xu,L.Huang, Enhancingdecentralizedfeder-
intelligenceandmachinelearningforimprovingglycemiccontrolin atedlearningfornon-iiddataonheterogeneousdevices, in:IEEE
diabetes:Bestpractices,pitfalls,andopportunities, IEEEReviews ICDE’23,2023,pp.2289–2302.doi:10.1109/ICDE55515.2023.00177.
inBiomedicalEngineering17(2024)19–41.doi:10.1109/RBME.2023. [24] R. Dai, L. Shen, F. He, X. Tian, D. Tao, Dispfl: Towards
3331297. communication-efficientpersonalizedfederatedlearningviadecen-
[5] E.Pellizzari,F.Prendin,G.Cappon,G.Sparacino,A.Facchinetti,A tralizedsparsetraining, in:ICML’22,volume162,2022,pp.4587–
deep-learningbasedalgorithmforthemanagementofhyperglycemia 4604.
intype1diabetestherapy, in:IEEEBSN’23,2023,pp.1–4.doi:10. [25] M.Bornstein,T.Rabbani,E.Wang,A.S.Bedi,F.Huang, SWIFT:
1109/BSN58485.2023.10330994. rapiddecentralizedfederatedlearningviawait-freemodelcommuni-
[6] M.Desgrouas,J.Demiselle,L.Stiel,V.Brunot,R.Marnai,S.Sarfati, cation,in:ICLR’23,2023.
M.Fiancette,F.Lambiotte,A.W.Thille,M.Leloup,etal., Insulin [26] D. H. Hagos, E. Tankard, D. B. Rawat, A scalable asyn-
therapyandbloodglucosemanagementincriticallyillpatients:a1- chronousfederatedlearningforprivacy-preservingreal-timesurveil-
day cross-sectional observational study in 69 french intensive care lancesystems, in:IEEEINFOCOM’23,2023,pp.1–6.doi:10.1109/
units,AnnalsofIntensiveCare13(2023)1–13. INFOCOMWKSHPS57453.2023.10226108.
[7] A. Mohebbi, A. R. Johansen, N. Hansen, P. E. Christensen, J. M. [27] S.Jang,H.Lim, Asyncfl:Asynchronousfederatedlearningusing
Tarp, M. L. Jensen, H. Bengtsson, M. Mørup, Short term blood majorityvotingwithquantizedmodelupdates(studentabstract), in:
glucosepredictionbasedoncontinuousglucosemonitoringdata, in: AAAI’22,2022,pp.12975–12976.doi:10.1609/AAAI.V36I11.21624.
IEEEEMBC’20,2020,pp.5140–5145.doi:10.1109/EMBC44109.2020. [28] H.Yang,X.Zhang,P.Khanduri,J.Liu,Anarchicfederatedlearning,
9176695. in:ICML’22,volume162,2022,pp.25331–25363.
[8] A.Lakhan,M.A.Mohammed,J.Nedoma,R.Martinek,P.Tiwari, [29] N. B. Truong, K. Sun, S. Wang, F. Guitton, Y. Guo, Privacy
A. Vidyarthi, A. Alkhayyat, W. Wang, Federated-learning based preservation in federated learning: An insightful survey from the
privacypreservationandfraud-enabledblockchainiomtsystemfor GDPRperspective,Comput.Secur.110(2021)102402.doi:10.1016/
Chengzhe Piao et al. : PreprintsubmittedtoElsevier Page 13 of 14Privacy Preserved Blood Glucose Level Cross-Prediction: An Asynchronous Decentralized Federated Learning Approach
J.COSE.2021.102402. glucosemonitoringwithandwithoutroutinebloodglucosemonitor-
[30] M.Joshi,A.Pal,M.Sankarasubbu,Federatedlearningforhealthcare inginadultswithwell-controlledtype1diabetes, DiabetesCare40
domain-pipeline,applicationsandchallenges,ACMTrans.Comput. (2017)538–545.doi:10.2337/dc16-2482.
Healthcare3(2022).doi:10.1145/3533708. [47] S.D.Favero,A.Facchinetti,C.Cobelli,Aglucose-specificmetricto
[31] M.Akter,N.Moustafa,T.M.Lynar,I.Razzak, Edgeintelligence: assesspredictorsandidentifymodels, IEEETrans.Biomed.Eng.59
Federated learning-based privacy protection framework for smart (2012)1281–1290.doi:10.1109/TBME.2012.2185234.
healthcaresystems, IEEEJ.Biomed.HealthInformatics26(2022) [48] T.Zhu,C.Uduku,K.Li,P.Herrero,N.Oliver,P.Georgiou, En-
5805–5816.doi:10.1109/JBHI.2022.3192648. hancing self-management in type 1 diabetes with wearables and
[32] A.Rahman,M.S.Hossain,G.Muhammad,D.Kundu,T.Debnath, deep learning, npj Digital Medicine 5 (2022) 78. doi:10.1038/
M. S. Rahman, M. S. I. Khan, P. Tiwari, S. S. Band, Federated S41746-022-00626-5.
learning-based AI approaches in smart healthcare: concepts, tax- [49] L.Cohen,Time-frequencyanalysis,volume778,PrenticehallNew
onomies,challengesandopenissues,Clust.Comput.26(2023)2271– Jersey,1995.
2311.doi:10.1007/S10586-022-03658-4. [50] A.Gani,A.V.Gribok,Y.Lu,W.K.Ward,R.A.Vigersky,J.Reifman,
[33] P. He, C. Lan, A. K. Bashir, D. Wu, R. Wang, R. Kharel, K. Yu, Universalglucosemodelsforpredictingsubcutaneousglucosecon-
Low-latencyfederatedlearningviadynamicmodelpartitioningfor centrationinhumans, IEEETrans.Inf.Technol.Biomed.14(2010)
healthcareiot, IEEEJ.Biomed.HealthInformatics27(2023)4684– 157–165.doi:10.1109/TITB.2009.2034141.
4695.doi:10.1109/JBHI.2023.3298446. [51] K.Li,C.Liu,T.Zhu,P.Herrero,P.Georgiou,Glunet:Adeeplearning
[34] B.Dolo,F.Loukil,K.Boukadi, Earlydetectionofdiabetesmellitus frameworkforaccurateglucoseforecasting,IEEEJ.Biomed.Health
using differentially private SGD in federated learning, in: IEEE Informatics24(2020)414–423.doi:10.1109/JBHI.2019.2931842.
AICCSA’22,2022,pp.1–8.doi:10.1109/AICCSA56895.2022.10017908. [52] S. M. Arnold, P. Mahajan, D. Datta, I. Bunner, learn2learn, 2019.
[35] Y.Su,C.Huang,W.Zhu,X.Lyu,F.Ji,Multi-partydiabetesmellitus URL:https://github.com/learnables/learn2learn.
riskpredictionbasedonsecurefederatedlearning, Biomed.Signal [53] L. Buitinck, G. Louppe, M. Blondel, F. Pedregosa, A. Mueller,
Process.Control.85(2023)104881.doi:10.1016/J.BSPC.2023.104881. O. Grisel, V. Niculae, P. Prettenhofer, A. Gramfort, J. Grobler,
[36] X. Guo, Q. Yao, J. T. Kwok, W. Tu, Y. Chen, W. Dai, Q. Yang, R. Layton, J. VanderPlas, A. Joly, B. Holt, G. Varoquaux, API
Privacy-preservingstackingwithapplicationtocross-organizational designformachinelearningsoftware:experiencesfromthescikit-
diabetesprediction, in:FederatedLearning-PrivacyandIncentive, learn project, in: ECML PKDD Workshop: Languages for Data
volume12500,2020,pp.269–283.doi:10.1007/978-3-030-63076-8\ MiningandMachineLearning,2013,pp.108–122.
_19. [54] T.Zhu,K.Li,J.Chen,P.Herrero,P.Georgiou, Dilatedrecurrent
[37] I.D.Falco,A.D.Cioppa,T.Koutny,M.Ubl,M.Krcma,U.Scafuri, neuralnetworksforglucoseforecastingintype1diabetes, J.Heal.
E.Tarantino, Afederatedlearning-inspiredevolutionaryalgorithm: InformaticsRes.4(2020)308–324.doi:10.1007/S41666-020-00068-2.
Applicationtoglucoseprediction, Sensors23(2023)2957.doi:10. [55] X.Yu,T.Yang,J.Lu,Y.Shen,W.Lu,W.Zhu,Y.Bao,H.Li,J.Zhou,
3390/S23062957. Deeptransferlearning:anovelglucosepredictionframeworkfornew
[38] S.Canali,V.Schiaffonati,A.Aliverti, Challengesandrecommen- subjectswithtype2diabetes,Complex&IntelligentSystems8(2021)
dations for wearable devices in digital health: Data quality, inter- 1–13.doi:10.1007/s40747-021-00360-7.
operability,healthequity,fairness, PLOSDigitalHealth1(2022) [56] T.Zhu,K.Li,P.Herrero,P.Georgiou, Personalizedbloodglucose
e0000104.doi:10.1371/journal.pdig.0000104. predictionfortype1diabetesusingevidentialdeeplearningandmeta-
[39] V.Sameera,A.Bindra,G.P.Rath,Humanerrorsandtheirprevention learning, IEEETransactionsonBiomedicalEngineering70(2023)
inhealthcare,JournalofAnaesthesiology,ClinicalPharmacology37 193–204.doi:10.1109/TBME.2022.3187703.
(2021)328.doi:10.4103/joacp.JOACP_364_19. [57] S.Langarica,M.Rodriguez-Fernandez,F.Nunez,F.J.DoyleIII, A
[40] T.Zhu,T.Chen,L.Kuang,J.Zeng,K.Li,P.Georgiou, Edge-based meta-learningapproachtopersonalizedbloodglucosepredictionin
temporalfusiontransformerformulti-horizonbloodglucosepredic- type1diabetes,ControlEngineeringPractice135(2023)105498.
tion, in: IEEE ISCAS’23, 2023, pp. 1–5. doi:10.1109/ISCAS46773. [58] Z. Zhang, A mathematical model for predicting glucose levels in
2023.10181448. critically-ill patients: The pignoli model, PeerJ 3 (2015) e1005.
[41] C. Piao, K. Li, Blood glucose level prediction: A graph- doi:10.7717/peerj.1005.
based explainable method with federated learning, arXiv preprint [59] G.Alfian,M.Syafrudin,J.Rhee,M.Anshari,M.Mustakim,I.Fahrur-
arXiv:2312.12541(2023). rozi, Bloodglucosepredictionmodelfortype1diabetesbasedon
[42] F. Prendin, J. Pavan, G. Cappon, S. Del Favero, G. Sparacino, extremegradientboosting,IOPConferenceSeries:MaterialsScience
A. Facchinetti, The importance of interpreting machine learning and Engineering 803 (2020) 012012. doi:10.1088/1757-899X/803/1/
models for blood glucose prediction in diabetes: an analysis using 012012.
shap,ScientificReports13(2023)16865. [60] H.Rubin-Falcone,I.Fox,J.Wiens, Deepresidualtime-seriesfore-
[43] A.D.Zahedani,A.Veluvali,T.McLaughlin,N.Aghaeepour,A.Hos- casting:Applicationtobloodglucoseprediction,in:KDH@ECAI’20,
seinian,S.Agarwal,J.Ruan,S.Tripathi,M.Woodward,N.Hashemi, volume2675,2020,pp.105–109.
M.Snyder, Digitalhealthapplicationintegratingwearabledataand [61] W.Potosnak,C.Challu,K.G.Olivares,A.Dubrawski, Forecasting
behavioralpatternsimprovesmetabolichealth,npjDigit.Medicine6 responsetotreatmentwithglobaldeeplearningandpatient-specific
(2023).doi:10.1038/S41746-023-00956-Y. pharmacokineticpriors.,Arxiv(2023)arXiv–2309.
[44] S.Hochreiter,J.Schmidhuber, Longshort-termmemory, Neural [62] B.Puszkarski,K.Hryniów,G.Sarwas, Comparisonofneuralbasis
Comput.9(1997)1735–1780.doi:10.1162/NECO.1997.9.8.1735. expansionanalysisforinterpretabletimeseries(n-beats)andrecurrent
[45] M.Reddy,P.Pesl,M.Xenou,C.Toumazou,D.Johnston,P.Georgiou, neuralnetworksforheartdysfunctionclassification, Physiological
P.Herrero,N.Oliver, Clinicalsafetyandfeasibilityoftheadvanced Measurement43(2022)064006.
boluscalculatorfortype1diabetesbasedoncase-basedreasoning:A [63] F.Chen,M.Luo,Z.Dong,Z.Li,X.He, Federatedmeta-learning
6-weeknonrandomizedsingle-armpilotstudy, DiabetesTechnology withfastconvergenceandefficientcommunication, arXivpreprint
&Therapeutics18(2016)487–493.doi:10.1089/dia.2015.0413. arXiv:1802.07876(2018).
[46] G. Aleppo, K. J. Ruedy, T. D. Riddlesworth, D. F. Kruger, A. L. [64] R.Zhang,Q.Xu,J.Yao,Y.Zhang,Q.Tian,Y.Wang, Federated
Peters,I.Hirsch,R.M.Bergenstal,E.Toschi,A.J.Ahmann,V.N. domaingeneralizationwithgeneralizationadjustment,in:IEEE/CVF
Shah,M.R.Rickels,B.W.Bode,A.Philis-Tsimikas,R.Pop-Busui, CVPR’23,2023,pp.3954–3963.doi:10.1109/CVPR52729.2023.00385.
H.Rodriguez,E.Eyth,A.Bhargava,C.Kollman,R.W.Beck,R.- [65] T.Zhu,L.Kuang,C.Piao,J.Zeng,K.Li,P.Georgiou, Population-
B.S.Group, Replace-bg:Arandomizedtrialcomparingcontinuous specificglucosepredictionindiabetescarewithtransformer-based
deeplearningontheedge,IEEETransactionsonBiomedicalCircuits
Chengzhe Piao et al. : PreprintsubmittedtoElsevier Page 14 of 14Privacy Preserved Blood Glucose Level Cross-Prediction: An Asynchronous Decentralized Federated Learning Approach
andSystems(2023)1–12.doi:10.1109/TBCAS.2023.3348844.
[66] R. Sergazinov, M. Armandpour, I. Gaynanova, Gluformer:
Transformer-basedpersonalizedglucoseforecastingwithuncertainty
quantification, in: IEEE ICASSP’23, 2023, pp. 1–5. doi:10.1109/
ICASSP49357.2023.10096419.
[67] E.Acuña,R.Aparicio,V.Palomino, Analyzingtheperformanceof
transformersforthepredictionofthebloodglucoselevelconsidering
imputationandsmoothing, BigDataCogn.Comput.7(2023)41.
doi:10.3390/BDCC7010041.
Chengzhe Piao et al. : PreprintsubmittedtoElsevier Page 15 of 14