Human-AI collectives produce the most accurate
differential diagnoses
Nikolas Zo¨ller1*, Julian Berger1, Irving Lin2, Nathan Fu2,
Jayanth Komarneni2, Gioele Barabucci3, Kyle Laskowski2,
Victor Shia4, Benjamin Harack5, Eugene A. Chu6, Vito Trianni7,
Ralf H.J.M. Kurvers1,8*, Stefan M. Herzog1*
1*Center for Adaptive Rationality, Max Planck Institute for Human
Development, Berlin, Germany.
2The Human Diagnosis Project, San Francisco , CA, USA.
3University of Cologne, Cologne,Germany.
4Harvey Mudd College, Claremont, CA, USA.
5Oxford University, Oxford, UK.
6Kaiser Permanente, Downey, CA, USA.
7ISTC-CNR, Roma, Italy.
8 Science of Intelligence Excellence Cluster, Technical University Berlin,
Berlin, Germany.
*Corresponding author(s). E-mail(s): zoeller@mpib-berlin.mpg.de;
kurvers@mpib-berlin.mpg.de; herzog@mpib-berlin.mpg.de;
Abstract
Artificial intelligence systems, particularly large language models (LLMs), are
increasingly being employed in high-stakes decisions that impact both individ-
uals and society at large, often without adequate safeguards to ensure safety,
quality, and equity. Yet LLMs hallucinate [1–4], lack common sense [5], and are
biased[6,7]—shortcomingsthatmayreflectLLMs’inherentlimitationsandthus
may not be remedied by more sophisticated architectures, more data, or more
human feedback. Relying solely on LLMs for complex, high-stakes decisions is
therefore problematic. Here we present a hybrid collective intelligence system
that mitigates these risks by leveraging the complementary strengths of human
experience and the vast information processed by LLMs. We apply our method
toopen-endedmedicaldiagnostics,combining40,762differentialdiagnosesmade
1
4202
nuJ
12
]IA.sc[
1v18941.6042:viXrabyphysicianswiththediagnosesoffivestate-of-theartLLMsacross2,133med-
ical cases. We show that hybrid collectives of physicians and LLMs outperform
bothsinglephysiciansandphysiciancollectives,aswellassingleLLMsandLLM
ensembles. This result holds across a range of medical specialties and profes-
sional experience, and can be attributed to humans’ and LLMs’ complementary
contributions that lead to different kinds of errors. Our approach highlights the
potential for collective human and machine intelligence to improve accuracy in
complex, open-ended domains [8] like medical diagnostics.
Keywords:MedicalDiagnostics,CollectiveIntelligence,LargeLanguageModels,
HealthInformatics
Diagnostic errors are among the most pressing issues in medical practice [9–11], caus-
inganestimated795,000deathsandpermanentdisabilitiesintheUnitedStatesalone
each year [12]. Reducing diagnostic errors—without incurring substantially higher
costs—is essential to improve patient outcomes worldwide. This challenge has moti-
vated a recent surge in diagnostic technologies exploiting artificial intelligence (AI)
to interpret medical records, tests, and images [13, 14]. Deep learning approaches in
medical imaging have shown great promise. Notable examples include mammography
interpretation, cardiac function assessment, and lung cancer screening, some of which
have progressed beyond the testing phase and entered clinical practice [15–17].
Recent years have also witnessed the rise of AI foundation models, especially
LLMs, which show remarkable abilities to process natural language, providing accu-
rateanswerstoquestionsinalmostanydomain,includingmedicine[18–21].However,
a recent meta-analysis [22] found that physicians often outperform LLMs, and that
LLMs differ vastly in performance, also between medical specialties. While LLMs’
performance in the medical domain keeps improving [20], their deployment in clinical
practice remains challenging due to the risk of errors (caused by, e.g., hallucinations
[1–4], biases [6, 7], and lack of common sense [5]) and concerns about their trust-
worthiness [23]. As these shortcomings may reflect inherent limitations of LLMs [24],
developing more sophisticated architectures or using more data or more human feed-
back may not sufficiently address these shortcomings. The tension between the vast
potential of AI-based solutions and the challenges of real-world deployment is not
limited to medical diagnostics. It is also apparent in other domains, especially those
involving high-stakes decisions whose effects are not immediate, such as strategies to
address climate change [25].
Here we present an approach that complements AI responses with human expert
knowledgeinopen-endedmedicaldiagnostics.Thismethod,whichcombinesAIwitha
collective intelligence (CI) approach, benefits from the diversity of solutions provided
by humans and LLMs. CI approaches harness the contributions of multiple experts
to reduce errors and find creative solutions to complex problems [26, 27]. In medical
diagnostics,severalstudieshavefoundthatthecollectivesolutionofmultiplediagnos-
ticians outperforms the average individual across a range of medical contexts [28–34].
These studies have focused on binary or small-scale decision problems (e.g., detecting
2a specific condition), but CI has also proved successful in open-ended medical prob-
lems. While in earlier studies the contributions of individual experts are manually
harmonizedandaggregatedintocollectivediagnoses[35],morerecentlythisapproach
has been fully automatized. Specifically, medical knowledge graphs and natural lan-
guage processing methods are leveraged to harmonize the free-text contributions of
individualexperts[36],whichcandiffersignificantlyduetotheopen-endednessofthe
solution space [8].
In a similar vein, hybrid systems that integrate state-of-the-art LLMs as peers in
a mixed human–AI collective hold promise for addressing complex decision problems
such as medical diagnostics. AI can provide complementary information without per-
petuating the errors and biases of human peers. At the same time, the diagnostic
process is not entirely outsourced to artificial systems, making it possible to benefit
from human experts’ ability to think outside the box, recognize context, and handle
contentious evidence, thus mitigating the risks of LLMs.
Combining the contributions of multiple humans and multiple LLMs is, however,
notstraightforward.AlthoughmanystudieshaveexploredhowtocombinemultipleAI
models(e.g.,ensemblelearningisanestablishedpracticeinmachinelearning[37,38]),
littleisknownabouthowtobestcombinetheoutputsofmultipleLLMs(butsee[39–
42] for specific use cases), or how to combine the responses of multiple LLMs with
thoseofhumanexperts,particularlyinopen-endeddomains.Inthisstudy,wedevelop
ageneral-purposemethodtocombinetheresponsesproducedbybothhumanexperts
and LLMs. Applying this method to a set of over 40,000 diagnoses, we show that
hybridhuman–AIcollectivesoutperformhuman-onlyandLLM-onlycollectivesacross
a variety of medical specialties and levels of professional experience. Additionally, we
demonstrate that when LLMs fail, physicians often provide correct diagnoses, thus
highlighting the crucial importance of maintaining expert involvement, even in the
presence of an ensemble of powerful AIs.
Medical cases, human data and LLM responses
The empirical basis for this work is a dataset from the Human Diagnosis Project
(Human Dx), an online collaborative platform for medical professionals and trainees.
Users from around the world can register on the platform, submit cases, review case
details,andprovidediagnoses.Thecasessubmittedarepublishedonlyifapprovedby
aneditorialboardoflicensedmedicalprofessionals.Eachcaseispresentedasavignette
mimickinginformationthatphysiciansencounterinreal-worldpracticeandcontaining
patient information such as symptoms, medical records, and clinical test results (see
Fig. 1). When responding to a case, users can provide either a single diagnosis or
a ranked list, commonly known as a differential diagnosis, either as free text or by
selecting from a medical taxonomy with an auto-complete feature that activates as
theytype(seeFig.1aforanillustrationoftheuserinterface).Werefertothisresponse
as a differential diagnosis, whether it contains one or multiple diagnoses. Once the
userhassubmittedtheirdifferentialdiagnosis,theyareshownthesolutionasprovided
by cases’ authors and vetted by an expert panel, which may consist of one or several
diagnoses. For our main analyses, we used a set of 2,133 medical cases and 40,762
3differential diagnoses from qualified physicians with different levels of professional
experience (see Methods). In the Supplement, we additionally present results of the
same analyses for medical students.
To compare and combine the human diagnoses with LLM outputs, we provided
the same set of case vignettes to five commercially available or open-source state-of-
the-art LLMs (Anthropic Claude 3 Opus, Google Gemini Pro 1.0, Meta Llama 2 70B,
MistralLarge,andOpenAIGPT-4)andpromptedthemodelstoprovidethefivemost
probable diagnoses, ordered by their probability of being correct (see Methods).
Harmonizing and aggregating open-ended answers
from doctors and LLMs
The procedure for harmonizing and aggregating the human judgements and the LLM
outputisillustratedinFig.1.Inshort,acollectivediagnosisisestablishedbyranking
each diagnosis with a weighted score that is calculated across all humans and LLMs
in the group, taking into account both the rank of the diagnosis in the individual
lists (i.e., higher-ranked diagnoses receive more weight) and the humans’ and LLMs’
performance on a training fold (see Methods). We used a five-fold cross-validation
approach. One fold was used as a training fold to select the best prompting config-
uration for each LLM and to calculate weights for each LLM and each human using
a method proposed by [43]. All humans were assigned the same weight based on the
averagehumanperformanceonthecasesinthetrainingfold.Thecollectivediagnoses
were then evaluated on the remaining four folds (see Methods, Extended Data Fig.
S1, and SI for details).
In order to make the open-ended diagnoses of users and LLMs comparable and
uniquely identifiable, we extended the method described in [36], which maps free-
text diagnoses to concepts (and their unique IDs) in the Systematized Nomenclature
of Medicine Clinical Terms (SNOMED CT) [44] (see Methods). SNOMED CT is a
comprehensive clinical terminology and coding system designed to standardize the
representationofmedicalconceptsandsupporttheaccuratecommunicationofclinical
information in healthcare. After matching diagnoses to SNOMED-CT concepts, the
generation of the collective differential diagnoses proceeds exploiting the SNOMED-
CT IDs (see Fig. 1).
Aggregating LLMs increases performance in
open-ended medical diagnostics
We start by presenting the cross-validated results for the baseline performance of the
five individual LLMs and all possible LLM ensembles. Depending on the use case for
an aggregated collective diagnosis, some performance metrics might be more suitable
thanothers.Forexample,ifthedifferentialdiagnosisofanLLM(ensemble),ahuman
collective, or a hybrid collective serves as a consideration set to support the decision
of a human physician, it may be sufficient that the correct solution is included in the
differential diagnosis at all, and less important that it is ranked first. Therefore, we
report several accuracy metrics, including top-5, top-3, and top-1 accuracy, where a
4Fig. 1: Illustration of the hybrid collective intelligence process, which
combines human diagnoses with LLM outputs to arrive at a collective
differential diagnosis. a, Screenshot of the interface that human users see when
diagnosing a patient case on the Human Dx platform via a mobile device. The infor-
mation provided can include a patient’s symptoms, test results, and medical record.
Users can uncover this information piece by piece and update their diagnosis accord-
ingly. In this analysis, we only consider users’ final differential diagnosis. The same
information shown to human users is also given to LLMs as part of a prompt (see
Methods). b, An illustrative example of the open-ended text responses given by users
and LLMs. Next, extending a method presented in [36] (see Methods and Extended
Data Fig. S1), each single diagnosis is subjected to several processing steps for stan-
dardization, after which it is assigned a unique ID in the SNOMED CT healthcare
terminology. c, Example of a SNOMED CT entry. Crucially, all listed synonyms are
matched to the same SNOMED CT ID. d, Diagnoses of humans and LLMs after the
matching step. e, Collective diagnosis after aggregating the diagnoses from humans
andLLMs.Inthisaggregation,LLMsandhumansareassigneddifferentweightsbased
on their performance in the training fold. The rank r of a diagnosis in a differential
diagnosis is taken into account through a 1/r scoring rule (see Methods).
differential diagnosis is evaluated as correct if the correct diagnosis is among the top
five,topthree,ortoponediagnoses,respectively(andtheaccuracyistheproportionof
such cases). For the fraction of cases where a case author has stated several diagnoses
as correct (34%), a nominated diagnosis is considered correct if it matches any of the
correct diagnoses. Additionally, we report the mean reciprocal rank (MRR) [45], a
well-established performance metric in the field of information retrieval, defined as
C
1 (cid:88) 1
MRR= , (1)
C r
i
i=1
5Fig. 2: Cross-validated performance of five individual LLMs (Anthropic
Claude 3 Opus, OpenAI GPT-4, Mistral Large, Google Gemini Pro 1.0
and Meta Llama 2 70B) and ensembles of all possible combinations of
LLMs. Panels show performance for four outcome metrics (y axes): Top-k indicates
the proportion of cases for which the correct diagnosis was among the k top-ranked
diagnoses(fork ={1,3,5});MRRshowsthemeanreciprocalrankofcorrectdiagnoses
across cases (see eq. 1). The x axis shows the number of LLMs in an ensemble. The
horizontaldashedlineshowstheaverageindividualperformanceofthephysicians(i.e.,
firstaveragedwithincases,thenacrossallcases).Someoftheensemblesoverploteach
other (see Table S1 in the supplement for the performance of all combinations).
where C corresponds to the number of cases on which the metric is evaluated and r
i
istherankofthecorrectanswerinthefinallistforcasei.Notethatifr >5orifthe
i
correctdiagnosisisnotpresentintheranking,wesetr =∞sothatthecontribution
i
of case i to the MRR is null.
As Fig. 2 shows, the individual LLMs differed notably in performance, but aggre-
gating multiple LLMs into ensembles generally increased diagnostic accuracy. The
ensembles performed much better than the worst individual LLM and generally as
well as, or better than, the best individual LLM. For top-5 accuracy, the ensemble of
all LLMs combined clearly outperformed each LLM individually, and this result held
acrossthefivemostcommonmedicalspecialtiesinourdata(cardiology,gastroenterol-
ogy, pulmonology and respirology, neurology, and infectious diseases; see Extended
Data Fig. S2). The same held for top-3 accuracy and MRR when comparing perfor-
mance across all cases, and for four of the five medical specialties (see Extended Data
Fig. S2). For top-1 accuracy, the ensemble of all LLMs performed better than four of
the five individual LLMs, but slightly worse than the best-performing LLM. Whether
or not it is advisable to aggregate several LLMs may therefore depend on the target
metric, but if the purpose is to provide a consideration set to support the decision
of a human physician (e.g., top-5 diagnoses), then LLM ensembles have the greatest
potential.
To put this performance into perspective, Extended Data Fig. S3 shows the per-
centage of physicians who were outperformed by (and/or tied with) individual LLMs
6andLLMensemblesacrossthesetofcasestheyhadsolved.Thispercentagewashigh-
est for an LLM ensemble incorporating all five LLMs (i.e., strictly outperformed 85%
of physicians and outperformed or tied with 93% of physicians). Comparing the indi-
vidual LLM performance with that of the human users showed that four of the five
LLMs outperformed the average physician.
Human–AI collective intelligence outperforms both
humans and LLMs
Next, we test the complementarity of human and LLM solutions in a hybrid CI
approach.Fig.3showsthecross-validatedperformancewhencombiningthediagnoses
of multiple physicians (human-only ensembles as a baseline) with any one of the five
individualLLMsorwithallLLMs.Forthebaselineofhuman-onlyensembles,increas-
ing the number of physicians increased diagnostic accuracy, with greater marginal
increases in accuracy for smaller than for larger group sizes. These results are in line
with earlier findings from a smaller set of Human Dx cases [35, 36].
Crucially, adding one LLM to the human diagnoses consistently increased perfor-
mance for both individual physicians and human-only ensembles of different sizes,
withthelargestincreaseattainedwhenaddingthebest-performingindividualLLMor
an all-LLM ensemble. For top-5 and top-3 performance metrics, adding the all-LLM
ensemble was as good as or better than adding the best-performing LLM. For top-1
Fig. 3: Cross-validated performance of human-only ensembles and hybrid
ensembles of humans and LLMs.Panelsshowperformanceforfouroutcomemet-
rics (y axes): Top-k indicates the proportion of cases for which the correct diagnosis
wasamongthek top-rankeddiagnoses(fork ={1,3,5});MRRshowsthemeanrecip-
rocal rank of correct diagnoses across cases (see eq. 1). The individual performance of
the five LLMs (and their combined performance in an all-LLMs ensemble) is shown
as the left-most square of each color in each panel. The x axis shows the number of
humans added to individual LLMs or to an all-LLMs ensemble.
7accuracy and MRR, adding either the best-performing LLM or the all-LLM ensemble
yielded the best results—which of the two depended on the size of the human group.
Even adding the worst-performing LLM, which by itself performed worse than the
average individual physician, generally led to a slight increase in performance across
all metrics.
From the perspective of human-only ensembles, comparing the performance of
ensembles of n humans with that of hybrid ensembles of n−1 humans plus one LLM
(i.e., the same overall group size of n inputs) showed that adding either the best or
second-best LLM or the all-LLM ensemble to a human-only ensemble outperformed
addinganotherhuman(dependingontheaccuracymetricandgroupsize,thisfinding
also tended to hold for the third- and fourth-best LLM; Fig. 3). From the perspective
of individual LLMs or an all-LLM ensemble, adding one or more human(s) increased
performance; this increase was most pronounced for the worst-performing LLMs.
As Extended Data Figs. S4 and S5 show, these results held across the five most
common medical specialties in our data and for medical students.
Complementarity of human- and LLM-generated
diagnoses
The results presented in Fig. 3 suggest complementarity of physicians and LLMs in
diagnosingopen-endedmedicalproblems.GiventhatmostLLMsoutperformtheaver-
age individual physician, however, how can adding a single physician to an individual
LLM—oreventoanensembleofLLMs—increasediagnosticaccuracy?Thekeyanswer
to this question is that humans and LLMs make different kinds of errors. The lit-
erature on both CI [46, 47] and machine ensembles [37, 48] recognizes that the less
correlated the errors of its members are, the more successful the ensemble will be.
Fig. 4a shows the percentage of cases in which individual physicians and LLMs
placedthecorrectdiagnosisonthesamerankorbothdidnotrankthecorrectdiagnosis
(highlighteddiagonalcells)andthepercentageofcasesinwhichindividualphysicians
andLLMsplacedthecorrectdiagnosisondifferentranks(oritwasonlymentionedby
either a physician or an LLM; all other cells). The results show that individual physi-
cians and LLMs did not assign the correct diagnosis to the same rank in a substantial
number of cases (range across LLMs: 46%–51%). Crucially, when LLMs did not list
the correct diagnosis at all (range across LLMs: 34%–54%; right-most columns), indi-
vidual humans did mention it in a substantial number of cases (range across LLMs:
30%–38%; right-most columns excluding bottom-right cells), most frequently rank-
ing it first (range across LLMs: 20%–27%, top-right cells). In other words, diagnoses
missed by LLMs were often made by individual physicians, frequently in first place.
Thus, although individual physicians performed worse overall than most LLMs (see
Fig. 2 and Extended Data Fig. S3), in a substantial number of cases they were able
to compensate for the LLMs’ errors. Similarly, when individual humans did not list
the correct diagnosis at all (49%, bottom rows), LLMs did in a substantial number of
cases (range across LLMs: 32%–51%; bottom columns excluding bottom-right cells),
most frequently ranking it first (range across LLMs: 15%–33%, bottom-left cells).
8Fig.4bshowsthesameanalysisforfive-physiciancollectivesandLLMs.Giventhat
human collectives outperformed individual humans (see Fig. 3), the diagnoses given
by five-physician collectives are even more complementary to the LLMs than are the
ones given by individual physicians. When LLMs did not list the correct diagnosis at
all (range across LLMs: 34%–54%; right-most columns), human ensembles did so in
the majority of cases (range across LLMs: 55%–65%; right-most columns excluding
bottom-right cells), most frequently ranking it first (range across LLMs: 26%–36%,
top-rightcells).Intriguingly,theoppositepatternwaslesspronounced.Whenhuman-
only ensembles did not list the correct diagnosis at all (22%; bottom rows), LLMs
did so in only the minority of cases (range across LLMs: 17%–32%). For a similar
complementarityanalysisofLLMswithrespecttoeachother,seeExtendedDataFig.
S6.
Fig.4c,dshowshowoftenindividualphysicians,five-physicianensemblesandLLMs
agree with each other on their top-ranked diagnosis. LLMs agree more among them-
selves than with physicians (Fig. 4c) and this difference is particularly pronounced in
situationswherebothhumanandLLMdiagnosesareincorrect(Fig.4d).Furthermore,
when humans and LLMs both make errors (Fig. 4d), they are less likely to assign the
same incorrect diagnosis to the first rank compared to their respective overall agree-
mentrate(whichincludescaseswhereeitherorbothhaverankedthecorrectdiagnosis
first; Fig. 4c). Extended Data Figs. S7 and S8 show that the above conclusions about
error diversity also hold when considering the full range of ranks 1 to 5. This error
diversity is crucial for a CI approach to be effective, and it is significantly more pro-
nounced among hybrid pairs of a physician and an LLM compared to between pairs
of different LLMs. In a collective aggregation scheme based on (weighted) majority
voting, this error diversity ensures that correct diagnoses accumulate more frequently
than incorrect ones, allowing the correct solutions to rise to the top of the collective
differential diagnosis.
Discussion
Our results demonstrate the potential of combining human medical expertise with
large language models (LLMs) to enhance accuracy and reduce errors in open-ended
medicaldiagnostics.Integratingthedifferentialdiagnosisofasinglehumandiagnosti-
cian with the output of a single LLM yielded a better performance than either alone.
AddinganLLMtomultiplephysicians’diagnosesalsoimprovedperformanceinnearly
all scenarios. The individual accuracy of the LLM influenced the performance gain,
with the highest gain from the best-performing LLM. But even the worst-performing
LLM, which was less accurate than the average human, showed positive effects.
Taking an LLM perspective, also the performance of LLMs could be boosted by
adding human judgements. Adding a single physician increased performance for all
LLMs even though individual physicians, on average, performed worse than most
LLMs; and LLM performance increased steadily with adding more humans. The
increase in performance was highest for the worst-performing LLM and lowest for the
best-performing LLM.
9An important component of (hybrid human–machine) CI is that different users
or machines produce independent and diverse errors [46, 47, 49, 50]. We find that
humans and LLMs indeed make complementary errors that disperse throughout the
vastsolutionspace,whilecorrectdiagnosesaccumulateandconvergewhenintegrating
human and LLM diagnoses.
PreviousworkhasshownthepotentialofAIandCIindividually,andtheirhybrid
combination for problems with well-defined, small solution spaces (e.g., categoriza-
tion, probabilistic forecasting, numerical estimation) [51–54]. Here we showed that
theseresultscanbegeneralizedtoopen-endedproblemscoveringavastsolutionspace
(therearemorethan360,000uniquemedicalconceptsintheMarch2023international
edition of SNOMED CT that we used), by using a general-purpose method to auto-
matically harmonize and aggregate the solutions generated by humans and LLMs.
While we demonstrated this method in the domain of medical diagnostics, we believe
that our approach can be generalized to different applications for which structured
domain knowledge is available, allowing the harmonization and principled aggrega-
tionofhumanexpertjudgementsandLLMresponses(e.g.,climatechangeadaptation
management [8]).
Limitations and future research
While our study demonstrates the potential of hybrid human–AI systems in medical
diagnostics, further research is necessary to ensure the safety, reliability, efficacy, and
ethical deployment of this technology in real-world clinical settings.
For instance, although vignette-based studies represent a validated and accepted
paradigm for the study of diagnostic decision-making processes in medicine [55], it
remains an open question as to how well our method translates to actual clinical
practice.Moreover,ourcasevignetteswereselectedbyanexpertpanelatHumanDx,
and users may flag suspicious cases for removal from the Human Dx platform. This
case selection procedure may have excluded very difficult or rare cases. Future work
could consider more ecologically valid or representative ways of selecting cases.
Furthermore, our analyses do not consider the consequences of the treatments
implied by the diagnoses. Future work could study whether our proposed approach
altersthelikelihoodofarrivingatapotentiallybeneficial(orharmful)treatment.Such
research must consider the decision context, as the recommended or accessible treat-
ments may vary depending on the cultural, regional, and institutional circumstances,
as well as the patients’ health insurance plan [e.g., 56].
Finally, our study was not designed to address risks related to fairness and equity
[see, e.g., 57–62]. For example, LLMs have been shown to perpetuate race-based
medicine in their responses [6]. This finding suggests that the clinical medical knowl-
edge encoded in LLMs [20, 21] is tainted by racism, which can leak into medical
diagnoses, resulting in worse health outcomes for disadvantaged groups. Future work
should directly study the extent to which the integration of humans and LLMs
mitigates or amplifies the biases of both parties in medical diagnostics (see also [63]).
Moregenerally,takingahuman-centeredapproachwhendesigninghybridsystems
is essential to compensate for the lack of transparency of AI models and for building
10trustamongallaffectedstakeholders[64–67].Suchanapproachmayhelpidentifyand
mitigate some of the problems of LLMs or hybrid systems already during the design
stage.
Future research could build on our approach in several ways. First, although we
used a systematic prompt engineering approach, more sophisticated techniques have
beendevelopedthatcouldfurtherboostaccuracy(e.g.,treeofthought[68,69],orself-
consistencywithtemperature/top-psampling[70]).Applyingsophisticatedmulti-level
prompt-engineering techniques to generalist foundational models can improve perfor-
mance and even outperform fine-tuned models for the medical domain [71]. Second,
moregenerally,combiningcomputationalmodelsofhumancognitionwithLLMsmight
improve a system’s reasoning performance (see, e.g., [72], for the case of induction).
Third, vignettes could be classified into categories (e.g., medical specialties, number
and type of case findings), and using tailored few-shot examples within these cate-
gorieswhenpromptingLLMsoradjustingweightsforLLMsbasedonthesecategories
may further boost accuracy. Additionally, more sophisticated weighting techniques
could be tested that adjust weights based on fairness, or LLM biases [62]. Fourth, we
onlyconsideredtext-basedcases;futureworkcouldtestthediagnosticperformanceof
large multi-modal models (and hybrid human–AI ensembles) on, for example, images
(e.g., x-rays or histopathological images) or sounds (e.g., echocardiograms) alongside
thetextualinformation[18,19].Fifth,futureworkcouldfurtherexplorethepotential
of hybrid CI with non-experts. Extended Fig. S5 demonstrates that hybrid ensembles
ofLLMsandmedicalstudentswereabletooutperformindividualphysiciansandeven
groups of physicians. Boosting the performance of less qualified individuals by lever-
aging LLMs might have particular potential for underserved regions where access to
experts is limited.
Conclusion
Our study demonstrates the power of hybrid human–AI collectives in the context of
medical diagnostics for general clinical practice. Hybrid collectives outperform both
individualhumanexpertsandLLMs(aswellashuman-onlyandLLM-onlycollectives)
in generating accurate differential diagnoses. This superior hybrid performance is a
direct consequence of physicians and LLMs making different kinds of errors: when
LLMsmissedthecorrectdiagnosis,individualphysiciansoftencontributedthecorrect
diagnosis, rescuing the hybrid performance.
Recent years have seen a surge of research and publications on the potential of
LLMs (e.g., in medical diagnostics; [20]). However, in both science and public dis-
course, there is increasing concern about the lack of safeguards to ensure the safety,
quality, and equity of LLM-based systems [23]. LLMs, despite their impressive capa-
bilities, hallucinate [1–4], lack common sense [5], and are biased [6, 7]—shortcomings
that may reflect LLMs’ inherent limitations [24] and may thus not be remedied by
more sophisticated architectures, more data, or more human feedback.
We posit that the time has come for a second wave of research on LLMs (and AI
in general) that is no longer content to showcase what LLMs can do, propose techni-
cal approaches to fix their flaws (e.g., [4]), and speculate about how human oversight
11could be implemented. Rather, it is crucial to study how to leverage the complemen-
tary strengths of humans and AI by combining the experience and common sense
of experts with the vast information processed by LLMs. In addition to technolog-
ical solutions aimed at addressing problems inherent in an AI system (e.g., using
retrieval-augmentedgenerativeAItotryaddressinghallucinations;[4]),incorporating
complementary human intelligence can help mitigate the risks of LLMs in ways that
purely technological solutions may not ever be able to.
12Fig. 4: Complementarity of solutions from individual humans and human-
onlyensemblesandLLMs.Panelsaandbshow,foreachofthefiveLLMs,matrices
withthepercentagesofcasesforall36combinationsoftheLLM(xaxis)andhumans
(y axis) assigning the correct diagnosis a particular rank (i.e., rank 1, 2, 3, 4, 5 or not
ranked).a,Resultsforindividualphysicians.b,Resultsforfive-physicianhuman-only
ensembles. The highlighted diagonal indicates cases where an LLM and the humans
assigned the correct diagnosis the same rank. Panels c and d show the percentage of
casesinwhichthesamediagnoseswereassignedrankone,comparingindividualphysi-
ciansand5-physicianensemblestoLLMs(leftside),anddifferentLLMstoeachother
(right side). c, Overall rank one agreement, regardless of whether the correct diag-
nosis was included. d, Rank one agreement when both diagnosticians were incorrect.
Results were extracted from the cross-validation procedure by recording the frequen-
cies with which physicians and LLMs assigned the same or a different rank to either
the correct or an incorrect diagnosis, averaged across all cases and the five folds (see
Methods). Note that due to rounding to integers, there may be small inconsistencies
when summing rows or columns across matrices or when comparing sums of values to
respective percentages reported in the main text.
13Methods
Human Dx: Medical diagnostics cases and data from human
solvers
Forouranalyses,weusedadatasetof2,133medicalcaseswithatotalof40,762diag-
nosesprovidedbymedicalexpertsthroughtheuserinterfaceoftheHumanDxapp(see
Fig. 1a). Beforehand, we excluded from our analyses all diagnoses that were incom-
pleteduetosubmissionerrorsorconnectivityissues.Wealsoexcludedthediagnosesof
users who bypassed the onboarding process and of “shadow banned” users, who were
permittedaccesstotheplatformbutexcludedfromanalysesduetounhelpfulbehavior
(e.g., submitting diagnoses consisting of random characters or using profanities). Test
accounts belonging to two Human Dx staff members were also excluded. Finally, we
excluded cases containing images (as the LLMs were not able to process these). The
medical experts consisted of 1,370 attending physicians (37.3%), 139 fellows (3.8%),
and 2,160 resident physicians (58.9%), representing senior doctors, doctors undergo-
ing specialized training, and doctors in training, respectively. Note that this tenure
information is based on self-reports by the users. As Extended Data Fig. S9a shows,
the performance distributions of these three tenure levels were similar; we therefore
combined them into a common category labelled “physicians.” An additional 11,772
diagnoseswerecontributedby1,037medicalstudents;onaverage,thesewerelessaccu-
rate(ExtendedDataFig.S9a,b).Forhybridhuman–LLMensembles(seeFig.3),only
cases diagnosed by a minimum of five physicians were analyzed (so that collectives of
up to five humans could be simulated), totaling 1,928 cases. The medical specialty of
a case (used for the robustness analyses reported in Extended Data Figs. S2 and S4)
was determined by prompting Anthropic Claude 3 Opus to identify the three most
probable specialties from a list of 145 specialties used internally by Human Dx (see
SI for the exact wording of the prompt). Only the most probable specialty was used
in the analyses shown in Extended Data Figs. S2 and S4.
LLMs: Prompt engineering and post-processing of responses
Prompt engineering can markedly affect the quality and format of LLM responses.
There is no established framework for prompt engineering, and which wording pro-
duces the desired response typically depends on the LLM used. Some studies have
foundthatshorterpromptsworkbetter[73];othersthatcomplexpromptsyieldbetter
responses [74]. In practice, prompts are generally engineered by trial and error [74].
We took a systematic, semi-exhaustive approach, building up prompts in a mod-
ular fashion by concatenating several text blocks (Extended Data Fig. S1). The most
basic block feeds the case vignette to the LLM verbatim. The case vignette describes
the patient’s symptoms, test results, and medical record. The LLM is then asked to
provide the five most probable diagnoses ordered by their likelihood of being correct
(i.e.,adifferentialdiagnosis).Weincludedseveraladditionaltextblocksintheprompt
and tested whether these additions increased diagnostic accuracy. The prompt that
performedbestinatrainingfoldofcaseswasthenusedfortheanalysisintheremain-
ing folds. Specifically, the additional text blocks assign the LLM the role of a medical
14expert(impersonation[75]),adviseittocheckthattheproposeddiagnosesareconsis-
tent with the case description (self-consistency), advise the LLM to report diagnoses
in SNOMED CT terminology (answer format SCT) or in common shorthand (answer
format common), or offer five examples of case vignettes with their correct diagnoses
(a technique known as few-shot prompting; [76]). In selecting the few-shot examples,
we sought to ensure variety in patients’ age (5 months to 89 years) and gender (3
female, 2 male) and the medical specialty. The resulting LLM responses constitute
thebasis ofthe resultsreported here.Fordetails ofthe exactwording ofprompts and
results of the validation process, see Extended Data Fig. S1 and SI.
Ourgeneralvalidationapproachisasfollows:Weusedfive-foldcross-validationon
the whole set of cases, using one fold of cases to select the best prompt and calculate
theweightsforhumansandLLMs(see“WeightedaggregationofLLMsand/orhuman
inputs”). The other four folds were used for assessing out-of-sample performance. We
report results averaged across the five cross-validation outcomes.
The raw LLM responses required some additional post-processing (which was not
needed for the human responses). Even when explicitly instructed to provide answers
in a specific format, some LLMs did not always comply and occasionally returned
verbose responses. However, these responses follow typical patterns that are easy to
recognize. Some LLMs, for example, start the response with an introductory sentence
beforeparsingthedifferentialdiagnosisintherequestedformat.Wethereforeremoved
the response until the first line break if the response started with “Sure,...,” “Here
is the...,” “Here are...,” “### Response:...,” “The probable...,” “The differential...,”
“The most probable...,” or “Based on...”. Furthermore, we removed various forms of
list numbering.
Matching raw text to unique medical concepts (SNOMED CT)
One of the main challenges when aggregating individual diagnoses in open-ended
medicaldiagnosticsisdiscerningwhichdiagnosescorrespondtothesamemedicalcon-
cept. The differential diagnoses given by humans and LLMs consist of raw text. Two
strings pointing to the same disease might differ slightly—for example, due to typos,
use of synonyms, or differences in spelling. To facilitate comparison of these open-
ended diagnoses, we developed a method and processing pipeline that leveraged the
comprehensive SNOMED CT healthcare terminology (March 2023 International Edi-
tion Release) and mapped the raw string responses to unique IDs in SNOMED CT
(extending a pipeline described in [36]).
The first step is string normalization, using routine natural language process-
ing tools to standardize all diagnoses—including the correct ones provided by cases’
authors. The normalization procedure involves removing stop words, converting
BritishEnglishtoUSEnglish,convertingpluraltosingular,andidentifyingacronyms;
specifically, we used the Norm1 pipeline, one of the Lexical Tools maintained by the
National Library of Medicine. The second step is to map concepts to SNOMED CT
IDs (Fig. 1c). This is done by comparing a normalized diagnosis string to the nor-
malized entries in SNOMED CT including all of their stored synonyms sharing the
same ID. A SNOMED CT ID is assigned to a diagnosis only when there is an exact
1https://lhncbc.nlm.nih.gov/LSG/Projects/lvg/current/docs/userDoc/tools/norm.html
15match between the sets of words—in other words, the compared strings having a Jac-
card similarity of 1. On the rare occasion that more than one SNOMED CT ID is
matched by this technique, SNOMED CT allows for differentiation by semantic tags.
WegavepreferencetoSNOMEDIDsaccordingtotheirsemantictagsinthefollowing
order: “disorder,” “finding,” “morphologic abnormality,” “body structure,” “person,”
“organism,” “specimen” (see [36] for the rationale behind this ordering), so that a
diagnosis was only matched to exactly one ID.
Applyingthisapproach,asdescribedin[36],producedamatchfor90%ofthecor-
rect case diagnoses, 78% of diagnoses given by LLMs (calculated across all prompts),
and 84% of diagnoses given by humans. For the diagnoses that could not be matched,
we employed a different approach. We created 768-dimensional vector embeddings of
allunique(active)SNOMEDCTconceptsandsynonymsusingasentence-transformer
model based on the pubmedbert model [77]—a domain-specific transformer model
trained on texts from the National Library of Medicine and fine-tuned over the MS-
MARCO dataset using the sentence-transformer framework [78]. We then created a
vectorembeddingofthediagnosistobematchedandassignedittheSNOMEDCTID
for which the cosine similarity between embedding vectors was highest. We were thus
able to match all remaining raw string diagnoses to exactly one SNOMED CT ID.
For example, the diagnosis “Chlamydia infection” which could not be matched before
was now correctly matched to the SNOMED CT concept “Chlamydial infection (dis-
order).”Likewise,“Humanimmunodeficiencyvirusdisease”wascorrectlymatchedto
the SNOMED CT concept “Human immunodeficiency virus infection (disorder).” As
a sanity check, we applied the sentence-transformer matching technique to all diag-
noses that were successfully matched in the first approach (i.e., using the pipeline
describedin[36])andfoundthatbothmethodsarrivedatthesameSNOMEDCTID
for 99.4% of diagnosis strings (given by humans or LLMs).
Weighted aggregation of LLMs and/or human inputs
To aggregate individual diagnoses into a collective diagnosis, we implemented a scor-
ing rule. After normalizing all differential diagnoses and matching them to unique
SNOMEDCTIDs,webuiltasetofallnominatedIDs(seeFig.1b–dandprevioussub-
section).Then,foreachdiagnostician(physicianorLLM)andeachdiagnosis,apartial
score was assigned that was discounted depending on the rank r in the differential
diagnosis (i.e., the list of diagnoses ordered in descending order of judged probabil-
ity of being the correct diagnosis). Following [35, 36], we employed a 1/r rule for the
rank-discounted partial score (i.e., the inverse rank of a diagnosis). Additionally, this
partial score was multiplied by a weight at the level of the diagnostician (see next
paragraph). Finally, for each nominated diagnosis, these partial scores were summed
up over all diagnosticians, and the ranking of the collective differential diagnosis was
defined as a list sorted in decreasing order of the overall score a diagnosis received.
Prior research on CI in medical diagnostics has shown that giving equal weight
to members in a collective when aggregating individual judgements into a collective
diagnosis(i.e.,usingasimpleequal-weightingcombinationrule)performswellaslong
as there is not much difference in individual performance [31]. However, if there are
16substantial differences in individual accuracy, giving the more competent individu-
als higher weights in the aggregation step may improve performance. We therefore
used the Weighted Majority Voting Ensemble (WMVE) approach described in [43] to
determineweightsforLLMsandhumans.Weightsweredeterminedonone-fifthofthe
cases and calculated for each configuration (i.e., combinations for the accuracy met-
ric used and which LLMs and/or the number of human experts). The performance
of the WMVE was then calculated on the remaining four-fifths of the cases. Results
are reported as the means of a five-fold cross-validation (see also “LLMs: Prompt
engineering and post-processing of responses”). At the start of the weight-learning
process, each diagnostician j in an ensemble of n diagnosticians (physicians or LLMs)
is assigned a weight of w = 1. For each case i in the training set, the weights are
j,0
updated according to w = w +α , where α = s
·(n−(cid:80)n
s )/n and s
j,i j,i−1 i i j,i j=1 j,i j,i
is the score of diagnostician j achieved on case i, which depends on the performance
metricused(fortop-k,itiseither1or0;forreciprocalrank,itis1/r);thatis,weesti-
mated weights separately for each metric we evaluated. This means that the weight
increasesifadiagnosticiancorrectlydiagnosesacaseinthetrainingset,withalarger
increase if the diagnoses of other diagnosticians in the ensemble are incorrect. It was
not possible to calculate a weight for each individual physician because many only
ratedafew(ornone)ofthecasesinthetrainingset.Wethereforecalculatedashared,
average weight for all physicians. To this end, for each case in the training set and
for each hybrid configuration with n humans, we built all possible groups of n physi-
cians (i.e., using the physicians who provided a differential diagnosis for that case)
and averaged over them. If the number of possible groups exceeded 100, we randomly
sampled100uniquegroups.Inmostcases,applyingsuchaweightedcombinationrule
outperformed a simple equal-weighting combination rule. However, even with equal
weights applied, LLM and hybrid ensembles generally outperformed individual LLMs
and physicians (see Extended Data Fig. S10).
Data availability
We include one Human Dx case along with the differential diagnoses provided by
humans and LLMs to illustrate our approach which can be found at https://github.
com/nikozoe/human ai collectives. The full dataset cannot be shared publicly due to
privacy and data protection regulations, but can be obtained by contacting Human
Dx.
Code availability
Thecodeusedtorunsimulationsandperformanalysesisavailableat:https://github.
com/nikozoe/human ai collectives.
Acknowledgements
We thank the Human Dx team for providing the data and supporting this research.
ThisworkwasfundedbytheMaxPlanckInstituteforHumanDevelopment,theEuro-
pean Union’s Horizon Europe research and innovation programme within the context
17of the project HACID (GA 101070588), and the Deutsche Forschungsgemeinschaft
(DFG, German Research Foundation) under Germany’s Excellence Strategy–EXC
2002/1“ScienceofIntelligence”–projectnumber390523135.WethankSusannahGoss
for editing the manuscript.
Author information
These authors jointly supervised this work: Ralf H.J.M. Kurvers, Stefan M. Herzog.
Authors and Affiliations
Center for Adaptive Rationality, Max Planck Institute for Human Devel-
opment, Berlin, Germany
Nikolas Z¨oller, Julian Berger, Ralf H.J.M. Kurvers, Stefan M. Herzog
The Human Diagnosis Project, San Francisco , CA, USA
Irving Lin, Nathan Fu, Jayanth Komarneni, Kyle Laskowski
University of Cologne, Cologne,Germany
Gioele Barabucci
Harvey Mudd College, Claremont, CA, USA
Victor Shia
Oxford University, Oxford, UK
Benjamin Harack
Kaiser Permanente, Downey, CA, USA
Eugene A. Chu
ISTC-CNR, Roma, Italy
Vito Trianni
Science of Intelligence Excellence Cluster, Technical University Berlin,
Berlin, Germany
Ralf H.J.M. Kurvers
Contributions
Following the CRediT standard [79].
• Conceptualization: N.Z., J.B., I.L., N.F., J.K., G.B., V.S., B.H., V.T., R.H.J.M.K.,
and S.M.H.
• Data curation: N.Z., J.B., I.L., N.F., and G.B.
• Formal analysis: N.Z., J.B., I.L., N.F., G.B., and S.M.H.
• Funding acquisition: I.L., J.K., G.B., V.T., R.H.J.M.K., and S.M.H.
• Investigation: N.Z., J.B., I.L., N.F., and G.B.
• Methodology: N.Z., J.B., I.L., N.F., G.B., K.A.L., V.S., B.H., E.A.C., V.T.,
R.H.J.M.K., and S.M.H.
• Project administration: N.Z., I.L., N.F., J.K., G.B., V.T., R.H.J.M.K., and S.M.H.
• Resources: I.L., N.F., and K.A.L.
• Software: N.Z., J.B., I.L., N.F., G.B., K.A.L., V.S., and B.H.
• Supervision: I.L., J.K., G.B., E.A.C., V.T., R.H.J.M.K., and S.M.H.
• Validation: N.Z., J.B., N.F., G.B., V.T., R.H.J.M.K., and S.M.H.
18• Visualization: N.Z., J.B., G.B., and S.M.H.
• Writing - original draft: N.Z., J.B., G.B., R.H.J.M.K., and S.M.H.
• Writing - review & editing: N.Z., J.B., I.L., N.F., J.K., G.B., K.A.L., V.S., B.H.,
E.A.C., V.T., R.H.J.M.K., and S.M.H.
Corresponding authors
Correspondence to Nikolas Z¨oller, Ralf H.J.M. Kurvers or Stefan M. Herzog.
Ethics declarations
We did not collect data explicitly for this study; instead, we analyzed existing data
provided by Human Dx. When users sign up on the Human Dx platform, they give
consentfortheirdatatobeprocessedandanalyzedforresearchpurposes.Weconsulted
the Ethics Committee of the Max-Planck Institute for Human Development and one
of their representatives provided guidelines for study procedures. We have complied
with all relevant ethical regulations regarding data protection.
Competing interests
Irving Lin and Jayanth Komarneni have personal financial interests in Human Dx.
NathanFu,GioeleBarabucciandKyleA.LaskowskiareHumanDxconsultants.Vic-
tor Shia, Benjamin Harack and Eugene A. Chu were previously employed by Human
Dx.
19Extended data figures and tables
1 Find the best prompt combination per LLM
Base “ n inP o fr o o e rxv mpid alae tin oo a nn t il ooy r nt th , a e n s o km .r o e s c t a p p r i to ub laa tb iole n d oi ff f te hr ee n ct aia sl e d iagnosis, T ce os mt b1 i6 n ad ti iff oe nr se n ot f prompts
Impersonation “you are a medical expert diagnosing a patient“
Self-consistency “ isC ch oe nc sk i sth tea nt te wac ith h d ei aff ce hr e fin nt dia inl d g ia ing tn ho es i cs a i sn e y do eu sr ca rn ips tw ioe nr . “
Few shot Five example cases with answers
… …
Best prompt combination
2 Weighted Majority Voting Ensemble
Start with same
weight for all Weights increase if a correct diagnosis is
m ene sm embe br ls e in provided, and they increase even more if
other LLMs or humans in the ensemble made
Update weights incorrect diagnoses.
for every case in
training fold
Determine final weights
Claude 3 Ge Pm roini LLaMA 2 M Lais rt gr eal GPT-4 HAv ue mra ag ne s
Execute over remaining folds
1 2 3 4 5 MRR ∀\Fold1
1 2 3 4 5 MRR ∀\Fold2 1/5
x
1 2 3 4 5 MRR ∀\Fold3 ∑(MRRFolds)
=
1 2 3 4 5 MRR ∀\Fold4 MRRCV
1 2 3 4 5 MRR ∀\Fold5
Cross Validation Folds MRR Results
Extended Data Fig. S1: Illustration of LLM prompt engineering and vali-
dation method. We nested our prompt engineering and Weighted Majority Voting
Ensemble (WMVE) [43] sequence in a five-fold cross-validation procedure. First, we
determined which prompt performed best for each LLM in the training fold (one-
fifth of the data; see Methods). Second, we calculated weights for each member of
the ensemble, also using the training fold. The weights were then used to aggregate
collective diagnoses and evaluate the ensemble’s performance on the remaining folds
(four-fifths of the data). This process yields one result per fold, of which the averages
are reported in the main text. We repeated this procedure for every metric reported
in the main text (i.e., top-1, top-3, top-5 and MRR).
20
dlof
rep
ecnOExtended Data Fig. S2: Cross-validated performance of five individual
LLMs and ensembles of all possible combinations of LLMs for the five
most common specialties in the dataset. Across all medical specialties, com-
bining several LLMs into a collective increased diagnostic accuracy relative to the
best-performing individual LLM across all performance metrics except top-1. In most
cases, the best results were obtained by combining all LLMs.
21Extended Data Fig. S3: Cross-validated relative performance of individual
humans and LLMs.a,PercentageofphysiciansoutperformedbyanLLMacrossthe
cases they solved. The analysis was limited to physicians who diagnosed five or more
cases (n = 1,997). A physician was outperformed on a case if the LLM (ensemble)
placed the correct diagnosis at a higher rank; a physician was counted as outper-
formedoveralliftheywereoutperformedmoreoftenthantheyoutperformedtheLLM
(ensemble) across their set of solved cases. b, Percentage of physicians outperformed
by or tied with an LLM (ensemble) across the cases they solved. c and d, Results
for the LLM ensemble only. c, Histogram of the number of physicians outperformed
on a certain percentage of cases by the LLM ensemble. d, Histogram of the number
of physicians outperformed or tied with on a certain percentage of cases by the LLM
ensemble.candddifferconsiderablyduetothesignificantnumberoftiesonthecase
level.
22ExtendedDataFig.S4:Cross-validatedperformanceofhuman-onlyensem-
bles and hybrid ensembles of humans and LLMs for the five most common
specialties in the dataset. Across all medical specialties, combining humans and
LLMs increased diagnostic accuracy relative to individual humans or LLMs. Increas-
ing the number of humans in the ensemble generally increased performance. The best
top-5 and top-3 accuracies were achieved by adding all LLMs to the ensemble, the
best top-1 or MRR performance was achieved by adding either all LLMs or, in some
cases, the best-performing individual LLM.
23Extended Data Fig. S5: Cross-validated performance of medical-student-
only ensembles and hybrid ensembles of medical students and LLMs.Panels
show performance for four outcome metrics (y axes): Top-k indicates the proportion
ofcasesforwhichthecorrectdiagnosiswasamongthektop-rankeddiagnoses(fork =
{1,3,5}); MRR shows the mean reciprocal rank of correct diagnoses across cases (see
eq.1).TheindividualperformanceofthefiveLLMs(andtheircombinedperformance
inanall-LLMsensemble)isshownastheleft-most squareofeachcolorineachpanel.
The x axis shows the number of medical students added to individual LLMs or an
all-LLMs ensemble. For comparison, the gray boxes and lines show the performance
of physicians on the same set of cases. Results are based on 974 medical cases, each
diagnosed by at least five medical students.
24Extended Data Fig. S6: Complementarity of solutions among LLMs: Rank
of correct diagnoses within the differential diagnoses of LLMs. Panels show,
for each of the ten possible pairs of LLMs, matrices with the percentages of cases
for all 36 combinations of the LLM at the top (x axis) and LLMs on the side (y
axis) assigning the correct diagnosis a particular rank (i.e., rank 1, 2, 3, 4, 5, or not
ranked).ThehighlighteddiagonalshowscaseswhereLLMsassignedthesamerankto
the correct diagnosis. Results were extracted from the cross-validation procedure by
recordingthefrequencieswithwhichtheLLMsassignedthecorrectdiagnosisthesame
or a different rank, averaged across all cases and the five folds (see Methods). Note
that due to rounding to integers, there may be small inconsistencies when summing
rows or columns across matrices.
25ExtendedDataFig.S7:AgreementamongphysiciansandLLMs.Panelsshow
thepercentageofcasesinwhichthesamediagnoseswereassignedtoaparticularrank
combination, comparing individual physicians and 5-physician ensembles to LLMs.
a, Overall agreement, regardless of whether the correct diagnosis was included in a
particular rank combination. b, Agreement when both diagnosticians were incorrect
forarankcombination.Resultswereextractedfromthecross-validationprocedureby
recordingthefrequencieswithwhichphysiciansandLLMsassignedthesamediagnosis
to a rank combination, averaged across all cases and the five folds (see Methods).
26Extended Data Fig. S8: Agreement among LLMs Panels show the percentage
of cases in which the same diagnoses were assigned to a particular rank combination,
comparing different LLMs to each other. a, Overall agreement, regardless of whether
the correct diagnosis was included in a particular rank combination. b, Agreement
when both LLMs were incorrect for a rank combination. Results were extracted from
the cross-validation procedure by recording the frequencies with LLMs assigned the
samediagnosistoarankcombination,averagedacrossallcasesandthefivefolds(see
Methods).
27a b
Attending 200
Fellow
100
Resident
Student
0
0.0 0.4 0.8 1.2 −1.0 −0.5 0.0 0.5 1.0
MRR MRR difference between physicians and students on a case
Extended Data Fig. S9: Comparison of individual human performance
across tenure levels. a, Density estimates of individual MRR values (x axis) for
humans who solved at least five cases by tenure level. Vertical lines represent median
MRR values within the tenure level. As the performance distributions of the three
most experienced tenure levels (i.e., attending physicians, fellows, and resident physi-
cians,representingseniordoctors,doctorsundergoingspecializedtraining,anddoctors
intraining,respectively)weresimilar,wecombinedthesethreegroups.b,Comparing
thepooledperformanceofphysicianswiththatofmedicalstudents.Foreachcase,the
mean MRR across all physicians and all medical students was calculated. The distri-
bution shows the difference in MRR between the two groups at the case level, with
positive(negative)valuesindicatinghigherMRRforphysicians(students).Physicians
andstudentsshowednodifferenceinMRRinmanycases,buttheMRRforphysicians
was generally higher MRR than that for medical students.
28
sesac
fo
tnuoCExtendedDataFig.S10:Effectofweightingoncross-validatedperformance
of LLM and hybrid human–LLM ensembles. a, Cross-validated performance (y
axis)offiveindividualLLMsandensemblesofallpossiblecombinationsofLLMswith
equal weights applied in the aggregation step. b, Signed difference in performance
between weighted and unweighted aggregation (y axis); higher values indicate greater
gainsfromapplyingweights.c,Cross-validatedperformanceofhybridphysician-LLM
ensembles with equal weights applied in the aggregation step (y axis). d, Signed
difference in performance between weighted and unweighted aggregation for hybrid
human–LLM ensembles. For most LLM- and hybrid ensembles, using the Weighted
Majority Voting Ensemble approach [43] increased diagnostic accuracy, particularly
for top-1 and MRR.
29Supplementary information
Extended Data Table S1: LLM ensemble performance based on 2,133 case
vignettes. Results reported are the averages of five-fold cross validation, where one
fold was used to determine the best prompt and weights for each LLM according to
WMVE (see Methods) and the other four folds were used to calculate performance.
Combo Top 1 Top 3 Top 5 MRR
Claude 3 0.487 0.618 0.650 0.553
Claude 3—Llama 2 70B 0.487 0.617 0.669 0.556
Claude 3—Llama 2 70B—Mistral Large 0.477 0.630 0.690 0.554
Claude 3—Mistral Large 0.487 0.628 0.684 0.558
Gemini Pro 0.408 0.519 0.554 0.464
Gemini Pro—Claude 3 0.486 0.633 0.693 0.562
Gemini Pro—Claude 3—Llama 2 70B 0.473 0.639 0.696 0.558
Gemini Pro—Claude 3—Llama 2 70B—Mistral Large 0.469 0.645 0.705 0.557
Gemini Pro—Claude 3—Mistral Large 0.474 0.643 0.703 0.561
Gemini Pro—Llama 2 70B 0.408 0.541 0.601 0.477
Gemini Pro—Llama 2 70B—Mistral Large 0.421 0.572 0.637 0.500
Gemini Pro—Mistral Large 0.415 0.569 0.630 0.493
Llama 2 70B 0.231 0.392 0.448 0.317
Llama 2 70B—Mistral Large 0.387 0.523 0.596 0.461
Mistral Large 0.387 0.519 0.571 0.456
GPT-4 0.444 0.587 0.627 0.514
GPT-4—Claude 3 0.487 0.645 0.698 0.565
GPT-4—Claude 3—Llama 2 70B 0.486 0.643 0.706 0.565
GPT-4—Claude 3—Llama 2 70B—Mistral Large 0.481 0.647 0.712 0.565
GPT-4—Claude 3—Mistral Large 0.486 0.649 0.710 0.567
GPT-4—Gemini Pro 0.449 0.621 0.676 0.534
GPT-4—Gemini Pro—Claude 3 0.488 0.655 0.713 0.573
GPT-4—Gemini Pro—Claude 3—Llama 2 70B 0.486 0.655 0.718 0.572
GPT-4—Gemini Pro—Claude 3—Llama 2 70B—Mistral Large 0.482 0.655 0.718 0.569
GPT-4—Gemini Pro—Claude 3—Mistral Large 0.484 0.655 0.717 0.570
GPT-4—Gemini Pro—Llama 2 70B 0.445 0.618 0.682 0.534
GPT-4—Gemini Pro—Llama 2 70B—Mistral Large 0.448 0.622 0.687 0.536
GPT-4—Gemini Pro—Mistral Large 0.447 0.619 0.683 0.535
GPT-4—Llama 2 70B 0.444 0.586 0.649 0.519
GPT-4—Llama 2 70B—Mistral Large 0.440 0.607 0.671 0.523
GPT-4—Mistral Large 0.439 0.606 0.663 0.520
30Extended Data Table S2: LLM performance per prompt and accuracy metric on
thewholedatasetof2,133cases.Notethatinthecross-validationresultspresentedin
the main text, prompts were selected on a subset (one fold) of these cases.
Prompt MRR Top 1 Top 3 Top 5
Claude 3
base common 0.499 0.441 0.557 0.585
base common fewshot 0.558 0.494 0.622 0.650
base common selfconsistent 0.509 0.449 0.568 0.594
base common selfconsistent fewshot 0.554 0.490 0.619 0.645
base impersonation common 0.492 0.433 0.550 0.578
base impersonation common fewshot 0.553 0.488 0.617 0.646
base impersonation common selfconsistent 0.501 0.440 0.562 0.586
base impersonation common selfconsistent fewshot 0.557 0.492 0.622 0.651
base impersonation sct 0.454 0.379 0.526 0.568
base impersonation sct fewshot 0.543 0.471 0.607 0.654
base impersonation sct selfconsistent 0.469 0.396 0.542 0.579
base impersonation sct selfconsistent fewshot 0.539 0.465 0.608 0.649
base sct 0.453 0.379 0.525 0.565
base sct fewshot 0.549 0.478 0.620 0.658
base sct selfconsistent 0.460 0.388 0.529 0.571
base sct selfconsistent fewshot 0.542 0.468 0.613 0.651
Gemini Pro
base common 0.386 0.339 0.430 0.458
base common fewshot 0.462 0.411 0.508 0.539
base common selfconsistent 0.391 0.346 0.432 0.460
base common selfconsistent fewshot 0.466 0.413 0.513 0.548
base impersonation common 0.391 0.343 0.436 0.463
base impersonation common fewshot 0.450 0.395 0.499 0.533
base impersonation common selfconsistent 0.398 0.350 0.443 0.470
base impersonation common selfconsistent fewshot 0.457 0.401 0.505 0.545
base impersonation sct 0.402 0.328 0.472 0.515
base impersonation sct fewshot 0.465 0.407 0.519 0.557
base impersonation sct selfconsistent 0.413 0.341 0.482 0.522
base impersonation sct selfconsistent fewshot 0.465 0.403 0.523 0.559
base sct 0.406 0.335 0.472 0.512
base sct fewshot 0.470 0.412 0.523 0.559
base sct selfconsistent 0.423 0.354 0.487 0.529
base sct selfconsistent fewshot 0.468 0.408 0.523 0.558
Llama 2 70B
base common 0.311 0.225 0.387 0.446
base common fewshot 0.271 0.228 0.305 0.339
base common selfconsistent 0.315 0.231 0.389 0.450
Continued on next page
31Table S2 – continued from previous page
Prompt MRR top1 top3 top5
base common selfconsistent fewshot 0.285 0.224 0.335 0.377
base impersonation common 0.296 0.211 0.373 0.431
base impersonation common fewshot 0.278 0.217 0.331 0.372
base impersonation common selfconsistent 0.303 0.215 0.383 0.440
base impersonation common selfconsistent fewshot 0.284 0.208 0.348 0.401
base impersonation sct 0.305 0.221 0.381 0.438
base impersonation sct fewshot 0.231 0.146 0.310 0.364
base impersonation sct selfconsistent 0.312 0.228 0.388 0.446
base impersonation sct selfconsistent fewshot 0.246 0.154 0.332 0.386
base sct 0.312 0.228 0.386 0.446
base sct fewshot 0.228 0.164 0.287 0.331
base sct selfconsistent 0.323 0.238 0.401 0.455
base sct selfconsistent fewshot 0.263 0.184 0.335 0.386
Mistral Large
base common 0.428 0.347 0.505 0.546
base common fewshot 0.468 0.395 0.532 0.581
base common selfconsistent 0.434 0.353 0.511 0.550
base common selfconsistent fewshot 0.467 0.395 0.532 0.580
base impersonation common 0.428 0.356 0.493 0.536
base impersonation common fewshot 0.460 0.390 0.525 0.570
base impersonation common selfconsistent 0.428 0.358 0.493 0.532
base impersonation common selfconsistent fewshot 0.463 0.391 0.528 0.577
base impersonation sct 0.321 0.237 0.400 0.451
base impersonation sct fewshot 0.445 0.373 0.505 0.561
base impersonation sct selfconsistent 0.326 0.244 0.409 0.450
base impersonation sct selfconsistent fewshot 0.446 0.372 0.508 0.566
base sct 0.347 0.256 0.435 0.484
base sct fewshot 0.447 0.372 0.512 0.568
base sct selfconsistent 0.355 0.261 0.442 0.498
base sct selfconsistent fewshot 0.449 0.374 0.515 0.570
GPT-4
base common 0.478 0.412 0.541 0.577
base common fewshot 0.519 0.451 0.584 0.616
base common selfconsistent 0.487 0.423 0.548 0.580
base common selfconsistent fewshot 0.516 0.446 0.584 0.618
base impersonation common 0.462 0.396 0.527 0.557
base impersonation common fewshot 0.503 0.434 0.570 0.600
base impersonation common selfconsistent 0.471 0.409 0.531 0.562
base impersonation common selfconsistent fewshot 0.509 0.443 0.575 0.606
base impersonation sct 0.410 0.329 0.486 0.531
base impersonation sct fewshot 0.504 0.425 0.575 0.623
Continued on next page
32Table S2 – continued from previous page
Prompt MRR top1 top3 top5
base impersonation sct selfconsistent 0.433 0.356 0.502 0.544
base impersonation sct selfconsistent fewshot 0.512 0.433 0.584 0.628
base sct 0.418 0.338 0.491 0.537
base sct fewshot 0.516 0.440 0.587 0.630
base sct selfconsistent 0.443 0.367 0.514 0.555
base sct selfconsistent fewshot 0.517 0.438 0.592 0.631
Example case vignette
40 year old female presenting with chest pain
Symptom: Worsening chest pain
• Onset: 2 weeks ago
• Associated with: Cough, dyspnea, fever
• Complicated by: Fatigue
Social history
• Recent construction in Ohio
Physical exam
• Lungs: Wheezing
Diagnostic: X-ray
• Interpretation: Normal
Verbatim text of LLM prompts
Below we list the text of the building blocks used to construct prompts for eliciting
differentialdiagnoses.ForallLLMs,wetestedallfeasiblecombinationsofthesebuild-
ing blocks. Diagnostic accuracies for each LLM and each prompt combination on the
whole dataset of 2133 cases are presented in Extended Data Table S2.
Base task: Provide only the most probable differential diagnosis, no explanation, no
recapitulation of the case information or task. Give a maximum of 5 answers, sorted
byprobabilityofbeingthecorrectdiagnosis,mostprobablefirst,removelistnumbering,
and respond with each answer on a new line. Be as concise as possible, no need to be
polite.
Impersonation: You are a medical expert diagnosing a patient.
Response format: common shorthand: In your answer use common shorthand
non-abbreviated diagnoses.
Response format: SCT:InyouranswerprovideonlytheappropriateSNOMEDCT
fully specified name, no id.
Self-consistency: Check that each differential diagnosis in your answer is consistent
with each finding in the case description.
33Few-shot prompt: Here are some examples of cases and their correct answers:
Case description: {case vignette}
Answer: {example solution} (5x)
The prompt used to have Anthropic Claude 3 Opus assign a medical specialty to a
case vignette was as follows:
In the list of medical specialties within the following <SP>\<SP>tags
<SP>{specialties one line}<\SP>, each specialty is separated by a comma. You are
given the following case in the <case>\<case>tags: <case>{case text}<\case>. Fol-
low the steps provided: 1. Determine the top 3 choices for which specialties best fit the
case. 2. Give your answer in an ordered numbered list starting with the most confident
answer first. Only answer with the list. Do not provide any additional explanation.
34References
[1] Hong, G., Gema, A.P., Saxena, R., Du, X., Nie, P., Zhao, Y., Perez-Beltrachini,
L., Ryabinin, M., He, X., Fourrier, C., Minervini, P.: The Hallucinations Leader-
board: An open effort to measure hallucinations in large language models. arXiv
(2024). https://arxiv.org/html/2404.05904v1
[2] Ji,Z.,Lee,N.,Frieske,R.,Yu,T.,Su,D.,Xu,Y.,Ishii,E.,Bang,Y.J.,Madotto,
A., Fung, P.: Survey of hallucination in natural language generation. ACM
Computing Surveys 55(12), 1–38 (2023) https://doi.org/10.1145/3571730
[3] Pal, A., Umapathi, L.K., Sankarasubbu, M.: Med-HALT: Medical Domain Hal-
lucination Test for large language models. arXiv (2023). https://arxiv.org/abs/
2307.15343
[4] Tonmoy, S.M.T.I., Zaman, S.M.M., Jain, V., Rani, A., Rawte, V., Chadha, A.,
Das, A.: A comprehensive survey of hallucination mitigation techniques in large
language models. arXiv (2024). http://arxiv.org/abs/2401.01313
[5] Williams, S., Huckle, J.: Easy problems that LLMs get wrong. arXiv (2024).
https://arxiv.org/html/2405.19616
[6] Omiye, J.A., Lester, J.C., Spichak, S., Rotemberg, V., Daneshjou, R.: Large lan-
guage models propagate race-based medicine. npj Digital Medicine 6(1), 195
(2023) https://doi.org/10.1038/s41746-023-00939-z
[7] Navigli, R., Conia, S., Ross, B.: Biases in large language models: Origins, inven-
tory,anddiscussion.JournalofDataandInformationQuality15(2),1–21(2023)
https://doi.org/10.1145/3597307
[8] Trianni, V., Nuzzolese, A.G., Porciello, J., Kurvers, R.H.J.M., Herzog, S.M.,
Barabucci, G., Berditchevskaia, A., Fung, F.: Hybrid collective intelligence for
decision support in complex open-ended domains. In: Lukowicz, P., Mayer, S.,
Koch, J., Shawe-Taylor, J., Tiddi, I. (eds.) HHAI 2023: Augmenting Human
Intellect. Frontiers in Artificial Intelligence and Applications, vol. 368, pp.
124–137. IOS Press, Amsterdam, Netherlands (2023). https://doi.org/10.3233/
FAIA230079
[9] Makary, M.A., Daniel, M.: Medical error: The third leading cause of death in the
US. BMJ 353, 2139 (2016) https://doi.org/10.1136/bmj.i2139
[10] Leape, L.L., Brennan, T.A., Laird, N., Lawthers, A.G., Localio, A.R., Barnes,
B.A., Hebert, L., Newhouse, J.P., Weiler, P.C., Hiatt, H.: The nature of adverse
events in hospitalized patients: Results of the Harvard Medical Practice Study
II.NewEnglandJournalofMedicine324(6),377–384(1991)https://doi.org/10.
1056/NEJM199102073240605
35[11] Graber, M.L., Franklin, N., Gordon, R.: Diagnostic error in internal medicine.
Archives of Internal Medicine 165(13), 1493–1499 (2005) https://doi.org/10.
1001/archinte.165.13.1493
[12] Newman-Toker,D.E.,Nassery,N.,Schaffer,A.C.,Yu-Moe,C.W.,Clemens,G.D.,
Wang, Z., Zhu, Y., Tehrani, A.S.S., Fanai, M., Hassoon, A., Siegal, D.: Burden
of serious harms from diagnostic error in the USA. BMJ Quality & Safety 33(2),
109–120 (2024) https://doi.org/10.1136/bmjqs-2021-014130
[13] Basu, K., Sinha, R., Ong, A., Basu, T.: Artificial intelligence: How is it changing
medical sciences and its future? Indian Journal of Dermatology 65(5), 365–370
(2020) https://doi.org/10.4103/ijd.IJD 421 20
[14] Mirbabaie, M., Stieglitz, S., Frick, N.R.: Artificial intelligence in disease diag-
nostics: A critical review and classification on the current state of research
guiding future direction. Health and Technology 11(4), 693–731 (2021) https:
//doi.org/10.1007/s12553-021-00555-5
[15] Rajpurkar, P., Chen, E., Banerjee, O., Topol, E.J.: AI in health and
medicine. Nature Medicine 28(1), 31–38 (2022) https://doi.org/10.1038/
s41591-021-01614-0
[16] Aggarwal,R.,Sounderajah,V.,Martin,G.,Ting,D.S.W.,Karthikesalingam,A.,
King,D.,Ashrafian,H.,Darzi,A.:Diagnosticaccuracyofdeeplearninginmedical
imaging: A systematic review and meta-analysis. npj Digital Medicine 4(1), 65
(2021) https://doi.org/10.1038/s41746-021-00438-z
[17] Dembrower, K., Crippa, A., Col´on, E., Eklund, M., Strand, F.: Artificial intel-
ligence for breast cancer detection in screening mammography in Sweden: A
prospective, population-based, paired-reader, non-inferiority study. The Lancet
Digital Health 5(10), 703–711 (2023) https://doi.org/10.1016/S2589-7500(23)
00153-X
[18] Lu, M.Y., Chen, B., Williamson, D.F.K., Chen, R.J., Zhao, M., Chow, A.K.,
Ikemura,K.,Kim,A.,Pouli,D.,Patel,A.,Soliman,A.,Chen,C.,Ding,T.,Wang,
J.J., Gerber, G., Liang, I., Le, L.P., Parwani, A.V., Weishaupt, L.L., Mahmood,
F.: A multimodal generative AI copilot for human pathology. Nature, 1–3 (2024)
https://doi.org/10.1038/s41586-024-07618-3 . Accessed 2024-06-14
[19] Moor,M.,Banerjee,O.,Abad,Z.S.H.,Krumholz,H.M.,Leskovec,J.,Topol,E.J.,
Rajpurkar, P.: Foundation models for generalist medical artificial intelligence.
Nature 616(7956), 259–265 (2023) https://doi.org/10.1038/s41586-023-05881-4
[20] Singhal, K., Azizi, S., Tu, T., Mahdavi, S.S., Wei, J., Chung, H.W., Scales,
N., Tanwani, A., Cole-Lewis, H., Pfohl, S., Payne, P., Seneviratne, M., Gamble,
P., Kelly, C., Babiker, A., Sch¨arli, N., Chowdhery, A., Mansfield, P., Demner-
Fushman, D., Agu¨era y Arcas, B., Webster, D., Corrado, G.S., Matias, Y.,
36Chou, K., Gottweis, J., Tomasev, N., Liu, Y., Rajkomar, A., Barral, J., Sem-
turs, C., Karthikesalingam, A., Natarajan, V.: Large language models encode
clinical knowledge. Nature 620(7972), 172–180 (2023) https://doi.org/10.1038/
s41586-023-06291-2
[21] Jiang, L.Y., Liu, X.C., Nejatian, N.P., Nasir-Moin, M., Wang, D., Abidin, A.,
Eaton, K., Riina, H.A., Laufer, I., Punjabi, P., Miceli, M., Kim, N.C., Orillac,
C., Schnurman, Z., Livia, C., Weiss, H., Kurland, D., Neifert, S., Dastagirzada,
Y.,Kondziolka,D.,Cheung,A.T.M.,Yang,G.,Cao,M.,Flores,M.,Costa,A.B.,
Aphinyanaphongs, Y., Cho, K., Oermann, E.K.: Health system-scale language
models are all-purpose prediction engines. Nature 619(7969), 357–362 (2023)
https://doi.org/10.1038/s41586-023-06160-y
[22] Takita,H.,Walston,S.L.,Tatekawa,H.,Saito,K.,Tsujimoto,Y.,Miki,Y.,Ueda,
D.:DiagnosticperformanceofgenerativeAIandphysicians:Asystematicreview
andmeta-analysis.ColdSpringHarborLaboratoryPress(2024).https://doi.org/
10.1101/2024.01.20.24301563
[23] Chakravorti, B.: AI’s trust problem. Harvard Business Review (2024, May 3).
https://hbr.org/2024/05/ais-trust-problem
[24] Lenat,D.,Marcus,G.:GettingfromgenerativeAItotrustworthyAI:WhatLLMs
might learn from Cyc. arXiv (2023). https://arxiv.org/abs/2308.04445
[25] Cowls, J., Tsamados, A., Taddeo, M., Floridi, L.: The AI gambit: Leveraging
artificial intelligence to combat climate change—opportunities, challenges, and
recommendations. AI & SOCIETY 38(1), 283–307 (2023) https://doi.org/10.
1007/s00146-021-01294-x
[26] Woolley,A.W.,Chabris,C.F.,Pentland,A.,Hashmi,N.,Malone,T.W.:Evidence
for a collective intelligence factor in the performance of human groups. Science
330(6004), 686–688 (2010) https://doi.org/10.1126/science.1193147
[27] Woolley, A.W., Gupta, P.: Understanding collective intelligence: Investigating
the role of collective memory, attention, and reasoning processes. Perspec-
tives on Psychological Science 19(2), 344–354 (2024) https://doi.org/10.1177/
17456916231191534
[28] Hasan,E.,Duhaime,E.,Trueblood,J.S.:Boostingwisdomofthecrowdformed-
ical image annotation using training performance and task features. Cognitive
Research: Principles and Implications 9(1), 31 (2024) https://doi.org/10.1186/
s41235-024-00558-6
[29] Hautz, W.E., K¨ammer, J.E., Schauber, S.K., Spies, C.D., Gaissmaier, W.: Diag-
nostic performance by medical students working individually or in teams. JAMA
313(3), 303–304 (2015) https://doi.org/10.1001/jama.2014.15770
37[30] Kattan, M.W., O’Rourke, C., Yu, C., Chagin, K.: The wisdom of crowds of doc-
tors:Theiraveragepredictionsoutperformtheirindividualones.MedicalDecision
Making 36(4), 536–540 (2016) https://doi.org/10.1177/0272989X15581615
[31] Kurvers, R.H., Herzog, S.M., Hertwig, R., Krause, J., Carney, P.A., Bogart, A.,
Argenziano, G., Zalaudek, I., Wolf, M.: Boosting medical diagnostics by pool-
ing independent judgments. Proceedings of the National Academy of Sciences
113(31), 8777–8782 (2016) https://doi.org/10.1073/pnas.1601827113
[32] K¨ammer,J.E.,Hautz,W.E.,Herzog,S.M.,Kunina-Habenicht,O.,Kurvers,R.H.:
The potential of collective intelligence in emergency medicine: Pooling medical
students’ independent decisions improves diagnostic performance. Medical Deci-
sion Making 37(6), 715–724 (2017) https://doi.org/10.1177/0272989X17696998
[33] Kurvers, R.H., De Zoete, A., Bachman, S.L., Algra, P.R., Ostelo, R.: Combin-
ing independent decisions increases diagnostic accuracy of reading lumbosacral
radiographs and magnetic resonance imaging. PloS One 13(4), 0194128 (2018)
https://doi.org/10.1371/journal.pone.0194128
[34] Blanchard, M.D., Herzog, S.M., K¨ammer, J.E., Z¨oller, N., Kostopoulou, O.,
Kurvers, R.H.J.M.: Collective intelligence increases diagnostic accuracy in a gen-
eral practice setting. Medical Decision Making 44(4), 451–462 (2024) https:
//doi.org/10.1177/0272989X241241001
[35] Barnett, M.L., Boddupalli, D., Nundy, S., Bates, D.W.: Comparative accu-
racy of diagnosis by collective intelligence of multiple physicians vs individual
physicians. JAMA Network Open 2(3), 190096 (2019) https://doi.org/10.1001/
jamanetworkopen.2019.0096
[36] Kurvers, R.H.J.M., Nuzzolese, A.G., Russo, A., Barabucci, G., Herzog, S.M.,
Trianni,V.:Automatinghybridcollectiveintelligenceinopen-endedmedicaldiag-
nostics. Proceedings of the National Academy of Sciences 120(34), 2221473120
(2023) https://doi.org/10.1073/pnas.2221473120
[37] Kuncheva, L.I.: Combining Pattern Classifiers: Methods and Algorithms, 2. ed
edn. Wiley. https://doi.org/10.1002/9781118914564
[38] Mienye,I.D.,Sun,Y.:Asurveyofensemblelearning:Concepts,algorithms,appli-
cations, and prospects. IEEE Access 10, 99129–99149 (2022) https://doi.org/10.
1109/ACCESS.2022.3207287
[39] Jiang, D., Ren, X., Lin, B.Y.: LLM-Blender: Ensembling large language models
with pairwise ranking and generative fusion (2023). https://arxiv.org/abs/2306.
02561
[40] Jiang, A.Q., Sablayrolles, A., Roux, A., Mensch, A., Savary, B., Bamford, C.,
38Chaplot,D.S.,Casas,D.,Hanna,E.B.,Bressand,F.,Lengyel,G.,Bour,G.,Lam-
ple, G., Lavaud, L.R., Saulnier, L., Lachaux, M.-A., Stock, P., Subramanian, S.,
Yang, S., Antoniak, S., Scao, T.L., Gervet, T., Lavril, T., Wang, T., Lacroix, T.,
Sayed,W.E.:Mixtralofexperts.arXiv(2024).https://arxiv.org/abs/2401.04088
[41] Yang, H., Li, M., Xiao, Y., Zhou, H., Zhang, R., Fang, Q.: One LLM is
not enough: Harnessing the power of ensemble learning for medical question
answering.medRxiv(2023).https://www.medrxiv.org/content/10.1101/2023.12.
21.23300380v1
[42] Barabucci, G., Shia, V., Chu, E., Harack, B., Fu, N.: Combining insights from
multiplelargelanguagemodelsimprovesdiagnosticaccuracy.arXiv(2024).https:
//arxiv.org/abs/2402.08806
[43] Dogan, A., Birant, D.: A weighted majority voting ensemble approach for
classification. In: 2019 4th International Conference on Computer Science and
Engineering (UBMK). IEEE, Samsun, Turkey (2019). https://doi.org/10.1109/
ubmk.2019.8907028
[44] Donnelly, K.: SNOMED-CT: The advanced terminology and coding system for
eHealth. Studies in Health Technology and Informatics 121, 279–290 (2006)
[45] Voorhees, E.M.: The TREC-8 question answering track report. In: Voorhees,
E.M., Harman, D.K. (eds.) Proceedings of The Eighth Text REtrieval Confer-
ence, TREC 1999. NIST Special Publication, vol. 500-246, pp. 77–82. National
Institute of Standards and Technology (NIST), Gaithersburg, MA (1999).
https://trec.nist.gov/pubs/trec8/t8 proceedings.html
[46] Ladha, K.K.: The Condorcet jury theorem, free speech, and correlated votes.
American Journal of Political Science 36(3), 617–634 (1992) https://doi.org/10.
2307/2111584
[47] Grofman, B., Owen, G., Feld, S.L.: Thirteen theorems in search of the truth.
TheoryandDecision15(3),261–278(1983)https://doi.org/10.1007/BF00125672
[48] Tumer, K., Ghosh, J.: Error correlation and error reduction in ensemble
classifiers. Connection Science 8(3-4), 385–404 (1996) https://doi.org/10.1080/
095400996116839
[49] Marshall, J.A., Kurvers, R.H., Krause, J., Wolf, M.: Quorums enable optimal
pooling of independent judgements in biological systems. eLife 8, 40368 (2019)
https://doi.org/10.7554/eLife.40368
[50] Steyvers, M., Tejeda, H., Kerrigan, G., Smyth, P.: Bayesian modeling of human–
AI complementarity. Proceedings of the National Academy of Sciences 119(11),
2111547119 (2022) https://doi.org/10.1073/pnas.211154711
39[51] Pescetelli, N.: A brief taxonomy of hybrid intelligence. Forecasting 3(3), 633–643
(2021) https://doi.org/10.3390/forecast3030039
[52] Peeters, M.M.M., Diggelen, J., Bosch, K., Bronkhorst, A., Neerincx,
M.A., Schraagen, J.M., Raaijmakers, S.: Hybrid collective intelligence in a
human–AI society. AI & Society 36(1), 217–238 (2020) https://doi.org/10.1007/
s00146-020-01005-y
[53] Steyvers, M., Kumar, A.: Three challenges for AI-assisted decision-
making. Perspectives on Psychological Science (2023) https://doi.org/10.1177/
17456916231181102
[54] Benjamin,D.M.,Morstatter,F.,Abbas,A.E.,Abeliuk,A.,Atanasov,P.,Bennett,
S., Beger, A., Birari, S., Budescu, D.V., Catasta, M., Ferrara, E., Haravitch, L.,
Himmelstein, M., Hossain, K.T., Huang, Y., Jin, W., Joseph, R., Leskovec, J.,
Matsui, A., Mirtaheri, M., Ren, X., Satyukov, G., Sethi, R., Singh, A., Sosic,
R., Steyvers, M., Szekely, P.A., Ward, M.D., Galstyan, A.: Hybrid forecasting of
geopolitical events. AI Magazine 44(1), 112–128 (2023) https://doi.org/10.1002/
aaai.12085
[55] Peabody, J.W., Luck, J., Glassman, P., Jain, S., Hansen, J., Spell, M., Lee,
M.: Measuring the quality of physician practice by using clinical vignettes: A
prospectivevalidationstudy.AnnalsofInternalMedicine141(10),771–780(2004)
https://doi.org/10.7326/0003-4819-141-10-200411160-00008
[56] Yabroff, K.R., Reeder-Hayes, K., Zhao, J., Halpern, M.T., Lopez, A.M., Bernal-
Mizrachi, L., Collier, A.B., Neuner, J., Phillips, J., Blackstock, W., Patel, M.:
Healthinsurancecoveragedisruptionsandcancercareandoutcomes:Systematic
review of published research. JNCI: Journal of the National Cancer Institute
112(7), 671–687 (2020) https://doi.org/10.1093/jnci/djaa048
[57] Hooker, S.: Moving beyond “algorithmic bias is a data problem”. Patterns 2(4)
(2021) https://doi.org/10.1016/j.patter.2021.100241 33982031
[58] van Giffen, B., Herhausen, D., Fahse, T.: Overcoming the pitfalls and perils
of algorithms: A classification of machine learning biases and mitigation meth-
ods. Journal of Business Research 144, 93–106 (2022) https://doi.org/10.1016/j.
jbusres.2022.01.076
[59] Wachter, S., Mittelstadt, B., Russell, C.: Bias preservation in machine learning:
The legality of fairness metrics under EU non-discrimination law. West Virginia
Law Review 123, 735–790 (2021) https://doi.org/10.2139/ssrn.3792772
[60] Weidinger, L., Mellor, J., Rauh, M., Griffin, C., Uesato, J., Huang, P.-S., Cheng,
M., Glaese, M., Balle, B., Kasirzadeh, A., Kenton, Z., Brown, S., Hawkins, W.,
Stepleton, T., Biles, C., Birhane, A., Haas, J., Rimell, L., Hendricks, L.A., Isaac,
W., Legassick, S., Irving, G., Gabriel, I.: Ethical and social risks of harm from
40language models. arXiv (2021). http://arxiv.org/abs/2112.04359
[61] Liang, P., Bommasani, R., Lee, T., Tsipras, D., Soylu, D., Yasunaga, M., Zhang,
Y., Narayanan, D., Wu, Y., Kumar, A., Newman, B., Yuan, B., Yan, B., Zhang,
C., Cosgrove, C., Manning, C.D., R´e, C., Acosta-Navas, D., Hudson, D.A.,
Zelikman,E.,Durmus,E.,Ladhak,F.,Rong,F.,Ren,H.,Yao,H.,Wang,J.,San-
thanam, K., Orr, L., Zheng, L., Yuksekgonul, M., Suzgun, M., Kim, N., Guha,
N., Chatterji, N., Khattab, O., Henderson, P., Huang, Q., Chi, R., Xie, S.M.,
Santurkar, S., Ganguli, S., Hashimoto, T., Icard, T., Zhang, T., Chaudhary, V.,
Wang,W.,Li,X.,Mai,Y.,Zhang,Y.,Koreeda,Y.:Holisticevaluationoflanguage
models. arXiv (2023). https://arxiv.org/abs/2211.09110
[62] Paulus, J.K., Kent, D.M.: Predictably unequal: Understanding and addressing
concerns that algorithmic clinical prediction may increase health disparities. npj
Digital Medicine 3(1), 99 (2020) https://doi.org/10.1038/s41746-020-0304-9
[63] Groh, M., Badri, O., Daneshjou, R., Koochek, A., Harris, C., Soenksen, L.R.,
Doraiswamy,P.M.,Picard,R.:Deeplearning-aideddecisionsupportfordiagnosis
of skin disease across skin tones. Nature Medicine 30(2), 573–583 (2024) https:
//doi.org/10.1038/s41591-023-02728-3
[64] Birhane, A., Isaac, W., Prabhakaran, V., Diaz, M., Elish, M.C., Gabriel, I.,
Mohamed,S.:Powertothepeople?Opportunitiesandchallengesforparticipatory
AI. In: Proceedings of the 2nd ACM Conference on Equity and Access in Algo-
rithms,Mechanisms,andOptimization.EAAMO’22.AssociationforComputing
Machinery, New York, NY (2022). https://doi.org/10.1145/3551624.3555290
[65] Carusi, A., Winter, P.D., Armstrong, I., Ciravegna, F., Kiely, D.G., Lawrie, A.,
Lu, H., Sabroe, I., Swift, A.: Medical artificial intelligence is as much social as
it is technological. Nature Machine Intelligence 5(2), 98–100 https://doi.org/10.
1038/s42256-022-00603-3
[66] Delgado,F.,Yang,S.,Madaio,M.,Yang,Q.:TheparticipatoryturninAIdesign:
Theoretical foundations and the current state of practice. In: Proceedings of the
3rd ACM Conference on Equity and Access in Algorithms, Mechanisms, and
Optimization. EAAMO ’23. Association for Computing Machinery, New York,
NY (2023). https://doi.org/10.1145/3617694.3623261
[67] Wiens, J., Saria, S., Sendak, M., Ghassemi, M., Liu, V.X., Doshi-Velez, F.,
Jung, K., Heller, K., Kale, D., Saeed, M., Ossorio, P.N., Thadaney-Israni, S.,
Goldenberg, A.: Do no harm: A roadmap for responsible machine learning for
health care. Nature Medicine 25(9), 1337–1340 (2019) https://doi.org/10.1038/
s41591-019-0548-6
[68] Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T.L., Cao, Y., Narasimhan, K.:
Tree of thoughts: Deliberate problem solving with large language models. arXiv
(2023). https://arxiv.org/abs/2305.10601
41[69] Long, J.: Large language model guided tree-of-thought. arXiv (2023). https://
arxiv.org/abs/2305.08291
[70] Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang, S., Chowdhery,
A., Zhou, D.: Self-consistency improves chain of thought reasoning in language
models. arXiv (2022). https://arxiv.org/abs/2203.11171
[71] Nori, H., Lee, Y.T., Zhang, S., Carignan, D., Edgar, R., Fusi, N., King, N.,
Larson, J., Li, Y., Liu, W., Luo, R., McKinney, S.M., Ness, R.O., Poon, H.,
Qin, T., Usuyama, N., White, C., Horvitz, E.: Can generalist foundation mod-
els outcompete special-purpose tuning? Case study in medicine. arXiv (2023).
https://arxiv.org/abs/2311.16452
[72] Bhatia, S.: Inductive reasoning in minds and machines. Psychological Review
(2023) https://doi.org/10.1037/rev0000446
[73] Zhang, Z., Zhang, A., Li, M., Smola, A.: Automatic chain of thought prompting
in large language models. arXiv (2022). https://arxiv.org/abs/2210.03493
[74] Fu,Y.,Peng,H.,Sabharwal,A.,Clark,P.,Khot,T.:Complexity-basedprompting
for multi-step reasoning. arXiv (2022). https://arxiv.org/abs/2210.00720
[75] Salewski,L.,Alaniz,S.,Rio-Torto,I.,Schulz,E.,Akata,Z.:In-contextimperson-
ation reveals large language models’ strengths and biases. In: Oh, A., Naumann,
T., Globerson, A., Saenko, K., Hardt, M., Levine, S. (eds.) Advances in Neu-
ral Information Processing Systems (NeurIPS 2023), vol. 36, pp. 72044–72057.
Curran Associates, Inc., New Orleans, LA (2023)
[76] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Nee-
lakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A.,
Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D.M., Wu, J., Win-
ter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark,
J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., Amodei, D.: Language
models are few-shot learners. arXiv (2020). https://arxiv.org/abs/2005.14165
[77] Gu, Y., Tinn, R., Cheng, H., Lucas, M., Usuyama, N., Liu, X., Naumann, T.,
Gao, J., Poon, H.: Domain-specific language model pretraining for biomedical
natural language processing. arXiv (2020). https://arxiv.org/abs/2007.15779
[78] Deka,P.,Jurek-Loughrey,A.,Deepak,P.:Improvedmethodstoaidunsupervised
evidence-based fact checking for online health news. Journal of Data Intelligence
3(4), 474–504 (2022) https://doi.org/10.26421/JDI3.4-5
[79] Holcombe, A.O., Kovacs, M., Aust, F., Aczel, B.: Documenting contributions
to scholarly articles using credit and tenzing. PLOS ONE 15(12), 1–11 (2021)
https://doi.org/10.1371/journal.pone.0244611
42