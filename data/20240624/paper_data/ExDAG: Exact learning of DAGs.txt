ExDAG: Exact learning of DAGs
Pavel Rytíř∗, Aleš Wodecki∗, Jakub Mareček
Faculty of Electrical Engineering
Czech Technical University in Prague
June 24, 2024
Abstract
There has been a growing interest in causal learning in recent years. Commonly used
representations of causal structures, including Bayesian networks and structural equation
models (SEM), take the form of directed acyclic graphs (DAGs). We provide a novel mixed-
integerquadraticprogrammingformulationandassociatedalgorithmthatidentifiesDAGson
up to 50 vertices, where these are identifiable. We call this method ExDAG, which stands
forExactlearningofDAGs. Althoughthereisasuperexponentialnumberofconstraintsthat
prevent the formation of cycles, the algorithm adds constraints violated by solutions found,
rather than imposing all constraints in each continuous-valued relaxation. Our empirical
resultsshowthat ExDAGoutperformslocal state-of-the-art solvers intermsof precision and
outperformsstate-of-the-artglobalsolverswithrespecttoscaling,whenconsideringGaussian
noise. Wealso providevalidation with respect to other noise distributions.
1 Introduction
Learning of causal representations and causal inference has received significant attention recently
(Peters et al., 2017; Schölkopf et al., 2021; Ahuja et al., 2023, e.g.). With the aim of tackling a
variety of challenges in a variety of application contexts, a wide range of models and method-
ologies (Pearl, 2009; Park et al., 2023; Buchholz et al., 2024; Lorch et al., 2024; Yu et al., 2021;
Zhang et al., 2024; Chen et al., 2021, e.g.) have been introduced. In the machine-learning liter-
ature, probabilistic graphical models (Koller and Friedman, 2009), in general, and Bayesian net-
works, in particular, are often used as a causal model. In statistics and biomedical applications,
structural equation models (Yuan and Bentler, 2006; Duncan, 2014) are used widely.
Learning of directed acyclic graphs (DAGs), in which the vertices correspond to the ran-
dom variables and the oriented edges represent their dependencies, underlies the learning of both
Bayesian networks and structural equation models, where algebraic manipulations can be inter-
preted as interventions on the causal system (Bottou et al., 2013). The identification of such a
structure is usually mediated by a score function, whose minimization identifies a class of graphs.
Alternatively, one may employ selective model averaging (Madigan and Raftery, 1994).
Inthe presentarticle,wefocus onthelearningofaDAGusingapolynomialscorefunctionun-
der the assumption of identifiability, which is given by persistent excitation (Willems et al., 2005,
Section 2),or equivalently, rankof the Henkelmatrix (Willems et al., 2005, Theorem1). Depend-
ing onthe constructionof the scorefunction (Heckerman, 2022), one maymaximize likelihoodfor
Gaussian and non-Gaussian noise. Our main contribution is not restricted to a particular noise
distribution, though.
We note that various extensions of these models abound. Notably the NeurIPS 2023 competi-
tionorganizedbythegCastleteam(Zhang et al.,2021)focussedoncausalstructurelearningfrom
∗Theseauthors contributed equallytothiswork.
1
4202
nuJ
12
]GL.sc[
1v92251.6042:viXraevent sequences with prior knowledge. Without prior knowledge, we improve upon the results of
NOTEARS on the only public dataset. A direct comparison with the winners (Li et al., 2023;
Yizhou et al., 2023; Xie and Jin, 2023) is non-trivial, as the other datasets remain private.
1.1 Main Contributions
Our contributions within the learning of a DAG, such as in the learning of a (dynamic) Bayesian
network, comprise the following.
• We consider the identification of static and dynamic Directed Acyclic Graphs (DAGs) that
leads to a mixed-integer linear program or mixed-integer quadratic problem. The mixed-
integer quadratic problem is the maximum likelihood estimator.
• We propose ExDAG, a branch-and-bound-and-cut algorithm for solving the formulation,
whichavoidstheuseofexponentiallymanyconstraintsattherootnodeandavoidsexponential-
time preprocessing steps.
• We perform a head-to-head comparison of ExDAG with two state-of-the-art solvers (NO-
TEARS and GOBNILP) and show, inter alia, that reliable reconstruction of general DAGs
to optimality on up to 50 nodes is possible on an ordinary laptop.
2 Problem Formulation and Related Work
Inordertocasttheproblemofscore-basedBayesiannetworklearningasamixed-integerprogram,
wedescribetheproblemasastructuralvectorautoregressivemodelidentification(Hoover and Demiralp,
2003; Kilian, 2011). To capture the data generation process of a set of identically distributed
stochastic processesona discrete finite setof time stamps {1,2,...,T}, consider d∈N stochastic
processes denoted X , where i ∈ {1,2,...,d} and t ∈ {1,2,...,T}. In learning Bayesian net-
i,t
works,we searchfor an adjacency matrix of a DAG (V,E),V =[d],E ⊆V ×V that describes the
dependence of these random variables. In learning dynamic Bayesian networks, we seek a set of
adjacency matrices, as follows. Let the auto-regressiveorder be denoted by p∈N, then
W ∈Rd,d, A ∈Rd,d, i∈{1,2,...,p}, (1)
i
denotetheadjacencymatricesoftheacyclicgraphofthenetworkattimetandtheauto-regressive
dependencies for the (t−i)-th level backwards in time, respectively. For simplicity, it is assumed
that these matrices remain constant over time. Denoting the data matrix at time t by X ∈Rn,d,
t
the following relationship holds:
X
t
=X tW +X t−1A 1+X t−2A 2+...+X t−pA p+Z, (2)
where Z ∈ Rn,d is the error vector, which is not assumed to be Gaussian. Although we limit
ourselvesto studying linear auto-regressivedependencies, a similarrelationshipcanbe formalized
for the case of non-linear auto-regression. By defining the matrices
A=A 1|A 2 ...|A p , Y t =X t−1|X t−2 ...|X t−p , (3)
the problem can be rewritten as
X =X W +Y A+Z. (4)
t t t
Now a costfunction, whichis the maximumlikelyhoodestimator for sufficiently smallregular-
ization is defined as
J(W,A)=kX−XW −YAk2 +λkWk+ηkAk, (5)
F
2where k·k denotes an arbitrary matrix norm and λ,η > 0 are sufficientely small regularization
coefficients. The problem of score based DAG learning can then be cast as
minJ(W,A),
W,A (6)
G(W)∈Γ ,
DAG
whereAneednotbeconstrained,sincecyclesareexcludedbyconstructionk·kdenotesanarbitrary
norm, which is usually chosen to be the l1-norm and k·k denotes the Frobenius norm. The cost
F
function in (6) becomes
J(W)=kX−XWk2 +λkWk. (7)
F
In the case ofstatic Bayesiannetworklearning,we setthe auto-regressivedepth to p=0, thereby
removing the need to find A or use data from past time stamps.
Underavarietyofidentifiabilityassumptions,ithasbeenshownthatthesolutionof(6)recovers
the DAG with high probability in the static case of Gaussian (van de Geer and Bühlmann, 2012;
Aragam et al., 2017) and non-Gaussian noise vectors (Shimizu et al., 2006; Loh and Bühlmann,
2013) in the case of p = 0. For Gaussian noise with zero mean, the identification for p ∈ N
follows from(Peters and Bühlmann, 2012). For non-Gaussiannoise with p∈N, the identifiability
follows from Marcinkiewicz’s theorem (Kagan et al., 1973) and independent component analysis
(Hyvärinen et al.,2010; Lanne et al., 2016). We referto Willems et al.(2005); Ahuja et al.(2023)
for further discussion of identifiability.
2.1 DAG Identification Using No Tears
One of the most successful methods used to solve (6) in the case of static Bayesian networks was
proposed by Zheng et al. (2018). In their formulation, a smooth function h:Rd,d →R such that
h(W)=0⇐⇒G(W)∈Γ (8)
DAG
is found and then used to arrive at a problem of the form
min F(W),
W∈Rd,d (9)
h(W)=0,
whose critical points can be found using a quasi-Newton method. Impressive numerical perfor-
mance is reported in many test instances despite the non-convex nature of the problem. This
method has been extended to dynamic Bayesian networks (Pamfil et al., 2020), recently. Just
as the static No Tears method, the dynamic variant exhibits outstanding numerical performance
(Pamfil et al., 2020) across both benchmark and real-worldinstances.
Nevertheless, the solutions found represent only first-order critical points. As the example in
Section4.2shows,the criticalpoints canbe arbitrarilyfar awayfromthe globalminimumevenin
the case of a two-vertex (d = 2) graph. As documented in Section 4.2, this suboptimality is also
observed when identifying larger instances.
3 Problem Formulation
Let us present an intentionally simplistic formulation for the identification of static and dynamic
DAGs. The construction is such that the acyclic constraints can be added using a callback at
runtime, which is key when scaling to larger instances. Another key feature is that this callback
makes use of a simple separation routine that has only quadratic complexity in the number of
graph vertices in the worst case.
Suppose that the description of the directed weighted graph is given by the following set of
variables
e ∈{0,1} is decision variable that is 1 if and only if there exists an edge from vertex i to j(i6=j)
i,j
w ∈R is the decision variable that represents the weight of edge e .
i,j i,j
3The objective function utilizes a penalized l norm:
p
n d p
J = X − X w +λ e, (10)
p (cid:12) i,j i,k k,j(cid:12)
Xi=1Xj=1(cid:12)
(cid:12)
Xk (cid:12)
(cid:12)
eX∈E
(cid:12) (cid:12)
which avoids the use of a bilinear term(cid:12) of (7), while bein(cid:12)g equivalent as long as we utilize the
additional constraints
w ≤ce , w ≥−ce for all k,j ∈{1,2,...,d}, (11)
k,j k,j k,j k,j
where c > 0 denotes a constant that corresponds to the biggest weight magnitude allowed. The
regularization constant λ > 0 in (10) is discussed in Section 4.2. The exponent p ∈ N is p = 1,2.
Althoughtheobjective(10)givesrisetoamixed-integerlinearformulationwhenp=1,theglobal
minimum would no longer representthe maximum likelihood estimator. (Cf. Section 2.) For this
reason, we set p=2 and deal with a mixed integer quadratic problem in the following sections.
Finally, let C be the set of cycles of a graph on d vertices, where each cycle c∈C of length k
is described by a set of edges, i.e., c={(i 1,i 2),(i 2,i 3),...,(i k−1,i 1)}. A constraintexcluding one
cycle c∈C from a solution in terms of e reads
e ≤k−1. (12)
i,j
(iX,j)∈c
A key challenge is the number of cycles, and thus the number of constraints (12).
3.1 The Branch-and-Bound-and-Cut Algorithm
A key contribution of ours is a branch-and-bound-and-cut algorithm for solving the formulation
above. We utilize the usual branch-and-bound algorithm (Achterberg, 2007, e.g.), but implement
cycle exclusion (12) using so-called “lazy” constraints. Lazy constraints are only checked when an
integer-feasible solution candidate has been identified. When a lazy constraint is violated, it is
included across all nodes of the branch-and-bound tree. In summary, at the root node, we utilize
only O(|E|) constraints (11). Subsequently, one introduces cycle-exclusion constraints (12), but
our numerical results (starting with Table 1) suggest that the more samples there are, the fewer
cycle-exclusion constraints one seems to need.
Notice that once a new mixed-integer feasible solution candidate is found, it is easy to detect
cycles therein using depth-first search (DFS). If a cycle is found, we add the corresponding lazy
constraint(12). The DFS algorithmhas a worst-casequadratic runtime in the number of vertices
of the graph, in constrast to algorithms separating related inequalities from a continuous-valued
relaxation(Borndörfer et al., 2020; Cook et al., 2011), such as the quadratic programin our case.
In particular, we have tried three variants of the addition of lazy constraints:
1. Adding only the lazy constraint for the first cycle found.
2. Adding only the lazy constraint for the shortest cycle found.
3. Adding multiple lazy constrains for all cycles found in the current integer-feasible solution
candidate.
We use Variant 3 throughout our numerical experiments, despite going contrary to the received
wisdom (Achterberg, 2007, Chapter 8.9) suggesting that one needs to add only a subset of cuts
and utilize a carefully crafted selection criterion to identify “good” cuts.
4 Numerical Results
Inthissection,wesummarizeourresultsandcompareouralgorithmicsolutiontotwoalternatives.
Inparticular,weprovidebenchmarkexperimentsthatsettheperformanceoftheproposedmethod
in the context of two state-of-the-art solvers, NOTEARS and GOBNILP.
4Thefirstofthese benchmarkexperimentsprovidesacomparisonwith alocal-searchalgorithm
calledNOTEARS(Zheng et al.,2018). Thissolutionmethod, basedontheuseofaquasi-Newton
method (BFGS) to solve the non-convex problem, is significant for its excellent results in prac-
tice and the ease of implementation. The second method, known for its global convergence, is
GOBNILP (Cussens, 2012), which is based on integer programming.
4.1 Setup Common to all the Benchmarking Experiments and Compar-
ison Metrics
We have implemented the branch-and-bound-and-cut algorithm utilizing Gurobi Optimizer 11,
which has been configured to use the simplex algorithm and to expect lazy constraints (lazy-
Constraints = 1). These parameter settings are important for three reasons. The simplex al-
gorithm produces corner points of the polyhedra given by (11) and any of the lazy constraints.
Corner points of the continuous-valued relaxation can be cut off by the constraints (12), in con-
trast to points in the interior of the optimal face, which would be obtained by a barrier solver
(Gondzio,2012). Second,whenGurobiexpects lazyconstraints,itavoidspruningthe branch-and-
bound-and-cut tree prematurely, which would have impacted the global convergence properties
(Sahinidis and Grossmann, 1991) otherwise. Third, lazy constraints are added directly to the LP
relaxation, without going through the cut filtering process (Achterberg, 2007, Chapter 8.9). The
Python source code is provided in the Supplementary Material and will be open-sourced upon
acceptance. In the following, we refer to the implementation as ExDAG.
In each of the benchmark experiments, we consider an initial graph,representedby a weighed
adjacencymatrixWinit,whichistobelearnt. Next,inputsaregeneratedfromWinit andGaussian
noise as in Zheng et al. (2018). Next, the inputs are used to estimate the structure of the DAG
using the relevant method (ExDAG, NOTEARS, DYNO-TEARS, GOBNILP), where we denote
the adjacency matrices generated by a method · by W·. The structure of the output adjacency
matrix often captures spurious relationships, which can result in an edge with a negligible weight
in the solution W· (Zhou, 2009; Wang et al., 2016). This effect is negated by setting a near-zero
threshold parameter δ >0, using which we eliminate the edges of weight smaller than δ from W·.
Inthe following,we denote the thresholdedadjacency matrixas Wδ. In experimentsin whichthe
·
true DAG is known, we select the optimal parameter δ > 0. For any two adjacency matrices V
and W one defines the symmetric Hamming distance (SHD)
d
ρ(V,W)= r (V,W), (13)
ij
iX,j=1
where
0 if V 6=0 and W 6=0 or V =0 and W =0
ij ij ij ij
r (V,W)=1 if V 6=0 and W 6=0 (14)
ij 2 ij ji
1 otherwise.
SHD is used as a score
describing
the similarity of the two DAGs in terms of edge placement and
iscommonlyusedtoassesthequalityofsolutions(Zheng et al.,2018;Cussens,2012;Pamfil et al.,
2020). The solutions will also be evaluated by comparing values of the cost function:
σ (V,W)=|J (V)−J (W)|, (15)
p p p
where J is defined by (10). In the comparison (15), we typically set p = 2, as in the maximum
p
likelihood estimator. (Cf. Section 2). The last of the metrics used to evaluate the quality of the
DAG estimation is the Frobenius norm, which we denote by k·k .
F
Weusetwowell-knownensemblesofrandomgraphs: theErdős–Rényimodel(ER)ofErdős et al.
(1960) and the scale free network model (SF) of Barabási and Albert (1999). We also use two
lesser knownensembles SF3 and ER2 utilized by Zheng et al.(2018) to match the experiments of
Zheng et al.(2018). Allthe experimentsinthefollowingsectionareperformedonaMacbookPro
M2, equipped with Apple M2 Pro, 16 GB of RAM, and Ventura 13.2.1.
54.2 Comparison with NOTEARS
As mentioned previously, NOTEARS is a state-of-the-art local solver for learning Bayesian net-
works. Let us demonstrate the local-searchnature. Consider a graphontwo nodes corresponding
to two random variables X and X . Let ǫ>0 and suppose the dataset contains only two entries
1 2
X = 1 1+ǫ . (16)
(cid:0) (cid:1)
The only two possible DAGs on two nodes are represented by the matrices
0 α 0 0
W = ,W = , (17)
α (cid:18) 0 0 (cid:19) β (cid:18) β 0 (cid:19)
where α,β >0. Assume now that we attempted to find stationary points of the cost functions
J (ǫ)=kX(ǫ)−X(ǫ)W k2,J (ǫ)=kX(ǫ)−X(ǫ)W k2, (18)
α α β β
whereα6=β sinceαandβ representdifferentinitialestimates. Computingthederivativesofeach
of the cost functions gives:
J′ (ǫ)=−2(1+ǫ)+2α(1+ǫ)2,J′ (ǫ)=2β−2(1+ǫ), (19)
α β
which results in the stationary points
1
α =1+ǫ,β = , (20)
0 0 1+ǫ
with the associated costs
J (ǫ)=1,J (ǫ)=1+ǫ. (21)
α0 β0
Asthedifferencebetweenthetwodatapointsincreases,thedifferencebetweenthelocalandglobal
minimum increases, i.e.,
lim (J (ǫ)−J (ǫ))=+∞. (22)
ǫ→+∞
α0 β0
Inthefollowingexperiments,additionalevidencefortheextentofthesuboptimalityofNOTEARS
is provided, as NOTEARS and ExDAG are benchmarked side by side. We restrict ourselves to
Gaussian noise initially; the behaviour of ExDAG with respect to different noise distributions is
explored in Section 4.4.
In(Zheng et al.,2018), the NOTEARS solverhasbeen benchmarkedwith respectto accuracy
onthe SFandERensembleson20nodes (d=20). The authorsbenchmarktheir methodwithre-
specttoFrobeniousnormfromtheoriginalgraph. ThereportedrangeiskWinit−WNOTEARSk
F
∈
(0.3,5.1). RunninganexperimentthatpartlymirrorstheonereportedbyZheng et al.(2018),our
aim is to study this quantity along with the SHD for d=16,20.
In Table 1, one can observe a few key features of the numerical solutions given by ExDAG
in comparison with NOTEARS. Focusing on ExDAGs performance first, one can observe that a
closetoperfectreconstruction(SHD=0andnegligiblekWinit−WNOTEARSk
F
≪1)ofthegraphis
providedaslongasthenumberofsamplessuffices. Noticealsothatthenumberoflazyconstraints
actuallyimposedismuchsmallerthanthenumberofpotentialcyclesinthegraphanddecreasesas
the number of samples increases. Note that the time limit was set to 1800 seconds, but whenever
there were a sufficient number of samples, the solution indicated was found within a minute for
eachof the cases. Table 1 thus suggests that at a cost of slightly largercomputational time (∼20
seconds for NOTEARS vs. ∼60 seconds for ExDAG), one arrives at solutions with no erroneous
edgesandanormevaluation(kWinit−WNOTEARSk F)closetozero,whereasinthecaseofthelocal
solutions provided by NOTEARS one typically has several falsly discovered edges and a norm of
one order higher. One would expect this favorablebehavior to get disrupted, when one scales the
larger instances, for this reason, this is investigated next.
Next, an experiment that compares scaling for larger instances d = 30,40,50 using Gaussian
noise is provided. Table 2 provides the precision figures and computation time for each of the
6Table 1: Reconstruction of DAGs on d = 16,20 samples from n = 20,103 samples from graph
ensembles of Zheng et al. (2018) using NOTEARS and ExDAG. In the two cases, where ExDAG
finisedcomputationwithinthe30-minutetimelimit,thesolutionwasproventobegloballyoptimal.
The best solution was known at the time indicated. The time in brackets documents the entire
durationoftheexperiment,inwhichonlythedualboundhasbeenupdatedsincethetimeoutside
of the brackets. λ∈(0.01,0.5)was used for the experiments involving ExDAG.
Solver Runtime Model n, d Best kWinit−WEx-BAYk
F
Constraints
[s] SHD added
ExDAG 1800 SF3 20, 16 17 6.1076 11834
ExDAG 73 (396) SF3 1000,16 0 0.2932 5652
ExDAG 1800 ER2 20, 16 30 7.6761 17547
ExDAG 73 (1800) ER2 1000,16 0 0.2371 13941
ExDAG 1800 SF3 20, 20 50 21.2620 36697
ExDAG 67 (1800) SF3 1000,20 0 0.3072 12700
ExDAG 1800 ER2 20, 20 37 13.1017 32474
ExDAG 15 ER2 1000,20 0 0.2257 6251
NOTEARS 22 SF3 20, 16 23 6.8307 N/A
NOTEARS 14 SF3 1000,16 5 1.418 N/A
NOTEARS 20 ER2 20, 16 23 7.5857 N/A
NOTEARS 5 ER2 1000,16 5 2.0905 N/A
NOTEARS 20 SF3 20, 20 52 14.38 N/A
NOTEARS 14 SF3 1000,20 0 0.39 N/A
NOTEARS 24 ER2 20, 20 35 10.3 N/A
NOTEARS 21 ER2 1000,20 7 1.82 N/A
instances using NOTEARS and ExDAG respectively. Due to memory restrictions (16 GB), the
sample quantity needs to be scaled down for d = 50 in order to perform the computation. Near
perfect reconstruction is observed in the case of ExDAG, whereas NOTEARS does not deliver
a result close to the global minimum. Each experiment was run 10 times with different seeds
and Table 2 provides the averages and maxima. The local search is heavily affected by the seeds,
whereas ExDAG failed only once at 50 nodes (SHD = 3), at the cost of additional computation
time. This single failure can likely be remedied by adding further samples, reducing the number
of lazy constraints needed.
Notice the impact of the implementation of cycle exclusion (12) via lazy constraints. While
the count of directed cycles in a graph of 50 nodes is at least of the order 1016, only 5 · 104
constraintswere utilized, showingthat not enumerating constraintsis crucialfor the performance
of the mixed-integer quadratic program, both in terms of runtime and memory requirements.
4.3 Comparison with GOBNILP
GOBNILP uses an integer programming formulation to find global minima for the problem of
learning DAGs utilizing a variety of scoring functions (Cussens, 2012, 2023). These functions are
realized by precomputed local scores, resulting in an additive structure of the scoring function.
Significantly, this is not possible for the maximumlikelihood estimator givenby J of (10). Thus,
2
the solutions provided by GOBNILP do not maximize likelihood in the reconstruction of the
DAGs. Wedocumentthisbypresentingasimpleexampleonthreenodes. Supposethattheinitial
adjacency matrix of the DAG to be identified reads
0 0 1.0
Winit = 0 0 0 . (23)
0 1.0 0
 
7Table 2: Finding a DAG with d=30,40,50and differentsample counts andGaussiannoiseusing
NOTEARS and ExDAG. For the solutions givenby ExDAG an interrupt timer of 30/60minutes,
thenodecountsamplepairings 30,104 , 40,104 , 50,5·103 ,λ=0.1ford=30,40andλ=0.3
for d=50 were used. (cid:0) (cid:1) (cid:0) (cid:1) (cid:0) (cid:1)
Solver Solution Generation Avg Max Avg. maximum Constraints
time (s) method, d Best Best Winit−W Eδ
x-BAY
added
SHD SHD (cid:13) (cid:13)
(cid:13) (cid:13)
ExDAG 2100 SF3, 30 0 0 0.0918,0.1128 <6·104
ExDAG 3960 SF3, 40 0 0 0.0987,0.1094 < 105
ExDAG 3960 SF3, 50 0.3 3 0.3567,1.322 < 5·104
NOTEARS 102 SF3, 30 4.3 19 1.6687,5.8353 N/A
NOTEARS 126 SF3, 40 6.6 24 1.7725,5.4515 N/A
NOTEARS 150 SF3, 50 5.3 15 1.8392,4.2440 N/A
UsingGOBNILPinanyoftheregimesGaussianLL,BGe,GaussianL0,GaussianBIC,GaussianAIC
(Cussens, 2012) one arrives at the structure
0 0 0
WGOB = 0 0 0 . (24)
1 1 0
 
One the other hand, using the ExDAG solver, we arrive at
0 0 c
WExDAG = 0 0 0 , (25)
0 b 0
 
where b → 1.0 and c → 1.0 as the number of samples increases, which means that Wδ
ExDAG
reconstructs the DAG with an arbitrary precision for a suitable δ >0.
Besides the aforementioned discrepancy, GOBNILP uses an exponential preprocessing step
to setup the local score functions, which is commonly reduced by using an artificial predecessor
constraint limiting the number of predecessors to a low number per node. We note that ExDAG
avoids such an exponential preprocessing step (cf. Section 3.1) and the number of callbacks does
not scale exponentially with the number of vertices in numerical experiments, as we have seen in
Section 4.2.
InTable3,GOBNILPandExDAGarecomparedhead-to-head,whereinbothcasestheGurobi
Optimizer 11 is used. The time limit parameter in Gurobi was set to 60 seconds for ExDAG and
to 3600 seconds for GOBNILP, but the overall time taken may be higher, due to the additional
post-processing required by Gurobi.
Since GOBNILP does not support negative weights, a comparison with respect to the weight
matrixisnotmeaningfulandweutilizethesymmetricHammingdistanceinstead. ExDAGoutper-
forms GOBNILPwith respectto both computationtime andSHD. This is inpartdue to the cost
function of ExDAG (10) being provably the maximum likelihood minimizer (see Section 2), while
GOBNILP optimizes with respect to a score function that does not posses such guarantees; even
attaining the global minimum on a given instance might not result in a faithful reconstruction
of the original DAG. In terms of SHD of the final solutions, ExDAG delivered the perfect recon-
struction of the edges of the DAG for instances up to 24 nodes, while GOBNILP produces lower
quality reconstructions of DAGs on up to 11 vertices and timed out for the larger instances. We
thusconcludethatforthe purposeofreconstructinggeneralDAGsunderGaussiannoise,ExDAG
outperforms GOBNILP.
8Table 3: A comparison between the runtimes and result quality of ExDAG and GOBNILP. The
best SHD distance is described in Section 4.1, d, n denote the number vertices and samples,
respectively and λ = 0.01. The dataset used for this example was SF3 with Gaussian noise
applied.
d Runtime for sample size 104 [s] Best SHD
ExDAG GOBNILP GOBNILP ExDAG
time limit 3600 seconds
7 5.6 4.7 4 0
8 7.4 7.3 1 0
9 9.5 42.6 4 0
10 11.9 204 38 0
11 15.1 3204 34 0
12 19.4 N/A N/A 0
14 30 N/A N/A 0
16 90 N/A N/A 0
18 108 N/A N/A 0
20 120 N/A N/A 0
22 150 N/A N/A 0
24 186 N/A N/A 0
Table 4: Finding a global solution on a DAG with d = 14 and n = 104 with different noise
distributions and graph generating methods, with λ=0.05.
ExDAG runtime Noise Generation Best SHD kWin−WEx-BAYkF
[s] distribution method
35.8 Gaussian SF3 0 0.0728
31.7 Gaussian ER2 0 0.0586
40.1 Uniform SF3 0 0.12865
27.7 Uniform ER2 0 0.06645
534 Exponential SF3 2 1.27423
300 Exponential ER2 3 1.63507
4.4 Finding DAGs using ExDAG with Different Noise Distributions
Since the cost function (7) is guaranteed to be the maximum likelihood estimator for data per-
turbed by non-Gaussiannoise,we providesome experiments in the directionas well. We consider
atime limitof1800secondsandagraphwith d=14vertices,wearriveatthe globalminimum in
eachofthe followingexampleswithin the documentedtime limit. Table4providesanoverviewof
theexperiments,whichwereperformedforGaussian,exponential,anduniformnoisedistributions
in which a time limit of 240 seconds is applied. One can observe that the reconstruction quality
is close to perfect for the case of Gaussian and uniform noise, but lacks precision in the case of
exponential noise, in which the convergence to the global minimum was observed to be slow (the
final gap figures were comparatively large).
4.5 Identifying DAGs from Datasets with Real Interpretation using
ExDAG
To test the capabilities of ExDAG further, we use it to learn a DAG from alarm.csv, the only
publicly available dataset from a competition held at NEURIPS 20231. ExDAG obtains a best
SHD of 55 with a Gscore 0.6258 with λ = 0.5, which improves upon NOTEARS substantially,
1Cf. https://codalab.lisn.upsaclay.fr/forums/13855/2071/
9where NOTEARS identifies DAG with the best SHD scoreof65 over100different seeds,with the
best Gscore of 0.5578. Notice that the identifiability in this case is not well understood. Indeed,
depending on the spectral properties of the system, a sufficient number of samples may or may
notbesufficient(Simchowitz et al.,2018)foridentifiability. Furthermore,themaximumlikelihood
estimator is not well understood, when the data points are from {0,1} and the range of the noise
is also {0,1}.
5 Conclusion
Novel formulations for identifying static and dynamic Bayesian networks based on the structural
vector autoregressivemodel were proposed. These formulationslead to a mixed-integer quadratic
optimization problem (MIQP) problem, whose solution is the maximum likelihood estimator for
a variety of noise distributions.
Thecycle-basedformulationoftheproblemallowsustoaddviolatedcycle-exclusionconstraints
atruntime. Althoughtheseparationofcycle-basedinequalitiesfromcontinuous-valuedrelaxations
is NP-Hardin some settings (Borndörfer et al., 2020), andonly heuristics are known(Cook et al.,
2011; Vo et al., 2023) in other settings, our approach is inspired by generalized Benders decom-
position for MIQP (Geoffrion, 1972) and the generation of subtour elimination constraints from
integer solutions (Aguayo et al., 2018) for the travelling salesman problem (Cook et al., 2011),
which makes it possible to have a separationmethod with quadratic complexity in the number of
vertices of the DAG (i.e., random variables).
All of the above culminates in a robust and near-exact reconstruction of DAGs up to 50
vertices on commodity hardware (see Table 2 and Section 4.1), which surpasses local methods in
precision and surpasses global methods in scalability, as DAGs of these dimensions have never
been identified using exact methods previously. Table 2 also indicates that for d=50, where the
numberofdirectedcyclesisofthe order1016,wehadtoimpose onlyupto 5·104 lazyconstraints.
At the same time, the results show that under certain conditions, identification may still be
challenging. For instance, in the case of exponential noise (see Table 4), one does not arrive at
near-perfect reconstruction even though the result is the maximum likelihood estimator. This
gives rise to two important questions: First, when does the maximum likelihood estimator lead
to the reconstruction of the original DAG (independent of ExDAG)? The likely answer considers
identifiability andsignal-to-noiseratio. Second,how toscaleto largerinstances,whenidentifiabil-
ityandsignal-to-noiseratiodoesnotallowforafullgraphreconstruction? Furtherresearchinthe
direction of scaling should be a priority. Likewise, one would like to test on additional real-world
instances.
10References
T.Achterberg. Constraint IntegerProgramming. PhDthesis,TechnischeUniversitaetBerlin,2007.
M.M.Aguayo,S.C.Sarin,andH.D.Sherali.Solvingthesingleandmultipleasymmetrictraveling
salesmen problems by generating subtour elimination constraints from integer solutions. IISE
Transactions, 50(1):45–53,2018.
K. Ahuja, D. Mahajan, Y. Wang, and Y. Bengio. Interventional causal representation learning.
In International conference on machine learning, pages 372–407.PMLR, 2023.
B. Aragam, A. A. Amini, and Q. Zhou. Learning directed acyclic graphs with penalized neigh-
bourhood regression,2017.
A.-L. Barabási and R. Albert. Emergence of scaling in random networks. Science, 286(5439):
509–512,1999.
R. Borndörfer, H. Hoppmann, M. Karbstein, and N. Lindner. Separation of cycle inequalities in
periodic timetabling. Discrete Optimization, 35:100552,2020.
L.Bottou,J.Peters,J.Quiñonero-Candela,D.X.Charles,D.M.Chickering,E.Portugaly,D.Ray,
P. Simard, and E. Snelson. Counterfactual reasoning and learning systems: The example of
computational advertising. Journal of Machine Learning Research, 14(11), 2013.
S. Buchholz, G. Rajendran, E. Rosenfeld, B. Aragam, B. Schölkopf, and P. Ravikumar. Learning
linear causal representations from interventions under general nonlinear mixing. Advances in
Neural Information Processing Systems, 36, 2024.
R.Chen,S.Dash,andT.Gao.Integerprogrammingforcausalstructurelearninginthepresenceof
latentvariables.InM.MeilaandT.Zhang,editors,Proceedingsofthe38thInternationalConfer-
enceonMachineLearning,volume139ofProceedingsofMachineLearningResearch,pages1550–
1560. PMLR, 18–24 Jul 2021. URL https://proceedings.mlr.press/v139/chen21c.html.
W. J. Cook, D. L. Applegate, R. E. Bixby, and V. Chvátal. The traveling salesman problem: a
computational study. Princeton university press, 2011.
J. Cussens. Bayesiannetwork learning with cutting planes, 2012. arXiv:1202.3713.
J. Cussens. Branch-price-and-cut for causal discovery. In M. van der Schaar, C. Zhang, and
D. Janzing, editors, Proceedings of the Second Conference on Causal Learning and Reasoning,
volume 213 of Proceedings of Machine Learning Research, pages 642–661. PMLR, 11–14 Apr
2023. URL https://proceedings.mlr.press/v213/cussens23a.html.
O. D. Duncan. Introduction to structural equation models. Elsevier, 2014.
P. Erdős, A. Rényi, et al. On the evolution of random graphs. Publ. math. inst. hung. acad. sci,
5(1):17–60,1960.
A. M. Geoffrion. Generalized benders decomposition. Journal of optimization theory and applica-
tions, 10:237–260,1972.
J.Gondzio. Interiorpointmethods 25yearslater. European Journal of Operational Research, 218
(3):587–601,2012.
D. Heckerman. A tutorial on learning with bayesian networks, 2022.
K. Hoover and S. Demiralp. Searching for the causal structure of a vector autoregression. SSRN
Electronic Journal, 04 2003. doi: 10.2139/ssrn.388840.
A.Hyvärinen,K.Zhang,S.Shimizu,andP.Hoyer.Estimationofastructuralvectorautoregression
model using non-Gaussianity. Journal of Machine Learning Research, 11:1709–1731,07 2010.
11A. M.Kagan,C.R. Rao,andL. U.I. Characterization problems in mathematical statistics. Wiley,
1973. Translation of Kharakterizatsionnyezadachi matematichesko˘ıstatistiki.
L.Kilian. StructuralVectorAutoregressions. CEPR DiscussionPapers8515,C.E.P.R. Discussion
Papers, Aug. 2011. URL https://ideas.repec.org/p/cpr/ceprdp/8515.html.
D. Koller and N. Friedman. Probabilistic Graphical Models: Principles and Techniques. 01 2009.
ISBN 978-0-262-01319-2.
M. Lanne, M. Meitz, and P. Saikkonen. Identification and estimation of non-Gaussian structural
vector autoregressions. Journal of Econometrics, 196, 10 2016. doi: 10.1016/j.jeconom.2016.06.
002.
M. Li, S. Liu, Y. Liu, and J. Shen. Xhs. In Advances in Neural Information Processing Systems,
volume 36, 2023. URL https://neurips.cc/virtual/2023/84314.
P.-L. Loh and P. Bühlmann. High-dimensional learning of linear causal networks via inverse
covariance estimation. Journal of Machine Learning Research, 15, 11 2013.
L.Lorch,A.Krause,andB.Schölkopf.Causalmodelingwithstationarydiffusions.InInternational
Conference on Artificial Intelligence and Statistics, pages 1927–1935.PMLR, 2024.
D.MadiganandA.E.Raftery. Modelselectionandaccountingformodeluncertaintyingraphical
models using Occam’s window. Journal of the American Statistical Association, 89(428):1535–
1546, 1994. doi: 10.1080/01621459.1994.10476894.
R. Pamfil, N. Sriwattanaworachai, S. Desai, P. Pilgerstorfer, P. Beaumont, K. Georgatzis, and
B. Aragam. Dynotears: Structure learning from time-series data. In International Conference
on Artificial Intelligence and Statistics, 2020.
J. Park, S. Buchholz, B. Schölkopf, and K. Muandet. A measure-theoretic axiomatisation of
causality. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors,
Advances in Neural Information Processing Systems, volume 36, pages 28510–28540. Curran
Associates, Inc., 2023.
J. Pearl. Causality. Cambridge University Press, 2 edition, 2009.
J.PetersandP.Bühlmann. IdentifiabilityofGaussianstructuralequationmodelswithequalerror
variances. Biometrika, 101, 05 2012. doi: 10.1093/biomet/ast043.
J. Peters, D. Janzing, and B. Schölkopf. Elements of causal inference: foundations and learning
algorithms. The MIT Press, 2017.
N. Sahinidis and I. Grossmann. Convergence properties of generalized ben-
ders decomposition. Computers & Chemical Engineering, 15(7):481–491, 1991.
ISSN 0098-1354. doi: https://doi.org/10.1016/0098-1354(91)85027-R. URL
https://www.sciencedirect.com/science/article/pii/009813549185027R.
B.Schölkopf,F.Locatello,S.Bauer,N.R.Ke,N.Kalchbrenner,A.Goyal,andY.Bengio. Toward
causal representation learning. Proceedings of the IEEE, 109(5):612–634, 2021. doi: 10.1109/
JPROC.2021.3058954.
S.Shimizu, P. O.Hoyer,A.Hyvärinen,andA.J.Kerminen. Alinearnon-Gaussianacyclicmodel
for causal discovery. J. Mach. Learn. Res., 7:2003–2030,2006.
M.Simchowitz,H.Mania,S.Tu,M.I.Jordan,andB.Recht. Learningwithoutmixing: Towardsa
sharpanalysisoflinearsystemidentification. InConferenceOnLearningTheory,pages439–473.
PMLR, 2018.
12S. van de Geer and P. Bühlmann. ℓ -penalized maximum likelihood for sparse directed acyclic
0
graphs. The Annals of Statistics, 41, 05 2012. doi: 10.1214/13-AOS1085.
T. Q. T. Vo, M. Baiou, V. H. Nguyen, and P. Weng. Improving subtour elimination constraint
generation in branch-and-cut algorithms for the TSP with machine learning. In International
Conference on Learning and Intelligent Optimization, pages 537–551.Springer, 2023.
X. Wang,D. Dunson, andC. Leng. No penalty no tears: Leastsquaresinhigh-dimensionallinear
models. In M. F. Balcan and K. Q. Weinberger, editors, Proceedings of The 33rd International
ConferenceonMachineLearning,volume48ofProceedingsofMachineLearningResearch,pages
1814–1822,New York, New York, USA, 20–22 Jun 2016. PMLR.
J.C.Willems,P.Rapisarda,I.Markovsky,andB.L.DeMoor. Anoteonpersistencyofexcitation.
Systems & Control Letters, 54(4):325–329,2005.
H. Xie and K. Jin. ustc-aig. In Advances in Neural Information Processing Systems, volume 36,
2023. URL https://neurips.cc/virtual/2023/84316.
C. Yizhou, A. Zeng, G. Huzhang, and H. Liu. Fakeikun. In Advances in Neural Information
Processing Systems, volume 36, 2023. URL https://neurips.cc/virtual/2023/84315.
Y.Yu,T.Gao,N.Yin,andQ.Ji. Dagswithnocurl: Anefficientdagstructurelearningapproach.
06 2021.
K.-H. Yuan and P. M. Bentler. Structural equation modeling. Handbook of statistics, 26:297–358,
2006.
K. Zhang, S. Zhu, M. Kalander,I. Ng, J. Ye, Z. Chen, and L. Pan. gcastle: A python toolbox for
causal discovery. arXiv preprint arXiv:2111.15155, 2021.
Z. Zhang, I. Ng, D. Gong, Y. Liu, M. Gong, B. Huang, K. Zhang, A. van den Hengel,
and J. Q. Shi. Analytic DAG constraints for differentiable DAG learning, 2024. URL
https://openreview.net/forum?id=Z8RPghUs3W.
X. Zheng, B. Aragam, P. K. Ravikumar, and E. P. Xing. Dags with no tears: Continuous opti-
mization for structure learning. Advances in neural information processing systems, 31, 2018.
S.Zhou.Thresholdingproceduresforhighdimensionalvariableselectionandstatisticalestimation.
In Y. Bengio, D. Schuurmans, J. Lafferty, C. Williams, and A. Culotta, editors, Advances in
Neural Information Processing Systems, volume 22. Curran Associates, Inc., 2009.
13