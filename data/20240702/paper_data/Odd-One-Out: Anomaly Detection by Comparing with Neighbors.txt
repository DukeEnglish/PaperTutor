Odd-One-Out: Anomaly Detection by Comparing with
Neighbors
AnkanBhunia ChangjianLi HakanBilen
UniversityofEdinburgh
https://github.com/VICO-UoE/OddOneOutAD
Abstract
Thispaperintroducesanovelanomalydetection(AD)problemthatfocuseson
identifying ‘odd-looking’ objects relative to the other instances within a scene.
UnlikethetraditionalADbenchmarks,inoursetting,anomaliesinthiscontext
are scene-specific, defined by the regular instances that make up the majority.
Sinceobjectinstancesareoftenpartlyvisiblefromasingleviewpoint,oursetting
providesmultipleviewsofeachsceneasinput. Toprovideatestbedforfuture
researchinthistask,weintroducetwobenchmarks,ToysAD-8KandPartsAD-15K.
Weproposeanovelmethodthatgenerates3Dobject-centricrepresentationsfor
eachinstanceanddetectstheanomalousonesthroughacross-examinationbetween
theinstances. Werigorouslyanalyzeourmethodquantitativelyandqualitativelyin
thepresentedbenchmarks.
1 Introduction
Anomalydetection(AD)[13,34]aimstodetectpatternsthatdeviatefromexpectedbehavior. In
standardcomputervisionADbenchmarks,thenon-conforming(oranomalous)patternsinimages
areofteneitherduetohigh-levelvariationssuchasintroductionofanobjectinstancefromanunseen
category [2, 8, 12] or low-level variations in object shape and texture [6, 16] (see Fig. 1(a)). In
thesebenchmarks,thedefinitionsofnormalandanomalouspatternsaretypicallynotpredefinedbut
implicitlylearnedthroughrepresentationsand/orclassifiers[21,2,16]thatareinvarianttoappearance
changeswithinthenormaldatadistributionwhilebeingsensitivetotheanomalousones. However,in
manyreal-worldapplicationssuchasqualitycontrolinproductionlines,productspecification,hence
thedefinitionof‘normality’,isnotonlyknowninadvancebutalsospecifictoeachobjectinstance.
Forexample,whileacoffeecupwitharedhandleisconsiderednormalandonewithabluehandle
isanomalouswhenthegoalistoproducered-handledcups,buttheoppositeistrueifthegoalisto
produceblue-handledcups. Thisinstance-specificdefinitionofnormalitycannotbeaddressedby
methodsthatrelyonaglobalandimplicitdefinitionofnormality.
Inthispaper,inspiredfromreal-worldvisualinspectionscenarios,weintroduceanewADproblem,
alongwithtwonewbenchmarkswithdifferentcharacteristicsandanovelsolution. Asdepictedin
Fig.1(b),inthisproposedsetting,eachimagecontainsascenewithmultipleinstancesofthesame
object(e.g.,coffeecups)includingeitheronlynormal,oramixofnormalandanomalousinstances
where the anomalous instances constitute the minority. As defining precise visual specifications
for shape and appearance is often infeasible, we assume that normal instances, which form the
majority,provideascene-specificreferencefor‘normality’. Ourobjectiveistodetectanomalous
instances in a scene while generalizing to previously unseen scenes including novel objects and
spatialconfigurations.
Unlikeexistingbenchmarks,ourtaskoftenrequiresaholisticunderstandingofthescenebycomparing
instanceswitheachother,althoughsomeanomalies,suchascracksandmisalignedparts,canbe
identifiedbyinspectingindividualinstances. Additionally,sinceperformingADfromasingleview
Preprint.Underreview.
4202
nuJ
82
]VC.sc[
1v99002.6042:viXra(a) Standard AD (b) Multi-object AD (ours) (c) Multi-view Multi-object AD (ours)
view 1 view 2
𝒐𝟏
𝒐𝟐 𝒐𝟑 𝒐𝟏 𝒐𝟐
𝒐𝟑
scene 1: theblue scene 2: thered
normal anomaly
handle cup is anomaly handle cup is anomaly
2 redhandles (𝒐𝟐,𝒐𝟑) & 1 bluehandle (𝒐𝟏)
Figure1: ThestandardandournewADsettings.
canbeambiguousduetoself-occlusionandocclusionbetweentheinstances,weprovidemultiple
viewsofthescenetocoveritsentirerelevantextent,unliketheexistingbenchmarksthatprovidea
singleviewasinput(seeFig.1(c)). Ourgoalistodetectanomaloussamplesinpreviouslyunseen
scenesfrommultipleviews.
Theproposedproblempresentsseveralchallengesrequiringthefollowingcapabilities: i)3Dunder-
standingofthesceneandregisteringtheviewsfrommultiplecameraviewpointswithoutgroundtruth
3Dknowledgewhileidentifyingpotentialocclusions,ii)aligningandcomparingobjectinstances
witheachotherwithouttheirrelativeposeinformationinbothtrainingandevaluation,iii)learning
representationsthatgeneralizetounseenobjectinstancesduringtesting. Toaddressthesechallenges,
weproposeanovelmethodthattakesasinputmultipleviewsofthesamescene,projectsthemintoa
3Dvoxelgrid,producesa3Dobject-centricrepresentationforeachinstance,andpredictstheirlabels
bycross-correlatinginstancesthroughanefficientattentionmechanism. Weleveragerecentadvances
in differentiable rendering [31] and self-supervised learning [33] to supervise 3D representation
learning. Specifically,werenderthevoxelrepresentationsformultipleviewpointsandmatchthem
withthegiven2Dviewstoproducegeometricallyconsistentfeatures. Additionally,weencourageour
modeltolearnpart-aware3Drepresentationsbydistillingfeaturesfrom2Dself-supervisedmodel
DINOv2[33],enhancingthecorrespondencematchingacrossinstances. Finally,sincethereisno
priorbenchmarkforthistask,weproposetwonewbenchmarks–ToysAD-8K andPartsAD-15K,
includinginstancesfromcommonsemanticobjectcategoriesaswellasmechanicalpartsrespectively
toprovideatestbedforfutureresearch. Ourmethodsignificantlyoutperformsvariousbaselinesthat
donotcompareinstanceswitheachother,andwerigorouslyanalyzevariousaspectsofourmodel
andbenchmarks.
2 RelatedWork
ADbenchmarks. AkeychallengeinADresearchisthescarcityoflargedatasetscontainingrealistic
anomalies. Earlier works [11, 36] focusing on high-level semantic anomalies often use existing
classificationdatasetsbytreatingasubsetofclassesasanomaliesandtheremainderasnormal. There
alsoexistseveraldatasetscontainingreal-worldanomalyinstances. Forexample,MVTec-AD[5]
includesindustrialobjectswithvariousdefectslikescratches,dents,andcontaminations,Carreraet
al.[10]presentsvariousdefectsinnanofibrousmaterial,andVisA[59]comprisescomplexindustrial
objectssuchasPCBs,aswellassimplerobjectslikecapsulesandcashews,spanningatotalof12
categories. Thesedatasetsassumethatobjectsarepose-aligned,andbothnormalimagesandtheir
anomalycounterpartshavethesamepose. Zhouetal.[58]proposeapose-agnosticframeworkby
introducingthePADdataset,whichcomprisesimagesof20LEGObricksofanimaltoysfromdiverse
viewpoints/poses. Unliketheworksdiscussedabove,wefocusonADinmulti-objectmulti-view
sceneenvironments,whereanomaliesarepredictedbyassessingtheirmutualsimilaritywithother
objectsinthescene. Theproposedsettingenablesourframeworktoworkseamlesslyonnovelobject
instanceswithoutrequiringfurthertraining.
Few-shotAD.Therearesomeworks[18,23,52,50,7]aimingtodetectanomaliesfromasmall
number of normal samples as support images. In our setting, the concept of normality in each
imageisalsolearnedfromafewinstancesonly. However,unlikethem,theconceptofnormalityis
scene-specific,andourmethodcangeneralizetopreviouslyunseeninstanceswithoutrequiringany
modificationbylearningfromasupportset. Inaddition,oursettinginvolvesmulti-objectmulti-view
datasamplesasinput,unlikethesingle-objectsingle-viewintheirs.
Multi-view3Dvision. Multi-view3Ddetection[37,40,51,48]isarelatedproblemthataimsto
predictthelocationsandclassesofobjectsin3Dspace,givenmulti-viewimagesofascenealong
withtheircorrespondingcameraposesasinput. Mostexistingworksfirstproject2Dimagefeatures
2ontoa3Dvoxelgrid,followedbyadetectionhead[29,53,9]thatoutputsthefinal3Dbounding
boxesandclasslabels. Whileourtaskcanbenaivelysolvedbytreatingitasa3Dobjectdetection
problemwithanomalyandnormalasthetwopossibleclasses,thisapproachhaslimitedabilityto
performeffectivecomparisonswithotherinstancesinthescene,whichisrequiredforfine-grained
AD. We compare our method to a 3D object detection technique in Sec. 4. Another related area
involvestakingmulti-viewimagesasinputandtrainingafeedforwardmodelfor3Dvolume-based
reconstruction[32,55,42]andnovelviewsynthesis[45,47,54,15,24]. Similarly,inthiswork,we
focusonlearningafeedforwardmodelinamulti-objectsceneenvironmentusingsparsemulti-view
imagesbutforAD.
Leveragingfoundationmodels. Large-scalepretrainingonimagedatasetshasshownimpressive
generalizationcapabilitiesonvarioustasks[35,25,33]. Previousworks[57,3]demonstratethat
features extracted from DINOv2 [33] serve as effective dense visual descriptors with localized
semanticinformationfordensecorrespondenceestimationtask. Somerecentworks[26,46]use
featuredistillationtechniquestoleverage2Dfoundationvisionmodelsfor3Dtask. Inspiredbythese
worksweutilizeDINOv2todistillitsdensesemanticknowledgeintoour3Dnetworkthatenables
ournetworktoinferrobustlocalcorrespondences,whichaidsinfine-grainedobjectmatching.
3 Method
3.1 Overview
Considerascenecontainingmultiplerigidobjects{o }N ofthesameinstance,whereN represents
n n=1
the number of objects in the scene. Each object’s pose is arbitrary and unknown. The goal is to
identifyanomaliesinthegroupofobjectsbyassessingtheirmutualsimilarity. Anobservationinour
settingconsistsofM-viewH ×W dimensionalRGBimagesI ={I }M ,andtheircorresponding
t t=1
camera projection matrices P = {P }M , P = K[R |T ], with intrinsic K , rotation R and
t t=1 t t t t t
translation T matrices. In our setting, M is 5, forming sparse-view inputs. Our goal is to learn
t
a mapping ψ from the multi-view images to object-centric anomaly labels y ∈ {0,1} and its
n
corresponding3Dboundingboxb ,definedas:
n
ψ :{(I ,P )}M (cid:55)→{(y ,b )}N . (1)
t t t=1 n n n=1
Note,labelsy aredefinedrelativetootherobjectsinthescene. Forexample,consideragroupof
n
threecoffeecups,ofwhichtwohaveredhandles,andonehasabluehandle. Inthisexample,the
lattercupisconsideredananomaly.
Ourarchitecture,illustratedinFig.2,iscomprisedofthreemaincomponents: First,the3Dfeature
fusionmoduleencodeseachviewimageandprojectsitto3D,formingafused3Dfeaturevolume.
Second,thefeaturedistillationblockisemployedtoenhancethe3Dfeaturevolumethroughdif-
ferentiablerendering. Thisfacilitatesfindinglocalcorrespondencesbetweenobjects. Finally,the
cross-instancematchingmoduleleveragesestablishedcorrespondencestocompareallsimilarobject
regionsinthesceneusingasparsevoxelattentionmechanism. Next,weelaborateondetails.
3.2 3DFeatureVolumeConstruction
We first extract 2D features F = E (I ) ∈ Rd×h×w for each input view using a shared CNN
t 2D t
encoderE ,wheredisthefeaturedimension. These2Dfeaturesarethenprojectedinto3Dvoxel
2D
spaceasfollows:
F =E (aggr({Π (F ,P )}M )), (2)
v 3D proj t t t=1
whereΠ back-projectseachviewfeatureF usingknowncameraintrinsicandextrinsic,generat-
proj t
ing3Dfeaturevolumesofsized×v ×v ×v . Thesefeaturevolumesareaggregatedoverallinput
x y z
viewsusinganaverageoperationasin[32,42]. Finally,a3DCNN-basednetworkE isemployed
3D
torefinetheaggregatedfeaturevolume,resultinginafinalvoxelrepresentationF .
v
We use volume rendering [31] to reconstruct the geometry and appearance of the scene. We im-
plementtherenderingoperationasin[24],whereforeach3Dquerypointonaray,weretrieveits
correspondingfeaturesbybilinearlyinterpolatingbetweentheneighboringvoxelgrids. Specifically,
wefirstapplyatwo-layered1×1×1convolutionblock,i.e.,α andα respectively,toobtaincolor
c σ
anddensityvolumesdenotedas(V ,V ). Then,thepixel-wisecoloranddensitymapsarecomposed
c σ
3{𝑰 ,𝑷 } % 3D Feature Fusion 𝑑 × 𝑣! × 𝑣" × 𝑣# ℬ(#) Cross-instance Matching ℳ
! ! !#$
𝒛 0 𝒛 0 𝒞 !"(𝒛#,𝒛%) 𝒛 ) 𝒛 1
𝒛 `
𝑭
) 𝒞 !"(𝒛#,𝒛$)
& 𝑝 𝑝
𝒛
1 Sparse Voxel Attention Block ×3
…
Linear Linear Linear
𝑰" ,𝑰"
! !" ℒ"#+𝜆 ℒ%&’( Feature …
𝛼 4 ! $ ! Distillation 𝑦!∈{0,1} 𝑦"∈{0,1} 𝑦#∈{0,1}
Volume DINOv2
𝛼 5 sg Rend` ering Φ’ ! Φ(𝑰 !) Binary Classification loss ℒ23&
ℛ Φ
𝛽 sg ℰ&’2D CNN ℰ(’3D CNN Π)*+,Backproject
𝑷 ! minimize difference 𝑰 ! ℬ Box Estimator 𝒞 -.Correspondence Function
[𝛼$,𝛼%]color and density heads 𝛽feature projector sg stop gradient Φ. / Rendered Features
Figure2: Overviewofourframework. Weextractfeaturesfromasequenceofinputviewsusing
a2DCNNandthenback-projectthemintoa3Dvolume,whichisthenrefinedusinga3DCNN,
resultinginF . Wethenextractobject-centricfeaturevolumes{z }N usingRoIpooling,where
v n n=1
B estimatestheRoIregionofeachobject. Thecross-instancematchingmoduleMthenlearnsa
correlationamongtheobjectsusingasparsevoxelattention. Toimprovethe3Drepresentationof
thescene,wedistilltheknowledgeofa2DvisionmodelnamelyDINOv2,andintegratethelearned
knowledgeintoour3Dnetworkviadifferentiablerendering.
byintegratingalongacamerarayusingvolumerendererR. Followingthis, wecomputetheL2
imagereconstructionlossLim. Formally,theimagerenderinganditscorrespondinglossfunctionfor
t
asingleviewpointP areshownbelow:
t
[Iˆ,Iˆ ]=R([V ,V ],P ), Lim =||I −Iˆ||2+λ ||I −Iˆ ||2, (3)
t tσ c σ t t t t σ tσ tσ
whereIˆ andIˆ arerenderedimageandmask. λ isalossweight.
t tσ σ
Weproposetoimprovethe3DvoxelrepresentationF byalsoreconstructingneuralfeaturesinstead
v
ofjustcoloranddensity. Wesupervisethefeaturereconstructionbyapretrained2Dimageencoder
Φasateachernetwork. WechooseDINOv2[33]astheteachernetworkduetoitsexcellentabilityto
capturevariousobjectgeometriesandcorrespondences.
We use a projector function β, implemented as a four-layered 1×1×1 convolution block that
projectsF toaneuralfeaturefieldV ,changingthechanneldimensionfromdtod . Similarto
v f f
colorrendering,wegeneraterenderedfeaturesΦˆ ofsized ×h ×w atagivenviewpointusing
t f f f
volumerendererR. Theobjectiveistominimizethedifferencebetweentherenderedfeaturesand
theteacher’sfeaturesΦ(I ). Wechoosecosinedistanceasourfeatureloss(Lfeat)whichwefind
t t
easiertooptimizecomparedtothestandardL2loss. Weapplystop-gradienttodensityinrendering
offeaturesΦˆ ,astheteacher’sfeaturesarenotfullymulti-viewconsistent[3],whichcouldharmthe
t
qualityofreconstructedgeometry. Thefinalreconstructionlossisthesumofallimageandfeature
reconstructionlossesweighedbyafactorλ :
f
M
(cid:88)
Lr = (Lim+λ Lfeat). (4)
t f t
t=1
ThekeybenefitsofreconstructingDINOv2featuresaretwofold. 1)Firstly,distillingfeaturesfrom
general-purposefeatureextractorspre-trainedonlargeexternaldatasets,incorporatesopen-world
knowledgeintothe3Drepresentation. Thisenablesourmodeltoperformsignificantlybetteron
unseenobjectinstancesorevenonnovelcategories,asdemonstratedintheexperiments. 2)Secondly,
thedistillationenforcesconsistent3Dscenerepresentation,leadingtoidenticalfeaturesforthesame
objectgeometries. Thisenablesthemodeltoinferrobustlocalcorrespondences(seeFig.4),which
aidsinfine-grainedobjectmatching.
4
ℰ
*)
Π
/.-,
ℰ
*+
looP
IoR
tcejbO
z3.3 Object-centric3DFeaturesExtraction
We extract bounding box regions of objects using a box estimator B. First, we obtain a voxel
reconstructionofthescenebyapplyingathresholdtothepredicteddensityV . Then,weemploy
σ
DBScan [20], a density-based clustering method to retrieve all bounding box regions {b }N
n n=1
correspondingtotheobjectsinthescene. Nextweobtainobject-centricfeaturevolumes{z }N ,
n n=1
eachwithasizeof8×8×8fromtheseregionsthroughapplyingRoIpooling[17].
Toextractcorrespondenceindicesgiventwoobjectswithfeaturevolumesz andz ,Weformally
n m
defineafunctionC :
k
C (z ,z )=top [β(z )Tβ(z )]. (5)
k n m k n m
The function returns top-k most relevant feature locations in z for each voxel location in z .
m n
This is achieved by first projecting each voxel feature using the same projector function β and
thencomputingtheirvoxel-levelpairwisesimilarity. WeuseC toperformsparseattention-based
k
comparisonsbetweenmultipleobjectvolumes,asdescribednext.
3.4 Cross-instanceMatching
Thecross-instancematchingmoduleMtakestheob-
𝑘×𝑁−1 active tokens
jectfeatures{z n}N n=1,effectivelylearnstocorrelate Object volume
themusingsparsevoxelattention,andsubsequently
𝑘tokens
predictstheirobject-specificlabels. ✓
𝑗∈𝒸!"#[𝑖]
, …
𝑖
Unlikethevanillaself-attentionmoduleinstandard ✕
transformers[19]thatusesalltokensfortheattention
computation, which is inefficient for our task and 𝒛 ! 𝒛 " 𝑚∈{1,…,𝑁}/{𝑛}
mayintroducenoisyinteractionswithirrelevantfeatures, potentiallydegradingperformance. To
overcomethis,wecomputethesparsevoxelattentiononlyamonggeometricallycorrespondingvoxel
locations,asshownintheinset.
Letz [i]∈Rddenotethei-thvoxelofthen-thobjectvolumeinthescene. Thequery,key,andvalue
n
embeddingsarecalculatedusinglinearprojectionsas:
Q [i]=WQz [i], K [i]=WKz [i], V [i]=WVz [i], (6)
n n n n n n
wheretheweightsWQ,WK andWV aresharedacrossallobjects.
Letcnm[i]denotethesetofallcorrespondingvoxelindicesobtainedusingEq.5. Then,theattention
k
iscalculatedas:
N (cid:18) (cid:19)
z¯ [i]=
(cid:88) (cid:88)
softmax
Q n[i √]K m[j]
V [j]. (7)
n m
d
m=1j∈cnm[i]
m̸=n k
Theupdatedfeaturevolumez¯ ispassedthrough3DCNNblockstodownsamplebyafactorof1/8,
n
whichisfinallyreshapedintoavectorandfedtoa2-layerMLPoutputtingthefinalpredictionyˆ .
n
Theclassificationlossiscalculatedas:
N
(cid:88)
Lbce = ℓ (yˆ ,y ), (8)
bce n n
n=1
where ℓ is the binary cross-entropy loss function. The total training loss of our framework is
bce
L=Lbce+λ Lr,whereλ isalossweight. Weemploystage-wisetraining: firstpretrainingwith
p p
onlythereconstructionloss(Lr),followedbyend-to-endtrainingwithbothlosses.
4 Experiments
4.1 Datasets
Asnopriordatasetexistsforthetask, weproposetwochallengingscenedatasets, bothessential
for thorough evaluation: ToysAD-8K and PartsAD-15K. ToysAD-8K includes real-world objects
from multiple categories. This allows us to evaluate our model’s ability to generalize to unseen
object categories. PartsAD-15K comprises a more diverse collection of mechanical object parts
5witharbitraryshapes,thusbeingfreefromanyclass-levelinductivebiases. Bothdatasetsincludea
widerangeoffine-grainedanomalyinstancesmotivatedbyreal-worldapplicationsininspectionand
qualitycontrol.Scenesaregeneratedwithdiversebackgrounds,illuminations,andcameraviewpoints
usingphoto-realisticraytracing[43]. Next,wediscussthedatagenerationforbothdatasets.
ToysAD-8K.Webeginwithasubsetof1050shapesfromtheToys4Kdataset[41]. Thesubsetcovers
awiderangeofobjectsfrom51categories. Weautomaticallycreateanomaliesforagiven3Dshape
byapplyingvariousdeformationstoboththegeometryandtexture. Thisincludesgeneratingrealistic
cracksandfracturesusing[39],applyingrandomgeometricdeformations[43]likebumps,bends,
andtwists,aswellasrandomlytranslating,rotating,andswappingmaterialsindifferentpartsof
theshapes. Intotal,wegenerated2345anomalyshapes. Togenerateeachscene,wefirstrandomly
chooseasetofobjectsconsistingofbothnormalandtheiranomalyofthesameinstances. Weensure
thatthemajorityofobjectsineachscenearenormal. Wealsohavefewsceneswherenoanomaly
ispresent. Therotationalposesfortheobjectsareobtainedusingrigidbodysimulation[4]. The
objectsarescaledandplacedintothesceneatrandomlocations,ensuringcollisionsdonotoccur. We
generatedtotal8K scenes. Eachsceneconsistsof3-6objectsrenderedin20views. Forthetraining
set,werandomlyselect5K scenesfromthe39categories. Webuildtwodisjointtestsets. Thefirst
one(seen)contains1K scenesfromtheseencategoriesbutwithunseenobjectinstances. Thesecond
one(unseen)contains2K scenesfromrestofthe12novelcategories.
PartsAD-15K.WeuseasubsetoftheABCdataset[27]thatconsistsof4200shapes. Wefollowthe
strategyabovetogenerateanomalies. Additionally,foreachshape,wesamplegeometricallyclose
instancesfromthedatasetandassignthemasanomaliestouseinthesamescene. Thisapproach
generatesalargesetofhigh-qualityanomaliesthatcloselyresembletheirnormalcounterpartsin
high-levelgeometry,butwithsubtleshapevariationsthatmakethemanomalousinthecontextof
thenormalones. Intotal,wegenerated10,203anomalyshapes. Usingtheseshapes,wecreated
15K scenes,eachconsistingof3to12objectsrenderedfrom20differentviewpoints. Wedividethe
datasetintoa12K trainingsetandtherestasthetestset. Notethattheproposeddatasets,source
code,andmodelswillbemadepublicbaseduponpublication.
4.2 Implementationdetails
WeuseaResNet50-FPN[30]asour2Dencoderbackbone. Our3Dbackboneconsistsofafour-scale
encoder-decoder-based3DCNN[32]. WeuseM = 5imagesasinput,eachwitharesolutionof
256×256. OurarchitectureisflexibletoacceptadifferentM duringinference. Duringtraining,we
consideratotalof2M views,whichweseparateintotwosets,eachcontainingM views. Weuseone
settobuildtheneuralvolumeandthecamerasoftheothersettorendertheresults,andviceversa.
The3DvolumeF contains96×96×16voxelswithavoxelsizeof4cm. Wesample128points
v
oneachrayforrendering. Werenderthefeatureswithaspatialdimensionof32×32. TheI in
tσ
Eq.(3)isthegroundtruthsegmentationmaskoftheinputimage,weonlyuseitfortraining. The
thresholdappliedtothedensityvolumeV issettobe0.2andweruntheDBScanalgorithmwithits
σ
defaultparameters. Weresizetheteacher’s(DINOv2)featurestothesamespatialdimensionforloss
computation. Thesefeaturesarepre-computedforallscenesusingpubliclyavailableweights,which
are not updated during distillation. These DINOv2 features are then reduced to d =128 channel
f
dimensionsusingPCAbeforedistillation. Weemploythreesparsevoxelattentionblocks,andeach
applies8-headedattention. Thevalueofkischosenas20. Thelossweightsλ ,λ ,andλ areall
σ f p
setto1. Wefirstpretrainthenetworkwithonlythereconstructionlossfor50epochs. Then,wetrain
thenetworkend-to-endwithboththereconstructionlossandthebinaryclassificationlossforanother
50epochs. Wemaintainabatchsizeof4,andusetheAdamoptimizerwithalearningrate2×10−5.
Finally,therun-timeofourmethodis65msonasingleA40GPUforatypicalscenewith5viewsas
input.
4.3 BaselineComparisons
Wequantitativelyevaluatetheanomalyclassificationresultsusingtwoevaluationmetrics–thearea
undertheROCcurve(AUC)andaccuracy. Apredictionisconsideredcorrectiftheboundingbox
IoUisgreaterthan0.5andthecorrespondinganomalyclassificationiscorrect. Wehavenotdesigned
aseparatelocalizationmetricasourestimatedboundingboxesareveryaccurate. Thesemetricsare
calculatedobject-wiseandthenaveragedacrossalltestscenes. Sincenopriorworkexistsonthis
task,wedefineseveralcompetitivebaselinesforcomparison. Specifically,in Tab.1,wecompare
6ourmethodwithtworelevantapproaches: areconstruction-basedbaselineandtwomulti-view3D
objectdetectionmethods. Forthereconstruction-baseddesign,wefirstemployCOLMAP[38]to
obtainapointcloudreconstructionofthemulti-objectsceneenvironment. Weusedefaultdense
reconstructionparametersbututilizetheprovidedgroundtruthcameramatrices. Since5viewsare
insufficient,weuseatotalof20viewstoreconstructthescene. Weextractindividualobjectpoint
clouds from the reconstructed scenes, and train a Siamese style network to obtain their pairwise
similarity. WeuseDGCNN[49](pretrainedonShapeNet[14])asthepointcloudfeatureextractor
andthetripletloss[22]tosupervisethenetwork. Finally,weemployavotingstrategytoaggregate
the pairwise distances of all objects in the scene and obtain their individual binary labels. We
observethattheaccuracyofthismethodissensitivetothereconstructionquality,leadingtopoor
performanceonbothdatasets. Next,weadaptImVoxelNet[37]andDETR3D[48],twomulti-view
object detection frameworks in our problem setting, aiming to locate and classify each object in
thesceneaseitheranomalyornormal. ImVoxelNetusesthesimilar2D-3Dprojectionasoursto
constructthevoxelrepresentationandthena3Ddetectionhead[29]outputsthefinalprediction.
DETR3Disatransformer-baseddesignandusesthesetpredictionloss[9]forend-to-enddetection
withoutNMS.Weusethegroundtruth3Dboundingboxestotrainthesemodelsandthereported
scoresarecalculatedobject-wisebasedontheoutputoftheclassificationhead. Weobservethatwhile
bothmethodsperformwellforlargecracksorfractures,theystrugglewhenintra-groupcomparison
is necessary. This is because they tend to memorize certain anomaly types without learning to
generalize to compare with other objects in the scene. Our method significantly outperforms all
baselines(Tab.1)onbothdatasets,highlightingtheeffectivenessofourdedicatedarchitecturefor
matchingcorrespondingregions. Notably,theperformancedropontheunseensetisrelativelylower
inourcase,attributedtotherobust3Drepresentationthateffectivelygeneralizestonovelcategories.
WeshowqualitativeresultsofourmethodinFig.3
Table1: Quantitativeresults. Wecompareourmethodtothreerelatedworksintwodatasetsand
reporttheresultsintermsofanomalydetectionAUCandaccuracy.
COLMAP[38] ImVoxelNet[37] DETR3D[48] Ours
Datasets
AUC Accuracy AUC Accuracy AUC Accuracy AUC Accuracy
ToysAD-8K-Seen 73.45 60.48 78.13 65.55 79.16 67.37 91.78 83.21
ToysAD-8K-Unseen 72.86 58.12 73.19 60.12 74.60 62.98 89.15 81.57
PartsAD-15K 72.78 61.34 72.80 64.34 74.49 65.11 86.12 79.68
(a) ToysAD-8K (b) PartsAD-15K
Figure3: ADqualitativeresultson(a)theunseentestcategoriesofToysAD-8Kand(b)thetestsetof
PartsAD-15Kusingourproposedframework. Thegreenboxdenotescorrectpredictions,whilethe
redboxindicateswherethemodelmissestheanomaly. Duetolimitedspace,oneviewisshownper
scene. Pleaserefertothesupplementarymaterialsformoreresults.
4.4 AblationsandModelAnalysis
Weconductseveralstudiesintotheperformanceoftheproposedmodelincludingchangestoarchi-
tecture,robustnessanalysisandreal-worldexperiments. Unlessstatedotherwise,allexperimentsin
thissectionarecarriedoutontheToysAD-8Kunseensetusing5inputviews.
Ablationofarchitecturedesign. Weablatethecorecomponentsinourmodelandreporttheresults
in Tab. 2. All variants include the 3D feature fusion module and are optimized at least for the
image reconstruction loss, which is essential for constructing scenegeometry. Variant A directly
7Figure4: Correspondencesareobtainedin3DspaceusingtheneuralfeaturefieldV ,thenprojected
f
onto2Dviewsusingcameramatricesforvisualization. Thefeaturefieldisrenderedatrespective
viewpoints,thefirst3PCAcomponentsareused,witheachmatchedtoadifferentcolorchannel.
mapsobject-centricfeaturestotheirbinarylabelsusinganMLPwithoutcomparingthem, while
forvariantB,standardattentionlayersareappliedtotheobject-centricfeaturestolearncross-object
correlations. Despitetheattentionlayers,Bonlyshowsaslightimprovement(+2.1%AUC)overA.
WethenintroduceDINOv2featuredistillation
Table2: AblationResultsonToysAD-8K
(variant C), which significantly boosts perfor-
manceby5.8%AUCand11.3%accuracy, in- Variants AUC Accuracy
dicating the importance of the part-aware 3D
A:baseline 79.13 66.78
representationandcorrespondencesforourtask B:A +vanillaattention 81.24 68.20
(alsoseeFig.4). Ourfinaldesignachievesthe C:B +featuredistillation 87.05 79.56
performancegainbyutilizingsparsevoxelatten-
Final:C +sparsevoxelattention 89.15 81.57
tion,whichfocusesonthetop-kmostrelevant
features. ThisleveragesrobustcorrespondenceslearnedthroughDINOv2featuredistillation,effec-
tivelyeliminatingnoisycorrelationsanddirectingattentionsolelytocorrespondingobjectregions.
Robustness. Ourmethodachievessomerobustnesstoocclusionbyeffectivelyusinginputfrom
multipleviewpoints. Fig.7illustratesexampleswhereourmodelaccuratelyidentifiestheanomalous
region,evenwhenoccludedinsomeviews. InFig.5(left),weshowhowwellourmodelcanperform
withadditionalorreducedinputviewsattesttime. Wetrainourmodelwith5viewsandthenevaluate
itusing1,3,5,10and20views. Theseresultsclearlyindicatethatourmodelcanperformreasonably
wellwithonly3-5views;however,additionalviewscanbeusedtoboostperformanceattesttime.
We analyze the impact of object count on
modelperformance(seeFig.5(right))usingthe
PartsAD-15K dataset. Increasing the number
ofobjectsinasceneusuallyleadstomoreoc-
clusionandlowerresolutionforeachindividual
object. However,theperformancedropbetween
thetwoextremesisminimal(<1.5%AUC)as
showninthefigure. Inaddition,weinvestigate
Figure 5: Impact of number of views (left) and
themodel’sabilitytogeneralizetomoreobjects
objectcount(right)onmodelperformance.
attesttimethanitobservedduringtraining. To
thisend,wetrainedourmodelonsceneswith3-7objectsandtestedontwosets: onewith3-7objects
(AUC:86.75)andanotherwith8-12objects(AUC:85.10). Theseresultsdemonstrateourmodel’s
adaptabilitytovaryingobjectcounts.
Inanotherstudy,wetestourprimaryobjectiveofchoosingthemajoritygroupasnormalandtherest
asanomaliesusinganexampleshowninFig.6.Wecreatefivescenes(a-e)byusingtwogeometrically
similarobjects,andgraduallyintroducemorefromone(1to5respectively)whilemaintainingatotal
ofsixobjects. Forexample,inscene(a),thefirstobjectappearsonlyonce,makingitananomaly.
Similarly,inscene(e),thesecondobjectappearsonlyonce,henceconsideredananomaly. Weutilize
aconsistentbackgroundacrossallscenestoensureuniformity. Asshowninthefigure,ourmodel
correctlyclassifiestheanomaliesineachscene. Wenotescene(c)presentsanambiguouscase,where
bothobjectsappearinequalnumbers. Despitethis,ourmodelisabletoseparatethetwogroups.
Realworldtesting. Hereweapplyourmodel,whichistrainedonthesyntheticdataset,onasmall
setofrealtestscenes. Eachsceneissetupinanindoorenvironmentwithadequatelightingandis
8(a) (b) (c) (d) (e)
Figure6: Wecreatefivescenes(a-e)withtwosimilarobjects,varyingtheircountsperscenewhile
maintainingaconstanttotal.Ourmodelcorrectlyidentifiesminoritiesasanomaliesinallcasesexcept
(c),wheretheequalnumberofobjectscreatesambiguity. Ourmodelstillselectsonegroup.
(a) (b)
Figure 7: Resolving (a)occlusion and (b) 3Dambiguity using multi-view images. The anomaly
‘sheep’in(a)hasamissingtail(onlyvisibleinthelastviewduetoocclusion),andthe‘hammer’
handlein(b)isbent(onlyapparentfromthelastview-angledueto3Dambiguities).
Figure 8: Three real-world scenes are tested using our method and it can successfully detect all
anomalousinstances. Fiveinputviewsareusedfortesting,whileoneviewforeachsceneisusedfor
visualization.
capturedusinga3Dscanningsoftware[1].Thisresultsinasetofinputviewswithgloballyoptimized
cameras. Fig.8illustratestheresultsforthreesuchscenes.
Limitations. Ourbenchmarkandmodelalsohaveafewlimitations. Firstly,thisworkfocusessolely
onalimitedsetofanomaliesthatarecommontomanufacturingscenarios;potentiallymissingother
anomaliesinreal-worldscenarios. Sinceacquiringrealdamagedobjectsisexpensiveanddifficult,
ourdatasetprimarilyusesshapesofsyntheticobjects. Moreover, ourmodelassumesthatobject
instancesarerigidandcannothandlearticulationsordeformations. Italsoassumesthatobjectsare
not touching or completely occluded in space. Moreover, a scene with mostly anomalies can be
challengingforourmodelduetothelackof‘normal’datapointsforcomparison(exceptforfractures
orcracks). Moreover,theperformanceisdependentontheanomalousregionbeingcapturedinat
leastoneview. Finally,inreal-worldscenetesting,thenoisycameraposesaswellasthegapbetween
syntheticandreal-worldenvironmentsmaydegradetheperformance.
5 Conclusion
Inthispaper,wehaveintroducedanovelADprobleminspiredbyreal-worldapplicationsalongwith
twonewbenchmarks. TheproposedtaskgoesbeyondthetraditionalADsettingandinvolvesacross
studyofobjectsinascenefrommultiplecameraviewpointstoidentifythe‘odd-looking’minority
group. Weshowthatourmodelisrobusttovaryingnumberofviewsandobjects,andoutperforms
thebaselinesthatdonotconsidercross-objectcorrelations.
BroaderImpacts. Theproposedtechniquescouldbepotentiallyusedinimprovingqualitychecks
andproductsafetybyprovidingwarningstotheusersinproductionlines. Theauthorsarenotaware
ofanypotentialharmthatmayarisewhenthetechnologyisused.
9References
[1] Polycam. https://github.com/PolyCam/polyform.
[2] FarukAhmedandAaronCourville. Detectingsemanticanomalies. InAAAI,2020.
[3] MohamedElBanani,AmitRaj,Kevis-KokitsiManinis,AbhishekKar,YuanzhenLi,MichaelRubinstein,
DeqingSun,LeonidasGuibas,JustinJohnson,andVarunJampani. Probingthe3dawarenessofvisual
foundationmodels. arXivpreprintarXiv:2404.08636,2024.
[4] David Baraff. Physically based modeling: Rigid body simulation. SIGGRAPH Course Notes, ACM
SIGGRAPH,2(1):2–1,2001.
[5] Paul Bergmann, Michael Fauser, David Sattlegger, and Carsten Steger. Mvtec ad–a comprehensive
real-worlddatasetforunsupervisedanomalydetection. InCVPR,2019.
[6] PaulBergmann,XinJin,DavidSattlegger,andCarstenSteger. Themvtec3d-addatasetforunsupervised
3danomalydetectionandlocalization. arXivpreprintarXiv:2112.09045,2021.
[7] AnkanBhunia,ChangjianLi,andHakanBilen. Looking3d:Anomalydetectionwith2d-3dalignment. In
ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,2024.
[8] HermannBlum,Paul-EdouardSarlin,JuanNieto,RolandSiegwart,andCesarCadena. Thefishyscapes
benchmark:Measuringblindspotsinsemanticsegmentation. IJCV,2021.
[9] NicolasCarion,FranciscoMassa,GabrielSynnaeve,NicolasUsunier,AlexanderKirillov,andSergey
Zagoruyko. End-to-endobjectdetectionwithtransformers. InEuropeanconferenceoncomputervision,
pages213–229.Springer,2020.
[10] DiegoCarrera,FabioManganini,GiacomoBoracchi,andEttoreLanzarone. Defectdetectioninsem
imagesofnanofibrousmaterials. IEEETransactionsonIndustrialInformatics,2016.
[11] RaghavendraChalapathy,AdityaKrishnaMenon,andSanjayChawla. Anomalydetectionusingone-class
neuralnetworks. arXivpreprintarXiv:1802.06360,2018.
[12] RobinChan,KrzysztofLis,SvenjaUhlemeyer,HermannBlum,SinaHonari,RolandSiegwart,PascalFua,
MathieuSalzmann,andMatthiasRottmann.Segmentmeifyoucan:Abenchmarkforanomalysegmentation.
arXivpreprintarXiv:2104.14812,2021.
[13] VarunChandola,ArindamBanerjee,andVipinKumar. Anomalydetection:Asurvey. ACMcomputing
surveys,2009.
[14] AngelXChang,ThomasFunkhouser,LeonidasGuibas,PatHanrahan,QixingHuang,ZimoLi,Silvio
Savarese,ManolisSavva,ShuranSong,HaoSu,etal. Shapenet:Aninformation-rich3dmodelrepository.
arXivpreprintarXiv:1512.03012,2015.
[15] AnpeiChen,ZexiangXu,FuqiangZhao,XiaoshuaiZhang,FanboXiang,JingyiYu,andHaoSu. Mvsnerf:
Fastgeneralizableradiancefieldreconstructionfrommulti-viewstereo. InProceedingsoftheIEEE/CVF
internationalconferenceoncomputervision,pages14124–14133,2021.
[16] LucasDeecke,LukasRuff,RobertAVandermeulen,andHakanBilen. Transfer-basedsemanticanomaly
detection. InICML,2021.
[17] JiajunDeng,ShaoshuaiShi,PeiweiLi,WengangZhou,YanyongZhang,andHouqiangLi. Voxelr-cnn:
Towardshighperformancevoxel-based3dobjectdetection. InProceedingsoftheAAAIconferenceon
artificialintelligence,pages1201–1209,2021.
[18] Choubo Ding, Guansong Pang, and Chunhua Shen. Catching both gray and black swans: Open-set
supervisedanomalydetection.InProceedingsoftheIEEE/CVFconferenceoncomputervisionandpattern
recognition,pages7388–7398,2022.
[19] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner,MostafaDehghani,MatthiasMinderer,GeorgHeigold,SylvainGelly,etal.Animageisworth
16x16words:Transformersforimagerecognitionatscale. arXivpreprintarXiv:2010.11929,2020.
[20] Martin Ester, Hans-Peter Kriegel, Jörg Sander, and Xiaowei Xu. Density-based spatial clustering of
applicationswithnoise. InInt.Conf.knowledgediscoveryanddatamining,1996.
[21] DanHendrycksandKevinGimpel. Abaselinefordetectingmisclassifiedandout-of-distributionexamples
inneuralnetworks. InICLR,2017.
[22] EladHoffer andNir Ailon. Deep metric learningusing tripletnetwork. In Similarity-BasedPattern
Recognition:ThirdInternationalWorkshop,SIMBAD2015,Copenhagen,Denmark,October12-14,2015.
Proceedings3,pages84–92.Springer,2015.
[23] ChaoqinHuang,HaoyanGuan,AofanJiang,YaZhang,MichaelSpratling,andYan-FengWang. Regis-
trationbasedfew-shotanomalydetection. InEuropeanConferenceonComputerVision,pages303–319.
Springer,2022.
10[24] HanwenJiang,ZhenyuJiang,KristenGrauman,andYukeZhu. Few-viewobjectreconstructionwith
unknowncategoriesandcameraposes. arXivpreprintarXiv:2212.04492,2022.
[25] AlexanderKirillov,EricMintun,NikhilaRavi,HanziMao,ChloeRolland,LauraGustafson,TeteXiao,
SpencerWhitehead,AlexanderCBerg,Wan-YenLo,etal. Segmentanything. InProceedingsofthe
IEEE/CVFInternationalConferenceonComputerVision,pages4015–4026,2023.
[26] SosukeKobayashi,EiichiMatsumoto,andVincentSitzmann. Decomposingnerfforeditingviafeature
fielddistillation. AdvancesinNeuralInformationProcessingSystems,35:23311–23330,2022.
[27] SebastianKoch,AlbertMatveev,ZhongshiJiang,FrancisWilliams,AlexeyArtemov,EvgenyBurnaev,
MarcAlexa,DenisZorin,andDanielePanozzo. Abc:Abigcadmodeldatasetforgeometricdeeplearning.
InTheIEEEConferenceonComputerVisionandPatternRecognition(CVPR),June2019.
[28] KiriakosNKutulakosandStevenMSeitz. Atheoryofshapebyspacecarving. Internationaljournalof
computervision,38:199–218,2000.
[29] AlexHLang,SourabhVora,HolgerCaesar,LubingZhou,JiongYang,andOscarBeijbom. Pointpillars:
Fastencodersforobjectdetectionfrompointclouds. InProceedingsoftheIEEE/CVFconferenceon
computervisionandpatternrecognition,pages12697–12705,2019.
[30] Tsung-YiLin,PiotrDollár,RossGirshick,KaimingHe,BharathHariharan,andSergeBelongie. Feature
pyramidnetworksforobjectdetection. InProceedingsoftheIEEEconferenceoncomputervisionand
patternrecognition,pages2117–2125,2017.
[31] BenMildenhall,PratulPSrinivasan,MatthewTancik,JonathanTBarron,RaviRamamoorthi,andRen
Ng. Nerf:Representingscenesasneuralradiancefieldsforviewsynthesis. CommunicationsoftheACM,
65(1):99–106,2021.
[32] ZakMurez,TarrenceVanAs,JamesBartolozzi,AyanSinha,VijayBadrinarayanan,andAndrewRabi-
novich. Atlas:End-to-end3dscenereconstructionfromposedimages. InComputerVision–ECCV2020:
16thEuropeanConference,Glasgow,UK,August23–28,2020,Proceedings,PartVII16,pages414–431.
Springer,2020.
[33] MaximeOquab,TimothéeDarcet,ThéoMoutakanni,HuyVo,MarcSzafraniec,VasilKhalidov,Pierre
Fernandez,DanielHaziza,FranciscoMassa,AlaaeldinEl-Nouby,etal. Dinov2:Learningrobustvisual
featureswithoutsupervision. arXivpreprintarXiv:2304.07193,2023.
[34] GuansongPang,ChunhuaShen,LongbingCao,andAntonVanDenHengel. Deeplearningforanomaly
detection:Areview. ACMcomputingsurveys,2021.
[35] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,SandhiniAgarwal,Girish
Sastry,AmandaAskell,PamelaMishkin,JackClark,etal. Learningtransferablevisualmodelsfrom
naturallanguagesupervision. InInternationalconferenceonmachinelearning,pages8748–8763.PMLR,
2021.
[36] LukasRuff,RobertVandermeulen,NicoGoernitz,LucasDeecke,ShoaibAhmedSiddiqui,Alexander
Binder,EmmanuelMüller,andMariusKloft. Deepone-classclassification. InICML,2018.
[37] DanilaRukhovich,AnnaVorontsova,andAntonKonushin. Imvoxelnet:Imagetovoxelsprojectionfor
monocularandmulti-viewgeneral-purpose3dobjectdetection. InProceedingsoftheIEEE/CVFWinter
ConferenceonApplicationsofComputerVision,pages2397–2406,2022.
[38] JohannesLutzSchönberger,EnliangZheng,MarcPollefeys,andJan-MichaelFrahm. Pixelwiseview
selectionforunstructuredmulti-viewstereo. InEuropeanConferenceonComputerVision(ECCV),2016.
[39] SilviaSellán,JackLuong,LeticiaMattosDaSilva,AravindRamakrishnan,YuchuanYang,andAlec
Jacobson. Breakinggood: Fracturemodesforrealtimedestruction. ACMTransactionsonGraphics,
42(1):1–12,2023.
[40] XuepengShi,QiYe,XiaozhiChen,ChuangrongChen,ZhixiangChen,andTae-KyunKim. Geometry-
based distance decomposition for monocular 3d object detection. In Proceedings of the IEEE/CVF
InternationalConferenceonComputerVision,pages15172–15181,2021.
[41] Stefan Stojanov, Anh Thai, and James M Rehg. Using shape to categorize: Low-shot learning with
an explicit shape bias. In Proceedings of the IEEE/CVF conference on computer vision and pattern
recognition,pages1798–1808,2021.
[42] JiamingSun,YimingXie,LinghaoChen,XiaoweiZhou,andHujunBao.Neuralrecon:Real-timecoherent
3dreconstructionfrommonocularvideo. InProceedingsoftheIEEE/CVFconferenceoncomputervision
andpatternrecognition,pages15598–15607,2021.
[43] BlenderDevelopmentTeam. Blender(version3.1.0)[computersoftware]. https://blender.org/,
2022.
[44] AnhThai,AhmadHumayun,StefanStojanov,ZixuanHuang,BikramBoote,andJamesMRehg.Low-shot
objectlearningwithmutualexclusivitybias. AdvancesinNeuralInformationProcessingSystems,36,
2024.
11[45] AlexTrevithickandBoYang. Grf: Learningageneralradiancefieldfor3dscenerepresentationand
rendering. InProceedingsoftheIEEE/CVFinternationalconferenceoncomputervision,2021.
[46] Vadim Tschernezki, Iro Laina, Diane Larlus, and Andrea Vedaldi. Neural feature fusion fields: 3d
distillationofself-supervised2dimagerepresentations. In2022InternationalConferenceon3DVision
(3DV),pages443–453.IEEE,2022.
[47] QianqianWang,ZhichengWang,KyleGenova,PratulPSrinivasan,HowardZhou,JonathanTBarron,
RicardoMartin-Brualla,NoahSnavely,andThomasFunkhouser.Ibrnet:Learningmulti-viewimage-based
rendering. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,
pages4690–4699,2021.
[48] YueWang,VitorCampagnoloGuizilini,TianyuanZhang,YilunWang,HangZhao,andJustinSolomon.
Detr3d:3dobjectdetectionfrommulti-viewimagesvia3d-to-2dqueries.InConferenceonRobotLearning,
pages180–191.PMLR,2022.
[49] Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E Sarma, Michael MBronstein, andJustin M Solomon.
Dynamicgraphcnnforlearningonpointclouds. ACMTransactionsonGraphics(tog),38(5):1–12,2019.
[50] Jhih-CiangWu,Ding-JieChen,Chiou-ShannFuh,andTyng-LuhLiu. Learningunsupervisedmetaformer
foranomalydetection. InProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision,
pages4369–4378,2021.
[51] EnzeXie,ZhidingYu,DaquanZhou,JonahPhilion,AnimaAnandkumar,SanjaFidler,PingLuo,and
JoseMAlvarez. M2BEV:Multi-camerajoint3ddetectionandsegmentationwithunifiedbirds-eyeview
representation. arXivpreprintarXiv:2204.05088,2022.
[52] GuoyangXie,JinbaoWang,JiaqiLiu,FengZheng,andYaochuJin.Pushingthelimitsoffewshotanomaly
detectioninindustryvision:Graphcore. arXivpreprintarXiv:2301.12082,2023.
[53] TianweiYin,XingyiZhou,andPhilippKrahenbuhl. Center-based3dobjectdetectionandtracking. In
ProceedingsoftheIEEE/CVFconferenceoncomputervisionandpatternrecognition,pages11784–11793,
2021.
[54] AlexYu,VickieYe,MatthewTancik,andAngjooKanazawa. pixelnerf:Neuralradiancefieldsfromoneor
fewimages. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,
pages4578–4587,2021.
[55] Weihao Yuan, Xiaodong Gu, Heng Li, Zilong Dong, and Siyu Zhu. 3d former: Monocular scene
reconstructionwith3dsdftransformers. arXivpreprintarXiv:2301.13510,2023.
[56] GregZaal,RobTuytel,RicoCilliers,JamesRayCock,AndreasMischok,SergejMajboroda,Dimitrios
Savva,andJuritaBurger. Polyhaven: acuratedpublicassetlibraryforvisualeffectsartistsandgame
designers,2021.
[57] JunyiZhang,CharlesHerrmann,JunhwaHur,LuisaPolaniaCabrera,VarunJampani,DeqingSun,and
Ming-HsuanYang. Ataleoftwofeatures: Stablediffusioncomplementsdinoforzero-shotsemantic
correspondence. AdvancesinNeuralInformationProcessingSystems,36,2024.
[58] QiangZhou,WeizeLi,LihanJiang,GuoliangWang,GuyueZhou,ShanghangZhang,andHaoZhao. Pad:
Adatasetandbenchmarkforpose-agnosticanomalydetection. InNeurIPS,2024.
[59] YangZou,JongheonJeong,LathaPemula,DongqingZhang,andOnkarDabeer. Spot-the-difference
self-supervisedpre-trainingforanomalydetectionandsegmentation.InEuropeanConferenceonComputer
Vision,pages392–408.Springer,2022.
12A Appendix/supplementalmaterial
This appendix is structured as follows: we present additional details of the proposed datasets in
Sec.A.1,trainingdetailsinSec.A.2,andadditionalqualitativeresultsinSec.A.3.
A.1 DataGenerationDetails
OurproposedsceneADdatasets,ToysAD-8KandPartsAD-15K,arebuiltupontwopubliclyavailable
3Dshapedatasets: Toys4K[41](CreativeCommonsandroyalty-freelicenses)andABC[27](MIT
license). FortheToysAD-8K,weselected1,050shapesfromtheToys4Kdataset,focusingonthe
mostcommonreal-worldobjectsacross51categories. Acompletelistofthesecategoriesisprovided
in Table 3. On the other hand, PartsAD-15K is a non-categorical dataset. For this dataset, we
randomlyselectedasubsetof4,200shapesfromthelarge-scaleABC.
Forbothdatasets,weconsiderthefollowinganomalytypes: cracks,fractures,geometricdeforma-
tions(e.g.,bumps,bends,andtwists),translation,rotation,materialmismatch,andmissingparts.
Additionally,forPartsAD-15K,weuseadifferentbutgeometricallysimilarinstanceinthesame
sceneasananomaly. Toobtainageometricallysimilarshape,weusefeature-basedKNNclustering.
Specifically, we extract DINOv2 features from multi-view images (rendered from multiple fixed
viewpoints)ofthe3Dshapes,concatenatethesefeatures,andusetheresultingconcatenatedfeatures
tobuildtheKNNcluster. Then,weretrieveasimilar3DshapebyqueryingtheKNNcluster. To
ensuretheretrievedshapeisgeometricallysimilar,wecalculatetheChamferdistancebetweenthe
shapesandonlyacceptashapeifthedistanceisbelowacertainthreshold.
Table3: DatasetcompositionofToysAD-8K
Categories
dinosaur,fish,frog,monkey,light,lizard,orange,boat,dog,lion,pig,cookie,panda,chicken,
Seen orange,ice,horse,car,airplane,cake,shark,donut,hat,cow,apple,bowl,hamburger,octopus,
giraffe,chess,bread,butterfly,cupcake,bunny,elephant,fox,deer,bus,bottle
Unseen mug,plate,robot,glass,sheep,shoe,train,banana,cup,key,penguin,hammer
Our anomaly generation process is automatic. To ensure the quality of the generated anomalies,
weperformseveralchecks. Forexample,duringpositionalorrotationalanomalycreation,ifapart
detachesfromthemainbodyduringdeformation,werejecttheanomalyandtryagainwithadjusted
parameters. Similarly,ifremovingapartmakestheshapeimpractical,wediscarditandtryremoving
adifferentpart. Forfractureanomaly,ifaparticularfractureremovesmorethan90%orlessthan10%
ofanobjectwediscardthesampleandregenerateanotherfracture. Finally,weensuretheanomalous
regionofanobjectisvisiblefromatleastoneviewpoint.
Togeneratearealisticsceneenvironment,weusePBRmaterials[44]forfloorsandHDRIenvironment
maps[56]forimage-basedlightingtoilluminatescenes. WerandomlyselectapairofPBRmaterial
andHDRIenvironmentmapsfromtheassetstorandomizethescenebackground. Objectsareplaced
randomlysuchthatnocollisionsoccur,andeachobject’srotationalposesareobtainedusingBlender’s
rigidbodysimulation[43]. WeemployedBlender2.93[43]withCyclesray-tracingrendererfor
photo-realisticrendering. Blender2.93isreleasedundertheGNUGeneralPublicLicense(GPL,or
“freesoftware”),andthePBRandHDRImapsarereleasedundertheCC0license.
Ourframeworkcaneasilybetrainedwithreal-worldmanufacturingsceneenvironments. Ourmodel
reliessolelyon2Dsupervision, makingthedatacollectionprocessmucheasier. Wealsodonot
needpreciseannotationof3Dboundingboxesastheyarenotusedduringtraining. Forannotating
instance-wiseanomalylabels,wecanuse2Dboundingboxes,whichcanbeprojectedin3Dusing
VisualHull[28],thenusedforcoarselocalization.
A.2 TrainingDetails
Wetrainourmodelintwostages. Inthefirststage,wetrainitwithjustimageandfeaturereconstruc-
tionlosses. Inthesecondstage,wetrainthemodelend-to-endwithbothreconstructionandbinary
classificationlosses. AllexperimentsareperformedonasingleNVIDIAA40GPUwithabatchsize
of4,utilizing28GBofGPUmemory. Thefirststagetakes36hourstocomplete,followedbyan
additional24hoursforthesecondstage.
13TheablationexperimentsareconductedonthesameworkstationwiththesameGPUbyremoving
oneorafewcorecomponentsfromthefullmethod. Specifically,variantmethodsAandBtake36
hourstotrainthefirststage,whileonlytaking14hourstotrainthesecondstage. Regardingvariant
methodC,ittakessimilar36and24hoursforthetwostagesasinthefullmethod.
WecompareourmethodwithCOLMAP(BSDlicense),ImVoxelNet(MITlicense),andDETR3D
(MIT license). For the COLMAP-based approach, we use DGCNN (MIT license) as a feature
extractor. Intermsoftrainingtimeofthesebaselinemethods, COLMAPtakes10hourstotrain,
ImVoxelNettakes24hourstotrain,andDETR3Dtakes2daystoconverge.
Thestandarddeviationofallourexperiments(includingtheablationsandourmethod)undermultiple
runsislessthan0.5.
A.3 AdditionalQualitativeResults
InFig.9andFig.10,wepresentadditionalqualitativeresultsonToysAD-8KandPartsAD-15K,
respectively.
14Figure9: AdditionalresultsontheunseensetofToysAD-8Kdataset. Forrows1to3,theanomalies
areeasytospotandself-explanatory. Inrow4,therearetwoanomalies: onewithbrokenouterparts
attheback(seeview5),andanotherwithatiltedroof(seeviews1and2). Forrow5,onelegis
broken(seeview2). Inrow6,theeyeismissing(seeview5).
15Figure10: AdditionalresultsonthePartsAD-15Kdataset.
16