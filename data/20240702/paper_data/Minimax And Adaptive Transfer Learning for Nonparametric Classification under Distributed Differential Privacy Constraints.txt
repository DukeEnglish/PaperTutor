Minimax And Adaptive Transfer Learning for Nonparamet-
ric Classification under Distributed Differential Privacy Con-
straints
Arnab Auddy
Department of Biostatistics, Epidemiology and Informatics,
University of Pennsylvania, Philadelphia, PA 19104.
T. Tony Cai
Department of Statistics and Data Science, The Wharton School,
University of Pennsylvania, Philadelphia, PA 19104.
Abhinav Chakraborty
Department of Statistics and Data Science, The Wharton School,
University of Pennsylvania, Philadelphia, PA 19104.
Summary. This paper considers minimax and adaptive transfer learning for nonparametric
classification under the posterior drift model with distributed differential privacy constraints.
Our study is conducted within a heterogeneous framework, encompassing diverse sample sizes,
varying privacy parameters, and data heterogeneity across different servers.
We first establish the minimax misclassification rate, precisely characterizing the effects of
privacy constraints, source samples, and target samples on classification accuracy. The results
reveal interesting phase transition phenomena and highlight the intricate trade-offs between
preservingprivacyandachievingclassificationaccuracy.Wethendevelopadata-drivenadaptive
classifier that achieves the optimal rate within a logarithmic factor across a large collection of
parameter spaces while satisfying the same set of differential privacy constraints. Simulation
studiesandreal-worlddataapplicationsfurtherelucidatethetheoreticalanalysiswithnumerical
results.
1. Introduction
Massive and diverse datasets are now routinely collected across a wide range of scientific
fields,includinggenomics,neuroimaging,astrophysics,climatestudies,andsignalprocess-
ing. In many applications, alongside the primary data from the target study, additional
datasetsfromdifferentpopulationsorenvironmentswithsimilarstructureshavealsobeen
collected. Transfer learning, which aims to improve learning performance in a target do-
main by transferring knowledge from different but related source domains, has become
a vibrant and promising area of research in machine learning. This concept has found
applications in areas such as computer vision, speech recognition, and genre classification.
Alongside the transfer learning framework, another crucial consideration in modern
data science is the preservation of privacy. Sensitive data are often spread across various
sources, each presenting unique challenges in privacy preservation (see, e.g., Guo et al.
[2024]). Differential privacy (DP) has emerged as a leading framework for ensuring that
statisticalanalysisresultsdonotcompromisetheconfidentialityofindividualdatapoints.
4202
nuJ
82
]TS.htam[
1v88002.6042:viXra2 Auddy, Cai, and Chakraborty
Originally introduced by Dwork et al. [2006], DP has garnered significant academic at-
tention and has been embraced by industry leaders like Google, Microsoft, and Apple, as
well as governmental entities such as the US Census Bureau (Abowd [2016]).
Addressing the distributed nature of data collection and analysis is crucial due to
its implications for privacy preservation and collaboration. This raises a number of nat-
ural questions: firstly, when and how can inference under the target distribution be im-
proved by leveraging data points distributed across several sources with disparate privacy
constraints? Secondly, what are the fundamental limits of inference for the target with
privacy-constrained learning from the sources?
The two questions outlined above frequently arise in modern data analysis, spurring
a flurry of recent research. The answers to both questions fundamentally depend on the
specificinferencetaskathand.Onlyrecentlyhasrigorousresearchexploredthetheoretical
performance of transfer learning under differential privacy constraints, addressing both
various parametric problems (Li et al. [2024]) and nonparametric problems (Ma and Yang
[2023]).
In this paper, we focus on the task of binary nonparametric classification, also re-
ferred to as domain adaptation in the literature. Our objective is to achieve statistically
optimal transfer learning under distributed DP constraints. We tackle the challenge of
heterogeneous data sources with distinct distributions, examining how privacy parame-
ters, sample sizes, and data heterogeneity affect classification performance. Our proposed
classifiers are designed to adapt to unknown data heterogeneity and parameters, while
maintaining statistical optimality. This adaptability enhances performance even under
strict privacy constraints, balancing privacy preservation with classification accuracy. Be-
fore delving into further details, we outline some fundamental concepts of classification
with transfer learning.
In a classification problem from a single source, we observe independent copies of
a tuple (X,Y) from a distribution P where X ∈ Rd are the covariates, and Y ∈ {0,1}
denotes the binary class labels. To measure the efficacy of transferring information from
a source distribution P to a different target distribution Q, several methods have been
proposed to quantify the similarity of P and Q. Building upon the intuition that transfer
is easier if both P and Q have high masses in a common region, divergence measures
have been used on either the covariate space or as discrepancy between labels. See, e.g.,
Ben-David et al. [2010], Germain et al. [2013], Cortes et al. [2019], Sugiyama et al. [2007]
and references therein. While these are general metrics of measuring the difference be-
tween P and Q, such approaches often tend to be pessimistic, as shown by Kpotufe and
Martinet [2021]. Incorporating the structure of the classification problem leads to describ-
ing more specific transfer models such as covariate shift: where one posits a difference in
the marginal distribution of X from P to Q, but assumes the posterior probabilities of
P(Y = 1|X) to be the same for both P and Q. On the other hand, label shift assumes
that the class probability P(Y = 1) differs from P to Q, but the class-conditional covari-
ate distributions P(X|Y = 1) remains the same. For more details we refer the reader to
Kpotufe and Martinet [2021], Sugiyama et al. [2007] for the covariate shift, and to Garg
et al. [2020], Lipton et al. [2018], Maity et al. [2022] for the label shift paradigms. More
flexible transfer mechanisms have also been considered, see, e.g., Reeve et al. [2021], Fan
et al. [2023].Minimax Optimal Transfer Learning for Classification under Distributed Differential Privacy 3
Inthispaperwefocusontheposteriordriftmodel,wherethecovariateshavethesame
marginal distribution under both the source and target distributions, but the posterior
probabilities P(Y = 1|X) undergoes a shift from the source to the target. Posterior drift
has often been studied in the literature: see Cai and Wei [2021], Liu et al. [2020a], Maity
etal.[2024],Scott[2019]andreferencestherein.Theposteriordriftmodelnaturallyarises
in a range of applications where the data is distributed and there are privacy concerns.
Here are a few examples:
• Healthcare Monitoring Across Hospitals: In applications where multiple hos-
pitals contribute data for healthcare monitoring (Yeung [2019]) (e.g., patient vital
signs, medical histories), each hospital’s data is sensitive and subject to privacy reg-
ulations like HIPAA. As medical practices evolve and patient demographics change,
the underlying distribution of health data in each hospital’s domain may drift over
time. However, due to privacy concerns, the data cannot be aggregated into a cen-
tral repository for analysis (see for e.g., Ju et al. [2020] which proposes a privacy-
preserving federated transfer learning architecture for EEG classification, achieving
higher accuracy without data sharing.). Consequently, the transfer learning model
trained on data from one hospital may experience posterior drift when applied to
another hospital’s data, leading to degradation in performance over time.
• Financial Fraud Detection in Banking Networks: Banks collaborate to detect
financial fraud by sharing transaction data (Phua et al. [2010], Chan et al. [1999]),
but due to privacy regulations and competitive concerns, they cannot directly share
sensitivecustomerinformation.Overtime,patternsoffinancialfraudmayevolvedue
to changes in customer behavior, economic conditions, or fraud tactics. However,
because of customer privacy concerns, each bank must maintain control over its
own data, making it challenging to aggregate data for analysis. As a result, transfer
learning models used for fraud detection (Lebichot et al. [2020]) may experience
posterior drift as the data distributions in different banks’ domains shift over time.
• Social Media Analysis Across Platforms: Social media platforms collect user-
generated content and engagement metrics for analyzing trends, sentiment analysis,
and targeted advertising (Saura et al. [2019]). However, due to privacy regulations
and platform policies, individual user data cannot be shared openly between social
mediaplatforms.Asonlinecommunitiesevolveanduserbehaviorsshiftwithtrending
topics, viral content, and platform updates, the underlying distribution of social
media data in each platform’s domain may vary. Therefore, learning methods for
socialmediaanalysis(Wangetal.[2020])mayencounterposteriordriftwhenapplied
across platforms, while simultaneously requiring to protect sensitive user data.
Nonparametric classification in the posterior drift model has been considered pre-
viously by Cai and Wei [2021], where the minimax rate of misclassification risk without
the privacy constraints is established. The present work builds upon their framework,
and establishes the optimal rates for classification using data that are distributed across
servers with varying quality and different privacy constraints. Our results reveal interest-
ing phase transition phenomena and highlight the intricate trade-offs between preserving
privacy and achieving classification accuracy. We propose statistically optimal adaptive
procedures that effectively balances this trade-off between privacy and accuracy.4 Auddy, Cai, and Chakraborty
Suppose the source and target distributions P and Q have similar covariate distribu-
tions,andconsiderthefunctionsη (x) := P(Y = 1|X = x)underP,andη (x) := P(Y =
P Q
1|X = x)underQ.Further,supposethatbothP andQhavethesamedecisionboundary,
that is, (η (x)−1/2)(η (x)−1/2) ≥ 0 for all x ∈ Rd. In this framework, Cai and Wei
Q P
[2021] quantified the efficiency of transfer learning at Q using data from P through the
so-called relative signal exponent γ, where γ > 0 is a number such that
(cid:12) 1(cid:12) (cid:12) 1(cid:12)γ
(cid:12) (cid:12) (cid:12) (cid:12)
(cid:12)η P(x)− (cid:12) ≥ (cid:12)η Q(x)− (cid:12) .
(cid:12) 2(cid:12) (cid:12) 2(cid:12)
When γ < 1, the covariates are better separated into the two classes in the source distri-
bution P than in Q. In such a case, the P-data are especially useful for transfer learning.
The situation reverses when γ > 1. In this paper, we describe a kernel based classifier
that leverages the information from sources, while preserving data privacy in the DP
framework. For both the source and the target, we compute:
1 XnP (cid:18) 1(cid:19) (cid:18)X i−x(cid:19) 1 XnQ (cid:18) 1(cid:19) (cid:18)X i−x(cid:19)
T (x) = Y − K and T (x) = Y − K ,
P n hd i 2 h Q n hd i 2 h
P i=1 Q i=1
(1)
for a suitable kernel K(·) and bandwidth h. It can be checked that T (x) and T (x) are
P Q
pointwise consistent estimators of
(cid:0)
η (x)−
1(cid:1)
g (x) and
(cid:0)
η (x)−
1(cid:1)
g (x) respectively,
P 2 P Q 2 Q
where g (·) and g (·) are the joint densities of the covariates under P and Q respectively.
P Q
Thus the sign of a suitable linear combination of T (x) and T (x) defines a reasonable
P Q
transfer learning classifier.
Theaboveclassificationprocedureishowever,notdifferentiallyprivate.Nonparamet-
ric classifiers with local privacy have previously been considered by Berrett and Butucea
[2019], Ma and Yang [2023]. In this paper, we focus on “server-level” privacy, where each
server can access its own set of unperturbed data, but imposes privacy constraints when
sharing aggregated information with other servers. Due to its immense applicability in
practice, such a distributed privacy setting has recently received significant attention in
the literature: see e.g., Cai et al. [2023], Acharya et al. [2023], Liu et al. [2020b], Levy
et al. [2021].
To emphasize the impact of distributed data privacy, we consider a scenario with m
source servers from distribution P, each holding n samples. Similar to (1) we define the
P
(j)
estimator for each j-th source server, denoted as T (·). For attaining differential privacy,
P
it is standard to add external noise, with an important tradeoff in mind. The noise level
must be enough to ensure the (ε,δ)-differential privacy guarantees (see Definition 2.1) for
both the source and the target, but it has to be added prudently so as to not worsen the
classification accuracy too much. In our case, we output a noisy test function as follows.
WeusetheGaussianprocessbasedmechanismusedbyHalletal.[2013]toprivatizekernel
(j)
estimators. With T (·) and T (·) as defined above, we compute:
Q P
(j) (j) (j)
TeQ(x) = T Q(x)+σ Qξ Q(x) and Te
P
(x) = T
P
(x)+σ Pξ
P
(x) for j = 1,...,m,
where ξ (·) and ξ(j) (·) are Gaussian processes with covariance function K(·/h), while σ2
P Q P
and σ2 are suitably chosen noise variances that depend on the privacy parameters (ε,δ),
QMinimax Optimal Transfer Learning for Classification under Distributed Differential Privacy 5
sample sizes and the bandwidth h. Next, we choose an appropriate weight w ∈ [0,1], and
define the privatized transfer weighted estimator
 
1 Xm
(j)
Te(x) = wTeQ(x)+(1−w)
m
Te
P
(x).
j=1
Finally our classifier becomes:
fb(x) := 1(Te(x) ≥ 0). (2)
We use the standard notion of excess risk (see, e.g., Audibert and Tsybakov [2007]) to
evaluate the performance of our classifier. Let f∗(x) = 1(η (x) ≥ 1/2) be the Bayes
Q Q
classifier for the target distribution. Then the excess risk of a classifier under the target
distribution is defined as:
h i
E Q(fb) = E P Q(Y ̸= fb) −P Q(Y ̸= f Q∗).
We assume that the margin assumption (see Definition 2.5) holds with parameter α for
Q, and η (x) is (β,L )-Ho¨lder for some 0 < β < 1. Then if the target and the source are
Q β
both constrained to be (ε,δ) differentially private, we prove the following minimax rate:
inf supE Q(fb)
fe∈M(ε,δ)P,Q
≍ (cid:20) L
N
(cid:26)(cid:18) n Q−β 2( β1+ +α d) ∨(n2 Qε2)−β 2( β1 ++ 2α d)(cid:19) ^(cid:18) (mn P)−β 2( β1 γ+ +α d) ∨(mn2 Pε2)− 2β β(1 γ+ +α 2d)(cid:19)(cid:27) ∧1(cid:21) (3)
where L is of order at most polylog(mn +n ), the infimum is over all possible transfer
N P Q
learning classifiers satisfying (ε,δ)-DP, and the supremum is over all distributions in the
posterior drift framework. The minimax rate is attained by the classifier fbin (2).
The minimax rate is determined by a trade-off between four quantities: the non-
private rates for the source and the target, as well as their privatized counterparts. An
interesting phenomenon emerges through the relative effect of the efficacy of transfer and
the cost of privacy, characterized by the parameters γ and ε along with server size m
and sample sizes n ,n . This discrepancy essentially means that a higher privacy cost is
P Q
incurred when data is more distributed, see Figure 1. For a detailed discussion, refer to
the remarks following Theorem 3.1.
Figure 1 illustrates the minimax rate as described in Equation (3), showcasing the
different regimes of differentially private transfer learning excess risk. Several intriguing
phenomenaareobserved.Forinstance,acrossvaryingprivacybudgetsε,theregimeswhere
target and source servers dominate are not contiguous. Additionally, the phase transitions
and the order in which they appear depend delicately on how distributed the data is and
the quality of the source data. For a detailed account of all the phase transitions and their
intricate dependence on the problem parameters, see the discussion following Corollary
3.2.
Inpracticalapplications,westrivetoborrowinformationfrommultiplesourceswhich
are very heterogeneous with respect to both the classification transfer quality, in terms
of γ, and the privacy constraints. Take for example, the heart disease dataset of Detrano6 Auddy, Cai, and Chakraborty
m = 3 , n =160 , n =250 , g =1.4 m = 3 , n =160 , n =100 , g =3
P Q P Q
0 0
terms
source−non−pvt
−1 −1 source−pvt
target−non−pvt
target−pvt
−2 −2
0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00
e e
Fig. 1.Relationshipoflogarithmofexcessriskwithεasgivenby (4),thesmoothnesslevelβ =0.25
and dimension d=2.
et al. [1989], we want to predict the propensity of heart disease in a patient, based on
their demographic information and clinical measurements. The data is collected from four
different hospitals in Cleveland, Hungary, Long Beach, and Switzerland. The number of
patientsanddiseaseprevalencearewidelyvariedateachofthesefourhospitals.Moreover,
amodelfittedsolelyononeservershowsdrasticallydifferentperformanceonotherservers,
thus pointing to significant heterogeneity in the data. Details can be found in Section 6.2.
Atthesametime,itisreasonabletoassumeeachhospitalhavingitsownprivacytolerance
level, determined by local guidelines. To tackle these challenges, in Section 4, we develop
minimax optimal methods in the heterogeneous setting, where the trade-offs between
transfer learning and privacy are more nuanced.
In particular, we show in Section 4 that even when the source distributions are
heterogeneous, with their own transfer parameters γ and privacy constraints ε ,δ our
j j j
weightedkernelbasedclassifiercontinuestobeminimaxoptimal.Howeverthebestchoices
of weights w and bandwidth h depends heavily on the knowledge of γ , which is typically
j
unknown. The additional noise due to privacy compounds this issue even further, since
choosing a worse bandwidth potentially implies adding a higher amount of noise than is
required, thus unnecessarily worsening the performance. To alleviate this issue, we take
a data-adaptive approach to choose the best bandwidth from a grid of possible choices.
ThisisbasedonthepopularLepskimethodfine-tunedtooursettingtoaccommodatethe
additionalnoiseforprivacy.Giventheprivacyparameters(ε,δ),ouradaptationalgorithm
outputs a classifier that attains the minimax rate, modulo a O(polylog(n )) factor, where
∗
n isthesamplesizeofthedatapooledacrossallservers.DetailscanbefoundinSection5.
∗
Therestofthepaperisorganizedasfollows.InSection2,weprovidethebackground
and formulate our problem in detail. Section 3 presents the minimax rate of excess risk
for our problem across various specific cases, as well as the most general case. Section 4
introduces our kernel based classifier, derives its excess risk bounds, and states the mini-
max lower bound. Section 5 describes the data-driven adaptive procedure for bandwidth
and weight selection. We evaluate our proposed method and compare it to existing work
via several numerical experiments on simulated and real datasets in Section 6. The paper
)ksir
ssecxe(gol
)ksir
ssecxe(golMinimax Optimal Transfer Learning for Classification under Distributed Differential Privacy 7
concludes with a discussion on possible future work in Section 7. All proofs can be found
in the supplementary material Auddy et al. [2024].
2. Problem Formulation
In this section, we outline the general framework for transfer learning under distributed
privacy constraints. Our dataset is distributed across m+1 servers, indexed by the set
{0,1,...,m}. The dataset is categorized as target and source. On server 0 (also called
the target server), we have n i.i.d. samples from the distribution P , while on server j
0 0
(the source servers) for j ∈ 1,...,m, we have n i.i.d. samples from distribution P . All
j j
of the probability measures {P }m are defined on the measurable space (Z,Z). Let
j j=0
Z(0) = {Z(0) }n0 denote the n realizations from P on the target server. Let us denote
i i=1 0 0
by Z(j) = {Z(j) }nj the n realizations from P on the jth source server for j = 1,...,m.
i i=1 j j
These servers serve as the source data, and our goal is to learn the model for our target
distribution P .
0
For each source server i.e j = 1,...,m, we send a (randomized) transcript Te(j) based
on Z(j) to the target server 0, where the law of the transcript is given by a distribu-
tion conditional on Z(j), P(·|Z(j)), on a measurable space (T,T ). For j = 1,...,m the
transcript Te(j) has to satisfy a (ε j,δ j)-differential privacy constraint.
Definition 2.1. The transcript Te(j) is (ε j,δ j)-differentially private if for all A ∈ A
and z,z′ differing in one individual datum, it holds that
(cid:16) (cid:17) (cid:16) (cid:17)
P Te(j) ∈ A|Z(j) = z ≤ eεjP Te(j) ∈ A|Z(j) = z′ +δ j.
The target server can look at the private transcripts {Te(j)}m and the target data Z(0)
j=1
while constructing the final private transcript Te. Hence Te satisfies (ε 0,δ 0)-interactive
differential privacy constraint, which is defined as follows:
Definition 2.2. The transcript Te is (ε 0,δ 0)-differentially private if for all A ∈ A
and z,z′ differing in one individual datum and for all t ∈ T for j = 1,...,m, it holds
j
that
(cid:16) (cid:17)
P Te ∈ A|Z(0) = z,Te(j) = t
j
for 1 ≤ j ≤ m
(cid:16) (cid:17)
≤ eε0P Te ∈ A|Z(0) = z′,Te(j) = t
j
for 1 ≤ j ≤ m +δ 0.
This privacy constraint can be understood as follows: if we condition on the outcome of
all other servers then the distribution of the final private transcript Te does not change
much if one of the datum on the target server is changed.
Intransferlearning,thefocusisonscenarioswheremultipleparties,suchashospitals,
possess heterogeneous data with differing underlying distributions. Employing distributed
protocols in such contexts ensures differential privacy while yielding outputs from each
participating party. Within this framework, transcripts generated by each source server
rely solely on its local data, with no exchange of information occurring between source
servers.Communicationissolelybetweenthesourceandtargetservers.Eachofthesource8 Auddy, Cai, and Chakraborty
servertransmitsitstranscriptstothetargetserver.Thetargetserverutilizingallthetran-
scripts (Te(1),...,Te(m)) from the other servers and target data Z(0), computes the final
private transcript Te. This scenario often arises when multiple trials involving a popula-
tion similar to that of the target server are conducted, yet individual locations, such as
hospitals, opt against consolidating their original data due to privacy apprehensions.
In the context of transfer learning for nonparametric classification our data looks like
(j) (j) (j)
a couple Z := (X ,Y ), for i = 1,...,n ; j = 1,...,m for the source servers, and
i i i j
(0) (0) (0) (j)
Z := (X ,Y ), for i = 1,...,n for the target server. We assume that Z takes
i i i 0 i
values in Z := [0,1]d×{0,1}. We regard X ∈ [0,1]d as a vector of features corresponding
to an object and Y ∈ {0,1} as a label indicating that the object belongs to one of
two classes. Our goal is to propose distributed DP protocols Te(j) for each server and
construct classifier fb: [0,1]d → {0,1} based on the final private transcript {Te}. Unlike
the traditional federated learning framework, there’s no central server; alternatively, we
can consider the target server as acting in a central capacity. We denote the vector of
privacy budgets as (ε,δ) = {(ε j,δ j)}m
j=0
and the class of distributed DP classifiers fbby
M . Next we denote
ε,δ
η (X(j)) := P(Y(j) = 1|X(j)) for the source servers j = 1,...,m; and
j
η (X(0)) := P(Y(0) = 1|X(0)) for the target server,
0
as the (source and target) regression functions of Y on X. We denote the marginal dis-
tribution of X for the jth server, j = 0,...,m as PX. Define the classification error of a
j
classifier f under the target distribution P as
0
R (f) := P (Y ̸= f(X))
0 0
TheBayesdecisionruleisaminimizeroftheoftheriskR (f)whichhastheformf∗(X) =
0 0
1{η (X) ≥ 1/2}. The goal of transfer learning is to transfer the knowledge gained from
0
the source data together with the information in the target data to construct a classifier
which minimizes the excess risk on the target data
E 0(fb) = E[R 0(fb)]−R 0(f 0∗)
Under the posterior drift model we quantify the similarity between the regression
functions {η }m and η as follows:
j j=1 0
Definition 2.3 (Relative Signal Exponent (RSE)). TheclassΓ(γ,C )withrel-
γ
ative signal exponent γ = (γ ,...,γ ) ∈ Rm and constants C = (C ,...,C ) ∈ Rm, is
1 m + γ 1 m +
the set of distribution tuples (P ,P ,...,P ) that satisfy for 1 ≤ j ≤ m
0 1 m
(a) sign(cid:0) η (x)− 1(cid:1) = sign(cid:0) η (x)− 1(cid:1) for all 1 ≤ j ≤ m and all x ∈ [0,1]d.
j 2 0 2
(b) (cid:12) (cid:12)η j(x)− 1 2(cid:12) (cid:12) ≥ C j(cid:12) (cid:12)η 0(x)− 21(cid:12) (cid:12)γj for some γ j > 0, for all 1 ≤ j ≤ m and all x ∈ [0,1]d.
Remark 2.1. The first part follows from the assumption that the Bayes classifier
f∗ is the same for both source and target populations. The second part introduces a
parameter γ which controls the signal strength of the source data from P, with respect to
the target data from Q. See Definition 1 and Remark 1 of Cai and Wei [2021].Minimax Optimal Transfer Learning for Classification under Distributed Differential Privacy 9
InadditiontotheRSEassumptionwealsoneedtoassumesmoothnessofη andcharacter-
0
ize it behavior near 1/2. These assumptions are standard in nonparametric classification
and was first introduced in Audibert and Tsybakov [2007].
Definition 2.4 (Ho¨lder Smoothness). The regression function η belongs to the
0
H¨older class of functions denoted by Σ(β,L) (0 < β ≤ 1) which is defined as the set of
functions satisfying:
|η (x)−η (x′)| ≤ L∥x−x′∥β for x,x′ ∈ [0,1]d.
0 0
Definition 2.5 (Margin Assumption (MA)). The margin class M(α,C ) with
α
α ≥ 0 and C > 0 is defined as the set of distributions P such that
α 0
PX(0 ≤ |η (X)−1/2| ≤ t) ≤ C tα for all t > 0.
0 0 α
Another definition is about marginal density of X, PX for j = 1,...,m.
j
Definition 2.6 (Common Support and Strong Density Assumption (SD)).
We assume that PX for j = 0,...,m have the identical support on a compact (c ,r ) reg-
j µ µ
ular set A ⊂ [0,1]d and has a density g w.r.t. the Lebesgue measure bounded away from
j
zero and infinity on A:
g ≤ g (x) ≤ g for x ∈ A and g (x) = 0 otherwise,
min j max j
where c ,r > 0 and 0 < g < g < ∞ are fixed constants. We denote the set of
0 0 min max
marginal distributions (PX,...,PX) which satisfy the above constraints as S(µ,c ,r )
0 m µ µ
where µ = (g ,g ).
min max
Remark 2.2. In this paper we focus our attention to the case when the marginal
densities have regular support and are bounded from below and above on their support.
Moreover we assume that αβ ≤ d throughout the paper. This is because in the other
regime (αβ > d), there is no distribution PX such that the regression function η crosses
0 0
1/2 in the interior of the support of PX (Audibert and Tsybakov [2007]) and hence this
0
case only contains the trivial cases for classification.
We put all the definitions together to define the class of distributions we consider in
the posterior drift model as
Π(γ,C ,β,L,α,C ,µ,c ,r )
γ α µ µ
:= {(P ,P ,...,P ) : (P ,P ,...,P ) ∈ Γ(γ,C ),η ∈ Σ(β,L),
0 1 m 0 1 m γ 0
PX ∈ M(α,C ),(PX,PX,...,PX) ∈ S(µ,c ,r )}
0 α 0 1 m µ µ
For the rest of the paper we will use the shorthand Π(α,β,γ,µ) or Π if there is no
confusion.
3. Main Results
In this section, we present the key findings of our paper, where we establish the minimax
rate of convergence for transfer learning under differential privacy constraints, specifically
addressing the nonparametric classification problem.We divide our results into two sub-
sections: Section 3.1 covers the homogeneous case, while Section 3.2 addresses the general
heterogeneous case.10 Auddy, Cai, and Chakraborty
3.1. Minimax Rates under Source Homogeneity
To derive meaningful and interpretable insights from our minimax rate, we first exam-
ine the scenario where the source servers are exchangeable in terms of the distributed
classification problem under transfer learning and privacy constraints. This homogeneous
scenario is characterized by equal sample sizes (n = n), privacy parameters (ε = ε,
j j
δ = δ) and transfer exponents (γ = γ) for all j = 1,...,m.
j j
Theorem 3.1. Suppose n = n,ε = ε,δ = δ and γ = γ for all j = 1,...,m and
j j j j
assume that δ = o((nm)−1). Then the minimax rate for the excess risk satisfies
inf sup E 0(fb) ≍ (cid:20) L N(cid:26) (cid:18) n 02β1 +d ∧(n2 0ε2 0)2β+1 2d(cid:19)
fb∈M(ε,δ)(P0,...,Pm)∈Π
+(cid:16) (mn)2βγ1
+d
∧(mn2ε2)2βγ1 +2d(cid:17)(cid:27)−β(1+α) ∧1(cid:21)
β(1+α)
for a sequence L
N
of order at most (log((δ∧δ 0)−1))2β(γ∧1)+d.
Note that the minimax rate depends on the sum of quantities: the first of which
determines the minimax rate for the problem using only the target data, while the sec-
ond corresponds to the minimax rate for the problem using solely the source data from
the source servers. Some remarks are in order regarding the minimax rate obtained of
Theorem 3.1 in some special settings:
Remark 3.1 (Private classification with no transfer). Whenthenumber
of servers m = 0 or there is no source data, i.e., n = 0 for all source servers we obtain the
single server minimax classification rate under the (ε,δ)-DP constraint, given by
inf sup E 0(fb) ≍ (cid:20) L
N
(cid:18) n 02β1 +d ∧(n2 0ε2 0)2β+1 2d(cid:19)−β(1+α) ∧1(cid:21) .
fb∈M(ε,δ)(P0,...,Pm)∈Π
Remark 3.2 (Non-private transfer learning). Whentheprivacyrequirements
are not stringent, the tradeoff is completely characterized by a comparison between the
(cid:16) (cid:17) 1
non-private rates of the target and the source. In particular, if ε > md/2n−βγ 2βγ+d, we
find the non-private transfer learning rates
inf sup E 0(fb) ≍
(cid:16)
n
0+(mn)22 ββ γ+ +d d(cid:17)−β 2( β1+ +α d)
fb∈M(ε,δ)(P0,...,Pm)∈Π
whichcoincideswiththeresultsofCaiandWei[2021],inthetransferhomogeneousregime
with equal source sample sizes.
In the current setting, we further emphasize how the requirement of privacy leads to
a worsening of the rate if the same amount of data is distributed across a larger number
of servers. Since the sources are all equivalent in terms of data quality (as quantified by
γ), traditional knowledge suggests a rate depending on the pooled source sample size mn.
In the m-server non-private transfer learning setup of Cai and Wei [2021], that is indeedMinimax Optimal Transfer Learning for Classification under Distributed Differential Privacy 11
the case, as also shown by the non-private rate of (mn)− 2β β(1 γ+ +α 2d) appearing in Theorem 3.1.
Note however that the private source rate is given by
m2β β(1 γ+ +α 2d) (mnε)−β β(1 γ+ +α d)
, so that if
the pooled sample size mn remains fixed, the rate worsens as m, the number of servers,
increases. This phenomenon is reminiscent of the minimax rate behavior observed in Cai
et al. [2023] in the non-private setting.
In order to further our understanding about interplay between the transfer exponent
γ and privacy parameters we restrict our attention to the case where the target server
has same privacy budget, i.e., ε = ε, δ = δ and the number of target samples is be-
0 0
tween n and mn. Other sample size regimes can be described similarly. As is clear from
Theorem 3.1, the minimax rate of decay for the excess risk is given in this case by:
E 0(fb) ≍ (cid:20) L
N
(cid:26)(cid:18) n 0−β 2( β1+ +α d) ∨(n2 0ε2)−β 2( β1 ++ 2α d)(cid:19) ^(cid:18) (mn)−β 2( β1 γ+ +α d) ∨(mn2ε2)− 2β β(1 γ+ +α 2d)(cid:19)(cid:27) ∧1(cid:21) .
(4)
Figure 1 illustrates the different regimes showing which of the four terms on the right
hand side of (4) ends up determining the overall rate of excess risk.
We will refer to the four terms on the right hand side in (4) as the non-private target
rate(NP ),theprivatetargetrate(P ),thenon-privatesourcerate(NP ),andtheprivate
t t s
source rate (P ) respectively. Depending on the value of the common privacy parameter
s
ε and the transfer exponent γ, the overall rate will be determined by one of these four
rates, as demonstrated by the following table. Corollary 3.2 formally states the results of
Table 1 along with the endpoints of the various transfer and privacy regimes.
Table 1. Minimax rate of excess risk at different transfer and privacy regimes. See Corollary 3.2.
Privacy
ε∈(0,ε(1)] ε∈(ε(1),ε(2)] ε∈(ε(2),ε(3)] ε∈(ε(3),1]
Transfer
(
P if ε≤ε(11)
γ ∈(0,1] t NP
P if ε>ε(11) s
s
(
1 NP if ε≤ε(21)
γ ∈(1,γ(∗)] ( t NP
P s if ε≤ε(11) P s if ε>ε(21) s
P if ε>ε(11)
t
γ ∈(γ(∗),∞) NP
t
Corollary 3.2. Suppose n = n,γ = γ∀1 ≤ j ≤ m, n ≤ n ≤ mn, and equal
j j 0
privacy budget ε = ε, δ = δ∀0 ≤ j ≤ m . Further assume that δ = o((mn)−1). Then
j j
the minimax rate for the excess risk are as given in Table 1 with the various regimes
characterized by the following endpoints:
h i
(a) γ(∗) = 1 (2β+d)logmn −d .
2β logn0
√ − β (cid:16) (cid:17) 1
(b) ε(1) = ( mn)−1∧n−1; ε(2) = n 2β+d; ε(3) = md/2n−βγ 2βγ+d.
0 012 Auddy, Cai, and Chakraborty
h √ i 1
 ( mn)β+dn− 0(βγ+d) β(γ−1) if γ ̸= 1,
√
(c) ε(11) = ε(1) if γ = 1, n ≤ mn2,
ε(2)
if γ = 1,
n0
>
√
mn2.
0
√ βγ+d
(d) ε(21) = ( mn)−1n β+d .
0
As we increase the value of ε, we observe interesting phenomena characterized by
distinct phase transitions. Not surprisingly, the rates behave differently based on whether
γ is small — where the quality of the source data is relatively better than the target
data — versus when γ is larger. The two transition points in γ are at γ = 1 and at
h i
γ(∗) = 1 (2β+d)logmn −d . The different rates can be described based on ultra-high,
2β logn0
high, moderate, and low privacy regimes.
First, in the ultra-high privacy regime where 0 < ε ≤ ε(1), the privacy requirements
are so severe that no classifier has disappearing excess risk in this regime, and a random
guess is the best one can do. Next, we move to the high privacy regime of ε(1) < ε ≤ ε(2),
where the private rates of both the target and the source dominate over their non-private
counterparts. Another interesting phenomenon emerges based on whether γ is smaller
than or greater than one. If γ < 1 and the privacy requirement is high, the target private
rates appear for very small ε, followed by the source private rates. This is because when
γ < 1, the source data are of better quality and hence for sufficiently small ε, the noise
added to the target dominates over the noise for the sources. The pattern reverses when
γ > 1 where the source private rates appear first.
Further increasing ε, we arrive at the moderate privacy regime where ε(2) < ε ≤ ε(3).
For small γ, characterized by γ ≤ 1, the high quality source data points prove to be
particularly beneficial. This results in the non-private source rate dominating over the
non-private target rate, as well as the noises added for privatizing the source and the
target. In contrast, when γ is very large, in particular γ > γ(∗), the source data are
particularly poor and not useful, so that the target non-private rate therefore dominates
over all the other contenders. A more interesting picture emerges for moderate γ given
by 1 < γ ≤ γ(∗). In this regime of intermediate transfer and medial privacy, we find the
non-private target rate first, followed by private source rates.
Finally for ε > ε(3), the low privacy scenario emerges. Here the effect of extraneous
noiseforprivacyisnotatallsignificant.Wethereforeonlyfindthenon-privateratesbased
on the transfer efficacy. For γ ≤ γ(∗), the source data are more useful, resulting in NP
s
governing the overall rate. As expected, for γ > γ(∗) the relatively poorer source data do
notcontributetothetransfer.Consequentlythenon-privateratesfromthetargetbecomes
dominant, reflecting the diminished relevance of the source data in such scenarios.
In the rest of this subsection, we describe another specialized setting where we allow
oneofthesourceserverstobepublic.Todemonstratetheeffectofpubliclyavailabledata,
we take m = 2 sources, with one private and one public source server. The minimax rate
for excess risk is then given by the following corollary:
2βγ+d
Corollary 3.3. Suppose that γ = γ = γ, ε = ∞, ε = ε = ε and n > n 2β+d ,
1 2 1 0 2 2 0
δ ∨δ = o(n−1). Then the minimax rate for the excess risk satisfies the following.
0 2 2Minimax Optimal Transfer Learning for Classification under Distributed Differential Privacy 13
−β(1+α)
(a) If n
1
> n 2, inf fb∈M(ε,δ)sup (P0,...,Pm)∈ΠE 0(fb) ≍ n
1
2βγ+d.
2βγ+d
(b) If n 2β+d ≤ n ≤ n , then
0 1 2

−β(1+α) βγ+d
inf sup E 0(fb) ≍
 LL
NN
(n
n1
2
22 εβ 2γ )+ −d
2β β(1 γ+ +α 2d)
ii ff nε
−
2≤ 1nn 12−
2
β βγ
γ1
+
+n
d
d12β <γ+d
ε ≤ L Nn−
2
2ββ γγ +d
fb∈M(ε,δ)(P0,...,Pm)∈Π L
n−β 2( β1 γ+ +α d)
if
n− 2β βγ γ+ +d
d < ε ≤ 1.
N 2 2
2βγ+d
(c) If n ≤ n 2β+d ≤ n , then
1 0 2
inf sup E 0(fb)
fb∈M(ε,δ)(P0,...,Pm)∈Π
 −β(1+α)
L Nn
1
2βγ+d if ε ≤ n e−β
≍ (cid:20) L N(cid:26)(cid:18) n 02β1 +d ∧(n2 0ε2)2β+1 2d(cid:19) +(cid:18) n 22βγ1 +d ∧(n2 2ε2)2βγ1 +2d(cid:19)(cid:27)−β(1+α) ∧1(cid:21) otherwise,
1 γ β(1+α)
wheren
e
= n 02β+d∧n 22βγ+d.HereL
N
isasequenceoforderatmost(log((δ 0∧δ 2)−1))2β(γ∧1)+d.
The above corollary demonstrates three different rates based on n , the size of the
1
public source server, with respect to the private source and the target. When n is larger
1
than n , the public source has a significantly large number of samples, enough for the
2
non-private rates from this server to dominate over the rest. Next, when n is relatively
1
largewithrespecttothetargetsamplesize,butstillsmallerthantheprivatesourceserver,
we find three different rates. In the high privacy regime of very small ε, the availability
of public data ensures that we do not have to pay an additional price of private rates
by adding noise. Next, the private rates of the source appear: the reason being that the
private source has higher number of samples, and in this intermediate privacy regime,
even after adding additional noise, the private source turns out to be more useful than the
publicserver.Forevenlowerprivacy,theextraneousnoiseleveldiminishesfurtherandthe
non-private rate from source server 2 appears. The final case is when the public data size
n is relatively smaller than the sample size in the two privacy constrained servers. In the
1
regime of very high privacy, the public data is still effective, and enables one to avoid the
private rates. For moderate and low privacy, the tradeoff is characterized by the relative
signal strengths and privacy requirements of the private source and target servers. The
exact rates would be similar to Table 1 in this scenario.
3.2. Minimax Rates in General Setting
We now turn our attention to the general case where the sample sizes n , transfer expo-
j
nents γ , privacy parameters (ε ,δ ) are all allowed to vary for 0 ≤ j ≤ m. Our main
j j j
result, captured in Theorem 3.4, quantifies the rate. The homogeneous case described
earlier can be thought of as a special case of this vastly more general setting.14 Auddy, Cai, and Chakraborty
Theorem 3.4. Let r ∈ R be the solution to the following equation:
+
m
(n ∧n2ε2rd)r2β+d+X (n ∧n2ε2rd)r2βγj+d = 1 (5)
0 0 0 j j j
j=1
The minimax rate for excess risk is given by
(cid:16) (cid:17)
inf sup E 0(fb) ≍ L Nrβ(1+α)∧1 . (6)
fb∈M(ε,δ)(P0,...,Pm)∈Π(α,β,γ,µ)
whenever P jn jδ
j
→ 0, for a sequence L
N
of order at most (−log(δ min))2ββ γ( m1+ inα +) d.
It is important to note that (5) always yields a positive solution, since the left-hand side
of (5) is a strictly increasing continuous function of r, ranging from 0 to ∞ as r varies
from 0 to ∞. Thus the function of r defined on the left side of (5) must take the value
1 somewhere in R . We now provide a brief commentary on the derived result, starting
+
with a comparison with non-private transfer learning.
Remark 3.3 (General non-private transfer learning). When the privacy
budget is large, setting ε = ∞ for j = 1,...,m in (5) we recover the non-private rate
j
 −β(1+α)
m 2β+d 2β+d
inf sup E 0(fb) ≍ n 0+X n j2βγj+d 
fb∈M(ε,δ)(P0,...,Pm)∈Π(α,β,γ,µ) j=1
when m is fixed. This coincides with Theorems 5 and 6 of Cai and Wei [2021]. Note
however that Theorem 3.4 allows the number of servers, m, to grow to ∞.
In the most general case, the minimax optimal rate of convergence exhibited in the
transfer learning problem under distributed privacy is determined by r, which has im-
plicit dependencies on various factors, including the number of servers, privacy parame-
ters, transfer exponents, and sample sizes. The value of r determines the radius within
which local methods can share information. In Section 4.1 we find that when using kernel
estimators, the optimal bandwidth choice is innately connected to r, and in fact differs
from the solution to (5) by at most a logarithmic factor. To further interpret the role of
r, note that (5) quantifies the exact contribution of each server to the entire classification
procedure. For each j ∈ {0,...,m}, the variance of the local estimator for the jth server
is determined by the sum of two quantities: the inverse of the sample size, and the vari-
ance of the additional noise required for privacy. The term n ∧n2ε2rd then appears as a
j j j
quantity proportional to the inverse of this variance and determines the precision of the
jth server.
4. Minimax Optimal Classification Procedure
In this section, we propose an optimal classifier and establish the minimax rate of con-
vergence. In the first subsection, we develop a nonparametric classifier for the target
population that appropriately utilizes information from the sources while satisfying pri-
vacy requirements for each server. In the second subsection, we prove a minimax lower
bound, demonstrating that our classifier achieves the minimax rate of convergence in the
distributed private transfer learning context.Minimax Optimal Transfer Learning for Classification under Distributed Differential Privacy 15
4.1. Classifier
We now describe a classifier for transfer learning with distributed privacy. Our method
has three main steps. First, we use a kernel estimator to estimate (η (x) − 1)g(x) for
j 2
j = 0,1,...,m. Second, then use a convex combination of these estimators, where the
weightsaredesignedtoborrowstrengthfromthesourceserversunderthetransferlearning
setup.ThethirdstepistoaddaGaussiannoisetotheweightedkernelestimatortosatisfy
privacy requirements. Our classifier is given by the sign of the noise perturbed weighted
estimator.
Consider a kernel K(t) supported on [−1,1]d with the following properties:
R
(a) K(t)dt = 1.
(b) K(·) is L -Lipschitz.
K
(c) max K(t) ≤ c .
K
t∈[−1,1]d
(d) min{K(t) : ∥t∥ ≤ 1/2} ≥ b .
K
(e) K is positive definite.
Let X = x be the test point we wish to classify. Suppose first that x ∈ [0,1]d. Then for
0 0
the source samples we compute
T(j)
(x ) :=
1 Xnj (cid:18)
Y(j)
−
1(cid:19)
K
X i(j) −x 0!
(7)
h 0 n hd i 2 h
j i=1
for 0 ≤ j ≤ m. To satisfy privacy requirements we follow the framework of Hall et al.
(j)
[2013] and a Gaussian process to the above estimator. Note that T belong to the RKHS
h
K given by linear combination {P θ K((X − x)/h) : θ ∈ R}. For two functions f =
i i i i
P θ K((X −x)/h) and g = P τ K((X −x)/h), their inner product under K is given by
i i i i i i
XX (cid:18)X i−X j(cid:19)
⟨f,g⟩ = θ τ K .
K i j
h
i j
(j)′ (j) (j) (j) (j)′ (j)′
LetT betheversionsofT with(X ,Y )replacedby(X ,Y )forj = 0,1,...,m.
h h 1 1 1 1
(j) (j)′
Then the RKHS norm of T (·)−T (·) can be bounded by
h h
√ √
c c
(j) (j)′ K (0) (0)′ K
∥T −T ∥ ≤ and ∥T −T ∥ ≤ . (8)
h h K n hd h h K n hd
Pj Q
Let us define (m+1) independent mean zero Gaussian processes ξ(j)(·) with covariance
kernels
(cid:18)s−t(cid:19)
Cov(ξj(s),ξ(j)(t)) = K for s,t ∈ [0,1].
h
and
q
2c log(2/δ )
K j
ξeh(·) =
n ε hd
ξ(j)(·) for j = 0,1,...,m. (9)
j j16 Auddy, Cai, and Chakraborty
We then release
n
(j) (j)
om
T
h
(x 0)+ξe
h
(x 0) . (10)
j=0
The next proposition asserts that the transcripts described above matches the required
distributed privacy requirements, and its proof follows by results from Hall et al. [2013].
Proposition 4.1. For any h ∈ [0,1] the transcripts {T h(j) (x 0) + ξe h(j) (x 0) : 0 ≤
j ≤ m} described above satisfies (ε ,δ ) differential privacy distributed across servers
j j
j ∈ {0,1...,m}.
The optimal bandwidth choice h is given by the solution to (5). To account for the
opt
additional δ factor for approximate privacy, we now define h which is the solution to:
opt,δ
(n
∧n2ε2rd)r2β+d+Xm
(n ∧n2ε2rd)r2βγj+d =
log(cid:18) 2 (cid:19)
(11)
0 0 0 j j j δ
j=1 min
Let us now define the weights
v = (n ∧n2ε2hd )hγjβ for j = 0,...,m
j j j j opt,δ opt,δ
v
j
u = for j = 0,...,m. (12)
j Pm v
j=0 P
(j) (j)
Withtheseweights,thetargetservercomputesaweightedaverageof{T
h
(x 0)+ξe
h
(x 0) :
0 ≤ j ≤ m} as follows:
m
Teh(x 0) := u 0(cid:16) T h(0) (x 0)+ξe(0)(x 0)(cid:17) +X u j(cid:16) T h(j) (x 0)+ξe(j)(x 0)(cid:17) . (13)
j=1
Finally our classifier is given by
fb(x 0) := 1(Tehopt,δ(x 0) ≥ 0) (14)
where h is the solution to (11). The following theorem provides an upper bound for
opt,δ
the excess risk of this classifier.
Theorem 4.2. Let r be the solution to (5). Let fb be the classifier defined in (14)
based on the weighted kernel estimator (13). Then,
(cid:18) (cid:18) 1 (cid:19)(cid:19) β(1+α)
sup E 0(fb) ≤ C ∗rβ(1+α) log 2βγmin+d
δ
(P0,...,Pm)∈Π(α,β,γ,µ) min
where C is a constant depending on L,d,α,β,γ , while γ = min{1,γ ,...,γ } and
∗ j min 1 m
δ = min{δ ,...,δ }.
min 0 mMinimax Optimal Transfer Learning for Classification under Distributed Differential Privacy 17
4.2. Minimax Lower Bounds
The above theorem bounds the error rate of our kernel based classifier. Alongside the
upper bound above, in this subsection we derive the minimax lower bound on the excess
risk, to establish that our kernel based classifier is minimax optimal up to logarithmic
factors.
We introduce a general data processing inequality which extends the findings pre-
sented in Cai et al. [2023]. This new result provides a bound on the total variation (TV)
distance between the push forward measures of the transcripts PTe and PTe, utilizing the
σ σ′
TV distance between their underlying distributions. Such an inequality may be of inde-
pendent interest beyond the current setting due to its broader applicability.
Lemma 1. For any subset S ⊆ {0,...,m}, the TV distance is bounded as follows:
TV(cid:16) PT σe,PT σe ′(cid:17) ≤ √ 2s X ε¯ j(eε¯j −1)+ X n jKL(P j,σ,P j,σ′)+4X eε¯jn jδ jρ j, (15)
j∈S j∈Sc j∈S
where ε¯ = 6n ε ρ and ρ = TV(P ,P ).
j j j j j j,σ j,σ′
A notable aspect of this lemma, in contrast to previous results in Cai et al. [2023], is
the inclusion of interactions between the target and source servers. Consequently, all the
information about the transcripts from the source servers is encapsulated within the final
transcript Te computed by the target server. This approach allows for heterogeneity not
only in sample sizes and privacy budgets but also in the data distributions across different
servers.
In constructing the lower bound, we formulate a family of distributions P for
j,σ
each j = 1,...,m, where σ is a vector of {+1,−1}M and M is suitably chosen. We
apply Assouad’s Lemma to bound the total variation distance between the push forward
measures of the transcripts PTe and PTe for σ and σ′ differing in one entry, as stipulated
σ σ′
by Lemma 1. The rest of the construction of the lower bound crucially depends on the
selection of the set S. This choice corresponds to the index of servers where the privacy
cost significantly outweighs the non-private risk. The major contributions to the upper
bound for the TV distance are ε¯ j(eε¯j −1) from the privacy-stringent servers, and the
KL divergence for the samples on non-stringent servers, n KL(P ,P ). The optimal
j j,σ j,σ′
balance of contributions is achieved when S = {j : ε ≤ (rdn )−1/2}.
j j
The following theorem establishes the fundamental cost of privacy for the nonpara-
metric classification problem in the distributed privacy setting.
Theorem 4.3. Suppose δ ’s are such that P n δ = o(1), then there exists a c > 0
j j j j
not depending on n for j = 0,...,m such that
j
inf sup E 0(fb) ≥ crβ(1+α)
fb∈M(ε,δ)(P0,...,Pm)∈Π
where r is the solution to (5).
A comparison with the upper bound on the excess risk obtained from Theorem 4.2 now
establishestherateoptimality,uptoalogarithmicfactor,ofourkernel-baseddifferentially
private distributed classifier.18 Auddy, Cai, and Chakraborty
5. Data-driven Adaptive Classifier
In practice, the smoothness and transfer exponent parameters are unknown, making it
challenging to select the correct bandwidth h. To address this, we will use an estimator
based on the Lepski method to choose h from a grid of possible values. While choosing
the exact optimal h is infeasible, this method adapts to the unknown parameters β and
γ ,...,γ .
1 m
This section is divided into two subsections. First, we consider the transfer homoge-
neous case, where the source populations have the same relative transfer exponent γ = γ
j
for j = 1,...,m with respect to the target. Here we allow m to grow at an appropriately
slow rate as n increases. In the second subsection, we address the general, heterogeneous
case with transfer exponents (γ ,...,γ ) with the important restriction that the number
1 m
of sources m is finite.
To choose the best candidate bandwidth, we define a grid of possible choices for h
as:
m
H = {2−j : j = 0,1,...,(logn )/d}, where n = X n ∧n2ε2.
∗ ∗ j j j
j=0
Let ∆m = {w : w ∈ [0,1],Pm w = 1} denote the m-dimensional simplex. For a weight
i i=0 i
vector w = (w ,w ,...,w ) ∈ ∆m we define
0 1 m
q
m m 2c log(2|H|/δ )|H|
Te(x 0,h,w) := X w jT h(j) (x 0)+X w
j
K
n ε hd
j ξ(j)(x 0) for h,w ∈ [0,1] (16)
j=0 j=0 j j
whereT(j) (·)isasdefinedin (7)andξ(j)(·)areindependentmeanzeroGaussianprocesses
h
with covariance kernel K(·/h).
5.1. Adaptation under Source Homogeneity
In this subsection we consider the sources to have transfer homogeneity, i.e., every source
has identical transfer exponent γ = γ for j = 1,...,m. It is then intuitive to weigh the
j
estimators T(j) and the noise ξ(j) with the weights proportional to n ∧ n2ε2hd for the
h j j j
sources. More specifically, this gives the restricted set of weights:
(cid:26) (1−w )u (h) (cid:27)
0 j
W(h) := (w ,w ,...,w ) : w ∈ [0,1], w = for j = 1,...,m (17)
0 1 m 0 j Pm u (h)
j=1 j
where u (h) := n ∧n2(ε /|H|)2hd. Note that the privacy requirements dictate that W
j j j j
depends on h. The rationale behind this set of weights comes from (12) restricted to
the transfer homogeneous setting. We determine the deviation of Te(x 0,h,w) around its
expectation through
!
c g 2c2 log(2|H|/δ )
v (h,w) = w2 K max + K 0
0 0 3n hd n2(ε /|H|)2h2d
0 0 0
 
+(1−w 0)2
Xm (P( mu j( uh) () h2 ))2(cid:18)c
3K
ng
m ha dx +
2 nc2
K
2(εlog /( |H2|H |)2| h/δ 2dj)(cid:19)
. (18)
j=1 j=1 j j j jMinimax Optimal Transfer Learning for Classification under Distributed Differential Privacy 19
The above metric measures the scale of noise present in Te(x 0,h,w). We therefore define
the signal to noise ratio as
(Te(x 0,h,w))2
ρ (h) = max . (19)
b0
w∈W(h) v 0(h,w)
Then the adaptive choice of h in the transfer homogeneous setting is given by

min{h ∈ H : ρ b0(h) > 4.5log(2n ∗|H|)} if maxρ b0(h) > 4.5log(2n ∗|H|)
h = h∈H
0
argmax ρ (h) otherwise.
hb0
Define
(Te(x 0,h 0,w))2
w = argmax .
∗(0) w∈W(h0) v (h ,w)
0 0
The adaptive classifier is now defined as
fb0(x) := 1(Te(x,h 0,w ∗(0)) > 0). (20)
Algorithm 1 summarizes the steps of the adaptive procedure.
Algorithm 1 Data-adaptive mechanism for bandwidth selection under Source Homo-
geneity
1: Input: test point: x ∈ [0,1]d; data: {(X(j) ,Y(j) ) : 1 ≤ i ≤ n ,j ∈ {0,1,...,m}};
0 i i j
privacy parameters: {(ε ,δ ) : j ∈ {0,1,...,m}}; kernel: K(·).
j j
2: Compute the grid of bandwidths: H = {2−j : j = 0,1,...,⌊log(n )/d⌋}.
∗
3: for h in H do
4: Compute the set of weights W(h) following (17).
5: Compute the signal-to-noise ratio:
(Te(x 0,h,w))2
ρ (h) = max
b0
w∈W(h) v 0(h,w)
where Te(x 0,h,w) and v 0(h,w) are as defined in (16) and (18) respectively.
6: Choose the bandwidth as

min{h ∈ H : ρ b0(h) > 4.5log(2n ∗|H|)} if maxρ b0(h) > 4.5log(2n ∗|H|)
h = h∈H
0
argmax ρ (h) otherwise.
h∈Hb0
7: Choose the best weight as
(Te(x 0,h 0,w))2
w = argmax .
∗(0) w∈W(h0) v (h ,w)
0 0
8: Output: fb0(x) := 1(Te(x,h 0,w 0) > 0).
The following theorem states the excess risk of the adaptive classifier in terms of the
regression function parameter α,β, the transfer exponent γ and the privacy constraints.20 Auddy, Cai, and Chakraborty
Theorem 5.1. Let r be the solution to (5) with γ
j
= γ for j = 1,...,m. Let fb0 be
the data adaptive classifier defined in (20). Then,
(cid:20) (cid:21)
sup E 0(fb0) ≤ C ∗′rβ(1+α) (log(n ∗|H|)log(2|H|/δ min))2ββ (( 11 ∧+ γα )+) d ∨|H|2β(1 d+α)
(P0,...,Pm)∈Π(α,β,γ,µ)
where C′ is a constant depending on m,L,d,α,β,γ, while δ = min{δ ,...,δ }.
∗ min 0 m
It is instructive to compare the above rate with the rate from Theorem 4.2. Since
|H| = polylog(n ) it follows that the excess risk of the adaptive estimator is worse by
∗
a multiplicative factor of polylog(n )). This is due to two reasons. Firstly, a factor of
∗
logn is expected as a cost of adaptation, and can be found in the transfer learning
∗
classification setup considered in Cai and Wei [2021]. Secondly, to ensure that the (ε ,δ )
j j
privacy requirements are satisfied throughout the adaptation procedure, we require that
for every h ∈ H our estimators are (ε /|H|,δ /|H|) differentially private. This contributes
j j
an extra factor of |H| to the rate obtained in Theorem 5.1.
An important special case of the above theorem is the server homogeneous case,
where sample sizes and the privacy parameters are the same for every server, i.e., n = n,
j
ε = ε and δ = δ for j = 0,1,...,m. The following corollary describes this special case.
j j
Corollary 5.2. Let r be the solution to (5) with n = n,ε = ε,δ = δ and γ = γ
j j j j
for all j = 1,...,m. Let fb0 be the data adaptive classifier defined in (20). Then,
sup E 0(fb0) ≤ C ∗′(cid:20) L( Nada)(cid:26) (cid:18) n 02β1 +d ∧(n2 0ε2 0)2β+1 2d(cid:19)
(P0,...,Pm)∈Π(α,β,γ,µ)
+(cid:16) (mn)2βγ1
+d
∧(mn2ε2)2βγ1 +2d(cid:17)(cid:27)−β(1+α) ∧1(cid:21)
(cid:20) (cid:21)
where L( Nada) is given by (log(n ∗|H|)log(2|H|/δ))2ββ (( 11 ∧+ γα )+) d ∨|H|2β(1 d+α) , and C ∗′ is a con-
stant depending on m,L,d,α,β,γ.
A comparison with Theorem 3.1 shows that this rate is minimax optimal up to a
factor polynomial in logarithmic terms.
5.2. General Adaptation for Multiple Sources
We now shift to the general setting where we no longer constrain γ ,...,γ to be all
1 m
equal. Note that for optimal estimation (as in Theorem 4.2), one requires knowledge of
potentially m many different parameters γ ,...,γ . The adaptation procedure therefore
1 m
requires optimizing over all possible weights in w ∈ ∆m. When m increases with n, the
adaptation to this growing number of parameters necessarily worsens the rate of decay for
the excess risk. We will not delve further into issue and focus instead on the case where
m is finite and does not increase with n.
In this general case we must consider all possible weight vectors w ∈ ∆m. As in (18)
earlier we compute an approximate variance of Te(x 0,h,w) as
v(h,w) =
Xm
w2
c Kg
max +
2c2 Klog(2|H|/δ j)!
. (21)
j 3n hd n2(ε /|H|)2h2d
j=0 j j jMinimax Optimal Transfer Learning for Classification under Distributed Differential Privacy 21
Let us define the signal-to-noise ratio index ρ(h):
b
(Te(x 0,h,w))2
ρ(h) = max ,
b
w∈∆m v(h,w)
In this general setting, the adaptive choice of h is given by

min{h ∈ H : r b(h) > C ∗log(n ∗|H|(m+1))} if maxρ b(h) > C ∗log(n ∗|H|(m+1))
h = h∈H
∗
argmax ρ(h) otherwise.
h b
where C = 2.25(m+1). Defining
∗
(T(x ,h ,w))2
0 ∗
w = argmax .
∗ w∈∆m v(h ,w)
∗
we obtain the adaptive classifier
fba(x) := 1(T(x,h ∗,w ∗) > 0). (22)
Similar to Algorithm 1 we have Algorithm 2 which presents the adaptive procedure al-
lowing for possible heterogeneity among servers.
Algorithm 2 Data-adaptive mechanism for bandwidth selection in the general case
1: Input: test point: x ∈ [0,1]d; data: {(X(j) ,Y(j) ) : 1 ≤ i ≤ n ,j ∈ {0,1,...,m}};
0 i i j
privacy parameters: {(ε ,δ ) : j ∈ {0,1,...,m}}; kernel: K(·).
j j
2: Compute the grid of bandwidths: H = {2−j : j = 0,1,...,⌊log(n )/d⌋}.
∗
3: for h in H do
4: Compute the signal-to-noise ratio:
(Te(x 0,h,w))2
ρ(h) = max
b
w∈∆m v(h,w)
where Te(x 0,h,w) and v(h,w) are as defined in (16) and (21) respectively.
5: For C = 2.25(m+1) choose the bandwidth as
∗

min{h ∈ H : ρ b(h) > C ∗log(2n ∗|H|)} if maxρ b(h) > C ∗log(2n ∗|H|)
h = h∈H
∗
argmax ρ(h) otherwise.
h∈Hb
6: Choose the best weight as
(Te(x 0,h 0,w))2
w = argmax .
∗ w∈∆m v(h ,w)
0
7: Output: fba(x) := 1(Te(x,h ∗,w ∗) > 0).
The following theorem verifies the efficacy of the general adaptive procedure.22 Auddy, Cai, and Chakraborty
Theorem 5.3. Let r be the solution to (5). Let fba be the data adaptive classifier
defined in (22). Then,
(cid:20) (cid:21)
sup E 0(fb) ≤ C ∗′rβ(1+α) (log(n ∗|H|)log(2|H|/δ min))2ββ γ( m1+ inα +) d ∨|H|2β(1 d+α)
(P0,...,Pm)∈Π(α,β,γ,µ)
where C′ is a constant depending on m,L,d,α,β,γ , while γ = min{1,γ ,...,γ } and
∗ j min 1 m
δ = min{δ ,...,δ }.
min 0 m
Remark 5.1. Oftentimes users might find it helpful to restrict the set of weights
based on prior knowledge. For example, under source homogeneity we allowed the re-
stricted set W(h). Similarly it is possible to use ad-hoc choices of weights w, based for
example, on the sample proportions for each server. Our algorithm 2 automatically allows
these specific choices of weights and can be used to select the bandwidth h to be used
with the classifiers weighted with respect to w.
6. Numerical Studies
The data-driven classifier proposed in this paper is easy to implement. In this section,
we examine the numerical performance of our methods through various simulated and
real data experiments. In the first subsection, we conduct simulation studies to compare
our proposed classifier with alternative methods across different parameter settings and
varying levels of problem difficulty. In the second subsection, we evaluate the performance
of the proposed classifier on a real dataset, verifying its practical effectiveness.
6.1. Simulation Study
We compare the prediction accuracy of our classifier with other existing methods in the
literature across various parameter settings.
6.1.1. Simulation Design
In our simulation study, we consider a setup with m source servers and a single target
server. All source servers are assumed to have the same data quality. Additionally, for
most of the simulation setups we assume that both the source and target servers have an
equal number of observations, denoted by n. The privacy budget for each server is given
by (ε,δ), where we set δ = n−2.
The data for both target and source servers is generated as follows: the marginal
distributionforX onboththetargetandsourceserversissupportedonatwo-dimensional
cube, [0,1]2, with a uniform distribution over its support.
The conditional distribution for the target server, given X, is defined as:
1 (cid:18)(cid:18) 1(cid:19)(cid:18) 1(cid:19)(cid:19)(cid:12) 1(cid:12)1 (cid:12) 1(cid:12)1!
(cid:12) (cid:12)4 (cid:12) (cid:12)4
P T(Y = 1|X) = η T(X) = 1∧ +sign x 1− x 2− (cid:12)x 1− (cid:12) (cid:12)x 2− (cid:12)
2 2 2 (cid:12) 2(cid:12) (cid:12) 2(cid:12)
+Minimax Optimal Transfer Learning for Classification under Distributed Differential Privacy 23
where for a ∈ R we write a = max{a,0}. For the source servers, the conditional distri-
+
bution is defined as:
(cid:18)1 (cid:18) 1(cid:19)(cid:12) 1(cid:12)γ(cid:19)
(cid:12) (cid:12)
P S(Y = 1|X) = 1∧ +sign η T(X)− (cid:12)η T(X)− (cid:12) .
2 2 (cid:12) 2(cid:12)
+
The proposed construction for these regression functions satisfies key assumptions,
such as smoothness and margin conditions. For a visual representation of these regression
functions, see Figure 2.
h ( x) h ( x) , g =0.5 h ( x) , g =1.5
T S S
1.00 1.00 1.00
0.75 0.75 0.75 h ( x)
1.00
0.75
0.50 0.50 0.50
0.50
0.25
0.25 0.25 0.25
0.00
0.00 0.00 0.00
0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00
Fig. 2. Regression functions η and η for γ ∈{0.5,1.5}.
T S
We would be comparing the following methods:
• Distributed Transfer Learning with kernels (DTK) is the proposed method
with two different choices of kernel, the gaussian kernel and the triangular kernel.
We select the best tuning parameters h the bandwidth and, w ∈ [0,1] the relative
weight of the target data w.r.t the source data. The parameter grid for those are
n o
w ∈ i : i ∈ {0,...,100} and h ∈ {2−i : i ∈ {1,...,7}}.
100
• Distributed Transfer Learning using histogram (DT-HIST) is a variant of
the method proposed in Berrett and Butucea [2019] adapted to the transfer learning
setting with distributed privacy. This method also has the the two parameters w
and h, where h denotes the histogram bin size for this estimator. We vary both the
parameters on the same grid as DTK.
• Adaptive Distributed Learning with kernels (AdaptDTK) This is the adap-
tive version of the DTK classifier, proposed in Section 5, which automatically tunes
the hyper-parameters h and w based on a Lepski-style method.
6.1.2. Effect of Source Data
In this section, we examine the impact of source data on classification performance. We
compare our transfer learning approach to a method that relies solely on target data. The
latter method, referred to as targetDTK, serves as our baseline for comparison.24 Auddy, Cai, and Chakraborty
To assess the utility of source data, we plot the best accuracy obtained from each
method across a range of hyperparameters, as detailed in Section 6.1.1. This analysis
providesinsightintothebenefitsofincorporatingsourcedatainourclassificationprocess.
In Figure 3, we contrast our transfer learning method with a naive approach that doesn’t
gamma: 0.5 gamma: 1 gamma: 1.5
0.90
Kernel
gaussian_kernel
0.88
triangular_kernel
Method
DTK
0.86
targetDTK
4 8 12 16 4 8 12 16 4 8 12 16
e
Fig. 3. Effect of source data against ε for γ ∈{0.5,1,1.5}
leverage source data. To ensure fairness, we limit the scenario to just one additional
source server (m = 1). Introducing multiple source servers would widen the performance
gap between transfer learning methods and non-transfer learning methods. Also we set
the number of observations on the source and target server to be n = 100, the reason for
choosing a comparatively smaller n is for better visualization purposes, for larger n we
expect the same phenomena , although the gain in accuracy would be difficult to perceive
visually.
Within this setup, we vary two problem parameters, ε and γ. Across various kernel
choices and values of ε and γ, our transfer learning method consistently outperforms the
alternatives. As privacy constraints relax (i.e., ε increases), performance improves across
all methods. Notably, as γ increases, the performance advantage gained from using source
servers diminishes. This phenomenon is expected: with higher γ, the quality of source
data decreases, consequently reducing the performance gain. Since it is always beneficial
to use source data we do not compare the target only method in the further sections.
6.1.3. Comparison of Our Adaptive Classifiers with Other Methods
In this section we compare our adaptive classifier AdaptDTK to non-adaptive methods
like DTK and DT-HIST across different ε, γ and m values. It is important to note that
both non-adaptive methods are tuned with optimal hyperparameters based on test data,
resulting in a potential performance advantage over the adaptive method.
However, our results show that the “cost of adaptation”—the difference in accuracy
between the adaptive method and the non-adaptive methods—is relatively small. Despite
ycaruccAMinimax Optimal Transfer Learning for Classification under Distributed Differential Privacy 25
the adaptive method’s lack of knowledge about oracle hyper-parameters, its performance
is only marginally lower compared to methods with oracle tuning. This indicates that our
adaptive approach is effective, even without precise hyperparameter tuning.
gamma: 0.5 gamma: 1 gamma: 1.5
0.92
Kernel
gaussian_kernel
0.88
None
triangular_kernel
0.84
Method
AdaptDTK
0.80 DT−HIST
DTK
0.76
4 8 12 16 4 8 12 16 4 8 12 16
e
Fig. 4. Accuracy v/s ε for γ ∈{0.5,1,1.5}
Effect of Privacy Budget: Here, we maintain the same experimental setup as described
in Section 6.1.2 but with larger n = 500. Our kernel-based method, AdaptDTK, along
with DTK (with oracle hyper-parameter tuning), consistently outperforms DT-HIST.
Particularly in scenarios with low privacy budgets, the performance gap is significant, in-
dicating that privatizing kernel-based methods empirically yields better results compared
to privatizing histogram bin count-based methods.
Regarding the choice of kernel, we note that the triangular kernel demonstrates ex-
ceptional adaptability, showing only minimal performance degradation compared to its
oracle-tuned counterpart. This difference in performance is likely attributed to the fact
that our theoretical adaptive procedure was developed with bounded kernels (like trian-
gular kernels) in mind.
Effect of distributed data: In our experimental setup, we maintain a constant total
sample size of 500 across both target and source servers. By varying the number of source
servers m from 1 to 20, we delve into the impact of data distribution under a fixed total
sample size. Additionally, we explore how the source data quality, represented by γ, and
the privacy budget, ε, influence the outcomes. Across all methods, a consistent pattern
emerges: as data becomes more distributed, accuracy declines. This observation aligns
with our theoretical findings. A notable trend is that performance degradation is more
markedathigherγ values.Thissuggestsadisproportionatelynegativeimpactonaccuracy
when the data is more fragmented, especially if the data quality is poorer.
The cost of adaptation also presents intriguing behavior. We note that the perfor-
mancegapbetweenadaptiveandnon-adaptivemethodswidenswithincreasingm,hinting
ycaruccA26 Auddy, Cai, and Chakraborty
gamma: 0.25 gamma: 1 gamma: 4
0.9
0.8
Kernel
0.7
gaussian_kernel
0.6 None
triangular_kernel
0.5
0.9 Method
AdaptDTK
0.8
DT−HIST
0.7 DTK
0.6
0.5
0 5 10 15 20 0 5 10 15 20 0 5 10 15 20
m
Fig. 5. Accuracy v/s m for γ ∈{0.25,1,4}
that distributed data may amplify the challenges of adaptation. When it comes to kernel
choice in the context of adaptation, distinct behaviors emerge depending on the specific
scenario. For instance, in regimes of high privacy with low data distribution (e.g., m = 1),
the triangular kernel outperforms its Gaussian counterpart in adaptation. Conversely, for
larger m values, the Gaussian kernel seems more adept. In scenarios of lower privacy
constraints, the triangular kernel consistently outperforms the Gaussian.
From a practical standpoint, the general recommendation is to opt for the Gaus-
sian kernel only when dealing with extensively distributed data (large m) coupled with
strict privacy requirements. In all other scenarios, the triangular kernel proves to be more
effective.
6.2. Real Data
Wenowapplyourprivate,distributed,nonparametricclassifiertoarealdatasettodemon-
strate its practical merits. We have selected the heart disease dataset from Detrano et al.
[1989], publicly available on the UCI Machine Learning Repository. The primary task is
to predict the prevalence of heart disease based on 13 covariates, including demographics
(age, sex) and clinical measurements (blood pressure, resting heart rate, cholesterol, chest
pain prevalence, etc.). This dataset comprises patient data from four hospitals in Cleve-
land, Hungary, Switzerland, and Long Beach. The distributed nature of the data, along
with the presence of sensitive patient information, makes it an ideal example for applying
our distributed differentially private classification algorithm.
In order to illustrate the heterogeneity across different servers, we evaluated the
performance of our kernel-based classification algorithm in a non-private and non-transfer
ycaruccA
eps:
1
eps:
8Minimax Optimal Transfer Learning for Classification under Distributed Differential Privacy 27
Average Accuracy
Switzerland
0.56 0.54 0.71 0.56
Long Beach
0.66 0.59 0.70 0.51
0.7
Hungary 0.6
0.74 0.78 0.75 0.61
Cleveland
0.75 0.79 0.76 0.54
Cleveland Hungary
Long
Beach Switzerland
Test Server
Fig. 6. Accuracy for different train-test server pairs
learning setting. Specifically, we selected 100 random samples from a designated training
server and assessed the model’s performance not only on a separate test set from the
same server but also on data from three additional servers. Figure 6 visualizes these
results, clearly showing that the same model exhibits varying levels of performance across
different servers.
The analysis reveals significant variations in the classification model’s performance,
which highlight the inherent heterogeneity in the datasets. Notably, when trained on
Cleveland data, the model achieves its highest accuracy on Hungarian data, suggesting
some level of similarity between these datasets. In contrast, models trained on Swiss data
exhibit markedly lower performance across all servers, including on local data, which
points to potential challenges with the Swiss dataset’s complexity or representativeness.
Meanwhile,thedatafromLongBeachdemonstratesconsistentperformanceacrossdiverse
test servers, suggesting its features may possess a higher degree of generalizability. This
variability underscores the need for adaptive modeling strategies in distributed systems
to manage data heterogeneity and enhance predictive accuracy.
To address these challenges and potentially enhance model performance, we apply
our transfer learning models to borrow strength from various sources. We will use the
Hungarian hospital as the transfer target, and the rest would be used at source datasets.
6.2.1. Implementation Details
The choice of variables is critical for an illustration of our method, since it is nonparamet-
ric and hence extremely susceptible to the curse of dimensionality. Additionally, the data
is plagued with missing observations for many of its columns. Among the 13 covariates
available,wefirstpickedthe9predictorswhichhadatleast90%non-missingobservations
across all the data. Among the rest, we removed fbs because it had 60% missingness for
Switzerland data. We also removed the categorical variable restecg since its three levels
lead to a higher dimensionality, while existing studies suggest its lower importance in
predicting heart disease. Our chosen set of variables is therefore of size 7, and consists of
age,sex,cp(chestpaintype),exang(exerciseinducedangina,yesorno),thalach(max-
revreS
niarT28 Auddy, Cai, and Chakraborty
imum heart rate), oldpeak (ST depression induced by exercise), and trestbps (resting
blood pressure). We further excluded the patients who had missing observations for these
variables and set aside a test set of size 150 from our target. This resulted in n = 142
0
patientsforthetarget(Hungary),whilethesourceshadn = 303,n = 141,andn = 116
1 2 3
patients for the hospitals in Cleveland, Long Beach and Switzerland respectively.
In order to better place the problem in our theoretical setting, we scale each of
the chosen covariates to range between 0 and 0.5. Another important distinction is the
centering for our estimators. An initial exploratory analysis for our data reveals that
the sources have different degrees of disease prevalence: 47%, 36%, 93%, and 79% for
Cleveland, Hungary, Switzerland, and Long Beach respectively. To alleviate this issue, we
re-weight the summands in our kernel estimator by source-specific mean of the disease
status.
As the true transfer parameters are unknown, we opt for the data-adaptive proce-
dures specified in Section 5. We will compare the following adaptation variations, each
incorporating different amounts of auxiliary information:
(a) AdaptAll: We choose the best bandwidth and weights following Section 5.2.
(b) AdaptTar:Thebestbandwidthischosenbasedsolelyonthetargetestimator.That
is, the weights are zero for each source server.
(c) AdaptSamp: The weights are taken to be proportional to the sample sizes for each
server.
(d) AdaptHomog:Theadaptationprocedureundersourcehomogeneity(seeSection5.1)
is used to choose the best bandwidth and weights.
We compare accuracy of the five different adaptation techniques against the common
privacy parameter ε. We also compare the different adaptation procedures on another
popular performance metric called the F1 score which is the harmonic mean of precision
and recall. Figure 7 shows the average accuracy and F1 score obtained by replicating our
procedure 200 times on different test and train folds.
0.8
0.7
Method
0.7 0.6 AdaptAll
AdaptHomog
AdaptSamp
0.6 0.5 AdaptTar
0.5 0.4
4 8 12 16 4 8 12 16
e e
Fig. 7. Accuracy and F1 score values for different privacy levels
ycaruccA
naeM
erocS
1F
naeMMinimax Optimal Transfer Learning for Classification under Distributed Differential Privacy 29
It is evident from Figure 7 that both the accuracy and the F1 score values improve
for all the classifiers as the privacy requirements become less stringent. Among the five
classifiers, AdaptTar, the one depending only on the target shows the poorest perfor-
mance, thus highlighting the benefits of transfer learning in this problem. On the other
hand, the sample size weighted adaptive classifier, denoted AdaptSamp is clearly the
best among the five. The all-adaptive classifier, AdaptAll, automatically adjusts to the
unknown weights and attains the same accuracy and F1 score values as AdaptSamp
for higher values of ε. Finally the AdaptHomog is strictly worse than our estimator
possibly indicating the inherent heterogeneity among the source servers. Overall, we find
a prediction accuracy of 81% and an F1 score of 73% in predicting heart disease from
the hospitals dataset, showing the effectiveness of our data-adaptive distributed private
transfer learning classification mechanism.
7. Discussion
In this paper, we establish the minimax misclassification rate in a heterogeneous dis-
tributed setting with varying sample sizes, privacy parameters, and data distributions
across servers under the posterior drift model. Our results precisely characterize the ef-
fects of privacy constraints, source sample sizes, and target sample size.
We rigorously quantify the trade-offs between data heterogeneity and variations in
privacy budgets. By analyzing these trade-offs through minimax optimality, we clarify
how differences in data distribution across servers impact the overall learning process and
model performance. Our findings highlight the critical balance needed between maintain-
ing privacy and effectively leveraging distributed data. The construction of the minimax
and data-driven adaptive classifiers addresses excess risk and encapsulates the inherent
trade-offs introduced by differential privacy. The impact of privacy constraints can be
mitigated, to some extent, by leveraging larger datasets. The nuanced behavior of these
classifiers, specifically how they scale with changes in the privacy constraints and sample
sizes, offers a promising avenue for optimizing distributed learning systems. Our results
are robust and theoretically justified within the defined heterogeneous settings and the
posterior drift model.
Motivated by several practical requirements, it would be of significant interest to
consider the transfer learning problem in other models with distributed differential pri-
vacy constraints. One natural direction is to consider distributed classification under the
covariate shift model with differential privacy constraints. Such an analysis could pro-
vide additional insights and methods for a range of potential applications. Another future
direction is to study distributed classification under parametric models such as logistic
regression. We leave these intriguing problems for future research.
Funding: The research was supported in part by NIH grants R01-GM123056 and R01-
GM129781.
References
John M Abowd. The challenge of scientific reproducibility and privacy protection for
statistical agencies. Census Scientific Advisory Committee, pages 1011–1020, 2016.30 Auddy, Cai, and Chakraborty
JayadevAcharya,YuhanLiu,andZitengSun. Discretedistributionestimationunderuser-
levellocaldifferentialprivacy. InInternational Conference on Artificial Intelligence and
Statistics, pages 8561–8585. PMLR, 2023.
Arnab Auddy, T. Tony Cai, and Abhinav Chakraborty. Supplement to “Minimax and
adaptive nonparametric classification for transfer learning under distributed differential
privacy constraints”. Technical Report, 2024.
Jean-YvesAudibertandAlexandreB.Tsybakov. Fastlearningratesforplug-inclassifiers.
The Annals of Statistics,35(2):608–633,2007. doi:10.1214/009053606000001217. URL
https://doi.org/10.1214/009053606000001217.
Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jen-
niferWortmanVaughan. Atheoryoflearningfromdifferentdomains. Machinelearning,
79:151–175, 2010.
ThomasBerrettandCristinaButucea.Classificationunderlocaldifferentialprivacy.arXiv
preprint arXiv:1912.04629, 2019.
T. Tony Cai and Hongji Wei. Transfer learning for nonparametric classification: Minimax
rate and adaptive classifier. The Annals of Statistics, 49(1):100 – 128, 2021. doi: 10.
1214/20-AOS1949. URL https://doi.org/10.1214/20-AOS1949.
T. Tony Cai, Abhinav Chakraborty, and Lasse Vuursteen. Optimal federated learning for
nonparametricregressionwithheterogeneousdistributeddifferentialprivacyconstraints.
Technical Report, 2023.
P.K. Chan, W. Fan, A.L. Prodromidis, and S.J. Stolfo. Distributed data mining in credit
card fraud detection. IEEE Intelligent Systems and their Applications, 14(6):67–74,
1999. doi: 10.1109/5254.809570.
Corinna Cortes, Mehryar Mohri, and Andr´es Munoz Medina. Adaptation based on gen-
eralized discrepancy. Journal of Machine Learning Research, 20(1):1–30, 2019.
Robert Detrano, Andras Janosi, Walter Steinbrunn, Matthias Pfisterer, Johann-Jakob
Schmid,SarbjitSandhu,KernHGuppy,StellaLee,andVictorFroelicher. International
application of a new probability algorithm for the diagnosis of coronary artery disease.
The American journal of cardiology, 64(5):304–310, 1989.
Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to
sensitivity in private data analysis. In Theory of Cryptography: Third Theory of Cryp-
tography Conference, TCC 2006, New York, NY, USA, March 4-7, 2006. Proceedings
3, pages 265–284. Springer, 2006.
Jianqing Fan, Cheng Gao, and Jason M Klusowski. Robust transfer learning with unreli-
able source data. arXiv preprint arXiv:2310.04606, 2023.
Saurabh Garg, Yifan Wu, Sivaraman Balakrishnan, and Zachary Lipton. A unified view
of label shift estimation. Advances in Neural Information Processing Systems, 33:3290–
3300, 2020.Minimax Optimal Transfer Learning for Classification under Distributed Differential Privacy 31
Pascal Germain, Amaury Habrard, Francois Laviolette, and Emilie Morvant. A pac-
bayesian approach for domain adaptation with specialization to linear classifiers. In
International conference on machine learning, pages 738–746. PMLR, 2013.
WeiGuo,FuzhenZhuang,XiaoZhang,YiqiTong,andJinDong. Acomprehensivesurvey
of federated transfer learning: Challenges, methods and applications. arXiv preprint
arXiv:2403.01387, 2024.
Rob Hall, Alessandro Rinaldo, and Larry Wasserman. Differential privacy for functions
and functional data. The Journal of Machine Learning Research, 14(1):703–727, 2013.
Ce Ju, Dashan Gao, Ravikiran Mane, Ben Tan, Yang Liu, and Cuntai Guan. Federated
transfer learning for eeg signal classification. In 2020 42nd annual international confer-
ence of the IEEE engineering in medicine & biology society (EMBC), pages 3040–3045.
IEEE, 2020.
Samory Kpotufe and Guillaume Martinet. Marginal singularity and the benefits of labels
in covariate-shift. The Annals of Statistics, 49(6):3299–3323, 2021.
Bertrand Lebichot, Yann-A¨el Le Borgne, Liyun He-Guelton, Frederic Obl´e, and Gianluca
Bontempi. Deep-learningdomainadaptationtechniquesforcreditcardsfrauddetection.
In Recent Advances in Big Data and Deep Learning: Proceedings of the INNS Big Data
and Deep Learning Conference INNSBDDL2019, held at Sestri Levante, Genova, Italy
16-18 April 2019, pages 78–88. Springer, 2020.
Daniel Levy, Ziteng Sun, Kareem Amin, Satyen Kale, Alex Kulesza, Mehryar Mohri,
and Ananda Theertha Suresh. Learning with user-level privacy. Advances in Neural
Information Processing Systems, 34:12466–12479, 2021.
Mengchu Li, Ye Tian, Yang Feng, and Yi Yu. Federated transfer learning with differential
privacy. arXiv preprint arXiv:2403.11343, 2024.
Zachary Lipton, Yu-Xiang Wang, and Alexander Smola. Detecting and correcting for
label shift with black box predictors. In International conference on machine learning,
pages 3122–3130. PMLR, 2018.
Ruiqi Liu, Kexuan Li, and Zuofeng Shang. A computationally efficient classification algo-
rithm in posterior drift model: phase transition and minimax adaptivity. arXiv preprint
arXiv:2011.04147, 2020a.
Yuhan Liu, Ananda Theertha Suresh, Felix Xinnan X Yu, Sanjiv Kumar, and Michael
Riley. Learning discrete distributions: user vs item-level privacy. Advances in Neural
Information Processing Systems, 33:20965–20976, 2020b.
Yuheng Ma and Hanfang Yang. Optimal locally private nonparametric classification with
public data. arXiv preprint arXiv:2311.11369, 2023.
Subha Maity, Yuekai Sun, and Moulinath Banerjee. Minimax optimal approaches to the
label shift problem in non-parametric settings. Journal of Machine Learning Research,
23(346):1–45, 2022.32 Auddy, Cai, and Chakraborty
Subha Maity, Diptavo Dutta, Jonathan Terhorst, Yuekai Sun, and Moulinath Banerjee.
A linear adjustment-based approach to posterior drift in transfer learning. Biometrika,
111(1):31–50, 2024.
CliftonPhua,VincentLee,KateSmith,andRossGayler. Acomprehensivesurveyofdata
mining-based fraud detection research. arXiv preprint arXiv:1009.6119, 2010.
Henry WJ Reeve, Timothy I Cannings, and Richard J Samworth. Adaptive transfer
learning. The Annals of Statistics, 49(6):3618–3649, 2021.
Jose Ramon Saura, Ana Reyes-Menendez, and Ferr˜ao Filipe. Comparing data-driven
methods for extracting knowledge from user generated content. Journal of Open Inno-
vation: Technology, Market, and Complexity, 5(4):74, 2019.
Clayton Scott. A generalized neyman-pearson criterion for optimal domain adaptation.
In Algorithmic Learning Theory, pages 738–761. PMLR, 2019.
Masashi Sugiyama, Shinichi Nakajima, Hisashi Kashima, Paul Buenau, and Motoaki
Kawanabe. Direct importance estimation with model selection and its application to
covariateshiftadaptation. Advances in neural information processing systems,20,2007.
Yongpeng Wang, Hong Yu, Guoyin Wang, and Yongfang Xie. Cross-domain recommen-
dation based on sentiment analysis and latent feature mapping. Entropy, 22(4):473,
2020.
Tina Yeung. Local health department adoption of electronic health records and health
information exchanges and its impact on population health. International Journal of
Medical Informatics, 128:1–6, 2019.