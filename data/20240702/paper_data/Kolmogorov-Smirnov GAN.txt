Kolmogorov–Smirnov GAN
Maciej Falkiewicz1,2 Naoya Takeishi3,4 Alexandros Kalousis2
1Computer Science Department, University of Geneva 2HES-SO/HEG Genève
3The University of Tokyo 4RIKEN
{maciej.falkiewicz, alexandros.kalousis}@hesge.ch
ntake@g.ecc.u-tokyo.ac.jp
July 1, 2024
Abstract
We propose a novel deep generative model, the Kolmogorov-Smirnov Generative
Adversarial Network (KSGAN). Unlike existing approaches, KSGAN formulates the
learning process as a minimization of the Kolmogorov-Smirnov (KS) distance, general-
ized to handle multivariate distributions. This distance is calculated using the quantile
function, which acts as the critic in the adversarial training process. We formally
demonstrate that minimizing the KS distance leads to the trained approximate distri-
bution aligning with the target distribution. We propose an efficient implementation
and evaluate its effectiveness through experiments. The results show that KSGAN
performs on par with existing adversarial methods, exhibiting stability during training,
resistancetomodedroppingandcollapse,andtolerancetovariationsinhyperparameter
settings. Additionally, we review the literature on the Generalized KS test and discuss
the connections between KSGAN and existing adversarial generative models.
pF(x) P Γcφ(λ)
pG(x) (cid:0) (cid:1)
cφ(x) D KS % %
λ
x xF G∼ ∼P PF
G
c cφ φ( (x xF G) ) P F Γcφ(λ)
P G(cid:0)Γcφ(λ)(cid:1)
(cid:0) (cid:1)
λ
Figure 1: A schematic depiction of how the Generalized Kolmogorov-Smirnov (KS) distance
between target P and approximate P distributions with respect to critic c is computed.
F G ϕ
The critic is evaluated on samples x () and x () from the target and approximate
F G
||| |||
distributions respectively. The λ threshold moves from to + establishing a stack of
−∞ ∞
levelsets. Ateachlevel,thefractionofdatapoints( and )belowthethresholdiscalculated
for each distribution independently. This produces• the P• Γ (λ) and P Γ (λ) curves.
F cϕ G cϕ
The Generalized KS distance is the largest absolute difference between the curves shown as
(cid:0) (cid:1) (cid:0) (cid:1)
in the right figure. Best viewed in color.
↕↕↕
1 Introduction
Generative modeling is about fitting a model to a target distribution, usually the data. A
fundamental taxonomy of models assigns them into prescribed and implicit statistical models
1
4202
nuJ
82
]LM.tats[
1v84991.6042:viXra[9],withpartialoverlapbetweenthetwoclasses. Prescribedmodelsdirectlyparameterizethe
distribution’s probability density function, while implicit models parameterize the generator
that allows samples to be drawn from the distribution. The ultimate application of the
model primarily dictates the choice between the two approaches. It does, however, have
consequences regarding the available types of divergences that we can minimize when fitting
themodel. Thedivergencesdifferinthestabilityofoptimizationandcomputationalefficiency,
as well as statistical efficiency, which all affect the final performance of the model.
The natural approach for fitting a prescribed model is maximum likelihood estimation
(MLE), equivalently formulated as minimization of Kullback–Leibler divergence. Likelihood
evaluation for normalized models is straightforward. In non-normalized models, density
evaluationisexpensive;inthiscontext,Hyvärinen[22]proposedthescorematchingobjective,
which can be interpreted as the Fisher divergence [30]. This approach is very effective for
simulation-free training of ODE[7]/SDE[42, 19]-based models which are state-of-the-art in
multiple domains today.
The principle driving the fitting of implicit statistical models is to push the model to
generate samples that are indistinguishable from the target. An inflection point for this
family of models came with the Generative Adversarial Network (GAN) [13], which took the
principle literally and introduced an auxiliary classifier trained in an adversarial process to
discriminatebetweenthetwodistributions. Theclassificationerrorgivenanoptimalclassifier
relates to the Jensen–Shannon divergence between generator and the target. Initial work in
this area involved applying heuristic tricks to deal with learning problems, namely vanishing
gradients, unstable training, and mode dropping or collapse. Further advancements focused
on using other distances based on the principle of adversarial learning of auxiliary models,
which were supposed to have certain favorable properties with respect to the original GAN.
The Bayesian inference community has been reluctant to adopt adversarial methods
[8], and the attempts to apply them in this context [40] indicate a credibility problem. A
significant drawback of approximate methods is the excessive reduction of diversity in the
distribution [17], the extremes of which lead to mode dropping [1]. In this work, we consider
another distance for training implicit statistical models, i.e., the Kolmogorov-Smirnov (KS)
distance, which, to the best of our knowledge, has not been used in this context before. The
distinctive feature of the KS distance is that it directly measures the coverage discrepancy of
each other’s credibility regions by the distributions under analysis at all confidence levels.
Thus, its minimization straightforwardly leads to the correct spread of the probability mass,
avoiding mode dropping, overconfidence, and mode collapse when applied with a sufficient
sampling budget.
We term the proposed model as Kolmogorov-Smirnov Generative Adversarial Network
(KSGAN). We show how to generalize the standard KS distance to higher dimensions
based on Polonik [38] in section 2, allowing our method to be used for multidimensional
distributions. Next, in section 3, we show how to efficiently leverage the distance in an
adversarial training process and show formally that the proposed algorithm leads to an
alignment of the approximate and target distributions. We support the theoretical findings
with empirical results presented in section 6.
2 Generalized Kolmogorov–Smirnov distance
We generalize the Kolmogorov–Smirnov (KS) distance (sometimes called simply Kolmogorov
distance) between continuous probability distributions on one-dimensional spaces to mul-
tidimensional spaces and show that it is a metric. The test statistic of the KS test is a
KS distance between empirical and target distributions (or two empirical in the case of the
two-sample case). For this reason, our proposal is directly inspired by the generalization of
the test introduced in Polonik [38].
Let us consider two probability measures P and P on a measurable space ( , ),
F G
X A
where the sample space is a vector space such as IRd and is the corresponding event
X A
2space; F : [0,1] and G: [0,1] are the cumulative distribution functions (CDFs)
of P andXP→ respectively.1 WX e→ say that P =P iff A , P (A)=P (A). When
F G F G F G
∀ ∈A
dim( )=1 then the KS distance is
X
D (P ,P ):= sup F(x) G(x). (1)
KS F G
| − |
x∈X
In the multivariate case, the problem with using the KS distance as is is that on a d-
dimensionalspace,thereare2d 1waysofdefiningaCDF.Thedistancehastobeindependent
−
of the particular definition and thus should be the largest across all the possibilities [35].
This, however, becomes prohibitive for any d>2. In other words, the challenge comes from
a multidimensional vector space not being a partially ordered set. Everything that follows in
this section consists of proposing a partial order, showing that, under certain conditions, a
probability distribution can be uniquely determined on its basis and operationalizing it in
an optimization problem.
We begin by bringing the classical result that
D (P ,P )= sup F(G−1(α)) α, (2)
KS F G
| − |
α∈[0,1]
where G−1 : [0,1] is the inverse CDF also called the quantile function. Einmahl
→ X
and Mason [10] show that there exists a natural generalization of the quantile function to
multivariate distribution, which we restate below.
Definition 1 (Generalized Quantile Function). Let v: IR be a measure, and
+
A→ C ⊂A
an arbitrary subset of the event space, then a function CP,C(α):[0,1] such that
→C
CP,C(α) argmin v(C):P(C)⩾α (3)
∈ { }
C∈C
is called the generalized quantile function in for P with respect to v2.
C
The generalized quantile function evaluated at level α yields a minimum-volume set [36]
whose probability is at least α, and it is the smallest with respect to v such set in , thus
C
the name. For the remainder of this paper, we assume that v is the Lebesgue measure.
It may seem that it is enough to plug CP G,C(α) in place of G−1(α) and P
F
in place of F
in eq. (2) to establish the Generalized KS distance but it turns out that such a distance does
not satisfy the positivity condition D (P ,P ) > 0 if P = P as the example below
KS F G F G
̸
shows.
Example 1 (Polonik [38]). Let P be the probability measure of a chi distribution with
F
one degree of freedom χ2 which has support on IR and P the probability measure of a
1 + G
standard Gaussian distribution (0,1) which has support on the whole IR. Given = we
(cid:112) N C A
have
P F(CP G,C(α))=α ∀α ∈[0,1], (4)
while clearly P =P . The statement in eq. (4) is easy to show by observing that x [0, )
F G
the density of P F̸ is twice the density of P
G
and CP G,C(α) are intervals centered∀ at∈ 0. ∞
Instead, a solution based on the quantile functions of both distributions is needed, which
we present in definition 2.
Definition 2 (Generalized Kolmogorov-Smirnov distance). Let the Generalized Kolmogorov-
Smirnov distance be formulated as follows:
D (P ,P ):= sup [P (C(α)) P (C(α))]. (5)
GKS F G F G
| − |
α∈[0,1]
C∈{CPG,C,CPF,C}
1InwhatfollowswewilluseP F forthetruedatadistributionandP G forthelearntone
2Inthegeneralcase,CP,C(α)atanygivenlevelαisnotuniquelydetermined,i.e. theremayexistseveral
setsC,C′∈C s.t. C≠ C′ thatsatisfytheconditionineq.(3). Forsimplicity,wewillcallallsuchsetsthe
(generalized)quantilesetsatlevelαandwriteCP,C(α)=C andCP,C(α)=C′ forallofthem.
3Such distance is symmetric, satisfying the triangle inequality as shown in appendix A.1.
For the remainder of this section, we will show that the Generalized KS distance in eq. (5)
meets the necessary D (P,P) = 0 and sufficient D (P ,P ) > 0 if P = P
GKS GKS F G F G
̸
conditions to consider it a metric. In the proof, we will rely on the probability density
function of P with respect to a reference measure v, which we denote with p: [0, ).
X → ∞
Let
Γ (λ):= x:p(x)⩾λ (6)
p
{ }
denote the density level set of p at level λ⩾0 (also called the highest density region [21]),
and let Π := Γ (λ):λ⩾0 . The following observations about level sets will introduce the
p p
{ }
fundamental tools to prove the necessary and sufficient conditions for the generalized KS
distance.
Remark 1 (The silhouette [37]). For any density p, the following holds
∞
p(x)= 1 (x)dλ, (7)
Γp(λ)
(cid:90)0
where 1 denotes the indicator function of a set C. The RHS of eq. (7) is called the
C
silhouette.
An immediate consequence of remark 1 is that Π ordered with respect to λ⩾0 fully
p
characterizes P, because p does. Graphically, the silhouette is a multidimensional stack of
level sets.
Remark 2. Density level sets are minimum-volume sets [38] The quantity P(C) λv(C)
−
is maximized over by Γ p(λ), and thus if Γ p(λ) , then Γ p(λ) = CP,C(α)3 at level
α=P(Γ (λ))= pA (x)1 (p(x))dx. ∈ C
p [λ,∞)
Below, we pre(cid:82)sent the fundamental theoretical result behind the proposed method, which
restates Lemma 1.2. of Polonik [38].
Theorem 1 (Necessary and sufficient conditions). Let v be a measure on ( , ). Suppose
that P and P are probability measures on ( , ) with densities (with refX ereA nce measure
F G
X A
v) f and g respectively. Assuming that
A.1 Π Π ;
f g
∪ ⊂C
A.2 CP F,C(α) and CP G,C(α) are uniquely determined4 in
C
with respect to v
the following two statements are equivalent:
S.1 P =P ;
F G
S.2 D (P ,P )=0.
GKS F G
See proof in Appendix A .
Meeting assumption A.1 is a demanding challenge, almost equivalent to learning the
target distribution. Below, we propose a relaxation of it, which we will use to show the
validity of our method.
Theorem 2 (Relaxation of assumption A.1). Theorem 1 holds if assumption A.1 is relaxed
to the case that contains sets that are uniquely determined with density level sets of P
F
and P up to a C set C such that
G
P (C′)=P (C′), (8)
∀C′∈2C F G
and let r :=P (C)=P (C), then the supremum in statement S.2 is restricted to [0,1 r].
F G
−
See proof in Appendix A .
3TheremaybeothersetsC=CP,C(α)butΓp(λ)willcertainlybeoneofthem.
4InthesensedefinedinPolonik[38]
43 Kolmogorov–Smirnov GAN
For the remainder of the paper, we will consider P as the target distribution represented by
F
adataset x ,andP astheapproximatedistributionthatwewanttotrainbyminimizing
F G
the Gener{ aliz} ed KS distance in eq. (5) with Stochastic Gradient Descent. We model P
G
as a pushforward g P of a simple (e.g., Gaussian, or Uniform) latent distribution P
θ# Z Z
supported on , with a neural network g : , parameterized with θ, which we call the
θ
Z Z →X
generator.
Themajorchallengeinutilizingeq.(5)isthenecessityoffindingtheCP,C(α)termswhich
is an optimization problem on its own. The idea that we propose in this work is to amortize
the procedure by modeling the generalized quantile functions CP F,C(α) and CP G,C(α) with
additionalneuralnetworkswhichhavetobetrainedinparalleltothegeneratorg . Therefore,
θ
our method is based on adversarial training [13], where optimization proceeds in alternating
phases of minimization and maximization for different sets of parameters. Hence the name
of the proposed method, the Kolmogorov–Smirnov Generative Adversarial Network.
3.1 Neural Quantile Function
The generalized quantile function defined in definition 1 is an infinite-dimensional vector
functionCP,C :[0,1] C . Suchobjectsdonothaveanexpressive,explicitrepresentation
→ ∈C
that allows for gradient-based optimization. Therefore, we use an implicit representation
inspired by density level sets in eq. (6). We propose to use neural level sets defined in
definition 3 that are modeled by a neural network c: IR, which we will refer to as the
X →
critic.
Definition 3 (Neural level set). Given a neural network c: IR, the neural level set at
X →
level λ is defined as5
Γ (λ):= x:c(x)⩽λ , and let Π := Γ (λ):λ IR . (9)
c c c
{ } { ∈ }
Neural level sets are used, for example, in image segmentation [6, 20] and surface
reconstruction from point clouds [3]. They fit our application because for computing the
Generalized KS distance in eq. (5), the explicit materialization of generalized quantiles is
not required as long as the probability measure can be efficiently evaluated on the implicitly
s Pp (e Γci (fi λe )d )s ⩾et αs. .W Foe rs aet pC rob= abΠ ilc i, tyan md eat sh uu rs eC PP ′, tΠ hc e(α fo) ll= owΓ inc g(λ hα o) l, dsw :ith λ α = argmin λ∈IR{λ :
c
}
P′(CP,Πc(α))=E x∼P′ 1 (−∞,λα](c(x)) , (10)
which shows that the terms in eq. (5) under neur(cid:2)al level sets can(cid:3)be Monte-Carlo estimated
given samples from the respective distributions. Assumption A.2 is satisfied by neural level
sets by construction.
TheformulationoftheGeneralizedKSdistanceineq.(5)includestwogeneralizedquantile
functions CP F,C(α) corresponding to target distribution P
F
and CP G,C(α) corresponding
to the approximate distribution P . Both have to be modeled with the respective neural
G
networksc andc ,whereweuseϕ= ϕ ,ϕ todenotethejointsetoftheirparameters.
ϕF ϕG
{
F G
}
In section 3.3, we show how to parameterize both critics with a single neural network. We
set =Π Π .
C
cϕF
∪
cϕG
3.2 Optimizing generator’s parameters θ
The Generalized KS distance in eq. (5) is a supremum over a unit interval and two functions;
thus, it can be upper-bounded as
D (P ,P )⩽ sup [P (C(α)) P (C(α))]. (11)
GKS F G F G
| − |
α∈[0,1]
C∈{CPG(cid:88),C,CPF,C}
5Please note that the direction of the inequality in eq. (9) is opposite of the one in eq. (6) which is a
conventionthatalignsthecriticwiththeenergyfunctionofEnergy-Basedmodels.
5Next, we plug in =Π Π to eq. (11) and use eq. (10) to get generator’s objective:
C
cϕF
∪
cϕG
Lg = sup |E x∼P F 1 (−∞,λ](c ϕ(x)) −E x∼P G 1 (−∞,λ](c ϕ(x)) | . (12)
λ∈IR
cϕ∈{c(cid:88)ϕG,cϕF}
(cid:2) (cid:2) (cid:3) (cid:2) (cid:3) (cid:3)
In practice, the expectations in eq. (12) are estimated on finite samples from the two
distributions, i.e. x mentioned before, and x sampled from the approximate dis-
F G
tribution P using{ the} reparametrization trick t{ o fa} cilitate backpropagation of gradients.
G
Therefore, the two terms become step functions in λ, and the supremum is located on one of
the steps. That way, a line search on IR reduces to a maximum over a finite set. To preserve
the differentiability of the cost function calculated in this way, we apply Straight-through
Estimator [4] in place of indication function 1. A schematic depiction of the process for a
single critic is shown in fig. 1.
3.3 Optimizing critics’ parameters ϕ
By optimizing critics’ parameters ϕ, we want to satisfy assumption A.1 so that Generalized
KS distance becomes a metric. For the problem posed in such a way, we lack supervision,
i.e., we do not know the target sets’ shapes. However, we can reformulate the problem as an
estimation of the density functions of the two considered measures P and P and use the
F G
obtained approximate density models to build level sets. We can constitute an optimization
problem for such a task based solely on finite sets of samples, which we have for P and
F
can arbitrarily generate from P . As the estimator, we propose to use the Energy-based
G
model (EBM) [43], which, thanks to the lack of constraints in the choice of architecture, can
be very expressive while having favorable computational complexity at inference. To carry
out EMB training effectively, we will introduce a new min-max game, the “min phase” of
which will turn out to be the initial objective in eq. (5), and in this way, we will close the
adversarial cycle.
Let the critic c (x) serve as the energy function. The density given by the EBM is then
ϕF
p (x)=exp( c (x))/Z , where Z = exp( c (x))dx is the normalizing constant
cϕF
−
ϕF cϕF cϕF
−
ϕF
called partition function. The standard technique for learning the model given target data
distribution P is MLE, where the likelihood(cid:82)
F
E x∼P F[logp cϕF(x)]=E x∼P F[ −c ϕF(x)] −logZ cϕF (13)
is maximized wrt ϕ . An unbiased estimate of the gradient of the second term can be
F
obtained with samples from the EBM itself, typically achieved with MCMC sampling. Many
approaches to avoid this expensive procedure have been described in the literature [43], and
among them, the one based on adversarial training [23] is the most appealing to us. It
introduces an auxiliary distribution P , such that the gradient of eq. (13) wrt ϕ is
aux(F) F
approximated with the gradient of
E x∼P F[ −c ϕF(x)] −E x∼P aux(F)[ −c ϕF(x)]. (14)
Consequently, an additional objective must be introduced, the optimization of
aux(F)
which will lead to the alignment of P L and P , where P denotes the probability
aux(F) cϕF cϕF
distribution with density p (x). We take an analogous approach to estimate c (x).
Whenwe(i)setc
(x):c =ϕF
c (x),and(ii)repurposeP asP andP
ϕ aG
sP ,
ϕG
−
ϕF G aux(F) F aux(G)
we show in appendix A.2 that the MLE objectives for the critics – now, denoted as c –
ϕ
simplify as Lc =E x∼P G[c ϕ(x)] −E x∼P F[c ϕ(x)], which is then maximized in an adversarial
game against the Generalized KS distance in eq. (5).
The standard approach for aligning the auxiliary distributions with their targets is to
use the Kullback–Leibler divergence. We propose using the Generalized KS distance instead.
We set Laux(F) = D GKS P G,P cϕ and Laux(P G) = D GKS P F,P −cϕ . By analyzing these
objectives in the fashion (cid:0)of sectio (cid:1)n 3.2, we note that Laux(cid:0)(P G) is the (cid:1)same as our original
6Algorithm 1: Learning a generative model by minimizing Generalized KS distance.
Input :Target distribution P ; latent distribution P ; generator network g ; critic
F Z θ
network c ; number of critic updates k ; number of generator updates k ;
ϕ ϕ θ
score penalty weight β;
Output:Trained model P approximating P ;
G F
1 repeat
2 for i=1 to k ϕ do
3 Draw batch x P F and z P Z ; // critic’s inner loop
{ }∼ { }∼
4 Rc ← |{z1 }| {z}∥∇xc ϕ(g θ(z)) ∥2 2+ |{x1 }| {x}∥∇xc ϕ(x) ∥2 2;
5 Lc ← |{z1 }| (cid:80){z}c ϕ(g θ(z)) − |{x1 }| {x}c (cid:80)ϕ(x);
6 Update ϕ b (cid:80)y using ∂(Lc ∂− ϕβRc) to m (cid:80)aximize Lc −β Rc;
7 for i=1 to k θ do
8 Draw batch x P F and z P Z ; // generator’s inner loop
{ }∼ { }∼
9 c F c ϕ(x): x and c G c ϕ(g θ(z)): z ;
{ }←{ { }} { }←{ { }}
10 λ c F c G ;
{ }←{ }∪{ }
11 Lg,F ←max {λ} |{z1 }| {cG}1 (−∞,λ](c G) − |{x1 }| {cF}1 (−∞,λ](c F) ;
12 Lg,G ←max {λ}(cid:12) (cid:12) (cid:12)|{x1 }|(cid:80) {cF}1 (−∞,−λ]( −c F) − |{(cid:80) z1 }| {cG}1 (−∞,−λ](cid:12) (cid:12) (cid:12)( −c G) ;
13 Lg ←Lg,F + Lg(cid:12) (cid:12),G; (cid:80) (cid:80) (cid:12) (cid:12)
14 Update θ by usi(cid:12)ng ∂ ∂L θg to minimize Lg; (cid:12)
15 until not converged;
16 return g θ#P Z
objective D (P ,P ) – which is symmetric – when we approximate sampling from P
GKS F G cϕ
w apit ph rot xh ime ata terg de wt id thist Prib .ut Tio hn erP efF or. e,A wn eal ho ag vo eus sl hy owfo nr tL ha au tx t(P hG e) auw xh ie lir ae rysa om bp jel cin tig vef sro am reP al− recϕ adi ys
G
integrated into the adversarial game.
In practice, we find the score penalty regularizer of Kumar et al. [26], derived from the
score matching objective, helpful to stabilize training. Therefore, we subtract it from
c
L
weighted by a hyperparameter β. In this way, we get a critic that is smoother and, therefore,
generates regular level sets that facilitate optimization. We summarize the proposed training
procedure in algorithm 1.
4 Discussion
In section 3.3, where we justify the choice of the critic’s objective function, we refer to
methods for training EBMs, which are approximate density distribution models. Thus, the
reader can expect that our proposed critic c in the limit of convergence of the algorithm
ϕ
will become a source of information about the density distribution of the target distribution
P accompanying the model that generates samples P . However, this does not happen as
F G
a consequence of the design choice (i), that is, the setup of c = c =c . An EBM can
ϕF
−
ϕG ϕ
only be equivalent to its inverse in the case of a uniform distribution. In addition, because
of design choice (ii), during training, the critic is not evaluated outside of the support of
P and P and, therefore, can reach arbitrary values there. Despite these observations, the
F G
Generalized KS distance present in our algorithm exposes sufficient conditions because of
theorem 2.
ThefeaturedistinguishingKSGANfromotheradversarialgenerativemodelingapproaches
is that regardless of the outcome of the critic’s inner problem, minimizing eq. (5) is justified
because Generalized KS distance, despite not meeting assumption A.1, is a pseudo-metric
[38]. For comparison, the dual representation of Wasserstein distance, used in WGAN [2]
7requires attaining the supremum in the inner problem.
The distances used for training generative models all fall into either the category of
f-divergences D (P ,P )= f(dP /dP )dP or integral probability metrics (IPMs)
f F G A F G G
D isF a( nP iF ns, tP aG n) ce= os fu Ip Pf M∈F w|E itx h∼P F(cid:82) =f(x 1) −E x∼P tGf( Ix R) |. oT rhec =lass 1icalone- αdime [n 0s ,i 1o ]na wlK heS nd his ata vn inc ge
F {
(−∞,t]
| ∈ } F {
G−1(α)|
∈ }
access to the inverse CDF of one of the distributions based on eq. (2). One can see the
Generalized KS distance from the perspective of IPM with = 1 α [0,1] & C
C(α)
{ anC dP F P,C,C arP eG N,C o} r} m. aA lis zs iu nm
g
i Fn lg owd sir [e 2c 4t ,a 3c 4c ],es ms et ao suC rP inF g,C tha end diC stP aG n, cCF e, cf oo mr{ e ex sa dm owp| l ne tw∈ oh aen linb eo st eh arP c∈ F
h.
G
5 Related work
The need to generalize the KS test, and therefore distance, to multiple dimensions arose
naturally from the side of practitioners who collected such data and wished to test related
hypotheses. It was first addressed by Peacock [35], where a two-dimensional test for
applications in astronomy was proposed. It involves considering all possible orders in
this space and using the one that maximizes the distance between the distributions. A
modification of this procedure has been proposed by Fasano and Franceschini [11] where
only four candidate CDFs have to be considered, causing the test to be applicable in three
dimensions, with eight candidates, under similar computational constraints. Chronologically,
the following approach was the one on which we base our work, proposed in Polonik [38] but
made possible by the author’s earlier work [36, 37]. To the best of our knowledge, the first
work that practically uses the theory developed by Polonik is Glazer et al. [12], which we
recommend as an introduction to our work. It proposes applying the Generalized KS test
based on the support vector machines for detecting distribution shifts in data streams.
Asaninstanceoftheadversarialgenerativemodelingfamily,ourworkisrelatedtoallthe
countless GAN [13] follow-ups. We highlight those that study the learning process from the
perspective of the distance being minimized. The work of Arjovsky and Bottou [1] provides
a formal analysis of the heuristic tricks used for stabilizing the training of GANs. The
f-GAN [33] proposes a unified training framework targeting f-divergences, which relies on a
variational lower bound of the objective that results in the adversarial process. Approaches
relying on the integral probability metric include FisherGAN [32], the Generative Moment
Matching Networks [29] based on MMD, just like the later, more sophisticated MMD GAN
[28], andfinallytheWassersteinGAN(WGAN)[2]withtheWGAN-GPfollow-up[16]which
shares common features with our work. Our maximum likelihood approach to fitting the
critic results in the same functional form of the loss as WGAN(-GP) uses. In addition, the
score penalty we use is similar to the gradient penalty of WGAN-GP.
6 Experiments
We evaluate the proposed method on eight synthetic 2D distributions (see appendix B.1 for
details) and two image datasets, i.e. MNIST [27] and CIFAR-10 [25]. We compare against
otheradversarialmethods,GANandWGAN-GP,usingthesameneuralnetworkarchitectures
and training hyper-parameters unless specified otherwise (see appendix C for details). All
thequantitativeresultsarepresentedbasedonfiverandominitializationsofthemodels. The
source code for all the experiments is provided at https://github.com/DMML-Geneva/ksgan.
In all KSGAN experiments, we relax the maximum in line 11 and line 12 of algorithm 1
with sample average. In all experiments, we re-use the last batch of samples from the latent
distribution (and target distribution in the case of KSGAN) from the critic’s optimization
inner loop as the first batch for the generator’s optimization inner loop.
8Table1: SquaredpopulationMMD 103 ( )betweentestdataandsamplesfromthemethods
× ↓
trained on 65536 samples, averaged over five random initializations with the standard
deviation calculated with Bessel’s correction in the parentheses. The proposed KSGAN with
k =1 performs on par with the WGAN-GP trained with five times the budget k =5. See
ϕ ϕ
appendix D.1 for qualitative comparison.
Method (k , k )
ϕ θ
Distribution GAN (5, 1) WGAN-GP (5, 1) KSGAN (1, 1)
swissroll 3.37 (1.023) 0.29 (0.119) 0.39 (0.100)
circles 2.98 (1.501) 0.27 (0.215) 0.49 (0.240)
rings 2.00 (1.264) 0.13 (0.082) 0.43 (0.162)
moons 1.41 (0.757) 0.35 (0.136) 0.53 (0.189)
8gaussians 3.57 (2.719) 0.35 (0.248) 0.32 (0.277)
pinwheel 1.66 (1.451) 0.27 (0.184) 0.40 (0.086)
2spirals 0.93 (0.822) 0.27 (0.191) 0.44 (0.232)
checkerboard 1.43 (0.899) 0.38 (0.296) 0.86 (0.468)
6.1 Synthetic distributions
Analyzing adversarial methods on synthetic, low-dimensional distributions is not popular.
However, we conduct such an experiment because we are interested in whether the model
generates samples from the support of the target distribution and how accurately it approxi-
mates the distribution. Working with small-dimensional distributions, we do not have to
be as concerned about the curse of dimensionality when calculating sample-based distances,
and we can visually compare the resulting histograms.
In table 1, we report the squared population MMD [15] between target and approximate
distributions, computed with Gaussian kernel on 65536 samples from each distribution.
Details about how we chose the kernel’s bandwidth can be found in appendix B.1. GAN and
WGAN-GP fail to converge with k = k = 1 (we do not report the results to economize
ϕ θ
on space); thus, we set k = 5 for them. The proposed KSGAN with k = 1 performs at
θ θ
a similar level to WGAN-GP, the better of the two former, despite using five times less
training budget. We present additional results on the synthetic datasets in appendix D.1,
which include performance with different training dataset sizes, non-default hyper-parameter
setups for KSGAN, and histograms of the samples for qualitative comparison.
6.2 MNIST
We use the 50000 training instances to train the models, and based on visual inspection of
the generated samples (reported in appendix D.2), we conclude that all the methods achieve
comparable,highsamplesquality. Toassessthequalityofthedistributionapproximation,we
useapre-trainedclassifieronthesamedataasthegenerativemodels(detailsinappendixB.2).
We run the same experiment on 3StackedMNIST [44], which has 1000 modes. We report the
results in table 2.
In this experiment, we set the training budget for all methods to k =1, k =1 for a fair
ϕ θ
comparison. WefindthatallmethodsalwaysrecoverallthemodeswiththestandardMNIST
target. However, GAN fails to distribute the probability mass uniformly between the digits.
As the number of modes increases with the 3StackedMNIST target, GAN demonstrates its
inferiority to other methods by losing 198 modes on average (four initialization cover approx.
985 modes, and one fails to converge, achieving only 98 modes). WGAN-GP and KSGAN
consistently recover all the modes while being on par regarding KL divergence, which differs
little between networks’ initialization.
9Table 2: The number of captured modes and Kullback-Leibler divergence between the
distribution of sampled digits and target uniform distribution averaged over five random
initializations with the standard deviation calculated with Bessel’s correction in the paren-
theses. All the methods were trained with the same budget k =1, k =1. WGAN-GP and
ϕ θ
KSGAN cover all the modes in all experiments while demonstrating low KL divergence.
MNIST 3StackedMNIST
Method (k , k ) # modes ↑ KL ↓ # modes ↑ KL ↓
ϕ θ
GAN (1,1) 10 (0.00) 0.6007 (0.27550) 808 (396.91) 1.4160 (1.36819)
WGAN-GP (1,1) 10 (0.00) 0.0087 (0.00499) 1000 (0.00) 0.0336 (0.00461)
KSGAN (1,1) 10 (0.00) 0.0056 (0.00045) 1000 (0.00) 0.0362 (0.00534)
Table3: InceptionScore(IS)andFréchetinceptiondistance(FID)metricsaveragedoverfive
random initializations with the standard deviation calculated with Bessel’s correction in the
parentheses. All the methods were trained with the same budget k =1, k =1. The scores
ϕ θ
for the training dataset are included in the top row, as “Real data” for reference. WGAN-GP
and KSGAN perform similarly on average, while KSGAN exhibits lower variance between
networks’ initialization.
Method (k , k ) IS ↑ FID ↓
ϕ θ
Real data 11.2643 5.8369
GAN (1,1) 6.6209 (0.59187) 47.9414 (10.78435)
WGAN-GP (1,1) 6.7351 (0.31735) 44.3026 (6.61652)
KSGAN (1,1) 6.6429 (0.16785) 41.1555 (3.26385)
6.3 CIFAR-10
We use the 50000 training instances to train the models and report the generated samples in
appendix D.3. We train the models in a fully unconditional manner, i.e., not using the class
information at all – contrary to many unconditional models that use class information in
normalization layers. We quantify the quality of fitted models by computing the Inception
Score (IS) [41] and Fréchet inception distance (FID) [18] from the test set and report the
results in table 3 based on five random initializations. For reference, in the table, we include
the IS of the training dataset and the FID between the training and test sets.
In this experiment, we set the training budget for all methods to k =1, k =1 for a
ϕ θ
fair comparison. All models fail to accurately approximate the target distribution, which is
evident from a quantitative comparison in table 3 and a qualitative one in appendix D.3.
KSGAN is characterized by the lowest variance between initializations among the methods
considered.
7 Conclusions and future work
Inthiswork,weinvestigatedtheuseofGeneralizedKolmogorov–Smirnovdistancefortraining
deep implicit statistical models, i.e., generative networks. We proposed an efficient way
to compute the distance and termed the resulting model Kolmogorov–Smirnov Generative
Adversarial Network because it uses adversarial learning. Based on the empirical evaluation
of the proposed model, the results of which we report, we conclude that it can be considered
as an alternative to existing models in its class. At the same time, we point out that many
properties of KSGAN have not been studied, and we leave this as a future work direction.
Interestingaspectstoexplorearethecharacteristicsoflearningdynamicswiththenumber
of generator updates exceeding the number of critic updates, alternative ways to train the
10critic, and alternative representations of generalized quantile sets. The natural scaling of the
Generalized KS distance may also prove beneficial regarding the interpretability of learning
curves, learning rate scheduling, or early stopping. In addition, we hope that our work
will draw the attention of the machine learning community to the Generalized KS distance,
applications of which remain to be explored.
Acknowledgments and Disclosure of Funding
We acknowledge the financial support of the Swiss National Science Foundation within the
MIGRATE project (grant no. 209434). The computations were performed at the University
of Geneva on "Baobab" and "Yggdrasil" HPC clusters.
References
[1] M. Arjovsky and L. Bottou. Towards principled methods for training generative adversarial
networks. In International Conference on Learning Representations, 2017.
[2] M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein generative adversarial networks. In
International conference on machine learning, pages 214–223. PMLR, 2017.
[3] M.Atzmon,N.Haim,L.Yariv,O.Israelov,H.Maron,andY.Lipman. Controllingneurallevel
sets. Advances in Neural Information Processing Systems, 32(NeurIPS), 2019.
[4] Y.Bengio,N.Léonard,andA.Courville.Estimatingorpropagatinggradientsthroughstochastic
neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.
[5] C.A.Carolan. Theleastconcavemajorantoftheempiricaldistributionfunction. TheCanadian
Journal of Statistics / La Revue Canadienne de Statistique, 30(2):317–328, 2002.
[6] G. Chen, Z. Yu, H. Liu, Y. Ma, and B. Yu. DevelSet: Deep Neural Level Set for Instant
Mask Optimization. IEEE Transactions on Computer-Aided Design of Integrated Circuits and
Systems, 42(12):5020–5033, 2023.
[7] R. T. Chen, Y. Rubanova, J. Bettencourt, and D. K. Duvenaud. Neural ordinary differential
equations. Advances in neural information processing systems, 31, 2018.
[8] K.Cranmer,J.Brehmer,andG.Louppe.Thefrontierofsimulation-basedinference.Proceedings
of the National Academy of Sciences, 117(48):30055–30062, 2020.
[9] P.J.DiggleandR.J.Gratton. Montecarlomethodsofinferenceforimplicitstatisticalmodels.
Journal of the Royal Statistical Society. Series B (Methodological), 46(2):193–227, 1984.
[10] J. H. J. Einmahl and D. M. Mason. Generalized Quantile Processes. The Annals of Statistics,
20(2), jun 1992.
[11] G. Fasano and A. Franceschini. A multidimensional version of the Kolmogorov–Smirnov test.
Monthly Notices of the Royal Astronomical Society, 225(1):155–170, mar 1987.
[12] A.Glazer,M.Lindenbaoum,andS.Markovitch. Learninghigh-densityregionsforageneralized
kolmogorov-smirnov test in high-dimensional data. Advances in Neural Information Processing
Systems, 1:728–736, 2012.
[13] I.Goodfellow,J.Pouget-Abadie,M.Mirza,B.Xu,D.Warde-Farley,S.Ozair,A.Courville,and
Y.Bengio. Generativeadversarialnets. InZ.Ghahramani,M.Welling,C.Cortes,N.Lawrence,
and K. Weinberger, editors, Advances in Neural Information Processing Systems, volume 27.
Curran Associates, Inc., 2014.
[14] W. Grathwohl, R. T. Chen, J. Bettencourt, I. Sutskever, and D. Duvenaud. Ffjord: Free-form
continuousdynamicsforscalablereversiblegenerativemodels. arXivpreprintarXiv:1810.01367,
2018.
11[15] A. Gretton, K. M. Borgwardt, M. J. Rasch, B. Schölkopf, and A. Smola. A kernel two-sample
test. Journal of Machine Learning Research, 13(25):723–773, 2012.
[16] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. C. Courville. Improved training of
wasserstein gans. Advances in neural information processing systems, 30, 2017.
[17] J.Hermans,A.Delaunoy,F.Rozet,A.Wehenkel,V.Begy,andG.Louppe.Acrisisinsimulation-
based inference? beware, your posterior approximations can be unfaithful. Transactions on
Machine Learning Research, 2022.
[18] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter. Gans trained by a two
time-scale update rule converge to a local nash equilibrium. Advances in neural information
processing systems, 30, 2017.
[19] J. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. Advances in neural
information processing systems, 33:6840–6851, 2020.
[20] P.Hu,B.Shuai,J.Liu,andG.Wang. Deeplevelsetsforsalientobjectdetection. Proceedings -
30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, 2017-Janua:
540–549, 2017.
[21] R. J. Hyndman. Computing and graphing highest density regions. The American Statistician,
50(2):120–126, 1996.
[22] A. Hyvärinen. Estimation of non-normalized statistical models by score matching. Journal of
Machine Learning Research, 6(24):695–709, 2005.
[23] T. Kim and Y. Bengio. Deep directed generative models with energy-based probability
estimation. arXiv preprint arXiv:1606.03439, 2016.
[24] I. Kobyzev, S. J. Prince, and M. A. Brubaker. Normalizing flows: An introduction and review
of current methods. IEEE transactions on pattern analysis and machine intelligence, 43(11):
3964–3979, 2020.
[25] A. Krizhevsky. Learning multiple layers of features from tiny images. 2009.
[26] R. Kumar, S. Ozair, A. Goyal, A. Courville, and Y. Bengio. Maximum entropy generators for
energy-based models. arXiv preprint arXiv:1901.08508, 2019.
[27] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document
recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.
[28] C.-L. Li, W.-C. Chang, Y. Cheng, Y. Yang, and B. Póczos. Mmd gan: Towards deeper
understanding of moment matching network. Advances in neural information processing
systems, 30, 2017.
[29] Y. Li, K. Swersky, and R. Zemel. Generative moment matching networks. In International
conference on machine learning, pages 1718–1727. PMLR, 2015.
[30] S.Lyu. Interpretationandgeneralizationofscorematching. InProceedings of the Twenty-Fifth
Conference on Uncertainty in Artificial Intelligence, pages 359–366, 2009.
[31] T. Miyato, T. Kataoka, M. Koyama, and Y. Yoshida. Spectral normalization for generative
adversarial networks. In International Conference on Learning Representations, 2018.
[32] Y. Mroueh and T. Sercu. Fisher gan. Advances in neural information processing systems, 30,
2017.
[33] S. Nowozin, B. Cseke, and R. Tomioka. f-gan: Training generative neural samplers using
variational divergence minimization. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and
R. Garnett, editors, Advances in Neural Information Processing Systems, volume 29. Curran
Associates, Inc., 2016.
12[34] G. Papamakarios, E. Nalisnick, D. J. Rezende, S. Mohamed, and B. Lakshminarayanan.
Normalizing flows for probabilistic modeling and inference. Journal of Machine Learning
Research, 22(57):1–64, 2021.
[35] J. A. Peacock. Two-dimensional goodness-of-fit testing in astronomy. Monthly Notices of the
Royal Astronomical Society, 202(3):615–627, mar 1983.
[36] W.Polonik. Minimumvolumesetsinstatistics: Recentdevelopments. InR.KlarandO.Opitz,
editors, Classification and Knowledge Organization, pages 187–194, Berlin, Heidelberg, 1997.
Springer Berlin Heidelberg.
[37] W. Polonik. The silhouette, concentration functions and ml-density estimation under order
restrictions. The Annals of Statistics, 26(5):1857–1877, 1998.
[38] W. Polonik. Concentration and goodness-of-fit in higher dimensions: (Asymptotically)
distribution-free methods. Annals of Statistics, 27(4):1210–1229, 1999.
[39] A. Radford, L. Metz, and S. Chintala. Unsupervised representation learning with deep
convolutional generative adversarial networks. In Y. Bengio and Y. LeCun, editors, 4th
International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico,
May 2-4, 2016, Conference Track Proceedings, 2016.
[40] P. Ramesh, J.-M. Lueckmann, J. Boelts, Á. Tejero-Cantero, D. S. Greenberg, P. J. Goncalves,
and J. H. Macke. GATSBI: Generative adversarial training for simulation-based inference. In
International Conference on Learning Representations, 2022.
[41] T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and X. Chen. Improved
techniques for training gans. Advances in neural information processing systems, 29, 2016.
[42] Y. Song and S. Ermon. Generative modeling by estimating gradients of the data distribution.
Advances in neural information processing systems, 32, 2019.
[43] Y. Song and D. P. Kingma. How to train your energy-based models. arXiv preprint
arXiv:2101.03288, 2021.
[44] A. Srivastava, L. Valkov, C. Russell, M. U. Gutmann, and C. Sutton. Veegan: Reducing mode
collapse in gans using implicit variational learning. Advances in neural information processing
systems, 30, 2017.
13A Proofs
Theorem 1 (Necessary and sufficient conditions). Let v be a measure on ( , ). Suppose
that P and P are probability measures on ( , ) with densities (with refX ereA nce measure
F G
X A
v) f and g respectively. Assuming that
A.1 Π Π ;
f g
∪ ⊂C
A.2 CP F,C(α) and CP G,C(α) are uniquely determined6 in
C
with respect to v
the following two statements are equivalent:
S.1 P =P ;
F G
S.2 D (P ,P )=0.
GKS F G
Proof of Theorem 1. The S.1 = S.2 direction is trivial to show and works without
⇒
satisfying the assumptions [38]. Therefore, we focus on showing that S.2 = S.1. Let
⇒
S (P)= (v(C),P(C)):C IR [0,1], (15)
C +
{ ∈C}⊂ ×
and denote with Γ(λ) the level set of density of P as defined in eq. (6), and let Π:= Γ(λ):
λ⩾0 . Further, let S˜ denote the least concave majorant [5] to S (P), that is, the sm{ allest
C C
conca} ve function from IR to [0,1] lying above S (P). S˜ is supported on the generalized
+ C C
quantiles of P in , i.e. on the points (v(CP,C(α)),P(CP,C(α))). Finally, let ∂S˜ C(P) be the
C
intersection of the extremal points of the convex hull of S (P) with the graph of S˜ . Given
C C
Π which we assume in A.1 for P and P , and in the light of remark 2 we have that
F G
for⊂ anC y set C such that (v(C),P(C)) ∂S˜ (P) there is a level λ for which C =Γ(λ), and it
C
is equal the left-hand derivative of
S˜∈
in the point v(C). From remark 1, we have that the
C
silhouette fully characterizes P, and therefore ∂S˜ (P) does it as well.
C
Eventually, we conclude the proof with the observation that given S.2, under Lemma
2.1 of Polonik [38] (where A.2 is utilized) we have that the extremal points of the convex
hulls of S C(P F) and S C(P G) are the same points, thus ∂S˜ P F(P) = ∂S˜ P G(P), and finally
P =P .
F G
Theorem 2 (Relaxation of assumption A.1). Theorem 1 holds if assumption A.1 is relaxed
to the case that contains sets that are uniquely determined with density level sets of P
F
and P up to a C set C such that
G
P (C′)=P (C′), (8)
∀C′∈2C F G
and let r :=P (C)=P (C), then the supremum in statement S.2 is restricted to [0,1 r].
F G
−
Proof of Theorem 2. The statement in eq. (8) is equivalent to saying that P = P on
F G
(C,2C). Analogously to the proof of theorem 1 we can show that P =P on ( C,2X\C).
F G
Byobservingthatprobabilitymeasuresareσ-additive,weconcludethatP =PX \ on( , ),
F G
X A
and thus the result of theorem 1 holds.
6InthesensedefinedinPolonik[38]
14A.1 Generalized KS distance satisfies triangle inequality
Let us consider three probability measures P , P , and P on a measurable space ( , ).
F G H
X A
D (P ,P )+D (P ,P )
GKS F H GKS H G
= sup [P (C(α)) P (C(α))]+ sup [P (C(α)) P (C(α))]
F H H G
| − | | − |
α∈[0,1] α∈[0,1]
C∈{CPF,C,CPH,C} C∈{CPH,C,CPG,C}
( =i) sup [P (C(α)) P (C(α))]+ sup [P (C(α)) P (C(α))]
F H H G
| − | | − |
α∈[0,1] α∈[0,1]
C∈{CPF,C,CPH,C,CPG,C} C∈{CPH,C,CPG,C,CPF,C}
= sup [P (C(α)) P (C(α))]+[P (C(α)) P (C(α))]
F H H G
| − | | − |
α∈[0,1]
C∈{CPF,C,CPH,C,CPG,C}
(ii)
⩾ sup [P (C(α)) P (C(α))]
F G
| − |
α∈[0,1]
C∈{CPF,C,CPH,C,CPG,C}
= sup [P (C(α)) P (C(α))]=D (P ,P )
F G GKS F G
| − |
α∈[0,1]
C∈{CPG,C,CPF,C}
In (i), we use the fact that the supremum of absolute difference in distribution coverage
is maximized with the generalized quantile function of one of them. In (ii), we apply triangle
inequality for absolute value. Thus we have shown that D (P ,P )+D (P ,P )⩾
GKS F H GKS H G
D (P ,P ) which is the triangle inequality for the Generalized KS distance.
GKS F G
A.2 Objective for the critic
Given two adversarial maximum likelihood objectives from Kim and Bengio [23], we (i) set
c (x):= c (x), and (ii) repurpose P as P and P as P , and show that:
ϕG
−
ϕF G aux(F) F aux(G)
1 1
2(E x∼P F[ −c ϕF(x)] −E x∼P aux(F)[ −c ϕF(x)])+ 2(E x∼P G[ −c ϕG(x)] −E x∼P aux(G)[ −c ϕG(x)])
1
= 2(E x∼P F[ −c ϕ(x)] −E x∼P G[ −c ϕ(x)]+E x∼P G[c ϕ(x)] −E x∼P F[c ϕ(x)])
=E x∼P G[c ϕ(x)] −E x∼P F[c ϕ(x)].
B Experiments details
In this section, we provide additional details about experiments conducted in the paper that
did not fit in the main text. All the models reported in the paper were trained under 12
hours on a single Nvidia GeForce GTX TITAN X GPU (12GB vRAM) with 32GB of RAM
and 2 CPU cores. We report results based on 645 models trained, which amounts to 7740
GPU hours at most. We estimate that about three times as much computing time was used
for preliminary experiments not reported in the paper.
B.1 Synthetic
The synthetic 2D distributions are adopted from the official code of Grathwohl et al. [14] –
https://github.com/rtqichen/ffjord. We randomly generate 65536 training and 65536
test instances from each distribution. In appendix D.1, we report the results of training the
models with fewer instances but evaluated using the entire test set.
We choose the bandwidth of the Gaussian filter in squared population MMD as the
median of L2 pairwise distances between 65536 instances sampled from the simulator. The
resulting values can be found in the code we provide with the paper.
15Table 4: Architectures for synthetic 2D datasets.
z IR8 (0,I)
∈ ∼N Linear(bias=True), 2 512
Linear(bias=True), 8 512 →
→ LeakyReLU(slope=0.2)
ReLU
Linear(bias=True), 512 512
Linear(bias=True), 512 512 →
→ LeakyReLU(slope=0.2)
ReLU
Linear(bias=True), 512 512
Linear(bias=True), 512 512 →
→ LeakyReLU(slope=0.2)
ReLU
Linear(bias=True), 512 1
Linear(bias=True), 512 2 →
→ (b) Critic
(a) Generator
B.2 MNIST
To detect the modes in the (3Stacked)MNIST experiments, we use a pre-trained classifier
from PyTorch examples, trained for 14 epochs of the train set of the original MNIST dataset.
We expect to find 10 and 1000 modes for the MNIST and 3StackedMNIST, respectively. We
measure the KL divergence between the classifier’s output and discrete uniform distribution
for both distributions.
B.3 CIFAR-10
We sample 32768 instances from reach model. We compute the Inception Score using
theimplementationfromhttps://github.com/sbarratt/inception-score-pytorch. We
compute the Fréchet inception distance using the implementation from https://github.
com/mseitzer/pytorch-fid.
C Architectures and hyper-parameters
C.1 Synthetic
For all of the methods and distributions, we use the same architecture, described in table 4,
with spectral normalization [31] on linear layers for GAN. In all cases, we train the generator
and critic with Adam(β =0.5, β =0.9) optimizer with a constant learning rate of 0.0001,
1 2
without L2 regularization or weight decay, for 128000 generator updates with batch size
equal to 512. We use the standard loss for GAN, enforcing class 1 for real samples and 0 for
generated samples. In WGAN-GP, we use 0.1 weight on gradient penalty (identified as a
good value in preliminary experiments, which we do not report), and in KSGAN β =1.0 as
the weight for score penalty.
C.2 MNIST
For the MNIST experiments, we use the DCGAN [39] architecture, without batch normal-
ization layers, with 128-dimensional latent Gaussian distribution. For the 3StackedMNIST
distribution,weincreasethenumberofinputandoutputchannelsforthecriticandgenerator,
respectively. We train the generator and critic with Adam(β = 0.5, β = 0.9) optimizer
1 2
with a constant learning rateof 0.0001, without L2 regularizationor weightdecay, for 200000
generator updates with batch size equal to 50. In the case of GAN for 3StackedMNIST, we
use a learning rate of 0.001 (identified as a good value in preliminary experiments, which we
do not report). We use the
16swissroll circles rings moons
0.008
0.006
0.004
0.002
0.000
−0.002
8gaussians pinwheel 2spirals checkerboard
0.008
0.006
0.004
0.002
0.000
−0.002
103 104 103 104 103 104 103 104
#traininginstances #traininginstances #traininginstances #traininginstances
GAN(5,1) WGAN-GP(5,1) KSGAN(1,1)
Figure 2: Squared population MMD between approximate and test distribution as a function
of the number of training instances. Solid lines denote the average over five random
initializations, and the shaded area represents the two-σ interval. Best viewed in color.
flipped loss for GAN, enforcing class 0 for real samples and 1 for generated samples. In
WGAN-GP, we use 10.0 weighton gradient penalty (identified as agood value in preliminary
experiments,whichwedonotreport),andinKSGANβ =1.0astheweightforscorepenalty.
C.3 CIFAR-10
For the CIFAR-10 experiments, we use ResNet architecture from Gulrajani et al. [16]. We
train the generator and critic with Adam(β = 0.0, β = 0.9) optimizer with a constant
1 2
learning rate of 0.0001, without L2 regularization or weight decay, for 199936 generator
updates with batch size equal to 64. We use the
flipped loss for GAN, enforcing class 0 for real samples and 1 for generated samples. In
WGAN-GP, we use 10.0 weighton gradient penalty (identified as agood value in preliminary
experiments,whichwedonotreport),andinKSGANβ =1.0astheweightforscorepenalty.
D Extended results
In this section, we report additional experiment results that did not fit in the main text.
This includes materials allowing a qualitative comparison of the trained models.
D.1 Synthetic data
In fig. 2, we report, extended relative to table 2 in the main text, a study of the quality
of trained models as measured by the squared population MMD. Solid lines denote the
average over five random initializations, and the shaded area represents the two-σ interval.
KSGAN performs on par with WGAN-GP while being trained with a five times less training
budget. Infig.3, weshowthehistogramsof65536samplesfromthemodels(asinglerandom
initialization), with a histogram of test data in the first column for reference. For KSGAN,
in addition to the configurations included in table 2, we include one with a training budget
matching that of GAN and WGAN-GP, and one with a training budget reduced by two,
where the critic is updated only every second update of the generator.
17
DMMnoitalupopderauqS
DMMnoitalupopderauqSFigure 3: Histograms of samples from distributions denoted on the top. Heatmap colors are
shared for all figures in each row. Best viewed in color.
18D.2 MNIST
In fig. 4, we show samples from one of the random initializations reported in table 2 in the
main text. All models demonstrate similar sample quality, while for GAN, the digit “1” is
over-represented, which corresponds with the high KL in table 2.
D.3 CIFAR-10
In fig. 5, we show samples from one of the random initializations reported in table 3 in the
main text. All models demonstrate similar, low sample quality.
19(a) GAN (1, 1)
(b) WGAN-GP (1, 1)
(c) KSGAN (1, 1)
Figure 4: Samples from the respective models trained on the MNIST dataset.
20(a) GAN (1, 1)
(b) WGAN-GP (1, 1)
(c) KSGAN (1, 1)
Figure 5: Samples from the respective models trained on the CIFAR-10 dataset. Best viewed
in color.
21