LLaVolta: Efficient Multi-modal Models
via Stage-wise Visual Context Compression
JienengChen*,LuoxinYe*,JuHe,Zhao-YangWang,DanielKhashabi‚Ä†,AlanYuille‚Ä†
Johns Hopkins University
Abstract
Whilesignificantadvancementshavebeenmadeincompressedrepresentationsfortextembeddingsin
largelanguagemodels(LLMs),thecompressionofvisualtokensinlargemulti-modalmodels(LMMs)
hasremainedalargelyoverlookedarea. Inthiswork,wepresentthestudyontheanalysisofredundancy
concerningvisualtokensandefficienttrainingwithinthesemodels. Ourinitialexperimentsshowthat
eliminating up to 70% of visual tokens at the testing stage by simply average pooling only leads to
a minimal 3% reduction in visual question answering accuracy on the GQA benchmark, indicating
significant redundancy in visual context. Addressing this, we introduce Visual Context Compressor,
which reduces the number of visual tokens during training to enhance training efficiency without
sacrificingperformance. Tominimizeinformationlosscausedbythecompressiononvisualtokenswhile
maintainingtrainingefficiency,wedevelopLLaVoltaasalitetrainingscheme. LLaVoltaincorporates
stage-wisevisualcontextcompressiontoprogressivelycompressthevisualtokensfromheavilytolightly,
andfinallynocompressionattheendoftraining,yieldingnolossofinformationwhentesting. Extensive
experimentsdemonstratethatourapproachenhancestheperformanceofMLLMsinbothimage-language
andvideo-languageunderstanding,whilealsosignificantlycuttingtrainingcosts.
Website https://beckschen.github.io/llavolta.html
Code https://github.com/Beckschen/LLaVolta
1. Introduction
TheadventofLLMs[31,32,40]hasmarkedanewerainthefieldofartificialintelligenceandnatural
languageprocessing. LLMscanplayaroleasauniversalinterfaceforageneral-purposeassistant,where
varioustaskinstructionscanbeexplicitlyrepresentedinlanguageandguidetheend-to-endtrainedneural
assistanttosolveataskofinterest. Forexample,therecentsuccessofChatGPT[31]andGPT-4[32]
have demonstrated the power of aligned LLMs in following human instructions, and have stimulated
tremendous interest in developing open-source LLMs [39, 41]. As the horizon of LLM applications
broadens and the availability of open-source LLMs increases, the integration of multi-modality into
thesemodelspresentsanewfrontierinexpandingtheircapabilities. Multi-modalLLMs[1,26,38,51]
(MLLMs),whichcanprocessandunderstandnotjusttextbutalsovisualinformation,standatthecutting
edgeofthisevolution.
WhileMLLMshavemadesignificantstrides,acrucialaspectthatremainsrelativelyunexploredisthe
efficientrepresentationandprocessingofvisualinformationwithinthesemodels. Substantialefforts[17,
*Equallycontributed
‚Ä†Equallyadvised
4202
nuJ
82
]VC.sc[
1v29002.6042:viXra(a) Performance vs. Visual Token Compression Rate (b) Attention to Visual Tokens vs. System Prompt Tokens
Figure1 | VisualtokensareredundantinMLLMs. Left: TheaccuracyoftheLLaVA-1.5-7B[26]modelon
theGQA[19]benchmarksvarieswithdifferentpercentagesofretainedvisualtokens. Theùë•-axisrepresentsthe
percentageoforiginalvisualtokenspreservedafterapplying1DaveragepoolingwithvaryingstridesizesùëÜapplied
inùëñ-thTransformerlayer. Right: Visualtokensreceivelessattentionfromthe[ANS]tokenaswegodeeperinto
itslayersofLLaVA-1.5-7Bmodel. Thesefindingscollectivelysuggestasignificantredundancywithinthevisual
tokensoftheMLLMs.
33, 50] have been dedicated to optimizing the efficient representation of text tokens through various
compressiontechniques[17,33,50],aimedatenhancinginferenceefficiencybyattentivelyselecting
importanttokens. However,theefficientlearningofvisualtokensinMLLMhasnotgarneredcomparable
attention. Naturally,thisraisesquestionsaboutthepotentialredundancypresentinvisualtokensandits
implicationsfortheoverallcomputationalefficiencyofMLLMs.
Westartourworkbyaddressingthequestion: Arevisualtokensredundantinmulti-modalLLMs?
Toexplorethis,wefirstexperimentwithsimplyreducingthenumberofvisualtokensinapre-trained
LLaVA-1.5-7B[26]attheinferencestageviaaveragepooling(¬ß3). AsshowninFig.1(left),ourinitial
resultsdemonstratethateliminatingupto70%ofvisualtokensbypoolingthemwithastrideof4starting
fromTransformerlayer2incursonlyaminimalperformancelossontheGQAbenchmark,specifically
a3%accuracyreduction. Additionally,wecomputeandpresenttheaverageattentionvaluesfromthe
[ANS] token to visual tokens and system prompt tokens across different Transformer layers in the
pre-trainedLLaVA-1.5-7B[26]. AsrevealedinFig.1(right;bluetrends),thevisualtokensaregenerally
lessattendedto,measuredbasedonaverageattentionfromthe[ANS]token,asthelayersgetdeeper.
Thesetwoearlyexplorationsindicatesignificantredundancyinvisualtokens.
Addressingthis,inthisworkwedevelopaneffectiveVisualContextCompressorthatcanbeintegrated
into the training of MLLMs. Surprisingly, a simple average pooler nested in LLMs stands out as the
mosteffectivecompressor,outperformingtheattention-based[17,50]andparametric[22]counterparts.
Weattributethistotworeasons: (1)Thesimplepoolingoperationmakestrainingstable,whereasprior
attention-based approaches [17, 50] are specifically designed for accelerating inference rather than
training. (2) Visual tokens in the deeper Transformer layers are less attended to (see Fig. 1 (right))
andparticularlyredundant,makingasimplecompressorplacedinadeeperTransformerlayereffective
enough. At a lower training cost, the LLaVA-1.5-7B [26] trained with the proposed Visual Context
Compressoriscompetitivewiththenon-compressedbaselineacrossvariousmulti-modalbenchmarks
(e.g., GQA [19] and MM-Vet [47]). This dual achievement highlights Visual Context Compressor‚Äôs
roleasapivotaladvancementinenhancingtheefficiencyandperformanceofMLLMsacrossvarious
multi-modalquestion-answeringbenchmarks.
To further mitigate the information loss caused by compressing visual tokens, especially under
a large compression ratio (CR), we have devised a LLaVA-powered lite training scheme, dubbed
2LLaVolta, which progressively employs Visual Context Compressor at multiple training stages with
differentcompressionratios(¬ß3.3). Specifically,LLaVoltaprogressesthroughseveralstages,beginning
withahighlevelofvisualtokencompressionandgraduallyreducingthecompressionratiountilthefinal
stages,wherefullvisualtokensareutilized. Thismulti-stageapproachallowsforadaptivecompression
levelsthatensuretrainingefficiencywithoutlosinginformationattesting,thusmaintainingtheoverall
effectivenessofthemodel.
Extensive experimental evaluations of LLaVolta have been conducted on thirteen widely-adopted
MLLMbenchmarksforbothimage-languageunderstandingandvideo-languageunderstanding,showing
promisingresults. WeobservethatLLaVoltanotonlyenhancestheperformanceofMLLMs,butalso
achievesasubstantialreductionintrainingcosts. Theseexperimentsvalidatetheeffectivenessofour
method,demonstratingitscapabilitytooptimizeresourceutilizationwhilemaintainingorevenimproving
modelperformance.
Insummary,ourpapermakesthefollowingcontributions:
‚Ä¢ WepresenttwoinitialstudiestoverifytheredundancyofvisualtokensinMLLMs.
‚Ä¢ We propose the Visual Context Compressor, a simple yet effective compression technique that
utilizesanaveragepooler,enhancingtheefficiencyofmulti-modalmodels.
‚Ä¢ WeproposetheLLaVoltaasanefficienttrainingschemebyleveragingVisualContextCompressor
atmultipletrainingstageswithaprogressivelydecreasingcompressionratio. Tothebestofour
knowledge,weareamongthefirsttoexploreefficienttrainingofMLLMs.
‚Ä¢ ExtensiveexperimentsshowthatourapproachnotonlyimprovestheperformanceofMLLMsin
image-languageandvideo-languageunderstandingacrossvariousbenchmarksbutalsoshowcases
efficiencygainsbyreducingtrainingcostsby16%.
2. Related Works
Multi-modalLLMs. Theevolutionoflargelanguagemodels[9,31,32]intotheirmulti-modalcounter-
parts[26,38]representsasignificantleapintheirabilitytofollowinstructionsandgeneralizeacrosstasks.
ThistransitionhasbeenmarkedbyseminalworkssuchasFlamingo[1],BLIP-2[22]andLLaVA[26],
whichhaveextendedLLMcapabilitiestoencompassvisualtasks,demonstratingimpressivezero-shot
generalization and in-context learning abilities. Progress in multi-modal LLMs has primarily been
drivenbyadvancementsinvisualinstructiontuning[26,51],leveragingvision-languagedatasetsand
refiningvisualinstruction-followingdata. Additionally,effortshavebeenmadetoenhancethegrounding
capabilitiesofmulti-modalLLMsthroughtheuseofspecializeddatasetsaimedatimprovingtask-specific
performance. Despitetheseadvancements,theexplorationofvisualcompressionwithinmulti-modal
LLMsremainsrelativelyunderdeveloped. Thedesignandoptimizationofcompressionstrategiesare
crucialformaximizingtheeffectivenessandefficiencyofmulti-modalLLMs,suggestingapotentialarea
forfutureresearchanddevelopment.
VisualRedundancy. Incomputervision,reducingredundancyiscrucialforcreatingefficientyet
effective models without losing accuracy [4]. Redundancy in images often arises from the inherent
characteristicsofnaturalscenes,includingrepetitivepatterns,textures,andareasofuniformcolor. These
features,whilecontributingtotherichnessanddetailofvisualperception,canleadtoinefficienciesin
bothstorageandprocessingwhennotadequatelyaddressed. Imagecompressionalgorithms[43]can
reducefilesizebyeliminatingorefficientlyencodingredundantdata. Thesemethodstakeadvantage
of human visual perception‚Äôs tolerances to subtly reduce data without significantly impacting image
quality. Advancedmachinelearningmodels,particularlyCNNsandautoencoders[3],offersophisticated
approachestominimizingredundancy. Transformers[42],asafundamentalarchitectureforLLMs[9,
32],applyself-attentionmechanismstodynamicallybindthemostinformativepartsoftokents. Vision
Transformers[6,7,11,15]trainedwithCLIPobjective[7,34]encodeanimagetoasequenceofvisual
features for multi-modal LLMs [26]. Nevertheless, visual tokens receive less attention in LLMs due
3to attention shrinkage [44], resulting a waste of computation. In this work, we focus on reducing the
redundancyofvisualtokensinMLLMs.
Efficient LLMs. Efficient inference and training for LLMs are important. Compressing input
sequencesforefficiencyreasonsinTransformersisnotanewideaforNLP.Muchworkisbeingdoneto
acceleratetheinferenceofLMs. Forexample,PyramidTransformervariants[10]and [18]areproposed
inEncoder-DecoderLMsthatprogressivelycompressthesequenceasthelayersgrowdeeperviapooling
or core-set selection. Nawrot et al. [30] propose adaptively compressing the sequence based on the
predictedsemanticboundarieswithinthesequence. Raeetal.[35]proposecompressingthefine-grained
pastactivationstocoarsermemories. VCC[50]compressthesequenceintoamuchsmallerrepresentation
ateachlayerbyprioritizingimportanttokens. Besidesefficientinference,acceleratingtrainingforLLMs
attracts attention as well. A staged training setup [36] is proposed which begins with a small model
andincrementallyincreasestheamountofcomputeusedfortrainingbyapplyingagrowthoperatorto
increasethemodeldepthandwidth. However,efficienttrainingforLLMsinmulti-modalscenariosis
rarelyexplored.
3. Method
Inthissection,wefirstintroduceanoverviewofmulti-modalLLMsin¬ß3.1. Then,wedefinetheproblem
ofvisualredundancyandintroduceVisualContextCompressorin¬ß3.2. Finally,wepresentourproposed
LLaVoltain¬ß3.3.
3.1. Preliminaries: AMulti-modalLLM
WestartbyreviewingthedesignoftheLLaVAfamily[25,26]. ForprocessinganinputimageXùë£,we
utilizethepre-trainedCLIPvisualencoderViT-L/14, asdetailedby[34], toextractthevisualfeature
Zùë£ = ùëî(Xùë£), where ùëî(.) indicates the visual encoder. To bridge the gap between visual and linguistic
modalities, the LLaVA [25, 26] framework as an MLLM implements a straightforward linear/MLP
transformation. ThisinvolvesatrainableprojectionmatrixW,whichmapsthevisualfeaturesZùë£ into
thelinguisticembeddingspace, producinglanguageembeddingtokensHùë£ = WZùë£. Thesetokensare
designedtomatchthedimensionalityofthewordembeddingswithintheLLM.
ForeachimageXùë£,onecangeneratemulti-turnconversationdata (X1 ùëû,X1 ùëé,¬∑¬∑¬∑ ,Xùëá ùëû,Xùëá ùëé) withùëá asthe
numberofturns. Onecanorganizethemasasequence,bytreatingallanswersastheassistant‚Äôsresponse
andtheinstructionXùë° attheùë°-thturnas:
instruct
Xùë° =
(cid:26) RandomChoose[X1 ùëû,Xùë£] or [Xùë£,X1 ùëû], ùë° = 1
(1)
instruct Xùë° , ùë° > 1
ùëû
Thisapproachestablishesastandardizedformatforthemulti-modalinstruction-followingsequence.
Itallowsfortheinstruction-basedtuningoftheLLMtobeappliedtothepredictiontokens,utilizingthe
model‚Äôsnativeauto-regressivetrainingobjective. Specifically,forasequencewithlengthùêø,thelikelihood
ofthetargetresponsesXùëé iscalculatedas:
ùêø
(cid:214)
ùëù(Xùëé|Xùë£,Xinstruct) = ùëù ùúÉ(ùë• ùëñ|Xùë£,Xinstruct,<ùëñ,Xùëé,<ùëñ), (2)
ùëñ=1
3.2. VisualContextCompressor
ProblemFormulation: Theredundancyobservedinimagesoftenarisesfrominherenttraitsofnatural
scenes,includingrepetitivepatterns,textures,andregionswithuniformcolor. Whilethesetraitsenrich
4visualperceptionbyofferingdetailanddepth,theycanalsopresentchallengesintermsofstorageand
processingefficiency. ConsideringtheinherentlimitationsofTransformersinhandlinglongsequences[2,
27,46],itiscriticaltominimizeanylengthredundanciestoobtainamoreeffectiveaccuracy/efficiency
trade-off.
TheobjectiveofthisstudyistodecreasethelengthofvisualtokensXùë£ (i.e.,itshiddenstatesHùë£ ifin-
sideLLMs),whilesimultaneouslymaximizingtheprobabilityofthetargetresponse ùëù(Xùëé|Xùë£,Xinstruct)
asdescribedinEquation(2).
Visual Context Compressor: A key design change that we introduce is a compressor layer that
compresses the dimensions of the visual inputs by reducing the effective number of visual tokens.
As depicted in Fig. 2, the compressor is simply an average pooler in our setting. It is applied to
the visual tokens in ùëò-th Transformer layer of an LLM. Formally, given the hidden visual tokens at
ùëò-th Transformer layer Hùëò ‚àà Rùêµ√óùê∂√óùêø, the compressor is expected to fulfill the following projection:
ùëì : Rùêµ√óùê∂√óùêø ‚Ü¶‚Üí Rùêµ√óùê∂√óùêø out,whichresultsincompressedvisualtokens
Hùëò ‚àà Rùêµ√óùê∂√óùêø out, where ùêø
out
= ùëÜùêø with ùë† as the compression stride. In ¬ß4, we explore multiple
variantsofcompressor ùëì toreducethetokenlength,includingrandomtokendropping[16]withdropping
ratio 1‚àí ùëÜ1, K-Means [20] with number of centroids set to ùëÅ ùê∂ = ùëÜùêø , attention-based token-centric
compression [50], attention-based token dropping [8, 17], and average pooling with stride ùë†. To our
surprise,wefindthatthesimpleaveragepooleristhemosteffectivecompressorforvisiontokenswithin
MLLMs, due to its stability during training detailed in ¬ß 4.4. Thus, we choose average pooler as the
compressor.
Note that the proposed Visual Context Compressor can be directly applied to any off-the-shelf
MLLMstoassessthevisualredundancy,asconductedin¬ß4.2. OnecanalsotrainanMLLMwithVisual
ContextCompressortoreducethenumberofvisualtokenswhilemaintainingcompetitivemulti-modal
performance.
Compression Ratio (CR)‚Ä°. For an LLM with ùëÅ "What is the
Transformer decoder layers, the compression ratio for dog up to?"
visualtokenscanbecalculatedas:
visual token + text token
ùëÅ ¬∑ùêø
CR = (ùëÅ‚àíùêæ)¬∑ùêø +ùêæ¬∑ùêø , (3) layer 1
out
where ùêæ isthe ùêæ-thTransformerlayerofamulti-modal LLM layer 2
‚Ä¶‚Ä¶
LLM;ùêøisthethelengthofvisualtokensinputintoVisual
layer K
Context Compressor; ùêø is the compressed length of ùêø
out
Compression Ratio
visual tokens generated by Visual Context Compressor,
ùëÅ"ùêø Visual Compressor
asillustratedinFig.2. ùëÅ‚àíùêæ "ùêø!"#+ùêæ"ùêø ùêø
!"#
Ourarchitecturemodificationsthusfarmostlyimpacts layer K+1
theinferenceefficiencyofMLLM,however,itsimpacton
‚Ä¶‚Ä¶
performance-compressiontrade-offremainsunclear. We
willstudythisquestioninthecontextoftrainingMLLMs layer N
withagoalofenhancingefficiencywithoutcompromising
ANS
performance. WethenmoveontofurtherutilizeVisual
ContextCompressortodesignanefficienttrainingscheme
Figure2 | ExampleofVisualContextCompres-
to incorporates Visual Context Compressor at various
sorinamulti-modalLLM.
stagesofthetrainingprocess.
‚Ä°DefinitionofcompressionratiofromWikipedia
53.3. LLaVoltaasaLiteTrainingScheme
Training with Visual Context Compressor not only facilitates efficient inference but also enhances
trainingefficiency. However,devisinganeffectivetrainingschemeposeschallengeswhenensuringfair
comparisonswiththeoriginalLLaVA[25],primarilyduetodifferencesinthenumberoftokensinvolved
ininference. Thisdiscrepancymayleadtoinformationloss,particularlywhenoperatingunderascenario
withahighcompressionratio. Totacklethisissue,wehavedevelopedalitetrainingschemeforLLaVA,
dubbedasLLaVolta,whichemploysstage-wisevisualcontextcompression. Generally,assumingthere
are ùëÅ ùë† totalstages,stageùëñinvolves ùëÅ1
ùë†
ofthetotaltrainingepochswithacompressionratioofùëü ùëñ,andthe
finalstageproceedswithoutanycompression. Essentially, astrainingprogresses, ùëñ increaseswhileùëü ùëñ
decreases.
Inthiswork,asdepictedinFig.3,weprimarilyexploreathree-stagetrainingpipelinethatprogres-
sivelyreducesthecompressionratio,asdetailedbelow:
TrainingStageI:HeavyCompression. TheMLLMtrainingatthefirstone-thirdofthetotaltraining
iterationscommenceswithaheavycompressionratio(>500%),whereVisualContextCompressor is
appliedinanearlylayeroftheLLMwithalargepoolingstride. Thissetupenablesaveryfasttraining
speed.
TrainingStageII:LightCompression. TheMLLMcontinuestrainingwithanotherone-thirdofthe
totaltrainingepochs. Atthisstage,VisualContextCompressorisappliedatonlythedeeperlayersofthe
LLMwithasmallerpoolingstridecomparedtoTrainingStageI.
TrainingStageIII:NoCompression. TheMLLMcontinuestrainingwiththefinalone-thirdofthe
totaltrainingepochs,followingthestandardMLLMtrainingprotocolwithoutcompression. Disabling
compression in the final stage ensures that the number of tokens remains consistent with the original
MLLMduringinference,avoidingthelossofinformationcausedbythereductionofvisualtokens.
Giventheabovemetaframework,wecaninstantiateafamilyoftrainingschemes,asdemonstrated
inTab.1. Thesingle-stage(non-compression)schemeisequivalenttotheMLLMbaseline. Formulti-
stagetraining,thecompressionstagecaneithergodeeperorwider. ‚Äúdeeper‚Äùimpliesanincreasein ùêæ
(Transformerlayer),while‚Äúwider‚Äùmeansadecreaseinthestrideofthepooler.
#Stages Scheme Stage Layer Stride CR #Epoch #Stages Scheme Stage Layer Stride CR #Epoch
Single nocompression ùëÜ1 / / 100% 1 ùëÜ1 2 8 557% 0.25
Two compression ùëÜ1 2 8 557% 0.5 ùëÜ2 2 2 188% 0.25
ùëÜ2 / / 100% 0.5 Four widerthendeeper
ùëÜ3 16 2 133% 0.25
ùëÜ1 2 8 557% 0.33
ùëÜ4 / / 100% 0.25
Three compr.deeper ùëÜ2 16 8 178% 0.33
ùëÜ1 2 8 557% 0.25
ùëÜ3 / / 100% 0.33
ùëÜ1 2 8 557% 0.33 Four deeperthenwider ùëÜ2 16 8 178% 0.25
Three compr.wider ùëÜ2 2 2 188% 0.33 ùëÜ3 16 2 133% 0.25
ùëÜ3 / / 100% 0.33 ùëÜ4 / / 100% 0.25
Table1 | InstantiationsofLLaVoltaschemes. deeperindicatesthatthecompressor‚ÄôspositionintheLLMshifts
fromtheshallowlayer(e.g.,2)toadeeperlayer(e.g.,16). widerindicatesthatthecompressor‚Äôsstridedecreases
whilethenumberofvisualtokensincreases.
Notethatalltrainingschemeswillbestandardizedtocompletejustoneepoch. Thus,inthethree-stage
training,eachstagewillreceiveonethirdofanepoch,whileinthefour-stagetraining,eachstagewill
receiveonefourthofanepoch. Effectsofnon-uniformstagesplittingarepresentedintheAppendix.
6Stage I Stage II Stage III
first 1/3 of iterations second 1/3 of iterations last 1/3 of iterations
heavy compression light compression no compression
‚ÄùHow many "Are the napkin ‚ÄùHow many
cats are in and the cup the yellow taxis are
this picture?" same color?" on the street?"
visual token + text token visual token + text token visual token + text token
layer 1 layer 1 layer 1
LLM Visual Compressor LLM layer 2 LLM layer 2
CR=800% layer 3 layer 3
layer 2
Visual Compressor layer 4
layer 3
CR=200% layer 5
layer 4 layer 4 ‚Ä¶‚Ä¶
‚Ä¶‚Ä¶ ‚Ä¶‚Ä¶ layer N-1
layer N layer N layer N
ANS wider ANS ANS
Figure 3 | Meta framework of LLaVolta, consisting with multiple training stages: Stage I with heavy visual
compression; Stage II with light visual compression in deeper layer with wider token window; Stage III with
standardMLLMtraining(asthereisalsonocompressioninstandardinference). Thiscanacceleratethetrainingby
16+%whilemaintainingperformance.
4. Experiments
Inthissection,webeginbydetailingtheexperimentalsetupin¬ß4.1. Next,weelaborateontheproof-of-
conceptinSection¬ß4.2. Followingthis,wevalidatetheproposedLLaVoltain¬ß4.3withanablation
studyin¬ß4.4. Finally,weassesstheextensibilitytovideo-languagein¬ß4.5.
4.1. ExperimentalSetup
WeadopttheVicuna-v1.5-7B[9]asthelanguagemodel,leveragingtheLLaMA2codebase[41]. We
leveragethepre-trainedCLIPViT-L/14[11,34]withaninputresolutionof336√ó336,resultingin576
visualtokens. WeemploytheLLaVAframework [25]toconnectthefrozenCLIPvisionencoderand
the Vicuna LLMs. Along with the projector, we train the entire LLM instead of parameter-efficient
finetuning. WefollowLLaVA-1.5 [25]toperformdatapreparationandtrainingscheduleforpretraining
andinstructiontuning. Weconductalltheexperimentswiththemachineof8√óNvidiaRTX6000Ada.
Due to multiple invalid image links in the dataset of instruction tuning stage, the scores of LLaVA-
1.5 reported in our analysis are reproduced by ourselves to ensure a fair comparison under the same
experimentalenvironment.
It is worth mentioning that assessing visual token redundancy only necessitates the inference of
existingoff-the-shelfmodels,whereastheotherexperimentsinvolvethetrainingofmulti-modalLLMs,
specificallyprojectorsandLLMs.
BenchmarksandMetrics: WeadoptthirteenbenchmarksspecificallydesignedforMLLMevalua-
tion,includingGQA[19],MM-Vet[47],ScienceQA(SQA)[29],MME[12],TextVQA[37],POPE[23],
MMBench[28],MMBench-CN[28],VQA-v2[13],LLaVA-Bench-in-the-Wild(LLaVAùëä)[26],VisWiz[14],
SEED-Image[21]andMMMU[49]. GQAandVQA-v2evaluatethemodel‚Äôsvisualperceptioncapabil-
itiesonopen-endedshortanswers. MME-Perceptionevaluatesmodel‚Äôsvisualperceptionwithyes/no
questions. ScienceQAwithmultiplechoiceareusedtoevaluatethezero-shotgeneralizationonscien-
tific question answering. TextVQA contains text-rich visual question answering. MMBench and the
7
repeedCNversionevaluateamodel‚Äôsanswerrobustnesswithall-roundshufflingonmultiplechoiceanswers.
MM-Vetevaluatesamodel‚Äôscapabilitiesinengaginginvisualconversations. Additionally,weextend
LLaVolta to video-language understanding, and follow Video-LLaVA [24] to evaluate the models on
MSVD-QA[5],MSRVTT-QA[45]andActivityNet-QA[48],wheretheaccuracyandscoreareassessed
usingGPT-Assistant.
Wereporttheofficialmetricscalculatedusingthestandardimplementationsprovidedforeachbenchmark
forafaircomparison. Latencyisreportedasthetimetakenduringinferenceuntilthefirstanswertoken
is produced. When reporting average performance in Table 2, the score of MME is divided by 2000,
as its range is from 800 to 2000. TFLOPs are profiled via DeepSpeed. For total number of tokens,
#Tokens = (cid:205)ùëÅ #Tokenùëñ . The training time is reported for one epoch of training during the LLaVA
ùëñ
instruction-tuningstage. TheCompressionRatio(CR)isdefinedasinEquation3.
4.2. ProofofConcept: VisualContextRedundancy
Toassesstheredundancyofvisualtokens,weperformaveragepoolingwithinanoff-the-shelfLLaVA-
1.5-7Bcheckpointatthetestingstage,usingdifferentpoolingstridesizesùëÜacrossvariousTransformer
layers ùêæ. AsshowninFig.1,themodelstillexhibitsstrongperformanceevenwhenretainingonly62.5%
ofthevisualtokens(ùëÜ = 4,ùêæ = 16)intheMM-Vetbenchmark,withouttheneedforadditionaltraining.
Whenadoptingthesamesetting(ùëÜ = 4,ùêæ = 16),asimilartrendcanbeobservedintheGQAbenchmark
aswell,wherethecompressedmodelonlyhas1%performancedropthantheuncompressedcounterpart.
Surprisingly,intheGQAbenchmark,eliminatingupto70%ofvisualtokens(ùëÜ = 4,ùêæ = 16)resultsin
amere3%decreaseinperformance. Thisproof-of-conceptshowsacertainlevelofredundancyinthe
visualtokenswithinMLLMs.
4.3. MainResults: LLaVolta
In this section, we present the main results of LLaVolta schemes instantiated in ¬ß 3.3. We conduct
a thorough evaluation of the multi-modal capability across 13 benchmarks. Tab. 2 demonstrates that
ourproposedLLaVoltanotonlyconsistentlylowerstrainingcostsby16%(15.3hoursvs.12.8hours)
but also surpasses the non-compression baseline. The four-stage training schemes achieves the best
performanceinnineoutofthethirteenbenchmarksandobtains61.9%averageperformance,improving
LLaVA-v1.5-7B[25]withmuchlessoverallTFLOPsandtrainingtime. Thisindicatesthenecessityof
designinganoptimallylitetrainingscheme.
Test Train
#Stages Scheme #Tokens‚Ä† CR‚Ä† TFLOPs‚Ä† Time GQA MMVet SQA MME VQAùëá POPE MMB MMBùê∂ùëÅ VQAùë£2 LLaVAùë§ VisWiz SEEDùêº MMMU Avg.
Single nocompression 18432 - 8.26 15.3h 62.6.49 31.91 70.8.59 146713 58.3.15 86.1.24 65.3.93 59.4.92 78.9.37 65.5.56 49.8.6 66.7.25 35.1.86 61.8.32
Two compression 10062 183% 5.20 12.8h 61.9.23 31.71.5 70.9.34 148023 58.3.46 86.5.33 64.8.23 59.01.1 78.5.20 67.3.91 47.21.8 64.9.17 34.9.11 61.5.40
Three compr.deeper 10597 174% 5.13 12.8h 62.1.01 30.5.40 70.5.23 147713 58.4.07 86.6.14 65.6.26 59.9.27 78.5.22 67.51.4 49.2.56 65.9.17 35.0.19 61.8.10
Three compr.wider 10407 177% 3.93 12.8h 61.11.6 31.8.61 71.0.28 143412 58.5.04 86.6.06 64.8.23 59.1.83 78.7.02 64.34.8 49.81.1 65.3.04 34.3.75 61.3.28
Four widerthendeeper 11088 166% 5.39 12.9h 62.1.09 31.6.58 71.4.36 144415 58.7.24 86.8.21 65.3.30 59.3.26 78.8.05 67.73.1 50.1.21 65.6.15 33.8.78 61.8.35
Four deeperthenwider 10863 170% 5.45 12.8h 62.1.07 31.5.20 70.5.16 147216 58.7.08 86.3.33 65.6.52 59.9.61 78.8.03 68.22.1 48.31.3 66.1.20 35.1.02 61.9.47
Table2 | PerformanceofLLaVolta. SeethedefinitionofeachtrainingschemeinTab.1. ‚Ä†: averageacrossstages.
Thederivedfivetrainingschemesachievecompetitiveresultswhilereducing16%trainingtime. Wereportthe
averageresultsacrossthreeruns,withthestandarddeviationwrittenatthebottomrightoftheaverageresult. The
four-stagetrainingachievesthehighestperformanceinnineofthirteenbenchmarks,outperformingthebaseline
(LLaVA-v1.5-7B)whilerequiringsignificantlyfewerTFLOPsandlesstrainingtime.
4.4. AblationStudy
Inthissection,weperformanablationstudyonthechoiceofvisualcompressorsbycomparingdifferent
compression methods. Additionally, we examine the effects of varying the stride and LLM layer in
trainingVisualContextCompressor.
8Compressor #Tokens CR GQA MM-Vet SQA MME VQAùëá POPE MMB MMBùê∂ùëÅ VQAùë£2 LLaVAùëä VisWiz SEEDùêº MMMU Avg.
Trainwithoutcompression;Testingwithcompression
RandomDropping 3312 556% 50.6 21.4 69.3 1142 46.5 55.8 39.7 33.3 59.3 47.6 47.2 52.2 34.3 47.3
K-Means 3312 556% 54.4 25.9 69.7 1155 49.0 78.6 55.3 46.1 69.3 57.6 48.9 56.1 32.9 54.0
FastV[8] 3312 556% 52.1 30.6 69.4 1298 53.4 65.6 60.1 53.0 68.6 54.8 50.0 56.3 34.9 54.9
VCC[50] 3582 514% 54.7 26.9 69.2 1246 49.2 72.3 60.8 52.0 68.1 55.6 47.8 57.0 34.8 54.7
AveragePooling 3312 556% 53.7 25.6 69.4 1150 47.7 70.1 56.4 46.5 67.0 55.6 50.0 55.7 34.3 53.0
Trainwithcompression;Testingwithcompression
RandomDropping 3312 556% 53.4 25.0 69.4 1186 49.4 64.9 52.0 41.1 59.7 51.5 47.9 52.6 34.6 50.8
K-Means 3312 556% 57.5 25.9 55.6 1279 51.4 79.4 62.6 54.6 75.7 59 46.1 59.2 34.1 57.9
FastV[8] 3312 556% 55.9 27.9 70.4 1327 49.7 79.8 62.9 55.9 69.5 61.7 49.6 56.8 35.1 57.0
VCC[50] 3582 514% 57.7 29.3 70.7 1398 53.0 83.6 65.0 55.8 74.1 58.0 48.2 60.1 35.0 58.5
AveragePooling 3312 556% 60.0 30.7 70.8 1450 55.1 85.5 65.0 59.5 75.9 66.9 46.4 62.6 33.8 60.4
Table3 | Comparisonamongdifferentvisualcompressors. Highervaluesarepreferred. Allmethodsexcept
VCCaresettothecompressionratioof556%toapproximateVCC‚Äôs514%[50]forafaircomparison. Thebest
scoresaremarkedasgrayandthesecondbestareunderlined. Attention-basedcompressors(i.e.,FastVandVCC)
excelduringtheinferencephase,yettheirapplicationtothetrainingphaseproveschallenging. Averagepooling
showsamorestableperformanceduringthetrainingphase.
ChoiceofVisualCompressors. Thedesignchoicesinclude(1)randomtokendropping,(2)K-Means
clustering,(3)averagepooling,(4)FastV[8],(5)VCC[17],(6)parametricpre-trainedQ-Former[22].
We have the following three observations. Firstly, Tab. 3 shows that the attention-based methods,
includingFastVandVCCwin9/13bestandsecondbestscores,showcasingthehighperformancewhen
compressingvisualtokensininference. However,theyareineffectivewhenappliedtotrainingbecause
thein-trainingattentionscoresareunstable. Secondly,andsurprisingly,theaveragepoolingobtainsthe
highest scores on eleven out of thirteen benchmarks when it is used to train MLLMs with a high CR.
Thirdly,Tab.4showsthatbothQ-Formerandaveragepoolingcanobtainreasonablygoodperformance
whentrainedwithextremelyhighCRs,andtheaveragepoolingperformsbetterwithlesstrainingcost.
ThereasoncouldbethattheQ-FormerresamplestokensoutsidetheLLM,potentiallycausingtheLLM
to overlook crucial information relevant to the response. In contrast, our approach employs average
poolingsubsequenttoTransformerlayer ùêæ,allowingtheinitial ùêæ layersoftheLLMtoeffectivelyretain
importantinformationfromuncompressedtokens. Giventhesethreeinsights,weselectaveragepooling
asourfavoredapproachforvisualcompression.
Train
Method #Param#Tokens CR TimeGQAMMVetSQAMMEVQAùëáPOPEMMBMMBùê∂ùëÅVQAùë£2LLaVAùë§VisWizSEEDùêºMMMUAvg.
Q-Former[22] 105M 1024 1800%10.4h 55.7 26.4 69.3 1217 49.2 83.0 57.7 50.7 71.4 64.6 52.6 55.1 34.0 56.2
Ours 0 855 2156% 9.2h 55.9 26.3 71.0 1321 51.6 82.5 63.3 55.9 74.5 63.1 47.8 57.3 35.7 57.8
Table 4 | Parametric vs. nonparametric visual compressor. We follow miniGPT-4 [51] that uses
Q-Formerpre-trainedfromBLIP-2[22]astheparametriccompressor(Allotheraspectsaremaintained
as in LLaVA to ensure a fair comparison). Ours: pooling with stride 64 on LLM layer 1 to ensure
comparable CRs. Our nonparametric compressor outshines the parametric Q-Former counterpart in
termsofbothperformanceandtrainingefficiency.
PerformanceAcrossCompressionRatios. Herein,wetrainthemulti-modalLLMwithourVisual
ContextCompressorinvarioussettings. AsdemonstratedinTab.5,theproposedmethodofferscertain
improvements and trade-offs compared to the state-of-the-art method, LLaVA-1.5-7B. We have the
following two observations. Firstly, in the heavy compression level, the performance of MLLM is
inverselyproportionaltothecompressionratio(linearlyscalingtothenumberofvisualtokens). Secondly,
theperformanceofMLLMsatthelightcompressionleveldoesnotcorrelatedirectlywiththenumberof
visualtokens, makingthisobservationsomewhatunexpected. WeattributethistotheMLLMsatthis
9levelofcompressionbeingrelativelyinsensitivetochangesinthecompressionratio. Thisindicatesthat
MLLMs trained at a light compression level will not hurt the model performance at all. For instance,
thesettingofstride16inlightcompressionlevelattainsa188%CRandalsooutperformsthebaseline
LLaVA-v1.5-7B across all four metrics. The above observations pave the way for developing a more
systematictrainingscheme.
Train
Stride #Tokens CR Latency TFLOPs time GQA MMVet SQA MME VQAùëá POPE MMB MMBùê∂ùëÅ VQAùë£2 LLaVAùë§ VisWiz SEEDùêº MMMU Avg.
HeavycompressioninLLMlayer2
8 3312 557% 37.9ms 2.14 12.0 59.9.13 30.1.92 70.9.17 144311 55.3.3 85.3.21 65.2.25 59.5.06 76.0.09 65.92.0 46.6.2 62.6.0 34.2.54 60.3.2
2 9792 188% 48.6ms 4.77 12.6 61.9.43 30.91.1 71.6.69 145018 57.6.08 86.3.22 67.2.05 59.9.4 78.0.17 66.4.85 48.7.25 65.9.49 34.1.34 61.6.08
LightcompressioninLLMlayer16
8 10368 178% 51.3ms 5.00 12.8 62.6.03 30.4.54 71.1.27 14629 58.2.01 86.0.09 65.3.52 58.9.57 78.8.12 63.91.1 51.4.15 66.8.23 35.81.4 61.8.04
2 13824 133% 58.8ms 6.40 14.2 61.9.45 31.51.0 70.8.49 146224 58.5.02 86.4.12 66.4.33 59.6.47 78.9.02 65.3.46 49.5.97 66.7.23 35.1.87 61.8.01
Base[25] 18432 100% 68.5ms 8.26 15.3h 62.6.49 31.91.0 70.8.59 146713 58.3.15 86.1.24 65.3.93 59.4.92 78.9.37 65.5.56 49.8.6 66.7.25 35.1.86 61.8.32
Table 5 | TrainingMLLMswithVisualContextCompressorinvariouscompressionlevels. Wereportthe
averageresultsacrossthreeruns,withthestandarddeviationwrittenatthebottomrightoftheaverageresult. In
the heavy compression range, the performance is inversely proportional to the compression ratio. In the light
compressionrange,theperformanceisnotsensitivetocompression. Performanceremainshighformodelsatthe
lightcompressionlevel.
Furthermore,weconductanablationstudyonthenumberofiterationsindifferentstages(uniformvs.
non-uniformstagesplitting),whichisdetailedintheAppendix.
4.5. ExtensibilitytoVideoMLLMs
WeextendourtrainingschemetoVideoLLaVA[24]andtheresultsinTab.6revealsimilarfindingsas
before: theproposedtrainingschemeachievecompetitiveresultswhilereducing9%trainingtime. Itis
worthmentioningVideoLLaVAdoesnotsupportDeepSpeedZeRO-3,unlikeLLaVA,whichresultsin
differentrelativeefficiencygains.
MSVD-QA MSRVTT-QA ActivityNet-QA Average
#Stages Scheme #Tokens‚Ä† CR‚Ä† TFLOPs‚Ä† Train-time
Score Acc Score Acc Score Acc Score Acc
Single nocompression 147456 - 29.68 40.7h 3.69 69.1 3.48 56.8 3.28 47.5 3.48 57.8
Two compression 80496 183% 17.73 37.1h 3.71 69.0 3.50 56.9 3.29 47.9 3.50 57.9
Three compr.deeper 84776 174% 17.29 37.1h 3.73 69.3 3.51 57.2 3.28 47.4 3.51 58.0
Three compr.wider 83256 177% 16.86 37.0h 3.72 69.0 3.51 57.2 3.29 47.7 3.51 58.0
Four widerthendeeper 88704 166% 18.32 37.2h 3.72 69.1 3.51 57.2 3.27 48.0 3.50 58.1
Four deeperthenwider 86904 170% 18.64 37.1h 3.74 69.8 3.49 56.9 3.27 47.8 3.50 58.2
Table6 | PerformanceofLLaVoltaonVideoLLaVA[24]. SeethedefinitionofeachtrainingschemeinTab.1. ‚Ä†:
averageacrossstages. Toimplementourmulti-stagetraining,weapplythesamecompressionprocessingtothe8
framesrepresentingthevideorespectively. Thederivedfivetrainingschemesachievecompetitiveresultswhile
reducing9%trainingtime.
5. Conclusion
In this work, we conduct two initial studies to investigate and verify the redundancy of visual tokens
inmulti-modalLLMs. Toaddressthis,weproposeVisualContextCompressor,astraightforwardyet
effectivecompressiontechniquethatemploysasimpleaveragepooler,seamlesslyintegratingintothe
training of MLLMs. This approach enhances training efficiency without compromising performance.
10To further mitigate the information loss brought by the token compression, we introduce LLaVolta, a
multi-stage training scheme that utilizes Visual Context Compressor with a progressively decreasing
compression rate. Experimental results on various visual question answering benchmarks verify the
effectivenessofLLaVoltainboostingperformancewhilealsodemonstratingefficiencygainsbyreducing
trainingcostsby16%. Tothebestofourknowledge,wearethefirsttoacceleratethetrainingofmulti-
modalLLMfromthecompressionperspective. WehopethattheproposedVisualContextCompressor
andLLaVoltawillinspiremorein-depthanalysisofvisualredundancyexistingincurrentMLLMsand
callforfuturedesignsofefficienttrainingforMLLMs.
References
[1] J.-B.Alayracetal.‚ÄúFlamingo:avisuallanguagemodelforfew-shotlearning‚Äù.In:Advancesin
neuralinformationprocessingsystems35(2022),pp.23716‚Äì23736.
[2] C. Anil et al. ‚ÄúExploring length generalization in large language models‚Äù. In: arXiv preprint
arXiv:2207.04901(2022).
[3] P.Baldi.‚ÄúAutoencoders,unsupervisedlearning,anddeeparchitectures‚Äù.In:ProceedingsofICML
workshoponunsupervisedandtransferlearning.JMLRWorkshopandConferenceProceedings.
2012,pp.37‚Äì49.
[4] H.Barlow.‚ÄúRedundancyreductionrevisited‚Äù.In:Network:computationinneuralsystems12.3
(2001),p.241.
[5] D.ChenandW.B.Dolan.‚ÄúCollectinghighlyparalleldataforparaphraseevaluation‚Äù.In:Proceed-
ingsofthe49thannualmeetingoftheassociationforcomputationallinguistics:humanlanguage
technologies.2011,pp.190‚Äì200.
[6] J.-N. Chen et al. ‚ÄúTransmix: Attend to mix for vision transformers‚Äù. In: Proceedings of the
IEEE/CVFConferenceonComputerVisionandPatternRecognition.2022,pp.12135‚Äì12144.
[7] J. Chen et al. ‚ÄúViTamin: Designing Scalable Vision Models in the Vision-language Era‚Äù. In:
ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.2024.
[8] L.Chenetal.‚ÄúAnimageisworth1/2tokensafterlayer2:Plug-and-playinferenceacceleration
forlargevision-languagemodels‚Äù.In:arXivpreprintarXiv:2403.06764(2024).
[9] W.-L.Chiangetal.‚ÄúVicuna:Anopen-sourcechatbotimpressinggpt-4with90%*chatgptquality‚Äù.
In:Seehttps://vicuna.lmsys.org(accessed14April2023)2.3(2023),p.6.
[10] Z. Dai et al. ‚ÄúFunnel-transformer: Filtering out sequential redundancy for efficient language
processing‚Äù.In:Advancesinneuralinformationprocessingsystems33(2020),pp.4271‚Äì4282.
[11] A. Dosovitskiy et al. ‚ÄúAn image is worth 16x16 words: Transformers for image recognition at
scale‚Äù.In:arXivpreprintarXiv:2010.11929(2020).
[12] C.Fuetal.‚ÄúMME:AComprehensiveEvaluationBenchmarkforMultimodalLargeLanguage
Models‚Äù.In:arXivpreprintarXiv:2306.13394(2023).
[13] Y.Goyaletal.‚ÄúMakingthevinvqamatter:Elevatingtheroleofimageunderstandinginvisual
questionanswering‚Äù.In:CVPR.2017.
[14] D. Gurari et al. ‚ÄúVizwiz grand challenge: Answering visual questions from blind people‚Äù. In:
ProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition.2018,pp.3608‚Äì
3617.
[15] J.Heetal.‚ÄúTransfg:Atransformerarchitectureforfine-grainedrecognition‚Äù.In:Proceedingsof
theAAAIconferenceonartificialintelligence.Vol.36.1.2022,pp.852‚Äì860.
[16] K.Heetal.‚ÄúMaskedautoencodersarescalablevisionlearners‚Äù.In:ProceedingsoftheIEEE/CVF
conferenceoncomputervisionandpatternrecognition.2022,pp.16000‚Äì16009.
11[17] L.Houetal.‚ÄúTokendroppingforefficientbertpretraining‚Äù.In:Proceedingsofthe60thAnnual
MeetingoftheAssociationforComputationalLinguistics.2022.
[18] X.Huangetal.‚ÄúPyramid-BERT:Reducingcomplexityviasuccessivecore-setbasedtokenselec-
tion‚Äù.In:arXivpreprintarXiv:2203.14380(2022).
[19] D. A. Hudson and C. D. Manning. ‚ÄúGqa: A new dataset for real-world visual reasoning and
compositionalquestionanswering‚Äù.In:ProceedingsoftheIEEE/CVFconferenceoncomputer
visionandpatternrecognition.2019,pp.6700‚Äì6709.
[20] T.Kanungoetal.‚ÄúAnefficientk-meansclusteringalgorithm:Analysisandimplementation‚Äù.In:
IEEEtransactionsonpatternanalysisandmachineintelligence24.7(2002),pp.881‚Äì892.
[21] B. Li et al. ‚ÄúSeed-bench: Benchmarking multimodal llms with generative comprehension‚Äù. In:
arXivpreprintarXiv:2307.16125(2023).
[22] J. Li et al. ‚ÄúBlip-2: Bootstrapping language-image pre-training with frozen image encoders
and large language models‚Äù. In: International conference on machine learning. PMLR. 2023,
pp.19730‚Äì19742.
[23] Y.Lietal.‚ÄúEvaluatingobjecthallucinationinlargevision-languagemodels‚Äù.In:arXivpreprint
arXiv:2305.10355(2023).
[24] B.Linetal.‚ÄúVideo-llava:Learningunitedvisualrepresentationbyalignmentbeforeprojection‚Äù.
In:arXivpreprintarXiv:2311.10122(2023).
[25] H.Liuetal.ImprovedBaselineswithVisualInstructionTuning.2023.
[26] H.Liuetal.‚ÄúVisualinstructiontuning‚Äù.In:Advancesinneuralinformationprocessingsystems36
(2024).
[27] N.F.Liuetal.‚ÄúLostinthemiddle:Howlanguagemodelsuselongcontexts‚Äù.In:arXivpreprint
arXiv:2307.03172(2023).
[28] Y.Liuetal.‚ÄúMMBench:IsYourMulti-modalModelanAll-aroundPlayer?‚ÄùIn:arXivpreprint
arXiv:2307.06281(2023).
[29] P. Lu et al. ‚ÄúLearn to explain: Multimodal reasoning via thought chains for science question
answering‚Äù.In:AdvancesinNeuralInformationProcessingSystems(2022).
[30] P.Nawrotetal.‚ÄúEfficienttransformerswithdynamictokenpooling‚Äù.In:arXivpreprintarXiv:2211.09761
(2022).
[31] OpenAI.ChatGPT.https://openai.com/blog/chatgpt/.2022.
[32] OpenAI.GPT-4TechnicalReport.2023.arXiv:2303.08774[cs.CL].
[33] G.QinandB.VanDurme.‚ÄúNugget:Neuralagglomerativeembeddingsoftext‚Äù.In:International
ConferenceonMachineLearning.PMLR.2023,pp.28337‚Äì28350.
[34] A. Radford et al. ‚ÄúLearning transferable visual models from natural language supervision‚Äù. In:
Internationalconferenceonmachinelearning.PMLR.2021,pp.8748‚Äì8763.
[35] J.W.Raeetal.‚ÄúCompressivetransformersforlong-rangesequencemodelling‚Äù.In:arXivpreprint
arXiv:1911.05507 (2019).
[36] S.Shenetal.‚ÄúStagedtrainingfortransformerlanguagemodels‚Äù.In:InternationalConferenceon
MachineLearning.PMLR.2022,pp.19893‚Äì19908.
[37] A.Singhetal.‚ÄúTowardsvqamodelsthatcanread‚Äù.In:CVPR.2019.
[38] G. Team et al. ‚ÄúGemini: a family of highly capable multimodal models‚Äù. In: arXiv preprint
arXiv:2312.11805(2023).
[39] G. Team et al. ‚ÄúGemma: Open models based on gemini research and technology‚Äù. In: arXiv
preprintarXiv:2403.08295(2024).
12[40] H.Touvronetal.‚ÄúLlama2:OpenFoundationandFine-TunedChatModels‚Äù.In:arXivpreprint
arXiv:2307.09288(2023).
[41] H. Touvron et al. ‚ÄúLlama: Open and efficient foundation language models‚Äù. In: arXiv preprint
arXiv:2302.13971(2023).
[42] A. Vaswani et al. ‚ÄúAttention is all you need‚Äù. In: Advances in neural information processing
systems30(2017).
[43] G.K.Wallace.‚ÄúTheJPEGstillpicturecompressionstandard‚Äù.In:IEEEtransactionsonconsumer
electronics38.1(1992),pp.xviii‚Äìxxxiv.
[44] G. Xiao et al. ‚ÄúEfficient streaming language models with attention sinks‚Äù. In: arXiv preprint
arXiv:2309.17453(2023).
[45] J. Xu et al. ‚ÄúMsr-vtt: A large video description dataset for bridging video and language‚Äù. In:
ProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition.2016,pp.5288‚Äì
5296.
[46] X.Yeetal.‚ÄúAnaloBench:BenchmarkingtheIdentificationofAbstractandLong-contextAnalo-
gies‚Äù.In:arXivpreprintarXiv:2402.12370(2024).
[47] W.Yuetal.‚ÄúMm-vet:Evaluatinglargemultimodalmodelsforintegratedcapabilities‚Äù.In:arXiv
preprintarXiv:2308.02490(2023).
[48] Z. Yu et al. ‚ÄúActivitynet-qa: A dataset for understanding complex web videos via question an-
swering‚Äù. In: Proceedings ofthe AAAI Conference on Artificial Intelligence. Vol.33. 01. 2019,
pp.9127‚Äì9134.
[49] X. Yue et al. ‚ÄúMmmu: A massive multi-discipline multimodal understanding and reasoning
benchmarkforexpertagi‚Äù.In:ProceedingsoftheIEEE/CVFConferenceonComputerVisionand
PatternRecognition.2024,pp.9556‚Äì9567.
[50] Z. Zeng et al. ‚ÄúVcc: Scaling Transformers to 128K Tokens or More by Prioritizing Important
Tokens‚Äù.In:AdvancesinNeuralInformationProcessingSystems36(2024).
[51] D.Zhuetal.‚ÄúMinigpt-4:Enhancingvision-languageunderstandingwithadvancedlargelanguage
models‚Äù.In:arXivpreprintarXiv:2304.10592(2023).
13Appendix
Intheappendix,weprovideadditionalinformationaslistedbelow:
‚Ä¢ ¬ßAprovidestheadditionalexperimentalresults.
A. Additional Experimental Results
A.1. Non-uniformStageSplitting
Bydefault,thetrainingtimeisevenlydividedacrosseachstage. Toexplorehowthecompressionstage
affectstotaltrainingtime,wemodifytherelativeproportionofdifferentstages. Thisvariationistestedin
thetwo-stagesetupreferencedinTab.1,adjustingfromthestandard50%inStage1and50%inStage2
todifferentdistributions. Tab.7belowdisplaystheresultsoftheseexperiments.
Stage1 Stage2 #Tokens CR GQA MMVet SQA MME VQAùëá POPE MMB MMBùê∂ùëÅ
0% 100% 18432 - 62.0 31.1 70.1 1453.0 58.2 85.9 64.3 58.3
25% 75% 11088 166% 62.1 31.7 70.6 1474.5 58.8 86.4 65.1 59.6
50% 50% 10863 170% 62.2 30.0 70.3 1443.5 57.5 85.8 64.8 59.7
75% 25% 10597 174% 61.6 32.2 70.8 1471.5 57.5 86.6 65.2 58.9
90% 10% 10407 177% 61.2 31.0 70.5 1447.5 56.3 86.4 64.4 56.9
100% 0% 10062 183% 55.9 29.5 64.1 1257.8 49.1 86.6 47.4 29.2
Table7 | Effectsofnon-uniformstagesplittingatthetwo-stageset-up. Performancedecreasesasthe
proportionofStage2decreases,albeitattheexpenseoflowercompressionratios.
WeobservethatastheStage2increasesfrom0%to100%,thereisagradualdecreaseinthemodel‚Äôs
performance across various metrics (such as GQA, MMVet, SQA, MME, VQA, POPE, MMB, and
MMBùê∂ùëÅ). Althoughthereisadeclineinperformance,itisrelativelyminorwhenthecompressionstage
makesupto50%ofthetrainingduration. However, whentheproportionofthecompressionstageis
reducedbelow50%,thedeclineinperformancebecomesmoresignificant. Inconclusion,keepingthe
compressionstagebetween0-50%ofthetrainingtimeminimizesperformancelosswhilestillachieving
significantcompressionratios.
14