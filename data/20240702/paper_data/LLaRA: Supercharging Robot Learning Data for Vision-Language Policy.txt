LLaRA: Supercharging Robot Learning Data for
Vision-Language Policy
XiangLi1 CristinaMata1 JongwooPark1 KumaraKahatapitiya1
YooSungJang1 JinghuanShang1 KanchanaRanasinghe1 RyanBurgert1
MuCai2 YongJaeLee2 MichaelS.Ryoo1
1StonyBrookUniversity 2UniversityofWisconsin-Madison
xiangli8@cs.stonybrook.edu
Abstract: Large Language Models (LLMs) equipped with extensive world
knowledgeandstrongreasoningskillscantacklediversetasksacrossdomains,of-
tenbyposingthemasconversation-styleinstruction-responsepairs. Inthispaper,
weproposeLLaRA:LargeLanguageandRoboticsAssistant,aframeworkwhich
formulatesrobotactionpolicyasconversations,andprovidesimprovedresponses
when trained with auxiliary data that complements policy learning. LLMs with
visualinputs,i.e.,VisionLanguageModels(VLMs),havethecapacitytoprocess
stateinformationasvisual-textualpromptsandgenerateoptimalpolicydecisions
intext.TotrainsuchactionpolicyVLMs,wefirstintroduceanautomatedpipeline
to generate diverse high-quality robotics instruction data from existing behavior
cloningdata.AVLMfinetunedwiththeresultingcollectionofdatasetsbasedona
conversation-styleformulationtailoredforroboticstasks,cangeneratemeaning-
fulrobotactionpolicydecisions. Ourexperimentsacrossmultiplesimulatedand
real-worldenvironmentsdemonstratethestate-of-the-artperformanceofthepro-
posedLLaRAframework.Thecode,datasets,andpretrainedmodelsareavailable
athttps://github.com/LostXine/LLaRA.
Keywords: VLM,RobotActionPolicy,InstructionTuningData
<task>Place all other D-inBC Temporal Relationship
objects into the <p>large
bowl</p> at <b>(0.513, Spatial Relationship Localization
0.382), {0.278, 0.487}
</b>.</task> inBC Action Prediction
List the actions required to perform the task
Future Prediction
given on this scene…
Detection
Step 1: Pick up the <p>carrot</p> at
<b>(0.262, 0.668)</b>, rotate <r>[-148]</r>
degrees, and drop it at <b>(0.541, 0.500)</b>.
…… …… ……
Env Agent
Figure1:LLaRAOverview.Weconvertexperttrajectoriesintoinstruction-tuningdata,whilealsogenerating
auxiliarydatathatcomplementspolicyinaself-supervisedfashion. Basedonsuch,wefinetuneapre-trained
Vision Language Model (VLM) as a robot policy. At inference, we query the VLM to generate actions in
naturallanguage,justasinaconversation.Areal-worldinferenceexampleisshowninthefigure.
1 Introduction
LargeLanguageModels(LLMs)suchasGPT-4[1],Llama[2,3],andGemini[4]exhibitunprece-
dented abilities across diverse language tasks. Such LLMs are often trained together with visual
4202
nuJ
82
]OR.sc[
1v59002.6042:viXradatabytakingadvantageofpretrainedvisionencoders,formingpowerfulVisionLanguageModels
(VLMs).EffectiveandefficienttrainingofsuchVLMssignificantlydependsonthestyleandformat
ofthevision-languagedatausedfortraining, leadingtotheexplorationofvariousinstructiontun-
ing[5,6]strategies. Infact,visualinstructiontuning,orformattingimage-textdatainconversation
style for VLM training has been studied extensively in [7, 8]. The resulting VLMs exhibit strong
vision-languageskills,butnotwithoutlimitationssuchasspatialawareness[9,10]orniche-domain
understanding[11,12]. Thelatterhasledtodomain-specificvisualinstructiontuning[11,12,13].
Several robot learning approaches also leverage pretrained LLMs / VLMs [14, 15, 16, 17, 18, 19]
withmore-recentworkfocusingonlow-levelrobotactions[20,21,22]. Thestrongperformanceof
suchapproachescanbeattributedtotheextensiveworldknowledge(e.g.,understandingofphysics,
human common-sense) [23, 24] and powerful reasoning abilities [25, 26] of underlying language
models. However,studiesoncuratingconversation-styledataandusingsuchforinstructiontuning
inroboticshavebeenratherlimited[27].
Motivated by the promising attributes of VLMs, we explore a formulation of VLM-based robot
actionpolicyinthispaper.OurgoalistoadaptaVLMasarobotactionpolicythatcanhandlediverse
visuomotorcontrolchallenges,aprocesswecallVisuomotorInstructionTuning. Morespecifically,
wetransformatypicalbehaviorcloning(BC)datasetintoaninstructiondataset,Instruct-BC(inBC),
which is then used to finetune a VLM. Given a state described in visual-textual modalities, our
VLMistrainedtogeneratesuitableactionsastext. Suchaformulationbasedonconversation-style
instruction-responsedataenablesustoinstruction-tuneandconvertaVLMintoarobotactionpolicy
effortlessly. However, we also note that the effectiveness of an instruction-tuned VLM depends
heavilyonthequalityofdataformulation[7,10,12],whichisnon-trivialtoautomate(or,scale)for
newdomains [11,12,13,9,10]. Therefore,tostrengthen(or,supercharge)suchdata,weconstruct
auxiliary datasets that complement policy learning from the same BC dataset, in a self-supervised
fashionwithoutanyexternaldata.
OuroverallframeworktermedLLaRA(LargeLanguageandRoboticsAssistant)generatesvisuo-
motorinstructiondatathatcanefficientlyandeffectivelyfinetuneaVLMintoarobotactionpolicy.
ItismotivatedbyLLaVA[7]whichisdesignedprimarilyforvisiontasks. Similarly,LLaRAoffers
theentireframework ofdatageneration, model formulation, andtrainingpipelinefor VLMs, now
specializedforrobotlearning. Wesummarizeourkeycontributionsasfollows:
1. Formulating robot manipulation tasks into instruction-response pairs described by natural
language,whichenablessuccessfulinstructiontuningofaVLMasapolicy.
2. A scalable pipeline for generating diverse high-quality robot instruction tuning data from
existingbehaviorcloningdata.
3. Identifyingandgeneratingauxiliaryinstructiondatathatfurtherenhancesrobotpolicylearn-
inginaself-supervisedmanner.
Weconductextensiveexperimentsontheproposedframeworktoestablishtheeffectivenessofboth
ourautomateddatagenerationpipelineandinstruction-tunedVLMinsolvingroboticstasks.
2 RelatedWork
Spatial Reasoning in VLMs: Several recent work investigate how VLMs can be modified for
spatial awareness within images [28, 29, 30, 31, 9, 10, 32, 33, 34], which is a critical ability of
a visuomotor policy. One line of work explores prompting using textual or specialized tokens to
encode locations, followed by instruction tuning on localization-specific datasets [31, 9, 10, 32].
In fact, several works [9, 10, 32] combine LLMs / VLMs with templating operations to generate
datasetsinformatsadheringtohumanconversation,usingexistingannotateddatasetsthatonlycon-
tainper-objectboundingboxannotations.Incontrast,ourframeworkextendsbeyondlocalizationto
directlypredictrobotactions. Analternatelineofworkexploresvisualpromptingbutwithoutany
focusonrobotlearning(e.g.,redcircles[34]orfree-formvisualscribbles[33]overlaidonimages
images to focus a VLM on specific regions). An extension of such ideas to robotics is explored
2inPIVOT[35], whereimagesannotatedwithobjectlocationsandtrajectoriesaregiventoaVLM
forgeneratingvisually-describedrobotactions. TheseideasarecomplementarytoLLaRA,where
additionalvisuomotorinstructiontuningdirectlygeneratesactionsoptimalforroboticstasks.
Agent and Policy Learning with LLMs / VLMs: Foundation LLMs / VLMs provide a source
ofcommon-sensedataforrobotpolicytraining. Qianetal.[36]showshowtotrainanaffordance
groundingVLMonaffordancedata.Ingelhagetal.[37]presentsasystemthatevaluatesthepotential
of instruction following using already-learned skills based on LLM / VLM, and otherwise learns
new skills from user demonstrations. Wu et al. [38] construct a read-and-reward framework that
extractsrelevantinformationfromAtarimanuals,evaluatingobject-agentinteractions.Yonedaetal.
[39] utilizes an LLM to maintain an estimate of the world state in text. More-recent work such
as Robotics Transformer [20, 21, 22], Gato [40], GR-1 [41] and Octo [42] learn generalist robot
policies with multi-modal sequence models, taking advantage of the flexibility of encoding and
decodingimages/actionsviatokenrepresentationssimilartoVLMs. VIMA[43]isatransformer-
based agent that processes interleaved visual and text prompts. In contrast, our method predicts
actions encoded only as texts, without introducing any new tokens or LLM / VLM architectural
modifications. LLaRAuniquelyreformulatesthesamedatainVIMA[43]forinstructiontuning.
InstructionDataGeneration: FinetuningLLMs/VLMswithinstructiondatahasshownalotof
potentialforvisiontasks[5,6,7,8]. Whileeffective,constructionofsuchdatasetswithhighquality
and quantity is beyond trivial [7], especially for specialized visual domains [11, 44, 12, 45] such
as robotics. Moreover, recent works highlight shortcomings in abilities such as spatial awareness
of VLMs instruction tuned on generic data [9, 10] leading to exploration of localization-specific
instructiondatageneration[31,9,10,32,8].YetthesemodifiedVLMssufferintheroboticsdomains
[46]. In contrast to these works, we build a robotics-specific instruction dataset used to train a
powerfulVLM,LLaRA,adeptatseveralroboticdomaintasks. ClosesttoourproposedLLaRAis
Zhenetal.[27],whichfocusedon3Dreasoningandinferencecapabilitiesformanipulationtasks,
butunlikeoursperformnodirectactionpredictionsintextualform.
Self-SupervisedLearninginRobotics: Self-supervisedlearning(SSL)haslongbeenexploredin
robotics[47,48],withrecentworkshowingaflavorofvisualrepresentationlearning [49,50,51].
Incontrast,weintroduceauxiliarySSLobjectives(i.e.,pretexttasks)basedoninstructionprompts,
motivatedbysimilarideasincomputervision(e.g.,utilizingspatialrelationsin[52],temporalrela-
tionsin[53,54]orfuturepredictionin[55]). Morespecifically,wecreateasetofdomain-specific
auxiliarydatasets(e.g.,forspatialortemporalreasoning),thatenableLLaRAtolearnrelationsthat
areusefulforthedownstreamroboticstasks,whenusedforfinetuning.
DiscussiononConcurrentWork: Withinthepast2weekspriortothereleaseofourwork,there
has been a number of concurrent work studying VLM-based action policy learning. They have
similaritieswithLLaRA,astheypromoteinstructiontuning,motivatedbypriorworksuchasLLaVA
[7].Ourkeydifferenceistheautomatedgenerationofauxiliarydatafromexistingrobottrajectories,
thatenablesrobotstolearnspatio-temporalrelationsinthescene. Wefurtherdiscussthembelow:
All such approaches employ a VLM as a robot action policy, processing both visual observations
andtextualinstructionsasinputs.OpenVLA[18]employsspecializedtokenstorepresentquantized
actions,amethodakintoRT-2[21]. Additionally,OpenVLAincorporatesDINOv2[56]asanaddi-
tionalvisualencoderalongwithSigLIP[57]. Thiscombinationhasprovenbeneficial,particularly
when the visual encoder is finetuned for robotics applications. RoboPoint [58] introduces a novel
point-basedactionspaceandascalabledatapipelinetailoredforspatialaffordanceprediction. Ac-
tions are delineated via points on an RGB image, which are subsequently translated to 3D space
using depth data. This approach eliminates the reliance on predefined action primitives [59, 60],
external object detectors [61, 62], and iterative visual prompting [35]. Another concurrent work,
LLARVA[19],differsbygeneratingboth2Dvisualtracesinimagecoordinatesandcorresponding
textualactionsasoutputs,withtheformerfunctioningasanauxiliarytask.
However, a significant gap shown in the aforementioned studies is the lack of comprehensive in-
vestigation into the methods for generating auxiliary datasets from existing robot data, as well as
3the implications of integrating such datasets. Our work LLaRA distinctively addresses this gap,
markingacriticaldivergencefromtheconcurrentstudies. Thatis,inadditiontodirectlysupervis-
ing a VLM based on robot actions, LLaRA framework automatically generate multiple auxiliary
datasetswithcomplementarytrainingobjectives(fromtheexistingrobottrajectories). Suchauxil-
iary datasets are designed to train a VLM to capture spatial/temporal relations between objects in
the scene, enabling better scene representations. We experimentally confirm that such additional
supervisionsignificantlybenefitsrobotactionpolicylearning.
3 Preliminary: VisualInstructionTuning
VisualInstructionTuningisaframeworktofinetunealargeVisionLanguageModel(VLM)using
language as task instructions. The objective is to develop a general-purpose multimodal model
capable of learning from diverse vision tasks described by language [63]. A typical example of
Visual Instruction Tuning is LLaVA [7], which consists of three neural networks as its model: a
pretrainedLLMθ ,apretrainedvisualencoderθ andanadapterlayerθ .
LLM V MLP
Consider a conversation data sample that contains a single image x and user instruction text x
v l
asinputs. Duringinference,thevisualencoderθ andadapterlayerθ processx successively
V MLP v
to produce a set of visual tokens that can be directly concatenated with the textual tokens of x
l
in language embedding space of LLM θ . Next, θ autoregressively generates new tokens
LLM LLM
conditioned on both the visual tokens from x and text tokens from x . These new tokens will be
v l
decodedintonaturallanguageastheoutputofthemodel.
A two-phase training process is executed in Liu LLM (θLLM)
et al. [7] with a next-token prediction objective
over the θ output in each phase. First, only
LLM Adapter Layer
θ is trained using image-caption pairs (similar to
MLP (θMLP)
datasets for CLIP [64] training). Then, both θ
MLP
and θ are finetuned using a specialized instruc-
LLM Visual Encoder
tion tuning dataset termed LLaVA_Instruct_150k (see Text Tokenizer
(θ V)
Fig. 2). LLaVA_Instruct_150k contains human-style
instruction-responseconversations,automaticallycreated
Images (x v) Text Input (x l)
by a powerful LLM [1] from a generic object detection Figure2:Background:LLaVAOverview.A
dataset (see Liu et al. [7] for details). In our paper, we LargeLanguageModel(LLM)isconnected
to the visual domain with suitable encoder
startfromapretrainedLLaVAmodelandonlyperforma
andadaptorneuralnetworks.
single-stagefinetune,updatingθ andθ .
LLM MLP
4 VisuomotorInstructionTuning
We leverage large Vision Language Models (VLMs) as a generalist to address diverse visuomotor
control challenges, a process we call Visuomotor Instruction Tuning. Specifically, we transform
a typical behavior cloning dataset into an instruction tuning dataset and subsequently finetune a
pretrainedVLMonthistailoreddataset. TheresultingLLaRAframeworkbenefitsfromthebroad,
inherentknowledgeembeddedwithintheVLM,enablingbettervisuomotortasklearning.
Inthissection, wepresentourLLaRAframeworktoa)convertasetofrobotmanipulationexpert
trajectories into a visuomotor instruction tuning dataset, and b) turn a VLM finetuned on such a
datasetintoacompetitiverobotpolicy. Inthenextsection,weshowthatsuchaVLMcanbefurther
improvedforrobotcontrolinaself-supervisedmannerusingauxiliaryinstructiontuningdatasets,
whicharealsoextractedfromthesameexistingexpertdemonstrations.
4.1 ProblemFormulation
Weconsiderabehavioralcloning(BC)settingoveraMarkovDecisionProcess(MDP),described
bythetuple(S,A,P). Here,s ∈ S denotesthestateattimestampt,a ∈ Arepresentstheaction
t t
4BC D-inBC
Task: Stack objects in the order {frame_0} {frame_1} {frame_2}. Step 1: Object Detection on Every Reference Image
<image1> <frame_0> <frame_1> <frame_2> Inpu t : < Ef nr ua mm ee r_ a0 te>
a nd describe every object found in the image.
Output: <scene> <p>red letter V</p> at <b>(0.254, 0.570), {0.094, 0.156}
</b>. <p>green paisley letter V</p> at <b>(0.500, 0.570), {0.094,
inBC 0.156}</b>. <p>rainbow letter V</p> at … <scene>
Step 1: VLM Query Step 2: VLM Query with Detection Results
Input: < <i tm asa kg >e S1> ta ck objects in the order frame_0 frame_1 frame_2 Inpu t : < <i tm asa kg >e 1 S> t a ck objects in this order <frame_0-desc>
</task>… <frame_1-desc> <frame_2-desc> </task>…
Output: Step 1: Pick up the object at <b>(0.465, 0.617)</b>, rotate Output: Step 1: Pick up the object at <b>(0.465, 0.617)</b>, rotate <r>[0]</r>
<r>[0]</r> degrees, and drop it at <b>(0.711, 0.617)</b>. degrees, and drop it at <b>(0.711, 0.617)</b>. Step 2: …
Step 2: …
Figure 3: Examples of inBC and D-inBC for Visuomotor Instruction Tuning: (left) BC is the original
behaviorcloningdata(actionisomitted). inBCisourinstructiontuningdatacreatedfromBC.(right)D-inBC
additionallyleveragesobjectdetectioninputstobettermakeuseofinformationspreadacrossmultipleimages.
BothinBCandD-inBCbelongtotheLLaRAfamily.
att,andP encapsulatesthetransitiondynamics,expressedasP(s |s ,a ).Ourgoalistofinetune
t+1 t t
aVLMasarobotpolicyπ thatbestrecoversanunknownpolicyπ∗ usingademonstrationdataset
containing multiple tasks D = {(si,ai)} collected by π∗. The robotic policy π is designed to be
conditioned on the task description, and our empirical findings suggest adding historical actions
is a beneficial condition. Consequently, the policy can be articulated as π(ai|si,ai), where h =
t t h
1,2,...,t−1,indicatingthatπtakesintoaccountallprecedingactionsuptotimet−1.
4.2 InstructionTuningDatafromTrajectories
Next, we introduce our method to turn a set of expert trajectories D into a visuomotor instruction
tuningdataset. Wefocusonthestationaryrobotmanipulationscenariofeaturingaroboticmanipu-
latorfixedaboveaflattable. Thevisualobservationforthissetupiscapturedbyafixedthird-person
viewcamerapositionedtoobservethetablesurface.
The policy closely follows the design of the original LLaVA [7]. The model consumes a single
imageandlanguageinstruction,generatingalanguageoutput. Foreachstatetransition,weconvert
the state action pair (s ,a ) into a single-round conversation. The current visual observation and
t t
textual task description, forming s , can be directly assimilated by a VLM as the user instruction.
t
However, the numerical action a needs conversion into textual format to be generated by a VLM.
IncontrasttotheapproachemployedbyRT-2[21],whichutilizesspecialtokenstodirectlyencode
numerical action values, our methodology adopts 2D Image Coordinates—normalized relative to
the image size—to represent positional actions. We establish a mapping between the numerical
action values and their corresponding 2D positions within the image. Additionally, any rotational
component of the action, denoted as a (such as the angle of a joint), is expressed textually as
r
“rotate <r>a </r> degrees”. MoredetailsarecoveredinSec.A.1.
r
Byimplementingthesestrategies,weeffectivelyconvertatrajectoryintoaformatcomprisingboth
imageandtext,whichisreadilyprocessedbytheVLM.Thisapproachalsofacilitatesthetransferof
themodelacrossdifferentroboticembodiments. Wenamethedatasetconvertedbythismethodas
Instruct-BC(inBC)andoneexampleispresentedinFig.3andmoredetailedexamplesareatTab.1
andTab.5. WeusethesamedatasetnameasaprefixtorefertoourLLaRAframeworktrainedwith
inBC data. Inscenarioswhereasingleobservations comprisesmultipleimages,systemssuchas
t
LLaVAsufferfromlowperformancebecauseitwasnevertrainedonmultipleimages(seeTab.10).
Toaccommodatethis,onepossiblesolutionistoswitchtoaVLMsetupwhichcanconsumemultiple
interleavedimageslikeQwen-VL[65]andLLaVA-NeXT[66]. Instead,weconverteachadditional
image into a language description with the help of object detection. Compared to inBC, we name
thismethodthattakesadditionalobjectdetectionresultsinthetextasDescription-Instruct-BC(D-
inBC,andourmodeltrainedonthisdataisreferredtowiththesamenameasearlier.Moreexamples
arealsoavailableinTab.1andTab.5.
5Table1: ComparisonbetweenourconvertedinstructiontuningdatasetsinBCandD-inBCforaVIMA-Bench
sample. The task description of the episode is in blue. The description of an object in the reference image
(oracledetectionresults)isinmagenta.Theactionhistoryisinorange.
{dragged_obj}
{base_obj}
Task Firstput{dragged_obj}into{base_obj}thenputtheobjectthatwaspre-
viouslyatitssouthintothesame{base_obj}.
Input
(currentfrontcameraview.)
<task>Firstput<p>rainbowletterT</p>into<p>woodenbowl</p>then
put the object that was previously at its south into the same <p>wooden
bowl</p>.</task>
Everyactionyoutakemustincludetwolocationsintheformatof<b>(x,
inBC y)</b>andoneclockwiserotationangleintheformatof<r>[r]</r>. The
firstlocationistheimagecoordinatewhereyouuseasuctioncuptopick
uptheobject,andthesecondlocationiswhereyouplacetheobject. The
image coordinate ranges from 0 to 1. The rotation angle indicates how
manydegreesyourotatetheobjectclockwise,anditrangesfrom-359to
359.
You have finished: Step 1: Pick up the object at <b>(0.480, 0.367)</b>,
rotate<r>[0]</r>degrees,anddropitat<b>(0.727,0.547)</b>.
Output Step 2: Pick up the <p>rainbow letter V</p> at <b>(0.500, 0.617)</b>,
rotate<r>[0]</r>degrees,anddropitat<b>(0.746,0.617)</b>.
Input
(currentfrontcameraview.)
<task>First put <p>rainbow letter T</p> at <b>(0.500, 0.594), {0.102,
0.188}</b>into <p>wooden bowl</p> at <b>(0.457, 0.531), {0.195,
0.328}</b>thenputtheobjectthatwaspreviouslyatitssouthintothesame
<p>woodenbowl</p>at<b>(0.457,0.531),{0.195,0.328}</b></task>
D-inBC Everyactionyoutakemustincludetwolocationsintheformatof<b>(x,
y)</b>andoneclockwiserotationangleintheformatof<r>[r]</r>. The
firstlocationistheimagecoordinatewhereyouuseasuctioncuptopick
uptheobject,andthesecondlocationiswhereyouplacetheobject. The
image coordinate ranges from 0 to 1. The rotation angle indicates how
manydegreesyourotatetheobjectclockwise,anditrangesfrom-359to
359.
You have finished: Step 1: Pick up the object at <b>(0.480, 0.367)</b>,
rotate<r>[0]</r>degrees,anddropitat<b>(0.727,0.547)</b>.
Output [SameasinBC]
6Figure4:AuxiliarydatasetsforVisuomotorInstructionTuning:Givenaninputtrajectory,wemakeuseof
expertinformation(e.g.,objectdetections)toformulateconversationsrelatedtoauxiliarysemantics,thatinturn
canbeusefulforpolicylearning. Notethehierarchyofdatausedamongdifferentanswertypes,highlighted
with the same color. Only the detections of relevant objects are shown in the figure (right). Please refer to
Tab.6andTab.7fortheactualexamplesofeachdataset.
4.3 InferencePipeline
During inference, the prompt for the Vision-Language Model (VLM) is prepared using the same
template as either inBC or D-inBC. As illustrated in Fig. 3, for a model trained on inBC, each
conversationturnencapsulatesthecurrentvisualobservation,thetaskdescription,andtheprevious
actionsdescribedinthetext.ForamodeltrainedonD-inBC,theprocessstartswithobjectdetection
toconvertanyextraimagesfromthetaskdescriptionintotext. Thetaskdescription,nowincluding
thedetectionoutcomes,isthencombinedwithvisualobservationandtheactionhistorytocomplete
instruction.
ThenLLaRAgeneratesthetextoutputthatcontainsnumericalvalues(e.g.,2Dimagecoordinates)
for a robot action. These numbers can be easily extracted from the text output by following the
decoratorsinthetemplate(e.g.,<b>,<r>). Theextractedrotationanglecanbedirectlymappedto
therotationoftheendeffector. The2Dimagecoordinateswillbefurtherconvertedtorobotaction
spaceviapredefinedmapping,whichcanbeestimatedusingvisualcalibrationinbothsimulatedand
real-worldenvironments.
Afterthat, therobotexecutestheaction, andanewobservationcanbecapturedfromtheenviron-
ment. Thisinitiatesanotherroundofinteraction,settingthestageforthesubsequentaction.
5 SuperchargingVisuomotorInstructionDataset
MotivatedbythesuccessofLLaVA[7]instructionfinetuningdata,andsubsequentdomain-specific
instructiondatasets(e.g.,forreasoning,grounding,orreferring)[9,10,31],LLaRAcreatesauxiliary
roboticsinstructiontuningdatasetsthatenhanceaVLM-basedpolicy. Theideaisthattheauxiliary
datasetswilldriveVLMstolearnabetterspatio-temporalunderstandingofthesceneandeventually
benefitrobotlearning.
More specifically, given a robot trajectory, we generate expert question-answer pairs formulating
conversations that reveal useful information for learning a policy indirectly. In addition to visual
observations,wemakeuseofobjectlabels,locations,andgeometries(e.g.,rotation)togetherwith
taskdescriptionsasexpertinformation. Weprimarilyintroduce6datasetvariants,augmentingex-
isting expert information, such as auxiliary semantics, to better finetune VLMs. In each dataset,
we consider a standard question, which is further rephrased multiple times using GPT-4 to infuse
areasonablediversity. Answersfollowsimplerule-basedtemplatesthatformatexpertinformation
as expected responses for each conversation style. Refer to Fig. 4 for formatting details and the
appendixforqualitativesamplesofeachdataset. Thefollowingparagraphsdescribeeachauxiliary
dataset.
7LocalizationDataset. Wegeneratesingle-turnconversationsonthelocationofasingleobjectina
givenobservation. Theobservationcanbeanyintermediatestep. Foreachconversation,theques-
tion asks for the location of an object and the answer is formulated as “<p>OBJ</p> at <b>(x,
y), {w, h}</b>”Here,(x, y)correspondstothemid-pointoftheobjectboundingboxand{w,
h} represents width and height. As in Sec. 4, the bounding box coordinates are given in the 2D
imagecoordinatesandnormalizedto[0,1].
Detection Dataset. In a more high-level setup, we generate conversations about the whole scene,
given by the locations of every object. The response is formulated as a list of bounding boxes as
above. ThisdatasetenablessceneunderstandingofthefinetunedVLM.
FuturePredictionDataset. Thisdatasetenablesdynamicsmodeling. Givenaninitialobservation,
thequeryoftheconversationincludesasingle-stepactionandasksforatextdescriptionofthenext
observation.Theresponsedescribesthesceneintermsofobjectboundingboxlocationsinthesame
formatasthedetectiondataset.
Spatial Relationship Dataset. Given two specified objects in an image, the query asks about
the 2D spatial configuration (e.g., left, right, above, below) and also the Euclidean distance
and the distance in two axes between the objects. The response describes exactly that, in
theformof“<p>OBJ1</p> is ... from <p>OBJ2</p> with 2D axial distance <d>(x,
y)</d> and euclidean distance <e>z</e>”. Additionally, one exemplar is provided in the
querytohelptheVLMunderstandtheinstructionbetter.
TemporalRelationshipDataset. Thisdatasetfocusesonhowthespatialrelationshipbetweentwo
objectschangesovertime(i.e.,betweentwotimesteps). Similartoactionpredictiondata,thissetup
alsoneedstwoobservations,andhencethesecondobservationisdescribedpurelyintext.Thequery
asksaboutthechangeofrelativespatiallocations(e.g.,getcloserorfurtheraway),andtheresponse
describesexactlythat,intermsofchangeinaxialandEuclideandistance.
Byintroducingtheaforementionedauxiliarydatasets,weexplicitlydrivetheVLMtostrengthenthe
ability of spatial and visuomotor understanding, which benefits robot learning. Note that all these
datasetscanbeautomaticallygeneratedfromtheexistingtrajectories,withoutintroducingexternal
data. InSec.B,weempiricallyexplorethebestpracticestoapplytheseauxiliarydatasets.
6 Experiments
We conduct experiments in both simulated environments and the real world. For the simulated
tasks,weturnaVLMintoageneralistfordiversetasks. Furthermore,weconductreal-worldrobot
experimentsusingthreeprotocols: zero-shotgeneralization,finetuning,andjointtraining.
6.1 SimulationExperiments
Settings. WeemployVIMA-Bench[43],asimulatedtable-toprobotmanipulationenvironmentto
evaluateVLMstrainedbyourroboticsinstructiontuningdataset.Theenvironmentcontains17tasks
and each task is associated with a multi-modal instruction, including text instructions and images
that refer to objects of interest or a particular scene arrangement. The robot action space is two
2Dcoordinatesforpickandplacepositionsandarotation. WefirstuniformlysubsampletheVIMA
dataset[43]toformthreesubsetswithdifferentsizes:VIMA-0.8k,VIMA-8k,andVIMA-80kwhere
thenumberindicatesthenumberofexperttrajectoriesinthedataset.
Methods. We compare three variants of our method: inBC, inBC + Aux, and D-inBC + Aux,
withbaselinesthatfollowstherecipeofRT-2[21],RT-2StyleandD-RT-2Style. Asintroducedin
Sec.4, inBC isthedatasetconvertedfromtheexperttrajectories, D-meanstheimagesinthetask
description are described by text and we will perform the object detection first during inference,
andAuxmeanstheauxiliarydatasetsintroducedinSec.5areincludedinthetrainingset,theletter
afterit(e.g.,(D))indicatesthedetailedconfigurationoftheauxiliarydataset(pleaserefertoTab.2).
Oracle means that the groundtruth bounding box of objects is used. RT-2 Style is similar to inBC
880
80 80
70
60
60 60
50
40 40 40 R DT -R-2 T- -S 2t -y Sl te yle + Oracle
30 LLaRA (inBC)
20 20 20 L LL La aR RA A ( (i Dn -B inC B + C +Au Ox r( aD c) le) )
10 LLaRA (D-inBC + Aux (D))
0 0 0 LLaRA (D-inBC + Aux (D) + Oracle)
103 104 105 103 104 105 103 104 105
Num of expert episodes Num of expert episodes Num of expert episodes
Figure5:PerformanceonthreeVIMAsubsets.x-axisisthenumberofexpertepisodesinthetrainingset.
buttheoutputoftheVLM(i.e.,robotactions)isasetofspecialtokensthatcanbedirectlymapped
toquantizedactions.
Wetrainallmethodsonthesethreedatasetsandevaluatethemwith3levelsofdifficultiesfollowing
thetestprotocol(L1toL3). DuetomissingdataintheoriginalVIMAdataset,wewerenotableto
evaluateourapproachonL4properly(pleasechecktheSec.Bformoredetails). Foreachsetting,
weevaluatethemethodwith20randomseedsandreporttheaveragesuccessratesofeachlevel.For
moreimplementationdetailsandcompleteresultspleaserefertoSec.A,Tab.12, Tab.13,Tab.14,
Tab.15,Tab.16,andTab.17.
Effectiveness of LLaRA Framework. Figure 5 illustrates the performance of selected methods
across various dataset scales. All methods generally benefit from increased training data. Key
observationsinclude:
• inBC consistently surpasses the RT-2 Style baseline, and similarly, D-inBC outperforms
D-RT-2 Style. This supports the effectiveness of our approach, which utilizes instruction
tuningstylebehaviorcloningfordatagenerationandallowsustobenefitfromtheexisting
VLMspretrainedinaconversationstyle.MethodsbasedonRT-2Styleimprovewhenmore
robotsupervisiondataisavailable;however,theysignificantlyunderperformcomparedto
ourmethodswhendataislimited.
• D-inBC generally excels over inBC owing to its explicit delineation of object locations.
ThistrendissimilarlyobservedbetweenD-RT-2StyleandRT-2Style.
• Auxiliary datasets prove to be highly beneficial, particularly when the size of the train-
ingsetislimited. However, inscenarioslikeVIMA-80kwherethedatasetissubstantial,
sometimesD-inBCexhibitsbetterperformanceandthegainfromtheauxiliarydatasetsis
marginal.
We hypothesize that when training data is adequate, the benefits of the auxiliary dataset diminish,
andthesedatamayevendistractthemodelfromeffectivelyperformingthebehaviorcloningtasks.
Itisadvisabletocarefullyregulatetheamountofauxiliarydatawhenthereisanabundanceofexpert
episodestoavoidoverwhelmingtheprimarylearningobjectives. Acomprehensiveablationonthe
auxiliary datasets and other designs is available at Sec. B. By default, we randomly sample from
eachauxiliarydatasetsothatthesizeofeachauxiliarydatasetisidenticaltoinBCorD-inBC.
Ablation on auxiliary datasets. We first study the effectiveness of auxiliary datasets. Tab. 2
showsthedifferentcombinationsoftheauxiliarydatasets. Foreachsetting,werandomlysamplea
sameamountofexamplesfromeachauxiliarydatasetandcombinethemwiththeconvertedbehavior
cloningdataset.The‘*’aftera‘✓’meansthereferenceimagesintheexperttrajectoriesarenotused
togeneratethisdataset,whichhaslessdiversity.
WetrainthemodelonbothinBC andD-inBC withdifferentauxiliarydatasetsettings. OnVIMA-
0.8k,wecontrolthetotalnumberofsamplesfromtheauxiliarydatasetrelativetothesamplesfrom
9
)%(
etaR
sseccuS
1L
)%(
etaR
sseccuS
2L
)%(
etaR
sseccuS
3LTable2: Namingofdifferentauxiliarydatasetconfigurations. Wealwaysrandomlysampleasameamountof
examplesfromeachdataset. Det.: relativesizeofthedetectiondataset;Loc.: relativesizeofthelocalization
dataset;Act.: relativesizeoftheactionpredictiondataset;Fut.: relativesizeofthefuturepredictiondataset;
Spa.:relativesizeofthespatialrelationshipdataset;Temp.:relativesizeofthetemporalrelationshipdataset.
Config Loc. Det. Act. Fut. Spa. Temp.
A ✓ ✓
A* ✓* ✓*
B ✓ ✓ ✓ ✓
C ✓ ✓ ✓ ✓ ✓
D ✓ ✓ ✓ ✓ ✓ ✓
D* ✓* ✓* ✓ ✓ ✓* ✓
40.0
35.0 32.5
37.5 32.5 30.0
35.0
30.0 27.5
32.5 27.5 25.0 A
30.0 A*
25.0 22.5 B
27.5 C
25.0 22.5 20.0 D
20.0 17.5 D*
1 2 3 4 5 6 7 1 2 3 4 5 6 7 1 2 3 4 5 6 7
Total dataset size relative to inBC Total dataset size relative to inBC Total dataset size relative to inBC
Figure6:inBCwithdifferentauxiliarydatasetsettings. EachmodelistrainedonVIMA-0.8kfor2epochs. In
general,themodelperformsbetterwithmoreauxiliarydata.
40 37.5
35 35.0
35 32.5
30
30.0 A
30 25 27.5 A*
B
25 20 25.0 C
D
22.5 D*
1 2 3 4 5 6 7 1 2 3 4 5 6 7 1 2 3 4 5 6 7
Total dataset size relative to D-inBC Total dataset size relative to D-inBC Total dataset size relative to D-inBC
Figure7:D-inBCwithdifferentauxiliarydatasetsettings. EachmodelistrainedonVIMA-0.8kfor2epochs.
Ingeneral,themodelperformsbetterwithmoreauxiliarydata.
theconvertedBCdatasetsandtrainallthemodelsfor2epochs. ResultsinFig.6andFig.7show
that in general, on VIMA-0.8k, the model performs better with more auxiliary data. Experiments
inFig.8alsoshowthattrainingonlyoninBCforlongerepochswillnotimprovetheperformance,
whichhintsthatthemodelisnotunderfitting.
Fig. 8, Fig. 9, and Fig. 10 show the performance change when training on different datasets for
longerepochs.
ComparisontoVIMA. WewouldfirstclarifythedifferencebetweenourmethodandVIMA[43]
in terms of the inputs. VIMA takes both front and top view images from the environment while
oursonlytakethefrontview. WetestthemostcapablemodelreleasedbyVIMA,whichistrained
on660kexperttrajectories, andtheresultsarelistedinTab.3. ComparedtoVIMA[43], ourbest
modelnotonlyachievesbetterperformancebutalsorequireslessinputandistrainedononly12%
ofthedatausedinVIMA.
6.2 Real-worldRobotExperiments
We further conduct zero-shot generalization, finetuning, and joint training experiments in a novel
real-worldenvironment.Forthezero-shotgeneralizationandfinetuning,weusethepretrainedmod-
10
)%(
etaR
sseccuS
1L
)%(
etaR
sseccuS
1L
)%(
etaR
sseccuS
2L
)%(
etaR
sseccuS
2L
)%(
etaR
sseccuS
3L
)%(
etaR
sseccuS
3L30
25 25 30
20 20
20 15 15
10 10 RT-2-Style
10 D-RT-2-Style + Oracle
5 5 LLaRA (inBC)
0 0 0 LLaRA (D-inBC + Oracle)
2 4 6 8 2 4 6 8 2 4 6 8
Relative number of iterations Relative number of iterations Relative number of iterations
Figure8:ModelperformancesonVIMA-0.8kforlongerepochs.
70 60 60
60 50 50
50
40 40
40
30 30
30
20 20 20 RT-2-Style D-RT-2-Style + Oracle
10 10 10 LLaRA (inBC)
0 0 0 LLaRA (D-inBC + Oracle)
2 4 6 8 2 4 6 8 2 4 6 8
Relative number of iterations Relative number of iterations Relative number of iterations
Figure9:ModelperformancesonVIMA-8kforlongerepochs.
80
80 80
70
70
70
60
60
60 50 RT-2-Style
D-RT-2-Style + Oracle
50 50 40 LLaRA (inBC)
LLaRA (D-inBC + Oracle)
2 4 6 8 2 4 6 8 2 4 6 8
Relative number of iterations Relative number of iterations Relative number of iterations
Figure10:ModelperformancesonVIMA-80kforlongerepochs.
Table3: ComparedtoVIMA[43],ourbestmodelnotonlyachievesbetterperformancebutalsorequiresless
inputandistrainedononly12%ofthedatausedinVIMA.
Method Config Data L1(%) L2(%) L3(%)
VIMA[43] VIMA-200M+Oracle 100% 80.7 81.9 77.9
D-inBC+Oracle 12% 87.3 85.4 82.1
LLaRA(Ours) D-inBC+Aux(B)+Oracle 12% 90.0 88.1 79.2
D-inBC+Aux(D)+Oracle 12% 83.8 88.1 78.8
elsfromSec.6.1onlytrainedonsimulatedVIMAdata. Wealsomakethesettingverychallenging
byselectingthepretrainedmodelonlyusing1.2%oftheVIMAtrainingdata(VIMA-8k). Thejoint
trainingsettingusessuchdataandtherobotdata(whichwementionbelow)together, trainingthe
modelfromscratch.
WeusearobotarmwithagripperandanRGBcamerapositionedabovethearmtocollectobserva-
tions. TheRGBcameraismountedinasinglepositionabovetherobotarm,providinga3rdperson
viewofthesceneforobservations. TherobotactionspaceisthesameasinVIMA.Forthemethods
that rely on object detection, we employed one-shot detection using OWLv2 [67] to identify and
generateboundingboxesforeachobjectintheimagesandasuffix‘OD’isaddedtotheendofthe
method.
WebenchmarkeachpolicyonthreetasksthatemulatethosefoundinVIMA-bench:
• T1: “Movethe{object}intothelargebowl.”
11
)%(
etaR
sseccuS
1L
)%(
etaR
sseccuS
1L
)%(
etaR
sseccuS
1L
)%(
etaR
sseccuS
2L
)%(
etaR
sseccuS
2L
)%(
etaR
sseccuS
2L
)%(
etaR
sseccuS
3L
)%(
etaR
sseccuS
3L
)%(
etaR
sseccuS
3L• T2: “Rotatethe{object}by{angle}degrees.”
• T3: “Movethe{object}ontopofthetray.”
wherethe{object}ischosenfromasetof10plastictoysandthe{angle}inT2ischosenbetween0
to180degrees. Inalltasks,allobjectsareplacedrandomlyonthetablebeforerunninganepisode.
AsuccessinT1andT3isiftheobjectisplacedmorethan50%insidethebowloronthetray. A
successinT2isiftheobjectappearstobeclockwiserotatedbytheanglebyvisualinspection. A
visualreferencefortypicaltaskstartstatesandsuccessfulendpositionsareprovidedinFig.11.
T1Start T2Start T3Start T1w/Distractors T2w/Distractor
Start Start
T1Success T2Success T3Success T1w/Distractors T2w/Distractor
Success Success
Figure11:Visualreferenceforstartandsuccessfulendpositionsinthreereal-worldtasks.Wealsoshowsome
exampleshavingdistractingobjectsinthescene.
Zero-shotGeneralization. WebenchmarkthemodelstunedinSec.6.1directlyinthereal-world
settingwithoutfurthertraining,alongwithanewbaselineGPT-4o[1](seeSec.A.5).Theresultsare
presentedastheupperpartofTab.4.Inthissetting,inBCachievesthebestoverallperformance.We
hypothesizethattheauxiliarydatasetsusedininBC+Aux(D)andD-inBC+Aux(C)+ODdrive
the model to focus more on VIMA domain, leading to overfitting. Meanwhile, GPT-4o achieves
commendableperformance, largelyattributabletoitsextensivedatasetandsubstantialparameteri-
zation. However,GPT-4oisnotapplicableforfinetuningorjointtrainingsettings,whichwediscuss
below.
Real-worldRobotDataset. Wefurthercollect1,256real-worldimagesfromthesamereal-world
robotsettingasthein-domaindata.Outoftheseimages,721featureasingleobjectplacedrandomly
and uniformly on the table by a robotic arm, while the remaining images display multiple objects
scattered across the table. We employed one-shot detection using OWLv2 [67] to identify and
generateboundingboxesforeachobjectintheimages.
Theseimages,alongwiththeirboundingboxdata,havebeenutilizedtocreatetwospecializedin-
domain instruction tuning datasets, each containing 2,447 single-turn conversations of equivalent
size: xArm-Det and xArm-Action. The xArm-Det dataset includes conversations that mirror the
structurefromourauxiliaryobjectlocalizationanddetectiondataset,with1,191examplesdedicated
toobjectlocalizationand1,256toobjectdetection.Incontrast,thexArm-Actiondatasetisdesigned
similarlytodataforD-inBC,featuringtwodistincttasks: Rotation: rotatinganobject(akintoTask
T2with1,191examples)andPutontop: puttingoneobjectontopofanother(reminiscentofTask
T3with1,256examples).
In the Rotation task, the agent is instructed to rotate a specific object by a random angle ranging
from -180 to 180 degrees, while ensuring the object’s position remains unchanged post-rotation.
Theexpertactionisstraightforwardlygeneratedbydirectlyreplicatingthespecifiedrotationangle
fromthetaskdescriptionandthepickandplacepointsaresettothecentroidoftheobjectbounding
boxfromobjectdetection.
InthePutonToptask,applicabletoimagescontainingmorethantwoobjects,werandomlyselect
twoobjects. Thetaskinvolvesgeneratingacommandthatdirectsplacingoneobjectatoptheother.
Theexpertactions,specificallythepickandplacepoints,aredeterminedusingthecentroidsofthe
boundingboxesofthetwoselectedobjectsfromobjectdetection.
12Table4:Real-worldrobotexperimentresults.‘Aux’meanstrainedonauxiliarydatasetsand‘OD’meansusing
an object detector (OWLv2, base-16) to describe the observation before the VLM query. In Protocol, ‘ZS’
standsforzero-shotevaluation,‘FT’meansthemodelisfirsttrainedonVIMAfor2epochsandthenfinetuned
onAdd.Datafor1epoch.‘JT’meansthemodelistrainedonthecombinationofVIMAandAdd.Datafor2
epochs.
Protocol Method Add.Data T1(%) T2(%) T3(%) Avg.(%)
RT-2Style 0 0 0 0
GPT-4o 20 45 30 31.6
ZS inBC 40 50 20 36.6
LLaRA inBC+Aux(D) 10 30 10 16.6
(Ours) D-inBC+OD 30 40 25 31.6
D-inBC+Aux(C)+OD 35 40 5 26.6
inBC xArm-Action 60 75 55 63.3
D-inBC+OD xArm-Det 65 95 45 68.3
LLaRA
JT D-inBC+OD xArm-Action 65 55 25 48.3
(Ours)
D-inBC+Aux(C)+OD xArm-Det 70 95 85 83.3
D-inBC+Aux(C)+OD xArm-Action 45 70 20 53.3
RT-2Style xArm-Det 0 0 0 0
inBC xArm-Det 0 0 0 0
FT inBC xArm-Action 30 45 5 26.6
LLaRA
D-inBC+OD xArm-Action 45 80 55 60
(Ours)
D-inBC+Aux(C)+OD xArm-Det 70 90 70 76.6
D-inBC+Aux(C)+OD xArm-Action 90 100 85 91.6
Tab.8showsexamplesfromthedatasets.
FinetuningandJointTraining. Inthefinetuningsetting,themodelstrainedonVIMAarefurther
tuned on the new real-world datasets for 1 epoch and the evaluation results are presented at the
bottom of Tab. 4. In the joint training setting, we combine both VIMA data with xArm-Det (or
xArm-Action)andfinetuneaVLMjointlyonbothsimulatedandreal-worlddatasets.
Ingeneral,finetuningoutperformsjointtraining,probablybecauseitallowsthemodeltofocusmore
onreal-worlddatadistribution,ratherthandistractingitwithsimulatedVIMAdata. Inordertoget
thebestperformance,theuseofxArm-Actionwasimportant. Interestingly,inthejointtrainingset-
ting,xArm-DetwasequallyormorebeneficialthanxArm-Action,alsohighlightingtheimportance
of detection and localization information from the real-world setting. Auxiliary data contributes
to a large boost in performance during finetuning, showing that explicitly uncovering information
fromtheexistingin-domaindatacanbenefitthemodel. inBCperformedrelativelywellinthejoint
trainingsetting,butitdisplayedverypoorperformancewhenfinetunedonxArm-Det. Interestingly,
it seems inBC pretrained with VIMA is not trained to take advantage of the such new in-domain
roboticsdata,performingworsethanzero-shot.
Similar to our findings in simulated experiments, LLaRA benefits from pretrained VLM since
LLaRA is trained on a dataset that has been organized in a conversation style that is similar to
pretrainingtheVLM.Incontrast,RT-2Stylerequiresalargeamountofdatasoitsuffersmorefrom
limiteddatainourreal-worldexperiments. Ingeneral, LLaRAdemonstratesarobustcapacityfor
generalization. OnecaneasilytransferthepretrainedLLaRAtoaparticulardomainwithminimal
datarequirementsandfinetuning.
Qualitative Examples on Unseen Real-world Tasks. In Fig. 12 we qualitatively show that
LLaRA can do some unseen tasks that are not in the training dataset. In the first example, we
askLLaRAto‘placeallotherobjectsintothelargebowl’. LLaRAisabletorecognizetheobjects
to move and not to move and outputs the correct actions. In the second example, we let LLaRA
‘get the weight of the tomato and put it back’. LLaRA can put the tomato on the scale and move
thetomatobacktothesimilarplaceastheinitialposition,showingtheunderstandingof‘puttingit
back’. TheseexamplesshowthepotentialgeneralizationabilityofLLaRA.
13LLaRA Input Observations LLaRA Input Prompt LLaRA Input Observations LLaRA Input Prompt
Place all other objects into Get the weight of the tomato
the large bowl. and put it back.
Robot Actions Robot Actions
Figure12:ExamplesofusingLLaRAforunseentasks.Weshowtheinputtextsfromtheuserandinputimages
ateachstepofthetask,andimagesabouttherobot’sactionstoaccomplisheachstep.
7 Conclusion
WepresentLLaRA,aframeworkthatturnsaninstruction-tunedvisionlanguagemodel(VLM)into
arobotpolicyusingcuratedinstructiontuningdatasets. Wefirstconstructaconversation-stylein-
struction dataset using robot trajectory data. A VLM instruction tuned on this data confirms the
viabilityofourframeworktotacklerobotmanipulationtasks. Wealsoconstructauxiliarydatasets
usingthesamerobottrajectorydatathroughself-supervision. Experimentsacrossseveralsynthetic
environments and robot manipulation tasks in the real world validate the effectiveness of our pro-
posedLLaRAframework.
Acknowledgments
ThisworkwassupportedbyElectronicsandTelecommunicationsResearchInstitute(ETRI)grant
funded by the Korean government foundation. [24ZB1200, Research of Human-centered au-
tonomousintelligencesystemoriginaltechnology]. TheauthorswouldliketothankXiaodongLiu
andHanyiYufortheirvaluableinput.
References
[1] OpenAI. GPT-4technicalreport. https://arxiv.org/abs/2303.08774,2023.
[2] H.Touvron,L.Martin,K.Stone,P.Albert,A.Almahairi,Y.Babaei,N.Bashlykov,S.Batra,
P.Bhargava,S.Bhosale,etal. Llama2: Openfoundationandfine-tunedchatmodels. arXiv
preprintarXiv:2307.09288,2023.
[3] M.AI. Llama3,2024. URLhttps://llama.meta.com/llama3/. Accessed: 2024-06-24.
[4] G.Team,R.Anil,S.Borgeaud,Y.Wu,J.-B.Alayrac,J.Yu,R.Soricut,J.Schalkwyk,A.M.
Dai,A.Hauth,etal. Gemini: afamilyofhighlycapablemultimodalmodels. arXivpreprint
arXiv:2312.11805,2023.
[5] R.Taori,I.Gulrajani,T.Zhang,Y.Dubois,X.Li,C.Guestrin,P.Liang,andT.B.Hashimoto.
Stanfordalpaca:Aninstruction-followingllamamodel.https://github.com/tatsu-lab/
stanford_alpaca,2023.
[6] W.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang, L. Zheng, S. Zhuang, Y. Zhuang,
J. E. Gonzalez, I. Stoica, and E. P. Xing. Vicuna: An open-source chatbot impress-
ing gpt-4 with 90%* chatgpt quality, March 2023. URL https://lmsys.org/blog/
2023-03-30-vicuna/.
14[7] H.Liu,C.Li,Q.Wu,andY.J.Lee. Visualinstructiontuning. InAdvancesinNeuralInforma-
tionProcessingSystems(NeurIPS),2023.
[8] H.Liu,C.Li,Y.Li,andY.J.Lee. Improvedbaselineswithvisualinstructiontuning. InPro-
ceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition(CVPR),
pages26296–26306,2024.
[9] K.Chen,Z.Zhang,W.Zeng,R.Zhang,F.Zhu,andR.Zhao. Shikra: Unleashingmultimodal
llm’sreferentialdialoguemagic. arXivpreprintarXiv:2306.15195,2023.
[10] K.Ranasinghe, S.N.Shukla, O.Poursaeed, M.S.Ryoo, andT.-Y.Lin. Learningtolocalize
objectsimprovesspatialreasoninginvisual-llms.InProceedingsoftheIEEE/CVFConference
onComputerVisionandPatternRecognition(CVPR),2024.
[11] C. Li, C. Wong, S. Zhang, N. Usuyama, H. Liu, J. Yang, T. Naumann, H. Poon, and J. Gao.
Llava-med: Trainingalargelanguage-and-visionassistantforbiomedicineinoneday. arXiv
preprintarXiv:2306.00890,2023.
[12] K.Kuckreja,M.S.Danish,M.Naseer,A.Das,S.Khan,andF.S.Khan. Geochat: Grounded
largevision-languagemodelforremotesensing. ProceedingsoftheIEEE/CVFConferenceon
ComputerVisionandPatternRecognition(CVPR),2024.
[13] Y. Hong, H. Zhen, P. Chen, S. Zheng, Y. Du, Z. Chen, and C. Gan. 3d-llm: Injecting the
3dworldintolargelanguagemodels. InAdvancesinNeuralInformationProcessingSystems
(NeurIPS),2023.
[14] A. Zeng, A. Wong, S. Welker, K. Choromanski, F. Tombari, A. Purohit, M. Ryoo, V. Sind-
hwani,J.Lee,V.Vanhoucke,etal. Socraticmodels:Composingzero-shotmultimodalreason-
ingwithlanguage. arXivpreprintarXiv:2204.00598,2022.
[15] A.Brohan,Y.Chebotar,C.Finn,K.Hausman,A.Herzog,D.Ho,J.Ibarz,A.Irpan,E.Jang,
R. Julian, et al. Do as i can, not as i say: Grounding language in robotic affordances. In
ConferenceonRobotLearning(CoRL),pages287–318.PMLR,2023.
[16] D. Driess, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter, A. Wahid, J. Tompson,
Q. Vuong, T. Yu, et al. Palm-e: An embodied multimodal language model. arXiv preprint
arXiv:2303.03378,2023.
[17] S.H.Vemprala,R.Bonatti,A.Bucker,andA.Kapoor. Chatgptforrobotics:Designprinciples
andmodelabilities. IEEEAccess,2024.
[18] M.J.Kim,K.Pertsch,S.Karamcheti,T.Xiao,A.Balakrishna,S.Nair,R.Rafailov,E.Foster,
G. Lam, P. Sanketi, et al. Openvla: An open-source vision-language-action model. arXiv
preprintarXiv:2406.09246,2024.
[19] D.Niu,Y.Sharma,G.Biamby,J.Quenum,Y.Bai,B.Shi,T.Darrell,andR.Herzig. Llarva:
Vision-action instruction tuning enhances robot learning. arXiv preprint arXiv:2406.11815,
2024.
[20] A.Brohan,N.Brown,J.Carbajal,Y.Chebotar,J.Dabis,C.Finn,K.Gopalakrishnan,K.Haus-
man, A. Herzog, J. Hsu, J. Ibarz, B. Ichter, A. Irpan, T. Jackson, S. Jesmonth, N. J. Joshi,
R.Julian,D.Kalashnikov,Y.Kuang,I.Leal,K.-H.Lee,S.Levine,Y.Lu,U.Malla,D.Manju-
nath,I.Mordatch,O.Nachum,C.Parada,J.Peralta,E.Perez,K.Pertsch,J.Quiambao,K.Rao,
M.Ryoo, G.Salazar, P.Sanketi, K.Sayed, J.Singh, S.Sontakke, A.Stone, C.Tan, H.Tran,
V.Vanhoucke,S.Vega,Q.Vuong,F.Xia,T.Xiao,P.Xu,S.Xu,T.Yu,andB.Zitkovich. Rt-
1: Roboticstransformerforreal-worldcontrolatscale. Roboticsscienceandsystems(RSS),
2023.
15[21] A.Brohan,N.Brown,J.Carbajal,Y.Chebotar,X.Chen,K.Choromanski,T.Ding,D.Driess,
A. Dubey, C. Finn, et al. Rt-2: Vision-language-action models transfer web knowledge to
roboticcontrol. arXivpreprintarXiv:2307.15818,2023.
[22] A. Padalkar, A. Pooley, A. Jain, A. Bewley, A. Herzog, A. Irpan, A. Khazatsky, A. Rai,
A. Singh, A. Brohan, et al. Open x-embodiment: Robotic learning datasets and rt-x mod-
els. arXivpreprintarXiv:2310.08864,2023.
[23] J. Yu, X. Wang, S. Tu, S. Cao, D. Zhang-Li, X. Lv, H. Peng, Z. Yao, X. Zhang, H. Li, et al.
Kola: Carefully benchmarking world knowledge of large language models. arXiv preprint
arXiv:2306.09296,2023.
[24] Z. Zhao, W. S. Lee, and D. Hsu. Large language models as commonsense knowledge for
large-scaletaskplanning. AdvancesinNeuralInformationProcessingSystems(NeurIPS),36,
2024.
[25] A. Creswell and M. Shanahan. Faithful reasoning using large language models.
ArXiv, abs/2208.14271, 2022. URL https://api.semanticscholar.org/CorpusID:
251929296.
[26] X. Liu, D. Yin, C. Zhang, Y. Feng, and D. Zhao. The magic of if: Investigating causal rea-
soning abilities in large language models of code. In Annual Meeting of the Association for
ComputationalLinguistics,2023. URLhttps://api.semanticscholar.org/CorpusID:
258968140.
[27] H.Zhen,X.Qiu,P.Chen,J.Yang,X.Yan,Y.Du,Y.Hong,andC.Gan. 3d-vla: A3dvision-
language-actiongenerativeworldmodel. arXivpreprintarXiv:2403.09631,2024.
[28] S.Zhang, P.Sun, S.Chen, M.Xiao, W.Shao, W.Zhang, K.Chen, andP.Luo. Gpt4roi: In-
structiontuninglargelanguagemodelonregion-of-interest. arXivpreprintarXiv:2307.03601,
2023.
[29] Y.Zhao,Z.Lin,D.Zhou,Z.Huang,J.Feng,andB.Kang.Bubogpt:Enablingvisualgrounding
inmulti-modalllms. arXivpreprintarXiv:2307.08581,2023.
[30] Y.Zang,W.Li,J.Han,K.Zhou,andC.C.Loy. Contextualobjectdetectionwithmultimodal
largelanguagemodels. arXivpreprintarXiv:2305.18279,2023.
[31] Z. Peng, W. Wang, L. Dong, Y. Hao, S. Huang, S. Ma, and F. Wei. Kosmos-2: Grounding
multimodallargelanguagemodelstotheworld. arXivpreprintarXiv:2306.14824,2023.
[32] H. You, H. Zhang, Z. Gan, X. Du, B. Zhang, Z. Wang, L. Cao, S.-F. Chang, and Y. Yang.
Ferret: Referandgroundanythinganywhereatanygranularity. ArXiv,abs/2310.07704,2023.
URLhttps://api.semanticscholar.org/CorpusID:263834718.
[33] M. Cai, H. Liu, D. Park, S. K. Mustikovela, G. P. Meyer, Y. Chai, and Y. J. Lee. Vip-llava:
Makinglargemultimodalmodelsunderstandarbitraryvisualprompts. InProceedingsofthe
IEEE/CVFConferenceonComputerVisionandPatternRecognition(CVPR),2024.
[34] A.Shtedritski,C.Rupprecht,andA.Vedaldi. Whatdoesclipknowaboutaredcircle: Visual
prompt engineering for vlms. In Proceedings of the International Conference on Computer
Vision(ICCV),2023.
[35] S. Nasiriany, F. Xia, W. Yu, T. Xiao, J. Liang, I. Dasgupta, A. Xie, D. Driess, A. Wahid,
Z.Xu,Q.Vuong,T.Zhang,E.T.-W.Lee,K.-H.L.Lee,P.Xu,S.Kirmani,Y.Zhu,A.Zeng,
K. Hausman, N. Heess, C. Finn, S. Levine, and B. Ichter. Pivot: Iterative visual prompting
elicitsactionableknowledgeforvlms. arXivpreprintarXiv:2402.07872,2024.
16[36] S.Qian,W.Chen,M.Bai,X.Zhou,Z.Tu,andE.L.Li. Affordancellm:Groundingaffordance
from vision language models. In Proceedings of the IEEE/CVF Conference on Computer
VisionandPatternRecognition(CVPR),2024.
[37] N. Ingelhag, J. Munkeby, v. J. Haastregt, A. Varava, M. C. Welle, and D. Kragic. A robotic
skill learning system built upon diffusion policies and foundation models. arXiv preprint
arXiv:2403.16730,2024.
[38] Y.Wu, Y.Fan, P.P.Liang, A.Azaria, Y.Li, andT.M.Mitchell. Readandreaptherewards:
Learningtoplayatariwiththehelpofinstructionmanuals. InAdvancesinNeuralInformation
ProcessingSystems(NeurIPS),2023.
[39] T.Yoneda, J.Fang, P.Li, H.Zhang, T.Jiang, S.Lin, B.Picker, D.Yunis, H.Mei, andM.R.
Walter. Statler: State-maintaining language models for embodied reasoning. arXiv preprint
arXiv:2306.17840,2023.
[40] S.Reed,K.Zolna,E.Parisotto,S.G.Colmenarejo,A.Novikov,G.Barth-Maron,M.Gimenez,
Y. Sulsky, J. Kay, J. T. Springenberg, T. Eccles, J. Bruce, A. Razavi, A. Edwards, N. Heess,
Y.Chen,R.Hadsell,O.Vinyals,M.Bordbar,andF.deNando. Ageneralistagent. InTrans.
onMachineLearningResearch,2022.
[41] H. Wu, Y. Jing, C. Cheang, G. Chen, J. Xu, X. Li, M. Liu, H. Li, and T. Kong. Unleash-
ing large-scale video generative pre-training for visual robot manipulation. arXiv preprint
arXiv:2312.13139,2023.
[42] Octo Model Team, D. Ghosh, H. Walke, K. Pertsch, K. Black, O. Mees, S. Dasari, J. Hejna,
C. Xu, J. Luo, T. Kreiman, Y. Tan, L. Y. Chen, P. Sanketi, Q. Vuong, T. Xiao, D. Sadigh,
C.Finn,andS.Levine. Octo: Anopen-sourcegeneralistrobotpolicy. InRoboticsscienceand
systems(RSS),Delft,Netherlands,2024.
[43] Y.Jiang,A.Gupta,Z.Zhang,G.Wang,Y.Dou,Y.Chen,L.Fei-Fei,A.Anandkumar,Y.Zhu,
andL.Fan. Vima: Generalrobotmanipulationwithmultimodalprompts. InProceedingsof
theInternationalConferenceonMachineLearning(ICML),2023.
[44] Y. Bazi, L. Bashmal, M. M. Al Rahhal, R. Ricci, and F. Melgani. Rs-llava: A large vision-
language model for joint captioning and question answering in remote sensing imagery. Re-
moteSensing,16(9):1477,2024.
[45] O.Thawkar,A.Shaker,S.S.Mullappilly,H.Cholakkal,R.M.Anwer,S.Khan,J.Laaksonen,
and F. S. Khan. Xraygpt: Chest radiographs summarization using medical vision-language
models. arXiv: 2306.07971,2023.
[46] K. Ranasinghe, X. Li, K. Kahatapitiya, and M. S. Ryoo. Understanding long videos in one
multimodallanguagemodelpass. arXivpreprintarXiv:2403.16998,2024.
[47] L.PintoandA.Gupta. Supersizingself-supervision: Learningtograspfrom50ktriesand700
robot hours. In IEEE International Conference on Robotics and Automation (ICRA), pages
3406–3413.IEEE,2016.
[48] P.Sermanet,C.Lynch,Y.Chebotar,J.Hsu,E.Jang,S.Schaal,S.Levine,andG.Brain. Time-
contrastivenetworks: Self-supervisedlearningfromvideo. InIEEEInternationalConference
onRoboticsandAutomation(ICRA),pages1134–1141.IEEE,2018.
[49] X.Li,J.Shang,S.Das,andM.Ryoo. Doesself-supervisedlearningreallyimprovereinforce-
mentlearningfrompixels? InAdvancesinNeuralInformationProcessingSystems(NeurIPS),
volume35,pages30865–30881,2022.
[50] X.Li,V.Belagali,J.Shang,andM.S.Ryoo. Crosswaydiffusion: Improvingdiffusion-based
visuomotor policy via self-supervised learning. IEEE International Conference on Robotics
andAutomation(ICRA),2024.
17[51] J.Wang,S.Dasari,M.K.Srirama,S.Tulsiani,andA.Gupta. Manipulatebyseeing: Creating
manipulationcontrollersfrompre-trainedrepresentations. InProceedingsoftheInternational
ConferenceonComputerVision(ICCV),pages3859–3868,2023.
[52] C.Doersch,A.Gupta,andA.A.Efros.Unsupervisedvisualrepresentationlearningbycontext
prediction. InProceedingsoftheInternationalConferenceonComputerVision(ICCV),pages
1422–1430,2015.
[53] S. Yun, J. Kim, D. Han, H. Song, J.-W. Ha, and J. Shin. Time is matter: Temporal self-
supervisionforvideotransformers. arXivpreprintarXiv:2207.09067,2022.
[54] K. Ranasinghe, M. Naseer, S. H. Khan, F. S. Khan, and M. S. Ryoo. Self-supervised video
transformer.ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecog-
nition(CVPR),pages2864–2874,2022.
[55] J. Walker, A. Razavi, and A. van den Oord. Predicting video with vqvae.
ArXiv, abs/2103.01950, 2021. URL https://api.semanticscholar.org/CorpusID:
232092596.
[56] M.Oquab,T.Darcet,T.Moutakanni,H.Vo,M.Szafraniec,V.Khalidov,P.Fernandez,D.Haz-
iza,F.Massa,A.El-Nouby,etal.Dinov2:Learningrobustvisualfeatureswithoutsupervision.
arXivpreprintarXiv:2304.07193,2023.
[57] X. Zhai, B. Mustafa, A. Kolesnikov, and L. Beyer. Sigmoid loss for language image pre-
training. In Proceedings of the International Conference on Computer Vision (ICCV), pages
11975–11986,2023.
[58] W.Yuan,J.Duan,V.Blukis,W.Pumacay,R.Krishna,A.Murali,A.Mousavian,andD.Fox.
Robopoint: A vision-language model for spatial affordance prediction for robotics. arXiv
preprintarXiv:2406.10721,2024.
[59] J.Liang,W.Huang,F.Xia,P.Xu,K.Hausman,B.Ichter,P.Florence,andA.Zeng. Codeas
policies: Languagemodelprogramsforembodiedcontrol. InIEEEInternationalConference
onRoboticsandAutomation(ICRA),pages9493–9500.IEEE,2023.
[60] I. Singh, V. Blukis, A. Mousavian, A. Goyal, D. Xu, J. Tremblay, D. Fox, J. Thomason, and
A. Garg. Progprompt: Generating situated robot task plans using large language models.
InIEEEInternationalConferenceonRoboticsandAutomation(ICRA),pages11523–11530.
IEEE,2023.
[61] W.Huang,C.Wang,R.Zhang,Y.Li,J.Wu,andL.Fei-Fei. Voxposer: Composable3dvalue
mapsforroboticmanipulationwithlanguagemodels. arXivpreprintarXiv:2307.05973,2023.
[62] F. Liu, K. Fang, P. Abbeel, and S. Levine. Moka: Open-vocabulary robotic manipulation
throughmark-basedvisualprompting. arXivpreprintarXiv:2403.03174,2024.
[63] J. Huang, J. Zhang, K. Jiang, H. Qiu, and S. Lu. Visual instruction tuning towards general-
purposemultimodalmodel: Asurvey. arXivpreprintarXiv:2312.16602,2023.
[64] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell,
P.Mishkin, J.Clark, etal. Learningtransferablevisualmodelsfromnaturallanguagesuper-
vision. InProceedingsoftheInternationalConferenceonMachineLearning(ICML),pages
8748–8763.PMLR,2021.
[65] J.Bai,S.Bai,S.Yang,S.Wang,S.Tan,P.Wang,J.Lin,C.Zhou,andJ.Zhou. Qwen-vl: A
frontierlargevision-languagemodelwithversatileabilities. arXivpreprintarXiv:2308.12966,
2023.
18[66] F.Li,R.Zhang,H.Zhang,Y.Zhang,B.Li,W.Li,Z.Ma,andC.Li.Llava-next:Tacklingmulti-
image, video, and 3d in large multimodal models, June 2024. URL https://llava-vl.
github.io/blog/2024-06-16-llava-next-interleave/.
[67] M. Minderer, A. Gritsenko, and N. Houlsby. Scaling open-vocabulary object detection. Ad-
vancesinNeuralInformationProcessingSystems(NeurIPS),36,2024.
[68] K.He,G.Gkioxari,P.Dollár,andR.Girshick.Maskr-cnn.InProceedingsoftheInternational
ConferenceonComputerVision(ICCV),pages2961–2969,2017.
[69] K.He, X.Zhang, S.Ren, andJ.Sun. Deepresiduallearningforimagerecognition. InPro-
ceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition(CVPR),
pages770–778,2016.
[70] T.-Y.Lin,M.Maire,S.Belongie,J.Hays,P.Perona,D.Ramanan,P.Dollár,andC.L.Zitnick.
Microsoftcoco: Commonobjectsincontext. InProceedingsoftheEuropeanConferenceon
ComputerVision(ECCV),pages740–755.Springer,2014.
19The appendix will cover all the implementation details, expanded experiment results, and further
discussions.
A ImplementationDetails
In this section, we provide more implementation details regarding preparing the datasets, training
themodel,andthereal-worldrobotexperimentsetting.
A.1 DatasetPreparation
Inthissubsection,wegivemoredetailsandexamplesonthemajordatasetsusedinthispaper.
A.1.1 BuildinBCDatasetfromExpertTrajectories
We formulate the dataset in a single-image single-turn conversation setting to emulate a policy
where the user queries the Vision Language Model (VLM) with the current observation, and the
VLM generates a response that can be directly translated into a concrete action, as introduced in
Sec. 4.2. The image and text from the current observation can be directly processed by the VLM
usingafixedtemplate,whiletheconversionofnumericalactionsintotextisfacilitatedthroughthe
useofnormalized2Dimagecoordinates. Tab.1containsanexampleusingaVIMA-Benchsample.
GiventhecurrentlimitationsofLLaVA[7],tooptimizeperformance,weproposetwotechniques:
• ActionHistoryinQueryEachquerytotheVLMincludesahistoryofactionspreviously
executedintheepisode. ThisapproachenhancestheVLM’sunderstandingoftaskcontext
withintheconstraintsofasingle-turnconversation. (SeetheorangetextinTab.1)
• Plan‘Twice’ActOnce-Multi-stepPlanningInactiongeneration,thedatasetisprepared
suchthattheVLMisdesignedtogenerateallsuccessiveactionsintheresponse,literally
performingmulti-stepactionplanning. However,onlythefirstactionisexecuted. Forthe
nextstate,wequerytheVLMagainwithanewobservationandaquestionandonlytake
thefirstactiongeneratedbytheVLM.
Thesedesignsprovedbeneficialinourablationstudy,asdetailedinTab.9.
TherestofthissectiondetailstheprocessonVIMA-Bench[43]. VIMA-Benchintroduces17sim-
ulatedrobotmanipulationtasksusingmultimodalpromptsthatcombinetextandreferenceimages.
Thefullaccompanyingdatasetincludes660kexperttrajectories,covering13ofthe17tasks.
Alltasksoccurinanenvironmentwherearobotarm,equippedwitheitheraspatulaorasuctioncup,
ispositionedalongsideaflattable. Multipleobjectswithdiversetexturesarerandomlyinitialized
onthetableaccordingtothespecifictasksetting.
Eachepisodeinthedatasetfeaturesamultimodaltaskdescriptionthatclarifiestheepisode’sgoal,
incorporating images referred to as ‘reference images.’ Two third-person view cameras capture
the scene, providing a top-down view and a front view looking down at the table. The dataset
includesRGB,instancesegmentationmasksandmetainformation(e.g.,texturesandshapes)ofthe
objectsinimagesforthereferenceimagesandtheobservationscapturedbythesecameras. Wefirst
extracttheboundingboxesofeachobjectfromtheinstance-levelsegmentationmask,asanobject
detectionoracle. Additionally,thedatasetcontainstheexpertactionsequencerequiredtocomplete
the episode. The action space consists of two poses: for the robot equipped with a spatula, these
posesindicatethestartandendpointsofapush;fortherobotwithasuctioncup,theyspecifywhere
therobotpicksupandplacesanobject.
DuetotheconstraintsofLLaVA[7]andourfindingspresentedinTab.10,ineachconversation,only
thecurrentimagefromthefrontviewcameraisretainedasthevisualinputtotheVLM.Otherrefer-
enceimagesaresubstitutedwithshortcaptionsthatdescribethetexture(color)andshapeofthere-
ferredobject,whicharedetailsextractedfromthemetainformation(SeeinBCinTab.1). However,
20in principle, there is nothing stopping us from extending the current framework to a multi-image
setting,takingadvantageofaVLMthatcanhandlemultipleinterleavingimagesinaconversation.
A.1.2 BuildD-inBCfrominBC
WhileinBCinTab.1hasshownitsgreatpowerinmanyaspectswhenthereferenceimagecontains
ascenethathasmultipleobjectsinsteadofone,theinBCwillfailtodeliveranyusefulinformation
(e.g.,Tab.5). Toaddressthisissue,weutilizeanobjectdetectortoparseanimageofasceneinto
alistofobjectswithitscorrespondingboundingbox. ThenewdatasetisnamedD-inBCbecausea
referenceimagewillbe‘described’asalistofobjectsnow. Tab.5showsanexamplewhereD-inBC
deliversmorecriticalinformationthaninBC.
A.1.3 AuxiliaryDatasets
TheauxiliarydatasetsarecreatedusingthetemplateoutlinedinFig.4. Duringdatasetgeneration,
for each sample, one template is randomly selected from a pool of 15 templates. These templates
wereinitiallyrephrasedfromasinglesentenceusingGPT-4[1]. Thefulllistofthepoolsarelisted
inTab.20,Tab.19,Tab.21,Tab.22,Tab.23,Tab.24,Tab.25,Tab.26,Tab.27,andTab.28. The
qualitativeexamplesareavailableinTab.6andTab.7.
A.2 Training
We initiate training using a pretrained LLaVA-1.5-7B [8] model and finetune all parameters, in-
cludingthelanguagemodelandtheprojectionlayer,withtheexceptionofthevisionencoder. The
trainingsettingscloselyalignwiththoseoftheoriginalLLaVAstage2. However,forVIMA-0.8k
andVIMA-8k,weemployabatchsizeof32,whereasforVIMA-80k,werestorethebatchsizeto
128.
A.3 Inference
Duringinference,forthemodelstrainedoninBC,anewconversationisinitiatedateachtimestamp.
Themodelisqueriedwithacurrentimageobservationandaninstructionthatmirrorsthoseinthe
dataset. Additionally, a prompt randomly selected from Tab. 18 is added to the instruction during
inferenceonly,althoughitappearstohavelimitedimpactinretrospect.
Modelsamplingisdisabledduringtraining,ensuringthatthemodelconsistentlyoutputsthetoken
withthehighestprobability. Thisapproachisdesignedtomaximizetheconsistencyandreliability
ofthemodel’sresponses.
ForthemodelstrainedonD-inBC,beforewequerytheVLM,wewouldfirstrunobjectdetectionon
allreferenceimagesandtakethedetectionresultstofilltheinstructiontemplateofD-inBC.Besides
this,thesettingsareidenticaltothemodelsintroducedahead.
In this paper, we explore three approaches to object detection. The first method involves using a
singlequeryfromTab.20andemployingthesameVLMasthepolicytoperformobjectdetection.
ThesecondapproachutilizesaMask-RCNN[68]model,whichfeaturesaResNet50[69]backbone
pretrainedontheCOCOdataset[70]andsubsequentlyfinetunedusingtheVIMAdataset. Asuffix
‘OD’willbeaddedtothemodelnameifthemodelusesthismethod.
Finally, we test the versions that use the groundtruth detection results. A suffix ‘Oracle’ will be
addedtothemodelnameifthemodelusesthisinformation.
A.4 RT-2-Stylebaseline
FortheRT-2-Stylebaseline,theprocedurebeginsbynormalizingeachelementoftheactionvector
toarangefrom0to1. Wethenquantizethesevaluesinto256binsandmaptheresultingdiscrete
valuestotokensindexedfrom31000to31255inthetokenizer. Thedatasetemployedissimilarto
21Table5:Anothercomparisonbetweentwoconvertedinstructiontuningdatasets.Inthisexample,thereference
imagesinthetaskdescriptiondepictsceneswithmultipleobjects. Thetaskdescriptionoftheepisodeisin
blue.Thedescriptionofanobjectinthereferenceimage(oracledetectionresults)isinmagenta.
{frame_0}
{frame_1}
OriginalBC
{frame_2}
Task Stackobjectsinthisorder{frame_0}{frame_1}{frame_2}
Input
(currentfrontcameraview.)
<task>Stackobjectsinthisorderframe_0frame_1frame_2.</task>
Everyactionyoutakemustincludetwolocationsintheformatof<b>(x,
y)</b>andoneclockwiserotationangleintheformatof<r>[r]</r>. The
firstlocationistheimagecoordinatewhereyouuseasuctioncuptopick
inBC uptheobject,andthesecondlocationiswhereyouplacetheobject. The
image coordinate ranges from 0 to 1. The rotation angle indicates how
manydegreesyourotatetheobjectclockwise,anditrangesfrom-359to
359.
Output Step 1: Pick up the <p>green paisley letter V</p> at <b>(0.465,
0.617)</b>, rotate <r>[0]</r> degrees, and drop it at <b>(0.711,
0.617)</b>.
Step2: Pickupthe<p>redletterV</p>at<b>(0.281,0.633)</b>,rotate
<r>[0]</r>degrees,anddropitat<b>(0.770,0.633)</b>.
Input
(currentfrontcameraview.)
<task>Stackobjectsinthisorder
<scene><p>red letter V</p> at <b>(0.254, 0.570), {0.094, 0.156}</b>.
<p>green paisley letter V</p> at <b>(0.500, 0.570), {0.094,
0.156}</b>. <p>rainbow letter V</p> at <b>(0.746, 0.570), {0.094,
0.156}</b>.</scene>
<scene><p>red letter V</p> at <b>(0.254, 0.570), {0.094, 0.156}</b>.
<p>rainbow letter V</p> at <b>(0.746, 0.578), {0.094, 0.133}</b>.
<p>green paisley letter V</p> at <b>(0.746, 0.516), {0.094,
0.148}</b>.</scene>
D-inBC
<scene><p>rainbow letter V</p> at <b>(0.746, 0.602), {0.094,
0.086}</b>. <p>green paisley letter V</p> at <b>(0.746, 0.531),
{0.094, 0.125}</b>. <p>red letter V</p> at <b>(0.746, 0.469), {0.094,
0.156}</b>.</scene>.</task>
Everyactionyoutakemustincludetwolocationsintheformatof<b>(x,
y)</b>andoneclockwiserotationangleintheformatof<r>[r]</r>. The
firstlocationistheimagecoordinatewhereyouuseasuctioncuptopick
uptheobject,andthesecondlocationiswhereyouplacetheobject. The
image coordinate ranges from 0 to 1. The rotation angle indicates how
manydegreesyourotatetheobjectclockwise,anditrangesfrom-359to
359.
Output [SameasinBC]
22Table6: Qualitativeexamplesoftheauxiliarydatasets. Inthistable,theexamplesofallthedatasetstakethe
sameinputimageasshowninthefirstrow.Thedescriptionofanobjectorascene(oracledetectionresults)is
inmagenta.Theactionisinorange.
ImageInput
(currentfrontcameraview.)
Input Couldyoutellmethelocationof<p>redletterV</p>intheimage? Use
the format <b>(x, y), w, h</b>, where x and y denote the center of the
boundingboxandwandharethewidthandheight. Coordinatesstartat
ObjectLocalization
thetopleftcornerandshouldbenormalizedbetween0and1.
Output <p>redletterV</p>at<b>(0.254,0.570),0.094,0.156</b>.
Input Enumerateanddescribeeveryobjectfoundintheimage. Foreachobject,
utilize the format <b>(x, y), w, h</b>, where x, y are the bounding box
centercoordinatesandw, harethedimensions(widthandheight)ofthe
boundingbox.Thecoordinatesbeginatthetopleftcornerandarenormal-
izedbetween0and1.
ObjectDetection
Output <scene>
<p>redletterV</p>at<b>(0.254,0.570),0.094,0.156</b>.
<p>greenpaisleyletterV</p>at<b>(0.500,0.570),0.094,0.156</b>.
<p>rainbowletterV</p>at<b>(0.746,0.570),0.094,0.156</b>.
<p>greenpaisleyblock</p>at<b>(0.668,0.414),0.094,0.227</b>.
</scene>
Input Canyouexplainwhatneedstobedonetoadjustthesceneshownintheim-
agetoresemblethesecondscene?Thesecondscene<scene><p>redletter
V</p> at <b>(0.254, 0.594), 0.094, 0.156</b>. <p>green paisley letter
V</p>at<b>(0.742,0.547),0.098,0.156</b>. <p>rainbowletterV</p>
at <b>(0.746, 0.609), 0.094, 0.125</b>. <p>green paisley block</p> at
<b>(0.668,0.414),0.094,0.227</b>.</scene>consistsofobjectbounding
boxesprovidedintheformat<b>(x,y),w,h</b>. Here,xandyrepresent
thecentercoordinates,andwandharethewidthandheight. Thecoordi-
natesshouldstartfromthetopleftcornerandbenormalizedtoascaleof
ActionPrediction 0to1. Everyactionyoutakemustincludetwolocationsintheformatof
<b>(x,y)</b>andoneclockwiserotationangleintheformatof<r>[r]</r>.
Thefirstlocationistheimagecoordinatewhereyouuseasuctioncupto
pickuptheobject,andthesecondlocationiswhereyouplacetheobject.
Theimagecoordinaterangesfrom0to1.Therotationangleindicateshow
manydegreesyourotatetheobjectclockwise,anditrangesfrom-359to
359.
Output Pickupthe<p>greenpaisleyletterV</p>at<b>(0.465,0.617)</b>,rotate
<r>[0]</r>degrees,anddropitat<b>(0.711,0.617)</b>.
Input Animagedepictsascenecontainingmultipleobjects.Nowyoupickupthe
<p>greenpaisleyletterV</p>at<b>(0.465,0.617)</b>,rotate<r>[0]</r>
degrees, and drop it at <b>(0.711, 0.617)</b>, what will the scene look
like? Writethelistofobjectboundingboxes. Theboundingboxesshould
beformattedas<b>(x,y),w,h</b>,wherexandydenotethecentercoor-
dinates,andwandharethewidthandheight. Thecoordinatesstartfrom
thetopleftcornerandarenormalizedtoascaleof0to1.
FuturePrediction
Output <scene>
<p>redletterV</p>at<b>(0.254,0.594),0.094,0.156</b>.
<p>greenpaisleyletterV</p>at<b>(0.742,0.547),0.098,0.156</b>.
<p>rainbowletterV</p>at<b>(0.746,0.609),0.094,0.125</b>.
<p>greenpaisleyblock</p>at<b>(0.668,0.414),0.094,0.227</b>.
</scene>
23Table 7: (Continued) Qualitative examples of the auxiliary datasets. In this table, the examples of all the
datasetstakethesameinputimageasshowninthefirstrow. Thedescriptionofanobjectorascene(oracle
detectionresults)isinmagenta.Theexamplarisinteal.
ImageInput
(currentfrontcameraview.)
Input Canyouexplaintherelativespatialpositionsof<p>redletterV</p>com-
paredto<p>greenpaisleyletterV</p>inthisimage? Usetermssuchas
left,right,above,below,etc. Also,determinethe2Dcenterdistanceand
the Euclidean center distance between them. Your output should match
thisformat: <p>greenpaisleyblock</p>isleftandtopfrom<p>rainbow
letterV</p>with2dcenterdistance(x,y)of<d>(-0.078,-0.156)</d>and
SpatialRelationship
euclideancenterdistanceof<e>0.175</e>.Thecoordinatesareimageco-
ordinatesnormalizedtoascaleof0to1startingfromthetopleftcorner.
Output <p>redletterV</p>isleftandbottomfrom<p>greenpaisleyletterV</p>
with2dcenterdistance(x,y)of<d>(-0.246,0.000)</d>andeuclideancen-
terdistanceof<e>0.246</e>.
Input At the initial timestamp, the image shows a scene, and the second
image described as <scene><p>red letter V</p> at <b>(0.254, 0.594),
0.094, 0.156</b>. <p>green paisley letter V</p> at <b>(0.742, 0.547),
0.098,0.156</b>. <p>rainbowletterV</p>at<b>(0.746,0.609),0.094,
0.125</b>. <p>green paisley block</p> at <b>(0.668, 0.414), 0.094,
0.227</b>.</scene> depicts the next timestamp. Can you describe the
change in the relative position of <p>red letter V</p> compared to
<p>greenpaisleyletterV</p>betweenthesetwotimestamps?Userelative
distancetermssuchasgettingcloserormovingfurtheraway, etc. Also,
findthechangeinthe2DcenterdistanceandtheEuclideancenterdistance
TemporalRelationship betweenthetwoimages.Youroutputmustfollowthisformat:<p>rainbow
letterV</p>movesfarawayfrom<p>greenpaisleyblock</p>. 2dcen-
terdistance(x,y)of<p>greenpaisleyblock</p>from<p>rainbowletter
V</p>changesby<d>(0.000,-0.039)</d>andEuclideancenterdistance
betweenthem<e>0.036</e>. Thecoordinatesareimagecoordinatesnor-
malizedtoascaleof0to1startingfromthetopleftcorner.
Output <p>greenpaisleyletterV</p>movesfarawayfrom<p>redletterV</p>.
2dcenterdistance(x,y)of<p>redletterV</p>from<p>greenpaisleyletter
V</p>changesby<d>(-0.242,0.047)</d>andEuclideancenterdistance
betweenthem<e>0.244</e>.
inBC, but it omits the prompt for the output format, and the action is represented by the mapped
specialtokens. Additionally, wehavepreparedaversionsimilartoD-inBC,namedD-RT-2Style,
whichtakesadvantageofobjectdetectiononreferenceimages.
A.5 Real-worldrobotexperiments
Inthissection,weintroducetheimplementationdetailsofourreal-worldrobotexperiments.
The object detector for all real-world experiments is an OWLv2 [67] (ViT-base, patch size 16)
runninginone-shotdetectionmode.Theobjectdetectortakesapromptimagefromthesamedomain
anddetectstheobjectsintheobservation. Thelistofplastictoysweusedas{object}(anddetected)
is: {duck,croissant,cupcake,blueberries,carrot,banana,grapes,donut,tomato,corn,pepper}
In the GPT-4o baseline, the model is fed the same prompt as in RT-2, with an additional prompt
describing the robot arm: “You are operating a robot arm at a table to complete a task specified
between<task></task>. Giventhecurrentimage,youneedtogenerateaseriesofactionstocontrol
therobotarmtoaccomplishthetask.”. Themodel’sanswerisparsedtoextractthepickandplace
points.
Tab.8showsexamplesofthereal-worlddatasetswecollected.
24Table8: Qualitativeexamplesofourreal-worlddatasetsxArm-DetandxArm-Action. Thedescriptionofan
objectintheimage(one-shotdetectionresults)isinmagenta.Thetaskdescriptionoftheepisodeisinblue.
ImageInput
Input Couldyoutellmethelocationof<p>blueberry</p>intheimage?
Usetheformat<b>(x,y),w,h</b>,wherexandydenotethecenterofthe
xArm-Det boundingboxandwandharethewidthandheight. Coordinatesstartat
Localization thetopleftcornerandshouldbenormalizedbetween0and1.
Output <p>blueberry</p>at<b>(0.478,0.725),0.058,0.125</b>.
Input Describealltheobjectsseenintheimage,andlistthemusingtheformat
<b>(x,y),w,h</b>. Thexandyvaluesarethecoordinatesforthecenter
of the bounding box, while w and h represent its width and height. The
coordinatesshouldbenormalizedfromthetopleftcorner,withinarange
of0to1.
Output <scene>
<p>cupcake</p>at<b>(0.440,0.128),0.063,0.123</b>.
xArm-Det
<p>corn</p>at<b>(0.237,0.826),0.061,0.246</b>.
Detection
<p>carrot</p>at<b>(0.608,0.135),0.152,0.113</b>.
<p>donut</p>at<b>(0.396,0.355),0.079,0.145</b>.
<p>grape</p>at<b>(0.318,0.534),0.077,0.143</b>.
<p>duck</p>at<b>(0.795,0.280),0.066,0.128</b>.
<p>green_pepper</p>at<b>(0.688,0.411),0.062,0.187</b>.
<p>blueberry</p>at<b>(0.478,0.725),0.058,0.125</b>.
<p>tomato</p>at<b>(0.543,0.421),0.078,0.154</b>.
</scene>
Input <task>Rotatethe<p>cupcake</p>at<b>(0.440,0.128),0.063,0.123</b>
by-50degrees.</task>
Everyactionyoutakemustincludetwolocationsintheformatof<b>(x,
y)</b>andoneclockwiserotationangleintheformatof<r>[r]</r>. The
firstlocationistheimagecoordinatewhereyouuseasuctioncuptopick
xArm-Action uptheobject,andthesecondlocationiswhereyouplacetheobject. The
Rotation image coordinate ranges from 0 to 1. The rotation angle indicates how
manydegreesyourotatetheobjectclockwise,anditrangesfrom-359to
359.
Output Pickupthe<p>cupcake</p>at<b>(0.440,0.128)</b>,rotate<r>[-50]</r>
degrees,anddropitat<b>(0.440,0.128)</b>.
Input <task>Move the <p>cupcake</p> at <b>(0.440, 0.128), 0.063,
0.123</b> on the top of the <p>corn</p> at <b>(0.237, 0.826), 0.061,
0.246</b>.</task>
Everyactionyoutakemustincludetwolocationsintheformatof<b>(x,
y)</b>andoneclockwiserotationangleintheformatof<r>[r]</r>. The
firstlocationistheimagecoordinatewhereyouuseasuctioncuptopick
xArm-Action
uptheobject,andthesecondlocationiswhereyouplacetheobject. The
Putontop
image coordinate ranges from 0 to 1. The rotation angle indicates how
manydegreesyourotatetheobjectclockwise,anditrangesfrom-359to
359.
Output Step 1: Pick up the <p>cupcake</p> at <b>(0.440, 0.128)</b>, rotate
<r>[0]</r>degrees,anddropitat<b>(0.237,0.826)</b>.
25B ExpandedExperiments
In this section, we include more experiment results. All the results are collected in Tab. 12 and
Tab.13(VIMA-0.8k),Tab.14andTab.15(VIMA-8k)andTab.16andTab.17(VIMA-80k).
ClarificationonL4results WeadoptedthetestingprotocolfromVIMA-Bench[43],conducting
evaluations across four levels of difficulty, from L1 to L4. However, during our analysis, we dis-
coveredaninconsistencyinthetrainingset: therotationinformationfortherobotendeffectorwas
recorded as zero for all sweeping tasks where the end effector is a spatula. Given the importance
ofthespatulaorientationinasweepingtaskandthefactthatsweepingtasksconstitute25%ofthe
evaluationsattheL4difficultylevel,weconcludedthatourabilitytoaccuratelyevaluateourmethod
atL4wascompromised. ConsideringthattheoriginalVIMAmodelreleasedbytheauthorsappears
toincludethisrotationinformation,wehavechosennottoreporttheresultsforL4inourstudy.
Ablationonactionhistoryandmulti-stepplanning AsdescribedinSec.A.1.1,weenableaction
historyandmulti-stepplanningwhengeneratinginBCandD-inBC.Tab.9showsthatthesedesigns
arehelpful.
Table9: Ablationonactionhistoryandmulti-stepplanning. His. standsforenablingactionhistoryandPlan
meansenablingmulti-stepplanning.AllmodelsaretrainedonVIMA-8kfor2epochs.
Method His. Plan L1(%) L2(%) L3(%)
inBC ✓ ✓ 57.3 46.2 42.9
inBC ✓ ✗ 43.5 36.9 36.7
inBC ✗ ✓ 45.8 39.6 35.8
D-inBC+OD ✓ ✓ 63.5 51.5 50.0
D-inBC+OD ✓ ✗ 58.5 41.5 47.1
D-inBC+OD ✗ ✓ 53.5 39.2 37.5
Ablation on multiple image inputs This ablation studies multiple image inputs within a single
conversation. Eachimageisprocessedbythevisionencoderandtheprojectionlayertogeneratea
seriesoftokens. Theseimagetokensarethenintegratedintothetextualconversationatthepoints
wherethecorrespondingimagesarereferenced.
However,becauseLLaVAistrainedwithoneimageperconversationinsteadofaninterleavingstyle.
Theperformancedropssignificantlywhenapplyingmultipleimageinputs,listedinTab.10.
Table10:Ablationonmultipleimageinputs(Mul.).AllmodelsaretrainedonVIMA-8kfor2epochs.
Method Mul. L1(%) L2(%) L3(%)
inBC ✗ 57.3 46.2 42.9
inBC ✓ 44.6 27.7 34.2
D-inBC+OD ✗ 63.5 51.5 50.0
D-inBC+OD ✓ 27.7 24.6 34.2
D-inBC+Aux(C) ✗ 64.6 58.8 49.6
D-inBC+Aux(C) ✓ 31.2 28.5 27.1
Ablation on object detector We study three types of object detectors in this paper: the VLM
model itself, an external object detector separately trained on the training set (the methods with
a suffix OD), and an oracle from the groundtruth dataset (Oracle). Fig. 13 shows that a reliable
objectdetectorishighlybeneficial,enhancingtheaccuracyofimage-basedinputsandconsequently
improvingmodelperformance.
2680
80 80
60
60 60
40
40 40 D-RT-2-Style + OD
D-RT-2-Style + Oracle
20 20 20 D-inBC + Aux (D)
D-inBC + Aux (D) + OD
0 0 0 D-inBC + Aux (D) + Oracle
103 104 105 103 104 105 103 104 105
Num of expert episodes Num of expert episodes Num of expert episodes
Figure13:Impactofdifferentobjectdetectors.
Few-shotexperimentsusingLLM/VLM Weexpandedourevaluationtoincludethefew-shot
performance of existing Large Language Models (LLMs) and Vision Language Models (VLMs).
ConsistentwiththeinferencesettingsdescribedinSec.A.3,weprovidedthreeadditionalexamples
fromeitherinBC orD-inBC foreachtestscenario. Theresultsoftheseevaluationsaredetailedin
Tab.11,whereweexaminethreevariables:
Vision: Whenthisoptionisenabled, themodelwillhaveaccesstothecurrentimageobservation,
allowingittointegratevisualdataintoitsresponse. Ifdisabled,themodeloperatessolelyasatext-
basedsystem.
Oracle:ThisvariableischeckedwhentheexamplesaredrawnfromtheD-inBCdataset.Itindicates
that the examples provided for few-shot learning include descriptive textual information about the
referenceimages,potentiallyofferingrichercontext.
Same-task: Whenthisischecked,theexamplesusedforpromptingarefromthesametasktypeas
thoseintheevaluationenvironment.
Table11:Three-shotperformanceofLLMs/VLMs
Output Vision Oracle Same-task L1(%) L2(%) L3(%) L4(%)
GPT-4o ✓ 1.4 1.4 0.0 N/A
GPT-4o ✗ 0.4 0.4 0.4 0.0
GPT-4o ✓ ✓ 1.4 1.8 0.5 N/A
GPT-4o ✓ ✗ 1.2 0.4 0.4 1.2
GPT-4o ✓ ✓ 5.9 4.5 4.0 N/A
GPT-4o ✓ ✗ 6.9 5.8 4.2 1.2
Llama370B(4-bit) ✓ 0.0 0.0 0.0 N/A
Llama370B(4-bit) ✗ 0.8 1.2 0.4 1.2
Llama370B(4-bit) ✓ ✓ 0.0 1.4 0.5 N/A
Llama370B(4-bit) ✓ ✗ 0.0 0.4 0.4 0.0
LlaVA-1.5-7B ✓ ✓ 0.9 1.4 0.5 N/A
LlaVA-1.5-7B ✓ ✗ 0.0 0.0 1.0 1.2
LlaVA-1.6-34B ✓ ✓ 1.4 1.4 0.5 N/A
LlaVA-1.6-34B ✓ ✗ 1.2 0.8 2.1 0.0
From the observed low performance, we infer that the data utilized for robot control significantly
deviates from the conversation contexts in which these models were originally trained. This indi-
cates a pressing need to further finetune the models specifically for robot control tasks to achieve
adequate performance. Additionally, our findings hint that visual input considerably enhances the
functionalityofGPT-4o.
27
)%(
etaR
sseccuS
1L
)%(
etaR
sseccuS
2L
)%(
etaR
sseccuS
3LC Limitations
Whileourmodelhasdemonstratedsignificantachievementsandholdsconsiderablepotential, itis
importanttoacknowledgethatthereremainsspaceforfurtheroptimization.
First,someconceptsinimagesstillcannotbeeasilyandpreciselydescribedbylanguage,whilein
manycasesthesefeaturesarecriticalforrobotcontrol.
Second, we still rely on the object detector trained on a limited number of classes which limit the
generalizationabilityofthismethod.
Third,theinformationextractedfromthedatasetcanbenoisy,whichmayleadtomodelconfusion.
In Tab. 1, there are discrepancies such as the location of an object in a reference image differing
fromitsactuallocationinthecurrentimageobservation. Atthesametime,inVIMAdataset,when
referringtotheshapeofanobjectwithoutacolor,theobjectinthereferenceimagewillappearin
gray,whichisatotallydifferentcolorthantheobjectwiththesameshapeinthecurrentobservations.
Finally, our method relies on 2D to 2D mapping to convert image coordinates into actual actions,
whichmayfacechallengeswhenthecomplex3Dmovementoftherobotisrequired.
Webelieveenhancementsinthementionedaspectscouldfurtherimproveperformanceandbroader
applicabilityincomplex,real-worldapplications.
28Table12: ResultsonVIMA-0.8kdataset. Ep: numberofepoch; U:normalizednumberofmodeliterations.
ThefollowingsizesofdatasetsarerelativetothesizeofinBC.Loc.: relativesizeofthelocalizationdataset;
Det.:relativesizeofthedetectiondataset;Act.:relativesizeoftheactionpredictiondataset;Fut.:relativesize
ofthefuturepredictiondataset;Spa.:relativesizeofthespatialrelationshipdataset;Temp.:relativesizeofthe
temporalrelationshipdataset.
Method Ep U Loc. Det. Act. Fut. Spa. Temp. L1(%) L2(%) L3(%)
RT-2Style 1 1 0.0 0.4 0.4
RT-2Style 2 2 1.2 0.4 0.8
RT-2Style 4 4 1.5 1.2 2.5
RT-2Style 6 6 2.3 1.5 0.8
RT-2Style 8 8 2.3 3.5 2.1
inBC 1 1 23.1 16.5 16.7
inBC 2 2 30.0 23.8 19.6
inBC 4 4 22.3 21.2 15.4
inBC 6 6 21.9 14.6 17.5
inBC 8 8 20.4 17.7 18.3
inBC 10 10 20.4 18.1 14.6
inBC+Aux 1 1.2 0.1* 0.1* 13.8 13.1 9.2
inBC+Aux 1 3 1* 1* 28.8 26.5 24.2
inBC+Aux 2 2.4 0.1* 0.1* 29.6 21.9 23.3
inBC+Aux 2 2.4 0.1 0.1 23.5 20.4 17.5
inBC+Aux 2 4 0.5 0.5 26.9 23.8 24.6
inBC+Aux 2 4 0.2 0.2 0.2 0.2 35.0 24.2 26.7
inBC+Aux 2 4 0.2 0.2 0.2 0.2 0.2 33.5 26.2 21.7
inBC+Aux 2 6 1* 1* 31.9 26.5 30.4
inBC+Aux 2 6 1 1 30.4 27.3 27.1
inBC+Aux 2 7 0.5 0.5 0.5 0.5 0.5 36.2 29.6 29.2
inBC+Aux 2 8 0.5* 0.5* 0.5 0.5 0.5* 0.5 33.5 29.2 33.3
inBC+Aux 2 8 0.5 0.5 0.5 0.5 0.5 0.5 31.9 29.6 26.7
inBC+Aux 2 10 1 1 1 1 37.7 30.8 28.7
inBC+Aux 2 12 1 1 1 1 1 38.8 34.6 29.6
inBC+Aux 2 14 1* 1* 1 1 1* 1 37.7 29.6 27.9
inBC+Aux 2 14 1 1 1 1 1 1 40.0 35.8 31.2
D-inBC+Aux 1 1.2 0.1* 0.1* 18.5 17.7 13.8
D-inBC+Aux 1 3 1* 1* 28.8 25.4 23.3
D-inBC+Aux 2 2.4 0.1* 0.1* 23.5 16.2 22.5
D-inBC+Aux 2 2.4 0.1 0.1 21.5 21.2 23.3
D-inBC+Aux 2 4 0.5 0.5 28.1 25.8 21.7
D-inBC+Aux 2 4 0.2 0.2 0.2 0.2 32.3 29.6 22.1
D-inBC+Aux 2 4 0.2 0.2 0.2 0.2 0.2 30.0 26.5 25.4
D-inBC+Aux 2 6 1* 1* 33.8 30.8 29.2
D-inBC+Aux 2 6 1 1 35.8 28.5 28.3
D-inBC+Aux 2 7 0.5 0.5 0.5 0.5 0.5 38.5 26.9 30.8
D-inBC+Aux 2 8 0.5* 0.5* 0.5 0.5 0.5* 0.5 37.3 26.5 34.6
D-inBC+Aux 2 8 0.5 0.5 0.5 0.5 0.5 0.5 34.2 29.6 30.0
D-inBC+Aux 2 10 1 1 1 1 34.6 33.1 28.3
D-inBC+Aux 2 12 1 1 1 1 1 40.0 38.5 37.5
D-inBC+Aux 2 14 1* 1* 1 1 1* 1 40.8 30.4 32.9
D-inBC+Aux 2 14 1 1 1 1 1 1 40.0 35.4 36.2
29Table13: ResultsonVIMA-0.8kdatasetwithobjectdetectororOracle. Ep: numberofepoch;U:normalized
numberofmodeliterations. ThefollowingsizesofdatasetsarerelativetothesizeofinBC.Loc.: relativesize
ofthelocalizationdataset;Det.:relativesizeofthedetectiondataset;Act.:relativesizeoftheactionprediction
dataset;Fut.:relativesizeofthefuturepredictiondataset;Spa.:relativesizeofthespatialrelationshipdataset;
Temp.:relativesizeofthetemporalrelationshipdataset.
Method Ep U Loc. Det. Act. Fut. Spa. Temp. L1(%) L2(%) L3(%)
D-RT-2Style+OD 1 1 0.0 0.0 0.0
D-RT-2Style+OD 2 2 0.4 1.5 2.5
D-RT-2Style+OD 4 4 1.9 1.5 2.5
D-RT-2Style+OD 6 6 1.9 2.3 0.8
D-RT-2Style+OD 8 8 1.9 1.2 0.0
D-inBC+OD 1 1 17.3 11.9 12.5
D-inBC+OD 2 2 17.3 14.6 15.0
D-inBC+OD 4 4 32.7 24.2 24.2
D-inBC+OD 6 6 28.1 23.1 21.7
D-inBC+OD 8 8 26.2 17.7 22.9
D-inBC+Aux+OD 2 6 1 1 36.9 25.8 30.4
D-inBC+Aux+OD 2 10 1 1 1 1 41.2 31.9 32.9
D-inBC+Aux+OD 2 12 1 1 1 1 1 44.6 33.5 37.9
D-inBC+Aux+OD 2 14 1 1 1 1 1 1 47.3 36.5 38.8
D-RT-2Style+Oracle 1 1 0.0 0.4 0.8
D-RT-2Style+Oracle 2 2 0.8 0.8 1.7
D-RT-2Style+Oracle 4 4 1.9 3.8 4.2
D-RT-2Style+Oracle 6 6 2.7 3.5 0.8
D-RT-2Style+Oracle 8 8 1.2 2.7 1.2
D-inBC+Oracle 1 1 16.9 13.5 10.4
D-inBC+Oracle 2 2 20.0 19.2 17.5
D-inBC+Oracle 4 4 37.7 26.2 29.2
D-inBC+Oracle 6 6 30.0 28.1 27.5
D-inBC+Oracle 8 8 27.7 23.1 27.9
D-inBC+Aux+Oracle 2 6 1 1 45.0 36.9 39.2
D-inBC+Aux+Oracle 2 10 1 1 1 1 43.8 38.5 37.9
D-inBC+Aux+Oracle 2 12 1 1 1 1 1 52.7 44.6 50.4
D-inBC+Aux+Oracle 2 14 1 1 1 1 1 1 49.6 46.2 47.9
30Table14:ResultsonVIMA-8kdataset.Ep:numberofepoch;U:normalizednumberofmodeliterations.The
followingsizesofdatasetsarerelativetothesizeofinBC.Loc.: relativesizeofthelocalizationdataset;Det.:
relativesizeofthedetectiondataset;Act.: relativesizeoftheactionpredictiondataset;Fut.: relativesizeof
thefuturepredictiondataset;Spa.: relativesizeofthespatialrelationshipdataset;Temp.: relativesizeofthe
temporalrelationshipdataset.
Method Ep U Loc. Det. Act. Fut. Spa. Temp. L1(%) L2(%) L3(%)
RT-2Style 1 1 3.1 3.5 0.8
RT-2Style 2 2 3.8 3.1 1.7
RT-2Style 4 4 8.1 6.5 5.8
RT-2Style 6 6 6.5 5.4 4.2
RT-2Style 8 8 6.2 6.5 4.2
inBC 1 1 44.6 34.2 36.2
inBC 2 2 57.3 46.2 42.9
inBC 4 4 58.5 48.5 47.1
inBC 6 6 54.6 45.4 40.4
inBC 8 8 52.3 47.3 47.1
inBC 10 10 53.1 47.7 47.9
inBC+Aux 2 2.4 0.1* 0.1* 59.6 50.0 45.4
inBC+Aux 2 2.4 0.1 0.1 57.3 52.3 45.4
inBC+Aux 2 4 0.5 0.5 60.4 56.2 52.1
inBC+Aux 2 4 0.2 0.2 0.2 0.2 59.6 59.2 51.2
inBC+Aux 2 6 1* 1* 59.2 52.3 48.3
inBC+Aux 2 6 1 1 57.7 56.2 53.8
inBC+Aux 2 7 0.5 0.5 0.5 0.5 0.5 58.1 55.0 47.9
inBC+Aux 2 10 1 1 1 1 60.4 55.8 54.2
inBC+Aux 2 12 1 1 1 1 1 60.8 55.4 50.0
inBC+Aux 2 14 1* 1* 1 1 1* 1 63.1 58.8 53.8
inBC+Aux 2 14 1 1 1 1 1 1 59.2 58.8 52.1
D-inBC+Aux 2 2.4 0.1* 0.1* 53.5 48.8 45.4
D-inBC+Aux 2 2.4 0.1 0.1 58.5 55.8 50.8
D-inBC+Aux 2 4 0.5 0.5 58.8 50.8 54.6
D-inBC+Aux 2 4 0.2 0.2 0.2 0.2 58.1 52.3 51.2
D-inBC+Aux 2 6 1* 1* 63.1 57.7 55.4
D-inBC+Aux 2 6 1 1 60.0 53.5 50.0
D-inBC+Aux 2 7 0.5 0.5 0.5 0.5 0.5 62.3 55.8 48.8
D-inBC+Aux 2 10 1 1 1 1 63.1 58.1 49.6
D-inBC+Aux 2 12 1 1 1 1 1 64.6 58.8 49.6
D-inBC+Aux 2 14 1* 1* 1 1 1* 1 62.3 54.6 51.2
D-inBC+Aux 2 14 1 1 1 1 1 1 62.3 57.7 52.1
D-inBC+Aux 4 4.8 0.1 0.1 58.5 47.7 49.6
D-inBC+Aux 4 12 1 1 61.2 57.7 49.6
D-inBC+Aux 4 20 1 1 1 1 62.3 56.5 49.6
D-inBC+Aux 4 24 1 1 1 1 1 63.5 56.2 52.5
31Table15: ResultsonVIMA-8kdatasetwithobjectdetectororOracle. Ep: numberofepoch;U:normalized
numberofmodeliterations. ThefollowingsizesofdatasetsarerelativetothesizeofinBC.Loc.: relativesize
ofthelocalizationdataset;Det.:relativesizeofthedetectiondataset;Act.:relativesizeoftheactionprediction
dataset;Fut.:relativesizeofthefuturepredictiondataset;Spa.:relativesizeofthespatialrelationshipdataset;
Temp.:relativesizeofthetemporalrelationshipdataset.
Method Ep U Loc. Det. Act. Fut. Spa. Temp. L1(%) L2(%) L3(%)
D-RT-2Style+OD 1 1 1.5 3.1 1.2
D-RT-2Style+OD 2 2 10.4 8.8 7.1
D-RT-2Style+OD 4 4 16.9 14.2 15.8
D-RT-2Style+OD 6 6 14.2 11.5 12.9
D-RT-2Style+OD 8 8 15.8 10.4 6.7
D-inBC+OD 1 1 55.4 45.8 45.0
D-inBC+OD 2 2 63.5 51.5 50.0
D-inBC+OD 4 4 63.8 46.9 55.8
D-inBC+Aux+OD 2 6 1 1 74.2 58.5 56.7
D-inBC+Aux+OD 2 10 1 1 1 1 75.4 66.2 57.9
D-inBC+Aux+OD 2 12 1 1 1 1 1 76.9 63.8 57.9
D-inBC+Aux+OD 2 12 1 1 1 1 1 79.2 64.2 58.3
D-inBC+Aux+OD 2 14 1* 1* 1 1 1* 1 72.7 55.4 57.1
D-inBC+Aux+OD 2 14 1 1 1 1 1 1 79.2 65.4 60.4
D-inBC+Aux+OD 2 14 1 1 1 1 1 1 80.8 66.2 60.0
D-inBC+Aux+OD 4 4.8 0.1 0.1 65.4 50.0 55.0
D-inBC+Aux+OD 4 12 1 1 76.9 63.1 55.8
D-RT-2Style+Oracle 1 1 1.2 1.9 1.2
D-RT-2Style+Oracle 2 2 9.6 7.7 7.5
D-RT-2Style+Oracle 4 4 21.9 17.7 15.0
D-RT-2Style+Oracle 6 6 13.5 16.2 11.2
D-RT-2Style+Oracle 8 8 16.2 11.9 11.7
D-inBC+Oracle 1 1 60.4 59.2 54.6
D-inBC+Oracle 2 2 73.5 62.7 63.3
D-inBC+Oracle 4 4 70.8 61.2 66.2
D-inBC+Oracle 6 6 71.9 61.5 59.6
D-inBC+Oracle 8 8 70.4 63.8 60.0
D-inBC+Aux+Oracle 2 6 1 1 78.1 73.8 65.4
D-inBC+Aux+Oracle 2 10 1 1 1 1 78.1 73.8 70.0
D-inBC+Aux+Oracle 2 12 1 1 1 1 1 78.5 78.1 71.2
D-inBC+Aux+Oracle 2 14 1* 1* 1 1 1* 1 76.2 65.8 66.2
D-inBC+Aux+Oracle 2 14 1 1 1 1 1 1 82.3 78.1 69.6
D-inBC+Aux+Oracle 4 4.8 0.1 0.1 71.9 64.6 61.7
D-inBC+Aux+Oracle 4 12 1 1 78.8 75.4 67.1
32Table16: ResultsonVIMA-80kdataset. Ep: numberofepoch; U:normalizednumberofmodeliterations.
ThefollowingsizesofdatasetsarerelativetothesizeofinBC.Loc.: relativesizeofthelocalizationdataset;
Det.:relativesizeofthedetectiondataset;Act.:relativesizeoftheactionpredictiondataset;Fut.:relativesize
ofthefuturepredictiondataset;Spa.:relativesizeofthespatialrelationshipdataset;Temp.:relativesizeofthe
temporalrelationshipdataset.
Method Ep U Loc. Det. Act. Fut. Spa. Temp. L1(%) L2(%) L3(%)
RT-2Style 1 1 53.1 46.9 42.1
RT-2Style 2 2 56.9 53.1 46.2
RT-2Style 4 4 49.6 43.8 39.6
RT-2Style 6 6 51.2 45.4 38.3
RT-2Style 8 8 46.2 43.5 35.8
inBC 2 2 66.2 62.3 56.2
inBC 4 4 67.3 58.8 54.2
inBC 6 6 65.4 60.0 53.8
inBC 8 8 65.8 56.5 54.2
inBC+Aux 1 3 1 1 62.7 60.4 50.8
inBC+Aux 1 5 1 1 1 1 66.9 63.8 56.2
inBC+Aux 1 6 1 1 1 1 1 66.2 59.6 55.8
inBC+Aux 1 7 1 1 1 1 1 1 68.1 60.8 54.6
inBC+Aux 2 6 1 1 65.8 64.6 56.7
inBC+Aux 2 10 1 1 1 1 69.2 66.9 55.8
inBC+Aux 2 12 1 1 1 1 1 65.4 60.0 50.0
inBC+Aux 2 14 1 1 1 1 1 1 68.1 65.0 51.7
inBC+Aux 4 12 1 1 65.8 62.7 49.6
inBC+Aux 4 20 1 1 1 1 65.8 63.1 52.5
inBC+Aux 4 24 1 1 1 1 1 63.8 59.6 48.8
inBC+Aux 4 28 1 1 1 1 1 1 68.1 65.0 52.9
inBC+Aux 6 18 1 1 67.7 61.5 51.2
inBC+Aux 6 30 1 1 1 1 67.3 60.4 49.2
inBC+Aux 6 36 1 1 1 1 1 61.9 60.4 47.1
inBC+Aux 6 42 1 1 1 1 1 1 65.4 63.8 52.9
inBC+Aux 8 24 1 1 67.3 62.7 52.1
inBC+Aux 8 40 1 1 1 1 66.9 61.2 51.7
inBC+Aux 8 48 1 1 1 1 1 66.2 58.1 50.0
inBC+Aux 8 56 1 1 1 1 1 1 66.9 63.8 50.8
D-inBC+Aux 1 3 1 1 66.2 63.5 57.5
D-inBC+Aux 1 5 1 1 1 1 66.2 60.8 57.1
D-inBC+Aux 1 6 1 1 1 1 1 65.4 60.4 52.5
D-inBC+Aux 1 7 1 1 1 1 1 1 63.8 61.9 54.2
D-inBC+Aux 2 6 1 1 68.5 61.9 54.6
D-inBC+Aux 2 10 1 1 1 1 70.0 65.4 56.2
D-inBC+Aux 2 12 1 1 1 1 1 69.6 62.7 56.2
D-inBC+Aux 2 14 1 1 1 1 1 1 70.4 63.5 56.7
D-inBC+Aux 4 12 1 1 67.7 65.4 60.0
D-inBC+Aux 4 20 1 1 1 1 68.1 66.5 57.5
D-inBC+Aux 4 24 1 1 1 1 1 66.5 61.2 52.9
D-inBC+Aux 4 28 1 1 1 1 1 1 69.6 58.5 52.9
D-inBC+Aux 6 18 1 1 69.2 63.8 57.9
D-inBC+Aux 6 30 1 1 1 1 70.0 68.1 58.8
D-inBC+Aux 6 36 1 1 1 1 1 66.5 62.3 47.5
D-inBC+Aux 6 42 1 1 1 1 1 1 70.0 61.9 54.2
D-inBC+Aux 8 24 1 1 67.7 61.5 50.8
D-inBC+Aux 8 40 1 1 1 1 69.6 66.5 57.1
D-inBC+Aux 8 48 1 1 1 1 1 71.2 69.2 51.7
D-inBC+Aux 8 56 1 1 1 1 1 1 68.1 67.7 57.9
33Table17: ResultsonVIMA-80kdatasetwithobjectdetectororOracle. Ep: numberofepoch;U:normalized
numberofmodeliterations. ThefollowingsizesofdatasetsarerelativetothesizeofinBC.Loc.: relativesize
ofthelocalizationdataset;Det.:relativesizeofthedetectiondataset;Act.:relativesizeoftheactionprediction
dataset;Fut.:relativesizeofthefuturepredictiondataset;Spa.:relativesizeofthespatialrelationshipdataset;
Temp.:relativesizeofthetemporalrelationshipdataset.
Method Ep U Loc. Det. Act. Fut. Spa. Temp. L1(%) L2(%) L3(%)
D-RT-2Style+OD 1 1 49.2 43.1 36.2
D-RT-2Style+OD 2 2 52.3 50.0 45.8
D-RT-2Style+OD 4 4 52.3 46.5 37.9
D-RT-2Style+OD 6 6 47.3 47.3 36.2
D-RT-2Style+OD 8 8 44.2 42.3 36.7
D-inBC+OD 1 1 81.9 75.0 67.5
D-inBC+OD 2 2 78.8 75.0 69.2
D-inBC+OD 4 4 81.2 75.4 68.3
D-inBC+OD 6 6 81.9 75.8 67.1
D-inBC+OD 8 8 83.8 73.8 69.2
D-inBC+Aux+OD 1 3 1 1 85.0 79.2 65.8
D-inBC+Aux+OD 1 5 1 1 1 1 85.0 77.7 68.3
D-inBC+Aux+OD 1 6 1 1 1 1 1 79.6 73.5 64.2
D-inBC+Aux+OD 1 7 1 1 1 1 1 1 84.2 74.6 66.2
D-inBC+Aux+OD 2 6 1 1 81.5 75.8 64.2
D-inBC+Aux+OD 2 10 1 1 1 1 86.2 83.1 69.6
D-inBC+Aux+OD 2 12 1 1 1 1 1 80.0 73.1 62.5
D-inBC+Aux+OD 2 14 1 1 1 1 1 1 85.8 77.3 66.7
D-inBC+Aux+OD 4 12 1 1 78.5 73.8 68.8
D-inBC+Aux+OD 4 20 1 1 1 1 82.3 77.3 71.7
D-inBC+Aux+OD 4 24 1 1 1 1 1 82.3 71.9 63.7
D-inBC+Aux+OD 4 28 1 1 1 1 1 1 83.5 71.5 62.5
D-inBC+Aux+OD 6 18 1 1 80.0 75.0 67.1
D-inBC+Aux+OD 6 30 1 1 1 1 86.2 81.5 69.6
D-inBC+Aux+OD 6 36 1 1 1 1 1 79.6 74.6 64.6
D-inBC+Aux+OD 6 42 1 1 1 1 1 1 85.0 77.3 62.1
D-inBC+Aux+OD 8 24 1 1 78.5 73.1 68.3
D-inBC+Aux+OD 8 40 1 1 1 1 86.5 82.7 70.8
D-inBC+Aux+OD 8 48 1 1 1 1 1 82.3 78.1 65.4
D-inBC+Aux+OD 8 56 1 1 1 1 1 1 82.7 76.5 68.3
D-RT-2Style+Oracle 1 1 71.2 66.5 61.3
D-RT-2Style+Oracle 2 2 74.2 70.4 69.2
D-RT-2Style+Oracle 4 4 73.1 70.4 63.3
D-RT-2Style+Oracle 6 6 67.7 64.2 60.4
D-RT-2Style+Oracle 8 8 69.2 61.5 55.4
D-inBC+Oracle 1 1 84.6 84.2 80.8
D-inBC+Oracle 2 2 84.2 82.3 77.5
D-inBC+Oracle 4 4 85.4 83.8 80.0
D-inBC+Oracle 6 6 86.9 85.8 80.8
D-inBC+Oracle 8 8 87.3 85.4 82.1
D-inBC+Aux+Oracle 1 3 1 1 88.8 85.0 79.2
D-inBC+Aux+Oracle 1 5 1 1 1 1 86.2 84.6 79.2
D-inBC+Aux+Oracle 1 6 1 1 1 1 1 82.3 77.3 69.6
D-inBC+Aux+Oracle 1 7 1 1 1 1 1 1 86.2 83.5 77.5
D-inBC+Aux+Oracle 2 6 1 1 88.5 84.2 77.5
D-inBC+Aux+Oracle 2 10 1 1 1 1 91.5 88.1 80.4
D-inBC+Aux+Oracle 2 12 1 1 1 1 1 81.2 80.4 67.5
D-inBC+Aux+Oracle 2 14 1 1 1 1 1 1 88.5 84.6 78.3
D-inBC+Aux+Oracle 4 12 1 1 85.0 87.3 78.8
D-inBC+Aux+Oracle 4 20 1 1 1 1 88.8 84.6 79.6
D-inBC+Aux+Oracle 4 24 1 1 1 1 1 81.5 77.3 65.8
D-inBC+Aux+Oracle 4 28 1 1 1 1 1 1 84.2 80.0 71.7
D-inBC+Aux+Oracle 6 18 1 1 84.6 85.4 76.7
D-inBC+Aux+Oracle 6 30 1 1 1 1 87.7 87.3 78.3
D-inBC+Aux+Oracle 6 36 1 1 1 1 1 81.2 80.8 70.8
D-inBC+Aux+Oracle 6 42 1 1 1 1 1 1 88.8 81.2 70.0
D-inBC+Aux+Oracle 8 24 1 1 86.9 81.5 75.8
D-inBC+Aux+Oracle 8 40 1 1 1 1 90.0 88.1 79.2
D-inBC+Aux+Oracle 8 48 1 1 1 1 1 82.3 83.1 70.4
D-inBC+Aux+Oracle 8 56 1 1 1 1 1 1 83.8 88.1 78.8
34Table18:Promptcandidatesforactiongenerationduringinference.
‘Couldyouwritedownwhatneedstobedonetocompletethetaskonthisscene?’
‘Listouttheactionsneededtoaccomplishthetaskinthisscene.’
‘Whatactionsarenecessarytoperformthetaskonthisscene?’
‘Canyoudescribewhatneedstobedoneonthisscenetocompletethetask?’
‘Whatstepsarerequiredtoperformthetaskshowninthisscene?’
‘Listtheactionsneededtoperformthetaskgivenbelow.’
‘Onthefollowingscene,couldyoulistwhatactionsarerequiredtoperformthetask?’
‘Describewhatactionsareneededonthisscenetocompletethetask.’
‘Whatdoyouneedtodoonthisscenetoaccomplishthetask?’
‘Listtheactionsrequiredtoperformthetaskgivenonthisscene.’
‘Couldyoupleasedescribethestepsneededtoperformthetaskonthisscene?’
‘Writedowntheactionsrequiredtoperformthetaskonthisscene.’
‘Pleasewritedowntheactionsrequiredtoperformthetaskshownbelow.’
‘Canyouexplainwhatneedstobedonetoperformthetaskinthisscene?’
‘Describetheactionsrequiredtocompletethetaskonthisscene.’
35Table19: Promptcandidatesforobjectlocalization. {object}willbereplacedwiththeactualobjectnamein
thedataset.
‘Where is {object} located in the image? Please use the format <b>(x, y),{w, h}</b>
wherexandyrepresentthecentercoordinatesoftheboundingbox,andwandharethe
widthandheight. Thecoordinatesstartfromthetopleftcornerandarenormalizedtoa
scaleof0to1.’
‘Canyouprovidethelocationof{object}intheimage?Formatitas<b>(x,y),{w,h}</b>,
withxandyasthecentercoordinatesoftheboundingboxandwandhasthewidthand
height.Thecoordinatesshouldbeginatthetopleftcornerandbenormalizedfrom0to1.’
‘Whatarethecoordinatesof{object}intheimage?Usetheformat<b>(x,y),{w,h}</b>,
where x and y are the center of the bounding box, and w and h represent the width and
height.Coordinatesshouldstartatthetopleftcornerandbenormalizedtoarangeof0to
1.’
‘Pleasespecifythelocationof{object}intheimage. Listitintheformat<b>(x,y),{w,
h}</b>,wherexandydenotetheboundingboxcentercoordinates,andwandharethe
widthandheight.Thecoordinatesbeginfromthetopleftcornerandshouldbenormalized
to0to1.’
‘Whatisthepositionof{object}withintheimage? Usetheformat<b>(x,y),{w,h}</b>
todescribeit,withxandyasthecentercoordinatesoftheboundingbox,andwandhas
thewidthandheight. Thecoordinatesstartatthetopleftcornerandarenormalizedtoa
scaleof0to1.’
‘Describe the location of {object} in the image using the format <b>(x, y),{w, h}</b>.
Inthisformat,xandydenotethecentercoordinatesoftheboundingbox,whilewandh
representitswidthandheight.Coordinatesshouldbenormalizedfromthetopleftcorner,
rangingfrom0to1.’
‘Canyoudetailthelocationof{object}intheimage? Formatitas<b>(x,y),{w,h}</b>,
where x and y indicate the bounding box center, and w and h represent the width and
height.Thecoordinatesshouldbenormalizedtoascaleof0to1startingfromthetopleft
corner.’
‘Provide the location of {object} in the image using the format <b>(x, y),{w, h}</b>.
Here,xandyarethecentercoordinatesoftheboundingbox,andwandharethewidth
andheight.Thecoordinatesbeginatthetopleftcornerandarenormalizedfrom0to1.’
‘Whereis{object}positionedintheimage? Usetheformat<b>(x,y),{w,h}</b>,where
xandydenotethecentercoordinatesoftheboundingbox,andwandharethewidthand
height.Thecoordinatesshouldbenormalizedtoarangeof0to1startingfromthetopleft
corner.’
‘Specifythelocationof{object}intheimageintheformat<b>(x,y),{w,h}</b>. Inthis
format,xandyrepresenttheboundingboxcenter,andwandharethewidthandheight.
Thecoordinatesshouldstartfromthetopleftcornerandbenormalizedbetween0and1.’
‘What is the exact position of {object} in the image? Format the coordinates as <b>(x,
y),{w,h}</b>,wherexandyarethecenteroftheboundingboxandwandhdenoteits
widthandheight. Thecoordinatesstartfromthetopleftcornerandarenormalizedtoa
scaleof0to1.’
‘Describewhere{object}islocatedintheimageusingtheformat<b>(x,y),{w,h}</b>.
Here,xandyindicatetheboundingboxcentercoordinates,andwandhspecifyitswidth
andheight.Thecoordinatesshouldbenormalizedstartingfromthetopleftcorner,within
therangeof0to1.’
‘Couldyoutellmethelocationof{object}intheimage? Usetheformat<b>(x, y),{w,
h}</b>,wherexandydenotethecenteroftheboundingboxandwandharethewidth
andheight. Coordinatesstartatthetopleftcornerandshouldbenormalizedbetween0
and1.’
‘Provide the coordinates of {object} in the image in the format <b>(x, y),{w, h}</b>.
Here, xandyarethecenteroftheboundingbox, whilewandhrepresentitswidthand
height.Thecoordinatesshouldstartfromthetopleftcornerandbenormalizedto0to1.’
‘How isthe {object}located inthe image? List itscoordinates usingthe format<b>(x,
y),{w,h}</b>,wherexandyarethecentercoordinatesoftheboundingbox,andwandh
indicateitswidthandheight. Thecoordinatesbeginatthetopleftcornerandarenormal-
izedtoarangeof0to1.’
36Table20:Promptcandidatesforobjectdetection.
‘Identifyanddescribeeachobjectintheimage.Foreachobject,listitintheformat<b>(x,
y),{w,h}</b>,wherexandyrepresentthecoordinatesoftheboundingboxcenter,andw
andhrepresentthewidthandheightoftheboundingbox. Theimagecoordinatesshould
startfromthetopleftcornerandbenormalizedbetween0and1.’
‘Catalogalltheobjectspresentintheimage.Foreveryobject,usetheformat<b>(x,y),{w,
h}</b>,withxandyindicatingthecenteroftheobject’sboundingboxcoordinates,and
wandhspecifyingthewidthandheight.Thecoordinatesarenormalizedfromthetopleft
corner,rangingfrom0to1.’
‘Listeachobjectintheimageanddescribeit. Usetheformat<b>(x,y),{w,h}</b>for
eachobject,wherexandydenotethecentercoordinatesoftheboundingbox,andwand
harethewidthandheightoftheboundingbox.Thecoordinatesshouldstartfromthetop
leftcornerandbenormalizedtoascaleof0to1.’
‘Providedescriptionsforallobjectswithintheimage. Eachobjectshouldbelistedusing
theformat<b>(x,y),{w,h}</b>,wherexandyarethecoordinatesoftheboundingbox
center, and w and h are the width and height. The coordinates should be normalized,
startingfromthetopleftcorner,withinarangeof0to1.’
‘Enumerate and describe every object found in the image. For each object, utilize the
format<b>(x,y),{w,h}</b>,wherex,yaretheboundingboxcentercoordinatesandw,h
arethedimensions(widthandheight)oftheboundingbox. Thecoordinatesbeginatthe
topleftcornerandarenormalizedbetween0and1.’
‘Detailalltheobjectswithintheimage, listingeachoneusingtheformat<b>(x, y),{w,
h}</b>. Here,xandyrepresentthecoordinatesoftheboundingboxcenter,whilewand
h indicate the width and height. The coordinates start from the top left corner and are
normalizedtotherangeof0to1.’
‘Documenteachobjectpresentintheimage.Foreachobject,usetheformat<b>(x,y),{w,
h}</b>,wherexandyarethecoordinatesofthecenteroftheboundingbox,andwandh
arethewidthandheight.Thecoordinatesshouldbenormalized,startingfromthetopleft
corner,andrangefrom0to1.’
‘Foreachobjectintheimage,provideadescriptionusingtheformat<b>(x,y),{w,h}</b>.
Here,xandydenotethecoordinatesoftheboundingboxcenter,andwandhrepresent
thewidthandheightoftheboundingbox. Thecoordinatesarenormalizedtoascaleof0
to1,startingfromthetopleftcorner.’
‘Describealltheobjectsseenintheimage,andlistthemusingtheformat<b>(x,y),{w,
h}</b>. Thexandyvaluesarethecoordinatesforthecenteroftheboundingbox,while
wandhrepresentitswidthandheight.Thecoordinatesshouldbenormalizedfromthetop
leftcorner,withinarangeof0to1.’
‘Identify and list each object found in the image. For each one, use the format <b>(x,
y),{w,h}</b>.Inthisformat,xandyarethecoordinatesfortheboundingboxcenter,and
wandharethewidthandheight. Thecoordinatesaretobenormalizedstartingfromthe
topleftcorner,rangingfrom0to1.’
‘Listanddescribeeachobjectintheimageusingtheformat<b>(x,y),{w,h}</b>. Here,
xandycorrespondtothecoordinatesoftheboundingboxcenter, andwandhspecify
thewidthandheightoftheboundingbox. Thecoordinatesshouldstartfromthetopleft
cornerandbenormalizedtotherangeof0to1.’
‘Provideadescriptionforeachobjectintheimage, formattedas<b>(x, y),{w, h}</b>.
The x and y values indicate the center coordinates of the bounding box, while w and h
represent the width and height. The coordinates start from the top left corner and are
normalizedbetween0and1.’
‘Catalog each object within the image, using the format <b>(x, y),{w, h}</b> for each
one. Inthisformat,xandyarethecoordinatesforthecenteroftheboundingbox,andw
andharethewidthandheight. Thecoordinatesshouldbenormalized,beginningatthe
topleftcornerandrangingfrom0to1.’
‘Enumeratealltheobjectsintheimage,providingdescriptionsforeachusingtheformat
<b>(x,y),{w,h}</b>.Thexandyvaluesrepresentthecentercoordinatesofthebounding
box,whilewandhindicateitswidthandheight. Thecoordinatesarenormalizedstarting
fromthetopleftcorner,withinarangeof0to1.’
‘Describeeachobjectintheimage,listingthemintheformat<b>(x,y),{w,h}</b>.Here,
xandydenotethecentercoordinatesoftheboundingbox,andwandhspecifythewidth
andheight.Thecoordinatesshouldbenormalizedfromthetopleftcorner,rangingfrom0
to1.’
37Table21:Promptcandidatesforactionprediction.{scene}willbereplacedwithalistofobjectsinthescene.
‘Could you detail the steps needed to transform the scene shown in the image into the
second scene? The second scene is provided as a collection of object bounding boxes
{scene}. The format for these bounding boxes is <b>(x, y), {w, h}</b>, where x and y
represent the center coordinates, and w and h are the width and height. The coordinates
shouldbenormalizedtoascaleof0to1,startingfromthetopleftcorner.’
‘Can you describe what actions are required to rearrange the scene shown in the image
tomatchthesecondscene? Thesecondsceneisgivenasasetofobjectboundingboxes
{scene}. Theseboundingboxesfollowtheformat<b>(x, y), {w, h}</b>, wherexandy
indicatethecentercoordinates, andwandhrepresentthewidthandheight. Thecoordi-
natesshouldstartfromthetopleftcornerandbenormalizedtoascaleof0to1.’
‘Couldyoulistthestepsnecessarytomodifythesceneshownintheimagetothesecond
scene? The second scene is described as a collection of object bounding boxes {scene}.
Theboundingboxformatis<b>(x,y),{w,h}</b>,withxandydenotingthecentercoor-
dinates,andwandhrepresentingthewidthandheight. Thecoordinatesarenormalizedto
ascaleof0to1,startingfromthetopleftcorner.’
‘Canyouexplainwhatneedstobedonetoadjustthesceneshownintheimagetoresemble
thesecondscene? Thesecondscene{scene}consistsofobjectboundingboxesprovided
intheformat<b>(x,y),{w,h}</b>. Here,xandyrepresentthecentercoordinates,andw
andharethewidthandheight. Thecoordinatesshouldstartfromthetopleftcornerand
benormalizedtoascaleof0to1.’
‘Could you outline the necessary actions to arrange the scene shown in the image into
thesecondscene? Thesecondsceneisdefinedbyacollectionofobjectboundingboxes
{scene}. Theseboundingboxesfollowtheformat<b>(x, y), {w, h}</b>, wherexandy
denotethecentercoordinates,andwandharethewidthandheight. Thecoordinatesstart
fromthetopleftcornerandshouldbenormalizedtoascaleof0to1.’
‘Canyouspecifywhatneedstobedonetoconvertthesceneshownintheimageintothe
secondscene? Thesecondsceneisprovidedasaseriesofobjectboundingboxes{scene}.
Theformatfortheseboundingboxesis<b>(x,y),{w,h}</b>,withxandyrepresenting
thecentercoordinates, andwandhindicatingthewidthandheight. Coordinatesshould
benormalizedfromthetopleftcornertoascaleof0to1.’
‘Couldyoudescribethestepsrequiredtochangethesceneshownintheimagetothesec-
ondscene?Thesecondsceneisdepictedasacollectionofobjectboundingboxes{scene}.
Theboundingboxformatis<b>(x,y),{w,h}</b>,wherexandydenotethecentercoor-
dinates,andwandhrepresentthewidthandheight. Thecoordinatesarenormalizedtoa
scaleof0to1startingfromthetopleftcorner.’
38Table22: (Continued)Promptcandidatesforactionprediction. {scene}willbereplacedwithalistofobjects
inthescene.
‘Can you list the actions necessary to transform the scene shown in the image into the
secondscene? Thesecondsceneisdescribedusingobjectboundingboxes{scene}. The
format of these bounding boxes is <b>(x, y), {w, h}</b>, where x and y are the center
coordinates,andwandhrepresentthewidthandheight.Coordinatesshouldbenormalized
toascaleof0to1startingfromthetopleftcorner.’
‘Could you explain the process to arrange the scene shown in the image to match the
second scene? The second scene is provided as a collection of object bounding boxes
{scene}. These bounding boxes are formatted as <b>(x, y), {w, h}</b>, where x and y
represent the center coordinates, and w and h are the width and height. The coordinates
shouldstartfromthetopleftcornerandbenormalizedtoascaleof0to1.’
‘Can you detail what needs to be done to rearrange the scene shown in the image to the
second scene? The second scene is given as a series of object bounding boxes {scene}.
Theboundingboxformatis<b>(x,y),{w,h}</b>,wherexandydenotethecentercoor-
dinates,andwandhrepresentthewidthandheight. Coordinatesshouldbenormalizedto
ascaleof0to1startingfromthetopleftcorner.’
‘Couldyouspecifythestepsneededtomodifythesceneshownintheimagetoresemble
thesecondscene?Thesecondsceneisdescribedasasetofobjectboundingboxes{scene}.
Theseboundingboxesfollowtheformat<b>(x,y),{w,h}</b>,wherexandyrepresent
the center coordinates, and w and h indicate the width and height. The coordinates start
fromthetopleftcornerandshouldbenormalizedtoascaleof0to1.’
‘Can you outline the necessary actions to change the scene shown in the image into the
second scene? The second scene {scene} consists of object bounding boxes provided in
theformat<b>(x,y),{w,h}</b>,wherexandydenotethecentercoordinates,andwand
h represent the width and height. Coordinates should be normalized to a scale of 0 to 1
startingfromthetopleftcorner.’
‘Couldyoudescribethestepstoadjustthesceneshownintheimagetothesecondscene?
Thesecondsceneisgivenasacollectionofobjectboundingboxes{scene}. Theformat
for these bounding boxes is <b>(x, y), {w, h}</b>, where x and y represent the center
coordinates,andwandharethewidthandheight. Thecoordinatesshouldstartfromthe
topleftcornerandbenormalizedtoascaleof0to1.’
‘Can you explain what needs to be done to transform the scene shown in the image into
the second scene? The second scene is depicted using object bounding boxes {scene}.
Thebounding boxformat is<b>(x, y), {w, h}</b>, withx andyrepresenting thecenter
coordinates,andwandhindicatingthewidthandheight. Thecoordinatesstartfromthe
topleftcornerandarenormalizedtoascaleof0to1.’
‘Couldyoudetailthestepsnecessarytoconvertthesceneshownintheimagetothesecond
scene? The second scene is described as a set of object bounding boxes {scene}. These
bounding boxes follow the format <b>(x, y), {w, h}</b>, where x and y represent the
centercoordinates, andwandhdenotethewidthandheight. Thecoordinatesshouldbe
normalizedtoascaleof0to1startingfromthetopleftcorner.’
39Table23:Promptcandidatesforfutureprediction.{pickandplace}willbereplacedwiththeactiontext.
‘Theimageshowsascenewithmultipleobjects. Nowyou{pickandplace},whatwillthe
scenelooklike? Listtheobjectboundingboxes. Theboundingboxformatis<b>(x, y),
{w, h}</b>, where x and y represent the center coordinates of the bounding box, and w
andhareitswidthandheight. Thecoordinatesshouldstartfromthetopleftcornerandbe
normalizedtoascaleof0to1.’
‘Animagedepictsascenecontainingmultipleobjects. Nowyou{pickandplace}, what
will the scene look like? Write the list of object bounding boxes. The bounding boxes
shouldbeformattedas<b>(x,y),{w,h}</b>,wherexandydenotethecentercoordinates,
andwandharethewidthandheight. Thecoordinatesstartfromthetopleftcornerand
arenormalizedtoascaleof0to1.’
‘The image presents a scene with several objects. Now you {pick and place}, what will
thescenelooklike? Listtheobjectboundingboxes. Theformatfortheseboundingboxes
is<b>(x,y),{w,h}</b>,wherexandyrepresentthecentercoordinates,andwandhare
thewidthandheight. Coordinatesshouldstartfromthetopleftcornerandbenormalized
toascaleof0to1.’
‘Displayedintheimageisascenecontainingmultipleobjects.Nowyou{pickandplace},
whatwillthescenelooklike?Writedownthelistofobjectboundingboxes.Thesebound-
ingboxesfollowtheformat<b>(x,y),{w,h}</b>,withxandyasthecentercoordinates,
andwandhasthewidthandheight. Thecoordinatesshouldbenormalizedstartingfrom
thetopleftcornertoascaleof0to1.’
‘Theimageillustratesascenewithmultipleobjects. Nowyou{pickandplace},whatwill
the scene look like? Write the list of object bounding boxes. The bounding boxes are
formatted as <b>(x, y), {w, h}</b>, where x and y denote the center coordinates, and w
andhrepresentthewidthandheight. Coordinatesshouldstartfromthetopleftcornerand
benormalizedtoascaleof0to1.’
‘Theimagedepictsascenewithseveralobjects. Nowyou{pickandplace},whatwillthe
scenelooklike? Listtheobjectboundingboxes. Theboundingboxformatis<b>(x, y),
{w,h}</b>,wherexandyrepresentthecentercoordinates,andwandhdenotethewidth
andheight. Thecoordinatesshouldbenormalizedtoascaleof0to1startingfromthetop
leftcorner.’
‘Intheimage,thereisascenewithmultipleobjects. Nowyou{pickandplace},whatwill
thescenelooklike?Writethelistofobjectboundingboxes.Theformatofthesebounding
boxesis<b>(x,y),{w,h}</b>,wherexandyindicatethecentercoordinates,andwand
h represent the width and height. The coordinates start from the top left corner and are
normalizedtoascaleof0to1.’
‘Animageshowsascenewithvariousobjects. Nowyou{pickandplace},whatwillthe
scenelooklike?Writedownthelistofobjectboundingboxes.Theboundingboxesfollow
theformat<b>(x,y),{w,h}</b>,wherexandydenotethecentercoordinates,andwand
h are the width and height. The coordinates should start from the top left corner and be
normalizedtoascaleof0to1.’
‘Theimagepresentsascenecontainingseveralobjects. Nowyou{pickandplace},what
will the scene look like? List the object bounding boxes. The bounding box format is
<b>(x,y),{w,h}</b>,wherexandyrepresentthecentercoordinates,andwandharethe
widthandheight. Coordinatesshouldstartfromthetopleftcornerandbenormalizedtoa
scaleof0to1.’
40Table 24: (Continued) Prompt candidates for future prediction. {pick and place} will be replaced with the
actiontext.
‘Theimagedisplaysascenewithmultipleobjects. Nowyou{pickandplace},whatwill
thescenelooklike? Writethelistofobjectboundingboxes. Theboundingboxesshould
beintheformat<b>(x,y),{w,h}</b>,wherexandydenotethecentercoordinates,andw
andhrepresentthewidthandheight. Thecoordinatesshouldstartfromthetopleftcorner
andbenormalizedtoascaleof0to1.’
‘Animageillustratesascenewithmultipleobjects. Nowyou{pickandplace},whatwill
thescenelooklike? Writedownthelistofobjectboundingboxes. Theseboundingboxes
are formatted as <b>(x, y), {w, h}</b>, where x and y represent the center coordinates,
andwandhdenotethewidthandheight. Coordinatesshouldbenormalizedtoascaleof
0to1startingfromthetopleftcorner.’
‘Theimageshowsascenewithvariousobjects. Nowyou{pickandplace},whatwillthe
scenelooklike? Listtheobjectboundingboxes. Theformatfortheseboundingboxesis
<b>(x, y), {w, h}</b>, with x and y representing the center coordinates, and w and h as
thewidthandheight. Coordinatesshouldstartfromthetopleftcornerandbenormalized
toascaleof0to1.’
‘Displayedintheimageisascenecontainingmultipleobjects.Nowyou{pickandplace},
whatwillthescenelooklike? Writethelistofobjectboundingboxes. Theboundingbox
formatis<b>(x,y),{w,h}</b>,wherexandydenotethecentercoordinates,andwandh
arethewidthandheight. Thecoordinatesstartfromthetopleftcornerandarenormalized
toascaleof0to1.’
‘Theimageillustratesascenewithvariousobjects. Nowyou{pickandplace},whatwill
the scene look like? List the object bounding boxes. The bounding boxes are formatted
as <b>(x, y), {w, h}</b>, where x and y indicate the center coordinates, and w and h
representthewidthandheight. Coordinatesshouldbenormalizedfromthetopleftcorner
toascaleof0to1.’
‘An image depicts a scene with multiple objects. Now you {pick and place}, what will
the scene look like? Write the list of object bounding boxes. The bounding box format
is <b>(x, y), {w, h}</b>, where x and y represent the center coordinates, and w and h
denotethewidthandheight. Thecoordinatesshouldstartfromthetopleftcornerandbe
normalizedtoascaleof0to1.’
41Table 25: Prompt candidates for spatial relationship. {ego_obj} and {ref_obj} will be replaced with object
namesand{example}willbereplacedwitharandomspatialrelationshipfromthesameimage.
"Canyoudescribetherelativespatiallocationsof{ego_obj}comparedto{ref_obj}inthis
image? Use relative location words like left, right, above, below, etc. Also, find the 2D
centerdistanceandtheEuclideancenterdistancebetweenthem. Youroutputmustfollow
thisformat: {example}. Thecoordinatesareimagecoordinatesnormalizedtoascaleof0
to1startingfromthetopleftcorner.’
‘Couldyoudescribetherelativespatialpositionsof{ego_obj}incomparisonto{ref_obj}
inthisimage? Usetermslikeleft,right,above,below,etc. Also,calculatethe2Dcenter
distanceandtheEuclideancenterdistancebetweenthem.Youroutputshouldbeformatted
asfollows: {example}. Thecoordinatesareimagecoordinatesnormalizedtoascaleof0
to1startingfromthetopleftcorner.’
‘Pleasedescribetherelativespatiallocationsof{ego_obj}comparedto{ref_obj}inthis
image. Use words like left, right, above, below, etc. Additionally, find the 2D center
distance and the Euclidean center distance between them. Your output must be in this
format: {example}. Thecoordinatesareimagecoordinatesnormalizedtoascaleof0to1
startingfromthetopleftcorner.’
‘Canyouexplaintherelativespatialpositionsof{ego_obj}comparedto{ref_obj}inthis
image? Use terms such as left, right, above, below, etc. Also, determine the 2D center
distanceandtheEuclideancenterdistancebetweenthem. Youroutputshouldmatchthis
format: {example}. Thecoordinatesareimagecoordinatesnormalizedtoascaleof0to1
startingfromthetopleftcorner.’
‘Describetherelativespatiallocationsof{ego_obj}comparedto{ref_obj}inthisimage
using words like left, right, above, below, etc. Also, calculate the 2D center distance
and the Euclidean center distance between them. Your output must follow this format:
{example}. Thecoordinatesareimagecoordinatesnormalizedtoascaleof0to1starting
fromthetopleftcorner.’
‘Couldyoudescribethespatialrelationshipbetween{ego_obj}and{ref_obj}inthisim-
age using relative location words like left, right, above, below, etc.? Also, find the 2D
center distance and the Euclidean center distance between them. Your output should be
formattedasfollows: {example}. Thecoordinatesareimagecoordinatesnormalizedtoa
scaleof0to1startingfromthetopleftcorner.’
‘Can you detail the relative spatial positions of {ego_obj} compared to {ref_obj} in this
image? Use words like left, right, above, below, etc. Also, determine the 2D center dis-
tanceandtheEuclideancenterdistancebetweenthem. Youroutputmustbeinthisformat:
{example}. Thecoordinatesareimagecoordinatesnormalizedtoascaleof0to1starting
fromthetopleftcorner.’
‘Couldyouexplainthespatialrelationshipbetween{ego_obj}and{ref_obj}inthisimage
usingtermssuchasleft,right,above,below,etc.? Also,calculatethe2Dcenterdistance
and the Euclidean center distance between them. Your output should match this format:
{example}. Thecoordinatesareimagecoordinatesnormalizedtoascaleof0to1starting
fromthetopleftcorner.’
‘Describetherelativespatialpositionsof{ego_obj}comparedto{ref_obj}inthisimage.
Use relative location words like left, right, above, below, etc. Also, find the 2D center
distance and the Euclidean center distance between them. Your output must follow this
format: {example}. Thecoordinatesareimagecoordinatesnormalizedtoascaleof0to1
startingfromthetopleftcorner.’
42Table26: (Continued)Promptcandidatesforspatialrelationship. {ego_obj}and{ref_obj}willbereplaced
withobjectnamesand{example}willbereplacedwitharandomspatialrelationshipfromthesameimage.
‘Can you describe how {ego_obj} is positioned relative to {ref_obj} in this image using
words such as left, right, above, below, etc.? Also, find the 2D center distance and the
Euclideancenterdistancebetweenthem.Youroutputshouldbeinthisformat:{example}.
Thecoordinatesareimagecoordinatesnormalizedtoascaleof0to1startingfromthetop
leftcorner.’
‘Couldyoudetailtherelativepositionsof{ego_obj}comparedto{ref_obj}inthisimage
usingtermslikeleft,right,above,below,etc.? Also,calculatethe2Dcenterdistanceand
the Euclidean center distance between them. Your output must be formatted as follows:
{example}. Thecoordinatesareimagecoordinatesnormalizedtoascaleof0to1starting
fromthetopleftcorner.’
‘Please describe the spatial relationship of {ego_obj} in comparison to {ref_obj} in this
image using relative location terms such as left, right, above, below, etc. Additionally,
findthe2DcenterdistanceandtheEuclideancenterdistancebetweenthem. Youroutput
shouldmatchthisformat: {example}. Thecoordinatesareimagecoordinatesnormalized
toascaleof0to1startingfromthetopleftcorner.’
‘Canyoudescribetherelativespatiallocationsof{ego_obj}comparedto{ref_obj}inthis
image? Userelativelocationwordslikeleft,right,above,below,etc. Also,calculatethe
2D center distance and the Euclidean center distance between them. Your output should
follow this format: {example}. The coordinates are image coordinates normalized to a
scaleof0to1startingfromthetopleftcorner.’
‘Couldyoudescribethespatiallocationsof{ego_obj}relativeto{ref_obj}inthisimage
usingwordssuchasleft,right,above,below,etc.?Additionally,findthe2Dcenterdistance
and the Euclidean center distance between them. Your output must be in this format:
{example}. Thecoordinatesareimagecoordinatesnormalizedtoascaleof0to1starting
fromthetopleftcorner.’
43Table27:Promptcandidatesfortemporalrelationship{scene}willbereplacedwithalistofobjects,{ego_obj}
and {ref_obj} will be replaced with object names and {example} will be replaced with a random temporal
relationshipfromthesameimage.
"The image shows a scene at the first timestamp, while the second image described as
{scene} shows the next timestamp. Can you describe the change in the relative location
of{ego_obj}comparedto{ref_obj}betweenthesetwotimestamps? Userelativedistance
words like getting closer or further away, etc. Also, find the change in the 2D center
distance and the Euclidean center distance between the two images. Your output must
follow this format: {example}. The coordinates are image coordinates normalized to a
scaleof0to1startingfromthetopleftcorner.’
‘In the first timestamp, the image shows a scene, and the second image described as
{scene} depicts the next timestamp. Can you describe the change in the relative loca-
tionof{ego_obj}comparedto{ref_obj}betweenthesetwotimestamps? Usetermslike
gettingcloserormovingfurtheraway,etc. Additionally,findthechangeinthe2Dcenter
distanceandtheEuclideancenterdistancebetweenthetwoimages. Youroutputmustfol-
lowthisformat: {example}. Thecoordinatesareimagecoordinatesnormalizedtoascale
of0to1startingfromthetopleftcorner.’
‘The scene in the first image is at the initial timestamp, and the second image described
as {scene} shows the subsequent timestamp. Can you explain the change in the relative
locationof{ego_obj}comparedto{ref_obj}betweenthesetwotimestamps? Usewords
likegettingcloserormovingfurtherapart,etc. Also,calculatethechangeinthe2Dcenter
distanceandtheEuclideancenterdistancebetweenthetwoimages.Youroutputshouldbe
formattedasfollows: {example}. Thecoordinatesareimagecoordinatesnormalizedtoa
scaleof0to1startingfromthetopleftcorner.’
‘At the first timestamp, the image shows a scene, and the second image described as
{scene}representsthenexttimestamp. Canyoudetailthechangeintherelativelocation
of{ego_obj}comparedto{ref_obj}betweenthesetwotimestamps? Userelativedistance
wordslikemovingcloserorgettingfurtheraway,etc. Additionally,findthechangeinthe
2DcenterdistanceandtheEuclideancenterdistancebetweenthetwoimages.Youroutput
mustfollowthisformat:{example}. Thecoordinatesareimagecoordinatesnormalizedto
ascaleof0to1startingfromthetopleftcorner.’
‘Thefirstimageshowsasceneataninitialtimestamp,andthesecondimagedescribedas
{scene}depictsthenexttimestamp. Canyoudescribethechangeintherelativeposition
of {ego_obj} compared to {ref_obj} between these two timestamps? Use terms such as
getting closer or moving further apart, etc. Also, determine the change in the 2D center
distance and the Euclidean center distance between the two images. Your output should
follow this format: {example}. The coordinates are image coordinates normalized to a
scaleof0to1startingfromthetopleftcorner.’
‘The initial timestamp shows a scene in the first image, and the second image described
as {scene} represents the next timestamp. Can you describe how the relative location of
{ego_obj} compared to {ref_obj} changes between these two timestamps? Use relative
distancewordslikegettingcloserormovingfurtheraway,etc. Also,findthechangeinthe
2DcenterdistanceandtheEuclideancenterdistancebetweenthetwoimages.Youroutput
mustbeinthisformat: {example}. Thecoordinatesareimagecoordinatesnormalizedto
ascaleof0to1startingfromthetopleftcorner.’
‘The image shows a scene at the first timestamp, and the second image described as
{scene} shows the subsequent timestamp. Can you detail the change in the relative lo-
cation of {ego_obj} compared to {ref_obj} between these two timestamps? Use words
likegettingcloserormovingfurtherapart,etc. Also,calculatethechangeinthe2Dcenter
distanceandtheEuclideancenterdistancebetweenthetwoimages.Youroutputshouldbe
formattedasfollows: {example}. Thecoordinatesareimagecoordinatesnormalizedtoa
scaleof0to1startingfromthetopleftcorner.’
44Table 28: (Continued) Prompt candidates for temporal relationship {scene} will be replaced with a list of
objects,{ego_obj}and{ref_obj}willbereplacedwithobjectnamesand{example}willbereplacedwitha
randomtemporalrelationshipfromthesameimage.
‘At the initial timestamp, the image shows a scene, and the second image described as
{scene}depictsthenexttimestamp. Canyoudescribethechangeintherelativeposition
of{ego_obj}comparedto{ref_obj}betweenthesetwotimestamps? Userelativedistance
termssuchasgettingcloserormovingfurtheraway,etc. Also,findthechangeinthe2D
center distance and the Euclidean center distance between the two images. Your output
mustfollowthisformat:{example}. Thecoordinatesareimagecoordinatesnormalizedto
ascaleof0to1startingfromthetopleftcorner.’
‘The scene in the first image is at the initial timestamp, and the second image described
as {scene} shows the following timestamp. Can you describe the change in the relative
locationof{ego_obj}comparedto{ref_obj}betweenthesetwotimestamps? Usewords
like getting closer or moving further apart, etc. Additionally, calculate the change in the
2DcenterdistanceandtheEuclideancenterdistancebetweenthetwoimages.Youroutput
shouldfollowthisformat: {example}. Thecoordinatesareimagecoordinatesnormalized
toascaleof0to1startingfromthetopleftcorner.’
‘The first image shows a scene at an initial timestamp, and the second image described
as {scene} depicts the next timestamp. Can you explain how the relative location of
{ego_obj} compared to {ref_obj} changes between these two timestamps? Use relative
distancewordslikemovingcloserorgettingfurtheraway,etc. Also,determinethechange
inthe2DcenterdistanceandtheEuclideancenterdistancebetweenthetwoimages. Your
output must follow this format: {example}. The coordinates are image coordinates nor-
malizedtoascaleof0to1startingfromthetopleftcorner.’
‘The image shows a scene at the initial timestamp, and the second image described as
{scene}showsthenexttimestamp. Canyoudescribethechangeintherelativepositionof
{ego_obj}comparedto{ref_obj}betweenthesetwotimestamps? Usewordslikegetting
closer or moving further apart, etc. Also, calculate the change in the 2D center distance
andtheEuclideancenterdistancebetweenthetwoimages. Youroutputshouldfollowthis
format: {example}. Thecoordinatesareimagecoordinatesnormalizedtoascaleof0to1
startingfromthetopleftcorner.’
‘At the first timestamp, the image shows a scene, and the second image described as
{scene}depictsthenexttimestamp. Canyoudetailhowtherelativelocationof{ego_obj}
comparedto{ref_obj}changesbetweenthesetwotimestamps?Userelativedistanceterms
suchasmovingcloserorgettingfurtheraway,etc. Also,findthechangeinthe2Dcenter
distanceandtheEuclideancenterdistancebetweenthetwoimages. Youroutputmustfol-
lowthisformat: {example}. Thecoordinatesareimagecoordinatesnormalizedtoascale
of0to1startingfromthetopleftcorner.’
‘The initial timestamp shows a scene in the first image, and the second image described
as {scene} represents the next timestamp. Can you describe the change in the relative
locationof{ego_obj}comparedto{ref_obj}betweenthesetwotimestamps? Usewords
like getting closer or moving further apart, etc. Also, determine the change in the 2D
center distance and the Euclidean center distance between the two images. Your output
shouldfollowthisformat: {example}. Thecoordinatesareimagecoordinatesnormalized
toascaleof0to1startingfromthetopleftcorner.’
‘The first image shows a scene at the initial timestamp, and the second image described
as {scene} shows the following timestamp. Can you describe how the relative position
of{ego_obj}comparedto{ref_obj}changesbetweenthesetwotimestamps? Useterms
like moving closer or getting further away, etc. Additionally, find the change in the 2D
center distance and the Euclidean center distance between the two images. Your output
mustfollowthisformat:{example}. Thecoordinatesareimagecoordinatesnormalizedto
ascaleof0to1startingfromthetopleftcorner.’
45