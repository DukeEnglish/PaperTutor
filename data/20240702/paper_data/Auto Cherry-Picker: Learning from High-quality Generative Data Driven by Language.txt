Auto Cherry-Picker : Learning from High-quality
Generative Data Driven by Language
YichengChen1,2,XiangtaiLi1,3,YiningLi1†,YanhongZeng1,
JianzongWu1,4,XiangyuZhao1,5,KaiChen1†
1ShanghaiAILaboratory2TongjiUniversity
3S-Lab,NanyangTechnologicalUniversity
4PekingUniversity5ShanghaiJiaoTongUniversity
Projectpage: https://yichengchen24.github.io/projects/autocherrypicker
Abstract
Diffusion-based models have shown great potential in generating high-quality
images with various layouts, which can benefit downstream perception tasks.
However,afullyautomaticlayoutgenerationdrivenonlybylanguageandasuitable
metricformeasuringmultiplegeneratedinstanceshasnotbeenwellexplored. In
thiswork,wepresentAutoCherry-Picker(ACP),anovelframeworkthatgenerates
high-qualitymulti-modaltrainingexamplestoaugmentperceptionandmulti-modal
training. Startingwithasimplelistofnaturallanguageconcepts,wepromptlarge
languagemodels(LLMs)togenerateadetaileddescriptionanddesignreasonable
layouts. Next,weuseanoff-the-shelftext-to-imagemodeltogeneratemultiple
images. Then,thegenerateddataarerefinedusingacomprehensivelydesigned
metrictoensurequality. Inparticular,wepresentanewmetric,CompositeLayout
andImageScore(CLIS),toevaluatethegeneratedimagesfairly. Oursynthetic
high-qualityexamplesboostperformanceinvariousscenariosbycustomizingthe
initialconceptlist,especiallyinaddressingchallengesassociatedwithlong-tailed
distribution and imbalanced datasets. Experiment results on downstream tasks
demonstratethatAutoCherry-Pickercansignificantlyimprovetheperformance
ofexistingmodels. Inaddition,wehavethoroughlyinvestigatedthecorrelation
between CLIS and performance gains in downstream tasks, and we find that a
betterCLISscoreresultsinbetterperformance. Thisfindingshowsthepotential
forevaluationmetricsastheroleforvariousvisualperceptionandMLLMtasks.
Codewillbeavailable.
1 Introduction
Recently,diffusion-basedimagegenerationmethods[64,71]havemaderemarkableprogress,which
hasledtovariousapplications,includingtext-to-imagegeneration(T2I)[22,23,63,67,77],image
editing [4, 27, 55, 66], video generation [24, 30, 31], art creation [18, 65], and more. Compared
withpreviousgenerativemodels[15,34,57],diffusionmodelscangeneratehigh-qualityandhigh-
resolutionexamples.Thus,oneessentialusageofthediffusion-basedmodelistocreateusefultraining
examplesforvariousdownstreamvisiontasks,suchassegmentation[37,43,84],detection[8,46],
andvisualrepresentationlearning[38,75]. Usinggenerateddataalleviatestheseveredemandfor
humanannotationandprovidesamorecontrollabledataproductionprocess.
†CorrespondingAuthor.
Preprint.Underreview.
4202
nuJ
82
]VC.sc[
1v58002.6042:viXraFigure1:IllustrationofqualityassessmentofgenerateddatasamplesusingCLIS.(a)and(c)compare
thequalityofsampleswithdifferentCLIS-LandCLIS-Iscores,respectively. SampleswithlowCLIS
failtoalignaccuratelywiththecondition(e.g.,containextraneousobjectsorexhibitvisualflaws).
(b)and(d)comparethepreferencesofCLISandCLIPscore[28].
Thus,previousworksexplorehowtogeneratetrainingexamplesforvarioustasksconditionedon
variousreferences,suchascaptioning[45],layoutguidance[8,80],referencemasks[52,73],and
referenceimage[87]. Inparticular,InstanceDiffusion[80]generatesimageswithpreciseinstance-
levelcontrol,includingattributebindingandlocalization,whilemaintaininghighfidelityconditioned
ondetailedinstances. However,thesedatagenerationapproachesstillrelyonmanualannotations.
In addition, due to the inherent randomness of generative models, the quality of generated data
tendstovary,potentiallyimpairingtheeffectivenessofusingthisdatatotraindownstreamtasks.
Consequently,appropriatequalityassessmentmetricsmustbeemployedtofilteroutthesynthetic
data,whichhasnotbeenextensivelystudied. Therefore,asillustratedinFigure2,currentmethods
arelimitedbyexpensivemanualannotationsandalackofeffectivequalityassessmentmetrics. This
studyaimstoexploreanewgenerationpipelinethatdoesnotrequirerealtextualorvisualannotations,
suchascaptionsandlayouts. Tosolvethisissue,severalessentialquestionsareraised: 1)Howcan
wereducetherelianceonannotations,includingcaptionandlayout? 2)Howdowemeasurethe
qualityformultipleinstances,andcanweproposeanewmetrictoselectgoodones? 3)Doesthe
proposedmetricreflectthefinaldownstreamperformancewhenusedfortraining?
Tothisend,weproposeAutoCherry-Picker(ACP),adatageneratorpipelinebasedonpre-trained
generativemodels,togenerateimagesandcorrespondingdetaileddescriptionsandlayoutannotations
simultaneouslyforperceptionandreasoningtasks. Ourmethodcomprisesagenerativemodels-based
raw data generator and a comprehensive data filter. To reduce the reliance on real annotations,
ourrawdatageneratoralterstheparadigmofcurrentdatasynthesis. Itispurelydrivenbynatural
language,i.e.,anarbitrarylistofobjects. WefirstuseLLMstosamplescene-leveldescriptionswith
fine-graineddetails,includingobjectattributes,relations,andspatiallayouts. Then,weadoptT2I
models to generate images conditioned on previous-generated information. This pipeline allows
ustoeasilyscaleupsyntheticexamplesfromasimplelistofinterestedobjects. Besides,wecan
address the unbalanced distribution problem by arranging the category proportion, especially in
long-tailedscenarios. Toensurethequalityofsyntheticdata,wedesignacomprehensivemetric,
namelyCompositeLayoutandImageScore(CLIS),tofiltertherawdatafromthegenerator. This
metricevaluatesthereasonablenessoflayoutgeneration(CLIS-L)andthequalityofimagegeneration
(CLIS-I).CLIS-Liscalculatedbycomparingthesimilaritytodatapriorfromtheopensourcedataset.
AsshowninFigure1(a,b),ahigh-qualitylayoutassessedbyCLIS-Lresemblesreal-worldlayoutsand
ismorelikelytoproducehigh-qualityimages. CLIS-Iiscalculatedbyevaluatingbothvisualquality
andalignmentwithtextdescriptions. AsshowninFigure1(c,d),ahigh-qualityimageassessedby
2+0.3R !aw "#D oa nt a
LVIS
H +ig 5h .2- !qu "#al oit ny LD Va ISta
AnnD ota at ta i ons LLM GeI nm ea rg ae to r O Lb ij se tct LLM L Fa iy lto eu rt GeI nm ea rg ae to r I Fm ila teg re
(a) (b)
Figure2: Comparedwithpreviousmethodsfortrainingsamplesynthesis. (a)Previousmethods,
suchasLMD[45],relyondetailedimagedescriptionsbyleveragingLLMasalayoutgeneratorand
diffusion-basedmodelsasanimagegenerator. (b)Ourmethod,ACP,synthesizestrainingsamples
conditionedsolelyonanobjectlistinnaturallanguageandautomaticallycherry-pickshigh-quality
onesbyevaluatingbothlayoutsandimages. High-qualitytrainingsamplesaremoreeffectiveon
downstreamtasks.
CLIS-Iexhibitshighvisualqualityandstrongalignmentwiththecorrespondingtextdescription. The
filteredscenegraphandcorrespondingimagesareusedastrainingexamples.
Throughcomprehensiveevaluations,ourdesignedCLISsignificantlyenhancestheperformanceof
the state-of-the-state generation model, InstanceDiffusion, from various generation perspectives,
includingimagefidelity,alignmenttotext,andlayoutcontrol. Additionally,weobserveapositive
correlationbetweentheCLISscoreandperformancegainsondownstreamtasks. Byscalingupthe
trainingexamplesgeneratedbyourAutoCherry-Pickerpipeline,weachievesubstantialperformance
improvements for perception and reasoning tasks, particularly in long-tail and open-vocabulary
scenarios. Specifically,ontheLVISdataset[21],weobservea+5.2%improvementinAPmask inthe
r
long-tailsettingusingMaskR-CNN[25]anda+1.3%improvementintheopen-vocabularysetting
usingGroundingDINO[51]. Additionally,weachieveascoreof+80.1ontheMMEbenchmarkand
animprovementof+0.4ontheGQAbenchmarkbasedonLLaVA-v1.5[49],validatingitsefficiency
inmulti-modalperceptionandreasoningsettings.
Wesummarizeourtechnicalcontributionsasfollows:
• WeproposeanewAutoCherry-Pickersystem,anoveltrainingdatageneratorpipelineforpercep-
tionandreasoningtasks. Ourmethodisconditionedsolelyonanobjectlist,eliminatingtheneedfor
textualorvisualannotations.
• WedesignedacomprehensivemetricCLIStofiltergenerateddataeffectively. Weevaluatethe
reasonablenessofgeneratedlayoutsandthequalityofgeneratedimagesbasedonpriorsfromreal
dataorpre-trainedlarge-scalemodels.
• Extensiveexperimentsonvisualandcross-modalityperceptionandreasoningbenchmarksdemon-
stratethatourmethodcanenhancemodelperformanceforvariousdownstreamtasks. Thecorrelation
betweenCLISandperformancegainamongdownstreamtasksiswellstudied.
2 RelatedWork
TexttoImageGeneration. Diffusion-basedapproaches[56,63,64,67]modelthetext-to-image
generationprocessasiterativedenoisingstepsfromrandomnoise. StableDiffusion[64]performs
diffusion steps in the latent space of pre-trained autoencoders to achieve efficient training and
sampling. Subsequent studies extend text-to-image diffusion models with layout controllability
byintroducingauxiliaryinputsignals[41,88]orspatialtokens[86]duringtraining. Anotherline
of works [2, 3, 9, 13, 70, 85] follows a training-free approach by directly intervening the cross-
attentionlayersduringthesamplingprocess. WithrecentprogressinthefieldofLargeLanguage
Models (LLM), LLM has been introduced in the T2I system to enhance text understanding and
alignment[11,16,19,44,60,61,62,90]. LMD[44]andLayoutLLM-T2I[62]firstlyuseLLMsas
text-guidedlayoutgeneratorsthroughin-contextlearning. Control-GPT[90]queryGPT-4togenerate
TikZ-encodedsketchreferencesforT2Imodels. DiffusionGPT[61]utilizesLLMtoaccommodate
diverse text input forms and select domain-expert models for superior generation quality. LLM
Blueprint[19]leveragesLLMstoextractcriticalcomponentsfromtextprompts. LayoutGPT[16]
focusesonnumericalandspatialcorrectnessforlayoutgeneration. Comparedtopreviousworks,we
extendthegenerationparadigmtobeconditionedonasimpleobjectlist.
3Learning from Synthetic Data. Deep learning models, especially for dense prediction tasks,
typicallyrequirelargeamountsofdata,whichcanbecostly. Therefore,manyworksusesynthetic
datatoapproximateinformationgatheredormeasuredintherealworld[79,82]. Synthetictraining
examplesareconditionedonvariousreferences. Someworks[8,52,73]utilizethelayout-to-image
paradigmtosynthesizetrainingsamples,conditioningonvisualannotationslikesegmentationmasks
orboundingboxes. Others[43,83]utilizeanoff-the-shelfperceptionmodeloradoptaperception
headtogetdenseannotationsofsyntheticimages,whicharegeneratedconditioningondetailedtext
description.Syntheticdatacanalsobeusedinself-supervisedlearningdomains[6,10,35,38,40,75].
Amongallthesestudies,noworksexploreafulllanguage-drivenpipeline.Tofillthisgap,ourmethod
isdrivenpurelybylanguagewithouttheneedforexpensivemanuallyannotateddenselabels.
Generative Model Evaluation. Assessment of AI-generated content is challenging due to its
subjectivenatureandthecomplexityoffactorscontributingtothegenerationquality. Metricslike
InceptionScore(IS)[68],FréchetInceptionDistance(FID)[29],andLPIPS[89]arecommonlyused
forqualityanddiversityassessment. SomemethodsutilizeVLMtoevaluatethealignmentbetween
textandgeneratedimage. CLIPScore[28]calculatesthecosinesimilaritybetweentextfeaturesand
generated-imagefeaturesextractedbyCLIP.BLIP-CLIP[7]appliesBLIP[39]togeneratecaptions,
thencalculatestheCLIPtext-textcosinesimilaritybetweenthegeneratedcaptionsandtextprompts.
Forlayoutqualityassessment,LayoutDM[33]furtherproposesMaximumIoU(Max.) tomeasure
thesimilaritybetweengeneratedandreallayoutsastheaverageboxIoUoftheoptimalinstance
matching. OurproposedCLISevaluatesinstance-levelresultsincomplexscenesandcombinesthe
reasonableness of layout and content quality in one shot, making it a suitable metric to generate
high-qualitydatafordownstreamtasks.
3 Method
OurAutoCherry-Pickerisatraining-freecross-modalityperceptionandreasoningdatasetgeneration
pipeline. Itcanproduceimageandscenegraphpairsconditionedonasimplelistofobjectswhile
automaticallyselectingthehigh-qualityonesfortrainingdownstreammodels. Weadoptanoff-the-
shelfLLMasaSceneGraphandLayoutGeneratorandcombineanoff-the-shelfdiffusionmodel
as an Image Generator. We first introduce the problem setting in Sec. 3.1. Then, we detail our
framework,includingtherawdatageneratorandthedatafilterinSec.3.2. Finally,weexplainthe
deploymentonvariousdownstreamtasksinSec.3.3.
3.1 TaskFormation
Wenowpresenttheproblemsettingofdatageneration. GivenanobjectlistO = {o ,o ,...,o },
1 2 n
therawgenerateddataisdenotedasD ,withdetailedscenegraphSGannotationsandasetofraw
r
imagesI.
D =G(O),D ={SG,I} (1)
r r
whereGrepresentsarawdatagenerator. Then,adatafilterF isappliedtotheinitiallygenerated
D toautomaticallyproduceahigh-qualitydatasetD withahigh-qualityscenegraphSG and
r h h
correspondingimagesI .
h
D =F(D ),D ={SG ,I } (2)
h o h h h
GivenD asnewlygenerateddata, wecanevaluatethemfromtwoaspects: 1)usinggeneration
h
metrics,suchasFID[29]. 2)usingD toco-traindownstreamtaskstocheckthegains.
h
3.2 ACPFramework
Weaimtodesignahigh-qualitycross-modalitytrainingdatageneratorconditionedonasimpleobject
listinnaturallanguage,asdepictedinFigure3. ThedesignofACPcomprisestwokeycomponents:
arawdatageneratorandadatafilter. Theformeraimstogeneratedata,whilethelatterselectsgood
oneswithourproposedCLISmetric.
RawDataGenerator. Weproposetogeneratedatasamplesbyharnessingtheinformationfroma
simpleobjectlist. Thisenablesustoeasilyscalethedatasizeandalignittospecificdownstream
tasks. Wetermsuchprocessasarawdatagenerator.
4(a) Raw DataGenerator
(b) Data Filter
Figure3: IllustrationofAutoCherry-Picker(ACP)pipeline. OurACPpipelinecontainsarawdata
generator (a) and a data filter (b) using CLIS. Conditioned on an input object list, Scene Graph
Generatorgeneratesdetailedattributes,relations,captions,andcorrespondinglayouts. Subsequently,
the Image Generator produces a set of images based on the scene graph. These raw layouts and
imagesarerefinedthroughfiltersusingCLIS-LandCLIS-I,respectively,toproducehigh-quality
trainingdata.
AsshowninFigure3(a),wefirstrequireLLMstosampledescriptionsfromtheinitialobjectlist,
leveragingitsin-contextlearningcapability[5]. Thedescriptioncontainsdetailedattributesofeach
object,relationsbetweendifferentobjects,andanoveralldensecaption.Ourmethodinvolvescrafting
specificpromptengineeringtemplatesthatguidetheLLMingeneratingtherequireddescription.
Then,weutilizethespatialreasoningabilityofLLMstoplanlayoutsgivenrelationsanddescriptions
ofobjects. TheLLMinvolvedintheaboveprocessisreferredtoasthescenegraph(description
andlayoutsoftheinputobjectlist)generator. Conditionedonthesynthesisscenegraph,weadopt
anoff-the-shelfdiffusion-basedimagegeneratortogenerateasetofinitialimagesbyinitiatingthe
reversediffusionprocesswithdifferentrandomnoise. FollowingtheapproachusedinStableRep[75]
andSynCLR[74],weproducefourimagesforeachscenegraph.
DataFilter. Fortherawgenerateddatafromstageone,weutilizeadatafiltertocherry-pickhigh-
qualitytrainingdata. AsdepictedinFigure3(b),weusetheLayoutFilterwithCLIS-Landthe
ImageFilterwithCLIS-Itoseparatelyassessthequalityofimagecontentsandlayouts. Wewillfirst
describethelayoutfilterandthentheimagefilter.
(a)LayoutFilter. Toenablemodelswithreasoningabilities,wealsoemphasizetherationalityof
scenegraphs. Weevaluatetherationalityoflayoutsinthescenegraphbasedonthepriorsofthe
groundtruthlayout. Specifically,weextractlayoutfromrealdatasetannotationslikeFlickr30K[59]
withcorrespondingcategoriesandrelationstoconstructanexamplepoolE. Givenasubjects,an
objecto,andtheirrelationr,weconsidertherelativesize,distance,anddirectionsimilaritybetween
5theircorrespondinglayoutsandlayoutsinE withthesamecategoriesandrelation. Formally,we
definetherelativesimilarityscorebetweenAandBasfollows:
|A −B |
S (A,B)=1− n n (3)
sim max(A ,B )
n n
whereA ,B representthenormalizationofAandB,respectively. Herewehavethesizescore:
n n
Area(s) Area(s )
S (s,o,r)= max {S ( , e )} (4)
size {se,oe}∈E(s,o,r) sim Area(o) Area(o e)
whereArearepresentstheareaofcorrespondinglayout.
Forthedistancescore,weconsiderbothIoUandrelativedistancebetweencentersoftwolayouts:
S (s,o,s ,o )=S (IoU(s,o),IoU(s ,o )) (5)
IoU e e sim e e
S (s,o,s ,o )=S (RelDist(s,o),RelDist(s ,o )) (6)
RelDist e e sim e e
S (s,o,r)= max (α·S (s,o,s ,o )+β·S (s,o,s ,o )) (7)
Dist IoU e e RelDist e e
{se,oe}∈E(s,o,r)
whereIoUandRelDistarefunctionstocalculateIoUandrelativedistancebetweentwoobjects. The
coefficientsαandβ controltheweightoftwoitems.
Fordirectionscore:
S (s,o,r)= max Norm(cos[Dir(s,o),Dir(s ,o )]) (8)
Dir e e
{se,oe}∈E(s,o,r)
whereDircalculatesthedirectionvectorbetweentwolayouts.
In summary, CLIS-L for generated layouts conditioned on subject, object, and relation can be
expressedas:
CLIS-L(s,o,r)=w S (s,o,r)+w S (s,o,r)+w S (s,o,r) (9)
1 size 2 Dist 3 Dir
wherewassignsdifferentsignificancetodifferentevaluationperspectives.
(b)ImageFilter. Toenableamodelwithstrongperceptionabilitiesandcomplexreasoningabilities,
weemphasizethevisualqualityoftheimageitselfanditsalignmentwiththecorrespondingcategory
description. Therefore, we filter images from these two perspectives. Specifically, we utilize a
pre-trainedmulti-modalcaptionmodeltodescribetheentireimageandlocalpartscorresponding
toeachlayout. Thisensuresthattheobjectinthegeneratedimageishighlyqualifiedenoughtobe
identifiedbyperceptionmodels. Then,wecalculatethesimilaritybetweenthepredicteddescription
from the caption model and the target description from the scene graph. The image filtering by
CLIS-Icanbeformulatedas:
CLIS-I=Sim(SG ,SG)=Sim(C(I,L),SG) (10)
pred
where L represents layouts in the scene graph. We adopt a pre-trained VLM, Qwen-VL [1], for
captionmodelC andanLLMtocalculatethesimilaritywithintextmodality. Bymanuallydesigning
taskinstructionsforLLM,wecanassigndifferentsignificancetodifferentpartsofthescenegraph.
3.3 DeploymentonDownstreamTasks
Wefirstgeneratetrainingsamplesforvisualperceptiontasksinlong-tailedinstancesegmentationand
open-vocabularyobjectdetection. Then,wegeneratecross-modalitytrainingsamplesforperception
andreasoningtasksinmulti-modalvisualquestionanswering. WeemployQwen1.5-14Basour
SceneGraphGeneratorasdescribedinAppendixA.1.
VisualPerceptionTrainingSamples. Tomaintainconsistencywiththedistributionoftheoriginal
trainingset,wesampledourobjectlistfromtrainingannotations. Weutilizetheimageandlayoutan-
notationfromourgenerateddata. Additionally,weadoptapre-trainedSAMtogeneratesegmentation
maskswithinthelayout.
6Model FID↓
StableDiffusion[64] 56.8
a majestic chestnut-colored horse with white socks agoldenretriever&ablackLabrador 80
72
BoxDiff-SD[85] 60.0
BoxDiff-GLIGEN[85] 61.0 56 60 60
GLIGEN[41] 63.5 38 46 37 48
InstanceDiffusion[80] 53.5 28
GLIGENw.CLIS 59.9(-3.6)
InstanceDiffusionw.CLIS 48.9(-4.6)
Table1: GenerationresultsofCLIS. Figure4: Consistentwithhumanjudgement.
Table2: CorrelationbetweenCLIS-Iandperformanceimprovementsonbothlong-tailedinstance
segmentationandopen-vocabularyobjectdetectionscenariosofLVISbenchmarks. Thebaseline
is trained on the original training set, while the others are trained on the training and synthetic
set. The annotations of rare categories are not used for open-vocabulary settings. We use Mask
R-CNNR50-FPN(1Xschedule)forlong-tailedinstancesegmentationandGrounding-DINOfor
open-vocabularyobjectdetection.
long-tailed open-vocabulary
Method RangeofCLIS-I
APbox APmask APbox APmask APbox APbox
r r r
baseline N/A 8.9 9.3 22.5 21.7 44.4 57.3
ImageDataFilter 70-75 9.9 10.7 22.8 22.1 42.5 57.4
ImageDataFilter 75-80 10.1 11.5 22.8 22.1 44.2 57.3
ImageDataFilter 80-85 11.2 12.1 23.4 22.6 45.8 57.7
Cross-modalityPerceptionandReasoningTrainingSamples. Wefurtherutilizeatemplateto
constructquestion-answerpairsforinstructionfine-tuningofVLLMs. Instructiondataforperception
isconstructedbasedonthegeneratedimageandscenegraph. Wefocusontwoaspects. 1)Localiza-
tion: Wecreateinstructionsthatrequiremodelstolocalizeanobjectbasedonadetaileddescription.
Conversely,wealsocreateinstructionsthatrequiremodelstodescribeanobject,givenitslocalization.
2)Attribute-binding: Weconstructinstructionsthatquerymodelsregardingattributes(e.g.,color,
count)ofcertainobjects. Additionally,weformulateinstructiondataforreasoning. Relation: We
createinstructionsthatquerymodelsabouttherelationshipbetweendifferentobjects,whichmaybe
spatialoraction-based. Theexpectedresponsesshouldbedescriptiveinnaturallanguage. Please
refertoAppendixA.3fordetailsoftemplatesusedtoconstructquestion-answerpairs.
CLISSetting. WeemployCLIStofilterhigh-qualitytrainingsamplesbysettingindependentscore
thresholdsforlayouts(CLIS-L)andimages(CLIS-I).Notably,layoutsassessedaslow-qualityare
excludedfromgeneratingcorrespondingimages.
4 Experiments
We first validate the proposed CLIS metric from two perspectives: 1) its correlation with image
fidelityofgeneratedsamplesand2)theperformancegainobservedindownstreamtaskswhenusing
CLISasatrainingdatafilter. Next,weproceedtoverifytheeffectivenessoftheACPsystemon
severaldownstreamtasksanddemonstrateitspotentialforcontinuousscalingupofdatasize. We
verifythedesignedmodulesinACPviaaseriesofablationstudies.
4.1 ImplementationDetails
Datasets. WeevaluategenerationqualityusingtheMS-COCO[47]datasetfollowing [11,13,14].
Topreventambiguousboundingboxannotations,wefirstfilterimagestothosecontainingatmost
oneinstanceofasinglecategory. Wethenrandomlysample1181images,eachwithafixedcaption,
fromtheCOCOvalidationset. Fordownstreamtasks,weconductexperimentsofobjectdetection
andinstancesegmentationonMS-COCOandLVISv1.0[21]datasets. Additionally,wecarryout
experimentsonimage-basedvisualquestionanswering(VQA)usingtheMME[17]andGQA[32]
benchmarks. TheMMEPerceptionbenchmarkisawidelyusedbenchmarktoevaluatetheperception
abilitiesofMLLMs. GQAisacomprehensivedatasetthatassessesvisualreasoningabilities.
7Table3: CorrelationbetweenCLIS-Landperformanceimprovementsonmulti-modalperceptionand
reasoningMMEandGQAbenchmarks. Basedonthesamepre-trainedweightoftheLLaVA-v1.5
model,thebaselineisinstructionfine-tunedonoriginaldatafrom[48],whiletheothersadditionally
trainedonsyntheticinstructionfine-tuningsetwithdifferentlayoutscorethresholdforfiltering.
Method ThresholdofCLIS-L MME GQA
baseline N/A 1434.4 58.9
LayoutDataFilter 50 1445.2 59.2
LayoutDataFilter 70 1494.2 59.5
Baselines. For generation models, we primarily select popular controllable diffusion-based text-
to-imagemodels,includeGLIGEN[41],BoxDiff[85]andInstanceDiffusion[80],followingtheir
officialsettings. WealsoincludeStableDiffusion[64]withthev1-5modelweightfromHugging-
face[81]asaT2Imodelbaseline. Fordownstreamtasks,weconsidertwopopularbaselines: Mask
R-CNN[25]andCenterNet2[93]forlong-tailedinstancesegmentation. Foropen-vocabularyobject
detection,weuseGrounding-DINO[51,92],andforVQA,weemployLLaVA-v1.5[48,49]. Please
refertoAppendix A.4forthespecificbaselinesettings.
EvaluationProtocols. Forassessingimage-levelquality,weemploytheFréchetInceptionDistance
(FID)[29]usingtheCOCO2017validationsetasthereferencedataset,computedwiththeInception
V3[72]. Additionally,wecomputeaCLIPscoretoevaluatethealignmentbetweencaptionsand
generatedimages. Forlayoutaccuracy,weusetheYOLOscore[42]toevaluatetheprecisionof
controlexertedbythegeneratedmodelbasedonthederivedlayoutcondition. Weadoptapre-trained
YOLOv8m following [80] and report the standard average precision (AP), which is averaged at
different IoU thresholds (from 0.5 to 0.95) across categories. For instance segmentation, AP is
utilizedastheevaluationmetricforsegmentationandobjectdetection. WealsoreportAPfornovel
andrarecategoriesinopen-vocabularyandlong-tailedscenarios. ForVQAbenchmarks,wereport
theaveragedscoreforMMEandaccuracyforGQA.
4.2 StudyEfficacyofCLIS
GenerationResults. WefirststudytheefficacyofourprocessedCLISbyevaluatingitsperformance
fromaconventionalgenerativeperspectiveanditsalignmentwithhumanjudgment. Asshownin
Table3.3,ourCLISenhancesgenerationoutcomes,evidencedbyadecreaseinFIDscoresbased
onbothGLIGEN[41]andInstanceDiffusion[80]. Thishighlightsitsefficacyandgeneralizability.
InFigure3.3,wepresentimagesgeneratedfromthesamescenegraph. Notably,thequalityofthe
imagesimprovesastheCLISincreases,confirmingitsconsistencywithhumanjudgment.
CorrelationwithPerformanceGainsonDownstreamTasks. Wefurtherevaluatetheefficacyof
ourproposedCLISbyanalyzingthecorrelationbetweenthescoresitassignsandtheperformance
gains in downstream tasks. We evaluate our image filter on visual perception tasks, specifically
long-tailedinstancesegmentationandopen-vocabularyobjectdetection,asshowninTable2. We
samplethesamenumberofgeneratedimages(10K)acrossdifferentscorerangesassignedbyour
image filter in CLIS. Our findings indicate that a relatively higher score assigned by the image
filtercorrelateswithgreaterperformancegaininvisualperceptiontasks,therebydemonstratingthe
effectivenessofCLISinenhancingvisualperceptiontasks.
WeadditionallyevaluatethelayoutfilterfromCLISforcross-modalityperceptionandreasoning
tasksofMLLMsonMMEandGQAbenchmarks. Initially,wegenerateaninstructionfine-tuning
datasetasdescribedinSec.3.3. Wethenfilterthesynthetictrainingsamplesusingdifferentlayout
score thresholds. The quantity of instruction fine-tuning data varies, as a higher score threshold
resultsinfewertrainingsamples. Nonetheless,asshowninTable3,addingsynthetictrainingsamples
enhancesperformancecomparedtobaseline,andahigherscorethresholdyieldsbetterresultson
bothMMEandGQAbenchmarks. Thisvalidatestheeffectivenessofourlayoutfilterinimproving
performanceoncross-modalityperceptionandreasoningtasks.
8Table4:Resultsonvisualperceptiondownstreamtasks.(left):LVISlong-tailedinstancesegmentation
benchmarks. (right): Open-vocabularyobjectdetectionbenchmarks.
Method Backbone APm rask APmask Dataset Method Backbone APb no ox vel APbox
MaskR-CNN[25] ResNet-50 9.3 21.7 Grounding-DINO Swin-T 31.7 48.7
LVIS
w.ACP ResNet-50 14.5(+5.2) 22.8(+1.1) w.ACP Swin-T 33.0(+1.3) 49.2
CenterNet2w.Copy-Paste[20] Swin-B 29.3 39.3 Grounding-DINO Swin-T 60.4 57.1
COCO
w.ACP Swin-B 30.7(+1.4) 39.6(+0.3) w.ACP Swin-T 60.8(+0.4) 56.9
Table 5: (left): ACP boosting the results on multi-modal MME and GQA benchmarks. (right):
ComparedwithX-PastedatagenerationmethodsonLVISbenchmark.
Method LMBackbone MME GQA Method Backbone APmask APmask
r
LLaVA-1.5 Vicuna-7B 1434.4 58.9
LLaVA-1.5 Vicuna-13B 1438.3 60.7 CenterNet2(baseline) ResNet-50 17.8 26.1
LLaVA-1.5 LLama-3-8B 1445.3 60.1 w.X-Paste[91] ResNet-50 17.9 28.0
w.ACP ResNet-50 19.2 28.0
LLaVA-1.5w.ACP Vicuna-7B 1514.5(+80.1) 59.3(+0.4)
4.3 SyntheticDatasetScaleUp
We conduct experiments to verify the generated data on visual perception tasks, including long-
tailedinstancesegmentationandopen-vocabularyobjectdetection. Wealsoexplorecross-modality
perceptionandreasoningtasks,i.e.,visualquestion-answeringtasks.
Long-tailedInstanceSegmentationBenchmark. Table4(left)presentsourresultsontheLVIS
long-tailedinstancesegmentationbenchmark. WeutilizedAutoCherry-Pickertoconstructatotalof
50Ktrainingsampleswithdetaileddescriptionsandcorrespondingannotations. ACPdemonstrates
significantperformancegainsoverthecommonly-usedMaskR-CNNbaseline,withanimprovement
of1.1%inAPmask andthemostnotableimprovementinrarecategories(+5.2%APmask). Wealso
r
observed consistent performance improvements with a stronger CenterNet2 baseline, employing
Swin-Basthebackboneandcopy-paste[20]fordataaugmentation,achievinga1.4%higherAPmask.
r
ThisunderscoresthestronggeneralizationabilityofACPacrossdifferentdetectorarchitecturesand
itseffectivenessinconjunctionwithexistingdataaugmentationmethods.
Open-vocabularyObjectDetectionBenchmark. Wefurtherdemonstratetheeffectivenessofthe
ACPinthechallengingopen-vocabularyobjectdetectionsetting. WeutilizeGrounding-DINO[51],
pre-trainedonObjects365[69],GoldG[36],GRIT[58],andV3Det[78],following [92]. Theresults
areshowninTable4(right). ACPstilloutperformsGrounding-DINOby1.3%inLVISAPbox and
novel
0.4%inCOCOAPbox ,despiteusingonlyanadditional50Kgeneratedtrainingsamples. Thisis
novel
notableconsideringtheGrounding-DINObaselineispre-trainedon61.8Mrealimages(30epochs×
128batchsize×16102iterations). Thisvalidateshowhigh-qualitysynthesisdatacancomplement
realdataeffectively.
Multi-modalImage-basedBenchmarks. WefurtherevaluatetheeffectivenessofACPoncross-
modalityperceptionandreasoningtasks. WeadoptLLaVA-v1.5withVicuna-7Basourbaseline.
Table 5 (left) indicates that ACP significantly enhances model perception ability on the MME
benchmark,achievingan80.1improvement,whichexceedstheperformanceofLLaVA-v1.5even
withstrongerlanguagemodelbackbonessuchasVicuna-13BandLLama-3-8B.Additionally,ACP
improves performance on the widely recognized GQA reasoning benchmark. This validates the
effectivenessofourmethodincross-modalitysettings.
ComparisonwithPreviousMethods. WecomparequantitativelywithX-Paste[91],usingCen-
terNet2withResNet-50backboneand1×trainingschedule. ItshouldbenotedthatwhileX-Paste
generates100Kimages,weutilizeonly50Kgeneratedimages. Additionally,toensureafaircompar-
ison,bothmethodsaretrainedonthesamenumberofrealimages,i.e.,45Kiterationswithabatch
sizeof64forX-Pasteand33.75Kiterationswithabatchsizeof64forACP.ResultsinTable5(right)
showa1.3%higherinAPmask,demonstratingthesuperiorityinsynthesizingimageswithreasonable
r
layouts,asopposedtocomposingtrainingsamplesbypastingmultiplesynthesizedinstancesontoa
background.
9Table 6: Ablation study on CLIS-I. We compare three variants of CLIS-I by filtering the same
generatedrawdataset. WereportFID,CLIP,andYOLOscoresforgenerationmetrics. WereportAP,
AP ,andAP ontheCOCOdetectionbenchmark.
50 75
Model VisualQuality Alignment FID↓ CLIPscore↑ YOLOscore↑ AP↑ AP50↑ AP75↑
InstanceDiffusion[80] (cid:37) (cid:37) 53.5 25.2 45.6 37.5 58.3 40.8
CLIS-I (cid:33) (cid:37) 48.4 25.6 46.0 37.3 58.3 40.5
CLIS-I (cid:37) (cid:33) 47.8 27.7 48.3 37.5 58.3 40.7
CLIS-I (cid:33) (cid:33) 48.9 25.8 47.9 37.7 58.5 40.9
4.4 AblationandAnalysis
WeconductanablationstudyonCLIS-IusingInstanceDiffusion[80]asourbaseline. ForCLIP-I
withthevisualqualitysetting,weexclusivelyemployamulti-modalcaptionmodeltoidentifyobjects.
Foralignment,wesimplifyCLIS-ItoCLIPscore. Basedonthesamerawdataset,weevaluatethe
filtereddata(1Ksamples)usingthesemethodsfrombothagenerationperspectiveandperformance
improvements in the downstream task. In particular, we use the COCO detection benchmark as
thedownstreamtask,combiningthefilteredsamplesandtheoriginaltrainingdatatotrainaFaster
R-CNNdetectoronastandard1×trainingschedule. Table6showsthatCLIS-Idemonstratesthe
mostsignificantperformancegaininthedownstreamtask,aligningwithourinitialmotivation. While
CLIS-I,focusingsolelyonalignment,excelsingenerationevaluation,itproducessuboptimalresults
inthedownstreamtask. ThisfindingemphasizesthatourproposedCLIShasastrongercorrelation
withdownstreamtaskperformancegainscomparedtoconventionalgenerationmetrics,suchasFID,
CLIPscore,andYOLOscore.
5 Conclusion
Inthispaper,weproposeAutoCherry-Picker,across-modalitytrainingdatageneratorconditioned
onasimpleobjectlistwithanoveldesignedCLISmetrictoensurethequalityofgenerateddata.
AutoCherry-Pickeriseffectiveinvariousdownstreamtasks,includingperceptionandreasoning
tasks,particularlyinimprovingtheperformanceinannotation-scarcescenarios. OurproposedCLIS
canbeusedtopickhigh-qualitygenerationexamples,wherewealsofindthegenerateddatawith
betterCLIPscorescanleadtobetterperformanceforperceptiontasks. Moreover,ourmethodcan
beeasilyadaptedtostrongerLLMsandimagegenerationmodels. Ourresearchbridgesthegap
betweenhigh-qualitygenerationdataanddownstreamperformance. Wehopeourresultscaninspire
generationmetricdesigninthefuture.
References
[1] JinzeBai,ShuaiBai,ShushengYang,ShijieWang,SinanTan,PengWang,JunyangLin,ChangZhou,and
JingrenZhou. Qwen-vl:Aversatilevision-languagemodelforunderstanding,localization,textreading,
andbeyond. arXivpreprintarXiv:2308.12966,2023. 6
[2] YogeshBalaji,SeungjunNah,XunHuang,ArashVahdat,JiamingSong,QinshengZhang,KarstenKreis,
MiikaAittala,TimoAila,SamuliLaine,etal. ediff-i:Text-to-imagediffusionmodelswithanensembleof
expertdenoisers. arXivpreprintarXiv:2211.01324,2022. 3
[3] OmerBar-Tal, LiorYariv, YaronLipman, andTaliDekel. Multidiffusion: Fusingdiffusionpathsfor
controlledimagegeneration. arXivpreprintarXiv:2302.08113,2023. 3
[4] TimBrooks,AleksanderHolynski,andAlexeiAEfros. Instructpix2pix:Learningtofollowimageediting
instructions. InCVPR,2023. 1
[5] TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhariwal,Arvind
Neelakantan,PranavShyam,GirishSastry,AmandaAskell,etal. Languagemodelsarefew-shotlearners.
InNeurIPS,2020. 5
[6] Lucy Chai, Jun-Yan Zhu, Eli Shechtman, Phillip Isola, and Richard Zhang. Ensembling with deep
generativeviews. InCVPR,2021. 4
[7] HilaChefer,YuvalAlaluf,YaelVinker,LiorWolf,andDanielCohen-Or. Attend-and-excite:Attention-
basedsemanticguidancefortext-to-imagediffusionmodels. TOG,2023. 4
10[8] KaiChen,EnzeXie,ZheChen,YiboWang,LanqingHong,ZhenguoLi,andDit-YanYeung.Geodiffusion:
Text-promptedgeometriccon-trolforobjectdetectiondatageneration. arXivpreprintarXiv:2306.04607,
2023. 1,2,4
[9] MinghaoChen,IroLaina,andAndreaVedaldi. Training-freelayoutcontrolwithcross-attentionguidance.
arXivpreprintarXiv:2304.03373,2023. 3
[10] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastivelearningofvisualrepresentations. InICML,2020. 4
[11] XiaohuiChen,YongfeiLiu,YingxiangYang,JianboYuan,QuanzengYou,Li-PingLiu,andHongxiaYang.
Reasonoutyourlayout:Evokingthelayoutmasterfromlargelanguagemodelsfortext-to-imagesynthesis.
arXivpreprintarXiv:2311.17126,2023. 3,7
[12] Wei-LinChiang,ZhuohanLi,ZiLin,YingSheng,ZhanghaoWu,HaoZhang,LianminZheng,Siyuan
Zhuang,YonghaoZhuang,JosephE.Gonzalez,IonStoica,andEricP.Xing. Vicuna: Anopen-source
chatbotimpressinggpt-4with90%*chatgptquality,March2023. 17
[13] G.Couairon,M.Careil,M.Cord,S.Lathuiliere,andJ.Verbeek. Zero-shotspatiallayoutconditioningfor
text-to-imagediffusionmodels. InICCV,2023. 3,7
[14] RunpeiDong,ChunruiHan,YuangPeng,ZekunQi,ZhengGe,JinrongYang,LiangZhao,JianjianSun,
HongyuZhou,HaoranWei,etal. Dreamllm:Synergisticmultimodalcomprehensionandcreation. arXiv
preprintarXiv:2309.11499,2023. 7
[15] PatrickEsser,RobinRombach,andBjornOmmer.Tamingtransformersforhigh-resolutionimagesynthesis.
InCVPR,2021. 1
[16] WeixiFeng,WanrongZhu,Tsu-juiFu,VarunJampani,ArjunAkula,XuehaiHe,SugatoBasu,XinEric
Wang,andWilliamYangWang. Layoutgpt: Compositionalvisualplanningandgenerationwithlarge
languagemodels. InNeurIPS,2024. 3
[17] ChaoyouFu,PeixianChen,YunhangShen,YuleiQin,MengdanZhang,XuLin,JinruiYang,XiawuZheng,
KeLi,XingSun,YunshengWu,andRongrongJi. Mme: Acomprehensiveevaluationbenchmarkfor
multimodallargelanguagemodels. arXivpreprintarXiv:2306.13394,2024. 7
[18] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel
Cohen-Or. Animageisworthoneword:Personalizingtext-to-imagegenerationusingtextualinversion.
arXivpreprintarXiv:2208.01618,2022. 1
[19] HananGani,ShariqFarooqBhat,MuzammalNaseer,SalmanKhan,andPeterWonka. Llmblueprint:
Enablingtext-to-imagegenerationwithcomplexanddetailedprompts. arXivpreprintarXiv:2310.10640,
2023. 3
[20] GolnazGhiasi,YinCui,AravindSrinivas,RuiQian,Tsung-YiLin,EkinDCubuk,QuocVLe,andBarret
Zoph. Simplecopy-pasteisastrongdataaugmentationmethodforinstancesegmentation. InCVPR,2021.
9
[21] AgrimGupta,PiotrDollar,andRossGirshick. Lvis:Adatasetforlargevocabularyinstancesegmentation.
InCVPR,2019. 3,7,16
[22] YueHan,JiangningZhang,JunweiZhu,XiangtaiLi,YanhaoGe,WeiLi,ChengjieWang,YongLiu,
XiaomingLiu,andYingTai. Ageneralistfacexvialearningunifiedfacialrepresentation. arXivpreprint
arXiv:2401.00551,2023. 1
[23] YueHan,JunweiZhu,KekeHe,XuChen,YanhaoGe,WeiLi,XiangtaiLi,JiangningZhang,Chengjie
Wang,andYongLiu. Faceadapterforpre-traineddiffusionmodelswithfine-grainedidandattribute
control. arXivpreprintarXiv:2405.12970,2024. 1
[24] WilliamHarvey, SaeidNaderiparizi, VadenMasrani, ChristianWeilbach, andFrankWood. Flexible
diffusionmodelingoflongvideos. InNeurIPS,2022. 1
[25] KaimingHe,GeorgiaGkioxari,PiotrDollár,andRossGirshick. Maskr-cnn. InICCV,2017. 3,8,9
[26] KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun. Deepresiduallearningforimagerecognition.
InCVPR,2016. 16
[27] AmirHertz,RonMokady,JayTenenbaum,KfirAberman,YaelPritch,andDanielCohen-Or. Prompt-to-
promptimageeditingwithcrossattentioncontrol. arXivpreprintarXiv:2208.01626,2022. 1
[28] JackHessel,AriHoltzman,MaxwellForbes,RonanLeBras,andYejinChoi. Clipscore:Areference-free
evaluationmetricforimagecaptioning. arXivpreprintarXiv:2104.08718,2021. 2,4
[29] MartinHeusel,HubertRamsauer,ThomasUnterthiner,BernhardNessler,andSeppHochreiter. Gans
trainedbyatwotime-scaleupdateruleconvergetoalocalnashequilibrium. InNeurIPS,2017. 4,8
[30] JonathanHo, WilliamChan, ChitwanSaharia, JayWhang, RuiqiGao, AlexeyGritsenko, DiederikP
Kingma, BenPoole, MohammadNorouzi, DavidJFleet, etal. Imagenvideo: Highdefinitionvideo
generationwithdiffusionmodels. arXivpreprintarXiv:2210.02303,2022. 1
[31] JonathanHo,TimSalimans,AlexeyGritsenko,WilliamChan,MohammadNorouzi,andDavidJFleet.
Videodiffusionmodels. InNeurIPS,2022. 1
[32] DrewAHudsonandChristopherDManning. Gqa: Anewdatasetforreal-worldvisualreasoningand
compositionalquestionanswering. InCVPR,2019. 7
[33] NaotoInoue,KotaroKikuchi,EdgarSimo-Serra,MayuOtani,andKotaYamaguchi. Layoutdm:Discrete
diffusionmodelforcontrollablelayoutgeneration. InCVPR,2023. 4
11[34] PhillipIsola,Jun-YanZhu,TinghuiZhou,andAlexeiAEfros.Image-to-imagetranslationwithconditional
adversarialnetworks. InCVPR,2017. 1
[35] AliJahanian, XavierPuig, YonglongTian, andPhillipIsola. Generativemodelsasadatasourcefor
multiviewrepresentationlearning. arXivpreprintarXiv:2106.05258,2021. 4
[36] AishwaryaKamath,MannatSingh,YannLeCun,GabrielSynnaeve,IshanMisra,andNicolasCarion.
Mdetr-modulateddetectionforend-to-endmulti-modalunderstanding. InICCV,2021. 9,17
[37] LaurynasKarazija,IroLaina,AndreaVedaldi,andChristianRupprecht. Diffusionmodelsforzero-shot
open-vocabularysegmentation. arXivpreprintarXiv:2306.09316,2023. 1
[38] DaiqingLi,HuanLing,AmlanKar,DavidAcuna,SeungWookKim,KarstenKreis,AntonioTorralba,and
SanjaFidler. Dreamteacher:Pretrainingimagebackboneswithdeepgenerativemodels. InICCV,2023. 1,
4
[39] JunnanLi,DongxuLi,CaimingXiong,andStevenHoi. Blip:Bootstrappinglanguage-imagepre-training
forunifiedvision-languageunderstandingandgeneration. InICML,2022. 4
[40] XiaojieLi,YiboYang,XiangtaiLi,JianlongWu,YueYu,BernardGhanem,andMinZhang. Genview:
Enhancingviewqualitywithpretrainedgenerativemodelforself-supervisedlearning. InarXivpreprint
arXiv:2403.12003,2024. 4
[41] YuhengLi,HaotianLiu,QingyangWu,FangzhouMu,JianweiYang,JianfengGao,ChunyuanLi,and
YongJaeLee. Gligen:Open-setgroundedtext-to-imagegeneration. InCVPR,2023. 3,7,8
[42] ZejianLi,JingyuWu,ImmanuelKoh,YongchuanTang,andLingyunSun. Imagesynthesisfromlayout
withlocality-awaremaskadaption. InICCV,2021. 8
[43] ZiyiLi,QinyeZhou,XiaoyunZhang,YaZhang,YanfengWang,andWeidiXie. Open-vocabularyobject
segmentationwithdiffusionmodels. InICCV,2023. 1,4
[44] LongLian,BoyiLi,AdamYala,andTrevorDarrell. Llm-groundeddiffusion:Enhancingpromptunder-
standingoftext-to-imagediffusionmodelswithlargelanguagemodels. arXivpreprintarXiv:2305.13655,
2023. 3
[45] Long Lian, Boyi Li, Adam Yala, and Trevor Darrell. LLM-grounded diffusion: Enhancing prompt
understandingoftext-to-imagediffusionmodelswithlargelanguagemodels. TMLR,2024. 2,3
[46] ShaoboLin,KunWang,XingyuZeng,andRuiZhao. Explorethepowerofsyntheticdataonfew-shot
objectdetection. InCVPR,2023. 1
[47] Tsung-YiLin,MichaelMaire,SergeBelongie,JamesHays,PietroPerona,DevaRamanan,PiotrDollár,
andCLawrenceZitnick. Microsoftcoco:Commonobjectsincontext. InECCV,2014. 7
[48] HaotianLiu,ChunyuanLi,YuhengLi,andYongJaeLee. Improvedbaselineswithvisualinstruction
tuning. arXivpreprintarXiv:2310.03744,2023. 8,17
[49] HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee. Visualinstructiontuning. InNeurIPS,2024.
3,8
[50] JialunLiu,YifanSun,ChuchuHan,ZhaopengDou,andWenhuiLi. Deeprepresentationlearningon
long-taileddata:Alearnableembeddingaugmentationperspective. InCVPR,2020. 16
[51] ShilongLiu,ZhaoyangZeng,TianheRen,FengLi,HaoZhang,JieYang,ChunyuanLi,JianweiYang,
HangSu,JunZhu,etal. Groundingdino:Marryingdinowithgroundedpre-trainingforopen-setobject
detection. arXivpreprintarXiv:2303.05499,2023. 3,8,9
[52] ShuangtingLiu,JiaqiZhang,YuxinChen,YifanLiu,ZengchangQin,andTaoWan. Pixelleveldata
augmentationforsemanticimagesegmentationusinggenerativeadversarialnetworks. InICASSP,2019.
2,4
[53] ZeLiu,YutongLin,YueCao,HanHu,YixuanWei,ZhengZhang,StephenLin,andBainingGuo. Swin
transformer:Hierarchicalvisiontransformerusingshiftedwindows. InICCV,2021. 17
[54] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint
arXiv:1711.05101,2017. 17
[55] ChenlinMeng,YutongHe,YangSong,JiamingSong,JiajunWu,Jun-YanZhu,andStefanoErmon.Sdedit:
Guidedimagesynthesisandeditingwithstochasticdifferentialequations.arXivpreprintarXiv:2108.01073,
2021. 1
[56] AlexNichol,PrafullaDhariwal,AdityaRamesh,PranavShyam,PamelaMishkin,BobMcGrew,Ilya
Sutskever,andMarkChen. Glide:Towardsphotorealisticimagegenerationandeditingwithtext-guided
diffusionmodels. arXivpreprintarXiv:2112.10741,2021. 3
[57] TaesungPark,Ming-YuLiu,Ting-ChunWang,andJun-YanZhu. Semanticimagesynthesiswithspatially-
adaptivenormalization. InCVPR,2019. 1
[58] ZhiliangPeng,WenhuiWang,LiDong,YaruHao,ShaohanHuang,ShumingMa,andFuruWei.Kosmos-2:
Groundingmultimodallargelanguagemodelstotheworld. arXivpreprintarXiv:2306.14824,2023. 9,17
[59] BryanAPlummer,LiweiWang,ChrisMCervantes,JuanCCaicedo,JuliaHockenmaier,andSvetlana
Lazebnik. Flickr30kentities:Collectingregion-to-phrasecorrespondencesforricherimage-to-sentence
models. InICCV,2015. 5
[60] Lu Qi, Yi-Wen Chen, Lehan Yang, Tiancheng Shen, Xiangtai Li, Weidong Guo, Yu Xu, and Ming-
HsuanYang. Generalizableentitygroundingviaassistanceoflargelanguagemodel. arXivpreprint
arXiv:2402.02555,2024. 3
12[61] JieQin,JieWu,WeifengChen,YuxiRen,HuixiaLi,HefengWu,XuefengXiao,RuiWang,andShilei
Wen. Diffusiongpt:Llm-driventext-to-imagegenerationsystem. arXivpreprintarXiv:2401.10061,2024.
3
[62] LeigangQu,ShengqiongWu,HaoFei,LiqiangNie,andTat-SengChua. Layoutllm-t2i:Elicitinglayout
guidancefromllmfortext-to-imagegeneration. InACMMM,2023. 3
[63] AdityaRamesh,PrafullaDhariwal,AlexNichol,CaseyChu,andMarkChen.Hierarchicaltext-conditional
imagegenerationwithcliplatents. arXivpreprintarXiv:2204.06125,2022. 1,3
[64] RobinRombach,AndreasBlattmann,DominikLorenz,PatrickEsser,andBjörnOmmer. High-resolution
imagesynthesiswithlatentdiffusionmodels. InCVPR,2022. 1,3,7,8
[65] NatanielRuiz,YuanzhenLi,VarunJampani,YaelPritch,MichaelRubinstein,andKfirAberman. Dream-
booth:Finetuningtext-to-imagediffusionmodelsforsubject-drivengeneration. InCVPR,pages22500–
22510,2023. 1
[66] ChitwanSaharia,WilliamChan,HuiwenChang,ChrisLee,JonathanHo,TimSalimans,DavidFleet,and
MohammadNorouzi. Palette:Image-to-imagediffusionmodels. InSIGGRAPH,2022. 1
[67] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar
Ghasemipour,RaphaelGontijoLopes,BurcuKaragolAyan,TimSalimans,etal. Photorealistictext-to-
imagediffusionmodelswithdeeplanguageunderstanding,2022. 1,3
[68] TimSalimans,IanGoodfellow,WojciechZaremba,VickiCheung,AlecRadford,andXiChen. Improved
techniquesfortraininggans. InNeurIPS,2016. 4
[69] ShuaiShao,ZemingLi,TianyuanZhang,ChaoPeng,GangYu,XiangyuZhang,JingLi,andJianSun.
Objects365:Alarge-scale,high-qualitydatasetforobjectdetection. InICCV,2019. 9,17
[70] JaskiratSingh,StephenGould,andLiangZheng.High-fidelityguidedimagesynthesiswithlatentdiffusion
models. InCVPR,2023. 3
[71] AbhishekSinha,JiamingSong,ChenlinMeng,andStefanoErmon. D2c:Diffusion-decodingmodelsfor
few-shotconditionalgeneration. InNeurIPS,2021. 1
[72] ChristianSzegedy,VincentVanhoucke,SergeyIoffe,JonShlens,andZbigniewWojna. Rethinkingthe
inceptionarchitectureforcomputervision. InCVPR,2016. 8
[73] WeiminTan,SiyuanChen,andBoYan. Diffss: Diffusionmodelforfew-shotsemanticsegmentation.
arXivpreprintarXiv:2307.00773,2023. 2,4
[74] YonglongTian,LijieFan,KaifengChen,DinaKatabi,DilipKrishnan,andPhillipIsola. Learningvision
frommodelsrivalslearningvisionfromdata. arXivpreprintarXiv:2312.17742,2023. 5
[75] YonglongTian,LijieFan,PhillipIsola,HuiwenChang,andDilipKrishnan. Stablerep:Syntheticimages
fromtext-to-imagemodelsmakestrongvisualrepresentationlearners. InNeurIPS,2024. 1,4,5
[76] HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,TimothéeLacroix,
BaptisteRozière,NamanGoyal,EricHambro,FaisalAzhar,AurelienRodriguez,ArmandJoulin,Edouard
Grave,andGuillaumeLample. Llama:Openandefficientfoundationlanguagemodels,2023. 17
[77] ChaoyangWang,XiangtaiLi,LuQi,HenghuiDing,YunhaiTong,andMing-HsuanYang. Semflow:
Bindingsemanticsegmentationandimagesynthesisviarectifiedflow. arXivpreprintarXiv:2405.20282,
2024. 1
[78] JiaqiWang,PanZhang,TaoChu,YuhangCao,YujieZhou,TongWu,BinWang,ConghuiHe,andDahua
Lin. V3det:Vastvocabularyvisualdetectiondataset. InICCV,2023. 9,17
[79] QiWang,JunyuGao,WeiLin,andYuanYuan. Learningfromsyntheticdataforcrowdcountinginthe
wild. InCVPR,2019. 4
[80] XudongWang,TrevorDarrell,SaiSakethRambhatla,RohitGirdhar,andIshanMisra. Instancediffusion:
Instance-levelcontrolforimagegeneration. arXivpreprintarXiv:2402.03290,2024. 2,7,8,10
[81] ThomasWolf,LysandreDebut,VictorSanh,JulienChaumond,ClementDelangue,AnthonyMoi,Pierric
Cistac,TimRault,RémiLouf,MorganFuntowicz,etal. Huggingface’stransformers: State-of-the-art
naturallanguageprocessing. arXivpreprintarXiv:1910.03771,2019. 8
[82] Erroll Wood, Tadas Baltrušaitis, Charlie Hewitt, Sebastian Dziadzio, Thomas J Cashman, and Jamie
Shotton. Fakeittillyoumakeit:faceanalysisinthewildusingsyntheticdataalone. InICCV,2021. 4
[83] WeijiaWu,YuzhongZhao,HaoChen,YuchaoGu,RuiZhao,YefeiHe,HongZhou,MikeZhengShou,
andChunhuaShen. Datasetdm: Synthesizingdatawithperceptionannotationsusingdiffusionmodels.
NeurIPS,2023. 4
[84] JiahaoXie,WeiLi,XiangtaiLi,ZiweiLiu,YewSoonOng,andChenChangeLoy.Mosaicfusion:Diffusion
modelsasdataaugmentersforlargevocabularyinstancesegmentation. arXivpreprintarXiv:2309.13042,
2023. 1
[85] JinhengXie,YuexiangLi,YawenHuang,HaozheLiu,WentianZhang,YefengZheng,andMikeZheng
Shou. Boxdiff:Text-to-imagesynthesiswithtraining-freebox-constraineddiffusion. InICCV,2023. 3,7,
8
[86] ZhengyuanYang,JianfengWang,ZheGan,LinjieLi,KevinLin,ChenfeiWu,NanDuan,ZichengLiu,Ce
Liu,MichaelZeng,andLijuanWang. Reco:Region-controlledtext-to-imagegeneration. InCVPR,2023.
3
13[87] HanrongYe, JasonKuen, QingLiu, ZheLin, BrianPrice, andDanXu. Seggen: Superchargingseg-
mentation models with text2mask and mask2img synthesis. arXiv preprint arXiv:2311.03355, 2023.
2
[88] LvminZhang,AnyiRao,andManeeshAgrawala. Addingconditionalcontroltotext-to-imagediffusion
models,2023. 3
[89] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable
effectivenessofdeepfeaturesasaperceptualmetric. InCVPR,2018. 4
[90] TianjunZhang,YiZhang,VibhavVineet,NeelJoshi,andXinWang.Controllabletext-to-imagegeneration
withgpt-4. arXivpreprintarXiv:2305.18583,2023. 3
[91] HanqingZhao,DianmoSheng,JianminBao,DongdongChen,DongChen,FangWen,LuYuan,CeLiu,
WenboZhou,QiChu,etal. X-paste:Revisitingscalablecopy-pasteforinstancesegmentationusingclip
andstablediffusion. arXivpreprintarXiv:2212.03863,2022. 9,16
[92] Xiangyu Zhao, Yicheng Chen, Shilin Xu, Xiangtai Li, Xinjiang Wang, Yining Li, and Haian Huang.
An open and comprehensive pipeline for unified object grounding and detection. arXiv preprint
arXiv:2401.02361,2024. 8,9,17
[93] XingyiZhou,VladlenKoltun,andPhilippKrähenbühl. Probabilistictwo-stagedetection. arXivpreprint
arXiv:2103.07461,2021. 8
14A ImplementationDetails
A.1 LLMsforACPPipeline
WeconductedexperimentsusingaseriesofLLMsasthescenegraphgeneratorinourACPpipeline
onalimiteddatascale. Specifically,weemployQwen-1.5-14B,Qwen-1.5-72B,andQwen-1.5-110B
togeneratescenegraphs. Eachmodelproduced15Ktrainingsamplesfromthesameinputobjectlist.
ThesesamplesarethenamalgamatedwiththeoriginaltrainingdataforCOCOdetectiontasks. We
applythemseparatelytoaMaskR-CNNbaselineunderastandard1×trainingschedule. Table7
illustrates that the performance of different LLMs is comparable in downstream tasks. For the
experimentsdescribed,weoptforthesmallerLLM,Qwen-1.5-14B,duetoitsfasterinferencespeed.
A.2 PromptsinACP
WeprovideourfullpromptintheACPpipelinewiththedescriptiongeneratorandlayoutgenerator.
PromptforDescriptionGenerator:
TaskDescription:
Yourtaskistogenerateadetaileddescriptionbasedonanobjectlist. Thedescription
should be a structured representation of a scene detailing its various elements and
theirrelationships. Thedescriptionconsistsof: 1. attributesofobjects: Theattributes
should be descriptive color or texture of the corresponding object. 2. Groups: A
groupofobjectsexhibitstrongspatialrelationshipsthatinteractwitheachother. 3.
Relationships: Thissectionillustratestheinteractionsorspatialrelationshipsbetween
variousobjectsorgroups. 4. Caption: Captionshouldbeasimpleandstraightforward
2-4sentenceimagecaption. Pleaseincludealltheobjectsinthecaptionandreferto
them in ’()’. Create the caption as if you are directly observing the image. Do not
mentiontheuseofanysourcedata. Donotusewordslike’indicate’,’suggest’,’hint’,
’likely’,or’possibly’.
Youcanrefertothefollowingexamplesasreferences.
In-contextlearningexamplesforDescriptionGenerator
Please provide a json format with description based on the following object
list.
PromptforLayoutGenerator:
TaskDescription:
Yourtaskistogeneratealayoutbasedonadetaileddescription. Thelayoutisalistof
jsonwith’object’and’bbox’. ’object’referstotheobjectnameinthepromptprovided,
while’bbox’isformulatedas[x,y,w,h],where"x,y"denotesthetopleftcoordinateof
theboundingbox. "w"denotesthewidth,and"h"denotestheheight. Thebounding
boxesshouldnotgobeyondtheimageboundaries. Thesixvalues"x,y,w,h,x+w,y+h"
arealllargerthan0andsmallerthan1.
Youcanrefertothefollowingexamplesasreferences.
In-contextlearningexamplesforLayoutGenerator
PleaseprovideajsonformatwithLayoutbasedonthefollowingprompt.
A.3 TemplatesforMulti-modalTasks
We provide templates for constructing question-answer pairs for multi-modal downstream tasks.
Forperception,wedesigntwotypesoftasks: localizationandattribute-binding. Localizationtasks
necessitatethatmodelspinpointanobjectdetailedintheinstructionsor,alternatively,describean
object situated at a specific location. Attribute-binding tasks require models to identify precise
15Table7: DifferentLLMsasscenegraphgeneratorinACPonCOCOdetectiontask.
SceneGraphGenerator APmask↑ APbox↑
Qwen1.5-14b 34.5 37.8
Qwen1.5-72b 34.5 37.8
Qwen1.5-110b 34.2 37.7
attributesofanobjectwithinagivenlocation. Forreasoning,wecraftrelationreasoningtasks. These
tasksrequiremodelstodeducetherelationshipbetweenaspecifiedsubjectandobjectbasedonthe
provideddescription.
Localization:
Question:
1. Where is the object described {attribute} located in the image in terms of the
boundingbox?
2. Whatisthelocationofobjectdescribed{attribute}intermsoftheboundingbox?
3. Localizetheobjectdescribed{attribute}intermsofboundingbox.
4. Provideaboundingboxfortheobjectdescribed{attribute}.
5. Generateaboundingboxfortheobjectdescribed{attribute}.
6. Describetheobjectlocatedat{layout}.
7. Provideacaptionfortheobjectat{layout}.
8. Whatisatlocation{layout}inimage?
Answer:
1-5: Itislocatedat{layout}.
6-8: Thereisa{attribute}.
Attribute-binding:
Question:
1. Whatisthecolorof{obj}?
2. Whatcoloristhe{obj}?
3. Whatcolordoyouthinkthe{obj}is?
4. Whichcoloristhe{obj}?
5. Whatisthenumberof{obj}?
6. Whatisthetotalcountof{obj}intheimage?
Answer:
1-4: {color}.
5-6: {number}.
Relation:
Question:
What is the relationship between the subject described {attribute1} and the object
described{attribute2}?
Answer:
{subject}{relation}{object}.
A.4 BaselineSettings
Ourspecificbaselinesettingsinexperimentsareasfollows:
• Mask R-CNN baseline. We follow the same setup outlined in [21]. Specifically, we adopt
ResNet-50[26]withFPN[50]backbone,usingthestandard1×trainingschedule.
• CenterNet2baseline. Wefollowthesetupoutlinedin[91]. Specifically,weusetwoconfigurations:
1)ResNet-50witha1×trainingschedule,and2)Swin-Bwitha4×trainingschedule. Weemploy
theAdamWoptimizerandutilizerepeatfactorsamplingwithanoversamplethresholdof10−3.
16Figure5: ComparisonbetweenCLISandotherprevalentmetrics. Eachpairofimagesisgenerated
onthesamescenegraph,withourCLISmetricfavoringtherightimageineachpair. In(a)and(b),
theCLIPscoreoverlookstheextraneouselephantontheleftandtheinaccuratespatialarrangement
betweenthechairandbed,respectively. For(c)and(d),theYOLOscorefailstoassessthedetailed
attributesorevaluatethesemanticrelationshipsbetweenobjects.
• Grounding-DINObaseline. Wefollowthesetupoutlinedin [92]. Specially,weusethemodel
pretrainedonObjects365[69], GoldG[36], GRIT[58], andV3Det[78]withSwin-T[53]asthe
backbone. Thefine-tuningprocessusesthestandard1×trainingschedule. WeuseAdamW[54]
optimizerwithaweightdecayof0.0001. Theinitiallearningrateis0.00005,droppedby10×atthe
8thand11thepochs.
• LLaVA-v1.5baseline. Wefollowthesetupoutlinedin [48]. Weadoptatwo-stagetrainingprocess.
FortheLLMbackbone,weadoptVicuna-7B[12],Vicuna-13B,andLLama-3-8B[76]. Weusean
AdamWoptimizerwithaweightdecayof0. Pre-trainingfor1epochwitha1e-3learningrateand
batchsizeof32,andfine-tuningfor1epochwitha2e-5learningrateandabatchsizeof16. The
warmupratioofthelearningrateis0.03.
B LimitationsandFutureWork
WhileACPgeneratesdatasamplesleveragingthecapabilitiesoflarge-scalepretrainedgenerative
models,italsoinheritstheirlimitations. Thisresultsincertaincompromiseswhencomparedtoreal
data. Futureimprovementscouldpotentiallycomefromintegratingmoreadvancedmodels,suchas
GPT-4. Additionally,thecomputationofCLIS-Lnecessitatesapoolofreal-datalayoutexamples,
whichcanberesource-intensive. Currently,ourlayoutexamplepoolisconstructedfromFlickr30K.
However,practicallimitationsincomputationalresourcesrestrictourcapacitytobuildandevaluatea
largerlayoutexamplepool. Weencouragemorefuturestudiesfocusingonthedesignofgeneration
metrics.
C MoreResultsofComparisonwithOtherMetrics
WeprovidevisualresultscomparingourCLISwithothermetrics. Usingthesamescenegraphfrom
ourpreviousgenerator,weproduceimagesevaluatedwithourCLISandothermetrics,suchasCLIP
and YOLO scores. As illustrated in Fig 5, our CLIS demonstrates superior performance in both
textualalignmentandvisualquality.
17Figure6: SynthetictrainingexamplesfromACP.Insettingswithimbalancedtrainingdata,suchas
long-tailscenarios,ACPcanproducehigh-qualitytrainingexamplesforrarecategoriestomitigate
thischallenge. Additionally,ACPcangeneratediversetrainingsampleswithdetailedattributesand
relationshipswithincomplexscenes.
D VisualizationofSyntheticTrainingSamples
Additionally,weshowcasevisualizationsofoursynthetictrainingsamplesinFig.6andFig.7. By
leveragingtheextensivevocabularyoflargegenerativemodels,wecanproducehigh-qualitytraining
samplesforrarecategories. Thesetrainingsamplesarecloselyalignedwiththeirrespectivescene
graphs,capturingbothdetailedattributedescriptionsandcomplexrelationshipsbetweenmultiple
objectseffectively.
18Task
Annotations:
[‘ferret’] x 2
[‘ferret’: <segmentation
mask>] x 2
Annotations:
A-1 [‘horse’] x 4
[‘horse’: <segmentation
mask>] x 4
Annotations:
[‘suitcase’] x 6 [‘suitcase’:
<segmentation mask>] x 6
Annotations:
[‘urinal’] x 2
[‘urinal’: <bounding
box>] x 2
Annotations:
A-2 [‘zebra’] x 2 [‘zebra’: <bounding box>]
x 2
[‘cup’, ‘cup’, Annotations:
‘plant’, ‘table’,
[‘cup’: <bounding
‘sofa’, ‘sofa’] box>] ……
Q1: Describe the object located at
Attributes: ["a <location1>.
small yellow bird”, A1: There is a red cardinal.
" ba l ur ee d j ac ya ”r ,d i "n aa l g” r, e e" na 1 2 Q b2 l: u eW h je ar ye li os c at th ee d o ib nj e tc ht e d ie ms ac gr ei b ie nd a
parakeet"] terms of bounding box?
A2: It is located at <location2>.
Q1: Whatis the color of the horse
Attributes: ["a 1 2 at <location1>?
majestic chestnut- A1: Chestnut-colored.
B-1 colored horse", "a Q2: Whatcolor is the horse at
sleek black horse"] <location2>?
A2: Sleek black.
Attributes: ["a Q1: What is the number of dog?
s wm ha il tl e b pr uo pw pn y ,a n "d a 1 A1: Two.
larger brown dog”, Q2: Whatcolor is the sofa at
"a cozy beige sofa", <location1>?
"a blue velvet sofa”] A2: Blue.
Attributes: [“a small
golden retriever”, “a man
wearing a blue t-shirt”] Q t1 h: e W mh aa nt ai ns d t th he e r de ol ga ?tionshipbetween
Relation: {“subject”: A1: A small golden retriever is
“dog”, “object”: “person”, playing fetch with a man wearing a
“relation”: “playing blue t-shirt.
fetch with”}
Attributes: [“a softblue
cushion”, “abeigesofa
with wooden legs”] Q1: What is therelationshipbetween
the sofa and the cushion?
B-2 Relation: {“subject”: A1: A sofa blue cushion is placed on
“cushion”, “object”: a beige sofa with wooden legs.
“sofa”, “relation”:
“placed on”}
Attributes: [“a man in a
wetsuit”, “a colorful
p ha ar ra ns ea si s”l ] being held by a Q t1 h: e W mh aa nt ai ns d t th he e r pe al ra at si ao in ls ?hipbetween
Relation: {“subject”: A1: Amanin a wetsuit holds onto a
“man”, “object”: colorful parasail by a harness.
“parasail”, “relation”:
“hold”}
CLIS
Figure7: SynthetictrainingsamplesofvarioustasksfromACP.TasksA-1andA-2correspondtoSegmentation
andDetection,respectively.TasksB-1andB-2pertaintomulti-modalperceptionandreasoning.Giventhesame
inputorscenegraphontheleft,theCLISofthesynthetictrainingsamplesincreasesalongthex-axis,withfinal
annotationsontheright.
19