[
    {
        "title": "Odd-One-Out: Anomaly Detection by Comparing with Neighbors",
        "authors": "Ankan BhuniaChangjian LiHakan Bilen",
        "links": "http://arxiv.org/abs/2406.20099v1",
        "entry_id": "http://arxiv.org/abs/2406.20099v1",
        "pdf_url": "http://arxiv.org/pdf/2406.20099v1",
        "summary": "This paper introduces a novel anomaly detection (AD) problem that focuses on\nidentifying `odd-looking' objects relative to the other instances within a\nscene. Unlike the traditional AD benchmarks, in our setting, anomalies in this\ncontext are scene-specific, defined by the regular instances that make up the\nmajority. Since object instances are often partly visible from a single\nviewpoint, our setting provides multiple views of each scene as input. To\nprovide a testbed for future research in this task, we introduce two\nbenchmarks, ToysAD-8K and PartsAD-15K. We propose a novel method that generates\n3D object-centric representations for each instance and detects the anomalous\nones through a cross-examination between the instances. We rigorously analyze\nour method quantitatively and qualitatively in the presented benchmarks.",
        "updated": "2024-06-28 17:59:51 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.20099v1"
    },
    {
        "title": "Web2Code: A Large-scale Webpage-to-Code Dataset and Evaluation Framework for Multimodal LLMs",
        "authors": "Sukmin YunHaokun LinRusiru ThusharaMohammad Qazim BhatYongxin WangZutao JiangMingkai DengJinhong WangTianhua TaoJunbo LiHaonan LiPreslav NakovTimothy BaldwinZhengzhong LiuEric P. XingXiaodan LiangZhiqiang Shen",
        "links": "http://arxiv.org/abs/2406.20098v1",
        "entry_id": "http://arxiv.org/abs/2406.20098v1",
        "pdf_url": "http://arxiv.org/pdf/2406.20098v1",
        "summary": "Multimodal large language models (MLLMs) have shown impressive success across\nmodalities such as image, video, and audio in a variety of understanding and\ngeneration tasks. However, current MLLMs are surprisingly poor at understanding\nwebpage screenshots and generating their corresponding HTML code. To address\nthis problem, we propose Web2Code, a benchmark consisting of a new large-scale\nwebpage-to-code dataset for instruction tuning and an evaluation framework for\nthe webpage understanding and HTML code translation abilities of MLLMs. For\ndataset construction, we leverage pretrained LLMs to enhance existing\nwebpage-to-code datasets as well as generate a diverse pool of new webpages\nrendered into images. Specifically, the inputs are webpage images and\ninstructions, while the responses are the webpage's HTML code. We further\ninclude diverse natural language QA pairs about the webpage content in the\nresponses to enable a more comprehensive understanding of the web content. To\nevaluate model performance in these tasks, we develop an evaluation framework\nfor testing MLLMs' abilities in webpage understanding and web-to-code\ngeneration. Extensive experiments show that our proposed dataset is beneficial\nnot only to our proposed tasks but also in the general visual domain, while\nprevious datasets result in worse performance. We hope our work will contribute\nto the development of general MLLMs suitable for web-based content generation\nand task automation. Our data and code will be available at\nhttps://github.com/MBZUAI-LLM/web2code.",
        "updated": "2024-06-28 17:59:46 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.20098v1"
    },
    {
        "title": "LLaRA: Supercharging Robot Learning Data for Vision-Language Policy",
        "authors": "Xiang LiCristina MataJongwoo ParkKumara KahatapitiyaYoo Sung JangJinghuan ShangKanchana RanasingheRyan BurgertMu CaiYong Jae LeeMichael S. Ryoo",
        "links": "http://arxiv.org/abs/2406.20095v1",
        "entry_id": "http://arxiv.org/abs/2406.20095v1",
        "pdf_url": "http://arxiv.org/pdf/2406.20095v1",
        "summary": "Large Language Models (LLMs) equipped with extensive world knowledge and\nstrong reasoning skills can tackle diverse tasks across domains, often by\nposing them as conversation-style instruction-response pairs. In this paper, we\npropose LLaRA: Large Language and Robotics Assistant, a framework which\nformulates robot action policy as conversations, and provides improved\nresponses when trained with auxiliary data that complements policy learning.\nLLMs with visual inputs, i.e., Vision Language Models (VLMs), have the capacity\nto process state information as visual-textual prompts and generate optimal\npolicy decisions in text. To train such action policy VLMs, we first introduce\nan automated pipeline to generate diverse high-quality robotics instruction\ndata from existing behavior cloning data. A VLM finetuned with the resulting\ncollection of datasets based on a conversation-style formulation tailored for\nrobotics tasks, can generate meaningful robot action policy decisions. Our\nexperiments across multiple simulated and real-world environments demonstrate\nthe state-of-the-art performance of the proposed LLaRA framework. The code,\ndatasets, and pretrained models are available at\nhttps://github.com/LostXine/LLaRA.",
        "updated": "2024-06-28 17:59:12 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.20095v1"
    },
    {
        "title": "LLaVolta: Efficient Multi-modal Models via Stage-wise Visual Context Compression",
        "authors": "Jieneng ChenLuoxin YeJu HeZhao-Yang WangDaniel KhashabiAlan Yuille",
        "links": "http://arxiv.org/abs/2406.20092v1",
        "entry_id": "http://arxiv.org/abs/2406.20092v1",
        "pdf_url": "http://arxiv.org/pdf/2406.20092v1",
        "summary": "While significant advancements have been made in compressed representations\nfor text embeddings in large language models (LLMs), the compression of visual\ntokens in large multi-modal models (LMMs) has remained a largely overlooked\narea. In this work, we present the study on the analysis of redundancy\nconcerning visual tokens and efficient training within these models. Our\ninitial experiments show that eliminating up to 70% of visual tokens at the\ntesting stage by simply average pooling only leads to a minimal 3% reduction in\nvisual question answering accuracy on the GQA benchmark, indicating significant\nredundancy in visual context. Addressing this, we introduce Visual Context\nCompressor, which reduces the number of visual tokens during training to\nenhance training efficiency without sacrificing performance. To minimize\ninformation loss caused by the compression on visual tokens while maintaining\ntraining efficiency, we develop LLaVolta as a lite training scheme. LLaVolta\nincorporates stage-wise visual context compression to progressively compress\nthe visual tokens from heavily to lightly, and finally no compression at the\nend of training, yielding no loss of information when testing. Extensive\nexperiments demonstrate that our approach enhances the performance of MLLMs in\nboth image-language and video-language understanding, while also significantly\ncutting training costs. Code is available at\nhttps://github.com/Beckschen/LLaVolta",
        "updated": "2024-06-28 17:57:14 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.20092v1"
    },
    {
        "title": "Auto Cherry-Picker: Learning from High-quality Generative Data Driven by Language",
        "authors": "Yicheng ChenXiangtai LiYining LiYanhong ZengJianzong WuXiangyu ZhaoKai Chen",
        "links": "http://arxiv.org/abs/2406.20085v1",
        "entry_id": "http://arxiv.org/abs/2406.20085v1",
        "pdf_url": "http://arxiv.org/pdf/2406.20085v1",
        "summary": "Diffusion-based models have shown great potential in generating high-quality\nimages with various layouts, which can benefit downstream perception tasks.\nHowever, a fully automatic layout generation driven only by language and a\nsuitable metric for measuring multiple generated instances has not been well\nexplored. In this work, we present Auto Cherry-Picker (ACP), a novel framework\nthat generates high-quality multi-modal training examples to augment perception\nand multi-modal training. Starting with a simple list of natural language\nconcepts, we prompt large language models (LLMs) to generate a detailed\ndescription and design reasonable layouts. Next, we use an off-the-shelf\ntext-to-image model to generate multiple images. Then, the generated data are\nrefined using a comprehensively designed metric to ensure quality. In\nparticular, we present a new metric, Composite Layout and Image Score (CLIS),\nto evaluate the generated images fairly. Our synthetic high-quality examples\nboost performance in various scenarios by customizing the initial concept list,\nespecially in addressing challenges associated with long-tailed distribution\nand imbalanced datasets. Experiment results on downstream tasks demonstrate\nthat Auto Cherry-Picker can significantly improve the performance of existing\nmodels. In addition, we have thoroughly investigated the correlation between\nCLIS and performance gains in downstream tasks, and we find that a better CLIS\nscore results in better performance. This finding shows the potential for\nevaluation metrics as the role for various visual perception and MLLM tasks.\nCode will be available.",
        "updated": "2024-06-28 17:53:18 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.20085v1"
    }
]