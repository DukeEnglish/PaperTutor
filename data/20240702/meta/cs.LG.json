[
    {
        "title": "LLaRA: Supercharging Robot Learning Data for Vision-Language Policy",
        "authors": "Xiang LiCristina MataJongwoo ParkKumara KahatapitiyaYoo Sung JangJinghuan ShangKanchana RanasingheRyan BurgertMu CaiYong Jae LeeMichael S. Ryoo",
        "links": "http://arxiv.org/abs/2406.20095v1",
        "entry_id": "http://arxiv.org/abs/2406.20095v1",
        "pdf_url": "http://arxiv.org/pdf/2406.20095v1",
        "summary": "Large Language Models (LLMs) equipped with extensive world knowledge and\nstrong reasoning skills can tackle diverse tasks across domains, often by\nposing them as conversation-style instruction-response pairs. In this paper, we\npropose LLaRA: Large Language and Robotics Assistant, a framework which\nformulates robot action policy as conversations, and provides improved\nresponses when trained with auxiliary data that complements policy learning.\nLLMs with visual inputs, i.e., Vision Language Models (VLMs), have the capacity\nto process state information as visual-textual prompts and generate optimal\npolicy decisions in text. To train such action policy VLMs, we first introduce\nan automated pipeline to generate diverse high-quality robotics instruction\ndata from existing behavior cloning data. A VLM finetuned with the resulting\ncollection of datasets based on a conversation-style formulation tailored for\nrobotics tasks, can generate meaningful robot action policy decisions. Our\nexperiments across multiple simulated and real-world environments demonstrate\nthe state-of-the-art performance of the proposed LLaRA framework. The code,\ndatasets, and pretrained models are available at\nhttps://github.com/LostXine/LLaRA.",
        "updated": "2024-06-28 17:59:12 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.20095v1"
    },
    {
        "title": "Scaling Synthetic Data Creation with 1,000,000,000 Personas",
        "authors": "Xin ChanXiaoyang WangDian YuHaitao MiDong Yu",
        "links": "http://arxiv.org/abs/2406.20094v1",
        "entry_id": "http://arxiv.org/abs/2406.20094v1",
        "pdf_url": "http://arxiv.org/pdf/2406.20094v1",
        "summary": "We propose a novel persona-driven data synthesis methodology that leverages\nvarious perspectives within a large language model (LLM) to create diverse\nsynthetic data. To fully exploit this methodology at scale, we introduce\nPersona Hub -- a collection of 1 billion diverse personas automatically curated\nfrom web data. These 1 billion personas (~13% of the world's total population),\nacting as distributed carriers of world knowledge, can tap into almost every\nperspective encapsulated within the LLM, thereby facilitating the creation of\ndiverse synthetic data at scale for various scenarios. By showcasing Persona\nHub's use cases in synthesizing high-quality mathematical and logical reasoning\nproblems, instructions (i.e., user prompts), knowledge-rich texts, game NPCs\nand tools (functions) at scale, we demonstrate persona-driven data synthesis is\nversatile, scalable, flexible, and easy to use, potentially driving a paradigm\nshift in synthetic data creation and applications in practice, which may have a\nprofound impact on LLM research and development.",
        "updated": "2024-06-28 17:59:01 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.20094v1"
    },
    {
        "title": "ProgressGym: Alignment with a Millennium of Moral Progress",
        "authors": "Tianyi QiuYang ZhangXuchuan HuangJasmine Xinze LiJiaming JiYaodong Yang",
        "links": "http://arxiv.org/abs/2406.20087v1",
        "entry_id": "http://arxiv.org/abs/2406.20087v1",
        "pdf_url": "http://arxiv.org/pdf/2406.20087v1",
        "summary": "Frontier AI systems, including large language models (LLMs), hold increasing\ninfluence over the epistemology of human users. Such influence can reinforce\nprevailing societal values, potentially contributing to the lock-in of\nmisguided moral beliefs and, consequently, the perpetuation of problematic\nmoral practices on a broad scale. We introduce progress alignment as a\ntechnical solution to mitigate this imminent risk. Progress alignment\nalgorithms learn to emulate the mechanics of human moral progress, thereby\naddressing the susceptibility of existing alignment methods to contemporary\nmoral blindspots. To empower research in progress alignment, we introduce\nProgressGym, an experimental framework allowing the learning of moral progress\nmechanics from history, in order to facilitate future progress in real-world\nmoral decisions. Leveraging 9 centuries of historical text and 18 historical\nLLMs, ProgressGym enables codification of real-world progress alignment\nchallenges into concrete benchmarks. Specifically, we introduce three core\nchallenges: tracking evolving values (PG-Follow), preemptively anticipating\nmoral progress (PG-Predict), and regulating the feedback loop between human and\nAI value shifts (PG-Coevolve). Alignment methods without a temporal dimension\nare inapplicable to these tasks. In response, we present lifelong and\nextrapolative algorithms as baseline methods of progress alignment, and build\nan open leaderboard soliciting novel algorithms and challenges. The framework\nand the leaderboard are available at\nhttps://github.com/PKU-Alignment/ProgressGym and\nhttps://huggingface.co/spaces/PKU-Alignment/ProgressGym-LeaderBoard\nrespectively.",
        "updated": "2024-06-28 17:55:24 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.20087v1"
    },
    {
        "title": "Token Erasure as a Footprint of Implicit Vocabulary Items in LLMs",
        "authors": "Sheridan FeuchtDavid AtkinsonByron WallaceDavid Bau",
        "links": "http://arxiv.org/abs/2406.20086v1",
        "entry_id": "http://arxiv.org/abs/2406.20086v1",
        "pdf_url": "http://arxiv.org/pdf/2406.20086v1",
        "summary": "LLMs process text as sequences of tokens that roughly correspond to words,\nwhere less common words are represented by multiple tokens. However, individual\ntokens are often semantically unrelated to the meanings of the words/concepts\nthey comprise. For example, Llama-2-7b's tokenizer splits the word\n\"northeastern\" into the tokens ['_n', 'ort', 'he', 'astern'], none of which\ncorrespond to semantically meaningful units like \"north\" or \"east.\" Similarly,\nthe overall meanings of named entities like \"Neil Young\" and multi-word\nexpressions like \"break a leg\" cannot be directly inferred from their\nconstituent tokens. Mechanistically, how do LLMs convert such arbitrary groups\nof tokens into useful higher-level representations? In this work, we find that\nlast token representations of named entities and multi-token words exhibit a\npronounced \"erasure\" effect, where information about previous and current\ntokens is rapidly forgotten in early layers. Using this observation, we propose\na method to \"read out\" the implicit vocabulary of an autoregressive LLM by\nexamining differences in token representations across layers, and present\nresults of this method for Llama-2-7b and Llama-3-8B. To our knowledge, this is\nthe first attempt to probe the implicit vocabulary of an LLM.",
        "updated": "2024-06-28 17:54:47 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.20086v1"
    },
    {
        "title": "Segment Anything without Supervision",
        "authors": "XuDong WangJingfeng YangTrevor Darrell",
        "links": "http://arxiv.org/abs/2406.20081v1",
        "entry_id": "http://arxiv.org/abs/2406.20081v1",
        "pdf_url": "http://arxiv.org/pdf/2406.20081v1",
        "summary": "The Segmentation Anything Model (SAM) requires labor-intensive data labeling.\nWe present Unsupervised SAM (UnSAM) for promptable and automatic whole-image\nsegmentation that does not require human annotations. UnSAM utilizes a\ndivide-and-conquer strategy to \"discover\" the hierarchical structure of visual\nscenes. We first leverage top-down clustering methods to partition an unlabeled\nimage into instance/semantic level segments. For all pixels within a segment, a\nbottom-up clustering method is employed to iteratively merge them into larger\ngroups, thereby forming a hierarchical structure. These unsupervised\nmulti-granular masks are then utilized to supervise model training. Evaluated\nacross seven popular datasets, UnSAM achieves competitive results with the\nsupervised counterpart SAM, and surpasses the previous state-of-the-art in\nunsupervised segmentation by 11% in terms of AR. Moreover, we show that\nsupervised SAM can also benefit from our self-supervised labels. By integrating\nour unsupervised pseudo masks into SA-1B's ground-truth masks and training\nUnSAM with only 1% of SA-1B, a lightly semi-supervised UnSAM can often segment\nentities overlooked by supervised SAM, exceeding SAM's AR by over 6.7% and AP\nby 3.9% on SA-1B.",
        "updated": "2024-06-28 17:47:32 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.20081v1"
    }
]