Preprint
DIFFUSING STATES AND MATCHING SCORES:
A NEW FRAMEWORK FOR IMITATION LEARNING
RunzheWu YidingChen GokulSwamy
CornellUniversity CornellUniversity CarnegieMellonUniversity
rw646@cornell.edu yc2773@cornell.edu gswamy@andrew.cmu.edu
KiantéBrantley WenSun
HarvardUniversity CornellUniversity
kdbrantley@g.harvard.edu ws455@cornell.edu
ABSTRACT
Adversarial Imitation Learning is traditionally framed as a two-player zero-sum
gamebetweenalearnerandanadversariallychosencostfunction,andcanthere-
fore be thought of as the sequential generalization of a Generative Adversarial
Network (GAN). However, in recent years, diffusion models have emerged as a
non-adversarialalternativetoGANsthatmerelyrequiretrainingascorefunction
via regression, yet produce generations of a higher quality. In response, we
investigatehowtoliftinsightsfromdiffusionmodelingtothesequentialsetting.
Weproposediffusingstatesandperformingscore-matchingalongdiffusedstates
to measure the discrepancy between the expert’s and learner’s states. Thus, our
approach only requires training score functions to predict noises via standard
regression,makingitsignificantlyeasierandmorestabletotrainthanadversarial
methods. Theoretically, we prove first- and second-order instance-dependent
bounds with linear scaling in the horizon, proving that our approach avoids
the compounding errors that stymie offline approaches to imitation learning.
Empirically, we show our approach outperforms both GAN-style imitation
learning baselines and discriminator-free imitation learning baselines across
various continuous control problems, including complex tasks like controlling
humanoidstowalk,sit,crawl,andnavigatethroughobstacles.
1 INTRODUCTION
Fundamentally, in imitation learning (IL, Osa et al. (2018)), we want to match the sequential
behavior of an expert demonstrator. Different notions of what matching should mean for IL have
beenproposedintheliterature,fromf-divergences(Ho&Ermon,2016;Keetal.,2021)toIntegral
Probability Metrics (IPMs, Müller (1997); Sun et al. (2019); Kidambi et al. (2021); Swamy et al.
(2021); Chang et al. (2021); Song et al. (2024)). To compute the chosen notion of divergence
from the expert demonstrations so that the learner can then optimize it, it is common to train a
discriminator (i.e. aclassifier)betweenexpertandlearnerdata. Thisdiscriminatoristhenusedas
arewardfunctionforapolicyupdate, anapproachknownasinversereinforcementlearning(IRL,
Abbeel & Ng (2004); Ziebart et al. (2008)). Various losses for training discriminators have been
proposed(Ho&Ermon,2016;Kostrikovetal.,2018;Fuetal.,2017;Keetal.,2021;Swamyetal.,
2021; Chang et al., 2024), and IRL has been applied in real life in domains like routing (Barnes
etal.,2023)andLLMfine-tuning(Wulfmeieretal.,2024).
As observed by Finn et al. (2016), inverse RL can be seen as the sequential generalization of a
GenerativeAdversarialNetwork(GAN)(Goodfellowetal.,2020).However,eveninthesingle-shot
setting, GANsare knowntosuffer fromissueslikeunstable trainingdynamicsandmode collapse
(Miyato et al., 2018; Arjovsky & Bottou, 2017). In contrast, Score-Based Diffusion Models (Ho
etal.,2020;Song&Ermon,2019;Songetal.,2020)areknowntobemorestabletotrainandproduce
higherqualitysamplesindomainslikeaudioandvideo(Rombachetal.,2022;Rameshetal.,2022;
Kong et al., 2020). Intuitively, diffusion models corrupt samples from a target distribution with
1
4202
tcO
71
]GL.sc[
1v55831.0142:viXraPreprint
Crawl
Time Vπ
Expert
1
Expert
0.75
SMILING 0.5
(ours)
0.25
0
DAC SMILING DAC BC
(ours)
Pole
Time Vπ
Expert
1
Expert
0.75
0.5
SMILING
(ours)
0.25
0
DAC SMILING DAC BC
(ours)
Figure1: Whole-bodyhumanoidcontrol via ILfromstatealone. The twopanelsillustratethe
crawl and pole tasks, respectively. In both tasks, we show the time-lapse frames of the expert
policyandthepolicieslearnedbyourmethod(SMILING)andDACafter3Mtrainingsteps. Inthe
crawltask,thegoalistocrawlthroughagreytunnel,whereboththeexpertandourssucceedandthe
crawlingmovementsaresimilar.However,DACcollapsesandfailstocompletethetask.Inthepole
task, the goal is to travel through a dense forest of poles. Ours successfully navigates through the
poles,thoughwithlessstabilitythantheexpert,whileDACcollapsestothegroundandcannotmove.
The bar graphs on the right show normalized policy performance, where SMILING significantly
outperforms DAC and Behavioral Cloning (BC) in both tasks, approaching expert performance in
crawl. NotethatBCusesexpertactions,whileDACandSMILINGlearnfromstatesalone.
noiseandtrainagenerativemodeltoreversethiscorruption. Critically,thesemodelsaretrainedvia
score-matching: anon-adversarial,purelyregression-basedprocedureinwhichanetworkistrained
tomatchthescore(i.e.,gradientofthelogprobability)ofthetargetdistribution.
Infact, priorworkhasshownthatdiffusionmodelsarepowerfulpolicyclassesforILduetotheir
ability to model multi-modal behavior (Chi et al., 2023). However, these approaches are purely
offline behavioral cloning (Pomerleau, 1988), and therefore can suffer from the well-known com-
poundingerrorissue(Rossetal.,2011)thataffectsallofflineapproachestoIL(Swamyetal.,2021).
Takentogether,theprecedingpointsbegthequestion:
Canwelifttheinsightsofdiffusionmodelstoinversereinforcementlearning?
OuranswertothisquestionisSMILING:anewIRLframeworkthatabandonsunstablediscriminator
training in favor of a simpler, score-matching based objective. We first fit a score function to the
expert’sstatedistribution.Then,ateachiterationofouralgorithm,wefitascorefunctiontothestate
distributionofthemixtureoftheprecedingpoliciesviastandardregression-basedscorematching,
beforeusingthecombinationofthesescorefunctionstodefineacostfunctionforthepolicysearch
step. Ourframeworktreatsdiffusionmodeltraining(score-matching)asablackboxwhichallows
ustotransferanyadvancementsindiffusionmodeltraining(e.g., betternoiseschedulingorbetter
2Preprint
training techniques) to IRL. In theory, rather than optimizing either an f-divergence or an IPM,
thiscorrespondstominimizinganovelsequentialgeneralizationoftheFisherdivergence(Johnson,
2004)wetermtheDiffusionScoreDivergence(DSDivergence).
Wedemonstratetheadvantagesofourframeworkinboththeoryandpractice. Intheory, weshow
that our approach can achieve first- and second-order instance-dependent regret bounds, as well
as a linear scaling in the horizon to model-misspecification errors arising from expert not being
realizablebythelearner’spolicyclassorpotentialoptimizationerrorinscore-matchingandpolicy
search. Thus,weestablishthat
Byliftingscore-matchingtoIRL,SMILINGprovablyavoidscompoundingerrorsandachieves
instance-dependentbounds.
Intuitively, the second-order bounds mean that the performance gap between our learned policy
andtheexpertautomaticallyshrinkswheneithertheexpertorthelearnedpolicyhaslowvariance
in terms of their performance under the ground-truth reward (e.g. when the expert and dynamics
are relatively deterministic). This is because when we perform score-matching, we are actually
minimizingthesquaredHellingerdistancebetweenthelearnerandtheexpert’sstatedistributions,
which has shown to play an important role in achieving second-order regret bounds in the Rein-
forcementLearningsetting(Wangetal.,2024c). Theabilitytoachieveinstance-dependentbounds
demonstratesthetheoreticalbenefitofDiffusionScoredivergenceoverothermetricssuchasIPMs
andf-divergences. Inpractice,weshowthatunderintheILfromobservationonlysetting(Torabi
et al., 2018; Sun et al., 2019), SMILING outperforms adversarial GAN-based IL baselines,
discriminator-freeILbaselines,andBehavioralCloning1oncomplextaskssuchascontrolling
humanoidstowalk,sit,crawl,andnavigatethroughpoles(seeFigure1). ThismakesSMILING
the first IRL method to solve multiple tasks on the recently released HumanoidBench benchmark
(Sferrazza et al., 2024) using only the state information of the expert demonstrations. We release
thecodeforallexperimentsathttps://github.com/ziqian2000/SMILING.
2 RELATED WORKS
Inverse Reinforcement Learning. Starting with the seminal work of Abbeel & Ng (2004), var-
ious authors have proposed solving the problem of imitation via inverse RL (Ziebart et al., 2008;
Ratliffetal.,2006;Sunetal.,2019;Kidambietal.,2021;Changetal.,2021). AsarguedbySwamy
etal.(2021),theseapproachescanbethoughtofasminimizinganintegralprobabilitymetric(IPM)
between learner and expert behavior via an adversarial training procedure – we refer interested
readers to their paper for a full list. Many other IRL approaches can instead be framed as mini-
mizing an f-divergence via adversarial training (Ke et al., 2021), including GAIL (Ho & Ermon,
2016),DAC(Kostrikovetal.,2018),AIRL(Fuetal.,2017),FAIRL(Ghasemipouretal.,2020),and
f-GAIL(Zhangetal.,2020). However,alloftheprecedingapproachesinvolvetrainingadiscrim-
inator, while our technique only requires fitting score functions and does not seek to approximate
either an f-divergence or an IPM. Wang et al. (2024a) also explored the use of diffusion models
for imitation learning. They insert the score matching loss into the f-divergence objective of the
discriminator. Thus,theirmethodalsobelongstothef-divergenceframework. Huangetal.(2024)
alsoutilizediffusionmodelsforimitationlearning,proposingtotrainaconditionaldiffusionmodel
asadiscriminatorforsingle-stepstatetransitionsbetweentheexpertandthecurrentpolicy. Their
approachremainswithintheclassicf-divergenceframeworkaswell. Additionally,variousauthors
haveproposedusingtechniquestoeitherstabilizethistrainingprocedure(e.g. viaboosting,Chang
etal.(2024))orreducingtheamountofinteractionrequiredduringtheRLstep(Swamyetal.,2023;
Renetal.,2024;Saporaetal.,2024)–aswefocusonimprovingtherewardfunctionusedininverse
RL,ourapproachisorthogonaltoandcouldbenaturallyappliedontopofthesetechniques.
Discriminator-FreeInverseReinforcementLearning.SQIL(Reddyetal.,2019)replacestraining
adiscriminatorwithafixedrewardfunction(+1forexpertdata,0forlearnerdata). Unfortunately,
thismeansthataperformantlearnerisdis-incentivizedtoperformexpert-likebehavior, leadingto
dramaticdropsinperformance(Bardeetal.,2020). TheAdRILalgorithmofSwamyetal.(2021)
usestechniquesfromfunctionalgradientstoaddressthisissue,butrequirestheuseoffanoff-policy
RLalgorithmtoimplement,whileourframeworkmakesnosuchassumptions. ASAF(Bardeetal.,
1ThisisdespitethefactthatBCrequirestheexpertactions,whilewedonot.
3Preprint
2020)proposesusingthepriorpolicytocomputetheoptimalf-divergencediscriminatorinclosed
form,whilewefocusonadifferentclassofdivergences. IQ-Learn(Gargetal.,2021)proposesto
performIRLinthespaceQfunctions,butcanthereforesufferfrompoorperformanceonproblems
withstochasticdynamics(Renetal.,2024)orwhentheQ-functionismorecomplexthanthereward
function (e.g. navigating to a goal in a maze). Jain et al. (2024) propose measuring differences
between learner and expert behavior in terms of successor features, but are only able to optimize
overdeterministicpolicies. DRIL(Brantleyetal.,2019)reliesonensemblemethodsandusesthe
disagreementamongmodelsintheensembleasasurrogatecostfunction,makingitchallengingto
provesimilartheoreticalguaranteestoourtechnique.
Incontrast,ourmethodinsteadtrainsscorefunctionsonthestatedistributionofthepolicyviathe
standard score matching objective. We show that our approach can require less expressive func-
tionapproximatorsthandiscriminator-basedmethods(seeSection5.1). Additionally,ourapproach
achievessolidfirst-andsecond-ordersamplecomplexityboundandalsodeliversstrongempirical
performanceonchallenginghumanoidcontroltasks.
DiffusionModelsforDecisionMaking. Diffusionmodelshavebeenwidelystudiedinthecontext
robotlearning, wherediffusionmodelshavedemonstratedastrongabilitytocapturecomplexand
multi-modaldata.Recentworkshaveexploredusingdiffusionmodelstoperformbehavioralcloning
Chi et al. (2023); Pearce et al. (2023); Block et al. (2023); Team et al. (2024) as well as using
diffusionmodelstocapturetrajectory-leveldynamics (Janneretal.,2022;Ajayetal.,2022). These
prior work exclusively rely on offline data for training. We instead focus on lifting insights from
diffusionmodelingandscorematchingtoIRL,whichusesbothofflineexpertdataandalsoonline
interactionwiththeenvironment. UnlikepriordiffusionBCwork,weusediffusionmodelsforboth
theexpert’sandthelearner’sstatedistributions,notjusttheexpert’saction-conditionaldistributions.
Thismeanswedonotrequireexpertactionlabels,whichcanbechallengingtoacquireinpractice.
3 PRELIMINARIES
3.1 MARKOVDECISIONPROCESSES
We consider finite-horizon Markov decision processes (MDPs) with state space Rd, action
space ,transitiondynamicsP,(unknown)costfunctionc⋆ : R,2 andhorizS on⊆ H. Thegoal
is to fiA nd a policy π : ∆( ) that minimizes the expectS ed→ cost H E c⋆(s) where dπ
s∼dπ
denotestheaveragestatS ed→ istributA ioninducedbythepolicyπ,i.e.,dπ(s) =· (cid:80)H Pr(s = s)/H
h=1 h
wheres denotesthestateattimeh. Thestatevaluefunctionofapolicyπ isdefinedasVπ(s) =
h
E (cid:80)H c⋆(s )whereτ =(s ,...,s )denotesthetrajectoryinducedbythepolicyπ.
τ∼π h=1 h 1 H
3.2 IMITATIONLEARNING
In imitation learning, we are usually given expert demonstration in the form of state-action-next-
statetuples,andthegoalistolearnapolicythatmimicstheexpertbehavior. Wedonothaveaccess
totheground-truthcostfunctionc⋆. Inthispaper,weconsiderahardersettingwhereonlystatesare
given. Specifically,givenadataset e = s(i) N ofexpertdemonstrations,thegoalistolearna
D { }i=1
policy π that minimizes the discrepancy between the state distributions of policy π and the expert
policyπe.Intheinversereinforcementlearningsetting,inadditiontotheexpertdemonstrations e,
D
wecaninteractwiththeenvironmenttocollectmore(reward-free)learnertrajectories.
3.3 DIFFUSIONMODELS
Generallyspeaking,theforwardprocessofadiffusionmodeladdsnoisetoasamplefromthedata
distributionp . Wecanformalizethisviaastochasticdifferentialequation(SDE).Forsimplicityin
0
thispaper,weconsidertheOrnstein-Uhlenbeck(OU)process:
dx = x dt+√2 dB , x p , (1)
t t t 0 0
− ∼
2Weassumethecostdependsonlyonstates.Theextensiontostate-actioncostsisstraightforward.
4Preprint
Algorithm1SMILING(Score-MatchingImitationLearnING)
Require: state-onlyexpertdemonstration e = s(i) N
D { }i=1
1: Estimatescorefunctionofexpertstatedistribution:
(cid:104) (cid:105)
ge argmin E E E g(s ,t) logq (s s) 2
← g∈G s∼Det∼U(T)st∼qt(·|s) ∥ t −∇st t t | ∥2
2: fork =1,2,...,K do
3: Estimatethescorefunctionoflearnerstatedistributions:
k−1
g(k)
argmin(cid:88)
E E E
(cid:104)
g(s ,t) logq (s s)
2(cid:105)
.
← g∈G i=1s∼dπ(i)t∼U(T)st∼qt(·|s) ∥ t −∇st t t | ∥2
4: Updatepolicyπ(k)viaRL(e.g.,SAC)oncostc(k)(Eq. 4)
π(k) RL(c(k))
←
5: endfor
where B is the standard Brownian motion in Rd. It is known that the reverse process (y )
t t t∈[0,T]
satisfiesthefollowingreverse-timeSDE(Anderson,1982):
dy =(y +2 logp (y )) dt+√2 dB , y p
t t T−t t t 0 T
∇ ∼
where U(T) denotes the uniform distribution on [0,T], and B now denotes reversed Brownian
t
motion. Thegradientofthelogprobability logp iscalledthescorefunctionofthedistribution
t
∇
p . Whenthescorefunctionisknown,wecanuseittogeneratesamplesfromp bysimulatingthe
t 0
reverseprocess. Estimatingthescorefunction logp iscalledscorematching,whichistypically
t
∇
doneviaminimizingthefollowingregression-basedloss:
(cid:13) (cid:13)2
min E E E (cid:13)g(x ,t) logq (x x)(cid:13) .
g
x∼p0t∼U(T)xt∼qt(·|x)(cid:13) t −∇xt t t
|
(cid:13)
2
Weuseq (x x)todenotetheconditionaldistributionattimetoftheforwardprocessconditioned
t t
ontheinitials| tatex,whichhasclosedformq (x x)= (xe−t,(1 e−2t)I).
t t
| N −
ApplyingDiffusiontoStateDistributions. Forapolicyπ,wewillusepπ todenotethemarginal
t
distributionattimetobtainedbyapplyingtheforwarddiffusionprocesstodπ,i.e.,initialsamples
aredrawnfrompπ := dπ. Whenpresentingasymptoticresults,wewilluseO()tohideconstants
0 ·
andO(cid:101)()tohideconstantsandlogarithmicfactors.
·
4 ALGORITHM
Webeginbyintroducinganoveldiscrepancymeasurebetweentwodistributionsthatwillbelever-
agedbyalgorithm:
Definition1(DiffusionScoreDivergence). FortwodistributionsP andQ,wedefinetheDiffusion
ScoreDivergence(DSDivergence)as
(cid:13) (cid:13)2
D (P,Q):= E E E (cid:13) logP (s ) logQ (s )(cid:13) .
DS (cid:13) t t t t (cid:13)
s∼Pt∼U(T)st∼qt(·|s) ∇ −∇ 2
Here q ( s) represents the conditional probability of the forward diffusion process at time t con-
t
·|
ditioned on the initial state s; P and Q denote the marginal distributions at time t obtained by
t t
applyingtheforwarddiffusionprocesstoP andQ,respectively. Wecall logP and logQ the
t t
∇ ∇
DiffusionScoreFunctionofP andQ.
Figure 2a illustrates the DS divergence. It measures the difference between two distributions by
comparing their diffusion score functions within one diffusion process. It is analogous to Fisher
divergencebutdiffersbyincorporatinganexpectationoverthediffusionprocess. DSdivergenceis
5Preprint
Expert Demos
Score
Matching
P ge ∇≈logpe
∇logP t(s t)
… Score
∇logQ t(s t) sT Replay Buffer Matching gπ ∇≈logpπ
Q N(0,I)
Policy RL
c(s):=−[ ge𝔼≈logq
t
2
2
… π 𝔼 gπ𝔼≈logq t 2 2]
(a) Illustration of DS Divergence (Defi-
nition1). (b)IllustrationofSMILING(Algorithm1).
Figure 2: Figure (a): The two curves represent the forward diffusion process of distributions P
and Q. DS Divergence measures the squared difference between the diffusion score functions,
logP (s )and logQ (s ),alongtheforwarddiffusionprocessofP. Figure(b): SMILINGfirst
t t t t
∇ ∇
pre-trains a diffusion model from the expert’s data. It then iteratively trains diffusion models on
learner’s data and performs RL to optimize a cost function formed by the learner’s score function
and the pre-trained expert score function. The cost function is designed to faithfully approximate
theDSdivergencebetweenthelearnerandtheexpert.
astrongdivergenceinthesensethat,whenevertheDSdivergencebetweenthetwodistributionsis
small,theKLdivergence,Hellingerdistance,andtotalvariationdistanceareallsmall(seeLemma2
andalsoChenetal.(2022);Okoetal.(2023)).
In our algorithmic framework Score-Matching Imitation LearnING — SMILING, we propose to
frameimitationlearningastheminimizationoftheDSdivergencebetweentheexpert’sandthe
(historyof)learnerstatedistributions. Webeginbydiscussinghowtodosobeforediscussingthe
theoreticalbenefitsofdoingso.
The first step is to pre-train a score function estimator ge(s,t) on expert’s state distribution
logpπe(s). This can be done via standard least-squares regression-based score-matching, as
∇s t
shown in Line 1. Here denotes the function class for score estimators. Then, we seek to find a
G
policyπthatminimizestheDSdivergencebetweenlearnerandexpertstatedistributions:
ℓ(π):= E ge(s ,t) logpπ(s ) 2, (2)
s∼dπ,t∼U(T),st∼qt(·|s)∥ t −∇st t t ∥2
where we have approximated the expert’s diffusion score function by ge(s ,t). However, ℓ(π) is
t
notdirectlycomputablesincewedonotknowthelearner’sscorefunction logpπ(s ). Anaive
approachwouldbetodirectlylearnanestimatorfor logpπ(s )viascore∇ ms at tchingt andt substitute
∇st t t
itintotheequation.However,asweproveinAppendixA,thiscanintroducesignificanterrorsdueto
theunboundednessofadifferenceofscorefunctionsandvariance-relatedconcerns. Wenowderive
amethodthatdoesnotsufferfromthisissue.
Recall that we defined q (s s) as the distribution of s given the initial sample s, which is
t t t
|
a simple Gaussian distribution with an appropriately scaled variance. To develop our method,
we first note that, luckily, logq (s s) is an unbiased estimator of the score logpπ(s ),
because E [ logq (s ∇ s)s ]t = t t l| ogpπ(s ) (Song et al., 2020). Given thi∇ s,st perhapt s tt he
s|st ∇st t t | ∇st t t
most immediate strategy would be to simply replace logpπ(s ) by its unbiased estimator,
∇st t t
logq (s s), in Eq. 2. However, in contrast to linear objectives like an IPM, to approx-
∇st t t
|
imate a squared objective like the DS Divergence accurately, we need to get both the expec-
tation (i.e. have an unbiased estimator) as well as the variance correct. If we don’t, then
E ge(s ,t) logq (s s) 2 will differ from ℓ(π) by a term related to
s∼dπ,t∼U(T),st∼qt(·|s) ∥ t − ∇st t t | ∥2
varianceoftheestimator logq (s s),i.e.
∇st t t
|
E logpπ(s ) logq (s s) 2. (3)
s∼dπ,t∼U(T),st∼qt(·|s)∥∇st t t −∇st t t | ∥2
To faithfully approximate ℓ(π), we need to estimate the variance term and subtract it from
E ge(s ,t) logq (s s) 2. We can do this by adding a term to our
s∼dπ,t∼U(T),st∼qt(·|s) ∥ t − ∇st t t | ∥2
6Preprint
objectivethatisminimizedatthevarianceandusinganotherfunction/networktooptimizeit. This
leadsustothefollowingestimatorforℓ(π):
ℓˆ(π):= E ge(s ,t) logq (s s) 2
s∼dπ,t∼U(T),st∼qt(·|s)∥ t −∇st t t | ∥2
min E g(s ,t) logq (s s) 2
− g∈G s∼dπ,t∼U(T),st∼qt(·|s)∥ t −∇st t t | ∥2
(cid:124) (cid:123)(cid:122) (cid:125)
MinimizedatthevarianceofEq.3
where is the function class for score estimators. Crucially, the highlighted term (orange) in the
G
above expression estimates the variance in Eq. 3 since one of its minimizers will be the Bayes
optimal logpπ(s ),i.e. theconditionalexpectationE [ logq (s s)].3 Observethatmini-
∇st t t s|st ∇st t t |
mizingthisorangetermjustrequiresasimpleregression-basedscorematchingobjective. So,while
naivescorematchingcausesissuesduetothevarianceoftheestimator,amorecleverapplicationof
scorematchingcanbeusedtofixthisconcern.
Withℓˆ(π)nowservingasavalidapproximationofℓ(π),theILproblemreducestosearchingfora
policytominimizeℓˆ(π),i.e.,min ℓˆ(π). Tofacilitatethis,wedefinepayoff (π,g)as:
π
L
(π,g):= E ge(s ,t) logq (s s) 2
L s∼dπ,t∼U(T),st∼qt(·|s)∥ t −∇st t t | ∥2
E g(s ,t) logq (s s) 2.
−s∼dπ,t∼U(T),st∼qt(·|s)∥ t −∇st t t | ∥2
Then,minimizingℓˆ(π)isequivalenttosolvingthetwo-playerzerosumgamemin max (π,g).
π g
L
Tosolvethisgame,weproposefollowingano-regretstrategyovergagainstabestresponseoverπ
(i.e. a dual algorithm (Swamy et al., 2021)). Specifically, Algorithm 1 applies Follow-the-Leader
(Shalev-Shwartz et al., 2012)) to optimize g in Line 3,4 and performs best response computation
overπviaRL(Line4)undercostfunctionck(s):
(cid:20)(cid:13) (cid:13)2 (cid:13) (cid:13)2(cid:21)
c(k)(s):= E E (cid:13)ge(s ,t) logq (s s)(cid:13) (cid:13)g(k)(s ,t) logq (s s)(cid:13) .
t∼U(T)st∼qt(·|s)
(cid:13) t −∇st t t
|
(cid:13) 2−(cid:13) t −∇st t t
|
(cid:13)
2
(4)
Remark 1 (Noise-prediction Form of the Cost Function). The cost function in Eq. 4 involves the
scorefunction logq (s s),whichhasaclosed-formexpressionformostmoderndiffusionmodels
t t
∇ |
includingDDPM.Specifically,whenthediffusionfollowstheOUprocess(Eq.1),thescorefunction
takes the form logq (s s) = (1 e−2t)−1(se−t s ), which is exactly the noise added to
t t t
∇ | − −
the original sample s during the diffusion process (recalling that for the OU process, q (s s) =
t t
(se−t,(1 e−2t)I)). Wedenotethisnoisebyϵ. Then,thecostfunction(Eq.4)isequivalen| tto
N −
(cid:20)(cid:13) (cid:13)2 (cid:13) (cid:13)2(cid:21)
c(k)(s):= E E (cid:13)ge(se−t+ϵ,t) ϵ(cid:13) (cid:13)g(k)(se−t+ϵ,t) ϵ(cid:13) .
(cid:13) (cid:13) (cid:13) (cid:13)
t∼U(T)ϵ∼N(0,I) − 2− − 2
ThiscloselyresemblesthenoisepredictioninDDPM.
To implement FTL, in Line 3, we use the classic idea of Data Aggregation (DAgger) (Ross
et al., 2011), which corresponds to aggregating all states collected from prior learned policies
π(1),...,π(k−1), and perform a score-matching least square regression on the aggregated dataset.
The RL procedure can take advantage of any modern RL optimizers. In our experiments, we use
SAC (Haarnoja et al., 2018) and DreamerV3 (Hafner et al., 2023), which serve as representative
model-freeandmodel-basedRLalgorithms.
Itisworthtopauseforamomentandcontrasttheprocedurewederivedabovewithmoretraditional
inverseRL/adversarialimitationlearningprocedures.Atthehighestlevel,wedonotexplicitlytrain
adiscriminatorandinsteadsimplyperformscorematchingontheexpertandlearnerstatedistribu-
tions. Doing so only requires the standard regression-based score-matching optimization, which
3NotethatsimilarideashavebeenusedintheofflineRLliteraturefordesigningmin-maxbasedalgorithms
forestimatingvaluefunctions(e.g.,Chen&Jiang(2019);Ueharaetal.(2021)).
4NotethatL(π,g)isasquare-lossfunctionalwithrespecttog.Sincesquarelossisstronglyconvex,FTLis
no-regret.Incontrast,whenoptimizinganIPM,onehastouseFollow-the-Regularized-Leader(FTRL)tohave
theno-regretproperty(e.g.,Sunetal.(2019);Swamyetal.(2021)),makingimplementationmorecomplicated.
7Preprint
has shown to be more stable than adversarial training in certain domains. On a more theoretical
level, the DS-Divergence we’re optimizing in our procedure is an upper-bound on the worst-case
IPM (the total variation distance) one often considers in IRL (Abbeel & Ng, 2004; Ziebart et al.,
2008; Swamy et al., 2021). On the positive side, minimizing this upper bound merely requires
score matching rather than explicitly training a discriminator. Score matching gives one stronger
theoretical guarantees w.r.t. second-order bounds than IPM-based methods, as we discuss in the
followingsection. Intuitively,thebenefitofmatchingthesecondmomentsandnotjustthefirstare
performanceboundsthatscalewiththevarianceofthedata-generatingprocess,whichcanbesig-
nificantlytighteronproblemswherestochasticityisrestrictedtoasubsetofstates(e.g. drivingon
highways, where merges are significantly more variable than actually being on the highway). On
thenegativeside,forproblemswherewecana-priorispecifyaclassofmoments(e.g. autonomous
driving, where it is relatively simple to write down the set of features a person could care about
when driving), minimizing an upper bound can be needlessly complex. Thus, we believe that as
the difficulty of specifying the class of reward functions grows, our method’s benefits over more
traditionalIPM-basedIRLtechniquesgrowsaswell.
5 THEORETICAL RESULTS
Our goal in this section is to demonstrate that SMILING achieves instance-dependent regret
bounds while at the same time avoid compounding errors when model misspecification, op-
timization error, and statistical error exist. In practice, we can only estimate the diffusion score
functionuptosomestatisticalerrororoptimizationerrorduetofinitesamples. Forsimilarreasons,
the RL procedure (Line 4) can only find a near-optimal policy especially when the policy class is
notrichenoughtocapturetheexpert’spolicy. Todemonstratethatouralgorithmcantoleratethese
errors,weexplicitlystudytheseerrorsinourtheoreticalanalysis,particuallyhowourregretbound
scaleswithrespecttotheseerrors. Weprovidethefollowingassumptionstoformalizetheseerrors.
Assumption1. Wehavethefollowingerrorbounds,correspondingrespectivelytoLines1,3and4
ofAlgorithm1:
(a) Thereexistsϵ >0suchthattheestimatorgeisaccurateuptoϵ :
score score
s∼E dπet∼UE (T)st∼qE t(·|s)(cid:2)(cid:13) (cid:13)ge(s t,t) −∇stlogpπ te (s t)(cid:13) (cid:13)2 2(cid:3) ≤ϵ2 score;
(b) There exists a function Regret(K) sublinear in K such that the sequence of score function
estimators g(k) K hasregretboundedbyRegret(K):
{ }k=1
(cid:88)K (cid:20)(cid:13) (cid:13)2(cid:21)
E E E (cid:13)g(k)(s ,t) logq (s s)(cid:13)
k=1s∼dπ(k)t∼U(T)st∼qt(·|s)
(cid:13) t −∇st t t
|
(cid:13)
2
K
min(cid:88)
E E E
(cid:104)
g(s ,t) logq (s s)
2(cid:105)
+Regret(K);
≤ g k=1s∼dπ(k)t∼U(T)st∼qt(·|s) ∥ t −∇st t t | ∥2
(c) Thereexistsϵ > 0suchthat, forallk = 1,...,K, theRLprocedurefindsanϵ -optimal
RL RL
policy within some function class Π: E c(k)(s) min E c(k)(s) ϵ . Note
thatwedonotassumeπe Π.
s∼dπ(k)
−
π∈Π s∼dπ
≤
RL
∈
Item(a)isastandardassumptionindiffusionprocessandscorematching(Chenetal.,2022;2023;
Lee et al., 2022). When the function class (hypothesis space) is finite, ϵ2 typically scales at
G score
arateofO(cid:101)(ln( )/N)intermsofthenumberofsamplesN. Foraninfinitefunctionclass,more
|G|
advanced bounds can be established (e.g., see Theorem 4.3 in Oko et al. (2023)). We emphasize
that the discretization error arising from approximating the diffusion SDE using Markov chains is
included in ϵ as well. Item (b) is similar to Item (a) but for the sequence of score function
score
estimators. ItcanbesatisfiedbyapplyingFollow-the-Leader(FTL)sincethelossfunctionalforgis
asquareloss. Otherno-regretalgorithms, suchasFollow-the-regularized-leader(FTRL)oronline
gradientdescent, canalsobeused. UndersimilarconditionsasinItem(a), onetypicallyachieves
Regret(K) = O(cid:101)(√K). Item (c) can be satisfied as long as an efficient RL algorithm is applied
8Preprint
at each iteration in Line 4. The rate of ϵ has been well-studied in the RL theory literature for
RL
(cid:112)
various of MDPs. For instance, in tabular MDPs, one can achieve ϵ
RL
= O(cid:101)( SA/M) where S
is the number of states, A is the number of actions, and M is the number of RL rollout samples.
Weemphasizethatϵ inthiscasedoesnotscalewithH asitisdefinedastheupperboundonthe
RL
averagedreturninsteadofthecumulativereturnoverH steps.
Inthetheoreticalanalysis,wewillnotassumeanymisspecificationofscorefunctions. Specifically,
foranypolicyπ,weassumethatitsscorefunctionliesinthefunctionclass (i.e., logpπ ).
However,weallowformisspecificationintheRLprocedure(i.e.,πemaynotG beinΠ∇
).
Thist en∈ abG
les
ustofocusmoreonthemisspecificationoftheRLprocedureunderourdefinedframework.
Next, we introduce the regularity assumptions on the diffusion process, which are standard in the
literatureondiffusionmodels
Assumption 2. For all π, the following two conditions hold: (1) the diffusion score function
logpπ is Lipschitz continuous with a finite constant for all t [0,T], and (2) the second mo-
m∇ entoft dπ isbounded: E (cid:2) s 2(cid:3) mforsomem>0. ∈
s∼dπ ∥ ∥2 ≤
ThefirstconditionensuresthatthescorefunctionbehaveswellsowecantransfertheDSdivergence
boundtoaKLdivergencebound. Thesecondconditionisneededfortheexponentialconvergence
of the forward diffusion process (i.e., the convergence to the standard Gaussian in terms of KL
divergence). We note that the second condition is readily satisfied in certain simple cases such as
whenthestatespaceisbounded.
Notably,ifwedisregarddiscretizationerrors,theLipschitzconstantwillnotappearinourtheoretical
results as long as it is finite, even if it is arbitrarily large. Similarly, the constant m only appears
withinlogarithmicfactorssoitcanbeexponentiallylarge. However,whendiscretizationerrorsare
considered,boththeLipschitzconstantandmwillappearinϵ andRegret(T)(see,e.g.,Chen
score
etal.(2022;2023);Okoetal.(2023)).
Now we are ready to present our main theoretical result. Let π(1:K) denote the mixture policy
of π(1),...,π(K) . Particularly, π(1:K) is executed by first choosing a policy π(k) uniformly at
{ }
randomandthenexecutingthechosenpolicy. Foranypolicyπ,wedefinethevarianceofitsreturn
as Varπ := Variance((cid:80)H c⋆(s )). Below is our main theoretical result with proof provided in
h=1 h
AppendixB.
Theorem 1. Under Assumptions 1 and 2, Algorithm 1 achieves the following instance-dependent
bounds:
(cid:18)(cid:113) (cid:19)
(Second-order)
Vπ(1:K) Vπe
=O(cid:101)
min(cid:0) Varπe,Varπ(1:K)(cid:1)
ϵ+ϵH ;
− ·
(cid:18)(cid:113) (cid:19)
(First-order)
Vπ(1:K) Vπe
=O(cid:101)
min(cid:0) Vπe,Vπ(1:K)(cid:1)
ϵH +ϵH
− ·
wherewedefinetotalerror
(cid:26) ϵ2 +ϵ +Regret(K)/K ifπe Π,
ϵ:= score RL ∈
ϵ2 +ϵ +Regret(K)/K+ϵ ifπe Π,
score RL mis ̸∈
andthemisspecificationϵ :=min ℓ(π)wherewerecallthatℓ()isdefinedinEq.2.
mis π∈Π
·
Here the misspecification error ϵ measures the minimum possible DS divergence to the pre-
mis
trainedexpertscorege.Wenotethatϵ isalgorithmic-pathindependentoncegivenge,i.e.,itdoes
mis
notdependonwhatthealgorithmdoes—itisaquantitythatispre-determinedwhentheILproblem
isformalizedandtheexpertscoregeispre-trained. IfweincreasetheexpressivenessofΠ,ϵ will
mis
decrease. Incontrast,interactiveILalgorithmsDAgger (Rossetal.,2011)andAggreVate(D)(Ross
& Bagnell, 2014; Sun et al., 2017), which claim to have no compounding errors, actually have
algorithmic-path dependent misspecification errors. This means their misspecification error does
not necessarily decrease and in fact can increase when one increases the capacity of Π (since it
affectsthealgorithm’sbehavior).
Itisknownthatthesecond-orderboundistighterandsubsumesthefirst-orderbound(Wangetal.,
2024c). Importantly, previous results on second-order bounds for sequential decision-making
9Preprint
have relied exclusively on the Maximum Likelihood Estimator (MLE) (Wang et al., 2023; 2024c;
Foster et al., 2024; Wang et al., 2024b;d), which in general is not computationally tractable even
for simple exponential family distributions (Pabbaraju et al., 2024). In this work, we show for
the first time that a computationally tractable alternative — diffusion score matching, can achieve
these instance-dependent bounds in the context of IRL. In particular, our second-order bounds (or
the first-order bound) scale with the minimum of Varπe and Varπ(1:K) (or minimum of Vπe and
Vπ(1:K)). Hence, ourboundsaresharperwheneveroneofthetwoissmall. Incontrast, boundsin
thesepriorRLworkdonotscalewiththeminofvariancesassociatedwiththelearnedpoliciesand
thecomparatorpolicyπe. ThissubtletyislikelyduetothedifferencebetweentheIRLsettingand
thegeneralRLsettinginsteadoftechniquesintheanalysis.
Improvedsampleefficiencywithrespecttothesizeofexpertdata. Let’snowfocusonsample
complexitywithrespecttoexpertdata,whichistypicallythemostexpensivedatatocollectinIL,to
highlightthebenefitsofourmethodoverpreviousapproaches. Forconciseness,weignoreallerrors
thatarepurelyrelatedtocomputation. Specifically,wesetϵ 0,asitonlydependsonthetime
RL
→
spenttoruntheRLprocedure,letthenumberofiterationsK soRegret(K)/K 0,andas-
sumeπe Π.Undertheseconditions,ourboundsolelydepend→ so∞ nϵ .Forsimplicit→ y,wefurther
score
assumea∈ finitescorefunctionclass ,neglectmisspecificationerrors(i.e., logpπ(x) forall
G ∇ t ∈ G
π),andassumethemagnitudeofthescorelossisO(1). Nowstandardregressionanalysis(Agarwal
(cid:112)
et al., 2019; Oko et al., 2023) yields ϵ = O( log( )/N) where N is the number of expert
score
|G|
samples. Pluggingthisbackintooursecond-orderbound,weobtain
K (cid:32)(cid:114) (cid:33)
1 (cid:88) Vπ(k) Vπe
=O
min(cid:0) Varπe,Varπ(1:K)(cid:1) log( |G|)
+
H ·log( |G|)
.
K − · N N
k=1
When the expert’s cost or the learned policy’s cost has low variance, i.e.,
Varπe
0 or
→
Varπ(1:K)
0,ourboundsimplifiestoO(H log( )/N). Thisshowsasignificantimprovement
→ · |G|
over prior sample complexity bounds for IRL with general function approximation, which are
(cid:112)
typicallyO(H log( )/N)(e.g.,IPM-basedmethods(Kidambietal.,2021;Changetal.,2021)).
|G|
ThisdemonstratesthebenefitofusingdiffusionprocessesandscorematchingoverIPMs.
5.1 COMPARISONTODISCRIMINATOR-BASED(F-DIVERGENCEANDIPM)METHODS
In this section, we compare DS divergence with f-divergence, particularly its discriminator-based
approximation(thekeymetricusedbyILalgorithmssuchasGAILandDAC):
(cid:2) (cid:3) (cid:2) (cid:3)
max E logf(s) + E log(1 f(s))
f∈F s∼p s∼q −
where (0,1)S isarestrictedfunctionclass. Itisknownthattheaboveisalowerboundofthe
F ⊆
Jensen-Shannon(JS)divergencebetweenpandq. Inparticular,whenp/(p+q) ,itisexactly
∈ F
equaltotheJSdivergence. However,representingp/(p+q)maybechallenging. Belowweshow
why score-matching is more preferred than discriminator-based f-divergence via the example of
exponentialfamilydistributions,
On the expressiveness of score functions and discriminators. Consider the distributions p
and q to be from the exponential family, p(s) = exp(wTϕ(s))/Z (and similar for q), where
p
ϕ is a quadratic feature map, and Z is the partition function. In this case, the score function
p
logp(s)= ϕ(s)issimplylinearins. Incontrast,theoptimaldiscriminatorforJSdivergence
s s
∇ ∇
ofpandqhastheformofp/(p+q),whichisinherentlynonlinearandcannotbecapturedbyalinear
functionons.Asaresult,ifweuselinearfunctionstomodeldiscriminators,thediscriminator-based
objectivecannotevenfaithfullyserveasatightlowerboundfortheJSdivergence.Thusminimizing
alooselowerboundoftheJS-divergencedoesnotimplyminimizingtheJS-divergenceitself.
Now consider integral probability metric (IPM), i.e., max (E f(s) E f(s)). When p
f∈F s∼q s∼p
−
andqarefromtheexponentialfamily,toperfectlydistinguishqandp,oneneedstodesignadiscrim-
inatorintheformofθ⊤ϕ(x)sinceϕisthesufficientstatisticsoftheexponentialfamilydistribution.
Inourexample,ϕisaquadraticfeaturemapping,meaningthatweneedanon-lineardiscriminator
forIPMaswell.
10Preprint
Hence, score matching only needs relatively weaker score function class to represent complicated
distributionstocontrolcertainpowerfuldivergenceincludingKLdivergenceandHellingerdistance.
However,forf-divergence-basedorIPM-basedmethods,weneedamoreexpressivediscriminator
functionclasstomakethemserveasatightlowerboundofthedivergences. Thisperhapsexplains
whyinpracticeforgenerativemodels,scorematchingbasedapproachingeneraloutperformsmore
traditional GAN based approach. In our experiment, we conduct an ablation study in Section 6.3
to compare our approach to a GAN-based baseline under linear function approximation for score
functions and discriminators, and we show that SMILING can still learn well even when the score
functionsarelinearwithrespecttothestates.
Can f-divergence or IPM-based approaches achieve second-order bounds? Wang et al.
(2024b)hasdemonstratedthatsimplymatchingexpectations(firstmoment)isinsufficienttoachieve
second-orderboundsinasimplersupervisedlearningsetting. MappingthistotheILsetting,ifthe
truecostislinearinfeatureϕ(s),i.e.,c⋆(s)=θ⋆ ϕ(s),andwedesigndiscriminatorg(s)=θ⊤ϕ(s),
then IL algorithm that learns a policy π to only· match policy’s expected feature E ϕ(s) to the
s∼π
expert’s E ϕ(s) cannot achieve a second-order regret bound. For f-divergence such as JS-
s∼πe
divergence, unless the discriminator class is rich enough to include the optimal discriminator, the
discriminator-based objective is only a loose lower bound of the true f-divergence. On the other
hand,makingthediscriminatorclassrichenoughtocapturetheoptimaldiscriminatorcanincrease
bothstatisticalandcomputationalcomplexityforoptimizingthediscriminator. Takingtheexponen-
tial family distribution as an example again, there the optimal discriminator relies on the partition
functionsZ andZ whichcanmakeoptimizationintractable(Pabbarajuetal.,2024).
q p
6 EXPERIMENTS
Weevaluateourmethod,SMILING,onasetofcontinuouscontroltasksfromtheDeepmindControl
Suite(Tassaetal.,2018)andHumanoidBench(Sferrazzaetal.,2024).Thesetasksvaryindifficulty,
assummarizedinTable1. Weelaborateontheexperimentalsetupbelow. Whilewewillcompareto
multiplebaselinesincludingnon-adversarialbaselines, wefocusontestingtheourhypothesisthat
ourscore-matchingbasedapproachshouldoutperformf-divergencebasedadversarialILapproach.
AdditionaldetailscanbefoundinAppendixC.
Baselines. We include Discriminator
Actor-Critic(DAC),IQ-Learn,andBehav- Table1: Spreadofevaluatedenvironments. DMCde-
ior Cloning (BC) as baselines. Our DAC notes DeepMind Control Suite and HB denotes Hu-
implementation is based on the original manoidBench.
designbyKostrikovetal.(2018)thatuses
JS-divergencebasedGANstyleobjective.
The implementation of IQ-Learn directly Task Difficulty Benchmark
uses the official version. We tried sev-
BallinCupCatch Easy DMC
eral configurations provided in their im-
CheetahRun Medium DMC
plementation for the MuJoCo tasks and
HumanoidWalk Hard DMC
reported the best one. The BC perfor-
HumanoidSit Hard HB
mance is reported as the maximum av-
HumanoidCrawl Hard HB
erage episode reward obtained across all
HumanoidPole Hard HB
checkpoints during training. We experi-
mented with both squared loss and MLE
forBCandfoundMLEisbetter. Consequently,allBCresultsbelowareusingMLE.
DiffusionModels. WeemploytheDenoisingDiffusionProbabilisticModel(DDPM)asthediffu-
sionmodel. ThescorefunctionismodeledviaanMLPwithonehiddenlayerof256units,andwe
usethesamearchitectureforthediscriminatorinDAC.Thediffusionprocessisdiscretizedinto5K
stepsandweusealearnabletimeembeddingof16dimensions. Weapproximatethecostfunction
(Eq.4)viaempiricalmeanusing500samples. Sincethescaleofthescorematchingloss(andcon-
sequentlyourcostfunction)issensitivetothedatascale(inthiscontext,thescaleofstatevector),
wenormalizethecostofeachbatchtohavezeromeanandastandarddeviationof0.1.
RL.ForDMCtasks,weuseSoftActor-Critic(SAC)(Haarnojaetal.,2018)astheRLalgorithmfor
both DAC and our method, based on the implementation provided by Yarats & Kostrikov (2020).
11Preprint
BallinCupCatch CheetahRun HumanoidWalk
1.00
0.75
0.50
0.25
0.00
0 1 2 3 4 0 1 2 3 0 1 2 3
Steps 105 Steps 106 Steps 106
× × ×
HumanoidSit HumanoidCrawl HumanoidPole
1.00
0.75
0.50
0.25
0.00
0 1 2 3 0 1 2 3 0 1 2 3
Steps 106 Steps 106 Steps 106
× × ×
SMILING(ours) DAC IQ-Learn πe BC
Figure 3: Learning curves for learning from state-only data across five random seeds. The x-axis
corresponds to the number of environment steps (also the number of policy updates). The y-axis
isnormalizedsuchthat theexpertperformanceisoneandthe randompolicyiszero. Ourmethod
clearlyoutperformsallbaselinesinfivetasksoutofsix,withasubstantialperformancegapagainst
baselinesinfourofthem.
For HumanoidBench tasks, we adopt DreamerV3 (Hafner et al., 2023) using the implementation
fromtheHumanoidBenchrepository(Sferrazzaetal.,2024). Additionally,wemaintainaseparate
“statebuffer”thatoperatesthesameasthereplaybufferbutonlystoresstates. Thisbufferisusedto
samplestatestoupdatethescorefunctioninourmethodandthediscriminatorinDAC.
Overall,wekeeptheimplementationofSMILINGandthebaselineDACascloseaspossible(e.g.,
usingthesameRLsolver)sotheonlydifferenceisdifferentobjectivefunctions(i.e.,SMILING
learnsscorefunctionswhileDAClearnsdiscriminatorstoapproximateJSdivergence). Thisallows
ustodemonstratethebenefitsofourscore-matchingapproachtothediscriminator-basedGAN-style
approaches. ForIQ-Learn,wesimplyusetheofficialimplementation.
ExpertPolicyandDemonstrations. WeusetheaforementionedRLalgorithms(SACandDream-
erV3) to train expert policies. Expert demonstrations are collected by executing the expert policy
in the environment for five trajectories. We set the horizon of all tasks to be 1K without early
termination,andthustheresultingdatasetconsistsof5Kstepspertask.
6.1 LEARNINGFROMSTATE-ONLYDEMONSTRATIONS
Figure3illustratestheresultsacrossfivecontroltaskswhentheexpertdemonstrationsarestate-only.
Wetrainedfor0.4MstepsonBall-in-cup-catchandfor3Mstepsonallothertasks.Ourmethod
matchesexpertperformanceandoutperformsbothDACandIQ-Learnonfiveofthem. Specifically,
ourmethodoutperformsDACandIQ-Learnbyalargemarginonfourtasks:ball-in-cup-catch,
humanoid-walk, humanoid-crawl, and humanoid-pole. For humanoid-sit, while none
achieves expert performance, ours converges faster. For cheetah-run, although DAC initially
shows faster convergence, it becomes unstable soon and oscillates significantly after 2M steps. In
contrast, our algorithm remains much more stable and surpasses DAC in 3M steps. We provide
videodemosofSMILINGandDAConthehumanoid-crawlandhumanoid-poletasksinFigure1
togiveaclearvisualcomparisonoftheirperformancedifferences.
We include BC in the plot as a reference, though it is not directly comparable since it uses state-
action demonstrations. It serves as a performance upper bound for Behavioral Cloning from Ob-
12
)dezilamron(πV
)dezilamron(πVPreprint
servation(BCO)(Torabietal.,2018),whichinferstheexpert’sactionsfromstate-onlydatabefore
training. Nevertheless,weobservethatbothourmethodandDACconsistentlyoutperformBC(and
thus,BCO).
6.2 LEARNINGFROMSTATE-ACTIONDEMONSTRATIONS
Ourmethodcanbeeasilyextendedtolearningfromstate-actiondemonstrationsbyappendingthe
actionvectortothestatevectorforbothtrainingandcomputingrewards. Hence,wealsoexploreits
performanceinsuchasetting. Theexperimentalsetupisidenticaltothatoflearningfromstate-only
data. The training curves are presented in Figure 4. The results are highly consistent with those
from the state-only setting, indicating that the performance gap is stable whether training on state
data alone or state-action pairs. This also demonstrates that SMILING is robust to different data
types.
BallinCupCatch CheetahRun HumanoidWalk
1.00
0.75
0.50
0.25
0.00
0 1 2 3 4 0 1 2 3 0 1 2 3
Steps 105 Steps 106 Steps 106
× × ×
HumanoidSit HumanoidCrawl HumanoidPole
1.00
0.75
0.50
0.25
0.00
0 1 2 3 0 1 2 3 0 1 2 3
Steps 106 Steps 106 Steps 106
× × ×
SMILING(ours) DAC IQ-Learn πe BC
Figure4: Learningcurvesforlearningfromstate-actiondataacrossfiverandomseeds. Thex-axis
correspondstothenumberofenvironmentsteps(alsothenumberofpolicyupdates). They-axisis
normalizedsuchthattheexpertperformanceisoneandtherandompolicyiszero. Theresultsare
consistentwiththosefromthestate-onlysetting.
6.3 EXPRESSIVENESSOFDIFFUSIONMODELS
In Section 5.1, we argued that score matching is more expressive than discriminator-based meth-
ods, which may explain why our approach generally outperforms DAC in previous experiments.
Tofurthersupportthisargument,weconductedanablationstudyonthecheetah-runtaskwhere
our method previously showed a slower convergence compared to DAC. Now we remove the ac-
tivationfunctionsinboththediscriminatorofDACandthescorefunctionofourmethod, making
them purely linear, and then re-run the experiments. The results are in Figure 5. Different from
the previous experiments where DAC outperformed our method, in this case, DAC’s performance
degradesnotablybyshowingslowerconvergenceandsignificantlyhighervariance. Incomparison,
ourmethodremainsmoreeffectiveandoutperformsDACclearly.
We attribute the performance degradation of DAC to the limited expressiveness of the discrimi-
nator, which results in mode collapse—a well-known issue in discriminator-based methods, orig-
inally identified in GANs. In such methods, if the discriminator’s performance falls signifi-
cantlybehindthegenerator(e.g., duetoinsufficienttrainingoralackofdiscriminatorexpressive-
ness, which is the case here), the generator will exploit this gap and produce single-mode data
13
)dezilamron(πV
)dezilamron(πVPreprint
thatcaneasilyfoolthediscriminator. Inthiscase,thegen-
erator is falling into a local minima. As the discriminator CheetahRun(LinearFunction)
gradually catches up and learns to identify this data, the 1.00
generator will discover new data to fool the updated dis-
criminator, falling into another local minimum. This per- 0.75
petual cycle of chasing leads to continuous oscillations in
the training process. We conjecture this is the exact rea- 0.50 Ours
sonforDAC’sinstabilityinthisexperiments—Withthedis- DAC
0.25
criminator reduced to a purely linear function, its expres- πe
sivenessissignificantlyweakened,allowingtheRLpolicy BC
0.00
to exploit it easily, and thus a perpetual chasing emerges.
0 1 2 3 4
However,ourmethodcanremainmoreeffectiveevenwith Steps 106
×
linearscorefunctions,whichalignswithourhypothesisin
Figure 5: Learning curves with lin-
Section5.1.
ear functions. Compared to using
MLPs, DAC shows a significant de-
7 CONCLUSION cline in performance of slower con-
vergence and higher variance, while
In this paper, we propose a new imitation learning frame- oursremainsmoreeffective.
work, Score-Matching Imitation Learning (SMILING), that
leveragesthediffusionscorefunctiontolearnfromexpertdemonstrations. Differentfromprevious
methods, our approach is not formulated from any f-divergence or IPM perspective. Instead, we
directly match the score function of the learned policy to that of the expert policy. Theoretically,
weshowthatourscorefunctionismoreexpressivethanlearningadiscriminatoronf-divergencein
somesense. Furthermore, weshowthatourmethodcanachievefirst-andsecond-orderboundsin
samplecomplexity,whichareinstance-dependentandtighterthanpreviousresults. Inpractice,we
demonstratethatourmethodoutperformsDACandIQ-Learnonasetofcontinuouscontroltasks,
especiallyonhardertasks.
REFERENCES
Pieter Abbeel and Andrew Y Ng. Apprenticeship learning via inverse reinforcement learning. In
Proceedingsofthetwenty-firstinternationalconferenceonMachinelearning,pp. 1,2004.
AlekhAgarwal, NanJiang, ShamMKakade, andWenSun. Reinforcementlearning: Theoryand
algorithms. CSDept.,UWSeattle,Seattle,WA,USA,Tech.Rep,32:96,2019.
Anurag Ajay, Yilun Du, Abhi Gupta, Joshua Tenenbaum, Tommi Jaakkola, and Pulkit Agrawal.
Is conditional generative modeling all you need for decision-making? arXiv preprint
arXiv:2211.15657,2022.
BrianDOAnderson. Reverse-timediffusionequationmodels. StochasticProcessesandtheirAp-
plications,12(3):313–326,1982.
MartinArjovskyandLéonBottou. Towardsprincipledmethodsfortraininggenerativeadversarial
networks. arXivpreprintarXiv:1701.04862,2017.
PaulBarde,JulienRoy,WonseokJeon,JoellePineau,ChrisPal,andDerekNowrouzezahrai.Adver-
sarialsoftadvantagefitting: Imitationlearningwithoutpolicyoptimization. AdvancesinNeural
InformationProcessingSystems,33:12334–12344,2020.
MattBarnes, MatthewAbueg, OliverFLange, MattDeeds, JasonTrader, DenaliMolitor, Markus
Wulfmeier, and Shawn O’Banion. Massively scalable inverse reinforcement learning in google
maps. arXivpreprintarXiv:2305.11290,2023.
AdamBlock,AliJadbabaie,DanielPfrommer,MaxSimchowitz,andRussTedrake. Provableguar-
anteesforgenerativebehaviorcloning:Bridginglow-levelstabilityandhigh-levelbehavior,2023.
URLhttps://arxiv.org/abs/2307.14619.
Kiante Brantley, Wen Sun, and Mikael Henaff. Disagreement-regularized imitation learning. In
InternationalConferenceonLearningRepresentations,2019.
14
)dezilamron(πVPreprint
Jonathan Chang, Masatoshi Uehara, Dhruv Sreenivas, Rahul Kidambi, and Wen Sun. Mitigating
covariate shift in imitation learning via offline data with partial coverage. Advances in Neural
InformationProcessingSystems,34:965–979,2021.
Jonathan Daniel Chang, Dhruv Sreenivas, Yingbing Huang, Kianté Brantley, and Wen Sun. Ad-
versarial imitation learning via boosting. In The Twelfth International Conference on Learning
Representations,2024.
HongruiChen,HoldenLee,andJianfengLu.Improvedanalysisofscore-basedgenerativemodeling:
User-friendly bounds under minimal smoothness assumptions. In International Conference on
MachineLearning,pp.4735–4763.PMLR,2023.
JinglinChenandNanJiang. Information-theoreticconsiderationsinbatchreinforcementlearning.
InInternationalConferenceonMachineLearning,pp.1042–1051.PMLR,2019.
SitanChen,SinhoChewi,JerryLi,YuanzhiLi,AdilSalim,andAnruRZhang. Samplingisaseasy
aslearningthescore: theoryfordiffusionmodelswithminimaldataassumptions. arXivpreprint
arXiv:2209.11215,2022.
Cheng Chi, Siyuan Feng, Yilun Du, Zhenjia Xu, Eric Cousineau, Benjamin Burchfiel, and Shu-
ran Song. Diffusion policy: Visuomotor policy learning via action diffusion. arXiv preprint
arXiv:2303.04137,2023.
ChelseaFinn,PaulChristiano,PieterAbbeel,andSergeyLevine. Aconnectionbetweengenerative
adversarial networks, inverse reinforcement learning, and energy-based models. arXiv preprint
arXiv:1611.03852,2016.
DylanJFoster,AdamBlock,andDipendraMisra. Isbehaviorcloningallyouneed? understanding
horizoninimitationlearning. arXivpreprintarXiv:2407.15007,2024.
Justin Fu, Katie Luo, and Sergey Levine. Learning robust rewards with adversarial inverse rein-
forcementlearning. arXivpreprintarXiv:1710.11248,2017.
Divyansh Garg, Shuvam Chakraborty, Chris Cundy, Jiaming Song, and Stefano Ermon. Iq-learn:
Inverse soft-q learning for imitation. Advances in Neural Information Processing Systems, 34:
4028–4039,2021.
SeyedKamyarSeyedGhasemipour,RichardZemel,andShixiangGu. Adivergenceminimization
perspective on imitation learning methods. In Conference on robot learning, pp. 1259–1277.
PMLR,2020.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
AaronCourville,andYoshuaBengio. Generativeadversarialnetworks. Communicationsofthe
ACM,63(11):139–144,2020.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximumentropydeepreinforcementlearning with astochasticactor. In Internationalconfer-
enceonmachinelearning,pp.1861–1870.PMLR,2018.
Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains
throughworldmodels. arXivpreprintarXiv:2301.04104,2023.
Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. Advances in neural
informationprocessingsystems,29,2016.
JonathanHo,AjayJain,andPieterAbbeel. Denoisingdiffusionprobabilisticmodels. Advancesin
neuralinformationprocessingsystems,33:6840–6851,2020.
Bo-RueiHuang,Chun-KaiYang,Chun-MaoLai,Dai-JieWu,andShao-HuaSun. Diffusionimita-
tionfromobservation. arXivpreprintarXiv:2410.05429,2024.
ArnavKumarJain,HarleyWiltzer,JesseFarebrother,IrinaRish,GlenBerseth,andSanjibanChoud-
hury. Revisitingsuccessorfeaturesforinversereinforcementlearning. InICML2024Workshop
onModelsofHumanFeedbackforAIAlignment,2024.
15Preprint
Michael Janner, Yilun Du, Joshua B Tenenbaum, and Sergey Levine. Planning with diffusion for
flexiblebehaviorsynthesis. arXivpreprintarXiv:2205.09991,2022.
OliverJohnson. Informationtheoryandthecentrallimittheorem. WorldScientific,2004.
Ioannis Karatzas and Steven Shreve. Brownian motion and stochastic calculus, volume 113.
springer,2014.
LiyimingKe,SanjibanChoudhury,MattBarnes,WenSun,GilwooLee,andSiddharthaSrinivasa.
Imitation learning as f-divergence minimization. In Algorithmic Foundations of Robotics XIV:
Proceedings of the Fourteenth Workshop on the Algorithmic Foundations of Robotics 14, pp.
313–329.Springer,2021.
Rahul Kidambi, Jonathan Chang, and Wen Sun. Mobile: Model-based imitation learning from
observationalone. AdvancesinNeuralInformationProcessingSystems,34:28598–28611,2021.
Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: A versatile
diffusionmodelforaudiosynthesis. arXivpreprintarXiv:2009.09761,2020.
Ilya Kostrikov, Kumar Krishna Agrawal, Debidatta Dwibedi, Sergey Levine, and Jonathan Tomp-
son. Discriminator-actor-critic: Addressing sample inefficiency and reward bias in adversarial
imitationlearning. arXivpreprintarXiv:1809.02925,2018.
Holden Lee, Jianfeng Lu, and Yixin Tan. Convergence for score-based generative modeling with
polynomialcomplexity. AdvancesinNeuralInformationProcessingSystems,35:22870–22882,
2022.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization
forgenerativeadversarialnetworks. arXivpreprintarXiv:1802.05957,2018.
Alfred Müller. Integral probability metrics and their generating classes of functions. Advances in
appliedprobability,29(2):429–443,1997.
Kazusato Oko, Shunta Akiyama, and Taiji Suzuki. Diffusion models are minimax optimal distri-
butionestimators. InInternationalConferenceonMachineLearning,pp.26517–26582.PMLR,
2023.
TakayukiOsa,JoniPajarinen,GerhardNeumann,JAndrewBagnell,PieterAbbeel,JanPeters,etal.
Analgorithmicperspectiveonimitationlearning. FoundationsandTrends®inRobotics,7(1-2):
1–179,2018.
ChiragPabbaraju, DhruvRohatgi, AnishPrasadSevekari, HoldenLee, AnkurMoitra, andAndrej
Risteski. Provablebenefitsofscorematching. AdvancesinNeuralInformationProcessingSys-
tems,36,2024.
TimPearce,TabishRashid,AnssiKanervisto,DaveBignell,MingfeiSun,RalucaGeorgescu,Ser-
gioValcarcelMacua,ShanZhengTan,IdaMomennejad,KatjaHofmann,etal. Imitatinghuman
behaviourwithdiffusionmodels. arXivpreprintarXiv:2301.10677,2023.
DeanAPomerleau. Alvinn: Anautonomouslandvehicleinaneuralnetwork. Advancesinneural
informationprocessingsystems,1,1988.
Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-
conditionalimagegenerationwithcliplatents. arXivpreprintarXiv:2204.06125,1(2):3,2022.
Nathan D Ratliff, J Andrew Bagnell, and Martin A Zinkevich. Maximum margin planning. In
Proceedingsofthe23rdinternationalconferenceonMachinelearning,pp.729–736,2006.
Siddharth Reddy, Anca D Dragan, and Sergey Levine. Sqil: Imitation learning via reinforcement
learningwithsparserewards. arXivpreprintarXiv:1905.11108,2019.
JuntaoRen,GokulSwamy,ZhiweiStevenWu,JAndrewBagnell,andSanjibanChoudhury. Hybrid
inversereinforcementlearning. arXivpreprintarXiv:2402.08848,2024.
16Preprint
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-
resolutionimagesynthesiswithlatentdiffusionmodels. InProceedingsoftheIEEE/CVFconfer-
enceoncomputervisionandpatternrecognition,pp.10684–10695,2022.
StephaneRossandJAndrewBagnell.Reinforcementandimitationlearningviainteractiveno-regret
learning. arXivpreprintarXiv:1406.5979,2014.
StéphaneRoss, GeoffreyGordon, andDrewBagnell. Areductionofimitationlearningandstruc-
turedpredictiontono-regretonlinelearning. InProceedingsofthefourteenthinternationalcon-
ference on artificial intelligence and statistics, pp. 627–635. JMLR Workshop and Conference
Proceedings,2011.
SilviaSapora,GokulSwamy,ChrisLu,YeeWhyeTeh,andJakobNicolausFoerster.Evil:Evolution
strategiesforgeneralisableimitationlearning. arXivpreprintarXiv:2406.11905,2024.
CarmeloSferrazza,Dun-MingHuang,XingyuLin,YoungwoonLee,andPieterAbbeel.Humanoid-
bench: Simulated humanoid benchmark for whole-body locomotion and manipulation. arXiv
preprintarXiv:2403.10506,2024.
Shai Shalev-Shwartz et al. Online learning and online convex optimization. Foundations and
Trends®inMachineLearning,4(2):107–194,2012.
YangSongandStefanoErmon.Generativemodelingbyestimatinggradientsofthedatadistribution.
Advancesinneuralinformationprocessingsystems,32,2019.
YangSong,JaschaSohl-Dickstein,DiederikPKingma,AbhishekKumar,StefanoErmon,andBen
Poole. Score-basedgenerativemodelingthroughstochasticdifferentialequations. arXivpreprint
arXiv:2011.13456,2020.
YudaSong,JAndrewBagnell,andAartiSingh. Hybridreinforcementlearningfromofflineobser-
vationalone. arXivpreprintarXiv:2406.07253,2024.
WenSun,ArunVenkatraman,GeoffreyJGordon,ByronBoots,andJAndrewBagnell. Deeplyag-
grevated: Differentiableimitationlearningforsequentialprediction. InInternationalconference
onmachinelearning,pp.3309–3318.PMLR,2017.
WenSun,AnirudhVemula,ByronBoots,andDrewBagnell. Provablyefficientimitationlearning
fromobservationalone.InInternationalconferenceonmachinelearning,pp.6036–6045.PMLR,
2019.
Gokul Swamy, Sanjiban Choudhury, J Andrew Bagnell, and Steven Wu. Of moments and match-
ing: Agame-theoreticframeworkforclosingtheimitationgap. InInternationalConferenceon
MachineLearning,pp.10022–10032.PMLR,2021.
GokulSwamy,DavidWu,SanjibanChoudhury,DrewBagnell,andStevenWu. Inversereinforce-
mentlearningwithoutreinforcementlearning.InInternationalConferenceonMachineLearning,
pp.33299–33318.PMLR,2023.
YuvalTassa,YotamDoron,AlistairMuldal,TomErez,YazheLi,DiegodeLasCasas,DavidBud-
den, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, et al. Deepmind control suite. arXiv
preprintarXiv:1801.00690,2018.
Octo Model Team, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep
Dasari, Joey Hejna, Tobias Kreiman, Charles Xu, et al. Octo: An open-source generalist robot
policy. arXivpreprintarXiv:2405.12213,2024.
FlemmingTopsoe. Someinequalitiesforinformationdivergenceandrelatedmeasuresofdiscrimi-
nation. IEEETransactionsoninformationtheory,46(4):1602–1609,2000.
FarazTorabi,GarrettWarnell,andPeterStone.Behavioralcloningfromobservation.arXivpreprint
arXiv:1805.01954,2018.
Masatoshi Uehara, Masaaki Imaizumi, Nan Jiang, Nathan Kallus, Wen Sun, and Tengyang Xie.
Finite sample analysis of minimax offline reinforcement learning: Completeness, fast rates and
first-orderefficiency. arXivpreprintarXiv:2102.02981,2021.
17Preprint
Bingzheng Wang, Guoqiang Wu, Teng Pang, Yan Zhang, and Yilong Yin. Diffail: Diffusion ad-
versarial imitation learning. In Proceedings of the AAAI Conference on Artificial Intelligence,
volume38,pp.15447–15455,2024a.
Kaiwen Wang, Kevin Zhou, Runzhe Wu, Nathan Kallus, and Wen Sun. The benefits of being
distributional: Small-loss bounds for reinforcement learning. Advances in Neural Information
ProcessingSystems,36,2023.
KaiwenWang,NathanKallus,andWenSun. Thecentralroleofthelossfunctioninreinforcement
learning. arXivpreprintarXiv:2409.12799,2024b.
Kaiwen Wang, Owen Oertell, Alekh Agarwal, Nathan Kallus, and Wen Sun. More bene-
fits of being distributional: Second-order bounds for reinforcement learning. arXiv preprint
arXiv:2402.07198,2024c.
ZhiyongWang,DongruoZhou,JohnLui,andWenSun. Model-basedrlasaminimalistapproach
tohorizon-freeandsecond-orderbounds. arXivpreprintarXiv:2408.08994,2024d.
MarkusWulfmeier,MichaelBloesch,NinoVieillard,ArunAhuja,JorgBornschein,SandyHuang,
ArtemSokolov,MattBarnes,GuillaumeDesjardins,AlexBewley,etal. Imitatinglanguagevia
scalableinversereinforcementlearning. arXivpreprintarXiv:2409.01369,2024.
Denis Yarats and Ilya Kostrikov. Soft actor-critic (sac) implementation in pytorch. https://
github.com/denisyarats/pytorch_sac,2020.
Xin Zhang, Yanhua Li, Ziming Zhang, and Zhi-Li Zhang. f-gail: Learning f-divergence for gen-
erative adversarial imitation learning. Advances in neural information processing systems, 33:
12805–12815,2020.
BrianDZiebart,AndrewLMaas,JAndrewBagnell,AnindKDey,etal. Maximumentropyinverse
reinforcementlearning. InAaai,volume8,pp.1433–1438.Chicago,IL,USA,2008.
18Preprint
A ERROR ANALYSIS OF OBJECTIVE
In this section, we show the following two things: (1) why directly replacing the diffusion score
function of π with an estimator gπ in ℓ(π) (Eq. 2) incurs large error; (2) why the error is small in
ourderivedobjectiveℓ(cid:98)(π)
Ontheonehand,replacingthescorefunctionofπwithgπ incursthefollowingerror:
(cid:104) (cid:105)
E E E ge(s ,t) gπ(s ) 2 ge(s ,t) logpπ(s ) 2
s∼dπt∼U(T)st∼qt(·|s) ∥ t − t ∥2−∥ t −∇st t t ∥2
(cid:104) (cid:105)
= E E E ( logpπ(s ) gπ(s ))(2ge(s ,t) gπ(s ) logpπ(s )) .
s∼dπt∼U(T)st∼qt(·|s) (cid:124)∇st t (cid:123)(cid:122)t − t (cid:125)(cid:124) t − (cid:123)t (cid:122)−∇st t t (cid:125)
(a) (b)
Herethescaleofterm(a)isboundedbythestatisticalerrorfromscorematching.However,thescale
ofterm(b)isunboundedsincege canarbitrarilydeviatefrombothgπ and logpπ(s ). Hence,
∇st t t
theerrorispotentiallyunbounded.
On the other hand, our derived objective is bounded. To see this, we note that our objective incur
thefollowingerror:
(cid:104) (cid:105)
E E E logpπ(s ) logq (s s) 2 gπ(s ) logq (s s) 2
s∼dπt∼U(T)st∼qt(·|s) ∥∇st t t −∇st t t | ∥2−∥ t −∇st t t | ∥2
(cid:104) (cid:105)
= E E E logpπ(s ) gπ(s ) 2 .
s∼dπt∼U(T)st∼qt(·|s) ∥∇st t t − t ∥2
HeretheequalityisbyLemma4. Weobservethatitisexactlyboundedbythestatisticalerrorfrom
scorematching.
B PROOF OF THEOREM 1
B.1 SUPPORTINGLEMMAS
ThefollowinglemmaisfromWangetal.(2023)andthefactthattriangulardiscriminationisequiv-
alenttothesquaredHellingerdistanceuptoamultiplicativeconstant:2D2 D 4D2 (Topsoe,
H ≤ △ ≤ H
2000).
Lemma 1. (Wang et al., 2023, Lemma 4.3) For two distributions p,q [0,1] over some random
∈
variable,denotetheirrespectivemeansbypandq. Then,itholdsthat
(cid:113)
p q 8 Var(p)D2 (p,q)+20D2 (p,q)
| − |≤ H H
where Var() denotes the variance, and D2 (p,q) := (cid:82) ((cid:112) p(x) (cid:112) q(x))2 dx/2 is the squared
· H −
Hellingerdistance.
Thefollowinglemmaisadaptedfromsomeknownresultsfromdiffusionprocessesandscorematch-
ing. ItshowsthataslongastheDSdivergence(Definition1)issmall,theHellingerdistanceisalso
smallundersomemildconditions. Similarresultscanbefoundinsometheoreticalpapersondiffu-
sionmodelssuchasChenetal.(2022);Okoetal.(2023).
Lemma 2. Let P ∆(Rd) and g Rd [0,T] Rd. Define Q ∆(Rd) as the distribution
∈ ∈ × → ∈
obtainedthroughthereverseprocessofdiffusionstartingfrom (0,I)bytreatingg asthe“score
N
function”oftheprocess. Specifically,Qisthedistributionofz ofthefollowingSDE:
T
dz =(z +2g(z ,T t)) dt+√2 dB , z (0,I).
t t t t 0
− ∼N
Weassumethefollowing:
(a) Forallt 0,thescore logP isLipschitzcontinuouswithfiniteconstant;
t
≥ ∇
(b) ThesecondmomentofP isupperbounded: E
(cid:2)
x
2(cid:3)
mforsomem>0.
x∼P ∥ ∥2 ≤
19Preprint
Then,if
(cid:104) (cid:105)
E E E g(x ,t) logP (x ) 2 ϵ2,
x∼Pt∼U(T)xt∼qt(·|x) ∥ t −∇xt t t ∥2 ≤
forsomeϵ>0,wehave
D2 (P,Q)=O(cid:0) Tϵ2+(m+d)exp( T)(cid:1) .
H −
ProofofLemma2. WeconsiderthefollowingthreestochasticprocessesspecifiedbySDEs:
dx =(x +2 logP (x )) dt+√2 dB , x P ;
t t T−t t t 0 T
∇ ∼
dy =(y +2g(y ,T t)) dt+√2 dB , y P ;
t t t t 0 T
− ∼
dz =(z +2g(z ,T t)) dt+√2 dB , z (0,I).
t t t t 0
− ∼N
Toclarifythenotation: thevariablesx ,y ,z abovearedefinedinthereversetimeorder,whereas
t t t
inthelemmastatement,xfollowforwardtimeorder.
BytriangleinequalityforHellingerdistance:
D2 (P,Q)=D2 (x ,z ) (cid:0) D (x ,y )+D (y ,z )(cid:1)2
H H T T ≤ H T T H T T
2D2 (x ,y )+2D2 (y ,z ).
≤ H T T H T T
Wewillboundthetwotermsseparately. SincethesquaredHellingerdistanceisupperboundedby
KLdivergence(i.e.,D2 D ),weseektoestablishtheKLdivergenceboundsinstead.
H ≤ KL
First, by Item (a) and Girsanov’s Theorem (Karatzas & Shreve, 2014) (also see, e.g., Chen et al.
(2022);Okoetal.(2023)),wehave
D2 (x ,y ) D (x y
)=O(cid:18)
T E
(cid:13)
(cid:13) logP (x ) g(x ,T
t)(cid:13) (cid:13)2(cid:19)
=O(cid:0) Tϵ2(cid:1) .
H T T ≤ KL T ∥ T ·t∼U(T)(cid:13) ∇ T−t t − t − (cid:13) 2
Next, by Item (b) and the convergence of OU process (e.g., Lemma 9 in Chen et al. (2023)), we
have:
(cid:0) (cid:1)
D (P (0,I))=O (m+d) exp( T) .
KL T
∥N · −
Bydata-processinginequalityforf-divergence,wehave
D2 (y ,z ) D2 (P , (0,I))
H T T ≤ H T N
D (P (0,I))
KL T
≤ ∥N
(cid:0) (cid:1)
=O (m+d) exp( T) .
· −
Pluggingthemback,wecompletetheproof.
Thefollowinglemmasarestandardresultsfromdiffusionprocessesandscorematching. Weshow
themhereforcompleteness.
Lemma3. Givenanyg,wehave
(cid:68) (cid:69) (cid:68) (cid:69)
E g(x ,t), logq (x ) = E E g(x ,t), logq (x x )
xt∼pt
t ∇xt t t
x0∼p0xt∼pt(xt|x0)
t ∇xt t t
|
0
where , denotestheinnerproduct.
⟨· ·⟩
20Preprint
ProofofLemma3. Itbasically followsfrom integrationbyparts. Westart fromtheleft-hand side
(LHS):
(cid:90)
LHS= g(x ,t) logp (x ) p (x ) dx
t ·∇xt t t
·
t t t
xt
(cid:90)
= g(x ,t) p (x ) dx
t ·∇xt t t t
xt
(cid:90)
=0 g(x ,t) p (x ) dx (integrationbyparts)
−
∇xt
·
t
·
t t t
xt
(cid:90) (cid:90)
= g(x ,t) p (x x ) p (x ) dx dx
−
∇xt
·
t
·
t t
|
0
·
0 0 t 0
x0 xt
(cid:90) (cid:90)
=0+ g(x ,t) p (x x ) p (x ) dx dx (integrationbypartsagain)
t ·∇xt t t
|
0
·
0 0 t 0
x0 xt
(cid:90) (cid:90)
= g(x ,t) logp (x x ) p (x x ) p (x ) dx dx
t ·∇xt t t
|
0
·
t t
|
0
·
0 0 t 0
x0 xt
=RHS.
Hence,thelemmaisproved.
Lemma4. Givenanyg,wehave
(cid:13) (cid:13)2 (cid:13) (cid:13)2
E (cid:13)g(x ,t) logp (x )(cid:13) E (cid:13)g(x ,t) logp (x x )(cid:13)
xt∼pt(cid:13) t −∇xt t t (cid:13) 2−x0∼p0,xt∼pt(xt|x0)(cid:13) t −∇xt t t
|
0 (cid:13)
2
(cid:13) (cid:13)2 (cid:13) (cid:13)2
= E(cid:13) logp (x )(cid:13) E (cid:13) logp (x x )(cid:13)
xt(cid:13) ∇xt t t (cid:13) 2−x0,xt(cid:13) ∇xt t t
|
0 (cid:13)
2
wherewenotethattheright-handsideoftheequationisindependentofg.
ProofofLemma4. Weprovethelemmabyexpandingthefirsttermontheleft-handside:
(cid:13) (cid:13)2
E (cid:13)g(x ,t) logp (x )(cid:13)
xt∼pt(cid:13) t −∇xt t t (cid:13)
2
(cid:13) (cid:13)2 (cid:68) (cid:69) (cid:13) (cid:13)2
= E (cid:13)g(x ,t)(cid:13) 2 E g(x ,t), logp (x ) + E (cid:13) logp (x )(cid:13)
xt∼pt(cid:13) t (cid:13)
2− xt∼pt
t ∇xt t t xt∼pt(cid:13) ∇xt t t (cid:13)
2
(cid:13) (cid:13)2 (cid:68) (cid:69) (cid:13) (cid:13)2
= E (cid:13)g(x ,t)(cid:13) 2 E g(x ,t), logp (x x ) +E(cid:13) logp (x )(cid:13) (Lemma3)
x0,xt(cid:13) t (cid:13)
2− x0,xt
t ∇xt t t
|
0 xt(cid:13) ∇xt t t (cid:13)
2
(cid:13) (cid:13)2 (cid:68) (cid:69) (cid:13) (cid:13)2
= E (cid:13)g(x ,t)(cid:13) 2 E g(x ,t), logp (x x ) + E (cid:13) logp (x x )(cid:13)
x0,xt(cid:13) t (cid:13)
2− x0,xt
t ∇xt t t
|
0 x0,xt(cid:13) ∇xt t t
|
0 (cid:13)
2
(cid:13) (cid:13)2 (cid:13) (cid:13)2
E (cid:13) logp (x x )(cid:13) +E(cid:13) logp (x )(cid:13)
−x0,xt(cid:13) ∇xt t t
|
0 (cid:13)
2
xt(cid:13) ∇xt t t (cid:13)
2
(cid:13) (cid:13)2 (cid:13) (cid:13)2 (cid:13) (cid:13)2
= E (cid:13)g(x ,t) logp (x x )(cid:13) E (cid:13) logp (x x )(cid:13) +E(cid:13) logp (x )(cid:13) .
x0,xt(cid:13) t −∇xt t t
|
0 (cid:13) 2−x0,xt(cid:13) ∇xt t t
|
0 (cid:13)
2
xt(cid:13) ∇xt t t (cid:13)
2
Thiscompletestheproof.
B.2 STATISTICALRESULTS
Fortheeaseofpresentation,wewillleveragethefollowingquantities:
(cid:104) (cid:105)
(π,g)= E E E ge(s ,t) logq (s s) 2 g(s ,t) logq (s s) 2
L s∼dπt∼U(T)st∼qt(·|s) ∥ t −∇st t t | ∥2−∥ t −∇st t t | ∥2
ByLemma4,theaboveisequivalenttothefollowingbyreplacingtheconditionalscorefunctions
withthemarginalonesforbothtermsintheexpectation:
(cid:104) (cid:105)
(π,g)= E E E ge(s ,t) logpπ(s ) 2 g(s ,t) logpπ(s ) 2 .
L s∼dπt∼U(T)st∼qt(·|s) ∥ t −∇st t t ∥2−∥ t −∇st t t ∥2
(5)
21Preprint
Onecanshowthat,fixingapolicyπ,wehave
(cid:104) (cid:105)
max (π,g)= E E E ge(s ,t) logpπ(s ) 2 =ℓ(π) (6)
g L s∼dπt∼U(T)st∼qt(·|s) ∥ t −∇st t t ∥2
wherethefirstequalityisbyobservingthatthesecondterminEq.(5)canbeminimizedtozeroby
choosingg = logpπ(s )(recallingthatwedonotassumemisspecificationforscorefunctions),
∇st t t
andthesecondequalityisbydefinition.
Boundingthedifferencebetweenπ(1:K)andπeiswhatweaimtodointhefollowinglemma.
Lemma5. UnderAssumptions1and2,assumingmisspecificationofRL(i.e.,πemaynotbeinΠ),
wehave
(cid:18) (cid:19)
T Regret(K)
D2 (dπ(1:K) ,dπe )=O T ϵ2 +T ϵ +T ϵ + · +(m+d)exp( T) .
H · score · mis · RL K −
SettingT =log( m+d )yieldsthefollowingbound:
ϵ2 score+ϵRL+ϵmis
(cid:18) (cid:19)
Regret(K)
D H2 (dπ(1:K) ,dπe )=O(cid:101) ϵ2 score+ϵ mis+ϵ RL+
K
whereO(cid:101)hideslogarithmicfactors.
ProofofLemma5. Items(b)and(c)inAssumption1impliesthefollowing:
K K
(cid:88) (cid:88)
max (π(k),g) (π(k),g(k))+Regret(K); (7)
g L ≤ L
k=1 k=1
(π(k),g(k)) min (π,g(k)) ϵ . (8)
RL
L −π∈ΠL ≤
CombiningEquation(7)andEquation(8),wehave
K K
(cid:88) (cid:88)
max (π(k),g) min (π,g(k))+K ϵ +Regret(K).
RL
g L ≤ π∈ΠL ·
k=1 k=1
Nowtheright-handsidecanbeupperboundedbyreplacingallg(k)withmaximizationoverg:
K K
(cid:88) (cid:88)
max (π(k),g) minmax (π,g)+K ϵ +Regret(K)
RL
g L ≤ π∈Π g L ·
k=1 k=1
K
(cid:88)
= minℓ(π)+K ϵ +Regret(K) (byEq. 6)
RL
π∈Π ·
k=1
=K ϵ +K ϵ +Regret(K) (9)
mis RL
· ·
Notethattheleft-handsideisequivalenttothefollowing
K
(cid:88)
max (π(k),g)=K max (π(1:K),g)=K ℓ(π(1:K)).
g L · g L ·
k=1
wherethefirstequalityisbythedefinitionof . Insertingitbacktothepreviousinequalityyields
L
anupperboundonℓ(π(1:K)):
Regret(K)
ℓ(π(1:K)) ϵ +ϵ +
≤ mis RL K
Nowwedefineanewdistributiontofacilitatetheanalysis: letd(cid:98)πe denotethedistributioninduced
bythereversediffusionprocessstartingfrom (0,I)bytreatinggeasthe“scorefunction”.Specif-
N
ically,d(cid:98)πe isthedistributionofs
T
ofthefollowingSDE:
ds =(s +2ge(s ,T t)) dt+√2 dB , s (0,I)
t t t t 0
− ∼N
22Preprint
wherewerecallthatB tisthestandardBrownianmotion. Wecanseethatd(cid:98)πe isapproximatingdπe.
Then,weinvokeLemma2andget
(cid:18) (cid:19)
T Regret(K)
D H2 (dπ(1:K) ,d(cid:98)πe )=O T ·ϵ mis+T ·ϵ RL+ ·
K
+(m+d)exp( −T) .
NowitremainstoboundD2 (dπe,d(cid:98)πe)sothatwecanapplythetriangleinequalitytogetanupper
H
boundonD2 (dπ(1:K),dπe). Sinceℓ(πe) ϵ2 (Item(a)inAssumption1),weinvokeLemma2
H ≤ score
againandimmediatelyget
(cid:18) (cid:19)
D2 (dπe ,d(cid:98)πe ) O T ϵ2 +(m+d)exp( T) .
H ≤ · score −
Weconcludetheproofbyputtingthemtogetherviatriangleinequality:
D2 (dπ(1:K) ,dπe ) 2D2 (dπ(1:K) ,d(cid:98)πe )+2D2 (d(cid:98)πe ,dπe )
H ≤ H H
(cid:18) (cid:19)
T Regret(K)
=O T ϵ +T ϵ2 +T ϵ + · +(m+d)exp( T) .
· mis · score · RL K −
Lemma6. UnderthesameconditionasLemma5butassumingnomisspecification(i.e.,πe Π),
∈
wehave
(cid:18) (cid:19)
Regret(K)
D H2 (dπ(1:K) ,dπe )=O(cid:101) ϵ2 score+ϵ RL+
K
ProofofLemma6. TheproofisalmostidenticaltothatofLemma5exceptthat, whenthereisno
misspecification, min ℓ(π) inEq. 9canbesimply upperboundedby ℓ(πe), whichis bounded
π∈Π
byϵ2 (Item(a)inAssumption1). Therestoftheproofisthesame.
score
B.3 MAINPROOF
Inthissection,weprovidethemainproofofTheorem1.First,thefollowingisbydefinitionofvalue
functions:
(cid:32) (cid:33)
Vπ(1:K) Vπe =H E c⋆(s,a) E c⋆(s,a)
− · s,a∼dπ(1:K) −s,a∼dπe
ByLemma1,itisboundedby
Vπ(1:K) Vπe
(10)
−
(cid:32) (cid:115) (cid:18) (cid:19) (cid:33)
H 8 Var E c⋆(s,a) D2 (c⋆ dπ(1:K),c⋆ dπe)+20D2 (c⋆ dπ(1:K) ,c⋆ dπe )
≤ · s,a∼dπe · H ∼ ∼ H ∼ ∼
(cid:113)
=8 Varπe D2 (c⋆ dπ(1:K),c⋆ dπe)+20HD2 (c⋆ dπ(1:K) ,c⋆ dπe )
· H ∼ ∼ H ∼ ∼
where c⋆ dπ(1:K) (and c⋆ dπe) denotes the distribution of cost rather than the distribution of
dπ(1:K) (an∼ ddπe)itself. Now∼ weapplythedataprocessinginequalityandget
(cid:113)
Vπ(1:K) Vπe 8 Varπe D2 (dπ(1:K),dπe)+20HD2 (dπ(1:K) ,dπe ).
− ≤ · H H
PluggingintheboundfromLemma5ifthereismisspecificationerrororLemma6otherwise, we
get
(cid:18) (cid:19)
(cid:112)
Vπ(1:K) Vπe =O(cid:101) Varπe ϵ+ϵH .
− ·
Similarly,wecanderivethesecondhalfofthesecond-orderbound(withVπ(1:K) replacedwithVπe
inside the square root) by applying Lemma 1 to Eq. 10 in the other direction. These conclude the
proofofthesecond-orderbound.
Thefirst-orderboundisadirectconsequenceofthesecond-orderboundasshowninTheorem2.1
inWangetal.(2023).
23Preprint
C DETAILS OF EXPERIMENTS
Implementation of RL Algorithms. We use the SAC implementation from Yarats & Kostrikov
(2020)andtheDreamerV3implementationfromSferrazzaetal.(2024)withoutmodifyingtheirhy-
perparameters. Theonlychangewemadeissubstitutingtheirground-truthcost(reward)functions
withourcostfunction. Forexample, whenevertheSACagentsamplesabatchoftransitionsfrom
thereplaybuffer,wereplacethecorrespondingcostvalueswiththosedefinedinouralgorithm. We
dothesameforDreamerV3.
ImplementationofOurMethodandDAC.Wemadeeffortstoaligntheimplementationdetailsof
bothalgorithmstoensureafaircomparison.Acomprehensivelistofhyperparametersisprovidedin
Table2. WeperformoneupdateofthescorefunctioninourmethodandthediscriminatorinDAC
per1KRLsteps. Theexpertdiffusionmodelistrainedfor4Kepochsusing5Kexpertstates(and
actions,iflearningfromstate-actionpairs).
Implementation of IQ-Learn. We used the official implementation from Garg et al. (2021). We
notethat,unlikeSMILINGandDACthatcanbebuiltonanyRLalgorithm,IQ-Learnisexclusively
tiedtoSAC.ThismakesitunclearhowtoimplementitontopofDreamerV3,aswedidforSMILING
andDAC.Therefore,weadheredtotheirSACimplementationtorunallexperimentsincludingthe
HumanoidBenchtasks.
Table2: HyperparametersofourmethodandDAC.Sharedcolumnsindicatethesamehyperparam-
eters.
Hyperparameter Ours DAC
NeuralNet MLP(1hiddenlayerw/256units)
Learningrate 5 10−3
×
Samplesusedperupdate 100,000
Batchsize 1,024
Discretizationsteps 5,000 N/A
Timeembeddingsize 16 N/A
24