Preprint
DEPTHSPLAT: CONNECTING GAUSSIAN SPLATTING
AND DEPTH
HaofeiXu1,2 SongyouPeng1 FangjinhuaWang1 HermannBlum1 DanielBarath1
AndreasGeiger2 MarcPollefeys1,3
1ETHZurich 2UniversityofTübingen,TübingenAICenter 3Microsoft
Better
Geometry&Rendering
Depth 3DGS
Unsupervised Pre-training
Image 1: Depth Image 2: Depth Novel View Validation curves of depth prediction error
Figure1: DepthSplatenablescross-taskinteractionsbetweenGaussiansplattinganddepth.
Left: BetterdepthleadstoimprovedGaussiansplattingreconstruction. Right: Unsuperviseddepth
pre-trainingwithGaussiansplattingleadstoreduceddepthpredictionerror.
ABSTRACT
Gaussiansplattingandsingle/multi-viewdepthestimationaretypicallystudiedin
isolation. Inthispaper,wepresentDepthSplattoconnectGaussiansplattingand
depthestimationandstudytheirinteractions. Morespecifically,wefirstcontribute
arobustmulti-viewdepthmodelbyleveragingpre-trainedmonoculardepthfea-
tures,leadingtohigh-qualityfeed-forward3DGaussiansplattingreconstructions.
WealsoshowthatGaussiansplattingcanserveasanunsupervisedpre-training
objectiveforlearningpowerfuldepthmodelsfromlarge-scaleunlabelleddatasets.
WevalidatethesynergybetweenGaussiansplattinganddepthestimationthrough
extensiveablationandcross-tasktransferexperiments. OurDepthSplatachieves
state-of-the-artperformanceonScanNet,RealEstate10KandDL3DVdatasetsin
termsofbothdepthestimationandnovelviewsynthesis,demonstratingthemutual
benefitsofconnectingbothtasks. Ourcode,models,andvideoresultsareavailable
athttps://haofeixu.github.io/depthsplat/.
1 INTRODUCTION
Novelviewsynthesis(Buehleretal.,2001;Zhouetal.,2018)anddepthprediction(Schönberger
etal.,2016;Eigenetal.,2014)aretwofundamentaltasksincomputervision,servingasthedriving
forcebehindnumerousapplicationsrangingfromaugmentedrealitytoroboticsandautonomous
driving. Therehavebeennotableadvancementsinbothareasrecently.
Fornovelviewsynthesis,3DGaussianSplatting(3DGS)(Kerbletal.,2023)hasemergedasapopular
techniqueduetoitsimpressivereal-timeperformancewhileattaininghighvisualfidelity. Recently,
advancesinfeed-forward3DGSmodels(Charatanetal.,2024;Chenetal.,2024;Szymanowiczetal.,
2024)havebeenintroducedtoalleviatetheneedfortediousper-sceneoptimization,alsoenabling
few-view3Dreconstruction. Thestate-of-the-artsparse-viewmethodMVSplat(Chenetal.,2024)
reliesonfeaturematching-basedmulti-viewdepthestimation(Xuetal.,2023)tolocalizethe3D
Gaussian positions, which makes it suffer from similar limitations (e.g., occlusions, texture-less
regions,andreflectivesurfaces)asothermulti-viewdepthmethods(Schönbergeretal.,2016;Yao
etal.,2018;Guetal.,2020;Wangetal.,2021;Duzcekeretal.,2021).
1
4202
tcO
71
]VC.sc[
1v26831.0142:viXra
TG
&
tupnI
talpSVM
talpShtpeDPreprint
Ontheotherhand,significantprogresshasbeenmadeinmonoculardepthestimation,withrecent
models (Yang et al., 2024a; Ke et al., 2024; Yin et al., 2023; Fu et al., 2024; Ranftl et al., 2020;
Eftekharetal.,2021)achievingrobustpredictionsondiversein-the-wilddata. However,thesedepths
typicallylackconsistencyacrossviews,constrainingtheirperformanceindownstreamtasks(Wang
etal.,2023;Yinetal.,2022). Inaddition,bothstate-of-the-artmulti-view(Caoetal.,2022;Xuetal.,
2023)andmonocular(Yangetal.,2024a;Piccinellietal.,2024;Yangetal.,2024b)depthmodelsare
trainedwithgroundtruthdepthsupervision,whichpreventsexploitinglargeunlabelleddatasetsfor
morerobustdepthpredictions.
Theintegrationof3DGSwithsingle/multi-viewdepthestimationpresentsacompellingsolution
to overcome the individual limitations of each technique while at the same time enhancing their
strengths. Tothisend,weintroduceDepthSplat,whichexploitsthecomplementarynatureofsparse-
viewfeed-forward3DGSandrobustsingle/multi-viewdepthestimationtoimprovetheperformance
forbothtasks.
Specifically,wefirstcontributearobustmulti-viewdepthmodelbyintegratingpre-trainedmonocular
depth features (Yang et al., 2024b) to the multi-view feature matching branch, which not only
maintains the consistency of multi-view depth models but also leads to more robust results in
situationsthatarehardtomatch(e.g.,occlusions,texture-lessregionsandreflectivesurfaces). The
predictedmulti-viewdepthmapsarethenunprojectedto3DastheGaussiancenters,andweusean
additionallightweightnetworktopredictotherremainingGaussianparameters. Theyarecombined
togethertoachievenovelviewsynthesiswiththesplattingoperation(Kerbletal.,2023).
Thankstoourimprovedmulti-viewdepthmodel,thequalityofnovelviewsynthesiswithGaussian
splattingisalsosignificantlyenhanced(seeFig.1left). Inaddition,ourGaussiansplattingmoduleis
fullydifferentiable,whichrequiresonlyphotometricsupervisiontooptimizeallmodelcomponents.
Thisprovidesanew,unsupervisedwaytopre-traindepthpredictionmodelsonlarge-scaleunlabeled
datasetswithoutrequiringgroundtruthgeometryinformation. Thepre-traineddepthmodelcanbe
furtherfine-tunedforspecificdepthtasksandachievessuperiorresultsovertrainingfromscratch
(seeFig.1right,whereunsupervisedpre-trainingleadstoimprovedperformance).
Weconductextensiveexperimentsonthelarge-scaleTartanAir(Wangetal.,2020),ScanNet(Dai
et al., 2017) and RealEstate10K (Zhou et al., 2018) datasets for depth estimation and Gaussian
splattingtasks,aswellastherecentlyintroducedDL3DV(Lingetal.,2023)dataset,whichfeatures
complex real-world scenes and thus is more challenging. Under various evaluation settings, our
DepthSplatachievesstate-of-the-artresults. Thestrongperformanceonbothtasksdemonstratesthe
mutualbenefitsofconnectingGaussiansplattinganddepthestimation.
2 RELATED WORK
Multi-ViewDepthEstimation. Asacorecomponentofclassicalmulti-viewstereopipelines(Schön-
bergeretal.,2016),multi-viewdepthestimationexploitsmulti-viewphotometricconsistencyacross
multipleimagestoperformfeaturematchingandpredictthedepthmapofthereferenceimage. Re-
cently,manylearning-basedmethods(Yaoetal.,2018;Guetal.,2020;Wangetal.,2021;Duzceker
etal.,2021;Wangetal.,2022;Dingetal.,2022;Caoetal.,2022)havebeenproposedtoimprove
depthaccuracy. Forexample,MVSNet(Yaoetal.,2018)usestheplane-sweepalgorithm(Collins,
1996)tobuilda3Dcostvolume,regularizesitwitha3DCNN,andthenregressesthedepthmap.
Thoughtheselearning-basedmethodssignificantlyimprovethedepthqualitywhencomparedto
traditional methods (Galliani et al., 2015; Schönberger et al., 2016; Xu & Tao, 2019), they can-
nothandlechallengingsituationswherethemulti-viewphotometricconsistencyassumptionisnot
guaranteed,e.g.,occlusions,low-texturedareas,andnon-Lambertiansurfaces.
MonocularDepthEstimation. Recently,wehavewitnessedsignificantprogressindepthestimation
fromasingleimage(Ranftletal.,2020;Bhatetal.,2023;Yinetal.,2023;Yangetal.,2024a;Ke
etal.,2024),andexistingmethodscanproducesurprisinglyaccurateresultsondiversein-the-wild
data. However,monoculardepthmethodsinherentlysufferfromscaleambiguitiesandarenotable
to produce multi-view consistent depth predictions, which are crucial for downstream tasks like
3Dreconstruction(Yinetal.,2022)andvideodepthestimation(Wangetal.,2023). Inthispaper,
weleveragethepowerfulfeaturesfromapre-trainedmonoculardepthmodel(Yangetal.,2024b)
to augment feature-matching based multi-view depth estimation, which not only maintains high
2Preprint
multi-viewconsistencybutalsoleadstosignificantlyimprovedrobustnessinchallengingsituations
suchaslow-texturedregionsandreflectivesurfaces.
MonocularandMulti-ViewDepthFusion. Severalpreviousmethods(Baeetal.,2022;Lietal.,
2023;Yangetal.,2022;Chengetal.,2024)trytofusemonocularandmulti-viewdepthstoalleviatethe
limitationsoffeaturematching-basedmulti-viewdepthestimations. However,theyeitherfusesingle-
andmulti-viewdepthpredictionswithadditionalnetworksorrelyonsophisticatedarchitectures. In
contrast,weidentifythepowerofoff-the-shellpre-trainedmonoculardepthmodelsandproposeto
augmentmulti-viewcostvolumeswithmonocularfeatures,whichnotonlyleadstoasimplemodel
architecturebutalsostrongperformance.
Feed-ForwardGaussianSplatting. Severalfeed-forward3DGaussiansplattingmodels(Charatan
etal.,2024;Szymanowiczetal.,2024;Chenetal.,2024;Weweretal.,2024;Tangetal.,2024;Xu
etal.,2024b;Zhangetal.,2024)havebeenproposedinliteraturethankstoitsefficiencyandabilityto
handlesparseviews.Inparticular,pixelSplat(Charatanetal.,2024)andSplatterImage(Szymanowicz
etal.,2024)predict3DGaussiansfromimagefeatures,whileMVSplat(Chenetal.,2024)encodesthe
featurematchinginformationwithcostvolumesandachievesbettergeometry. However,itinherently
suffers from the limitation of feature matching in challenging situations like texture-less regions
andreflectivesurfaces. Inthispaper,weproposetointegratemonocularfeaturesfrompre-trained
monoculardepth modelsformore robustdepth predictionandGaussian splatting reconstruction.
We additionally study the interactions between depth and Gaussian splatting tasks. Another line
ofworklikeLGM(Tangetal.,2024),GRM(Xuetal.,2024b),andGS-LRM(Zhangetal.,2024)
reliessignificantlyonthetrainingdataandcompute,discardingexplicitfeaturematchingcuesand
learningeverythingpurelyfromdata. Thismakesthemexpensivetotrain(e.g.,GS-LRM(Zhang
etal.,2024)istrainedwith64A100GPUsfor2days),whileourmodelcanbetrainedin1daywith
8GPUs. Moreover,ourGaussiansplattingmodule,inreturn,enablespre-trainingdepthmodelfrom
large-scaleunlabelleddatasetswithouttheneedforgroundtruthdepths.
DepthandGaussianSplatting. Inadditiontotheexperimentalsetup(feed-forward)studiedinthis
paper,recently,therehasbeenanotherlineofwork(Chungetal.,2024;Turkulainenetal.,2024)
whichappliesanadditionaldepthlossintheGaussiansplattingoptimizationprocess. Weremark
thatthesetwoapproaches(feed-forwardvs. per-sceneoptimization)areorthogonal. Inparticular,
ourapproachfocusesonexploringtheadvancednetworkarchitecturesandthepoweroflarge-scale
trainingdata,whiletheoptimizationmethodsmainlystudytheeffectoflossfunctionsforregularizing
theoptimizationprocess.
3 DEPTHSPLAT
GivenN inputimages{Ii}N ,(Ii ∈ RH×W×3, whereH andW aretheimagesizes)withcor-
i=1
respondingprojectionmatrices{P }N , (P ∈ R3×4, computedfromtheintrinsicandextrinsic
i i=1 i
matrices),ourgoalistopredictdenseper-pixeldepthD ∈RH×W andper-pixelGaussianparam-
i
eters{(µ ,α ,Σ ,c )}H×W×N foreachimage,whereµ ,α ,Σ andc arethe3DGaussian’s
j j j j j=1 j j j j
position,opacity,covariance,andcolorinformation. AsshowninFig.2,thecoreofourmethodisa
multi-viewdepthmodelaugmentedwithmonoculardepthfeatures,whereweobtainthepositionµ
j
ofeachGaussianbyunprojectingdepthto3Dwithcameraparameters,andotherGaussianparameters
arepredictedbyanadditionallightweighthead.
Morespecifically,ourdepthmodelconsistsoftwobranches: oneformodelingfeaturematching
informationusingcostvolumes,andanotherforextractingmonocularfeaturesfromapre-trained
monoculardepthnetwork. Thecostvolumesandmonocularfeaturesareconcatenatedtogetherfor
subsequentdepthregressionwitha2DU-Netandasoftmaxlayer. Forthedepthtask,wetrainour
depthmodelwithgroundtruthdepthsupervision. Ourfullmodelfornovelviewsynthesisistrained
withthephotometricrenderingloss,whichcanalsobeusedasanunsupervisedpre-trainingstagefor
thedepthmodel. Inthefollowing,weintroducetheindividualcomponents.
3.1 MULTI-VIEWFEATUREMATCHING
Inthisbranch,weextractmulti-viewfeatureswithamulti-viewTransformerarchitectureandthen
buildmultiplecostvolumesthatcorrespondtoeachinputview.
3Preprint
Multi-View CCCooosst st t
Transformer
VVVooollu luummmeeesss
Multi-View Branch
Unproject GT Novel View
CCCooonnncccaaatteteennnaaattetee 2D U-Net
Loss
Render
Multi-View Images
Per-View MMMoon ono noc ocu cul ulaalr ar r
ViT FFFeea eat atuutr ure res ess
Monocular Branch 3D Gaussians Novel View
Figure2:DepthSplatconnectsdepthestimationand3DGaussiansplattingwithasharedarchitecture,
which enables cross-task transfer. In particular, DepthSplat consists of a multi-view branch to
modelfeature-matchinginformationandasingle-viewbranchtoextractmonocularfeatures. The
per-view cost volumes and monocular features are concatenated for depth regression with a 2D
U-Netarchitecture. Forthedepthestimationtask,wetrainthedepthmodelwithgroundtruthdepth
supervision. FortheGaussiansplattingtask,wefirstunprojectalldepthmapsto3DastheGaussian
centers, andinparallel, weuseanadditionalheadtopredicttheremainingGaussianparameters.
Novelviewsarerenderedwiththesplattingoperation. Thefullmodelfornovelviewsynthesisis
trainedwiththephotometricrenderingloss,whichcanalsobeusedasanunsupervisedpre-training
stageforthedepthmodel.
Multi-ViewFeatureExtraction. ForN inputimages,wefirstusealightweightweight-sharing
ResNet(Heetal.,2016)architecturetogets×downsampledfeaturesforeachimageindependently.
Tohandledifferentimageresolutions,wemakethedownsamplingfactorsflexiblebycontrollingthe
numberofstride-23×3convolutions. Forexample,thedownsamplingfactorsis4iftwostride-2
convolutionsareusedand8ifthreeareused. Toexchangeinformationacrossdifferentviews,weuse
amulti-viewSwinTransformer(Liuetal.,2021;Xuetal.,2022;2023)whichcontainssixstacked
self- and cross-attention layers to obtain multi-view-aware features {F i}N i=1, Fi ∈ RH s×W s ×C,
where C is the feature dimension. More specifically, cross-attention is performed between each
referenceviewandotherviews. Whenmorethantwoimages(N >2)aregivenasinput,weperform
cross-attention between each reference view and its top-2 nearest neighboring views, which are
selectedbasedontheircamerapositiondistancestothereferenceview. Thismakesthecomputation
tractablewhilemaintainingcross-viewinteractions.
FeatureMatching. Weencodethefeaturematchinginformationacrossdifferentviewswiththe
plane-sweepstereoapproach(Collins,1996;Xuetal.,2023). Morespecifically,foreachviewi,we
firstuniformlysampleDdepthcandidates{d }D fromthenearandfardepthrangesandthen
m m=1
warpthefeatureFj ofviewj toviewiwiththecameraprojectionmatrixandeachdepthcandidate
d . ThenweobtainDwarpedfeatures{Fj→i}D thatcorrespondtofeatureFi. Wethenmeasure
m dm m=1
theirfeaturecorrelationswiththedot-productoperation(Xu&Zhang,2020;Chenetal.,2024). The
costvolumeC i ∈RH s×W s ×D forimageiisobtainedbystackingallcorrelations. Accordingly,we
obtaincostvolumes{C }N forallinputimages{I }N . Formorethantwoinputviews,similarto
i i=1 i i=1
thestrategyincross-viewattentioncomputation,weselectthetop-2nearestviewsforeachreference
viewandcomputefeaturecorrelationswithonlytheselectedviews. Thisenablesourcostvolume
constructiontoachieveagoodspeed-accuracytrade-offandscaleefficientlytoalargernumberof
inputviews. Thecorrelationvaluesforthetwoselectedviewsarecombinedwithaveraging.
3.2 MONOCULARDEPTHFEATUREEXTRACTION
Despitetheremarkableprogressinmulti-viewfeaturematching-baseddepthestimation(Yaoetal.,
2018;Wangetal.,2022;Xuetal.,2023)andGaussiansplatting(Chenetal.,2024),theyinherently
sufferfromlimitationsinchallengingsituationslikeocclusions,texture-lessregions,andreflective
surfaces. Thus,weproposetointegratepre-trainedmonoculardepthfeaturesintothecostvolume
representationtohandlescenariosthatarechallengingorimpossibletomatch.
Morespecifically, weleveragethe pre-trainedmonoculardepthbackbone fromtherecentDepth
Anything(Yangetal.,2024b)modelthankstoitsimpressiveperformanceondiversein-the-wilddata.
ThemonocularbackboneisaViT(Dosovitskiyetal.,2020;Oquabetal.,2023)model,whichhasa
4Preprint
patchsizeof14andoutputsafeaturemapthatis1/14spatialresolutionoftheoriginalimage. We
simplybilinearlyinterpolatethespatialresolutionofthemonocularfeaturestothesameresolution
asthecostvolumeinSec.3.1andobtainthemonocularfeatureF mi ono ∈RH s×W s ×Cmono forinput
imageI ,whereC isthedimensionofthemonocularfeature. Thisprocessisperformedforall
i mono
inputimagesinparallelandweobtainmonocularfeatures{F mi ono ∈RH s×W s ×Cmono}N i=1,whichare
subsequentlyusedforper-viewdepthmapestimations.
3.3 FEATUREFUSIONANDDEPTHREGRESSION
Toachieverobustandmulti-viewconsistentdepthpredictions,wecombinethemonocularfeature
F mi ono ∈RH s×W s ×Cmono andcostvolumeC i ∈RH s×W s ×D viasimpleconcatenationinthechannel
dimension. Asubsequent2DU-Net(Rombachetal.,2022;Ronnebergeretal.,2015)isusedto
regressdepthfromtheconcatenatedmonocularfeaturesandcostvolumes. Thisprocessisperformed
foralltheinputimagesinparallelandforeachimage,itoutputsatensorofshape H × W ×D,
s s
whereDisthenumberofdepthcandidates. WethennormalizetheDdimensionwiththesoftmax
operationandperformaweightedaverageofalldepthcandidatestoobtainthedepthoutput.
Wealsoapplyahierarchicalmatching(Guetal.,2020)architecturewhereanadditionalrefinement
stepat2×higherfeatureresolutionisemployedtoimprovetheperformancefurther.Morespecifically,
basedonthecoarsedepthprediction,weperformacorrespondencesearchonthe2×higherfeature
mapswithintheneighborsofthe2×upsampledcoarsedepthprediction. Sincewealreadyhavea
coarsedepthprediction,weonlyneedtosearchasmallerrangeatthehigherresolution,andthus,we
constructasmallercostvolumecomparedtothecoarsestage. Sucha2-scalehierarchicalarchitecture
notonlyleadstoimprovedefficiencysincemostcomputationisspentonlowresolution,butalso
leadstobetterresultsthankstotheuseofhigher-resolutionfeatures(Guetal.,2020). Similarfeature
fusionanddepthregressionprocedureisusedtogethigher-resolutiondepthpredictions,whichare
subsequentlyupsampledtothefullresolutionwithalearnedupsampler(Ranftletal.,2021).
3.4 GAUSSIANPARAMETERPREDICTION
Forthetaskof3DGaussiansplatting,wedirectlyunprojecttheper-pixeldepthmapsto3Dwith
thecameraparametersastheGaussiancentersµ . Weappendanadditionallightweightnetworkto
j
predictotherGaussianparametersα ,Σ ,c ,whichareopacity,covariance,andcolor,respectively.
j j j
Withallthepredicted3DGaussians,wecanrendernovelviewimageswiththeGaussiansplatting
operation(Kerbletal.,2023).
3.5 TRAININGLOSS
We study the properties of our proposed model on two tasks: depth estimation and novel view
synthesiswith3DGaussiansplatting(Kerbletal.,2023). Thelossfunctionsareintroducedbelow.
Depthestimation. Wetrainourdepthmodel(withouttheGaussiansplattinghead)withℓ lossand
1
gradientlossbetweentheinversedepthsofpredictionandgroundtruth:
L =α·|D −D |+β·(|∂ D −∂ D |+|∂ D −∂ D |), (1)
depth pred gt x pred x gt y pred y gt
where ∂ and ∂ denotes the gradients on the x and y directions, respectively. Following Uni-
x y
Match(Xuetal.,2023),weuseα=20andβ =20.
View synthesis. We train our full model with a combination of mean squared error (MSE) and
LPIPS(Zhangetal.,2018)lossesbetweenrenderedandgroundtruthimagecolors:
M
L = (cid:88) (cid:0) MSE(Im ,Im)+λ·LPIPS(Im ,Im)(cid:1) , (2)
gs render gt render gt
m=1
whereM isthenumberofnovelviewstorenderinasingleforwardpass. TheLPIPSlossweightλis
setto0.05followingMVSplat(Chenetal.,2024).
5Preprint
Table 1: DepthSplat model variants. We explore different monocular backbones and different
multi-viewmodels(1-scaleand2-scalefeaturesforhierarchicalmatchingasdescribedinSec.3.3),
wherebothlargermonocularbackbonesand2-scalehierarchicalmodelsleadtoconsistentlyimproved
performanceforbothdepthestimationandviewsynthesistasks.
Depth(TartanAir) 3DGS(RealEstate10K) Param
Monocular Multi-View
AbsRel↓ δ 1 ↑ PSNR↑ SSIM↑ LPIPS↓ (M)
ViT-S 1-scale 8.46 93.02 26.76 0.877 0.123 37
ViT-B 1-scale 6.94 94.46 27.09 0.881 0.119 113
ViT-L 1-scale 6.07 95.52 27.34 0.885 0.118 354
ViT-S 2-scale 7.01 94.56 26.96 0.880 0.122 40
ViT-B 2-scale 6.22 95.31 27.27 0.885 0.120 117
ViT-L 2-scale 5.57 96.07 27.44 0.887 0.119 360
Table 2: Ablations. We evaluate the contribution of the monocular feature branch and the cost
volume branch, as well as different monocular features. Our results indicate that the monocular
featureandcostvolumearecomplementary,withlargeperformancedropswhenremovingeitherone.
Thepre-trainedDepthAnythingnetworkweightsachieveoverallthebestperformance.
Depth(TartanAir) 3DGS(RealEstate10K)
Module Method
AbsRel↓ δ ↑ PSNR↑ SSIM↑ LPIPS↓
1
full 8.46 93.02 26.76 0.877 0.123
w/omonofeature 12.25 88.00 26.27 0.866 0.130
Components w/ocostvolume 11.34 90.02 23.09 0.761 0.187
singlebranch 11.26 90.84 25.99 0.858 0.134
ConvNeXt-T 10.50 91.13 26.28 0.867 0.130
Midas 9.53 91.61 26.40 0.869 0.129
Monocularfeatures DINOv2 8.93 92.49 26.68 0.874 0.125
DepthAnythingv1 8.38 93.23 26.70 0.875 0.125
DepthAnythingv2 8.46 93.02 26.76 0.877 0.123
4 EXPERIMENTS
ImplementationDetails. WeimplementourmethodinPyTorch(Paszkeetal.,2019)andoptimize
ourmodelwiththeAdamW(Loshchilov&Hutter,2017)optimizerandcosinelearningratelearning
rate schedule with warm up in the first 5% of the total training iterations. We adopt the xForm-
ers(Lefaudeuxetal.,2022)libraryforourmonocularViTbackboneimplementation. Weusealower
learningrate2×10−6 forthepre-trainedmonocularbackbone,andotherremaininglayersusea
learningrateof2×10−4. Thefeaturedownsamplingfactorsinourmulti-viewbranch(Sec.3.1)
is chosen based on the image resolution. More specifically, for experiments on the 256 × 256
resolutionRealEstate10K(Zhouetal.,2018)dataset,wechooses=4. Forhigherresolutiondatasets
(e.g., TartanAir (Wang et al., 2020), ScanNet (Dai et al., 2017), KITTI (Geiger et al., 2013) and
DL3DV(Lingetal.,2023)),wechooses = 8. OurhierarchicalmatchingmodelsinSec.3.3use
2-scalefeatures,i.e.,1/8and1/4,or1/4and1/2resolutions.
TrainingDetails. Fordepthexperiments,wemainlyfollowthesetupofUniMatch(Xuetal.,2023)
for fair comparisons. More specifically, we train our depth model on 4 GH200 GPUs for 100K
iterationswithatotalbatchsizeof32. ForGaussiansplattingexperimentsonRealEstate10K(Zhou
etal.,2018),weusethesametrainingandtestingsplitsofpixelSplat(Charatanetal.,2024)and
MVSplat(Chenetal.,2024),andtrainourmodelon8GH200GPUsfor150Kiterationswithatotal
batchsizeof32,whichtakesabout1day. ForexperimentsontheDL3DV(Lingetal.,2023)dataset,
weevaluateontheofficialbenchmarksplitwith140scenes,whileotherremaining9896scenesare
usedfortraining. Wefine-tuneourRealEstate10Kpre-trainedmodelon4A100GPUsfor100K
iterationswithatotalbatchsizeof4,wherethenumberofinputviewsisrandomlysampledfrom2
to6. Weevaluatethemodel’sperformanceondifferentnumberofinputviews(2,4,6). Ourcode
andtrainingscriptsareavailableathttps://github.com/cvg/depthsplat.
6Preprint
Image 1 Depth (w/o mono feature) Error (w/o mono feature) Image 1 Depth (w/o mono feature) Error (w/o mono feature)
Image 2 Depth (w/ mono feature) Error (w/ mono feature) Image 2 Depth (w/ mono feature) Error (w/ mono feature)
Figure 3: Effect of monocular features for depth on ScanNet. The monocular feature greatly
improves challenging situations like texture-less regions (e.g., the wall in the first example) and
reflectivesurfaces(e.g.,therefrigeratorinthesecondexample).
Input Image 1 Image 1: Depth Image 1: Depth Render Error Novel View Render
GT Novel View
(w/ mono feature) (w/o mono feature) (w/o mono feature) (w/o mono feature)
Image 2: Depth Image 2: Depth Render Error Novel View Render
Input Image 2 (w/ mono feature) (w/o mono feature) (w/ mono feature) (w/ mono feature) GT Novel View
Figure4: Effectofmonocularfeaturesfor3DGSonRealEstate10K.Withoutmonocularfeatures,
themodelstrugglesatpredictingreliabledepthforpixelsthatarenotabletofindcorrespondences
(e.g.,theloungerchairhighlightedwiththereadrectangle),whichsubsequentlycausesmisalignment
intherenderedimageduetotheincorrectgeometry.
4.1 MODELVARIANTS
Wefirststudyseveraldifferentmodelvariantsforbothdepthestimationandviewsynthesistasks. In
particular,weexploredifferentmonocularbackbones(Yangetal.,2024b)(ViT-S,ViT-B,ViT-L)and
differentmulti-viewmodels(1-scaleand2-scale). Weconductdepthexperimentsonthelarge-scale
TartanAir(Wangetal.,2020)syntheticdataset,whichfeaturesbothindoorandoutdoorscenesand
hasperfectgroundtruthdepth. TheGaussiansplattingexperimentsareconductedonthestandard
RealEstate10K(Zhouetal.,2018)dataset. Followingcommunitystandards,wereportthedepth
evaluationmetrics(Eigenetal.,2014)ofAbsRel(relativeℓ error)andδ (percentageofcorrectly
1 1
estimatedpixelswithinathreshold)andnovelviewsynthesismetrics(Kerbletal.,2023)ofPSNR,
SSIM,andLPIPS.TheresultsinTable1demonstratethatbothlargermonocularbackbonesand
2-scalehierarchicalmodelsleadtoconsistentlyimprovedperformanceforbothtasks.
FromTable1,wecanalsoseethatbetterdepthnetworkarchitectureleadstoimprovedviewsynthesis.
InAppendixA.1,weconductadditionalexperimentstostudytheeffectofdifferentinitializations
forthedepthnetworktotheviewsynthesisperformance. Ourresultsshowthatabetterdepthmodel
initializationalsocontributestoimprovedrenderingquality. Insummary,bothbetterdepthnetwork
architectureandbetterdepthmodelinitializationleadtoimprovednovelviewsynthesisresults.
7Preprint
Table3: UnsupervisedDepthPre-TrainingwithGaussianSplatting. Wefirstperformunsuper-
visedpre-trainingwithGaussiansplattingonRealEstate10Kandthenmeasurethedepthperformance
on TartanAir, ScanNet and KITTI after fine-tuning for the depth task. Compared to previous su-
pervisedpre-trainedmodelsDepthAnything(formonocularViT)andUniMatch(formulti-view
Transformer),ournewunsupervisedpre-trainingimprovesperformanceconsistentlyinallmetrics.
TheimprovementsareespeciallysignificantonthechallengingdatasetslikeTartanAirandKITTI.
TartanAir ScanNet KITTI
Initialization
AbsRel↓ δ ↑ AbsRel↓ δ ↑ AbsRel↓ δ ↑
1 1 1
DepthAnything(onlymono) 11.12 89.97 6.78 96.13 11.67 85.96
UniMatch+DepthAnything(mv&mono) 10.86 90.55 6.70 96.14 11.56 87.27
DepthSplat(fullmodel) 10.20 91.10 6.60 96.27 10.68 89.92
4.2 ABLATIONSTUDYANDANALYSIS
Inthissection,westudythepropertiesofourkeycomponentsontheTartanAirdataset(fordepth
estimation)andRealEstate10Kdataset(forviewsynthesiswithGaussiansplatting).
MonocularFeaturesforDepthEstimationandGaussianSplatting. InTable2,wecompareour
fullmodel(full)withthemodelvariantwherethemonoculardepthfeaturebranch(withaViT-S
modelpre-trainedbyDepthAnythingv2(Yangetal.,2024b))isremoved(w/omonofeature),leaving
only the multi-view branch. We can observe a clear performance drop for both depth and view
synthesistasks. InFig.3,wevisualizethedepthpredictionsanderrormapsofbothmodelsonthe
ScanNetdataset. Thepuremulti-viewfeaturematching-basedapproachstrugglesalotattexture-less
regionsandreflectivesurfaces. Atthesametime,ourfullmodelachievesreliableresultsthanksto
thepowerfulpriorcapturedinmonoculardepthfeatures. Wealsoshowthevisualcomparisonsfor
theGaussiansplattingtaskinFig.4withtwoinputviews. Forregions(e.g.,theloungechairs)that
onlyappearinasingleimage,thepuremulti-viewmethodisunabletofindcorrespondences. Itthus
producesunreliabledepthpredictions,leadingtomisalignmentintherenderednovelviewsdueto
theincorrectgeometry.
We also experiment with removing the cost volume (w/o cost volume) in the multi-view branch
andobservedasignificantperformancedrop. Thisindicatesthatobtainingscale-andmulti-view
consistent predictions with a pure monocular depth backbone is challenging, which constrains
achievinghigh-qualityresultsindownstreamtasks.
Fusion Strategy. We compare with alternative strategies for fusing the monocular features for
multi-viewdepthestimation. Inparticular,wecomparewithMVSFormer(Caoetal.,2022)which
constructsthecostvolumewithmonocularfeatures. Morespecifically,wereplaceourmulti-view
featureextractorwithaweight-sharingViTmodelandusetheViTfeaturestobuildthecostvolume
asdoneinMVSFormer. Thisleadstoasingle-brancharchitecture(singlebranchinTable2),unlike
ourtwo-branchdesignwherethemonocularfeaturesarenotusedtobuildthecostvolume. Wecan
observethatourfusionstrategyperformssignificantlybetterthanthesingle-branchdesign,potentially
becauseourtwo-branchdesigndisentanglesfeaturematchingandobtainingmonocularpriors,which
makesthelearningtaskeasier.
DifferentMonocularFeatures. InTable2,wealsoevaluatedifferentmonocularfeatures,including
theConvNeXt-T(Liuetal.,2022)featuresusedinAFNet(Chengetal.,2024),andotherpopular
monocularfeaturesincludingMidas(Ranftletal.,2020)andDINOv2(Oquabetal.,2023). The
pre-trainedDepthAnythingmonocularfeaturesachieveoverallthebestperformance.
4.3 UNSUPERVISEDDEPTHPRE-TRAININGWITHGAUSSIANSPLATTING
ByconnectingGaussiansplattinganddepth,ourDepthSplatprovidesawaytopre-trainthedepth
modelinafullyunsupervisedmanner. Inparticular,wefirsttrainourfullmodelonthelarge-scale
unlabelledRealEstate10Kdataset(contains∼67KYoutubevideos)withonlytheGaussiansplatting
renderingloss(Eqn.2),withoutanydirectsupervisiononthedepthpredictions. Afterpre-training,
wetakethepre-traineddepthmodelandfurtherfine-tuneittothedepthtaskonthemixedTartanAir
andVKITTI2(Cabonetal.,2020)datasetswithgroundtruthdepthsupervision.
8Preprint
Table4: Two-viewDepthEstimationonScan-
Net. OurDepthSplatoutperformsallpriormeth- Table 5: Two-view 3DGS on RealEstate10K.
odsbyclearmargins. OurDepthSplatachievesthebestperformance.
Method AbsRel↓RMSE↓RMSE ↓ Method PSNR↑SSIM↑LPIPS↓
log
DeMoN 0.231 0.761 0.289 pixelNeRF 20.43 0.589 0.550
BA-Net 0.161 0.346 0.214 GPNR 24.11 0.793 0.255
DeepV2D 0.057 0.168 0.080 AttnRend 24.78 0.820 0.213
NeuralRecon 0.047 0.164 0.093 MuRF 26.10 0.858 0.143
DRO 0.053 0.168 0.081 pixelSplat 25.89 0.858 0.142
UniMatch 0.059 0.179 0.082 MVSplat 26.39 0.869 0.128
DepthSplat(ours) 0.044 0.119 0.059 DepthSplat(ours) 27.44 0.887 0.119
In Table 3, we evaluate the performance on both in-domain TartanAir test set and the zero-shot
generalization on unseen ScanNet and KITTI datasets. We compare with only initializing the
monocularbackbonewithDepthAnythingandadditionallyinitializingthemulti-viewTransformer
backbone with UniMatch (Xu et al., 2023). Our approach achieves the best results on all three
datasets. Itisalsointerestingtoobservethatourpre-trainingcontributesmoreonmorechallenging
datasets(i.e.,TartanAirandKITTIwhichfeaturecomplexlarge-scalescenesunlikeScanNetonly
contains indoor scenes). We also note that both Depth Anything and UniMatch are trained with
groundtruthsupervision,whileourDepthSplatistrainedwithonlyphotometriclosses. Giventhe
increasingpopularityofviewsynthesis(Zheng&Vedaldi,2024;Wengetal.,2023)andmulti-view
generativemodels(Shietal.,2023;Blattmannetal.,2023),newmulti-viewdatasets(Lingetal.,
2023) and models (Voleti et al., 2024) are gradually introduced, our approach provides a way to
pre-traindepthmodelsonlarge-scaleunlabelledmulti-viewimagedatasets. Thiscouldpotentially
furtherimprovethemulti-viewconsistencyandrobustnessofexistingdepthmodels(Yangetal.,
2024a;Piccinellietal.,2024),whichareusuallytrainedwithgroundtruthdepthsupervision.
4.4 BENCHMARKCOMPARISONS
ComparisonsonScanNetandRealEstate10K.Table4andTable5comparethedepthandnovel
viewsynthesisresultsonthestandardScanNetandRealEstate10Kbenchmarks,respectively.Forboth
comparisons,thenumbersofinputimagesaretwo. WecanobservethatourDepthSplatoutperforms
previousmethodsTeed&Deng(2019);Yuetal.(2021);Xuetal.(2024a);Chenetal.(2024)by
clearmarginsonbothdatasetsforbothtasks. Thevisualcomparisonwithpreviousbestmethodsis
showninFig.5andFig.6,whereourmethodsignificantlyimprovestheperformanceonchallenging
scenariosliketexture-lessregionsandocclusions.
Comparisons on DL3DV. To further evaluate the performance on complex real-world scenes,
weconductcomparisonswiththelateststate-of-the-artGaussiansplattingmodelMVSplat(Chen
etal.,2024)ontherecentlyintroducedDL3DVdataset(Lingetal.,2023). Wealsocomparethe
results of different numbers of input views (2, 4 and 6) on this dataset. We fine-tune MVSplat
andourRealEstate10Kpre-trainedmodelsonDL3DVtrainingscenesandreporttheresultsonthe
benchmark test set in Table 6. Our DepthSplat consistently outperforms MVSplat in all metrics.
VisualcomparisonswithMVSplatontheDL3DVdatasetareshowninFig.7,whereMVSplat’sdepth
qualitylagsbehindourDepthSplatduetomatchingfailure,whichleadstoblurryanddistortedview
synthesisresults. WeshowmorevisualcomparisonresultsinAppendixA.2. Itisalsoworthnoting
thatourmethodscalesmoreefficientlytomoreinputviewsthankstoourlightweightlocalfeature
matchingapproach(Sec3.1),whichisunliketheexpensivepair-wisematchingusedinMVSplat.
Weinvitethereaderstoourprojectpagehttps://haofeixu.github.io/depthsplat/
forthevideoresultsondifferentnumberofinputviews(6and12),whereourDepthSplatisableto
reconstructlarger-scaleor360scenesfrommoreinputviews,withoutanyoptimization.
9Preprint
Image Depth(UniMatch) Error(UniMatch) Depth (DepthSplat) Error (DepthSplat)
Figure 5: Depth Comparison on ScanNet. Our DepthSplat performs significantly better than
UniMatch(Xuetal.,2023)onchallengingparts(e.g.,edgesofthecouchandpillows).
Input GT pixelSplat MVSplat DepthSplat
Figure6: VisualSynthesisonRealEstate10K.OurDepthSplatperformssignificantlybetterthan
pixelSplat(Charatanetal.,2024)andMVSplat(Chenetal.,2024)inchallengingregions.
Table6: ComparisonsonDL3DV.OurDepthSplatnotonlyconsistentlyoutperformsMVSplaton
differentnumberofinputviews,butalsoscalesmoreefficientlytomoreinputviews.
Method #Views PSNR↑ SSIM↑ LPIPS↓ Time(s)
MVSplat 17.54 0.529 0.402 0.072
DepthSplat 2 19.05 0.610 0.313 0.101
MVSplat 21.63 0.721 0.233 0.146
DepthSplat 4 22.82 0.766 0.188 0.124
MVSplat 22.93 0.775 0.193 0.263
DepthSplat 6 23.83 0.808 0.158 0.161
Image 1: Depth Image 2: Depth Novel View
Figure 7: Visual Comparisons on DL3DV. Our DepthSplat performs significantly better than
MVSplat(Chenetal.,2024)onregionsthatarehardtomatch(e.g.,repeatedpatterns).
10
TG
&
tupnI
talpSVM
talpShtpeDPreprint
5 CONCLUSION
Inthispaper,weintroduceDepthSplat,anewapproachtoconnectingGaussiansplattinganddepth
toachievestate-of-the-artresultsonScanNet,RealEstate10KandDL3DVdatasetsforbothdepth
and view synthesis tasks. We also show that our model enables unsupervised pre-training depth
with Gaussian splatting rendering loss, providing a way to leverage large-scale unlabelled multi-
viewimagedatasetsfortrainingmoremulti-viewconsistentandrobustdepthmodels. Ourcurrent
modelrequirescameraposeinformationasinputalongwiththemulti-viewimages,removingthis
requirementwouldbeexcitingfuturework.
ACKNOWLEDGEMENT
We thank Yuedong Chen for his generous help with the DL3DV dataset. Andreas Geiger was
supportedbytheERCStartingGrantLEGO-3D(850533)andtheDFGEXCnumber2064/1-project
number390727645.
REFERENCES
GwangbinBae,IgnasBudvytis,andRobertoCipolla. Multi-viewdepthestimationbyfusingsingle-
viewdepthprobabilitywithmulti-viewgeometry. InCVPR,2022.
Shariq Farooq Bhat, Ibraheem Alhashim, and Peter Wonka. Zoedepth: Zero-shot transfer by
combiningrelativeandmetricdepth. InCVPR,2023.
AndreasBlattmann,TimDockhorn,SumithKulal,DanielMendelevitch,MaciejKilian,Dominik
Lorenz,YamLevi,ZionEnglish,VikramVoleti,AdamLetts,etal. Stablevideodiffusion: Scaling
latentvideodiffusionmodelstolargedatasets. arXiv,2023.
ChrisBuehler,MichaelBosse,LeonardMcMillan,StevenGortler,andMichaelCohen. Unstructured
lumigraphrendering. InACMTOG,2001.
Yohann Cabon, Naila Murray, and Martin Humenberger. Virtual kitti 2. arXiv preprint
arXiv:2001.10773,2020.
ChenjieCao,XinlinRen,andYanweiFu. Mvsformer: Learningrobustimagerepresentationsvia
transformersandtemperature-baseddepthformulti-viewstereo. TMLR,2022.
DavidCharatan,SizheLi,AndreaTagliasacchi,andVincentSitzmann. pixelsplat: 3dgaussiansplats
fromimagepairsforscalablegeneralizable3dreconstruction. InCVPR,2024.
YuedongChen,HaofeiXu,ChuanxiaZheng,BohanZhuang,MarcPollefeys,AndreasGeiger,Tat-Jen
Cham,andJianfeiCai. Mvsplat: Efficient3dgaussiansplattingfromsparsemulti-viewimages. In
ECCV,2024.
JunDaCheng,WeiYin,KaixuanWang,XiaozhiChen,ShijieWang,andXinYang. Adaptivefusion
ofsingle-viewandmulti-viewdepthforautonomousdriving. InCVPR,2024.
JaeyoungChung,JeongtaekOh,andKyoungMuLee.Depth-regularizedoptimizationfor3dgaussian
splattinginfew-shotimages. InCVPR,2024.
RobertTCollins. Aspace-sweepapproachtotruemulti-imagematching. InCVPR,1996.
Angela Dai, Angel X Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias
Nießner. Scannet: Richly-annotated3dreconstructionsofindoorscenes. InCVPR,2017.
YikangDing,WentaoYuan,QingtianZhu,HaotianZhang,XiangyueLiu,YuanjiangWang,andXiao
Liu. Transmvsnet: Globalcontext-awaremulti-viewstereonetworkwithtransformers. CVPR,
2022.
AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,DirkWeissenborn,XiaohuaZhai,Thomas
Unterthiner,MostafaDehghani,MatthiasMinderer,GeorgHeigold,SylvainGelly,etal. Animage
isworth16x16words: Transformersforimagerecognitionatscale. arXiv,2020.
11Preprint
Arda Duzceker, Silvano Galliani, Christoph Vogel, Pablo Speciale, Mihai Dusmanu, and Marc
Pollefeys. Deepvideomvs: Multi-viewstereoonvideowithrecurrentspatio-temporalfusion. In
CVPR,2021.
AinazEftekhar,AlexanderSax,JitendraMalik,andAmirZamir. Omnidata: Ascalablepipelinefor
makingmulti-taskmid-levelvisiondatasetsfrom3dscans. InICCV,2021.
DavidEigen,ChristianPuhrsch,andRobFergus. Depthmappredictionfromasingleimageusinga
multi-scaledeepnetwork. NeurIPS,2014.
XiaoFu, WeiYin, MuHu, KaixuanWang, YuexinMa, PingTan, ShaojieShen, DahuaLin, and
XiaoxiaoLong. Geowizard: Unleashingthediffusionpriorsfor3dgeometryestimationfroma
singleimage. arXiv,2024.
SilvanoGalliani,KatrinLasinger,andKonradSchindler. Massivelyparallelmultiviewstereopsisby
surfacenormaldiffusion. InICCV,2015.
AndreasGeiger,PhilipLenz,ChristophStiller,andRaquelUrtasun. Visionmeetsrobotics: Thekitti
dataset. TheInternationalJournalofRoboticsResearch,32(11):1231–1237,2013.
XiaodongGu,ZhiwenFan,SiyuZhu,ZuozhuoDai,FeitongTan,andPingTan. Cascadecostvolume
forhigh-resolutionmulti-viewstereoandstereomatching. InCVPR,2020.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. InCVPR,2016.
BingxinKe,AntonObukhov,ShengyuHuang,NandoMetzger,RodrigoCayeDaudt,andKonrad
Schindler. Repurposing diffusion-based image generators for monocular depth estimation. In
CVPR,2024.
BernhardKerbl,GeorgiosKopanas,ThomasLeimkühler,andGeorgeDrettakis. 3dgaussiansplatting
forreal-timeradiancefieldrendering. ACMTOG,2023.
BenjaminLefaudeux,FranciscoMassa,DianaLiskovich,WenhanXiong,VittorioCaggiano,Sean
Naren,MinXu,JieruHu,MartaTintore,SusanZhang,etal. xformers: Amodularandhackable
transformermodellinglibrary,2022.
RuiLi,DongGong,WeiYin,HaoChen,YuZhu,KaixuanWang,XiaozhiChen,JinqiuSun,and
YanningZhang. Learningtofusemonocularandmulti-viewcuesformulti-framedepthestimation
indynamicscenes. InCVPR,2023.
LuLing,YichenSheng,ZhiTu,WentianZhao,ChengXin,KunWan,LantaoYu,QianyuGuo,Zixun
Yu,YawenLu,etal. Dl3dv-10k: Alarge-scalescenedatasetfordeeplearning-based3dvision.
arXiv,2023.
ZeLiu,YutongLin,YueCao,HanHu,YixuanWei,ZhengZhang,StephenLin,andBainingGuo.
Swintransformer: Hierarchicalvisiontransformerusingshiftedwindows. InICCV,2021.
ZhuangLiu,HanziMao,Chao-YuanWu,ChristophFeichtenhofer,TrevorDarrell,andSainingXie.
Aconvnetforthe2020s. InCVPR,2022.
IlyaLoshchilovandFrankHutter. Decoupledweightdecayregularization. arXiv,2017.
Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov,
PierreFernandez,DanielHaziza,FranciscoMassa,AlaaeldinEl-Nouby,etal. Dinov2: Learning
robustvisualfeatureswithoutsupervision. arXiv,2023.
AdamPaszke,SamGross,FranciscoMassa,AdamLerer,JamesBradbury,GregoryChanan,Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style,
high-performancedeeplearninglibrary. InNeurIPS,2019.
LuigiPiccinelli,Yung-HsuYang,ChristosSakaridis,MattiaSegu,SiyuanLi,LucVanGool,and
FisherYu. Unidepth: Universalmonocularmetricdepthestimation. InCVPR,2024.
12Preprint
RenéRanftl,KatrinLasinger,DavidHafner,KonradSchindler,andVladlenKoltun. Towardsrobust
monoculardepthestimation: Mixingdatasetsforzero-shotcross-datasettransfer. IEEETPAMI,
2020.
RenéRanftl,AlexeyBochkovskiy,andVladlenKoltun. Visiontransformersfordenseprediction. In
ICCV,2021.
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-
resolutionimagesynthesiswithlatentdiffusionmodels. InCVPR,2022.
OlafRonneberger,PhilippFischer,andThomasBrox. U-net: Convolutionalnetworksforbiomedical
imagesegmentation. InMICCAI.Springer,2015.
JohannesLSchönberger,EnliangZheng,Jan-MichaelFrahm,andMarcPollefeys. Pixelwiseview
selectionforunstructuredmulti-viewstereo. InECCV,2016.
YichunShi,PengWang,JianglongYe,MaiLong,KejieLi,andXiaoYang. Mvdream: Multi-view
diffusionfor3dgeneration. arXivpreprintarXiv:2308.16512,2023.
Stanislaw Szymanowicz, Christian Rupprecht, and Andrea Vedaldi. Splatter image: Ultra-fast
single-view3dreconstruction. InCVPR,2024.
JiaxiangTang,ZhaoxiChen,XiaokangChen,TengfeiWang,GangZeng,andZiweiLiu. Lgm: Large
multi-viewgaussianmodelforhigh-resolution3dcontentcreation. arXiv,2024.
ZacharyTeedandJiaDeng. Deepv2d: Videotodepthwithdifferentiablestructurefrommotion. In
ICLR,2019.
MatiasTurkulainen,XuqianRen,IaroslavMelekhov,OttoSeiskari,EsaRahtu,andJuhoKannala.
Dn-splatter: Depthandnormalpriorsforgaussiansplattingandmeshing. arXiv,2024.
VikramVoleti,Chun-HanYao,MarkBoss,AdamLetts,DavidPankratz,DmitryTochilkin,Christian
Laforte,RobinRombach,andVarunJampani. Sv3d:Novelmulti-viewsynthesisand3dgeneration
fromasingleimageusinglatentvideodiffusion. arXiv,2024.
FangjinhuaWang,SilvanoGalliani,ChristophVogel,PabloSpeciale,andMarcPollefeys. Patch-
matchnet: Learnedmulti-viewpatchmatchstereo. InCVPR,2021.
FangjinhuaWang,SilvanoGalliani,ChristophVogel,andMarcPollefeys. Itermvs: Iterativeprobabil-
ityestimationforefficientmulti-viewstereo. InCVPR,2022.
WenshanWang,DelongZhu,XiangweiWang,YaoyuHu,YuhengQiu,ChenWang,YafeiHu,Ashish
Kapoor,andSebastianScherer. Tartanair: Adatasettopushthelimitsofvisualslam. In2020
IEEE/RSJInternationalConferenceonIntelligentRobotsandSystems(IROS),pp.4909–4916.
IEEE,2020.
YiranWang,MinShi,JiaqiLi,ZihaoHuang,ZhiguoCao,JianmingZhang,KeXian,andGuosheng
Lin. Neuralvideodepthstabilizer. InICCV,2023.
HaohanWeng,TianyuYang,JiananWang,YuLi,TongZhang,CLChen,andLeiZhang. Consis-
tent123: Improveconsistencyforoneimageto3dobjectsynthesis. arXiv,2023.
ChristopherWewer,KevinRaj,EddyIlg,BerntSchiele,andJanEricLenssen.latentsplat:Autoencod-
ingvariationalgaussiansforfastgeneralizable3dreconstruction. arXivpreprintarXiv:2403.16292,
2024.
HaofeiXuandJuyongZhang. Aanet: Adaptiveaggregationnetworkforefficientstereomatching. In
CVPR,2020.
HaofeiXu,JingZhang,JianfeiCai,HamidRezatofighi,andDachengTao. Gmflow: Learningoptical
flowviaglobalmatching. InCVPR,2022.
HaofeiXu,JingZhang,JianfeiCai,HamidRezatofighi,FisherYu,DachengTao,andAndreasGeiger.
Unifyingflow,stereoanddepthestimation. IEEETPAMI,2023.
13Preprint
HaofeiXu,AnpeiChen,YuedongChen,ChristosSakaridis,YulunZhang,MarcPollefeys,Andreas
Geiger,andFisherYu. Murf: Multi-baselineradiancefields. InCVPR,2024a.
QingshanXuandWenbingTao. Multi-scalegeometricconsistencyguidedmulti-viewstereo. In
CVPR,2019.
YinghaoXu,ZifanShi,WangYifan,HanshengChen,CeyuanYang,SidaPeng,YujunShen,and
GordonWetzstein. Grm: Largegaussianreconstructionmodelforefficient3dreconstructionand
generation. arXiv,2024b.
LiheYang,BingyiKang,ZilongHuang,XiaogangXu,JiashiFeng,andHengshuangZhao. Depth
anything: Unleashingthepoweroflarge-scaleunlabeleddata. InCVPR,2024a.
LiheYang,BingyiKang,ZilongHuang,ZhenZhao,XiaogangXu,JiashiFeng,andHengshuang
Zhao. Depthanythingv2. arXiv,2024b.
Zhenpei Yang, Zhile Ren, Qi Shan, and Qixing Huang. Mvs2d: Efficient multi-view stereo via
attention-driven2dconvolutions. InCVPR,2022.
YaoYao,ZixinLuo,ShiweiLi,TianFang,andLongQuan. Mvsnet:Depthinferenceforunstructured
multi-viewstereo. InECCV,2018.
WeiYin,JianmingZhang,OliverWang,SimonNiklaus,SimonChen,YifanLiu,andChunhuaShen.
Towardsaccuratereconstructionof3dsceneshapefromasinglemonocularimage. IEEETPAMI,
2022.
WeiYin,ChiZhang,HaoChen,ZhipengCai,GangYu,KaixuanWang,XiaozhiChen,andChunhua
Shen. Metric3d: Towardszero-shotmetric3dpredictionfromasingleimage. InCVPR,2023.
AlexYu,VickieYe,MatthewTancik,andAngjooKanazawa. pixelnerf: Neuralradiancefieldsfrom
oneorfewimages. InCVPR,2021.
KaiZhang,SaiBi,HaoTan,YuanboXiangli,NanxuanZhao,KalyanSunkavalli,andZexiangXu.
Gs-lrm: Largereconstructionmodelfor3dgaussiansplatting. arXiv,2024.
RichardZhang,PhillipIsola,AlexeiAEfros,EliShechtman,andOliverWang. Theunreasonable
effectivenessofdeepfeaturesasaperceptualmetric. InCVPR,2018.
ChuanxiaZhengandAndreaVedaldi. Free3d: Consistentnovelviewsynthesiswithout3drepresen-
tation. InCVPR,2024.
TinghuiZhou,RichardTucker,JohnFlynn,GrahamFyffe,andNoahSnavely. Stereomagnification:
learningviewsynthesisusingmultiplaneimages. ACMTOG,2018.
A APPENDIX
A.1 DEPTHPRE-TRAININGFORGAUSSIANSPLATTING
We further study the effect of depth pre-training for Gaussian splatting experiments. Unlike the
pre-trainedmodelsDepthAnything(Yangetal.,2024b)andUniMatch(Xuetal.,2023),whichare
trainedforeithermonocularandmulti-viewfeaturesseparately,weperformjointtrainingofourfull
two-branchdepthmodelonthedepthdatasets. Wethencomparetheresultsofdifferentinitializations
tothedepthnetworkforthefullGaussiansplattingmodel. WecanseefromTable7thatimproved
depthinitializationleadstobetterviewsynthesisresults.
A.2 MOREVISUALCOMPARISONSONDL3DV
WeshowmorevisualcomparisonresultsonDL3DVinFig.8withdifferentnumberofinputviews,
whereourDepthSplatconsistentlyoutperformsMVSplatinchallengingregions.
14Preprint
Table7: DepthtoGaussianSplattingTransfer. Wecomparedifferentpre-trainednetworkweights
for initializing the depth network when training our full DepthSplat model for view synthesis.
ComparedtousingDepthAnythingandUniMatchpre-trainedmonocularandmulti-viewnetwork
weightsforinitializingthemonocularViTandmulti-viewTransformerfeatures,ourjointlytrained
2-branchdepthmodel(fullmodel)performsbestonallmetrics.
Initialization PSNR↑ SSIM↑ LPIPS↓
DepthAnything(onlymono) 26.59 0.874 0.1256
UniMatch+DepthAnything(mv&mono) 26.76 0.877 0.1234
DepthSplat(fullmodel) 26.81 0.878 0.1225
Input
GT
MVSplat
DepthSplat
2-View Input 4-View Input 6-View Input
Figure8: DifferentnumberofinputviewsonDL3DV.OurDepthSplatperformsconsistentlybetter
thanMVSplat(Chenetal.,2024).
15