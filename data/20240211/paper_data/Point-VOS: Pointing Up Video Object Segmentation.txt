Point-VOS: Pointing Up Video Object Segmentation
IdilEsenZulfikar1,* SabarinathMahadevan1,* PaulVoigtlaender2,* BastianLeibe1
1RWTHAachenUniversity,Germany 2GoogleResearch
1{zulfikar, mahadevan, leibe}@vision.rwth-aachen.de 2voigtlaender@google.com
pointvos.github.io
Abstract
BEFORE
Training
Current state-of-the-art Video Object Segmentation
(VOS) methods rely on dense per-object mask annota- (a):
tions both during training and testing. This requires time-
VOS
consuming and costly video annotation mechanisms. We
First Frame
propose a novel Point-VOS task with a spatio-temporally
Reference
sparse point-wise annotation scheme that substantially re-
duces the annotation effort. We apply our annotation NOW Training
schemetotwolarge-scalevideodatasetswithtextdescrip-
tionsandannotateover19M pointsacross133Kobjectsin
(b):
32K videos. Basedonourannotations, weproposeanew
Point-VOS benchmark, and a corresponding point-based Point-VOS
trainingmechanism,whichweusetoestablishstrongbase- First Frame
lineresults. WeshowthatexistingVOSmethodscaneasily Reference
beadaptedtoleverageourpointannotationsduringtrain-
Figure1. ComparisonoftheconventionalVOStaskwithour
ing, and can achieve results close to the fully-supervised new Point-VOS task. (a) The conventional VOS task utilizes
performancewhentrainedonpseudo-masksgeneratedfrom densesegmentationmaskforeachframeduringtrainingandini-
these points. In addition, we show that our data can be tializesthefirst-framereferencewithdensemasks.(b)Wepropose
used to improve models that connect vision and language, tochangethisparadigmanduseonlyspatiallysparsepointanno-
by evaluating it on the Video Narrative Grounding (VNG) tationsonasparsesubsetofframesduringtraining,andonlyafew
task. We will make our code and annotations available at pointsforthefirst-framereferenceinitialization. Greenandblue
https://pointvos.github.io. dotsrepresentforegroundpointsandreddotsbackgroundpoints.
videosegmentationdatasets[21,39,40,52]areusuallyrel-
atively small in scale, and past community efforts to scale
1.Introduction them up to at least several thousand videos required sub-
stantialannotationeffort[3,15,17,66]. Giventhecommu-
Video Object Segmentation (VOS) has grown into a very
nity’scleartrendtoconnectvisiontolanguage[42,60]and
popular research field [6, 39, 51, 53] that has shown con-
the consequent need for even larger datasets [44], there is
siderable progress over the past few years [11, 13, 70],
thusanurgentneedtoreducetheannotationcostforvideos.
branching out into new downstream tasks with language
Some approaches try to mitigate this problem by re-
referring expressions [25] or user interactions [8]. New
ducing the reliance of vision models on annotated train-
datasets have been instrumental in advancing progress in
ing data, e.g., by self-supervised learning [61, 64] or ex-
VOS[3,17,21,52,59,66]. However,therelativelycostly
ploitingimage-levelmaskannotations[1].Anotherstrategy
annotationprocessnecessaryforcreatingVOSdatasetshas
hasbeentocreatesemi-automaticannotationpipelines[59]
so far been a major limiting factor. The traditional VOS
thatgeneratepseudoground-truthmasksfrommorereadily
task requires temporally dense object segmentation masks
available data, such as existing bounding box annotations.
fortheframesofeachtrainingvideo. Asaresult, existing
Nevertheless,neitherofthosepresentsageneralsolution.
Inthiswork,weaddresstheannotationcostproblemby
*Equalcontribution. Theorderingoftheauthorswasdeterminedby
alast-minutecoinflip. proposing an entirely point-based VOS framework, Point-
1
4202
beF
8
]VC.sc[
1v71950.2042:viXraVOS. Inspired by recent point-guided image segmentation Positive Negative Ambiguous
Dataset Videos Objects Annotations
methods [5, 10], Point-VOS moves away from using full Points Points Points
mask supervision and instead relies on spatio-temporally DAVIS’16[39] 50 50 3.4K - - -
sparse point annotations as weak supervision signals for DAVIS’17[40] 90 205 13.5K - - -
YT-VOS[66] 4.4K 7.7K 197K - - -
VOS(seeFig.1).Ourpoint-basedformulationenablesusto
BURST[3] 2.9K 16K 600K - - -
designanefficientsemi-automaticannotationpipeline(see
VISOR[15] 7.8K † 271K - - -
Fig. 4) that requires substantially less annotation effort to VOST[52] 713 † 175K - - -
createhuman-validatedground-truthvideoannotations. MOSE[17] 2.1K 5.2K 431K - - -
We demonstrate the value of our proposed annotation PV-Oops 8.4K 13.1K 93K 548K 1.2M 18K
pipelinebyannotatingtwolarge-scalevideodatasets,Point- PV-Kinetics 23.9K 120K 965K 5.2M 12.6M 253K
VOS Oops [19] (PV-Oops) and Point-VOS Kinetics [23] Table 1. Comparison of VOS datasets with ours. Our Point-
(PV-Kinetics), with altogether 19M points for 133K ob- VOSdataismuchlargercomparedtoexistingVOSdatasets.“An-
jectsin32K videos. Ourannotationscoveralmostanorder notations”countsforeachobjectinhowmanyframesitisanno-
ofmagnitudemorevideosandobjectsthanmajorprevious tated.†:Thenumberofobjectsisnotreported.
VOSdatasets[3,17,21,39,40,52,59,66](seeTab.1). In
videodatasets,PV-OopsandPV-Kinetics. Bydesign,those
particular, weshowhowourannotationpipelinecanmake
datasets feature multi-modal vision-language annotations
useofexistinginformationfromtheVideoLocalizedNar-
that connect open-vocabulary language object descriptions
ratives corpus (VidLN [60]) in order to bootstrap the an-
to the corresponding video object annotations. (4) We es-
notation process by automatically converting mouse traces
tablishanewbenchmarkbasedonthesedatasets,wherewe
fromVidLNintoPoint-VOSinitializations.
trainandtestVOSmethodseitheronpointannotationsoron
WelaunchanewPoint-VOSbenchmarkbasedonthese
pseudomasks,andpresentstrongbaselines. (5)Werealize
datasets,whereVOSmethodsareexpectedtouseonlypoint
thepotentialofmulti-modalvision-languageannotationsin
annotationsbothastrainingsupervisionandastest-timeini-
ourproposeddatasetsandshowcasetheiruseforlanguage-
tialization. We also develop two strong baselines by (i)
guidedVOS.
adapting the state-of-the-art VOS method STCN [13] to
workdirectlywithpointsinsteadofmasks,and(ii)training
2.RelatedWork
STCNonpseudo-masksgeneratedfrompointannotations.
Ourexperimentsshowthat,despitetheweakerlevelofsu-
Video Object Segmentation Datasets. DAVIS [39, 40]
pervision, this Point-VOS STCN baseline already reaches
isoneofthefirstdenselyannotatedVOSdatasets,with90
more than 90% of the performance of the original STCN
videos. Later, the YouTube-VOS (YT-VOS) dataset [66]
whenappliedtotheDAVISbenchmark[40](seeSec.4.2).
with 4.4K videos further advanced the state-of-the-art.
Finally, we also show a direct use case of our point
Later datasets [3, 15, 17, 41, 52] focused on specific VOS
annotations for language-guided VOS. As a consequence
sub-challenges; among them, VISOR [15] is the largest in
of our use of VidLN data to bootstrap the point annota-
terms of the number of videos with 7.8K kitchen videos.
tion pipeline, the point annotations in PV-Oops and PV-
DespitethegrowinginterestintheVOStask,VOSdatasets
Kineticsareconnectedtonounsinlongerlanguagecaptions
arestillsmallinscalemainlyduetotheirexpensiveanno-
(the Video Localized Narratives), describing the referred
tationprocess. Incontrast,weintroduceamuchmoreeffi-
object’s actions in the video. Thus, our annotations are
cientpoint-wiseannotationschemethatenablesustoanno-
multi-modal and bridge the gap between open-vocabulary
tateabout32K videos,4timesmorethanVISOR.
language object descriptions and the corresponding video
object segmentations. We showcase the usefulness of Fully-Supervised VOS Methods. Early VOS meth-
the multi-modal annotations by training a Video Narrative ods[7,31,49,55,56]useonlinelearningattesttimewhich
Grounding(VNG)[60]modelusingourdatasets,resulting makesthemveryslow. Thefollowingmethods[37,57,69]
insignificantimprovementsontwoVNGbenchmarks. alleviate this by propagating predictions frame-by-frame,
using limited context and accumulating errors on the way.
In summary, we make the following contributions: (1)
Recent methods [11, 13, 38] address this by incorporating
We propose the new Point-VOS task for point-guided
a larger temporal context using an external memory (e.g.,
VOS, that includes weakly supervised training on spatio-
STM [38] and its extensions [11–13, 46]). More recent
temporally sparse point annotations. (2) We propose a
works[1,2,70]useTransformerswithspatio-temporalat-
novel and efficient semi-automatic annotation pipeline for
tention. All of these methods rely on dense segmentation
Point-VOS that substantially reduces the annotation effort
masks. Incontrast,weusemuchcheaperpointannotations
for creating human-validated ground-truth video annota-
inourwork.
tions. (3) We demonstrate the value of our proposed an-
notation pipeline by annotating and releasing two large Weakly-Supervised VOS Methods. Weakly supervised
2VOS PET TAP-Vid Point-VOS 75.0%
73.46
72.14
Pointannotationsfortraining ✗ ✗ ✓ ✓ 71.48 71.46
Pointannotationsfortest-time ✗ ✓ ✓ ✓ 70.0% 6699..8308 6699..7462 6 69 9. .8 08 8 7 70 0. .7 04 2 70.38 70.58 70.28
68.50 68.38 68.46
Arbitrarypointlocationonobject ✗ ✓ ✗ ✓ 67.66 67.48 67.06 66.58
Temporallysparseannotations ✗ ✗ ✗ ✓ 65.0%
66 655 4.. .84 408
4 64.28 64.32 64.40 64.52
63.68 63.78
Simple,fast&efficientannotations ✗ ✗ ✗ ✓ 62.28
Multi-modalannotations (✓) ✗ ✗ ✓ Training Supervisions (#Points)
60.0% 30 Point w/ Random Sampling
555888...886482 20 Point w/ Random Sampling
Table 2. Comparison of the design decisions of our new
10 Point w/ Random Sampling
spatially-temporallysparsepointannotationschemeinPoint- 56.86 5 Point w/ Random Sampling
1 Point w/ Random Sampling
VOS with the annotation schemes in other tasks: VOS [39], 55.0%
53.80
P Po oi in nt
t
iE nx aem Vp idla er o-g (u Ti Ade Pd -VT idra )c [k 1i 8n ]g
.
( (P ✓E )T :) M[ u3 l] ti-a mnd odT alra ac nk nin og tatA ion ny
s
52.46 52.76 52.90 52.22 51.96
51.34
50.56 50.40
areonlyavailableforsomeextensionsofVOSdatasets[16,25,45] 50.0%
thatinitializebyareferringexpression. 1 2 3 4 5 10 20 30 Mask
Reference frame supervision (#Points)
Figure2. Trainingvs.test-timepointsupervisionresultsusing
VOS approaches can be mainly classified into two types: simulatedpointsontheDAVISvalidationset. ♦representsour
(i) The first type aims at reducing the first-frame super- chosensetting,i.e.10pointsfortrainingsupervisionand10points
vision at test time by either using points [3] or bounding for test-time supervision. We run each experiment 5 times and
boxes [29, 50, 58, 62, 71] instead of dense masks while reportthemeanscore.
stillrelyingondenseannotationsfortraining. (ii)Thesec-
uses dense masks that are expensive to annotate. PET [3]
ondtype[1,24,59]reducesthetrainingsupervisionbyex-
usespointinitializationattest-timebutstilldensemasksfor
clusively using image-level datasets or by using bounding
training. TAP-Vid [18] uses exact point correspondences
boxesinsteadofmasks, whilestillrelyingondensemasks
for training that are extremely costly to annotate. In con-
forthereferenceattesttime. Unlikethesemethods,weuse
trast,Point-VOSexpectsthatonlyrandompointsonobjects
weakpointsupervisionbothattrainingandtesttimes.
are annotated and the annotation effort is even further re-
Point Supervision for Images. Point supervision during ducedbymakinguseoftemporalsparsity.
training has already been explored for various image tasks
suchasinstancesegmentation[5,9,10,27,32,43,47,48], 3.PointAnnotationsforVOS
actionrecognition[35,36]andobjectcounting[28]. [10]
In the conventional VOS task, the training set contains N
annotatepointsonobjectsandshowthatinstancesegmenta-
tionmethodscanbeeffectivelytrainedonthem. [5]usean
videos V={v1,v2,...,vN}, where each video v ∈ V with
efficientmobilefriendlypointannotationschemetocollect T v frames and O v objects consist of a set of images I v=
a new image dataset. However, unlike our method, these {I1,I2,...,ITv} and a set of dense segmentation masks M v=
methods work only with images and sometimes require
{mt,o|t∈{1,...,Tv},o∈{1,...,Ov}}. At test-time, the input is a
video v ∈/ V and the corresponding reference segmenta-
object-levelboundingboxes[10]forannotatingpoints.
tion masks MR ={mR,mR,...,mR } for OR objects in a sin-
PointSupervisionforVideos. Existingvideo-levelpoint
1 2 OR
gleframe(usuallythefirst). TheconventionalVOSmethod
basedannotations[18,72]aremainlyusedforpointtrack-
then has to generate temporally consistent segmentation
ing,whichrequirespointcorrespondencesthatareagainex- masks Mˆ v={mˆt,o|t∈{1,...,Tv},o∈{1,...,OR}} for the OR fore-
pensive to obtain and not relevant for tasks such as VOS.
groundobjectsforeachframeofthevideo.
Incontrast,wefocusonannotatingrandompoints,i.e.,we
InourproposedPoint-VOStask,weupdatethistaskby
want points that are on a certain object, but we do not re- replacing the training masks M and reference masks MR
quire points in different frames to correspond to the exact
with point annotations, and by working with a sparse set
samepartoftheobject. Tothebestofourknowledge,[3]is
of frames. Hence, in a Point-VOS training set, each video
theonlypreviousattemptthatutilizespointannotationsfor v ∈V hasasetofimagesI v={I1,I2,...,ITv}andasetofpoint
VOS-related tasks. However, different from our work, [3] annotations P v={Pt,o|t∈T vsparse,o∈{1,...,Ov}} with T vsparse ⊂
makes use of point annotations only to initialize the first {1,...,Tv},|T vsparse|≪T v,whereeachP t,oisasetofpoints
frame at test time while keeping the training supervision for object o in frame t. At test-time, a Point-VOS method
unchanged. isinitializedwithreferencepointsPR ={PR,PR,...,PR }on
1 2 OR
VideoAnnotations. Wealsocomparethedesigndecisions ORobjectsinthereferenceframe.TheexpectedoutputMˆ
v
of the point annotation scheme in Point-VOS with exist- is the same as for the original VOS task, i.e., a predicted
ingvideoannotationschemesinTab.2. ConventionalVOS segmentationmaskforeachframeofeachobject.
3
F&J3.1.SimulatingPointAnnotations
70.28
70.0%
69.00
To study the effect of training and initializing with points 68.30 68.38 68.46
rather than masks, we perform a series of experiments 67.5% 67.66 6 67 7. .4 18 0 67.06 67.27 66.80 67.60 68.93
wherewetrainjointlyontheDAVISandYT-VOStraining 66.37 66.30 67.30 66.93 67.43 66.23
setsandevaluateontheDAVISvalidationset. 65.0% 64.44 66.17 66.00 66.33 64.47 6 64 4. .6 27 7 6644..8537 65.33
63.63 63.83 6633..5830 63.73
First,weanalyzethenumberofpointsrequiredfortrain- 6632..1873 63.17
ingsupervisionandfortest-timeinitialization. Forthis,we 62.5% 62.57
samplepointsrandomlyfromeachoftheavailableground- 60.37
60.0% 59.33
truthmasksineveryannotatedvideoframesuchthatthese 58.88
57.83
points are at least d = 20 pixels apart from each other. 57.5% 57.63 #Frames
When the required number of points under the distance All Frames
55.27
20 Frame w/ Evenly-Spaced Sampling
constraint cannot be sampled, e.g., when the ground-truth 55.0%
10 Frame w/ Evenly-Spaced Sampling
masks are very small, we retain the maximum possible 52.97 7 Frame w/ Evenly-Spaced Sampling
52.5% 5 Frame w/ Evenly-Spaced Sampling
numberofpointsundertheconstraint.
We then use an STCN [13] version that is adapted to
1 2 3 4 5 10 20 30 Mask
work with points (see Sec. 4), and train multiple models Reference frame supervision (#Points)
Figure 3. STCN results on DAVIS validation set for vary-
with varying numbers of points used for training supervi-
ingtemporalsparsity,whentrainedon10randomlysampled
sion. WeevaluateeachmodelontheDAVISvalidationset points per frame per object. ⋆ represents our chosen setting,
withdifferentnumbersofpointsforinitializationontheref- i.e.10pointsfortrainingsupervisionand10pointsfortest-time
erenceframe. Fig.2showsthattrainingwith10points(—) supervision, on10frames. Weruneachexperiment3timesand
achievesgoodresultsandaddingmorepointsduringtrain- reportthemeanscore.
ing only gives minor gains. When we have less than 10
startingpointwerequireonlyaroughlocalizationofit(e.g.,
points during training, the performance strongly degrades.
byafewpoints)inasingleframeofthevideo.Wethencon-
For inference, we find that more points on the reference
vertthisroughlocalizationintoapseudo-maskusingthein-
frameleadtobetterresults.However,thisincreasesthetest-
teractivesegmentationmethodDynaMITe[43]. calledDy-
time annotation burden and is a trade-off which is highly
naMITe[43].
application dependent. As a result, we propose to evalu-
ate different numbers of points (up to 10) on the reference Pseudo-mask Propagation. We feed the pseudo-mask
frame (♦). We also tried other sampling strategies such as from the previous step into STCN [13] to propagate it
farthest-point sampling, but random sampling gave better both forward and backward in time to all other frames of
results(seesupplementfordetails). the video and obtain pixel-wise binary probability maps
Next, we analyze the number of frames required for P={p1,p2,...,pT}forallframes1,...,T ofthevideo.
training on randomly sampled points 10 per frame per ob-
PointSampling. BecausebothDynaMITeandSTCNcan
ject. Starting from all frames, we sub-sample (evenly-
introduce errors in the previous process, we do not use
spaced) up to 20 frames for each video. Fig. 3 shows the
the resulting pseudo-masks directly, but instead, we use
resultsofSTCNtrainedonsuchatemporallysparsetrain-
theSTCNoutputprobabilitymapstosamplepointsandlet
ingset. WefindthattheperformanceofSTCNsaturatesat
humanannotatorsverifythem.Wesub-sampletheprobabil-
10 frames (—), where increasing it further does not yield
itymapstemporallyequally-spacedto10frames. Then,for
anynoticeableperformanceimprovements,andhavingless
eachobjectandeachretainedframe,wethresholdtheprob-
than 10 frames deteriorates the performance. Here again,
abilitymapinto(i)ahighprobabilityregionthatlikelyrep-
wetrydifferentframesamplingtechniques,suchasrandom
resents the foreground object, (ii) a low probability region
sampling, but do not observe any performance difference
thatlikelyrepresentsthebackground,and(iii)anuncertain
(detailsinthesupplement). Insummary,wefind10points
region. PointsfromtheuncertainregionarehardforSTCN
perobjecton10framestobeagoodsetting(⋆).
andhencemightprovideavaluablelearningsignalafterbe-
ingmanuallyannotated. Foreachofthe10frames,wethen
3.2.Semi-AutomaticAnnotationScheme
randomly sample 10 potential background points from the
Wedesignaveryefficientsemi-automaticpointannotation low-probability region, 7 potential foreground points from
pipelinetoannotatevideoswithpoints(Fig.4). Insteadof the high-probability region, and 3 ambiguous points from
annotatingpointsinmultipleframesmanuallyfromscratch, the uncertain region. Here, we again ensure that each of
weaimtogeneratepointcandidatesautomaticallythatthen these points are at least d distance apart from each other,
onlyneedtobequicklyverifiedbyhumanannotators. andweobtainintotalupto200pointsperobject.
Pseudo-mask Generation. To annotate an object, as a AnnotatorVerification. Weshowtheannotatorstherough
4
F&JRepeat
1 2 3 4 Zoom
Pseudo Mask Pseudo Mask Point Annotation Annotator
Generation Propagation Generation Verification
(cid:210)Is this a point on the dog with the mouse trace?(cid:211)
YES NO
UNSURE
Figure4. Semi-automaticannotationpipelineusedtoannotateVidLNdata. Wefirstextractamousetracesegmentforeachnoun
inVidLNcaptions,andconvertitintoapseudomaskusingDynaMITe.WethenuseSTCNtopropagatethepseudo-maskacrossthevideo.
WethenusetheSTCNoutputprobabilitymapstosamplesparsepointannotationsandletannotatorsverifythem. Greencirclesrepresent
foregroundpointsandredcirclesbackgroundpoints.
localization information used to generate the points, over- generatesabinarysegmentationmaskm asoutput. More
k
laid on the image, so that they understand which object detailscanbefoundinthesupplement.
shouldbeconsidered. Next,weshowthemtheforeground
PointVerification. Onaverage,weget147pointsperob-
point candidates one by one overlaid on the image (see
ject to be verified and, following this procedure, an anno-
Fig. 4, right). They use a hotkey to either accept or re-
tator spends on average 140 seconds per object, i.e., 0.95
jectthisforegroundpointcandidateortoindicateitisam-
seconds per point. In contrast, annotating a single dense
biguous. We repeat the same procedure with the back-
mask can take ∼80s [30]. If we consider annotating an
ground point candidates, and finally with the points with
object with a mask in each frame of a single video in the
highuncertainty. Bybatchingpointsofthesametype(e.g.,
DAVIS training set with an average of 70 frames, it takes
foreground candidates) together, the annotators can very
about 5600s, which is 40 times slower than our annota-
quicklyverifythem.
tionscheme. Thisdemonstratesthatourannotationscheme
achieves an extreme speedup and lets us annotate much
3.3.Point-VOSDatasets
larger datasets than existing VOS benchmarks. Moreover,
For our annotations, we choose two large datasets from [18]reportthatannotatingpointcorrespondencesover250
VideoLocalizedNarratives(VidLN[60]). VidLNprovides frames for 10 objects with 3 points per object takes 3.3
annotations, where annotators speak to provide a caption hours,i.e.,1,188sperobject.Thisis8timesslowerthanour
for the video, and while speaking, they move their mouse annotationscheme,whichshowsthatourpointannotations
pointerovertheobjecttheyrefertoinmultiplekey-frames arealsomuchfasterthanannotatingpointcorrespondences.
to provide a rough localization. Leveraging VidLN anno- ThestatisticsofourpointannotationsinTab.1showthat
tations has primarily two advantages for us: (i) the mouse intotalweannotatedmorethan19M pointsfor133K ob-
traces can be used to automatically select foreground ob- jectsin32K videos. Thus,weannotatedsignificantlymore
jects in a video, and correspondingly give us a free rough videos and objects than the largest existing VOS datasets
localizationasstartingpointforourannotationscheme;and VISOR[15]andBURST[3].Ourannotationscover4times
(ii) the associated text description can be used to develop morevideosthanVISOR,consistingof7.8K videos,and8
multi-modal VOS algorithms. We build on the “location- timesmoreobjectsthanBURSTwith16K objects.
output question” annotations from Oops [19] because they
PV-Oops. Oops [19] consists of fail videos of uninten-
provideasetofmousetracesfornounsthatarealreadyver-
tional action, often filmed by amateurs in diverse environ-
ified to have good quality. Additionally, we choose Kinet-
ments. Theycontainalotofcamerajitterandmotionblur,
ics[23],becauseitisbyfarthelargestVidLNdataset.
making tracking and segmentation challenging. We create
To convert the continuous mouse traces into segments,
thePoint-VOSOopsdataset(PV-Oops),andannotatemore
we first use the NLP toolkit spaCy to find nouns in the
than13Kobjectsin8.4Kvideosthataresplitintoatraining
VidLNcaptions,andthenforeachnounretrievearoughlo-
andavalidationset. Wealsocreatea PV-Oopsbenchmark
calizationbymousetracesegmentsT={t1,t2,...,tn}onkey-
tomeasurePoint-VOSperformanceonthischallengingdo-
framesF={f1,f2,...,fn}providedbyVidLN[60]. Weextract
main. For 991 objects in the validation set, we annotate
the mouse trace segment t on the key frame f on which
k k points in the first frame for initialization and dense pixel
it has the largest area. Each noun is thus localized with a
masksforupto3framesperobjectforevaluation. Bycon-
correspondingmousetracesegmentonasinglekey-frame.
ducting simulation experiments on DAVIS, we found that
Instance Segmentation from Mouse Traces. We adapt theresultsevaluatedagainsteithermasksinonly3frames,
DynaMITetoworkwithmousetracesinsteadofuserclicks or in all frames, correlate extremely well, meaning that 3
and hence our version of DynaMITe takes a mouse trace frames are sufficient for evaluation purposes (see supple-
segment t and the corresponding frame f as input, and mentfordetails).
k k
5(cid:210)eruption(cid:211)
(cid:210)bouncy castle(cid:211)
Figure5. ExamplepointannotationsforPV-Oops(top)andPV-Kinetics(bottom). Theobjectsareconnectedtonounsfromalarge
vocabulary.Greendotsrepresentforegroundpointsandreddotsbackgroundpoints.
PV-Kinetics. Kinetics [23] is a very large-scale action Wecomputeallscoresondensegroundtruthmasks.
recognition dataset with 650K videos that cover 700 ac-
Point-STCNBaseline. Asafirstbaseline,weadaptSTCN
tion classes. The Kinetics videos are approximately 10s
to work with points both for training supervision and test-
longandareannotatedwithactionlabels. SimilartoOops,
timeinitialization,andwecallthisadaptationPoint-STCN.
for the Point-VOS Kinetics (PV-Kinetics) dataset, we use
Point-STCNmakesminimalchangestotheoriginalSTCN
the subset of videos with VidLN annotations and annotate
model, showing that existing VOS methods can be easily
120K objectswithpointsacross23.9K videos.
adaptedtoworkwithourPoint-VOSdatasets.
With our new annotations, we obtain the largest VOS-
relateddatasetintermsofthenumberofvideosthatcover
Pre-training FT Mean 1-point 2-point 5-point 10-point
awiderangeofhumanactions. Fig.5showssomeexample
pointannotationsfromPV-OopsandPV-Kinetics.Morede- PV-Oops ✗ 59.8 48.6 57.8 65.5 67.7
tailedstatisticsandmoreannotationvisualizationsareavail- PV-Kinetics ✗ 50.4 29.5 41.5 60.7 69.7
ableinthesupplement. PV-Oops+PV-Kinetics ✗ 54.2 35.2 45.9 63.6 71.9
— ✓ 61.3 49.4 60.8 67.2 67.7
4.Experiments
PV-Oops ✓ 62.8 50.6 62.4 67.7 70.4
4.1.Point-VOSBenchmark PV-Kinetics ✓ 62.8 48.0 61.7 69.6 72.0
PV-Oops+PV-Kinetics✓ 63.1 48.4 61.4 69.5 72.9
WeproposeanewbenchmarkforthePoint-VOStaskinor-
der to evaluate what a method can achieve by using point Table3.PV-DAVISbenchmarkresults(J&F)ofPoint-STCN.
annotations for training and testing. At test time, for each FT:fine-tuningonPV-DAVISandPV-YT.
foreground object, we provide multiple sets of point ini-
tializations with varying degrees of sparsity (1, 2, 5, or 10
points) on the corresponding reference frame, and also re-
Pre-training FT G JS FS JU FU
port their mean scores. This reflects different trade-offs at
test-timebetweenannotationeffortandresultquality. PV-Oops ✗ 51.6 60.8 62.0 40.1 43.5
For training, we use point annotations from our an- PV-Kinetics ✗ 49.6 48.2 50.6 46.3 53.4
notated PV-Oops and PV-Kinetics datasets, in addition to PV-Oops+PV-Kinetics ✗ 52.2 52.4 54.2 47.7 54.5
Point-VOS versions PV-DAVIS and PV-YT of the popu- — ✓ 51.9 59.2 60.5 41.7 46.5
lar VOS training sets, that we create by sub-sampling the PV-Oops ✓ 54.4 61.1 62.6 44.6 49.5
object masks both spatially and temporally, as explained PV-Kinetics ✓ 56.6 61.5 64.4 46.9 53.6
in Sec. 3.1. The methods are then evaluated on the vali- PV-Oops+PV-Kinetics ✓ 57.2 62.5 64.7 47.7 53.7
dation sets of PV-DAVIS, PV-YT, and PV-Oops. For PV-
DAVISandPV-Oops,weusethepopularJ&F metric,and Table4.PV-YTbenchmarkresultsofPoint-STCNwheninitial-
report the mean score for the different point initilizations. izedwith10-points.FT:fine-tuningonPV-DAVISandPV-YT.
On the PV-YT validation set, consistent with the original
task, we report the J and F scores for both seen and un- The original STCN method is first pre-trained on syn-
seen classes, along with their overall mean G. Owing to theticvideosequencescreatedbyaugmentingstaticimages,
the limited evaluations permitted by the YT-VOS evalua- andthenfine-tunedonthedenselylabelledDAVISandYT-
tion server, we only consider initialization with 10 points. VOS video datasets. Additionally, STCN also uses a syn-
6Training Mean 1-point 2-point 5-point 10-point Pre-training FT Mean 1-point 2-point 5-point10-point
PV-DAVIS+PV-YT 48.6 40.5 47.4 52.7 53.8 — ✗ 65.6 55.2 67.4 69.5 70.4
PV-Kinetics 42.5 27.3 35.7 50.3 56.7 PV-Oops ✗ 67.2 59.0 69.8 69.1 70.9
PV-Oops 61.2 54.6 60.2 64.4 65.5 PV-Kinetics ✗ 68.9 59.9 71.1 71.6 73.0
PV-Oops+PV-Kinetics ✗ 70.4 61.1 72.5 73.2 74.8
Table5.PV-Oopsbenchmarkresults(J&F)ofPoint-STCN.
— ✓ 70.3 61.6 72.0 72.7 75.0
thetic dataset called BL30K [12], which we do not use in PV-Oops ✓ 70.6 61.0 72.4 73.1 75.8
our work. For Point-STCN, we also start from static im- PV-Kinetics ✓ 71.0 62.4 72.9 73.6 75.0
age pre-training, and then directly train on our spatially- PV-Oops+PV-Kinetics ✓ 71.6 63.0 73.4 74.4 75.8
temporallysparsePoint-VOSdata. Here,westartbytrain- PV-Oops+PV-Kinetics∗ ✓ 67.4 44.8 69.1 77.0 78.8
ingonPV-DAVISandPV-YT,andthenfurtherexplorethe
benefits of adding our new PV-Oops and PV-Kinetics data Table6. PV-DAVISbenchmarkresults(J&F)ofSTCN[13]
as an additional pre-training step. More details about the trained with pseudo-masks. FT: fine-tuning on PV-DAVIS
implementationofPoint-STCNareinthesupplement. andPV-YT,*:usingSAMpseudo-masks.
Tabs.3and4demonstratethatthenewlyannotatedPV-
Oops and PV-Kinetics data bring large improvements as Pre-training FT G JS FS JU FU
compared to starting from static images, especially for the — ✗ 63.0 63.8 66.0 58.2 63.7
settingswherewefine-tunethesemodelsonPV-YTandPV- PV-Oops ✗ 63.6 67.9 69.9 55.5 61.0
DAVIS.E.g.,themeanJ&F onPV-DAVISimprovesfrom PV-Kinetics ✗ 67.3 67.8 70.5 62.0 69.0
61.3% to 63.1%, and G on PV-YT improves from 51.9% PV-Oops+PV-Kinetics ✗ 68.3 70.0 72.7 62.1 68.5
to 57.2% when pre-training with both PV-Oops and PV-
— ✓ 67.7 69.4 72.7 60.9 67.8
Kinetics. ThisdemonstratesthatournewPV-OopsandPV-
PV-Oops ✓ 67.7 69.5 73.0 60.3 68.0
Kinetics annotations serve as good initilizations for adapt-
PV-Kinetics ✓ 68.1 70.6 73.7 60.4 67.6
ing models to other target domains. For results without
PV-Oops+PV-Kinetics ✓ 68.7 70.5 73.7 61.8 68.8
fine-tuning, we observe that training on PV-Kinetics im-
provesthescoresonlywhensufficientlymanypoints(>5) PV-Oops+PV-Kinetics∗ ✓ 73.7 75.5 77.6 68.1 73.9
are available as test-time initialization. This could be at-
tributedtothedomainmismatchbetweenKineticsandYT- Table7. PV-YTbenchmarkresultsofSTCN[13]trainedwith
VOS/DAVIS,hencerequiringmoretest-timeinformation. pseudo-masks, and evaluated on 10-points. FT: fine-tuning
onPV-DAVISandPV-YT,*:usingSAMpseudo-masks.
Tab. 5 shows the results on PV-Oops. Using our PV-
Oops annotations improve the performance from 48.6%
Training Mean 1-point 2-point 5-point 10-point
to 61.2% J&F as compared to Point-STCN trained on
just PV-DAVIS and PV-YT. This shows that using points, — 57.1 50.7 56.7 60.4 60.8
VOS models can be adapted to target domains with a PV-DAVIS+PV-YT 61.3 55.9 61.2 63.9 64.3
stronglyreducedannotationeffort. PV-Kinetics 61.3 55.4 60.9 64.2 64.7
PV-Oops 64.9 59.7 64.9 67.4 67.7
Pseudo-mask Baseline. As an alternative to training on
pointsdirectly,weconsiderusingthepointstofirstgenerate PV-Oops∗ 57.7 40.5 57.6 65.9 66.7
pseudo-masksandthentrainSTCNonthosepseudo-masks.
Table 8. PV-Oops benchmark results (J&F) of STCN [13]
Forthis,weusethesametrainingprocedurethatwasused
trained with pseudo-masks, starting from static-image pre-
totrainoriginalSTCN,butreplacethebootstrappedcross-
training.*:usingSAMpseudo-masks.
entropy loss with a more robust Huberised cross-entropy
loss[34,59]toreducetheinfluenceoferrorsinthepseudo- tialization. Recently, the very strong SAM [27] segmenta-
masks. At test-time, we first generate pseudo-masks from tionmodelbecameavailable,soforsomeadditionalsetups
the different point initializations and then use these masks we create masks with SAM with ViT-H backbone instead
asreference. Thepseudo-masksprovidemuchmoreuseful ofDynaMITeandalsoreportthoseresults.
informationthanpoints,butrequireanadditionalmodelat Similar to the previous baseline setup, we first
test-timeandincreasetherun-time. train STCN on pseudo-masks generated from PV-DAVIS
We generally use DynaMITe [43] to generate pseudo- and PV-YT, and then later include the additional data
masks from the point annotations. For the training set, fromPV-OopsandPV-Kineticsasanadditionalpre-training
we feed both positive and negative points for each object step. Tabs. 6 and 7 show that the performance of STCN
ineveryannotatedframetoDynaMITe, whileforthevali- consistently improves with additional Point-VOS data on
dation set, we only use the available foreground point ini- bothPV-DAVISandPV-YT.Withoutfine-tuning,theaddi-
7Task Training Testing J&F Pre-Training UVO-FT OVIS-VNG UVO-
➀ VNG
VOS Masks Masks 85.3
➁ Point-VOS Points† Points 67.7 No-FT FT
➂ Point-VOS* Pseudo-Masks† Pseudo-Masks 76.9
Static ✗ 28.5 32.4 39.6
➃
Hybrid Masks Points 78.3 Static+PV-Oops ✗ 24.5 31.4 32.9
➄
Hybrid* Masks Pseudo-Masks 80.4 Static+PV-Kinetics ✗ 33.9 35.1 51.8
Table9. AblationsonDAVISusingdifferenttrainingandtest- Static ✓ 32.0 32.7 46.4
timesupervisions. *: SAMpseudo-masks,†:temporallysparse. Static+PV-Oops ✓ 31.8 32.6 48.0
Hybrid:masksduringtraining,pointsattest-time. Static+PV-Kinetics ✓ 32.0 35.0 52.8
Static+PV-Kinetics∗ ✓ 32.9 34.4 52.5
tional Point-VOS data improves the mean J&F for PV-
DAVIS from 65.6% to 70.4% as compared to just using
Table 10. OVIS-VNG and UVO-VNG results (J&F) of RF-
the static image pre-training, showing that the additional
VNG.AllmodelsstartwithCOCO-PNGpre-training. UVO-FT:
video annotations are very helpful. Likewise, the results
fine-tuning on UVO-VNG data, FT: fine-tuning on OVIS-VNG
onPV-YTimprovetheG from63.0%to68.3%. Alsoafter data,No-FT:nofine-tuning.*:usingSAMpseudo-masks.
fine-tuning, consistent improvements can be seen on both
datasets. On DAVIS, using SAM pseudo-masks instead of
4.3.Language-GuidedTasks
DynaMITe is beneficial when more points are available at
test-time,butleadstosignificantlyworseresultsforinitial-
As described in Sec. 3.2, each object that we annotated is
izationwithonly1or2points. ThisislikelybecauseSAM
linked to a noun in a sentence. Hence, those multi-modal
tendstogeneratepart-basedsegmentationsforalownum-
annotations can be used to improve models connecting vi-
berofpoints.
sionandlanguage,e.g.,fortheVideoNarrativeGrounding
Tab. 8 again shows that fine-tuning STCN on the PV- (VNG) [60] task. In VNG, the input to a method is a text
Oops training data significantly improves the results on description(e.g.,“Agreenparrotwithared-blackneckline
thePV-Oopsbenchmarkfrom57.1%meanJ&F to64.9%, isplayingwiththeotherparrot”[60])inwhichtheposition
ascomparedtojustusingstaticimages,furtherboostingthe ofcertainnouns(e.g.,bothinstancesof“parrot”)ismarked.
baselineperformance. Foreachmarkednoun,theVNGmethodhastosegmentthe
correspondingnounineachframeofthevideo.
4.2.AblationStudy
Our multi-modal point annotations link each point to
a noun in a sentence, which matches the setup of VNG.
In Tab. 9, we compare the conventional VOS task
We combine the pseudo-masks generated by DynaMITe
with Point-VOS on the DAVIS validation set. The train-
basedontheannotatedpointswiththelanguageannotations
ingandtestingcolumnsdenotetheannotationsavailableat
to create our new Oops-VNG and Kinetics-VNG training
trainingandtest-time,respectivelyforeachtasksetup.
sets that cover 133K objects in 32K videos. This is more
In the original VOS setup (➀), STCN achieves 85.3%
than 3 times larger than the existing VNG datasets OVIS-
J&F. The Point-VOS setup (➁) just on points yields
VNG [41, 60] and UVO-VNG [60, 63] that span 45K ob-
67.7% J&F, which is around 80% of the original VOS
jectsin8Kvideos.
quality(➀).However,whenweusepseudo-masks(➂),the
gap closes more and we achieve 76.9% J&F, which is Weconductexperimentsusingthestate-of-the-artVNG
morethan90%oftheoriginalVOSquality(➀).Thisresult method RF-VNG [60]. The original RF-VNG is trained
isremarkable,consideringthat➂hasathree-folddisadvan- in 3 steps: 1) pre-training on static images of the COCO-
tage compared to ➀: 1) weak point supervision instead of PNG [20] training set, 2) optional training on UVO-VNG,
masksduringtraining,2)temporalsparsityduringtraining, 3) optional fine-tuning on the OVIS-VNG training set for
3)sparsepointinitializationinsteadofmasksattest-time. evaluation on the OVIS-VNG validation set. We use the
sametrainingrecipe,butinsertanotherpre-trainingstepbe-
WealsoconsideraHybridtasksetting,whichusesdense
tweensteps1)and2),wherewetrainRF-VNG[60]onour
maskannotationsfortrainingbutpoint-basedinitialization
newOops-VNGorournewKinetics-VNGtrainingsets.
fortesting(seesupplementforimplementationdetails).Re-
sultsontheHybridsetupshowthatswitchingfromVOSto Tab. 10 demonstrates that adding our new data signifi-
point-based training has a larger negative effect on quality cantlyimprovesthebaselineresultsfortheVNGtask. E.g.,
than switching to point-based initialization (➀ → ➃ vs. ➃ thebestresultonOVIS-VNGimprovesfrom32.7%J&F
→➁).Again,theuseofpseudo-masksimprovesresults(➄) to 35.0%, and for UVO-VNG, from 46.4% to 52.8%, i.e.,
andshrinksthegaptowardstheoriginalVOSsetup. weachieveanimprovementof6.4percentagepoints.
85.Conclusion Qi, and Hengshuang Zhao. Focalclick: Towards practical
interactiveimagesegmentation. InCVPR,2022. 3
In this work, we have proposed a point-based VOS task,
[10] Bowen Cheng, Omkar Parkhi, and Alexander Kirillov.
Point-VOS, and a point-wise annotation scheme, which is
Pointly-supervisedinstancesegmentation. InCVPR,2022.
much more efficient than the existing dense-mask annota- 2,3
tionscheme. Weusethistoannotatetwolarge-scalemulti- [11] HoKeiChengandAlexanderG.Schwing. XMem: Long-
modal VOS datasets that are much larger than the largest TermVideoObjectSegmentationwithanAtkinson-Shiffrin
available densely annotated VOS datasets. In addition, we MemoryModel. InECCV,2022. 1,2
also introduce a point-based training strategy for the VOS [12] HoKeiCheng,Yu-WingTai,andChi-KeungTang.Modular
methodsandcorrespondinglyshowthatexistingVOSmeth- interactive video object segmentation: Interaction-to-mask,
ods can be easily adapted to leverage our point annota- propagationanddifference-awarefusion. InCVPR,2021. 7
tions. Finally, our experiments show the benefits of our [13] HoKeiCheng,Yu-WingTai,andChi-KeungTang.Rethink-
newly annotated point data by advancing the state-of-the- ingSpace-TimeNetworkswithImprovedMemoryCoverage
forEfficientVideoObjectSegmentation. InNeurIPS,2021.
artperformanceforvariousuni-modalandmulti-modal(vi-
1,2,4,7,12,16,20,21,22
sion+language)benchmarks.
[14] J.Cheng,Y.-H.Tsai,W.-C.Hung,S.Wang,andM.-H.Yang.
Acknowledgments. WewouldliketothankRodrigoBenenson,AmitKu- Fast and Accurate Online Video Object Segmentation via
marRana,andVittorioFerrarifortheirhelpfulfeedbackanddiscussions. TrackingParts. InCVPR,2018. 12
Wealsowouldliketothankallourannotators. Thisprojectwaspartially [15] AhmadDarkhalil,DandanShan,BinZhu,JianMa,Amlan
fundedbyBMBFprojectNeuroSys-D(03ZU1106DA)andEUNetworkof Kar,RichardHiggins,SanjaFidler,DavidFouhey,andDima
ExcellenceTAILOR(H2020-ICT-2019-952215). Thecomputeresources Damen.EPIC-KITCHENSVISORBenchmark:VIdeoSeg-
for several experiments were granted by the Gauss Centre for Super- mentations and Object Relations. In NeurIPS Track on
computinge.V.throughtheJohnvonNeumannInstituteforComputing DatasetsandBenchmarks,2022. 1,2,5
ontheGCSSupercomputerJUWELSatJulichSupercomputingCentre.
[16] HenghuiDing, ChangLiu, ShutingHe, XudongJiang, and
Chen Change Loy. MeViS: A Large-scale Benchmark for
Video Segmentation with Motion Expressions. In ICCV,
References 2023. 3
[17] Henghui Ding, Chang Liu, Shuting He, Xudong Jiang,
[1] AliAthar, JonathonLuiten, AlexanderHermans, DevaRa-
Philip HS Torr, and Song Bai. MOSE: A new dataset for
manan,andBastianLeibe.Hodor:High-levelobjectdescrip-
video object segmentation in complex scenes. In ICCV,
torsforobjectre-segmentationinvideolearnedfromstatic
2023. 1,2
images. InCVPR,2022. 1,2,3
[18] Carl Doersch, Ankush Gupta, Larisa Markeeva, Adria` Re-
[2] AliAthar, AlexanderHermans, JonathonLuiten, DevaRa-
casens, Lucas Smaira, Yusuf Aytar, Joa˜o Carreira, Andrew
manan,andBastianLeibe. TarViS:AUnifiedArchitecture
Zisserman,andYiYang.Tap-vid:Abenchmarkfortracking
forTarget-basedVideoSegmentation. InCVPR,2023. 2
anypointinavideo. InNeurIPS,2022. 3,5
[3] Ali Athar, Jonathon Luiten, Paul Voigtlaender, Tarasha
[19] DaveEpstein,BoyuanChen,andCarlVondrick. Oops!Pre-
Khurana, Achal Dave, Bastian Leibe, and Deva Ramanan.
dictingUnintentionalActioninVideo. InCVPR,2020. 2,
BURST: A Benchmark for Unifying Object Recognition,
5
Segmentation and Tracking in Video. In WACV, 2023. 1,
[20] Cristina Gonza´lez, Nicola´s Ayobi, Isabela Herna´ndez, Jose´
2,3,5
Herna´ndez,JordiPont-Tuset,andPabloArbela´ez. Panoptic
[4] Linchao Bao, Baoyuan Wu, and Wei Liu. CNN in MRF:
narrativegrounding. InCVPR,2021. 8
Video object segmentation via inference in a CNN-based
[21] Lingyi Hong, Wenchao Chen, Zhongying Liu, Wei Zhang,
higher-orderspatio-temporalMRF. InCVPR,2018. 12
Pinxue Guo, Zhaoyu Chen, and Wenqiang Zhang. LVOS:
[5] RodrigoBenensonandVittorioFerrari. Fromcolouring-in
A Benchmark for Long-term Video Object Segmentation.
topointillism:revisitingsemanticsegmentationsupervision. arXivpreprintarXiv:2211.10181,2022. 1,2
arXivpreprintarXiv:2210.14142,2022. 2,3
[22] Joakim Johnander, Martin Danelljan, Emil Brissman, Fa-
[6] ThomasBroxandJitendraMalik. ObjectSegmentationby hadShahbazKhan,andMichaelFelsberg. Agenerativeap-
LongTermAnalysisofPointTrajectories. InECCV,2010. pearancemodelforend-to-endvideoobjectsegmentation.In
1 CVPR,2019. 12
[7] Sergi Caelles, Kevis-Kokitsi Maninis, Jordi Pont-Tuset, [23] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang,
LauraLeal-Taixe´,DanielCremers,andLucVanGool. One- Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola,
ShotVideoObjectSegmentation. InCVPR,2017. 2,12 TimGreen,TrevorBack,PaulNatsev,etal.Thekineticshu-
[8] Sergi Caelles, Alberto Montes, Kevis-Kokitsi Maninis, manactionvideodataset. arXivpreprintarXiv:1705.06950,
Yuhua Chen, Luc Van Gool, Federico Perazzi, and Jordi 2017. 2,5,6
Pont-Tuset. The 2018 DAVIS Challenge on Video Object [24] A. Khoreva, R. Benenson, E. Ilg, T. Brox, and B. Schiele.
Segmentation. arXiv:1803.00557,2018. 1 Luciddatadreamingforobjecttracking. InCVPRW,2017.
[9] XiChen,ZhiyanZhao,YileiZhang,ManniDuan,Donglian 3
9[25] Anna Khoreva, Anna Rohrbach, and Bernt Schiele. Video [42] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
objectsegmentationwithlanguagereferringexpressions. In Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
ACCV,2019. 1,3,12 AmandaAskell,PamelaMishkin,JackClark,etal.Learning
[26] Diederik P Kingma and Jimmy Ba. Adam: A method for transferable visual models from natural language supervi-
stochastic optimization. arXiv preprint arXiv:1412.6980, sion.InInternationalconferenceonmachinelearning,pages
2014. 15 8748–8763.PMLR,2021. 1
[27] AlexanderKirillov,EricMintun,NikhilaRavi,HanziMao, [43] Amit Rana, Sabarinath Mahadevan, Alexander Hermans,
ChloeRolland,LauraGustafson,TeteXiao,SpencerWhite- andBastianLeibe. DynaMITe: DynamicQueryBootstrap-
head,AlexanderCBerg,Wan-YenLo,etal. Segmentany- pingforMulti-objectInteractiveSegmentationTransformer.
thing. InICCV,2023. 3,7,21,22 InICCV,2023. 3,4,7,14,20
[28] Issam H. Laradji, Negar Rostamzadeh, Pedro O. Pinheiro, [44] Christoph Schuhmann, Romain Beaumont, Richard Vencu,
David Va´zquez, and Mark Schmidt. Where are the Blobs: Cade Gordon, Ross Wightman, Mehdi Cherti, Theo
CountingbyLocalizationwithPointSupervision. InECCV, Coombes, Aarush Katta, Clayton Mullis, Mitchell Worts-
2018. 3 man,etal.Laion-5b:Anopenlarge-scaledatasetfortraining
[29] Fanchao Lin, Hongtao Xie, Yan Li, and Yongdong nextgenerationimage-textmodels. AdvancesinNeuralIn-
Zhang. Query-Memory Re-Aggregation for Weakly- formationProcessingSystems,35:25278–25294,2022. 1
supervisedVideoObjectSegmentation. InAAAI,2021. 3 [45] SeongukSeo, Joon-YoungLee, andBohyungHan. Urvos:
[30] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Unifiedreferringvideoobjectsegmentationnetworkwitha
Hays, Pietro Perona, Deva Ramanan, Piotr Dolla´r, and large-scale benchmark. In Computer Vision–ECCV 2020:
C.LawrenceZitnick. Microsoftcoco: Commonobjectsin 16th European Conference, Glasgow, UK, August 23–28,
context. InECCV,2014. 5 2020, Proceedings, Part XV 16, pages 208–223. Springer,
[31] Jonathon Luiten, Paul Voigtlaender, and B. Leibe. PRe- 2020. 3
MVOS: Proposal-generation, Refinement and Merging for [46] HongjeSeong,JunhyukHyun,andEuntaiKim. Kernelized
VideoObjectSegmentation. InACCV,2018. 2,12 memorynetworkforvideoobjectsegmentation. InECCV,
[32] Sabarinath Mahadevan, Paul Voigtlaender, and Bastian 2020. 2
Leibe. Iteratively Trained Interactive Segmentation. In
[47] Konstantin Sofiiuk, Ilia Petrov, Olga Barinova, and Anton
BMVC,2018. 3
Konushin.f-brs:Rethinkingbackpropagatingrefinementfor
[33] Kevis-Kokitsi Maninis, Sergi Caelles, Yuhua Chen, Jordi
interactivesegmentation. InCVPR,2020. 3
Pont-Tuset,LauraLeal-Taixe´,DanielCremers,andLucVan
[48] Konstantin Sofiiuk, Ilia Petrov, and Anton Konushin. Re-
Gool. VideoObjectSegmentationWithoutTemporalInfor-
vivingIterativeTrainingwithMaskGuidanceforInteractive
mation. InIEEETPAMI,2018. 12
Segmentation. arXivpreprintarXiv:2102.06583,2021. 3
[34] Aditya Krishna Menon, Ankit Singh Rawat, Sashank J.
[49] MingjieSun, JiminXiao, EngGeeLim, YanchunXie, and
Reddi, and Sanjiv Kumar. Can gradient clipping mitigate
JiashiFeng. AdaptiveROIgenerationforvideoobjectseg-
labelnoise? InICLR,2020. 7
mentationusingreinforcementlearning. PR,2020. 2
[35] Pascal Mettes and Cees G. M. Snoek. Pointly-Supervised
[50] Mingjie Sun, Jimin Xiao, Eng Gee Lim, Bingfeng Zhang,
ActionLocalization. IJCV,2018. 3
andYaoZhao. Fasttemplatematchingandupdateforvideo
[36] Pascal Mettes, Jan C. van Gemert, and Cees G. M. Snoek.
objecttrackingandsegmentation. InCVPR,2020. 3
SpotOn: ActionLocalizationfromPointly-SupervisedPro-
[51] P.Sundberg, T.Brox, M.Maire, P.Arbelaez, andJ.Malik.
posals. InECCV,2016. 3
Occlusionboundarydetectionandfigure/groundassignment
[37] Seoung Wug Oh, Joon-Young Lee, Kalyan Sunkavalli,
fromopticalflow. InCVPR,2011. 1
and Seon Joo Kim. Fast Video Object Segmentation by
[52] PavelTokmakov, JieLi, andAdrienGaidon. Breakingthe
Reference-Guided Mask Propagation. In CVPR, 2018. 2,
“Object”inVideoObjectSegmentation. InCVPR,2023. 1,
12
2
[38] SeoungWugOh,Joon-YoungLee,NingXu,andSeonJoo
Kim. Videoobjectsegmentationusingspace-timememory [53] David Tsai, Matthew Flagg, and James M. Rehg. Motion
networks. InICCV,2019. 2,12 Coherent Tracking with Multi-label MRF optimization. In
[39] F.Perazzi, J.Pont-Tuset, B.McWilliams, L.VanGool, M. BMVC,2010. 1
Gross,andA.Sorkine-Hornung. ABenchmarkDatasetand [54] CarlesVentura,MiriamBellver,AndreuGirbau,AmaiaSal-
EvaluationMethodologyforVideoObjectSegmentation. In vador,FerranMarques,andXavierGiro-iNieto.Rvos:End-
CVPR,2016. 1,2,3 to-endrecurrentnetworkforvideoobjectsegmentation. In
[40] JordiPont-Tuset,FedericoPerazzi,SergiCaelles,PabloAr- CVPR,2019. 12
bela´ez, Alexander Sorkine-Hornung, and Luc Van Gool. [55] PaulVoigtlaenderandBastianLeibe. OnlineAdaptationof
The2017DAVISChallengeonVideoObjectSegmentation. Convolutional Neural Networks for the 2017 DAVIS Chal-
arXiv:1704.00675,2017. 1,2,12 lengeonVideoObjectSegmentation. InCVPRW,2017. 2,
[41] JiyangQi,YanGao,YaoHu,XinggangWang,XiaoyuLiu, 12
Xiang Bai, Serge Belongie, Alan Yuille, Philip Torr, and [56] PaulVoigtlaenderandBastianLeibe. Onlineadaptationof
SongBai.OccludedVideoInstanceSegmentation:ABench- convolutional neural networks for video object segmenta-
mark. IJCV,2022. 2,8 tion. InBMVC,2017. 2
10[57] Paul Voigtlaender, Yuning Chai, Florian Schroff, Hartwig
Adam,BastianLeibe,andLiang-ChiehChen. Feelvos:Fast
end-to-end embedding learning for video object segmenta-
tion. InCVPR,2019. 2,12
[58] PaulVoigtlaender,JonathonLuiten,PhilipHSTorr,andBas-
tianLeibe. Siamr-cnn: Visualtrackingbyre-detection. In
CVPR,2020. 3,12
[59] PaulVoigtlaender,LishuLuo,ChunYuan,YongJiang,and
Bastian Leibe. Reducing the Annotation Effort for Video
ObjectSegmentationDatasets. InWACV,2021. 1,2,3,7
[60] Paul Voigtlaender, Soravit Changpinyo, Jordi Pont-Tuset,
Radu Soricut, and Vittorio Ferrari. Connecting Vision and
LanguagewithVideoLocalizedNarratives. InCVPR,2023.
1,2,5,8,13,16
[61] Carl Vondrick, Abhinav Shrivastava, Alireza Fathi, Sergio
Guadarrama,andKevinMurphy. Trackingemergesbycol-
orizingvideos. InECCV,2018. 1
[62] QiangWang, LiZhang, LucaBertinetto, WeimingHu, and
PhilipHSTorr.Fastonlineobjecttrackingandsegmentation:
Aunifyingapproach. InCVPR,2019. 3,12
[63] Weiyao Wang, Matt Feiszli, Heng Wang, and Du Tran.
Unidentified video objects: A benchmark for dense, open-
worldsegmentation. InICCV,2021. 8
[64] Xiaolong Wang, Allan Jabri, and Alexei A. Efros. Learn-
ingCorrespondencefromtheCycle-ConsistencyofTime.In
CVPR,2019. 1
[65] Ziqin Wang, Jun Xu, Li Liu, Fan Zhu, and Ling Shao.
RANet: Ranking Attention Network for Fast Video Object
Segmentation. InICCV,2019. 12
[66] NingXu,LinjieYang,YuchenFan,DingchengYue,Yuchen
Liang, Jianchao Yang, and Thomas Huang. Youtube-vos:
Alarge-scalevideoobjectsegmentationbenchmark. arXiv
preprintarXiv:1809.03327,2018. 1,2,12
[67] Shuangjie Xu, Daizong Liu, Linchao Bao, Wei Liu, and
PanZhou.MHP-VOS:MultipleHypothesesPropagationfor
VideoObjectSegmentation. InCVPR,2019. 12
[68] L.Yang,YanranWang,XuehanXiong,JianchaoYang,and
AggelosK.Katsaggelos. EfficientVideoObjectSegmenta-
tionviaNetworkModulation. InCVPR,2018. 12
[69] LinjieYang,YuchenFan,andNingXu. Videoinstanceseg-
mentation. InICCV,2019. 2
[70] ZongxinYang,YunchaoWei,andYiYang. AssociatingOb-
jectswithTransformersforVideoObjectSegmentation. In
NeurIPS,2021. 1,2
[71] Bin Zhao, Goutam Bhat, Martin Danelljan, Luc Van Gool,
and Radu Timofte. Generating masks from boxes by min-
ingspatio-temporalconsistenciesinvideos. InProceedings
oftheIEEE/CVFInternationalConferenceonComputerVi-
sion,pages13556–13566,2021. 3
[72] Yang Zheng, Adam W. Harley, Bokui Shen, Gordon Wet-
zstein,andLeonidasJ.Guibas. Pointodyssey: Alarge-scale
synthetic dataset for long-term point tracking. In ICCV,
2023. 3
11Point-VOS: Supplementary Material
Abstract
70%
65.60
62.60 63.40 63.70 63.60
Inthissupplementary,weprovidetheexperimentalresultsoftheaddi-
60%
tionalsimulationsin AppendixA,thedetailsforannotatingthePoint-VOS
OopsvalidationsetinAppendixB,thestatisticsforPoint-VOSdatasetsin
52.00
A qup ap le in tad ti ix veC r, et sh ue lti sm ip nl Aem ppe en nta dt ii xon E.detailsin AppendixDandtheadditional 50% 48.40 4 49 6. .0 30 0 4 49 7. .0 20 0 4498..3100 4499..9200 50.50
42.20
40.40
40%
Training Supervisions (#Points)
A.AdditionalSimulations 30 Point w/ FPS
30% 20 Point w/ FPS
10 Point w/ FPS
FarthestPointSamplingStrategy. InSec. 3.1ofthemainpaper,we 5 Point w/ FPS
ran a number of point simulation experiments on DAVIS [40] and YT- 20% 1 Point w/ FPS
VOS[66]toanalysetheeffectofusingpointannotationsbothduringtrain- 11.90 13.00 12.90 13.00 12.80 13.40
ingandtesting. Fortheseexperiments,thesimulatedpointsaresampled
10%
randomly from the available ground truth segmentation masks for each
frame. 3.10 2.80 2.80 2.80 2.60 2.70
Inadditiontosamplingthepointsrandomly,wealsoconsiderusingthe 0%
farthest-pointsampling(FPS)technique. TheFPSalgorithmstartsfrom 1 5 10 20 30 Mask
Reference frame supervision (#Points)
somerandominitialpointinthegiveninputpointset,andtheniteratively
Figure6. FPSpointsamplingresultsontheDAVISvalidation
selectsasinglepointthathasthelargestdistancefromallthepreviously
set. Wevarythenumberofsampledpointsperobjectfortraining
sampledones.Forourpointsimulations,insteadofstartingfromarandom
point,wealwaysstartfromapointthatbestrepresentsthecenterofthe supervision and the number of sampled points on the reference
inputmask. Tosamplethiscenter, wefirstgenerateEuclideandistance frame.
transformsfromtheground-truthsegmentationmasksforeachforeground
objectandthecommonbackground. Wethensamplethepointthathas
Random frame sampling results
thelargestdistancefromeachofthesedistancetransformsandfurtheruse for 10 Point on the DAVIS validation set
theseasthestartingpointsfortheFPSalgorithm. TheFPSalgorithmis 70.28
thenseparatelyappliedonthepointsthatrepresenteachoftheforeground 70.0%
objectsandthebackgroundstartingfromthecorrespondingcenterpoint. 68.30 68.38 68.46 68.57
67.66 67.48 68.43
a sig mai uS n li am u tesi dl ear pP oto io nit tnh st .e -S Hp To eC rin eNt as gti aom intu r ,l aa wit ni eo m dn oue nlx t op ip te l ar ei pm pme lyon dt as e nlp ysr te o es n men d pt i oe f rd f ae lrin e sn pS t ae rnc su i. tm y3 .b.1 Ae, r lw so oe f , 66 57 .. 05 %% 64.44 6 66 5. .2 68 8 6 66 5. .1 75 5 6 6 67 5 5. . .0 9 66 5 1 6 66 6. .5 23 2 6 66 6. .3 36 6 6 66 6. .7 71 3 65.06
notethatinbothrandomandFPSpointsamplingstrategies,weruneach 63.29 63.02 62.86 63.19 63.09 63.09
experiment 5 times and report the mean score. In Fig. 6, we show the 62.30 62.41
resultsfortheFPSpointsamplingstrategyontheDAVISvalidationset.It 62.5% 62.76 60.81 60.99 61.05 61.08 61.41 61.57
canbeseenthattheperformanceofPoint-STCNismuchworsewhenwe 59.85
60.0%
useFPSinsteadoftherandomsamplingstrategy(seeFig. 2inthemain 58.88
paper),e.g.,forFPSweachievethebestresultbytrainingwith30points, 57.91 57.65
whichisalmostonparwithusing10pointsastrainingsupervisioninthe 57.5% 57.57 #Frames
randompointsamplingstrategy.Thus,wedecidedtousetherandompoint All Frames
samplingstrategyforourpointannotations. 55.0% 54.52 20 Frame w/ Random Sampling
10 Frame w/ Random Sampling
RandomlySub-samplingFrames. Alongwiththeevenly-spacedsub- 7 Frame w/ Random Sampling
samplingstrategyexplainedinSec.3.1ofthemainpaper,wealsotryto 52.5% 52.19 5 Frame w/ Random Sampling
sub-sampleframesrandomly. Similarlytoevenly-spacedsub-sampling,
startingfromallframes,werandomlysub-sampleupto20framesforeach 1 2 3 4 5 10 20 30 Mask
video. Fortrainingsupervision,wekeepagainthesetupof10randomly First frame supervision (#Points)
Figure 7. Results on the DAVIS validation set for randomly
sampledpointsperframeperobject.Also,weruneachexperiment3times
forbothevenly-spacedandrandomsub-samplingstrategiesandreportthe sub-samplingframes.Wevarythenumberofrandomlysampled
meanscore. frames.
Wedemonstratetheresultsfortherandomsub-samplingstrategyon
theDAVISvalidationsetinFig.7.Weobtainverysimilarresultsforboth Inthatway,wewoulddiminishtheannotationeffortwhileannotatingthe
strategies. WecannotobserveanotabledifferencecomparedtoFig. 3 validationsetaswell,increasingcostandtime-efficiency.
inthemainpaper,sowedecidedtomakeuseoftheevenly-spacedsub-
Tothisend,werunanalysisexperimentsonDAVISbenchmarkresults.
samplingstrategy.
First,wegeneratetemporallysparsevalidationsetsfromtheDAVISvali-
Evaluatingontemporallysparsevideos. Toannotatetheground-truth dationsetbysub-sampling3,4,5,and10framesevenlyspaced,i.e.,we
segmentationmasksfortheevaluationofthePoint-VOSOops(PV-Oops) obtain4temporallysparsevalidationsetsconsistingofsub-sampled3,4,
benchmark, we also make the following key design decision. As the 5,or10frames.Then,wegetthemethods[4,7,13,14,22,25,31,33,37,
consecutiveframesareextremelycorrelatedandredundant,wequestion 38,54,55,57,58,62,65,67,68]fromtheDAVISbenchmarkleaderboard,
whetherevaluatingtheresultsonasparsesubsetofframesissufficient. evaluatethemonasparsesetofframes.Finally,wecomparetheseresults
12
F&J
F&JTEMP-3 TEMP-4
1.0 groundtruth 1.0 groundtruth
stcn stcn
stm stm
0.9 premvos 0.9 premvos
mhpvos mhpvos
cinm cinm
0.8 feelvos 0.8 feelvos
osvoss osvoss
ranet ranet
0.7 v sio as mw rl cnn 0.7 v sio as mw rl cnn
rgmp rgmp
onavos onavos
0.6 agame 0.6 agame
osvos osvos
rvos rvos
0.5 favos 0.5 favos
siammask siammask
osmn osmn
0.5 0.6 0.7 0.8 0.9 1.0 0.5 0.6 0.7 0.8 0.9 1.0
Temporally Sparse J&F Temporally Sparse J&F
TEMP-5 TEMP-10
1.0 1.0
groundtruth groundtruth
stcn stcn
stm stm
0.9 premvos 0.9 premvos
mhpvos mhpvos
cinm cinm
0.8 feelvos 0.8 feelvos
osvoss osvoss
ranet ranet
0.7 voswl 0.7 voswl
siamrcnn siamrcnn
rgmp rgmp
onavos onavos
0.6 agame 0.6 agame
osvos osvos
rvos rvos
0.5 favos 0.5 favos
siammask siammask
osmn osmn
0.5 0.6 0.7 0.8 0.9 1.0 0.5 0.6 0.7 0.8 0.9 1.0
Temporally Sparse J&F Temporally Sparse J&F
Figure8. TemporallydenseJ&F vs.temporallysparseJ&F results. Weget18methods(coloreddots)fromtheleaderboardofthe
DAVISbenchmarkandevaluatethemonthe4differenttemporallysparseDAVISvalidationsets. TEMP-3showstheresultsevaluatedon
3sub-sampledframes,TEMP-4on4sub-sampledframes,TEMP-5on5sub-sampledframes,andTEMP-10on10sub-sampledframes.
withtheresultsonthetemporallydensevalidationset,i.e., theoriginal imentforevaluatingonasparsesubsetofframes(seeAppendixA),we
DAVISvalidationsetwithallframes.AsseeninFig.8,theresultsareex- decidedtoannotatetemporallysparsesegmentationmasksfortheevalua-
tremelycorrelatedforalltemporallysparsevalidationsets.Inotherwords, tionofthePV-Oopsbenchmarkwith3ground-truthframes.
evenwithonly3ground-truthframesperobjectforevaluation,theranking Whileannotating3ground-truthframes, westartbyfirstannotating
betweenmethodsdoesnotchangeinalmostallcases(exceptwhentheir the frame with the mouse trace segment for each video. Note that the
performanceisextremelyclosetoeachother). mousetracecomesfromthelocation-outputquestionsofVidLN[60]for
thePV-Oopsdataset.IntheoriginalVidLNlocation-outputtask(whichwe
donotconsiderinourwork),amaskintheframewiththemousetraceis
B.AnnotatingPoint-VOSOopsValidationSet
approximatelyevaluatedbycomparingittothemousetrace.Byannotating
asegmentationmaskforthisframe, wemakesurethatourannotations
WestartannotatingthePoint-VOSOops(PV-Oops)validationsetbyfirst canbeusedtoreplacetheoriginalVidLNevaluation,thatcomparesthe
annotatingthereferenceframewithpoints. Wegeneratepoint-wisean- predictedmaskwiththemousetrace,withamorepreciseevaluation,that
notationsonthesub-sampled(evenly-spaced)10framesfromeachvideo comparesthepredictedmaskwiththeannotatedmask.
andaskhumanannotatorstoverifytheminthesamewayasforthetrain- Afterannotatingtheframewithmousetrace,wecheckeachvideoand
ingpointannotations. Then,wecheckeachvideotodecidethereference eliminatethevideos,iftheframewiththemousetraceistemporallybefore
frame. In each video, we assign the first frame that contains at least 7 thereferenceframe,orexactlyonthereferenceframe.Fromtheremaining
foregroundpointsasthereferenceframeandremoveallframesbeforethe videos,wesub-sample(evenly-spaced)3framesfromtheframescoming
referenceframe.Incase,wecannotfindaframeinthevideowithatleast afterthereferenceframewithpointannotations,andwecheckwhetherthe
7foregroundpoints,weeliminatethevideo. Wealsocheckwhetherwe framewiththemousetraceisinthe3sub-sampledframes. Iftheframe
haveenoughframesafterthereferenceframe.Ifthereisnoframeafterthe withthemousetraceisinthe3sub-sampledframes,wekeeptheother
referenceframewithatleast3foregroundpointsand1backgroundpoint, 2sub-sampledframesandannotatethemwithground-truthmasks. Ifthe
wealsodropthevideo. framewiththemousetraceisnotinthe3sub-sampledframes,wedrop
Afterwards,weannotatetheground-truthsegmentationmasksforthe theframethatistemporallyclosesttothemousetraceframeandsendthe
evaluationofthePV-Oopsbenchmark.Informedbythesimulationexper- other2framestoannotation.
13
F&J
esneD
yllaropmeT
F&J
esneD
yllaropmeT
F&J
esneD
yllaropmeT
F&J
esneD
yllaropmeTDataset VideosAnnotationsObjectsPositiveNegativeAmbiguous 3 Frame 3 Frame
Points Points Points 7.9% 4 Frame 7.0% 4 Frame
Point-VOSOops 7.4K 93K 12K 541K 1.2M 18K 10 Frame 40.3% 8.6% 8.3% 5 Frame 7.0% 6.7% 5 Frame
Point-VOSKinetics 23.9K 965K 120K 5.2M 12.6M 253K 10 Frame 48.6% 6.6% 6 Frame
Point-VOSDAVIS 60 600 145 9.7K 6K — 7.7% 6 Frame 6.9%
7 Frame
Point-VOSYouTube 3471 34.6K 6.4K 472K 346K — 8.0% 7.6%
11.0% 8.1% 7 Frame 9.5% 8 Frame
Point-VOSOops 991 3.5K 991 7.3K 9.9K 91 9 Frame 8 Frame 9 Frame
Point-VOSDAVIS 30 1.9K 61 558 300 — (a)PV-Oops (b)PV-Kinetics
Point-VOSYouTube 507 614 1K 9.8K 6K —
Figure 9. The distribution of frames for PV-Oops and PV-
Table 11. Statistics for the Point-VOS datasets. Annotations Kinetics. Thedistributionofframesmeanssummingupframes
heremeanssummingupframescontainingatleastoneannotated ineachvideo, whichcontainsatleastonepositivepointannota-
object.NotethatforPoint-VOSDAVISandPoint-VOSYouTube, tion.
wesampledthepointsfromthegroundtruthmasks,whileforall
otherdatasets,weannotatednewpoints. the distribution of the positive points has more probability mass on the
leftthanthedistributionofthenegativepointsinbothPV-OopsandPV-
C.Point-VOSDatasetsStatistics Kinetics.Sincewefixedthenumberofbackgroundpointsto10pointsfor
annotating,thedistributionofthenegativepointshasprobabilitymassat
thecenterforbothPV-OopsandPV-Kinetics.
Overview. InTab.11,wepresentthedetailedstatisticsforthetraining
andvalidationsplitsofthePoint-VOSdatasets.
Point-VOSOops(PV-Oops)andPoint-VOSKinetics(PV-Kinetics)are D.ImplementationDetails
the datasets that we annotated with new points. In total, we collected
19.7M points where 5.8M points are annotated as positive points and
Point-STCN. Amajoradvantageofusingpointannotationsisthatitcan
13.9M points as negative points. Also, 271K points are annotated as
beusedtotrainexistingVOSmodelswithoutmakingdrasticchangesto
ambiguouspoints.Wedonotuseanyambiguousannotationsinourexper-
eithertheinherentmodelorthetrainingstrategy. Weshowthisbyeas-
iments.
ilyadaptingSTCNtoworkwithourpointannotationswhilekeepingmost
InPV-Oops,thereare541Kpositivepointsand1.2Mnegativepoints ofthenetworkstructureintact. Specifically,wemakethefollowingmod-
in the training split, and also 7.3K positive points and 9.9K negative ifications to STCN: (i) The value encoder of STCN now takes a set of
points in the validation split. In PV-Kinetics, there are 5.2M positive sparsepoints(thatwerepresentasasparsesegmentationmask)foreach
pointsand12.6Mnegativepoints. ofthereferenceforegroundobjectsinthefirstframemaskinsteadofthe
InadditiontothePV-OopsandPV-Kineticsdatasets,wealsogener- densepixel-levelmasks.Toleveragethesepointannotations,similartothe
atedthePoint-VOSversionsoftheDAVISandYouTube-VOS(YT-VOS) originalSTCNpre-processingpipeline,weapplyaugmentationslikeaffine
datasets. For Point-VOS DAVIS (PV-DAVIS) and Point-VOS YouTube transformationsandconvertthepointsintoamaskthathasonlynon-zero
(PV-YT),wesamplethespatiallytemporallysparsepointsfromtheground elementsonthelocationsofthepoints. Weconcatenatethepointmasks
truth masks. Since the original DAVIS and YT-VOS datasets are mas- withtheinputimagewhichisthenprocessedbythevalueencoder. (ii)
sivelysmallerthanPV-OopsandPV-Kinetics,thetotalpositiveandneg- Insteadofusingabootstrappedcross-entropylossonthepredicteddense
ativepointsarealsoverymuchlessinPV-DAVISandPV-YT.Thereare posteriorprobabilities,weuseapoint-wisecrossentropylosswherethe
9.7K positivepointsand6K negativepointsinthePV-DAVIStraining lossisappliedtoonlytheoutputvectorsatsparsepointlocationsthatare
split, 558positiveand300negativepointsinthePV-DAVISvalidation annotatedintheground-truth. Weusebilinearinterpolationontheoutput
split. PV-YTcontains472K positiveand346K negativepointsinthe probabilitymaptoapproximatethepredictionsontheprecisepointloca-
trainingsplit,and9.8Kpositiveand6Knegativepointsinthevalidation tions. Duringtraining, weuseboththepositiveandthenegativepoints
split.NotethattherearefewerannotationsinbothPV-DAVISandPV-YT forthelosscomputation. Foreachtrainingsample,wesample3frames
comparedtotheoriginalDAVISandYT-VOSdatasetsaswesub-sample fromavideo.Oneofthoseframesisconsideredthereferenceframewhich
10frames. weusedforinitialization. Thetwootherframesareconsideredthetarget
FrameDistribution. Inadditiontothedetailedstatistics,wealsoana- frames, onwhichwecalculatetheloss. Onlythepositive(foreground)
lyzethedistributionofframesinthetrainingsplitsofPV-OopsandPV- pointsareusedasinitializationinthereference(first)frameduringboth
Kinetics. Duringtheannotationprocess, weprovided10 framestothe trainingandtesting, whilebothpositiveandnegativepointsareusedto
humanannotatorsforannotations.Here,thedistributionofframesmeans, calculatethelossonthetargetframes.
wecheckeachvideoaftertheannotationprocessandsumuptheframesin DynaMITeAdaptation. DynaMITe[43]wasoriginallydesignedtopro-
eachvideo,whichhaveatleastonepositivepointannotation. cessuserinteractionsintheformofuserclicks. Since,forourpointan-
Fig.9showstheframedistributionforPV-Oops(see Fig.9a)and notationscheme,onlyamousetraceisavailableonthereferenceframes
PV-Kinetics(seeFig.9b). Asseen,morethan40%ofthevideosinboth foreachforegroundobject,weadaptDynaMITetoworkwithatraceas
PV-OopsandPV-Kineticshaveallframeswithpositivepointannotations inputforgeneratingareferencemask. Thosereferencemasksarelater
(seeredslice). Also,morethan30%ofthevideosinbothPV-Oopsand fedtoSTCNforpropagation(seeSec. 3.2ofthemainpaper). Toadapt
PV-Kineticscontainmorethan5frameswithpositivepointannotations DynaMITe,wefirstsampletheimagefeaturesthatcorrespondtoeachof
(seechameleon,green,caramelmacchiatoandorangeslices). thepixel-locationscoveredbytheinputmousetrace,andperformaglobal
averagepoolingoperationtogenerateasinglefeaturevector.Thisfeature
PointDistribution. Finally,weanalyzethedistributionofthepositiveand
vectoristhenprojectedlinearlytogenerateaquerythatcorrespondswith
negativepointsinthetrainingsplitsofPV-OopsandPV-Kinetics. Here,
thetrace,similartotheclickfeaturesinDynaMITe. Thisqueryisthen
thedistributionofpointsmeansreportingthetotalnumberofvideosinthe
usedbytheInteractiveTransformermoduleinDynaMITetogeneratethe
differentrangesofthenumberofpointannotations.
outputmaskfortheobjectofinterest.
WeshowthedistributionofpointsinFig.10forPV-Oops(seeFig.10a)
andPV-Kinetics(seeFig.10b).Asseen,weobservesimilarpointdistribu- Training Details. We train Point-STCN with points and STCN
tionsinbothPV-OopsandPV-Kinetics. Asthesizeoftheobjectsvaries, with pseudo-masks on Point-VOS DAVIS (PV-DAVIS) and Point-VOS
14
niart
lav5000 4985 P No es gi ati tv ie ve P Po oin ints ts
4594
4000
3000
2600
2000
1567
1299 1408 1345 1319 1238
1090
1000 939
502
305
0 15 22 56 93
96208181
53
177
92 48 16
(a)PV-Oops
40761 Positive Points
40000 Negative Points
35000
30000
28759
25000
20000 19562
16396 15964
15000 14745
12685
11791
10565
10000 9853 9840
6687
5000 41255307 5523 4638
0 511 497 672 1060 1456 1739 2221 3208 2092 893 530 324 236 161 1033406 822565 57921 20350 71
(b)PV-Kinetics
Figure10.ThedistributionofthepositiveandnegativepointsinPV-OopsandPV-Kinetics.Thex-axisrepresentsthedifferentranges
forthenumberofpoints,andthey-axisrepresentsthetotalnumberofvideos. Also,weshowtheprecisenumbersforthetotalvideosat
thetopofthebars.
YouTube-VOS(PV-YT)jointlyforatotalof38Kiterations. Thelearning DAVIS and PV-YT, we build a combined dataset by repeating the PV-
rateisreducedafter30Ksteps. OnPoint-VOSOops(PV-Oops),Point- DAVISdataset5timesandPV-YT1time,tocompensateforthesmaller
STCNandSTCNaretrainedintotal60Kiterations,andthelearningrate sizeofPV-DAVIS.Similarly,whentrainingjointlyonPV-OopsandPV-
is reduced after 50K steps. On Point-VOS Kinetics (PV-Kinetics), and Kinetics,webuildacombineddatasetbyrepeatingPV-Oops5timesand
alsojointtrainingonPV-OopsandPV-Kinetics,wetrainPoint-STCNand PV-Kinetics1timeinordertocompensateforthesmallersizeofPV-Oops.
STCN in total 190K iterations and reduce the learning rate after 150K
steps. Moreover, for each training of Point-STCN and STCN, we use
Adam[26]andstartwithalearningrateof10−5 andreduceitto10−6
Following the original STCN setup, when training jointly on PV- after a certain number of training steps as indicated above. We set the
15
oediV
latoT
oediV
latoT
01-0
01-0
02-01
02-01
03-02
03-02
04-03
04-03
05-04
05-04
06-05
06-05
07-06
07-06
08-07
08-07
09-08
09-08
001-09
001-09
011-001
011-001
021-011
021-011
031-021
031-021
041-031
041-031
051-041
051-041 061-051
061-051
071-061
071-061
081-071
081-071
091-081
091-081
002-091
002-091weightdecayto10−7andthebatchsizeto4. WeconductallSTCNand
Point-STCNtrainingswith8V100GPUs,andallinferenceexperiments
onasingle3090GPU.
For training ReferFormer, we closely follow the setup used by
VidLN[60].
HybridTask. InSec.4.1ofthemainpaper,weintroducedtheHybridtask
(ataskinbetweenVOSandPoint-VOS).IntheVOStask,densesegmen-
tationmasksareusedbothduringtrainingandfortest-timeinitialization,
while,inthePoint-VOStask,spatiallytemporallysparsepointannotations
areusedinbothcases.FortheHybridtask,spatiallyandtemporallydense
masksareusedduringtraining,whileonlypointsareusedontherefer-
enceframeattest-time.ThismeansthattheHybridtaskfollowsthesetup
fromVOSattrainingtime,whileitfollowsthesetupfromPoint-VOSat
test-time.
In the Hybrid setup, we make use of dense masks to train Hybrid-
STCNwhileweinitializethereferenceframewithsparsepoints. Recall
thatSTCNuses3framesduringtraining,fromwhichoneisthereference
frameandtwoarethetargetframes. IntheHybridsetup, weinitialize
STCNwithpointsinthereferenceframeandapplyafullmasklossin
thetargetframes. Attest-time,Hybrid-STCNcanthenbeinitializedwith
pointsandachievesbetterresultsthanPoint-STCN,asweusemoresuper-
visionduringtraining.
E.AdditionalQualitativeResults
InFig.11andFig.12,weprovidetheadditionalexamplepointannotations
for Point-VOS Oops (PV-Oops) and Point-VOS Kinetics (PV-Kinetics).
Wesuccessfullyannotatedmulti-modalpointsfordifferentandchalleng-
ingscenes,andalsotheobjectsfromalargevocabulary.
In Fig. 13 and Fig. 14, we also show the examples of ambiguous
pointannotationsfromPV-OopsandPV-Kinetics,i.e.thepointannota-
tions where the human annotators indicated that they were unsure. We
observethatwehaveambiguouspointannotationsinparticularcasesfor
bothPV-OopsandPV-Kinetics,e.g.,ifthegivenpointisinachallenging
lightingconditionorattheborder.
In Fig.15, Fig.17, and Fig.19, wepresent thetracking resultsof
Point-STCN(trainedwithpoints)onPoint-VOSOops(PV-Oops),Point-
VOSDAVIS(PV-DAVIS)andPoint-VOSYouTube(PV-YT),respectively.
Also, in Fig. 16, Fig. 18 and Fig. 20, we demonstrate the results of
STCN[13](trainedwithpseudo-masks)onPV-Oops,PV-DAVISandPV-
YT,respectively.
16(cid:210)man(cid:211)
(cid:210)kid(cid:211)
(cid:210)man(cid:211)
(cid:210)person(cid:211)
(cid:210)langur(cid:211)
(cid:210)bird(cid:211) (cid:210)bird(cid:211)
Figure11. AdditionalexamplepointannotationsforPoint-VOSOops. Weareabletohavemulti-modalpointannotationsincluttered
scenes(firstrow),fastmotion(thirdrow),challenginglightingconditions(fourthrow),andmotionblur(fifthrow). Greendotsrepresent
positivepointsandreddotsnegativepoints.
17(cid:210)treadmill(cid:211)
(cid:210)person(cid:211)
(cid:210)golf ball(cid:211)
(cid:210)metal(cid:211)
(cid:210)man(cid:211)
“girl”
Figure 12. Additional example point annotations for Point-VOS Kinetics. We can provide multi-modal point-wise annotations for
objectsfromalargevocabulary(firstandfourthrows),sceneswithfastmotion(secondrow),smallobjects(thirdrow),andalsoscenes
withdifficultlightingconditions(fourthrow)andmotionblur(fifthrow).Greendotsrepresentpositivepointsandreddotsnegativepoints.
18(cid:210)bag(cid:211) (cid:210)sink(cid:211)
(cid:210)man(cid:211) (cid:210)dog(cid:211)
(cid:210)man(cid:211) (cid:210)kid(cid:211)
(cid:210)boy(cid:211) (cid:210)woman(cid:211)
(cid:210)ball(cid:211) (cid:210)spinning wheel(cid:211) (cid:210)baby(cid:211)
(cid:210)langur(cid:211)
(cid:210)boy(cid:211)
(cid:210)grassy surface(cid:211) (cid:210)cement(cid:211)
Figure13. ExampleambiguouspointannotationsfromPoint-
VOSOops.Weobservethatthehumanannotatorsindicateunsure Figure14. ExampleambiguouspointannotationsfromPoint-
ifthegivenpointisinchallenginglightingcondition(firstrow))
VOS Kinetics. Similarly, the human annotators indicate unsure
oratborder(secondrow),oratmotionblur(thirdrow),orifthe if the given point is in challenging lighting condition (first col-
object is ambiguous (fourth row). Green dots represent positive umn,firsttworows)oratborder(secondcolumn,firsttworows),
annotations,reddotsnegativeannotations,andgraydotsambigu- oratmotionblur(firstcolumn,lasttworows),oriftheobjectis
ousannotations. ambiguous(secondcolumn,lasttworows). Greendotsrepresent
positiveannotations,reddotsnegativeannotations,andgraydots
ambiguousannotations.
19Figure15. TrackingresultsofPoint-STCNonPV-Oops. ThemodelistrainedonPV-Oopswithpoints,thenevaluatedonthe10-point
setup.
Figure16. TrackingresultsofSTCN[13]onPV-Oops. ThemodelistrainedonPV-OopswithDynaMITe[43]pseudo-masks, then
evaluatedon10pointssetup.
20Figure17.TrackingresultsofPoint-STCNonPV-DAVIS.Themodelisfirstpre-trainedonPV-OopsandPV-Kineticswithpoints,then
fine-tunedonPV-DAVISandPV-YTwithpoints,andfinallyevaluatedonthe10-pointsetup.
Figure18. TrackingresultsofSTCN[13]onPV-DAVIS.Themodelispre-trainedonPV-OopsandPV-Kineticswithpseudo-masks,
thenfine-tunedonPV-DAVISandPV-YTwithpseudo-masks,andfinallyevaluatedon10pointssetup. Thepseudo-masksaregenerated
fromSAM[27].
21Figure19. TrackingresultsofPoint-STCNonPV-YT.Themodelisfirstpre-trainedonPV-OopsandPV-Kineticswithpoints, then
fine-tunedonPV-DAVISandPV-YTwithpoints,andfinallyevaluatedonthe10-pointsetup.
Figure20. TrackingresultsofSTCN[13]onPV-YT.Themodelispre-trainedonPV-OopsandPV-Kineticswithpseudo-masks,then
fine-tunedonPV-DAVISandPV-YTwithpseudo-masks,andfinallyevaluatedon10pointssetup. Thepseudo-masksaregeneratedfrom
SAM[27].
22