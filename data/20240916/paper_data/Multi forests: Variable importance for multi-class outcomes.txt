Multi forests: Variable importance for multi-class outcomes
Roman Hornung 1,2,* and Alexander Hapfelmeier 3
1Institute for Medical Information Processing, Biometry and Epidemiology, LMU
Munich, Munich, Germany
2Munich Center for Machine Learning (MCML), Munich, Germany
3Institute of AI and Informatics in Medicine, TUM School of Medicine and Health,
Technical University of Munich, Munich, Germany
*Corresponding author: Roman Hornung, hornung@ibe.med.uni-muenchen.de
September 16, 2024
Abstract
In prediction tasks with multi-class outcomes, identifying covariates specifically associated
withoneormoreoutcomeclassescanbeimportant. Conventionalvariableimportancemeasures
(VIMs)fromrandomforests(RFs),likepermutationandGiniimportance,focusonoverallpredic-
tiveperformanceornodepurity,withoutdifferentiatingbetweentheclasses. Therefore,theycan
beexpectedtofailtodistinguishclass-associatedcovariatesfromcovariatesthatonlydistinguish
between groups of classes. We introduce a VIM called multi-class VIM, tailored for identifying
exclusively class-associated covariates, via a novel RF variant called multi forests (MuFs). The
treesinMuFsusebothmulti-wayandbinarysplitting. Themulti-waysplitsgeneratechildnodes
foreachclass,usingasplitcriterionthatevaluateshowwellthesenodesrepresenttheirrespective
classes. Thissetupformsthebasisofthemulti-classVIM,whichmeasuresthediscriminatoryabil-
ityofthesplitsperformedintherespectivecovariateswithregardtothissplitcriterion. Alongside
the multi-class VIM, we introduce a second VIM, the discriminatory VIM. This measure, based
onthebinarysplits,assessesthestrengthofthegeneralinfluenceofthecovariates,irrespectiveof
their class-associatedness. Simulation studies demonstrate that the multi-class VIM specifically
ranksclass-associatedcovariateshighly, unlikeconventionalVIMswhichalsorankothertypesof
covariates highly. Analyses of 121 datasets reveal that MuFs often have slightly lower predictive
performance compared to conventional RFs. This is, however, not a limiting factor given the
algorithm’s primary purpose of calculating the multi-class VIM.
1 Introduction
Random forests (RFs) [Breiman, 2001] are among the most widely used classical machine learning
methods for covariate-based prediction of outcomes. They exhibit strong predictive performance, can
capturecomplexrelationshipsbetweencovariatesandtheoutcome,andconsideronlytheorderofthe
covariate values, making them well suited in the presence of various kinds of covariate distributions.
Additionally, as demonstrated in Probst et al. [2019], they typically provide good prediction results
even without the optimization of tuning parameters.
AfurtherlikelyreasonforthepopularityofRFsisthevariableimportancemeasures(VIMs)asso-
ciated with them, which enable ranking of the covariates according to the strength of their influence
on the outcome. For simplicity, we use the term “influence” here, but actually refer to the observed
correlation between the covariates and the outcome. It is important to note that VIMs typically
do not employ causal inference techniques (see e.g., Pearl [2009]), which is why they do not allow
1
4202
peS
31
]LM.tats[
1v52980.9042:viXrato determine whether covariates have a causal impact on the outcome. Throughout the rest of the
paper, we will often use “influence” or “influential” in this simplified context, indicating the observed
correlation, which may not necessarily imply a causal relationship.
Notably, the permutation VIM, already introduced in Breiman [2001], was highlighted in Molnar
etal.[2020]asamilestonedevelopmentininterpretablemachinelearning. Alongsidethepermutation
VIM, the Gini importance has long been a popular VIM, primarily due to its lower computational
demand. However, concerns about the use of the Gini importance in the context of CART trees
were expressed as early as in Breiman et al. [1984]. Later, Strobl et al. [2007] demonstrated that
this measure tends to overestimate the importance of covariates with many potential split points. In
recentyears,severalbias-correctedversionsoftheGiniimportancehavebeendeveloped. Forexample,
Nembrini et al. [2018] adjust the Gini importance using permuted versions of the covariates, whereas
Li et al. [2019], Loecher [2020], and Zhou and Hooker [2021] use the out-of-bag (OOB) data from the
trees to achieve unbiased Gini importance estimates.
In two-class classification problems, influential continuous covariates have value ranges with dis-
tinct regions predominantly containing observations of one class over the other. This facilitates inter-
pretations like: smaller covariate values likely indicate class A and larger values likely indicate class
B. In multi-class classification scenarios, which are the focus of this paper, the situation is more
complex. Here, a covariate can have a strong influence without value regions associated with specific
classes. Such a covariate might only distinguish well between two or three class groups, but may not
be uniquely associated with single classes.
It is likely, and is supported by the simulation studies in this paper, that conventional VIMs
assign high importance values to covariates irrespective of their specific association with classes of
the outcome. This is problematic in applications where identifying covariates associated with distinct
classes is of interest. Conventional VIMs assess overall predictive performance or the purity of child
nodesproducedbythesplits,withoutdifferentiatingbetweentheclassesoftheoutcome. Furthermore,
theseVIMsarebasedontreesthatusebinarysplitting. Ineachsplit,theobservationsintherespective
node are divided into two child nodes based on a split point in the values of a covariate. Here, it is
likelythatadifferentcovariateisusedinthenextsplit. Therefore,thistypeofsplittingdoesgenerally
notfacilitatefindingsubdivisionsofthevaluesofthecovariateswhereseveralintervalsmainlycontain
observationsofspecificclasses. Multi-waysplittingcanovercomethislimitationbydividingeachnode
intomorethantwochildnodes,eachrepresentingadjacentintervalsinthecovariatesusedforsplitting.
This paper proposes a VIM specifically designed for multi-class outcomes, the multi-class VIM,
alongsideavariantofRFs,multi forests (MuFs),whichusemulti-waysplittingtailoredformulti-class
outcomes. The multi-class VIM aims to prioritize covariates that exhibit regions substantially con-
tainingobservationsofspecificclasses, witheachregionbeinguniquetoadifferentclass. Throughout
the remainder of this paper, the term “class-associated covariates” will frequently be used to refer to
these types of covariates. Covariates that merely differentiate among groups of classes without clear
class-specific associations should be ranked lower than class-associated covariates in applications in
which it is of interest to specifically identify covariates that are associated with one or more classes
of the outcome. Note that the purpose of developing the MuF algorithm was mainly to realize the
multi-class VIM. MuFs were not expected to have better predictive performance than conventional
RFs.
MuFs perform C-way splits in (quasi-)continuous covariates or categorical covariates with at least
C categories (see Section 3 for details), assigning a different one of the C classes of the outcome
to each of the C child nodes. The split criterion rewards regions within the splitting covariate’s
rangesubstantiallycontainingobservationsfromsingleclasses. Incontrast, thecriterionisnegatively
affected by regions with overlapping class-specific distributions of the covariate without a particular
class being overrepresented. Similar to in the case of the Gini importance, this criterion is also used
for calculating the multi-class VIM, exclusively using OOB data to avoid overfitting biases akin to
those found with the Gini importance. Additionally, a second VIM based on the Gini importance,
the discriminatory VIM, is introduced that evaluates the discriminatory ability of binary splits. This
2measure can be used to also select covariates that can only distinguish well between groups of classes
without being specifically associated with individual classes, as is the case also with conventional
VIMs. At each node in the trees of MuFs, a random decision determines whether the split will be
multi-wayorbinary. Thisintegrationofbinarysplitsalsoaddressestheissueofrapiddatadivisionby
multi-waysplits,discussedinChapter9.2ofHastieetal.[2009],whichcancompromisethepredictive
performance of the forest.
The remainder of this paper is organized as follows: Section 2 provides a brief overview of related
work, specifically on decision tree approaches that use multi-way splitting and covariate selection
methods for multi-class outcomes. Section 3 details the algorithm for training and predicting with
MuFs. Theapproachesforcomputingthemulti-classVIMandthediscriminatoryVIMarecoveredin
Section4. Section5presentsthedesignandresultsofasimulationstudy,whichfocusesoncomparing
the multi-class VIM with conventional VIMs in terms of their ability to prioritize class-associated
covariates. Section 6 discusses the results of a benchmark study based on 121 datasets, comparing
the predictive performance of MuFs against that of conventional RFs, which serve as the reference
method. ThediscussioninSection7summarizestheproposedmethodology,theresultsobtained,and
addresses further relatedpoints. Finally, Section 8 succinctly summarizes themain conclusions of the
paper.
2 Related work
To the best of our knowledge, no decision tree approaches in the existing literature specifically use
multi-way splitting to model multi-class outcomes. Consequently, there are also no such approaches
tailored to estimate VIMs aimed at identifying class-associated covariates.
There are also relatively few decision tree approaches that employ multi-way splitting at all.
Notable among these are Fayyad and Irani [1993] and Yen and Chu [2007], which generate multi-way
splitsbyrepeatedbinarysplittinginthesamecovariate. Berzaletal.[2004]formtheintervalsforthe
multi-way splits through hierarchical clustering, selecting the subdivision within the hierarchy that
minimizes a node impurity criterion. In contrast, the algorithm introduced by Elomaa and Rousu
[1999] facilitates the identification of multi-way splits that are optimal among all possible multi-
way splits, provided that the used evaluation metric meets a specific criterion for well-behavedness.
Generallyspeaking,allalgorithmsthatproducemulti-waysplitsindecisiontreescanbecharacterized
as supervised discretization techniques. These techniques convert continuous covariates into discrete
oneswhileconsideringtheirassociationwiththeoutcome. Foracomprehensivereviewofdiscretization
techniques, refer to Garcia et al. [2013].
Furthermore, to the best of our knowledge, there are no other VIMs specifically designed for
multi-class outcomes in the literature. Nevertheless, there are several covariate selection methods for
multi-classoutcomes. ExamplesincludeForman[2004],Chenetal.[2006],Zinietal.[2015],andSong
et al. [2017]. These methods typically focus either on the differences between each individual class
and the other classes, or on the differences among all classes.
Forman [2004] and Chen et al. [2006] belong to the former category. Forman [2004] addresses
text classification using binary covariates, treating the C-class prediction problem as a set of C
one-vs-rest prediction problems, and selecting covariates for each based on a conventional covariate
ranking method. This procedure ensures that for each class, covariates are selected that effectively
discriminate between that class and the other classes. Chen et al. [2006] also adopt a one-vs-rest
classification approach, performing recursive feature elimination concurrently across all C models. In
each iteration, they compute the maximum variable importance values for the C classifiers for each
covariate, then remove the covariates for which this maximum is the smallest across the classifiers.
The margin difference used by the support vector machines is employed as the VIM in this approach.
Conversely, Zini et al. [2015] and Song et al. [2017] target differences among all classes. Zini et al.
[2015] introduces a variant of the group lasso [Yuan and Lin, 2006] designed to select covariates that
effectivelydistinguishbetweenallclasses. Songetal.[2017]implementaniterativecovariateselection
3process based on the class-specific mean differences calculated for each covariate across all possible
pairs of classes. As discussed in Zini et al. [2015] and in Section 7 of this paper, the strategy of
selecting covariates that distinguish well between all classes may not always be appropriate.
Akinola et al. [2022] provide an overview of multi-class covariate selection methods based on
metaheuristic algorithms, which represent a specific category of randomized wrapper algorithms.
Beyond being a VIM, the multi-class VIM also differs in another key aspect from the covari-
ate selection methods for multi-class outcomes discussed above. The fact that it specifically targets
covariates exhibiting regions substantially containing observations of specific classes allows for inter-
pretations linked to individual classes. For instance, with an imaginary covariate X ranked high by
the multi-class VIM, observations with low X values may typically belong to class A, those with high
X values to class B, and those with very high X values to class C.
3 Multi forests: training and prediction
LikeconventionalRFs,MuFsarecomprisedoflargenumbersofdecisiontrees. Eachtreeisconstructed
usingadifferentsubsampleorbootstrapsampleoftheobservations. Thissampleisrecursivelydivided
bycovariate-basedsplitsintoprogressivelysmallersubgroupscallednodesuntilastoppingcriterionis
met. Thenodesthatarenotsubdividedfurtherarereferredtoasendnodes. UnlikeconventionalRFs,
where the splits are exclusively binary—each node dividing into two child nodes—MuFs incorporate
multi-way splits, where nodes are divided into more than two child nodes. Specifically, a K-way split
in a covariate x is conducted using K−1 split points I <···<I . Observations with x values
s 1 K−1 s
intheinterval]−∞,I ]areallocatedtothefirstchildnode,thoseintheinterval]I ,I ]tothesecond,
1 1 2
and so on. Training the trees consists of selecting these covariate-based splits.
PredictioninMuFs,asinconventionalRFs,isbasedontheendnodesinwhichtheobservationsto
be predicted are located after traversing the trees through the covariate-based splits. Predictions can
be expressed either as predicted class probabilities or as point predictions of the most likely class of
the outcome. Predicted class probabilities are calculated by averaging the frequency distributions of
the classes in the end nodes where the observations to be predicted are found, as described by Malley
et al. [2012]. Point predictions can be derived either through the highest predicted class probabilities
or through majority voting as in conventional RFs, where the class most frequently appearing in the
relevant end nodes is predicted.
The algorithms for training a MuF, prediction, and computing the multi-class VIM and the dis-
criminatoryVIM(Section4)areimplementedinthediversityForestRpackage,currentlyavailable
on CRAN in version 0.5.0. This package, closely based on the popular RF implementation ranger
[Wright and Ziegler, 2017], performs tree construction, prediction, and VIM calculation using a fast
C++ implementation.
3.1 Multi forests as diversity forests
MuFs represent so-called diversity forests (DFs) [Hornung, 2022]. The latter are RFs trained using
the DF algorithm that facilitates complex splitting procedures in RFs by drastically reducing the
number of candidate splits evaluated during split selection. A DF variant published prior to MuFs
is interaction forests Hornung and Boulesteix [2022], which model pairwise interactions via bivariable
splits and rank bivariate interaction effects between all variable pairs according to their impact on
prediction.
The set of candidate splits for each split is found in DFs as follows:
For m=1,...,nsplits:
1. Draw randomly a so-called split problem (described below).
2. From the drawn split problem, select randomly one or several splits, where the probabilities of
choosing specific splits can be unequal and interdependent.
4The nature of the split problems varies depending on the RF variant. Generally, a split problem
encompasses all possible splits with, depending on the RF variant, one or more covariates. In MuFs,
split problems consist of the possible binary and multi-way splits that can be performed with the
individual covariates. The detailed algorithm for selecting splits in MuFs is more complex and is
described in Section 3.3.
The effectiveness of splits primarily depends on the covariates used rather than the exact posi-
tioning of the splits within the covariate values [Hornung, 2022]. This is due to the large variation in
predictive information across covariates. Typically, no single split in continuous covariates distinctly
outperformsallotherpossiblesplitsinthatcovariate,suggestingthatthespecificchoiceofsplitwithin
a covariate is less critical. This variability in split effectiveness across covariates is the key reason for
theefficiencyoftheDFalgorithm, asthequalityofsplitsdiffersmorestronglybetweendifferentsplit
problems than within the same split problem. Consequently, it is sufficient to sample only one or a
few candidate splits per split problem to identify effective splits with this approach.
The designation “random forests” for forests trained using the DF algorithm is justified, as in the
eponymousarticlebyBreiman[2001], thistermwasusedtodenoteabroadercategoryofforeststhan
the specific method commonly referred to by this term today.
3.2 Handling of unordered categorical covariates
As previously described, multi-way splitting in MuFs aims to identify regions within the covariate
values substantially associated with specific classes. However, this process requires ordered covariate
values. Consequently, nominal covariates, that is, categorical covariates without inherent ordering,
must be treated differently from other covariates in order to enable their inclusion in multi-way
splitting.
ThisisperformedbyorderingthecategoriesofnominalcovariatespriortotrainingtheMuF,treat-
ing them in the same manner as metric and ordered categorical covariates. Specifically, an approach
outlined in Coppersmith et al. [1999] is applied. Initially, the frequencies of the respective observa-
tions from the different outcome classes are calculated for each category of the categorical covariate.
This produces a matrix of dimensions n ×C, where n represents the number of categories and,
cat cat
as before, C represents the number of outcome classes. Principal component analysis (PCA) is then
performedonthismatrix,withthecategories(i.e. theobservationsinthePCA)weightedaccordingto
theirfrequencyofoccurrence. Thecategoriesaresubsequentlyorderedbasedonthevaluesofthefirst
principalcomponentderivedfromthePCA,ensuringthatcloselypositionedcategoriesexhibitsimilar
class distributions. These ordered categories are encoded as 1,...,n . This method is available for
cat
RFs in the R package ranger when using the option respect.unordered.factors="order" and is
discussed and empirically evaluated in Wright and K¨onig [2019] for conventional RFs.
3.3 Split selection procedure in the construction of the trees
Before we detail the split selection procedure used in the trees to split the nodes, we first offer a
simplified representation of the procedure in Algorithm 1 to provide a more accessible overview.
We consider four different versions of this split selection procedure, arising from two variants of
two components of the algorithm. The first component affects the degree to which the purity of child
nodes, with respect to the classes assigned to them, is rewarded. The variant “Squared” rewards this
much more than the variant “Non-Squared”. The second component pertains to the split criterion
used to evaluate the binary splits. The variant “Gini” assesses binary splits as in classic CART trees,
whereas the variant “Assign Classes” employs a criterion more similar to that used for the multi-way
splits. The latter variant aims to make the values of the discriminatory VIM more comparable in
terms of their value ranges to the values of the multi-class VIM.1 Since these VIMs rely on the split
1NoteagainthatthediscriminatoryVIMvaluesarecalculatedbasedonthesplitcriterionvaluesofthebinarysplits
andthemulti-classVIMvaluesarecalculatedbasedonthesplitcriterionvaluesofthemulti-waysplits.
5Algorithm 1 Sketch of the split selection procedure in multi forests
1: Check whether at least one of three stop criteria is met. If so, stop the procedure.
2: Draw the candidate splits:
For s=1,...,mtry:
Draw a covariate j and generate one or more multi-way splits in the values of this covariate.
s
3: Determine the best split:
3.1: Decide randomly whether the split should be multi-way or binary.
3.2: If the split is to be multi-way:
3.2.1: Evaluate each multi-way split from the multi-way splits sampled in Step 2 using a split
criterion that measures how strongly the classes are represented in the child nodes to
which they are assigned.
3.2.2: Select the multi-way split that is associated with the best value of the split criterion.
If the split is to be binary:
3.2.1: Evaluate each split point within the multi-way splits sampled in Step 2 using a split
criterion that measures the purity of the two resulting child nodes.
3.2.2: Select the binary split associated with the best value of the split criterion.
criteria,thechoiceofversionsimpactsthecalculationoftheVIMsaswell,whichiswhytheseversions
will again play a role in Section 4, in which the VIMs are treated.
It is crucial to note that we did not propose these four versions of the split selection procedure
as equally viable options, as our conjecture was that these versions tend to give results of varying
quality. Instead, they represent alternative possibilities. Based on the simulation study results and
the real data analysis, one version will be recommended above the others.
The split selection procedure is described in detail below:
1. Stop if at least one of the following conditions is met: a) the current node l consists exclusively
of observations of a certain class, that is, it is pure, b) the current node has at most as many
observations as the allowed minimum node size nmin, which is a tuning parameter, c) none of
the available covariates has at least one potential split point, that is, all available covariates are
constant in the current node.
2. Determine the set C of the c classes present in the current node l.
l l
3. Sampling of the candidate split variables and the candidate splits.
Set m←1.
For s=1,...,mtry:
(a) Randomlydrawacovariatex fromthesetofallcovariatesthathaveatleastonepotential
js
split point.
(b) Determine the N unique values of x in the current node and sort them. Label the sorted
js
values with a ,...,a .
1 N
(c) If N ≤c :
l
Place the split points I <···<I between all neighboring unique x values:
m,1 m,N−1 js
I =(a +a )/2, where t=1,...,N −1.
m,t t t+1
6Set m←m+1.
If N >c :
l
Repeat npervar times, where npervar is a tuning parameter:
i. Randomly select c − 1 split points, I < ··· < I , from the midpoints
l m,1 m,cl−1
between all neighboring unique x values, with the constraint that between all
js
neighboring split points there are at least ⌊N ⌋ unique x values.
2cl js
ii. Set m←m+1.
4. Determination of the best split.
(a) Decide at random whether the split should be multi-way or binary.
(b) If the split is to be multi-way:
i. Perform the following for all multi-way splits drawn in Step 3:
If N ≥c :
l
In this case, which is the most common case with metric covariates, we assign
each class to a different one of the c child nodes.
l
Moreprecisely,theclassesareassignedtothechildnodesinsuchawaythatthe
following criterion is maximized:
(cid:88)
p2 . (1)
c,Ias
m,c
c∈Cl
HereIas ,...,Ias denotetheindicesofthechildnodestowhichtheclassesare
assignem d, ,1 wherem Ia,C sl
denotesthechildnodetowhichtheclasscisassigned. Fur-
m,c
thermore, p denotesthe proportion ofobservations ofclass c inthe Ias -th
c,I mas ,c m,c
childnode. Forlargernumbersofclasses,thenumberM!ofpossibleassignments
of the classes to the child nodes quickly becomes very large. However, finding
the optimal allocations Ias ,...,Ias can be carried out computationally effi-
m,1 m,Cl
ciently also in such cases using the Hungarian algorithm [Kuhn, 1955].
After determining the vector of indices of the child nodes to which the classes
are assigned, Ias ,...,Ias , the split criterion is calculated as follows:
m,1 m,Cl
(cid:88) p2 n l,I mas ,c, (2)
c,I mas ,c n l
c∈Cl
where n denotes the number of observations in the Ias -th child node and
l,I mas ,c m,c
n denotes the number of observations in the current node l. In the variant
l
“Non-Squared”, both equation (1) and equation (2) use the non-squared pro-
portions p instead of the squared proportions. The variant “Squared” is
c,Ias
m,c
correspondingly the variant with squared proportions.
If N <c :
l
In this case, a similar split criterion is used:
(cid:88) p2 n l,I mas ,c, where Ias = argmax p2 . (3)
c,I mas ,c n l m,c k∈{1,N−1} c,k
c∈Cl
7Thequantitiesoccurringinformula(3)aredefinedinthesamewayasinformula
(2). Inthesplitcriteriondefinedinformula(3),differentclassescanbeassigned
to the same child node, unlike in formula (2). This is also unavoidable, as the
number of classes is greater than the number of child nodes. If there are several
maxima in the proportions p (k ∈ {1,...,N − 1}) in formula (3), Ias is
c,k m,c
assigned a random index from the indices of the child nodes with maximum
proportions. In the variant “Non-Squared”, again the non-squared proportions
p are used in equation (3).
c,Ias
m,c
ii. Select the (multi-way) split that is associated with the best split criterion value.
If the split is to be binary:
Variant “Gini”:
i. CalculatetheGinisplitcriterion,whichisalsousedinCARTtrees,foreachofthe
split points sampled in Step 3. That is, in the first step, the Gini split criterion is
calculated for the binary split induced by I , in the second step it is calculated
1,1
for the binary split induced by I , etc. until all splits generated in Step 3 have
1,2
been evaluated.
ii. Select the split point associated with the best value of the Gini split criterion.
Variant “Assign Classes”:
i. Asinthevariant“Gini”,allsplitpointssampledinStep3areevaluated. However,
the following split criterion is used instead of the Gini criterion:
(cid:88) p2 n l,I mas ,c, where Ias =argmaxp2 . (4)
c,I mas ,c n l m,c k∈{1,2} c,k
c∈Cl
The quantities occurring in this formula are defined in the same way as in the
formulas (2) and (3). If p = p , Ias is chosen at random from {1,2}. In
c,1 c,2 m,c
the variant “Non-Squared”, again the non-squared proportions p are used in
c,k
equation (4).
ii. Select the split point that is associated with the best value of the above split
criterion.
The restriction described in Step 3.(c)i. mandates that between any two neighboring split points,
there should be at least ⌊N ⌋ unique x values. Note again that in the context of multi-way splits,
2cl js
a different class is assigned to each child node in the calculation of the split criterion and in that
of the multi-class VIM. Against this background, the restriction is intended to prevent the child
nodes from having substantially disparate sizes. If some child nodes contained few observations, the
correspondingclasseswouldberepresentedbysmallregionswithinthecovariate’srange. Thesesmall
intervals would hardly allow meaningful or relevant interpretations. This is because knowing that a
class exists substantially within a region that contains only a small proportion of the observations
provides limited useful insight. Additionally, allowing very small child nodes could lead to unstable
VIM values. Node partitions featuring very small child nodes may result in markedly different split
criterion values compared to the majority of the split criterion values obtained for the respective
covariate. Given that both the multi-class VIM and the discriminatory VIM are calculated based on
these split criterion values, such outlying values could adversely affect the stability of the calculated
VIM values.
Therestrictionusedinthesamplingofthesplitpointsmentionedinthelastparagraphalsoaffects
the binary splits. This is because the split points for the multi-way splits and the binary splits are
8sampled in the same way. The purpose of this approach is to align the discriminatory VIM values
more closely with those of the multi-class VIM.
Unlike the supervised discretization techniques mentioned in Section 2, MuFs employs a simpler
approachforidentifyingmulti-waysplitsinthevaluesoftherandomlysampledcovariates. Here,only
a small number of npervar candidates of multi-way splits are sampled per covariate, following the
DF algorithm. This method might lead to more splits in the trees, as the nodes achieve purity later
comparedtomethodsusingcomplexoptimization. Consequently,thetreesmaybecomemorecomplex
and their predictions more varied. This variability can enhance the predictive performance of RFs.
As Breiman [2001] has shown, the predictive performance of RFs benefits from a weak correlation
between the predictions of the trees. Given that multi-way splits typically segregate the data rapidly,
it is potentially particularly beneficial to avoid optimizing the splits here.
4 Multi-class VIM and discriminatory VIM
Asdescribedintheintroduction,weproposetwodistincttypesofVIMs: themulti-classVIMandthe
discriminatory VIM. The multi-class VIM assesses the influence of covariates based on how strongly
they are associated with different classes. Specifically, it measures the extent to which regions within
therangesofcovariatevaluesaredominatedbyobservationsofparticularclasses. Incontrast,thedis-
criminatoryVIM,likeconventionalVIMs,evaluateshoweffectivelycovariatescandistinguishbetween
the classes. Covariates with high discriminatory VIM values may not necessarily be associated with
specific classes, but may only be suitable well for distinguishing between different groups of classes.
The discriminatory VIM is calculated for all covariates, whereas the multi-class VIM is calculated
onlyforcontinuouscovariatesandcategoricalcovariateswithatleastC categories,whereC represents
the number of outcome classes. The technical reason for this is that the split criterion on which the
multi-class VIM is based is calculated differently depending on whether the number of unique values
of the covariate is at least as many as the number of classes or fewer. Consequently, the multi-class
VIMvalueswouldnotbeequallycomparableacrosscovariateswithvaryingnumbersofuniquevalues.
Additionally,ifacovariatehasfewercategoriesthanthereareclasses,itisimpossibletoseparateallC
classeswell. Therefore,suchcovariatesmayarguablyhavelesspotentialforthemulti-classprediction
problem.
The multi-class VIM M Cl VIM and the discriminatory VIM Disc VIM for covariate s are cal-
s s
culated as mean values of tree-specific contributions to these measures across all B trees:
B B
1 (cid:88) 1 (cid:88)
M Cl VIM = m cl vim , Disc VIM = disc vim (5)
s B s,b s B s,b
b=1 b=1
The tree-specific contributions of the multi-class VIM are defined as follows:
 (cid:80)
n (oob m cl vim −oob m cl vim perm ) #{L }>0
 l∈Ls,b l l l s,b
m cl vim = 0 #{L }=0 (6)
s,b s,b

HereL denotesthesetofallnodesintreebthatusecovariatesinamulti-waysplit, wherenosplit
s,b
incovariatesoccurredinthepathtothisnode, neithermulti-waynorbinary, andaretraversedbyat
least one OOB observation. Additionally, n represents the number of (in-bag) observations in node
l
l. Moreover, oob m cl vim is the value of the multi-way split criterion calculated on the OOB data,
l
where only those OOB observations are used that pass through node l:
(cid:88)
oob m cl vim = p2 , (7)
l OOB,c,Ias
c
c∈Cl
wherep istheproportionofOOBobservationsinchildnodeIas ofnodelthatarefromclass
OOB,c,I cas c
c and Ias is the child node to which class c was assigned during the construction of the tree, that is,
c
9using the in-bag data. In the variant “Non-Squared” (see Section 3.3), the non-squared proportions
p are used in equation (7) accordingly. As mentioned in the introduction, the rationale for
OOB,c,Ias
c
using only OOB data in the calculation of the VIM is to avoid biases caused by overfitting similar to
those of the (uncorrected) Gini importance.
To calculate oob m cl vim perm , the values of the covariate s in the OOB observations that pass
l
node l are randomly permuted and then processed in the same manner as for the calculation of
oob m cl vim .
l
SubtractingthesplitcriterioncalculatedontheOOBdataafterpermutationofthecovariateserves
two purposes. First, it makes the multi-class VIM values of non-informative covariates fluctuate
around zero, facilitating the assessment of which covariates are likely to have an effect and which
are not. Second, this subtraction helps correct for potential bias. Among others, Loecher [2020]
demonstratedthatVIMscalculatedbasedonOOBdatacanalsobebiased. Theprincipleofcalculating
a measure on the OOB data and then subtracting the corresponding value after permutation of the
covariate is also applied in the calculation of the permutation VIM. However, with the latter, this
procedureresultsincorrelatedcovariatesreceivingvariableimportancevaluesthataretoohigh. This
isbecausethepermutationofcovariatesthatarestronglycorrelatedwithothersleadstoextrapolations
into areas of the covariate space with low data density, where the trees provide poor predictions
[Hooker et al., 2021]. This issue does not arise with our approach, since the permutation only affects
the discriminatory capacity of the split at the node where it is performed and not also that of splits
in other covariates at other locations in the trees.
Therationaleforconsideringonlythosenodesinthesplitcriterionforwhichtherehasnotyetbeen
a split in the covariate, either multi-way or binary, in the path to the node is that this prevents bias
in the VIM. After the first multi-way split has been performed in a covariate, this covariate naturally
hashardlyanyinfluenceontheoutcomeinallsubsequentnodesbelowthesplitnode. Similarly, after
the first binary split in a covariate, it is likely that this covariate will have a strongly attenuated and
modified influence in all subsequent nodes. Therefore, the multi-way split criterion values for this
covariate in these nodes do not reflect the actual effect strength of the covariate.
Lastly, the reasons why the tree-specific VIM contributions in formula (6) are multiplied by the
node size n are twofold. On one hand, this multiplication accounts for the fact that splits in larger
l
nodes are more influential. On the other hand, it reflects that covariates with stronger effects tend to
be used for splitting earlier than those with weaker effects.
The calculation of disc vim is carried out as follows:
s,b
 (cid:80) n (oob disc vim −oob disc vim perm ) #{L˜ }>0
 l∈L˜ s,b l l l s,b
disc vim s,b = 0 #{L˜ s,b}=0 (8)

Here, L˜ represents the set of all nodes in tree b that use covariate s in a binary split, where in the
s,b
path to this node there was no previous split in covariate s, neither multi-way nor binary, and are
traversed by at least one OOB observation. As in formula (7), n denotes the number of observations
l
in node l. In the variant “Assign Classes”, the quantities oob disc vim and oob disc vim perm are
l l
calculated analogously as in the case of the multi-class VIM. Here, in the variant “Non-Squared”,
the non-squared proportions p are used accordingly. In the variant “Gini”, the Gini index,
OOB,c,Ias
c
calculated exclusively using the corresponding OOB data, is used in the calculation of oob disc vim
l
and oob disc vim perm .
l
The reasons for the different components of the discriminatory VIM are analoguous to those for
the multi-class VIM.
The discriminatory VIM serves the same purpose as conventional VIMs such as the permutation
VIM.However,theprocedureofcalculatingthediscriminatoryVIMisverysimilartothatofthemulti-
class VIM. Consequently, the values of the discriminatory VIM can be expected to be more directly
comparablewiththoseofthemulti-classVIMthanVIMscalculatedbyacompletelydifferentmethod.
By comparing the covariate-specific multi-class and discriminatory VIM values, it may become more
10convenient to identify which covariates are specifically linked with individual classes and which can
distinguish effectively merely between groups of classes.
Itisessentialthatthemulti-classVIMisbasedonthesplitcriterionusedforthemulti-waysplits.
This approach ensures that the VIM measures the importance of the covariates specifically in terms
of their association with individual classes as opposed to groups of classes. This presents a different
purpose compared to in the case of the Gini importance, which also uses the split criterion in the
computation of the VIM values. The Gini importance assesses the general influence of the covariates
on the prediction, without focusing on specific types of this influence.
5 Simulation study: Comparison of multi-class VIM and dis-
criminatory VIM with conventional VIMs
The motivation behind developing the multi-class VIM was to create a VIM for data with multi-class
outcomes, capable of identifying covariates specifically associated with one or more classes. More
specifically, it should identify what we defined in the introduction as “class-associated covariates” –
covariates with value regions where observations of certain classes are substantially found.
To identify class-associated covariates, they must be ranked higher by the multi-class VIM than
other influential covariates that only distinguish well between groups of classes. In this simulation
study, we investigate the extent to which this is the case. We also evaluate how conventional VIMs
perform in this respect. If conventional VIMs also consistently rank the class-associated covariates
highest, the multi-class VIM would not offer added value compared to conventional VIMs.
The discriminatory VIM proposed alongside the multi-class VIM ranks covariates similarly to
conventionalVIMs. Itassesseshowwellcovariatesseparateobservationsofdifferentclasses. However,
itdoesnotconsiderhowspecificallycovariatesareassociatedwithindividualclasses. Inthissimulation
study, we also investigate the extent to which the discriminatory VIM provides results comparable
to conventional VIMs. Furthermore, we examine whether comparing the multi-class VIM values with
the discriminatory VIM values provides a clearer picture. This comparison may clarify even better
which covariates are class-associated. It may also indicate which covariates are only suitable for
distinguishing between groups of classes.
AllRcodenecessarytoreproducethesimulationstudyandtherealdataanalysis(Section6),along
withthedatasetsused,isavailableonGithub(https://github.com/RomanHornung/MultiForests_
code_and_data).
5.1 Study design
5.1.1 Compared methods
The following VIMs were compared: multi-class VIM, discriminatory VIM, permutation VIM, and
correctedGiniimportance[Nembrinietal.,2018]. AsdescribedinSection3.3,weconsiderfourdiffer-
entversionsoftheMuFalgorithm, alsoaffectingthecalculationofthemulti-classanddiscriminatory
VIMs. These variants are identified as follows in the presentation of the results: wsquared wgini
for the version with squared proportions p2 and split criterion “Gini” for the binary splits,
c,Ias
m,c
wosquared wgini for the version without squaring the proportions p and split criterion “Gini”
c,Ias
m,c
for the binary splits, wsquared wogini for the version with squared proportions p2 and split cri-
c,Ias
m,c
terion “Assign Class” for the binary splits, and wosquared wogini for the version without squaring
the proportions p and split criterion “Assign Class” for the binary splits. The permutation VIM
c,Ias
m,c
and the corrected Gini importance are denoted by perm and gini corr, respectively.
Per MuF, ntree = 5000 trees were trained with npervar = 5 randomly sampled multi-way splits
per sampled covariate. These values were chosen based on an informal analysis of some datasets
used in Section 6 (results not shown). This analysis suggested that the value npervar = 5 provides
sufficiently stable VIMs, with highly correlated VIM values between two forests trained on the same
11data. Furthermore,subsamplinginsteadofbootstrapwasusedtodrawtheobservationsforeachtree.
Here, a proportion of prop=0.7 of the data was subsampled in each case. This choice was based on
real data analyses using many datasets presented in Probst et al. [2019], where subsampling with this
proportion provided the best prediction results on average with conventional RFs. The parameter
mtry, whichrepresentsthenumberofcandidatecovariatesevaluatedateachsplitinthetree, wasset
√
to ⌊ p⌋, where p is the total number of covariates in the dataset. This is the most common default
choice for this parameter in RFs.
For the RFs trained to compute permutation VIM and corrected Gini importance, ntree=5000,
√
mtry =⌊ p⌋, and subsampling with prop=0.7 were also used. This ensured that the comparison of
the VIMs is not confounded by different method configurations.
5.1.2 Data generating processes
We simulated data with normally distributed covariates and multi-class outcomes, considering three
settings for the number of outcome classes: C = 4,6,10. The classes were balanced, that is, each
represented by approximately the same number of observations. For each value of C, the following
samplesizeswereconsidered: n=100,500,1000,2000. ForeachcombinationofC andn,500datasets
were simulated and analyzed using the different variable importance methods.
Eachdatasetcontained50standard-normallydistributedanduncorrelatednoisecovariateswithout
meandifferencesbetweenclasses, denotedasX . Informativecovariatesweredividedintocovariates
no
withmeandifferencesbetweentwo(X )orthree(X )groupsofclasses, andclass-associated
twogr thr gr
covariates where one or more classes had different means than all other classes (X , X ,
clas1 clas2
X , X ). For each type of informative covariate considered, that is, for X , X ,
clas3 clas4 twogr thr gr
X etc., three covariates were generated per dataset. Depending on the number of outcome
clas1
classes, the sets of informative covariates differed slightly. All covariates had a variance of 1 within
classes.
Table 1 shows which covariates were present in each setting and their class-specific mean values.
The class-specific distributions of the covariates are shown in Figure S1 (supplementary material).
The class-specific mean values of the informative covariates were chosen such that the differences
between neighboring classes, with respect to the covariate values, were 0.75 or 1. These differences
were classified as moderate and strong differences, respectively, in the simulation study by Janitza
etal.[2013]. AnexceptionwasmadeforX ,wherethedifferencesweresetatalargervalueof1.5.
tw gr
Thiswasbecauseonlytwogroupsofclassesdifferedforthiscovariate,withnomeandifferenceswithin
these groups, necessitating a greater difference between the two groups to obtain a signal strength
similar to that of other informative covariates.
The mean differences for X and X were chosen smaller than for X and X
clas3 clas4 clas1 clas2
because more classes differ for the former. The more classes differ, the larger the VIM values of
the corresponding covariates become. If the same mean differences had been used, the VIM values
for X and X would have been very large, incomparably larger than those of X and
clas3 clas4 clas1
X .
clas2
Table 1 shows that for C = 10, some pairs of classes have the same mean values for X and
clas3
X . This is because, in scenarios with large numbers of classes, it is of interest to also identify
clas4
covariates that contain regions specific to small subsets of classes, rather than to individual classes
exclusively. In applications with many classes, it is unrealistic to have covariates that can distinguish
between a large proportion of the classes. Moreover, if such covariates are present, they are expected
to have a strong signal, receiving very large variable importance values even from conventional VIMs.
5.1.3 Evaluation
For evaluation, we used an approach similar to that of Janitza et al. [2013], which is based on the
Area Under the ROC Curve (AUC). Typically, the AUC is used in the context of diagnostic tests
where the test results are continuous risk scores. In this context, the AUC represents the probability
12Table 1: Means of the covariates in the simulation study.
C =4
X X X X X X X
no tw gr thr gr clas1 clas2 clas3 clas4
c=1 0 0 0 0 0
c=2 0 0 0 0 0.75
c=3 0 1.5 0 1 1.5
c=4 0 1.5 1 2 2.25
C =6
c=1 0 0 0 0 0 0
c=2 0 0 0 0 0 0
c=3 0 0 1 0 0 0
c=4 0 1.5 1 0 0 0.75
c=5 0 1.5 2 0 1 1.5
c=6 0 1.5 2 1 2 2.25
C =10
c=1 0 0 0 0 0 0 0
c=2 0 0 0 0 0 0 0
c=3 0 0 0 0 0 0 0
c=4 0 0 0 0 0 0 0
c=5 0 0 1 0 0 0 0.75
c=6 0 1.5 1 0 0 0 0.75
c=7 0 1.5 1 0 0 0.75 1.5
c=8 0 1.5 2 0 0 0.75 1.5
c=9 0 1.5 2 0 1 1.5 2.25
c=10 0 1.5 2 1 2 2.25 3
that the test assigns a higher risk score to a randomly selected diseased patient than to a randomly
selected healthy patient.
We used the AUC to measure how strongly the VIMs under consideration assign different im-
portance values to different types of covariates. Specifically, in each case we examined two different
types of covariates. For example, we compared the covariates X , which have mean differences
twogr
only between two groups of classes, and the noise covariates X without mean differences. Here,
no
the corresponding AUC would represent the probability that a randomly selected X covariate
twogr
receives a higher VIM value than a randomly selected noise covariate. Another example is the com-
parison between the covariates X , where two classes have different mean values than all other
clas2
classes, and the covariates X . Here, the AUC would represent the probability that a randomly
twogr
selected X covariate receives a higher VIM value than a randomly selected X covariate.
clas2 twogr
SuchinterpretationsaremoreusefulthancomparingrawVIMvalues. ThisisbecausetheAUCvalues
quantify the extent to which the different VIMs assign higher values to the respective covariates of
interest than to other covariates.
Each AUC value was calculated as the mean of 500 AUC values, each derived from one of the
500 simulated datasets for each setting. Each AUC value for the individual datasets was calculated
using only the VIM values obtained for the two types of covariates considered. For example, when
comparingthecovariatetypesX andX ,onlysixVIMvaluescouldbeusedperdataset,as
clas2 twogr
each dataset contained only three covariates of each type. Consequently, the individual AUC values
calculated from the datasets have a high variance. However, after averaging over the 500 datasets,
the AUC values had an acceptably low variance. This was verified by calculating a 95% confidence
interval for each of the AUC values obtained after averaging. In the calculation of these confidence
intervalsweutilizedthefactthatmeansofindependentnumericalvaluesareasymptoticallynormally
distributed, even if the individual values are not normally distributed.
13As described at the beginning of this section, a further aim of the simulation study was to in-
vestigate the comparison of the multi-class VIM values with the discriminatory VIM values. This
comparison may allow an even clearer picture of which covariates are class-associated and which are
merely suited to distinguish between groups of classes. To investigate this, in addition to the raw
multi-class VIM values, we also considered the differences between the multi-class VIM values and
the corresponding discriminatory VIM values in place of the raw VIM values in the AUC analysis.
5.2 Results
Raw VIM values
Figure 1 shows the raw VIM values of the multi-class VIM and the discriminatory VIM for
the wsquared wgini version and those of perm for all datasets with n = 500. We focus on the
wsquared wgini version because, as we will show later, this version will be the recommended one.
This recommendation will be based on the results described in this section and in Section 6. Here,
this version performed best in ranking class-associated covariates high and in terms of prediction,
respectively. Figures S2 to S13 (supplementary material) show the multi-class and discriminatory
VIM values for all four versions for all four n values considered.
The discriminatory VIM in Figure 1 behaves reassuringly similar to the permutation VIM. Both
VIMsassignedpredominantlyhighVIMvaluesbothtothecovariatesthatdistinguishbetweengroups
of classes (X , X ) and to the class-associated covariates. One exception was X , which
twogr thr gr clas1
was assigned the lowest values among the informative covariates for all VIMs. However, X still
clas1
had consistently higher VIM values than the noise covariates. The overlap of VIM values between
informativeandnon-informativecovariateswouldhavebeengreaterwithsmallereffectsizes. However,
as shown by the class-specific distributions of the covariates in Figure S1 (supplementary material),
the effect sizes are not unrealistically large. It is expected that the VIMs reliably assign larger values
to covariates with moderate to strong signals than to pure noise covariates. We did not focus on the
generalabilityoftheVIMstodistinguishbetweeninformativeandnon-informativecovariates. Instead,
our attention was on the degree to which the multi-class VIM assigns higher values to covariates
specificallyassociatedwithindividualclasses,asopposedtothosethatmerelydistinguishwellbetween
groups of classes. Additionally, we explored how this VIM differs from conventional VIMs and the
discriminatory VIM in this regard.
Figure 1 confirms that, unlike the discriminatory VIM and perm, the multi-class VIM tends to
assignhighervaluestoclass-associatedcovariates. ThisisgenerallytruewiththeexceptionofX .
clas1
Incomparison,itassignslowervaluestocovariatesthatonlydistinguishwellbetweengroupsofclasses.
In the descriptions below, we will quantify these differences and also include the other three versions
of the MuF algorithm and the corrected Gini importance in our comparison.
Comparability between multi-class VIM values and discriminatory VIM values for the different MuF
versions
Themotivationbehindconsideringthevariant“AssignClass”wasthatthesplitcriterionusedinthis
case for the binary splits is more similar to the split criterion used for the multi-way splits than the
Gini split criterion. Consequently, the resulting discriminatory VIM values could have more similar
valuerangestothoseofthemulti-classVIMthanwhenusingtheGinisplitcriterion(variant“Gini”).
However, this is not reflected in the results of our simulation.
A comparison of the discriminatory and multi-class VIM values across the different MuF versions
showsthatthediscriminatoryVIMvaluesdifferstronglybetweenthedifferentversions(FiguresS2to
S13inthesupplementarymaterial). Thisdifferenceisespeciallynotablebetweenthevariants“Assign
Class” and “Gini”. However, the discriminatory VIM values for the variant “Assign Class” are not
more similar to the corresponding multi-class VIM values than for the variant “Gini”.
Upon reflection, this is not surprising. The split criterion used for the “Assign Class” variant is
onlysuperficiallysimilartothesplitcriterionusedformulti-waysplitting. Animportantdifferenceis
14wsquared_wgini perm
C = 4 C = 4
10.0
VIM type
discriminatory
7.5 multi−class 0.04
5.0
0.02
2.5
0.0 0.00
Xno Xtwo_gr Xcl_as1 Xcl_as2 Xcl_as3 Xno Xtwo_gr Xcl_as1 Xcl_as2 Xcl_as3
C = 6 C = 6
0.04
4
0.03
3
2 0.02
1 0.01
0
0.00
Xno Xtwo_gr Xthr_gr Xcl_as1 Xcl_as2 Xcl_as3 Xno Xtwo_gr Xthr_gr Xcl_as1 Xcl_as2 Xcl_as3
C = 10 C = 10
2 0.02
1 0.01
0 0.00
Xno Xtwo_gr Xthr_gr Xcl_as1 Xcl_as2 Xcl_as3 Xcl_as4 Xno Xtwo_gr Xthr_gr Xcl_as1 Xcl_as2 Xcl_as3 Xcl_as4
Covariates Covariates
Figure 1: VIM values obtained for wsquared wgini and the permutation VIM (perm) obtained for
all simulated datasets with n = 500. For visual clarity, the VIM values of only five of the 50 noise
covariates are shown.
thatwithbinarysplitting,morethanoneclassisassignedtoeachsub-nodewhencalculatingthesplit
criterion. Incontrast, withmulti-waysplitting, eachsub-nodeisassignedauniqueclass, exceptwhen
thenumberofuniquecovariatevaluesislessthanthenumberofoutcomeclasses. Anexceptionwhere
the discriminatory VIM values are much more similar to the multi-class VIM values for the variant
“Assign Class” than for the variant “Gini” are the noise covariates (Figure S14 in the supplementary
material). In the case of these covariates, however, this comparability is not relevant. This is because
interpretations regarding the nature of the influence of noise covariates are naturally not meaningful.
Insummary,thepresumedadvantagethatthevariant“AssignClass”wouldleadtodiscriminatory
VIM values more comparable to those of the multi-class VIM is not confirmed by the results of the
simulation study.
AUC analysis: Influential versus noise covariates
The mean AUC values with confidence intervals for the comparison of the influential covariates with
the noise covariates can be found in the Tables S1 to S3 (supplementary material). For all VIMs,
the influential covariates almost always had higher VIM values than the noise covariates. Only in the
15
seulav
MIV
seulav
MIVcase of the smallest number of cases n=100 and almost exclusively for X were the AUC values
clas1
noticeably smaller than 1. For C = 4, the values obtained for X were around 0.95. For C = 6,
clas1
they were between 0.8 and 0.9, and for C =10, they were between 0.65 and 0.8.
InthecaseofthediscriminatoryVIM,theAUCvaluesforwsquared wginiandwosquared wgini
were smaller than those for wsquared wogini and wosquared wogini. For the latter two, the AUC
values were comparable to those for the conventional VIMs perm and gini corr. Nevertheless, in
general, the discriminatory VIM associated with the variant “Gini” does not appear to be notably
disadvantaged compared to the discriminatory VIM associated with the variant “Assign Class”. For
larger numbers of cases or covariates with stronger influences than X , both variants behaved
clas1
similarly. They provided consistently larger VIM values for the influential covariates than for the
noise covariates.
The AUC values obtained for X and n = 100 with the multi-class VIM were consistently
clas1
slightly smaller than the AUC values obtained with the conventional VIMs but differed only slightly
among the four different variants. The confidence intervals are narrow across settings, only in one
case wider than 0.03.
AUC analysis: Class-associated covariates versus X – C =4
two gr
The mean AUC values for the comparison of the VIM values of the class-associated covariates with
those of the X covariates for the setting C = 4 are shown in Figure 2. The AUC values of the
twogr
multi-class VIMs are generally larger than those for the conventional VIMs.
There are only differences between the various multi-class VIM versions for X and X
clas2 clas3
in the case of the smaller sample sizes. Here, wosquared wogini performed the worst and
wosquared wgini the second worst. For larger numbers of cases, the AUC values for all multi-class
VIMs are close to or equal to one. This indicates that the multi-class VIM values of the X and
clas2
X covariates were almost always greater than those of the X covariates. For larger sample
clas3 twogr
sizes, the AUC values of the conventional VIMs also approached one but remained further behind
those of the multi-class VIM versions. The multi-class VIM values obtained for X were smaller
clas1
than the multi-class VIM values obtained for X in the vast majority of cases. However, except
twogr
in the case of wosquared wgini, the AUC values of X were very high for all multi-class VIM
clas1
versions when the discriminatory VIM values were subtracted.
The reason for this is that the drop from X to X was lower for these multi-class VIM
twogr clas1
versions than for the corresponding discriminatory VIM versions. Thus, a comparison between the
multi-class VIM and the discriminatory VIM would have offered added value here. Only such a
comparison could have indicated that X is a class-associated covariate.
clas1
AUC analysis: Class-associated covariates versus X and X – C =6
two gr thr gr
Figure 3 shows the comparison of the VIM values between the class-associated covariates and the
X and X covariates for the setting C = 6. As in the setting C = 4, the multi-class VIM
twogr thr gr
valuesofthecovariatesX andX arealmostalwaysgreaterthanthoseoftheX values.
clas2 clas3 twogr
This is not observed for the conventional VIMs. Here, the AUC values for these covariates are always
approximately0.5,regardlessofthenumberofcasesconsidered. Thisillustratesthattheconventional
VIMsare conceptuallynot suitablefordistinguishing theclass-associatedcovariates fromthe X
twogr
covariates.
As in the setting C = 4, the versions wosquared wogini and wosquared wgini perform worst.
The versions wsquared wgini and wsquared wogini perform very similarly, with wsquared wgini
being associated with slightly higher AUC values.
The differences between the multi-class VIM values of the class-associated covariates and the
X covariates are much smaller than those of the former and the X covariates. Only the
thr gr twogr
wsquared wgini and wsquared wogini versions provided larger AUC values than the conventional
VIMs. Thesevalueswerearound0.8,indicatingthatthemulti-classVIMvaluesoftheclass-associated
covariates were larger than those of the X covariates in most cases.
thr gr
16n = 100 n = 500
1.00
0.75
0.50 Method
wsquared_wgini
0.25 wosquared_wgini
wsquared_wogini
wosquared_wogini
0.00
perm
n = 1000 n = 2000 gini_corr
1.00
Type
0.75 conventional
multi−class
0.50 multi−class diff.
0.25
0.00
X X X X X X
cl_as1 cl_as2 cl_as3 cl_as1 cl_as2 cl_as3
Figure 2: Mean AUC values per considered sample size and method for C =4. The line types distin-
guish the different VIM types, where conventional corresponds to conventional VIMs, multi-class
to multi-class VIMs, and multi-class diff. to the differences between the multi-class VIM values
and the corresponding discriminatory VIM values.
Comparison with X Comparison with X
two_gr thr_gr
n = 100 n = 500 n = 100 n = 500
1.00 1.00
0.75 0.75
Method
0.50 0.50 wsquared_wgini
wosquared_wgini
0.25 0.25 wsquared_wogini
wosquared_wogini
0.00 0.00
perm
n = 1000 n = 2000 n = 1000 n = 2000 gini_corr
1.00 1.00
Type
0.75 0.75
conventional
0.50 0.50 multi−class
multi−class diff.
0.25 0.25
0.00 0.00
X X X X X X X X X X X X
cl_as1 cl_as2 cl_as3 cl_as1 cl_as2 cl_as3 cl_as1 cl_as2 cl_as3 cl_as1 cl_as2 cl_as3
Figure 3: Mean AUC values per considered sample size and method for C =6. The line types distin-
guish the different VIM types, where conventional corresponds to conventional VIMs, multi-class
to multi-class VIMs, and multi-class diff. to the differences between the multi-class VIM values
and the corresponding discriminatory VIM values.
17
CUA
CUA CUAForX andespeciallyX ,theAUCvaluesofthemulti-classVIMvaluesaftersubtracting
clas2 clas3
the discriminatory VIM values are worse than those of the raw multi-class VIM values. The cause of
this is particularly evident in the case of X by examining the raw VIM values (Figures S6 to S9
clas3
inthesupplementarymaterial). Here,notonlyarethemulti-classVIMvaluesforX higherthan
clas3
thosefortheothercovariates,butthesametrendholdsforthediscriminatoryVIMvalues,albeittoa
lesser extent. Therefore, subtracting the discriminatory VIM values results in differences for X
clas3
that are less distinct from those for X than when using the raw multi-class VIM values.
thr gr
As also seen for C = 4, in the comparison with X , the AUC values for X are much
twogr clas1
larger in many cases after subtracting the discriminatory VIM values. It is noteworthy that in the
comparison with X , only the AUC value of wsquared wogini is high when the corresponding
thr gr
discriminatory VIM values are subtracted. This is because, for wsquared wogini, the drop in the
discriminatory VIM values from X to X is much greater than the drop in the multi-class
thr gr clas1
VIM values. This pattern is not observed for the other MuF versions.
AUC analysis: Class-associated covariates versus X and X – C =10
two gr thr gr
The AUC values obtained for the C = 10 setting are shown in Figure 4. Here, wsquared wgini and
wsquared wogini delivered the best results by a wide margin. The differences between the AUC
values achieved for these two methods are very small. It is particularly striking that only these two
methods assigned larger multi-class VIM values to the covariates X and X than to the
clas2 clas3
X covariates. With the exception of X , the conventional VIMs largely assigned smaller
thr gr clas4
values to the class-associated covariates than to the X and X covariates. The covariate
twogr thr gr
X receivedthehighestvaluesamongallcovariatesforallVIMs,includingtheconventionalVIMs.
clas4
This suggests that the influence of X is so strong that it is also prioritized by the conventional
clas4
VIMs,eventhoughothercovariateswithstronginfluencesbutnoclass-specificassociationarepresent.
This demonstrates that conventional VIMs do not automatically rank class-associated covariates
lower than covariates without class-specific influence. However, the multi-class VIM helps to specif-
ically select class-associated covariates, as it ranks covariates without class-specific influence lower.
After subtracting the discriminatory VIM values, the AUC values for X for two MuF versions
clas4
(wsquared wogini, wosquared wogini) are smaller for the comparison with X . This is because
thr gr
both the multi-class VIM values and the discriminatory VIM values for the X covariates are
clas4
larger than those for the X covariates.
thr gr
ForboththecomparisonoftheX covariateswiththeX covariatesandwiththeX
clas1 twogr thr gr
covariates, subtracting the corresponding discriminatory VIM values resulted in large AUC values for
all MuF variants except wosquared wgini. As for C = 4 and C = 6, this is because, for these
variants, the decrease between X and X and between X and X was weaker
twogr clas1 thr gr clas1
for the multi-class VIM than for the discriminatory VIM (Figures S10 to S13 in the supplementary
material).
The exact AUC values shown in Figures 2 to 4 are listed in Tables S4 to S8 (supplementary
material), with 95% confidence intervals. These intervals are narrow and never wider than 0.05.
Summary and conclusions
To summarize, the multi-class VIMs typically ranked the class-associated covariates highest. This is
unliketheconventionalVIMs,whichrankedtheinformativecovariateswithoutclass-specificinfluence
either similarly high or higher. This pattern held except in the setting with the smallest number
of classes. Another exception was the setting with the smallest sample size (n = 100), where the
multi-class VIMs did not reliably rank the class-associated covariates highest.
Thetwoversionswsquared wginiandwsquared woginiperformedbetterthantheotherversions.
This suggests that squaring the proportions p is important to reliably identify class-associated
c,Ias
m,c
covariates. In hindsight, this result is not surprising. By definition, class-associated covariates have
regions that substantially contain observations from specific classes. Squaring the proportions p
c,Ias
m,c
rewards these regions more than when the proportions are not squared.
18Comparison with Xtwo_gr Comparison with Xthr_gr
n = 100 n = 500 n = 100 n = 500
1.00 1.00
0.75 0.75
Method
0.50 0.50 wsquared_wgini
wosquared_wgini
0.25 0.25 wsquared_wogini
wosquared_wogini
0.00 0.00
perm
n = 1000 n = 2000 n = 1000 n = 2000 gini_corr
1.00 1.00
Type
0.75 0.75
conventional
0.50 0.50 multi−class
multi−class diff.
0.25 0.25
0.00 0.00
Xcl_as1Xcl_as2Xcl_as3Xcl_as4 Xcl_as1Xcl_as2Xcl_as3Xcl_as4 Xcl_as1Xcl_as2Xcl_as3Xcl_as4 Xcl_as1Xcl_as2Xcl_as3Xcl_as4
Figure4: MeanAUCvaluesperconsideredsamplesizeandmethodforC =10. Thelinetypesdistin-
guish the different VIM types, where conventional corresponds to conventional VIMs, multi-class
to multi-class VIMs and multi-class diff. to the differences between the multi-class VIM values
and the corresponding discriminatory VIM values.
Comparing the multi-class VIM values with the discriminatory VIM values can help identify co-
variatesthathaverelativelysmallmulti-classVIMvaluesbutarestillclass-associated. Suchcovariates
alsohavecorrespondinglysmalldiscriminatoryVIMvalues. AnexampleisX inFigure1. Addi-
clas1
tionally,comparingthediscriminatoryVIMvaluescanidentifycovariatesthat,whilehavingrelatively
large multi-class VIM values, exhibit particularly large discriminatory VIM values, indicating they
areonlyslightlyclass-associated. ExamplesinFigure1areX forC =6andC =10andX
twogr thr gr
for C =10.
However,comparingwiththediscriminatoryVIMvaluesisnotalwaysinformative. Class-associated
covariatescanalsohavehighvalueswithrespecttothediscriminatoryVIM.Forexample,inFigure1
for C = 6 and C = 10, the class-associated covariates with the strongest influences have both the
largest multi-class VIM values and the largest discriminatory VIM values among all covariates.
The results did not indicate that the split criterion associated with the variant “Assign Class” for
the binary splits would lead to discriminatory VIM values more similar to the multi-class VIM values
than the classic Gini split criterion. On the contrary, with the “Assign Class” split criterion, the dis-
criminatoryVIMvaluesweresystematicallylargerthanthemulti-classVIMvalues. Conversely,using
the Gini split criterion for the binary splits resulted in discriminatory VIM values that were higher
for some covariates and lower for others compared to the multi-class VIM values. This variability
could provide clearer insights into which covariates are class-associated and which are influential but
not class-associated. However, it is unclear to what extent the observation that the discriminatory
VIM values are larger for some covariates and smaller for others than the multi-class VIM values is
generalizable. Therefore, direct comparisons between the multi-class VIM values and the discrimina-
tory VIM values should always be made with caution to avoid overinterpretation. In case of doubt,
interpretationshouldbelimitedtothemulti-classVIMvaluesifthegoalistoidentifycovariateswith
the strongest class-associated effects.
Due to the better performance of the variants “Squared” and “Gini” observed in this simulation
study, we recommend the version wsquared wgini for identifying class-associated covariates.
19
CUA CUA6 Real data analysis: Comparison of the predictive perfor-
mance of multi forests with that of conventional random
forests
As outlined in the introduction, the primary goal of developing the MuF algorithm was to realize
the multi-class VIM. It was not anticipated that MuFs would surpass conventional RFs in terms of
predictive performance.
Nevertheless, it is reasonable to expect that users will employ MuFs not only to identify class-
associated covariates but also for prediction purposes. Consequently, if MuFs demonstrate substan-
tially inferior predictive performance compared to conventional RFs, caution should be advised when
usingthemforprediction. Therefore,weinvestigatedthepredictiveperformanceofMuFs,inaddition
to evaluating the performance of the multi-class VIM and the associated discriminatory VIM. Our
analysis involved 121 real datasets with multi-class outcomes. In this study, we compared the predic-
tive performance of the four versions of MuFs, as described in Section 3.3, with that of conventional
RFs across four different performance metrics.
6.1 Data
The datasets were primarily obtained from the Open Science online platform OpenML [Vanschoren
et al., 2013]. We generally searched for datasets with multi-class outcomes, but in particular made
sure that we included all suitable datasets from the curated dataset collection OpenML-CC18 [Bischl
et al., 2021]. These were complemented by appropriate datasets from the Penn Machine Learning
Benchmarks (PMLB) collection [Romano et al., 2022].
Initially, datasets were filtered to include those with three to 20 outcome classes, a maximum of
15000 observations, between two and 500 covariates, and no missing values. The restrictions on the
number of observations and covariates served to limit the computational burden. The datasets that
passed these initial filters were then manually curated. Here we excluded redundant and many sim-
ulated datasets. However, we kept those simulated datasets whose descriptions indicated that they
realistically represented the properties of real data. There were often groups of related, very similar
datasets in the collection. In general, we selected only one dataset from each such group. Excep-
tions were groups of related datasets in which the datasets involved appeared adequately different.
Furthermore, datasets with repeated measurements were excluded.
The selected datasets were pre-processed by removing categorical covariates with more than 50
categories,aswellasovertlynon-informativecovariatessuchassubjectIDs. Insomecases,categorical
covariates were cast as metric covariates in R following download from OpenML. We converted these
covariates into factors. This step was necessary for the MuF and RF implementations to correctly
identify them as categorical covariates. Further details on the preprocessing are provided in the
corresponding commented R script in the code publicly available on GitHub.
Ultimately, 121datasetswereincludedinthecomparisonstudy. Detailedinformationaboutthese
datasets,suchasthenumberofobservations,covariates,andoutcomeclasses,isavailableinTablesS9
to S11 (supplementary material).
Despite meticulous efforts to maintain high dataset quality, it cannot be guaranteed that all
datasets meet stringent quality standards. However, given the large number of datasets, and our
approach to interpreting results in aggregate rather than individually, the likelihood that minor qual-
ity issues have influenced our overall findings is minimal.
6.2 Study design
Using the 121 datasets described above, we compared the predictive performance of the four versions
of the MuF algorithm outlined in Section 3.3 with that of conventional RFs.
20FortheMuFs,asinthesimulationstudy(Section5),subsamplingwithaproportionofprop=0.7
was used instead of bootstrap for sampling observations for tree training. In each split, npervar =5
√
multi-way splits were again evaluated per sampled covariate and mtry was again set to ⌊ p⌋, with p
representing the number of covariates in the respective dataset. To limit the computational burden,
the number of trees per forest, ntree, varied with the dataset size: 5000 trees for datasets up to 5000
observations (86% of all datasets) and 1000 trees for datasets exceeding 5000 observations.
The configuration of the RFs was again adapted to that of the MuFs to avoid confounding effects
duetodifferentmethodconfigurations. ThesamenumbersoftreeswereusedaswithMuFsandmtry
√
was set to ⌊ p⌋. Subsampling with prop = 0.7 was again used instead of bootstrap. Additionally,
as with MuFs, the categories of unordered categorical covariates were sorted using the PCA-based
approach by Coppersmith et al. [1999].
In the presentation of the results, the different versions of the MuF algorithm are labeled as
wsquared wgini, wosquared wgini, wsquared wogini, and wosquared wogini, consistent with the
previous section. The conventional RFs are also labeled in typewriter font as RF for consistency.
Weusedthefollowingperformancemetrics: AUNU,AUNP[Fawcett,2001],theBrierscore[Brier,
1950], and the accuracy. Both AUNU and AUNP are based on the AUC. For both measures, C AUC
valuesaresummedup, whereforc∈{1,...,C}thec-thAUCvalueiscalculatedfortheclassification
problem“classcyesvs.classcno”. Thedifferencebetweenthetwomeasuresisintheweightingofthe
AUCvalues. FortheAUNP,theAUCvaluesareweightedwiththecorrespondingclassfrequencies. In
contrast, when calculating the AUNU, they are weighted with 1/C, meaning that the simple average
oftheAUCvaluesisformed. TheAUNPthereforeweightslargeclassesmoreheavily,whiletheAUNU
weights all classes equally, regardless of how many observations they are represented by.
The Brier score measures the quality of probability predictions in relation to the true classes.
Therefore, the Brier score is a measure of the calibration of models. However, it is also a measure
of the discrimination in the sense that it measures how well observations of different classes can be
correctlydistinguishedfromeachotherbytheevaluatedmodel. TheAUC,ontheotherhand, isonly
ameasure ofdiscrimination, but notofcalibration. Theaccuracyshould be interpretedwithcaution,
as its value can depend heavily on the class distribution. For all forest types, the classes with the
highest predicted probabilities were taken as the predicted classes when calculating the accuracy. For
brevity, the Brier score and accuracy are hereafter referred to as “Brier” and “ACC”, respectively.
The performance metrics were estimated for each dataset-method combination using 5-fold strat-
ified cross-validation, repeated five times.
6.3 Results
Table 2 presents the medians of the cross-validated performance metrics obtained by the methods
acrossall121datasets,alongwiththecorrespondingfirstandthirdquartiles. Thedifferencesinthese
median values are generally small. However, RF consistently shows the best performance, except in
the case of AUNU. For AUNU, wsquared wgini had a slightly higher median value than RF.2 For
the other performance metrics, wsquared wgini ranked as the second-best, with the median of the
ACC values for wsquared wgini matching that of wsquared wogini. Apart from this, the rankings
among the medians achieved with the different MuF versions are not entirely consistent across the
different performance metrics. The difference in performance between the different MuF versions and
RF is more marked than between the MuF versions themselves. Although the differences among the
methodsaremainlysmall,thedifferencebetweenRFandtheMuFversionsisnotablygreaterforBrier.
The rankings between the methods with regard to the first and third quartiles are largely consistent
with those with regard to the medians.
Toobtainamoredetailedinsightintodifferencesinperformancebetweenthemethods,weexamine
Figure5. Thisfigurevisualizestheranksthemethodsachievedforeachdatasetcomparedtotheother
2However,Figure5discussedbelowdoesnotsupportthesuggestionthatwsquared wginioutperformsRFconcerning
theAUNU.
21Table 2: Performance of the methods summarized over the 121 datasets. The values outside the
parentheses show the medians of the cross-validated performance metrics, and the values inside the
parentheses show the corresponding first and third quartiles (25% and 75% quantiles). Except in the
case of Brier, larger values indicate better performance.
method AUNU AUNP Brier ACC
RF 0.9537 0.9571 0.3039 0.8159
[0.7860, 0.9972] [0.7866, 0.9972] [0.0960, 0.4927] [0.6187, 0.9616]
wsquared wgini 0.9575 0.9534 0.3353 0.8111
[0.7876, 0.9973] [0.7836, 0.9970] [0.1253, 0.5128] [0.6192, 0.9589]
wosquared wgini 0.9570 0.9528 0.3365 0.8110
[0.7852, 0.9973] [0.7785, 0.9970] [0.1258, 0.5126] [0.6181, 0.9580]
wsquared wogini 0.9572 0.9531 0.3394 0.8111
[0.7832, 0.9970] [0.7782, 0.9970] [0.1261, 0.5136] [0.6201, 0.9549]
wosquared wogini 0.9542 0.9523 0.3378 0.8103
[0.7885, 0.9969] [0.7770, 0.9969] [0.1270, 0.5173] [0.6131, 0.9544]
methods, where lower ranks indicate better relative performance. It is evident across all performance
metricsthatRFachievedthebestpredictiveperformancemostfrequently. Thisisparticularlyevident
for Brier, where RF delivered the best predictions for almost 70% of the datasets. Additionally,
this analysis confirms that wsquared wgini consistently provided the second-best results. Across
all performance metrics, wsquared wgini delivered the best or second-best results more often than
the other MuF versions. Notably, RF not only achieved the best results most frequently but also
frequently the worst results. RF performed worst across all performance metrics more often than the
MuF versions, with theexception of wosquared wogini. The latterversionmost frequently delivered
theworstresultsforallperformancemetricsandgenerallyrankedworstamongthecomparedmethods.
The differences in the rankings between wosquared wgini and wsquared wogini are small; however,
the former generally yielded marginally better results than the latter.
Given that wsquared wgini yielded the best predictive performance and wosquared wogini the
worst, the variants “Squared” and “Gini” seem beneficial not only in identifying class-associated
covariates (Section 5) but also in prediction. Thus, wsquared wgini is the sole MuF version rec-
ommended and is the only version available in the R package diversityForest, which implements
MuFs.
We tested for differences between the performance metric values obtained with RF and the MuF
versions. However,wedidnottestfordifferencesamongtheMuFversions. Thisapproachreducedthe
number of comparisons and, consequently, the number of p-values requiring adjustment for multiple
testing. Asaresult,thepowerofthetestswasincreasedoverwhatwouldhavebeenachievedwithmore
comparisons. Furthermore,itisofminimalrelevancetodeterminewhetherthepredictiveperformance
ofthedifferentMuFversionsisstatisticallysignificantlydifferent. Asnotedabove,thesefourversions
are alternatives, and only one was recommended for application. The recommendation was based
solely on the observed performance. It was not relevant here whether the observed performance of
the selected version was statistically significantly different from that of the other versions.
We conducted paired Wilcoxon tests, treating the cross-validated performance metrics from the
different datasets as independent observations. The four p-values obtained for each performance
metricwerethenadjustedformultipletestingusingtheHolm-Bonferroniprocedure. Table3presents
the test results. Except for the comparison between wsquared wgini and RF for AUNU and AUNP,
alldifferencestestedarestatisticallysignificant. Theeffectsizevalues,r,forAUNU,AUNP,andACC
ranged from small to medium, while those for Brier indicated larger performance differences between
the MuF versions and RF.
We acknowledge that the p-values listed in Table 3 might be slightly underestimated. This is due
to the fact that, as outlined in Section 6.1, there are occasionally groups among the datasets used
22AUNU AUNP
125
100
75
50
25
Rank
1, 1.5
0
2, 2.5
ACC Brier
3, 3.5
125
4, 4.5
5
100
75
50
25
0
RF gini gini gini gini RF gini gini gini gini
w w o o w w o o
_ _ w w _ _ w w
d d _ _ d d _ _
e e d d e e d d
uar uar are are uar uar are are
q q u u q q u u
ws wos wsq osq ws wos wsq osq
w w
Method
Figure 5: Ranks of the MuF versions and RF with respect to the different performance metrics. Each
stackedbarrepresentsthenumberofdatasetsforwhichtherespectivemethodachievedtheindicated
ranks among all other methods.
that are related for different reasons. The datasets within these groups are therefore not completely
independent. However,asdescribedinSection6.1,weselectedonlyonedatasetfromgroupscontaining
very similar datasets, which should minimize dependencies. Moreover, the p-values are in most cases
substantiallylowerthanthesignificancelevelα=0.05,makingthepresenceoffalsepositivesinthese
results unlikely.
In Figure 6, we plot the performance metric values of wsquared wgini and RF obtained for the
individualdatasetsagainsteachother. Thecorrelationbetweenthesevaluesisconsistentlystrong. For
AUNUandACC,theobserveddifferencesbetweenthetwomethodsarenegligible. However,consistent
23
stesatad
fo
rebmuNTable 3: Results of Wilcoxon tests between the values of the cross-validated metrics obtained with
the different MuF versions and RF. The values outside and inside of the round brackets represent the
effect sizes r and the Holm-Bonferroni adjusted p-values of the tests, respectively.
method AUNU AUNP ACC Brier
wsquared wgini 0.14 (0.134) 0.10 (0.287) 0.27 (0.003) 0.55 (< 0.001)
wosquared wgini 0.27 (0.006) 0.25 (0.015) 0.35 (< 0.001) 0.58 (< 0.001)
wsquared wogini 0.29 (0.004) 0.26 (0.015) 0.31 (0.001) 0.60 (< 0.001)
wosquared wogini 0.38 (< 0.001) 0.33 (0.001) 0.40 (< 0.001) 0.63 (< 0.001)
with the Wilcoxon test results, the differences for Brier are more pronounced, especially in datasets
containing many classes (C ≥ 10). Figure S15 (supplementary material) is a version of Figure 6
in which we present the results separately according to whether the datasets contain categorical
covariates or not. This figure indicates that the larger Brier differences predominantly occured in
datasets lacking categorical covariates. Potential reasons for this observation will be explored in the
subsequent discussion section.
AUNU ACC Brier
1.00 1.00 1.00
Number of classes
0.99 C<10 0.99
C‡ 10 0.95
0.95 0.50
0.80
0.25
0.80
0.15
0.50
0.05
0.50 0.20 0.01
0.50 0.80 0.95 0.991.00 0.20 0.50 0.80 0.950.991.00 0.010.05 0.150.25 0.50 1.00
wsquared_wgini wsquared_wgini wsquared_wgini
Figure 6: Dataset-specific cross-validated performance metric values: wsquared wgini versus RF.
The white diamonds mark the datasets that have ten or more outcome classes. The dashed lines
represent the diagonal. Axes were transformed for visual clarity: A negative complementary square
root transformation was applied to AUNU and ACC, and a square root transformation was used
for Brier. AUNP is not included here due to its results being very similar to AUNU and for space
considerations. Figure S16 (supplementary material) shows a version of this figure that includes
AUNP.
7 Discussion
Overview of the proposed methodology and the results of the comparative studies
In this paper, we have presented and evaluated a variant of RFs, named MuFs, tailored for datasets
withmulti-classoutcomesinconjunctionwithtwoVIMs: themulti-classVIMandthediscriminatory
VIM. Unlike conventional RFs that use binary splitting only, MuFs in addition employ multi-way
splits, dividing the nodes into more than two child nodes. The split criterion used here is aimed at
finding multi-way splits where each child node substantially comprises observations from a different
class.
24
FR FR FRThe objective of the multi-class VIM is to identify what we termed class-associated covariates.
These covariates are specifically linked to one or more classes and are characterized by having value
regions where these classes are prominently represented. Conventional VIMs tend to also highly rank
covariatesthatmerelydifferentiatebetweengroupsofclassesratherthanbeingassociatedwithspecific
classes. These covariates should be assigned lower importance scores by the multi-class VIM. On the
otherhand,likeconventionalVIMs,thediscriminatoryVIMaimstohighlyrankinfluentialcovariates
of any type, irrespective of their association with specific classes.
In our simulations, the multi-class VIM ranked class-associated covariates higher than other in-
fluential covariates in the great majority of cases, except in the case of the smallest sample size
considered. In contrast, the conventional VIMs generally failed to differentiate class-associated co-
variatesfromotherinfluentialcovariates. Anexceptionoccurredwithclass-associatedcovariateswith
very strong influences; these were ranked highest by all VIMs evaluated. The discriminatory VIM
displayed behavior very similar to that of the conventional VIMs.
TheextensiverealdataanalysisrevealedthatinmanycasesMuFshaveaslightlyinferiorpredictive
performance compared to conventional RFs. The differences were minor in terms of accuracy and the
performancemetricsAUNUandAUNP,whichassessdiscriminationability. Slightlylargerdifferences
were observed for the Brier score, which additionally evaluates model calibration. Consequently,
the probability predictions made by MuFs appear to be slightly less well calibrated than those of
conventional RFs, without this considerably impairing their classification performance.
Importance of interpretability and problem-specific predictive performance
The primary focus of the MuF algorithm is, however, not prediction but rather interpretation, facili-
tatedbythemulti-classVIMandthediscriminatoryVIM.Consequently,thefactthatMuFsoftendo
notsurpassconventionalRFsinpredictiveperformanceisnotaparticularlimitation. Manyclassifica-
tionandregressionmethodsremainwidelyusedinpracticedespiteevidencethatothermethodsoften
yield superior prediction results. An important reason for this is that, like MuFs, these methods of-
ten provide benefits in terms of interpretability. For instance, classical generalized linear models offer
clear,easilycommunicablecovariateeffects,whichisasubstantialadvantageovermanycontemporary
machine learning models.
Another reason for the continued use of methods that achieve better prediction results less fre-
quently than other methods is the “No Free Lunch” theorem. According to this theorem, no clas-
sification or regression method can universally perform best across all possible prediction problems
[Wolpert and Macready, 1997]. This highlights the importance of selecting methods based on context
and their specific performance in the prediction problem being addressed.
Inscenarioswheretargetingthehighestpossiblepredictiveperformanceiscritical, werecommend
considering alternative methods in addition to MuFs, such as conventional RFs.
Suboptimal predictive performance: Possible reasons and problems for large numbers of classes
One potential cause of MuFs’ suboptimal predictive performance is the fact already mentioned in the
introductionthatmulti-waysplitspartitionthedatarecursivelyveryquickly. Thiscouldresultinfewer
covariates being used within each tree compared to conventional RFs and the interactions between
the covariates being less exploited. Although incorporating binary splits in the MuF algorithm was
intended to mitigate these issues, it likely did not fully resolve them.
These challenges are likely especially pronounced in scenarios with large numbers of outcome
classes. In such cases, each node is divided into many child nodes, leading to particularly rapid
data partitioning. This was also reflected in the real data analysis (Section 6), where datasets with
the strongest declines in MuFs’ predictive performance relative to RFs typically had many outcome
classes (C ≥ 10). Consequently, for prediction problems involving 10 or more outcome classes, we
recommendusingtheMuFalgorithmsolelytocomputethemulti-classanddiscriminatoryVIMvalues.
Forprediction,conventionalRFsorotherappropriatemethodsshouldbeusedinsuchscenarios. Note
also that the MuF algorithm was not designed for scenarios involving very large numbers of outcome
25classes. This is also the reason why only datasets with up to 20 outcome classes were included in the
real data analysis.
Furthermore, in the MuF algorithm, the split criterion used for multi-way splits that involve a
number of unique covariate values at least as great as the number of outcome classes exclusively
rewardsclass-associatedcovariates. Thislikelyresultsinanunder-selectionofotherinfluentialcovari-
ates for such splits. In contrast, conventional RFs generally select influential covariates, irrespective
of class association. This likely results in conventional RFs more effectively leveraging the available
predictive information within the covariates.
In the real data analysis, we observed that the predictive performance of MuFs was worse relative
to that of conventional VIMs in datasets lacking categorical covariates. This may be attributed to
the general potential reasons for the suboptimal predictive performance of MuFs discussed above.
First, it is to be expected that the recursive partitioning of the data is particularly fast when only
continuous covariates are present. This is because multi-way splits based on continuous covariates
generallyresultinmorechildnodesthanthosebasedoncategoricalcovariates. Second,theadvantage
of conventional RFs that they take into account influential covariates, also if they are not class-
associated, can be expected to diminish as the proportion of categorical covariates increases. This is
because, for categorical covariates, the MuF algorithm generally does not employ the split criterion
that rewards only class-associated covariates. Consequently, the limitation of MuFs in exploiting the
predictiveinformationinthecovariateslesseffectively,becomeslesspronouncedwhenmorecategorical
covariates are present.
The importance of class-specific intervals for interpretability
Inthemulti-waysplitsofMuFs,thecovariatevaluesaregroupedintoadjacentintervalscorresponding
to different classes. Therefore, only those class-associated covariates where one or more intervals
containnumerousobservationsofdifferentclassesareadequatelyconsidered. Covariateswithmultiple
intervals dominantly representing the same classes are likely less prioritized.
Given that the multi-class VIM is based on the multi-way splits, it is thus likely that the last-
mentioned types of covariates, hereafter referred to as “type II covariates”, generally receive lower
multi-class VIM values than the first-mentioned types of covariates, hereafter referred to as “type
I covariates”. This distinction is beneficial for interpretability because type I covariates have more
easilyinterpretableinfluences. Forinstance, theinfluenceofatypeIcovariatecouldbethefollowing:
For small values of the covariate, class A may be likely, for slightly larger values, however, class B
maybemorelikelyandforlargevalues,classC mightbemostlikely. Incontrast, apossibleinfluence
of a type II covariate could be the following: For small or very large values of the covariate, class A
may likely, while for moderate or large values, classes B or C might be more likely, respectively.
Reason for not optimizing the tuning parameters
Similar to conventional RFs, the MuF algorithm incorporates several tuning parameters. To limit
the computational effort, these were not optimized in our comparative studies. This approach is
supported by the findings of Probst et al. [2019] based on many datasets, which suggest that the
performance of RFs is only slightly affected by the tuning parameter values. Since MuFs are special
casesofdiversityforests,whichinturnareasubclassofRFs,itisreasonabletoassumesimilarfindings
apply to MuFs. In addition, the influence of the parameter nsplits on the performance of diversity
forests was investigated in Hornung [2022] in an extensive empirical analysis. Here, similar to with
conventionalRFs,theinfluenceofthisparameterwasverysmallforthemajorityofthedatasetsused.
Note that the parameter nsplits is referred to as mtry in the MuF algorithm, deliberately mirroring
the terminology used in conventional RFs to highlight the similarities of these parameters.
Intheanalysesreportedinthispaper,formosttuningparameterswechosedefaultvaluesbasedon
empiricalevidence-basedrecommendationsfromexistingliteratureonconventionalRFs(subsampling,
prop, mtry). As outlined in Section 5.1.1, the default values for the parameters ntree and npervar
were determined using some of the datasets from the real data analysis. However, it can be safely
assumed that this did not lead to an optimistic bias in the results. The selection of the default values
26was not based on the predictive performance nor on the VIM values obtained. It was based solely on
the observed stability of the VIM values obtained.
Identification of covariates associated with all classes: Conceptual difficulties and possible realization
in the MuF algorithm
Theaimofthemulti-classVIMistoidentifycovariatesspecificallyassociatedwithoneormoreclasses.
Bycontrast,theapproachesofZinietal.[2015]andSongetal.[2017],asdiscussedinSection2,focus
onidentifyingcovariatesassociatedwithallclasses. Consequently,thedistributionsofthesecovariates
exhibit differences between all classes.
However, the intention to exclusively search for such covariates might not always be reasonable.
Onereasonisthatthesecovariatesoftenrepresentonlyasmallproportionofallinformativecovariates.
Additionally, for covariates that do exhibit differences across all classes, these differences are unlikely
to be large if they are only detectable through specialized procedures. This is because covariates
showing notably strong differences across all classes typically exert such a strong influence that they
arealreadyhighlyrankedbyconventionalVIMs. Conversely,ifcovariatesshowonlyminordifferences
across all classes, their influence is likely not of substantive interest.
Forscenarioswhereitisstillrelevanttoidentifyexclusivelycovariateswithdifferencesbetweenall
classes, a potential modification of the MuF algorithm could be developed. In this modification, both
the multi-way splitting criterion and the calculation of the multi-class VIM might be based on the
minimum of the squared proportions p2 , rather than on their sum. By maximizing the minimum
c,Ias
m,c
proportions, specific types of multi-way splits would be favored. Such splits would ensure that none
oftheresultingintervalscontainmerelyafewobservationsfromtheclassesassignedtotherespective
child nodes. However, this potential modification of the MuF algorithm has not been explored, and
thus no assertions can be made about its efficacy.
Graphical exploration of covariate influences to supplement the multi-class VIM analysis
Whether the goal is to identify covariates associated with all classes or, more generally, covariates
associatedwithoneormorespecificclasses, multi-classVIMvaluesdonotprovideinformationabout
the forms of the influences of these covariates. To gain insights into how these covariates impact the
outcome, graphicalrepresentationtechniquescanbeused. ThediversityForestRpackageincludes
functions that estimate and visualize the dependency structures of the multi-class outcome on the
covariates. Each covariate of interest is represented in a separate plot. The package offers two types
of visualizations: kernel density estimate-based plots and boxplot-based plots.
Kernel density estimate-based plots display the within-class distributions of the covariate in a
single graph, where the density estimates are scaled according to the class proportions to account
for unequal class sizes. These plots facilitate learning which classes are most dominant in different
covariate value regions. On the other hand, boxplot-based plots present the estimated within-class
distributions side by side, using boxplots. This type of visualization makes it easier to determine
where in the covariate values most of the observations from each class are concentrated, which can be
less apparent in kernel density estimate-based plots, especially when the number of classes is large.
However, unlike kernel density estimates, the boxplot-based plots often do not allow to judge which
classes dominate specific regions.
Westronglyencourageinterestedreaderstoconsulttheillustrativeanalysesintheexamplesections
of the plotting functions within the diversityForest R package. These examples demonstrate how
these visualizations can be used in a multi-forest analysis.
8 Conclusions
MuFs represent a variant of RFs tailored for multi-class outcomes. The most important aspect of
the MuF algorithm, however, is not the predictive models it can create, but the multi-class VIM.
This tool enables the specific identification of class-associated covariates, which are distinguished
27by their specific association with one or more classes. Conventional VIMs do not allow this, as
they target influential covariates indiscriminately without focusing on class association. Similarly,
the discriminatory VIM, also part of the MuF algorithm, allows for the identification of influential
covariates of any type.
Compared to conventional RFs, MuFs frequently demonstrate slightly lower predictive perfor-
mance, particularly in terms of calibration and for datasets with many outcome classes. Therefore,
for prediction, other methods besides MuFs, such as conventional RFs, should be considered in the
following situations: 1) when it is critical to attain the maximum possible predictive performance, 2)
when dealing with datasets with many outcome classes.
However, the primary purpose and distinctive feature of MuFs lie in their ability to specifically
identify class-associated covariates using the multi-class VIM. Given this specialized functionality,
the often slightly reduced predictive performance compared to conventional RFs does not represent a
particular limitation.
Acknowledgements
This work was supported by the German Science Foundation [DFG-Einzelf¨orderung HO 6422/1-3 to
Roman Hornung]. It is also related to the COAT project funded by the German Science Foundation
[DFG, Projektnummer (grant number): 447467169 to Alexander Hapfelmeier].
Supplementary material and R code
The supplementary material can be found at the following link:
https://www.ibe.med.uni-muenchen.de/organisation/mitarbeiter/070_drittmittel/hornung/
multiforests_suppfiles/suppmat_multiforests.pdf. The R code used to produce the results
showninthemainpaperandinthesupplementarymaterialisavailableonGitHub(https://github.
com/RomanHornung/MultiForests_code_and_data).
References
O. O. Akinola, A. E. Ezugwu, J. O. Agushaka, R. A. Zitar, and L. Abualigah. Multiclass feature
selection with metaheuristic optimization algorithms: a review. Neural Comput & Applic, 34(22):
19751–19790, 2022. doi: 10.1007/s00521-022-07705-4.
F. Berzal, J.-C. Cubero, N. Mar´ın, and D. S´anchez. Building multi-way decision trees with numerical
attributes. Inf Sci, 165(1–2):73–90, 2004. doi: 10.1016/j.ins.2003.09.018.
B. Bischl, G. Casalicchio, M. Feurer, P. Gijsbers, F. Hutter, M. Lang, R. G. Mantovani, J. N. van
Rijn, and J. Vanschoren. OpenML benchmarking suites. arXiv:1708.03731v2, 2021.
L. Breiman. Random forests. Mach Learn, 45:5–32, 2001. doi: 10.1023/A:1010933404324.
L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. Classification and Regression Trees.
Chapman and Hall/CRC, Boca Raton, Florida, 1984. doi: 10.1201/9781315139470.
G. W. Brier. Verification of forecasts expressed in terms of probability. Mon Weather Rev, 78(1):1–3,
1950. doi: 10.1175/1520-0493(1950)078⟨0001:VOFEIT⟩2.0.CO;2.
X.-w. Chen, X. Zeng, and D. Van Alphen. Multi-class feature selection for texture classification.
Pattern Recognit Lett, 27(14):1685–1691, 2006. doi: 10.1016/j.patrec.2006.03.013.
D. Coppersmith, S. J. Hong, and J. R. Hosking. Partitioning nominal attributes in decision trees.
Data Min Knowl Discov, 3:197–217, 1999. doi: 10.1023/A:1009869804967.
28T. Elomaa and J. Rousu. General and efficient multisplitting of numerical attributes. Mach Learn,
36:201–244, 1999. doi: 10.1023/A:1007674919412.
T. Fawcett. Using rule sets to maximize ROC performance. In Proceedings 2001 IEEE international
conference on data mining, pages 131–138. Ieee, 2001. doi: 10.1109/ICDM.2001.989510.
U. M. Fayyad and K. B. Irani. Multi-interval discretization of continuous-valued attributes for clas-
sification learning. In Y. W. Teh and M. Titterington, editors, Proceedings of the Thirteenth Inter-
national Joint Conference on Artificial Intelligence, pages 1022–1027, 1993.
G.Forman. Apitfallandsolutioninmulti-classfeatureselectionfortextclassification. InProceedings
of the twenty-first international conference on Machine learning, page 38, 2004. doi: 10.1145/
1015330.101535.
S. Garcia, J. Luengo, J. A. S´aez, V. Lopez, and F. Herrera. A survey of discretization techniques:
Taxonomy and empirical analysis in supervised learning. IEEE transactions on Knowledge and
Data Engineering, 25(4):734–750, 2013. doi: 10.1109/TKDE.2012.35.
T. Hastie, R. Tibshirani, and J. Friedman. The Elements of Statistical Learning: Data Mining,
Inference, and Prediction, volume 2. Springer, 2009. doi: 10.1007/978-0-387-84858-7.
G. Hooker, L. Mentch, and S. Zhou. Unrestricted permutation forces extrapolation: variable impor-
tance requires at least one more model, or there is no free variable importance. Stat Comput, 31:
82, 2021. doi: 10.1007/s11222-021-10057-z.
R. Hornung. Diversity forests: Using split sampling to enable innovative complex split procedures in
random forests. SN Comput Sci, 3:1–16, 2022. doi: 10.1007/s42979-021-00920-1.
R. Hornung and A.-L. Boulesteix. Interaction forests: Identifying and exploiting interpretable
quantitative and qualitative interaction effects. Comput Stat Data Anal, 171:107460, 2022. doi:
10.1016/j.csda.2022.107460.
S.Janitza,C.Strobl,andA.-L.Boulesteix. AnAUC-basedpermutationvariableimportancemeasure
for random forests. BMC Bioinformatics, 14:119, 2013. doi: 10.1186/1471-2105-14-119.
H. W. Kuhn. The Hungarian method for the assignment problem. Nav Res Logist Q, 2(1-2):83–97,
1955. doi: 10.1002/nav.3800020109.
X. Li, Y. Wang, S. Basu, K. Kumbier, and B. Yu. A debiased mdi feature importance measure for
random forests. Advances in Neural Information Processing Systems, 32, 2019.
M. Loecher. Unbiased variable importance for random forests. Commun Stat-Theor M, 51(5):1413–
1425, 2020. doi: 10.1080/03610926.2020.1764042.
J.D.Malley,J.Kruppa,A.Dasgupta,K.G.Malley,andA.Ziegler. Probabilitymachines: consistent
probability estimation using nonparametric learning machines. Methods Inf Med, 51(01):74–81,
2012. doi: 10.3414/ME00-01-0052.
C.Molnar,G.Casalicchio,andB.Bischl. Interpretablemachinelearning-abriefhistory,state-of-the-
art and challenges. In I. Koprinska et al., editors, ECML PKDD 2020 Workshops. ECML PKDD
2020. Communications in Computer and Information Science, volume 1323, pages 417–431, 2020.
doi: 10.1007/978-3-030-65965-3 28.
S. Nembrini, I. R. K¨onig, and M. N. Wright. The revival of the gini importance? Bioinformatics, 34
(21):3711–3718, 2018. doi: 10.1093/bioinformatics/bty373.
J. Pearl. Causal inference in statistics: An overview. Statist Surv, 3:96–146, 2009. doi: 10.1214/
09-SS057.
29P. Probst, A.-L. Boulesteix, and B. Bischl. Tunability: Importance of hyperparameters of machine
learning algorithms. J Mach Learn Res, 20(53):1–32, 2019.
J. D. Romano, T. T. Le, W. L. Cava, J. T. Gregg, D. J. Goldberg, P. Chakraborty, N. L.
Ray, D. Himmelstein, W. Fu, and J. H. Moore. Pmlb v1.0: an open-source dataset collec-
tion for benchmarking machine learning methods. Bioinformatics, 38(3):878–880, 2022. doi:
10.1093/bioinformatics/btab727.
Q.Song, H.Jiang, andJ.Liu. Featureselectionbasedonfdaandf-scoreformulti-classclassification.
Expert Syst Appl, 81:22–27, 2017. doi: 10.1016/j.eswa.2017.02.049.
C. Strobl, A.-L. Boulesteix, A. Zeileis, and T. Hothorn. Bias in random forest variable importance
measures: Illustrations, sources and a solution. BMC Bioinformatics, 8:25, 2007. doi: 10.1186/
1471-2105-8-25.
J. Vanschoren, J. N. van Rijn, B. Bischl, and L. Torgo. OpenML: Networked science in machine
learning. SIGKDD Explor, 15(2):49–60, 2013. doi: 10.1145/2641190.2641198.
D. H. Wolpert and W. G. Macready. No free lunch theorems for optimization. IEEE transactions on
evolutionary computation, 1(1):67–82, 1997. doi: 10.1109/4235.585893.
M. N. Wright and I. R. K¨onig. Splitting on categorical predictors in random forests. PeerJ, 7:e6339,
2019. doi: 10.7717/peerj.6339.
M. N. Wright and A. Ziegler. ranger: A fast implementation of random forests for high dimensional
data in C++ and R. J Stat Softw, 77:1–17, 2017. doi: 10.18637/jss.v077.i01.
E.YenandI.-W.M.Chu. Relaxinginstanceboundariesforthesearchofsplittingpointsofnumerical
attributes in classification trees. Inf Sci, 177(5):1276–1289, 2007. doi: 10.1016/j.ins.2006.08.014.
M.YuanandY.Lin. Modelselectionandestimationinregressionwithgroupedvariables. J R Statist
Soc B, 68(1):49–67, 2006. doi: 10.1111/j.1467-9868.2005.00532.x.
Z. Zhou and G. Hooker. Unbiased measurement of feature importance in tree-based methods. ACM
Trans Knowl Discov Data, 15(2):1–21, 2021. doi: 10.1145/3429445.
L.Zini,N.Noceti,G.Fusco,andF.Odone.Structuredmulti-classfeatureselectionwithanapplication
to face recognition. Pattern Recognit Lett, 55:35–41, 2015. doi: 10.1016/j.patrec.2014.07.004.
30