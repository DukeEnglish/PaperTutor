Reinforcement Learning Discovers Efficient Decentralized
Graph Path Search Strategies
AlexeiPisacane∗1,Victor-AlexandruDarvariu2,MircoMusolesi1,3
1UniversityCollegeLondon 2UniversityofOxford 3UniversityofBologna
Abstract
Graphpathsearchisaclassiccomputerscienceproblemthathasbeenrecently
approachedwithReinforcementLearning(RL)duetoitspotentialtooutperform
priormethods. ExistingRLtechniquestypicallyassumeaglobalviewofthe
network,whichisnotsuitableforlarge-scale,dynamic,andprivacy-sensitive
settings. An area of particular interest is search in social networks due to its
numerous applications. Inspired by seminal work in experimental sociology,
whichshowedthatdecentralizedyetefficientsearchispossibleinsocialnetworks,
weframetheproblemasacollaborativetaskbetweenmultipleagentsequipped
withalimitedlocalviewofthenetwork. Weproposeamulti-agentapproach
forgraphpathsearchthatsuccessfullyleveragesbothhomophilyandstructural
heterogeneity. Ourexperiments,carriedoutoversyntheticandreal-worldsocial
networks, demonstrate that our model significantly outperforms learned and
heuristicbaselines. Furthermore,ourresultsshowthatmeaningfulembeddings
forgraphnavigationcanbeconstructedusingreward-drivenlearning.
1 Introduction
GraphpathsearchisafundamentaltaskinComputerScience,pivotalinvariousdomainssuchas
knowledgebases[1], robotics[2], andsocialnetworks[3]. Givenastartnodeandendnode, the
goalistofindapathfromasourcetoadestinationinthegraphthatconnectsthemandoptimizes
desideratasuchasminimizingpathlength. Werefertosearchstrategiesthatachievethisasefficient.
Theproblemisgenerallyframedfromacentralizedperspectivewithaglobalviewofthenetwork,
which is impractical or infeasible for several applications. In peer-to-peer networks [4], where
privacyisaprimaryconcern,acentralizedagentposessignificantrisks[5–7]. Largegraphsmayalso
inducescalabilitybottlenecksasthestoragerequirementsofacentralizeddirectorystrainmemory
limitations[8]. Moreover,indynamicnetworks,maintainingaconsistentglobalviewofthetopology
maybeimpossible[9].Graphpathsearchisofparticularinterestinsocialnetworksgiventheinherent
commercialapplicationsandpotentialfornewinsightsfromasocialsciencesperspective[10,11].
Inthispaper,wewillstudytheproblemofdecentralizedpathgraphsearchusinglocalinformation.
Wewillconsidersocialnetworksandwewilldiscusshowtheproposedmethodcanbedirectlyapplied
to any networks for which topological and node attribute information is available. Indeed, prior
experimentsinhumansocialnetworks,suchasStanleyMilgram’srenowned“smallworld"experiment
[12]2revealstheexistenceofshortpathsinsocialnetworksthatarediscoverablesolelythroughlocal
graphtopologyandhigh-levelnodeattributes,e.g.,characteristicsoftheindividuals,suchastheir
∗Correspondencetopisacane.alexei@gmail.com.
2ThegoalofMilgram’sexperimentwastoinvestigatethedegreeofconnectednessamongpeopleinthe
UnitedStates,leadingtotheconceptof“sixdegreesofseparation”,theideathatanytwopeopleonEarth(i.e.,
thenodesintheglobalsocialnetwork)areconnectedbyachainofnomorethansixacquaintances,whichhas
alsoenteredpopularculture[13].MilgramselectedparticipantsfromNebraskaandKansas.Eachparticipant
wasgivenaletterandinstructedtosendittoatargetperson,astockbrokerlivinginBoston. However,they
couldonlysendthelettertosomeonetheyknewpersonally,characterizedbycertain(node)attributeswhothey
thoughtmightbeclosertothetarget.Eachrecipientoftheletterwouldthenforwardittosomeonetheyknew
personally,continuingthisprocessuntiltheletterreachedthetarget.Onaverage,ittookaboutsixstepsforthe
letterstoreachthestockbrokerinBoston[14].
Preprint.Preliminarywork.
4202
peS
21
]GL.sc[
1v23970.9042:viXraReinforcementLearningDiscoversEfficientDecentralizedGraphPathSearchStrategies
occupation,theirhigh-school,andsoon. Manysocialnetworksexhibittwokeypropertiesthatmake
decentralizedsearchwithpartialinformationpossibleandefficient. Thefirst,homophily[15],reflects
thetendencyofindividualstoconnectwithotherswhosharesimilarattributes. Thesecondisthe
heterogeneityoflocalstructureinmanynetworks,inwhichnodesareoftenorganizedintohighly
connectedcommunities[16],withasmallernumberofweakties[17,18]orcentralconnectors[19]
bridgingthesenodeclustersandactingasshortcuts.
ReinforcementLearning(RL)hasrecentlybeenemployedwithacentralizedperspectivefordiscover-
inglearnedheuristicsforgraphsearch[20,21]andreasoningoverpathsinknowledgegraphs[22,23],
inawaythatcomplementsoroutperformsclassicalgorithms. MotivatedbythepromiseofRLand
thegoaltoattaindecentralizedgraphpathsearch,inthispaper,weproposeamulti-agentRLformu-
lationintheDecentralizedPartiallyObservableMarkovDecisionProcess(Dec-POMDP)framework.
Agentshaveonlylocalvisibilityofgraphtopologyandneighborattributes,andcooperatetowards
finding paths to the target node. We propose a method for learning in this Dec-POMDP that, in
accordancewiththeCentralizedTrainingandDecentralizedExecution(CTDE)paradigm[24],trains
anactor-criticmodelwithnoderepresentationslearnedbyaGraphAttentionNetwork[25]with
sharedparameters. Theseembeddingsarecomputedviaamessage-passingprocedurestartingfrom
the raw node attributes and the graph topology. For this reason we name the resulting method
GARDEN:GraphAttention-guidedRoutingforDEcentralisedNetworks. Atexecutiontime,the
policyisusedinadecentralizedfashionbyallagents.
Weconductexperimentsonsyntheticandreal-worldsocialnetworkgraphswithupto600nodesto
evaluateourmodeldesign. Ourfindingshighlightthesuperiorabilityofourmethodtoutilizeboth
homophilyandlocalconnectivitybetterwhencomparedtolearnedandhandcraftedbaselinemodels.
Moreover,wefindthatthelearnedembeddingsaremeaningfulrepresentationsfornavigationinhigh-
dimensionalfeaturespace. OurresultsshowthatthedynamicsobservedinMilgram’sexperimentcan
emergeusingreward-drivenlearning. RLisabletoconstructalatentfeaturespacethateffectively
incorporates both node attribute information and network structure for use in graph path search.
Therefore, this work supports the notion that decentralized graph path search can succeed given
appropriaterepresentations,andshowsapossiblemechanismforhowrepresentationssimilartothose
inherentlyusedbyindividualsmaybeconstructed.
2 RelatedWork
2.1 NetworkScience
Searchisacommonoperationinnetworkapplications. Variousclassicalgorithms[26]ensurepath
discoverybetweentwonodesunderspecificconditions. Theyrequiremaintainingglobalknowledge
ofthegraphstructure,which,aswehaveargued,isimpracticalincertaincasesduetoconsiderations
ofprivacy,scalability,anddynamicity. Wethereforefocusourattentionongraphpathsearchusing
onlylocalinformation.
As previously discussed, our inspiration for studying this problem is Milgram’s “small world"
experiment[12]. Thefindings,latervalidatedonalargerscale[27],supportthishypothesisinsocial
networks,whicharecharacterizedbyshortmeanpathlengths. Subsequentresearch[28]highlighted
thediscoveryofeffectiveroutingstrategies,emphasisingtheconceptofhomophily[15],whichstates
thatindividualsseekconnectionstoothersthataresimilartothemselves.
Inadditiontohomophily,manynetworksarecharacterizedbyapower-lawdegreedistribution[29]
and exhibit heterogeneity in node degree. In such networks, a few “hub" nodes with numerous
connections coexist with many nodes having a relatively small degree. Highly connected nodes
thereforeofferpotentialshortcutsinsearchtrajectoriesbybridgingsparselyconnectedcommunities.
Foreffectivesearch,findingthebridgingnodebetweentwocommunitiesisoftenrequired. Relying
solely on homophily or node degree may be ineffective, as the bridging node might lack a large
degreeorsignificantattributesimilaritywiththetargetnode. Innetworkswithlargeclusters,anagent
mayspendconsiderabletimenavigatingthecurrentclusterbeforereachingthedesiredcommunity.
Identifyingweakties[17]betweencommunitiesischallengingusingonlynodeattributesordegrees;
therefore,aneffectivesearchforweaktiesrequiresawarenessofcandidatenodes’neighborhoods.
A useful lens for viewing this problem is through a “hidden metric space" [30] of node features.
Assuming node features are representative of their position within this space, the probability of
2ReinforcementLearningDiscoversEfficientDecentralizedGraphPathSearchStrategies
edge-sharingincreaseswithdecreasingpairwiseattributedistance. Empiricalevidencesupportsthe
efficiencyofnavigationusingthisunderlyingmetric. Ifanapproximationtothehiddenmetricusing
onlylocalgraphstructureisfeasible,adecentralizedstrategycouldinvolvemovingtowardnodes
minimizingtheapproximatemetric[30,31].
TheapproachofSimsekandJensen[32]ismostcloselyrelatedtooursasitalsotreatstheproblem
of search by leveraging both homophily and the node degree disparity. The algorithm uses an
estimateofthestatisticalrelationshipbetweentheattributesimilarityandconnectionprobability
whosecomputationrequiresknowledgeoftheattributesofallthenodesinthenetwork. Incontrast,
ourmethoddoesnotrequiretheavailabilityofthisglobalinformation.
2.2 ReinforcementLearningforGraphRoutingandSearch
ReinforcementLearningmethodshavebeenappliedforavarietyofgraphoptimizationproblemsin
recentyearsasamechanismfordiscovering,bytrial-and-error,algorithmsthatcanoutperformclassic
heuristicsandmetaheuristics[33]. TheirappealstemsfromtheflexibilityoftheRLframeworkto
addressawidevarietyofproblems,requiringmerelythattheycanbeformulatedasMDPs. Themost
relevantworksinthisareatreatroutingandsearchproblemsovergraphs.
OneofthefirstinstancesofapplyingRLtosuchscenariosistheadaptiveQ-routingalgorithm[34].
TheauthorsformulatedtheMDPwithstatesasatupleofthecurrentnodeanddestinationnode. They
usedatabularQ-learningalgorithm[35]whereagentsateachnodeonagraphmaintainalookup
tableofroutingtimeestimatesforneighbor-destinationpairs. Thismethodallowsforadaptation
innetworkswithchangingtopologyusingdynamicupdatesanddoesnotrequireacentralrouting
controller. Severalextensionsandvariationsonthisideahavebeenexplored[36,37],butallsuffer
fromthemainpitfalloftabularRLmethods: poorscalability.
Interest in learning to route has been reignited recently by several works that employ function
approximationforbetterscalability. Notably,GraphNeuralNetworks(GNNs)[38]haveemerged
as a suitable learning representation for allowing RL policies to generalize to similar yet unseen
conditions and graph topologies. In this line of work, Valadarsky et al. [39] considered learning
routingpoliciesonadatasetofhistoricaltracesofpairwisedemandsandapplyingtheminnewtraffic
conditions. TheMDPisframedaslearningasetofedgeweightsfromwhichtheroutingstrategyis
determined. MorerecentworkbyAlmasanetal.[40]leveragedaGNNrepresentationtrainedusinga
policygradientalgorithm. Theyframeactionsasthechoiceofamiddle-pointforagivenflowgiven
startandtargetnodes,withpreviousactionchoicesbecomingpartofthestate.
Anotherimportantlineofworkstudieshowtoperformsearchongraphs. Incontrasttorouting,for
searchtasksthereisnonotionofalinkloadassociatedwithtraversingaparticularnodeoredgein
thegraph. NotablecontributionsinthisdirectionincludeworkbyPándyetal.[21],whereRLagents
aretaskedwithlearningaheuristicfunctionforaugmentationofA*search.
Theproblemofknowledgegraphcompletionmayalsobeviewedasgraphtraversalininstanceswith
heterogeneousedgetypes[41]andwithatargetnodethatisnotspecifiedapriori.Dasetal.[22]
proposedanMDPformulationofthistask,inwhichanagentchoosesthenextrelationshiptotraverse
giventhecurrentnode. Aproportionofthetruerelationshipsintheknowledgegraphismaskedand
usedtoprovidetherewardsignalfortrainingtheagentviaREINFORCE.TheM-Walkmethod[23]
buildsfurtherinthisdirectionbyleveragingthedeterminismofthetransitiondynamics. Therefore,
trainingwithtrajectoriesfromaMonteCarloTreeSearch(MCTS)policy[42]canovercomethe
rewardsparsityassociatedwiththerandomexplorationofmodel-freemethods.
Lastly,wenotethat,whiletheworksreviewedinthissectionsharefeaturesofourMDPandmodel
design,nonearedirectlyapplicabletotheproblemformulation. Chiefly,weconsideradecentralized
graphpathsearchscenarioinwhicheachagenthasonlypartialvisibilityofthenetwork.
3 Methods
Inthissection,wefirstintroduceourdecentralizedmathematicalformulationofthegraphsearch
problem. Next, we describe the proposed multi-agent reinforcement learning algorithm, which
leverageslearnablegraphembeddings.
3ReinforcementLearningDiscoversEfficientDecentralizedGraphPathSearchStrategies
3.1 MDPFormulation
3.1.1 State&ActionSpace
We frame the search problem as a Decentralized Partially Observable Markov Decision Process
(Dec-POMDP) [43] taking place over an attributed, undirected, and unweighted graph structure
G=(V,E)withnnodesandmedges. Anagentisplacedoneachnodeu ∈V inthegraph,while
i
theedgesE indicatedirectbidirectionalcommunicationlinksbetweenagents. Anattributevector
x ∈Rdisassociatedwitheachagent. Theaimistofindapathstartingfromaninitialnodeu to
ui src
adesignatedtargetnodeu .
tgt
AteachtimesteptofanMDPepisode,anodeu receivesamessagem∈Rdspecifyingtheattributes
i
x ofthetargetnode(butnotitsidentity). Itchoosesasanactiononeofitsneighbours,denoted
utgt
N(u ),topassthemessageonto. Allotheragentstakenoactionatthisstep. Wedenotethepresence
i
orabsenceofamessageatagivennodeu attimetwiththebinaryindicatorM(i).
i t
ToensureMDPstationarity,thestateofthereceivernodeateachstepisconditionedonthecurrent
episode’stargetnodewithvariableu . Weemphasizethatthisisatechnicaldetailrequiredtodefine
tgt
theMDP,andtheagentdoesnothavevisibilityofthelocationofthetargetnodebeyondtheattributes
giveninthemessage. Thestatesandactionsarethereforedefinedas:
S ={S(i)|u ∈V} (1)
t t i
S(i) =(M(i),u ) (2)
t t tgt
A ={A(i)|u ∈V} (3)
t t i
A(i)|(M(i) =1)=N(u ) (4)
t t i
A(i)|(M(i) =0)=∅. (5)
t t
3.1.2 ObservationSpace
Inaccordancewithourmotivations,weprovideagentswithonlylocalobservationsofthegraph
topology. Concretely, we equip each agent u with observations of 1-hop ego subgraphs G
i ui
centeredonitsneighboringnodes,includingvisibilityofpairwiseedgesbetween1-hopneighbors.
Symmetrically,thetransmittedmessagecomprisesanegographcenteredonthetargetnode. More
formally,theobservationprovidedtoagentiattimetisdefinedas:
O(i)|(M(i) =1)=({G |u∈N(u )},G ) (6)
t t ui i utgt
O(i)|(M(i) =0)={G |u∈N(u )}, (7)
t t ui i
where
G =(V ,E ) (8)
ui ui ui
V =N(u )∪{u } (9)
ui i i
E ={(u ,u )∈E|u ∈V ∧u ∈V }. (10)
ui k l k ui l ui
Asitispossibletonotefromthedefinitionabove,theobservationwillbedifferentdependingonthe
absenceorpresenceofthemessageonthenodeitself.
3.1.3 TransitionsandRewards
Ifanactionispermitted,themessagemovesdeterministicallytotheselectednode,updatingstates
accordingly:
(cid:40)
M(i) =
1ifM t(j) =1,A( tj) =u i,
(11)
t+1 0otherwise.
Theepisodeendswhenthemessagereachesthetargetnode,yieldingacollectiverewardof+1forthe
agentsandterminatingtheepisode. Additionally,topreventagentsfrombecomingstuckinaction
cycles,weintroduceatruncationcriterion: afterT interactionstepswiththeenvironment,the
max
episodeendswithoutrewardingtheagents. Moreformally:
(cid:40)
R(i) =
1ifM t(i) =1,A t(i) =u tgt,
(12)
t+1 0otherwise.
4ReinforcementLearningDiscoversEfficientDecentralizedGraphPathSearchStrategies
3.2 ModelDescription
Inourdesign,weemploythecommonmulti-agentReinforcementLearningparadigmofCentralized
TrainingwithDecentralizedExecution(CTDE)[44]. Weconsiderafullycollaborativesettingin
which the agents are all rewarded if messages are successfully delivered to the target node. The
collaborativeobjectiveisformulatedsuchthateachagentselectingtheoptimalnextactionresultsin
anoptimaltrajectorythroughthegraph. Therefore,theoptimaltrajectorycanbeconstructedina
decentralizedmanner.
AsitiscommonintheCDTEparadigm,weutilizeparameter-sharingacrossagentnetworks. Inthe
trainingscheme,acentralizedagentreceiveslocalizedobservationsfromindividualagentsateach
step,andistaskedwithselectingoptimalactionsinthesearchpath. Theoptimaldecisionisfirst
learnt,andthenreplicatedanddistributedtoindividualnodesatexecutiontime.
At each training step, a central agent is given incomplete observations and receives sparse and
delayedrewardsfromtheenvironment. Giventhesespecifications,weproposetheuseofavariantof
theAdvantageActor-Critic(A2C)algorithm[45]topromoteadequateexplorationwithacceptable
sampleefficiency.TheA2Cvaluenetworkisalsolearnedinacentralizedfashiontoguidethetraining
ofthepolicynetwork. Learningastochasticpolicy(ratherthanadeterministicone)isimportant
fortheproblemunderconsiderationgiventhatashortpathtothetargetmaynotbeavailablevia
a particular neighbor despite a high level of attribute similarity. Lastly, we incorporate entropy
regularization to ensure the policy maintains a high degree of randomness while still aiming to
maximizetheexpecteddiscountedreturn.
3.2.1 PolicyDesign
Weformulatethechoiceofneighbortowhichthemessageshouldbetransmittedbasedonvalues
outputbyanMLP-parameterizedpolicynetworkf . Thepolicynetworkisappliedforeachneighbor
π
u ∈N(u )ofthenodeu thatiscurrentlyinpossessionofthemessageattimet,andtheSoftMax
j i i
functionisusedtoderiveaprobabilitydistribution. Concretely,thepolicyisdefinedas:
exp(f ([x ||x ]))
π(u |u )=
π uj utgt
, (13)
j i (cid:80) exp(f ([x ||x ]))
uk∈N(ui) π uk utgt
where[·||·]denotesconcatenation. Similarly,toestimatethevaluefunction,weusethecurrentnode
u andthetargetnodeu attributesthatarepassedthroughanMLP-parameterizedvaluenetworkf :
i tgt v
v(u ,u )=f ([x ||x ]). (14)
i tgt v ui utgt
Itisinterestingtonotethatthenodefeaturesthatareusedasinputtothepolicyandvaluenetworks
will impact the effectiveness of the learned policies. The simplest choice is to use the raw node
features x , andwe denote theresultingalgorithm as MLPA2C.Wealsoconsider thesimplest
ui
extension to this model that minimally incorporates local graph topology by augmenting node
attributeswithnodedegrees,i.e.,xWD =[x ||deg(u )]. WerefertothisasMLPA2CWD.
ui ui i
3.2.2 GARDEN
Recallthe“hiddenmetric"hypothesisdiscussedinSection2.1,whichpositsthataviablepolicycan
bemotivatedbymovingthroughthegraphtoreducenodedistance,providedagoodapproximation
oftheunderlyingmetricisobtained. Insteadofprescribingthattherawnodeattributesshouldbe
usedtoapproximatethismetric,weproposethatrelevantnodefeatures,whichcapturethepotentially
complexinterplaybetweenattributesandtopologies,canbelearned. Todoso,wesuggestreplacing
rawnodeattributeswithlearnedembeddingsxGATobtainedfromaGraphAttentionNetwork(GAT)
ui
[25]. Theseembeddingsarecomputedviaamessage-passingprocedurestartingfromtherawnode
attributesandthegraphtopology.
The method, which we refer to as Graph Attention-guided Routing for Decentralized Networks
(GARDEN),isshownusingpseudocodeinAlgorithm1. Theparametersoff aretrainedimplicitly
rep
as we take gradient descent steps over the combined episodic loss (cid:80) L(π) + L(v). The node
t t t
embeddingsarerecalculatedatthestartofeachepisode.Thenotation[[·]]denotesthepartialstopping
(cid:80)
ofgradients,andH(p)denotestheentropyofadiscretedistribution,givenby p log(p ).
i i i
5ReinforcementLearningDiscoversEfficientDecentralizedGraphPathSearchStrategies
Algorithm1GraphAttention-guidedRoutingforDEcentralizedNetworks(GARDEN).
1: Input: PolicyNetworkf π,ValueNetworkf v,GraphRepresentationNetworkf rep,EgoGraphs
{G |u∈V},entropyregularizationcoefficientλ,discountfactorγ.
u
2: Output: Learnedpolicy,valueandrepresentationnetworksf π,f v,f rep.
3: fori=1toN do
episodes
4: InitializeepisodebufferB
5: Samplestartingnodeu,targetnodeu
tgt
6: m=x utgt
7: forw ∈V do
8: ComputexGATusingf
w rep
9: endfor
10: t=0
11: while u̸=u andt<T do
tgt max
12: Sampleactionu′ ∼π(u;m)
13: Movemessagetonodeu′,observerewardr
14: Storetransition(u,u′,r)inB
15: u←u′
16: t=t+1
17: endwhile
18: InitializeepisodelossL=0
19: for(u,u′,r)inBdo
20: Aˆ=r+[[f v([xG u′AT||m])]]−f v([xG uAT||m])
21: L(π) =−Aˆlogf π([xG u′AT||m])−λH(π(u;m))
22: L(v) =Aˆ2
23: L=L+L(π)+L(v)
24: endfor
25: TakegradientdescentsteponLw.r.t. parametersoff v,f π,f
rep
26: endfor
4 ExperimentalSetup
4.1 Datasets
Real-worldGraphs. Toassesstheperformanceofdecentralizedgraphstrategiesonreal-worlddata,
weconsiderseveralegographsfromtheFacebooksocialnetwork[46]presentintheSNAP[47]
repository. ThesegraphsdepictindividualsandtheirFacebookfriendships. Eachnodeisequipped
with binary, anonymized attributes collected through surveys. Due to computational budget con-
straints,weselectthelargestconnectedcomponentsof5graphssuchthattheyhavebetween100and
600nodes. High-leveldescriptivestatisticsforthesegraphsarepresentedinTable4intheAppendix.
SyntheticGraphs. Weadditionallyconsidersyntheticallygeneratedgraphsthatarebothattributed
anddisplayhomophily. Thisallowsforthecreationofadiverserangeofgraphswithvaryingdegrees
of sparsity, enabling evaluations under different synthetic conditions. We follow the generative
graphconstructionprocedureproposedbyKaiserandHilgetag[48],whichsamplesnodeattributes
uniformlyfromaunitbox[0,1]dandcreatesedgesstochasticallyaccordingtotherulep((u,u′)∈
E)=max(1,βe−α∥xu−x u′∥2),whereαandβ arescalingcoefficients.
4.2 Baselines
The learned baselines we use are the MLPA2C and MLPA2CWD techniques as introduced in
Section3.2.1. Furthermore, weconsiderasuiteofheuristicbaselinesthatutilizehomophilyand
graphstructureforgraphpathsearch. Thesimplestbaseline,GreedyWalker,selectsthenextnode
greedilybasedonthesmallestEuclideanattributedistance: π(u)=argmin ∥x −x ∥ .
u′∈N(u) u′ utgt 2
Given that deterministic policies may result in action loops, we generalize this to a stochas-
tic agent (DistanceWalker) that acts via a SoftMax policy over attribute distances with a tuned
temperature parameter: π(τ)(u′|u) = (cid:80) u′′∈e Nxp (( u− )e∥ xx pu (−′− ∥x xu u∥ ′′2 −/ xτ u) ∥2/τ). Similarly, we consider the
stochasticConnectionWalkeragent,whichusesaSoftMaxpolicyovernodedegree: π(τ)(u′|u)=
6ReinforcementLearningDiscoversEfficientDecentralizedGraphPathSearchStrategies
exp(deg(u′)/τ)
.Lastly,theRandomWalkerbaselineselectsuniformlyatrandombetween
(cid:80) u′′∈N(u)exp(deg(u′′)/τ)
nodesfromthecurrentneighbourhood: π(u′|u)= 1 .
deg(u)
The stochastic agents use a temperature parameter τ to control greediness. To perform a fair
comparisonwithourlearnedmodels,weindividuallytunethetemperaturefortheDistanceWalker
andConnectionWalkermodelsusingavalidationsetforeachgraph.
4.3 ModelEvaluation&Selection
Forevaluatingmodels,weconsiderthefollowingmetrics:
1. MeanOracleRatioR¯ :theratiobetweenepisodelengthandtheshortestpathlengthaveraged
oracle
overallsource-destinationpairs;
2. TruncationRateR : %ofepisodesexceedingthetruncationlengthT ;
trunc max
3. WinRateR : %ofepisodeswhereagivenagentobtainstherelativeshortestpathlength,with
win
tiesbrokenrandomly.
Tomitigatepotentialmemorizationofroutesduringtraining,especiallywhennodesareuniquely
identifiablebasedonattributes,wepartitionthenodesetV intothreedisjointgroups: Vtrain,Vval,
andVtestatratiosof80%/10%/10%. Thesourcenodeu issampleduniformlyatrandomfromV,
src
whilethetargetnodeu dependsonwhethertraining,validation,ortestingisperformed.Fortraining,
tgt
wealwayssamplea“fresh”source-targetpair,whileforvalidationandevaluationthesource-target
pairsareserializedandstored(suchthattheperformanceevaluatedoverthemisconsistent). The
MeanOracleRatioR¯ isusedastheprimarymetricformodelvalidationandevaluation.
oracle
4.4 SensitivityAnalysisofGraphDensityParameter
Givenaconstantgraphsize,reducedgraphdensitydiminishesavailablepathstoatarget[49]. This
intensifies exploration challenges and heightens the risk of truncated episodes, yielding sparser
reward signals in training. Motivated by this rationale, we assess GARDEN across a set of
generated graphs with diverse sparsity levels. We randomly generate 10 graph topologies for
β ∈ {0.01,0.05,0.1,0.2,0.3,0.4,0.5,0.75,1.0}withnumberofnodesn = 200andα = 30. We
trainGARDENseparatelyforeachvalueofβ andgaugeitsperformanceagainstthebaselines.
4.5 AblationofNodeRepresentation
WeassessourGNN-basedmodelagainstalternativedesignsthroughanablationstudyonsynthetic
graphs. Usingfiverandomseedsandfixedgraphparameters(n=200,α=30,β =5),weconduct
experimentsonourthreemodeldesigns:theMLPA2Cmodelusingonlytherawnodeattributesx,the
MLPA2CvariantincorporatingbothnodeattributesanddegreesxWD,andtheproposedGNN-based
GARDENmethod,whichemployslearnedgraphembeddingsxGAT.
5 ExperimentalResults
5.1 FacebookGraphs
As shown in Table 1, we find that GARDEN significantly outperforms baselines across all the
real-worlddatasetsandmetricswehavetestedon. Giventhevarietyofattributedimensionsand
densities,asdisplayedinTable4intheAppendix,wemayarguethatingraphswithhighamountsof
latentstructure,ourmodelisrobusttothesefactors.
InFigure3intheAppendix,wevisualizethevaluefunctionlearnedbyGARDENonthesesocial
networkegographs. ThishighlightsthatthevaluesobtainedbyGARDENserveasareliableproxy
forgraphdistance,assigninghighestvaluestonodesinthetarget’sclusterorclusterswithstrong
connectivity to the target’s community. Furthermore, it demonstrates the interpretability of the
proposedtechniqueforgraphpathsearch.
5.2 SensitivityAnalysisofGraphDensityandTemperatureParameters
7ReinforcementLearningDiscoversEfficientDecentralizedGraphPathSearchStrategies
Table1: Metricsobtainedbythemethodsonthe5socialnetworkegographs. GARDENconsistently
yieldsthebestperformance,followedbytheDistanceWalkermethod.
Metric GreedyWalker DistanceWalker ConnectionWalker RandomWalker GARDEN(Ours)
R¯ oracle(↓) 27.17±1.09 15.98±0.79 33.39±1.15 33.17±1.16 10.95±0.76
31.78±1.22 13.01±0.90 34.62±1.17 34.27±1.18 9.89±0.78
27.35±1.19 9.99±0.78 38.84±1.42 38.88±1.43 8.74±0.76
25.69±0.88 14.33±0.65 28.25±1.00 28.01±1.01 12.08±0.71
28.29±0.66 22.71±0.70 28.11±0.69 28.22±0.67 16.97±0.74
R trunc(↓) 79.00 39.70 76.90 76.70 14.90
78.00 19.50 68.50 64.10 12.60
70.00 13.90 69.80 67.20 13.50
88.00 44.00 81.50 79.40 40.10
96.00 72.70 89.50 90.00 48.20
R win(↑) 7.90 26.20 2.70 2.50 60.70
10.90 29.90 4.30 3.90 51.00
11.90 33.80 1.50 2.00 50.80
6.10 32.10 5.30 8.20 48.30
2.20 24.90 7.60 10.50 54.80
In Figure 1, we show the validation
performanceasafunctionoftheSoft-
Max temperature τ of the stochas-
tic DistanceWalker and Connection-
Walkerbaselines. Forbothmethods,
a middle-ground temperature value
yields shorter path lengths. Further-
more,performanceismoresensitive
toτ fortheDistanceWalkermethod.
The sensitivity analysis for the syn-
thetic graph density parameter β is
Figure 1: Mean Oracle Ratio obtained by the stochastic
showninFigure2. GARDENconsis-
baselinesonthevalidationdatasetasafunctionofthetem-
tentlymatchesorsurpassesbaseline
peratureτ forvaryingvaluesofβ.
performanceforallβ valuesforboth
MeanOracleRatioandWinRatemetrics. However, DistanceWalkeroutperformsourmodelfor
higherβ inTruncationRate. Inthissetting,DistanceWalkerbenefitsfromknowledgeofthe“true”
nodeattributesdetermininglinkgenerationandhighβ valuesleadingtomostconnectionsbeing
realized. Thisisincontrastwiththegaponreal-worlddatasets,forwhichthismetricisnotavailable:
indeed,GARDENmaybeseenasrecoveringanunderlying“hiddenmetric”.
5.3 AblationofNodeRepresentation
AsshowninTable2,GARDENsignificantlyoutper- Table 2: Ablation results obtained by pair-
formstheMLPA2CandMLPA2CWDdesignsthat ing different node representations with the
onlyuserawnodeattributesandMLP-parameterized proposedproblemformulationandReinforce-
policiesinbothR andR¯ .ThestandardMLP- mentLearningalgorithm.
trunc oracle
parameterized model achieves the best Win Rate
R win, and the differences with respect to this met- Agent R¯ oracle(↓) R trunc(↓) R win(↑)
ric are less conclusive. This can be explained by
GARDEN 1.95±0.09 1.68 34.44
thearbitrarybreakingoftiesperformedwhenpath MLPA2CWD 2.23±0.13 2.94 30.78
lengthsmatchforallmethods,coupledwiththehigh MLPA2C 2.31±0.12 4.32 34.78
graph density parameter β = 5 leading to shorter
pathlengths.
6 ConclusionsandFutureWork
Inthispaper,wehaveconsideredtheproblemofdecentralizedsearchingraphs,whichismotivated
byprivacy,scalability,anddynamicityrequirementsofmanynetworkmodelingscenarios. Despite
the lack of a central view of the network, the homophily and community structure observed in
8ReinforcementLearningDiscoversEfficientDecentralizedGraphPathSearchStrategies
Figure 2: Metric values obtained by the methods as a function of the synthetic graph density
parameterβ. GARDENgenerallyperformsbest,butitisnotablysurpassedbyDistanceWalkerinthe
truncationrateforhighvaluesofβ.
manynetworkscanallowfordecentralizedagentstofindshortpathstoagiventarget,asfamously
demonstratedbytheMilgramexperiment[12].
WehaveproposedtheGARDENmethodtoaddressthisproblem,whichtrainsagentsinacentralized
fashionandallowsfordecentralizedpolicyexecution. Ourapproachisbasedonmessagerouting
policiesthatarelearnedusingReinforcementLearning,pairedwithnodefeatureslearnedbyaGraph
NeuralNetworkspecificallyforthetask. Ourresultsshowthatourmethodcanoutperformstochastic
routingpoliciesbasedonattributeorstructuralinformationalone. Itispossibletoobservesignificant
improvementswhensearchingonreal-worldsocialnetworkgraphswithnon-triviallatentstructures
andhigh-dimensionalnodeattributes.
Forsimplicity,wehaveconsideredamemory-lesssearchprocedurethatisakintoabiasedrandom
walk. Thismeansthattheagentscannotreacttotheunsuccessfulexplorationofagivenregionofthe
graphbeforearrivingatapreviouslyvisitednode,andthesamedistributionoveractionswillapply
independentlyofthehistoricaltrajectory. Theproblemformulationcanbeextendedbyincluding
thehistoryofvisitednodesinthemessagemandforbiddingusingalready-visitednodesasactions.
RNNs may be used to encode the history as input to the learned models, as performed in other
learning-basedgraphsearchworks[21,23].
Webelievethatourresultsprovideevidenceforasortof“hiddenmetric”hypothesis,showinghowa
latentfeaturespaceamenableforgraphnavigationcanberecoveredbyreward-drivenlearning. An
interestingaspectthatcanbeconsideredbyfutureworkistocomparetheseemergentrepresentations
withthemeansinwhichindividualstakedecisionsforroutingmessagesinexperimentsconducted
overrealsocialnetworks.
9ReinforcementLearningDiscoversEfficientDecentralizedGraphPathSearchStrategies
References
[1] Fabian M. Suchanek, Gjergji Kasneci, and Gerhard Weikum. Yago: a core of semantic
knowledge. InWWW’07,2007. 1
[2] StevenM.LaValle. PlanningAlgorithms. CambridgeUniversityPress,2006. 1
[3] DuncanJ.Watts,PeterSheridanDodds,andM.E.J.Newman. Identityandsearchinsocial
networks. Science,296(5571),2002. 1
[4] AndrewOram. Peer-to-Peer: HarnessingtheBenefitsofaDisruptiveTechnology. O’Reilly
MediaInc,2001. 1
[5] Réka Albert, Hawoong Jeong, and Albert-László Barabási. Error and attack tolerance of
complexnetworks. Nature,406(6794):378–382,2000. 1
[6] JamesAspnes,ZoëDiamadi,andGauriShah. Fault-tolerantroutinginpeer-to-peersystems. In
PODC’02,pages223–232,2002.
[7] Duncan S Callaway, Mark EJ Newman, Steven H Strogatz, and Duncan J Watts. Network
robustnessandfragility: Percolationonrandomgraphs. PhysicalReviewLetters,85(25):5468,
2000. 1
[8] YatinChawathe,SylviaRatnasamy,LeeBreslau,NickLanham,andScottShenker. Making
Gnutella-likeP2Psystemsscalable. InSIGCOMM’03,pages407–418,2003. 1
[9] Silvia Giordano. Mobile ad hoc networks. Handbook of Wireless Networks and Mobile
Computing,pages325–346,2002. 1
[10] DavidEasleyandJonKleinberg. Networks,Crowds,andMarkets: ReasoningaboutaHighly
ConnectedWorld. CambridgeUniversityPress,2010. 1
[11] StanleyWassermanandKatherineFaust. SocialNetworkAnalysis: MethodsandApplications.
CambridgeUniversityPress,1994. 1
[12] Jeffrey Travers and Stanley Milgram. An experimental study of the small world problem.
Sociometry,32(4):425–443,1969. 1,2,9
[13] JohnGuare. Sixdegreesofseparation. InTheContemporaryMonologue: Men,pages89–93.
Routledge,2016. 1
[14] DuncanJWatts. SixDegrees: TheScienceofaConnectedAge. WWNorton&Company,2004.
1
[15] MillerMcPherson,LynnSmith-Lovin,andJamesMCook. Birdsofafeather: Homophilyin
socialnetworks. AnnualReviewofSociology,27(1):415–444,2001. 2
[16] SantoFortunato. Communitydetectioningraphs. PhysicsReports,486(3-5):75–174,2010. 2
[17] MarkSGranovetter. Thestrengthofweakties. AmericanJournalofSociology,78(6):1360–
1380,1973. 2
[18] SinanAral. Thefutureofweakties. AmericanJournalofSociology,121(6):1931–1939,2016.
2
[19] MarkE.J.Newman. Thestructureandfunctionofcomplexnetworks. SIAMReview,45(2):
167–256,2003. 2
[20] MohakBhardwaj,SanjibanChoudhury,andSebastianScherer. Learningheuristicsearchvia
imitation. InCoRL’17,2017. 2
[21] MichalPándy,WeikangQiu,GabrieleCorso,PetarVelicˇkovic´,ZhitaoYing,JureLeskovec,and
PietroLiò. Learninggraphsearchheuristics. InLoG’22,2022. 2,3,9
[22] Rajarshi Das, Shehzaad Dhuliawala, Manzil Zaheer, Luke Vilnis, Ishan Durugkar, Akshay
Krishnamurthy,AlexSmola,andAndrewMcCallum. Goforawalkandarriveattheanswer:
Reasoningoverpathsinknowledgebasesusingreinforcementlearning. InICLR’18,2018. 2,3
[23] YelongShen,JianshuChen,Po-SenHuang,YuqingGuo,andJianfengGao. M-walk: Learning
towalkovergraphsusingMonteCarloTreeSearch.AdvancesinNeuralInformationProcessing
Systems,31,2018. 2,3,9
[24] JakobFoerster,GregoryFarquhar,TriantafyllosAfouras,NantasNardelli,andShimonWhiteson.
Counterfactualmulti-agentpolicygradients. InAAAI’18,2018. 2
10ReinforcementLearningDiscoversEfficientDecentralizedGraphPathSearchStrategies
[25] PetarVelicˇkovic´,GuillemCucurull,ArantxaCasanova,AdrianaRomero,PietroLio,andYoshua
Bengio. Graphattentionnetworks. InICLR’18,2018. 2,5
[26] ThomasH.Cormen,CharlesE.Leiserson,RonaldL.Rivest,andCliffordStein. Introductionto
Algorithms. MITPress,2022. 2
[27] PeterSheridanDodds,RobyMuhamad,andDuncanJWatts. Anexperimentalstudyofsearch
inglobalsocialnetworks. Science,301(5634):827–829,2003. 2
[28] PeterDKillworthandHRussellBernard.Thereversalsmall-worldexperiment.Socialnetworks,
1(2):159–192,1978. 2
[29] Albert-LászlóBarabásiandRékaAlbert. Emergenceofscalinginrandomnetworks. Science,
286(5439):509–512,1999. 2
[30] MariánBoguñá,DmitriKrioukov,andKimberlyC.Claffy. Navigabilityofcomplexnetworks.
NaturePhysics,5(1):74–80,2009. 2,3
[31] JonKleinberg. Thesmall-worldphenomenon: Analgorithmicperspective. InSTOC’00,pages
163–170,2000. 3
[32] ÖzgürSimsekandDavidJensen. Decentralizedsearchinnetworksusinghomophilyanddegree
disparity. InIJCAI’05,2005. 3
[33] Victor-AlexandruDarvariu,StephenHailes,andMircoMusolesi. Graphreinforcementlearning
forcombinatorialoptimization: Asurveyandunifyingperspective. TransactionsonMachine
LearningResearch,2024. 3
[34] Justin Boyan and Michael Littman. Packet routing in dynamically changing networks: A
reinforcementlearningapproach. AdvancesinNeuralInformationProcessingSystems,6,1993.
3
[35] Christopher J. C. H. Watkins. Learning from delayed rewards. PhD thesis, University of
Cambridge,1989. 3
[36] SamuelChoiandDit-YanYeung.Predictiveq-routing:Amemory-basedreinforcementlearning
approachtoadaptivetrafficcontrol. AdvancesinNeuralInformationProcessingSystems,8,
1995. 3
[37] LeonidPeshkinandVirginiaSavova.Reinforcementlearningforadaptiverouting.InIJCNN’02,
2002. 3
[38] WilliamL.Hamilton. GraphRepresentationLearning. Morgan&ClaypoolPublishers,2020. 3
[39] Asaf Valadarsky, Michael Schapira, Dafna Shahaf, and Aviv Tamar. Learning to route. In
HotNets’17,pages185–191,2017. 3
[40] PaulAlmasan, JoséSuárez-Varela, KrzysztofRusek, PereBarlet-Ros, andAlbertCabellos-
Aparicio. Deep reinforcement learning meets graph neural networks: Exploring a routing
optimizationusecase. ComputerCommunications,196:184–194,2022. 3
[41] KelvinGuu,JohnMiller,andPercyLiang. Traversingknowledgegraphsinvectorspace. In
EMNLP’15,pages318–327,2015. 3
[42] CameronBBrowne,EdwardPowley,DanielWhitehouse,SimonMLucas,PeterICowling,
PhilippRohlfshagen,StephenTavener,DiegoPerez,SpyridonSamothrakis,andSimonColton.
Asurveyofmontecarlotreesearchmethods. IEEETransactionsonComputationalIntelligence
andAIinGames,4(1):1–43,2012. 3
[43] FransOliehoekandChristopherAmato. AConciseIntroductiontoDecentralizedPOMDPs.
Springer,2016. 4
[44] Ryan Lowe, Yi I Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch.
Multi-agentactor-criticformixedcooperative-competitiveenvironments. AdvancesinNeural
InformationProcessingSystems,2017. 5
[45] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lilli-
crap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep
reinforcementlearning. InICML’16,pages1928–1937.PMLR,2016. 5
[46] JureLeskovecandJulianMcauley.Learningtodiscoversocialcirclesinegonetworks.Advances
inNeuralInformationProcessingSystems,2012. 6
11ReinforcementLearningDiscoversEfficientDecentralizedGraphPathSearchStrategies
[47] JureLeskovecandAndrejKrevl. SNAPDatasets: Stanfordlargenetworkdatasetcollection.
http://snap.stanford.edu/data,June2014. 6
[48] MarcusKaiserandClausCHilgetag. Spatialgrowthofreal-worldnetworks. PhysicalReview
E,69(3):036103,2004. 6
[49] LászlóLovász. Randomwalksongraphs. Combinatorics, PaulErdo˝sisEighty, 2(1-46):4,
1993. 7
[50] DiederikPKingmaandJimmyBa. Adam: Amethodforstochasticoptimization. InICLR’15,
2015. 12
A Appendix
A.1 ImplementationDetails
In a future version, we will release code that enables reproducibility of the results presented in
thiswork. Thiswillincludeinstructionsonhowtosetupthedependencies,downloadthepublicly
availabledata,andrunthemethods.
WetrainourmodelsusingtheAdamoptimizer[50]for200,000episodes,evaluatingperformance
every100episodesontheserializedvalidationset. Earlystoppingisappliedbasedonthevalidation
lossR¯ . Unlessotherwisestated,wetrainandevaluatemodelsover10randomseeds,reporting
oracle
confidenceintervalswhereappropriate. Table3presentsthehyperparameterconfigurationshared
acrossthethreemodeldesigns.Wefixγ =0.99andthemaximumepisodelengthT =100.Lastly,
max
whenprovidingnodeinputfeaturestotheGAT,weappendabinaryindicatorvariablethatsignals
thataparticularnodeuisthecenteroftheegographtotherawattributesdefinedas[x ||I[w =u]].
w
Thisisusedtodistinguishthenodefromwhichthemessagemustbesent.
Parameter Value
ActorNetworkHiddenDimensions 64
ActorNetworkLayers 3
CriticNetworkHiddenDimensions 64
CriticNetworkLayers 3
GraphAttentionNetworkHiddenDimensions 64
GraphAttentionNetworkHeads 1
GraphAttentionNetworkLayers 3
EntropyRegularizationCoefficientλ 1×10−3
Table3: Hyperparameterconfigurationusedforallthelearning-basedmodels.
A.2 AdditionalTablesandFigures
In Figure 3, we plot GARDEN’s learned value function across the social network ego graphs.
Brighter colors indicate a higher estimated value function relative to the target node, which is
indicatedwithablackarrowandchosenrandomlyfromtherespectivetestsets. Forcomparison,we
alsoplottheimplicitpreferabilityscore−∥x −x ∥ /τ generatedbythebest-performingbaseline,
u utgt 2
DistanceWalker,forthesamesource-targetpairs. DistanceWalkerstruggleswithEuclideanpairwise
attribute distance due to high dimensionality and sparsity of node attributes. Conversely, values
obtainedbyGARDENserveasareliableproxyforgraphdistance,assigninghighestvaluestonodes
inthetarget’sclusterorclusterswithstrongconnectivitytothetarget’scommunity.
Graph n l G ρ d
1 148 2.69 0.16 105
2 168 2.43 0.12 63
3 224 2.52 0.13 161
4 324 3.75 0.05 224
5 532 3.45 0.03 262
Table4: Numberofnodesn,averageshortestpathlengthl ,edgedensityρandattributedimension
G
dforeachreal-worldegograph.
12ReinforcementLearningDiscoversEfficientDecentralizedGraphPathSearchStrategies
Figure3: Visualizationofthelearnedvaluefunctionv(u,u )learnedbyGARDENforeachnode
tgt
u (left) and preferability score −∥x −x ∥ /τ of the DistanceWalker baseline (right) for the
u utgt 2
social network graphs. The black arrows indicate the target node, while the color intensities of
theothernodesareproportionaltothevaluefunctionlearnedbyGARDEN(left)orbaselinescore
(right). Concretely,darkrednodesindicatehighproximitytothetarget,whiledarkbluenodesreflect
lowproximity. GARDENrecoversmeaningfulvaluesforperforminggraphnavigation,effectively
leveragingproximityinbothnodeattributesandtopologicalstructure.
13