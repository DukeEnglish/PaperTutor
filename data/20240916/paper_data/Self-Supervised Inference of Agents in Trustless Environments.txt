SELF-SUPERVISED INFERENCE OF AGENTS IN TRUSTLESS
ENVIRONMENTS
VladyslavLarin IvanNikitin AlexanderFirsov
Fortytwo Fortytwo Fortytwo
vlarin@fortytwo.network inikitin@fortytwo.network afirsov@fortytwo.network
September16,2024
ABSTRACT The evolution of decentralized AI has been facilitated
by advances in blockchain technology, which provides a
Inthispaper,weproposeanovelapproachwhereagents
robustframeworkfortrustlessoperations.Smartcontracts
canformswarmstoproducehigh-qualityresponseseffec-
enabletheautomationofservice-levelagreements(SLAs)
tively. This is accomplished by utilizing agents capable
and ensure that all parties adhere to predefined rules
of data inference and ranking, which can be effectively
without the need for intermediaries. This is particularly
implemented using LLMs as response classifiers. We
beneficial in scenarios where trust and transparency are
assess existing approaches for trustless agent inference,
paramount,suchasfinancialservicesandhealthcare[2].
define our methodology, estimate practical parameters,
andmodelvarioustypesofmaliciousagentattacks. Our Several innovative frameworks and protocols have been
method leverages the collective intelligence of swarms, developed to support decentralized AI inference. The
ensuring robust and efficient decentralized AI inference AI Protocol’s Decentralized Inference Clusters (DePIN)
with better accuracy, security, and reliability. We show exemplify the shift from traditional centralized models
that our approach is an order of magnitude faster than todistributedframeworks,enablingefficientandscalable
othertrustlessinferencestrategiesreachinglessthan125 AI services by leveraging decentralized resources [3].
msvalidationlatency. Similarly, platforms like Nesa offer decentralized query
marketplacesthatsupporthigh-securityandprivacymea-
sures,facilitatingthedeploymentofAImodelsintrustless
environments[4].
1 Introduction
Despite these advancements, significant challenges re-
main in achieving fast, trustless inference on large
Decentralized AI inference represents a transformative artificial neural networks (ANNs). The computational
shift in the deployment and utilization of artificial in- demands and latency associated with decentralized op-
telligence. Traditional AI systems rely heavily on cen- erations can hinder performance, especially for real-
tralized infrastructures, which can lead to computational time applications. Additionally, ensuring the integrity
bottlenecksandsinglepointsoffailure. Thesecentralized and security of the inference process in the presence of
systems often face challenges related to scalability and maliciousactorsisacomplexproblemthatrequiresrobust
adaptability in meeting the growing demand for AI solutions.
inference.
This paper explores the current state-of-the-art in de-
To address these limitations, a distributed approach has centralized AI inference, highlighting the advantages
emerged,spreadingAIcomputationacrossmultiplenodes and limitations of various approaches. In light of
in a network. This method enhances system resilience existing challenges, we propose a novel method for self-
and enables more efficient resource utilization. In such supervised inference of agents in trustless environments,
decentralized systems, tasks such as model training, leveraging the collective intelligence of agent swarms
inference, and data processing are performed across a to ensure high-quality responses. We demonstrate that
distributed network. Consequently, this eliminates the Large Language Models (LLMs) can efficiently and
needforacentralauthority,allowingforgreaterflexibility quickly perform the response ranking necessary for
andpotentiallyreducingoperationalcosts. [1]. swarm consensus. Additionally, our approach incorpo-
1
4202
peS
21
]AM.sc[
1v68380.9042:viXrarates mechanisms for detecting and mitigating malicious Enclaves and Trusted Execution Environments
behaviors, ensuring the integrity and reliability of the (TEEs). TEEs,suchasIntelSGXandARMTrustZone,
inferenceprocess. provide secure environments for AI model inference.
Their limited scalability and availability overhead make
them less suitable for large-scale, real-time applications
2 RelatedWork
[12].
Various approaches have been proposed for trustless AI
Homomorphic Encryption. Fully Homomorphic En-
inference in decentralized environments. Each method
cryption (FHE) allows computations on encrypted data,
offers unique advantages but also has significant draw-
preservingprivacyandvalidity. However, thesignificant
backs, particularly when applied to large artificial neural
performanceoverheadandresourcerequirementslimitits
networks(ANNs).
practicalityforreal-timeAIinference[13].
Proof of Quality (PoQ). PoQ ensures robust and effi-
cientdeploymentofgenerativeAImodelsonblockchain VerifiableComputation. InteractiveproofsandProba-
architecture by focusing on validating model responses bilistically Checkable Proofs (PCPs) enable verification
using lightweight, simple model assessors. However, of computations in a decentralized manner. The com-
the trade-off between guaranteed accuracy and inference plexityandresourceintensityofthesemethodslimittheir
latency is significant, leading to less than 70 percent practicalityforlarge-scaleneuralnetworks[14].
accuracy for fast inference, making it less suitable for
quality-demandingapplications[5].
3 Self-SupervisedAgents
Zero-Knowledge Machine Learning (ZKML).
Collaborativeagent-basedframeworkshavedemonstrated
ZKML combines zero-knowledge proofs (ZKPs) with
strong results in improving both the accuracy and ef-
machine learning for verifiable AI model inferences.
ficiency of AI systems, particularly when compared to
Despite its strong guarantees, ZKML’s computational
monolithicsolutions,bydistributingtasksacrossspecial-
overhead can be significant, limiting its practicality for
ized agents [11]. Our approach leverages the collective
large-scaleapplications[6].
intelligenceofswarmsofagentscapableofbothinference
and ranking. These agents form dynamic networks that
Halo2ZK-SNARKProtocol. TheHalo2protocolem-
ensurehigh-qualityresponseswhileeffectivelymitigating
ploysPlonkisharithmetizationtoefficientlyexpressDNN
theriskofmaliciousbehavior.Inthissection,wedescribe
inference and generate zero-knowledge proofs. While
the architecture, practical parameters, and methods for
it ensures high security at lower costs than ZKML, the
detectingandmitigatingmaliciousattacks.
computationalexpensesarestillprohibitiveforverylarge
neuralnetworks[7].
3.1 AgentArchitecture
Optimistic Machine Learning (OPML). OPML in-
troduces a novel framework that leverages optimistic Each agent in the swarm performs data inference and
rollupsformachinelearninginferencesontheblockchain, quality ranking. Agents communicate with each other
significantlyreducingcomputationalcostsandimproving to form a consensus on the best responses, ensuring
efficiency. Unlike ZKML, which uses zero-knowledge robustnessandaccuracy.
proofs to achieve verifiable AI inferences, OPML relies There is a valid question whether we can use the
on a fraud-proof protocol that separates execution from single LLM both for content generating and response
proving, thereby optimizing resource usage while main- ranking. Recent studies [15] answer positively to this
taining security. However, this trade-off comes with the question, moreover MetaRanking approach achieves
need for a challenge period, which may delay finality over 80 percent accuracy with ranking complex GPT4
comparedtoinstantZKPverification[8]. responses using lightweight Phi2 model[16]. Thus, each
agent can easily provide reasonable ranking capabilities
Federated Learning. Federated learning enables de- totheswarmusingitslightweightexpertLLM.
centralizedtrainingbyaggregatingmodelupdatesinstead
Theproposedsystememploysamulti-agentarchitecture,
of raw data. However, communication overhead and
where each agent is designed as a composite entity
synchronization challenges can slow down the overall
encompassingseveralkeycomponents:
processinheterogeneousenvironments[9].
1. PrimaryCognitiveModule: Thiscorecomponentis
Blockchain-based Model Verification. Smart con-
responsible for both content generation and ranking
tracts on blockchains can verify AI model inference by
tasks. Itcanbeimplementedaseither:
checkingtheconsistencyofmodeloutputs.Theexecution
of verification algorithms can be slow and resource- • ALargeLanguageModel(LLM),or
intensive,hinderingreal-timeinference[10]. • AnExpertSystem
22. AuxiliaryProcessingUnit(Optional): Thismodule A client request is broadcast to the swarm. Participating
augments the primary cognitive capabilities by per- agents generate and submit encrypted responses to the
forming: swarm. It’s important to note that only willing agents
• Pre-processingoperationsoninputdata can take part in the response generation, filtering client
• Post-processingoperationsongeneratedcontent requests for their expertise and applicability. After
a predetermined time interval, ∆t, the responses are
AuxillaryUnitcanconnecttotheexternalworldand
decrypted using keys submitted by each participating
be represented like tools (interpreters, calculators,
agent.
knowledge bases, filters) as well as third-party ser-
vices (Math processors, Internet access and Search
Engine,CloudMLProvidersandCo-Pilots)
R =E(r ,k ), D(R ,k )=r (1)
i i i i i i
This modular architecture allows flexibility in agent im-
plementation while maintaining a standardized interface where R is the encrypted response, r is the original
i i
forinter-agentcommunicationandsystemintegration. response, k is the encryption key, E is the encryption
i
function, and D is the decryption function. This
The primary cognitive module’s dual functionality in
encryption-decryption mechanism mitigates potential at-
content generation and ranking enables efficient task
tacks involving the copying of highly-rated nodes’ re-
execution, while the optional auxiliary processing unit
sponses.
providesenhancedadaptabilitytodiverseinputandoutput
requirements.
Selective Ranking Phase Following the response gen-
eration,aselectiverankingprocessisemployed:
Asubsetofagents,S ⊂A,ispseudo-randomlyselected
j
to rank each agent’s response, where A is the set of
participating agents. The selection is based on a recent
blockhash,H ,ensuringrandomnessandfairness. Each
b
agent a ∈ A ranks responses from approximately one-
i
thirdofotheragents,excludingitsownresponse.
|A|−1
S =f(H ,A\a ), |S |≈ (2)
j b j j 3
where f is the selection function based on the block
hash. This selective ranking approach reduces the risk
of collusion and prevents any single agent from exerting
undueinfluenceovertheconsensus.
FinalSelectionPhase Thefinalselectionoftheoptimal
responseproceedsasfollows:
Rankings are submitted to the swarm in encrypted form
topreventcopyingattacks. Afteradecryptionround,the
bestresponseisselectedbasedonweightedrankings.The
weightofeachrankingisdeterminedbytheratingofthe
Figure1: SchematicrepresentationoftheAgentarchitec- rankingagent.
ture
(cid:88)
3.2 Swarm-basedConsensusMechanismfor r best =arg,max w j ·rank(r i,a j) (3)
OptimalResponseSelection ri j∈Si
This section outlines a novel swarm-based consensus where w is the weight (rating) of agent a , and
j j
mechanism designed to coordinate agents and select the rank(r ,a )istherankassignedtoresponser byagent
i j i
most appropriate response from a pool of generated a .
j
answers. The process is divided into three main phases:
Theselectedbestresponse,r ,isreturnedtotheclient.
responsegeneration,selectiveranking,andfinalselection. best
Thismechanismensuresafair, decentralized, androbust
ResponseGenerationPhase Theprocessinitiateswith selectionprocessthatleveragesthecollectiveintelligence
thefollowingsteps: oftheswarmwhilemitigatingpotentialvulnerabilities.
33.3 AgentsRating-RankingAbilityandQuality
Estimation
Trustless systems need to be robust and stable with the
highestamountofnoisebeingintroducedbymaliciousor
lazyactors.Whilethesmarm’srankingalgorithmguaran-
teesconsensuswithenoughswarmsize,aneffectiveagent
ranking ability and answer quality estimation enhance
thesemechanismsbyensuringthatonlythemostreliable
nodesparticipateintheconsensusprocess. Ourapproach
leverages the statistical properties of score deviations
from the mean, assuming that the collective rankings by
nodesconformtoanormaldistribution,aconsequenceof
theCentralLimitTheorem.
3.3.1 Methodology
Each agent in the network ranks other agents’ contri-
butions based on specific criteria. For each ranking
cycle, the deviation of a node’s score from the mean
scoreiscomputed. Thestandarddeviationofthesescore
deviations across multiple cycles is used as a metric to
estimatethenode’srankingability:
(cid:118)
(cid:117) N
(cid:117) 1 (cid:88)
σ
i
=(cid:116)
N −1
(x
ij
−x j)2 (4)
j=1
whereσ isthestandarddeviationofscoredeviationsfor
i
nodei,x isthescoregivenbynodeitonodej,andx
ij j
isthemeanscorereceivedbynodej.
Limitingthismetriconlytotheblockswithahighnumber
ofparticipants,weassumethatthesumofscoresgivento
eachnodeapproximatesanormaldistribution(theconse-
quence of the Central Limit Theorem). This assumption
allows us to use statistical methods to systematically
analyzetherankingbehaviors.
Nodes with lower values of σ are considered more
i
aligned with the collective decision-making process,
indicatingahigherreliabilityintheirrankingassessments
orresponsequality.Thesenodesaregivenmoreinfluence
in the consensus process, leading to a more robust and
secureagentswarm.
Finally,toestimatethenode’srating(rankingability)we
cancalculatetheinvertedσ value:
i
Rating =1−σ (5)
i i
3.3.2 Nodes’RatingEstimation
We conducted several simulations to estimate ranking
ability. Inoursetup,weused10testagentswithdifferent
actual ranking abilities, and we simulated various num-
bers of test consensus rounds to calculate the proposed
ranking metric. As we observed, even 10 simulation
Figure2: Swarm-basedConsensusMechanism
rounds are sufficient to provide a rough estimation of an
agent’srankingability.
44 AdversarialAgentDetectionand
Mitigation
We model various types of malicious attacks, including
those producing random outputs (lazy agents), incon-
sistent quality outputs (buggy agents), and deliberate
attemptsbymaliciousagentstoforgeoutputsforpersonal
gain. Whilelazyandbuggyagents’outputsaremitigated
using ranking and rating systems, our approach includes
mechanismsfordetectingandisolatingmaliciousagents’
influence on the cluster, ensuring the integrity and relia-
bilityoftheinferenceprocess.
Figure3: RankingEstimationSimulation-10Rounds
4.1 SybilAttacks
A Sybil attack poses a significant security threat in
trustless networks, where a single entity creates multi-
ple fake identities to gain disproportionate influence or
control. A Self-Supervised Agent Inference approach
necessitates a balanced incentive model to render Sybil
attacksfinanciallyunfeasible.
4.1.1 Methodology
Each participating node in the network is tasked with
completing a specific job, which involves computing
LLM requests. This job completion is essential as it
directly influences the consensus process by providing a
measurableoutputthatcanbeverifiedandrankedbyother
nodes.
Figure4: RankingEstimationSimulation-100Rounds
Nodesthatwishtoparticipateinthesolutionprocessare
requiredtopurchaseaticket. Thiseconomicdisincentive
is a critical component in preventing Sybil attacks, as
the cost of creating numerous fake identities becomes
prohibitivelyhighduetotherequiredtokenpriceforeach
participatingidentity.
The reward mechanism in this consensus model is de-
signed to promote the best contributions. Only the
nodewhosesolutionwinsthemajorityapprovalfromthe
ranked nodes receives the major portion of the rewards,
which include both the intrinsic value derived from
solvingtheproblem(e.g.,transactionfees,blockrewards)
andpartoftheticketvaluefromnodesthatofferedpoorer
solutions. This further aligns the incentives of the nodes
withtheoverallhealthandsecurityofthenetwork.
4.1.2 SybilAttackProfitabilitySimulation
Figure5: RankingEstimationSimulation-1000Rounds
The following graph displays the profitability of Sybil
attacks in an agent swarm, with varying numbers of
This ranking estimation mechanism provides a statis- total nodes and token deposit requirements. The x-axis
tically sound method for assessing the reliability and represents the total number of nodes in the network,
accuracyofagentsinconsensus. Byintegratingthissys- ranging from 10 to 500. The y-axis shows the range of
tem, agent swarm enhances their security and efficiency, tokendepositsrequiredbyeachnode,from0to2tokens.
ensuring that only the most competent nodes govern the Inthisscenario,wesimulatearewardof20tokensforthe
consensusprocess. winningnode.
54.2 PromptEngineeringAttacks
Oneofthepotentialvulnerabilitiesintheswarmofagents
approachisthepromptengineeringattacks.Theseattacks
occur when malicious nodes attempt to manipulate the
inference process by crafting inputs (prompts) that can
exploit the ranking mechanism of other agents in the
network. The goal of such an attack is to have the
malicious agent’s output ranked higher than it deserves,
thereby influencing the overall consensus of the swarm
andpotentiallycompromisingthesystem’sintegrity.
Prompt engineering attacks can take various forms, but
they generally fall into two main categories: low-
frequency token attacks and common-sense prompt at-
tacks. Bothtypesofattacksaimtodeceiveormanipulate
the underlying language models (LLMs) used by other
agents to skew the ranking of responses in favor of the
Figure6: ProfitabilityofSybilattacks attacker.
4.2.1 Low-FrequencyTokenAttacks
The color coding indicates the financial outcome of the
Sybilattack: Low-frequencytokenattacksinvolvetheinsertionofrare
orspecialtokens,suchasuniqueUnicodecharacters,into
the generated text by the malicious agent. The premise
• White areas represent scenarios where the attack
behind this attack is that by introducing these low-
resultsinnegativeprofits,makingthemunprofitableor
frequency tokens, the malicious response might exploit
unattractiveforattackers.
biases or vulnerabilities in the language models of other
• Shades of green signify varying levels of positive agents, causing them to rank the manipulated response
profits, with darker greens indicating higher profits more favorably. This tactic could potentially work in
fromtheattack. scenarios where the language models disproportionately
weight the presence of rare tokens as indicative of novel
orimportantinformation.
Aredlineonthegraphmarksthebreak-evenpoint,where
the profits from the attack shift from positive to zero. However, a key countermeasure against such attacks in
Above this line, attacks are not profitable (white area), our decentralized AI framework is the heterogeneity of
providing a clear visual guide on setting token deposit the agents’ LLMs. Since each agent in the swarm
thresholds to deter malicious activities effectively. This operates either a varied or uniquely tuned LLM, the ef-
line is crucial to understanding how to scale security fectivenessoflow-frequencytokenattacksissignificantly
measuresbasedonnetworksizeandpotentialrewardsto diminished. An insertion that may trigger a favorable
maintaintheintegrityofthenetwork. bias in one model is unlikely to have the same effect
across a diverse set of models. The diversity of LLM
In summary, the graph demonstrates that setting a ticket
architectures, training data, and tokenization processes
price as low as 1% of the potential reward is sufficient
meansthattheseraretokensdonotconsistentlyinfluence
to make an attack unprofitable, even in a moderately
the ranking process, thereby reducing the chances of a
sized agent swarm, effectively enhancing the network’s
successfulattack. Thisdiversityactsasanaturaldefense
resilienceagainstSybilattacks.
mechanism, ensuring that no single token-based strategy
Theselectiverankingbasedonhashfunctionsreducesthe canuniversallydeceivetheswarm.
overhead typically associated with each node evaluating
everyothernode’ssubmission,whichcanbeparticularly
4.2.2 Common-SensePromptAttacks
burdensome in large networks. Furthermore, by decen-
tralizingtherankingandnotallowingnodestoranktheir
Common-sense prompt attacks are another form of ma-
ownsolutions, thesysteminherentlyguardsagainstself-
nipulation where the malicious agent embeds statements
promotionandfavoritism,thusenhancingthesecurityand
withinthepromptthatareintendedtoinfluencetheother
integrityoftheconsensusprocess.
agents’ LLMs to rank their response higher. Examples
Thisconsensusmechanismleverageseconomicincentives ofsuchmanipulativestatementsincludephraseslike“this
and algorithmic randomness to create a robust defense answeristhebest”orothersimilarassertionsthataimto
against Sybil attacks, ensuring that the blockchain main- exploit basic common-sense reasoning or self-referential
tainsitsdecentralized,secure,andtransparentnature. biaseswithintheLLMs.
6Eachagentintheswarmisinherentlymotivatedtoprotect exceptional performance is attributed to several key
itself against manipulations to maximize its received factors:
incentive. Luckily such a self-referential protection
mechanism is grounded in the “common-sense” ca- • Parallel Processing: The swarm architecture allows
pabilities of the agents’ LLMs, and can be utilized formassivelyparallelresponsegeneration. Allagents
to penalize responses that attempt to unduly influence takingpartintheroundcansimultaneouslyprocessthe
rankingthroughnon-substantivemeans. input query, effectively avoiding latency on the initial
inferencestep.
5 Evaluation • Selective Ranking: By having each agent rank only a
subsetofresponses,wedrasticallyreducethetimere-
quiredfortheevaluationphasewithoutcompromising
Weestimatethecomputationalresourcesrequiredforour
thequalityofselection.
approach, considering factors such as network latency,
processingpower,anddatathroughput. Ourmodelshows • Rapid Ranking Process: To rank a response, only a
that a swarm of agents can operate very efficiently in a single token needs to be inferred, which is orders of
decentralizedenvironment. magnitudefasterthangeneratingafullresponse. This
allowsforquickandefficientqualityassessment(rank-
ing)withoutsignificantlyimpactingoveralllatency.
Inference Latency Comparison. Table 1 compares
the inference latencies of various decentralized AI ap- • Weighted Ranking Aggregation: The final selection
proaches, revealing significant differences in perfor- phase employs a computationally efficient weighted
mance. Proof of Quality (PoQ) demonstrates the lowest ranking system that quickly identifies the optimal
latencyat50msforMobileNetv2, thoughitisachieved response.
with low accuracy guarantees (< 70%) due to validation
• Asynchronous Operations: Many of the consensus
using a simple BERT transformer [5]. In stark contrast,
mechanism’s steps occur asynchronously, allowing
ZKML exhibits extremely high latency, taking over 24
for overlapping operations that further reduce overall
hours for ResNet-50 inference, rendering it impractical
latency.
for large-scale models [6]. The Halo2 ZK-SNARK
protocol shows improvement but still requires 2457.5
This combination of parallelism, efficient cryptographic
seconds (about 41 minutes) for MobileNet v2, which
operations, intelligent agent coordination, and our con-
remains prohibitive for very large neural networks [7].
sensus technique enables our system to leverage the full
OPMLoffersasignificantreductioncomparedtoZKML,
power of the Llama 3 405B model while maintaining
completing ResNet-50 inference in 3.6 hours, due to
inference latencies below 125 ms. The single-token
its challenge period [8]. Trusted Execution Environ-
ranking approach is particularly crucial, as it allows for
ments like Intel SGX achieve relatively low latency
quality assessment at a fraction of the time required for
at 230 ms for VGG-16, but struggle with scalability
full response generation. This represents a significant
[12]. Homomorphic Encryption, while preserving pri-
advancement in decentralized AI, offering performance
vacy,incurshighlatencyat788secondsforSqueezeNet,
comparable to centralized solutions while preserving the
making it unsuitable for real-time applications [13].
benefitsofswarm-basedconsensusanddecentralization.
Federated Learning, Blockchain-based Verification, and
Verifiable Computation approaches lack specific latency
figures in the literature for direct comparison, but are
6 Conclusion
generally understood to face significant overhead due
to communication costs, resource-intensive operations,
and computational complexity, respectively [9, 10, 14]. This paper presents a novel AI inference approach that
This comparison highlights the ongoing challenge in leverages the collective intelligence of agent swarms
achieving both low-latency and secure decentralized AI in decentralized environments. By employing agents
inference. While some approaches like PoQ and TEEs capable of data inference and response ranking, our
offerpromisinglatencyfigures,theycomewiththeirown methodenablesLargeLanguageModels(LLMs)tofunc-
limitations. The trade-off between security, efficiency, tion effectively as response classifiers while addressing
and practicality remains a key area for further research criticalchallengesincludingsecurity,reliability,andrapid
anddevelopmentindecentralizedAI,andSelf-Supervised responsetimes.
Agent Inference shows a promising approach to this Our proposed method demonstrates significant improve-
problem. mentsoverexistingtrustlessinferencestrategiesinterms
ofspeed,accuracy,andresiliencetomaliciousattacks.By
Ultra-Low Latency Inference on Large Language modeling various types of malicious agent behavior, we
Models. Our swarm-based consensus mechanism have developed and validated mechanisms to detect and
demonstrates remarkable efficiency when applied to mitigate such threats, ensuring the integrity of the infer-
large language models such as Llama 3 405B, achieving enceprocess. Thisapproachprovidesascalablesolution
inference latencies of under 125 milliseconds. This adaptabletothegrowingdemandsofAIapplications.
7Table1: ComparisonofInferenceLatencyforDecentralizedAIApproaches
Approach Latency Model/Dataset Limitations
PoQ 50ms MobileNetv2 Lowinferenceaccuracyguarantees(<70%)
ZKML >24h ResNet-50 Impracticalatscale
ZK-SNARKHalo2 2457.5s MobileNetv2 Prohibitiveforlargenets
OPML 3.6h ResNet-50 Challengeperioddelays
FederatedLearning N/A – Communicationoverhead
BlockchainVerification N/A – Resource-intensive
TEEs(SGX) 230ms VGG-16 Limitedscalability
HE 788s SqueezeNet Notreal-timecapable
VerifiableComputation N/A – Smallnetworksonly
Ours <125ms Llama3405B –
The methodologies and insights presented here suggest [8] KD Conway, Cathie So, Xiaohang Yu, Kartin Wong.
thatdecentralizedAIsystemshavethepotentialtomatch ”opML: Optimistic Machine Learning on Blockchain.”
or surpass the performance of traditional centralized arXivpreprintarXiv:2401.17555,2024.
systems. Our work represents a significant step forward [9] J. Wang, A. K. Sahu, G. Joshi, and S. Kar, “Matcha:
intrustlessandscalableAIinference,highlightingthepo- A matchingbased link scheduling strategy to speed up
tentialofswarm-basedintelligencetoovercomeinherent distributed optimization,” IEEE Trans. Signal Process.,
challengesintheseenvironments. vol.70,pp.5208–5221,Nov.2022.
[10] Sanghyeon Park, Junmo Lee, Soo-Mook Moon.
Future research may build upon this work to further
”Blockchain-based Model Verification for AI Inference.”
improve response quality and accuracy through more
arXivpreprintarXiv:2305.04062,2024.
complex inter-agent collaboration, and advancement of
heterogeneousagentnetworks.Additionally,effortscould [11] YunxiaoShi,MinXu,HaiminZhang,XingZi,andQiang
Wu. ”A Learnable Agent Collaboration Network Frame-
be directed towards optimizing scalability and exploring
work for Personalized Multimodal AI Search Engine.”
applications in diverse, complex real-world scenarios.
arXivpreprintarXiv:2409.00636,2024.
These advancements will pave the way for more secure,
efficient, and reliable AI applications across various [12] Jean-Baptiste Truong, William Gallagher, Tian Guo,
domains,contributingtoanewparadigmofdecentralized RobertJ.Walls.”Memory-EfficientDeepLearningInfer-
ence in Trusted Execution Environments” arXiv preprint
andpermissionlessinference.
arXiv:2104.15109,2021.
[13] Qian Lou, Lei Jiang. ”HEMET: A Homomorphic-
References
Encryption-Friendly Privacy-Preserving Mobile Neural
Network Architecture” arXiv preprint arXiv:2106.00038,
[1] SAKSHI. ”Decentralized AI Platforms.” arXiv preprint 2021.
arXiv:2307.16562,2024.
[14] Tiantian Gong, Aniket Kate, Alexandros Psomas, Athina
[2] POKT Network. ”Decentralized AI: Permissionless Terzoglou. ”V3rified: Revelation vs Non-Revelation
LLM Inference on POKT Network.” arXiv preprint Mechanisms for Decentralized Verifiable Computation”
arXiv:2405.20450,2024. arXivpreprintarXiv:2408.07177,2024.
[3] AIProtocol.”DecentralizedInferenceClusters.”AIProto- [15] Zhang,Y.,Sun,Q.,Xu,Z.,etal.”RRescue:RankingLLM
colWhitepaper,2024. Responses to Enhance Reasoning Over Context.” arXiv
[4] Nesa AI. ”Introduction to Nesa.” Nesa Documentation, preprintarXiv:2311.09136,2023.
2024.
[16] Wang, H., Zhu, Y., Lin, Z., et al. ”Meta Ranking:
[5] Zhang,Zhenjie,Rao,Yuyang,Xiao,Hao,Xiao,Xiaokui, Less Capable Language Models are Capable for Single
Yang, Yin. ”Proof of Quality: A Costless Paradigm for Response Judgement.” arXiv preprint arXiv:2402.12146,
TrustlessGenerativeAIModelInferenceonBlockchains.” 2024.
arXivpreprintarXiv:2405.17934,2024.
[6] Feng, B., Qin, L., Zhang, Z., Ding, Y., andChu, S.Zen.
”An optimizing compiler for verifiable, zero-knowledge
neural network inferences.” Cryptology ePrint Archive,
2021.
[7] Daniel Kang, Tatsunori Hashimoto, Ion Stoica, Yi
Sun. ”Scaling up Trustless DNN Inference with Zero-
Knowledge Proofs.” arXiv preprint arXiv:2210.08674,
2024.
8