Latent Space Score-based Diffusion Model for
Probabilistic Multivariate Time Series Imputation
Guojun Liang Najmeh Abiri Atiye Sadat Hashemi
School of Information Technology School of Information Technology School of Information Technology1
Halmstad University Halmstad University Halmstad University1
Halmstad, Sweden Halmstad, Sweden Lund University2
guojun.liang@hh.se najmeh.abiri@hh.se atiye-sadat.hashemi@hh.se
Jens Lundstro¨m Stefan Byttner Prayag Tiwari
School of Information Technology School of Information Technology School of Information Technology
Halmstad University Halmstad University Halmstad University
Halmstad, Sweden Halmstad, Sweden Halmstad, Sweden
jens.r.lundstrom@hh.se stefan.byttner@hh.se prayag.tiwari@hh.se
Abstract—Accurate imputation is essential for the reliability by the physics law. With the development of the transformer
andsuccessofdownstreamtasks.Recently,diffusionmodelshave structure [1], attention is widely used in some imputation
attracted great attention in this field. However, these models ne-
models [11]. SAITS [21] employs a self-supervised training
glectthelatentdistributioninalower-dimensionalspacederived
scheme with two diagonal-masked self-attention blocks and
from the observed data, which limits the generative capacity
of the diffusion model. Additionally, dealing with the original a weighted-combination block to impute missing data. Im-
missing data without labels becomes particularly problematic. putation methods can be broadly categorized into two types:
To address these issues, we propose the Latent Space Score- discriminativeandgenerativeapproaches.Mostdiscriminative
Based Diffusion Model (LSSDM) for probabilistic multivariate
models belong to supervised learning, which needs access to
time series imputation. Observed values are projected onto
the ground truth values for training. However, the require-
low-dimensional latent space and coarse values of the missing
data are reconstructed without knowing their ground truth ment can not be satisfied in most real scenarios since most
values by this unsupervised learning approach. Finally, the datasets suffer from the original missing phenomenon. They
reconstructed values are fed into a conditional diffusion model over-rely on the ground truth of the missing values and are
to obtain the precise imputed values of the time series. In
easily affected by the noise, thereby not accounting for the
this way, LSSDM not only possesses the power to identify the
uncertainty in the imputed values. Generative Deep Learning
latent distribution but also seamlessly integrates the diffusion
model to obtain the high-fidelity imputed values and assess the Methods, based on their ability to yield varied imputations
uncertainty of the dataset. Experimental results demonstrate that reflect the inherent uncertainty in the imputation [13],
that LSSDM achieves superior imputation performance while [14], are research hotspots in MTSI. GP-VAE [19] employs
also providing a better explanation and uncertainty analysis
the convolutional neural network (CNN) and VAE to find
of the imputation mechanism. The website of the code is
out the latent distribution by maximizing the Evidence Lower
https://github.com/gorgen2020/LSSDM imputation.
IndexTerms—Diffusionmodel,multivariatetimeseries,impu- Bound (ELBO) on the marginal likelihood. Even though it
tation, variational graph autoencoder is an efficient, interpretable latent space, it has smoother and
potentially less detailed outputs. The diffusion model [8] has
I. INTRODUCTION
attracted a lot of attention due to its high-quality output
Multivariate time series imputation (MTSI) is a crucial and better theoretical strength and robustness. CSDI [23]
approachforaddressingmissingdata,andscholarshavedevel- imputes the missing values based on the conditional diffusion
oped a range of MTSI models from diverse perspectives and probability model, which treats different nodes as multiple
applications [9] [16], including healthcare, transportation, etc. features of the time series and uses a Transformer to capture
Leveraging the strengths of recurrent neural network (RNN) featuredependencies.PriSTI[12]imputesmissingvalueswith
architectures, such as GRU and LSTM, Bi-directional GRU the help of the extracted conditional feature by a conditional
are employed in Brits [20] for MTSI tasks. Some researchers diffusion framework to calculate temporal and spatial global
state that the time series owns the latent graph structure [3] correlations, but this method is hard to apply to the health
and utilizes a Graph Neural Network (GNN) to implement care datasets such as electronic health records (EHRs) since
the imputation by its neighbor’s features [15]. Grin [2] is therearenoapparentspatialglobalcorrelationandgeographic
developed to deal with the graph structure by GNN with dependency of this irregular healthcare datasets. Nonetheless,
bidirectionalGRU.HSPGNN[4]incorporatesthephysicslaw these diffusion models do not consider the latent distribution
into the GNN structure and tries to obtain the missing values of the dataset, which limits the further generative capability
4202
peS
31
]GL.sc[
1v71980.9042:viXraD D
ne ne ……
o …… o
nisi nisi
g g S
at
Condition dn
Sampler
dra and the explanation of the imputation mechanism. Also, the Condition
Linear G imputation performance degrades dramatically when the orig-
a
interpolation u
aiss inal missing data has a significant proportion of the dataset
D D
N N
n sincetheydonothavethegroundtruthoftheoriginalmissing ne ne
S
Reverse p Dr eno oisc eess
Z
Denoise
……
Denoise
EnG cC oN
der
gnisio …… gnisio d
m
Da iet fa
n
futit soo
in
oea ndd Md
a
obn doo evis lee
,
(Lwto
Se
Sth Dpe
r
Mose
p
)om
fs
oeo rd
t
phe rel os.
bL
aT
a
bo
te iln
ia std tid
S
cr pe mas cs
ue
lt th
iS
ve
c ao
rq
ir
au
e
te
-
eBst tai io
s
men
d
es
N
CGredocnE
gnisio …… gnisio
G
dradnat
1
…… XT
s ge er nie es rati im vepu mta oti do en lsto ofu Vn Aify
E
t ah ne du dn is ffu up se ir ov nis ie nd Man Td SIs .up Le Sr Sv Dise Md
Sampler naissua
notonlypossessesthepowertoidentifythelatentdistribution
Denoise Denoise Denoise N N
Denoise
b hu igt ha -l fiso des le ita ym il mes ps uly tei dnt ve ag lr ua ete ss at nh ded asif sf eu ss sio thn em uo nd ce el rtt ao ino tb yta oin
f
t th he
e
gnisio …… gnisio
Not d Gis ucr ae st se ia b nu pt rc oo cn et si snuous Denoise Latent dataset. Moreover, the unsupervised learning VAE framework Forward process variant Fig. 1: The framework of LSSDM.
canhandletheoriginalmissingdataandthesimulatedmissing
Continuous HMM
data effectively without knowing the ground truth values,
Z0 ……
which makes these models more feasible in real scenarios.
The target function can thus be decomposed into two terms.
…… II. METHODOLOGY For the first term, considering the graph structure, we can
maximizetheELBOusingtheVariationalGraphAutoencoder
A. Preliminary
(VGAE) algorithm. The VGAE is trained to accurately re-
Forward process
As for the MTSI problem, we denote XN×D as the input construct missing data from the latent representation without
0
…… featurematrix,whereN isthenumberofsensorsandD isthe access to the ground truth of the missing values, as it is an
…..
Personal latent `` lengthofthetimeseries.Fortheimputationtask,M∈RN×D unsupervised generative model. To mitigate the sparsity of
Space X0 …… XT is used to indicate the missing mask, where M ij = 0 means the input features, during the training stage, we use linear
Add noise Add noise Add noise that the j-th sensor at time step i-th is missing, otherwise, interpolation to estimate coarse values X˜ta for all missing
Sha Sr pe a l ca etent ` ``….. Add noise M ij =1.ForFacilitatingsubsequentdiscussions,Xc 0o =X⊙ values, including both the original and simulated missing
M (⊙ is the Hadamard product) is adopted as the conditional values. The deduction is shown as follows:
Continuous HMM By
EM algorithm observed features while Xta = X⊙(1−M) as the missing (cid:90)
Traffic Trajectory
… maS t sa er rg iee st. oM wo nre tho eve gr, raA phre sp trr ue cs te0 un rt es ,t ih .ee .,a td hj eac te rn aft fim ca dtr oi mx aif int .he time log p(Xc 0o,X¯t 0a|Z 0)p(Z 0)dZ 0 ≥E qϕ(Z0|Xc 0o,X˜t 0a,A)
Observation p logp (Xco,X¯ta|Z )−KL[q (Z |Xco,X˜ta,A)||p(Z )],
el ψ 0 0 0 ϕ 0 0 0 0
Learning Prediction orf
B. Neural network architecture
(2)
m where ϕ and ψ are the learnable parameters. we assume
GCN
8800%% ffoorr tthhee ttrraaiinniinngg ddaattaasseett 2200%% ffoorr tthhee tteesstt ddaattaasseett The Framework of LSSDM is shown in Fig 1. Firstly, the posterior distribution obey the Gaussian distribution TTTiiimmmeee
the conditional observed values Xc 0o together with the coarse q (Z |Xco,X˜ta,A) = N(u ,Σ ). Then, the posterior dis- …………....
Prior distribution values of the linear interpolation X˜ta are projected onto ϕ 0 0 0 ϕ ϕ Dynamic GCN layer
tribution can be estimated by GCN model [18]:
the stochastic variable of low dimension latent space Z
0 (cid:16) (cid:17) (cid:16) (cid:17) …… ……
by the graph convolutional neural network (GCN) structure. H(l+1) =GCN A,H(l) =σ LH(l)w(l) , (3) D D Secondly,thereconstructedimputedvaluesX¯ta arecalculated Temporal Attention
e e 0
gnision …… gnision dnatS
TTTTiiiimmmmeeee
a Fb ry ie nat fh le le yd ,t tr o wan ets hf e ao dr fm oo pre w tr a Ds rt d ir fu fdc Wt iu f afr vue esii on [n 2t 5h p ]e ro td c oe ec s lo s ead b re y nr as tt hda edg ie n n. g oT it sh h ee en in, noX¯ i ts ht 0 ea e. w D adh − de i1 2r ne ( gI st −h ele A f-c)L oDa np n21l ea , cc w ti ia h on e nr sem ,DIat ir s ii sx th thec ea id dn e en gb t re i et eyf mmor aam tt rru ii xxla .,t we wd hh ei rca ehs HmL (e la )n= ∈s Dynamic GCN layer w H0
dra denoisingprocess,whichisconditionalontheobservedvalues RN×F istheinputofl layer,whileH(l+1) ∈RN×E isoutpSuptatial Attention Physics Law
Noising

Sampler aissuaG X byc 0o s. aF min pa lil nly g,t fh re omacc tu hr eat se taim ndp au rt ded Gv aa ul su se is anXˆ nt 0a oic sa en tb he roo ub gt hain te hd
e
o lef arl nl aa by le er pw arit ah mE etee r,m ab ne ddd σin rg e. prI en sea nd td sit ti ho en, nw on( ll i) ne∈ arR aF c× tiE vati is ona
D gy rn aa pm hic
Temporal Attention
Phy imsic ps u-i tn af to
iC
or
nmon eds traint
Matrix Multiplication
Noising nisioN …… nisioN n lea Trn oed ded tee rn moi is ni eng thn eeu pr ra el cin se etw imor pk u. tation value and latent dis- f eu nn cc ot dio ern. frI an mt eh wis ors kt .ud Ty h, ew me eaa ndo ap nt dth ve ari2 a- nla cy ee sr haG rC eN thLD a mey pan alaa ts rcm ifii xaic n r t L sh t te - TTTiiimmmeee TTiimmee
g …… g Latent Space tribution in the projected space, we apply Bayes’ rule to layerparametersbutareindependentinthesecondlayer.Thus, …….. L2 loss ++ Regularization ……..
compute the probability distribution of the dataset, logp(X ) the output of the latent variable can be expressed as: Valid value
0
Noising Continuous hidden States Latent Space as follows: u ϕ=GCN(X,A)=Sigmoid(L(ReLU(LXW(1)))W u(2)) (4) Objective IM mi pss uin teg d v va alu lue e
……
logp(X 0)=logp(Xc 0o,Xt 0a)=logp(Xt 0a|Xc 0o)+logp(Xc 0o) logΣ ϕ=GCN(X,A)=Sigmoid(L(ReLU(LXW(1)))W Σ(2)). Function Hadamard product
(cid:124) (cid:123)(cid:122) (cid:125)
Evidence The latent space distribution is obtained through the GCN.
Spatio-tempora(cid:90)l
Sampler Atte≈ntiloong[ p(Xta|Xco,X¯ta)p(Xco,X¯ta|Z )p(Z )dZ ] During the reconstruction stage, we utilize a combination of
0 0 0 0 0 0 0 0
Matrix Right Multiply a Transformer and 1D CNN to reconstruct the observed data,
(cid:90)
Latent Space Observed Values =log p(Xco,X¯taM|Zatrix) pLe(fZt Mu)ltdipZly +logp(Xta|Xco,X¯ta). represented as f ψ as follows:
0 0 0 0 0 0 0 0
D (1) X¯ =f (Z )=CNN(CNN(transformer(Z ))). (5)
e 0 ψ 0 0
n
o
gnisi
P
mask TTTiiimmmeee
oN oN High Order GCN layer
nisi
……
nisi
g g
N High Order GCN layer X=X−XPmask
o U+P nisi
X (1−M) X
g
TTTiiimmmeee
…………....
MLP Xˆ
phy
TTTiiimmmeee TTTiiimmmeee  H…… …… X Xˆt t− +T T'=Xt−T'−Xt−T' Pmask
………….... Physics-incorporated Xt+T
L t−M:t Layer
XCoastimputation www
D D Physics-incorporated
HHHmmm
ne ne High Order GCN layer Layer
o …… o
gnisi gnisi
Spatial Attention
Pmask Spatial Attention
LSTM
InhomD oy gn ena em oi uc
s PDE
Dynamic
Latent Space atS Temporal Attention Pre-missing dataset Simulate missing mask Training dataset graph Physi Ic ms- pin uc tao tr ip oo nrated
gnisioN
gnisioneD
Sampler
naissuaG
dradn
Time Conv diseR Regula ++rization
Loss
TTTiiimmmeee TTT ……iii ……mmm ..eee ..
High Order GCN layer
Physics Law Constraint MM aa tt rr ii xx LR eig fth Mt M ulu til pti lp yly
=
Dynamic Laplacian matrix …TTT …iiimmm ..eee Temporal Attention
V Ma il si sd
in
v ga l vu ae
lue
……TTii ..mmee
Right Multiplication
nisioN
……
nisioN Spa Ati to t- et ne tm iop noral High Order GCN layer nnoc lau
L2 loss
Function
Valid value
Temporal Attention ++ Regularization TTTiiimmmeee TTT ……iii ……mmm ..eee
..
Simulate missing mask Test dataset
HHaaddaammaarrdd pprroodduucctt
HHaaddaammaarrdd pprroodduucctt L1 loss Imputed value Left Multiplication
g g MM aa tt rr ii xx LR eig fth Mt M ulu til pti lp yly
High Order GCN layer
noitce IM mi pss uin teg
d
v va alu lue
e
Hadamard product L2 loss O Fub nje cc tt ii ov ne 1 - = X out=Xˆ phy (~P mask)
Ut−M:t+Pˆt−M:t
D
e
n
o Spatial Attention
nisi
g
Latent Space
Hadamard product
……
TTTiiimmmeee TTiimmee
……..
TTiimmee ………...... TTrraaiinneedd
…….. GGNNNN ooouuutttpppuuuttt111
T0
Observed Values
N N
ssuunniimm
o o
gnisi
……TTT ……iiimmm ....eee ……
Data MPr Lep Process
gnisi
Lm t
……
L0 t
XX vvaalliidd
H0
… H…
noc
laudiseR
Spatial Attention Spatial Attention
…….... TT GGrraa NNiinn NNeedd ooouuutttpppuuuttt
Attention
Matrix Right Multiply
High
OT ri dm ee
r
C Go Cn Nv
layer
sc
mis ry ofh
nP
-i
Regu Ll 2a
++
r loiz sa stion
FuL no cs ts
io n
n e
e d
w Lm TTTiiimmmeee TTTiiimmmeee oitc Matrix Left Multiply High Order GCN layer
Dynamic GCN Layer ………….... n …..
Dynamic
Spatial Attention
Inhomogeneous PDE
Dynamic GCN Layer
Dynamic
graph Physics-incorporated
Imputation
Dynamic Laplacian matrix TTTiiimmmeee LSTM+Temporal Supervised UnSupervise
…….. L Attention TTiimmee learning learning
t ……..
TTiimmee
Valid value …….. Conditional Unconditional
Right Multiplication probability probability
Missing value Supervise
HHaaddaammaarrdd pprroodduucctt L1 loss Imputed value Left Multiplication learning
Generative Discriminative
model model
TTiimmee
……..
Valid value
Missing value
Supervise Imputed value
X Xˆt t+ +T
T
learning

D D SSaammpplleerr FFrroomm
e e
n n
o …… o
nisi nisi
S
g g at
n
d
a
r
d
Sampler G
a
u
aiss
N …… N n
nisio nisio
Learning Stage
Learning Stage
g …… g
Continuous hidden States
Latent Space
D D …… Probabilistic
ne ne Decoder
o …… o
Attention nisi nisi
S
Sample From
g g at
dn Training Stage
dra p(X0)=N(B0Z0,R02)
Sampler G Observed Values
a D D
gnisioN …… …… gnisioN
naissu
Continuous
hidd
en
F So tr aw tea srd algorithm
p(X0co|Z0)
Latent Space
Forward algorithm gnisione
Condition
o… n…
gnisione
dnatS
Continuous hidden States Latent Space a
r
…… …… Sampler aG d
u
SSaammppllee FFrroomm
aiss
SSaammppllee FFrroomm Sampling Stage N …… N n
o o
Sampler Observed Values
 gnisi
……
gnisi
Sampler Observed Values q =( NZ0 ((| BX 0T0c Bo)
0)−1B0TX0co,[(B0TB0)−1B0TR0]2I) Pr Eo nb ca ob dil eis rtic Pr Dob eca ob dil eis rtic
q q( (ZX t0T |A Z| tZ −10))=N(B0Z0,R02)
neD neD Z0~N((B0TB0)−1B0TX0,[(B0TB0)−1B0TR0]2) SSaammppllee FFrroomm
0~N(0,I) p(Z0|X0Ta) q(Z1|Z0) gnisio …… gnisio
Sampler SSaammppllee FFrroomm
Sampler
Forward algorithm
Sampler
Latent Space natS
q(X 0co|Z 0) Continuous hidden States Latent Space drad
…… …… G
a
u
q(Z T|Z T−1) aiss
n
N …… N
o o
Observed Values nisi nisi
g …… g
XX TTaa00
XX 00
XX TTaa00
X
0
TaT
L t
X =X −Xt−T' t−T' t−T' Pmask
TaX0
Z1
TaX1
Z0:T Z ~N(0,1)0
ZT
TaXT
q

(X Tat |Xq
P
L
Z1
X1
Ta)t−1Ta(Z
|ZtTa(Z
)0
t
Ta )t−1
X 0
w H 0
q ( X
X 1
Ta | Xt−1
? q (
ZT
Ta )t
TaZ t−1 | Z
w
Tat
H
)
w
Z T
XX11
XXTT−−11
XT
mH
H m
 T 0
T −1 T T −1 T 2p(Z )=N((B B ) B X ,[(B B ) B R ]I)0 0 0 0 0 0 0 0 0
Tap (Z |X )0 0
~0 N (0,I)
Pmask
pp((XX00
||ZZ00)) TTaa Z0

~~00 NN ((00,,II))

Z
0
= (B
0T
T −1B ) B0
0
T0 X
0
+ (B TB0 −1) B0 TR0
0
T 0
ˆXphy ~P +Xmask valid Pmask
X_coast
X = Xvalid
X (1−M )t−M:t t−M:t
imputation
Pmask

L
t
ˆXt:t+M
ˆX
phy
~ P
m
mL t
+ Xask
X t:t+M

valid
L t
w
P
m
MH
mL
ask
L 0t
X =out
X valid
H
ˆX ~ P + Xphy mask
XX vv aa lliidd
0H
ww HH
0
w
valid
H
X
w
0L
t
ww 00HH
H
H
2
mH
U +Pt−M:t t−M:t
L
t
ww mmLL
L mt
T 0where ψ represents the learnable parameters of the CNN mentioned above, the objective function of Eq. 1 can be
and Transformer. Assuming that the prior latent distribution transformed as:
follows a standard Gaussian distribution, p(Z 0) = N(0,I), L=L 1+L
2
=E ϵ∼N(0,I)(cid:2) ∥Xc 0o−f ψ(u ϕ+Σ ϕ·ϵ)∥ 2⊙M
w teh rmer ,e thI ei os bjt eh ce tivId een futi nty ctiom nat cri ax
n
a bn ed reig fon ro mri un lg atet dhe usc io nn gsta thn et + (cid:13) (cid:13)ϵ−ϵ θ(X¯t ta |Xc 0o,X¯t 0a,t)(cid:13) (cid:13) 2⊙(1−M)(cid:3)
1
reparameterization trick as follows: − (tr(Σ )+uTu logdet(Σ ))
2 ϕ ϕ ϕ ϕ
L =E ||Xco−f (u +Σ ∗ϵ)|| ⊙M (11)
1 ϵ∼N(0,I) 0 ψ ϕ ϕ 2 The details of the LSSDM algorithm are shown in Algorithm
1 (6)
− (tr(Σ )+uTu logdet(Σ )), 1. After training, we obtain the parameter λ = {ϕ,ψ,θ}.
2 ϕ ϕ ϕ ϕ
Through the learned parameters, we can calculate more ac-
Where ϵ is the standard Gaussian noise and tr is the matrix curate missing values by sampling from the noise, and details
tracewhiledetisthedeterminantofthematrix.Consequently, of the total imputation algorithm are shown in Algorithm 2.
the reconstructed values X¯ta, generated by the VGAE model,
Algorithm 1 Training of LSSDM
will serve as input for the diffusion model in the second term
of Eq. 1. The reconstructed values X¯ta are used to calculate Input: Time series values Xco, mask M and Adjacent
0
theprobabilityofp(Xta|Xco,X¯ta)withinthediffusionframe- matrix A
0 0 0
work. During the diffusion process, noise is added to X¯ta for Output: The model parameters λ={ϕ,ψ,θ}
0
T time step, which can be formulated as: Calculate the linear interpolation values X˜ta and Lapla-
0
cian matrix L;
T
q(X¯ta |Xco,X¯ta)=(cid:89) q(X¯ta|X¯ta ,Xco,X¯ta) Initialize variables λ;
1:T 0 0 t t−1 0 0 for each epoch do
t=1
and q(X¯ta|X¯ta ,Xco,X¯ta)=N(√ α X¯ta ,(1−α )I), Calculate the mean u ϕ and Σ ϕ variance by Eq. 4;
t t−1 0 0 t t−1 t Optimize L in Eq, 6 by gradient descent step;
(7) 1
Obtain the reconstructed values X¯ by Eq. 5;
where α evolves over time according to a fixed value. In the 0
t Add noise to X¯ta by Eq. 7;
denoising stage, the neural network p θ is adopted to learn the 0
Optimize L in Eq. 10 by taking gradient step on
noise and restore the signal from the standard Gaussian noise. 2
∇ (1−M)⊙||ϵ−ϵ (X¯ta|Xco,X¯ta,t))|| ;
Thus, as for the second term of Eq. 1, we can also use the θ θ t 0 0 2
end for
ELBO again as follows:
return λ
logp(Xt 0a|Xc 0o,X¯t 0a)≥E q(Xt 1a :T|Xc 0o,X¯t 0a)logp qθ (( XX t 1t 0 a :a T:T || XX c 0c 0 oo ,, X¯X¯ t 0t 0 aa )) Algorithm 2 Imputation of LSSDM
=E logp θ(Xt Ta|Xc 0o,X¯t 0a)(cid:81)T t=1p θ(Xt ta −1|Xt ta,Xc 0o,X¯t 0a) Input: Observed values Xc 0o, mask M, Adjacent matrix
q (cid:81)T q(Xta |Xta,Xco,X¯ta) A and λ=={ϕ,ψ,θ}
p (Xta|Xco,t= X¯1 ta)p t (− X1 ta|Xt ta,X0 co,X0 ¯ta) Output: The predicted missing values Xˆt 0a
=E log θ T 0 0 θ 0 1 0 0 Calculate the linear interpolation values X˜ta;
q q(Xt Ta|Xt ta,Xc 0o,X¯t 0a)
Input Xco and X˜ta to Eq. 4 to obtain u
a0
nd Σ ;
0 0 ϕ ϕ
+E
(cid:88)T logp θ(Xt ta −1|Xt ta,Xc 0o,X¯t 0a)
.
Sample noise ϵ and reparameterize by Z
0
=u ϕ+Σ ϕ∗ϵ;
q q(Xta |Xta,Xco,X¯ta)
t=2 t−1 t 0 0 (8) Obtain X¯t 0a =(1−M)⊙X¯ 0 by Eq. 5;
Our goal is to learn the conditional distribution p in the for t=T to 1 do
θ
reverse process. As described in DDPM [6], the reverse Calculate X¯t ta by Eq. 7;
process involves denoising Xt ta to recover Xt 0a, which is Sample Xˆt ta −1 using Eq. 9;
defined as: end for
return Xˆta
p (Xta |Xta,Xco,X¯ta) 0
θ t−1 t 0 0
=N(u (Xta|Xco,X¯ta,t),σ(Xta|Xco,X¯ta,t),
θ t 0 0 t 0 0
u θ(Xt ta|Xc 0o,X¯t 0a,t)= √1 αtXt ta− √ 1−1− α¯tα √ αtϵ θ(Xt ta|Xc 0o,X¯t 0a,t () 9,
) To testify
tI hI eI. pE erX foP rE mR aI nM cE eN oT fS oA uN rD mR oE dS eU l,L PT hS
ysioNet 2012
where α¯ =(cid:81)t α . From Eq. 7 and 9, with the parameteri- [5], AQI-36 [22], and PeMS-BAY [24] datasets are used.
t i=1 i
zation, the second term of the objective function in Eq. 1 can AQI-36 is a benchmark for the time series imputation from
be simplified to: the Urban Computing project in China. PM2.5 pollutant is
collected hourly from May 2014 to April 2015 with various
L =E (1−M)⊙||ϵ−ϵ (X¯ta|Xco,X¯ta,t))|| . missing patterns and with 24.6% originally missing rate. Four
2 ϵ∼N(0,I) θ t 0 0 2
(10) months (March, June, September, and December) are used
To represent the reverse process, we use the same neural as the test set without overlapping with the training dataset.
network framework as CSDI [23]. Through the deduction PhysioNet 2012 (P12) is a healthcare dataset from the ICUTABLE I: The imputation result of baselines.
that is 48 hours long with 4000 patients and 35 variables.
PeMS-BAY is a famous traffic flow time series dataset, which P12 PeMS-BAY
AQI-36
was collected with 325 road sensors in the San Francisco Bay Model @50% @Blockmissing
MAE CPRS MAE CPRS MAE CPRS
Area for nearly six months and re-sampled into 5 minutes Transformer[1] 12.00±0.60 — 0.297±0.002 — 1.70±0.02 —
GRIN[2] 12.08±0.47 — 0.403±0.003 — 1.14±0.01 —
by previous work [24]. For the P12 and PeMS-BAY datasets, SAITS[21] 18.13±0.35 — 0.296±0.002 — 1.56±0.01 —
M2DMTF[7] 14.61±0.49 — 0.7002±0.001 — 2.49±0.02 —
70%ofthetotaltimeseriesisusedfortraining,while10%for HSPGNN[4] 11.19±0.20 — 0.321±0.003 — 1.10±0.02 —
ImputeFormer[17] 11.58±0.20 — 0.480±0.150 — 0.95±0.02 —
validation and 20% for evaluation. For AQI-36, we used the
GP-VAE[19] 25.71±0.30 0.3377 0.511±0.007 0.7981 2.86±0.15 0.0436
same mask as [2]. As for the PeMS-BAY dataset, we adopt PriSTI[12] 9.35±0.22 0.1267 0.611±0.006 0.5514 0.97±0.02 0.0127
CSDI[23] 9.57±0.10 0.1053 0.660±0.002 0.5608 1.16±0.01 0.0135
blockmissingandpointsmissingmasksas[2]whileP12only LSSDM(ours) 8.89±0.23 0.0969 0.262±0.002 0.2890 0.89±0.02 0.0095
point missing as [23].
C. Uncertainty and latent state analysis
A. Experimental setting
Unlike traditional deterministic imputation, which provides
The epoch and batch size are 200 and 16, respectively. a single predicted value, probabilistic forecasts provide a
Residualconnectionisapplied,andthelearningrateis0.0001. distribution or range of possible outcomes along with their
The attention heads of the transformer adopt 8. For the associated probabilities. CRPS [10] measures how well the
diffusion stage, we adopt the same hyperparameters setting as predicted cumulative distribution function (CDF) matches the
CSDI [23]. As for the P12 dataset, we use the Identity matrix observed outcome. In this study, we adopt the same measure
as the Laplacian matrix since there is no pre-defined graph as CSDI [23]. We generate 100 samples to approximate the
in this dataset. Notably, to highly reproduce real application probabilitydistributionasintheprevioussection.Theresultis
scenarios, the simulated missing values are not allowed to showninTab.I.LSSDMoutperformstheprobabilisticbaseline
take part in the training stage and are only used as the final models, which better quantify the uncertainty accurately.
evaluation, which is the same training and evaluation protocol To explore the generative mechanism, 50% simulated miss-
as [2], [17] and different from [12], [23] since they allow ing and no simulated missing input on the P12 dataset into
it. Also, we only consider the out-of-sample scenarios, which the trained VAE model and compared with the latent space
meansthetrainingandevaluationseriesaredisjointsequences. distribution of LSSDM. The missing effect is tested, and
we average all dimensions of u and Σ to plot the final
ϕ ϕ
latent Gaussian distributions, which are shown in Fig 3. It
B. Result
demonstrates that the missing data can affect the latent space
largely and cause the deviation of the latent space, which can
The results of different datasets with different missing
not be neglected by the downstream task. However, LSSDM
patterns are shown in Tab. I. The result indicates our model
matches the no-simulated missing latent distribution better
achievesthebestimputationperformanceinthedifferentmiss-
and provides insight into the latent distribution of the dataset,
ing situations under different datasets of different domains,
which is quite beneficial for downstream tasks.
whichdemonstratesthegeneralizationandperformanceofour
model. Interestingly, reconstructing the missing values from 8
LSSDM latent distribution
the projective low-dimension latent space can greatly improve L wa itt he on ut td ii mst pri ub tu at tio ion n
Ground-truth latent distribution
the generative capability of the diffusion model, especially in 6
dealing with the originally missing data effectively by this
unsupervised learning framework without the demand of the 4
ground truth values. To investigate the performance of the
2
   
 * 5 , 1       6 $ , 7 6
   + 6 3 * 1 1  7 U D Q V I R U P H U
 , P S X W H ) R U P H U    / 6 6 ' 0
     3  / 6 U L 6  6 7  ' ,  0       + 6 3 * 1 1 0 -0.2 0 0.2
    Fig. 3: The latent distribution of different input on P12.
        
   
IV. CONCLUSION
        
  In this study, we propose LSSDM for MSTI. LSSDM
                                                                learns the latent space of the dataset by GCN, and the coarse
 P L V V L Q J  U D W H  P L V V L Q J  U D W H
reconstructedmissingvaluesareobtainedbyTransformerand
(a)PeMS-BAY. (b)P12.
CNN. Then, the accurate imputation values can be achieved
Fig. 2: MAE at different missing rates on different datasets. byinputtingthereconstructedvaluesintothediffusionmodel.
By leveraging this innovative framework, we can obtain high-
SOTAmodels,weplottheMAEofPeMS-BAYandP12under fidelity imputation with a better explanation of the imputation
differentmissingrates.TheresultsareshowninFig.2,which mechanism. Also, this model can utilize the unsupervised
indicates that our model obtains the best robustness. VGAE model to deal with the originally missing data without
 ( $ 0  ( $ 0the label, which demonstrates more feasibility and practical- REFERENCES
ity in a real application. Experimental results on real-world
datasets indicate that LSSDM achieves superior imputation [1] A.Vaswani,“Attentionisallyouneed,”Adv.NeuralInf.Process.Syst.,
performance and uncertainty analysis. NeuralInformationProcessingSystemsFoundation,vol.5999,2017.
[2] A. Cini, I. Marisca, and C. Alippi, “Filling the G ap s: Multivariate
time series imputation by graph neural networks,” in Proc. Int. Conf.
Learn.Representations.
[3] D.Pakiyarajah,E.Pavez,andA.Ortega,“Irregularity-awarebandlimited
approximationforgraphsignalinterpolation,”inProc.IEEEInt.Conf.
Acoust.,Speech,SignalProcess.(ICASSP),Apr.2024,pp.9801-9805.
[4] G.Liang,P.Tiwari,S.Nowaczyk,andS.Byttner,“Higher-orderspatio-
temporal physics-incorporated graph neural network for multivariate
time series imputation,” in Proc. 33rd ACM Int. Conf. Inf. Knowl.
Manag.,2024.
[5] I. Silva,G. Moody, D.J. Scott,L. A. Celi,and R. G.Mark, “Predict-
ing in-hospital mortality of ICU patients: The physionet/computing in
cardiologychallenge2012,”inProc.Comput.Cardiol.,IEEE,2012,pp.
245–248.
[6] J.Ho,A.Jain,andP.Abbeel,“Denoisingdiffusionprobabilisticmodels,”
Adv.NeuralInf.Process.Syst.,vol.33,pp.6840–6851,2020.
[7] J.Fan,“Multi-modedeepmatrixandtensorfactorization,”inProc.Int.
Conf.Learn.Representations,2021.
[8] J.L.AlcarazandN.Strodthoff,“Diffusion-basedtimeseriesimputation
andforecastingwithstructuredstatespacemodels,”Trans.Mach.Learn.
Res.
[9] J. Qiu, S. R. Jammalamadaka, and N. Ning, “Multivariate time series
analysis from a Bayesian machine learning perspective,” Ann. Math.
Artif.Intell.,vol.88,pp.1061–1082,2020.
[10] J. E. Matheson and R. L. Winkler, “Scoring rules for continuous
probabilitydistributions,”Manage.Sci.,vol.22,no.10,pp.1087–1096,
1976.
[11] M. Xu, A. Moreno, S. Nagesh, V. Aydemir, D. Wetter, S. Kumar,
and J. M. Rehg, “Pulseimpute: A novel benchmark task for pulsative
physiological signal imputation,” Adv. Neural Inf. Process. Syst., vol.
35,pp.26874–26888,2022.
[12] M. Liu, H. Huang, H. Feng, L. Sun, B. Du, and Y. Fu, “Pristi: A
conditionaldiffusionframeworkforspatiotemporalimputation,”inProc.
IEEE39thInt.Conf.DataEng.(ICDE),2023,pp.1927–1939.
[13] M.ChoiandC.Lee,“Conditionalinformationbottleneckapproachfor
timeseriesimputation,”inProc.12thInt.Conf.Learn.Representations,
2023.
[14] N. Abiri, B. Linse, P. Ede´n, and M. Ohlsson, “Establishing strong
imputationperformanceofadenoisingautoencoderinawiderangeof
missingdataproblems,”Neurocomputing,vol.365,pp.137–146,2019.
[15] N. Viswarupan, G. Cheung, F. Lan, and M. S. Brown, “Mixed graph
signal analysis of joint image denoising/interpolation,” in Proc. IEEE
Int. Conf. Acoust., Speech, Signal Process. (ICASSP), Apr. 2024, pp.
9431-9435.
[16] S.Ding,B.Xia,J.Sui,andD.Bu,“Accurateinterpolationofscattered
data via learning relation graph,” in Proc. IEEE Int. Conf. Acoust.,
Speech,SignalProcess.(ICASSP),Apr.2024,pp.7290-7294.
[17] T. Nie, G. Qin, W. Ma, Y. Mei, and J. Sun, “ImputeFormer: Low
rankness-induced transformers for generalizable spatiotemporal impu-
tation,”inProc.30thACMSIGKDDConf.Knowl.Discov.DataMin.,
2024,pp.2260–2271.
[18] T.N.KipfandM.Welling,“Semi-supervisedclassificationwithgraph
convolutionalnetworks,”inProc.5thInt.Conf.Learn.Representations
(ICLR),2017.[Online].Available:https://arxiv.org/abs/1609.02907
[19] V. Fortuin, D. Baranchuk, G. Ra¨tsch, and S. Mandt, “Gp-vae: Deep
probabilistic time series imputation,” in Proc. Int. Conf. Artif. Intell.
Stat.,PMLR,2020,pp.1651–1661.
[20] W.Cao,D.Wang,J.Li,H.Zhou,L.Li,andY.Li,“Brits:Bidirectional
recurrent imputation for time series,” Adv. Neural Inf. Process. Syst.,
vol.31,2018.
[21] W.Du,D.Coˆte´,andY.Liu,“Saits:Self-attention-basedimputationfor
timeseries,”ExpertSyst.Appl.,vol.219,p.119619,2023.
[22] X.Yi,Y.Zheng,J.Zhang,andT.Li,“ST-MVL:Fillingmissingvalues
in geo-sensory time series data,” in Proc. 25th Int. Joint Conf. Artif.
Intell.,2016.
[23] Y.Tashiro,J.Song,Y.Song,andS.Ermon,“Csdi:Conditionalscore-
based diffusion models for probabilistic time series imputation,” Adv.
NeuralInf.Process.Syst.,vol.34,pp.24804–24816,2021.[24] Y. Li, R. Yu, C. Shahabi, and Y. Liu, “Diffusion convolutional
recurrent neural network: Data-driven traffic forecasting,” in Proc.
Int. Conf. Learn. Representations (ICLR), 2018. [Online]. Available:
https://github.com/liyaguang/DCRNN
[25] Z.Kong,W.Ping,J.Huang,K.Zhao,andB.Catanzaro,“DiffWave:A
versatilediffusionmodelforaudiosynthesis,”inProc.Int.Conf.Learn.
Representations(ICLR),2021.