EurographicsSymposiumonRendering2024 PublishedinCOMPUTERGRAPHICS
EGarcesandE.Haines forum
(GuestEditors) Volume43(2024),Number4
A Diffusion Approach to Radiance Field Relighting using
Multi-Illumination Synthesis
Y.Poirier-Ginter1,2 ,A.Gauthier1 ,J.Phillip3 ,J.-F.Lalonde2 andG.Drettakis1
1Inria,UniversitéCôted’Azur,France;2UniversitéLaval,Canada;3AdobeResearch,UnitedKingdom
Figure1: Ourmethodproducesrelightableradiancefieldsdirectlyfromsingle-illuminationmulti-viewdataset,byusingpriorsfromgenerative
dataintheplaceofanactualmulti-illuminationcapture.
Abstract
Relighting radiance fields is severely underconstrained for multi-view data, which is most often captured under a single
illumination condition; It is especially hard for full scenes containing multiple objects. We introduce a method to create
relightableradiancefieldsusingsuchsingle-illuminationdatabyexploitingpriorsextractedfrom2Dimagediffusionmodels.
Wefirstfine-tunea2Ddiffusionmodelonamulti-illuminationdatasetconditionedbylightdirection,allowingustoaugmenta
single-illuminationcaptureintoarealistic–butpossiblyinconsistent–multi-illuminationdatasetfromdirectlydefinedlight
directions.Weusethisaugmenteddatatocreatearelightableradiancefieldrepresentedby3DGaussiansplats.Toallowdirect
controloflightdirectionforlow-frequencylighting,werepresentappearancewithamulti-layerperceptronparameterizedon
lightdirection.Toenforcemulti-viewconsistencyandovercomeinaccuraciesweoptimizeaper-imageauxiliaryfeaturevector.
Weshowresultsonsyntheticandrealmulti-viewdataundersingleillumination,demonstratingthatourmethodsuccessfully
exploits2Ddiffusionmodelpriorstoallowrealistic3Drelightingforcompletescenes.
Keywords:NeRF,RadianceField,Relighting
1. Introduction Oneapproachtoovercomethisdifficultyistocaptureamulti-
illuminationdatasetwhichbetterconditionstheinverseproblem
butcomesatthecostofaheavycapturesetup[DHT∗00].Another
Radiancefieldshaverecentlyrevolutionized3Dscenecapturefrom
images[MST∗20].Suchcapturestypicallyinvolveamulti-viewset optionistousepriors,whichistypicallydonebytraininganeu-
ralnetworkonsyntheticdatatopredictintrinsicpropertiesorrelit
ofphotographstakenunderthesamelightingconditions.Relighting
images.However,creatingsufficientlylarge,variedandphotoreal-
suchradiancefieldsishardsincelightingandmaterialproperties
istic3Dscenesisbothchallengingandtime-consuming.Assuch,
areentangled(e.g.,isthisashadoworsimplyadarkercolor?)and
methodsrelyingonthese—orsimpler—priorsoftendemonstrate
theinverseproblemill-posed.
Authorsversion
4202
peS
31
]VC.sc[
1v74980.9042:viXra2of14 Y.Poirier-Ginteretal./ADiffusionApproachtoRadianceFieldRelighting
resultsonisolatedmaskedobjects[BBJ∗21],ormakesimplifyingas- 2.1. RadianceFields
sumptionssuchasdistantenvironmentlighting[BJB∗21,ZSD∗21].
Radiancefieldmethodshaverevolutionized3Dscenecaptureusing
Othermethodshavehandledmorecomplexilluminationmodels,
multi-viewdatasets(photosorvideo)asinput.Inparticular,Neural
includingfullscenes[PMGD21,PGZ∗19],butcanbelimitedin
RadianceFields(NeRFs)[MST∗20]learntosynthesizenovelviews
thecomplexityofthegeometryandmaterialsthatmustreconstruct
of a given scene by regressing its radiance from a set of input
well.Finally,methodsthatdependonaccurateestimatesofsurface
images(multiplephotosorvideosofa3Dscene).Structurefrom
normals[JLX∗23,GGL∗23]oftenproducelimitedlevelsofrealism
motion[Ull79,SF16]isusedtoestimatethecameraposesforall
whenrelighting.
imagesandraysarecastthroughthecenterofallpixels.Amulti-
Attheotherendofthespectrum,diffusionmodels(DMs,e.g., layerperceptron(MLP)c θparameterizedby3Dpositionandview
[RBL∗22]),trainedonbillionsofnaturalimages,haveshownex- directionisusedtorepresenttheradianceandopacityofthescene.
ceptionalabilitiestocapturerealimagedistributionpriorsandcan Theoptimizationobjectiveissimplythemeansquarederror:
s thy en yth ce as niz be ec co om ntp role llx edlig inht vi an rg ioe uf sfe wc ats y. sW [Zh Ril Ae 2re 3c ],e en xt tp rar co tg inre gs ls igs hh to inw gs
-
L NeRF=E o,d,c∗(cid:104) ||c θ(o,d)−c∗||2 2(cid:105) ,
specificpriorsfromthesemodels,especiallyforfull3Dscenes,has where o is a ray’s origin, d its direction, and c∗ the target RGB
notyetbeendemonstrated.
color value of its corresponding pixel. The predicted color for
thatpixelisobtainedbyintegratingacolorfieldc weightedbya
Inthispaper,webuildontheseobservationsandpresentanew θ
densityfieldσ followingtheequationofvolumerendering.The
methodthatdemonstratesthatitispossibletocreaterelightableradi- θ
original NeRF was slow to train and to render; A vast number
ancefieldsforcompletescenesfromsinglelow-frequencylighting
of methods [TTM∗22] have been proposed to improve the orig-
conditioncapturesbyexploiting2Ddiffusionmodelpriors.Wefirst
inaltechnique,e.g.,accelerationstructures[MESK22],antialias-
proposetofine-tuneapre-trainedDMconditionedonthedominant
ing [BMT∗21], handling larger scenes [BMV∗22] etc. Recently,
light source direction. For this, we leverage a dataset of images
3DGaussianSplatting(3DGS)[KKLD23]introducesanexplicit,
withmanylightingconditionsofthesamescene[MGAD19],which
primitive-basedrepresentationofradiancefields.Theanisotropic
enablestheDMtoproducerelitversionsofanimagewithexplicit
natureofthe3DGaussiansallowstheefficientrepresentationoffine
controloverthedominantlightingdirection.Weusethis2Drelight-
detail,andthefastGPU-acceleratedrasterizationusedallowsreal-
ingnetworktoaugmentanystandardmulti-viewdatasettakenunder
timerendering.Weuse3DGStorepresentradiancefieldsmainlyfor
singlelightingbygeneratingmultiplerelitversionsofeachimage,
performance,butanyotherradiancerepresentation,e.g.,[CXG∗22],
effectivelytransformingitintoamulti-illuminationdataset.Given
couldbeusedinstead.Radiancefieldsaremostcommonlyusedin
thisaugmenteddataset,wetrainanewrelightableradiancefield
thecontextofsingle-lightconditioncaptures,i.e.,theimagesareall
withdirectcontrolonlightingdirection,whichinturnenablesreal-
capturedunderthesamelighting.Asaresult,thereisnodirectway
isticinteractiverelightingoffullsceneswithlightingandcamera
tochangethelightingofcapturedscenes,severelyrestrictingthe
viewcontrolinrealtimeforlow-frequencylighting.Webuildon3D
utilityofradiancefieldscomparedtotraditional3Dgraphicsassets.
GaussianSplatting[KKLD23],enhancingtheradiancefieldwith
Ourmethodusesdiffusionmodelstosimulatemulti-lightconditions
asmallMulti-LayerPerceptronandanauxiliaryfeaturevectorto
fromasingle-lightcapturethusallowingtherelightingofradiance
accountfortheapproximatenatureofthegeneratedlightingsandto
fields.
handlelightinginconsistenciesbetweenviews.
Insummary,ourcontributionsare:
2.2. SingleImageRelighting
• Anew2Drelightingneuralnetworkwithdirectcontrolonlighting Singleimagerelightingapproacheshavemostlybeenrestrictedto
direction,createdbyfine-tuningaDMwithmulti-lightingdata. humanfaces,withthemostrecentmethodsusinggenerativepriors
• Amethodtoaugmentsingle-lightingmulti-viewcapturetoanap- [WZL∗08,SYH∗17,SKCJ18,FRV∗23,PTS23,PLMZ23].Recently,
proximatemulti-lightingdataset,byexploitingthe2Drelighting humanbodyrelightinghasalsobeenstudied [KE19,LSY∗21]as
network. theirstructurealsoallowsforthedesignofeffectivepriors.Because
• Aninteractiverelightableradiancefieldthatprovidesdirectcon- theyaremuchlessconstrained,relightinggenericscenesfromasin-
trolonlightingdirection,andcorrectsforinconsistenciesinthe gleimageisamuchharderproblemthathaseludedresearchersuntil
neuralrelighting. recently.Whilesomeapproacheshavefocusedonspecificrelighting
effectssuchasshadows[LLZ∗20,SLZ∗22,SZB21,VZG∗23],they
Wedemonstrateoursolutiononsyntheticandrealindoorscenes,
areapplicablesolelyforthepurposeofobjectcompositing.Full
showingthatitprovidesrealisticrelightingofmulti-viewdatasets
scenerelightinghasbeenexploredbyMurmannetal.[MGAD19],
capturedunderasinglelightingconditioninrealtime.
who present a dataset of real indoor scenes lit by multiple light-
ing directions. They show that training a U-net on their dataset
allows for full scene relighting—in this work, we also leverage
their dataset but train a more powerful ControlNet [ZRA23] for
2. RelatedWork
therelightingtask.Otherworksinclude[TÇE∗21]whofocuson
Ourmethodproposesarelightableradiancefield.Wereviewwork sky relighting and [LSB∗22,ZLZ∗22] which leverage image-to-
onradiancefieldsandtheirrelightablevariants,anddiscussdiffusion imagetranslation.Methodsforoutdoorsceneshavealsobeenpro-
modelsandfine-tuningmethodswebuildon. posed[YME∗20,LGZ∗20,YS22].Ofnote,SIMBAR[ZTS∗22]and
AuthorsversionY.Poirier-Ginteretal./ADiffusionApproachtoRadianceFieldRelighting 3of14
OutCast[GRP22]producesrealistic,user-controllable,hard,cast 3Dassets(meshesandSVBRDFs)andapplyingphysically-based
shadowsfromthesun.Incontrast,wefocusonindoorsceneswhich renderingalgorithms.IBL-NeRF[CKK23]allowsforscene-scale
oftenexhibitsoftshadowsandmorecomplexlightingeffects.Fi- materialestimationbutbakestheilluminationintoaprefilteredlight-
nally,theconcurrentworkofZengetal.[ZDP∗24]usesdiffusion fieldwhichpreventsrelighting.Recently,NeRF-OSR[RES∗22],
modelstorelightisolatedobjectsusingenvironmentmaps.Incon- I2-SDF[ZHY∗23],andWangetal.[WSG∗23]focusedonscene
trasttothesesolutions,wefocusonclutteredindoorsceneswhich scale,singleilluminationrelightingscenesusingbothimplicitand
oftenexhibitsoftshadowsandmorecomplexlightingeffects. explicitrepresentations.Whiletheycanachievereasonableresults,
theyoftenlackoverallrealism,exhibitingbumpyoroverlysmooth
shadingduringrelighting.Incontrast,ouruseofdiffusionpriors
2.3. Multi-viewRelighting
providesrealistic-lookingoutput.
While single-view methods produce good results on restricted
datasets such as faces, they are often limited by the lack of ac-
curategeometry,requiredtosimulatelighttransport.Tothispoint,
multi-viewdatacanprovideamoreaccurateandcompletegeometric 2.4. DiffusionModels
reconstruction.ForexamplePhilipetal.[PMGD21,PGZ∗19]build
Diffusion Models (DMs) [SDWMG15,HJA20] made it possible
onmulti-viewstereo(MVS)reconstructionofthescene,andlearn
totraingenerativemodelsondiverse,high-resolutiondatasetsof
apriorfromsyntheticdatarenderedundermultiplelightingcondi-
billionsofimages.Thesemodelslearntoinvertaforwarddiffusion
tions.Despitecorrectingformanyofthereconstructionartifacts,
processthatgraduallytransformsimagesintoisotropicGaussian
thesemethodsarerestrictedbythequalityofMVSreconstruction.
noise,byaddingrandomGaussiannoiseϵt ∼N(0,I)toanimage
Nimieretal.[NDDJK21]alsopresentascene-scalesolutionbut
inT steps.DMstrainaneuralnetworkg withparametersφtolearn
requireacomplexpipelinethatoptimizesintexturespace.Gaoet φ
todenoisewiththeobjective:
al.[GCD∗20]usearoughproxyandneuraltexturestoallowobject
relighting. (cid:104) (cid:105)
L Diffusion=E
x,ϵ,1≤t≤T
||g φ(xt|t)−y t||2
2
,
Morerecentlyradiancefieldshavealsobeenusedasageometric
representation for relighting. Most methods work on the simple in which target y is often set to ϵ. After training, sampling can
t
caseofasingleisolatedobjectwhilewetargetlargerscenes.Such be performed step-by-step, by predicting x t−1 from xt for each
methodstypicallyassumelightingtobedistant,oftenprovidedas timestep t which is expensive since T can be high (e.g., 1000);
anenvironmentmap.NeRFactor[ZSD∗21]usesaBi-Directional
fasteralternativesincludedeterministicDDIM[SME20]sampling,
Reflectance Distribution Function (BRDF) prior from measure- thatcanperformsamplingofcomparablequalitywithfewersteps
mentsandestimatesnormalsandvisibility,whileNERV[SDZ∗21] (i.e., 10-50× larger steps). Stable Diffusion [RBL∗22] performs
andZhangetal.[ZSH∗22]predictsvisibilitytoallowindirectil-
denoisinginalower-dimensionallatentspace,byfirsttraininga
luminationestimation.NERD[BBJ∗21],PhySG[ZLW∗21],and
variational encoder to compress images; for instance, in Stable
DE-NeRF[WSLG23]useefficientphysically-basedmaterialand DiffusionXL[PEL∗23],imagesaremappedtoalatentspaceof
lightingmodelstodecomposearadiancefieldintospatiallyvary- sizeR128×128×4.Inapre-pass,thedatasetiscompressedusingthis
ing BRDFs, while Neural-PIL [BJB∗21] learns the illumination autoencoder,andatext-conditioneddiffusionmodelisthentrained
integrationandlow-dimensionalBRDFpriorsusingauto-encoders. directlyinthislatentspace.
TensorIR[JLX∗23]usesamixedradianceandphysics-basedfor-
mulationtorecoverintrinsicproperties.NeRO[LWL∗23]focuses Diffusionmodelshaveanimpressivecapacitytosynthesizehighly
on specular objects showing very promising results. Relightable realisticimages,typicallyconditionedontextprompts.Thepower
Gaussians[GGL∗23]usethemorerecent3DGaussianrepresen- ofDMsliesinthefactthatthebillionsofimagesusedfortraining
tationalongwithray-tracingtoestimatepropertiesofobjects.GS- containanextremelyrichrepresentationofthevisualworld.How-
IR[LZF∗24],GaussianShader[JTL∗24]andGIR[SWW∗23]also ever,extractingtherequiredinformationforspecifictasks,without
buildon3DGaussiansplatting,proposingdifferentapproachesto incurringthe(unrealistic)costofretrainingDMsisnotstraightfor-
estimatemorereliablenormalswhileapproximatingvisibilityand ward.Asetofrecentmethodsshowthatitispossibletofine-tune
indirectillumination;theseworkwellforisolatedobjectsunderdis- DMs with a typically much shorter training process to perform
tantlighting.However,thesemethodsstrugglewithmorecomplex
specifictasks(e.g.,[GAA∗23,RLJ∗23]).AnotableexampleisCon-
scene-scaleinputandnear-fieldilluminationbutcanworkorbe trolNet[ZRA23]whichproposedanefficientmethodforfine-tuning
adaptedtobothsingleandmulti-illuminationinputdata. StableDiffusionwithaddedconditioning.Inparticular,theydemon-
stratedconditionalgenerationfromdepth,Cannyedges,etc.,with
Feedingmulti-viewmulti-illuminationdatatoarelightableradi-
andwithouttextprompts;Wewillbuildonthissolutionforour2D
ancefieldindeedenablesbetterrelightingbutatthecostofhighly-
relightingmethod.
controlledcaptureconditions[XZC∗23,ZCD∗23,TDMS∗23]oran
extendeddatasetofunconstrainedilluminations[BEK∗22,LGF∗22]. Inasimilarspirit,therehasbeensignificantevidenceinrecent
In our method, we use a Diffusion Model to simulate multi- yearsthatlatentspacesofgenerativemodelsencodematerialin-
illuminationdata,liftingthecaptureconstraintswhilebenefiting formation[BMHF23,BF24].Recentworkshowsthepotentialto
from the lighting variations. Another body of work [MHS∗22, fine-tuneDMstoallowdirectmaterialediting [SJL∗23].Nonethe-
HHM22,YZL∗22,ZYL∗23,LWC∗23,LCL∗23]achieveobjector less,weareunawareofpublishedmethodsthatuseDMfine-tuning
scenerelightingfrommulti-viewimagesbyextractingtraditional toperformrealisticrelightingoffullandclutteredscenes.
Authorsversion4of14 Y.Poirier-Ginteretal./ADiffusionApproachtoRadianceFieldRelighting
Figure2:Weusethesingle-view,multi-illuminationdatasetofMurmannetal.[MGAD19]totrainControlNet[ZRA23]onsingleview
supervisedrelighting.Thenetworkacceptsanimage(alongwithitsestimateddepthmap)andatargetlightdirectionasinputandproducesa
relitversionofthesamesceneunderthedesiredtargetlighting.
3. Method
Ourmethodiscomposedofthreemainparts.First,wecreatea2D
relightingneuralnetworkwithdirectcontroloflightingdirection
(Sec.3.1).Second,weusethisnetworktoaugmentamulti-view
capturewithsinglelightingintoamulti-lightingdataset,byusing
ourrelightingnetwork.Theresultingdatasetcanbeusedtocreate
aradiancefieldrepresentationofthe3Dscene(Sec.3.2).Finally,
wecreatearelightableradiancefieldthataccountsforinaccuracies
in the synthesized relit input images and provides a multi-view
consistentlightingsolution(Sec.3.3). Figure3:Toprow:fivediffusesphererenderedbyouroptimized
lighting direction and shading parameters — thedirection isin-
dicatedbyabluedotatthepointofmaximumspecularintensity;
3.1. Single-ViewRelightingwith2DDiffusionPriors Bottom row: the corresponding target gray spheres obtained by
Relighting a scene captured under a single lighting condition is averagingthediffusespherescapturedinallspheres.Wefoundthe
severelyunderconstrained,giventhelighting/materialambiguity, lightingdirectionsbyminimizingtheL 1distancebetweenthetop
andthusrequirespriorsabouthowappearancechangeswithillu- andbottomrow.
mination.Arguably,largeDMsmustinternallyencodesuchpriors
sincetheycangeneraterealisticcomplexlightingeffects,butexist-
ingarchitecturesdonotallowforexplicitcontroloverlighting. whichbestreproducesthistargetwhenrenderingagrayballwith
asimplisticPhongshadingmodel.Morespecifically,weminimize
We propose to provide explicit control over lighting by fine-
tuningapre-trainedStableDiffusion(SD)[RBL∗22]modelusing theL 1errorwhenjointlyoptimizingforanambientlighttermand
shadingparameters(albedo,specularintensityandhardness,aswell
ControlNet[ZRA23]onamulti-illuminationdataset.Asillustrated
asaFresnelcoefficient).Fig.3illustratesthisprocess.
inFig.2,theControlNetacceptsasinputanimageaswellasa
targetlightdirection,andproducesarelitversionofthesamescene
3.1.2. ControllingRelightingDiffusion
underthedesiredlighting.TotraintheControlNet,weleveragethe
datasetofMurmannetal.[MGAD19],whichcontainsN=1015 WetrainControlNettopredictrelitversionsoftheinputimageby
realindoorscenescapturedfromasingleviewpoint,eachlitunder conditioningitonatargetlightingdirection.LetusdenoteasetX
M=25different,controlledlightingdirections.Weonlykeepthe ofimagesofagivensceneinthemulti-lightdatasetofMurmann
18non-frontfacinglightdirections. etal.[MGAD19],whereeachimageX ∈X hasassociatedlight
k
directionl .Ourapproach,illustratedinFig.2,trainsonpairsof
k
3.1.1. LightingDirection lightingdirectionsofthesamescene(includingtheidentitypair).
Thedenoisingobjectivebecomes
Tocapturethescenesusingsimilarlightdirections,Murmannetal.
(cid:104) (cid:105)
reliedonacamera-mounteddirectionalflashcontrolledbyaservo L 2D=E
ϵ,X,t,i,j
||g ψ(Xt,i;t,Xj,Dj,l i)−y t||2
2
, (1)
motor.Apairofdiffuseandmetallicspheresarealsovisibleineach
scene;weleveragetheformertoobtaintheeffectivelightingdirec- where Xt,i is the noisy image at timestept ∈[1,T], where i,j∈
tions.Usingastargettheaverageofalldiffusespheresproduced [1,M],andwhereψaretheControlNetoptimizableparametersonly.
bythesameflashdirection,wefindthelightingdirectionl∈R3 XjisanotherimagefromthesetandDjisitsdepthmap(obtained
AuthorsversionY.Poirier-Ginteretal./ADiffusionApproachtoRadianceFieldRelighting 5of14
withtheapproachofKeetal.[KOH∗24])—botharegivenasinput
totheControlNetsubnetwork.Inshort,thenetworkistrainedto
denoiseinputimageXigivenitslightdirectionl iwhileconditioned
ontheimageXj correspondingtoanotherlightingdirectionl j of
thesamescene.Here,wedonotusetextconditioning:theempty
textstringisprovidedtothenetwork.
Specifically, the light direction l i is encoded using the first 4
bandsofsphericalharmonics,followingthemethodofMülleret
Figure5:Importanceofconservingedgesharpnesswhenrelighting.
al.[MESK22].Theresultingvectorisaddedtothetimestepem-
Left:inputimage.Middle:naiveControlNetrelighting;notehow
beddingpriortofeedingittothelayersofControlNet’strainable
theedgesdonotmatchtheinputandhowthetextisillegible.Right:
copy.
ourfinalrelightingafterfine-tuningtheconditonaldecodernetwork
from[ZFC∗23].
3.1.3. ImprovingtheDiffusionQuality
SinceControlNetwasnotspecificallydesignedforrelighting,adapt-
ingitnaivelyasdescribedaboveleadstoinaccuratecolorsanda
alearningrateof10−4 for20kstepswhentrainingatresolution
lossincontrast(seeFig.4),aswellasdistortededges(seeFig.5).
768×512and50kstepsatresolution1536×1024.Notethatthis
Theseerrorsalsodegrademulti-viewconsistency.
stepisindependentoftheControlNettraining.Fig.5showsthe
effectofthesechanges;notehowtheedgesarewobblyandthetext
isillegiblewithoutthem.
Examplerelightingresultsobtainedusingour2Drelightingnet-
workonimagesoutsideofthedatasetareshowninFig.6.Observe
howtherelitimagesproducedbyourmethodarehighlyrealistic
andlightdirectionsareconsistentlyreproducedacrossscenes.A
naivesolutionforradiancefieldrelightingwouldbetoapplythis2D
networktoeachsynthesizednovelview.However,theControlNet
Figure4:Importanceofpost-relightingcolorandcontrastadjust-
isnotmulti-viewconsistent,andsuchanaivesolutionresultsin
ments.Left:inputimage.Middle:naiveControlNetrelighting;the
significantflickering.Pleaseseetheaccompanyingvideoforaclear
bottlehasthewrongcolorandthecontrastispoor.Right:ourre-
illustration.
lightingaftertrainingwith[LLLY23]andaftercolor-matchingthe
input.
3.2. AugmentingMulti-View/Single-LightingDatasets
Weadopttwostrategiestoimprovecolorationandcontrast.First, Givenamulti-viewsetIofimagesofascenecapturedunderthe
wefollowtherecommendationsof[LLLY23]toimproveimage samelighting(suitablefortrainingaradiancefieldmodel),wenow
brightness—wefoundthemtoalsohelpforcolor.Inparticular,using leverageourlight-conditionedControlNetmodeltosynthetically
√ √
the“v-parameterized”objectiveyt = α¯t·ϵ− 1−α¯t·x,instead relighteachimageinI.Weassumethe3DposeofeachimageIi∈I
ofthemoreusualyt =ϵ,provedcritical;inthisequation,1−α¯t isknownapriori,forexampleviaColmap[SF16,SZPF16].We
givesthevarianceofthenoiseattimestept.Second,aftersampling, thensimplyrelighteachIi∈Itothecorresponding18knownlight
wecolor-matchpredictionstotheinputimagetocompensatefor directionsinthedatasetfromMurmannetal.[MGAD19](excluding
thedifferencebetweenthecolordistributionofthetrainingdata thedirectionswheretheflashpointsforward),(seeSec.3.1).We
andthatofthescene.Thisisdonebysubtractingtheper-channel nowhaveafullmulti-lighting,multi-viewdataset.Thisprocessis
mean and dividing by the standard deviation for the prediction, illustratedinFig.7.
thenaddingthemeanandstandarddeviationoftheinput,inthe
LABcolorspace.Thisiscomputedoverall18lightingconditions
3.3. TrainingaLighting-ConsistentRadianceField
together (i.e., the mean over all lighting directions) to conserve
relativebrightnessacrossallconditions.Fig.4showstheeffectof Giventhegeneratedmulti-light,multi-viewdataset,wenowdescribe
thesechanges;withoutthem,thebottleisblueinsteadofgreenand oursolutiontoprovidearelightableradiancefield.Inparticular,we
overallcontrastispoor. buildonthe3DGSframeworkofKerbletal.[KKLD23].Ourre-
quirementsaretwofold:first,defineanaugmentedradiancefield
Tocorrectthedistortededges,weadapttheasymmetricautoen-
thatcanrepresentlightingconditionsfromdifferentlightingdirec-
coderapproachofZhuetal.[ZFC∗23],whichconsistsincondition-
tions;second,allowdirectcontrolofthelightingdirectionusedfor
ingthelatentspacedecoderwiththe(masked)inputimageforthe
relighting.
inpaintingtask.Inourcase,weignorethemaskingandfine-tune
thedecoderonthemulti-illuminationdataset[MGAD19].Ateach Theoriginal3DGS[KKLD23]radiancefieldusessphericalhar-
fine-tuningstep,weencodeanimageandconditionthedecoder monics(SH)torepresentview-dependentillumination.Toencode
on an image from the same scene with another random lighting varyingillumination,wereplacetheSHcoefficientswitha3-layer
direction.Thedecoderisfine-tunedwiththeAdamoptimizerat MLPc ofwidth128whichtakesasinputthelightdirectionalong
θ
Authorsversion6of14 Y.Poirier-Ginteretal./ADiffusionApproachtoRadianceFieldRelighting
InputImage ControlNetRelightings
Figure6:Relightingresultswithourlight-conditionedControlNet.Fromasingleinputimage(leftcolumn),theControlNetcangenerate
realisticrelitversionsfordifferenttargetlightdirections(othercolumns).Pleasenoticerealisticchangesinhighlightsfordifferentlight
directions(toprow),aswellasthesynthesisofcastshadows(bottomrow).
Figure7:Givenamulti-view,single-illuminationdatasetweuseourrelightingControlNettogenerateamulti-view,multi-illuminationdataset.
with the viewing direction. Both vectors have a size of 16 after ancefieldslike3DGSrelyonmulti-viewconsistency,andbreaking
encoding. itintroducesadditionalfloatersandholesinsurfaces.
Sincelightdirectionsarecomputedwithrespecttoalocalcamera To allow the neural network to account for this inconsistency
referenceframe(c.f.Sec.3.1),wesubsequentlyregisterthemtothe andcorrectaccordingly,weoptimizeaper-imageauxiliarylatent
worldcoordinatesystem(obtainedfromColmap)byrotatingthem vectoraofsize128.Similarapproachesforvariableappearance
accordingtotheir(known)camerarotationparameters: havebeenusedforNeRFs[MBRS∗21].Therefore,inadditiontothe
l′=R il, (2) lightingdirectionl′,weconditiontheMLPwithper-viewauxiliary
parametersa:
whereR iisthe3×3camera-to-worldrotationmatrixofimageIi
fromitsknownpose. G
c(o,d)= ∑wgc θ(xg,d|av,l′), (3)
WeconditiontheMLPwiththesphericalharmonicsencodingof
g=1
thegloballyconsistentlightingdirectionl′,whichenablestraininga
3DGSrepresentationonourmulti-lightingdataset.Whilethisstrat- whereg∈[1,G]sumsovertheGgaussians(see[KKLD23]),xg/wg
egyworkswellforstaticimages,itresultsininconsistentlighting aretheirfeatures/weights,distheviewdirection,otherayorigin,
acrossviewsdespiteaccountingforcamerarotationinEq.2.Radi- and c is the predicted pixel color. Note that for novel views at
AuthorsversionY.Poirier-Ginteretal./ADiffusionApproachtoRadianceFieldRelighting 7of14
useStableDiffusion[RBL∗22]v2.1asabackbone.Oursourcecode
anddatasetswillbereleaseduponpublication.
Wefirstpresenttheresultsofour3Drelightableradiancefield,
bothforsyntheticandreal-worldscenes.Wethenpresentaquanti-
tativeandqualitativeevaluationofourmethodbycomparingitto
previousworkandfinallypresentanablationoftheauxiliaryvector
afromSec.3.3.
4.1. TestDatasets
Sincetherearenorealmulti-viewmulti-illuminationindoordatasets
offullscenesavailableforourevaluation,weusesyntheticscenes
toallowquantitativeevaluation.Forthispurpose,wedesigned4
synthetictestscenes(KITCHEN,LIVINGROOM,OFFICE,BEDROOM).
TheywerecreatedinBlenderbydownloadingartist-made3Drooms
fromEvermotionandmodifyingthemtoincreaseclutter:ineach
room,wegatheredobjectsandplacedthemonatableoracountertop.
Wealsocreatedsimpler,diffuse-onlyversionstoevaluatehowscene
Figure8:Overviewofourradiancefieldtrainingscheme.Toalle-
clutteraffectstherelightingresults.Foreachsyntheticscene,wefirst
viatepotentialinconsistenciesinlightingdirections,wecondition
builtastandardmulti-view(single-lighting)datasetconsistingof4
our3DGS-basedradiancefieldbothontheilluminationdirection
camerasweeps(left-to-right,atvaryingelevations)of50framesfor
encodingandonoptimizedauxiliaryvectors(onepertrainingim-
trainingandone(atadifferentelevation)of100framesfortesting.
age).Thesevectorsmodelthedifferencesbetweenpredictionsand
Wesimulatedthelightdirectionofthe2Dtrainingdatasetwitha
letusfiteachviewtoconvergence.
spotlightwithintensityof2kWandradius0.1locatingontopofthe
cameraandpointingawayfromit.Weusedthesamesetofcamera
flashdirectionsasinthedatasetofMurmannetal.[MGAD19].We
inference, we use as latent vector the mean of all training view thenrenderallframesin736×512usingtheCyclespathtracer.
latentsi.e.E v[av]. Pleasenotethattheeffectivelightingdirectionwillbedependenton
theexactconfigurationoftheroom.Thisconfigurationisourbest
Wefirsttrain3DGSwiththeunlitimagesasa“warmup”stage
efforttoproduceagroundtruthusableforcomparison.
for5Kiterations,thentrainthefullmulti-illuminationsolutionfor
another 25K iterations, using all 18 back-facing light directions Inaddition,wealsocapturedasetofrealscenes(KETTLE,HOT
(seeSec.3.1).Themulti-illuminationnatureofthetrainingresults PLATES,PAINTGUN,CHESTOFDRAWERSandGARAGEWALL),
inanincreasein“floaters”.AsobservedbyPhilipandDeschain- forwhichweperformedastandardradiance-fieldmulti-viewcap-
tre[PD23],floatersareoftenpresentclosetotheinputcameras;the ture,bytakingbetween90–150imagesoftheenvironment,inan
explicitnatureof3DGSallowsustoreducetheseeffectively.In approximate sphere (or hemisphere) around the scene center of
particular,wecalculateaznearvalueforallcamerasbytakingthe interest.
zvalueofthe1stpercentileofnearestSfMpointsandscalingthis
valuedownby0.9.Duringtraining,ateachstep,allgaussianprimi-
4.2. 3DRelightingResults
tivesthatprojectwithintheviewfrustumofacamerabutarelocated
infrontofitsznearplaneareculled.Finally,giventhecomplexityof Webeginbyshowingqualitativeresultsonthesetofrealscenesthat
modelingvariablelighting,weobservedthattheoptimizationsome- wecaptured.Here,weusedaresolutionof1536×1024,training
timesconvergestoblurryresults.Tocounterthis,weoverweight for150Kiterations.Weshowqualitativeresultsforthesescenes
threefront-facingviews(left,right,andcenter),byoptimizingfor usingour3DrelightableradiancefieldinFig.9.Inaddition,wealso
oneoftheseviewseverythreeiterations.Thisprovidesmarginal showresultsfortwoscenesfromtheMipNeRF360dataset,namely
improvementinresults;allimagesshownarecomputedwiththis COUNTERandROOM.
method,butitisoptional.
AsourmethodislightweightandonlyaddsasmallMLPover
ThefullmethodforrelightableradiancefieldsisshowninFig.8. thecore3DGSarchitecture,itrunsinteractivelyforbothnovelview
Atinference,wecandirectlychoosealightingdirection,anduse synthesisandrelightingat30fpsonanA6000GPU.Memoryusage
efficient3DGSrenderingforinteractiveupdateswithmodifiedlight- iscomparabletotheoriginal3DGS.Pleaseseethevideoforinterac-
ing.Ourlatentvectorsandfloaterremovalremovemost,butnotall, tiverelightingresultsonthesescenesandadditionalsyntheticscenes.
artifactsintroducedbythemulti-viewinconsistencies;thiscanbe Weseethatourmethodproducesrealisticandplausiblerelighting
seenintheablationsattheendofthesupplementalvideo. results.Also,notethatoursolutionistemporallyconsistent.
4. ResultsandEvaluation 4.3. Evaluation
Ourmethodwasimplementedbyleveragingpubliclyavailableim- Baselines. Wecompareourresultstothemethodof[PMGD21]
plementationsofControlNet[ZRA23]and3DGS[KKLD23].We which is specifically designed for complete scenes, Ten-
Authorsversion8of14 Y.Poirier-Ginteretal./ADiffusionApproachtoRadianceFieldRelighting
Figure9: Qualitativerelightingresultsfortherealscenes,fromlefttoright:CHESTOFDRAWERS,KETTLE,MIPNERFROOMandGARAGE
WALL,foramovinglightsource.Thelightingdirectionisindicatedinthegrayballinthelowerright.Pleaseseethesupplementalvideofor
moreresults.Pleasenotehowthehighlights(leftgroup)andshadows(rightgroup)havechanged.
Figure10:Qualitativecomparisononrealscene KETTLE.Fromlefttoright,fromthesameviewpoint:inputlightingcondition(view
reconstructedusing3DGaussianSplatting),targetlighting,ourrelighting,Philipetal.[PMGD21]relighting.Topandbottomrowsare
twodifferentlightingconditions.Philipetal.[PMGD21]exhibitsmuchmoregeometryandshadingartifactscomparedtoourmethod;in
particularimpreciseMVSpreprocessingresultsinmissinggeometry.
Method→ Ours Outcast[GRP22] R3DGS[GGL∗23] TensoIR[JLX∗23]
Scene↓/Metrics PSNR↑ LPIPS↓ SSIM↑ PSNR↑ LPIPS↓ SSIM↑ PSNR↑ LPIPS↓ SSIM↑ PSNR↑ LPIPS↓ SSIM↑
SimpleBedroom 20.57 0.156 0.868 17.24 0.207 0.808 17.79 0.174 0.830 15.77 0.471 0.595
SimpleKitchen 17.45 0.154 0.855 17.91 0.205 0.822 18.55 0.197 0.807 20.52 0.382 0.701
SimpleLivingroom 22.12 0.136 0.884 21.09 0.125 0.878 20.34 0.166 0.857 17.45 0.444 0.598
SimpleOffice 18.59 0.131 0.868 18.97 0.196 0.811 20.40 0.173 0.808 18.22 0.446 0.644
ComplexBedroom 17.70 0.145 0.791 15.26 0.221 0.694 16.69 0.186 0.741 14.42 0.434 0.555
ComplexKitchen 19.28 0.152 0.811 18.44 0.178 0.771 19.28 0.168 0.755 16.70 0.471 0.533
ComplexLivingroom 18.61 0.163 0.800 17.94 0.187 0.783 18.39 0.175 0.770 16.82 0.382 0.602
ComplexOffice 20.20 0.096 0.858 17.22 0.169 0.781 18.93 0.144 0.776 15.78 0.468 0.529
Table1: Quantitativeresultsofour3Drelightingonthesyntheticdatasets(wheregroundtruthisavailable),comparedtopreviouswork,from
lefttoright:OutCast[GRP22](runonindividualimagesfrom3DGS[KKLD23]),Relightable3DGaussians[GGL∗23],andTensoIR[JLX∗23].
Arrowsindicatehigher/lower(↑/↓)isbetter.Resultsarecolorcodedby best, second- and third-best.
AuthorsversionY.Poirier-Ginteretal./ADiffusionApproachtoRadianceFieldRelighting 9of14
GT Ours Outcast R3DG TensoIR
Figure11: Weshowcomparativeresultsofourmethodofsyntheticsceneswherethe(approximate)groundtruthisavailable(left),and
comparetopreviousmethods.Ourapproachisclosertothegroundtruthlighting,capturingtheoverallappearanceinarealisticmanner.
Authorsversion10of14 Y.Poirier-Ginteretal./ADiffusionApproachtoRadianceFieldRelighting
soIR[JLX∗23]andRelightableGaussians[GGL∗23].Giventhat multi-illuminationdatasetandfine-tuningalargepretrainedgen-
mostothermethodsdonothandlefullsceneswell,wealsocreatea erative model. Our results show that we can synthesize realistic
newbaseline,byfirsttraining3DGS[KKLD23]ontheinputdata relightingofcapturedscenes,whileallowinginteractivenovel-view
andrenderatestpathusingnovelviewsynthesis;WethenuseOut- synthesisbybuildingonsuchpriors.Ourmethodshowslevelsof
Cast[GRP22]torelighteachindividualrenderedframeusingthe realismforrelightingthatsurpassthestateoftheartforcluttered
targetdirection.WetrainedTensoIR[JLX∗23]usingthedefault indoorscenes(asopposedtoisolatedobjects).
configurationbutmodifiedthe“density_shift”parameterfrom−10
to−8toachievebestresultsonourdata.ForRelightable3DGaus-
sians[GGL∗23],wetraintheir“Stage1”for30Kiterationsand
“Stage2”foranadditional10KtorecovertheBRDFparameters.
Wethenrelightthescenesusing360°environmentmapsrendered
inBlenderusingagenericemptyroomandasimilarcamera/flash
setupforgroundtruth.Finally,toimprovethebaselineswenormal-
izethepredictionsofallmethods;wefirstsubtractthechannel-wise
meananddivideoutthechannel-wisestandarddeviation,andthen
multiplyandaddthecorrespondingparametersofthegroundtruths.
TheseoperationsareperformedinLABspaceforallmethods.
Experimentalmethodology. Weuseoursynthetictestscenesfor
providingquantitativeresults.Tocompareourmethod,werendered
200novelviewswith18differentlightingdirectionstoevaluatethe
relightingqualityforeachmethodbycomputingstandardimage
qualitymetrics.Giventhecomplexityofsetupfor[PMGD21],we Figure12:Examplelimitationsofourapproach,withourprediction
onlyshowqualitativeresultsfor1realsceneinFig.10.Here,our (top)vsgroundtruth(bottom).OurControlNetmistakenlyproduces
methodwastrainedat768×512resolutionfor200kiterations,with a shadow at the top of the image while there should not be any
abatchsizeof8andalearningrateof10−4.
(redarrow),presumablyassumingthepresenceofanothertopshelf.
Additionally,thehighlightpositionissomewhatincorrect(yellow
Results. WepresentquantitativeresultsinTable1.Wepresentper- arrow),ostensiblybecausewedefinelightdirectioninamanner
sceneresultsonthefollowingimagequalitymetrics:PSNR,SSIM, thatisnotfullyphysicallyaccurate.
andLPIPS[ZIE∗18].Theresultsdemonstratethatourmethodout-
performsallothersinallbutafewscenarios,whereitstillachieves
One limitation of the proposed method is that it does not en-
competitiveperformance.
forcephysicalaccuracy:thetargetlightdirectionisnoisyandthe
QualitativecomparisonsareshowninFig.11;ontheleftweshow ControlNetreliesmostlyonitspowerfulStableDiffusionpriorto
thegroundtruthrelitimagerenderedinBlender,andwethenshow relightratherthanperformingphysics-basedreasoning.Forexam-
ourresults,aswellasthosefromOutcast[GRP22],Relightable3D ple, Fig. 12 shows that ControlNet can hallucinate shadows due
Gaussians[GGL∗23]andTensoIR[JLX∗23].Pleaserefertothe
tounseengeometry,whilethereshouldnotbeany.Giventhatwe
supplementaryHTMLviewerformoreresults.Weclearlyseethat definelightdirectioninamannerthatisnotfullyphysicallyaccu-
ourmethodisclosertothegroundtruth,visuallyconfirmingthe rate,thepositioningofhighlightcanbeinaccurate,asisalsoshown
quantitativeresultsinTab.1.TensoIRhasdifficultyreconstructing inFig.12.Inaddition,theappearanceembeddingscancorrectfor
thegeometry,andRelightable3DGaussianstendtohavea“splotchy” globalinconsistenciesindirectlyanddonotexplicitlyrelyonthe
look due to inaccurate normals. Outcast has difficulty with the learned3Drepresentationoftheradiancefield.Ourmethoddoes
overall lighting condition and can add incorrect shadows, but in notalwaysremoveormoveshadowsinafullyaccuratephysically-
manycasesproducesconvincingresultssinceitoperatesinimage basedmanner.Whileourmethodclearlydemonstratesthat2Ddiffu-
space.Ourresultsshowthatbyusingthediffusionpriorwemanage sionmodelpriorscanbeusedforrealisticrelighting,theabilityto
toachieverealisticrelighting,surpassingthestateoftheart. performmorecomplexrelighting—ratherthanjustchanginglight
Ourmethodwastrainedforindoorscenes;Fig.13givesaddi- direction—requiressignificantfutureresearch,e.g.,byusingmore
tionalControlNetresultsonout-of-distributionsamples,showing generaltrainingdataaswellaswaystoencodeanddecodecomplex
thatitcangeneralizetosomeextenttounseenscenesandlighting lighting.
conditions,althoughtherealismislowerthanforin-distribution Aninterestingdirectionforfutureworkwouldbetryingtoen-
samples. forcemulti-viewconsistencymoreexplicitlyinControlNet,e.g.by
leveragingsingle-illuminationmulti-viewdata.Anotherinteresting
5. Conclusion
direction is to develop solutions that would guide the predicted
Wehavepresentedthefirstmethodtoeffectivelyleveragethestrong relightingmakingitmoreaccurate,leveragingthe3Dgeometric
prior of large generative diffusion models in the context of radi- informationavailableintheradiancefieldmoreexplicitly.
ance field relighting. Rather than relying on accurate geometry,
materialand/orlightingestimation,ourapproachmodelsrealistic Acknowledgements This research was funded by the ERC Ad-
illuminationdirectly,byleveragingageneral-purposesingle-view, vanced grant FUNGRAPH No 788065 http://fungraph.
AuthorsversionY.Poirier-Ginteretal./ADiffusionApproachtoRadianceFieldRelighting 11of14
Figure13: Weshowtheresultsofour2Drelightingnetworkonout-of-distributionimages(StyleGAN-generatedwomanandMipNeRF360
BICYCLE,GARDEN,andSTUMP).Onhumanfaces,ControlNetmaychangetheexpressionaswellasthelighting,orcreateexcessiveshininess;
onoutdoorscenes,whiletheoveralllightingdirectionisplausible,thenetworkfailstogeneratesufficientlyhardshadows.
inria.fr/,supportedbyNSERCgrantDGPIN2020-04799and References
theDigitalResearchAllianceCanada.Theauthorsaregratefulto
[BBJ∗21] BOSSM.,BRAUNR.,JAMPANIV.,BARRONJ.T.,LIUC.,
AdobeandNVIDIAforgenerousdonations,andtheOPALinfras-
LENSCHH.:Nerd:Neuralreflectancedecompositionfromimagecollec-
tructurefromUniversitéCôted’Azur.ThankstoGeorgiosKopanas tions.InIEEE/CVFInt.Conf.Comput.Vis.(2021).2,3
andFrédéricFortier-Chouinardforhelpfuladvice.
[BEK∗22] BOSSM.,ENGELHARDTA.,KARA.,LIY.,SUND.,BAR-
RONJ.T.,LENSCHH.P.,JAMPANIV.:SAMURAI:Shapeandmaterial
fromunconstrainedreal-worldarbitraryimagecollections.InAdv.Neural
Inform.Process.Syst.(2022).3
Authorsversion12of14 Y.Poirier-Ginteretal./ADiffusionApproachtoRadianceFieldRelighting
[BF24] BHATTADA.,FORSYTHD.A.:Stylitgan:Promptingstyleganto [LCL∗23] LIANGR.,CHENH.,LIC.,CHENF.,PANNEERS.,VIJAYKU-
producenewilluminationconditions.InIEEE/CVFConf.Comput.Vis. MARN.:Envidr:Implicitdifferentiablerendererwithneuralenvironment
PatternRecog.(2024).3 lighting.InIEEE/CVFInt.Conf.Comput.Vis.(2023).3
[BJB∗21] BOSS M., JAMPANI V., BRAUN R., LIU C., BARRON J., [LGF∗22] LIQ.,GUOJ.,FEIY.,LIF.,GUOY.: Neulighting:Neural
LENSCHH.: Neural-pil:Neuralpre-integratedlightingforreflectance lightingforfreeviewpointoutdoorscenerelightingwithunconstrained
decomposition.InAdv.NeuralInform.Process.Syst.(2021).2,3 photocollections.InSIGGRAPHAsia2022ConferencePapers(2022).3
[BMHF23] BHATTADA.,MCKEED.,HOIEMD.,FORSYTHD.:Style- [LGZ∗20] LIUA.,GINOSARS.,ZHOUT.,EFROSA.A.,SNAVELYN.:
ganknowsnormal,depth,albedo,andmore.3 Learningtofactorizeandrelightacity.InEur.Conf.Comput.Vis.(2020).
2
[BMT∗21] BARRON J. T., MILDENHALL B., TANCIK M., HEDMAN
P.,MARTIN-BRUALLAR.,SRINIVASANP.P.:Mip-nerf:Amultiscale
[LLLY23] LINS.,LIUB.,LIJ.,YANGX.: Commondiffusionnoise
schedulesandsamplestepsareflawed.arXivpreprintarXiv:2305.08891
representationforanti-aliasingneuralradiancefields.InIEEE/CVFInt.
(2023).5
Conf.Comput.Vis.(2021).2
[BMV∗22] BARRONJ.T.,MILDENHALLB.,VERBIND.,SRINIVASAN [LL AZ r∗ sh2 a0 d] owL gaIU n:D S. h, aL dO owNG geC n. e, rZ atH ivA eN aG dvH e. r, saY riU alH n. e, twD oO rN kG foX r. a, uX gmIA eO ntC ed.:
P.P.,HEDMANP.:Mip-nerf360:Unboundedanti-aliasedneuralradiance
realityinsinglelightscenes. InIEEE/CVFConf.Comput.Vis.Pattern
fields.IEEE/CVFConf.Comput.Vis.PatternRecog.(2022).2
Recog.(2020).2
[CKK23] CHOIC.,KIMJ.,KIMY.M.:IBL-NeRF:Image-basedlighting [LSB∗22] LIZ.,SHIJ.,BIS.,ZHUR.,SUNKAVALLIK.,HAŠANM.,
formulationofneuralradiancefields.Comput.Graph.Forum42,7(2023). XUZ.,RAMAMOORTHIR.,CHANDRAKERM.:Physically-basedediting
3 ofindoorscenelightingfromasingleimage.InEur.Conf.Comput.Vis.
[CXG∗22] CHENA.,XUZ.,GEIGERA.,YUJ.,SUH.:Tensorf:Tenso- (2022).2
rialradiancefields.InEur.Conf.Comput.Vis.(2022).2 [LSY∗21] LAGUNASM.,SUNX.,YANGJ.,VILLEGASR.,ZHANGJ.,
[DHT∗00] DEBEVEC P., HAWKINS T., TCHOU C., DUIKER H.-P., SHUZ.,MASIAB.,GUTIERREZD.: Single-imagefull-bodyhuman
relighting.Eur.Graph.Symp.Render.(2021).2
SAROKINW.,SAGARM.: Acquiringthereflectancefieldofahuman
face.InProceedingsofthe27thannualconferenceonComputergraphics [LWC∗23] LIZ.,WANGL.,CHENGM.,PANC.,YANGJ.:Multi-view
andinteractivetechniques(2000),pp.145–156.1 inverserenderingforlarge-scalereal-worldindoorscenes.InIEEE/CVF
[FRV∗23] FUTSCHIKD.,RITLANDK.,VECOREJ.,FANELLOS.,ORTS-
Conf.Comput.Vis.PatternRecog.(2023).3
ESCOLANOS.,CURLESSB.,SY`KORAD.,PANDEYR.:Controllable [LWL∗23] LIU Y., WANG P., LIN C., LONG X., WANG J., LIU L.,
lightdiffusionforportraits. InIEEE/CVFConf.Comput.Vis.Pattern KOMURAT.,WANGW.:Nero:Neuralgeometryandbrdfreconstruction
Recog.(2023).2 ofreflectiveobjectsfrommultiviewimages.ACMTrans.Graph.(2023).
3
[GAA∗23] GALR.,ARARM.,ATZMONY.,BERMANOA.H.,CHECHIK
G.,COHEN-ORD.:Encoder-baseddomaintuningforfastpersonalization [LZF∗24] LIANGZ.,ZHANGQ.,FENGY.,SHANY.,JIAK.:Gs-ir:3d
oftext-to-imagemodels.ACMTrans.Graph.42,4(2023),1–13.3 gaussiansplattingforinverserendering.InProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition(CVPR)(2024).
[GCD∗20] GAOD.,CHENG.,DONGY.,PEERSP.,XUK.,TONGX.: 3
Deferredneurallighting:free-viewpointrelightingfromunstructured
photographs.ACMTrans.Graph.39,6(nov2020).3
[MBRS∗21] MARTIN-BRUALLAR.,RADWANN.,SAJJADIM.S.M.,
BARRON J. T., DOSOVITSKIY A., DUCKWORTH D.: NeRF in the
[GGL∗23] GAOJ.,GUC.,LINY.,ZHUH.,CAOX.,ZHANGL.,YAO Wild:NeuralRadianceFieldsforUnconstrainedPhotoCollections. In
Y.:Relightable3dgaussian:Real-timepointcloudrelightingwithbrdf IEEE/CVFConf.Comput.Vis.PatternRecog.(2021).6
decompositionandraytracing.arXiv:2311.16043(2023).2,3,8,10
[MESK22] MÜLLERT.,EVANSA.,SCHIEDC.,KELLERA.: Instant
[GRP22] GRIFFITHS D., RITSCHEL T., PHILIP J.: Outcast:Outdoor neuralgraphicsprimitiveswithamultiresolutionhashencoding. ACM
single-imagerelightingwithcastshadows. InComput.Graph.Forum Trans.Graph.41,4(July2022),102:1–102:15.2,5
(2022),vol.41,pp.179–193.3,8,10
[MGAD19] MURMANNL.,GHARBIM.,AITTALAM.,DURANDF.:A
[HHM22] HASSELGRENJ.,HOFMANNN.,MUNKBERGJ.:Shape,light, multi-illuminationdatasetofindoorobjectappearance.InIEEE/CVFInt.
andmaterialdecompositionfromimagesusingmontecarlorenderingand Conf.Comput.Vis.(2019).2,4,5,7
denoising.Adv.NeuralInform.Process.Syst.35(2022),22856–22869.3 [MHS∗22] MUNKBERGJ.,HASSELGRENJ.,SHENT.,GAOJ.,CHEN
[HJA20] HOJ.,JAINA.,ABBEELP.:Denoisingdiffusionprobabilistic W.,EVANSA.,MÜLLERT.,FIDLERS.:ExtractingTriangular3DMod-
models.InAdv.NeuralInform.Process.Syst.(2020).3 els,Materials,andLightingFromImages.InIEEE/CVFConf.Comput.
Vis.PatternRecog.(June2022),pp.8280–8290.3
[JL XX U∗2 Z3 .] ,SUJI HN .H :T., eL nsIU oirI :., TX enU soP r. i, alZ iH nvA eN rsG eX re. n, dH erA inN g.S I., nB IEI ES E., /CZ VH FOU CoX n. f,
.
[MST∗20] MILDENHALLB.,SRINIVASANP.P.,TANCIKM.,BARRON
Comput.Vis.PatternRecog.(2023).2,3,8,10
J.T.,RAMAMOORTHIR.,NGR.:Nerf:Representingscenesasneural
radiancefieldsforviewsynthesis.InEur.Conf.Comput.Vis.(2020).1,2
[JTL∗24] JIANGY.,TUJ.,LIUY.,GAOX.,LONGX.,WANGW.,MA
[NDDJK21] NIMIER-DAVIDM.,DONGZ.,JAKOBW.,KAPLANYAN
Y.: Gaussianshader:3dgaussiansplattingwithshadingfunctionsfor
A.:Materialandlightingreconstructionforcomplexindoorsceneswith
reflectivesurfaces,2024.3
texture-spacedifferentiablerendering.Comput.Graph.Forum(2021).3
[KE19] KANAMORIY.,ENDOY.:Relightinghumans:occlusion-aware [PD23] PHILIPJ.,DESCHAINTREV.:Floatersnomore:Radiancefield
inverserenderingforfull-bodyhumanimages.ACMTrans.Graph.37,6 gradientscalingforimprovednear-cameratraining.InEGSRConference
(2019).2 proceedingsDL-track(2023),TheEurographicsAssociation.7
[KKLD23] KERBLB.,KOPANASG.,LEIMKÜHLERT.,DRETTAKISG.: [PEL∗23] PODELL D., ENGLISH Z., LACEY K., BLATTMANN A.,
3dgaussiansplattingforreal-timeradiancefieldrendering.ACMTrans. DOCKHORNT.,MÜLLERJ.,PENNAJ.,ROMBACHR.:Sdxl:Improv-
Graph.42,4(July2023).2,5,6,7,8,10 inglatentdiffusionmodelsforhigh-resolutionimagesynthesis. arXiv
[KOH∗24] KEB.,OBUKHOVA.,HUANGS.,METZGERN.,DAUDT preprintarXiv:2307.01952(2023).3
R.C.,SCHINDLERK.:Repurposingdiffusion-basedimagegenerators [PGZ∗19] PHILIPJ.,GHARBIM.,ZHOUT.,EFROSA.A.,DRETTAKIS
formonoculardepthestimation.InIEEE/CVFConf.Comput.Vis.Pattern G.:Multi-viewrelightingusingageometry-awarenetwork.ACMTrans.
Recog.(2024).5 Graph.38,4(2019),78–1.2,3
AuthorsversionY.Poirier-Ginteretal./ADiffusionApproachtoRadianceFieldRelighting 13of14
[PLMZ23] PAPANTONIOU F. P., LATTAS A., MOSCHOGLOU S., [TTM∗22] TEWARIA.,THIESJ.,MILDENHALLB.,SRINIVASANP.,
ZAFEIRIOUS.:Relightify:Relightable3dfacesfromasingleimagevia TRETSCHK E., YIFAN W., LASSNER C., SITZMANN V., MARTIN-
diffusionmodels.InIEEE/CVFInt.Conf.Comput.Vis.(2023).2 BRUALLAR.,LOMBARDIS.,ETAL.:Advancesinneuralrendering.In
Comput.Graph.Forum(2022),vol.41,WileyOnlineLibrary,pp.703–
[PMGD21] PHILIPJ.,MORGENTHALERS.,GHARBIM.,DRETTAKIS
735.2
G.:Free-viewpointindoorneuralrelightingfrommulti-viewstereo.ACM
Trans.Graph.40,5(2021),1–18.2,3,7,8,10 [Ull79] ULLMANS.: Theinterpretationofstructurefrommotion. Pro-
ceedingsoftheRoyalSocietyofLondon.SeriesB,Biologicalsciences
[PTS23] PONGLERTNAPAKORNP.,TRITRONGN.,SUWAJANAKORNS.: 203,1153(January1979),405—426.2
Difareli:Diffusionfacerelighting.InIEEE/CVFInt.Conf.Comput.Vis.
(2023).2 [VZG∗23] VALENÇAL.,ZHANGJ.,GHARBIM.,HOLD-GEOFFROYY.,
LALONDEJ.-F.: Shadowharmonizationforrealisticcompositing. In
[RBL∗22] ROMBACHR.,BLATTMANNA.,LORENZD.,ESSERP.,OM- SIGGRAPHAsia2023ConferencePapers(2023).2
MERB.: High-resolutionimagesynthesiswithlatentdiffusionmodels.
[WSG∗23] WANG Z., SHEN T., GAO J., HUANG S., MUNKBERG J.,
InIEEE/CVFConf.Comput.Vis.PatternRecog.(2022).2,3,4,7
HASSELGRENJ.,GOJCICZ.,CHENW.,FIDLERS.:Neuralfieldsmeet
[RES∗22] RUDNEVV.,ELGHARIBM.,SMITHW.,LIUL.,GOLYANIK explicitgeometricrepresentationsforinverserenderingofurbanscenes.
V., THEOBALT C.: Nerfforoutdoorscenerelighting. InEur.Conf. InIEEE/CVFConf.Comput.Vis.PatternRecog.(2023).3
Comput.Vis.(2022).3
[WSLG23] WUT.,SUNJ.-M.,LAIY.-K.,GAOL.:De-nerf:Decoupled
[RLJ∗23] RUIZN.,LIY.,JAMPANIV.,PRITCHY.,RUBINSTEINM., neuralradiancefieldsforview-consistentappearanceeditingandhigh-
ABERMANK.:Dreambooth:Finetuningtext-to-imagediffusionmodels frequencyenvironmentalrelighting.InACMSIGGRAPH(2023).3
forsubject-drivengeneration.InIEEE/CVFConf.Comput.Vis.Pattern [WZL∗08] WANGY.,ZHANGL.,LIUZ.,HUAG.,WENZ.,ZHANG
Recog.(2023),pp.22500–22510.3 Z.,SAMARASD.: Facerelightingfromasingleimageunderarbitrary
unknownlightingconditions.IEEETrans.PatternAnal.Mach.Intell.31,
[SDWMG15] SOHL-DICKSTEINJ.,WEISSE.,MAHESWARANATHAN
11(2008),1968–1984.2
N., GANGULI S.: Deepunsupervisedlearningusingnonequilibrium
thermodynamics.3 [XZC∗23] XUY.,ZOSSG.,CHANDRANP.,GROSSM.,BRADLEYD.,
[SDZ∗21] SRINIVASAN P. P., DENG B., ZHANG X., TANCIK M., lG igO hT tiA nR g.D IO nP IE.: ER Ee /n Ce Vrf F:R Ine t.li Cgh ot na fb .l Ce on meu pr ua tl .Vra id s.ia (2n 0ce 23fi )e .l 3dswithnearfield
MILDENHALLB.,BARRONJ.T.:Nerv:Neuralreflectanceandvisibility
fieldsforrelightingandviewsynthesis.InIEEE/CVFConf.Comput.Vis. [YME∗20] YUY.,MEKAA.,ELGHARIBM.,SEIDELH.-P.,THEOBALT
PatternRecog.(2021).3 C.,SMITHW.A.:Self-supervisedoutdoorscenerelighting.InEur.Conf.
Comput.Vis.(2020).2
[SF16] SCHÖNBERGER J. L., FRAHM J.-M.: Structure-from-motion
revisited.InIEEE/CVFConf.Comput.Vis.PatternRecog.(2016).2,5 [YS22] YUY.,SMITHW.A.P.:Outdoorinverserenderingfromasingle
imageusingmultiviewself-supervision.IEEETrans.PatternAnal.Mach.
[SJL∗23] SHARMA P., JAMPANI V., LI Y., JIA X., LAGUN D., DU- Intell.44,7(2022),3659–3675.2
RANDF.,FREEMANW.T.,MATTHEWSM.: Alchemist:Parametric
[YZL∗22] YAOY.,ZHANGJ.,LIUJ.,QUY.,FANGT.,MCKINNOND.,
control of material properties with diffusion models. arXiv preprint
TSINY.,QUANL.:Neilf:Neuralincidentlightfieldforphysically-based
arXiv:2312.02970(2023).3
materialestimation.InEur.Conf.Comput.Vis.(2022).3
[SKCJ18] SENGUPTAS.,KANAZAWAA.,CASTILLOC.D.,JACOBS [ZCD∗23] ZENGC.,CHENG.,DONGY.,PEERSP.,WUH.,TONGX.:
D.W.:Sfsnet:Learningshape,reflectanceandilluminanceoffacesinthe Relightingneuralradiancefieldswithshadowandhighlighthints. In
wild.InIEEE/CVFConf.Comput.Vis.PatternRecog.(2018).2 ACMSIGGRAPH2023ConferenceProceedings(2023).3
[SLZ∗22] SHENG Y., LIU Y., ZHANG J., YIN W., OZTIRELI A. C., [ZDP∗24] ZENG C., DONG Y., PEERS P., KONG Y., WU H., TONG
ZHANGH.,LINZ.,SHECHTMANE.,BENESB.:Controllableshadow X.:Dilightnet:Fine-grainedlightingcontrolfordiffusion-basedimage
generationusingpixelheightmaps.InEur.Conf.Comput.Vis.(2022).2 generation.InACMSIGGRAPH2024ConferenceProceedings(2024).3
[SME20] SONGJ.,MENGC.,ERMONS.:Denoisingdiffusionimplicit [ZFC∗23] ZHUZ.,FENGX.,CHEND.,BAOJ.,WANGL.,CHENY.,
models.InInt.Conf.Learn.Represent.(2020).3 YUANL.,HUAG.:Designingabetterasymmetricvqganforstablediffu-
sion,2023.5
[SWW∗23] SHIY.,WUY.,WUC.,LIUX.,ZHAOC.,FENGH.,LIUJ.,
ZHANGL.,ZHANGJ.,ZHOUB.,DINGE.,WANGJ.:Gir:3dgaussian [ZHY∗23] ZHUJ.,HUOY.,YEQ.,LUANF.,LIJ.,XID.,WANGL.,
inverserenderingforrelightablescenefactorization.Arxiv(2023).3 TANG R., HUA W., BAO H., ET AL.: I2-sdf:Intrinsicindoorscene
reconstructionandeditingviaraytracinginneuralsdfs. InIEEE/CVF
[SYH∗17] SHUZ.,YUMERE.,HADAPS.,SUNKAVALLIK.,SHECHT- Conf.Comput.Vis.PatternRecog.(2023).3
MANE.,SAMARASD.: Neuralfaceeditingwithintrinsicimagedis-
entangling. InIEEE/CVFConf.Comput.Vis.PatternRecog.(2017). [ZIE∗18] ZHANGR.,ISOLAP.,EFROSA.A.,SHECHTMANE.,WANG
2 O.:Theunreasonableeffectivenessofdeepfeaturesasaperceptualmetric.
InIEEE/CVFConf.Comput.Vis.PatternRecog.(2018).10
[SZB21] SHENGY.,ZHANGJ.,BENESB.: Ssn:Softshadownetwork
[ZLW∗21] ZHANGK.,LUANF.,WANGQ.,BALAK.,SNAVELYN.:
forimagecompositing.InIEEE/CVFConf.Comput.Vis.PatternRecog.
PhySG:Inverserenderingwithsphericalgaussiansforphysics-based
(2021).2
materialeditingandrelighting.InIEEE/CVFConf.Comput.Vis.Pattern
[SZPF16] SCHÖNBERGERJ.L.,ZHENGE.,POLLEFEYSM.,FRAHM Recog.(2021).3
J.-M.: Pixelwiseviewselectionforunstructuredmulti-viewstereo. In [ZLZ∗22] ZHUZ.-L.,LIZ.,ZHANGR.-X.,GUOC.-L.,CHENGM.-
Eur.Conf.Comput.Vis.(2016).5
M.:Designinganillumination-awarenetworkfordeepimagerelighting.
[TÇE∗21] TÜRE M., ÇIKLABAKKAL M. E., ERDEM A., ERDEM E., IEEETrans.ImageProcess.31(2022),5396–5411.2
SATILMIS¸ P.,AKYÜZA.O.:Fromnoontosunset:Interactiverendering, [ZRA23] ZHANG L., RAO A., AGRAWALA M.: Adding conditional
relighting,andrecolouringoflandscapephotographsbymodifyingsolar controltotext-to-imagediffusionmodels.InIEEE/CVFInt.Conf.Comput.
position.InComput.Graph.Forum(2021),vol.40,pp.500–515.2 Vis.(2023).2,3,4,7
[TDMS∗23] TOSCHIM.,DEMATTEOR.,SPEZIALETTIR.,DEGRE- [ZSD∗21] ZHANGX.,SRINIVASANP.P.,DENGB.,DEBEVECP.,FREE-
GORIOD.,DISTEFANOL.,SALTIS.: Relightmynerf:Adatasetfor MANW.T.,BARRONJ.T.:Nerfactor:Neuralfactorizationofshapeand
novelviewsynthesisandrelightingofrealworldobjects.InIEEE/CVF reflectanceunderanunknownillumination. ACMTrans.Graph.40,6
Conf.Comput.Vis.PatternRecog.(June2023),pp.20762–20772.3 (2021),1–18.2,3
Authorsversion14of14 Y.Poirier-Ginteretal./ADiffusionApproachtoRadianceFieldRelighting
[ZSH∗22] ZHANGY.,SUNJ.,HEX.,FUH.,JIAR.,ZHOUX.:Mod-
elingindirectilluminationforinverserendering. InIEEE/CVFConf.
Comput.Vis.PatternRecog.(2022).3
[ZTS∗22] ZHANGX.,TSENGN.,SYEDA.,BHASINR.,JAIPURIAN.:
Simbar:Singleimage-basedscenerelightingforeffectivedataaugmenta-
tionforautomateddrivingvisiontasks.InIEEE/CVFConf.Comput.Vis.
PatternRecog.(062022).2
[ZYL∗23] ZHANGJ.,YAOY.,LIS.,LIUJ.,FANGT.,MCKINNOND.,
TSINY.,QUANL.: Neilf++:Inter-reflectablelightfieldsforgeometry
andmaterialestimation.InIEEE/CVFInt.Conf.Comput.Vis.(2023).3
Authorsversion