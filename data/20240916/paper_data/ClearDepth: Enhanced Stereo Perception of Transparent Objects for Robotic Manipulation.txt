ClearDepth: Enhanced Stereo Perception of Transparent Objects
for Robotic Manipulation
Kaixin Bai1,2, Huajian Zeng2,3, Lei Zhang1,2†, Yiwen Liu2,3, Hongli Xu3, Zhaopeng Chen2, Jianwei Zhang1
Abstract—Transparentobjectdepthperceptionposesachal- obstacle in 3D reconstruction of transparent objects is the
lengeineverydaylifeandlogistics,primarilyduetotheinability scarcity of reliable feature points. Innovations addressing
of standard 3D sensors to accurately capture depth on trans- this include extracting structural details, such as boundary
parentorreflectivesurfaces.Thislimitationsignificantlyaffects
identification [6], and improving hardware capabilities for
depth map and point cloud-reliant applications, especially in
roboticmanipulation.Wedevelopedavisiontransformer-based moreaccuratedepthimagery[7].Deeplearninghasplayeda
algorithmforstereodepthrecoveryoftransparentobjects.This pivotalroleinlearningandinterpretingthecomplexgeomet-
approachiscomplementedbyaninnovativefeaturepost-fusion ricfeaturesoftheseobjects,therebyenhancingtheprecision
module, which enhances the accuracy of depth recovery by and reliability of 3D models. For deep learning tasks in-
structural features in images. To address the high costs associ-
volving transparent objects, extensive and accurate datasets
atedwithdatasetcollectionforstereocamera-basedperception
of transparent objects, our method incorporates a parameter- are crucial. Various techniques have been used for dataset
aligned, domain-adaptive, and physically realistic Sim2Real collection, including using markers for object poses [8], [9],
simulation for efficient data generation, accelerated by AI substituting transparent objects with opaque ones [10], and
algorithm. Our experimental results demonstrate the model’s manual manipulation in 3D software [11]. However, these
exceptional Sim2Real generalizability in real-world scenarios,
methods are labor-intensive and often result in noisy depth
enabling precise depth mapping of transparent objects to
assist in robotic manipulation. Project details are available at maps. To overcome these challenges, simulation engines are
https://sites.google.com/view/cleardepth/. increasinglyusedtogeneratesyntheticdata[11],[5],though
balancing the realism of ray-tracing with the efficiency of
I. INTRODUCTION
rasterization remains a significant challenge. Our approach
Transparent objects, such as glass bottles or cups, are includes developing an AI-accelerated tool for generating
commonly found in domestic service robotics and logistics realistic simulation datasets for transparent object depth
goodssortingscenarios.Theirtransparentnature,particularly recovery. This tool enables direct application of models
the complexity of refraction and reflection, poses challenges on actual sensors without post-training, producing instance
for imaging and recognition [1]. Accurate handling of these segmentation, poses, and depth maps. We also introduce
objects is crucial for robots that rely on precise three- a stereo depth recovery network specifically designed for
dimensional information. Whether it’s accurately grasping transparent objects. Utilizing a cascaded vision transformer
glass items in home services or efficiently sorting trans- (ViT) backbone, it efficiently extracts contextual structural
parent goods at logistics centers, advanced algorithms and information. This network outperforms previous CNN and
technologies are required for machine vision systems. This ViT-based models, particularly for high-resolution images
includes the development of algorithms capable of handling and robotic manipulation. Our contributions address the
materials with high reflectivity and refraction, or new sensor limitationsofexistingdepthestimationmethodsanddatasets
technologies to better capture the characteristics of trans- for transparent objects. We present a novel AI-enhanced
parent objects. These advancements will enhance the ability stereo-based simulation dataset generation method, creating
of robots to handle transparent objects in various environ- a detailed, noise-free dataset. Additionally, our end-to-end
ments, push the application of robotic vision technology stereo depth recovery network operates independently of
into broader fields, and enable robots to perform complex mask priors and includes a post-processing structure that
tasksmoreautonomously.Inaddressingthedepthperception enhances structural details. This stereo imaging algorithm
challenges posed by transparent objects, researchers have can be directly integrated into existing robotic grasping
developed innovative approaches. Initial efforts focused on pipelines. The whole pipeline is as in Fig. 1.
usingdeeplearningtechniquestoreconstructdepthinforma-
II. RELATEDWORK
tion from incomplete depth images captured by RealSense
cameras,optimizingforgreateraccuracy[2],[3].Subsequent A. Perception of Transparent Objects
studies explored stereo vision systems [4] and multi-view Robotics faces significant challenges in perceiving trans-
approaches [5], leveraging neural networks to enhance per- parent objects due to their low contrast and complex light
ception capabilities. Despite these advancements, real-world behavior, which affects sensor accuracy in determining their
applications still face challenges, such as inconsistent depth position and shape. Traditional sensors like RGB and RGB-
inputs and the complexities of multi-view imaging. A major D cameras often struggle with transparent objects because
they focus on intensity data, missing the nuances of optical
†Correspondingauthor.lei.zhang-1@studium.uni-hamburg.de properties. To address this, research has increasingly turned
1MIN-Fakulta¨t Fachbereich Informatik TAMS, University of Hamburg
to polarized cameras, which reduce reflections, increase
{name.surname}@studium.uni-hamburg.de
2AgileRobotsSE{name.surname}@agile-robots.com contrast, and provide richer light information, enhancing
3TechnicalUniversityofMunich{name.surname}@tum.de the detection of transparent objects’ shape and position [7],
4202
peS
31
]OR.sc[
1v62980.9042:viXraSynthetic Left and Right Image Disparity Ground Truth
Transformed Point Cloud Humanoid Robotic Settings
sim2real
Transparent
Real Left and Right Image Depth Inferenced Disparity
Recovery
Sec 3.1 Stereo Depth Image Recovery For Transparent Objects
Objects Assets with Modified Glass Material
ZED 2 CameraParameters
Indoor Scene with Mesh Models
...
Sec 3.2 Synthetic Dataset Generation Sec 4.4 Transparent Object Robotic Grasping
Fig. 1: In this work, we explore how to reconstruct depth maps of transparent objects using Sim2Real technology combined
with stereo vision, thereby enabling precise manipulation of transparent objects by robots. In the Section III, we focus
on introducing a simulation data generation method enhanced by AI, aimed at achieving an optimal balance between the
effectiveness and speed of data generation. The Section IV conducts both qualitative and quantitative analyses comparing
the performance of our method with the baseline, and demonstrates the efficacy of our approach through robot grasping
experiments.
[12]. However, their high cost limits widespread use in inspiredbySuperGlue[28],usestransformerswithpositional
robotics. Some studies, like [13], enhance transparent object embedding and attention mechanisms for binocular dense
trackingbycombiningCNNandtransformer-basedencoders, matching, producing disparity and depth maps. However,
while [14] employs Sim2Real strategies with simulated these methods are computationally intensive and slow in
datasets. Research such as [15] addresses transparent object inference,limitingtheirsuitabilityforhigh-resolutionimages
matting by predicting alpha channel values, aiding image and downstream robotic tasks.
editing. For robotic manipulation and pose estimation, re-
C. Transparent Object Datasets
cent works have expanded perception models to include
In the open-source community, various datasets for trans-
multi-task extensions [8], [11], [16]. Depth recovery and
parent objects are available, such as real-world datasets [8],
reconstruction remain particularly challenging due to light
[29], [30]. In Depth Anything v1, the authors used both
interactions, with techniques like Nerf and volumetric ren-
open-source real and synthetic datasets for training, but in
dering used for surface reconstruction [17], [7], [18], [19],
v2, they switched entirely to synthetic data. Introducing
andstereoandmultiviewapproachesfordepthandgeometry
even 10% real data can significantly degrade performance
regression[4],[20],[9].Theseeffortsutilizevarioussensors,
due to sensor noise in the labels, which is particularly
including RGB-D, stereo-vision, and multi-view systems, to
detrimental in depth recovery tasks. The challenge of ac-
improvetransparentobjectperception[3],[2],[8],[11],[16],
curately annotating complex, occluded real-world scenes led
[9],[4],[5],[18],[7].Thefield,whilecomplex,isadvancing
us to prioritize synthetic data. To mitigate potential sim2real
withdeeplearningandsensortechnology,aimingtoenhance
gaps,weusedray-tracingrendererstosimulaterealisticlight
accuracy and reliability.
refraction. Existing synthetic datasets, such as [10], [31],
B. Deep Learning-based Stereo Depth Recovery
[9], [32], [33], [16], typically feature transparent objects
Deep learning-based stereo matching methods have re- on desktops. However, these datasets lack the complexity
cently outperformed traditional approaches, with 2D convo- necessary for generalization to real-world scenarios like
lutional models [21], [22] offering simplicity and efficiency. kitchens,bedrooms,andoffices,whereserviceandhumanoid
These models achieve high accuracy even on limited com- robots operate. Moreover, these datasets often require ex-
putational resources, making them suitable for engineering tensive pre- or post-processing, such as segmentation or
applications,thoughtheystillrequireimprovementsinaccu- background reconstruction, which is impractical for end-to-
racyandrobustnessdueto3Dcostspaceconstraints.3Dcon- endalgorithmscrucialtoembodiedintelligenceapplications.
volutional networks [23], [24] provide better interpretability SyntheticdatasetswithHDRIbackgrounds,suchas[5],also
and higher disparity map accuracy but require optimization face challenges. HDRI backgrounds lack depth value labels,
due to their computational demands. RAFT-Stereo [25], an leading to confusion and poor generalization, especially in
extension of RAFT [26], applies optical flow techniques to zero-shot scenarios for service robots. Simulation data for
stereo matching, improving generalization and robustness Realsense cameras, like [16], [33], involve complex render-
with its lightweight GRU module, but struggles with global ing processes that reduce efficiency. The process includes
context extraction due to its CNN architecture. STTR [27], rendering the RGB image, simulating IR projection withspotlights, and generating noisy depth maps, which is both the interpolation of positional embeddings in the original
complex and time-consuming. In designing our dataset and ViT structure, especially when dealing with varying input
generation process, we address the limitations of existing imagesizes,herebysubstitutingpositionalembeddingswith
datasets while anticipating the needs of future embodied learnable depth-wise convolutions. The equation is as 3.
intelligence algorithms. Our dataset includes indoor scenes
x =MLP(GELU(Conv (MLP(x ))))+x (3)
typical for service robots, enriched with background depth out 3×3 in in
values for transparent objects. It features both container-
Then, we concatenate multi-scale feature maps from dif-
type and cosmetic transparent objects, carefully designed to
ferent ViT blocks by upsampling them to a unified scale of
overcome current deficiencies and ensure future scalability. 1. This combined feature map undergoes further refinement
4
through a precise 1·1 convolution, facilitating optimal di-
III. METHOD
mension adjustment.
A. Network Overview
2) Structural Feature Post-Fusion: In this work, we pro-
Transparent objects refract background textures, making pose a modified GRU-based architecture to optimize dispar-
texture features less relevant and structural features more itymapsinacoarse-to-finemanner.OurPost-Fusionmodifi-
critical for imaging and perception tasks. In [34], it was cationisparticularlyintroducedtoenhancetheprocessingof
found that CNNs excel at recognizing textures, while ViTs transparentobjects.Wemadethismodificationbecause,dur-
are better at recognizing shapes. Traditional ViTs downsam- ing our depth estimation experiments on transparent objects,
ple inputs and outputs to a smaller size, using learnable we observed that unlike regular objects, depth estimation
upsampling. While effective in many vision tasks, ViTs fortransparentobjectsparticularlyrequiresprecisestructural
are challenging to train, computationally intensive, and less information. Additionally, for transparent objects, due to the
effective at fine-grained feature extraction. To address this, refractive nature of their background texture features, the
models like SegFormer [35] and DinoV2 [36] use cascaded featuresimilarityinformationobtainedthroughdotproductis
structures to enhance ViTs for fine-grained tasks like depth notsuitableforreconstructingtransparentobjects.Therefore,
estimation and semantic segmentation, providing multi-scale it makes sense to introduce the structural information of
features and easing training. the image itself during the GRU iterations. This approach
For transparent objects, structural features are crucial. ensures that the structural information extracted at different
Considering the fine-grained needs of stereo tasks and the resolutions remains consistent throughout the iterative pro-
demand for lightweight, fast models in robotics, we utilize cess.
MixVisionTransformer B5 for feature extraction. Traditional The core update equations in our model are defined as
stereo deep learning networks rely on feature dot products follows:
for similarity, which is inadequate for transparent objects
x =[C ,d ,c ,c ,c ] (4)
duetobackgroundtexturerefraction.Incorporatingstructural k k k k r h
feature priors in the GRU loop is essential. While cross- z k=σ(Conv([h k−1,x k],W z)+c k), (5)
attention or spatial-attention structures could be used, they r =σ(Conv([h ,x ],W )+c ), (6)
k k−1 k r r
are parameter-heavy and slow in inference. Therefore, we
h˜ =tanh(Conv([r ⊙h ,x ],W )+c ), (7)
designed a lightweight structural feature post-fusion module k k k−1 k h h
to introduce these features into the GRU loop, enhancing h k=(1−z k)⊙h k−1+z k⊙h˜ k, (8)
performance by leveraging the low-level structural feature.
Here, x is a concatenation of several feature maps,
Our network structure is as in Fig. 2. k
including the correlation C , the current disparity d , and
1) Cascaded Vision Transformer Backbone.: Our back- k k
structural context features c , c , and c . Specifically, c ,
bone architecture, conforming to the B5 settings of Seg- k r h k
c , and c represent structural features derived from the left
Former, initiates with overlap patch embedding for initial r h
image. These features are incorporated as residuals into the
image tokenization, crucial for preserving local features.
GRU loop, allowing for enhanced participation of structural
This process sequentially passes the tokens through four
information during the disparity map refinement process.
transformer blocks (N = 4), effectively generating feature
Then, Our approach decode GRUs at each resolutions to
maps at scaled dimensions of 1, 1, 1 , and 1 . To optimize
4 8 16 32 obtainmulti-scaledisparityupdatesforcoarsetofinegradual
computational efficiency, the model incorporates efficient
optimization:
self-attention, which significantly reduces the computational
burden from O(N2) to O(N2 ) by implementing a reduction △d =Decoder(h ), (9)
R k,1 k,1
ratioR.Thisreductionisachievedbyfirstreshapingtheinput 32 32
sequence from N·C to N×(C·R) by 2d convolutional layer △d k,1 =Decoder(h k,1 +Interp(△d k,1 )), (10)
R 16 16 32
with the stride 8,4,2,1 for different ViT blocks, as detailed △d =Decoder(h +Interp(△d )), (11)
k,1 k,1 k,1
in equation 1, and then adjusting the sequence dimensions 8 8 16
back toC through linear layers, as described in equation 2. whereDecoderconsistoftwoconvolutionallayersandInterp
isbilinearinterpolationscaledupbyafactoroftwo.Finally,
N
Kˆ =Reshape( ,C·R)(K) (1) the updated disparity is calculated as:
R
K=Linear(C·R,C)(Kˆ) (2) d k+1=d k+△d k (12)
Additionally, the Mix-FFN module in the architecture ad- In summary, to address the challenges of transparent
dresses the challenge of performance degradation due to objects, we selected an appropriate image feature extractor.·,· C1 C2 · · · C4
Correlation Pyramid
L x n
Left Image Overlap
Patch
Embedding
Conv_GRU
Feature Encoder
x n
x2
Right Image x2
flow_head
0 ···
Context Encoder
Fig. 2: Our stereo depth recovery network for transparent objects. Feature extraction is performed on left and right images,
with additional processing by a shared weighted context encoder for the left image. A correlation pyramid, created by
merging the feature maps, is refined through a GRU loop, enhancing structural information for depth recovery. The output
is a refined disparity map.
Synthetic Dataset Generation Pipeline enhanced by deep learning
Objects Assets
Indoor Scene
Ray Tracing AI Denoiser RA eI soS lu up tie or n Ge sn ee gr mat ee nd t as tt ie or ne o / nR oG rmB a / l d mep apth /
Fig. 3: We utilize hardware-accelerated ray tracing, OptiX
AI denoiser, and AI-driven super-resolution. This process
accelerates the dataset generation process.
Additionally, considering the unique difficulties of transpar- Fig. 5: Sample images from our SynClearDepth dataset are
ent objects and the need for lightweight models in robotics, presented, showcasing randomly placed transparent objects
we designed a structural feature post-fusion architecture. in various indoor scenes (bathroom, dining room, kitchen,
Every detail of our network structure is tailored to the living room) under different lighting conditions. The objects
characteristics of transparent object scenarios. include common transparent items like cosmetic packaging,
In the comparative experiments section, the visual results glass containers, etc.
demonstrate that our model significantly enhances the stereo
imaging of transparent objects.
B. Synthetic Dataset Generation
speeding up the process by cutting nearly two-thirds data
generation time while maintaining high-quality outputs. Our
dataset generation process is illustrated in Fig. 3. Our pro-
posed dataset SynClearDepth comprises 16 selected objects,
including10commontransparentcontainersindailylifeand
6productswithglassmaterial,asshowninFig.4.Toensure
that the background of the generated dataset also contains
depthvalueandavoidambiguitiesindepthrecoverytraining
Fig. 4: The left image displays rendered models, with the due to lack of dataset labeling of background part, we col-
firsttworowsfeaturingtransparentcontainerssuchasvases, lected indoor scenes in combination with the object models,
bowls, and cups, while the last two rows showcase cosmetic encompassing 6 bathrooms, 3 dining rooms, 5 kitchens, and
products made of transparent glass and plastic. The right 6 living rooms. Through this combination, we generated
image shows the real objects used for Sim2Real testing. 14,091 image sets, each containing left and right RGB
images with size 1280*720, ground truth depth, instance
To enhance dataset generation efficiency, we utilized Op- segmentation maps, as well as object and camera poses, as
tiX’s AI denoiser and deep learning super-resolution tech- illustrated in Fig. 5. In the dataset generation process, we
niques [37], notably reducing the time from an average of applied domain randomization to object types, quantities,
12.77 seconds to 4.40 seconds per set (including stereo positionsandposes,lightingconditions,andcamerashooting
RGB, depth, masks, and object-camera poses), significantly angles.C. metrics
1) AvgErr (Average Error): Represents the average dis-
parity error across all pixels, indicating the general
accuracy of the disparity map. (a) Left image (b) RAFT-Stereo[25]
2) RMS(RootMeanSquareError):Measuresthesquare
rootoftheaveragesquareddisparityerror,reflectingthe
overall deviation from the ground truth.
3) Bad0.5(%),Bad1.0(%),Bad2.0(%),Bad4.0(%):
(c) StereoBase[43] (d) Ours
These metrics indicate the percentage of pixels where
the disparity error exceeds 0.5, 1.0, 2.0, and 4.0 pixels, Fig. 6: Visual comparisons on KITTI 2015 with SOTA
respectively, highlighting the proportion of significant StereoBase and baseline RAFT-Stereo. Our method is more
errors in the disparity map. robust to overall scene details.
Together, these metrics provide a comprehensive assess-
mentofstereomatchingperformance,balancingbothoverall
accuracy and the frequency of large errors. cannot fully reflect imaging quality [42], we conducted a
qualitative analysis on the KITTI dataset. Fig. 6 demon-
IV. EXPERIMENTS
strates a competitive comparison focused on detail recovery,
A. Technical Specifications
our method shows exceptional proficiency in reconstructing
Our network is firstly pre-trained on CREStereo depthdetailsofforegroundobjects,significantlyoutstripping
dataset [38] and Scene Flow dataset [39], and then fine- alternative approaches by a substantial margin.
tunedonourproposedSynClearDepthdatasetfortransparent
object stereo imaging. Our model is trained on 1 block Methods AvgErr RMS bad0.5(%) bad1.0(%) bad2.0(%) bad4.0(%)
of NVIDIA RTX A6000 with batch size 8 and the whole IGEV-Stereo[44] 2.0766 5.5301 58.9743 36.127 19.9668 10.7771
DLNR[45] 3.097 8.4269 28.1088 21.8442 16.481 11.9046
training lasts for 300,000 steps. We use AdamW [40] as Selective-IGEV[46] 1.273 4.3365 34.8229 17.6288 9.561 5.8707
RAFT-Stereo[25] 2.245 8.8016 29.7356 17.4521 10.835 6.2107
optimizer, the learning rate is set to 0.0002, updated with clearDepth 2.138 8.7282 24.7329 16.3178 9.8459 5.76
a warm-up mechanism and used one-cycle learning rate TABLE II: Quantitative results on transparent object dataset
scheduler. The final learning rate when training finished is compared with stereo SOTA methods.
0.0001. The input size of the model is resized to 360×720.
Fine-tune for transparent objects takes the same training
3) Evaluation on Our Transparent Object Dataset: To
parameters as pretraining on the opaque dataset.
validate the efficacy of our model and our proposed dataset
B. Qualitative and Quantitative Studies for Stereo Depth for the task of depth recovery of transparent objects using
Estimation stereo vision, we fine-tuned our pre-trained model on the
SynClearDepth dataset. This fine-tuning process involved
1) Quantitative Analysis on Middlebury Dataset: The
utilizing the same training parameters established during the
Middlebury 2014 dataset comprises 23 pairs of images
pre-training phase. Additionally, we finetuned the baseline
designate for training and validation purposes. We refine
model, RAFT-Stereo [25] and other SOTA models on Mid-
ourmodel overthese23 pairs,conducting fine-tuningacross
dlebury benchmark, on the SynClearDepth dataset and con-
4,000 iterations with an image resolution of 384×1024.
ductedaperformancecomparison.Thiscomparativeanalysis
Benchmark against standard baseline approaches RAFT-
aimed to underscore the advancements and improvements
Stereo and CREStereo using various stereo evaluation met-
our model offers in the specific context of stereo-based
rics further underscores the efficacy of our approach, as
depth perception for transparent objects. Tab. II displays the
outlined in Tab. I. More comparison results with other
quantitative validation results. The visualization results of
methods can be found at [41].
the stereo imaging for transparent objects are illustrated in
Fig. 7. From the visualization results, it is evident that the
Methods AvgErr RMS bad0.5(%) bad1.0(%) bad2.0(%) bad4.0(%)
RAFT-Stereo[25] 1.27 8.41 27.7 9.37 4.14 2.75 imaging performance of our network, specifically designed
CREStereo[38] 1.15 7.70 28.0 8.25 3.71 2.04
for transparent objects, significantly surpasses that of other
clearDepth 1.33 8.68 25.30 7.39 3.48 2.00
stereo imaging methods in the transparent object regions.
TABLEI:QuantitativeresultsonMiddleburryStereoEvalua- 4) Ablation Study of Feature Post-Fusion Module: To
tionBenchmark[41].Allmetricshavebeencalculatedusing ascertain the impact of our feature post-fusion module, we
undisclosed weighting factors. The outcomes unequivocally embarked on a series of ablation studies. These studies
demonstrate that our technique significantly outperforms the compared networks equipped with and without the feature
baseline method. post-fusion module, specifically focusing on our transparent
objectdataset,SynClearDepth.TheresultsinTab.IIIdemon-
2) Quantitative Analysis on KITTI Dataset: We fine-tune strate that the inclusion of the feature post-fusion module
our pre-trained model using the KITTI 2015 training set significantly improves the network’s overall performance in
across 5,000 steps, employing image crops sized at 320× processing transparent objects. This enhancement is partic-
1000. The learning rate is established at 0.00001, with the ularly evident in challenging scenarios involving complex
batch size held at 3. In terms of GRU updates, we perform transparency and light refraction, underscoring the module’s
22 iterations during training, adjusting to 32 iterations for effectivenessincapturingandintegratingcrucialfeaturesfor
testing.GiventhatthelabelsandmetricsoftheKITTIdataset more accurate depth estimation and object recognition inRGB Left Image IGEV-Stereo DLNR Selective-IGEV RAFT-Stereo clearDepth
Fig. 7: The visualization results of our transparent object stereo depth reconstruction method compare with other SOTA
stereo depth estimation methods by fine-tuning on SynClearDepth dataset.
such contexts. The ablation study utilizes 100,000 steps in fill the dataset background, avoiding ambiguities for depth
training. recovery networks. We also propose a stereo depth recov-
ery network for transparent objects, which enhances the
C. Robotic Grasping Experiments underlying structural information, allowing our network to
We employ the ZED 2 stereo camera with our proposed recover the depth of transparent objects implicitly without
network, integrated with a robotic arm and a five-fingered the need for a transparent object mask prior. We conducted
dexterous hand, to perform grasping tasks. We tested the comprehensivecomparativeexperimentsonourmodelusing
success rate of grasping transparent objects in service robot both public datasets and our proprietary dataset for trans-
scenarios,followingtheexperimentalsetupofFFHCluttered- parent objects. Additionally, we performed ablation studies
Grasping [47]. Using our stereo method, we generated point to evaluate the design of our model. These experiments
cloudsofsceneswithtransparentobjectsandperformedpose demonstrated the exceptional performance of our model,
estimation. We then planned the grasping posture for the highlighting its robustness and accuracy in various scenarios
five-fingered hand. We conducted 150 experiments for each for transparent object perception, aiding in the manipulation
scenario, varying the number of transparent objects from 1 tasks of transparent objects by robots.
to 5, and the results are shown in Table IV. We achieved
REFERENCES
an average success rate of 86.2%. Our approach integrates
seamlesslyintoexistingroboticgraspingframeworkswithout [1] J.Jiang,G.Cao,J.Deng,T.-T.Do,andS.Luo,“Roboticperception
of transparent objects: A review,” IEEE Transactions on Artificial
needing additional processing of intermediate results. Our
Intelligence,2023.
method does not require multi-view images capture, thus [2] T.Li,Z.Chen,H.Liu,andC.Wang,“Fdct:Fastdepthcompletionfor
maintaining the robot’s operational efficiency. Additionally, transparentobjects,”IEEERoboticsandAutomationLetters,2023.
[3] K. Chen, S. Wang, B. Xia, D. Li, Z. Kan, and B. Li, “Tode-trans:
our data generation process is fast and efficient, enabling
Transparentobjectdepthestimationwithtransformer,”in2023IEEE
rapid expansion of datasets for further tasks. Our find- InternationalConferenceonRoboticsandAutomation(ICRA). IEEE,
ings demonstrate that our transparent object depth recovery 2023,pp.4880–4886.
[4] K. Chen, S. James, C. Sui, Y.-H. Liu, P. Abbeel, and Q. Dou,
network significantly improves the robot’s ability to grasp
“Stereopose: Category-level 6d transparent object pose estimation
transparent objects. fromstereoimagesviaback-viewnocs,”in2023IEEEInternational
Conference on Robotics and Automation (ICRA). IEEE, 2023, pp.
2855–2861.
Methods AvgErr RMS bad0.5(%) bad1.0(%) bad2.0(%) bad4.0(%)
w/oFusion 6.90 15.48 43.34 29.63 21.52 16.62 [5] Y.R.Wang,Y.Zhao,H.Xu,S.Eppel,A.Aspuru-Guzik,F.Shkurti,
FeatureFusion 2.64 8.59 27.23 16.87 11.28 7.72 andA.Garg,“Mvtrans:Multi-viewperceptionoftransparentobjects,”
in2023IEEEInternationalConferenceonRoboticsandAutomation
TABLE III: Ablation study for the feature post-fusion mod- (ICRA). IEEE,2023,pp.3771–3778.
[6] Y. Cao, Z. Zhang, E. Xie, Q. Hou, K. Zhao, X. Luo, and J. Tuo,
ule in clearDepth with 100,000 steps on SynClearDepth
“Fakemixaugmentationimprovestransparentobjectdetection,”arXiv
dataset. preprintarXiv:2103.13279,2021.
[7] M. Shao, C. Xia, D. Duan, and X. Wang, “Polarimetric inverse
rendering for transparent shapes reconstruction,” arXiv preprint
arXiv:2208.11836,2022.
ObjectNumber 1 2 3 4 5 [8] H.Fang,H.-S.Fang,S.Xu,andC.Lu,“Transcg:Alarge-scalereal-
successrate 98% 92% 86% 78% 76% worlddatasetfortransparentobjectdepthcompletionandagrasping
baseline,” IEEE Robotics and Automation Letters, vol. 7, no. 3, pp.
TABLE IV: Grasping success rate with varying number of 7383–7390,2022.
[9] H. Xu, Y. R. Wang, S. Eppel, A. Aspuru-Guzik, F. Shkurti, and
transparent objects.
A. Garg, “Seeing glass: joint point cloud and depth completion for
transparentobjects,”arXivpreprintarXiv:2110.00087,2021.
[10] S. Sajjan, M. Moore, M. Pan, G. Nagaraja, J. Lee, A. Zeng, and
V. CONCLUSION S.Song,“Cleargrasp:3dshapeestimationoftransparentobjectsfor
manipulation,” in 2020 IEEE International Conference on Robotics
In this paper, we propose a new stereo transparent object andAutomation(ICRA). IEEE,2020,pp.3634–3642.
[11] X. Chen, H. Zhang, Z. Yu, A. Opipari, and O. Chadwicke Jenkins,
datasetSynClearDepth,containingRGB,depth,mask,object
“Clearpose: Large-scale transparent object dataset and benchmark,”
poses, and camera poses. Compared to other real datasets, in European Conference on Computer Vision. Springer, 2022, pp.
our simulated data does not introduce the inherent noise 381–396.
of real 3D sensors. Unlike other simulated datasets, we
have collected a large number of indoor scene models to[12] A.Kalra,V.Taamazyan,S.K.Rao,K.Venkataraman,R.Raskar,and [33] J. Shi, Y. Jin, D. Li, H. Niu, Z. Jin, H. Wang et al., “Asgrasp:
A.Kadambi,“Deeppolarizationcuesfortransparentobjectsegmen- Generalizabletransparentobjectreconstructionandgraspingfromrgb-
tation,” in Proceedings of the IEEE/CVF Conference on Computer dactivestereocamera,”arXivpreprintarXiv:2405.05648,2024.
VisionandPatternRecognition,2020,pp.8602–8611. [34] S.Tuli,I.Dasgupta,E.Grant,andT.L.Griffiths,“Areconvolutional
[13] K. Garigapati, E. Blasch, J. Wei, and H. Ling, “Transparent object neural networks or transformers more like human vision?” arXiv
tracking with enhanced fusion module,” in 2023 IEEE/RSJ Interna- preprintarXiv:2105.07197,2021.
tionalConferenceonIntelligentRobotsandSystems(IROS). IEEE, [35] E.Xie,W.Wang,Z.Yu,A.Anandkumar,J.M.Alvarez,andP.Luo,
2023,pp.7696–7703. “Segformer: Simple and efficient design for semantic segmentation
[14] A.Lukezic,Z.Trojer,J.Matas,andM.Kristan,“Trans2k:Unlocking with transformers,” Advances in Neural Information Processing Sys-
the power of deep models for transparent object tracking,” arXiv tems,vol.34,pp.12077–12090,2021.
preprintarXiv:2210.03436,2022. [36] M.Oquab,T.Darcet,T.Moutakanni,H.Vo,M.Szafraniec,V.Khali-
[15] H. Cai, F. Xue, L. Xu, and L. Guo, “Transmatting: Tri-token dov,P.Fernandez,D.Haziza,F.Massa,A.El-Noubyetal.,“Dinov2:
equipped transformer model for image matting,” arXiv preprint Learning robust visual features without supervision,” arXiv preprint
arXiv:2303.06476,2023. arXiv:2304.07193,2023.
[16] Q.Dai,J.Zhang,Q.Li,T.Wu,H.Dong,Z.Liu,P.Tan,andH.Wang, [37] N. Ahn, B. Kang, and K.-A. Sohn, “Fast, accurate, and lightweight
“Domainrandomization-enhanceddepthsimulationandrestorationfor super-resolution with cascading residual network,” in Proceedings of
perceivingandgraspingspecularandtransparentobjects,”inEuropean theEuropeanconferenceoncomputervision(ECCV),2018,pp.252–
ConferenceonComputerVision. Springer,2022,pp.374–391. 268.
[17] Z.Li,X.Long,Y.Wang,T.Cao,W.Wang,F.Luo,andC.Xiao,“Neto: [38] J.Li,P.Wang,P.Xiong,T.Cai,Z.Yan,L.Yang,J.Liu,H.Fan,and
Neuralreconstructionoftransparentobjectswithself-occlusionaware S.Liu,“Practicalstereomatchingviacascadedrecurrentnetworkwith
refraction-tracing,”arXivpreprintarXiv:2303.11219,2023. adaptivecorrelation,”inProceedingsoftheIEEE/CVFconferenceon
[18] Q.Dai,Y.Zhu,Y.Geng,C.Ruan,J.Zhang,andH.Wang,“Graspnerf: computervisionandpatternrecognition,2022,pp.16263–16272.
Multiview-based6-dofgraspdetectionfortransparentandspecularob- [39] N.Mayer,E.Ilg,P.Ha¨usser,P.Fischer,D.Cremers,A.Dosovitskiy,
jectsusinggeneralizablenerf,”in2023IEEEInternationalConference and T. Brox, “A large dataset to train convolutional networks
onRoboticsandAutomation(ICRA). IEEE,2023,pp.1757–1763. for disparity, optical flow, and scene flow estimation,” in IEEE
[19] Z. Li, Y.-Y. Yeh, and M. Chandraker, “Through the looking glass: InternationalConferenceonComputerVisionandPatternRecognition
neural3dreconstructionoftransparentshapes,”inProceedingsofthe (CVPR), 2016, arXiv:1512.02134. [Online]. Available: http://lmb.
IEEE/CVFConferenceonComputerVisionandPatternRecognition, informatik.uni-freiburg.de/Publications/2016/MIFDB16
2020,pp.1262–1271. [40] I.LoshchilovandF.Hutter,“Decoupledweightdecayregularization,”
[20] H. Zhang, A. Opipari, X. Chen, J. Zhu, Z. Yu, and O. C. Jenkins, 2019.
“Transnet: Transparent object manipulation through category-level [41] “Middlebury stereo vision page,” https://vision.middlebury.edu/
poseestimation,”arXivpreprintarXiv:2307.12400,2023. stereo/.
[21] N.Mayer,E.Ilg,P.Hausser,P.Fischer,D.Cremers,A.Dosovitskiy, [42] L.Yang,B.Kang,Z.Huang,Z.Zhao,X.Xu,J.Feng,andH.Zhao,
and T. Brox, “A large dataset to train convolutional networks for “Depthanythingv2,”arXiv:2406.09414,2024.
disparity, optical flow, and scene flow estimation,” in Proceedings [43] X. Guo, J. Lu, C. Zhang, Y. Wang, Y. Duan, T. Yang, Z. Zhu,
of the IEEE conference on computer vision and pattern recognition, and L. Chen, “Openstereo: A comprehensive benchmark for stereo
2016,pp.4040–4048. matchingandstrongbaseline,”2023.
[22] H. Xu and J. Zhang, “Aanet: Adaptive aggregation network for effi- [44] G. Xu, X. Wang, X. Ding, and X. Yang, “Iterative geometry encod-
cient stereo matching,” in Proceedings of the IEEE/CVF Conference ing volume for stereo matching,” in Proceedings of the IEEE/CVF
onComputerVisionandPatternRecognition,2020,pp.1959–1968. Conference on Computer Vision and Pattern Recognition, 2023, pp.
[23] Y.Cao,J.Xu,S.Lin,F.Wei,andH.Hu,“Gcnet:Non-localnetworks 21919–21928.
meetsqueeze-excitationnetworksandbeyond,”inProceedingsofthe [45] H.Zhao,H.Zhou,Y.Zhang,J.Chen,Y.Yang,andY.Zhao,“High-
IEEE/CVF international conference on computer vision workshops, frequencystereomatchingnetwork,”inProceedingsoftheIEEE/CVF
2019,pp.0–0. Conference on Computer Vision and Pattern Recognition, 2023, pp.
[24] J.-R. Chang and Y.-S. Chen, “Pyramid stereo matching network,” in 1327–1336.
Proceedings of the IEEE conference on computer vision and pattern [46] X. Wang, G. Xu, H. Jia, and X. Yang, “Selective-stereo: Adaptive
recognition,2018,pp.5410–5418. frequencyinformationselectionforstereomatching,”inProceedings
[25] L.Lipson,Z.Teed,andJ.Deng,“Raft-stereo:Multilevelrecurrentfield oftheIEEE/CVFConferenceonComputerVisionandPatternRecog-
transformsforstereomatching,”in2021InternationalConferenceon nition,2024,pp.19701–19710.
3DVision(3DV). IEEE,2021,pp.218–227. [47] L.Zhang,K.Bai,G.Huang,Z.Chen,andJ.Zhang,“Multi-fingered
[26] Z. Teed and J. Deng, “Raft: Recurrent all-pairs field transforms robotic hand grasping in cluttered environments through hand-object
for optical flow,” in Computer Vision–ECCV 2020: 16th European contactsemanticmapping,”arXivpreprintarXiv:2404.08844,2024.
Conference,Glasgow,UK,August23–28,2020,Proceedings,PartII
16. Springer,2020,pp.402–419.
[27] Z.Li,X.Liu,N.Drenkow,A.Ding,F.X.Creighton,R.H.Taylor,and
M. Unberath, “Revisiting stereo depth estimation from a sequence-
to-sequence perspective with transformers,” in Proceedings of the
IEEE/CVF international conference on computer vision, 2021, pp.
6197–6206.
[28] P.-E. Sarlin, D. DeTone, T. Malisiewicz, and A. Rabinovich, “Su-
perglue: Learning feature matching with graph neural networks,” in
Proceedings of the IEEE/CVF conference on computer vision and
patternrecognition,2020,pp.4938–4947.
[29] J. Kim, M.-H. Jeon, S. Jung, W. Yang, M. Jung, J. Shin, and
A. Kim, “Transpose: Large-scale multispectral dataset for trans-
parent object,” The International Journal of Robotics Research, p.
02783649231213117,2024.
[30] J.Jiang,G.Cao,T.-T.Do,andS.Luo,“A4t:Hierarchicalaffordance
detection for transparent objects depth reconstruction and manipula-
tion,”IEEERoboticsandAutomationLetters,vol.7,no.4,pp.9826–
9833,2022.
[31] L. Zhu, A. Mousavian, Y. Xiang, H. Mazhar, J. van Eenbergen,
S. Debnath, and D. Fox, “Rgb-d local implicit function for depth
completion of transparent objects,” in Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, 2021, pp.
4649–4658.
[32] J. Ichnowski, Y. Avigal, J. Kerr, and K. Goldberg, “Dex-nerf: Using
a neural radiance field to grasp transparent objects,” arXiv preprint
arXiv:2110.14217,2021.