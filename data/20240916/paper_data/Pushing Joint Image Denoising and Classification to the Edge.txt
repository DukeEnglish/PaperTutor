Pushing Joint Image Denoising and Classification
to the Edge
Thomas C. Markhorst* , Jan C. Van Gemert , and Osman S. Kayhan
CV Lab, Delft University of Technology
{t.c.markhorst}@tudelft.nl
Abstract. In this paper, we jointly combine image classification and
image denoising, aiming to enhance human perception of noisy images
capturedbyedgedevices,likelow-lightsecuritycameras.Insuchsettings,
it is important to retain the ability of humans to verify the automatic
classificationdecisionandthusjointlydenoisetheimagetoenhancehu-
man perception. Since edge devices have little computational power, we
explicitly optimize for efficiency by proposing a novel architecture that
integrates the two tasks. Additionally, we alter a Neural Architecture
Search (NAS) method, which searches for classifiers to search for the
integrated model while optimizing for a target latency, classification ac-
curacy, and denoising performance. The NAS architectures outperform
our manually designed alternatives in both denoising and classification,
offering a significant improvement to human perception. Our approach
empowersuserstoconstructarchitecturestailoredtodomainslikemed-
ical imaging, surveillance systems, and industrial inspections.
Keywords: ImageDenoising·NeuralArchitectureSearch·ImageClassification
1 Introduction
The intersection of edge devices, such as security cameras, and deep learning
hassparkedaninterestinoptimizingneuralnetworksforinferencetime,further
referred to as latency. Common tasks to optimize for such efficiency are object
classification and object detection, which unlock automatic recognition. How-
ever, in noisy settings, the recognition accuracy might not be perfect and it is
important to allow the ability to validate the automatic recognition by human
inspection. Thus, in addition to automatic recognition, the perceptual quality
of the processed image is equally significant. In particular, this is relevant for
images containing noise, which can arise from various sources such as low-light
conditions, sensor noise, or other recording conditions. We focus on using an
efficient model that can be used on the edge with the aim of enhancing human
perception for validating the recognition output of noisy images.
Domains relying on human image perception but challenged by noisy im-
ages, like medical imaging [26], surveillance systems [35], and industrial in-
4202
peS
31
]VC.sc[
1v34980.9042:viXra2 T.C. Markhorst et al.
(a) (b) (c) (d)Cls:Human
Fig.1: We take a noisy image (a), which can be interpreted as an animal (b) or
human(c).Wedenoiseandclassifytheimage(a),aimingtoimprovehumanperception
resulting in (d). Note, in a real application (b) and (c) would not be available, which
increases the difficulty of interpreting the noisy image. Artist:DALL-E-2[31]
spections [11], can benefit from recently proposed denoising Convolutional Neu-
ral Networks (CNNs) [15,48]. As CNNs denoise better than traditional meth-
ods[8,14].FastCNNdenoisers[13,49]arerequiredtoaccommodatethereal-time
requirement of the affected domains. However, denoisers are not able to remove
all noise, which is not always enough for human image perception.
We further improve human understanding of the image by combining de-
noising with machine perception, like image classification. From the Human-
ComputerCooperationstrategiesin[41],weusetheClassifierasAidtoHuman.
Where the image classifier can be used as a direct advisor or an independent
agent to the security guard, the latter being analogous to a second opinion in
medical diagnosis. In different use cases, fusing skills of humans and computers
has been shown to improve performance beyond using only one type [2,24,41].
Therefore,weinvestigatemodelsthatcanleveragethebenefitsofbothdenoising
and classification to enhance human understanding in real time.
A model combining both denoising and classification is studied in [21], fo-
cusing on denoising performance. In addition, we optimize for efficiency, which
is required for edge devices, and classification. Our efficiency definition is based
ontwoelements:(i)latencyreductionwhile(ii)retainingdenoisingperformance
and classification accuracy. These elements could be optimized using indepen-
dent classification and denoising models. However, we propose an architecture
combining the tasks more efficiently.
First, we employ established model design approaches to enhance indepen-
dent denoising and classification models, such as model scaling [25,37] and ef-
ficient operators [33]. Although the models are optimized, they still operate
separately, resulting in unnecessary overhead. Hence we propose and compare
two methods that join both tasks, yielding a novel and efficient architecture.
Adjustingthisarchitectureforeachdeviceanddesiredlatencycanbelabori-
ousandrequiresexpertknowledge.Theseissueshaverecentlygarneredinterest,
leadingtotheemergenceofnewautomatedarchitecturesearchtechniques,which
have achieved competitive results in image classification [38,42]. Moreover, re-
cent Neural Architecture Search (NAS) approaches incorporate latency in theirPushing Joint Image Denoising and Classification to the Edge 3
loss function, enabling the design of architectures tailored to specific latency
requirements [16,19,42]. Combining NAS with the proposed architecture pro-
vides a seamless and efficient approach to designing denoising and classification
models for diverse use cases.
We find that our proposed efficiency-focused architecture consistently out-
performs our more straightforward one. This is observed for both the manually
andNASdesignedmodels.Inaddition,ourNASmodelssignificantlyoutperform
the manually designed ones in denoising and classification performance.
Wehavethefollowingcontributions.(i)Weintroduceanovelarchitectureto
efficientlycombinedenoisingandclassification.Thenoveltyliesinsharinganen-
coderbetweenthedenoiserandtheclassifier.(ii)Weproposemodificationstoan
existing NAS method for classification [19] to stabilize its search, improving the
performanceofthefoundarchitectures.(iii)WeextendanexistingNASmethod
to search for a model that combines denoising and classification, optimized for
a target latency, classification accuracy, and denoising performance.
Since no prior work proposes a joint efficient model for denoising and classi-
fication,westudythetasks bothseparatelyandjointinSec.3.Thefindingsare
used as expert knowledge to construct the NAS method in Sec. 4.1
2 Related work
Denoising. Image denoising aims to reconstruct a clean image x from its ob-
served noisy variant y. This relation can be formulated as y =x+n, where we
assume n to be additive white Gaussian noise (AWGN). Neural network-based
denoisers offer faster inference and good performance compared to traditional
denoisingmethodslikeBM3D[8]andWNNM[14].Theinterestindeeplearning
for denoising started with DnCNN [48], a simple Convolutional Neural Network
(CNN). Encoder-decoder architectures became popular due to their efficient
hierarchical feature extraction. Specifically, UNet [32] whose skip-connections
between the encoder and decoder enhance the denoising process as shown in
follow-upmethods[15,23,29].TheinterestintheUNetstructurecontinueswith
transformerarchitectures[12,39].Inthispaper,ourdenoisersarebasedonUNet,
ensuring our findings can translate to most related work.
Efficient classification. Optimization for efficiency is generally achieved
by either compressing pre-trained networks [28] or designing small networks di-
rectly [33,38]. We focus on efficient design, for which handcrafted models and
neural architecture search (NAS) play essential roles. Studies proposing hand-
crafted models often introduce efficient operators [17,33,50] or scaling meth-
ods[37].TheseefficientoperatorsareusedinNASmethods[38,42]aimingforthe
automated design of efficient neural networks. Such an operator is the inverted
residualwithalinearbottleneck(MBConv),asintroducedinMobileNetV2[33].
Inourmodels,westudyscalingmethodsandMBConv’sefficiencycharacteristic.
Neural Architecture Search. The use of reinforcement learning (RL) for
neural architecture search introduced efficient architectures with competitive
1 Project site: https://thomas-markhorst.github.io4 T.C. Markhorst et al.
classification performance [16,30,36,38]. However, their discrete search space is
computationally expensive. Differentiable NAS (DNAS) methods [3,22,42] sig-
nificantly reduce this cost by relaxing the search space to be continuous using
learnablevectorsαforselectingcandidateoperations,whichallowsforgradient-
based optimization. The popularity of DNAS started with DARTS [22], which
searches a cell structure. Due to the complex design and repetitiveness through-
out the network of the cell structure, follow-up works [19,42] search operators
for every layer instead of constructing repeating cells.
Pitfalls of DNAS are the collapse of search into some fixed operations and a
performance drop when converting from the continuous search network to the
discretizedinferencenetwork[6,45,46].TF-NAS[19]addressestheseissueswith
an adaptation in the search algorithm, which lets the search model mimic the
discretebehavioroftheinferencemodel.Inaddition,TF-NASsearchesanarchi-
tecturewithatargetlatencybyaddingalatencylosstothesearchoptimization.
Because of these properties, we use TF-NAS as a baseline for our NAS study.
Existing NAS methods for denoising are either not reproducible [27], have a
cell-based search space [47], or do not have an encoder-decoder [5] architecture.
Instead, we use a layer-based search space and encoder-decoder structure.
Jointclassificationanddenoising.In[43],thepositiveinfluenceofdenois-
ing methods on classification performance is discussed. Moreover, [21] proposed
a joint model where a VGG classifier [34] is attached to a denoiser similar to
UNet.Thismethod[21]reportsaqualitativeimprovementofthedenoisedimages
when adding the classification loss to the denoiser’s optimization, whereas [43]
reportsaquantitativeimprovement.Althoughthesemodelsdenoiseandclassify
well,theyarenotoptimizedforefficiency.Inthispaper,wedesignajointimage
denoising and classification method for edge devices.
3 Exploiting Expert Knowledge
We start in a controlled setting with separate baseline models for classification
and denoising. Additionally, methods to increase their respective efficiency are
studied, resulting in a reduced version of the baseline denoiser and classifier.
Both the construction and efficiency improvement of the models are described
in Suppl. A, where a UNet (Fig. 2) and simple 2-block CNN (Fig. 2.i and 2.ii)
are used as baseline denoiser and classifier respectively. This section describes
how the different sizes of the classifiers and denoisers are used to study joining
methods and their efficiency.
Dataset & settings. For the experiments in this section, we generate a
controlled synthetic data set to study the behavior of the classifier and denoiser
when applying model scaling, replacing the convolutional operations, and com-
bining both models. The dataset consists of 30k images, each with a random
constant background in a gray tint [0.1 - 0.3] with two randomly placed non-
overlapping MNIST [10] digits. We use two digits to increase the complexity of
the denoising task. For experiments including classification, the two digits are
extracted from the image using ground truth locations. These extracted digitsPushing Joint Image Denoising and Classification to the Edge 5
b Encoder
U-Net denoiser (i) Sequential
Decoder
Classifier
c=2 Linear
Conv
Skip
m⋅b Max pool
UpConv
or
(ii) Integrated
Fig.2: AUNet,withhyperparametersbasefeaturemapwidth(b),depth(d),channel
multiplier (m) and convolutions per layer (c). For the joint model either attach the
classifier to form (i) the Sequential model or (ii) the Integrated model.
are separately used as input for the classifier. In the experiments where noise
is required, for either denoising or noisy classification, synthetic Gaussian noise
is added. This noise is zero mean, and the intensity of the noise is controlled
using the standard deviation (σ) of the distribution. Fig. 3a shows a sample,
and Fig. 3b its noisy variant. To test the model behavior on an extensive noise
range,everymodelistrainedandtestedonelevenσ valuesevenlyspacedonthe
interval [0,1] (Tabs. 1, 7 and 8). The models are trained using Adam optimizer
⋅
with 1E-3 learning rate (LR), plateau LR scheduler, and 100 epochs.
Since the experiments with the controlled data set are not targeted at a
specific device, the metric defining efficiency should not depend on a device.
Suchametriciscomputationalpower,mostcommonlydefinedasFloatingPoint
Operations (FLOPs), which we use as the primary metric. Despite being device
dependent, we assess latency as a secondary metric. The latency is measured
with a batch size of 32, 100 warm-up inference passes and averaged over 1000
inference passes. Classification performance is quantified using accuracy, while
fordenoisingperformancethePeakSignal-to-NoiseRatio(PSNR)andStructural
SimilarityIndex(SSIM)metrics[40]areused.Higherisbetterforallourmetrics.
3.1 Joint model: DC-Net
Experimentalsetup.Weconstructabaselineandreducedjointmodel,Denoising-
Classifying Network (DC-Net). Both the baseline and reduced DC-Net use the
sameclassifier(MB2.5-MSuppl.A).WhereasUNet-SandUNetareusedforthe
reduced and baseline DC-Net respectively.
Forjoiningthedenoiserandclassifier,weproposetwomodels:(i)aSequential
model where the classifier is attached after the denoiser (Fig. 2.i), and (ii) an
Integrated model, the classifier is attached to the UNet encoder (Fig. 2.ii). For
the Integrated model, classification and denoising branches share the encoder.
The benefits of the Integrated model could come in threefold. First, using a
shared encoder removes the need for a second large classifier, as in the Sequen-
tial method. Second, the decoder and classifier branches could run in parallel
comparedtorunningsequentially,whichcanresultinlowerlatency.Thirdly,the
3=d6 T.C. Markhorst et al.
(a) GT (b) σ=0.8 (c) Int. S (d) Seq.S (e) Int. L (f) Seq.L
Fig.3: Ground-truthsample(a),whichisthetargetforthedenoiserwhengivennoisy
image(b).SstandsforthereducedmodelandLforthebaseline.(c-f)arethecropped
denoisedoutputsforinput(b)andtheredsquaresindicatethezoomed-inregions.For
higher noise levels, the denoising performance of the Sequential model is worse than
the Integrated model.
decoder is only optimized for denoising, since the optimization of the classifier
does not influence it anymore. It can result in better image quality.
The models are trained using a weighted combination of the Cross-Entropy
and Charbonnier loss [1,20] (Eq. 1). We report the metrics averaged over all 11
noise levels, σ in [0, 1].
L=0.1·L +0.9·L (1)
CE Char
Exp 1. Integrated vs. Sequential.Whichjoiningmethodperformsbetter
for the baseline, and does the same conclusion hold when reducing its size?
We compare the Sequential and Integrated models. In Tab. 1, we see that for
both the baseline and reduced DC-Net models, the Integrated version performs
significantly better atdenoising, while the Sequential version performsbetter at
classification.ThedifferenceindenoisingperformanceisvisualizedinFig.3.We
see that both the reduced (3c) and baseline (3e) Integrated models reconstruct
the digit clearly. Whereas both sizes of the Sequential model (3d and f) fail to
reconstruct the digit.
Conclusion. The integrated model has a slightly lower classification accu-
racycomparedtotheSequentialmodel,yetithassuperiorperformanceinterms
ofimagequality.Whenaimingforimprovedhumanperception,itisstillrequired
for the human to see the content of the image. Therefore, the Integrated model
is more suitable for joint denoising and classification and is called DC-Net.
Table 1: Comparison of the reduced and baseline joint models. Both the Integrated
and Sequential methods trained on the synthetic noise dataset. The integrated model
performssignificantlybetterindenoisingandslightlyworseinclassification.Theinte-
grated model also scales down better.
DC-Net Type FLOPs(M)↓Lat.(ms)↓PSNR↑SSIM↑Acc.(%)↑
Integrated 1301.8 7.14 32.8 0.97 88.1
Baseline
Sequential 1302.1 7.55 27.1 0.95 89.6
Integrated 51.2 2.41 29.9 0.97 86.2
Reduced
Sequential 51.5 2.83 25.2 0.92 87.6Pushing Joint Image Denoising and Classification to the Edge 7
Learned Removed 88.5 88.0
# Operations
0.8 M MB B- -k k3 3- -e e3 6 88.0 4 6 87.5 C-NAS M (ours)
M MB B- -k k5 5- -e e3 6 87.5 8 87.0 MobileNetV2 0.6 86.5
87.0
86.0
0.4 86.5 TF-NAS C
85.5
86.0 85.0 ResNet-18
0.2
85.5 84.5
0.0 0 25 50 75 0 25 50 75 85.0 6.0 8.0 12.0 84.0 7.2 7.4 7.6 7.8 8.0 8.2 8.4 8.6
Epochs Epochs Target Latency (ms) Latency (ms)
Fig.4: Stage-5:block-4’s Fig.5: Acc for different Fig.6: Comparing clas-
α values for Removed and search spaces, showed for sifiers with similar la-
Learned β. Search is more different target latencies. tency on Imagenet100.
stable when β is removed. Using fewer operations is Our model outperforms
the most robust. other methods.
4 Neural Architecture Search
We follow similar experimentation strategies as in the previous section. TF-
NAS[19]isusedtoconstructaclassifier,whichweuseasabasisforourdenoiser
and joint model. All our proposed models in this Section contain searchable
blocks, the models and which parts are searchable are defined in Figure 7.
Dataset & settings.ThefollowingexperimentsareconductedonImagenet
[9],randomlycroppedto224x224pixels.Toreducesearchandtrainingtime,100
classes (Imagenet 100) from the original 1000 classes were chosen, as in [19]. In
the experiments requiring noise, Gaussian noise is sampled uniformly with a
continuous range of σ in [0,1] (Tabs. 2, 3, 4, 5 and 6).
The models are searched using SGD with momentum, 2E-2 LR with 90
epochs. Afterward, the found architecture is trained from scratch with 2E-1 LR
for 250 epochs. All other settings are similar to [19]. The loss function depends
on the task of the experiment, Cross-Entropy with label smoothing for classifi-
cation (L ), combined Charbonnier and SSIM losses for denoising (L ), and
CE Den
a weighted combination for the joint model (L ), see Eq. 2, 3.
Both
L =0.8·L +0.2·L (2)
Den Char SSIM
L =0.1·L +0.9·L (3)
Both CE Den
Since our NAS method uses a latency look-up table constructed for our de-
vice,theseexperimentstargetaspecificdevice,GeForceRTX3090GPU.There-
fore latency is suitable for defining efficiency in the NAS experiments.
4.1 Classification: C-NAS
Experimental Setup. Since TF-NAS [19] learns β’s to control the number of
convolutional operators per stage, β’s can reduce the model size. However, in
eulav
)%(
ycaruccA
)%(
ycaruccA8 T.C. Markhorst et al.
the models proposed by [19], only 2 out of 24 stages are reduced by β. So the
β’s have little effect on the found architectures, yet they make the search space
more complex. Therefore, we propose a version of TF-NAS where the β’s are
removed so that all convolutional blocks are used.
The candidate operations in the search space of TF-NAS are MBConvs with
8 different configurations, see Suppl. B. The configurations differ in kernel size,
expansion rate, and in- or excluding a squeeze- and excitation layer (SE) [18].
The classification experiments are performed using data without noise, as
the aim is to examine the NAS method, which is designed for clean images.
We investigate key components of TF-NAS and try to improve its stability and
classification performance.
Exp 1. Learned vs. Removed β. We conduct an experiment to study the
effect of removing β on the search quality. The SE-layer is excluded from the
candidate blocks, halving the search space to ensure the number of candidate
operations does not cause search instability. We set a low target latency of 6
ms, as learning β should have a positive effect on small networks. For both the
learned and removed settings, we run two searches, search 1 and 2.
Fig. 4 shows that when β is learned, the α’s selecting a candidate operation
oscillateandthereforedonotdecideonanarchitecture.WhereaswithRemoved
β, the search is stable. This stability is reflected in the performance, as the
average accuracy of the Removed β models is 86.3%, compared to 84.2% for
Learned β. The separate results for each model are shown in Suppl. C.
Exp2.Numberofoperatorsinsearchspace.Doesreducingthenumber
of operators during search positively influence the performance of the found
models? We test this by comparing the performance of architectures searched
with three different search space sizes, {4, 6, or 8} operations, defined in Suppl.
B. For each of these search spaces, three different latency targets are used: {6,
8, and 12} ms.
In Fig. 5, we see that for lower target latencies, 6 and 8 ms, using fewer
operations in the search space does not alter performance significantly. When
targeting 12 ms latency, reducing the number of operations in the search space
does show a significant improvement. Additionally, we find that when using the
largersearchspaces,theoperatorsfromthesmallsearchspacearestillpreferred
for lower latencies.
Exp 3. Compare with original TF-NAS. How do architectures found
usingourproposedchangestoTF-NASperformcomparedtomodelswithsimilar
latency? We compare our model, C-NAS M, with TF-NAS C, MobileNetV2,
and ResNet-18. MobileNetV2 our model have similar latency, architecture, and
operatortypes.ResNetonlydiffersinthatitusestheConvoperator.Weinclude
these standard baseline architectures to indicate where C-NAS, see Fig. 7.i,
stands on Imagenet100.
Fig. 6 shows that the model found using our method has lower latency yet
higher accuracy than TF-NAS C as proposed in [19]. The model is searched
with target latency 8.0. We observe that our search method is able to find a
model that matches its target latency. Although ResNet-18 and MobileNetV2Pushing Joint Image Denoising and Classification to the Edge 9
runfasterthanourmodel,ourclassificationaccuracyissuperior,especiallywhen
compared to ResNet-18, which only uses Convs.
Conclusion. By Removing β and reducing the number of operators used
in the search, the search stability increases, and we find architectures that have
better accuracy. An architecture found using our changes classifies better than
a TF-NAS architecture with similar latency.
The comparison between our model and ResNet-18 shows that our search
space is able to compete with widely accepted Conv-based classifiers. Moreover,
our model performs on par with MobileNetV2, a manually designed classifier
using MBConvs.
4.2 Denoising: D-NAS
Experimental setup. To construct a denoiser, D-NAS (Fig. 7.ii), we use the
first six stages of a found C-NAS classifier, which has four levels of resolution.
Afterwards, we attach a UNet style decoder by using both a transposed convo-
lution and two normal convolutions for each decoder level. Like UNet, we also
add skip connections between the encoder and decoder layers. The decoder is
not searched.
Exp 1. D-NAS vs UNet denoiser. Does our denoiser D-NAS perform
similarly to the UNet denoisers? For this experiment, we use UNet-S (Sec. A.2)
{d =4,b =8,c =2,m =1.5},withalatencyof9.2msandthelargerUNet-M,
{d = 4, b = 16, c = 2, m = 2} with a latency of 16.9 ms. We compare them
with our D-NAS M, with similar latency.
Table 2: ComparisonofD-NASandUNetvariantsfordenoising.D-NASoutperforms
slightly faster UNet-S, but UNet-M denoises best at the cost of 45% higher latency.
UNetparams:
Model Lat.(ms)↓PSNR↑SSIM↑
d b m
UNet-S 4 8 1.5 9.2 25.0 0.69
UNet-M 416 2 16.9 25.9 0.72
D-NASM - - - 11.6 25.6 0.71
Tab. 2 shows that D-NAS M outperforms UNet-S by 0.6 dB PSNR and 2%
SSIM, at the cost of 2.4 ms latency. However, the 7.7 ms slower UNet variant,
UNet-M, denoises better than our proposed model, by 0.3 dB and 1% SSIM.
Conclusion. D-NAS performs similarly to our baseline UNets. Therefore
D-NAS is a suitable denoising architecture and it can form the backbone of our
Integrated model.
4.3 Joint Model: DC-NAS
Experimental setup. To construct the joint model, we use the Integrated
setup. The Integrated model, DC-NAS, is constructed similarly to D-NAS. We10 T.C. Markhorst et al.
Table 3: Comparison of DC-NAS models searched for three different latencies, with
their corresponding C-NAS model, classifier baseline, and denoiser baseline. Our Inte-
grated models perform similar or better than their corresponding baselines, with the
advantage of having a joint denoising and classification network. *See Suppl. A.2.
Classification Denoising
Model Type Lat.(ms)↓
Acc.(%)↑ PSNR↑SSIM↑
MobileNetV3[16] Classifier 4.9 70.4 - -
C-NASS(ours) Classifier 5.9 73.5 - -
UNet-S*[32] Denoiser 9.2 - 25.0 0.69
DC-NetS(ours) Integrated 10.0 61.9 24.5 0.68
DC-NASS(ours) Integrated 10.3 74.3 25.4 0.70
EfficientNetV2-b0[38]Classifier 9.0 75.4 - -
C-NASM(ours) Classifier 7.9 75.5 - -
LPIENet0.25x[7] Denoiser 12.7 - 24.1 0.65
DC-NASM(ours) Integrated 13.7 76.0 25.4 0.70
EfficientNetV2-b1[38]Classifier 11.8 76.7 - -
C-NASL(ours) Classifier 12.0 76.0 - -
UNet-M*[32] Denoiser 16.9 - 25.9 0.72
LPIENet0.5x[7] Denoiser 19.8 - 24.7 0.68
DC-NASL(ours) Integrated 17.9 76.4 25.2 0.70
connect the decoder after the first six stages of C-NAS (Fig. 7, but still use the
remainingC-NASstages(Fig.7.iii))asaclassificationbranch.Thedesignchoices
for DC-NAS are discussed in the ablations study (Sec. 4.4).
Using our search method, we search for DC-NAS models of three different
sizes{S,M,L}.ApartfromourmanuallydesignedIntegratedmodel,wecompare
our searched models with separate state-of-the-art classifiers and denoisers, as
there are no existing models that jointly optimize denoising, classification, and
efficiency. For each DC-NAS model, we also separately train the classifier (C-
NAS) to evaluate the influence of joint denoising. The classifier and denoiser
baselines are chosen to have similar latency as the C-NAS or D-NAS model on
which the corresponding DC-NAS is based.
Results. We discuss the results in Tab. 3 in three separate sections for the
different target latencies. Our smallest Integrated model, DC-NAS S, outper-
forms both of its classifier baselines MobileNetV3 and C-NAS S. Note, that the
latter shows that the classifier, C-NAS S, performs better when integrated into
DC-NAS S. Moreover, our Integrated model denoises better than its baseline
UNet-S (Suppl. A). DC-NAS S also significantly outperforms our manually de-
signed DC-Net S (Reduced), which is the only Integrated baseline. We display
the denoising results of DC-Net, UNet-S, and DC-NAS S in Fig. 8. We observe
better denoising on smooth areas, sharper edges, and more realistic color recon-
struction for DC-NAS S.
The results of DC-NAS M follow a similar pattern, where our DC-NAS out-
performs its baseline denoiser and classifier, using C-NAS M in the Integrated
model boosts accuracy by 0.5%. When comparing DC-NAS M to DC-NAS S,Pushing Joint Image Denoising and Classification to the Edge 11
(ii) D-NAS (iv) Sequential
Decoder 1 C-NAS
3x3 Conv
(i) C-NAS
MBConv Decoder 2
3x3 Conv
Stage 3 Decoder 3
MBConv
Stage 4 Decoder 4 Stage 3
Stage 4
Stage 5
Stage 5
Stage 6
Decoder 4
Stage 6
Decoder 1-4: (iii) Integrated
ConcatDeneactoederC 4onv x2
Stage 7 Stage 7
Up Conv
Stage 8 Stage 8
Encoder - Searchable 1x1 Conv 1x1 Conv
Encoder - Manually picked
Decoder - Manually picked Pool + Linear Pool + Linear
Fig.7: C-NASandD-NASarchitecture.Connectingblock(iii)forDC-NASandblock
(iv)forDC-NAS .Duringsearchmodelswithvariouslatenciescanbeobtained.Only
seq
the orange stages are searchable in the encoder and classifier.
the classification performance improves by 1.7%, yet the denoising performance
plateaus. LPIENet denoises the worst. Comparing DC-NAS M’s denoising per-
formance to 3.2 ms slower UNet-M we observe slightly worse denoising perfor-
mance.However,ourIntegratedmodeldenoisesandclassifieswithlowerlatency.
ForDC-NASL,weobservethatboththeclassificationanddenoisingbaseline
slightly outperform our Integrated model. EfficientNetV2-b1 has 0.3% higher
classification accuracy than DC-NAS L, and UNet-M improves with 0.7 dB
PSNR and 2% SSIM. However, our Integrated model performs both denoising
and classification at a similar latency as UNet-M, which only denoises. When
comparingDC-NASLwithDC-NASM,weagainnoteanimprovementinclassi-
fication performance. However, the PSNR score drops by 0.2 dB while the more
important SSIM score remains at 0.70.
Conclusion. Our results demonstrate that the proposed DC-NAS models
perform similar or better than their denoising and classification baselines for
their target latency. In addition, the searched model performs better than our
manually designed joint denoiser and classifier.
4.4 Ablation Study
Exp 1. Encoder search. C-NAS forms the encoder of DC-NAS and contains
the searchable operations within DC-NAS. We test multiple search approaches:12 T.C. Markhorst et al.
(i) using clean images, and (ii) using noisy images. For both approaches, we
searchtheencoderusingonlyclassificationlossL .Inaddition,wealsosearch
Cls
the DC-NAS encoder on noisy images using the combined denoising and classi-
fication loss L . Therefore it searches for the optimal encoder for both tasks
Both
within the Integrated model DC-NAS. Regardless of the search method, the
found models are trained using noisy images and the combined loss.
Tab.4,showsthatwhenusingL withnoisyimagesduringsearchimproves
Cls
classification accuracy by 0.3%. Surprisingly, the denoising performance is the
same. Using both the denoising and classification objectives during the search
reduces the classification accuracy. Caused by the denoising loss complicating
the search, without improving denoising performance. Therefore, we search our
DC-NAS models by only using L loss.
Cls
Exp 2. Compare Integrated vs. Sequential. We compare DC-NAS and
DC-NAS models with similar latency. Where DC-NAS is our Sequential
seq seq
model, which is constructed by attaching C-NAS to the output of D-NAS, see
Fig. 7.iv. Since the searched classifier is used twice in DC-NAS , the Sequen-
seq
tial model has a higher latency than the Integrated variant. To counter this, a
smaller C-NAS model is used in both the encoder and classifier of DC-NAS .
seq
The classifier, C-NAS, used to construct DC-NAS L has a latency of 6.7 ms.
seq
Whereas in DC-NAS L, the classifier has a latency of 12 ms. Note, that these
models were searched by using clean instead of noisy images, as this holds for
both models it is still a fair comparison.
We see that both models have similar latency and the same classification ac-
curacy,however,DC-NASLimprovesdenoisingperformancewith0.5dBPSNR
and 1% SSIM (Tab. 5). This improvement is caused by DC-NAS L’s Integrated
design as this allows for a bigger encoder without increasing latency.
Table 4: Different search strategies for DC-NAS, using (i) clean or noisy images and
(ii) L or L +L . Searching on Noisy images with only L performs best.
Cls Cls Den Cls
Search
Lat.(ms)Acc.(%)↑SSIM↑PSNR↑
ImagesLoss
Clean L 13.9 75.7 25.4 0.70
Cls
Noisy L 13.7 76.0 25.4 0.70
Cls
Noisy L 13.8 75.5 25.4 0.70
Both
Exp 3. Decoder tuning.TheDC-NASmodelsfoundinTab.3and5,have
similardenoisingperformance.ThesemodelsdifferonlyinthetypeofMBConvs
that are selected during search in the encoder. We test the hypothesis if the
denoising performance is influenced by adjusting the operators in the decoder
whileretainingthelatency.DC-NASMisusedasabasisinthisexperiment.We
construct three alternatives. First, the convolutional operators in the decoder
are replaced with MBConvs (MB-k3-e3) [33], which significantly increases the
latency of the model. To account for this, we scale down the decoder by (i)Pushing Joint Image Denoising and Classification to the Edge 13
Table 5: ComparingSequentialandIntegratedDC-NAS,classificationperformanceis
similar, yet the Integrated model is faster and denoises better.
Model Lat.(ms)Acc.(%)↑PSNR↑SSIM↑
DC-NAS L 18.3 76.0 25.0 0.69
seq
DC-NASL 17.9 76.0 25.5 0.70
(a) σ=0.1 (b) DC-Net (c) UNet-S (d) DC-NAS (e) GroundTruth
(f) σ=0.2 (g) DC-Net (h) UNet-S (i) DC-NAS (j) GroundTruth
Fig.8: Denoising performance of DC-NAS S and its baselines. Left to right: noisy
image, the denoiser outputs of size S, and the clean image. Comparing (d) and (b,c),
we see better performance in smooth areas and more correct colors in (d). With (i)
and(g),weobserveabettercolorreconstructionfor(i).Moreover,(i)haslessartifacts
than(h).Hence,DC-NASSdenoisesbetterthantheotherdenoisersofsimilarlatency.
using 1 instead of 2 convolutional operations (MBConv) per layer or (ii) using 3
instead of 4 decoder layers.
In Tab. 6, we see that using the MBConvs compared to Convs improves the
denoising performance. However, at the cost of a 14 ms latency increase, only
causedbytheMBConvdecoder.WhenreducingthecomplexityoftheMBConv
decoder with 1 operator and 3 layers, the denoising performance reduces to the
original level again, but the latency is still higher than for DC-NAS M which
has only standard convolutional layers in the decoder block.
Conclusion. We have seen that the Integrated combining method outper-
formsitsSequentialcounterpartindenoising.Toconstructtheintegratedmodel
(DC-NAS), we find that searching for a classifier on noisy data, without taking
thedenoisingobjectiveintoaccountresultsinthebestclassificationperformance.14 T.C. Markhorst et al.
Table 6: The influence of altering the Conv operators in the DC-NAS M decoder
to MBConv and scaling down the MBConv alternative by reducing the number of
operators or decoder layers. Using the standard convolutional layers is more efficient.
Decoder
Lat.(ms)Acc.(%)↑PSNR↑SSIM↑
Operator Scaling
Conv - 13.7 76 25.4 0.70
MBConv - 27.7 75.5 25.8 0.71
MBConv 1operator 16.4 75.4 25.3 0.70
MBConv 3layers 22.1 75.1 25.4 0.70
Surprisingly, the search method does not influence the denoising performance.
Furthermore,manuallyalteringthedecoderdoesnotbenefitdenoisingefficiency
either. However, the NAS denoising experiments demonstrate that our denois-
ing setup is competitive. Since tuning the decoder operators does not improve
performance, our method is focused on searching for only the encoder of the
integratedmodel.Themodelsfoundbythisapproach,outperformourmanually
designed models with similar latency.
5 Limitations & Conclusion
One limitation of our NAS method is its inability to alter the decoder. It is
designed this way as manually altering the decoder does not improve efficiency.
However, when targeting a significantly different latency, a change in denoising
architecture could be required. Therefore, designing model scaling rules for the
searched models is of interest, similar to the EfficientNets [37,38].
Anotherlimitationisthefixationofβ inourNASmethod.Althoughthisim-
provesthestabilityofsearchandnetworkperformance,learningβwhileretaining
a stable search would be preferred. This would introduce more possibilities in
the search space for optimizing efficiency.
In addition, the latency of Integrated models can be reduced further by run-
ning the denoising and classification branches in parallel.
To conclude, we show that using efficient operators and scaling methods
proposed in previous work [25,33,37] are relevant for denoising and noisy clas-
sification. In addition, we present the Integrated model DC-Net to join the two
tasksefficientlyandshowthattheIntegrateddesignismoresuitableacrossvar-
ious latencies than the Sequential variant. To simplify the design process of the
joint model when targeting a latency, we present a NAS method. We alter an
existing NAS method to improve the stability and performance of the search.
This method searches a classifier. Using the searched classifier as a basis, we
build the Integrated DC-NAS model. We demonstrate that the proposed model
outperforms the manually constructed model. We believe that our study can be
a precursor of efficient joint low-level and high-level computer vision tasks.Pushing Joint Image Denoising and Classification to the Edge 15
References
1. Barron,J.T.:Amoregeneralrobustlossfunction.CoRRabs/1701.03077(2017),
http://arxiv.org/abs/1701.03077
2. Bosch, N., D’Mello, S.K.: Can computers outperform humans in detecting user
zone-outs? implications for intelligent interfaces. ACM Trans. Comput.-Hum. In-
teract. 29(2) (jan 2022). https://doi.org/10.1145/3481889, https://doi.org/
10.1145/3481889
3. Cai, H., Zhu, L., Han, S.: Proxylessnas: Direct neural architecture search on tar-
get task and hardware. CoRR abs/1812.00332 (2018), http://arxiv.org/abs/
1812.00332
4. Charbonnier, P., Blanc-Feraud, L., Aubert, G., Barlaud, M.: Two deterministic
half-quadraticregularizationalgorithmsforcomputedimaging.In:Proceedingsof
1stInternationalConferenceonImageProcessing.vol.2,pp.168–172vol.2(1994).
https://doi.org/10.1109/ICIP.1994.413553
5. Cheng, A., Wang, J., Zhang, X.S., Chen, Q., Wang, P., Cheng, J.: DPNAS:
neural architecture search for deep learning with differential privacy. CoRR
abs/2110.08557 (2021), https://arxiv.org/abs/2110.08557
6. Chu,X.,Zhou,T.,Zhang,B.,Li,J.:FairDARTS:eliminatingunfairadvantagesin
differentiablearchitecturesearch.CoRRabs/1911.12126(2019),http://arxiv.
org/abs/1911.12126
7. Conde, M.V., Vasluianu, F., Vazquez-Corral, J., Timofte, R.: Perceptual im-
age enhancement for smartphone real-time applications. In: Proceedings of the
IEEE/CVFWinterConferenceonApplicationsofComputerVision(WACV).pp.
1848–1858 (January 2023)
8. Dabov, K., Foi, A., Katkovnik, V., Egiazarian, K.: Image denoising by sparse 3-d
transform-domain collaborative filtering. IEEE Transactions on Image Processing
16(8), 2080–2095 (2007). https://doi.org/10.1109/TIP.2007.901238
9. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-
scale hierarchical image database. In: 2009 IEEE conference on computer vision
and pattern recognition. pp. 248–255. Ieee (2009)
10. Deng, L.: The mnist database of handwritten digit images for machine learning
research. IEEE Signal Processing Magazine 29(6), 141–142 (2012)
11. Dey, B., Halder, S., Khalil, K., Lorusso, G., Severi, J., Leray, P., Bayoumi,
M.A.: SEM image denoising with unsupervised machine learning for better de-
fect inspection and metrology. In: Adan, O., Robinson, J.C. (eds.) Metrology,
Inspection, and Process Control for Semiconductor Manufacturing XXXV. vol.
11611, p. 1161115. International Society for Optics and Photonics, SPIE (2021).
https://doi.org/10.1117/12.2584803, https://doi.org/10.1117/12.2584803
12. Fan,C.M.,Liu,T.J.,Liu,K.H.:Sunet:Swintransformerunetforimagedenoising.
In: 2022 IEEE International Symposium on Circuits and Systems (ISCAS). IEEE
(May 2022). https://doi.org/10.1109/iscas48785.2022.9937486, http://dx.
doi.org/10.1109/ISCAS48785.2022.9937486
13. Gu,S.,Li,Y.,Gool,L.V.,Timofte,R.:Self-guidednetworkforfastimagedenoising.
In: Proceedings of the IEEE/CVF International Conference on Computer Vision
(ICCV) (October 2019)
14. Gu, S., Zhang, L., Zuo, W., Feng, X.: Weighted nuclear norm minimization with
application to image denoising. In: 2014 IEEE Conference on Computer Vision
andPatternRecognition.pp.2862–2869(2014).https://doi.org/10.1109/CVPR.
2014.36616 T.C. Markhorst et al.
15. Gurrola-Ramos, J., Dalmau, O., Alarcón, T.E.: A residual dense u-net neu-
ral network for image denoising. IEEE Access 9, 31742–31754 (2021). https:
//doi.org/10.1109/ACCESS.2021.3061062
16. Howard,A.,Sandler,M.,Chu,G.,Chen,L.,Chen,B.,Tan,M.,Wang,W.,Zhu,Y.,
Pang, R., Vasudevan, V., Le, Q.V., Adam, H.: Searching for mobilenetv3. CoRR
abs/1905.02244 (2019), http://arxiv.org/abs/1905.02244
17. Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., An-
dreetto, M., Adam, H.: Mobilenets: Efficient convolutional neural networks for
mobile vision applications. CoRR abs/1704.04861 (2017), http://arxiv.org/
abs/1704.04861
18. Hu, J., Shen, L., Sun, G.: Squeeze-and-excitation networks. CoRR
abs/1709.01507 (2017), http://arxiv.org/abs/1709.01507
19. Hu, Y., Wu, X., He, R.: TF-NAS: rethinking three search freedoms of latency-
constrained differentiable neural architecture search. CoRR abs/2008.05314
(2020), https://arxiv.org/abs/2008.05314
20. Ignatov,A.,Timofte,R.,Liu,S.,Feng,C.,Bai,F.,Wang,X.,Lei,L.,Yi,Z.,Xiang,
Y., Liu, Z., Li, S., Shi, K., Kong, D., Xu, K., Kwon, M., Wu, Y., Zheng, J., Fan,
Z., Wu, X., Zhang, F., No, A., Cho, M., Chen, Z., Zhang, X., Li, R., Wang, J.,
Wang,Z.,Conde,M.V.,Choi,U.J.,Perevozchikov,G.,Ershov,E.,Hui,Z.,Dong,
M., Lou, X., Zhou, W., Pang, C., Qin, H., Cai, M.: Learned smartphone isp on
mobile gpus with deep learning, mobile ai & aim 2022 challenge: Report (2022),
https://arxiv.org/abs/2211.03885
21. Liu, D., Wen, B., Jiao, J., Liu, X., Wang, Z., Huang, T.S.: Connecting image
denoising and high-level vision tasks via deep learning. CoRR abs/1809.01826
(2018), http://arxiv.org/abs/1809.01826
22. Liu,H.,Simonyan,K.,Yang,Y.:DARTS:differentiablearchitecturesearch.CoRR
abs/1806.09055 (2018), http://arxiv.org/abs/1806.09055
23. Liu,P.,Zhang,H.,Zhang,K.,Lin,L.,Zuo,W.:Multi-levelwavelet-cnnforimage
restoration. CoRR abs/1805.07071 (2018), http://arxiv.org/abs/1805.07071
24. LoperaTellez,O.:Underwaterthreatrecognition:Areautomatictargetclassifica-
tion algorithms going to replace expert human operators in the near future? In:
OCEANS 2019 - Marseille. pp. 1–4 (2019). https://doi.org/10.1109/OCEANSE.
2019.8867168
25. Matuszewski, D.J., Sintorn, I.M.: Reducing the u-net size for practical scenarios:
Virusrecognitioninelectronmicroscopyimages.ComputerMethodsandPrograms
inBiomedicine178,31–39(2019).https://doi.org/https://doi.org/10.1016/
j.cmpb.2019.05.026, https://www.sciencedirect.com/science/article/pii/
S0169260719300859
26. Mohd Sagheer, S.V., George, S.N.: A review on medical image denoising algo-
rithms. Biomedical Signal Processing and Control 61, 102036 (2020). https:
//doi.org/https://doi.org/10.1016/j.bspc.2020.102036, https://www.
sciencedirect.com/science/article/pii/S1746809420301920
27. Możejko,M.,Latkowski,T.,ŁukaszTreszczotko,Szafraniuk,M.,Trojanowski,K.:
Superkernel neural architecture search for image denoising (2020)
28. O’Neill, J.: An overview of neural network compression. CoRR abs/2006.03669
(2020), https://arxiv.org/abs/2006.03669
29. Park, B., Yu, S., Jeong, J.: Densely connected hierarchical network for image de-
noising.In:2019IEEE/CVFConference onComputerVision andPatternRecog-
nition Workshops (CVPRW). pp. 2104–2113 (2019). https://doi.org/10.1109/
CVPRW.2019.00263Pushing Joint Image Denoising and Classification to the Edge 17
30. Pham,H.,Guan,M.Y.,Zoph,B.,Le,Q.V.,Dean,J.:Efficientneuralarchitecture
searchviaparametersharing.CoRRabs/1802.03268(2018),http://arxiv.org/
abs/1802.03268
31. Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., Chen, M.: Hierarchical text-
conditional image generation with clip latents (2022)
32. Ronneberger,O.,Fischer,P.,Brox,T.:U-net:Convolutionalnetworksforbiomedi-
calimagesegmentation.CoRRabs/1505.04597(2015),http://arxiv.org/abs/
1505.04597
33. Sandler, M., Howard, A.G., Zhu, M., Zhmoginov, A., Chen, L.: Inverted residuals
andlinearbottlenecks:Mobilenetworksforclassification,detectionandsegmenta-
tion. CoRR abs/1801.04381 (2018), http://arxiv.org/abs/1801.04381
34. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale
image recognition (2015)
35. Singh, P., Shankar, A.: A novel optical image denoising technique using convolu-
tional neural network and anisotropic diffusion for real-time surveillance applica-
tions.JournalofReal-TimeImageProcessing18(5),1711–1728(Oct2021).https:
//doi.org/10.1007/s11554-020-01060-0, https://doi.org/10.1007/s11554-
020-01060-0
36. Tan, M., Chen, B., Pang, R., Vasudevan, V., Le, Q.V.: Mnasnet: Platform-aware
neural architecture search for mobile. CoRR abs/1807.11626 (2018), http://
arxiv.org/abs/1807.11626
37. Tan,M.,Le,Q.V.:Efficientnet:Rethinkingmodelscalingforconvolutionalneural
networks. CoRR abs/1905.11946 (2019), http://arxiv.org/abs/1905.11946
38. Tan, M., Le, Q.V.: Efficientnetv2: Smaller models and faster training. CoRR
abs/2104.00298 (2021), https://arxiv.org/abs/2104.00298
39. Wang, Z., Cun, X., Bao, J., Liu, J.: Uformer: A general u-shaped transformer
for image restoration. CoRR abs/2106.03106 (2021), https://arxiv.org/abs/
2106.03106
40. Wang, Z., Bovik, A., Sheikh, H., Simoncelli, E.: Image quality assessment: from
error visibility to structural similarity. IEEE Transactions on Image Processing
13(4), 600–612 (2004). https://doi.org/10.1109/TIP.2003.819861
41. Williams, D.P., Couillard, M., Dugelay, S.: On human perception and auto-
matic target recognition: Strategies for human-computer cooperation. In: 2014
22nd International Conference on Pattern Recognition. pp. 4690–4695 (2014).
https://doi.org/10.1109/ICPR.2014.802
42. Wu, B., Dai, X., Zhang, P., Wang, Y., Sun, F., Wu, Y., Tian, Y., Vajda, P., Jia,
Y., Keutzer, K.: Fbnet: Hardware-aware efficient convnet design via differentiable
neural architecture search. CoRR abs/1812.03443 (2018), http://arxiv.org/
abs/1812.03443
43. Wu,J.,Timofte,R.,Huang,Z.,VanGool,L.:Ontherelationbetweencolorimage
denoising and classification. arXiv preprint arXiv:1704.01372 (2017)
44. Wu, Y., He, K.: Group normalization. CoRR abs/1803.08494 (2018), http://
arxiv.org/abs/1803.08494
45. Xie, S., Zheng, H., Liu, C., Lin, L.: SNAS: stochastic neural architecture search.
CoRR abs/1812.09926 (2018), http://arxiv.org/abs/1812.09926
46. Ye,P.,Li,B.,Li,Y.,Chen,T.,Fan,J.,Ouyang,W.:β-darts:Beta-decayregular-
ization for differentiable architecture search (2022)
47. Zhang,H.,Li,Y.,Chen,H.,Shen,C.:IR-NAS:neuralarchitecturesearchforimage
restoration. CoRR abs/1909.08228 (2019), http://arxiv.org/abs/1909.0822818 T.C. Markhorst et al.
48. Zhang, K., Zuo, W., Chen, Y., Meng, D., Zhang, L.: Beyond a gaussian denoiser:
Residual learning of deep CNN for image denoising. CoRR abs/1608.03981
(2016), http://arxiv.org/abs/1608.03981
49. Zhang,K.,Zuo,W.,Zhang,L.:Ffdnet:TowardafastandflexiblesolutionforCNN
based image denoising. CoRR abs/1710.04026 (2017), http://arxiv.org/abs/
1710.04026
50. Zhang, X., Zhou, X., Lin, M., Sun, J.: Shufflenet: An extremely efficient con-
volutional neural network for mobile devices. CoRR abs/1707.01083 (2017),
http://arxiv.org/abs/1707.01083Pushing Joint Image Denoising and Classification to the Edge 1
A Efficient Classification & Denoising: Additional results
The joint models in Section 3 are constructed using a separate denoiser and
classifier. We describe the baseline models and several methods to construct the
reduced versions.
Overview of the models used in the main paper. UNet-S: {d = 4, b
= 8, c = 2, m = 1.5}, which is also called Reduced UNet. UNet-M: {d = 4,
b = 16, c = 2, m = 2}. UNet: {d = 5, b = 64, c = 2, m = 2}, which is also
called Baseline UNet. MB2.5-M: the classifier described in Section A.1 with an
MBConv (expansion rate = 2.5) as second convolutional layer.
A.1 Efficient Classification
Experimental setup.Ourbaselineclassifier(Conv-L)consistsoftwoconvolu-
tional, one global max pooling, and a linear layer. Each convolutional layer also
has a group normalization [44], max pooling, and ReLU activation function.
To construct the reduced version, we use two methods similar to previous
works[33,37].Inthefirstmethod,wereplacethesecondconvolutionallayerwith
an MBConv layer. Three expansion rates are used {1,2.5,4}: (i) rate 1 is the
lowest possible value, (ii) rate 4 matches the number of FLOPs of the baseline,
and (iii) rate 2.5 is in the middle of those two. The second reduction method
is to lower the number of filters in the baseline, also called the model width.
Usingthesetechniques,modelswiththreedifferentFLOPsizesareconstructed,
{S, M, L}. We use the following naming scheme, Conv-x and MBe-x, where x
represents the FLOP size and e is the expansion rate of the MBConv.
The models are trained using Cross Entropy loss. We report the accuracy
averaged over all 11 noise levels.
Exp. 1: Conv vs MBConv comparison. According to [33], the MBConv
layershouldbemoreefficientthananormalconvolutionallayer.Therefore,when
comparing the two operators in our network, we expect the version with an
MBConv layer to need fewer FLOPs for the same accuracy. In Table 7, the MB
models with expansion rates 2.5 (MB2.5-M) and 4 (MB4-L) classify better than
the Conv-L model with fewer FLOPs. However, with an expansion rate of 1
(MB1-S), the accuracy drops 7% compared to Conv-L. Therefore, [33]’s theory
also holds for the noisy classifier, but only for the higher expansion rates.
Exp. 2: MBConv width & expansion rate scaling. Since MBConv
layers can be used to improve efficiency, we question how to further reduce the
MB model’s FLOP size. We compare two options: (i) reducing the expansion
rate and (ii) scaling the width of the network. We take MB4-L as the starting
model, as this is our best and largest model.
From the MB models with size S in Table 7, MB1-S performs the worst. It
only has a reduced expansion rate from 4 to 1. MB4-S, which is obtained by
scaling the width of MB-L, increases classification performance by only 0.4%.
However, when slightly reducing MB4-L’s width and expansion rate, we derive
MB2.5-S, which reaches 58.4% accuracy, significantly outperforming both other
S-sized MB models. So the combination of the two methods is most effective.2 T.C. Markhorst et al.
Table7:Classificationbaselineandreducedmodels,designedforthreedifferentFLOP
targets:{S,M,L},tocomparescalingmethods:expansionrateandmodelwidth.Each
section of rows is used by the experiments from Sec. A.1 defined in the Exp. column.
MB models scale down more efficiently than normal Conv models.
Exp.Model SizeExp.rateFLOPs(K)↓Lat.(ms)↓Acc(%)↑
Conv-L L - 447 0.336 63.2
MB1-S S 1 177 0.300 56.2
1-3
MB2.5-M M 2.5 350 0.384 64.1
MB4-L L 4 424 0.468 64.9
MB2.5-S S 2.5 178 0.390 58.4
2-3
MB4-S S 4 188 0.403 56.6
Conv-S S - 163 0.281 55.4
3
Conv-M M - 345 0.317 61.4
Exp. 3: Conv width scaling. In this experiment, we compare the width
scaling of the Conv-L model. Table 7 shows that all S-sized MB models out-
perform Conv-S, MB2.5-S even by 3.0%. MB2.5-M also outperforms Conv-M,
by 2.7%. Therefore, scaling is more efficient for the MB models than the Conv
models when optimizing for FLOPs.
Conclusion. The MBConv layer can replace the convolutional layers. We
find that compared to the Conv models, the MB models also scale down more
efficiently by first reducing the expansion rate, possibly followed by a width
reduction. Scaling effectively reduces the number of FLOPs.
MB2.5-M has the second-best accuracy with low FLOPs and a latency close
to the baseline, Conv-L. Therefore, MB2.5-M is used as the reduced classifier.
We also use MB2.5-M as the new baseline classifier as it outperforms the old
baseline, Conv-L, in FLOPs and accuracy.
It is important to note that the reduction in FLOPs instantiated by using
MBConvs, does not translate to a latency reduction in these experiments. This
issue is discussed previously in [38]. Since the target of these experiments is
FLOPs and the latency increase is manageable, we place minimal emphasis on
the latency.
A.2 Efficient Denoising
Experimental setup.Fordenoising,thebaselineandreducedversionarecon-
structedbyperformingahyperparameterstudyonUNetsimilarto[25].Figure2
showstheUNetarchitecturealongwithitshyperparameterstotune.Weexplore
theparametersoneatatime,startingwiththenumberofbasefeaturesmapsb,
then the UNet depth d, the feature map multiplier m, and the number of con-
volutional blocks per layer c. In the original UNet: {b = 64, d = 5, m = 2, c =
2}.Alteringthesehyperparameterscangreatlyreducethemodelsize.Similarto
theclassificationexperiments,wealsostudytheabilityoftheMBConvoperatorPushing Joint Image Denoising and Classification to the Edge 3
(a) Scaling b (b) Scaling d
24.0 d:4 d:3 d:5
b:16 b:32 b:64 d:4 d:5 d:4
23.0 b:8 d:3d:5 d:3
d:2 d:2
22.0 d:2 b = 8
b = 16
21.0 d = 4 b = 32
b:4
(c) Scaling c (d) Scaling m
b:16 b:32 b:64 b:16 b:32 b:64
23.5 b:8 b:8
b:64
b:32 m:2
m:1.5
23.0 b:16
b:8 c = 2 c = 2
c = 1 b = 8
m:1
22.5
108 109 108 109
FLOPs FLOPs
Fig.9: UNet hyperparameter (Figure 2) scaling experiments. Shows how altering a
specific hyper-parameter influences denoising performance and FLOPs. We only show
PSNR results of σ=0.8, as the other results show the same trend. We find that b and
m scale down efficiently, d and c do not.
to increase efficiency in the denoiser. The models are trained using Charbonnier
loss [4].
Exp. 1: The base feature map width b. In this experiment, we aim to
find the relevant range of b. Since b is multiplied at every level, the number of
feature maps throughout all layers depends on it, which makes it a powerful
hyper-parameter. We use d = 4 and the other hyper-parameters as in the orig-
inal UNet, then we test b ∈ {4, 8, 16, 32, 64}. The trend in Figure 9.a shows
that the performance and FLOPs increase with b. We observe that the trend is
significantlydisruptedbyb =4.Conversely,theperformancedifferencebetween
b =32andb =64issmall,butthenetworksizequadrupled.Thereforeinfurther
experiments, we focus on b ∈ {8, 16, 32}.
Exp. 2: The UNet depth d. Given the robustness of b, we are interested
inhowreducingd comparesintermsofefficiency.Figure9.bdisplaystheperfor-
manceofthearchitectureswiththeselectedb ∈{8,16,32}testingd ∈{2,3,4,
5}.Weobservethatreducingd causesadropindenoisingperformance,whereas
b retains performance better, also in Figure 9.a. Therefore b scales down more
efficiently. The models with d = 3 or 4 denoise most efficient. Especially for the
smaller models, d = 4 performs well.
Exp.3:Thenumberofconvblocksperlayerc.Doesreducingc further
increaseefficiency?Totestthis,wetakethebest-performingsettings,d =4and
b ∈ {8, 16, 32, 64}, and compare c = 1 and c = 2. Figure 9.c shows that the
RNSP
RNSP4 T.C. Markhorst et al.
model with c = 2 outperforms c = 1. Therefore reducing c does not benefit the
model’s efficiency.
Exp. 4: The feature map multiplier m. We test if our smallest model
could be further reduced in size by lowering m. We take d = 4 and b = 8,
and compare m ∈ {1, 1.5, 2}. Figure 9.d shows that the reduction to m = 1.5
retainsperformance.Form =1,theperformancedrops.Reducingm to1.5could
therefore be used to scale down the model when further reducing b significantly
decreases performance.
Conclusion. To construct the reduced and baseline denoiser, we use the
smallest and largest values from the found hyperparameter ranges. Resulting in
baseline (UNet): {b = 32, d = 4, m = 2, c = 2} and reduced (UNet-S): {b =
8, d = 4, m = 1.5, c = 2}. Table 8 compares the two models for a selection
of the noise levels. Although the reduced model has significantly fewer FLOPs
andlowerlatency,thedenoisingperformanceisrelativelysimilartothebaseline
denoiser.
TheUNethyper-parameterexperimentsarereplicatedusingMBConvs,which
lead to similar findings. Moreover, the Conv UNet slightly outperforms the MB
model. Therefore, the Conv model is used.
Table 8: Compares Baseline and Reduced UNet denoisers. The reduced model has
significantly lower FLOPs and latency yet similar denoising performance.
Noiselevel(σ)
Model FLOPs(M)↓Lat.(ms)↓ Metric
0.2 0.4 0.8 1
PSNR↑33.929.523.822.3
UNet 1301.8 7.10
SSIM↑ 0.990.980.950.92
PSNR↑33.228.723.322.0
UNet-S 51.2 2.38
SSIM↑ 0.990.980.940.92
B Search space
In Section 4.1, different variations of the TF-NAS search space are used [19].
Table 9 displays the candidate operations and for which search space size they
are used. The search space with 4 operators is constructed using the MBConvs
without SE-layer, as this is most common in recent NAS methods [38,42]. For
the 6-operator search space, we add the possibility of using an SE layer on the
operatorswherethekernelsizeisthreeandtheexpansionrateisthreeorsix.We
use the two smallest operators as they can be used for smaller target latencies
too. The search space with 8 operators simply uses all combinations.Pushing Joint Image Denoising and Classification to the Edge 5
Table 9: Overview of the candidate blocks for the different search space sizes {4, 6,
8}.MBConvoperators are used withdifferentkernelsizes k, expansionrate e, andin-
or excluding the squeeze- and excitation-layer.
Name KernelExpansion rateSE-layer 4 6 8
MB-k3-e3 3 3 - ✓✓✓
MB-k3-e6 3 6 - ✓✓✓
MB-k5-e3 5 3 - ✓✓✓
MB-k5-e6 5 6 - ✓✓✓
MB-k3-e3-se 3 3 ✓ - ✓✓
MB-k3-e6-se 3 6 ✓ - ✓✓
MB-k5-e3-se 5 3 ✓ - - ✓
MB-k5-e6-se 5 6 ✓ - - ✓
C Learned vs. Removed β: Additional results
In Experiment 1 of Section 4.1, we test the influence of removing β from the
searchapproach.ThemodelswithRemovedβ significantlyoutperformthemod-
els with Learned β in accuracy. Besides, the found models are more similar for
Removed than Fixed, Removed β differs only 0.04ms and 0.2% accuracy, while
Learned β differs 0.57ms and 1.4% accuracy. This indicates that the search for
Removed is more stable.
Table 10: Comparesfoursearchedmodelswithtargetlatency6ms.Trainedonclean
images.Twomodelsaresearchedwithoutβandtheothertwousinglearnedβ.Removed
outperforms learned β.
Type SearchidLAT(ms)↓Acc(%)↑
1 5.85 86.2
Removedβ
2 5.81 86.4
1 5.04 84.9
Learnedβ
2 4.47 83.5