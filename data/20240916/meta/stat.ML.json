[
    {
        "title": "Model-independent variable selection via the rule-based variable priorit",
        "authors": "Min LuHemant Ishwaran",
        "links": "http://arxiv.org/abs/2409.09003v1",
        "entry_id": "http://arxiv.org/abs/2409.09003v1",
        "pdf_url": "http://arxiv.org/pdf/2409.09003v1",
        "summary": "While achieving high prediction accuracy is a fundamental goal in machine\nlearning, an equally important task is finding a small number of features with\nhigh explanatory power. One popular selection technique is permutation\nimportance, which assesses a variable's impact by measuring the change in\nprediction error after permuting the variable. However, this can be problematic\ndue to the need to create artificial data, a problem shared by other methods as\nwell. Another problem is that variable selection methods can be limited by\nbeing model-specific. We introduce a new model-independent approach, Variable\nPriority (VarPro), which works by utilizing rules without the need to generate\nartificial data or evaluate prediction error. The method is relatively easy to\nuse, requiring only the calculation of sample averages of simple statistics,\nand can be applied to many data settings, including regression, classification,\nand survival. We investigate the asymptotic properties of VarPro and show,\namong other things, that VarPro has a consistent filtering property for noise\nvariables. Empirical studies using synthetic and real-world data show the\nmethod achieves a balanced performance and compares favorably to many\nstate-of-the-art procedures currently used for variable selection.",
        "updated": "2024-09-13 17:32:05 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.09003v1"
    },
    {
        "title": "A Bayesian Approach to Clustering via the Proper Bayesian Bootstrap: the Bayesian Bagged Clustering (BBC) algorithm",
        "authors": "Federico Maria QuettiSilvia FiginiElena ballante",
        "links": "http://arxiv.org/abs/2409.08954v1",
        "entry_id": "http://arxiv.org/abs/2409.08954v1",
        "pdf_url": "http://arxiv.org/pdf/2409.08954v1",
        "summary": "The paper presents a novel approach for unsupervised techniques in the field\nof clustering. A new method is proposed to enhance existing literature models\nusing the proper Bayesian bootstrap to improve results in terms of robustness\nand interpretability. Our approach is organized in two steps: k-means\nclustering is used for prior elicitation, then proper Bayesian bootstrap is\napplied as resampling method in an ensemble clustering approach. Results are\nanalyzed introducing measures of uncertainty based on Shannon entropy. The\nproposal provides clear indication on the optimal number of clusters, as well\nas a better representation of the clustered data. Empirical results are\nprovided on simulated data showing the methodological and empirical advances\nobtained.",
        "updated": "2024-09-13 16:14:54 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.08954v1"
    },
    {
        "title": "Multi forests: Variable importance for multi-class outcomes",
        "authors": "Roman HornungAlexander Hapfelmeier",
        "links": "http://arxiv.org/abs/2409.08925v1",
        "entry_id": "http://arxiv.org/abs/2409.08925v1",
        "pdf_url": "http://arxiv.org/pdf/2409.08925v1",
        "summary": "In prediction tasks with multi-class outcomes, identifying covariates\nspecifically associated with one or more outcome classes can be important.\nConventional variable importance measures (VIMs) from random forests (RFs),\nlike permutation and Gini importance, focus on overall predictive performance\nor node purity, without differentiating between the classes. Therefore, they\ncan be expected to fail to distinguish class-associated covariates from\ncovariates that only distinguish between groups of classes. We introduce a VIM\ncalled multi-class VIM, tailored for identifying exclusively class-associated\ncovariates, via a novel RF variant called multi forests (MuFs). The trees in\nMuFs use both multi-way and binary splitting. The multi-way splits generate\nchild nodes for each class, using a split criterion that evaluates how well\nthese nodes represent their respective classes. This setup forms the basis of\nthe multi-class VIM, which measures the discriminatory ability of the splits\nperformed in the respective covariates with regard to this split criterion.\nAlongside the multi-class VIM, we introduce a second VIM, the discriminatory\nVIM. This measure, based on the binary splits, assesses the strength of the\ngeneral influence of the covariates, irrespective of their\nclass-associatedness. Simulation studies demonstrate that the multi-class VIM\nspecifically ranks class-associated covariates highly, unlike conventional VIMs\nwhich also rank other types of covariates highly. Analyses of 121 datasets\nreveal that MuFs often have slightly lower predictive performance compared to\nconventional RFs. This is, however, not a limiting factor given the algorithm's\nprimary purpose of calculating the multi-class VIM.",
        "updated": "2024-09-13 15:40:29 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.08925v1"
    },
    {
        "title": "Latent Space Score-based Diffusion Model for Probabilistic Multivariate Time Series Imputation",
        "authors": "Guojun LiangNajmeh AbiriAtiye Sadat HashemiJens LundströmStefan ByttnerPrayag Tiwari",
        "links": "http://arxiv.org/abs/2409.08917v1",
        "entry_id": "http://arxiv.org/abs/2409.08917v1",
        "pdf_url": "http://arxiv.org/pdf/2409.08917v1",
        "summary": "Accurate imputation is essential for the reliability and success of\ndownstream tasks. Recently, diffusion models have attracted great attention in\nthis field. However, these models neglect the latent distribution in a\nlower-dimensional space derived from the observed data, which limits the\ngenerative capacity of the diffusion model. Additionally, dealing with the\noriginal missing data without labels becomes particularly problematic. To\naddress these issues, we propose the Latent Space Score-Based Diffusion Model\n(LSSDM) for probabilistic multivariate time series imputation. Observed values\nare projected onto low-dimensional latent space and coarse values of the\nmissing data are reconstructed without knowing their ground truth values by\nthis unsupervised learning approach. Finally, the reconstructed values are fed\ninto a conditional diffusion model to obtain the precise imputed values of the\ntime series. In this way, LSSDM not only possesses the power to identify the\nlatent distribution but also seamlessly integrates the diffusion model to\nobtain the high-fidelity imputed values and assess the uncertainty of the\ndataset. Experimental results demonstrate that LSSDM achieves superior\nimputation performance while also providing a better explanation and\nuncertainty analysis of the imputation mechanism. The website of the code is\n\\textit{https://github.com/gorgen2020/LSSDM\\_imputation}.",
        "updated": "2024-09-13 15:32:26 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.08917v1"
    },
    {
        "title": "Adjoint Matching: Fine-tuning Flow and Diffusion Generative Models with Memoryless Stochastic Optimal Control",
        "authors": "Carles Domingo-EnrichMichal DrozdzalBrian KarrerRicky T. Q. Chen",
        "links": "http://arxiv.org/abs/2409.08861v1",
        "entry_id": "http://arxiv.org/abs/2409.08861v1",
        "pdf_url": "http://arxiv.org/pdf/2409.08861v1",
        "summary": "Dynamical generative models that produce samples through an iterative\nprocess, such as Flow Matching and denoising diffusion models, have seen\nwidespread use, but there has not been many theoretically-sound methods for\nimproving these models with reward fine-tuning. In this work, we cast reward\nfine-tuning as stochastic optimal control (SOC). Critically, we prove that a\nvery specific memoryless noise schedule must be enforced during fine-tuning, in\norder to account for the dependency between the noise variable and the\ngenerated samples. We also propose a new algorithm named Adjoint Matching which\noutperforms existing SOC algorithms, by casting SOC problems as a regression\nproblem. We find that our approach significantly improves over existing methods\nfor reward fine-tuning, achieving better consistency, realism, and\ngeneralization to unseen human preference reward models, while retaining sample\ndiversity.",
        "updated": "2024-09-13 14:22:14 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.08861v1"
    }
]