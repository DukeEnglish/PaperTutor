[
    {
        "title": "The unknotting number, hard unknot diagrams, and reinforcement learning",
        "authors": "Taylor ApplebaumSam BlackwellAlex DaviesThomas EdlichAndrás JuhászMarc LackenbyNenad TomaševDaniel Zheng",
        "links": "http://arxiv.org/abs/2409.09032v1",
        "entry_id": "http://arxiv.org/abs/2409.09032v1",
        "pdf_url": "http://arxiv.org/pdf/2409.09032v1",
        "summary": "We have developed a reinforcement learning agent that often finds a minimal\nsequence of unknotting crossing changes for a knot diagram with up to 200\ncrossings, hence giving an upper bound on the unknotting number. We have used\nthis to determine the unknotting number of 57k knots. We took diagrams of\nconnected sums of such knots with oppositely signed signatures, where the\nsummands were overlaid. The agent has found examples where several of the\ncrossing changes in an unknotting collection of crossings result in hyperbolic\nknots. Based on this, we have shown that, given knots $K$ and $K'$ that satisfy\nsome mild assumptions, there is a diagram of their connected sum and $u(K) +\nu(K')$ unknotting crossings such that changing any one of them results in a\nprime knot. As a by-product, we have obtained a dataset of 2.6 million distinct\nhard unknot diagrams; most of them under 35 crossings. Assuming the additivity\nof the unknotting number, we have determined the unknotting number of 43 at\nmost 12-crossing knots for which the unknotting number is unknown.",
        "updated": "2024-09-13 17:59:52 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.09032v1"
    },
    {
        "title": "INN-PAR: Invertible Neural Network for PPG to ABP Reconstruction",
        "authors": "Soumitra KunduGargi PandaSaumik BhattacharyaAurobinda RoutrayRajlakshmi Guha",
        "links": "http://arxiv.org/abs/2409.09021v1",
        "entry_id": "http://arxiv.org/abs/2409.09021v1",
        "pdf_url": "http://arxiv.org/pdf/2409.09021v1",
        "summary": "Non-invasive and continuous blood pressure (BP) monitoring is essential for\nthe early prevention of many cardiovascular diseases. Estimating arterial blood\npressure (ABP) from photoplethysmography (PPG) has emerged as a promising\nsolution. However, existing deep learning approaches for PPG-to-ABP\nreconstruction (PAR) encounter certain information loss, impacting the\nprecision of the reconstructed signal. To overcome this limitation, we\nintroduce an invertible neural network for PPG to ABP reconstruction (INN-PAR),\nwhich employs a series of invertible blocks to jointly learn the mapping\nbetween PPG and its gradient with the ABP signal and its gradient. INN-PAR\nefficiently captures both forward and inverse mappings simultaneously, thereby\npreventing information loss. By integrating signal gradients into the learning\nprocess, INN-PAR enhances the network's ability to capture essential\nhigh-frequency details, leading to more accurate signal reconstruction.\nMoreover, we propose a multi-scale convolution module (MSCM) within the\ninvertible block, enabling the model to learn features across multiple scales\neffectively. We have experimented on two benchmark datasets, which show that\nINN-PAR significantly outperforms the state-of-the-art methods in both waveform\nreconstruction and BP measurement accuracy.",
        "updated": "2024-09-13 17:48:48 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.09021v1"
    },
    {
        "title": "An Efficient and Streaming Audio Visual Active Speaker Detection System",
        "authors": "Arnav KunduYanzi JinMohammad SekhavatMax HortonDanny TormoenDevang Naik",
        "links": "http://arxiv.org/abs/2409.09018v1",
        "entry_id": "http://arxiv.org/abs/2409.09018v1",
        "pdf_url": "http://arxiv.org/pdf/2409.09018v1",
        "summary": "This paper delves into the challenging task of Active Speaker Detection\n(ASD), where the system needs to determine in real-time whether a person is\nspeaking or not in a series of video frames. While previous works have made\nsignificant strides in improving network architectures and learning effective\nrepresentations for ASD, a critical gap exists in the exploration of real-time\nsystem deployment. Existing models often suffer from high latency and memory\nusage, rendering them impractical for immediate applications. To bridge this\ngap, we present two scenarios that address the key challenges posed by\nreal-time constraints. First, we introduce a method to limit the number of\nfuture context frames utilized by the ASD model. By doing so, we alleviate the\nneed for processing the entire sequence of future frames before a decision is\nmade, significantly reducing latency. Second, we propose a more stringent\nconstraint that limits the total number of past frames the model can access\nduring inference. This tackles the persistent memory issues associated with\nrunning streaming ASD systems. Beyond these theoretical frameworks, we conduct\nextensive experiments to validate our approach. Our results demonstrate that\nconstrained transformer models can achieve performance comparable to or even\nbetter than state-of-the-art recurrent models, such as uni-directional GRUs,\nwith a significantly reduced number of context frames. Moreover, we shed light\non the temporal memory requirements of ASD systems, revealing that larger past\ncontext has a more profound impact on accuracy than future context. When\nprofiling on a CPU we find that our efficient architecture is memory bound by\nthe amount of past context it can use and that the compute cost is negligible\nas compared to the memory cost.",
        "updated": "2024-09-13 17:45:53 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.09018v1"
    },
    {
        "title": "VAE Explainer: Supplement Learning Variational Autoencoders with Interactive Visualization",
        "authors": "Donald BertucciAlex Endert",
        "links": "http://arxiv.org/abs/2409.09011v1",
        "entry_id": "http://arxiv.org/abs/2409.09011v1",
        "pdf_url": "http://arxiv.org/pdf/2409.09011v1",
        "summary": "Variational Autoencoders are widespread in Machine Learning, but are\ntypically explained with dense math notation or static code examples. This\npaper presents VAE Explainer, an interactive Variational Autoencoder running in\nthe browser to supplement existing static documentation (e.g., Keras Code\nExamples). VAE Explainer adds interactions to the VAE summary with interactive\nmodel inputs, latent space, and output. VAE Explainer connects the high-level\nunderstanding with the implementation: annotated code and a live computational\ngraph. The VAE Explainer interactive visualization is live at\nhttps://xnought.github.io/vae-explainer and the code is open source at\nhttps://github.com/xnought/vae-explainer.",
        "updated": "2024-09-13 17:40:01 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.09011v1"
    },
    {
        "title": "SGFormer: Single-Layer Graph Transformers with Approximation-Free Linear Complexity",
        "authors": "Qitian WuKai YangHengrui ZhangDavid WipfJunchi Yan",
        "links": "http://arxiv.org/abs/2409.09007v1",
        "entry_id": "http://arxiv.org/abs/2409.09007v1",
        "pdf_url": "http://arxiv.org/pdf/2409.09007v1",
        "summary": "Learning representations on large graphs is a long-standing challenge due to\nthe inter-dependence nature. Transformers recently have shown promising\nperformance on small graphs thanks to its global attention for capturing\nall-pair interactions beyond observed structures. Existing approaches tend to\ninherit the spirit of Transformers in language and vision tasks, and embrace\ncomplicated architectures by stacking deep attention-based propagation layers.\nIn this paper, we attempt to evaluate the necessity of adopting multi-layer\nattentions in Transformers on graphs, which considerably restricts the\nefficiency. Specifically, we analyze a generic hybrid propagation layer,\ncomprised of all-pair attention and graph-based propagation, and show that\nmulti-layer propagation can be reduced to one-layer propagation, with the same\ncapability for representation learning. It suggests a new technical path for\nbuilding powerful and efficient Transformers on graphs, particularly through\nsimplifying model architectures without sacrificing expressiveness. As\nexemplified by this work, we propose a Simplified Single-layer Graph\nTransformers (SGFormer), whose main component is a single-layer global\nattention that scales linearly w.r.t. graph sizes and requires none of any\napproximation for accommodating all-pair interactions. Empirically, SGFormer\nsuccessfully scales to the web-scale graph ogbn-papers100M, yielding\norders-of-magnitude inference acceleration over peer Transformers on\nmedium-sized graphs, and demonstrates competitiveness with limited labeled\ndata.",
        "updated": "2024-09-13 17:37:34 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.09007v1"
    }
]