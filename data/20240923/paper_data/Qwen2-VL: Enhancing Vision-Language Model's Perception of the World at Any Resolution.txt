Qwen2-VL: Enhancing Vision-Language Model’s Perception
of the World at Any Resolution
PengWang* ShuaiBai* SinanTan* ShijieWang* ZhihaoFan* JinzeBai*†
KeqinChen XuejingLiu JialinWang WenbinGe YangFan KaiDang MengfeiDu
XuanchengRen RuiMen DayihengLiu ChangZhou JingrenZhou JunyangLin†
QwenTeam AlibabaGroup
Abstract
WepresenttheQwen2-VLSeries,anadvancedupgradeofthepreviousQwen-VLmodels
thatredefinestheconventionalpredetermined-resolutionapproachinvisualprocessing.
Qwen2-VLintroducestheNaiveDynamicResolutionmechanism, whichenablesthe
modeltodynamicallyprocessimagesofvaryingresolutionsintodifferentnumbersof
visualtokens. Thisapproachallowsthemodeltogeneratemoreefficientandaccurate
visualrepresentations,closelyaligningwithhumanperceptualprocesses.Themodelalso
integratesMultimodalRotaryPositionEmbedding(M-RoPE),facilitatingtheeffective
fusionofpositionalinformationacrosstext,images,andvideos. Weemployaunified
paradigmforprocessingbothimagesandvideos,enhancingthemodel’svisualperception
capabilities.Toexplorethepotentialoflargemultimodalmodels,Qwen2-VLinvestigates
thescalinglawsforlargevision-languagemodels(LVLMs).Byscalingboththemodel
size-withversionsat2B,8B,and72Bparameters-andtheamountoftrainingdata,the
Qwen2-VLSeriesachieveshighlycompetitiveperformance.Notably,theQwen2-VL-72B
modelachievesresultscomparabletoleadingmodelssuchasGPT-4oandClaude3.5-
Sonnetacrossvariousmultimodalbenchmarks,outperformingothergeneralistmodels.
Codeisavailableathttps://github.com/QwenLM/Qwen2-VL.
1 Introduction
Intherealmofartificialintelligence,LargeVision-LanguageModels(LVLMs)representasignificantleap
forward,buildinguponthestrongtextualprocessingcapabilitiesoftraditionallargelanguagemodels. These
advancedmodelsnowencompasstheabilitytointerpretandanalyzeabroaderspectrumofdata,including
images,audio,andvideo. ThisexpansionofcapabilitieshastransformedLVLMsintoindispensabletoolsfor
tacklingavarietyofreal-worldchallenges. Recognizedfortheiruniquecapacitytocondenseextensiveand
intricateknowledgeintofunctionalrepresentations,LVLMsarepavingthewayformorecomprehensive
cognitivesystems. Byintegratingdiversedataforms,LVLMsaimtomorecloselymimicthenuancedwaysin
whichhumansperceiveandinteractwiththeirenvironment. Thisallowsthesemodelstoprovideamore
accuraterepresentationofhowweengagewithandperceiveourenvironment
Recentadvancementsinlargevision-languagemodels(LVLMs)(Lietal.,2023c;Liuetal.,2023b;Daietal.,
2023;Zhuetal.,2023;Huangetal.,2023a;Baietal.,2023b;Liuetal.,2023a;Wangetal.,2023b;OpenAI.,
2023;Teametal.,2023)haveledtosignificantimprovementsinashortspan. Thesemodels(OpenAI,2023;
Touvronetal.,2023a,b;Chiangetal.,2023;Baietal.,2023a)generallyfollowacommonapproachofvisual
encoder→cross-modalconnector→LLM.Thissetup,combinedwithnext-tokenpredictionastheprimarytraining
methodandtheavailabilityofhigh-qualitydatasets(Liuetal.,2023a;Zhangetal.,2023;Chenetal.,2023b;
∗Equalcorecontribution,†Correspondingauthor
1
4202
peS
81
]VC.sc[
1v19121.9042:viXraFigure 1: Qwen2-VL capabilities: Multilingual image text understanding, code/math reasoning, video
analysis,livechat,agentpotential,andmore. SeeAppendixfordetails.
Lietal.,2023b),hasdrivenmuchoftheprogress. Additionalfactorslikelargermodelarchitectures(Alayrac
et al., 2022), higher-resolution images (Li et al., 2023a,d), and advanced techniques such as mixture-of-
expertmodels(MoE)(Wangetal.,2023b;Yeetal.,2023b),modelensembles(Linetal.,2023),andmore
sophisticatedconnectors(Yeetal.,2023a)betweenvisualandtextualmodalitieshavealsoplayedakeyrole
inenhancingLVLMs’abilitytoprocesscomplexvisualandtextualinformationmoreeffectively.
However,currentlargevision-languagemodels(LVLMs)aretypicallyconstrainedbyafixedimageinputsize.
StandardLVLMsencodeinputimagestoafixedresolution(e.g.,224×224),oftenbyeitherdownsampling
orupsamplingtheimages(Zhuetal.,2023;Huangetal.,2023a),orbyemployingascale-then-padding
approach(Liuetal.,2023b,a). Whilethisone-size-fits-allstrategyenablesprocessingofimagesatconsistent
resolutions,italsolimitsthemodel’sabilitytocaptureinformationatdifferentscales,particularlyleadingto
asignificantlossofdetailedinformationinhigh-resolutionimages. Consequently,suchmodelsfallshortof
perceivingvisualinformationwiththesamesensitivitytoscaleanddetailashumanvision.
Additionally,mostLVLMsrelyonastatic,frozenCLIP-style(Radfordetal.,2021)visionencoder,raising
concerns about whether the visual representations produced by such pre-trained models are adequate,
particularlyforcomplexreasoningtasksandprocessingintricatedetailswithinimages. Recentworks(Bai
etal.,2023b;Yeetal.,2023a)haveattemptedtoaddresstheselimitationsbyfine-tuningthevisiontransformer
(ViT)duringtheLVLMtrainingprocess,whichhasshowntoyieldimprovedresults. Tofurtherenhancethe
model’sadaptabilitytovaryingresolutions,weintroducedynamicresolutiontrainingintheLVLMtraining
process. Specifically,weemploya2DRotaryPositionEmbedding(RoPE)intheViT,thusallowingthemodel
tobettercaptureinformationacrossdifferentspatialscales.
Whenitcomestovideocontent,whichisessentiallyasequenceofframes,manyexistingmodelscontinueto
treatitasanindependentmodality. However,understandingthedynamicnatureofreality,asmanifestedin
videos,iscrucialformodelsaimingtograspthecomplexitiesoftherealworld. Unliketext,whichisinherently
one-dimensional,thereal-worldenvironmentexistsinthreedimensions. Theuseofone-dimensionalposition
embeddingsincurrentmodelssignificantlylimitstheirabilitytomodelthree-dimensionalspaceandtemporal
dynamicseffectively. Tobridgethisgap,wehavedevelopedMultimodalRotaryPositionEmbedding(M-
2Table1: ModeldescriptionsofQwen2-VL.
ModelName VisionEncoder LLM ModelDescription
Themostefficientmodel,designedtorunon-device.Itdeliversadequate
Qwen2-VL-2B 675M 1.5B
performanceformostscenarioswithlimitedresources.
Theperformance-optimizedmodelintermsofcost,significantlyup-
Qwen2-VL-7B 675M 7.6B graded for text recognition and video understanding capabilities. It
deliverssignificantperformanceacrossabroadrangeofvisualtasks.
The most capable model, further improvements in visual reasoning,
Qwen2-VL-72B 675M 72B instruction-following,decision-making,andagentcapabilities.Itdelivers
optimalperformanceonmostcomplextasks.
RoPE),whichemploysseparatecomponentstorepresenttemporalandspatialinformation. Thisenablesthe
modeltonaturallycomprehenddynamiccontent,suchasvideosorstreamingdata,improvingitsabilityto
understandandinteractwiththeworld.
Furthermore,comparedtothescalingoflargelanguagemodels(LLMs),currentLVLMsarestillintheearly
stagesofexploringtheimpactofscalingintermsoftrainingdataandmodelparameters. Theexplorationof
scalinglawsforLVLMs—howincreasesinmodelanddatasizeaffectperformance—remainsanopenand
promisingareaofresearch.
Inthiswork, weintroducethenewestadditiontothelargevision-languagemodelsoftheQwenfamily:
Qwen2-VLseries,whichcomprisesthreeopen-weightmodelswithtotalparametercountsof2billion,8
billion,and72billion. AsshowninFigure1,thekeyadvancesinQwen2-VLinclude:
• State-of-the-art understanding across various resolutions and aspect ratios: Qwen2-VL achieves
leadingperformanceonvisualbenchmarks,includingDocVQA,InfoVQA,RealWorldQA,MTVQA,
MathVista,andothers.
• Comprehensionofextended-durationvideos(20min+): Qwen2-VLiscapableofunderstanding
videosover20minutesinlength,enhancingitsabilitytoperformhigh-qualityvideo-basedquestion
answering,dialogue,contentcreation,andmore.
• Robustagentcapabilitiesfordeviceoperation:Withadvancedreasoninganddecision-makingabilities,
Qwen2-VLcanbeintegratedwithdevicessuchasmobilephones,robots,etc.,enablingautonomous
operationbasedonvisualinputsandtextinstructions.
• Multilingual support: To serve a global audience, beyond English and Chinese, Qwen2-VL now
supports multilingual context understanding within images, including most European languages,
Japanese,Korean,Arabic,Vietnamese,andothers.
2 Approach
TheQwen2-VLseriesconsistsofmodelsof3sizes,whichareQwen2-VL-2B,Qwen2-VL-7BandQwen2-VL-
72B.Table1liststhehyper-parametersandimportantinformation. Notably,Qwen2-VLemploysa675M
parameterViTacrossvarious-sizedLLMs,ensuringthatthecomputationalloadoftheViTremainsconstant
regardlessofthescaleoftheLLM.
2.1 ModelArchitecture
Figure2illustratesthecomprehensivestructureofQwen2-VL.WehaveretainedtheQwen-VL(Baietal.,
2023b)framework,whichintegratesvisionencodersandlanguagemodels. Forvariousscaleadaptations,we
3Figure2: Qwen2-VLiscapableofaccuratelyidentifyingandcomprehendingthecontentwithinimages,
regardlessoftheirclarity,resolution,orextremeaspectratios.
haveimplementedaVisionTransformer(ViT)(Dosovitskiyetal.,2021)withapproximately675million
parameters, adept at handling both image and video inputs. In terms of language processing, we have
optedforthemorepowerfulQwen2(Yangetal.,2024)seriesoflanguagemodels. Tofurtherenhancethe
model’sabilitytoeffectivelyperceiveandcomprehendvisualinformationinvideos,weintroducedseveral
keyupgrades:
NaiveDynamicResolution AkeyarchitecturalimprovementinQwen2-VListheintroductionofnaive
dynamicresolutionsupport(Dehghanietal.,2024). Unlikeitspredecessor,Qwen2-VLcannowprocess
imagesofanyresolution,dynamicallyconvertingthemintoavariablenumberofvisualtokens. Tosupport
this feature, we modified ViT by removing the original absolute position embeddings and introducing
2D-RoPE(Suetal.,2024;Su,2021)tocapturethetwo-dimensionalpositionalinformationofimages. Atthe
inferencestage,imagesofvaryingresolutionsarepackedintoasinglesequence,withthepackedlength
controlledtolimitGPUmemoryusage. Furthermore,toreducethevisualtokensofeachimage,asimple
MLPlayerisemployedaftertheViTtocompressadjacent2×2tokensintoasingletoken,withthespecial
<|vision_start|>and<|vision_end|>tokensplacedatthebeginningandendofthecompressedvisualtokens.
As a result, an image with a resolution of 224×224, encoded with a ViT using patch_size=14, will be
compressedto66tokensbeforeenteringLLM.
MultimodalRotaryPositionEmbedding(M-RoPE) Anotherkeyarchitecturalenhancementistheinnova-
tionofMultimodalRotaryPositionEmbedding(M-RoPE).Unlikethetraditional1D-RoPEinLLMs,which
islimitedtoencodingone-dimensionalpositionalinformation,M-RoPEeffectivelymodelsthepositional
informationofmultimodalinputs. Thisisachievedbydeconstructingtheoriginalrotaryembeddinginto
threecomponents: temporal,height,andwidth. Fortextinputs,thesecomponentsutilizeidenticalposition
4Figure3: AdemonstrationofM-RoPE.Bydecomposingrotaryembeddingintotemporal,height,andwidth
components,M-RoPEcanexplicitlymodelthepositionalinformationoftext,images,andvideoinLLM.
IDs,makingM-RoPEfunctionallyequivalentto1D-RoPE(Su,2024). Whenprocessingimages,thetemporal
IDsofeachvisualtokenremainconstant,whiledistinctIDsareassignedtotheheightandwidthcomponents
basedonthetoken’spositionintheimage. Forvideos,whicharetreatedassequencesofframes,thetemporal
IDincrementsforeachframe,whiletheheightandwidthcomponentsfollowthesameIDassignmentpattern
asimages. Inscenarioswherethemodel’sinputencompassesmultiplemodalities,positionnumberingfor
eachmodalityisinitializedbyincrementingthemaximumpositionIDoftheprecedingmodalitybyone. An
illustrationofM-RoPEisshowninFigure3. M-RoPEnotonlyenhancesthemodelingofpositionalinforma-
tionbutalsoreducesthevalueofpositionIDsforimagesandvideos,enablingthemodeltoextrapolateto
longersequencesduringinference.
UnifiedImageandVideoUnderstanding Qwen2-VLemploysamixedtrainingregimenincorporatingboth
imageandvideodata,ensuringproficiencyinimageunderstandingandvideocomprehension. Topreserve
videoinformationascompletelyaspossible,wesampledeachvideoattwoframespersecond. Additionally,
weintegrated3Dconvolutions(CarreiraandZisserman,2017)withadepthoftwotoprocessvideoinputs,
allowingthemodeltohandle3Dtubesinsteadof2Dpatches,thusenablingittoprocessmorevideoframes
withoutincreasingthesequencelength(Arnabetal.,2021). Forconsistency,eachimageistreatedastwo
identical frames. To balance the computational demands of long video processing with overall training
efficiency,wedynamicallyadjusttheresolutionofeachvideoframe,limitingthetotalnumberoftokensper
videoto16384. Thistrainingapproachstrikesabalancebetweenthemodel’sabilitytocomprehendlong
videosandtrainingefficiency.
2.2 Training
FollowingQwen-VL(Baietal.,2023b),weadoptathree-stagetrainingmethodology. Inthefirststage,we
focusexclusivelyontrainingtheVisionTransformer(ViT)component,utilizingavastcorpusofimage-text
pairstoenhancesemanticunderstandingwithintheLargeLanguageModel(LLM).Inthesecondstage,we
unfreezeallparametersandtrainwithawiderrangeofdataformorecomprehensivelearning. Inthefinal
stage,welocktheViTparametersandperformexclusivefine-tuningoftheLLMusinginstructionaldatasets.
Themodelispre-trainedonadiversedatasetthatincludesimage-textpairs,opticalcharacterrecognition
(OCR)data,interleavedimage-textarticles,visualquestionansweringdatasets,videodialogues,andimage
knowledgedatasets. Ourdatasourcesprimarilycomprisecleanedwebpages,open-sourcedatasets,and
synthetic data. The cutoff date for our data knowledge is June 2023. This diverse data composition is
instrumentalindevelopingarobustmultimodalunderstandingcapability.
Duringtheinitialpre-trainingphase,Qwen2-VLisexposedtoacorpusofaround600billiontokens. The
LLMcomponentofQwen2-VLisinitializedusingtheparametersfromQwen2(Yangetal.,2024),while
thevisionencoderofQwen2-VLisinitializedwiththeViTderivedfromDFN.However,thefixedposition
embeddingintheoriginalDFN’sViT(Fangetal.,2023)isreplacedbyRoPE-2D.Thispre-trainingphase
primarilyfocusesonlearningimage-textrelationships,textualcontentrecognitionwithinimagesthrough
5OCR,andimageclassificationtasks. Suchfoundationaltrainingisinstrumentalinenablingthemodelto
developarobustunderstandingofcorevisual-textualcorrelationsandalignments.
Thesecondpre-trainingphasemarksasignificantprogression,involvinganadditional800billiontokensof
image-relateddata. Thisstageintroducesahighervolumeofmixedimage-textcontent,facilitatingamore
nuancedunderstandingoftheinterplaybetweenvisualandtextualinformation. Theincorporationofvisual
questionansweringdatasetsrefinesthemodel’scapacitytorespondtoimage-relatedqueries. Moreover,
theinclusionofmultitaskingdatasetsispivotalindevelopingthemodel’sabilitytonavigatediversetasks
concurrently,askillofparamountimportancewhendealingwithcomplex,real-worlddatasets. Concurrently,
purely textual data continues to play a crucial role in maintaining and advancing the model’s linguistic
proficiency.
Throughoutthepre-trainingstages,Qwen2-VLprocessesacumulativetotalof1.4trilliontokens. Specifically,
thesetokensencompassnotonlytexttokensbutalsoimagetokens. Duringthetrainingprocess,however,we
onlyprovidesupervisionforthetexttokens. Thisexposuretoextensiveanddiverselinguisticandvisual
scenariosensuresthatthemodeldevelopsadeepunderstandingoftheintricaterelationshipsbetweenvisual
andtextualinformation,therebylayingarobustfoundationforvariousmultimodaltasks.
During the instruction fine-tuning phase, we employ the ChatML (Openai, 2024) format to construct
instruction-following data. This dataset encompasses not only pure text-based dialogue data but also
multimodalconversationaldata. Themultimodalcomponentsincludeimagequestion-answering,document
parsing,multi-imagecomparison,videocomprehension,videostreamdialogue,andagent-basedinteractions.
Ourcomprehensiveapproachtodataconstructionaimstoenhancethemodel’scapabilitytounderstandand
executeawiderangeofinstructionsacrossvariousmodalities. Byincorporatingdiversedatatypes,weseek
todevelopamoreversatileandrobustlanguagemodelcapableofhandlingcomplex,multimodaltasksin
additiontotraditionaltext-basedinteractions.
2.2.1 DataFormat.
InlinewithQwen-VL,Qwen2-VLalsoemploysspecialtokenstodistinguishvisionandtextinputs. Tokens
<|vision_start|> and <|vision_end|> are inserted at the start and end of the image feature sequence to
demarcatetheimagecontent.
DialogueData. Intermsofdialogueformat,weconstructourinstructiontuningdatasetusingtheChatML
format,whereeachinteraction’sstatementismarkedwithtwospecialtokens(<|im_start|>and<|im_end|>)
tofacilitatedialoguetermination. Thesectionsmarkedinblueindicatethesupervisedparts.
TheDatasetFormatExampleofChatML
<|im_start|>user
<|vision_start|>Picture1.jpg<|vision_end|><|vision_start|>Picture2.jpg<|vision_end|>Whatdothe
twopictureshaveincommon?<|im_end|>
<|im_start|>assistant
BothpicturesareofSpongeBobSquarePants. <|im_end|>
<|im_start|>user
Whatishappeninginthevideo?<|vision_start|>video.mp4<|vision_end|><|im_end|>
<|im_start|>assistant
Theprotagonistinthevideoisfryinganegg.<|im_end|>
VisualGrounding. Toendowthemodelwithvisualgroundingcapabilities,boundingboxcoordinates
are normalized within [0, 1000) and represented as "(X ,Y ),(X ,Y )". Tokens
topleft topleft bottomright bottomright
<|box_start|>and<|box_end|>areutilizedtodemarcateboundingboxtext. Toaccuratelylinkbounding
6boxeswiththeirtextualdescriptions,weintroducetokens<|object_ref_start|>and<|object_ref_end|>to
indicatethecontentthattheboundingboxreferences,therebyallowingthemodeltoeffectivelyinterpretand
generateprecisedescriptionsofspecificregions.
ReferringGrounding
<|vision_start|>Picture1.jpg<|vision_end|>
<|object_ref_start|>the eyes on a giraffe<|object_ref_end|><|box_start|>(176,106),(232,160)
<|box_end|>
VisualAgent. TodevelopQwen2-VLasageneral-purposeVL-Agent,wetreatvariousagenttasks,suchas
UIOperations,RoboticControl,Games,andNavigation,assequentialdecision-makingproblems,enabling
Qwen2-VLtoaccomplishtasksthroughmulti-stepactionexecution. Foreachtask,wefirstdefineasetof
permissibleactionsandkeywordspattern(underline)forfunctioncall(QwenTeam,2024). Qwen2-VLthen
analyzestheobservations,performsreasoningandplanning,executestheselectedactions,andinteracts
withtheenvironmenttoacquirenewobservations. Thiscyclerepeatsiterativelyuntilthetaskissuccessfully
completed. Byintegratingvarioustoolsandleveragingthevisionperceptioncapabilitiesoflargevision-
languagemodels(LVLMs),Qwen2-VLisabletoiterativelyexecuteincreasinglycomplextasksinvolving
real-worldvisualinteractions.
VisualAgent
<|im_start|>system
Youareahelpfulassistant.
#Actions
##Youhavethefollowingactions.
###Tap
Tap: A gentle tap that commands, chooses, or navigates through a smartphone’s user interface.
Parameters: [{"name": "point","description": "Thespecificspotofinterestonthemonitor,denotedby
thecoordinates(x,y)wherexandyrangefrom0to1000.","required": True}]
###Home
Home: Gotophone’shomescreen. Parameters: []
###OtherActions...
##Continuouslytakeactionuntilthetaskiscompleted.
*FUNCTION*:Theactiontotake,shouldbeoneof{Actions}.
*ARGS*:Theinputoftheaction.
*RESULT*:Actionresults.
*RETURN*:Replybasedonactionresults. <|im_end|>
<|im_start|>user
FindapizzarestaurantnearbyinMap. <|vision_start|>Screenshot_1.jpg<|vision_end|><|im_end|>
<|im_start|>assistant
Severale-mailsaredisplayingonthephonescreen. ToopenMap,Ineedgobacktothehomescreen
andfindthecorrespondingappicon.
*FUNCTION*: Home*ARGS*:{}
*RESULT*:<|vision_start|>Screenshot_2.jpg<|vision_end|>
*RETURN*:Ireturntothehomescreen. Next,IneedtofindtheiconofMapandtaponit.
*FUNCTION*:Tap*ARGS*:{"point": (348,291)}
*RESULT*:<|vision_start|>Screenshot_3.jpg<|vision_end|>
*RETURN*:[Thinkingforthenextaction.]
[Othersubsequentactions.] ......
IhavefoundthepizzarestaurantnearbyinMap. <|im_end|>
72.3 MultimodalModelInfrastructure
TheQwen2-VLmodelsweretrainedonAlibabaCloud’sPAI-LingjunIntelligentComputingService(Alibaba-
Cloud,2024c)withitsscalablecomputing,autoresumingandstragglerdetection.
Storage. WeuseAlibabaCloud’sultra-speedCPFS(CloudParallelFileStorage)(Alibaba-Cloud,2024a)to
buildastoragesystemofQwen2-VLpre-trainingandpost-training. Wedecoupledthetextdataandvision
datastorage. WesimplystoretextdataonCPFSandusemmapforefficientaccess. Forvisiondata,weuse
AlibabaCloud’sOSS(ObjectStorageService)(Alibaba-Cloud,2024b)forpersistentstorage. Duringtraining,
weaccessedvisiondatathroughOSS’spython-clientconcurrentlyandtunedtheconcurrencyandretrying
parameterstoavoidreachingtheQPS(queriespersecond)limit. Wealsofoundthatvideodatadecodingis
amainbottleneck,especiallyforlongvideos. Afterseveralattemptswithopen-source(FFmpeg-Developers,
2024)andin-housesoftwarefailed,weoptedforacachingdecodingtechnique. Checkpointingsaveseach
GPU’soptimizerandmodelstatesonCPFS.
Parallelism. Weuse3Dparallelismwhichcombinesdataparallelism(DP)(Lietal.,2020),tensorpar-
allelism(TP)(Krizhevskyetal.,2012;Shoeybietal.,2019)andpipelineparallelism(PP)(Huangetal.,
2019;Narayananetal.,2021;Lamy-Poirier,2023)toscaleQwen2-VLmodeltraining. Wealsoleveragedeep-
speed’szero-1redundancyoptimizer(Rajbhandarietal.,2020)toshardstatesformemorysaving. Sequence
parallelism(SP)(Korthikantietal.,2023)withselectivecheckpointingactivation(Chenetal.,2016)was
leveragedtoreducememoryusage. WhenenablingTPtraining,wealwaysshardthevisionencoderandlarge
languagemodelstogetherbutnotthevisionmergerduetoitsrelativelyfewparameters. WefoundtheTP
trainingwouldresultindifferentmodelshared-weightsduetotheconvolutionoperator’snon-deterministic
behavior1. Weresolvedthisissuebyperformingofflinereductionofthesharedweights,therebyavoidingan
additionalall-reducecommunicationstep. Thisapproachresultedinonlyaminimalimpactonperformance.
Weleverage1F1BPP(Narayananetal.,2021)forQwen2-VL72Btraining. Wecombinethevisionencoder,
visionadapterandseveralLLM’sdecoderlayersintoonestage,andevenlysplittheremainingdecoderlayers.
Notethatthevisionandtextsequencelengthsaredynamicforeachdatapoint. Webroadcastthedynamic
sequencelengthsbeforeinitiatingthe1F1Bprocessandaccesstheshapeinformationusingbatchindices. We
alsoimplementedaninterleaved1F1BPP(Narayananetal.,2021)butfounditisslowerthanthestandard
1F1Bsetting.
Software. WeusePyTorch(Paszkeetal.,2019;Anseletal.,2024)version2.1.2withCUDA11.8(Nvidia,
2024b)fortraining. Additionally,weleverageflash-attention(Daoetal.,2022;Dao,2024;Shahetal.,2024)
forefficienttraininginboththevisionencoderandtheLLM.Wealsoutilizefusedoperators(Nvidia,2024a)
suchasLayerNorm(Baetal.,2016),RMSNorm(ZhangandSennrich,2019),andAdam(Loshchilovand
Hutter, 2019). Besides this, we leverage the overlap of communication and computation during matrix
multiplicationinourtrainingprocess.
3 Experiments
Inthissection,wefirstevaluatethemodel’sperformancebyconductingacomparativeanalysisacrossa
varietyofvisualbenchmarks,demonstratingtheadvantagesofourapproach. Subsequently,wecarryouta
detailedexaminationofspecificcapabilities,includinggeneralvisualperception,documentunderstanding,
multilingualrecognitioninimages,videocomprehension,andagentabilities. Finally,wepresentanablation
studytoinvestigateseveralkeycomponentsofourapproach.
1https://pytorch.org/docs/stable/notes/randomness.html
8Table2: PerformanceComparisonofQwen2-VLModelsandState-of-the-art.
Benchmark PreviousSoTA Claude-3.5Sonnet GPT-4o Qwen2-VL-72B Qwen2-VL-7B Qwen2-VL-2B
MMMUval(Yueetal.,2023) 66.1(X.AI,2024b) 68.3 69.1 64.5 54.1 41.1
DocVQAtest(Mathewetal.,2021) 94.1(Chenetal.,2024c) 95.2 92.8 96.5 94.5 90.1
InfoVQAtest(Mathewetal.,2021) 82.0(Chenetal.,2024c) - - 84.5 76.5 65.5
AI2D(Kembhavietal.,2016) 87.6(Chenetal.,2024c) 80.2(94.7) 84.6(94.2) 88.1 83.0 74.7
ChartQAtest(Masryetal.,2022) 88.4(Chenetal.,2024c) 90.8 85.7 88.3 83.0 73.5
TextVQAval(Singhetal.,2019) 84.4(Chenetal.,2024c) - - 85.5 84.3 79.7
OCRBench(Liuetal.,2023e) 852(Yaoetal.,2024) 788 736 877 866 809
MTVQA(Tangetal.,2024) 23.2(Teametal.,2023) 25.7 27.8 30.9 25.6 18.1
VCReneasy(Zhangetal.,2024c) 84.7(Chenetal.,2024c) 63.9 91.6 91.9 89.7 81.5
VCRzheasy(Zhangetal.,2024c) 22.1(Chenetal.,2024c) 1.0 14.9 65.4 59.9 46.2
RealWorldQA(X.AI,2024a) 72.2(Chenetal.,2024c) 60.1 75.4 77.8 70.1 62.9
MMEsum(Fuetal.,2023) 2414.7(Chenetal.,2024c) 1920.0 2328.7 2482.7 2326.8 1872.0
MMBench-ENtest(Liuetal.,2023d) 86.5(Chenetal.,2024c) 79.7 83.4 86.5 83.0 74.9
MMBench-CNtest(Liuetal.,2023d) 86.3(Chenetal.,2024c) 80.7 82.1 86.6 80.5 73.5
MMBench-V1.1test(Liuetal.,2023d) 85.5(Chenetal.,2024c) 78.5 82.2 85.9 80.7 72.2
MMT-Benchtest(Yingetal.,2024) 63.4(Chenetal.,2024b) - 65.5 71.7 63.7 54.5
MMStar(Chenetal.,2024a) 67.1(Chenetal.,2024c) 62.2 63.9 68.3 60.7 48.0
MMVetGPT-4-Turbo(Yuetal.,2024) 67.5(OpenAI.,2023) 66.0 69.1 74.0 62.0 49.5
HallBenchavg(Guanetal.,2023) 55.2(Chenetal.,2024c) 49.9 55.0 58.1 50.6 41.7
MathVistatestmini(Luetal.,2024a) 69.0(X.AI,2024b) 67.7 63.8 70.5 58.2 43.0
MathVision(Wangetal.,2024) 30.3(OpenAI,2023) - 30.4 25.9 16.3 12.4
Table3: PerformanceofQwen2-VLandGPT-4ooninternalmultilingualOCRbenchmarks.
Language Korean Japanese French German Italian Russian Vietnamese Arabic
GPT-4o 87.8 88.3 89.7 88.3 74.1 96.8 72.0 75.9
Qwen2-VL-72B 94.5 93.4 94.1 91.5 89.8 97.2 73.0 70.7
3.1 ComparetoSOTAs
Weevaluatethevisualcapabilitiesofourmodelthroughvariousvisualbenchmarks,videotasks,andagent-
basedassessments. Qwen2-VLdemonstrateshighlycompetitiveperformanceatthesamescale,achieving
new state-of-the-art (SoTA) results. Overall, our 72B model consistently delivers top-tier performance
acrossmostevaluationmetrics,frequentlysurpassingevenclosed-sourcemodelssuchasGPT-4o(OpenAI,
2024)andClaude3.5-Sonnet(Anthropic,2024). Notably,itexhibitsasignificantadvantageindocument
understandingtasks. However,intheMMMU(Yueetal.,2023)benchmark,ourmodelstilllagsbehind
GPT-4o to some extent, indicating that Qwen2-VL-72B has room for improvement when handling more
complexandchallengingproblemsets.
3.2 QuantitativeResults
Inthissection,wepresentanextensiveevaluationoftheQwen2-VLseriesacrossanarrayofdatasets,offering
acomprehensiveunderstandingofthemodel’scapabilitiesinvariousaspects.
3.2.1 GeneralVisualQuestionAnswering
Torigorouslyassessourmodels’capabilitiesingeneralvisualquestionansweringtasks,weconductextensive
evaluations across a diverse array of state-of-the-art benchmarks: RealWorldQA (X.AI, 2024a), MMStar
(Chenetal.,2024a),MMVet(Yuetal.,2024),MMT-Bench(Yingetal.,2024),MMBench(Liuetal.,2023d),
MMbench-1.1 (Liu et al., 2023d), MME (Fu et al., 2023), and HallusionBench (Guan et al., 2023). The
Qwen2-VLseriesexhibitsexceptionalperformanceacrossthesebenchmarks,withthe72Bmodelconsistently
achievingorsurpassingstate-of-the-artresults,whilethe7Band2Bvariantsalsodemonstraterobustcapa-
bilities. OnRealWorldQA,whichevaluatesreal-worldspatialcomprehension,Qwen2-VL-72Bachievesa
scoreof77.8,surpassingboththepreviousstate-of-the-art(72.2)andformidablebaselinessuchasGPT-4o
9Table4: PerformanceofQwen2-VLandothermodelsonvideobenchmarks.
Benchmark PreviousSoTA Gemini1.5-Pro GPT-4o Qwen2-VL-72B Qwen2-VL-7B Qwen2-VL-2B
MVBench(Lietal.,2024) 69.6 - - 73.6 67.0 63.2
PerceptionTesttest(Patrauceanetal.,2024) 66.9 - - 68.0 62.3 53.9
EgoSchematest(Mangalametal.,2023) 62.0 63.2 72.2 77.9 66.7 54.9
Video-MME(wo/wsubs)(Fuetal.,2024) 66.3/69.6 75.0/81.3 71.9/77.2 71.2/77.8 63.3/69.0 55.6/60.4
Table5: PerformanceComparisonofQwen2-VL-72BacrossvariousagentbenchmarksandGPT-4o. SR,GC,
TMandEMareshortforsuccessrate,goal-conditionsuccess,typematchandexactmatch. ALFRED,R2R
andREVERIEareperformanceinvalid-unseen.
Benchmark Metric PreviousSoTA GPT-4o Qwen2-VL-72B
TM - 90.2 93.1
General FnCall
EM - 50.0 53.2
TM 83.0(Hongetal.,2023) 70.0 89.6
UIOperations AITZ(Zhangetal.,2024b)
EM 47.7(ZhanandZhang,2023) 35.3 72.1
NumberLine(Zhaietal.,2024) SR 89.4(Zhaietal.,2024) 91.5 100.0
BlackJack(Zhaietal.,2024) SR 40.2(Zhaietal.,2024) 34.5 42.6
CardGames
EZPoint(Zhaietal.,2024) SR 50.0(Zhaietal.,2024) 85.5 100.0
Point24(Zhaietal.,2024) SR 2.6(Liuetal.,2023b) 3.0 4.5
SR 67.7(Luetal.,2023) - 67.8
RoboticControl ALFRED(Shridharetal.,2020a)
GC 75.3(Luetal.,2023) - 75.8
R2R(Andersonetal.,2018) SR 79.0(Chenetal.,2022) 43.7 51.7
Navigation
REVERIE(Qietal.,2020) SR 61.0(Sigurdssonetal.,2023) 31.6 31.0
(75.4),thusdemonstratingsuperiorunderstandingofphysicalenvironments. ForMMStar,abenchmark
designedtoassessgenuinemultimodalcapabilitiesthroughvisuallyindispensablesamples,Qwen2-VL-72B
attains68.3,outperformingthepreviousbestof67.1andhighlightingitsproficiencyinintegratingvisual
andtextualinformation. OnMMVet,whichevaluatestheintegrationofcorevision-languagecapabilities
across16complexmultimodaltasks,Qwen2-VL-72Bachievesaremarkable74.0,significantlyoutperforming
strongcompetitorsincludingGPT-4V(67.5)andshowcasingitsversatilityinaddressingdiversemultimodal
challenges. IntheMMT-Benchevaluation,whichassessesadvancedreasoningandinstructionfollowing
across32coremeta-tasksand162subtasksinmultimodalunderstanding, Qwen2-VL-72Bachieves71.7,
markedlysurpassingthepreviousbest(63.4)anddemonstratingitsprowessinapplyingexpertknowledge
andexecutingdeliberatevisualrecognition,localization,reasoning,andplanning. OnMMBench,which
evaluatesfine-grainedabilitiesacross20dimensions,Qwen2-VL-72Bexhibitsstrongperformance,achieving
86.5ontheEnglishtestset,matchingthestate-of-the-art,and86.6ontheChinesetestset,establishinga
newbenchmark. ForMME,whichmeasuresawidespectrumofperceptionandcognitionabilitiesacross14
subtasks,Qwen2-VL-72Bachievesacumulativescoreof2482.7,significantlyoutperformingthepreviousbest
(2414.7),underscoringitsadvancedcapabilitiesinbothvisualperceptionandhigh-levelcognitiontasks.
These comprehensive results underscore the Qwen2-VL series’ exceptional proficiency in general visual
questionansweringtasks. Themodelsdemonstrateadvancedcapabilitiesinreal-worldspatialcomprehension,
genuinemultimodalintegration,complexreasoning,instructionfollowing,andabroadrangeofperception
and cognition tasks. The consistent superior performance across diverse benchmarks, particularly the
outstandingresultsofthe72Bmodel,positionstheQwen2-VLseriesasaleadingsolutioninthefieldof
visual question answering. Our models excel in handling visually indispensable tasks, integrating core
vision-languagecapabilities,anddemonstratingexpertiseacrossdiversemultimodalscenarios,rangingfrom
fundamentalperceptiontaskstocomplexreasoningandplanning. Thisexhaustiveevaluationhighlightsthe
Qwen2-VLseries’versatilityandeffectivenessinaddressingthemultifacetedchallengesposedbystate-of-
the-artmultimodalbenchmarks,therebysettinganewstandardforlargevision-languagemodels.
103.2.2 DocumentandDiagramsReading
Wetestedourmodel’sOCRanddocumentanddiagramcomprehensiononDocVQA(Mathewetal.,2021),
ChartQA(Masryetal.,2022),InfoVQA(Mathewetal.,2021),TextVQA(Singhetal.,2019),AI2D(Kembhavi
etal.,2016)datasets. TheDocVQA/InfoVQA/ChartQAdatasetfocusesonthemodel’sabilitytocomprehend
textindocuments/high-resolutioninfographics/charts,whiletheTextVQAdatasetexaminestheabilityto
comprehendtextinnaturalisticimages. TheOCRBenchdatasetisaadatasetofmixedtasks,whichfocuses
onmathematicalformulaparsingandinformationextractioninadditiontothetext-basedVQA.TheAI2D
datasetfocusesonmultiple-choicequestionsonscientificdiagramscontainingtext. Inaddition,wealso
testedtheOCRandformularecognitioncapabilitiesofourmodelonOCRBench(Liuetal.,2023e),aswell
asthemultilingualOCRcapabilitiesofourmodelontheMTVQA(Tangetal.,2024)dataset.
TheexperimentalresultsshowthatourmodelachievesSoTAlevelinseveralmetrics,includingDocVQA,
InfoVQA, TextVQA and OCRBench, demonstrating that our model has good comprehension of textual
contentinimagesfrommultipledomains.
3.2.3 MultilingualTextRecognitionandUnderstanding
Inparticular,ourmodelsurpassesallexistinggeneral-purposeLVLMsinmultilingualOCR.Ourmodelnot
onlyoutperformsexistingLVLMs(includingproprietarymodelssuchasGPT-4o,Claude3.5Sonnet,etc.) on
thepublic-availableMTVQAdataset,italsooutperformsGPT-4oonthein-houseinternalbenchmarkacross
allforeignlanguagesexceptArabic(Table3).
3.2.4 MathematicalReasoning
We’veconductedexperimentsontheMathVista(Luetal.,2024a)andMathVision(Wangetal.,2024)datasets
toassessmathematicalreasoningcapabilities. MathVistaisacomprehensivebenchmarkfeaturing6,141
diverseexamplesofmathematicalandvisualtasks. TheMathVisiondatasetcomprises3,040mathproblems
embedded in visual contexts from actual math competitions, covering 16 mathematical disciplines and
varying in difficulty across five levels. These challenges underscore the necessity for LVLMs to exhibit
strongvisualcomprehension,adeepunderstandingofmathematics,andsoundlogicalreasoningskills. The
Qwen2-VLserieshasdemonstratedsuperiorperformanceonMathVista,achievinga70.5outperforming
otherLVLMs. Additionally,ithassetanewopen-sourcebenchmarkonMathVisionwith25.9.
3.2.5 ReferringExpressionComprehension
Regarding visual localization task, we evaluate Qwen2-VL on RefCOCO, RefCOCO+, and RefCOCOg
datasets (Kazemzadeh et al., 2014; Mao et al., 2016). The results, as depicted in Table 6, demonstrate
thatQwen2-VLattainstop-tierresultsamonggeneralistmodels. Benefitingfromamorerationalstructure
design,Qwen2-VLisabletoperceivedetailsinhigh-resolutionimages,leadingtosignificantimprovements
overQwen-VL.Thesuperiorityofthesemodelsincomparisontobothgeneralistandspecializedmodels
highlights their potential for advancing the field of visual localization and their capacity for real-world
implementationintasksrequiringprecisevisualunderstanding.
3.2.6 VideoUnderstanding
Weevaluateourmodelsonvariousvideounderstandingtasks, withrelatedbenchmarkscoveringshort
videosofafewsecondstolongvideosofuptoonehour. Table4presentstheperformanceofQwen2-VL
andbaselinemodels. Overall,Qwen2-VLdemonstratesstrongresultsacross2B,7B,and72Bsizes,with
Qwen2-VL-72BachievingthebestperformanceonMVBench(Lietal.,2024),PerceptionTest(Patraucean
etal.,2024),andEgoSchema(Mangalametal.,2023). ThisshowcasesQwen2-VL’ssuperiorcapabilitiesin
11Table6: PerformanceComparisononReferringExpressionComprehensionTask.
RefCOCO RefCOCO+ RefCOCOg
Type Model
val test-A test-B val test-A test-B val test
OFA-L(Wangetal.,2022) 80.0 83.7 76.4 68.3 76.0 61.8 67.6 67.6
Shikra(Chenetal.,2023a) 87.0 90.6 80.2 81.6 87.4 72.1 82.3 82.2
Qwen-VL(Baietal.,2023b) 89.4 92.3 85.3 83.1 88.3 77.2 85.6 85.5
Ferretv2(Zhangetal.,2024a) 92.6 95.0 88.9 87.4 92.1 81.4 89.4 90.0
CogVLM(Wangetal.,2023b) 92.8 94.8 89.0 88.7 92.9 83.4 89.8 90.8
Generalist InternVL2 (Chenetal.,2024c) 82.3 88.2 75.9 73.5 82.8 63.3 77.6 78.3
2b
InternVL2 (Chenetal.,2024c) 87.1 91.1 80.7 79.8 87.9 71.4 82.7 82.7
8b
InternVL2 (Chenetal.,2024c) 92.2 94.8 88.4 88.8 93.1 82.8 89.5 90.3
76b
Qwen2-VL 87.6 90.6 82.3 79.0 84.9 71.0 81.2 80.3
2b
Qwen2-VL 91.7 93.6 87.3 85.8 90.5 79.5 87.3 87.8
7b
Qwen2-VL 93.2 95.3 90.7 90.1 93.8 85.6 89.9 90.4
72b
G-DINO-L(Liuetal.,2023c) 90.6 93.2 88.2 82.8 89.0 75.9 86.1 87.0
Specialist UNINEXT-H(Yanetal.,2023) 92.6 94.3 91.5 85.2 89.6 79.8 88.7 89.4
ONE-PEACE(Wangetal.,2023a) 92.6 94.2 89.3 88.8 92.2 83.2 89.2 89.3
videounderstandingtasks,andscalingupQwen2-VLyieldssignificantimprovements. Forthechallenging
Video-MMEbenchmark(Fuetal.,2024),whichincludesvideosuptoonehour,itisnoteworthythatwe
limitedthemaximumnumberofframesextractedpervideoto768duringevaluation,potentiallyimpacting
performanceonlongervideos. FutureworkwillfocusonextendingQwen2-VLtosupportlongersequences,
therebyaccommodatinglongervideos.
3.2.7 VisualAgent
Qwen2-VLisevaluatedfirstforitsabilitytointeractwiththeenvironmentviafunctioncallsandthenfor
its capacity to complete complex sequential decision tasks through multiple rounds of interaction. The
implementationisbasedontheQwen-Agentframework(QwenTeam,2024).
Function Calling Unlike function calling in LLMs (Yan et al., 2024; Srinivasan et al., 2023; Chen et al.,
2023c),functioncallinginLVLMsofteninvolvesextractinginformationfromvisualcues. Duetotheabsence
ofpublicbenchmarksforevaluatingthecapabilitiesofLVLMsinfunctioncalling,weconstructedourinternal
evaluationdataset.
To construct the evaluation dataset, we undertook the following procedures (Chen et al., 2023c): Scene
Categorization, Image Collection, Image Content Extraction, and Question/Functions/Arguments Gen-
eration. Firstly,weclassifiedscenesintocategoriesbasedondifferentvisualapplications. Subsequently,
wedownloadedandmeticulouslyselectedhigh-quality,representativeimagesfromtheinternetforeach
category. Thereafter,utilizinganadvancedLVLM(Baietal.,2023b),weanalyzedeachimagetoextractkey
visualelementsandtextualinformation. Finally,basedonthecontentinformationfromtheimages,weused
anadvancedLLM(Yangetal.,2024)togenerateaseriesofquestionsthatrequiredspecificfunctionsto
answer,alongwithspecifyingtheinputparametersneededforthesefunctioncalls.
SimilartothefunctioncallingevaluationmethodinLLMs(Yanetal.,2024),wedesignedtwometricsto
evaluatetheaccuracyofthefunctionselectionandthecorrectnessoftheargumentsinput. Specifically,Type
Match(TM),iscalculatedastheratiooftimesthemodelsuccessfullyinvokedthecorrectfunctiontothetotal
numberofcallsattempted. ExactMatch(EM),foreachfunctioncalling,wecheckedwhetherthearguments
passedtothefunctionexactlymatchedthoserecordedintheimage’scontentinformation,calculatingthis
correctnessratio.
AsshowninTable5,theperformanceofQwen2-VLinbothTypeMatch(93.1vs. 90.2)andExactMatch(53.2vs.
50.0)overGPT-4osubstantiatestheefficacyofQwen2-VL’scapabilityinfunctioncalling,therebyunderscoring
12itssignificantpotentialforapplicationexpansionthroughexternaltoolintegration.
TheevaluationresultsdemonstratedthatGPT-4ounderperformed,primarilyduetotwofactors: inscenarios
whereuncertaintyarises,GPT-4odemonstratesaconservativeapproachbyavoidingusingexternaltools.
TheOpticalCharacterRecognition(OCR)capabilityofGPT-4oisoutperformedbyQwen2-VL,particularly
inthecontextofChinesecharacters.
UIOperations/Games/Robotics/Navigation ToassessQwen2-VL’sabilitytogenerallyhandlecomplex
tasks,weconductevaluationsacrossmultipleVLagenttasks,includingmobileoperations(Zhangetal.,
2024b;Rawlesetal.,2024b;Luetal.,2024b;Rawlesetal.,2024a),roboticcontrol(Kolveetal.,2017;Shridhar
etal.,2020a;InoueandOhashi,2022;Luetal.,2023;Jiangetal.,2022;Huangetal.,2023b),cardgames(Zhai
etal.,2024),andvision-languagenavigation(). Asthesetasksneedmultipleactionstocompletetasks,we
keepthehistory(observation,action)throughQwen2-VLsupportsa32Kcontextlength,thenappendeach
newobservationimageaftereveryaction,enablingcontinuousreasoningaboutsubsequentsteps.
UIOperations: weevaluateQwen2-VLusingtheAITZtask(Zhangetal.,2024b),whichconstructsacore
cleantestsetderivedfromAITW(Rawlesetal.,2024b). Basedoncommonoperationpatternsofphone,we
defineactionssuchastap,inputandswipe(Rawlesetal.,2024b)forQwen2-VLtointeractwithon-screen
iconsfortaskcompletion. Forexample,whenQwen2-VListaskedwithfindingapizzarestaurantnearbyby
GoogleMaps,itshouldinput"pizza"inthesearchterm,swipetoselecttheappropriaterestaurant,andtap
thecorrespondinglink. FollowingtheAITZsetting,wereportbothtypematch(correctnessoftap,input,or
swipe)andexactmatch(correctnessoftaplocation,inputtext,orswipedirection). Withthesupportof
groundingcapabilityonUI,Qwen2-VLsurpassesGPT-4andpreviousSoTA(Zhangetal.,2024b;Zhanand
Zhang,2023).
RoboticControl: weevaluateQwen2-VLontheALFREDtask(Shridharetal.,2020a)inAI2THOR(Kolve
etal.,2017). Thetaskrequiresagenttoperformcomplexhouseholdtasks,suchastoastingbreadandslicing
anappletoprepareameal. Toworkinthevirtualenvironment,wedefinehigh-levelactions(GotoLocation,
Pickup,PutDown,Open,Close,Clean,Heat,Cool,Slice)(Shridharetal.,2020b)astheactionset. Moreover,
agentneedstolocalizeobjectsformanipulation(e.g.,itcanonlypickupanappleiftheappleisrecognized).
Toimprovetheaccuracyofmanipulation,weintegrateSAM(Kirillovetal.,2023). ALFREDtaskreports
tasksuccessrate(SR)(e.g.,preparingdinner)andsub-goalcompletionmetrics(GC)(e.g.,whetherthe
breadistoastedortheappleissliced). Qwen2-VLslightlyoutperformsthepreviouslyspecializedmodel
ThinkBot(Luetal.,2023)onthevalid-unseenset.
CardGames: weleveragethecardgameenvironmentfromRL4VLM(Zhaietal.,2024)toassessQwen2-VL’s
performanceinaseriesofcard-basedgames: NumberLine,BlackJack,EZPoint,andPoint24. Eachgame
presentsdistinctchallenges: (1)reachingatargetnumberusing+1or-1operations,(2)drawingorholding
cardstocompeteagainstthedealer,(3)applyingbasicarithmeticoperationstoreachatotalof12,and(4)
usingarithmeticoperationstoachieveatotalof24. Wereportthesuccessrateofthetasks. Theynotonly
evaluateagentcapabilitiesbutalsorequirestrongOCRskillstorecognizethesecardsandunderstandthe
progressionofthegame. Qwen2-VLdemonstratessuperiorperformanceacrossalltasks.
Vision-LanguageNavigation: weevaluateQwen2-VLontheVision-and-LanguageNavigation(VLN)task
usingtheR2R(Andersonetal.,2018)andREVERIE(Qietal.,2020). InVLN,themodelmustautonomously
determinethenextlocationbasedoninstruction,currentobservations. Wereportthesuccessrate(SR)of
VLMinreachingthepredetermineddestinationforthistask. TheperformanceofQwen2-VLiscomparable
tothatofGPT-4o,butbothmodelsfallsignificantlybehindcurrentspecializedVLNmodels(Chenetal.,
2022;Sigurdssonetal.,2023). Weattributethisgaptotheincompleteandunstructuredmapinformation
generatedbythemodelfrommultipleimages. Accuratelymodelingmapsandlocationsina3Denvironment
remainsamajorchallengeformultimodalmodels.
13Table 7: Qwen2-VL-7B under fixed/dynamic image tokens. Adjusting image sizes only results in small
perturbationsinperformance,demonstratingtherobustnesstovaryingimagesizes. Moreover,thedynamic
resolutionstrategyachievestop-tierperformancewhileconsumingfewertokensonaverage,demonstrating
theefficiencyofourmodel.
Strategy AverageImageTokens InfoVQA RealWorldQA OCRBench MMMU
val
64 28.85 56.47 572 53.33
576 65.72 65.88 828 52.78
FixedImageTokens
1600 74.99 69.54 824 52.89
3136 77.27 70.59 786 53.44
DynamicImageTokens 1924 75.89 70.07 866 53.44
Figure4:Qwen2-VL-7Bwithdifferentmin_pixels. Smallimagesareupscaledtosurpassaspecifiedmin_pixels
thresholdbeforeinputintothemodel. Increasingtheimagesizewithinareasonablerangeshowsenhanced
performanceonperceptualtaskslikeInfoVQA,HallusionBench,andOCRBench.
3.3 AblationStudy
Inthissection,wepresentablationstudiesonimagedynamicresolution,M-RoPE,andmodelscale. These
experimentsaimtoprovideinsightsintotheimpactofthesekeycomponentsonourmodel’sperformance.
3.3.1 DynamicResolution
AsshowninTable7,wecomparetheperformancebetweendynamicresolutionandfixedresolution. For
fixedresolution,weresizetheimagestoensureaconstantnumberofimagetokensbeinginputtothemodel,
ratherthanresizingtoaspecificheightandwidth,asthiswoulddistorttheoriginalaspectratio. Fordynamic
resolution,weonlysetmin_pixels=100×28×28andmax_pixels=16384×28×28,allowingthenumber
ofimagetokensdependprimarilyontheimage’snativeresolution. Itcanbeobservedthatadjustingimage
sizesonlyresultsinsmallperturbationsinperformance,demonstratingthemodelrobustnesstovarying
imagesizes. Moreover,dynamicresolutionapproachismoreefficient. Wecanobservethatnosinglefixed
resolutionachievesoptimalperformanceacrossallbenchmarks. Incontrast,thedynamicresolutionapproach
consistentlyachievestop-tierperformancewhileconsumingfewertokensonaverage.
Additionally,weobservethatmerelyincreasingtheimagesizedoesnotalwaysleadtoimprovedperformance.
Itismoreimportanttochooseanappropriateresolutionfordifferentimages. AsdetailedinFigure4,we
upscalesmallimagestosurpassaspecifiedmin_pixelsthreshold. Evaluationsonupscaledimagesshows
enhancedperformanceonperceptualtaskslikeInfoVQA,HallusionBench,andOCRBench. Weattribute
thesegainstoincreasedcomputationalload. However,forOCRBench,atoo-highmin_pixelsvalueleadstoa
severeperformancedecline. ThisislikelybecauseOCRBenchcontainsnumerousextremelysmallimages,
andexcessiveenlargementcausestheseimagestodeviatefromthetrainingdatadistribution,turningthem
intoout-of-distributionsamples. Incontrast,theeffectofincreasingmin_pixelsontheMMMUbenchmark
isnegligible. WehypothesizethattheperformancebottleneckinMMMUismorerelatedtothemodel’s
14Table8: AblationstudiesofM-RoPE.Comparedto1D-RoPE,usingM-RoPEachievesbetterperformancein
downstreamtasks,particularlyinvideobenchmarks. RWQmeansRealworldQA.
ImageBenchmarks VideoBenchmarks
MathVista MMB MMStar RWQ DocVQA ChartQA InfoVQA TextVQA PerceptionTest NextQA STAR
1D-RoPE 39.2 58.6 36.7 54.5 82.5 68.0 50.8 71.3 46.6 43.9 55.5
M-RoPE 43.4 60.6 36.7 53.7 82.8 68.4 50.3 71.8 47.4 46.0 57.9
Training Sequence Length
Figure5: EvaluatethelengthextrapolationcapabilityofQwen2-VL-72BonVideo-MMEMediumVideo.
WiththehelpofM-RoPE,themodeldemonstratedrobustperformancewhentheinferencelengthexceeded
themaximumtraininglengthof16384tokens.
reasoningcapabilityratherthanimageresolution.
3.3.2 M-RoPE
Inthissubsection,wedemonstratetheeffectivenessofM-RoPE.First,wevalidateitscapabilityonvarious
downstreamtasks. WeemployQwen2-1.5BandViT-Lasthebackboneandreporttheresultsofthepre-
trainedmodels. AsshowninTable8,comparedto1D-RoPE,usingM-RoPEachievesbetterperformance
indownstreamtasks,particularlyinvideobenchmarks. Furthermore,weassessthelengthextrapolation
capabilityofM-RoPEonVideo-MMEmedium-lengthvideos. Figure5illustratestheperformanceofQwen2-
VL-72Batdifferentinferencelengths. LeveragingM-RoPE,themodeldemonstratesrobustresultsacross
variousinferencelengths. Notably,despitelimitingthemaximumtokenspervideoto16Kduringtraining,
themodelstillexhibitsexceptionalperformanceatamaximuminferencelengthof80Ktokens.
3.3.3 ModelScaling
Weevaluatetheperformanceofmodelsofvaryingscalesacrossmultiplecapabilitydimensions. Specifically,
wecategorizethesedimensionsintocomplexcollege-levelproblem-solving,mathematicalabilities,document
and table comprehension, general scenario question-answering, and video comprehension. The overall
capabilityofamodelisassessedbyaveragingitsscoresacrossdifferentbenchmarksassociatedwitheach
dimension.
Inparticular,weusetheMMMU(Yueetal.,2023)benchmarktorepresentcollege-levelproblem-solving
ability, while the average scores from MathVista (Lu et al., 2024a) and MathVision (Wang et al., 2024)
serveasindicatorsofmathematicalability. Forgeneralscenarioquestion-answering,wecomputetheav-
eragescoreacrosstheRealWorldQA(X.AI,2024a),MMBench-V1.1(Liuetal.,2023d),MMT-Bench(Ying
et al., 2024), HallBench (Guan et al., 2023), MMVet (Yu et al., 2024), and MMStar (Chen et al., 2024a)
15Figure6: ModelPerformanceScalingAcrossCapabilitiesandTrainingProgress. Asmodelsizeandthe
volume of training data increase, performance consistently improves across a range of capabilities and
benchmarks.
benchmarks. Documentandtablecomprehensioncapabilityisreflectedthroughtheaveragescorefrom
benchmarkslikeDocVQA(Mathewetal.,2021),InfoVQA(Mathewetal.,2021),ChartQA(Masryetal.,
2022),TextVQA(Singhetal.,2019),OCRBench(Liuetal.,2023e),andMTVQA(Tangetal.,2024). Lastly,
videocomprehensionabilityismeasuredbyaveragingscoresacrossMVBench(Lietal.,2024),Perception-
Test(Patrauceanetal.,2024),EgoSchema(Mangalametal.,2023),andVideo-MME(Fuetal.,2024).
AsillustratedinFigure6(a),thereisaconsistentimprovementinperformancewithincreasingmodelsize,
particularlywithrespecttomathematicalabilities,whichshowapositivecorrelationwiththenumberof
modelparameters. Ontheotherhand,foropticalcharacterrecognition(OCR)-relatedtasks,evensmaller-
scalemodelsexhibitrelativelystrongperformance.
As shown in Figure 6(b), we visualize the relationship between model performance and the number of
trainingtokensduringthesecondstageofpretrainingforQwen2-VL-7B.Asthenumberoftrainingtokens
increases,themodelperformanceimproves;however,performanceonvisionquestionanswering(VQA)tasks
exhibitssomefluctuation. Incontrast,fortaskssuchasAI2D(Kembhavietal.,2016)andInfoVQA(Mathew
etal.,2021)—bothofwhichinvolveunderstandingtextualandgraphicalinformationinimages—themodel
performanceshowssteadyimprovementastrainingdataisaugmented.
4 Conclusion
WehavepresentedtheQwen2-VLseries,theversatilelargevision-languagemodels,includingthreeopen-
weightmodelswithtotalparametercountsof2,8,and72billion. Qwen2-VLmatchestheperformanceof
top-tiermodelslikeGPT-4oandClaude3.5-Sonnetinarangeofmultimodalscenarios,surpassingallother
open-weightLVLMmodels. Qwen2-VLseriesintroducesnaivedynamicresolutionandmultimodalrotary
positionembedding(M-RoPE)tofuseinformationacrossmodalseffectivelyandbecapableofunderstanding
videosover20minutesinlength. Withadvancedreasoninganddecision-makingabilities,Qwen2-VLcan
be integrated with devices such as mobile phones, robots, etc. Furthermore, Qwen2-VL now supports
understandingmultilingualtextswithinimages,includingmostEuropeanlanguages,Japanese,Korean,
Arabic,Vietnamese,andothers.
WehavemadetheQwen2-VLmodelweightsopenlyaccessible,whichenablesresearchersanddevelopersto
harnessthefullpotentialinavarietyofapplicationsandresearchprojects. WeaimtoadvanceAItechnologies
andenhancetheirbeneficialeffectsonsocietybydedicatingourselvestotheseendeavors.
16Acknowledgements
WeexpressourgratitudetoJuanZhu,FanHong,JieZhang,YongLiofAlibabaCloud’sPAIteam(Alibaba-
Cloud,2024c)forsupportingthetraininginfrastructureofQwen2-VL.Thisworkwasalsosupportedby
QwenLLMteam(Yangetal.,2024),andweespeciallythankNaNi,YichangZhang,JianxinMa,BowenYu,
ZherenFufortheirdatacontributionandinsightfuldiscussion.
References
Jean-BaptisteAlayrac,JeffDonahue,PaulineLuc,AntoineMiech,IainBarr,YanaHasson,KarelLenc,Arthur
Mensch,KatherineMillican,MalcolmReynolds,etal. Flamingo: avisuallanguagemodelforfew-shot
learning. InNeurIPS,2022. 2
Alibaba-Cloud. Cloud parallel file storage (cpfs), 2024a. URL https://www.alibabacloud.com/en/
product/cpfs. 8
Alibaba-Cloud. Objectstorageservice(oss),2024b. URLhttps://www.alibabacloud.com/en/product/
object-storage-service. 8
Alibaba-Cloud. Pai-lingjunintelligentcomputingservice,2024c. URLhttps://www.alibabacloud.com/en/
product/pai-lingjun. 8,17
Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko Sünderhauf, Ian Reid, Stephen
Gould,andAntonVanDenHengel. Vision-and-languagenavigation: Interpretingvisually-grounded
navigationinstructionsinrealenvironments. InCVPR,2018. 10,13
JasonAnsel, EdwardZ.Yang, HoraceHe, NataliaGimelshein, AnimeshJain, MichaelVoznesensky, Bin
Bao,PeterBell,DavidBerard,EvgeniBurovski,GeetaChauhan,AnjaliChourdia,WillConstable,Alban
Desmaison,ZacharyDeVito,EliasEllison,WillFeng,JiongGong,MichaelGschwind,BrianHirsh,Sher-
lockHuang,KshiteejKalambarkar,LaurentKirsch,MichaelLazos,MarioLezcano,YanboLiang,Jason
Liang,YinghaiLu,C.K.Luk,BertMaher,YunjiePan,ChristianPuhrsch,MatthiasReso,MarkSaroufim,
MarcosYukioSiraichi,HelenSuk,ShuntingZhang,MichaelSuo,PhilTillet,XuZhao,EikanWang,Keren
Zhou,RichardZou,XiaodongWang,AjitMathews,WilliamWen,GregoryChanan,PengWu,andSoumith
Chintala. Pytorch2: Fastermachinelearningthroughdynamicpythonbytecodetransformationandgraph
compilation. InASPLOS,2024. 8
Anthropic. Claude3.5sonnet,2024. URLhttps://www.anthropic.com/news/claude-3-5-sonnet. 9
AnuragArnab,MostafaDehghani,GeorgHeigold,ChenSun,MarioLučić,andCordeliaSchmid. Vivit: A
videovisiontransformer. InICCV,2021. 5
LeiJimmyBa,JamieRyanKiros,andGeoffreyE.Hinton. Layernormalization. arXiv:1607.06450,2016. 8
JinzeBai,ShuaiBai,YunfeiChu,ZeyuCui,KaiDang,XiaodongDeng,YangFan,WenbinGe,YuHan,Fei
Huang,etal. Qwentechnicalreport. arXiv:2309.16609,2023a. 1
JinzeBai,ShuaiBai,ShushengYang,ShijieWang,SinanTan,PengWang,JunyangLin,ChangZhou,and
JingrenZhou. Qwen-vl: Afrontierlargevision-languagemodelwithversatileabilities. arXiv:2308.12966,
2023b. 1,2,3,5,12
JoaoCarreiraandAndrewZisserman. Quovadis,actionrecognition? anewmodelandthekineticsdataset.
InCVPR,2017. 5
Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra: Unleashing
multimodalllm’sreferentialdialoguemagic. arXiv:2306.15195,2023a. 12
17LinChen,JisongLi,XiaoyiDong,PanZhang,ConghuiHe,JiaqiWang,FengZhao,andDahuaLin.Sharegpt4v:
Improvinglargemulti-modalmodelswithbettercaptions. arXiv:2311.12793,2023b. 1
LinChen, JinsongLi, XiaoyiDong, PanZhang, YuhangZang, ZehuiChen, HaodongDuan, JiaqiWang,
Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models?
arXiv:2403.20330,2024a. 9,15
ShizheChen,Pierre-LouisGuhur,MakarandTapaswi,CordeliaSchmid,andIvanLaptev. Thinkglobal,act
local: Dual-scalegraphtransformerforvision-and-languagenavigation. InCVPR,2022. 10,13
TianqiChen,BingXu,ChiyuanZhang,andCarlosGuestrin. Trainingdeepnetswithsublinearmemorycost.
arXiv:1604.06174,2016. 8
ZehuiChen,WeihuaDu,WenweiZhang,KuikunLiu,JiangningLiu,MiaoZheng,JingmingZhuo,Songyang
Zhang, Dahua Lin, Kai Chen, et al. T-eval: Evaluating the tool utilization capability step by step.
arXiv:2312.14033,2023c. 12
ZheChen,WeiyunWang,HaoTian,ShenglongYe,ZhangweiGao,ErfeiCui,WenwenTong,KongzhiHu,
JiapengLuo,ZhengMa,etal. Howfararewetogpt-4v? closingthegaptocommercialmultimodalmodels
withopen-sourcesuites. arXiv:2404.16821,2024b. 9
ZheChen,WeiyunWang,HaoTian,ShenglongYe,ZhangweiGao,ErfeiCui,WenwenTong,KongzhiHu,
JiapengLuo, ZhengMa, etal. Internvl2: Betterthanthebest—expandingperformanceboundariesof
open-sourcemultimodalmodelswiththeprogressivescalingstrategy,2024c. URLhttps://internvl.
github.io/blog/2024-07-02-InternVL-2.0. 9,12
Wei-LinChiang,ZhuohanLi,ZiLin,YingSheng,ZhanghaoWu,HaoZhang,LianminZheng,SiyuanZhuang,
Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot
impressinggpt-4with90%*chatgptquality,2023. URLhttps://lmsys.org/blog/2023-03-30-vicuna/.
1
WenliangDai, JunnanLi, DongxuLi, AnthonyMengHuatTiong, JunqiZhao, WeishengWang, Boyang
Li,PascaleFung,andStevenHoi. Instructblip: Towardsgeneral-purposevision-languagemodelswith
instructiontuning. arXiv:2305.06500,2023. 1
TriDao. Flashattention-2: Fasterattentionwithbetterparallelismandworkpartitioning. InICLR,2024. 8
TriDao,DanielY.Fu,StefanoErmon,AtriRudra,andChristopherRé. Flashattention: Fastandmemory-
efficientexactattentionwithio-awareness. InNeurIPS,2022. 8
Mostafa Dehghani, Basil Mustafa, Josip Djolonga, Jonathan Heek, Matthias Minderer, Mathilde Caron,
AndreasSteiner,JoanPuigcerver,RobertGeirhos,IbrahimMAlabdulmohsin,etal. Patchn’pack: Navit,a
visiontransformerforanyaspectratioandresolution. InNeurIPS,2024. 4
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Un-
terthiner,MostafaDehghani,MatthiasMinderer,GeorgHeigold,SylvainGelly,JakobUszkoreit,andNeil
Houlsby. Animageisworth16x16words: Transformersforimagerecognitionatscale. InICLR,2021. 4
AbhimanyuDubey,AbhinavJauhri,AbhinavPandey,AbhishekKadian,AhmadAl-Dahle,AieshaLetman,
AkhilMathur,AlanSchelten,AmyYang,AngelaFan,etal. Thellama3herdofmodels. arXiv:2407.21783,
2024. 45,47,48
AlexFang,AlbinMadappallyJose,AmitJain,LudwigSchmidt,AlexanderToshev,andVaishaalShankar.
Datafilteringnetworks. arXiv:2309.17425,2023. 5
FFmpeg-Developers. ffmpegtool,2024. URLhttp://ffmpeg.org/. 8
ChaoyouFu,PeixianChen,YunhangShen,YuleiQin,MengdanZhang,XuLin,ZhenyuQiu,WeiLin,Jinrui
Yang,XiawuZheng,etal. Mme: Acomprehensiveevaluationbenchmarkformultimodallargelanguage
models. arXiv:2306.13394,2023. 9
18ChaoyouFu,YuhanDai,YondongLuo,LeiLi,ShuhuaiRen,RenruiZhang,ZihanWang,ChenyuZhou,
YunhangShen,MengdanZhang,etal. Video-mme: Thefirst-evercomprehensiveevaluationbenchmarkof
multi-modalllmsinvideoanalysis. arXiv:2405.21075,2024. 10,12,16
TianruiGuan,FuxiaoLiu,XiyangWu,RuiqiXian,ZongxiaLi,XiaoyuLiu,XijunWang,LichangChen,Furong
Huang,YaserYacoob,DineshManocha,andTianyiZhou. Hallusionbench: Anadvanceddiagnosticsuite
forentangledlanguagehallucination&visualillusioninlargevision-languagemodels. arXiv:2310.14566,
2023. 9,15
WenyiHong,WeihanWang,QingsongLv,JiazhengXu,WenmengYu,JunhuiJi,YanWang,ZihanWang,
YuxiaoDong,MingDing,etal. Cogagent: Avisuallanguagemodelforguiagents. arXiv:2312.08914,2023.
10
Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei
Cui,OwaisKhanMohammed,QiangLiu,etal. Languageisnotallyouneed: Aligningperceptionwith
languagemodels. arXiv:2302.14045,2023a. 1,2
SiyuanHuang,ZhengkaiJiang,HaoDong,YuQiao,PengGao,andHongshengLi. Instruct2act: Mapping
multi-modalityinstructionstoroboticactionswithlargelanguagemodel. arXiv:2305.11176,2023b. 13
YanpingHuang,YoulongCheng,AnkurBapna,OrhanFirat,DehaoChen,MiaXuChen,HyoukJoongLee,
Jiquan Ngiam, Quoc V. Le, Yonghui Wu, and Zhifeng Chen. Gpipe: Efficient training of giant neural
networksusingpipelineparallelism. InNeurIPS,2019. 8
YukiInoueandHirokiOhashi. Prompter: Utilizinglargelanguagemodelpromptingforadataefficient
embodiedinstructionfollowing. arXiv:2211.03267,2022. 13
YunfanJiang,AgrimGupta,ZichenZhang,GuanzhiWang,YongqiangDou,YanjunChen,LiFei-Fei,Anima
Anandkumar,YukeZhu,andLinxiFan. Vima: Generalrobotmanipulationwithmultimodalprompts.
arXiv:2210.03094,2022. 13
SaharKazemzadeh,VicenteOrdonez,MarkMatten,andTamaraBerg. Referitgame: Referringtoobjectsin
photographsofnaturalscenes. InEMNLP,2014. 11
AniruddhaKembhavi, MikeSalvato, EricKolve, MinjoonSeo, HannanehHajishirzi, andAliFarhadi. A
diagramisworthadozenimages. InECCV,2016. 9,11,16
AlexanderKirillov,EricMintun,NikhilaRavi,HanziMao,ChloeRolland,LauraGustafson,TeteXiao,Spencer
Whitehead,AlexanderCBerg,Wan-YenLo,etal. Segmentanything. InICCV,2023. 13
Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli VanderBilt, Luca Weihs, Alvaro Herrasti, Matt Deitke,
Kiana Ehsani, Daniel Gordon, Yuke Zhu, et al. Ai2-thor: An interactive 3d environment for visual ai.
arXiv:1712.05474,2017. 13
VijayAnandKorthikanti,JaredCasper,SangkugLym,LawrenceMcAfee,MichaelAndersch,Mohammad
Shoeybi,andBryanCatanzaro. Reducingactivationrecomputationinlargetransformermodels. InMLSys,
2023. 8
AlexKrizhevsky,IlyaSutskever,andGeoffreyE.Hinton. Imagenetclassificationwithdeepconvolutional
neuralnetworks. InNeurIPS,2012. 8
JoelLamy-Poirier. Breadth-firstpipelineparallelism. InMLSys,2023. 8
BoLi,PeiyuanZhang,JingkangYang,YuanhanZhang,FanyiPu,andZiweiLiu. Otterhd: Ahigh-resolution
multi-modalitymodel. arXiv:2311.04219,2023a. 2
ChenLi,YixiaoGe,DianLi,andYingShan. Vision-languageinstructiontuning: Areviewandanalysis.
arXiv:2311.08172,2023b. 2
19JunnanLi,DongxuLi,SilvioSavarese,andStevenHoi. Blip-2: Bootstrappinglanguage-imagepre-training
withfrozenimageencodersandlargelanguagemodels. arXiv:2301.12597,2023c. 1
KunchangLi,YaliWang,YinanHe,YizhuoLi,YiWang,YiLiu,ZunWang,JilanXu,GuoChen,PingLuo,
etal. Mvbench: Acomprehensivemulti-modalvideounderstandingbenchmark. InCVPR,2024. 10,11,16
ShenLi,YanliZhao,RohanVarma,OmkarSalpekar,PieterNoordhuis,TengLi,AdamPaszke,JeffSmith,
BrianVaughan, PritamDamania, etal. Pytorchdistributed: Experiencesonacceleratingdataparallel
training. InVLDB,2020. 8
Zhang Li, Biao Yang, Qiang Liu, Zhiyin Ma, Shuo Zhang, Jingxu Yang, Yabo Sun, Yuliang Liu, and Xi-
angBai. Monkey: Imageresolutionandtextlabelareimportantthingsforlargemulti-modalmodels.
arXiv:2311.06607,2023d. 2
ZiyiLin,ChrisLiu,RenruiZhang,PengGao,LongtianQiu,HanXiao,HanQiu,ChenLin,WenqiShao,
KeqinChen, JiamingHan, SiyuanHuang, YichiZhang, XumingHe, HongshengLi, andYuJiaoQiao.
Sphinx: Thejointmixingofweights,tasks,andvisualembeddingsformulti-modallargelanguagemodels.
arXiv:2311.07575,2023. 2
HaotianLiu,ChunyuanLi,YuhengLi,andYongJaeLee. Improvedbaselineswithvisualinstructiontuning.
arXiv:2310.03744,2023a. 1,2
HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee. Visualinstructiontuning. arXiv:2304.08485,
2023b. 1,2,10
ShilongLiu,ZhaoyangZeng,TianheRen,FengLi,HaoZhang,JieYang,ChunyueLi,JianweiYang,HangSu,
Jun-JuanZhu,andLeiZhang. Groundingdino: Marryingdinowithgroundedpre-trainingforopen-set
objectdetection. arXiv:2303.05499,2023c. 12
YuanLiu,HaodongDuan,BoLiYuanhanZhang,SongyangZhang,WangboZhao,YikeYuan,JiaqiWang,
ConghuiHe,ZiweiLiu,KaiChen,andDahuaLin. Mmbench: Isyourmulti-modalmodelanall-around
player? arXiv:2307.06281,2023d. 9,15
Yuliang Liu, Zhang Li, Mingxin Huang, Biao Yang, Wenwen Yu, Chunyuan Li, Xucheng Yin, Cheng lin
Liu,LianwenJin,andXiangBai. Ocrbench: Onthehiddenmysteryofocrinlargemultimodalmodels.
arXiv:2305.07895,2023e. 9,11,16
IlyaLoshchilovandFrankHutter. Decoupledweightdecayregularization. InICLR,2019. 8
GuanxingLu,ZiweiWang,ChangliuLiu,JiwenLu,andYansongTang. Thinkbot: Embodiedinstruction
followingwiththoughtchainreasoning. arXiv:2312.07062,2023. 10,13
PanLu,RanGong,ShibiaoJiang,LiangQiu,SiyuanHuang,XiaodanLiang,andSong-ChunZhu. Inter-gps:
Interpretablegeometryproblemsolvingwithformallanguageandsymbolicreasoning. InACL,2021. 31
PanLu, HritikBansal, TonyXia, JiachengLiu, ChunyuanLi, HannanehHajishirzi, HaoCheng, Kai-Wei
Chang,MichelGalley,andJianfengGao. Mathvista: Evaluatingmathematicalreasoningoffoundation
modelsinvisualcontexts. InICLR,2024a. 9,11,15
QuanfengLu, WenqiShao, ZitaoLiu, FanqingMeng, BoxuanLi, BotongChen, SiyuanHuang, Kaipeng
Zhang,YuQiao,andPingLuo. Guiodyssey: Acomprehensivedatasetforcross-appguinavigationon
mobiledevices. arXiv:2406.08451,2024b. 13
KarttikeyaMangalam,RaiymbekAkshulakov,andJitendraMalik. Egoschema: Adiagnosticbenchmarkfor
verylong-formvideolanguageunderstanding. InNeurIPS,2023. 10,11,16
JunhuaMao,JonathanHuang,AlexanderToshev,OanaCamburu,AlanLYuille,andKevinMurphy. Gener-
ationandcomprehensionofunambiguousobjectdescriptions. InCVPR,2016. 11
20AhmedMasry,DoXuanLong,JiaQingTan,ShafiqJoty,andEnamulHoque. Chartqa: Abenchmarkfor
questionansweringaboutchartswithvisualandlogicalreasoning. arXiv:2203.10244,2022. 9,11,16
MineshMathew,DimosthenisKaratzas,andCVJawahar. Docvqa: Adatasetforvqaondocumentimages.
InWACV,2021. 9,11,16
DeepakNarayanan,MohammadShoeybi,JaredCasper,PatrickLeGresley,MostofaPatwary,VijayKorthikanti,
DmitriVainbrand,PrethviKashinkunti,JulieBernauer,BryanCatanzaro,AmarPhanishayee,andMatei
Zaharia. Efficientlarge-scalelanguagemodeltrainingonGPUclustersusingmegatron-lm. InSC,2021. 8
Nvidia. Apex,2024a. URLhttps://github.com/NVIDIA/apex. 8
Nvidia. Cuda,2024b. URLhttps://developer.nvidia.com/cuda-toolkit. 8
OpenAI. Gpt-4technicalreport. arXiv:2303.08774,2023. 1,9
OpenAI. Gpt-4v(ision)systemcard,2023. URLhttps://openai.com/research/gpt-4v-system-card. 1,9
Openai. Chatmldocuments,2024. URLhttps://github.com/openai/openai-python/blob/main/chatml.
md. 6
OpenAI. Hellogpt-4o,2024. URLhttps://openai.com/index/hello-gpt-4o. 9
AdamPaszke,SamGross,FranciscoMassa,AdamLerer,JamesBradbury,GregoryChanan,TrevorKilleen,
ZemingLin,NataliaGimelshein,LucaAntiga,AlbanDesmaison,AndreasKöpf,EdwardZ.Yang,Zachary
DeVito, MartinRaison, AlykhanTejani, SasankChilamkurthy, BenoitSteiner, LuFang, JunjieBai, and
SoumithChintala. Pytorch: Animperativestyle,high-performancedeeplearninglibrary. InNeurIPS,2019.
8
VioricaPatraucean,LucasSmaira,AnkushGupta,AdriaRecasens,LarisaMarkeeva,DylanBanarse,Skanda
Koppula,MateuszMalinowski,YiYang,CarlDoersch,etal. Perceptiontest: Adiagnosticbenchmarkfor
multimodalvideomodels. InNeurIPS,2024. 10,11,16
YuankaiQi,QiWu,PeterAnderson,XinWang,WilliamYangWang,ChunhuaShen,andAntonvanden
Hengel. Reverie: Remoteembodiedvisualreferringexpressioninrealindoorenvironments. InCVPR,
2020. 10,13
AlibabaGroupQwenTeam. Qwen-agentframework,2024. URLhttps://github.com/QwenLM/Qwen-Agent.
7,12
AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,SandhiniAgarwal,GirishSastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural
languagesupervision. InICML,2021. 2
SamyamRajbhandari,JeffRasley,OlatunjiRuwase,andYuxiongHe. Zero: memoryoptimizationstoward
trainingtrillionparametermodels. InSC,2020. 8
ChristopherRawles,SarahClinckemaillie,YifanChang,JonathanWaltz,GabrielleLau,MarybethFair,Alice
Li,WilliamBishop,WeiLi,FolawiyoCampbell-Ajala,etal. Androidworld: Adynamicbenchmarking
environmentforautonomousagents. arXiv:2405.14573,2024a. 13
ChristopherRawles,AliceLi,DanielRodriguez,OrianaRiva,andTimothyLillicrap. Androidinthewild: A
large-scaledatasetforandroiddevicecontrol. InNeurIPS,2024b. 13
JayShah,GaneshBikshandi,YingZhang,VijayThakkar,PradeepRamani,andTriDao. Flashattention-3:
Fastandaccurateattentionwithasynchronyandlow-precision. arXiv:2407.08608,2024. 8
MohammadShoeybi,MostofaPatwary,RaulPuri,PatrickLeGresley,JaredCasper,andBryanCatanzaro.
Megatron-lm:Trainingmulti-billionparameterlanguagemodelsusingmodelparallelism.arXiv:1909.08053,
2019. 8
21Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke
Zettlemoyer,andDieterFox. Alfred: Abenchmarkforinterpretinggroundedinstructionsforeveryday
tasks. InCVPR,2020a. 10,13
MohitShridhar,XingdiYuan,Marc-AlexandreCôté,YonatanBisk,AdamTrischler,andMatthewHausknecht.
Alfworld: Aligningtextandembodiedenvironmentsforinteractivelearning. arXiv:2010.03768,2020b. 13
GunnarASigurdsson,JesseThomason,GauravSSukhatme,andRobinsonPiramuthu. Rrex-bot: Remote
referringexpressionswithabagoftricks. InIROS,2023. 10,13
AmanpreetSingh,VivekNatarajan,MeetShah,YuJiang,XinleiChen,DhruvBatra,DeviParikh,andMarcus
Rohrbach. Towardsvqamodelsthatcanread. InCVPR,2019. 9,11,16
VenkatKrishnaSrinivasan,ZhenDong,BanghuaZhu,BrianYu,DamonMosk-Aoyama,KurtKeutzer,Jiantao
Jiao,andJianZhang. Nexusraven: acommercially-permissivelanguagemodelforfunctioncalling. In
NeurIPSWorkshop,2023. 12
JianlinSu. Transformerupgradepath: 4.rotarypositionencodingfortwo-dimensionalpositions,2021. URL
https://www.spaces.ac.cn/archives/8397. 4
JianlinSu. Transformerupgradepath: 17.insightsintomultimodalpositionalencoding,2024. URLhttps:
//spaces.ac.cn/archives/10040. 5
Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced
transformerwithrotarypositionembedding. InNeurocomputing,2024. 4
JingqunTang,QiLiu,YongjieYe,JinghuiLu,ShuWei,ChunhuiLin,WanqingLi,MohamadFitriFaizBin
Mahmood,HaoFeng,ZhenZhao,YanjieWang,YuliangLiu,HaoLiu,XiangBai,andCanHuang. Mtvqa:
Benchmarkingmultilingualtext-centricvisualquestionanswering. arXiv:2405.11985,2024. 9,11,16
GeminiTeam,RohanAnil,SebastianBorgeaud,YonghuiWu,Jean-BaptisteAlayrac,JiahuiYu,RaduSoricut,
JohanSchalkwyk,AndrewMDai,AnjaHauth,etal. Gemini: Afamilyofhighlycapablemultimodal
models. arXiv:2312.11805,2023. 1,9
HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,TimothéeLacroix,
BaptisteRozière,NamanGoyal,EricHambro,FaisalAzhar,etal. Llama: Openandefficientfoundation
languagemodels. arXiv:2302.13971,2023a. 1
HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,NikolayBash-
lykov,SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,etal. Llama2: Openfoundationandfine-tuned
chatmodels. arXiv:2307.09288,2023b. 1
KeWang,JuntingPan,WeikangShi,ZimuLu,MingjieZhan,andHongshengLi. Measuringmultimodal
mathematicalreasoningwithmath-visiondataset. arXiv:2402.14804,2024. 9,11,15
PengWang,AnYang,RuiMen,JunyangLin,ShuaiBai,ZhikangLi,JianxinMa,ChangZhou,JingrenZhou,
and Hongxia Yang. Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-
sequencelearningframework. InICML,2022. 12
Peng Wang, Shijie Wang, Junyang Lin, Shuai Bai, Xiaohuan Zhou, Jingren Zhou, Xinggang Wang, and
Chang Zhou. One-peace: Exploring one general representation model toward unlimited modalities.
arXiv:2305.11172,2023a. 12
WeihanWang,QingsongLv,WenmengYu,WenyiHong,JiQi,YanWang,JunhuiJi,ZhuoyiYang,LeiZhao,
XixuanSong,etal. Cogvlm: Visualexpertforpretrainedlanguagemodels. arXiv:2311.03079,2023b. 1,2,12
X.AI. Grok-1.5visionpreview. https://x.ai/blog/grok-1.5v,2024a. 9,15
X.AI. Grok-2betarelease. https://x.ai/blog/grok-2,2024b. 9
22B. Yan, Yi Jiang, Jiannan Wu, D. Wang, Ping Luo, Zehuan Yuan, and Huchuan Lu. Universal instance
perceptionasobjectdiscoveryandretrieval. InCVPR,2023. 12
FanjiaYan,HuanzhiMao,CharlieCheng-JieJi,TianjunZhang,ShishirG.Patil,IonStoica,andJosephE.
Gonzalez. Berkeleyfunctioncallingleaderboard,2024. URLhttps://gorilla.cs.berkeley.edu/blogs/
8_berkeley_function_calling_leaderboard.html. 12
AnYang,BaosongYang,BinyuanHui,BoZheng,BowenYu,ChangZhou,ChengpengLi,ChengyuanLi,
DayihengLiu,FeiHuang,etal. Qwen2technicalreport. arXiv:2407.10671,2024. 4,5,12,17
ZhengyuanYang,LinjieLi,KevinLin,JianfengWang,Chung-ChingLin,ZichengLiu,andLijuanWang. The
dawnoflmms: Preliminaryexplorationswithgpt-4v(ision). arXiv:2309.17421,2023. 29,43
YuanYao,TianyuYu,AoZhang,ChongyiWang,JunboCui,HongjiZhu,TianchiCai,HaoyuLi,WeilinZhao,
ZhihuiHe,etal. Minicpm-v: Agpt-4vlevelmllmonyourphone. arXiv:2408.01800,2024. 9
QinghaoYe,HaiyangXu,GuohaiXu,JiaboYe,MingYan,YiyangZhou,JunyangWang,AnwenHu,Pengcheng
Shi,YayaShi,etal. mplug-owl: Modularizationempowerslargelanguagemodelswithmultimodality.
arXiv:2304.14178,2023a. 2
Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Haowei Liu, Qi Qian, Ji Zhang, Fei Huang, and Jingren
Zhou. mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration.
arXiv:2311.04257,2023b. 2
KainingYing,FanqingMeng,JinWang,ZhiqianLi,HanLin,YueYang,HaoZhang,WenboZhang,YuqiLin,
ShuoLiu,JiayiLei,QuanfengLu,RunjianChen,PengXu,RenruiZhang,HaozheZhang,PengGao,Yali
Wang,YuQiao,PingLuo,KaipengZhang,andWenqiShao. Mmt-bench: Acomprehensivemultimodal
benchmarkforevaluatinglargevision-languagemodelstowardsmultitaskagi. arXiv:2404.16006,2024. 9,
15
WeihaoYu,ZhengyuanYang,LinjieLi,JianfengWang,KevinLin,ZichengLiu,XinchaoWang,andLijuan
Wang. Mm-vet: Evaluatinglargemultimodalmodelsforintegratedcapabilities. InICML,2024. 9,15
XiangYue,YuanshengNi,KaiZhang,TianyuZheng,RuoqiLiu,GeZhang,SamuelStevens,DongfuJiang,
WeimingRen,YuxuanSun,etal. Mmmu: Amassivemulti-disciplinemultimodalunderstandingand
reasoningbenchmarkforexpertagi. arXiv:2311.16502,2023. 9,15
YuexiangZhai,HaoBai,ZipengLin,JiayiPan,ShengbangTong,YifeiZhou,AlaneSuhr,SainingXie,Yann
LeCun,YiMa,etal.Fine-tuninglargevision-languagemodelsasdecision-makingagentsviareinforcement
learning. arXiv:2405.10292,2024. 10,13
Zhuosheng Zhan and Aston Zhang. You only look at screens: Multimodal chain-of-action agents.
arXiv:2309.11436,2023. 10,13
BiaoZhangandRicoSennrich. Rootmeansquarelayernormalization. InNeurIPS,2019. 8
Haotian Zhang, Haoxuan You, Philipp Dufter, Bowen Zhang, Chen Chen, Hong-You Chen, Tsu-Jui Fu,
William Yang Wang, Shih-Fu Chang, Zhe Gan, and Yinfei Yang. Ferret-v2: An improved baseline for
referringandgroundingwithlargelanguagemodels. arXiv:2404.07973,2024a. 12
Jiwen Zhang, Jihao Wu, Yihua Teng, Minghui Liao, Nuo Xu, Xiao Xiao, Zhongyu Wei, and Duyu Tang.
Androidinthezoo: Chain-of-action-thoughtforguiagents. arXiv:2403.02713,2024b. 10,13
PanZhang,XiaoyiDongBinWang,YuhangCao,ChaoXu,LinkeOuyang,ZhiyuanZhao,ShuangruiDing,
SongyangZhang,HaodongDuan,HangYan,etal. Internlm-xcomposer: Avision-languagelargemodel
foradvancedtext-imagecomprehensionandcomposition. arXiv:2309.15112,2023. 1
TianyuZhang, SuyuchenWang, LuLi, GeZhang, PerouzTaslakian, SaiRajeswar, JieFu, BangLiu, and
YoshuaBengio. Vcr: Visualcaptionrestoration. arXiv:2406.06462,2024c. 9
DeyaoZhu,JunChen,XiaoqianShen,XiangLi,andMohamedElhoseiny. Minigpt-4: Enhancingvision-
languageunderstandingwithadvancedlargelanguagemodels. arXiv:2304.10592,2023. 1,2
23A ModelCapabilitiesandQualitativeExamples
Inthissection,wepresentsomepracticalexamplesofourQwen2-VL.
A.1 GeneralChatandOCR
TheQwen2-VLmodelsarenowmoreadeptataccuratelydescribingandidentifyingcomplexinformation
withinimages,aswellasprovidingdetailedbackgroundandansweringrelatedquestions. Besides,thetext
processingcapabilitiesoftheQwen2-VLmodelshaveseensignificantimprovements,particularlyconcerning
therecognitionofChineseandEnglishtextwithinimages.
Multi-objectRecognition
Outputthecolorandnumberofeachbox.
ModelResponse
Theimageshowsastackofcolorfulblockswithnumbersonthem. Hereisthecolorandnumberof
eachblockfromtoptobottom:
-Toprow(singleblock): Bluewiththenumber9-Secondrow(twoblocks): Lightbluewiththe
number7,andgreenwiththenumber8-Thirdrow(threeblocks): Purplewiththenumber4,pink
withthenumber5, andlightgreenwiththenumber6-Bottomrow(fourblocks): Redwiththe
number0,orangewiththenumber1,yellowwiththenumber2,andlightgreenwiththenumber3
Figure7: Whenpresentedwithanimageofcubesofdifferentcolors,themodelsidentifytheirlayoutandthe
colorofeachcube.
24Plantidentification
Whatkindoffloweristhis?
ModelResponse
Theflowersinthepictureareforget-me-nots(Myosotis). Thesesmall,delicateflowersareknownfor
theirvibrantcolors,typicallyblue,pink,orwhite,andareoftenusedinbouquetsandgardensfor
theircharmingappearance.
Figure8: Themodeldisplaysanadeptnessinrecognizingflowersinphotographs.
25DocumentParsingwithDenseFormulas
ConvertthecontentintheimagetoMarkdown
ModelResponse
1.Ifi>0,letd′′:=(i−1,d−i−l+1,l).LetVdandVd′′ber+1-dimensionalsubspacesofH0(Ld)andH0(Ld′′),respectively,suchthatφd,d′′(Vd)=V dX ′′1,0 .Then
thedistributivityholdsinVd′′ifandonlyif
dim(VdX2,0 +VdX3,0
)−dim(V
dX ′′2,0
+V
dX ′′3,0
)=r+1−dim(V
dX ′′1,0
+V
dX ′′2,0
+V
dX ′′3,0
)
2.Ifi+l≤d−1,letd′′′:=(i,d−i−l−1,l+1).LetVdandVd′′′ber+1-dimensionalsubspacesofH0(Ld)andH0(Ld′′′),respectively,suchthatφd′′′,d(Vd′′′)=VdX3,0
.
ThenthedistributivityholdsinVdifandonlyif
dim(V dX ′′1 ′,0 +V dX ′′2 ′,0 )−dim(VdX1,0 +VdX2,0 )=r+1−dim(VdX1,0 +VdX2,0 +VdX3,0 ),
wherethemapsφd,d′′andφd′′′,dinProposition3.14arethemapslinkingthecorrespondingsheaves.AnotherimportantresultisProposition3.16,whichestablishesaninequalityforanyexact
limitlinearseries.Specifically,ourProposition3.16says:
Let{(Ld,Vd)}dbeanexactlimitlinearseriesofdegreedanddimensionr.Then
 
(cid:88)
d
dim  VdX1,0 +VdXVd 2,0 +VdX3,0 ≥r+1.
AsaconsequenceofProposition3.14andProposition3.16,inCorollary3.17,wegetthefollowingcharacterizationofexactlimitlinearseriessatisfyingthedistributivityateachmultidegree:
Let{(Ld,Vd)}dbeanexactlimitlinearseriesofdegreedanddimensionr.Then
 
(cid:88)
d
dim  VdX1,0 +VdXVd 2,0 +VdX3,0 =r+1
ifandonlyifthedistributivityholdsinVdforanyd.
InSection3,wealsostudysimplelimitlinearseries.InProposition3.18,wecharacterizethemultidegreeswhichdetermineallsimplebases.InProposition3.20,followingtheconstructioninMuñoz
[5],Proposition4.4,wepresentacertainstructureoflinearlyindependentsectionsforanyexactlimitlinearseriessatisfyingthedistributivityproperty.Inpropositions3.21and3.22,weobtainsome
propertiesofthatstructure.Thesepropertiesimplytheexistenceofasimplebasis(wegiveaproofofthatinProposition3.23).
WeconcludeSection3withacharacterizationofsimplelimitlinearseriesamongtheexactlimitlinearseries.Specifically,ourCorollary3.24says:
Let {(Ld,Vd)}d be an exact limit linear series of degree d and dimension r. Then the following statements are equivalent: 1. {(Ld,Vd)}d is simple. 2.
 
(cid:80) ddim  VdX1,0 +VdXVd 2,0 +VdX3,0 =r+1.3.ThedistributivityholdsinVdforanyd.
Inasubsequentwork,givenarefinedlimitlinearseries,wewillconstructallsimpleextensionsbycombiningourProposition3.14andthemethodofMuñoz[6]fortheconstructionofallexactextensions.
Figure9: Literarywritinginmultiplelanguagesbasedonvisualstimuli.
26MultilingualTextRecognition
转录图中内容，并识别图中每段内容的语言。
(Translation:Transcribethepictureandidentifythelanguageofeachparagraphinit.)
ModelResponse
1.汉语，也称为“华语”。是中国使用人数最多的语言，也是世界上作为第一语言使用人数最多的语言。是中华优秀传统文化的重要载体。繁体
字，又“繁中文”，“化字”/“字”相。一般是指字化被化字所代替的字。-语言：中文
2.日本は地方ごとに多な方言があり、とりわけ琉球で方言差が著しい。-语言：日语
3.한국어(韓國語),조선말(朝鮮말)는대한민국과조선민주주의인민공화국의공용어이다.둘은표기나문법에서는차이가없지만동사어미나표현
에서차이가있다. -语言：韩语
4.Lefrançaisestunelangueindo-européennedelafamilledeslanguesromanesdontleslocuteurssontappelés«francophones».-语言：法语
5.Elespañolocastellanoesunalenguaromanceprocedentedellatínhablado,pertenecientealafamiliadelenguasindoeuropeas.-语言：西班
牙语
6.Alínguaportuguesa,tambémdesignadaportuguês,éumalínguaindo-europeiaromânicaflexivaocidentaloriginadanogalego-português
faladonoReinodaGalizaenonortedePortugal.-语言：葡萄牙语
7. IsceanndenateangachaCeilteachaíanGhaeilge(nóGaeilgenahÉireannmarathugtaruirthicorruair),agusceanndenatrícinnde
theangachaCeilteachaaradtugtarnateangachaGaelacha(Gaeilge,GaeilgeMhanannagusGaeilgenahAlban)goháirithe.-语言：爱尔兰语
8.EnglishisaWestGermaniclanguageintheIndo-Europeanlanguagefamily,whosespeakers,calledAnglophones,originatedinearlymedieval
EnglandontheislandofGreatBritain.-语言：英语
9.DiedeutscheSprachebzw.DeutschisteinewestgermanischeSprache,dieweltweitetwa90bis105MillionenMenschenalsMuttersprache
undweiterenrund80MillionenalsZweit-oderFremdsprachedient.-语言：德语
10.Językpolski,polszczyzna—językzgrupyzachodniosłowiańskiej(doktórejnależąrównieżczeski,kaszubski,słowacki,językiłużyckieczy
wymarłyjęzykdrzewiański),stanowiącejczęśćrodzinyindoeuropejskiej.-语言：波兰语
11. Ηελληνικήγλώσσαανήκειστηνινδοευρωπαϊκήοικογένειακαιαποτελείτομοναδικόμέλοςτουελληνικούκλάδου. -语言：
希腊语
12.TiếngViệthayViệtngữlàmộtngônngữthuộcngữhệNamÁ,đượccôngnhậnlàngônngữchínhthứctạiViệtNam. -语言：越南语
13.МонголхэлньМонголулсыналбанёсныхэлюм. -语言：蒙古语
-语言：俄语
16.KiswahilinilughayaKibantuyenyemisamiatimingiyaKiarabu(35%),lakinisasayaKiingerezapia(10%),inayozungumzwakatikaeneo
kubwalaAfrikayaMashariki.-语言：斯瓦希里语
Figure10: Themodeldisplaysanadeptnessinrecognizingmultilingualtextsinimages.
27TextInformationRecognition&Processing
TranslateallthetextinthepictureintoEnglishandstandardizetheformat
ModelResponse
Thetextintheimagereads:
LearningPrompt
Guo Moruo’s collection of poems, "The Goddess", is a representative work of modern Chinese poetry. It
expresses new content and forms, embodying the spirit of the May Fourth Movement. "Standing on the
EdgeoftheEarth"isarepresentativepoemfrom"TheGoddess". Thepoetimaginesstandingontheedge
oftheearthtoobserveitfromallangles, shoutingandsinging, imaginingthesurgingclouds, magnificent
Arcticice,andviolentPacificOcean,praisingthepowertodestroytheoldworldandcreateanewlife. This
poem embodies the free, and bold spirit of the May Fourth Movement. When reading, pay attention to
connectingwiththespecificcontextoftheMayFourthMovementtounderstandthecontentandformofthispoem.
WenYiduo’s"RedCandle"usestheclassicalimageof"candle"togiveitanewmeaning,praisingtheredcandle
foritsdedicationtolightinguptheworld.Payattentiontohowthepoetusesthedialoguebetween"redcandle"
and"redcandle"toexpresstheconfusionandhopeofyouth,aswellasthefirmpursuitofideals. Theuseof
fantasyandemotionalrhetoric,theuseofrhetoricaldevicessuchasalliterationandassonance,andtheformation
ofrhythmicbeautythroughthearrangementoflongandshortsentencesinthepoemarealsoimportantaspects
toappreciate.
ChangYao’s"SnowCappedPeakontheSideoftheSun"isrichincontent.Whenreading,payattentiontothe
developmentofimaginationandtastetheheavyandmajesticatmospherecreatedbytheimagesof"snow-capped
peak","sun",and"spider".Feelthepowerfulcontainedinit.
Shelley’s"ToaSkylark"hasprofoundimplicationsandnovelimagery.Payattentiontothepoet’semotionsand
understandthesymbolicmeaningoftheskylark.Graspthecharacteristicsoftheinterlockingbetweensections
oftheworkandappreciatethepoet’suseofboldandflexibleimagery.
Whenlearningthisgroupofpoems,relymoreonreadingaloudtoclarifytheemotionalcontextofthepoem
andgrasptheconnotationoftheimage. Feeltheemotionsexpressedbythepoetandunderstandtheuseof
symbolism.
Figure11: RecognitionofdenseChinesetext. Themodeliscapableofdirectlyconvertinglengthy,closely
packedChinesecharactersintostandardEnglishwithaccuracyandfluency.
28MultilingualTextRecognition
Whatisthetext?
ModelResponse
ThetextonthewallisinPortugueseandreads: “nãoénormaltermedodeandarsozinhanarua!!!”
Thistranslatesto: “Itisnotnormaltobeafraidtowalkaloneonthestreet!!!”
Figure12: Themodeldisplaysanadeptnessinrecognizingmultilingualtextsinimages. Imagesource: (Yang
etal.,2023)
29MultilingualTextRecognition
识别图中所有文字，无需添加标点。
(Translation: Recognizeallthetextintheimagewithoutaddingpunctuation.)
ModelResponse
尊敬的老师
展信舒颜
气温骤降
树叶无声无息的掉落
教诲如春风
师恩深似海
愿温馨、喜悦与幸福
如同吐露芬芳的花朵
永远洋溢在您幸福的时光里
Figure13: Themodeldisplaysanadeptnessinrecognizingmultilingualtextsinimages.
30A.2 InformationextractionandVisualReasoning
AnotableadvancementintheQwen2-VLmodelsistheirenhancedvisualreasoningcapability. Thisadvance-
mentallowsthemodelstointerpretandcomprehendcomplexrepresentationssuchasflowcharts,diagrams,
andothersymbolicsystems.
MathematicalProblemSolving
FindthelengthofAC intheisoscelestriangleABC.
ModelResponse
SincetriangleABC isisosceles,weknowthatAC =BC. Therefore,wecansettheexpressionsfor
AC andBC equaltoeachotherandsolveforx
4x+1=5x−0.5
Subtracting4xfrombothsides,weget:
1=x−0.5
Adding0.5tobothsides,weget:
1.5=x
Nowthatweknowthevalueofx,wecansubstituteitbackintotheexpressionforAC tofindits
length:
AC =4x+1
AC =4(1.5)+1
AC =6+1
AC =7
Therefore,thelengthofAC is7.
Figure14: Themodeldisplaysanadeptnessinsolvingmathematicalproblems. Imagesource: Luetal.(2021)
31MathematicalProblemSolving
Readtheimageandsolveitstepbystep.
ModelResponse
Theimageshowstwogeometricfigures,arectangularprismandacube,withtheirdimensionslabeled. The
problemaskstocalculatethesurfaceareaandvolumeofthesefigures.
1.Fortherectangularprism:
Length(l)=4cm
Width(w)=2cm
Height(h)=3cm
SurfaceArea(SA)ofarectangularprism=2lw+2lh+2wh
Volume(V)ofarectangularprism=lwh
Substitutingthegivenvalues:
SA=2(4cm)(2cm)+2(4cm)(3cm)+2(2cm)(3cm)
SA=16cm2+24cm2+12cm2SA=52cm2
V=(4cm)(2cm)(3cm)V=24cm3
2.Forthecube:
Sidelength(a)=3dm
SurfaceArea(SA)ofacube=6a2Volume(V)ofacube=a3
Substitutingthegivenvalue:
SA=6(3dm)2SA=6(9dm2)SA=54dm2
V=(3dm)3V=27dm3
So,thesurfaceareaandvolumeforeachfigureareasfollows:
Fortherectangularprism:
SurfaceArea=52cm2
Volume=24cm3
Forthecube:
SurfaceArea=54dm2
Volume=27dm3
Figure15: Solvingamathproblem. Themodelcalculatesthesurfaceareaandvolumeofthesefigureswith
step-by-stepexplanation
32AlgorithmicProblemSolving
Solvethealgorithmicprobleminthewebpage.
33ModelResponse
Tosolvethe“SnakeinMatrix”problem,weneedtoimplementafunctionthattakesthesizeofthematrixn
andalistofcommands,andreturnsthefinalpositionofthesnakeafterexecutingallthecommands.Here’sa
step-by-stepapproachtosolvethisproblem:
1. InitializeVariables:
• Initializethecurrentpositionofthesnakeas0.
• InitializethedirectionofthesnakeasRIGHT(initially,thesnakemovestotheright).
2. DefineMovementFunctions:
• Createafunctiontomovethesnakebasedonthecurrentdirectionandthesizeofthematrixn.
• Updatethepositionofthesnakeaccordingly.
3. ProcessCommands:
• Iteratethrougheachcommandinthelistofcommands.
• Foreachcommand,updatethedirectionandpositionofthesnake.
4. ReturnFinalPosition:
• Afterprocessingallcommands,returnthefinalpositionofthesnake.
Here’sthePythoncodetoimplementthesolution:
class Solution:
def finalPositionOfSnake(self, n: int, commands: List[str]) -> int:
# Initialize variables
position = 0
direction = "RIGHT"
# Define movement functions
def move_right():
nonlocal position
position += 1
def move_down():
nonlocal position
position += n
def move_left():
nonlocal position
position -= 1
def move_up():
nonlocal position
position -= n
# Process commands
for command in commands:
if command == "RIGHT":
move_right()
elif command == "DOWN":
move_down()
elif command == "LEFT":
move_left()
elif command == "UP":
move_up()
# Return final position
return position
ThiscodedefinesaclassSolutionwithamethodfinalPositionOfSnakethattakesthesizeofthematrixnand
alistofcommands,andreturnsthefinalpositionofthesnakeafterexecutingallthecommands.Themovement
functions(move_right,move_down,move_left,move_up)areusedtoupdatethepositionofthesnakebasedon
thecurrentdirection.
Figure16: Themodeldisplaysanadeptnessinsolvingalgorithmicproblems.
34WebContentRecognition
Extractallpagetitlesinthescreenshotofsearchresult.
ModelResponse
Thepagetitlesinthescreenshotofthesearchresultare:
1. 你好，Qwen2
2. QwenLM/Qwen2
3. Qwen2-aQwenCollection
4. 快速开始
5. Qwen2-7B
6. Qwen2-Math，新一代数学模型
7. Qwen
8. Qwen/Qwen2-7B-Instruct
9. [2407.10671] Qwen2 Technical Report
10. qwen2
Figure17: Themodeldisplaysanadeptnessinrecognizecontentfromthewebpage.
35OCRandMathematicalProblemSolving
PleasetellmehowmanydaysarebetweenRuka’sbirthdayandYuki’sbirthday?Thinkcarefullystepbystep.
ModelResponse
TodeterminethenumberofdaysbetweenRuka’sandYuki’sbirthdays,weneedtocalculatethedifference
betweentheirrespectivebirthdates.
1. Ruka’sbirthdayisonJune24.
2. Yuki’sbirthdayisonSeptember17.
Let’sbreakitdownstepbystep:
1. FromJune24toJune30:
• June24toJune30is6days.
2. FromJuly1toAugust31:
• Julyhas31days.
• Augusthas31days.
• TotaldaysinJulyandAugust:31+31=62days.
3. FromSeptember1toSeptember17:
• September1toSeptember17is17days.
Now,weaddallthesedaystogether:
• June24toJune30:6days
• July1toAugust31:62days
• September1toSeptember17:17days
Totaldays=6+62+17=85days
Therefore,thereare85daysbetweenRuka’sandYuki’sbirthdays.
Figure18: ThemodeldisplaysanadeptnessinOCRandmathematicalreasoning.
36OCRandFormatFollowing
WhataretheLinuxversionsandtheirreleasedatesinthepicture?ReturnresultsasaJSONlist.
ModelResponse
[
{
"version": "6.11-rc3", "release_date": "2024-08-11"
},
{
"version": "6.10.4", "release_date": "2024-08-11"
},
{
"version": "6.9.12 [EOL]", "release_date": "2024-07-27"
},
{
"version": "6.6.45", "release_date": "2024-08-11"
},
{
"version": "6.1.104", "release_date": "2024-08-11"
},
{
"version": "5.15.164", "release_date": "2024-07-27"
},
{
"version": "5.10.223", "release_date": "2024-07-27"
},
{
"version": "5.4.281", "release_date": "2024-07-27"
},
{
"version": "4.19.319", "release_date": "2024-07-27"
},
{
"version": "next-20240812", "release_date": "2024-08-12"
}
]
Figure19: ThemodeldisplaysanadeptnessinOCRandfollowingformats.
37OCRandFormatFollowing
将图中的每日的最高气温和最低气温整理成表格。(Translation: Organize the daily highest and
lowesttemperaturesfromtheimageintoatable.)
ModelResponse
以下是图中每日的最高气温和最低气温整理成的表格：
日期 最高气温 最低气温
08/12 31°C 23°C
08/13 30°C 22°C
08/14 30°C 23°C
08/15 31°C 24°C
08/16 31°C 24°C
08/17 30°C 24°C
08/18 30°C 23°C
08/19 30°C 22°C
希望这对你有帮助！
Figure20: ThemodeldisplaysanadeptnessinOCRandfollowingformats.
38OCRforLargeImages
qwen-vl-max-0809对视频内容的理解功能存在哪些限制？
(Translation: Whatlimitationsexistinqwen-vl-max-0809’sabilitytounderstandvideocontent?)
ModelResponse
qwen-vl-max-0809对视频内容的理解功能存在以下限制：视频文件大小最大为150MB，视频文件
格式支持MP4、AVI、MKV、MOV、FLV、WMV等，视频尺寸无限制，但视频文件会被调整到
约600K像素数，更大尺寸的视频文件不会有更好的理解效果。(Translation: qwen-vl-max-0809has
thefollowinglimitationsonthevideocontentunderstanding: themaximumvideofilesizeis150MB,
thevideofileformatssupportedareMP4,AVI,MKV,MOV,FLV,WMV,etc. Thereisnolimitationon
thevideosize,butthevideofilewillberesizedtoabout600Kpixels,andvideofilesoflargersizes
won’thavebettervideounderstandingresults.)
Figure21: ThemodeldisplaysanadeptnessinlargeimagesOCR.
39A.3 VideoUnderstanding
Multi-RoundVideoChat
Pleasedescribethevideoindetail.
ModelResponse
Thevideobeginswithamanspeakingtothecamera,followedbyagroupofpeoplesittinginacontrol
room. Thecamerathencutstotwomenfloatinginsideaspacestation,wheretheyareseenspeaking
tothecamera. Themenappeartobeastronauts,andtheyarewearingspacesuits. Thespacestation
isfilledwithvariousequipmentandmachinery,andthecamerapansaroundtoshowthedifferent
areasofthestation. Themencontinuetospeaktothecamera,andtheyappeartobediscussingtheir
missionandthevarioustaskstheyareperforming. Overall,thevideoprovidesafascinatingglimpse
intotheworldofspaceexplorationandthedailylivesofastronauts.
UserQuery
Whatisthecoloroftheclothesthetwoastronautswearing?
ModelResponse
Thetwoastronautsarewearingblueandblackclothes.
Figure22: Themodeldisplaysanadeptnessinrecognizingmulti-roundvideochat.
40Multi-VideoUnderstanding
视频中的人在做什么？
(Translation:Whatarethepeopleinthevideodoing?)
ModelResponse
视频中的人正在使用吸尘器清理地板。
(Translation:Themaninthevideoisusingavacuumcleanertocleanthefloor.)
UserQuery
如果要进一步清洁地面的话，接下来该怎么做？
(Translation:WhatshouldIdonextifIwanttocleanthefloorfurther?)
ModelResponse
如果要进一步清洁地面，可以使用拖把或湿布擦拭地面，以去除吸尘器无法清除的污渍和灰尘。
(Translation: Forfurthercleaningofthefloor,wipethefloorwithamopordampclothtoremovestainsanddirt
thatthevacuumcleanercannotremove.)
UserQuery
已经用拖把清洁了地面，评价一下清洁工作？
(Translation:Alreadycleanedthefloorwithamopandpleasecommentonthecleaning?)
ModelResponse
从视频中可以看出，清洁工作已经完成，地面已经变得干净整洁。
(Translation: Asyoucanseefromthevideo,thecleaninghasbeencompletedandthefloorhasbeenmadeclean
andtidy.)
UserQuery
通过你看到的，接下来还能做些什么？
(Translation:Bywhatyousee,whatelsecanyoudonext?)
ModelResponse
接下来可以将拖把清洗干净，晾干备用。
(Translation:Nextyoucancleanthemop,dryitandsetitaside.)
Figure23: Themodeldisplaysanadeptnessinrecognizingmulti-videounderstanding.
41A.4 VisualAgentCapability
TheQwen2-VLalsoexcelsinlocationandagenttasks.
LocateWhateverYouDesire
Detecttheboundingboxof’theredcar’
ModelResponse
<|object_ref_start|>theredcar<|object_ref_end|><|box_start|>(701,531),(869,675)<|box_end|>
Figure24: Ourmodelswereabletolocatespecificelementswithinimages,suchasidentifyingtheredcar
accurately.
42VisualReferringPrompting
Whatdoesthepointednewstalkabout?
ModelResponse
ThepointednewswithagreenlogoofOpenAIandatitle"LechatbotChatGPTd’OpenAIdispose
désormaisd’instructionspersonnaliséespourstockervospréférences"istalkingaboutthenewfeature
ofChatGPTthatcannowstorepersonalizedinstructionstobetterserveusers.
Figure25: Ourmodeliscapableofannotatingandposingquestionsregardingthecontentpresentinwebpage
screenshots,demonstratingitspotentialasavisualagent. Imagesource: (Yangetal.,2023)
43FunctionCalling-Basic
几点到达，到的时候目的地天气怎么样
(Translation: Whattimedoesthisflightarrive? Whatistheweatherlikeatthedestinationwhenit
arrives?)
FunctionInput
北京
FunctionResult
时间: 2024-07-3014:00,风向: 西南风,风力:0-3级微风<5.4m/s,温度: 29°C,天气: 小雨
时间: 2024-07-3015:00,风向: 西南风,风力:0-3级微风<5.4m/s,温度: 28°C,天气: 小雨
时间: 2024-07-3016:00,风向: 西南风,风力:0-3级微风<5.4m/s,温度: 28°C,天气: 小雨
时间: 2024-07-3017:00,风向: 西南风,风力:0-3级微风<5.4m/s,温度: 28°C,天气: 小雨
时间: 2024-07-3018:00,风向: 西南风,风力:0-3级微风<5.4m/s,温度: 27°C,天气: 中雨
时间: 2024-07-3019:00,风向: 南风,风力:0-3级微风<5.4m/s,温度: 27°C,天气: 中雨
时间: 2024-07-3020:00,风向: 南风,风力:0-3级微风<5.4m/s,温度: 27°C,天气: 中雨
时间: 2024-07-3021:00,风向: 南风,风力:0-3级微风<5.4m/s,温度: 26°C,天气: 小雨
时间: 2024-07-3022:00,风向: 南风,风力:0-3级微风<5.4m/s,温度: 26°C,天气: 小雨
时间: 2024-07-3023:00,风向: 南风,风力:0-3级微风<5.4m/s,温度: 26°C,天气: 小雨
（避免显示过长，省略部分信息。）
ModelResponse
根据航班信息，您乘坐的中国国航CA1723航班预计将于2024年7月30日20:30抵达北京首都机场T3航
站楼。
在您到达时，北京的天气预报显示会有中雨，并且温度约为27℃。请注意携带雨具并做好防雨措
施。祝您旅途愉快！
(Translation: Accordingtoflightinformation,yourAirChinaflightCA1723isexpectedtoarriveat
BeijingCapitalAirportT3Terminalat20:30onJuly30,2024. Whenyouarrive,theweatherforecastin
Beijingshowsmoderaterainandthetemperatureisabout27℃.Pleaseremembertobringraingear
andtakerainprotectionmeasures. Haveapleasantjourney!)
Figure26: Themodelidentifiedthedestinationandarrivaltimeintheimage,calledthe24-hourweather
queryfunction,correctlyinputthedestination,andextractedtheweatheratthearrivaltimefromthequery
results,thusenablingittoanswertheuser’squestion.
44FunctionCalling-CodeInterpreter
Implementtheflowchart,representingmodulesasfunctionswithoutimplementationdetails. Write
thecodeandvalidateitusingthecodeinterpreter.
FunctionInput
# Placeholder functions for the modules
def speech_adapter(input_audio):
# Placeholder for speech adapter implementation
pass
# To avoid excessive length,
# the remaining functions without implementation details are omitted here.
# Example usage
input_audio = "talk about the weather in San Francisco."
speech_adapter_output = speech_adapter(input_audio)
speech_encoder_output = speech_encoder(speech_adapter_output)
system_prompt_output = system_prompt()
dialogue_history_output = dialogue_history()
llama_3_output = llama_3(speech_encoder_output, system_prompt_output, \
dialogue_history_output)
text_normalization_output = text_normalization(llama_3_output)
prosody_model_output = prosody_model(text_normalization_output)
output_audio = synthesizer(prosody_model_output)
FunctionResult
Finishedexecution.
ModelResponse
Thecodehasbeenexecutedwithoutanyerrors,whichmeansthatthestructureofthecodematches
theflowchartandtherearenosyntaxissues. However,sincethefunctionsareplaceholdersanddo
notperformanyactualoperations,wecannotvalidatethecorrectnessoftheimplementation. The
codesimplyrepresentstheflowchartbydefiningeachmoduleasafunctionanddemonstratinghow
thefunctionscouldbecalledinsequencetoprocessinputaudioandproduceoutputaudio.
Figure27: Themodelidentifiedthestepsintheflowchart,draftedthesimulationcodeasrequiredwhile
omittingthedetails,andthensuccessfullyexecuteditthroughthecodeinterpreter. Imagesource:(Dubey
etal.,2024)
45FunctionCalling-CodeInterpreter
Counttheaveragevalueofeachrowinthischartanddrawabarcharttoshowit. Writethecodeand
verifyitusingthecodeinterpreter.
FunctionInput
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# Extracting the numerical values from the table
data = {
"Model": ["Llama 3 8B", "Gemma 2 9B", "Mistral 7B", "Llama 3 70B", \
"Mixtral 8x22B", "GPT-3.5 Turbo", "Llama 3 405B", "GPT-4", \
"GPT-4o", "Claude 3.5 Sonnet", "Nemotron 4 340B"],
"HumanEval": [72.6, 54.3, 40.2, 80.5, 75.6, 68.0, 89.0, 86.6, 90.2, \
92.0, 73.2],
"HumanEval+": [67.1, 48.8, 32.3, 74.4, 68.3, 62.8, 82.3, 77.4, 86.0, \
82.3, 64.0],
"MBPP": [60.8, 59.2, 42.6, 75.4, 66.2, 71.2, 78.8, 80.2, 81.4, 76.6, \
75.4],
"MBPP EvalPlus (base)": [72.8, 71.7, 49.5, 86.0, 78.6, 82.0, 88.6, \
83.6, 87.8, 90.5, 72.8]
}
# Create a DataFrame
df = pd.DataFrame(data)
# Calculate the average for each row
df["Average"] = df.mean(axis=1)
# Plotting the bar chart
plt.figure(figsize=(12, 6))
plt.bar(df["Model"], df["Average"], color="skyblue")
plt.xlabel("Model")
plt.ylabel("Average Score")
plt.title("Average Pass@1 Scores on Code Generation Benchmarks")
plt.xticks(rotation=45, ha="right")
plt.tight_layout()
plt.show()
46FunctionResult
ModelResponse
TheaveragePass@1scoresoncodegenerationbenchmarksforeachmodelhavebeencalculatedand
aredisplayedinthetableabove. Thebarchartshowstheaveragescoresforeachmodel,providinga
visualcomparisonoftheirperformanceacrossthedifferentbenchmarks.
Figure28: Themodelanalyzedthetable,andwrotethecodetocalculatethemeanofeachrowinthetable
andtoplottheresultsasabarchart,whichwassuccessfullyexecutedandvisualizedbythecodeinterpreter.
Imagesource:(Dubeyetal.,2024)
47FunctionCalling-CodeInterpreter
Implementtheformula,writethecodeandverifyitusingthecodeinterpreter.
FunctionInput
import numpy as np
def softmax(x):
"""Compute softmax values for each sets of scores in x."""
e_x = np.exp(x - np.max(x))
return e_x / e_x.sum(axis=0)
def top_k(p, k):
"""Return the indices of the top k elements in the probability vector p."""
return np.argsort(p)[-k:][::-1]
# Example input
x = np.array([1, 2, 3, 4, 5]) # Example input vector
G = lambda x: x # Simple gating network (identity function for demonstration)
E = lambda x, i: x + i # Simple expert functions (add the expert index to the input)
# Compute probabilities
p = softmax(G(x))
# Select top k experts
k = 3
top_experts = top_k(p, k)
# Compute output
y = np.sum([p[i] * E(x, i) for i in top_experts])
p, top_experts, y
FunctionResult
(array([0.01165623, 0.03168492, 0.08612854, 0.23412166, 0.63640865]), array([4, 3, 2]),
31.451165958047895)
ModelResponse
The probabilities assigned to each expert based on the input vector (x) are: (p = [0.01165623,
0.03168492,0.08612854,0.23412166,0.63640865]).
Thetop(k=3)expertsselectedbasedontheseprobabilitiesaretheexpertswithindices4,3,and2
(indescendingorderofprobability). Theoutput(y)computedasaweightedsumoftheoutputsof
thesetopexpertsisapproximately31.4512.
ThisexampledemonstratesasimpleimplementationoftheMoEmodel’soperation. Inareal-world
scenario, thegatingnetwork(G)andtheexperts(E )wouldbemorecomplex, involvingneural
i
networklayersandpotentiallynon-lineartransformations.
Figure29: Themodelunderstoodtheformula,implementedthecodeasrequired,andsuccessfullyexecuted
itinthecodeinterpretertoobtaintheresults. Imagesource: (Dubeyetal.,2024)
48VLAgent-UIOperations
SystemMessage
Youareahelpfulassistant.
#Actions
##Youhavethefollowingactions.
###Click
Click: Aquick, lightfingertippressthatcommands, selects, ornavigatesthroughaphone’suser
interface. Parameters: [{"name": "point","description": "Thespecificpointofinterestonthescreen,
markedbythecoordinate(x,y).""required": true,"schema": "string"}]
###Type
Type: Engagingwithasmartphone’sinterfacebyenteringtextforvariouspurposeslikemessaging,
searching,orcommandexecution. Parameters: [{"name": "text","description": "Thetexttobetyped
onasmartphone.","required": true,"schema": "string"}]
###Swipe
Swipe: Touch the screen and move your finger to a new location for new content. Parameters:
["name": "direction", "description": "Determine theswipe route from the cardinal directions: up,
down,left,orright.""required": true,"schema": "string","choices": ["up","down","left","right"]]
###Back
Back: GoinreversetothepriorinterfaceorexitingthecurrentcontextParameters: []
###Home
Home: Returntothedevice’shomescreenfromanyapplicationortask. Parameters: []
###Enter
Enter: Executeacommandtoconfirminputsorselections. Parameters: []
###Done
Done: Indicatethecompletionofatask. Parameters: []
49VLAgent-UIOperations
##Continuouslytakeactionuntilthetaskiscompleted.
*FUNCTION*: Theactiontotake,shouldbeoneof{Click,Type,Swipe,Back,Home,Enter,Done}
*ARGS*: Theinputoftheaction.
*RESULT*: Actionresults.
*RETURN*: Replybasedonactionresults.
User
Yourtaskis: What’sagoodrestaurantinSanDiego?
![](Screenshot_1)
Assistant
Thescreenshowstheemailsetuppage,whichisnotrelevanttofindingarestaurant. Possibleactions
aretoexittheemailsetupandnavigatetoasearchengineormapapplicationwhereIcanlookup
restaurantsinSanDiego. *FUNCTION*: Home*ARGS*: {}
*RESULT*: ![](Screenshot_2)
*RETURN*: Thescreenshowsthehomescreenwithvariousapps,includingGoogleChromeand
MapswhichcouldbeusedtosearchforarestaurantinSanDiego. Possibleactionsaretotaponthe
GoogleChromeapptoopenitandthenuseittosearchforagoodrestaurantinSanDiego.
*FUNCTION*: Click*ARGS*: {"point": "(687,792)"}
*RESULT*: ![](Screenshot_4)
*RETURN*: Thecurrentscreenshowssearchresultsforskateboards,whichisnotrelatedtothequery
aboutfindingagoodrestaurantinSanDiego. PossibleactionsaretoclickontheGooglesearchbarat
thetopofthescreenandenterthecorrectsearchterm"goodrestaurantinSanDiego"tofindrelevant
information.
*FUNCTION*: Click*ARGS*: {"point": "(596,213)"}
*RESULT*: ![](Screenshot_6)
*RETURN*: Thecurrentscreenshowssearchsuggestionsrelatedtoskateboardspeeds,whichisnot
relevanttothequeryaboutfindingagoodrestaurantinSanDiego. Possibleactionsaretoclickonthe
"X"buttontohecorrectqueryforfindingagoodrestaurantinSanDiego.
*FUNCTION*: Click*ARGS*: {"point": "(927,135)"}
*RESULT*: ![](Screenshot_8)
*RETURN*: ThescreenshowstheGooglesearchbarwithalistofprevioussearches,noneofwhich
arerelatedtofindingarestaurantinSanDiego. Possibleactionsaretotype"goodrestaurantinSan
Diego"intothesearchbarandthenpressentertoperformthesearch.
*FUNCTION*: Type*ARGS*: {"text": "What’sagoodrestaurantinSanDiego?"}
*RESULT*: ![](Screenshot_9)
*RETURN*: ThescreenshowsalistofsearchsuggestionsrelatedtofindinggoodrestaurantsinSan
Diego,indicatingthattheuserhastypedinaquerybutnotyetexecutedit. Possibleactionsareto
selectoneofthesuggestedsearchesorpressenteronthekeyboardtoexecutethecurrentsearchand
viewtheresults.
*FUNCTION*: Enter*ARGS*: {}
*RESULT*: ![](Screenshot_10)
*RETURN*: Thescreenshowsalistoftop-ratedrestaurantsinSanDiegowithdetailssuchasratings,
pricerange,andopeninghours. Possibleactionsaretostopandsetthequeryascompleted.
*FUNCTION*: Done*ARGS*: {}
Figure30: Qwen2-VLasanagentunderstandsthequerywithrespecttoUIoperation,utilizesthepre-defined
actionsinsystemmessage,andfulfillthetaskstep-by-step.
50VLAgent-CardGame
SystemMessage
Youareahelpfulassistant.
#Actions
##Youhavethefollowingactions.
###Hit
Hit: Getanewcard. Parameters: []
###Stand
Stand: Stoptogetanewcard. Parameters: []
##Continuouslytakeactionuntilthetaskiscompleted.
*FUNCTION*: Theactiontotake,shouldbeoneof{Hit,Stand}
*ARGS*: Theinputoftheaction.
*RESULT*: Actionresults.
*RETURN*: Replybasedonactionresults.
User
Youareablackjackplayer. Youareobservingthecurrentgamestateintheimage. Withthegiven
functions,yourgoalistodealerbygettingascloseto21pointsaspossiblewithoutexceedingit.
![](Screenshot_1)
Assistant
Ihave8points,andthedealerhas10points. Ishouldhit.
*FUNCTION*: Hit*ARGS*: {}
*RESULT*:
![](Screenshot_2)
*RETURN*: Ihave15points,andthedealerhas10points. Ishouldhit.
*FUNCTION*: Hit*ARGS*: {}
*RESULT*:
![](Screenshot_3)
*RETURN*: Ihave17points,andthedealerhas10points. Ishouldstand.
*FUNCTION*: Stand*ARGS*: {}
*RESULT*: DealerCard: Q,5,Q.Thetotalpointofdealeris25>21.
PlayerCard: 4,4,7,2. Thetotalpointofplayeris17.
Playerwin!
Figure31: Qwen2-VLrecognizesthesecardsandutilizesHitandStandtoplaytheblackjack.
51