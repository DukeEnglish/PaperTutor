Putting Data at the Centre of
Offline Multi-Agent Reinforcement Learning
Claude Formanek c.formanek@instadeep.com
University of Cape Town & InstaDeep, South Africa
Louise Beyers l.beyers@instadeep.com
InstaDeep, South Africa
Callum Rhys Tilbury c.tilbury@instadeep.com
InstaDeep, South Africa
Jonathan P. Shock jonathan.shock@uct.ac.za
University of Cape Town, South Africa
Arnu Pretorius a.pretorius@instadeep.com
InstaDeep, Rwanda
Abstract
Offline multi-agent reinforcement learning (MARL) is an exciting direction of research that
uses static datasets to find optimal control policies for multi-agent systems. Though the
fieldisbydefinitiondata-driven,effortshavethusfarneglecteddataintheirdrivetoachieve
state-of-the-art results. We first substantiate this claim by surveying the literature, showing
how the majority of works generate their own datasets without consistent methodology and
provide sparse information about the characteristics of these datasets. We then show why
neglecting the nature of the data is problematic, through salient examples of how tightly
algorithmic performance is coupled to the dataset used, necessitating a common foundation
for experiments in the field. In response, we take a big step towards improving data usage
and data awareness in offline MARL, with three key contributions: (1) a clear guideline for
generating novel datasets; (2) a standardisation of over 80 existing datasets, hosted in a
publicly available repository, using a consistent storage format and easy-to-use API; and (3)
a suite of analysis tools that allow us to understand these datasets better, aiding further
development. These contributions are all publicly available on our website.1
Keywords: offline multi-agent reinforcement learning, offline reinforcement learning,
multi-agent systems, reinforcement learning, datasets
1 Introduction
Many complex real-world problems can naturally be formulated as multi-agent systems—e.g.
managing traffic (Zhang et al., 2019), controlling fleets of ride-sharing vehicles (Sykora et al.,
2020) or a network of trains (Mohanty et al., 2020), optimising electricity grid usage (Khattar
and Jin, 2022), and improving dynamic packet routing in satellite communication (Lozano-
Cuadra et al., 2024). Improving on solutions to such problems is an important endeavour,
because of the potentially immense societal benefits that they offer. Multi-Agent Reinforce-
1. https://instadeepai.github.io/og-marl/
©2024Formaneketal.
4202
peS
81
]GL.sc[
1v10021.9042:viXrament Learning (MARL) is a promising avenue to finding solutions to such problems, but the
field faces a host of hurdles which must first be overcome. One key difficulty is the access to
accurate and efficient simulators, for online experience generation and exploration. To learn
robust policies, extensive interactions with an environment are usually required (Yu, 2018),
which makes simulator efficiency of paramount importance. Yet, for real-world applicability,
the fidelity between the simulator and reality must also be maintained. Unfortunately, this
balance of achieving high throughput in an online simulator while maintaining realistic
dynamics is difficult, and practitioners must often rely on more basic environments with
simplifying assumptions. The situation is particularly challenging when there are many
agents interacting in complex ways, as is the case in MARL.
Whattypicallydoesexistinsuchsystemsasthosedescribedaboveistheabilitytocapture
large amounts of useful data. Across a range of complex control scenarios, even in situations
where many agents are acting and the physical dynamics are not well understood (i.e. where
designing a bespoke simulator would be very challenging), it may be straightforward to record
data during operation. This opportunity is what offline RL leverages, by bridging the gap
between RL and supervised learning. In the offline domain, the aim is to develop algorithms
that use large, existing datasets of sequential decision-making transitions (whether recorded
from the real-world, or created in simulation, or a mixture thereof) to learn optimal control
policies, which can later be deployed online (Levine et al., 2020). The offline paradigm
promises to help unlock the full potential of RL when applied to the real-world, where success
has thus far been limited (Dulac-Arnold et al., 2021; Rafael Figueiredo Prudencio, 2024). In
the multi-agent setting, algorithms are designed to learn a joint policy from a static dataset
of previously collected multi-agent transitions, generated by a set of interacting behaviour
policies.
Single-agent offline RL has enjoyed relatively widespread research attention and suc-
cess (Prudencio et al., 2023). Core to such developments, the community has benefited
greatly from standardised and publicly available datasets—as found in libraries such as
D4RL (Fu et al., 2020) and RL Unplugged (Gulcehre et al., 2020). Yet, in the multi-agent
case, such offerings are limited both in number and in quality. In fact, we argue in this paper
that work in offline MARL has disproportionately focused on algorithmic innovation, and
neglected the role of data almost entirely. It is common practice for authors to generate
their own datasets for their experiments, almost as an after-thought, while little effort has
been made to understand how the quality and content of these multi-agent datasets affect
training dynamics and final performance. Such insights have been immensely valuable in the
single-agent setting (Schweighofer et al., 2022), and it is known that multi-agent systems face
additional complexities (Tilbury et al., 2024), where similar insights would be particularly
useful.
Importantly, it should be clear that the end-goal for offline MARL is for systems to be
deployed in the real world: real datasets, yielding real policies, useful in real applications.
Yet, the path there is long and winding. To make progress, we focus in this paper on a
simplified context: cooperative scenarios (where existing online MARL research is most
mature, and many real-world examples exist), with data recorded from simulators. Solving
these simpler problems does not magically solve the more complex ones, but we start here
as a necessary first step towards deploying MARL in the wild. Notice that this journey
parallels the one that computer vision has undertaken, as an example of a more established
2field of machine learning—before starting to tackle the enormously challenging task of, say,
end-to-end learning from raw pixels for a self-driving car in the real-world (e.g. Bojarski
et al. (2016)), it was important to first tackle simpler problems, like handwritten digit
recognition (LeCun et al., 1989). Focusing on fundamental datasets like MNIST (LeCun
et al., 2010) has been integral to the progress in computer vision. We make similar efforts
for offline MARL here.
Our paper is structured in the following way. We start surveying the current state of
the field in Section 2, by studying how authors have, until now, been treating data in their
research—with the evidence showing a general lack of care in the way data is considered. We
build on this finding in Section 3 to show why this carelessness is problematic. Through four
clear examples, we show how the specifics of the data has a significant impact in the learned
performance of algorithms, something which has previously been overlooked. We respond in
Section 4 with three contributions: firstly, a clear guideline on how data should be treated in
offline MARL going forward; secondly, a standardised set of datasets, comprising over 80
environment-scenario-quality combinations, with a well-documented and accessible API,
and an easy mechanism to extend this repository; and finally, useful tooling for researchers
to understand the nature of their datasets, as an initial effort to promote data awareness in
offline MARL.
2 The Current State of Datasets for Offline MARL
Offline MARL remains a relatively nascent field, with only a handful of papers released on
the topic to date (see Table 1), but progress is accelerating. We want to understand how
authors have been handling the data component of their research in the work done thus far
by trying to answer the question: what is the state of the field, with respect to data itself?
To do so, we present a comprehensive survey of work in empirical offline MARL, from leading
peer-reviewed academic venues, to assess (1) how their data were generated and (2) what
information about their datasets was provided. Though a simple assessment, we find these
two axes already particularly telling in what they reveal. Table 1 summarises our findings.
From Table 1, we firstly notice that the majority of papers assessed generated their own
datasets. Each paper also creates these datasets in different ways—using a wide variety of
underlying online algorithms to learn policies for the generated trajectories. The dataset
labels themselves also vary across papers, with no consistent naming convention. Information
about the dataset properties is also sparse. To measure a given dataset’s ‘quality’, the mean
episode return of trajectories is often reported, but even this metric is not always given.
The return distribution is mostly ignored, except for the occasional proxy of reporting the
standard deviation. Essentially, there is little information presented on the contents and
diversity of the experience in a given dataset. Yet these aspects of a dataset are crucial to
the resulting performance of offline learning. In the single-agent literature, it has been shown
that dataset properties have a marked impact on results (Schweighofer et al., 2022). In the
multi-agent context, other complex aspects of coordination (Tilbury et al., 2024; Barde et al.,
2024) make the contents and characteristics of datasets even more important to understand.
We observe here that the field of offline MARL has struggled to find common ground
to benchmark proposed algorithms. Even accepting that many authors generate their own
datasets for their papers, there has been carelessness in reporting information about such
3Table 1: A survey of the use of data in empirical offline MARL literature, focusing on
(1.) where the data came from, and (2.) which properties of the data were reported. We
notice that most of the assessed papers self-generated their data. We also see that there
is no consistent naming strategy in the dataset qualities, and that the mean and standard
deviation of the datasets are often absent.
Paper SourceofDatasets DatasetLabels Means,µ StandardDeviations
good 15<µ<20
MAICQ Self-generatedusing
medium 10<µ<15 Notgiven
(Yangetal.,2021) DOP(Wangetal.,2020)
poor 0<µ<10
MADT Self-generatedusing
replay Given Given
(Mengetal.,2021) MAPPO(Yuetal.,2022)
OfflineMARLwith Someself-generatedusingrandompoliciesor good
Knowledgedistillation expertpoliciesfromPPO(Schulmanetal.,2017), normal Given Given
(Tsengetal.,2022) andsomefromMADT(Mengetal.,2021) poor
random
OMAR Self-generatedusing medium-replay
Notgiven Notgiven
(Panetal.,2022) MATD3(Ackermannetal.,2019) medium
expert
medium
CFCQL Self-generatedusing medium-replay
Notgiven Notgiven
(Shaoetal.,2023) QMIX(Rashidetal.,2018) expert
mixed
good
OMAC RandomSubsetsfrom
medium Notgiven Notgiven
(WangandZhan,2023) MADT(Mengetal.,2021)
poor
good
OMIGA RandomSubsetsfrom
medium Given Notgiven
(Wangetal.,2023b) MADT(Mengetal.,2021)
poor
Self-generatedusing lowquality
SITFramework
QMIX(Rashidetal.,2018) mediumquality Given Notgiven
(Tianetal.,2023)
andFacMAC(Pengetal.,2021) randomquality
AlberDICE Self-generatedusing expert
Notgiven Notgiven
(Matsunagaetal.,2023) MAT(Wenetal.,2022) medium-expert
Someself-generatedusingrandompolicies random
ValueDeviation&
orusingQMIX(Rashidetal.,2018) medium
TransitionNormalisation Notgiven Notgiven
orSAC(Haarnojaetal.,2018), replay
(JiangandLu,2023)
andsomedecomposedfromD4RL(Fuetal.,2020) expert
inadequate 0<µ<10
StateAugmentationviaSelf-Supervision Self-generatedusing
moderate 10<µ<15 Notgiven
(Wangetal.,2023a) QMIX(Rashidetal.,2018)
superb 15<µ<20
good
MADiff DatasetsfromFormaneketal.(2023a)
medium Given Given
(Zhuetal.,2023) andfromPanetal.(2022)
poor
random
Self-generatedusing medium
MOMA-PPO
MAPPO(Yuetal.,2022), replay Given Notgiven
(Bardeetal.,2024)
anddecomposedfromD4RL(Fuetal.,2020) expert
expert-mix
4µ
•
7 Median IQM Mean
•
10
•
13
•
16
12 15 18 12 15 18 12 15
(a) Mean episode return for each
(b) A comparison of the performance obtained using
dataset. Datasets from the
datasets with different mean episode returns. Results
three SMAC scenarios 5m_vs_6m,
areaggregated(acrosstwoalgorithms, 10randomseeds,
3s5z_vs_3s6z and 2s3z were gen-
32 evaluation episodes and three scenarios) using boot-
erated for each mean episode re-
strap confidence intervals (Agarwal et al., 2021).
turn.
Figure 1: To demonstrate the effect that the mean episode return of a dataset has on the
final performance of an offline MARL algorithm, we generated four datasets with mean
episode returns given in Figure 1a. We then train an offline MARL algorithm for 50k training
steps on each of the datasets and compare the final performance of the algorithm across the
different datasets. We repeat the experiment across three different SMAC scenarios, two
different algorithms (IQL+CQL (Formanek et al., 2024) and MAICQ (Yang et al., 2021)) and 10
random seeds. The aggregated results are given in Figure 1b.
datasets. Ultimately, the current reality points to a general lack of consideration around the
role of data in the field, where authors are failing to adequately control for the impact that
data can have on experimental results. We will now show why a lack of such data-centrism
is problematic for the field.
3 Why Dataset Characteristics Matter
Claims of algorithmic improvement become moot if acommon basis of data is missing, since
a dataset is one of the control variables in empirical offline MARL experiments which can
impact performance significantly. We illustrate this point by giving four examples that
progressively show how algorithmic results are tightly coupled with data. These examples
are not intended to make sweeping claims about the field, but should rather serve as a series
of “proof by existence” demonstrations cautioning researchers of how peculiarities of datasets
may be influencing their experimental findings. When reflecting on this evidence, it becomes
clear that overlooking data is problematic for the field, and that ultimately, there is a serious
need for a shift in current research practices.
Dataset Mean. We begin our illustration with likely the most intuitive example: what
happens to final performance when the average return of the dataset changes? We construct
four distinct datasets with increasing means on three scenarios (5m_vs_6m, 3s5z_vs_3s6z
and 2s3z) from the SMACv1 environment (Samvelyan et al., 2019), by subsampling episodes
from OG-MARL datasets (Formanek et al., 2023a). For each dataset, we fix the standard
deviation at approximately 2.0 as calculated over 2000 episodes. We then train two offline
MARL algorithms, IQL+CQL (Formanek et al., 2024) and MAICQ (Yang et al., 2021), on the
individual datasets and report the final evaluation episode return, averaged across 10 random
5µ σ
80 •
60 10 0.5
40 •
20 10 1.0
0 •
10 2.0
40 •
10 4.0
20 •
10 6.0
0
10 (b) Dataset mean and standard deviation.
5
Median IQM Mean
0
10
5
0 12 14 16 12.0 13.5 15.0 12.0 13.5 15.0
8 (c) The aggregated results across scenarios
6
4 (5m_vs_6m, 3s5z_vs_3s6z and 2s3z), algo-
2
0 rithms (IQL+CQL and MAICQ) and random
0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0
seeds are given for each dataset. The me-
(a) Histograms of the episode returns of five
dian, interquartile mean (IQM), and mean
datasets with the same mean but different stan-
are all given with bootstrap confidence inter-
dard deviations.
vals (Agarwal et al., 2021).
Figure 2: To demonstrate the surprising effect that the standard deviation (std) can have
on the performance of an offline MARL experiment we generate 5 datasets that each had
the same mean but differing std. We then train two offline MARL algorithms, IQL+CQL and
MAICQ, on the data and report the final performance. We repeat the experiment across three
different SMAC scenarios, two different algorithms, 10 random seeds and an evaluation batch
size of 32. We then aggregate the results as per Gorsane et al. (2022).
seeds. We present the aggregated results using bootstrap confidence intervals (Agarwal et al.,
2021; Gorsane et al., 2022) in Figure 1.
Weseethattheaggregatedfinalperformanceof IQL+CQLandMAICQispositivelycorrelated
with the mean episode return of the dataset used for training. As the average quality of
the data improves, so does the offline learning from this dataset—an intuitive result. This
relationshipisoftenusedinthesingle-agentliterature,wherethefinalperformanceisreported
as a percentage of the mean return in the dataset (Agarwal et al., 2019; Gulcehre et al.,
2020), but doing so is rare in the multi-agent domain. In fact, the simple metric of the
dataset mean is sometimes omitted entirely, as shown in Table 1.
Dataset Spread. Another important property of a dataset in offline RL is the diversity
of the experience (Agarwal et al., 2019). In the single-agent setting, it is well-understood
that many offline RL algorithms benefit from more diverse datasets (Schweighofer et al.,
2022). Usually, this benefit arises because diverse experience leads to better coverage of the
state and action spaces, resulting in fewer out-of-distribution actions, which are known to
cause issues when training offline (Fujimoto et al., 2019). However, the effect of diversity
in the multi-agent setting is less well understood and should not be taken for granted. For
example, Tilbury et al. (2024) demonstrate a more complex relationship between dataset
6µ σ
•
3688 2885
•
3665 2866
80
(b) Mean and standard deviation.
60
40 5000
4000
20
3000
2000
0
0 2000 4000 6000 8000
Episode return 1000
(a) Histogram of the episode returns for two 0
datasets with similar means and standard de- 0.0 0.2 0.4 0.6 0.8 1.0
Training steps (x1e6)
viations. (c) Training curves.
Figure 3: We generate two datasets on 2-Agent Halfcheetah each with very similar episode
return means and standard deviations, but distinct data distributions. We then train
MADDPG+CQL on each dataset and report its performance over 1 million training steps. We
repeat the experiment over 10 random seeds.
diversity and final performance in the offline MARL setting than might be initially assumed.
In the literature, the standard deviation (std) of the data’s episode returns is sometimes
reported as a proxy for diversity.
We thus continue our illustration with another example: what happens when solely the
std of the returns in a dataset changes? We now construct five distinct datasets, each with
thesamemean, butwithincreasingspreadaroundthatmean, keepingthenumberofincluded
episodes constant. We create these datasets by subsampling from OG-MARL. Once again,
we look at the three SMAC scenarios 5m_vs_6m, 3s5z_vs_3s6z and 2s3z. Figure 2 shows
the respective histograms of the datasets, and the corresponding aggregated results across
two algorithms (IQL+CQL and MAICQ) and 10 random seeds.
In this situation, we begin to see a more complex relationship emerge. Rather than a
simple linear relationship between diversity and performance, optimal results are found at
intermediate levels of std. This result is less intuitive than before (where a higher mean
return simply meant higher performance), and begins to hint at the impact of multi-agent
dynamics. It is worth reiterating that very few papers report the std (or some other proxy
for diversity) of their datasets—just three of the thirteen in Table 1 had done so.
Dataset Distribution. Our illustration continues with a question that builds on the
previous two: controlling for equal mean and std, can two different dataset distributions yield
different results? We construct two more datasets subsampled from OG-MARL with this
property.2 We look at the 2halfcheetah scenario from MAMuJoCo (Peng et al., 2021). We
2. This setup is reminiscent of Anscombe’s Quartet (Anscombe, 1973), comprising four datasets that have
almost identical summary statistics, yet look markedly different when visualised.
7
tnuoC
nruter
edosipE1000 (b) Means and standard deviations of the episode
800 returns in subsampled datasets.
600 Dataset Mean Stddev #Traj #Trans
•
CFCQL 12.05 4.36 4992 140073
400 •
OG-MARL 12.05 4.36 4992 134985
200
0
17.5
15.0
1000
12.5
800
10.0 600
7.5
400
5.0
200
2.5
0
6 8 10 12 14 16 18 20 0.0
Episode return 0.0 12.5 25 37.5 50 6622..55 75 87.5 100
Training steps (x1e5)
(a) Histograms of the subsampled
(c) Training curves
5m_vs_6m datasets.
Figure 4: We use two subsampled datasets of the 5m_vs_6m scenario from SMACv1, with
almost identical distributions, but from two different sources (Formanek et al., 2023a; Shao
et al., 2023) (the Medium quality in both cases). We then train IQL+CQL on each dataset and
report its final performance. We repeat the experiment over 10 random seeds.
train MADDPG (Lowe et al., 2017) with CQL (Kumar et al., 2020) on these respective datasets,
and show the results in Figure 3.
We can see here that an offline algorithm, when trained on two datasets with nearly
identical summary statistics, can yield significantly different final performances. In essence,
these metrics of a dataset only paint a limited view of the underlying distributions, yet
these distributions may have a notable impact on the results. Here, we note that even those
authors from Table 1 who have reported the mean and std of their datasets, have nonetheless
omitted a visualisation of their datasets for further understanding. Therefore, authors might
be missing key insights on the characteristics of the data they are using in their work.
Dataset Coverage. We conclude our illustration with possibly the most illuminating
example of the subtleties of dataset charateristics and their effect on performance. We
ask: can we have a situation where the return distributions are very similar, such that the
summary statistics are similar and the histograms are closely aligned, yet yield significantly
different results for the same offline algorithm?
For this example, we look at two publicly available datasets, from OG-MARL (Formanek
etal.,2023a)andCFCQL(Shaoetal.,2023)respectively. Weconsiderthe5m_vs_6mscenario
from SMACv1 (Samvelyan et al., 2019), and take the Medium quality dataset from each
source. To further control the experiment, we subsample the original datasets to around
140k transitions, matching the distributions of episode returns in the process. The result is
not only equal mean and std but also nearly identical, visually indistinguishable histograms,
which we show in Figure 4a and detail in Table 4b. We train IQL+CQL on both datasets,
across 10 random seeds. The results are illustrated in Figure 4c.
8
tnuoC
tnuoC
nruter
edosipETable 2: The Joint State-Action Coverage (Joint-SACo) scores for two datasets on the
5m_vs_6m SMAC task, one from OG-MARL and the other from CFCQL (Shao et al., 2023).
A lower score means there is less Joint State-Action coverage in the dataset, i.e. there are
more repeated transitions in the dataset.
Dataset Quality Joint-SACo score
• CFCQL Medium 0.10
• OG-MARL Medium 0.83
We see here a curious outcome—despite the return distributions being essentially the
same, the achieved algorithm performance is significantly different. The difference cannot be
explained solely by the statistics and distribution of the episode returns. Evidently, there are
other significant differences between the datasets which escape the reach of our current lens.
We note that episode return is itself a summary of a trajectory, and is an abstraction of the
actual experience in the dataset.
To better understand what is happening here, we extend to the multi-agent setting the
approach from Schweighofer et al. (2022) to measure dataset diversity. Specifically, we extend
their State-Action Coverage (SACo) metric to a version that operates on the joint state and
action space of agents in MARL. We define this metric in the same way as SACo: the ratio
between the number of unique state-action pairs and the total number of state-action pairs
in a dataset. In the multi-agent case, the difference is that the action refers to the joint
action of all agents in the system. We use this Joint-SACo metric on the 5m_vs_6m datasets
from Figure 4a, with the results given in Table 2.
Whereas the datasets from this example have almost identical return distributions, we
see that they have very different values for state-action coverage. In fact, from Table 2
we observe that 90% of the data from Shao et al. (2023) are repetitions of previously seen
state-action pairs, compared to just 17% in the data from Formanek et al. (2023a). This
finding illuminates how complexities in multi-agent dynamics fail to be encapsulated solely
in episode return values, and why we must be particularly careful.
Takeaways. Much of the multi-agent literature uses datasets that are self-generated.
Furthermore, there is often carelessness in the amount of effort put towards reproducibility
and comparison. Datasets are often labelled with qualitative descriptors such as “Good”,
“medium” or “bad”, descriptors which ought to have bearing on some property of the dataset.
However, the most obvious property, episode return mean, is only sometimes reported, and
rarely is episode return std reported. Our four progressive examples hope to illustrate the
following key point. The mean, std, and distribution of episode returns are significantly
important when learning from a multi-agent dataset, with a notable impact on learned
performance. But they are not enough, and even when controlling for them, significant
differences may still occur. Crucially, the outcomes from these examples do not hold in
every scenario, but that is exactly the point: these peculiar dynamics can arise when not
controlling for the dataset used, and what we witness can conceivably be leveraged by authors
to erroneously present algorithmic advancements as more performant than what in reality is
truly the case.
9What is our main message, then? Ultimately, we see an urgent need for authors to pay
attention to data in offline MARL research. At the very least, if authors are generating their
own data for their experiments, they must provide easy access to this data for future work,
hosted in perpetuity, with ample documentation of the generation procedure, the contents,
and a reasonable quantitative description including the episode return distributions, the
state-action coverage, and so on. In addition to such efforts, there is evidently a growing
need for the standardisation of datasets in offline MARL. In the absence of a common
foundation, it is impossible for researchers to speak the same language, introducing doubt
into the veracity of algorithmic developments. We also see the groundwork of a fruitful new
avenue of multi-agent research—how does the characteristics of a dataset impact multi-agent
learning, with all the associated complex dynamics that inevitably arise?
4 Putting Data at the Centre of Offline MARL
Having illustrated how a lack of consideration for the impact of data in offline MARL is
problematic, we take a step towards alleviating some of the issues we have identified in the
previous section. By making three data-driven contributions to the community, we hope to
bring data closer to the centre of research in the field. Our first contribution is a set of clear
guidelinesforhowresearchersshouldapproachgeneratingdatasetsinofflineMARL.Ifthereis
animportantreasonforauthorstogeneratetheirowndatasets, thereshouldatleastbesound
principles to follow. Secondly, we significantly enhance the standardisation of data in the
field by converting over 80 datasets from prior works into a consistent format (Toledo et al.,
2023), which has an emphasis on speed, ease-of-use, clear documentation, and integration
into existing frameworks. We upload these datasets to Hugging Face for reliable access in
perpetuity.3 We recommend using existing OG-MARL (Formanek et al., 2023a) datasets for
future research and encourage any new datasets generated by the community to be added
to the repository following the standards and formats outlined there. That said, we still
convertedallthedatasetswecouldgetaccesstofrompriorworkforthesakeofcontinuityand
the possibility of comparing with those works. Finally, we present an ever-growing repository
of open-source tools that can be used to access, analyse, and edit these standardised datasets,
for future research.
4.1 Dataset guidelines
The gold standard solution is to standardise all of the existing datasets (which we attempt
through OG-MARL) and to use a shared methodology when generating novel datasets.
When generating a new dataset, there are certainly basic guiding principles that ought to be
followed to ensure good scientific practice. We outline such guidelines in the blue box below.
4.2 Standardising existing datasets
The single-agent offline RL community have benefited significantly from the widespread
adoption of common datasets such as RL Unplugged (Gulcehre et al., 2020) and D4RL (Fu
et al., 2020). The offline MARL field could similarly benefit from the adoption of a common
set of benchmark datasets.
3. https://huggingface.co/datasets/InstaDeepAI/og-marl/
10Guidelines for generating new datasets for offline MARL research
1. Is a new dataset really necessary?
• Is there an existing dataset in the field you could use instead?
• If a new dataset is required for your research, make sure you document why and how exactly
your dataset is different.
2. Have you documented all of the relevant information regarding how you generated
your data?
• Document which environment you used and how people can access the environment. Make
suretobeexplicitabouttheversionoftheenvironmentused. Thisistoensureproperversion
control for comparisons as later environment versions might be released in the future (in some
cases from authors different than the original creators).
• Document relevant high-level environment properties e.g. number of agents, action size,
observation size, sparse/dense reward and so on.
• Document how you generated the dataset. For example, which online MARL algorithm
collected the experience and how did you sub-sample the data?
3. Have you included a quantitative analysis of the composition of your data?
• Report the following summary statistics for your datasets: episode returns min, mean, max
and standard deviation, as well as the number of episodes and transitions in your dataset.
• Include plots of the episode return distribution e.g. histograms or violin plots.
• Include a measure of action-space coverage for your dataset e.g. Joint-SACo.
4. Can other researchers access your datasets? And will they still be able to in a year
from now?
• Make a download link easily accessible. Ensure the link will not expire and that downloads
are successful from different regions of the world.
• Consider adding your datasets to a community-driven datasets repository such as OG-
MARL (Formanek et al., 2023a).
• Use a dataset format that is widely adopted in the field. Or include sufficient documentation
on how to load and use your dataset.
• Include a dataset licence.
Although there are multiple possible starting points for standardisation, we recommend
OG-MARL (Formanek et al., 2023a). OG-MARL is solely focused on providing standardised
datasets to the community, rather than hosting datasets that exist only as a by-product of
algorithmic research. These datasets have already been cited in multiple works (Formanek
et al., 2023b; Zhu et al., 2023; Yuan et al., 2023; Formanek et al., 2024; Tilbury et al., 2024;
Putla et al., 2024; Ruhdorfer et al., 2024; Jing et al., 2024). The repository is ever-growing
and extendable by the community, and supports a wide range of environments.
We standardise the format of these datasets to the open-source and industry-supported
utility of Vault (Toledo et al., 2023), because of its focus on speed, clear documentation,
and easy accessibility. For future offline MARL research, we strongly recommend using
OG-MARL datasets in the Vault format. However, to enable comparisons to past works in
the literature, we have additionally converted datasets from other authors into the Vault
11Table 3: Complete list of datasets converted into the Vault format (Toledo et al., 2023).
Though we strongly recommend using OG-MARL in future research, we convert other
datasets for continuity with past research. Environment sources: MAMuJoCo (Peng et al.,
2021), SMACv1 (Samvelyan et al., 2019), SMACv2 (Ellis et al., 2022), MPE (Lowe et al.,
2017), RWARE (Christianos et al., 2020)
Source Environment Scenario Datasets
Formaneketal.(2023a) MAMuJoCo 2halfcheetah good,medium,poor
2ant good,medium,poor
4ant good,medium,poor
SMACv1 2s3z good,medium,poor
3m good,medium,poor
3s5z_vs_3s6z good,medium,poor
5m_vs_6m good,medium,poor
8m good,medium,poor
SMACv2 terran_5_vs_5 replay
zerg_5_vs_5 replay
Panetal.(2022) MAMuJoCo 2halfcheetah expert,medium-replay,medium,random
MPE simple-spread expert,medium-replay,medium,random
simple-tag expert,medium-replay,medium,random
simple-world expert,medium-replay,medium,random
Shaoetal.(2023) SMACv1 2s3z expert,medium-replay,medium,mixed
3s_vs_5z expert,medium-replay,medium,mixed
5m_vs_6m expert,medium-replay,medium,mixed
6h_vs_8z expert,medium-replay,medium,mixed
Wangetal.(2023b) SMACv1 corridor good,medium,poor
2c_vs_64zg good,medium,poor
5m_vs_6m good,medium,poor
6h_vs_8z good,medium,poor
MAMuJoCo 2ant expert,medium-expert,medium-replay,medium
3hopper expert,medium-expert,medium-replay,medium
6halfcheetah expert,medium-expert,medium-replay,medium
Matsunagaetal.(2023) RWARE tiny-2g expert
tiny-4g expert
tiny-6ag expert
small-2ag expert
small-4ag expert
small-6ag expert
Totalnumberofdatasets: 88
format. In Table 3, we provide a list of the datasets converted and made available to the
community in OG-MARL, hosted on the Hugging Face platform.
4.3 Dataset analysis tools
Our final contribution towards improving data awareness in offline MARL is a set of tools
which can be used to download, subsample, combine, and analyse datasets. These tools,
which live in OG-MARL, can be used on any dataset which conforms to the Vault API. The
tools are accompanied by a demonstrative notebook4, which explains how to use them and
4. https://github.com/instadeepai/og-marl/blob/main/examples/dataset_analysis_demo.ipynb
12Dataset utilities
Simplified loading of datasets
• Support for downloading all 88 Vault datasets from OG-MARL.
• Support for downloading a Vault from a user-specified URL.
Dataset analysis tools
• Dataset structure: describe_structure prints the pytree structure of each dataset in the
Vault, and gives the number of transitions and trajectories in each dataset.
• Episodereturns: describe_episode_returnsplotsforeachdatasetintheVaultthehistogram
and violin plot of its episode returns, and outputs a table containing the episode return mean,
standard deviation, minimum, and maximum.
• Coverageproperties: describe_coverageproducesalog-logplotofcountfrequenciesofunique
state-action pairs, as well as the Joint-SACo value for each dataset.
• Summary: descriptive_summaryplotsepisodereturnhistograms,andoutputsatablecontain-
ingepisodereturnmean,standarddeviation,minimumandmaximum,Joint-SACo,numberof
transitions and number of trajectories in each dataset.
Tools to subsample and combine Vaults
• Subsample a Vault to within one trajectory’s length of a specified number of transitions.
• Combine a list of datasets into one larger dataset.
• Subsample two datasets to have near-identical episode return distributions.
• Subsample a dataset to have a specific episode return distribution.
provides enough understanding of the Vault and OG-MARL systems to be able to work on
custom tools and workflows. Our set of utilities are outlined below.
Weprovideademonstrationofourtoolsusingthe2s3zscenariofromSMACv1(Samvelyan
et al., 2019), with the dataset from OG-MARL (Formanek et al., 2023a). We focus on dataset
analysis (where we give insights into dataset composition), as well as subsampling and
combining tools (which can be used for a variety of reasons: to make datasets smaller so
that they use less memory, to create datasets for ablations, and to combine datasets when
more training data is required).
Analysis Our analysis tools cover all requirements stipulated in the analysis section of our
datasetgenerationguidelines. Weprovidefourhigh-levelfunctionstogeneratevariousinsights
for a user-specified selection of datasets in the Vault format. Calling descriptive_summary
with a provided Vault will generate a summary such as the one illustrated by Figure 5, with
both tabular and histogram information returned. Users can further access episode return
violin plots by calling describe_episode_returns, detailed structural information about
the Vault by calling describe_structure, and state-action count information by calling
describe_coverage.
From these outputs, we can now analyse the dataset and notice interesting insights. For
example, we can see in Table 5 that the Poor dataset contains far fewer trajectories than the
Good and Medium datasets, despite containing a similar number of transitions. On average,
the episodes in the Poor dataset contain more than 100 transitions, which means that the
episodes usually roll out to their full length. We also notice that there is not much of a
13(a) Tabular values returned from the descriptive_summary function
Dataset Mean Stddev Min Max #Transitions #Trajectories Joint-SACo
Good 18.32 2.95 0.00 21.62 995829 18616 0.98
Medium 12.57 3.14 0.00 21.30 996256 18605 0.98
Poor 6.88 2.06 0.00 13.61 996418 9942 0.96
Poor Medium Good
10000
7500
5000
2500
0
0 10 20 0 10 20 0 10 20
Episode return Episode return Episode return
(b) Histograms returned from the descriptive_summary function
Figure 5: The results returned when calling descriptive_summary on the 2s3z Vault from
OG-MARL (Formanek et al., 2023a)
difference between the Joint-SACo of the datasets, though each dataset’s coverage is diverse,
with at most 4% being repeated pairs in each case. The information gained here can at a
glance help a researcher understand a dataset better.
Subsampling and combining datasets Our tools also allow researchers to stitch and
slice datasets. While it is important to keep datasets standardised, experiments may require
datasets which do not yet exist; there may be constrained memory requirements, new
experiments, or ablations requiring new datasets. Rather than expecting researchers to
create entirely new datasets for such experiments, which might introduce inconsistencies
between datasets available in the field, we give researchers the tools to flexibly subsample and
combine existing datasets according to the properties that their experiments require. Figure
6 illustrates an assortment of such examples—showing how datasets can be subsampled or
combined, and showing how desired distributions can be systematically created. We also
provide a notebook in the OG-MARL open-source repository that demonstrates how to
subsample a dataset according to a user-specified episode return histogram.5 Our hope is
that even though our subsampling tools allow researchers to manipulate data more easily,
our analysis tools make it easy enough to uncover dataset discrepancies that researchers
should be dissuaded from changing datasets without disclosing their amendments.
Our work opens up an easy-to-use pipeline for data use, generation and understanding in
offline MARL. Researchers can now easily expose the contents of datasets for flexible and
well-informed access to the material on which their algorithms train. We hope the impact
of our work is better understanding not only of datasets in offline MARL, but also of the
environments we use and how a data collecting policy interacts with these.
5. https://github.com/instadeepai/og-marl/blob/main/examples/dataset_subsampling_demo.ipynb
14
ycneuqerFMedium Good
1500
1000
500
0
0 10 20 0 10 20
Episode return Episode return
(a) Original Medium and Good datasets sub-
sampled to have similar episode return distri-
butions.
300
"Poor" distribution
250 Target distribution 150 Poor_Table_Mountain
200 100
150
50
100
50 0 3 4 5 6 Episode7 return 8 9 10 11
0 (c) Histogram of the dataset with the desired
0 50 100 150 200 250 300
Bin number target distribution, after being subsampled
(b) Poor dataset distribution versus a target from Poor.
distribution.
Poor Medium Good Poor_Medium_Good
250 300
200
200
150
100
100
50
0 0
10 20 10 20 10 20 10 20
Episode return Episode return Episode return Episode return
(d) Datasets subsampled to 20000 transitions each. (e) Combined datasets.
Figure 6: Illustrative dataset subsampling examples, using the 2s3z Vault.
5 Conclusion
In this paper, we have surveyed the literature and found that data is largely neglected in the
data-driven field of offline MARL. We have shown why paying attention to data in offline
MARL research is crucial, through simple yet illuminating examples. We contribute to the
community: a guideline for generating multi-agent datasets; a standardised repository of over
80 environment-scenario-quality combinations, with a well-documented and accessible
API; and useful tooling to aid the understanding of these datasets. In conjunction, these
efforts aim to catalyse progress by aligning the field towards scientific rigour. In doing
so, along with other standardisation efforts—e.g. standardised baselines (Formanek et al.,
2024)—we feel that the discipline is ripe with opportunity to solve hard problems. We
encourage researchers to adopt and extend our offerings, working collectively to push the
field forward. By working from a strong foundation together, significant breakthroughs can
be made.
15
ycneuqerF
ycneuqerf
edosipE
ycneuqerF
ycneuqerF
ycneuqerFBroader Impact Statement
Our contribution opens the door to an offline MARL field in which careful attention is
paid to data. However, the path from our contribution to that goal needs to be taken
collectively by the community. If our work becomes widely adopted, then we may be shaping
what datasets in offline MARL research look like - standardisation by nature places some
restriction on the object being standardised, but our guidelines and contributions take into
account that some level of flexibility is also required for progress. It is possible that, using
our tools, the field could trend towards either using pre-existing standardised datasets (which
is preferable) or again generating their own datasets for new experiments (which will be easy
to do using our tooling). It is also true that the subsampling and combining tools which
we provide can both improve accessibility and standardisation of datasets but also allow
researchers to manipulate data without disclosing their alterations.
Wemustconsidernotonlyresearchimpact, butreal-worldimpact. Ourlong-termgoalfor
impactistobuildsolidfoundationsonwhichofflineMARLcandevelop, whichmayaccelerate
the progress in the field. The potential effects of progress in offline MARL are vast, but our
work specifically assists in standardising and analysing datasets. We hope that transparency
around the nature of datasets will filter through not only in research but also into real-world
scenarios. Dataset transparency in the real world is, however, a far more complex topic since
datasets from real-world scenarios may contain sensitive information. Care must therefore
be taken in performing and presenting our suggested analyses on real-world datasets.
As with any Deep Learning research, our contribution is likely to have computational
expense implications. If datasets are standardised, fewer new datasets need to be created,
and fewer baselines need to be rerun. Both of these effects reduce computational expense.
Overall, while we have hopes for the impact of our research, we acknowledge areas in which
it may be misused. We urge the community to adopt the guidelines of our work in the hopes
that progress can be made with care.
Acknowledgments and Disclosure of Funding
This work was supported by InstaDeep Ltd.
16References
Johannes Ackermann, Volker Gabler, Takayuki Osa, and Masashi Sugiyama. Reducing
overestimation bias in multi-agent domains using double centralized critics, 2019.
Rishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi. An optimistic perspective on
offline reinforcement learning. ArXiv Preprint, 2019.
Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron C Courville, and Marc
Bellemare. Deep reinforcement learning at the edge of the statistical precipice. Advances
in Neural Information Processing Systems, 2021.
Francis J Anscombe. Graphs in statistical analysis. The american statistician, 27(1):17–21,
1973.
Paul Barde, Jakob Foerster, Derek Nowrouzezahrai, and Amy Zhang. A model-based solution
to the offline multi-agent reinforcement learning coordination problem. In Proceedings of
the 23rd International Conference on Autonomous Agents and Multiagent Systems, pages
141–150, 2024.
Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp,
Prasoon Goyal, Lawrence D Jackel, Mathew Monfort, Urs Muller, Jiakai Zhang, et al. End
to end learning for self-driving cars. arXiv preprint arXiv:1604.07316, 2016.
Filippos Christianos, Lukas Schäfer, and Stefano Albrecht. Shared experience actor-critic for
multi-agent reinforcement learning. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan,
and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages
10707–10717. Curran Associates, Inc., 2020.
Gabriel Dulac-Arnold, Nir Levine, Daniel J Mankowitz, Jerry Li, Cosmin Paduraru, Sven
Gowal, and Todd Hester. Challenges of real-world reinforcement learning: definitions,
benchmarks and analysis. Springer Machine Learning, 2021.
Benjamin Ellis, Skander Moalla, Mikayel Samvelyan, Mingfei Sun, Anuj Mahajan, Jakob N
Foerster, and Shimon Whiteson. Smacv2: An improved benchmark for cooperative
multi-agent reinforcement learning. ArXiv Preprint, 2022.
Claude Formanek, Asad Jeewa, Jonathan Shock, and Arnu Pretorius. Off-the-grid marl:
Datasets and baselines for offline multi-agent reinforcement learning. In Extended Abstract
at the International Conference on Autonomous Agents and Multiagent Systems, 2023a.
Claude Formanek, Callum Rhys Tilbury, Jonathan Phillip Shock, Kale ab Tessera, and Arnu
Pretorius. Reduce, reuse, recycle: Selective reincarnation in multi-agent reinforcement
learning. Workshop on Reincarnating Reinforcement Learning at ICLR, 2023b.
Claude Formanek, Callum Rhys Tilbury, Louise Beyers, Jonathan Shock, and Arnu Pretorius.
Dispelling the mirage of progress in offline marl through standardised baselines and
evaluation. arXiv preprint arXiv:2406.09068, 2024.
17Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets
for deep data-driven reinforcement learning. ArXiv Preprint, 2020.
Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning
without exploration. International Conference on Machine Learning, 2019.
Rihab Gorsane, Omayma Mahjoub, Ruan John de Kock, Roland Dubb, Siddarth Singh, and
Arnu Pretorius. Towards a standardised performance evaluation protocol for cooperative
MARL. Advances in Neural Information Processing Systems, 2022.
Caglar Gulcehre, Ziyu Wang, Alexander Novikov, Thomas Paine, Sergio Gómez, Konrad
Zolna, Rishabh Agarwal, Josh S Merel, Daniel J Mankowitz, Cosmin Paduraru, et al. Rl
unplugged: A suite of benchmarks for offline reinforcement learning. Advances in Neural
Information Processing Systems, 2020.
TuomasHaarnoja,AurickZhou,PieterAbbeel,andSergeyLevine.Softactor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In International
conference on machine learning, pages 1861–1870. PMLR, 2018.
Jiechuan Jiang and Zongqing Lu. Offline decentralized multi-agent reinforcement learning.
In ECAI, pages 1148–1155, 2023.
Yuheng Jing, Kai Li, Bingyun Liu, Yifan Zang, Haobo Fu, QIANG FU, Junliang Xing, and
Jian Cheng. Towards offline opponent modeling with in-context learning. In The Twelfth
International Conference on Learning Representations, 2024.
Vanshaj Khattar and Ming Jin. Winning the citylearn challenge: Adaptive optimization
with evolutionary search under trajectory-based guidance. ArXiv Preprint, 2022.
Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for
offline reinforcement learning. Advances in Neural Information Processing Systems, 2020.
Yann LeCun, Bernhard Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne
Hubbard, and Lawrence D Jackel. Backpropagation applied to handwritten zip code
recognition. Neural computation, 1(4):541–551, 1989.
Yann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database. ATT Labs
[Online]. Available: http://yann.lecun.com/exdb/mnist, 2, 2010.
Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning:
Tutorial, review, and perspectives on open problems. ArXiv Preprint, 2020.
Ryan Lowe, Yi I Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch.
Multi-agent actor-critic for mixed cooperative-competitive environments. Advances in
neural information processing systems, 2017.
Federico Lozano-Cuadra, Mathias D Thorsager, Israel Leyva-Mayorga, and Beatriz Soret.
An open source multi-agent deep reinforcement learning routing simulator for satellite
networks. arXiv preprint arXiv:2407.11047, 2024.
18Daiki E. Matsunaga, Jongmin Lee, Jaeseok Yoon, Stefanos Leonardos, Pieter Abbeel, and
Kee-EungKim. Alberdice: Addressingout-of-distributionjointactionsinofflinemulti-agent
rl via alternating stationary distribution correction estimation. In A. Oh, T. Naumann,
A.Globerson, K.Saenko, M.Hardt, andS.Levine, editors, Advances in Neural Information
Processing Systems, volume 36, pages 72648–72678. Curran Associates, Inc., 2023.
Linghui Meng, Muning Wen, Yaodong Yang, Chenyang Le, Xiyun Li, Weinan Zhang, Ying
Wen, Haifeng Zhang, Jun Wang, and Bo Xu. Offline pre-trained multi-agent decision
transformer: One big sequence model conquers all starcraftii tasks. ArXiv Preprint, 2021.
Sharada Mohanty, Erik Nygren, Florian Laurent, Manuel Schneider, Christian Scheller,
Nilabha Bhattacharya, Jeremy Watson, Adrian Egli, Christian Eichenberger, Christian
Baumberger, Gereon Vienken, Irene Sturm, Guillaume Sartoretti, and Giacomo Spigler.
Flatland-rl : Multi-agent reinforcement learning on trains. ArXiv Preprint, 2020.
LingPan,LongboHuang,TengyuMa,andHuazheXu. Planbetteramidconservatism: Offline
multi-agent reinforcement learning with actor rectification. In International conference on
machine learning, pages 17221–17237. PMLR, 2022.
Bei Peng, Tabish Rashid, Christian Schroeder de Witt, Pierre-Alexandre Kamienny, Philip
Torr, Wendelin Böhmer, and Shimon Whiteson. Facmac: Factored multi-agent centralised
policy gradients. Advances in Neural Information Processing Systems, 2021.
RafaelFigueiredoPrudencio, MarcosROAMaximo, andEstherLunaColombini. Asurveyon
offline reinforcement learning: Taxonomy, review, and open problems. IEEE Transactions
on Neural Networks and Learning Systems, 2023.
Harsha Putla, Chanakya Patibandla, Krishna Pratap Singh, and P Nagabhushan. A pilot
study of observation poisoning on selective reincarnation in multi-agent reinforcement
learning. Neural Processing Letters, 56(3):161, 2024.
Esther Luna Colombini Rafael Figueiredo Prudencio, Marcos R O A Maximo. A survey on
offline reinforcement learning: Taxonomy, review, and open problems. IEEE Trans Neural
Netw Learn Syst., pages 35(8):10237–10257, 2024.
Tabish Rashid, Mikayel Samvelyan, Christian Schroeder, Gregory Farquhar, Jakob Foerster,
and Shimon Whiteson. Qmix: Monotonic value function factorisation for deep multi-agent
reinforcement learning. International Conference on Machine Learning, 2018.
Constantin Ruhdorfer, Matteo Bortoletto, Anna Penzkofer, and Andreas Bulling. The
overcooked generalisation challenge, 2024.
Mikayel Samvelyan, Tabish Rashid, Christian Schroeder de Witt, Gregory Farquhar, Nantas
Nardelli, Tim GJ Rudner, Chia-Man Hung, Philip HS Torr, Jakob Foerster, and Shimon
Whiteson. The starcraft multi-agent challenge. International Conference on Autonomous
Agents and MultiAgent Systems, 2019.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal
policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
19Kajetan Schweighofer, Marius-constantin Dinu, Andreas Radler, Markus Hofmarcher, Vi-
hang Prakash Patil, Angela Bitto-Nemling, Hamid Eghbal-zadeh, and Sepp Hochreiter. A
dataset perspective on offline reinforcement learning. In Conference on Lifelong Learning
Agents, 2022.
Jianzhun Shao, Yun Qu, Chen Chen, Hongchang Zhang, and Xiangyang Ji. Counterfactual
conservative q learning for offline multi-agent reinforcement learning. Advances in Neural
Information Processing Systems, 37, 2023.
Quinlan Sykora, Mengye Ren, and Raquel Urtasun. Multi-agent routing value iteration
network. International Conference on Machine Learning, 2020.
Qi Tian, Kun Kuang, Furui Liu, and Baoxiang Wang. Learning from good trajectories
in offline multi-agent reinforcement learning. Proceedings of the AAAI Conference on
Artificial Intelligence, 37:11672–11680, 06 2023. doi: 10.1609/aaai.v37i10.26379.
Callum Rhys Tilbury, Claude Formanek, Louise Beyers, Jonathan P Shock, and Arnu
Pretorius. Coordinationfailureincooperativeofflinemarl. arXivpreprintarXiv:2407.01343,
2024.
Edan Toledo, Laurence Midgley, Donal Byrne, Callum Rhys Tilbury, Matthew Macfarlane,
Cyprien Courtot, and Alexandre Laterre. Flashbax: Streamlining experience replay buffers
for reinforcement learning with jax, 2023. URL https://github.com/instadeepai/
flashbax/.
Wei-Cheng Tseng, Tsun-Hsuan Johnson Wang, Yen-Chen Lin, and Phillip Isola. Offline
multi-agent reinforcement learning with knowledge distillation. In S. Koyejo, S. Mohamed,
A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information
Processing Systems, volume 35, pages 226–237. Curran Associates, Inc., 2022.
SiyingWang, XiaodieLi, HongQu, andWenyuChen. Stateaugmentationviaself-supervision
in offline multi-agent reinforcement learning. IEEE Transactions on Cognitive and Devel-
opmental Systems, 2023a.
Xiangsen Wang and Xianyuan Zhan. Offline multi-agent reinforcement learning with coupled
value factorization. In Proceedings of the 2023 International Conference on Autonomous
Agents and Multiagent Systems, pages 2781–2783, 2023.
Xiangsen Wang, Haoran Xu, Yinan Zheng, and Xianyuan Zhan. Offline multi-agent rein-
forcement learning with implicit global-to-local value regularization. Advances in Neural
Information Processing Systems, 37, 2023b.
Yihan Wang, Beining Han, Tonghan Wang, Heng Dong, and Chongjie Zhang. Off-policy
multi-agent decomposed policy gradients. arXiv preprint arXiv:2007.12322, 2020.
Muning Wen, Jakub Grudzien Kuba, Runji Lin, Weinan Zhang, Ying Wen, Jun Wang, and
Yaodong Yang. Multi-agent reinforcement learning is a sequence modeling problem. ArXiv
Preprint, 2022.
20Yiqin Yang, Xiaoteng Ma, Chenghao Li, Zewu Zheng, Qiyuan Zhang, Gao Huang, Jun
Yang, and Qianchuan Zhao. Believe what you see: Implicit constraint approach for offline
multi-agent reinforcement learning. Advances in Neural Information Processing Systems,
34:10299–10312, 2021.
Chao Yu, Akash Velu, Eugene Vinitsky, Jiaxuan Gao, Yu Wang, Alexandre Bayen, and
Yi Wu. The surprising effectiveness of ppo in cooperative multi-agent games. Advances in
Neural Information Processing Systems, 35:24611–24624, 2022.
Yang Yu. Towards sample efficient reinforcement learning. International Joint Conference
on Artificial Intelligence, 2018.
Lei Yuan, Ziqian Zhang, Lihe Li, Cong Guan, and Yang Yu. A survey of progress on
cooperative multi-agent reinforcement learning in open environment, 2023.
Huichu Zhang, Siyuan Feng, Chang Liu, Yaoyao Ding, Yichen Zhu, Zihan Zhou, Weinan
Zhang, Yong Yu, Haiming Jin, and Zhenhui Li. CityFlow: A multi-agent reinforcement
learning environment for large scale city traffic scenario. ACM International World Wide
Web Conference, 2019.
Zhengbang Zhu, Minghuan Liu, Liyuan Mao, Bingyi Kang, Minkai Xu, Yong Yu, Stefano
Ermon, and Weinan Zhang. Madiff: Offline multi-agent learning with diffusion models.
Arxiv Preprint, 2023.
21