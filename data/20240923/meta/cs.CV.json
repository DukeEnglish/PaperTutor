[
    {
        "title": "Vista3D: Unravel the 3D Darkside of a Single Image",
        "authors": "Qiuhong ShenXingyi YangMichael Bi MiXinchao Wang",
        "links": "http://arxiv.org/abs/2409.12193v1",
        "entry_id": "http://arxiv.org/abs/2409.12193v1",
        "pdf_url": "http://arxiv.org/pdf/2409.12193v1",
        "summary": "We embark on the age-old quest: unveiling the hidden dimensions of objects\nfrom mere glimpses of their visible parts. To address this, we present Vista3D,\na framework that realizes swift and consistent 3D generation within a mere 5\nminutes. At the heart of Vista3D lies a two-phase approach: the coarse phase\nand the fine phase. In the coarse phase, we rapidly generate initial geometry\nwith Gaussian Splatting from a single image. In the fine phase, we extract a\nSigned Distance Function (SDF) directly from learned Gaussian Splatting,\noptimizing it with a differentiable isosurface representation. Furthermore, it\nelevates the quality of generation by using a disentangled representation with\ntwo independent implicit functions to capture both visible and obscured aspects\nof objects. Additionally, it harmonizes gradients from 2D diffusion prior with\n3D-aware diffusion priors by angular diffusion prior composition. Through\nextensive evaluation, we demonstrate that Vista3D effectively sustains a\nbalance between the consistency and diversity of the generated 3D objects.\nDemos and code will be available at https://github.com/florinshen/Vista3D.",
        "updated": "2024-09-18 17:59:44 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.12193v1"
    },
    {
        "title": "DynaMo: In-Domain Dynamics Pretraining for Visuo-Motor Control",
        "authors": "Zichen Jeff CuiHengkai PanAadhithya IyerSiddhant HaldarLerrel Pinto",
        "links": "http://arxiv.org/abs/2409.12192v1",
        "entry_id": "http://arxiv.org/abs/2409.12192v1",
        "pdf_url": "http://arxiv.org/pdf/2409.12192v1",
        "summary": "Imitation learning has proven to be a powerful tool for training complex\nvisuomotor policies. However, current methods often require hundreds to\nthousands of expert demonstrations to handle high-dimensional visual\nobservations. A key reason for this poor data efficiency is that visual\nrepresentations are predominantly either pretrained on out-of-domain data or\ntrained directly through a behavior cloning objective. In this work, we present\nDynaMo, a new in-domain, self-supervised method for learning visual\nrepresentations. Given a set of expert demonstrations, we jointly learn a\nlatent inverse dynamics model and a forward dynamics model over a sequence of\nimage embeddings, predicting the next frame in latent space, without\naugmentations, contrastive sampling, or access to ground truth actions.\nImportantly, DynaMo does not require any out-of-domain data such as Internet\ndatasets or cross-embodied datasets. On a suite of six simulated and real\nenvironments, we show that representations learned with DynaMo significantly\nimprove downstream imitation learning performance over prior self-supervised\nlearning objectives, and pretrained representations. Gains from using DynaMo\nhold across policy classes such as Behavior Transformer, Diffusion Policy, MLP,\nand nearest neighbors. Finally, we ablate over key components of DynaMo and\nmeasure its impact on downstream policy performance. Robot videos are best\nviewed at https://dynamo-ssl.github.io",
        "updated": "2024-09-18 17:59:43 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.12192v1"
    },
    {
        "title": "Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution",
        "authors": "Peng WangShuai BaiSinan TanShijie WangZhihao FanJinze BaiKeqin ChenXuejing LiuJialin WangWenbin GeYang FanKai DangMengfei DuXuancheng RenRui MenDayiheng LiuChang ZhouJingren ZhouJunyang Lin",
        "links": "http://arxiv.org/abs/2409.12191v1",
        "entry_id": "http://arxiv.org/abs/2409.12191v1",
        "pdf_url": "http://arxiv.org/pdf/2409.12191v1",
        "summary": "We present the Qwen2-VL Series, an advanced upgrade of the previous Qwen-VL\nmodels that redefines the conventional predetermined-resolution approach in\nvisual processing. Qwen2-VL introduces the Naive Dynamic Resolution mechanism,\nwhich enables the model to dynamically process images of varying resolutions\ninto different numbers of visual tokens. This approach allows the model to\ngenerate more efficient and accurate visual representations, closely aligning\nwith human perceptual processes. The model also integrates Multimodal Rotary\nPosition Embedding (M-RoPE), facilitating the effective fusion of positional\ninformation across text, images, and videos. We employ a unified paradigm for\nprocessing both images and videos, enhancing the model's visual perception\ncapabilities. To explore the potential of large multimodal models, Qwen2-VL\ninvestigates the scaling laws for large vision-language models (LVLMs). By\nscaling both the model size-with versions at 2B, 8B, and 72B parameters-and the\namount of training data, the Qwen2-VL Series achieves highly competitive\nperformance. Notably, the Qwen2-VL-72B model achieves results comparable to\nleading models such as GPT-4o and Claude3.5-Sonnet across various multimodal\nbenchmarks, outperforming other generalist models. Code is available at\n\\url{https://github.com/QwenLM/Qwen2-VL}.",
        "updated": "2024-09-18 17:59:32 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.12191v1"
    },
    {
        "title": "Bundle Adjustment in the Eager Mode",
        "authors": "Zitong ZhanHuan XuZihang FangXinpeng WeiYaoyu HuChen Wang",
        "links": "http://arxiv.org/abs/2409.12190v1",
        "entry_id": "http://arxiv.org/abs/2409.12190v1",
        "pdf_url": "http://arxiv.org/pdf/2409.12190v1",
        "summary": "Bundle adjustment (BA) is a critical technique in various robotic\napplications, such as simultaneous localization and mapping (SLAM), augmented\nreality (AR), and photogrammetry. BA optimizes parameters such as camera poses\nand 3D landmarks to align them with observations. With the growing importance\nof deep learning in perception systems, there is an increasing need to\nintegrate BA with deep learning frameworks for enhanced reliability and\nperformance. However, widely-used C++-based BA frameworks, such as GTSAM,\ng$^2$o, and Ceres, lack native integration with modern deep learning libraries\nlike PyTorch. This limitation affects their flexibility, adaptability, ease of\ndebugging, and overall implementation efficiency. To address this gap, we\nintroduce an eager-mode BA framework seamlessly integrated with PyPose,\nproviding PyTorch-compatible interfaces with high efficiency. Our approach\nincludes GPU-accelerated, differentiable, and sparse operations designed for\n2nd-order optimization, Lie group and Lie algebra operations, and linear\nsolvers. Our eager-mode BA on GPU demonstrates substantial runtime efficiency,\nachieving an average speedup of 18.5$\\times$, 22$\\times$, and 23$\\times$\ncompared to GTSAM, g$^2$o, and Ceres, respectively.",
        "updated": "2024-09-18 17:59:29 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.12190v1"
    },
    {
        "title": "Massively Multi-Person 3D Human Motion Forecasting with Scene Context",
        "authors": "Felix B MuellerJulian TankeJuergen Gall",
        "links": "http://arxiv.org/abs/2409.12189v1",
        "entry_id": "http://arxiv.org/abs/2409.12189v1",
        "pdf_url": "http://arxiv.org/pdf/2409.12189v1",
        "summary": "Forecasting long-term 3D human motion is challenging: the stochasticity of\nhuman behavior makes it hard to generate realistic human motion from the input\nsequence alone. Information on the scene environment and the motion of nearby\npeople can greatly aid the generation process. We propose a scene-aware social\ntransformer model (SAST) to forecast long-term (10s) human motion motion.\nUnlike previous models, our approach can model interactions between both widely\nvarying numbers of people and objects in a scene. We combine a temporal\nconvolutional encoder-decoder architecture with a Transformer-based bottleneck\nthat allows us to efficiently combine motion and scene information. We model\nthe conditional motion distribution using denoising diffusion models. We\nbenchmark our approach on the Humans in Kitchens dataset, which contains 1 to\n16 persons and 29 to 50 objects that are visible simultaneously. Our model\noutperforms other approaches in terms of realism and diversity on different\nmetrics and in a user study. Code is available at\nhttps://github.com/felixbmuller/SAST.",
        "updated": "2024-09-18 17:58:51 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.12189v1"
    }
]