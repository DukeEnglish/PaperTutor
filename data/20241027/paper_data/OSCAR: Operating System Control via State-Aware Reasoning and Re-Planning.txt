Preprint
OSCAR: OPERATING SYSTEM CONTROL VIA
Oscar
STATE-AWARE REASONING AND RE-PLANNING
XiaoqiangWang1,2 BangLiu1,2†
1DIRO&InstitutCourtois,Universite´ deMontre´al 2Mila-QuebecAIInstitute
{xiaoqiang.wang, bang.liu}@umontreal.ca
ABSTRACT
Large language models (LLMs) and large multimodal models (LMMs) have
showngreatpotentialinautomatingcomplextaskslikewebbrowsingandgaming.
However, their ability to generalize across diverse applications remains limited,
hinderingbroaderutility.Toaddressthischallenge,wepresentOSCAR:Operating
System Control via state-Aware reasoning and Re-planning. OSCAR is a gener-
alist agent designed to autonomously navigate and interact with various desktop
and mobile applications through standardized controls, such as mouse and key-
board inputs, while processing screen images to fulfill user commands. OSCAR
translateshumaninstructionsintoexecutablePythoncode,enablingprecisecon-
trol over graphical user interfaces (GUIs). To enhance stability and adaptability,
OSCAR operates as a state machine, equipped with error-handling mechanisms
andtask-drivenre-planning,allowingittoefficientlyadjusttoreal-timefeedback
andexceptions. WedemonstrateOSCAR’seffectivenessthroughextensiveexperi-
mentsondiversebenchmarksacrossdesktopandmobileplatforms,whereittrans-
forms complex workflows into simple natural language commands, significantly
boostinguserproductivity. Ourcodewillbeopen-sourceuponpublication.
1 INTRODUCTION
LargeLanguageModels(LLMs)(Ouyangetal.,2022;Achiametal.,2023;Dubeyetal.,2024)and
LargeMultimodalModels(LMMs)(Lietal.,2023;Teametal.,2023;Liuetal.,2024a;Reidetal.,
2024) have demonstrated exceptional performance on tasks requiring complex reasoning (Liang
et al., 2022; Srivastava et al., 2023; Suzgun & Kalai, 2024), particularly when combined with ad-
vancedplanningtechniques(Weietal.,2022;Wangetal.,2023b;c)andexternaltools(Yangetal.,
2023c;Liuetal.,2023a). Thesemodel-centricagentsshowrevolutionarypotentialforautomating
real-worldtaskssuchaswebbrowsing(Guretal.,2023),gaming(Krzywinska,2024),andsoftware
development(Hongetal.). However,despiteimpressiveresults,theseagentsstruggletogeneralize
acrossdifferentapplicationsduetovariationsinobservationandactionspaces.Inreal-worldscenar-
ios,workflowsofteninvolveswitchingbetweenapplicationsandinteractingwithdiversegraphical
or command-line interfaces. This raises an intriguing and practical question: can we build a gen-
eralist agent capable of following user instructions across various applications using standardized
operatingsystem(OS)controlslikemouseandkeyboardinputs,whileprocessingscreenoutputs?
Recent work has explored graphical user interface (GUI) control on mobile devices, with a focus
on smartphone GUI understanding (You et al., 2024; Fan et al., 2024; Wu et al., 2024a) and task
automation(Yangetal.,2023d;Guanetal.,2024;Zhang&Zhang,2024;Wangetal.,2024a). For
desktopcomputers,existingapproachessimulatetasksinblack-boxsystemslikeAAAgames(Tan
et al., 2024) and office workflows (Wang et al., 2024c). Some methods extend this to general OS
controlviavisualquestionansweringandhumanactiontrajectories(Hongetal.,2024;Chenetal.,
2024b;Chengetal.,2024). However,thesesystemsoftenlackreal-timefeedbackfromtheOSand
struggle to adapt dynamically when task execution fails. Without a grounded executable environ-
ment,thesemethodsfallshortinreal-worldscenarios,wherereal-timefeedbackandadaptiveaction
adjustmentarecrucialfornavigatingnewGUIenvironments,similartohumanbehavior. Recently,
†Correspondingauthor.CanadaCIFARAIChair.
1
4202
tcO
42
]IA.sc[
1v36981.0142:viXraPreprint
UserInstruction:“Pleaseopenthe‘Report.docx’fileandprintit.”
CompareFixed
Agentthatonlyworks Agentthatworksin
and Dynamic
instaticenvironment dynamicenvironment
Environments
ReferenceActionTrajectory Alternative ActionTrajectory
Action1:ClickontheStartMenu
Action1:Navigateto‘C:\Documents\Report.docx’
Action2:Searchfor‘Report.docx’
Action2:Openthefile
Action3:Openthefile
Action3:Printusingthe‘Ctrl+P’keyboardshortcut.
Action4:Printusingthe‘Print’optioninWord
Figure1:ComparisonofagentactionsequencesinstaticanddynamicOSenvironmentsforthetask
of opening and printing ’Report.docx’. The static environment (left) requires a fixed action
trajectory and fails if the agent deviates. The dynamic environment (right) allows for alternative
action trajectories, enabling the agent to adapt and successfully complete the task using different
validmethods.
newexecutableenvironments(Zhengetal.,2024b;Xuetal.,2024;Xieetal.,2024b)haveemerged,
offering dynamic feedback and enabling agents to modify their actions on the fly, paving the way
formoreautonomous,adaptiveagentsinOScontroltasks.
AsdepictedinFigure1,consideranagenttaskedwithopening“Report.docx”andprintingit.In
astaticenvironment,theagentmustfollowapredeterminedsequenceofactions—clickingtheStart
Menu,searchingfor“Report.docx”,openingthefile,andprintingusingthe“Print”optionin
Word. Anydeviationfromthissequence,suchasnavigatingdirectlyto“C:\Documents\”direc-
tory, openingthefile, andprintingusingtheCtrl+Pshortcut, resultsinfailurebecausethestatic
environmentcannotacceptmultiplevalidsolutions. Incontrast,adynamicenvironmentallowsthe
agenttoadaptitsactionsbasedonreal-timefeedback,successfullycompletingthetaskusingvarious
validmethods. Thisexamplehighlightstheimportanceofadaptabilityinreal-worldsettings,where
agentsmusthandleunforeseenchangesorerrors. Toaddressthislimitation,weproposeleveraging
a large multimodal model (LMM) to develop a generalist agent capable of interpreting user com-
mands, interacting with graphical user interfaces (GUIs), and adjusting its strategy in response to
real-timefeedback.
Toachievethis,weidentifythreekeychallengesinbuildingsuchageneralistagentfordynamicex-
ecutableenvironments:1)UnifiedControlInterfaces:Theagentmustseamlesslyoperatestandard
inputmethodslikemouseandkeyboardacrossvariousapplications. Thisinvolvesexecutingprecise
actionssuchasmousemovements,clicks,scrolling,andusingkeyboardshortcuts(e.g.,Ctrl+Cfor
copyingcontent),allbasedonvisualinputs; 2)GUIGrounding: Theagentneedstointerpretthe
screenandaccuratelyidentifyrelevantelements,suchasbuttons,menus,ortextfields.Forexample,
wheninstructedtoperformawebsearch,itmustlocateandinteractwiththesearchboxbycorrectly
grounding the user instructions to the on-screen components; 3) Exploration-Based Simulation
andRe-planning: Similartohowhumansnavigateunfamiliarsoftwareinterfaces,theagentmust
havetheabilitytoexploreandadjustitsplandynamically. Thisincludesretryingactions,handling
exceptions like software crashes, and adapting its strategy based on real-time feedback from the
system. By addressing these challenges, we aim to develop a robust agent capable of navigating
awiderangeofcomputerapplicationsinaflexibleandreliablemanner. Thisdynamicinteraction
betweentheagentandtheoperatingsystem—drivenbyreal-timefeedback—formsthefoundation
ofourapproach,movingbeyondthelimitationsofstatic,pre-scriptedworkflows.
In this paper, we introduce OSCAR, a general-purpose agent designed to autonomously interact
withdynamicOSenvironmentsthroughcode-centriccontrol. OSCARgeneratesexecutablePython
code to directly interface with the OS, enabling semantically clear and precise actions, ensuring
broad applicability across diverse tasks. To enhance GUI understanding, OSCAR augment screen
2Preprint
observation with visual grounding and semantic grounding inputs by leveraging the OS window
API to extract interactable elements and their spatial layout. OSCAR operates as a state machine,
continuously looping through planning, action, and re-planning to handle execution failures and
system exceptions. To optimize efficiency, we incorporate task-driven re-planning, allowing the
agent to adjust specific tasks rather than entire workflows, minimizing overhead and enhancing
adaptabilityindynamicenvironments.
WevalidatedOSCAR’seffectivenessandgeneralizabilityacrossdiversebenchmarksinvolvingboth
desktopandsmartphoneOSenvironments. OntheGAIA(Mialonetal.,2023)benchmark,OSCAR
outperformedpreviousmethods,achievinga28.7%averagesuccessrate,withanotable13.5%suc-
cessrateonthemostcomplexLevel3tasks,nearlydoublingthepriorstate-of-the-artperformance.
OntheOSWorld(Xieetal.,2024b)andAndroidWorld(Rawlesetal.,2024)benchmarks, OSCAR
consistentlysurpassedotheragents,achievinga24.5%successrateonOSWorld,and61.6%onAn-
droidWorld, demonstratingsuperioradaptabilityacrossreal-timedynamicOStasks. Theseresults
highlight OSCAR’s advancement in transforming tedious tasks into natural language commands,
showcasingitsadaptabilityandstronggeneral-purposecapability.
2 METHODOLOGY
Inthissection,weintroduceOSCAR,anintelligentagentdesignedforgeneral-purposecontroland
navigation within operating systems. As illustrated in Figure 2, OSCAR operates as a state ma-
chine (Girault et al., 1999; Yannakakis, 2000), enabling it to handle dynamic OS environments
through systematic state transitions. This framework allows OSCAR to efficiently process user in-
structions,observetheenvironment,planandexecuteactions,andverifyoutcomes,whilemanaging
potentialOSexceptions. Wenowdetailthestatetransitionprocess,highlightinghowOSCARinte-
gratesGUIgrounding,task-drivenre-planning,andcode-centriccontrolineachoperationalstate.
2.1 FORMULATIONOFSTATETRANSITIONS
[Init → Observe]. Inthe[Init]state,OSCARawaitsuserinstructions. Uponreceivinga
command,thesystemtransitionstothe[Observe]statetobeginprocessingtheinput. Thisisthe
startingpointforeachtask,andtheagentreturnstothisstateaftercompletingorterminatingatask.
[Observe → Plan]. After receiving the user’s request, OSCAR captures a screenshot of the
currentenvironmentandinterpretsitbyperformingGUIgroundingdetailedinSection2.2. Thisin-
volvesidentifyingscreenelements,suchasbuttonsandinputfields,tounderstandtheuserinterface
context. Thesystemthentransitionstothe[Plan]state.
[Plan → Execute, Plan → Verify]. Inthe[Plan]state,OSCARgeneratesanaction
planbasedonthecurrentscreenshot,userinstructions,contextmemory,andanypreviousverifica-
tionfeedbackfromtheOS(ifavailable).AsdetailedinSection2.3,itutilizestask-drivenre-planning
toinvokethemodelbackendanddeterminethenextaction.
• If more actions are needed to complete the task, OSCAR stores the planning results and gener-
atedactionsinthecontext memoryandtransitionstothe [Execute]statetointeractwiththe
operatingsystemviaexecutablePythoncode.
• If no further actions are necessary (the whole task completion is indicated), OSCAR transitions
directlytothe[Verify]state.
[Execute → Plan, Execute → Observe → Plan]. In the [Execute] state, the
Pythoncodeisexecutedtointeractwiththeoperatingsystem. Therearetwopossibleoutcomes:
• If execution fails due to invalid code (e.g., syntax errors or attempts to access non-existent GUI
elements),OSCARtransitionsbacktothe[Plan]state,incorporatingtheinterpreter’serrormes-
sageforre-planning.
• Ifexecutionsucceeds,OSCARfirsttransitionstothe[Observe]statetocaptureanewscreen-
shot,reflectingtheupdatedstateoftheenvironment.Subsequently,OSCARmovestothe[Plan]
statetoplanthenextactionbasedonthenewcontext.
3Preprint
State Transition
[User Request] [Context Memory] System Error Handling
Data Input / Output
[Init] [Observe] [Plan] [Execute]
More action needed
Invalid action
A
[Success]
erom
oNdedeen
n oitca
m
=<
stp
mD etN
ta
sliaf
noitacifireV
xa
[Reset] [Fail] [Verify] [Error]
Verification fails
AND attempts > max
Figure 2: Illustration of the state machine model used in OSCAR. The model consists of multi-
plestates—[Init],[Observe],[Plan],[Execute],[Verify],[Success],[Fail],
[Reset], and [Error]—and handles transitions between them. Transitions are triggered by
userrequest,planningcompletion,verificationresults,orOSerrors,allowingfordynamicinterac-
tionwiththeenvironment.
[Verify → Success, Verify → Plan, Verify → Fail]. In the [Verify]
state,OSCARrunsevaluationscriptstovalidatetheoutcomesoftheexecutedactions. Thesescripts
checksystemorapplicationsettingsandanalyzefilecontenttoconfirmthattheintendedtaskswere
successfully completed. Based on the results, OSCAR either transitions to the [Success] state
if verification passes or returns to the [Plan] state if it fails. If the failure exceeds the allowed
maximumnumberofattempts,OSCARtransitionstothe[Fail]state.
[Success → Init]. Ifthetaskverificationpasses,OSCARentersthe[Success]state,sig-
nalingsuccessfultaskcompletionandnotifyingtheuser.Thesystemthentransitionstothe[Init]
state,readytoprocessthenextuserquery.
[Fail → Reset]. If the task cannot be completed after the maximum number of allowed at-
tempts,OSCARtransitionstothe[Fail]state,notifyingtheuserofthefailureandthentransition-
ingtothe[Reset]state.
[Plan → Error, Execute → Error, Verify → Error, Error → Reset].
OSCARtransitionstothe[Error]statewhenacriticalsystemexceptionorcrashoccurs,suchas
alocalmodelbackendfailureorwhentoomanyfilesorprocessesareopenintheOS.Inthisstate,
the task is terminated, and the user is notified of the error. User intervention may be required to
resolvetheissuebeforeOSCARtransitionstothe[Reset]state.
[Reset → Init]. Inthe[Reset]state,OSCARrestorestheoperatingsystemtoitspre-query
configurationbyterminatingprocessesandclosingfilehandlers.Oncetheresetiscomplete,OSCAR
returnstothe[Init]state,readytoprocessthenextuserquery.
In a nutshell, the state machine architecture of OSCAR introduces continuous feedback loops, en-
abling dynamic interaction and error recovery, which enhances its robustness in dynamic OS en-
vironments. Additionally, unlike previous methods that relied on linear action sequences and re-
planning from scratch (Yang et al., 2023d; Zhang et al., 2024a; Wu et al., 2024c), OSCAR’s state
machineintegratesreal-timeverificationfeedbackforfine-grained,task-drivenre-planning,signif-
icantlyimprovingefficiencyandadaptability. Mostimportantly,itsmodularstatetransitionsallow
forflexiblegeneralizationacrossdiverseOSenvironments,suchasdesktopandsmartphoneOS.
4Preprint
UserInstruction:“OpenNotepad,createanewfilenamed‘draft.txt’,
type‘Thisisadraft.’,andsaveittotheDocumentsfolder.”
[Init] [Observe]
GUI-groundedobservation
Wemapthenumerical
Theoriginalscreenshot
coordinatesofUIelements
containsnobounding
intoboundingboxestoget
boxesofUIelements.
SoMvisualprompt.
27
Semantic Grounding Input Screenshot Visual Grounding Input
Figure3: IllustrationofGUI-groundedobservationinOSCAR,whichincludesoriginalscreenshot,
semanticgroundinginput,andset-of-mark(SoM)-basedvisualgroundinginput.
2.2 GUI-GROUNDEDOBSERVATION
WhileLLMsexhibitstrongcapabilitiesinunderstandinggeneralvisualinformationandgrounding
in broad domains, feeding a screenshot into the model to facilitate planning and output control
overthescreenremainsinsufficient. ThisinsufficiencystemsfromthefactthatGUIimagesdiffer
significantly from natural images (Cheng et al., 2024), as they are densely packed with text and
diverse interaction elements, such as icons and widgets, often rendered at a small scale relative to
high-resolution screens. As a result, it is difficult for models to accurately locate all interaction
elementsandunderstandGUIsemantics. Forinstance,both and couldrepresentasettings
icon,dependingontheapplication.
Toaddressthis,weintroduceadual-groundingobservationapproachtoenhanceGUIunderstanding,
i.e.incorporating both visual grounding and explicit semantic grounding. Firstly, we leverage a
Set-of-Mark (SoM) prompting (Yang et al., 2023a) technique to enhance GUI visual grounding.
SoM prompting, a visual prompting technique that adds marks to image regions to significantly
improveLMMperformanceonfine-grainedvisiontasks.Specifically,weutilizenativewindowAPI
toextracttheAccessibility(A11Y)tree, akindofstructuralrepresentationprovidingthelocation,
properties, and states of UI components (Consortium, 2018). Based on the A11Y tree, we extract
precisenumericalcoordinatesofUIelementsandmapthemintoboundingboxestogenerateSoM
visualprompts(Figure3).TheA11Ytreeoffersgreaterprecisionandrobustnessthanthecommonly
adopteddetection+OCRpipeline(Gaoetal.,2023;Wangetal.,2024a),particularlyincomplex
screenswithnumerousUIelementswhereOCRoftenfails(seeSection3.1forablationanalysis).
In addition to visual grounding, we further enhance GUI understanding through explicit semantic
grounding by adding descriptive labels to key elements, such as: (ID: 14, Label: Start, X :
1
0.35, Y : 0.95, X : 0.38, Y : 1.00). These labels not only offer semantic descriptions of
1 2 2
UI components but also facilitate code-centric control by allowing precise references to elements
(e.g.byelementID).
Bycombiningthescreenshotwithdual-groundingobservations,OSCARcannotonlygrasptheover-
alllayoutandcontextoftheGUI,butalsofocusonrelevantareasofthescreen, whileflexiblyre-
ferringtospecificelementswhenneeded. ThisapproachsignificantlyenhancesGUIunderstanding,
ensuringrobustandefficienttaskexecutionindynamicOSenvironments.
2.3 TASK-DRIVENRE-PLANNING
Interacting with dynamic environments for open-ended tasks has been well-studied in domain-
specific agents, such as those agents in Minecraft (Wang et al., 2023a;d) and data analysis (Guo
etal.;Zhangetal.). Iterativeplanningwithexplorationinself-instructedtaskcurriculahasproven
effective, as agents adjust their plans based on environmental feedback. These methods typically
involvetwostages: explorationanddeployment. Duringtheexplorationphase,agentscomprehen-
5Preprint
[Plan] Code-Centric Action
[Observe] [Plan] [Execute]
[Self-Reflection on Task Re-planning]
1. This is the initial planning phase with no feedback
received, Task re-planning is not necessary at this time. [Execute]
2. No task list exists in context memory; creating a new
[Input] task list based on the user's instruction.
[New Task List]
[Instruction]&[Demonstrations] 1. Open Notepad.
2. Type "This is a draft." into Notepad.
[Context Memory]: 3. Save file as "draft.txt" in the Documents folder.
Task List: [ ]
History Actions: [ ] [Self-Reflection on Next Actions]
[Observation Input]: [CuA rd rd eit nio t n Ta al sa kc ]ti o 1n /3s a Or pe e r ne q Nu oir tee pd a. d
User Request 1. Click on the Start Menu button on the taskbar.
GUI-grounded Observation 2. Type "Notepad" into the search bar.
[Feedback Input]: 3. P thre es ss e E an rt ce hr ro er s ucl ltic sk t oo n o pth ee n N ito .tepad application from
Execution Feedback on Action
Verification Feedback on Task [Current Action] Click on the Start Menu
```python
mouse.move(id=14) # Move to “Start”
Task-driven Re-planning mouse.single_click() # Click on "Start”
```
Figure 4: Illustration of task-driven re-planning and code-centric control in OSCAR. Based on the
current observation, context memory, and real-time OS feedback from execution or verification,
OSCAR generates a refined task list and determines the next action. The action refers to GUI ele-
mentsusingsemanticgroundinginputandincludesexecutablePythoncodetocontroltheOS,such
asclickingthe“Start”button(id=14inFigure3)andlaunchingapplications.
sivelyinteractwiththeenvironmenttogatherknowledgeandexperience. Inthedeploymentphase,
agentsapplythelearnedstrategiesfromexplorationtooperateandnavigatenewenvironments.
However,whilenavigatingdynamicoperatingsystemssharesthegoalofdeterminingfeasibleaction
sequencesforcomplextasks,itintroducessignificantefficiencychallenges,asagentsmustrespond
promptly to user requests. The plan-after-fully-exploration approach is inefficient for OSCAR in
these contexts. To balance efficiency and effectiveness, we introduce task-driven re-planning,
whilestoringactiontrajectoriesandplanningresultsincontextmemorytosummarizeandleverage
past experiences. Specifically, we draw inspiration from plan-and-solve prompting (Wang et al.,
2023b; Zhang et al., 2024b), a planning-based chain-of-thought (Wei et al., 2022) approach that
simplifies complex tasks by breaking them into a hierarchy of sub-tasks and mapping them into
executableactions. AsshowninFigure4,weinstantiatethisconceptastwo-levelplanning. Level
1: Decompose user instructions into tasks using standardized operating procedures (SOPs) (Hong
et al.), improving clarity in task decomposition. Level 2: For each task, generate actions step-by-
step,interleavingplanningandexecutionwithinOSCAR’sstatemachine.
Asignificantadvantageoftask-drivenre-planningisfine-grainedself-refinement(Shinnetal.,2023;
Taoetal.,2024),i.e.whennegativefeedbackisreceivedfromdynamicevaluationinthestatetran-
sitionof[Verify] → [Plan],OSCARcanre-planonlyspecifictasks,ratherthanre-planning
the entire workflow or just the current action. This approach improves planning efficiency by en-
abling fine-grained re-planning of tasks. It also helps avoid error propagation (Zhang & Zhang,
2024), where incorrect actions early on prevent successful completion of user requests, regardless
ofhowwellsubsequentactionsareplanned. Forexample,inaworkflowinvolvingmultipleapplica-
tions—extractinginformationfromaWorddocument,observingafigureinPhotos,andsummariz-
ingcontentinPowerPoint—eachtaskrequiresseveralinteractions. Errorsinearliertasks, suchas
copyingtextorcapturinganimage,willpropagateandresultinincorrectsummariesinPowerPoint.
Formally, thecompletepromptinputforinvokingthemodelissummarizedinFigure4, whichin-
cludesuserrequest,contextmemory,GUI-groundedobservationandfeedbackfrombothexecution
andverificationphases. ThefullversionofsystempromptcanbefoundintheAppendixB.
2.4 CODE-BASEDACTION
As portrayed in Figure 4, leveraging the textualized SoM from observed screenshots, OSCAR can
easily refer interaction elements on the screen using element ID or numerical coordinates. This
allowsOSCARtogeneratecodetocontroltheseelementswithlogicallyclearsemantics. Tooper-
ationalizeOSCAR’sactionspace, weemploythewidely-usedPyAutoGUIlibrary1 formouseand
keyboardcontrol. Thislibraryenablesvariousmousebehaviors(movement, left-click, right-click,
1https://pyautogui.readthedocs.io/
6Preprint
double-click, scroll) and keyboard interactions (single key presses, key shortcuts). Further details
aresummarizedinTable5.
3 EXPERIMENTS
Benchmarks.WeevaluateOSCARonreal-worldworkflowautomationbenchmarksinvolvingcom-
plex user requests. The first benchmark is GAIA (Mialon et al., 2023), which consists of 466
question-answering (QA) structured into three levels: Level 1 includes simple tasks requiring no
morethanfivesteps; Level2involvesmorecomplextaskswith5-10stepsandmultipletools; and
Level3presentsadvancedtasksrequiringover10actionsandtoolusage. Thesecondbenchmarkis
OSWorld(Xieetal.,2024b),aninteractivedynamicenvironmentwithreal-timeOSfeedback. Itin-
cludes369taskscoveringOSsettings,officesoftware,dailyapplications(e.g.Chrome),professional
tools(e.g.VSCode),andmulti-applicationtasks.Withoutagold-standardreferenceactionsequence,
theenvironmentallowsformultiplevalidsolutions,whichareevaluatedthroughdynamicexecution
testing—verifyingmodifiedfilesordisplayedtextcontentinwindows. Additionally,similartoOS-
World, AndroidWorld(Rawlesetal.,2024)providesadynamicsmartphoneOSenvironmentwith
116tasksspreadacross20diverseapplications,andhumanannotateddifficultylevel:easy,medium,
hard. PleaserefertoAppendixDandAppendixEformoreexperimentsontheGUIunderstanding
andstaticnavigationbenchmark.
Baselines. We compare OSCAR with seven Table 1: Real-world workflow results on the
generalist agents designed to handle dynamic GAIA benchmark using the exact match metric.
OS feedback. For the desktop OS environ- SinceMMACdoesnotpubliclyreleasetheircode,
ment, we include Cradle (Tan et al., 2024), wereportMMAC’sresultsasstatedintheirpaper
UFO(Zhangetal.,2024a),FRIDAY(Wuetal., andusethesamebasemodel(i.e.GPT-4-turbo)in
2024c), and MMAC (Song et al., 2024). For allofthebaselinemodels,forafaircomparison.
the smartphone OS environment, we evalu-
ate against M3A (Rawles et al., 2024), Ap-
Model Level1 Level2 Level3 Average
pAgent (Yang et al., 2023d), and Mobile
GPT-4-turbo 9.7 6.9 0.0 5.5
Agent (Wang et al., 2024a). Implementation
GPT-4plugins 30.3 9.7 0.0 13.3
details of OSCAR and these baselines are pro-
UFO 36.9 16.1 5.4 19.4
videdinAppendixB. FRIDAY 40.9 20.1 6.1 22.4
MMAC 45.2 20.8 6.1 24.0
Results. Table 1 summarizes the results on
OSCAR 47.0 25.6 13.5 28.7
the GAIA benchmark, where OSCAR achieves
the best performance across all three levels of
Table 2: Quantitative results on the OSWorld
workflow complexity. In particular, for Level
benchmark, measured by success rate (SR). All
3 tasks, OSCAR significantly outperforms pre-
baselines incorporate the SoM visual prompt as
vious methods, achieving 13.5% compared to
auxiliaryGUI-groundedinputanduseGPT-4oas
MMAC’s 6.1%, demonstrating the effective-
thebasemodeltoensureafaircomparison.
ness of OSCAR’s task-based planning. Addi-
tionally, as shown in Table 2, OSCAR consis-
Model OS Office Daily Prof. Multi Avg.
tently surpasses other methods across various
applications in dynamic desktop OS environ- Cradle 16.7 3.5 6.6 20.4 5.5 10.5
UFO 37.5 6.8 12.8 14.3 10.9 16.5
ments. Inchallengingtasksinvolvingmultiple
FRIDAY 45.8 8.5 14.1 18.4 6.9 18.8
applications,OSCARachievesa12.9%success
OSCAR 58.3 12.0 16.7 22.4 12.9 24.5
rate, outperforming the multi-agent baseline,
UFO,whichleveragesdualagentstocoordinate
Table3:QuantitativeresultsontheAndroidWorld
workflowdecompositionandexecution. When
benchmark using the same model and input set-
adaptingOSCAR’sactionspacetoamobileen-
tingsasOSWorld.
vironment, as shown in Table 3, it achieves
betteraverageperformancethanthetwo-phase
Model Easy Medium Hard Average
approach(comprehensiveexplorationfollowed
by execution) of AppAgent, particularlyin the M3A 41.0 33.3 26.3 33.5
medium and hard subsets, highlighting the ef- MobileAgent 49.2 41.7 31.6 40.8
fectiveness and efficiency of OSCAR’s task- AppAgent 82.0 55.6 42.1 59.9
drivenre-planning. OSCAR 65.6 66.7 52.6 61.6
7Preprint
3.1 ABLATIONANALYSIS
We conduct ablation analysis on the individual components of OSCAR, including GUI-grounded
observationandvariousplanningtechniques. Specifically,wefirstcompareourGUI-groundedob-
servationagainstbaselinethatomitsthedual-groundinginput,i.e.feedingrawscreenshotsasinput.
Additionally,wereplaceA11Ytree-basedextractionwithaDetection+OCRpipeline.
For the baselines in planning techniques, we
Raw Screenshot
replace our task-driven re-planning with state- Office Detection+OCR
of-the-artmethodsusedinmulti-stepdecision- Direct Prompt
making tasks, particularly for long action se- Daily ReAct
quences. These include ReAct (Yao et al., 1.0 Chain-of-Action
2022b), plan-and-solve (Wang et al., 2023b),
0.80.9 OSCAR
0.7
andchain-of-action(Zhang&Zhang,2024). OOSS
The results of different baselines on the OS-
World benchmark are illustrated in Figure 5.
Professional
We have the following observation: 1) Both
GUI-grounding and task-driven re-planning
Multi
significantly enhance performance. Specifi-
cally,rawscreeninputwithoutGUIgrounding Figure 5: Decomposed performance of different
and direct prompts without fine-grained plan- ablation baselines. Scores are re-scaled using
ning achieve only 70% and 80% of OSCAR’s max-minnormalizationforeachcapabilitytoim-
full performance, respectively. 2) The Detec- proveclarity.
tion+OCR pipeline is less effective than the
original A11Y tree-based method, particularly on the subset of professional tools with numerous
UIelements, whereitonlymarginallyoutperformsrawscreenshotinput. Furthermore, theDetec-
tion+OCRmethodintroducesadditionalprocessingtime,reinforcingtheA11Ytreeasthesuperior
choice for dynamic OS environments. 3) Advanced planning strategies can significantly enhance
workflowperformance. Forinstance,ReActandChain-of-Actionachieveresultsthatarecompara-
bletoOSCARindailyapplicationandofficesoftwarescenarios. 4)However, withoutconsidering
real-timeOSfeedbackandefficientre-planning,ReActandChain-of-Actionstruggleinprofessional
softwareandmulti-applicationscenarios, highlightingOSCAR’sadvantageinadaptingtodynamic
OSenvironments.
3.2 IN-DEPTHANALYSIS
Instance-levelanalysisonplanningefficiency. TobetterunderstandwhyOSCARachievessuperior
performance, particularly in dynamic OS environments, we take a closer look at the final success
rateresultsandconductaninstance-levelanalysisforbothsuccessfulandfaileduserrequestsonthe
OSWorldbenchmark. Specifically,forthesuccessfulcaseswithOSCAR,wetrackthenumberofre-
planningoccurrencesbeforeverificationfailuresexceedtheallowedmaximumnumberofattempts
i.e.theupperboundforre-planningisthemaximumnumberofallowedattempts. Wealsotrackthe
totalactionstepstakenandtheratioofthesuccessfulactionpathlengthtothetotalsteps, serving
asaproxyfortheactionmatchingscoreindynamicenvironments,wherenoreferenceactionpath
exists as it does in static environments (Rawles et al., 2023). It is used to quantify the planning
andexecutionefficiencyinthefail-and-re-planningsetting,isalsoreferredasprocessscore(PS)by
Wangetal.(2024a),orascompletionrate(CR)byZhangetal.(2024a).
For failed cases, following Xu et al. (2024), we categorize failures into three classes: False Com-
pletion(FC),wheretheagentincorrectlybelievesthetaskiscompleted; ReachStepLimit(RSL),
where the agent reaches the maximum step limit without completing the task; and Invalid Action
(IA), where the agent produces outputs that do not follow instructions, including invalid formats,
nonexistent actions, or incorrect action parameters. Since OSCAR can handle invalid actions and
falsecompletionsthroughexecutionandverificationfeedback,i.e.[Execute] → [Plan]and
[Verify] → [Plan] state transitions, FC and IA errors do not occur in OSCAR. We further
analyzeasubclassofRSL,wherere-planninggeneratesthesametasklistoractiontrajectorythat
has already been marked as a verification failure in previous attempts. We refer to this subclass
asRedundantRe-plan(RR).Forcomparison,wealsoanalyzethesemetricsforFRIDAY,themost
competitivebaselineindynamicOSenvironments,asshowninTable2.
8Preprint
OSCAR requires fewer re-planning attempts. As shown in Figure 6, in the successful re-
quests, over 80% of the samples using OSCAR required fewer than 3 re-planning attempts,
whereasinFRIDAY,morethan50%ofthesuccessfulsamplesneeded3to4re-planningattempts
(the maximum allowed re-planning attempts in our ex-
periments is 4, after which the case is deemed a fail-
ure). ThisdistributionhighlightsOSCAR’sefficiencyad- 30 OSCAR
vantages,asitleveragestask-drivenre-planningtofocus FRIDAY
on high-level task lists and perform fine-grained adjust- 20
ments,ratherthanre-planningtheentireworkflow. These
findings align with our goal of adapting to dynamic OS 10
feedbackwhileimprovingefficiency.
0
OSCAR’s re-planning includes smaller, more efficient
100
steps. The proxy action matching score indicates that
OSCAR consistently takes smaller, more efficient steps 80
duringre-planning,whileFRIDAY’sscoreworsensasthe 60
numberofre-planningattemptsincreases.Thisefficiency
40
is due to OSCAR’s ability to learn from previous trials,
usingthestoredtasklistandactionhistoryinitscontext 20
memorytooptimizesubsequenttasklistsandactiontra- 0
0 1 2 3 4
jectoriesuponreceivingverificationfailurefeedback. Number of re-planning attempts
OSCAR’s failure cases involve less redundant re- Figure 6: Planning efficiency analysis
planning. As shown in the failure case statistics in Ta- ofsuccessfulcases.
ble 4, although OSCAR may not always complete the
user request within the allowed attempts, its re-planning
Table4: FailurecasestatisticsforFalse
typically avoids repeating previously explored steps. In
Completion (FC), Reach Step Limit
contrast, FRIDAY’stendencytore-plantheentirework-
(RSL), and Invalid Action (IA). The
flow frequently (52.8%) results in generating an action
subclass of RSL, Redundant Re-plan
trajectory that has already been verified as unsuccessful.
(RR),isalsoreportedasaratiorelative
Thisfindingcomplementsthesuccesscaseresults,where
tothetotalnumberoffailurecases.
most of OSCAR’s successful cases required only 1-2 re-
planningattempts. Model FC RSL(RR) IA
Qualitativeexamples.AsillustratedinFigure7,OSCAR FRIDAY 9.1% 70.4%(52.8%) 20.5%
OSCAR – 100%(15.2%) –
effectively handles complex requests involving multi-
ple applications, i.e.OS→Office→OS→Daily, showcas-
ingitsflexibleandeffectiveplanningcapabilities. PleaserefertoAppendixFformorequalitative
examples.
4 RELATED WORKS
GUIagents. LLMandLMM-basedagents(Wangetal.,2024b;Xieetal.,2024a;Madaanetal.,
2023), equipped with advanced planning module (Xu et al., 2023; Shinn et al., 2023), have been
developed across various environments, including robotics (Driess et al., 2023; Zitkovich et al.,
2023), web browsing (Yao et al., 2022a; Gur et al., 2023), gaming (Fan et al., 2022), software
development (Yang et al., 2023b), automating benchmark construction (Liu et al., 2024b), data
analysis (Zhang et al.), and AI for science (Xiao et al., 2024). Although recent all-in-one agent
developmentplatforms(Wuetal.,2023;Xieetal.,2023;Hongetal.) havebeenreleased,mostof
theseagentsoperatewithinspecificdomains,limitingtheirbroaderapplicability.Amongthem,GUI
agents—capableofinteractingwithvariousdesktopandsmartphoneGUIslikehumanusers—offer
broader applicability in automating real-world workflows (Mialon et al., 2023). Some agents are
continuallypre-trained(Chengetal.,2024)orfine-tuned(Chenetal.,2024b)onGUI-specificdata.
Others simulate GUI control in sandbox environments, such as AAA games (Tan et al., 2024) or
office workflows (Wang et al., 2024c), which require internal application-specific APIs to interact
withtheenvironment. Inabroadercontext,someagentsinteractwithbasicOSAPIsbutareoften
designedforstatic,pre-definedenvironments(Reedetal.,2022;Hongetal.,2024)withoutground-
inginreal-timeexecutableenvironments. Otheragentsfollowlinearactionsequencesandperform
re-planningfromscratch(Yangetal.,2023d;Zhangetal.,2024a;Wuetal.,2024c)whenverification
9
gnihctaM
noitcA
yxorP
noitubirtsiDPreprint
Figure 7: Qualitative results when processing user request “Could you please convert ‘Pre.pptx’
to video and play it with VLC?” on the OSWorld benchmark. Some intermediate steps and other
regionsofthescreenshothavebeenomittedforclarity.
fails,lackingfine-grainedre-planningstrategies,whichmakesthemlessefficientinreal-worldsce-
narios. Motivatedbytheselimitations,wedesignOSCARtohandlereal-timedynamicOSfeedback
usinganefficient,state-aware,task-drivenre-planningstrategy.
Synergizing LLMs and LMMs with OS. Beyond GUI agents, another line of work explores
integrating LLMs and LMMs with OSs in two key areas: 1) optimizing or tuning traditional OS
functionsusingLLMs,and2)integratingLLMsintoOSkernels(LLMasOS)toserveassystem-
levelinterfaces,facilitatinglocalagentoperationsanddeployment. Theformerincludesoptimizing
CPUloadbalancing(Lietal.,2024),improvingstorageaccess(Wuetal.,2024b),andidentifying
and repairing code vulnerabilities (Islam et al., 2024). The latter focuses on OS-level hardware
adaptation and resource management (Kamath & Yadalam, 2024) as well as agent-level resource
schedulingandsharing(Meietal.,2024;Zhuoetal.,2024),suchasmanagingagentmemoryand
enabling efficient communication among multiple heterogeneous agents sharing the same model
back-end. Unlike these approaches, OSCAR functions as a generalist GUI agent, acting as an OS
co-pilottoenhanceuserexperienceandproductivity.
5 CONCLUSION
Inthiswork,weintroducedOSCAR,ageneralistagentdesignedtoautonomouslynavigateandin-
teractwithdynamicOSenvironmentsusingacode-centriccontrolframework. Byleveragingtask-
driven re-planning and GUI-grounded observations, OSCAR demonstrates robust adaptability and
effectivenessacrossbothdesktopandsmartphoneOStasks. Ourexperimentsonreal-worldwork-
flowautomationbenchmarks,includingGAIA,OSWorld,andAndroidWorld,showedthatOSCAR
outperformsexistingmethods,achievingsignificantimprovementsintasksuccessrates,particularly
forcomplex,multi-steptasks.Ablationstudiesfurtherconfirmedtheimportanceofkeycomponents
10Preprint
like GUI-grounded observation and task-driven re-planning for enhancing performance and mini-
mizingredundantre-planning. Overall,OSCARoffersaversatile,efficientsolutionforautomating
workflows,makingitapowerfultoolforimprovingproductivityindynamicOSenvironments.
REFERENCES
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Ale-
man,DiogoAlmeida,JankoAltenschmidt,SamAltman,ShyamalAnadkat,etal. Gpt-4technical
report. arXivpreprintarXiv:2303.08774,2023.
DongpingChen,YueHuang,SiyuanWu,JingyuTang,LiuyiChen,YilinBai,ZhigangHe,Chenlong
Wang, Huichi Zhou, Yiqiang Li, et al. Gui-world: A dataset for gui-oriented multimodal llm-
basedagents. arXivpreprintarXiv:2406.10819,2024a.
Wentong Chen, Junbo Cui, Jinyi Hu, Yujia Qin, Junjie Fang, Yue Zhao, Chongyi Wang, Jun Liu,
GuirongChen,YupengHuo,etal. Guicourse: Fromgeneralvisionlanguagemodelstoversatile
guiagents. arXivpreprintarXiv:2406.11317,2024b.
Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Li YanTao, Jianbing Zhang, and Zhiy-
ong Wu. SeeClick: Harnessing GUI grounding for advanced visual GUI agents. In Lun-
Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting
of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 9313–9332,
Bangkok, Thailand, August 2024. Association for Computational Linguistics. URL https:
//aclanthology.org/2024.acl-long.505.
World Wide Web Consortium. Core accessibility api mappings 1.1. https://www.w3.org/
TR/core-aam-1.1/,2018. Accessed: [Insertdateofaccess].
Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, and
YuSu. Mind2web: towardsageneralistagentfortheweb. InProceedingsofthe37thInterna-
tionalConferenceonNeuralInformationProcessingSystems,pp.28091–28114,2023.
Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter,
AyzaanWahid, JonathanTompson, QuanVuong, TianheYu, etal. Palm-e: anembodiedmulti-
modallanguagemodel. InProceedingsofthe40thInternationalConferenceonMachineLearn-
ing,pp.8469–8488,2023.
AbhimanyuDubey,AbhinavJauhri,AbhinavPandey,AbhishekKadian,AhmadAl-Dahle,Aiesha
Letman,AkhilMathur,AlanSchelten,AmyYang,AngelaFan,etal. Thellama3herdofmodels.
arXivpreprintarXiv:2407.21783,2024.
LinxiFan,GuanzhiWang,YunfanJiang,AjayMandlekar,YuncongYang,HaoyiZhu,AndrewTang,
De-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo: building open-ended embodied
agents with internet-scale knowledge. In Proceedings of the 36th International Conference on
NeuralInformationProcessingSystems,pp.18343–18362,2022.
Yue Fan, Lei Ding, Ching-Chen Kuo, Shan Jiang, Yang Zhao, Xinze Guan, Jie Yang, Yi Zhang,
andXinEricWang. Readanywherepointed: Layout-awareguiscreenreadingwithtree-of-lens
grounding. arXivpreprintarXiv:2406.19263,2024.
DifeiGao,LeiJi,ZechenBai,MingyuOuyang,PeiranLi,DongxingMao,QinchenWu,Weichen
Zhang,PeiyiWang,XiangwuGuo,etal.Assistgui:Task-orienteddesktopgraphicaluserinterface
automation. arXivpreprintarXiv:2312.13108,2023.
Alain Girault, Bilung Lee, and Edward A Lee. Hierarchical finite state machines with multiple
concurrency models. IEEE Transactions on computer-aided design of integrated circuits and
systems,18(6):742–760,1999.
GoogleCloud. CloudVisionAPI. https://cloud.google.com/vision. Accessed: Octo-
ber5,2023.
11Preprint
Yanchu Guan, Dong Wang, Zhixuan Chu, Shiyu Wang, Feiyue Ni, Ruihua Song, and Chenyi
Zhuang. Intelligentagentswithllm-basedprocessautomation. InProceedingsofthe30thACM
SIGKDDConferenceonKnowledgeDiscoveryandDataMining,pp.5018–5027,2024.
Siyuan Guo, Cheng Deng, Ying Wen, Hechang Chen, Yi Chang, and Jun Wang. Ds-agent: Auto-
mateddatasciencebyempoweringlargelanguagemodelswithcase-basedreasoning. InForty-
firstInternationalConferenceonMachineLearning.
Izzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdari, Yutaka Matsuo, Douglas Eck, and
Aleksandra Faust. A real-world webagent with planning, long context understanding, and pro-
gramsynthesis. arXivpreprintarXiv:2307.12856,2023.
Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Jinlin Wang, Ceyao
Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, et al. Metagpt: Meta programming for
a multi-agent collaborative framework. In The Twelfth International Conference on Learning
Representations.
WenyiHong,WeihanWang,QingsongLv,JiazhengXu,WenmengYu,JunhuiJi,YanWang,Zihan
Wang, Yuxiao Dong, Ming Ding, et al. Cogagent: A visual language model for gui agents.
In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.
14281–14290,2024.
NafisTanveerIslam,JosephKhoury,AndrewSeong,GonzaloDeLaTorreParra,EliasBou-Harb,
and Peyman Najafirad. Llm-powered code vulnerability repair with reinforcement learning and
semanticreward. arXivpreprintarXiv:2401.03374,2024.
AdityaKKamathandSujayYadalam. Herdingllamas: Usingllmsasanosmodule. arXivpreprint
arXiv:2401.08908,2024.
Tanya Krzywinska. Being a determined agent in (the) world of warcraft: text/play/identity. In
Videogame,player,text,pp.101–119.ManchesterUniversityPress,2024.
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image
pre-trainingwithfrozenimageencodersandlargelanguagemodels. InInternationalconference
onmachinelearning,pp.19730–19742.PMLR,2023.
Tiangang Li, Shi Ying, Yishi Zhao, and Jianga Shang. Batch jobs load balancing scheduling in
cloudcomputingusingdistributionalreinforcementlearning. IEEETransactionsonParalleland
DistributedSystems,35(1):169–185,2024.
PercyLiang,RishiBommasani,TonyLee,DimitrisTsipras,DilaraSoylu,MichihiroYasunaga,Yian
Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic evaluation of language
models. arXivpreprintarXiv:2211.09110,2022.
HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee. Visualinstructiontuning. Advances
inneuralinformationprocessingsystems,36,2024a.
MinghaoLiu,ZonglinDi,JiahengWei,ZhongruoWang,HengxiangZhang,RuixuanXiao,Haoyu
Wang,JinlongPang,HaoChen,AnkitShah,etal. Automaticdatasetconstruction(adc): Sample
collection,datacuration,andbeyond. arXivpreprintarXiv:2408.11338,2024b.
ShilongLiu,HaoCheng,HaotianLiu,HaoZhang,FengLi,TianheRen,XueyanZou,JianweiYang,
HangSu,JunZhu,etal. Llava-plus: Learningtousetoolsforcreatingmultimodalagents. arXiv
preprintarXiv:2311.05437,2023a.
Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. G-eval:
NLG evaluation using gpt-4 with better human alignment. In Houda Bouamor, Juan Pino, and
Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Lan-
guageProcessing, pp.2511–2522, Singapore, December2023b.AssociationforComputational
Linguistics.doi:10.18653/v1/2023.emnlp-main.153.URLhttps://aclanthology.org/
2023.emnlp-main.153.
12Preprint
Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri
Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: iterative refinement
withself-feedback. InProceedingsofthe37thInternationalConferenceonNeuralInformation
ProcessingSystems,pp.46534–46594,2023.
Kai Mei, Zelong Li, Shuyuan Xu, Ruosong Ye, Yingqiang Ge, and Yongfeng Zhang. Llm agent
operatingsystem. arXivpreprintarXiv:2403.16971,2024.
Gre´goire Mialon, Cle´mentine Fourrier, Craig Swift, Thomas Wolf, Yann LeCun, and Thomas
Scialom. Gaia: abenchmarkforgeneralaiassistants. arXivpreprintarXiv:2311.12983,2023.
LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida,CarrollWainwright,PamelaMishkin,Chong
Zhang,SandhiniAgarwal,KatarinaSlama,AlexGray,etal. Traininglanguagemodelstofollow
instructionswithhumanfeedback. InAdvancesinNeuralInformationProcessingSystems,2022.
Shuofei Qiao, Runnan Fang, Ningyu Zhang, Yuqi Zhu, Xiang Chen, Shumin Deng, Yong Jiang,
PengjunXie,FeiHuang,andHuajunChen. Agentplanningwithworldknowledgemodel. arXiv
preprintarXiv:2405.14205,2024.
Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana Riva, and Timothy Lillicrap. Android in
thewild:alarge-scaledatasetforandroiddevicecontrol.InProceedingsofthe37thInternational
ConferenceonNeuralInformationProcessingSystems,pp.59708–59728,2023.
Christopher Rawles, Sarah Clinckemaillie, Yifan Chang, Jonathan Waltz, Gabrielle Lau, Mary-
bethFair,AliceLi,WilliamBishop,WeiLi, FolawiyoCampbell-Ajala,etal. Androidworld: A
dynamic benchmarking environment for autonomous agents. arXiv preprint arXiv:2405.14573,
2024.
Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov,
Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, et al.
Ageneralistagent. arXivpreprintarXiv:2205.06175,2022.
Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-
baptisteAlayrac,RaduSoricut,AngelikiLazaridou,OrhanFirat,JulianSchrittwieser,etal.Gem-
ini1.5: Unlockingmultimodalunderstandingacrossmillionsoftokensofcontext. arXivpreprint
arXiv:2403.05530,2024.
DillonReis,JordanKupec,JacquelineHong,andAhmadDaoudi. Real-timeflyingobjectdetection
withyolov8. arXivpreprintarXiv:2305.09972,2023.
NoahShinn,FedericoCassano,AshwinGopinath,KarthikNarasimhan,andShunyuYao.Reflexion:
language agents with verbal reinforcement learning. In Proceedings of the 37th International
ConferenceonNeuralInformationProcessingSystems,pp.8634–8652,2023.
ZiruiSong,YaohangLi,MengFang,ZhenhaoChen,ZechengShi,andYuanHuang.Mmac-copilot:
Multi-modal agent collaboration operating system copilot. arXiv preprint arXiv:2404.18074,
2024.
Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam
Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adria` Garriga-Alonso, et al. Beyond the
imitationgame: Quantifyingandextrapolatingthecapabilitiesoflanguagemodels. Transactions
onMachineLearningResearch,2023.
MiracSuzgunandAdamTaumanKalai. Meta-prompting: Enhancinglanguagemodelswithtask-
agnosticscaffolding. arXivpreprintarXiv:2401.12954,2024.
Weihao Tan, Ziluo Ding, Wentao Zhang, Boyu Li, Bohan Zhou, Junpeng Yue, Haochong Xia,
Jiechuan Jiang, Longtao Zheng, Xinrun Xu, et al. Towards general computer control: A mul-
timodalagentforreddeadredemptioniiasacasestudy. arXivpreprintarXiv:2403.03186,2024.
Zhengwei Tao, Ting-En Lin, Xiancai Chen, Hangyu Li, Yuchuan Wu, Yongbin Li, Zhi Jin, Fei
Huang, Dacheng Tao, and Jingren Zhou. A survey on self-evolution of large language models.
arXivpreprintarXiv:2404.14387,2024.
13Preprint
Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu,
RaduSoricut,JohanSchalkwyk,AndrewMDai,AnjaHauth,etal. Gemini: afamilyofhighly
capablemultimodalmodels. arXivpreprintarXiv:2312.11805,2023.
Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan,
andAnimaAnandkumar. Voyager: Anopen-endedembodiedagentwithlargelanguagemodels.
arXivpreprintarXiv:2305.16291,2023a.
Junyang Wang, Haiyang Xu, Jiabo Ye, Ming Yan, Weizhou Shen, Ji Zhang, Fei Huang, and Jitao
Sang.Mobile-agent:Autonomousmulti-modalmobiledeviceagentwithvisualperception.arXiv
preprintarXiv:2401.16158,2024a.
Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, and Ee-Peng
Lim. Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large lan-
guage models. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings
of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers), pp. 2609–2634, Toronto, Canada, July 2023b. Association for Computational Linguis-
tics. doi: 10.18653/v1/2023.acl-long.147. URL https://aclanthology.org/2023.
acl-long.147.
LeiWang,ChenMa,XueyangFeng,ZeyuZhang,HaoYang,JingsenZhang,ZhiyuanChen,Jiakai
Tang,XuChen,YankaiLin,etal. Asurveyonlargelanguagemodelbasedautonomousagents.
FrontiersofComputerScience,18(6):186345,2024b.
Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian Ma, Yitao Liang, and Team Craft-
Jarvis.Describe,explain,planandselect:interactiveplanningwithlargelanguagemodelsenables
open-worldmulti-taskagents. InProceedingsofthe37thInternationalConferenceonNeuralIn-
formationProcessingSystems,pp.34153–34189,2023c.
Zihao Wang, Shaofei Cai, Anji Liu, Yonggang Jin, Jinbing Hou, Bowei Zhang, Haowei Lin,
ZhaofengHe,ZilongZheng,YaodongYang,etal. Jarvis-1: Open-worldmulti-taskagentswith
memory-augmentedmultimodallanguagemodels. arXivpreprintarXiv:2311.05997,2023d.
ZilongWang,YuedongCui,LiZhong,ZiminZhang,DaYin,BillYuchenLin,andJingboShang.
Officebench: Benchmarkinglanguageagentsacrossmultipleapplicationsforofficeautomation.
arXivpreprintarXiv:2407.19056,2024c.
JasonWei,XuezhiWang,DaleSchuurmans,MaartenBosma,FeiXia,EdChi,QuocVLe,Denny
Zhou,etal. Chain-of-thoughtpromptingelicitsreasoninginlargelanguagemodels. Advancesin
NeuralInformationProcessingSystems,35:24824–24837,2022.
Qinchen Wu, Difei Gao, Kevin Qinghong Lin, Zhuoyu Wu, Xiangwu Guo, Peiran Li, Weichen
Zhang, Hengxu Wang, and Mike Zheng Shou. Gui action narrator: Where and when did that
actiontakeplace? arXivpreprintarXiv:2406.13719,2024a.
Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li,
LiJiang,XiaoyunZhang,andChiWang. Autogen:Enablingnext-genllmapplicationsviamulti-
agentconversationframework. arXivpreprintarXiv:2308.08155,2023.
Ronglong Wu, Zhirong Shen, Zhiwei Yang, and Jiwu Shu. Mitigating write disturbance in non-
volatilememoryviacouplingmachinelearningwithout-of-placeupdates. In2024IEEEInterna-
tionalSymposiumonHigh-PerformanceComputerArchitecture(HPCA),pp.1184–1198.IEEE,
2024b.
ZhiyongWu, ChengchengHan, ZichenDing, ZhenminWeng, ZhoumianzeLiu, ShunyuYao, Tao
Yu,andLingpengKong. Os-copilot:Towardsgeneralistcomputeragentswithself-improvement.
arXivpreprintarXiv:2402.07456,2024c.
YijiaXiao, EdwardSun, YiqiaoJin, QifanWang, andWeiWang. Proteingpt: Multimodalllmfor
proteinpropertypredictionandstructureunderstanding. arXivpreprintarXiv:2408.11363,2024.
JunlinXie,ZhihongChen,RuifeiZhang,XiangWan,andGuanbinLi. Largemultimodalagents: A
survey. arXivpreprintarXiv:2402.15116,2024a.
14Preprint
TianbaoXie,FanZhou,ZhoujunCheng,PengShi,LuoxuanWeng,YitaoLiu,TohJingHua,Jun-
ning Zhao, Qian Liu, Che Liu, et al. Openagents: Anopen platformfor language agentsin the
wild. arXivpreprintarXiv:2310.10634,2023.
Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Jing
Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, et al. Osworld: Benchmarking multimodal
agents for open-ended tasks in real computer environments. arXiv preprint arXiv:2404.07972,
2024b.
Binfeng Xu, Zhiyuan Peng, Bowen Lei, Subhabrata Mukherjee, Yuchen Liu, and Dongkuan Xu.
Rewoo:Decouplingreasoningfromobservationsforefficientaugmentedlanguagemodels.arXiv
preprintarXiv:2305.18323,2023.
Tianqi Xu, Linyao Chen, Dai-Jie Wu, Yanjun Chen, Zecheng Zhang, Xiang Yao, Zhiqiang Xie,
YongchaoChen,ShilongLiu,BochenQian,etal. Crab: Cross-environmentagentbenchmarkfor
multimodallanguagemodelagents. arXivpreprintarXiv:2407.01511,2024.
Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, and Jianfeng Gao. Set-of-mark
promptingunleashesextraordinaryvisualgroundingingpt-4v. arXivpreprintarXiv:2310.11441,
2023a.
JohnYang,AksharaPrabhakar,KarthikNarasimhan,andShunyuYao. Intercode:standardizingand
benchmarking interactive coding with execution feedback. In Proceedings of the 37th Interna-
tionalConferenceonNeuralInformationProcessingSystems,pp.23826–23854,2023b.
RuiYang,LinSong,YanweiLi,SijieZhao,YixiaoGe,XiuLi,andYingShan. Gpt4tools: teaching
large language model to use tools via self-instruction. In Proceedings of the 37th International
ConferenceonNeuralInformationProcessingSystems,pp.71995–72007,2023c.
ZhaoYang,JiaxuanLiu,YuchengHan,XinChen,ZebiaoHuang,BinFu,andGangYu. Appagent:
Multimodalagentsassmartphoneusers. arXivpreprintarXiv:2312.13771,2023d.
MihalisYannakakis. Hierarchicalstatemachines. InIFIPInternationalConferenceonTheoretical
ComputerScience,pp.315–330.Springer,2000.
Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable
real-worldwebinteractionwithgroundedlanguageagents. AdvancesinNeuralInformationPro-
cessingSystems,35:20744–20757,2022a.
Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.
React: Synergizingreasoningandactinginlanguagemodels. arXivpreprintarXiv:2210.03629,
2022b.
KeenYou,HaotianZhang,EldonSchoop,FlorisWeers,AmandaSwearngin,JeffreyNichols,Yinfei
Yang,andZheGan. Ferret-ui: Groundedmobileuiunderstandingwithmultimodalllms. arXiv
preprintarXiv:2404.05719,2024.
ChaoyunZhang,LiqunLi,ShilinHe,XuZhang,BoQiao,SiQin,MinghuaMa,YuKang,Qingwei
Lin,SaravanRajmohan,etal.Ufo:Aui-focusedagentforwindowsosinteraction.arXivpreprint
arXiv:2402.07939,2024a.
CongZhang,DeikDerrickGohXin,DexunLi,HaoZhang,andYongLiu. Meta-taskplanningfor
languageagents. arXivpreprintarXiv:2405.16510,2024b.
TianyiZhang,VarshaKishore,FelixWu,KilianQWeinberger,andYoavArtzi. Bertscore: Evaluat-
ingtextgenerationwithbert. InInternationalConferenceonLearningRepresentations,2019.
WenqiZhang,YongliangShen,WeimingLu,andYuetingZhuang. Data-copilot: Bridgingbillions
of data and humans with autonomous workflow. In ICLR 2024 Workshop on Large Language
Model(LLM)Agents.
15Preprint
ZhuoshengZhangandAstonZhang. Youonlylookatscreens: Multimodalchain-of-actionagents.
In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Findings of the Association for
Computational Linguistics ACL 2024, pp. 3132–3149, Bangkok, Thailand and virtual meeting,
August 2024. Association for Computational Linguistics. URL https://aclanthology.
org/2024.findings-acl.186.
BoyuanZheng, BoyuGou, JihyungKil, HuanSun, andYuSu. Gpt-4v(ision)isageneralistweb
agent,ifgrounded. arXivpreprintarXiv:2401.01614,2024a.
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,
Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and
chatbotarena. arXivpreprintarXiv:2306.05685,2023.
Longtao Zheng, Zhiyuan Huang, Zhenghai Xue, Xinrun Wang, Bo An, and Shuicheng Yan.
Agentstudio: A toolkit for building general virtual agents. arXiv preprint arXiv:2403.17918,
2024b.
ZhaoZhuo,RongzhenLi,KaiLiu,HuhaiZou,KaiMaoLi, JieYu, TianhaoSun,andQingboWu.
Kaos: Largemodelmulti-agentoperatingsystem. arXivpreprintarXiv:2406.11342,2024.
BriannaZitkovich, TianheYu, SichunXu, PengXu, TedXiao, FeiXia, JialinWu, PaulWohlhart,
StefanWelker,AyzaanWahid,etal.Rt-2:Vision-language-actionmodelstransferwebknowledge
toroboticcontrol. InConferenceonRobotLearning,pp.2165–2183.PMLR,2023.
16Preprint
A OVERVIEW
IntheAppendix,wepresent:
• ImplementationdetailsinAppendixB.
• BaselinedetailsinAppendixC.
• ExperimentsonGUIunderstandingbenchmarksinAppendixD.
• ExperimentsonstaticGUInavigationbenchmarksinAppendixE.
• AdditionalqualitativeresultsinAppendixF.
B IMPLEMENTATIONS
Observationspace. IndynamicOSenvironments,weextractSet-of-Mark(SoM)usingnativesys-
temAPIstoobtaintheAccessibility(A11Y)tree,asdescribedinSection2.2. Forablationstudyin
Section3.1andotherbenchmarkswithoutadynamicOSenvironment,asdescribedinAppendixD
and Appendix E, i.e.only providing a screenshot, we employ an Detection+OCR pipeline to ex-
tractSoM.Specifically, wefollowGaoetal.(2023);Wangetal.(2024a)anduseYOLO-v8(Reis
etal.,2023)andGoogleOCR(GoogleCloud)toparsetheGUIintoSoMvisualprompts, serving
asauxiliaryinputsforscreenobservation.
Action space. The action space of OSCAR in desktop OS and smartphone OS is summarized in
Table5. ThisactionspaceisusedinthedynamicOSenvironments,i.e.OSWorld(Xieetal.,2024b)
andAndroidWorld(Rawlesetal.,2024). FortheGUIunderstandingbenchmarkdescribedinAp-
pendix D and the static GUI navigation benchmark in Appendix E, we adapt the action space to
meet the benchmark requirements, i.e.free-form answering text format in the GUI understanding
benchmark, and structural output including predefined action types and selected elements or loca-
tioncoordinates.
Basemodel. Toensureafaircomparison,wesetthebasemodelofOSCARandallbaselinemodels
toGPT-4o,i.e.gpt-4o-2024-05-13,exceptfortheresultsonGAIAinTable1,whicharebased
onGPT-4-turbo,i.e.gpt-4-turbo-2024-04-09,sincethebaselineMMAC(Songetal.,2024)
doesnotpubliclyreleasetheircodeandtheirresultsarebasedonGPT-4-turbo. Thetemperatureof
responsegenerationissetto0.1toreducethevarianceintextgeneration. Weprovide8in-context
demonstrationexamplestohelpthemodelbetterunderstandtheinstruction. Theseexamplesdonot
includeascreenshotbutprovideadescriptionofthecurrentscreen. Allbaselinesarealsoprovided
with8in-contextdemonstrationstoensureafaircomparison. Thefullversionofsystempromptare
providedinFigure10andFigure11.
Experiment setup. We conduct evaluation experiments on 2 A100 GPUs. Since fine-tuning the
base model is not involved and it is accessed via API, the GPU is mainly required for the Detec-
tion+OCR pipeline. As this pipeline is efficient on CPU machines, all experiments can also run
onregularWindows11machineswithWSLvirtualizationsupport,whichisusedforencapsulating
the development and test environments in Docker containers. The maximum number of allowed
attempts per run is set to 4. We report the average results across 4 runs for each model on each
benchmark.
C BASELINE DETAILS
We employ four types of baselines for a comprehensive and fair comparison with OSCAR, cate-
gorized along two orthogonal dimensions: 1) whether the baseline is based on general-purpose
out-of-the-box LMMs, or specialized LMMs that have been continually pre-trained (without hu-
man annotations) or fine-tuned (with curated human annotations) on GUI-specialized data, and 2)
the target GUI scenario, whether the agent is developed for desktop OS or smartphone OS. These
baselinesaresummarizedinTable6. Tothebestofourknowledge,OSCARisthefirstagentcapa-
bleofnavigatingbothdesktopandsmartphoneOSenvironmentswhilerespondingtoreal-timeOS
feedback.
17Preprint
Table5: TheformulationofactionspaceofOSCARtonavigateindesktopOS(toppart)andsmart-
phoneOS(bottompart).
Action Parameter Description
id:int MovethemousecursortotheGUIelementlabeledwithid
move
(x:float,y:float) Movethemousecursortogivencoordinate(x,y)
singleclick – Clicktheleftbuttonofmouseatcurrentposition
doubleclick – Clicktheleftbuttontwiceofmouseatcurrentposition
rightclick – Clicktherightbuttonofmouseatcurrentposition
scroll dist:int Scrollthemousewheelwithdistancedist
drag (x 1:int,y 1:int,x 2:int,y 2:int) Holdanddragthemousecursorfrom(x 1,y 1)to(x 2,y 2)
press key:str Pressgivenkeyorkeyboardshortcutsincurrentwindow
write text:str Writedownthegiventextincurrentwindow
id:int TapontheGUIelementlabeledwithid
tap
(x:float,y:float) Tapthescreenongivencoordinate(x,y)
id:int PressandholdtheGUIelementlabeledwithid
longtap
(x:float,y:float) Pressandholdscreenongivencoordinate(x,y)
Swipeonanelementlabeledwithidinagiven
swipe (id:int,dir:str,dist:float)
directiondir(up,down,left,right)anddistancedist.
Swipefromthecoordinate(x,y)onthescreeninagiven
swipe (x:int,y:int,dir:str,dist:float)
directiondir(up,down,left,right)anddistancedist.
write text:str Writedownthegiventextincurrenttextfield
Table 6: Baselines for comparison with OSCAR, categorized by general-purpose out-of-the-box
(OOTB) vs. specialized fine-tuned (FT) base LMMs and their target GUI environment (desktop
orsmartphoneOS).OSCARuniquelynavigatesbothenvironmentswithreal-timeOSfeedback.
Agent BaseModel DesktopOS SmartphoneOS DynamicFeedback
✗ ✓ ✗
Auto-GUI(Zhang&Zhang,2024) OOTB
✓ ✗ ✗
SeeAct(Zhengetal.,2024a) OOTB
✓ ✓ ✗
CogAgent(Hongetal.,2024) FT
✓ ✓ ✗
SeeClick(Chengetal.,2024) FT
✓ ✓ ✗
GUICourse(Chenetal.,2024b) FT
✗ ✓ ✓
AppAgent(Yangetal.,2023d) OOTB
✗ ✓ ✓
MobileAgent(Wangetal.,2024a) OOTB
✗ ✓ ✓
M3A(Rawlesetal.,2024) OOTB
✓ ✗ ✓
WebAgent(Guretal.,2023) FT
✓ ✗ ✓
FRIDAY(Wuetal.,2024c) OOTB
✓ ✗ ✓
UFO(Zhangetal.,2024a) OOTB
✓ ✗ ✓
MMAC-Copilot(Songetal.,2024) OOTB
✓ ✗ ✓
Cradle(Tanetal.,2024) OOTB
✓ ✓ ✓
OSCAR OOTB
D GUI UNDERSTANDING BENCHMARK
BenchmarksandEvaluation. TotestifyOSCARwhetherpossessarobustunderstandingofvarious
GUIscenarios,includingdifferentOSplatformandmulti-windowinteractions,wefirstlyevaluation
OSCARonacomprehensiveGUIunderstandingbenchmark-GUI-World(Chenetal.,2024a). GUI-
World covering six GUI scenarios across Desktop OS and Smartphone OS and formulated as a
visualquestion-answeringtask. Specifically,Givenoneormultiplescreenshots,theagentoutputsa
summarizedcaption,layoutdescription,andGUIelements,orinfersrelationsbetweenscreenshots.
Following Chen et al. (2024a), we evaluate performance using automatic metrics for natural lan-
guagegeneration,suchasBERTScore(Zhangetal.,2019)andLLM-as-a-Judgemethodology(Liu
etal.,2023b;Zhengetal.,2023),oraccuracymetricformultiple-choicequestions.
Results. As shown in Table 7, we observe that: 1) OSCAR achieves the best GUI understanding
performanceacrossfivetypesofGUIdomains,exceptforwebsites,wherethestate-of-the-artagent
uses an advanced parser to extract HTML as input. When HTML text is provided to OSCAR as
additional input, it also demonstrates state-of-the-art performance in website GUI understanding.
This success can be attributed to OSCAR’s GUI-grounded observation, which we further analyze
inSection3.2. 2)Fine-tuningondomain-specificdataslightlycompromisesperformanceinmore
generaldomains. Forexample,theWebAgentachieves83oniOSGUI,significantlylowerthanits
18Preprint
Table7: QuantitativeresultsontheGUI-WorldbenchmarkcoveringsixtypesofGUIdomains.
Software Website XR Multi iOS Android
Model
MC Free MC Free MC Free MC Free MC Free MC Free
SeeAct 93.9 4.328 91.1 4.167 90.6 4.031 90.1 4.172 84.8 3.750 92.3 3.865
Auto-GUI 94.8 4.422 90.5 4.131 89.0 3.904 88.0 4.073 84.0 3.666 91.4 3.742
CogAgent 94.4 4.322 87.5 3.976 90.1 4.031 88.3 4.086 88.7 4.193 93.6 4.056
SeeClick 91.0 4.083 88.5 4.038 89.5 3.893 89.7 4.176 88.2 4.078 94.3 4.124
GUICourse 92.4 4.156 88.9 4.038 90.3 4.057 88.6 4.111 88.4 4.168 92.9 4.058
AppAgent 86.5 3.644 84.3 3.805 90.8 4.159 89.5 4.176 90.6 4.398 95.0 4.326
MobileAgent 88.6 3.822 86.0 3.877 90.6 4.047 89.7 4.199 91.1 4.482 94.9 4.230
M3A 88.1 3.799 84.1 3.803 92.1 4.270 94.1 4.456 89.6 4.278 93.5 4.127
WebAgent - - - - - - - - - - - -
FRIDAY 95.1 4.406 89.4 4.090 87.7 3.662 86.5 3.991 85.2 3.768 92.0 3.845
UFO 94.4 4.352 91.0 4.182 91.4 4.159 89.8 4.179 84.8 3.778 90.6 3.649
MMAC - - - - - - - - - - - -
OSCAR 96.4 4.509 89.2 4.035 94.4 4.551 95.5 4.527 92.7 4.585 96.4 4.524
OSCAR+HTML - - 92.2 4.235 - - - - - - - -
state-of-the-artperformanceof93onwebsiteGUI.3)Theaverageperformancedifferenceamong
theagentsismarginal,highlightingthestrongsingle-stepGUIunderstandingcapabilityofthebase
model,GPT-4o,usedinourexperiments.
E STATIC GUI NAVIGATION BENCHMARK
Benchmarks and Evaluation. We evaluate OSCAR on GUI navigation benchmarks involving
multi-step decision-making in pre-defined interaction episodes, which includes widely adopted
datasetssuchasMind2Web(Dengetal.,2023)(DesktopOS)andAITW(Rawlesetal.,2023)(Smart-
phoneOS).Thesebenchmarksconsistofhigh-leveltaskdescriptions, goldreferencesequencesof
actions,andcorrespondingobservationsinHTMLandscreenshots. Giventhetaskdescription,his-
toricalactions,andscreenstates,themodelpredictsthenextaction. BorrowingsettingfromCheng
etal.(2024);Rawlesetal.(2023),weevaluateperformanceusingtheStepSuccessRate(boththe
selectedelementandpredictedoperationarecorrect),TaskSuccessRate(allstepsarecorrect),and
a screen-wise action matching score (the number of correct steps divided by the total number of
steps). Notably a click action is correct if its touch and lift points are within 14% of the screen
distancefromthegoldactionoroccurwithinthesameboundingbox. Ascrollactionisconsidered
correctifitfollowsthesamescrollaxisasthegoldaction.
Results. Tables 8 and Table 9 quantitatively summarize the GUI navigation results on desktop
OSandsmartphoneOS,respectively. Weobservethat: 1)OSCARwithoutre-planningconsistently
achievesthebestperformanceonmulti-stepnavigationtasks,outperformingcompetitivebaselines
such as UFO and AUTO-GUI, particularly on cross-website and cross-domain data, demonstrat-
ingitsgeneralapplicability. 2)Fine-tuningonspecificGUIdataforsingle-steppredictionsmakes
limitedcontributionstomulti-stepdecision-making, asseenwithCogAgent, whichachievescom-
petitive results in GUI understanding (Table 7) but performs poorly in multi-step GUI navigation
tasks. Apossibleexplanationisthatdomain-specificfine-tuningincreasestheprobabilityofhallu-
cinated actions when intermediate feedback is not available from static environments (Qiao et al.,
2024).
F QUALITATIVE RESULTS
Figure8andFigure9presentqualitativeresultsofOSCAR’sonthedailyapplicationandprofessional
tool,respectively.
19Preprint
Table 8: Desktop OS GUI navigation results on the Mind2Web benchmark in terms of element
accuracy(Ele.Acc),OperationF1(Op.F1)andstepsuccessrate(StepSR).
Cross-Task Cross-Website Cross-Domain
Model
Ele.Acc Op.F1 StepSR Ele.Acc Op.F1 StepSR Ele.Acc Op.F1 StepSR
SeeAct 31.8 89.3 29.6 25.5 85.0 20.4 26.6 87.3 23.6
CogAgent 31.1 88.6 28.8 25.6 84.8 20.4 27.1 87.7 24.2
SeeClick 28.3 86.9 25.5 21.4 80.5 16.4 23.3 85.1 20.9
GUICourse 31.8 89.6 29.6 26.4 85.7 21.2 27.8 88.4 25.0
WebAgent - - - - - - - - -
FRIDAY 31.3 89.4 28.8 27.2 86.0 22.2 28.4 89.0 25.5
UFO 33.5 90.1 31.3 27.2 86.2 22.1 27.9 88.4 24.8
MMAC - - - - - - - - -
OSCARw/oRe-plan 35.5 92.4 33.9 29.6 88.3 24.5 29.8 90.0 26.5
Table9: SmartphoneOSGUInavigationresultsontheAITWbenchmarkintermsofactionmatch-
ingscore.
Model General Install GoogleApps Singlestep Webshopping
Auto-GUI 67.9 76.7 71.2 84.5 70.5
CogAgent 61.0 72.0 64.7 73.8 65.0
SeeClick 54.2 66.7 54.6 63.5 57.6
GUICourse 64.1 73.5 66.3 78.0 66.2
AppAgent 58.1 70.7 59.9 71.7 61.5
MobileAgent 59.5 71.4 61.4 73.9 63.8
M3A 65.4 75.7 68.0 82.9 68.0
OSCARw/oRe-plan 71.4 78.7 74.8 88.6 73.0
20Preprint
Figure8: Qualitativeresultswhenprocessinguserrequest“PleaseopenNotepad,createanewfile
named“draft.txt”,type“Thisisadraft.”,andsaveittotheDocumentsfolder.”.
Figure 9: Qualitative results when processing user request “Install the pylance extension in VS
Code.”.
21Preprint
OSCARSystemPrompt1/2
YouareaTaskPlannerwithadvancedtask-drivenre-planningcapabilities,designedtoefficientlycompleteuserobjectivesbydynamicallyadapting
basedonfeedbackandcontext.Yourcoreresponsibilitiesinvolvebreakingdowncomplextasks,executingactionsstep-by-step,andadjustingplanswhen
necessary.Youwillstoretasklistsandactionhistoriesincontextmemorytoensuretasksarecompletedeffectivelyandutilizetextualmemorytoupdate
andimproveyourfuturedecisions.
Inthissystem,task-drivenre-planningplaysacentralrole.Youfollowadetailedtwo-levelplanningapproachinspiredbyStandardOperating
Procedures(SOPs)andreal-timefeedbacktoensuretasksarecompletedsmoothly.Youractionsareguidedbybothcurrentobservationsand
storedmemory.
Task-DrivenRe-Planning:
• Level1: TaskDecompositionUsingSOPs: Decomposeuserinstructionsintohigh-leveltasksusingStandardOperatingProcedures
(SOPs)forclarityandstructure.SOPshelpbreakdowncomplexgoalsintomanageabletasksandavoidmissedstepsormisinterpretations.
• Level2:ActionExecutionandFeedbackIntegration:Foreachtask,generateactionsandexecutethemstep-by-step.Aftereachaction,
verifyifit’sontrackbycomparingtheactualresultswiththeexpectedones.Adjustplansdynamicallyifdeviationsoccurorbasedonuser
feedback.Storetheresultsandtrajectoryforfuturesteps.
Inputs:
• UserObjective:Theoverallgoaltheuserwishestoaccomplish.
• ContextMemory:
– OldTaskList:Previoustasksgeneratedandstored.
– HistoryActions:Sequenceofexecutedactions.
• ObservationInput:
– Rawcurrentscreenimage.
– Annotatedcurrentscreenwithredboundingboxes,taggedwiththeirrespectiveIDs.
• FeedbackInput:Feedbackrelatedtoprioractionsoruserinput.
• WindowTitle:Thenameofthecurrentlyactivewindow.
• AllOpenWindows:Listofallopenapplicationsandwindows.
• CandidateScreenElements:
– ID:Uniqueidentifierfortheelement.
– Content:Descriptionortextassociatedwiththeelement.
– Location:Normalizedlocationonthescreen.
Outputs:
• ScreenAnnotation:Summarizewhatisvisibleonthescreenandexplainhowitrelatestothetaskobjective.
• Task-DrivenRe-Planning:
– ReviewtheOldTaskListandHistoryActionstoreflectonprevioussteps.
– Determinewhetherre-planningisnecessarybasedoncurrentobservationsorfeedback.Adjustthetasklistaccordingly.
• NewTaskList:
– CreateanewtasklistusingSOPsifnoneexists.
– Updatethetasklistdynamicallybasedonfeedbackorobservations.
• Multi-StepPlanning:Breakdowntheuser’sobjectiveintosmaller,actionablesteps. Foreachstep,decidewhichscreenelementsto
interactwithandproviderationale.Adjusttheplanasneeded.
• DecisionGeneration:Chooseahigh-leveldecisionbasedonthetask’sstatus:
– COMMAND,DONE,orWAIT.
• ActionExecution:OutputaPythoncodeblocktoexecutethenextaction:
computer.mouse.move(id=14) # Move to the Start Menu button.
computer.mouse.single_click() # Click to open the Start Menu.
• TextualMemory:UpdateContextMemorywiththenewtasklistandactions.
Figure10: Part1/2ofsystempromptofOSCAR.
22Preprint
OSCARSystemPrompt2/2
ImportantReminders:
• EnsureclarityintaskdecompositionbyusingSOPs.
• Re-planbasedonfeedbackorunexpectedoutcomes.
• Storebothtasksandactionsinmemorytotrackprogressandavoidrepetition.
• Verifyprogressateachstage,executingactionsstep-by-step.
Task List Example:
[New Task List]
1. Open Notepad.
2. Type "This is a draft."
3. Save the document as "draft.txt."
[Current Task] 1/3 Open Notepad.
High-level Decision Example:
COMMAND # or DONE, WAIT
Action Example:
computer.mouse.move(id=14) # Move to the Start Menu button.
computer.mouse.single_click() # Click to open the Start Menu.
Memory Example:
[New Task List]
1. Navigate to Amazon.
2. Search for "laptop."
3. Add the first item to the cart.
[Current Task] 1/3 Navigate to Amazon.
Figure11: Part2/2ofsystempromptofOSCAR.
23