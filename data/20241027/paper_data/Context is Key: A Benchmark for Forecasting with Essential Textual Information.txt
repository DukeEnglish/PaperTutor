Preprint. Underreview.
CONTEXT IS KEY: A BENCHMARK FOR FORECASTING
WITH ESSENTIAL TEXTUAL INFORMATION
∗AndrewR.Williams1,2,3,∗ArjunAshok1,2,3,††E´tienneMarcotte1†ValentinaZantedeschi1,5,
JithendaraaSubramanian1,2,7,RolandRiachi2,JamesRequeima6,AlexandreLacoste1,
IrinaRish2,3,†NicolasChapados1,2,4†AlexandreDrouin1,2,5
1ServiceNowResearch,2Mila-Que´becAIInstitute,3Universite´ deMontre´al,
4PolytechniqueMontre´al,5Universite´ Laval,6UniversityofToronto,7McGillUniversity
ABSTRACT
Forecastingisacriticaltaskindecisionmakingacrossvariousdomains. While
numericaldataprovidesafoundation,itoftenlackscrucialcontextnecessaryfor
accuratepredictions. Humanforecastersfrequentlyrelyonadditionalinformation,
suchasbackgroundknowledgeorconstraints,whichcanbeefficientlycommu-
nicated through natural language. However, the ability of existing forecasting
modelstoeffectivelyintegratethistextualinformationremainsanopenquestion.
Toaddressthis,weintroduce“ContextisKey”(CiK),atimeseriesforecasting
benchmarkthatpairsnumericaldatawithdiversetypesofcarefullycraftedtextual
context, requiring models to integrate both modalities. We evaluate a range of
approaches,includingstatisticalmodels,timeseriesfoundationmodels,andLLM-
basedforecasters,andproposeasimpleyeteffectiveLLMpromptingmethodthat
outperformsallothertestedmethodsonourbenchmark. Ourexperimentshighlight
theimportanceofincorporatingcontextualinformation,demonstratesurprising
performancewhenusingLLM-basedforecastingmodels, andalsorevealsome
oftheircriticalshortcomings. Bypresentingthisbenchmark,weaimtoadvance
multimodalforecasting,promotingmodelsthatarebothaccurateandaccessibleto
decision-makerswithvariedtechnicalexpertise. Thebenchmarkcanbevisualized
athttps://servicenow.github.io/context-is-key-forecasting/v0/.
1 INTRODUCTION
The estimation of future conditions is the foundation of decision making (Hyndman & Athana-
sopoulos,2018)andintelligence(Wang,2019). Articulatedastime-seriesforecasting,thisproblem
pervadesmanydomainsofscienceandcommerce. Accurateforecastingreliesonseveralcrucial
decisions up to the practitioner (Hyndman & Athanasopoulos, 2018), in particular on: 1. Model
selection: Choosingtheappropriateforecastingmodelforagivenproblem,and2.Incorporating
priorinformation: Determiningwhatrelevantinformationshouldbeincludedinthemodelandhow
to effectively integrate it. This involves decisions about statistical priors, inductive biases in the
modelarchitecture,andotherformsofdomainknowledgeintegration. Traditionally,thesedecisions
haveheavilyreliedonexpertknowledgeandmanualintervention. However,recentadvancements
in machine learning have shown particular promise in democratizing time-series forecasting by
automatingbothmodelselectionandtheintegrationofpriorinformation.
Inthewakeofthefoundationmodelparadigmshift(Bommasanietal.,2021),severalworks(e.g.,
Liang et al. (2024); Chen et al. (2023); Lim & Zohren (2021)) have addressed automatic model
selectionbylearningflexible,adaptablemodelsthatcanbeappliedacrossvariousproblemscenarios.
Unfortunately,whencomparedtotraditionalstatisticalmethods,currentapproachesprovidedebatable
performanceimprovementswhilerequiringsignificantlymoreresources(Garza&Mergenthaler-
Canseco,2024). Moreover,thesemodelstypicallycastinputsandoutputsaspurelynumericaltime
series, which leaves no room for the context that human experts typically rely on to focus their
modellingefforts.
Analternativeclassofrecentapproaches(Jinetal.,2024;Liuetal.,2024b;Requeimaetal.,2024)
adaptlargelanguagemodels(LLMs)forforecastingandleveragenaturallanguageasanintuitive
∗Equalcontribution.Correspondenceto:{andrew.williams1, arjun.ashok}@servicenow.com
†Corecontributors.
1
4202
tcO
42
]GL.sc[
1v95981.0142:viXraPreprint. Underreview.
Context: “This series contains the power production of a photovoltaic power plant in the state of Alabama.
Over the previous 90 days, the maximum power production happened on average at 11:22:13.”
Context-unaware Context-aware
25
0
Figure1: AnexampletaskfromtheproposedContextisKey(CiK)benchmarkwithforecastsproducedbya
context-awaremodel.Left:Usingthenumericalhistoryaloneleadstopoorforecasts,asnothingindicatesa
reversiontozero.Right:Awarenessofthecontextsignificantlyimprovestheforecastsasitrevealsthatnopower
willbeproducedduringthenight(viadeductivereasoning)andenablesestimatingthepeakhourofproduction.
interfacetointegratesideinformation. Thesemethodsovercomeasignificantlimitationoftraditional
forecasting techniques by eliminating the need to manually encode priors or design specialized
models. Theyfurtherholdthepromisetocaptureabroaderrangeofpriorknowledgeandcontext,
potentiallyleadingtomorecomprehensiveandaccurateforecasts. Unfortunately,thereareasofyet
nosystematicevaluationsofthesemodels’abilitiestojointlyleveragehistoricalobservationsand
naturallanguageforforecasting. Whileseveralbenchmarksforcontext-aidedforecastinghavebeen
recentlyreleased(Zhangetal.,2023;Liuetal.,2024a;Xuetal.,2024;Emamietal.,2024;Merrill
etal.,2024),theircontextsarenotguaranteedtobeusefulforimprovingperformance. Hence,itis
stillanopenquestionastowhatextentexistingmethodscanimprovetheirpredictionsbyleveraging
crucially-relevantinformationprovidedintextualform.
Tothisend,weproposetheContextIsKey(CiK,pronouncedkick)benchmarkofforecastingtasks
withnumericalinput-outputpairsandessentialtextualcontext. Thebenchmarkisdesignedtoassess
a forecaster’s ability to utilize both the numerical data and key information contained within the
accompanyingtext, astheaccuracyoftheforecastsreliesheavilyoneffectivelyleveragingboth;
seeFig.1foranexamplewherecontextisimperativetoforecastaccuracy.
Ourcontributionsare:
• Wemanuallycurateandrelease71forecastingtasks(Sec.3)spanning7domains,whichcover
various kinds of contextual information (Sec. 3.2), and in addition to basic natural language-
processingandtime-seriesanalysis,requirevariouscapabilities(Sec.3.3).
• WeintroducetheRegionofInterestCRPSmetric(RCRPS)toevaluatecontext-aidedforecasting
performance(Sec.4),whichprioritizescontext-sensitivewindowsinthepredictionandaccounts
forconstraintsatisfaction.
• WeevaluatevariousmethodsonCiK(Sec.5),includingstatisticalmodels,timeseriesfoundation
modelsusingonlynumericaldata,andLLM-basedforecasterscapableofincorporatingcontext.
WeintroduceDirectPrompt,asimplepromptingmethodthatachievesthebestresultsonCiK.Our
analysisexploreskeyfactorssuchastheimpactofcontextconditioning,promptingtechniques,
modelcapabilities,anddiscussesfailuremodesofmodels.
2 PROBLEM SETTING
Context-AidedForecasting Thisworkaddressestheproblemofcontext-aidedforecasting,where
thegoalistoproducestatisticalforecastsbyincorporatingrelevantsideinformation(i.e.,context).Let
X =[X ,...,X ]representaseriesofrandomvariablescorrespondingtohistoricalobservations,
H 1 t
where each X ∈ X ⊆ R, and let X = [X ,...,X ] represent future observations. In the
τ F t+1 T
classicalstatisticalforecastingproblem,theobjectiveistoestimatethejointdistributionofthefuture
observationsgiventhehistoricaldata:
P(X |X ).
F H
Wefurtherassumeaccesstocontext,denotedC∈C,whichisadditionaldataofarbitrarynature(C)
thatcontainsinformationrelevantforpredictingX andcomplementarytothehistoryX . Thetask
F H
thenbecomesestimatingthedistribution:
P(X |X ,C).
F H
2
0


0:00 0


0:30 0


0:60 0


0:90 0


0:21 0


0:51 0


0:81 0
0:12 0


0:00 0


0:30 0


0:60 0


0:90 0


0:21 0


0:51 0


0:81 0
0:12Preprint. Underreview.
Retail
Energy
Mechanics
Synthetic 4.2% 8.5% 9.9% 50 Deductive Reasoning
4.2%
Economics Retrieval From Memory
4.2% Traffic 40
15.5% Mathematical Reasoning
30
Retrieval From Context
20 Instruction Following
36.6% 16.9% 10 Causal Reasoning
Climatology Analogical Reasoning
Public Safety
0 Causal History Future Intemporal Covariate 0 10 20 30 40
(a) Domain Distribution (b) Task Count for Context Sources (c) Task Count for Capabilities
Figure2:Overview:ThetasksintheCiKbenchmarkrelyonreal-worldnumericaldata,from7domains,as
wellassyntheticdata(left),coupledwithnaturallanguagecontextcapturingupto5differentaspectsofthe
dynamicalprocess(center),andrequireupto7non-trivialcapabilitiestounlockaccurateforecasts(right).
Crucially,werestrictourfocustorelevantcontext,whichwedefineascontextthatdoesnotdegrade
thepredictionoffuturetimesteps. Formally,forx ∼ X | X ,C,givensomelossfunctionL
F F H
assessingapredictivedistributionoverX againstarealizationx ,L:P(X )×x →R,weare
F F F F
interestedinsystemswhere,inexpectation,forecaststhatleveragecontextperformbetter:1
E L(P(X |X ,C),x )≤ E L(P(X |X ),x ).
F H F F H F
xF xF
Furthermore,althoughthenatureofthecontextC canvarywidely,wespecificallyconcentrateon
contextcommunicatedthroughnaturallanguage.
3 CONTEXT IS KEY: A NATURAL LANGUAGE CONTEXT-AIDED
FORECASTING BENCHMARK
WepresenttheContextisKey(CiK)benchmark,acollectionofprobabilisticforecastingtasksthat
cannotbesolvedwithoutintegratingnaturallanguagecontextualinformationwithnumericaldata.
CiKconsistsof71distincttasksspanningsevenapplicationdomains(Sec.3.1)andthatcanbeinstan-
tiatedindifferentways,e.g.,bychangingtargettimeseriesorbyselectingdifferenttimewindows.
Thesetasksencompassdiversetypesofcontextualinformation(e.g.,pasteventsandknowncausalre-
lationships;Sec.3.2),andaredesignedsuchthatvariouscapabilities(e.g.,causalreasoning;Sec.3.3)
arerequiredtofullyleveragethecontextandunlockaccurateforecasts(seeFig.2foranoverview).
AnexampletaskisillustratedinFig.1andothersaregiveninAppendixB.Thecompletesetof
taskscanbeexploredathttps://servicenow.github.io/context-is-key-forecasting/v0/and
thesourcecodeisavailableathttps://github.com/ServiceNow/context-is-key-forecasting.
3.1 DOMAINSANDNUMERICALDATASOURCES
The vast majority (95%) of tasks in CiK draw numerical data from 2,644 real-world time series
acquiredfrompublicsources. Theseseriescoverarangeofdomains: Climatology(solarirradiance
and cloud coverage (Sengupta et al., 2018)); Economics (unemployment rates across states and
counties (U.S. Bureau of Labor Statistics, 2024)); Energy (electricity consumption and produc-
tion(Godahewaetal.,2021));Mechanics(experimentalpropertiesofphysicalsystems(Gamella
etal.,2024));PublicSafety(firedepartmentinterventioncounts(VilledeMontre´al,2020));Trans-
portation(highwaysegmentoccupancyratesandaveragespeeds(Chenetal.,2001));andRetail(cash
withdrawalsfromvariousATMs(Godahewaetal.,2021)). Theremaining5%oftasksusesimulated
datafromdynamicalsystemscraftedspecificallyforthetasks. Overall,thetimeseriesinCiKexhibit
diversesamplingfrequencies,withobservationsrangingfromevery10minutestomonthlyintervals.
AdditionaldetailsondatasourcescanbefoundinAppendixA.1.
Memorization mitigation: Using publicly available real-world data introduces the risk that
pretrained LLMs and time-series foundation models may have memorized portions of the data,
1Usingthenegativelog-probabilityasthelossfunctionwouldmakethisstatementequivalentto:theentropy
ofP(X |X ,C)mustbelowerthanthecrossentropyofP(X |X ,C)andP(X |X ).
F H F H F H
3Preprint. Underreview.
2 This is the num inb e thr eo f M tr ea rs ch ie f ri -r He oin cc hi ed le an gt as - Mre as ip so on nd ne ed u vto e b by o rM ouo gn htreal firefighters 5
A policy will be implemented
from July to August
this year that is expected to
prevent all field fires
3
In other years, the yearly average 6
number of incidents was 99 and 1
the month with the most incidents However, field fires do
was August. not cause trash fires
Field
fires
z
4 This series bte yn fd irs e ft io g hc to e- ro sc ic nu tr h w e i sth a mfie el d b ofir re os u gre hsponded to Trash
fires
Figure3:IllustrationofaCiKtaskannotatedwithtypesofnaturallanguagecontext:⃝1 Theshortnumerical
historyismisleading,suggestinganincreasingtrend.However,contextualinformationcompensatesandenables
accurateforecasts:⃝2 Theintemporalinformation(c )revealsthenatureoftheseries,implyingaseasonal
I
patternwithgreaterprevalenceinthesummermonthsduetoweather. ⃝3 Thehistoricalinformation(c )
H
complementstheshorthistorybyprovidinghigh-levelstatisticsonpastvalues.⃝4 Thecovariateinformation
(c )revealsanassociationwithanotherquantity:fieldfires,reinforcingpotentialseasonalbehavior.⃝5 In
cov
addition,thefutureinformation(c )revealsafutureefforttoreducefieldfires.Couldthisimpactfuturevalues
F
ofthetargetseries?⃝6 No,thecausalinformation(c )providestheanswer.
causal
potentiallyinflatingevaluationperformance. Tomitigatethis,weemployseveralstrategies. First,
weprioritizelivedatasourcesthatarecontinuouslyupdated,suchasChenetal.(2001)andVillede
Montre´al(2020),ensuringthedataiscollectedafterthetrainingcut-offdatesofthemodelsbeing
evaluated. Second,whereapplicable,weusederivedseriesthatarenotdirectlyavailableintheraw
data, such as converting an incident log into time series (Ville de Montre´al, 2020). Finally, as a
lastresort,weapplyminortransformations,suchasaddingnoiseorshiftingtimestamps,butuse
thesesparinglytoavoidmisalignmentbetweencommon-senseknowledge(e.g.,holidaydates)and
the numerical data. Details on the mitigation methods used for each data source are provided in
AppendixA.1.
3.2 NATURALLANGUAGECONTEXT
Foreachtaskinthebenchmark,wejointlysamplenumericaldatafromoneoftheseriesdescribedin
Sec.3.1andthencarefullycraftthenaturallanguagecontextnecessarytounlockaccurateforecasts.
Insomecases,thiscontextispurelydescriptive,providinginformationaboutthegeneralnatureofthe
targetvariableanditshistoricalbehavior,asseeninthetaskillustratedinFig.1. Inothercases,the
rawnumericaldataisadjustedtoreflecttheinfluenceofthecontext. Forexample,inonetaskbased
ondatafromGodahewaetal.(2021),anATMisexpectedtobeinaccessibleduringaspecificperiod,
leading to zero withdrawals (visualized in Appendix B.3). In another task, electricity demand is
projectedtosurgeduetoanincomingweatherevent(visualizedinAppendixB.2). Insuchcases,we
modifytheseriestoincorporatethesepatternsandincludesuchinformationintothecorresponding
context.
Overall, we include diverse forms of natural language context, capturing various aspects of the
processunderlyingthetimeseriesandrevealingcomplementaryknowledgethatcouldbeprovided
byahumanexpertoranexternalinformationsource. Thetypesofcontextaredescribedbelowand
exemplifiedinthetaskillustratedinFig.3. SeveraladditionalexamplesaregiveninAppendixB.
Intemporalinformation(c ) Informationabouttheprocessthatremainsinvariantintime. For
I
example,adescriptionoftheprocessandthenatureofthetargetvariable,asinFig.3(point⃝2 ),
patternsthatcannotbeinferredfromtheavailablenumericaldata(e.g.,long-periodseasonalities),or
constraintsonvalues(e.g.,positivity).
Historicalinformation(c )Informationaboutthepastbehavioroftheseriesthatisnotreflectedin
H
theavailablenumericalhistory. Forexample,statisticsonpastvaluesoftheseries,asinFig.3(point
⃝3 ),oranexplanationforspuriouspatternstobedisregardedatinference(e.g.,periodicanomalies
duetosensormaintenance).
4Preprint. Underreview.
Covariate information (c ) Information about any additional variables that are statistically
cov
associated with the variable of interest and that may help prediction. For instance, a variable
correlatedwiththetargetvalues(asinFig.3point⃝4 ).
Futureinformation(c ) Informationrelevanttothefuturebehaviorofthetimeseries. Forexample,
F
ascenariotobesimulated(asinFig.3point⃝5 )orexpectedeventsalongwithanyentailedconstraints
(e.g.,aninventoryshortagerestrictingfuturesalesamounts).
Causal information (c ) Information about causal relationships between covariates and the
causal
targetvariable. Forexample,ifthecovariatesareknowntocauseorareconfoundedwiththetarget
variable(asinFig.3point⃝6 ).
Finally,wenotethat,incontrastwiththeworkofZhangetal.(2023);Merrilletal.(2024);Liuetal.
(2024a);Emamietal.(2024)whichrelyonLLMsorscrapednewsarticles,allcontextualinformation
anddatatransformationsintheCiKbenchmarkaremanuallycraftedtoensurequalityandrelevance.
3.3 MODELCAPABILITIES
Inadditiontoforecastingandnaturallanguageunderstanding,alltasksaredesignedsuchthatfully
utilizingthecontextualinformationrequiresarangeofcapabilities,includinginstructionfollowing,
variousformsofreasoning,andretrieval.
For example, to solve the task in Fig. 3 , the model could retrieve from memory that Montreal
experiencessnowfallandcoldweatherduringthewintermonths. Itcouldtheninferthattrashfires
arelesslikelytooccurduringthisperiodthroughdeductivereasoning. Thischainofthoughtreveals
aseasonalpatternthatisnotapparentintheshortnumericalhistory. Additionally,throughcausal
reasoning, it is apparent that, despite a strong association between field fires and trash fires, the
interventiondescribedin⃝5 isunlikelytoreducethefrequencyofthelatter. Failuretorecognizethis
distinctionwouldleadtoinaccurateforecasts.
A list of all capabilities with definitions is available in Appendix A.4 and the capa-
bilities required to solve each task are documented at https://servicenow.github.io/
context-is-key-forecasting/v0/. The distributions of tasks per capability and context type
are shown in Fig. 2, while the distribution of lengths of the numerical historical data, prediction
horizonsandnaturallanguagecontextcanbefoundinAppendixA.5. Multipleexampletasksfrom
CiKareprovidedinAppendixB,alongwithanexplanationoftheirsourcesofnaturallanguage
contextandthecapabilitiesrequiredtosolvethem.
4 REGION OF INTEREST CONTINUOUS RANKED PROBABILITY SCORE
Alongsidethetasks,weintroducetheRegionofInterestCRPS(RCRPS),anovelproperscoring
ruledesignedspecificallyforcontext-aidedprobabilisticforecasting. Thisnewscoringruleisan
extensionoftheContinuousRankedProbabilityScore(CRPS;Gneiting&Raftery(2007)),aproper
scoringrulethatprovidesacomprehensiveassessmentofforecastqualitybyevaluatingtheentire
predictivedistributionratherthanfocusingsolelyonsummarystatistics. Importantly,sinceitisbased
ontheCRPS,theRCRPScanbecalculatedusingonlysamplesfromthepredictivedistribution,and
socanbeusedevenincaseswhereclosed-formdistributionsareunavailable. TheRCRPSextends
theCRPSviatwokeycomponents: aregionofinterestandameasureofconstraintsatisfaction. This
allowsassessingbothforecastaccuracyandtheintegrationofcontextualinformation.
Region of interest (RoI): The score reweighs a strict subset of time steps, denoted by I ⊆
[t+1,...,T], whose values are heavily informed by the context. For example, in the ATM task
describedinSec.3.2(visualizedinAppendixB.3),thiswouldcorrespondtothetimeintervalsduring
whichtheATMisexpectedtobeunavailable. Inothertasks,suchasthoseinFigs.1and3,where
thecontextinformsthevalueofallfuturetimepoints,wesettheRoItoanemptyset,essentially
weightingalltimestepsequally(forreadability,wereportthedefinitionofRCRPSforthisspecial
caseinAppendixE).
Constraintsatisfaction: Thescorepenalizesviolationsofconstraints,whetherexplicitlyorim-
plicitlyincludedinthecontext,bymeasuringatask-specificfunction,denotedbyv ,whosevalue
C
iszeroforanytrajectorythatsatisfiestheconstraintsand>0foranytrajectorythatviolatesthem.
ConcreteexamplesaregiveninAppendixE.4. Fortaskswhosecontextdoesnotimplyconstraints,
weusev (·)≡0.
C
5Preprint. Underreview.
Given an inferred forecast distribution X(cid:101)F and a ground truth x F, the scoring rule is defined as:
(cid:34) (cid:35)
1 (cid:88) (cid:16) (cid:17) 1 (cid:88) (cid:16) (cid:17) (cid:16) (cid:17)
RCRPS(X(cid:101)F,x F):=α· 2|I|· CRPS X(cid:101)i,x
i
+ 2|¬I|· CRPS X(cid:101)i,x
i
+β· CRPS v C(X(cid:101)F),0 ,
i∈I i∈¬I
wherethetermsrespectivelyaccountforCRPSinsidetheRoI,CRPSoutsideoftheRoI,andconstraint
satisfaction. Wenotethatthelasttermisinspiredbythethreshold-weightedCRPSofGneiting&
Ranjan(2011)andthatitvanisheswhenallconstraintsaresatisfied. Theαtermisatask-dependent
scalingfactorthatisusedtoensurethatscorevaluesfortaskswithnumericaldataofvariousscales
canbeaggregated;itscalculationisdescribedinAppendixE.1. Finally,β isascalingfactorthat
controls the impact of constraint violation on the score; we use β = 10 in our experiments. For
additionaldetailsanddiscussionontheRCRPRproperness,wereferthereadertoAppendixE.
5 EXPERIMENTS AND RESULTS
Inthissection,wedefineourevaluationprotocol(Sec.5.1)andoutlinethebaselinemodelsevaluated
on CiK (Sec. 5.2). We then present results on the benchmark (Sec. 5.3), along with an analysis
offactorsaffectingmodelperformance. Finally, welookatareasforimprovementbyanalyzing
forecastingerrors(Sec.5.4)andinferencecost(Sec.5.4).
5.1 EVALUATIONPROTOCOL
EachtaskinCiKhasmanyuniquespecifications,i.e. instancesarisingfromthevarioustimeseries
andwindowsintheassociatednumericaldata,aswellasminorvariationsinnaturallanguagecontext.
Inordertomaketheevaluationreproducibleandaffordable,wedeterministicallyselect5instances
ofeachtaskforevaluation. Foreachinstance,wegenerate25independentsamplesfromeachmodel
for evaluation. Since many of the tasks in the benchmark share similarities due to having been
createdfromthesamedatasourcesorusingvariantsofthesamecontext,weidentifytheseclusters
ofsimilartasks,anddesignaweightingschemesuchthateachclusterhasequaltotalweightinour
aggregatescore(seeAppendixA.2formoredetails). Finally,topreventtheaggregatescoresfrom
beingdominatedbyrareinstanceswheresomemodelsgiveforecastswhichareordersofmagnitudes
awayfromthegroundtruth,weintroduceanupperboundof5totheRCPRSvalueforeachinstance,
whichintuitivelyrepresentsthevalueaforecastwouldgetifthedistancebetweentheforecastand
theground-truthwas5timesbiggerthantherangeoftheground-truthoftheinstance.
5.2 BASELINES
Weevaluateawidevarietyofmodelsrangingfrommethodsbasedonlanguagemodelstostate-of-
the-artnumericaltimeseriesfoundationmodelsandclassicalstatisticalforecastingmethods. Since
CiKismeanttobeanevaluationbenchmarkandhencedoesnothaveacorrespondingtrainingset,
weonlydirectlyevaluatemodelsthatsupportzero-shotinference(suchasLLMsandtimeseries
foundationmodels),andthosewhichcanbefitdirectlytothefewhistoricaldatapointsofeachtask
instanceevaluated,suchastraditionalstatisticalmodels. Weoutlinethesemethodsbelowandrefer
thereadertoAppendixDforadditionaldetails.
LLM-basedForecasters: Weconsidertwoprompt-basedapproaches: LLM-processes(LLMP;
Requeimaetal.(2024))andasimpleapproachwhichwepropose,called“DirectPrompt”,wherewe
instructthemodeltodirectlyoutputaforecastasastructuredoutput,ratherthanpromptingitmultiple
timesasin(Requeimaetal.,2024)(describedindetailinAppendixD.1). Foreachofthese, we
evaluateavarietyofLLMswithdiversearchitecturesandsizes,suchasGPT-4o(Achiametal.,2023),
Mixtral-8x7B(Jiangetal.,2024)),Llama-3-8b(Dubeyetal.,2024),andLlama-3.1-405b(Dubey
etal.,2024). 2 Next,weevaluatemultimodalforecastingmodels,UniTime(Liuetal.,2024b)and
Time-LLM(ETTh1)Jinetal.(2024)eachtrainedaccordingtotheirrespectiveauthors’guidelines
(detailedinAppendixD.3). Foralloftheseapproaches, inferenceisperformedzero-shotonthe
benchmarkandwecomparetheirperformancewithandwithoutthenaturallanguagecontext.
Quantitative Forecasting Models: To contrast the performance of LLM-based forecasters, we
alsoevaluateanumberofmodelsthatareonlycapableofprocessingnumericaldata(nonatural
language). Thisincludesexponentialsmoothing(GardnerJr.,1985),ETS(Hyndmanetal.,2008),
andARIMA(Boxetal.,2015),threesimple,buttime-testedstatisticalapproaches,aswellasfour
2ForLLMP,wedonotconsiderLlama-3.1-405bandGPTmodelsasLLMPrequiresloadingmodelweights
intomemory,whichisinfeasibleduetoresourcelimitationsandconfidentiality,respectively.
6Preprint. Underreview.
Table1: ResultsontheCiKbenchmark. Startingfromtheleft,thefirstcolumnshowstheRCRPSaveraged
overalltasks.Thesecondcolumnshowstherankofeachmethodw.r.t.otherbaselines,averagedoveralltasks.
TheremainingcolumnsshowtheaverageRCRPSstratifiedbymodelcapabilities(Sec.3.3).Allaveragesare
weightedaccordingtotheschemedescribedinSec.5.1andaccompaniedbystandarderrors.Lowerisbetter
andthebestaveragesareinbold.Anasterisk(*)denotesmodelsthatdonotusenaturallanguagecontext.
Average Average Instruction Retrieval Reasoning
Model RCRPS Rank Following FromContext FromMemory Deductive Analogical Mathematical Causal
DirectPrompt(ours)
Llama-3.1-405B-Inst 0.159±0.008 4.469±0.192 0.140±0.013 0.109±0.002 0.191±0.006 0.133±0.001 0.167±0.008 0.316±0.028 0.376±0.039
Llama-3-70B-Inst 0.518±0.030 10.406±0.190 0.504±0.038 0.371±0.071 0.523±0.048 0.461±0.048 0.694±0.117 0.573±0.044 0.643±0.049
Llama-3-8B-Inst 1.647±0.069 15.130±0.171 1.604±0.131 0.199±0.010 1.568±0.067 2.133±0.082 1.555±0.008 1.589±0.177 1.840±0.238
Mixtral-8x7B-Inst 1.061±0.058 13.381±0.230 0.857±0.077 0.296±0.049 1.077±0.078 1.352±0.117 1.145±0.144 1.000±0.086 1.096±0.106
GPT-4o 0.276±0.010 4.368±0.149 0.180±0.004 0.087±0.003 0.519±0.029 0.113±0.006 0.447±0.029 0.590±0.033 0.769±0.046
GPT-4o-mini 0.353±0.022 8.930±0.177 0.296±0.043 0.419±0.014 0.471±0.012 0.218±0.005 1.024±0.033 0.475±0.080 0.578±0.112
LLMP
Llama-3-70B-Inst 0.550±0.013 8.038±0.205 0.645±0.018 0.284±0.015 0.392±0.014 0.519±0.026 0.312±0.019 0.453±0.020 0.495±0.028
Llama-3-70B 0.237±0.006 6.560±0.254 0.310±0.011 0.126±0.009 0.217±0.007 0.134±0.003 0.241±0.019 0.294±0.008 0.329±0.010
Llama-3-8B-Inst 0.484±0.010 9.457±0.166 0.345±0.002 0.138±0.004 0.910±0.030 0.242±0.008 1.278±0.069 0.617±0.022 0.787±0.030
Llama-3-8B 0.313±0.023 9.499±0.323 0.404±0.043 0.124±0.003 0.280±0.026 0.179±0.014 0.267±0.015 0.530±0.084 0.661±0.117
Mixtral-8x7B-Inst 0.264±0.004 8.496±0.256 0.344±0.004 0.127±0.003 0.224±0.005 0.179±0.010 0.173±0.009 0.348±0.005 0.405±0.007
Mixtral-8x7B 0.262±0.008 8.619±0.208 0.348±0.012 0.146±0.022 0.230±0.016 0.153±0.002 0.230±0.041 0.354±0.007 0.414±0.009
MultimodalModels
UniTime 0.371±0.002 13.495±0.091 0.271±0.003 0.179±0.001 0.318±0.001 0.510±0.003 0.333±0.001 0.332±0.001 0.384±0.001
Time-LLM(ETTh1) 0.476±0.001 16.662±0.075 0.448±0.002 0.192±0.000 0.373±0.000 0.538±0.003 0.397±0.001 0.382±0.001 0.440±0.001
TSFoundationModels*
Lag-Llama 0.329±0.004 13.157±0.235 0.355±0.007 0.181±0.003 0.324±0.003 0.272±0.006 0.342±0.006 0.386±0.009 0.449±0.012
Chronos 0.326±0.002 11.962±0.139 0.385±0.002 0.138±0.002 0.288±0.002 0.249±0.002 0.295±0.003 0.362±0.003 0.417±0.004
TimeGEN 0.354±0.000 14.345±0.090 0.402±0.000 0.176±0.000 0.308±0.000 0.279±0.000 0.324±0.000 0.377±0.000 0.431±0.000
Moirai 0.520±0.006 12.458±0.266 0.414±0.004 0.155±0.004 0.260±0.003 0.751±0.015 0.276±0.008 0.337±0.007 0.397±0.010
StatisticalModels*
ARIMA 0.480±0.006 12.320±0.180 0.399±0.006 0.160±0.002 0.517±0.012 0.522±0.013 0.706±0.026 0.354±0.007 0.403±0.010
ETS 0.522±0.009 14.310±0.203 0.407±0.009 0.228±0.010 0.682±0.018 0.571±0.019 0.855±0.035 0.453±0.012 0.479±0.015
Exp-Smoothing 0.603±0.013 14.936±0.132 0.571±0.021 0.334±0.013 0.743±0.018 0.557±0.019 0.899±0.035 0.673±0.038 0.782±0.053
state-of-the-arttimeseriesfoundationmodels: Lag-Llama(Rasuletal.,2023),Chronos(Ansarietal.,
2024)3 ,Moirai(Wooetal.,2024),andTimeGEN(Garzaetal.,2023). Wenotethatexponential
smoothing,ETS,andARIMAarefittedtoeachtaskinstance’snumericalhistory,whilethefoundation
modelsareevaluatedzero-shot.
5.3 RESULTSONTHEBENCHMARK
OurmainresultsareshowninTab.1. Atahighlevel,weobservethatthebest-performingbaselines
combine pretrained LLMs with prompting strategies like Direct Prompt and LLMP, with a bias
towardthelargestmodels. IntermsofRCRPS,Llama-3.1-405B-Inst(DirectPrompt)significantly
outperforms all of its counterparts. As can be seen in Fig. 4, it achieves this only with context.
GPT-4o(DirectPrompt)performsworsewithrespecttoRCRPS,butcomparesfavorablyinterms
ofaveragerank,takingthebestaveragerankbyasmallmargin. Thisdiscrepancyisduetostrong
failuresonsomeofthetasks,whichwediscussinSec.5.4. OthermodelslikeLlama-3-70B(LLMP),
Mixtral-8x7B-Inst(LLMP),Mixtral-8x7B(LLMP),andLlama-3-8B(LLMP)areonparwithGPT-4o
(DirectPrompt)intermsofRCRPS.Interestingly,allofthesebaselinesoutperformUniTimeand
Time-LLM,whichalsorelyonLLMs(GPT-2&LLaMA-7B).WediscussthisgapinAppendixD.3.
Finally,asemphasizedinFig.5,weobservethatthebest-performingLLMbaselinessignificantly
outperformpurelyquantitativemodels. Inwhatfollows,weexaminevariousaspectsoftheseresults.
Explainingtheperformanceof
With Context
LLM-basedapproaches No Context
1.5 Direct Prompt
LLM Process
The stronger performance of LLM base- Multimodal 1.0
linescouldbeduetotwofactors: (i)prop-
erly leveraging the natural language con- 0.5
text and (ii) being more proficient at nu-
m d h bai ase n ser edi n lc ,t ia a F nl n eigg sf .o le m4re t s ahc h kea o eis wrt ui c sn sog c en l. et or ai fW rb tu ee hvt ei it o dh n ceu os ns . ncO tea et n t xt he tt am h t te op mt o io mnt so e t
-
Llama-30 .. 1-0 405B-Inst G GP LlPT- aT-4 m4o ao -- M3 im -i x7 tn ri 0 aB l- -I L8 ln x ast 7 mB a- -I 3n -st 8 LlB a-I mns at - M3 i- Mx i7 t xr0 ta rB l- al8 -x 87 xB 7B Ll-I La ln as mt a m- La l3 -- a38 - mB 8 aB -- 3I -n 7st 0 TB i-I mn es U -t n Li LTi M m (e ETTh1)
provetheirforecasts. Forexample,Llama-
Figure4: Performancewithandwithoutcontext(loweris
3.1-405B-Inst(DirectPrompt)improvesby
better).Fullbarsshowperformancewithcontext;stripedbars
67.1%withcontext. Thisisreflectedinthe
showperformancewithout.Allmodelsimprovewithcontext,
qualityoftheforecasts,whereweobserve
exceptDPMixtral-8x7B-Inst,LLMPLlama-3-70B-Instand
clearimprovementsespeciallyinregionsof Time-LLM.Llama-3.1-405B-Instimprovessignificantlywith
interestandimprovedsatisfactionofcon- context,exhibitingthebestaggregateRCRPS.
3ResultsreportedhereareonChronos-LargeandMoirai-Large.ResultsonallversionsareinApp.C.2.
7
SPRCR
etagerggA
)retteB
si
rewoL(Preprint. Underreview.
Beats all Beats 5 or 6 Beats 3 or 4 Beats 1 or 2 Beats none
With context Without context
DP Llama-3.1-405B-Inst
DP GPT-4o
LLMP Llama3-70B
LLMP Llama3-70B-Inst
LLMP Llama3-8B-Inst
LLMP Mixtral-8x7B-Inst
DP Llama-70B-Inst
DP GPT-4o-mini
LLMP Llama3-8B
LLMP Mixtral-8x7B
DP Mixtral-8x7B-Inst
DP Llama-8B-Inst
UniTime
TimeLLM (ETTh1)
0% 25% 50% 75% 100% 0% 25% 50% 75% 100%
Figure5:ProportionoftasksforwhichLLM-basedbaselinesoutperformthe7quantitativeforecastingbaselines
(seeSec.5.2).AbaselineisconsideredtooutperformanotheronataskifitsmeanRCPRSisloweronsaidtask.
Resultsareshownforvariantsthatuse(left)anddonotuse(right)thenaturallanguagecontext.Afullgreen
barwouldindicatethatthebaselineisbetteronalltasks,whereasafullredbarwouldindicatethatitisworse
everywhere.AveragesareweightedaccordingtoSec.5.1.
straints(seeAppendixC.4forexamples). Othermodelsshowmuchslighterimprovementsand,in
threecases,evenadegradationinperformance. Thesecanbeexplainedeitherbythecontextbeing
ignored,orbysignificantfailuresinusingcontext,impoverishingoverallperformance(seeSec.5.4).
Ontheotherhand,Fig.5(right)showsthatsomeLLMbaselinesaresurprisinglygoodforecasters
whencomparedtoquantitativeforecastingmodelsinano-contextsetting. Forinstance,multiple
Llama-3-basedmodelsusedwiththeLLMPstrategyoutperformatleast5ofthequantitativebaselines
onthemajorityoftasks. ThisisfurthersubstantiatedbyresultsinAppendixC.2. Incontrast,other
baselines, including the best models Llama-3.1-405B-Inst (Direct Prompt) and GPT-4o (Direct
Prompt),showmuchweakernumericalforecastingabilitieswithoutcontext,suggestingthattheir
performanceismostlyduetoleveragingthecontext.
ComparingtheLLMPandDirectPromptingStrategies
Clear patterns emerge when comparing these strategies. First, as shown in Fig. 5 (right), LLMP
baselinesexhibitstrongernumericalforecastingperformancewithoutcontextthanDirectPrompt
baselines. This advantage likely stems from LLMP’s closer alignment with the forecasting task:
LLMPsimplypromptstheLLMtoautoregressivelypredictthenextvalueinthetimeseries–atask
wellsuitedfornon-instructiontunedLLMs. ThiscontrastswithDirectPromptingwhichrequires
outputforecaststobestructured,complicatingtheoveralltask.
Thislineofreasoningleadsustooursecondobservation;asreflectedinTab.1andFig.5,instruction
tuningappearstodegradeLLMPperformance,withLlama-3modelsshowingatwofolddecrease
inperformanceaftertuning—abehaviorpreviouslyobservedbyGruveretal.(2024). Interestingly,
instructiontuningdoesnotdegradeMixtral-8x7Bperformance. Finally,whileinstructiontuning
generallyharmsLLMP,itisessentialformodelsusedwiththeDirectPromptstrategy. Again,Direct
Promptrequiresforecaststobeproducedinaspecificstructure,askillthatbasemodelstypically
honeduringpost-training(seeAppendixD.1.1fordetails).
NoBaselineExcelsAcrossAllCapabilities
BasedontheresultsinTab.1,itisevidentthatsomemodelspossessthenecessarycapabilitiesto
effectivelyutilizethecontextualinformationprovided. However,weobservethatnosinglemodelis
thebestacrossallcapabilities. Llama-3.1-405B-Inst(DirectPrompt),ouroveralltop-performing
baseline, outperforms its counterparts in only 4 out of 7 capabilities. This finding indicates that
the benchmark remains unsolved, leaving significant room for advancements from the research
community.
8Preprint. Underreview.
103
Direct Prompt
Direct Prompt LLM Process
LLM Process
1 Q Mu ua ltn imtit oa dt aiv le 102 Q Mu ua ltn imtit oa dt aiv le
0.6 E Ex Tp S-Smoothing 101
0.4 Arima
Mixtral-8x7B 100
0.3 Lag-Llama Chronos Llama-3-8B
0.2 Llama-3-70B 101
0.15 Llama-3.1-405B-Inst
0 10 106 107 108 109 1010 1011
Parameter Count Llam Mia- xt3 r- a8 l-B- 8I x Ln l7s at B-I mn a-Gs Lt l3P - aT- 7 mG4 0 aPo B -- TI - 3.n 4 1s Lo - lt - 4 a0mi m5n ai B -- 3I -n M8 L is lt B xa- tI rmn als a -t - 83- x M8 7 iB xB t-I r L Ln a l lls a- at 8 m mx a a7 - -B 3 3- - T7 7 i0 0 mB B e-I -U Ln Lns i Et MT xi ( p-m E Se TT mh o1 o) thing M To iir mai eG CE hrN onos ET AS LR aI g-M LA lama
Figure6:Overviewofinferencecosts.(Left)ComparisonofaverageRCRPS(perTab.1),vs.theparameter
countofeachbaselinemodel(lowerisbetterforboth).TheGPTfamily,aswellasTimeGEN,areleftoutasthere
isnoinformationonthemaboutparametercount.ThedashedlineillustratestheParetofront:modelsabove
andtotherightofthisfrontaredominated.Quantitativeforecastersdominatethelow-parameterregime,while
LLM-basedmethodssuchasLLMPLlama-3-8Bor3-70BandDP3.1-405B-Instoffersuperiorperformancefor
ahigherparametercount.(Right)Inferencetimeinseconds,forallbaselines,averagedoveralltasks.Several
quantitativemethodsaremuchfasteronaveragethanLLM-basedmethods. However,therearesignificant
differencesininferencetimebetweentheLLM-basedforecasters: fortheLlamamodels,LLMProcesstakes
aboutanorderofmagnitudemoretimetorunonaveragethanDirectPrompt.
5.4 ERRORANALYSIS
Wefindthatmodelsoccasionallyreturnforecaststhatmissthegroundtruthbyalargemargin. A
significantfailuredenotesaforecastthatoverorundershootsbyatleastfivetimestherangeofthe
groundtruth;atthatpoint,wecliptheRCRPSto5asexplainedinSec.5.1. Despitethiscap,such
unpredictablebehaviourimpactstheresultsofTab.1: GPT-4owithDirectPrompt,whileemergingas
atop-performerinmosttasks(asreflectedinitsaveragerank),providessignificantlyhigheraggregate
RCRPSthanmodelsrankedworse,suchasMixtral-8x7BwithLLMP.Asanexample,DirectPrompt
withGPT-4ofailssignificantlyinataskwithacontextinvolvingscientificnotation(seeFig.16;more
examplescanbefoundinAppendixC.5). Thesefindingsunderscoretheneedforfutureworkto
developmorerobustmodelsthatcanhandlecontexteffectivelywhileavoidingsignificantfailures.
5.5 INFERENCECOST
Akeypracticalaspectforforecastingapplicationsistheinferencetimeofmodelsandtheirassociated
cost. Fig.6(left)showsthat,whileLlama-3.1-405B-InstructhasthebestRCRPS,itcomesatthe
costofasignificantlyhigherparametercountthanthequantitativeforecasters. Thisemphasizesthat,
whileLLMscanbepowerfulcontext-awareforecasters,theycomewithasteepcomputationalcost,
highlightingtheneedforefficientmodelsthatbalancebothaccuracyandresourcedemands. Ofnote
isalsothatmanyLLMbaselinesareParetodominatedbyquantitativeforecasterssuchasLag-Llama
andChronos. Thissuggeststhattheabilitytoingesttextisnotenoughandthatacarefulchoiceof
LLMandpromptingstrategyiscrucialforParetoefficiency.
Fig.6(right)emphasizesthedisparityininferencetimebetweenLLMsandquantitativemodels.
LLMstakesignificantlylongertomakepredictions,withthemostaccurateLLMshavinginference
timesthatareordersofmagnitudehigherthantheirquantitativecounterparts. Quantitativemodels,
in contrast, maintain much lower inference times, making them far more efficient for practical
use. The high computational demands of context-aware LLMs hinder their practical use in real-
world forecasting, especially where speed and cost matter. The clear benefits of incorporating
contextwarrantsresearchintomakingthemmoreefficient,aimingtomatchthecost-effectivenessof
traditionalmodelsandenablingtheirdeploymentinlarge-scaleforecasting.
6 RELATED WORK
Wereviewtwostreamsofrelatedwork: (i)workthatintroducerelatedbenchmarksanddatasets,and
(ii)workthatrepurposeLLMstoobtainfoundationmodelsforcontext-aidedforecasting.
9
SPRCR
etagerggA
)sdnoces(
emitnuRPreprint. Underreview.
BenchmarksandDatasets Merrilletal.(2024)presentabenchmarkdesignedtoevaluateLLMs’
abilitytoreasonabouttimeseries,withcontext-aidedforecastingasoneassessedcapability. Their
approachdiffersfromoursinseveralimportantways. First,theyfocusonpurelysynthetictimeseries,
whichmaynotaccuratelyreflectreal-worlddynamics,whereasourbenchmarkisbasedprimarilyon
real-worlddata. Second,theirevaluationislimitedtopointforecastingmetrics,whichdonotmeasure
thequalityofthefullforecastdistribution.Incontrast,weadoptprobabilisticforecastingmetrics,e.g.,
thecontinuousrankedprobabilityscore(CRPS;c.f.Gneiting&Raftery,2007),toassessthequality
ofentireforecastdistributions. OtherrelateddatasetsincludeTime-MMD(Liuetal.,2024a),which
integratestextextractedfromreportsandwebsearches,TGTSF(Xuetal.,2024),whichincorporates
informationsuchasweatherreportsandnewsarticles,SysCaps(Emamietal.,2024),whichincludes
LLM-generateddescriptionsofbuildingenergyconsumptionsystems,andTS-Insights(Zhangetal.,
2023),whichincludesLLM-generateddescriptionsoftrendsandseasonalities. Thekeydistinction
betweentheseworksandoursliesintheroleoftextualinformation: whileintheseworks,thetextis
notessentialtogeneratinghigh-qualityforecasts,inourbenchmark,accurateforecastscannotbe
achieved,bydesign,withouteffectivelyleveragingtheprovidedtextualinformation.
RepurposingLLMsforForecasting Anaturalapproachtothisproblemistobuildforecasting
methodsbasedonLLMs. Xue&Salim(2023)showedthatforecastingcouldbeframedasaquestion-
answering problem. Subsequently, Gruver et al. (2024) and Requeima et al. (2024) showed that
zero-shotsequencecompletioncouldgenerateaccurateforecastsandthattextualside-information
couldbeusedtoinfluenceforecasts. However,theiranalysisislimitedtoillustrativeexamplesrather
thanacomprehensiveevaluation. OtherapproachesincorporatetimeseriesintopretrainedLLMs(Jin
etal.,2024;Liuetal.,2024b;Zhangetal.,2024)byintroducingspecialtokensusedtorepresent
patchedtimeseriespatterns;ormodifyingtheirencoderstoaccountfortimeseriesdata(Jiaetal.,
2024). Whilethesemethodsshowpromisingresults, theirevaluationsprimarilyrelyondatasets
wherethecontextualinformationisnotguaranteedtoimproveforecastsovernumericaldataalone.
Asaresult,itremainsunclearwhethertheirsuccessisdrivenbyaccuratenumericalforecastingorby
effectivelyincorporatingcontext;thisshortcomingmotivatesourinvestigationintothisquestion.
7 DISCUSSION
In this work, we propose the Context is Key (CiK) benchmark: a collection of forecasting tasks
thatrequirecombininghistoricaldatawithcriticalnaturallanguagecontext. Weevaluatearange
ofmodelsonCiK,includingourproposedLLMpromptingmethod,DirectPrompt,whichachieves
thebestperformance. Weanalyseanddiscussthefailuremodesofthesemodels,andourfindings
underscorethecriticalroleofcontextualinformationinimprovingforecasts,whilealsorevealing
boththeunexpectedstrengthsandnotablelimitationsoftheinvestigatedLLM-basedforecasters.
Limitations: Whileourbenchmarkprovidesvaluableinsightsintotheintegrationofcontextual
information in time series forecasting, it is important to acknowledge its limitations. Our study
excludes modalities other than time series data and text, and excludes multivariate time series
scenarios. Althoughwecarefullyanddeliberatelydesignedthetaskstoassesshowwelltimeseries
forecasterscanintegratecontextualinformation,ourfocuswasonrelationshipsbetweencontextand
forecaststhatarediscernibletohumans. Hence,ourbenchmarkdoesnotexplicitlyevaluateamodels’
capacitytoleveragelatentrelationshipsthatmighteludehumanobservation. Moreover,whiletasks
aredesignedtorequirecertaincapabilities,wedonotguaranteethatalternativeapproachestosolving
themdonotexist. Ourcollectionofcapabilitiesandcontexttypeswasnotintendedtobeexhaustive
butrathertoserveastoolsforanalyzingforecasters’performanceonthebenchmark. Whilewehave
takenstepstomitigatememorizationconcerns,asdiscussedinSec.3.1,achievingabsolutecertainty
inthisregardischallengingwithoutstrictlyheld-outdata.
Futurework: Thereareseveralpromisingavenuesforfutureworkinthecontext-aidedforecasting
setting. Enhancementstothebenchmarkcouldincludetasksthatrequiremultivariateforecastingor
incorporateadditionalmodalities,suchasimages,structureddatabases,orspatiotemporaldata. Tasks
thatdeliberatelychallengecontextlengthlimitationsorprobespecificweaknessesoflanguagemodels
would also be valuable additions. Furthermore, this benchmark strongly motivates research into
developingmoreaccurateandefficientmultimodalforecastingmodels,whichitiswell-positionedto
support. Lastly,asmodelsbecomemorerobust,theycouldbeintegratedintoagenticsystemswith
conversationalinterfaces,allowingforecaststobeaugmentedwithhumanexpertiseandautomatically
retrieved facts (e.g., via search engines). Such advancements would represent a significant step
towardautomatinganddemocratizingaccesstopowerfulforecastingtools.
10Preprint. Underreview.
ACKNOWLEDGEMENTS
The authors are grateful to Midan Kim, Torsten Scholak, Mohammad Reza Samsami, Oussama
BoussifandCanChenfortheirvaluablefeedbackandsuggestions. Thisresearchwassupportedby
MitacsAccelerateGrantsandenabledbycomputeresourcesprovidedbyServiceNowResearchand
theFrontiersupercomputer. ThelatterresourceswereawardedthroughtheFrontierDDallocation
andINCITE2023programfortheproject“ScalableFoundationModelsforTransferableGeneralist
AI”andweresuppliedbytheOakRidgeLeadershipComputingFacilityattheOakRidgeNational
Laboratory,withsupportfromtheOfficeofScienceoftheU.S.DepartmentofEnergy.
REFERENCES
JoshAchiam,StevenAdler,SandhiniAgarwal,LamaAhmad,IlgeAkkaya,FlorenciaLeoniAleman,
DiogoAlmeida,JankoAltenschmidt,SamAltman,ShyamalAnadkat,etal.GPT-4technicalreport.
arXivpreprintarXiv:2303.08774,2023. 6
AlexanderAlexandrov, KonstantinosBenidis, MichaelBohlke-Schneider, ValentinFlunkert, Jan
Gasthaus,TimJanuschowski,DanielleC.Maddix,SyamaRangapuram,DavidSalinas,Jasper
Schulz,LorenzoStella,AliCanerTu¨rkmen,andYuyangWang. GluonTS:ProbabilisticandNeural
TimeSeriesModelinginPython. JournalofMachineLearningResearch,21(116):1–6,2020. URL
http://jmlr.org/papers/v21/19-820.html. 48
SamAllen,DavidGinsbourger,andJohannaZiegel. Evaluatingforecastsforhigh-impactevents
usingtransformedkernelscores.SIAM/ASAJournalonUncertaintyQuantification,11(3):906–940,
2023. doi: 10.1137/22M1532184. 48
AbdulFatirAnsari,LorenzoStella,CanerTurkmen,XiyuanZhang,PedroMercado,HuibinShen,
OleksandrShchur,SyamaSundarRangapuram,SebastianPinedaArango,ShubhamKapoor,etal.
Chronos: Learningthelanguageoftimeseries. arXivpreprintarXiv:2403.07815,2024. 7,46
RishiBommasani, DrewAHudson, EhsanAdeli, RussAltman, SimranArora, SydneyvonArx,
MichaelSBernstein,JeannetteBohg,AntoineBosselut,EmmaBrunskill,etal. Ontheopportuni-
tiesandrisksoffoundationmodels. arXivpreprintarXiv:2108.07258,2021. 1
GeorgeE.P.Box,GwilymM.Jenkins,GregoryC.Reinsel,andGretaM.Ljung. Timeseriesanalysis:
forecastingandcontrol. JohnWiley&Sons,fifthedition,2015. 6
ChaoChen,KarlPetty,AlexanderSkabardonis,PravinVaraiya,andZhanfengJia. Freewayperfor-
mancemeasurementsystem: miningloopdetectordata. Transportationresearchrecord,1748(1):
96–102,2001. 3,4,15,18
ZongleiChen,MinboMa,TianruiLi,HongjunWang,andChongshouLi. Longsequencetime-series
forecastingwithdeeplearning: Asurvey. InformationFusion,97:101819,2023. 1
AbhimanyuDubey,AbhinavJauhri,AbhinavPandey,AbhishekKadian,AhmadAl-Dahle,Aiesha
Letman,AkhilMathur,AlanSchelten,AmyYang,AngelaFan,etal. TheLlama3herdofmodels.
arXivpreprintarXiv:2407.21783,2024. 6,41
PatrickEmami,ZhaonanLi,SaumyaSinha,andTrucNguyen. Syscaps: Languageinterfacesfor
simulationsurrogatesofcomplexsystems. arXivpreprintarXiv:2405.19653,2024. 2,5,10
JuanL.Gamella,PeterBu¨hlmann,andJonasPeters. Thecausalchambers: Realphysicalsystemsas
atestbedforAImethodology. arXivpreprintarXiv:2404.11341,2024. 3,15,23
EveretteS.GardnerJr. Exponentialsmoothing: Thestateoftheart. JournalofForecasting,4(1):
1–28,1985. doi: https://doi.org/10.1002/for.3980040103. URLhttps://onlinelibrary.wiley.
com/doi/abs/10.1002/for.3980040103. 6
AzulGarzaandMaxMergenthaler-Canseco. Nixtlafoundation-time-series-arena. https://github.
com/Nixtla/nixtla/tree/main/experiments/foundation-time-series-arena,2024. 1
Azul Garza, Cristian Challu, and Max Mergenthaler-Canseco. TimeGPT-1. arXiv preprint
arXiv:2310.03589,2023. 7,47
11Preprint. Underreview.
TilmannGneitingandAdrianERaftery. Strictlyproperscoringrules,prediction,andestimation.
JournaloftheAmericanstatisticalAssociation,102(477):359–378,2007. 5,10,48
TilmannGneitingandRoopeshRanjan. Comparingdensityforecastsusingthreshold-andquantile-
weightedscoringrules. JournalofBusiness&EconomicStatistics,29(3):411–422,2011. doi:
10.1198/jbes.2010.08110. 6,48
RakshithaGodahewa,ChristophBergmeir,GeoffreyIWebb,RobJHyndman,andPabloMontero-
Manso. Monashtimeseriesforecastingarchive. arXivpreprintarXiv:2105.06643,2021. 3,4,15,
16
NateGruver,MarcFinzi,ShikaiQiu,andAndrewGWilson. Largelanguagemodelsarezero-shot
timeseriesforecasters. AdvancesinNeuralInformationProcessingSystems,36,2024. 8,10
RobHyndman,AnneBKoehler,JKeithOrd,andRalphDSnyder. Forecastingwithexponential
smoothing: thestatespaceapproach. SpringerScience&BusinessMedia,2008. 6
RobJHyndmanandGeorgeAthanasopoulos. Forecasting: principlesandpractice. OTexts,2018. 1
FurongJia,KevinWang,YixiangZheng,DefuCao,andYanLiu. Gpt4mts: Prompt-basedlarge
languagemodelformultimodaltime-seriesforecasting. ProceedingsoftheAAAIConferenceon
ArtificialIntelligence,38(21):23343–23351,Mar.2024. doi: 10.1609/aaai.v38i21.30383. URL
https://ojs.aaai.org/index.php/AAAI/article/view/30383. 10
Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris
Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand,
GiannaLengyel,GuillaumeBour,GuillaumeLample,Le´lioRenardLavaud,LucileSaulnier,Marie-
AnneLachaux,PierreStock,SandeepSubramanian,SophiaYang,SzymonAntoniak,TevenLe
Scao,The´ophileGervet,ThibautLavril,ThomasWang,Timothe´eLacroix,andWilliamElSayed.
Mixtralofexperts,2024. URLhttps://arxiv.org/abs/2401.04088. 6
Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James Y. Zhang, Xiaoming Shi, Pin-Yu Chen,
YuxuanLiang,Yuan-FangLi,ShiruiPan,andQingsongWen. Time-LLM:Timeseriesforecasting
byreprogramminglargelanguagemodels. InTheTwelfthInternationalConferenceonLearning
Representations,2024. URLhttps://openreview.net/forum?id=Unb5CVPtae. 1,6,10,45
Yuxuan Liang, Haomin Wen, Yuqi Nie, Yushan Jiang, Ming Jin, Dongjin Song, Shirui Pan, and
QingsongWen. Foundationmodelsfortimeseriesanalysis: Atutorialandsurvey. arXivpreprint
arXiv:2403.14735,2024. 1
BryanLimandStefanZohren. Time-seriesforecastingwithdeeplearning: asurvey. Philosophical
TransactionsoftheRoyalSocietyA,379(2194):20200209,2021. 1
Haoxin Liu, Shangqing Xu, Zhiyuan Zhao, Lingkai Kong, Harshavardhan Kamarthi, Aditya B
Sasanur,MeghaSharma,JiamingCui,QingsongWen,ChaoZhang,etal. Time-MMD:Anew
multi-domainmultimodaldatasetfortimeseriesanalysis. arXivpreprintarXiv:2406.08627,2024a.
2,5,10
XuLiu,JunfengHu,YuanLi,ShizheDiao,YuxuanLiang,BryanHooi,andRogerZimmermann.
Unitime: A language-empowered unified model for cross-domain time series forecasting. In
ProceedingsoftheACMonWebConference2024,pp.4095–4106,2024b. 1,6,10,44,45
MikeAMerrill,MingtianTan,VinayakGupta,TomHartvigsen,andTimAlthoff. Languagemodels
stillstruggletozero-shotreasonabouttimeseries. arXivpreprintarXiv:2404.11757,2024. 2,5,
10
Kashif Rasul, Arjun Ashok, Andrew Robert Williams, Arian Khorasani, George Adamopoulos,
Rishika Bhagwatkar, Marin Bilosˇ, Hena Ghonia, Nadhir Vincent Hassen, Anderson Schnei-
der, et al. Lag-Llama: Towards foundationmodelsfortimeseries forecasting. arXivpreprint
arXiv:2310.08278,2023. 7,46
James Requeima, John Bronskill, Dami Choi, Richard E Turner, and David Duvenaud. LLM
processes: Numericalpredictivedistributionsconditionedonnaturallanguage. arXivpreprint
arXiv:2405.12856,2024. 1,6,10,43,44
12Preprint. Underreview.
ManajitSengupta,YuXie,AnthonyLopez,AronHabte,GalenMaclaurin,andJamesShelby. The
nationalsolarradiationdatabase(NSRDB). Renewableandsustainableenergyreviews,89:51–60,
2018. 3,15
Maxime Taillardat, Olivier Mestre, Michae¨l Zamo, and Philippe Naveau. Calibrated ensemble
forecastsusingquantileregressionforestsandensemblemodeloutputstatistics. MonthlyWeather
Review,144(6):2375–2393,2016. doi: 10.1175/MWR-D-15-0260.1. 48,49
U.S.BureauofLaborStatistics. Unemploymentrate[variouslocations],2024. URLhttps://fred.
stlouisfed.org/. Accessedon2024-08-30,retrievedfromFRED. 3,15
VilledeMontre´al.Interventionsdespompiersdemontre´al,2020.URLhttps://www.donneesquebec.
ca/recherche/dataset/vmtl-interventions-service-securite-incendie-montreal. Up-
datedon2024-09-12,accessedon2024-09-13. 3,4,15
PeiWang. Ondefiningartificialintelligence. JournalofArtificialGeneralIntelligence,10(2):1–37,
2019. 1
JasonWei,MaartenBosma,VincentYZhao,KelvinGuu,AdamsWeiYu,BrianLester,NanDu,
AndrewMDai,andQuocVLe. Finetunedlanguagemodelsarezero-shotlearners. arXivpreprint
arXiv:2109.01652,2021. 41
GeraldWoo, Chenghao Liu, AkshatKumar, Caiming Xiong, Silvio Savarese, andDoyen Sahoo.
Unifiedtrainingofuniversaltimeseriesforecastingtransformers.arXivpreprintarXiv:2402.02592,
2024. 7,47
Zhijian Xu, Yuxuan Bian, Jianyuan Zhong, Xiangyu Wen, and Qiang Xu. Beyond trend and
periodicity: Guidingtimeseriesforecastingwithtextualcues. arXivpreprintarXiv:2405.13522,
2024. 2,10
HaoXueandFloraDSalim. Promptcast: Anewprompt-basedlearningparadigmfortimeseries
forecasting. IEEETransactionsonKnowledgeandDataEngineering,2023. 10
Michae¨lZamoandPhilippeNaveau. Estimationofthecontinuousrankedprobabilityscorewith
limitedinformationandapplicationstoensembleweatherforecasts. MathematicalGeosciences,
50(2):209–234,2018. doi: 10.1007/s11004-017-9709-7. 48,49
WeiqiZhang,JiexiaYe,ZiyueLi,JiaLi,andFugeeTsung. Dualtime: Adual-adaptermultimodal
languagemodelfortimeseriesrepresentation. arXivpreprintarXiv:2406.06620,2024. 10
YunkaiZhang,YawenZhang,MingZheng,KezhenChen,ChongyangGao,RuianGe,SiyuanTeng,
AmineJelloul,JinmengRao,XiaoyuanGuo,Chiang-WeiFang,ZeyuZheng,andJieYang. Insight
miner: Alarge-scalemultimodalmodelforinsightminingfromtimeseries. InNeurIPS2023AI
forScienceWorkshop,2023. URLhttps://openreview.net/forum?id=E1khscdUdH. 2,5,10
HaoyiZhou,ShanghangZhang,JieqiPeng,ShuaiZhang,JianxinLi,HuiXiong,andWancaiZhang.
Informer: Beyondefficienttransformerforlongsequencetime-seriesforecasting. InTheThirty-
FifthAAAIConferenceonArtificialIntelligence,AAAI2021,VirtualConference,volume35,pp.
11106–11115.AAAIPress,2021. 45
13Preprint. Underreview.
Appendix
Table of Contents
A AdditionalDetailsontheBenchmark 14
A.1 DataSources. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
A.2 Weightingschemefortasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
A.3 Standarderrorsandaverageranks . . . . . . . . . . . . . . . . . . . . . . . . . 16
A.4 ModelCapabilities. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
A.5 Tasklengths . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
B Examplesoftasksfromthebenchmark 17
B.1 Task:ConstrainedPredictions . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
B.2 Task:ElectricalConsumptionIncrease . . . . . . . . . . . . . . . . . . . . . . 19
B.3 Task:ATMMaintenance. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
B.4 Task:MontrealFireHighSeason . . . . . . . . . . . . . . . . . . . . . . . . . 21
B.5 Task:SolarPrediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
B.6 Task:SpeedFromLoad . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
C AdditionalResults 24
C.1 Resultspartitionedbytypesofcontext . . . . . . . . . . . . . . . . . . . . . . 24
C.2 Extendedresultsonallmodels . . . . . . . . . . . . . . . . . . . . . . . . . . 25
C.3 Significantfailurespermodel . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
C.4 Visualizationsofsuccessfulcontext-awareforecasts . . . . . . . . . . . . . . . 27
C.5 Visualizationsofsignificantfailures . . . . . . . . . . . . . . . . . . . . . . . . 35
C.6 CostofAPI-basedmodels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
C.7 ImpactofIrrelevantInformationinContext . . . . . . . . . . . . . . . . . . . . 39
D ImplementationDetailsofModels 41
D.1 DirectPrompt . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
D.2 LLMP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
D.3 UniTimeandTime-LLM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
D.4 Lag-Llama . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46
D.5 Chronos . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46
D.6 Moirai . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47
D.7 TimeGEN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47
D.8 ExponentialSmoothing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47
D.9 ETSandARIMA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47
E Detailsoftheproposedmetric 47
E.1 Scalingforcross-taskaggregation . . . . . . . . . . . . . . . . . . . . . . . . . 48
E.2 CRPSandtwCRPS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
E.3 EstimatingtheCRPSusingsamples . . . . . . . . . . . . . . . . . . . . . . . . 49
E.4 Constraint-violationfunctions . . . . . . . . . . . . . . . . . . . . . . . . . . . 50
E.5 CovarianceoftwoCRPSestimators . . . . . . . . . . . . . . . . . . . . . . . . 50
A ADDITIONAL DETAILS ON THE BENCHMARK
A.1 DATASOURCES
Welistherethedomainsandtherespectivesourcesoftimeseriesdataweuseinthevarioustasks
intheCiKbenchmark. Wealsoshowthenumberoftasksthatuseeachsource’sdataandlistany
memorizationmitigationstrategiesusedforeachdataset.
14Preprint. Underreview.
• Traffic(11tasks):
– Trafficoccupancyrate: Weusetrafficoccupancyrate(%)datafromtheCalifornia
PerformanceMeasurementSystem(PeMS)(Chenetal.,2001),withfrequencyhourly.
Thisdatasetcontainsatotalof446series.
* As this is a live dataset (updated frequently), we use data from 2024 (i.e. data
afterthecutoffdatesofLLMsused)anddonotapplyanymemorizationmitigation
strategy.
• Climatology(12tasks):
– Solarirradianceandcloudcoverdata(9tasks): Weusesolarirradianceandcloud
coverdatafortheAmericasin2022(Senguptaetal.,2018),withfrequencyeither10
minutesorhourly. Weextractasubsetof45seriesfromthisdatasetforthebenchmark.
* Tomitigatememorization,weshiftthedatesbyonedayahead.
– Solarphotovoltaicpowerproduction(3tasks): Timeseriesreflectingsolarpower
production in Alabama during 2006 (Godahewa et al., 2021), with a frequency 10
minutes. Thisdatasetcontainsatotalof137series, butourtasksonlyuseasingle
aggregatedseriesgeneratedfromthem.
* To mitigate memorization, we add gaussian noise to the data with a standard
deviationof3%ofthestandarddeviationofthedataineachrespectivesampled
window.
• PublicSafety(26tasks):
– FireDepartmentInterventionLogs: Logsofnumberofinterventionscarriedoutby
theMontrealFireDepartmentduetotheoccurenceofvariouskindsofincidents(such
astrashfires,fieldfires,nauticalaccidents,bikeaccidents)(VilledeMontre´al,2020).
Thedatawasprocessedfromarawlogandaggregatedtomonthlyfrequency. This
datasetcontainsatotalof48series.
* Duetoitbeingprocessed,wedonotapplyanyspecialmemorizationmitigation
strategyontop.
• Mechanics(3tasks):
– CausalChambers: Experimentaldatacollectedfromthewindtunnelphysicalsystem
from Gamella et al. (2024), released in April 2024. We make use of the loadin,
pressuredownwind,pressureambientandspeedinseries(downsamplingthemto
1sfrequency)tobuildout-of-distributionforecastingtaskswherethetargetvaluescan
be inferred from the driver variate provided as covariate and the description of the
physicalsystemgiveninthecontext. Weselectasubsetof17seriesfromthisdataset
forthebenchmark.
* Sincethedataisreleasedin2024andafterthecutoffdatesoftheLLMsused,we
donotapplyanymemorizationmitigationtechniquetotransformthedata.
• Economics(3tasks):
– FRED:Americanunemploymentdataatthestateandcountylevels,fromtheFederal
ReserveBankofSt. Louis(U.S.BureauofLaborStatistics,2024), withfrequency
monthly. Weextractasubsetof1769seriesfromthisdatasetforthebenchmark.
* As this is a live dataset (updated frequently), we use data from 2024 (i.e. data
afterthecutoffdatesofLLMsused)anddonotapplyanymemorizationmitigation
strategy.
• Retail(6tasks):
– NN5ATMcashwithdrawals: TheNN5datasetofATMcashwithdrawalsintheUK
fromtheMonashTimeSeriesForecastingRepository(Godahewaetal.,2021),with
frequencydaily. Thisdatasetcontainsatotalof111series.
* To mitigate memorization, we add gaussian noise to the data with a standard
deviationof3%ofthestandarddeviationofthedataineachrespectivesampled
window.
• Energy(7tasks):
15Preprint. Underreview.
– Electricityconsumption: Electricityusagefrom2012to2014fromtheMonashTime
Series Forecasting Repository (Godahewa et al., 2021), with frequency daily. This
datasetcontainsatotalof321series.
* To mitigate memorization, we add gaussian noise to the data with a standard
deviationof3%ofthestandarddeviationofthedataineachrespectivesampled
window.
• SyntheticData(3tasks): Weemployabivariatesetupwheretheparentvariableisdrawn
fromacategoricaldistribution,andthechildvariableisgeneratedusingacontinuouslinear
StructuralVectorAutoregressive(SVAR)modelwithGaussiannoise,withalagof3anda
noisescaleof0.1.
– Sincethisdataissynthetic,wedonotapplyanymitigationtechniqueontopofdata
tomitigatememorization. Sinceourmodelsassumeatimestamp,weusedatesfrom
2025,andafrequencyofdailywhenweinputthisdatatoourmodels.
Dependingonthetaskandthecontextusedinthetask,appropriatehistoryandpredictionlengthsare
usedinthetask.
A.2 WEIGHTINGSCHEMEFORTASKS
To take full advantage of the available data, we create multiple tasks using each data source, by
varyingthespecificcontextualinformationweprovidetothemodels. Sincewedonotwantour
aggregateresultstobedominatedbythefewdatasetsforwhichtherearealargernumberoftasks,
weweightthecontributionofeachtasktothevariousaggregatedresults.
Todefinetheweightofeachtask,wefirstgroupthetasksinclusters. Theseclustersareprimarily
definedbasedontheoriginaldatasourceusedtocreatethetasks. However,whentasksarefunda-
mentallydifferent,duetonottestingthesamecapabilities,weputthemindifferentclustersdespite
themusingthesamedatasource. Forexample,fortaskscreatedusingtheSolarirradianceandcloud
coverdata,allofwhichaskmodelstoforecasttheirradiance,thetasksformthreedistinctclusters:
onefortasksaskingmodelstodoforecastwithveryshorthistory(lessthanaday),onefortasks
givingthecloudcoverascovariate,andthefinalonefortaskswherethemodelsaregivenatight
upperboundonthepossibleirradiance. Oncewedefinetheseclusters,wesimplyequalweightto
eachcluster,andequalweighttoeachtaskinsideeachcluster.
A.3 STANDARDERRORSANDAVERAGERANKS
To get the standard errors shown in Tab. 1, we first compute the standard error for tasks using
the method described in Appendix E.5. We then aggregate them according to each task weight,
byassumingthaterrorsforeachareindependentandthususingtheformulaforthevarianceofa
weightedsumofindependentvariables.
Totakeintoconsiderationtheuncertaintywehaveforthescores,wecomputeaverageranksthrough
asimplesimulation. Inthissimulation,wereplacetheRCRPSforeachtaskandmodelpairbyan
independentGaussianvariableofmeanequalstotheonewemeasured,andofstandarddeviation
equalstothestandarderror. Wethendrawfromthisdistributionandcomputetheweightedaverage
ranksforeachmodel. TheresultsshowninTab.1arethemeanandstandarddeviationmeasured
from10,000repetitionsofthissimulation.
A.4 MODELCAPABILITIES
Weprovideadetailedexplanationofeachmodelcapabilityhere.NotethattasksintheCiKbenchmark
neednotbemutuallyexclusivewiththemodelcapabilitiestheyrequire;tasksaretaggedwithoneor
moremodelcapabilities.
Instructionfollowing(24Tasks): Usingdirectinstructionsavailableinthecontext. Instructions
couldexpressconstraintstobesatisfied,ortheexpectedeffectofanevent,forexample.
Retrieval: Retrievingfactsfrommemoryorcontext.
• Retrievalfrommemory(35Tasks): Retrievingfrommemoryfactsthatenableinterpretationof
thecontext,suchasrelevantphysicalconstantsorquantitativelaws.
16Preprint. Underreview.
• Retrievalfromcontext(25Tasks):Retrievingrelevantinformationfromcontextanddistinguishing
itfromirrelevantinformation.
Reasoning: Reasoningaboutinformationincontextormemory.
• Analogical Reasoning (6 tasks): Making analogies between entities or events, for instance,
applyingknowledgefromapasteventthatissimilartoanupcomingone.
• MathematicalReasoning(32tasks): Performingcalculationsoverthecontext,e.g. solvingan
equation.
• DeductiveReasoning(39tasks): Inferringnewfactsnotexplicitlymentionedinthecontext,e.g.
inferringfromthecontextthatcertainvaluesarelogicallyimpossibletooccur.
• CausalReasoning(22tasks): Derivingorusingcausalinformationfromthecontexttoreason
aboutactions(suchasinterventions).
A.5 TASKLENGTHS
Fig.7providesanoverviewofthedistributionofthelengthsofthenaturallanguagecontext,numerical
historyandtarget(predictionhorizon)forasetoffiveinstancesforeachtaskintheCiKbenchmark.
Figure7:Histogramsdepictingthedistributionoflengthsforthecontext,numericalhistoryandtargetlengthof
asetoffiveinstancesforeachtaskinCiK.Wemeasurethelengthofthenaturallanguagecontextincharacters,
andthenumericalsequencesinfloats.
B EXAMPLES OF TASKS FROM THE BENCHMARK
Inthissection,wefeaturemultipleexamplesfromthebenchmarktoexemplifyexactlywhatatask
is,whatcontextsourcesrepresent(Sec.3.2),andhowthesetasksencouragetheuseofcapabilities
(Sec. 3.3). To visualize all tasks in the benchmark, we refer the reader to https://servicenow.
github.io/context-is-key-forecasting/v0/.
17Preprint. Underreview.
B.1 TASK: CONSTRAINEDPREDICTIONS
Domain:Traffic
Contextsources:Futureinformation
Capabilities:InstructionFollowing
Context:“Supposethatintheforecast,thevaluesareboundedaboveby11.88,thevaluesarebounded
belowby7.06.”
Thistask,whichwerefertoas“BoundedPredictionConstraintBasedOnPredictionQuantiles”,isa
forecastingtaskwherewemodifytheforecasthorizon(ingreenintheplot)byboundingoneorboth
ofitsextremesaccordingtoitsunmodifiedgroundtruth’squantilevalues. Weverbalizethesebounds
inthecontext,andthemodelisexpectedtointerpretandrespectthem.
SincewedrawthisseriesfromthePeMSdataset(Chenetal.,2001),wetagitsdomainas“Traffic”.
Thecontextdirectlyreferstothefuture,hencethecontextsourceistaggedas“Futureinformation”.
Finally, since the model is expected to obey the constraints in the context, we tag the evaluated
capabilityas“Instructionfollowing”.
Sincethecontextcontainsconstraints,theRegionofInterestCRPSmetricthatweintroduce(Sec.4)
heavilypenalizesforecaststhatexceedtheseconstraints: modelsthatdonotincorporatetheinfor-
mationaboutboundsinthecontext,suchasquantitativeforecastingmodels,wouldnotbeableto
predictthegroundtruth(orangeline)becauseitslowerboundismuchhigherthanthatofthehistory.
Inthiscase,theregionofinterestforthemetricistheentireforecasthorizonbecausethecontext
applieseverywhere. Althoughstatisticalforecastersmaypickupontheseasonalitypresentinthe
history(blackline),theywouldobtainworsescoresthanmodelscapableofprocessingthecontext
andadjustingthelowerboundoftheirpredictions.
18Preprint. Underreview.
B.2 TASK: ELECTRICALCONSUMPTIONINCREASE
Domain:Energy
Contextsources:Futureinformation,Covariateinformation
Capabilities:Instructionfollowing,Retrievalfromcontext
Context:“ThisistheelectricityconsumptionrecordedinKilowatt(kW)incityA.Aheatwavestruck
thecity,whichbeganon2012-10-0918:00:00andlastedforapproximately3hours,sawtemperatures
soartounprecedentedlevels.Accordingtothecity’selectricityprovider,powerconsumptionduring
thepeakoftheheatwavereachedapproximately5timesthetypicalusageforthistimeofyear.”
History
16000 Ground Truth
Forecast
14000 Region of Interest
12000
10000
8000
6000
4000
2000
The“ShortNewsElectricityIncrease”taskintroducesalargeshockintheforecasthorizonthatis
onlyreferredtointhecontext. Hence,themodelmustinterpretthecontextappropriatelytoforecast
thespike.
Sincethisseriesrepresentselectricityconsumption(Sec.3.1),wetagitacomingfromthe“Energy”
domain. Thecontextsourcesforthistaskaretwofold:thefirstcontextsourceis“Futureinformation”,
whichrepresentsknowledgeofthefive-foldincreaseintypicalusageduringtheshock. Thesecond
sourceofcontext,“Covariateinformation”,representstheoccurrenceofaheatwave,whichcoincides
withthetiminganddurationoftheshock. Themodelmustthereforeinterpretboththeinformation
onthemagnitudeoftheshockfromthefutureinformation,aswellasthetiminganddurationof
thesockfromthecovariateinformation. Together,thesepiecesofinformationenableanaccurate
forecastdespitethelackofinformationabouttheshockinthetask’snumericalhistory.
Theskillsforthistaskaretaggedas“Instructionfollowing”and“Retrievalfromcontext”. While
instruction following involves interpreting the context to include the shock in the prediction, the
modelmustalsoretrievefromthecontexttherelevantinformation,asthereisunneededinformation
inthecontextaswell: anaccurateforecastdoesnotrequireknowingthatthetemperaturehasreached
unprecedentedlevels.
Inthistask,wealsoseea“RegionofInterest”(RoI),characterizedbyadarkerregionoftheforecast
horizon. ThisRoIrepresentstheregionoftheforecasthorizonforwhichthecontextisrelevant,i.e.
theperiodduringwhichtheincreasedpowerconsumptionoccurred. AsdetailedinSec.4,thisregion
ofinterestistakingintoaccountintheRCRPSmetric.
19
40-01-2102 50-01-2102 60-01-2102 70-01-2102 80-01-2102 90-01-2102 01-01-2102Preprint. Underreview.
B.3 TASK: ATMMAINTENANCE
Domain:Retail
Contextsources:Intemporalinformation,Covariateinformation
Capabilities:Instructionfollowing,Deductivereasoning
Context:“Thisisthenumberofcashwithdrawalsfromanautomatedtellermachine(ATM)inan
arbitrarylocationinEngland.TheATMwasundermaintenancefor7days,periodicallyevery14
days,startingfrom1996-11-3000:00:00. AssumethattheATMwillnotbeinmaintenanceinthe
future.”
History
Ground Truth
40 Forecast
Region of Interest
30
20
10
0
The “Automated Teller Machine (ATM) Under Period Maintenance“ task represents the history
of withdrawals from an ATM that undergoes regular maintenance. This maintenance introduces
a periodic, easily forecastable signal into the history. However, the context explicitly states that
the forecast should assume the ATM will not be in maintenance during the forecast. Therefore,
forecastingmodelsareexpectedtoignorethissignal.
SincethisseriesrepresentsATMwithdrawals,wetagitas“Retail”. Thecontextincludesinformation
suchasthelocationoftheATM,andthereforeprovides“Intemporalinformation”.Asthemaintenance
frequencyanddurationisalsodescribed,thecontextsourcesinclude“Covariateinformation”.
Thistaskistaggedwithtwocapabilities. “Instructionfollowing”isnecessarybecausethemodel
mustassumethattheATMwillnotbeinmaintenanceinthefuture. However,themodelmustuse
“Deductivereasoning”todeterminewhatandwhentheimpactofthemaintenancewas–reducingthe
numberofwithdrawalsto0every14days–,andavoidincludingthatpatternintheforecast. The
RoIrepresentswhenthemaintenanceperiodswouldhaveoccurredintheforecasthorizon,whichis
likelywhereforecastingmodelsthatdonotleveragethecontextwillforecast0. Whileaquantitative
forecastingmodelwouldfindsuchasignalirresistible,context-awaremodelsshouldavoidrepeating
thepatternintheforecast.
Wealsonotethattheseriesisnotquite0duringthemaintenanceperiods. Thisisaconsequence
ofusingoneofourmemorizationmitigationschemes(AppendixA.1, paragraph“Memorization
mitigation”).
20
21-6991 10-7991 20-7991 30-7991 40-7991 50-7991Preprint. Underreview.
B.4 TASK: MONTREALFIREHIGHSEASON
Domain:PublicSafety
Contextsources:Intemporalinformation,Historicalinformation
Capabilities:Deductivereasoning,Mathematicalreasoning,Retrievalfrommemory
Context:“TheMontrealFireDepartmentisinchargeofrespondingtovariouskindofpublicsafety
incidents. This is the number of field fire incidents responded to by Montreal firefighters in the
boroughofRivie`re-des-Prairies-Pointe-aux-Trembles.Inotheryears,theyearlyaveragenumberof
incidentswas106withthebusiestmonthbeingJune.”
The“MontrealFieldFireWithExplicitShortHistory”taskrequirespredictingthenumberoffield
fireincidentsduringthesummer,sowetagitasbeingpartofthe“PublicSafety”domain.
Thecontextcontainsinformationfromtwodifferentsources: itcontains“Intemporalinformation”,
suchasthelocationandnatureoftheincidents. However,italsocontains“Historicalinformation”,
whichverbalizesstatisticsaboutpastvaluesoftheseries,beyondthenumericaldata. Thatis,the
yearlyaveragenumberofincidents,alongwiththeknowledgethatJuneisthemonthwiththemost
incidents.
This task is tagged with many skills and involves several steps of interpretation to arrive at a
reasonable forecast. We first note that the task requires “Retrieval from memory”: an important
pieceofinformationforthispredictionisthatwintersinMontreal,acityinthenorthernhemisphere,
arelongandharsh, withtemperaturesreaching−40◦C.Secondly, thetaskrequiresthemodelto
use“Deductivereasoning”todeducethat,sincetemperaturesaresocoldduringthewintermonths,
fieldsarelikelycoveredinsnowandareratherunlikelytocatchfire. Finally,themodelcanemploy
“Mathematicalreasoning”todeterminehowmanyfieldfiresarelikelytooccuronaverageinthe
forecasthorizon,giventhetotalnumberoffieldfiresthathavealreadyblazedduringthehistory.
Note that “Retrieval from memory” tasks do not explicitly ask the model to return information
retrieved from memory; rather, we tag tasks as such because they cannot be solved without key
informationthatisnotpresentinthehistoryorthecontext.
21Preprint. Underreview.
B.5 TASK: SOLARPREDICTION
Domain:Climatology
Contextsources:Intemporalinformation
Capabilities:Analogicalreasoning,Deductivereasoning,Retrievalfrommemory
Context: “Thisseriesestimatesthepowerproductionforagivendayofanewsolarpowerplant
locatedinthestateofGeorgia,whichhasaclimatesimilartoAlabama’s.”
25 History
Ground Truth
Forecast
20
15
10
5
0
The“ExplicitSimilarLocationandDaySolarForecast”taskrequiresforecastingthepowerproduction
ofasolarpowerplantbasedonaveryshorthistoryandinformationaboutthesimilaritybetweenits
climateandthatofanadjacentlocation. Wethereforetagthedomainofthisseriesas“Climatology”.
Withoutthe“Intemporalinformation”thatthecontextprovides,itisquitepossiblyimpossibleto
accuratelyforecasttheparabola-likeshapeofthegroundtruth: thehistorycontainsveryfewdefining
characteristics,whichmakesitinterchangeablewiththatofmanypotentialprocessesandtherefore
manypossibleforecasts. Themodelmustuse“Deductivereasoning”toforeseethisreversiontozero
basedonthefactthatsolarpanelsdonotproduceelectricityatnight.
However,theinformationinthecontextaloneisnotsufficienttoprovideanaccurateforecast:nothing
indicatesthetimeatwhichproductionshouldpeak.Itmustthereforerelyon“Retrievalfrommemory”
toretrieveinformationaboutAlabama’sclimateandthen“Analogicalreasoning”toapplyittothe
presentproblem.
22
00
70-80
30
70-80
60
70-80
90
70-80
21
70-80
51
70-80
81
70-80
12
70-80Preprint. Underreview.
B.6 TASK: SPEEDFROMLOAD
Domain:Mechanics
Contextsources:Causalinformation,Intemporalinformation,Covariateinformation
Capabilities:Causalreasoning,Mathematicalreasoning,Instructionfollowing
Context:“Thewindtunnelisachamberwithonecontrollablefanthatpushesairthroughit.Wecan
controltheloadofthefan(correspondingtothedutycycleofthepulse-width-modulationsignal)and
measureitsspeed(inrevolutionsperminute).Thefanisdesignedsoitssteady-statespeedscales
broadlylinearlywiththeload.Unlesscompletelypoweredoff,thefanneveroperatesbelowacertain
speed,correspondingtoaminimumeffectiveloadbetween0.1and0.2.Thetaskistoforecastthe
speedofthefan.Theloadisbetween0and1.Atfullload(=1),thefanturnsatamaximumspeed
of3000rpm. Theloadissetto: 0.0until05:47:09,0.1from05:47:09until05:47:29,0.0from
05:47:29until05:48:01,0.2from05:48:01until05:48:27,0.1from05:48:27until05:48:49,0.0
from05:48:49until05:49:00.”
History
650
Ground Truth
Forecast
600
550
500
450
400
350
300
The“SpeedFromLoad”taskcombinesmanydifferentcontextsourcesandcapabilitiestoproduce
a forecastof the revolutionsper minute (RPM)of a fan ina wind tunnelbased on itsload. This
task,basedontheCausalChambersdataset(Gamellaetal.,2024),isthereforetaggedaspartofthe
“Mechanics”domain.
Astheplotshows,producinganaccurateforecastofthegroundtruth(orangeline)fromthenumerical
historyalone(blackline)isessentiallyimpossible. However,thecontextofthetaskisquiterich: it
provides“Intemporalinformation”onthenatureofthetask,suchasthelimitsoftheloadandofthe
fan,“Covariateinformation”thatdescribestheloadduringthehistoryandfuture,aswellas“Causal
information”onthecontrolthattheloadexertsonthefan, aswellastheproportionalityoftheir
relationship.
To leverage the context requires multiple skills: firstly, “Instruction following” is necessary to
understand that the task is to forecast the speed of the fan (as opposed to e.g. the load) and to
applythecorrectloadsattherightmoments. Secondly,themodelmustuse“Causalreasoning”to
understandthatthechangesintheloadwilldirectlyimpactthespeedofthefan. Finally,themodel
mustleverage“Mathematicalreasoning”tocalculatethespeedofthefanasafunctionoftheload.
23
03:64:50 00:74:50 03:74:50 00:84:50 03:84:50Preprint. Underreview.
C ADDITIONAL RESULTS
C.1 RESULTSPARTITIONEDBYTYPESOFCONTEXT
Table Tab. 2 provides a view of the results partitioned by the types of context. One can observe
that Direct Prompt - Llama-3.1-405B-Instruct achieves the best performance at tasks where the
context involves intemporal, future or covariate information, while GPT-4o has an upper hand
attasksinvolvinghistoricalcontextinformation. LLMPwithLlama-3-70B-Instructachievesthe
best performance in tasks that involve causal information in the context. This provides a view
complementarytothatofpartitioningbymodelcapabilities(asinTab.1),andemphasizesthatno
singlemodelisthebestatprocessingalltypesofcontext,leavingroomforadvancementsinmodels
inthefuture.
Table2:ResultsontheCiKbenchmarkaggregatedoveralltasksandkindsofcontext.Thefirstcolumnshows
theRCRPSaveragedoveralltasks.Thesecondcolumnshowstherankofeachmethodw.r.t.otherbaselines,
averagedoveralltasks. TheremainingcolumnsshowtheaverageRCRPSstratifiedbycontextsource. All
averagesareweightedaccordingtotheschemedescribedinSec.5.1andaccompaniedbystandarderrors.
Lowerisbetterandthebestmeansareinbold.*denotesmodelsthatdonotusenaturallanguagecontext.
Model AverageRCRPS AverageRank cI cH cF ccov ccausal
DirectPrompt(ours)
Llama-3.1-405B-Inst 0.159±0.008 4.469±0.192 0.174±0.010 0.146±0.001 0.085±0.003 0.169±0.010 0.398±0.045
Llama-3-70B-Inst 0.518±0.030 10.406±0.190 0.621±0.042 0.308±0.064 0.301±0.033 0.452±0.032 0.704±0.056
Llama-3-8B-Inst 1.647±0.069 15.130±0.171 2.355±0.100 0.813±0.115 1.332±0.094 1.185±0.087 2.041±0.271
Mixtral-8x7B-Inst 1.061±0.058 13.381±0.230 1.263±0.082 0.561±0.111 0.691±0.094 0.724±0.053 1.232±0.121
GPT-4o 0.276±0.010 4.368±0.149 0.220±0.007 0.118±0.001 0.108±0.001 0.265±0.012 0.858±0.053
GPT-4o-mini 0.353±0.022 8.930±0.177 0.474±0.035 0.139±0.002 0.141±0.001 0.345±0.030 0.644±0.128
LLMP
Llama-3-70B-Inst 0.550±0.013 8.038±0.205 0.455±0.018 0.516±0.028 0.690±0.018 0.588±0.018 0.392±0.028
Llama-3-70B 0.237±0.006 6.560±0.254 0.213±0.005 0.121±0.008 0.233±0.012 0.198±0.004 0.360±0.011
Llama-3-8B-Inst 0.484±0.010 9.457±0.166 0.477±0.013 0.161±0.006 0.264±0.003 0.316±0.008 0.878±0.035
Llama-3-8B 0.313±0.023 9.499±0.323 0.334±0.035 0.123±0.004 0.232±0.012 0.291±0.031 0.739±0.134
Mixtral-8x7B-Inst 0.264±0.004 8.496±0.256 0.242±0.007 0.173±0.004 0.268±0.009 0.220±0.002 0.437±0.007
Mixtral-8x7B 0.262±0.008 8.619±0.208 0.250±0.008 0.119±0.003 0.254±0.013 0.229±0.007 0.457±0.011
MultimodalModels
UniTime 0.371±0.002 13.495±0.091 0.455±0.002 0.154±0.000 0.226±0.003 0.396±0.001 0.422±0.001
TimeLLM 0.476±0.001 16.662±0.075 0.517±0.002 0.183±0.000 0.376±0.002 0.446±0.001 0.482±0.001
TSFoundationModels*
Lag-Llama 0.329±0.004 13.157±0.235 0.333±0.005 0.167±0.005 0.277±0.006 0.301±0.004 0.495±0.014
Chronos 0.326±0.002 11.962±0.139 0.314±0.002 0.179±0.003 0.316±0.002 0.252±0.002 0.460±0.004
TimeGEN 0.354±0.000 14.345±0.090 0.333±0.000 0.177±0.000 0.348±0.000 0.291±0.000 0.474±0.000
Moirai 0.520±0.006 12.458±0.266 0.596±0.009 0.140±0.001 0.364±0.002 0.510±0.008 0.438±0.011
StatisticalModels*
ARIMA 0.480±0.006 12.320±0.180 0.565±0.010 0.200±0.007 0.307±0.003 0.390±0.006 0.440±0.011
ETS 0.522±0.009 14.310±0.203 0.627±0.014 0.362±0.014 0.323±0.008 0.401±0.010 0.508±0.017
Exp-Smoothing 0.603±0.013 14.936±0.132 0.700±0.020 0.493±0.016 0.438±0.009 0.492±0.017 0.827±0.060
24Preprint. Underreview.
C.2 EXTENDEDRESULTSONALLMODELS
Table3:ExtendedresultsontheCiKbenchmarkaggregatedoveralltasks.ThefirstcolumnshowstheRCRPS
averagedoveralltasks.Thesecondcolumnshowstherankofeachmethodw.r.t.otherbaselines,averagedover
alltasks.AllaveragesareweightedaccordingtotheschemedescribedinSec.5.1andaccompaniedbystandard
errors.Lowerisbetterandthebestmeansareinbold.
Model AverageRCRPS AverageRank
WithContext
DirectPrompt(ours)
Llama-3.1-405B-Inst 0.159±0.008 6.369±0.384
Llama-3-70B-Inst 0.518±0.030 17.350±0.399
Llama-3-8B-Inst 1.647±0.069 27.425±0.360
Mixtral-8x7B-Inst 1.061±0.058 24.010±0.499
GPT-4o 0.276±0.010 6.973±0.304
GPT-4o-mini 0.353±0.022 13.704±0.315
LLMP
Llama-3-70B-Inst 0.550±0.013 13.469±0.399
Llama-3-70B 0.237±0.006 9.646±0.542
Llama-3-8B-Inst 0.484±0.010 14.465±0.356
Llama-3-8B 0.313±0.023 14.497±0.713
Mixtral-8x7B-Inst 0.264±0.004 12.505±0.482
Mixtral-8x7B 0.262±0.008 12.826±0.381
MultimodalModels
UniTime 0.371±0.002 24.928±0.135
Time-LLM 0.476±0.001 31.632±0.117
WithoutContext
DirectPrompt(ours)
Llama-3.1-405B-Inst 0.473±0.005 25.194±0.219
Llama-3-70B-Inst 0.714±0.035 28.141±0.348
Llama-3-8B-Inst 1.900±0.059 34.027±0.265
Mixtral-8x7B-Inst 0.847±0.045 26.253±0.494
GPT-4o 0.441±0.008 23.450±0.289
GPT-4o-mini 0.423±0.006 26.131±0.199
LLMP
Llama-3-70B-Inst 0.378±0.004 19.742±0.349
Llama-3-70B 0.312±0.006 16.413±0.376
Llama-3-8B-Inst 0.503±0.009 23.125±0.337
Llama-3-8B 0.345±0.003 18.819±0.278
Mixtral-8x7B-Inst 0.383±0.015 18.454±0.361
Mixtral-8x7B 0.306±0.007 17.097±0.386
MultimodalModels
UniTime 0.405±0.002 26.875±0.128
Time-LLM 0.454±0.002 30.067±0.125
TSFoundationModels
Lag-Llama 0.329±0.004 22.781±0.626
Chronos-Tiny 0.328±0.001 20.783±0.335
Chronos-Mini 0.341±0.001 21.926±0.331
Chronos-Small 0.328±0.002 20.126±0.269
Chronos-Base 0.672±0.003 22.519±0.283
Chronos-Large 0.326±0.002 19.255±0.311
TimeGEN 0.354±0.000 26.700±0.121
Moirai-Small 0.565±0.031 26.323±0.353
Moirai-Base 0.624±0.013 26.065±0.275
Moirai-Large 0.520±0.006 21.041±0.718
StatisticalModels
ARIMA 0.480±0.006 19.780±0.386
ETS 0.522±0.009 24.259±0.476
Exp-Smoothing 0.603±0.013 25.858±0.241
25Preprint. Underreview.
C.3 SIGNIFICANTFAILURESPERMODEL
Weobservethatinafewinstancesinthebenchmark,somemodelstendtoobtainsignificantlyworse
performancewhenevaluatedwithcontext. Inourevaluation,wetermallinstanceswheretheRCRPS
valueofamodelisgreaterthan5,assignificantfailuresofthemodelonthoseinstances. Wefound5
asasuitablevalueforanalyzingsuchfailures,asitintuitivelyrepresentsthevalueaforecastwould
getifthedistancebetweentheforecastandtheground-truthwas5timesbiggerthantherangeof
theground-truthforthetask. WhenweaggregatetheRCRPSofinstancesinthebenchmark(such
as in Tab. 1), we cap the RCRPS of such significant failures to 5, to avoid outliers with a much
higherRCRPSaffectingtheaggregatescore. InTab.4,weshowthenumberofsuchinstancesinour
evaluationofthebenchmarkwherewefoundmodelstohavesignificantfailures(outofatotalof355
evaluatedinstances). Interestingly,somemodelssuchasDirectPromptwithLlama-3.1-405B-Instruct
and LLMP with Llama-3-70B and Llama-3-8B are more robust to such significant failures, and
donotincursuchfailures. Ontheotherhand,modelssuchasLLMPwithLlama-3-70B-Instand
Llama-3-8B-Instachievesignificantfailuresinalargenumberofinstancesfromthebenchmark. We
postulatethatthisisbecauseofmodelsmisinterpretingcontext. Itisstillanopenquestionastohow
toincreasetherobustnessofmodelstopreventorreducesuchsignificantfailures. Wevisualizesuch
significantfailuresinAppendixC.5.
Table4:Numberofinstanceswithsignificantfailuresinmodelsthatsupportcontext
Model Numberofinstanceswithsignificantfailures
DirectPrompt(ours)
Llama-3.1-405B-Inst 0
Llama-3-70B-Inst 1
Llama-3-8B-Inst 3
Mixtral-8x7B-Inst 5
GPT-4o 5
GPT-4o-mini 2
LLMP
Llama-3-70B-Inst 18
Llama-3-70B 0
Llama-3-8B-Inst 12
Llama-3-8B 0
Mixtral-8x7B-Inst 1
Mixtral-8x7B 1
MultimodalModels
UniTime 0
Time-LLM 2
26Preprint. Underreview.
C.4 VISUALIZATIONSOFSUCCESSFULCONTEXT-AWAREFORECASTS
Context:“Thisseriesrepresentstheoccupancyrate(%)capturedbyahighwaysensor.
Considerthatthemeterwillbeofflineformaintenancebetween2024-04-1113:00:00and2024-04-11
15:00:00,whichresultsinzeroreadings.”
SensorMaintenanceInPredictionTask
25
Forecast
History
Ground Truth
20 Region of Interest
5%-95%
10%-90%
15 25%-75%
10
5
0
(a)WithoutContext
SensorMaintenanceInPredictionTask
25
Forecast
History
Ground Truth
20 Region of Interest
5%-95%
10%-90%
15 25%-75%
10
5
0
(b)WithContext
Figure8:Exampleofsuccessfulcontext-awareforecastingbyDirectPromptwithLlama-3.1-405B-Instruct
27
50-40-4202
50-40-4202
60-40-4202
60-40-4202
70-40-4202
70-40-4202
80-40-4202
80-40-4202
90-40-4202
90-40-4202
01-40-4202
01-40-4202
11-40-4202
11-40-4202
21-40-4202
21-40-4202Preprint. Underreview.
Context:“ThisseriescontainsDiffuseHorizontalIrradianceforalocationinSinaloa,Mexico.The
DiffuseHorizontalIrradianceisthetotalamountofsunenergy(inWattspersquaredmeter)arriving
indirectlyonahorizontalsurface,ignoringthedirectsunlight. Evenwhentherearenocloudsto
scatterthesunlight,therewillstillbesomeDiffuseHorizontalIrradiance,sincecloudsarenotthe
onlycauseoflightscattering.Whentherearenoclouds,theDiffuseHorizontalIrradianceismostly
afunctionofthepositionofthesuninthesky,withonlysmallvariationsfromfactorssuchaswater
vapouranddustparticleslevels.Ifthecloudcoverislight,theDiffuseHorizontalIrradiancewill
increaseduetotheincreasescatteringofsunlight,butheavycloudcoverwilldecreaseitdueto
somesunlightnolongerbeingabletoreachtheground.
Atthebeginningoftheseries,theweatherwascloudy.
At2022-07-1211:00:00,theweatherbecameclear.
At2022-07-1219:00:00,theweatherbecamecloudy.
At2022-07-1312:00:00,theweatherbecameclear.
At2022-07-1313:00:00,theweatherbecamecloudy.
At2022-07-1406:00:00,weexpectthattheweatherwillbecomeclear.
At2022-07-1407:00:00,weexpectthattheweatherwillbecomecloudy.
At2022-07-1410:00:00,weexpectthattheweatherwillbecomeclear.
At2022-07-1418:00:00,weexpectthattheweatherwillbecomecloudy.”
ExplicitDiffuseHorizontalIrradianceFromCloudStatus
400 Forecast
History
350 Ground Truth
5%-95%
300 10%-90%
25%-75%
50%
250
200
150
100
50
0
(a)WithoutContext
ExplicitDiffuseHorizontalIrradianceFromCloudStatus
400 Forecast
History
350 Ground Truth
5%-95%
300 10%-90%
25%-75%
50%
250
200
150
100
50
0
(b)WithContext
Figure9:Exampleofsuccessfulcontext-awareforecastingbyDirectPromptwithLlama-3.1-405B-Instruct
28
00
31-70
00
31-70
21
31-70
21
31-70
00
41-70
00
41-70
21
41-70
21
41-70
00
51-70
00
51-70
21
51-70
21
51-70Preprint. Underreview.
Context:“Thisisthenumberofcashwithdrawalsfromanautomatedtellermachine(ATM)inan
arbitrarylocationinEngland.
ConsiderthatthebuildingwhichcontainstheATMisclosedfrom1997-09-0500:00:00,for8days.”
ATMBuildingClosedTask
60
50
40
30
20
10 Forecast
History
0 Ground Truth
Region of Interest
5%-95%
10 10%-90%
25%-75%
20
(a)WithoutContext
ATMBuildingClosedTask
60
Forecast
History
Ground Truth
50
Region of Interest
5%-95%
10%-90%
40
25%-75%
30
20
10
0
(b)WithContext
Figure10:Exampleofsuccessfulcontext-awareforecastsbyDirectPromptwithGPT-4o
29
40-7991
40-7991
50-7991
50-7991
60-7991
60-7991
70-7991
70-7991
80-7991
80-7991
90-7991
90-7991
01-7991
01-7991Preprint. Underreview.
Context: “ TheMontreal Fire Departmentis in chargeof respondingto various kindof public
safetyincidents. ThisisthenumberoffieldfireincidentsrespondedtobyMontrealfirefightersin
theRivie`re-des-Prairies-Pointe-aux-Tremblesborough.Inotheryears,theyearlyaveragenumberof
incidentswas106withthebusiestmonthbeingJune.
TheMayorisdeterminedtocompletelyeradicatethiskindofincident.Fortunately,thecity’spublic
safetyresearchgroupidentifiedthatfieldfiresandtrashfirestendtoco-occur. Whentheamount
offieldfiresincreases,theamountoftrashfiresalsotendstoincrease. Thesameholdswhenthey
decrease.
TheMayorhasaplan:theywillimplementdailysprayingofallpilesoftrashwithwaterstartingon
2022-06.”
MontrealFireFieldAndTrashNeutralToneImplicitCausalConfoundingTask
120 Forecast
History
Ground Truth
100 5%-95%
10%-90%
25%-75%
80 50%
60
40
20
0
(a)WithoutContext
MontrealFireFieldAndTrashNeutralToneImplicitCausalConfoundingTask
Forecast
17.5
History
Ground Truth
15.0 5%-95%
10%-90%
25%-75%
12.5
50%
10.0
7.5
5.0
2.5
0.0
(b)WithContext
Figure11:Exampleofsuccessfulcontext-awareforecastsbyDirectPromptwithGPT-4o
30
10-2202
10-2202
30-2202
30-2202
50-2202
50-2202
70-2202
70-2202
90-2202
90-2202
11-2202
11-2202Preprint. Underreview.
Context:“ThisistheUnemploymentRateforOkaloosaCounty,inFlorida.
Forreference,hereistheUnemploymentRateforafewAmericanstatesduringthesameperiod:
Pennsylvania
——————–
(2023-08-0100:00:00,4.2)
(2023-09-0100:00:00,3.0)
(2023-10-0100:00:00,3.1)
(2023-11-0100:00:00,2.9)
(2023-12-0100:00:00,2.9)
(2024-01-0100:00:00,3.5)
(2024-02-0100:00:00,3.7)
(2024-03-0100:00:00,3.4)
(2024-04-0100:00:00,2.9)
(2024-05-0100:00:00,3.2)
(2024-06-0100:00:00,3.7)
(2024-07-0100:00:00,4.0)
Florida
——————–
(2023-08-0100:00:00,3.3)
(2023-09-0100:00:00,3.1)
(2023-10-0100:00:00,3.1)
(2023-11-0100:00:00,3.0)
(2023-12-0100:00:00,2.9)
(2024-01-0100:00:00,3.1)
(2024-02-0100:00:00,3.1)
(2024-03-0100:00:00,3.3)
(2024-04-0100:00:00,3.1)
(2024-05-0100:00:00,2.9)
(2024-06-0100:00:00,3.5)
(2024-07-0100:00:00,3.8)
Wisconsin
——————–
(2023-08-0100:00:00,3.4)
(2023-09-0100:00:00,2.9)
(2023-10-0100:00:00,2.8)
(2023-11-0100:00:00,2.7)
(2023-12-0100:00:00,2.9)
(2024-01-0100:00:00,2.8)
(2024-02-0100:00:00,3.3)
(2024-03-0100:00:00,3.5)
(2024-04-0100:00:00,3.0)
(2024-05-0100:00:00,3.0)
(2024-06-0100:00:00,3.3)
(2024-07-0100:00:00,3.3)”
UnemploymentCountyUsingExplicitMultipleStateData UnemploymentCountyUsingExplicitMultipleStateData
3.8
Forecast Forecast
3.4 History History
Ground Truth 3.6 Ground Truth
5%-95% 5%-95%
10%-90% 10%-90%
3.2 25%-75% 3.4 25%-75%
50% 50%
3.0 3.2
3.0
2.8
2.8
2.6
2.6
(a)WithoutContext (b)WithContext
Figure12:Exampleofsuccessfulcontext-awareforecastsbyLLMPwithMixtral-8x7B-Instruct
31
80-3202 90-3202 01-3202 11-3202 21-3202 10-4202 20-4202 30-4202 40-4202 50-4202 60-4202 70-4202 80-3202 90-3202 01-3202 11-3202 21-3202 10-4202 20-4202 30-4202 40-4202 50-4202 60-4202 70-4202Preprint. Underreview.
Context:“Supposethatintheforecast,thevaluesareboundedbelowby0.80.”
OraclePredUnivariateConstraintsTask
Forecast
2.5 History
Ground Truth
5%-95%
2.0 10%-90%
25%-75%
50%
1.5
1.0
0.5
0.0
(a)WithoutContext
OraclePredUnivariateConstraintsTask
Forecast
2.5 History
Ground Truth
5%-95%
2.0 10%-90%
25%-75%
50%
1.5
1.0
0.5
0.0
(b)WithContext
Figure13:Exampleofsuccessfulcontext-awareforecastsbyLLMPwithMixtral-8x7B-Instruct
32
03-10-4202
03-10-4202
13-10-4202
13-10-4202
10-20-4202
10-20-4202
20-20-4202
20-20-4202
30-20-4202
30-20-4202
40-20-4202
40-20-4202
50-20-4202
50-20-4202
60-20-4202
60-20-4202Preprint. Underreview.
Context:“Thisseriescontainstheamountofsunlight(inWattspersquaredmeter)arrivingona
horizontalsurface,foralocationinAlaska,UnitedStates.”
LocaleInfoHalfDaySolarForecastTask
1750 Forecast
History
Ground Truth
1500
5%-95%
10%-90%
1250 25%-75%
50%
1000
750
500
250
0
(a)WithoutContext
LocaleInfoHalfDaySolarForecastTask
800 Forecast
History
700 Ground Truth
5%-95%
10%-90%
600
25%-75%
50%
500
400
300
200
100
0
(b)WithContext
Figure14:Exampleofsuccessfulcontext-awareforecastsbyLLMPwithLlama-70B
33
00
32-21
00
32-21
30
32-21
30
32-21
60
32-21
60
32-21
90
32-21
90
32-21
21
32-21
21
32-21
51
32-21
51
32-21
81
32-21
81
32-21
12
32-21
12
32-21Preprint. Underreview.
Context:“TheMontrealFireDepartmentisinchargeofrespondingtovariouskindofpublicsafety
incidents.ThisseriescontainsthenumberoffieldfireincidentsrespondedtobytheMontrealFire
DepartmentintheRosemont-LaPetite-Patrieborough.Onaverage,theyrespondto58incidentsper
yearandthemonthwiththemostincidentswasJune.
TheMayorisdeterminedtocompletelyeradicatethiskindofincident.Fortunately,thecity’spublic
safetyresearchgroup,ateamofhighlyqualifiedexperts,identifiedthatfieldfiresandgasleakstend
toco-occur.Whentheamountoffieldfiresincreases,theamountofgasleaksalsotendstoincrease.
Thesameholdswhentheydecrease.
TheMayorhasaplan:theywillimplementastrictprohibitionofusinganyformofcombustiblegas
inthecitystartingon2023-06.Inarecentinterview,theyclaimed,”Thisisabulletproofplan,andI
amcertainitwillimmediatelyputanendtofieldfires.””
MontrealFireFieldAndGasConvincingToneImplicitCausalConfoundingTask
Forecast
175 History
Ground Truth
5%-95%
150
10%-90%
25%-75%
125 50%
100
75
50
25
0
(a)WithoutContext
MontrealFireFieldAndGasConvincingToneImplicitCausalConfoundingTask
20.0 Forecast
History
17.5 Ground Truth
5%-95%
15.0 10%-90%
25%-75%
50%
12.5
10.0
7.5
5.0
2.5
0.0
(b)WithContext
Figure15:Exampleofsuccessfulcontext-awareforecastsbyLLMPwithLlama-70B
34
10-3202
10-3202
30-3202
30-3202
50-3202
50-3202
70-3202
70-3202
90-3202
90-3202
11-3202
11-3202Preprint. Underreview.
C.5 VISUALIZATIONSOFSIGNIFICANTFAILURES
Context: “GivenarevariablesX 0andX 1,whereX 0isacovariateandX 1isthevariableto
forecast.VariablesaregeneratedfromalinearStructuralVectorAutoregressive(SVAR)modelwith
additivegaussnoiseandanoisescaleof1.487e-03,withlag=3.
ThetaskistoforecastthevalueofthevariableX 1attimet,giventhevaluesofthecovariateX 0
andthevariableX 1itselfattimest-1, ... t-3. Forthefirst128days, thecovariateX 0takesa
valueof8from2024-02-21to2024-03-11,12from2024-03-12to2024-05-06,12from2024-05-07
to2024-06-27. Forthenext32days, thecovariateX 0takesavalueof30from2024-06-28to
2024-07-13,60from2024-07-14to2024-07-14,60from2024-07-15to2024-07-29.Eachdaycanbe
treatedasatimestepfortheforecastingtask.Thecausalparentsaffectthechildvariablesatdifferent
lags.
Thecausalparentsforeachvariableisgivenbelow:
NoparentsforX 0atanylag.
ParentsforX 1atlag1:[’X 0’,’X 1’]affecttheforecastvariableas0.527*X 0+-0.895*X 1.
ParentsforX 1atlag2:[’X 0’,’X 1’]affecttheforecastvariableas1.380*X 0+-0.758*X 1.
ParentsforX 1atlag3:[’X 0’,’X 1’]affecttheforecastvariableas-0.661*X 0+-0.793*X 1.”
FullCausalContextImplicitEquationBivarLinSVAR
Forecast
0.8 History
Ground Truth
5%-95%
10%-90%
0.6 25%-75%
50%
0.4
0.2
0.0
(a)WithoutContext
(b)WithContext
Figure16:ExampletoshowasignificantfailurecaseofDirectPromptwithGPT-4owhereitsperformance
worsenswithcontext
35
30-4202 40-4202 50-4202 60-4202 70-4202Preprint. Underreview.
Context:“ThisseriescontainstheroadoccupancyratesonafreewayintheSanFranciscoBayarea.
ThedaysforwhichtheforecastisrequiredareThursday2024-07-04,Friday2024-07-05,Saturday
2024-07-06.Notethat2024-07-04isaholidayduetoIndependenceDay.Notethattrafficonthis
freewaytypicallyreducesonholidays.”
ExplicitWithDatesAndDaysTrafficForecastTaskwithHolidaysInPredictionWindow
40
35
30 Forecast
History
25 Ground Truth
Region of Interest
20 5%-95%
10%-90%
15 25%-75%
10
5
(a)WithoutContext
ExplicitWithDatesAndDaysTrafficForecastTaskwithHolidaysInPredictionWindow
2000
Forecast
History
1750
Ground Truth
Region of Interest
1500 5%-95%
10%-90%
1250 25%-75%
1000
750
500
250
(b)WithContext
Figure17: ExampletoshowasignificantfailurecaseofLLMPwithLlama-3-70Bwhereitsperformance
worsenswithcontext
36
72-60-4202
72-60-4202
82-60-4202
82-60-4202
92-60-4202
92-60-4202
03-60-4202
03-60-4202
10-70-4202
10-70-4202
20-70-4202
20-70-4202
30-70-4202
30-70-4202
40-70-4202
40-70-4202
50-70-4202
50-70-4202
60-70-4202
60-70-4202Preprint. Underreview.
Context:“Thisseriesrepresentstheoccupancyrate(%)capturedbyahighwaysensor.Thesensor
hadacalibrationproblemstartingfrom2024-04-2013:00:00whichresultedinanadditivetrendin
theseriesthatincreasesby0.0072ateveryhour.Attimestep2024-04-2413:00:00,thesensorwas
repairedandthisadditivetrendwilldisappear.”
SensorTrendAccumulationTask
Forecast
2.5 History
Ground Truth
5%-95%
2.0 10%-90%
25%-75%
50%
1.5
1.0
0.5
0.0
(a)WithoutContext
SensorTrendAccumulationTask
2000
Forecast
History
1750
Ground Truth
5%-95%
1500 10%-90%
25%-75%
1250 50%
1000
750
500
250
0
(b)WithContext
Figure18: ExampletoshowasignificantfailurecaseofLLMPwithLlama-3-70Bwhereitsperformance
worsenswithcontext
37
81-40-4202
81-40-4202
91-40-4202
91-40-4202
02-40-4202
02-40-4202
12-40-4202
12-40-4202
22-40-4202
22-40-4202
32-40-4202
32-40-4202
42-40-4202
42-40-4202
52-40-4202
52-40-4202Preprint. Underreview.
Context:“TheMontrealFireDepartmentisinchargeofrespondingtovariouskindofpublicsafety
incidents.ThisseriescontainsthenumberoffieldfireincidentsrespondedtobytheMontrealFire
DepartmentintheL’Iˆle-Bizard-Sainte-Genevie`veborough.Onaverage,theyrespondto19incidents
peryearwiththebusiestmonthbeingJune.
TheMayorisdeterminedtocompletelyeradicatethiskindofincident.Fortunately,thecity’spublic
safetyresearchgroup,ateamofhighlyqualifiedexperts,identifiedthatfieldfiresandtrashfirestend
toco-occur.Whentheamountoffieldfiresincreases,theamountoftrashfiresalsotendstoincrease.
Thesameholdswhentheydecrease.
TheMayorhasaplan:theywillimplementdailysprayingofallpilesoftrashwithfireretardantfoam
startingon2023-06.Inarecentinterview,theyclaimed,”Thisisabulletproofplan,andIamcertain
itwillimmediatelyputanendtofieldfires.””
MontrealFireFieldAndTrashConvincingToneImplicitCausalConfoundingTask
2.00
Forecast
History
1.75
Ground Truth
5%-95%
1.50 10%-90%
25%-75%
1.25 50%
1.00
0.75
0.50
0.25
0.00
(a)WithoutContext
MontrealFireFieldAndTrashConvincingToneImplicitCausalConfoundingTask
10
Forecast
History
Ground Truth
8 5%-95%
10%-90%
25%-75%
50%
6
4
2
0
(b)WithContext
Figure19: ExampletoshowasignificantfailurecaseofDirectPromptwithLlama-3-8B-Instructwhereit
misinterpretsthecontext
38
10-3202
10-3202
30-3202
30-3202
50-3202
50-3202
70-3202
70-3202
90-3202
90-3202
11-3202
11-3202Preprint. Underreview.
C.6 COSTOFAPI-BASEDMODELS
Tab.5providesthecostincurredinevaluatingGPT-4o(versiongpt-4o-2024-05-13)andGPT-4o-mini
(version gpt-4o-mini-2024-07-18) with the Direct Prompt method on CiK (as per the evaluation
protocolused,describedinSec.5.1).
Table5: Costs($CAD)ofevaluatingtheGPT-4ofamilyofmodelsonCiK.“Total”representsthetotalcost
ofevaluatingeachmodelontheCiKbenchmark.The“Per-instanceaverage”andthe“Per-instancemedian”
aretheaverageandmediancostofrunningasingleinstanceforagiventask,inotherwordstheaverageand
mediancostofgenerating25sampletrajectoriesforagivenexampleofatask.Asareminder,eachtaskinCiK
isevaluatedover5instancesinourevaluationprotocol.
Model Total Per-instanceaverage Per-instancemedian
GPT-4o $143.83 $0.288 $0.170
GPT-4o(nocontext) $139.50 $0.279 $0.160
GPT-4o-mini $13.79 $0.040 $0.040
GPT-4o-mini(nocontext) $13.32 $0.038 $0.040
C.7 IMPACTOFIRRELEVANTINFORMATIONINCONTEXT
Manyofourtasksincludecovariatesinitscontextwhicharehighlyusefulforthemodelstoaccurately
predictthetargettimeseries. Onequestionis: DotheLLM-basedmodelsperformwellforsuch
tasksduetocorrectlyunderstandingthatsaidcovariatesarehelpfulorbecausetheyblindlyusethe
provideddatawithoutaskingthemselvesifthedataisactuallyrelevant?
Asawaytogetsomeinsightonthisquestion,wetookataskwherethemodelshavetoforecastthe
unemploymentdataofanAmericancounty,giventheunemploymentdataofthestatethecountyisin.
Wethenmodifythistaskbyfirsttryingtomisleadthemodelbywronglysayingthatthestate-level
datawasfromanotherstate(withoutchangingthedataitself),thenbygivingthedatafromtheother
state(whileexplicitlytellingthemodelthatdataisfromsaidotherstate),beforefinallyremovingthe
state-leveldataaltogether. Theresultforthisexperimentwith5instancespertaskforDirectPrompt
-GPT-4oisshowninTab.6,whiletheforecastsforasingleinstancesareshowninFig.20. From
these,weseethatthemodelaggressivelyuseddatawhichismarkedasbeingfromanotherstate,
eventhoughifthedatawasactuallyfromsaidotherstate,theperformancewouldbeclosertonot
havinganystate-leveldata. Thisshowsthatthemodelisliabletotakeanyinformationprovidedas
beinguseful,eventhoughitsusefulnessismarginal.
Table6: AbilityoftheDirectPrompt-GPT-4omodeltoaccuratelypredicttheunemployementlevelofan
Americancounty,givenvariouscovariates.Theseresultsareaveragedover5instances.
Availabledata RCPRS
Datafromthecorrectstate,accuratelytagged 0.0583
Datafromthecorrectstate,inaccuratelytagged 0.0557
Datafromanincorrectstate,accuratelytagged 0.1966
Nostate-leveldata 0.2630
39Preprint. Underreview.
UnemploymentCountyUsingSingleStateData UnemploymentCountyNoInfo
3.50 Forecast 3.2 Forecast
History History
3.25 Ground Truth 3.0 Ground Truth
5%-95% 5%-95%
3.00 10%-90% 2.8 10%-90%
25%-75% 25%-75%
2.75 50% 2.6 50%
2.50 2.4
2.2
2.25
2.0
2.00
1.8
1.75
1.6
1.50
(a)Thetaskinourbenchmark:thecontextcontainstheunem- (b)Thecontextonlymentionsthatthistimeseriesisanunem-
ploymentrateofthestatethecountyisin,correctlytaggedwith ploymentrate,andofwhichcountyitis. Nostate-levelunem-
thestatename. ployementdataisprovided.
UnemploymentCountyWrongState UnemploymentCountyWrongDataAndState
Forecast 3.25 Forecast
3.25 History History
Ground Truth 3.00 Ground Truth
3.00 5%-95% 5%-95%
10%-90% 10%-90%
25%-75% 2.75 25%-75%
2.75 50% 50%
2.50
2.50
2.25
2.25
2.00
2.00
1.75
1.75
1.50
1.50
(c)Thestate-levelunemploymentrateisincorrectlytaggedas (d)Thecontextcontainstheunemploymentrateofanotherstate
beingfromanotherstate. thantheonethecountyisin,whichiscorrectlytagged.
Figure20:ForecastsdonebyDirectPrompt-GPT-4o,withvaryinginformationinthecontext.Thetaskisto
forecasttheforecasttheunemploymentrateofanAmericancounty.
40
80-3202
80-3202
90-3202
90-3202
01-3202
01-3202
11-3202
11-3202
21-3202
21-3202
10-4202
10-4202
20-4202
20-4202
30-4202
30-4202
40-4202
40-4202
50-4202
50-4202
60-4202
60-4202
70-4202
70-4202
80-3202
80-3202
90-3202
90-3202
01-3202
01-3202
11-3202
11-3202
21-3202
21-3202
10-4202
10-4202
20-4202
20-4202
30-4202
30-4202
40-4202
40-4202
50-4202
50-4202
60-4202
60-4202
70-4202
70-4202Preprint. Underreview.
D IMPLEMENTATION DETAILS OF MODELS
D.1 DIRECTPROMPT
D.1.1 METHOD
For Direct Prompt, we propose to use a simple prompt template that we describe below, where
((context)) is replaced with the context of the respective task, ((history)) is replaced with the
historicalvaluesinthegivenformat,and((pred time))isreplacedwiththepredictiontimesteps.
Thepromptedmodelisexpectedtooutputpredictionsinthegiventemplatestyle(i.e. withinthe
givenforecasttags,inthegivenformat)forallpredictiontimestepsintheprompt. Notably,unlike
LLMPwhichconsistsofpredictingthesinglenextdigitinaloop,DirectPromptexpectsmodels
toforecastinasinglepassinahighlystructuredformat,whichrequiresmodelstounderstandand
adheretothetemplate.
”
I have a time series forecasting task for you.
Here is some context about the task. Make sure to factor in any background knowledge,
satisfy any constraints, and respect any scenarios.
<context>
((context))
</context>
Here is a historical time series in (timestamp, value) format:
<history>
((history))
</history>
Now please predict the value at the following timestamps: ((pred time)).
Return the forecast in (timestamp, value) format in between<forecast>and</forecast>tags.
Do not include any other information (e.g., comments) in the forecast.
Example:
<history>
(t1, v1)
(t2, v2)
(t3, v3)
</history>
<forecast>
(t4, v4)
(t5, v5)
</forecast>
”
Weobservethatmodelsoftenproducesamplesthatfailtoadheretothestructureandaretherefore
rejected. Whensampling25samplesfromthemodel,withDirectPrompt,weallowretryingfora
maximumofK timesuntilweobtain25validsamples. Ifwedonothave25validsamplesfromthe
modelattheendofK retries,werecordafailureofthemodelandattributethemodeltheRCRPS
upperboundof5forthattask. Inpractice, wefindthatlargermodels(Llama-3.1-405B-Instruct,
GPT-4oandGPT-4o)canproduce25validforecastswith1to3retries. Howeverwiththesmaller
models(suchasLlama-3-70B-Instruct,Llama-3-8B-InstructandMixtral-8x7B-Instruct),upto10
retriescanberequiredtoobtain25validforecasts. Further,wefoundthatwithoutanexplicit“Donot
includeanyotherinformation(e.g.,comments)intheforecast.”,modelsoftenincludedunwanted
informationalongwiththeforecasts.
Instruction-tuning is necessary for models to work with Direct Prompt Direct Prompting
requiresforecaststobeproducedinaspecificstructure. Togeneratestructuredoutputs,modelsneed
to be steerable (Dubey et al., 2024), a capability that is typically elicited from base models with
post-trainingmethodssuchasinstructiontuning(Weietal.,2021). Weobservethisinourevaluations
aswefindthatseveralbasemodels,includingLlama-3-8B,Llama-3-70B,Mixtral-8x7B,andeven
41Preprint. Underreview.
thebiggestbasemodelwetried,Llama-3.1-405B,areincapableofgeneratingoutputsadheringtothe
structurerequiredforDirectPrompt,despiteincreasingthenumberofretriestoashighas50retries.
WithDirectPrompt,thesemodelsoftenoutputirrelevantinformation,sometimescompletingsolely
thecontextasatextcompletiontask,andinothercasesregurgitatingforecastingdatasetsthatthey
havememorized.
ExtensionsofDirectPrompt Whileverysimple,suchprompttemplatescanbepowerfultools
tounderstandhowLLMsperformcontext-aidedforecasting: asthepromptgivescontroloverthe
structureandcontentoftheoutput(particularlyforinstruction-tunedmodels),onemayconstruct
other,moreinvolvedtemplatestructuresintheprompt. Forinstance,aprompttemplatecouldask
LLMstoexplainthereasoningbehindtheir(context-aided)forecasts,andmore. Weleaveittofuture
worktounderstandhowsuchprompt-basedtechniquescanleadtomoredetailedevaluationsand
giveusbetterinsightsintowhatthemodelsarecapableof.
D.1.2 IMPLEMENTATIONDETAILS
WeusedasingleH100GPUtoruntheDirectPromptapproachforLlama-3-8B-Instruct,and2H100
GPUsforLlama-3-70B-InstructandMixtral-8x7B-Instruct. WequeriedLlama-3.1-405b-Instruct
fromanexternally-hostedserverrunningon8H100s. WeusetheOpenAIAPItoperforminference
ontheproprietaryGPT-4oandGPT-4o-minimodels. Weprovidethecostincurredintheinferenceof
thesemodelswiththeDirectPromptmethodinAppendixC.6.
D.1.3 EXAMPLEPROMPT
Apromptusedinanexampletaskfromthebenchmarkisgivenbelow.
42Preprint. Underreview.
”
I have a time series forecasting task for you.
Here is some context about the task. Make sure to factor in any background knowledge,satisfy
any constraints, and respect any scenarios.
<context>
Background: This is hourly traffic data.
Scenario: Suppose that there is an accident on the road and there is 40.0% of the usual
traffic from 2024-04-24 17:00:00 for 6 hours.
</context>
Here is a historical time series in (timestamp, value) format:
<history>
(2024-04-23 00:00:00, 0.1)(2024-04-23 01:00:00, 0)(2024-04-23 02:00:00, 0)(2024-04-23
03:00:00, 0)(2024-04-23 04:00:00, 0.1)(2024-04-23 05:00:00, 0.2)(2024-04-23 06:00:00,
0.3)(2024-04-23 07:00:00, 0.5)(2024-04-23 08:00:00, 0.5)(2024-04-23 09:00:00, 0.4)
(2024-04-23 10:00:00, 0.5)(2024-04-23 11:00:00, 0.5)(2024-04-23 12:00:00, 0.4)
(2024-04-23 13:00:00, 0.6)(2024-04-23 14:00:00, 0.8)(2024-04-23 15:00:00, 1.2)
(2024-04-23 16:00:00, 1.2)(2024-04-23 17:00:00, 1.3)(2024-04-23 18:00:00, 0.6)
(2024-04-23 19:00:00, 0.3)(2024-04-23 20:00:00, 0.3)(2024-04-23 21:00:00, 0.3)
(2024-04-23 22:00:00, 0.1)(2024-04-23 23:00:00, 0.1)(2024-04-24 00:00:00, 0.1)
(2024-04-24 01:00:00, 0)(2024-04-24 02:00:00, 0)(2024-04-24 03:00:00, 0.1)(2024-04-24
04:00:00, 0.1)(2024-04-24 05:00:00, 0.2)(2024-04-24 06:00:00, 0.3)(2024-04-24 07:00:00,
0.5)(2024-04-24 08:00:00, 0.6)(2024-04-24 09:00:00, 0.5)(2024-04-24 10:00:00, 0.4)
(2024-04-24 11:00:00, 0.5)(2024-04-24 12:00:00, 0.6)
</history>
Now please predict the value at the following timestamps: [’2024-04-24 13:00:00’ ’2024-04-24
14:00:00’ ’2024-04-24 15:00:00’ ’2024-04-24 16:00:00’ ’2024-04-24 17:00:00’
’2024-04-24 18:00:00’ ’2024-04-24 19:00:00’ ’2024-04-24 20:00:00’ ’2024-04-24 21:00:00’
’2024-04-24 22:00:00’ ’2024-04-24 23:00:00’ ’2024-04-25 00:00:00’ ’2024-04-25
01:00:00’ ’2024-04-25 02:00:00’ ’2024-04-25 03:00:00’ ’2024-04-25 04:00:00’ ’2024-04-25
05:00:00’ ’2024-04-25 06:00:00’ ’2024-04-25 07:00:00’ ’2024-04-25 08:00:00’
’2024-04-25 09:00:00’ ’2024-04-25 10:00:00’ ’2024-04-25 11:00:00’ ’2024-04-25
12:00:00’].
Return the forecast in (timestamp, value) format in between<forecast>and</forecast>tags.
Do not include any other information (e.g., comments) in the forecast.
Example:
<history>
(t1, v1)
(t2, v2)
(t3, v3)
</history>
<forecast>
(t4, v4)
(t5, v5)
</forecast>
”
D.2 LLMP
InthissectionweoutlineLLM-processes(LLMP;Requeimaetal.(2024)),oneoftheprompt-based
baselines evaluated in Sec. 5.3. Prompts are constructed by first providing textual information
followed by the numerical history. The context may include background knowledge, a scenario
descriptionandtaskconstraints, replacedby((background)), ((scenario))and((constraints)),
respectively, in the prompt template below. The numerical history (((history))) is provided by
convertingthenumericaldatatotextwherevaluesareseparatedbycommas(,)andtuplesbynewline
characters(\n). TheLLMthenoutputsthecontinuationofthestringprompt,forecasingthethevalue
43Preprint. Underreview.
forthenexttimeindex(((nextindex))). Thisforecastandthenexttimeindexisappendedtothe
promptallowingtheLLMtoautoregressivelycompletetheentireforecast. Numericalsamplesare
rejectediftheydonotadheretoadecimalrepresentationformat. SeeRequeimaetal.(2024))forfull
details.
ThefollowingistheprompttemplateusedtoconstructpromptsfortheLLMPbaseline:
”
Forecast the future values of this time series, while considering the following background
knowledge, scenario, and constraints.
Background knowledge:
((background))
Scenario:
((scenario))
Constraints:
((constraints))
((history))
((next index))
”
Apromptusedinanexampletaskfromthebenchmarkisgivenbelow:
”
Forecast the future values of this time series, while considering the following background
knowledge, scenario, and constraints.
Background knowledge:
This is hourly traffic data.
Scenario:
Suppose that there is an accident on the road and there is 40.0% of the usual traffic from
2024-04-24 17:00:00 for 6 hours.
Constraints:
2024-04-23 00:00:00,0.1\n2024-04-23 01:00:00,0\n2024-04-23 02:00:00,0\n2024-04-23 03:00:00,0
\n2024-04-23 04:00:00,0.1\n2024-04-23 05:00:00,0.2\n2024-04-23 06:00:00,0.3\n2024-04-23
07:00:00,0.5\n2024-04-23 08:00:00,0.5\n2024-04-23 09:00:00,0.4\n2024-04-23
10:00:00,0.5\n2024-04-23 11:00:00,0.5\n2024-04-23 12:00:00,0.4\n2024-04-23 13:00:00,0.6
\n2024-04-23 14:00:00,0.8\n2024-04-23 15:00:00,1.2\n2024-04-23 16:00:00,1.2\n2024-04-23
17:00:00,1.3\n2024-04-23 18:00:00,0.6\n2024-04-23 19:00:00,0.3\n2024-04-23
20:00:00,0.3\n2024-04-23 21:00:00,0.3\n2024-04-23 22:00:00,0.1\n2024-04-23 23:00:00,0.1
\n2024-04-24 00:00:00,0.1\n2024-04-24 01:00:00,0\n2024-04-24 02:00:00,0\n2024-04-24
03:00:00,0.1\n2024-04-24 04:00:00,0.1\n2024-04-24 05:00:00,0.2\n2024-04-24 06:00:00,0.3
\n2024-04-24 07:00:00,0.5\n2024-04-24 08:00:00,0.6\n2024-04-24 09:00:00,0.5\n2024-04-24
10:00:00,0.4\n2024-04-24 11:00:00,0.5\n2024-04-24 12:00:00,0.6\n2024-04-24 13:00:00,
”
We used a single H100 GPU to run the LLMP approach for the following models: Llama-3-8B,
and Llama-3-8B-Instruct. We used 2 H100 GPUs for the following models: Mixtral-8x7B, and
Mixtral-8x7B-Instruct,andusedused8H100GPUsforthefollowingmodels: Llama-3-70B,and
Llama-3-70B-Instruct.
D.3 UNITIMEANDTIME-LLM
Formultimodalmodels, wejointlytrainUniTime(Liuetal.,2024b)onitsensembleofdatasets:
ETTm1,ETTm2,ETTh1,ETTh2,Electricity,Weather,ExchangeandIllness.
44Preprint. Underreview.
WealsoevaluateTime-LLM(Jinetal.,2024),anothermultimodalmodelbuiltontopoftheLlama
architecture. WetrainTime-LLMonETTh1accordingtotheauthors’suggestedspecifications,and
wecomparetheperformanceofbothmodelswithandwithoutcontext.
UniTime: WetrainUniTime(Liuetal.,2024b)usingasingleseedononeAMDInstinctMI200
GPUforapproximately14hours. Itfeaturesalightweighttransformerwithmaximumcontextlength
of 210 and a pre-trained GPT2 language model as backbone, of which only the first half of the
transformerlayersareused. Thetimeseriesbaselineemploysnon-overlappingpatchembeddings
generatedwithakernelsizeandstrideof16,andamaximuminputsequencelengthof96. Whenthe
totaltokenizedlengthexceedsthearchitecture’scapacity,wetruncatethecontext.
UnlikeTime-LLM,UniTimeisjointlytrainedonalldatasetssimultaneously. Batchesweregenerated
byfirstchoosingadatasetuniformlyatrandomthenreturningabatchfromtheassociateddataloader.
Toaccountfordomainconvergencespeedimbalance,amaskrateof0.5isusedandthetrainingbatch
sizeisvariedaccordingtothedataset(detailsinthedataconfigdirectoryoftheUniTimeGitHub
repository). Trainingwasconductedfor10epochsofthemixeddataset,withcosinedecayfroman
initiallearningrateof1e-4toaminimumof1e-6overamaximumperiodof20epochs. Theresults
ofourtrainingontheoriginaldatasetsaregiveninTab.7.
Finally, in order to accelerate training, we added BF16 automatic mixed precision training and
gradientaccumulationtotheoriginaltrainingprocedure.
Time-LLM: WetrainTime-LLM(Jinetal.,2024)ontheETTh1dataset(Zhouetal.,2021)witha
predictionlengthof96. WetrainusingasingleseedonfourAMDInstinctMI200GPUs,withan
averagetrainingtimeperrunofapproximately13hours. Trainingwasconductedusingabatchsize
of8perdeviceand4gradientaccumulationsteps,alongwitha1Cyclelearningrateschedulewitha
maximumlearningrateof1e-3. Inaddition,runswereacceleratedusingDeepSpeedStage2and
BF16automaticmixedprecision.
Trainingwasconductedoveramaximumof50epochswithearlystopping,andatime-basedsplit
of70%fortraining,10%forvalidation,and20%fortesting,wherethemostrecentwindowswere
reservedforthetestset. Allrunsweretrainedwithaninputsequencelengthof512,withoverlapping
patchembeddingsgeneratedwithakernelsizeof16andastrideof8. TheresultsontheETTh1test
setaregiveninTab.8.
WhenevaluatingonCiKtaskswhichdonotconformtoTime-LLM’srequirements,wemakethe
followingmodificationstothemethod:
• Forshorthistorytaskswherethehistorylength|X H|islessthan5,wechangethetopk
operator’skvaluefrom5to|X H|inthecalculatelags()function.
• Fortaskswherethelengthofthepredictionwindow|X |exceedsthetrainedprojection
F
head’soutputdimension(inourcase,96),werepeatthelastpredictedvalue|X |−96times.
F
Thisoccursforveryfewtasks(3tasks)withpredictionwindowsof97or98stepsdepending
onthesampledinstance,whichweassumeleadstoanegligibleimpactonevaluatedresults.
Table7: EvaluationresultsforUniTimeontheirtestsplits. Resultsarecomparabletotheoriginalpaper,
althoughMSEonIllnessisapproximately20%higherforpredictionlengths36,48,60.
Dataset MeanSquaredError(MSE)
PredictionLength 96 192 336 720
ETTh1 0.395 0.435 0.469 0.468
ETTh2 0.291 0.368 0.413 0.422
ETTm1 0.336 0.377 0.409 0.465
ETTm2 0.181 0.248 0.315 0.417
Exchange 0.090 0.180 0.322 0.862
Weather 0.179 0.224 0.278 0.354
Electricity 0.198 0.202 0.217 0.257
24 36 48 60
Illness 2.284 2.515 2.572 2.455
45Preprint. Underreview.
Table8:ETTh1testsetresultsforTime-LLMtrainedonETTh1.
Time-LLM MSE MAE
ETTh1-pl96 0.3846123 0.4149854
WhyDoTime-LLMandUniTimeNotBenefit(More)FromContext? LookingattableAp-
pendix C.2, we see that context actually harms the performance of Time-LLM’s forecasts. Two
possiblereasonsforthisare: 1)Time-LLM’sadaptationprocedureisunlikelytoretainthebackbone
LLM’slanguage-processingcapabilities,and2)Time-LLM’ssingle-datasettrainingprocedureis
unlikelytogeneralizetounseentimeseriespatterns. PartofTime-LLM’smodeladaptationinvolves
traininglinearlayersattheinputandoutputofthelanguagemodel. AlthoughthebackboneLLM
remains frozen, these linear layers must be trained, and Time-LLM opts for a highly structured
promptingformatwhichinvolvesdomainknowledge,taskinstructionsandinputstatistics. Sincethe
trainingdataforthelinearlayersconsistsofoutputrepresentationsbasedonthesehighlystructured
prompts,itisnotevidentthattheresultingarchitecturewillgeneralizetomorediversecontextual
descriptionssuchasthosefoundinCiK.Furthermore, althoughwehavenotconductedaformal
analysisofthediversityoftheETTh1dataset,itisnotaprioriobviousthatsuchadatasetwouldhave
asufficientdiversityofpatternstotrainatimeseriesfoundationmodel.
Interestingly,UniTime’sperformancedoesbenefitfromcontextforsometasks(seeFig.21).However,
theaggregateRCRPSandrankofUniTimewithrespecttoothermodelsindicatethatitstillstruggles
toproduceforecastscompetitivewithevenquantitativeforecastingmethods.
Context:“Supposethatintheforecast,thevaluesareboundedaboveby6.29.”
Figure21:AcomparisonofforecastsfromUniTimewithoutcontext(left)andwithcontext(right).Onaverage
across5instances,UniTime’sRCRPSis64%betterwithcontextthanwithoutonthe“BoundedPrediction
ConstraintBasedOnPredictionQuantiles”task.
D.4 LAG-LLAMA
WeusethepubliclyavailableimplementationofLag-Llama(Rasuletal.,2023)locatedathttps:
//github.com/time-series-foundation-models/, and its associated pre-trained weights. The
modelinferencewasdoneonasingleH100GPU.
D.5 CHRONOS
We use the publicly available implementation of Chronos (Ansari et al., 2024) located at https:
//github.com/amazon-science/chronos-forecasting.Weevaluated(seeAppendixC.2)ourtasks
onall5availablemodels:chronos-tiny,chronos-mini,chronos-small,chronos-baseandchronos-large,
andreportedtheresultsofthebestperformingmodel,chronos-largeinTab.1. Themodelinference
wasdoneonasingleH100GPU.
46Preprint. Underreview.
D.6 MOIRAI
We use the publicly available implementation of Moirai (Woo et al., 2024) located at https:
//github.com/SalesforceAIResearch/uni2ts. We evaluated (see Appendix C.2) our tasks on
the 3 following models: moirai-1.0-R-small (located at https://huggingface.co/Salesforce/
moirai-1.0-R-small), moirai-1.0-R-base (located at https://huggingface.co/Salesforce/
moirai-1.0-R-base) and moirai-1.0-R-large (located at https://huggingface.co/Salesforce/
moirai-1.0-R-large)andreportedtheresultsofthebestperformingmodel,moirai-1.0-R-largein
Tab.1. ThemodelinferencewasdoneonasingleH100GPU.
D.7 TIMEGEN
WeaccessTimeGEN-1,anoptimizationoftheTimeGPTmodel(Garzaetal.,2023),usingtheAPI
madeavailablethroughthenixtlaPythonpackage. Unlikeallotherbaselines,weonlygenerate
pointforecastswithTimeGENduetoitsprobabilisticmoderequiringmuchlongerhistoricaldata
thanisavailableininstancesevaluatedinthebenchmark. ThisisthereasontheRCPRSvaluesfor
TimeGENhavezerostandarderror.
D.8 EXPONENTIALSMOOTHING
WeusedtheExponentialSmoothingimplementationfromthestatsmodelsPythonpackage,namely
thestatsmodels.tsa.holtwinters.ExponentialSmoothingclass. Bothtrendandseasonalcompo-
nentsofthemodelsaresettobeadditive. Theseasonalperiodlengthiseithersetmanuallyfortasks
wherethesimpleguessusingthetimeseriesfrequencyisincorrect. Ifthereisnotatleasttwofull
seasonalperiodsinthehistorywindowofthetimeseries,wedisabletheseasonalcomponentofthe
model. Sincesomeofthebenchmarktaskscanhaveasfewas3timestepsinthehistorywindow,we
alsodisablethetrendcomponentifwehavelessthan5timestepsinsaidwindow.
D.9 ETSANDARIMA
WeusedtheimplementationsofETSandARIMAfromtheforecastRpackage,usingrpy2for
compatibilitywithPython. ForETS,weusetheetsmethod,whichwecallwithautomaticerror,
trend,andseasonalitycomponents. IntherarecaseswheretheETSforecastcontainsNaNvalues,we
manuallyswitchoffthetrendcomponentandreruntheforecast. TheARIMAresultsarecomputed
using the auto.arima method. If the ARIMA fits fail, we rerun it with restricted parameter and
disabledseasonality.
E DETAILS OF THE PROPOSED METRIC
TheCiKbenchmarkisdesignedtodeterminewhethermodelscanimprovetheirprobabilisticforecasts
byleveragingassociatedtextualinformation(seeSec.2). Tosupportthisgoal,theevaluationmetric:
1. shouldbeaproperscoringrule,suchthatamodelwhoperfectlyknowswhatthecorrect
forecastisshouldhavenoreasontofavoranotherprediction;
2. mustbeeasytocomputeusingafinitesamplefromtheforecastdistribution,sincemany
modelsdonotprovideafunctionalformfortheirforecasts.
Toaccountfortheimportanceofleveragingrelevantcontext,themetricshouldalso:
1. penalizeobviouslyimpossibleforecasts,i.e. thatcanbeinferredasimplausiblefromthe
contextualinformation;
2. takeasimilarrangeofvaluesacrossdifferenttasks,topreventsometaskstodominate
thescoreasweaveragetheresultsacrosstasks;
3. prioritizeforecastqualityfortimestepswithrelevantcontext,evenifthesetimestepsare
asmallportionoftheforecasthorizon.
47Preprint. Underreview.
To satisfy the first two properties, we start with the Continuous Ranked Probability Score
(CRPS) (Gneiting & Raftery, 2007), a reliable strictly proper scoring rule for univariate proba-
bilitydistribution,andtakeitsmeanoveralltimesteps. TocomputetheCRPSfromafinitenumber
ofsamples,weusetheestimatorbasedonitsprobabilityweightedmomentform(Taillardatetal.,
2016),sinceitisunbiased(Zamo&Naveau,2018). SeeAppendixE.3formoredetailsaboutthis
estimator.
ManyofourtasksarebuilttoincludeinformationaboutahardconstraintonX intheirC,which
F
canbewrittenasv (x )=0. Ifwewereonlyinterestedtomeasurebyhowmuchaforecastbreaks
C F
theconstraint,wecouldtakeinspirationfromthethreshold-weightedCRPS(Gneiting&Ranjan,
2011)byusingv asitschainingfunction(Allenetal.,2023):
C
(cid:16) (cid:17)
twCRPS vC(X(cid:101)F,x F)≡CRPS v C(X(cid:101)F),v C(x F) , (1)
whereX(cid:101)F istheforecastofX
F
tobeevaluated. Since,byconstruction,theground-truthx
F
always
satisfytheconstraints,wehavev (x )=0. Butsincewedonotcareonlyaboutwhetherforecasts
C F
breakconstraints,wesumboththeoriginalCRPSandthistwCRPS,butweweightthelaterbya
factorofβ =10,todenotetheadditionalinterestweshowtotheseerrors. SeeAppendixE.4forthe
variousv usedinthebenchmark.
C
OnecommonapproachtonormalizetheCRPStogetsimilarrangesformultipleproblemsistodivide
itbythemeanabsolutevalueofthetargetground-truthoftheforecastedseries(Alexandrovetal.,
2020). Thishastwoissues: themetricisnolongerproper,anditleadstomuchlargervaluesfor
seriesclosetozerothanthosefarfromit. Tosolvethefirstissue,wetakeadvantagethatwecan
generatemanymoreinstancesfromeachofourtasks,bycomputinganormalizationfactorαfrom
25instancesnotincludedinthebenchmark. ThedetailsofthiscalculationsareinAppendixE.1.
Manytasksinourbenchmarkcontainscontextualinformationwhichishighlyrelevantforasmall
fractionofthetimestepsintheforecastingwindow,whilebeingonlymarginallyrelevantforthe
majority of the time steps. If we were to weight these two categories equally, then the score for
amodelwhichignoresthecontextwouldbehardtodistinguishfromthescoreofonewhodoes
not. Wecorrectthisissuebyidentifyingthesubsetoftimestepswithrelevantinformation,which
wecalltheRegionofInterest(RoI).WethenweighttheCRPStogivehalfweighttotheRoItime
stepsandhalfweighttothenon-RoItimesteps. Therefore,weobtainourmetric,whichwecallthe
Region-of-InterestCRPS(RCRPS):
 α ·(cid:20) 2|1 I|·(cid:80)CRPS(cid:16) X(cid:101)i,x i(cid:17) + 2|¬1 I|· (cid:80) CRPS(cid:16) X(cid:101)i,x i(cid:17) +β· CRPS(cid:16) v C(X(cid:101)F),0(cid:17)(cid:21) if|I|>0
RCRPS(X(cid:101)F,x F):=
α
·(cid:20)
|¬1
I|·i∈ (cid:80)I CRPS(cid:16)
X(cid:101)i,x
i(cid:17)
+β·
CRi∈ P¬ SI (cid:16)
v
C(X(cid:101)F),0(cid:17)(cid:21)
, if|I|=0
i∈¬I
whereI isthesetoftimestepsintheRoI,¬I isthesetoftimestepsintheforecastbutnotinthe
RoI,αistheaforementionedscalingfactor,andwedropthefactoroftwoandthefirstsumfortasks
wherethereisnomeaningfulRoI.
E.1 SCALINGFORCROSS-TASKAGGREGATION
TherationalebehindscalingtheRCPRSistoallowustoaverageitsvaluefromdiversetaskswithout
the average being dominated by the forecast quality for tasks with time series with large values.
Analternativeargumentis: allotherconditionsbeingequal,aforecasterthatiswrongby10inits
forecastforatimeserieswhichgoesfrom25to30isworsethanonethatiswrongby100inits
forecastforatimeserieswhichgoesfrom2500to3000. Furthermore,wehavemultipletasksfor
whichsomeinstanceshaveconstantx ornearlyso,oftenwithvaluesclosetozero. Duetothese
F
tasks,wecannotsimplyuseascalingwhichonlydependsonsaidinstancesx . Instead,wetake
F
advantageofourbenchmarkabilitytogenerateaverylargenumberofinstancesforeachtasksby
usingM =25instancesnotincludedinourbenchmark. Giventheground-truthfuturevaluesxmfor
F
theseinstance,thescalingfactorβ foranindividualtaskisasfollow:
(cid:20)(cid:80) (max xm−min xm)(cid:21)−1
α= m i i i i . (2)
M
48Preprint. Underreview.
Properness Inanidealscenario,allinstancesofataskswouldbefullyindependent. Inthatcase
thenEq.(2)wouldnotcontainanyinformationaboutthetargettimeseriesinthebenchmarkinstances,
makingtheRCPRSaproperscoringrule. However,duetopossibleoverlapsinthetimewindows
usedwhencreatingtheinstancesandtoauto-correlations,wecannotguaranteeindependencebetween
instances,andthuswecannotguaranteethattheRCPRSisactuallyaproperscoringrule. Notethat
thisdeviationfromaproperscoringruleisminor,andhasamuchsmallereffectthantheonedueto
thecommonapproachofnormalizingtheCRPSusingtheMeanAbsoluteValueoftheground-truth.
E.2 CRPSANDTWCRPS
GivenaunivariateforecastX(cid:101) andaground-truthrealizationx,theContinuousRankedProbability
Score(CRPS)canbedefinedinitsintegralasfollow:
(cid:90) ∞
CRPS(X(cid:101),x)= dy(cid:2) Φ (y)−1(y ≥x)(cid:3)2 , (3)
X(cid:101)
−∞
whereΦ (y)istheCumulativeDistributionFunctionofX(cid:101),and1istheindicatorfunction.
X(cid:101)
TherearemultiplewaystocomputetheCRPS,butaparticularlyinterestingonewhichshowcasesits
linktotheMeanAbsoluteErroristheenergyformoftheCRPS:
1
CRPS(X(cid:101),x)=E |X−x|− E |X−X′|. (4)
X∼X(cid:101) 2 X,X′∼X(cid:101)
Wegetthethreshold-weightedCRPS(twCRPS)fromEq.(4)byaddingaweightingfunctionw(x)to
it:
(cid:90) ∞
twCRPS(X(cid:101),x)= dyw(y)(cid:2) Φ (y)−1(y ≥x)(cid:3)2 . (5)
X(cid:101)
−∞
TogettheenergyformofthetwCRPS,wemustcomputethechainingfunctionv(x)fromw(x):
(cid:90)
v(x)−v(x′)= dyw(y). (6)
[x,x′)
Usingv(x),wecanwritethetwCRPSas:
1
twCRPS(X(cid:101),x)=E |v(X)−v(x)|− E |v(X)−v(X′)|. (7)
X∼X(cid:101) 2 X,X′∼X(cid:101)
Eq.(7)canreadilybegeneralizedtoamultivariateforecast,byusinganyRd →Rchainingfunction.
E.3 ESTIMATINGTHECRPSUSINGSAMPLES
Computing the CRPS using Eq. (3) or Eq. (4) directly would be extremely hard for most of the
baselines included in our experiments. Instead, it is more computationally convenient to use an
estimator of the CRPS which uses a finite number of samples x , ..., x from the forecasting
1 M
distribution. AnunbiasedestimatoroftheCRPScreatedfromEq.(4)is:
M M M
1 (cid:88) 1 (cid:88) (cid:88)
CRPS(X(cid:101),x)≈
M
|x n−x|−
2M(M −1)
|x n−x n′|. (8)
n=1 n=1n′=1
However,thisestimatorisrelativelycostly,havingaO(M2)timecomplexity.
AfasterestimatorwhichgivesthesameresultasEq.(8)(uptonumericalaccuracy)istheonebased
ontheprobabilityweightedmomentformoftheCRPS(Taillardatetal.,2016;Zamo&Naveau,
2018):
M M M
1 (cid:88) 1 (cid:88) 2 (cid:88)
CRPS(X(cid:101),x)≈
M
|x n−x|+
M
x n−
M(M −1)
(n−1)x n, (9)
n=1 n=1 n=1
where the x have been sorted in ascending order. We used Eq. (9) in our metric, since it is as
n
accurateasEq.(8),whileonlyhavingaO(MlogM)timecomplexity.
49Preprint. Underreview.
E.4 CONSTRAINT-VIOLATIONFUNCTIONS
Inselectingconstraint-violationfunctionsv forourvarioustasks,wehavethefollowingrequire-
C
ments: itshouldbeinvarianttothenumberoftimestepsintheforecastingwindowanditshouldbe
multipliedbyαifallnumericaldatainataskistransformedusingx→αx+β. Herearethev we
C
useinsomeofourbenchmarktasks:
• Constantupper-boundconstraintx ≤τ+:
i
T
1 (cid:88)
v (x )= max(0,x −τ+),
C F T −t i
i=t+1
• Constantlower-boundconstraintx ≥τ−:
i
T
1 (cid:88)
v (x )= max(0,τ−−x ),
C F T −t i
i=t+1
• Constantlower-boundandupper-boundconstraintsτ− ≤x ≤τ+:
i
T
1 (cid:88)
v (x )= max(0,τ−−x )+max(0,x −τ+),
C F T −t i i
i=t+1
• andVariableupper-boundconstraints,onasubsetoftimestepsx ≤τ+∀i∈C:
i i
1 (cid:88)
v (x )= max(0,x −τ+).
C F |C| i i
i∈C
E.5 COVARIANCEOFTWOCRPSESTIMATORS
OneapproachtocomputestandarderrorontheRCRPSistocomputetheempiricalstandarddeviation
based on the 5 instances we use for each task. However, such a method would overestimate the
standarderror,sinceitwouldconsiderboththevariancecomingfromtheselectionofinstancesofa
giventask,andthevariancecomingfromthemodelssamplingprocesses. Sinceallmodelsaretested
usingtheexactsameinstances,thevariancecomingfromtheirselectionisnotrelevant,andthuswe
needawaytoignoreit.
Todoso,wetakeadvantagethattheRCRPSisaweightedsumofmultipleCRPSestimates. Since
thoseestimatesarenotindependentfromoneanother,wecancomputeanestimateofthevarianceof
theRCPRSunderthesamplingprocessbycomputinganestimateofthecovariancematrixbetween
thevariousCRPSestimates,followedbytheappropriateweightedsum.
Let says we want to compute the covariance between the CRPS for variable i and the CRPS for
variablej,usingM independentandidenticallydistributedsamplesfromthejointdistributionofX(cid:101)i
andX(cid:101)j.
(cid:16) (cid:16) (cid:17) (cid:16) (cid:17)(cid:17)
Cov CRPS X(cid:101)i,x
i
,CRPS X(cid:101)j,x
j
=
(cid:32)
1 (cid:88) 1 (cid:88)
Cov
M
|X(cid:101)i,n−x i|−
2M(M −1)
|X(cid:101)i,n−X(cid:101)i,n′|,
n n̸=n′
(cid:33)
1 (cid:88) 1 (cid:88)
M
|X(cid:101)j,n−x j|−
2M(M −1)
|X(cid:101)j,n−X(cid:101)j,n′| ,
n n̸=n′
wherethesumsareoverthevarioussamplesnandx andx andtheground-truthvalues.
i j
50Preprint. Underreview.
Aftersometediousalgebraicmanipulations,weobtainthefinalformulaforthecovarianceoftwo
CRPSestimates:
(cid:16) (cid:16) (cid:17) (cid:16) (cid:17)(cid:17)
Cov CRPS X(cid:101)i,x
i
,CRPS X(cid:101)j,x
j
=
1
−
M
E|X(cid:101)i−x i| E |X(cid:101) j′ −x j|
X(cid:101)i X(cid:101)j′
1
+
M
E|X(cid:101)i−x i| E E |X(cid:101) j′ −X(cid:101) j′′|
X(cid:101)i X(cid:101)j′X(cid:101)j′′
1
+
M
E E|X(cid:101)i−X(cid:101) i′| E |X(cid:101) j′′−x j|
X(cid:101)iX(cid:101)i′ X(cid:101)j′′
2M −3
−
2M(M −1)
E E|X(cid:101)i−X(cid:101) i′| E E |X(cid:101) j′′−X(cid:101) j′′′|
X(cid:101)iX(cid:101)i′ X(cid:101)j′′X(cid:101)j′′′
1
+
M
E |X(cid:101)i−x i|·|X(cid:101)j −x j|
(X(cid:101)i,X(cid:101)j)
1
−
M
E E|X(cid:101)i−X(cid:101) i′|·|X(cid:101)j −x j|
(X(cid:101)i,X(cid:101)j)X(cid:101)i′
1
−
M
E E |X(cid:101)i−x i|·|X(cid:101)j −X(cid:101) j′|
(X(cid:101)i,X(cid:101)j)X(cid:101)j′
1
+
2M(M −1)
E E |X(cid:101)i−X(cid:101) i′|·|X(cid:101)j −X(cid:101) j′|
(X(cid:101)i,X(cid:101)j)(X(cid:101)i′,X(cid:101)j′)
M −1
+
M(M −1)
E E E |X(cid:101)i−X(cid:101) i′|·|X(cid:101)j −X(cid:101) j′′|,
(X(cid:101)i,X(cid:101)j)X(cid:101)i′X(cid:101)j′′
wherevariableswiththesamenumberofapostrophes(′)aredrawntogetherandthosewithdifferent
numberofapostrophesareindependentvariables.
TogetanestimateofcovarianceusingourM samples,wecanestimateeachofthesetermsusing
theirrespectiveunbiasedestimators. Oncewehavecomputeanestimateofthevarianceforasingle
taskinstance,theoverallvarianceforafulltaskiscomputedusingtheformulaforthevarianceof
theaverageofmultipleindependentvariables. Oneslightdisadvantageofusingthismethod,isthat
itoffersnowguaranteethattheRCPRSvarianceestimatewillbenon-negative,sointherarecases
wheretheestimateforthevarianceofafulltaskisnegative,weclipitto0.
51