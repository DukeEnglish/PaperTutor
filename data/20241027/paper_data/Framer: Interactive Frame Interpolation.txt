Preprint
FRAMER: INTERACTIVE FRAME INTERPOLATION
WenWang1,2,QiuyuWang2,KechengZheng2,HaoOuyang2,ZhekaiChen1,BiaoGong2,
HaoChen1,YujunShen2,ChunhuaShen1
1ZhejiangUniversity 2AntGroup
Strat Frame Generated Frames End Frame
(a) Same input image pair with varying drags
(b) Image morphing (inputs highlighted by dashed boxes)
Figure 1: Showcases produced by our Framer. It facilitates fine-grained customization of local
motions and generates varying interpolation results given the same input start and end frame
pair (first 3 rows). Moreover, Framer handles challenging cases and can realize smooth image
morphing(last2rows). Theinputtrajectoriesareoverlayedontheframes.
ABSTRACT
WeproposeFramerforinteractiveframeinterpolation,whichtargetsproducing
smoothly transitioning frames between two images as per user creativity. Con-
cretely, besides taking the start and end frames as inputs, our approach supports
customizing the transition process by tailoring the trajectory of some selected
keypoints. Such a design enjoys two clear benefits. First, incorporating human
interactionmitigatestheissuearisingfromnumerouspossibilitiesoftransforming
oneimagetoanother,andinturnenablesfinercontroloflocalmotions. Second,
asthemostbasicformofinteraction,keypointshelpestablishthecorrespondence
acrossframes,enhancingthemodeltohandlechallengingcases(e.g.,objectson
the start and end frames are of different shapes and styles). It is noteworthy
that our system also offers an “autopilot” mode, where we introduce a module
to estimate the keypoints and refine the trajectory automatically, to simplify
the usage in practice. Extensive experimental results demonstrate the appealing
performanceofFrameronvariousapplications,suchasimagemorphing,time-
lapse video generation, cartoon interpolation, etc. The code, the model, and the
interfacewillbereleasedtofacilitatefurtherresearch.
Projectpage: aim-uofa.github.io/Framer
1
4202
tcO
42
]VC.sc[
1v87981.0142:viXraPreprint
1 INTRODUCTION
Thecreationofseamlessandvisuallyappealingtransitionsbetweenframes(Dongetal.,2023)isa
fundamentalrequirementinvariousapplications,includingimagemorphing(Aloraibi,2023),slow-
motion video generation (Reda et al., 2022), and cartoon interpolation (Xing et al., 2024). Users
often need to control the motion trajectories, deformation dynamics, and temporal coherence of
interpolated frames to achieve specific outcomes. Therefore, incorporating interactive capabilities
intoframeinterpolationframeworksiscrucialforexpandingthepracticalapplicability.
Traditional video frame interpolation methods (Jiang et al., 2018; Xu et al., 2019; Liu et al.,
2020; Niklaus & Liu, 2020; Sim et al., 2021; Lee et al., 2020; Ding et al., 2021) often rely on
estimatingopticalflowormotiontopredictintermediateframesdeterministically. Whilesignificant
progresshasbeenmadeinthisarea,theseapproachesstruggleinscenariosinvolvinglargemotion
or substantial changes in object appearance, due to an inaccurate flow estimation. What’s more,
whentransformingoneimagetoanother,therecanbenumerousplausiblewaysobjectsandscenes
cantransition. Adeterministicresultmaynotalignwithuserexpectationsorcreativeintent.
Orthogonaltoexistingmethods,weproposeFramer,aninteractiveframeinterpolationframework
designed to produce smoothly transitioning frames between two images. Our approach allows
users to customize the transition process by tailoring the trajectories of selected keypoints, thus
directlyinfluencingthemotionanddeformationofobjectswithinthescene. Suchdesignofferstwo
significant benefits. First, the incorporation of keypoint-based interaction resolves the ambiguity
inherent in transforming one image into another, allowing for precise control over how specific
regions of the image move and change. As shown in Fig. 1a, users can control the movements of
the dog’s paw and head through simple and intuitive interactions. Second, keypoint trajectories
establishexplicitcorrespondencesacrossframes,whichisespeciallybeneficialinchallengingcases
whereobjectschangeinshape,style,orevensemanticmeaning. AsshowninFig.1b,thekeypoint
trajectories establish the correspondences between keypoints from Poke´mon in varying forms and
helpproduceasmooth“evolution”processofPoke´mon.
Concretely, we view video frame interpolation from a generative perspective and finetune a
large-scale pre-trained image-to-video diffusion model (Blattmann et al., 2023a) on open-domain
video datasets (Nan et al., 2024) to facilitate video frame interpolation. The additional last-
frame conditioning is introduced during the fine-tuning process. Afterward, a point trajectory
controllingbranchisintroducedtotaketheadditionalpointtrajectoryinputs,thusguidingthevideo
interpolation process. During inference, Framer supports the “interactive” mode for customized
videoframeinterpolation,followinguser-inputpointtrajectories.
Understandingthatmanualkeypointannotationmaynotalwaysbedesirable,weofferan“autopilot”
mode for Framer. Technically, we propose a novel bi-directional point-tracking method that
estimates the trajectories of matched points over the entire video sequence, by analyzing both
forward and backward motions between frames. It automates the process of obtaining keypoint
trajectories, enabling Framer to generate motion-natural and temporally coherent interpolation
resultswithoutrequiringextensiveuserinput. The“autopilot”modesimplifiestheworkflowwhile
stillbenefitingfromtheenhancedcorrespondenceprovidedbythepointstrajectories.
WeconductextensiveexperimentstoevaluatetheperformanceofFrameracrossvariousapplica-
tions,includingimagemorphing,time-lapsevideogeneration,andcartooninterpolation.Theresults
demonstrate that Framer produces smooth and visually appealing transitions, outperforming
existing methods, particularly in cases involving complex motions and significant appearance
changes. Bycombiningthestrengthsofgenerativemodelswithuser-guidedinteractions,Framer
improvesboththequalityandcontrollabilityoftheinterpolatedframes.
2 RELATED WORK
2.1 VIDEOFRAMEINTERPOLATION
Video framer interpolation (VFI) aims to synthesize intermediate frames from two successive
video frames. Most previous methods view VFI as a low-level task, assuming a moderate motion
between frames. These methods can roughly be categorized as flow-based methods and kernel-
2Preprint
based methods. Specifically, the flow-based methods leverage estimated optical flow for frame
synthesis (Jiang et al., 2018; Xu et al., 2019; Liu et al., 2020; Niklaus & Liu, 2020; 2018; Sim
et al., 2021; Huang et al., 2020; Jin et al., 2023; Xue et al., 2019; Park et al., 2020; 2021; Kong
etal.,2022). Bycontrast,thekernel-basedmethodsrelyonspatiallyadaptivekernelstosynthesize
the interpolated pixels (Lee et al., 2020; Cheng & Chen, 2022; Ding et al., 2021; Niklaus et al.,
2017;Cheng&Chen,2020;Guietal.,2020;Luetal.,2022). Whiletheformerpotentiallysuffers
from inaccurate flow estimation, the latter are often constrained by kernelsize. To obtain the best
of both worlds, some methods combine the flow- and kernel-based methods for end-to-end video
frameinterpolation(Baoetal.,2019;2021;Danieretal.,2022;Lietal.,2022). Wereferreadersto
(Dongetal.,2023)foramorecomprehensivesurveyonthesemethods.
Recently,inspiredbythegenerativecapacityoflarge-scalepre-trainedvideodiffusionmodels,some
methodsattempttotackleVFIfromagenerationperspective(Danieretal.,2024;Fengetal.,2024;
Jain et al., 2024; Xing et al., 2023; Wang et al., 2024a). For example, LDMVFI (Danier et al.,
2024)formulatesVFIasaconditionalgenerationproblemandutilizesalatentdiffusionmodelfor
perceptually oriented video frame interpolation. Similarly, VIDIM (Jain et al., 2024) leverages
cascaded diffusion models to generate high-fidelity interpolated videos with nonlinear motions.
Though progress has been made, these methods still have difficulties in tackling large differences
betweenthestartingandendingframes. Moreover,theyfocusongeneratingasingledeterministic
solutionforvideoframeinterpolation,withoutcontrollability. Differently,wecangeneratemultiple
plausiblesolutionsunderlargemotionchanges,andallowsimpleandintuitivedraginteractionfor
user-intendedinterpolationresults.
2.2 VIDEODIFFUSIONMODELS
Large-scale pre-trained video diffusion models (Brooks et al., 2024; Blattmann et al., 2023b; Ge
et al., 2023; Chen et al., 2023; 2024; Wang et al., 2023a; Blattmann et al., 2023a) have shown
unprecedented generation results in visual quality, diversity, and realism. These methods leverage
textorstartingimagecontrols,whichareofteninsufficientinprecisionandinteractiveness. Inspired
by the success in controllable image generation (Zhang et al., 2023; Mou et al., 2024b), several
worksattempttoaddadditionalcontrolstovideodiffusionmodels. Earlyexplorations(Wangetal.,
2023b;Guoetal.,2023)utilizestructuralcontrols,likesketchanddepthmaps,forvideogeneration.
However,thesecontrolsignalsareoftendifficulttoobtainduringsampling,limitingtheirpractical
applications. Differently,recentworksfocusonmotioncontrolandintroducetrajectorycontrolfor
object motion (Wu et al., 2024; Mou et al., 2024a; Yin et al., 2023) and camera pose control for
camera motion (Wang et al., 2024b; He et al., 2024; Bahmani et al., 2024). Both control signals
canbeobtainedthrougheasyandintuitiveuserinteractions. Inthispaper,weenhancethecreative
potentialandflexibilityofthevideoframerinterpolationprocess,allowinguserstoproduceplausible
frameinterpolationresultsfollowingtheircontrol.
3 METHOD
Given two frames, I0 and In, indicating the start and end frame in a video, our goal is to
generatetheplausiblecontiguousvideoI = {Ii}n bysamplingfromtheconditionaldistribution
i=0
p(cid:0)
I
|I0,In(cid:1)
.Here,nisthenumberofframesinthevideo.Ourmethod,termedFramer,supports
a user-interactive mode for customized point trajectories and an “autopilot” mode for video frame
interpolation without trajectory inputs, as shown in Fig. 2a and Fig. 2b. In the following, we will
introducehowweaddframeconditionstothevideodiffusionmodeltoachievevideointerpolationin
Sec.3.1.Tosupportuser-interactivedragcontrol,weintroduceacontrolbranchinSec.3.2forpoint
trajectory guidance, which also enhances point correspondences across frames. In the “autopilot”
mode, we estimate trajectories of matched points in the video with our novel bi-directional point
trackingmethod,asillustratedinSec.3.3.
3.1 MODELARCHITECTURE
Large-scale pre-trained video diffusion models have a strong visual prior on the appearance,
structure, and movement of open-world objects (Brooks et al., 2024). Our approach builds on the
video diffusion model to exploit this prior. Considering that the Image-to-Video (I2V) diffusion
3Preprint
(a) User Interactive Mode (c) Trajrectory Controlling Branch
emarF
tratS
Condition
Encoder
emarF
dnE
Point Trajectory zero-conv
UNet Encoder
User customizes the keypoints Trajectory Output Video
emarF
tratS
SSttaarrtt FFrraammee
Output
emarF
dnE
NNooiissee
Video
EEnndd FFrraammee 3D-UNet
Point Trajectory Trajectory Output Video
Matching Initialization Updating
(b) Autopilot Mode (d) Video Frame Interpolation Fine-tuning
Figure 2: Framer supports (a) a user-interactive mode for customized point trajectories and (b)
an “autopilot” mode for video frame interpolation without trajectory inputs. During training, (d)
we fine-tune the 3D-UNet of a pre-trained video diffusion model for video frame interpolation.
Afterward, (c) we introduce point trajectory control by freezing the 3D-UNet and fine-tuning the
controllingbranch.
modelnaturallysupportsfirst-frameconditioning,wechoosetherepresentativeI2Vdiffusionmodel,
StableVideoDiffusion(SVD)(Blattmannetal.,2023a),asourbasemodel,asshowninFig.2d.
Based on the I2V model, we need to introduce additional end-frame conditioning to realize video
interpolation. To preserve the visual prior of the pre-trained SVD as much as possible, we follow
theconditioningparadigmofSVDandinjectend-frameconditionsinthelatentspaceandsemantic
space,respectively. Specifically,weconcatenatetheVAE-encodedlatentfeatureofthefirstframe,
denotedasz0,withthenoisylatentofthefirstframe,asdidinSVD.Additionally,weconcatenate
thelatentfeatureofthelastframe, zn, withthenoisylatentoftheendframe, consideringthatthe
conditionsandthecorrespondingnoisylatentsarespatiallyaligned.Inaddition,weextracttheCLIP
image embedding of the first and last frames separately and concatenate them for cross-attention
featureinjection. TheU-Netϵ istrainedusingthedenoisingscorematchingobjective:
θ
L=E zt,z0,zn,t,ϵ∼N(0,I)(cid:104)(cid:13) (cid:13)ϵ−ϵ θ(cid:0) z t;t,z0,zn(cid:1)(cid:13) (cid:13)2(cid:105) . (1)
3.2 INTERACTIVEFRAMEINTERPOLATION
Ambiguity remains given the start and end frames, especially when the distinction between the
two frames is large. The reason is that multiple plausible interpolation results can be obtained by
sampling video from the conditional distribution P
(cid:0)
I
|I0,In(cid:1)
for the same input pair. To better
alignwiththeuserintention,weintroduceacontrolbranchforcustmizedpointtrajectoryguidance.
Technically,wetrainapointtrajectory-basedcontrolbranchforcorrespondencemodeling,asshown
in Fig. 2c. During training, we use the following steps to obtain the point trajectory as control
signals. Firstly, werandomlyinitializesomesampledpointsaroundafixedsparsegridinthefirst
frame,anduseCo-Tracker(Karaevetal.,2023)toobtainthetrajectoriesofthesepointsinthewhole
video. Secondly,weremovetrajectoriesthatarenotvisibleinmorethanhalfofthevideoframes.
Lastly, we sample the point trajectories with larger motions with greater probability. Considering
thattheusersusuallyonlyinputasmallnumberofpointtrajectories,wekeeponly1to10trajectories
duringtraining. PleaserefertotheApp.Aformoredetails.
After obtaining the sampled point trajectories, we follow DragNUWA (Yin et al., 2023) and
DragAnything(Wuetal.,2024)totransformthepointcoordinatesintoaGaussianheatmap,denoted
as c , which is used as input to the control module. We follow the conditioning mechanism in
traj
ControlNet (Zhang et al., 2023) to incorporate the trajectory control. Specifically, we copy the
encoderof3D-UNettoencodethetrajectorymapandadditintothedecoderofU-Netafterzero-
4Preprint
SI MFT at F ce ha int gure (a) Forward Point Tracking Start Frame Feature
End Frame Feature
Start Nearest Middle Frame Feature
Frame Neighbor
Point
Unpdating
End Nearest
Frame Neighbor
Bi-directional
Consistency
(b) Backward Point Tracking
Figure 3: Point trajectory estimation. The point trajectory is initialized by interpolating the
coordinates of matched keypoints. In each de-noising step, we perform point tracking by finding
the nearest neighbor of keypoints in the start and end frames, respectively. Lastly, We check the
bi-directionaltrackingconsistencybeforeupdatingthepointcoordinate.
convolution(Zhangetal.,2023). Thistrainingprocesscanberepresentedas:
L=E zt,z0,zn,t,ϵ∼N(0,I)(cid:104)(cid:13) (cid:13)ϵ−ϵc θ(cid:0) z t;t,z0,zn,c traj(cid:1)(cid:13) (cid:13)2(cid:105) . (2)
Here,ϵc isthecombinationofthedenoisingU-NetandtheControlNetbranch.
θ
Discussion. Theintroductionofpointtrajectorycontrolnotonlyfacilitatesuserinteraction,butalso
enhancesthecorrespondenceamongpointsfromdifferentframes. Asdemonstratedinexperiments,
thisapproachenablesthemodeltoeffectivelytacklechallengingcases,suchaswhenthestartand
endframesdiffersignificantly.
3.3 “AUTOPILOT”MODEFORFRAMEINTERPOLATION
In practical applications, users may not always prefer manual drag controls. For this reason, we
propose an “autopilot” mode to enhance the ease of use of our Framer. It mainly contains a
trajectoryinitializationandatrajectoryupdatingprocess,asillustratedinFig.2b.
Trajectory Initialization. Given the start and end frames of the input video, we can obtain the
matching points between the two frames by applying feature-matching algorithms. The matched
points are denoted as {p }m , where m is the number of matching points. p denotes the known
i i=1 i
anchorpointsonthetrajectory. Atinitialization,itcontainsthematchedpointsonthefirstandlast
frames, i.e., p = [p0,pn]. Althoughvaryingfeaturematchingalgorithmsarefeasible, weusethe
i i i
classicalSIFTfeaturematching(Lowe,2004)hereforitssimplicityandeffectiveness.Subsequently,
wecanobtainthei-thtrajectorycˆ byinterpolatingtheanchorpointsp . Theestimatedtrajectory
i i
forallmmatchedpoints,denotedascˆ ={cˆ}m ,areusedastheinputconditioninEq.(2).
traj i i=1
Trajectory Updating. Although the initial trajectory provides temporally consistent point
correspondence,thetrajectoryobtainedbyconnectingpointsinthefirstandlastframesmaynotbe
accurate. InspiredbyDragGAN(Panetal.,2023)andDragDiffusion(Shietal.,2023),weperform
pointtrackingusingtheintermediatefeatureinU-Nettoupdatethetrajectories.Specifically,ineach
denoisingstep,weinterpolatetheU-Netfeaturestotheimageresolution,denotedasF.Hereweuse
thefeatureofthepenultimateupsampledblockinU-Net, sinceitenjoysagoodtrade-offbetween
featureresolutionanddiscriminativeness. WeuseF(p)torepresentthefeatureofthepointp,which
isobtainedviabilinearinterpolation,sincethecoordinatesmaynotbeintegers.
In each denoising step, we apply point tracking to update the coordinates of the middle frame
points. We use nearest neighbor search in a feature patch around the point. The feature patch
represents a set of points whose distance to point p is less than r, and is denoted as Ω(p,r) =
{(x,y)||x−x |<r,|y−y |<r}. For a middle frame point pk in the k-th frame, we find the
p p i
nearestpointrelativetotheanchorpointp0via:
i
pk,0 := argmin (cid:13) (cid:13)F(cid:0) qk(cid:1) −F(cid:0) p0(cid:1)(cid:13) (cid:13) . (3)
i i i 1
q ik∈Ω(pk i,r1)
5Preprint
Blended Inputs GT AMT RIFE FLAVR FILM LDMVFI DynamiCrafter SVDKFI Ours
Figure4: Qualitativecomparison. ‘GT”strandsforgroundtruth. Foreachmethod,weonlypresent
themiddleframeof7interpolatedframes. ThefullresultscanbeseeninFig.S4andFig.S5inthe
Appendix.
Similarly,wecanobtainthenearestpointrelativetothelastanchorpointpn:
i
pk,n := argmin (cid:13) (cid:13)F(cid:0) qk(cid:1) −F(pn)(cid:13) (cid:13) . (4)
i i i 1
q ik∈Ω(pk i,r1)
AsshowninFig.3,tofurtherensuretheaccuracyofthecoordinatesoftheupdatedpoints,wecheck
theconsistencyofthetwonearestpointsobtainedbymatchingwithp0 andpn. Whenthedistance
i i
(cid:16) (cid:17)
betweenthetwoislessthanathresholdr ,i.e.,pk,n ∈Ω pk,0,r ,weupdatethepointcoordinates
2 i i 2
bysettingpk =(pk,0+pk,n)/2. Then,weaddthepointtotheanchorpointslistp andinterpolate
i i i i
p togettheupdatedtrajectoryc ,whichisusedastheinputconditiontothenextdenoisingstep.
i i
4 EXPERIMENTS
4.1 IMPLEMENTATIONDETAILS
OurmethodisbuiltonSVDandtrainedonthehigh-qualityOpenVidHD-0.4Mdataset(Nanetal.,
2024). During the training of U-Net, we fixed the spatial attention and residual blocks, and only
fine-tuned the input convolutional and temporal attention layers. The model is trained for 10,000
iterations using the AdamW optimizer (Loshchilov & Hutter, 2019) with a learning rate of 1e-4.
Weobtainedthepointtrajectoriesbypre-processingthevideousingtheCo-Tracker(Karaevetal.,
2023). Whentrainingthecontrolmodule,wefixedtheU-Netandoptimizedthecontrolmodulefor
10,000 steps using the AdamW optimizer, with a learning rate of 1e-4. All training is performed
on16NVIDIAA100GPUs,andthetotalbatchsizeis16. During“autopilot”modesampling,we
keepm = 5bestmatchingkeypointsfortrajectoryguidance,andthedistancethresholdsforpoint
trackingaresetasr =5andr =3. PleaserefertoApp.Aformoredetails.
1 2
4.2 COMPARISON
Existingmethodsdonotsupportdrag-userinteraction.
Thus, we use the “autopilot” mode of Framer to
SSVVDDKKFFII 11..22%%
makefaircomparisons. Weselectbaselinesfromtwo LLDDMMVVFFII 00..77%%
distinctcategories. Thefirstcategoryincludesthelat- FFIILLMM 44..44%%
FFLLAAVVRR 00..88%%
estgeneraldiffusion-basedvideointerpolationmodels, RRIIFFEE 00..77%%
AAMMTT 11..77%%
including LDMVFI (Danier et al., 2024), Dynamic-
FFrraammeerr 9900..55%%
Crafter(Xingetal.,2023),andSVDKFI(Wangetal.,
2024a). Thesecondcategoryencompassestraditional
video interpolation methods, such as AMT (Li et al., Figure5: Reultsonhumanpreference.
2023), RIFE (Huang et al., 2020), FLAVR (Kalluri
et al., 2023), and FILM (Reda et al., 2022). We
conduct quantitative and qualitative analyses, as well as user studies, on two publicly available
datasets: DAVIS(Pont-Tusetetal.,2017)andUCF101(Soomroetal.,2012).
Qualitative Comparison. As shown in Fig. 4, our method produces significantly clearer textures
and natural motion compared to existing interpolation techniques. It performs especially well in
scenarios with substantial differences between the input frames, where traditional methods often
6Preprint
DAVIS-7 UCF101-7
PSNR↑ SSIM↑ LPIPS↓ FID↓ FVD↓ PSNR↑ SSIM↑ LPIPS↓ FID↓ FVD↓
AMT(Lietal.,2023) 21.66 0.7229 0.2860 39.17 245.25 26.64 0.9000 0.1878 37.80 270.98
RIFE(Huangetal.,2020) 22.00 0.7216 0.2663 39.16 319.79 27.04 0.9020 0.1575 27.96 300.40
FLAVRKallurietal.(2023) 20.94 0.6880 0.3305 52.23 296.37 26.50 0.8982 0.1836 37.79 279.58
FILM(Redaetal.,2022) 21.67 0.7121 0.2191 17.20 162.86 26.74 0.8983 0.1378 16.22 239.48
LDMVFI(Danieretal.,2024) 21.11 0.6900 0.2535 21.96 269.72 26.68 0.8955 0.1446 17.55 270.33
DynamicCrafter(Xingetal.,2023) 15.48 0.4668 0.4628 35.95 468.78 17.62 0.7082 0.3361 61.71 646.91
SVDKFI(Wangetal.,2024a) 16.71 0.5274 0.3440 26.59 382.19 21.04 0.7991 0.2146 44.81 301.33
Framer(Ours) 21.23 0.7218 0.2525 27.13 115.65 25.04 0.8806 0.1714 31.69 181.55
FramerwithCo-Tracker(Ours) 22.75 0.7931 0.2199 27.43 102.31 27.08 0.9024 0.1714 32.37 159.87
Table1: Quantitativecomparisonwithexistingvideointerpolationmethodsonreconstructionand
generativemetrics,evaluatedonall7generatedframes.
Strat Frame Generated Frames End Frame
gard
o/w
1
gard
2
gard
Figure6: Resultsonuserinteraction. Thefirstrowisgeneratedwithoutdraginput,whiletheother
twoaregeneratedwithdifferentdragcontrols. Customizedtrajectoriesaresoverlaidonframes.
failtointerpolatecontentaccurately. Comparedtootherdiffusion-basedmethodslikeLDMVFIand
SVDKFI,Framerdemonstratessuperioradaptabilitytochallengingcasesandoffersbettercontrol.
Quantitative Comparison. As discussed in VIDIM (Jain et al., 2024), reconstruction metrics
like PSNR, SSIM, and LPIPS fail to capture the quality of interpolated frames accurately, since
theypenalizeotherplausibleinterpolationresultsthatarenotpixel-alignedwiththeoriginalvideo.
While generation metrics such as FID offer some improvement, they still fall short as they do not
account for temporal consistency and evaluate frames in isolation. Despite this, we present the
quantitativemetricsforvarioussettingsonbothdatasets,whereourmethodachievesthebestFVD
scoreamongallbaselinesasinTab.1. WealsoevaluateFramerwith5randompointtrajectories
fromground-truthvideos,estimatedusingCo-Tracker. Ascanbeseen,“FramerwithCo-Tracker”
achievessuperiorperformanceeveninreconstructionmetric.Foramorecomprehensiveassessment
ofquality,werecommendreviewingthesupplementarycomparisonvideos.
UserStudy. Sincequantitativemetricsfallshortinreflectingvideoquality,wefurtherassessedour
method’sperformancethroughauserstudy.Inthisstudy,participantsreviewedvideosetsgenerated
from the same input frame pair by both existing methods and our Framer. Participants assessed
up to 100 randomly ordered video sets and selected the one they found most realistic. In total,
20 participants provided 1,000 ratings across these video sets. As illustrated in Fig. 5, the results
demonstrateastrongpreferenceamonghumanratersfortheoutputsproducedbyourmethod.
Strat Frame Generated Frames End Frame
Figure7: Novelviewsynthesisonbothstatic(1strow)anddynamicscenes(2ndrow).
7Preprint
Strat Frame Generated Frames End Frame
Figure8: Applicationsoncartoon(1strow)andsketch(2ndrow)interpolation.
Strat Frame Generated Frames End Frame
Figure9: Applicationsontime-lapsingvideogeneration.
4.3 APPLICATIONS
Optional drag control. Given the same input start and end frames, multiple plausible results can
satisfy the goal of video interpolation. With Framer, users can direct the motion of the entities
ininputframeswithsimpledragsfortheirintention,orsimplyobtainadefaultinterpolationresult
withoutdrags.AsshowninFig.6,thesealmovesinvaryingdirectionsgiventhesameinputframes.
Novelviewsynthesis(NVS)isaclassical3Dvisiontask,withawiderangeofapplications. Using
imagesofdifferentviewpointsasthestartandendframesofthevideorespectively,wecanrealize
the NVS from sparse viewpoint input by performing video interpolation. As shown in Fig. 7, our
method achieves pleasing NVS in both static scenes (first row) and dynamic scenes (second and
third rows). Taking the second row as an example, the house gradually moves out of the scene as
thecamerakeepsmovingforward. Inthemeantime,thecarmovesintheoppositedirectiontothe
cameraandgraduallytakesupalargerproportionintheframe.
Cartoon and sketch interpolation. We can dramatically simplify the process of cartoon video
production, by interpolating manually created cartoon images. To this end, we tested our
method on cartoon data. Although our method is not specifically trained on cartoon videos, it
producesappealingcartoonvideoresultsandsupportsbothcolorimagesandsktechdrawingframe
interpolation,asshowninFig.8. Forexample,ourmethodsuccessfullymodelsthemotionoftwo
objects,i.e.,thefrontvehiclepullssidewayswhiletherearvehiclefollows,asshowninthefirstrow.
Inthethirdrow,Framerproducesasmoothmotionofthehandliftinginsketchdrawings.
Time-lapsing video generation. Time-lapse photography can vividly demonstrate slow changes
thataredifficulttodetectwiththenakedeye. Typically,itrequiressufficientstoragespacetohold
a large amount of image data and a complex post-processing procedure to organize and edit the
images. Video interpolation provides a simple and effective way to obtain time-lapse videos by
interpolatingframeswithonlyafewimagesofkeymoments.AsshowninFig.9,Framerproduces
thesmoothchangeofmoonwaxingandwaning.
Strat Frame Generated Frames End Frame y-t
Figure10: Applicationsonslow-motionvideogeneration. They-tslicehighlightedinredonvideo
framesisvisualizedontheright.
8Preprint
Strat Frame Generated Frames End Frame
Figure11: Applicationsonimagemorphing. Customizedtrajectoriesaresoverlaidonendframes.
o/w.jart
o/wetad
pu
.jart
o/wlanoitcerid
-
ib
Inputs with
SIFT matching
rem arF)sruO
(
Frame 1 Frame 2 Frame 3 Frame 4 Frame 5 Frame 6 Frame 7
Figure12:Ablationsoneachcomponent.“w/otrajectory”denotesinferencewithoutguidancefrom
point trajectory, “w/o traj. update” indicates inference without trajectory updates, and “w/o bi”
suggeststrajectoryupdatingwithoutbi-directionalconsistencyverification.
Slow-motionvideogenerationenhancesvisualeffectsbyhighlightingfinedetailsandallowscloser
examination of fast phenomena. Our Framer inherently supports fast frame interpolation, as
demonstratedinFig.10,enablingsmoothslow-motioneffectssuitableforfilmsandanimations.
Image morphing (Aloraibi, 2023) is a popular image transformation technique with many
applications in computer vision and computer graphics. Given two topologically similar images,
it aims to generate a series of reasonable intermediate images. Using tue two images as the start
andendframes,Framercanproducenaturalandsmoothimagemorphingresults. Forexample,in
Fig.1,weshowthe“evolution”processofPokemon. MorecasescanbefoundinFig.S13.
4.4 ABLATIONSSTUDIES
We conducted ablation studies on the individual components of Framer to validate their
effectiveness. The results are illustrated in Fig. 12. Our observations are as follows. First, when
the trajectory guidance is removed (denoted as “w/o traj.”), the foreground motorcycle exhibits
significantdistortion,asshowninthe1strowofFig.12. Conversely,withtheinclusionoftrajectory
guidance,thetemporalconsistencyofthevideoisnotablyenhanced,asdepictedinthe2ndrow. We
believe this is due to the enhancement of point correspondence modeling across frames. Second,
removing trajectory updates (denoted as “w/o traj. update”) or updating the trajectory without bi-
directional consistency checks (denoted as “w/o bi-directional”) results in blurring in the wheel
regionsoftheoutputvideo. Wesuspecttheblurringiscausedbytheguidanceofunnaturalmotion
from inaccurate trajectories, which conflicts with the generation prior in the pre-trained diffusion
model,leadingtolocalblurring. Incontrast,ourmethodproducesvideoframeinterpolationresults
withnaturalmotionandsmoothtemporalcoherence. ThequantitativeresultsinTabs.S1andS2in
App.Bfurthersupportthesefindings,showingasimilartrendtothequalitativeablationexperiments.
9Preprint
5 CONCLUSION AND FUTURE WORK
Inthispaper,weintroduceFramer,aninteractiveframeinterpolationpipelinedesignedtoproduce
smoothly transitioning frames between two images, guided by user-defined point trajectories. By
harnessing user input point controls from the start and end frames, we effectively guide the video
interpolation process. Moreover, our method offers an “autopilot” mode that introduces a module
toautomaticallyestimatekeypointsandrefinetrajectorieswithoutmanualinput. Throughextensive
experimentsanduserstudies,wedemonstratethesuperiorityofourmethodinachievingpromising
results in terms of both the quality and controllability of the interpolated frames. However,
challengesremain,particularlyintransitioningbetweendifferentclips.Apotentialsolutioninvolves
splittingtheclipsintoseveralkeyframesandtheninterpolatingthesekeyframessequentially. Future
workwillfocusonaddressingthesechallenges.
REFERENCES
Alyaa Aloraibi. Image morphing techniques: A review. Technium: Romanian Journal of Applied
SciencesandTechnology,2023.
Sherwin Bahmani, Ivan Skorokhodov, Aliaksandr Siarohin, Willi Menapace, Guocheng Qian,
MichaelVasilkovsky,Hsin-YingLee,ChaoyangWang,JiaxuZou,AndreaTagliasacchi,DavidB.
Lindell, and Sergey Tulyakov. VD3D:taming large videodiffusion transformersfor 3d camera
control. arXiv: ComputingResearchRepo.,abs/2407.12781,2024.
Wenbo Bao, Wei-Sheng Lai, Chao Ma, Xiaoyun Zhang, Zhiyong Gao, and Ming-Hsuan Yang.
Depth-awarevideoframeinterpolation. InIEEEConf.Comput.Vis.PatternRecog.,2019.
Wenbo Bao, Wei-Sheng Lai, Xiaoyun Zhang, Zhiyong Gao, and Ming-Hsuan Yang. Memc-net:
Motion estimation and motion compensation driven neural network for video interpolation and
enhancement. IEEETrans.PatternAnal.Mach.Intell.,2021.
Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik
Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, Varun Jampani, and Robin
Rombach. Stablevideodiffusion: Scalinglatentvideodiffusionmodelstolargedatasets. arXiv:
ComputingResearchRepo.,abs/2311.15127,2023a.
AndreasBlattmann, RobinRombach, HuanLing, TimDockhorn, SeungWookKim, SanjaFidler,
and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion
models. InIEEEConf.Comput.Vis.PatternRecog.,2023b.
Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe
Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video
generationmodelsasworldsimulators. OpenAItechnicalreports,2024.
Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo
Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter1:
Open diffusion models for high-quality video generation. arXiv: Computing Research Repo.,
abs/2310.19512,2023.
Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying
Shan.Videocrafter2:Overcomingdatalimitationsforhigh-qualityvideodiffusionmodels.arXiv:
ComputingResearchRepo.,abs/2401.09047,2024.
Xianhang Cheng and Zhenzhong Chen. Video frame interpolation via deformable separable
convolution. InAssoc.Adv.Artif.Intell.,2020.
XianhangChengandZhenzhongChen.Multiplevideoframeinterpolationviaenhanceddeformable
separableconvolution. IEEETrans.PatternAnal.Mach.Intell.,2022.
DuolikunDanier,FanZhang,andDavidBull. St-mfnet: Aspatio-temporalmulti-flownetworkfor
frameinterpolation. InIEEEConf.Comput.Vis.PatternRecog.,2022.
Duolikun Danier, Fan Zhang, and David Bull. LDMVFI: video frame interpolation with latent
diffusionmodels. InAssoc.Adv.Artif.Intell.,2024.
10Preprint
Tianyu Ding, Luming Liang, Zhihui Zhu, and Ilya Zharkov. CDFI: compression-driven network
designforframeinterpolation. InIEEEConf.Comput.Vis.PatternRecog.,2021.
JiongDong,KaoruOta,andMianxiongDong. Videoframeinterpolation:Acomprehensivesurvey.
ACMTrans.Multim.Comput.Commun.Appl.,2023.
Haiwen Feng, Zheng Ding, Zhihao Xia, Simon Niklaus, Victoria Ferna´ndez Abrevaya, Michael J.
Black, and Xuaner Zhang. Explorative inbetweening of time and space. arXiv: Computing
ResearchRepo.,abs/2403.14611,2024.
SongweiGe,SeungjunNah,GuilinLiu,TylerPoon,AndrewTao,BryanCatanzaro,DavidJacobs,
Jia-Bin Huang, Ming-Yu Liu, and Yogesh Balaji. Preserve your own correlation: A noise prior
forvideodiffusionmodels. InInt.Conf.Comput.Vis.,2023.
ShuruiGui,ChaoyueWang,QihuaChen,andDachengTao.Featureflow:Robustvideointerpolation
viastructure-to-texturegeneration. InIEEEConf.Comput.Vis.PatternRecog.,2020.
Yuwei Guo, Ceyuan Yang, Anyi Rao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Sparsectrl:
Adding sparse controls to text-to-video diffusion models. arXiv: Computing Research Repo.,
abs/2311.16933,2023.
Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo Dai, Hongsheng Li, and Ceyuan Yang.
Cameractrl: Enabling camera control for text-to-video generation. arXiv: Computing Research
Repo.,abs/2404.02101,2024.
Zhewei Huang, Tianyuan Zhang, Wen Heng, Boxin Shi, and Shuchang Zhou. RIFE: real-time
intermediate flow estimation for video frame interpolation. arXiv: Computing Research Repo.,
abs/2011.06294,2020.
Siddhant Jain, Daniel Watson, Eric Tabellion, Aleksander Holynski, Ben Poole, and Janne
Kontkanen. Video interpolation with diffusion models. arXiv: Computing Research Repo.,
abs/2404.01203,2024.
Huaizu Jiang, Deqing Sun, Varun Jampani, Ming-Hsuan Yang, Erik G. Learned-Miller, and
Jan Kautz. Super slomo: High quality estimation of multiple intermediate frames for video
interpolation. InIEEEConf.Comput.Vis.PatternRecog.,2018.
Xin Jin, Longhai Wu, Guotao Shen, Youxin Chen, Jie Chen, Jayoon Koo, and Cheul-Hee Hahm.
Enhancedbi-directionalmotionestimationforvideoframeinterpolation. InIEEEWinterConf.
Appl.Comput.Vis.,2023.
TarunKalluri,DeepakPathak,ManmohanChandraker,andDuTran. FLAVR:flow-agnosticvideo
representationsforfastframeinterpolation. InIEEEWinterConf.Appl.Comput.Vis.,2023.
NikitaKaraev,IgnacioRocco,BenjaminGraham,NataliaNeverova,AndreaVedaldi,andChristian
Rupprecht. Cotracker: It is better to track together. arXiv: Computing Research Repo.,
abs/2307.07635,2023.
LingtongKong,BoyuanJiang,DonghaoLuo,WenqingChu,XiaomingHuang,YingTai,Chengjie
Wang,andJieYang. Ifrnet: Intermediatefeaturerefinenetworkforefficientframeinterpolation.
InIEEEConf.Comput.Vis.PatternRecog.,2022.
Hyeongmin Lee, Taeoh Kim, Tae-Young Chung, Daehyun Pak, Yuseok Ban, and Sangyoun Lee.
Adacof: Adaptive collaboration of flows for video frame interpolation. In IEEE Conf. Comput.
Vis.PatternRecog.,2020.
Changlin Li, Guangyang Wu, Yanan Sun, Xin Tao, Chi-Keung Tang, and Yu-Wing Tai. H-VFI:
hierarchicalframeinterpolationforvideoswithlargemotions.arXiv:ComputingResearchRepo.,
abs/2211.11309,2022.
ZhenLi,Zuo-LiangZhu,LinghaoHan,QibinHou,Chun-LeGuo,andMing-MingCheng. AMT:
all-pairs multi-field transforms for efficient frame interpolation. In IEEE Conf. Comput. Vis.
PatternRecog.,2023.
11Preprint
Yihao Liu, Liangbin Xie, Siyao Li, Wenxiu Sun, Yu Qiao, and Chao Dong. Enhanced quadratic
videointerpolation. InEur.Conf.Comput.Vis.Worksh.,2020.
Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In Int. Conf. Learn.
Represent.,2019.
David G. Lowe. Distinctive image features from scale-invariant keypoints. Int. J. Comput. Vis.,
2004.
Liying Lu, Ruizheng Wu, Huaijia Lin, Jiangbo Lu, and Jiaya Jia. Video frame interpolation with
transformer. InIEEEConf.Comput.Vis.PatternRecog.,2022.
Chong Mou, Mingdeng Cao, Xintao Wang, Zhaoyang Zhang, Ying Shan, and Jian Zhang.
Revideo: Remakeavideowithmotionandcontentcontrol. arXiv: ComputingResearchRepo.,
abs/2405.13865,2024a.
Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, and Ying Shan.
T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion
models. InAssoc.Adv.Artif.Intell.,2024b.
KepanNan,RuiXie,PenghaoZhou,TiehanFan,ZhenhengYang,ZhijieChen,XiangLi,JianYang,
andYingTai.Openvid-1m:Alarge-scalehigh-qualitydatasetfortext-to-videogeneration.arXiv:
ComputingResearchRepo.,abs/2407.02371,2024.
SimonNiklausandFengLiu. Context-awaresynthesisforvideoframeinterpolation. InIEEEConf.
Comput.Vis.PatternRecog.,2018.
Simon Niklaus and Feng Liu. Softmax splatting for video frame interpolation. In IEEE Conf.
Comput.Vis.PatternRecog.,2020.
Simon Niklaus, Long Mai, and Feng Liu. Video frame interpolation via adaptive separable
convolution. InInt.Conf.Comput.Vis.,2017.
Xingang Pan, Ayush Tewari, Thomas Leimku¨hler, Lingjie Liu, Abhimitra Meka, and Christian
Theobalt. Drag your GAN: interactive point-based manipulation on the generative image
manifold. InErikBrunvand,AllaSheffer,andMichaelWimmer(eds.),SIGGRAPH,2023.
Junheum Park, Keunsoo Ko, Chul Lee, and Chang-Su Kim. BMBC: bilateral motion estimation
withbilateralcostvolumeforvideointerpolation. InEur.Conf.Comput.Vis.,2020.
Junheum Park, Chul Lee, and Chang-Su Kim. Asymmetric bilateral motion estimation for video
frameinterpolation. InInt.Conf.Comput.Vis.,2021.
Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbela´ez, Alexander Sorkine-Hornung,
andLucVanGool. The2017DAVISchallengeonvideoobjectsegmentation. arXiv: Computing
ResearchRepo.,abs/1704.00675,2017.
Fitsum A. Reda, Janne Kontkanen, Eric Tabellion, Deqing Sun, Caroline Pantofaru, and Brian
Curless. FILM:frameinterpolationforlargemotion. InEur.Conf.Comput.Vis.,2022.
Yujun Shi, Chuhui Xue, Jiachun Pan, Wenqing Zhang, Vincent Y. F. Tan, and Song Bai.
Dragdiffusion: Harnessing diffusion models for interactive point-based image editing. arXiv:
ComputingResearchRepo.,abs/2306.14435,2023.
HyeonjunSim,JihyongOh,andMunchurlKim. XVFI:extremevideoframeinterpolation. InInt.
Conf.Comput.Vis.,2021.
Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. UCF101: A dataset of 101 human
actionsclassesfromvideosinthewild. arXiv: ComputingResearchRepo.,abs/1212.0402,2012.
Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang.
Modelscopetext-to-videotechnicalreport. arXiv: ComputingResearchRepo.,abs/2308.06571,
2023a.
12Preprint
XiangWang,HangjieYuan,ShiweiZhang,DayouChen,JiuniuWang,YingyaZhang,YujunShen,
Deli Zhao, and Jingren Zhou. Videocomposer: Compositional video synthesis with motion
controllability. InAdv.NeuralInform.Process.Syst.,2023b.
XiaojuanWang,BoyangZhou,BrianCurless,IraKemelmacher-Shlizerman,AleksanderHolynski,
and Steven M Seitz. Generative inbetweening: Adapting image-to-video models for keyframe
interpolation. arXiv: ComputingResearchRepo.,abs/2408.15239,2024a.
Zhouxia Wang, Ziyang Yuan, Xintao Wang, Yaowei Li, Tianshui Chen, Menghan Xia, Ping Luo,
and Ying Shan. Motionctrl: A unified and flexible motion controller for video generation. In
SIGGRAPH,2024b.
WeijiaWu, ZhuangLi, YuchaoGu, RuiZhao, YefeiHe, DavidJunhaoZhang, MikeZhengShou,
Yan Li, Tingting Gao, and Di Zhang. Draganything: Motion control for anything using entity
representation. arXiv: ComputingResearchRepo.,abs/2403.07420,2024.
Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Xintao Wang, Tien-Tsin Wong, and Ying
Shan. Dynamicrafter: Animating open-domain images with video diffusion priors. arXiv:
ComputingResearchRepo.,abs/2310.12190,2023.
Jinbo Xing, Hanyuan Liu, Menghan Xia, Yong Zhang, Xintao Wang, Ying Shan, and Tien-Tsin
Wong. Tooncrafter: Generative cartoon interpolation. arXiv: Computing Research Repo.,
abs/2405.17933,2024.
Xiangyu Xu, Li Si-Yao, Wenxiu Sun, Qian Yin, and Ming-Hsuan Yang. Quadratic video
interpolation. InAdv.NeuralInform.Process.Syst.,2019.
TianfanXue, BaianChen, JiajunWu, DonglaiWei, andWilliamT.Freeman. Videoenhancement
withtask-orientedflow. Int.J.Comput.Vis.,2019.
Zeyue Xue, Guanglu Song, Qiushan Guo, Boxiao Liu, Zhuofan Zong, Yu Liu, and Ping Luo.
RAPHAEL:text-to-imagegenerationvialargemixtureofdiffusionpaths. InAdv.NeuralInform.
Process.Syst.,2023.
Shengming Yin, Chenfei Wu, Jian Liang, Jie Shi, Houqiang Li, Gong Ming, and Nan Duan.
Dragnuwa: Fine-grained control in video generation by integrating text, image, and trajectory.
arXiv: ComputingResearchRepo.,abs/2308.08089,2023.
Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image
diffusionmodels. InInt.Conf.Comput.Vis.,2023.
13Preprint
APPENDIX
A MORE IMPLEMENTATION DETAILS
During training, we sample 14 consecutive frames from videos, with a spatial resolution of
512×320. Specifically,wecenter-cropthevideotoanaspectratioof512/320,thenresizethevideo
frames to the resolution of 512×320. Random horizontal flip is utilized for data augmentation.
We sample the video in temporal dimension, with a frame interval of 2. For the training of the
point trajectory-based ControlNet, we sample 1 to 10 trajectories with larger motions for training.
Specifically, we follow ReVideo (Mou et al., 2024a) and sample the trajectories by setting the
normalizedlengthsofthetrajectoriesassamplingprobabilities. During“autopilot”modesampling,
weusetheEulersamplerwith30diffusionstepsintotal. ForpointtrackinginSec.3.3,weusethe
outputfeatureoftheseconddecoderblockinthe3D-UNet. Weresizetheshortersideofthevideo
tothelengthof512,thencentercropthevideototheresolutionof512×320.
B MORE DETAILED ABLATION RESULTS
Qualitativeresultsforablationstudies. InFig.12, weshowthequalitativeresultsforablation
studies. We supplement these results with the quantitative results in Tab. S1 and Tab. S2, which
showasimilartrendtothequalitativeablationexperiments.
DAVIS-7 UCF101-7
PSNR↑ SSIM↑ LPIPS↓ FID↓ FVD↓ PSNR↑ SSIM↑ LPIPS↓ FID↓ FVD↓
w/otrajectory 20.19 0.6831 0.2787 28.25 128.71 24.16 0.8677 0.1798 32.64 195.54
w/otraj.updating 20.82 0.7054 0.2621 27.33 120.73 24.69 0.8748 0.1842 31.95 187.37
w/obi-directional 20.94 0.7102 0.2602 27.23 116.81 24.73 0.8746 0.1845 31.66 183.74
Framer(Ours) 21.23 0.7218 0.2525 27.13 115.65 25.04 0.8806 0.1714 31.69 181.55
TableS1: Ablationsoneachcomponent,evaluatingall7generatedframes.“w/otrajectory”denotes
inference without guidance from point trajectory, “w/o traj. updating” indicates inference without
trajectory updating, and “w/o bi” suggests trajectory updating without bi-directional consistency
verification.
DAVIS-7(mid-frame) UCF101-7(mid-frame)
PSNR↑ SSIM↑ LPIPS↓ FID↓ PSNR↑ SSIM↑ LPIPS↓ FID↓
w/otrajectory 19.30 0.6504 0.3093 57.10 23.14 0.8523 0.1967 54.98
w/otraj. updating 19.84 0.6700 0.2935 55.37 23.60 0.8590 0.2009 53.83
w/obi-directional 19.95 0.6739 0.2919 54.75 23.65 0.8586 0.2016 53.54
Framer(Ours) 20.18 0.6850 0.2845 55.13 23.92 0.8646 0.1889 53.33
Table S2: Ablations on each component, evaluating only the middle frame out of all 7 generated
frames. “w/o trajectory” denotes inference without guidance from point trajectory, “w/o traj.
updating”indicatesinferencewithouttrajectoryupdating,and“w/obi”suggeststrajectoryupdating
withoutbi-directionalconsistencyverification.
Ablations on diffusion feature for point tracking. As detailed in Sec. 3.3, we perform point
tracking using the diffusion feature for point trajectory updating. Here we perform ablated
experiments on the selection of the diffusion feature. The results are shown in Fig. S1. It can
be seen that in both DAVIS-7 and UCF-7, point tracking with the output feature from the second
diffusionblockgivesrisetothebest-performingresultsinFVD.
Ablations on diffusion steps for correspondence guidance. We ablate the diffusion steps for
correspondenceguidancebyonlyapplyingtheguidanceattheearlystepsorlatestepsindiffusion
sampling. TheresultsareshowninFig.S2. Ascanbeseen,theearlystepsareoftenmoreimportant
thanthelatestepsforcorrespondencemodeling. Forexample,onDAVIS-7,apleasingFVDcanbe
obtainedwhenperformingguidanceonlyon0-18diffusionsteps. Bycontrast,performingguidance
only on 18-30 diffusion steps brings litter improvements. We speculate that this is because the
14Preprint
FigureS1: Ablationsondiffusionfeatureforpointtrackingattesttime,experimentsconductedon
DAVIS-7(left)andUCF101-7(right).
FigureS2: Ablationsonthestartandenddiffusionstepsforcorrespondenceguidance,experiments
conductedonDAVIS-7(left)andUCF101-7(right). Weuseatotalsamplingstepof30.
earlydiffusionstepsfocusonthestructuralinformationofthevideo,whilethelatediffusionsteps
focus on the texture and details (Xue et al., 2023). The correspondence guidance at early steps
alreadyhelpsthemodelobtainareasonablevideostructure.Intheimplementation,wesimplyapply
correspondenceguidanceinalldiffusionsteps,withoutdetailedsearchesonthehyper-parameter.
Ablations on the number of trajectories for correspondence guidance. As described in
Sec. 3.3, we use m trajectories for correspondence guidance during sampling. Here we perform
ablatedexperimentsonthishyper-parameter,andtheresultisshowninFig.S3. Itcanbeseenthat
samplingwiththe5trajectoriesleadstothebestperformance. Thuswesetm=5bydefault.
C MORE DETAILS ON COMPARISON WITH PREVIOUS METHODS
Benchmark. We follow the practice of VIDIM (Jain et al., 2024) and perform the quantitative
evaluation on DAVIS-7 and UCF101-7 datasets using both reconstruction and generative metrics.
Both DAVIS-7 and UCF101-7 are obtained by sampling 7 consecutive video frames from the
corresponding datasets. We use all videos in the DAVIS dataset and a subset of 400 videos in
theUCF101dataset.
More results on comparisons. In Tab. S3 we provide the quantitative comparison based on the
middleframeofthe7interpolatedvideoframes. Besides,inFig.S4,Fig.S5,Fig.S6,andFig.S7,
weshowmorequalitativelycomparisonswithexitingmethods.
15Preprint
Figure S3: Ablations on the number of trajectories for guidance during sampling, experiments
conductedonDAVIS-7(left)andUCF101-7(right).
DAVIS-7(mid-frame) UCF101-7(mid-frame)
PSNR↑ SSIM↑ LPIPS↓ FID↓ PSNR↑ SSIM↑ LPIPS↓ FID↓
AMT(Lietal.,2023) 20.59 0.6834 0.3564 100.36 25.24 0.8837 0.2237 75.97
RIFE(Huangetal.,2020) 20.74 0.6813 0.3102 80.78 25.68 0.8842 0.1835 59.33
FLAVRKallurietal.(2023) 19.93 0.6514 0.4074 118.45 24.93 0.8796 0.2164 79.86
FILM(Redaetal.,2022) 20.28 0.6671 0.2620 48.70 25.31 0.8818 0.1623 41.23
LDMVFI(Danieretal.,2024) 19.87 0.6435 0.2985 56.46 25.16 0.8789 0.1695 43.01
DynamicCrafter(Xingetal.,2023) 14.61 0.4280 0.5082 77.65 17.05 0.6935 0.3502 97.01
SVDKFI(Wangetal.,2024a) 16.06 0.4974 0.3719 53.49 20.03 0.7775 0.2326 69.26
Framer(Ours) 20.18 0.6850 0.2845 55.13 23.92 0.8646 0.1889 53.33
FramerwithCo-Tracker(Ours) 21.94 0.7693 0.2437 55.77 25.86 0.8868 0.1873 54.64
TableS3: Quantitativecomparisonwithexistingvideointerpolationmethodsonreconstructionand
generativemetrics,evaluatedonlyonthemiddleframeoutofall7generatedframes.
D MORE QUALITATIVE RESULTS
We provide more qualitative results on drag control, novel view synthesis, cartoon and sketch
interpolation, time-lapsing video generation, slow-motion video generation, and image morphing
inFig.S8,Fig.S9,Fig.S10,Fig.S11,Fig.S12,andFig.S13,respectively.
E DISCUSSIONS ON LIMITATIONS
Framer is built on top of the large-scale pre-trained video diffusion model, thus it inherits the
limitations of the pre-trained model. Moreover, the point trajectories in Framer rely on the
matching points between the input image pair for interpolating complex motions. While this is a
step forward compared with current models that can only simply motions, our method still faces
difficulties when the differences between the front and back frames are so large that no matched
pointscanbefoundatall. Thus,wewillexploremorepowerfulpre-trainedvideodiffusionmodels,
as well as training video interpolation models on larger-scale video data in the future. Lastly, our
approach currently only supports drag control and does not explore other interaction methods. In
thefuture,wewillcontinuetoexploreotheruser-friendlycontrolssuchastextcontrolandcamera
posecontrol.
16Preprint
T
G
T
M
A
E
F
I
R
R
V
A
L
F
M
L
I
F
I
F
V
M
D
L
r
e
t
f
a
r
C
im
a
n
y
D
I
F
K
D
V
S
s
r
u
O
r
e
k
c
a
r
t
/
w
s
r
u
O
Frame 1 Frame 2 Frame 3 Frame 4 Frame 5 Frame 6 Frame 7
FigureS4: Morequalitativecomparisonwithexistingmethods. “GT”strandsforgroundtruth.
17Preprint
T
G
T
M
A
E
F
I
R
R
V
A
L
F
M
L
I
F
I
F
V
M
D
L
r
e
t
f
a
r
C
im
a
n
y
D
I
F
K
D
V
S
s
r
u
O
r
e
k
c
a
r
t
/
w
s
r
u
O
Frame 1 Frame 2 Frame 3 Frame 4 Frame 5 Frame 6 Frame 7
FigureS5: Morequalitativecomparisonwithexistingmethods. “GT”strandsforgroundtruth.
18Preprint
T
G
T
M
A
E
F
I
R
R
V
A
L
F
M
L
I
F
I
F
V
M
D
L
r
e
t
f
a
r
C
im
a
n
y
D
I
F
K
D
V
S
s
r
u
O
r
e
k
c
a
r
t
/
w
s
r
u
O
Frame 1 Frame 2 Frame 3 Frame 4 Frame 5 Frame 6 Frame 7
FigureS6: Morequalitativecomparisonwithexistingmethods. “GT”strandsforgroundtruth.
19Preprint
T
G
T
M
A
E
F
I
R
R
V
A
L
F
M
L
I
F
I
F
V
M
D
L
r
e
t
f
a
r
C
im
a
n
y
D
I
F
K
D
V
S
s
r
u
O
r
e
k
c
a
r
t
/
w
s
r
u
O
Frame 1 Frame 2 Frame 3 Frame 4 Frame 5 Frame 6 Frame 7
FigureS7: Morequalitativecomparisonwithexistingmethods. “GT”strandsforgroundtruth.
20Preprint
Strat Frame Generated Frames End Frame
1
gard
2
gard
FigureS8: Moreresultsonuserinteraction. Weshowtheresultsoftwotrajectorycontrolswiththe
sameinputimagepair.
Strat Frame Generated Frames End Frame
FigureS9: Moreresultsonnovelviewsynthesis. Thefirstandsecondrowsshowresultsonstatic
anddynamicscenes,respectively.
Strat Frame Generated Frames End Frame
(a) Cartoon Interpolation
(b) Sketch Interpolation
FigureS10: Moreresultson(a)cartoonand(b)sketchinterpolation.
21Preprint
Strat Frame Generated Frames End Frame
FigureS11: Moreresultsontime-lapsingvideogeneration.
Strat Frame Generated Frames End Frame x-t
FigureS12:Moreresultsonslow-motionvideogeneration.Thex-tslicehighlightedinredonvideo
framesisvisualizedontheright.
Strat Frame Generated Frames End Frame
FigureS13: Moreresultsonimagemorphing.
22