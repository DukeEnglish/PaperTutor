Preprint
PIXELGAUSSIAN: GENERALIZABLE 3D GAUSSIAN
RECONSTRUCTION FROM ARBITRARY VIEWS
XinFei1,2,∗ WenzhaoZheng1,2,† YueqiDuan1 WeiZhan2
MasayoshiTomizuka2 KurtKeutzer2 JiwenLu1
1TsinghuaUniversity 2UniversityofCalifornia,Berkeley
feix21@mails.tsinghua.edu.cn; wenzhao.zheng@outlook.com
https://wzzheng.net/PixelGaussian
ExistingUniform Pixel-wise Paradigm
Pixel-wise
Gaussian
Prediction Pixel-Aligned
Gaussians
2 views 4 views 6 views
Adaptive
Gaussians
pixelSplatMVSplat Ours pixelSplatMVSplat Ours
Geometric
Gaussian
Complexity
Split&Prune
Estimation
OurProposedAdaptive PixelGaussianParadigm
Figure 1: Most existing generalizable 3D Gaussian splatting methods (e.g., pixelSplat (Charatan
etal.,2023),MVSplat(Chenetal.,2024))assignafixednumberofGaussianstoeachpixel,leading
toinefficiencyincapturinglocalgeometryandoverlapacrossviews. Differently,ourPixelGaussian
dynamically adjusts the Gaussian distributions based on geometric complexity in a feed-forward
framework. Withcomparableefficiency,PixelGaussian(trainedusing2views)successfullygener-
alizestovariousnumbersofinputviewswithadaptiveGaussiandensities.
ABSTRACT
We propose PixelGaussian, an efficient feed-forward framework for learning
generalizable 3D Gaussian reconstruction from arbitrary views. Most existing
methodsrelyonuniformpixel-wiseGaussianrepresentations,whichlearnafixed
numberof3DGaussiansforeachviewandcannotgeneralizewelltomoreinput
views. Differently,ourPixelGaussiandynamicallyadaptsboththeGaussiandis-
tribution and quantity based on geometric complexity, leading to more efficient
representations and significant improvements in reconstruction quality. Specifi-
cally, we introduce a Cascade Gaussian Adapter to adjust Gaussian distribution
according to local geometry complexity identified by a keypoint scorer. CGA
∗WorkdonewhilevisitingUCBerkeley.
†Correspondingauthor.
1
4202
tcO
42
]VC.sc[
1v97981.0142:viXraPreprint
leveragesdeformableattentionincontext-awarehypernetworkstoguideGaussian
pruningandsplitting, ensuringaccuraterepresentationincomplexregionswhile
reducingredundancy.Furthermore,wedesignatransformer-basedIterativeGaus-
sian Refiner module that refines Gaussian representations through direct image-
Gaussianinteractions. OurPixelGaussiancaneffectivelyreduceGaussianredun-
dancy as input views increase. We conduct extensive experiments on the large-
scaleACIDandRealEstate10Kdatasets,whereourmethodachievesstate-of-the-
art performance with good generalization to various numbers of views. Code:
https://github.com/Barrybarry-Smith/PixelGaussian.
1 INTRODUCTION
Novelviewsynthesis(NVS)seekstoreconstructa3Dscenefromaseriesofinputviewsandgener-
atehigh-qualityimagesfrompreviouslyunseenviewpoints. High-qualityandreal-timereconstruc-
tionandviewsynthesisarecrucialforautonomousdriving(Tonderskietal.,2023;Khanetal.,2024;
Tianetal.,2024;Huangetal.,2024a),roboticsperception(Wilder-Smithetal.,2024; Jiangetal.,
2023a)andvirtualoraugmentedreality(Yangetal.,2024; Zhengetal.,2024).
NeRF-based methods ( Mildenhall et al., 2020; Hu et al., 2022; Liu et al., 2020; Neff et al.,
2021) have achieved remarkable success by encoding 3D scenes into implicit radiance fields, yet
sampling volumes for NeRF rendering is costly in both time and memory. Recently, Kerbl et al.
(2023)proposedtorepresent3Dscenesexplicitlyusingasetof3DGaussians,enablingmuchmore
efficient and high-quality rendering via a differentiable rasterizer. Still, the original 3D Gaussian
Splattingrequiresseparateoptimizationoneachsinglescene,whichsignificantlyreducesinference
efficiency. Totacklethisproblem,recentresearcheshaveaimedatgenerating3DGaussiansdirectly
fromafeed-forwardnetworkwithoutanyper-sceneoptimization(Charatanetal.,2023; Chenetal.,
2024; Liuetal.,2024; Szymanowiczetal.,2024; Zhengetal.,2024). Typically,theseapproaches
adhere to a paradigm where a fixed number of Gaussians is predicted for each pixel in the input
views. The Gaussians derived from different views are then directly merged to construct the final
3Dscenerepresentation. However,suchaparadigmlimitsthemodelperformanceastheGaussian
splatsareuniformlydistributedacrossimages,makingitdifficulttocapturelocalgeometricdetails
effectively. Additionally, as the number of input views increases, directly merging Gaussians can
degradereconstructionperformanceduetosevereGaussianoverlapandredundancyacrossviews.
Toaddressthis,weproposePixelGaussian,whichenablesdynamicadaptiononboth3DGaussian
distributionandquantity. Tobespecific,wefirstuniformlyinitializeGaussianpositionsfollowing
Chen et al. (2024) to accurately localize the Gaussian centers. To identify geometry complexity
acrossimages,wethencomputearelevancescoremapforeachinputviewfromimagefeaturesin
anend-to-endmanner.Undertheguidanceofscoremaps,weconstructaCascadeGaussianAdapter
(CGA),whichleveragesdeformableattention(Xiaetal.,2022)tocontrolthepruningandsplitting
operations. AfterCGA,moreGaussiansareallocatedtoregionswithcomplexgeometryforprecise
reconstruction,whileunnecessaryandduplicateGaussiansacrossviewsareprunedtoreduceredun-
dancyandimproveefficiency. SincetheseGaussianrepresentationsstillfallshortinfullycapturing
theimagedetails,wefurtherintroduceatransformer-basedIterativeGaussianRefiner(IGR)torefine
3D Gaussians through direct image-Gaussian interactions. Finally, we employ rasterization-based
renderingusingtherefinedGaussianstogeneratenovelviewsattargetviewpoints.
We conduct extensive experiments on ACID (Liu et al.) and RealEstate10K (Zhou et al., 2018)
benchmarks for large-scale3D scene reconstruction and novel viewsynthesis. PixelGaussian out-
performs existing methods on different input views with a comparable inference speed. Notably,
existinggeneralizable3DGaussiansplattingmethods(pixelSplat(Charatanetal.,2023)andMVS-
plat (Chen et al., 2024)) fail to achieve good results when transferring to more input views while
ourmethoddemonstratesconsistentperformance. Thisisbecauseexistingpixel-wisemethodsgen-
erate uniform pixel-aligned Gaussian predictions, and our model mitigates Gaussian overlap and
redundancyacrossviewsbydynamicallyadjustingtheirdistributionbasedonlocalgeometrycom-
plexity. VisualizationsandablationsfurtherdemonstratethatbothCGAandIGRblocksarecrucial
inadaptingGaussiandistribution,enablingtheproposedPixelGaussiantocapturegeometrydetails
andachievebetterreconstructionaccuracy.
2Preprint
2 RELATED WORK
Multi-ViewStereo. Multi-ViewStereo(MVS)aimstoreconstructa3Drepresentationfrommulti-
view images of a given scene or object. Since accurate depth estimation is essential for reliable
3D reconstruction from 2D inputs, most MVS methods ( Gu et al., 2020; Ding et al., 2021; Yao
etal.,2018)requiregroundtruthdepthforsupervisionintrainingprocess.Additionally,point-based
MVS approachesgenerally separate the processesof depth estimation andpoint cloud fusion pro-
cesses. Recently, inspired by efficient Gaussian representations proposed by Kerbl et al. (2023),
Chenetal.(2024)introducestodirectlypredictdepthforpixel-wiseGaussiansfromacostvolume
structure without requiring depth supervision, significantly improving model scalability and flexi-
bility. Therefore,followingasimilarapproach,weconstructalightweightcostvolumetofacilitate
depthestimation,whichservesasanefficientinitializationfor3DGaussiansinourPixelGaussian.
Per-scene 3D Reconstruction. Neural Radiance Fields (NeRF) have revolutionized the field of
3Dreconstructionbyrepresentingscenesasimplicitneuralfields(Mildenhalletal.,2020). Subse-
quent researches have focused on overcoming the limitations of the original NeRF to improve its
performanceandbroadenitsapplicability. Someresearchesaimtoimprovetheefficiencyfornovel
view synthesis ( Hu et al., 2022; Fridovich-Keil et al., 2022; Yu et al., 2021a; Liu et al., 2020;
Neffetal.,2021). Moreover, severalstudiesconcentrateoncapturingintricategeometryandtem-
poralinformationtoachieveaccurateanddynamicreconstruction(Lietal.,2021; Duetal.,2021;
Pumarola et al., 2020; Tian et al., 2023; Wang et al., 2022). Compared to implicit NeRF-based
methods,3DGaussianSplatting(3DGS)(Kerbletal.,2023)representsa3Dscenarioasasetofex-
plicit 3D Gaussians, enabling a rasterization-based splatting rendering process that is significantly
moreefficientinbothtimeandmemory. Giventhat3DGSstillrequiresmillionsof3DGaussians
to represent a single scene, numerous studies have focused on achieving real-time rendering and
minimizingmemoryusage(Fanetal.,2023; Katsumataetal.,2024; Luetal.,2024).Additionally,
someresearchesfocusonenhancingthereconstructionqualityof3DGSbyemployingmulti-scale
rendering(Yanetal.,2024),advancedshadingmodels(Jiangetal.,2023b)orincorporatingphysi-
callybasedpropertiesforrealisticrelighting(Gaoetal.,2023).However,thesemethodsstillrequire
per-sceneoptimizationandrelyondenseinputviews,whichcanbecomputationallyexpensiveand
limittheirscalabilityforlarge-scaleordynamicscenes.
Generalizable 3D Reconstruction. PixelNeRF (Yu et al., 2021b) pioneers the approach of pre-
dictingpixel-wisefeaturesdirectlyfrominputviewstoreconstructneuralradiancefields. Follow-
ing methods incorporate volume or transformer architectures to improve the performance of feed-
forwardNeRFmodels(Chenetal.,2021a; Xuetal.,2024; Miyatoetal.,2024; Sajjadietal.,2022;
Duetal.,2023).However,thesefeed-forwardNeRFapproachestypicallydemandsubstantialmem-
ory and computational resources due to the expensive per-pixel volume sampling process (Wang
etal.,2021a;Joharietal.,2022;Barronetal.,2021;Garbinetal.,2021;Reiseretal.,2021;Mu¨ller
etal.,2022). Withtheadventof3DGS,PixelSplat(Charatanetal.,2023)initiatesashifttowards
feed-forward Gaussian-based reconstruction. It takes sparse input views to directly predict pixel-
wise 3D Gaussians by leveraging epipolar geometry to learn cross-view features. MVSplat (Chen
etal.,2024)constructsacostvolumestructurefordepthestimation,whichsignificantlyboostsboth
modelefficiencyandreconstructionquality. Additionally, MVSGaussian(Liuetal.,2024)further
improvesmodelperformancebyintroducinganefficienthybridGaussianrenderingprocess. More-
over, SplatterImage (Szymanowicz et al., 2024) and GPS-Gaussian (Zheng et al., 2024) predict
pixel-wise3DGaussiansforobject-centricorhumanreconstruction.
However, these feed-forward methods are constrained by the pixel-wise Gaussian prediction
paradigm, which limits the model’s performance as the Gaussian splats are uniformly distributed
across images. Such a paradigm inadequately captures intricate geometries, while also causing
Gaussian overlap and redundancy across views, ultimately resulting in severe rendering artifacts.
In comparison, PixelGaussian consists of a Cascade Gaussian Adapter (CGA), allowing for dy-
namic adaption on both Gaussian distribution and quantity. Visualizations demonstrate that CGA
iscapableofallocatingmoreGaussiansinareasrichingeometricdetails,whilereducingduplicate
Gaussians in similar regions across input views. Furthermore, we introduce an Iterative Gaussian
Refiner (IGR), enabling direct interaction between 3D Gaussians and local image features via de-
formableattention.ExperimentalresultsshowthatIGReffectivelyleveragesimagefeaturestoguide
Gaussiansincapturingthefullinformationcontainedwithintheimages,significantlyenhancingthe
model’sabilitytocapturelocalintricategeometry.
3Preprint
Uniform Pixel-wise Paradigm
Image
Encoder
Input Images Aggregated Features Cost Volume Depth Maps Initial Gaussians
Refined Gaussians Deformable Attn. Adaptive Gaussians Split & Prune
Scorer
Context-Aware
Hypernet
Novel Views Iterative Gaussian Refiner (IGR) Cascade Gaussian Adpater (CGA)
Figure2: OverviewofPixelGaussian. Givenmulti-viewinputimages,weinitialize3DGaussians
usingalightweightimageencoderandcostvolume.CascadeGaussianAdapter(CGA)thendynam-
ically adapts both the distribution and quantity of Gaussians. By leveraging local image features,
IterativeGaussianRefiner(IGR)furtherrefinesGaussianrepresentationsviadeformableattention.
Finally,novelviewsarerenderedfromtherefined3DGaussiansusingrasterization-basedrendering.
3 PROPOSED APPROACH
Inthissection,wepresentourmethodtolearngeneralizableGaussianrepresentationsfromarbitrary
views. Given an arbitrary set of input images I = {I }N ∈ RN×H×W×3 and corresponding
i i=1
camera poses C = {C }N , our PixelGaussian aims to learn a mapping M from images to 3D
i i=1
Gaussiansforscenereconstruction:
M:{(I ,C )}N (cid:55)→{(µ ,s ,r ,α ,sh )}NK, (1)
i i i=1 j j j j j j=1
where N is the total number of 3D Gaussians, which adaptively varies depending on the scene
K
context. EachGaussianisparameterizedbyitspositionµ ,scalings ,rotationr ,opacityα and
j j j j
sphericalharmonicssh .
j
AsillustratedinFigure2,wefirstusealightweightcostvolumefordepthestimationandGaussian
position initialization. We then introduce Cascade Gaussian Adapter (CGA), which dynamically
adapts both Gaussian distribution and quantity based on local geometric complexity. Finally, we
explain how Iterative Gaussian Refiner (IGR) enables direct image-Gaussian interactions, further
refiningGaussiandistributionandrepresentationsforenhancedreconstruction.
3.1 GAUSSIANINITIALIZATION
PositionInitialization. FollowingtheinstructionsofMVSplat(Chenetal.,2024),wefirstextract
image features via a 2D backbone consisting of CNN and Swin Transformer (Liu et al., 2021).
Specifically, CNN encodes multi-view images to corresponding feature maps, while Swin Trans-
former performs both self-attention and cross-view attention to better leverage information cross
views. Then,weobtaintheaggregatedmulti-viewfeaturesF ={F }N .
i i=1
ToinitializeGaussianpositionsprecisely,weconstructalightweightcostvolume(Yaoetal.,2018)
fordepthestimation,denotedasΦ . WethenpredictGaussiancentersasfollows:
depth
µ=P−1(Φ (F),C) (2)
depth
whereP−1(·)standsforunprojectionoperation.
Parameter Initialization. For each Gaussian center µ , we randomly set corresponding scaling
j
s ∈R3,rotationr ∈R4,opacityα ∈R1,sphericalharmonicssh ∈RC withinaproperrange.
j j j j
wethengettheinitialGaussianssetG ={(µ ,s ,r ,α ,sh )}HW ∈RHW×(11+C).
j j j j j j=1
3.2 CGA:CASCADEGAUSSIANADAPTER
After obtaining the initial Gaussian set G, we introduce Cascade Gaussian Adapter (CGA) driven
byamulti-viewkeypointscorerΨ, asshowninFigure3(a). CGAcontainsasetofcontext-aware
4Preprint
Deformable Attn.
Refine
Scorer
&
Decode
Aggregated Features Score Maps Density Maps
Sample
Context-Aware Split &
Hypernet
Encode
Uniform Gaussians Prune Adaptive Gaussians Gaussian Queries Refined Gaussians
(a) CGA block (b) IGR block
Figure 3: Illustration of the proposed CGA and IGR Blocks. (a) CGA comprises a keypoint
scorer followed by a series of hypernetworks that produce context-aware thresholds to guide the
splitting and pruning of Gaussians. (b) IGR further facilitates direct image-Gaussian interactions,
enablingGaussianrepresentationstocaptureandextractlocalgeometricfeaturesmoreeffectively.
hypernetworksHwhichdynamicallycontrolandguidethefollowingGaussianpruningandsplitting
operations. Thisapproachensuresthatregionswithcomplexgeometrydetailsarerepresentedbya
greaternumberofGaussians,whileareaswithpoorgeometrycanberepresentedwithfewerGaus-
sians. Inparallel,CGAeffectivelyremovesredundantGaussianstopreventGaussianoverlapacross
views. Compared to previous pixel-wise methods, which rigidly allocate a fixed number of Gaus-
sians per pixel, our design dynamically adapts both distribution and quantity of Gaussians based
ongeometriccomplexity. Thisflexibilityallowsforamoreaccuratecaptureoflocalgeometryand
mitigatestheproblemofGaussianoverlap,therebyimprovingtheoverallqualityofreconstruction.
Given the aggregated features F derived in Section 3.1, Ψ computes relevance score maps R =
{R }N ∈ RN×H×W, where each score map R is obtained by a learnable weighted average of
i i=1 i
contributionsfromdifferentviews:
(cid:32) (cid:32) N (cid:33)(cid:33)
R=Ψ(F)=softmax MLP
(cid:88)
α ·F , α =
exp(β i)
, (3)
i i i (cid:80)N
exp(β )
i=1 j=1 j
whereA = [α ,α ,...,α ]T ∈ RN representsthecontributionfactorofeachview,andisdeter-
1 2 N
minedbylearnableparametersβ (i=1,2,...,N).
i
WefirstintroduceasetofhypernetworksH={H }K togeneratecontext-awarethresholds.CGA
k k=1
iscomposedofK stages,whereeachstageH takesscoremapsRalongwithGaussiansetG =
k k
{(µ(k),s(k),r(k),α(k),sh(k))}Nk ∈ RNk×(11+C) asinput,andoutputsthresholdsτ(k) ,τ(k) ∈ R
j j j j j j=1 high low
forsplittingandpruning. Asillustratedinequation4, wefirstsampleandembedGaussiansetG
k
intoGaussianscorequeriesQ(k). Thenweprojectsampledreferencepointsµ(k) ontoscoremaps
r
RwithcorrespondingcameraparametersC. Finally,weupdatequeriesQ(k) withweightedscores
r
fromS andgetboththresholdsthroughasimpleMLP.Initially,wesetG =G.
1
N
τ(k) ,τ(k) =H (G ,R,C)=MLP((cid:88) α ·DA(Q(k),R ,P(µ(k),C ))), (4)
high low k k i r i i
i=1
whereDA(·),P(·)denotethedeformableattentionfunctionandprojectionoperation,respectively.
Then,weobtainGaussian-wisescoresbyprojectingGaussiancentersontoscoremapsR. Toelab-
orate, let S
k
= {s( ijk)} ∈ RN×Nk be the score matrix for Gaussian set G k, where each score s( ijk)
is the value at the projection point of the j −th Gaussian center in R , or 0 if it does not project
i
ontoanyregioninR . ThefinalGaussian-wisescoresSavg arethencomputedbyaveragingscores
i k
acrossdifferentviews:
Savg =ST ·A, (5)
k k
5Preprint
OnceGaussian-wisescoresareobtained, regionswithhigherscores, indicatingmorecomplexge-
ometrydetails,undergosplittingoperationtoallocatemoreGaussiansforfinerrepresentations. For
regionswithlowerscores,weapplyanopacity-basedpruningoperation,graduallyreducingGaus-
sianopacityandscalingtominimizetheirimpactandreduceredundancy.
Splitting. ForGaussiang(k) ∈G withscorehigherthanτ(k),wegenerateM separatenewGaus-
j k high
siansformoredetailedrepresentations:
G(k) =SplitNet(g(k))∈RM×(11+C), (6)
j j
whereSplitNet(·)isasimpleMLP-basednetworkthatensuresallparameterswithinproperrange.
ThenewlygeneratedGaussiansarethendirectlyconcatenatedwiththeexistingGaussiansetG .
k
Pruning. ForGaussiang(k) ∈ G withscorelowerthanτ(k), weapplyanopacity-basedpruning
j k low
operationratherthandirectlyremovingit. Specifically,wesetapredefinedopacitythresholdτ . If
α
theGaussianopacityisgreaterthanτ ,wegraduallyreduceitsopacityandscaling:
α
α(k) →γ ·α(k), s(k) →γ ·s(k), (7)
j α j j s j
whereγ <1andγ <1arereductionfactors.Otherwise,thecurrentGaussianisremovedentirely
α s
fromGaussiansetG .
k
AfterK-stageadaptationintheCascadeGaussianAdapter, theinitialuniform3DGaussianrepre-
sentationsaretransformedintoadaptiveforms. Gaussiansareredistributedaccordingtogeometric
complexity,resultinginamoreefficientandcontext-awarerepresentation.
3.3 IGR:ITERATIVEGAUSSIANREFINER
ThoughCGAallowsforamoreoptimalGaussiandistribution,theGaussianrepresentationsstillfall
short in capturing the full information contained in the images. Inspired by the efficiency demon-
stratedbyGaussianFormer(Huangetal.,2024b)inoccupancyprediction,wedesignatransformer-
based Iterative Gaussian Refiner (IGA) to further extract local geometric information from input
views, asshown inFigure3(b). In thisprocess, weleverage deformableattentiontoenable direct
image-Gaussian interactions, enhancing the ability for 3D Gaussians to more accurately capture
intricategeometrydetailsinreconstructionandviewsynthesis.
IGR is composed of B attention and refinement blocks. In Section 3.2, CGA adapts the original
Gaussian set G to G = G . To continue, we first sample and embed G into Gaussian queries Q.
K
In each block, deformable attention is first applied between Gaussian queries Q and multi-view
features F to update Gaussian representations. This is followed by a refinement stage where a
residualmodulefurtherfine-tunesthequeries. TheoverallprocessofIGRcanbeformulatedas:
N
(cid:88)
Q =Φ ( α ·DA(Q ,F ,P(µ(b),C ))) b=1,2,...,B, (8)
b ref i b−1 i i
i=1
whereDA(·),Φ (·),P(·)denotethedeformableattentionfunction,refinementlayerandprojec-
ref
tionoperation,F ,C ,α representstheimagefeature,cameraparametersandcontributionfactorof
i i i
inputviewI ,respectively. Q (b=1,2,...,B)standsforoutputqueriesfromtheb−thIGRblock,
i b
andµ(b)istheGaussiancenterofcurrentstage. Initially,wesetQ =Q.
0
Finally, the refined Gaussian queries are decoded into Gaussian parameters G through a simple
f
MLP to ensure all parameters within proper range, and then can be used for rasterization-based
renderingatnovelviewpoints.
G ={(µf,sf,rf,αf,shf)}NK =MLP(Q ). (9)
f j j j j j j=1 B
Ourfullmodeltakesground-truthtargetRGBimagesatnovelviewpointsassupervision,allowing
forefficientend-to-endtraining. ThetraininglossiscalculatedasalinearcombinationofMSEand
LPIPS(Zhangetal.,2018)losses,withlossweightsof1and0.05,respectively.
6Preprint
Table 1: Results of Novel View Synthesis on the RealEstate10K and ACID benchmarks. We
reporttheaveragePSNRandLPIPS(Zhangetal.,2018)onthetestset,whereallmodelsaretrained
with2referenceviewsandinferredwith2,3,and4referenceviews.
2→2Views 2→3Views 2→4Views
Datasets Methods
PSNR↑ LPIPS↓ PSNR↑ LPIPS↓ PSNR↑ LPIPS↓
pixelNeRF 20.25 0.556 21.15 0.525 21.68 0.518
MuRF 25.95 0.146 26.23 0.137 26.40 0.134
RealEstate10K pixelSplat 25.67 0.145 22.35 0.242 20.12 0.283
MVSplat 26.25 0.130 22.94 0.236 20.74 0.268
PixelGaussian 26.72 0.126 26.79 0.123 26.85 0.122
pixelNeRF 20.66 0.530 21.33 0.518 21.40 0.506
MuRF 27.94 0.163 28.22 0.154 28.35 0.149
ACID pixelSplat 28.06 0.152 23.73 0.276 20.15 0.294
MVSplat 28.26 0.144 23.85 0.268 20.32 0.275
PixelGaussian 28.63 0.140 28.72 0.137 28.78 0.137
Compared to the uniform pixel-wise paradigm, our PixelGaussian approach dynamically adapts
boththeGaussiandistributionandquantitywithintheCascadeGaussianAdapter. Additionally,the
IterativeGaussianRefinerrefinesGaussianrepresentationstocaptureintricategeometricdetailsin
theinputviews. ThisdesignachievesmoreefficientGaussiandistributionswhilemitigatingoverlap
andredundancycommoninpixel-wisemethods.
4 EXPERIMENTS
4.1 EXPERIMENTALSETTINGS
Datasets. To assess the performance of our model, we conduct experiments on two extensive
datasets: ACID (Liu et al.) and RealEstate10K (Zhou et al., 2018). The ACID dataset consists
ofvideoframescapturingnaturallandscapescenes,comprising11,075scenesinthetrainingsetand
1,972 scenes in the test set. RealEstate10K provides video frames from real estate environments,
with67,477scenesallocatedfortrainingand7,289scenesreservedfortesting. Themodelistrained
withtworeferenceviews,andfournovelviewsareselectedforevaluation. Duringtesting,however,
we perform multiple experiments where 2, 3, and 4 views are selected for reference, while four
novelviewsselectedforevaluationineachscenario.
ImplementationDetails. Wesettheresolutionsofinputimagesas256x256. InCascadeGaussian
Adapter(CGA),weapplyK =3stagesofcascadeGaussianadaption.Asforthesplittingoperation,
theSplitNetgeneratesM =1separatenewGaussians,whereasthepruningprocessusesreduction
factorsγ = γ = 0.5andopacitythresholdτ = 0.3. WeuseB = 3blocksinIterativeGaussian
α s α
Refiner (IGR) to extract local geometry from input views. We implement our PixelGaussian with
Pytorchandtrainthemodelon8NVIDIAA6000GPUsfor300,000iterationswithAdamoptimizer.
MoretrainingdetailsareprovidedinSectionA.2.
4.2 MAINRESULTS
NovelViewSynthesis. AsshowninTable1andFigure4,ourproposedPixelGaussianconsistently
outperformspreviousNeRF-basedmethodsandpixel-wiseGaussianfeed-forwardnetworksacross
allsettingswith2,3,and4referenceviews. Notably,asthenumberofinputviewsincreases,there-
constructionperformanceofbothpixelSplat(Charatanetal.,2023)andMVSplat(Chenetal.,2024)
degradessignificantly, whilePixelGaussianshowsaslightimprovement. Thisisbecauseprevious
methodsdirectlymergemultipleviewsbyback-projectingpixel-wiseGaussiansto3Dspacebased
ondepthmaps. WithoutthecapabilitytoadaptthequantityanddistributionofGaussiansdynam-
ically, pixel-wise methods often produce duplicated Gaussians with significant overlap, and their
spatialpositioningissuboptimal. Incontrast,PixelGaussianisabletooptimizeboththedistribution
andquantityofGaussiansviaCGA,whileIGRblocksfacilitatedirectinteractionbetweenGaussian
queriesandlocalimagefeatures,resultinginmoreaccuratereconstructions.
7Preprint
Image Input pixelSplat MVSplat PixelGaussian Ground Truth
Figure 4: Visualization resultson ACIDand RealEstate10Kbenchmarks. Pixel-wisemethods
sufferfromGaussianoverlapduetosuboptimalGaussiandistributions,whereasPixelGaussianen-
ablesdynamicGaussianadaptionandimprovedlocalgeometryrefinement.
Table2: ComparisonofPSNRandGaussianQuantityonRealEstate10KDataset. Wepresent
theaveragePSNRandthenumberofGaussians(K)forinferenceusing2,4,and6inputviews.
2→2Views 2→4Views 2→6Views
Methods
PSNR↑ #Gaussians PSNR↑ #Gaussians PSNR↑ #Gaussians
pixelSplat 25.67 393K 20.12 786K 19.36 1179K
MVSplat 26.25 131K 20.74 262K 20.24 393K
PixelGaussian 26.72 188K 26.85 240K 26.89 278K
Multi-ViewComparison. WefurthercomparemodelperformanceandGaussianquantitiesofdif-
ferentmethodsacrossvariousinputviewsinTable2. ThoughourmethodrequiresmoreGaussians
than MVSplat (Chen et al., 2024) with 2 input views due to more frequent splitting than pruning,
itachievesbetterreconstructionwithfewerGaussiansasthenumberofviewsincreases. Inregions
withrichergeometricdetails, CGAblocksfirstsplitmoreGaussiansforfinerrepresentations, fol-
lowedbyIGRtofurtherrefinetheseGaussiansusingdeformableattentiononlocalimagefeaturesto
bettercaptureandreconstructgeometricdetails.Meanwhile,CGAprunesduplicateandoverlapping
Gaussians across views to control the growth of overall Gaussian quantity as the number of input
viewsincreases.
Efficiency Analysis. We explore the efficiency of PixelGaussian compared with dominant pixel-
wisemethodsonasingleNVIDIAA6000GPU.Allmodelsareinferredwith4inputviewsonthe
RealEstate10K(Zhouetal.,2018)dataset. Wereporttheaverageinferencelatency, memorycost,
number of Gaussians and rendering FPS in Table 3. Undeniably, PixelGaussian requires higher
latencyandmemoryusagethanMVSplat(Chenetal.,2024)duetotheextracostofCGAandIGR
blocks. However, thistrade-offallowsPixelGaussiantoachievehigherrenderingFPSbyutilizing
fewerGaussiansandfarmorebetterreconstructionqualityasthenumberofinputviewincreases.
4.3 EXPEIMENTALANALYSIS
Inthissection, wefurtherinvestigateandconductexperimentstodemonstratetheeffectivenessof
ourPixelGaussian.WefirstvisualizethescoremapsSandGaussiandensitymaps.Then,wepresent
the cascade adaption process of CGA. Finally, we conduct ablation studies on our model. These
8Preprint
Table3: Efficiencyanalysis. InferenceonRealEstate10K(Zhouetal.,2018)datasetusing4input
views,reportingaveragelatency,memorycost,Gaussianquantity,andrenderingFPS.
Methods Latency Memory #Gaussians RenderingFPS
pixelSplat 298ms 11.78G 786K 64
MVSplat 126ms 3.17G 262K 133
PixelGaussian 235ms 4.39G 240K 140
Gaussian density
(Gaussian / pixel)
Input Views Score Maps Gaussian Density Novel Views
Figure 5: Visualization of score maps and Gaussian distributions on RealEstate10K dataset.
Cascade Gaussian Adapter dynamically adjusts Gaussian distribution and quantity based on score
maps. More Gaussians are allocated to detail-rich regions for more precise representations, while
pruningminimizesGaussianredundancyandoverlapacrossviews.
experimentsshowthatCGAdynamicallyadaptsboththedistributionandquantityofGaussiansac-
cordingtogeometriccomplexity,whileIGRfurtherextractlocalfeaturesviadirectimage-Gaussian
interactions,offeringsignificantimprovementsovertraditionalpixel-wisemethods.
Score Maps and Density Maps. As shown in 3, relevance score maps S are derived from multi-
viewimagefeaturesthroughthekeypointscorerΨ. TofurtherunderstandthesignificanceofS and
itsimpactonthesubsequentGaussiansplittingandpruningoperations,wevisualizetherelevance
scoremapsontheRealEstate10K(Zhouetal.,2018)dataset. Furthermore, weprojectthecenters
oftheadaptiveGaussianafterCGA(i.e.{µf}NK),ontoeachinputviewasGaussiandensitymaps.
j j=1
Figure 5 illustrates that in this end-to-end learning framework, keypoint scorer Ψ is able to learn
such score maps where local regions with richer and more complex geometric details tend to re-
ceivehigherscore,whichguidesthefollowingSplitNettoallocatemoreGaussiansformoreprecise
representations. In contrast, regions with fewer geometric details receive lower scores, leading to
pruningtoreducerepresentationcomplexitywhilepreservingoverallrepresentationefficiency.
Cascade Gaussian Adaption. To present the cascade Gaussian adaption process in CGA
blocks more explicitly, we visualize Gaussian density maps during model inference on the
RealEstate10K(Zhouetal.,2018)datasetwith2inputviews. AsillustratedinFigure6,theoriginal
Gaussian set G is initialized on a pixel-wise basis. Following this, score maps S guide hypernet-
worksHtogeneratecontext-awarethresholds,whichinturndirectthesubsequentGaussiansplitting
andpruningoperations.MoreGaussiansareallocatedinregionswithrichergeometricdetails,while
duplicateGaussiansacrossviewstendtobepruned,leadingtoamoreefficientandoptimalGaussian
distributionforscenerepresentation.
Ablation Study. To further investigate the architecture of PixelGaussian, we conduct ablation
studies by inferring our model on RealEstate10K (Zhou et al., 2018) test dataset with 4 input
views. We first introduce a vanilla model, where the initial Gaussian set G is directly used to
render novel views. Then, we adopt rigid CGA blocks without context-aware Hypernetworks
H, which means Gaussian set G goes through splitting and pruning based on fixed thresholds
(τ(k) = 0.8,τ(k) = 0.2,k = 1,2,...,K). WefurtheraddHyperNetworksHtogeneratecontext-
high low
9Preprint
Gaussian density (Gaussian / pixel)
Score Maps Stage 0 (131K) Stage 1 (152K) Stage 2 (175K) Stage 3 (186K)
Figure6: VisualizationofGaussianadaptionprocessinCascadeGaussianAdapter. Duringin-
ferenceonRealEstate10K(Zhouetal.,2018)dataset,CGAprogressivelyconcentratesmoreGaus-
siansongeometricallycomplexregions,whilepruningduplicateGaussiansacrossviewstocontrol
thegrowthofoverallGaussianquantity.
Table 4: Ablations on the components of PixelGaussian. We report the average PSNR,
LPIPS(Zhangetal.,2018),andthenumberofGaussians(K)ofmodelinference.
Methods PSNR↑ LPIPS↓ #Gaussians
Vanilla 20.34 0.272 262K
+RigidCascadeGaussianAdapter 22.46 0.220 225K
+HyperNetworksH 25.80 0.140 240K
+IterativeGaussianRefiner 26.85 0.122 240K
aware thresholds. Finally, we adopt IGR blocks to refine the Gaussian representations via image-
Gaussian interactions. As shown in Table 4, HyperNetworks H utilizes score maps S to generate
context-aware thresholds, enabling a more dynamic and efficient Gaussian distribution for scene
representation compared to rigid splitting and pruning. Furthermore, IGR blocks refine the Gaus-
siansetiterativelyviadeformableattentionbetweenGaussiansandimagefeatures,enhancingtheir
abilitytodescribeandreconstructintricatelocalgeometricdetails.
5 CONCLUSION AND DISCUSSIONS
In this paper, we have presented PixelGaussian to learn generalizable 3D Gaussian reconstruction
fromarbitraryinputviews. Thecoreinnovationofourapproachiscontext-awareCascadeGaussian
Adapter(CGA),whichdynamicallysplitsGaussiansinregionswithcomplexgeometricdetailsand
prunesredundantones. Further,weincorporatedeformableattentionwithinIterativeGaussianRe-
finer (IGR), facilitating direct image-Gaussian interactions to improve local geometry reconstruc-
tions. Compared to previous uniform pixel-wise methods, PixelGaussian is able to dynamically
adapt both Gaussian distribution and quantity guided by the complexity of local geometry details,
allocating more to detailed regions and reducing redundancy across views, thus leading to better
performanceinreconstructionandviewsynthesis.
DiscussionsandLimitations. AlthoughPixelGaussiancanadjustthedistributionof3DGaussians
dynamically,theinitialGaussiansarestillderivedfrompixel-wiseunprojection. Whenweinitialize
the Gaussian centers completely at random, the model fails to converge. Moreover, deformable
attention in IGR consumes substantial computational resources when the number of Gaussians is
extremely large, highlighting the need for a more efficient approach to represent 3D scenes with
fewer Gaussians. Furthermore, PixelGaussian is unable to perceive the unseen parts of 3D scenes
beyondtheinputviews,suggestingthepotentialneedtoincorporategenerativemodels.
10Preprint
REFERENCES
JonathanT.Barron,BenMildenhall,MatthewTancik,PeterHedman,RicardoMartin-Brualla,and
PratulP.Srinivasan.Mip-nerf:Amultiscalerepresentationforanti-aliasingneuralradiancefields,
2021.
David Charatan, Sizhe Li, Andrea Tagliasacchi, and Vincent Sitzmann. pixelsplat: 3d gaussian
splatsfromimagepairsforscalablegeneralizable3dreconstruction. InarXiv,2023.
Anpei Chen, Zexiang Xu, Fuqiang Zhao, Xiaoshuai Zhang, Fanbo Xiang, Jingyi Yu, and Hao Su.
Mvsnerf: Fastgeneralizableradiancefieldreconstructionfrommulti-viewstereo. arXivpreprint
arXiv:2103.15595,2021a.
Chun-Fu(Richard)Chen,RameswarPanda,andQuanfuFan. RegionViT:Regional-to-LocalAtten-
tionforVisionTransformers. InArXiv,2021b.
YuedongChen,HaofeiXu,ChuanxiaZheng,BohanZhuang,MarcPollefeys,AndreasGeiger,Tat-
Jen Cham, and Jianfei Cai. Mvsplat: Efficient 3d gaussian splatting from sparse multi-view
images. arXivpreprintarXiv:2403.14627,2024.
ZhiyangChen, YousongZhu, ChaoyangZhao, GuoshengHu, WeiZeng, JinqiaoWang, andMing
Tang. Dpt: Deformable patch-based transformer for visual recognition. In ACM MM. ACM,
October2021c.
Christopher B. Choy, Danfei Xu, JunYoung Gwak, Kevin Chen, and Silvio Savarese. 3d-r2n2: A
unifiedapproachforsingleandmulti-view3dobjectreconstruction,2016.
Yikang Ding, Wentao Yuan, Qingtian Zhu, Haotian Zhang, Xiangyue Liu, Yuanjiang Wang, and
XiaoLiu.Transmvsnet:Globalcontext-awaremulti-viewstereonetworkwithtransformers,2021.
XiaoyiDong,JianminBao,DongdongChen,WeimingZhang,NenghaiYu,LuYuan,DongChen,
andBainingGuo. Cswintransformer: Ageneralvisiontransformerbackbonewithcross-shaped
windows,2022.
AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,DirkWeissenborn,XiaohuaZhai,Thomas
Unterthiner,MostafaDehghani,MatthiasMinderer,GeorgHeigold,SylvainGelly,JakobUszko-
reit,andNeilHoulsby. Animageisworth16x16words: Transformersforimagerecognitionat
scale,2021.
Yilun Du, Yinan Zhang, Hong-Xing Yu, Joshua B. Tenenbaum, and Jiajun Wu. Neural radiance
flowfor4dviewsynthesisandvideoprocessing. InICCV,2021.
Yilun Du, Cameron Smith, Ayush Tewari, and Vincent Sitzmann. Learning to render novel views
fromwide-baselinestereopairs. CVPR,2023.
ZhiwenFan,KevinWang,KairunWen,ZehaoZhu,DejiaXu,andZhangyangWang.Lightgaussian:
Unbounded3dgaussiancompressionwith15xreductionand200+fps,2023.
Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo
Kanazawa. Plenoxels: Radiancefieldswithoutneuralnetworks. InCVPR,2022.
Jian Gao, Chun Gu, Youtian Lin, Hao Zhu, Xun Cao, Li Zhang, and Yao Yao. Relightable
3d gaussian: Real-time point cloud relighting with brdf decomposition and ray tracing.
arXiv:2311.16043,2023.
StephanJGarbin,MarekKowalski,MatthewJohnson,JamieShotton,andJulienValentin. Fastnerf:
High-fidelityneuralrenderingat200fps. arXivpreprintarXiv:2103.10380,2021.
Xiaodong Gu, Zhiwen Fan, Zuozhuo Dai, Siyu Zhu, Feitong Tan, and Ping Tan. Cascade cost
volumeforhigh-resolutionmulti-viewstereoandstereomatching,2020.
KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun. Deepresiduallearningforimagerecog-
nition. InCVPR,2016.
11Preprint
TaoHu,ShuLiu,YilunChen,TianchengShen,andJiayaJia.Efficientnerf:Efficientneuralradiance
fields. InCVPR,pp.12902–12911,June2022.
NanHuang,XiaobaoWei,WenzhaoZheng,PengjuAn,MingLu,WeiZhan,MasayoshiTomizuka,
Kurt Keutzer, and Shanghang Zhang. S3gaussian: Self-supervised street gaussians for au-
tonomousdriving. arXivpreprintarXiv:2405.20323,2024a.
Yuanhui Huang, Wenzhao Zheng, Yunpeng Zhang, Jie Zhou, and Jiwen Lu. Gaussianformer:
Scene as gaussians for vision-based 3d semantic occupancy prediction. arXiv preprint
arXiv:2405.17429,2024b.
AndrewJaegle,FelixGimeno,AndrewBrock,AndrewZisserman,OriolVinyals,andJoaoCarreira.
Perceiver: Generalperceptionwithiterativeattention,2021.
Mengqi Ji, Juergen Gall, Haitian Zheng, Yebin Liu, and Lu Fang. Surfacenet: An end-to-end 3d
neuralnetworkformultiviewstereopsis. InICCV,pp.2307–2315,2017.
ChenxingJiang,HanwenZhang,PeizeLiu,ZehuanYu,HuiCheng,BoyuZhou,andShaojieShen.
H -mapping: Real-timedensemappingusinghierarchicalhybridrepresentation. IEEERobotics
2
andAutomationLetters,8(10):6787–6794,October2023a. ISSN2377-3774.
YingwenqiJiang,JiadongTu,YuanLiu,XifengGao,XiaoxiaoLong,WenpingWang,andYuexin
Ma. Gaussianshader: 3dgaussiansplattingwithshadingfunctionsforreflectivesurfaces,2023b.
M. Johari, Y. Lepoittevin, and F. Fleuret. Geonerf: Generalizing nerf with geometry priors. In
CVPR,2022.
KaiKatsumata,DucMinhVo,andHidekiNakayama. Acompactdynamic3dgaussianrepresenta-
tionforreal-timedynamicviewsynthesis,2024.
BernhardKerbl,GeorgiosKopanas,ThomasLeimku¨hler,andGeorgeDrettakis. 3dgaussiansplat-
tingforreal-timeradiancefieldrendering. ACMTransactionsonGraphics,42(4),July2023.
Mustafa Khan, Hamidreza Fazlali, Dhruv Sharma, Tongtong Cao, Dongfeng Bai, Yuan Ren, and
Bingbing Liu. Autosplat: Constrained gaussian splatting for autonomous driving scene recon-
struction,2024.
ZhengqiLi,SimonNiklaus,NoahSnavely,andOliverWang.Neuralsceneflowfieldsforspace-time
viewsynthesisofdynamicscenes. InCVPR,2021.
Andrew Liu, Richard Tucker, Varun Jampani, Ameesh Makadia, Noah Snavely, and Angjoo
Kanazawa. Infinite nature: Perpetual view generation of natural scenes from a single image.
InICCV.
LingjieLiu,JiataoGu,KyawZawLin,Tat-SengChua,andChristianTheobalt. Neuralsparsevoxel
fields. NeurIPS,2020.
Tianqi Liu, Guangcong Wang, Shoukang Hu, Liao Shen, Xinyi Ye, Yuhang Zang, Zhiguo Cao,
WeiLi, andZiweiLiu. Mvsgaussian: Fastgeneralizablegaussiansplattingreconstructionfrom
multi-viewstereo. arXivpreprintarXiv:2405.12218,2024.
ZeLiu,YutongLin,YueCao,HanHu,YixuanWei,ZhengZhang,StephenLin,andBainingGuo.
Swintransformer: Hierarchicalvisiontransformerusingshiftedwindows. InICCV,2021.
TaoLu,MulinYu,LinningXu,YuanboXiangli,LiminWang,DahuaLin,andBoDai. Scaffold-gs:
Structured3dgaussiansforview-adaptiverendering. InCVPR,pp.20654–20664,2024.
BenMildenhall,PratulP.Srinivasan,MatthewTancik,JonathanT.Barron,RaviRamamoorthi,and
RenNg. Nerf: Representingscenesasneuralradiancefieldsforviewsynthesis. InECCV,2020.
TakeruMiyato,BernhardJaeger,MaxWelling,andAndreasGeiger. Gta: Ageometry-awareatten-
tionmechanismformulti-viewtransformers. InICLR,2024.
ThomasMu¨ller,AlexEvans,ChristophSchied,andAlexanderKeller. Instantneuralgraphicsprim-
itiveswithamultiresolutionhashencoding. ACMTrans.Graph.,41(4):102:1–102:15,July2022.
12Preprint
T. Neff, P. Stadlbauer, M. Parger, A. Kurz, J. H. Mueller, C. R. A. Chaitanya, A. Kaplanyan, and
M. Steinberger. Donerf: Towards real-time rendering of compact neural radiance fields using
depthoraclenetworks. ComputerGraphicsForum,40:45–59,2021.
AlbertPumarola,EnricCorona,GerardPons-Moll,andFrancescMoreno-Noguer. D-nerf: Neural
radiancefieldsfordynamicscenes. arXivpreprintarXiv:2011.13961,2020.
Christian Reiser, Songyou Peng, Yiyi Liao, and Andreas Geiger. Kilonerf: Speeding up neural
radiancefieldswiththousandsoftinymlps,2021.
Mehdi S. M. Sajjadi, Henning Meyer, Etienne Pot, Urs Bergmann, Klaus Greff, Noha Radwan,
Suhani Vora, Mario Lucic, Daniel Duckworth, Alexey Dosovitskiy, Jakob Uszkoreit, Thomas
Funkhouser,andAndreaTagliasacchi. SceneRepresentationTransformer: Geometry-FreeNovel
ViewSynthesisThroughSet-LatentSceneRepresentations. CVPR,2022.
Vincent Sitzmann, Justus Thies, Felix Heide, Matthias Nießner, Gordon Wetzstein, and Michael
Zollho¨fer. Deepvoxels: Learningpersistent3dfeatureembeddings. InProc.CVPR,2019.
Shuyang Sun, Xiaoyu Yue, Song Bai, and Philip Torr. Visual parser: Representing part-whole
hierarchieswithtransformers,2022.
Stanislaw Szymanowicz, Christian Rupprecht, and Andrea Vedaldi. Splatter image: Ultra-fast
single-view3dreconstruction. InCVPR,2024.
M.Tatarchenko,A.Dosovitskiy,andT.Brox. Octreegeneratingnetworks: Efficientconvolutional
architecturesforhigh-resolution3doutputs. InICCV,2017.
FengruiTian,ShaoyiDu,andYueqiDuan. MonoNeRF:Learningageneralizabledynamicradiance
fieldfrommonocularvideos. InICCV,October2023.
Qijian Tian, Xin Tan, Yuan Xie, and Lizhuang Ma. Drivingforward: Feed-forward 3d gaussian
splattingfordrivingscenereconstructionfromflexiblesurround-viewinput,2024.
Adam Tonderski, Carl Lindstro¨m, Georg Hess, William Ljungbergh, Lennart Svensson, and
Christoffer Petersson. Neurad: Neural rendering for autonomous driving. arXiv preprint
arXiv:2311.15260,2023.
Liao Wang, Jiakai Zhang, Xinhang Liu, Fuqiang Zhao, Yanshun Zhang, Yingliang Zhang, Minye
Wu,JingyiYu,andLanXu.Fourierplenoctreesfordynamicradiancefieldrenderinginreal-time.
InCVPR,pp.13524–13534,2022.
QianqianWang,ZhichengWang,KyleGenova,PratulSrinivasan,HowardZhou,JonathanT.Bar-
ron, Ricardo Martin-Brualla, Noah Snavely, and Thomas Funkhouser. Ibrnet: Learning multi-
viewimage-basedrendering. InCVPR,2021a.
WenhaiWang,EnzeXie,XiangLi,Deng-PingFan,KaitaoSong,DingLiang,TongLu,PingLuo,
andLingShao. Pyramidvisiontransformer: Aversatilebackbonefordensepredictionwithout
convolutions,2021b.
MaximumWilder-Smith, VaishakhPatil, andMarcoHutter. Radiancefieldsforroboticteleopera-
tion,2024.
Zhuofan Xia, Xuran Pan, Shiji Song, Li Erran Li, and Gao Huang. Vision transformer with de-
formableattention,2022.
Haozhe Xie, Hongxun Yao, Xiaoshuai Sun, Shangchen Zhou, and Shengping Zhang. Pix2vox:
Context-aware3dreconstructionfromsingleandmulti-viewimages. InICCV,2019.
HaofeiXu,AnpeiChen,YuedongChen,ChristosSakaridis,YulunZhang,MarcPollefeys,Andreas
Geiger,andFisherYu. Murf: Multi-baselineradiancefields. InCVPR,2024.
Zhiwen Yan, Weng Fei Low, Yu Chen, and Gim Hee Lee. Multi-scale 3d gaussian splatting for
anti-aliasedrendering,2024.
13Preprint
JianweiYang,ChunyuanLi,PengchuanZhang,XiyangDai,BinXiao,LuYuan,andJianfengGao.
Focalself-attentionforlocal-globalinteractionsinvisiontransformers,2021.
YuanwangYang,QiaoFeng,Yu-KunLai,andKunLi. R2human: Real-time3dhumanappearance
renderingfromasingleimage,2024.
YaoYao,ZixinLuo,ShiweiLi,TianFang,andLongQuan. Mvsnet: Depthinferenceforunstruc-
turedmulti-viewstereo. InECCV,2018.
Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, and Angjoo Kanazawa. PlenOctrees for
real-timerenderingofneuralradiancefields. InICCV,2021a.
Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelNeRF: Neural radiance fields
fromoneorfewimages. InCVPR,2021b.
XiaoyuYue,ShuyangSun,ZhanghuiKuang,MengWei,PhilipTorr,WayneZhang,andDahuaLin.
Visiontransformerwithprogressivesampling,2021.
Pengchuan Zhang, Xiyang Dai, Jianwei Yang, Bin Xiao, Lu Yuan, Lei Zhang, and Jianfeng Gao.
Multi-scale vision longformer: A new vision transformer for high-resolution image encoding,
2021.
RichardZhang,PhillipIsola,AlexeiAEfros,EliShechtman,andOliverWang. Theunreasonable
effectivenessofdeepfeaturesasaperceptualmetric. InCVPR,2018.
ShunyuanZheng,BoyaoZhou,RuizhiShao,BoningLiu,ShengpingZhang,LiqiangNie,andYebin
Liu.Gps-gaussian:Generalizablepixel-wise3dgaussiansplattingforreal-timehumannovelview
synthesis. InCVPR,2024.
TinghuiZhou,RichardTucker,JohnFlynn,GrahamFyffe,andNoahSnavely.Stereomagnification:
Learningviewsynthesisusingmultiplaneimages,2018.
Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr:
Deformabletransformersforend-to-endobjectdetection.arXivpreprintarXiv:2010.04159,2020.
14Preprint
A APPENDIX
A.1 PRELIMINARY
A.1.1 3DGAUSSIANSPLATTING
3DGaussianSplatting(Kerbletal.,2023)representsa3DsceneasasetofexplicitGaussianprim-
itivesasfollows:
G ={g |g =(µ ,Σ ,α ,sh )}N (10)
i i i i i i i=i
whereeachGaussianhasacenterµ ,acovarianceΣ ,anopacityα andsphericalharmonicssh .
i i i i
Furthermore, given the scaling matrix S and rotation matrix R, we can calculate the covariance
matrix:
Σ=RSSTRT (11)
As explicit Gaussian primitives G can be rendered via an rasterization-based operation, such ap-
proachismuchmorecheaperinbothtimeandmemorycomparedtoimplicitneuralfields(Milden-
halletal.,2020; Barronetal.,2021; Tianetal.,2023; Chenetal.,2021a; Garbinetal.,2021)or
voxel-basedrepresentations(Sitzmannetal.,2019; Jietal.,2017; Xieetal.,2019; Tatarchenko
etal.,2017; Choyetal.,2016).
A.1.2 DEFORMABLEATTENTION
Since the introduction of ViT (Dosovitskiy et al., 2021), numerous efficient attention mechanisms
havebeenproposedtofurtherenhancethescalabilityandreducethecomplexityofViT(Liuetal.,
2021; Chen et al., 2021b; Wang et al., 2021b; Yang et al., 2021; Zhang et al., 2021; Dong et al.,
2022; Jaegle et al., 2021; Sun et al., 2022; Zhu et al., 2020; Yue et al., 2021; Chen et al., 2021c).
Amongthem,DeformableAttentionTransformer(DAT)(Xiaetal.,2022)standsoutforitsabilityto
effectivelycapturemulti-scalefeaturesandadaptivelyfocusonimportantregionswhilemaintaining
lightweight computational overheads. Therefore, we apply deformable attention in both the Score
HypernetworksH(equation4)andIterativeGaussianRefiner(IGR)(equation8)inourPixelGaus-
sianmodel. Weelaborateequation8asanexample.
GivenGaussiancentersµ,correspondingqueriesQ,featuremapsF ={F }N andcameraparam-
i i=1
eters C = {C }N , we first project Gaussian centers to pixel coordinates to get reference points,
i i=1
denoteasR={R }N :
i i=1
R =P(µ,C ),(i=1,2,...,N) (12)
i i
where P(·) denotes projection operation. Next, for Gaussian queries Q, we perform deformable
samplingfromfeaturemapsF atreferencepointsRusingbilinearinterpolationasfollows:
(cid:88)
ϕ(F ,(r ,r ))= g(r ,p )·g(r ,p )·F (p ,p ) (13)
i x y x x y y i x y
(px,py)
whereg(x,y) = max(0,1−|a−b|),(r ,r ),F denotereferencepointandfeaturemap,respec-
x y i
tively. ThenwecanupdatequeriesQviaattention:
N
(cid:88)
Q= α ·(W ·ϕ(F ,R )) (14)
i i i i
i=1
W =softmax(Q·ϕ(F ,R )T) (15)
i i i
where α ,(i = 1,2,...,N) represents view weights as illustrated in equation 3. Following this
i
paradigm, weincorporate deformableattentionin thescorehypernetworks H togenerate context-
awarethresholds,andinIGRblockstofurtherrefineGaussianrepresentationsusingimagefeatures.
A.2 MOREIMPLEMENTATIONDETAILS
TrainingDetails. AsmentionedinSection4.1, ourmodelistrainedon8NVIDIAA6000GPUs,
ThebatchsizeforasingleGPUissetto4,whereeachbatchcontainsalarge-scale3Dscenewith
two reference views and four inference views. Detailed settings of image backbone to generate
aggregated features F, context-aware score hypernetworks H in Cascade Gaussian Adapter and
15Preprint
Table5: DetailsofGaussianParameterRange. Table6: DetailsofTrainingSettings
GaussianParameters Criterion Config Setting
scalings∈R3 s ,s ,s ∈[0.50,15.00] optimizer Adam
x y z
rotationr ∈R4 ||r||=1 scheduler CosineAnnealing
opacityα∈R α∈[0,1] learningrate 2×10−4
harmonicssh∈RC decayingcoefficients weightdecay 1×10−4
Table7: DetailsofNetworkArchitecture.
Block Setting
CNNlayers [4,4,4]
CNNchannels [32,64,128]
Transformerlayers [2,2,2,2,2,2]
Transformerchannels [128,128,128,128,128,128]
ScoreHyperNetworkslayers [2,2,2]
ScoreHyperNetworkschannels [1,1,1]
IGRlayers [2,2,2]
IGRchannels [128,128,128]
Iterative Gaussian Refiner are illustrated in Table 7. For MLP-based networks in equation 6 and
equation9,theyallfollowcriteriainTable5toensureallGaussianparameterswithinproperrange.
Network Architecture. As illustrated in Table 7, the image backbone comprises a ResNet-based
CNN (He et al., 2016) for feature extraction and a Swin Transformer (Liu et al., 2021) for multi-
view attention. For context-aware score hypernetworks H in CGA, we employ K = 3 stages for
adaptiveGaussianadaption.Then,weadoptB =3deformableattentionblocksinIGRforGaussian
representationrefinement.
A.3 MORERESULTS
We provide more visualization comparisons of PixelGaussian and previous dominant pixel-wise
methods(Charatanetal.,2023; Chenetal.,2024)onACID(Liuetal.) andRealEstate10K(Zhou
etal.,2018)benchmarks. Additionally,weprovidevisualizationsofmorescoremapsandGaussian
densitymapsfromourPixelGaussianmodel.AsshowninFigure7andFigure8,PixelGaussianben-
efitsfromdynamicadaptionofGaussiandistributions,resultinginsuperiorreconstructioncompared
topreviouspixel-wisemethods.
Gaussian density
(Gaussian / pixel)
Input Views Score Maps Gaussian Density Novel Views
Figure7: VisualizationofscoremapsandGaussiandistributionsofCascadeGaussianAdapter.
16Preprint
ACID: 1c0cf7bd20565135 PSNR: 16.9 PSNR: 21.6 PSNR: 29.7
Re10k: 20d86cff490c0c42 PSNR: 16.3 PSNR: 18.2 PSNR: 27.7
ACID: 2af46be517a82ff2 PSNR: 23.5 PSNR: 26.1 PSNR: 30.1
Re10k: 622ef98e83ed5239 PSNR: 20.2 PSNR: 21.4 PSNR: 30.6
ACID: 3e88f9a1bc066559 PSNR: 16.1 PSNR: 16.6 PSNR: 20.8
Re10k: 9490f1219dbbb6ce PSNR: 18.4 PSNR: 18.6 PSNR: 31.3
Image Input Ground Truth pixelSplat MVSplat PixelGaussian
Figure8: VisualizationresultsonACID(Liuetal.) andRealEstate10K(Zhouetal.,2018)bench-
marks.
17