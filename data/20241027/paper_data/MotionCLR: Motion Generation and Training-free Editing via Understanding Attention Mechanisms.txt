MotionCLR: Motion Generation and Training-free
Editing via Understanding Attention Mechanisms
Ling-HaoChen1,2∗,WenxunDai1,XuanJu3,ShunlinLu4,LeiZhang2†
1TsinghuaUniversity,2InternationalDigitalEconomyAcadamy(IDEAResearch)
3TheChineseUniversityofHongKong,4TheChineseUniversityofHongKong,Shenzhen
{thu.lhchen,wxdai2001,juxuan.27,shunlinlu0803}@gmail.com,{leizhang}@idea.edu.cn
Projectpage: https://lhchen.top/MotionCLR
(a) (b)
a man walks a man jumps.
a man walks
a man jumps.
Motion deemphasizing a man jumps
a man jumps. a man walks
Motion emphasizing a man dances
(c) Example-based motion generation
example motion generated diverse motions
(d) Motion style transfer (e) Source prompt: “a man walks and then squats down.”
style reference content reference transferred result before shifting after shifting
Figure1: MotionCLR(/"moUSnklIr/)supportsversatilemotiongenerationandediting. The
blueandredfiguresrepresentoriginalandeditedmotions.(a)Motiondeemphasizingandemphasizing
viaadjustingtheweightof“jump”. (b)In-placereplacingtheactionof“walks”with“jumps”and
“dances”. (c)Generatingdiversemotionwiththesameexamplemotion. (d)Transferringmotion
stylereferringtotwomotions(styleandcontentreference). (e)Editingthesequentialityofamotion.
Abstract
This research delves into the problem of interactive editing of human motion
generation. Previousmotiondiffusionmodelslackexplicitmodelingoftheword-
leveltext-motioncorrespondenceandgoodexplainability,hencerestrictingtheir
fine-grainededitingability. Toaddressthisissue,weproposeanattention-based
motiondiffusionmodel,namelyMotionCLR,withCLeaRmodelingofattention
mechanisms. Technically,MotionCLRmodelsthein-modalityandcross-modality
interactionswithself-attentionandcross-attention,respectively. Morespecifically,
theself-attentionmechanismaimstomeasurethesequentialsimilaritybetween
framesandimpactstheorderofmotionfeatures. Bycontrast,thecross-attention
mechanism works to find the fine-grained word-sequence correspondence and
activatethecorrespondingtimestepsinthemotionsequence. Basedonthesekey
properties,wedevelopaversatilesetofsimpleyeteffectivemotioneditingmethods
viamanipulatingattentionmaps,suchasmotion(de-)emphasizing,in-placemotion
replacement,andexample-basedmotiongeneration,etc. Forfurtherverificationof
theexplainabilityoftheattentionmechanism,weadditionallyexplorethepotential
ofaction-countingandgroundedmotiongenerationabilityviaattentionmaps. Our
experimental results show that our method enjoys good generation and editing
abilitywithgoodexplainability.
∗ThisworkwasdonewhileLing-HaoChenwasaninternatIDEAResearch.Thecontributionstatementis
onpage12.
†Correspondingauthor.
Technicalreport,apreprint(MotionCLRv1.0).
4202
tcO
42
]VC.sc[
1v77981.0142:viXra1 Introduction
Recently, text-driven human motion generation [Ahn et al., 2018, Petrovich et al., 2022, Tevet
et al., 2022b, Lu et al., 2024, Guo et al., 2024a, Hong et al., 2022, Wang et al., 2022, 2024] has
attractedsignificantattentionintheanimationcommunityforitsgreatpotentialtobenefitversatile
downstreamapplications,suchasgamesandembodiedintelligence. Asthegeneratedmotionquality
inoneinferencemightbeunsatisfactory,interactivemotioneditingisvaluedasacrucialtaskinthe
community. Toprovidemoreinteractiveeditingcapabilities,previousworkshavetriedtointroduce
some human-defined constraints into the editing framework, such as introducing a pre-defined
trajectoryforacontrollablegeneration[Xieetal.,2024a,Daietal.,2024,Shafiretal.,2024]or
key-framesformotionin-betweening[Chenetal.,2023a,Tangetal.,2022,Harveyetal.,2020].
Despitesuchprogress,theconstraintsintroducedintheseworksaremainlyin-modality(motion)
constraints,whichrequirelaboriouseffortsintherealanimationcreationpipeline. Suchinteraction
fashionsstronglyrestricttheinvolvinghumansintheloopofcreation. Inthiswork,weaimtoexplore
anaturaleditingfashionofintroducingout-of-modalitysignals,suchaseditingtexts. Forexample,
whengeneratingamotionwiththeprompt“a man jumps.”,wecancontroltheheightortimesof
the“jump”actionviaadjustingtheimportanceweightoftheword“jump”. Alternatively,wecan
alsoin-placereplacetheword“jump”intootheractionsspecifiedbyusers. Moreover,inthiswork,
wewouldliketoequipthemotiongenerationmodelwithsuchabilitieswithoutre-training.
However,thekeylimitationofexistingmotiongenerationmodelsisthatthemodelingofprevious
generativemethodslacksexplicitword-leveltext-motioncorrespondence. Thisfine-grainedcross-
modalitymodelingnotonlyplaysacrucialroleintext-motionalignment,butalsomakesiteasierfor
fine-grainedediting. Toshowtheproblem,werevisitprevioustransformer-basedmotiongeneration
models [Tevet et al., 2022b, Zhang et al., 2024b, 2023b, Chen et al., 2023b]. The transformer-
encoder-likemethods[Tevetetal.,2022b,Zhouetal.,2024]treatthetextualinputasonespecial
embeddingbeforethemotionsequence. However,textembeddingsandmotionembeddingsimply
substantially different semantics, indicating unclear correspondence between texts and motions.
Besides, this fashion over-compresses a sentence into one embedding, which compromises the
fine-grainedcorrespondencebetweeneachwordandeachmotionframe. Althoughtherearesome
methods [Zhang et al., 2024b, 2023b] to perform texts and motion interactions via linear cross-
attention,theyfusethediffusiontimestepembeddingswithtextualfeaturestogetherintheforward
process. Thisoperationunderminesthestructuraltextrepresentationsandweakenstheinputtextual
conditions. Throughtheseobservations,wearguethatthefine-grainedtext-motioncorrespondencein
thesetwomotiondiffusionfashionsisnotwellconsidered. Therefore,itisurgenttobuildamodel
withgoodexplainabilityandclearmodelingoffine-grainedtext-motioncorrespondence.
Toresolvetheseissues,inthiswork,weproposeamotiondiffusionmodel,namelyMotionCLR,with
aCLeaRmodelingofthemotiongenerationprocessandfine-grainedtext-motioncorrespondence.
ThemaincomponentofMotionCLRisaCLRblock,whichiscomposedofaconvolutionlayer,a
self-attentionlayer,across-attentionlayer,andanFFNlayer. Inthisbasicblock,thecross-attention
layer isused toencode thetextconditions for each word. More specifically, the cross-attention
operationbetweeneachwordandeachmotionframemodelsthetext-motioncorrespondenceexplicitly.
Meanwhile, the timestep injection of the diffusion process and the text encoding are modeled
separately. Besides,theself-attentionlayerinthisblockisdesignedformodelingtheinteraction
betweendifferentmotionframesandtheFFNlayerisacommondesignforchannelmixing.
Motivatedbypreviousprogressintheexplainalityoftheattentionmechanism[Vaswanietal.,2017,
Maetal.,2023,Haoetal.,2021,Xuetal.,2015,Hertzetal.,2023,Cheferetal.,2021b,a],thiswork
delvesintothemathematicalpropertiesofthebasicCLRblock,especiallythecross-attentionand
self-attentionmechanisms. IntheCLRblock,thecross-attentionvalueofeachwordalongthetime
axisworksasanactivatortodeterminetheexecutiontimeofeachaction. Besides,theself-attention
mechanismintheCLRblockmainlyfocusesonminingsimilarmotionpatternsbetweenframes. Our
empiricalstudiesverifytheseproperties. Basedonthesekeyobservations,weshowhowwecan
achieveversatilemotioneditingdownstreamtasks(e.g.motion(de-)emphasizing,in-placemotion
replacement,andmotionerasing)bymanipulatingcross-attentionandself-attentioncalculations. We
verifytheeffectivenessoftheseeditingmethodsviabothqualitativeandquantitativeexperimental
results. Additionally,weexplorethepotentialofactioncountingwiththeself-attentionmapand
showhowourmethodcanbeappliedtocopewiththehallucinationofgenerativemodels.
Beforedelvingintothetechnicaldetailsofthiswork,wesummarizeourkeycontributionsasfollows.
2• Weproposeanattention-basedmotiondiffusionmodel,namelyMotionCLR,withclearmodeling
of the text-aligned motion generation process. MotionCLR achieves comparable generation
performancewithstate-of-the-artmethods.
• For the first time in the human animation community, we clarify the roles that self- and cross-
attentionmechanismsplayinoneattention-basedmotiondiffusionmodel.
• Thanks to these observations, we propose a series of interactive motion editing downstream
tasks (see Fig. 1) via manipulating attention layer calculations. We additionally explore the
potentialofourmethodtoperformgroundedmotiongenerationwhenfacingfailurecases.
2 RelatedWorkandContribution
Text-drivenhumanmotiongeneration[Plappertetal.,2018,Ahnetal.,2018,LinandAmer,2018,
AhujaandMorency,2019,Bhattacharyaetal.,2021,Tevetetal.,2022a,Petrovichetal.,2022,Hong
etal.,2022,Guoetal.,2022b,Zhangetal.,2024b,Athanasiouetal.,2022,Tevetetal.,2022b,Wang
etal.,2022,Chenetal.,2023b,Dabraletal.,2023,Yuanetal.,2023,Zhangetal.,2023a,Shafir
etal.,2024,Zhangetal.,2023b,Karunratanakuletal.,2023,Jiangetal.,2024,Zhangetal.,2024e,
Xiaoetal.,2024,Xieetal.,2024a,Luetal.,2024,Wanetal.,2024,Guoetal.,2024a,Liuetal.,
2024,Hanetal.,2024,Xieetal.,2024b,Zhouetal.,2024,Petrovichetal.,2024,Barqueroetal.,
2024,Wangetal.,2024,Huangetal.,2024,Zhangetal.,2024a]usestextualdescriptionsasinput
tosynthesizehumanmotions. OneofthemaingenerativefashionsisakindofGPT-like[Zhang
etal.,2023a,Luetal.,2024,Guoetal.,2024a,Jiangetal.,2024]motiongenerationmethod,which
compressesthetextinputintooneconditionalembeddingandpredictsmotioninanauto-regressive
fashion. Besides,thediffusion-basedmethod[Tevetetal.,2022b,Zhangetal.,2024b,2023b,Zhou
etal.,2024,Chenetal.,2023b,Daietal.,2024]isanothergenerativefashioninmotiongeneration.
Notethatmostworkwiththisfashionalsoutilizestransformers[Vaswanietal.,2017]asthebasic
networkarchitecture. Althoughthesepreviousattemptshaveachievedsignificantprogressinthepast
years,thetechnicaldesignoftheexplainabilityoftheattentionmechanismisstillnotwellconsidered.
Wehopethisworkwillprovideanewunderstandingofthesedetails.
Motioneditingaimstoeditamotionsatisfyinghumandemand. Previousworks[Daietal.,2024,
Dabraletal.,2023,Kimetal.,2023]attempttoeditamotioninacontrollingfashion,likemotion
inbetweeningandjointcontrolling. Therearesomeothermethods[Raabetal.,2023,Abermanetal.,
2020b,Jangetal.,2022]tryingtocontrolthestyleofamotion. However,theseworksareeither
designedforaspecifictaskorcannoteditfine-grainedmotionsemantics,suchastheheightortimes
ofa“jump”motion. Raabetal.[2024a]performmotionfollowingviareplacingthequeriesinthe
self-attention. Goeletal.[2024]proposetoeditamotionwithaninstruction. However,thefine-
grainedtext-motioncorrespondenceinthecross-attentionstilllacksanin-depthunderstanding. There
arealsosomemethodsdesignedformotiongeneration[Lietal.,2002]orediting[LeeandShin,1999,
Holdenetal.,2016,Athanasiouetal.,2024],whicharelimitedtoadapttodiversedownstreamtasks.
Comparedtomotionediting,thefieldofdiffusion-basedimageeditinghasbeenlargelyexplored.
Previousstudieshaveachievedexceptionalrealismanddiversityinimageediting[Hertzetal.,2023,
Hanetal.,2023,Parmaretal.,2023,Caoetal.,2023,Tumanyanetal.,2023,Zhangetal.,2023c,
Mouetal.,2024,Juetal.,2024]bymanipulatingattentionsmaps. However,relevantinteractive
editingtechniquesandobservationsarestillunexploredinthehumananimationcommunity.
Ourkeyinsightsandcontributionoverpreviousattention-basedmotiondiffusionmodels[Tevet
etal.,2022b,Zhangetal.,2024b,2023b,Zhouetal.,2024,Chenetal.,2023b,Daietal.,2024]liein
theclearexplainabilityoftheself-attentionandcross-attentionmechanismsindiffusion-basedmotion
generationmodels. Thecross-attentionmoduleinourmethodmodelsthetext-motioncorrespondence
at the word level explicitly. Besides, the self-attention mechanism models the motion coherence
between frames. Therefore, we can easily clarify what roles self-attention and cross-attention
mechanismsplayinthisframework,respectively. Tothebestofourknowledge,itisthefirsttime
inthehumananimationcommunitytoclarifythesemechanismsinonesystemandexplorehowto
performtraining-freemotioneditinginvolvinghumansintheloop.
3 BaseMotionGenerationModelandUnderstandingAttentionMechanisms
In this section, we will introduce the proposed motion diffusion model, MotionCLR, composed
ofseveralbasicCLRmodules. Specifically,wewillanalyzethetechnicaldetailsoftheattention
mechanismtoobtainanin-depthunderstandingofthis.
3T T
Conv1d
Samp. block Samp. block
CLR block
t
T/2 T/2
Self-attn. Samp. block Samp. block
CLR block
T/4 T/4
Cross-attn. Samp. block Samp. block
text Down/Up sample T/8 T/8
FFN Samp. block Samp. block
T/16
CLR block Sampling block MotionCLR Denoising Network
Figure2: SystemoverviewofMotionCLRarchitecture. (a)ThebasicCLRblockincludesfour
layers. (b)Thesampling(a.k.a.Samp.) blockincludestwoCLRblocksandonedown/up-sampling
operation. (c)MotionCLRisaU-Net-likearchitecture,composedofseveralSamplingblocks.
3.1 HowDoesMotionCLRModelFine-grainedCross-modalCorrespondence?
Regardingtheissuesofthepreviousmethods(seeSec.1),wecarefullydesignasimpleyeteffective
motiondiffusionmodel,namelyMotionCLR,withfine-grainedword-leveltext-motioncorrespon-
dence. TheMotionCLRmodelisaU-Net-likearchitecture[Ronnebergeretal.,2015]. Here,we
namethedown/up-samplingblocksintheMotionCLRassamplingblocks. Eachsamplingblock
includestwoCLRblocksandonedown/up-samplingoperation. InMotionCLR,theatomicblockis
theCLRblock,whichisourkeydesign. Specifically,aCLRblockiscomposedoffourmodules,
• Convolution-1Dmodule,a.k.a.Conv1d(·),isusedfortimestepinjection,whichisdisentangled
withthetextinjection. Thedesignprinciplehereistodisentanglethetextembeddingsandthe
timestepembeddingsforexplicitmodelingforbothconditions.
• Self-attention module is designed for learning temporal coherence between different motion
frames. Notably,differentfrompreviousworks[Tevetetal.,2022b,Zhouetal.,2024,Shafiretal.,
2024], self-attention only models the correlation between motion frames and does not include
anytextualinputs. Thekeymotivationhereistoseparatethemotion-motioninteractionfromthe
text-motioninteractionoftraditionalfashions[Tevetetal.,2022b].
• Cross-attentionmoduleplaysacrucialroleinlearningtext-motioncorrespondenceintheCLR
block. Ittakesword-leveltextualembeddingsofasentenceforcross-modalityinteraction,aiming
toobtainfine-grainedtext-motioncorrespondenceatthewordlevel. Specifically,theattentionmap
explicitlymodelstherelationshipbetweeneachframeandeachword,enablingmorefine-grained
cross-modalitycontrolling(DetailedcomparisonwithpreviousmethodsinAppendixC.3).
• FFNmoduleworksasanadditionalfeaturetransformationandextraction[Daietal.,2022,Geva
etal.,2021],whichisanecessarycomponentintransformer-basedarchitectures.
Insummary,inthebasicCLRblock,wemodelinteractionsbetweenframesandcross-modalcor-
respondence,separatelyandexplicitly. MoredetailedcomparisonswithpreviousworkareinAp-
pendixC.3. Weanalyzebothself-attentionandcross-attentionofMotionCLRinfollowingsections.
3.2 PreliminariesofAttentionMechanism
Thegeneralattentionmechanismhasthreekeycomponents,query(Q),key(K),andvalue(V),
respectively. TheoutputX′oftheattentionmechanismcanbeformulatedas,
√
X′ =softmax(QK⊤/ d)V, (1)
whereQ∈RN1×d,K,V ∈RN2×d. Here,distheembeddingdimensionofthetextorone-frame
motion. Inthefollowingsection,wetaket=0,1,···,T asdiffusiontimesteps,andf =1,2,···,F
astheframenumberofmotionembeddingsX∈RF×d. Forconvenience,wenameS=QK⊤asthe
√
similaritymatrixandsoftmax(QK⊤/ d)astheattentionmapinthefollowingsections.
4(a) (d)
(b)
(c)
Figure 3: Empirical study of attention mechanisms. We use “a person jumps.” as an example. (a)
Keyframesofgeneratedmotion. (b)TheroottrajectoryalongtheY-axis(vertical). Thecharacterjumpson
∼15−40f,∼60−80f,and∼125−145f,respectively.(c)Thecross-attentionbetweentimestepsandwords.
The“jump”wordishighlyactivatedaligningwiththe“jump”action.(d)Theself-attentionmapvisualization.
Itisobviousthatthecharacterjumpsthreetimes.Differentjumpssharesimilarlocalmotionpatterns.
3.3 MathematicalPreliminariesofAttentionMechanisminMotionCLR
Theself-attentionmechanismusesdifferenttransformationsofmotionfeaturesXasinputs,
Q=XW , K=XW , V=XW , (2)
Q K V
whereQ,K,V∈RF×d,F =N =N . Wetakeadeeplookattheformulationoftheself-attention
1 2
mechanism. As shown in Eq. (1), the attention calculation begins with a matrix multiplication
operation,meaningthesimilarity(S = QK⊤ ∈RF×F)betweenQandK. Specifically,foreach
√
rowiofS, itobtainstheframemostsimilartoframei. Here disanormalizationterm. After
obtainingthesimilarityforallframes,thesoftmax(·)operationisnotonlyanormalizationfunction,
butalsoworksasa“soft”max(·)functionforselectingtheframemostsimilartoframei. Assuming
thej-thframeisselectedastheframemostsimilartoframeiwiththemaximumactivation,thefinal
multiplicationwithVwillapproximatelyreplacethemotionfeatureV atthei-throwofX′. Here,
j
theoutputX′istheupdatedmotionfeature. Insummary,wehavethefollowingremark.
Remark1. Theself-attentionmechanismmeasuresthemotionsimilarityofallframesandaimsto
selectthemostsimilarframesinmotionfeaturesateachplace(DetaileddiagraminAppendixC).
Thecross-attentionmechanismofMotionCLRusesthetransformationofamotionasaquery,and
thetransformationoftextualwordsaskeysandvalues,
Q=XW , K=CW , V=CW , (3)
Q K V
whereC ∈ RL×d isthetextualembeddingsofLwordtokens,Q ∈ RF×d, K,V ∈ RL×d. Note
thatW inEq.(2)andEq.(3)arenotthesameparameters,butareusedforconvenience. Asshown
⋆
inEq.(3),KandVareboththetransformedtextfeatures.RecallingEq.(1),thematrixmultiplication
operationbetweenQandKmeasuresthesimilarity(S=QK⊤)betweenmotionframesandwords
inasentence. Similartothatinself-attention,thesoftmax(·)operationworksasa“soft”max(·)
functiontoselectwhichtransformedwordembeddinginVshouldbeselectedateachframe. This
operationmodelsthemotion-textcorrespondenceexplicitly. Therefore,wehavethesecondremark.
Remark2. Thecross-attentionfirstcalculatesthesimilaritymatrixtodeterminewhichword(a.k.a.
valueinattention)shouldbeactivatedatthei-thframeexplicitly.Thefinalmultiplicationoperationwith
valuesplacesthesemanticfeaturesoftheircorrespondingframes.(DetaileddiagraminAppendixC)
5length of text: L
length of text: L
length of text: L
shifting
replace V (value)
revising attention value reference attention map edited attention map
(a) Motion (de-)emphasizing via (b)In-placemotionreplacement (c)Motionsequenceshiftingvia
in/de-creasingcross-attentionvalue. viareplacingself-attentionmap. shiftingcross-attentionmap.
Figure4:Motioneditingviamanipulatingattentionmaps.
3.4 EmpiricalEvidenceonUnderstandingAttentionMechanisms
Toobtainadeeperunderstandingoftheattentionmechanismandverifythemathematicalanalysisof
attentionmechanisms,weprovidesomeempiricalstudiesonsomecases. Duetothepagelimits,we
leavemorevisualizationresultsforempiricalevidenceinAppendixD.
As shown in Fig. 3, we take the sentence “a person jumps.” as an example. Besides the
keyframe visualization (Fig. 3a), we also visualize the root trajectory along the Y-axis (vertical
height,inFig.3b). AscanbeseeninFig.3,thecharacterjumpsat∼ 15−40f,∼ 60−80f,and
∼125−145f,respectively. Notethat,asshowninFig.3c,theword“jump”issignificantlyactivated
aligningwiththe“jump”actionintheself-attentionmap. Thisnotonlyverifiesthesoundnessof
thefine-grainedtext-motioncorrespondencemodelinginMotionCLR,butalsomeetsthetheatrical
analysisofmotion-text(Q-K)similarity.Thismotivatesustomanipulatetheattentionmaptocontrol
whentheactionwillbeexecuted. ThedetailswillbeintroducedinSec.4.
Wealsovisualizetheself-attentionmapinFig.3d. AsanalyzedinSec.3.3,theself-attentionmap
evaluatesthesimilaritybetweenframes. AscanbeseeninFig.3d,theattentionmaphighlightsnine
areaswithsimilarmotionpatterns,indicatingthreejumpingactionsintotal. Besidesthetimeareas
thatthe“jmup”wordisactivatedarealignedwiththejumpingactions. Thehighlightedareasinthe
self-attentionmapareoflineshapes,indicatingthetaking-off,in-the-air,andlandingactionsofa
jumpwithdifferentdetailedmovementpatterns.
4 VersatileApplicationsviaAttentionManipulations
AnalysisinSec.3.3hasrevealedtherolesthatattentionmechanismsplayinMotionCLR.Inthis
section,wewillshowversatiledownstreamtasksofMotionCLRviamanipulatingattentionmaps.
Motionemphasizingandde-emphasizing. Inthetext-drivenmotiongenerationframework,the
processisdrivenbytheinputtext. AsdiscussedinSec.3.3,theverboftheactionwillbesignificantly
activated in the cross-attention map when the action is executed. As shown in Fig. 4a, if we
increase/decreasetheattentionvalueofaverbinthecross-attentionmap,thecorrespondingaction
will be emphasized/de-emphasized. Besides, this method can also be extended to the grounded
motiongeneration,whichwillbeintroducedinSec.6.
Motionerasing. Motionerasingisaspecialcaseofmotionde-emphasizing. Wetreatitasaspecial
caseofmotionde-emphasizing. Whenthedecreased(de-emphasized)cross-attentionvalueofan
actionissmallenough,thecorrespondingactionwillbeerased.
In-placemotionreplacement. Inrealscenarios,wewouldliketoeditsomelocalmotioncontents
ofthegeneratedresult. Assumingwegenerateareferencemotionatfirst,wewouldliketoreplace
one action in the reference motion with another in place. Therefore, the batch size of inference
examplesistwoduringtheinferencestage,wherethefirstisthereferencemotionandtheotheristhe
editedmotion. AsdiscussedinSec.3.3,thecross-attentionmapdetermineswhenanactionhappens.
Motivatedbythis,wereplacethecross-attentionmapoftheeditedmotionastheoneofthereference
motion. AsshowninFig.4b,weusethereplacedattentionmaptomultiplythevaluematrix(text
features)toobtaintheoutput.
Motion sequence shifting. It is obvious that the generated motion is a combination of different
actions along the time axis. Sometimes, users would like to shift a part of the motion along the
time axis to satisfy the customized requirements. As shown in Fig. 4c, we can shift the motion
6
F
:noitom
fo
htgnel
F
:noitom
fo
htgnel
F
:noitom
fo
htgnelR-Precision↑
Methods FID↓ MM-Dist↓ Multi-Modality↑
Top1 Top2 Top3
TM2T[2022b] 0.424±0.003 0.618±0.003 0.729±0.002 1.501±0.017 3.467±0.011 2.424±0.093
T2M[2022a] 0.455±0.003 0.636±0.003 0.736±0.002 1.087±0.021 3.347±0.008 2.219±0.074
MDM[2022b] - - 0.611±0.007 0.544±0.044 5.566±0.027 2.799±0.072
MLD[2023b] 0.481±0.003 0.673±0.003 0.772±0.002 0.473±0.013 3.196±0.010 2.413±0.079
MotionDiffuse[2024b] 0.491±0.001 0.681±0.001 0.782±0.001 0.630±0.001 3.113±0.001 1.553±0.042
T2M-GPT[2023a] 0.492±0.003 0.679±0.002 0.775±0.002 0.141±0.005 3.121±0.009 1.831±0.048
ReMoDiffuse[2023b] 0.510±0.005 0.698±0.006 0.795±0.004 0.103±0.004 2.974±0.016 1.795±0.043
MoMask[2024a] 0.521±0.002 0.713±0.002 0.807±0.002 0.045±0.002 2.958±0.008 1.241±0.040
MotionCLR 0.542±0.001 0.733±0.002 0.827±0.003 0.099±0.003 2.981±0.011 2.145±0.043
MotionCLR∗ 0.544±0.001 0.732±0.001 0.831±0.002 0.269±0.001 2.806±0.014 1.985±0.044
Table1:ComparisonwithdifferentmethodsontheHumanML3Ddataset.The“∗”notationdenotesthe
DDIMsamplinginferencedesignchoiceandtheotheristheDPM-solversamplingchoice.
↑0.50
↑0.15
vinilla
↓0.05
↓0.10
(a)Theheightofthecharacter’sroot.Thehighlighted (b)Visualizationoftheeditedmotionsondifferent
areaisobviouswhencomparingdifferentweights. (de-)emphasizingweightsettings.
Figure5:Motion(de-)emphasizing.Differentweightsof“jump”(↑or↓)in“a man jumps.”.
sequentiality by shifting the self-attention map. As discussed in Sec. 3.3, self-attention is only
relatedtothemotionfeaturewithoutrelatedtothesemanticcondition,whichisourmotivationon
manipulating the self-attention map. Thanks to the denoising process, the final output sequence
shouldbeanaturalandcontinuoussequence.
Example-basedmotiongeneration. AsdefinedbyLietal.[2023b],example-basedmotiongener-
ationaimstogeneratenovelmotionsreferringtoanexamplemotion. InMotionCLRsystem,this
taskisaspecialcaseofthemotionsequenceshifting. Thatistosay,wecanshufflethequeriesofthe
self-attentionmaptoobtainthediversemotionsreferringtotheexample.
Motionstyletransfer. Asdiscussedinthetechnicaldetailsoftheself-attentionmechanism,the
valuesmainlycontributetothecontentsofmotionandtheattentionmapdeterminestheselected
indicesofmotionframes. Whensynthesizingtwomotionsequences(M andM respectively),we
1 2
onlyneedtoreplaceQsinM withthatinM toachievethestyleofM intoM ’s. Specifically,
2 1 2 1
queries(Qs)inM determinewhichmotionfeatureinM isthemostsimilartothatinM ateach
2 2 1
timestep. Accordingly,thesemostsimilarmotionfeaturesareselectedtocomposetheeditedmotion.
Besides,theeditedmotioniswiththemotioncontentofM whileimitatingthemotionstyleofM .
2 1
WeleavemoretechnicaldetailsandpseudocodesinAppendixF.
5 Experiments
5.1 MotionCLRModelEvaluation
The implementations of the MotionCLR are in Appendix E.1. We first evaluate the generation
performanceoftheMotionCLR.Weextendtheevaluationmetricsofpreviousworks[Guoetal.,
2022a]. (1)Motionquality: FIDisadoptedasametrictoevaluatethedistributionsbetweenthe
generatedandrealmotions.(2)Motiondiversity:MultiModality(MModality)evaluatesthediversity
basedonthesametextanddiversitycalculatesvarianceamongfeatures. (3)Text-motionmatching:
FollowingGuoetal.[2022a], wecalculatetheR-Precisiontoevaluatethetext-motionmatching
7(a)“a man jumps.”case.Thesecondjumpingactioniserased.
(b)“waving hand”case.Thefinal1/3wavingactionisremoved.
Figure6:Motionerasingresults.Casestudyof“a man jumps.”and“waving hand”cases.
(a)Rootheightandthemotionof“a man (b)Rootvelocityandmotionvisualizationoftheedited“a man
jumps.”beforeediting. jumps.”→“a man walks.”motion.
a man runs and then sits. a man jumps and then sits.
(c)Motionin-placereplacementresultsofamotionincludingmultipleactions.
Figure7:In-placemotionreplacement.Replacingthe“jumps”in“a man jumps.”as“walks”,theedited
motionandthevanillamotionsharethesametemporalareaoftheactionexecution.
accuracy and MM-Dist to show the distance between texts and motions. The results are shown
in Tab.1, indicatinga comparableperformance withthe state-of-the-artmethod. Especially, our
resulthasahighertext-motionalignmentoverbaselines,owingtotheexplicitfine-grainedcross-
modalitymodeling. AsshowninTab.1,bothDDIMandDPM-solversamplingworkconsistently
wellcomparedwithbaselines. WeleavemorevisualizationandqualitativeresultsinAppendixA.
5.2 InferenceOnlyMotionEditing
Motion(de-)emphasizingandmotionerasing. Forquantitativeanalysis,
we construct a set of prompts to synthesize motions, annotating the key
weight TMR-sim. FID
verbsinthesentencebyhumanresearchers. ThemetrichereistheTMR
-0.60 52.059 0.776
similarity(TMR-sim.)[Petrovichetal.,2023]usedformeasuringthetext- -0.50 52.411 0.394
motion similarity (between 0 and 1). The comparison in Tab. 2 shows -0.40 53.294 0.235
-0.30 53.364 0.225
thatthede-emphasizingmakesthemotionlesssimilartothetext,andthe
baseline 53.956 0.217
emphasizingonesaremorealignedatthebeginningofincreasingweights. +0.30 54.311 0.210
When adjusting weights are too large, the attention maps are corrupted, +0.40 54.496 0.208
+0.50 54.532 0.223
resultinginartifacts. Therefore,thesuggestedvalueoftheweightsranges
+0.60 54.399 0.648
from−0.5to+0.5. Wemainlyprovidethevisualizationresultsofmotion
Table2:Abaltiononmo-
(de-)emphasizing in Fig. 5. As shown in Fig. 5, the edited results are
tion(de-)emphasizing.
alignedwiththemanipulatedattentionweights. Especially,ascanbeseen,
inFig.5a,theheightofthe“jump”actionisaccuratelycontrolledbythecross-attentionweightof
theword“jump”. Foranextremelylargeadjustingweight,e.g.↑1.0,thetimesofthejumpingaction
alsoincrease. Thisisbecausethelow-activatedtimestepsofthevanillageneratedmotionmighthave
alargercross-attentionvaluetoactivatethe“jump”action. Asmotionerasingisaspecialcaseof
motionde-emphasizing,wedonotprovidemorequantitativeonthisapplication. Weprovidesome
visualizationresultsinFig.6. AscanbeseeninFig.6a,thesecondjumpingactioniserased. Besides,
the“wavinghand”caseshowninFig.6bshowsthatthefinal1/3wavingactionisalsoremoved.
MoreexperimentsareinAppendixA.1andA.2.
8(a)Prompt:“a person walks straight and then waves.”(b)Prompt:“a man gets up from the ground.”Original
Original(blue)vs.shifted(red)motion. (blue)vs.shifted(red)motion.
Figure9: Comparisonbetweenoriginalandshiftedmotions.Timebarsareshownindifferentcolors.(a)
Theoriginalfigureraiseshandsafterthewalkingaction.Theshiftedonehastheoppositesequentiality.(b)The
keysquattingactionisshiftedtotheendofthesequence,andthestanding-byactionisshiftedtothebeginning.
(a)Examples(blue)andgenerated(red)motions. (b)Roottrajectoryvisualization.
Figure10:Diversegeneratedmotionsdrivenbythesameexample.Prompt:“a person steps sideways
to the left and then sideways to the right.”. (a) The diverse generated motions driven by the
sameexamplemotionsharesimilarmovementcontent. (b)Theroottrajectoriesofdiversemotionsarewith
similarglobaltrajectories,butnotthesame.
In-place motion replacement. Different from naïve replacing prompts for motion replacement,
in-placemotionreplacementnotonlyreplacestheoriginalmotionatthesemanticlevel,butalsoneeds
toreplacemotionsattheexacttemporalplace. Fig.7aandFig.7bshowtherootheighttrajectoryand
theroothorizontalvelocity,respectively. Inthiscase,theeditedandoriginalmotionsharethesame
timezonetoexecutetheaction. Besides,theeditedmotionissemanticallyalignedwiththe“walk”.
Fig.7calsoshowsresultsofreplacing“runs”as“jumps”withoutchangingthesittingaction.
Motionsequenceshifting. Here,weprovidesomecomparisonsbetweentheoriginalmotionand
theeditedone. InFig.9,wetake“ ”and“ ”torepresentdifferenttimebars,whoseorders
representthesequentially. AscanbeseeninFig.9a,theexecutionofwavinghandsisshiftedtothe
beginningofthemotion. Besides,asshowninFig.9b,thesquattingactionhasbeenmovedtothe
endofthemotion. Theseresultsshowthattheeditingoftheself-attentionmapsequentialityhasan
explicitcorrespondencewiththeeditingmotionsequentially. MoreresultsareinAppendixA.6.
Example-basedmotiongeneration. Theexample-based
generated
motion generation [Li et al., 2023b] task has two basic 80 example
requirements. (1)Thefirstoneisthegeneratedmotions 60
40
shouldsharesimilarmotiontextures[Lietal.,2002]with
20
theexamplemotion. Weobservethehigh-dimensionstruc-
0
tureofmotionsviadimensionalityreduction.Asthet-SNE
20
visualizationresultsshowninFig.8,generatedmotions 40
drivenbythesameexamplearesimilartothegivenexam- 60
ple(inthesamecolor). (2)Besides,differentgenerated 8 F0 ig80ure860: t-S40NEv20isua0lizat20iono40fdif6f0erent
motionsdrivenbythesameexampleshouldbediverse. As example-basedgeneratedresults.Different
showninFig.10,thesegeneratedresultsarediversenot colorsimplydifferentdrivenexamples.
onlyinlocalmotions(Fig.10a)butalsointheglobaltrajectory(Fig.10b). Furthermore, results
inFig.10alsosharethesimilarmotiontextures. WeleavemorevisualizationresultsinAppendixA.7.
Motion style transfer. As shown in Fig. 11, in the MotionCLR framework, the style reference
motionprovidesstyleandthecontentreferencemotionprovideskeysandvalues. Ascanbeseen
inFig.11,alleditedresultsarewell-stylizedwithstylemotionsandkeepthemainmovementcontent
withthecontentreference.
9style reference content reference transferred result
(a) Style reference: “the person dances very happily”, content reference: “the man is walking”.
Thetransferredresultshowsafigurewalkinginaback-and-forthhappypace.
style reference content reference transferred result
(b) Style reference: a man is doing hip-hop dance”, Content reference: a person runs around a
circle”.Thestylizedresultshowsarunningmotionwithbenthands,shakingleftandright.
Figure11:Motionstyletransferresults.Thestylereference,contentreferences,andthetransferredresultsare
shownfromlefttorightforeachcase.
5.3 AblationStudy
Weprovidesomeablationstudiesonsometechnicalde-
R-Precision↑
signs. (1)Thesettingw/oseparatewordmodelingshows Ablation FID↓
Top1 Top2 Top3
poorerqualitativeresultswiththew/separatewordsetting.
Theseparateword-levelcross-attentioncorrespondence (1) 0.512 0.705 0.792 0.544
(2) 0.509 0.703 0.788 0.550
benefitsbettertext-to-motioncontrolling,whichiscritical
MotionCLR 0.544 0.732 0.831 0.269
formotionfine-grainedgeneration. (2)Thesettingofin-
jectingtexttokensbeforemotiontokensperformsworse Table3:Ablationstudiesbetweendifferent
thantheMotionCLR.Thisvalidatestheeffectivenessof technicaldesignchoices.
introducingthecross-attentionforcross-modalcorrespondence. Theablationstudiesadditionally
verifythebasicmotivationofmodelingword-levelcorrespondenceinMotionCLR.
5.4 ActionCountingfromAttentionMap
AsshowninFig.3,thenumberofexecutedactionsina
generatedmotionsequencecanbeaccuratelycalculated
viatheself-attentionmap. Wedirectlydetectthenumber
ofpeaksineachrowoftheself-attentionmapandfinally
average this of each row. In the technical implementa-
tion,toavoidsuddenpeaksfrombeingdetected,weapply
averagedownsamplingandGaussiansmoothing(parame-
terizedbystandarddeviationσ). Weleavemoretechnical
detailsinAppendixG.
Figure12: Actioncountingerrorratecom-
Weconstructasetoftextpromptscorrespondingtodif-
parison.Roottrajectory(Traj.)vs.attention
ferentactionstoperformthecountingcapabilityviathe
map(Ours).“σ”isthesmoothingparameter.
self-attention map. The counting number of actions is
labeledbyprofessionalresearchers. ThedetailsoftheevaluationsetaredetailedinAppendixE.3. As
the“walking”actioniscomposedofsub-actionsoftwolegs,theatomicunitofthisactioncountingis
setas0.5. Wecompareourmethodtocountingwiththeverticalroottrajectory(Traj.) peakdetection.
AsshowninFig.12,countingwiththeself-attentionmapmostlyworksbetterthancountingwithroot
trajectory. BothsettingsuseGaussiansmoothingtoblursomejitters. Ourmethoddoesnotrequire
toomuchsmoothingregularizationduetothesmoothnessoftheattentionmap,whilecountingwith
roottrajectoryneedsthisoperation. Thiscasestudyrevealstheeffectivenessofunderstandingthe
self-attentionmapinMotionCLR.
10
)%(
etar
rorrevinilla
result
edited
retuslt
Figure13:Comparisonbetweenw/vs.w/ogroundedmotiongenerationsettings.Therootheightandmotion
visualizationofthetextualprompt“apersonjumpsfourtimes”.
6 FailureCasesAnalysisandCorrection
There are few generative methods that can escape the curse of hallucination. In this section, we
will discuss some failure cases of our method and analyze how we can refine these results. The
hallucinationofcountingisanotoriouslytrickyproblemforgenerativemodels,attractingsignificant
attentioninthecommunityandlackingaunifiedtechnicalsolution. Consideringthatthisproblem
cannotbethoroughlyresolved,wetrytopartiallyrevealthisissuebyadditionallyprovidingtemporal
grounds. Forexample,ifthecountingnumberofanactionisnotalignedwiththetextualprompt,we
cancorrectthisbyspecifyingthetemporalgroundsofactions. Technically,thetemporalmaskcanbe
treatedasasequenceofweightstoperformthemotionemphasizingandde-emphasizing. Therefore,
groundedmotiongenerationcanbeeasilyachievedbyadjustingtheweightsofwords.
Specifically,weshowsomefailurecasesofourmethod.AsshowninFig.13,thegeneratedresultof“a
person jumps four times”failstoshowfourtimesofjumpingactions,butseventimes. Tomeet
therequirementofcountingnumbersinthetextualprompts,weadditionallyinputatemporalmask,
includingfourjumpingtimesteps,toprovidetemporalgrounds. Fromtherootheightvisualization
andthemotionvisualization,thetimesofthejumpingactionhavebeensuccessfullycorrectedfrom
seven to four. Therefore, our method is promising for grounded motion generation to reveal the
hallucinationofdeepmodels.
Moreover,othereditingfashionsarealsopotentialwaystocorrecthallucinationsofgeneratedresults.
Forexample,themotionsequenceshiftingandin-placemotionreplacementfunctionscanbeused
forcorrectingsequentialerrorsandsemanticmisalignments,respectively.
7 ConclusionandFutureWork
Conclusion. In this work, we propose a diffusion-based motion generation model, MotionCLR.
Withthismodel,wecarefullyclarifytheself-attentionandthecross-attentionmechanismsinthe
MotionCLR. Based on both theoretical and empirical analysis of the attention mechanisms in
MotionCLR,wedevelopedversatilemotioneditingmethods. Additionally,wenotonlyverifythe
actioncountingabilityofattentionmaps,butalsoshowthepotentialofmotioncorrections. Webuild
auserinterfaceinAppendixBtodemonstratehowourmethodcansupportinteractiveediting.
Limitationandfuturework. AsshowninSec.6,ourmodelcanalsonotescapethehallucination
curseofgenerativemodels. Therefore, weleavethegroundedmotiongenerationasfuturework.
AsdiscussedinLuetal.[2024], theCLIPmodelusedinMotionCLRisstillabitunsatisfactory.
Therefore,wewillprovidetoken-leveltext-motionalignmentencoderstoprovidetextualconditions.
Besides,ourfutureworkwillalsofocusonscalingtheMotionCLRtogeneratemorevividmotions.
Borderimpactstatement. ThedevelopmentofMotionCLR,adiffusion-basedmotiongeneration
model, has the potential to impact various fields of motion synthesis and editing significantly.
However,thecomplexityoftheMotionCLRanditsperformancelimitationsundercertainconditions
mayleadtoerrorsorinaccuraciesinpracticalapplications.Thiscouldresultinnegativeconsequences
incriticalfieldssuchashumanoidsimulationandautonomousdriving. Therefore,itisnecessaryto
furtheroptimizethemodelandcarefullyassessitsreliabilitybeforewidespreaddeployment.
11Acknowledgement
TheauthorteamwouldliketoacknowledgeDr. JingboWangfromShanghaiAILaboratoryandDr.
XingyuChenfromPekingUniversityforhisconstructivesuggestionsanddiscussionsondownstream
applications. WealsowouldliketoacknowledgeMr. HongyangLiandMr. ZhenhuaYangfrom
SCUTfortheirdetaileddiscussiononsometechnicaldetailsandwriting. Mr. BohongChenfrom
ZJUalsoprovideduswithinsightfulfeedbackontheevaluationandthepresentations. Weconvey
ourthankstoallofthem.
ContributionStatement
Ling-HaoChencontributestothekeydesignofMotionCLR,codeimplementation,andvisualization.
Wenxun Dai and Shunlin Lu contribute to motion kinematics and visualization to accelerate the
progressofthisproject. XuanJucontributestothemethodsandtechnicalimplementationsofsome
motioneditingapplications. LeiZhangistheadvisorthroughouttheproject. Allauthorscommitto
thewritingofthispaper.
12References
KfirAberman,PeizhuoLi,DaniLischinski,OlgaSorkine-Hornung,DanielCohen-Or,andBaoquan
Chen. Skeleton-awarenetworksfordeepmotionretargeting. ACMTOG,39(4):62–1,2020a.
KfirAberman,YijiaWeng,DaniLischinski,DanielCohen-Or,andBaoquanChen. Unpairedmotion
styletransferfromvideotoanimation. ACMTOG,39(4):64–1,2020b.
Abubakar Abid, Ali Abdalla, Ali Abid, Dawood Khan, Abdulrahman Alfozan, and James Zou.
Gradio:Hassle-freesharingandtestingofmlmodelsinthewild. arXivpreprintarXiv:1906.02569,
2019.
HyeminAhn,TimothyHa,YunhoChoi,HwiyeonYoo,andSonghwaiOh. Text2action: Generative
adversarialsynthesisfromlanguagetoaction. InICRA,pages5915–5920,2018.
ChaitanyaAhujaandLouis-PhilippeMorency. Language2pose: Naturallanguagegroundedpose
forecasting. In3DV,pages719–728,2019.
TenglongAo,ZeyiZhang,andLibinLiu. Gesturediffuclip: Gesturediffusionmodelwithcliplatents.
ACMTOG,42(4):1–18,2023.
Nikos Athanasiou, Mathis Petrovich, Michael J Black, and Gül Varol. Teach: Temporal action
compositionfor3dhumans. In3DV,pages414–423,2022.
NikosAthanasiou,MathisPetrovich,MichaelJBlack,andGülVarol. Sinc: Spatialcompositionof
3dhumanmotionsforsimultaneousactiongeneration. InICCV,pages9984–9995,2023.
NikosAthanasiou,AlpárCeske,MarkosDiomataris,MichaelJ.Black,andGülVarol. MotionFix:
Text-driven3dhumanmotionediting. InSIGGRAPHAsia,2024.
GermanBarquero,SergioEscalera,andCristinaPalmero. Seamlesshumanmotioncompositionwith
blendedpositionalencodings. InCVPR,pages457–469,2024.
UttaranBhattacharya,NicholasRewkowski,AbhishekBanerjee,PoojaGuhan,AniketBera,and
Dinesh Manocha. Text2gestures: A transformer-based network for generating emotive body
gesturesforvirtualagents. InVR,pages1–10,2021.
ZhongangCai,JianpingJiang,ZhongfeiQing,XinyingGuo,MingyuanZhang,ZhengyuLin,Haiyi
Mei,ChenWei,RuisiWang,WanqiYin,etal. Digitallifeproject: Autonomous3dcharacterswith
socialintelligence. InCVPR,pages582–592,2024.
MingdengCao,XintaoWang,ZhongangQi,YingShan,XiaohuQie,andYinqiangZheng. Masactrl:
Tuning-freemutualself-attentioncontrolforconsistentimagesynthesisandediting. InICCV,
pages22560–22570,2023.
HilaChefer,ShirGur,andLiorWolf.Genericattention-modelexplainabilityforinterpretingbi-modal
andencoder-decodertransformers. InICCV,pages397–406,2021a.
HilaChefer,ShirGur,andLiorWolf. Transformerinterpretabilitybeyondattentionvisualization. In
CVPR,pages782–791,2021b.
Ling-HaoChen,JiaweiZhang,YewenLi,YirenPang,XiaoboXia,andTongliangLiu. Humanmac:
Maskedmotioncompletionforhumanmotionprediction. InICCV,pages9544–9555,2023a.
XinChen,BiaoJiang,WenLiu,ZilongHuang,BinFu,TaoChen,andGangYu. Executingyour
commandsviamotiondiffusioninlatentspace. InCVPR,pages18000–18010,2023b.
SetarehCohan,GuyTevet,DanieleReda,XueBinPeng,andMichielvandePanne. Flexiblemotion
in-betweeningwithdiffusionmodels. InACMSIGGRAPH,pages1–9,2024.
PeishanCong,ZiyiWangZhiyangDou,YimingRen,WeiYin,KaiCheng,YujingSun,Xiaoxiao
Long,XingeZhu,andYuexinMa. Laserhuman: Language-guidedscene-awarehumanmotion
generationinfreeenvironment. arXivpreprintarXiv:2403.13307,2024.
Jieming Cui, Tengyu Liu, Nian Liu, Yaodong Yang, Yixin Zhu, and Siyuan Huang. Anyskill:
Learningopen-vocabularyphysicalskillforinteractiveagents. InCVPR,pages852–862,2024.
13RishabhDabral,MuhammadHamzaMughal,VladislavGolyanik,andChristianTheobalt. Mofusion:
Aframeworkfordenoising-diffusion-basedmotionsynthesis. InCVPR,pages9760–9770,2023.
DamaiDai,LiDong,YaruHao,ZhifangSui,BaobaoChang,andFuruWei. Knowledgeneuronsin
pretrainedtransformers. InACL,pages8493–8502,2022.
WenxunDai,Ling-HaoChen,JingboWang,JinpengLiu,BoDai,andYansongTang. Motionlcm:
Real-timecontrollablemotiongenerationvialatentconsistencymodel. ECCV,2024.
ChristianDillerandAngelaDai. Cg-hoi: Contact-guided3dhuman-objectinteractiongeneration. In
CVPR,pages19888–19901,2024.
MarkosDiomataris,NikosAthanasiou,OmidTaheri,XiWang,OtmarHilliges,andMichaelJBlack.
Wandr: Intention-guidedhumanmotiongeneration. InCVPR,pages927–936,2024.
Ke Fan, Junshu Tang, Weijian Cao, Ran Yi, Moran Li, Jingyu Gong, Jiangning Zhang, Yabiao
Wang, Chengjie Wang, and Lizhuang Ma. Freemotion: A unified framework for number-free
text-to-motionsynthesis. ECCV,2024.
BinFeng,TenglongAo,ZequnLiu,WeiJu,LibinLiu,andMingZhang. Robustdancer: Long-term
3ddancesynthesisusingunpaireddata. arXivpreprintarXiv:2303.16856,2023.
MorGeva,RoeiSchuster,JonathanBerant,andOmerLevy. Transformerfeed-forwardlayersare
key-valuememories. InEMNLP,pages5484–5495,2021.
Anindita Ghosh, Rishabh Dabral, Vladislav Golyanik, Christian Theobalt, and Philipp Slusallek.
Remos: Reactive3dmotionsynthesisfortwo-personinteractions. ECCV,2023.
PurviGoel,Kuan-ChiehWang,CKarenLiu,andKayvonFatahalian. Iterativemotioneditingwith
naturallanguage. InACMSIGGRAPH,pages1–9,2024.
KehongGong,DongzeLian,HengChang,ChuanGuo,ZihangJiang,XinxinZuo,MichaelBiMi,
andXinchaoWang. Tm2d: Bimodalitydriven3ddancegenerationviamusic-textintegration. In
ICCV,pages9942–9952,2023.
Chuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji, Xingyu Li, and Li Cheng. Generating
diverseandnatural3dhumanmotionsfromtext. InCVPR,pages5152–5161,2022a.
ChuanGuo,XinxinZuo,SenWang,andLiCheng. Tm2t: Stochasticandtokenizedmodelingforthe
reciprocalgenerationof3dhumanmotionsandtexts. InECCV,pages580–597,2022b.
ChuanGuo,YuxuanMu,MuhammadGoharJaved,SenWang,andLiCheng. Momask: Generative
maskedmodelingof3dhumanmotions. InCVPR,pages1900–1910,2024a.
ChuanGuo,YuxuanMu,XinxinZuo,PengDai,YouliangYan,JuweiLu,andLiCheng. Generative
humanmotionstylizationinlatentspace. ICLR,2024b.
XinyingGuo,MingyuanZhang,HaozheXie,ChenyangGu,andZiweiLiu. Crowdmogen: Zero-shot
text-drivencollectivemotiongeneration. arXivpreprintarXiv:2407.06188,2024c.
BoHan, HaoPeng, MinjingDong, YiRen, YixuanShen, andChang Xu. Amd: Autoregressive
motiondiffusion. InAAAI,pages2022–2030,2024.
LigongHan, SongWen, QiChen, ZhixingZhang, KunpengSong, MengweiRen, RuijiangGao,
YuxiaoChen,DiLiu0003,QilongZhangli,etal. Improvingtuning-freerealimageeditingwith
proximalguidance. WACV,2023.
Yaru Hao, Li Dong, Furu Wei, and Ke Xu. Self-attention attribution: Interpreting information
interactionsinsidetransformer. InAAAI,volume35,pages12963–12971,2021.
Félix G Harvey, Mike Yurick, Derek Nowrouzezahrai, and Christopher Pal. Robust motion in-
betweening. ACMTOG,39(4):60–1,2020.
AmirHertz,RonMokady,JayTenenbaum,KfirAberman,YaelPritch,andDanielCohen-Or. Prompt-
to-promptimageeditingwithcrossattentioncontrol. ICLR,2023.
14Daniel Holden, Jun Saito, and Taku Komura. A deep learning framework for character motion
synthesisandediting. ACMTOG,35(4):1–11,2016.
FangzhouHong,MingyuanZhang,LiangPan,ZhongangCai,LeiYang,andZiweiLiu. Avatarclip:
Zero-shottext-drivengenerationandanimationof3davatars. ACMSIGGRAPH,2022.
ZhiHou,BaoshengYu,andDachengTao. Compositional3dhuman-objectneuralanimation. arXiv
preprintarXiv:2304.14070,2023.
YihengHuang,HuiYang,ChuanchenLuo,YuxiWang,ShibiaoXu,ZhaoxiangZhang,ManZhang,
andJunranPeng. Stablemofusion: Towardsrobustandefficientdiffusion-basedmotiongeneration
framework. ACMMM,2024.
Deok-KyeongJang,SoominPark,andSung-HeeLee. Motionpuzzle: Arbitrarymotionstyletransfer
bybodypart. ACMTOG,41(3):1–16,2022.
BiaoJiang,XinChen,WenLiu,JingyiYu,GangYu,andTaoChen. Motiongpt: Humanmotionasa
foreignlanguage. NeurIPS,2024.
NanJiang,TengyuLiu,ZhexuanCao,JiemingCui,YixinChen,HeWang,YixinZhu,andSiyuan
Huang. Full-bodyarticulatedhuman-objectinteraction. ICCV,3,2022.
XuanJu,AilingZeng,YuxuanBian,ShaotengLiu,andQiangXu. Pnpinversion: Boostingdiffusion-
basededitingwith3linesofcode. InICLR,2024.
RoyKapon,GuyTevet,DanielCohen-Or,andAmitHBermano. Mas:Multi-viewancestralsampling
for3dmotiongenerationusing2ddiffusion. InCVPR,pages1965–1974,2024.
Korrawe Karunratanakul, Konpat Preechakul, Supasorn Suwajanakorn, and Siyu Tang. Guided
motiondiffusionforcontrollablehumanmotionsynthesis. InCVPR,pages2151–2162,2023.
KorraweKarunratanakul,KonpatPreechakul,EmreAksan,ThaboBeeler,SupasornSuwajanakorn,
andSiyuTang. Optimizingdiffusionnoisecanserveasuniversalmotionpriors. InCVPR,pages
1334–1345,2024.
JihoonKim,JiseobKim,andSungjoonChoi. Flame: Free-formlanguage-basedmotionsynthesis&
editing. InAAAI,volume37,pages8255–8263,2023.
NileshKulkarni,DavisRempe,KyleGenova,AbhijitKundu,JustinJohnson,DavidFouhey,and
LeonidasGuibas. Nifty: Neuralobjectinteractionfieldsforguidedhumanmotionsynthesis. In
CVPR,pages947–957,2024.
JeheeLeeandSungYongShin. Ahierarchicalapproachtointeractivemotioneditingforhuman-like
figures. InACMSIGGRAPH,pages39–48,1999.
JiamanLi,JiajunWu,andCKarenLiu. Objectmotionguidedhumanmotionsynthesis. ACMTOG,
42(6):1–11,2023a.
JiamanLi,AlexanderClegg,RoozbehMottaghi,JiajunWu,XavierPuig,andCKarenLiu. Control-
lablehuman-objectinteractionsynthesis. ECCV,2024.
WeiyuLi,XuelinChen,PeizhuoLi,OlgaSorkine-Hornung,andBaoquanChen. Example-based
motionsynthesisviagenerativemotionmatching. ACMTOG,42(4),2023b. doi:10.1145/3592395.
YanLi,TianshuWang,andHeung-YeungShum. Motiontexture: atwo-levelstatisticalmodelfor
charactermotionsynthesis. InACMSIGGRAPH,pages465–472,2002.
Han Liang, Wenqian Zhang, Wenxuan Li, Jingyi Yu, and Lan Xu. Intergen: Diffusion-based
multi-humanmotiongenerationundercomplexinteractions. IJCV,pages1–21,2024.
Xiao Lin and Mohamed R Amer. Human motion modeling using dvgans. arXiv preprint
arXiv:1804.10652,2018.
JinpengLiu,WenxunDai,ChunyuWang,YijiCheng,YansongTang,andXinTong. Plan,posture
andgo: Towardsopen-worldtext-to-motiongeneration. ECCV,2024.
15LibinLiu,KangKangYin,MichielVandePanne,TianjiaShao,andWeiweiXu. Sampling-based
contact-richmotioncontrol. InACMSIGGRAPH,pages1–10,2010.
Yunze Liu, Changxi Chen, and Li Yi. Interactive humanoid: Online full-body motion reaction
synthesiswithsocialaffordancecanonicalizationandforecasting.arXivpreprintarXiv:2312.08983,
2023.
ChengLu,YuhaoZhou,FanBao,JianfeiChen,ChongxuanLi,andJunZhu. Dpm-solver: Afastode
solverfordiffusionprobabilisticmodelsamplinginaround10steps. NeurIPS,pages5775–5787,
2022.
ShunlinLu,Ling-HaoChen,AilingZeng,JingLin,RuimaoZhang,LeiZhang,andHeung-Yeung
Shum. Humantomato: Text-alignedwhole-bodymotiongeneration. ICML,2024.
JieMa,YalongBai,BinengZhong,WeiZhang,TingYao,andTaoMei.Visualizingandunderstanding
patchinteractionsinvisiontransformer. IEEETNNLS,2023.
ChongMou,XintaoWang,JiechongSong,YingShan,andJianZhang. Dragondiffusion: Enabling
drag-stylemanipulationondiffusionmodels. ICLR,2024.
Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun Li, Jingwan Lu, and Jun-Yan Zhu.
Zero-shotimage-to-imagetranslation. InACMSIGGRAPH,pages1–11,2023.
AdamPaszke,SamGross,FranciscoMassa,AdamLerer,JamesBradbury,GregoryChanan,Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style,
high-performancedeeplearninglibrary. NeurIPS,2019.
FabianPedregosa,GaëlVaroquaux,AlexandreGramfort,VincentMichel,BertrandThirion,Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. Scikit-learn:
Machinelearninginpython. IMLR,12:2825–2830,2011.
XiaogangPeng,YimingXie,ZizhaoWu,VarunJampani,DeqingSun,andHuaizuJiang. Hoi-diff:
Text-driven synthesis of 3d human-object interactions using diffusion models. arXiv preprint
arXiv:2312.06553,2023.
MathisPetrovich,MichaelJBlack,andGülVarol. Temos: Generatingdiversehumanmotionsfrom
textualdescriptions. InECCV,pages480–497,2022.
MathisPetrovich,MichaelJBlack,andGülVarol. Tmr: Text-to-motionretrievalusingcontrastive
3dhumanmotionsynthesis. InICCV,pages9488–9497,2023.
MathisPetrovich, OrLitany, UmarIqbal, MichaelJBlack, GulVarol, XueBinPeng, andDavis
Rempe. Multi-tracktimelinecontrolfortext-driven3dhumanmotiongeneration. InCVPRW,
pages1911–1921,2024.
Ekkasit Pinyoanuntapong, Pu Wang, Minwoo Lee, and Chen Chen. Mmm: Generative masked
motionmodel. InCVPR,pages1546–1555,2024.
MatthiasPlappert,ChristianMandery,andTamimAsfour. Learningabidirectionalmappingbetween
humanwhole-bodymotionandnaturallanguageusingdeeprecurrentneuralnetworks. RAS,109:
13–26,2018.
SigalRaab,InbalLeibovitch,PeizhuoLi,KfirAberman,OlgaSorkine-Hornung,andDanielCohen-
Or. Modi: Unconditional motion synthesis from diverse data. In CVPR, pages 13873–13883,
2023.
Sigal Raab, Inbar Gat, Nathan Sala, Guy Tevet, Rotem Shalev-Arkushin, Ohad Fried, Amit H
Bermano,andDanielCohen-Or. Monkeysee,monkeydo: Harnessingself-attentioninmotion
diffusionforzero-shotmotiontransfer. ACMSIGGRAPHAsia,2024a.
SigalRaab,InbalLeibovitch,GuyTevet,MoabArar,AmitHaimBermano,andDanielCohen-Or.
Singlemotiondiffusion. InICLR,2024b.
OlafRonneberger,PhilippFischer,andThomasBrox. U-net: Convolutionalnetworksforbiomedical
imagesegmentation. InMICCAI,pages234–241.Springer,2015.
16Yonatan Shafir, Guy Tevet, Roy Kapon, and Amit H Bermano. Human motion diffusion as a
generativeprior. InICLR,2024.
JiamingSong,ChenlinMeng,andStefanoErmon. Denoisingdiffusionimplicitmodels. InICLR,
2021.
XiangjunTang,HeWang,BoHu,XuGong,RuifanYi,QilongKou,andXiaogangJin. Real-time
controllablemotiontransitionforcharacters. ACMTOG,41(4):1–10,2022.
ChenTessler,YunrongGuo,OfirNabati,GalChechik,andXueBinPeng. Maskedmimic: Unified
physics-basedcharactercontrolthroughmaskedmotioninpainting. ACMSIGGRAPHASIA,2024.
GuyTevet,BrianGordon,AmirHertz,AmitHBermano,andDanielCohen-Or.Motionclip:Exposing
humanmotiongenerationtoclipspace. InECCV,pages358–374,2022a.
Guy Tevet, Sigal Raab, Brian Gordon, Yonatan Shafir, Daniel Cohen-Or, and Amit H Bermano.
Humanmotiondiffusionmodel. InICLR,2022b.
NarekTumanyan,MichalGeyer,ShaiBagon,andTaliDekel. Plug-and-playdiffusionfeaturesfor
text-drivenimage-to-imagetranslation. InCVPR,pages1921–1930,2023.
AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,Łukasz
Kaiser,andIlliaPolosukhin. Attentionisallyouneed. NeurIPS,2017.
Weilin Wan, Zhiyang Dou, Taku Komura, Wenping Wang, Dinesh Jayaraman, and Lingjie Liu.
Tlcontrol: Trajectoryandlanguagecontrolforhumanmotionsynthesis. ECCV,2024.
ZanWang,YixinChen,TengyuLiu,YixinZhu,WeiLiang,andSiyuanHuang. Humanise:Language-
conditionedhumanmotiongenerationin3dscenes. NeurIPS,pages14959–14971,2022.
ZanWang,YixinChen,BaoxiongJia,PuhaoLi,JinluZhang,JingzeZhang,TengyuLiu,YixinZhu,
WeiLiang,andSiyuanHuang. Moveasyousayinteractasyoucan: Language-guidedhuman
motiongenerationwithsceneaffordance. InCVPR,pages433–444,2024.
Qianyang Wu, Ye Shi, Xiaoshui Huang, Jingyi Yu, Lan Xu, and Jingya Wang. Thor: Text to
human-objectinteractiondiffusionviarelationintervention. arXivpreprintarXiv:2403.11208,
2024.
ZeqiXiao,TaiWang,JingboWang,JinkunCao,WenweiZhang,BoDai,DahuaLin,andJiangmiao
Pang. Unifiedhuman-sceneinteractionviapromptedchain-of-contacts. InICLR,2024.
YimingXie,VarunJampani,LeiZhong,DeqingSun,andHuaizuJiang. Omnicontrol: Controlany
jointatanytimeforhumanmotiongeneration. InICLR,2024a.
Zhenyu Xie, Yang Wu, Xuehao Gao, Zhongqian Sun, Wei Yang, and Xiaodan Liang. Towards
detailedtext-to-motionsynthesisviabasic-to-advancedhierarchicaldiffusionmodel. InAAAI,
pages6252–6260,2024b.
KelvinXu,JimmyBa,RyanKiros,KyunghyunCho,AaronCourville,RuslanSalakhudinov,Rich
Zemel,andYoshuaBengio. Show,attendandtell: Neuralimagecaptiongenerationwithvisual
attention. InICML,pages2048–2057.PMLR,2015.
SiruiXu,ZhengyuanLi,Yu-XiongWang,andLiang-YanGui. Interdiff: Generating3dhuman-object
interactionswithphysics-informeddiffusion. InICCV,pages14928–14940,2023a.
SiruiXu,Yu-XiongWang,andLiangyanGui. Stochasticmulti-person3dmotionforecasting. In
ICLR,2023b.
Sirui Xu, Ziyin Wang, Yu-Xiong Wang, and Liang-Yan Gui. Interdreamer: Zero-shot text to 3d
dynamichuman-objectinteraction. arXivpreprintarXiv:2403.19652,2024.
HeyuanYao,ZhenhuaSong,BaoquanChen,andLibinLiu. Controlvae: Model-basedlearningof
generativecontrollersforphysics-basedcharacters. ACMTOG,41(6):1–16,2022.
17HeyuanYao,ZhenhuaSong,YuyangZhou,TenglongAo,BaoquanChen,andLibinLiu. Moconvq:
Unifiedphysics-basedmotioncontrolviascalablediscreterepresentations. ACMTOG,43(4):1–21,
2024.
YeYuan,JiamingSong,UmarIqbal,ArashVahdat,andJanKautz. Physdiff: Physics-guidedhuman
motiondiffusionmodel. InICCV,pages16010–16021,2023.
JianrongZhang,YangsongZhang,XiaodongCun,YongZhang,HongweiZhao,HongtaoLu,XiShen,
andYingShan. Generatinghumanmotionfromtextualdescriptionswithdiscreterepresentations.
InCVPR,pages14730–14740,2023a.
JiaxuZhang,XinChen,GangYu,andZhigangTu. Generativemotionstylizationofcross-structure
characterswithincanonicalmotionspace. InACMMM,2024a.
MingyuanZhang,XinyingGuo,LiangPan,ZhongangCai,FangzhouHong,HuirongLi,LeiYang,
andZiweiLiu. Remodiffuse: Retrieval-augmentedmotiondiffusionmodel. InICCV,2023b.
MingyuanZhang,ZhongangCai,LiangPan,FangzhouHong,XinyingGuo,LeiYang,andZiweiLiu.
Motiondiffuse: Text-drivenhumanmotiongenerationwithdiffusionmodel. IEEETPAMI,2024b.
MingyuanZhang,DaishengJin,ChenyangGu,FangzhouHong,ZhongangCai,JingfangHuang,
Chongzhi Zhang, Xinying Guo, Lei Yang, Ying He, et al. Large motion model for unified
multi-modalmotiongeneration. arXivpreprintarXiv:2404.01284,2024c.
MingyuanZhang,HuirongLi,ZhongangCai,JiaweiRen,LeiYang,andZiweiLiu. Finemogen:
Fine-grainedspatio-temporalmotiongenerationandediting. NeurIPS,36,2024d.
SiweiZhang,QianliMa,YanZhang,ZhiyinQian,TaeinKwon,MarcPollefeys,FedericaBogo,and
SiyuTang. Egobody: Humanbodyshapeandmotionofinteractingpeoplefromhead-mounted
devices. InECCV,pages180–200.Springer,2022.
Yan Zhang, Michael J Black, and Siyu Tang. Perpetual motion: Generating unbounded human
motion. arXivpreprintarXiv:2007.13886,2020.
YaqiZhang,DiHuang,BinLiu,ShixiangTang,YanLu,LuChen,LeiBai,QiChu,NenghaiYu,and
WanliOuyang. Motiongpt: Finetunedllmsaregeneral-purposemotiongenerators. InAAAI,pages
7368–7376,2024e.
YuechenZhang,JinboXing,EricLo,andJiayaJia. Real-worldimagevariationbyaligningdiffusion
inversionchain. NeurIPS,2023c.
ZeyiZhang,TenglongAo,YuyaoZhang,QingzheGao,ChuanLin,BaoquanChen,andLibinLiu.
Semantic gesticulator: Semantics-aware co-speech gesture synthesis. ACM TOG, 43(4):1–17,
2024f.
KaifengZhao,YanZhang,ShaofeiWang,ThaboBeeler,andSiyuTang. Synthesizingdiversehuman
motionsin3dindoorscenes. InICCV,pages14738–14749,2023.
ChongyangZhong, LeiHu, ZihaoZhang, andShihongXia. Attt2m: Text-drivenhumanmotion
generationwithmulti-perspectiveattentionmechanism. InICCV,pages509–519,2023.
LeiZhong,YimingXie,VarunJampani,DeqingSun,andHuaizuJiang. Smoodi: Stylizedmotion
diffusionmodel. ECCV,2024.
WenyangZhou,ZhiyangDou,ZeyuCao,ZhouyingchengLiao,JingboWang,WenjiaWang,Yuan
Liu,TakuKomura,WenpingWang,andLingjieLiu. Emdm: Efficientmotiondiffusionmodelfor
fast,high-qualitymotiongeneration. ECCV,2024.
ZixiangZhouandBaoyuanWang. Ude: Aunifieddrivingengineforhumanmotiongeneration. In
CVPR,pages5632–5641,2023.
WentaoZhu,XiaoxuanMa,DongwooRo,HaiCi,JinluZhang,JiaxinShi,FengGao,QiTian,and
YizhouWang. Humanmotiongeneration: Asurvey. IEEETPAMI,2023.
18Appendix
Contents
A SupplementalExperiments 20
A.1 WhatistheSelf-attentionMaplikeinMotion(De-)emphasizing?. . . . . . . . . . 20
A.2 WhatistheDifferencebetweenMotion(De-)emphasizinginMotionCLRandAd-
justingClassifier-freeGuidanceWeights? . . . . . . . . . . . . . . . . . . . . . . 21
A.3 MoreExperimentalResultsofIn-placeMotionReplacement . . . . . . . . . . . . 22
A.4 ComparisonwithManipulationNoisyMotionsintheDiffusionProcess . . . . . . 23
A.5 MotionGenerationResultVisualization . . . . . . . . . . . . . . . . . . . . . . . 24
A.6 MoreVisualizationResultsofMotionSequenceShifting . . . . . . . . . . . . . . 25
A.7 MoreVisualizationResultsonExampel-basedMotionGeneration . . . . . . . . . 26
A.8 DetailedVisualizationResultsofGroundedMotionGenration . . . . . . . . . . . 27
B UserInterfaceforInteractiveMotionGenerationandEditing 28
C DetailedDiagramofAttentionMechanisms 30
C.1 MathematicalVisualizationofSelf-attentionMechanism . . . . . . . . . . . . . . 30
C.2 MathematicalVisualizationofCross-attentionMechanism . . . . . . . . . . . . . 30
C.3 TheBasicDifferencewithPreviousDiffusion-basedMotionGenerationModelsin
Cross-modalModeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
D MoreVisualizationResultsofEmpiricalEvidence 33
E ImplementationandEvaluationDetails 35
E.1 ImplementationDetails . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
E.2 ComparedBaselines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
E.3 EvaluationDetails . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
F DetailsofMotionEditing 37
F.1 PseudoCodesofMotionEditing . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
F.2 SupplementaryforMotionStyleTransfer . . . . . . . . . . . . . . . . . . . . . . 40
G DetailsofActionCountinginaMotion 41
19A SupplementalExperiments
A.1 WhatistheSelf-attentionMaplikeinMotion(De-)emphasizing?
ThisexperimentisanextensionoftheexperimentshowninmFig.5.
Weprovidemoreexamplesofhowincreasingordecreasingweightsimpactmotion(de-)emphasizing
anderasing. AsseeninFig.14,theattentionmapsillustratethatreducingtheweights(e.g.,↓0.05
and↓0.10)resultsinlessactivations,whileincreasingweights(e.g.,↑0.33and↑1.00)leadstomore
activations. Thevanillamapservesasareferencewithoutanyadjustments. However,asindicated,
excessivelyhighweightssuchas↑1.00introducesomeartifacts,emphasizingtheneedforcareful
tuningofweightstomaintaintheintegrityofthegeneratedmotionoutputs. Thisdemonstratesthe
importanceofcarefulweighttuningtoachievethedesiredmotionemphasisorerasure.
Compared to Fig. 14a, Fig. 14b shows two fewer trajectories. This reduction is due to the de-
emphasizingeffect,wherethecharacter’ssecondjumpwasnotfullyexecuted,resultinginjustanarm
motion(Fig.5b). Consequently,thetwoactionsbecamedistinguishable,leadingtofewerdetected
twotrajectories. InFig.14c,thesecondjumpinghasbeencompletelyerased,resultinginonlyone
trajectory,furtherdemonstratinghowde-emphasizingsignificantlyaffectsmotionexecution.
(a) vinilla (b) ↓0.05 (c) ↓0.10
(d) vinilla (e) ↑0.33 (f) ↑1.00
Figure 14: Additional visualization results for different (de-)emphasizing weights. The self-
attention maps show how varying the different weights (e.g., ↓ 0.05, ↓ 0.10, ↑ 0.33, and ↑ 1.00)
affecttheemphasisonmotion.
20A.2 WhatistheDifferencebetweenMotion(De-)emphasizinginMotionCLRandAdjusting
Classifier-freeGuidanceWeights?
Inthispart,wewouldliketodiscussthedifferencebetweenreweightingthecross-attentionmapand
adjustingclassifier-freeguidanceweights.
AsshowninTab.4,weablatehowdifferentwsaffecttheresults. Theresultssuggestthatadjustment
ofwimpactsthequalityofgeneratedresults,makingw =2.5aneffectivechoice. Whenwincreases,
thetext-motionalignmentincreasesconsistently,andthegenerationquality(FID)requiresatrade-off.
w 1 1.5 2 2.5 3 3.5
FID 0.801 0.408 0.318 0.217 0.317 0.396
TMR-sim. 51.987 52.351 53.512 53.956 54.300 54.529
Table 4: Differnent editing results when changing ws. In MotionCLR, w = 2.5 is the default
designchoiceforthedenoisingsampling.
However,astheclassifier-freeguidancemainlyworksforthesemanticalignmentbetweentextand
motion,itcannotcontroltheweightofeachword. Wetakethe“a man jumps.” asanexample
forafaircomparison,whichisthecaseusedinthemaintext3. AsshowninFig.15,thegenerated
motionswithdifferentwvaluesillustratethatwcannotinfluencesboththeheightandfrequencyof
thejump. Nevertheless,theclassifier-freeguidanceislimitedinitsabilitytocontrolmoredetailed
aspects,suchastheexactheightandnumberofactions. Therefore,whilewimprovestext-motion
alignment,itcannotachievefine-grainedadjustments.
Figure 15: The effect of varying w in classifier-free guidance on generated motions. While
changingwinfluencesthegeneralalignmentbetweenthetext“a man jumps.” andthegenerated
motion,itdoesnotprovideprecisecontroloverfinerdetailslikejumpheightandfrequency.
3SuggestedtorefertoFig.5forcomparison.
21A.3 MoreExperimentalResultsofIn-placeMotionReplacement
Semanticsimilarityofeditedmotions. Inthein-placemotionreplacementapplication,wemeasure
theeditingqualityandthetext-motionsimilarity. Toverifythis,weconstructasetofprompts,tagged
withtheeditedwords. InTab.5,wecompareourmethod(oursreplaced)withtheuneditedones
(unreplaced)andthegeneratedmotionsdirectlychangingprompts(pseudo-GT).Ascanbeseen
inTab.5,themotionsofthethreegroupshavesimilarqualities. Besides,theeditedmotionissimilar
tothepseudo-GTgroup,indicatingthegoodsemanticalignmentoftheeditedresults.
FID↓ TMR-sim.→
direct(pseudoGT) 0.315 0.543
unreplaced 0.325 0.567
unreplaced(unpairedT-M) 0.925 0.490
oursreplaced 0.330 0.535
Table5: Comparisonbetweenthegenerationresultwithdirectlychangingpromptsandthein-
placereplacementinMotionCLR.Thesemanticsofeditingresultsaresimilartothemotiondirectly
generatedbythechangedprompt. Thesettingdifferencebetween“unreplaced”with“unreplaced
(unpairedT-M)”isthatthelattertextsareeditedsentences.
Ablationstudyofdifferentattentionlayers. Tofurtherexploretheimpactofattentionmanipulation
inin-placemotionreplacement,weconductanablationstudybyvaryingthelayersinMotionCLR
formanipulation, showninTab.6. Thetableliststheresultsfordifferentrangesofmanipulated
attentionlayers. Itcanbeobservedthatmanipulatingdifferentattentionlayersinfluencestheediting
qualityandthesemanticsimilarity(TMR-sim.). Inparticular,manipulatingthelayersfrom1to18
achievesthebestsemanticconsistency,demonstratingtheeffectivenessofeditingacrossmultiple
attentionlayersformaintainingsemanticalignmentintheeditedmotion. Thelesseffectivenessof
manipulatingmiddlelayersismainlyduetothefuzzysemanticspresentinthemiddlelayersofthe
U-Net. Astheselayerscapturemoreabstractwithreducedtemporalresolution,theprecisedetails
andlocalizedinformationbecomelessdistinct. Consequently,manipulatingtheselayershasalimited
impactonthefinaloutput,astheycontributelessdirectlytothefine-graineddetailsforthetask.
begin end FID↓ TMR-sim.↑
8 11 0.339 0.472
5 14 0.325 0.498
1 18 0.330 0.535
Table6: Theablationstudyofmanipulatingdifferentattentionlayers. The“begin”and“end”
representthebeginningandthefinallayerformanipulation.
22A.4 ComparisonwithManipulationNoisyMotionsintheDiffusionProcess
As the diffusion denoising process can manipulate the motion directly in the denoising process,
thisisabaselineforcomparisonwithourmotionshiftingandexample-basedmotiongeneration
applications. Here,forconvenience,weonlytaketheexample-basedmotiongenerationapplication
asanexamplefordiscussion. Inthissection,weconductacomparisonbetweenourproposedediting
methodanddiffusionmanipulationinthemotionspace,focusingontheFIDanddiversitymetrics.
The200samplesusedinthisexperimentwereconstructedbyresearchers. AsdepictedinTab.7,
the“Diff. manipulation”servesforourcomparison. OurmethodachievesanFIDvalueof0.427,
indicating a relatively high generation quality, while the “Diff. manipulation” achieves a higher
FID of 0.718, demonstrating worse fidelity. Conversely, in terms of diversity, the “MotionCLR
manipulation”exhibitsahigherdiversity(Div.) scoreof2.567comparedtothe1.502ofthe“Diff.
manipulation.” These results verify our method is better than manipulating noisy motions in the
denoisingprocess. Themainreasonforthebetterqualityanddiversitymainlyreliesonthemany
timesofmanipulationofself-attention,butnotthemotion. Directlymanipulatingthemotionresults
insomejitters,makingmoreeffortformodelstosmooth.Besides,theshufflingtimesofmanipulating
theself-attentionmapsarehigherthanthebaseline,contributingtothebetterdiversityofourmethod.
FID↓ Div. ↑
Diff. manipulation 0.718 1.502
MotionCLRmanipulation 0.427 2.567
Table7: ComparisononFIDanddiversityvalueswithmanipulatingself-attentioninthemotion
spaceofthedenoisingprocess.
23A.5 MotionGenerationResultVisualization
WerandomlychosesomeexamplesofthemotiongenerationresultsinFig.16. Thevisualization
resultsdemonstratethatMotionCLRcangeneratecoherentandrealistichumanmotionsbasedon
diversetextualdescriptions. Thegeneratedsequencescapturevariousactionsrangingfromsimple
gesturestomorecomplexmovements,indicatingthecapabilitytohandleawiderangeofhuman
behaviors. Overall, the qualitative results suggest that MotionCLR effectively translates textual
promptsintohuman-likemotionswithaclearunderstandingoftexts. Thisdemonstratesthepotential
forapplicationsinscenariosrequiringaccuratemotiongenerationbasedonnaturallanguageinputs.
a person walks forward, turns a person side steps to their left, person takes one step backward
and then sits on a chair. before side stepping to their right. to their left
person walks forwards slowly a person waves, takes a step back a person holds their arms out in
and normally without swinging front of them, squats, then
arms swivels from side to side
a person takes a few steps, a person is waving with his hand. a man kicks with something or
squats twice, then turns and someone with his right leg.
walks back
someone seems to be looking for a person appears to be hitting a a man walks clockwise in a
a balance not to fall ball with their right hand. circle
Figure16: HumanmotiongenerationresultofMotionCLR.
24A.6 MoreVisualizationResultsofMotionSequenceShifting
WepresentfurthercomparisonsbetweentheoriginalandeditedmotionsinFig.17. Thetimebars,
indicated by “ ” and “ ,” represent distinct phases of the motion, with their sequential
arrangementreflectingtheprogressionofthemotionovertime.
In Fig. 17a, we observe that the action of crossing the obstacle, originally positioned earlier in
the sequence, is shifted towards the end in the edited version. This adjustment demonstrates the
model’scapacitytorearrangecomplexmotionseffectivelywhilemaintainingcoherence. Similarly,
Fig.17bshowsthestanding-byactionbeingrelocatedtotheendofthemotionsequence. Thischange
emphasizesthemodel’sabilitytohandlesignificantalterationsinthetemporalarrangementofactions.
Theseresultscollectivelyindicatethatoureditingprocess,drivenbytheattentionmapsequentiality,
exhibitsahighlevelofcorrespondencewiththeintendededitstothemotion’ssequence. Themodel
accuratelycapturesandreplicatesthedesiredmodifications,ensuringthattherestructuredmotion
retainsanaturalandlogicalflow,therebyvalidatingtheeffectivenessofourmotioneditingapproach.
(a) Prompt: “the person is walking (b) Prompt: “a person walks then
forward on uneven terrain.” Original jumps.” Original (blue) vs. shifted (red)
(blue)vs.shifted(red)motion. motion.
Figure17: Comparisonbetweenoriginalmotionandtheshiftedmotion.Theshiftedtimebarsareshownin
differentcolors.(a)Theoriginalfigurecrossestheobstacleafterthewalkingaction.Theshiftedmotionhasthe
oppositesequentiality.(b)Thekeywalkingandjumpingactionsareshiftedtothebeginningofthesequence,
andthestanding-byactionisshiftedtotheend.
25A.7 MoreVisualizationResultsonExampel-basedMotionGeneration
We provide some visualization results to further illustrate the effectiveness of our approach in
generating diverse motions that adhere closely to the given prompts. In Fig. 18, the example
motionof“a person kicking their feet”istakenasthereference,andmultiplediversekick
motions are generated. These generated motions not only exhibit variety but also maintain key
characteristics of the original example. Similarly, in Fig. 19, the example motion of “a person
walking in a semi-circular shape while swinging arms slightly” demonstrates the
capabilitytogeneratediversewalkingmotionsthatmaintainthedistinctfeaturesofthesourcemotion.
Thegeneratedtrajectories,inFig.18bandFig.19b,showthatdiversemotionsfollowdifferentpaths
whileretainingsimilaritieswiththeoriginalone,confirmingtheeffectivenessofourmethod.
(a)Theexamplemotion(blue)and (b)Thetrajectoryvisualizationsoftheexamplemotion
thegenerateddiversemotion(red). anddiversemotions.
Figure 18: Diverse generated results of blue example generated by the prompt “a person
kicks their feet.”. Theexample-basedgeneratedkickmotionsarediverseandsimilartothe
sourceexample.
(a)Theexamplemotion(blue)and (b)Thetrajectoryvisualizationsoftheexamplemotion
thegenerateddiversemotion(red). anddiversemotions.
Figure19: Diversegeneratedresultsofblueexamplegeneratedbytheprompt“person walks
in a semi circular shape while swinging arms slightly.”. Theexample-basedgener-
atedwalkingmotionsarediverseandsimilartothesourcewalkingexample.
26A.8 DetailedVisualizationResultsofGroundedMotionGenration
(a)Therootheightcomparison.Theredareadenotesthetimestepstoexecuteactions.
vinilla
result
edited
retuslt
(b)Themotionvisualization.Thevanillageneratedresult(blue)vs.editedresult(red)w/temporalgrounds.
Figure20: Comparisonbetweenw/vs.w/ogroundedmotiongenerationsettings. Therootheight
andmotionvisualizationofthetextualprompt“apersonjumpsfourtimes”.
As depicted in Fig. 20, we provide a detailed comparison between the motion generation results
withandwithoutgroundedsettings. Whilethemaintext(Sec.6)hasalreadydiscussedthegeneral
differencesbetweenthesesettings,hereintheappendix,wefurtherextractandvisualizetheroot
height trajectory separately for a clearer and more detailed comparison. This approach helps in
highlightingtheeffectivenessofourmethodinaddressingmotionhallucinationissuesandensuring
thatthegeneratedmovementscloselyalignwiththegiventextualprompts.
InFig.20a,therootheightcomparisondistinctlyshowsthedifferencebetweentheeditedandvanilla
results. Thered-shadedregionsindicatethetimestepswherethespecifiedactions(“jumps four
times”)shouldoccur. Withoutgroundedmotiongeneration,thevanillaresulttendstogeneratemore
thantherequirednumberofjumps,resultinginmotionhallucination.However,withtheincorporation
oftemporalgrounding,oureditedresultaccuratelyperformstheactionfourtimes,aligningwith
thetextualprompt. Fig.20bfurthervisualizesthemotionsequences. Itisevidentthatthetemporal
groundingguidesthemotiongenerationprocess,ensuringconsistencywiththeinputprompt. The
editedresultfollowsthecorrectsequenceofactions,demonstratingtheadvantageofusinggrounded
motionsettingstoavoidcommonhallucinationsingenerativemodels.
Overall,thesedetailedvisualizationresultsconfirmtheimportanceofincorporatingtemporalground-
ingintomotiongenerationtasks,asithelpsmitigatehallucinationsingenerativemodels,ensuring
thegeneratedmotionsaremorefaithfullyalignedwiththeintendedtextualdescriptions.
27B UserInterfaceforInteractiveMotionGenerationandEditing
Tohaveabetterunderstandingofourtask,webuildauserinterfacewithGradio[Abidetal.,2019].
Weintroducethedemoasfollows.
InFig.21,weillustratethestepsinvolvedingeneratingandvisualizingmotionsusingtheinteractive
interface. Fig.21adisplaystheinitialstepwheretheuserprovidesinputtextsuchas“amanjumps”
andadjustsmotionparameters. Oncethesettingsarefinalized, thesystembeginsprocessingthe
motionbasedontheseinputs,asseenintheleftpanel. Fig.21bshowcasesthegeneratedmotion
basedontheuser’sinput. Theinterfaceprovidesarenderedoutputoftheskeletonperformingthe
describedmotion. Thispresentationallowsuserstoeasilycorrelatetheinputparameterswiththe
resultinganimation. Thegeneratedmotioncanfurtherbeeditedbyadjustingparameterssuchasthe
lengthofthemotion,emphasizingorde-emphasizingcertainactions,orreplacingactionsaltogether,
dependingonuserrequirements. Thisprocessdemonstrateshowtheinterfacefacilitatesaworkflow
frominputtoreal-timemotionvisualization.
(a)Motiongenerationinterfaceexample. (b)GeneratedMotionExample
Figure21: Motiongenerationanditsoutputexamples.
Thelogicalsequenceofoperationsisasfollows:
1. Inputthetext: Usersstartbyenteringtextdescribingthemotion(e.g.,“a man jumps.”) orset
theframesofmotionstogenerate(asshowninFig.21a).
2. Generatetheinitialmotion: Thesystemgeneratesthecorrespondingskeletonmotionsequence
basedontheinputtext(asshowninFig.21b).
3. Motionediting: WeshowsomedownstreamtasksofMotionCLRhere.
• Motionemphasizing/de-emphasizing: Userscanselectaspecificwordfromthetext(e.g.,
“jumps”)andadjustitsemphasisusingaweightslider(range[-1,1])(asseeninFig.22a). For
example,settingtheweightto0.3willeitherincreasethejumpmotion’sintensity.
• In-placereplacement: Ifuserswanttochangetheaction,theycanselectthe“replace”op-
tion. For example, replacing “jumps” with “walks” will regenerate the motion, showing a
comparisonbetweentheoriginalandneweditedmotions(asshowninFig.22b).
• Example-basedmotiongeneration: Userscangeneratemotionsequencesbasedonpredefined
examplesbysettingparameterslikechunksizeanddiffusionsteps. Afterspecifyingthenumber
ofmotionstogenerate,thesystemwillcreatemultiplevariationsoftheinputmotion,providing
diverse options for further refinement (as illustrated in Fig. 22d). The progress bars of the
processarevisualizedinFig.22c.
28(a)Motion(de-)Emphasizinginterface. (b)In-placereplacementexample.
(c)Example-basedmotiongenerationprogress. (d)Example-basedmotiongenerationresults.
Figure22: Differentinterfacesandsupportingfunctionsforinteractivemotionediting.
29C DetailedDiagramofAttentionMechanisms
C.1 MathematicalVisualizationofSelf-attentionMechanism
Inthemaintext(Eq.(2)),weintroducedtheself-attentionmechanismofMotionCLR,whichutilizes
differenttransformationsofmotionasinputs. Themotionembeddingsserveasboththequery(Q),
key(K),andvalue(V),capturingtheinternalrelationshipswithinthesequenceofmotionframes.
At timestep f, which motion
At timestep f, which motion feature should be selected?
feature should be selected?
soft max motion embs. (F x d), F=6
motion embs. (F x d), F=6 motion embs. (F x d), F=6 F soft max F d
F
F F
F d F
softmax
Q (mod tions) KT (motions) similarity simQ ilK aT rity V (motions) X’
√
(a)S=QK⊤. (b)X′ =softmax(S/ d)V.
Figure23: MathematicalVisualizationofSelf-attentionMechanism. ThisfiguretakesF =6as
anexample. (a)Thesimilaritycalculationwithqueriesandkeys(differentframes). (b)Thesimilarity
matrixpicks“value”softheattentionmechanismandupdatesmotionfeatures.
Fig.23providesadetailedmathematicalvisualizationofthisprocess:
(1) Similarity Calculation. In the first step, the similarity between the motion embeddings at
differentframesiscomputedusingthedotproduct,representedbyS=QK⊤. Thismeasurement
reflects the internal relationship/similarity between different motion frames within the sequence.
Fig.23aillustrateshowthesoftmax(·)operationisappliedtothesimilaritymatrixtodetermine
whichmotionfeatureshouldbeselectedatagivenframef.
(2)FeatureUpdating. Next,thesimilarityscoresareusedtoweightthemotionembeddings(V)
√
andgenerateupdatedfeaturesX′,asshownbytheequationX′ = softmax(QK⊤/ d)V. Here,
thesimilaritymatrixappliesitsselectionofvalues(V)toupdatethemotionfeatures. Thisprocess
allowstheself-attentionmechanismtodynamicallyadjusttherepresentationofeachmotionframe
basedonitsrelevancetootherframesinthesequence.
In summary, the self-attention mechanism aims to identify and emphasize the most relevant mo-
tion frames in the sequence, updating the features to enhance their representational capacity for
downstreamtasks. Themostessentialcapabilityofcross-attentionistoorderthemotionfeatures.
C.2 MathematicalVisualizationofCross-attentionMechanism
Inthemaintext(Eq.(3)),weintroducedthecross-attentionmechanismofMotionCLR,whichutilizes
thetransformationofmotionasaquery(Q)andthetransformationoftextasakey(K)andvalue
(V)toexplicitlymodelthecorrespondencebetweenmotionframesandwords.
At timestep f, which word At timestep f, which word
should be activated? should be activated?
soft max
word embs. (L x d), L=5
motion embs. (F x d), F=6 word embs. (L x d), L=5 L soft max L d
L
F d F F L
softmax
d QKT X’
Q (motions) KT (text) similarity similarity V (text)
√
(a)S=QK⊤. (b)X′ =softmax(S/ d)V.
Figure24: MathematicalVisualizationofCross-attentionMechanism. ThisfiguretakesF =6
andN =5asanexample. (a)Thesimilaritycalculationwithqueriesandkeys. (b)Thesimilarity
matrixpicks“value”softheattentionmechanismandupdatesfeatures.
30Fig.24providesadetailedmathematicalvisualizationofthisprocess:
(1) Similarity Calculation. In the first step, the similarity between the motion embeddings (Q)
withF framesandthetextembeddings(K)withN wordsiscomputedthroughthedotproduct,
representedbyS = QK⊤. Thissimilaritymeasurementreflectstherelationshipbetweenmotion
framesandwords. Fig.24ashowshowthesoftmax(·)operationisappliedtothesimilaritymatrix
todeterminewhichwordshouldbeactivatedatagivenframef.
(2)FeatureUpdating. Next,thesimilarityscoresareusedtoweightthetextembeddings(V)and
√
generateupdatedfeaturesX′,asshownbytheequationX′ = softmax(QK⊤/ d)V. Here,the
similaritymatrixappliesitsselectionofvalues(V)toupdatethefeatures. Thisprocessestablishes
anexplicitcorrespondencebetweentheframesandspecificwords.
Insummary,thesimilaritycalculationprocessdetermineswhichframe(s)shouldbeselected,andthe
featureupdatingprocess(multiplicationwithV)istheexecutionoftheframe(s)placement.
C.3 TheBasicDifferencewithPreviousDiffusion-basedMotionGenerationModelsin
Cross-modalModeling
Asdiscussedinthemaintext(seeSec.1),despitetheprogressesinhumanmotiongeneration[Zhang
etal.,2024d,Caietal.,2024,Zhangetal.,2024c,Guoetal.,2024c,Raabetal.,2024b,Kaponetal.,
2024,Cohanetal.,2024,Fanetal.,2024,Xuetal.,2024,2023a,b,Yaoetal.,2022,Fengetal.,
2023,Aoetal.,2023,Yaoetal.,2024,Zhangetal.,2024f,Liuetal.,2010,Abermanetal.,2020a,
Karunratanakuletal.,2024,Lietal.,2024,2023a,Gongetal.,2023,ZhouandWang,2023,Zhong
etal.,2023,Zhuetal.,2023,Athanasiouetal.,2023,Zhongetal.,2024,Guoetal.,2024b,Zhang
etal.,2024c,Athanasiouetal.,2024,Zhaoetal.,2023,Zhangetal.,2022,2020,Diomatarisetal.,
2024,Pinyoanuntapongetal.,2024,DillerandDai,2024,Pengetal.,2023,Houetal.,2023,Liu
etal.,2023,Congetal.,2024,Cuietal.,2024,Jiangetal.,2022,Kulkarnietal.,2024,Tessleretal.,
2024,Liangetal.,2024,Ghoshetal.,2023,Wuetal.,2024],therestilllacksaexplicitmodeling
ofword-levelcross-modalcorrespondenceinpreviouswork. Toclarifythis,ourmethodmodelsa
fine-grainedword-levelcross-modalcorrespondence.
��−1
��−1 �
EOS
text
right
to
Denoising Transformer encoder AR Transformer walks cross-attention
man
a
text text
�� �
(a) MDM-like fashion (b) Auto-regressive fashion (c) Word-level corr�e�spondence
Figure25: Comparisonwithpreviousdiffusion-basedmotiongenerationmodels. (a)MDM-like
fashion: Tevetetal.[2022b]anditsfollow-upmethodstreattextembeddingsasawholeandmix
themwithmotionrepresentationsusingadenoisingTransformer. (b)Auto-regressivefashion: Zhang
etal.[2023a]anditsfollow-upmethodsconcatenatethetextwiththemotionsequenceandfeedthem
intoanauto-regressivetransformerwithoutexplicitcorrespondencemodeling. (c)Ourproposed
methodestablishesfine-grainedword-levelcorrespondenceusingcross-attentionmechanisms.
AsillustratedinFig.25,themajordistinctionbetweenourproposedmethodandpreviousdiffusion-
basedmotiongenerationmodelsliesintheexplicitmodelingofword-levelcross-modalcorrespon-
dence. IntheMDM-likefashionTevetetal.[2022b](seeFig.25a),previousmethodsusuallyutilize
a denoising transformer encoder that treats the entire text as a single embedding, mixing it with
themotionsequence. Thisapproachlackstheabilitytocapturethenuancedrelationshipbetween
individualwordsandcorrespondingmotionelements,resultinginanover-compressedrepresentation.
AlthoughwewitnessthatZhangetal.[2024b]alsointroducescross-attentioninthemotiongeneration
31process,itstillfacestwoproblemsinrestrictingthefine-grainedmotioneditingapplications. First
of all, the text embeddings are mixed with frame embeddings of diffusion, resulting in a loss of
detailedsemanticcontrol. Ourapproachdisentanglesthediffusiontimestepinjectionprocessinthe
convolution module to resolve this issue. Besides, the linear cross-attention in MotionDiffuse is
differentfromthecomputationprocessofcross-attention,resultinginalackofexplanationofthe
word-levelcross-modalcorrespondence. Theauto-regressive(AR)fashion[Zhangetal.,2023a]
(Fig.25b)adoptsasimpleconcatenationoftextandmotion,whereanARtransformerprocesses
themtogether. However,thisfashionalsofailstoexplicitlyestablishafine-grainedcorrespondence
betweentextandmotion,astheARtransformermerelyregardsthetextandmotionembeddingsas
oneunifiedsequence.
Ourapproach(showninFig.25c)introducesacross-attentionmechanismthatexplicitlycaptures
theword-levelcorrespondencebetweentheinputtextandgeneratedmotionsequences. Thisallows
ourmodeltomaintainaclearandinterpretablemappingbetweenspecificwordsandcorresponding
motionpatterns,significantlyimprovingthequalityandalignmentofgeneratedmotionswiththe
textualdescriptions. Byintegratingsuchaword-levelcross-modalmodelingtechnique,ourmethod
notonlyachievesmoreaccurateandrealisticmotiongenerationbutalsosupportsfine-grainedword-
levelmotionediting. Thiscapabilityenablesuserstomakepreciseadjustmentstospecificpartsof
thegeneratedmotionbasedontextualprompts,addressingthecriticallimitationspresentinprevious
diffusion-based motion generation models and allowing for more controllable and interpretable
editingatthewordlevel.
32D MoreVisualizationResultsofEmpiricalEvidence
In the main text, we introduced the foundational understanding of both cross-attention and self-
attentionmechanisms,emphasizingtheirabilitytocapturetemporalrelationshipsanddependencies
across motion sequences. As a supplement, we provide a new, more detailed example here. As
shown in Fig. 26, this visualization illustrates how different attention mechanisms respond to a
complexsequenceinvolvingbothwalkingandjumpingactions. Specifically,weusegreendashed
boxes to highlight the “walk” phases and red dashed boxes to indicate the “jump” phases. This
allows us to clearly distinguish the temporal patterns associated with each action. Besides, we
observedthattheword“jump”reachesitshighestactivationduringthecrouchingphase,whichlikely
correlateswiththismomentbeingboththestartofthejumpingactionandthe“poweraccumulation
phase”. Thissuggeststhattheattentionmechanismaccuratelycapturesthepreparatorystageofthe
movement,highlightingitscapabilitytorecognizethenuancesofmotioninitiationwithincomplex
sequences. Thecross-attentionmapeffectivelyalignskeyactionwordslike“walk”and“jump”with
theirrespectivemotionsegments,whiletheself-attentionmaprevealsrepeatedmotionpatternsand
similaritiesbetweenthewalkingandjumpingcycles.
(a) Horizontal distance
(d) Self-attention map
(b) Vertical height
(c) Cross-attention map (e) Motion Visualization
Figure26:Empiricalstudyofattentionpatterns.Weusetheexample“a person walks stop and then
jumps.”(a)Horizontaldistancetraveledbythepersonovertime,highlightingdistinctwalkingandjumping
phases.(b)Theverticalheightchangesoftheperson,indicatingvariationsduringwalkingandjumpingactions.
(c)Thecross-attentionmapbetweentimestepsandthedescribedactions. Noticethat“walk”and“jump”
receiveastrongerattentionsignalcorrespondingtothewalkandjumpsegments.(d)Theself-attentionmap,
whichclearlyidentifiesrepeatedwalkingandjumpingcycles,showssimilarpatternsinthesub-actions. (e)
Visualizationofthemotionsequences,demonstratingthewalkingandjumpingactions.
Continuing with another case study, in Fig. 27, we examine how attention mechanisms respond
to a sequence that primarily involves walking actions with varying intensity. In this instance,
we observe that both the horizontal distance (Fig. 27a) and vertical height (Fig. 27b) reflect the
man walks straight. The cross-attention map (Fig. 27c) reveals how the word “walks” related to
walkingmaintainsconsistentactivation,indicatingthatMotionCLRhasaword-levelunderstanding
throughoutthesequence. Theself-attentionmap(Fig.27d)furtheremphasizesrepeatedwalking
patterns, demonstrating that the mechanism effectively identifies the temporal consistency and
structureofthewalkingphases. Themotionvisualization(Fig.27e)reinforcesthisfinding,showing
aclear,uninterruptedwalkingmotion.
33Moreimportantly,wecanobservethatthewalkingactionconsistsofatotaloffivesteps: threesteps
withtherightfootandtwowiththeleftfoot. Theself-attentionmap(Fig.27d)clearlyrevealsthat
stepstakenbythesamefootexhibitsimilarpatterns,whilemovementsbetweendifferentfeetshow
distinctdifferences. Thisobservationindicatesthattheself-attentionmechanismeffectivelycaptures
the subtle variations between repetitive motions, further demonstrating its sensitivity to nuanced
motioncapturecapabilitywithinthesequence.
Besides, different from the jumping, the highlights in the self-attention map of the walking are
rectangular. Thereasonisthatthelocalmovementsofwalkingaresimilar. Incontrast,thejumping
includes several sub-actions, resulting in the highlighted areas in the self-attention maps being
elongated.
(a) Horizontal distance
(d) Self-attention map
(b) Vertical height
(c) Cross-attention map (e) Motion Visualization
Figure27: Empiricalstudyofattentionpatternsinaconsistentwalkingsequence. Weusetheexample:
“a man walks.”.(a)Thehorizontaldistancetraveledovertimereflectsasteadywalkingmotion.(b)Vertical
heightchangesindicateminimalvariation,characteristicofwalkingactions.(c)Thecross-attentionmapshows
thatthe“walks”wordmaintainsconsistentactivation. (d)Theself-attentionmaphighlightstherepeated
walkingcycles,capturingthetemporalstability.(e)Visualizationofthemotionsequence.
34E ImplementationandEvaluationDetails
E.1 ImplementationDetails
TheMotionCLRmodelistrainedontheHumanML3DdatasetwithoneNVIDIAA-100GPUbased
onPyTorch[Paszkeetal.,2019]. Thelatentdimensionofthemotionembeddingis512. Wetake
theCLIP-ViT-Bmodeltoencodertextasword-levelembeddings. Thetrainingprocessutilizesa
batchsizeof64,withalearningrateinitializedat2e−4anddecayingatarateof0.9every5,000
steps. Additionally,aweightdecayof1e−2isemployedtoregularizethemodelparameters. Forthe
diffusionprocess,themodelistrainedover1,000diffusionsteps. Weincorporateaprobabilityof0.1
forconditionmaskingtofacilitateclassifier-freeguidancelearning. Duringtraining,dropoutissetat
0.1topreventoverfitting,andallnetworksinthearchitecturefollowan8-layerTransformerdesign.
Intheinferencestage,allstepsofthedenoisingsamplingaresetas10consistently. Forthemotion
erasingapplication,wesettheerasingweightas0.1asdefault. MotionCLRsupportsbothDDIM-
sampling[Songetal.,2021]andDPM-soler-sampling[Luetal.,2022]methods,with1,000asfull
diffusionsteps. Forthein-placementmotionreplacementandthemotionstyletransferapplication,
asthemotionsemanticsmainlydependontheinitialdenoisingsteps,wesetthemanipulatingsteps
until5asdefault. Formotion(de-)emphasizing,wesupportbothmultiplications(largerthan1for
emphasizing,lowerthan1forde-emphasizing)andaddition(largerthan0foremphasizing,lower
than 0 for de-emphasizing) to adjust the cross-attention weights. For the example-based motion
generation,theminimummanipulatingtimeofamotionzoneis1s(a.k.a.chunksize=20forthe20
FPSsetting). Ateachstep, allattentionmapsatalllayerswillbemanipulatedateachdenoising
timestep. Userscanadjusttheparametersfreelytoachieveinteractivemotiongenerationandediting
(moredetailsofuserinterfaceinAppendixB).
E.2 ComparedBaselines
Here,weintroducedetailsofbaselinesinTab.1forourcomparison.
TM2T[Guoetal.,2022b]exploresthereciprocalgenerationof3Dhumanmotionsandtexts. It
usesmotiontokensforcompactrepresentation,enablingflexiblegenerationforbothtext2motionand
motion2texttasks.
T2M[Guoetal.,2022a]generatesdiverse3Dhumanmotionsfromtextusingatwo-stageapproach
involvingtext2lengthsamplingandtext2motiongeneration. Itemploysamotionsnippetcodeto
capturesemanticcontextsformorefaithfulmotiongeneration.
MDM[Tevetetal.,2022b]usesadiffusion-basedapproachwithatransformer-baseddesignfor
generatinghumanmotions. Itexcelsathandlingvariousgenerationtasks,achievingsatisfyingresults
intext-to-motiontasks.
MLD[Chenetal.,2023b]usesadiffusionprocessonmotionlatentspaceforconditionalhuman
motiongeneration. ByemployingaVariationalAutoEncoder(VAE),itefficientlygeneratesvivid
motionsequenceswhilereducingcomputationaloverhead.
MotionDiffuse[Zhangetal.,2024b]isadiffusionmodel-basedtext-drivenframeworkformotion
generation. Itprovidesdiverseandfine-grainedhumanmotions,supportingprobabilisticmapping
andmulti-levelmanipulationbasedontextprompts.
T2M-GPT[Zhangetal.,2023a]combinesaVQ-VAEandGPTtogeneratehumanmotionsfromthe
text. Withitssimpleyeteffectivedesign,itachievescompetitiveperformanceandoutperformssome
diffusion-basedmethodsonspecificmetrics.
ReMoDiffuse[Zhangetal.,2023b]integratesretrievalmechanismsintoadiffusionmodelformotion
generation, enhancing diversity and consistency. It uses a Semantic-Modulated Transformer to
incorporateretrievalknowledge,improvingtext-motionalignment.
MoMask [Guo et al., 2024a] introduces a masked modeling framework for 3D human motion
generationusinghierarchicalquantization. Itoutperformsothermethodsingeneratingmotionsand
isapplicabletorelatedtaskswithoutfurtherfine-tuning.
35E.3 EvaluationDetails
Motion(de-)emphasizing. Toevaluatetheeffectivenessofmotion(de-)emphasizingapplication,we
construct100promptstoverifythealgorithm. Allofthesepromptsareconstructedbyresearchers
manually. Wetakesomesamplesfromourevaluationsetasfollows.
... ...
3 the figure leaps high
4 a man is waving hands
... ...
Each line in the examples represents the index of the edited word in the sentence, followed by
thecorrespondingprompt. Theseindicesindicatethekeyverbsoractionsthataresubjecttothe
(de-)emphasizing during the evaluation process. The prompts were carefully selected to cover a
diverserangeofactions,ensuringthatourmethodistestedondifferenttypesofmotiondescriptions.
Forinstance,intheprompt“3 the figure leaps high”,thenumber3indicatesthattheword
“leaps”isthethirdwordinthesentenceandisthetargetactionfor(de-)emphasizing. Thisformat
ensuresasystematicevaluationofhowthemodelrespondstoadjustingattentionweightsonspecific
actionsacrossdifferentprompts.
Example-based motion generation. To further evaluate our example-based motion generation
algorithm, we randomly constructed 7 test prompts. We used t-SNE [Pedregosa et al., 2011]
visualizationtoanalyzehowcloselythegeneratedmotionsresembletheprovidedexamplesinterms
of motion textures. For each case, the generated motion was assessed based on two criteria: (1)
similaritytotheexample,and(2)diversityacrossdifferentgeneratedresultsfromthesameexample.
Action counting. To thoroughly evaluate the effectiveness of our action counting method, we
constructedatestsetcontaining70prompts. Thesepromptsweremanuallydesignedbyresearchers
toensurediversity. Eachpromptcorrespondstoamotionsequencegeneratedbyourmodel,andthe
groundtruthactioncountswerelabeledbyresearchersbasedontheobservableactionswithinthe
generatedmotions.
36F DetailsofMotionEditing
Inthissection,wewillprovidemoretechnicaldetailsaboutthemotioneditingalgorithms.
F.1 PseudoCodesofMotionEditing
Motion(de-)emphasizing. Motion(de-)emphasizingmainlymanipulatethecross-attentionweights
oftheattentionmap. KeycodesareshownintheL16-18ofCode1.
1 def forward(self, x, cond, reweighting_attn, idxs):
2 B, T, D = x.shape
3 N = cond.shape[1]
4 H = self.num_head
5
6 # B, T, 1, D
7 query = self.query(self.norm(x)).unsqueeze(2).view(B, T, H, -1)
8 # B, 1, N, D
9 key = self.key(self.text_norm(cond)).unsqueeze(1).view(B, N, H,
-1)
10
11 # B, T, N, H
12 attention = torch.einsum(’bnhd,bmhd->bnmh’, query, key) / math.
sqrt(D // H)
13 weight = self.dropout(F.softmax(attention, dim=2))
14
15 # reweighting attention for motion (de-)emphasizing
16 if reweighting_attn > 1e-5 or reweighting_attn < -1e-5:
17 for i in range(len(idxs)):
18 weight[i, :, 1 + idxs[i]] = weight[i, :, 1 + idxs[i]] +
reweighting_attn
19
20 value = self.value(self.text_norm(cond)).view(B, N, H, -1)
21 y = torch.einsum(’bnmh,bmhd->bnhd’, weight, value).reshape(B, T, D
)
22 return y
Code1: Pseudocodesformotion(de-)emphasizing.
In-place motion replacement. The generation of two motions (B=2) are reference and edited
motions. Asthecross-attentionmapdetermineswhentoexecutetheaction. Therefore,replacingthe
cross-attentionmapdirectlyisastraightforwardway,whichisshowninL16-17ofCode2.
1 def forward(self, x, cond, manipulation_steps_end):
2 B, T, D = x.shape
3 N = cond.shape[1]
4 H = self.num_head
5
6 # B, T, 1, D
7 query = self.query(self.norm(x)).unsqueeze(2).view(B, T, H, -1)
8 # B, 1, N, D
9 key = self.key(self.text_norm(cond)).unsqueeze(1).view(B, N, H,
-1)
10
11 # B, T, N, H
12 attention = torch.einsum(’bnhd,bmhd->bnmh’, query, key) / math.
sqrt(D // H)
13 weight = self.dropout(F.softmax(attention, dim=2))
14
15 # replacing the attention map directly
16 if self.step <= manipulation_steps_end:
17 weight[1, :, :, :] = weight[0, :, :, :]
18
19 value = self.value(self.text_norm(cond)).view(B, N, H, -1)
20 y = torch.einsum(’bnmh,bmhd->bnhd’, weight, value).reshape(B, T, D
)
21 return y
Code2: Pseudocodesforin-placemotionreplacement.
37Motion sequence shifting. Motion sequence shifting aims to correct the atomic motion in the
temporal order you want. We only need to shift the temporal order of Qs, Ks, and Vs in the
self-attentiontoobtaintheshiftedresult. KeycodesareshownintheL13-24andL32-36ofCode3.
1 def forward(self, x, cond, time_shift_steps_end, time_shift_ratio):
2 B, T, D = x.shape
3 H = self.num_head
4
5 # B, T, 1, D
6 query = self.query(self.norm(x)).unsqueeze(2)
7 # B, 1, T, D
8 key = self.key(self.norm(x)).unsqueeze(1)
9 query = query.view(B, T, H, -1)
10 key = key.view(B, N, H, -1)
11
12 # shifting queries and keys
13 if self.step <= time_shift_steps_end:
14 part1 = int(key.shape[1] * time_shift_ratio)
15 part2 = int(key.shape[1] * (1 - time_shift_ratio))
16 q_front_part = query[0, :part1, :, :]
17 q_back_part = query[0, -part2:, :, :]
18 new_q = torch.cat((q_back_part, q_front_part), dim=0)
19 query[1] = new_q
20
21 k_front_part = key[0, :part1, :, :]
22 k_back_part = key[0, -part2:, :, :]
23 new_k = torch.cat((k_back_part, k_front_part), dim=0)
24 key[1] = new_k
25
26 # B, T, N, H
27 attention = torch.einsum(’bnhd,bmhd->bnmh’, query, key) / math.
sqrt(D // H)
28 weight = self.dropout(F.softmax(attention, dim=2))
29 value = self.value(self.text_norm(cond)).view(B, T, H, -1)
30
31 # shifting values
32 if self.step <= time_shift_steps_end:
33 v_front_part = value[0, :part1, :, :]
34 v_back_part = value[0, -part2:, :, :]
35 new_v = torch.cat((v_back_part, v_front_part), dim=0)
36 value[1] = new_v
37 y = torch.einsum(’bnmh,bmhd->bnhd’, weight, value).reshape(B, T, D
)
38 return y
Code3: Pseudocodesformotionsequenceshifting.
38Example-basedmotiongeneration. Togeneratediversemotionsdrivenbythesameexample,we
onlyneedtoshuffletheorderofqueriesinself-attention,whichisshowninL13-23ofCode4.
1 def forward(self, x, cond, steps_end, _seed, chunk_size, seed_bar):
2 B, T, D = x.shape
3 H = self.num_head
4
5 # B, T, 1, D
6 query = self.query(self.norm(x)).unsqueeze(2)
7 # B, 1, T, D
8 key = self.key(self.norm(x)).unsqueeze(1)
9 query = query.view(B, T, H, -1)
10 key = key.view(B, N, H, -1)
11
12 # shuffling queries
13 if self.step == steps_end:
14 for id_ in range(query.shape[0]-1):
15 with torch.random.fork_rng():
16 torch.manual_seed(_seed)
17 tensor = query[0]
18 chunks = torch.split(tensor, chunk_size, dim=0)
19 shuffled_index = torch.randperm(len(chunks))
20 shuffled_chunks = [chunks[i] for i in shuffled_index]
21 shuffled_tensor = torch.cat(shuffled_chunks, dim=0)
22 query[1+id_] = shuffled_tensor
23 _seed += seed_bar
24
25 # B, T, T, H
26 attention = torch.einsum(’bnhd,bmhd->bnmh’, query, key) / math.
sqrt(D // H)
27 weight = self.dropout(F.softmax(attention, dim=2))
28 value = self.value(self.text_norm(cond)).view(B, N, H, -1)
29 y = torch.einsum(’bnmh,bmhd->bnhd’, weight, value).reshape(B, T, D
)
30 return y
Code4: Pseudocodesforexample-basedmotiongeneration.
Motionstyletransfer. Inthegenerationoftwomotions(B=2),weonlyneedtoreplacethequeryof
thesecondmotionwiththefirstone,whichisshowninL13-14ofCode5.
1 def forward(self, x, cond, steps_end):
2 B, T, D = x.shape
3 H = self.num_head
4
5 # B, T, 1, D
6 query = self.query(self.norm(x)).unsqueeze(2)
7 # B, 1, T, D
8 key = self.key(self.norm(x)).unsqueeze(1)
9 query = query.view(B, T, H, -1)
10 key = key.view(B, N, H, -1)
11
12 # style transfer
13 if self.step <= self.steps_end:
14 query[1] = query[0]
15
16 # B, T, T, H
17 attention = torch.einsum(’bnhd,bmhd->bnmh’, query, key) / math.
sqrt(D // H)
18 weight = self.dropout(F.softmax(attention, dim=2))
19 value = self.value(self.text_norm(cond)).view(B, N, H, -1)
20 y = torch.einsum(’bnmh,bmhd->bnhd’, weight, value).reshape(B, T, D
)
21 return y
Code5: Pseudocodesformotionstyletransfer.
39F.2 SupplementaryforMotionStyleTransfer
Asdiscussedinthemaintext,motionstyletransferisaccomplishedbyreplacingthequery(Q)from
thecontentsequence(M )withthatfromthestylesequence(M ). Thisreplacementensuresthat
2 1
whilethecontentfeaturesfromM arepreserved,thestylefeaturesfromM areadopted,resulting
2 1
inasynthesizedmotionsequencethatcapturesthestyleofM withthecontentofM .
1 2
Out Out Out
Map Map Map
Q K V Q K V Q K V
(a) Direct generating (b) Direct generating (c) Generating
style reference. content reference. transfered result.
Figure28: Theillustrationofmotionstyletransferprocess. (a)Directgeneratingstylereference: The
styleinformationisgenerateddirectlyusingthequery(Q),key(K),andvalue(V)fromthestyle
referencemotionsequence(blue). (b)Directgeneratingcontentreference: Thecontentinformation
isgenerateddirectlyfromthecontentreferencemotionsequence(orange). (c)Generatingtransferred
result: Thefinaltransferredmotionsequencecombinesthestylefromthestylereferencesequence
withthecontentfromthecontentreferencesequence,usingQfromthestylereference(blue)andK,
Vfromthecontentreference(orange).
Fig.28providesavisualexplanationofthisprocess. Theself-attentionmechanismplaysacrucial
role,wheretheattentionmapdeterminesthecorrespondencebetweenthestyleandcontentfeatures.
The pseudo code snippet provided in Code 5 exemplifies this process. By setting “query[1] =
query[0]”inthecode,thequeryforthesecondmotion(M )isreplacedbythatofthefirstmotion
2
(M ),whicheffectivelytransfersthemotionstylefromM toM . Insummary,thismotionstyle
1 2 1
transfer method allows one motion sequence to adopt the style characteristics of another while
maintainingitsowncontent.
40G DetailsofActionCountinginaMotion
ThedetailedprocessofactioncountingisdescribedinCode6. Theattentionmapisfirstsmoothed
usingaGaussianfiltertoeliminatenoise,ensuringthatminorfluctuationsdonotaffectpeakdetection.
We then downsample the smoothed matrix to reduce computational complexity and normalize it
withina0-1rangeforconsistentpeakdetectionacrossdifferentmotions.
Thepseudocodeprovideddemonstratesthecompleteprocess,includingpeakdetectionusingheight
anddistancethresholds. Theexperimentalresultsindicatethatthisapproachismorereliableand
lesssensitivetonoisecomparedtousingtheroottrajectory,thusconfirmingtheeffectivenessofour
methodinaccuratelycountingactionswithinageneratedmotionsequence.
1 """
2 Input: matrix (the attention map array with shape (T, T))
3 Output: float (counting number)
4 """
5
6 # Apply Gaussian smoothing via gaussian_filter in scipy.ndimage
7 smoothed_matrix = gaussian_filter(matrix, sigma=0.8)
8
9 # Attention map down-sampling
10 downsample_factor = 4
11 smoothed_matrix = downsample_matrix(smoothed_matrix, downsample_factor
)
12
13 # Normalize the matrix to 0-1 range
14 normalized_matrix = normalize_matrix(smoothed_matrix)
15
16 # Detect peaks with specified height and distance thresholds
17 height_threshold = normalized_matrix.mean() * 3 # you can adjust this
18 distance_threshold = 1 # you can adjust this
19 peaks_positions_per_row = detect_peaks_in_matrix(normalized_matrix,
height=height_threshold, distance=distance_threshold)
20
21 # Display the peaks positions per row
22 total_peak = sum([len(i) if len(i) > 0 else 0 for i in
peaks_positions_per_row])
23 sum_ = sum([1 if len(i) > 0 else 0 for i in peaks_positions_per_row])
24
25 return total_peak / sum_
Code6: Pseudocodesforactioncounting.
.
41