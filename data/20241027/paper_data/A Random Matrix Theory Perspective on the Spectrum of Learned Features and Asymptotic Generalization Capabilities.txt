A Random Matrix Theory Perspective on the Spectrum of
Learned Features and Asymptotic Generalization Capabilities
YatinDandi1,2,LucaPesce2,HugoCui1,5,FlorentKrzakala2,YueM.Lu3,andBrunoLoureiro4
1
StatisticalPhysicsOfComputationlaboratory,ÉcolePolytechniqueFédéraledeLausanne(EPFL),1015Lausanne,Switzerland
2InformationLearning&Physicslaboratory,ÉcolePolytechniqueFédéraledeLausanne(EPFL),1015Lausanne,Switzerland
3JohnA.PaulsonSchoolofEngineeringandAppliedSciences,HarvardUniversity
4Départementd’Informatique,ÉcoleNormaleSupérieure(ENS)-PSL&CNRS,F-75230Pariscedex05,France
5CenterofMathematicalSciencesandApplications,HarvardUniversity
Abstract
Akeypropertyofneuralnetworksistheircapacityofadaptingtodataduringtraining.Yet,ourcurrentmathematical
understandingoffeaturelearninganditsrelationshiptogeneralizationremainlimited. Inthiswork,weprovidea
randommatrixanalysisofhowfully-connectedtwo-layerneuralnetworksadapttothetargetfunctionafterasingle,
butaggressive,gradientdescentstep. Werigorouslyestablishtheequivalencebetweentheupdatedfeaturesandan
isotropicspikedrandomfeaturemodel,inthelimitoflargebatchsize.Forthelattermodel,wederiveadeterministic
equivalentdescriptionofthefeatureempiricalcovariancematrixintermsofcertainlow-dimensionaloperators.This
allowsustosharplycharacterizetheimpactoftrainingintheasymptoticfeaturespectrum,andinparticular,provides
atheoreticalgroundingforhowthetailsofthefeaturespectrummodifywithtraining.Thedeterministicequivalent
furtheryieldstheexactasymptoticgeneralizationerror,sheddinglightonthemechanismsbehinditsimprovementin
thepresenceoffeaturelearning.Ourresultgoesbeyondstandardrandommatrixensembles,andthereforewebelieveit
isofindependenttechnicalinterest.Differentfrompreviouswork,ourresultholdsinthechallengingmaximallearning
rateregime,isfullyrigorousandallowsforfinitelysupportedsecondlayerinitialization,whichturnsouttobecrucial
forstudyingthefunctionalexpressivityofthelearnedfeatures.Thisprovidesasharpdescriptionoftheimpactoffeature
learninginthegeneralizationoftwo-layerneuralnetworks,beyondtherandomfeaturesandlazytrainingregimes.
1 Introduction
An essential property of neural networks is their capacity to extract relevant low-dimensional features from high-
dimensionaldata.Thisfeaturelearningisusuallysignaledbyanarrayoftelltalephenomena—suchastheimprovement
ofthetesterrorovernon-adaptivemethods[Bach,2021],orthelengtheningofthetailsinthespectraofthenetwork
weightsandactivations[MartinandMahoney,2021,Martinetal.,2021,Wangetal.,2024]. Yet,aprecisetheoretical
characterizationofthelearnedfeatures,andhowtheytranslateintotheaforementionedgeneralizationandspectral
properties,isstilllargelylacking,andarguablyconstitutesoneofthekeyopenquestionsinmachinelearningtheory.
Inthiswork,weprovidearigorousanswertothesequestionsfortwolayerneuralnetworkstrainedwithasinglebut
largegradientstep.Moreprecisely,
• weprovideanexactcharacterizationofstatisticsassociatedtothelearnedfeatures,andinparticulartheachieved
testerror;
• wequantitativelycharacterizehowfeaturelearningresultsinmodifiedtailsinthespectrumofthefeaturecovariance
matrix,asillustratedinFig.1.
Ourresultsprovideasharpmathematicaldescriptionoffeaturelearninginthiscontextandallowustoexplorethe
interplaybetweenrepresentationallearningandgeneralization.Beforeexposingourmaintechnicalresults,wefirstoffer
anoverviewofknownresultsonfeaturelearning(orthelackthereof)intwo-layerneuralnetworks,soastoputour
workincontext.
1
4202
tcO
42
]LM.tats[
1v83981.0142:viXra2.00
initialization
trained (theory)
1.75 trained (simulations)
1.50
1.25
1.00
0.75
0.50
0.25
0.00
0 1 2 3 4 5
eigenvalue
Figure1: Bulkspectrumoftheempiricalfeaturescovarianceatinitialization(dashedblue)andaftertraining
(green);theredlinecorrespondstothetheoreticalcharacterizationderivedinthismanuscript.
Models— Thepresentmanuscriptaddressesthesimplestclassofneuralnetworkarchitectures(usedinFig.1),namely
fully-connected,shallowtwo-layerneuralnetworks:
p
1 (cid:88)
f(x;W,a)= √ a σ(w⊤x), (1)
p j j
j=1
whereW ={w ,j ∈[p]}∈Rp×d anda={a ,j ∈[p]}∈Rp denotethefirstandsecondlayerweights,respectively,
j j
andσisanactivationfunction.Motivatedbythelazyregimeoflarge-widthnetworks[Jacotetal.,2018,Chizatetal.,
2019],thegeneralizationpropertiesoftwo-layerneuralnetworkshavebeenthoroughlyinvestigatedinthesimplecase
whereonlythesecond-layerweightsaaretrained(typicallybyridgeregression),whilethefirst-layerweightsw =w0
arefixedat(typicallyrandom)initialization.Thismodel,whichisequivalenttotheRandomFeatures(RF)approximation
ofkernelmethodsintroducedbyRahimiandRecht[2007],isparticularlyamenabletomathematicaltreatment. The
reasonisthat,besidesbeingaconvexproblem,intheasymptoticregimewheren,p=Θ(d)withd→∞,therandom
featuremapφ(x)=σ(w⊤x)statisticallybehavesasalinear functionwithadditivenoise—aresultoftenreferredtoas
theGaussianEquivalencePrinciple(GEP)[Goldtetal.,2022,HuandLu,2022,MeiandMontanari,2022]. Whilethis
surprisingpropertymakestheproblemtractablewithrandommatrixtheoryarguments,itimpliesthatinthisregime
randomfeaturescanlearn,atbest,alinearapproximationoftheunderlyingtargetfunction.Thissetsabenchmarkfor
thefundamentallimitationofnotadaptingthefirst-layerweightstothedata.
Gradientdescent— Going(literally)onestepbeyondrandomfeatures,Baetal.[2022]showedthattrainingthefirst
layerweightswithasingleGradientDescent(GD)steponabatchofdata{(x ,y ):µ∈[n ]}fromthesametarget
µ µ 0
distribution:
1 (cid:88)
w1 =w0−η∇ (y −f(x ;a0,w0))2
j j wj2n µ µ
0
µ∈[n0]
followedbyridgeregressiononthesecondlayerweightsawithnfreshsamplescandrasticallychangethestoryabove,
√
dependingonthescalingofthelearningrateηwheren∝d,n ∝d.Moreprecisely,theyshowedthatforη =Θ ( d),
0 d
GaussianEquivalenceasymptoticallyholdsasd→∞,implyingthatanexactasymptotictreatmentbasedontheGEPstill
holds.However,intheregimeofanaggressivelearningrateη =Θ (d)(theso-calledMaximalUpdateparametrization
d
[Yangetal.,2022])theyshowedthatthefirst-layerweightsadapttothedatadistribution,translatingintoanimprovement
overtheRFlower-bound.
Thisfindinghassparkedconsiderableinterestinexactlycharacterizingtheasymptoticgeneralizationerrorachieved
under this single, large step setting. Dandi et al. [2023] proved a novel Conditional Gaussian Equivalence Principle
2(cGEP),andderivedmoregenerallower-boundsfortheperformanceoftwo-layerneuralnetworksafterthegradient
step.However,theseboundsdonotprovideafine-graineddescriptionofwhatislearnedinthefeaturelearningregime.
ReachingsuchasharpdescriptioninsteadrequiresaddressingthechallengingRandomMatrixTheory(RMT)problemof
characterizingthenon-lineartransformationofahighlystructuredrandommatrixσ(W −ηG)withGdenotingthe
gradientmatrix,intheregimeΘ (∥ηG∥ )=Θ (||W|| ).Monirietal.[2023]providedthefirstresultinthisdirection
d F d F
fortheintermediatelearningrateregimeη =Θ d(d1/2+ζ),withζ ∈(0,1/2),provingapolynomialGEPtogetherwithan
exactRMTanalysisoftheasymptoticgeneralizationerror.Theirfindings,however,donotholdinthemorechallenging
MaximalUpdateregimeη =Θ(d).Leveragingtheresultsonthelow-rankapproximationoftheGradientmatrix[Ba
etal.,2022]andthecGEPfrom[Dandietal.,2023],Cuietal.[2024]studiedthelatterregimeapproximatingthetwo-layer
networkbyaSpikedRandomFeaturesModel(SRFM)withthenon-rigorousreplicamethod[Mezardetal.,1986].Though
suchheuristicargumentsareinspirational,theyoftenlackinterpretabilitywhencomparedtootherapproaches.
Summaryofmainresults— Inthiswork,weprovidearigorousRMTtreatmentoffeaturelearningafterasingle
GDstepinthechallengingMaximalUpdatestepsizeregime. BeyondprovingtheconjecturedresultsfromCuietal.
[2024],ouranalysisextendsthesefindingsinseveralkeydirections,enablingaquantitativeexplorationanddeeper
understanding of fundamental aspects of feature learning, particularly with respect to generalization and spectral
properties.Specifically:
• We prove a deterministic equivalent description for the empirical feature matrix after the gradient step. This
characterizationisnon-asymptoticintheproblemdimensions,andinparticularsharplyholdsintheMaximal-
updatescalingintheproportionalregimewheren,p,η = Θ(d)andn = Ω(d1+ϵ)asd → ∞,forsomeϵ > 0.
0
Thisresultcharacterizesthespectralpropertiesaswellasthegeneralizationerroruponupdatingthesecondlayer.
• Ourproofproceedsthroughmultiplestagesofdeterministicequivalencesandtheasymptoticdescriptionofthe
high-dimensionalfeaturesthroughlow-dimensionalnon-linearfunctions.
• Wederiveanexactasymptoticformulaforthegeneralizationerrorofridgeregressiononfeaturesupdatedviaa
gradientstepintheproportionalhigh-dimensionalregime,wheren,p,η =Θ(d)withd→∞.Ourresultoffersa
rigorousproofoftheconjecturesinCuietal.[2024],whileextendingtheminseveraldirections,asitappliesto
finitelysupportedsecond-layerinitializationandstructuredfirst-layerinitialization.
• Usingthedeterministicequivalent,wedemonstratehowthe“spikes”intheweightsresultingfromfeaturelearning
altertheentireshapeofthefeaturecovariancespectrumanditstailbehavior.Thisobservationalignswiththe
empiricalfindingsofWangetal.[2024]andprovidesarigorousfoundationfortheminthecontextofoursetting.
• Finally,wepreciselycharacterizetheeffectofthevariabilityinthesecond-layerinitializationonthefunctional
expressivityofthenetworkafteroneGDstep.
These findings provide a detailed understanding of the consequences of feature learning in our setting, helping to
establishseveralwidelyacceptedintuitionsonamorequantitativeandrigorousbasis.
Furtherrelatedworks
Fixedfeaturemethods– Aplethoraofworkscharacterizedthegeneralizationcapabilitiesoftwo-layernetworks
inthehigh-dimensionalregimewhenthefirsthiddenlayerisnottrained,withthemostprominentexampleofsuch
fixedfeaturemethodbeingkernelmachines[Bordelonetal.,2020,Canataretal.,2021,Cuietal.,2021,2023,Dietrich
etal.,1999,Donhauseretal.,2021,Ghorbanietal.,2019,2020,OpperandUrbanczik,2001,Xiaoetal.,2022].Thisclass
of algorithms is amenable to theoretical analysis and comes with sharp generalization guarantees. However, these
methodsadapttorelevantlow-dimensionalstructuresatmuchhighersamplecomplexitiesthanfullytrainedtwo-layer
networks.Insimpleterms,thesamplecomplexityofkernelmethodsisnotdrivenbythepresence(orlackthereof)of
alow-dimensionaltargetsubspace. IdenticalconsiderationsholdforRandomFeatureModels,wherethenumberof
samplesninthegeneralizationguaranteesisreplacedwithmin(n,p),withpbeingthenumberofrandomfeatures
[Geraceetal.,2020,MeiandMontanari,2022,Meietal.,2022,Huetal.,2024,Aguirre-Lópezetal.,2024].
Featurelearning– Thediscussionaboveportraysthelimitationsoffixedfeaturemethods.Inspiredbythis,alarge
bodyofworkhasstudiedthesharpseparationbetweenthegeneralizationcapabilitiesofsuchmethodsversusfully
trainedtwo-layernetworksthatlearnfeaturesthroughgradient-basedtraining. Manyoftheseworksfallunderthe
umbrellaoftheso-calledmean-fieldregime[ChizatandBach,2018,Meietal.,2018,RotskoffandVanden-Eijnden,2022,
SirignanoandSpiliopoulos,2020].Theauthorsmappedtheoptimizationoftwo-layershallownetworksontoaconvex
3probleminthespaceofmeasuresontheweightsandpavedthewayforunderstandinghowfeaturesarelearnedin
thehigh-dimensionalregime.Thearguablymostpopulardatamodelinthetheoreticalcommunityforaddressingthis
questionisthemulti-indexdatamodelwithisotropicGaussiandata.Thissettinghasattractedconsiderableattentionin
thetheoreticalcommunitywithnumerousworksthathaveanalyzedthefeaturelearningcapabilitiesofshallownetworks
trainedwithgradient-basedschemes[BenArousetal.,2021,Abbeetal.,2023,Baetal.,2024,BardoneandGoldt,2024,
Berthieretal.,2023,Biettietal.,2023,Damianetal.,2024,Dandietal.,2023,Paquetteetal.,2021,Veigaetal.,2022,Zweig
andBruna,2023,Dandietal.,2024].
Deterministicequivalents– Deterministicequivalentsoflargeempiricalcovariancematriceshavebeenextensively
studied,beginningwiththeseminalworkofMarchenkoandPastur[1967].ThiswasextendedbyBurdaetal.[2004],
KnowlesandYin[2017]toseparabledatacovariances,andfurtherbyBaiandZhou[2008],LouartandCouillet[2018],
Chouard[2022]fornon-separablecovariances.Thesemethodologieshaveenabledpreciseasymptoticcharacterizations
ofthelearningdynamicsinsingle-layerneuralnetworks[Louartetal.,2018]anddeeprandomfeaturemodels[Schröder
etal.,2023,2024,Boschetal.,2023].
2 Notations and Setting
ConsiderasupervisedlearningproblemwithtrainingdataD = {(x ,y ) ∈ Rd+1,µ ∈ [N]}. Asmotivatedinthe
µ µ
introduction, ourgoalistostudytheproblemoffeaturelearningwithtwo-layerneuralnetworks definedin (1). A
widespreadintuitioninthemachinelearningliteratureforwhylearningispossibledespitethecurseofdimensionalityis
thatrealdatadistributionstypicallyexhibitlow-dimensionallatentstructures[Bellmanetal.,1957].Toreflectandmodel
thisintuition,weassumeourtrainingdatahavebeenindependentlydrawnfromanisotropicGaussiansingle-index
model:
y =f (x )=g(x⊤w⋆), x ∼N(0,I ), (2)
µ ⋆ µ µ µ d
wheretheunitnormvectorw⋆denotesthetargetweightsandg :R→Risthelinkfunction.Werefertotheappendixfor
theextensiontomoregeneralstochasticmapping.Notethatin2,thehigh-dimensionalcovariatesX =(x ) ∈Rn×d
µ µ∈[n]
areisotropicinRd,andthereforethestructureinthedatadistributionisintheconditionaldistributionofthelabelsy|x,
whichdependsonthecovariatesonlythroughtheirprojectionontoa1-dimensionalsubspaceofRd.Therefore,learning
featuresinthismodeltranslatetolearningthetargetweightw⋆.
GivenabatchofN samplesD = {(x ,y ),µ∈[N]}independentlydrawnfrommodel(2),weareinterestedin
µ µ
studyinghowourtwo-layerneuralnetwork(1)learnsthetargetfeaturew⋆throughtheEmpiricalRiskMinimization
(ERM)onthetrainingdata.Wefollowthesameprocedureasin[Baetal.,2022,Dandietal.,2023,Monirietal.,2023,Cui
etal.,2024]andconsiderthefollowingtwo-steptrainingprocedure:
1. LetW0anda0denotethefirstandsecondlayerweightsatinitialization.Considerapartitionofthetrainingdata
D =D ∪D intwodisjointsetsofsizen andn:=N −n ,respectively.First,weapplyasinglegradientstep
0 1 0 0
onthesquarelossforthefirst-layerweights,keepingthe2ndlayera0fixed:
w1=w0−ηg0
j j j
g0 =
1
√
(cid:88)(cid:16)
f(x ;W0,a0)−y
(cid:17)
a0x σ′(w0⊤ x )
j n p µ µ j µ j µ
0
µ∈[n0]
Inthisfirstrepresentationlearningstep,thehiddenlayerweightsadapttothelow-dimensionalrelevantfeatures
fromthedata.
2. GiventheupdatedweightsW1,weupdatethesecond-layerweightsviaridgeregressionontheremainingdata
D :
1
(cid:88) (cid:16) (cid:17)2
aˆ =argmin y −f(x ;a,W1) +λ||a||2
λ µ µ 2
a∈Rp
µ∈[n]
(cid:16) (cid:17)−1
= Φ⊤Φ/p+λI n Φ⊤y/√ p (3)
wherewedefinedthefeaturematrixΦ∈Rn×pwithelementsϕ =σ(x⊤w1)andthelabelvectory =(y ) .
µj µ j µ µ∈[n]
Inthefollowing,wewillassumethefollowinginitialconditionsforthetrainingprotocolabove:
4Assumption 2.1 (Initialization). We assume the first layer weights are initialized uniformly at random from the
hyperspherew j0 ∼ Unif(Sd−1(1))andthesecond-layerweightsreada0
j
= a˜0 j/√ p,wherethe{a˜0 j,j ∈ [p]}areO d(1)
scalarsinitializedi.i.d.bysamplingfromadimension-independentvocabularyofsizek,withprobabilitiesπ =(π ) .
q q∈[k]
Our goal in the following is two-fold. First, to characterize the properties of the empirical feature matrix Φ⊤Φ.
Second,tocharacterizethegeneralizationerrorassociatedwiththeminimizerofequation(3),whichisdefinedas:
(cid:20)(cid:16) (cid:17)2(cid:21)
ε =E E y −f(x ;W1,aˆ )
gen D1 ynew,xnew new new λ
where the expectation is over the joint distribution defined by the model in 2. In particular, we will focus on the
proportionalhigh-dimensionalregimewithMaximalUpdatescaling,whichweformalizeinthefollowingassumption.
Assumption 2.2 (High-dimensional regime). We assume that n = Ω(d1+ϵ) for some ϵ > 0. We work under the
0
proportionalregimewithMaximalUpdatescaling,definedasthelimitwheren,p,η,d→∞atfixedratios:
n p η
α:= , β := , η˜:=
d d d
Assumption 2.3 (Activation function). σ is odd, uniformly Lipschitz such that σ′′,σ′′′ exist almost surely and are
boundedinabsolutevaluebysomeconstantC almostsurelywithrespecttotheLebesguemeasure.Furthermore,gis
uniformlyboundedandLipschitzwithE (cid:2) g(z)(cid:3) =0andE (cid:2) g′(z)(cid:3) ̸=0.
z∼N(0,1) z∼N(0,1)
3 Asymptotics of the First Gradient Step
Weintroduceinthissectionourfirstresultthatestablishesarigorousframeworkforstudyingtheexactasymptotics
afteroneGDstep.Thisquestionhasbeenthesubjectofintensetheoreticalscrutinyinrecentyears.First,Baetal.[2022]
provedthatinthehigh-dimensionalregime,specifiedinAssumption2.2,thehiddenlayerweightsafteroneGDstepare
approximatelylowrank:
W1 =W0+uv⊤+∆ (4)
where the spiked structure is identified by: i) u = ηc1c⋆ 1a0/√ p is proportional to the second layer at initialization
a0,thelearningrateη,andthefirstHermitecoefficientsofthenetworkactivationσ andthetargetactivationg,i.e,
c = E (cid:2) σ(ξ)ξ(cid:3), c⋆ = E (cid:2) g(ξ)ξ(cid:3); ii)v liesalongthefirstHermitecoefficientofthetargetf (·), i.e.,
1 ξ∼N(0,1) 1 ξ∼N(0,1) ⋆
v ∝(cid:80) y x .Thespikecomponentviscorrelatedwiththetargetvectorw⋆ineq.(2).Itispreciselythepresence
µ∈[n] µ µ
ofsuchcorrelatedcomponentsthatenablesthetrainedmodeltosurpasstheRandomFeaturesperformanceDamianetal.
[2022],Abbeetal.[2022,2023],Dandietal.[2023]. The“noise”term∆,arisesfromhigher-ordercomponentsofthe
activations(seedetailsinF.2)
The decomposition in eq. (4) underlies the analysis of Ba et al. [2022], Moniri et al. [2023], Cui et al. [2024] in the
propotionalregimep∝d,p∝n.Heuristically,onecouldhopetoanalyzethetrainedweightsbymappingtheproblem
toaSpikedRandomFeatureModel(SRFM),wheretheweightsF inafeaturemapσ(Fx)—canbedecomposedasthe
sumofarandombulk(F )andaspike:
0
F =F +uv⊤
0
Alongtheselines,Cuietal.[2024]approximatethenoiseterm∆ineq.(4)asanisotropicGaussianmatrixtoreach
anasymptoticdescriptionoftheequivalentSRFMmodelusingnon-rigoroustoolsfromStatisticalPhysics.Ithowever
remainedunclearwhethertheuniform/GaussianisotropicdescriptionofW0+∆ineq.(4)isanaccurateapproximation
ofthebulkintheactualGDstep.Weofferarigorousanswertothevalidityoftheapproximation.
Anisotropicbulkcovariance– Weprovidetheasymptoticdescriptionofthecovarianceforthebulkweightsin
eq.(4)afteroneGDstep. Forn0/d = Θ(1),weunveilthepresenceofanisotropic componentsthatcontrastwiththe
uniformapproximationconsideredinCuietal.[2024],whichcorrespondstotakingadiagonalapproximationofthe
covariance.WerefertoAppendixFforformalresultsandadditionalinvestigations.
5Diagonalapproximationregime– Ontheotherhand,weprovablyshowthatthediagonalapproximationconsidered
inCuietal.[2024]forthecovarianceofthebulkweightsisvalidassoonasthenumberofsamplesn usedintheGD
0
stepissufficientlylarge(n =Θ(d1+ϵ)foranyϵ>0).Inthisregime,thespikevinEquation4canbefurtherreplaced
0
bythesignalw∗:
Lemma3.1. LetW(1) ∈Rp×ddenotetheweightmatrixafterthefirstgradientstep.Then,underAssumptions2.1,2.2,and
2.3:
(cid:13) (cid:13)
(cid:13) (cid:13)W(1)−(cid:16) W(0)+u(w⋆)⊤(cid:17)(cid:13)
(cid:13)
−−a −.s
→0.
(cid:13) (cid:13) d→∞
2
wherew⋆isthetargetvectorineq.(2),andu=ηc1c⋆ 1a0/√ pasdefinedineq.(4),withηbeingthelearningrate.
Fromeq.(4),weseethatthefinitesupportassumptionforthesecondlayer(2.1)translatestofinitesupportofthe
entriesu andwedenotewithA = {ζu,...,ζu}itsvocabularywiththecorrespondingprobabilitiesπ = (π ) .
i u 1 k q q∈[k]
TheaboveLemmarigorouslycharacterizestheregimesinwhichtheisotropicSRFMapproximationofCuietal.[2024]is
justifiedandcorrectlydescribesthenetworkafteroneGDstep.
4 Main Results
Wearenowinthepositiontostateourmaintechnicalresults.Namely,arigorouscharacterizationoftheempiricalfeature
matrixafteronegradientstepthroughadeterministicequivalent description.Thispictureenablesthecharacterizationof
thefeaturesspectrumandtheresultingasymptoticgeneralizationerror.
Extendedfeaturesandresolvent– Asiswellestablishedinrandommatrixtheory,theresolventG(z)=(1Φ⊤Φ−
p
zI)−1,whereΦ=σ(X(W1)⊤),allowstheextractionofalargeclassofsummarystatisticsrelatedtothespectrumofΦ
[BaiandZhou,2008,Andersonetal.,2010].Toadditionallycharacterizethegeneralizationerror,andcapturethemean
dependenceof{ϕ ,µ∈[n]}onthespikecomponentsκ d =ef x⊤w⋆,weconstructanaugmentedversionofG(z).We
µ µ µ
findthattherelevantstatisticsinoursetuparecapturedbytheresolventofcertainextended featuresthatweintroduce
below:
Definition4.1(Extendedresolvent). Let(X,y)denoteabatchofdatadrawnfromtheGaussiansingle-indexmodelin
2,andconsiderthefeaturematrixΦ=σ(X(W1)⊤)afterthefirstgradientstep(Eq.1),withκ =x⊤w⋆.Lets denote
µ µ q
thesubsetofcoordinatessuchthatu = ζu andthe“mean”ϕ¯q = 1 (cid:80) Φ . Wedefinetheextendedfeatures
j q µ |sq| j∈sq µ,j
ϕe ∈R(p+k+1)andtheextendedresolvent G (z)∈R(p+k+1)×(p+k+1)for,z ∈C/R+as:
µ e
 
y (cid:32) (cid:33)−1
ϕe
=ϕ¯µ
, G (z)d =ef
(Φe)⊤Φe
−zI (5)
µ  ϕ˜µ e p
µ
whereϕ˜ =ϕ −ϕ¯ ,Φe ={ϕe ∈Rp+k+1,µ∈[n]}
µ,j µ,j j µ
Afewcommentsaboutthedefinitionoftheextendedfeaturesextendedfeaturesϕe andtheresolventG (z)in4.1are
µ e
inplace.First,duetotheextensivespikeandthefinitesupportoverubyAssumption2.1,eachsubsets possessesa
q
non-zeromeanϕ¯q = 1 (cid:80) Φ ,asymptoticallyconvergingtoc (κ ,ζu)definedbelow.
µ |sq| j∈sq µj 0 µ q
Definition4.2(ShiftedHermitecoefficient). WedefinetheshiftedHermitecoefficientc (κ,ζ)oftheactivationσ(·)
ℓ
c (κ,ζ)=E [σ(z+κζ)h (z)]
ℓ z∼N(0,1) ℓ
where(h ) denotetheHermitepolynomials.
ℓ ℓ>0
Therefore,unliketypicalrandom-matrixensembles,themeansϕ¯q haveO(1)fluctuationsduetoalargedependence
µ
onκ.Notethatthemeansϕ¯q canbeequivalentlydescribedastheprojectionsofϕ alongthedirections(e1,··· ,ek)
µ µ
definedas:
(cid:40)
1 1 ifu =ζu
eq = √ j q , j ∈[p], q ∈[k]. (6)
j p 0 otherwise
6Bydecomposingϕ as(cid:80)k ϕ¯qeq+ϕ˜ ,werealizethatthefirsttermvariesonlyalongak-dimensionalsubspacewith
µ q=1 µ µ
variationsgovernedbyκ ,whilethesecondtermcontributestothe“bulk"ofthefeaturecovariance.Thesurrogateform
µ
ϕe splitsthefeaturesintoϕ¯ ,ϕ˜ preciselytoaccountforthesedifferentscalesoffluctuations.Toexcludedegeneracies
µ µ µ
arisingfromtheleading-ordercontributionsfromϕ¯,weintroducethefollowingadditionalassumption.
Assumption4.3(non-degeneracy). Theactivationσandthevocabularyoveru,A ={ζu,··· ,ζu}aresuchthatthe
u 1 k
setofvectors[c (κ,ζu),···c (κ,ζu)]spanRk asκvariesoverR.
1 1 1 k
DeterministicEquivalent– TheextendedresolventG (z)isahigh-dimensionalrandommatrix, inheritingthe
e
randomnessfromthetrainingdata(X,y)andtheinitializationweightsW0.ToreachthedeterministicequivalentG
e
fortheaboveextendedresolventG (z),ourproofproceedsbysubsequentlyaddressingandremovingtherandomness
e
overthedataX,andtheweightsW0,eventuallyobtaininganequivalentdependentonlyonthecoefficientsu and
i
theprojectionsofW0 onw⋆ denotedasθ := W0w⋆ ∈ Rp.This“special"dependenceonu ,θ isexpected,sinceby
i i
Lemma3.1,thesedeterminethecomponentalongthespikeintheupdatedmatrixW1,whiletheremainingdirectionsin
theweightsmaintainisotropicdependenceandareaveragedout. Theresultingdescriptionischaracterizedthrough
low-dimensionalkernelsandfunctions. Concretely, weshowthatforalargeclassoffunctionsF associatedtothe
feature matrix covariance Φ⊤Φ and labels y ∈ Rn, with entries {y }n , F(Φ⊤Φ,y) −a −. →s F⋆(θ,u) In contrast to
µ µ=1
thehighdimensionalmatrices(X,W1),F⋆(θ,u)dependsonlyonsequencesofscalars(θ,u),turningF⋆(θ,u)into
finite-dimensionalexpectations.
AsstatedinAssumption2.1,weconsiderafinitelysupportedsecondlayer,leadingtotheentriesofubeingsupported
onfinitely-manyvaluesA ={ζu,··· ,ζu}withprobabilitiesπ =(π ) .FromLemma3.1andequation4,wesee
u 1 k q q∈[k]
thatneuronswithidenticalvaluesofu containidenticalcontributionsfromthespike.Thisleadstothedeterministic
i
equivalentG oftheextendedresolventinheritingablockstructure,withblockscorrespondingtodifferentvaluesofu .
e i
Letp ,··· ,p denotethenumberofneuronswithu takingvaluesζu,··· ,ζurespectively.Then,bythestronglawof
1 k i 1 k
largenumbers pq −a −. →s π asp→∞.Withoutlossofgenerality,weassumethattheneuronsarearrangedsuchthat:
p q
[u ,··· ,u ]=[ζu1 ,ζu1 ,...ζu1 ]
1 p 1 1×p1 2 1×p2 k 1×pk
Tocompactlyexpressthisblockstructure,weintroduceanotationforblock-structuredmatricesandvectors:
Definition4.4. Letp ,···p bethesequencedefinedabovewith pq −a −. →s π . LetC ∈ Rk×k beafixedmatrix. We
1 k p q
definetheextendedmatrixC as:
e
 
C 1 , ··· ··· C 1
11 p1×p1 1k p1×pk
C 1 , C 1 , ··· C 1 
C
e
=

21 p
.
.2×p1 22 p
.
.2×p2
. .
2k p
.
.2×pk ,
. . . .
Wearenowreadytostatethedefinitionoftheextendeddeterministicequivalent:
Definition4.5(Deterministicequivalent). LetC+,C−denotethesetofcomplexnumberswithpositiveandnegative
imaginarypartsrespectively.Supposethatz ∈C/R+.LetV⋆ ∈Ck×k,ν⋆ ∈Ck,b⋆ ∈Ck beuniquelydefinedthrough
thefollowingconditions:
(i) V⋆,ν⋆,b⋆satisfythefollowingsetofself-consistentequations:
(cid:34) (cid:35)
c (κ,ζu)c (κ,ζu)
V⋆ (z)=E α 1 q 1 q′
qq′ κ 1+χ(z;κ)
 
ν q⋆(z)=E
κ(cid:88) 1α +c2 ℓ( χκ (, zζ ;qu κ)
)
ℓ≥2
b⋆(z)=π β(cid:0) L (z)+(diag(ν⋆(z))−zI )) (cid:1)−1 ,
q q q,q k q,q
whereκ∼N(0,1),(c (κ,ζ)) aredefinedin4.2and(χ(z;κ),L(z))readasfollows:
ℓ ℓ>0
(cid:88) (cid:88) (cid:88)
βχ(z;κ)= ψ c (κ,ζu)c (κ,ζu)+ b⋆ c2(κ,ζu),
qq′ 1 q 1 q′ q ℓ q
q,q′∈[k] q∈[k] ℓ≥2
(cid:16) (cid:17)−1
L(z)= V⋆(z)−1+diag(b⋆(z)) ,
7whereψ(z)∈Rk×k isdefinedas:
ψ(z)=b⋆(z)−L(z)⊙(b⋆(z)(b⋆(z))⊤),
(ii) V⋆,ν⋆,b⋆areanalyticmappingssatisfyingV⋆ :C+ →C−fori,j ∈[k],ν⋆ :C+ →C−fori∈[k],b⋆ :C+ →C+
i,j i i
(cid:12) (cid:12)
fori∈[k].Forz ∈C+withimaginarypartζ >0,(cid:12)b⋆(z)(cid:12)≤ πq ∀q ∈[p]
(cid:12) q (cid:12) βζ
Wedefinethedeterministicequivalentextendedresolvent G (z)∈R(p+1)×(p+1)as:
e
(cid:34) (cid:35)−1
A∗ −zI (A∗ )⊤⊙θ⊤
G (z)= 11 k 21 ,
e θ⊙A∗ A∗ +αS∗⊙θθ⊤
21 22 e
whereθ =Ww⋆ ∈Rp,andS⋆ ∈Rp×p,A∗ ∈R(k+1)×(k+1),A∗ ∈Rp×(k+1)aredefinedas:
e 11 21
(cid:34) (cid:35)
c (κ,ζu)c (κ,ζu)
S∗ =E (κ2−1) 1 i 1 j
κ 1+χ(z;κ)
(cid:20) (cid:21)
α
A∗ = E ι ι⊤ ,
11 κ 1+χ(z;κ)
(cid:20) (cid:21)
c (κ,u )
A∗ [j,:]=αE 1 j κι⊤ ,∀j ∈[p]
21 κ 1+χ(z;κ)
(cid:32) (cid:18) π (cid:19) b⋆ b⋆ b⋆ (cid:33)
A∗ = diag −diag( )(V−1+diag( ))−1⊙θθ⊤diag( ) ,
22 βb⋆ πβ ⋆ πβ e πβ
e
where κ ∼ N(0,1),ι = (g(κ),c (κ,ζu),···c (κ,ζu))⊤ and the subscript e in S⋆, A⋆ refers to the block matrix
0 1 0 k e 22
notationinDefinition4.4.
Wearenowinapositiontostateourmainresult,whichstatesthatG (z)approximatesG (z)for“typical"linear
e e
functionals.
Theorem4.6(Deterministicequivalent). ConsidertheextendedresolventG (z)(4.1)associatedtoabatchoftrainingdata
e
(X,y)2afteronegradientstep.LetG (z)denotethedeterministicequivalent(4.5).Then,underAssumptions2.1,2.2,2.3
e
and4.3,withneuronsarrangedasEquation4,foranyz ∈C/R+andsequenceofdeterministicmatricesA∈C(p+1)×(p+1)
(cid:16) (cid:17)
with∥A∥ =tr (AA∗)1/2 uniformlyboundedind:
tr
a.s
Tr(AG (z))−−−→Tr(AG (z)).
e e
d→∞
TheclassoflinearfunctionalsAcharacterizedaboveincludesweightedtracesaswellaslow-rankprojectionsRubio
andMestre[2011].AdirectconsequenceofTheorem4.6isthatityieldstheStieltjestransformofthebulkcovariance:
Corollary4.7(Stieltjestransform). Letµ ddenotetheempiricalspectralmeasureofthebulkcovarianceΦ˜⊤Φ˜/p.Letm d(z)
denotetheStieltjestransformm (z)=(cid:82) 1 dµ (λ).Letb⋆(z)beasdefinedin4.5,then:
d λ−z d
k
m (z)−−a −.s →β(cid:88) b⋆(z).
d q
d→∞
q=1
Furthermore,asdiscussedpreviously,thedeterministicequivalentG (z)containsallthenecessarysummarystatistics
e
forafullasymptoticcharacterizationofthegeneralizationerror.Thisistheobjectiveofthefollowingtheorem.
Theorem4.8(GeneralizationError). Undertheproportionalasymptotics2.2,thegeneralizationerror2isgivenbythe
followinglow-dimensional,deterministicformula:
(cid:104) (cid:105)
lim E[ε ]=E Λ ({τ ,τ } ,τ ,τ )
gen κ κ 0,q 1,q q∈[k] 2 3
n,d,p→∞
where{τ ,τ ,,q ∈ [k]},τ ,τ arecertainscalardeterministicfunctionsofV⋆(−λ),ν⋆(−λ)reportedinAppendixE
0,q 1,q 2 3
alongwiththepreciseexpressionofthefunctionΛ (·).
κ
8k=1 Theory
0.6 k=1 Simulation
k=2 Theory
k=2 Simulation
k=4 Theory
k=4 Simulation
0.5
0.4
0.3
0.2
0.5 1.0 1.5 2.0 2.5
= # Samples / # Features
Figure2:Increasefittingaccuracythroughsecondlayervariability:Illustrationofthebenefitsoflargersupport
forthe2nd layervaluesσ = ReLu,σ = tanh. Theoretical(continuouslines)andnumerical(dots)predictionsfor
⋆
the generalization error as a function of the number of samples per dimension α for different values of the second
layervocabularysizek ∈(1,2,4). Thenumericalsimulationsareaveragedover5seedsandfixedhyper-parameters
λ=0.01,γ =0.5,β =1.5,p=2048.Notethesignificantdropinthegeneralizationerrorfork >1.Thechoiceofthe
probabilitiesπ ={π } andthevocabularyζ ={ζ } forthenumericalillustrationare:a)k=1:π ={1},ζ =
q q∈[k] q q∈[k]
{1};b)k=2:π ={0.9,0.1},ζ ={1,−1};c)k=4:π ={0.7,0.1,0.1,0.1},ζ ={1,−0.5,1.5−2}
IntepretationandproofSketch– Unlikethehigh-dimensionalinteractionsinthetruefeaturecovarianceΦ⊤Φ,the
interactionsbetweenneuronsi,jinG (z)dependonlyonthescalarsu ,θ.G (z)thereforereflectsthestructureofanon-
e i e
linearlow-dimensionalKernelonu ,θ.Notethatthedimensionsoftheorder-parametersV⋆ ∈Ck×k,ν⋆ ∈Ck,b⋆ ∈Ck
i
growswiththesupportsizek.Inthecontinuoussupportlimitk →∞,weexpectV⋆andν⋆,b⋆toconvergetocertain
limitingKernelsandfunctionsrespectively,satisfyingfunctionalfixedpointequations.Theprecisecharacterizationof
thisregimeconstitutesaninterestingavenueforfutureresearch.
Asmentionedearlier,ourproofproceedsthroughtwostagesofdeterministicequivalent,successivelyeliminating
therandomnessoverX andW0respectively.AcrucialaspectofouranalysisistodecoupletherandomnessofX,W0
alongthespikew∗ andtheorthogonalsubspace–thisisduetothefactthatthedependenceofthefeaturesϕ on
µ
κ =x⊤w⋆ isofalargerorderthanonthecomponentsofx intheorthogonalspace. Conditionedonκ ,weshow
µ µ µ µ
thatthecovarianceofϕ canbewell-approximatedthroughanequivalentlinearmodel.However,duetothevariability
µ
inκ,thedescriptionoftheresolventdoesnotreducetoastandardrandommatrixtheoryensemble.Lastly,weobtain
thegeneralizationerrorthroughtheintroductionofcertainperturbationtermsintothedeterministicequivalent,with
theresolventactingasa“generatingfunction"foradditionalrelevantstatistics.Thedetailedproofsareprovidedinthe
Appendices.
5 Consequences of the Main Results
Tightcharacterizationofthefeaturecovariancespectrum– Thesingularvaluesofthefeaturesafterone,but
non-maximalgradientstep–η ≍pζ+1/2withζ <1/2–havebeencharacterizedinMonirietal.[2023],showinghow
aseriesofℓspikesappearafterthestep,inadditiontotheRFbulkcorrespondingtothefeaturesatinitialization,as
characterizedine.g.[PenningtonandWorah,2017,BenigniandPéché,2021,2022,FanandWang,2020,Louartetal.,
2018].Thenumberofspikesisgivenbytheintegerℓsuchthatℓ−1/2ℓ<ζ <ℓ/2ℓ+2.Notethatinthemaximalstepsize
limitζ =1/2,thecorrespondingℓdiverges,anditislargelyunclearhowthebulkandspikesrecombineintothelimiting
featurescovariancespectrum.
Ourresultsfurtherprovideatightasymptoticcharacterizationofthebulkspectrum(representedinredinFig.1for
k =1,α=0.8,σ =ReLu,g =sin,η˜=3.3).NotehowthisbulkismodifiedfromtheunspikedRFspectrum(thedashed
9
rorrE
noitazilareneGbluelineinFig.1),displayinginparticularawidersupportandlongertailsinthisparticularinstance.Thespectrumalso
exhibitsoutlyingeigenvaluesoforderΘ(d),arisingfromthemeansϕ¯ ,notrepresentedinFig.1forreadability.This
µ
theoreticalresulttiesinwithnumerouspreviousempiricalobservations[MartinandMahoney,2021,Martinetal.,2021,
Wangetal.,2024]thatheaviertailscanemergeafterfeaturelearning,abehaviourwhichfurthertendstocorrelatewith
bettergeneralizationabilities.Interestingly,thisphenomenonpersistsevenwhenthenetworkistrainedwithmultiple
largestochasticGDsteps,orwithadaptiveoptimizerssuchasAdam[Kingma,2014],asempiricallyobservedbyWang
etal.[2024].
Onaqualitativelevel,thedepartureofthebulkfromitsuntrainedshapecanbeintuitivelyseenasaresultofthe
recombinationbetweentheuntrainedbulkandsomeofthespikespredictedbyMonirietal.[2023],astheyproliferate
whenζ →1/2.Westressthatinadditiontothesespikes,thefeaturecovariancespectrumincludeskadditionalspikes
relatedtothenon-zeromeanpropertyofthefeaturesΦ. Such“spuriousspikes”arenotillustratedinFig.1andare
presentduetothefinitesupportassumptionforthesecondlayer(Assumption2.1).Itisaninterestingavenueoffuture
researchtostudytherecombinationofsuchspuriousspikeswiththebulkinthecontinuouslysupportedsecondlayer
limit(k →∞).
Precisecharacterizationofthelearnedfeatures– Whiletwo-layerneuralnetworksareknowntobeuniversal
approximators[Cybenko,1989,Horniketal.,1989],aprecisecharacterizationoftheapproximationspacespannedby
agiventrainedneuralnetworkfeaturemapsremainstoalargeextentelusive. Aswediscussinthisparagraph,for
featuresresultingfromamaximalupdateonthefirstlayerweights,thediversityofthesecond-layerinitializationa0,
namelythenumberofdifferentvaluesitscomponentstake,playsacrucialroleinallowingforexpressivefeaturemaps.
Indeed,thereisanetincreaseoftheexpressivityoftheneuralnetworkforlargervocabularysizesk.Moreprecisely,
thenetworkisabletoexpressnon-linearfunctionsinκ–theprojectiononthespike.ThisisreflectedinDefinition4.5
containingnon-lineardependenceonκalongthefunctionalbasis{c (·,ζ ),c (·,ζ )} ,withc (κ,ζ )≡κc (κ,ζ ).
0 q 1 q q∈[k] 1 q 1 q
Thisfunctionalbasisislarger,andthustheneuralnetworkmoreexpressive,forlargervocabularysizesk,i.e.whenthe
secondlayerisinitializedwithmorevariability.
Toillustratethispointconcretely,considerfordefinitenessthecaseofanerrorfunctionactivationσ(·) = erf(·).
Then,{c 0(·,ζ q)}
q∈[k]
= {erf(ζq/√ 3·)} q∈[k]. Whileauniformsecondlayerinitializationa0 ∝ 1
p
(k = 1)onlyallows
thenetworktoexpressmonotonicsigmoidfunctions,allowingthesecondinitializationtotakek =2valuesalready
allowstoexpressnon-monotonicfunctionwithaderivativewhichcanchangesigntwice. Pushingthesecondlayer
diversitytovocabulariesofsizek ≥3furtherenrichesthepoolofexpressiblefunctionswithfurthernon-monotonic
functions. Dependingonthefunctionalformofthetargetactivationσ ,thevariabilityofthesecondlayercanthus
⋆
proveparticularlyinstrumentalinreachingagoodapproximationandlearning.InFig.2,weillustratetheroleofthe
vocabularysizek,forthesimplestpossiblesetting(single-indextarget,σ =relu,σ =tanh),byplottingthetesterror
⋆
asafunctionofthesamplecomplexityαforvaryingvocabularysizesk ∈{1,2,4}.Weobserveanetdecreaseinthe
testerrorperformancewithincreasingsecondlayervariabilitykatinitialization.Furthermore,thetheoreticalresults
(plottedascontinuouslines)areinaccordancewithnumericalsimulations(dots).
6 Conclusions and Limitations
We present a rigorous random matrix theory analysis of feature learning in a two-layer neural network, when the
firstlayeristrainedwithasingle,butaggressive,gradientstep,inthelimitwherethenumberofsamplesn,theinput
dimensiond,thehiddenlayerwidthpandthelearningrateη,jointlytendtoinfinityatproportionalrates.Werigorously
justifyhowthetrainedneuralnetworkcanbeapproximatedbyaspikedrandomfeaturesmodelinthelimitoflarge
batch sizes. We derive a deterministic equivalent for the empirical covariance matrix of the resulting features. We
furtherprovideatightasymptoticcharacterizationofthetesterror,whenthesecondlayerissubsequentlytrainedwith
ridgeregression. OurresultsprovidearigorousprooftotheheuristicworkofCuietal.[2024],whileextendingitin
multipleaspects.Inparticular,weallowfornon-uniforminitializationforthesecond-layerweights.Wediscusshow
thesecond-layervariabilityenhancestheexpressivityofthetrainednetwork,anditsabilitytofitasingle-indextarget.
Amongthelimitationsarethefinitelysupportedsecondlayerinitialization,theuseofGaussiandata,andasymptotic
natureoftheresults.Wenote,however,thatnumericalexperimentsshowsthatpredictionsareaccurateevenatfairly
moderatesizes.Secondly,anumberofuniversalityresultsshowsthatsuchensemblesextendoverlargerdatasets[Dudeja
etal.,2023,Geraceetal.,2024,Loureiroetal.,2021,Pesceetal.,2023,Wangetal.,2022].
10Acknowledgements
WewouldliketothankDennyWu,EdgarDobriban,LenkaZdeborováandLudovicStephanforstimulatingdiscussions.
WealsothanktheInstitutd’ÉtudesScientifiquesdeCargèseforthehospitalityduringthe“Statisticalphysics&machine
learningbacktogetheragain”workshop,wherethisworkstarted.BLacknowledgesfundingfromtheChooseFrance-
CNRSAIRisingTalentsprogram.YDandHCacknowledgesupportfromtheSwissNationalScienceFoundationgrant
SNFSSMArtNet(grantnumber212049).YD,LP,andFZacknowledgesupportfromtheSNFSgrantOperaGOST(grant
number200390).TheworkofYMLwassupportedinpartbytheHarvardFASDean’sFundforPromisingScholarship
and bya Harvard CollegeProfessorship. HC acknowledges support from the Centerof Mathematical Sciencesand
Applications(CMSA)ofHarvardUniversity.
11Supplementary material
A Structure of the Appendix
TheAppendixisorganizedasfollows:
• InsectionB,welistsomenotations,definitionsandpreliminaryresultsutilizedthroughouttheproof.
• InsectionC,weprovethetheisotropic-spikeapproximationofthegradientandshowthatinthelimitα→∞,it
sufficestoestablishthedeterministicequivalentandgeneralizationerrorundertheisotropic-spikeapproximation.
• InsectionD,weprovethemainTheorem4.6characterizingtheasymptoticdeterministicequivalentofthesample
covarianceoftheextendedfeatures.
• InsectionE,weshowhowthegeneralizationerrorcanbeexpressedthroughcertainfunctionalsofthedeterministic
equivalentandfinallyobtainTheorem4.6.
• Finally in Section F, we provide proofs of certain auxiliary results used in the analysis along with additional
theoreticalinvestigations
B Preliminaries
B.1 StochasticDomination
Throughouttheanalysis,weusethefollowingnotationforcontrollinghigh-probabilityboundsoverstochasticerror
terms:
DefinitionB.1. [Stochasticdominance[LuandYau,2022]]Wesaythatasequenceofrealorcomplexrandomvariables
X isstochasticallydominatedbyanothersequenceY ifforallϵ>0andk,thefollowingholdsforlargeenoughd:
d d
Pr(cid:2) |X| >dϵ|Y| (cid:3) ≤d−k.
d d
Wedenotetheaboverelationthroughthefollowingnotation:
X =O (Y).
≺
Wefurtherdenote:
|X−Y|=O (Z),
≺
as:
X−Y =O (Z).
≺
SimilarlyforvectorsX,Y ∈Rkd forsomesequenceofdimensionsk d,weusetheshorthand:
X−Y =O (Z),
≺
todenote:
∥X−Y∥=O (Z),
≺
Itiseasytocheckthatstochasticdominanceisclosedunderunionsofpolynomiallymanyeventsind.Wewilloften
exploitthiswhiletakingunionsoverp=O(d)neuronsandn=O(d)samples.Furthermore,≺absorbspolylogarithmic
factorsi.e:
X =O (Y) =⇒ X =O ((polylogd)Y)
≺ ≺
Furthermore,itsubsumesexponentialtailboundsoftheform:
Pr[X >tY
]≤e−tα
,
d d
forsomeα>0,aswellaspolynomialtailsofarbitrarilylargedegree:
C
Pr[X >tY ]≤ k,
d d tk
forsomesequenceofconstantsC dependentonk.
k
12DefinitionB.2(HermiteExpansion). Letf :R→Rbesquare-integrablefunctionw.r.ttheGaussianmeasure.Then,f
admitsaseriesexpansionintheorthonormalbasisofHermitepolynomialsgivenby:
∞
(cid:88)
f(x)= a h (x),
i i
i=0
wheretheconvergenceholdsinL w.r.ttheGaussianmeasure.
2
LemmaB.3(ResolventIdentity). Let,A,B ∈Rp×pbetwoinvertiblematrices,then:
A−1−B−1 =A−1(B−A)B−1
LemmaB.4. Letz ∈C/R+bearbitaryandletA∈RN×N denoteap.s.dmatrix.Letζ =max((cid:12) (cid:12)Im(z)(cid:12) (cid:12),−Re(z).Then:
(cid:13) (cid:13) 1
(cid:13)(A−zI N)(cid:13)≤
ζ
LemmaB.5(Burkholder’sinequality(Lemma2.12inBaiandZhou[2008])). LetX ,i=1,···nbeacomplex-valued
i
Martingaledifferencesequencew.r.tafiltrationF ,thenforanyp≥1,thereexistsaconstantK suchthat:
i p
(cid:12) (cid:12)p  
(cid:12) n (cid:12) n
E (cid:12) (cid:12) (cid:12)(cid:88) X i(cid:12) (cid:12)
(cid:12)
 ≤K p(E (cid:88) X i2 )p/2
(cid:12)i=1 (cid:12) i=1
DefinitionB.6(Lipschitzconcentration[LouartandCouillet,2018]). ArandomvariableX ∈Rdissaidtobeα-Lipschitz
concentratedifforany1-Lipschitzfunctionf:
P[f(X)≥t]≤α(t)
DefinitionB.7(Tracenorm). ForanyA∈Cp×p,theTracenormisdefinedas:
(cid:16)√ (cid:17)
∥A∥ =Tr A⋆A ,
tr
whereA⋆denotesHermitianconjugateofA.
LemmaB.8(Trace-norminequality). ForanyA,B ∈Cp×p:
∥AB∥ ≤∥A∥ ∥B∥,
tr tr
where∥B∥denotestheoperatornorm.
Definition B.9 (Schur complement). Let p,q two non-negative integers such that p+q > 0, consider the matrix
A∈R(p+q)×(p+q):
(cid:18) (cid:19)
A A
A= 11 12
A A
21 22
IfA isinvertible,theSchurcomplementoftheblockA ofthematrixAisthematrixA/A ∈Rp×pdefinedas:
22 22 22
A/A =A −A A−1A
22 11 12 22 21
Similarly,theSchurcomplementoftheblockA ofthematrixAisthematrixA/A ∈Rq×q definedas:
11 11
A/A =A −A A−1A
11 22 21 11 12
LemmaB.10. [Theorem5.2.2inVershynin[2018]]Letz ∼N(0,I )denoteastandardGaussianrandomvectorinRN
N
andletf :RN →RbeaLipschitz-functionwithLipschiz-constantboundedbyL.Then:
(cid:20)(cid:12) (cid:12) (cid:21)
Pr
(cid:12)f(z)−E(cid:2) f(z)(cid:3)(cid:12)≥t ≤2e−ct2/L
(cid:12) (cid:12)
√
LemmaB.11. [Theorem5.1.3in[Vershynin,2018]]Letz ∼U(SN−1( d))denotearandomvectoruniformlysampledfrom
√
theN dimensionalsphereofradius N.Then,foranyLipschitz-functionf :RN →RwithLipschiz-constantboundedby
L:
(cid:20)(cid:12) (cid:12) (cid:21)
Pr
(cid:12)f(z)−E(cid:2) f(z)(cid:3)(cid:12)≥t ≤2e−ct2/L
(cid:12) (cid:12)
13LemmaB.12. [Theorem5.3inVershynin[2010]andTheorem4.3.5in[Vershynin,2018]]Underassumptions2.1,theoperator
norms∥W ∥,∥X∥satisfy,forsomeconstantsC ,C
0 1 2
1 √ √ 1
∥W ∥≤ √ C ( d+ p)+O (√ )
0 1 ≺
d d
√ √
∥X∥≤C ( n+ d)+O (1)
2 ≺
LemmaB.13. [O’Donnell,Proposition11.33,p.338]TheHermitepolynomials(h α) α∈Nformacompleteorthonormalbasis.
Further,foranyρ∈[−1,1]andtwostandardnormalGaussianrandomvariablesz,z′thatareρ-correlated,wehave
(cid:40)
E (cid:2) h (z)h (z′)(cid:3) = ρα ifα=β,
z,z′ α β
0 ifα̸=β.
LemmaB.14. Letf (z)andf (z)betwotwice-differentiablefunctionswiththefirstandsecondderivativesbounded
1 2
almostsurely.Supposethatf (z),f (z)admitthefollowingHermiteexpansions:
1 2
(cid:88) (cid:88)
f (z)= c h (z) f (z)= c˜ h (z),
1 k k 2 k k
k≥0 k≥0
where(cid:8) h (z)(cid:9) denotethesetofnormalizedHermitepolynomials.Giventhreeunitnormvectorsw ,w ,w′suchthat:
k k 1 2
(cid:12) (cid:12) 1
(cid:12)w⊤w′(cid:12)=O (√ )
(cid:12) i (cid:12) ≺
d
fori=1,2.Thenforg ∼N(0,I),thefollowingholds:
(cid:104) (cid:105) 1
E f (w⊤g)f (w⊤g)g⊤w′ =c c˜ (w⊤w′)+c˜ c (w⊤w′)+O ( ), (7)
1 1 2 2 0 1 2 0 1 1 ≺ d
(cid:104) (cid:105)
E f (w⊤g)f (w⊤g)(g⊤w′)2 =c c˜ +O(d−1/2), (8)
1 1 2 2 0 0
C Spiked isotropic approximation
Inthissection,wejustifytheassumptionofanisotropicbulkintheregimeα →∞,WefirstestablishLemma3.1and
0
subsequentlycontroltheresultingapproximationerrorfordeterministicequivalenceandgeneralizationerrors.
C.1 ProofofLemma3.1
Letyˆ ,··· ,yˆ denotetheoutputsofthenetworkatinitialization,i.e:
1 n
yˆ =f(x ;W(0),a)
µ µ
Westartbyexpressingthegradientas:
1
G= √ diag{a ,...a }σ′(W0X)diag{y −yˆ ,...,y −yˆ }X ∈Rp×d,
n p 1 p 1 1 n n
whereX ∈n×ddenotesthematrixwithrowsthedatavectorsinthefirstbatch,{y } arethecorrespondinglabels,
i i∈[n]
andσ′(·)isthederivativeofthestudentactivationfunction.
Next,wedecomposeσ′as:
σ′(x)=c +σ′ (x), (9)
1 >1
where
√
σ′ (x)d =ef(cid:88) k!µ h (x),
>1 k k−1
k≥2
14andc denotetheHermitecoefficientsofσ.Usingthedecompositionin(46),wecanrewritetheweightmatrixW1as
k
η √ √
W1 =W0+uv⊤+ diag{ pa ,... pa }σ′ (W0X)diag{y ,...,y }XT
n 1 p >1 1 n
(cid:124) (cid:123)(cid:122) (cid:125)
∆1
η √ √
− diag{ pa ,... pa }σ′ (W0Z)diag{yˆ ,...,yˆ }ZT
n 1 p >1 1 n
(cid:124) (cid:123)(cid:122) (cid:125)
∆2
√
whereu=µ η paandv =X⊤y/n.
1
Ourgoalisthereforetoboundthecontributionsfrom∆ ,∆ .Westartbyexpressing∆ asasumofrankoneterms:
1 2 1
1
(cid:88)n0
∆ = b x⊤,
1 n µ µ
µ=1
√ √
whereb⊤ =y σ′(W0X)[ pa ,... pa ].
µ µ 1 p
Let{c⋆ k} k∈NdenotetheHermitecoefficientsofg.AnapplicationofStein’sLemmaandHermiteexpansionyields(See
Lemma7inDamianetal.[2022]orLemma4inDandietal.[2023])yieldsthefollowingexpansionforthei rowof∆ :
th 1
∞ ∞
(cid:104) (cid:105) (cid:88) (cid:88)
E bix = c µ⋆ ⟨(w⋆)⊗k+1,w⊗k⟩+ c c⋆⟨(w⋆)⊗k,w⊗kw ⟩
xµ µ µ k+1 k+1 i k+2 k i i
k=1 k=1
Byassumptionc
2
= 0, thereforethetermc 2,c⋆ 2w⋆⟨w⋆,w i⟩vanishes. Since⟨w⋆,w i⟩ = O ≺(√1 d)foralli ∈ [p], the
remainingtermsareboundedintermbyO ( 1 ).Weobtain:
≺ d1.5
(cid:13) (cid:13)
(cid:13) (cid:13) (cid:13)E xµ(cid:104) bi µx µ(cid:105)(cid:13) (cid:13)
(cid:13)
=O ≺( d1 ),
F
whichresultsinthefollowingboundontheoperatornorm:
(cid:13) (cid:13)E x[∆ 1](cid:13) (cid:13)=O ≺(√1 ).
d
Next,notethattheuniformboundednessofσ′,σ∗implythat(cid:13) (cid:13)b µ(cid:13) (cid:13)2 <CpforsomeconstantC >0.Considerthe
symmetrizedmatrix:
(cid:34) (cid:35)
0 b x⊤
B = µ µ ∈R(p+d)×(p+d)
µ x b⊤ 0
µ µ
Then,forallk ≥1:
B2k+1
=(cid:34) 0 (cid:13) (cid:13)x µ(cid:13) (cid:13)2k(cid:13) (cid:13)b µ(cid:13) (cid:13)2k b µx⊤ µ(cid:35)
(10)
µ (cid:13) (cid:13)x µ(cid:13) (cid:13)2k(cid:13) (cid:13)b µ(cid:13) (cid:13)2k x µb⊤
µ
0
B2k
=(cid:34)(cid:13) (cid:13)x µ(cid:13) (cid:13)2k(cid:13) (cid:13)b µ(cid:13) (cid:13)2k−2 b µb⊤
µ
0 (cid:35)
µ 0 (cid:13) (cid:13)x µ(cid:13) (cid:13)2k−2(cid:13) (cid:13)b µ(cid:13) (cid:13)2k x µx⊤
µ
Now,letz ∈Rp+dwith∥z∥=1andletz =z[:p],z =z[p:]denotethefirstpandtheremainingcomponentsof
p d
zrespectively.FromEquation(10),wehave:
E(cid:104) z⊤B µ2k+1z(cid:105) =E(cid:104)(cid:13) (cid:13)x µ(cid:13) (cid:13)2k(cid:13) (cid:13)b µ(cid:13) (cid:13)2k ⟨b µ,z p⟩⟨x µ,z d⟩(cid:105) +E(cid:104)(cid:13) (cid:13)x µ(cid:13) (cid:13)2k(cid:13) (cid:13)b µ(cid:13) (cid:13)2k ⟨b µ,z p⟩⟨x µ,z d⟩(cid:105) .
ApplyingCauchy–SchwarztoeachtermintheRHSyields:
E(cid:104) z⊤B µ2k+1z(cid:105) ≤E(cid:104)(cid:13) (cid:13)x µ(cid:13) (cid:13)4k(cid:13) (cid:13)b µ(cid:13) (cid:13)4k(cid:105)1/2 E(cid:104) (⟨b µ,z p⟩)2(⟨x µ,z d⟩)2(cid:105)1/2 +E(cid:104)(cid:13) (cid:13)x µ(cid:13) (cid:13)4k(cid:13) (cid:13)b µ(cid:13) (cid:13)4k(cid:105)1/2 E(cid:104) (⟨b µ,z p⟩)2(⟨x µ,z d⟩)2(cid:105)1/2
(11)
Now,theboundednessofσ′,σ⋆implythat(cid:13) (cid:13)b µ(cid:13) (cid:13)4k ≤C 14kp2kforsomeconstantC 1.While(cid:13) (cid:13)x µ(cid:13) (cid:13)2isasub-exponential
randomvariablewithparameterdandtherefore[Vershynin,2018]:
E(cid:104)(cid:13) (cid:13)x µ(cid:13) (cid:13)4k(cid:105) ≤C 22kd2k2k2k,
15forsomeconstantC >0.Therefore:
2
E(cid:104)(cid:13) (cid:13)x µ(cid:13) (cid:13)4k(cid:13) (cid:13)b µ(cid:13) (cid:13)4k(cid:105) ≤C 22kd2k2k2k×C 14kp2k
Finally,byassumption2.3,σ′,gareuniformly-lipschitz.Furthermore,withhighprobabiltyoverW,∥W∥≤C for
3
someconstantC .Wethereforeobtainthatx→⟨b ,z ⟩isuniformlylipschitzinxwithhighprobabiltyoverW.
4 µ p
Furthermore,applyingLemmaB.13toσ′,gandusingc =0,yields:
2
E(cid:2)
⟨b ,z
⟩(cid:3) =O(√1
).
µ p
d
Therefore,E(cid:2) (⟨b ,z ⟩)2(⟨x ,z ⟩)2(cid:3)1/2isfurtherboundedbysomeconstantC >0.
µ p µ d 4
Sincep/d=β isaconstant,substitutinginEquation(11)weobtain:
(cid:104) (cid:105)
E z⊤B2k+1z ≤(C d)2k−1d.
µ 4
Similarly,
(cid:104) (cid:105)
E z⊤B2kz ≤(C d)2k−2d,
µ 5
forsomeconstantC >0.
5
Therefore:
(cid:104) (cid:105)
E Bk ≺(C d)k−2dI ,
µ 4 p+d
forsomeconstantC .
4
Subsequently,weapplythematrix-Bernsteininequalityforself-adjointmatriceswithsubexponentialtails(Theorem
6.2inTropp[2012])toobtainthat:
(cid:13) (cid:13) 
Pr (cid:13) (cid:13) (cid:13) (cid:13)n1 (cid:88)n B µ−(cid:13) (cid:13) (cid:13)E(cid:2) B µ(cid:3)(cid:13) (cid:13) (cid:13)(cid:13) (cid:13) (cid:13) (cid:13)≥t ≤de−t(logd)2/(c1+c2t),
(cid:13) µ=1 (cid:13)
forsomeconstantsc ,c >0.Borel-CantelliLemmathenimplies:
1 2
d→∞
∥∆ ∥−−−→0.
1
a.s
Now,tobound∆ ,weuse:
2
∥∆ 2∥≤ nη(cid:13) (cid:13)diag{√ pa 1,...√ pa p}(cid:13) (cid:13)(cid:13) (cid:13) (cid:13)σ >′ 1(W0Z)(cid:13) (cid:13) (cid:13)(cid:13) (cid:13)diag{yˆ 1,...,yˆ n}(cid:13) (cid:13)∥X∥.
Weshowthat:
(cid:13) (cid:13) 1
(cid:13)diag{yˆ 1,...,yˆ n}(cid:13)=O ≺(√ )
d
Recallthat:
p
1 (cid:88)
yˆ = √ a σ(w⊤x )
µ p j j µ
j=1
ByTheorem3.1.1inVershynin[2018],(cid:12) (cid:12) (cid:12)(cid:13) (cid:13)x µ(cid:13) (cid:13)−√ d(cid:12) (cid:12) (cid:12)forµ∈[n]areindependentsub-Gaussianrandomvariables.
Therefore:
√
(cid:13) (cid:13) (cid:112)
sup (cid:13)x µ(cid:13)= d+O ≺( logn).
µ∈[n]
Therefore,wemayconditiononthehigh-probabilityevent:
√
(cid:13) (cid:13)
E
n
={sup (cid:13)x µ(cid:13)≤C d},
µ∈[n]
for some C > 1. Conditioned on the event E , Lemma B.11 and assumption 2.3 imply that for each µ ∈ [n],
√ n
pa {σ(w⊤x ),j ∈[p]}areindependentsub-Gaussianrandomvariableswithmean0.(Recallthatbyassumption2.3,
j j µ
σisodd).Therefore∀µ∈[n]:
1
yˆ =O (√ ),
µ ≺
d
16whereweabsorbedpolylogarithmicfactorsthroughthestochasticdominationnotation(DefinitionB.1).
Lastly,itremainstoshowthatthespikevconvergestow⋆.Recallthatv := 1 X⊤y.Byassumptions2.3,y xi are
c⋆ n µ µ
independentsub-Gaussianrandomvariablesfori∈[d].Therefore: 0
(cid:112)
∥v−w⋆∥=O ( d/n ).
≺ 0
Since,n =O(d1+ϵ),weobtain:
0
v −−→w⋆
a.s
ItremainstoshowthattheequivalenceinthesenseofLemma3.1extendstogeneralizationerroranddeterministic
equivalence:
PropositionC.1. LetGe,(G )edenotetheextendedresolventswithfeaturesW(1)andW˜ =W0+ηu(w⋆)⊤respectively.
⋆
Analogously,letaˆ,aˆ denotetheridge-regressionpredictorscorrespondingtoW(1)andW˜ respectively.Then,forsequenceof
⋆
(cid:16) (cid:17)
deterministicmatricesA∈C(p+1)×(p+1)with∥A∥ =Tr (AA∗)1/2 uniformlyboundedind:
tr
(cid:12) (cid:12)
(cid:12)Tr(cid:0) AG (z)(cid:1) −Tr(AG )e(cid:12)−−a −.s →0. (12)
(cid:12) e ⋆ (cid:12)
d→∞
(cid:12) (cid:12)
(cid:12)e (aˆ,W(1))−e (aˆ ,W(1))(cid:12)−−a −.s →0.
(cid:12) gen gen ⋆ (cid:12)
d→∞
Proof. Considerthematrix:
Ξ=σ(XW(1⊤))−σ(XW˜⊤)∈Rn×p.
AstraightforwardconsequenceoftheproofofLemma3.1andAssumption2.3,LemmaB.10isthateachrowofΞisa
sub-Gaussianrandomvectorwithsub-GaussiannormsO(polylogdϵ ).Therefore,throughtailboundsontheoperator
d
normsofmatriceswithsub-Gaussianrows[Vershynin,2010],weobtain:
1
√ ∥Ξ∥−−→0 (13)
p a.s
TheclaiminEq(12)thenfollowssincebyLemmaB.8:
(cid:12) (cid:12) (cid:12)Tr(cid:0) AG e(z)(cid:1) −Tr(AG ⋆)e(cid:12) (cid:12) (cid:12)≤∥A∥ Tr(cid:13) (cid:13)G e(z)−G ⋆(cid:13) (cid:13)
C(cid:13) (cid:13)
≤∥A∥ (cid:13)σ(XW(1)⊤ )−σ(XW˜⊤)(cid:13),
Trζ2(cid:13) (cid:13)
forsomeconstantC. Inthelastline,weusedLemmaB.3andζ isdefinedasinLemmaB.4. Similarly,Equation(13)
impliesthealmostsureconvergenceforthegeneralizationerror.
D Deterministic Equivalent
TheproofofTheorem4.6proceedsinthreeparts:
• ApproximationofthecovarianceE(cid:2) ϕϕ⊤(cid:3)throughthecovarianceR⋆ ofaconditional-Gaussianequivalentmodel
κ
atfixedvaluesofW.R⋆ possessesablock-structureduetothefinite-supportassumptionoveru
κ i
• First-stage deterministic equivalent: here we average over the randomness in the inputs X and construct a
deterministicequivalentdependentonW andexpressedasafunctionalofR⋆.
κ
• Second-stagedeterministicequivalent:HerewefurtheraverageovertherandomnessinW toestablishdetermin-
isticequivalentsforR⋆ andassociatedtransforms.
κ
Wedescribeeachofthesestagesbelow:
17D.1 Gaussian-equivalentcovarianceapproximation
Westartbyshowingthatconditionedonκ,thecovarianceoftheextendedfeaturesϕe canbewell-approximatedthrough
µ
thatofanequivalentlinearmodel.ThisapproximationissimilartotheconditionalGaussianequaivalenceinDandietal.
[2023],Cuietal.[2024].However,weemphasizethatwedonotdirectlyutilizeanyuniversalityresultongeneralization
errorsestablishedinrecentworks[LuandYau,2022,MontanariandSaeed,2022,Dandietal.,2023].Instead,wewill
directlyutilizetheapproximationofthecovarianceinouranalysisofthespectrumandgeneralizationerrorsinthe
subsequentsections.
Recallthedefinitionofthesurrogatefeaturevectorsinequation(5):
 
y
µ
ϕe =ϕ¯ 
µ  µ
ϕ˜
µ
Weobservethatthesamplecovariancematrixofz possesesthefollowingblockstructure:
µ
 
yy⊤ yΦ¯⊤ yΦ˜⊤
(Φe)⊤(Φe)=Φ¯y⊤ Φ¯Φ¯⊤ Φ˜Φ¯⊤
µ µ  
Φ˜y⊤ Φ¯Φ˜⊤ Φ˜Φ˜⊤
Define:
(cid:34) (cid:35) (cid:34) (cid:35)
yy⊤ yΦ¯⊤ Φ˜y⊤ (cid:104) (cid:105)
Σ := Σ := Σ := Φ˜Φ˜⊤
11 Φ¯y⊤ Φ¯Φ¯⊤ 21 Φ˜Φ¯⊤ 22
PropositionD.1. Foravectoru∈Rm,letc (u,κ)∈Rmdenotethevectorwithentriesc (u ,κ).LetE [·]denotethe
j j i κ
expectationw.r.txconditionedonthesigma-algebrageneratedbyκ:=x⊤w∗.Define:
(cid:34) (cid:35)
g2(κ) g(κ)c (κ,u )⊤
R⋆ (κ)= 0 π ,
11 g(κ)c (κ,u ) c (κ,u )c (κ,u )⊤
0 π 0 π 0 π
whereu denotesthevector(u ,··· ,u )
π 1 k
R⋆ (κ)=c (κ,u)⊙θκ(cid:2) σ (κ) c (κ,u )··· c (κ,u )(cid:3) ,
21 1 ⋆ 0 1 0 k
(cid:18) (cid:19)
(cid:88)
R⋆ (κ)=(c (κ,u)c (κ,u)⊤)⊙WW⊤)+diag( c2(κ,u) +(κ2−1)c (κ,u)c (κ,u)⊤⊙θθ⊤
22 1 1 k 1 1
k≥2
1
+ c (κ,u)c (κ,u)⊤
d 2 2
Then,thereexistsanℓ∈Nsuchthat,thefollowingholdsalmostsurelyoverκ:
sup (cid:13) (cid:13)E κ[Σ 11]−R 1⋆ 1(κ)(cid:13) (cid:13)=O
≺(√κℓ
) (14)
κ∈R/E0 d
sup (cid:13) (cid:13)E κ[Σ 21]−R⋆ 21(κ)(cid:13) (cid:13)=O
≺(√κℓ
) (15)
κ∈R/E0 d
sup (cid:13) (cid:13)E κ[Σ 22]−R 2⋆ 2(κ)(cid:13) (cid:13)=O
≺(√κℓ
) (16)
κ∈R/E0 d
Proof. Letw denotetheithrowofW andx∼N(0,I ).BytherotationalinvarianceoftheGaussianmeasure,wecan
i d
expressxas:
x=κw∗+(I −w∗(w∗)⊤)ξ,
d
whereξ ∼N(0,I )isindependentofx
d
Weobtain:
x⊤w1 =κu +w⊤ξ+w⊤w∗(κ−ξ⊤w∗)
i i i
18Therefore,
ϕ (x)=σ(κu +w⊤ξ+w⊤ξ(−w⊤g)+w⊤ξ(u−ξ⊤g)).
i i i i i i
Since
(cid:12) (cid:12) κ
(cid:12)w⊤w∗(κ−ξ⊤w∗)(cid:12)=O (√ ),
(cid:12) i (cid:12) ≺
d
byassumption2.3anapplicationoftheTaylor’stheoremyields:
ϕ =σ(κu +w⊤ξ)+σ′(κu +w⊤ξ)(cid:2) w⊤w∗(κ−ξ⊤w∗)(cid:3)
i i i i i i
+ 1 σ′′(κu +w⊤ξ)(cid:2) w⊤w∗(κ−ξ⊤w∗)(cid:3)2 +O (κ3d−3/2).. (17)
2 i i i ≺
Tosimplifytheexpectationsofthesecond,thirdterms,weleveragetheHermiteexpansionandLemmasB.13,B.14.
WebeginbyexpandingσalongtheHermite-basisforafixedvalueofκu :
i
σ(κu +z)=c (κu )+c (κu )h (z)+c (κu )h (z)+...+
i 0 i 1 i 1 2 i 2
√
Usingtheidentityh′(z)= kh (z),wealsohave
k k−1
√ √
σ′(u κ+z)=c (κ,u )+ 2c (κ,u )h (z)+ 3c (κ,u )h (z)+... (18)
i 1 i 2 i 1 3 i 2
and
√ √
σ′′(u κ+z)= 2c (κ,u )+ 6c (κ,u )h (z)+... (19)
i 2 i 3 i 1
ConsiderthesecondterminEquation(17):
E (cid:104) σ′(κu +w⊤ξ)(cid:2) w⊤w∗(κ−ξ⊤w∗)(cid:3)(cid:105) =κ(w⊤w∗)E(cid:104) σ′(κu +w⊤ξ)(cid:105) −(w⊤w∗)E(cid:104) σ′(κu +w⊤ξ)ξ⊤w∗(cid:105)
κ i i i i i i i i i
From(18),thethefirsttermintheRHSequalsκ(w⊤w∗)c (κ,u )whilefromLemmaB.13,thesecondtermequals
i 1 i
(cid:104) (cid:105) √
E σ′(κu +w⊤ξ)ξ⊤w∗ = 2c (κ,u )w⊤w∗.
κ i i 2 i i
Combining,weobtain:
E (cid:104) σ′(κu +w⊤ξ)(cid:2) w⊤w∗(κ−ξ⊤w∗)(cid:3)(cid:105) =κ(w⊤w∗)c (κ,u )−√ 2c (κ,u )(w⊤w∗)2.
κ i i i i 1 i 2 i i
Next,considerthethirdterminEquation(17):
E (cid:20) 1 σ′′(κu +w⊤ξ)(cid:2) w⊤w∗(κ−ξ⊤w∗)(cid:3)2(cid:21) =(w⊤w∗)21 E (cid:104) σ′′(κu +w⊤ξ)(cid:2) κ2−2ξ⊤w∗+(ξ⊤w∗)2(cid:3)(cid:105)
κ 2 i i i i 2 κ i i
AgainapplyingEquation(19)andLemmaB.13toeachtermintheRHSyields,simplifiesasfollows:
(cid:20) (cid:21)
1 1
E κ2(w⊤w∗)2σ′′(κu +w⊤ξ) = √ κ2c (κ,u )
κ 2 i i i 2 2 i
(cid:104) (cid:105) √ 1
E (w⊤w∗)2σ′′(κu +w⊤ξ)ξ⊤w∗ = 6c (κ,u )(w⊤w∗)3 =O ( )
κ i i i 3 i i ≺ d3/2
(cid:20) (cid:21)
1 1 1
E (w⊤w∗)2σ′′(κu +w⊤ξ)(ξ⊤w∗)2 = √ c (κ,u )(w⊤w∗)2+O ( ),
κ 2 i i i 2 2 i i ≺ d2
where in the second equation, we used that c (κ,u ) is uniformly bounded in κ by Assumption 2.3 and in the last
√ 3 i
equation,weused(ξ⊤w∗)2 = 2h (ξ⊤w∗)+1.
2
Combining,weobtain:
c (κ,u)(cid:104) (cid:105)
E [ϕ ]=c (κ,u)+c (κ)kw⊤w⋆+ 2√ (w⊤w⋆)2(κ2−1) +O (d−3/2). (20)
κ i 0 1 i i ≺
2
givesus:
E [ϕ]=c (κ,u)1 +c (κ,u)⊙W(cid:0) κw⋆)+O (d−1)
κ 0 p 1 ≺
19Now,since⟨w ,w⋆⟩,··· ,⟨w ,w⋆⟩∈Rdarei.i.d.sub-Gaussianrandomvariables,weobtainthat:
1 p
E (cid:2) ϕ¯(cid:3) =c (κ,ζu).
κ q 0 q
Furthermore,byAssumption2.3andsince∥W∥
2
<Ca.sforlargeenoughC,weobtainthatϕ¯ qisanO(√1 d)-Lipschitz
functionofx,weobtain:
ϕ¯−E (cid:2) ϕ¯(cid:3) =O (√1 )
κ ≺
d
Therefore,inwhatfollows,wewillutilize:
1
ϕ˜ =ϕ −c (κ,u )+O (√ ). (21)
i i 0 i ≺
d
CombiningtheaboveapproximationwithEquation(20)andusingtheuniformboundednessofgdirectlyyieldsthe
estimatesforΣ ,Σ inEquations(14)and(15).
11 21
TostudytheblockΣ ,weseparatethecaseofdiagonalandoff-diagonalentries.Fortheformer,weapplyLemma
22
B.13toEquation(17)toobtain:
E
(cid:104) ϕ˜2(cid:105) =(cid:88)
c2(κ,u )+O
(κ/√
d). (22)
κ i k i ≺
k≥1
Fortheoff-diagonaltermswithi̸=j,westartbynotingthat
(cid:104) (cid:105) (cid:104) (cid:105)
E ϕ˜ϕ˜⊤ =(I −P)E ϕϕ⊤ (I −P)⊤,
κ p κ p
whereP denotestheprojectiononthedirectionse1,···ek definedinEquation(6).TheTaylorexpansionin(17)andthe
approximation(21)thengivesus:
(cid:104) (cid:105) (cid:104) (cid:105)
E ϕ˜ϕ˜ =E (σ(κu +w⊤ξ)−ϕ¯)(σ(κu +w⊤ξ)−ϕ¯ ))
κ i j κ i i i j j j
(cid:124) (cid:123)(cid:122) (cid:125)
(A)
+E (cid:104) (σ(κu +w⊤ξ)−c (κ,u ))σ′(κu +w⊤ξ)(cid:2) w⊤w⋆(κ−ξ⊤w⋆)(cid:3)(cid:105) +(B)
κ i i 0 i j j j i↔j
(B)
(cid:124) (cid:123)(cid:122) (cid:125)
+ 1 E (cid:104) (σ(κu +w⊤ξ)−c (κ,u ))σ′′(κu +w⊤ξ)(cid:2) w⊤w⋆(κ−ξ⊤w⋆)(cid:3)2(cid:105) +(C)
2 κ i i 0 i j j j i↔j
(cid:124) (cid:123)(cid:122) (cid:125)
(C)
+E (cid:104) σ′(κu +w⊤ξ)(cid:2) w⊤w⋆(κ−ξ⊤w⋆)(cid:3) σ′(κu +w⊤ξ)(cid:2) w⊤w⋆(κ−ξ⊤w⋆)(cid:3)(cid:105)
κ i i i j j j
(cid:124) (cid:123)(cid:122) (cid:125)
(D)
+O(κ3d−3/2),
where(B) ,(C) denotethecorrespondingtermswiththerolesofi,j interchanged.Next,weconsidereachterm
i↔j i↔j
ontheright-handside.ApplyingLemmaB.13,weobtainthatthetermAresultsin:
(cid:16) (cid:17)
(A)=(I −P) c (κ,u)c (κ,u)⊤⊙W⊤W +c (κ,u)c (κ,u)⊤⊙(W⊤W)2 (I −P)⊤+O (d−3/2).
p 1 1 2 2 p ≺
Fortermsin(B),weapplyLemmaB.14withw ,w =w ,w andw′ =w⋆toobtain:
1 2 i j
(cid:16)√ (cid:17)
(B)+(B) = 2c (κ,u )c (κ,u )w⊤w (w +w )⊤(κw⋆)
i↔j 1 i 2 j i j i j
−2c (κ,u )c (κ,u )(cid:2) (w⊤w⋆)(w⊤w⋆)]+O (d−3/2).
1 i 1 j i j ≺
Similarly,fortermsinC,LemmaB.14directlyimpliesthat:
(C)+(C) =O (d−3/2).
i↔j ≺
Finally,
(D)=c (κ,u )c (κ,u )(κ2+1)(w⊤w⋆)(w⊤w⋆)+O (d−3/2)
1 i 1 j i j ≺
20(cid:104) (cid:105)
We are now ready to establish an approximation of the correlation matrix E ϕ˜ϕ˜⊤ , up to an error term whose
operatornormisofsizeO ≺(d−1/2).Startingwith(A),sincew i⊤eq =O ≺(√1 d)fori∈[p],q ∈[k],wehave:
(cid:13) (cid:13)
(cid:13) (cid:13)(I p−P)(cid:16) c 1(κ,u)c 1(κ,u)⊤⊙W⊤W(cid:17) (I p−P)⊤−c 1(κ,u)c 1(κ,u)⊤⊙W⊤W(cid:13) (cid:13)=O ≺(√1 ).
(cid:13) (cid:13) d
Therefore,theextratermcomparedtoR⋆ (κ)istheonecontaining(w⊤w )2.Write
22 i j
(cid:114)
1 2p
(w⊤w )2 = + (H ) , (23)
i j d d2 2 ij
H ∈Rp×pisasymmetricmatrixsuchthat
2
√
( dw⊤w )2−1
(H ) = i√ j 1 .
2 ij i̸=j
2p
ThisisinfactaKernelgrammatrixwherethe“kernel”functionisthesecondHermitpolynomialh (z).Asstudiedine.g.
2
[Zhou&Montanari,2013],[Lu&Yau,2023],thespectrumofthiskernelmatrixisasymptoticallyequaltothesemicircle
(cid:113)
law.Moreover,theoperatornormofthismatrixisboundedbyO (1).Itfollowsthattheterm 2p(H ) in(23)canbe
≺ d2 2 ij
ignoredduetoitsvanishingoperatornorm.Thus,whatisleftisthecontributionfromtheterm 1,whichvanishessince
d
(I −P)c (κ,u)c (κ,u)⊤(I −P)⊤ =0.
p 2 2 p
Foreachk,leth bethevectorinRp,definedas
k,w⋆
√
(h ) d =ef h ( dw⊤w⋆)
k,w⋆ i k i
√
withh beingthekthHermitepolynomial.Bysubstituting(w⊤w⋆)2 = 1(h ( ddw⊤w⋆)−1)
k i d 2 i
Next,weexamine(B)+(B) andcaptureitsmainterms.First,notethat(w⊤w )w⊤(κw⋆)canbeignored,since
i↔j i j i
itcorrespondstoamatrix
(cid:110) (cid:111)
W diag κw⊤w⋆ W⊤,
i
whoseoperatornormisboundedbyO (d−1/2)byLemmaB.12.
≺
Letθ =W⋆w fori∈[p]Theremainingcomponentsin(B)+(B) canbeexpressedas:
i i i↔j
−2c (κ,u)c (κ,u)θ θ
1 1 i j
Itisalsoeasytoverifythatthelastcomponent(D)canbeexpressedasfollows:
(D)=c (κ,u)c (κ,u)(κ2+1)θ θ ,
1 1 i j
ThetermsBandDcombinetogivethecomponent:
(κ2−1)c (κ,u)c (κ,u)⊤⊙θθ⊤
1 1
CombiningwiththecontributionfromAandthediagonaltermsinEquation(22),weobtainEquation(16).
Beforemovingfurther,wenotedownaboundonR⋆ thatwillproveusefullate:
κ
PropositionD.2. LetR⋆ beasdefinedinPropositionD.1.Then,∃ℓ∈Nsuchthat:
κ
∥R⋆∥=O (κℓ)
κ ≺
.
Proof.
TheaboveisadirectconsequenceofPropositionD.1andAssumption2.3,withthelaterimplyingthat(cid:12)
(cid:12)c
o(κ,u)(cid:12)
(cid:12)≤
K|uκ|forsomeconstantK while(cid:12) (cid:12)c j(κ,u)(cid:12) (cid:12)areuniformlyboundedforj =1,2,3.
21D.2 First-StageEquivalent
Wenowproceedwithestablishingthefirststageofdeterministicequivalence,aimingtoremovetherandomnessw.r.tX.
Concretely,ourgoalinthissectionistoconstructasequenceofmatricesGe dependentonW butnotX suchthatGe
W W
isdeterministicallyequivalenttoGeforallsuitably-boundedlinearfunctionsindependentofX (butpossiblydepending
onW).IdeallyGe willpossessasimplerstructurethanGe,allowingustofurthersimplifyitinthenextstage.
W
Forsubsequentusage,weshallestablishamorequantitativeversionofdeterministicequivalentthanTheorem4.6,
withanexpliciterrorboundofO ≺(√1 ).
d
PropositionD.3. LetX ,W beasequenceofrandommatricesgeneratedasinAssumption2.1.Letz ∈C/R+bearbitrary.
d d
LetG (z)denotethethefollowingsequenceofrandommatricesdependentonlyonW:
W
(cid:18)
α
(cid:19)−1
G (z):= Σ⋆+ζL−zI ,
W p
where:
E (cid:2) Tr(GµR⋆)(cid:3)
χ (κ)d =ef W,X e κ , (24)
d p
andtheexpectationisw.r.tbothW,X conditionedontheprojectionκ alongthespikevasfixed.SinceGµisindependentof
µ e
κ,χ (κ)isafixedfunctionofκalone.Thesubscriptdistoremindusthatχ (κ)definedasinEquation(24)isanunknown
d d
dimension-dependentfunction.
Wefurtherdefine:
(cid:20) (cid:21)
1
Σ⋆ =E R⋆(κ ) ,
κ∼N(0,1) 1+χ(κ) µ
withR⋆(κ)denotingthematrix:
(cid:18) R⋆(κ) R⋆(κ) (cid:19)
R⋆(κ)= 11 12
R⋆(κ) R⋆(κ)
21 22
andR⋆(κ) ,R⋆(κ) ,R⋆(κ) ,R⋆(κ) areasdefinedinPropositionD.1.
11 12 21 22
ThenforanysequenceofmatricesA,possiblydependentonW,butnotX:
(cid:12) (cid:12)Tr(cid:8) AG (z)(cid:9) −Tr(cid:8) AG (z)(cid:9)(cid:12) (cid:12)=O (∥A √∥ Tr) (25)
(cid:12) W e (cid:12) ≺
d
Proof. Westartbydefining:
1
χ (µ):= (ϕe)⊤Gµϕe,
X,W p µ e µ
Notethatunlikeχ(κ)whichissolelyafunctionofκ,χ (µ)isarandomvariabledependingonX,W,z .Wefurther
X,W µ
define:
(cid:104) (cid:105)
R =E ϕe(ϕe)⊤|κ=κ
µ µ µ µ
Weproceedthroughaleave-one-outargument,analogoustotheoneforWishart-typematrices[BaiandZhou,2008].
Bydefinition,G satisfies:
e
n
1 (cid:88)
G ( ϕe(ϕe)⊤−zI)=I.
e p µ µ
µ=1
LetGµ :=((cid:80) ϕe(ϕe)⊤+λI)−1forµ∈[n]denotetheresolventwiththesampleµremoved.
e ν̸=µ µ µ
UsingtheSherman-Morrisonformula,wecanexpressG foranyµ∈[n]as:
e
1
G =Gµ− ϕe(ϕe)⊤.
e e 1+ 1(ϕe)⊤Gµϕe µ µ
p µ e µ
Next,foreachµ∈[n],wereplaceG inthetermG (ϕe(ϕe)⊤)bytheabovedecompositiontoobtain:
e e µ µ
n
(cid:88) 1 1
(Gµ− ϕe(ϕe)⊤)ϕe(ϕe)⊤−zIG =I. (26)
p e 1+ 1(ϕe)⊤Gµϕe) µ µ µ µ e
µ=1 p µ e µ
22Thefirstsetoftermssimplifyas:
1
G (ϕe(ϕe)⊤)=(Gµ− Gµϕe(ϕe)⊤Gµ)ϕe(ϕe)⊤
e µ µ e 1+ 1(ϕe)⊤Gµϕe e µ µ e µ µ
p µ e µ
1
= Gµϕe(ϕe)⊤.
1+χ (µ) e µ µ
X,W
Substitutingin(26)yields:
n
1 (cid:88) 1
Gµϕe(ϕe)⊤−zIG =I,
p 1+χ (µ) e µ µ e
X,W
µ=1
Byconcentrationofχ (µ)andaveragingoverϕe(ϕe)⊤,weexpectthefirsttermtobewell-approximatedby
X,W µ µ
(cid:80) ( 1 Gµϕe(ϕe)⊤).Wetherefore,isolatetheerrorsinχ (µ)toobtain:
µ∈[n] 1+χd(κ) e µ µ X,W
1 (cid:88) 1
( Gµϕe(ϕe)⊤+E Gµϕe(ϕe)⊤+E Gµϕe(ϕe)⊤+E GµR⋆(κ ))−zG =I. (27)
p 1+χ (µ) e µ µ 1,µ e µ µ 2,µ e µ µ 3,µ e µ e
d
µ∈[n]
whereE ,E ,E accountfortheerrorinχ (µ)duetorandomnessoverϕ ,approximationofcovarianceR by
1,µ 2,µ 3,µ X,W µ µ
(cid:16) (cid:17)
R⋆ andtheconcentrationofTr GµR⋆ toχ(κ)respectively.
µ e µ
(ϕe)⊤Gµϕe)−
Tr(Gµ eRµ)
E = µ e µ p
1,µ
(1+ 1(ϕe)⊤Gµϕe)(1+
Tr(Gµ eRµ)
)
p µ e µ p
Tr(Gµ eRµ)
−
Tr(Gµ eR µ⋆)
E = p p
2,µ
(1+
Tr(Gµ eR µ⋆)
)(1+
Tr(Gµ eR µ⋆)
)
p p
Tr(GµR⋆)
e µ −χ(κ)
E = p
3,µ
(1+χ(κ))(1+
Tr(Gµ eR µ⋆)
)
p
Next,duetotheaverageovernsamplesandthesmallerrorinreplacingGµbyG ,wefurtherexpect(cid:80) ( 1 Gµϕe(ϕe)⊤)
e e µ∈[n] 1+χ(κ) e µ µ
tobewell-approximated(inthesenseofdeterministicequivalence)byG ((cid:80) 1 R⋆).
e µ∈[n] 1+χ(κ) µ
Therefore,weexplicitlyintroducetheabovetermintoEquation(27)atthecostofadditionalerrors:
1 (cid:88) 1
G ( R⋆)−zG =I+∆,
p e 1+χ(κ) µ e
µ∈[n]
where:
(cid:88)
∆= ∆ ,
µ
µ∈[n]
where:
(cid:18) (cid:19)
1 1 1
∆ = G R⋆ −Gµ ϕe(ϕe)⊤−GµE ϕe(ϕe)⊤−GµE ϕe(ϕe)⊤−GµE ϕe(ϕe)⊤
µ p 1+χ (µ) e µ e1+χ (µ) µ µ e 1,µ µ µ e 2,µ µ µ e 3,µ µ µ
W W
Withtheaboveerrortermsdefined,we’renowreadytoanalyzethelinearfunctionalTr(cid:0)
AG
(z)(cid:1)inEquation(25).
e
Weperformtheanalysisintwo-stages:
• ShowthatTr(cid:8) AG (z)(cid:9) →Tr(cid:8) AG (z)(cid:9).
e w
• ShowthatTr(cid:8) AG (z)(cid:9)concentratesarounditsexpectation.
e
Westartwiththefirststage.Wehave:
E(cid:2)
Tr(AG
)(cid:3) =E(cid:20) 1 Tr(cid:110) (A(I+∆)(αR⋆−zI)−1)(cid:111)(cid:21)
,
e p
23whereR⋆ :=(cid:80) 1 R .
Ournextgoaµ l∈ i[ sn t] h1 e+ rχ efWo( rµ e) toµ
boundthecontributionfrom∆.
TheindependenceofGµandϕe,impliesthat:
e µ
(cid:20) (cid:21) (cid:20) (cid:21)
1 1
E Gµ ϕe(ϕe)⊤ =E Gµ R
e1+χ (µ) µ µ e1+χ (µ) µ
W W
Thus,wehave:
E(cid:20) 1 Tr(cid:0)
A∆
(cid:1)(cid:21) =E(cid:20) Tr(cid:16)
A(E +E +E
)Gµϕe(ϕe)⊤(cid:17)(cid:21)
p µ 1,µ 2,µ 3,µ e µ µ
(cid:124) (cid:123)(cid:122) (cid:125)
T1 (28)
+ 1 1 E(cid:20) Tr(cid:16) AG R⋆(cid:17)(cid:21) − 1 1 E(cid:104) Tr(cid:0) AGµR (cid:1)(cid:105)
1+χ (µ)p e µ p1+χ (µ) e µ
W W
(cid:124) (cid:123)(cid:122) (cid:125)
T2
WestartwiththetermT .WeproceeedbyboundingthecontributionsfromE ,E ,E .
1 1,µ 2,µ 3,µ
First,notethatcyclicityofthetraceimpliesthatfori=1,2,3:
(cid:18) 1 (cid:19) 1 (cid:16) (cid:17)
Tr A(E )Gµϕe(ϕe)⊤ =(E ) Tr (ϕe)⊤AGµϕe
p i,µ e µ µ i,µ p µ e µ
Ourstrategyistoboundthetailbehaviorof(cid:12) (cid:12)E i,µ(cid:12) (cid:12),Tr(cid:16) p1(ϕe µ)⊤AGµ eϕe µ(cid:17) andtranslatethemintoexpectationbounds
ontheproduct.
Letϵ := 1(ϕe)⊤Gµ(ϕe)− Tr(Gµ eRµ) ,ϵ := Tr(Gµ eRµ) − Tr(Gµ eR µ⋆) ,ϵ := Tr(Gµ eR µ⋆) −χ (κ).Then:
1,µ p µ e µ p 2,µ p p 3,µ p d
ϵ
E = 1,µ
1,µ
(1+
Tr(Gµ eRµ)
+ϵ )(1+
Tr(Gµ eRµ)
)
p 1,µ p
ϵ ϵ2
= 1,µ + 1,µ ,
(1+
Tr(Gµ eRµ)
)2 (1+
Tr(Gµ eRµ)
+ϵ )(1+
Tr(Gµ eRµ)
)
p p 1,µ p
andanalogousrelationsholdforE ,E .
2,µ 3,µ
(cid:16) (cid:17) (cid:16) (cid:17)
WestartwithboundingthetermTr 1(ϕe)⊤AGµϕe .SinceTr 1(ϕe)⊤AGµϕe isaquadraticforminthefeatures
p µ e µ p µ e µ
ϕe,onecouldhopetoexploitaHanson-Wrighttypeinequality.Thisrequiresϕe tobe“well-concentrated"insomesense.
µ µ
Fortunately,assumption2.3directlyyieldsthefollowingregularityproperty:
PropositionD.4. Letξ ∼N(0,I )bedefinedthroughthedecomposition:
µ d
x =κw⋆+(I−w⋆(w⋆)⊤)ξ
µ µ
Then,w.h.pasd→∞,Themapξ →ϕe isLipschitz-continuous
µ µ
TheLipschitz-continuityofϕ allowsustoapplyageneralizationoftheHanson-wrightinequalityforLipschitz-maps
µ
ofindependentGaussiani.i.dvectors[LouartandCouillet,2018],leadingtothefollowingresult:
LemmaD.5. ForanysequenceofmatricesM ∈Cp×p,independentofϕe:
µ
(cid:12) (cid:12)
(cid:12)(ϕe)⊤Mϕe −Tr(cid:0) MR (cid:1)(cid:12)=O (∥M∥ ),
(cid:12) µ µ µ (cid:12) ≺ F
ApplyingtheaboveLemmawithM =AGµandnotingthat:
e
∥AGµ∥ ≤∥AGµ∥
e F e Tr
≤∥A∥ ∥Gµ∥
Tr e
1
≤ ∥A∥
ζ Tr
24Therefore,LemmaD.5yields:
(cid:26) 1 (cid:27) ∥A∥ κℓ
Tr ( (ϕe)⊤AGµϕe) =O ( Tr ).
p µ e µ ≺ p
WenextclaimthattoobtainthedesiredboundonTr(A∆),itsufficestoshowthat:
κℓ
ϵ =O (√ ),
i,µ ≺
d
fori=1,2,3.Toseethis,notethatR⋆ ispositivesemi-definiteandGµhasapositiveHermitiancomponent.Therefore,
µ e
theabsolutevaluesoftheterm 1 inE isuniformlyboundedby1, andsimilarlyfor
E ,E
.Therefore,fori=1,2,(1 3+
:
Tr(G pµ eRµ) +ϵ1,µ)(1+Tr(G pµ eRµ)
)
1,µ
2,µ 3,µ
E =O (ϵ +ϵ2 ).
i,µ ≺ i,µ i,µ
ApplyingCauchy-ShwartzinequalitytothesequenceofvariablesE ,(z⊤Az )yields:
i,µ µ µ
(cid:20)(cid:12) (cid:12)(cid:21)
1 1
E (cid:12)E ,(z⊤Az )(cid:12) =O(κℓ √ ),
(cid:12) i,µ µ µ (cid:12) p d
√
forsomeℓ∈Nandi=1,2,3.Subsequently,combiningwithsup (κ )=O( logn)andsummingoverµ=1···n,
µ µ
weobtain:
polylogd
E[T ]=O( √ )
1
d
Inlightoftheabovediscussion,wenowmovetoboundingϵ fori=1,2,3:
i,µ
PropositionD.6. Letϵ := 1(ϕe)⊤Gµ(ϕe)− Tr(Gµ eRµ) ,ϵ := Tr(Gµ eRµ) − Tr(Gµ eR µ⋆) ,ϵ := Tr(Gµ eR µ⋆) −χ (κ).
1,µ p µ e µ p 2,µ p p 3,µ p d
Thenfori=1,2,3,∃ℓ∈Nsuchthat:
κℓ
ϵ =O (√ )
i,µ ≺
d
Proof. We start with ϵ . Since 1(ϕe)⊤Gµ(ϕe) −
Tr(Gµ eRµ)
corresponds to the deviation of the quadratic form
1,µ p µ e µ p
(ϕe)⊤Gµ(ϕe)fromit’smean,wemayagainapplythegeneralizedHanson-Rightinequality(LemmaD.5). Notethat
µ e µ
√
∥Gµ∥ ≤ 1 byLemmaB.4implying∥Gµ∥ ≤ p.SineGµisindependentofϕe,LemmaD.5yields:
e 2 ζ e F ζ e µ
κℓ
ϵ =O (√ )
1,µ ≺
d
ϵ isdirectlyboundedthroughPropositionD.1
2,µ
Boundingthethirdtermcontainingϵ ismorechallengingandwilltakeupthebulkoftheremainingdiscussion.
3,µ
(cid:16) (cid:17)
Toboundϵ ,werequireestablishingtheconcentrationofTr GµR⋆ aroundχ (κ),bothw.r.tX,W (recallthe
3,µ e µ d
definitionofχ(κ)inEquation(24)).
AstandardwayofachievingthisisthroughtheuseofMartingale-basedarguments[BaiandZhou,2008,Chengand
Singer,2013,Hastieetal.,2022].Weproceedthroughasimilartechnique,however,ouranalysisiscomplicatedbythe
presenceofstructuredcovarianceR⋆ andthejoint-randomnessoverX,W.
µ
LemmaD.7. Almostsurelyoverκ ∼N(0,1):
µ
(cid:12) (cid:12) (cid:12) (cid:12)Tr(cid:16) Gµ eR µ⋆(cid:17) −χ d(κ µ)(cid:12) (cid:12) (cid:12) (cid:12)=O ≺(√κℓ µ d),
Proof. As mentioned above, the proof follows through a martingale argument. Concretely, proceed by succesively
applyingBurkholder’sinequalityw.r.tthedoobmartingalesonfiltrationsgeneratedbyX,W respectively.Wefirststart
byconditioningonthefollowinghigh-probabilityeventoverW:
1
E ={W :∥W∥ =O(1),⟨w ,w ⟩=O (√ )}.
w 2 i j ≺ d
Itiseasytocheckthatthemomentsofχ (κ)arebounded,andtherefore,theerrorintheexpectationuponrestriction
d
toE canbeboundedbyO (1k )forarbitrarilylargek.
w ≺ d
25LetE denotetheconditionalexpectationw.r.tthesigma-algebrageneratedbyx . Weapplythefollowing
µ µµ′<µ
martingaledecomposition:
Tr(G
R⋆)−E(cid:2)
Tr(G
R⋆)(cid:3)
e κ e κ
n
(cid:88)
= E Tr(G R⋆)−E Tr(G R⋆).
µ e κ µ−1 e κ
µ=1
Lete =E Tr(G R⋆)−E Tr(G R⋆).Wehave:
µ µ e κ µ−1 e κ
E (Tr(G R⋆)−E Tr(G R⋆))=E (Tr(G R⋆)−Tr(GµR⋆))−E (Tr(G R⋆)−Tr(GµR⋆)),
µ e κ µ−1 e κ µ e κ e κ µ−1 e κ e κ
whereweusedthatE Tr(GµR⋆))=E Tr(GµR⋆))sinceGµdoesnotdependonx
µ−1 e κ µ e κ e µ
ApplyingtheSherman-Morrisonformulayields:
1 1
E Tr(G R⋆)−Tr(GµR⋆)=− E (ϕe)⊤R⋆ϕe.
µ e κ e κ p µ1+(ϕe)⊤Gµϕe µ κ µ
µ e µ
(cid:12) (cid:12)
Now,(cid:12) (cid:12) (cid:12)1+(ϕe µ)1 ⊤Gµ eϕe µ(cid:12) (cid:12) (cid:12)≤1while(ϕe µ)⊤R κ⋆ϕe µhasmomentsboundedpolynomiallyinκbyLemmaD.5.
Therefore,LemmaB.5combinedwithMarkov’sinequalityforlargeenoughp,implies:
(cid:12) (cid:12)
(cid:12) (cid:12) (cid:12)p1 Tr(cid:16) Gµ eR µ⋆(cid:17) −χ W(κ)(cid:12) (cid:12) (cid:12)=O ≺(√1 d)
Where:
χ
(κ)d =efE (cid:2)
Tr(G
R⋆)(cid:3)
,
W X e κ
wheretheexpectationisonlyw.r.tthematrixX.
Itremainstoshowbyestablishingconcentrationw.r.tW that:
(cid:12) (cid:12) 1
(cid:12)χ W(κ)−χ(κ)(cid:12)=O ≺(√ ),
d
Weagainconditiononthefollowinghigh-probabilityevent:
√
E ={X :∥W∥ =O( d),⟨w ,w ⟩=O (1)}.
x 2 i j ≺
Weagainapplyamartingaleargument,exceptthatunliketheconcentrationw.r.tX,bothG ,R⋆nowdependonW.
e
Let E denote the conditional expectation w.r.t the sigma-algebra generated by w . Let G (−i) denote the
i jj<i e
extendedresolventobtainedaftertheremovalofthe(k+1+i)throwandcolumnin(Φe)⊤Φe,exceptforthediagonal
(k+1+i),(k+1+i)entry.Notethatthiscorrespondstoafinite-rankperturbation:
1
(Φe)⊤Φe−e b⊤−b e⊤ ,
p (k+1+i) i i (k+1+i)
whereb⊤
i
denotesthe(normalized)(k+1+i)throwof(Φe)⊤Φegivenby:
1
b⊤ = (Φe)⊤(Φe)
i p i:
Analogously, letR κ⋆(−i)beobtainedbytheremovalofthe(k+1+i)th rowandcolumninR κ⋆ (exceptforthe
diagonal(k+1+i),(k+1+i)entry).
UsingtheWoodbury-matrixidentity,wehave:
(cid:20) (cid:21)
G =G (−i)−G
(−i)(cid:2)
e b
(cid:3)
(
0 1 +Ψ)−1(cid:2)
e b
(cid:3)⊤
G (−i),
e e e (k+1+i) i 1 0 (k+1+i) i e
whereΨ=(cid:2) e b (cid:3)⊤ G (−i)(cid:2) e b (cid:3) ∈R2×2
(k+1+i) i e (k+1+i) i
26Lete =E E (cid:2) Tr(G R⋆)(cid:3) −E E (cid:2) Tr(G R⋆)(cid:3).WeexpressTr(G R⋆)as:
i i x e κ i−1 x e κ e κ
Tr(G
R⋆)=Tr(cid:0)
G
(−i)R⋆(−i)(cid:1)
+∆,
e κ e κ
where∆arisesfromthelow-rankprojections.
ToApplyLemmaB.5, itremainstoboundthesecondmomentsoftheresidualterms∆. Itiseasytocheckthat
conditionedonthehigh-probabilityeventsE w,E x,for √anyuniformlyLipschitzf : Rp → R,f(√1 pb i)isauniformly
Lipschitzmapofw .Therefore,LemmaB.11impliesthat pb isaLipschitz-concentratedvectorinthesenseofDefinition
i i
9inCouilletandLiao[2022].ThereforethegeneralizedHanson-Wrightinequalityimpliesthat:
(cid:12) (cid:12)
(cid:12) (cid:12) (cid:12)b⊤
i
G e(−i)b i−E(cid:104) b⊤
i
G e(−i)b i(cid:105)(cid:12) (cid:12) (cid:12)=O ≺(√1 d).
Similarly,weobtainthattheremainingentriesofΨconcentratearoundtheirexpectations.Asaresult,δdecomposesinto
quadraticformsine ,b whichcanbeboundedasO (1).ApplyingLemmaB.5thencompletestheproof.
(k+1+i) i ≺ p
TheproofofLemmaD.7completestheproofofPropositionD.6andthusboundsT inEquation(28)
1
WenowreturntoboundingthetermT inEquation(28).Letζ beasdefinedinLemmaB.4First,PropositionD.1and
2
LemmasB.4,B.8implythat:
(cid:12) (cid:12)
(cid:12) (cid:12) (cid:12)Tr(cid:0) AGµ eR µ(cid:1) −Tr(cid:16) AGµ eR µ⋆(cid:17)(cid:12) (cid:12) (cid:12)≤O ≺(√1 d).
Therefore,wemayreplaceR µbyR µ⋆ atthecostofanerrorO ≺(√1 d).
Next,weapplytheSherman-MorrisonformulatoG −Gµtoobtain
e e
(cid:20) (cid:16) (cid:17)(cid:21) (cid:20) (cid:16) (cid:17)(cid:21) 1 (cid:34) 1 (cid:16) (cid:17)(cid:35)
E Tr AG R⋆ −E Tr AGµR⋆ = E Tr Aϕe(ϕe)⊤R⋆
e µ e µ p 1+(ϕe)⊤Gµϕe µ µ µ
µ e µ
Bythecyclicityoftrace,thesecondtermcanbeexpressedasthequadraticform(ϕe)⊤R⋆Aϕe
µ µ µ
Westartbynotingthat,LemmasB.4B.8imply:
E(cid:104) Tr(cid:0)
AG eR
µ(cid:1)(cid:105) ≤(cid:13)
(cid:13)AG eR
µ(cid:13)
(cid:13)
Tr
≤∥A∥
Tr(cid:13)
(cid:13)G eR
µ(cid:13)
(cid:13)≤
C
ζ
,
forsomeconstantC.
Therefore,usingLemmasB.3andB.8,weobtain::
(cid:12) (cid:12)
(cid:12) (cid:12) (cid:12)1+χ1 (µ)E(cid:104) Tr(cid:0) AG eR µ(cid:1)(cid:105) − 1+χ1 (µ)E(cid:104) Tr(cid:0) AGµ eR µ(cid:1)(cid:105)(cid:12) (cid:12) (cid:12)=O ≺(∥A p∥ Tr).
W W
Wethereforeobtain:
E(cid:104) Tr(cid:0)
A∆
(cid:1)(cid:105) =O(√1
)
µ
d
Theconvexityof||thenimpliesthat
E(cid:2)
Tr(AG
)(cid:3) =E(cid:20) Tr(cid:110) (A(αR⋆+λI)−1)(cid:111)(cid:21) +O(pol √ylogd
).
e
d
Next,weshowthattheaveragingoverκallowsustoreplaceR⋆withΣ⋆:
LemmaD.8. Thereexistsℓ∈Nsuchthatalmostsurelyoverκ:
χ (κ)=O(κℓ)
µ
Proof. RecallthatGe doesn’tdependonκ,whileR⋆ dependsonκonlythroughscalars,c (κ,u )forj =1,2,3.The
µ µ j i
claimthendirectlyfollowsusingassumption2.3.
27Sinceκ forµ∈[n]areindependentGaussians,theaboveLemmaimpliesthatthecovariance(cid:80) 1 R⋆(κ )
µ µ∈[n] 1+χW(µ) µ
canfurtherbyreplacedbyΣ⋆inpropositionD.3withadditionalerrorO(√1 ).
d
It remains to show that Tr(G A) concentrates around its expectation w.r.t X. This follows from a martingale
e
argumentidenticaltothefirstpartoftheproofofLemmaD.7,withtheroleofR⋆beingreplacedbyA.Then,wehave
thefollowingmartingaledecomposition:
n
Tr(G A)−E(cid:2) Tr(G A)(cid:3) = (cid:88) E Tr(G A)−E Tr(G A).
e e µ e µ−1 e
µ=1
Lete =E Tr(G A)−E Tr(G A).Wehave:
µ µ e µ−1 e
E (Tr(G A)−E Tr(G A))=E (Tr(G A)−Tr(GµA))−E (Tr(G A)−Tr(GµA))
µ e µ−1 e µ e e µ−1 e e
whereGµdenotestheresolventwiththeµ exampleremoved.Next,exactlyasinLemmaD.7,weapplythegeneralized
e th
Hanson-Wrightinequalitytoshowthat:
1
(Tr(G A)−Tr(GµA))=O ( ).
e e ≺ p
LemmaB.5forp=4andMarkov’sinequalitythenimplythat,withhigh-probability:
Tr(G A)=E(cid:2) Tr(G A)(cid:3) +O (∥A √∥ Tr),
e e ≺
d
completingtheproofofPropositionD.3
D.3 SecondstageofDeterministicEquivalent(AveragingoverW)
SubstitutingR⋆ fromPropositionD.1,weobtainthatΣ∗inpropositionD.3possesesthefollowingstructure:
µ
(cid:32) (cid:33)
A∗ (A∗ )⊤⊙θ⊤
Σ∗ = 11 21
θ⊙A∗
21
(Ve⊙WW⊤+diag(νe)))+αSe⊙θθ⊤,
where:
(cid:20) (cid:21)
α
A∗ =E R⋆ (κ)
11 κ 1+χ(z;κ) 11
 (cid:34) (cid:35)
α σ2(κ) σ (κ)c (κ,u )⊤
=E
κ 1+χ(z;κ) σ
(κ)c⋆
(κ,u ) c
(κ⋆
,u
)0
c
(κ,π
u )⊤ 
⋆ 0 π 0 π 0 π
(cid:20) (cid:21)
α
A∗ =E R⋆ (κ)
11 κ 1+χ(z;κ) 21
andV,ν ,S aredefinedas:
e
(cid:20) (cid:21)
(c (κ,u )c (κ,u )
V(d)(ζ)=E α 1 i 1 j
i,j κ 1+χ(s,m)
 
ν i(d)(ρ)=E κ((cid:88) 1α +c2 k χ( (κ κ, ,u Wi) )
k≥2
(cid:34) (cid:20) (cid:21)(cid:35)
(c (κ,u )c (κ,u )
S =E (κ2−1)E 1 i 1 j ,
κ 1+χ(κ)
withκ∼N(0,1)throughout.
28NotethatA∗ ,A∗ aredeterministic,whilethetermsdependentonθ,includingδ contributefinite-rankspikes.
11 21
Therefore,thebulkstatisticsofΣ⋆ariseoutofthetermV ⊙WW.
e
Toaverage-outtherandomnessoverW in,itwillbeconvenienttofirstextractadeterministicequivalentforthe
followingmatrix:
M∗ =(V(d)⊙WW⊤+diag(ν(d))−zI )−1
e e p
whereV ,diag(ν(d)) denotetheextendedblock-structuredmatricesasperdefinition4.4.
e e
Additionally,toexpressχ (κ)self-consistentlyintermsofV,ν,wewillrequirethefollowingadditionalfunctional:
W
1/d∗Tr(cid:0) e ⊙WM∗W ⊙e (cid:1)
i j
LemmaD.9(DeterministicEquivalentforBlock-StructuredWishart). LetC(z)∈Ck×k,D(z)∈Ckbeanalyticmappings
suchthatC(z) : C+ → C− andD(z) : C+ → C− entry-wisewith(cid:12) (cid:12)C i,j(cid:12) (cid:12),|D i|uniformlyboundedbysomeconstant
independentofζ. Furthermore, supposethatD(z)isdiagonal. LetC (z),D ∈ Cp×p denotetheextendedmatricesas
e e
definedinDefinition4.4.LetR denotetheblockstructuredresolventdefinedas:
C,D
R d =ef ((C )⊙W˜0(W˜0)⊤+diag(D )−zI )−1.
C,D e e p
DefineM∗(C ,D )asthediagonalmatrix:
e e
b⋆
M∗(C ,D ):=diag( ),
e e πβ
whereabove,b⋆,π ∈Rk aredividedelement-wiseandb⋆satisfiesthefollowingself-consistentequation:
b⋆(C,D) =π β((C−1+diag(b⋆))−1+(diag(D)−zI ))−1,
q ui p q,q
forq ∈[p].ThenforanysequenceofmatricesA∈Cp×p:
(cid:12) (cid:12)Tr{AM⋆}−Tr(cid:8) AR (cid:9)(cid:12) (cid:12)=O (∥A √∥ Tr)
(cid:12) C,D (cid:12) ≺
d
Furthermore,theexpression:
(cid:16) (cid:17)
K∗ (C,D)=1/d∗Tr ei⊙WM∗W ⊙ej
i,j
satisfiesthefollowingdeterministicequivalence:
1
K∗ =ψ(C,D)+O (√ )
≺
d
whereψ(C,D)isdefinedas:
ψ(C,D)=b⋆(C,D)−L(C,D)⊙(b⋆(C,D)(b⋆(C,D))⊤), (29)
with:
(cid:16) (cid:17)−1
L(C,D)= (C)−1+diag(b⋆(C,D)) .
Proof. TheproofreliesontheobservationthatduetotheblockstructureinV,removingacoordinateinWW⊤results
inafiniterankperturbationtoM⋆.Wethenproceedthroughaleave-oneoutargumentsimilartosectionD.Theproof
isdeferredtosectionF.
InlightofLemmaD.9,weobtainthefollowingcandidateforthedeterministicequivalenttoG (z):
w
α
( Σ˜⋆−zI),
p
whereΣ˜⋆isobtainedbyreplacing(V(d)⊙WW⊤+diag(ν(d))+λI )=(M∗)−1byM⋆(V,diag(ν))−1.
e e p
However,whileobtainingM⋆(V,diag(ν))−1,weaveragedoverthedependenceonθ = Ww⋆ inadditiontothe
remainingcomponentsofW.Thisisundesirablesincethedependenceonθcapturesthecorrelationsamongstblocks
Σ ,Σ intheextendedresolventwhichwillberelevantforthecharacterizationofthegeneralizationerror.Weobtain
22 21
backthisdependencethroughinthefollowingresult:
29LemmaD.10. LetM⋆ =(V(d)⊙WW⊤+diag(ν(d))+λI )−1.Define:
d e e p
b⋆ b⋆ b⋆
Mθ :=(diag( ) −(V−1+diag(b⋆))−1⊙( ) ( )⊤⊙θθ⊤),
d πβ e d πβ e πβ e
whereψ(V ,ν )isasdefinedinLemmaD.9.ForanysequenceofmatricesA∈Cp×p,possiblydependentonθ =Ww⋆:
d d
Tr(cid:8) (M⋆A)(cid:9) =Tr(cid:110) (MθA)(cid:111) +O (∥A √∥ Tr). (30)
d d ≺
d
Proof. Toobtainbackthedependenceonθ,wewrite:
M∗ =(V(d)⊙(W (W )⊤)+V(d)⊙θθ⊤)+diag(ν(d))+λI )−1,
d e ⊥ ⊥ e e p
whereW =W −θ(w⋆)⊤denotesthecomponentsoftheweightsupontheremovalofthecomponentsθalongw⋆.
⊥
Next,notethatV(d)⊙θθ⊤isafinite-rankperturbationalongdirectionse1,··· ,ekdefinedinEquation(6).Concretely,
e
letE ∈Rp×k denotethematrixwithcolumnse1,···ek anddefineE =θ⊙E.Then,V(d)⊙θθ⊤canbeexpressedas:
θ e
V(d)⊙θθ⊤ =E V E⊤. (31)
e θ d θ
Therefore,weapplytheWoodburymatrixidentitytoobtain:
M∗ =M˜ −M˜E (V−1+E⊤M˜E )−1E M˜
θ d θ θ θ
, whereM˜ = (V(d) ⊙(W (W )⊤)+diag(ν(d))−zI )−1 denotestheinverseupontheremovalofcomponents
e ⊥ ⊥ e p
alogθ.Bytherotationalinvarianceofw ,M˜ asymptoticallysharesthedeterministicequivalentforM⋆ describedin
i
LemmaD.9.SinceM˜ isindependentofθ,andθ areasymptoticallydistributedasN(0,1),E⊤M˜E inturnsimplifies
i d θ θ
todiag(b⋆(V ,ν )).WethenreplacetheoccurancesofM˜ withM⋆(V ,ν )= b⋆ toobtain:
d d d d πβ
b⋆ b⋆ b⋆
diag( )−diag( ) E (V−1+diag(b⋆))−1E diag( ) .
πβ πβ e θ d θ πβ e
Subsequently,analogoustoEquation(31),wehavethefollowingequivalentrepresentationofthemiddle-block:
E (V−1+diag(b⋆))−1E =(V−1+diag(b⋆))−1⊙θθ⊤.
θ d θ d e
Wethusobtain:
b⋆ b⋆ b⋆
M⋆ ≃diag( )−(V−1+diag(b⋆))−1⊙( ) ( )⊤⊙θθ⊤
πβ d πβ e πβ e
where≃denotesdeterministicequivalenceinthesenseofEquation(30),whichfollowsfrompropositionD.9.
Wecanfinallyclaimthesecondlevelofdeterministicequivalence,byreplacingG (z)withasequenceofmatrices
W
dependingonlyonθ,u:
PropositionD.11. Considerthesequenceof(random)resolventsdefinedby:
(cid:18)
α
(cid:19)−1
G (z)= Σ⋆−zI ,
W p
,and:
(cid:32) (cid:33)
A∗ −zI (A∗ )⊤⊙θ⊤
G˜ e(z)= 1 θ1 ⊙A∗k+1 (b⋆ − b⋆(V−1+21 diag(b⋆))−1⊙θθ⊤b⋆)
21 πβ πβ d πβ e πβ
Then,foranysequenceofdeterministicmatricesA:
(cid:12) (cid:12)
(cid:12) (cid:12)Tr(cid:0) G W(z)A(cid:1) −Tr(cid:16) G˜ e(z)A(cid:17)(cid:12) (cid:12)=O ≺(∥ √A∥ tr)
(cid:12) (cid:12) d
30Proof. ApplyingLemmaB.3twice,weobtain:
α
G W(z)−G˜ e(z)= pG W(z)((M d∗)−1−Mθ d−1 )G˜ e(z)
α
pG W(z)(M d∗)−1(M∗−Mθ d−1 )Mθ d−1 G˜ e(z)
ByLemmaB.4,(cid:13) (cid:13)G W(z)(cid:13) (cid:13)(cid:13) (cid:13) (cid:13)G˜ e(z)(cid:13) (cid:13) (cid:13)areboundedby ζ1 whilethenormsof(cid:13) (cid:13)(M d∗)−1(cid:13) (cid:13),(cid:13) (cid:13) (cid:13)(Mθ)− d1(cid:13) (cid:13) (cid:13)areboundedbyconstants
duetoLemmaB.12.Therefore:
(cid:12) (cid:12) (cid:12) (cid:12)
(cid:12) (cid:12) (cid:12)Tr(cid:0) G W(z)A(cid:1) −Tr(cid:16) G˜ e(z)A(cid:17)(cid:12) (cid:12) (cid:12)≤ ζC 2(cid:12) (cid:12) (cid:12)Tr(cid:0) G W(z)M⋆(cid:1) −Tr(cid:16) G˜ e(z)M dθ(cid:17)(cid:12) (cid:12)
(cid:12)
ApplyingLemmaD.10thencompletestheproof.
D.4 Self-consistentEquationforχ(κ)
G˜(z)isalmostidenticaltothedesiredequivalentmatrixG (z)inTheorem4.6,exceptforχ (κ)stillbeingdimension-
e d
dependentunknowns.WeresolvethisbyutilizingD.3toobtainaself-consistentequationforχ(κ)
LemmaD.12. Letψ,b⋆beasdefinedinLemmaD.9.Then,almostsurelyoverκ∼N(0,1),thefollowingholds:
(cid:88) (cid:88) (cid:88) κℓ
χ (κ,z)= ψ (V ,ν )c (κ,ζu)c (κ,ζu)+ b⋆((V ,ν )) c2(κ,ζu)+O (√ ).
d qq′ d d 1 q 1 q′ q d d ℓ q ≺
d
q,q′∈[k] q∈[k] ℓ≥2
(cid:20) (cid:16) (cid:17)(cid:21)
Proof. We first recall that ∥R⋆∥ = O (κℓ) for some ℓ ∈ N. Now, note that χ (κ) = E Tr G˜ (z)R⋆ involves
κ 2 ≺ d e κ
dependencyonW inR⋆.Therefore,wecannotdirectlyapplyPropositionD.3andmustresorttoPropositionD.3.we
κ
have:
χ
(κ)=E(cid:20) 1 Tr(cid:0)
G
(z)R⋆(cid:1)(cid:21)
+O
(√κℓ
)
d p W κ ≺ d
WestartbyexpressingG (z)throughaSchur-complementdecomposition:
W
(cid:34) (cid:35)
C −C Q⊤
G (z)= W W W , (32)
W −Q C G˜ +Q C Q⊤,
W W W Q W W
,where:
G˜ =((V ⊙WW⊤+diagν +λI ))+αS⊙θθ⊤−zI)−1,
W e d p
andC ,Q inEquation(32)contributeonlyfinite-rankcomponentsofboundedoperatornorm.Since∥R⋆∥ =O (κℓ),
W W κ 2 ≺
suchcomponentscontributeO(1)toE(cid:104) 1Tr(cid:8) G (z)R⋆ (κ)(cid:9)(cid:105) andcanthereforebeignored.We’releftwith:
p p W 22
1 (cid:20) (cid:110) (cid:111)(cid:21)
E Tr G˜ R⋆ (κ)
p W 22
,whichevaluatesto:
(cid:20) (cid:110) (cid:111)(cid:21)
E Tr (c (κ,u)c (κ,u)⊤)⊙WW⊤)(V ⊙WW⊤+diag(ν )+λI )−1
1 1 e d p
(cid:124) (cid:123)(cid:122) (cid:125)
T1
  
 (cid:18) (cid:88) (cid:19)  1
+ETr diag( c2(κ,u) (V ⊙WW⊤+diag(ν )+λI )−1 +O( ),
 k e d p  p
 
k≥2
(cid:124) (cid:123)(cid:122) (cid:125)
T2
(cid:18) (cid:19)
whereweagainsupressedcontributionsfromfinite-rankterms.Sincediag( (cid:80) c2(κ,u) isindependentofW,by
k≥2 k
LemmaD.9,T simplifiesto:
2
(cid:88) (cid:88)
b⋆((V ,ν )) c2(κ,ζu)
q d d ℓ q
q∈[k] ℓ≥2
WhileT canbedecomposedintotermsoftheformTr(cid:0) ei⊙WM∗W ⊙ej(cid:1),whichconvergetoψ(V,ν)byLemmaD.9.
1
31D.5 Self-consistentequationsforV⋆,D⋆
UsingLemmaD.9,weobtainadeterministicequivalentG′ thatdoesn’tdependontherealizationsofX,W.However,the
e
quantitiesV ,D stilldependondimensiondduetothedimensiondependentdefinitionofχ (κ).WhileD.12definesa
d d d
self-consistentequationforχ(κ),itremainsaninfinite-dimensional(functional)orderparameter.Instead,weshowthat
wecandirectlyconstructdimension-independentself-consistentequationsonV ,ν :
d d
PropositionD.13. ForC,D ∈Rk,k,letψ beasdefinedinTheorem4.6.Define:
i,j
k k
1 (cid:88) 1 (cid:88) (cid:88)
χ (κ)= ψ (C,D)c (u ,κ)c (u ,κ)+ (b ( c2(κ,u ))
C,D β i,j 1 i 1 j β ii k i
i=1,j=1 i=1 k≥2
Then,V,Ddefinedas:
(cid:20) (cid:21)
α
V =E c (u ,κ)c (u ,κ) ,
1+χ (κ) 1 i 1 J
d
 
α (cid:88)
D =E 
1+χ (κ)
c2 k(u i,κ)
d
k≥2
satisfy:
(cid:34) (cid:35)
α 1
V =E c (u ,κ)c (u ,κ) +O (√ )
i,j 1+χ V,D(κ) 1 i 1 J ≺ d
 
α (cid:88) 1
D i,i =E  1+χ V,D(κ) c2 k(u i,κ)+O ≺(√ d).
k≥2
Proof. Notethat α isuniformlyLipschitzinκ.Considerindependentsampleκ ,··· ,κ′ forsomen′ ∝d.Then
√ 1+χV,D(κ) 1 n
sup|κ |= logdand:
i
(cid:34) (cid:35) n′
α 1 (cid:88) α 1
E c (u ,κ)c (u ,κ) = c (u ,κ)c (u ,κ)+O (√ ).
1+χ V,D(κ) 1 i 1 J n′ 1+χ V,D(κ i) 1 i 1 J ≺ d
i=1
PropositionD.13thenfollowsfromPropositionD.11andtheself-consistentequationforχ(κ)inLemmaD.12.
Next,weshowthattheabovefixedpointequationsarecontractive,allowingustotranslateapproximatesatisfiability
todistanceofV ,D fromtheuniquefixedpointssatisfiedbyV⋆,D⋆definedinTheorem4.6:
d d
LemmaD.14. Letz ∈C+.DefineS(z)asthesetofC,D ∈C−k×k ×C−k satisfying:
• b⋆(C,D)∈C+.
• (cid:12) (cid:12)b⋆(C,D)(cid:12) (cid:12)≤ πi
βζ
Then,thereexistsC >0suchthatforζ =Im(z)>C,thefixedpointiterationdefinedinPropositionD.13i.e:
F
:C−k×k ×C−k →C−k×k ×C−k
(cid:34) (cid:35)  
α α (cid:88)
F(C,D)=E
1+χ
(κ)c 1(u,κ)c 1(u,κ)⊤ ,E 
1+χ (κ)
c2 k(u,κ),
V,D C,D
k≥2
iscontractiveinS(z),i.eforanyC,D:
(cid:13) (cid:13) (cid:13) (cid:13)
(cid:13)F(C 1,D 1)−F(C 2,D 2)(cid:13)≤(cid:13)(C 1,D 1)−(C 2,D 2)(cid:13)
32Proof. Fromthedefinitionofχ(κ),wedirectlyobtainthatforanyC,Dsuchthat|b|≤ βπ
ζ
(cid:12) (cid:12)
(cid:12)χ(κ,b 1)−χ(κ,b 2)(cid:12)≤C 1∥b 1−b 2∥+C 2|b|∥C 1,D 1−C 2,D 2∥
Therestriction(cid:12) (cid:12)b⋆ i(z)(cid:12) (cid:12)≤ βπ ζi thereforeimpliesthat(cid:12) (cid:12)χ C,D(κ)(cid:12) (cid:12)≤ K
ζ
forsomeK >0.Now,since:
(cid:12) (cid:12)
(cid:12) (cid:12) (cid:12)1+χ1 (κ)(cid:12) (cid:12) (cid:12)≤1−(cid:12) (cid:12)χ V,D(κ)(cid:12) (cid:12),
(cid:12) V,D (cid:12)
forsmallenough(cid:12) (cid:12)χ V,D(κ)(cid:12) (cid:12),weobtainthefollowingentry-wiseupper-boundforany-feasiblesolutionofC,D:
|C|≤K′,|D|≤K′,
forsomeK′ >0.ThisensuresthattheboundednessconditiononC,DinLemmaD.9applies.
Recallthedefinitionofb⋆(C,D):
b⋆(C,D)=π β((C−1+diag(b))−1+(diag(D)−zI ))−1
ui p p,p
Subsequently,wemayapplyLemmaB.3toobtain:(SeealsoLemmaF.1intheproofofLemmaD.9)
(cid:13) (cid:13)
K′′
(cid:13)b(C 1,D 1)−b(C 2,D 2)(cid:13)≤
ζ2
∥C 1,D 1−C 2,D 2∥,
,forsomeconstantK′′ >0CombiningtheabovewithD.5yields:
(cid:12) (cid:12)
K′′′
(cid:12)χ(κ,C 1,D 1)−χ(κ,C 2,D 2)(cid:12)≤
ζ
∥C 1,D 1−C 2,D 2∥
,forsomeconstantK′′′ >0.
D.6 ProofofTheorem4.6
LemmaD.14andtheBanach-fixedpointtheoremimplythatF(C,D)admituniquefixedpointsV⋆(z),ν⋆(z)forζ >C
andthat:
V (z)−−→V⋆(z), ν (z)−−ν →⋆ (z)
d d
a.s a.s
Next,wenotethatfromPropositionD.11,foreachq ∈[k],b⋆(z) canbeexpressedas:
q
(cid:88)p (v⊤eq)2 1
b⋆(z) = i +O (√ ).
q z−λ i ≺ d
i=1
Theorem4.6thenfollowsbynotingthatbythestandardpropertiesofStieltjestransformsBaiandZhou[2008],
b⋆(z),V⋆(z),ν⋆(z)admituniqueanalytic-continuationstoC/R+.
E Generalization Error
Havingobtainedthefull-deterministicequivalentinTheorem4.6,wenowshowhowitcanbeexploitedtoyieldthe
asymptoticgeneralizationerrorafteragradientstep.
Wewillproceedasfollows:
• UsethecovarianceapproximationinPropositionD.1toidentifycertain“statistics"involvingtheresolventthat
characterizetheasymptoticgeneralizationerror.
• Relatethesestatisticstoprojectionsoftheextendentresolventontocertaindeterministicmatrices. Introduce
perturbationtermstoextractmorecomplicatedfunctionals.
• ReplacethesestatisticswiththecorrespondingquantitiesobtainedthroughthedeterministicequivalentG ,and
e
wherenecessaryaverageoverθtoobtaintheasymptoticgeneralizationasafunctionoftheparametersV⋆,ν⋆,b⋆
inTheorem4.6.
33E.1 OrderParametersforGeneralizationError
WestartbyusingthecovarianceapproximationD.1tosimplifytheexpressionforthegeneralizationerrorwithan
arbitraryfixedchoiceofa∈Rp
√
LemmaE.1. Leta ∈ Rp beafixedvectorsuchthat∥a∥/ p = O(1)andlete1,··· ,ek denotethe“spike"directions
definedinEquation(6).Definefori∈[k]:
a⊤e a⊤e ⊙w⋆ a⊤C⊙WW⊤a a⊤Da
τd d =ef √ i, τd d =ef √i , τd d =ef and τd d =ef ,
0,i p 1,i p 2 p 4 p
whereC =E(cid:2) c (κ,u)c (κ,u)⊤(cid:3) andD =E (cid:104) (cid:80) c2(κ,u )(cid:105) .Then,thegeneralizationerrorcanbeexpressedas:
1 1 j,j, κ k≥2 k j
 2
k k
E(cid:2) e g(cid:3) =E σ ⋆(s)−(cid:88) c 0(κ,u j)τ
0,j
−(cid:88) c 1(κ,u j)κτ 1,j)  +τ
3
j=1 j=1
(33)
 
k k
(cid:88) (cid:88)
−E ( c 1(κ,u j)τ 1,j)2+( c 1(κ,u j)τ 2,j)2 +τ 3+O ≺(d−1/2).
j=1 j=1
Proof. Thepredictionatapointxunderthesimplifiedupdatedweightsisgivenby:
k
f(x,W˜,a)=(cid:88) τ ϕ¯j +a⊤ϕ˜
0,j x x
j=1
Thegeneralizationerroristhengivenby:
E(cid:2)
e
(a)(cid:3) =E(cid:104)
[σ
(s)−f(x,W˜,a)]2(cid:105)
g ⋆
E(cid:2)
e
(cid:3)canbeequivalentexpressedthroughthefollowingquadraticformappliedtotheextendedfeaturesϕe:
g µ
E(cid:2) e (a)(cid:3) =E(cid:104) (ϕe)⊤u u⊤ϕe(cid:105)
g µ a a µ
(cid:20) (cid:16) (cid:17)(cid:21)
=E Tr u u⊤ϕe(ϕe)⊤
a a µ µ
whereu =[1,−τ ,a˜]∈Rp+k+1.ByPropositionD.1,wehave:
a 0
(cid:20) (cid:16) (cid:17)(cid:21) (cid:20) (cid:16) (cid:17)(cid:21) polylogd
E Tr u u⊤ϕe(ϕe)⊤ =E Tr u u⊤R⋆ +O( √ ).
a a µ µ a a κ
d
ExpandingeachofthetermsinR⋆ thenyieldsEquation(33).
κ
E.2 ExtendedResolventtoGeneralizationError
From Lemma E.1, we note that obtaining the limiting generalization error requires characterizing the functionals
τd ,τd ,··· whenaˆissetastheridge-regressionestimator:
0,i 1,i
(cid:88) (cid:16) (cid:17)2
aˆ =argmin y −f(x ;a,W1) +λ||a||2
λ i i 2
a∈Rp
i∈[n]
(cid:16) (cid:17)−1
= Φ⊤Φ+λI Φ⊤y.
n
Toextracttheserelevantfunctionals,weintroducecertainperturbationtermsintheextendedresolvent.
G (ρ ,ρ )d =ef(cid:16) (Φe)⊤(Φe)+λI+ρ L+ρ D′(cid:17)−1 ∈R(p+1)×(p+1),
e 1 2 1 2
34wherethematricesD′,L∈R(p+1)×(p+1)aregivenby:
(cid:34) (cid:35) (cid:20) (cid:21)
0 0 0 0
L= k+1,k+1 k+1,p , D′ = k+1,k+1 k+1,p ,
0 C⊙WW⊤ 0 D
p,k+1 p,k+1
withC = E(cid:2) c (κ,u)c (κ,u)⊤(cid:3)andD = E (cid:104) (cid:80) c2(κ,u )(cid:105) . Wewilldemonstratethattheintroductionofthe
1 1 j,j, κ k≥2 k j
perturbationtermsL,D′allowsextractionofalltherelevantfunctionalsinLemmaE.1.
PropositionE.2. Letaˆ denotetheridge-regressionminimizerwithregularizationλ>0i.e:
λ
(cid:88) (cid:16) (cid:17)2
aˆ =argmin y −f(x ;a,W1) +λ||a||2
λ i i 2
a∈Rp
i∈[n]
(cid:16) (cid:17)−1
= Φ⊤Φ+λI Φ⊤y.
n
LetG˜(ρ ,ρ )denotetheresolventofthe“bulk"componenti.e:
1 2
G˜(ρ ,ρ )=(ϕ˜ϕ˜⊤+λI )−1
1 2 n
ConsiderthefollowingSchur-complementdecompositionofG :
e
(cid:34) (cid:35)
C −CQ⊤
G (ρ ,ρ )= ,
e 1 2 −QC P
where
P =G˜(ρ ,ρ )+QCQ⊤,
1 2
Q=G˜(ρ ,ρ )Σ /p,
1 2 21
whereΣ =Φ˜(cid:2) y Φ¯(cid:3) withy d =ef [y1,y2,...,yn],Φ¯ d =ef [ϕ¯1,ϕ¯2,...,[ϕ¯n],and
12
(cid:34) (cid:35)
C−1 = ∥y∥2/p+λ y⊤Φ¯/p − 1(cid:104) y⊤(cid:105) Φ˜⊤G(ρ ,ρ )Φ˜(cid:2) y Φ¯(cid:3) . (34)
y⊤Φ¯/p Φ¯⊤Φ¯/p+λ p 1 2
Letτˆ,τˆ ,τˆ ,τˆ beasdefinedinLemmaE.1,then:
0 1i 2i 3
1
τˆ =(C−1) ((C−1) +λ(1− diag(π )))−1 (35)
0 01 11 p u
(cid:20) (cid:21)
1 τˆ
τˆ =(θ⊙ej)⊤Q +O(√0 )
1,j −τˆ 0 p
τˆ =(cid:2) 1 −τ∗(cid:3) ∂ (cid:104) (C)−1(cid:105) (cid:20) 1 (cid:21) +O(τˆ 02 ) (36)
2 0 ∂ρ 1 ρ=0 −τˆ 0 p
and
τˆ
=(cid:2)
1
−τ∗(cid:3) ∂ (cid:104) (C)−1(cid:105) (cid:20) 1 (cid:21) +O(τˆ 02
).
3 0 ∂ρ 2 ρ=0 −τˆ 0 p
,whereO(τˆ )denoteserrorboundedasC∥τˆ ∥forsomeconstantC >0.
0 0
Proof. Wedecomposeaˆ intoprojectionsalonge1,··· ,ek andtheorthogonalcomplement,denotedasa˜=(I−Π)a,
λ
whereΠdenotestheprojectionoperatoralonge1,··· ,ek.
Theridge-regressionobjectivecanbere-expressedas:
R(τ 0,a˜)= τ0m ∈Rin
k,a˜
µ(cid:88) =n 1(cid:0) y µ−ϕ¯⊤ µτ 0−a˜⊤ϕ¯ µ(cid:1)2 +λ(cid:13) (cid:13) (cid:13) (cid:13)√1
π
·τ 0(cid:13) (cid:13) (cid:13) (cid:13)2 +λ∥a˜∥2, (37)
where·denoteselement-wisemultiplicationbyπ ∈Rk,accountingforthevariabilityinthenumberofentrieswith
valueζuforq ∈[k].
q
35Theoptimalityconditioncansimilarlybeexpressedintermsofτ ,a˜.DifferentiatingtheobjectiveinEq.(37)w.r.tτ
0 0
yields:
τˆ =((cid:88) ϕ¯ y −(cid:88) ϕ¯ ϕ˜⊤G˜ϕ˜ y )((cid:88) ϕ¯ ϕ¯⊤−(cid:88) ϕ¯ ϕ˜ G˜ϕ˜ ϕ¯ +λdiag(π))−1.
0 µ µ µ µ µ ν µ µ µ µ µ µ
µ µ,ν µ µ,ν
Similarly,differentiatingw.r.ta˜,weobtain:
(cid:88) ϕ¯ ϕ¯⊤τ +λdiag(1/π)τ =(cid:80) ϕ¯ y −(cid:80) ϕ¯ ϕ˜ a˜
µ µ 0 0 µ µ µ µ µ µ
µ
Simplifying,weobtainthatorthogonalcomponentoftheminimizera˜isgivenby:
a˜=G˜(cid:88) ϕ˜ (y −ϕ¯⊤τ ),
µ µ µ 0
µ
whereG˜ isdefinedasbefore:
G˜ d =ef ((cid:88) ϕ˜ (ϕ˜ )⊤+λI)−1.
µ µ
µ
Therefore,weobtain:
(cid:20) (cid:21)
1
a˜=Q ,
−τˆ
0
Isolatingthecontributionsfromtheprojectionsalonge1,··· ,ek inτ ,i,weobtain:
1
a⊤e ⊙w⋆ a˜⊤e ⊙w⋆ τˆ
√i = √i +O(√0 )
p p p
ComparingtheabovewithEquation(34),weobtainEquation(35).
Theexpressionsforτˆ ,τˆ thenfollowdirectlyfromtheexpressionsfor ∂ (cid:2) (C)−1(cid:3) , ∂ (cid:2) (C)−1(cid:3)
2i 3 ∂ρ1 ρ=0 ∂ρ2 ρ=0
E.3 DeterministicEquivalentfortheperturbedResolvent
NotethattherelationsestablishedinPropositionE.2stillinvolvethefullhigh-dimensionalmatricesX,W.Ourgoalnow
willbetousethedeterministicequivalenceweestablishedearliertoobtaindeterministiclimitsforeachofthequantities.
We begin by incorporating the perturbation terms into our deterministic equivalence. Fortunately, this simply
correspondstorescalingcertaintermsbyconstants,asweexplainbelow:
TheoremE.3. LetV⋆ ∈Ck×k,ν⋆ ∈Ck,b⋆ ∈Ck beuniquelydefinedthroughthefollowingconditions:
• V⋆,ν⋆,b⋆satisfythefollowingsetofself-consistentequations:
(cid:34) (cid:35)
c (κ,ζu)c (κ,ζu)+ρ +ρ χ
V⋆ (z)=E α 1 q 1 q′ 1 1 V,d,b
qq′ κ 1+χ (κ)
V,d,b
 
ν q⋆(z)=E κ(cid:88) 1α +c2 ℓ χ(κ,ζ qu ()
κ)
+ρ 2(cid:88) c2 ℓ(κ,ζ qu)..,
V,d,b
ℓ≥2 ℓ≥2
b⋆(z)=π β((V⋆)−1+diag(b⋆)+(diag(ν⋆)−zI ))−1
q q p q,q
whereκ ∼ N(0,1)(c (κ,ζ)) aredefinedin4.2andwhereκ ∼ N(0,1),(c (κ,ζ)) aredefinedin4.2and
ℓ ℓ>0 ℓ ℓ>0
(χ(z;κ),L(z))readasfollows:
(cid:88) (cid:88) (cid:88)
βχ(z;κ)= ψ c (κ,ζu)c (κ,ζu)+ b⋆ c2(κ,ζu),
qq′ 1 q 1 q′ q ℓ q
q,q′∈[k] q∈[k] ℓ≥2
(cid:16) (cid:17)−1
L(z)= V⋆(z)−1+diag(b⋆(z)) ,
whereψ(z)∈Rk×k isdefinedas:
ψ(z)=b⋆(z)−L(z)⊙(b⋆(z)(b⋆(z))⊤),
36• V⋆,ν⋆,b⋆areanalyticmappingssatisfyingV⋆ :C+ →C−fori,j ∈[k],ν⋆ :C+ →C−fori∈[k],b⋆ :C+ →C+
i,j i i
fori∈[k].
• (cid:12) (cid:12)b⋆(z)(cid:12) (cid:12)≤ πi.
i βζ
DefineG (z,ρ ,ρ )∈R(p+1)×(p+1)as:
e 1 2
(cid:34) (cid:35)−1
A∗ −zI (A∗ )⊤⊙θ⊤
G (z)= 11 k 21 ,
e θ⊙A∗ A∗ +αS∗⊙θθ⊤
21 22 e
whereA∗ ,A∗ ,S areidenticaltoTheorem4.6.
11 12 e
(cid:16) (cid:17)
Then, foranyz ∈ C/R+ andsequenceofdeterministicmatricesA ∈ C(p+1)×(p+1) with∥A∥ = Tr (AA∗)1/2
tr
uniformlyboundedind:
a.s
Tr(AG (z,ρ ,ρ ))−−−→Tr(AG (z,ρ ,ρ ))).
e 1 2 e 1 2
d→∞
Proof. Weobservethattheintroductionofperturbationtermsρ ,ρ simplyamountstoshiftingV⋆,ν⋆byconstants.To
1 2
seethis,firstnotethattheperturbationmatricesC⊙WW⊤,DdonotdependonX.ThereforetheproofofProposition
D.3appliestoG (z,ρ ,ρ )withtheonlymodificationbeingtheinclusionofL,D′alongwith−zI asconstantterms.
e 1 2
Subsequently,inthesecondstage,thestructureofL,D′allowsthemtobedirectlyabsorbedintothecomponentM⋆
withchangesuptoconstantsinV ,ν .
d d
As a direct corollary, we obtain the deterministic equivalents of the Schur-complement Representations in the
followingsense:
CorollaryE.4. LetG (ρ ,ρ )denotethedeterministicequivalentfortheperturbedresolventasperTheoremE.3withzset
e 1 2
as−λ.Definethefollowingblock-matrixrepresentationsforG ,G :
e e
(cid:34) (cid:35)
C −CQ⊤
G (ρ ,ρ )= .
e 1 2 −QC P
(cid:34) (cid:35)
C −CQ⊤
G (ρ ,ρ )= . (38)
e 1 2 −QC P
Then:
1
C =C+O (√ ),
≺
d
Furthermore,foranyfixedr ∈Rp−k with∥r∥=O(1):
1
Qr =Qr+O (√ ),
≺
d
Proof. TheaboveequivalencesfollowfromTheorem4.6bysettingAasmatricesactingontheindividualblocksof
G (ζ,ρ).Concretely,toobtainC wesetAasthematrixwithA =1and0otherwise.
e i,j i,j
WederivetheresultingexpressionsforC,Qbelowforsubsequentusage:
LemmaE.5. ConsidertheSchur-decompositionoftheequivalentresolventG (withz =−λ)definedbyEquation(38)and
e
letψ,S beasdefinedinTheoremE.3.ThematricesC,Qsatisfy:
1
C−1 =A∗ +λI −(A∗ )⊤((ψ)−1+αS)−1A∗ +O (√ )
11 k+1 21 21 ≺ p
b⋆ 1 θ θ⊤ 1
Q=((diag( ) − (diag(b⋆)−ψ(V ,ν )) ⊙ )−1+αS ⊙θθ⊤)−1θ⊙A⋆ +O (√ ),
πβ e β2 d d e ππ e 21 ≺ p
whereintheexpressionforC,we’vefurtheraveragedoverθ.
37Proof. Throughout,weshallusethefollowingsimplification:
1
(ei⊙θ)(ej ⊙θ)=πδ +O (√ ) (39)
i=j ≺
d
,whichfollowsfromthedefinitionofei,πandθ beingindependentsub-Gaussianrandomvariables.
i
ByTheoremE.3andthedefinitionofC,wehave:
C =A∗ +λI
11 k+1
b⋆ 1 θ θ⊤
−(A⋆ )⊤⊙θ⊤((diag( ) − (diag(b⋆)−ψ(V ,ν )) ⊙ )−1+αS ⊙θθ⊤)−1θ⊙A⋆
21 πβ e β2 d d e ππ e 21
(cid:124) (cid:123)(cid:122) (cid:125)
T
,
Tosimplifytheaboveexpression,weapplythematrix-WoodburyidentitytothetermT.Wefirstrecognizefromthe
definitionofψinEquation(29)thatthefollowingrelationholds:
b⋆ b⋆ 1 θ θ⊤
(V−1+diag(b⋆))−1⊙( ) ( )⊤⊙θθ⊤ = (diag(b⋆)−ψ(V ,ν )) ⊙
d πβ e πβ e β2 d d e ππ
Further,notingthatαS ⊙θθ⊤correspondstoafinite-rankperturbationE S E⊤alongwithadditionalsimplifications
e θ e θ
byEquation(39),weobtain:
b⋆ 1 θ θ⊤
T =(A⋆ )⊤⊙θ⊤((diag( ) − (diag(b⋆)−ψ(V ,ν )) ⊙ )θ⊙A⋆
21 πβ e β2 d d e ππ 21
1
−(A⋆ )⊤ψ((αS)−1+ψ)−1ψA⋆ +O (√ )
21 21 ≺
d
1
=(A⋆ )⊤ψA⋆ −(A⋆ )⊤ψ((αS)−1+ψ)−1ψA⋆ +O (√ )
21 21 21 21 ≺
d
1
=(A⋆ )⊤((αS)+ψ−1)A⋆ +O (√ )
21 21 ≺
d
Next,forQ,theexpressionfollowsbydirectionsubstitution.RecallfromPropositionE.2,thatQ=G˜(ρ ,ρ )Σ /p.
1 2 21
FromthestructureofG(ρ ,ρ )inTheoremE.3,weobtainthatthecorrespondingmatrixQ,intheequivalentG(ρ ,ρ )
1 2 1 2
issimilarlygivenby:
b⋆ 1 θ θ⊤
((diag( ) − (diag(b⋆)−ψ(V ,ν )) ⊙ )−1+αS ⊙θθ⊤)−1θ⊙A⋆ .
πβ e β2 d d e ππ e 21
E.4 Extractingorder-parametersthroughthedeterministicequivalent
Armedwiththedeterministicequivalentfortheperturbedresolvent(TheoremE.3),itremainstojustifyandextractthe
quantitiesτ ,τ ,τ ,τ .
0 1 2 3
LemmaE.6. LetC−1,C−1denotethecorrespondingblocksofG andG respectively.Then,w.h.pasd→∞
e e
∂ (cid:16) (cid:17) ∂ (cid:16) (cid:17) 1
C−1 = C−1 +O ( )
∂ρ 1 ρ=0 ∂ρ 1 ρ=0 ≺ d1/4
∂ (cid:16) (cid:17) ∂ (cid:16) (cid:17) 1
C−1 = C−1 +O( )
∂ρ 2 ρ=0 ∂ρ 2 ρ=0 d1/4
Proof. Wehave:
(cid:34) (cid:35) (cid:34) (cid:35)
∂2 (cid:16) C−1(cid:17)
=
1 y⊤ Φ˜⊤G˜(WW⊤)2GΦ˜(cid:2)
y
Φ¯(cid:3)
+
1 y⊤ Φ˜⊤G˜(WW⊤)G(WW⊤)Φ˜(cid:2)
y
ϕ¯(cid:3)
.
∂ζ2 ζ p Φ¯⊤ p Φ¯⊤
38Recallthat∥G∥
µ
< γ1 and(cid:13) (cid:13)WW⊤(cid:13) (cid:13)=O ≺(1)byLemmaB.4.Bythesubmultiplicityofoperatornorms,wehavethat
norm ∂2 (cid:0) C−1(cid:1) isuniformlyboundedinζ,dwithhigh-probabilityasd→∞.Therefore,applyingthemean-value-
∂ζ2 ζ
theorementry-wisetoC−1andC−1yieldsthatforanyζ >0:
⋆
∂ (cid:16) (cid:17) (C−1(ζ)−C−1(0)
C−1 = +Kζ,
∂ζ ζ=0 ζ
and
∂ (cid:16) (cid:17) (C−1(ζ)−C−1(0)
C−1 = +Kζ,
∂ζ ζ=0 ζ
forsomeconstantK >0.
CombiningtheabovewiththeboundsfromCorollaryE.4yields:
∂ (cid:16) (cid:17) ∂ (cid:16) (cid:17) 1 1
C−1 = C−1 +Kζ+ O (√ )
∂ρ 1 ρ=0 ∂ρ 1 ρ=0 ρ ≺ d
Therefore,settingζ = 1 yields:
d1/4
∂ (cid:16) (cid:17) ∂ (cid:16) (cid:17) 1
C−1 = C−1 +O ( )
∂ρ 2 ρ=0 ∂ρ 2 ρ=0 ≺ d1/4
E.5 ProofofTheorem4.8
TheproofofTheorem4.8followsdirectlyfromLemmaE.1combinedwiththenextresult,whichdefinestheparameters
τˆ ,τˆ ,τˆ ,τˆ usingthefixedpointparametersV⋆,ν⋆:
0 1 2 3
(cid:16) (cid:17)2
Proposition E.7. Let aˆ = argmin (cid:80) y −f(x ;a,W1) +λ||a||2 denote the ridge-regression estimator after
λ i i 2
a∈Rp i∈[n]
agradientupdatetothefirstlayer. Letτˆ ,τˆ ,τˆ ,τˆ bedefinedasinE.1witha = aˆ andletC⋆(V⋆,ν⋆),ψ(V⋆,ν⋆)be
0 1 2 3 λ
functionsofV⋆,ν⋆definedinTheorems4.6,LemmaD.9withzsetas−λandletS ∈Rk×k beasdefinedinTheorem4.6.
Then,withV⋆,ν⋆beingtheuniquesolutionstothefixed-pointequationsdefinedinTheorem4.8.
1 1
τˆ =(C−1) ((C−1) +λ(1− diag(π )))−1+O (√ )
0 01 11 p u ≺ d
 
1
(cid:0) τˆ 1(cid:1) =((ψ)−1+αS)−1A˜∗ 21   −τˆ .
.
.0,1   +O ≺(√1 d) (40)
 
−τˆ
0,k
τˆ =(cid:2) 1 −τˆ (cid:3) ∂ (cid:104) (C)−1(cid:105) (cid:20) 1 (cid:21) +O (√1 ) (41)
2 0 ∂ρ 1 ρ=0 −τˆ 0 ≺ d
and
τˆ =(cid:2) 1 −τˆ (cid:3) ∂ (cid:104) (C)−1(cid:105) (cid:20) 1 (cid:21) +O (√1 ) (42)
2 0 ∂ρ 2 ρ=0 −τˆ 0 ≺ d
where:
C−1 =A∗ +λI −(A∗ )⊤((ψ)−1+αS)−1A∗ ,
11 k+1 21 21
andA˜∗ ∈Rk×k+1isdefinedanalogoustoA∗ inTheorem4.6butwithu ,··· ,u replacedbyζu,··· ,ζui.e:
21 21 1 p 1 k
(cid:34) (cid:35)
c (κ,ζu)
A∗ [j,:]=αE 1 j κι⊤ , ∀j ∈[k]
21 κ 1+χ(z;κ)
39Proof. Forτ∗,theresultfollowsdirectlyfromPropositionE.2,CorollaryE.4andLemmaE.5.Thisfurtherimpliesthatτˆ
0 0
hasentriesO(1).
Therefore,wemayagainapplyPropositionE.2toobtain:
τˆ =(cid:2) 1 −τ∗(cid:3) ∂ (cid:104) (C)−1(cid:105) (cid:20) 1 (cid:21) +O(1 ) (43)
2 0 ∂ρ 1 ρ=0 −τˆ 0 p
and
τˆ
=(cid:2)
1
−τ∗(cid:3) ∂ (cid:104) (C)−1(cid:105) (cid:20) 1 (cid:21) +O(1
).
3 0 ∂ρ 2 ρ=0 −τˆ 0 p
Theexpressionsforτˆ ,τˆ inEquations(41),(43)arethendirectconsequencesofCorollaryE.4,LemmaE.5andLemma
2 3
E.6(tojustifydifferentiatingthroughthedeterministicequivalent).Itremainstoobtaintheresultingexpressionforτˆ .
1
ApplyingPropositionE.2,andusingτˆ =O(1),weobtain:
0
(cid:20) (cid:21)
1 1
τˆ =(θ⊙ej)⊤Q +O(√ ).
1,j −τˆ 0 p
Next,applyingLemmaE.5andCorollaryE.4withr =(θ⊙ej)yields:
b⋆ 1 θ θ⊤
τˆ =(θ⊙ej)⊤((diag( ) − (diag(b⋆)−ψ(V ,ν )) ⊙ )−1+αS ⊙θθ⊤)−1θ⊙A⋆
1,j πβ e β2 d d e ππ e 21
Subsequently,analogoustothederivationofC inLemmaE.5,wesimplifytheaboveexpressionusingtheWoodbury
identitytoobtainEquation(40).
F Proofs of auxilary results
F.1 ProofofLemmaD.9
Weproceedusingtheleave-oneoutargumentfortheWishartspectrum.LetW denotetheweightmatrixwiththei
−i th
columnremovedanddefine:
E :=w ⊙[e ,··· ,e ]∈Rp×k.
wi i 1 k
Then:
C ⊙WW⊤ =C ⊙W W⊤ +C ⊙w w⊤.
e e −i −i e i i
SimilartothetermV ⊙θθ⊤terminSectionD.3,weobservethatC ⊙w w⊤isarankkmatrixgeneratedthrough
e e i i
thevectorse ,··· ,e :
1 k
C ⊙w w⊤ =E CE⊤,
e i i wi wi
TheWoodburymatrixidentityyields:
R=R −R E ((C)−1+E⊤R E )−1E⊤R ,
−i −i wi wi −i wi wi −i
where:
R =(C ⊙W W⊤ +(diag(D) −zI ))−1
−i e −i −i e p
Wesubstitutetheaboveintotherelation:
R(C ⊙WW⊤+(diag(D) −zI ))=I
e e p
toobtain:
d d
(cid:88) (cid:88)
R E CE⊤ − R E ((C)−1+B )−1B CE⊤ +R(diag(D) −zI )=I
−i wi wi −i wi m m wi e p
i=1 i=1
Therefore,weobtain:
d
(cid:104) (cid:105) (cid:88)
dRE E CE⊤ − R E (C−1+B )−1B CE⊤ +R(diag(D) −zI )=I+∆
w w −i wi m m wi e p
i=1
40where∆includeserrortermsandB isank×kmatrixgivenby:
m
(cid:104) (cid:105)
B =E E⊤R E . (44)
m w −i w
BoundingtheerrortermsexactlyasinPropositionD.3,weobtainthatcontributionsfromδareoforderO ≺(∥A √∥Tr)
d
Now,since(C−1+B )−1B =I−(C−1+B )−1(C)−1,weobtain:
m m m
∥A∥
dRE ((C)−1+B )−1E⊤ +R(diagD −zI )=I+O ( √ Tr).
wi m wi e p ≺
d
whichleadstothefollowingdeterministicequivalentforR(C,D):
R=(M+(diagD −zI ))−1,
e p
where:
(cid:104) (cid:105)
M=dE E (C−1+B )−1E⊤ , (45)
w m w
Toobtainthedeterministicequivalent,itremainstonextderiveaself-consistentequationforthematrixB itself.
m
SubstitutingEquation(45)intoEquation(44)weobtainthatB isdiagonalwithentriessatisfying
m
b :=B =π β(((C)−1+diag(b))−1+(diag(D)−zI ))−1,
q q,q q p q,q
forq ∈[p].
Lastly,itremainstoestablishtheuniquenessofthefixedpointsofb :
q
LemmaF.1. LetF :C+ →C+denotethemapping:
b
F(b) =π β(((C)−1+diag(b))−1+(diag(D)−zI ))−1,
q q p q,q
forq ∈[k].Thenforlargeenoughζ andC,DwithentriesinC−satisfying|C| ≤K forsomeconstantK independentof
ij
ζ :
(cid:13) (cid:13)F(b′)−F(b)(cid:13)
(cid:13) <
K′
∥b−b∥
2 ζ2 2
,forb,b′satisfying|b| ≤ π andsomeconstantK >0.
q βζ
Proof. LemmaB.3andLemmaB.12alongwiththeboundsonb,C,Dimply:
(cid:13) (cid:13)F(b′)−F(b)(cid:13) (cid:13)
2
≤ K ζ21(cid:13) (cid:13) (cid:13)((C)−1+diag(b′))−1−((C)−1+diag(b))−1(cid:13) (cid:13) (cid:13),
forsomeconstantK >0.AgainapplyingLemmaB.3andusingtheboundsonentriesofC yields:
1
(cid:13) (cid:13)F(b′)−F(b)(cid:13)
(cid:13) ≤
K 2(cid:13) (cid:13)b′−b(cid:13)
(cid:13)
2 ζ2 2
ThecontrolofalltheerrortermsandconcentrationboundsfollowsezactlyasinPropositionD.3
F.2 Equivalencebetweengradientupdateandthespikedrandomfeaturemodel
Inthissection,wediscusstheregimen =α dforsomeconstantα . Inthisregime,weshowthatthe“bulk"inthe
0 0 0
gradientupdatepossessesanon-isotropiccomponent.
ThestartingpointisthefollowingrelationshipbetweentheinitialweightmatrixW0andthenewversionW1after
onegradientstep:
W1 =W0+(ηp)G,
whereGisthegradientmatrixconstructedas
1
G= √ diag{a ,...a }σ′(W0X)diag{y ,...,y }XT,
n p 1 p 1 n
41whereX isand×n matrixwhosecolumnsarethedatavectorsinthefirstbatch,{y } arethecorrespondinglabels,
0 i i∈[n]
andσ′(·)isthederivativeofthestudentactivationfunction.
ConsidertheHermiteexpansionofσ:
σ(x)=c +c h (x)+c h (x)+....
0 1 1 2 2
Wecanthenwrite
σ′(x)=c +σ′ (x), (46)
1 >1
where
√
σ′ (x)d =ef(cid:88) k!c h (x).
>1 k k−1
k≥2
Usingthedecompositionin(46),wecanrewritetheweightmatrixW1as
η √ √
W1 =W0+ diag{ pa ,... pa }σ′ (W0X)diag{y ,...,y }XT +uv⊤,
n 1 p >1 1 n
0
(cid:124) (cid:123)(cid:122) (cid:125)
W(cid:102)
√
whereu=c c⋆η paandv = 1 Xy/n ,wherec⋆denotesthefirstHermitecoefficientofthetargetactivationg.Next,
1 1 c⋆ 0 1
1
weexamineW(cid:102),whichisthe“bulk”oftheweightmatrixW1afteronegradientstep.
Observethat,conditionedonX andy,therowsofW(cid:102)areindependentcenteredrandomvectors.Fori∈[n],theith
rowofW(cid:102),denotedbyb
i
∈Rd,is
√
η( pa )
b =w + i Xdiag{y ,...,y }σ′ (X⊤w ). (47)
i i n 1 n >1 i
0
PropositionF.2. LetE [·]denotetheconditionalexpectationwithrespecttow ,withX,(cid:8) y (cid:9) anda keptfixed.
wi i j j∈[n0] i
Then √ √
(cid:104) (cid:105) 2 2c η( pa )
d·E b b⊤ =I+ 2 i Xdiag{y ,...,y }X⊤
wi i i n 1 n
0
√
√ (cid:18) 1 (cid:19)2 6c2η2( pa )2
+2η2c2 2( pa i)2 nXdiag{y 1,...,y n0}X⊤ + 3
n2
i Xyy⊤X⊤ (48)
0
 
(cid:88) √ (cid:18) 1 (cid:110) (cid:111) (cid:19)
+ k!c2 kη2( pa i)2(d/n 0) n0Xdiag y 12,...,y n2
0
X⊤ +∆,
k≥3
where∆∈Rd×dissmallerrortermsuchthat
∥∆∥ ≺d−1/2.
op
Proof. From(47),
√
(cid:104) (cid:105) (cid:104) (cid:105) η( pa ) (cid:104) (cid:105)
E b b⊤ =E w w⊤ + i Xdiag{y ,...,y }E σ′ (X⊤w )w⊤ +(∗)⊤
wi i i wi i i n 1 n0 wi >1 i i
0
(cid:124) (cid:123)(cid:122) (cid:125)
(49)
(∗)
√
η2( pa )2 (cid:104) (cid:105)
+ i Xdiag{y ,...,y }E σ′ (X⊤w )(σ′ (X⊤w ))⊤ diag{y ,...,y }X⊤.
n2 1 n0 wi >1 i >1 i 1 n0
0
Bysymmetry,itisstraightforwardtocheckthatE (cid:2) w w⊤(cid:3) =I/d.Tocomputethelastthreetermsontheright-hand
wi i i
sideoftheaboveequation,wefirstmakethesimplifyingassumptionsthatw ∼N(0,I/d)andthateachcolumnofZ
√ i
hasafixednormequalto d.Notethattheseassumptionsareasymptoticallyaccurateasd→∞.Forfinited,wecan
absorbtheerrorsintroducedbytheseassumptionsintotheerrormatrix∆.Undertheaboveassumptions,itiseasyto
checkthat:
√
(cid:104) (cid:105) 2c
E σ′ (X⊤w )w⊤ = 2X⊤
wi >1 i i d
and
 
(cid:104) (cid:105) 2c2 6c2 (cid:88)
E
wi
σ >′ 1(X⊤w i)(σ >′ 1(X⊤w i))⊤ = d2X⊤X+ d311⊤+ k!c3 kI n+∆(cid:101),
k≥3
42(cid:13) (cid:13)
where(cid:13)∆(cid:101)(cid:13) ≺d−1/2.Substitutingtheseestimatesinto(49)givesus(48),with
(cid:13) (cid:13)
op
(cid:18) (cid:19)
√ 1
∆=η2( pa i)2(d/n 0) n0Xdiag{y 1,...,y n}∆(cid:101) diag{y 1,...,y n}X⊤ .
Letusfurtherevaluatethetraceoftheexpression(48)inthesettingofCuietal.[2024],foroddactivations(implying
√
inparticularµ =0),anduniforminitializations∀i, pa =1.Since
2 i
1 (cid:20) 1 (cid:110) (cid:111) (cid:21) 1(cid:88)d
Tr Xdiag y2,...,y2 X⊤ = E [g(w⊤x)2x2]
d n 1 n d x∼N(0,Id) ⋆ i
0
i=1
d
1(cid:88)(cid:16) (cid:17)
= 2w⋆E [g(w⊤x)g′(w⊤x)x ]+E [g(w⊤z)2]
d i x∼N(0,Id) ⋆ ⋆ i z∼N(0,Id) ⋆
i=1
≍E [g(ξ)2]
ξ∼N(0,1)
Thus,
 
1 (cid:20) (cid:104) (cid:105)(cid:21) (cid:88) 1 1
dTr E
wi
b ib⊤
i
=1+ k!c2 kη2
α
E ξ∼N(0,1)[g(ξ)2]=1+E ξ∼N(0,1)[σ >′ 1(ξ)2]η2
α
E ξ∼N(0,1),
0 0
k≥3
whichcorrespondstothetermcinCuietal.[2024]uptoconventions.
F.3 ProofofLemmaB.14
Proof. Westartbyrewritingg =z+w′(s−z⊤w′),wheres∼N(0,1)andz ∼N(0,I)areindependent.Itfollowsthat
f (w⊤g)f (w⊤g)g⊤w′ =sf (cid:0) w⊤z+(w⊤w′)(s−z⊤w′)(cid:1) f (cid:0) w⊤z+(w⊤w′)(s−z⊤w′)(cid:1)
1 1 2 2 1 1 1 2 2 2
=sf (w⊤z)f (w⊤z)+sf (w⊤z)f′(w⊤x)(w⊤w′)(s−z⊤w′)
1 1 2 2 1 1 2 2 2
+sf (w⊤z)f′(w⊤x)(w⊤w′)(s−z⊤w′)+O (d−1).
2 2 1 1 1 ≺
Takingexpectation,andapplyingLemmaB.13,weget(7).Theestimatein(8)canbetreatedsimilarly.Notethat
f (w⊤g)f (w⊤g)(θ⊤g)2 =s2f (w⊤x)f (w⊤z)+O (d−1/2),
1 1 2 2 1 1 2 2 ≺
whichimmediatelyleadsto(8).
References
FrancisBach.Thequestforadaptivity,2021.URLhttps://francisbach.com/quest-for-adaptivity/.
CharlesHMartinandMichaelWMahoney. Implicitself-regularizationindeepneuralnetworks:Evidencefromrandom
matrixtheoryandimplicationsforlearning. JournalofMachineLearningResearch,22(165):1–73,2021.
CharlesHMartin,TongsuPeng,andMichaelWMahoney. Predictingtrendsinthequalityofstate-of-the-artneural
networkswithoutaccesstotrainingortestingdata. NatureCommunications,12(1):4122,2021.
ZhichaoWang,AndrewEngel,AnandDSarwate,IoanaDumitriu,andTonyChiang. Spectralevolutionandinvariance
inlinear-widthneuralnetworks. AdvancesinNeuralInformationProcessingSystems,36,2024.
ArthurJacot,FranckGabriel,andClémentHongler. Neuraltangentkernel:Convergenceandgeneralizationinneural
networks. Advancesinneuralinformationprocessingsystems,31,2018.
LenaicChizat,EdouardOyallon,andFrancisBach. Onlazytrainingindifferentiableprogramming. Advancesinneural
informationprocessingsystems,32,2019.
Ali Rahimiand BenjaminRecht. Randomfeatures for large-scalekernel machines. Advances inneural information
processingsystems,20,2007.
43SebastianGoldt,BrunoLoureiro,GalenReeves,FlorentKrzakala,MarcMézard,andLenkaZdeborová. Thegaussian
equivalenceofgenerativemodelsforlearningwithshallowneuralnetworks. Proceedingsofthe2ndMathematicaland
ScientificMachineLearningConference,145:426–471,2022.
HongHuandYueMLu. Universalitylawsforhigh-dimensionallearningwithrandomfeatures. IEEETransactionson
InformationTheory,2022.
SongMeiandAndreaMontanari. Thegeneralizationerrorofrandomfeaturesregression:Preciseasymptoticsandthe
doubledescentcurve. CommunicationsonPureandAppliedMathematics,75(4):667–766,2022.
JimmyBa,MuratAErdogdu,TaijiSuzuki,ZhichaoWang,DennyWu,andGregYang. High-dimensionalasymptoticsof
featurelearning:Howonegradientstepimprovestherepresentation. InAdvancesinNeuralInformationProcessing
Systems,volume35,pages37932–37946,2022.
GregYang,EdwardJHu,IgorBabuschkin,SzymonSidor,XiaodongLiu,DavidFarhi,NickRyder,JakubPachocki,Weizhu
Chen,andJianfengGao. Tensorprogramsv:Tuninglargeneuralnetworksviazero-shothyperparametertransfer.
arXivpreprintarXiv:2203.03466,2022.
YatinDandi,FlorentKrzakala,BrunoLoureiro,LucaPesce,andLudovicStephan. Howtwo-layerneuralnetworkslearn,
one(giant)stepatatime. arXivpreprintarXiv:2305.18270,2023.
BehradMoniri,DonghwanLee,HamedHassani,andEdgarDobriban. Atheoryofnon-linearfeaturelearningwithone
gradientstepintwo-layerneuralnetworks. arXivpreprintarXiv:2310.07891,2023.
HugoCui,LucaPesce,YatinDandi,FlorentKrzakala,YueMLu,LenkaZdeborová,andBrunoLoureiro. Asymptoticsof
featurelearningintwo-layernetworksafteronegradient-step. arXivpreprintarXiv:2402.04980,2024.
MMezard,GParisi,andMVirasoro. SpinGlassTheoryandBeyond. WORLDSCIENTIFIC,1986. doi:10.1142/0271. URL
https://www.worldscientific.com/doi/abs/10.1142/0271.
BlakeBordelon,AbdulkadirCanatar,andCengizPehlevan. Spectrumdependentlearningcurvesinkernelregressionand
wideneuralnetworks. InProceedingsofthe37thInternationalConferenceonMachineLearning,volume119,pages
1024–1034,2020.
AbdulkadirCanatar,BlakeBordelon,andCengizPehlevan.Spectralbiasandtask-modelalignmentexplaingeneralization
inkernelregressionandinfinitelywideneuralnetworks. NatureCommunications,12(1):2914,2021.
HugoCui,BrunoLoureiro,FlorentKrzakala,andLenkaZdeborová. Generalizationerrorratesinkernelregression:The
crossoverfromthenoiselesstonoisyregime. AdvancesinNeuralInformationProcessingSystems,34:10131–10143,2021.
HugoCui,BrunoLoureiro,FlorentKrzakala,andLenkaZdeborová. Errorscalinglawsforkernelclassificationunder
sourceandcapacityconditions. MachineLearning:ScienceandTechnology,4(3):035033,2023.
RainerDietrich,ManfredOpper,andHaimSompolinsky. Statisticalmechanicsofsupportvectornetworks. Phys.Rev.
Lett.,82:2975–2978,Apr1999. doi:10.1103/PhysRevLett.82.2975.
KonstantinDonhauser,MingqiWu,andFannyYang. Howrotationalinvarianceofcommonkernelspreventsgeneraliza-
tioninhighdimensions. InInternationalConferenceonMachineLearning,pages2804–2814.PMLR,2021.
BehroozGhorbani,SongMei,TheodorMisiakiewicz,andAndreaMontanari. Limitationsoflazytrainingoftwo-layers
neuralnetwork. AdvancesinNeuralInformationProcessingSystems,32,2019.
BehroozGhorbani,SongMei,TheodorMisiakiewicz,andAndreaMontanari. Whendoneuralnetworksoutperform
kernelmethods? AdvancesinNeuralInformationProcessingSystems,33:14820–14830,2020.
M.OpperandR.Urbanczik. Universallearningcurvesofsupportvectormachines. Phys.Rev.Lett.,86:4410–4413,2001.
LechaoXiao,HongHu,TheodorMisiakiewicz,YueLu,andJeffreyPennington. Preciselearningcurvesandhigher-order
scalingsfordot-productkernelregression. AdvancesinNeuralInformationProcessingSystems,35:4558–4570,2022.
FedericaGerace,BrunoLoureiro,FlorentKrzakala,MarcMézard,andLenkaZdeborová. Generalisationerrorinlearning
withrandomfeaturesandthehiddenmanifoldmodel.InInternationalConferenceonMachineLearning,pages3452–3462.
PMLR,2020.
44SongMei,TheodorMisiakiewicz,andAndreaMontanari. Generalizationerrorofrandomfeatureandkernelmethods:
hypercontractivityandkernelmatrixconcentration. AppliedandComputationalHarmonicAnalysis,59:3–84,2022.
HongHu,YueMLu,andTheodorMisiakiewicz. Asymptoticsofrandomfeatureregressionbeyondthelinearscaling
regime. arXivpreprintarXiv:2403.08160,2024.
Fabián Aguirre-López, Silvio Franz, and Mauro Pastore. Random features and polynomial rules. arXiv preprint
arXiv:2402.10164,2024.
LenaicChizatandFrancisBach. Ontheglobalconvergenceofgradientdescentforover-parameterizedmodelsusing
optimaltransport. Advancesinneuralinformationprocessingsystems,31,2018.
SongMei,AndreaMontanari,andPhan-MinhNguyen. Ameanfieldviewofthelandscapeoftwo-layerneuralnetworks.
ProceedingsoftheNationalAcademyofSciences,115(33):E7665–E7671,2018.
GrantRotskoffandEricVanden-Eijnden. Trainabilityandaccuracyofartificialneuralnetworks:Aninteractingparticle
systemapproach. CommunicationsonPureandAppliedMathematics,75(9):1889–1935,2022.
JustinSirignanoandKonstantinosSpiliopoulos.Meanfieldanalysisofneuralnetworks:Acentrallimittheorem.Stochastic
ProcessesandtheirApplications,130(3):1820–1852,2020.
GerardBenArous,RezaGheissari,andAukoshJagannath. Onlinestochasticgradientdescentonnon-convexlossesfrom
high-dimensionalinference. TheJournalofMachineLearningResearch,22(1):4788–4838,2021.
EmmanuelAbbe,EnricBoixAdsera,andTheodorMisiakiewicz. Sgdlearningonneuralnetworks:leapcomplexityand
saddle-to-saddledynamics. InTheThirtySixthAnnualConferenceonLearningTheory,pages2552–2623.PMLR,2023.
JimmyBa,MuratAErdogdu,TaijiSuzuki,ZhichaoWang,andDennyWu. Learninginthepresenceoflow-dimensional
structure:aspikedrandommatrixperspective. AdvancesinNeuralInformationProcessingSystems,36,2024.
LorenzoBardoneandSebastianGoldt. Slidingdownthestairs:howcorrelatedlatentvariablesacceleratelearningwith
neuralnetworks. arXivpreprintarXiv:2404.08602,2024.
RaphaëlBerthier,AndreaMontanari,andKangjieZhou. Learningtime-scalesintwo-layersneuralnetworks. arXiv
preprintarXiv:2303.00055,2023.
AlbertoBietti,JoanBruna,andLoucasPillaud-Vivien. Onlearninggaussianmulti-indexmodelswithgradientflow.
arXivpreprintarXiv:2310.19793,2023.
AlexDamian,EshaanNichani,RongGe,andJasonDLee. Smoothingthelandscapebooststhesignalforsgd:Optimal
samplecomplexityforlearningsingleindexmodels. AdvancesinNeuralInformationProcessingSystems,36,2024.
CourtneyPaquette,KiwonLee,FabianPedregosa,andElliotPaquette.Sgdinthelarge:Average-caseanalysis,asymptotics,
andstepsizecriticality. InAnnualConferenceComputationalLearningTheory,2021.
RodrigoVeiga,LudovicStephan,BrunoLoureiro,FlorentKrzakala,andLenkaZdeborová. Phasediagramofstochastic
gradientdescentinhigh-dimensionaltwo-layerneuralnetworks. AdvancesinNeuralInformationProcessingSystems,
35:23244–23255,2022.
AaronZweigandJoanBruna. Symmetricsingleindexlearning. arXivpreprintarXiv:2310.02117,2023.
YatinDandi,EmanueleTroiani,LucaArnaboldi,LucaPesce,LenkaZdeborova,andFlorentKrzakala. Thebenefitsof
reusingbatchesforgradientdescentintwo-layernetworks:Breakingthecurseofinformationandleapexponents. In
Forty-firstInternationalConferenceonMachineLearning,2024. URLhttps://openreview.net/forum?id=
iKkFruh4d5.
VladimirAlexandrovichMarchenkoandLeonidAndreevichPastur. Distributionofeigenvaluesforsomesetsofrandom
matrices. MatematicheskiiSbornik,114(4):507–536,1967.
ZdzislawBurda,AGörlich,AndrzejJarosz,andJerzyJurkiewicz. Signalandnoiseincorrelationmatrix. PhysicaA:
StatisticalMechanicsanditsApplications,343:295–310,2004.
AnttiKnowlesandJunYin. Anisotropiclocallawsforrandommatrices. ProbabilityTheoryandRelatedFields, 169:
257–352,2017.
45ZhidongBaiandWangZhou. Largesamplecovariancematriceswithoutindependencestructuresincolumns. Statistica
Sinica,pages425–442,2008.
CosmeLouartandRomainCouillet. Concentrationofmeasureandlargerandommatriceswithanapplicationtosample
covariancematrices. arXivpreprintarXiv:1805.08295,2018.
ClémentChouard. Quantitativedeterministicequivalentofsamplecovariancematriceswithageneraldependence
structure. arXivpreprintarXiv:2211.13044,2022.
CosmeLouart,ZhenyuLiao,andRomainCouillet. Arandommatrixapproachtoneuralnetworks. TheAnnalsofApplied
Probability,28(2):1190–1248,2018.
DominikSchröder,HugoCui,DaniilDmitriev,andBrunoLoureiro. Deterministicequivalentanderroruniversalityof
deeprandomfeatureslearning. InInternationalConferenceonMachineLearning,pages30285–30320.PMLR,2023.
Dominik Schröder, Daniil Dmitriev, Hugo Cui, and Bruno Loureiro. Asymptotics of learning with deep structured
(random)features. arXivpreprintarXiv:2402.13999,2024.
DavidBosch,AshkanPanahi,andBabakHassibi. Preciseasymptoticanalysisofdeeprandomfeaturemodels. InThe
ThirtySixthAnnualConferenceonLearningTheory,pages4132–4179.PMLR,2023.
R.Bellman,RandCorporation,andKarremanMathematicsResearchCollection.DynamicProgramming.RandCorporation
researchstudy.PrincetonUniversityPress,1957.
AlexandruDamian,JasonLee,andMahdiSoltanolkotabi. Neuralnetworkscanlearnrepresentationswithgradient
descent. InPo-LingLohandMaximRaginsky, editors, ProceedingsofThirtyFifthConferenceonLearningTheory,
volume178ofProceedingsofMachineLearningResearch,pages5413–5452,2022.
EmmanuelAbbe,EnricBoixAdsera,andTheodorMisiakiewicz. Themerged-staircaseproperty:anecessaryandnearly
sufficientconditionforsgdlearningofsparsefunctionsontwo-layerneuralnetworks. InConferenceonLearning
Theory,pages4782–4887.PMLR,2022.
GregWAnderson,AliceGuionnet,andOferZeitouni. Anintroductiontorandommatrices. Number118.Cambridge
universitypress,2010.
FranciscoRubioandXavierMestre. Spectralconvergenceforageneralclassofrandommatrices. Statistics&probability
letters,81(5):592–602,2011.
JeffreyPenningtonandPratikWorah. Nonlinearrandommatrixtheoryfordeeplearning. Advancesinneuralinformation
processingsystems,30,2017.
LucasBenigniandSandrinePéché. Eigenvaluedistributionofsomenonlinearmodelsofrandommatrices. Electronic
JournalofProbability,26:1–37,2021.
LucasBenigniandSandrinePéché. Largesteigenvaluesoftheconjugatekernelofsingle-layeredneuralnetworks. arXiv
preprintarXiv:2201.04753,2022.
ZhouFanandZhichaoWang. Spectraoftheconjugatekernelandneuraltangentkernelforlinear-widthneuralnetworks.
Advancesinneuralinformationprocessingsystems,33:7710–7721,2020.
DiederikPKingma. Adam:Amethodforstochasticoptimization. arXivpreprintarXiv:1412.6980,2014.
GeorgeCybenko. Approximationbysuperpositionsofasigmoidalfunction. Mathematicsofcontrol,signalsandsystems,
2(4):303–314,1989.
KurtHornik,MaxwellStinchcombe,andHalbertWhite. Multilayerfeedforwardnetworksareuniversalapproximators.
Neuralnetworks,2(5):359–366,1989.
RishabhDudeja,YueM.Lu,andSubhabrataSen.Universalityofapproximatemessagepassingwithsemirandommatrices.
TheAnnalsofProbability,51(5):1616–1683,2023.
FedericaGerace,FlorentKrzakala,BrunoLoureiro,LudovicStephan,andLenkaZdeborová. Gaussianuniversalityof
perceptronswithrandomlabels. PhysicalReviewE,109(3):034305,2024.
46BrunoLoureiro,CedricGerbelot,HugoCui,SebastianGoldt,FlorentKrzakala,MarcMezard,andLenkaZdeborová.
Learningcurvesofgenericfeaturesmapsforrealisticdatasetswithateacher-studentmodel. AdvancesinNeural
InformationProcessingSystems,34:18137–18151,2021.
LucaPesce,FlorentKrzakala,BrunoLoureiro,andLudovicStephan.Aregaussiandataallyouneed?theextentsandlimits
ofuniversalityinhigh-dimensionalgeneralizedlinearestimation. InInternationalConferenceonMachineLearning,
pages27680–27708.PMLR,2023.
TianhaoWang,XinyiZhong,andZhouFan.Universalityofapproximatemessagepassingalgorithmsandtensornetworks.
arXivpreprintarXiv:2206.13037,2022.
YueMLuandHorng-TzerYau. Anequivalenceprincipleforthespectrumofrandominner-productkernelmatriceswith
polynomialscalings. arXivpreprintarXiv:2205.06308,2022.
RomanVershynin. High-dimensionalprobability:Anintroductionwithapplicationsindatascience,volume47. Cambridge
universitypress,2018.
RomanVershynin. Introductiontothenon-asymptoticanalysisofrandommatrices. arXivpreprintarXiv:1011.3027,2010.
JoelATropp. User-friendlytailboundsforsumsofrandommatrices. Foundationsofcomputationalmathematics,12:
389–434,2012.
AndreaMontanariandBasilN.Saeed. Universalityofempiricalriskminimization. InPo-LingLohandMaximRaginsky,
editors,ProceedingsofThirtyFifthConferenceonLearningTheory,volume178ofProceedingsofMachineLearning
Research,pages4310–4312.PMLR,02–05Jul2022.
XiuyuanChengandAmitSinger. Thespectrumofrandominner-productkernelmatrices. RandomMatrices:Theoryand
Applications,2(04):1350010,2013.
TrevorHastie,AndreaMontanari,SaharonRosset,andRyanJTibshirani. Surprisesinhigh-dimensionalridgelessleast
squaresinterpolation. Annalsofstatistics,50(2):949,2022.
RomainCouilletandZhenyuLiao. Randommatrixmethodsformachinelearning. CambridgeUniversityPress,2022.
47