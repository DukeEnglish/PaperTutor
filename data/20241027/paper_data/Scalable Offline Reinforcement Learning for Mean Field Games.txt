Scalable Offline Reinforcement Learning for Mean Field Games
AxelBrunnbauer JulianLemmel ZahraBabaiee
TUWien TUWien TUWien
Vienna,Austria Vienna,Austria Vienna,Austria
axel.brunnbauer@tuwien.ac.at julian.lemmel@tuwien.ac.at zahra.babaiee@tuwien.ac.at
SophieNeubauer RaduGrosu
DatenVorsprungGmbH TUWien
Vienna,Austria Vienna,Austria
sophie@datenvorsprung.at radu.grosu@tuwien.ac.at
ABSTRACT andLions[23]andHuangetal.[17],provideascalableapproxima-
Reinforcementlearningalgorithmsformean-fieldgamesoffera tiontolarge-Nplayergamesbymodelingtheinteractionsbetween
scalableframeworkforoptimizingpoliciesinlargepopulationsof individualagentsandastatisticalrepresentationofthepopulation,
interactingagents.Existingmethodsoftendependononlineinter- termedthemean-field.Thisapproachhasshowngreatpromise
actionsoraccesstosystemdynamics,limitingtheirpracticality inreducingthecomplexityofmulti-agentinteractions,allowing
inreal-worldscenarioswheresuchinteractionsareinfeasibleor forthedevelopmentofmoretractablesolutionsinenvironments
difficulttomodel.Inthispaper,wepresentOfflineMunchausen withmanyagents.EarlyworksonMFGstackledproblemsofrela-
MirrorDescent(Off-MMD),anovelmean-fieldRLalgorithmthat tivelysmallscale,oftenunderrestrictiveassumptionssuchaslinear
approximatesequilibriumpoliciesinmean-fieldgamesusingpurely dynamicsandquadraticcostfunctions.Thesesimplifiedmodels,
offlinedata.Byleveragingiterativemirrordescentandimportance whilemathematicallyelegant,limittheapplicabilityofMFGstoreal-
samplingtechniques,Off-MMDestimatesthemean-fielddistribu- worldsystemsthatexhibitcomplex,nonlinearbehavior.However,
tionfromstaticdatasetswithoutrelyingonsimulationorenvi- recentadvancesinthefieldhavefocusedonscalingMFGsolutions
ronmentdynamics.Additionally,weincorporatetechniquesfrom byleveragingdeepreinforcementlearning(DRL)techniques.These
offlinereinforcementlearningtoaddresscommonissueslikeQ- methodsuseneuralnetworkfunctionapproximatorstocompute
valueoverestimation,ensuringrobustpolicylearningevenwith equilibriumpoliciesinMFGs,asdemonstratedin[24,32,33].Such
limiteddatacoverage.Ouralgorithmscalestocomplexenviron- approaches have enabled significant progress in applying MFG
mentsanddemonstratesstrongperformanceonbenchmarktasks theorytomorepracticalandlarge-scaleenvironments.
likecrowdexplorationornavigation,highlightingitsapplicability Despitetheseadvances,acriticalissueremains:mostexisting
toreal-worldmulti-agentsystemswhereonlineexperimentationis MFG-basedRLalgorithmsrelyononlineinteractionwiththeen-
infeasible.WeempiricallydemonstratetherobustnessofOff-MMD vironment.Inmanyreal-worldapplications,particularlythosein-
tolow-qualitydatasetsandconductexperimentstoinvestigateits volvinglargepopulationsofagentsorhumaninteractions(e.g.,
sensitivitytohyperparameterchoices. trafficrouting,crowddynamics,orrecommendationsystems),on-
lineinteractioniseitherimpracticalorethicallyunjustifiable.For
example,insystemswithhumanagents,itisoftencostlyorintru-
KEYWORDS
sivetocollectreal-timedata,andcontinuousexplorationcouldlead
Mean-FieldGames,DeepReinforcementLearning,OfflineRein- tounintendedconsequencessuchasuserdissatisfactionorsafety
forcementLearning risks.Furthermore,environmentswithmanyagentscanbedifficult
tomodelaccurately,andreal-timeexperimentationinsuchsystems
maynotbepossible.
1 INTRODUCTION Insingle-agentRL,offlinelearningisawellresearchedareaand
ReinforcementLearning(RL)hasemergedasafoundationaltoolfor allowstosolvethisproblembyenablingthelearningofpolicies
solvingsequentialdecision-makingproblemsacrossadiverserange frompre-collected,fixeddatasets,eliminatingtheneedforonline
ofdomains,includingrobotics,healthcare,autonomoussystems, interactions.Theseofflinemethodshaveprovenhighlyeffectivein
andgametheory.However,whileRLtechniqueshaveseensignifi- settingswherereal-timeinteractionislimitedorexpensive.How-
cantsuccessinsingle-agentsettings,thetransitiontomulti-agent ever, the application of offline RL techniques to MFGs remains
reinforcementlearning(MARL)presentsuniquechallenges,suchas underexplored.CurrentMFGmethodshavelargelyoverlookedthe
theexponentialgrowthinthestateandactionspacesasthenumber offlinesetting,wherenoonlineinteractionwiththeenvironment
ofinteractingagentsincreases,makingtheproblemsignificantly isavailableduringlearning.
morecomplexbothintermsofcomputationandcoordination. Tobridgethisgap,weproposeOfflineMunchausenMirror
TraditionalMARLmethodsoftendonotscaletomany-agent Descent(Off-MMD),anofflinemean-fieldRLalgorithm.Off-MMD
settingsandrapidlybecomeinfeasibleinenvironmentswithlarge extendstherecentlyintroducedDeepMunchausenOnlineMirror
populationsofagents.Asthenumberofagentsgrows,learning Descent(D-MOMD)methodbyLaurièreetal.[24]andcombines
effectivestrategiescanbecomecomputationallyprohibitive.To thescalabilityofMFGapproximationswiththedataefficiencyof
addressthisissue,Mean-FieldGames(MFGs),introducedbyLasry offlineRL.Tothebestofourknowledge,Off-MMDisthefirstdeep
4202
tcO
32
]GL.sc[
1v89871.0142:viXraoffline RL algorithm specifically designed for MFGs, capable of dynamics,rewards,andevenpolicies.However,inthiswork,we
handlingarbitrarilysizeddatasets.Thisinnovationopensthedoor focusonthecasewhereonlytherewardsdependonthemean-field:
forapplyingMFGtheoryinreal-worldsystemswhereonlinedata
collectionisprohibitive.TheprimarychallengeinadaptingMFGs
𝑟
𝑡
:S×A×Δ
S
↦→R. (1)
Givenapolicy𝜋,themean-fieldflow𝜇𝜋 isdefinedbythefol-
totheofflinesettingstemsfromthefactthatthesesystemstypi-
lowingrecursiverelation,denotedbythemean-fieldevaluation
callyrequireestimatingthedistributionofagents(themean-field),
whichcomplicatesthedirectadaptationofonlinealgorithms.To
operator𝜙(𝜋)=𝜇 𝜋:
addressthis,werepurposeideasfromoff-policypolicyevaluation 𝜇 𝑡𝜋 +1(𝑠′)= ∑︁ ∑︁ 𝑝(𝑠′|𝑠,𝑎)𝜋(𝑎|𝑠)𝜇 𝑡𝜋 (𝑠), (2)
(OPE)toapproximatethemean-fielddistributionusingofflinedata.
𝑠∈S𝑎∈A
Additionally,weapplyarobustregularizationmechanismtomiti- 𝜇 0𝜋 =𝑚0, (3)
gatetheeffectsofdistributionalshift,awell-knownissueinoffline
RLwherethelearningpolicyencountersstatesoractionsnotsuffi-
where𝑝(𝑠′|𝑠,𝑎)representsthetransitiondynamicsoftheenviron-
cientlyrepresentedinthedataset.Thisregularizationstabilizesthe
ment,and𝑚0 istheinitialdistributionoverstates.Thegoalfor
policylearningprocess,ensuringmorereliableperformanceeven anagentistofindapolicy𝜋 : S ↦→ Δ A whichmaximizesthe
expectedsumofrewardswithrespecttoagivenmean-fieldflow𝜇:
inunderrepresentedareasofthestatespace.
WeempiricallyvalidateOff-MMDonasuiteofbenchmarktasks 𝑇
todemonstrateitsefficacy.Specifically,weevaluateitsperformance max 𝐽(𝜋,𝜇)=E 𝜋(cid:104)∑︁ 𝛾𝑡𝑟(𝑠 𝑡,𝑎 𝑡,𝜇 𝑡)(cid:105)
𝜋
ontasksinvolvingbothexplorationandcrowdnavigation,two 𝑡=0
commonchallengesinthefieldofMFGs.Additionally,weexplore subjectto: 𝑠0∼𝜇0
thesensitivityofOff-MMDtothequalityofthedatasetsusedfor 𝑎 𝑡 ∼𝜋(·|𝑠 𝑡)
training, assessing how variations in state-action coverage and
𝑠 𝑡+1∼𝑝(·|𝑠 𝑡,𝑎 𝑡).
trajectoryqualityimpactperformance.Wefurtherinvestigatethe
Bymakingtherewarddependonlyonotheragentsviathemean-
importanceoftheproposedregularizationterm,designedtoprevent
field,insteadoftheindividualstatesandactionsofallotheragents,
theoverestimationof𝑄-values—awell-documentedissueinoffline
weobtainamuchsmalleroptimizationproblemtosolve.
RLalgorithms.Insummary,ourcontributionsare:
(1) OfflineMunchausenMirrorDescent,anoveldeepRLalgo- LearninginMFGs. Incontrasttosingle-agentRL,wherewe
optimizeastationaryrewardsignal,algorithmsforMFGstypically
rithmforofflinelearningforMFGs.
aim to find policies that are close to some equilibrium concept
(2) Extensiveevaluationoftheperformanceandablationstud-
becausetheirperformancelargelydependsonotheragents.The
ieswithrespecttoperformanceanddatasetquality.
conceptofNash-Equilibria(NE),acommonsolutionconceptin
game theory, has been extended to MFGs [23] and is the main
2 BACKGROUND
optimizationtargetforalgorithmssolvingnon-cooperativeMFGs.
Sequentialdecisionmakingproblemscommonlymakeuseoffinite
horizonMarkovdecisionprocess(MDP).AfinitehorizonMDPis Definition2.1. Thebestresponse(BR)toamean-fieldflow𝜇is
thesolutionoftheoptimizationproblem
atuple⟨S,A,𝑟,𝑝,𝛾,𝐻,𝜇0⟩consistingofasetofstatesS,asetof
actionsA,arewardfunction𝑟 :S×A ↦→R,stochasticdynam- 𝜋∗=argmax𝐽(𝜋,𝜇)=𝐵𝑅(𝜇).
ics𝑝 : S×A ↦→ Δ ,discountfactor𝛾 ∈ (0,1),horizon𝐻 ∈ N 𝜋
S
andinitialstatedistribution𝜇0 ∈Δ S.Problemsinvolvingalarge Definition2.2(𝜖-MFNE). A𝜖-Mean-FieldNashEquilibriumwith
numberofinteractingagentsbecomeintractableasthesizeofthe 𝜖 ≥0isdefinedasatuple(𝜋,𝜇𝜋)forwhichthefollowingholds:
stateandactionspacegrowsexponentiallywiththenumberof sup𝐽(𝜋′,𝜇𝜋 ) ≤ 𝐽(𝜋,𝜇𝜋 )+𝜖.
agents.Inmany-agentgames,whereagentsareanonymousand 𝜋′∈Π
identical,Mean-FieldGames(MFGs)offeraneffectiveframework
EarlymethodstosolveMFGsprimarilyinvolvedsolvingcoupled
toapproximateNashequilibriabysimplifyingtheinteractionsbe-
partialdifferentialequations,typicallyaforward-backwardsys-
tweenagents.FirstintroducedbyLasryandLions[23]andHuang
temofHamilton-Jacobi-BellmanandFokker-Planck-Kolmogorov
etal.[17],MFGsaddressthiscomplexitybymodelinginteractions
equations,tocomputethevaluefunctionanddistributionflowof
throughthedistributionofagentstates,ratherthantrackingindivid-
agents[1,2].However,thesemethodsstrugglewithscalabilityin
ualagents.Thisreducesthedimensionalityoftheproblem,making
high-dimensionalstate-actionspacesandcomplexenvironments.
itmoretractableandallowstoapproximatefinite,𝑁-playergames.
AlgorithmsforsolvingMFGsusingRLcommonlyrelyonsome
InMFGs,arepresentativeagentinteractswiththemean-field(i.e.,
formoffixedpointiteration,alternatingbetweenpolicyupdates
thedistributionofallagents),ratherthandirectlyinteractingwith
andthemean-fielddistributioncomputation.Thereby,theyusea
eachindividualagent.Consequently,theproblembecomesopti-
bestresponsecomputationstepbeforecomputingthemean-field.
mizingasinglepolicywithrespecttothispopulationdistribution,
Onconvergence,thefixedpointiteration
whichleadstomorecomputationallyefficientalgorithms.
Inthiswork,weconsiderstochastic,finite-horizonMFGswitha
𝜙(𝜋∗)=𝜙(𝐵𝑅(𝜇∗))=𝜇∗
finitesetofstatesSandactionsA.Themean-field,whichisthe yields a MFNE. However, generally convergence is not guaran-
distributionofagentsoverstatesattime𝑡,isdenotedby𝜇
𝑡
∈Δ S. teed [12] and methods from algorithmic game theory, such as
Inthemostgeneralform,MFGsallowformean-field-dependent Fictitious Play (FP) [4], are used to stabilize training. In recentyears,machinelearningapproaches,particularlyRL,havebeen AlsoCuiandKoeppl[12]andPerrin[31]introducedeepRLbased
exploredasapromisingalternativeforsolvingMFGs[8,14,40]. approachesonMFGs.Otherworksaddresstheassumptionofiden-
OneofthecentralchallengesinusingRLtosolveMFGsliesin ticalagentsandextendMFGstomulti-populationgames[7,13].
thenon-stationarityintroducedbymulti-agentinteractions,which Subramanianetal.[35]introducedecentralizedMFGs,allowingto
complicatesthelearningprocess.DeepLearningvariantsofFP[15] lifttheassumptionofindistinguishableagents.Inverse-RL(IRL)
wereadaptedtotheMFGsettingstoscaletolargerstateandaction methodshavebeenappliedtotheMFGsettingtoinferunknown
spaces[6,32,33,37]. rewardsignals[10,11].Yangetal.[39]introduceanIRLapproach
Inthiswork,wefocusonaclassofalgorithmsthatevaluatea thatlearnsthedynamicsandtherewardmodelfromdata.How-
policyinsteadofcomputingaBRateachiteration[5].Specifically, ever,theirworkfocusesonbehaviorpredictionratherthanfinding
weadapttheOnlineMirrorDescentalgorithm(OMD)[24,30]tothe MFNE.Recentworkonmodel-basedalgorithmsinthemean-field
offlinesetting.OMDalternatesbetweenpolicyevluationandmean- control(MFC)setting,asubclassofMFGsinwhichagentsfully
fieldupdates,asoutlinedinAlgorithm1.ThemaindifferencetoBR cooperate,canlearnamodeloftheenvironmentandusethatto
algorithmsisthatOMDtracksthesumofprevious𝑄-valuesinstead optimizeaMFCpolicywithbettersampleefficiency[16,29].Jusup
ofpolicies.ThepolicyupdateinOMDisasoftmaxoverthesumof etal.[18]extendthistosafety-constrainedproblems.However,
previous𝑄-values.However,itisnotstraightforwardtosumup𝑄 althoughthoseapproacheslearnamodeloftheenvironment,they
functionsinthecaseofnonlinearfunctionapproximators,suchas stillassumeaccesstotheenvironmenttogeneratenewdatafor
neuralnetworks.ThiscanbeavoidedbyapplyingtheMunchausen exploration.
Trick,asitwasproposedin[24]: DespitemanyadvancesinthefieldofMFGs,thedirectionof
purelyofflinelearningremainsanunderexploredtopic.SAFARI[9]
𝑖
𝜋𝑖 =softmax(cid:16)1∑︁ 𝑄𝑗(cid:17) (4) is,toourknowledge,theonlyapproachspecificallydesignedfor
𝜏
𝑗=0 offlinemean-fieldRL,whichisnotdirectlycomparabletoourap-
=argmax
𝜋∈Δ
A(cid:16) ⟨𝜋,𝑄𝑖 ⟩−𝜏KL(cid:0)𝜋||𝜋𝑖−1(cid:1)(cid:17) (5) p inro ua sc inh gas RKit Hd Soe (s Ren po rt oa dp up cr io nx gim Kea rt ne eM lHFN ilbE e. rI tts Sm paa ci en )i en mn bo ev da dti io nn gslie tos
=argmax 𝜋∈Δ A(cid:16) ⟨𝜋,𝑄 (cid:32)𝑖 (cid:32)(cid:32)(cid:32)(cid:32)+ (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)𝜏 (cid:32) ln (cid:32)(cid:32)(cid:32)𝜋 (cid:32)(cid:32)(cid:32)(cid:32)𝑖 (cid:32)(cid:32)− (cid:32)(cid:32)1 ⟩−𝜏 ⟨𝜋 (cid:32)(cid:32)(cid:32),ln (cid:32)𝜋 (cid:32)(cid:32)⟩(cid:17) (6) m reo gude lal rt ih zee dm ve aa lun e-fi ie teld rad tii ost nri bb au st eio dn o, nco fim xeb din te rd ajew ci tt oh ria en s.u Wnc he ir leta ti hn ety o- -
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
𝑄˜𝑖 H(𝜋) reticallyrobust,withdataset-dependentperformancebounds,SA-
=softmax(cid:16)1 𝑄˜𝑖(cid:17)
. (7)
F reA qR uI irf ea ic ne vs es ri tg inn gifi aca Gn rt amsca mla ab tri il xity thi as tsu ge ros. wT sh qe uaR dK raH tS icae lm lyb wed itd hin thg es
𝜏
datasetsize,leadingtosubstantialmemorydemandsandcomputa-
This insight allows us to apply policy evaluation directly on𝑄˜
tionalbottlenecks.
toavoidthesummationof𝑄-functions.InEquation(6), ⟨𝜋,𝑄𝑖⟩
denotestheshorthandnotationfor(cid:205) 𝑎𝜋(𝑎|𝑠)𝑄𝑖(𝑠,𝑎).Themodified
4 METHOD
BellmanOperatoristhendefinedas:
In this section we discuss the foundational components of Off-
(B𝜇𝜋𝑄˜ )(𝑠,𝑎)=𝑟˜+𝛾E 𝑠′,𝑎′[𝑄˜ (𝑠′,𝑎′)−𝜏 ln𝜋𝑖−1 (𝑎′|𝑠′)] (8)
MMD.Inparticular,weelaboratehowwecanleveragemethods
𝑟˜(𝑠,𝑎,𝜇)=𝑟(𝑠,𝑎,𝜇)+𝜏𝛼ln𝜋𝑖−1 (𝑎|𝑠), (9) fromofflinepolicyevaluationtoestimatethemean-field𝜇 from
staticdatasetsandprovideamodifiedversionoftheD-MOMD
withmodifiedreward𝑟˜,whichpenalizesdeviationsfromtheprevi-
algorithmtoadaptittotheofflinelearningregime.
ouspolicy𝜋𝑖−1.Thehyperparameter𝜏 actsasatemperatureand
scalesthesumof𝑄-valuestoavoidprematureconvergencewhereas
4.1 OfflineMean-FieldEstimation
𝛼 isaregularizationparametertocontrolhowfaranewpolicy
Fixedpointalgorithmsforsolvingmean-fieldgames,asdiscussedin
canbefromthepreviouspolicy.Foramoredetailedderivation,we
Section2,iteratebetween(1)evaluatingthemean-fielddistribution
referto[24].
ofagentsand(2)best-responsecomputationorpolicyevaluation.
Thefirststep,mean-fieldestimation,canbedoneviadirectcompu-
Algorithm1MunchausenOnlineMirrorDescentforMFGs[24]
tationifonehasaccesstothetransitionmodelorviaMonte-Carlo
1: for𝑖=1...𝐿do samplesifasimulatorisprovided.Inscenarioswhereonlyastatic
2:
Mean-FieldUpdate:𝜇𝑖 ←𝜙(𝜋𝑖)
datasetofpreviouslycollectedenvironmentinteractionsareavail-
3:
RegularizedPolicyEvaluation:𝑄˜𝑖+1←B𝜋𝑖𝑄˜𝑖
able,onlinealgorithmslikeD-MOMD[24]arenotapplicable.
𝜇𝑖
4:
PolicyUpdate:𝜋𝑖+1(·|𝑠)←softmax( 𝜏1𝑄˜𝑖+1(𝑠,·)) Therefore,weseektoapproximatethemean-fieldflow
5: endfor 𝜇 𝑡𝜋 +1(𝑠′)=∑︁∑︁ 𝑝(𝑠′|𝑠,𝑎)𝜋(𝑎|𝑠)𝜇 𝑡𝜋 (𝑠) (10)
𝑠∈𝑆𝑎∈𝐴
withouthavingaccesstothetransitionmodel𝑝(𝑠′|𝑠,𝑎).Oneap-
3 RELATEDWORK proach to mitigatethis is toleveragethe data tolearn a model
Recentyearsbroughtupmanyworksthataddressvariouslimi- ofthedynamics.However,themodelsmaybeinaccurateinlarge
tationsofMFGs.Laurièreetal.[24]introducedthebasisofour actionspaces,wherenotallactionsarefrequentlyvisited.More-
work,DeepMunchausenMirrorDescent,adeepneuralnetwork over,approximatingtheenvironmentdynamicswithneuralnet-
based variant of OMD [30] with strong empirical performance. worksmightcauseadditionalbiasesfromcovariateshiftsduetothechangeofpolicies[38].Inthiswork,weleveragethefactthat 4.2 OfflineMunchausenMirrorDescent
thisproblemcanbeequivalentlytreatedasanoff-policystateden- InSection4.1,weintroducedanofflinemethodforestimatingthe
sityestimationproblem.OPEmethodsaredesignedtoestimate mean-fielddistributionofapolicy.Thismethodcan,inprinciple,
quantitiessuchasrewardsorvaluefunctionsfromoff-policysam- bedirectlyappliedtoD-MOMDtoadaptittotheofflinelearning
ples.Werepurposethisideatoestimatethestatedistributionunder setting.TheupdateruleinouralgorithmfollowstheclassicTD-
a new policy. In particular, we are interested to estimate 𝜇 𝜋(𝑠) errorminimizationapproach,asusedinDQN[27],wherethetarget
fromsamplesthatarenotcollectedfrom𝜋 butsomeother,pos- 𝑄-valuesareparameterizedby𝜃¯.Specifically,theobjectiveisto
siblyunknown,behaviorpolicy𝜋 𝛽 andwithouthavingaccessto minimizethetemporal-differenceerrorwherethepolicyevaluation
environmentdynamics𝑝(𝑠′|𝑠,𝑎). operatorisdefinedasinEquation(8):
Let𝑑 𝜇𝜋(𝑠,𝑎)=𝜋(𝑎|𝑠)𝜇 𝜋(𝑠)bethejointstate-actiondistribution
givenamean-field𝜇 𝜋 andpolicy𝜋.Werestatethemean-fieldflow minE (𝑠,𝑎,𝑠′)∼D(cid:104) (cid:0)𝑄 𝜃(𝑠,𝑎)−(B𝜇𝜋𝑄 𝜃¯)(𝑠,𝑎)(cid:1)2(cid:105) . (15)
asanexpectationover𝑑𝜋: 𝜃
𝜇
However,naivelyapplyingoff-policyalgorithmstoofflineRL
𝜇 𝜋𝑡+1 (𝑠′)=E (𝑠,𝑎)∼𝑑 𝜇𝜋 𝑡(cid:104) 𝑝(𝑠′|𝑠,𝑎)(cid:105) . (11) t na os tk wst ey lp li rc ea pl rly esl ee nad tes dto inth the eov de ar te as st ei tm .Tat oio an ddo rf e𝑄 ss-v ta hl iu se is ssfo ur e,a wct eio in ns
-
corporatearegularizationtermfollowingtheideaofConservative
Ingeneral,wecouldapplyanyOPEmethodcapableofestimating
Q-Learning(CQL)[21],whichisdesignedtolearnaconservative
density(ratios)suchasmodel-basedestimators[19,41]orDICE-
lowerboundofthetrue𝑄-function.CQLintroducesaregularized
styleapproachesforestimatingstationarydistributions[26,36],as
versionoftheBellmanequation,wheretheobjectivebalancesthe
Off-MMDisagnostictotheestimationmethod.Inthisworkwe
maximizationofthe𝑄-valuesoverthedatasetandtheminimiza-
decideforusingimportancesamplingtoestimate𝜇𝑡+1.Inparticu-
𝜋 tionofthetemporal-differenceerror.ThegeneralCQLoptimization
lar,wemakeuseofthemarginalizedimportancesampling(MIS)
problemisgivenas:
estimator of Xie et al. [38] because of its theoretical properties
a ren qd uii rts easi dm dip til oic nit ay lsc to epm sp ,sa ure chd ato sso ot lh ve inr ga ap npr ino nac eh rs o, pw timhi ic zh att iy op ni pca roll by
-
m 𝑄inm 𝜋a ˜xE (𝑠,𝑎)∼D,𝜋˜(cid:2)𝑄(𝑠,𝑎)(cid:3) −E (𝑠,𝑎)∼D,𝜋𝛽(cid:2)𝑄(𝑠,𝑎)(cid:3)
(16)
lem[28,42]orfittinganothermodel[19,41]. +(cid:12) (cid:12)𝑄−B∗𝑄(cid:12) (cid:12)2 +R(𝜋˜),
Let𝑑(𝑠,𝑎)=𝜋 𝛽(𝑎|𝑠)𝑑(𝑠)bethejointstate-actiondistributionof
thedatasetcollectedunderbehaviorpolicy𝜋 .Inpractice,𝜋 isof- where𝜋˜ representsapolicyusedtodefinethejoint-actiondistribu-
𝛽 𝛽
tenunknownandcanbeapproximatedusingthestate-conditional tionoverwhichweminimizethestate-actionvalues.Thesecond
empiricaldistributionoveractionsinD[21].Wecanapplyimpor- termencouragestighterboundsbymaximizing𝑄-valuesunderthe
tancesamplingtoreformulateEquation(11)as datasetdistribution.ThelasttermistheclassicBellmanequation
minimizingtheTDerrorwitharegularizationtermRappliedto
(cid:34)𝑑𝜋 (𝑠,𝑎) (cid:35) 𝜋˜.ForspecificchoicesofR,theinnermaximizationproblemcan
𝜇 𝜋𝑡+1 (𝑠′)=E (𝑠,𝑎)∼𝑑 𝑑𝜇𝑡 (𝑠,𝑎) 𝑝(𝑠′|𝑠,𝑎) (12) besolvedinclosedform.Acommonchoicefor R istousethe
KLdivergencetosomeprioractiondistribution𝜌.Ifwechose𝜌
=E
(𝑠,𝑎)∼𝑑(cid:104)𝜋
𝜋
𝛽(𝑎 (𝑎|𝑠 |) 𝑠)𝜇 𝑑𝜋𝑡 (( 𝑠𝑠 )) 𝑝(𝑠′|𝑠,𝑎)(cid:105)
. (13)
t ro egb ue lat rh ie zeu dn ,i cf lo or sm edd -fis ot rr mibu lot sio sn fuo nv ce tr ioa nct fi oo rn Os, ffw -Meo Mb Dta :inanentropy
Thisfactorizationofthestate-actiondistributionallowsustoapply L(𝜃)=𝜂E (𝑠,𝑎)∼D(cid:104) log∑︁ exp𝑄 𝜃(𝑠,𝑎′)−𝑄 𝜃(𝑠,𝑎)(cid:105)
MIStoapproximateEquation(13)usingsamples(𝑠𝑖,𝑎𝑖,𝑠𝑖 )from 𝑎′ (17)
𝑡 𝑡 𝑡+1
finite dataset D. Let𝑑ˆ (𝑠 𝑡) = |D1
|
(cid:205) 𝑖|D|1[𝑠 𝑡(𝑖) = 𝑠 𝑡] denote the +E (𝑠,𝑎,𝑠′)∼D(cid:104) (cid:0)𝑄 𝜃(𝑠,𝑎)−(B𝜇𝜋𝑄 𝜃¯)(𝑠,𝑎)(cid:1)2(cid:105) ,
empiricalstatedistributionattime𝑡,thenthemarginalizedstate
where𝜂isahyperparametertocontroltheimportanceoftheregu-
distributioncanbeestimatedrecursivelyby
larization.
𝜇 𝜋𝑡+1 (𝑠)≈ |D1
|
∑︁| 𝑖D =0| 𝜇 𝑑𝜋𝑡
ˆ
(( 𝑠𝑠 𝑡(𝑡 𝑖(𝑖 )) )) 𝜋𝜋 𝛽( (𝑎 𝑎𝑡( 𝑡(𝑖 𝑖) )|𝑠 |𝑠𝑡( 𝑡(𝑖 𝑖) ))
)
1[𝑠 𝑡( +𝑖) 1=𝑠]
(14)
E toqu cT oah mte io pp n use t( eu 14d t) ho t e- oc lo ocd soe smf fo p ur u ntO ce tff it o- h nM e aM o nfflD di ui ns pes dmh ao tew ean tn h-i fi en e pA l adl rg a ao n mr di et E th eqm ru sa2 𝜃t. iW vo in ae ( su 1 ts 7 oe )
-
𝜇 𝜋0 (𝑠)=𝑑ˆ (𝑠0). c sch ha est mic ag ar sad Ai le gn ot rd ite hs mcen 1.t.Off-MMDthusfollowsthesameiterative
This yields an unbiased estimator of 𝜇 with polynomial error
𝜋
boundwithrespecttotimehorizon𝐻,whichreducestoO(𝐻)in 5 EXPERIMENTALEVALUATION
somecases,suchasboundedmaximumexpectedreturns[38].Note WeevaluatetheOff-MOMDalgorithmontwogrid-worldproblems
that,unlikeintypicalsingle-agentofflineRLscenarios,wecan introducedbyLaurièreetal.[24]andcompareitsperformance
notdirectlyestimatereward𝑟 ,asitdependsnonlinearlyon𝜇 in againsttheonlinevariant.Wealsoconductexperimentstoinvesti-
𝑡 𝑡
general.Thus,forthegeneralcase,werequireaccesstothereward gatethesensitivitytothequalityofthedatasetandinvestigatethe
function.Forspecialcases,suchasrewardfunctionsmonotonicin importanceoftheconservativeconstraintonthelossfunction.The
𝜇,wecouldinprincipleapproximate𝑟 directly. algorithmsandtheenvironmentsareimplementedinJAX[3]and
𝑡Algorithm2OfflineMunchausenMirrorDescent(Off-MMD) ExplorationTask. Inthistask,agentsstartintheleftupper
1:
Input:DatasetD,initialparameters𝜃1 cornerandmustspreadevenlyacrossallfourrooms.Thereward
functionisdefinedas
2: for𝑖=1...𝐿do
3: Estimatemean-field𝜇𝑖 usingeq.(14) 𝑟(𝑠 𝑡,𝑎 𝑡,𝜇 𝑡)=−log𝜇 𝑡(𝑠 𝑡),
4: for𝑗 =1...𝐵do
5: SamplebatchB:{(𝑠 𝑡𝑘,𝑎𝑘 𝑡,𝑠 𝑡𝑘 +1)} 𝑘𝑁 =1∼D w hih gi hc eh ri rn ec wen at ri dv siz wes ha eg ne ln ot ws -t do eo nc sc iu typy stl ae ts es sc ar ro ew rd ee ad chs eta dt .e Ts, hl eea od pi tn ig mt ao
l
6: Relabelrewardusing𝜇 𝑡𝑖:𝑟 𝑡𝑘 =𝑟(𝑠 𝑡𝑘,𝑎𝑘 𝑡,𝜇 𝑡𝑖)
policyforthistaskspreadsevenlyacrossthewholestate-space.
7: Update:𝜃 𝑖 ←𝜃 𝑖 −∇𝜃L(𝜃 𝑖)usingeq.(17)
InFigure1a,wepresenttheexploitabilityover100iterationsof
8: endfor
ouralgorithm.Theonlinevariantrapidlyconvergestotheexpected
9:
𝜃𝑖+1←𝜃𝑖
10:
Updatepolicy:𝜋(𝑎|𝑠)=softmax(cid:0) 𝜏1𝑄¯ 𝜃𝑖+1(𝑠,𝑎)(cid:1) o Mu Mtc Dom ,ee a. cW hte rae iv na el dua ot ne ath de ap tae sr ef to orm fda in ffc ee reo nf tt qh ure ae liti yn .s Wtan hc ee ns tro af inO eff d-
11: endfor
onsufficientlyhigh-qualitydatasets,Off-MMDconsistentlylearns
policiesthatperformwell.Figure1billustratestheevolutionofthe
buildoncodebyKostrikov[20]andLanctotetal.[22].Thecode mean-fieldovertime,supportingthisobservation.Asexpected,the
fortheexperimentsisavailableonGitHub.1 policytrainedondatageneratedbyauniformrandompolicyfails
tospreadevenlyacrossallrooms,particularlyinthelower-right
5.1 ExperimentSetup room,duetoinsufficientcoverageofthisregioninthedataset.
Tomakerunscomparablewitheachother,weemployExploitability CrowdModellingwithCongestion. Inthistask,agentsalso
asanevaluationcriteria(alsooftenreferredtoasRegret): startintheupperleftcorner.Differentlytotheexplorationtask,
E(𝜋,𝜇)=max𝐽(𝜋′,𝜇)−𝐽(𝜋,𝜇). agentsmustnavigatetothetargetpositioninthelowerrightcor-
𝜋′ nerwhileavoidinghigh-densityareas.Furthermore,wesimulate
ItdirectlymeasureshowfaralearnedpolicyisfromaMFNEby congestioneffectsbypenalizingmovementswhenagentsarein
quantifyingthepotentialutilityanagentcangainbydeviatingfrom crowdedareas.Therewardfunctionisdefinedas
itspolicy,withlowerexploitabilityindicatingbetterequilibrium
approximation.Weusethesameevaluationprotocolasin[24]and
𝑟(𝑠 𝑡,𝑎 𝑡,𝜇 𝑡)=−||𝑠
𝑡
−𝑠target||−𝜇 𝑡(𝑠 𝑡)||𝑎||−log𝜇 𝑡(𝑠 𝑡).
computethegroundtruthmean-fieldandexploitability. Thisisamorecomplexrewardfunction,asitposesatrade-offof
Both, Off-MMD and D-MOMD, optimize a𝑄 function repre- conflictinggoalsfortheagents.Theresults,showninFigure2a,
sentedasaneuralnetworkwith3layersof128parametersand demonstrateconvergenceofOff-MMD(Exp)andOff-MMD(Int)
ReLUactivations.Thehyperparamersettingsarethesameforall towardsthebaseline.Notably,thepolicytrainedontherandom
instancesofOff-MMDoveralltasks,excepttheablationstudies. datasetalsoperformsreasonablywell.Wehypothesizethatthe
Formoredetails,werefertoAppendixA. distancepenaltyprovideseffectiveguidance,enablingthepolicy
tosolvethetaskevenwithlimitedstate-actioncoverageincertain
5.2 PerformanceEvaluation partsofthestatespace.
Weevaluatetheperformanceofouralgorithmontwodistincttasks
withinagridworldenvironmentconsistingoffourseparatedrooms 5.3 ImpactofDatasetQuality
connectedbynarrowcorridors,asdescribedin[24,25].Agentscan
InofflineRL,weareinterestedintherobustnessofpolicyperfor-
choosefromfiveactions:moveup,down,left,right,orstayinplace.
mancestodatasetquality.Inthisexperiment,weaimtoinvestigate
Ifanactionresultsinacollisionwithawall,theagentremainsin
thesensitivityofOff-MMDtochangesinthequalityofthetrajecto-
itscurrentposition.Thetimehorizonforeachepisodeissetto40
riesinthedatasetandthecoverageofthestatespace.Buildingon
timesteps.Thetwotasksweevaluateareexplorationandcrowd
themethodologyofSchweighoferetal.[34],wecategorizedatasets
navigation.Foreachtask,wetrainOff-MMDonthreedatasetsof
basedontwocriteria:state-actioncoverageandtrajectoryquality.
varyingqualityandcompareitsexploitabilityagainstthebaseline.
ThefollowingvariantsofOff-MMDareincludedintheevaluation: Definition5.1(State-ActionCoverage). Let𝑢 𝑠,𝑎(D) denotethe
• D-MOMD:Theonlinebaselinealgorithm. numberofuniquestate-actionpairsinadatasetD,thenthestate-
• Off-MMD(Exp):Trainedondatacollectedbyafullytrained actioncoverageofthisdatasetisdefinedas
D-MOMDpolicy. 𝑢 𝑠,𝑎(D)
• Off-MMD(Int):Trainedondatacollectedfromaninter- Coverage(D)= |𝑆||𝐴| . (18)
mediatecheckpoint.
• Off-MMD(Rand):Trainedondatacollectedfromauni- Definition5.2(TrajectoryQuality). Let𝑔(D)denotetheaverage
formrandompolicy. episodereturnofadataset.Furthermore,letDminandDexpertbe
referencedatasets,collectedbyasuboptimalandanexpertpolicy,
Alldatasetscontain100Kepisodeswith40timestepseach.Thesub-
sequentsectionspresenttheevaluationresultsfortheexploration
respectively.ThetrajectoryqualityofanotherdatasetDisdefined
as
andcrowdnavigationtasks.
Quality(D)=
𝑔(D)−𝑔(Dmin)
. (19)
1https://github.com/axelbr/offline-mmd 𝑔(Dexpert)−𝑔(Dmin)Exploration Navigation
300
D-MOMD 600 D-MOMD
250 Off-MMD(Rand) Off-MMD(Rand)
Off-MMD(Int) 500 Off-MMD(Int)
200 Off-MMD(Expert) Off-MMD(Expert)
400
150
300
100 200
50 100
0
0 20 40 60 80 100 0 20 40 60 80 100
Iteration Iteration
(a)Exploitability (a)Exploitability
t=1 t=5 t=10 t=20 t=40 t=1 t=5 t=10 t=20 t=40
10−3 10−2 10−1 100 10−3 10−2 10−1 100
(b)Mean-FieldEvolution (b)Mean-FieldEvolution
Figure 1: (a) Off-MMD can approximate the performance Figure2:Off-MMDperformsbestwithintermediateandex-
of D-MOMD on the Exploration task when being trained pertqualitydatasets.Comparedtotheexplorationtask,the
onreasonablygooddatasets.Trainingrunswereconducted policytrainedontherandombehaviordatasetperformsbet-
over10seedsfor100iterationsofOff-MMDandD-MOMD. ter.ExperimentsettingsarethesameasinFigure1.
Wereportthemeanexploitabilityandthe95%confidence
interval. (b) Evolution of the mean-field over timesteps𝑡.
behaviorcanbeexplainedbythechallengesinherentinmulti-agent
Darkerareasindicatehigherstatedensity.
systems:thepolicyreturnisstronglyinfluencedbythebehavior
ofotheragents.Therefore,performanceachievedunderonemean-
Inourexperiments,weusedatasetscollectedbytheexpertpolicy fieldsettingmaynotbecomparabletoanother.
andtherandompolicyforcomputingthenormalizationboundsin InFigure4,wevisualizetheapproximationofthemean-field
Equation(19).Usingthethreedatasetsintroducedpreviouslyfor underdatasetsofvaryingqualityforafixedpolicy.Theleftmostcol-
thenavigationtask,wegenerate100syntheticdatasetsbyrandomly umnshowstheempiricalstatedistributionofthedatasetswhereas
subsamplingepisodes.Thedatasetsizesrangefrom1,000to100,000 thecenterandrightcolumnshowsthemean-fieldundersomefixed
episodes.WethentrainpoliciesusingOff-MMDonthesedatasets policy.Thegroundtruthmean-fieldservesasareferenceforthe
toevaluatetheeffectofdatasetqualityonperformance. approximationintheright-mostcolumnandisthereforeindepen-
Figure3presentstheexploitabilityofOff-MMDwhentrainedon dentofthedatasetquality.Figure4showshowtheapproximation
datasetswithvaryingstate-actioncoverageandtrajectoryquality. ofthemean-fieldchangeswiththestatecoverageinthedataset.
Theresultsindicateastrongcorrelationbetweenstate-actioncov- Thisisparticularlyprevalentinthefirstrow,wherewechosethe
erageandperformance,whereastrajectoryqualityappearstobea datasetwiththeloweststatecoveragefromthesetofsynthetic
weakerpredictor,exceptinextremecasessuchasexpertdemon- datasets.Theapproximationcannotprovideestimatesforunvis-
strationsorfullyrandomdatasets(highlightedinFigure3).This itedpartsofthestatespace.However,forthestatesthatareinthe
ytilibatiolpxE
enilnO
trepxE
tnI
dnaR
ytilibatiolpxE
enilnO
trepxE
tnI
dnaRExploitabilityvs. DatasetQuality Empirical GroundTruth Approximation
1.0
160
0.8
140
0.6
120
0.4
100
0.2 80
0.0 60
0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45
State-ActionCoverage
Expert Intermediate Random
Figure3:Exploitabilityvs.Data-Quality:Eachpointrepre-
sentsatrainingrunofOff-MMDonadatasetwithspecific
state-actioncoverageandtrajectoryquality.Thecolorindi-
catestheexploitabilityofthepolicyafter100iterationswith
darkercolorsindicatinghigherexploitability.Forreference,
wealsomarkthedatasetsusedinpreviousexperiments. Figure4:Theleftcolumnshowstheempiricalstatedistri-
butionofdatasetscollectedbybehaviorpolicy𝜋𝛽
(fromthe
navigationtask)withdifferentstatecoverages.Thecenter
dataset,itproducescorrectestimationsofthemean-fielddistribu- columnshowstheground-truthmean-fieldthatisgenerated
tion.Inscenarioswithhigherqualitydatasets,weobserveaccurate bythenewpolicytoevaluateandtherightcolumnshows
approximationsoftheground-truthdistribution. theapproximatedmean-fieldofthatpolicyusingjustthe
dataset.Themean-fieldsarepickedat𝑡 = 15.Whitespots
5.4 EffectofRegularization indicatenostate-actioncoverageinthisarea.
Inthefollowing,weinvestigatetheimportanceoftheCQLregu-
RegularizationStrength
larizationtermwithrespecttotherobustnesstovaryinglevelsof
600
state-coverageinthedatasetandthetrainingstabilityofOff-MMD. η=0.0
Thefirstexperiment,showninFigure5,aimstoexaminethe η=0.5
500
effect of the regularization hyperparameter𝜂 on the quality of η=1.0
thepolicy.Weconducttrainingrunswithvaryingregularization η=2.0
400
strengths,rangingfrom0(e.g.noregularization)to5acrossdatasets η=5.0
D-MOMD
withdifferentlevelsofstate-actioncoverage.Specifically,weselect 300
fivedatasetsclosesttoeachstate-actioncoveragebin,withcover-
agevaluesrangingfrom0.15to0.45.Off-MMDistrainedfor100 200
iterationsoneachofthesedatasets,allowingustoanalyzetheinflu-
enceofregularizationunderdiversecoverageconditions.Figure5 100
showsthatmoderateregularizationallowstoreachcomparable
exploitabilityasD-MOMDonhigherstate-actioncoverageswhile 0.15 0.20 0.25 0.30 0.35 0.40 0.45
beingsignificantlymorerobustthanOff-MMDwithoutregulariza- DatasetCoverage
tion.Thebestfinalperformanceisachievedwith𝜂 =2.0,which
coincideswiththerecommendedsettingforCQL[21].Furthermore, Figure5:Wereporttheexploitabilityofpolicieswithdif-
wecanseethatlargervaluesfortheregularizationhyperparameter ferentvaluesof𝜂 ineq.(17).Wetraineachconfiguration
𝜂 dampenthedifferenceinperformanceoverdifferentcoverage on5datasetsthathavestate-actioncoveragesclosetoaspe-
levels,buthurtthemaximumachievableperformance. cificvalue,rangingfrom0.15to0.45.Wereportthemean
Inanotherablationexperiment,weinvestigatethetrainingstabil- exploitabilityofthepoliciesafter100iterations.Forrefer-
ityofOff-MMD,specificallyfocusingonmonotonicimprovement, ence,wealsoplottheperformanceoftheonlinebaseline
underdifferentvaluesoftheregularizationparameter𝜂.Figure6 after100iterations.
presentstheresultsoffivetrainingrunsontheexpertdatasetofthe
navigationtask,eachwithadifferentvalueof𝜂.Theresultsshow
thattheregularizationtermplaysacrucialroleinstabilizingthe
ytilauQyrotcejarT
ytilibatiolpxE
ytilauQwoL
ytilauQmuideM
ytilauQhgiH
ytilibatiolpxEOptimizationStability agents.Addressingthislimitationoffersapromisingavenueforfu-
tureresearch.Onepotentialsolutioncouldinvolveadaptingmodel-
600 η=0.0
basedalgorithmsspecificallydesignedforofflineRLsettings,to
η=0.5
500 η=1.0 handlemean-fielddependenciesindynamics.Suchadvancements
η=2.0 wouldbroadenthescopeofOff-MMD,makingitapplicabletoa
400 η=5.0 widerrangeofmulti-agentsystemswhereinteractionsbetween
agentsandtheenvironmentaremoreintertwined.
300
200 REFERENCES
[1] YvesAchdou,FabioCamilli,andItaloCapuzzo-Dolcetta.2012. MeanField
100 Games:NumericalMethodsforthePlanningProblem. SIAMJournalonCon-
trolandOptimization50,1(2012),77–109. https://doi.org/10.1137/100790069
arXiv:https://doi.org/10.1137/100790069
0 20 40 60 80 100 [2] YvesAchdouandItaloCapuzzo-Dolcetta.2010.MeanFieldGames:Numerical
Iteration Methods.SIAMJ.Numer.Anal.48,3(2010),1136–1162. https://doi.org/10.1137/
090758477arXiv:https://doi.org/10.1137/090758477
[3] JamesBradbury,RoyFrostig,PeterHawkins,MatthewJamesJohnson,Chris
Figure6:Weoptimizepolicieswithdifferentvaluesof𝜂on Leary,DougalMaclaurin,GeorgeNecula,AdamPaszke,JakeVanderPlas,Skye
Wanderman-Milne,andQiaoZhang.2018.JAX:composabletransformationsof
thesameexpertdatasetofthenavigationtasksandplotthe Python+NumPyprograms. http://github.com/jax-ml/jax
exploitabilityovertrainingiterations.Weshowthemean [4] GeorgeW.Brown.1951.IterativeSolutionofGamesbyFictitiousPlay.InActivity
and95%confidenceintervalover10seeds.
AnalysisofProductionandAllocation,T.C.Koopmans(Ed.).Wiley,NewYork.
[5] Cacace,Simone,Camilli,Fabio,andGoffi,Alessandro.2021.Apolicyiteration
methodformeanfieldgames.ESAIM:COCV27(2021),85. https://doi.org/10.
1051/cocv/2021081
trainingdynamics.Lowervaluesof𝜂leadtooscillationsinpolicy [6] PierreCardaliaguetandSaeedHadikhanloo.2017.Learninginmeanfieldgames:
Thefictitiousplay.ESAIM:Control,OptimisationandCalculusofVariations23,2
performanceand,insomecases,evenresultindivergence,asseen (2017),569–591. https://doi.org/10.1051/cocv/2016004
intheunregularizedcase.Incontrast,highervaluesof𝜂 reduce [7] ReneCarmona,DanielCooney,ChristyGraves,andMathieuLauriere.2019.
StochasticGraphonGames:I.TheStaticCase. arXiv:1911.10664[math.OC]
performancefluctuationsbetweeniterations,contributingtomore
https://arxiv.org/abs/1911.10664
stableandconsistentlearningprogress. [8] RenéCarmona,MathieuLaurière,andZongjunTan.2021.Model-FreeMean-
FieldReinforcementLearning:Mean-FieldMDPandMean-FieldQ-Learning.
arXiv:1910.12802[math.OC] https://arxiv.org/abs/1910.12802
6 CONCLUSION
[9] MinshuoChen,YanLi,EthanWang,ZhuoranYang,ZhaoranWang,andTuoZhao.
We present Offline Munchausen Mirror Descent (Off-MMD), a 2021.PessimismMeetsInvariance:ProvablyEfficientOfflineMean-FieldMulti-
AgentRL.InAdvancesinNeuralInformationProcessingSystems,M.Ranzato,
novelalgorithmdesignedforlearningequilibriumpoliciesinmean- A.Beygelzimer,Y.Dauphin,P.S.Liang,andJ.WortmanVaughan(Eds.),Vol.34.
fieldgamesusingonlyofflinedata.Thisapproachaddressesthe CurranAssociates,Inc.,17913–17926. https://proceedings.neurips.cc/paper_
files/paper/2021/file/9559fc73b13fa721a816958488a5b449-Paper.pdf
limitationsofexistingmethodsthatrelyoncostlyandoftenim-
[10] YangChen,LiboZhang,JiamouLiu,andShuyueHu.2022. Individual-Level
practicalonlineinteractions.Byleveragingimportancesampling InverseReinforcementLearningforMeanFieldGames.InProceedingsofthe21st
and𝑄-valueregularizationtechniques,Off-MMDprovidesaneffi- InternationalConferenceonAutonomousAgentsandMultiagentSystems(Virtual
cientmeanstoapproximatethemean-fielddistributionfromstatic
Event,NewZealand)(AAMAS’22).InternationalFoundationforAutonomous
AgentsandMultiagentSystems,Richland,SC,253–262.
datasets,ensuringscalabilityandrobustnessincomplexenviron- [11] YangChen,LiboZhang,JiamouLiu,andMichaelWitbrock.2023.Adversarial
ments.Ourempiricalevaluationsdemonstratedthealgorithm’s InverseReinforcementLearningforMeanFieldGames.InProceedingsofthe2023
InternationalConferenceonAutonomousAgentsandMultiagentSystems(London,
strongperformanceacrosstwocommonbenchmarksforMFGs, UnitedKingdom)(AAMAS’23).InternationalFoundationforAutonomousAgents
eveninscenarioswithlimiteddatacoverageorsub-optimaldatasets. andMultiagentSystems,Richland,SC,1088–1096.
[12] KaiCuiandHeinzKoeppl.2021.ApproximatelySolvingMeanFieldGamesvia
Withitsabilitytoscaleandadapttoreal-worldmulti-agentsystems,
Entropy-RegularizedDeepReinforcementLearning.InProceedingsofThe24th
Off-MMDopensnewavenuesforapplyingRLbasedalgorithms
InternationalConferenceonArtificialIntelligenceandStatistics(Proceedingsof
forMFGstosettingswhereonlineexperimentationisinfeasible, MachineLearningResearch,Vol.130),ArindamBanerjeeandKenjiFukumizu
(Eds.).PMLR,1909–1917. https://proceedings.mlr.press/v130/cui21a.html
irresponsibleordifficulttomodel.Webelievethatthisworklays
[13] ChristianFabian,KaiCui,andHeinzKoeppl.2023.LearningSparseGraphon
thefoundationforfutureresearchintoofflinelearningmethods MeanFieldGames.InProceedingsofThe26thInternationalConferenceonArtificial
forcomplex,large-scalemulti-agentinteractions,bridgingthegap IntelligenceandStatistics(ProceedingsofMachineLearningResearch,Vol.206),
FranciscoRuiz,JenniferDy,andJan-WillemvandeMeent(Eds.).PMLR,4486–
betweenofflineRLandMFGs.Futureresearchdirectionsinclude 4514. https://proceedings.mlr.press/v206/fabian23a.html
applicationstoreal-worlduse-casessuchaslocationrecommen- [14] XinGuo,AnranHu,RenyuanXu,andJunziZhang.2019. LearningMean-
dationsystemsortrafficrouting,twoproblemdomainssuffering
FieldGames.InAdvancesinNeuralInformationProcessingSystems,H.Wallach,
H.Larochelle,A.Beygelzimer,F.d'Alché-Buc,E.Fox,andR.Garnett(Eds.),Vol.32.
fromovercrowdingeffectsduetoselfishagents. CurranAssociates,Inc. https://proceedings.neurips.cc/paper_files/paper/2019/
file/030e65da2b1c944090548d36b244b28d-Paper.pdf
[15] JohannesHeinrich,MarcLanctot,andDavidSilver.2015.FictitiousSelf-Playin
6.1 Limitations
Extensive-FormGames.InProceedingsofthe32ndInternationalConferenceon
WhileOff-MMDmarksafirststeptowardsscalableofflineRLal- MachineLearning(ProceedingsofMachineLearningResearch,Vol.37),Francis
BachandDavidBlei(Eds.).PMLR,Lille,France,805–813. https://proceedings.
gorithmsforMFGs,itiscurrentlylimitedtoenvironmentswhere mlr.press/v37/heinrich15.html
thedynamicsareindependentofthemean-field.Thisassumption [16] JiaweiHuang,BatuhanYardim,andNiaoHe.2024.OntheStatisticalEfficiency
restrictsitsapplicabilitytoscenarioswheretheenvironmentdy- ofMean-FieldReinforcementLearningwithGeneralFunctionApproximation.
InProceedingsofThe27thInternationalConferenceonArtificialIntelligenceand
namicsarenotarbitrarilyinfluencedbythecollectivebehaviorof Statistics(ProceedingsofMachineLearningResearch,Vol.238),SanjoyDasgupta,
ytilibatiolpxEStephanMandt,andYingzhenLi(Eds.).PMLR,289–297. https://proceedings. Intelligence,IJCAI-21,Zhi-HuaZhou(Ed.).InternationalJointConferenceson
mlr.press/v238/huang24a.html ArtificialIntelligenceOrganization,356–362. https://doi.org/10.24963/ijcai.2021/
[17] MinyiHuang,RolandMalhame,andPeterCaines.2006.Largepopulationsto- 50MainTrack.
chasticdynamicgames:Closed-loopMcKean-VlasovsystemsandtheNash [33] SarahPerrin,JulienPerolat,MathieuLauriere,MatthieuGeist,RomualdElie,
certainty equivalence principle. Commun. Inf. Syst. 6 (01 2006). https: andOlivierPietquin.2020.FictitiousPlayforMeanFieldGames:Continuous
//doi.org/10.4310/CIS.2006.v6.n3.a5 TimeAnalysisandApplications.InAdvancesinNeuralInformationProcessing
[18] MatejJusup,BarnaPásztor,TadeuszJanik,KenanZhang,FrancescoCorman, Systems,H.Larochelle,M.Ranzato,R.Hadsell,M.F.Balcan,andH.Lin(Eds.),
AndreasKrause,andIlijaBogunovic.2024.SafeModel-BasedMulti-AgentMean- Vol.33.CurranAssociates,Inc.,13199–13213. https://proceedings.neurips.cc/
FieldReinforcementLearning.InProceedingsofthe23rdInternationalConference paper_files/paper/2020/file/995ca733e3657ff9f5f3c823d73371e1-Paper.pdf
onAutonomousAgentsandMultiagentSystems(Auckland,NewZealand)(AAMAS [34] KajetanSchweighofer,Marius-constantinDinu,AndreasRadler,MarkusHof-
’24).InternationalFoundationforAutonomousAgentsandMultiagentSystems, marcher,VihangPrakashPatil,AngelaBitto-nemling,HamidEghbal-zadeh,
Richland,SC,973–982. andSeppHochreiter.2022. ADatasetPerspectiveonOfflineReinforcement
[19] Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Learning.InProceedingsofThe1stConferenceonLifelongLearningAgents(Pro-
Joachims.2020. MOReL:Model-BasedOfflineReinforcementLearning.In ceedingsofMachineLearningResearch,Vol.199),SarathChandar,RazvanPascanu,
Advances in Neural Information Processing Systems, H. Larochelle, M. Ran- andDoinaPrecup(Eds.).PMLR,470–517. https://proceedings.mlr.press/v199/
zato,R.Hadsell,M.F.Balcan,andH.Lin(Eds.),Vol.33.CurranAssociates, schweighofer22a.html
Inc.,21810–21823. https://proceedings.neurips.cc/paper_files/paper/2020/file/ [35] SriramGanapathiSubramanian,MatthewE.Taylor,MarkCrowley,andPascal
f7efa4f864ae9b88d43527f4b14f750f-Paper.pdf Poupart.2022. DecentralizedMeanFieldGames. ProceedingsoftheAAAI
[20] IlyaKostrikov.2021.JAXRL:ImplementationsofReinforcementLearningalgo- ConferenceonArtificialIntelligence36,9(Jun.2022),9439–9447. https://doi.org/
rithmsinJAX. https://doi.org/10.5281/zenodo.5535154 10.1609/aaai.v36i9.21176
[21] Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. 2020. [36] JunfengWen,BoDai,LihongLi,andDaleSchuurmans.2020.Batchstationary
ConservativeQ-LearningforOfflineReinforcementLearning.InAdvances distributionestimation.InProceedingsofthe37thInternationalConferenceon
in Neural Information Processing Systems, H. Larochelle, M. Ranzato, MachineLearning(ICML’20).JMLR.org,Article945,11pages.
R. Hadsell, M.F. Balcan, and H. Lin (Eds.), Vol. 33. Curran Associates, [37] QiaominXie,ZhuoranYang,ZhaoranWang,andAndreeaMinca.2021.Learning
Inc.,1179–1191. https://proceedings.neurips.cc/paper_files/paper/2020/file/ WhilePlayinginMean-FieldGames:ConvergenceandOptimality.InProceedings
0d2b2061826a5df3221116a5085a6052-Paper.pdf ofthe38thInternationalConferenceonMachineLearning(ProceedingsofMachine
[22] Marc Lanctot, Edward Lockhart, Jean-Baptiste Lespiau, Vinicius Zambaldi, LearningResearch,Vol.139),MarinaMeilaandTongZhang(Eds.).PMLR,11436–
SatyakiUpadhyay,JulienPérolat,SriramSrinivasan,FinbarrTimbers,KarlTuyls, 11447. https://proceedings.mlr.press/v139/xie21g.html
ShayeganOmidshafiei,DanielHennes,DustinMorrill,PaulMuller,TimoEwalds, [38] Tengyang Xie, Yifei Ma, and Yu-Xiang Wang. 2019. Towards Opti-
RyanFaulkner,JánosKramár,BartDeVylder,BrennanSaeta,JamesBradbury, mal Off-Policy Evaluation for Reinforcement Learning with Marginalized
DavidDing,SebastianBorgeaud,MatthewLai,JulianSchrittwieser,ThomasAn- Importance Sampling. In Advances in Neural Information Processing Sys-
thony,EdwardHughes,IvoDanihelka,andJonahRyan-Davis.2019.OpenSpiel: tems, Vol. 32. https://proceedings.neurips.cc/paper_files/paper/2019/file/
AFrameworkforReinforcementLearninginGames.CoRRabs/1908.09453(2019). 4ffb0d2ba92f664c2281970110a2e071-Paper.pdf
arXiv:1908.09453[cs.LG] http://arxiv.org/abs/1908.09453 [39] JiachenYang,XiaojingYe,RakshitTrivedi,HuanXu,andHongyuanZha.
[23] J.M.LasryandPierre-LouisLions.2007.Meanfieldgames.JapaneseJournal 2018.DeepMeanFieldGamesforLearningOptimalBehaviorPolicyofLarge
ofMathematics2(2007),229–260. https://api.semanticscholar.org/CorpusID: Populations.InInternationalConferenceonLearningRepresentations. https:
1963678 //openreview.net/forum?id=HktK4BeCZ
[24] MathieuLaurière,SarahPerrin,SertanGirgin,PaulMuller,AyushJain,Theophile [40] YaodongYang,RuiLuo,MinneLi,MingZhou,WeinanZhang,andJunWang.
Cabannes,GeorgiosPiliouras,JulienPérolat,RomualdElie,OlivierPietquin,etal. 2018.MeanFieldMulti-AgentReinforcementLearning.InProceedingsofthe35th
2022.Scalabledeepreinforcementlearningalgorithmsformeanfieldgames.In InternationalConferenceonMachineLearning(ProceedingsofMachineLearning
InternationalConferenceonMachineLearning.PMLR,12078–12095. Research,Vol.80),JenniferDyandAndreasKrause(Eds.).PMLR,5571–5580.
[25] MathieuLaurière,SarahPerrin,JulienPérolat,SertanGirgin,PaulMuller,Ro- https://proceedings.mlr.press/v80/yang18d.html
mualdÉlie,MatthieuGeist,andOlivierPietquin.2024.LearninginMeanField [41] TianheYu,GarrettThomas,LantaoYu,StefanoErmon,JamesYZou,Sergey
Games:ASurvey. arXiv:2205.12944[cs.LG] https://arxiv.org/abs/2205.12944 Levine,ChelseaFinn,andTengyuMa.2020.MOPO:Model-basedOfflinePolicy
[26] JongminLee,WonseokJeon,ByungjunLee,JoellePineau,andKee-EungKim. Optimization.InAdvancesinNeuralInformationProcessingSystems,H.Larochelle,
2021.OptiDICE:OfflinePolicyOptimizationviaStationaryDistributionCorrec- M.Ranzato,R.Hadsell,M.F.Balcan,andH.Lin(Eds.),Vol.33.CurranAssociates,
tionEstimation.InProceedingsofthe38thInternationalConferenceonMachine Inc.,14129–14142. https://proceedings.neurips.cc/paper_files/paper/2020/file/
Learning(ProceedingsofMachineLearningResearch,Vol.139),MarinaMeilaand a322852ce0df73e204b7e67cbbef0d0a-Paper.pdf
TongZhang(Eds.).PMLR,6120–6130. https://proceedings.mlr.press/v139/lee21f. [42] RuiyiZhang*,BoDai*,LihongLi,andDaleSchuurmans.2020.GenDICE:Gen-
html eralizedOfflineEstimationofStationaryValues.InInternationalConferenceon
[27] VolodymyrMnih,KorayKavukcuoglu,DavidSilver,AndreiA.Rusu,JoelVeness, LearningRepresentations. https://openreview.net/forum?id=HkxlcnVFwB
MarcG.Bellemare,AlexGraves,MartinA.Riedmiller,AndreasKirkebyFidjeland,
GeorgOstrovski,StigPetersen,CharlieBeattie,AmirSadik,IoannisAntonoglou,
HelenKing,DharshanKumaran,DaanWierstra,ShaneLegg,andDemisHassabis.
2015.Human-levelcontrolthroughdeepreinforcementlearning.Nature518
(2015),529–533. https://api.semanticscholar.org/CorpusID:205242740
[28] OfirNachum,YinlamChow,BoDai,andLihongLi.2019.DualDICE:Behavior-
AgnosticEstimationofDiscountedStationaryDistributionCorrections.InAd-
vancesinNeuralInformationProcessingSystems,H.Wallach,H.Larochelle,
A.Beygelzimer,F.d'Alché-Buc,E.Fox,andR.Garnett(Eds.),Vol.32.Cur-
ranAssociates,Inc. https://proceedings.neurips.cc/paper_files/paper/2019/file/
cf9a242b70f45317ffd281241fa66502-Paper.pdf
[29] BarnaPásztor,AndreasKrause,andIlijaBogunovic.2023.EfficientModel-Based
Multi-AgentMean-FieldReinforcementLearning. TransactionsonMachine
LearningResearch(2023). https://openreview.net/forum?id=gvcDSDYUZx
[30] JulienPérolat,SarahPerrin,RomualdElie,MathieuLaurière,GeorgiosPiliouras,
MatthieuGeist,KarlTuyls,andOlivierPietquin.2022.ScalingMeanFieldGames
byOnlineMirrorDescent.InProceedingsofthe21stInternationalConference
onAutonomousAgentsandMultiagentSystems(VirtualEvent,NewZealand)
(AAMAS’22).InternationalFoundationforAutonomousAgentsandMultiagent
Systems,Richland,SC,1028–1037.
[31] SarahPerrin.2022.ScalingupMulti-agentReinforcementLearningwithMeanField
GamesandVice-versa.Theses.UniversitédeLille. https://theses.hal.science/tel-
04284876
[32] SarahPerrin,MathieuLaurière,JulienPérolat,MatthieuGeist,RomualdÉlie,and
OlivierPietquin.2021.MeanFieldGamesFlock!TheReinforcementLearning
Way.InProceedingsoftheThirtiethInternationalJointConferenceonArtificialA HYPERPARAMETERS
ThehyperparametersfortheexperimentsontheperformanceevaluationareshowninTable1.ForD-MOMD,weprovidehyperparameters
inTable2.
Hyperparameter Value Description
LearningRate 0.001 Stepsizeforgradientupdates
𝜏 20.0 Temperature
𝛼 0.99 Weightformomentuminupdates
𝛾 0.99 Discountfactor
𝜂 3.0 RegularizationtermforCQL
B 2000 Numberofbatches
N 512 Batchsize
Table1:HyperparametersforOff-MMD.
Hyperparameter Value Description
BufferSize 100,000 Sizeofthereplaybuffer
N 256 Numberofsamplesdrawnfromthebufferperupdate
Updatesteps 4000 Gradientupdatesperiteration
LearningRate 0.001 Stepsizeforgradientupdates
𝜖(start) 1.0 Initialvalueof𝜖-greedyexploration
𝜖(finish) 0.1 Finalvalueof𝜖
𝜖annealtime 1,000,000 Numberofstepstoannealepsilonfromstarttofinish
𝜏 20.0 Temperature
𝛼 0.99 Weightformomentuminupdates
𝛾 0.99 DiscountFactor
TargetUpdateInterval 200 Stepsbetweentargetnetworkupdates
TrainingInterval 10 Numberofenvironmentstepsperupdate
Table2:HyperparametersforD-MOMD