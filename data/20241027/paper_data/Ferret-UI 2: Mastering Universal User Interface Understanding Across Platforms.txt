Preprint
FERRET-UI 2: MASTERING UNIVERSAL USER IN-
TERFACE UNDERSTANDING ACROSS PLATFORMS
ZhanghengLi†,KeenYou,HaotianZhang,DiFeng,HarshAgrawal,XiujunLi,
MohanaPrasadSathyaMoorthy,JeffNichols,YinfeiYang,ZheGan
Apple
ABSTRACT
Building a generalist model for user interface (UI) understanding is challenging
duetovariousfoundationalissues,suchasplatformdiversity,resolutionvariation,
anddatalimitation. Inthispaper, weintroduceFerret-UI2, amultimodallarge
languagemodel(MLLM)designedforuniversalUIunderstandingacrossawide
range of platforms, including iPhone, Android, iPad, Webpage, and AppleTV.
Building on the foundation of Ferret-UI, Ferret-UI 2 introduces three key inno-
vations: support for multiple platform types, high-resolution perception through
adaptive scaling, and advanced task training data generation powered by GPT-
4o with set-of-mark visual prompting. These advancements enable Ferret-UI 2
to perform complex, user-centered interactions, making it highly versatile and
adaptablefortheexpandingdiversityofplatformecosystems. Extensiveempiri-
calexperimentsonreferring,grounding,user-centricadvancedtasks(comprising
9subtasks×5platforms),GUIDEnext-actionpredictiondataset,andGUI-World
multi-platformbenchmarkdemonstratethatFerret-UI2significantlyoutperforms
Ferret-UI,andalsoshowsstrongcross-platformtransfercapabilities.
1 INTRODUCTION
User interfaces (UIs) are central to human-computer interaction, shaping how users interact with
digital systems. The complexity of UIs has evolved with the proliferation of platforms such as
smartphones,tablets,webplatforms,andsmartTVs. Despitethisincreasingdiversity,manycurrent
approachestoUIunderstandingandinteraction(Hongetal.,2023;Wangetal.,2024b;Kapooretal.,
2024),particularlythoseinmulti-platformecosystems,facelimitations.
One prominent effort in this space is Ferret-UI (You et al., 2024), which has advanced the field
of referring and grounding UIs. However, though taking an any-resolution approach (Liu et al.,
2024a),Ferret-UIisconstrainedbyafixedgroundingresolution(i.e.,336×672and672×336),and
focuses on single-type platforms (i.e., mobile devices including iPhone and Android), limiting its
applicability in the context of today’s highly diverse platform landscape. For example, as illus-
tratedinFigure1,onenotabledifferenceamongthesefourexemplifiedplatformsisresolution,the
nativeresolutionofiPhonediffersfromthatofiPad, WebUIandalsoAppleTV,directlyapplying
Ferret-UIacrosstheseplatformspresentssignificantchallenges.Anothermajorchallengeisthelack
of platform-specific, high-quality data, given different platforms. Though Ferret-UI’s approach to
trainingdatagenerationcanbeextendedtotheseplatforms,itprimarilyreliesontext-basedGPT-4
prompting,whereboundingboxesarerepresentedinapurelytextualformat. Thisabsenceofvisual
input and spatial relationships between UI elements diminishes the quality of training data, which
inturnlimitstheperformanceandeffectivenessoftheresultingmodel.
Toaddresstheselimitations,weintroduceFerret-UI2,amultimodallargelanguagemodel(MLLM)
designedtounderstanddiverseUIscreensandrespondtouserintentthroughsingle-stepinteractions
across multiple platforms. Building on the foundation of Ferret-UI (You et al., 2024), Ferret-UI 2
significantly enhances UI perception and user interaction capabilities via three key advancements:
(i)multi-platformsupport,(ii)dynamichigh-resolutionimageencoding,and(iii)high-qualitymul-
timodaltrainingdatageneration.
†UniversityofTexasatAustin.WorkdoneduringaninternshipatApple.
1
4202
tcO
42
]VC.sc[
1v76981.0142:viXraPreprint
Figure 1: Real examples of a single Ferret-UI 2 model interacting with four different platforms
(iPhone,iPad,Webpage,andAppleTV)forUIunderstanding. RefertoAppendixDformoreexam-
plesincludingmulti-stepinteractions.
2Preprint
First, Ferret-UI 2 extends its compatibility beyond mobile platforms (iPhone and Android), incor-
porating additional platforms like tablets, webpages, and smart TVs. Figure 1 illustrates visual
examplesofFerret-UI2interactingwithusersacrossfourtypicalscreentypes. Thismulti-platform
supportenablesbroaderapplicabilityandallowsthesystemtoseamlesslyscaleacrossavarietyof
userenvironments.
Second, Ferret-UI 2 supports high-resolution image encoding via the any-resolution method (Liu
etal.,2024a;Zhangetal.,2024c). However,goingbeyondthat,weintroduceanenhancedadaptive
griddingapproachtomaintainperceptioncapabilitiesattheoriginalresolutionoftheUIscreenshot,
ensuring more accurate recognition of visual elements. By leveraging human-collected bounding
box annotations, we enhance referring and grounding precision, which allows for more detailed
understandingofUIcomponentsandtheirrelationships.
Third,Ferret-UI2leverageshigh-qualitytrainingdataforbothelementaryandadvancedtasks. For
elementarytasks, weconvertsimplereferringandgroundingdataintoconversations, allowingthe
model to develop a fundamental understanding of diverse UI screens. For advanced tasks, which
focusonuser-centered,free-formconversations,wereplacethetext-basedGPT-4prompting(where
boundingboxesaredescribedonlyintext)withGPT-4ousingset-of-markvisualprompting(Yang
et al., 2023) for training data generation. This approach enhances spatial understanding of UI el-
ements, resulting in higher-quality training data. Additionally, unlike previous methods that use
straightforward instructions such as “click on [bbox location]”, Ferret-UI 2 performs single-step
user-centeredinteractions. Forexample,whengivenacommandlike“pleaseconfirmsubmission”,
thesystemunderstandsandexecutestheintendedaction, ratherthansimplyfollowingmechanical
clickinstructions. Overall,ourcontributionsaresummarizedasfollows.
• WepresentFerret-UI2,amultimodalLLMthatsetsitselfapartfrompreviouseffortsbysupport-
ing a broaderrange of platforms, includingiPhone, Android, iPad, Webpage, and AppleTV. We
upgradeFerret-UIacrossmultiplefronts,includingbetterinstruction-tuningdataformodeltrain-
ing,high-resolutionimageencodingforenhancedperformance,andnewreferringandgrounding
benchmarkstailoredfordifferentplatforms.
• We demonstrate that Ferret-UI 2 advances the UI referring and grounding performance on dif-
ferent platforms. On three categories of tasks (referring, grounding, and user-centric advanced
tasks, comprising9subtasks×5platforms), Ferret-UI2outperformsFerret-UI,andalsoshows
competitiveperformancecomparedtoGPT-4o. Besides,Ferret-UI2alsoexhibitsstrongtransfer
capabilitiesacrossplatforms. Finally,Ferret-UI2achievedstrongperformanceonrecentbench-
markslikeGUIDE(Chawlaetal.,2024)andGUI-World(Chenetal.,2024a).
2 RELATED WORK
UIagentshavegarneredsignificantattentioninrecentresearch,particularlyinmultimodalmodels
that seek to automate complex UI tasks across diverse platforms. Many works have advanced the
fieldbytacklingspecificchallengesrelatedtosingle-platformandmulti-platformUIunderstanding,
interaction,andautomation.
Single-Platform UI Agents. Single-platform UI agents focus on automating tasks on a specific
deviceecosystem,suchasAndroid,iOS,desktopenvironments,orwebpages. Onthemobileside,
DigiRL (Bai et al., 2024), AppAgent V2 (Li et al., 2024c), AutoDroid (Wen et al., 2024), and
MobileFlow(Nongetal.,2024)proposedAndroidagentstargetingonhuman-likeinteractions. For
web-basedagents,systemslikeWebShop(Yaoetal.,2022),WebArena(Zhouetal.,2023),LASER
(Maetal.,2023),WebAgent(Guretal.,2023),AutoWebGLM(Laietal.,2024),WebVoyager(He
etal.,2024)andAgent-E(Abuelsaadetal.,2024)exploredagentsthatnavigateandperformtasks
within web environments, while MindSearch agent Chen et al. (2024b) focused on AI engine for
websearch. AssistGUI(Gaoetal.,2023),OS-Copilot(Wuetal.,2024),SYNAPSE(Zhengetal.,
2023)andUFO(Zhangetal.,2024a)exploredthemorecomplexcomputerOSinteraction. These
efforts have significantly improved task-specific automation, although their single-platform nature
limitscross-platformflexibility.
Multi-PlatformUIAgents. Multi-platformUIagentshaveemergedtoaddressthegrowingcom-
plexityofdeviceecosystems,supportingavarietyofdevicesandplatforms,includingmobile,web,
and desktop environments. Zheng et al. (2024a); Cheng et al. (2024) demonstrated GPT-4V as a
3Preprint
Widget Invalid
Screen-
Label Data
wide OCR
Mapping Filtering
Picture-
only OCR
Query with task
generation prompts
GPT-4o
Raw data collection:
screenshots, widget
bounding boxes and labels
Figure2: IllustrationoftheCore-setdatagenerationpipeline.
generalist agent when grounded. Recent works like OmniACT (Kapoor et al., 2024) support both
desktop and web interfaces, while CogAgent (Hong et al., 2023) supports UI navigation on both
PCwebpagesandAndroiddevices. Furthermore,Mind2Web(Dengetal.,2024)andMobile-Agent
(Wangetal.,2024b)arealsonotableresearchworksthatenableagentstooperateseamlesslyacross
differentplatforms. Mobile-AgentV2(Wangetal.,2024a)featuresHarmonyOSandAndroidOS
for non-English and English scenarios. Ferret-UI (You et al., 2024) focuses on mobile UI under-
standingforbothAndroidandiPhonescreenshotsusingmultimodalLLMs(McKinzieetal.,2024;
Zhang et al., 2024b), with a focus on referring and grounding capabilities. These agents aim to
performmorecomplex,user-intent-basedinteractionsacrossmultipledevicetypes,pavingtheway
fortrulygeneralistmultimodalagents.
UI-AgentBenchmarks. TheevaluationofUIagentsrequiresspecializedbenchmarkstotestvar-
iousaspectsofUIinteraction,includingtaskexecution,navigation,andinteractionunderstanding.
Rico(Dekaetal.,2017)remainsafoundationaldatasetformobileappinteraction,whileMobile-Env
(Zhangetal.,2023),AndroidEnv(Toyamaetal.,2021),AndroidWorld(Rawlesetal.,2024a),An-
droidin-the-Wild(Rawlesetal.,2024b),AndroidControl(Lietal.,2024b),andAMEX(Chaietal.,
2024)providebenchmarksformobiledevicecontrol. WindowsAgentArena(Bonattietal.,2024)
introduces benchmark focusing on PC windows environment. OSWorld (Xie et al., 2024) takes
a broader approach by providing benchmarks for agents in real computer environments, including
Ubuntu,MacOS,andWindows. Web-basedinteractionbenchmarksincludeWebSRC(Chenetal.,
2021), Mind2Web (Deng et al., 2024) and WebCanvas (Pan et al., 2024) focusing on structural
reading comprehension and task execution in web environments. More recently, MobileAgent-
Bench (Wang et al., 2024c) and VisualWebBench (Liu et al., 2024b) have introduced taxonomies
designedtoevaluatetheperformanceofmultimodalagentsacrossbothmobileandwebinterfaces.
VisualAgentBench (Liu et al., 2024c) expands this with a focus on multimodal LLMs as visual
foundationagents. GUIOdyssey(Luetal.,2024)providesbenchmarksforcross-Appnavigation.
GUI-World(Chenetal.,2024a)pioneersincoveringmulti-platformbenchmarkingforGUIagents,
whileCRAB(Xuetal.,2024)furthertestscross-environmenttasksforGUIagents. Thesebench-
markscontributetoagrowingneedforunified,multi-platformevaluationsthatcanassessUIagents’
adaptability,precision,andefficiency.
Comparedtotheaforementionedworks,Ferret-UI2isthefirsttotargetuniversalUIunderstanding
acrossdiverseplatforms,includingsmartphones,tablets,webinterfaces,andsmartTVs. Itfocuses
onfoundationalcapabilitieslikefine-grainedreferring,grounding,andreasoning,aimingtocreate
ageneralistagentforversatileUInavigation.
3 FERRET-UI 2
Inthissection,wefirstdescribehowwecurateourtrainingdatasetsfromtherawdataannotations
(Section3.1)andthendescribethemodelarchitecture(Section3.2).
4Preprint
Table1: AsummaryofdatasetsacrossvariousplatformsusedtotrainFerret-UI2. Thescreenshot
resolutionstatisticsareshowninAppendixE.
TrainingData Platform #Images(k) TaskTypes
iPhone 112
ElementaryTasks
Android 61
(Referring,Grounding),
Core-set iPad 19
AdvancedTasks(ComprehensiveDescription,
Web 321
Multi-RoundPerceptionQAandInteractionQA)
AppleTV 16
GUIDE Web 51 NextActionPrediction
GroundUI-18k Web 18 SimpleInteraction
Spotlight Android 66 Screen2Word,WidgetCaption,Taperception
3.1 DATASETCONSTRUCTION
Weconstructourowndatasetinordertotrainastrongmulti-platformUIunderstandingmodel. A
flowdiagramofourcompletedatasetgenerationpipelineisshowninFigure2.
Raw Annotation Collection. The primary dataset used for training Ferret-UI 2 is a combination
ofdatafromvariousplatformtypes,includingiPhone,Android,iPad,Webpage,andAppleTV.The
datacollectionprocessvariesdependingontheplatformtype:
• iPhone, iPad, and AppleTV: We use human-collected iPhone, iPad and AppleTV data under
diverse usage scenarios and human-annotated widget bounding box coordinates and labels. To
saveannotationcosts,wedonotcollecttextannotations;instead,textboundingboxesarereplaced
byscreen-wideOCR-detectedtextandboundingboxesusinganOCRconfidencethresholdof0.5.
• Webpage: ThewebdataisderivedfromtheWebUIdataset(Wuetal.,2023). Boundingboxesof
alltypesofUIwidgetsandtextannotationsfornon-picturewidgetsaredirectlyparsedfromthe
source HTML view hierarchy tree, providing high-quality annotations. For picture widgets, we
furtheruseOCRtodetecttextscontainedinthepictures.
• Android: TheAndroiddataforscreenshots,boundingboxesandtextannotationsistransformed
fromtheRICOdataset(Dekaetal.,2017).SimilartotheWebUIdataset,wealsoperformpicture-
onlyOCRtocompletethemissingtextannotationsinpicturewidgets.
For all the collected data, we perform data filtering including: (i) filter out or narrow down out-
of-bound bounding boxes and remove empty screenshots with no remaining bounding boxes after
box filtering; (ii) since we do not intend to add multilingual support for the Ferret-UI 2 model,
screenshotswithmorethan5%non-ASCIIcharactersinthetextannotationsarealsoremoved.
Despitethedifferenttypesoflabelspacesfromvarioussources,wefilteroutboundingboxesasso-
ciatedwithlessrelevantlabels(e.g.,UItypes)anduniformlymaptheremaininglabelstoacommon
label space containing 13 classes: ‘Checkbox’, ‘Button’, ‘Container’, ‘Dialog’, ‘Icon’, ‘PageCon-
trol’,‘Picture’,‘SegmentedControl’,‘Slider’,‘TabBar’,‘Text’,‘TextField’,and‘Toggle’,andobtain
amulti-platformuniformeddatasetwithrawUIwidgetannotations. Weprovidetheoriginallabel
statisticsandtheirconvertedlabelsinAppendixF.
WenametheabovecollectedscreenshotdatasettheCore-set,whichwewillusetoconstructtheele-
mentaryandadvancedtaskdata. Besides,wealsoemploythird-partytrainingdatasetstoenrichour
datasourceandavoidoverfittingourpredefinedtasks. Acompletestatisticsofthetrainingdataset
ofFerret-UI2issummarizedinTable1,indicatingthatthedatasetdistributionisveryunbalanced
across different platforms. In particular, the number of iPad and AppleTV screenshots is signifi-
cantlysmallerthanthatofotherplatforms. Toaddressthis, we(i)assigndifferentlossweightsto
differentplatformsduringtraining, and(ii)generateallthreetypesofadvancedtasksforeachex-
ampleofiPadandAppleTVplatformsandgenerateonly1typeofadvancedtaskforeachexample
ofotherplatforms.
Compared to the training dataset of Ferret-UI, which relies on model-detected bounding boxes,
Ferret-UI2’strainingdatasetpredominantlyutilizeseitherhuman-collectedannotationsorbounding
5Preprint
Figure3:Exampleofset-of-markvisualprompting(Yangetal.,2023)(left)andoneofitsgenerated
advancedtasktrainingexample(right).
boxesdirectlyparsedfromthesourceHTML,resultinginasignificantimprovementinannotation
quality,evidencedlaterinourquantitativeevaluationinSection4.2.
Task Data Generation. For task data generation, we follow the paradigm of Ferret-UI data con-
struction,whichincludesbothelementarytasksandadvancedtasks.
Elementarytasks(Figure2)consistof3referringtasksand3groundingtasks.Specifically,referring
tasks include (i) OCR: recognizing the text given a text bounding box, (ii) widget classification:
predict the UI type of the elements, and (iii) tapperbility: predict whether the selected widget is
tappableforinteraction; meanwhile, groundingtasksinclude(i)widgetlisting: listallthewidgets
in the screen, (ii) find text: find the location of a given text, and (iii) find widget: find the widget
giventhewidgetdescription.
For advanced tasks, we prompt GPT-4o with bounding box annotations of a given screenshot and
require GPT-4o to generate QA tasks related to the UI widgets in the screenshot. Unlike Ferret-
UI, which mainly focuses on spatial descriptions due to the limitation of using textual prompts
without image information (i.e., screenshots) for bounding box annotations, Ferret-UI 2 leverages
GPT-4otogenerateadvancedtaskdatathatcoversavarietyofaspectsofUIunderstanding. Thisis
possiblebecauseGPT-4odemonstratesanimprovedabilitytocomprehendthespatialrelationships
betweenUI widgetswhenprovided withthescreenshot asinput. Specifically, weprompt GPT-4o
togenerate3typesofadvancedtasks(showninFigure2)including: (i)comprehensivedescription:
describeglobalandlocalfunctionalitiesofthescreen,(ii)multi-roundperceptionQA:multi-round
questionansweringregardingtheUIperceptioncapabilities, and(iii)multi-roundinteractionQA:
multi-roundquestionansweringregardingthesingle-stepanduser-centricUIinteractionsbasedon
the current screen status. More detailed requirements and prompts for GPT-4o when generating
advancedtasksareprovidedinAppendixB.
WeempiricallyfindithardforGPT-4otofindthelocationofreferredUIwidgetswiththeoriginal
screenshot as input (i.e., bad grounding capability). To address this, we use Set-of-Mark (SoM)
visual prompting (Yang et al., 2023) when generating multi-round perception and interaction QA
training samples. One example of SoM prompting and its generated data sample is shown in Fig-
ure3,whereeachUIwidgetismarkedwithacorner-styleboundingboxandauniquenumbertag
for easy identification. Furthermore, the same class of UI widgets have the same color for visual
6Preprint
Figure4:OverviewoftheFerret-UI2modelarchitecture,whichallowsforseamlessUIunderstand-
inganduser-centeredsingle-stepinteractionswithhigh-resolutionsupport.
Algorithm1: AdaptiveN-gridding
Require: Originalresolution: w×h,gridsize: 336×336,sizelimitN
Ensure: OptimalgriddingsizeN andN (N ,N ∈N+)
w h w h
1: N wbest,N hbest ←0,∆ best ←∞,N w0 ← 3w 36,N h0 ← 33h 6
2: forN w =1toN do ▷Traverseallgridconfiguarations
3: forN h =1toN −N w do
(cid:114)
(cid:12) (cid:12)(cid:12) (cid:12)
4: ∆ ← (cid:12)Nw − Nw0(cid:12)(cid:12)Nh − Nh0(cid:12) ▷Getaspectratiochange
aspect (cid:12)Nh Nh0(cid:12)(cid:12)Nw Nw0(cid:12)
5: ∆ ←
|Nw×Nh−Nw0×Nh0|
▷Getrelativepixelchangeforresizing
pixel Nw0×Nh0
6: if∆ >∆ ×∆ then
best aspect pixel
7: (N wbest,N hbest)←(N w,N h)
8: ∆ ←∆ ×∆
best aspect pixel
9: endif
10: endfor
11: endfor
12: return(N wbest,N hbest)
promptingtohelpGPT-4obetterdifferentiatetheboundingboxesofspatiallycloseornestedwid-
gets. PleaserefertoFigure5forvisualpromptsonotherplatforms,andAppendixCforadditional
examplesofgenerateddataforadvancedtasks.
AsaforementionedinTable1,inadditiontothetasksgeneratedontheCore-set,weaugmenttraining
data with additional third-party datasets, including GroundUI-18k (Zheng et al., 2024b), GUIDE
(Chawlaetal.,2024)andSpotlight(Li&Li,2023).
3.2 MODELARCHITECTURE
As shown in Figure 4, the model architecture of Ferret-UI 2 directly builds upon Ferret-UI (You
etal.,2024),whichusestheAny-Resolution(AnyRes)method(Liuetal.,2024a)toenhancerefer-
ringandgrounding,enablingtheencodertocapturediverseimageresolutions.
7Preprint
Table2: Resultsonourconstructedbenchmarksforelementaryandadvancedtasks,aswellasthe
GUIDEbenchmark(Chawlaetal.,2024). Resultsonelementaryandadvancedtasksareaveraged
overallplatforms,includingiPhone,Android,iPad,Webpage,andAppleTV.Eachplatformincludes
6elementarytasksand3advancedtasks.(†)Intasksthatrequirereferring,GPT-4oisequippedwith
set-of-mark(SoM)promptingbyaddingaredrectangularboxtoscreenshotsforthereferredwidget.
NotethatSoMvisualpromptingisnotusedforFerret-UIandFerret-UI2.
Elementary Advanced GUIDEBench
Model Backbone
Refer Ground GPT-4oScore Multi-IoU BertScore IoU
Ferret-UI Vicuna-13B 64.15 57.22 45.81 18.75 41.15 26.91
Gemma-2B 75.20 78.13 80.25 40.51 83.71 51.13
Ferret-UI2 Llama3-8B 80.28 82.79 89.73 41.15 91.37 55.78
Vicuna-13B 81.34 81.31 86.25 41.71 88.81 54.71
GPT-4o - 56.47 12.14 77.73 7.06 75.31 9.64
GPT-4o+SoM-Prompt† - 87.91 - 84.33 7.36 - -
Specifically, the CLIP image encoder first extracts both global (derived from the low-resolution
overview image) and local features (corresponding to high-resolution sub-images) from the UI
screenshot. Then, these image features are flattened and sent into the LLM. The Visual Sampler
identifies and selects the relevant UI regions based on user instructions. The model then outputs
groundeddescriptionsforperceptionorinteractionwiththeUIelements.
AdaptiveGridding. Localimagefeaturesareextractedbycalculatingtheoptimalgridsizeusing
our proposed adaptive N-gridding mechanism, then resizing and encoding the visual features of
each grid. This is a key model innovation compared to Ferret-UI. As shown in Algorithm 1, the
optimalgriddingsizeN andN isdeterminedwhenthegriddingandresizingbasedon(N ,N )
w h w h
lead to minimal aspect ratio change times the relative pixel number change, under the constraint
N +N ≤N,whereN isthesizelimit. WiththesizelimitN,thetotalnumberofgridsisupper
w h
boundedby⌊N2⌋.ComparedtotheAnyResmodulethathasunboundedcost,thekeydifferencesof
4
adaptiveN-griddingisitautomaticallyfindstheoptimalgriddingconfiguration,i.e.,leastresolution
distortion (aspect ratio change and pixel number change) within a predefined inference cost limit
thatN ×N ≤⌊N2⌋,whichisbothinformation-preservingandefficientforlocalencoding. With
w h 4
adaptivegridding,Ferret-UI2understandsUIscreensandprovidesuser-centeredinteractionswith
anoptimalconfigurationatanyresolutiongiventheinferencecostlimitspecifiedassizelimitN.
4 EXPERIMENTS
4.1 EXPERIMENTSETUP
Training Data. The training datasets are summarized in Table 1, which can be divided into
two categories: (i) datasets constructed by our own, which include elementary task data and ad-
vancedtaskdataacrossallplatformsasintroducedinSection3.1,and(ii)publicdatasetsincluding
GroundUI-18k(Zhengetal.,2024b),asimpleuser-centeredinteractiondatasetonwebpagescreen-
shots,GUIDE(Chawlaetal.,2024),anext-actionpredictiondatasetonwebpagescreenshots, and
Spotlight(Li&Li,2023),anAndroidUIunderstandingandinteractiondataset.
Model. Following Ferret-UI (You et al., 2024), Ferret-UI 2 uses a CLIP ViT-L/14 model as the
image encoder; for the LLM backone, besides Vicuna-13B (Chiang et al., 2023) as used in the
originalFerret-UI,wealsotried2additionalLLMsatmobilescales, includingGemma-2B(Team
etal.,2024)andLlama3-8B(Dubeyetal.,2024). Astodynamichigh-resolutionimageencoding,
wesetthesizelimitN to8,sothatthemaximalgridnumberis16foradaptivegridding.
Evaluation. Atahighlevel, modelevaluationfallsintotwobroadcategories: (i)benchmarkswe
constructed, and (ii) public benchmarks. For our benchmarks, we created a total of 45, including
6 elementary tasks and 3 advanced tasks per platform type, across 5 platforms. For elementary
tasks, we follow the evaluation metrics outlined by You et al. (2023). For advanced tasks, we use
GPT-4otoscoregeneratedanswersforagivenscreenshotanduserquery,visuallypromptingGPT-
8Preprint
Table3: Zero-shotperformanceofFerret-UI2ontheGUI-Worldbenchmark(Chenetal.,2024a).
GPT-4Score
Model
iOS Android Webpage Average
MiniGPT4Video(Ataallahetal.,2024) 1.501 1.342 1.521 1.455
VideoChat2(Lietal.,2024a) 2.169 2.119 2.221 2.170
Chat-Univi(Jinetal.,2024) 2.337 2.390 2.349 2.359
GUI-Vid(Chenetal.,2024a) 2.773 2.572 2.957 2.767
QWen-VL-MAX(Baietal.,2023) 2.779 2.309 2.656 2.580
Ferret-UI(Youetal.,2024) 2.713 2.791 2.411 2.638
Ferret-UI2 2.881 2.954 3.013 2.948
Gemini-Pro1.5(Reidetal.,2024) 3.213 3.220 3.452 3.295
GPT-4o 3.558 3.561 3.740 3.619
4owitharedrectangularboundingbox. AdvancedtasksaretestedusingGPT-4oevaluationscore
andmulti-IoU.Themulti-IoUiscalculatedbyfirstmatchingpredictedboundingboxeswithground
truthboundingboxesandthencalculatetheaverageIoUofeachpairofboundingboxes(IoU=0if
nomatch). Furthermore,weconductnext-actionpredictiontestgivenpreviousactionhistoryonthe
GUIDEbenchmark(Chawlaetal.,2024), andevaluatethesemanticsimilarityw.r.t. thereference
answer and grounding Intersection-over-Union (IoU). Additionally, we evaluate our model on the
recentlyreleasedGUI-Worldbenchmark(Chenetal.,2024a)onthesupportedplatformsfollowing
theoriginalGPT-4evaluationprotocolasinChenetal.(2024a),whichdoesnotincludeevaluating
thegroundingcapabilityforinteraction-relatedUItasks.
4.2 EXPERIMENTRESULTS
Main results. Our main results are summarized in Table 2, which shows the comparative perfor-
manceofdifferentmodelsonourconstructedelementaryandadvancedtasks,aswellastheGUIDE
benchmark (Chawla et al., 2024). Note, that for each data entry corresponding to the elementary
andadvancedtasks,itisanaverageacrossallplatforms. Thedetailedresultsoneachplatformare
providedinTable6ofAppendixA.Below,wehighlightafewobservations.
• Ferret-UI 2, powered by Llama-3-8B, delivers the best results across most metrics. It achieves
thehighestGPT-4oscoreonadvancedtasks,withanotable89.73,surpassingFerret-UIby43.92
pointsandGPT-4oby12.0points.Notably,Ferret-UI2withLlama-3-8Balsoachievesthehighest
IoUscoreontheGUIDEbenchmarkwith55.78,indicatingsuperiorgroundingcapabilityoverthe
othermodels.
• Ferret-UI 2, equipped with Vicuna-13B, also performs well, e.g., achieving a strong Multi-IoU
scoreof41.71onadvancedtasks. Despitebeingsixtimessmaller,Ferret-UI2withGemma-2B
deliverscompetitiveperformanceacrosstheboard.
• In contrast, GPT-4o struggles with fine-grained UI understanding, as shown by its low referring
(56.47) and grounding (12.14) scores in the elementary tasks. Its Multi-IoU and IoU scores in
advancedtasksandtheGUIDEbenchmarkarealsolow.
Overall, the results demonstrate the versatility of Ferret-UI 2 in handling UI understanding tasks
acrossdifferentplatforms.
ResultsonGUI-World. Tofurtherdemonstratethezero-shotperformanceofusingFerret-UI2out
ofthebox, wefurthertestourmodelontherecentlyreleasedGUI-Worldbenchmark(Chenetal.,
2024a). ResultsaresummarizedinTable3. Clearly,Ferret-UI2doesnotoverfittothetrainingdata,
andcangeneralizewelltothetestdatainthewild.Notably,Ferret-UI2outperformsGUI-Vid(Chen
et al., 2024a), a model developed in the GUI-World paper, on supported platforms including iOS,
AndroidandWebpages.
4.3 ABLATIONSTUDY
Cross-PlatformTransferability. InTable4,weevaluatethezero-shotplatform(domain)transfer
capabilityoftheFerret-UI2modelbytrainingitonelementarytasksfromoneplatformandtesting
9Preprint
Table 4: Zero-shot cross-platform transfer results of Ferret-UI 2. For simplicity, we train and test
themodelonlyusingdatacorrespondingtotheelementarytasks.
Test-Referring Test-Grounding
Training
iPhone iPad AppleTV Web Android iPhone iPad AppleTV Web Android
iPhone 86.3 68.1 31.2 45.3 71.2 84.1 65.2 43.1 51.7 63.1
iPad 67.5 80.2 40.7 51.5 63.3 64.5 82.1 32.1 38.5 53.8
AppleTV 29.1 45.1 79.3 54.2 36.4 33.7 41.2 81.6 52.1 29.7
Web 59.2 57.4 41.2 85.5 41.7 54.0 51.2 46.5 87.5 45.9
Android 72.5 60.7 35.7 51.2 86.2 66.7 48.9 29.7 44.1 83.9
Table5: AblationresultsofthearchitectureanddatasetimprovementsofFerret-UI2w.r.t. Ferret-
UI-anyRes(Youetal.,2024), i.e., thehigh-resolutionversionofFerret-UIequippedwiththeany-
resolutionmodule. iPhonev1referstothedatasetontheiPhoneplatformoriginallyusedbyFerret-
UI,whileiPhonev2isthedatausedbyFerret-UI2. Modelsareevaluatedontheadvancedtasks.
TestData
TrainingData Model iPhonev1 iPhonev2
GPT4oScore Multi-IoU GPT4oScore Multi-IoU
Ferret-UI-anyRes 91.3 36.89 68.3 27.13
iPhonev1
Ferret-UI2 93.7(+2.4) 37.12(+0.23) 70.2(+1.9) 28.21(+1.08)
Ferret-UI-anyRes 86.2 35.89 85.97 39.81
iPhonev2
Ferret-UI2 88.1(+1.9) 36.43(+0.54) 89.7(+3.73) 41.73(+1.92)
it on other platforms. These results provide insights into how well the model generalizes across
different platforms. We observe similar performance patterns across both referring and grounding
tasks. Specifically,
• iPhonetransferswelltoiPadandAndroidplatformsonbothtasks,achievingatleast68.1and71.2
scoresonreferringtasksand65.2and63.1scoresongroundingtasksduetoitsdiversescreenshot
contents(becauseofthelargescreenshotnumber)andsimilarresolutionsandaspectratioswith
other two platforms. iPad and Android can also transfer fairly well to the iPhone domain, all
achievingaround65scores.
• AppleTV andWeb donot transfervery well to the mobiledomains, includingiPhone, Android,
andiPad,achievingthehighestreferringscoreof59.2andgroundingscoreof54.0,possiblybe-
causetheyaremostlylandscapescreenshots,whichisincontrasttothemostlyportraitscreenshots
inmobileplatforms. ModelstrainedonotherdomainsallachievepoorperformanceontheAp-
pleTVtestdomain,withthehighestscoreofaround40onbothkindsoftasks,whichisreasonable
since there is a large domain gap in terms of AppleTV’s content distribution compared to other
domains.
Theresultssuggestthat(i)iPhone,iPadandAndroidhavesimilarcontentdistribution,whichhelp
them generalize to each other; (ii) models trained on more diverse contents (e.g., around 100k
iPhone data) can generalize better to other platforms; (iii) platforms with similar resolution and
aspect ratios may transfer better to each other; and (iv) good transferability among some of these
platformscontributestothecross-platformperformanceofFerret-UI2.
Ablation on Architecture and Dataset Improvements. Table 5 presents a comparison between
the Ferret-UI and Ferret-UI 2 model trained on different versions of the iPhone dataset. Models
are evaluated on the test set corresponding to advanced tasks. The results indicate that both the
architecturalenhancements,particularlytheadaptiveN-gridding,andtheimproveddataset(iPhone
v2)contributetoperformancegains.
Specifically,whenevaluatedontheiPhonev1testset,Ferret-UI2showsaslightimprovementover
Ferret-UI, with the GPT4o score increased from 91.3 to 93.7 and the Multi-IoU score increased
from 36.89 to 37.12. However, the improvements are more pronounced on the iPhone v2 dataset.
Here, Ferret-UI 2 achieves a GPT4o score of 89.7, outperforming Ferret-UI’s 85.97, along with
10Preprint
a substantial Multi-IoU score boost from 39.81 to 41.73. These results suggest that while both
architecture and dataset enhancements contribute to overall performance, the new dataset plays a
moresignificantroleindrivingimprovements,particularlyonmorechallengingtasks.
5 CONCLUSIONS
In this paper, we presented Ferret-UI 2, a multimodal large language model designed to improve
UI understanding and interaction across diverse platforms. With multi-platform support, high-
resolutionimageencodingwithadaptivegridding,andimproveddatageneration,Ferret-UI2outper-
formsFerret-UIonalltestedbenchmarks. Themodeldemonstratesstrongzero-shottransferability
acrossplatforms,establishingFerret-UI2asasolidfoundationforuniversalUIunderstanding. Fu-
tureworkwillfocusonincorporatingadditionalplatformtypesandbuildingageneralistagentfor
universalUInavigation.
ACKNOWLEDGEMENTS
TheauthorswouldliketothankForrestHuang,ZhenYang,HaoxuanYou,AmandaSwearnginand
AlexanderToshevforvaluableguidance,suggestions,andfeedback.
REFERENCES
Tamer Abuelsaad, Deepak Akkil, Prasenjit Dey, Ashish Jagmohan, Aditya Vempaty, and Ravi
Kokku. Agent-e: From autonomous web navigation to foundational design principles in agen-
ticsystems. arXivpreprintarXiv:2407.13032,2024.
KirolosAtaallah,XiaoqianShen,EslamAbdelrahman,EssamSleiman,DeyaoZhu,JianDing,and
MohamedElhoseiny. Minigpt4-video: Advancingmultimodalllmsforvideounderstandingwith
interleavedvisual-textualtokens. arXivpreprintarXiv:2404.03413,2024.
Hao Bai, Yifei Zhou, Mert Cemri, Jiayi Pan, Alane Suhr, Sergey Levine, and Aviral Kumar. Di-
girl: Trainingin-the-wilddevice-controlagentswithautonomousreinforcementlearning. arXiv
preprintarXiv:2406.11896,2024.
Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang
Zhou,andJingrenZhou.Qwen-vl:Afrontierlargevision-languagemodelwithversatileabilities.
arXivpreprintarXiv:2308.12966,2023.
RogerioBonatti,DanZhao,FrancescoBonacci,DillonDupont,SaraAbdali,YinhengLi,JustinWa-
gle,KazuhitoKoishida,ArthurBucker,LawrenceJang,etal. Windowsagentarena: Evaluating
multi-modalosagentsatscale. arXivpreprintarXiv:2409.08264,2024.
Yuxiang Chai, Siyuan Huang, Yazhe Niu, Han Xiao, Liang Liu, Dingyu Zhang, Peng Gao, Shuai
Ren, and Hongsheng Li. Amex: Android multi-annotation expo dataset for mobile gui agents.
arXivpreprintarXiv:2407.17490,2024.
Rajat Chawla, Adarsh Jha, Muskaan Kumar, Mukunda NS, and Ishaan Bhola. Guide: Graphical
userinterfacedataforexecution. arXivpreprintarXiv:2404.16048,2024.
DongpingChen,YueHuang,SiyuanWu,JingyuTang,LiuyiChen,YilinBai,ZhigangHe,Chenlong
Wang, Huichi Zhou, Yiqiang Li, et al. Gui-world: A dataset for gui-oriented multimodal llm-
basedagents. arXivpreprintarXiv:2406.10819,2024a.
Xingyu Chen, Zihan Zhao, Lu Chen, Danyang Zhang, Jiabao Ji, Ao Luo, Yuxuan Xiong, and
Kai Yu. Websrc: A dataset for web-based structural reading comprehension. arXiv preprint
arXiv:2101.09465,2021.
ZehuiChen,KuikunLiu,QiuchenWang,JiangningLiu,WenweiZhang,KaiChen,andFengZhao.
Mindsearch: Mimickinghumanmindselicitsdeepaisearcher. arXivpreprintarXiv:2407.20183,
2024b.
11Preprint
Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Yantao Li, Jianbing Zhang, and Zhiy-
ong Wu. Seeclick: Harnessing gui grounding for advanced visual gui agents. arXiv preprint
arXiv:2401.10935,2024.
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng,
SiyuanZhuang,YonghaoZhuang,JosephE.Gonzalez,IonStoica,andEricP.Xing. Vicuna: An
open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https:
//lmsys.org/blog/2023-03-30-vicuna/.
Biplab Deka, Zifeng Huang, Chad Franzen, Joshua Hibschman, Daniel Afergan, Yang Li, Jeffrey
Nichols, and Ranjitha Kumar. Rico: A mobile app dataset for building data-driven design ap-
plications. In Proceedings of the 30th annual ACM symposium on user interface software and
technology,pp.845–854,2017.
XiangDeng,YuGu,BoyuanZheng,ShijieChen,SamStevens,BoshiWang,HuanSun,andYuSu.
Mind2web: Towardsageneralistagentfortheweb. NeurIPS,2024.
AbhimanyuDubey,AbhinavJauhri,AbhinavPandey,AbhishekKadian,AhmadAl-Dahle,Aiesha
Letman,AkhilMathur,AlanSchelten,AmyYang,AngelaFan,etal. Thellama3herdofmodels.
arXivpreprintarXiv:2407.21783,2024.
DifeiGao,LeiJi,ZechenBai,MingyuOuyang,PeiranLi,DongxingMao,QinchenWu,Weichen
Zhang,PeiyiWang,XiangwuGuo,etal.Assistgui:Task-orienteddesktopgraphicaluserinterface
automation. arXivpreprintarXiv:2312.13108,2023.
Izzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdari, Yutaka Matsuo, Douglas Eck, and
Aleksandra Faust. A real-world webagent with planning, long context understanding, and pro-
gramsynthesis. arXivpreprintarXiv:2307.12856,2023.
HongliangHe,WenlinYao,KaixinMa,WenhaoYu,YongDai,HongmingZhang,ZhenzhongLan,
and Dong Yu. Webvoyager: Building an end-to-end web agent with large multimodal models.
arXivpreprintarXiv:2401.13919,2024.
WenyiHong,WeihanWang,QingsongLv,JiazhengXu,WenmengYu,JunhuiJi,YanWang,Zihan
Wang,YuxiaoDong,MingDing,etal. Cogagent: Avisuallanguagemodelforguiagents. arXiv
preprintarXiv:2312.08914,2023.
PengJin,RyuichiTakanobu,WancaiZhang,XiaochunCao,andLiYuan.Chat-univi:Unifiedvisual
representationempowerslargelanguagemodelswithimageandvideounderstanding. InCVPR,
2024.
RaghavKapoor,YashParagButala,MelisaRussak,JingYuKoh,KiranKamble,WaseemAlshikh,
andRuslanSalakhutdinov.Omniact:Adatasetandbenchmarkforenablingmultimodalgeneralist
autonomousagentsfordesktopandweb. arXivpreprintarXiv:2402.17553,2024.
HanyuLai,XiaoLiu,IatLongIong,ShuntianYao,YuxuanChen,PengboShen,HaoYu,Hanchen
Zhang, Xiaohan Zhang, Yuxiao Dong, et al. Autowebglm: Bootstrap and reinforce a large lan-
guagemodel-basedwebnavigatingagent. arXivpreprintarXiv:2404.03648,2024.
Gang Li and Yang Li. Spotlight: Mobile ui understanding using vision-language models with a
focus,2023.
KunchangLi,YaliWang,YinanHe,YizhuoLi,YiWang,YiLiu,ZunWang,JilanXu,GuoChen,
Ping Luo, et al. Mvbench: A comprehensive multi-modal video understanding benchmark. In
CVPR,2024a.
Wei Li, William Bishop, Alice Li, Chris Rawles, Folawiyo Campbell-Ajala, Divya Tyamagundlu,
and Oriana Riva. On the effects of data scale on computer control agents. arXiv preprint
arXiv:2406.03679,2024b.
Yanda Li, Chi Zhang, Wanqi Yang, Bin Fu, Pei Cheng, Xin Chen, Ling Chen, and Yunchao Wei.
Appagentv2: Advancedagentforflexiblemobileinteractions. arXivpreprintarXiv:2408.11824,
2024c.
12Preprint
Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee.
Llava-next: Improved reasoning, ocr, and world knowledge, January 2024a. URL https://
llava-vl.github.io/blog/2024-01-30-llava-next/.
Junpeng Liu, Yifan Song, Bill Yuchen Lin, Wai Lam, Graham Neubig, Yuanzhi Li, and Xiang
Yue. Visualwebbench: How far have multimodal llms evolved in web page understanding and
grounding? arXivpreprintarXiv:2404.05955,2024b.
XiaoLiu,TianjieZhang,YuGu,IatLongIong,YifanXu,XixuanSong,ShudanZhang,HanyuLai,
Xinyi Liu, Hanlin Zhao, et al. Visualagentbench: Towards large multimodal models as visual
foundationagents. arXivpreprintarXiv:2408.06327,2024c.
Quanfeng Lu, Wenqi Shao, Zitao Liu, Fanqing Meng, Boxuan Li, Botong Chen, Siyuan Huang,
KaipengZhang,YuQiao,andPingLuo. Guiodyssey:Acomprehensivedatasetforcross-appgui
navigationonmobiledevices. arXivpreprintarXiv:2406.08451,2024.
Kaixin Ma, Hongming Zhang, Hongwei Wang, Xiaoman Pan, Wenhao Yu, and Dong Yu. Laser:
Llm agent with state-space exploration for web navigation. arXiv preprint arXiv:2309.08172,
2023.
BrandonMcKinzie,ZheGan,Jean-PhilippeFauconnier,SamDodge,BowenZhang,PhilippDufter,
DhrutiShah,XianzhiDu,FutangPeng,FlorisWeers,etal. Mm1: Methods,analysis&insights
frommultimodalllmpre-training. arXivpreprintarXiv:2403.09611,2024.
Songqin Nong, Jiali Zhu, Rui Wu, Jiongchao Jin, Shuo Shan, Xiutian Huang, and Wenhao Xu.
Mobileflow: Amultimodalllmformobileguiagent. arXivpreprintarXiv:2407.04346,2024.
YichenPan,DehanKong,SidaZhou,ChengCui,YifeiLeng,BingJiang,HangyuLiu,YanyiShang,
ShuyanZhou,TongshuangWu,etal. Webcanvas: Benchmarkingwebagentsinonlineenviron-
ments. arXivpreprintarXiv:2406.12373,2024.
Christopher Rawles, Sarah Clinckemaillie, Yifan Chang, Jonathan Waltz, Gabrielle Lau, Mary-
bethFair,AliceLi,WilliamBishop,WeiLi, FolawiyoCampbell-Ajala,etal. Androidworld: A
dynamic benchmarking environment for autonomous agents. arXiv preprint arXiv:2405.14573,
2024a.
Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana Riva, and Timothy Lillicrap. An-
droidinthewild: Alarge-scaledatasetforandroiddevicecontrol. NeurIPS,2024b.
Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-
baptisteAlayrac,RaduSoricut,AngelikiLazaridou,OrhanFirat,JulianSchrittwieser,etal.Gem-
ini1.5: Unlockingmultimodalunderstandingacrossmillionsoftokensofcontext. arXivpreprint
arXiv:2403.05530,2024.
GemmaTeam,MorganeRiviere,ShreyaPathak,PierGiuseppeSessa,CassidyHardin,SuryaBhu-
patiraju,Le´onardHussenot,ThomasMesnard,BobakShahriari,AlexandreRame´,etal. Gemma
2: Improvingopenlanguagemodelsatapracticalsize. arXivpreprintarXiv:2408.00118,2024.
Daniel Toyama, Philippe Hamel, Anita Gergely, Gheorghe Comanici, Amelia Glaese, Zafarali
Ahmed,TylerJackson,ShiblMourad,andDoinaPrecup. Androidenv: Areinforcementlearning
platformforandroid. arXivpreprintarXiv:2105.13231,2021.
JunyangWang,HaiyangXu,HaitaoJia,XiZhang,MingYan,WeizhouShen,JiZhang,FeiHuang,
andJitaoSang. Mobile-agent-v2: Mobiledeviceoperationassistantwitheffectivenavigationvia
multi-agentcollaboration. arXivpreprintarXiv:2406.01014,2024a.
Junyang Wang, Haiyang Xu, Jiabo Ye, Ming Yan, Weizhou Shen, Ji Zhang, Fei Huang, and Ji-
taoSang. Mobile-agent: Autonomousmulti-modalmobiledeviceagentwithvisualperception,
2024b.
LuyuanWang,YongyuDeng,YiweiZha,GuodongMao,QinminWang,TianchenMin,WeiChen,
andShoufaChen. Mobileagentbench: Anefficientanduser-friendlybenchmarkformobilellm
agents. arXivpreprintarXiv:2406.08184,2024c.
13Preprint
HaoWen,YuanchunLi,GuohongLiu,ShanhuiZhao,TaoYu,TobyJia-JunLi,ShiqiJiang,Yunhao
Liu, Yaqin Zhang, and Yunxin Liu. Autodroid: Llm-powered task automation in android. In
Proceedingsofthe30thAnnualInternationalConferenceonMobileComputingandNetworking,
pp.543–557,2024.
JasonWu,SiyanWang,SimanShen,Yi-HaoPeng,JeffreyNichols,andJeffreyBigham. Webui: A
datasetforenhancingvisualuiunderstandingwithwebsemantics. ACMConferenceonHuman
FactorsinComputingSystems(CHI),2023.
ZhiyongWu, ChengchengHan, ZichenDing, ZhenminWeng, ZhoumianzeLiu, ShunyuYao, Tao
Yu,andLingpengKong. Os-copilot:Towardsgeneralistcomputeragentswithself-improvement.
arXivpreprintarXiv:2402.07456,2024.
Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Jing
Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, et al. Osworld: Benchmarking multimodal
agents for open-ended tasks in real computer environments. arXiv preprint arXiv:2404.07972,
2024.
Tianqi Xu, Linyao Chen, Dai-Jie Wu, Yanjun Chen, Zecheng Zhang, Xiang Yao, Zhiqiang Xie,
YongchaoChen,ShilongLiu,BochenQian,etal. Crab: Cross-environmentagentbenchmarkfor
multimodallanguagemodelagents. arXivpreprintarXiv:2407.01511,2024.
Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, and Jianfeng Gao. Set-of-mark
promptingunleashesextraordinaryvisualgroundingingpt-4v. arXivpreprintarXiv:2310.11441,
2023.
Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable
real-worldwebinteractionwithgroundedlanguageagents. NeurIPS,2022.
HaoxuanYou,HaotianZhang,ZheGan,XianzhiDu,BowenZhang,ZiruiWang,LiangliangCao,
Shih-FuChang,andYinfeiYang. Ferret:Referandgroundanythinganywhereatanygranularity,
2023.
KeenYou,HaotianZhang,EldonSchoop,FlorisWeers,AmandaSwearngin,JeffreyNichols,Yinfei
Yang,andZheGan. Ferret-ui: Groundedmobileuiunderstandingwithmultimodalllms. arXiv
preprintarXiv:2404.05719,2024.
ChaoyunZhang,LiqunLi,ShilinHe,XuZhang,BoQiao,SiQin,MinghuaMa,YuKang,Qingwei
Lin,SaravanRajmohan,etal.Ufo:Aui-focusedagentforwindowsosinteraction.arXivpreprint
arXiv:2402.07939,2024a.
Danyang Zhang, Hongshen Xu, Zihan Zhao, Lu Chen, Ruisheng Cao, and Kai Yu. Mobile-env:
anevaluationplatformandbenchmarkforllm-guiinteraction. arXivpreprintarXiv:2305.08144,
2023.
HaotianZhang,MingfeiGao,ZheGan,PhilippDufter,NinaWenzel,ForrestHuang,DhrutiShah,
Xianzhi Du, Bowen Zhang, Yanghao Li, et al. Mm1. 5: Methods, analysis & insights from
multimodalllmfine-tuning. arXivpreprintarXiv:2409.20566,2024b.
HaotianZhang,HaoxuanYou,PhilippDufter,BowenZhang,ChenChen,Hong-YouChen,Tsu-Jui
Fu, William Yang Wang, Shih-Fu Chang, Zhe Gan, et al. Ferret-v2: An improved baseline for
referringandgroundingwithlargelanguagemodels. arXivpreprintarXiv:2404.07973,2024c.
BoyuanZheng, BoyuGou, JihyungKil, HuanSun, andYuSu. Gpt-4v(ision)isageneralistweb
agent,ifgrounded. arXivpreprintarXiv:2401.01614,2024a.
Longtao Zheng, Rundong Wang, Xinrun Wang, and Bo An. Synapse: Trajectory-as-exemplar
promptingwithmemoryforcomputercontrol. InICLR,2023.
Longtao Zheng, Zhiyuan Huang, Zhenghai Xue, Xinrun Wang, Bo An, and Shuicheng Yan.
Agentstudio: A toolkit for building general virtual agents. arXiv preprint arXiv:2403.17918,
2024b.
Shuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng,
TianyueOu,YonatanBisk,DanielFried,etal. Webarena: Arealisticwebenvironmentforbuild-
ingautonomousagents. arXivpreprintarXiv:2307.13854,2023.
14Preprint
A DETAILED RESULTS ON ELEMENTARY AND ADVANCED TASKS
Table6showstheperformanceon9subtasks×5platformsoftheFerret-UI2model.Weemprically
findthatOCR,widgetlistingandcomprehensivedescriptionarerelativelydifficulttaskscompared
toothertaskswithinthesametasktype.
Table6: TheperformancebreakdownofFerret-UI2withLlama-3-8Bbackbone. Notethatweonly
reportGPT-4oevaluationscoresandomitthemulti-IoUscoresforadvancedtasksforsimplicity.
TestDomain
Tasktype Task
iPhone iPad AppleTV Web Android
OCR 75.3 69.3 74.3 80.5 74.5
Refer WidgetClassify 79.1 78.7 82.5 83.6 82.0
Tapperbility 89.2 86.0 - 84.3 85.6
WidgetListing 76.7 74.9 71.6 79.7 76.8
Ground FindText 85.2 84.0 82.2 90.2 82.2
FindWidget 88.6 86.8 87.7 87.8 86.4
Comprehensive 89.0 86.5 84.1 83.7 85.6
AdvancedTasks Perception 94.1 87.5 86.6 93.2 92.1
Interaction 93.5 92.7 86.1 96.4 94.7
B DETAILED GPT-4O PROMPTS FOR GENERATING ADVANCED TASK DATA
In this section, we elaborate how we prompt GPT-4o to generate training data for advanced tasks.
Inparticular,wehavethefollowingrequirementswhengeneratingeachtypeofadvancedtasks.
1. ComprehensiveDescription: Provideaone-sentencedescriptionoftheoverallfunctionalityof
theUIpageshowninthescreenshot. Then,describethescreenshotindetailbydividingitinto
severalareas/groupsandexplainingthefunctionalityofeacharea/group.
2. Multi-RoundPerceptionQA:
(a) Basic Perceptual Understanding: Interpret the content of referred widgets, ground the
positions of the widgets based on the widget descriptions, and describe the functionality
andstatus(enabled,disabled,selected,hovered)ofeachwidget.
(b) ContextualAwareness: UnderstandthecontextinwhichUIwidgetsarepresented,includ-
ingtherelationshipsbetweendifferentUIcomponentsandhowtheycontributetotheoverall
userexperience.
(c) Layout and Hierarchy Recognition: Recognize how widgets are grouped and nested
withintheUIlayout.
3. Multi-RoundInteractionQA:
(a) Generateinstructionsforuser-centeredinteraction. Forexample, “pleasehelpmeconfirm
submission”insteadof“pleaseclickon[Box0]button”.
(b) Identifyandinteractwithbuttons,links,icons,scrollbars,andtoggles,andrecognizewid-
getsthatcannotbeinteractedwith,suchasstatictextandbackgroundimages.
(c) Identify and interact with input fields, dropdowns, checkboxes, and radio buttons, as well
as their current states (e.g., checked, unchecked, filled, empty) in the context of specific
interactiontasks.
(d) Predict current step for potential navigation paths based on currently visible widgets like
tabs,menusandbreadcrumbscontrols.
15Preprint
C ADVANCED TASK EXAMPLES
ComprehensiveDescription
OVERALL: This UI page is for adjusting settings related to downloads, language, news,
andnotifications.
DETAIL:
• TopNavigationBar: ContainsmenuoptionssuchasHome,Account,Settings(currently
selected),andDownloads,allowinguserstonavigatebetweendifferentsections.
• GamesSection: Featuresacheckboxlabeled”DownloadoverWiFionly,”whichiscur-
rentlychecked,indicatingthatdownloadsarerestrictedtoWiFiconnections.
• LanguageSection: Offersadropdownmenutoselectthelanguage,currentlysettoEn-
glish,withapinkarrowindicatingmoreoptions.
• Title Screen Section: Includes a checkbox for ”Show Sago News,” which is checked,
meaningnewsupdateswillbedisplayedonthetitlescreen.
• Notifications Section: Contains a checkbox for ”Display notifications,” which is
unchecked,allowingtheusertoenableordisablenotifications.
• Close Button: Located in the top right corner, represented by an ’X’, used to close the
settingspage.
Multi-RoundPerceptionQA
Q:Whatdoesthe”SERVEWARE”textrepresent?
A:Thetext”SERVEWARE”[718,72,948,108]islikelyacategorylabelindicatingthatthe
itemsdisplayedarerelatedtoservingware.
Q:Canyouidentifyanyproductnamesandtheircorrespondingprices?
A:Yes,theproduct”ARVBRO¨LLOP”[35,847,233,907]ispricedat$12.99[10,6,112,
32]and”IKEA365+”[609,787,809,823]ispricedat$2.99[1174,1336,1627,2119].
Q:Whatisthefunctionoftheshoppingcarticon?
A:Theshoppingcarticon[630,2157,690,2217]istypicallyusedtoviewtheitemsinyour
cartorproceedtocheckout.
Q:Howistheproductratingdisplayed?
A:Theproductratingisdisplayedwithstarsymbolsandanumericalrating. Forexample,
”ARVBRO¨LLOP”hasaratingof4.9nexttothestars[[38,956,100,986],[248,1004,294,
1030]].
Q:Whatdoesthe”Filter”buttondo?
A: The ”Filter” button [1564, 170, 1630, 198] is likely used to apply specific criteria to
narrowdowntheproductsearchresults.
Q:Arethereanynavigationelementsonthescreen?
A: Yes, there seems to be a navigation bar at the bottom with icons for different sections,
likeahouseiconforhome[611,268,1057,714]andapencilicon[1320,2157,1380,2217]
whichmightbeforeditingorcreatingalist.
16Preprint
Multi-RoundInteractionQA
Q:HowcanIrefreshthecontentshown?
A:Youcantapontherefreshicon[272,736,424,882]toreloadthecontent.
Q:Whatdoesthe”General”tagreferto?
A: The ”General” tag [1074, 900, 1443, 926] categorizes the content related to general
events,liketheCaltechBeaverNameReveal.
Q:HowcanIviewmorerecentactivities?
A:Youcantapthe”Recent”sectionlabel[90,598,202,630]toviewmorerecentactivities.
Q:Whatisthelogodisplayedonthispage?
A:ThelogoatthetopofthepageistheSCIACNetworklogo[198,56,444,128]represent-
ingthenetworkconductingtheevent.
Q:HowdoIchecktheinstitutionhostingtheevent?
A: The eventis hostedby theCalifornia Instituteof Technology, asshown inthe text[11,
499,483,542]belowtheeventdetails.
Visual Prompting
Web AppleTV
Figure5:ExamplesofvisualpromptingusingGPT-4otogeneratetaskdataforMulti-RoundPercep- Apple Confidential. Internal use only.
tionQAandMulti-RoundInteractionQA.EachUIwidgetisannotatedwithacorner-stylebounding
box,whereonlythecornersofthewidgetarehighlightedbysmalllines,leavingtherestofthebox
open. Thisminimalisticboundingstyleisaccompaniedbyauniquenumbertagplacednearoneof
the corners, making it easy to identify and reference specific UI widgets for further interaction or
perceptionanalysis.
17Preprint
Figure6: RealexamplesoftheFerret-UI2modelinteractingwithAndroid.
D ADDITIONAL INFERENCE EXAMPLES
Figures6,7,8and9showmorequalitativeinferenceexamplesofFerret-UI2ondifferentplatforms.
Additionally, in Figure 10, we show an example of Ferret-UI 2 performing multi-step interactions
onreal-timewebpagesfollowingGUIDE-styleQAs.
E RESOLUTION STATISTICS
In this section, we present the resolution statistics for images collected from various platforms,
categorizedbydevicetypesandresolutions,assummarizedinTable7.
F LABEL STATISTICS AND MAPPING RESULTS OF ORIGINAL DATA ACROSS
PLATFORMS
Wedemonstratethelabelstatisticsoforiginaldataacrossplatformsandtheirmappingresultsinto
auniformlabelspaceforbetterjointtrainingasshowninFigures11,12,13and14. Themapping
resultsareobtainedviaGPT-4suggestionsandadditionalhumanreviews. Notethat“Other”label
aftermappingindicatesthewidgetinformationwillbedeprecated.
18Preprint
Figure7: RealexamplesoftheFerret-UI2modelinteractingwithiPhone.
Table7: Resolutionstatisticsbydevicetype.
DeviceType Resolution NumberofImages
828×1792 83,250
1125×2436 6,055
iPhone
1792×828 4,686
2436×1125 104
2224×1668 4,829
iPad 1668×2224 14,312
1242×2208 19
AppleTV 1920×1080 16,152
1280×720 53,500
1366×768 53,500
1536×864 53,500
WebUI
1920×1080 53,500
2048×2732 53,500
1170×2532 53,500
540×960 14,092
1080×1920 52,102
Android
1920×1080 55
960×540 12
19Preprint
Figure8: RealexamplesoftheFerret-UI2modelinteractingwithiPad.
Figure9: RealexamplesoftheFerret-UI2modelinteractingwithAppleTV.
20Preprint
Figure 10: An example of the Ferret-UI 2 model performing multi-step interactions on real-time
webpagesfollowingGUIDE-styleQAs.
21Preprint
Text
Icon Checkbox
Others
TextField
SegmentControl
Picture
Container
TabBarItem
Figure11: Thestatisticsoforiginallabels(wedge)andconvertedlabels(legend)oniOS(iPhone+
iPad)platforms. Eachcolorrepresentsoneconvertedlabel,andeachwedgerepresentsoneoriginal
labelfromsourcedata.
Text
Image
BackgroundImage RC ah de ioc k Bb uo tx ton
Input
Drawer
Advertisement
Web View
Icon Toolbar
Text Button
List Item Card
Figure12: Thestatisticsoforiginallabels(wedge)andconvertedlabels(legend)onAndroidplat-
form. Eachcolorrepresentsoneconvertedlabel,andeachwedgerepresentsoneoriginallabelfrom
sourcedata.
22Preprint
SegmentControl
OtherUI
Icon
Container
Text
Picture
Figure 13: The statistics of original labels (wedge) and converted labels (legend) on AppleTV
platform. Eachcolorrepresentsoneconvertedlabel, andeachwedgerepresentsoneoriginallabel
fromsourcedata.
Img Button
ListItem
Link
Generic
Heading
Paragraph
List
LineBreak
Section
GridCell
StaticText
Figure14: Thestatisticsoforiginallabels(wedge)andconvertedlabels(legend)onwebpageplat-
form. Eachcolorrepresentsoneconvertedlabel,andeachwedgerepresentsoneoriginallabelfrom
sourcedata.
23