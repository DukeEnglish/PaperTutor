MissNODAG: Differentiable Learning of Cyclic Causal Graphs
from Incomplete Data
Muralikrishnna G. Sethuraman Razieh Nabi Faramarz Fekri
Georgia Institute of Technology Emory University Georgia Institute of Technology
Abstract 2018), infer the causal structure by enforcing condi-
tional independencies observed in the data, though
Causal discovery in real-world systems, such they often struggle with scalability due to the large
as biological networks, is often complicated number of required conditional independence tests.
byfeedbackloopsandincompletedata. Stan- Score-based methods, such as the GES algorithm
dardalgorithms, whichassume acyclicstruc- (Meek, 1997; Hauser and Bu¨hlmann, 2012), optimize
tures or fully observed data, struggle with a penalized score function, like the Bayesian Informa-
these challenges. To address this gap, we tion Criterion, over the space of candidate graphs,
proposeMissNODAG,adifferentiableframe- usually employing greedy search techniques. There
work for learning both the underlying cyclic also exist hybrid methods which combine elements of
causalgraphandthemissingnessmechanism both approaches, leveraging conditional independence
from partially observed data, including data testsalongsidescoreoptimization(Tsamardinosetal.,
missing not at random. Our framework in- 2006; Solus et al., 2017; Wang et al., 2017). Re-
tegrates an additive noise model with an cent advances have introduced differentiable discov-
expectation-maximization procedure, alter- erymethods,suchastheNOTEARSalgorithm(Zheng
nating between imputing missing values and etal.,2018),whichframeslearningofadirectedacyclic
optimizing the observed data likelihood, to graph (DAG) as a continuous optimization problem,
uncover both the cyclic structures and the enabling scalable and efficient solutions via gradient-
missingnessmechanism. Wedemonstratethe based methods. Following NOTEARS, several ex-
effectiveness of MissNODAG through syn- tensions have been developed for learning DAGs un-
theticexperimentsandanapplicationtoreal- der various assumptions in observational settings (Yu
world gene perturbation data. et al., 2019; Ng et al., 2020, 2022; Zheng et al., 2020;
Lee et al., 2019).
Most causal discovery methods make one or both of
1 INTRODUCTION
the following assumptions: (i) the data is fully ob-
served, and (ii) the underlying graph is acyclic. How-
Causal discovery, the process of identifying causal re-
ever, real-world systems often violate these assump-
lationships from data, is fundamental across scien-
tions. Biologicalsystems, suchasgeneregulatorynet-
tificdomainssuchasbiology,economics,andmedicine
works,andsocio-economicprocessesfrequentlyexhibit
(Spirtes et al., 2000; Sachs et al., 2005; Zhang et al.,
feedback loops (cycles) (Freimer et al., 2022), while
2013;Segaletal.,2005;ImbensandRubin,2015). Un-
missingdataiscommoninpracticalapplications(Get-
derstanding these relationships is crucial for predict-
zenetal.,2023). Thesecomplexitiessignificantlylimit
ing how systems respond to interventions, enabling
theapplicabilityofstandardcausaldiscoverymethods.
informed decision-making in complex systems (Solus
etal.,2017;Suliketal.,2017;Sethuramanetal.,2023). Missingdatamechanismsareclassifiedintothreecate-
Traditionally, causal relationships are modeled using gories: missing completely at random (MCAR), miss-
directed graphs, where nodes represent variables, and ing at random (MAR), and missing not at random
directed edges encode cause-effect relationships. (MNAR) (Little and Rubin, 2019). One common ap-
proach to dealing with missing data involves discard-
Existing causal discovery methods are typically di-
ing incomplete samples or excluding variables with
vided into two main categories: constraint-based and
missing data (Carter, 2006; Van den Broeck et al.,
score-based approaches. Constraint-based methods,
2015; Strobl et al., 2018), which is only suitable when
such as the PC algorithm (Spirtes et al., 2000; Tri-
data is MCAR or when missingness rate is negligi-
antafillou and Tsamardinos, 2015; Heinze-Deml et al.,
4202
tcO
42
]LM.tats[
1v81981.0142:viXraMissNODAG: Differentiable Cyclic Causal Graph Learning from Incomplete Data
ble. Otherwise, it leads to performance degradation ontheapproachofGaoetal.(2022). FollowingSethu-
as the missingness increases. Another common ap- raman et al. (2023) and Behrmann et al. (2019), we
proachisimputation-basedmethodswherethemissing leverageresidualnormalizingflowstomodeldatalike-
data is first imputed before applying causal discovery lihood. Through synthetic experiments, we show that
algorithm on the data. Some notable imputation al- MissNODAG outperforms state-of-the-art imputation
gorithms include multivariate imputation by chained techniques combined with causal learning on partially
equations (MICE) (White et al., 2011), MissForest missing interventional data.
(Stekhoven and Bu¨hlmann, 2012), optimal transport
We organize the paper as follows. In Section 2, we
(Muzellec et al., 2020), and a few deep learning based
describe the problem setup and outline the model-
approaches(Lietal.,2019;Luoetal.,2018). However,
ing assumptions. Section 3 introduces the proposed
imputation-based methods typically assume that data
expectation-maximization-based MissNODAG frame-
is MAR, which can introduce bias when data is actu-
work. In Section 4, we validate MissNODAG on vari-
ally MNAR–a common occurrence in practice (Singh,
ous synthetic datasets. Section 5 concludes the paper.
1997;Wangetal.,2020;Kyonoetal.,2021;Gaoetal.,
All proofs are deferred to the appendix.
2022). ResearchaddressingMNARincludes(Gainand
Shpitser, 2018), which uses reweighted observed cases
2 PROBLEM SETUP
asinputtothePCalgorithmalongsideaweightedcor-
relationmatrix. Additionally,Tuetal.(2019)extends
the PC algorithm by incorporating corrections to ac- A list of notations is provided in Appendix A.
countforbothMARandcertaincasesofMNAR,while
Structural Equation Model. Let G(X) denote
also learning the underlying missingness mechanisms.
a possibly cyclic causal graph with a set of vertices
Furthermore, while the acyclicity assumption simpli- X = (X 1,...,X K), representing a vector of K vari-
fiescomputationsbyfactorizingjointdistributionsinto ables connected by directed edges. We will abbreviate
conditionaldensities, manyreal-worldsystemsfeature G(X) as simply G, when the vertex set is clear from
cyclic relationships. Feedback loops are common in the given context. We assume the following structural
both biological systems and socio-economic processes equation model (SEM) with additive noise terms to
(Sachs et al., 2005; Freimer et al., 2022). Several ap- capture the functional relationships between variables
proaches have been developed to relax the acyclicity in G (Bollen, 1989; Pearl, 2009a):
assumption, allowing for cyclic causal graphs. For (cid:0) (cid:1)
X =f pa (X ) +ϵ , k =1,...,K, (1)
k k G k k
example, early work by Richardson (1996) extended
theconstrained-basedframeworktoaccountforcycles, where pa G(X k) denotes the parents of X k in G, that
Lacerdaetal.(2008)generalizedtheICAbasedcausal is, pa G(X k) = {X ℓ ∈ X : X ℓ → X k}. We assume
discovery for linear non-Gaussian cyclic graphs. More that self-loops (edges of the form X k → X k) are ab-
recent score-based methods for learning cyclic graphs sent in G(X), as this could lead to model identifia-
include (Huetter and Rigollet, 2020; Am´endola et al., bility issues (Hyttinen et al., 2012). The function f k
2020;MooijandHeskes,2013;Drtonetal.,2019). Ad- describestherelationshipbetweenX k anditsparents,
ditionally, methods such as those proposed by Hytti- with ϵ k as the exogenous noise term, assumed to be
nen et al. (2012) and Huetter and Rigollet (2020) fo- mutually independent (no unmeasured confounders),
cusonlearningcyclicgraphsfrominterventionaldata. and collected as ϵ = (ϵ 1,...,ϵ K). Let F(X) collect
Sethuraman et al. (2023) further extended this line of the functions f k(pa G(X k)), for all k. The structural
approachtononlinearcyclicdirectedgraphs,eliminat- equations in (1) can be written as follows:
ing the need for augmented Lagrangian-based solvers
X =F(X)+ϵ. (2)
by directly modeling the data likelihood.
Let id denote the identity map, so (id−F) maps X to
Contributions. In this work, we address two major
ϵ. We assume that this mapping is bijective, ensuring
limitations in causal discovery: handling informative
theexistenceof(id−F)−1,andthatboth(id−F)and
MNAR data and accommodating cyclic relationships.
(id−F) are differentiable. The former ensures that
We introduce MissNODAG, a novel framework that
there is a unique X for a given ϵ, thus, we can express
adapts an expectation-maximization (EM) procedure
X as X = (id−F)−1(ϵ). This assumption is needed
to learn cyclic causal graphs from incomplete inter-
for our developments in Section 3.2, and is naturally
ventionaldata,applicabletobothlinearandnonlinear
satisfied when the underlying graph is acyclic (Mooij
structuralequationmodelsaswellasMCAR,MAR,or
and Heskes, 2013; Sethuraman et al., 2023).
MNAR missingness models. MissNODAG alternates
between imputing missing values and maximizing the Intervention operations can be readily incorporated
expected log-likelihood of the observed data, building into (2). In this work, we focus exclusively on surgi-
cal, or hard, interventions (Spirtes et al., 2000; Pearl,Muralikrishnna G. Sethuraman, Razieh Nabi, Faramarz Fekri
2009a). Graphically, this corresponds to removing all
incoming edges to the intervened variable. Following
X1 X2 X3 X1 X2 X3
similarnotationalconventionin(Hyttinenetal.,2012;
Sethuraman et al., 2023), we can decompose X into R1 R2 R3 R1 R2 R3
disjoint sets, X =X ∪X , where X represents the
set of intervened
varI iablesO
in an
intervI
entional exper-
Y1 Y2 Y3 Y1 Y2 Y3
(a) (b)
iment, and X represents the set of purely observed
O
variables. Let D ∈ {0,1}K×K be a diagonal matrix Figure1: Examplem-graphswiththreevariablesillus-
where D = 1 if X ∈ X , and 0 otherwise. Under trating: (a) An MNAR mechanism considered in our
kk k O
this setting, the SEM in (2) is now modified to: MissNODAG framework; (b) An MNAR mechanism
whereRsareconnectedandthefulllawisidentifiable.
X =DF(X)+Dϵ+C, (3)
where C denotes a vector of size K representing inter- these graphs by G (V), where V = (X,R,Y). Two
m
vention assignments for variables in X. Specifically, examples of missing data graphs (or m-graphs), with
C = X if X ∈ X , and C = 0 otherwise. Let K =3 substantive variables, are provided in Figure 1;
k k k I k
ϵ denotetheexogenousnoisetermscorrespondingto deterministicrelationsaredrawningraytodistinguish
O
variablesinX . Letp (ϵ )andp (X )bethejoint them from probabilistic ones.
O ϵO O XI I
probability densities of ϵ and X , respectively. We
O I Graphically, an MCAR mechanism has no incoming
thus have,
edges into any missingness indicator in R, a MAR
p X(X)=p XI(X I) p ϵO(cid:0) ϵ O(cid:1) (cid:12) (cid:12)det J (id−DF)(X)(cid:12) (cid:12), (4) m are ec fh ua ln lyis om bsh ea rvs ep da ,r ae nn dts ao nf Mm Niss Ain Rgn me es cs hi an nd ii sc mato inr vs ot lh va est
wheredetJ (X)denotesthedeterminantofthe missingness indicators with parents in X. Identifying
(id−DF)
vector-valued Jacobian matrix of the function (id − thefullortargetlawinanm-graphwithMNARmech-
DF) at X. See a proof in Appendix B.1. anismsfrom observational dataisnot always possible.
Previousworkhasextensivelystudiedidentificationin
Missing Data Model. Given sampled data on X,
graphicalmodelsofmissingdata(Bhattacharyaetal.,
let R = (R ,...,R ) be the vector of binary miss-
1 K 2020; Nabi et al., 2020; Mohan and Pearl, 2021; Nabi
ingness indicators with R = 1 if X is observed and
k k et al., 2022). Nabi et al. (2020) have shown that the
R = 0 if X is missing. We only observe a coars-
k k fulllawinanm-graphisidentifiedif and only if there
ened version of X , denoted by Y , which is deter-
k k are no edges of the form X →R (no self-censoring)
ministically defined as Y = X when R = 1, and k k
k k k and X →R ←R (no colluders).
Y = ? if R = 0. Let Y = (Y ,...,Y ) denote the j k j
k k 1 K
coarsened variables. Additionally, we have access to Given partially observed data from a set of interven-
S = (S ,...,S ) where S is a binary indicator of tionalexperiments,ourobjectiveistolearntheunder-
1 K k
intervention, such that S = 0 if X is intervened on lying full law that generated the sample. Specifically,
k k
(i.e., X ∈ X ), and S = 1 otherwise. We assume this involves learning both the underlying target law
k I k
we have n i.i.d. copies of (Y,R,S), and the dataset is and the missingness mechanism.
denotedbyD ={y(i),r(i),s(i)}n , wherey(i),r(i),s(i)
i=1
represent the i-th observed values of Y,R,S.
3 THE MissNODAG FRAMEWORK
We define a missing data model as the collection of
distributions over the variables X,R,Y. By chain We assume the target law p(X) and the missingness
rule of probabilities, we can express p(X,R,Y) as mechanism p(R|X) are parameterized by finite vec-
p(X)p(R|X)p(Y|X,R). Werefertop(X)asthetarget tors θ and ϕ, respectively. Thus, we write the full law
law,p(R|X)asthemissingnessmechanism,p(X,R)as p(X,R) as p(X,R|θ,ϕ) = p(X|θ) p(R|X,ϕ). In order
the full law, while p(Y|X,R) is the coarsening mech- to learn the full law, we proceed by maximizing the
anism, which is deterministically defined. Borrowing log-likelihood of the observed data law.
ideas from the graphical models of missing data (Mo-
Let Γ = {k : r(i) = 1} and Ω = {k : r(i) = 0}
han et al., 2013; Nabi et al., 2022), we use graphs to i k i k
represent the sets of indices for the variables that are
encodeassumptionsaboutp(X,R,Y). Specifically,we
observed and missing, respectively, in the i-th sample;
assumethattherelationsbetweenvariablesinthetar-
thus y(i) = x(i). Consequently, the observed data law
get law p(X) are directed and can include cycles, and Γi
the missingness mechanism p(R|X) factorizes accord-
p(cid:0) x(i),r(i)(cid:1)
can be written down as
Γi
ing to a DAG, where pa (R ) can only be a subset
G k (cid:90)
of X and R\R k. Finally, due to deterministic rela- p(cid:0) x(i),r(i) |θ,ϕ(cid:1) = p(cid:0) x(i),x(i),r(i) |θ,ϕ(cid:1) dx(i). (5)
tions, Y has only two parents R and X . We denote
Γi Γi Ωi Ωi
k k kMissNODAG: Differentiable Cyclic Causal Graph Learning from Incomplete Data
This integration is generally intractable due to of (9)andontheconvergenceanalysisofMissNODAG
marginalization over missing variables and the lack of are provided in Appendix B.3.
a closed-form solution. We address this intractability
Identifiabilityanddatarequirements. Forafam-
inmaximizingtheobserveddatalikelihoodwhendata
ilyofinterventions,maximizing(8)withcompletedata
is generated from an m-graph (Section 2). First, we
(assuming no missingness) is equivalent to maximiz-
provide an overview of the MissNODAG framework,
ing the data likelihood. In the linear SEM with in-
followedbydetailsoncomputingthelog-likelihoodfor
finite samples, Sethuraman et al. (2023) showed that
missing and observed variables, and discuss imputing
therecoveredgraphbelongstothesameinterventional
themissingvariablesunderlinearandnonlinearSEMs.
quasi-equivalence class (Ghassami et al., 2020) as the
ground truth. With single-node interventions for all
3.1 The Overall Procedure
nodes,theequivalenceclasscollapsestothetruecausal
graph (Hyttinen et al., 2012). In the presence of miss-
Assuming the data D = {y(i),r(i),s(i)}n is gener-
i=1 ingdata,theEMalgorithmdoesnotdirectlyoptimize
ated from an experiment where the full law p(X,R)
the observed data likelihood, limiting convergence to
is represented via m-graphs, our goal is to learn the
entire m-graph structure by maximizing, L˜(D,θ,ϕ), a astationarypoint. However,ourexperimentsindicate
that MissNODAG performs similarly to NODAGS-
regularized log-likelihood of the observed data law:
Flow trained on complete data (assuming no missing-
n ness), especially when single-node interventions cover
m θ,a ϕx (cid:88) logp(cid:0) x( Γi i),r(i)|θ,ϕ(cid:1) −λ 1R(θ)−λ 2R(ϕ), (6) all nodes and the missing probability is low (see Sec-
i=1 tion 4). If the target law factorizes according to a
DAG, identifiability from observational data is possi-
where R(·) is a regularization function that promotes
ble with modifications; see Appendix C.
sparsity and λ ,λ are regularization parameters.
1 2
As discussed, computing p(x( Γi i),r(i)|θ,ϕ) is gener- 3.2 Computational Details of the E-step
ally intractable, with no closed-form solution. How-
ever, (6) can be solved using the iterative penalized Computing the expected log-likelihood in the E-step
expectation-maximization (EM) method (Chen et al., canbechallengingforadirected(cyclic)graph. First,
2014). Unlike imputation methods that directly sam- let’slookatlogp(cid:0) x(i),x(i),r(i)|Θ(cid:1) in(7),whichequals:
ple missing values, the EM algorithm starts with an
Γi Ωi
initialparameterΘ0 =(θ0,ϕ0)andalternatesbetween logp (cid:0) x(i),x(i)|θ(cid:1) +logp(cid:0) r(i)|x(i),x(i),ϕ(cid:1) . (10)
thefollowingtwostepsatiterationtuntilconvergence:
X Γi Ωi Ωi Γi
(cid:0) (cid:1)
E-step: Use the current estimates of the model pa- 3.2.1 Target Law, logp X |θ
X
rameters, Θt = (θt,ϕt), and the non-missing data to
As per (4), computing logp (X|θ) in (10) requires:
compute the expected log-likelihood of the full data, X
denoted by Q(Θ|Θt), and given by: (cid:12) (cid:12)
log(cid:12)detJ (id−DF)(X)(cid:12). (11)
n
(cid:88) E (cid:104) logp(cid:0) x(i),x(i),r(i) |Θ(cid:1)(cid:105) . (7) In the worst case, the Jacobian matrix may require
i=1
x( Ωi i)∼p(·|x( Γi i),r(i),Θt) Γi Ωi gradient calls in the order of K2. To that end, fol-
lowing Sethuraman et al. (2023), we employ contrac-
tiveresidualflows (Behrmannetal.,2019;Chenetal.,
M-step: MaximizeQ(Θ|Θt),computedintheE-step,
2019) to compute the log-determinant of the Jacobian
with respect to Θ:
in a tractable manner.
Θt+1 =argmax Q(Θ|Θt)−λ R(θ)−λ R(ϕ). (8)
1 2 Modeling the SEM in (2). We assume the SEM
Θ
functions in F(X) in (2) are Lipschitz with Lipschitz
We use stochastic gradient-based solvers to solve the
constant less than one. Such functions are called con-
maximization problem, alternating between updating
tractive functions. It then follows from Banach fixed
theparametersofthetargetlaw,θ,andtheparameters
point theorem (Rudin, 1953) that the mapping func-
of the missingness mechanism, ϕ. Note that,
tion (id−DF) is contractive and invertible.
n
(cid:88) logp(x(i),r(i) |Θ)≥Q(Θ|Θt)−const. (9) Weuseneuralnetworkstomodelthecontractivefunc-
Γi tions in F(X). As shown by Behrmann et al. (2019),
i=1
neural networks can be constrained to be contrac-
This inequality indicates that we maximize a lower tive during the training phase by rescaling the layer
bound of the log-likelihood as we update the parame- weights by their corresponding spectral norm. While
tersduringtheM-step. Moredetailsonthederivation contractivity is a sufficient condition for the existenceMuralikrishnna G. Sethuraman, Razieh Nabi, Faramarz Fekri
of(id−F)−1,itisnotnecessary. Whentheunderlying We note that the remainder of (4) can be efficiently
graph governing the target law p(X) is a DAG, it is computed with a forward pass of the neural network.
possibletohavenon-contractivefunctionsinF(X)for
which (id−F)−1 exists; see (Sethuraman et al., 2023) 3.2.2 Calculating Expectation via Imputation
for more details.
Combining (4), (7), (14), and (15), we finally get
Naive implementation of a neural network may not
produce promising results as it may introduce self- n (cid:40)
cycles. To circumvent this issue and simultaneously
Q(Θ|Θt)∝(cid:88)
E
logp(cid:16) r(i)|x(i),x(i),ϕ(cid:17)
add sparsity penalization, we introduce a dependency i=1
x( Ωi i)|x( Γi i),r(i);Θt Ωi Γi
dm ea ps ek ndm ea nt cr ii ex sM bet∼ we{ e0 n,1 t} hK e× nK odto esex inpli tc hi etly gre an pc ho ,de wt ih the
+logp
ϵO(cid:16)
ϵ( Oi)
i(cid:17)
−E
N,W(cid:34) (cid:88)N W m⊤ ·J PDm
NF
(ℓ(x ≥(i) m)W
)
(cid:35)(cid:41)
,
zero diagonal entries to mask out the self-loops. Thus m=1
the SEM model F(X) takes the following form
whereϵ(i) =(id−D F)(x(i)),D isthediagonalmatrix
i i
[F (X)] =[NN (M ⊙X)] , (12) corresponding to the interventional mask for the i-th
θ k θ ∗,k k
sample, i.e., D =diag(s(i),...,s(i)).
where NN denotes a fully connected neural network i 1 K
θ
function with parameters θ, M denotes the k-th The expectation in the approximation for Q(Θ|Θt)
∗,k
column of M, and ⊙ denotes the Hadamard prod- generally lacks a closed-form solution. Therefore,
uct. The dependency mask is sampled from Gumbel- it must be approximated by the sample mean, us-
softmaxdistribution(Jangetal.,2016),M ∼p(M|θ) ing samples drawn from the posterior distribution
and the parameters θ are updated during the training p(x(i)|x(i),r(i),Θt). This presents two main chal-
Ωi Γi
(M-step). In this case, the sparsity penalty R(θ) in lenges: (i) the posterior distribution may not have a
(6) is set as an L1 norm, i.e., R(θ)=E ∥M∥ . closed form, and (ii) direct sampling may be infeasi-
M∼p(·|θ) 1
ble even when the posterior distribution can be evalu-
Computing the log-determinant in (11). Wenote
(cid:12) (cid:12) (cid:12) (cid:12) ated. The difficulty arises due to the presence of non-
that log(cid:12)detJ (id−DF)(X)(cid:12) = log(cid:12)det(I −DJ F)(X)(cid:12),
linear relations in F(X) and the missingness mecha-
where I is the K×K identity matrix. Thus, the log-
nism p(r(i)|x(i),x(i),ϕt), which may preclude straight-
determinant of the Jacobian matrix can be computed Ωi Γi
forward sampling. Therefore, we employ rejection
usinganunbiasedestimatorbasedonthepowerseries
sampling (Koller and Friedman, 2009) to draw sam-
expansion introduced by Behrmann et al. (2019),
ples from a proposal distribution Q(x(i)), from which
(cid:12) (cid:12) (cid:88)∞ 1 (cid:110) (cid:111) samples can be readily generated. Ωi
log(cid:12)detJ (X)(cid:12)=− Tr Jm (X) , (13)
(cid:12) (id−DF) (cid:12) m DF
m=1 To that end, a constant c 0 > 0 is chosen such that
where J Dm F(X) denotes the Jacobian matrix to the m- c 0Q(x Ω(i i)) ≥ p(x Ω(i i)|x Γ(i i),r(i),Θt) for all i = 1,...,n.
However, as stated earlier the posterior distribution is
thpower. (13)isguaranteedtoconvergewhenF(X)is
not readily available. Thus, from Bayes rule, we have
contractive(Hall,2013). Inpractice,(13)iscomputed
by truncating the number of terms in the summation
p(x(i),x(i)|θt)p(r(i)|x(i),x(i),ϕt)
to a finite number. This, however, introduces bias in p(x(i)|x(i),r(i),Θt)= Ωi Γi Ωi Γi ,
estimating the log-determinant of the Jacobian. In Ωi Γi p(x(i),r(i)|Θt)
ordertocircumventthisissuewefollowthestepstaken
Γi
by Chen et al. (2019). The power series expansion where the denominator p(x(i),r(i)|Θt) can be evalu-
is truncated at a random cut-off N ∼ pN(N), where
ated using fully observed
daΓ ti
a. The first term in the
pN is a probability distribution over the set of natural
numeratorcanbecomputedefficiently,asdiscussedin
numbers N. Each term in the finite power series is
Section 3.2.1. The second term is addressed in Sec-
then re-weighted to obtain the following estimator:
tion 3.2.3. Before that, we explain how these calcu-
log(cid:12) (cid:12)detJ (id−DF)(X)(cid:12) (cid:12)=−E N(cid:34) (cid:88)N mTr ·(cid:8) PJ NDm (F ℓ( ≥X m)(cid:9) )(cid:35) ,(14) l ia mt pio un ts ats ii om np pli rf oy ceu dn ud re er ism so ur me mre as rt ir zi ec dtiv ine Am lo gd oe ril ts h. mT 1h .e
m=1 Linear SEMs with MAR mechanisms. Whenthe
where PN is the cumulative density function of pN. underlyingSEMislinearwithadditiveGaussiannoise
Gradient calls are still required in the order of K. We and the missingness mechanism is MAR, it is possible
can reduce this further by using the Hutchinson trace to sample from the exact posterior. Under this setup,
estimator (Hutchinson, 1989), where W ∼N(0,I): we can ignore the missingness mechanism and solely
focus on the target law. Consider X = B⊤X + ϵ,
(cid:110) (cid:111) (cid:104) (cid:105)
Tr J Dm F(X) =E W W⊤J Dm F(X)W , (15) where ϵ ∼ N(0,Λ−1), Λ = diag(1/σ 12,...,1/σ K2 ) andMissNODAG: Differentiable Cyclic Causal Graph Learning from Incomplete Data
Algorithm 1 Impute-Rejection classisknownastheblock-parallelMNAR model(Mo-
Require: MinibatchdataB={y(i),r(i),s(i)}nB ,with han et al., 2013; Nabi et al., 2022). Figure 1(a) illus-
i=1 trates this MNAR mechanism with K =3 variables.
sampling distribution Q(x).
Ensure: Imputed data B˜={x(i),r(i),s(i)}nB . Under this MNAR class, p(R|X,ϕ) in (18) is identi-
(cid:98) i=1
1: for i=1 to n B do fied as a function of the observed data by identify-
2: Sample x˜( Ωi i) ∼Q(x Ωi). ingeachconditionalfactorviap(R k|pa Gm(R k),R −∗
k
=
3: Pick u∼U[0,1]. 1,ϕ ), where R∗ denotes the missingness indicators
k −k
p(x˜(i),y(i)|θt)p(r(i)|x˜(i),y(i),ϕt) for pa (R ). Each conditional factor is modeled us-
4: if u≤ Ωi Γi Ωi Γi then Gm k
c0Q(x˜( Ωi i)) ing the expit function, with ϕ
k
={w k,z k}:
5 6:
:
endA ic fcept sample: x (cid:98)( Ωi i) =x˜( Ωi i); x (cid:98)( Γi i) =y Γ(i i). p(cid:0) R
k
=0|pa Gm(R k),ϕ k(cid:1) =expit(cid:0) w k⊤X+z k(cid:1) . (19)
7: end for Maximizing the M-step with respect to ϕ reduces to
return B˜={x (cid:98)(i),r(i),s(i)}n i=B 1 solving a sparsity-regularized logistic regression, with
our choice of
R(ϕ)=(cid:80)K
∥w ∥ .
k=1 k 1
Block-parallelMNARiswell-suitedformodelingmiss-
B is the weighted adjacency matrix of G. X is also
ingnessmechanismsincross-sectionalstudies,surveys,
Gaussian distributed, with X ∼N(0,Λ−1) where
X orretrospectivestudies. Forexample,considerastudy
Λ =(I−B)Λ(I−B⊤). (16) analyzing the relationship between smoking (X 1), tar
X
accumulation in the lungs (X ), and a bronchitis di-
2
Thus, based on the properties of the Gaussian dis- agnosis (X 3), where missing entries in the data are
tribution, the posterior distribution of the missing indicatedbyR 1,R 2,andR 3. Undertheblock-parallel
nodes, conditioned on the observed nodes, also fol- MNAR model, X 2 →R 1 ←X 3 suggests that whether
lows a Gaussian distribution (Koller and Friedman, an individual’s smoking status is recorded depends
2009), i.e., x(i)|x(i) ∼ N(µ˜(i),Λ˜(i)) such that Λ˜(i) = on their tar accumulation and bronchitis diagnosis.
Ωi Γi This could happen when smoking history is only in-
[Λ ] , and η˜(i) =−Λ˜(i)x(i), where η˜(i) =Λ˜(i)µ˜(i).
X Ωi,Ωi Γi quired after a suspected diagnosis of tar accumulation
The conditional expectation in (7) is estimated by and bronchitis. Similarly, X 3 → R 2 implies that a
imputing the missing values for each data instance suspected bronchitis diagnosis might prompt inquiries
by sampling from the Gaussian distribution described intotaraccumulation. Additionally,smokersaremore
above. In an interventional setting, X = (X ,X ) likelytobetestedforbothtaraccumulationandbron-
I O
with X I ∼N(0,I), (16) can be modified as follows chitis, represented by R 2 ← X 1 → R 3, and detecting
hightarlevelsinthelungsmaytriggerbronchitistest-
Λ X,I=(I−DB)(DΛ−1D+(I−D))−1(I−DB⊤), (17) ing, reflected by X 2 → R 3. Another use case of the
block-parallel model is discussed in the context of the
where Λ denotes the inverse covariance matrix of prisoner’s dilemma by Nabi et al. (2022). This model
X,I
X; see Appendix B.2 for a proof. The rest of the im- was also explored in the study by Tu et al. (2019).
putationprocedureremainsthesame,exceptforusing
Extensions to other MNAR models. Identifi-
Λ in place of Λ in the Gaussian distribution.
X,I X ability is critical for the EM algorithm since, with-
(cid:0) (cid:1) out it, multiple parameter sets may yield the same
3.2.3 Missingness Mechanism, logp R|X,ϕ
likelihood, preventing EM from converging. Beyond
block-parallel MNAR, other identifiable MNAR mod-
In order to compute the log-likelihood in the E-step,
we also need to compute
logp(cid:0) r(i)|x(i),x(i),ϕ(cid:1)
, which
els, such as those with no colluders with an exam-
Ωi Γi ple provided in Figure 1(b) (Nabi et al., 2020) (see
according to m-graphs can be factorized as
Section 2), offer viable directions for extending our
(cid:89)K
(cid:0) (cid:1)
framework. To learn the full law p(X,R), additional
p(R|X,ϕ)= p R |pa (R ),ϕ . (18)
k Gm k k constraints are required to ensure that p(R|X) forms
k=1 a DAG. Specifically, we can introduce a constraint on
In developing MissNODAG, we focus on a class of the adjacency matrix W ∈ RK×K representing edges
MNAR models, where for any R ∈ R, pa (R ) ⊆ between missingness indicators, restricting the search
k Gm k
X \X , i.e., there are no self-censoring edges and no spacetoDAGsusingTr(eW⊙W)−K =0(Zhengetal.,
k
connections between the missingness indicators in R. 2018). However, this alone does not eliminate collud-
This implies R ⊥X ,R |pa (R ), for all R ∈R, ers, which will require further refinement, a topic we
k k −k Gm k k
based on the Markov properties (Pearl, 2009b). Here, leave for future work. If the goal is to learn only the
R denotes the set R excluding R . This MNAR target law and an identifiable MNAR mechanism is
−k kMuralikrishnna G. Sethuraman, Razieh Nabi, Faramarz Fekri
Linear SEM (ER-1) Linear SEM (ER-2) Nonlinear SEM (ER-1) Nonlinear SEM (ER-2)
4 3 10
6
3
2 8
2 4
1
1 6
2
0 0
0.1 0.2 0.3 0.4 0.5 0.1 0.2 0.3 0.4 0.5 0.1 0.2 0.3 0.4 0.5 0.1 0.2 0.3 0.4 0.5
Av. Missing Prob Av. Missing Prob Av. Missing Prob Av. Missing Prob
nodags+clean missnodag missdag missforest optransport nodags+clean missnodag missdag missforest optransport
(a) Linear SEM (b) Nonlinear SEM
Figure2: Comparisonofresultsforlearningcausalgraphstructure(targetlaw)underlinear(left)andnonlinear
(right) SEMS with MNAR missingness mechanism (K =10). Each missingness indicator has at most 3 parents.
fixed,theEMalgorithmcanbesimplified. Inthiscase, single-node interventions with n = 500 per interven-
after learning the parameters of the fixed missingness tion. Each intervened node was sampled from a stan-
mechanism (e.g., through weighted estimating equa- dard normal distribution, and ϵ ∼ N(0,0.252I). We
tions (Seaman and White, 2013; Bhattacharya et al., assumed the intervened node was never missing.
2020)),theoptimizationin(8)focusesonlyonthetar-
WeevaluatedMissNODAGandthebaselinesusingthe
getlawparameters,θ,simplifyingtheprocessevenfor
Structural Hamming Distance (SHD), which counts
non-block-parallel MNAR models.
thenumberofoperations(deletion,addition,reversal)
needed to match the estimated graph to the true one.
4 EXPERIMENTS All models were trained for 100 training epochs, with
missingness probabilities varying from 0.1 to 0.5, and
experimentsrepeatedon10randomgraphs. Figure2a
The code for MissNoDAG is available at the reposi-
shows the SHD between the true and the estimated
tory: https://github.com/muralikgs/missnodag.
graph, focusing on the target law recovery (edges be-
Using synthetic data, we compared MissNODAG to tween X ’s). MissNODAG outperformed all baselines
k
MissDAG (missdag) (Gao et al., 2022), an EM- forER-1graphs. Whenthemissingrateisbelow0.4,it
based causal discovery method limited to DAGs matched the performance of NODAGS-Flow on com-
and MAR missingness. We also tested state-of-the- plete data (assuming no missingness). A similar trend
art imputation methods, including optimal transport was observed for ER-2 graphs, further demonstrating
(optransport) (Muzellec et al., 2020), and MissFor- MissNODAG’sefficacy. Figure2alsoshowstherecov-
est (missforest) (Stekhoven and Bu¨hlmann, 2012), ery performance of NODAGS-Flow on complete data
followed by causal graph learning from the imputed (nodags+clean) for comparison.
data using NODAGS-Flow (Sethuraman et al., 2023),
Nonlinear SEM with MNAR Mechanism. The
see Appendix E for details on the implementation of
causal function for the nonlinear SEM is defined as
MissNODAG and the baselines.
F(X)=tanh(W⊤X).Non-zeroentriesofW ∈RK×K
Inallexperiments,wegeneratedcyclicdirectedgraphs aresampledfromauniformdistributionandscaledto
withK =10nodesusingtheErd˝os-R´enyi(ER)model, make F contractive, with a Lipschitz constant of 0.9.
varying edge density between 1 and 2 (ER-1 and ER- The dataset includes single-node interventions with
2). Self-loops were removed, and the m-graph de- sample size n=500, and noise terms are Gaussian.
scribed in Section 3.2.3 was used to create partially
We used neural networks with a single hidden layer
missing data. MissNODAG’s initial estimate for miss-
andtanhactivationtolearnthecausalfunctionF(X).
ingness parameters was obtained via logistic regres-
The objective function (8) was optimized using Adam
sion on complete cases in the data set (samples with
optimizer (Kingma and Ba, 2014). All models were
no missing values), then refined through the EM pro-
trained for 100 epochs, with the average missingness
cedure. We evaluated MissNODAG against baselines
probability ranging from 0.1 to 0.5. Figure 2b shows
on linear and nonlinear SEMs with MNAR and MAR
themeanSHDover10trails,whereMissNODAGcon-
mechanisms.
sistently outperforms all baselines at both edge densi-
Linear SEM with MNAR Mechanism. The edge ties, with the performance gap widening as edge den-
coefficients of the causal graph were sampled from a sity increases.
uniform distribution over (−0.6,−0.25) ∪ (0.25,0.6),
Learning Missingness Mechanism. In addition to
and rescaled for contractivity. The data consisted of
DHS DHSMissNODAG: Differentiable Cyclic Causal Graph Learning from Incomplete Data
# Samples: 5000 # Samples: 10000 Linear SEM (ER-1) Linear SEM (ER-2)
6 4 2.5
10.0 ER-1 ER-1
ER-2 ER-2 3 2.0
7.5 4 1.5
2
1.0
5.0 2 1 0.5
2.5 0 0.0
0 0.1 0.2 0.3 0.4 0.5 0.1 0.2 0.3 0.4 0.5
Av. Missing Prob Av. Missing Prob
0.1 0.2 0.3 0.4 0.5 0.1 0.2 0.3 0.4 0.5
Av. Missing Prob Av. Missing Prob nodags+clean missnodag missdag missforest optransport
Figure 3: Comparison of results for learning the miss- Figure 4: Comparison of results for learning causal
ingness mechanism (K =10). graph structure (target law) under linear SEMs with
MAR missingness mechanism (K =10).
5 DISCUSSION
learning the target law, MissNODAG also learns the
In this work, we proposed MissNODAG, a novel
underlyingmissingnessmechanism, asdetailedinSec-
differentiable causal discovery framework capable of
tion 3.2.3. Figure 3 shows the error in recovering the
learning nonlinear cyclic causal graphs along with
m-graphedgescorrespondingtothemissingnessmech-
the missingness mechanism from incomplete interven-
anism (i.e., X to R edges), using SHD as the metric.
tional data. The framework employs an expectation-
We evaluated the performance on training sets with
maximization algorithm that alternates between im-
5000 and 10000 samples. Each missingness indicator
puting missing values and optimizing model parame-
is restricted to have at most 3 parents. As seen in
ters. We demonstrated how imputation can be effi-
Figure 3, when the average the missing probability is
ciently achieved using rejection sampling, and in the
low (<0.3), MissNODAG perfectly recovers the miss-
case of linear SEMs with MAR missingness, by di-
ingness mechanism with 10000 samples, while SHD is
rectly sampling from the posterior distribution. One
below 2.5 with 5000 samples. Recovery performance
of the key strengths of MissNODAG is its ability to
decreases as the missingness probability increases.
handlecyclicdirectedgraphsandMNARmissingness,
Roles of Sample Size and Missingness Spar- asignificant advancementovermethodsthattypically
sity. Weconductedadditionalsimulationstoexamine focus on DAGs and MAR mechanisms.
MissNODAG’s performance on target law recovery as
Future research directions include: (1) incorporating
functions of sample size and missingness mechanism
realistic measurement noise models in the SEMs to
sparsity (controlled by the parent set cardinality of
enhancerobustnessinreal-worlddatasets, asexplored
the missingness indicators). Due to space constraints,
in linear DAG models by Saeed et al. (2020); (2) scal-
we defer these analyses to Appendices D.1 and D.2.
ing the current framework to larger graphs using low-
Linear SEM and MAR Mechanism. For MAR rank models and variational inference techniques, as
missingness and linear SEMs, missing values can be explored in acyclic structures by Lopez et al. (2022);
imputedbysamplingfromtheexactposteriordistribu- (3) allowing for unobserved confounders within the
tionofmissingvariablesgiventheobservedones,asde- modeling assumptions, as was explored in the case
tailedinSection3.2.2. ResultsaresummarizedinFig- of latent DAGs with complete observations by Bhat-
ure 4. For ER-1, MissNODAG outperforms all base- tacharya et al. (2021); and (4) generalizing our frame-
linesandmatchesNODAGS-Flowoncleandatawhen work to broader classes of identifiable MNAR models,
the missing probability is below 0.4. For ER-2, over- while tackling the challenges of non-identifiability in
all performance slightly decreases, but MissNODAG the full or target laws, as explored in DAG models by
continues to outperform all baselines. Nabi and Bhattacharya (2023) and Guo et al. (2023).
DAG Learning from Observational Data. We
References
conducted experiments on learning DAGs from par-
tially observed observational data with MNAR miss- Am´endola, C., Dettling, P., Drton, M., Onori, F., and
ingness. See Appendix D.3 for details. Wu, J. (2020). Structure learning for cyclic linear
causalmodels. InConference on Uncertainty in Ar-
Real-data Application. WetestedMissNODAGon
tificial Intelligence, pages 999–1008. PMLR.
a gene regulatory network with gene expression data
from genetic interventions (Frangieh et al., 2021). See Behrmann, J., Grathwohl, W., Chen, R. T., Duve-
Appendix D.4 for details. naud, D., and Jacobsen, J.-H. (2019). Invertible
DHS
DHSMuralikrishnna G. Sethuraman, Razieh Nabi, Faramarz Fekri
residual networks. In International Conference on with continuous additive noise models. Advances
Machine Learning, pages 573–582. PMLR. inNeuralInformationProcessingSystems,35:5024–
5038.
Bhattacharya, R., Nabi, R., Shpitser, I., and Robins,
J. M. (2020). Identification in missing data mod- Getzen, E., Ungar, L., Mowery, D., Jiang, X., and
els represented by directed acyclic graphs. In Un- Long, Q. (2023). Mining for equitable health: As-
certainty in artificial intelligence, pages 1149–1158. sessing the impact of missing data in electronic
PMLR. health records. Journal of biomedical informatics,
139:104269.
Bhattacharya, R., Nagarajan, T., Malinsky, D., and
Shpitser, I. (2021). Differentiable causal discovery Ghassami, A., Yang, A., Kiyavash, N., and Zhang,
under unmeasured confounding. In International K. (2020). Characterizing distribution equivalence
Conference on Artificial Intelligence and Statistics, andstructurelearningforcyclicandacyclicdirected
pages 2314–2322. PMLR. graphs. In International Conference on Machine
Learning, pages 3494–3504. PMLR.
Bollen, K. A. (1989). Structural equations with latent
variables, volume 210. John Wiley & Sons. Guo,A.,Zhao,J.,andNabi,R.(2023).Sufficientiden-
tification conditions and semiparametric estimation
Carter, R. L. (2006). Solutions for missing data in
under missing not at random mechanisms. In Un-
structural equation modeling. Research & Practice
certainty in Artificial Intelligence, pages 777–787.
in Assessment, 1:4–7.
PMLR.
Chen, L. S., Prentice, R. L., and Wang, P. (2014). A
Hall,B.C.(2013). LieGroups, LieAlgebras, andRep-
penalized em algorithm incorporating missing data
resentations, pages 333–366. Springer New York,
mechanism for gaussian parameter estimation. Bio-
New York, NY.
metrics, 70(2):312–322.
Hauser, A. and Bu¨hlmann, P. (2012). Characteriza-
Chen, R.T., Behrmann, J., Duvenaud, D.K., andJa-
tion and greedy learning of interventional markov
cobsen, J.-H. (2019). Residual flows for invertible
equivalence classes of directed acyclic graphs. The
generative modeling. Advances in Neural Informa-
Journal of Machine Learning Research, 13(1):2409–
tion Processing Systems, 32.
2464.
Drton, M., Fox, C., and Wang, Y. S. (2019). Com-
Heinze-Deml, C., Peters, J., and Meinshausen, N.
putation of maximum likelihood estimates in cyclic
(2018). Invariant causal prediction for nonlinear
structural equation models. The Annals of Statis-
models. Journal of Causal Inference, 6(2).
tics, 47(2):663 – 690.
Huetter, J.-C. and Rigollet, P. (2020). Estimation
Frangieh, C. J., Melms, J. C., Thakore, P. I., Geiger-
rates for sparse linear cyclic causal models. In Pe-
Schuller, K. R., Ho, P., Luoma, A. M., Cleary,
ters, J. and Sontag, D., editors, Proceedings of the
B., Jerby-Arnon, L., Malu, S., Cuoco, M. S.,
36th Conference on Uncertainty in Artificial Intelli-
et al. (2021). Multimodal pooled Perturb-CITE-seq
gence (UAI), volume 124 of Proceedings of Machine
screensinpatientmodelsdefinemechanismsofcan-
Learning Research, pages 1169–1178. PMLR.
cerimmuneevasion.Naturegenetics,53(3):332–341.
Hutchinson, M. F. (1989). A stochastic estimator
Freimer, J. W., Shaked, O., Naqvi, S., Sinnott-
of the trace of the influence matrix for Laplacian
Armstrong, N., Kathiria, A., Garrido, C. M., Chen,
smoothing splines. Communications in Statistics-
A. F., Cortez, J. T., Greenleaf, W. J., Pritchard,
Simulation and Computation, 18(3):1059–1076.
J. K., and Marson, A. (2022). Systematic discov-
ery and perturbation of regulatory genes in human Hyttinen, A., Eberhardt, F., and Hoyer, P. O. (2012).
Tcellsrevealsthearchitectureofimmunenetworks. Learninglinearcycliccausalmodelswithlatentvari-
Nature Genetics, pages 1–12. ables. The Journal of Machine Learning Research,
13(1):3387–3439.
Friedman, N. (1998). The bayesian structural em al-
gorithm. In Conference on Uncertainty in Artificial Imbens, G. W. and Rubin, D. B. (2015). Causal in-
Intelligence. ference in statistics, social, and biomedical sciences.
Cambridge University Press.
Gain, A. and Shpitser, I. (2018). Structure learn-
ing under missing data. In International confer- Jang, E., Gu, S., and Poole, B. (2016). Categori-
ence on probabilistic graphical models, pages 121– calreparameterizationwithGumbel-Softmax.arXiv
132. PMLR. preprint arXiv:1611.01144.
Gao, E., Ng, I., Gong, M., Shen, L., Huang, W., Liu, Kingma, D. P. and Ba, J. (2014). Adam: A
T., Zhang, K., and Bondell, H. (2022). Missdag: method for stochastic optimization. arXiv preprint
Causal discovery in the presence of missing data arXiv:1412.6980.MissNODAG: Differentiable Cyclic Causal Graph Learning from Incomplete Data
Koller, D. and Friedman, N. (2009). Probabilistic transport. In International Conference on Machine
graphical models: principles and techniques. MIT Learning, pages 7130–7140. PMLR.
press.
Nabi, R. and Bhattacharya, R. (2023). On testability
Kyono, T., Zhang, Y., Bellot, A., and van der Schaar, and goodness of fit tests in missing data models. In
M. (2021). Miracle: Causally-aware imputation via Uncertainty in Artificial Intelligence, pages 1467–
learning missing data mechanisms. Advances in 1477. PMLR.
Neural Information Processing Systems, 34:23806–
Nabi, R., Bhattacharya, R., and Shpitser, I. (2020).
23817.
Fulllawidentificationingraphicalmodelsofmissing
Lacerda, G., Spirtes, P., Ramsey, J., and Hoyer, P. O. data: Completenessresults. InInternationalconfer-
(2008). Discovering cyclic causal models by inde- enceonmachinelearning,pages7153–7163.PMLR.
pendent components analysis. In Proceedings of the Nabi, R., Bhattacharya, R., Shpitser, I., and Robins,
Twenty-Fourth Conference on Uncertainty in Arti- J. (2022). Causal and counterfactual views of miss-
ficial Intelligence,UAI’08,page366–374,Arlington, ing data models. arXiv preprint arXiv:2210.05558.
Virginia, USA. AUAI Press.
Ng, I., Ghassami, A., and Zhang, K. (2020). On the
Lee, H.-C., Danieletto, M., Miotto, R., Cherng, S. T., role of sparsity and DAG constraints for learning
and Dudley, J. T. (2019). Scaling structural learn- linear dags. Advances in Neural Information Pro-
ing with NO-BEARS to infer causal transcriptome cessing Systems, 33:17943–17954.
networks. In Pacific Symposium on Biocomputing
Ng, I., Zhu, S., Fang, Z., Li, H., Chen, Z., and Wang,
2020, pages 391–402. World Scientific.
J. (2022). Masked gradient-based causal structure
Li, S. C.-X., Jiang, B., and Marlin, B. (2019). Learn- learning. In Proceedings of the 2022 SIAM Inter-
ing from incomplete data with generative adversar- national Conference on Data Mining (SDM), pages
ialnetworks. InInternational Conference on Learn- 424–432. SIAM.
ing Representations.
Pearl, J. (2009a). Causality. Cambridge University
Little, R. J. and Rubin, D. B. (2019). Statistical anal- Press, 2 edition.
ysis with missing data, volume 793. John Wiley &
Pearl, J. (2009b). Causality: Models, Reasoning, and
Sons.
Inference. Cambridge University Press, 2 edition.
Lopez, R., Hu¨tter, J.-C., Pritchard, J., and Regev, A.
Richardson, T. (1996). A discovery algorithm for di-
(2022). Large-scale differentiable causal discovery
rected cyclic graphs. In Proceedings of the Twelfth
of factor graphs. Advances in Neural Information
international conference on Uncertainty in artificial
Processing Systems, 35:19290–19303.
intelligence, pages 454–461.
Luo,Y.,Cai,X.,Zhang,Y.,Xu,J.,etal.(2018). Mul-
Rudin, W. (1953). Principles of Mathematical Analy-
tivariate time series imputation with generative ad-
sis. McGraw-Hill Book Company, Inc., New York-
versarial networks. Advances in neural information
Toronto-London.
processing systems, 31.
Sachs, K., Perez, O., Pe’er, D., Lauffenburger, D. A.,
Meek, C. (1997). Graphical Models: Selecting causal and Nolan, G. P. (2005). Causal protein-signaling
and statistical models. PhD thesis, Carnegie Mellon networks derived from multiparameter single-cell
University. data. Science, 308(5721):523–529.
Mohan, K. and Pearl, J. (2021). Graphical models for Saeed, B., Belyaeva, A., Wang, Y., and Uhler, C.
processing missing data. Journal of the American (2020). Anchored causal inference in the presence
Statistical Association, 116(534):1023–1037. ofmeasurementerror. InConference on uncertainty
Mohan, K., Pearl, J., and Tian, J. (2013). Graphical in artificial intelligence, pages 619–628. PMLR.
models for inference with missing data. In Burges, Seaman, S. R. and White, I. R. (2013). Review of
C., Bottou, L., Welling, M., Ghahramani, Z., and inverse probability weighting for dealing with miss-
Weinberger, K., editors, Advances in Neural Infor- ing data. Statistical methods in medical research,
mation Processing Systems, volume 26. Curran As- 22(3):278–295.
sociates, Inc.
Segal, E., Pe’er, D., Regev, A., Koller, D., Friedman,
Mooij, J. M. and Heskes, T. (2013). Cyclic causal N., and Jaakkola, T. (2005). Learning module net-
discovery from continuous equilibrium data. In Un- works. Journal of Machine Learning Research,6(4).
certainty in Artificial Intelligence.
Sethuraman, M. G., Lopez, R., Mohan, R., Fekri, F.,
Muzellec, B., Josse, J., Boyer, C., and Cuturi, M. Biancalani, T., and Huetter, J.-C. (2023). Nodags-
(2020). Missing data imputation using optimal flow: Nonlinear cyclic causal structure learning. InMuralikrishnna G. Sethuraman, Razieh Nabi, Faramarz Fekri
Proceedings of The 26th International Conference interventions. Advances in Neural Information Pro-
on Artificial Intelligence and Statistics, volume 206 cessing Systems, 30.
ofProceedings of Machine Learning Research, pages
White, I. R., Royston, P., and Wood, A. M. (2011).
6371–6387. PMLR.
Multipleimputationusingchainedequations: issues
Singh, M. (1997). Learning bayesian networks from and guidance for practice. Statistics in medicine,
incomplete data. AAAI/IAAI, 1001:534–539. 30(4):377–399.
Solus, L., Wang, Y., Matejovicova, L., and Uhler, C. Wu, C. F. J. (1983). On the Convergence Proper-
(2017). Consistency guarantees for permutation- ties of the EM Algorithm. The Annals of Statistics,
based causal inference algorithms. arXiv preprint 11(1):95 – 103.
arXiv:1702.03530. Yu, Y., Chen, J., Gao, T., and Yu, M. (2019). DAG-
Spirtes, P., Glymour, C. N., Scheines, R., and Hecker- GNN: DAG structure learning with graph neural
man, D. (2000). Causation, prediction, and search. networks. In International Conference on Machine
MIT press. Learning, pages 7154–7163. PMLR.
Zhang, B., Gaiteri, C., Bodea, L.-G., Wang, Z., McEl-
Stekhoven, D. J. and Bu¨hlmann, P. (2012).
wee, J., Podtelezhnikov, A. A., Zhang, C., Xie, T.,
Missforest—non-parametric missing value impu-
Tran,L.,andDobrin,R.(2013). Integratedsystems
tation for mixed-type data. Bioinformatics,
approach identifies genetic nodes and networks in
28(1):112–118.
late-onsetAlzheimer’sdisease.Cell,153(3):707–720.
Strobl, E. V., Visweswaran, S., and Spirtes, P. L.
Zheng, X., Aragam, B., Ravikumar, P. K., and Xing,
(2018). Fastcausalinferencewithnon-randommiss-
E. P. (2018). DAGs with NO TEARS: Continu-
ingness by test-wise deletion. International journal
ous optimization for structure learning. In Bengio,
of data science and analytics, 6:47–62.
S., Wallach, H., Larochelle, H., Grauman, K., Cesa-
Sulik, J. J., Newlands, N. K., and Long, D. S. (2017). Bianchi, N., and Garnett, R., editors, Advances in
Encoding dependence in bayesian causal networks. Neural Information Processing Systems, volume 31.
Frontiers in Environmental Science, 4:84.
Zheng, X., Dan, C., Aragam, B., Ravikumar, P., and
Triantafillou, S. and Tsamardinos, I. (2015). Xing, E. (2020). Learning sparse nonparametric
Constraint-based causal discovery from multi- DAGs. In Chiappa, S. and Calandra, R., editors,
ple interventions over overlapping variable sets. Proceedings of the Twenty Third International Con-
The Journal of Machine Learning Research, ference on Artificial Intelligence and Statistics, vol-
16(1):2147–2205. ume 108, pages 3414–3425.
Tsamardinos, I., Brown, L. E., and Aliferis, C. F.
(2006). The max-min hill-climbing bayesian net-
work structure learning algorithm. Machine learn-
ing, 65(1):31–78.
Tu, R., Zhang, C., Ackermann, P., Mohan, K., Kjell-
str¨om, H., and Zhang, K. (2019). Causal discovery
in the presence of missing data. In The 22nd In-
ternationalConferenceonArtificialIntelligenceand
Statistics, pages 1762–1770. PMLR.
Van den Broeck, G., Mohan, K., Choi, A., Dar-
wiche, A., and Pearl, J. (2015). Efficient algorithms
for bayesian network parameter learning from in-
complete data. In Proceedings of the Thirty-First
ConferenceonUncertaintyinArtificialIntelligence,
UAI’15, page 161–170, Arlington, Virginia, USA.
AUAI Press.
Wang,Y.,Menkovski,V.,Wang,H.,Du,X.,andPech-
enizkiy, M. (2020). Causal discovery from incom-
pletedata: adeeplearningapproach. arXivpreprint
arXiv:2001.05343.
Wang, Y., Solus, L., Yang, K., and Uhler, C. (2017).
Permutation-basedcausalinferencealgorithmswithMissNODAG: Differentiable Cyclic Causal Graph Learning from Incomplete Data
Supplementary Materials
The appendix is structured as follows. Appendix A offers a summary of the notations used throughout the
manuscriptforeaseofreference. Appendix Bcontainsalltheproofs. Appendix Cdetailsmodificationstothe
MissNODAG framework necessary when the search space is confined to DAGs. Appendix D provides details
on three sets of additional simulations and a real-data application. Appendix E provides additional details on
the implementation of the baselines and the MissNODAG framework.
A GLOSSARY
A comprehensive list of notations used in the manuscript is provided in Table 1.
Table 1: Glossary of terms and notations
Symbol Definition Symbol Definition
X, x Variables, values G,G Graph, m-graph
m
ϵ Exogenous noise terms k Graph node index variable
R Missingness indicators K Total nodes in G
Y Coarsened version of X pa (X ) Parent set of X in G
G k k
S Intervention indicator vector F,f SEM functions
k
X Set of intervened nodes id Identity map
I
X Set of non-intervened nodes (id−F) Function mapping X to ϵ
O
p (ϵ) Exogenousnoisedensityfunction J (X) Jacobian matrix of F at X
ϵ F
p (X),p(X,R) Target, full laws I K×K Identity matrix
X
p(Y|X,R) Coarsening mechanism D Matrix of intervention masks
p(R|X) Missingness mechanism B Weighted adjacency matrix of
linear SEM
p(M|θ) Gumbel-Softmaxdistributionfor M DependencymaskforSEMfunc-
sampling M tion F
Θ=(θ,ϕ) Parameters of p (X), p(R|X) Λ Inverse covariance matrix
X
Θt Parameters at EM t-th iteration D Interventionmaskcorresponding
i
to i-th sample
Γ,Ω Index sets for observed, missing i Sample index variable
nodes
w ,z Parameters of p(R |pa (R )) n Total sample size
k k k Gm k
(y(i),r(i),s(i)) i-th sample n Mini batch size in each EM
B
A⊙B Hadamard product of A and B m Power series expansion index for
log-determinant of Jacobian
∥·∥ L1 norm N Number of power series terms
1
Q(·) Proposal distribution for rejec- W Gaussian randon variable used
tion sampling for computing trace of Jacobian
R(·) Sparsity inducing regularizer eA Matrix exponent of AMuralikrishnna G. Sethuraman, Razieh Nabi, Faramarz Fekri
B PROOFS
B.1 Joint Density of Variables X: Target Law
Consider the structural equation model X = F(X)+ϵ, which implies X = (id−F)−1(ϵ). Using the properties
of probability density functions, the joint distribution of X =(X ,...,X ) is given by,
1 K
(cid:12) (cid:12)
p (X)=p (cid:0) (id−F)(X)(cid:1)(cid:12)detJ (X)(cid:12), (20)
X ϵ (cid:12) (id−F) (cid:12)
where p (ϵ) is the probability density function of the exogenous noise vector ϵ. Under an interventional setting
ϵ
(X ,X ), all incoming edges to the nodes in X are removed, leading to the following structural equations:
I O I

X˜ if X ∈X
X = k k I
k
f (cid:0) pa (X )(cid:1) if X ∈/ X
k G k k I
That is, X is set to a known value X˜ if it is intervened upon, and the structural equation remains unchanged
k k
when X is purely observed. The above equation can be written more concisely as follows:
k
(cid:16) (cid:17)
X =d · f (pa (X ))+ϵ +C , for k =1,...,K. (21)
k k k G k k k
where d = 1{X ∈/ X }, and 1{·} is the indicator function, and C = X˜ if X ∈ X , and C = 0 otherwise.
k k I k k k I k
Let D∈RK×K be a diagonal matrix such that D =d . Thus, (21) can now be combined for k =1,...,K to
kk k
obtain the following equation,
X =DF(X)+Dϵ+C, (22)
where F(X) is defined in Section 2 and C =(C ,...,C ). Thus we have, (id−DF)(X)=Dϵ+C, this implies
1 K
that X =(id−DF)−1(Dϵ+C). Let X ∼p (X ). By the probability rules, we can write (20) as:
I XI I
(cid:12) (cid:12)
p (X)=p (X ) p (ϵ ) (cid:12)detJ (X)(cid:12), (23)
X XI I ϵO O (cid:12) (id−DF) (cid:12)
where ϵ is the exogenous noise terms of variables in X .
O O
B.2 Precision Matrix under Interventions with MAR Missingness Mechanism
Consider the following linear SEM with Gaussian exogenous noise term,
X =B⊤X+ϵ, (24)
where B ∈ RK×K is the weighted adjacency matrix of G, and ϵ ∼ N(0,Λ−1), and Λ = diag(1/σ2,...,1/σ2 )
1 K
is the inverse covariance matrix of ϵ. Thus we have, X = (I −B⊤)−1ϵ. From the properties of a Gaussian
distribution, X also follows Gaussian distribution with zero mean and covariance matrix given by,
(cid:104) (cid:105)
E[XX⊤]=E (I−B⊤)−1ϵϵ⊤(I−B⊤)−⊤ =(I−B⊤)−1E[ϵϵ⊤](I−B⊤)−⊤ =(I−B⊤)−1Λ−1(I−B⊤)−⊤.
Therefore, the inverse covariance matrix of X is given by Λ = (I −B)Λ(I −B⊤). Given an interventional
X
setting X =(X ,X ), we can write (24) as:
I O
X =DB⊤X+Dϵ+C, (25)
where D is the intervention mask matrix, and C denotes the value of the intervened nodes. Hence, X =
(I−DB⊤)−1(Dϵ+C). Theintervenednodesaresampledfromstandardnormaldistribution,i.e.,X ∼N(0,I).
I
Again, X is Gaussian distributed with zero mean and covariance matrix given by,
E[XX⊤]=(I−DB⊤)−1E(cid:2) (Dϵ+C)(Dϵ+C)⊤(cid:3) (I−DB⊤)−⊤
=(I−DB⊤)−1(cid:0) DΛ−1D+(I−D)(cid:1) (I−DB⊤)−⊤.
Hence, the inverse covariance of X under the interventional setting is given by,
Λ =(I−DB)(cid:0) DΛ−1D+(I−D)(cid:1)−1 (I−DB⊤).
X,IMissNODAG: Differentiable Cyclic Causal Graph Learning from Incomplete Data
B.3 Convergence Analysis
Here we provide the convergence analysis of MissNODAG. Our analysis relies on the convergence of the EM
algorithm (Wu, 1983; Friedman, 1998). The crux of the analysis depends on establishing that the total log-
likelihood of the non-missing nodes in the data set either increases or stays the same in each iteration of the
algorithm. That is,
n n
(cid:88)
logp
(cid:0) x(i),r(i)|Θt+1(cid:1) ≥(cid:88)
logp
(cid:0) x(i),r(i)|Θt(cid:1)
. (26)
X Γi X Γi
i=1 i=1
To that end, note that
n n n
(cid:88) logp (x(i),r(i)|Θ)=(cid:88) logp (x(i)x(i),r(i)|Θ)−(cid:88) logp (x(i)|x(i),r(i),Θ).
X Γi X Γi Ωi X Ωi Γi
i=1 i=1 i=1
Taking expectation with respect x(i)|x(i),r(i) on both side, we get
Ωi Γi
n n
(cid:88) logp (x(i),r(i)|Θ)=(cid:88) E logp (x(i),r(i)|Θ)
i=1
X Γi
i=1
x( Ωi i)|x( Γi i),r(i),Θt X Γi
n n
=(cid:88) E logp (x(i)x(i)|Θ)−(cid:88) E logp (x(i)|x(i),Θ). (27)
i=1
x( Ωi i)|x( Γi i);Θt X Γi Ωi
i=1
x( Ωi i)|x( Γi i);Θt X Ωi Γi
(cid:124) (cid:123)(cid:122) (cid:125)
=Q(Θ|Θt)
The first term on the RHS in the above equation is nothing but Q(Θ|Θt). This is maximized in the M-step, i.e.,
Q(Θ|Θt+1)≥Q(Θ|Θt). On the other hand,
(cid:88) i=n 1E
x( Ωi i)|x( Γi
i),r(i);Θtlogp X
p
X(x (x( Ωi
(
Ωi) i| i)x |x( Γi i
(
Γ) i, i)r ,( ri () i, )Θ ,Θt+ t)1) =−D KL(cid:16) p X(x( Ωi i)|x( Γi i),r(i),Θt)(cid:13) (cid:13)p X(x Ω(i i)|x( Γi i),r(i),Θt+1)(cid:17) ≤0.
Thus,
n n
(cid:88) E logp (x(i)|x(i),r(i),Θt+1)≤(cid:88) E logp (x(i)|x(i),r(i),Θt). (28)
i=1
x( Ωi i)|x( Γi i),r(i);Θt X Ωi Γi
i=1
x( Ωi i)|x( Γi i),r(i);Θt X Ωi Γi
From combining (27) and (28), we can see that at the end of the M-step (26) is satisfied. Similar to the
previous results on EM convergence (Wu, 1983; Friedman, 1998), MissNODAG reaches a stationary point of the
optimization objective.
C LEARNING DAGS FROM INCOMPLETE OBSERVATIONAL DATA
Whenthecausalgraphassociatedwiththetargetlawp (X)isaDirectedAcyclicGraph(DAG),itissometimes
X
possible to recover the true causal structure from purely observational data. For example, when the exogenous
noisevariableϵisGaussiandistributedwithequalvariance,i.e.,ϵ∼N(0,σ2I),thentheDAGisidentifiablefrom
observational data. This, however, requires constraining the search space in the optimization problem defined
by (8) to the set of all DAGs. To achieve this, we leverage the trace-exponential constraint introduced by Zheng
et al. (2018). Let M ∼p(M|θ); by adding the following constraint to (8), we ensure that the resulting graph is
acyclic:
(cid:104) (cid:16) (cid:17) (cid:105)
E Tr eM⊙M −K =0, (29)
M∼p(M|θ)
where eA denotes the matrix exponential of A, and ⊙ signifies the Hadamard product (elementwise multiplica-
tion) of two matrices. In the current formulation, the SEM function F is constrained to be contractive. While
thisconditionguaranteesbijectivityof(id−F)forcyclicgraphs,itisoverlyrestrictiveforDAGs. Thus,inspired
by Sethuraman et al. (2023), we introduce a preconditioning term Λ to redefine F. This modification enablesMuralikrishnna G. Sethuraman, Razieh Nabi, Faramarz Fekri
the modeling of non-contractive parent-child dependencies while maintaining efficient computation of the log-
determinant of the Jacobian. For more details, refer to Sethuraman et al. (2023). Thus, the maximization
objective in (8) takes the following form:
(cid:104) (cid:16) (cid:17) (cid:105)
Θt+1 =argmax Q(Θ|Θt)−λ R(θ)−λ R(ϕ)+λ ·E Tr eM⊙M −K .
1 2 DAG M∼p(M|θ)
Θ
Following Ng et al. (2020), this optimization can be solved using stochastic gradient based methods. We present
results on learning DAGs from synthetic partially observed data in Appendix D.3.
D ADDITIONAL EXPERIMENTS
D.1 Target Law Recovery: Performance as a Function of Sample Size
We evaluated the performance of the target law recovery as a function of sample size. The average missingness
probabilitywassetto0.2,andeachmissingnessindicatorwasrestrictedtohaveatmostthreeparents. Thedata
was generated as described in Section 4. The results for the linear SEM are shown in Figure 5 and the results
for the nonlinear SEM are shown in Figure 6.
In the linear setting (Figure 5), the performance of all methods improves as the number of training samples
increases. MissNODAG performs comparably to NODAGS-Flow trained on complete data (no missing samples
in the data set), even with significantly fewer samples than the baseline models. A similar trend is seen in the
nonlinear case (Figure 6), where all models show improved results with larger sample sizes. For a lower edge
density (ER-1),the models perform similarly. However, with a higher edge density (ER-2), the performance gap
widens, particularly with smaller sample sizes.
Linear SEM (ER-1) Linear SEM (ER-2)
1.0 1.0
0.8 0.8
0.6 0.6
0.4 0.4
0.2 0.2
0.0 0.0
5000 10000 15000 20000 25000 5000 10000 15000 20000 25000
# samples # samples
nodags+clean missnodag missforest optransport
Figure 5: Results of target law recovery for linear SEM with varying training set sizes. The average missing
probability was set to 0.2, and each R has a parent set cardinality of 3.
k
Nonlinear SEM (ER-1) Nonlinear SEM (ER-2)
3.0 6.5
2.5 6.0
2.0 5.5
1.5
5.0
5000 10000 15000 20000 25000 5000 10000 15000 20000 25000
# samples # samples
nodags+clean missnodag missforest optransport
Figure6: Resultsoftargetlawrecoveryfornonlinear SEMwithvaryingtrainingsetsizes. Theaveragemissing
probability was set to 0.2, and each R has a parent set cardinality of 3.
k
D.2 Target Law Recovery: Performance as a Function of Cardinalities for pa (R )
Gm k
We also evaluated target law recovery performance as a function of the parent set cardinality of the missingness
indicators, which reflects the sparsity of the missingness mechanism. For this analysis, the training set size was
fixed at 5000 samples. Results for the linear and nonlinear SEMs are shown in Figures 7 and 8, respectively.
DHS
DHSMissNODAG: Differentiable Cyclic Causal Graph Learning from Incomplete Data
In the linear case (Figure 7), the target law graph is perfectly recovered when the missingness probability is
below 0.4. However, as the parent set cardinality increases, recovery performance declines. A similar trend is
observed for the nonlinear SEM (Figure 8), where larger parent set cardinality also results in a noticeable drop
in recovery performance.
Linear SEM (ER-1) Linear SEM (ER-2)
8
8
6
6
4
4
2 2
0 0
0.1 0.2 0.3 0.4 0.5 0.1 0.2 0.3 0.4 0.5
Av. Missing Prob Av. Missing Prob
|pa m(Rk)| 3 |pa m(Rk)| 4
Figure 7: Results of target law recovery in linear SEM as the parent set cardinality of each R is varied.
k
Nonlinear SEM (ER-1) Nonlinear SEM (ER-2)
8 14
12
6
10
4
8
2 6
0.1 0.2 0.3 0.4 0.5 0.1 0.2 0.3 0.4 0.5
Av. Missing Prob Av. Missing Prob
|pa m(Rk)| 3 |pa m(Rk)| 4
Figure 8: Results of target law recovery in nonlinear SEM as the parent set cardinality of each R is varied.
k
D.3 Target Law Recovery: Learning DAGs from Partially Observed Observational Data
Figures 9 and 10 present the results of learning DAGs from partially observed observational data. We followed
the same procedure described in section 4 to generate the data, with the additional constraint that the re-
sulting graphs are acyclic. The SEM functions used for both linear and nonlinear cases were designed to be
non-contractive. Details on the learning methodolgy for DAGs is described in Appendix C. In both settings,
MissNODAG outperforms the baselines, although its performance decreases compared to learning cyclic graphs
from interventional data. We attribute this drop to the increased optimization complexity introduced by the
DAG constraint.
Linear SEM (ER-1) Linear SEM (ER-2)
5
4
4
3
3
2
2 1
0.1 0.2 0.3 0.4 0.5 0.1 0.2 0.3 0.4 0.5
Av. Missing Prob Av. Missing Prob
nodags+clean missnodag missforest optransport
Figure 9: Results of target law recovery for linear SEM when the target factorizes according to a DAG, with
MNAR mechanism where R has a parent set cardinality of 3.
k
DHS
DHS
DHSMuralikrishnna G. Sethuraman, Razieh Nabi, Faramarz Fekri
Nonlinear SEM (ER-1) Nonlinear SEM (ER-2)
7.5
5
7.0
4
6.5
3 6.0
2 5.5
5.0
1
0.1 0.2 0.3 0.4 0.5 0.1 0.2 0.3 0.4 0.5
Av. Missing Prob Av. Missing Prob
nodags+clean missnodag missforest optransport
Figure 10: Results of target law recovery for nonlinear SEM when the target factorizes according to a DAG,
with MNAR mechanism where R has a parent set cardinality of 3.
k
D.4 Data Application: Gene Perturbation
Here we present an experiment focused on learning causal graph structure corresponding to a gene regulator
networkfromageneexpressiondatawithgeneticinterventions. Inparticular,wefocusonthePerturb-CITE-seq
dataset (Frangieh et al., 2021), a type of data set that allows one to study causal relations in gene networks
at an unprecedented scale. It contains gene expressions taken from 218,331 melanoma cells split into three cell
conditions: (i) control (57,627 cells), (ii) co-culture (73,114 cells), and (iii) interferon (INF)-γ (87,590 cells).
Due to the practical and computational constraints, of the ap- Table 2: List of genes chosen from Perturb-
proximately20,000genesinthegenome,werestrictourselvesto CITE-seq dataset (Frangieh et al., 2021).
asubsetof10genes,followingtheexperimentalsetupofSethu-
raman et al. (2023), summarized in Table 2. We then took
STAT1 B2M LGALS3 PTMA
all the single-node interventions corresponding to the 10 genes.
SSR2 CTPS1 TM4SF1 MRPL47
Each cell condition is treated as a separate data set over which
the models were trained separately. DNMT1 TMED10
Since the data set does not provide a ground truth causal graph, it is not possible to directly compare the
performance using SHD. Instead, we compare the performance of the causal discovery methods based on its
predictive performance over unseen interventions. To that end, we perform a 90-10 split on the three data sets.
Thesmallersetistreatedasthetestset, whichisthenusedforperformancecomparisonbetweenMissNODAGS
and the baselines. As a performance metric, we use the predicted negative log-likelihood (NLL) over the test set
after training the models for 100 epochs.
Cocult Control IFN-
1.34
1.450
1.38
1.425 1.32
1.400 1.36
1.30
1.375
1.34
1.28
1.350
0.1 0.2 0.3 0.4 0.5 0.1 0.2 0.3 0.4 0.5 0.1 0.2 0.3 0.4 0.5
Av. Missing Prob Av. Missing Prob Av. Missing Prob
nodags+clean missnodag missforest optransport
Figure 11: Predictive performance over unseen interventions on Perturb-CITE-seq Frangieh et al. (2021) data.
In all the three cell conditions, MissNODAG outperforms the baseline methods, with the difference being the
most significant when the average missingness probability is low. MissNODAG matches the performance of
NODAGS-Flow trained on clean data when the missingness probability is 0.1 and for the case of control cell
condition, the performance of MissNODAG is comparable to clean data untill the missingness probability is 0.2.
LLN
DHSMissNODAG: Differentiable Cyclic Causal Graph Learning from Incomplete Data
E IMPLEMENTATION DETAILS
In this section, we describe the implementation details of the MissNODAG framework and the baseline models
used for performance comparisons.
MissNODAG. We implemented our framework using the Pytorch library in Python and the code used in
running the experiments can be found in the codes folder within the supplementary materials. The code for the
proposed model can be found at https://github.com/muralikgs/missnodag.
StartingwithaninitializationofthemodelparametersΘ0,wealternatebetweentheE-stepandM-stepuntilthe
parameters converge. In the E-step, Algorithm 1 is used for imputing the missing data, followed by maximizing
the expected likelihood of the non-missing nodes in the M-step. We follow the same setup as Sethuraman et al.
(2023) for modeling the causal functions, i.e., neural networks (NN) along with dependency mask with entries
parameterized by Gumbel-softmax distribution, and for computing the log-determinant of the Jacobian, i.e.,
power series expansion followed by Hutchinson trace estimator. Poisson distribution is used for pN for sampling
the number of terms in the expansion to reduce the bias introduced while limiting the number of terms in the
power series expansion of log-determinant of the Jacobian, see section 3.2. The final objective in the M-step is
maximized using Adam optimizer (Kingma and Ba, 2014).
The learning rate in all our experiments was set to 10−2. The neural network models used in our experiments
contained one multi-layer perceptron layer. No nonlinearities were added to the neural networks for the linear
SEM experiments. We used tanh activation for the nonlinear SEM experiments and ReLU activation for the
experiments on the perturb-CITE-seq data set. The regularization constant λ was set to 10−2 for the synthetic
experiments and 10−3 for the perturb-CITE-seq experiments. All experiments were performed on NVIDIA
RTX6000 GPUs.
Baselines. For the baseline NODAGS-Flow, we modify the code base provided by Sethuraman et al. (2023) to
use the imputed samples for maximizing the likelihood. The hyperparameters of NODAGS-Flow was set to the
values described in the previous subsection.
MissForest imputation is performed using the publicly available python library missingpy. We use the code-
base provided by Muzellec et al. (2020) for optimal transport imputation, and the codebase provided by the
authors was used for MissDAG (Gao et al., 2022). The default parameters are used for Missforest, optimal
transport imputation, and MissDAG. The codes for all the baselines can be found inside the codes folder in the
supplementary materials.