Leveraging Graph Neural Networks and Multi-Agent
Reinforcement Learning for Inventory Control in Supply
Chains
Niki Kotecha, Antonio del Rio Chanona
aImperial College London, Department of Chemical Engineering, London, SW7 2AZ,
Abstract
Inventory control in modern supply chains has attracted significant at-
tention due to the increasing number of disruptive shocks and the challenges
posed by complex dynamics, uncertainties, and limited collaboration. Tradi-
tional methods, which often rely on static parameters, struggle to adapt to
changing environments. This paper proposes a Multi-Agent Reinforcement
Learning (MARL) framework with Graph Neural Networks (GNNs) for state
representation to address these limitations.
Our approach redefines the action space by parameterizing heuristic in-
ventory control policies, making it adaptive as the parameters dynamically
adjust based on system conditions. By leveraging the inherent graph struc-
ture of supply chains, our framework enables agents to learn the system’s
topology,andweemployacentralizedlearning,decentralizedexecutionscheme
that allows agents to learn collaboratively while overcoming information-
sharing constraints. Additionally, we incorporate global mean pooling and
regularization techniques to enhance performance.
We test the capabilities of our proposed approach on four different supply
chain configurations and conduct a sensitivity analysis. This work paves the
way for utilizing MARL-GNN frameworks to improve inventory management
in complex, decentralized supply chain environments.
Keywords: Inventory Control, Supply Chain Optimization, Multi-Agent
Reinforcement Learning, Graph Neural Networks
Preprint submitted to Computers & Chemical Engineering October 25, 2024
4202
tcO
42
]AM.sc[
1v13681.0142:viXra1. Introduction
Modern supply chains are complex and operate under uncertain envi-
ronments. These uncertainties can lead to disruptions and sub-optimal per-
formance, often due to operational failures or poor coordination between
different parts of the supply chain. The inventory control problem, a se-
quential decision-making problem, is additionally challenged by stochastic
and volatile factors such as lead times and seasonal demand patterns, often
resulting in sub-optimal performance. The impact of disruptions such as the
bullwhip (demand amplification) or ripple effect (disruption propagation) ef-
fect can be alleviated through collaborative efforts among different entities
within a supply chain (de Almeida et al., 2015).
1.1. Inventory Control
The theory of inventory control can be traced back to the news-vendor
problem (Edgeworth, 1888; Clark and Scarf, 1960), and the first widely used
numerical solutions for inventory optimization seems to be the Economic
Order Quantity (EOQ) model (Erlenkotter, 1990). These works were funda-
mental to the widely known policies in inventory control theory: (R,S), (s,S),
(R,s,S)and(R,Q)(FedergruenandZheng,1992;Silveretal.,1998). Overthe
years, control theory has had an influence on production planning and inven-
tory control (Aggarwal, 1974; Jackson et al., 2020). The complexity of mod-
ern systems has driven dynamic programming applications, unfortunately,
obtaining exact analytical solutions often proves infeasible due to computa-
tional demands (Bellman, 1952). This has fuelled the adoption of advanced
techniques beyond traditional closed-form solutions. On one hand, stochas-
tic (Ku¨c¸u¨kyavuz, 2011) and distributionally-robust optimization approaches
(Bertsimas et al., 2019; Qiu et al., 2021), while promising, face challenges
such as online tractability, limiting their practical applicability in real-world
supply chain scenarios. On the other hand, traditional approaches leveraging
heuristics, while offering a simpler implementation, struggle to adapt to the
dynamic and complex nature of modern supply chains.
1.2. Reinforcement learning for Inventory Control
Reinforcement learning (RL) has emerged as a promising alternative for
addressing the challenges of stochastic sequential decision-making problems.
Itsabilitytoexcelincomplex,dynamicenvironmentsandhandleuncertainty,
places RL as a valuable tool to enhance decision making in supply chains.
2Whilecloselylinkedtodynamicprogramming, RLoffersageneralsolutionto
identify approximate optimal policies for stochastic processes by leveraging
the Bellman optimality equation to iteratively update value functions and
improve decision-making policies over time. RL also provides a cost-effective
solution for decision-making by enabling offline training, which reduces the
online computational overhead compared to optimization approaches that
require continual updates in receding or shrinking horizon frameworks. How-
ever,RL’seffectivenessreliesontheavailabilityofinformationsharingamong
the entities within the supply chain. In scenarios where multiple intercon-
nected entities are involved, ensuring seamless information exchange can be
challengingduetofactorssuchasdataprivacyconcerns, proprietaryinforma-
tion, or communication constraints. This limitation may hinder the adoption
and implementation of RL-based approaches in complex supply chain envi-
ronments, highlighting the need for strategies to address information sharing
challenges effectively.
1.3. Multi-Agent Reinforcement Learning
This challenge in information sharing motivates the exploration of multi-
agent reinforcement learning (MARL) approaches in supply chain manage-
ment. MARL frameworks allow multiple entities within a supply chain to
autonomously learn and adapt their decision-making processes while inter-
acting with each other. By leveraging MARL, supply chain entities can col-
laborate and coordinate their actions more effectively, leading to improved
overall performance and resilience against disruptions. Additionally, MARL
provides a scalable approach to handle the complexities of large-scale supply
chain systems by distributing decision-making across multiple agents.
However, despite its potential, MARL faces several challenges. One is-
sue is the non-stationarity of the learning environment inherent in MARL
(Tan, 1993; Tampuu et al., 2017). This occurs when multiple agents are
learning simultaneously so the transition dynamics are not stationary. One
setting commonly used to overcome this is to use a central critic during train-
ing that has access to global observations and actions (Nekoei et al., 2023).
This is known as the Centralised Training, Decentralized Execution learn-
ing paradigm. However, while adding more information can help mitigate
the non-stationarity problem, it can also lead to new issues. Naively con-
catenating all available information can result in information overload and
inefficiencies (Lowe et al., 2017; Yu et al., 2022; Nayak et al., 2023). This in-
discriminate accumulation of data can cause policy overfitting, where agents
3develop strategies that perform well on the excessive training information
but poorly in real-world scenarios due to the lack of generalization (Nayak
et al., 2023; Hu et al., 2021). Therefore, the development of novel techniques
for smart information aggregation is required to avoid policy overfitting and
enhance the efficacy of MARL frameworks in supply chain management.
1.4. Related Work
Given the inherent complexity and uncertainty in inventory management
systems, there are several approaches that address specific challenges within
this domain. In this section, we highlight some related previous work and
methodologies tackling the inventory management problem.
One example to find optimal inventory control policies is using traditional
heuristics. These are simple, exact methods which suit relatively simple
problems. Common examples include reorder point heuristics (Federgruen
and Zheng, 1992; Silver et al., 1998) or the Economic Order Quantity model
(Erlenkotter, 1990). However, a key limitation of these heuristics is their
lack of adaptability. They rely on pre-defined parameters that may not be
effective in dynamic environments with fluctuating demand or unforeseen
circumstances.
Moreover, inventory management problems can also be tackled using op-
timization techniques like Linear Programming (LP) (Janssens and Ramaek-
ers, 2011) or Mixed-Integer Linear Programming (MILP) (You and Gross-
mann, 2008). LP is well-suited for problems with linear relationships, while
MILPoffersmoreflexibilitybyincorporatingintegervariablestoaddressspe-
cific complexities like minimum order quantities. However, these methods do
not directly account for uncertainty.
If the optimal policy is analytically intractable, methods like stochastic
optimization can be used to find an approximate optimal policy (Grossmann
et al., 2016; You and Grossmann, 2011). While stochastic optimization often
finds good solutions, it does not guarantee optimality. In specific scenar-
ios, exact numerical methods like dynamic programming might be applicable
(Berovic and Vinter, 2004; Perez et al., 2021). However, these methods are
often limited by scale due to computational demands. This leads to approx-
imate numerical methods. These methods provide scalability but may be at
a sub-optimal performance. Additionally, Approximate Dynamic Program-
ming (ADP) is a powerful technique specifically designed for problems with
both dynamics (e.g., decisions impact future states) and stochasticity (e.g.,
4uncertain outcomes) (Katanyukul et al., 2011). ADP leverages approxima-
tions and sampling techniques to find good solutions for complex inventory
management problems that might be intractable for traditional methods.
Reinforcement Learning methods, closely related to dynamic programming,
also belong to the approximate numerical methods class.
Reinforcement learning has emerged as a promising approach to address
the dynamic and complex nature of inventory management systems (Gian-
noccaro and Pontrandolfo, 2002; Bharti et al., 2020a; Boute et al., 2022;
Rangel-Martinez and Ricardez-Sandoval, 2024). Single-Agent RL method-
ologies have been explored extensively in this context to solve a Markov
Decision Process (MDP), the mathematical framework used to model deci-
sionmakinginaninventorymanagementsystem. Unliketraditionalheuristic
approaches that rely on pre-defined rules, RL leverages interaction with the
environment to learn optimal policies. Reinforcement learning, particularly
deep reinforcement learning, leverages neural networks to approximate the
value functions and policies, enabling it to handle high-dimensional state
and action spaces. Methods like Q-learning (Chaharsooghi et al., 2008; Kara
and Dogan, 2018; Bharti et al., 2020b; Vera, 2021), Deep Q-Networks (DQN)
(Oroojlooyjadidetal.,2022;Meisherietal.,2020),andPolicyGradientmeth-
ods (Stranieri and Stella, 2022; Siems et al., 2023; Burtea and Tsay, 2024)
have shown promise in developing adaptive and scalable inventory policies
that can learn from interactions with the environment over time. However,
RL often struggles with integer or mixed-integer decisions, which are com-
mon in inventory management problems (e.g., order quantities). As the scale
of inventory systems increases, the complexity of the decision-making pro-
cess grows, resulting in a significantly larger action space. This increased
complexity can hinder the effectiveness of RL algorithms, as they require
extensive training data and longer convergence times to identify optimal
policies. This limitation is particularly relevant because studies using RL
for inventory control often represent the act of order replenishment as a dis-
crete integer decision Chaharsooghi et al. (2008); Stranieri and Stella (2022),
rather than leveraging pre-defined heuristic parameters like reorder point (s)
or order-up-to level (S) which could speed up adoption in industry.
There has been a growing interest to leverage the graph structure of a
supply chain by using Graph Neural Networks (GNNs). The main idea is
to leverage GNNs to learn the hidden representation of the data encoded
as a graph structure (Munikoti et al., 2022). This allows RL agents to effi-
ciently adapt to changes in the problem domain. The application of GNNs
5to RL has been widely studied for a series of well known combinatorial op-
timization problems (Khalil et al., 2017; Mazyavkina et al., 2021) such as
the vehicle routing problem (Munikoti et al., 2022), travel salesman problem
(Munikoti et al., 2022) and the job shop scheduling problem (Zhang et al.,
2020a,b). GNNs have been shown to improve performance across different
graph sizes and types. These works provide an initial foundation but they
consider simplified supply chain instances with deterministic lead times and
assume centralized information. Unfortunately, real-world supply chains face
inherent information sharing constraints. These constraints can arise due to
technical limitations caused by incompatible systems, the inherent complex
nature of supply chain structures, or even due to privacy concerns about
sharing sensitive data.
To address these limitations and leverage the collaborative nature of sup-
ply chains, multi-agent RL approaches have gained significant momentum
but remain limited. These approaches model multiple agents, each repre-
senting a decision-maker within the supply chain, and allow them to interact
and learn from each other. The application of MARL to the inventory man-
agement problem is relatively limited compared to other domains including
single-agent RL. However, there is growing recognition of the potential ben-
efits of MARL in addressing the dynamic and collaborative nature of inven-
tory management in multi-agent environments. Liu et al. (2022b) applied
Heterogeneous-Agent Proximal Policy Optimization (HAPPO) to a serial
supply chain which showed overall better performance than single-agent RL.
The study also concluded that information sharing between entities helps al-
leviate the bullwhip effect. Another study conducted by (Feng et al., 2022b),
applied a Decentralized PPO framework on a single store problem with a
large number of stock keeping units. This approach accelerated policy learn-
ing compared to standard MARL algorithm. Additionally, (Khirwar et al.,
2023) applied a PPO variation MARL framework to a multi-echelon en-
vironment, and it outperformed other base-stock policies. Sultana et al.
(2020), applied a multi-agent advantage actor critic algorithm to a multi-
echelon, multi-product system but assumed a lead time of zero. Finally,
Mousa et al. (2023) provided an analysis of multi-agent reinforcement learn-
ing algorithms for decentralised inventory management systems. The study
showed that MAPPO outperformed other MARL methods and further high-
lighted MARL as a promising decentralized control solution for large-scale
stochastic systems. While these studies presented in inventory management
have made progress in applying MARL techniques, there are still opportuni-
6ties for improvement. One area deserving further attention is how to leverage
the inherent graph structure of a supply chain to enhance RL frameworks’
performance. By exploiting the connectivity and dependencies between dif-
ferent supply chain nodes, we can develop more effective algorithms that
address the complexities of multi-agent inventory management.
1.5. Motivation
In this work, we propose leveraging the capabilities of GNNs to learn hid-
den representations of agents and their interactions. These representations
arethenintegratedintoaMARLframeworktofindoptimalinventorypolicies
in a multi-echelon supply chain network. Our contributions are summarized
as follows:
•
We propose the redefinition of the action space from order replenish-
ment to parametrize a heuristic inventory control policy where both
parameters can be dynamically adjusted based on current system dy-
namics. This ensures early adoption of new optimization techniques
due to its interpretability whilst accommodating real-world complex-
ities. Moreover, by focusing on parameterized heuristics, we can ef-
fectively navigate the challenges that RL faces with integer decisions,
such as those related to order quantities.
•
We leverage the inherent graph structure of a supply chain to aid col-
laboration between entities in a supply chain.
•
We reduce dimensionality and increase scalability of a MARL-GNN
framework by incorporating a global mean pooling aggregation mech-
anism within our algorithmic framework.
•
We introduce Gaussian perturbations into the value function and per-
form a sensitivity analysis on the perturbation intensity to reduce pol-
icy overfitting and address potential distributional shift, serving as a
regularization technique.
•
We demonstrate the effectiveness of our method in terms of scalability
through four multi-echelon supply chain scenarios.
The rest of this paper is organized as follows: Section 2 provides the
background on multi-agent reinforcement learning, Section 3 describes our
proposed decentralized decision-making framework in more detail, Section 4
7discusses the experimental results obtained and finally Section 5 concludes
this paper and provides an outlook for future work.
2. Preliminaries
We first provide a background into the components that contribute to-
wards our methodology including single agent Proximal Policy Optimisation
(PPO) and the multi-agent extensions of PPO.
2.1. Single Agent Reinforcement Learning - PPO
Proximal Policy Optimisation (PPO) is a popular first-order, on-policy1
single agent reinforcement learning method. PPO is an actor-critic algorithm
where a policy π and value function V (s) are two separate neural networks
θ ϕ
parameterised by θ and ϕ respectively. In actor critic algorithms like PPO,
V (s) is introduced to reduce the variance but may introduce a bias in π .
ϕ θ
ThetwovariationsofPPOareusingapenaltyfunctionoraclippingfunction.
The later is known to be crucial for its performance as the clipping function
constraints the ratio between the new and old policy within a certain range
to prevent large policy updates that may lead to instability.
The PPO policy loss can be defined as:
(cid:20) (cid:18) (cid:18) (cid:19) (cid:19)(cid:21)
π (a s ) π (a s )
= E min θ t | t A ,clip θ t | t ,1 ϵ,1+ϵ A
Lθ (st,at)∼π
θold π (a s )
t
π (a s ) −
t
θ t t θ t t
old | old |
(1)
where π (a s ) represents the probability of taking actions a in state s
θ t t t t
|
under the current policy parameterized by θ, π (a s ) is fixed during the
θ t t
old |
policy update step and represents the probability of taking action a in state
t
s under the old policy (from the previous iteration) parameterized by θ , a
t old t
and s are the action and state respectively taken at time step t, ϵ represents
t
ahyperparameterthatdetermineshowmuchthenewpolicycandeviatefrom
the old policy and clip( ,1 ϵ,1+ϵ) is the clipping function that constraints
· −
the ratio of the new to old policy’s probabilities within a certain range which
1Technically, PPO still employs off-policy corrections, meaning it reuses samples col-
lected during training, but it does not explicitly use a replay buffer. PPO updates the
policy network using a surrogate objective function that constrains the policy update to
be within a certain proximity of the previous policy. This avoids the need to store and
sample experiences
8prevents the policy from making overly large updates that could lead to
instability. Finally, A is the advantage function defined as:
t
A = R +γV (s ) V (s ) (2)
t t ϕ t+1 ϕ t
−
where R and V (s ) is the reward and value function at time step t respec-
t ϕ t
tively and γ is the discount factor. The general advantage estimation (GAE),
used to compute the advantage, is given by:
∞
(cid:88)
AˆGAE = (γλ)lδ (3)
t t+l
l=0
where the variable l represents the index of the future time step relative to
the current time step t. This summation captures the discounted temporal
difference (TD) errors, δ , where δ = R +γV(s ) V(s ) is the
t+l t+l t+l t+l+1 t+l
−
TD error. GAE is commonly used in single-agent reinforcement learning
as it allows for a bias-variance trade-off through its hyperparameter λ and
the summation is modulated by the term (γλ)l. This formulation helps in
accurately estimating the advantage by weighing the importance of future
rewards and the associated value function estimates.
2.2. Multi-Agent Reinforcement Learning
Two multi-agent variants of the popular single agent PPO algorithm are
possible: IPPO and MAPPO.
Independent PPO (IPPO) IPPO is an independent learning algo-
rithm which breaks down a problem with n agents into n decentralised single
agent problems. A value function, Vi(s), and policy, πi, are present for
ϕ θ
each agent in IPPO, taking local inputs. Despite showing good overall per-
formance in certain multi-agent settings, IPPO can lead to non-stationarity
in the environment. This occurs because each agent’s policy is updated si-
multaneously which affects the state transition probability, p(s′,r s,ai,π),
|
which becomes non-stationary. Therefore, theoretically, the convergence of
the Bellman Equation, shown in Equation 4 is not guaranteed.
(cid:88) (cid:88)
Vπi (s) = πi(ai s) p(s′,r s,ai,π)(r+v (s′)) (4)
πi
| |
a s′,r
Multi-agent PPO (MAPPO) MAPPO utilizes a centralized value
function V (s) that takes global inputs. The objective function can be de-
ϕ
9noted as:
(cid:34) (cid:32)
L(oi,s,ai;a−,θ k,θ) = N1 (cid:88)N E min ππ θi( (a ai | io oi) i)Aπ θ oi ld(oi,s,a−),
θi
i=1 old |
(cid:32) (cid:33) (cid:33)(cid:35)
clip
π θi(ai |oi)
,1 ϵ,1+ϵ Aπ θ oi ld(oi,s,a−)
π (ai oi) −
θi
old |
(5)
Where a is the current agent action, a− is the action set of all agents, s
is the global state, o is the local observation, The advantage function Aπ θ oi ld
is computed using the GAE method in a similar manner to Equation 3.
However, when GAE is applied in a multi-agent setting with a shared
value function, the advantage estimated for each agent can be identical. In
MAPPO, this is always true in fully cooperative cooperative environments
where all agents share a common reward function and experience the same
state transitions. This makes it challenging to accurately quantify and dis-
tinguish the unique contribution of each individual agent to the overall per-
formance, even though the policy gradient considers the actions of all agents
when updating the policy for each agent. This is known as implicit multi-
agent credit. In contrast, in IPPO, where each agent has its own independent
value function, the advantage estimates differ across agents, even in coopera-
tive environments. This is because each agent calculates its own GAE based
on its individual observations, rewards, and value function.
δ
L
E(cid:2)
Aπ θk
(cid:0) o,s,a−(cid:1)(cid:3)
(6)
δπi(ai s ) ∝
t t
|
An additional complication is that for a large number of agents, the number
of possible joint actions becomes vast, so exploring the joint action space and
createenoughexcitationforallagentstocomputethetruegradientisimprac-
tical. Therefore, learning algorithms rely on sampling techniques to estimate
the gradient which may not adequately explore the joint agent space. This
may lead to the problem of policy overfitting in cooperative multi-agent envi-
ronments. Several studies have been conducted to reduce credit assignment
issues through techniques like reward shaping (Zhou et al., 2020), employing
individual critics (Hernandez-Leal et al., 2019) and communication protocols
(Feng et al., 2022a). Other methods have been developed to tackle the effects
of credit assignment by reducing policy overfitting with exploration bonuses
(Yarahmadi, 2023) or regularized policy gradients (Liu et al., 2021).
10Figure 1: Schematic showcasing the Centralised Training Decentralised Execution frame-
work
Centralized Training Decentralized Execution (CTDE). CTDE is
a framework used in cooperative MARL to overcome some of the shortcom-
ings mentioned above. In this setting, agents take global state information in
a centralized manner to help train policies that can execute on a decentral-
ized manner at execution with local inputs only as shown in Figure 1. Our
paperfocusesonacooperativeMARLsettingwhereagentsonlyshareacom-
mon reward function. It is widely known that sharing information between
agents helps stabilize learning and deals with the non-stationarity problem
inherent in multi-agent problems. Despite the increased performance, shar-
ing information by na¨ıvely concatenating local information leads to the curse
of dimensionality as the global state increases with the number of agents.
3. Methodology
In this section, we outline our approach for a graph-based multi-agent
PPO algorithm. First, we discuss the integration of graph neural networks
(GNNs) into our framework. Next, we illustrate how noise injection is uti-
lized as a regularizer within the value function in our approach. Finally, we
define the inventory management problem, outlining its key components and
objectives.
3.1. P-GCN-MAPPO
In environments where the graph structure can be leveraged, Graph
Neural Networks (GNNs) are commonly integrated into RL frameworks.
11GNNs were developed to efficiently leverage the structure and properties
of graphs. GNNs operate on graph-structured data and are able to capture
complex relationships and dependencies inherent in graphs. In this work,
we use Graph Convolutional Networks (GCNs) combined with Multi-Agent
Proximal Policy Optimization (MAPPO) and a Pooling strategy, hence P-
GCN-MAPPO. In Section 4 we conduct computational experiments to ana-
lyze the different components of this methodology in an effort to distill their
contribution to the overarching framework. GCNs update the representa-
tion of a node by aggregating and transforming the features of its neigh-
bouring nodes and itself. This allows the model to capture and propagate
local information, effectively learning patterns and dependencies from the
graph structure of a supply chain. This ability to capture complex relation-
ships is particularly beneficial for supply chain problems as understanding
the intricate relationships between various entities can significantly optimize
decision-making.
The adjacency matrix, A RN×N, is a N N matrix which is used to
∈ ×
express the directed graph topology where N is the number of nodes (or ver-
tices)inthegraph. Inthismatrix,A = 1indicatesthatthereisaconnection
ij
between node i and node j. Each node i is associated with a node feature x
i
which encapsulates information specific to that node where x RD where D
i
∈
represents the dimensionality of the features vector. These individual node
features collectively form the node feature matrix, X RN×D. At each time
∈
step, the node feature matrix, X, captures the evolving state of the graph.
Both A and X are fed into a graph convolution layer, allowing the model to
capture relational information between nodes.
f(X,A) := σ(D− 21 (A+I)D− 21 XW) (7)
where A RN×N is the adjacency matrix, I RN×N is the identity matrix,
D RN×∈ N is the degree matrix of A + I, X∈ RN×D is the node feature
ma∈ trix, W RD×W′ is the layer’s weights where∈ W′ is the number of output
∈
features and σ(.) is the activation function (e.g. ReLU). This results in an
embedded vector for each node, h RW′ . In this work, three convolution
i
∈
layers are used where the embedded vector at each node, h , is the input of
i
the next layer. This is described mathematically as:
H := f (X,A) (8)
1 W1
H := f (H ,A) (9)
2 W2 1
H := f (H ,A) (10)
3 W3 2
12where H
1
RN×W 1′,H
2
RN×W 2′,H
3
RN×W 3′ are the embedded node
∈ ∈ ∈
matrices at layers 1,2 and 3 and W ,W ,W are the weight matrices that
1 2 3
parameterize each layer. The terms W′,W′,W′ Z+ are defined as positive
1 2 3 ∈
integers representing the number of output features for each layer respec-
tively. These values are hyperparameters that determine the dimensionality
of the embedded vectors produced by each layer.
The resulting latent representations are then passed to our centralized
value function V (s), a fully connected network parameterized by ϕ. This
ϕ
function assesses the value of actions, aiding in variance reduction of the
policy π , another fully connected network parameterized by θ, which maps
θ
states to actions. To train the policies, we employ Multi-Agent Proximal
Policy Optimization (MAPPO). Mini-batches are sampled, and the objective
is to minimize the loss function for each batch shown in Equation 5 which is
used to update the parameters, θ and ϕ.
However, as the number of agents increases, the dimensionality of the
node feature changes. To address this, we propose integrating global mean
pooling within our framework, ensuring dimensionality remains constant as
the number of agents increases.
PoolingoperatorsingraphswereinspiredbypoolingmethodsinConvolu-
tional Neural Networks (CNNs). Therefore, instead of simply concatenating
all the latent hidden representations and feeding them directly into our cen-
tral value function, we enhance computational efficiency by employing the
global mean pool operator. The global mean pooling operator aggregates
information across all nodes in a graph by taking the average of the node
features. The GNN outputs hidden feature matrix, H RN×W 3′ , can be
∈
denoted as H = h ,h ,...,h where N is the number of nodes and h is
1 2 N i
{ }
the GNN output feature vector for node i. The global mean pooling operator
computes the mean of each feature across all nodes.
N
1 (cid:88)
Global Mean Pool(H) = h (11)
i
N
i=1
This effectively reduces the input dimensions to our critic from N W′
× 3
to 1 W′. This is shown in Figure 2. While this dimensionality reduction
× 3
improvescomputationalefficiency,there’sapotentialdrawback: itmayresult
in the loss of specific spatial information. However, since each individual
actor still leverages local information for each node, the loss is confined to
the critic. Moreover, the critic might not require the full level of spatial
13Figure 2: Methodology diagram outlining the P-GCN-MAPPO framework, illustrating
the key components and processes involved.
detail to assess the overall value of a state. The high-level features extracted
throughtheglobalmeanpoolingmechanismmightsuffice. Thisideahasbeen
explored in several studies such as Fujimoto et al. (2018) who highlights the
critic’s focus on good enough value approximation for policy improvement,
suggesting it may not need full state representation and Lyu et al. (2023)
whichprovedthatcentralizedcriticsmaynotbebeneficial; particularlystate-
based critics can introduced unexpected bias. This aligns with the idea that
the critic might not need a perfect representation but an accurate enough
estimate for policy guidance. Finally, the global mean pooling mechanisms
can also help mitigate the risk of overfitting, a common problem in RL where
the model performs well on training data but poorly on unseen data. By
reducing the number of features, the model focuses on the most relevant
aspects, potentially leading to better generalization.
3.2. Noisy P-GCN-MAPPO
The problem of policy over-fitting occurs as the estimated advantage is
the same for all agents leading to the lack of credit assignment. Theoret-
14ically, this can be solved by ensuring the shared advantage value does not
affect other agents by decomposing the centralized advantage value for each
agent. However, given the nature of multi-agent systems, this is impractical.
Therefore, noise can be introduced as a regularize to reduce bias in advan-
tage values. Noise has been extensively studied and used in several forms
across different components of reinforcement learning, including action space
exploration and observation perturbation. In this work, we focus on intro-
ducing noise specifically in the value function which then propagates to the
estimation of advantage values as shown in Figure 3. This approach offers
several potential benefits and implications:
1. Reduces Overfitting and Biases. Introduction of noise reduces
over-fitting and biases in advantage value estimations. The introduc-
tionofrandomnessintothevaluefunction,meansanypatternsorbiases
that may arise from the agent’s limited experience or observations are
disrupted.
2. Exploration Enhancement. The introduction of noise in the value
function promotes exploration by injecting randomness into the agent’s
value estimates.
We sample a Gaussian noise,
ϵ (0,σ2) (12)
n
∼ N
where σ2 is the variance (intensity) of the noise added to the samples. The
value of σ is a hyperparameter controlling the noise level, we analyse the
effect of this hyperparameter in Section 4.2. The global state, s, is then
inputted into the centralized value network V to which the sampled noise,
ϕ
ϵ , is added.
n
V (s) V (s)+ϵ (13)
ϕ ϕ n
←
The addition of the random noise disturbs the value function uniformly
for all agents which is propagated to the advantage function. Uniform noise
injection introduces variability into the advantage values, promoting explo-
ration and adaptability across the entire agent population. This in turn pro-
vides benefits such as preventing overfitting caused by sampled advantage
values with deviations and environmental non-stationarity. However, it’s im-
portant to note that this approach may sacrifice the potential for agents to
fully exploit their individual characteristics and preferences, which tailored
noise could accommodate.
15(a) Overview of the training phase, illustrating the collaboration between the actor, critic, and graph
convolutionalneuralnetworksformultipleactors.
(b)Illustrationofvaluefunctionnoiseinjectionwithintheactor-criticframeworkfor
asingleactor.
Figure 3: Visual representations of the tr1a6ining processes: (a) the interaction among
multipleactorsduringtraining,and(b)theroleofnoiseinjectioninenhancingthelearning
dynamics shown with a single actor.An additional advantage of Proximal Policy Optimization (PPO), is the
inherent clipping mechanism of the algorithm which helps mitigate poten-
tial instability caused by the noise. The clipping ensures policy updates
remain close to the previous policy allowing for exploration benefits intro-
duced through value function noise injection without compromising training
stability.
3.3. Problem Statement
The cooperative nature of the MARL scenario that we consider means
agents coordinate towards a common goal, receiving a shared reward. This
sequential decision making problem is modelled as a Decentralized Partially
Observable Markov Decision Process (Dec-POMDP), which can be defined
as a tuple ,A, , ,Ω, ,γ . is the set of all valid states, A := A1
⟨S T R O ⟩ S ×
AN is the joint action space where Ai is the set of actions available
··· ×
for each agent i, Ω := Ω1 ΩN is the joint observation space and N
× ··· ×
denotesthenumberofagents. Ateachtimestep, eachagentiexecutesaction
ai i Rna with a joint action a = (cid:10) a1,...,aN(cid:11) and transitions from state
s ∈ A ⊆Rns to s′ Rns with state transition probability P(s′ s,a) =
(∈ s,aS ,s⊆ ′). Each ag∈ enS t i⊆ receives observation oi Ωi Rn oi determi| ned by
T ∈ ⊆
(s′,i) which maps the new state s′ to an observation oi Ωi for each
O ∈ S ∈
agent i. In other words, the observation function : 1,...,N Ω
O S × { } →
provides each agent i with a local observation oi based on the next state s′,i.
(cid:10) (cid:11)
The joint observation can be defined as o = o1, ,oN and each agent
shares the same reward function (s,a) R . Each·· a· gent has policy πi and
R(cid:10) ∈ (cid:11)
the joint policy is denoted as π = π1, , πN . The optimal joint policy
··· (cid:104) (cid:105)
is found through maximising the joint expected reward E (cid:80)t=T γtr where
t=0 t
r = (s , a ) R at each time step t where T is the time horizon and
t t t
R ∈
the discount factor γ [0,1].
∈
To find the optimal inventory policy in the inventory management prob-
lem, we propose a mathematical formulation of the supply chain dynamics
as an optimization problem, characterized by each time period t, over a fixed
horizon of T time periods. The variables are defined in Table 1.
Equation (14) is the objective function that maximizes total profit across
the supply chain system. The system is treated as a collaborative framework,
where all agents in the network share a common objective function, reward
r . Equation (15) and (16) show how the inventory and backlog are updated
t
over time. Equations (17) and (18) restrict the quantity of goods a node
17Table 1: Variables and Parameters for the Inventory Management Problem
Symbol Description
i Node i N where N is the total number of nodes
g Rnid Amount∈ of goods shipped from node i to downstream nodes
o ∈ R Replenishment order
r
d
∈R
Demand from downstream nodes
v ∈ R On-hand inventory
b
∈R
Backlog
q ∈ R Acquisition or incoming goods
v ∈ R On-hand inventory at the start of each period
0
b
∈R
Backlog at the start of each period
0
P
∈RnN
Price of goods sold
C ∈ RnN Order replenishment costs
V ∈ RnN Storage costs
B ∈ RnN Backlog costs
V ∈ RnN Maximum limits on node storage
max
O ∈ RnN Maximum limits on replenishment order quantities
r,max
i
R∈
Upstream node of i
u
i
∈R
Downstream node of i
d
bi =∈ (cid:80) bi d Total backlog of node i from downstream nodes
gi =
(cid:80)j∈Di
gi d Total shipment of node i to downstream nodes
j∈Di
Set of direct downstream nodes j of node i where j
i i
D ∈ D
Set of nodes with customer demand
C
c Customer demand
can ship downstream, ensuring it does not exceed the on-hand inventory or
the downstream demand and backlog. Equation (19) and 20 capture the
lead time of a shipment, indicating that goods shipped to node i will take
τi periods to reach the downstream stage. The amount of inventory hold
or ordered is also constrained to a maximum value through Equations 23.
The interaction of the different flows between two nodes can also be seen in
Figure 4.
18Figure 4: Schematic showing the inventory flow between two nodes in an inventory man-
agement system
N T
(cid:88)(cid:88)
max Pigi[t] Cioi[t] Vivi[t] Bibi[t], (14)
− r − −
i=1 t=1
subject to
vi[t] = vi[t] gi[t]+qi[t], i, t, (15)
0 − ∀ ∀
bi d[t] = bi d[t] gi d[t]+di d[t], i, d , (16)
0 i
− ∀ ∀ ∈ D
gi d[t] bi d[t]+di d[t], i, t, d (17)
0 i
≤ ∀ ∀ ∀ ∈ D
gi[t] vi[t]+qi[t], i, t, (18)
≤ 0 ∀ ∀
qi[t] = giu[t τi], i = 1,t τi (19)
− ∀ ̸ ≥
q1[t] = o1[t τ1], t τ1 (20)
r − ≥
di d = od, i, d , (21)
r ∀ ∀ ∈ Di
with
di[t] = ci[t], i , t (22)
∀ ∈ C ∀
oi[t] Oi ,vi[t] Vi , i, t, (23)
r ≤ r,max ≤ max ∀ ∀
The demand and lead time are uncertain, modelled as a Poisson random
variable with parameter λ and µ respectively. Therefore, the probability of
k demands in a time period t is given by:
(λt)kexp−kt
P(D = k) = (24)
k!
Moreover, inventory management system can be modelled as a graph
G = (W,E) where W represents different entities in the supply chain (e.g.
factories, manufacturers, warehouses), E denotes the relationship between
these entities, and each node i W corresponds to an agent responsible
∈
for managing inventory at that entity’s location. Each node i has a node
19feature xi X, representing the observation space oi for each agent. The
∈
observation set for each agent i is oi = [v,b,p,d−1,...,d−M,o−1,...,o−M]
r r
where oi Ωi. In the observation set, a new variable is introduced, p,
∈
which is the pipeline inventory equal to the sum of order replenishment that
has not yet arrived at the node from other upstream nodes. To mitigate
the problem of partial observability, we include demand history and order
history up to M time-steps in the past where M is a hyperparameter. While
we chose not to incorporate Recurrent Neural Networks (RNNs) in this work,
which could effectively handle sequential data and temporal dependencies,
we acknowledge their potential as an alternative approach. Our decision
to include a fixed window of past observations instead allows for a simpler
implementation while still capturing relevant historical context.
However, this inclusion of historical data introduces a violation of the
Markov property, which states that the future state of the system only de-
pendsonthecurrentstateandnotonthesequenceofeventsthatprecededit.
However, real-world decision-making processes rarely exhibit perfect Marko-
vian behavior. Therefore, including historical data is a well-established ap-
proachtoaugmenttheobservationspaceinpartiallyobservableenvironments
Liu et al. (2022a); Mousa et al. (2023); Uehara et al. (2022).
The neighbourhood of a node i, denoted by M(i) is defined as the set of
neighbouring nodes j connected to node i via edges in E. Mathematically,
this can be represented as M(i) = j (i,j) E . For each node i, the
{ | ∈ }
features of its neighbours j are represented as xi, capturing the relevant
j
observations of the neighbouring agents.
Moreover, the action space is traditionally modelled as the order replen-
ishment quantity, o. However, in this paper we parameterize a heuristic
inventory policy such that for each agent i, at each time step t, the policy
outputs a mean and standard deviation for both si and Si . There-
inv,t ord,t
fore, π (oi) = (µ ,σ ,µ ,σ ) and the heuristic inventory pa-
i si si Si Si
inv,t inv,t ord,t order,t
rameters are sampled from a Gaussian distribution, si (µ ,σ2 ),
inv,t ∼ N si inv,t si inv,t
Si (µ ,σ2 ). A min-max post processing step is then used to
ord,t ∼ N S oi rd,t S oi rd,t
scale the values to a suitable range denoted by a subscript s, leading to
(si ,Si ) where si is the reorder point and Si is the order-up-to-
tinv,s t
ord,s
tinv,s t
ord,s
level. When the inventory reaches a level of si , an inventory order is
tinv,s
placed where oi = Si vi. The neural network architecture for each ac-
r,t t ord,s − t
tor, includingthepost-processingstepthatresultsintheorderreplenishment
quantity, is illustrated in Figure 5.
20Figure 5: Neural network architecture for actor, illustrating sampling from a Gaussian
distribution, followed by a post-processing step, leveraging an inventory heuristic policy
to generate actions in a continuous action space.
Moreover, in inventory management, optimal order quantity policies such
as (si ,Si ) are often characterized by discrete functions as they show
tinv,s t
ord,s
abrupt changes in order quantity at specific inventory levels Dehaybe et al.
(2024). This poses a challenge for neural networks, which are inherently
continuous function approximators as directly approximating such discrete
policies can lead to instability and poor performance in neural network-based
models. To address this, in this paper, the actor network outputs a normal
distribution (Gaussian) over the (si ,Si ) parameters. A key note is that
tinv t
ord
this approach does not limit the network’s ability to learn the optimal pol-
icy as it simply expresses the policy in a way that may be easier to learn.
Moreover, unlike traditional heuristic methods where the policy is not con-
ditioned on the state of the system, an RL approach means the heuristic
(si ,Si ) policy is conditioned on the state of the system such as on-
tinv,s t
ord,s
hand, pipeline inventory and backlog. This enables the agent to dynamically
adjust the reorder point and order-up-to level based on the current inventory
situation, potentially leading to optimal and flexible decision-making.
Finally, while the actual order replenishment quantities in our environ-
ment are discrete, we model the action space for each agent as continuous
within the range of [ 1,1]. This approach facilitates scalability to problems
−
withawiderrangeofpossibleordersizes. Tobridgethegapbetweenthecon-
tinuous action space and the discrete nature of order quantities, we employ
21Figure6: Schematicillustratingtheexecutionofthetrainedpoliciessimultaneouslyalong-
side heuristic inventory control policies.
a post-processing step using min-max scaling. During this step, the agent’s
chosen action is scaled to the appropriate range for the order replenishment
variable and then rounded to the nearest integer value.
4. Results and Discussion
In this section four case studies of different inventory management config-
urations are used to illustrate the effectiveness of our proposed methodology
as shown in Figure 7. We compare our proposed methodology (Noisy P-
GCN-MAPPO and P-GCN-MAPPO) against other MARL methods includ-
ing: IPPO, MAPPO, and Graph-based-MAPPO(G-MAPPO). This allows
us to compare against state-of-the-art as well as analyse the contribution
of the different components of our method. One of the key differences be-
tween the methods is the varying levels of information input to the value
function which can be illustrated qualitatively in Figure 8. This is further
benchmarked against single agent RL (PPO specifically) and a heuristic (s,S)
policy. In the later, a static heuristic policy is found for each node in the
network, where optimal parameters are found using a derivative-free method,
22Figure 7: Different supply chain configuration case studies with varying number of nodes
implemented using SciPy Virtanen et al. (2020) with a multi-start approach
combined with local search.
All variant MARL algorithms are implemented in the Ray RLLib frame-
work Liang et al. (2018), and all the hyperparameters are kept the same as
the baseline algorithms to ensure fairness.
4.1. Execution
To evaluate the performance and robustness of our proposed methodol-
ogy, we considered different scenarios involving a varying number of agents:
6, 12, 18, and 24 agents. Each scenario represents a different level of com-
plexity and coordination required among the agents. In order to assess our
methodology, we observe the performance of the trained policies on 20 sim-
ulated test episodes with 50 time-steps each. Both the lead time and de-
mand uncertainty remained constant throughout the test episodes and each
methodology. All methodologies follow the CTDE framework where agents
only require local information at execution. The execution of the MARL
algorithms in these scenarios was assessed through performance metrics in-
cluding cumulative reward, backlog and inventory levels.
23CombinedAgents’
Observations(Local+Global)
GNNAggregated
AgentObservations(Global)
GNNMeanPool GNNMeanPool
(GlobalSummary) (GlobalSummary)
IndividualAgent’s
Observation(Local)
IPPO MAPPO G-MAPPO P-GCN-MAPPO NoisyP-GCN-MAPPO
MARLMethod
Figure 8: Qualitative representation of varying levels of information input to the value
function. The different levels illustrate how changes in observability can affect perfor-
mance.
25000
Static(s,S)Heuristic
SingleAgentPPO
IPPO
MAPPO
G-MAPPO
P-GCN–MAPPO
20000 NoisyP-GCN-MAPPO
15000
10000
5000
0
6 12 18 24
NumberofAgents
Figure 9: Cumulative Profit (with Standard Deviation) for the different methodologies
across the four different supply chain configurations
24
noitamrofnIfoleveL
tfiorPevitalumuCFirstly, the bar graph shown in Figure 9 compares the cumulative profit
for the different methodologies in the different supply chain network config-
urations. Notably, the proposed methodology, Noisy P-GCN-MAPPO, has
on par performance or outperforms the other methodologies. This advan-
tage becomes increasingly pronounced as the number of agents rises. This
trend suggests that Noisy P-GCN-MAPPO may be particularly effective in
large-scale, multi-agent inventory control scenarios. The decrease in cumula-
tive profit from the 18 agent to 24 agent configuration can be attributed to
differences in number of retailers present and the specific environment config-
uration chosen. Figure 9 also shows that while the static (si ,Si ) policy
tinv,s t
ord,s
achieves similar cumulative profit performance to Noisy P-GCN-MAPPO, it
exhibits a significantly higher standard deviation. This variability suggests
that the static heuristic is less stable, potentially hindering its robustness in
situations with unexpected changes to the environment.
This distinction is crucial in real-world applications, where supply chain
networks and other multi-agent systems inevitably encounter unexpected
events. RL’s ability to maintain good performance in uncertain environ-
ments is one of the key advantages compared to traditional methods. There-
fore, the results highlight the strength of RL methods, particularly Noisy
P-GCN-MAPPO, as its lower standard deviation suggests greater potential
for robustness and adaptability in uncertain environments.
Figure 10a, illustrates the median backlog for different algorithms. The
static (si ,Si ) heuristic exhibits a significant increase in backlog as the
tinv,s t
ord,s
number of agents rises. This trend suggests potential difficulties in handling
the growing complexity of the environment with more agents (e.g., increased
number of interactions, coordination challenges). The static nature of the in-
ventory policy in the static (si ,Si ) heuristic might also contribute, as
tinv,s t
ord,s
it cannot adapt to changing environmental conditions. In contrast, our pro-
posed methodology addresses this limitation by redefining the action space
to parametrize a heuristic policy. This allows our policy to be more adaptive
and adjust its behavior based on the current state of the environment, po-
tentially leading to better handling of increased complexity and reducing the
backlog. However, from Figure 10b, only minimal differences were present in
the final on-hand inventory across methodologies.
Our methodology is also compared with other MARL methods. The
notable difference between them is the information available to the critic.
One key challenge in MARL is balancing access to global information (miti-
gating non-stationarity) with scalability limitations of naively combining all
25800 S S I M G P NPt i -- on APa GM igt sPO Ci l yAc e P NP P( OA –s P -Mg, GOS e CA) n Nt PH -P Pe Mu P Or AOis Pti Pc O 11 57 05 00 S S I M G P NPt i -- on APa GM igt sPO Ci l yAc e P NP P( OA –s P -Mg, GOS e CA) n Nt PH -P Pe Mu P Or AOis Pti Pc O
1250
600
1000
400 750
500
200
250
0 6 12 NumberofAgents 18 24 0 6 12 NumberofAgents 18 24
(a)Backlog (b)Inventory
Figure 10: Median metric along with standard deviations for the different methodologies
and configurations
agent states. Our proposed methodologies, P-G-MAPPO and Noisy P-GCN-
MAPPO,addressthisbyleveraginggraph-basedapproachesandtheinherent
structural properties of the problem. This allows agents to exploit both local
and global information effectively.
Figure 11 shows how IPPO consistently outperforms MAPPO, and this
performance gap widens with a greater number of agents. This motivated
the development of smarter methods of information aggregation, by lever-
aging graph-based approaches, where information can be harnessed whilst
not suffering from performance shortcomings. This highlights the need for
smarter information aggregation. G-MAPPO, P-GCN-MAPPO and Noisy
P-GCN-MAPPOincorporateGraphNeuralNetworks(GNNs)toenablecom-
munication and information sharing while maintaining focus on local details.
However, from Figure 11, the G-MAPPO method does not always out-
perform MAPPO and IPPO methods. This may occur for similar reasons
why IPPO outperforms MAPPO. Too much information may be captured
leading to the policies overfitting. Therefore, P-GCN-MAPPO outperforms
G-MAPPO, MAPPO and IPPO in all 4 case studies. This occurs as the
integration of a pooling mechanism, can help aggregate information from
different parts of the graph, reducing the dimensionality of the data acting
as a regularizer and potentially reducing overfitting. The message passing
inherent in graph neural networks allows for agents to inherently communi-
cate with each other whilst the pooling mechanism prevents overfitting of the
policies. Despite losing local information in the critic, the global mean pool-
ing mechanism allows for global information to be captured in the central
26
golkcaBnaideM yrotnevnIegarevAcritic that helps reduce the variance in the individual actors that are trained
with local information.
Our final proposed methodology hypothesizes that the addition of noise
into the value function, propagates variability into the advantage function
potentially reducing overfitting. The execution curves shown in Figure 11
show the impact of adding noise to the value function on our methodology’s
performance. Theresultsshowthatwithasmallnumberofagents(6agents),
adding noise into the value function does not outperform P-GCN-MAPPO
compared to when the number of agents is larger (24 agents). When the
number of agents is 6, the overall state space is smaller compared to a large
agent system. Therefore, the addition of noise will just add unnecessary
randomness which could disrupt the learning process as opposed to benefit
the training of the policies. However, as the number of agents increases, it
becomes harder for traditional MARL algorithms to explore the entire space
efficiently. Therefore, large agent systems are more prone to getting stuck
in local optima. The addition of noise to the value function encourages the
agents to explore a wider range of actions, leading towards better solutions.
The addition of noise can also be seen as regularization, where the policies
become more robust to the complex environment.
Moreover, it is useful to compare the change in entropy during the train-
ing phase for each of the different methods. In the context of MARL, entropy
quantifies the uncertainty or randomness in the policy’s action distribution
(Sutton, 2018). During the training process, as the agents learn, the entropy
in the policy distribution decreases as the agents learn better policies. Fig-
ure 12 shows that the rate of entropy decrease differs between the different
methods. The addition of noise to the value function introduces controlled
variability which propagates into the advantage estimates. This helps main-
tain a higher entropy for a longer period during training which is desirable
as high entropy promotes exploration. During MARL training, agents need
to balance exploring new actions and exploiting known good actions. By
exploring a wider range of actions (due to high entropy), agents are more
likely to discover better strategies and avoid getting stuck in sub-optimal
solutions. This can be especially important in complex environments where
the best course of action depends on what other agents are doing. Figure 12
shows that Noisy P-GCN-MAPPO, maintains a higher entropy throughout
training, promoting exploration and reducing policy overfitting leading to
superior performance.
27I MP AP PO PO 15000 I MP AP PO PO
4000 G-MAPPO G-MAPPO
P-GCN-MAPPO P-GCN-MAPPO
NoisyP-GCN-MAPPO 12500 NoisyP-GCN-MAPPO
3000
10000
2000 7500
1000 5000
2500
0
0
−1000
0 10 20 Period 30 40 50 0 10 20 Period 30 40 50
(a)6nodes (b)12nodes
IPPO 20000 IPPO
MAPPO MAPPO
G-MAPPO G-MAPPO
20000 P-GCN-MAPPO P-GCN-MAPPO
NoisyP-GCN-MAPPO NoisyP-GCN-MAPPO
15000
15000
10000
10000
5000
5000
0
0
−5000
0 10 20 Period 30 40 50 0 10 20 Period 30 40 50
(c)18nodes (d)24nodes
Figure 11: Execution curves for the 4 different supply chain configurations.
IPPO
MAPPO
2.75 G-MAPPO
P-GCN-MAPPO
NoisyP-GCN-MAPPO
2.50
2.25
2.00
1.75
1.50
1.25
0 10 20 Training30Iteration 40 50 60
Figure12: Figureshowsthechangeinentropyduringthetrainingprocessforthedifferent
methods in the 18 agent configuration
28
tfiorPevitalumuCegarevA
tfiorPevitalumuCegarevA
yportnE
tfiorPevitalumuCegarevA
tfiorPevitalumuCegarevA4.2. Sensitivity to Gaussian Noise
In addition to acting as a regularizer, the incorporation of Gaussian Noise
into the value function of MARL algorithms facilitates a balance between
effective exploration and exploitation, a well known phenomena in the field
of reinforcement learning. The following section will explore this sensitivity
to noise intensity, examining how both insufficient and excessive noise can
negatively impact the performance of MARL agents.
The standard deviation values were changed from 0.0, 0.1, 0.2, 0.5, 1.0
and 2.0. Figure 13a shows the cumulative profit of the agents at execution of
the trained policies. The performance increases up to a standard deviation of
0.5. After which, when the noise intensity increases, the overall performance
decreases. This occurs as when the standard deviation of the Gaussian distri-
bution is small, there’s insufficient noise, leading to a lack of regularization.
This can result in overfitting, limited exploration, and a tendency to settle
into local optima. This is particularly detrimental in scenarios with large
state spaces, where effective exploration becomes crucial for agents to learn
optimal policies. Conversely, when the standard deviation is 1.0 and 2.0 the
overall performance becomes worse as seen in Figure 13a. This occurs when
the noise intensity is too high, excessive noise can introduce significant ran-
domness, increasing the variance in the value function which disrupts the
learning process which may hinder convergence.
Moreover, the entropy during training is compared in Figure 13b for the
noise value that performed best [0.5], the noise value that performed worst
[0.1]andthebasecasescenario[0.0]. Theresultsshowthatusinganoiselevel
with standard deviation = 0.5 leads to a higher final entropy compared to
the scenario with no noise (standard deviation = 0.0). However, throughout
the training process, when the noise intensity is high (standard deviation =
2.0), the entropy remains higher, emphasizing that this noise has introduced
significant randomness leading to suboptimal policies.
Therefore, finding a level of noise becomes important, as it can act as a
regularizer, promoting exploration in complex environments while maintain-
ing a level of stability that allows for effective learning. This delicate balance
is particularly important in settings with a large number of agents, where
the vast state space and intricate interactions require a measured approach
to exploration via noise injection.
29(a)Executioncurvesatdifferentlevelsofnoise (b)Changeinentropyfrom3differentnoiselevels
Figure 13: Sensitivity of noise intensity was performed for the 18 agent configuration
4.3. Scalability
This section explores the scalability of the different methodologies exam-
ined in this paper.
Firstly, Figure 14, depicts the training curves for 60 iterations across the
four different configurations consisting of 6, 12, 18 and 24 agents. As the
number of agents increases, it can be seen that it takes slightly longer to
reach convergence due to the increased complexity of the learning process.
However, thefigurealsoshowsthatthepoliciesforallconfigurationsconverge
within the 60 iterations. Figure 14 also shows that despite the injection of
noise in the value function, this does not cause instability in training due to
the inherent clipping mechanism in PPO which ensures the policy update
stays within a certain limit, maintaining stability.
Moreover, Figure 15, compares the mean training time per iteration in
seconds both in terms of methodologies and number of agents. As expected,
as the number of agents increases, the training time per iteration increases
due to the increased complexity of the learning process. The methodologies
leveraging the graph structure also have a higher mean training time per
iteration because they require complex message passing between agents in
the graph. G-MAPPO, P-GCN-MAPPO, Noisy P-GCN-MAPPO utilize a
GNN which requires training alongside the policies. Therefore, extracting
and processing information from the graph structure itself adds computa-
tional overhead compared to simpler MARL approaches. As the number of
agents increases, the training time per iteration increases at a faster rate
for the methodologies using GNNs as the communication and information
processing becomes increasingly complex as the number of agents and com-
304000
10000
2000
0
0
−10000
−2000
−20000
−4000 I MP AP PO
PO
−30000 I MP AP PO
PO
G-MAPPO G-MAPPO
P-GCN-MAPPO P-GCN-MAPPO
−6000
0 10 20 Itera3t0ion 40
N 5o 0isyP-GCN-MAPP 6O
0
−40000
0 10 20 Itera3t0ion 40
N 5o 0isyP-GCN-MAPP 6O
0
(a)6agents (b)12agents
10000
0
0
−10000 −20000
−20000
−40000
−30000
−40000
−60000
IPPO IPPO
MAPPO MAPPO
−50000 G P-- GM CA NP -P MO
APPO
−80000 G P-- GM CA NP -P MO
APPO
NoisyP-GCN-MAPPO NoisyP-GCN-MAPPO
−60000
0 10 20 Itera3t0ion 40 50 60 0 10 20 Itera3t0ion 40 50 60
(c)18agents (d)24agents
Figure 14: Figure shows the training curves for 60 iterations for the 4 different configura-
tions
31
draweR
draweR
draweR
draweRplexity of the structure increases. However, Figure 15 shows that methods
with an integrated pooling mechanism have faster training times than those
without one. This further emphasises the scalability advantage of integrating
a pooling mechanism compared to a standard GNN architecture in a MARL
framework. Theintegrationofthepoolingmechanismreducesthedimension-
ality of the input to the central critic, reducing the computational overhead
which leads to faster training times. Therefore, as opposed to na¨ıvely con-
catenating information, MARL frameworks that employ smart information
aggregation techniques can offer several advantages: reduced computational
complexity, improved scalability and potential for improved performance by
efficientaggregationthatcanleadtobetterrepresentationoftheglobalstate.
6Agents
12Agents
700 18Agents
24Agents
600
500
400
300
200
100
IPPO MAPPO G-MAPPO P-GCN-MAPPO NoisyP-GCN-MAPPO
Methods
Figure 15: Box plot showing the mean training time per iteration (s) along with the
associated standard deviation. Note: a z-test score was performed to remove anomalies of
values which were 2 standard deviations greater than the mean value.
5. Conclusions and Future Work
In this work, we propose a new methodology that develops a decentral-
ized decision-making framework for inventory management. Our framework
leveragestheinherentgraphstructureandreducespolicyoverfittingbyGaus-
sian noise injection in the value function. Our approach also offers several
advantagesthatenhancetheefficiencyandeffectivenessofinventorysystems.
Firstly, our approach redefines the action space by parameterizing a
known heuristic inventory control policy which is often discontinuous in na-
ture. Unliketraditionalheuristicmethodswherethepolicyisnotconditioned
on the state of the system, leveraging an RL approach means the heuristic
32
)s(noitaretIrePemiTgniniarTnaeMis conditioned on the state of the system. This not only allows for a more
flexible decision-making framework but ensures earlier adoption of novel op-
timization techniques in industry due to its interpretability.
Secondly, our methodology overcomes information sharing constraints at
an online level but trains the control policies in a collaborative framework.
This ensures effective coordination and communication is present within the
different entities of the inventory management system. The communication
between entities is further enhanced by leveraging the inherent graph struc-
ture of a supply chain. This also shifts the costs from online to offline,
resulting in a more efficient stochastic optimal control policy. The method-
ology results in a closed-loop solution rather than an open-loop optimization
problem which enhances fast, real-time decision-making capabilities.
Overall we show that the addition of noise, particularly in a system with
a larger number of agents, reduces policy overfitting. We emphasize the
importance of balancing effective exploration and exploitation by performing
a sensitivity analysis on the noise intensity introduced in the value function.
Our results show that too little noise leads to insufficient exploration whilst
too much noise will increase the variance in the value function, disrupting
the learning process. One should note that the amount of noise added will
vary on the complexity of the environment and the number of agents present
in the system.
In summary, as opposed to na¨ıvely concatenating information, MARL
frameworks that leverage information aggregation techniques can offer sev-
eraladvantages: reducedcomputationalcomplexity, improvedscalabilityand
potential for improved performance by efficient aggregation that can lead to
better representation of the global state.
Future work will focus on integrating attention networks that weigh the
importance of neighboring nodes which will allow the frameworks to retain
more local, spatial information. The work will also be expanded to more
complex systems with a larger number of products and a non-stationarity
demand to mimic real-world conditions.
Thecodesareavailableat: Thecodesareavailableat: github.com/OptiMaL-
PSE-Lab/MultiAgentRL InventoryControl.
Acknowledgements
Niki Kotecha acknowledges support from I-X at Imperial College London.
33References
Aggarwal, S.C., 1974. A review of current inventory theory and its
applications. International Journal of Production Research 12, 443–
482. URL: https://doi.org/10.1080/00207547408919568, doi:10.
1080/00207547408919568.
de Almeida, M.M.K., Marins, F.A.S., Salgado, A.M.P., Santos, F.C.A.,
da Silva, S.L., 2015. Mitigation of the bullwhip effect considering trust
and collaboration in supply chain management: a literature review. The
InternationalJournalofAdvancedManufacturingTechnology77, 495–513.
Bellman, R., 1952. On the theory of dynamic programming. Proceedings of
the national Academy of Sciences 38, 716–719.
Berovic, D.P., Vinter, R.B., 2004. The application of dynamic programming
to optimal inventory control. IEEE Transactions on automatic control 49,
676–685.
Bertsimas, D., Sim, M., Zhang, M., 2019. Adaptive distributionally robust
optimization. Management Science 65, 604–618.
Bharti, S., Kurian, D.S., Pillai, V.M., 2020a. Reinforcement learning for in-
ventory management, in: Innovative Product Design and Intelligent Man-
ufacturing Systems: Select Proceedings of ICIPDIMS 2019, Springer. pp.
877–885.
Bharti, S., Kurian, D.S., Pillai, V.M., 2020b. Reinforcement learning for in-
ventory management, in: Innovative Product Design and Intelligent Man-
ufacturing Systems: Select Proceedings of ICIPDIMS 2019, Springer. pp.
877–885.
Boute, R.N., Gijsbrechts, J., Van Jaarsveld, W., Vanvuchelen, N., 2022.
Deep reinforcement learning for inventory control: A roadmap. European
Journal of Operational Research 298, 401–412.
Burtea, R., Tsay, C., 2024. Constrained continuous-action reinforcement
learning for supply chain inventory management. Computers & Chemical
Engineering 181, 108518.
34Chaharsooghi, S.K., Heydari, J., Zegordi, S.H., 2008. A reinforcement learn-
ing model for supply chain ordering management: An application to the
beer game. Decision Support Systems 45, 949–959.
Clark, A.J., Scarf, H., 1960. Optimal policies for a multi-echelon inventory
problem. Management science 6, 475–490.
Dehaybe,H.,Catanzaro,D.,Chevalier,P.,2024. Deepreinforcementlearning
for inventory optimization with non-stationary uncertain demand. Euro-
pean Journal of Operational Research 314, 433–445.
Edgeworth, F.Y., 1888. The mathematical theory of banking. Journal of the
Royal Statistical Society 51, 113–127.
Erlenkotter, D., 1990. Ford whitman harris and the economic order quantity
model. Operations Research 38, 937–946.
Federgruen, A., Zheng, Y.S., 1992. An efficient algorithm for computing
an optimal (r, q) policy in continuous review stochastic inventory sys-
tems. Operations Research 40, 808–813. URL: https://doi.org/10.
1287/opre.40.4.808, doi:10.1287/opre.40.4.808.
Feng, L., Xie, Y., Liu, B., Wang, S., 2022a. Multi-level credit assignment
for cooperative multi-agent reinforcement learning. Applied Sciences 12,
6938.
Feng, M., Liu, G., Zhao, L., Song, L., Bian, J., Qin, T., Zhou, W., Li, H.,
Liu, T.Y., 2022b. Multi-agent reinforcement learning with shared resource
in inventory management. URL: https://openreview.net/forum?id=
-uZp67PZ7p.
Fujimoto, S., Hoof, H., Meger, D., 2018. Addressing function approxima-
tion error in actor-critic methods, in: International conference on machine
learning, PMLR. pp. 1587–1596.
Giannoccaro, I., Pontrandolfo, P., 2002. Inventory management in supply
chains: a reinforcement learning approach. International Journal of Pro-
duction Economics 78, 153–161.
Grossmann, I.E., Apap, R.M., Calfa, B.A., Garc´ıa-Herreros, P., Zhang, Q.,
2016. Recent advances in mathematical programming techniques for the
35optimizationofprocesssystemsunderuncertainty. Computers&Chemical
Engineering 91, 3–14.
Hernandez-Leal, P., Kartal, B., Taylor, M.E., 2019. A survey and critique of
multiagent deep reinforcement learning. Autonomous Agents and Multi-
Agent Systems 33, 750–797.
Hu, J., Hu, S., Liao, S.w., 2021. Policy regularization via noisy advantage
values for cooperative multi-agent actor-critic methods. arXiv preprint
arXiv:2106.14334 .
Jackson, I., Tolujevs, J., Kegenbekov, Z., 2020. Review of inventory con-
trol models: A classification based on methods of obtaining optimal con-
trol parameters. Transport and Telecommunication Journal 21, 191–
202. URL: https://doi.org/10.2478/ttj-2020-0015, doi:10.2478/
ttj-2020-0015.
Janssens, G.K., Ramaekers, K.M., 2011. A linear programming formulation
for an inventory management decision problem with a service constraint.
Expert Systems with Applications 38, 7929–7934.
Kara, A., Dogan, I., 2018. Reinforcement learning approaches for specifying
ordering policies of perishable inventory systems. Expert Systems with
Applications 91, 150–158.
Katanyukul, T., Duff, W.S., Chong, E.K., 2011. Approximate dynamic pro-
gramming for an inventory problem: Empirical comparison. Computers &
Industrial Engineering 60, 719–743.
Khalil, E., Dai, H., Zhang, Y., Dilkina, B., Song, L., 2017. Learningcombina-
torialoptimizationalgorithmsovergraphs. Advancesinneuralinformation
processing systems 30.
Khirwar, M., Gurumoorthy, K.S., Jain, A.A., Manchenahally, S., 2023. Co-
operative multi-agent reinforcement learning for inventory management.
URL: https://arxiv.org/abs/2304.08769, doi:10.48550/ARXIV.2304.
08769.
Ku¨¸cu¨kyavuz, S., 2011. Mixed-integer optimization approaches for determin-
isticandstochasticinventorymanagement, in: TransformingResearchinto
Action. INFORMS, pp. 90–105.
36Liang, E., Liaw, R., Nishihara, R., Moritz, P., Fox, R., Goldberg, K., Gon-
zalez, J., Jordan, M., Stoica, I., 2018. Rllib: Abstractions for distributed
reinforcement learning, in: International conference on machine learning,
PMLR. pp. 3053–3062.
Liu, I.J., Jain, U., Yeh, R.A., Schwing, A., 2021. Cooperative exploration
for multi-agent deep reinforcement learning, in: International conference
on machine learning, PMLR. pp. 6826–6836.
Liu, Q., Chung, A., Szepesva´ri, C., Jin, C., 2022a. When is partially observ-
ablereinforcementlearningnotscary?, in: ConferenceonLearningTheory,
PMLR. pp. 5175–5220.
Liu, X., Hu, M., Peng, Y., Yang, Y., 2022b. Multi-agent deep reinforcement
learning for multi-echelon inventory management. SSRN Electronic Jour-
nal URL: https://doi.org/10.2139/ssrn.4262186, doi:10.2139/ssrn.
4262186.
Lowe, R., Wu, Y.I., Tamar, A., Harb, J., Pieter Abbeel, O., Mordatch, I.,
2017. Multi-agent actor-critic for mixed cooperative-competitive environ-
ments. Advances in neural information processing systems 30.
Lyu, X., Baisero, A., Xiao, Y., Daley, B., Amato, C., 2023. On centralized
critics in multi-agent reinforcement learning. Journal of Artificial Intelli-
gence Research 77, 295–354.
Mazyavkina, N., Sviridov, S., Ivanov, S., Burnaev, E., 2021. Reinforcement
learning for combinatorial optimization: A survey. Computers & Opera-
tions Research 134, 105400.
Meisheri, H., Baniwal, V., Sultana, N.N., Khadilkar, H., Ravindran, B.,
2020. Using reinforcement learning for a large variable-dimensional inven-
tory management problem, in: Adaptive Learning Agents Workshop at
AAMAS, pp. 1–9.
Mousa, M., van de Berg, D., Kotecha, N., del Rio-Chanona, E.A., Mowbray,
M., 2023. An analysis of multi-agent reinforcement learning for decentral-
ized inventory control systems. arXiv preprint arXiv:2307.11432 .
Munikoti, S., Agarwal, D., Das, L., Halappanavar, M., Natarajan, B., 2022.
Challenges and opportunities in deep reinforcement learning with graph
37neural networks: A comprehensive review of algorithms and applications.
arXiv preprint arXiv:2206.07922 .
Nayak, S., Choi, K., Ding, W., Dolan, S., Gopalakrishnan, K., Balakrishnan,
H., 2023. Scalable multi-agent reinforcement learning through intelligent
information aggregation, in: International Conference on Machine Learn-
ing, PMLR. pp. 25817–25833.
Nekoei, H., Badrinaaraayanan, A., Sinha, A., Amini, M., Rajendran, J.,
Mahajan, A., Chandar, S., 2023. Dealing with non-stationarity in de-
centralized cooperative multi-agent deep reinforcement learning via multi-
timescale learning, in: Conference on Lifelong Learning Agents, PMLR.
pp. 376–398.
Oroojlooyjadid, A., Nazari, M., Snyder, L.V., Tak´aˇc, M., 2022. A deep
q-network for the beer game: Deep reinforcement learning for inventory
optimization. Manufacturing & Service Operations Management 24, 285–
304.
Perez, H.D., Hubbs, C.D., Li, C., Grossmann, I.E., 2021. Algorithmic ap-
proaches to inventory management optimization. Processes 9, 102.
Qiu, R., Sun, Y., Sun, M., 2021. A distributionally robust optimization
approach for multi-product inventory decisions with budget constraint and
demand and yield uncertainties. Computers & Operations Research 126,
105081.
Rangel-Martinez, D., Ricardez-Sandoval, L.A., 2024. A recurrent reinforce-
ment learning strategy for optimal scheduling of partially observable job-
shop and flow-shop batch chemical plants under uncertainty. Computers
& Chemical Engineering , 108748.
Siems, J., Schambach, M., Schulze, S., Otterbach, J.S., 2023. Interpretable
reinforcement learning via neural additive models for inventory manage-
ment. arXiv preprint arXiv:2303.10382 .
Silver, E.A., Pyke, D.F., Peterson, R., et al., 1998. Inventory management
and production planning and scheduling. volume 3. Wiley New York.
Stranieri, F., Stella, F., 2022. A deep reinforcement learning approach to
supply chain inventory management. arXiv: 2204.09603 .
38Sultana, N.N., Meisheri, H., Baniwal, V., Nath, S., Ravindran, B., Khadilkar,
H., 2020. Reinforcement learning for multi-product multi-node inven-
tory management in supply chains. URL: https://arxiv.org/abs/2006.
04037, doi:10.48550/ARXIV.2006.04037.
Sutton, R.S., 2018. Reinforcement learning: An introduction. A Bradford
Book .
Tampuu, A., Matiisen, T., Kodelja, D., Kuzovkin, I., Korjus, K., Aru, J.,
Aru, J., Vicente, R., 2017. Multiagent cooperation and competition with
deep reinforcement learning. PloS one 12, e0172395.
Tan, M., 1993. Multi-agent reinforcement learning: Independent vs. coop-
erative agents, in: Proceedings of the tenth international conference on
machine learning, pp. 330–337.
Uehara, M., Sekhari, A., Lee, J.D., Kallus, N., Sun, W., 2022. Provably
efficient reinforcement learning in partially observable dynamical systems.
Advances in Neural Information Processing Systems 35, 578–592.
Vera, J.M., 2021. Inventory control under unobserved losses with latent
state learning, in: 2021 7th International Conference on Computer and
Communications (ICCC), IEEE. pp. 1594–1599.
Virtanen, P., Gommers, R., Oliphant, T.E., Haberland, M., Reddy, T., Cour-
napeau, D., Burovski, E., Peterson, P., Weckesser, W., Bright, J., van
der Walt, S.J., Brett, M., Wilson, J., Millman, K.J., Mayorov, N., Nel-
˙
son, A.R.J., Jones, E., Kern, R., Larson, E., Carey, C.J., Polat, I., Feng,
Y., Moore, E.W., VanderPlas, J., Laxalde, D., Perktold, J., Cimrman,
R., Henriksen, I., Quintero, E.A., Harris, C.R., Archibald, A.M., Ribeiro,
A.H., Pedregosa, F., vanMulbregt, P., SciPy1.0Contributors, 2020. SciPy
1.0: Fundamental Algorithms for Scientific Computing in Python. Nature
Methods 17, 261–272. doi:10.1038/s41592-019-0686-2.
Yarahmadi, H., 2023. Improving the credit assignment problem in multi-
agent systems. Ph.D. thesis. University of Antwerp.
You, F., Grossmann, I.E., 2008. Mixed-integer nonlinear programming mod-
els and algorithms for large-scale supply chain design with stochastic in-
ventory management. Industrial & Engineering Chemistry Research 47,
7802–7817.
39You, F., Grossmann, I.E., 2011. Stochastic inventory management for tac-
tical process planning under uncertainties: Minlp models and algorithms.
AIChE Journal 57, 1250–1277.
Yu, C., Velu, A., Vinitsky, E., Gao, J., Wang, Y., Bayen, A., Wu, Y., 2022.
The surprising effectiveness of ppo in cooperative multi-agent games. Ad-
vances in Neural Information Processing Systems 35, 24611–24624.
Zhang, C., Song, W., Cao, Z., Zhang, J., Tan, P.S., Chi, X., 2020a. Learn-
ing to dispatch for job shop scheduling via deep reinforcement learning.
Advances in Neural Information Processing Systems 33, 1621–1632.
Zhang, C., Song, W., Cao, Z., Zhang, J., Tan, P.S., Chi, X., 2020b. Learn-
ing to dispatch for job shop scheduling via deep reinforcement learning.
Advances in Neural Information Processing Systems 33, 1621–1632.
Zhou, M., Liu, Z., Sui, P., Li, Y., Chung, Y.Y., 2020. Learning implicit credit
assignment for cooperative multi-agent reinforcement learning. Advances
in neural information processing systems 33, 11853–11864.
40Appendix A. Hyperparameter Values
The table below summarizes the hyperparameter values for the MARL
algorithms. These were kept consistent. It is worth noting these were the
predefined values used in the Ray RLlib library (Liang et al., 2018).
Table A.2: MARL hyperparameter values.
Hyperparameter Value range
Learning rate, α 1 10−5
×
Clip parameter, ϵ 0.3
Discount factor, γ 0.99
GAE parameter, λ 1.0
Initial KL coefficient, β 0.2
KL target, d 0.01
targ
Batch Size 4000
|D|
Train Batch Size 32
Epochs 60
FC1 size 128
FC2 size 128
Appendix B. Supply Chain Network Environment Parameters
The tables below shows the environment parameters for the inventory
management system.
ConnectedNodes
Node NodeCosts NodePrices MaxInventory MaxOrder InitialInventory TargetInventory StockCosts BacklogCosts
(Downstream)
0 0.5 4.0 100 100 100 10 0.5 2.5 1,2
1 1.0 6.0 100 100 100 10 0.5 2.5 3,4
2 1.0 6.0 100 100 100 10 0.5 2.5 4,5
3 1.5 8.0 100 100 100 10 0.5 2.5 None
4 1.5 8.0 100 100 100 10 0.5 2.5 None
5 1.5 8.0 100 100 100 10 0.5 2.5 None
Table B.3: Summary of 6 Supply Chain Node Parameters Including Costs, Prices, Inven-
tory Targets, Maximum Capacities and Downstream Connectivity
41ConnectedNodes
Node NodeCosts NodePrices MaxInventory MaxOrder InitialInventory TargetInventory StockCosts BacklogCosts
(Downstream)
0 0.5 4.0 100 100 100 10 0.5 2.5 1,2
1 1.0 6.0 100 100 100 10 0.5 2.5 3,4
2 1.0 6.0 100 100 100 10 0.5 2.5 5,6
3 1.5 8.0 100 100 100 10 0.5 2.5 7,8
4 1.5 8.0 100 100 100 10 0.5 2.5 9
5 1.5 8.0 100 100 100 10 0.5 2.5 10,11
6 1.5 8.0 100 100 100 10 0.5 2.5 None
7 2.0 10.0 100 100 100 10 0.5 2.5 None
8 2.0 10.0 100 100 100 10 0.5 2.5 None
9 2.0 10.0 100 100 100 10 0.5 2.5 11
10 2.0 10.0 100 100 100 10 0.5 2.5 11
11 2.0 10.0 100 100 100 10 0.5 2.5 None
Table B.4: Summary of 12 Supply Chain Node Parameters Including Costs, Prices, In-
ventory Targets, Maximum Capacities and Downstream Connectivity
ConnectedNodes
Node NodeCosts NodePrices MaxInventory MaxOrder InitialInventory TargetInventory StockCosts BacklogCosts
(Downstream)
0 0.5 4.0 100 100 100 10 0.5 2.5 1,2
1 1.0 6.0 100 100 100 10 0.5 2.5 3,4
2 1.0 6.0 100 100 100 10 0.5 2.5 5,6
3 1.5 8.0 100 100 100 10 0.5 2.5 7,8
4 1.5 8.0 100 100 100 10 0.5 2.5 9
5 1.5 8.0 100 100 100 10 0.5 2.5 10
6 1.5 8.0 100 100 100 10 0.5 2.5 None
7 2.0 10.0 100 100 100 10 0.5 2.5 11,12,13
8 2.0 10.0 100 100 100 10 0.5 2.5 12
9 2.5 12.0 100 100 100 10 0.5 2.5 14
10 2.5 12.0 100 100 100 10 0.5 2.5 15
11 2.5 12.0 100 100 100 10 0.5 2.5 16,17
12 2.5 12.0 100 100 100 10 0.5 2.5 None
13 2.5 12.0 100 100 100 10 0.5 2.5 17
14 2.5 12.0 100 100 100 10 0.5 2.5 17
15 3.0 14.0 100 100 100 10 0.5 2.5 None
16 3.0 14.0 100 100 100 10 0.5 2.5 None
17 3.0 14.0 100 100 100 10 0.5 2.5 None
Table B.5: Summary of 18 Supply Chain Node Parameters Including Costs, Prices, In-
ventory Targets, Maximum Capacities and Downstream Connectivity
ConnectedNodes
Node NodeCosts NodePrices MaxInventory MaxOrder InitialInventory TargetInventory StockCosts BacklogCosts
(Downstream)
0 0.5 4 100 100 100 10 0.5 2.5 1,2
1 1.0 6 100 100 100 10 0.5 2.5 3,4
2 1.0 6 100 100 100 10 0.5 2.5 5,6
3 1.5 8 100 100 100 10 0.5 2.5 7,8
4 1.5 8 100 100 100 10 0.5 2.5 9
5 1.5 8 100 100 100 10 0.5 2.5 10
6 1.5 8 100 100 100 10 0.5 2.5 None
7 2.0 10 100 100 100 10 0.5 2.5 11,12,13
8 2.0 10 100 100 100 10 0.5 2.5 12
9 2.0 10 100 100 100 10 0.5 2.5 14
10 2.0 10 100 100 100 10 0.5 2.5 15
11 2.5 12 100 100 100 10 0.5 2.5 16,17
12 2.5 12 100 100 100 10 0.5 2.5 None
13 2.5 12 100 100 100 10 0.5 2.5 17
14 2.5 12 100 100 100 10 0.5 2.5 17,18
15 2.5 12 100 100 100 10 0.5 2.5 19
16 3.0 14 100 100 100 10 0.5 2.5 20
17 3.0 14 100 100 100 10 0.5 2.5 20,21
18 3.0 14 100 100 100 10 0.5 2.5 22
19 3.0 14 100 100 100 10 0.5 2.5 22,23
20 3.5 16 100 100 100 10 0.5 2.5 None
21 3.5 16 100 100 100 10 0.5 2.5 None
22 3.5 16 100 100 100 10 0.5 2.5 None
23 3.5 16 100 100 100 10 0.5 2.5 None
Table B.6: Summary of 24 Supply Chain Node Parameters Including Costs, Prices, In-
ventory Targets, Maximum Capacities and Downstream Connectivity
42