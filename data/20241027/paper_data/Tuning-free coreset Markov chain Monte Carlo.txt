Tuning-free coreset Markov chain Monte Carlo
Naitong Chen Jonathan H. Huggins Trevor Campbell
Department of Statistics Department of Mathematics & Statistics Department of Statistics
University of British Columbia Boston University University of British Columbia
naitong.chen@stat.ubc.ca huggins@bu.edu trevor@stat.ubc.ca
Abstract
A Bayesian coreset is a small, weighted sub-
set of a data set that replaces the full data
duringinferencetoreducecomputationalcost.
The state-of-the-art coreset construction al-
gorithm, Coreset Markov chain Monte Carlo
(Coreset MCMC), uses draws from an adap-
tiveMarkovchaintargeting the coresetposte-
riortotrainthecoresetweightsviastochastic
gradient optimization. However, the quality
of the constructed coreset, and thus the qual-
ity of its posterior approximation, is sensitive
to the stochastic optimization learning rate.
In this work, we propose a learning-rate-free
stochastic gradient optimization procedure,
Hot-start Distance over Gradient (Hot DoG), Figure 1: Relative Coreset MCMC posterior approxi-
fortrainingcoresetweightsinCoresetMCMC mation error (average squared coordinate-wise z-score)
without user tuning effort. Empirical results using ADAM with different learning rates versus the
demonstrate that Hot DoG provides higher proposed Hot DoG method (with fixed r =0.001). Me-
quality posterior approximations than other dian values after 200,000 optimization iterations across
learning-rate-free stochastic gradient meth- 10 trials are used for the relative comparison for a vari-
ods, and performs competitively to optimally- ety of datasets, models, and coreset sizes. Above the
tuned ADAM. horizontal black line (100) indicates that the proposed
Hot DoG method outperformed ADAM.
1 INTRODUCTION
Bayesian inference provides a flexible framework for weighted subset of data that replaces the full data
parameter estimationand uncertainty quantification in set during inference, leveraging the insight that large
statistical models. Markov chain Monte Carlo (Robert datasets often exhibit a significant degree of redun-
and Casella, 2004; Robert and Casella, 2011; Gelman dancy. With a carefully constructed coreset, one can
etal.,2013,Chs.11and12),thestandardmethodology significantly reduce the computational cost of infer-
for performing Bayesian inference, involves simulating ence while still obtaining samples from a high quality
carefully constructed Markov chains whose stationary approximation of the full Bayesian posterior. In fact,
distribution is the target Bayesian posterior. In the givenadatasetofN points, acoresetofsizeO(logN)
large-scale data setting, this procedure can become issufficientforprovidinganear-exactposteriorapprox-
prohibitivelyexpensive,asitrequiresiteratingoverthe imation in exponential family and other sufficiently
entire data set to simulate the next state. simple models (Naik et al., 2022, Thms. 4.1 and 4.2;
Chen et al., 2022, Prop. 3.1) and O(polylogN) is suffi-
Bayesian coresets (Huggins et al., 2016) are a pop-
cientformoregeneralcases(Campbell,2024, Cor.6.1).
ular approach for speeding up Bayesian inference in
the large-scale data setting. A Bayesian coreset is a Constructing a coreset involves picking the data points
4202
tcO
42
]OC.tats[
1v37981.0142:viXraTuning-free coreset Markov chain Monte Carlo
to include in the coreset and assigning each data point Algorithm 1 CoresetMCMC
its corresponding weight. The state-of-the-art method,
Require: θ , κ , S, M
0 w
Coreset MCMC (Chen and Campbell, 2024), selects
▷ Initialize coreset weights
coreset points by sampling them uniformly from the w = N, m=1,··· ,M
0m M
full data set, and learns the weights using stochastic
for t=0,...,T do
gradientoptimizationtechniques,e.g.,ADAM(Kingma
▷ Subsample the data
andBa,2014), wherethegradientsareestimatedusing
S ←Unif(S,[N]) (without replacement)
t
MCMC draws targeting the current coreset posterior.
▷ Compute gradient estimate
However, as we demonstrate in this paper, there are
gˆ ←g(w ,θ ,S ) (Eq. (4))
t t t t
two issues with this approach. First, the quality of
w ← stochastic gradient step(w ,gˆ)
t+1 t t
the constructed coreset is sensitive to the learning rate
▷ Step each Markov chain
of the stochastic optimization algorithm. And second,
for k =1,...,K do
gradient estimates using MCMC draws are affected
θ ∼κ (·|θ )
stronglyinearlyiterationsbyinitializationbias,leading
(t+1)k wt+1 tk
end for
to poor optimization performance.
end for
To address these challenges, we first propose Hot-
start Distance over Gradient (Hot DoG), a tuning-free
can then be written as
stochastic gradient optimization procedure that can be
used for learning coreset weights in Coreset MCMC. 1 (cid:32) (cid:88)M (cid:33)
π (θ):= exp w ℓ (θ) π (θ), (1)
Hot DoG is a stochastic gradient method combining w Z(w) m m 0
techniques from Do(W)G (Ivgi et al., 2023; Khaled m=1
et al., 2023), ADAM (Kingma and Ba, 2014), and where w ∈RM is a vector of coreset weights. Recent
+
RMSProp (Hinton et al., 2012) to set learning rates coresetconstructionmethodsuniformlyselectM points
automatically. Hot DoG also includes an automated to include in the coreset (Naik et al., 2022; Chen et al.,
warm-up phase prior to weight optimization, which 2022; Chen and Campbell, 2024), and then optimize
guardsagainstusageoflowqualityMCMCdrawswhen theweightsofthoseM pointsasavariationalinference
estimating the objective function gradients. Fig. 1 problem (Campbell and Beronov, 2019),
demonstrates that Hot DoG performs competitively to
w⋆ =argminD (π ||π) s.t. w ≥0, (2)
optimally-tuned ADAM across a wide range of models, KL w
w∈RM
datasets, and coreset sizes, and can be multiple orders
of magnitude more accurate than ADAM using other with objective function gradient
learning rates. Beyond the result in Fig. 1, we provide
∇ D (π ||π) (3)
an extensive empirical investigation of the reliability of w KL w
  
Hot DoG in comparison to other methods across many ℓ 1(θ)
. (cid:88) (cid:88)
different synthetic and real experiments. =Cov  . , w ℓ (θ)− ℓ (θ).
πw .  m m n 
ℓ (θ) m n
M
2 BACKGROUND
2.2 Coreset MCMC
2.1 Bayesian Coresets
The key challenge in solving Eq. (2) is that π does
w
not admit tractable i.i.d. draws, and so unbiased esti-
We are given a data set (X )N of N observations,
n n=1 mates of the gradient in Eq. (3) are not readily avail-
a log-likelihood ℓ := logp(x | θ) for observation n
n n
able. Coreset MCMC (Chen and Campbell, 2024) is
given θ ∈Θ, and a prior density π (θ). We would like
0
an adaptive algorithm that addresses this issue. The
to sample from the Bayesian posterior with density
method first initializes weights w ∈ RM and K ≥ 2
0
samples θ =(θ ,...,θ )∈ΘK. At iteration t∈N,
(cid:32) N (cid:33) 0 01 0K
1 (cid:88) given coreset weights w and samples θ ∈ΘK, it then
π(θ):= exp ℓ (θ) π (θ), t t
Z n 0 updates the weights w → w using the stochastic
t t+1
n=1
gradient estimate based on the draws θ ,
t
where Z is the unknown normalizing constant. A g(w ,θ ,S )= (4)
t t t
B tea ry mes sia wn itc hor aese wt er igep hl ta ec des suth mes ou vm ero ave sr uN bsl eo tg- ol fik se il zih eo Mod
, 1
(cid:88)K ℓ¯ 1(θ
.
.tk) (cid:32)
(cid:88) w ℓ¯ (θ )−N (cid:88) ℓ¯(θ
)(cid:33)
,
where M ≪N. Without loss of generality, we assume K−1  .  tm m tk S s tk
thatthesearethefirstM points. Thecoresetposterior k=1 ℓ¯ M(θ tk) m s∈StNaitong Chen, Jonathan H. Huggins, Trevor Campbell
given problem, the performance can vary by orders of
magnitudeasonevariesthelearningrate. Furthermore,
the default ADAM learning rate of 10−3 (Kingma and
Ba,2014)providespoorresultsinmostoftheproblems
tested. As a result, careful tuning of the learning rate
is required to obtain high quality posterior approxima-
tions. This usually involves a search on a log-scaled
grid, which is computationally wasteful as the results
for all but one of the parameter values are thrown out.
Moreover, in practice determining which learning rate
provides the best posterior approximation may not be
straightforward.
A number of recent works in the literature propose
learning-rate-free stochastic optimization methods to
addressthisissue(CarmonandHinder,2022;Ivgietal.,
2023; Khaled et al., 2023; Defazio and Mishchenko,
Figure 2: Coreset MCMC posterior approximation 2023; Mishchenko and Defazio, 2024). Many of these
error (average squared coordinate-wise z-score) using methods are shown empirically to work competitively
ADAM with different learning rates for a variety of compared to optimally-tuned SGD on a wide range
datasets, models, and coreset sizes. The lines indicate of large-scale, non-convex deep learning problems. Al-
median values after 200,000 optimization iterations though different at first glance, all of these methods
across 10 trials. arise from the same insight. Suppose one would like to
solve the stochastic optimization problem
where S t ⊆ [N] is a uniform subsample of indices of minE[f(x,ξ)],
size S, and ℓ¯ (θ ) = ℓ (θ )− 1 (cid:80)K ℓ (θ ). To x∈Rd
n tk n tk K j=1 n tj
completetheiteration,themethodupdatesthesamples
whereforallξ,f(·,ξ)isconvexandweonlyhaveaccess
by independently drawing θ ∼ κ (θ ,·) for
(t+1)k wt+1 tk
to unbiased stochastic gradient g =∂f(x ,ξ ). Define
each k ∈[K], where κ is a family of Markov kernels t t t
w the initial optimality gap d =∥x −x⋆∥ and the sum
that have invariant distribution π . The pseudocode 0 0
w of all gradient norms G = (cid:80) ∥g ∥2. By setting
for Coreset MCMC is outlined in Algorithm 1. T t≤T t
the SGD learning rate
3 TUNING-FREE CORESET MCMC d
γ⋆ = √ 0 ,
G
T
A key design choice when using Coreset MCMC is to
specify how gradient estimates are used to optimize theaverageiteratex¯= 1 (cid:80) x satisfiestheoptimal
T t≤T t
the weights. One can use ADAM (Kingma and Ba, error bound
2014),whichisusedasthedefaultoptimizerforCoreset
√
MCMC(ChenandCampbell,2024): atiterationt,with d G
E[f(x¯,ξ)]−E[f(x⋆,ξ)]≤ 0 T
γ t >0 being the user-specified learning rate, we set T
(cid:18) (cid:19)
mˆ
w ←proj w −γ √ t , after T iterations (Carmon and Hinder, 2022; Orabona
t+1 ≥0 t t vˆ t +ϵ andCutkosky,2020). Learning-rate-freemethodsthere-
fore essentially try to estimate or bound the initial
where mˆ and vˆ are exponential averages of past gra-
t t
dients (gˆ)t and their element-wise squares, and ϵ optimality gap d 0, which is unknown in practice. To
i i=0
the best of our knowledge, there are four state-of-the-
is a small constant. There are a wide range of other
art methods that do this in a manner that does not
first-order stochastic methods available that could be
require multiple optimization runs, knowledge of un-
used(e.g.,vanillastochasticgradientdescent,AdaGrad
known constants, or the ability to query the objective
(Duchi et al., 2011), etc.). However, like ADAM, most
function: DoG(Ivgietal.,2023),DoWG(Khaledetal.,
of these algorithms require setting a learning rate γ .
t
2023), D-Adaptation (Defazio and Mishchenko, 2023)
And as we show in Fig. 2, the quality of samples ob-
andprodigy(MishchenkoandDefazio,2024). DoGand
tained from Coreset MCMC can be highly sensitive to
DoWG run vanilla stochastic gradient descent (SGD),
the selected learning rate. In particular, Fig. 2 shows
thatwhenusingADAM,nosinglelearningrateapplies
w ←proj (w −γ g ),
well across all problems and coreset sizes; and for a t+1 ≥0 t t tTuning-free coreset Markov chain Monte Carlo
(a) DoG (b) DoWG (c) D-Adaptation SGD (d) prodigy ADAM
Figure 3: Traces of average squared coordinate-wise z-scores between the true and approximated posterior for
a Bayesian linear regression example with M =1,000 coreset points. We evaluate four learning-rate-free SGD
methods: DoG and DoWG (with varying initial learning rate parameter), and D-Adaptation SGD and prodigy
ADAM (with default initial lower bound 10−6). The optimally-tuned ADAM baseline is shown in green. Results
display the median after 200,000 optimization iterations across 10 trials.
with learning rate schedules Algorithm 2 HotDoG
r r2 Require: β 1 =0.9, β 2 =0.999, ϵ=10−8, r =10−3
γ t = √ Gt t (DoG), γ t = (cid:113) (cid:80) t≤Tt r t2∥g t∥2 (DoWG), (5) v
0
←0, mT 0, ←θ 0, 0w ,0 d
0
←0, c←0, h←false
for t=1,...,T do
where r is set to some small constant and, for t≥1,
0 if h then
c←c+1
r =max∥w −w ∥.
t i≤t t 0 S t ←Unif(S,[N]) (without replacement)
gˆ =g(w ,θ ,S ) (Eq. (4))
t t−1 t−1 t
ForD-Adaptationandprodigy,r t inEq.(5)isreplaced v ←β v +(1−β )gˆ2
t 2 t−1 2 t
with a lower bound d on d , which is updated using
t 0 m ←β m +(1−β )gˆ
t 1 t−1 1 t
estimatedcorrelationsbetweenthegradientg andstep
t d ←β d +(1−β )max{|w −w |,d }
t 1 t−1 1 t−1 0 t−1
direction w 0−w t: vˆ ←v /(1−βc)
t t 2
  mˆ ←m /(1−βc)
d t+1 =max(cid:80)t i= (cid:13)0d i⟨g i,w 0− (cid:13) w i⟩ ,d t . dˆ tt ← ( rt 1 if t==1 1 else d t/(1−β 1c−1) )
 (cid:13) (cid:13)(cid:80)t i=0d ig i(cid:13) (cid:13)  w
t
←w t−1−dˆ t(cid:16) diag(cid:16) (cvˆ t)21(cid:17) +ϵI(cid:17)−1 ⊙mˆ
t
else
D-Adaptation replaces r in Eq. (5) (DoG) with d ,
t t w ←w , v ←v , m ←m , d ←d
while prodigy replaces r in Eq. (5) (DoWG) with t t−1 t t−1 t t−1 t t−1
t end if
d . Both D-Adaptation have SGD and ADAM-based
t for k =1,...,K do
variants. Allfourmethodshavebeenshownempirically
θ ∼κ (·|θ ) ▷ record ℓ
to match the performance of optimally-tuned SGD. tk wt (t−1)k tk
end for
Fig.3showstheresultsfromdirectapplicationsofDoG, ▷ Hot-start test
(cid:16) (cid:17)
DoWG, D-Adaptation (SGD), and prodigy (ADAM) h←(true if h else HotStartTest (ℓ )t,K ,t )
iki=1,k=1
toCoresetMCMC.Weseethatthequalityofposterior end for
approximation from all of four methods are orders of return w
T
magnitudeworsethanoptimally-tunedADAM.Withθ
0
initializedfarawayfromhighdensityregionsofπ ,the
w0
initialgradientestimatesarelargeinmagnitude,which
Polyak step size (Loizou et al., 2021). These methods
leadstosmalllearningrates. Theaccumulationofthese
are not applicable in our setting as they require eval-
large gradient norms in the learning rate denominator
uating the objective function. Recall that due to the
eventually causes the learning rate to vanish, halting
unknown Z(w) term in Eq. (1), we do not have access
theprogressofcoresetweightoptimization. Weaddress
to estimates of the objective function.
these problems in the next section.
Before concluding this section, we note that there are
4 HOT DOG
other approaches for making SGD free of learning rate
tuning: some methods involve using stochastic ver-
sions of line search (Vaswani et al., 2019; Paquette In this section, we develop our novel Markovian opti-
and Scheinberg, 2020), and others do the same for the mizationmethod,Hot-startDoG (HotDoG),presentedNaitong Chen, Jonathan H. Huggins, Trevor Campbell
in Algorithm 2. Our method extends the original DoG Algorithm 3 HotStartTest
optimizer in two ways: (1) we add a tuning-free hot-
Require: (ℓ )t,K , t, c=0.5
start test that automatically detects when the Markov ik i=1,k=1
n=ceil(t/3)
chainshaveproperlymixedandstochasticgradientesti-
for k =1,...,K do
m opa tt ie ms iza are tiost na ;b ale n, da (t 2w )h wic eh ap po pi ln yt aw ce ces lt ea rr at tic oo nre ts ee ct hw nie qi ugh est s2
1
← n−1 2min a,b∈R(cid:80)2 i=n n+1(a+bi−ℓ ik)2
to DoG.
s2
2
← n−1 2min a,b∈R(cid:80)t i=2n+1(a+bi−ℓ ik)2
u ←
|( n1 (cid:80)2 i=n n+1ℓik)−( n1 (cid:80)t i=2n+1ℓik)|
4.1 Hot-start test
endk
for
max{s1,s2}
return (true if median(u ,...,u )<c else false)
1 K
Poorly initialized Markov chain states θ can be detri-
0
mental to the performance of learning-rate-free meth-
ods in Coreset MCMC. Fig. 5, and especially Figs. 5c
We define, for each k ∈[K],
to 5e show that this is likely due to the bias of initial
gradient estimates. In particular, the initial gradient
u =
|m 1−m 2|
,
estimatesoftenhaveanormordersofmagnitudelarger k max{s ,s }
1 2
than they would if they had been computed using i.i.d.
and use the median of (u )K as our test statistic.
draws, resulting in a quickly vanishing learning rate in ℓk k=1
This test statistic is checked against a threshold c; the
Eq.(5). Therefore,itiscrucialtohot-starttheMarkov
test passes when the median test statistic becomes
chains to ensure they are properly mixed before train-
smaller than c. The pseudocode for the hot-start test
ing the coreset weights. There are MCMC convergence
isgiveninAlgorithm3. Fromourexperiments, wefind
diagnostics that could be used for this purpose (e.g, R(cid:98)
that setting c=0.5 works well in general.
(Vehtarietal.,2021));manyworkonlywithreal-valued
variables, and are overly stringent for our application.
4.2 Acceleration
We require a test that works for general coreset poste-
riors of the form Eq. (1) and checks only that gradient
ToaccelerateDoG,webeginbynotingthatthedenom-
estimates have stabilized reasonably.
inator of the DoG learning rate in Eq. (5) is similar
To address this challenge, we propose keeping the to that of AdaGrad (Duchi et al., 2011) in that it is
weightsfixedattheirinitialization(i.e.,w ←w )un- a cumulative sum of some function of the gradient.
t+1 t
tilahot-starttestpasses. Forthetest,foreachMarkov Therefore, we can leverage the idea used in RMSProp
chain k ∈[K], we split the iterates i=1,...,t into 3 (Hinton et al., 2012) for accelerating AdaGrad to ac-
segments, each of equal length n=⌈t/3⌉. We compute celerate DoG. In particular, at iteration t, we can
the average log-potentials for the two latter segments replace (cid:80) ∥gˆ∥2 with tvˆ, the bias-corrected expo-
i≤t i t
m , m , and the standard deviations of residual errors nential moving average of the squared gradient. This
1 2
s ,s from a linear fit: allows us to exponentially decrease the weights of past
1 2
gradient norms. As a result, the effect of the early
m =
1 (cid:88)2n
ℓ m =
1 (cid:88)t
ℓ
∥gˆ t∥2 terms on the learning rate diminishes over time,
1 n ik 2 n ik resultinginlessconservativelearningrates. Toaccount
i=n+1 i=2n+1 for situations where the gradient estimates differ in
2n
1 (cid:88) scale across dimensions, we apply the above accelera-
s2 = min (a+bi−ℓ )2
1 n−2a,b∈R ik tion technique in a coordinate-wise fashion and obtain
i=n+1
the following update rule for vˆ:
t
t
s2 2 = n−1 2am ,b∈in R (cid:88) (a+bi−ℓ ik)2, v t =βv t−1+(1−β)gˆ t2, vˆ t = 1−v t βt,
i=2n+1
whereβ ∈(0,1),v =0,andgˆ2 denotesthevectorwith
where ℓ ik is the log-potential for chain k at iteration i, 0 t
each entry of gˆ squared. We further apply the same
t
(cid:88)M idea to r t, the maximum distance traveled from w 0,
ℓ ik = w 0mℓ m(θ ik). and gˆ t, the gradient estimate itself, thereby arriving
m=1 at our proposed optimization procedure. Note that in
Algorithm2,themaxoperatordenotescoordinate-wise
Our test monitors the difference between m and m
1 2 maximum and the ⊙ operater denotes coordinate-wise
relative to s and s . A small difference between m
1 2 1 multiplication.
andm indicatesthattheMarkovchainstateshavesta-
2
bilized. Theresidualstandarderrorsfromthelinearfits In Hot DoG, we set the exponential decay rates, β
1
allows us to remove trends from the noise computation. and β , to be the same as those in Kingma and Ba
2Tuning-free coreset Markov chain Monte Carlo
(2014), and we set the initial learning rate r to a small can be found in Appendix A.2. Without the hot-start
constant (default 10−3) following the recommendation test,thetracesoftenhitalongplateau,beforetheeffect
of Ivgi et al. (2023). of exponentially-weighted averaging is able to decay
early large gradient norms. On the other hand, with
5 EXPERIMENTS burn-in, we begin by simulating from Markov chains
targeting π , and start optimizing the coreset weights
w0
only after the hot-start test has passed. In terms of
In this section, we demonstrate the effectiveness of Hot
the number of log potential evaluations, Hot DoG with
DoG and compare our method against other learning-
burn-inleavestheplateausoonerthanwithoutburn-in.
rate-free stochastic gradient methods: optimally-tuned
ADAM from a log scale grid search, as well as prodigy Fig. 5 examines the behaviour of the hot-start test in
ADAM (Mishchenko and Defazio, 2024), DoG (Ivgi more detail, showing the traces of the gradient esti-
et al., 2023), and DoWG (Khaled et al., 2023) over matenorms∥gˆ∥andteststatisticsmedian(u ,...,u )
t 1 K
different initial parameters. We compare the quality across optimization iterations when using Hot DoG.
of posterior the approximations over different coreset Note that here we only show plots for M = 1000;
sizes M and weight optimization procedures for each the same plots for other coreset sizes can be found
experiment. Following Chen and Campbell (2024), in Appendix A.2. In some experiments, the Markov
for all optimization methods, we set the number of chains are initialized reasonably well where the gradi-
Markov chains to K =2 and subsample size to S =M ent norms are already stabilized, and the test passes
in Eq. (4). We set the Markov kernel κ w to the hit- almost immediately. In others, the Markov chains are
and-run slice sampler with doubling (B´elisle et al., initialized poorly and the gradient norms are large,
1993; Neal, 2003) for all real data experiments. For but nevertheless, the hot-start test passes shortly after
the Gaussian location model, we use a kernel that they stabilize. Across all of these experiments, a test
directly samples from π w (Chen and Campbell, 2024, statistic threshold of 0.5 worked well.
Sec. 3.4); for the sparse regression example, we use
Robustness to fixed parameter r. Figure 6 pro-
Gibbs sampling (George and McCulloch, 1993).
vides an examination of the robustness of the proposed
We compare these algorithms using six different method to the fixed initial learning rate parameter r.
Bayesian models (two synthetic, four real): a synthetic Across all experiments, different values of r spanning
Gaussianlocation,asyntheticsparseregression,a(non- multiple orders of magnitude result in similar posterior
conjugate) linear regression, a logistic regression, a approximations across optimization iterations. Note
Poisson regression, and a Bradley-Terry model. See that M is 1000 for all plots in Fig. 6. The same trends
Appendix A.1 for details. We use Stan (Carpenter can be observed over different coreset sizes (see Ap-
etal.,2017)toobtainfulldatainferenceresultsforreal pendix A.2).
data experiments, and Gibbs sampling (George and
Comparisonwithotherrelatedmethods. Figure7
McCulloch, 1993) for the sparse regression model with
shows a comparison between our method and DoG,
discrete variables. For all experiments, we measure
DoWG, ADAM, as well as prodigy ADAM. We fix
the posterior approximation quality using the average
squared z-score, defined as 1 (cid:80)D (µi−µˆi)2, where µ r =0.001 and c=0.5 for Hot DoG. Since the hot-start
D i=1 σi i test itself can be applied to all methods, Hot DoG is
and σ are, respectively, the coordinate-wise mean and
i
compared against others both with and without burn-
standard deviation estimated using the full data pos-
in. The posterior approximation quality of Hot DoG is
terior, and µˆ is the coordinate-wise mean estimated
i
orders of magnitude better than all other methods in
usingdrawsfromCoresetMCMC.Thisestimateiscom-
manysettingstested,andremaincompetitiveotherwise.
puted in a streaming fashion using the second half of
In particular, Hot DoG is capable of matching the
all draws simulated at the time; note that this includes
performanceofoptimally-tunedADAMwithouttuning.
draws from π before the hot-start test passes.
w0
Each algorithm was run on 8 single-threaded cores
6 CONCLUSION
of a 2.1GHz Intel Xeon Gold 6130 processor
with 32GB memory. Code for these experiments
This paper introduced Hot DoG, a learning-rate-free
is available at https://github.com/NaitongChen/
stochastic gradient method designed for learning core-
automated-coreset-mcmc-experiments. More ex-
set weights within the framework of Coreset MCMC.
perimental details and additional plots are in Appen-
Our method extends DoG, but includes adjustments
dices A.1 and A.2.
tailored to the Markovian setting of Coreset MCMC.
Effect of hot-start test. Fig. 4 compares Hot DoG In particular, Hot DoG includes a hot-start test for
withandwithoutthehot-starttestforM =1000across detectingwhentostarttrainingcoresetweights,aswell
all experiments; the same plots for other coreset sizes as acceleration techniques. The quality of constructedNaitong Chen, Jonathan H. Huggins, Trevor Campbell
(a) Gaussian location (b) Sparse regression (c) Linear regression
(d) Logistic regression (e) Poisson regression (f) Bradley-Terry
Figure 4: Traces of average squared coordinate-wise z-scores between the true and approximated posterior across
all experiments, obtained using Hot DoG with and without hot-start test. All figures share the legend in Fig. 4c.
The coreset size M is 1000 and each line represents a different initial learning rate parameter. The lines indicate
the median from 10 runs. Orange lines indicate runs with hot-start test and blue lines without.
(a) Gaussian location (b) Sparse regression (c) Linear regression
(d) Logistic regression (e) Poisson regression (f) Bradley-Terry
Figure 5: Trace of gradient estimate norms (blue) and hot-start test statistics (green) before weight optimization
across all experiments with M =1000. The orange horizontal line is the test statistic threshold c=0.5.Tuning-free coreset Markov chain Monte Carlo
(a) Gaussian location (b) Sparse regression (c) Linear regression
(d) Logistic regression (e) Poisson regression (f) Bradley-Terry
Figure 6: Traces of average squared coordinate-wise z-scores between the true and approximated posterior across
all experiments, obtained from Hot DoG and optimally-tuned ADAM. All figures share the legend in Fig. 6c. The
coreset size M =1000 and each line represents a different initial learning rate parameter. The lines indicate the
median from 10 runs.
(a) DoG (b) DoWG (c) ADAM (d) prodigy ADAM
(h) prodigy ADAM with hot-
(e) DoG with hot-start (f) DoWG with hot-start (g) ADAM with hot-start start
Figure 7: Relative Coreset MCMC posterior approximation error (average squared coordinate-wise z-scores) using
different optimization algorithms (labeled in the subfigure captions) versus the proposed Hot DoG method (with
fixed r =0.001 and c=0.5). Median values after 200,000 optimization iterations across 10 trials are used for the
relative comparison for a variety of datasets, models, and coreset sizes. Above the horizontal black line (100)
indicates that the proposed Hot DoG method outperformed the method it compared to.Naitong Chen, Jonathan H. Huggins, Trevor Campbell
coresets by Hot DoG and their corresponding posterior Ahmed Khaled, Konstantin Mishchenko, and Chi Jin.
approximation is robust to all of its input parame- DoWG unleashed: an efficient universal parameter-
ters. Empirically, Hot DoG produces better posterior freegradientdescentmethod. InAdvances in Neural
approximations than other learning-rate-free stochas- Information Processing Systems, 2023.
tic gradient methods, and is competitive to those of
Geoffrey Hinton, Nitish Srivastava, and Kevin Swersky.
optimally-tuned ADAM.
Neural networks for machine learning lecture 6a:
overview of mini-batch gradient descent, 2012.
Acknowledgements
Trevor Campbell and Boyan Beronov. Sparse varia-
tional inference: Bayesian coresets from scratch. In
T.C. was supported by NSERC Discovery Grant
Advances in Neural Information Processing Systems,
RGPIN-2019-03962, and J.H.H. was partially sup-
2019.
ported by National Science Foundation CAREER
award IIS-2340586. We acknowledge the use of the John Duchi, Elad Hazan, and Yoram Singer. Adap-
ARC Sockeye computing platform from the University tive subgradient methods for online learning and
of British Columbia. stochastic optimization. Journal of Machine Learn-
ing Research, 12(61):2121–2159, 2011.
References Yair Carmon and Oliver Hinder. Making SGD
parameter-free. In Conference on Learning Theory,
Christian Robert and George Casella. Monte Carlo
2022.
Statistical Methods. Springer, 2nd edition, 2004.
Aaron Defazio and Konstantin Mishchenko. Learning-
ChristianRobertandGeorgeCasella.Ashorthistoryof
rate-free learning by D-adaptation. In International
Markov chain Monte Carlo: subjective recollections
Conference on Machine Learning, 2023.
from incomplete data. Statistical Science, 26(1):102–
Konstantin Mishchenko and Aaron Defazio. Prodigy:
115, 2011.
an expeditiously adaptive parameter-free learner. In
Andrew Gelman, John Carlin, Hal Stern, David Dun- International Conference on Machine Learning,2024.
son, Aki Vehtari, and Donald Rubin. Bayesian Data
Francesco Orabona and Ashok Cutkosky. Interna-
Analysis. CRC Press, 3rd edition, 2013.
tional Conference on Machine Learning tutorial on
JonathanHuggins,TrevorCampbell,andTamaraBrod- parameter-free stochastic optimization, 2020.
erick. Coresets for scalable Bayesian logistic regres-
Sharan Vaswani, Aaron Mishkin, Issam Laradji, Mark
sion. In Advances in Neural Information Processing
Schmidt, Gauthier Gidel, and Simon Lacoste-Julien.
Systems, 2016.
Painless stochastic gradient: interpolation, line-
Cian Naik, Judith Rousseau, and Trevor Campbell. search,andconvergencerates. InAdvancesinNeural
Fast Bayesian coresets via subsampling and quasi- Information Processing Systems, 2019.
Newton refinement. In Advances in Neural Informa- CourtneyPaquetteandKatyaScheinberg. Astochastic
tion Processing Systems, 2022. line search method with expected complexity analy-
sis. Society for Industrial and Applied Mathematics
Naitong Chen, Zuheng Xu, and Trevor Campbell.
Journal on Optimization, 30(1):349–376, 2020.
Bayesian inference via sparse Hamiltonian flows. In
Advances in Neural Information Processing Systems, Nicolas Loizou, Sharan Vaswani, Issam Laradji, and
2022. SimonLacoste-Julien. Stochasticpolyakstep-sizefor
SGD: an adaptive learning rate for fast convergence.
Trevor Campbell. General bounds on the quality of
InInternational Conference on Artificial Intelligence
Bayesiancoresets.InAdvancesinNeuralInformation
and Statistics, 2021.
Processing Systems, 2024.
Aki Vehtari, Andrew Gelman, Daniel Simpson, Bob
Naitong Chen and Trevor Campbell. Coreset Markov
Carpenter, and Paul-Christian Bu¨rkner. Rank-
chain Monte Carlo. In International Conference on
normalization,folding,andlocalization: animproved
Artificial Intelligence and Statistics, 2024.
R(cid:98) for assessing convergence of MCMC (with discus-
Diederik Kingma and Jimmy Ba. Adam: a method for sion). Bayesian Analysis, 16(2):667–718, 2021.
stochastic optimization. In International Conference Claude B´elisle, Edwin Romeijn, and Robert Smith.
on Learning Representations, 2014. Hit-and-run algorithms for generating multivariate
distributions. Mathematics of Operations Research,
Maor Ivgi, Oliver Hinder, and Yair Carmon. Dog is
18(2):255–266, 1993.
SGD’s best friend: a parameter-free dynamic step
size schedule. In International Conference on Ma- RadfordNeal. Slicesampling. The Annals of Statistics,
chine Learning, 2023. 31(3):705–767, 2003.Tuning-free coreset Markov chain Monte Carlo
Edward George and Robert McCulloch. Variable selec-
tion via Gibbs sampling. Journal of the American
Statistical Association, 88(423):881–889, 1993.
Bob Carpenter, Andrew Gelman, Matthew Hoffman,
DanielLee,BenGoodrich,MichaelBetancourt,Mar-
cus Brubaker, Jiqiang Guo, Peter Li, and Allen Rid-
dell. Stan: a probabilistic programming language.
Journal of Statistical Software, 76(1):1—-32, 2017.
Aprad Elo. The Rating of Chessplayers, Past and
Present. Arco, 1st edition, 1978.Naitong Chen, Jonathan H. Huggins, Trevor Campbell
A APPENDIX
A.1 Details of Experiments
A.1.1 Model Specification
In this subsection, we describe the six examples (two synthetic and four real data) that we used for our
experiments. Processed versions of all datasets used for the experiments are available at https://github.com/
NaitongChen/automated-coreset-mcmc-experiments. For each of the regression models, we are given a set of
points (x ,y )N , each consisting of features x ∈Rp and response y .
n n n=1 n n
Bayesian sparse linear regression: This is based on Example 4.1 from George and McCulloch (1993). We use
the model
σ2 ∼InvGam(ν/2,νλ/2),
iid
∀i∈[p], γ ∼ Bern(q),
i
(cid:16) (cid:17)
β |γ i ∼nd N 0,(1(γ =0)τ +1(γ =1)cτ)2 ,
i i i i
∀n∈[N], y |x ,β,σ2 i ∼nd N (cid:0) x⊤β,σ2(cid:1) ,
n n n
where we set ν =0.1,λ=1,q =0.1,τ =0.1, and c=10. Here we model the variance σ2, the vector of regression
coefficients β =(cid:2) β ... β (cid:3)⊤ ∈Rp and the vector of binary variables γ =(cid:2) γ ... γ (cid:3)⊤ ∈{0,1}p indicating
1 p 1 p
theinclusionofthepthfeatureinthemodel. WesetN =50,000,p=10,β⋆ =(cid:2) 0 0 0 0 0 5 5 5 5 5(cid:3)⊤ ,
and generate a synthetic dataset by
iid
∀n∈[N], x ∼ N (0,I),
n
ϵ
i ∼id
N
(cid:0) 0,252(cid:1)
,
n
y =x⊤β⋆+ϵ .
n n n
Bayesian linear regression: We use the model
(cid:2)
β
logσ2(cid:3)⊤
∼N(0,I),
∀n∈[N],y |x ,β,σ2 i ∼nd N (cid:0)(cid:2) 1 x⊤(cid:3) β,σ2(cid:1) ,
n n n
where β ∈Rp+1 is a vector of regression coefficients and σ2 ∈R is the noise variance. Note that the prior here is
+
notconjugateforthelikelihood. ThedatasetconsistsofflightdelayinformationfromN =98,673observationsand
was constructed using flight delay data from https://www.transtats.bts.gov/Homepage.asp and historical
weather information from https://www.wunderground.com/. We study the difference, in minutes, between
the scheduled and actual departure times against p = 10 features including flight-specific and meteorological
information.
Bayesian logistic regression: We use the model
iid
∀i∈[p+1], β ∼ Cauchy(0,1),
i
∀n∈[N], y
i ∼nd Bern(cid:16)(cid:0) 1+exp(cid:0) −(cid:2)
1
x⊤(cid:3) β(cid:1)(cid:1)−1(cid:17)
,
n n
where β =(cid:2) β ... β (cid:3)⊤ ∈Rp+1 is a vector of regression coefficients. Here we use the same dataset as in
1 p+1
linear regression, but instead model the relationship between whether a flight is cancelled using the same set of
features. Note that of all flights included, only 0.058% were cancelled.
Bayesian Poisson regression: We use the model
β ∼N(0,I),
ind
(cid:18) (cid:18) (cid:104)
1
x⊤(cid:105) β(cid:19)(cid:19)
∀n∈[N],y |x ,β ∼ Poiss log 1+e n ,
n nTuning-free coreset Markov chain Monte Carlo
where β ∈ Rp+1 is a vector of regression coefficients. The dataset consists of N = 15,641 observations,
and we model the hourly count of rental bikes against p = 8 features (e.g., temperature, humidity at the
time, and whether or not the day is a workday). The original bike share dataset is available at https:
//archive.ics.uci.edu/dataset/275/bike+sharing+dataset.
The remaining two non-regression models are specified as follows.
Gaussian location: We use the model
θ ∼N(0,I),
iid
∀n∈[N],X ∼ N(θ,I),
n
where θ,X ∈Rd. Here we model the mean θ. We set N =10,000,d=20 and generate a synthetic dataset by
n
iid
∀n∈[N],x ∼ N(0,I).
n
Bradley-Terry model: We use the model
iid
θ ∼ N(0,I),
(cid:16) (cid:17)
∀n∈[N],y |h ,v ,θ i ∼nd Bern (1+exp((θ −θ )/400))−1 ,
n n n vn hn
where θ ∈ Rd. The dataset was constructed using games statistics from https://www.nba.com/stats and
consists of data of N =26,651 NBA games between the 2004 and 2022 seasons. h and v are the home team
n n
and visitor team IDs for the nth game in the dataset, and y denotes the outcome of the game (y = 1 if the
n n
home team won and y =0 if the visitor team won). θ ∈Rd represents the Elo ratings or relative skill levels (Elo,
n
1978, Ch. 1) for each of the d=30 teams. We model the Elo ratings using outcomes of pairwise comparisons
between teams using game outcomes.
A.1.2 Parameter Settings
Forfull-datainferenceresultsofallexamplesexceptforthesparselinearregressionmodel,weranStan(Carpenter
et al., 2017) with 10 parallel chains, each taking 100,000 steps with the first 50,000 discarded, for a combined
500,000 draws. For full-data inference result of the sparse linear regression example, we use the Gibbs sampler
developed by George and McCulloch (1993) to generate 200,000 draws, with the first half discarded as burn-in.
To account for changes in w, for all real data experiments, we use the hit-and-run slice sampler with doubling
(B´elisle et al., 1993; Neal, 2003); for the Gaussian location model, we use a kernel that directly samples from π
w
(Chen and Campbell, 2024, Sec. 3.4). for the sparse regression, we use the Gibbs sampler developed by George
and McCulloch (1993).
We use Stan (Carpenter et al., 2017) to obtain full data inference results for real data experiments, and Gibbs
sampling(GeorgeandMcCulloch,1993)forthesparseregressionmodelwithdiscretevariables. Thetrueposterior
distribution for the Gaussian location model is available in closed form.
For ADAM, we test multiple learning rates over a log scale grid
(cid:0) 10k(cid:1)
for k =−3,−2,...,1. For each experiment
under each coreset size, the optimally-tuned ADAM is the one that obtained the lowest average squared z-score
after 200,000 iterations of weight optimization. For all learning-rate-free methods, we test different initial
parameters (initial lower bound for prodigy ADAM and r for Hot DoG, DoG, and DoWG) over a log scaled grid
0
(cid:0) 10k(cid:1)
for k =−3,−2,...,1.
For the logistic regression example, to account for the class imbalance problem, we include all observations from
the rare positive class if the coreset size is more than twice as big as the total number of observations with
positive labels. Otherwise we sample our coreset to have 50% positive labels and 50% negative labels. Coreset
points are uniformly subsampled for all other models.
A.2 Additional Results
Figs. 4 to 6 in the main text show the traces of average squared coordinate-wise z-scores, as well as the gradient
estimate norms and hot-start test statistics for Hot DoG when M =1000. In this subsection, we show the sameNaitong Chen, Jonathan H. Huggins, Trevor Campbell
sets of plots for M =100 and M =500. Similarly to Fig. 4, Figs. 8 and 9 compare Hot DoG with and without
hot-start test. Similarly to Fig. 5, Figs. 10 and 11 show the gradient estimate norms and hot-start test statistics
during burn-in. Similarly to Fig. 6, Figs. 12 and 13 compare Hot DoG (with hot-start test) and optimally-tuned
ADAM. We see that all plots show the same trends as the ones in Section 5, where M =1000. As a result, we
arrive at similar observations as in Section 5. In particular, Hot DoG with burn-in leaves the plateau sooner than
without burn-in; the hot-start test passes and thus burn-in terminates shortly after gradient norms are stabilized;
Hot DoG is robust to the fixed parameter r.
(a) Gaussian location (b) Sparse regression (c) Linear regression
(d) Logistic regression (e) Poisson regression (f) Bradley-Terry
Figure 8: Traces of average squared coordinate-wise z-scores between the true and approximated posterior across
all experiments, obtained using Hot DoG with and without hot-start test. All figures share the legend in Fig. 8c.
The coreset size M is 100 and each line represents a different initial learning rate parameter. The lines indicate
the median from 10 runs. Orange lines indicate runs with hot-start test and blue lines without.Tuning-free coreset Markov chain Monte Carlo
(a) Gaussian location (b) Sparse regression (c) Linear regression
(d) Logistic regression (e) Poisson regression (f) Bradley-Terry
Figure 9: Traces of average squared coordinate-wise z-scores between the true and approximated posterior across
all experiments, obtained using Hot DoG with and without hot-start test. All figures share the legend in Fig. 9c.
The coreset size M is 500 and each line represents a different initial learning rate parameter. The lines indicate
the median from 10 runs. Orange lines indicate runs with hot-start test and blue lines without.
(a) Gaussian location (b) Sparse regression (c) Linear regression
(d) Logistic regression (e) Poisson regression (f) Bradley-Terry
Figure 10: Trace of gradient estimate norms (blue) and hot-start test statistics (green) before weight optimization
across all experiments with M =100. The orange horizontal line is the test statistic threshold c=0.5.Naitong Chen, Jonathan H. Huggins, Trevor Campbell
(a) Gaussian location (b) Sparse regression (c) Linear regression
(d) Logistic regression (e) Poisson regression (f) Bradley-Terry
Figure 11: Trace of gradient estimate norms (blue) and hot-start test statistics (green) before weight optimization
across all experiments with M =500. The orange horizontal line is the test statistic threshold c=0.5.
(a) Gaussian location (b) Sparse regression (c) Linear regression
(d) Logistic regression (e) Poisson regression (f) Bradley-Terry
Figure 12: Traces of average squared coordinate-wise z-scores between the true and approximated posterior across
all experiments, obtained from Hot DoG and optimally-tuned ADAM. All figures share the legend in Fig. 12c.
The coreset size M =100 and each line represents a different initial learning rate parameter. The lines indicate
the median from 10 runs.Tuning-free coreset Markov chain Monte Carlo
(a) Gaussian location (b) Sparse regression (c) Linear regression
(d) Logistic regression (e) Poisson regression (f) Bradley-Terry
Figure 13: Traces of average squared coordinate-wise z-scores between the true and approximated posterior across
all experiments, obtained from Hot DoG and optimally-tuned ADAM. All figures share the legend in Fig. 13c.
The coreset size M =500 and each line represents a different initial learning rate parameter. The lines indicate
the median from 10 runs.