DELT: A Simple Diversity-driven EarlyLate Training for Dataset Distillation
ZhiqiangShen* AmmarSherif* ZeyuanYin ShitongShao
VILALab,MBZUAI
{zhiqiang.shen, zeyuan.yin}@mbzuai.ac.ae {ammarsherif90, 1090784053sst}@gmail.com
Abstract PriorDatasetDistillation OurDELTDistillation
bird IPC1# bird IPC1# !
Recentadvancesindatasetdistillationhaveledtosolutions
in two main directions. The conventional batch-to-batch
IPC2# IP #C2
#
matching mechanism is ideal for small-scale datasets and batch-to-global IPC 1:N batch-to-global IPC 1:N Plotcurvesoflr Full matching matching
includesbi-leveloptimizationmethodsonmodelsandsyn- # Full
theses, such as FRePo, RCIG, and RaT-BPTT, as well as IPCN IPCN # "
other methods like distribution matching, gradient match- Prior Ours 优化之后距离远近
ing, and weight trajectory matching. Conversely, batch- Computation !×# #+ #−./ …+./
to-global matching typifies decoupled methods, which are
particularly advantageous for large-scale datasets. This
IPC=1加灰⾊表示不
approachhasgarneredsubstantialinterestwithinthecom- 适⽤，只能是初始化
munity, as seen in SRe2L, G-VBSM, WMDD, and CDA. A IPC 1 IPC N 起作⽤
Early-optimizedimage Late-optimizedimage
primary challenge with the second approach is the lack of
OurDELTgeneratedimages
diversity among syntheses within each class since samples Figure1.DistillingdatasetstoIPC requiresN×T iterationsin
N
are optimized independently and the same global supervi- traditionaldistillationprocesses(left)butfeweriterationprocesses
sionsignalsarereusedacrossdifferentsyntheticimages. In inourEarlyLatestrategy(right).
thisstudy,weproposeanewDiversity-drivenEarlyLate
Training (DELT) scheme to enhance the diversity of im-
ages in batch-to-global matching with less computation. cessible and affordable for the general public. Previous
Our approach is conceptually simple yet effective, it par- approaches [3, 4, 6, 14, 18, 21, 36, 42, 43, 47] primarily
titions predefined IPC samples into smaller subtasks and employ a batch-to-batch matching technique, where infor-
employs local optimizations to distill each subset into dis- mation like features, gradients, and trajectories from a lo-
tributionsfromdistinctphases, reducingtheuniformityin- cal original data batch are used to supervise and train a
duced by the unified optimization process. These distilled corresponding batch of generated data. The strength of
imagesfromthesubtasksdemonstrateeffectivegeneraliza- this method lies in its ability to capture fine-grained in-
tion when applied to the entire task. We conduct exten- formation from the original data, as each batch’s super-
sive experiments on CIFAR, Tiny-ImageNet, ImageNet-1K, vision signals vary. However, the downside is the neces-
and its sub-datasets. Our approach outperforms the pre- sitytorepeatedlyinputbothoriginalandgenerateddatafor
viousstate-of-the-artby2∼5%onaverageacrossdifferent eachtrainingiteration, whichsignificantlyincreasesmem-
datasets and IPCs (images per class), increasing diversity ory usage and computational costs. Recently, a new de-
perclassbymorethan5%whilereducingsynthesistimeby coupled method [19, 39, 40] has been proposed to sepa-
upto39.3%forenhancingthetrainingefficiency. rate the model training and data synthesis, also it lever-
agesthebatch-to-globalmatchingtoavoidinputtingorigi-
naldataduringdistilleddatageneration. Thissolutionhas
1.Introduction demonstrated great advantage on large-scale datasets like
ImageNet-1K [28, 40] and ImageNet-21K [39]. However,
Intheeraoflargemodelsandlargedatasets,datasetdistil- asshowninFig.2,asignificantlimitationofthisapproach
lation has emerged as a crucial strategy to enhance train- is the lack of diversity caused by the mechanism of syn-
ing efficiency and make advanced technologies more ac- thesizingeachdatapointindividually,wheresupervisionis
repetitively applied across various synthetic images. For
*Equal contribution. Work done while Ammar and Shitong visiting
MBZUAI.Codeisat:https://github.com/VILA-Lab/DELT. instance, SRe2L [40] utilizes globally-counted layer-wise
1
4202
voN
92
]VC.sc[
1v64991.1142:viXra
… … … …(1)SRe2L
(2)CDA
(3)Ours[1×1initialized]
ClassIndex
Figure2.Left:Intra-classsemanticcosinesimilarityafterapretrainedResNet-18modelonImageNet-1Kdataset,lowervaluesarebetter.
Right:SyntheticimagesfromSRe2L,CDAandourDELT.
running means and variances from the pre-trained model which is 1/j of the standard count (where typically j = 4
for supervising different intra-class image synthesis. This or 8), meaning the total number of optimization iterations
methodology results in a severely limited diversity within requiredisjustabout2/3ofpriorbatch-to-globalmatching
thesamecategoryofgeneratedimages. methods,suchasSRe2LandCDA.Wefurthervisualizethe
To address this issue, a few prior studies [28, 32] have average cosine similarity between each sample of 50 IPCs
proposed to enlarge diversity within each class. For in- using the associated cluster centroid within the same class
stance,G-VBSM[28]utilizesadiversesetoflocal-match- on ImageNet-1K, as shown in Fig. 2 left subfigure, DELT
global matching signals derived from multiple backbones illustratesasmallersimilaritysignificantly,andalsoshows
and statistical metrics, to achieve more precise and effec- substantially better visual diversity than other counterparts
tivematchingthanthesingularmodel. However,asthedi- acrossallclasses,asintherightsubfigureofFig.2.
versity of matching models grows, the overall complexity We conduct extensive experiments on various datasets
of the framework also increases thus diminishing its con- of CIFAR-10, Tiny-ImageNet, ImageNet-1K and its sub-
ciseness. RDED[32]cropseachoriginalimageintomulti- sets. On ImageNet-1K, our proposed approach achieves
plepatchesandrankstheseusingrealismscoresgenerated 66.1%underIPC50withResNet-101,outperformingprevi-
by an observer model. Then it stitches every four chosen ousstate-of-the-artRDEDby4.9%.Onsmall-scaledatasets
patchesfrompreviousstageintoasinglenewimagetopro- of CIFAR-10, our approach also obtains 2.5% and 19.2%
duceIPC-numbereddistilledimagesforeachclass. RDED improvementoverRDEDandSRe2LusingResNet-101.
isefficienttocombinemultipleimagesbutdoesnotenhance Ourmaincontributionsinthisworkareasfollows:
oroptimizethevisualcontentonthedistilleddataset, thus • We propose asimple yet effective EarlyLate training
thediversityandrichnessofinformationarelargelydepen-
scheme for dataset distillation to enhance intra-class di-
dentonthedistributionoftheoriginaldataset.
versityofsyntheticimagesforbatch-to-globalmatching.
Oursolution,termedtheEarlyLatetrainingscheme,
• Wedemonstrateempiricallythattheproposedmethodcan
isstraightforwardandalsoorthogonaltothesepriormeth- generate optimized images at different distances with a
ods: by initializing each image in the same category at a fastspeed,toenlargeinformativenessamonggenerations.
differentstartingpointforoptimization,weensurethatthe • We conducted extensive experiments and ablations on
finaloptimizedresultsvarysignificantlyacrossimages. We various datasets across different scales to prove the ef-
also use teacher-ranked real image patches to initialize the fectivenessoftheproposedapproach2.
synthetic images. This prevents some images from being
short-optimized and ensures they provide sufficient infor- 2.RelatedWork
mation. As shown in Fig. 1 of the computation compar-
ison, our approach not only enhances intra-class diversity Dataset Distillation. Dataset distillation or condensa-
butalsodramaticallyreducesthecomputationalloadofthe tion [36] focuses on creating a compact yet representative
trainingprocesswith39.3%onImageNet-1K.Specifically, subsetfromalargeoriginaldataset. Thisenablesmoreef-
while conventional training requires T optimization itera- ficientmodeltrainingwhilemaintainingtheabilitytoeval-
tions per image or batch, in our EarlyLate scheme, the uate on the original test data distribution and achieve sat-
first image undergoes T iterations (where T = T). Sub- isfactory performance. Previous works [3, 4, 6, 14, 18,
1 1
sequent batches are processed with progressively fewer it- 21, 36, 42, 43, 47] mainly designed how to better match
erations, such as T (T = T −RI1) for the nextset, and the distribution between original data and generated data
2 2 1
soforth. TheiterationsforthefinalbatcharereducedtoRI inabatch-to-batchmanner,suchasthedistributionoffea-
1RIisthenumberofrounditerationsandwillbeintroducedinSec.4.3. 2OursyntheticimagesonImageNet-1Kareavailableatlink.
2tributionsfromthesubtasksdemonstrateeffectivegeneral-
1 Gradient/Trajectory/Feature,etc.
3
ization when applied to the entire task. The recently pro-
. ℒ 2
posed LPLD [38] batches images by class, leveraging the
0 3 Gradient/Trajectory/Feature,etc. naturalindependencebetweenclasses,andintroducesclass-
wisesupervisionforalignment. Toenhancediversity,itin-
(1)Batch–to-BatchMatching
troducesfiner-grainedrandomnessatthebatchlevel,while
weightpretraining ! representsalocal
Original
dataset . ℒ +, batchin#and$ this method is different from our training-based approach
Globalmean, thatisderivedfromanoptimizationperspective.
varstatistics
weightsharing
0 3 . 1 Batc sh tam tise ta icn s,var ℒ 2 3.Approach
Preliminaries. The objective of a regular dataset distilla-
(2)Batch–to-GlobalMatching
tion task is to generate a compact synthetic dataset S =
Figure3. Batch–to-batchvs. batch-to-globalmatchinginDD.θ (cid:0) (cid:1)
f {(xˆ ,yˆ ),..., xˆ ,yˆ } as a student dataset that cap-
1 1 |S| |S|
indicatesweightsarepretrainedandfrozeninsynthesisstage.
turesasubstantialamountoftheinformationfromalarger
(cid:0) (cid:1)
tures [42], gradients [43], or the model weighO tth tre ar jl eo cs ts ol -ikeCEalnadbeRliesdnodtaptlaostteetdTinth=efi{g(uxre1s,y 1),..., x |T|,y |T| }, which
serves as the teacher dataset. Here, yˆ represents the soft
ries[3,6].Theprimaryoptimizationmethodusedisbi-level
labelforthesyntheticsamplexˆ,andthesizeofS ismuch
optimization[20,41],whichinvolvesoptimizingmodelpa-
smaller than T, yet it retains the essential information of
rametersandupdatingimagessimultaneously.Forinstance,
theoriginaldatasetT. Thelearninggoalusingthisdistilled
using gradient matching, the process can be formulated as
dataistotrainapost-validationmodelwithparametersθ:
tominimizethegradientdistance:
θ =argminL (θ), (3)
S∈m Ri Nn ×dD(∇ θℓ(S;θ),∇ θℓ(T;θ))=D(S,T;θ) (1) S θ S
L (θ)=E [ℓ(ϕ (xˆ),yˆ;θ)], (4)
where the function D(·,·) is defined as a distance metric S (xˆ,yˆ)∈S θS
such as MSE [37], θ denotes the model parameters, and where ℓ is a standard loss function such as soft cross-
∇ ℓ(·;θ) represents the gradient, utilizing either the origi- entropyandϕ representsthemodel.
θ θS
naldatasetT oritssyntheticversionS. N isthenumberof Theprimaryaimofdatasetdistillationistoproducesyn-
d-dimensional synthetic data. During distillation, the syn- theticdatathatensuresminimalperformancedifferencebe-
theticdatasetS andmodelθareupdatedalternatively, tween models trained on the synthetic dataset S and those
trained on the original dataset T using validation data V.
S ←S−λ∇ SD(S,T;θ), θ ←θ−η∇ θℓ(θ;S), (2) TheoptimizationprocedureforgeneratingS isgivenby:
whereλandηarelearningratesdesignatedforS andθ. argmin(sup{|ℓ(ϕ (x ),y )
θT val val
Diversity in Dataset Distillation. Batch-to-global match- S,|S| (5)
ingusedin[8,19,28,38–40]tracksthedistributionofBN −ℓ(ϕ (x ),y ) .
statistics derived from original dataset for the local batch
θS val val (xval,yval)∼V
synthetic data. However, this type of approach can easily where(x ,y )arethesampleandlabelpairsintheval-
val val
encounterdiversityissueswithinthesameclassduetothe idation set of the real dataset T. The learning task then
optimization objective. Fig. 3 illustrates the difference of focusesonthe<data,label>pairswithinS,maintaininga
batch-to-batch and batch-to-global matching mechanisms, balancedrepresentationofdistilleddataacrosseachclass.
wherebrepresentsalocalbatchindataT andS. Initialization. Previous dataset distillation methods [28,
Moreover,fortherecentadvancesofmulti-stagedataset 39, 40] on large-scale datasets like ImageNet-1K and 21K
distillationmethods,MDC[14]proposestocompressmul- employ Gaussian noise by default for data initialization in
tiple condensation processes into a single one by includ- the synthesis phase. However, Gaussian noise is random
inganadaptivesubsetlossontopofthebasiccondensation andlacksanysemanticinformation. Intuitively, usingreal
loss,sothattoobtaindatasetswithmultiplesizes. PDD[4] images provide a more meaningful and structured starting
generatesmultiplesmallbatchesofsyntheticimages, each point, and this structured start can lead to quicker conver-
batchisconditionedontheaccumulateddatafromprevious gence during optimization because the initial data already
batches. Unlike PDD, our current synthetic batch is inde- contains useful features and patterns that are closer to the
pendentwithdifferentoperationiterationsandnotrelevant targetdistribution,whichfurtherenhancesrealism,quality,
to any previous batches. D3 [25] partitions large datasets andgeneralizationofthesynthesizedimages. Asshownin
into smaller subtasks and employs locally trained experts Fig.2rightsubfigure,ourgeneratedimagesexhibitbothdi-
todistilleachsubsetintodistributions. Thesedistilleddis- versityandahighdegreeofrealisminsomecases.
3Diversity-driven IPC Concatenation
+ Concatenation node
Images Per Class whose rank
CDA Gradient updates for iterations
Scheduler + +
Figure4.TheproposedDELTlearningprocedureviaamulti-roundEarlyLatescheme.
Selection Criteria. Here, we introduce how to select real as images become easier to predict with more updates by
image patches to initialize the synthetic images. In our fi- classlabels,trainingprimarilyoneasydatapointscanhin-
nal syntheses, a significant fraction of our data has been dermodelgeneralization. Therefore,ourmethodenhances
subjecttolimitedoptimizationiterations, makingeffective generalizationbygeneratingdatasampleswithvaryingdif-
Top Medium Selection Criteriainitialization crucial. A proper initialization also dramat- ficulty levels, acting as a regularizer by limiting the opti-
ically minimizes the overall computational load required mizationprocesstoasmallervolumeofimagepixelspace.
for the updating on data. Prior approach [32] has demon- Previouswork[1]studieshowtoperformsimpleearlystop-
5 1 4
strated that choosing representative data patches from the ping on dUifnferorellnetd lvaeyresriso’nweights with progressive retrain-
4 2 5
originaldatasetwithouttrainingcanyieldfavorableperfor- ing to mitigate noisy labels. Unlike it, we are pioneering
7 3 3mance without any additional training. Our observation, tostudybothearlyandlatetrainingwhenoptimizingdata.
1 T Re aa nc kh ee rr 4 6however, underscores that applying iterative refinement to Moreover, we improve the efficiency of our approach by
3 5 2originalpatchescanleadtomarkedlyimprovedresults. performing gradient updates in a single scan. Initially, we
6 6 As illustrated in Fig. 5, our selection criterion is based conductasinglegradientloop,continuallyintroducingnew
on a pretrained teacher model as a ranker, we calculate datafordistillationbyconcatenatingthematdifferenttime
2 7
allpatches’probabilitiesandsortthemastheinitialization stamps. Consequently, the M batch receives the synthetic
pool. Then, we choose a patch of images scoring closer imagesofallprecedingbatches,IPC 0:Mk−1,asfinalgener-
to the per-class median as initialization for optimization. ations. Thisprocesscanbesimplifiedasfollows:
Moredetailsregardinginitializationandordercanbefound IPC =[xˆ ,xˆ ,...,xˆ ,...,xˆ ] (6)
0:Mk−1 0 1 k−1 Mk−1
in Appendix. The motivation is that such images have a (cid:124) (cid:123)(cid:122) (cid:125)
medium difficulty level to the teacher, so they have more
IPC0:k−1
(cid:124) (cid:123)(cid:122) (cid:125)
room for information enhancement via distillation gradi- ...
ents. WefurtThoepr Memedpiuirmic Salellyecvtiaolnid Cartietetrhiaisstrategybycom- (cid:124) IPC0(cid:123) :M(cid:122)
k−1
(cid:125)
paringdifferentstrategiesinTable4b.
where [xˆ ,xˆ ,...,xˆ ] refers to the concatenation of
0 1 Mk−1
generated images. M is the number of batches, k is the
2 1 0.91 1 number of generated images in each batch. We train these
0.87
3 2 2 differentbatchesatdifferentstartingpoints,eachbatchgoes
throughacompletedlearningphase,butthetotalnumberof
5 3 0.63 3
Teacher iterationsvaries. Then,themultipleIPCsofxˆareconcate-
IPC initialization
1 Ranker 4 0.45 natedintoasimplebatch. Becauseofitsearly-latetraining
4 5 0.23 property,werefertothisschemeasEarlyLatetraining.
Figure5.Selectioncriteriawithateachranker. Datasynthesis. OurEarlyLateoptimizationprocedure
canbeformulatedasamulti-stagetrainingscheme:
Diversity-drivenIPCConcatenationTraining.Asshown
(cid:0) (cid:0) (cid:1) (cid:1)
Round1: argmin ℓ ϕ x ,y +R
Random cirnopF piogo.l4, to further emphasize diversity and avoid poten- θT (cid:101)IPC0:k−1 reg
tial distribution bias from initialization, we optimize the
CIPC0:k−1,|C|
initialized images starting from different points. The mo- ...
tivationbehindthisdesignisthatdifferentdatasamplesre- RoundM−1: argmin ℓ(cid:0) ϕ (cid:0) x (cid:1) ,y(cid:1) +R
quirevaryingnumbersofiterationstoconvergeastheearly CIPC0:Mk−1,|C|
θT (cid:101)IPC0:Mk−1 reg
stopping [24] from other research domain. Importantly, (7)
4
…where C is the target distilled dataset. The number of
batches M > 1 (If M = 1, training will degenerate into
a way without EarlyLate). This process is referred to
in Fig. 4 bottom row. R is the regularization term, we
reg
alsoutilizetheBatchNormdistributionregularizationterm
toimprovethequalityofthegeneratedimages:
Rreg(x˜)=(cid:88)(cid:12)
(cid:12) (cid:12)µ l(x (cid:101))−BNR
lM(cid:12)
(cid:12)
(cid:12)
+(cid:88)(cid:12)
(cid:12) (cid:12)σ l2(x˜)−BNR
lV(cid:12)
(cid:12)
(cid:12)
(8)
2 2
l l
3×3 4×4 5×5 6×6
wherelistheindexofBNlayer,µ (x)andσ2(x)aremean
l (cid:101) l (cid:101) Figure 6. Mosaic splicing patterns on ImageNet-1K using real
and variance. BNR l M and BNR lV are running mean and imagepatchesastheinitialization. Ineachblock,theleftcolumn
running variance in pre-trained model at l-th layer, which isthestartingrealimageinitializedsamplesandrightisthefinal
aregloballycounted. optimizedsyntheses.Fromtoptobottomareimagesgeneratedby
TrainingProcedure.Regardingconcatenationtraining,we earlytrainingandlatetraining.
elaborate: Our EarlyLate enhances the diversity of the
syntheticdatabyvaryingthenumberofiterationsfordiffer- tion methods [32, 40, 43], we use ConvNet [10], ResNet-
ent IPCs during data synthesis phase. This means the first 18/ResNet-101 [13], EfficientNet-B0 [33], MobileNet-
IPC can be recovered for the largest number of iterations V2[27],MnasNet1 3[34],andRegNet-Y-8GF[26],asour
like4KwhilethelastIPCwillonlyberecoveredusing500 backbone for training or post-validation. All our experi-
iterations. To make this process efficient, we share the re- mentsareconductedon4NVIDIARTX4090GPUs.
coverytime(ontheGPU)acrossthedifferentIPCsviacon- As shown in Table 1, our approach establishes the new
catenationtominimizetimeasmuchaspossible. Therefore state-of-the-art accuracy in 13 out of 15 of the configura-
the first image IPC will start recovery for a couple of iter- tionsonfivedatasetsfromsmall-scaleCIFAR-10tolarge-
ations, and when it completes iteration 3,500 the last IPC scale ImageNet-1K using either relatively large backbone
willjoinitintherecoveryphasetogetits500iterations. architectureofResNet-101orsmallMobileNet-v2,inmany
As illustrated in Fig. 4, our learning procedure is ex- cases with significant margins of improvement. The re-
tremely simple using an incremental learning process: We sults using small-scale architecture ConvNet are shown in
splitthetotalIPCstobelearnedintomultiplebatches. The Table2, ourapproachalsoachievesthestate-of-the-artac-
training begins with the first batch. Following a prede- curacyin7outof9oftheconfigurationsonthreedatasets.
fined number of iterations, the second batch commences
4.2.Cross-architecturegeneralization
itsiterativetraining,andthisprocesscontinuessequentially
with subsequent batches. Batch-to-global matching algo- An important characteristic of distilled datasets is their
rithm [39] of Eq. 8 has been utilized between each round. effectiveness in generalizing to novel training architec-
In our DELT, later sub-batches will join the previous sub- tures. In this context, we assess the transferability of
batchesintherecovery/trainingstageinsteadoffreezingthe DELT’s distilled datasets tailored for ImageNet-1K with
earliersub-batches. 10 images per class. Following previous studies [32,
40], we test our models using five distinct architectures:
4.Experiments ResNet-18 [13], MobileNet-V2 [27], MnasNet1 3 [34],
EfficientNet-B0 [33], and RegNet-Y-8GF [26]. As shown
4.1.DatasetsandResultDetails
inTable5,ourproposedapproachdemonstratessignificant
We first run DELT on five standard benchmark tests in- better performance than other competitive methods on all
cluding CIFAR-10 (10 classes) [16], Tiny-ImageNet (200 thesearchitectures.
classes) [17], ImageNet-1K (1,000 classes) [7] and it
4.3.AblationStudy
variants of ImageNette (10 classes) [9], and ImageNet-
100 (100 classes) [35] with performances reported in Ta- Mosaic splicing pattern. Mosaic stitching method [2] in
ble 1 and Table 2. The evaluation protocol is follow- RDED selects four crops from the train set as the optimal
ing prior works [32, 40]. We compare DELT to six hyper-parameter,andputsthecontentsofthefourcropsinto
baseline dataset distillation algorithms including Match- asyntheticimagethatisdirectlyusedforpost-validation.In
ing Training Trajectories (MTT) [3], Improved Distri- thiswork,consideringthatweusedifferentdifficultylevels
bution Matching (IDM) [45], TrajEctory Matching with of selection for initialization, we examine different strate-
ConstantMemory(TESLA)[6], Squeeze-Recover-Relabel giesoftheMosaicsplicingpatterns,including1×1,2×2,
(SRe2L) [40], Difficulty-Aligned Trajectory-Matching 3×3,4×4,and5×5patches,asillustratedinFig.6. The
(DATM) [12], Realistic-Diverse-Efficient Dataset Distilla- ablation results are shown in Table 4a, it can be observed
tion (RDED) [32]. Following previous dataset distilla- that1×1achievesthebestaccuracy.
5ResNet-18 ResNet-101 MobileNet-v2
Dataset IPC SRe2L[40] RDED[32] DELT(Ours) SRe2L[40] RDED[32] DELT(Ours) RDED[32] DELT(Ours)
1 16.6±0.9 22.9±0.4 24.0±0.8 13.7±0.2 18.7±0.1 20.4±1.0 18.1±0.9 20.2±0.4
CIFAR-10 10 29.3±0.5 37.1±0.3 43.0±0.9 24.3±0.6 33.7±0.3 37.4±1.2 29.2±1.1 29.3±0.3
50 45.0±0.7 62.1±0.1 64.9±0.9 34.9±0.1 51.6±0.4 54.1±0.8 39.9±0.5 42.9±2.2
1 19.1±1.1 35.8±1.0 24.1±1.8 15.8±0.6 25.1±2.7 19.4±1.7 26.4±3.4 19.1±1.0
ImageNette 10 29.4±3.0 61.4±0.4 66.0±1.4 23.4±0.8 54.0±0.4 55.4±6.2 52.7±6.6 64.7±1.4
50 40.9±0.3 80.4±0.4 88.2±1.2 36.5±0.7 75.0±1.2 83.3±1.1 80.0±0.0 85.7±0.4
1 2.62±0.1 9.7±0.4 9.3±0.5 1.9±0.1 3.8±0.1 5.6±1.0 3.5±0.1 3.5±0.5
Tiny-ImageNet 10 16.1±0.2 41.9±0.2 43.0±0.1 14.6±1.1 22.9±3.3 42.8±0.9 24.6±0.1 26.5±0.5
50 41.1±0.4 58.2±0.1 55.7±0.5 42.5±0.2 41.2±0.4 58.5±0.3 49.3±0.2 51.3±0.5
10 9.5±0.4 36.0±0.3 28.2±1.5 6.4±0.1 33.9±0.1 22.4±3.3 23.6±0.7 15.8±0.2
ImageNet-100 50 27.0±0.4 61.6±0.1 67.9±0.6 25.7±0.3 66.0±0.6 70.8±2.3 51.5±0.8 55.0±1.8
100 - 74.5±0.4 75.1±0.2 - 73.5±0.8 77.6±1.8 70.8±1.1 76.7±0.3
10 21.3±0.6 42.0±0.1 45.8±0.1 30.9±0.1 48.3±1.0 48.5±1.6 32.3±0.2 35.1±0.5
ImageNet-1K 50 46.8±0.2 56.5±0.1 59.2±0.4 60.8±0.5 61.2±0.4 66.1±0.5 52.8±0.4 56.2±0.3
100 52.8±0.3 59.8±0.1 62.4±0.2 62.8±0.2 - 67.6±0.3 56.2±0.1 58.9±0.3
Table1. ComparisonwithSOTAdatasetdistillationmethodsusingrelativelylarge-scalebackbonesonfivebenchmarksacrossdifferent
scales. MobileNet-v2ismodifiedtomatchthelowresolutionsofCIFAR-10andTiny-ImageNetfollowing[44]. Duetothetablespace
limitation,someothermethodsthatareweakerorcomparabletoRDEDarenotlisted,suchasCDAandG-VBSM.SinceIPC=1isnot
applicabletouseEarlyLatestrategy,thusunderIPC=1setting,thesingleimageineachclassisoptimizedwithaconstantiteration.
ConvNet
Dataset IPC MTT[3] IDM[45] TESLA[6] DATM[12] RDED[32] DELT(Ours)
1 8.8±0.3 10.1±0.2 - 17.1±0.3 12.0±0.1 12.4±0.8
Tiny-ImageNet 10 23.2±0.2 21.9±0.3 - 31.1±0.3 39.6±0.1 40.0±0.4
50 28.0±0.3 27.7±0.3 - 39.7±0.3 47.6±0.2 48.6±0.2
10 - 17.1±0.6 - - 29.6±0.1 24.7±1.5
ImageNet-100 50 - 26.3±0.4 - - 50.2±0.2 51.9±1.1
100 - - - - 58.6±0.4 61.5±0.5
1 - - 7.7±0.2 - 6.4±0.1 8.8±0.5
ImageNet-1K 10 - - 17.8±1.3 - 20.4±0.1 31.3±0.8
50 - - 27.9±1.2 - 38.4±0.2 41.7±0.1
Table2. ComparisonwithSOTAdatasetdistillationmethodsusingsmall-scalebackbonearchitectureonthreedatasets. Following[3,32,
45],Conv-4forTiny-ImageNetandImageNet-1K,Conv-6forImageNet-100.Entriesmarkedwith“-”aremissingduetoscalabilityissue.
Initialization. We examine how different initialization 500RIachievesthebestaccuracy.
strategies affect final performance, including: choosing
Early-onlyvs. EarlyLate. Early-onlyisequivalenttous-
lowest probability crops, medium probability crops and
ingconstantMItooptimizeeachimage.Thiswilltransform
highestprobabilitycrops.OurresultsareshowninTable4b.
to baseline batch-to-global matching of CDA [39] + real
Overall,theperformancegapbetweendifferentstrategiesis
image initialization. Our results in Table 4d clearly show
notsignificant,andselectingthemediumprobabilitycrops
that the EarlyLate training bring a significant improve-
astheinitializationachievesthebestaccuracy.
mentonfinalperformance. Moreimportantly,thisstrategy
Optimization iterations. We examine two types of opti-
isthekeyfactorinenhancinggenerationdiversity.
mization iterations: maximum iteration (MI) for the ear-
Realimagestitchingvs.Minimaxdiffusionvs.Ours.We
liest batch training and round iteration (RI). MI presents
furthercompareourapproachwithrealimagestitching[32]
thenumberofoptimizationiterationsthattheearliestbatch
anddiffusiongeneration[11]. Theresultsarepresentedin
goes through, i.e., the maximum number of iterations for
Table 4e. While the first two methods produce more real-
the first batch’s gradient updating. RI represents the num-
istic images, each image contains limited information. In
berofiterationsusedforeachroundinFig.4. Itessentially
contrast,ourmethodachievesthebestfinalperformance.
indicatestheiterationgapbetweentheoptimizationoftwo
adjacentbatches. AsshowninTable4c,wetestMIvalues Withoutrealimageinitialization.OurEarlyLatestrat-
of1K,2K,and4K,using500and1KiterationsforeachRI. egyenhancestheperformanceof1∼2%overtheinitializa-
NotethatwhenMIissetto1K,itisnotfeasibletouse1K tion. Without initialization, our method improves consis-
asRI.Theresultsshowthat4K(sameas[39,40])MIand tentlywith2.4%asshowninTable3.
6Strategy SRe2L[40]w/oInitw/oEarlyLate CDA[39]w/oInitw/oEarlyLate CDA[39]w/oInitw/EarlyLate
Acc. 46.8 53.5 55.9
(+2.4)
Table3.PerformancecomparisonwithoutrealimageinitializationonImageNet-1KwithIPC50.
Selectioncriteria Top1acc
#Patches Top1acc
Lowestprobability 57.55
1×1 57.57
Mediumprobability 57.67
2×2 56.92
Highestprobability 57.03
3×3 56.62
4×4 56.71 (b) Selection criteria. Initializing 1×1 images selected according to
5×5 56.51 teachermodel’sprobability
(a)Numberofpatches.Ablationoninitializingdifferentnumbersofscor- Dataset CDA[39]+Ourinit. Ours
ingpatches.ResultsarefromResNet-18onImageNet-1Kfor500iterations
ImageNet-1K 43.5 45.8
tosynthesize50IPCs.
Tiny-ImageNet 42.2 43.0
Iterations RoundIterations CIFAR-10 39.4 43.0
(MI) 500 1K
(d)Ablationoninit.andEarlyLateunderIPC10.
1K 44.87 43.71
IPC RDED[32] MinimaxDiffusion[11] Ours
2K 45.61 44.40
4K 46.42 44.66 10 42.0 44.3 45.8
50 56.5 58.6 59.2
(c)RoundIterations. Top-1acc. ofourmethodforIPC10usingdifferent
rounditerationswithResNet-18. (e)Comparisonwithrealanddiffusiongenerateddata.
Table4.AblationexperimentsonvariousaspectsofourframeworkwithResNet-18onImageNet-1K.
Recover/Validation ResNet-18 EfficientNet-B0 MobileNet-V2 MnasNet1 3 RegNet-Y-8GF
SRe2L[40] 41.9 41.9 33.1 39.3 51.5
CDA[39] 42.2 43.9 34.2 39.7 52.9
ResNet-18 G-VBSM[28] 41.4 42.6 33.5 40.1 52.2
RDED[32] 42.3 42.8 34.4 40.0 54.8
Ours 46.4 47.1 36.1 40.7 57.5
(+4.1) (+4.3) (+1.7) (+0.7) (+2.7)
Table5.Cross-architecturegeneralization.ResultsareevaluatedonIPC10.
4.4.ComputationalAnalysis havetheirowncharacteristics.MinimaxDiffusionleverages
thediffusionmodeltosynthesizeimageswhichiscloseto
For image optimization-based methods like SRe2L and
therealones. However,asinouraboveablation,bothreal
CDA,thetotalcomputationalcostiscalculatedasN ×T,
and diffusion-generated data are inferior to ours. MTT re-
where N is the MI. In our EarlyLate scheme, the first
sults show noticeable artifacts and distortions, the objects
batchimagesundergoT iterations(whereT = T). Sub-
1 1 in all images are located in the middle of the generations,
sequent batches are processed with progressively fewer it-
the diversity is limited. IDC results also show distorted
erations, such as T (T = T −RI) for the next set, and
2 2 1 andlessrecognizabledogimages,butdiversityisincreased.
soforth. TheiterationsforthefinalbatcharereducedtoRI
SRe2Lexhibitssomedogfeaturesbutwithsignificantdis-
whichis1/j ofthestandardcount(wherej =4or8inour
tortionsandsimilarsimplebackground.SCDDshowsmore
ablation),thetotalnumberofouroptimizationiterationsre-
recognizable dog features but still the color is simple and
quiredisN ×T − j(j−1)RI,whichisroughly2/3ofprior
2 monochromatic, the same situation happens in CDA. G-
batch-to-globalmatchingmethods.Ourrealtimeconsump-
VBSM shows more colorful patterns, possibly due to re-
tionsfordatagenerationareshowninTable6,notethatthe
coveryfrommultipledifferentnetworks,butallgenerations
smaller the dataset like CIFAR, the more time is spent on
are in the same pattern and the diversity is not large. Our
loadingandprocessingthedata,ratherthantraining.
approach’s synthetic images exhibit a higher degree of di-
versity, including both compressed distorted images from
4.5.VisualizationofDELT
long-optimized initializations and clear, recognizable dog
imagesfromshort-optimizedinitializations,auniquecapa-
Fig. 7 illustrates a comprehensive visual comparison be-
bilitynotpresentinothermethods.
tween randomly selected synthetic images from our dis-
tilled dataset and those from the real image patches [32],
4.6.ApplicationI:Data-freeNetworkPruning
MinimaxDiffusion [11], MTT [3], IDC [15], SRe2L [40],
SCDD[46],CDA[39]andG-VBSM[28]distilleddata. It Our distilled dataset acts as a multifunctional training tool
canbeobservedthattheimagesgeneratedbyeachmethod and boosts the adaptability for diverse downstream appli-
7Real M-Diffusion MTT IDC SRe2L SCDD CDA G-VBSM Ours
Class 207
Figure7.Distilleddatasetvisualizationcomparedwithotherimageoptimization-basedmethods.
Dataset(hours)undersame4Kiterationsforallmethods
Method ImageNet-1K Tiny-ImageNet CIFAR-10
G-VBSM[28] 114.1 5.5 0.195
SRe2L[40] 29.0 5.0 0.084
CDA[39] 29.0 5.0 0.084
Ours(RI=1K) 18.8 3.6 0.084
(↓35.2%) (↓28.0%) (↓0.0%)
Ours(RI=500) 17.6 3.4 0.083
(↓39.3%) (↓32.0%) (↓1.1%)
Table6. Actualcomputationalconsumption(hoursunderIPC50)indatasynthesiswithimageoptimization-basedmethodsonasingle
NVIDIA4090GPU.Atotal4Kiterationsareusedforallmethodsanddatasetstoensurefaircomparisons.“RI”representsrounditerations.
cations. We validate its utility in the scenario of data-free
network pruning [31]. Table 7 shows the applicability of 40
SRe2L
G-VBSM
our dataset in this task when pruning 50% weights, where 35 Ours
it significantly surpasses previous methods such as SRe2L 30
andRDEDunderIPC10and50. 25
20
SRe2L[40] RDED[32] Ours 15
10
IPC10 12.5 13.2 17.9
(+4.7)
IPC50 31.7 42.8 44.8 100 200 300 400 500 600 700 800 900
(+2.0) Class Number
Figure8.Continuallearningresults.
Table 7. Accuracy of data-free network pruning using slim-
perimentsonImageNet-1K,comparingourresultswiththe
ming[22]onVGG11-BN[30].
baselines G-VBSM and SRe2L. As shown in Fig. 8, our
DELTdistilleddatasetsignificantlyoutperformsG-VBSM,
4.7.ApplicationII:ContinualLearning
with an average improvement of about 10% in 100-step
WeexaminetheeffectivenessofDELTgeneratedimagesin class-incrementallearningtask. Thishighlightsthesignifi-
thecontinuallearningscenario.Followingthesetupinprior cantbenefitsofdeployingDELT,particularlyinmitigating
studies[40,42],weperform100-stepclass-incrementalex- thechallengesofcontinuallearning.
8
)%(
ycaruccA
1-poT5.Conclusion dataset distillation through directed weight adjustment. In
Advancesinneuralinformationprocessingsystems,2024. 3
Wehaveintroducedanewtrainingstrategy, EarlyLate,
[9] Fastai.Fastai/imagenette:Asmallersubsetof10easilyclas-
toimproveimagediversityinbatch-to-globalmatchingsce-
sifiedclassesfromimagenet,andalittlemorefrench. 5
nariosfordatasetdistillation. Theproposedapproachorga-
[10] Spyros Gidaris and Nikos Komodakis. Dynamic few-shot
nizespredefinedIPCsamplesintosmaller,manageablesub-
visual learning without forgetting. In Proceedings of the
tasks and utilizes local optimizations. This strategy helps IEEE conference on computer vision and pattern recogni-
in refining each subset into distributions characteristic of tion,pages4367–4375,2018. 5
different phases, thereby mitigating the homogeneity typi- [11] JianyangGu,SaeedVahidian,VyacheslavKungurtsev,Hao-
cally caused by a singular optimization process. The im- nanWang,WeiJiang,YangYou,andYiranChen. Efficient
ages refined through this method exhibit robust general- datasetdistillationviaminimaxdiffusion.InCVPR,2024.6,
ization across the entire task. We have extensively evalu- 7
ated this approach on CIFAR, Tiny-ImageNet, ImageNet- [12] ZiyaoGuo,KaiWang,GeorgeCazenavette,HuiLi,Kaipeng
1K, and its variants. Our empirical findings indicate that Zhang,andYangYou. Towardslosslessdatasetdistillation
ourapproachsignificantlyoutperformspriorstate-of-the-art viadifficulty-alignedtrajectorymatching.InTheTwelfthIn-
methodsacrossvariousIPCconfigurations. ternationalConferenceonLearningRepresentations,2024.
5,6
Acknowledgments [13] KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun.
Deep residual learning for image recognition. In Proceed-
ThisresearchissupportedbytheMBZUAI-WISJointPro- ingsoftheIEEEconferenceoncomputervisionandpattern
gramforAIResearchandtheGoogleResearchawardgrant. recognition,pages770–778,2016. 5
[14] Yang He, Lingao Xiao, Joey Tianyi Zhou, and Ivor Tsang.
Multisizedatasetcondensation. ICLR,2024. 1,2,3
References
[15] Jang-Hyun Kim, Jinuk Kim, Seong Joon Oh, Sangdoo
Yun, Hwanjun Song, Joonhyun Jeong, Jung-Woo Ha, and
[1] Yingbin Bai, Erkun Yang, Bo Han, Yanhua Yang, Jiatong
HyunOhSong.Datasetcondensationviaefficientsynthetic-
Li,YinianMao,GangNiu,andTongliangLiu. Understand-
data parameterization. In Proceedings of the 39th Interna-
ingandimprovingearlystoppingforlearningwithnoisyla-
tionalConferenceonMachineLearning,2022. 7
bels. AdvancesinNeuralInformation Processing Systems,
[16] AlexKrizhevsky,GeoffreyHinton,etal. Learningmultiple
34:24392–24403,2021. 4
layersoffeaturesfromtinyimages. 2009. 5
[2] Alexey Bochkovskiy, Chien-Yao Wang, and Hong-
[17] Ya Le and Xuan Yang. Tiny imagenet visual recognition
Yuan Mark Liao. Yolov4: Optimal speed and accuracy of
challenge. CS231N,7(7):3,2015. 5
objectdetection. arXivpreprintarXiv:2004.10934,2020. 5
[3] George Cazenavette, Tongzhou Wang, Antonio Torralba, [18] Saehyung Lee, Sanghyuk Chun, Sangwon Jung, Sangdoo
Alexei A Efros, and Jun-Yan Zhu. Dataset distillation Yun, and Sungroh Yoon. Dataset condensation with con-
by matching training trajectories. In Proceedings of the trastive signals. In International Conference on Machine
IEEE/CVF Conference on Computer Vision and Pattern Learning,pages12352–12364.PMLR,2022. 1,2
Recognition,pages4750–4759,2022. 1,2,3,5,6,7 [19] HaoyangLiu,TianchengXing,LuweiLi,VibhuDalal,Jin-
[4] XuxiChen,YuYang,ZhangyangWang,andBaharanMirza- grui He, and Haohan Wang. Dataset distillation via the
soleiman.Datadistillationcanbelikevodka:Distillingmore wassersteinmetric. arXivpreprintarXiv:2311.18531,2023.
timesforbetterquality.InTheTwelfthInternationalConfer- 1,3
enceonLearningRepresentations,2024. 1,2,3 [20] Risheng Liu, Jiaxin Gao, Jin Zhang, Deyu Meng, and
[5] EkinDCubuk,BarretZoph,JonathonShlens,andQuocV ZhouchenLin. Investigatingbi-leveloptimizationforlearn-
Le. Randaugment: Practical automated data augmen- ingandvisionfromaunifiedperspective: Asurveyandbe-
tation with a reduced search space. In Proceedings of yond. IEEETransactionsonPatternAnalysisandMachine
the IEEE/CVF conference on computer vision and pattern Intelligence,44(12):10045–10067,2021. 3
recognitionworkshops,pages702–703,2020. 11 [21] SonghuaLiu,KaiWang,XingyiYang,JingwenYe,andXin-
[6] JustinCui,RuochenWang,SiSi,andCho-JuiHsieh.Scaling chaoWang. Datasetdistillationviafactorization. Advances
updatasetdistillationtoimagenet-1kwithconstantmemory. in Neural Information Processing Systems, 35:1100–1113,
In International Conference on Machine Learning, pages 2022. 1,2
6565–6590.PMLR,2023. 1,2,3,5,6 [22] Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang,
[7] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Shoumeng Yan, and Changshui Zhang. Learning efficient
andLiFei-Fei. Imagenet: Alarge-scalehierarchicalimage convolutionalnetworksthroughnetworkslimming. InPro-
database. In2009IEEEconferenceoncomputervisionand ceedingsoftheIEEEinternationalconferenceoncomputer
patternrecognition,pages248–255.Ieee,2009. 5 vision,pages2736–2744,2017. 8
[8] Jiawei Du, Xin Zhang, Juncheng Hu, Wenxin Huang, and [23] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,
Joey Tianyi Zhou. Diversity-driven synthesis: Enhancing James Bradbury, Gregory Chanan, Trevor Killeen, Zeming
9Lin,NataliaGimelshein,LucaAntiga,etal.Pytorch:Anim- [39] ZeyuanYinandZhiqiangShen. Datasetdistillationinlarge
perativestyle, high-performancedeeplearninglibrary. Ad- dataera. arXivpreprintarXiv:2311.18838,2023. 1,3,5,6,
vancesinneuralinformationprocessingsystems,32,2019. 7,8,11
11 [40] ZeyuanYin,EricXing,andZhiqiangShen.Squeeze,recover
[24] Lutz Prechelt. Early stopping-but when? In Neural Net- andrelabel: Datasetcondensationatimagenetscalefroma
works:Tricksofthetrade,pages55–69.Springer,2002. 4 newperspective. InNeurIPS,2023. 1,3,5,6,7,8,11
[25] TianQin,ZhiweiDeng,andDavidAlvarez-Melis. Distribu- [41] Yihua Zhang, Prashant Khanduri, Ioannis Tsaknakis,
tionaldatasetdistillationwithsubtaskdecomposition. arXiv Yuguang Yao, Mingyi Hong, and Sijia Liu. An introduc-
preprintarXiv:2403.00999,2024. 3 tion to bi-level optimization: Foundations and applications
[26] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, in signal processing and machine learning. arXiv preprint
Kaiming He, and Piotr Dolla´r. Designing network design arXiv:2308.00788,2023. 3
spaces.InProceedingsoftheIEEE/CVFconferenceoncom- [42] BoZhaoandHakanBilen. Datasetcondensationwithdis-
puter vision and pattern recognition, pages 10428–10436, tributionmatching. InIEEE/CVFWinterConferenceonAp-
2020. 5 plications of Computer Vision, WACV 2023, Waikoloa, HI,
[27] MarkSandler,AndrewHoward,MenglongZhu,AndreyZh- USA,January2-7,2023,2023. 1,2,3,8
moginov, and Liang-Chieh Chen. Mobilenetv2: Inverted [43] BoZhao,KondaReddyMopuri,andHakanBilen. Dataset
residuals and linear bottlenecks. In Proceedings of the condensation with gradient matching. arXiv preprint
IEEE conference on computer vision and pattern recogni- arXiv:2006.05929,2020. 1,2,3,5
tion,pages4510–4520,2018. 5
[44] Borui Zhao, Quan Cui, Renjie Song, Yiyu Qiu, and Jiajun
[28] Shitong Shao, Zeyuan Yin, Muxin Zhou, Xindong Zhang,
Liang. Decoupledknowledgedistillation. InProceedingsof
andZhiqiangShen. Generalizedlarge-scaledatacondensa-
the IEEE/CVF Conference on computer vision and pattern
tionviavariousbackboneandstatisticalmatching.InCVPR,
recognition,pages11953–11962,2022. 6
2024. 1,2,3,7,8
[45] GanlongZhao,GuanbinLi,YipengQin,andYizhouYu.Im-
[29] ZhiqiangShenandEricXing. Afastknowledgedistillation
proved distribution matching for dataset condensation. In
frameworkforvisualrecognition. InEuropeanConference
ProceedingsoftheIEEE/CVFConferenceonComputerVi-
onComputerVision,pages673–690.Springer,2022. 11
sion and Pattern Recognition, pages 7856–7865, 2023. 5,
[30] KarenSimonyanandAndrewZisserman. Verydeepconvo-
6
lutional networks for large-scale image recognition. arXiv
[46] MuxinZhou,ZeyuanYin,ShitongShao,andZhiqiangShen.
preprintarXiv:1409.1556,2014. 8
Self-superviseddatasetdistillation: Agoodcompressionis
[31] Suraj Srinivas and R Venkatesh Babu. Data-free param-
allyouneed. arXivpreprintarXiv:2404.07976,2024. 7
eter pruning for deep neural networks. arXiv preprint
[47] YongchaoZhou,EhsanNezhadarya,andJimmyBa.Dataset
arXiv:1507.06149,2015. 8
distillationusingneuralfeatureregression.AdvancesinNeu-
[32] PengSun,BeiShi,DaiweiYu,andTaoLin.Onthediversity
ralInformationProcessingSystems,35:9813–9827,2022.1,
andrealismofdistilleddataset: Anefficientdatasetdistilla-
2
tionparadigm. InCVPR,2024. 2,4,5,6,7,8,11
[33] MingxingTanandQuocLe. Efficientnet:Rethinkingmodel
scalingforconvolutionalneuralnetworks. InInternational
conferenceonmachinelearning,pages6105–6114.PMLR,
2019. 5
[34] MingxingTan, BoChen, RuomingPang, VijayVasudevan,
Mark Sandler, Andrew Howard, and Quoc V Le. Mnas-
net: Platform-aware neural architecture search for mobile.
InProceedingsoftheIEEE/CVFconferenceoncomputervi-
sionandpatternrecognition,pages2820–2828,2019. 5
[35] Yonglong Tian, Dilip Krishnan, and Phillip Isola. Con-
trastivemultiviewcoding. InComputerVision–ECCV2020:
16th European Conference, Glasgow, UK, August 23–28,
2020, Proceedings, Part XI 16, pages 776–794. Springer,
2020. 5
[36] Tongzhou Wang, Jun-Yan Zhu, Antonio Torralba, and
Alexei A Efros. Dataset distillation. arXiv preprint
arXiv:1811.10959,2018. 1,2
[37] ZhouWangandAlanCBovik. Meansquarederror: Love
itorleaveit? anewlookatsignalfidelitymeasures. IEEE
signalprocessingmagazine,26(1):98–117,2009. 3
[38] LingaoXiaoandYangHe. Arelarge-scalesoftlabelsnec-
essary for large-scale dataset distillation? In Advances in
neuralinformationprocessingsystems,2024. 3
10Appendix
A.Limitations
Ourmethodeffectivelyavoidstheissueofinsufficientdata
diversitygeneratedbybatch-to-globalmethodsandreduces
thecomputationalcostofthegenerationprocess. However,
there is still a performance gap when training the model
onourgenerateddatacomparedtotrainingontheoriginal
dataset. Also,ourshort-optimizeddataexhibitssimilarap-
pearance and semantic information to the original images,
whichhasdemonstratedbetterprivacyprotectionthanprior
train-freemethodsbutmaystillpotentiallyleaktheprivacy
oftheoriginaldatasettosomeextent.
B.TrainingDetails
Forreproducibility,weprovideallourhyper-parameterset-
tings used in our experiments in Table 8, we outline such
detailsbelow.
SqueezingandPre-trainedmodels. Followingtheprevi-
ousworks[32,39,40],weusetheofficialPyTorch[23]pre-
trainedResNet-18modelforImageNet-1K,andweusethe
sameofficialTorchvision[23]codetoobttaiinnyo-uimrpareg-teranienetd
models,ResNet-18andConvNet,fortheotherdatasets.
Figure9. SyntheticimagevisualizationsonTiny-ImageNetgen-
Ranking. For our initialization, we simply use ResNet-18 eratedbyourDELT.
pre-trainedmodelstorankandselectthemediumimagesas
initialization for all our datasets, except for ImageNet-100
where we simply extract the medium images based on the
rankingsoftheoriginalImageNet-1K.
Recovery. For our synthetic stage, we provide the details
ofgeneralhyper-parametersusedfordifferentdatasets,in-
cluding ImageNet-1K, ImageNet-100, ImageNette, Tiny-
ImageNet, and CIFAR10, in Table 8b. Synthesizing a sin-
gleimageperclass,i.e.,IPC=1,isspecialaswecannotuse
rounds,soweapplyindividualnumbersofiterationsbased
on both the dataset scale and the validation teacher model
asoutlinedinTable8c.
Validation. Thisincludesthesoft-labelgeneration[29]as
used in SRe2L, post-training and evaluation. We outline
such details in Table 8a. We use timm’s version of Ran-
dAugment[5]withdifferentsettingsdependingonthesyn-
thesizeddatasetbeingvalidated,asshowninTable8c.
C.MoreVisualization
We provide more visualizations on synthetic Tiny-
ImageNet,ImageNetteandCIFAR-10datasetsinFig.9,10, Figure10. SyntheticimagevisualizationsonImageNettegener-
11. Ineachfigure,eachcolumnrepresentsadifferentclass, atedbyourDELT.
with images progressing from long optimization at the top
toshortoptimizationatthebottom.
frameworks. The performance comparison for IPC 50 on
ImageNet-1K is shown in Table 9. It can be observed that
D.MoreAblation
theproposedEarlyLatestrategyenhancestheperformance
Performance comparison w/ and w/o EarlyLate. We byaround1%withtheinitialization.
compare using the recent CDA and SRe2L as the base Comparison of random, ascending and descending or-
11(a)Validationsettings (b)Recoverysettings
config value config value
optimizer AdamW α 0.01
BN
0.001(all) optimizer Adam
baselearningrate
0.0025(MobileNet-v2) baselearningrate 0.25
weightdecay 0.01
momentum β ,β =0.5,0.9
1 2
100(IPC50)
batchsize 100
batchsize 50(IPC10)
learningrateschedule cosinedecay
10(IPC1)
recoveryiteration 4,000
learningrateschedule cosinedecay
rounditeration 500[IPC10,50,100]
trainingepoch 300
initialization topmedium
RandAugment
augmentation RandomResizedCrop augmentation RandomResizedCrop
RandomHorizontalFlip
(c)Dataset-specificsettingsinrecovery
config CIFAR10 Tiny-ImageNet ImageNette ImageNet-100 ImageNet-1K
RandAugment(m) 5 4 6 6 6
RandAugment(n) 4 3 2 2 2
RandAugment(mstd) 1.0 1.0 1.0 1.0 1.0
2K(R18) 500(R18) 1K(R18) - 3K(Conv4)
3K(R101) 500(R101) 1K(R101) - -
IPC1RecoveryIterations
2K(MobileNet) 500(MobileNet) 2K(MobileNet) - -
- 1K(Conv4) 4K(Conv5) - -
Table8.Hyper-parametersettings.
Initialization SRe2L+w/Initw/oEarlyLate CDA+Initw/oEarlyLate CDA+w/Init+w/EarlyLate(Ours)
2×2 55.3 56.9 58.2
(+1.3)
3×3 55.8 56.6 58.1
(+1.5)
4×4 55.2 56.7 57.4
(+0.7)
5×5 54.6 56.5 57.3
(+0.8)
Table9.Performancecomparisonw/andw/oEarlyLateonImageNet-1KunderIPC50.
Order DELT To order them, we start with the median, and we go back
Random 67.9 and forth expanding the window around the median until
Ascending 67.2 wecoverthenumberofIPCs,refertoFig.5ofthemainpa-
Descending 67.7 per for details. The rationale is that these patches present
OurDELT 68.2
a medium difficulty level for the teacher, allowing more
potential for information enhancement through distillation
Table 10. Impact of using different ordering on ImageNet-100
gradientswhilehavingagoodstartingpointofinformation.
whenhavingthesameinitializedimagesofthemedianprobability.
Weempiricallyvalidateitbycomparingdifferentstrategies
inTable4bofthemainpaper.
SelectionStrategy DELT
AsshowninTable10,wepresenttheimpactofusingdif-
Random 67.7
Ascending 66.9 ferentorderingonImageNet-100whenhavingthesameini-
Descending 67.3 tializedimages, thosearoundthemedian. Wealsoinclude
OurDELT 68.2
a comparison of different initialization strategies based on
the order in Table 11. Unlike Table 10, the initialized im-
Table11. Comparisonoftheperformanceofdifferentinitializa-
agesherearedifferentacrossdifferentstrategies.
tionstrategies.Theinitializedimagesaredifferent.
Different initial crop ranges in random crop augmen-
tation for our DELT method. Table 12 compares differ-
ders by patch probability. In our DELT, we select the entinitialcroprangesinrandomcropaugmentationforour
N patcheswithscoresaroundthemedianfromtheteacher, DELTmethod. Asshownintheresults,the0.08-1.0range
wherethescorerepresentstheprobabilityofthetrueclass. yieldsthebestperformance,whichistheablationandsup-
12DELT (RI=500)
CDA/SRe2L
200
G-VBSM
150
100
50
0
20 40 60 80 100
Number of Synthetaic IPCs
50
DELT (RI=500)
DELT (RI=1K)
40
CDA/SRe2L
30
20
10
20 40 60 80 100
Number of Synthetaic IPCs
Figure12.Visualizationofcomputationtimeconsumptiononour
DELTandothermethodsincludingCDA,SRe2L,andG-VBSM.
DELTandCDA/SRe2Lmaintainrelativelylowandconsis-
tentcomputationtimes, withDELTslightlyoutperforming
CDA/SRe2L.ThebottomsubfigurefurthercomparesDELT
with RIs of 500 and 1,000 against CDA/SRe2L, highlight-
ingthatbothconfigurationsofDELTofferlowerorcompa-
Figure11.SyntheticimagevisualizationsonCIFAR-10generated
rablecomputationtimestoCDA/SRe2LacrossIPCvalues,
byourDELT.
withminimalincreaseastheIPCcountrises. Theseresults
emphasizeDELT’sefficiencyincomputationtime,particu-
portforthedefaultsettinginourframework.
larlyincomparisontoG-VBSM,makingitacomputation-
RandomCropRange Top1-acc
allyefficientchoiceforscenarioswithlargerdatasets.
0.08-1.0 67.8
0.2-1.0 67.3
0.5-1.0 66.3
0.8-1.0 66.3
Table12. Differentinitialcroprangesinrandomcropaugmenta-
tionforourDELTmethod.
E.ComputationalEfficiency
Fig. 12 illustrates the computation time required for our
DELTmethodcomparedtoothermethods,includingCDA,
SRe2L, and G-VBSM, at various numbers of IPCs. The
top subfigure shows a comparison of DELT (with a RI
of 500), CDA/SRe2L, and G-VBSM, where G-VBSM
demonstrates the highest computation time, scaling signif-
icantly as the number of IPCs increases. In contrast, both
13
)sruoH(
emiT
latoT
)sruoH(
emiT
latoT