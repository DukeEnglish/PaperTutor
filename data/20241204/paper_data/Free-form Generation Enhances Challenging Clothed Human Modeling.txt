Free-form Generation Enhances Challenging Clothed Human Modeling
HangYe XiaoxuanMa HaiCi WentaoZhu YizhouWang
SchoolofComputerScience,CenteronFrontiersofComputingStudies,PekingUniversity
{yehang, maxiaoxuan, cihai, wtzhu, yizhou.wang}@pku.edu.cn
(a) (b)
Replication
Part Seg.
Deformation
LBS-based
Unclothed Clothed
Generation
Human Human
Free-form
POP FITE Ours
Figure1. (a)Anoverviewofourframeworkformodelingclothedhumans.Basedonthespecificmodelingneedsofdifferentregions,
we employ a dedicated strategy to handle various clothing areas. Specifically, for loose regions (green) that are less affected by body
movementsandrequiremorefreedom, weproposefree-formgenerationtoenhanceflexibility. Fornear-bodyclothingareas(blue), we
apply LBS-based deformation, while unclothed regions (yellow) that do not require deformation can be directly replicated. (b) Visual
comparisonbetweenpriorarts(POP[39], FITE[33])andourmethodonchallengingclothing. Ourmethodcapturesmorehigh-
fidelitydetailsandachievessuperiorvisualqualityandrealism.Codeisavailableathttps://alvinyh.github.io/FreeCloth.
Abstract tion. As for the generated regions, which correspond to
looseclothingareas,weintroduceanovelfree-form,part-
Achieving realistic animated human avatars requires ac- aware generator to model them, as they are less affected
curate modeling of pose-dependent clothing deformations. bymovements. Thisfree-formgenerationparadigmbrings
Existing learning-based methods heavily rely on the Lin- enhancedflexibilityandexpressivenesstoourhybridframe-
earBlendSkinning(LBS)ofminimally-clothedhumanmod- work, enablingittocapturetheintricategeometricdetails
els like SMPL to model deformation. However, these of challenging loose clothing, such as skirts and dresses.
methods struggle to handle loose clothing, such as long Experimental results on the benchmark dataset featuring
dresses, where the canonicalization process becomes ill- looseclothingdemonstratethatourmethodachievesstate-
defined when the clothing is far from the body, leading to of-the-artperformancewithsuperiorvisualfidelityandre-
disjointedandfragmentedresults. Toovercomethislimita- alism,particularlyinthemostchallengingcases.
tion, we propose a novel hybrid framework to model chal-
lengingclothedhumans. Ourcoreideaistousededicated 1.Introduction
strategiestomodeldifferentregions,dependingonwhether
they are close to or distant from the body. Specifically, we The emergence of clothed 3D human characters, often re-
segment the human body into three categories: unclothed, ferred to as “digital avatars”, has swiftly evolved into a
deformed, and generated. We simply replicate unclothed fundamental aspect across diverse industries such as gam-
regionsthatrequirenodeformation. Fordeformedregions ing[23],animation[16],virtualtry-on[44],etc. However,
closetothebody, weleverageLBStohandlethedeforma- itremainsanopenproblemtocreateavatarswithnaturally
1
4202
voN
92
]VC.sc[
1v24991.1142:viXrad d a gse i if nf fi wo gcr r aum i nnlti dkn t l sg o e ksc ic . dl ao dp At ith nlui t grn heg o [2utd h ,gr 1e hiv 1ie ,cnn 3otr 4nib c v ]y a e at nd ce t hi iv g io ee e n vr o as eme l psb e roo t or ld my uy t io i sop f ino nc ss gle o s rs t eu, h scs i uhni ln g tac s, s ,e s ti ru ht ic egi yhs -
Method
w/o
2
D re wn /d oe cr li on tg hi wn /g ot Le Bm Spl ofia pet l ee d nsurf la oc oe sem co ld otel hi in ng g
are highly dependent on artistic efforts and expert knowl- POP[39] ✗ ✓ ✓ ✓ ✗
edge. Toautomatethisprocess, recentstudies[38–40,70] SkiRT[40] ✗ ✗ ✗ ✓ ✓
adopt a data-driven approach to learn the pose-dependent FITE[33] ✗ ✗ ✗ ✗ ✓
CloSET[70] ✓ ✗ ✓ ✓ ✓
clothingdeformation. Specifically,theypredictlocaltrans-
Ours ✓ ✓ ✓ ✓ ✓✓
formationinthecanonicalspace,whichisfurtheraddedon
top of the human body template and then driven by LBS Table1.Comparisonofourmethodwithexistingworks.
transformation.
Nevertheless, this posing procedure often fails in terms
basedtransformations. Byprioritizingpart-awareposede-
of garments that differ greatly from the body shape and
tails over a direct global pose code, this approach ensures
topology, especially loose clothing such as skirts and long
a closer alignment between the generated clothing and the
dresses. The deformed shape is usually constrained to the
given poses, thereby enhancing the high fidelity and real-
minimally-clothed body, leading to split-like artifacts in
ismoftheresults. AsdemonstratedinFig.1(b),thanksto
modelingthelongdress(seeresultsofPOP[39]inFig.1).
the flexibility of the generator, our hybrid framework suc-
This is mainly due to the poorly defined canonicalization
cessfullygeneratesrealisticandintricatewrinklesforloose
process [40] in the region far away from the body, such
dressesandskirts,eliminatingpant-likeartifacts.
as the area between the legs. To alleviate this issue, re-
We conduct evaluations on long dresses and skirts with
cent works [33, 40, 70] propose a coarse-to-fine approach
diverse lengths, styles, and tightness levels. Experimen-
forpredictingdeformationsbasedonlearnedclothingtem-
tal results on the benchmark dataset featuring loose cloth-
plates. However, these approaches still confine the defor-
ing demonstrate that our method achieves state-of-the-art
mation within the LBS-based transformation, without ad-
(SOTA) performance with superior visual fidelity and re-
dressingthefundamentalchallengeofaccuratelymodeling
alism, particularly in the most challenging cases. To the
complexclothingfarfromthebody.Asaresult,thesemeth-
bestofourknowledge,wearethefirsttoleveragefree-form
ods still struggle to model loose and challenging clothing
generation to tackle learning-based clothed human model-
accurately(seeFig.1(b)andFig.4forcomparison).
ing. Being single-staged and end-to-end, our simple yet
Inthiswork,werevisitthetaskofclothedhumanmodel- effective paradigm significantly enhances the expressive-
ingfromanovelperspective. Ourpivotalinsightisthatin- ness of clothed avatars. It excels at capturing fine details
tegratingstructuralpriors,i.e.LBS,significantlyfacilitates of loose clothing without the need to render 2D positional
thetask,whereasrelyingentirelyonLBShampersflexibil- maps [33, 39, 40], extract subject-specific clothing tem-
ity. To that end, we propose a hybrid framework, leverag- plates [33, 40], or learn continuous LBS fields [33, 40],
ingthecomplementaryadvantagesofLBS-basedandLBS- while also offering the flexibility to model open surfaces.
freetechniques. Toachievethis, wefirstconductpartseg- Weemphasizethekeystrengthsofourfree-formparadigm
mentation to categorize surface points on the human body whencomparedtorecentSOTAmethodsinTab.1.
into three types: unclothed (yellow), deformed (blue), and Ourmaincontributionsaresummarizedasfollows:
generated (green), as shown in Fig. 1 (a). The yellow ar- • We propose a novel perspective on hybrid modeling for
eas represent unclothed parts (e.g. head, hands, and feet), clothed humans, allowing for customized modeling of
usually not covered by garments, which need no deforma- differentbodyareasbasedonpartsegmentation.
tion.Theblueareasrepresentpartsclosetothebody,where • We propose a free-form generator with structure-aware
we perform LBS-based deformation [33, 39, 40, 70]. The poseencodingtomodellooseclothingthatenhancesflex-
greenareasindicatelooseclothingregionsthatdeviatesig- ibilityandexpressiveness.
nificantly from the body and are therefore less affected by • Our hybrid framework, merging the merits of LBS de-
body movements. For these regions, we introduce a free- formation and free-form generator, delivers SOTA per-
formgeneratortomodelthedynamics. Finally,weobtaina formanceandenhancedvisualfidelityandrealism,espe-
completelyclothedhumanbymergingthethreebranches. ciallyinthemostchallengingcases.
To guide the free-form generation of loose clothing for
a posed human point cloud, we introduce structure-aware 2.RelatedWork
poseencoding.Weextractpart-basedposefeaturesfromthe
2.1.3DRepresentationsforClothedHuman
unclothedpointcloudandtransformthemintoaposecode.
Thegeneratorthenpredictslooseareasconditionedonthis Surface Meshes are efficient and compatible representa-
pose code and the garment type, without relying on LBS- tions for modeling 3D clothed humans. Prevailing ap-
2proaches represent clothing either as a deviation from the LBS-free Animation. A relevant work DPF [51] es-
body[4,6,37,45,62,63]orasaseparatelayer[14,15,30, capes LBS by directly optimizing a smooth deformation
48]. Nonetheless,thefixedtopologyofmeshesstrugglesto field.DPFgeneratesvisuallyimpressiveresultsbutrequires
generalizeacrossvaryingclothingtypes. frame-wiseoptimization,restrictingitspracticality.
In this work, we propose a hybrid approach to model
Neural Implicit Field offers more topological flexibil-
clothed humans, which can better exploit the complemen-
ity[41,47]andispromisingforreconstructingoranimating
taryadvantagesofLBS-basedandLBS-freeapproaches.
clothedhumans[7,8,40,54–56,68]. However,extracting
thesurfacefromthefieldiscomputationallyexpensive,lim-
3.Method
iting its practical use. Another line of works [50, 61, 64]
optimize neural radiance fields (NeRF) [42] from 2D hu- Our objective is to dress an unclothed and posed human
man images but lacks explicit geometry for accurate pose body with a specific clothing type and create a realistic
controlinanimation. Recently,3DGaussianSplatting[25] clothedhuman. TheoverallpipelineisillustratedinFig.2.
is introduced to improve real-time rendering with high vi- Consideringthevaryingimpactofbodymovementsondif-
sualfidelity. Severalstudies[21,29,32,43,46,71]employ ferentregionsofclothing,weproposeanovelhybridframe-
this explicit modeling technique to represent textured hu- workthatcombinesthreedistinctstrategiestomodelthese
manmodels. regions,i.e.unclothed,deformed,andgenerated. First,we
identifythesethreetypesofregionstocreateaclothing-cut
HybridApproachesemergerecently.DMTet[12,60]con-
map(Sec.3.1). Thenweproposetwoessentialmodulesto
sistsofanexplicittetrahedralgridandanimplicitdistance
modelthedeformedandgeneratedpartsaccordingly:(1)an
field. TeCH[20]andHumanNorm[19]explorethepoten-
LBS-based local deformation network (Sec. 3.2) to model
tial of DMTet in generating high-fidelity clothed humans
near-body clothing deformation, and (2) a free-form gen-
withenhancedgeometricdetails.
eration module that focuses on handling the more distant
Point Clouds enjoy efficiency as well as flexibility. Nev- clothing regions (Sec. 3.3). Finally, we describe the train-
ertheless, it’s still an open challenge to generate high- ingstrategyinSec.3.4.
resolution point clouds with fine geometric details. Prior
3.1.HumanPartSegmentation
works[3,9,10,13,38]grouppointsintopatchestomodel
the clothing but suffers from inter-patch discontinuity. While the free-form generator liberates the constraint of
POP[39]furtherimprovesthisbyintroducingfine-grained LBS-based deformation and enhances the expressiveness,
featureswithUVmaps. Anotherlineofworks[51,69,70] ourhybriddesignintroducesanimportantquestion: howto
focus on eliminating the “seaming” artifact. We use point automatically determine whether a point on the body sur-
clouds as our representation, as they offer greater topo- faceshouldbedeformedorgenerated? Toaddressthis,we
logical flexibility and faster inference speeds [39, 69] than computeagarment-specificclothing-cutmaptoexplicitly
meshesandimplicitfields. segmentthehumanbodyintodistinctregions, guidingour
modulestohandledifferentpartsexclusively.
2.2.AnimatingClothedHumanAvatars Wefirstlocatetheexposedareasunaffectedbygarment
coverage, such as the head, hands, and feet, which do not
LBS is a predominant technique in animating human undergodeformations. LetXu = {xu}Nu representthese
i i=1
avatars, enabling the rigid transformation of the surface
unclothed body points. Then, our key design is to seg-
pointincorrespondencewiththearticulatedmovementsof
ment the regions that are occluded by loose clothing, e.g.,
theunderlyingskeleton[2,36,49].
skirtsordresses,anddisableLBS-baseddeformationwithin
theseareas. Thisisachievedutilizingthefoundationmodel
LBS-based Animation. We roughly classify LBS-based SAM[28]tosegmenttheloosepartsfromtherenderednor-
methodsintoexplicitandimplicitmethods. Explicitmeth- mal maps. We then back-project the detected regions into
ods deal with explicit 3D data like meshes [37] and point 3Dspace,effectivelyidentifyingthelooseareas. Thispro-
clouds [38, 39, 70]. A common approach is to predict lo- cessalsouncoverstheremainingunclothedregions,suchas
caltransformation[38,39,70]relativetopre-definedskin- parts of the legs not covered by a skirt, which are merged
ning weights on SMPL [36]. Implicit methods further ex- into Xu, corresponding to the yellow region in Fig. 2.
tendLBStoimplicitfields. Skinningweightscanbeeither Please refer to the supplementary material for details of
learnedfromdata[7,22,40,56],obtainedbynearestneigh- defining the garment-specific clothing-cut map. Then, for
bor[5]ordiffusion[33]. However,itisnon-trivialtogetan near-body regions denoted in blue, we perform the LBS-
accurate skinning field without direct supervision or suffi- basedlocaldeformation(Sec.3.2), whileformodelingthe
cient data. Due to the inherent rigid transformation, LBS- loose regions denoted in green, we employ the free-form
basedmethodsareseverelylimitedtotightclothing. generation(Sec.3.3).
3Part Segmentation(Sec. 3.1) Local LBS-based Local Deformation (Sec. 3.2)
Pose Code
Residual Local
Attach Posed Transform
Coordinates Pose
as Features Encoder
Default UV Map Posed Vertices Canonical Vertices Pose Normal Deformed Points
Decoder
Local
Garment Code Canonical S
Preprocess Coordinate derah
Global
Pose Code
Unclothed Points
Full Point Cloud
Pose Encoder GarmGl eo nb ta Cl ode Generator Generated Points
Posed Points
Clothing-Cut Map Free-form Generation for Loose Clothing (Sec. 3.3)
Figure2. Overviewofourhybridframework. Givenanunclothedandposedbody,andaspecificgarmenttype,ourgoalistocreate
a realistic clothed human. We first segment the human parts into three different regions (Sec. 3.1): unclothed parts (yellow) need no
deformation, deformedparts(blue), andgeneratedparts(green). Thehybridframeworkcomprisestwoessentialmodules: (1)anLBS-
based local deformation network (Sec. 3.2) to obtain pose-dependent deformed points Xd that are close to the human body, and (2) a
free-formgeneratorthatfocusesongeneratingthemorelooseclothingregionsXg (Sec.3.3). Bymergingtheunclothed,deformed,and
generatedpoints,weultimatelyobtainthecompletepointcloudofaclothedhumanX.
3.2.LBS-basedLocalDeformation thebarycentriccoordinatesb =[b ,b ,b ]andtheasso-
i i1 i2 i3
ciated vertex indices s = [s ,s ,s ] of a body surface,
Giventhatclothingnearthebodysurfaceismoreinfluenced i i1 i2 i3
weobtainitslocalposecodezp ∈RMp asfollows:
by body movements, we can leverage the body structural i
priors to better guide the deformation of clothing in these
3
areas using LBS provided by a parametric human model, zp =(cid:88) (b ·ϕp ). (2)
i.e. SMPL-X [49] used in this work. Given a posed and
i ij sij
j=1
unclothed body model, we denote the posed vertices as
V = {v }Nt , where N is the number of vertices. We
k k=1 t
definethecorrespondingverticesinthecanonicalspaceas GarmentCode. Tocontroltheclothingtype,weintroduce
Vc = {vc k}N k=t 1. Unless otherwise stated, the superscript a spatial-aligned local garment code ϕg
k
∈ RMg for each
letterscrepresent“canonical”inthefollowingnotation. canonicalvertexonthebodysurfacefollowing[39]. Simi-
lartothecontinuouslocalposecodezp,weapplybarycen-
i
LocalPoseCode. Tomodelthepose-dependentdeforma- tricinterpolationtoconvertthediscretecodeintoacontinu-
tionoftheclothing,anaivewayistoconditionthedeforma- ousgarmentcodezg ∈RMg foranybodysurfacepointp ,
i i
tiononasingleposeencoding[7,56].However,laterworks asdefinedbyEq.(3). Thelocalgarmentcodeϕg islearned
k
pointoutthatfine-grainedper-pointgeometricposeencod- inanauto-decoding[47]manneranditissharedacrossall
ingscanserveasabetterposecondition[39,40,70].There- humanposes.
fore,followingCloSET[70],weemployPointNet++[53]to 3
(cid:88)
extract multi-scale local pose feature ϕp k ∈ RMp for each zg i = (b ij ·ϕg sij). (3)
body vertex v (see Fig. 2 for illustration) as defined in j=1
k
Eq.(1),whereM pdenotesthenumberoffeaturechannels. Inadditiontothelocalgarmentcodezg,wealsointroduce
i
{ϕp}Nt =E (Vc,V). (1)
aglobalgarmentcodehg ∈RMg,whichissharedwiththe
k k=1 d oneusedinourfree-formgenerationmodule. Thisensures
Note that we treat the canonical vertices Vc as a point consistencyinthetypesofclothinggeneratedbybothmod-
cloud, where the coordinates of each posed vertex are re- ules. Moredetailsabouttheglobalgarmentcodehg willbe
gardedasthefeaturesofeachpoint.Thesefeaturesarethen discussedinthenextsection.
usedasinputstothePointNet++E . Toobtaincontinuous
d
local pose code for any point p located on the body sur- LBS-basedLocalDeformation.Foranyquerypointp lo-
i i
face manifold, we diffuse the local pose feature ϕp on the catedon theposed bodysurface manifold, we concatenate
k
body surface by applying barycentric interpolation. Given its local pose code zp, the corresponding canonical point
i
4pc,andthegarmentcodeszg,hg togetherasafeaturevec- slightabuseofnotation).WealsoreplacetheoriginalPoint-
i i
torandpassthroughaposedecoderD[39]topredictdefor- Net [52] with PointNet++ [53] as pose encoder E to ex-
g
mationinthecanonicalspace: tract part-wise local features hp for each part, which are
k
then fused to produce a part-based pose code hp ∈ RMp.
[rc i,nc i]=D(zp i,zg i,hg,pc i), (4) Notethathp issharedforallvertices,whilethelocalpose
code zp in Sec. 3.2 is unique for each vertex. In contrast
where rc, nc denote per-vertex displacement and normal, i
i i todirectlyextractingglobalfeaturesfromtheoverallpoint
respectively. SeeFig.2fortheworkflow.
cloud,thisstructure-awaredesignbettercapturesthecorre-
Wethenaddthepredicteddeformationtothecanonical
lationbetweentheloosegarmentandtheunderlyingskele-
points pc and then apply a local transformation T using
i i ton.
LBSweighttoobtainthedeformedpointsxd intheposed
i hp =Max-Pooling({E (P )}Kb ). (7)
space,followingthecommonLBS-baseddeformationprac- g k k=1
ticeinrecentworks[33,39,40,70]: Giventhepart-basedposecodehpandthegarmentcode
hg,wegenerateasetofpointsXg intheposedspace:
xd =p +T ·rc =T ·(pc+rc). (5)
i i i i i i i
Likewise,thepredictednormalnc i istransformedtond i ac- Xg ={xg i}N i=g 1 =G(hp,hg), (8)
cordinglyviatherotationcomponentR ofthetransforma-
i wherehg isusedtocontrolthegarmenttypeandisshared
tionT :
i
nd =R ·nc, (6) between the two modules to ensure consistency. Note
i i i
that the LBS transformation is not involved in this pro-
whereT iiscomputedusingbarycentricinterpolationofthe cess, which circumvents the limitations of estimating non-
LBS-inducedbonetransformationpredefinedintheSMPL- rigid deformation of loose clothing, hence enabling “free-
X[49]bodymodel. Nowweobtainadeformedpointcloud form”generation. Fordetailedarchitecture,pleasereferto
Xd = {xd i}N i=d 1 withitsnormalsNd = {nd i}N i=d 1 thatcap- Sec.A.1inthesupplementarymaterial.
turesthepose-dependentclothingdeformation. Forclarity,
weomitthenormalnotationinFig.2. 3.4.Training
3.3.Free-formGenerationforLooseClothing Our method is trained in an end-to-end manner, where the
networks and the global garment codes are jointly opti-
Although LBS-based deformation works well for points
mizedusingthelossfunctiondefinedbelow:
thatareclosetothebodysurface, itencounterschallenges
when dealing with points that are farther away from the L = λ L +λ L +λ L +λ L +λ L .
cd cd n n rd rd rg rg col col
body.Thisiscausedbytheill-definedcanonicalizationpro-
(9)
cess[40]forthosepoints,resultingindifficultyinestimat-
ing the nonrigid transformations. This limitation becomes
ReconstructionLosses. Followingpreviousworks[39,40,
particularly evident when handling loose clothing such as
70], we employ the normalized Chamfer distance L to
skirts,asobservedintheresultsofPOP[39]inFig.4,where cd
minimizethebi-directionaldistancesbetweenthepredicted
theskirtsaretornapart. Giventheuniquecharacteristicsof
fullpointcloudX andtheground-truth(GT)humanpoint
skirts,itbecomesinfeasibletoaccuratelymodelpointsthat
cloud:
aredistantfromthebody,suchasthoselocatedbetweenthe
legs, solely relying on the LBS deformation. Conversely, N M
1 (cid:88) 1 (cid:88)
it should be considered as a separate and flexible part. L = min∥x −xˆ ∥2+ min∥x −xˆ ∥2,
cd N j i j 2 M i i j 2
Note that in another line of work [14, 15, 30, 48, 57], al- i=1 j=1
though these approaches model the garment as a separate (10)
layer, theystillrelyontheLBStomanipulatethegarment where xˆ j is the point sampled from the surface of the GT
deformation. Incontrast,weproposeafree-formapproach scan,N andM denotethenumberofthepredictedandGT
tomodelingloosegarments,gettingridofLBSentirely. points,respectively.AndthenormallossL niscalculatedas
the average L distance between the predicted normal and
1
itsnearestcounterpartintheGTpointcloud:
Structure-aware Pose Encoding. Conceptually, generat-
ingloosegarmentsgivenaspecificposecanbeinterpreted
N
as a point cloud completion task. To better condition on 1 (cid:88)
L = ∥n −nˆ(argmind(x ,xˆ ))∥ , (11)
humanposes,wemodifytheoff-the-shelfSpareNet[66]to n N
i=1
i xˆj i j 1
be structure-aware as our generator. We first segment the
humanbodysurfaceintoK semanticpartsanduniformly where nˆ(·) represents the normal of a GT point cloud and
b
sample posed points {P }Kb from these parts (with a n denotestheestimatednormalgeneratedbyourmodel.
k k=1 i
5Table2. QuantitativecomparisonofdifferentmethodsontheReSynth[39]datasetforeachsubject. WereportFIDscoresforthe
renderedmulti-viewnormalmaps,alongwithMSEerrors(inunitsof10−2)betweenthesemapsandtheGTnormals. Thebestresults
arehighlightedinbold,andthesecondbestareunderlined. ThesubjectIDsarelistedindescendingorderbasedontheloosenessofthe
clothing.Notably,theadvantagesofourmethodbecomemorepronouncedforthemostchallengingcases.
Subject All felice-004 janett-025 christine-027 anna-001 beatrice-025
Metric FID↓ MSE↓ FID↓ MSE↓ FID↓ MSE↓ FID↓ MSE↓ FID↓ MSE↓ FID↓ MSE↓
POP[39] 57.87 2.88 66.43 5.80 52.55 2.02 61.09 2.64 51.48 2.05 57.82 1.86
SkiRT[40] 53.32 2.72 63.27 5.70 48.23 2.03 55.84 2.44 50.26 1.81 54.00 1.60
FITE[33] 39.02 2.70 38.61 5.09 35.81 2.09 40.83 2.52 38.21 1.97 41.62 1.82
Ours 37.75 2.61 42.41 5.24 27.95 1.92 37.43 2.35 39.63 1.89 41.24 1.68
RegularizationLosses. Toconstraintthedeformedpoints Human GPT-4o
not far away from the body, we introduce a regularization POP 0.8 0.0
term L to penalize the L -norm of the pose-dependent
rd 2 SkiRT 5.9 16.0
displacement rc. In addition, the local and the global gar-
i FITE 29.9 28.0
mentcodes{zg,hg}areregularizedbytheirL -norm:
2 Ours 63.4 56.0
1
(cid:88)Nd (cid:88)Nd
1
0 20 40 60 0 20 40 60
L = ∥rc∥2, L = ∥zg∥2+∥hg∥2. Preference Rate (%) Preference Rate (%)
rd N i 2 rg N i 2 2
d i=1 i=1 d Figure3. Perceptualstudyresults. Acrossallexamples,63.4%
(12) ofhumanuserspreferourmethodoverthebaselines.Additionally,
ourmodelreceives56.0%ofthevotesfromtheGPT-4omodel[1].
CollisionLoss. Drawinginspirationfromtheliteratureon Theseresultshighlightthesignificantsuperiorityofourapproach,
garmentanimation[31,58,59],weproposeacollisionloss particularlyinhandlingthemostchallengingclothing.
topreventintersectionsbetweentheclothingandtheunder-
lyingbody,whichiscomputedusingthefollowingformula:
Metrics. As noted in prior work [33, 51], conventional
1
(cid:88)Ng regression-basedmetricslikeChamferDistancedonotac-
L = max{ϵ−d(xg),0}, (13) curatelyreflectmodelperformance. Instead,wefollowap-
c N j
g j=1 proaches in 3D human reconstruction [67, 68] by comput-
ing Mean Squared Error (MSE) between rendered multi-
whered(xg)representsthesigneddistancefunction(SDF)
j view normal maps from the point cloud and the GT. Ad-
valueofthegeneratedpointsrelativetotheunderlyingbody
ditionally, sinceouravatarmodelingisgenerative, weem-
field,andϵisapredefinedthresholdthatregulatesthemin-
ploy Fréchet Inception Distance (FID [17]) for evaluation
imumdistancebetweenthebodyandthegarment.
following Chupa [26]. To further assess visual quality, we
conductaperceptualstudywith50volunteers. Wealsouti-
4.Experiments
lizetheGPT-4omodel[1]toselectthebestresultsfromall
Datasets. Wetrainandevaluateourmethodandbaselines methods. Detailsoftheperceptualstudyanddiscussionsof
on the ReSynth [39] dataset, which is a synthetic dataset theevaluationmetricsareprovidedinSec.A.5andSec.B.1
capturing clothed human subjects with intricate geometric ofthesupplementarymaterials.
details and complex pose-dependent clothing deformation.
4.1.ComparisonwiththeState-of-the-arts
We use the official training and test split as [39]. Similar
to SkiRT [40], our main focus in this study lies in accu- Quantitative Evaluation. Tab. 2 presents the FID scores
rately modeling loose clothing. Our evaluation centers on andmeasuredMSEerrorsfortherenderedmulti-viewnor-
fivesubjectsadornedinskirtsanddressesofvariousstyles, mal maps. These metrics effectively characterize visual
lengths,andtightnesslevels. qualitywhilemaintainingproximitytothereferenceimage.
OurmethodachievesSOTAperformance,surpassingother
Baseline. To evaluate the representation power of our baselineswiththelowestFIDscoresandMSEerrors. This
model,wecompareitwiththeSOTApoint-basedmethods demonstrates that hybrid modeling enhances performance,
(Sec.4.1): POP[39],SkiRT[40]andFITE[33]. Notethat particularlyforlooseskirts(e.g.,subjectjanett-025).
asCloSET[70]isnotopen-source, weare unabletocom- Fig.3showsourperceptualstudy,where63.4%ofpar-
pareitwithourmethod. ticipants prefer the results produced by our method due to
6
dohteMPOP
POP SkiRT FITE Ours GT
Figure4. Qualitativecomparisonbetweenbaselinesandourmethodformodelinglooseclothing. SubjectIDsfromtoptobottom:
“felice-004”,“janett-025”and“christine-027”.Bestviewedzoomed-inonacolorscreen.
superiorvisualqualityandcloserresemblancetotheGT.In
comparison, FITE [33], SkiRT [40], and POP [39] receive
29.9%, 5.9%, and 0.8% of the votes, respectively. These
findingsarefurthersupportedbyGPT-4o[1],whichshows
56% preference for our method, aligning with the human
study. Our advantage is particularly pronounced for chal-
lenging cases with loose clothing, where over 85% of hu-
man evaluators favor our method for the two most diffi-
cult skirts (a detailed breakdown of the votes is available
inTab.B1). Thishighlightstheeffectivenessofourhybrid
approachinmodelinglooseclothing. Fortighterskirts,our POP SkiRT FITE Ours GT
modelperformsonparwithFITE,whichalsogeneratessat- Figure 5. Visualization results of loose clothing. Our model
isfactoryresults.However,FITEexhibitsan“open-surface” effectively avoids redundant points on the open surface of loose
artifact,whichisnotvisibleinthestudy’sfront-facingren- skirts,alimitationinFITE[33],andgeneratesmoreaccuratege-
derings. Wewilldiscussthislimitationindetaillater. Refer ometrythanPOP[39]andSkiRT[40].
tothesupplementarymaterialsforvisualizationresults.
Moreover, thecoarse-to-finerefinementfailstocapturein-
QualitativeResults.Wepresentthequalitativeresultswith tricate details, often leading to noisy surfaces and loss of
zoomed-in details in Fig. 4. To perform a holistic evalu- sharpstructures(seemeshingresultsinFig.4).Ourmethod
ation of the 3D geometry, we employ Poisson reconstruc- significantly outperforms the baselines in visual quality.
tion[24]toconvertthepoint-basedrepresentationintoatri- Leveraging an LBS-free generation module, our approach
angular mesh. As can be seen, due to the inherent flaw of effectivelyhandlesthecomplex,looseregionsofskirtsand
LBSposing,POP[39]suffersfromthe“split”artifactsfor dresses. This results in natural, high-fidelity details that
skirtsanddresses. Inaddition, thedistributionofpointsis closely resemble the GT, along with smooth and densely
severelynon-uniform,lackingrealisticdetailssuchaswrin- distributed points, demonstrating the representative power
kles. SkiRT[40]mitigatesthisissuetosomeextent,butthe ofourhybridframework.
results remain unsatisfactory. FITE [33] achieves a more To enhance clarity and better highlight the quality of
uniformpointdensityafterresamplingontheclothingtem- looseclothing,wespecificallypresentavisualcomparison
plate. However, it introduces unnatural, overly bent wrin- ofthegeneratedclothinFig.5. FITE[33]deformsalearn-
klesinlongdressesduetothepoorlydefinedLBSprocess. ableimplicittemplaterepresentedinsigneddistancefields,
7(a) Deformation-Only (b) Generation-Only (c) w/o Collision Loss (d) w/o Part Feature (e) Full Model
Figure6. Ablationstudy. (a)showstheLBS-baseddeformedpointcloud,while(b)illustratestheoutcomesachievedbyonlyapplying
free-formgenerationtothelowerbody. Ablation(c)examinestheeffectivenessoftheproposedcollisionloss. Withoutpart-awarepose
featureextraction,(d)showsanimproperskirtorientationthatfailstoalignwiththegivenpose. Ultimately,ourfullmodel(e)showcases
thehighestvisualquality.Pleasezoomintoexaminethedetailsofthegeneratedskirtintheredbox.
man body. Overall, we conclude that combining collision
losswithposeaugmentationyieldsmorerobustresults.
Part-awareGenerator. AsdepictedinFig.6(d),thelack
of part-aware pose feature encoding results in a misalign-
ment between the skirt’s orientation and the driven pose.
This highlights that our structure-aware design facilitates
w/oClothing-cut Map w/Clothing-cut Map thegeneratortolearnposeconditioningmoreaccurately.
Figure 7. Ablation study of utilizing clothing-cut maps. The
clothing-cut map effectively guides the free-form generator to
Clothing-cut Map. To assess the efficacy of the clothing-
modelloosegarmentswithacontinuousanddetailedsurface. In
cutmap,weconductanablationstudywhereweremovethe
contrast,anaiveapproachtohybridmodelingofthedresscauses
segmentationguidancewhengeneration. Asdepictedinthe
ittotearapartandtheunderlyinglegtointersectwiththegarment.
left case of Fig. 7, this results in penetration artifacts, dis-
continuities,andmissingdetailsinthelongdress. Notethe
which struggle with handling the open surfaces of loose
dress splits around the left leg. In contrast, by introducing
skirts. In contrast, our model avoids the surface “sealing”
clothing-cutmaps,thegeneratorcanmodelthewholedress
issuesseeninFITE[33]andeliminatesthe“split-up”arti-
holistically,avoidingconflictswiththedeformationmodule
facts found in POP [39] and SkiRT [40], preserving high-
andimprovingvisualqualitygreatly.
qualitygenerationoflooseclothing.
5.Conclusion
4.2.AblationStudy
Hybrid Paradigm. To validate the efficacy of our hybrid We present a hybrid point-based solution for modeling
paradigm, we implement a simple deformation-only base- challenging clothed humans, which integrates LBS defor-
mationandfree-formgenerationtotackledifferentclothing
line (i.e. CloSET [70] without explicit template decompo-
regions. To synergize the strengths of these two modules,
sition). As shown in Fig. 6 (a), it still suffers from pant-
we propose to segment the body surface into unclothed,
likeartifacts. However, entirelydiscardingtheuseofLBS
deformed, and generated regions, yielding a clothing-cut
poseschallengesinaccuratelymodelingarticulatedhumans
map. Our innovative framework effectively eliminates
as shown in ablation (b), where we generate full points on
the pant-like and inhomogeneous density artifacts in
thelowerpartofthebodyfromaglobalposefeature. The prior methods when modeling skirts and long dresses.
results appear noisy and discontinuous, particularly in ar- The free-form generator provides enhanced topological
ticulated regions such as legs. This motivates us to take flexibility and expressiveness, enabling our model to
a hybrid approach, which integrates the deformer and the generate realistic and high-quality wrinkle details. We
generator modules. Fig. 6 (e) verifies our analysis that the assess our model with varying skirt lengths, tightness,
hybrid method further pushes the upper bound of the ex- and styles, and the experimental results demonstrate
the superior representational power of the proposed
pressivenessofLBS-basedmethodswhilereasoningabout
framework. We believe that this novel hybrid modeling
thearticulatedmotioncorrectly.
opens up new possibilities in this domain. Addition-
ally, our point-based hybrid modeling can be integrated
CollisionLoss. AsdepictedinFig.6(c)and(e),thecolli- with recent advancements in 3DGS [25] to enhance tex-
sionlossimposesconstraintsonthefree-formgeneratorto ture rendering, which we plan to explore in future work.
produceloosecomponentsthatdonotintersectwiththehu-
8References [13] Thibault Groueix, Matthew Fisher, Vladimir G Kim,
BryanCRussell, andMathieuAubry. Apapier-mâchéap-
[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ah-
proachtolearning3dsurfacegeneration. InProceedingsof
mad,IlgeAkkaya,FlorenciaLeoniAleman,DiogoAlmeida,
theIEEEconferenceoncomputervisionandpatternrecog-
JankoAltenschmidt, SamAltman, ShyamalAnadkat, etal.
nition,pages216–224,2018. 3
Gpt-4 technical report. arXiv preprint arXiv:2303.08774,
[14] Peng Guan, Loretta Reiss, David A Hirshberg, Alexander
2023. 6,7,3,4
Weiss, and Michael J Black. Drape: Dressing any person.
[2] IlyaBaranandJovanPopovic´.Automaticriggingandanima- ACMTransactionsonGraphics(ToG),31(4):1–10,2012. 3,
tionof3dcharacters.ACMTransactionsongraphics(TOG), 5
26(3):72–es,2007. 2,3 [15] Erhan Gundogdu, Victor Constantin, Amrollah Seifoddini,
[3] JanBednarik,ShaifaliParashar,ErhanGundogdu,Mathieu MinhDang,MathieuSalzmann,andPascalFua. Garnet: A
Salzmann, and Pascal Fua. Shape reconstruction by learn- two-stream network for fast and accurate 3d cloth draping.
ingdifferentiablesurfacerepresentations. InProceedingsof In Proceedings of the IEEE/CVF International Conference
theIEEE/CVFConferenceonComputerVisionandPattern onComputerVision,pages8739–8748,2019. 3,5
Recognition,pages4716–4725,2020. 3 [16] Tong He, Yuanlu Xu, Shunsuke Saito, Stefano Soatto, and
[4] Bharat Lal Bhatnagar, Garvita Tiwari, Christian Theobalt, TonyTung.Arch++:Animation-readyclothedhumanrecon-
andGerardPons-Moll.Multi-garmentnet:Learningtodress structionrevisited.InProceedingsoftheIEEE/CVFinterna-
3d people from images. In Proceedings of the IEEE/CVF tionalconferenceoncomputervision, pages11046–11056,
international conference on computer vision, pages 5420– 2021. 1
5430,2019. 3 [17] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,
[5] Bharat Lal Bhatnagar, Cristian Sminchisescu, Christian Bernhard Nessler, and Sepp Hochreiter. Gans trained by a
Theobalt,andGerardPons-Moll. Loopreg: Self-supervised twotime-scaleupdateruleconvergetoalocalnashequilib-
learningofimplicitsurfacecorrespondences,poseandshape rium. Advances in neural information processing systems,
for3dhumanmeshregistration. AdvancesinNeuralInfor- 30,2017. 6
mationProcessingSystems,33:12909–12922,2020. 3 [18] Tianxin Huang, Xuemeng Yang, Jiangning Zhang, Jinhao
[6] AndreiBurov,MatthiasNießner,andJustusThies.Dynamic Cui, Hao Zou, Jun Chen, Xiangrui Zhao, and Yong Liu.
surface function networks for clothed human bodies. In Learningtotrainapointcloudreconstructionnetworkwith-
ProceedingsoftheIEEE/CVFInternationalConferenceon outmatching. InEuropeanConferenceonComputerVision,
ComputerVision,pages10754–10764,2021. 3 pages179–194.Springer,2022. 3
[19] XinHuang,RuizhiShao,QiZhang,HongwenZhang,Ying
[7] XuChen, YufengZheng, MichaelJBlack, OtmarHilliges,
Feng, Yebin Liu, and Qing Wang. Humannorm: Learning
and Andreas Geiger. Snarf: Differentiable forward skin-
normaldiffusionmodelforhigh-qualityandrealistic3dhu-
ningforanimatingnon-rigidneuralimplicitshapes. InIn-
man generation. In Proceedings of the IEEE/CVF Confer-
ternational Conference on Computer Vision (ICCV), pages
ence on Computer Vision and Pattern Recognition, pages
11594–11604,2021. 3,4
4568–4577,2024. 3
[8] Boyang Deng, John P Lewis, Timothy Jeruzalski, Gerard
[20] YangyiHuang,HongweiYi,YuliangXiu,TingtingLiao,Ji-
Pons-Moll,GeoffreyHinton,MohammadNorouzi,andAn-
axiangTang,DengCai,andJustusThies.Tech:Text-guided
drea Tagliasacchi. Nasa neural articulated shape approxi-
reconstruction of lifelike clothed humans. arXiv preprint
mation. In Computer Vision–ECCV 2020: 16th European
arXiv:2308.08545,2023. 3
Conference, Glasgow, UK, August 23–28, 2020, Proceed-
[21] HyunJunJung, NikolasBrasch, JifeiSong, EduardoPerez-
ings,PartVII16,pages612–628.Springer,2020. 3
Pellitero, Yiren Zhou, Zhihao Li, Nassir Navab, and Ben-
[9] ZhantaoDeng,JanBednaˇrík,MathieuSalzmann,andPascal
jaminBusam. Deformable3dgaussiansplattingforanimat-
Fua.Betterpatchstitchingforparametricsurfacereconstruc-
ablehumanavatars.arXivpreprintarXiv:2312.15059,2023.
tion. In2020InternationalConferenceon3DVision(3DV),
3
pages593–602.IEEE,2020. 3
[22] Yash Kant, Aliaksandr Siarohin, Riza Alp Guler, Menglei
[10] TheoDeprelle,ThibaultGroueix,MatthewFisher,Vladimir Chai,JianRen,SergeyTulyakov,andIgorGilitschenski. In-
Kim,BryanRussell,andMathieuAubry. Learningelemen- vertible neural skinning. In Proceedings of the IEEE/CVF
tary structures for 3d shape generation and matching. Ad- Conference on Computer Vision and Pattern Recognition,
vancesinNeuralInformationProcessingSystems,32,2019. pages8715–8725,2023. 3
3 [23] Ladislav Kavan, Dan Gerszewski, Adam W Bargteil, and
[11] AndrewFeng,DanCasas,andAriShapiro. Avatarreshap- Peter-PikeSloan.Physics-inspiredupsamplingforclothsim-
ingandautomaticriggingusingadeformablemodel.InPro- ulationingames. InACMSIGGRAPH2011papers,pages
ceedingsofthe8thACMSIGGRAPHConferenceonMotion 1–10.2011. 1
inGames,pages57–64,2015. 2 [24] MichaelKazhdanandHuguesHoppe.Screenedpoissonsur-
[12] Jun Gao, Wenzheng Chen, Tommy Xiang, Alec Jacobson, facereconstruction. ACMTransactionsonGraphics(ToG),
Morgan McGuire, and Sanja Fidler. Learning deformable 32(3):1–13,2013. 7
tetrahedralmeshesfor3dreconstruction. AdvancesInNeu- [25] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler,
ralInformationProcessingSystems,33:9936–9947,2020. 3 and George Drettakis. 3d gaussian splatting for real-time
9radiancefieldrendering.ACMTransactionsonGraphics,42 [39] QianliMa, JinlongYang, SiyuTang, andMichaelJBlack.
(4):1–14,2023. 3,8 The power of points for modeling humans in clothing. In
[26] ByungjunKim,PatrickKwon,KwanghoLee,MyunggiLee, InternationalConferenceonComputerVision(ICCV),pages
Sookwan Han, Daesik Kim, and Hanbyul Joo. Chupa: 10974–10984,2021. 1,2,3,4,5,6,7,8,9,10,11,12
Carving 3d clothed humans from skinned shape priors [40] QianliMa, JinlongYang, MichaelJBlack, andSiyuTang.
using 2d diffusion probabilistic models. arXiv preprint Neuralpoint-basedshapemodelingofhumansinchalleng-
arXiv:2305.11870,2023. 6 ingclothing.In2022InternationalConferenceon3DVision
[27] Diederik P Kingma and Jimmy Ba. Adam: A method for (3DV),pages679–689.IEEE,2022. 2,3,4,5,6,7,8,1
stochastic optimization. arXiv preprint arXiv:1412.6980, [41] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Se-
2014. 1 bastianNowozin,andAndreasGeiger.Occupancynetworks:
[28] AlexanderKirillov,EricMintun,NikhilaRavi,HanziMao, Learning3dreconstructioninfunctionspace.InProceedings
ChloeRolland,LauraGustafson,TeteXiao,SpencerWhite- oftheIEEE/CVFconferenceoncomputervisionandpattern
head,AlexanderCBerg,Wan-YenLo,etal. Segmentany- recognition,pages4460–4470,2019. 3
thing. InProceedingsoftheIEEE/CVFInternationalCon- [42] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,
ferenceonComputerVision,pages4015–4026,2023. 3,1, JonathanTBarron,RaviRamamoorthi,andRenNg. Nerf:
2 Representingscenesasneuralradiancefieldsforviewsyn-
[29] MuhammedKocabas,Jen-HaoRickChang,JamesGabriel, thesis. Communications of the ACM, 65(1):99–106, 2021.
Oncel Tuzel, and Anurag Ranjan. Hugs: Human gaussian 3
splats. arXivpreprintarXiv:2311.17910,2023. 3 [43] Arthur Moreau, Jifei Song, Helisa Dhamo, Richard Shaw,
[30] ZorahLahner,DanielCremers,andTonyTung. Deepwrin- YirenZhou,andEduardoPérez-Pellitero. Humangaussian
kles: Accurateandrealisticclothingmodeling. InProceed- splatting: Real-timerenderingofanimatableavatars. arXiv
ingsoftheEuropeanconferenceoncomputervision(ECCV), preprintarXiv:2311.17113,2023. 3
pages667–684,2018. 3,5 [44] ShanthikaNaik,KunwarSingh,AstitvaSrivastava,Dhawal
[31] Dohae Lee and In-Kwon Lee. Multi-layered unseen gar- Sirikonda, AmitRaj, VarunJampani, andAvinashSharma.
ments draping network. arXiv preprint arXiv:2304.03492, Dress-me-up: A dataset & method for self-supervised 3d
2023. 6 garmentretargeting.arXivpreprintarXiv:2401.03108,2024.
[32] ZheLi,ZerongZheng,LizhenWang,andYebinLiu. Ani- 1
matablegaussians: Learningpose-dependentgaussianmaps [45] AlexandrosNeophytouandAdrianHilton. Alayeredmodel
for high-fidelity human avatar modeling. arXiv preprint ofhumanbodyandgarmentdeformation. In20142ndIn-
arXiv:2311.16096,2023. 3 ternationalConferenceon3DVision,pages171–178.IEEE,
[33] Siyou Lin, Hongwen Zhang, Zerong Zheng, Ruizhi Shao, 2014. 3
andYebinLiu. Learningimplicittemplatesforpoint-based [46] Haokai Pang, Heming Zhu, Adam Kortylewski, Christian
clothedhumanmodeling. InEuropeanConferenceonCom- Theobalt,andMarcHabermann. Ash: Animatablegaussian
puterVision,pages210–228.Springer,2022. 1,2,3,5,6,7, splats for efficient and photoreal human rendering. arXiv
8,4 preprintarXiv:2312.05941,2023. 3
[34] LijuanLiu,YouyiZheng,DiTang,YiYuan,ChangjieFan, [47] Jeong Joon Park, Peter Florence, Julian Straub, Richard
and Kun Zhou. Neuroskinning: Automatic skin binding Newcombe,andStevenLovegrove. Deepsdf:Learningcon-
for production characters with deep graph networks. ACM tinuous signed distance functions for shape representation.
TransactionsonGraphics(ToG),38(4):1–12,2019. 2 InProceedingsoftheIEEE/CVFconferenceoncomputervi-
[35] Minghua Liu, Lu Sheng, Sheng Yang, Jing Shao, and Shi- sionandpatternrecognition,pages165–174,2019. 3,4
Min Hu. Morphing and sampling network for dense point [48] Chaitanya Patel, Zhouyingcheng Liao, and Gerard Pons-
cloudcompletion.InProceedingsoftheAAAIconferenceon Moll. Tailornet: Predicting clothing in 3d as a function of
artificialintelligence,pages11596–11603,2020. 3 human pose, shape and garment style. In Proceedings of
[36] MatthewLoper,NaureenMahmood,JavierRomero,Gerard the IEEE/CVF conference on computer vision and pattern
Pons-Moll, and Michael J Black. Smpl: A skinned multi- recognition,pages7365–7375,2020. 3,5
personlinearmodel. InSeminalGraphicsPapers: Pushing [49] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani,
theBoundaries,Volume2,pages851–866.2023. 3 Timo Bolkart, Ahmed AA Osman, Dimitrios Tzionas, and
[37] Qianli Ma, Jinlong Yang, Anurag Ranjan, Sergi Pujades, Michael J Black. Expressive body capture: 3d hands,
GerardPons-Moll,SiyuTang,andMichaelJBlack. Learn- face, and body from a single image. In Proceedings of
ingtodress3dpeopleingenerativeclothing.InProceedings the IEEE/CVF conference on computer vision and pattern
oftheIEEE/CVFConferenceonComputerVisionandPat- recognition,pages10975–10985,2019. 3,4,5
ternRecognition,pages6469–6478,2020. 3 [50] SidaPeng, YuanqingZhang, YinghaoXu, QianqianWang,
[38] Qianli Ma, Shunsuke Saito, Jinlong Yang, Siyu Tang, and Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body:
Michael J Black. Scale: Modeling clothed humans with a Implicit neural representations with structured latent codes
surfacecodecofarticulatedlocalelements. InProceedings for novel view synthesis of dynamic humans. In Proceed-
oftheIEEE/CVFConferenceonComputerVisionandPat- ingsoftheIEEE/CVFConferenceonComputerVisionand
ternRecognition,pages16082–16093,2021. 2,3 PatternRecognition,pages9054–9063,2021. 3
10[51] Sergey Prokudin, Qianli Ma, Maxime Raafat, Julien [64] Chung-Yi Weng, Brian Curless, Pratul P Srinivasan,
Valentin, and Siyu Tang. Dynamic point fields. arXiv JonathanTBarron,andIraKemelmacher-Shlizerman. Hu-
preprintarXiv:2304.02626,2023. 3,6,2 mannerf: Free-viewpointrenderingofmovingpeoplefrom
[52] CharlesRQi,HaoSu,KaichunMo,andLeonidasJGuibas. monocular video. In Proceedings of the IEEE/CVF con-
Pointnet: Deep learning on point sets for 3d classification ferenceoncomputervisionandpatternRecognition, pages
and segmentation. In Proceedings of the IEEE conference 16210–16220,2022. 3
oncomputervisionandpatternrecognition,pages652–660, [65] TongWu, LiangPan, JunzheZhang, TaiWang, ZiweiLiu,
2017. 5 andDahuaLin. Density-awarechamferdistanceasacom-
[53] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J prehensivemetricforpointcloudcompletion.arXivpreprint
Guibas. Pointnet++: Deephierarchicalfeaturelearningon arXiv:2111.12702,2021. 3
pointsetsinametricspace. Advancesinneuralinformation [66] Chulin Xie, Chuxin Wang, Bo Zhang, Hao Yang, Dong
processingsystems,30,2017. 4,5,1 Chen,andFangWen. Style-basedpointgeneratorwithad-
[54] Shenhan Qian, Jiale Xu, Ziwei Liu, Liqian Ma, and versarialrenderingforpointcloudcompletion. InProceed-
Shenghua Gao. Unif: United neural implicit functions for ingsoftheIEEE/CVFConferenceonComputerVisionand
clothed human reconstruction and animation. In European PatternRecognition,pages4619–4628,2021. 5,1
Conference on Computer Vision, pages 121–137. Springer, [67] YuliangXiu,JinlongYang,XuCao,DimitriosTzionas,and
2022. 3 Michael J Black. Econ: Explicit clothed humans obtained
[55] ShunsukeSaito,ZengHuang,RyotaNatsume,ShigeoMor- fromnormals. arXivpreprintarXiv:2212.07422,2022. 6
ishima,AngjooKanazawa,andHaoLi. Pifu: Pixel-aligned [68] YuliangXiu,JinlongYang,DimitriosTzionas,andMichaelJ
implicitfunctionforhigh-resolutionclothedhumandigitiza- Black. Icon: Implicit clothed humans obtained from nor-
tion. InProceedingsoftheIEEE/CVFinternationalconfer- mals. In 2022 IEEE/CVF Conference on Computer Vi-
enceoncomputervision,pages2304–2314,2019. sionandPatternRecognition(CVPR),pages13286–13296.
[56] Shunsuke Saito, Jinlong Yang, Qianli Ma, and Michael J IEEE,2022. 3,6
Black. Scanimate: Weakly supervised learning of skinned
[69] Ilya Zakharkin, Kirill Mazur, Artur Grigorev, and Victor
clothed avatar networks. In Proceedings of the IEEE/CVF
Lempitsky. Point-based modeling of human clothing. In
Conference on Computer Vision and Pattern Recognition,
ProceedingsoftheIEEE/CVFInternationalConferenceon
pages2886–2897,2021. 3,4,2 ComputerVision,pages14718–14727,2021. 3
[57] Igor Santesteban, Miguel A Otaduy, and Dan Casas.
[70] HongwenZhang, Siyou Lin, Ruizhi Shao, YuxiangZhang,
Learning-basedanimationofclothingforvirtualtry-on. In
Zerong Zheng, Han Huang, Yandong Guo, and Yebin Liu.
Computer Graphics Forum, pages 355–366. Wiley Online
Closet: Modeling clothed humans on continuous surface
Library,2019. 5
with explicit template decomposition. In Proceedings of
[58] IgorSantesteban,NilsThuerey,MiguelAOtaduy,andDan
theIEEE/CVFConferenceonComputerVisionandPattern
Casas. Self-supervised collision handling via generative
Recognition,pages501–511,2023. 2,3,4,5,6,8,1
3d garment models for virtual try-on. In Proceedings of
[71] Shunyuan Zheng, Boyao Zhou, Ruizhi Shao, Boning Liu,
theIEEE/CVFConferenceonComputerVisionandPattern
Shengping Zhang, Liqiang Nie, and Yebin Liu. Gps-
Recognition,pages11763–11773,2021. 6
gaussian: Generalizable pixel-wise 3d gaussian splatting
[59] Yidi Shao, Chen Change Loy, and Bo Dai. Towards
for real-time human novel view synthesis. arXiv preprint
multi-layered 3d garments animation. arXiv preprint
arXiv:2312.02155,2023. 3
arXiv:2305.10418,2023. 6
[72] Qian-Yi Zhou, Jaesik Park, and Vladlen Koltun. Open3d:
[60] TianchangShen,JunGao,KangxueYin,Ming-YuLiu,and
A modern library for 3d data processing. arXiv preprint
Sanja Fidler. Deep marching tetrahedra: a hybrid repre-
arXiv:1801.09847,2018. 1
sentationforhigh-resolution3dshapesynthesis. Advances
in Neural Information Processing Systems, 34:6087–6101,
2021. 3
[61] Shih-Yang Su, Frank Yu, Michael Zollhöfer, and Helge
Rhodin. A-nerf:Articulatedneuralradiancefieldsforlearn-
inghumanshape,appearance,andpose.AdvancesinNeural
InformationProcessingSystems,34:12278–12291,2021. 3
[62] ZhaoqiSu, TaoYu, YangangWang, andYebinLiu. Deep-
cloth: Neural garment representation for shape and style
editing.IEEETransactionsonPatternAnalysisandMachine
Intelligence,45(2):1581–1593,2022. 3
[63] GarvitaTiwari,BharatLalBhatnagar,TonyTung,andGer-
ard Pons-Moll. Sizer: A dataset and model for parsing 3d
clothingandlearningsizesensitive3dclothing.InComputer
Vision–ECCV 2020: 16th European Conference, Glasgow,
UK,August23–28,2020,Proceedings,PartIII16,pages1–
18.Springer,2020. 3
11Free-form Generation Enhances Challenging Clothed Human Modeling
Supplementary Material
InSec.A,weelaborateontheimplementationdetailsof derthefrontandbackviewnormalmapstocoverallbody
ourproposedmethodandtheexperimentalsetups. Wepro- points.ThesenormalmapsarefedintoSAMtolocateloose
videadditionalresultsandextendeddiscussionsinSec.B. clothing including skirts and dresses. The segmented re-
sults are shown in Fig. A1. We back-project the detected
A.ImplementationDetails pixelcoordinatesinto3Dspaceandemploynearestneigh-
bor search to assign each point on the UV map to the full
A.1.ModelArchitecture
scan,filteringthecorrespondingloosepartsonthebodysur-
IntheimplementationofourposeencodernetworkE ,the face.Theextractedclothing-cutmapsforall5subjectsfrom
d
PointNet++ [53] abstracts the point features for L = 4 theReSynth[39]datasetarevisualizedinFig.A2.
levels, and the numbers of the abstracted points are 2048, Toensurefaircomparisons,wemergepointsfromthree
512, 128, and 32 at each level, respectively. Both the lo- sources,i.e.combiningN u,N d,andN gpoints,andemploy
calandglobalposecodesshareafeaturedimensionalityof farthest point sampling (FPS) to obtain the final full point
M = 256, whereas the garment code is represented by a setwithN =47911pointstomatchthebaselines[39,40].
p
M =64-dimensionallearnableparameter.
g A.3.Training
Thestructure-awareposeencoderE forextractingpose
g
featureembeddingforthefree-formgenerationmodulepos- Wetrainournetworkfor1000epochsontheReSynth[39]
sesses a similar architecture with E . Given our focus on dataset,usingtheAdam[27]optimizerwithabatchsizeof
d
modeling skirts and long dresses, we selectively sample 8 and a learning rate of 3.0×10−4. The loss weights are
posedpointsfromK = 4localpartssituatedonthelegs, settoλ =1×104,λ =1.0,λ =2×103,λ =1and
b p n rd rg
includingtheleftupperleg, leftlowerleg, rightupperleg, λ =2×10−2 tobalancelossterms. Followingprevious
col
andrightlowerleg.Specifically,weuniformlysample2048 works [39, 70], we only activate the normal loss from the
pointsfromeachpart,whicharetheninputtedintoE tode- 400th epoch. The training procedure takes about 20 hours
g
rive part-aware local features. A final global max-pooling onasingleRTX3090GPU.
layerisprependedtoextracttheglobalposefeatures. Given limited 3D training data, we enhance the robust-
As for the free-form generator G, We modify a sim- nessofourfree-formgeneratortoout-of-distributionposes
pleyeteffectivestyle-basedpointgenerator,SpareNet[66]. by balancing the pose distribution. Specifically, we ap-
SpareNetemployspointmorphingtechniquestomapaunit plyrandomhorizontalflipsalongthex-axis,leveragingthe
square [0,1]2 onto a 3D surface. Specifically, we utilize symmetryofthehumanbody.
K surfaceelements(8inourexperiments)toconstructthe
A.4.Baselines
loose garment. For simplicity, we omit the refiner mod-
uleandadversarialrendering. Empirically,weobservethat ForPOP[39]andFITE[33],wedirectlyutilizetheofficial
refinement following the hybrid modeling of the garment model weight provided for inference. As for SkiRT [40],
doesn’t yield performance improvements. The number of wetrainthemodelusingtheofficialcodeandsuccessfully
generatedpoints,denotedasN g,ismanuallyconfiguredto reproducetheresultsreportedintheoriginalpaper. Weper-
either32768forlongdressesor16384forskirts. forminferenceusingthetrainedmodelweight.
A.2.Garment-specificClothing-cutMap A.5.DetailsonPerceptualStudy
Here we provide comprehensive details on computing the We follow the official rendering scripts including camera
garment-specificclothing-cutmaps,asoutlinedinthemain and lighting configurations for baseline methods [33, 39,
paper. Following the methodology in POP [39], all base- 40]andours,wheretheoutputpointcloudisrenderedusing
line approaches [33, 40, 70] uniformly sample point sets asurfel-basedrendererinOpen3D[72]withapointsizeof
fromtheUVmapataresolutionof256×256. Specifically, 5.Toassessthegeometricvisualquality,werenderthefront
N = 47911 points are sampled. We start by segmenting andthebackviewsatahighresolutionof1024×1024.The
d
the unclothed regions, including the head, hands, and feet, deployedbaselinemodelsarediscussedaboveinSec.A.4.
whichcontainuptoN =13240points. 50 participants are presented with a set of 25 examples
u
Then we apply the off-the-shelf image segmentation consisting of different subjects and poses, randomly sam-
model, SAM [28], to automatically identify the loose re- pledfromtheReSynth[39]datasetresultswithoutcherry-
gion. Specifically, we select the frame that closely resem- picking. Ineachexample,theGTreferenceisalwaysputin
bles the canonical pose in the training sequence and ren- theleftmostcolumn,andwerandomizetheorderingofthe
1Front
View
Back
View
beatrice-025 anna-001 janett-025 christine-027 felice-004
FigureA1. ThesegmentedlooseregionsofeachclothintheReSynth[39]dataset. Weidentifythelooseregionsinthefrontandback
viewnormalmapsutilizingthesegmentationmodelSAM[28].
beatrice-025 anna-001 janett-025 christine-027 felice-004
FigureA2.Theclothing-cutmapsforfivesubjectsintheReSynth[39]dataset.Thefirstrowdepictstheclothing-cutmapsdistinguished
bythreedifferentcolors,whilethesecondrowillustratesthecorrespondingsegmentedregions. Specifically,theyellowcolordenotesthe
maskedregion,theblueindicatesthebodypartsrequiringdeformation,andthegreenmarkstheloosepartstobemodeledutilizingfree-
formgeneration.Finally,thelastrowdisplaysthecompletepredictionsgeneratedbyourmodel.
resultsofdifferentmethodsontheright. Fig.A3showsan giventothefirstpoint,whichistheoverallvisualquality.
example.Foreachexample,theparticipantsareaskedtose-
lectthemostpreferredsingleoptionbasedonthefollowing B.ExtendedResultsandDiscussions
two criteria: (1) realism, wrinkle details, smoothness, uni-
B.1.DiscussionsontheEvaluationMetric
formity,andthepresenceofartifactscontributetotheover-
allvisualqualityoftheclothingshape;(2)thesimilarityto AspointedoutbyDPF[51]andFITE[33], weemphasize
the reference GT result. Due to the inherent randomness that conventional metrics used in the previous works [39,
in the generated results, an exact match with the reference 40,56],Chamferdistance(CD)andL normaldiscrepancy
1
effect may not be necessary. Therefore, priority should be (NML)implicitlyassumeaone-to-onemappingfrombody
2FigureB4.Anexampleillustratingthestochasticityofclothing
shapewithtwosimilargivenposes.
FigureA3. Exampleofperceptualstudyimage. Werandomize
theorderingoftheresultsofdifferentmethodsperexample. We CD: 6.57 CD: 14.06
alwaysputtheGTresultintheleftmostcolumn.
pose to the clothing shape. However, in reality, the cloth-
ingshapepossessesdiversityandrandomnesswhichcanbe
influencedbymanyotherfactorssuchasthemotionspeed
andthehistory[51]. Consequently,givenasimilarorsame
pose,multipleclothingstatusescanbereasonable,asillus-
trated in Fig. B4. Our model generates plausibly-looking
results that, may not conform strictly to the ground truth, Ours (4k) Ours Ground Truth
henceobtaininghighCDerrors.
FigureB5. IllustrationoftheparadoxoflowerCDerrorwith
TofurtherhighlightthelimitationsoftheCDmetric,we
worsevisualquality. Reducingthenumber ofgeneratedpoints
examineacaseinvolvinggeneratedpointsforalongdress.
signficantlydecreasestheCDerror,yetresultsinvisuallyunsatis-
Whenreducingthenumberofpointsgeneratedbythefree-
factoryoutcomeswithnon-uniformpointdensity.
formgeneratorN from32768to4096, theCDerrorsub-
g
stantially decreases (from 14.06 to 6.57, a 53.3% reduc-
tion), asshowninFig.B5. However, thisreductioncomes
oftheclothingshape;(2)similaritytothereferenceground
at the cost of point density uniformity and detail, such as
truth(GT)result. Priorityshouldbegiventothefirstcrite-
wrinkles. This observation has been proven in previous
rion,whichemphasizestheoverallvisualquality."Theper-
worksthattheCDmetriclacksawarenessofthepointden-
subjectpreferenceratesofhumanusersandGPT-4oarepre-
sity distribution [18, 35, 65]. This phenomenon also helps
sentedinTab.B1,denotedasPR-HandPR-G,respectively.
explainwhymethodslikePOP[39]andSkiRT[40]achieve
As shown, the results are generally consistent between the
lower CD errors despite significantly lower point densities
two,withourmethoddemonstratingsignificantadvantages
onlooseskirtsanddresses.
in handling challenging cases. For the two most difficult
Therefore,buildingonthelimitationsdiscussed,weem- skirts, thepreferenceratesfromGPT-4oreach80%, while
ploy a generation-based metric, FID, which compares dis- human users show a preference rate exceeding 85%, con-
tributions and relaxes the strict one-to-one mapping con- firming the effectiveness of our hybrid design in modeling
straint,alongsidethereconstruction-basedMSElosstoas- looseclothing.Fortighterskirts,ourmodelperformsonpar
sess our model’s quality holistically. This approach bet- with FITE [33] and SkiRT [40], both of which rely purely
teralignswiththemodel’sobjectivethantraditionalrecon- onLBSbutstillgeneratepromisingresults.
structionmetrics.
Forreference,wealsofollowpreviousworks[33,39,40,
70]toevaluatetheChamferDistance(CD)andtheL nor-
B.2.QuantitativeResults 1
maldiscrepancy(NML),asspecifiedbytheformulasinthe
In addition to the user study, we also employ the GPT-4o main paper. The default units for reporting CD and NML
model [1] to select the best result across all methods. The are ×10−4m2 and ×10−1, respectively. Tab. B2 presents
testing prompt is as follows: "Select the most preferred thequantitativeerrorsontheReSynth[39]dataset.Notably,
option based on the following two criteria: (1) realism, while FITE [33] exhibits the highest visual quality among
wrinkle details, smoothness, uniformity, and the presence the baselines, it also results in significantly larger quanti-
of artifacts, which contribute to the overall visual quality tative errors. This further verifies that the CD metric may
3TableB1.PerceptualstudyresultsontheReSynth[39]datasetforeachsubject.Wereportthepreferencerates(PR-H)obtainedfrom
aperceptualstudyinvolving50participants,alongsidethepreferencerates(PR-G)votedbytheGPT-4o[1]model. Thefinalscoresare
generallyconsistentwiththoseofthehumanparticipants.Thebestresultsarehighlightedinbold,andthesecondbestareunderlined.The
subjectIDsarelistedindescendingorderbasedontheloosenessoftheclothing.
SubjectID felice-004 janett-025 christine-027 anna-001 beatrice-025 Average
Method PR-H↑ PR-G↑ PR-H↑ PR-G↑ PR-H↑ PR-G↑ PR-H↑ PR-G↑ PR-H↑ PR-G↑ PR-H↑ PR-G↑
POP[39] 0.4% 0.0% 0.8% 0.0% 0.8% 0.0% 1.2% 0.0% 0.8% 0.0% 0.8% 0.0%
SkiRT[40] 3.2% 0.0% 0.0% 0.0% 3.2% 0.0% 12.8% 20.0% 10.4% 60.0% 5.9% 16.0%
FITE[33] 28.0% 40.0% 6.0% 20.0% 10.8% 20.0% 54.0% 40.0% 50.8% 20.0% 29.9% 28.0%
Ours 68.4% 60.0% 93.2% 80.0% 85.2% 80.0% 32.0% 40.0% 38.0% 20.0% 63.4% 56.0%
TableB2.AdditionalquantitativecomparisonofdifferentmethodsontheReSynth[39]datasetforeachsubject.
SubjectID felice-004 christine-027 janett-025 anna-001 beatrice-025
Method CD↓ NML↓ CD↓ NML↓ CD↓ NML↓ CD↓ NML↓ CD↓ NML↓
POP[39] 7.34 1.24 1.72 0.97 1.24 0.89 0.62 0.82 0.34 0.75
SkiRT[40] 6.45 1.25 1.54 0.99 1.10 0.82 0.58 0.81 0.31 0.77
FITE[33] 11.27 2.38 2.16 1.15 1.52 1.05 0.74 0.91 0.46 0.85
Ours 10.61 1.78 2.18 1.01 1.59 0.94 0.81 0.84 0.48 0.74
POP SkiRT FITE Ours GT
FigureB6. Qualitativecomparisonbetweenbaselinesandourmodelformodelinglooseclothing,withhighlighteddetails. Subject
IDsfromtoptobottom:“felice-004”and“janett-025”.Bestviewedzoomed-inonacolorscreen.
notaccuratelyreflectperformance,asdiscussedinSec.B.1. fivesubjectsfromtheReSynth[39]dataset,asillustratedin
Ourmodelshows comparableperformancetoFITEin CD Figs. B14 to B18. We recommend zooming in to observe
errorswhilesignificantlyreducingthenormaldiscrepancy, finerdetails,particularlythewrinklesinskirtsanddresses.
whichcorroboratesourobservationthatFITEgeneratesun- Pleaserefertothevisualizationdemoinoursupplementary
natural and excessively bent wrinkles, whereas our model materials,whichincludessequencesoftestingdatatobetter
effectivelycapturescomplexlocaldetails. demonstratethehigh-qualityperformanceofourmethod.
Asdiscussedinthemainpaper,theadvantagesofourhy-
B.3.MoreQualitativeResults
brid modeling approach become particularly evident with
Inthissection,wepresentadditionalvisualizationcompar- loose skirts or dresses, as illustrated by the examples in
isons that extend the results discussed in the main paper. Figs. B14 and B15. For tighter skirts, LBS-based models
Fig. B6 illustrates the details of the generated loose cloth- like FITE [33] already perform well since the clothing ad-
ing, which are highlighted within the red box. Further- herescloselytothebody. AsshowninFigs.B17andB18,
more, we showcase three testing examples for each of the FITE [33] generates nearly perfect outputs that closely re-
4B.4.Multi-SubjectExperiments
In this study, we explore the potential of hybrid model-
ingforlooseclothingandsignificantlyimprovetheperfor-
manceunderasingle-subjectsetting,comparedwithprevi-
ousworks[33,40,70]. Nevertheless,ourhybridparadigm
can also be naturally extended to modeling multiple gar-
ments,conditionedonvariousglobalgarmentcodes.
Surprisingly, our unified, multi-subject model demon-
stratespromisingperformanceinmodelingvarioustypesof
skirtsandlongdresses,confirmingtheexpressivepowerof
ourfree-formgenerator. Toexplorethelearnedlatentspace
ofgarmentcodes,weperforminterpolationexperimentsfo-
cusing on two crucial attributes: length and tightness. As
shown in Fig. B8, our model allows effective control over
garmentlengththroughmanipulationofthegarmentcode.
POP SkiRT FITE Ours GT Furthermore,whenvaryingthelength,thegeneratedskirts
Figure B7. Visualization results of loose clothing. As shown, smoothly transition from tight to loose. In summary, our
whileFITE[33]successfullycapturesintricatedetailslikewrin- model successfully disentangles pose-related effects from
kles in tighter skirts, it still faces the “open-surface” challenge. garment-specific features, providing controllable and real-
In contrast, our model generates more accurate geometry and isticgenerationresults.
achievessuperiorvisualquality.
B.5.FittingNon-skirtClothing
Although the main focus of this paper is to investigate the
hybridmodelingofloosegarmentssuchasskirtsandlong
dresses, we also conduct experiments to handle non-skirt
clothing,e.g. suits. Notethattheglobalposefeatureisex-
tracted from the PointNet++ [53] without part-aware local
feature learning. Visualization results (Fig. B9) illustrate
the generator’s capacity to autonomously learn and repre-
Skirt Long Dress
sentloosecomponents, suchascollars. Thisdemonstrates
theflexibilityandthepromisingexpressivenessofthepro-
posedfree-formgenerator.
Tight Skirt Loose Skirt
FigureB8. Interpolationresultswhenvaryingthelengthand
thetightnessoftheskirt.
semble the ground truth, and our model produces results FigureB9. Ourhybridmodelcanalsohandlenon-skirtcloth-
comparabletothoseofFITE[33]. However,itisnotewor- ingsuchassuits. Asshownontheleft-handside,thefree-form
thy that FITE [33] still fails to fully eliminate redundant generationmodulecanmodelthelooseregionssuchascollars.
points on the open surface of tighter skirts (see Fig. B7).
Thisrepresentsalimitationcomparedtoourmethod.
B.6.MoreAblationStudies
Above all, our approach stands out in its ability to op-
erate without subject-specific templates coupled with LBS NumberofPatches. Weconductablationstudiestoma-
fields, allowing for more flexible, multi-subject modeling. nipulatethenumberofpatchesK utilizedinthefree-form
This opens up new possibilities for avatar modeling while generator. This experiment also serves to illustrate the in-
maintaininghighperformance. herent complexity involved in modeling loose garments.
5Front
View
(a) K=2 (b) K=4 (c) K=8
Back
View
(d) K=16 (e) K=32 (f) K=64
FigureB10.AblationstudiesofthenumberofpatchesKused
inthefree-formgenerator. Thevisualizationsrevealthatopting
forK =2leadstosmoothedresultswithoutdetails. Empirically, felice christine janett
selectingK = 8yieldsthebestvisualoutcomes. Inothercases, Figure B11. Visualization of the generated K = 8 patches
thesurfacebecomesprogressivelynoisier,compromisingtheclar- which comprise the loose dress and skirts, marked in differ-
ityoffine-grainedelementslikeclothingwrinkles. entcolors.
As a case study, we select the long dress, which features
intricate details such as wrinkles. The results, shown in
Fig. B10, are compared across different patch sizes: K =
2,4,8,16,32,64.
Thevisualizationresult(a)showstheinadequacyofset-
ting K = 2 for capturing the intricate details of the long
dress, resulting in over-smoothed outcomes. In addition,
empirical findings indicate that the setting K = 8 is suffi-
cienttogeneratehigh-qualitydetails,producingsuperiorvi-
sualresults.WhenKsurpasses8orissetto4,thegenerated
surface exhibits increased noise, leading to a loss of clar-
ity in fine-grained details. Notably, features such as cloth-
ingwrinklesbecomelessdistinctandsharplydefined. This
observationsuggeststhatmodelingtheostensiblycomplex
long dress may be less daunting than anticipated. Further-
more,itverifiestheremarkableexpressivenessofourhybrid
framework. Unlessotherwisestated,K = 8isselectedfor
ourexperiments.
Tobetterinvestigatethepropertiesofthefree-formgen-
w/oClothing-cut Map w/Clothing-cut Map
erator,wevisualizeK =8patchesoftwolooseskirtsanda
FigureB12.Moreablationstudyresultsofemployingclothing-
longdress,eachrenderedindifferentcolors. Asillustrated
cut map. Split-up artifacts between two modules can manifest
in Fig. B11, our free-form generator successfully recovers
undervariousposes,especiallywhenthelegapproachesthedress
authenticfine-graineddetailswhilepreservinggoodlocality surface,resultingindisjointed,noisyareasandtornappearances.
withineachpatch. Generally, thepointswithineachpatch Theuseoftheclothing-cutmapnotablymitigatesthisissue.
are arranged in a vertical direction, and different patches
seamlesslyintegratetoformacompletesurface. Thesere-
sults, derived from data-driven learning, suggest that de- Clothing-cut Map. As discussed in the main paper, di-
composing a loose garment into several “vertical" patches rectly employing the generation module to fill in the loose
isaplausibleapproachfordetailedmodeling. Additionally, regions in the previous LBS-based framework can cause
wevisualizetheconvergenceprocessofthefree-formgen- split-upartifacts. Herewepresentadditionalcasesdemon-
eratorthroughoutthetrainingphaseinthedemo. stratingthatthisissueoccursundervariousposes,asillus-
6(a) penetration artifact (b) seams artifact
FigureB13.Twotypicalfailuremodes.(a)Inchallengingposes,
thegeneratedskirtordressoccasionallycollideswiththehuman
body. (b)Attheboundariesbetweendeformedandgeneratedre-
gions,ourmodelsometimesproducesdiscontinuous"seams".
tratedinFig.B12. Thisissuebecomesparticularlyevident
whentheunderlyinglegapproachesthesurfaceofthedress.
Insuchinstances,deformedpointsfromthelegandthegen-
erated region become disjoint, causing the dress to appear
torn. Additionally,apartialshapeoftheunderlyinglegcan
beobservedthroughthecracksinthebrokendress.
B.7.LimitationsandFailureCases
Currently,ourmethodfocusesonsingle-framemodelingof
clothed humans and does not consider the temporal cues
thatcouldprovideconstraintsforclothingdeformationdue
to motion. Consequently, discontinuities may appear in
transitionsbetweenframes. Futureworkcould explorein-
corporating temporal information to achieve smoother and
morerealisticmodelingresults.
Despite employing pose augmentation to mitigate the
limitationsposedbyscarcetrainingdata,ourmodelremains
susceptibletofailurewhenconfrontedwithextremelychal-
lengingposes,resultinginclothingpenetrationartifacts,as
depicted in Fig. B13 (a). This issue is particularly notice-
ablewhentheskirtbecomestighter. Trainingourfree-form
generator on a larger dataset could enhance its robustness
to out-of-distribution poses and reduce such artifacts. Ad-
ditionally,whileourpipelineemploysdifferentstrategiesto
handledeformedandgeneratedareas,wecannotguarantee
theperfectblendingofpointcloudsfromtwomodules. As
illustratedinFig.B13(b),“seams”attheboundaryregions
areoccasionallyobserved.
However, it is crucial to note that our experiments un-
derscorethepromiseandversatilityofourproposedhybrid
approach. Bytranscendingthelimitationsimposedbyrely-
ingsolelyonLBS-baseddeformation, ourmethoddemon-
stratesnotableexpressivecapabilities. Webelievethatwith
largerdatasets,ourapproachhasconsiderablepotentialfor
superiorperformanceinfutureapplications.
7POP SkiRT FITE Ours GT
FigureB14.AdditionalQualitativecomparisonbetweenbaselinesandourmodel.ThesubjectIDis“felice-004”fromtheReSynth[39]
dataset.Bestviewedzoomed-inonacolorscreen.
8POP SkiRT FITE Ours GT
FigureB15.AdditionalQualitativecomparisonbetweenbaselinesandourmodel.ThesubjectIDis“janett-025”fromtheReSynth[39]
dataset.Bestviewedzoomed-inonacolorscreen.
9POP SkiRT FITE Ours GT
Figure B16. Additional Qualitative comparison between baselines and our model. The subject ID is “christine-027” from the
ReSynth[39]dataset.Bestviewedzoomed-inonacolorscreen.
10POP SkiRT FITE Ours GT
FigureB17.AdditionalQualitativecomparisonbetweenbaselinesandourmodel.ThesubjectIDis“anna-001”fromtheReSynth[39]
dataset.Bestviewedzoomed-inonacolorscreen.
11POP SkiRT FITE Ours GT
Figure B18. Additional Qualitative comparison between baselines and our model. The subject ID is “beatrice-025” from the
ReSynth[39]dataset.Bestviewedzoomed-inonacolorscreen.
12