2024-12-2
Perception Test 2024: Challenge Summary and
a Novel Hour-Long VideoQA Benchmark
1 1 1,2 1,3 1
JosephHeyward ,JoãoCarreira ,DimaDamen ,AndrewZisserman andVioricaPătrăucean
1GoogleDeepMind,2UniversityofBristol,3UniversityofOxford
Followingthesuccessful2023edition,weorganisedtheSecondPerceptionTestchallengeasahalf-day
workshop alongside the IEEE/CVF European Conference on Computer Vision (ECCV) 2024, with the
goalofbenchmarkingstate-of-the-artvideomodelsandmeasuringtheprogresssincelastyearusingthe
PerceptionTestbenchmark. Thisyear,thechallengehadseventracks(upfromsixlastyear)andcovered
low-levelandhigh-leveltasks,withlanguageandnon-languageinterfaces,acrossvideo,audio,andtext
modalities;theadditionaltrackcoveredhour-longvideounderstandingandintroducedanovelvideoQA
benchmark1h-walkVQA.Overall,thetasksinthedifferenttrackswere: objecttracking,pointtracking,
temporalactionlocalisation,temporalsoundlocalisation,multiple-choicevideoquestion-answering,
groundedvideoquestion-answering,andhour-longvideoquestion-answering. Wesummariseinthis
reportthechallengetasksandresults,andintroduceindetailthenovelhour-longvideoQAbenchmark
1h-walkVQA.
Keywords: perception, evaluation
1. Introduction
Multimodal video models have witnessed a
tremendous boost in performance these past
couple of years, with both proprietary and
open-sourced models pushing the bound-
aries of machine perception capabilities, e.g.,
Flamingo(Alayracetal.,2022),SeViLA(Yuetal., Figure 1 | Top-1 accuracy of recent VLMs vs hu-
2023), GPT-4V (OpenAI, 2023), Gemini (Team, man baseline on the Perception Test multiple-
2024a), Reka (Team, 2024c), Llama 3-V (Team, choicevideoQAtask. Weincludetheresultspub-
2024b). In 2023, we introduced the Perception lished by models’ authors where available, other-
Test benchmark (Pătrăucean et al., 2023) to wise we ran the models independently (GPT-4V,
comprehensively measure the performance of SeViLA, Flamingo).
video models on different perception tasks and
across modalities. It can be observed that the
performanceofvideo-languagemodelsissteadily featuring 7 challenge tracks (compared to six
increasing over time on the video-language tracks at the first edition).
tracks in our benchmark, but there is still a
significantgapcomparedtohumanperformance; Benchmark: The Perception Test (Pătrăucean
et al., 2023) is a comprehensive benchmark that
see Figure 1. Additionally, other tasks such as
uses purposefully-designed real-world videos to
tracking and temporal segmentation still require
diagnose perception capabilities like memory, un-
specialised models with handcrafted pipelines.
derstanding of intuitive physics and geometry,
To keep track of progress over time, we set up
abstract patterns, and semantics. The bench-
a yearly public challenge using our benchmark
mark consists of 11.6k videos, with audio, up
and we invite participants to submit their best
to 35s long, filmed by diverse crowd-sourced
model’s predictions. This year, we organised the
participants following scripts designed to show
second edition as a workshop at ECCV 2024,
perceptually-interesting situations. The focus is
Correspondingauthor(s):viorica@google.com
© 2024GoogleDeepMind.Allrightsreserved
4202
voN
92
]VC.sc[
1v14991.1142:viXraPerceptionTest2024:ChallengeSummaryandaNovelHour-LongVideoQABenchmark
on probing generalisation and transfer capabili-
ties, so the benchmark only provides a relatively
small training set to be used for fine-tuning or
prompting, and the rest is used for evaluation.
Thevideoshavesixtypesofannotationsenabling
languageand non-language evaluations, across
video, audio, and text modalities. More details
about the Perception Test and data samples are
available on our github repository1 and on the
workshop website2.
Additional benchmark: In addition, this year, to
assess models’ capability of reasoning over very Figure 2 | Average video length in our newly-
longtemporalcontext,weintroduce1h-walkVQA proposed 1h-walk VQA benchmark compared to
– a novel small-scale benchmark based on the existing benchmarks.
Walking Tours dataset (Venkataramanan et al.,
2024); see details in Section 2.
ited.
Challenge tracks: The videos in the Percep-
The test set was made available 2.5 months
tion Test benchmark are annotated with the fol-
beforethesubmissiondeadline. Forthetestphase,
lowing human-collected labels: object tracks,
the limit was set to 2 submissions per day, 30
point tracks, action segments, sound segments,
submissionsintotal. Onlytheresultsmadepublic
multiple-choice video question-answers, and
on the test leaderboard were considered for the
grounded video question-answers; the additional
competition.
dataset included this year was annotated with
multiple-choicevideoquestion-answers. Foreach
type of annotation, we define a corresponding 2. 1h-walk VQA: A Novel Hour-Long
challenge track. We describe in the next sections VideoQA Benchmark
the setup, metrics, and results in each track.
WerelyontheWalkingToursdataset(Venkatara-
Challenge setup: We relied on the open-source
manan et al., 2024) to create a small-scale but
eval.ai platform to set up the different challenge
verychallengingbenchmarktoassessmodels’abil-
tracks. Each track had 2 phases (validation and
ity to understand and reason over very long tem-
test), each phase using the corresponding valida-
poral contexts (hour-long). The Walking Tours
tion and test splits of the Perception Test bench-
dataset contains ten 1-hour (or longer) Youtube
markandthenewlyaddeddataset. Foreachsub-
videos with natural audio (no narrations3), that
mission, the participants had to indicate the eval-
depict city tours filmed by people while walking
uation mode (fine-tuning, few-shot, or zero-shot
around different cities. Figure 2 shows a compar-
evaluation). In some tracks, the participants had
ison in terms of video length between the pro-
to indicate if the model used the audio modality
posed benchmark and existing datasets. We aug-
as well or not (for action and sound localisation,
ment this dataset with 70 manually-curated chal-
multiple-choice video QA). For test submissions,
lenging5-way question-answer pairsthatrequire
the participants were required to also upload a
reasoningovervideoand/oraudiomodalities. We
short report describing their method (architec-
name 1h-walk VQA the resulting benchmark.
ture, pre-training datasets and tasks, etc.). The
validation phase served as a sanity check for par- Collectingchallengingquestionsthatspanlong
ticipants’ submission pipelines. The number of temporal contexts is very difficult, even for hu-
submissions for the validation phase was not lim- mans. Often, the questions in existing bench-
marks can be answered from a single frame or a
1https://github.com/google-deepmind/
perception_test 3It is important that these videos are not narrated to
2https://ptchallenge-workshop.github.io/ ensurenoshortcutthroughlanguagecanbeused.
2PerceptionTest2024:ChallengeSummaryandaNovelHour-LongVideoQABenchmark
Figure3 | Exampleofacountingquestionin1h-walkVQAthatspansmorethan30minutes. Weshow
the relevant frames and their associated timestamps. The correct answer is marked in bold.
short clip (Papalampidi et al., 2023). To ensure Split # videos # questions
that our questions require long context, we ran Train - -
several iterations of annotation collection with Validation 3 11
human raters. In a first iteration, each rater was Test 7 59
tasked to watch an hour-long video and propose
different types of questions: 2 questions that re- Table 1 | Splits in 1h-walk VQA benchmark used
for the hour-long video QA task.
quire one video segment to be answered, 2 ques-
tions that require 2 temporally-separated video
segmentstobeanswered,1questionthatrequires more than 30 minutes. More visualisations can
morethan2videosegmentstobeanswered,and be found on the challenge website4.
1questionthatrequiresvideoandaudiotobean-
Thissmallbenchmarkisintendedforzero-shot
swered. Our team manually reviewed all the pro-
evaluation. We do not provide any training or
videdquestionsandselectedthosethatcannotbe
fine-tuning data. We only provide a very small
answeredfromasingleframeoraveryshortclip.
validationsplittobeusedforsanitychecksinthe
We then ran a second iteration of annotations,
public challenge; see Table 1.
moretargetedtoparticularevents,wherewefirst
ran a detection step to localise in time particular
(repeated) events and then we designed ques-
3. Overall Results Summary
tions based on those timestamps. For example,
we asked raters to mark all the video segments We received 680 submissions from 123 teams
where the person wearing the camera crosses a acrossallseventracksinbothphases,upfrom475
bridge, or walks up some stairs; or when a tower submissionsfrom63teamsin2023. Weawarded
clock is visible in the video, or a distinct sound 2 prizes per track (best and runner-up) to sub-
can be heard. We include in the appendix the missionsthatobtainedthebest(andsecondbest)
list of unique questions selected for our final 1h- results in the test leaderboard, with prizes to-
walk VQA benchmark and we provide in Figure 3
an example of a counting question that spans
4https://eval.ai/web/challenges/
challenge-page/2330/overview
3PerceptionTest2024:ChallengeSummaryandaNovelHour-LongVideoQABenchmark
Rank Team name IoU
Baseline Dummy static 0.640
Runner-up FAUgeddaboudit 0.813
Best NJUST-THU 0.734
Table 2 | Object tracking results
4.1. Object tracking
Figure 4 | Per-track performance improvement Task description: For this task, the model re-
comparedtobaselinesandcomparedtobestmod- ceives a video and a bounding box representing
els from 2023, respectively. an object, and it is required to track the object
throughout the video sequence.
Metric: Theevaluationmetricforthistaskisaver-
ageIntersectionoverUnion(IoU).Itiscalculated
as the average intersection over union between
the predicted bounding boxes and the ground
truth bounding boxes for each tracked object.
Dataset: As in the 2023 edition, to make the
evaluation task more accessible, we used only a
randomly selected subset of 1000 videos from
the validation split of the Perception Test for the
validation phase, and 1000 videos from the test
split of the Perception Test for the test phase. We
Figure 5 | Per-task performance improvement kept the same selection of videos as in the 2023
of top models during the 2024 test submission edition.
phase.
Baselines: We provide a simple dummy baseline
forthistask,whichalwaysassumesthattheobject
is static, i.e. it outputs as predictions the initial
talling20kEUR(upfrom15kEURin2023). The bounding box received as input.
topperformingmodelsimprovedwhencompared
Results: Theresultsforthetop-2competingmod-
tothewinningmodelsfromlastyearinalltracks.
els are compared to the baseline in Table 2. The
Figure4. Figure5showstheevolutionofthetop-
top performing model relies on the recent LO-
performing models during the test submission
RAT(Linetal.,2024)andshowsagoodimprove-
phase of this year’s edition for each track. The
ment over the best submission from last year on
reports of the winning submissions are available
both moving objects and moving camera cate-
on the workshop website.
gories in our dataset; see Figure 6 and check the
authors’ report on our workshop page for more
details.
4. Challenge Tracks, Results, Awards
4.2. Point tracking
In the following we describe each track and the
performance achieved in the challenge. For the Task description: In the single point tracking
technical report per team, including winners’ af- task, the model receives a video and the 2D co-
filiationsandnames,pleaserefertotheworkshop ordinates of a point, and it is required to track
website: https://ptchallenge-workshop. the point throughout the video sequence, also
github.io/. accounting for occlusions.
4PerceptionTest2024:ChallengeSummaryandaNovelHour-LongVideoQABenchmark
Rank Team name Jaccard
Baseline Dummy static 0.418
Runner-up NJUST_kmg 0.472
Best SV (v0.6) 0.474
Table 4 | Point tracking results
Figure 6 | Baseline vs best results 2023 vs best
results 2024 split by camera and object motion
for the object tracking task.
Split # videos # point tracks
Train 28 1758
Validation 73 4362
Figure 7 | Baseline vs best results 2023 vs best
Test 44 2527
results2024splitbycameraandpointmotionfor
Table3 | Datasetusedforthepointtrackingtask. the point tracking task.
taskusingadummystaticbaseline,whichalways
Metric: The evaluation metric for this challenge
assumes that the point is static and visible in all
is the average Jaccard, proposed in TAP-Vid (Do-
frames.
ersch et al., 2022). It takes into account the Oc-
clusion Accuracy – a simple classification accu- Results: Table 4 shows the results of the top-2
racy for the point occlusion prediction on each competingmodelscomparedtoourstaticdummy
frame, and the Position accuracy – for frames baseline. The best results were obtained by SV
wherethepointisvisible,itmeasuresthefraction (v0.6) using the LocoTrack model (Cho et al.,
of points that are within a certain threshold of 2024)thatperformstrackingofallpointssimulta-
their ground truth; it assumes that the images neously, leveraging bidirectional correspondence
are resized to 256x256 pixels and the accuracy and matching smoothness constraints – these
is averaged across 5 thresholds: 1, 2, 4, 8, and bring significant improvement especially for the
16 pixels. The final Jaccard metric calculates the casewherethecameraisstaticandthepointsare
fraction of true positives, which are points within moving; see Figure 7. Please check the workshop
the threshold of any visible ground truth points, website for more details on the method included
dividedbytruepositivesplusfalsepositives(points in the submission report.
that are predicted as visible but the ground truth
is either occluded or farther than the threshold)
plus false negatives (ground truth visible points 4.3. Temporal action localisation
that are predicted as occluded or the prediction
Task description: In the temporal action local-
is farther than the threshold). The overall metric
isation task, the model receives a video and is
is Jaccard averaged across all thresholds.
requiredtolocaliseandclassifytheactionsoccur-
Dataset: We use the same dataset as in 2023 for ring in the video according to a predefined set of
this task, specifically the subset of videos from classes; there are 63 action classes in total.
the Perception Test that have point tracking an-
Metric: Theevaluationmetricforthischallengeis
notations; see details in Table 3.
meanaverageprecision(mAP).Itiscalculatedas
Baselines: We provide baseline results for this theaverageprecisionoverdifferentactionclasses
5PerceptionTest2024:ChallengeSummaryandaNovelHour-LongVideoQABenchmark
Rank Team name mAP Rank Team name mAP
Baseline ActionFormer 0.156 Baseline ActionFormer 0.102
Runner-up AITC (test_wbf_mamba) 0.518 Runner-up JNU-Boat 0.461
Best NJUST–_KMG 0.550 Best NJUST_KMG0 0.493
Table 5 | Temporal action localisation results Table 6 | Temporal sound localisation results.
andIoUthresholds. FortheIoUthresholdsineval- Metric: Similar to the action localisation task
above, the metric for this challenge is mean aver-
uation we use [0.1 → 0.5] with 0.1 increments,
age precision (mAP). It is calculated as the aver-
similar to (Damen et al., 2022).
ageprecisionoverdifferentsoundclassesandIoU
Dataset: We use the videos from the Perception thresholds. For the IoU thresholds in evaluation
Test for this challenge, as in the 2023 edition.
we use [0.1 → 0.5] with 0.1 increments.
To facilitate experimentation, we also provide
features for the video / audio modalities that Dataset: As for the temporal action localisation
task above, we provide the same features for all
participants could optionally use for their sub-
the videos in the Perception Test.
missions: video features extracted using TSP (Al-
wassel et al., 2021) and audio features extracted
Baselines: We provide baseline results for this
using MMV (Alayrac et al., 2020).
task using the same model as in the action local-
isation task ActionFormer (Zhang et al., 2022),
Baselines: The baseline for this task is Action-
adapted to the sound localisation task by fine-
Former (Zhang et al., 2022) that we fine-tuned
tuning on our sound annotations belonging to
for the set of classes present in our benchmark.
the train split.
Results: Theresultsofthetop-2competingmeth-
ods are included in Table 5 and are compared Results: Table 6 shows the performance of the
top-2competingmethodsinthistrack,compared
against our baseline. Figure 8 shows the confu-
to our baseline (ActionFormer). Figure 9 com-
sion matrices of the best 2024 submission and
pares the confusion matrices of the best model
best 2023 submission.
in 2024 and best submission in 2023. The 2024
The top entry this year was submitted by
top entry was submitted by NJUST_KMG0 team
NJUST–_KMG Team and uses a multimodal
and relies on an ActionFormer architecture with
ActionFormer with video features obtained
video features extracted using VideoMAE (Tong
from UMT (Liu et al., 2022) and Video-
etal.,2022)andUMT-Large(Lietal.,2023),and
MAEv2 (Wang et al., 2023) and audio fea-
audio features using BEATS (Chen et al., 2023)
tures from BEATS (Chen et al., 2023) and CAV-
andtwovariantsofCAV-MAE(Gongetal.,2023)
MAE (Gong et al., 2023). Please check the au-
fine-tuned on AudioSet and VGGSound, respec-
thors’ report on our workshop page for more de-
tively. Thevideoandaudiofeaturesfromallthese
tails.
models are extracted independently and concate-
nated to form the input for ActionFormer, with
the audio modality having a larger number of
4.4. Temporal sound localisation
features compared to the video, which the au-
Task description: In the temporal sound local- thors found to enhance performance; check the
isation task, the model receives a video and is workshop website for more details.
requiredtolocaliseandclassifythesoundevents
occurring in the video according to a predefined
4.5. Multiple-choice video QA
set of sound classes; there are 16 sound classes
in our dataset. For the challenge, we consider Task description: In the multiple-choice video
only12classes,excludingclasseslikeBackground, question-answering (mc-vQA) task, the model
Background-Other, Human-Other, Animal-Other receives, in parallel with the video, a question
due to their ambiguity. and three possible answers, out of which only
6PerceptionTest2024:ChallengeSummaryandaNovelHour-LongVideoQABenchmark
Figure8 | Confusionmatrixofthebest2023submission(left)vsbest2024submission(right)forthe
temporal action localisation task. To be considered as a prediction for a certain segment, the model’s
confidencehastobeabove0.1andIoUthresholdbetweenthepredictionandgroundtruthabove0.1.
Ground truth actions are listed on the y-axis, sorted by their frequency and entries are normalised by
rows.
one is correct, and the model has to pick one an-
swer. The questions cover four skill areas (Mem-
ory, Abstraction, Physics, Semantics) and require
differenttypesofreasoning(Descriptive,Explana-
tory,Predictive,Counterfactual),acrossvideo,au-
dio, and text modalities. The questions are also
tagged with skills in each area such as: event
recall (Memory), object counting (Abstraction),
collision (Physics), action recognition (Seman-
tics) and more.
Metric: Theevaluationmetricforthischallengeis
top-1 accuracy. It is calculated as the percentage
of questions where the model’s predicted option
id (1 out of 3) matches the ground truth option
Figure 9 | Confusion matrices of the best 2023 id.
submission (left) vs best 2024 submission (right)
for the temporal sound localisation task. The Dataset: Weusethesamesetofvideosandques-
tions as in the 2023 challenge. Recall that each
ground truth classes are listed on the y-axis, or-
video in the dataset has a number of multiple-
deredbyfrequency,withscoresbeingnormalized
choice video QA tasks associated, each question
over rows.
having3options,outofwhichonlyoneiscorrect.
Baselines: We provide baseline results for this
task using a dummy frequency-based baseline,
with multiple setups: 0-shot, few-shot, all-shot.
Results: Table 7 shows the performance of the
top-2 competing models compared to our fre-
7PerceptionTest2024:ChallengeSummaryandaNovelHour-LongVideoQABenchmark
Figure 10 | Random and human baselines vs best 2023 vs best 2024 detailed by areas and types of
reasoning for the multiple-choice video QA task.
Figure 11 | Baseline vs best model 2023 vs best model 2024 detailed by skills for the multiple-choice
video QA task.
quency baselines. Rank Team name top-1
Baseline 1 Frequency (0-shot) 0.335
Both top-2 competing models relied on the
Baseline 2 Frequency (8-shot) 0.510
samemodel,namelyQwenVL2(7B)(Wangetal.,
Baseline 3 Frequency (all-shot) 0.552
2024) fine-tuned on our provided training set.
Runner-up TTgogogo (fine-tuned) 0.764
The best performing model employed test-time
Best SEU-2023 (fine-tuned) 0.765
augmentationandensembling,whilsttherunner-
upusedhardminingandoptionsshufflingduring Table 7 | Multiple-choice video QA results.
fine-tuning.
Figure 10 shows the performance of the best
However, Figure 10 shows that there is still a
2024 submission compared to the top 2023 sub-
significant gap compared to the human baseline,
mission. We can observe small improvements in
which, importantly, is collected in a zero-shot
Physics, Memory, and Semantics, with more no-
setting, i.e. the human participants received no
ticeable improvement in the Predictive reasoning
specific training to perform the task as detailed
type. Whendetailedperskill(Figure11),wesee
intheoriginalPerceptionTestpaper(Pătrăucean
small improvements across almost all skills.
et al., 2023).
8PerceptionTest2024:ChallengeSummaryandaNovelHour-LongVideoQABenchmark
Rank Team name HOTA
Baseline MDETR+static 0.057
Runner-up UCF_CRCV 0.241
Best Research newbie 0.270
Table 8 | Grounded video question-answering re-
sults.
4.6. Grounded video QA
Taskdescription: InthegroundedvideoQAtask,
the model receives a video and a question/query Figure 12 | Baseline vs best results 2023 vs best
as input, and it is required to track throughout results 2024 in terms of overall HOTA, detection,
the video the object(s) that represent the answer and assignment accuracy for the grounded video
to the question. This is a novel type of grounded QA task.
video QA task.
Metric: The evaluation metric for this track is Inthehour-longvideoquestion-answeringtask,
HOTA (Higher Order Tracking Accuracy) (Luiten the model receives, in parallel with the video, a
et al., 2020). It unifies the detection, association, question and five possible answers, out of which
and localization accuracy into a single metric. onlyoneiscorrect,andthemodelhastopickone
answer.
Dataset: We use the videos from the Perception
Test that have annotations for this task matching Metric: Theevaluationmetricforthischallengeis
the 2023 dataset. top-1 accuracy. It is calculated as the percentage
of questions where the model’s predicted option
Baselines: We provide a simple baseline that
id (1 out of 5) matches the ground truth option
runs MDETR detector Kamath et al. (2021) on
id.
the middle frame of the video using the given
question as query, then it keeps the detections Dataset: We use the 1h-walk VQA benchmark
static throughout the video. introduced in section 2.
Results: The top-2 results for this track are in- Baselines: Weconsiderthedummyrandombase-
cluded in Table 8 compared to our baseline. The lineforthistask,whichobtains20%. Wealsopro-
top model used Gemini for obtaining a language vide a zero-shot human baseline: each question
answer to the provided question, which was in the dataset was answered by 10 participants.
thengroundedusingGroundingDINO(Liuetal., Each participant received 27 questions. The aver-
2024); finally, the predictions were tracked over agetimeforcompletingthebatchof27questions
timeusingSAM2(Ravietal.,2024). Therunner- was3h50mandtheoverallaccuracywas99.64%.
up solution used a similar combination of 3 com-
Results: The top-2 results for this track are
ponents, with Llava-OneVision (Li et al., 2024)
included in Table 9, compared to the above
in charge of question-answering, OWLv2 (Min-
baselines. The top submission employs Gemini
derer et al., 2023) for grounding the answers,
together with a zero-shot chain-of-thought ap-
and SAM2 (Ravi et al., 2024) for tracking. Fig-
proach. The model extracts keywords and task
ure 12 compares the top model to the best 2023
clues from the questions, and processes video
submission, showinga significant improvement
segments of up to 30 minutes long in a sliding-
in performance.
window fashion, using previous windows as con-
text when processing the next window; please
check the workshop website for more details.
4.7. Hour-long video QA
These results are very promising, given how chal-
Task description: lengingthesequestionsare. However,thereisstill
9PerceptionTest2024:ChallengeSummaryandaNovelHour-LongVideoQABenchmark
Rank Team name HOTA References
Baseline Random 0.2000
Baseline Human 0.9964 J.-B.Alayrac,A.Recasens,R.Schneider,R.Arand-
Runner-up JJ_James 0.3729 jelović, J. Ramapuram, J. De Fauw, L. Smaira,
Best blackmonkey 0.4407 S.Dieleman,andA.Zisserman. Self-supervised
multimodal versatile networks. Advances in
Table 9 | Hour-long video question-answering re- Neural Information Processing Systems, 33:25–
sults. 37, 2020.
J.-B. Alayrac, J. Donahue, P. Luc, A. Miech,
a considerable gap between the top submissions
I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Mil-
and human performance.
lican, M. Reynolds, R. Ring, E. Rutherford,
S. Cabi, T. Han, Z. Gong, S. Samangooei,
M. Monteiro, J. Menick, S. Borgeaud, A. Brock,
5. Discussion
A. Nematzadeh, S. Sharifzadeh, M. Binkowski,
R. Barreira, O. Vinyals, A. Zisserman, and
The Second Perception Test challenge was very
K. Simonyan. Flamingo: a visual language
successful, attracting a large number of submis-
model for few-shot learning. In A. H. Oh,
sions from more than hundred teams across all
A. Agarwal, D. Belgrave, and K. Cho, edi-
tracks. Weobserveagreatimprovementinperfor-
tors, Advances in Neural Information Processing
mance on all tracks compared to last year, espe-
Systems, 2022. URL https://openreview.
cially in the grounded video QA track where the
net/forum?id=EbMuimAbPbs.
2023 best submission struggled to outperform a
basicbaseline. Inaddition,thenewly-addedtrack
H. Alwassel, S. Giancola, and B. Ghanem. TSP:
on hour-long video QA received strong submis-
Temporally-sensitive pretraining of video en-
sions, showing promising hour-long video under-
coders for localization tasks. In Proceedings
standing capabilities. The proposed small-scale
of the IEEE/CVF International Conference on
benchmark 1h-walk VQA was created through Computer Vision Workshops, pages 3173–3183,
a manual annotation collection process, but we
2021.
hopethatitcaninspirethecreationoflarger-scale
hour-long challenging benchmarks by, e.g., run- S. Chen, Y. Wu, C. Wang, S. Liu, D. Tomp-
ningfirstspecialisedeventdetectorsandthende- kins, Z. Chen, W. Che, X. Yu, and F. Wei.
signing questions based on these detections. For BEATs: Audio pre-training with acoustic
nextyear’seditionofthechallenge,weplantofur- tokenizers. In A. Krause, E. Brunskill,
ther emphasise the zero-shot evaluation regime K. Cho, B. Engelhardt, S. Sabato, and J. Scar-
andincentiviseparticipantstouseasinglemodel lett, editors, Proceedings of the 40th Interna-
foraddressingalltracks–inthespiritoftheorig- tional Conference on Machine Learning, vol-
inal Perception Test. ume 202 of Proceedings of Machine Learn-
ing Research, pages 5178–5193. PMLR, 23–29
Jul2023. URLhttps://proceedings.mlr.
Acknowledgements
press/v202/chen23ag.html.
We would like to thank Relja Arandjelovic for re-
S. Cho, J. Huang, J. Nam, H. An, S. Kim, and J.-Y.
viewing this report. We are grateful to Google
Lee. Local all-pair correspondence for point
DeepMind for providing the funding for the
awards and to Ashwani Sharma from Google.org
tracking. In ECCV2024, 2024.
andElderBromleyfromAimGroupforensuringa
D.Damen,H.Doughty,G.M.Farinella,A.Furnari,
smoothhandlingoftheawards. Specialthanksto
J. Ma, E. Kazakos, D. Moltisanti, J. Munro,
the Eval AI team for their support while running
T. Perrett, W. Price, and M. Wray. Rescaling
the challenges.
egocentricvision: Collection,pipelineandchal-
lenges for EPIC-KITCHENS-100. International
10PerceptionTest2024:ChallengeSummaryandaNovelHour-LongVideoQABenchmark
Journal of Computer Vision (IJCV), 130:33–55, J.Luiten,A.Osep,P.Dendorfer,P.Torr,A.Geiger,
2022. L. Leal-Taixé, and B. Leibe. Hota: A higher or-
der metric for evaluating multi-object tracking.
C. Doersch, A. Gupta, L. Markeeva, A. R. Conti- InternationalJournalofComputerVision,pages
nente, L. Smaira, Y. Aytar, J. Carreira, A. Zis- 1–31, 2020.
serman, and Y. Yang. TAP-vid: A bench-
mark for tracking any point in a video. In M. Minderer, A. A. Gritsenko, and N. Houlsby.
Scaling open-vocabulary object detection.
Thirty-sixth Conference on Neural Information
Processing Systems Datasets and Benchmarks In Thirty-seventh Conference on Neural In-
Track, 2022. URL https://openreview. formation Processing Systems, 2023. URL
net/forum?id=Zmosb2KfzYd. https://openreview.net/forum?id=
mQPNcBWjGc.
Y. Gong, A. Rouditchenko, A. H. Liu, D. Har-
wath, L. Karlinsky, H. Kuehne, and J. R. OpenAI. Gpt-4v(ision) system card. 2023. URL
Glass. Contrastive audio-visual masked https://api.semanticscholar.org/
autoencoder. In The Eleventh Interna- CorpusID:263218031.
tional Conference on Learning Representations,
P. Papalampidi, S. Koppula, S. Pathak, J. Chiu,
2023. URL https://openreview.net/
J. Heyward, V. Patraucean, J. Shen, A. Miech,
forum?id=QPtMRyk5rb.
A. Zisserman, and A. Nematzdeh. A simple
A. Kamath, M. Singh, Y. LeCun, I. Misra, G. Syn- recipe for contrastively pre-training video-first
naeve,andN.Carion. Mdetr–modulateddetec- encoders beyond 16 frames. 2024 IEEE/CVF
tionforend-to-endmulti-modalunderstanding. Conference on Computer Vision and Pattern
arXiv preprint arXiv:2104.12763, 2021. Recognition(CVPR),pages14386–14397,2023.
URL https://api.semanticscholar.
B.Li,Y.Zhang,D.Guo,R.Zhang,F.Li,H.Zhang, org/CorpusID:266174654.
K. Zhang, Y. Li, Z. Liu, and C. Li. Llava-
onevision: Easy visual task transfer. arXiv V. Pătrăucean, L. Smaira, A. Gupta, A. R. Con-
preprint arXiv:2408.03326, 2024. tinente, L. Markeeva, D. Banarse, S. Kop-
pula, J. Heyward, M. Malinowski, Y. Yang,
K.Li,Y.Wang,Y.Li,Y.Wang,Y.He,L.Wang,and C. Doersch, T. Matejovicova, Y. Sulsky,
Y. Qiao. Unmasked teacher: Towards training- A. Miech, A. Frechette, H. Klimczak, R. Koster,
efficient video foundation models, 2023. J. Zhang, S. Winkler, Y. Aytar, S. Osin-
dero, D. Damen, A. Zisserman, and J. Car-
L. Lin, H. Fan, Z. Zhang, Y. Wang, Y. Xu, and reira. Perception test: A diagnostic bench-
H. Ling. Tracking meets lora: Faster training, mark for multimodal video models. In Ad-
larger model, stronger performance. In ECCV,
vances in Neural Information Processing Sys-
2024. tems, 2023. URL https://openreview.
net/forum?id=HYEGXFnPoq.
S. Liu, Z. Zeng, T. Ren, F. Li, H. Zhang, J. Yang,
C. Li, J. Yang, H. Su, J. Zhu, et al. Ground- N.Ravi,V.Gabeur,Y.-T.Hu,R.Hu,C.Ryali,T.Ma,
ing dino: Marrying dino with grounded pre- H. Khedr, R. Rädle, C. Rolland, L. Gustafson,
trainingforopen-setobjectdetection. InECCV, E. Mintun, J. Pan, K. V. Alwala, N. Carion, C.-Y.
2024. Wu,R.Girshick,P.Dollár,andC.Feichtenhofer.
Sam2: Segmentanythinginimagesandvideos.
Y. Liu, S. Li, Y. Wu, C. W. Chen, Y. Shan, and arXiv preprint arXiv:2408.00714, 2024. URL
X. Qie. Umt: Unifiedmulti-modal transformers https://arxiv.org/abs/2408.00714.
for joint video moment retrieval and highlight
detection. In Proceedings of the IEEE/CVF Con- G. Team. Gemini: A family of highly capable
ferenceonComputerVisionandPatternRecogni- multimodal models, 2024a. URL https://
tion (CVPR), pages 3042–3051, 2022. arxiv.org/abs/2312.11805.
11PerceptionTest2024:ChallengeSummaryandaNovelHour-LongVideoQABenchmark
L. . Team. The llama 3 herd of models,
2024b. URL https://arxiv.org/abs/
2407.21783.
R. Team. Reka core, flash, and edge: A se-
ries of powerful multimodal language mod-
els, 2024c. URL https://arxiv.org/abs/
2404.12387.
Z. Tong, Y. Song, J. Wang, and L. Wang. Video-
mae: Masked autoencoders are data-efficient
learners for self-supervised video pre-training.
In S. Koyejo, S. Mohamed, A. Agarwal, D. Bel-
grave, K. Cho, and A. Oh, editors, Advances
in Neural Information Processing Systems, vol-
ume 35, pages 10078–10093. Curran Asso-
ciates, Inc., 2022.
S.Venkataramanan,M.N.Rizve,J.Carreira,Y.M.
Asano, and Y. Avrithis. Is imagenet worth 1
video? learning strong image encoders from 1
long unlabelled video. In International Confer-
ence on Learning Representations, 2024.
L. Wang, B. Huang, Z. Zhao, Z. Tong, Y. He,
Y. Wang, Y. Wang, and Y. Qiao. Videomae v2:
Scaling video masked autoencoders with dual
masking. InProceedingsoftheIEEE/CVFConfer-
enceonComputerVisionandPatternRecognition
(CVPR), pages 14549–14560, June 2023.
P. Wang, S. Bai, S. Tan, S. Wang, Z. Fan, J. Bai,
K. Chen, X. Liu, J. Wang, W. Ge, Y. Fan,
K. Dang, M. Du, X. Ren, R. Men, D. Liu,
C. Zhou, J. Zhou, and J. Lin. Qwen2-vl: En-
hancing vision-language model’s perception of
the world at any resolution. arXiv preprint
arXiv:2409.12191, 2024.
S. Yu, J. Cho, P. Yadav, and M. Bansal. Self-
chained image-language model for video local-
ization and question answering. arXiv preprint
arXiv:2305.06988, 2023.
C. Zhang, J. Wu, and Y. Li. Actionformer: Local-
izingmomentsofactionswithtransformers. In
EuropeanConferenceonComputerVision,2022.
A. Appendix
12PerceptionTest2024:ChallengeSummaryandaNovelHour-LongVideoQABenchmark
1 HowmanystatuefigureswerethereabovethegateseenjustbeforetheDIESELfashionstore?
2 Thepersonholdingthecamerawalksaroundablockstructurewithdrawingsonit.Whatdidthedrawingscontainintheorderinwhichtheywereseen?
3 Thepersonholdingthecameragoesaroundablockstructurewithdrawingsonit.Whilewalkingaroundthisblock,ayoungwomanisseenlightingacigar.Whatkind
ofdrawingisonthesidefacingthiswoman?
4 Atsomepointduringthevideo,thetimecanbeinferredfromabellringing.Howmanytimesdidthebellringandwhatwasthetimeofday?
5 WhenpassingbyMulliganspub,thereisacouplecomingfromtheoppositedirectiononthesamesideasthepubandthewomaniswearingagreenoutfit.Wherewas
thiscouplefirstseenandwhatweretheydoingthen?
6 WhichofthefollowingistrueaboutthemomentwhenthepersonholdingthecameraenterstheMulligan’spub?
7 Inwhichofthesetimeintervalsdoesthepersonholdingthecamerawalkdownthestairs?
8 WhenpassingbyHSBCbank,therearetwomenwalking,oneofthemcarryingaguitarcaseandsomeotherrectangularcase.Wheredidyouseethembeforeand
whatwasdifferentaboutthemthen?
9 Inthisvideo,whendoesthepersonholdingthecamerawalkupthestairs?
10 Inwhichofthesetimeintervalsdoesthepersonholdingthecamerawalkupthestairs?
11 WhenthepersonentersMohamedAliLane,whatmuralsappearinorderontherightwallasthepersonwalksdownthestreet?
12 HowmanypottedplantsappearinfrontoftheterracenexttoLandesmuseumoneachside?
13 TheFraumunsterChurchclocktowerappearstwice.Whichtimeisitviewedfromacrossthewaterandwhattimeofdaydiditshowthen?
14 Howmanydogsdidthepersonencounterduringthevideo?
15 Thepersonholdingthecamerapassesbytwooutdoorplaceswherepeoplesellgoods,thefirstonearound33:20timestampandthesecondonearound51:00
timestamp.Whichplacewasmorecrowded?
16 ThepersonholdingthecameracrossesPontedellaPagliatwicetakingglimpsesofBridgeofSighs.Howmuchtimepassedbetweenthe2crossings?
17 Whichmuralappearstwotimesontheperson’swalkaroundMohamedAliLane?
18 Whattypesofcatsareseensittingatopofcarsatlessthan3minutesdistancefromeachotherinorder?
19 Howmanymovingtramsappearinthefirst6minutesofthevideo?
20 HowmanytimesdoesthepersonwalkpastanH&M?
21 ThepersoncrossesthestreetatthecrossingwithMcDonaldsaround47:00timestamp.Atwhatvaluedidthetrafficlightcounterstartcountingdownwards?
22 Around32:40timestamp,thepersonholdingthecameraentersinahotel.Whichofthefollowingstatementsiscorrect?
23 Thepersonholdingthecameracrossesmultiplebridges,sometimescrossingmorethanoncethesamebridge.Thebridgecrossedaround27:00timestampisitthe
sameastheonecrossedaround7:00timestamp?
24 Thepersonenterstwochurchesinthefirsthalfhourofthevideo.Whichonehasstainedglasswindowsandwhichonehasclearglasswindows?
25 Thepersonholdingthecamerapassesbyagroupofdancersinalargeplaza.Whichofthefollowingstatementsiscorrect?
26 Thepersonholdingthecamerafilmsagroupofmusicians:2dressedinwhiteshirts,oneinstripedshirtandoneinblacktshirt.Inwhichorderdothesehappeninthe
background?
27 Thepersonholdingthecamerafilmsagroupoffourmusicians.Whatweretheywearing?
28 Inwhichorderwerethefollowinglandmarksvisited?
29 Aroundwhattimeofthevideoisthereanambulanceheard?
30 Aroundwhattimeofthedayisthereanambulanceheard?
31 Whichstatementiscorrect?
32 Whattimeofdaydoesthesecondclock-towerfilmedbythepersonindicate?
33 Whattimesofdaydothefirsttwoclock-towersencounteredindicate?
34 Aroundwhattimeofdaydidthetourstart?
35 Whichofthefollowingstatementsistrue?
36 Whichofthefollowingstatementsisfalse?
37 Amuraldepictingapinkpigisfilmedbythepersonholdingthecamera.Whichofthefollowingstatementsiscorrect?
38 Howmanywomencouldbeseenboardingthetramwhenthepersonholdingthecamerawascrossingabridgeforthefirsttime?
39 AfterpassingbyZorbarestaurantandturningonthestreettotheleft,howmanyuberdeliverypeopledidthepersonholdingthecameraencounteronthatstreet?
40 Whatdoesthepersonholdingthecameradoaftercrossingthebridgearound28:00timestamp?
41 WhenpassingbyCafféNero,whatsoundcanbeheard?
42 Howmanysologuitarbuskersappearthroughoutthevideoandwhere?
43 Thereisabuskerwithaguitarintheoutdoormallwearingacup.Whatsongaretheysinging?
44 HowmanyotherfountainsdidthepersoncrossbybeforethefountaininfrontoftheRoyaleChulanHotel?
45 Howmanyfountainsdidthepersoncrossbyintotalinthevideo?
46 Whichofthefollowingistrueaboutthefirsttemplevisitedwherechantingcanbeheard?
47 Thepersoncrossesawoodenbridgetwiceinthevideo.Whichtimearetheremorepeopleonthebridge?
48 WhatcanbeseenwhenthecameralookstotheleftwhilecrossingthebridgebeforereachingthequartetplayinginfrontofPaoloSarpistatue?
49 Whatstatueisbehindthequartetplayingmusic?
50 WhatcanbeseenwhenthecameralookstotheleftwhilecrossingthefirstbridgeafterpassingbythequartetplayinginfrontofPaoloSarpistatue?
51 Atwhattimeofdaydoesthetourstart?
52 HowmanyflagsareonthefenceinfrontofDefenceEnergyDepartment?
53 Apersonwithmimefacepaintisseenbuttoninguptheirshirtbyacanal.Whereisthispersonseenagain?
54 Thefirsttimethepersonenteredinachurch,howmuchtimedidtheyspendinside?
55 Ofallthetimestimeswhenthepersonseesagroupofswansonthewater,whichtimearetheremoreadultswans?
Table 10 | List of unique questions in the proposed hour-long video QA benchmark using Walking
Tours videos. Some questions were used over multiple videos resulting in the total of 70 QAs.
13