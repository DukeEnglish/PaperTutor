HVAC-DPT: A Decision Pretrained Transformer for
HVAC Control
AnaïsBerkes
DepartmentofComputerScience&Technology
UniversityofCambridge
UnitedKingdom
amcb6@cam.ac.uk
Abstract
Buildingoperationsconsumeapproximately40%ofglobalenergy,withHeating,
Ventilation,andAirConditioning(HVAC)systemsresponsibleforupto50%of
thisconsumption[1,2]. AsHVACenergydemandsareexpectedtorise,optimising
systemefficiencyiscrucialforreducingfutureenergyuseandmitigatingclimate
change[3]. Existingcontrolstrategieslackgeneralisationandrequireextensive
traininganddata,limitingtheirrapiddeploymentacrossdiversebuildings. This
paperintroducesHVAC-DPT,aDecision-PretrainedTransformerusingin-context
ReinforcementLearning(RL)formulti-zoneHVACcontrol. HVAC-DPTframes
HVACcontrolasasequentialpredictiontask,trainingacausaltransformeroninter-
actionhistoriesgeneratedbydiverseRLagents.ThisapproachenablesHVAC-DPT
torefineitspolicyin-context,withoutmodifyingnetworkparameters,allowingfor
deploymentacrossdifferentbuildingswithouttheneedforadditionaltrainingor
datacollection. HVAC-DPTreducesenergyconsumptioninunseenbuildingsby
45%comparedtothebaselinecontroller,offeringascalableandeffectiveapproach
tomitigatingtheincreasingenvironmentalimpactofHVACsystems.
1 Introductionandrelatedwork
AdvancedcontrollershavethepotentialtosignificantlyreduceHVACenergyconsumption[4],but
mostbuildingscontinuetorelyoninefficient,rule-basedsystems. Althoughvariousmodel-based,
data-driven, and learning-based HVAC control strategies have been proposed [4, 5], it remains a
significantchallengetoscalethesemethodsacrossdiversebuildingtypes. ModelPredictiveControl
islimitedbyitsrelianceonpreciseandbuilding-specificmodels,whileRLrequiresextensivetraining,
lastingmonthsoryears[6],whichoftenleadstosuboptimalperformanceandoccupantdiscomfort
duringthelearningphase[7]. RLalsosuffersfromseveresampleinefficiency,demandingsignificant
amountsofsensordataandrequiringretrainingforeachnewbuilding. Evenwithtransferlearning,
significant data collection and customisation is still needed to address the variability in building
structuresandthermaldynamicsbetweenthebuildingsusedfortrainingandnewtargetbuildings
[8,9].
Thetransformerarchitecture[10]hasbeenwidelyadoptedinkeyareasofmachinelearning. One
majorfeatureoftransformersisin-contextlearning,whichmakesitpossibleforthemtoadapttonew
tasksafterextensivepretraining[11]. Recentresearch,suchastheDecision-PretrainedTransformer
(DPT)byLeeetal. [12]andAlgorithmDistillationbyLaskinetal. [13],effectivelyusestransformer-
basedin-contextlearningforsequentialdecision-making. Thesemethodspredictactionsbasedona
querystateandhistoricalenvironmentdynamicswithouttheneedforweightupdatesaftertheinitial
pretrainingphase. Additionally,recentworkdemonstratesthattransformerspretrainedondiverse
datasets can generalise to new RL tasks in-context, offering a promising approach for extracting
38thConferenceonNeuralInformationProcessingSystems(NeurIPS2024).
4202
voN
92
]GL.sc[
1v64791.1142:viXrageneralistpoliciesfromofflineRLdata[14,15,16]. Nevertheless,theapplicationofin-contextRL
toHVACcontrolremainsunexplored.
Inresponse,weintroduceHVAC-DPT,apretraineddecisiontransformerthatusesin-contextRLto
optimiseHVACsystemsacrossmultiplebuildingzoneswithoutrequiringpriordataoradditional
training for new buildings. HVAC-DPT overcomes the limitations of existing control methods
by enabling scalable, data-efficient, and generalisable deployment across diverse building types,
removing the need for retraining and pre-deployment data collection in new environments. In a
year-longevaluationusingEnergyPlus[17],HVAC-DPTreducedHVACenergyconsumptionby
45%comparedtobaselineoperations,demonstratingitstransformativepotentialtoreducethecarbon
footprintofbuildingoperations.
Figure1: Schematicoverviewoftheproposedmethodology.
2 Problemdefinition
HVACsystemsconsistofoneormoreairhan-
dling units (AHUs) and variable air volume Table1: Statevariablesandactionsforeachagent.
(VAV)systems,asillustratedinFigure3. Opti-
misingHVACcontrolcanbeframedasasequen- State Unit
tialdecision-makingproblem,whereanagent
Zonemeantemperature °C
interactswiththebuilding,adjustscontrols(e.g.,
Zonemeanhumidity %
VAV actuators), and receives rewards to learn
Zoneoccupancy Binary
controlpolicies.
Outdoortemperature °C
While a single agent could manage the entire Solarradiation W
building,thisapproachlimitspolicyadaptability Houroftheday Integer
to buildings with different state-action spaces,
Action Unit
suchasthosewithvaryingnumbersofVAVsys-
tems.Consequently,wemodelHVACcontrolas VAVminimumdamperposition %
amulti-agentreinforcementlearning(MARL)
task, where each agent controls a single zone,
enablingindependentmanagementacrosszones,similartotheapproachin[6].
Themulti-agentMarkovdecisionprocessisdefinedasatuple(N;S;A ;R ;T;H),whereN denotes
i i
thenumberofagents,S isthestatespace,A istheactionspaceforagenti,R:S×A→∆(R)is
i
therewardfunction,T :S×A→∆(S)isthetransitionfunction,andH isthehorizon.
ThestatespaceS,observedbyallagents,includessixsensorreadings,detailedinTable1. Each
agent’sactionspaceA correspondstotheminimumdamperpositionintheirVAVsystem,ranging
i
from0(closed)to1(fullyopen). TherewardR foreachagentisthenegativeenergyconsumption
i
oftheVAVsystemduringthetransitionfromstatestos′. ThetransitionfunctionT isdetermined
byEnergyPlus. EachepisodehasalengthofH. AgentscontinuouslycontroltheVAVsystems,but
2thecontrolproblemismodelledasepisodic,withonemonthat15-minuteintervalsconstitutingan
episode.
3 HVAC-DPT
Themethod,illustratedinFigure1,consistsofthreestepsandbuildsuponthemethodpresentedby
Leeetal. [12]: (1)AdatasetBofRLagentinteractionsiscollectedaftertrainingapolicylibrary
of diverse RL agents in N buildings. (2) A transformer model is trained to predict action labels
basedonaquerystateandthein-contextdatasetofinteractionsDsampledfromB. (3)Oncetrained,
HVAC-DPTcanbedeployedonlineinanewbuildingbyqueryingitforpredictionsoftheoptimal
actionindifferentstates.
Algorithm1HVAC-DPT
1: //DatasetGeneration
2: InitializeemptydatasetB
3: fori←1toN do
4: Sampletrainingbuildingτ ∼T
pre
5: Buildpolicylibrary: traindiverseRLpoliciesπi forallzonesi∈[m]
τ
6: SampleinteractiondatasetD ∼D (·;τ)fromallπi
pre τ
7: Samplesi ∼D anda⋆ ∼πi(·|s )
query query τ query
8: Add(si ,D,a⋆)toB
query
9: endfor
10: //PretrainingPhase
11: InitializemodelM θ withparametersθ
12: whilenotconvergeddo
13: Sample(si ,D,a⋆)fromB
query
14: Predictpˆ j(·)=M θ(·|si query,D j)
15: ComputeMSElosswithrespecttoa⋆andbackpropagatetoupdateθ
16: endwhile
17: //OnlineDeployment
18: InitializeDi ={}forallzones
19: Sampletargetbuildingτ′ ∼T
test
20: forep←1tomax_epsdo
21: forh←1toH do
22: s 1 ←reset(τ′)
23: forzonei←1toNτ′ do
zones
24: ai
h
∼M θ(·|si h,Di)
25: si ,ri ←step(τ′,ai)
h+1 h h
26: Add(si,ai,ri,...)toDi
1 1 1
27: endfor
28: endfor
29: endfor
DatasetGeneration. ThepretrainingdatasetBiscollectedforN trainingbuildings. HVAC-DPT
generatesapolicylibraryofdiverseProximalPolicyOptimisation(PPO)RLagentsforthedifferent
zonesineachtrainingbuildingτ sampledfromthedistributionovertrainingbuildingsT . Both
pre
policyandenvironmentdiversityareusedduringtraining,asin[6]. Rolloutsofthesepoliciesare
usedtosampleanin-contextdatasetD ={s ,a ,s′,r } oftransitiontuplestakeninallzones
j j j j j∈[n]
ofτ.
Pretraining. Aquerystatesi issampledforeachzoneandalabela⋆issampledfromanagent
query
inthepolicylibrary. Thein-contextdatasetD andquerystates areusedtotrainamodelto
query
predicttheRL-labeledactiona⋆ viasupervisedlearning. Formally,wetrainaGPT-2transformer
modelM parameterisedbyθ,whichoutputsadistributionoveractionsA,tominimisetheexpected
3lossoversamplesfromthepretrainingdistribution:
(cid:88)
minE ℓ(M (·|s ,D ),a⋆). (1)
θ
Ppre θ query j
j∈[n]
whereP isthejointpretrainingdistributionoverbuildings,in-contextdatasets,querystatesand
pre
actionlabels. AswehaveacontinuousA,wesetthelosstobetheMeanSquaredError(MSE).
Online deployment. The model M can be deployed online in an unseen target building τ′
θ
by initialising an empty Di = {} for each zone i in Nτ . HVAC-DPT samples an action
zones
ai ∼M (·|si,Di)foreachzoneiateachtime-step. Diissubsequentlyfilledwiththeinteractions
h θ h
{si,ai,ri,...,si ,ai ,ri } collected during each episode. A key distinction to traditional RL
1 1 1 H H H
algorithmsisthattherearenoupdatestotheparametersofM . Oncedeployed,HVAC-DPTsimply
θ
performsacomputationthroughitsforwardpasstogenerateadistributionoveractionsconditioned
onthein-contextDiandquerystatesi.
h
4 Results
WeusedEnergyPlus[17]andCOBS[18]totrain100diversepoliciesforB ;furtherdetailsare
train
providedinAppendixB.Fourcommonlyusedcontrollerswerecompared[19,6]:
(1) The Baseline controller, which main-
tains damper openings at 50%; (2) The
Expertcontroller,implementedintheEn-
ergyPlusmodelanddesignedspecifically
foreachbuildingbyHVACengineers;(3)
SARL, a single agent RL policy that con-
trolsallzones’dampersbasedoninterac-
tionwiththetargetbuilding;and(4)MARL,
whichcontrolsindividualzonesusingthe
MARLframework.
Figure 2 demonstrates HVAC-DPT’s per-
formanceinB ,whichdiffersfrom
Denver
B in size and HVAC design, affect-
train
ing state and action spaces. HVAC-DPT Figure2: HVACenergyconsumption(MWh)ofdiffer-
reducesenergyconsumptionby45%com- entcontrollersduringthefirst12monthsofdeployment
paredtotheBaseline. HVAC-DPTisonly inbuildingB .
Denver
5%lesseffectivethantheExpertcontroller,
despitehavingnopriorknowledgeofthebuilding. TheSARLandMARLcontrollersperform74%
and70%worse,respectively,duetotheextensivetrainingrequiredtoachieveoptimalperformance,
whichcantakeupto1,250years[6]. MoredetailsaregiveninAppendixC
5 Conclusion
This paper introduces HVAC-DPT, a pretrained decision transformer that uses in-context RL to
optimiseHVACsystems. Withinthefirstyearofdeploymentinnewbuildings,HVAC-DPTreduces
energyconsumptionby45%and70%comparedtobaselineoperationsandRLagentsrespectively,all
withoutadditionaltrainingordatacollection. ThisdemonstratesHVAC-DPT’sabilitytogeneralise
effectivelyacrossbuildings,addressingcriticalchallengesinHVACcontrol,suchasscalability,data
dependency,andtrainingefficiency. FutureworkwillvalidateHVAC-DPTinreal-worldsettings,
reinforcingitspotentialasawidelydeployablesolutionforsustainablebuildingmanagement.
References
[1] EuropeanCommission. Infocus: Energyefficiencyinbuildings,202ß.
[2] LuisPérez-Lombard,JoséOrtiz,andChristinePout. Areviewonbuildingsenergyconsumption
information. Energyandbuildings,40(3):394–398,2008.
4[3] Mat Santamouris. Cooling the buildings–past, present and future. Energy and Buildings,
128:617–638,2016.
[4] JánDrgonˇa,JavierArroyo,IagoCupeiroFigueroa,DavidBlum,KrzysztofArendt,Donghun
Kim,EnricPerarnauOllé,JurajOravec,MichaelWetter,DragunaLVrabie,etal. Allyouneed
toknowaboutmodelpredictivecontrolforbuildings. AnnualReviewsinControl,50:190–232,
2020.
[5] ZheWangandTianzhenHong. Reinforcementlearningforbuildingcontrols: Theopportunities
andchallenges. AppliedEnergy,269:115036,2020.
[6] AakashKrishnaGS,TianyuZhang,OmidArdakanian,andMatthewETaylor. Mitigatingan
adoptionbarrierofreinforcementlearning-basedcontrolstrategiesinbuildings. Energyand
Buildings,285:112878,2023.
[7] Tianyu Zhang, Gaby Baasch, Omid Ardakanian, and Ralph Evins. On the joint control of
multiplebuildingsystemswithreinforcementlearning. InProceedingsoftheTwelfthACM
InternationalConferenceonFutureEnergySystems,pages60–72,2021.
[8] IlyaZisman,VladislavKurenkov,AlexanderNikulin,ViacheslavSinii,andSergeyKolesnikov.
Emergence of in-context reinforcement learning from noise distillation. arXiv preprint
arXiv:2312.12275,2023.
[9] TianyuZhang,MohammadAfshari,PetrMusilek,MatthewETaylor,andOmidArdakanian.
Diversityfortransferinlearning-basedcontrolofbuildings. InProceedingsoftheThirteenth
ACMInternationalConferenceonFutureEnergySystems,pages556–564,2022.
[10] AVaswani. Attentionisallyouneed. AdvancesinNeuralInformationProcessingSystems,
2017.
[11] ViacheslavSinii,AlexanderNikulin,VladislavKurenkov,IlyaZisman,andSergeyKolesnikov.
In-contextreinforcementlearningforvariableactionspaces. arXivpreprintarXiv:2312.13327,
2023.
[12] JonathanLee,AnnieXie,AldoPacchiano,YashChandak,ChelseaFinn,OfirNachum,and
EmmaBrunskill. Supervisedpretrainingcanlearnin-contextreinforcementlearning. Advances
inNeuralInformationProcessingSystems,36,2024.
[13] MichaelLaskin,LuyuWang,JunhyukOh,EmilioParisotto,StephenSpencer,RichieSteiger-
wald,DJStrouse,StevenHansen,AngelosFilos,EthanBrooks,etal. In-contextreinforcement
learningwithalgorithmdistillation. arXivpreprintarXiv:2210.14215,2022.
[14] XiangyuanZhang,WeichaoMao,HaoranQiu,andTamerBas¸ar. Decisiontransformerasa
foundationmodelforpartiallyobservablecontinuouscontrol. arXivpreprintarXiv:2404.02407,
2024.
[15] SubhojyotiMukherjee,JosiahPHanna,QiaominXie,andRobertNowak. Pretrainingdecision
transformerswithrewardpredictionforin-contextmulti-taskstructuredbanditlearning. arXiv
preprintarXiv:2406.05064,2024.
[16] Licong Lin, Yu Bai, and Song Mei. Transformers as decision makers: Provable in-context
reinforcementlearningviasupervisedpretraining. arXivpreprintarXiv:2310.08566,2023.
[17] DruryBCrawley,LindaKLawrie,FrederickCWinkelmann,WalterFBuhl,YJoeHuang,
Curtis O Pedersen, Richard K Strand, Richard J Liesen, Daniel E Fisher, Michael J Witte,
etal. Energyplus: creatinganew-generationbuildingenergysimulationprogram. Energyand
buildings,33(4):319–331,2001.
[18] TianyuZhangandOmidArdakanian. Cobs: Comprehensivebuildingsimulator. InProceedings
ofthe7thACMInternationalConferenceonSystemsforEnergy-EfficientBuildings,Cities,and
Transportation,pages314–315,2020.
[19] Ki Uhn Ahn, Deuk-Woo Kim, Hyun Mi Cho, and Chang-U Chae. Alternative approaches
tohvaccontrolofchatgenerativepre-trainedtransformer(chatgpt)forautonomousbuilding
systemoperations. Buildings,13(11):2680,2023.
[20] R.AmericanSocietyofHeating,A.-C.Engineers. EnergyStandardforBuildingsExceptLow-
RiseResidentialBuildings. ASHRAEInc,PeachtreeCorners,GA,USA,standard90.1-2019
edition,2019.
5A Additionalsystemdetails
Figure3: Illustrationofanairloopinamulti-zonebuildingequippedwithaforced-airheatingand
coolingsystem[6].
ThecontrollerinEnergyPlusadjuststheAHUandotherVAVcontrolpointstoensurethermalcomfort
byregulatingthesupplyairtemperatureand/orreheatcoilpower[6].
B Additionalexperimentdetails
DatasetGeneration Thecontrolagentsinthepolicylibraryaretrainedfollowingtheapproach
outlinedin[6]. WeusePPOwithaclippingparameterϵ = 0.2,whichconstrainspolicyupdates
withinatrustregiontoensurestability. Theactorandcriticnetworksareimplementedwithtwo
hiddenlayers,eachconsistingof64units,andusethehyperbolictangentastheactivationfunction.
Thelearningrateisfixedat0.0003,andthebatchsizeissetto2,976,correspondingtothelengthof
oneepisode.TheEnergyPlusmodel,usedforsimulatingbuildingoperations,operateswith15-minute
timesteps,andeachepisodespansonemonth. WeatherdatafromJanuary1991isemployedfor
training. AllpoliciesaretrainedusingPPOwithinamulti-agentreinforcementlearning(MARL)
frameworkfor1,000episodes,incorporatingbothenvironmentandpolicydiversityasdescribedin
[6].
Pretraining The HVAC-DPT model was trained using the policy library under the following
conditions: a horizon of 2,967 steps, a learning rate of 0.001, and a dropout rate of 0.0. The
Transformermodelarchitectureconsistedofthreelayers,witheightattentionheadsandanembedding
dimensionof128. Thetrainingprocesswascarriedoutfor100trajectoriesover118epochsusingthe
AdamWoptimizerwithaweightdecayof0.0001. ThelossfunctionemployedwasMSE.Themodel
wasevaluatedusingatestsplitof20%,andthetrainingwasconductedusingthePyTorchframework.
OnlineDeployment WetrainedHVAC-DPTonB ,asmallofficeprototypebuildingasdefined
train
bytheASHRAEStandard90.1[20]. B islocatedinDenver,Colorado,andcontainsfivethermal
train
zones, eachhavinganAHUandVAVsystem. Thetotalfloorareaofthisbuildingis511.16m2.
Weusedtheapproachpresentedin[6]tobuildadiversepolicylibraryofPPOagents. Weanalysed
HVAC-DPT’sperformanceonanunseenbuildingB ,amediumofficeprototype. B is
Denver Denver
locatedinDenver,Colorado,andcontains15thermalzonesacrossthreefloors. Eachfloorconsistsof
5zonesandhasanAHUand5VAVsystems. Thetotalfloorareaofthisbuildingis4,982.19m2. We
usedweatherdatafromtheyear2000andaveragemonthlyenergyconsumptionvaluesover10runs.
C Additionalresults
6Table2: MonthlytotalHVACenergyconsumption(inMWh)ofdifferentcontrollersduringthefirst
yearafterdeploymentinB .
Denver
Controller Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec
Baseline 56.75 62.52 56.30 55.78 44.54 73.15 73.38 71.65 57.57 50.87 53.87 74.38
Expert 29.51 35.77 29.92 29.15 23.88 61.16 61.84 59.92 42.88 27.11 28.07 43.62
MARL 67.74 74.63 67.84 68.46 56.73 84.75 84.42 81.74 67.13 58.39 62.24 81.85
SARL 67.25 74.55 68.03 69.15 57.78 85.92 86.05 83.97 69.75 61.23 65.24 84.85
HVAC-DPT 32.38 38.40 32.58 31.67 26.23 63.01 63.71 61.70 44.88 29.77 30.80 46.71
Table3: YearlypercentiledifferenceoftotalHVACenergyconsumptioncomparedtoHVAC-DPT
duringthefirstyearafterdeploymentinB .
Denver
Controller ∆
HVAC−DPT
Baseline +45.62%
Expert -5.78%
MARL +70.56%
SARL +74.12%
7