SCALABLE OUT-OF-DISTRIBUTION ROBUSTNESS IN
THE PRESENCE OF UNOBSERVED CONFOUNDERS
ParjanyaPrajaktaPrashant1∗, S.BaharanKhatami1, BrunoRibeiro2, BabakSalimi1
1UniversityofCaliforniaSanDiego,USA
2PurdueUniversity,USA
ABSTRACT
Weconsiderthetaskofout-of-distribution(OOD)generalization,wherethedistri-
butionshiftisduetoanunobservedconfounder(Z)affectingboththecovariates
(X) and the labels (Y). In this setting, traditional assumptions of covariate and
labelshiftareunsuitableduetotheconfounding,whichintroducesheterogeneity
in the predictor, i.e., Yˆ = f (X). OOD generalization differs from traditional
Z
domain adaptation by not assuming access to the covariate distribution (Xte) of
the test samples during training. These conditions create a challenging scenario
for OOD robustness: (a) Ztr is an unobserved confounder during training, (b)
Pte(Z) ̸= Ptr(Z), (c) Xte is unavailable during training, and (d) the posterior
predictive distribution depends on Pte(Z), i.e., Yˆ = E [f (X)]. In gen-
Pte(Z) Z
eral,accuratepredictionsareunattainableinthisscenario,andexistingliterature
hasproposedcomplexpredictorsbasedonidentifiabilityassumptionsthatrequire
multipleadditionalvariables.Ourworkinvestigatesasetofidentifiabilityassump-
tionsthattremendouslysimplifythepredictor,whoseresultingelegantsimplicity
outperformsexistingapproaches.
1 INTRODUCTION
In this paper, we explore a class of out-of-distribution (OOD) tasks that involve an unobserved
confounderZ,introducingdataheterogeneityandconfoundingtherelationshipbetweencovariates
X andthelabelY. ThisheterogeneityleadsthepredictorYˆ = f (X)tovarywithZ,resultingin
Z
ashiftinthedistributionP(X,Y)whenP(Z)changes. Suchshiftsoftencausepoorgeneralization
formodelstrainedusingempiricalriskminimization(Quiñonero-Candelaetal.,2022).
AddressingOODtasksinthepresenceoflatentconfoundersisparticularlychallengingbecauseZ
is unobserved during both training and testing, its distribution shifts (Ptr(Z) ̸= Pte(Z)), and the
posteriorpredictivedistributiondependsonPte(Z)(i.e.,Yˆ =E [f (X)]).
Zte Z
Existingapproachestypicallyfocusondistributionshiftsin X orY, assumingZ affectsonlyone
ofthem(Shimodaira,2000;Liptonetal.,2018;Arjovskyetal.,2019;Sagawaetal.,2020;Krueger
etal.,2021;Liuetal.,2021b). Recentstudies(Alabdulmohsinetal.,2023;Tsaietal.,2024)handle
caseswheretheunobservedconfounderZshifts.Thesemethodsrelyonadditionalvariablessuchas
proxyandconceptvariables,wheretheconceptvariableCservesasaconceptbottleneck,mediating
thedependencebetweenX andY insubgroupsindexedbyZ. However,inmanypracticalcases,a
conceptvariablemightnotexistorbeobservable.Furthermore,thesemethodsoftenrelyoncomplex
andhard-to-traintechniques,suchasgenerativemodelsorkernelmethods (Luetal.,2014;Lucas
etal.,2019;Paulusetal.,2020).
Motivatedbythesechallenges,weproposeasimplerandscalableapproachwhichrequiresonlyone
additional variable. Inspired by Box’s principle that “all models are wrong, but some are useful,”
we introduce a set of identifiability assumptions that simplify the estimator for learning the latent
distribution. Unlikepriorwork,wedonotrequireaconceptvariableanddemonstratethatidentifia-
bilityisachievablewithonlyaproxyvariableormultipledatasources. Basedonouridentifiability
∗Correspondingauthor:pprashant@ucsd.edu
1
4202
voN
92
]GL.sc[
1v32991.1142:viXratheory,weintroduceanovelregularizer,whichiscrucialforensuringtheconvergenceofourmodel,
asshowninSection6.
Contributions. Weproposeatwo-stagemethodforgeneralizingtoashiftintheunobservedlatent
confounder using proxies. First, we learn the posterior distribution of the latent confounder given
the covariates (Ptr(Z|X)) by training an encoder-decoder network that takes X as input and out-
puts a distribution over the proxy variable P(S|X). We prove that this distribution is identifiable
under some assumptions. Using the learned latent distribution, we then train a mixture-of-experts
model(MoE),withoneexpertperconfounderassignmentf (X),trainedwithaspecially-designed
Z
regularizer (Theorem 2). During inference, we learn Pte(Z|X) by addressing the shift in Z as a
label shift problem and re-weighting Ptr(Z|X) accordingly. We then apply the mixture model to
there-weighteddistribution. Ourmethodreliesonlyoneitherhavingaccesstoaproxyvariableor
multiple unlabeled data sources. Despite its simplicity and scalability, our approach outperforms
existingmethods.
2 RELATED WORK
InvariantOODmethods InvariantOODmethodsfocusonlearningrepresentationsthatremain
stable across training and test environments. Techniques such as IRM (Arjovsky et al., 2019),
GroupDRO (Sagawa et al., 2020), HRM (Liu et al., 2021b), and V-REx (Krueger et al., 2021)
leverage environment labels to train an invariant predictor. Other invariant approaches do not re-
quire environment labels (Nam et al., 2020; Creager et al., 2021; Liu et al., 2021a; Nam et al.,
2022). Liuetal.(2024b)relaxthestrictinvarianceassumptionbutstillfocusonasinglepredictor
forallenvironments. Additionally,Kauretal.(2022)extendinvariantpredictiontomultipleshifts.
Nevertheless,empiricalstudieshavedemonstratedthatsuchmethodsmayunderperformcompared
toempiricalriskminimization (Rosenfeldetal.,2020;Gulrajani&Lopez-Paz,2020),andtheoreti-
cally,theinvariantpredictorassumptiondoesnotholdinmanyscenarios (Schrouffetal.,2022;Lin
etal.,2022).
ProxyMethods Proxy-basedapproacheshaverecentlybeenemployedtoaddresssub-population
shifts (Alabdulmohsin et al., 2023; Tsai et al., 2024), which assume access to proxy and concept
variablestomakeaccuratepredictionsunderenvironmentalshifts. Tsaietal.(2024)eliminatethe
need for concept variables if multiple labeled domains with proxy variables are accessible. How-
ever, these methods assume that proxy variables are observable in the test data and that test data
are available during training. Proxy methods have also been explored in causal inference (Miao
et al., 2018; Tchetgen et al., 2020; Xu & Gretton, 2023), where Miao et al. (2018) and Tchetgen
etal.(2020)establishidentifiabilitywhenmultipleproxiesfortheenvironmentareobserved. Xu&
Gretton(2023)assumethelabelY isadeterministicfunctionofX andZ. Ourapproachdiffersby
relaxingseveralassumptions:(1)werequireonlyoneadditionalvariable-aproxyvariableoraccess
tomultipletrainingdomains;(2)wedonotrequireaccesstothetestdistributionduringtraining;(3)
proxyvariablesarenotrequiredinthetestset;(4)weallowbothcausalandanti-causalrelationships
betweenfeaturesandthelabel;and(5)weofferasimpleralgorithmthatavoidsgenerativemodeling
orkernelmethods.
DomainAdaptation Domainadaptationmethodstypicallyassumeaccesstothetestdistribution
duringtraining.Manyofthesemethodsseektolearnaninvariantfeaturespacebyassumingastable
predictor (Sun&Saenko,2016;Ganinetal.,2016). Gargetal.(2023)considerforrelaxed-label
shift, where P(y) shifts and P(x|y) changes in a constrained manner. Test-time adaptation meth-
odshaverecentlybeendevelopedtomodifymodelsduringinferencetomatchthetestdistribution
(Zhang et al., 2021; Wang et al., 2020; Sun et al., 2020; Niu et al., 2023). For instance, Zhang
etal.(2021)adoptameta-learningstrategy, whileWangetal.(2020)andSunetal.(2020)retrain
the model to minimize entropy and use self-supervision at test time, respectively. Recently, test-
time adaptation methods addressing spurious correlations have been proposed (Sun et al., 2023;
Eastwoodetal.,2024). However,thesemethodsdonotaddressconfoundershifts (Alabdulmohsin
etal.,2023).
CausalityinOODandDomainAdaptation CausalreasoninghasbeenappliedtoOODgener-
alizationanddomainadaptationproblems (Schölkopfetal.,2012;2021;Christiansenetal.,2021;
2Z S U Z
X Y X Y
(a)CausalDAGforproxytask (b)CausalDAGformulti-sourcetask
Figure 1: Causal diagrams. The shaded circle denotes unobserved variable and the solid circle
denotesobservedvariable. X isthecovariate,Y isthelabel,Z istheconfounder,Sistheproxyfor
theconfounder(intheproxytask),andU isthedatasetidforthemulti-sourcetask.
Sunetal.,2021;Luetal.,2021;Zhangetal.,2015;Mahajanetal.,2021). Causalmodelshelpelu-
cidate distribution shifts and identify invariant relationships (Schölkopf et al., 2012; Christiansen
etal.,2021). Sunetal.(2021)assumethatlatentspacecanbedisentangledintospuriousandcausal
features, learningthecausallatentspacefrommultiplelabeledsourcestoproduceastablepredic-
tor. Wangetal.(2022)re-weightdatatoremoveconfoundingeffectsbetweenlabelsandcovariates,
assumeY → X andrequireseveraltrainingdomainsforidentifiability. However,unobservedcon-
foundersarenotconsideredintheseworks.
3 NOTATION AND PROBLEM FORMULATION
In this section, we introduce our tasks and formally define our notation. Let X ∈ X ⊆ Rnx be
theinputvariable(e.g.,animage,agraph,aset),Y ∈ Ybethetargetvariable,andZ ∈ [n ]with
z
n ≥ 2 (where [n ] := {1,...,n }) denote the latent confounder, such as socioeconomic status,
z z z
socioculturaltraits,localeconomicconditions,andothercontextualfactors.LetPtrandPtedenote
trainandtestdistributionsrespectively. WeuseP fordistributionsthatareinvariantacrosstrainand
test. Underthesesettings,weconsidertwotasks,thatwegivearbitrarynamesforclarity.
Proxy OOD Task This task is described by the causal DAG shown in Figure 1a. We have an
additionalvariableSwhichisaproxyforthelatentconfounder(Pearl,2010;Kuroki&Pearl,2014).
Kuroki&Pearl(2014)describeasettingwhereparticipationinaSocialServicesProgramX affects
children’s cognitive ability Y. The latent socio-economic status Z acts as a confounder. Father’s
occupationandfamilyincomeS isusedasaproxyvariableforZ. Thegoalistotrainamodelthat
generalizes to test data where the distribution of the latent confounder has shifted (see the causal
DAGinFigure1a)usingtheseproxies.
Definition1(ProxyOOD). AssumethedatagenerationprocessfortheinputvariableX,thetarget
labelY, thelatentconfoundervariableZ, andproxyS followsthecausalDAGinFigure1a. Let
the training data Dtr = {(xi,yi,si)}Ntr ∼ Ptr(X,Y,S) be collected with respect to the training
confounderdistributionPtr(Z),i.e.,Pi= tr1 (X,Y,S) = (cid:80) P(X,Y,S | Z)Ptr(Z). IntheOOD
Z∈[nz]
test data, the confounder distribution shifts, i.e., Ptr(Z) ̸= Pte(Z), while the causal mechanism
betweenX,Y,Z,andSremainsinvariant. TheOODextrapolationgoalistolearnapredictorthat
canaccuratelypredictthemissingtargetlabelsinatestdatasetDte = {xi}Nte ∼ Pte(X)witha
i=1
newtestconfounderdistributionPte(Z).
Wefurtherassumethefollowingconditions:
1. DiscreteProxy: S ∈[n ]. IfS iscontinuous,wecandiscretizeit.
s
2. Overlappingconfoundersupport: supp(Pte(Z))⊆supp(Ptr(Z)).
Multi-source OOD Task This task does not assume any observed proxies but multiple training
datasets from different sources or domains with varying distributions of the confounder as shown
bythecausalDAGinFigure1b. Werequireonlyonedomaintobelabeled. Thetaskistotraina
modelthatgeneralizestounseendomainswiththedomaincausingashiftinthedistributionofthe
3latent confounder Z. For example, consider data from various health centers to predict healthcare
outcomes such as disease prevalence Y based on attributes like age, gender and diet X. Socio-
economicfactors(latentconfounderZ)confoundtherelationshipbetweenX andY. Supposeonly
the data from one health center is labeled due to resource constraints. The goal is to use labeled
datafromonehealthcenterandunlabeleddatafromseveralhealthcenterstotrainamodelthatcan
generalizetopredictoutcomesinanunseenhealthcenter.
Definition 2 (Multi-source OOD). Assume the data generation process for the input variable
X, the target label Y, the latent confounder Z, and dataset identifier U follows the causal
DAG in Figure 1b. Let P (X,Y) denote the distribution for the kth dataset, i.e., P (X,Y) =
k k
(cid:80)
P(X,Y | Z)P(Z | U = k). We assume we have access to P (X,Y) and P (X) for
Z∈[nz] 1 k
2 ≤ k ≤ n . IntheOODtestdata, weassumeweseeanewdistribution, i.e., Pte = P (X),
u nu+1
while the causal mechanism between X, Y, and Z remains invariant. The OOD extrapolation
goal is to learn a predictor that can accurately predict the missing target labels in a test dataset
Dte ={xi}Nte ∼Pte(X).
i=1
Wefurtherassumethefollowingconditions:
(cid:0) (cid:1) (cid:0) (cid:1)
1. Overlappingconfoundersupport: supp P(Z |U =n +1) ⊆supp P(Z |U =1) .
u
2. Distinctdomains: P(Z |U =k)̸=P(Z |U =l).
4 IDENTIFIABILITY
In general, latent confounder distributions with proxy cannot be identified without any assump-
tions(Pearl,2010). Ourmethodfocusesonassumptionsdesignedtoimprovepracticalapplicability.
Below, we present sufficient assumptions, justify their relevance, and contrast them with assump-
tionsfoundinexistingliterature. WhilewedefinetheassumptionsfortheProxyDAG(variable
SinFigure1a),thesameassumptionsapplytotheMulti-sourceOODtask(variableU inFig-
ure1b). SincethevaluesofZ canbearbitrarilypermuted,weestablishidentifiabilityofthelatent
distributionuptoapermutationofthevalues.
Assumption 1 (Structural Compatibility). The causal DAGs in Figure 1 are Markov-compatible
withthetrainandtestdistributions(Spirtesetal.,2000).
This assumption is commonly used while establishing identifiability results. In our case, this as-
sumption implies the following conditional independencies: (1) for the Proxy task (S ⊥ X | Z),
and(2)fortheMulti-sourcetask(U ⊥X |Z).
Assumption2(LinearlyIndependentDistributions). LetM denotethematrixofconditionalprob-
abilitiesP(S | Z),whereeachentryM isgivenbyP(S = s | Z = z). WeassumethatM has
zs
fullrank,specificallyrank(M)=n .
z
Previousworksusesimilarfull-rankassumptionstoestablishidentifiability (Alabdulmohsinetal.,
2023; Tsai et al., 2024). This assumption ensures the identifiability of the latent confounder Z.
Without it, degenerate cases where each data point forms its own cluster (i.e., ∀i,Z = i) would
i
preventidentification.
Assumption 3 (Sufficient Support). We assume that the cardinality of the support of the proxy
variablesS(denotedn )isatleastaslargeasthesupportofthelatentconfounderZ (denotedn ),
s z
i.e.,n ≥n (Alabdulmohsinetal.,2023).
s z
Thisassumptionguaranteesthattheproxyvariablescontainenoughinformationtodistinguishbe-
tweendifferentconfounderassignments(uptopermutations).
Assumption4(WeakOverlap). Thereexistsη
∈(cid:0)1,1(cid:3)
suchthat,foreachi∈{1,2,...,n },
2 z
maxP(Z =i|X =x)≥η.
x
Intuitively, this assumption suggests that for each latent variable assignment, Z = i, there is a
specific region in the data where it is dominant. For some observation X = x, the probability of
Z = i given X = x is greater than 1. This prevents excessive overlap between latent variables,
2
ensuringtheycanbemoreeasilydistinguishedbasedontheobserveddata.
4Next, we show that under these assumptions, the latent distribution is approximately identifiable,
with the identifiability error being a function of η. As η increases, indicating less overlap, the
error decreases. For large values of η (i.e., η → 1), the latent distribution becomes increasingly
identifiable,andtheerrorapproacheszero.
Theorem 1 (Approximate Identifiability of Latent Variables). Let P(X,S,Z) and Q(X,S,Z) be
two joint distributions over the observed variables X, proxy variables S, and latent variables Z.
Suppose that the marginal distributions over X and S are identical, i.e., P(X,S) = Q(X,S),
andthatAssumptions1through4aresatisfiedforbothP andQ. Then,thereexistsapermutation
π of {1,2,...,n } such that for all x ∈ supp(X) and for each i ∈ {1,2,...,n }, conditional
z z
distributionsP(Z |X =x)andQ(Z |X =x)satisfythefollowingbound:
(cid:18) (cid:19)
1−η
sup |P(Z =i|X =x)−Q(Z =π(i)|X =x)|≤O (1)
2η−1
x∈supp(X)
whereη
∈(cid:0)1,1(cid:3)
isasdefinedinAssumption4.
2
ProofSketch Theproofproceedsinthreestagesasfollows. 1)Wefirstestablishthereisaunique
valueofn thatiscompatiblewithagivenobserveddistributionP(X,S). SinceS isdiscrete,the
z
conditional distribution P(S | X = x) can be viewed as a vector for each x ∈ supp(X). The
collectionofthesevectorsspansasubspaceinRns. Weshowthatthedimensionofthissubspace
isn . 2)WeshowthattheconditionaldistributionsP(Z | X = x)andQ(Z | X = x)arerelated
z
(cid:80)
through a linear transformation. Since S ⊥ X | Z, we have: P(S | X = x) = P(S |
z∈[nz]
Z = z)P(Z = z | X = x) and similarly for Q. Because P(S | X) = Q(S | X) for all
x, it follows that there exists a matrix A such that P(Z | X = x) = AQ(Z | X = x). 3)
Finally,usingAssumption4,whichlimitstheoverlapbetweenlatentvariables,weshowthatAcan
(cid:16) (cid:17)
beapproximatedbyapermutationmatrixwithanerrorboundedbyO 1−η . Thisestablishesa
2η−1
bound on the difference between P(Z | X = x) and Q(Z | X = x), yielding the approximate
identifiabilityresult. TheformalproofisprovidedinAppendixA.1.
FullidentifiabilityfollowsdirectlyfromTheorem1.
Corollary1(FullIdentifiability). Ifη = 1,thenthelatentvariablesarefullyidentifiable. Thatis,
forallx∈supp(X)andforeachi∈{1,2,...,n },thereexistsapermutationπsuchthat
z
P(Z =i|X =x)=Q(Z =π(i)|X =x).
Next,wearguethatasthedimensionalityofXincreases,η →1,makingthelatentdistributionmore
identifiable. Intuitively, higher-dimensional covariates provide more information, making it easier
todistinguishbetweenthelatentvariableassignments,naturallydrivingη closerto1andreducing
overlap.TheWeakOverlapassumptioninAssumption4contrastswiththestrictoverlapassumption
oftenusedincausalinferenceliterature,whereitistypicallyassumedthat1−η ≤P(Z |X)≤ηfor
someη ∈ (0,1)(vanderLaan&Rose,2011). Ithasbeenshownthatasdimensionalityincreases,
satisfyingstrictoverlapbecomesincreasinglydifficult(D’Amouretal.,2021). Inspiredbythis,we
show that as the dimensionality of X increases and the features in X are sufficiently informative,
η = max P(Z = j | X = x) approaches 1. This result builds on Proposition 2 of D’Amour
x
etal.(2021),confirmingthatinhigh-dimensionalspaces,theWeakOverlapassumptioniseasierto
satisfy,leadingtobetteridentifiabilityofthelatentvariables.
Proposition1. LetZ ∈[n ]andX ∈Rdim(X). Further,assumethateachfeatureX issufficiently
z i
discriminative of the latent values of Z, conditioned on all previous features X . Specifically,
1:i−1
foralli,j,thereexistsϵ > 0suchthat, E KL(P(X |X ,Z =i)∥P(X |X ,Z =j))
P(X|Z=i) i 1:i−1 i 1:i−1
isgreaterthanϵ. Then,asdim(X)→∞,thereexistsnoη ∈(0,1)suchthat
maxP(Z =i|X =x)≤η ∀i,x.
x
Additional Discussion of Assumptions Assumptions 1–3 are common in the literature (Miao
etal.,2018;Alabdulmohsinetal.,2023;Tsaietal.,2024). IncontrasttopriorworkssuchasTsai
et al. (2024) and Miao et al. (2018), which depend on additional assumptions like the informative
variableassumption(requiringthatP(Z |X)doesnottakeafinitesetofvalues)andtheavailability
of both concept and proxy variables, our approach hinges on Assumption 4 for identifiability. We
providefurtherdiscussioninAppendixB.
5Figure2:Thisfigureillustratesthemethodologyofourproposedmodel,whichisstructuredintotwo
primaryphases: theTrainingPhase,wherewefirstestimatethelatentdistributionandsubsequently
trainamixture-of-expertsmodelforprediction; andtheTestingPhase, wherethemodelestimates
Pte(Z) (w)andreweightsthegatingfunctiontopredicttheoutputfortestinputXte.
Ptr(Z)
5 METHODOLOGY
In this section, we provide a formal description of our proposed framework, which involves two
mainphases: trainingandinference. TrainingPhase: Webeginbyestimatingthelatentdistribution
Ptr(Z | X). To ensure identifiability of the latent distribution, we introduce a novel regularizer.
Subsequently,wetrainamixture-of-expertsmodeltopredictY fromX ,whereeachexpertcor-
responds to a specific latent confounder assignment. Inference Phase: In this phase, we estimate
Pte(Z), the marginal distribution of the confounder in the test data. Using this estimate, we ob-
taintheshifteddistributionPte(Y | X)byre-weightingtheexperts. Thisre-weightingadjuststhe
learnedmodeltoaccountfortheshiftinthelatentconfounder. ThesestepsareillustratedinFigure
2.
5.1 TRAINING
Intraining, weestimatethelatentdistributionPtr(Z | X)andtrainamixture-of-expertspredictor
to predict the labels Y. The algorithm is the same for both proxy OOD and multi-source OOD
tasks. WedetailtheprocessfortheproxyOODsettinghere,notingthatthesamestepsapplytothe
multi-sourceOODsetting.
ESTIMATING LATENT DISTRIBUTION: Encoder-decodermodelsareoftenusedtoobtainthela-
tent distributions (Kingma & Welling, 2013). We design a encoder-decoder model for our task
leveragingtheassumptionsoutlinedinSection4.
First,Assumption1(S ⊥X |Z)impliesthatPtr(S |X =x)canbefactorizedasfollows:
(cid:88)
Ptr(S |X =x)= P(S |Z =z)Ptr(Z =z|X =x), (2)
z∈[nz]
ThisfactorizationallowsustodecomposethedistributionPtr(S |X)intotwocomponents: P(S |
Z)andPtr(Z |X). ThelatentvariableZ servesasanintermediaterepresentationthatcapturesthe
relevantinformationfromX aboutS. ToaccuratelyreconstructS fromX,weneedtomodelboth
Ptr(Z |X)andP(S |Z)effectively. WeparameterizePtr(Z |X)usinganeuralnetworkencoder
ϕ : Rdx → [0,1]dz. TheencodertransformsX intoadistributionoverZ usingasoftmaxfunction
in the final layer. The conditional distribution P(S | Z), which is a discrete distribution, is then
modeledbyamatrixM ∈[0,1]dz×ds,whichactsasadecodermappingZ toS.
TheparametersϕandM canbelearnedbysolvingthefollowingoptimizationproblem:
ϕ∗,M∗ =argminE [L (M⊺ ϕ(X),S)] (3)
(X,S)∼Dtr S
ϕ,M
6where M⊺ϕ(x) = (cid:80) ϕ (x)M , ϕ (x) denotes the zth component of ϕ(x), and M represents
z z z,: z z,:
thezthrowofM andL isthelossfunction.
S
AlthoughthefactorizationinEquation(2)isnotunique,meaningthatvariousM andϕcanminimize
the loss in Equation (3), enforcing Assumption 4 (weak overlap) for η = 1 can help us uniquely
identify M and ϕ. Recall that, for η = 1, Assumption 4 implies for each confounder variable
assignment, there exists x ∈ supp(X) such that P(Z = j | X = x) = 1. However, directly
enforcing this assumption is infeasible due to the vast number of potential X values to check for
large datasets. Therefore, we design a regularization loss on the matrix M which makes training
tractableandgivesusaconditionthateffectivelyenforcesAssumption4tofindZ byregularizing
thereconstructionlossinEquation(3).
Theorem 2. Consider the set M, which denotes the set of all matrices M associated with some
distributionP(S | Z)forwhichthereexistsaϕ(.)associatedwithdensityP(Z | X)suchthatM
and ϕ(.) are minimizers of the reconstruction loss in Equation (3). Let M∗ ∈ M be the matrix
associatedwithaZ thatsatisfiesAssumption4. Thenitholdsthat
L (M∗)≤L (M), ∀M ∈M,
var var
where
(cid:32)
1
(cid:88)ns (cid:18)
1
(cid:19)2(cid:33)
L (M)= max M − . (4)
var z∈{1,...,nz} n s
s=1
s,z n z
In 2, each row M represents P(S | Z = i), and L (M) is the maximum variance among these
i var
rows.TheresultindicatesthatamongallmatricesM thatsatisfythefactorizationM⊤ϕ(x)=P(S |
X = x)andminimizethereconstructionloss,thematrixM∗ thatsatisfiesAssumption4willhave
theminimumhighestvarianceofitsrows. TheformalproofisprovidedinAppendixA.4.
Therefore,ourfinaloptimizationobjectiveis
ϕ∗,M∗ =argminL (MTϕ(X),S)+λ·L (M) (5)
S var
ϕ,M
withL asinEquation(3)andL asinEquation(13).
S var
Mixture-of-Experts model. In this phase, we use the estimated latent distribution Ptr(Z | X)
from the previous phase as a mixture distribution for our mixture-of-experts (MoE) model, which
capturestheheterogeneityofthepredictiontask.
MoEmodelsconsistofmultipleexpertneuralnetworksE (x)withagatingfunctiontoweighand
k
combinetheiroutputsbasedontheinputfeatures(Jacobsetal.,1991).Forourproblem,eachexpert
is specialized for a different Z, making P(Y | X) different for each Z. The mixture distribution
Ptr(Z | X) determines the weights for combining the experts’ outputs. However, learning one
distinctexpertpervalueofZ mayleadtooverfitting,especiallyforvaluesofZ thatareassociated
withasmallnumberofdatasamples. Tomitigatethis,weshareallparametersexceptforthefinal
layer of the experts. The shared layers learn a common representation of the input features X,
followingtheintuitioninKirichenkoetal.(2022)thatlowerlayerscanlearnusefulrepresentations
across different confounder assignments. The prediction for a particular input x generated by our
MoEmodelisgivenby:
(cid:88)nz
MoE(x)= P(Z =k |X =x)E (h(x)),
k
k=1
wheren isthenumberofexperts. Here,h(x)denotesthesharedlowerlayersthatlearnacommon
z
representationoftheinputfeatures,whileE (h(x))representstheoutputofthek-thexpertnetwork.
k
ThemixtureweightsP(Z = k | X = x)determinethecontributionofeachexpert’soutputtothe
finalprediction. Figure2showsanillustrationofthearchitecture.
Trainingthismixtureiskeytoourapproachsinceitallowsthemodeltocaptureheterogeneitymore
effectively than using Z as a feature. This is because the mixture decomposes the prediction into
the mixture P(Z = k | X = x) and P (Y | X). In both of our tasks, a shift in the confounder
Z
onlyaffectsthegatingfunctionP(Z = k | X = x),whileP (Y | X)remainsstableinbothtrain
Z
andtest. Byadjustingandre-weightingthegatingfunction,themodelcanrespondtochangesinthe
latentconfounder.
75.2 INFERENCE
Duringinference,thereisashiftinP(Z)fromtraintotest,meaningthatPtr(Z |X)̸=Pte(Z |X).
ThisshiftaffectsthemixtureforourMoEmodel. SinceZ causesX,thisscenarioisanalogousto
a label shift task, where P(X | Y) stays invariant, i.e., Ptr(X | Z) = Pte(X | Z), but there is a
shiftinP(Z)fromtraintotest,i.e.,Ptr(Z)̸=Pte(Z). Inthiscase,Pte(Z |X)canbeobtainedby
re-weightingPtr(Z |X)asfollows:
Ptr(Z =z |X)Pte(Z=z)
Pte(Z =z |X)= Ptr(Z=z) (6)
(cid:80) Ptr(Z =z′ |X)Pte(Z=z′)
z′ Ptr(Z=z′)
Thisre-weighting,however,reliesonthedensityratio
Pte(Z)
(w),whichisdirectlyunobservedbut
Ptr(Z)
can be estimated from data. For instance, the method-of-moments approach introduced by Lipton
et al. (2018) can be used for this estimation. The intuition behind their approach is that, since Z
causes X, P(f(X) | Z) remains invariant from training to testing for any function f(X). By
observingthechangesinthemarginaldistributionoff(X),wecanaccuratelyestimatetheshiftin
P(Z).
ThemotivationforlearningPte(Z | X)throughre-weightingPtr(Z | X)insteadofestimatingZ
withourencoder-decoderframeworkfromtestdataistwofold: 1)Weaimtocaptureandadaptto
changesintheunderlyingdistributionofthelatentconfounderbetweentrainingandtestphases. 2)
We do not assume access to proxy variables for proxy tasks and multi-source data during the test
phase.
6 EXPERIMENTS
Inthissection,wedemonstratetheeffectivenessofourapproachonbothsyntheticandrealdata.
Baselines We compare our method against several baselines, focusing on out-of-distribution
(OOD) and domain adaptation techniques. For OOD methods, we include Invariant Risk Mini-
mization(IRM)(Arjovskyetal.,2019),VarianceRiskExtrapolation(VREx)(Kruegeretal.,2021),
andGroupDRO(Sagawaetal.,2020), whichhaveaccesstothelatentconfounderduringtraining.
For domain adaptation, we consider Domain-Adversarial Neural Networks (DANN) (Ganin et al.,
2016)andDeepCORAL(Sun&Saenko,2016).Additionally,weincludeProxyDomainAdaptation
(ProxyDA)(Tsaietal.,2024)asaproxy-baseddomainadaptationmethod. Itisimportanttonote
that some methods have access to test data during training: ProxyDA, DeepCORAL, and DANN
use test data for domain adaptation purposes. Furthermore, GroupDRO, IRM, and VREx utilize
ground-truthZ labelsduringtraining,providingthemwithadditionalinformationaboutthedataset
structure. ProxyDA requires both proxies and source/domain labels, but our approach only uses
one. Toensureafaircomparisonfortasks,weprovideProxyDAwithrandomnoiseforthevariable
whichisnotavailable. FurtherdetailsonthebaselinesanddatasetscanbefoundinAppendixC.
Synthetic Data For the proxy OOD task, we define Z ∈ {0,1,2}, with S (proxy) ∈ {0,1,2},
and X ∈ R3, while Y ∈ {0,1}. The distribution of Z shifts from P(Z) = [0.15,0.35,0.5]
during training to P(Z) = [0.7,0.2,0.1] during testing. For the multi-source OOD task, Z ∈
{0,1,2}, X ∈ R3, and Y ∈ {0,1}. The training datasets have different distributions of P(Z),
namely[0,0.7,0.3],[0.8,0.1,0.1], and[0.1,0.0,0.9], withthetestdistributionshiftingtoP(Z) =
[0.7,0.2,0.1]. Thesetasksassessourmethod’srobustnesstosignificantshiftsinZ. Theconditional
distributionP(Y | X,Z)ismodeledasaBernoullidistribution,wheretheparameterisafunction
ofX andZ,withsignificantlydifferentvaluesforeachZ.
Performance on Synthetic Data Table 1 shows the comparative performance on synthetic
datasets. We report test accuracy on out-of-distribution tasks, presenting the mean accuracy over
five runs along with the standard deviation. Our method significantly outperforms all baselines
on both datasets. This is because most baselines rely on a single predictor, which cannot capture
the heterogeneity in P(Y | X). For ProxyDA, since only one of the proxy or multiple sources is
available,itisunabletoeffectivelyadapttotheshift. Additionally,weobserveanimprovementin
performancewiththeregularizer.
8Table1: PerformanceonSyntheticData
ProxyOODTask Multi-sourceOODTask
Method
ACCOOD↑ ACCOOD↑
ERM 0.484(0.001) 0.466(0.006)
GroupDRO 0.465(0.11) 0.443(0.016)
IRM 0.465(0.001) 0.453(0.005)
DeepCORAL 0.463(0.010) 0.464(0.005)
VREx 0.523(0.034) 0.490(0.027)
DANN 0.467(0.007) 0.459(0.005)
ProxyDA 0.485(0.047) 0.462(0.012)
Ours(λ=0) 0.870(0.021) 0.794(0.037)
Ours 0.896(0.002) 0.811(0.039)
Real Data For real data, we use datasets from Ding et al. (Ding et al., 2021), covering predic-
tiontasksrelatedtoincome,employment,health,transportation,andhousing. Thesedatasetsspan
multipleyearsandallstatesoftheUnitedStates,allowingresearcherstostudytemporalshiftsand
geographicvariations(Liuetal.,2024a). TheinputX includesfeaturessuchaseducation,occupa-
tion,maritalstatus,anddemographicattributeslikesexandrace.
FortheProxyOODtask,weevaluateusingtheACSEmploymentdataset,whichpredictsemploy-
ment status using 54 features. We consider disability status (Z) as the latent confounder. Here,
Z = 1representsdisabledindividualsandZ = 0representsothers. Proxiessuchaspublicinsur-
ance coverage and independent living status (S) are used to infer disability. The distribution of Z
during training is Ptr(Z = 0) = 0.95 and Ptr(Z = 1) = 0.05, while the test distribution flips to
Pte(Z =0)=0.05andPte(Z =1)=0.95.
FortheMulti-sourceOODtask,weusetheACSIncomedatasettopredictindividualincomeusing
76 features. We consider the state of residence (Z) as the latent confounder. We train on three
syntheticsplits: 20%PuertoRico(PR)and80%California(CA);20%SouthDakota(SD)and80%
California(CA);and20%PR,20%SD,and60%CA.Testingisperformedonasplitof90%PR,
5%SD,and5%CA.
Table2: PerformanceonRealData(ProxyandMulti-sourceOODtasks)
ACSEmploymentTask ACSIncomeTask
Method
ACCOOD↑ ACCOOD↑
ERM 0.670(0.005) 0.805(0.008)
GroupDRO 0.654(0.014) 0.807(0.026)
IRM 0.663(0.006) 0.819(0.022)
DeepCORAL 0.664(0.007) 0.776(0.021)
VREx 0.702(0.014) 0.851(0.008)
DANN 0.663(0.007) 0.773(0.031)
ProxyDA 0.689(0.006) 0.856(0.040)
Ours(λ=0) 0.710(0.008) 0.856(0.052)
Ours 0.709(0.005) 0.883(0.001)
Performance on Real Data Table 2 shows the comparative performance on ACS Employment
and ACS Income. We report test accuracy on out-of-distribution tasks, presenting the mean accu-
racyoverfiverunsalongwiththestandarddeviation. Notably,ourmethodoutperformsallbaselines
inbothdatasets,demonstratingitsrobustabilitytohandleshiftsinP(X,Y)causedbylatentcon-
foundershifts.
Scalability To evaluate the scalability of our approach, we measured memory consumption and
runtime during training on the Proxy task with synthetic data for different sample sizes. Figure 3
compares our method with ProxyDA, showing peak memory usage and runtime as functions of
sample size. ProxyDA, as expected, shows cubic time complexity, typical of kernel methods. In
contrast,ourapproachtheoreticallyhaslineartimecomplexityforafixednumberofiterations. In
practice, early stopping reduces the number of iterations for larger sample sizes, thereby lowering
9Figure3: TrainingtimeandmemoryusageforOurmethodandProxyDA
runtimefurtherasseenintheplot. Regardingmemoryourmethod’smemoryusagestaysconstant
whileProxyDA’smemoryusageincreaseswithsamplesize. ThismakesitinfeasibleforProxyDA
toscaletodatasetswithlargesamplesizes.
7 CONCLUSION
Inthiswork,wetackledthechallengingtaskofout-of-distributiongeneralizationinthepresenceof
unobservedconfounders. Weintroducedasimpleyetscalablemethodthatfirstestimatesthedistri-
butionoflatentconfounderandthenadaptstheclassifiertothetestdistribution. Ourapproachnot
onlytheoreticallyprovestheidentifiabilityofthelatentdistributionbutalsoempiricallyoutperforms
existingbaselinesacrossmultipletasks.Thisdemonstratestherobustnessandpracticaleffectiveness
of our method, providing a strong foundation for future research in developing OOD-robust tech-
niquesthathandleunobservedconfounders. Ourmethodoperatesunderrelativelysimpleassump-
tions, however, there may be scenarios where these assumptions fail to hold. Future work should
considerunderstandingandidentifyingtheselimitationsandbuilduponourproposedmethod.
10REFERENCES
Ibrahim Alabdulmohsin, Nicholas Chiou, Alexander D’Amour, Arthur Gretton, Sanmi Koyejo,
Matt J. Kusner, Stephen R. Pfohl, Olakunle Salaudeen, Jessica Schrouff, and Kai-Wei Tsai.
Adapting to latent subgroup shifts via concepts and proxies. In International Conference on
ArtificialIntelligenceandStatistics,pp.9637–9661.PMLR,2023.
MartinArjovskyetal. Invariantriskminimization. arXivpreprintarXiv:1907.02893,2019.
RuneChristiansen,NiklasPfister,MartinEmilJakobsen,NicolaGnecco,andJonasPeters.Acausal
frameworkfordistributiongeneralization. IEEETransactionsonPatternAnalysisandMachine
Intelligence,44(10):6614–6630,2021.
Elliot Creager, Jörn-Henrik Jacobsen, and Richard Zemel. Environment inference for invariant
learning. InInternationalConferenceonMachineLearning.PMLR,2021.
FrancesDing,MoritzHardt,JohnMiller,andLudwigSchmidt.Retiringadult:Newdatasetsforfair
machinelearning. Advancesinneuralinformationprocessingsystems,34:6478–6490,2021.
AlexanderD’Amour,PengDing,AviFeller,LihuaLei,andJasjeetSekhon.Overlapinobservational
studieswithhigh-dimensionalcovariates. JournalofEconometrics,221(2):644–654,2021.
Cian Eastwood, Shashank Singh, Andrei L Nicolicioiu, Marin Vlastelica Pogancˇic´, Julius von
Kügelgen, and Bernhard Schölkopf. Spuriosity didn’t kill the classifier: Using invariant pre-
dictions to harness spurious features. Advances in Neural Information Processing Systems, 36,
2024.
Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François
Laviolette,MarioMarch,andVictorLempitsky. Domain-adversarialtrainingofneuralnetworks.
Journalofmachinelearningresearch,17(59):1–35,2016.
Saurabh Garg, Nick Erickson, James Sharpnack, Alex Smola, Sivaraman Balakrishnan, and
ZacharyChaseLipton. Rlsbench: Domainadaptationunderrelaxedlabelshift. InInternational
ConferenceonMachineLearning,pp.10879–10928.PMLR,2023.
Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. arXiv preprint
arXiv:2007.01434,2020.
RobertAJacobs,MichaelIJordan,StevenJNowlan,andGeoffreyEHinton. Adaptivemixturesof
localexperts. Neuralcomputation,3(1):79–87,1991.
JivatNeetKaur,EmreKiciman,andAmitSharma. Modelingthedata-generatingprocessisneces-
saryforout-of-distributiongeneralization. arXivpreprintarXiv:2206.07837,2022.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114,2013.
PolinaKirichenko,PavelIzmailov,andAndrewGordonWilson. Lastlayerre-trainingissufficient
forrobustnesstospuriouscorrelations. arXivpreprintarXiv:2204.02937,2022.
David Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan Binas, Dinghuai
Zhang,RemiLePriol,andAaronCourville. Out-of-distributiongeneralizationviariskextrapo-
lation(rex). InInternationalConferenceonMachineLearning,pp.5815–5826.PMLR,2021.
Manabu Kuroki and Judea Pearl. Measurement bias and effect restoration in causal inference.
Biometrika,101(2):423–437,2014.
Yong Lin, Shengyu Zhu, Lu Tan, and Peng Cui. Zin: When and how to learn invariance without
environment partition? Advances in Neural Information Processing Systems, 35:24529–24542,
2022.
ZacharyC.Lipton,Yu-XiangWang,andAlexSmola. Detectingandcorrectingforlabelshiftwith
blackboxpredictors. InInternationalConferenceonMachineLearning,pp.3122–3130.PMLR,
2018.
11Evan Z Liu, Behzad Haghgoo, Annie S Chen, Aditi Raghunathan, Pang Wei Koh, Shiori Sagawa,
Percy Liang, and Chelsea Finn. Just train twice: Improving group robustness without training
group information. In International Conference on Machine Learning, pp. 6781–6792. PMLR,
2021a.
Jiashuo Liu, Tianyu Wang, Peng Cui, and Hongseok Namkoong. On the need for a language de-
scribing distribution shifts: Illustrations on tabular datasets. Advances in Neural Information
ProcessingSystems,36,2024a.
JiashuoLiu,JiayunWu,JiePeng,XiaoyuWu,YangZheng,BoLi,andPengCui. Enhancingdistri-
butional stability among sub-populations. In International Conference on Artificial Intelligence
andStatistics,pp.2125–2133.PMLR,2024b.
JiashuoLiuetal.Heterogeneousriskminimization.InInternationalConferenceonMachineLearn-
ing.PMLR,2021b.
ChaochaoLuetal. Invariantcausalrepresentationlearningforout-of-distributiongeneralization. In
InternationalConferenceonLearningRepresentations,2021.
ZhiyunLu,AvnerMay,KuanLiu,AlirezaBagheriGarakani,DongGuo,AurélienBellet,LinxiFan,
MichaelCollins,BrianKingsbury,MichaelPicheny,etal. Howtoscaleupkernelmethodstobe
asgoodasdeepneuralnets. arXivpreprintarXiv:1411.4000,2014.
James Lucas, George Tucker, Roger Grosse, and Mohammad Norouzi. Understanding posterior
collapseingenerativelatentvariablemodels. 2019.
DivyatMahajan,ShrutiTople,andAmitSharma. Domaingeneralizationusingcausalmatching. In
Internationalconferenceonmachinelearning,pp.7313–7324.PMLR,2021.
WangMiao,ZhiGeng,andEricJ.TchetgenTchetgen. Identifyingcausaleffectswithproxyvari-
ablesofanunmeasuredconfounder. Biometrika,105(4):987–993,2018.
Junhyun Nam, Hyuntak Cha, Sungsoo Ahn, Jaeho Lee, and Jinwoo Shin. Learning from failure:
De-biasingclassifierfrombiasedclassifier. AdvancesinNeuralInformationProcessingSystems,
33:20673–20684,2020.
JunhyunNam, JaehyungKim, JaehoLee, andJinwooShin. Spreadspuriousattribute: Improving
worst-groupaccuracywithspuriousattributeestimation. arXivpreprintarXiv:2204.02070,2022.
Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Zhiquan Wen, Yaofo Chen, Peilin Zhao, and
Mingkui Tan. Towards stable test-time adaptation in dynamic wild world. arXiv preprint
arXiv:2302.12400,2023.
Max B Paulus, Chris J Maddison, and Andreas Krause. Rao-blackwellizing the straight-through
gumbel-softmaxgradientestimator. arXivpreprintarXiv:2010.04838,2020.
JudeaPearl. Onmeasurementbiasincausalinference. InProceedingsoftheTwenty-SixthConfer-
enceonUncertaintyinArtificialIntelligence,pp.425–432,2010.
Joaquin Quiñonero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil D Lawrence.
Datasetshiftinmachinelearning. MitPress,2022.
ElanRosenfeld,PradeepRavikumar,andAndrejRisteski. Therisksofinvariantriskminimization.
arXivpreprintarXiv:2010.05761,2020.
S.Sagawa, P.W.Koh, T.B.Hashimoto, andP.Liang. Distributionallyrobustneuralnetworksfor
groupshifts: Ontheimportanceofregularizationforworst-casegeneralization. InInternational
ConferenceonLearningRepresentations,2020.
BernhardSchölkopf,DominikJanzing,JonasPeters,EleniSgouritsa,KunZhang,andJorisMooij.
Oncausalandanticausallearning. arXivpreprintarXiv:1206.6471,2012.
Bernhard Schölkopf, Francesco Locatello, Stefan Bauer, Nan Rosemary Ke, Nal Kalchbrenner,
Anirudh Goyal, and Yoshua Bengio. Toward causal representation learning. Proceedings of
theIEEE,109(5):612–634,2021.
12Jessica Schrouff, Nathan Harris, Sanmi Koyejo, Ibrahim M. Alabdulmohsin, Elisabeth Schnider,
Kaitlyn Opsahl-Ong, Anne Brown, Shalmali Roy, Daria Mincu, Chen Chen, et al. Diagnosing
failuresoffairnesstransferacrossdistributionshiftinreal-worldmedicalsettings. InAdvances
inNeuralInformationProcessingSystems,volume35,pp.19304–19318,2022.
Hidetoshi Shimodaira. Improving predictive inference under covariate shift by weighting the log-
likelihoodfunction. JournalofStatisticalPlanningandInference,90(2):227–244,2000.
Peter Spirtes, Clark N. Glymour, and Richard Scheines. Causation, Prediction, and Search. MIT
Press,Cambridge,MA,2000.
Baochen Sun and Kate Saenko. Deep coral: Correlation alignment for deep domain adaptation.
In Computer Vision–ECCV 2016 Workshops: Amsterdam, The Netherlands, October 8-10 and
15-16,2016,Proceedings,PartIII14,pp.443–450.Springer,2016.
QingyaoSun,KevinPMurphy,SaynaEbrahimi,andAlexanderD’Amour. Beyondinvariance:test-
timelabel-shiftadaptationforaddressing"spurious"correlations.AdvancesinNeuralInformation
ProcessingSystems,36:23789–23812,2023.
Xinwei Sun et al. Recovering latent causal factor for generalization to distributional shifts. In
AdvancesinNeuralInformationProcessingSystems,volume34,pp.16846–16859,2021.
YuSun,XiaolongWang,ZhuangLiu,JohnMiller,AlexeiEfros,andMoritzHardt. Test-timetrain-
ingwithself-supervisionforgeneralizationunderdistributionshifts. InInternationalconference
onmachinelearning,pp.9229–9248.PMLR,2020.
Eric J Tchetgen Tchetgen, Andrew Ying, Yifan Cui, Xu Shi, and Wang Miao. An introduction to
proximalcausallearning. arXivpreprintarXiv:2009.10982,2020.
Katherine Tsai, Stephen R Pfohl, Olawale Salaudeen, Nicole Chiou, Matt Kusner, Alexander
D’Amour, Sanmi Koyejo, and Arthur Gretton. Proxy methods for domain adaptation. In In-
ternationalConferenceonArtificialIntelligenceandStatistics,pp.3961–3969.PMLR,2024.
MarkJ.vanderLaanandSherriRose. TargetedLearning. Springer,NewYork,NY,2011.
DequanWang, EvanShelhamer, ShaotengLiu, BrunoOlshausen, andTrevorDarrell. Tent: Fully
test-timeadaptationbyentropyminimization. arXivpreprintarXiv:2006.10726,2020.
Xinyi Wang et al. Causal balancing for domain generalization. arXiv preprint arXiv:2206.05263,
2022.
Liyuan Xu and Arthur Gretton. Kernel single proxy control for deterministic confounding. arXiv
preprintarXiv:2308.04585,2023.
KunZhang,MingmingGong,andBernhardSchölkopf. Multi-sourcedomainadaptation: Acausal
view. InProceedingsoftheAAAIConferenceonArtificialIntelligence,volume29,2015.
Marvin Zhang, Henrik Marklund, Nikita Dhawan, Abhishek Gupta, Sergey Levine, and Chelsea
Finn. Adaptiveriskminimization: Learningtoadapttodomainshift. AdvancesinNeuralInfor-
mationProcessingSystems,34:23664–23678,2021.
13A PROOFS
A.1 PROOFOFTHEOREM1
Theorem 1 (Approximate Identifiability of Latent Variables). Let P(X,S,Z) and Q(X,S,Z) be
two joint distributions over the observed variables X, proxy variables S, and latent variables Z.
SupposethatthemarginaldistributionsoverX andS areidentical,i.e.,P(X,S) = Q(X,S),and
that Assumptions 1 through 3 are satisfied for both P and Q. Then, there exists a permutation π
of {1,2,...,n } such that for all x ∈ supp(X) and for each i ∈ {1,2,...,n }, the conditional
z z
distributionsP(Z =i|X =x)andQ(Z =π(i)|X =x)satisfythefollowingbound:
(cid:18) (cid:19)
1−η
sup |P(Z =i|X =x)−Q(Z =π(i)|X =x)|≤O (7)
2η−1
x∈supp(X)
whereη
∈(cid:0)1,1(cid:3)
isasdefinedinAssumption4.
2
Proof. Theprooffollowsthreemainsteps:
1. We first establish there is a unique value of n that is compatible with a given observed
z
distributionP(X,S).
2. WeshowthattheconditionaldistributionsP(Z | X = x)andQ(Z | X = x)arerelated
throughalineartransformation.Wefurthershowthatthislineartransformationmustsatisfy
certainconstraints.
3. Finally, using Assumption 4, which limits the overlap between latent variables, we show
(cid:16) (cid:17)
thatAcanbeapproximatedbyapermutationmatrixwithanerrorboundedbyO 1−η .
2η−1
ThisestablishesaboundonthedifferencebetweenP(Z |X =x)andQ(Z |X =x).
1. EstablishingDimensionoftheSubspaceSpannedbyP(S |X =x)=n
z
Under Assumption 1, we have that S ⊥ X | Z. Therefore, the conditional distribution can be
expressedas:
(cid:88)nz
P(S |X =x)= P(S |Z =z)P(Z =z |X =x).
z=1
LetusconsiderP(S |Z)asamatrixM
P
∈Rnz×ns whereeachrowcorrespondstoP(S |Z =z)
forz ∈{1,2,...,n }. ByAssumption2,thematrixM hasfullrank,i.e.,rank(M )=n .
z P P z
Since P(S | X = x) is a linear combination of the rows of M , the set of all such vectors
P
{P(S |X =x)} lieswithintherowspaceofM .
x∈supp(X) P
Given that M has full rank, the dimension of this subspace is exactly n . Formally, the
P z
spanof{P(S |X =x)}satisfies:
dim(span{P(S |X =x)|x∈supp(X)})=n .
z
Therefore,thereisauniquen foragivenobserveddistribution.
z
2. RelatingP(Z |X =x)andQ(Z |X =x)throughaLinearTransformation
GiventhatP(X,S)=Q(X,S),itfollowsthatforallx∈supp(X):
P(S |X =x)=Q(S |X =x).
14UsingAssumption1,wecandecomposetheseconditionaldistributionsas:
(cid:88)nz
P(S |X =x)= P(S |Z =z)P(Z =z |X =x)=MTP(Z |X =x), (8)
P
z=1
(cid:88)nz
Q(S |X =x)= Q(S |Z =z)Q(Z =z |X =x)=MTQ(Z |X =x), (9)
Q
z=1
wherewedefinethematricesM
P
∈Rnz×ns andM
Q
∈Rnz×ns suchthat:
(M ) =P(S =s|Z =z), (M ) =Q(S =s|Z =z).
P z,s Q z,s
GiventhatP(S |X =x)=Q(S |X =x)forallx,wehave:
MTP(Z |X =x)=MTQ(Z |X =x).
P Q
SinceAssumption2ensuresthatM hasfullrankandAssumption3guaranteesthatn ≥ n ,the
P s z
matrixM⊤admitsaleftMoore-Penrosepseudo-inverse,denotedbyM† ∈Rnz×ns,satisfying:
P P
M†M⊤ =I ,
P P nz
whereI isthen ×n identitymatrix. MultiplyingbothsidesoftheequationbyM† yields:
nz z z P
M†M⊤P(Z |X =x)=M†M⊤Q(Z |X =x),
P P P Q
whichsimplifiesto:
P(Z |X =x)=AQ(Z |X =x), (10)
wherewedefinethetransformationmatrixAas:
A=M†M⊤.
P Q
Constraints: WeshowthatAmustsatisfycertainconstraintsandcannotbeanarbitrarymatrix.
1)We claim that 1⊤A = 1⊤, where 1 ∈ Rnz is the vector of ones. In other words, the sum of
elementsineachcolumnofAis1.
Proof: GiventhatP(S |X =x)=Q(S |X =x)forallx,thecolumnsofM⊤ andM⊤ spanthe
P Q
samesubspace. Therefore,thereexistsamatrixC ∈Rnz×nz suchthat:
M⊤C =M⊤.
P Q
Thisimplies,
C =M†M⊤ =A
P Q
We know that both M and M represent probability distributions P(S | Z) and Q(S | Z),
P Q
meaningthat:
1⊤M⊤ =1⊤, 1⊤M⊤ =1⊤,
P Q
where1 ∈ Rns isthevectorofones. ApplyingtheseequalitiestotheequationM⊤A = M⊤,we
P Q
get:
1⊤M⊤A=1⊤M⊤.
P Q
Substitutingtheknownequalities:
1⊤A=1⊤, (11)
ThismeansthatthesumoftheelementsineachcolumnofAis1.
2)WederiveupperandlowerboundsontheentriesofA.
Consider any row r of A. By the definition of conditional probabilities and equation equation 10,
wehave:
(cid:88)nz
P(Z =r |X =x)= A Q(Z =i|X =x), ∀x∈supp(X).
ri
i=1
15SinceP(Z =r |X =x)isavalidprobability,itsatisfies:
(cid:88)nz
0≤ A Q(Z =i|X =x)≤1.
ri
i=1
Subtracting 1 frombothsides,weobtain:
2
1
(cid:88)nz (cid:18) 1(cid:19)
1
− ≤ A − Q(Z =i|X =x)≤ .
2 ri 2 2
i=1
Letλ=max
i(cid:12)
(cid:12)A ri−
21(cid:12)
(cid:12). Withoutlossofgenerality,assumethatthemaximumisachievedati=1.
Then:
(cid:12) (cid:12)
(cid:12)(cid:88)nz (cid:18) 1(cid:19) (cid:12) 1
(cid:12) A − Q(Z =i|X =x)(cid:12)≤ .
(cid:12) ri 2 (cid:12) 2
(cid:12) (cid:12)
i=1
Applyingthetriangleinequality,wehave:
(cid:12) (cid:12) (cid:12) (cid:12)(cid:18) A r1− 1 2(cid:19) Q(Z =1|X =x)(cid:12) (cid:12) (cid:12) (cid:12)−(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)(cid:88)nz (cid:18) A ri− 21(cid:19) Q(Z =i|X =x)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)≤ 21 .
i=2
Since|A − 1|≤λforalli,itfollowsthat:
ri 2
(cid:88)nz
1
λQ(Z =1|X =x)−λ Q(Z =i|X =x)≤ .
2
i=2
Simplifying,weget:
1
λQ(Z =1|X =x)−λ(1−Q(Z =1|X =x))≤ ,
2
whichfurthersimplifiesto:
1
λ(2Q(Z =1|X =x)−1)≤ .
2
Giventhatthereexistsx∗ ∈ supp(X)suchthatQ(Z = 1 | X = x∗) ≥ η, whereη ∈ (cid:0)1,1(cid:3) , we
2
substitutetoobtain:
1 1
λ(2η−1)≤ ⇒ λ≤ .
2 2(2η−1)
Therefore,eachentryA satisfies:
ri
1−η 1−η
− ≤A ≤1+ . (12)
2η−1 ri 2η−1
3)WederivelowerboundsonthemaximumentryineachrowofA
Letµ=max A . FromthedefinitionofP(Z =r |X =x),wehave:
i ri
(cid:88)nz (cid:88)nz
P(Z =r |X =x)= A Q(Z =i|X =x)≤µ Q(Z =i|X =x)=µ.
ri
i=1 i=1
Sincemax P(Z =r |X =x)≥η,itfollowsthatµ≥η.
x
3. Acanbeapproximatedbyapermutationmatrix
WeaimtoshowthatthereexistsapermutationmatrixΠsuchthat:
(cid:18) (cid:19)
1−η
|A −Π |<O , ∀i,j.
ij ij 2η−1
If n = 1, A is trivially the identity matrix and is equal to a permutation matrix. Henceforth, we
z
assumen ≥2. Considerthefollowingcases:
z
16Case1: 1 < η ≤ nz+1 Forthiscase,consideranygenericpermutationmatrixΠ. Fromequation
2 nz+2
equation12,wehave:
(cid:18) (cid:19)
1−η n +2 1−η 1−η (n +2)(1−η) 1−η 1−η
|A −Π |≤1+ ≤ z + ≤(n +2)(1−η)+ ≤ z + =O .
ij ij 2η−1 n +2 2η−1 z 2η−1 2η−1 2η−1 2η−1
z
Case2: nz+1 <η ≤1
nz+2
Startingwiththegiveninequalityη > nz+1,wehave
nz+2
η(n +2)>n +1
z z
n η−n >1−2η
z z
n (1−η)<2η−1
z
(1−η)
(2η−1)>(n −2)
z 2η−1
(1−η)
2η >1+(n −2)
z 2η−1
Theaboveinequality,equationequation11andequation12implythatnomorethanoneelementin
anycolumncanbegreaterthanorequaltoη.
Since each row must have at least one element ≥ η and A is a square matrix, it follows that each
columnhasexactlyoneelement≥η. Withoutlossofgenerality,considerthatforcolumnc,thefirst
elementis≥η,i.e. A ≥η.
1c
Then,
(cid:88)nz
A =1−A
ic 1c
i=2
SinceallA ≥− 1−η ,wehave:
ic 2η−1
1−η
maxA ≤(1−η)+(n −2)
i̸=1 ic z 2η−1
Also,
(cid:26) (cid:27)
1−η
|1−A |≤max 1−η,
1c 2η−1
and
(cid:26) (cid:27)
1−η 1−η
|A |≤max (1−η)+(n −2) ,
ic z 2η−1 2η−1
DefinethepermutationmatrixΠsuchthat:
(cid:26)
1 ifA ≥η,
Π = ij
ij 0 otherwise.
Byconstruction,Πsatisfies:
(cid:26) (cid:27) (cid:18) (cid:19)
(n −2)(1−η) (1−η) 1−η
|Π −A |≤max 1−η, z , =O .
ij ij 2η−1 2η−1 2η−1
Wenowobtainthefinalboundonthedifferenceoftheposteriordistributions.
Fromequationequation10,wehaveP(Z|X =x)=AQ(Z|X =x). Usingthis,wecanderive:
17(cid:12) (cid:12)
(cid:12) (cid:12)(cid:88)nz (cid:12)
(cid:12)
|P(Z =i|X =x)−Q(Z =π(i)|X =x)|=(cid:12) A ijQ(Z =j|X =x)−Q(Z =π(i)|X =x)(cid:12)
(cid:12) (cid:12)
(cid:12)j=1 (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)(cid:88)nz (cid:12)
(cid:12)
=(cid:12) (A
ij
−Π ij)Q(Z =j|X =x)(cid:12)
(cid:12) (cid:12)
(cid:12)j=1 (cid:12)
(cid:88)nz
≤ |A −Π ||Q(Z =j|X =x)|
ij ij
j=1
(cid:88)nz
≤max|A −Π | |Q(Z =j|X =x)|
ij ij
j
j=1
=max|A −Π |
ij ij
j
where we used the triangle inequality and the fact that
(cid:80)nz
|Q(Z = j|X = x)| = 1 since
j=1
Q(Z|X =x)isaprobabilitydistribution.
Fromouranalysisinstep6,weknowthat:
(cid:18) (cid:19)
1−η
max|A −Π |≤O
j ij ij 2η−1
Therefore,wecanconclude:
(cid:18) (cid:19)
1−η
sup |P(Z =i|X =x)−Q(Z =π(i)|X =x)|≤O ,
2η−1
x∈supp(X)
forapermutationπcorrespondingtomatrixΠ
whichprovesTheorem1.
A.2 PROOFOFCOROLLARY1
Corollary1(FullIdentifiability). Ifη = 1,thenthelatentvariablesarefullyidentifiable. Thatis,
forallx∈supp(X)andforeachi∈{1,2,...,n },thereexistsapermutationπsuchthat
z
P(Z =i|X =x)=Q(Z =π(i)|X =x).
Proof. TheprooffollowsdirectlyfromTheorem1. Whenη = 1,theboundinequationequation7
becomes:
(cid:18) (cid:19)
1−η
sup |P(Z =i|X =x)−Q(Z =π(i)|X =x)|≤O
2η−1
x∈supp(X)
(cid:18) (cid:19)
1−1
=O
2(1)−1
=O(0)
=0
Therefore,
P(Z =i|X =x)=Q(Z =π(i)|X =x)
for all x and i, proving that the latent variables are fully identifiable up to permutation when η =
1.
18A.3 PROOFOFPROPOSITION1
ThispropositionandproofcloselyfollowsD’Amouretal.(2021).
Proposition1. LetZ ∈[n ]andX ∈Rdim(X). Further,assumethateachfeatureX issufficiently
z i
discriminative of the latent values of Z, conditioned on all previous features X . Specifically,
1:i−1
foralli,j,thereexistsϵ > 0suchthat, E KL(P(X |X ,Z =i)∥P(X |X ,Z =j))
P(X|Z=i) i 1:i−1 i 1:i−1
isgreaterthanϵ. Then,asdim(X)→∞,thereexistsnoη ∈(0,1)suchthat
maxP(Z =i|X =x)≤η ∀i,x.
x
Proof. Forsimplicity,weassumeZ ∈{0,1}. Weproceedbycontradiction.
Supposethereexistsanη ∈(0,1)suchthatforallx:
P(Z =0|X =x)≤η and P(Z =1|X =x)≤η
Thisimpliesthatforallx:
1−η ≤P(Z =0|X =x)≤η and 1−η ≤P(Z =1|X =x)≤η
ItfollowsthatP(Z =0)>0andP(Z =1)>0. Letγ =P(Z =1)/P(Z =0). Foranyx:
P(X =x|Z =0) P(Z =0|X =x) 1 η 1
= · ≤ ·
P(X =x|Z =1) P(Z =1|X =x) γ 1−η γ
Takinglogarithms:
P(X =x|Z =0) η
log ≤log −logγ
P(X =x|Z =1) 1−η
Now,considertheKLdivergencebetweenP(X |Z =0)andP(X |Z =1):
KL(P(X |Z =0)∥P(X |Z =1))
(cid:20) (cid:21)
P(X |Z =0)
=E log
X∼P(X|Z=0) P(X |Z =1)
η
≤log −logγ
1−η
ThisimpliesthattheKLdivergenceisbounded. However,bythechainruleofKLdivergenceand
theconditiongivenintheproposition:
KL(P(X |Z =0)∥P(X |Z =1))
dim(X)
(cid:88)
= E [KL(P(X |X ,Z =0)∥P(X |X ,Z =1))]
P(X|Z=0) i 1:i−1 i 1:i−1
i=1
≥dim(X)·ϵ
Asdim(X)→∞,theright-handsideapproachesinfinity,contradictingtheboundednessweestab-
lishedearlier. Therefore,ourinitialassumptionmustbefalse. Weconcludethatasdim(X) → ∞,
thereexistsnoη ∈(0,1)suchthatmax P(Z =z |X =x)≤ηforallz ∈{0,1}.
x
A.4 PROOFOFTHEOREM2
Theorem 2. Consider the set M, which denotes the set of all matrices M associated with some
distributionP(S | Z)forwhichthereexistsaϕ(.)associatedwithdensityP(Z | X)suchthatM
andϕ(.)areminimizersofthereconstructionlossE [L (M⊺ϕ(X),S)]. LetM∗ ∈Mbe
(X,S)∼Dtr S
thematrixassociatedwithaZ thatsatisfiesAssumption4forη =1. Thenitholdsthat
L (M∗)≤L (M), ∀M ∈M,
var var
where
(cid:32)
1
(cid:88)ns (cid:18)
1
(cid:19)2(cid:33)
L (M)= max M − . (13)
var z∈{1,...,nz} n s
s=1
s,z n s
19Proof. Toprovethis,wefirstprovethefollowinglemma.
Lemma1. Letv,w ,w ,...,w ∈ [0,1]k bevectorsrepresentingprobabilitydistributions, i.e.,
1 2 n
(cid:80)k
w
=1forall1≤i≤nand(cid:80)k
v =1. Supposev
=(cid:80)n
α w
where(cid:80)n
α =1
j=1 ij j=1 j i=1 i i i=1 i
andα ≥ 0foralli. Thatis, v isaconvexcombinationofthevectorsw . LetVar(v)denotethe
i i
varianceofthiscategoricaldistribution. Undertheseconditions,thevarianceofvsatisfies:
n
(cid:88)
Var(v)≤ α Var(w )≤maxVar(w ).
i i i
i
i=1
Proof. Recallthedefinitionofvarianceforavectorv∈Rk isgivenby:
k
1 (cid:88)
Var(v)= (v −v¯)2,
k j
j=1
where v is the j-th component of v and v¯ is the mean of the components of v. Given v =
j
(cid:80)n
α w ,eachcomponentv ofvcanbeexpressedas:
i=1 i i j
n
(cid:88)
v = α w .
j i ij
i=1
Themeanv¯ofthecomponentsofvcanbecalculatedas:
k k n
1 (cid:88) 1 (cid:88)(cid:88)
v¯= v = α w .
k j k i ij
j=1 j=1i=1
Sinceeachw sumsto1,itfollowsthat:
i
 
n k n
(cid:88) 1 (cid:88) (cid:88)
v¯= α i
k
w ij= α iw¯ i.
i=1 j=1 i=1
Now,substitutingbackintothevarianceformula,weget:
k (cid:32) n (cid:33)2
1 (cid:88) (cid:88)
Var(v)= α (w −w¯) .
k i ij i
j=1 i=1
ApplyingtheAM-RMSinequality:
(cid:32) n (cid:33)2 (cid:32) n (cid:33)
(cid:88) (cid:88)
α (w −w¯) ≤ α (w −w¯)2 ,
i ij i i ij i
i=1 i=1
andsinceα2 ≤α ∈[0,1],wecansimplifythevarianceexpression:
i i
n k
(cid:88) 1 (cid:88)
Var(v)≤ α (w −w¯)2.
ik ij i
i=1 j=1
Finally,as 1 (cid:80)k (w −w¯)2istheexpressionforthevarianceofw ,weconclude:
k j=1 ij i i
20n
(cid:88)
Var(v)≤ α Var(w )≤maxVar(w ).
i i i
i
i=1
Now,wediscusstheproofofTheorem2.
Since(ϕ∗(.),M∗)satisfiestheweakoverlapAssumption4forη = 1,weknowthat∃x∗ suchthat
ϕ∗(x∗)=1foralli,1≤i≤n (becauseϕ(x)=P(Z |X =x)). Therefore,weknowthat
i z
(cid:88)
∀i,∃x∗suchthatP(S |X =x∗)= ϕ∗(x∗)M∗ =M∗ .
z z,: i,:
z
Moreover,foranyvalidϕ (x∗),
z
(cid:88)
P(S |X =x∗)= ϕ (x∗)M .
z z,:
z
Therefore,foralli,
(cid:88)
M∗ = ϕ (x∗)M .
i,: z z,:
z
Hence,usingLemma1,foralliwehave
Var(M∗)≤maxVar(M ).
i,: z,:
z
Hence,
maxVar(M∗ )≤maxVar(M ).
z,: z,:
z z
SinceM z∗ ,:andM
z,:
∈Rns,themeanoftheseis n1 s. Hence,
(cid:32)
1
(cid:88)ns
1
(cid:33)
maxVar(M )=L (M)= max (M − )2 ,
z z,: var z∈{1,...,nz} n s
s=1
s,z n s
(cid:32)
1
(cid:88)ns
1
(cid:33)
maxVar(M∗ )=L (M∗)= max (M∗ − )2 .
z z,: var z∈{1,...,nz} n s
s=1
s,z n s
Therefore,
L (M∗)≤L (M).
var var
B DETAILED DISCUSSION ON ASSUMPTIONS
In this section, we comprehensively discuss our assumptions and compare them to those in our
closestrelatedwork,ProxyDomainAdaptation(ProxyDA)(Tsaietal.,2024). Wethenexaminethe
practicalityoftheseassumptions.NotethatProxyDAoperatesundertwosettings:onewithmultiple
sourcesandaproxyvariable,andtheotherwithaconceptvariableandaproxyvariable. Sinceour
modelisdesignedtoworkwitheithermultiplesourcesoraproxyvariable,wewillonlycompareit
withtheformersetting.
Our assumptions (i) In real-world tasks, test data is rarely available during training and train
data is rarely available during inference. Hence, the OOD task we consider is more realistic and
challenging. (ii) We only require U (multiple training sources) or S, a proxy for the confounder,
not both simultaneously. Additionally, we do not require a proxy at test time, relaxing previous
assumptions. Moreover,forthemulti-sourcetask,onlyoneofthesourcesneedstobelabeled. (iii)
UnlikeProxyDA,weallowforabi-directionalarrowbetweenX andY inourcausaldiagram. The
causalrelationshipbetweenXandY canvarywidelybasedontheproblem.Forexample,inahouse
pricepredictionproblem,itislikelythatX → Y sincefeaturesofthehouselikelycausallyaffect
thepriceofthehouse. However, inimageclassificationproblems, itiscommontohaveY → X,
where the category of the object causes the distribution of pixels in the image. Furthermore, in
21Assumptions Ours ProxyDomainAdaptation
Task Out-of-distribution DomainAdaptation
Observedvariables X,Y and(UorS) X,Y,UandS
Numberoflabeledsources 1 multiple(≥nz)
Conditionalindependence S⊥X,Y |Z(orU⊥X,Y |Z) U⊥X,Y,S|ZandS⊥X|Z
CausalitybetweenXandY X↔Y X→Y
Fullrank P(S|Z)(orP(U|Z))mustbefullrank P(S|Z)mustbefullrankandP(Z|U,X)mustbefullrankforX
Discreteunobservedconfounder Yes Yes
Informativevariable No Yes
Existenceofhigh-likelihoodsample Yes No
Proxyavailableattesttime No Yes
ns≥nz Yes Yes
nzknown No No
Table3: ComparisonofAssumptionsbetweenOursandTsaietal.(2024)
many cases, some features in X might cause Y and others may be effects of Y. For example, in
ourincomepredictiontask,itislikelythateducationcausesincomebuthealthinsurancecoverage
iscausedbyincome. BotharefeaturesinX. Therefore,allowingabidirectionalcausalrelationship
betweenX andY increasesthescopeofourmethod. (iv)Weareslightlymorerestrictiveregarding
conditionalindependence. WhileProxyDAallowsS tocauseY, wedonotallowthatrelation. If
S causesY, wecannotallowY tocauseX. ThisisbecauseifS causesY andY causesX, then
S ⊥X |Z doesnothold. Therefore,thereisaninherenttrade-offbetween(iii)and(iv).
(v)Asfarasweknow,alllatentshiftmethods(Alabdulmohsinetal.,2023;Tsaietal.,2024)make
full rank assumptions. Full rank assumptions ensure diversity in distributions with respect to the
confounder and are necessary to prove identifiability. However, we do not require any full rank
conditiononP(Z |S,X)orP(Z |X)forallX,whichcouldbequitestrict. Forexample,thefull
rankassumptiononP(Z |S,X)forallX doesnotallowZ tobedeterministicforanyX.
(vi)Whiletheinformativevariableassumptioniscommonintheliterature(Miaoetal.,2018;Tsai
etal.,2024),wedonotrequirethisassumption.Thisassumptiondoesnotallowcertaindistributions,
forexample,thosewhereP(Z |X)cantakeonlyafinitesetofvalues.
Oneofthekeyassumptionswemake,whichotherpapersdonot, istheweakoverlapassumption.
OurapproximateidentifiabilityresultinTheorem1showsthatwecanboundthedifferencebetween
theposteriorsofanytwopossibledistributionsbyO( 1−η ). Proposition1showsthatAssumption
2η−1
4wouldholdforanηarbitrarilycloseto1forhigh-dimensionaldata. Therefore,giventheobserved
distribution P(X,S), we can approximate the latent distribution by an arbitrarily small error with
theincreaseindimensionofX.
C EXPERIMENTAL DETAILS
In this section, we provide additional details on our dataset construction, baselines, model imple-
mentation,andmodelselection.
C.1 SYNTHETICDATA
ProxyOODTask FortheproxyOODtask,wedefinethelatentconfounderZ ∈{0,1,2}andthe
proxyS ∈ {0,1,2}withX ∈ R3 astheobservedfeaturevector. TheconditionaldistributionofS
givenZ,denotedasP(S |Z),ismodeledwithdifferentprobabilitiesdependingonthevalueofZ.
Specifically,theconditionalprobabilitiesforeachvalueofZ aregivenby:
P(S =0|Z =0)=0.7, P(S =1|Z =0)=0.3,
P(S =0|Z =1)=0.8, P(S =1|Z =1)=0.1, P(S =2|Z =1)=0.1,
P(S =0|Z =2)=0.1, P(S =2|Z =2)=0.9.
ThefeaturevectorX =(X ,X ,X )isgeneratedconditionallyonZ asfollows:
1 2 3
X =−Z +Z+ϵ , X =2Z +ϵ , X =3Z +ϵ ,
1 x 1 2 x 2 3 x 3
where Z ∼ N(0,1) is a Gaussian random variable, and ϵ ∼ N(0,0.2), ϵ ∼ N(0,1), ϵ ∼
x 1 2 3
N(0,1)arenoiseterms.
22Finally,thebinarytargetvariableY isgeneratedasaBernoullirandomvariablebasedonalogistic
functionofX andZ. TheconditionalprobabilityofY givenX andZ is:
P(Y =1|X,Z)=σ(f(X,Z)),
whereσ(·)isthesigmoidfunctionandtheargumentprob(X,Z)is:
(cid:26)
−0.6X +2.7X +0.6X −0.6Z+1.5 ifZ =0,
f(X,Z)= 1 2 3
1.5X +(1.2−6Z)X +2.1X −0.6Z+1.5 ifZ ̸=0.
1 2 3
Forthetrainingset,thelatentvariableZ issampledfromamultinomialdistributionwithprobabili-
tiesP(Z)=[0.15,0.35,0.5]. Forthetestset,theprobabilitiesareshiftedtoP(Z)=[0.7,0.2,0.1]
tosimulatedistributionshift.
Multi-source OOD Task For the multi-source OOD task, we also define Z ∈ {0,1,2} as the
latentconfounderandX ∈R3asthefeaturevector. TheconditionaldistributionP(X |Z)follows
thesamestructureasintheproxyOODtask.
ThedifferenceliesinhowthetrainingandtestdistributionsforZ aredefined. Duringtraining,we
usemultiplesourcedomains,eachwithdifferentdistributionsofZ. Forexample,wesamplefrom
threesourcedomainswithdistributions:
P(Z)=[0,0.7,0.3], P(Z)=[0.8,0.1,0.1], P(Z)=[0.1,0.0,0.9].
Inthetestdomain,thedistributionshiftsto:
P(Z)=[0.7,0.2,0.1].
ThetargetvariableY isgeneratedsimilarlytotheproxyOODtask,withP(Y | X,Z)followinga
logisticfunctionbasedonX andZ.
C.2 REALDATASETS
WeconstructourdatasetsusingthefolktablesPythonpackage(Dingetal.,2021). Below,we
providedetailedinformationabouteachdatasetandtherationalebehindoursplits.
ACSEmployment The ACSEmployment dataset is used to predict whether an individual is em-
ployed.Disabilitystatusisthelatentconfounder(Z),whereZ =1denotesindividualswithdisabil-
ities, andZ = 0denotesindividualswithoutdisabilities. Thissetupmodelsreal-worldconditions
where disabled individuals might face different employment opportunities and challenges. Public
insurancecoverageandindependentlivingstatusserveasproxiesfordisabilitystatus. Inthetrain-
ingset,Ptr(Z = 0) = 0.95andPtr(Z = 1) = 0.05,whileinthetestset,Pte(Z = 0) = 0.05and
Pte(Z =1)=0.95. Boththetrainingandtestsetscontain170,000sampleseach. Theinputhas54
features.
ACSIncome TheACSIncomedatasetisusedtopredictanindividual’sincome. Thestateofresi-
dence(Z)isthelatentconfounder. Wecreatedthreesyntheticsplitsfortraining:
• 20%PuertoRico(PR)and80%California(CA)
• 20%SouthDakota(SD)and80%California(CA)
• 20%PR,20%SD,and60%CA
Testingisperformedonasplitof90%PR,5%SD,and5%CA.Thetrainingsetcomprises10,502
samples,whilethetestsetcontains3,481samples. Theinputhas76features.
TocreatesignificantshiftsinP(Y | X)acrosssubpopulations,wefirsttrainedaclassifierondata
from California and tested it on data from all other states. Additionally, we trained a classifier
for each state and tested it within the same state. We selected Puerto Rico and South Dakota as
theyexhibitedthelargestaccuracydisparitiescomparedtoCalifornia. Thismethodsimulatesreal-
worldscenarioswhereeconomicconditionsandjobmarketsvarysignificantlyacrossstates,thereby
affectingincomelevels.
23C.3 IMPLEMENTATIONDETAILS
Thetrainingwasperformedonan11thGenIntel(R)Core(TM)i7-11390HCPUandonanIntel(R)
Xeon(R)Gold6230CPU@2.10GHz. Theexperimentstookapproximately48hourstorun. Our
architecture employs a two-hidden-layer neural network for ϕ(·). Furthermore, we use a Batch
Normalizationlayerbeforethesoftmaxlayertostabilizetraining.
WeimposeaconstraintonthematrixM suchthatthesumofthevaluesineachrowequals1,andwe
clipallvaluestoremainwithintherangeof0to1. InTheorem2,weshowhowencouragingdistri-
butionswithlowvarianceleadstorecoveringtheground-truthlatentdistribution.Theregularization
parameterλistunedonthevalidationsetfrom{1e-3,1e-2,1e-1,1e0,1e1}.
Foroptimization, weutilize theAdamoptimizer witha learningrateof 1×10−3. While training
ϕ(·),werunthemodelfor100epochsandperformearlystoppingbasedonaheld-outvalidationset
fromthesamedistributionasthetrainingset.
Forthemixtureofexpertsmodel,wetrainthemodelfor25epochswithearlystoppingbasedona
validationsetfromthesamedistributionasthetrainingset.
Baselines We implement GroupDRO (Sagawa et al., 2020), IRM (Arjovsky et al., 2019), V-
REx (Krueger et al., 2021), DeepCORAL (Sun & Saenko, 2016), and DANN (Ganin et al., 2016)
using the DomainBed code (Gulrajani & Lopez-Paz, 2020). We use early stopping based on a
validationsetfor allthesebaselines. WeimplementProxyDA (Tsai etal.,2024)usingtheir pub-
licly available GitHub repository. For V-REx, IRM, and DeepCORAL, we tune the regularization
penaltyfrom{1e-2,1e-1,1e0,1e1}. ForGroupDRO,wetunethegroup-stepsizefrom{1e-2,1e-1,
1e0, 1e1}. For ProxyDA, we used random noise for the other variable. We subsampled all train-
ing datasets on the real tasks to 5000 samples respectively due to memory constraints. We tuned
threehyperparameters: cme,m0,andscaleusingavalidationsetfromthesamedistributionasthe
trainingset, withintherange{1e-4, 1e-3, 1e-2, 1e-1, 1e0}. FortheACSEmploymentdataset, we
transformed the data into a lower dimension using Gaussian Random Projection as suggested by
Tsaietal.(2024),tuningthenumberofdimensionsfrom{2,4,8,16}.
24