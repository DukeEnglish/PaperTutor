Transfer Learning for High-dimensional Quantile
Regression with Distribution Shift
Ruiqi Bai1 Yijiao Zhang1,∗ Hanbo Yang2 Zhongyi Zhu1,∗
1Department of Statistics and Data Science, Fudan University
2School of the Gifted Young, University of Science and Technology of China
December 2, 2024
Abstract
Information from related source studies can often enhance the findings of a target
study. However, the distribution shift between target and source studies can severely
impact the efficiency of knowledge transfer. In the high-dimensional regression set-
ting, existing transfer approaches mainly focus on the parameter shift. In this paper,
we focus on the high-dimensional quantile regression with knowledge transfer under
three types of distribution shift: parameter shift, covariate shift, and residual shift.
We propose a novel transferable set and a new transfer framework to address the
above three discrepancies. Non-asymptotic estimation error bounds and source de-
tection consistency are established to validate the availability and superiority of our
method in the presence of distribution shift. Additionally, an orthogonal debiased ap-
proach is proposed for statistical inference with knowledge transfer, leading to sharper
asymptotic results. Extensive simulation results as well as real data applications fur-
ther demonstrate the effectiveness of our proposed procedure.
Keywords: transferlearning; high-dimensionalquantileregression; distributionshift;
transferable set; orthogonal debiasing
∗Corresponding authors
1
4202
voN
92
]EM.tats[
1v33991.1142:viXra1 Introduction
Previousexperiencescanoffervaluableinsights forlearningnewtasks. Transfer learn-
ing, the technique of improving the performance of target learners on target domains
by leveraging knowledge from different but related source domains (Zhuang et al.,
2021), has shown significant success across various application scenarios. These ap-
plications range from machine learning problems such as natural language processing
(Ruder et al., 2019) and computer vision (Shao et al., 2015), to science problems such
as protein representation (Fenoy et al., 2022) and drug discovery (Turki et al., 2017),
owing to the increasing richness of data sources. However, our understanding of
transfer learning from a statistical perspective remains incomplete, with the funda-
mental challenge being the detection and utilization of valuable information within
heterogeneous source studies.
In this paper, we focus on the transfer learning for quantile regression in the
context of high-dimensional setting. Quantile regression (QR) models the impact of
covariates on the conditional response distribution and has achieved great success in
many applications. For QR in various areas, see Koenker (2017) for a detailed review.
One of QR’s most appealing features is its ability to describe the impact of covariates
not only on the mean but also on the tails of the response distribution with robustness
to heteroscedastic errors. Given potential non-negligible divergence between target
and source distributions, QR can provide a more comprehensive assessment of what
can be transferred from the source studies. Moreover, we focus on the regime of
high-dimensional models, since the error bounds of high-dimensional estimators are
heavily limited by the sample size. This motivates us to explore multiple external
datasets to improve the performance of high-dimensional estimators on the target
task.
To set up the framework, suppose we have a target dataset ((x(0) ) ,y(0) )n 0 and
i ⊤ i i=1
(0)
the parameter of interest is the quantile of the response y conditional on covariates
i
x(0) Rp ata givenquantile levelτ (0,1), denotedby β∗ (sometimes we usew(0) for
i
∈ ∈
unity). Also, we have K independent source datasets ((x(k) ) ,y(k) )nk K , where
i ⊤ i i=1 k=1
{ }
the corresponding quantile coefficients are denoted by w(k),k = 1,...,K. Denote
the positive covariance matrix Σ(k) := E [x(k) (x(k) ) ] for each study. In the high-
i i ⊤
dimensional setting, the number of covariates p is very large, possibly larger than the
2overall sample size N := K n . Assume a linear QR model, which is
k=0 k
P
y(k) = (x(k) ) w(k) +ǫ(k) , i = 1,...,n , k = 0,...,K, (1)
i i ⊤ i k
where each model residual ǫ(k) satisfies P (ǫ(k) 0 x(k) ) = τ with conditional density
i i i
≤ |
function f(k)( x(k) ). Assume the target parameter β∗ is sparse, where the number of
i
·|
∗
non-zero coefficients satisfies β = s min n ,p . Also, we define the contrast
0 0
k k ≪ { }
vector δ(k) := w(k) β∗ (obviously δ(0) = 0). Our goal is to estimate the target
−
∗
parameter β , with the help of abundant external data.
Three types of distribution shift. For the QR model (1) specified on all
studies, there may exist three types of distribution shifts between the target and
source domains:
1. parameter shift: β∗ = w(k), i.e., the model coefficients differ.
6
(0) (0) (k) (k)
2. residual shift: P(ǫ x ) = P(ǫ x ), i.e., the conditional distributions of
i i i i
| 6 |
model residuals differ.
(0) (k)
3. covariate shift: P(x ) = P(x ), i.e., the marginal covariate distributions dif-
i i
6
fer.
Both parameter shift and residual shift can be seen as the posterior/label shift, mean-
(0) (0)
ing that the conditional distribution of responses differs between P(y x ) and
i i
|
(k) (k)
P(y x ). Many examples have revealed that transferring source studies under
i i
|
distribution shift may pose challenges to the final performance and lead to the neg-
ative transfer (Wang et al., 2019; Zhang et al., 2022). Hence, the core issue of this
paper is:
How to identify the transferable information from different source studies under
distribution (covariate/parameter/residual) shift? How to conduct valid prediction
and inference for high-dimensional quantile regression with knowledge transfer?
Methodologically, transfer learning falls within the field of multi-source data anal-
ysis, a prominent topic in contemporary statistics. Numerous approaches have been
developedformulti-sourcesettings, varyingbytasktype, datacategory, andotherfac-
tors. These include federated learning (McMahan et al., 2017), distributed learning
(Jordan et al.,2019;Fan et al.,2023), fusionlearning(Shen et al.,2020;Li and Luedtke,
32023), multi-tasklearning(Duan and Wang,2023;Knight and Duan,2023), andtrans-
fer learning (Pan and Yang, 2009; Zhuang et al., 2021), each with its unique charac-
teristics and structures. For transfer learning, it aims to leverage information from
source domains to improve the performance of the target task, with the core challenge
being how to extract valuable source information while addressing data heterogene-
ity. Transfer learning has been studied across a variety of contexts, including linear
regression (Li et al., 2022), generalized linear regression (Tian and Feng, 2023), non-
parametric classification (Cai and Wei, 2021), nonparametric regression (Cai and Pu,
2024), graphical models (Li et al., 2023), and causal inference (Wei et al., 2023).
The main focus of this paper lies on the transfer learning for high-dimensional
linear models, where the goal is estimation and inference for the target parameter.
For parameter estimation, a seminal and widely adopted approach is the two-step
framework introduced in Li et al. (2022). In this method, the oracle transferable
set A := δ(k) h is defined, where source studies with a small contrast
h 1
{k k ≤ }
of parameters are considered informative. The two-step procedure first combines
the data from A to obtain a pooled estimate, then debiases this estimate using
h
the target data. Tian and Feng (2023) then extended this approach to generalized
linear regression, providing a consistent source detection algorithm based on cross-
validated loss comparison. Huang et al. (2023) and Zhang and Zhu (2023) further de-
veloped direct extensions of the two-step framework and source detection procedure
for high-dimensional quantile regression and smoothed quantile regression, respec-
tively. Among the limited literature for statistical inference with knowledge transfer,
Tian and Feng (2023), Li et al. (2024), and Huang et al. (2023) applied the one-step
correction technique (Zhang and Zhang, 2014; van de Geer et al., 2014) to debias the
previous transfer estimate on the target study.
As defined by the transferable set A , the two-step approach only addresses pa-
h
rameter shift, leaving it susceptible to covariate and residual shifts across multiple
sources. However, studies in machine learning have shown that it is essential to ac-
count for both covariate and label shifts in heterogeneous data settings, such as image
recognition Park et al. (2023) and disease diagnosis Chen et al. (2022). Additionally,
due to these shifts, prior debiased transfer estimators achieved only √n -normality,
0
providing no improvement over the original target baseline. To reduce the impact
of covariate shift, Li et al. (2024) proposed jointly estimating the w(k) of each study
using ℓ -minimization, Zhao et al. (2023) utilized a residual importance-weighted al-
1
4gorithm to make better use of source observations, and He et al. (2024) added a
fused-regularizer to the pooling step of the two-step approach. While these works
addressed covariate shift and introduced remedies, none of them account for resid-
ual shift. However, several studies suggest that addressing complex label shift issues
requires a distributional approach, such as density estimation (Lipton et al., 2018;
Garg et al., 2020; Lee et al., 2024), rather than simply assuming a linear relationship
and comparing coefficients. In our context of quantile regression, the distributional
information around each quantile differs across sources, making it essential to handle
residual shift to accurately capture the information at each quantile of the target
response. To demonstrate how severe residual shift can lead to negative transfer, we
provide a motivating example through simulations of existing transfer methods.
ThemotivatingexamplefollowsasimilarsetuptothesimulationinZhang and Zhu
∗ (0)
(2023). We set p = 500,s = 10,n = 200,β = (0.5 ,0 ) ,x (0 ,Σ )
0 s p s ⊤ i p x
− ∼ N
with Σ = (0.7i j ) . We consider only one source study with sample size
x | − | 1 i,j p
≤ ≤
n varying from 100 to 500, and we assume no parameter or covariate shift, i.e.,
1
δ(1) = 0,x(1) (0 ,Σ ). The key change lies in the residual distributions,
i p x
∼ N
(0) (1)
where ǫ is standard normal, but ǫ follows one of the four types of distributions:
i i
1) standard normal: (0,1); 2) Cauchy distribution: (0,3); 3) mixed Gaussian:
N C
z ( 3,0.5) + (1 z) (3,0.5), z ernoulli(τ); 4) noisy normal: (0,52). All
N − − N ∼ B N
residuals are generated with necessary translational adjustments for the τ-th quantile
at zero.
It is worth noting that in this setting, the source study is included in the ora-
cle transferable set by the previous definition (Li et al., 2022; Tian and Feng, 2023).
∗
We compare four methods for estimating the target parameter β at 0.2-th quantile,
which are: 1) L1-QR: ℓ -penalized quantile regression (Belloni and Chernozhukov,
1
2011) on the target data; 2) Pooling: ℓ -penalized quantile regression on the entire
1
dataset; 3) TransQR 2step: two-step transfer for quantile regression (Huang et al.,
2023); 4)TransSQR 2step: two-steptransferforsmoothedquantileregression(Zhang and Zhu,
2023). We repeat these methods 50 times and report the average of ℓ -estimation er-
2
rors in Figure 1.1.
As shown in Figure 1.1, it is clear that, apart from the normal setting which
demonstrates the anticipated transfer benefit, all other settings reveal negative trans-
fer effects. Pooling, TransQR 2step, and TransSQR 2step perform significantly worse
than the baseline L1-QR on the target data, strongly suggesting that assessing in-
5(1) Normal (2) Cauchy
1.2
0.3
1.0
0.8
0.2
0.6
Method
0.1 0.4
L1−QR
100 200 300 400 500 100 200 300 400 500
Pooling
(3) Mixed (4) Noisy
TransQR_2step
1.0
TransSQR_2step
1.5
0.8
1.0
0.6
0.5
0.4
100 200 300 400 500 100 200 300 400 500
Sample Size of Source Study n
1
Figure 1.1: Average ℓ -estimation errors of four methods under residual shift
2
formation transferability based solely on parameter similarity is insufficient in cases
involving residual shift. This observation raises an interesting question: how can
we effectively measure and control residual shift? Note that the adverse impact of
residual shift fundamentally arises from information loss around the target quantile.
So, what constitutes a meaningful representation of local distributional information?
The answer is density. Motivated by this, we propose a novel transferable set:
= k : 1 k K, δ(k) h ,E [f(0)(0 x(0) )] h E [f(k)(0 x(k) )] ,
h 1 1 i 2 i
C { ≤ ≤ k k ≤ | ≤ | }
where h ,h represent oracle yet unknown transferability levels. This set is designed
1 2
to filter out source studies characterized by large parameter discrepancies and small
densities at the τ-th quantile, thus enabling the simultaneous management of pa-
rameter and residual shifts. We will further elaborate on the rationale behind the
construction in Section 2. Using this transferable set, we develop a whole transfer
learning framework to address parameter, covariate, and residual shifts concurrently
within the high-dimensional quantile regression domain. Our contributions are:
• We introduce the concept of residual shift in the multi-source setting and define a
new transferable set to handle both parameter and residual shifts, offering deeper
6
rorrE
L
2insights into managing distribution shift under knowledge transfer. For param-
eter estimation, we extend the constrained ℓ -minimization algorithm (Li et al.,
1
2024) to quantile regression, which remains robust in the presence of covariate
shift. Furthermore, we develop a corresponding source detection procedure and
integrate it into a unified one-shot estimation algorithm.
• In theory, we derive non-asymptotic ℓ /ℓ -error bounds for our transfer estima-
1 2
tors, broadening existing results to account for multiple types of distribution
shifts, not just parameter shift. The resulting convergence rate surpasses that of
single-task error bounds (Belloni and Chernozhukov, 2011; Wang and He, 2024),
underscoring the advantage of our approach, especially when divergent sources
are involved. The detection consistency of our source screening procedure is also
established.
• A debiased transfer approach based on Neyman’s orthogonality (Belloni et al.,
2019) is proposed for statistical inference on quantile coefficients with knowledge
transfer. Our method avoids the estimation of high-dimensional precision ma-
trices and allows non-identical data distributions, which helps to alleviate the
trouble by covariate and residual shift under knowledge transfer. To the best of
our knowledge, our method is the first to debias on informative sources to en-
hance inference efficiency. By integrating valuable source studies, our approach
achieves sharper asymptotic rate than previous results, leading to more efficient
debiased estimates and shorter confidence intervals.
The rest of the article is organized as follows. In Section 2, we propose our transfer
learning approach under distribution shift. We will introduce our transferable set and
transfer framework. Non-asymptotic error bounds are revealed subsequently. Section
3 describes the source detection procedure, accompanied by consistency guarantees.
In Section 4, we present the orthogonal debiased approach for statistical inference
with knowledge transfer, along with the corresponding asymptotic theory. Section 5
reports simulation results on parameterestimation and statistical inference, highlight-
ing the advantages of our methods. Finally, in Section 6 we apply our methods to the
Genotype-Tissue Expression (GTEx) dataset, predicting brain tissue gene expression
levels by leveraging information from other tissues.
72 Transfer Learning under Distribution Shift
We begin this section with some necessary notations. For two sequences of positive
numbers a and b , we write a . b if a cb for some universal constant
n n n n n n
{ } { } ≤
c (0, ), and a & b if a cb for some universal constant c (0, ). We say
n n n ′ n ′
∈ ∞ ≥ ∈ ∞
a b if a . b and a & b . For any q [0, ] and vector x = (x , ,x ) Rp,
n n n n n n 1 p
≍ ∈ ∞ ··· ∈
we write x for its l -norm. Let a b and a b denote the minimum and maximum
q q
k k ∧ ∨
of a and b, respectively. For a vector v Rp and a subset S 1,2,...,p , we use
∈ ⊆ { }
v to denote the restriction of vector v to the index set S. The sub-Gaussian norm
S
of a random variable u R is u = inf t > 0 : E [exp(u2/t2)] 2 and the sub-
ψ
∈ k k 2 { ≤ }
Gaussian norm of a random vector U Rn is U = sup U,v .
∈ k kψ 2 v 2=1,v Rn kh ikψ 2
k k ∈
The sub-exponential norm of a random variable u R is u = inf t > 0 :
ψ
∈ k k 1 {
E [ u /t] 2 and the sub-exponential norm of a random vector U Rn is U =
ψ
| | ≤ } ∈ k k 1
sup U,v . The expectation symbol E [( )(k) ] := E [( )(k) ] means
kv k2=1,v ∈Rn kh ikψ 1 · i (x( ik),y i(k)) · i
(k) (k)
obtaining expectation with the joint distribution P(x ,y ).
i i
2.1 Methodology of Tackling Three Types of Distribution
Shift
Parameter Shift. Naturally we should consider a source study as informative if
its parameter contrast δ(k) is relatively small. We choose the ℓ -sparsity to quantify
1
the informative level, as in many practical applications, parameter shift typically
occurs across multiple dimensions without the overall magnitude growing too fast.
Formally, the screening criterion is to reject source studies with δ(k) larger than
1
k k
an oracle transferring level, which has been widely adopted in the previous transfer
work (Li et al., 2022; Tian and Feng, 2023).
Residual Shift. Let’s start from the simplest low-dimensional linear model to
understand how to tackle the residual shift. Suppose the target and source task as
target model: y(0) = (x(0) ) β∗ +ǫ(0) , i = 1,...,n, ǫ(0) (0,σ2),
i i ⊤ i i 0
∼ N
 source model: y(1) = (x(1) ) β∗ +ǫ(1) , i = 1,...,n, ǫ(1) (0,σ2).
 i i ⊤ i i 1
∼ N
Denotethe
covariance matrix Σ :=
E [x(0) (x(0)
) ] =
E [x(1) (x(1)
) ]. Based on the
x i i ⊤ i i ⊤
knowledge of linear models, the target and pooling OLS estimator β ,β satisfy:
tar pool
β β∗ 0, σ 02 Σ 1 , β β∗ 0, σ 02 +bσ 12 Σb 1 . (2)
tar − ∼ N n −x ! pool − ∼ N 4n −x !
(cid:16) (cid:17) (cid:16) (cid:17)
b b
8If σ2 is much larger than σ2, i.e., the source study is much more noisy, then the
1 0
pooling estimator will be much less efficient compared to the target one. From this
perspective, addressing the residual shift is actually to avoid the increasing noise
caused by incorporating divergent source studies.
Moving to the low-dimensional quantile regression with the same setting, we uti-
lize the traditional quantile loss ρ (x) := x(τ I (x 0)) on the target data and
τ
− ≤
the pooling data respectively. Based on the large sample theory of M-estimators
(Van der Vaart, 2000) and necessary regular conditions, it is easy to verify that
τ(1 τ) 2τ(1 τ)
β β∗ d 0, − Σ−1 , β β∗ d 0, − Σ−1 ,
tar − −→ N n(f(0)(0))2 x pool− −→ N n(f(0)(0)+f(1)(0))2 x
(cid:16) (cid:17) (cid:18) (cid:19) (cid:16) (cid:17) (cid:18) (cid:19)
b b
where f(0)( ),f(1)( ) are density functions of residuals ǫ(0) ,ǫ(1) respectively. When
i i
· ·
f (0) (√2 1)f (0), β is less efficient than β . It can be seen that the
1 0 pool tar
≤ −
residual density values f(0)(0) and f(1)(0) serve a similar role for QR estimators as
b b
residual variances σ2 and σ2 do for OLS estimators, both indicating the quality of
0 1
information available for estimation. In fact, this relationship was evident in our
motivating example, where unreliable source studies had considerably lower density
values around the τ-th quantile compared to the target study. When a source study
has a very low density near the τ-th quantile, its QR estimator often performs poorly
due to the noisy information, making it unlikely that transferring from such a study
would be beneficial. With this in mind, we propose another screening criterion to
discard source studies with residual densities below a specified oracle threshold, as
they are unlikely to contribute valuable information beyond introducing noise. For
simplicity, we denote f(k) := f(k)(0 x(k) ) as the conditional density of residual ǫ(k) at
i i i
|
zero, hereinafter referred to as the τ-th density.
(k)
Remark 1. Rejecting sources with lower τ-th density f , i.e., leveraging sources
i
with higher τ-th density, can also be explained from the linear regression example (2).
If a source study has a negligible error term, i.e., σ2 = 0, then it means accurately
1
identifying the target parameter, leading to a wonderful transferable study. That is
exactly why we choose this measure in the transferable set to address the residual shift.
Additionally, with balanced study sizes, minimizing the estimator’s mean square error
(MSE) benefits from a smaller contrast δ(k) to reduce bias, while a larger τ-th
1
k k
density helps to control variance. We will illustrate this point through theoretical
results later in this section.
9Transferable Set. From the analysis above, we need to control the transferring
level for both parameter contrast δ(k) and τ-th density f(k)(0 x(k) ), in order to
1 i
k k |
achieve efficient knowledge transfer in the presence of parameter and residual shift.
Hence we propose the oracle transferable set as defined in the introduction:
h
C
E [f(0)
]
= k : 1 k K, δ(k) h , i h , (3)
Ch  ≤ ≤ k k1 ≤ 1 E [f(k) ] ≤ 2 
 i 
where h ,h are oracle transferring levels revealing the magnitude of parameter and
1 2
residual shifts respectively. One level h controls the parameter similarity, while
1
the other level h regulates the expectation of source τ-th density E [f(k) ] compared
2 i
to the target
E [f(0)
], facilitating the exclusion of heterogeneous source studies. Both
i
requirements in the new transferable set (3) are actually elucidated in the earlier text.
To guarantee the improvement of transfer learning, we only leverage the information
from source studies included in this set . Strictly speaking, both transferring levels
h
C
should be related to the sample size, since the impact caused by source studies differs
with varying sample sizes under the same degree of shift. However, the impact from
sample sizes is too hard to derive, where each level’s optimal choice depends on all
target and source studies’ characteristics, as shown in the theoretical analysis later.
Thus for simplicity, we set two universal oracle levels in (3) with implicitly assuming
balanced sample sizes.
Covariate Shift. The negative effect of covariate shift is mainly brought by
the divergence of covariance matrices Σ(k) between target and source studies. As
mentioned by He et al. (2024), in the two-step transfer approach with the transfer-
able set , the pooling estimator in the first step comes with an asymptotic bias
h
C
δ := ( Σ(k)) 1 Σ(k)δ(k). This bias can make the contrast δ(k) ampli-
Pool k h − k h
∈C ∈C
fied by thePfactor CΣ :=P1 +
|Ch
|max
k ∈Ch
sup
1 ≤j ≤p
ke
⊤j
(Σ(k)
−
Σ(0))(
k ∈Ch
Σ(k)) −1 k1,
which may diverge when Σ(k)s are dissimilar. To address this problemP, here we gener-
alize the constrained optimization algorithm proposed by Li et al. (2024) to the field
∗
of quantile regression, for jointly estimating the target parameter β and contrast
vectors δ(k) . Our transfer framework is formalized as follows:
{ }k ∈Ch
β, δ(k) = argmin λ β + λ δ(k) ,
(cid:18)
b
{
b
}k ∈Ch
(cid:19) β, {δ(k) }k∈Ch
β k k1
k X ∈Ch
k k k1
(4)
S (β +δ(k)) λ , for k 0 ,
subject to k
nk
k∞ ≤
k
∈ {
}∪Ch
 S (β +δ(k)) λ ,
 k k ∈{0 }∪Ch nk k∞ ≤ β
 P
10whereS (β) := ∂L(k)(β)/∂β isthesubgradientofthequantilelossfunctionL(k)(β) :=
nk
nk ρ (y(k) (x(k) ) β) for each study k 0 , λ , λ are tuning pa-
i=1 τ i − i ⊤ ∈ { }∪Ch β { k }k ∈{0 }∪Ch
Prameters. It is important to note that the ℓ -sparsity regularizer for each contrast δ(k)
1
allows usto befree from the similarity restriction of covariance matrices, hence remov-
ing the CΣ factor and achieving robustness under covariance heterogeneity (Li et al.,
2024; He et al., 2024). We will further reveal this property in the subsequent theo-
retical analysis.
Remark 2. Here we give some explanations for our transfer framework. The objec-
(k)
tive function in (4) encourages sparse solutions of β and δ . The constraint
{ }k ∈Ch
S (β) λ is inherited from the target model, imposing that β should be identi-
n 0 b b
k 0 k∞ ≤
fied as the true parameter β∗ in the target task. The constraint S (β+δ(k)) λ
k
nk
k∞ ≤
k
comes from the k-th source model, imposing that δ(k) should be identified as the true
contrast w(k) β∗ . The last constraint S (β + δ(k)) λ aggregates
− k k ∈{0 }∪Ch nk k∞ ≤ β
target study and studies in the oracle tranPsferable set , revealing that our estima-
h
C
tion leverages information from valuable sources. In contrast to the two-step transfer
framework (Li et al., 2022; Tian and Feng, 2023), our algorithm can simultaneously
estimate β∗ and δ(k), providing more information for the downstream inference task.
Remark 3. Zhao et al. (2023) proposed a residual importance weighted transfer ap-
proach to tackle distribution shift for high-dimensional linear regression. Specifically,
they set the oracle weights as ω =
f(0)(y(k) (x(k)
)
β∗
) /
f(k)(y(k) (x(k)
)
(β∗
+
k i i ⊤ i i ⊤
− −
δ(k))), then conduct the weighted LASSO to correctly identify β∗ from the pooled data.
Such weighting construction may still lose efficacy due to residual shift. If the source
is abnormal like the motivating example, then the corresponding weight will approach
infinity. However, we have shown that this source study ought to be discarded. There-
fore, their approach may not be applicable to scenarios involving residual shift in our
context.
To solve the constrained ℓ -minimization problem (4), we approximate (4) with
1
a joint ℓ -penalized quantile regression (ℓ -QR) form, leveraging the equivalence be-
1 1
tween Lasso and Dantzig selectors Candes and Tao (2007); Bickel et al. (2009):
β, δ(k) = argmin L(k)(β +δ(k))+λ β + λ δ(k) .
(cid:18)
b
{
b
}k ∈Ch
(cid:19) β, {δ(k) }k∈Ch

k ∈{X0 }∪Ch
β k k1
k X ∈Ch
k k k1 

 (5)
11This formulation is similar to the co-training step in He et al. (2024), with replacing
δ(k) by w(k) β. As suggested by Li et al. (2024), the algorithm for solving (5)
−
proceeds naturally using iterative alternating direction methods. At a high level, our
method works as follows. We first obtain an initial estimator β by ℓ -QR solely on
0 1
the target data (or the estimator β from the last iteration). Then for each source
t 1 b
−
(k) (k)
k we subtract (x ) β from the response y and obtain the estimator
h i ⊤ t 1 b i
∈ C
(k) − (k)
δ by ℓ -QR on the refined source dataset. After that, we take δ into
t 1 b { t }k ∈Ch
the objective function (5), solve for the estimator β , and begin the next round. The
b t b
iteration stops when estimators converge, where the detailed procedure is summarized
b
in Algorithm 1. We defer the practical choice of tuning parameters λ , λ
β { k }k ∈{0 }∪Ch
to the simulation section.
Algorithm 1: Parameter estimation with oracle transferable set
h
C
Input: Target data ((x(0) ) ,y(0) )n 0 , source data ((x(k) ) ,y(k) )nk ,
{ i ⊤ i i=1 } { i ⊤ i i=1 }k ∈Ch
tuning parameters λ , λ , iteration step T.
β { k }k ∈{0 }∪Ch
Compute the initial estimate β ℓ -QR on [((x(0) ) ,y(0) )n 0 ,λ ].
0 1 i ⊤ i i=1 0
←
for t = 1 : T do
b
1. Compute δ(k) ℓ -QR on [((x(k) ) ,y(k) (x(k) ) β )nk ,λ ],k ;
t 1 i ⊤ i i ⊤ t 1 i=1 k h
← − ∈ C
−
b b
2. Solve the pooled ℓ -QR problem
1
(k)
β = argmin L(k)(β +δ )+λ β .
t  t β k k1
β
k X0 h 
b ∈{ }∪C b
 
(k) (k)
Output: β, δ := β , δ .
{ { }k ∈Ch} { T { T }k ∈Ch}
b b b b
2.2 Theory on Estimation Error
∗
Now we study the non-asymptotic convergence rate of β β obtained by the transfer
−
approach (4) with transferable set , where is an arbitrary prespecified set for
b
C C
theoretical generality. Under the high-dimensional sparse setting, we characterize
the geometric structure of β β∗ and δ(k) δ(k) K through the restrictive sets
k=1
− { − }
(Belloni and Chernozhukov, 2011; Negahban et al., 2012; Wang and He, 2024), which
b b
12are denoted by
for β −β∗ with ℓ 0-sparsity: Γ H = {v ∈ Rp : kv Sc k1 ≤ kv S k1},
for δ(k) b δ(k) with ℓ -sparsity: Γ(k) = v Rp : v v +2 δ(k) ,
− 1 W { ∈ k Sa(k)c k1 ≤ k Sa(k) k1 k Sa(k)c k1 }
b
where S = j : β = 0,1 j p ,S(k) = j : δ(k) > a,1 j p with a
∗j a j
{ 6 ≤ ≤ } { | | ≤ ≤ }
threshold a > 0. The sets Γ(k) K depends on a, but we omit the dependence in
W k=1
{ }
notation for simplicity. β β∗ and δ(k) δ(k) K will be shown to lie in the sets
k=1
− { − }
Γ and Γ(k) K with high probability respectively in the supplementary materials.
H { W }k=1 b b
Conditions below constitute a set of basic assumptions for establishing the statistical
properties, where the index k refers to each study k 0,...,K .
∈ { }
(k) (k) (k)
Condition 1. The conditional density value at the τ-th quantile f = f (0 x ) is
i i i
|
positive, continuously differentiable, and uniformly upper bounded. Also, there exist
(k) (k)
a universal positive constant b such that the derivatives f (t x ) exist and are
0 i ′ i
|
uniformly bounded for t b .
0
| | ≤
Condition 2. There exist universal positive constants m ,m ,m such that
1 2 u
vTE [f(k) x(k) (x(k) ) ]v E [f(k) x(k) (x(k) ) ]v
min

i vi
2
i ⊤ , k i i
v
i ⊤ k∞
 ≥
m 1E [f i(k) ],
 k k2 k k∞ 
max vTE [f i(k) x v( ik) 2(x( ik) ) ⊤]v , kE [f i(k) x( ik v) (x( ik) ) ⊤]v k∞
 ≤
m 2E [f i(k) ],
 k k2 k k∞ 
 (k) 
for any non-zero vector v Γ (v Γ when k = 0). Also, the spectral norm of
∈ W ∈ H
covariance matrix satisfies λ (Σ(k)) m .
max u
≤
(k)
Condition 3. Each covariate x is a mean-zero sub-Gaussian random vector with
i
sub-Gaussian norm uniformly bounded by a positive constant m .
g
Condition 4. There exists a > 0 and corresponding index set S(k) such that δ(k) =
k ak
(k) (k) (k) (k)
δ +δ with δ s , δ η .
Sa(k k) Sa(k k)c k Sa(k k)k0 ≤ k k Sa(k k)c k1 ≤ k
Local boundness and smoothness of conditional density functions in Condition
(k)
1 are very familiar in QR literatures. Here a uniform lower bound of f is not
i
assumed, since we want to reveal the effect of residual shift in the error bounds.
Condition 2 assumes a modified version of restricted eignevalue condition (RSC) with
incorporatingtheτ-thdensityE [f(k)
].
ThereasonforplacingE [f(k)
]ontheright-hand
i i
13side is to technically derive error bounds involving each study’s τ-th density. Note
that if the study model is homogeneous, then this condition will return to the original
RSC form. Condition 3 needs covariates following sub-Gaussian distributions, which
relaxes theformer requirementofboundeddesign(Tian and Feng, 2023;Huang et al.,
2023). As for Condition 4, we impose an approximate sparse structure on δ(k) with
the sparse support S(k) to be different across source studies, which is also similarly
ak
assumed in Zhang and Zhu (2023) and Huang et al. (2023).
Theorem 1. (Convergence rate for β with known transferable set )
C
Assume that Conditions 1-4 is satisfied. Define n = n + n . Let β is obtained
b 0 k k
C ∈C
from (4) with transferable set . For each study k 0 P, let
b
C ∈ { }∪C
logp
λ = c n logp, λ = c n ,
β λ
q C
k λ kv
u un k ∧n 0
t
where c is a large enough constant. Let r = s logp/(n n )+η , if
λ k k k 0 k
∧
q
slogp s(logp)3/2 logp logp
+ +r = o (6)
n1 0/4 √n k n k kv u u√n k ∧n 0 v u un k ∧n 0
t t 
holds for each source study k , then for any constant ε > 0, with probability at
∈ C
least 1 ε ( +1)A exp( A logp), we have that
1 2
− − |C| −
n logp δ(k)
kβ −β∗
k2 ≤
C
1
√n Cs nlo Eg [p
f(k) ]
+C 2v
u
uPk ∈C k rnk n∧n E0k
[f(k)
]k1 , (7)
k 0 k i u k 0 k i
b ∈{ }∪C u ∈{ }∪C
t
P P
sn logp δ(k)
kβ −β∗ k1
≤
C 1 s√n C nlo Eg [p
f(k) ]
+C 2v u uPk ∈C k rn nk ∧ En 0 [k
f(k)
]k1 , (8)
k 0 k i u k 0 k i
b ∈{ }∪C u ∈{ }∪C
t
where A ,A ,C ,C are poPsitive constants and their dPetailed formation are shown in
1 2 1 2
the supplementary materials.
Remark 4. The assumption (6) is a technical requirementto bound the infinite norms
(k)
of subgradients in our proofs. Note that r is actually the ℓ -error bound of each δ
k 1
under soft sparsity. A sufficient condition for (6) is to let s6(logp)2 = o(n ) and
k b
1/4
r k = o(n k− ) for each k
∈
C, which is not harsh in the high-dimensional setting.
Compared to Theorem 1 in Li et al. (2024) under the similar transfer framework,
we relaxed their target size assumption n n , and our choice for λ frees the
0 β
|C| ≪ C
dependence on the unknown parameters s and h . Moreover, our analysis allows the
1
size of transferable set goes to infinity.
|C|
14∗
The ℓ /ℓ -error bounds of β β in (7)/(8) contain two parts. The first part
2 1
−
can be seen as the inherent estimation error slogp/n accompanied by the variance
b
C
term
nkE [f(k)
] from the residual
shq
ift. The second part represents the bias
k ∈{0
}∪C
nC i
term Pinvolving each contrast δ(k) from source’s parameter shift weighted by the
1
k k
sample size n , and also includes the variance term. The error bounds reveal that to
k
improve the transfer performance, source studies need to possess small δ(k) as well
1
k k
as large
E [f(k)
], strongly confirming the rationality of our screening criteria (3). Due
i
to the weighted summation form, the optimal informative level of δ(k) and E [f(k) ]
1 i
k k
is nearly impossible to derive, since the impact of each source depends on its sample
size proportion n /n and other sources’ shift level, let alone the implicit dynamic
k
C
choice of set . That’s why we offer the compromised universal levels h ,h in our
1 2
C
transferable set . For better understanding of our Theorem 1, here we give several
h
C
quantitative explanations under some simplified settings, taking the ℓ error bound
2
(7) as the example.
• (Parameter Shift) If all τ-th densities E [f(k) ] are neglected, then the result (7)
i
can be simplified to
slogp n logp
β β∗ . + k δ(k) .
k − k2 v n v u n vn n k k1
b u u t C u u tk X ∈C Cu u t k ∧ 0
Paralleltotheprevioustransfertheory, thefirstterm slogp/n enjoysasharper
C
q
rate than the single-task ℓ -error slogp/n (Belloni and Chernozhukov, 2011),
2 0
q
which indeed reflects the superiority of transfer learning. The second term is
a weighted bias term by parameter shift, and if n = O(n ),k , this term
0 k
∈ C
can be simplified to (logp/n )1/4 max δ(k) , corresponding to the result in
0 k 1
∈C k k
Theorem 1 of Li et al. (2024). Wq hen the contrast magnitude max δ(k) =
k 1
∈C k k
o(s logp/n ), i.e., smaller than the order of single-task ℓ -error, the transfer
0 1
q
estimator will improve the target task performance.
• (Residual Shift) To our knowledge, this non-asymptotic result containing τ-th
density
E [f(k)
] is new to the transfer literature, since we keep the density items
i
along our proving process. For the first term, we can give a simple example to
show that why residual shift could impact the transfer performance. Comparing
the single-task ℓ -error to the error (7) with only one source study, if the source
2
study has huge residual shift such that E [f(1) ] < √n 0(√n 0+n 1 −√n 0)E [f(0) ], then
i n i
1
15incorporating this source study will lead to negative transfer. In the second
term, for simplicity assuming n = O(n ),k , the error changes to
0 k
∈ C
logp n δ(k)
k k 1
v u uv u
u
n 0 · P
k
∈ 0C nk kE [fk i(k) ].
ut ∈{ }∪C
t
P
The first part logp/n is inherited from the target study, since we need the
0
target informatq ion to estimate each δ(k). The second part clearly reveals the
effect of both parameter/residual shift weighted by sample sizes, which again
corroborates the validity of the construction of our transferable set.
• (Covariate Shift) Our framework can relax the elusive conditions about the
covariance matrices across studies (Assumption 4 in Tian and Feng (2023), Con-
dition 4 in Zhang and Zhu (2023), Assumption 4 in Huang et al. (2023)). In
contrast, the two-step transfer framework may amplify the error bounds by a
factor CΣ, which may can diverge when the covariance matrices Σ(k),k are
∈ C
dissimilar to Σ(0). To put it succinctly, our results neither assume the similar-
ity of Σ(k), nor involve the factor CΣ mentioned above, hence achieving better
performance under covariate shift.
Corollary 1. Under Conditions 1-4 and assumptions in Theorem 1, take the oracle
transferable set = and suppose n = O(n ),k . Then for any constant ε > 0,
h 0 k h
C C ∈ C
with probability at least 1 ε ( +1)A exp( A logp), we can obtain that
h 1 2
− − |C | −
slogp logp
∗ 1
kβ
ora
−β
k2
.
v n
h
2
+(
n
)4 h 1h 2,
u h 0 q
u C
b t
As shown in Corollary 1, the terms h and h regulate the degree of parameter and
1 2
residual shift, respectively, striking a balance between performance improvement and
the risk of negative transfer. For the first term, a sufficient condition to avoid negative
transfer is to ensure that h = o( n /n ), which indicates that the minimum τ-th
2 h 0
C
density in the oracle transferable sq et should exceed n /n E [f(0) ]. The second term
0 h i
C
q
highlights the nuanced relationship between the parameter shift h (measured by
1
max δ(k) ) and the residual shift h (measured by max E [f(0) ]/E [f(k) ]), with
k ∈Ch k k1 2 k ∈Ch i i
a sufficient condition being h h = o(s logp/n ). This generalizes the previous result
1 2 0
(logp/n )1/4√h inLi et al.(2024), allq owingforbettererrorcontrolbymanagingboth
0 1
parameter and residual shift.
163 Transferable Set Detection
Since the oracle transferable set may be unknown in practice, here we propose a
h
C
detection procedure to screen out the source studies satisfying the requirements in
(3). We can first get an initial estimate β through ℓ -QR on the target study, then
1
remove the partaccountedbyβ on each sourcestudyand conductℓ -QR to obtainthe
e 1
(k) (k)
contrast estimators δ K . With contrast estimators δ K , screening studies
k=1 e k=1
{ } { }
with small δ(k) is relatively simple. We can directly calculate each δ(k) and set
1 e e 1
k k k k
a threshold to reject the dissimilar studies. As implied by Corollary 1, h should be no
1e
more than the single-task error, here we set the threshold as t logp/n , where t is a
1 0 1
q (k)
thresholding parameter. The first screening set is then defined as = k : δ
1 1
C { k k ≤
t logp/n . We will show that under some regularity conditions, appropriate choice
1 0 b e
}
ofq t will lead to consistent selection conforming with the requirement δ(k) h .
1 1 1
k k ≤
The second requirement of the oracle transferable set is to reject source studies
h
C
with tiny expectation of τ-th density E [f(k) ] compared to the target E [f(0) ]. We
i i
employ the approxmation technique with the Powell bandwidth b (Koenker, 2005)
k
to estimate each
E [f(k)
]. Specifically, for each study we calculate
i
f(k)(β +δ(k) ) = 1 nk I y(k) (x(k) ) (β +δ(k) ) b /2b ,
n | i − i ⊤ | ≤ k k
k i=1 (cid:18) (cid:19)
X
e e e e e
where the practical choice of bandwidth b will be recommended in the simulation
k
(k)
section. Denote the second screening set = k : f(k)(β + δ ) t f(0)(β) with
2 2
C { ≥ }
the thresholding parameter t . We will show that under some regularity conditions,
2 b e e e e e
appropriate choice of t will lead to consistent selection conforming with the require-
2
ment E [f(0) ]/E [f(k) ] h . The final detected transferable set is = . Note
i i 2 1 2
≤ C C ∩ C
(k)
that the calculation of estimators β, δ K is exactly the first round of Algorithm
k=1 b b b
{ }
1 for all source studies. Also in our simulations, Algorithm 1 usually converges in one
e e
step, hence here we propose a unified one-shot algorithm for transferable set detection
and parameter prediction, which is shown in Algorithm 2.
We next investigate the theoretical properties of Algorithm 2, which determines
the transferable set . The key ingredient is to establish the detection consistency of
C
towards the oracle but unknown transferable set .
b h
C C
Cb ondition 5. For each source study k, let Ω(k) := C sk logp +2η , where η is
1 3E[f i(k)] rnk ∧n
0
k k
defined in Condition 4, and C is a positive constant specified in the supplementary
3
17Algorithm 2: Parameter prediction with transferable set detection
Input: Target data ((x(0) ) ,y(0) )n 0 , source data ((x(k) ) ,y(k) )nk K ,
i ⊤ i i=1 i ⊤ i i=1 k=1
{ } { }
tuning parameters λ and λ , bandwidth parameters
β k 0 k K
{ } ≤ ≤
b .
k 0 k K
{ } ≤ ≤
1. Compute β ℓ -QR on [((x(0) ) ,y(0) )n 0 ,λ ].
1 i ⊤ i i=1 0
←
2. Compute ee ach δ(k) ℓ -QR on [((x(k) ) ,y(k) (x(k) ) β)nk ,λ ],1 k K.
1 i ⊤ i i ⊤ i=1 k
← − ≤ ≤
e (k) e
3. Compute = k : δ t logp/n and
1 1 1 0
C { k k ≤ }
2
= k : fb(k)(β +δ(k e) ) t 2f(0)q (β) , where
C { ≥ }
f(k)(β +δ(k) ) = 1 nk I ( y(k) (x(k) ) (β +δ(k) ) b )/2b .
b e e nke i=1
|
ie −e i ⊤
| ≤
k k
P
e e e e e
4. Determine the transferable set = , then solve the pooled ℓ -QR problem
1 2 1
C C ∩C
b b b
(k)
β = argmin L(k)(β +δ )+λ β .
 β
k
k1
β k
X0

b ∈{ }∪C e
 
b
Output: β.
b
materials. Denote the set := k 1,...,K : δ(k) h . satisfies that
h 1 h
C 1 { ∈ { } k k ≤ } C 1
inf δ(k) Ω(k) > sup δ(k) +Ω(k) .
k ∈Chc 1 (cid:18)k k1 − 1 (cid:19) k ∈Ch 1 (cid:18)k k1 1 (cid:19)
Condition 6. For each study k, let
s+s η logp logp logp
Ω(k) := C k + k ( ) 1/4 + +s +r ,
2 4 s b kn
k
sb kn
k
n
k
n
0
− v
u
n
k
v
u
n
0
k 
∧ u u
 t t 
where r is defined in Theorem 1 (r = 0), and C is a positive constant specified in the
k 0 4
supplementary materials. Denote the set := k 1,...,K : E [f(0) ]/E [f(k) ]
Ch 2 { ∈ { } i i ≤
h . satisfies that
2 h
} C 2
inf k ∈Ch 2(E [f i(k) ] −Ω( 2k) )
>
sup k ∈Chc 2(E [f i(k) ]+Ω( 2k) )
.
E [f(0) ]+Ω(0) E [f(0)
]
Ω(0)
i 2 i 2
−
Remark 5. Condition 5 and 6 assume a significant gap of quantile coefficients and
τ-th densities to ensure the identifiability of transferable source studies satisfying
(k) (k)
the oracle requirement in (3). In fact, Ω ,Ω are estimation error bounds for
1 2
18δ(k) δ(k) and f(k) E [f(k) ] respectively. Similar conditions are also assumed
1 i
k − k | − |
in Tian and Feng (2023); Zhang and Zhu (2023); Huang et al. (2023). Moreover,
e e
since we don’t adopt the detection method by comparing loss functions, we relax the
restrictions on the similarity of covariance matrices (for example Assumption 6 in
Huang et al. (2023)).
Theorem 2. Assuming Conditions 1-6 and assumptions in Theorem 1, choose the
thresholding parameters t ,t in Algorithm 2 as
1 2
logp logp
inf δ(k) Ω(k) / t sup δ(k) +Ω(k) / ,
k ∈Chc 1 (cid:18)k k1 − 1 (cid:19) v u u n 0 ≥ 1 ≥ k ∈Ch 1 (cid:18)k k1 1 (cid:19) v u u n 0
t t
inf k ∈Ch 2(E [f i(k) ] −Ω( 2k) )
t
sup k ∈Chc 2(E [f i(k) ]+Ω( 2k) )
.
E [f(0) ]+Ω(0) ≥ 2 ≥ E [f(0) ] Ω(0)
i 2 i 2
−
Then for the detected transferable set in Algorithm 2, with any constant ε > 0,
C
b
P ( = ) 1 ε 2(K +1)A exp( A logp).
h 1 2
C C ≥ − − −
b
Consequently, our practical β obtained from Algorithm 2 can enjoy the same ℓ /ℓ -
1 2
convergence rates in Corollary 1.
b
4 Statistical Inference with Knowledge Transfer
4.1 Debiased Estimator based on Neyman Orthogonality
∗
We now turn to the statistical inference for the target parameter, β , focusing
specifically on the first component, β , without loss of generality. Previous stud-
1∗
ies (Tian and Feng, 2023; Li et al., 2024; Huang et al., 2023) have addressed this in-
ference task building on the principles of debiased lasso (Zhang and Zhang, 2014;
van de Geer et al., 2014; Javanmard and Montanari, 2014). In these approaches, the
primary method involves estimating the precision matrix of the loss function and
applying a one-step correction to the ℓ -penalized estimate. Unfortunately, these
1
methods only achieve √n -normality for the debiased estimators, offering little im-
0
provement over debiasing based solely on the target study. The key reason for this
limitation is that their debiasing procedures need the covariance matrix to be identi-
cal for all samples, a condition that is violated when covariate shift occurs. Also, they
19do not offer any measures to adjust for the parameter contrast δ(k). As a result, the
√n -normality is suboptimal, especially given the wealth of transferable information
0
from source studies. In an ideal scenario where δ(k) = 0 and the residuals ǫ(k) follow
i
a normal distribution (0,1), meaning all source studies are as informative as the
N
target study, we should attain a √N-normality by pooling all studies together. The
challenge, then, is how to effectively leverage the information from these transferable
source studies to enhance inference efficiency, particularly under distributional shifts.
To address this challenge, we propose a novel approach motivated by the Neyman
orthogonal idea (Belloni et al., 2019). Before introducing the whole algorithm, we
use the k-th source study as a methodological warm-up. Denote v as the last p 1
1
− −
dimensions of the vector v Rp. We can rewrite the original model (1) as a partially
∈
linear quantile regression
(k) (k) (k) (k) (k) (k)
y = x β +(x ) w +r +ǫ , i = 1,...,n ,
i i,1 1∗ i, 1 ⊤ 1 i i k
− −
(k) (k) (k)
where r = x δ is regarded as an approximation error. A natural moment
i i,1 1
condition for identifying β is to solve the equation (in α)
1∗
nk
E I y(k) x(k) α+(x(k) ) w(k) +r(k) τ x(k) = 0. (9)
i i,1 i, 1 ⊤ 1 i i,1
≤ −
i=1(cid:20) (cid:26) − − (cid:27) (cid:21)
X
(k)
The high-dimensional nuisance parameter w will bring trouble for identifying β ,
1 1∗
−
since thefirst-orderderivative of (9)with respectto this nuisanceparameteratα = β
1∗
is non-zero:
nk nk
∂ E I y(k) x(k) β +(x(k) ) w +r(k) τ x(k) = f(k) x(k) x(k) = 0.
w i
≤
i,1 1∗ i, 1 ⊤ i
−
i,1(cid:12) i i,1 i, 1
6
i X=1(cid:20) (cid:26) − (cid:27) (cid:21) (cid:12) (cid:12)w=w( −k 1) i X=1 −
(cid:12)
(cid:12)
If conducting valid inference on β directly with mome(cid:12)nt condition (9), the estimator
1∗
(k)
for w needs to converge faster than 1/√n , which is difficult to achieve in theory.
1 k
−
To address this problem, we adopt the Neyman orthogonality to make the moment
condition immune to the first-order error from the nuisance estimator. The technique
(k) (k)
is based on the linear projection of the regressor of interest x on the x weighted
i,1 i, 1
−
(k)
by the τ-th density f :
i
(k) (k) (k) (k) (k) (k)
f x = f (x ) θ +v , i = 1,...,n , (10)
i i,1 i i, 1 ⊤ 0 0i k
−
where the coefficient θ(k) argmin E nk [f(k) x(k) f(k) (x(k) ) θ]2, satisfying the
0 θ i=1 i i,1 i i, 1 ⊤
∈ −
−
relationship E nk [f(k) x(k) v(k) ] = 0. WPe then construct the orthogonal score func-
i=1 i i, 1 0i
−
tion P
20ψ(k) (α,w(k) ,θ(k) ,f(k) ) := I y(k) x(k) α+(x(k) )⊤w(k) +r(k) τ f(k) x(k) f(k) (x(k) )⊤θ(k) ,
i −1 0 i { i ≤ i,1 i,−1 −1 i }− i i,1 − i i,−1 0
h ih (1i1)
and solve for E nk [ψ(k) (α,w(k) ,θ(k) ),f(k) ] = 0, where this moment condition can
i=1 i 1 0 i
−
(k)
satisfy the orthoPgonality with respect to high-dimensional nuisance parameters w
1
−
(k)
and θ , i.e.
0
nk nk
∂ E ψ(k) (β ,w,θ(k) ,f(k) ) = f(k) x(k) v(k) = 0,
w i 1∗ 0 i (cid:12) i i, 1 0i
i X=1(cid:20) (cid:21)(cid:12) (cid:12)w=w( −k 1) i X=1 −
(cid:12)
nk (cid:12)
∂ E ψ(k) (β ,w(k) ,θ,f(k) ) (cid:12) = ∂ E [ 0 ] = 0.
θ
i X=1(cid:20)
i 1∗ −1 i (cid:21)(cid:12)
(cid:12) (cid:12)θ=θ( 0k)
θ x( ik)
(cid:12)
(cid:12)θ=θ( 0k)
(cid:12) (cid:12)
(cid:12) (cid:12)
(k) (cid:12)(k)
Consequently, the estimators for w and θ will only introduce a second-order bias,
1 0
−
allowing the use of high-dimensional sparse estimates. It is important to note that
(k)
none of the debiased approaches can be applied to the approximate error term, r =
i
(k) (k) (k) (k)
x δ , because each δ shares the same covariate x with the target parameter
i,1 1 1 i,1
β . We will address the requirements and potential solutions for handling this term
1∗
in the context of knowledge transfer later.
Now let’s discuss why this orthogonal approach can conduct knowledge transfer
for statistical inference in the presence of distribution shift. The key point is that
this approach can avoid the computation of precision matrices and free the identical
covariance matrix assumption across studies, which exactly accommodates the cir-
cumstance under covariate shift. To tackle the parameter shift and residual shift, we
only incorporate moment conditions of target and sources in the oracle transferable
set , where the oracle moment condition is to solve
h
C
nk
E ψ(k) (α,w(k) ,θ(k) ,f(k) ) = 0. (12)
i 1 0 i
k X0 hi X=1(cid:20) − (cid:21)
∈{ }∪C
The parameter identified from (12) leverages information from informative source
studies rather than only debiasing on the target, hence could achieve better inference
performance than previous works.
We outline several points here for practical implementation. Firstly, we estimate
(k)
the τ-th density f by
i
2h
(k) k
f = , (13)
i (k) (k) (k) (k)
(x ) β (x ) β
b
i ⊤ τ+hk
−
i ⊤ τ −hk
b b
21(k)
where β are computed via ℓ -QR at (τ h )-th quantile with tuning param-
τ hk 1
±
k
±
eters λ(k) on the k-th study respectively. The bandwidth parameter, h , is set to
τb k
1/6
min n− ,τ(1 τ)/2 , as suggested by Koenker (2005); Giessing and Wang (2023).
{ k − }
(k)
Next, each high-dimensional projection parameter θ is determined by ℓ -penalized
0 1
weighted least squares
nk 2
(k) (k) (k) (k) (k) (k)
θ argmin f x f (x ) θ +λ θ , (14)
i i,1 i i, 1 ⊤ θ 1
∈ θ i=1(cid:20) − − (cid:21) k k
X
b b b
(k)
where λ is a tuning parameter and its value will be shown in the simulation section.
θ
(k)
Foreachhigh-dimensionalnuisanceparameterw , we calculate thepost-selectedQR
1
−
(k)
estimator (denoted by w ), since the refitting step after feature screening can lead
1
−
to better finite sample behaviors (Belloni et al., 2019). Specifically, we screen each
f
(k)
source study’s covariates by setting threshold for each element of β + δ obtained
in Algorithm 2, then apply traditional QR on the selected features. The empirical
b e
score function corresponding to (11) is consequently defined as
ψ(k) (α,w(k) ,θ(k) ,f(k) ) := I y(k) x(k) α+(x(k) )⊤w(k) τ f(k) x(k) f(k) (x(k) )⊤θ(k) .
i −1 i i ≤ i,1 i,−1 −1 − i i,1 − i i,−1
(cid:16) n o (cid:17)(cid:20) (cid:21)
b f b b f b b b(15)
Finally, following the moment condition (12), we aggregate the empirical score func-
tions within the set from Algorithm 1 and compute the debiased estimate α for β
1∗
C
by
b
b
nk
(k) (k) (k) (k)
argmin ψ (α,w ,θ ,f ) ,
(cid:12) i 1 i (cid:12)
α ∈AC (cid:12) (cid:12)k X0 i X=1 − (cid:12) (cid:12)
(cid:12) (cid:12) ∈{ }∪C b f b b (cid:12) (cid:12)
where
A
= {α
∈
R : |α −β b1 |(cid:12) (cid:12)
≤
10[n b−1
k 0
n i=k 1(x( i,k 1) )2] −1/(cid:12) (cid:12)2/logn
}
is the search
C C ∈{ }∪C C
region for theoretical consideration, asPsuggestedPby Belloni et al. (2019). The whole
b
b b b b
inference procedure is summarized in Algorithm 3.
(k)
Tackling the trouble by approximation error r . As noted by Belloni et al.
i
(2019), the orthogonal approach remains theoretically valid as long as E [(r(k) )2] =
i
(E [x(k) ]δ(k) )2 staysbelow a certain threshold. In the transfersetting, introducing more
i1 1
(k)
external data increases the requirement for the contrast δ , where a sufficient the-
1
1/2
oretical bound is o(n− ). However, constructing consistent selection procedures or
C
hypothesis tests for this condition is nearly impossible, especially when dynamically
b
selecting . Although there is no theoretical guarantee, we propose a practical qual-
C
ity control procedure to validate the transfer debiased estimator. We can use prior
b
22(k)
knowledge to exclude certain source studies with non-negligible δ and perform a
1
bootstrap test to check the normality of the debiased estimator. If the normality
e
hypothesis is rejected, we revert to debiasing only the target study, yielding a result
similar to previous approaches. The full procedure is outlined in Algorithm 4 in the
supplementary materials.
Algorithm 3: Inference for β with knowledge transfer
1∗
(k)
Input: Detected transferable set and estimators β, δ from
C { }k
Algorithm 2, target study ((x(0) ) ,y(0) )n 0 , source∈ sC tudies
b i ⊤ i i=1b e
{ } b
((x(k) ) ,y(k) )nk , tuning parameters λ(k),λ(k) , threshold
{ i ⊤ i i=1 }k { θ }k 0
¯ ∈C ∈{ }∪C
parameter λ.
b b
(k)
1. For each study k 0 , estimate the τ-th densities f by (13), conduct
i
∈ { }∪C
(k) (k) (k) (k) (k) (k)
the projection (14), and calculate the residuals v = f x f (x ) θ .
b i i b i,1 i i, 1 ⊤
−
−
b b b
2. For each study k 0 , compute w(k) by trbaditional QR on selected data
∈ { }∪C
((x(k) ) ,y(k) )nk , where the index set S(k) = j : β +δ(k) > λ¯ , then
{ i,S(k) ⊤ i i=1 } b c { | j j | }
enlarge w(k) (with zero) to the p-dimension bal vector w b(k). e
b
3. Incorporcate empirical score functions (15) within targfet and transferable source
studies and solve for
nk
(k) (k) (k) (k)
α argmin ψ (α,w ,θ ,f ) .
(cid:12) i 1 i (cid:12)
∈ α ∈AC (cid:12) (cid:12)k X0 i X=1 − (cid:12) (cid:12)
b (cid:12) (cid:12) ∈{ }∪C b f b b (cid:12) (cid:12)
(cid:12) (cid:12)
Output: Debiased estimator αb . (cid:12) b (cid:12)
b
4.2 Asymptotic Normality for Debiased Estimator
Before examining the asymptotic normality of the estimator α in Algorithm 3, we
introduce regularity conditions that ensure the validity of the inference results. As in
b
Theorem 1, we assume an arbitrarily specified transferable set . Let c and C denote
C
given positive constants. Although these could vary across studies, incorporating
them into the growth conditions in Theorem 3 below would complicate the analysis
significantly. Therefore, for simplicity, we treat them as universal fixed values.
23(k)
Condition 7. For each study k 0 , (i) assume each θ C (ii)
0 2
∈ { } ∪ C k k ≤
There exists a universal sparsity level s and vectors θ(k) such that s =
∗ k 0 ∗
{ } ∈{ }∪C
max s+s + θ(k) , where each θ(k) satisfies (x(k) ) θ(k) = (x(k) ) θ(k) +
k 0 k 0 i, 1 ⊤ 0 i, 1 ⊤
∈{ }∪C{ k k } − −
r(k) with θ(k) θ(k) = Cs logp/n , E [(r(k) )2] Cs /n . (iii) We have c
iθ
k
0
−
k1 ∗ k iθ
≤
∗ k
≤
min E [(f(k) x(k) v(k) )2]1/2 q max E [(f(k) x(k) v(k) )3]1/3 C.
1<j p i i,j 0i 1<j p i i,j 0i
≤ ≤ ≤ ≤
Condition 8. For each study k 0 , (i) E [(ξ x(k) )2(r(k) )2] C ξ 2E [(r(k) )2],
⊤ i, 1 i 2 i
∈ { }∪C ≤ k k
−
for any ξ Rp 1. (ii) nkE [(r(k) )2] = o(1/√n ). (iii) nk E [f(k) v(k) r(k) ] =
∈ − k ∈C nC i C k ∈C nC| i 0i i |
o(1/√n ). P P
C
(k)
Condition 9. For each study k 0 , (i) conditional density f( x ) are
i
∈ { } ∪ C ·|
continuously differentiable and uniformly lower bounded by a positive constant f.
(ii) For u = τ h , assume that u-quantile(y(k) x(k) ) = (x(k) ) w(k) + ǫ(k) , where
k i i i ⊤ u iu
± |
E [ǫ i( uk) ]
≤
o(n k−1/2 ), |ǫ( iuk)
| ≤
o(h k) and |w( uk)
|0
≤
s ∗.
Condition 7-9 actually generalize the assumptions in Belloni et al. (2019) to the
transfer setting. Condition 7 follows the original settings, providing the necessary re-
(k)
quirementsforestablishingconvergenceratesofweightedLassoestimators θ .
k 0
{ } ∈{ }∪C
(k)
Condition 7 (ii) is to assume the projection parameter θ in (10) can be approx-
0 b
imated by the sparse vector θ(k) up to a small error. while Condition 7 (iii) im-
(k) (k) (k)
poses necessary mild moment conditions on the variables f x v . Condition 8
i i,j 0i
(k)
set up the moment requirements for approximation error r . Since the debiased
i
sample size n is larger than the target size n , Condition 8 (ii) and (iii) have to
0
C
link each source’s quantity rk to the whole sample size n . Admittedly, due to
i
C
the need of approximate orthogonality, each
E [f(k) v(k) r(k)
] =
E [f(k) x(k) v(k) δ(k)
] =
i 0i i i i1 0i 1
E [(f(k) (x(k)
)
θ(k)
+
v(k) )v(k) δ(k)
] =
E [(v(k) )2]δ(k)
needs to very small, where a suffi-
i i, 1 ⊤ 0 0i 0i 1 0i 1
−
(k) 1/2
cient condition is δ = o(n− ). This requirement is understandable since we need
1
C
smaller parameter contrast for better normality results, plus we don’t have any ap-
proaches to remove this error. Condition 9 is for the estimation of the conditional
density at (τ h )-th quantiles, where the approximately sparse QR model is assumed
k
±
around the quantile index τ. All of these conditions follow the corresponding version
of single-study case established in Belloni et al. (2019).
Theorem 3. Under Condition 1-9, if the parameters satisfy the following growth
conditions
slogp n logp
+ k δ(k) = o(1/√n ),
1 C
n
C
n Csn
k
n 0k k
Xk∈C ∧
24n 1 s∗logp λ(k) s∗
k +h2 + θ = o(n−1/4 ),
n C "h ks n k k n k # C
k∈X{0}∪C
2
n s∗logp n h2 logp
max s∗+ k + k k = o(1/√n ).
k∈{0}∪C

(h kλ θ(k) )2 λ θ(k) ! 

n C C
Then our estimator α based on Algorithm 3 satisfies
 
σ−1√n (bα β∗) = U (τ)+o (1), U (τ) ❀ (0,1),
C C − 1 C p C N
−1/2
where U (τ) = τ(1 b τ) n kE[(v(k) )2] 1 nk ψ(k) (α,w(k) ,θ(k) ,f(k) ),
C  − n 0i  · √n i −1 0 i
k∈X{0}∪C C C k∈X{0}∪CXi=1
 −2 
n n
and σ2 = kE[f(k) x(k) v(k) ] kE[τ(1 τ)(v(k) )2].
C  n i i,1 0i  · n − 0i
k∈X{0}∪C C k∈X{0}∪C C
 
Moreover, the estimator σ2 = τ(1 τ)n [ nk (v(k) )2] 1 is consistent.
k 0 i=1 i −
C − C ∈{ }∪C
With the asymptoticbnormality of α aP nd consiP stentbvariance estimators σ2 pro-
C
vided in Theorem 3, we can construct valid confidence intervals and hypothesis test
b b
procedures for the objective component β . Now we give some explanations for re-
1∗
sults in Theorem 3. Firstly, our estimator α enjoys √n -normality, which is much
C
sharper than the previous results, demonstrating the effectiveness of our debiased
b
transfer methods. Secondly, conditions of Theorem 3 allow for nonidentical covariate
distributions across studies, as well as robust to the divergence of covariance matri-
ces between target and source studies, demonstrating our methods’ superiority under
covariate shift. Moreover, note that the asymptotic variance σ2 includes v(k) that is
0i
C
(k)
inversely proportional to the τ-th density f , clearly revealing that residual shift
i
may increase the variance and deteriorate the inference efficiency, and that’s why
we choose to only debias on the detected transferable set in Algorithm 3. Although
the transferable set and orthogonal technique can alleviate the bias a lot, leveraging
source data still needs parameter shift small enough, especially for the approxima-
(k) (k)
tion error x δ . In the following simulations we will show that transfer debiased
i1 1
(k)
estimators can enjoy sharper inference results if the contrast level δ is small, while
1
may lose power when higher parameter shift is introduced.
5 Simulation
In this section, we conduct extensive numerical experiments to evaluate the empirical
performance of our proposed methods. Before we implement the algorithms on the
25simulated data, we first explain how to choose the parameters for regularization and
detection thresholds, which are very important in practice.
5.1 A Practical Guide for Tuning Parameters
There are several hyperparameters in our transfer procedures, including detection
thresholds t ,t , tuning parameters λ K ,λ , bandwidth parameters b K in
1 2 k k=0 β k k=0
{ } { }
Algorithm 2,
λ(k),λ(k) ,λ¯
in Algorithm 3. To attain better performance in
{ τ θ }k 0
∈{ }∪C
the simulations, here we propose several ways to determine these hyperparameters.
b
Toselecttheℓ -QRtuningparameters,wedeviatefromthevanillapenalizedquan-
1
tile regressionandinsteadfollow theλ-constructionideain Belloni and Chernozhukov
(2011); Giessing and Wang (2023). Specifically, the regularization term changes from
λ β toλ τ(1 τ) p σ β , withσ2 := n 1 nk (x(k) )2 andλ = c Λ(k)(α x(k) nk ),
k
k1
−
j=1 k,j
|
j
|
k,j −k i=1 ij ∗
·
τ ∗
|{
i }i=1
where Λ(k)(q α x(k) n Pk ) is the (1 α )-th quantilePof Λ(k) x(k) nk and
τ ∗ |{ i }i=1 b − ∗ b τ |{ i }i=1
1 nk (τ 1 U τ )x(k)
Λ(k) x(k) nk := max − { i ≤ } ij ,
τ |{ i }i=1 1 j p(cid:12)n τ(1 τ)σ2 (cid:12)
≤ ≤ (cid:12) (cid:12) k i X=1 − k,j (cid:12) (cid:12)
(cid:12) q (cid:12)
(cid:12) (cid:12)
with U ,...,U are i.i.d. Uniform(0,(cid:12)1) random variables,b indep(cid:12)endent of the co-
1 nk
variates x(k) nk . Such a pilot construction can save the computation cost a lot
i i=1
{ }
compared to the cross-validation. α is set as 0.05 uniformly, while we set c = 1
∗ ∗
for λ 0,λ β∗, c
∗
= 1.5 for {λ
k
}K k=1, c
∗
= 1 for {λ( 0k .5)
}k 0
, c
∗
= 0.95 for {λ( 0k .7)
}k 0
∈{ }∪C ∈{ }∪C
(k)
and c = 0.9 for λ . This procedure can be executed through R functions
∗ { 0.2 }k 0 b b
∈{ }∪C
LassoLambdaHat() in the quantreg package.
b
For the bandwidth parameters b K , we follow the construction details in
k k=0
{ }
(k)
Huang et al. (2023) with estimators β, δ K in Algorithm 2. We set the threshold
k=1
{ }
parameter t = 5,t = 0.3 in Algorithm 2 for simplicity, although one can still choose
1 2 e e
t ,t by cross-validation. In our simulations the performance of transfer estimators is
1 2
not sensitive to the choice of t ,t as long as they fall into a proper interval. We also
1 2
tried other values and the results are similar. For simplicity, we set the post-selected
¯ (k)
threshold λ = 0.01. Then for the tuning parameter λ in the projection, we use the
θ
common high-dimensional choice c logp/n in the experiments, where the constant
θ k
q
c is set as 0.1.
θ
265.2 Estimation Errors
We follow a similar setup as in Zhang and Zhu (2023); Huang et al. (2023). Consider
a target study with size n = 200, dimension p = 500, and sparsity level s = 10. We
0
simulate two types of target model:
(0) (0) ∗ (0) (0) (0) ∗ (0) (0)
Homogeneous: y = (x ) β +ǫ , Heterogeneous: y = (x ) β + x ǫ ,
i i ⊤ i i i ⊤ i,1 i
| |
(16)
where β∗ = (1 ,0 ) , x(0) (0 ,Σ ) with Σ = 0.7i j , and ǫ(0) follows
s p s ⊤ i p x x | − | i
− ∼ N 1 i,j p
the shifted standard normal distribution with quantil(cid:16)e levels(cid:17)τ ≤ ≤ 0.2,0.5,0.7 .
∈ { }
The source study number K = 5 and follows the homogeneous/heterogeneous
model type (16) with (y(k) ,x(k) ,w(k),ǫ(k) )nk . To simulate the distribution shift, we
i i i i=1
consider the following parameter configurations for the source studies.
• Covariate shift. For each source study k = 1,...,K, we set each covariate
x(k) 0 ,Σ +ǫ ǫT with ǫ 0 ,0.32I .
i p x x x x p p
∼ N ∼ N
(cid:16) (cid:17) (cid:16) (cid:17)
(k)
• Parameter shift. For each source study k = 1,...,K, denote ζ as indepen-
i
dent Rademacher random variables (taking values in 1, 1 with equal proba-
{ − }
bility). Following the notation in Section 3, we denote the first oracle set , in
h
C 1
which the ℓ -norm of parameter contrast δ(k) = w(k) β∗ of each source
1 1 1
k k k − k
study is relatively small. We set
β +h /100 r(k) I j H(k) [s/2] , k ,
w(k) =  j∗ 1 · j · ∈ ∪ ∈ Ch 1
j h 1/10 r j(k) I j Hn (k) [s/2] , o k hc ,
· · ∈ ∪ ∈ C 1
n o

where [s/2] = 1,...,s/2 , H(k) is a random subset chosen from s/2+1,...,p
{ } { }
with |H(k) | = 50. Itcanbeverifiedthatmax k ∈Ch 1 kδ(k) k1 < h 1 < min k ∈Chc 1 kδ(k) k1.
In this setting, we consider h = 5.
1
(k)
• Residual shift. For each source study k = 1,...,K, the residual r either
i
follows the standard normal distribution, or it follows one type of the following
abnormal distributions:
1. (Cauchy) cauchy distribution: (0,3);
C
2. (Mixed)mixedgaussian: z ( 3,0.5)+(1 z) (3,0.5), wherez ernoulli(τ);
N − − N ∼ B
3. (Noisy) noisy setting: (0,52).
N
27All distributions have already been shifted to meet P (ǫ(k) 0 x(k) ) = τ. We
i i
≤ |
denote the set containing all standard normal source studies by , where we
h
C 2
can let h 2 = 4 satisfy max k ∈Ch
2
E [f i(0) ]/E [f i(k) ] < h 2 < min k ∈Chc
2
E [f i(0) ]/E [f i(k) ].
To clearly demonstrate the negative impact of residual shift, we set the sample
size to n = 100 I k +200 I k c .
k · { ∈ Ch 2} · { ∈ Ch 2}
Basedonthedefinitionin(3), wedefine = asthetransferableoracleset.
h h h
C C 1∩C 2
To capture both parameter and residual shift in one setting, we vary the parametric
transferable set from 1 to 5, and assume that each source study follows a divergent
h
C 1
(k)
distribution with probability 1/2. For example, in the “Cauchy” setting, each ǫ
i
(0)
either follows a normal distribution like ǫ or a Cauchy distribution with equal
i
probability. The same applies to the “Mixed” and “Noisy” settings. We compare six
specific methods, which are: 1) L1-QR: conduct ℓ quantile regression only on the
1
target data; 2) Pooling: Algorithm 1 on all source studies; 3) Oracle: Algorithm 1
on the oracle transferable set ; 4) TransQR: Algorithm 2, our transfer approach;
h
C
5) Oracle PS: Algorithm 1 on (only considering parameter shift), the baseline in
h
C 1
the previous literature; 6) TransQR 2step: two-step transfer approach proposed in
Huang et al. (2023).
We evaluate the performance of each method in terms of the ℓ -error β β 2, the
2 2
k − k
ℓ -error β β and the average prediction error n 1 n 0 [(xt) (β β)]2, where the
1 1 −0 i=1 i ⊤ b
k − k −
test samples xt n 0 generate in the same way of x(0) Pn 0 . We repeat the experiment
b i i=1 i i=1 b
{ } { }
for 100 times and average the results for each setting respectively, where the average
ℓ -errors are shown in Figure 5.1 and 5.2. More simulation results are reported in the
2
supplementary materials.
From Figure 5.1 and 5.2, we highlight three points.
1. As we expect, the Oracle approach has the best performance across all scenarios
because it transfers knowledge exactly from the ideal source studies. Moreover,
more transferable source studies can produce smaller estimation errors while they
do not affect the performance of the single task L1-QR. This fact corresponds
our motivation that one can benefit from transfer learning, and conforms with
our theoretical results.
2. Similar to the previous work, the Pooling approach doesn’t perform well when
thereare scarcetransferablesourcestudies. Thisphenomenonstronglypointsout
the importance of the source detection step before knowledge transfer. It needs to
280.2 0.5 0.7
0.8
0.6
0.4
Method
0.8
L1−QR
Oracle
0.6
TransQR
Pooling
0.4
Oracle_PS
TransQR_2step
0.2
0.8
0.6
0.4
1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
Size of C
h1
Figure 5.1: ℓ -estimation errors of various methods under homogeneous model at
2
quantile levels τ = 0.2,0.5,0.7 .
{ }
emphasize that the residual shift indeed can influence the transfer performance,
sincetheOracle PS methodperformsmuchworserthantheOracle method. Note
that Oracle PS is also conducted on the prespecified studies, which is actually
regarded as the oracle method in all previous literature. The only difference from
the Oracle is that Oracle PS only considers parameter shift, i.e., small δ(k) for
1
k k
screening transferable sets, which demonstrates that our proposed τ-th density
ratio is indeed an important index for knowledge transfer under residual shift.
3. It is amazing that our proposed method TransQR almost match the oracle
method, while the two-step approach TransQR 2step is not satisfactory. The
failure of the two-step transfer may be due to its sensitivity to covariate shift,
as well as its detection procedure’s inability to effectively screen out the source
studies with residual shift. This fact reflects that our method is more effective
for transferable set detection and parameter estimation under distribution shift.
29
rorrE
L
2
Cauchy
Mixed
Noisy0.2 0.5 0.7
0.6
0.4
0.2
Method
0.6 L1−QR
Oracle
0.4 TransQR
Pooling
Oracle_PS
0.2
TransQR_2step
0.6
0.4
0.2
1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
Size of C
h1
Figure 5.2: ℓ -estimation errors of various methods under heterogeneous model at
2
quantile levels τ = 0.2,0.5,0.7
{ }
5.3 Inference Results
To give some intuitive understandings of Algorithm 3, we conduct simulations for
statistical inference on β under the heterogeneous model setting in (16) with some
1∗
(k)
adaptive changes. We let = 5,h = 10 with varying δ 0.01,0.1 (control-
|Ch 1| 1 | 1 | ∈ { }
(k)
ling approximation error r i ), also n 0 = n k |k ∈Ch
2
= 300 and n k |k ∈Chc
2
= 500. In the
simulations above we have verified the detection consistency, hence for computational
simplicity we assume := 1,2,3 is known, and assume the set in c follows the
Ch 2 { } Ch 2
mixed gaussian distribution.
We compare five specific methods, including:
1. Debias tar: debiasing on the target study by Algorithm 3 with the L1-QR esti-
mate (baseline);
2. Debias trans pool: debiasing on all studies by Algorithm 3 with the Pooling es-
30
rorrE
L
2
Cauchy
Mixed
Noisytimate;
3. Debias trans tar: debiasing on the target study by Algorithm 3 with the Tran-
sQR estimate;
4. Debias trans: debiasing on the target and transferable source studies by Algo-
rithm 3 with the TransQR estimate;
5. Debias dl: debiased Lasso approach proposed in Huang et al. (2023) with the
TransQR 2step estimate.
Note that Debias trans is our primary approach for comparison. However, if the
quality control step (Algorithm 4) rejects the normality test, we recommend using a
more conservative approach, namely Debias trans tar. As will be demonstrated later,
(k)
when δ = 0.1, Debias trans fails due to the non-negligible approximation error
1
| |
(k)
r , whereas Debias trans tar proves to be the more robust option. We assess the
i
inference efficiency of various methods across four dimensions: (a) coverage proba-
bility of 95% confidence intervals, (b) bias comparison, (c) average length of 95%
confidence intervals, and (d) density plots of normalized estimates against (0,1).
N
The experiment was repeated 1,000 times using three methods at τ 0.2,0.5,0.7 .
∈ { }
The results (a) are summarized in Table 1-2. The results (b)-(d) are summarized in
Figure 5.3-5.4.
Table 1: Coverage probability of 95% confidence intervals of various methods with
(k)
δ = 0.01
1
Method τ = 0.2 τ = 0.5 τ = 0.7
Debias tar 0.953 0.939 0.951
Debias trans pool 0.917 0.934 0.909
Debias trans tar 0.955 0.950 0.962
Debias trans 0.951 0.954 0.945
Debias dl 0.938 0.954 0.961
From Table 1-2 and Figure 5.3-5.4, we highlight three points.
1. From Table 1, our method Debias trans enjoys the better coverage performance,
whichisthenearestto0.95amongallmethods. SinceDebias tar, Debias trans tar
31Table 2: Coverage probability of 95% confidence intervals of various methods with
(k)
δ = 0.1
1
Method τ = 0.2 τ = 0.5 τ = 0.7
Debias tar 0.953 0.939 0.951
Debias trans pool 0.789 0.766 0.756
Debias trans tar 0.96 0.96 0.96
Debias trans 0.846 0.809 0.817
Debias dl 0.951 0.962 0.961
and Debias dl only solve the target score functions approach, their coverage prob-
abilities are somewhat fluctuant around 0.95. The Debias trans pool approach
has the worst coverage properties, which again demonstrates that introducing
source stuides with large residual shift will bring negative effect to the transfer
efficiency.
2. From the estimation bias and interval length in Figure 5.3, we can see that
Debias trans haveless bias and shorterinterval lengthsthanDebias trans tar and
Debias tar, collaborating to the theory that our approach has faster asymptotic
rates by incorporating informative sources. Although Debias trans pool also has
the comparable biases and intervals (utilizing the most studies), it sacrificed
coverage accuracy due to residual shift. Moreover, since Debias dl debiasing on
the target study, it only obtains √n -rate and hence has relatively larger bias
0
and longer interval lengths.
3. By comparing the density plots with the standard normal (orange dashed line) in
Figure 5.3, it can be seen that normalized estimates of all these methods are close
to the standard normal, except that Debias trans pool at τ = 0.7, Debias tar,
Debias trans tar at τ = 0.2 lack a little bit efficiency. Therefore, under small
(k)
δ our transfer approach can explore source information validly and conduct
1
more efficient inference than previous works, which will offer more insights for
statistical inference under knowledge transfer.
(k)
4. From Table 2 and Figure 5.4, when δ is large, unfortunately our Debias trans
1
| |
performs much worse than the target debiasing methods, which is in our expecta-
320.2 0.5 0.7
0.50
Method
0.25 Debias_tar
Debias_trans_pool
0.00
Debias_trans_tar
−0.25 Debias_trans
Debias_dl
−0.50
0.2 0.5 0.7
1.5
Method
Debias_tar
1.0
Debias_trans_pool
Debias_trans_tar
0.5
Debias_trans
Debias_dl
0.0
0.2 0.5 0.7
0.5
Method
0.4
Debias_tar
0.3 Debias_trans_pool
0.2 Debias_trans_tar
Debias_trans
0.1
Debias_dl
0.0
−5.0 −2.5 0.0 2.5 5.0−5.0 −2.5 0.0 2.5 5.0−5.0 −2.5 0.0 2.5 5.0
(k)
Figure 5.3: Inference results of various methods with δ = 0.01
1
tion since Condition 8 is no longer satisfied. The density plots in Figure 5.4 show
that the transfer debiased estimate is no longer normal, and the Shapiro-Wilk
test rejects the normality hypothesis. In a similar way, the bootstrap approach in
Algorithm4 canalsoindicate usthatdebiasingonsourcestudieswill generateun-
reliable estimates. Therefore, we recommend only debiasing on the target study,
namely the Debias trans tar approach, which can keep the coverage proportions
with more stable interval lengths than Debias dl.
33
saiB
slavretnI
fo
htgneL
ytisneD0.2 0.5 0.7
0.50
Method
0.25 Debias_tar
Debias_trans_pool
0.00
Debias_trans_tar
−0.25 Debias_trans
Debias_dl
−0.50
0.2 0.5 0.7
1.5
Method
Debias_tar
1.0
Debias_trans_pool
Debias_trans_tar
0.5
Debias_trans
Debias_dl
0.0
0.2 0.5 0.7
0.5 Method
0.4 Debias_tar
0.3 Debias_trans_pool
Debias_trans_tar
0.2
Debias_trans
0.1
Debias_dl
0.0
−5.0 −2.5 0.0 2.5 5.0−5.0 −2.5 0.0 2.5 5.0−5.0 −2.5 0.0 2.5 5.0
(k)
Figure 5.4: Inference results of various methods with δ = 0.1
1
6 Application
The proposed transfer learning algorithm is applied to the Genotype-Tissue Expres-
sion (GTEx) data, available at https://gtexportal.org/. This dataset contains gene
expression levels from 49 tissues, involving 838 individuals, and includes 1,207,976 ob-
servationsfor38,187genes. Followingthepreviouswork(Li et al.,2022;Zhang and Zhu,
2023; Zhang et al., 2024), our study examines gene regulation within the central ner-
vous system (CNS) across various tissues. The CNS-related genes are grouped under
MODULE 137, which includes 545 genes, along with an additional 1,632 genes that
show significant enrichment in the same experiments as those in the module. Please
34
saiB
slavretnI
fo
htgneL
ytisneDrefer to https://www.gsea-msigdb.org/gsea/msigdb/cards/MODULE 137.html for a
detailed description of this module.
Our focus is on the expression levels of the genes JAM2 and SH2D2A at the lower
quantile. Specifically, we aim to predict the expression levels of these two genes at
τ = 0.2 within a target tissue by leveraging the expression of other central nervous
system (CNS) genes. Recent research has identified that, JAM2 and SH2D2A play a
significant role in various cellular functions and disease progressions, particularly in
cancer and immune response.
• JAM2 (Junctional Adhesion Molecule 2) is a gene that encodes a protein involved
in tight junctions between epithelial and endothelial cells, playing a crucial role in
cell adhesion and maintaining barrier functions. It is associated with various cel-
lular processes such as cell proliferation and migration. Studies have shown that
low expression of JAM2 is linked to poor prognosis in several cancers, including
lung adenocarcinoma (LUAD), where reduced JAM2 levels correlate with tumor
progression and metastasis. Investigating its low expression helps in understand-
ing cancer progression and potentially identifying therapeutic targets for limiting
tumor growth (Dong et al., 2024).
• SH2D2A encodes the T-cell-specific adapter protein (TSAd), which is essential
forT-cell signalingandactivation. The expressionofSH2D2Ais tightly regulated
at both the transcriptional and translational levels, with cAMP signaling shown
to induce its mRNA expression in T cells. However, low levels of SH2D2A protein
may affect immune responses. Notably, reduced SH2D2A expression can enhance
T-cell-mediated anti-tumor immunity, making it an interesting target in cancer
immunotherapy research (Berge et al., 2012).
We analyze 13 brain tissues as separate target studies, estimating the model for
each tissue individually. The target data is divided into five folds, where each fold
is used as a prediction set, while the remaining four folds are used for training. The
methods L1-QR, Pooling, TransQR 2step, and TransQR from the simulation section
are applied in this analysis. Detailed data pre-processing and tuning parameter se-
lection can be found in the supplementary materials. Figure 6.1 shows the relative
prediction errors of Pooling, TransQR 2step, and our approach, TransQR, compared
to L1-QR across the target tissues, with the black horizontal line representing a ra-
tio of 1. As expected, TransQR consistently achieves the lowest prediction errors,
35significantly improving the performance of the target task in nearly all cases. We ob-
serve that the two-step approach, TransQR 2step, tends to select more source studies
than TransQR, indicating that our transferable set detection can effectively identify
valuable studies even under parameter or residual shift. Moreover, in some instances,
TransQR 2step does not provide substantial improvements over Pooling and L1-QR,
further demonstrating the robustness of our transfer framework in addressing co-
variate shift. Compared to the mean regression results reported in Li et al. (2022);
Zhao et al. (2023), our results offer richer insights for understanding the whole dis-
tribution of gene expression.
JAM2
1.00
0.75
0.50
0.25
Method
0.00
Pooling
SH2D2A
Trans
1.00
TransQR_2step
0.75
0.50
0.25
0.00
Amygdala A.C.cortex C.B.gangli Ca .hemisphere Cerebellum Cortex F.cortex Hippocampu Hs
ypothala
Nm .u Cs .A.B.ganglia P.B.ganglia S.C.cervical S.nigra
Tissue
Figure 6.1: Relative prediction errors for the expression level of gene JAM2 and
SH2D2A under quantile level τ = 0.2.
7 Discussion
To tackle the possible negative transfer caused by distribution shift, we introduce a
novel transferable set with screening both δ(k) and E [f(k) ] to address the param-
1 i
k k
eter and residual shift, also we adopt the ℓ -minimization framework to avoid the
1
trouble from covariate shift. Along with the transferable set detection and parame-
36
rorrE
noitciderP
evitaleRter estimation, we propose an orthogonal debiased approach for statistical inference
with leveraging informative sources. Theoretical results as well as numerical experi-
ments validate the effectiveness of our proposed method. There are several avenues
to generalize our method, such as
• We can construct source weights proportional to their respective τ-th densities
to achieve sharper results. Leveraging the density at objective quantiles to en-
hance the efficiency of estimators has been a common approach in the literature
onweighted quantile regression(Jiang et al.,2012;Huang et al.,2015;Lu and Fan,
2015; Xiong and Tian, 2022), as the Hessian matrices of check loss functions are
directly proportional to τ-th densities. Furthermore, abundant source information
at multiple quantiles can be utilized to predict the target study’s objective quan-
tile coefficient. For instance, we can extend panel quantile regression (Zhang et al.,
2019) or GMM quantile regression (Firpo et al., 2022) to the transfer setting.
• Our transferable set adopts a common all-in-or-all-out manner, thus in (3) we use
the expectation of τ-th densities as the transferability measure. In fact, we can
generalize this measure to select partial informative data from each source study,
especially when the density is strongly dependent on the covariates. To be specific,
define the transferable dataset (not transferable set) as
h′
C
1 1
= (x(k) ,y(k) ) : 1 k K, δ(k) h , h .
Ch′  i i ≤ ≤ k k1 ≤ 1 f(k) ≤ 2 f(0)
 i i 
 
Then we use the data in to conduct knowledge transfer, where we can leverage
h′
C
more information from related source studies.
• Due to the privacy or data storage concern, we may not be able to combine the
individual-level data of all studies in a single machine. Assume we can only trans-
mit summary statistics between studies, and the final result is calculated on the
machine with target data (referred as the target machine). By carefully observing
our Algorithm 2, we can see that almost every step is based on the local data
and the summary statistics from other studies. Briefly speaking, we just need
(k)
to transmit the estimators like β, δ K between each source machine and the
k=1
{ }
target machine. The target machine can then determine the transferable set .
e e
C
The only step needed to pool the data from different studies is the last step of
b
Algorithm 2. To accommodate this step to the distributed setting, we can use
37existing algorithms (for example Algorithm 1 in Chen et al. (2020)) to conduct
distributed high-dimensional quantile regression. Therefore, our algorithm can be
easily generalized to the communication-efficient version with multiple data storage
nodes.
There are many interesting topics of transfer learning for future research. For
example, distributionally robust transfer learning aims to obtain an optimized tar-
get distribution in a prespecified set with feasible distributions related to the source
distributions. The present works mainly focus on the feasible distribution set con-
structed by the convex hull of source distributions (Xiong et al., 2023; Wang et al.,
2023). We can extend the feasible set in terms of the Wasserstein distance metric
(Blanchet et al., 2019; Gao and Kleywegt, 2023), which may help us to select source
domains more effectively, with learning a target model more robust to various distri-
bution shifts.
Acknowledgment
TheresearchofZhuispartially supportedbytheNationalNaturalScienceFoundation
of China 12071087, 12331009.
38References
Belloni, A. and Chernozhukov, V. (2011). ℓ -penalized quantile regression in high-
1
dimensional sparse models. The Annals of Statistics, 39(1):82–130.
Belloni, A., Chernozhukov, V., and Kato, K. (2019). Valid post-selection inference in
high-dimensional approximately sparse quantile regression models. Journal of the
American Statistical Association, 114(526):749–758.
Berge, T., Grønningsæter, I. H. B., Lorvik, K. B., Abrahamsen, G., Granum, S.,
Sundvold-Gjerstad, V., Corthay, A., Bogen, B., and Spurkland, A. (2012). SH2D2A
modulates T cell mediated protection to a B cell derived tumor in transgenic mice.
PloS one, 7(10):e48239.
Bickel, P. J., Ritov, Y., and Tsybakov, A. B. (2009). Simultaneous analysis of Lasso
and Dantzig selector. The Annals of Statistics, 37(4):1705–1732.
Blanchet, J., Kang, Y., and Murthy, K. (2019). Robust wasserstein profile inference
and applications to machine learning. Journal of Applied Probability, 56(3):830–857.
Cai, T. T. and Pu, H. (2024). Transfer learning for nonparametric regres-
sion: Non-asymptotic minimax analysis and adaptive procedure. arXiv preprint
arXiv:2401.12272.
Cai, T. T. and Wei, H. (2021). Transfer learning for nonparametric classification:
Minimax rate and adaptive classifier. The Annals of Statistics, 49(1):100–128.
Candes, E. and Tao, T. (2007). The Dantzig selector: Statistical estimation when p
is much larger than n. The Annals of Statistics, 35(6):2313–2351.
Chen, L., Zaharia, M., and Zou, J. Y. (2022). Estimating and explaining model
performance when both covariates and labels shift. In Koyejo, S., Mohamed, S.,
Agarwal, A., Belgrave, D., Cho, K., and Oh, A., editors, Advances in Neural In-
formation Processing Systems, volume 35, pages 11467–11479. Curran Associates,
Inc.
Chen, X., Liu, W., Mao, X., and Yang, Z. (2020). Distributed high-dimensional re-
gression under a quantile loss function. The Journal of Machine Learning Research,
21(1):7432–7474.
39Dong, Y., Zhang, J., Xie, S., Di, S., Fan, B., and Gong, T. (2024). JAM2 is a prog-
nostic biomarker and inhibits proliferation, metastasis and epithelial–mesenchymal
transition in lung adenocarcinoma. The Journal of Gene Medicine, 26(2):e3679.
Duan, Y. and Wang, K. (2023). Adaptive and robust multi-task learning. The Annals
of Statistics, 51(5):2015–2039.
Fan, J., Guo, Y., and Wang, K. (2023). Communication-efficient accurate statistical
estimation. Journal of the American Statistical Association, 118(542):1000–1010.
Fenoy, E., Edera, A. A., and Stegmayer, G. (2022). Transfer learning in proteins:
evaluating novel protein learned representations for bioinformatics tasks. Briefings
in Bioinformatics, 23(4):bbac232.
Firpo, S., Galvao, A. F., Pinto, C., Poirier, A., and Sanroman, G. (2022). Gmm
quantile regression. Journal of Econometrics, 230(2):432–452.
Gao, R. and Kleywegt, A. (2023). Distributionally robust stochastic optimization
with wasserstein distance. Mathematics of Operations Research, 48(2):603–655.
Garg, S., Wu, Y., Balakrishnan, S., and Lipton, Z. (2020). A unified view of label
shift estimation. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin,
H., editors, Advances in Neural Information Processing Systems, volume 33, pages
3290–3300. Curran Associates, Inc.
Giessing, A. and Wang, J. (2023). Debiased inference on heterogeneous quantile
treatmenteffects with regression rank scores. Journal of the Royal Statistical Society
Series B: Statistical Methodology, 85(5):1561–1588.
Hahn, J. (1995). Bootstrapping quantile regression estimators. Econometric Theory,
11(1):105–121.
He, Z., Sun, Y., and Li, R. (2024). Transfusion: Covariate-shift robust transfer
learning for high-dimensional regression. In Proceedings of The 27th International
Conference on Artificial Intelligence and Statistics, volume 238. PMLR.
Huang, J., Wang, M., and Wu, Y. (2023). Estimation and inference for transfer learn-
ing with high-dimensional quantile regression. arXiv preprint arXiv:2211.14578.
Huang, M. L., Xu, X., and Tashnev, D. (2015). A weighted linear quantile regression.
Journal of Statistical Computation and Simulation, 85(13):2596–2618.
40Javanmard, A. and Montanari, A. (2014). Confidence intervals and hypothesis test-
ing for high-dimensional regression. The Journal of Machine Learning Research,
15(1):2869–2909.
Jiang, X., Jiang, J., and Song, X. (2012). Oracle model selection for nonlinear models
based on weighted composite quantile regression. Statistica Sinica, 22(4):1479–1506.
Jordan, M. I., Lee, J. D., and Yang, Y. (2019). Communication-efficient distributed
statistical inference. Journal of the American Statistical Association, 114(526):668–
681.
Knight, P. and Duan, R. (2023). Multi-task learning with summary statistics. In
Advances in Neural Information Processing Systems, volume 36. Curran Associates,
Inc.
Koenker, R. (2005). Quantile regression, volume 38. Cambridge university press.
Koenker, R. (2017). Quantile regression: 40 years on. Annual Review of Economics,
9:155–176.
Lee, S.-h., Ma, Y., and Zhao, J. (2024). Doubly flexible estimation under label shift.
Journal of the American Statistical Association, pages 1–13.
Li, S., Cai, T. T., and Li, H. (2022). Transfer learning for high-dimensional linear
regression: Prediction, estimation and minimax optimality. Journal of the Royal
Statistical Society Series B: Statistical Methodology, 84(1):149–173.
Li, S., Cai, T. T., and Li, H. (2023). Transfer learning in large-scale gaussian graph-
ical models with false discovery rate control. Journal of the American Statistical
Association, 118(543):2171–2183.
Li, S. and Luedtke, A. (2023). Efficient estimation under data fusion. Biometrika,
110(4):1041–1054.
Li, S., Zhang, L., Cai, T. T., and Li, H. (2024). Estimation and inference for
high-dimensional generalized linear models with knowledge transfer. Journal of
the American Statistical Association, 119(546):1274–1285.
Lipton, Z., Wang, Y.-X., and Smola, A. (2018). Detecting and correcting for label
shift with black box predictors. In Dy, J. and Krause, A., editors, Proceedings of
41the 35th International Conference on Machine Learning, volume 80 of Proceedings
of Machine Learning Research, pages 3122–3130. PMLR.
Lu, X. and Fan, Z. (2015). Weighted quantile regression for longitudinal data. Com-
putational Statistics, 30:569–592.
McMahan, B., Moore, E., Ramage, D., Hampson, S., and y Arcas, B. A. (2017).
Communication-efficient learning of deep networks from decentralized data. In Ar-
tificial intelligence and statistics, pages 1273–1282. PMLR.
Negahban, S. N., Ravikumar, P., Wainwright, M. J., and Yu, B. (2012). A Unified
Framework for High-Dimensional Analysis of M-Estimators with Decomposable
Regularizers. Statistical Science, 27(4):538–557.
Pan, S. J. and Yang, Q. (2009). A survey on transfer learning. IEEE Transactions
on knowledge and data engineering, 22(10):1345–1359.
Park, S., Yang, S., Choo, J., and Yun, S. (2023). Label shift adapter for test-time
adaptation under covariate and label shifts. In Proceedings of the IEEE/CVF In-
ternational Conference on Computer Vision (ICCV), pages 16421–16431.
Ruder, S., Peters, M. E., Swayamdipta, S., and Wolf, T. (2019). Transfer learn-
ing in natural language processing. In Proceedings of the 2019 Conference of the
North American Chapter of the Association for Computational Linguistics: Tutori-
als, pages 15–18. Association for Computational Linguistics.
Shao, L., Zhu, F., and Li, X. (2015). Transfer learning for visual categorization: A
survey. IEEE Transactions on Neural Networks and Learning Systems, 26(5):1019–
1034.
Shapiro, S. S. and Wilk, M. B. (1965). An analysis of variance test for normality
(complete samples). Biometrika, 52(3-4):591–611.
Shen, J., Liu, R. Y., and Xie, M.-g. (2020). ifusion: Individualized fusion learning.
Journal of the American Statistical Association, 115(531):1251–1267.
Tian, Y. and Feng, Y. (2023). Transfer learning under high-dimensional generalized
linear models. Journal of the American Statistical Association, 118(544):2684–2697.
42Turki, T., Wei, Z., and Wang, J. T. L. (2017). Transfer learning approaches to
improve drug sensitivity prediction in multiple myeloma patients. IEEE Access,
5:7381–7393.
van de Geer, S., Bu¨hlmann, P., Ritov, Y., and Dezeure, R. (2014). On asymptotically
optimal confidence regions and tests for high-dimensional models. The Annals of
Statistics, 42(3):1166–1202.
Van der Vaart, A. W. (2000). Asymptotic statistics, volume 3. Cambridge university
press.
Wang, L. and He, X. (2024). Analysis of global and local optima of regularized quan-
tile regression in high dimensions: A subgradient approach. Econometric Theory,
40(2):233–277.
Wang, Z., Bu¨hlmann, P., and Guo, Z. (2023). Distributionally robust machine learn-
ing with multi-source data. arXiv preprint arXiv:2309.02211.
Wang, Z., Dai, Z., Poczos, B., and Carbonell, J. (2019). Characterizing and avoiding
negative transfer. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR), pages 11285–11294.
Wei, S., Moore, R., Zhang, H., Xie, Y., and Kamaleswaran, R. (2023). Transfer causal
learning: Causal effect estimation with knowledge transfer. In ICML 3rd Workshop
on Interpretable Machine Learning in Healthcare (IMLH).
Xiong, W. and Tian, M. (2022). Weighted quantile regression theory and its appli-
cation. Journal of Data Science, 17(1):145–160.
Xiong, X., Guo, Z., and Cai, T. (2023). Distributionally robust transfer learning.
arXiv preprint arXiv:2309.06534.
Zhang, C.-H. and Zhang, S. S. (2014). Confidence intervals for low dimensional
parameters in high dimensional linear models. Journal of the Royal Statistical
Society Series B: Statistical Methodology, 76(1):217–242.
Zhang, R., Zhang, Y., Qu, A., Zhu, Z., and Shen, J. (2024). Concert: Covariate-
elaborated robust local information transfer with conditional spike-and-slab prior.
arXiv preprint arXiv:2404.03764.
43Zhang, W., Deng, L., Zhang, L., and Wu, D. (2022). A survey on negative transfer.
IEEE/CAA Journal of Automatica Sinica, 10(2):305–329.
Zhang, Y., Wang, H. J., and Zhu, Z. (2019). Quantile-regression-based clustering for
panel data. Journal of Econometrics, 213(1):54–67.
Zhang, Y. and Zhu, Z. (2023). Transfer learning for high-dimensional quantile regres-
sion via convolution smoothing. arXiv preprint arXiv:2212.00428.
Zhao, J., Zheng, S., and Leng, C. (2023). Residual importance weighted transfer
learning for high-dimensional linear regression. arXiv preprint arXiv:2311.07972.
Zhuang, F., Qi, Z., Duan, K., Xi, D., Zhu, Y., Zhu, H., Xiong, H., and He, Q.
(2021). A comprehensive survey on transfer learning. Proceedings of the IEEE,
109(1):43–76.
448 Supplementary Materials
This appendix has two subsections. Section 8.1 offers the detailed quality control
process forthe debiased estimates obtained through Algorithm 3. Section 8.2 provides
additional simulations for parameter estimation under different scenarios, as well as
implementation details for real data applications.
8.1 Quality control for Debiased Estimators
Algorithm 4: Quality control for Algorithm 3
Input: Same as Algorithm 3.
A) Pre-control: for each source study k , if there has prior knowledge that the
∈ C
(k) (k)
parameter contrast δ is non-negligible, or δ is larger than a prespecified
1 b 1
| |
constant, then we remove this source study. Denote the new detected
e
transferable set .
C
B) Post-control: noremality test with quantile bootstrap.
(1) Conduct the quantile pair bootstrap (Hahn, 1995) with R times on the data
((x(k) ) ,y(k) )nk to obtain the debiased estimates α R .
{ i ⊤ i i=1 }k 0 { r }r=1
∈{ }∪C
(2) Check the normality of α R by Shapiro-Wilk test (Shapiro and Wilk,
r r=1 b
{e }
1965).
b
- If rejecting null hypothesis (p-value < 0.05), we choose to only debias on
the target study by argmin n 0 ψ(0) (α) , where the search region
α ∈A0 | i=1 i |
= α R : α β 10[n 1 Pn 0 (x(0) )2] 1/2/logn , which is exactly
0 1 −0 i=1 bi,1 − 0
A { ∈ | − | ≤ }
the approach in Belloni et al. (20P19).
b
- If not rejecting null hypothesis, conduct Algorithm 3 with .
C
e
Output: Debiased estimate α.
b
458.2 Additional Experimental Results
8.2.1 Real data
We provide the complete name for each target issue, which is shown in Table 3.
Table 3: Complete Name for 13 Target Brain Issues in the Application
Tissue Abbreviation Tissue Abbreviation
brain amygdala Amygdala brain hippocampus Hippocampus
brain anterior cingulate
A.C.cortex brain hypothalamus Hypothalamus
cortex ba24
braincaudatebasalgan- brain nucleus accum-
C.B.ganglia N.C.A.B.ganglia
glia bens basal ganglia
brain cerebellar hemi- brain putamen basal
C.hemisphere P.B.ganglia
sphere ganglia
brain spinal cord cervi-
brain cerebellum Cerebellum S.C.cervical
cal c-1
brain cortex Cortex brain substantia nigra S.nigra
brain frontal cortex ba9 F.cortex
There are some additional points in our practical implementation:
• The gene expression data is pre-processed by filtering out genes with constant
expression levels and applying standard normalization.
• We find that the one-shot calculation in Algorithm 2 may lead to unsatisfactory
results, since the initial target estimator is sometimes unstable. Therefore, we it-
erate the procedure 3 times, which is actually equivalent to performing Algorithm
1 with source detection for T = 3.
• The threshold parameters t , t in Algorithm 1 are set to 4 and 0.7 respectively,
1 2
except that for predicting SH2D2A in tissues C.B.ganglia, Cerebellum, Cortex,
{
N.C.A.B.ganglia , we set t = 10,t = 0.5. The other regularization parameters
1 2
}
follow the same pivotal selection.
• We observe that source data appears less beneficial at τ = 0.5 but seems to be
quite helpful at τ = 0.2. This suggests that the distribution shift between the
target and source studies may vary with the quantile level of model responses.
468.2.2 Simulation under different settings
For the parameter estimation with knowledge transfer, We conduct the additional
experiments under the following three circumstances.
1. Enlarge the source study number K = 20.
2. Increase the parameter shift level h = 20.
1
(k)
3. Let each ǫ ,k follow the t-distribution with freedom 3, i.e, t(3).
i ∈ Ch 2
The rest of the settings remain the same as the configurations in Section 4.2. Here we
still show the average ℓ -error with 100 repetitions for the six comparative methods.
2
The results shown in each figure are similar to the information reported in Section 5.
470.2 0.5 0.7
0.75
0.50
0.25
Method
0.75
L1−QR
Oracle
0.50 TransQR
Pooling
0.25 Oracle_PS
TransQR_2step
0.75
0.50
0.25
5 10 15 20 5 10 15 20 5 10 15 20
Size of C
h1
Figure 8.1: ℓ estimation errors of various methods under homogeneous model with
2
K = 20
48
rorrE
L
2
Cauchy
Mixed
Noisy0.2 0.5 0.7
0.5
0.4
0.3
0.2
0.1
Method
0.5
L1−QR
0.4
Oracle
0.3 TransQR
Pooling
0.2
Oracle_PS
0.1
TransQR_2step
0.5
0.4
0.3
0.2
0.1
5 10 15 20 5 10 15 20 5 10 15 20
Size of C
h1
Figure 8.2: ℓ estimation errors of various methods under heterogeneous model with
2
K = 20
49
rorrE
L
2
Cauchy
Mixed
Noisy0.2 0.5 0.7
1.5
1.2
0.9
0.6
1.5 Method
L1−QR
1.2
Oracle
TransQR
0.9
Pooling
0.6 Oracle_PS
TransQR_2step
1.5
1.2
0.9
0.6
1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
Size of C
h1
Figure 8.3: ℓ estimation errors of various methods under homogeneous model with
2
η = 20
50
rorrE
L
2
Cauchy
Mixed
Noisy0.2 0.5 0.7
1.25
1.00
0.75
0.50
0.25
Method
1.00 L1−QR
Oracle
0.75 TransQR
Pooling
0.50 Oracle_PS
TransQR_2step
0.25
1.25
1.00
0.75
0.50
0.25
1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
Size of C
h1
Figure 8.4: ℓ estimation errors of various methods under heterogeneous model with
2
η = 20
51
rorrE
L
2
Cauchy
Mixed
Noisy0.2 0.5 0.7
0.8
0.6
0.4
1.0 Method
L1−QR
0.8
Oracle
TransQR
0.6
Pooling
0.4 Oracle_PS
TransQR_2step
1.0
0.8
0.6
0.4
1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
Size of C
h1
Figure 8.5: ℓ estimation errors of various methods under homogeneous model with t
2
distributions
52
rorrE
L
2
Cauchy
Mixed
Noisy0.2 0.5 0.7
0.6
0.4
0.2
0.8
Method
L1−QR
0.6
Oracle
TransQR
0.4
Pooling
Oracle_PS
0.2
TransQR_2step
0.8
0.6
0.4
0.2
1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
Size of C
h1
Figure 8.6: ℓ estimation errors of various methods under heterogeneous model with
2
t distributions
53
rorrE
L
2
Cauchy
Mixed
Noisy