Tract-RLFormer: A Tract-Specific RL policy
based Decoder-only Transformer Network
Ankita Joshi1, Ashutosh Sharma1, Anoushkrit Goel1, Ranjeet Ranjan Jha2,
Chirag Ahuja3, Arnav Bhavsar1, and Aditya Nigam1
1 Indian Institute of Technology (IIT) Mandi, India
2 Indian Institute of Technology (IIT) Patna, India
3 Post-Graduate Inst. of Medical Edu. and Research (PGIMER), Chandigarh, India
Abstract. Fiber tractography is a cornerstone of neuroimaging, en-
ablingthedetailedmappingofthebrain’swhitematterpathwaysthrough
diffusion MRI. This is crucial for understanding brain connectivity and
function, making it a valuable tool in neurological applications. Despite
its importance, tractography faces challenges due to its complexity and
susceptibility to false positives, misrepresenting vital pathways. To ad-
dress these issues, recent strategies have shifted towards deep learning,
utilizingsupervisedlearning,whichdependsonprecisegroundtruth,or
reinforcement learning, which operates without it. In this work, we pro-
poseTract-RLFormer,anetworkutilizingbothsupervisedandreinforce-
ment learning, in a two-stage policy refinement process that markedly
improves the accuracy and generalizability across various data-sets. By
employing a tract-specific approach, our network directly delineates the
tractsofinterest,bypassingthetraditionalsegmentationprocess.Through
rigorousvalidationondatasetssuchasTractoInferno,HCP,andISMRM-
2015, our methodology demonstrates a leap forward in tractography,
showcasingitsabilitytoaccuratelymapthebrain’swhitemattertracts.
Keywords: Tractography · Transformers · Reinforcement Learning
1 Introduction
Tractography is an advanced reconstruction technique in neuroscience, that
leveragesdiffusionMRItocreatedetailedvisualrepresentationsofbrain’swhite
matter pathways. This technology has played a crucial role in assisting neuro-
surgeonswithmeticulouspre-surgicalplanning,benefitingpatientswitharange
of neurological disorders [7], by enabling a deeper analysis of the white mat-
ter. Over the years, a range of tractography algorithms have been developed,
to map critical neurological pathways. Deterministic algorithms [2] trace fiber
paths directly based on the most probable direction of water molecule diffu-
sion, offering clear but sometimes oversimplified views of white matter tracts.
In contrast, probabilistic algorithms [4] incorporate the inherent uncertainty in
diffusion data to predict multiple potential pathways, resulting in detailed fiber
reconstruction. Global algorithms [8] attempt to reconcile the deterministic and
4202
voN
8
]GL.sc[
1v75750.1142:viXra2 A. Joshi et al.
probabilisticapproachesbyoptimizingwhole-braintractographyreconstructions
to capture the complex architecture of brain connectivity.
Despite these advancements, tractography still faces challenges such as the
crossing-fibers issue (also known as the bottleneck phenomenon) [12] due to its
ill-posed nature. These issues arise because the algorithms rely on local diffu-
sion information to reconstruct the brain’s complete fiber network, occasionally
resulting in erroneous projection of fiber pathways or false positive connections.
To overcome these obstacles, recent research employs machine learning and
deeplearning(DL)approachestoenhancetractographyaccuracy.SupervisedDL
techniques [15,3] for tractography rely on accurate and comprehensive ground
truthdatatotrainandvalidatethealgorithms,whichisverydifficulttoobtain.
In this regard, recent works [21,20] have proposed deep reinforcement learning
(DRL)-basedapproaches,thatlearntoperformtractographybyinteractingwith
theenvironment.Thesetechniques,leveragingdeepneuralnetworks,enhancethe
ability to predict brain fiber configurations, promising significant advancements
infibermappingqualityforneurologicalresearchandclinicalapplications.How-
ever, improving tractographyalgorithms for effective useacross diverse datasets
remains a challenge for further research in the field.
Recently, transformers have shown remarkable performance in various do-
mains, including language modeling[23], image recognition[6], time series fore-
casting[27],andevenproteinstructureprediction[11].Theirrobustperformance
in various sequence prediction tasks demonstrates their ability to capture long-
range dependencies and contextual information effectively, making them well-
suited for mapping neural pathways in tractography. Building on their success
in related fields, we now extend the generalization, transfer learning, and au-
toregressive capabilities of transformers (GPT), to the tractography domain in
anovelhybridframework.WeadoptanRLframework([5])togeneratetrain-
ing data for our GPT model, Tract-RLFormer, reducing the need for extensive
ground-truthtypicallyneededtotraintransformers.Thisaddressesasignificant
challenge of applying transformers for tractography where ground-truth fibers
are very difficult to obtain.
This approach also represents a significant departure from traditional meth-
ods,asitsimplifiesthetractographyprocessbytargetingspecifictracts,thereby
eliminatingtheneedforcomplexandoftencumbersomesegmentationalgorithms
employed post-tractography. Moreover our data-driven approach has the poten-
tial to utilize data from RL agents trained across diverse neuroimaging environ-
ments. Our key contributions are as follows:
1. Datadrivenpolicylearningviahybridframework:WeproposeTract-
RLFormer, a GPT-based network trained by leveraging both reinforcement
learning (RL) and supervised learning (SL) paradigms, to approximate and
refine a policy for tract generation that outperforms recent RL-algorithms.
2. Innovative Tract-Specific Generation: We train Tract-RLFormer to
generatethetractofinterest,utilizingourdevelopedMaskRefinementMod-
ule (MRM) to generate tracking masks for the target tract, bypassing seg-
mentation overhead.Tract-RLFormer 3
3. Generalization: Through extensive testing on diverse datasets (TractoIn-
ferno, HCP, ISMRM2015), we demonstrate our network’s superior perfor-
manceandgeneralizationcapabilitiesacrossdifferentneuroimagingcontexts.
2 Related Work
Research in fiber tractography has transitioned from traditional deterministic
andprobabilisticmethodstomachinelearninganddeepreinforcementlearning.
Supervised learning and the exploratory dynamics of deep reinforcement learn-
ing unlock several possibilities for accurately mapping the brain’s connectivity.
Below, we review some recent works in these paradigms.
Supervised Machine Learning based Algorithms: In several studies,
machine learning techniques have been explored to enhance fiber tractography
with promising results. Notably, [13,14] utilized a Random Forest classifier in
a supervised learning setting to identify 25 distinct fiber bundles, leveraging
data from the ISMRM2015 dataset [12]. The effectiveness of their approach
wasassessedusingtheTractometertool,demonstratingtheclassifier’sabilityto
accurately distinguish between different fiber pathways. Building on this foun-
dation, subsequent research shifted focus towards regression-based methods for
fibertracking.[15]suggestedtoemployaGated-RecurrentUnit(GRU)modelto
predictnewtrackingstepsfromdiffusionsignalresampledto100directions.This
method advances the field, moving beyond traditional classification techniques
toofferamorenuancedunderstandingoffibertractdevelopment.Advancingthe
understanding of deep learning’s potential for tractography, [3] applied both de-
terministic and probabilistic approaches to the task. In [25], authors introduced
an innovative method known as iFOD3, utilizing a feed-forward neural network
to analyze raw, resampled signals. This approach considers the spatial context
of streamlines, incorporating seed points located at the interface between white
and gray matter—a notable departure from conventional methods that focus
solely on white matter. This broader perspective on seed point placement con-
tributed to the method’s enhanced performance. In a subsequent development
in [24], authors presented a probabilistic machine learning model that outputs
Fischer-von-Mises distributions rather than deterministic paths. This approach
marked improvement over previous techniques, offering a more accurate and
effective means of mapping the intricate networks of brain fiber tracts. These
advancements underscore the rapidly evolving landscape of fiber tractography,
highlighting the critical role of machine learning and deep learning in pushing
the boundaries of neuroimaging research.
ReinforcementLearningbasedAlgorithms:Contrarytothesupervised
trainingcommoninmachineanddeeplearningapproaches(poseschallengesdue
tothedifficultyofgeneratinglargescalegroundtruthdata),theauthorsexplored
reinforcementlearning(RL)-basedapproachforfibertractographyin[21].Inthis
approach, tractography is conducted similar to classical methods, wherein a re-
ward function is employed by a learning model to generate streamlines based
on local fiber orientation. Unlike the supervised paradigm for tractography, the4 A. Joshi et al.
RL-based model does not utilize reference streamlines while training. In [21],
the Twin-Delayed Deep-Deterministic Policy Gradient (TD3) algorithm [9] and
the Soft Actor-Critic (SAC) algorithm were employed for RL-based fiber trac-
tography to reduce false positives and enhance model generalization. In [20],
the authors further examined different aspects of the RL framework, such as
algorithm choice, seeding strategies, state representation, and reward functions,
paving the way for advancements in this domain.
3 Proposed Methodology
Inthissection,webeginbydiscussingthedataanditspreprocessing,followedby
a systematic presentation of our proposal. We utilize three public diffusion MRI
datasets (Table 1). These datasets include a series of diffusion weighted images
Table 1. Description of the three public DWI Datasets
Dataset Subjects DWI data Distortion Corrections
TractoInferno 284 b=1000 s/mm2; resolu-N4 bias field; eddy-current;
[16] tion=1mm isometric head-motion
HCP [22] 1200 b=1000/2000/3000 s/mm2;EPI; eddy-current; subject-
270 directions; resolution=motion
1.25mm isometric
ISMRM [12] 1 b=1000s/mm2;32directions;eddy currents; head motion
resolution=2mm isometric (by our preprocessing)
(DWI) that capture the diffusion of water molecules in tissue. Each voxel in a
DWIcontainsinformationaboutthemagnitudeanddirectionofwaterdiffusion,
reflecting the underlying tissue micro-structure.
Diffusion MRI Pre-processing: We process DWI data to extract crucial
information, including Spherical Harmonics Coefficients (SHC), Fiber Orienta-
tion Distribution Functions (fODF), and fiber peaks. Initially, the DWI data
is projected into an 8th order spherical harmonics basis, yielding 45 SHC vol-
umes. The fODF, representing the distribution of fiber orientations within each
voxel, is then computed, providing essential local information regarding stream-
lineorientation.Subsequently,usingthefODF,localfiberdirections(peaks)are
computed,whichareusedtodefinetherewardfunctionfortrainingnetworksin
the RL framework, as elaborated in Section 3.2.
Moreover, in traditional tractography methods, white matter masks are typ-
ically derived from DWI data to perform whole-brain tractography, followed by
segmentation of specific tracts. In contrast, we generate tailored masks for each
tract, as detailed in Section 3.1. Our models are trained and tested within these
masks, allowing for precise and efficient tract-specific analysis.
We propose an iterative policy learning framework for tract-specific generation,
delineated as a five-step process (see Fig. 1). In this framework, we start byTract-RLFormer 5
training an RL agent (TD3) to learn a policy by exploration (within the track-
ing mask) to generate a tract of interest. We call it as level-1 policy. Using this
initial policy, the agent interacts with the (tracking) environment by taking ac-
tions (tracking steps). The agent’s experience (policy rollouts) is collected and
sampled to train a refined version of the policy, by our T-RLF model, which
learns in a data-driven manner through general pre-training and tract-specific
fine-tuning. Our study focuses on seven principal white matter (WM) tracts:
Corpus Callosum (CC), left and right Pyramidal (PYT), Arcuate Fasciculus
(AF), and Cingulum (CG) Tracts. The selection of these seven tracts is based
on their clinical significance and frequent analysis as suggested in [18,20]. To
conduct such tract-specific training and generation, we first compute a tracking
regionofinterest(mask)tailoredforeachtractusingourMaskRefinementMod-
ule(MRM), describedin3.1.Followingthis, weproceedwiththe fivesequential
steps depicted in Fig. 1, detailed in subsequent subsections of the methodology.
Fig.1. Overview of the proposed Iterative Policy Learning for Tract-Specific Genera-
tionusingDWIdata.(a)AnRLagent(π )interactswiththeenvironment(E)tolearn
θ
anoptimallevel-1policy(π ).(b)Thispolicyisusedtogeneratetract-specificroll-
θopt
outs, denoted as ’experience replay’. (c) and (d) illustrate the offline, auto-regressive
trainingoftheproposedTract-RLFormerϕ,referredtoasT-RLF,overtheseroll-outs.
In (c), T-RLF undergoes general pre-training, while in (d) it is fine-tuned to learn an
optimaltract-specificpolicy(π ).(e)showsthetestingphase,whereT-RLF,which
ϕopt
has learned the new level-2 policy (π ), performs tracking in environment E to
ϕopt
producethedesiredtract.Trainingandtrackingstepsareshowninyellowandorange
backgrounds, respectively.
3.1 Mask Refinement Module (MRM)
We combine reference tracts from 2 Atlases, namely HCP842 [26], and Re-
cobundlesX [17] to develop a fiber template for each of the seven tract classes.6 A. Joshi et al.
To obtain the mask of a given tract for any subject, the template fibers of the
tractarealignedtothesubject’sbrainspace[1],creatinganinitialmaskwhichis
thendilatedby5millimeterstogetanaugmentedregionofinterest(ROI).This
ROIisfurtherrefinedbyourMaskRefinementModule(MRM),whichproduces
a tracking mask for a specific tract utilizing the fiber orientation information of
the given subject. It consists of a fully connected neural network (FCNN) that
refines the augumented ROI to obtain an estimate of the ground-truth mask for
a given subject. The process starts with a larger mask and progressively refines
itbyeliminatingitsvoxelsbasedontheSphericalHarmonicsCoefficients(SHC)
in the local neighborhood. The input for each voxel is the SHC (45 per voxel) of
thevoxelitselfanditssiximmediateneighbors,concatenatedwiththeexpanded
mask values, resulting in an input size of 322 (7 * 46).
The neural network architecture comprises three hidden layers with 512,
256, and 128 neurons, respectively. Each layer employs a ReLU activation func-
tionandisfollowedbybatchnormalizationandadropoutlayer(0.5).Theoutput
layer uses a sigmoid activation function, which determines the probability of re-
tainingeachvoxelintherefinedmask.Voxelswithanoutputprobabilitygreater
than0.5arekeptinthepredictedmask,whilethosewithlowerprobabilitiesare
eliminated. Training is performed voxel-wise, using Binary Cross Entropy as
the loss function to compare the predicted mask value with the ground truth
for each voxel. The model was trained with 50 subjects randomly selected from
the TractoInferno dataset over 100 epochs. The resulting mask is then dilated
by 1mm to produce the final refined tracking mask for the given subject.
3.2 RL Policy Learning
We learn a Level-1 policy by training a reinforcement learning (RL) agent (π )
θ
to perform fiber tracking. The RL agent learns the policy through exploration
within the tracking environment (E) (see Fig. 1 (a)).
Environment Details: Adopting the RL framework from [21], we train
an RL agent within the 3D diffusion MRI voxel space. The training process
starts from seed voxels chosen within a 3D tract-specific mask (M), obtained
fromMRM.Atanygivenvoxel,theenvironmentpresentsstate(s )totheagent
t
and rewards the agent’s actions based on their alignment with the fODF peak,
aidinginthelearningoftheoptimizedpolicyπ .Thetrackingcontinuesuntil
θopt
the streamline exits the mask (M), surpasses a maximum length (l), or deviates
significantly (>60°) from the previous tracking direction.
The state (s ) is defined by 45 spherical harmonic (SH) coefficients and
t
trackingmask(M)valuesfromthecurrentandsixneighboringvoxels,alongwith
thefourprevioustrackingdirections,amountingto334dimensions(7×(45+1)+
3×4). The predicted action (a ) is a 3D vector representing tracking/fiber di-
t
rection. The action space of the environment is continuous, allowing the agent
toexploreawiderangeofpotentialfiberdirections,withvaluesintherange[-1,
1].Thereward (r )attime-steptisgivenbytheabsolutedotproductbetween
t
the agent’s predicted action (a ) and the closest fODF peak (p ), weighted by
t iTract-RLFormer 7
thedotproductoftheaction(a )withtheagent’sprevioustrackingstep(u )
t t−1
(defined below).
(cid:12) (cid:12)
r
t
=(cid:12) (cid:12)max(p i·a t)(cid:12) (cid:12)×(a t·u t−1) (1)
(cid:12) pi (cid:12)
Training details: During the agent’s exploration phase, the transitions
(s,a,r,s′)arerecordedinareplaybufferforbatch-wisepolicyoptimization.We
utilize the TD3 algorithm to train 7 tract-specific agents. It has an Actor and
two Critic networks (along with their time delayed target networks).
The actor and critic networks are both fully-connected neural networks with
two ReLU activated hidden layers of 1024 neurons each. The actor has a 334
dimensional input layer and 3-neuron tanh activated output layer, while the
critic has a 337 dimensional input layer and a single neuron tanh output layer
(similarto[21]).Eachtract-specificRLagentistrainedonfivesubjectsfromthe
TractoInferno dataset (1030, 1079, 1119, 1180, and 1198), for 50 batches (4096
episodes each) per subject, hence a total of 1,024,000 (250*4096) episodes. We
train the TD3 agent in 5 different instances of the environment (E) specified by
each subject’s distinct diffusion data, fODF peaks, and tracking mask. Training
is conducted at 7 seeds per voxel and a step-size of 0.375mm, with fiber lengths
between 20mm and 200mm. Maximum possible episode length is set to 530
(200/0.375). Other hyper-parameters include: learning rate: 8.56e-06, Discount
factor (γ): 0.776, and Exploration noise (σ ): 0.334.
train
3.3 T-RLF: Policy Refinement
This subsection involves the training steps of our T-RLF model. We train a
GPT-based network, to learn a refined, level−2 policy (π ) for tract-specific
ϕopt
fiber generation. It is trained on the policy rollouts of the level−1 TD3 policy
(π ) (ref: Section 3.2) to interpret and generate fiber data within the agent’s
θopt
experiencespace.Thisisaccomplishedthroughatwo-stageprocess:(a)Initially,
theTract-RLFormerundergoesageneric,tract-agnosticpre-training.(b)This
is followed by fine-tuning for the downstream task of tract-specific generation.
Together, these constitute the next three steps (out of five), namely training
data generation (Fig. 1(b)) and the two-stage training process (Fig. 1(c, d)) of
T-RLF. Each component of the training framework is discussed in detail below.
Unlikepriormethodsthatgeneratefiberpointsbytrainingondiffusioninfor-
mation along ground truth fiber streamlines, our network, T-RLF, learns from
the sequence of state-action-reward (s, a, r) tuples (policyroll-outs) of a trained
RL agent (Fig. 2). T-RLF is trained on trajectories derived from the policy
roll-outs of seven tract-specific TD3 agents. Each trajectory is represented as
τ = (R ,s ,a ,R ,...,R ,s ,a ), where R is the scalar sum of rewards
0 0 0 1 T T T t
from time-step t to the episode’s end, s is a 334-dimensional state vector, and
t
a is a 3-dimensional action (see Section 3.2).
t
Training Data Generation: To generate training data trajectories, we
initiate tracking for the 7 trained TD3 agents (see Section 3.2) on 5 training8 A. Joshi et al.
Fig.2. Data Representation for T-RLF: Tract specific policy refinement using a
trajectory-based approach in an RL agent’s experience space. The figure illustrates
a k length fiber streamline f in human brain voxel space, represented as a trajectory
τ =(R ,s ,a ,R ,s ,a ,.....,R ,s ,a ). Each point in the streamline corresponds to
0 0 0 1 1 1 k k k
a state, action, and return-to-go tuple at a time-step t.
subjects from the TractoInferno dataset. For each of the 7 tracts, we save all
tracking episodes (until termination) as (R,s,a) sequences, called trajectories.
Tracking is conducted for all 5 subjects within their tract-specific masks using 7
seeds per voxel, resulting in a total of (cid:80)5 7×n tract-specific trajectories
s=1 vs,i
for the ith tract, where n is the number of voxels in the ith tract’s mask
vs,i
for the sth subject. From these, 50,000 trajectories are selected per tract, with
10,000 from each subject. Half of these (5,000) are the longest trajectories for
that subject’s ith tract, while the other half represent the streamline variability
ofthetract.Thisyieldsatract-specificdatasetτ foreachtractiusedformodel
i
fine-tuning for downstream tasks. From the 350,000 (7 x 50,000) trajectories of
theseventract-specificdatasets(τ fori=1to7),atotalof150,000trajectories
i
areselected.Half(75,000)ofthesearethelongesttrajectories,andtheotherhalf
are randomly selected, resulting in a mixed tract dataset τ used for generic
mix
tract-agnostic pre-training.
Itshouldbenotedthatn varieswiththetractandsubject.n isthetotal
vs,i vs,i
number of voxels within the tracking mask for tract i (M ) when aligned to the
i
space of subject s. Moreover, the minimum and maximum length of trajectories
in the τ dataset (representative of all tracts) are 48 and 292 respectively. It
mix
is later used to determine the training parameter of GPT model.
Model Architecture: Tract-RLFormer adopts the GPT architecture (as
shown in Fig. 3) tomodel trajectories autoregressively [5].Thenetwork consists
of 4 decoder layers with 1 attention head each (n_heads=1), a context length
(K) of 40, an embedding dimension (d) of 128, ReLU activation functions, and
a dropout rate of 0.1. These parameters were selected after thorough experi-
mentation presented in section 4.3. It begins with a dedicated embedding layer
of 128 dimensions (as shown in Fig. 3) for each component of the trajectory:Tract-RLFormer 9
Fig.3.DataDrivenPolicyLearning:VisualrepresentationoftrainingTract-RLFormer
for action prediction at time-step t, using context information from K length fiber
(Section3.3).Theinputsequencetuples<R,s,a>arecausallymaskedfroma onwards
t
and processed through embedding layers emb , emb , and emb , with a learnable
R s a
positionalencodinglayer(PE).EmbeddingsareprocessedbyLdecoderblocks(L=3
for pre-training, L = 4 for fine-tuning), incorporating Multi-Head Attention (MHA)
and Multi-Layer Perceptron (MLP), to generate predicted action aˆ.
t
state (s), action (a), and return-to-go (R). Subsequently, a trainable positional
encoding layer processes the timestep sequence (of max_ep_len) as input, gen-
erating positional/timestep embeddings of dimensionality d = 128, where each
timestep (t) has 3 tokens <R , s , a >. The maximum possible episode length
t t t
(max_ep_len)controlslengthofepisode.Itissetto530becausethemaximum
lengthofafiberis200mm,equivalentto530stepsforaTractoInfernosubject(as
1 step corresponds to 0.375 mm; refer 3.4). If an episode exceeds 530 timesteps,
it is truncated to this length. Embeddings for each component of the trajectory
(state, action, return-to-go) are then combined and fed into the decoder lay-
ers. We utilize four decoder blocks, where each block includes a multi-headed
self-attentionmechanismfollowedbyposition-wisefeed-forwardnetworks.After
processing through the decoder blocks, the output is passed through an output
embeddinglayer,fromwhichweobtainthepredictedactionofdimension(3,1).
Training Details: The proposed T-RLF model is trained to generate an
optimal level-2 policy, (π ), specifically tailored for tract-specific generation.
ϕopt
It undergoes a two-stage training process, starting with general pre-training
on mixed tract dataset (τ ), followed by tract-specific fine-tuning on tract-
mix
specific dataset (τ ). The first three decoder layers are pre-trained over 0.15
i
million mixed trajectories (taken from τ ), containing a total of 30 million
mix
transitions for 30 iterations. Later the 4th decoder layer is fine-tuned on the
tract-specific trajectories buffer for 10 additional iterations. In each iteration,10 A. Joshi et al.
the model undergoes 10,000 training steps, each processing a batch_size= 128
number of K-length trajectories. A batch of 128 tokens of < R ,s,a > are
t
sampled from training data (τ) and stacked for a context length (K = 40)
and fed as an input to the T-RLF. It passes through an embedding layer with
128 dimensions, and positional encoding is added, resulting in a (128 x 120 x
128) matrix and is processed by the 4 decoder layers with causal masking (Fig.
3).Thedecoderoutputismappedthroughanoutputembeddinglayertopredict
theaction.UnliketheTD3agent,T-RLFdoesnotinteractwiththeenvironment
duringitstrainingprocess.Instead,itistrainedentirelyinanofflinemodeusing
onlytrajectorydatasets(τ’s).ForthecontextlengthK,a5-steploss(accounting
for current and 2 steps in both forward and backward directions) is computed,
aggregating the angular difference between predicted and actual action at each
time-step.
K−2(cid:32) 2 (cid:33)
(cid:88) (cid:88)
L= cos−1(a ·aˆ ) (2)
t+i t+i
t=2 i=−2
The learning of weights for π is facilitated by this 5-step loss function, in
ϕopt
order to generate more effective and robust actions. Here, a and aˆ are the
t+i t+i
true and predicted actions at (t+i)th timestep respectively.
Similar to [5], T-RLF training is conditioned to generate action (a ) using
t
return (R ) at each timestep. During inference, R is initialized to an expert
t t
return value or the longest trajectory return. In our case, the longest trajectory
length is 292, and since the maximum possible reward at each timestep is 1, we
initializeR to300(∼1xexpertreturn).Thiswasexperimentallyverifiedamong
t
variousvalues:100,200,300,500,and600.Formodeltraining,weemployedthe
AdamW optimizer, set with a learning rate of 1e-4 and a weight decay of 1e-4.
3.4 T-RLF: Inference
The final step in our fiber tract generation method involves using the trained
T-RLF models to perform tracking, followed by cleaning the resulting tracts.
Having learnt the refined policy (π ), T-RLF can function autonomously as
ϕopt
a generic substitute for TD3 agent. Consequently, it can independently
perform fiber streamline generation in the same environment (E) as
detailed in section 3.2, without relying on the original TD3 agent. Fiber genera-
tion (tracking) is executed within tract-specific masks obtained from MRM and
is initialised with 7 seeds per voxel, and R is set to R = 300. Tracking step
t 0
size is (empirically selected) and is dataset-specific, 0.375mm for the TractoIn-
ferno,0.468mmforHCP,and0.75mmfortheISMRMdataset.Ateachstep,the
return-to-go (R ) is reduced by the achieved reward and predicted action(a ),
t t
newstate(s′),andR areappendedtothecontextwindowtoserveasinputfor
t t
the next prediction. This auto-regressive process by Tract-RLFormer generates
the fiber tract of interest. Finally, the tracts undergo a Cleaning procedure
using a fast streamline search (FSS) [19] to eliminate any extraneous fibers, by
comparing the predicted tract with the atlas reference tracts (representing gen-
eral anatomical structure). Our tracts are confined to masks generated by theTract-RLFormer 11
MRMmodule,tailoredtoeachsubject’sfiberorientation.Thisapproachensures
thattractgenerationremainsconfinedtoregionsproximatetotheactualneural
fibers of the subject, thus mitigating the risk of false positives. Consequently,
we can perform a high radius search using FSS, without incurring a major risk
of high overreach. This high radius search ensures that accurate fibers are not
discarded based on minor deviations from atlas tracts.
Performance Parameters: In order to evaluate the quality of our genera-
tion,theGroundTruthtractisalignedtoMontrealNeurologicalInstitute(MNI)
space using Advanced Normalization Tools (ANTs) [1], facilitating comparison
with our cleaned tracts that are already in MNI space. The Dice (D), Over-
lap (OvL), and Overreach (OvR) scores (similar to [21,20]) are then computed
against the ground truth tract and are reported in Section 4. The Dice score as-
sessesboththeaccuratecoverageandtheminimizationofextraneousextensions
beyondthegroundtrutharea,wherevaluesnear1signifyahighsimilaritylevel.
Overlap measures the intersection of the generated tract with the ground truth,
while Overreach indicates how much the generated tract exceeds the ground
truth, with lower scores suggesting greater precision.
4 Results & Discussion
In this section, we present the outcomes of our evaluation of tract-specific T-
RLFmodelsundervariousexperimentalsetups,includingcomparativeanalysis,
generalizationperformance,andanablationstudy.WetrainedTD3andT-RLF
models, on eight tracts— seven principal white matter tracts (refer Section 3)
andORtract(foranalysisin4.1)usingfivetrainsubjects(id:1030,1079,1119,
1180,and1198)oftheTractoInfernodatasetandreportedtheirperformanceon
various test subjects across different datasets in subsequent subsections. Ad-
ditionally, we assess their effectiveness relative to supervised approaches and
traditional tractography methods that do not incorporate learning.
4.1 Comparative Analysis
This section provides a comparative analysis of our model, T-RLF, against su-
pervised learning, traditional tractography, and state-of-the-art (SOTA) rein-
forcement learning (RL) algorithms, using Dice scores to evaluate performance
across three major white matter bundles: PYT,OR, and CC. As presented in
Table2,allmethodsaretestedonsubject 1006fromtheTractoInfernodataset
(similar to [21,20] for fair comparison). For the first and third tabular subparts
of Table 2, the models are trained on ISMRM data. The second subpart does
not involve training (classical methods). These 3 subparts are assessed using
whole-brain tractography and segmentation [16][20]. Additionally, the last sub-
part details the performance of our T-RLF and the TD3 model, where T-RLF
was specifically trained on trajectories derived from the TD3 agent.
In Table 2, our framework outperforms the state-of-the-art method (PFT)
forPYTandORtracts,demonstratingitsrobustnessintract-specifictractogra-12 A. Joshi et al.
Table 2.ComparisonofmeanDicescoresfortheOR,PYT,andCCtractsforsubject
1006 from TractoInferno dataset. Supervised learning scores are from [16]; RL-based
scores,withstd.dev.,arefrom[20].Thelast2rowsincludesscoresforT-RLFandTD3,
evaluatedusingourtract-specificapproach.Thehighestandsecondhighestscoresare
highlightedingreenandred,respectively.‘*’denotestract-specificsettingformethods.
Algorithm OR PYT CC
DET-SE 0.569 0.665 0.658
DET-Cosine 0.598 0.708 0.646
Prob-Sphere 0.599 0.695 0.648
Prob-Gaussian 0.542 0.723 0.668
Prob-Mixture 0.436 0.522 0.614
DET 0.516 0.475 0.345
PROB 0.549 0.740 0.590
PFT 0.644±0.136 0.753±0.010 0.827 ±0.008
VPG 0.369±0.135 0.434±0.128 0.428±0.182
A2C 0.225±0.108 0.323±0.082 0.222±0.025
ACKTR 0.397±0.171 0.559±0.028 0.584±0.054
TRPO 0.330±0.154 0.498±0.062 0.594±0.048
PPO 0.440±0.187 0.619±0.042 0.650±0.028
DDPG 0.612±0.063 0.630±0.045 0.731±0.006
TD3 0.555±0.097 0.603±0.045 0.688±0.035
SAC 0.598±0.098 0.658±0.028 0.753 ±0.010
SAC Auto 0.608±0.088 0.655±0.032 0.747±0.019
DET∗ 0.648 0.752 0.713
PROB∗ 0.652 0.765 0.731
TD3∗ 0.644 0.764 0.720
T-RLF (Ours) 0.673 0.772 0.738
phy. Additionally, T-RLF shows comparable performance to state-of-the-art RL
algorithms for CC tract.
Furthermore, the TD3 agent demonstrates markedly improved performance
within our tract-specific generation framework. Testing of tract-specific TD3 on
the ISMRM or HCP datasets cannot be conducted due to the absence of the
evaluated tracts in these datasets. However, the enhancement in TD3’s perfor-
mance in our tract-specific setting can be attributed to the training approach
rather than dataset consistency. This is evidenced by TD3’s comparable or su-
perior performance on different tracts across the ISMRM and HCP datasets, as
detailed further in Tables 3, 4.
Moreover,dicescoresforDETandPROBimprovedforalltractsinthetract-
specific setting, especially for CC, where DET increased by 106.67% (0.345 to
0.713) and PROB by 23.89% (0.590 to 0.731). The enhanced tracking perfor-
mance of DET and PROB, despite not being trained, is indicative of the ef-
fectiveness of our tract-specific masks. Also, in the whole-brain setting, there is
a huge difference between DET (0.475) and PROB (0.740) scores on the PYT
tract(Table2),whereas thisgapissignificantlysmallerinthe tract-specificset-
ting (marked with ‘*’), where the tract-specific performance of DET∗ (0.752)Tract-RLFormer 13
and PROB∗ (0.765) align closely with each other and with the T-RLF and TD3
methods. The consistency and stability observed for these classical methods are
attributed to our tract-specific approach.
4.2 Generalization Performance Evaluation
Inthissection,wepresenttheperformanceevaluationofourT-RLFmodelacross
threedistinctdatasets(Tables3,4),demonstratingitseffectivenessandgeneral-
izability. The averaged results include analyses across five test subjects in the
TractoInferno (TtoI) dataset (id: 1160, 1078, 1159, 1061, and 1171), four from
the HCP dataset (id: 930449, 992774, 959574, and 987983), and one from the
ISMRM dataset. A visual comparison across datasets and subjects is presented
in Fig. 4(a). We also compare the performance of T-RLF with classical algo-
rithms, which were employed using tract-specific masks, and the tract-specific
TD3 agent, from which the training data for T-RLF was derived (Fig. 4(b)).
Fig.4. Visual comparison of reconstructed tracts illustrating (a): Intra-dataset vari-
ability, Inter-dataset variability, and (b): Variability across tracts reconstructed by
differentalgorithms.ThedepictedtractsincludetheleftPYT,CG,andapartofCC.
ThealgorithmsevaluatedinbottomsectionoffigureareT-RLF(ours),TD3,andPFT.
InTable3,weseethatT-RLFmodeldisplaysanotablegeneralizationperfor-
mance.Interestingly,theclassicaldeterministic(DET)andprobabilistic(PROB)
methods exhibit slightly better performance than learnable methods in some
cases (Tables 3,4).14 A. Joshi et al.
Table3.Performancemetrics(in%)fortheCGandAFtracts,trainedontheTractoIn-
fernodatasetandtestedacrossmultipledatasetstoevaluategeneralization.Trackingis
performed using our proposed tract-specific generation method. A dash (‘-’) indicates
theabsenceofground-truthtractsinthecorrespondingdataset,precludingevaluation.
Cingulum (CG) Arcuate Fasciculus(AF)
Left Right Left Right
DatasetAlgo. Dice OvL OvR Dice OvL OvR Dice OvL OvR Dice OvL OvR
T-RLF 53.3 42.5 16.6 45.6 33.7 13.9 61.8 51.2 13.4 41.8 27.9 5.40
TD3 53.0 42.3 16.9 45.2 33.6 14.3 61.6 51.0 13.7 41.6 27.7 5.60
HCP DET 55.2 46.4 21..5 52.6 41.7 16.3 62.8 52.5 14.0 43.9 30.0 6.6
PROB 57.6 51.8 27.9 56.1 45.5 16.6 65.5 57.9 18.3 47.4 33.7 8.6
PFT 67.3 55.2 7.8 59.6 45.4 6.4 71.3 71.9 29.7 69.9 71.6 33.3
T-RLF 61.0 56.8 28.6 56.5 52.6 34.9 52.7 45.1 27.8 39.5 36.5 49.8
TD3 60.0 55.1 27.3 54.9 49.8 32.4 51.8 44.3 28.2 38.4 34.9 46.9
TtoI DET 61.2 58.8 32.3 58.2 54.7 33.7 54.6 46.3 24.7 45.4 41.7 46.9
PROB 67.9 69.1 33.6 64.7 64.6 36.7 62.3 57.2 27.2 50.3 48.3 50.9
PFT 55.9 48.8 25.4 54.5 51.3 38.6 62.8 60.1 31.5 53.9 62.8 88.0
T-RLF 54.2 46.6 25.5 52.8 44.1 23.1 - - - - - -
TD3 53.1 44.8 23.7 51.2 41.6 21.1 - - - - - -
ISMRM DET 57.5 51.9 28.5 57.7 52.4 29.2 - - - - - -
PROB 61.1 59.3 35.1 64.0 65.4 39.0 - - - - - -
PFT 55.4 49.4 28.9 57.3 49.4 22.9 - - - - - -
As previously mentioned in section 4.1, the consistency observed in Tables
3,4 for the classical methods (DET and PROB) is due to our tract-specific ap-
proach.Thisimprovementandstabilizationmaybeattributedtotheelimination
of premature termination issues in narrow and deep WM regions, as described
in [10], facilitated by the refined spatial exploration enabled by MRM in our
tract-specificapproach.ItcanbeobservedfromTables2and4,thattheperfor-
mance of PFT declined in the tract-aware setting, dropping from 75% to 66.2%
inthePYTandfrom82%to55%intheCC(referTable4).Thisdeclinecanbe
attributed to use of Continuous Map Criterion (CMC) as a stopping criterion
for fiber tracking. The CMC terminates fiber tracking based on Partial Volume
Estimate (PVE) maps, allowing tractography to continue until the streamline
Table 4. Results are presented for the left and right parts of PYT and a segment of
CContheTractoInferno(TtoI)dataset.Trackingforallalgorithmsisconductedusing
our proposed tract-specific generation method.
Pyramidal Tract (PYT) Corpus Callosum
Left Right (CC)
Dataset Algo. Dice OvL OvR Dice OvL OvR Dice OvL OvR
T-RLF 70.3 64.1 17.2 70.1 63.1 16.9 70.4 71.2 32.6
TD3 69.4 62.5 15.9 69.2 61.4 15.8 68.1 64.8 26.1
TtoI DET 72.7 79.3 38.8 70.3 76.2 40.7 70.1 72.6 35.8
PROB 77.6 79.5 25.3 74.8 72.5 21.3 72.6 76.4 36.7
PFT 66.2 55.7 12.4 65.9 57.8 17.4 54.9 51.2 36.1Tract-RLFormer 15
correctly stops in the gray matter. This approach may generate fibers beyond
our tract-specific masks, leading to increased overreach (see Fig. 4(b)) and con-
sequently lower Dice scores. Furthermore, fibers generated outside the tracking
mask may be erroneous and subsequently filtered or cleaned via FSS, resulting
in a lower OvL score.
Summarization: In summary, our results demonstrate that we surpass su-
pervised methods (Table 2). Additionally, we consistently outperform the TD3
model (Tables 2- 4), which served as the basis for training T-RLF. Notably, our
tract-specific setting not only improves TD3 performance but also the perfor-
mance of classical methods like DET and PROB compared to the whole-brain
setting. This suggests a promising new direction of data driven policy learn-
ing for tract specific fiber generation in limited ground truth scenarios that can
naturally scale up effectively.
4.3 Ablation Study
We conducted an ablation study to determine the optimal configuration for our
T-RLF model. The evaluation presented in Table 5, identified the best archi-
tecture with n_heads=1, K=40, and an embedding dimension of d=128. This
study highlights the importance of a larger context in tractography, illustrat-
ing how a broader temporal receptive field can enhance the model’s ability to
generate accurate fiber tracts.
Table 5.Dicescores(in%)averagedover7tractsofsubject1006fromTractoInferno
dataset,atdifferentvaluesofT-RLFparameters:numberofattentionheads(n_heads),
context length (K), and embedding dimension (d). Best score is in bold.
K =20 K =30 K =40
d=128 d=512 d=128 d=512 d=128 d=512
n_heads = 1 64.7 65.2 66.2 67.3 68.6 67.6
n_heads = 2 63.4 66.3 66.2 67.5 68.0 68.2
Wealsoexaminedtheimpactoftwokeycomponents:MaskRefinementMod-
ule (MRM) discussed in section 3.1, and the tract-specific policy fine-tuning as
detailedinsection3.3.Table6reportstheresultsfortheT-RLFnetworktrained
over TractoInferno dataset. We have observed that initial tracking masks led to
a significant overreach (OvR), extending beyond actual region of interest. This
OvR was notably reduced after incorporating MRM, leading to improved Dice
and overlap metrics across all tracts. Furthermore, fine-tuning the network spe-
cific to each tract allowed it to learn better and robust tract-specific diffusion
characteristics,resultinginadditionalimprovementsintheperformancemetrics.
5 Conclusion
Tractography can be an essential tool in neuroimaging, enabling the detailed
mapping of neural pathways crucial for both clinical and research applications.16 A. Joshi et al.
Table6.Averageperformancemetrics(in%)obtainedusingTract-RLFormerhighlight
theimpactoftheMRMontestsubjectsfromtheTractoInfernodataset.Thetablealso
comparestheperformanceofthepre-trainednetworkwiththefine-tunednetworkpost
MRM application, illustrating the effect of policy fine-tuning on the same dataset.
Tract Without MRM With MRM
Pre-trained Fine-tuned
Dice OvL OvR Dice OvL OvR Dice OvL OvR
PYT 44.1 30.4 5.6 65.9 55.1 11.8 70.3 67.2 24.7
CG 41.1 45.4 80.3 51.5 45.7 30.3 58.7 54.7 31.7
AF 34.2 34.4 65.1 45.8 39.5 36.1 46.1 40.8 38.8
CC 58.9 59.0 41.9 66.2 59.9 20.8 70.4 71.2 32.6
Our work significantly advances this field by introducing a data driven Tract-
RLFormer framework which is a tract-specific, transformer-based network inte-
grating supervised and reinforcement learning paradigms. A distinctive feature
of our Tract-RLFormer is its ability to train within the reinforcement learning
experience space, independent of ground truth fibers. The fine-tuning stage of
our model focuses and refines its capabilities in generating the tracts of inter-
est. This approach demonstrates its excellent generalization performance across
various datasets as well as scalability. Our data-driven approach has the po-
tential to utilize data from any reinforcement learning agents trained in diverse
neuroimaging environments. Moreover, our innovative tract-specific modeling
approach simplifies the reconstruction process by directly generating the target
tract, thus avoiding the complex and error-prone segmentation step.
6 Acknowledgment
This research was supported by SERB Core Research Grant Project No: CRG/
2020/005492, IIT Mandi.
References
1. Avants, B.B., et al.: Advanced normalization tools. Insight j 2(365), 1–35 (2009)
2. Basser, P.J.: Fiber-tractography via diffusion tensor mri (dt-mri). In: Proceedings
of the 6th Annual Meeting ISMRM, Sydney, Australia. vol. 1226, p. 14 (1998)
3. Benou,I.,RiklinRaviv,T.:Deeptract:Aprobabilisticdeeplearningframeworkfor
white matter fiber tractography. In: MICCAI: Shenzhen, China, October 13–17,
2019. pp. 626–635. Springer (2019)
4. Berman,J.I.,Chung,S.,Mukherjee,P.,Hess,C.P.,Han,E.T.,Henry,R.G.:Prob-
abilistic streamline q-ball tractography using the residual bootstrap. Neuroimage
39(1), 215–222 (2008)
5. Chen, L., et al.: Decision transformer: Reinforcement learning via sequence mod-
eling. NeurIPS 34, 15084–15097 (2021)
6. Dosovitskiy,A.,Beyer,L.,Kolesnikov,A.,Weissenborn,D.,Zhai,X.,Unterthiner,
T., et al.: An image is worth 16x16 words: Transformers for image recognition at
scale. arXiv preprint arXiv:2010.11929 (2020)Tract-RLFormer 17
7. Essayed, W.I., Zhang, F., Unadkat, P., Cosgrove, G.R., Golby, A.J., O’Donnell,
L.J.: White matter tractography for neurosurgical planning: A topography-based
review of the current state of the art. NeuroImage: Clinical 15, 659–672 (2017)
8. Fillard,P.,Poupon,C.,Mangin,J.F.:Anovelglobaltractographyalgorithmbased
on an adaptive spin glass model. In: MICCAI. pp. 927–934. Springer (2009)
9. Fujimoto, S., Hoof, H., Meger, D.: Addressing function approximation error in
actor-critic methods. In: ICML. pp. 1587–1596. PMLR (2018)
10. Girard, G., Whittingstall, K., Deriche, R., Descoteaux, M.: Towards quantita-
tive connectivity analysis: reducing tractography biases. Neuroimage 98, 266–278
(2014)
11. Jumper,J.,Evans,R.,Pritzel,A.,Green,T.,etal.:Highlyaccurateproteinstruc-
ture prediction with alphafold. nature 596(7873), 583–589 (2021)
12. Maier-Hein, K.H., et al.: The challenge of mapping the human connectome based
on diffusion tractography. Nature communications 8(1), 1349 (2017)
13. Neher, P.F., Côté, M.A., Houde, J.C., Descoteaux, M., Maier-Hein, K.H.: Fiber
tractography using machine learning. Neuroimage 158, 417–429 (2017)
14. Neher, P.F., et al.: A machine learning based approach to fiber tractography us-
ing classifier voting. In: MICCAI 2015: 18th International Conference, Munich,
Germany, October 5-9, 2015, Proceedings, Part I 18. pp. 45–52. Springer (2015)
15. Poulin,P.,etal.:Learntotrack:deeplearningfortractography.In:MICCAI2017:
20thInternationalConference,QuebecCity,QC,Canada,September11-13,2017,
Proceedings, Part I 20. pp. 540–547. Springer (2017)
16. Poulin, P., et al.: Tractoinferno-a large-scale, open-source, multi-site database for
machine learning dmri tractography. Scientific Data 9(1), 725 (2022)
17. Rheault, F.: Population average atlas for recobundlesx (May 2023), https://doi.
org/10.5281/zenodo.7950602
18. Rheault,F.,etal.:Bundle-specifictractographywithincorporatedanatomicaland
orientational priors. NeuroImage 186, 382–398 (2019)
19. St-Onge,E.,Garyfallidis,E.,Collins,D.L.:Faststreamlinesearch:Anexacttech-
nique for diffusion mri tractography. Neuroinformatics 20(4), 1093–1104 (2022)
20. Théberge, A., Desrosiers, C., Boré, A., Descoteaux, M., Jodoin, P.M.: What mat-
ters in reinforcement learning for tractography. MIA 93, 103085 (2024)
21. Théberge, A., et al.: Track-to-learn: A general framework for tractography with
deep reinforcement learning. MIA 72, 102093 (2021)
22. VanEssen,D.C.,Ugurbil,K.,Auerbach,E.,Barch,D.,Behrens,T.E.,Bucholz,R.,
Chang, A., Chen, L., Corbetta, M., Curtiss, S.W., et al.: The human connectome
project: a data acquisition perspective. Neuroimage 62(4), 2222–2231 (2012)
23. Vaswani, A., et al.: Attention is all you need. Advances in neural information
processing systems 30 (2017)
24. Wegmayr, V., Buhmann, J.M.: Entrack: Probabilistic spherical regression with
entropy regularization for fiber tractography. International Journal of Computer
Vision 129(3), 656–680 (2021)
25. Wegmayr, V., Giuliari, G., Holdener, S., Buhmann, J.: Data-driven fiber trac-
tography with neural networks. In: 2018 IEEE 15th international symposium on
biomedical imaging (ISBI 2018). pp. 1030–1033. IEEE (2018)
26. Yeh, F.C., et al.: Population-averaged atlas of the macroscale human structural
connectome and its network topology. Neuroimage 178, 57–68 (2018)
27. Zhou, H., Zhang, et al.: Informer: Beyond efficient transformer for long sequence
time-series forecasting. In: Proceedings of the AAAI conference on artificial intel-
ligence. vol. 35, pp. 11106–11115 (2021)