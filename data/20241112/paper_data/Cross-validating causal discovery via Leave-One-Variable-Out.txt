CROSS-VALIDATING CAUSAL DISCOVERY VIA
LEAVE-ONE-VARIABLE-OUT
DanielaSchkoda PhilippFaller
TechnicalUniversityofMunich,Germany KarlsruheInstituteofTechnology,Germany
daniela.schkoda@tum.de AmazonResearchTu¨bingen,Germany
philipp.faller@partner.kit.edu
PatrickBlo¨baum DominikJanzing
AmazonResearchTu¨bingen,Germany AmazonResearchTu¨bingen,Germany
bloebp@amazon.com janzind@amazon.com
ABSTRACT
We propose a new approach to falsify causal discovery algorithms without ground truth,
whichisbasedontestingthecausalmodelonapairofvariablesthathasbeendroppedwhen
learningthecausalmodel. Tothisend,weusethe”Leave-One-Variable-Out(LOVO)”pre-
dictionwhereY isinferredfromX withoutanyjointobservationsofX andY,givenonly
training data from X,Z ,...,Z and from Z ,...,Z ,Y. We demonstrate that causal
1 k 1 k
modelsonthetwosubsets,intheformofAcyclicDirectedMixedGraphs(ADMGs),often
entailconclusionsonthedependenciesbetweenX andY,enablingthistypeofprediction.
ThepredictionerrorcanthenbeestimatedsincethejointdistributionP(X,Y)isassumed
tobeavailable,andX andY haveonlybeenomittedforthepurposeoffalsification. After
presenting this graphical method, which is applicable to general causal discovery algo-
rithms,weillustratehowtoconstructaLOVOpredictortailoredtowardsalgorithmsrelying
onspecificaprioriassumptions,suchaslinearadditivenoisemodels. Simulationsindicate
thattheLOVOpredictionerrorisindeedcorrelatedwiththeaccuracyofthecausaloutputs,
affirmingthemethod’seffectiveness.
Keywords Out of variable generalization, Benchmarking causal models, Benchmarking without ground
truth.
1 Introduction
Causaldiscovery(Spirtesetal.,1993),theinferenceof(typicallyacyclic)causalgraphsfromobservational
data, has attained substantial research interest since the development of the PC algorithm (Spirtes et al.,
1993; Glymour et al., 2019), which leverages the causal Markov condition and faithfulness assumption.
Research gained further momentum after it was observed that additional assumptions render identification
solvablealsowithinMarkovequivalenceclasses,see,e.g.,KanoandShimizu(2003);Shimizuetal.(2006);
Sun et al. (2006); Hoyer et al. (2008); Zhang and Hyva¨rinen (2009); Peters et al. (2011); Kocaoglu et al.
(2017); Gnecco et al. (2021); Rolland et al. (2022); Montagna et al. (2023). These approaches are mean-
4202
voN
8
]LM.tats[
1v52650.1142:viXraCross-validatingcausaldiscovery
whilecomplementedbysupervisedlearningmethods: Lopez-Pazetal.(2015)treatscause-effectinference
asabinaryclassificationproblem,Nautaetal.(2019);Lachapelleetal.(2020);Zhengetal.(2020);Keetal.
(2023)usetechniquesfromdeeplearningtolearnmultivariatecausalgraphsusingarchitecturestailoredfor
learning properties of probability distributions. Further, it has been shown that data from changing envi-
ronments helps in identification of causal models (Tian and Pearl, 2001; Peters et al., 2016; Zhang et al.,
2017;Mooijetal.,2020;Rothenha¨usleretal.,2021). However,evenafterdecadesofcreativecontributions,
it is fair to say that causal discovery did not experience any widely celebrated breakthroughs in practical
applicationsdespiteinterestingsuccessstories,e.g.,Shenetal.(2020);Lagemannetal.(2023). Onereason,
ifnotthemainone,isthatresearchersworkingonpracticalapplicationsfindithardtojudgewhichmethod
works best or if any works reasonably well for their use case. Extensive evaluations are mainly performed
onsimulateddata,apracticeaboutwhichseriousdoubtsareinorder(Reisachetal.,2021). Despitetheexis-
tenceofinstructiveexamplesforinterventionaldata(Lagemannetal.,2023),suchas,forinstance,knockout
experiments in genetics (Hamilton et al., 1989), it is a serious limitation to entirely rely on interventions.
This is not only because interventional experiments are expensive, but also because interventions cannot
necessarily be attributed to single nodes (Zhang et al., 2023). Thus, some datasets may require a separate
discussionaboutwhatnodeisintervenedon, whichmotivatedresearchonlearningcausalgraphsfromun-
known intervention targets (Jaber et al., 2020). Further, ”ground truth” reported in the literature (Sachs
et al., 2005) has later been challenged elsewhere (Mooij et al., 2020, Section 5.8). In other words, despite
all the interesting experimental data sets, automatic retrieval of a vast number of interventional data for
trustworthy benchmarking seems currently out of reach. For practical applications, researchers often solve
causal inference tasks like treatment effect estimation in directed acyclic graphs (DAGs) with comparable
lowcomplexityandraisedoubtsaboutwhethermorecomplexDAGscanbetrusted(Imbens,2020).
Motivatedbythelackofbenchmarkingdata, Falleretal.(2024)suggestsa”self-compatibility”check,
which applies causal discovery algorithms to subsets of variables and quantifies to what extent the algo-
rithm’soutputsonsubsetscontradicttheoutputontheentiresetofvariables. Thepaperdiscussesdifferent
measures of disagreement, but since some disagreement is not unlikely, it is non-trivial to set thresholds
below which we want to trust the algorithm and another threshold above which we consider the outcomes
random. This paper builds heavily on the idea of Faller et al. (2024) with the difference that it focuses on
onespecific,well-definedlearningtask: giventhevariablesX,Y andZ := {Z ,...,Z },inferY fromX
1 k
whentrainingdataisonlyfromP(X,Z)andP(Y,Z),butnodatafromP(X,Y)isgiven. Sincethetarget
of this learningtask is simple, namely to inferthe conditional distributionP(Y|X = x)or the conditional
expectationE[Y|X = x],itiseasytodefinesuccessviaasimplelossfunctionandtodefineanaturalbase-
line. This task is related to compatibility because Faller et al. (2024) showed examples where outputs of
causaldiscoveryonX,Z andY,Z entailcausalmodelsonX,Y,Z forwhichP(X,Y,Z)canbeuniquely
computedfromP(X,Z)andP(Y,Z). Inthiswork,wefurtherexploreandutilizethistoevaluateinferred
causal relationships. Certainly, the predicted joint distribution could also be verified from infinite data, but
inferringonlythebivariaterelationbetweenX andY isstatisticallymorewell-behaved. Whileourtaskcan
be seen as missing data problem (for which causal structure is known to enable better imputation, see e.g.
MohanandPearl(2021)),here,missingnesscomesfromdroppingonpurposefortestingcausalhypotheses.
We call our scenario ”Leave-One-Variable-Out (LOVO)” cross-validation in analogy to leave-one-out
(LOO)cross-validationinstatisticallearning(Stone,2018). WhileLOOcross-validationevaluatesamodel
prediction for a datapoint that has not been used for learning, we test the prediction of the causal model at
a variable pair (X,Y) whose relation has not been used. We will discuss conditions under which causal
modelsrenderthistasksolvable. Inotherwords,whilestatisticallearningisbasedonaninductivebiasthat
allows to interpolate a function at a point that has not been seen before, causal learning may ”interpolate”
dependencesbetweenvariablepairsthathavenotbeenseentogether. Thisway,wefurtherelaborateonthe
viewofpreviousworks(Tsamardinosetal.,2012;Janzing,2018;DhirandLee,2019;Greseleetal.,2022;
Guoetal.,2024)thatcausalmodelshelpforgeneralizationacrossvariables,called”out-of-variablegeneral-
2Cross-validatingcausaldiscovery
ization”inGuoetal.(2024),withthedifferencethatwepredictastatisticalrelationthatisactuallyknown,
only for the purpose of testing. One of the early works that observed that causal models enable predicting
relations between variables not observed together can be found in Tsamardinos et al. (2012), where Maxi-
malAncestralGraphs (MAGs)onoverlappingsubsets ofvariablesareusedto inferdependenciesbetween
variables,eachofwhichonlyoccursinoneofthesubsets.
Structureofthepaper: AfterSection2formallydefinesLOVOpredictionandlaysoutthegeneralstruc-
ture of LOVO cross-validation, in Section 3 we examine under which conditions causal models enable
LOVO prediction and propose a practical estimation method. Section 4 defines the ”non-causal baseline,”
i.e.,asimpleLOVOpredictionruletobeusedwhennothingisknownaboutthecausalstructure. Section5
reports experiments, where we apply LOVO prediction to the causal discovery algorithms DirectLiNGAM
(Shimizuetal.,2011)andRepetetiveCausalDiscovery(MaedaandShimizu,2020). Allproofsaregivenin
theappendix.
Notationandtechnicalassumptions: Tosimplifymathematicaldiscussionsandnotation,wewillalways
assume that the joint distribution P(X,Y,Z) has a density (the probability mass function for the case of
discrete variables) with respect to a product measure. The vector of all variables is denoted by W =
(X,Y,Z). Further, except for results that explicitly refer to continuous variables, we use discrete sum
over probabilities without being explicit about replacing them with integrals over densities otherwise. For
standardconceptsofcausaldiscoverylikecausalMarkovconditionandd-separationinDAGs,andMarkov
equivalence,werefertotheliterature,e.g.,Spirtesetal.(1993);Pearl(2000).
2 BuildingblocksforLOVOcross-validation
We interpret LOVO prediction as the task of inferring P(Y|X), or the reduced problem of inferring the
regression function f(x) = E[Y|X = x], or the correlation ρ , from P(X,Z) and P(Y,Z). This
XY
prediction is then used to assess the reliability of a causal discovery algorithm as follows: We select a
pair of variables (X,Y) from W and run the causal discovery method separately on (X,Z) and (Y,Z).
TheoutputsG ,G informtheconstructionofacausallyinformedLOVOpredictorwheneverthegraphical
X Y
modelsenablesuchapredictor. Next,itspredictionerrorisestimatedbycomparingittoanestimateobtained
from the joint distribution P(Y,X). To obtain the overall LOVO cross-validation error, we repeat the
procedure for all choices of pairs (X,Y) from W. Finally, to decide whether the error is still acceptable
or so large that we should reject the outcome of the causal discovery algorithm, we compare the cross-
validation error to the error of a baseline LOVO predictor, which estimates P(Y|X) from P(X,Z) and
P(Y,Z)withoutusinganycausalinformation.
Sinceweperformcausalinferenceonsubsetsofthewholedataset,weneedaclassofcausalgraphsthat
enables marginalizations. Following Faller et al. (2024), we use acyclic directed mixed graphs (ADMGs)
(Richardson, 2003) which contain the usual causal edges → as well as edges ↔ (indicating a confounding
paththatcannotbeblockedbyanyobservedvariable). ThereexistslightlydifferentdefinitionsofADMGs
acrosstheliterature,varyinginwhethertoallowco-occurrenceofbothtypesofedgesasconfoundedcausal
links A →↔ B. We allow these confounded causal links1 and say that a child B of A is a confounded
↔
child, if A → B, and an unconfounded child if A → B is the sole edge between the two nodes, similarly
for parents. If A ↔ B, we say that A and B are siblings. We denote the children, parents, and siblings
of a node A by ch(A),pa(A), and sib(A), respectively. Moreover, we use the symbol − to denote any
type of directed or bidirected edge, and A ̸− B to indicate that A and B are not connected by an edge.
For details of marginalization in ADMGs we refer to Richardson (2003), but it is rather intuitive: directed
paths A → ∗ → B turn into edges A → B when marginalizing over the mediators, confounding paths
A ← ∗ → B or A ← ∗ ↔ B into bidirected edges A ↔ B when marginalizing over the common cause.
1Nonetheless,ourapproachcanbeadaptedtotheotherdefinition,seeSubsection5.1.
3Cross-validatingcausaldiscovery
ByG,werefertotheDAGorADMGfortheentiresetofnodesW2,andG ,G areitsmarginalizations
X Y
whenleavingoutY,X. Wheneverwedonotexplicitlymentionthegraph,graphicalconditionsrefertothe
jointgraphG.
3 ConstructingLOVOpredictorsviacausaldiscovery
3.1 ConnectionofcausalityandLOVOprediction
We first consider a small toy scenario on three variables (X,Y,Z), whose joint graph is assumed to be
a DAG, to provide an intuition under which conditions causally informed LOVO prediction is feasible.
Besidesthat,weillustratethatitmakessensetoproceedbythefollowingsteps:
1. InferthejointDAG(orlaterADMG)GfromthetwomarginalgraphsG ,G .
X Y
2. UsethejointgraphGtoreconstructP(X,Y)fromP(X,Z)andP(Y,Z).
Supposeacausaldiscoveryalgorithmyieldstheoutputs
G = X → Z, G = Z → Y
X Y
whenappliedtotheleave-one-outsubsets(X,Z),(Y,Z). First,wewanttoexploreiftheseoutputsallowus
todrawconclusionsabouttheentiregraph. Employingthemarginalizationrulesmentionedearlier,wefind
thattheedgeX → Z ∈ G canariseifandonlyifX → Z,X → Y → Z,orbothstructuresarecontained
X
in G. Since the second structure contradicts G = Z → Y, we conclude X → Z ∈ G. Similarly, we
Y
obtain that Z → Y ∈ G. Lastly, we need to check if G could contain other edges: By acyclicity, the only
→
potential additional edge is X → Y. However, if X → Y ∈ G, then G would be Z ↔ Y. Knowing G,
Y
wecanconcludeX ⊥⊥ Y | Z,andtherefore,
P(X,Z,Y) = P(X,Z)P(Y|Z),
whichdeterminesE[Y|X = x]. Thus,ifthetrueunderlyinggraphisX → Z → Y,weareabletoconstruct
aLOVOpredictor. WhileweuseaconditionalindependencestatemententailedbyGhere,wewanttostress
that the LOVO predictor does not solely rely on conditional independence. Note, however, that not every
LOVOpredictorisbasedonX ⊥⊥ Y |Z. Theorem6orTable1inthesupplementcontaincaseswhereitcan
be inferred despite X ̸⊥⊥ Y |Z, using other statistical properties entailed by the joint causal model. If, for
instance,thejointDAGisX → Y → Z,linearmodelsenabletheidentificationofthestructurecoefficient
fromX toY. Moreover,intheprecedingsteptoinferGfromG ,G ,weheavilymakeuseofthearrows’
X Y
directions. If, in the example above, we had only gained knowledge on the skeletons of G ,G , the joint
X Y
graph could be any graph, except for the three graphs where Z is an isolated node. This is not special to
this example; instead inferring E[Y|X = x] cannot follow from the conditional independencies observed
in P(X,Z) and P(Y,Z) via any mathematical laws (except for degenerate cases, e.g, when Z uniquely
determinesX,Y). Thisinsightisformalizedinthefollowinglemma:
Lemma1(NoprobabilisticlawenablesLOVOprediction). LetX,Y bereal-valuedvariableswhosecon-
ditional distributions P(Y|Z = z) P(Y|Z = z) have densities p(x|z) and p(y|z) with respect to the
Lebesquemeasure. LetZ = {Z ,...,Z }bevariableswitharbitraryrange. ThenP(X,Z)andP(Y,Z)
1 k
canneveruniquelydetermineP(X,Y). Inparticular,eventhesignoftheircorrelationisambiguous.
Theproofisquiteexplicitabouttheremainingambiguity: whengeneratingP(X,Z)andP(Y,Z)via
structural equation models with noise variables NX and NY, respectively, the dependences between NX
Z Z Z
and NY only influence the joint distribution, but not the marginals. Note that Z can consist of multiple
Z
variableshere;thus,thelemmaisageneralresult.
However,alsocausalmodelsdonotalwaysenableLOVOprediction. Forexample,G = Z → X → Y,
can not be uniquely reconstructed from its marginal graphs G = Z → X,G = Z → Y since, e.g.,
X Y
2ThisimplicitlyassumesthatthejointdataisMarkovtosomeADMG.
4Cross-validatingcausaldiscovery
X Y X Y
G= G = G =
X Y
Z Z Z Z Z Z
1 2 1 2 1 2
Figure1: Excludeedgesbasedonthemarginalgraphs.
X ← Z → Y and Z → Y → X have the same marginal graphs. While the graph G = X → Y ← Z
is uniquely determined from its marginal graphs, G = X ̸− Z and G = Z → Y here, the second step
X Y
fails. BecauseX ⊥⊥ Z,wecannotcombineP(X,Z)andP(Y,Z)toextractinformationontheconnection
strengthfromX toY. AppendixApresentsanoverviewoftherealizabilityofLOVOforallpossiblegraphs
consistingofthreenodesandtwoedges,indicatingthatinmanycases,oneofthetwostepsfails. However,
inpractice,itsufficesifwecanconstructaLOVOpredictoronlyforafewpairsofnodesinthegraph;then
we simply compute the cross validation error as an average over those pairs for which we can construct
the LOVO predictor. Furthermore, small graphs are particularly challenging for LOVO prediction because
theoverlapbetween(X,Z)and(Y,Z)issmall. Thenextsectionandthesimulationsrevealthatforlarger
graphs,wetypicallydiscoveratleastone(andoftenseveral)pairsthatcanbehandled.
3.2 LOVOpredictionviaparentadjustment
Thissectiondiscussesthegeneralcase,inwhichZ maycontainmultiplevariables,andthejointgraphmay
be a DAG or an ADMG. Analogously to our first example, whenever there is a set Z that renders X and
S
Y conditionallyindependent,wecandefineaLOVOpredictorviatheequation
(cid:88)
P(y|x) = P(y|z )P(z |x).
S S
zS
Giventhatgraphswithmorethanthreenodesoftencontainatleastsomepairsofconditionallyindependent
nodes,wemainlyrelyonthisLOVOpredictor. Specifically,inaDAG,X andY areconditionallyindepen-
dent if and only if they are not connected by an edge. In this case, the union of parents of X and Y is a
d-separatingset. Incontrast,inADMGs,theabsenceofadirectlinkdoesnotguaranteetheexistenceofan
↔
m-separatingset(e.g. forX → Z → Y). Here,theunionofparentsism-separatingifthereisnolinkand
alltheparentsareunconfounded. Hence,thequestionariseshowtoidentifytheseunlinkedpairswithonly
themarginalgraphsavailable. Forexample,Figure1showshowaddingtheedgeX → Y orY → X toG,
introducesadditionaledgesinG ,representedbytheblueandorangeedges. Thesedifferencesallowusto
Y
deducetheabsenceofanedge,asformalizedinthefollowinglemma.
Lemma 2 (excluding links in ADMGs). Let G an ADMG whose marginalizations are G and G . If X
X Y
hasachildinG thatisneitherasiblingnorachildofY inG ,orthesameholdswithreversedtheroles
X Y
ofX andY,thenX ̸− Y inG.
Mostcausaldiscoveryalgorithmsassumecausalsufficiency,whichistypicallyviolatedinleave-one-out
datasets. However,alsoinpractice,causalsufficiencycanrarelybeguaranteed. Therefore,wearguethata
causal discovery method should at least be robust in the sense that if one confounder exists, it may not be
abletolearnthecorrespondingbidirectededges,butitshouldnotmessupthedirectedstructureentirely. In
thiscontext,withknowledgelimitedtothedirectedpart,wecanmakeuseofthefollowinglemma.
Lemma 3 (excluding links from directed part). If G is an ADMG, and Gdir,Gdir are the directed parts of
X Y
itsmarginalgraphs,and
(1) X occurscausallyafterY inthesensethatinGdir thereexistsanancestorofX thatisatthesame
X
timeadescendantofY inGdir,or
Y
(2) X hasaparentinGdir thatisnoparentofY inGdir,or
X Y
(3) Y hasachildinGdir thatisnochildofX inGdir,
Y X
5Cross-validatingcausaldiscovery
thenGdoesnotcontaintheedgeX → Y. IfGisaDAG,andinaddition,oneoftheconditionsholdswith
reversedroles,X andY arenotconnectedbyanedgeinG.
If G is a DAG, and we also know the bidirected parts of G ,G , we can trace each bidirected edge
X Z
C ↔ D inG backtoC andD beingchildrenofY inthejointmodel. Thisadditionalinformationallows
X
us to decide whether an edge exists and, if so, determine its type in even more cases. While for the time
being, we are only interested in conditions for the absence of an edge, which is required for the LOVO
predictor suggested above, the conditions for distinguishing the type of a present edge become relevant in
thenextsection.
Lemma4(determiningedgetypesinDAGs). AssumeGisaDAGwithmarginalizationsG ,G . ThenX
X Y
hasatleasttwochildreninGifandonlyifG containsatleastonebidirectededge. Moreover,
Y
(1) ifX hasatleasttwochildren,thenX → Y ∈ GifandonlyifY hasasiblinginG .
X
(2) IfY hasatleasttwochildren,andX hasfewer,thenX → Y ifandonlyifX hasmultiplechildren
inG .
Y
(3) Inthecasethatbothhavefewerthantwochildren,
(a) ifchGX(X) ̸= chGY(Y),thenX ̸− Y;
(b) ifneitherpaGX(X) ⊆ paGY(Y),norviceversa,thenX ̸− Y;
(c) ifchGX(X) = chGY(Y) = {C},andneitherpaGY(Y) ⊆ paGX(X) ⊆ paGY(Y)∪paGY(C)
norviceversa,thenX ̸− Y;
(d) ifchGX(X) = chGY(Y) = {C},andneitherpaGY(Y) ⊆ paGX(X) ⊆ paGY(Y)∪paGY(C)
noratthesametimepaGX(X) ⊆ paGY(C)andpaGY(Y) ⊆ paGX(C),thenX → Y.
(4) AlltheabovecriteriaholdforreversedrolesofX andY.
The lemma is exhaustive in the sense that if none of the conditions apply, it is impossible to deter-
mine whether X and Y are linked. Once a pair is identified as unlinked, the next step is to derive a d- or
m-separatingsetbyinferringtheunionofparentsandverifyingtheirunconfoundednesswithX,Y. Advan-
tageously,ifthereisnoedgeX−Y inG,theparentsandthesiblingsofX arethesameinbothG andG,
X
as are those of Y. This allows us to directly assess the parents and their unconfoundedness from G ,G ,
X Y
andleadstothefollowingLOVOpredictor.
Theorem5(LOVObyadjustingunionofparents). LetallparentsofX beunconfounded. Likewise,letall
parentsofY beunconfounded. IfZ denotestheunionoftheparentsofX andY,thenwehave
S
(cid:88)
P(y|x) = P(y|z )P(z |x). (1)
S S
zS
Toconstructthispredictorinpractice,weresorttothefollowingsimpleprocedure:
Three-stepLOVOpredictor
(1) LearnapredictorPˆ(Y|Z)oraregressionfunctionfˆ withfˆ(z) := Eˆ[Y|Z = z].
(2) Applythepredictortothez-valuesofthepairs(x,z)sampledfromP(X,Z)togenerateartificial
pairs(x,yˆ),withyˆsampledfromPˆ(y|z)orchosenasyˆ:= fˆ(z),respectively.
(3)
UsethesepairstolearnthepredictorPˆ(Y|X),Eˆ[Y|X]orρˆ
.
XY
Westressthat, asintheexampleonthreenodes, Theorem5reliesonconditionalindependencestatements
postulatedbyG. Incontrast,theprecedingstepstoinferthenon-existenceoftheedgeandthejointparents
and from the marginal distributions employ the built-in inductive bias of causal models, particularly the
faithfulnessofthejointmodel. AlthoughwedonotclaimthatLOVOpredictionnecessitatescausalmodels,
theresultsinthissectionsuggestthattheyareanaturalwaytosolvethistask.
6Cross-validatingcausaldiscovery
3.3 LOVOtailoredtoLinearnon-GaussianAcyclicModels(LiNGAM)
Somecausaldiscoveryalgorithmsarebasedonstructuralequationmodels,suchasthelinearadditivenoise
model(LiNGAM).ThelinearadditivenoisemodelfortheDAGGpostulatesthat
(cid:88)
W = λ W +N , i = 1,...,k+2, (2)
i ij j j
Wj∈pa(i)
where the λ are real coefficients and the N are independent centered non-Gaussian variables. Since the
ij j
structure matrix Λ = (λ ) collecting all coefficients is sparse according to the acyclic graph G, it can be
ij
transformedintoastrictlylowertriangularmatrixthroughsimultaneousrowandcolumnpermutations. We
assumefaithfulness,thatis,foralledges(i,j) ∈ G,thetotalcausaleffect
(cid:88) (cid:89)
m = λ (3)
ij lk
πdirectedpath k→ledgeonπ
fromjtoi
isnotzero. ThisassumptionisfulfilledforLebesguealmostallstructurematricesΛcompatiblewithafixed
graph G. Reflecting the model, the corresponding algorithms commonly output not only a DAG but also
thematrixΛ. Thus,whendevelopingaLOVOpredictor,itappearsnaturaltoincorporatethelearnedmatrix
inordertofalsifythealgorithm’sentireoutput. Additionally,thisenablesLOVOpredictionevenifadirect
link exists, as long as we can determine its type using Lemma 4. Recall that the lemma leaves only a few
exceptions where finding out about the edge type is impossible; for instance, it is possible whenever X or
Y hasatleasttwochildrenorwhentheydonothavepreciselythesamechildreninthemarginalgraphs.
Theorem 6 (LOVO via LiNGAM). If PW follows a linear additive noise model for some DAG G, the
edge type between X and Y, (X → Y,Y → X, or X ̸− Y), can be inferred using Lemma 4, and not
ch(Y) = {X,Z },ch(X) = {Z }orviceversa,then3
j j
(1) thestructurematrixΛcanbeuniquelyidentifiedfromP(X,Z)andP(Y,Z).
(2) If,inaddition,allsecondandhigherordermomentsofN arefinite,P(X,Z)andP(Y,Z)uniquely
determinesP(X,Y,Z),exceptforameasurezerosetofmomentsofN.
Asintheprevioustheorem,allgraphicalassumptionsinthetheoremcanbeverifiedfromthemarginal
graphs. Knowingthestructurematrix,wecanconstructtheLOVOpredictorasfollows. WritingZ forthe
S
parentsofY inZ,andusingthatunderthemodelassumptionsY = λ Z +λ X +N ,andN is
YZS S YX Y Y
centeredandindependentof(X,Z ),weobtain
S
E(Y | X = x) = λ E(Z | X = x)+λ ,
YZ YX
whichcanbeestimatedfromP(X,Z).
4 Baseline: LOVOpredictioninabsenceofcausalinformation
AlthoughthetwoproposedLOVOpredictorsprovideareasonableapproximationofE(Y | X = x)when-
ever the marginal graphs G ,G are accurate, some error will persist. To decide which level of deviation
X Y
is still acceptable, we ask whether the causal information helped the prediction. To this end, we compare
thepredictionerrortotheerrorofthebestLOVOpredictorwithoutcausalinformation,alsocalledbaseline
predictor. Note that one may consider P(Y|X) = P(Y) (that is, assuming independence of X and Y)
as the best predictor in the absence of any causal knowledge. We reject this idea for two reasons: First,
3Lemma4assumesG ,G tobeADMGs,whereasmostLiNGAMbasedcausaldiscoveryalgorithmsproduceDAGsentailing
X Y
explicit latent nodes. However, such a DAG can be easily transformed into an equivalent ADMG by replacing each structure
W ←L→W ,withLalatentnode,byW ↔W .
1 2 1 2
7Cross-validatingcausaldiscovery
the dependences between X,Z and between Y,Z may be so strong that it is impossible that X and Y are
independent,seeNo4. inTable1inthesupplement,lastcolumn. Second,thepredictorP(Y|X) = P(Y)
isunlikelytobetherightoneingraphswithseveralnodesunlessoneassumerelativelysparsegraphs.
Instead of assuming independent X,Y as the best ”causally agnostic” predictor, we use the ”Max-
Ent prediction” (Jaynes, 2003), which is the joint distribution that maximizes entropy subject to the given
marginal distributions P(X,Z) and P(Y,Z) (Garrido Mejia et al., 2022). It is given by the unique joint
distributionwithX ⊥⊥ Y |Z. Toseethis,notethatthejointentropyreads(CoverandThomas,1991)
H(X,Y,Z) = H(X,Z)+H(Y|X,Z) = H(X,Z)+H(Y|Z)−I(Y : X|Z),
whichismaximalwhentheconditionalmutualinformationI(Y : X|Z)vanishes.
To justify MaxEnt as a reasonable approach for our purpose, we remind the reader of the intuition that
theMaxEntdistributionisthe”mostmixed”distributionwithinthesetofdistributionssatisfyingthegiven
bivariate constraints, which seems like a better compromise rather than choosing distributions closer to
the boundary. Gru¨nwald and Dawid (2004) provides a game-theoretic view on MaxEnt and shows that it
minimizestheworst-caselogarithmiccross-entropyloss.
Definition7(MaxEntBaselinepredictor). GivenP(X,Z)andP(Y,Z),theMaxEntbaselinepredictoris
definedby
(cid:88)
PMaxEnt(y|x) = P(y|z)P(z|x). (4)
z
One can easily show, see Lemma 8 in the appendix, that the MaxEnt predictor is correct for all DAGs
whose ”moral graph”4 (Lauritzen, 1996) does not have an edge X − Y. Since the overall shape of the
MaxEntpredictoralignswiththeoneoftheparentadjustmentLOVOpredictor,wecanagainusethethree-
step procedure to estimate it from finite data. A notable difference between the two predictors is that the
MaxEntbaselinegenerallyadjustsformorevariables. Incontrast,comparisonofregressionmodelswithan
equalnumberoffeaturesis”fairer”withrespecttostatisticalinaccuraciesentailedbyfinitedata. Therefore,
when using the MaxEnt predictor as a baseline against the parent adjustment predictor, we recommend
comparingagainstrandomadjustmentsetsZ ofequalsize. Inthisslightlymodifiedversion,thebaseline
R
is generically worse than parent adjustment whenever P(X,Y,Z) is Markov to a graph G, in which Z
R
doesnotd-separateX andY.
5 Experiments
5.1 LOVOpredictiongiventhetruemarginalgraphsG ,G
X Y
ThisexperimentexaminestheLOVOpredictorsproposedinTheorems5and6. First,wewanttoshedlight
on the question of how frequently Lemmas 2 - 4 succeed in excluding links. To this end, we randomly
generate1000Erdo˝s–Re´nyiDAGson10nodesbychoosingarandomorderingandtheninsertingeachedge
withprobabilitypvaryingbetween0.1and0.9. Foreachgeneratedgraph,wecheckif,foratleastonepair
of nodes (X,Y), we can rule out that they are linked based on the marginal graphs G ,G . For Lemma
X Y
2, we use ADMGs instead of DAGs; they are generated following the same procedure, except that we fix
p = 0.3 and additionally include bidirected edges with a probability q ∈ [0.1,0.9]. Figure 2 shows in how
manyrunsnosinglepairwithoutedgecanbefound,andtherefore,LOVOpredictionwouldnotbepossible.
InAppendixD.1,weillustratetheaveragenumberofidentifiedpairswithoutedgesineachgraph.
Next,weassesstheparentadjustmentLOVOpredictor,whereweusethecorrelationρbetweenX and
Y astheestimationtargetsinceitiseasiertoestimateascalarratherthantheentirefunctionP(Y | X = x)
orE[Y | X = x]. Wegenerategraphsasabovewithp = 0.3,and,forADMGs,q = 0.1. Toobtaindatain
accordance with the graphs, we employ a linear additive noise model, with noise uniformly distributed on
4Themoralgraphistheundirectedgraphobtainedbyremovingorientationsandconnectingparentsofacommonchild.
8Cross-validatingcausaldiscovery
Figure 2: For Lemma 2 and small values of q, Lemma 3 and p ∈ [0.3,0.7], and Lemma 4 regardless of p,
onlyinfewgraphsnosingleunlinkedpaircanbedetected,sothatLOVOisrealizableinmostcases.
Figure3: WhenprovidedwiththetruemarginalgraphsG andG ,theparentadjustmentLOVOpredictor
X Y
andtheLiNGAMLOVOpredictoroutperformthebaseline.
[−1,1] and coefficients drawn uniformly from [−1,−0.5]∪[0.5,1]. We set the sample size to n = 5000.
Again, based on the true marginal graphs G ,G , for each pair (X,Y), we evaluate whether they might
X Y
belinked. Ifnot,wecomputethethree-stepLOVOpredictorρˆLOVO accordingtoTheorem5,aswellasthe
baseline predictor ρˆBase, for which we calculate a MaxEnt predictor with a random adjustment set of the
samesizeastheunionofparentsmultipletimes,andthentaketheaverage. Moreover,wedirectlycalculate
the sample correlation coefficient ρˆ from P(X,Y) in order to estimate the prediction errors ρˆBase − ρˆ,
ρˆLOVO − ρˆ. For a more accurate error assessment, in the above steps, we never use all samples; instead,
we split the data into three parts of sample size n/3 each. The first two parts are used for P(X,Z) and
P(Y,Z), respectively, required in the three-step procedure, while the third part is reserved to estimate ρˆ.
Finally,weaveragetheresultsacrossallpairstoderivethecross-validationerrorsCVLOVO,CVBase,which
arecomparedinFigure3(leftandmiddle). Asbefore,weuseADMGscombinedwithLemma2andDAGs
with Lemma 4. In the ADMG setting, the LOVO predictor abstains in 0.5% of the replications, and in the
DAGsetting,never.
To analyze the LOVO via LiNGAM predictor, we sample DAGs and data as before and again use the
correlation as the estimation target. The right plot in Figure 3 compares the prediction error of LOVO to
theMaxEntbaselinepredictorwithallvariablesZ astheadjustmentset. Again,theLOVOpredictornever
abstains.
5.2 LOVOappliedtoDirectLiNGAMandRCD
Next, we apply the LOVO predictor to two causal discovery algorithms, namely DirectLiNGAM (Shimizu
etal.,2011)andRepetitiveCausalDiscovery(RCD)(MaedaandShimizu,2020). Thefirstmethodassumes
causal sufficiency, and, correspondingly, we rely on Lemma 3. The second method is able to detect latent
common causes. However, it represents them slightly differently by relying on the alternative definition of
ADMGs,whichforbidstheco-occurenceofadirectedandabidirectededge. AswedetailinAppendixD.2,
9Cross-validatingcausaldiscovery
Figure4: ThescatterplotsshowLOVOversusbaselinelossforparentadjustmentLOVOappliedtographs
estimatedwithDirectLiNGAM,andRCD;andforDLLOVOprediction.
Figure 5: The scatter plots show how LOVO performance correlates with causal discovery performance.
The LOVO error increases with the number of pairs misidentified as unlinked and with the SHD. The cor-
responding Spearman correlation coefficients included in the titles all significantly deviate from zero, with
p-values0.0,0.0,0.0,and4·10−44.
nevertheless,allstepsoftheparentadjustmentLOVOpredictorremainvalid,apartfromasmallmodification
required when using Lemma 2. We sample DAGs and the corresponding data as before, but double the
sample size and use the first half to learn G ,G . For DirectLiNGAM, we use p = 0.5 to ensure that
X Y
Lemma 3 often applies. For RCD, we adhere to p = 0.3 but decrease the number of nodes to 5 because
of its slower execution time. Figure 4 (left and middle) compares the LOVO cross-validation error to the
baseline. For DirectLiNGAM, LOVO abstained in 23% of the cases, and for RCD in 3%. To examine
whethertheLOVOcross-validationerrorindeedincreaseswiththenumberofmistakesinthelearnedgraphs,
we repeat the above experiment with varying sample sizes for learning the graphs, specifically, n =
learn
100,500,1000,5000. In Appendix D.3, we plot the LOVO loss for each value of n . Moreover, we
learn
concatenate all the results to calculate the Spearman correlation coefficient of the LOVO cross-validation
errorand
1. whetheranedgeX −Y existsinG,averagedoverall(X,Y)usedinthecross-validation.
2. the sum of the Structural Hamming Distances (SHDs) of Gˆ to G and of Gˆ to G , averaged
X X Y Y
overall(X,Y)usedinthecross-validation.
Including the first measurement is motivated by the fact that the parent adjustment LOVO predictor relies
ontheabsenceofanedge. Consequently,ifthelearnedmarginalgraphsimplythatnoedgeexists,whileit
actuallydoes,wecanexpecttheLOVOpredictortobeinaccurate. Thesecondmeasurementmorestraight-
forwardly evaluates the accuracy of the learned graphs. We obtain significant positive correlations in all
cases,aspresentedinFigure5.
10Cross-validatingcausaldiscovery
X Z 𝑒
!,$
Y Z
… …
… … Feed
… … Encoder ∥ Forward 𝜌 !,#
… …
𝑒
#,$
Figure 6: Architecture of our DL LOVO predictor: the encoder learns appropriate features of the two
marginaldistributionsfromwhichthecorrelationofX andY isinferred.
5.3 TrainingDLfortrivariateLOVO
So far, we have constructed LOVO predictors either by restricting to unlinked pairs or from assuming
LiNGAM. To support the hypothesis that also causal models without such restrictive assumptions help for
LOVO predictions, we now show that a deep learning architecture that has been proposed for causal dis-
covery can be modified to a LOVO predictor without explicit parametric restrictions. To this end, we built
on Ke et al. (2023), who use a transformer-based architecture to directly infer the adjacency matrix of the
causalgraphfromagivendataset. Weapplytheencoderpartfromtheirarchitecturetoeachmarginaldataset
to get representation vectors e and e . These representations are concatenated and used as input to a
X,Z Y,Z
feed-forward layer (see Figure 6 for an overview and Section D.4 in the appendix for more details). This
way, themodelcanbetrainedtopredictthecorrelationρ fromgivenmarginaldatasets. Figure4, right
X,Y
plot,showsthatitoutperformsourbaselineinmostcases.
ToshedlightonthechallengingquestionofwhetherourDLLOVOpredictorimplicitlylearnsacausal
representation,wetrytopredictthecausalstructurebetweenX andZ fromthelearnedrepresentatione .
X,Z
Ifasecondmodelcouldlearntomaptherepresentationtothecausalstructure,thissuggeststhatthelearned
featuresaresuitableforbothtasks. Tothisend,wethentrainasimplefeed-forwardnetworktopredictthe
underlyingcausalstructure,encodedascategories{→,←,↔,̸−}(seeagainSectionD.4formoredetails).
As a na¨ıve baseline, we consider the average training label (where the categories are represented via one-
hot-encoding). Indeed, we can predict the causal structure better than our baseline (see Figure 9 in the
appendix).
6 Conclusions
We have shown that causal hypotheses built via applying causal discovery to two Leave-One-Variable-Out
datasets can, in principle, enable the prediction of the statistical relations between the two variables X,Y
that were dropped. As a concrete LOVO predictor, we first propose prediction via adjusting for parents,
which relies on the absence of edges and is applicable to general causal discovery algorithms that produce
DAGsorADMGs. WefurtherdemonstratehowLOVOpredictioncanbecustomizedforspecificstructural
equation models, such as LiNGAM, enabling prediction even when a direct link is present. In simulation
experiments, we observe a significant correlation between the LOVO prediction error and the accuracy of
the estimated causal graphs. This reinforces our conjecture that the goodness of LOVO predictions can be
utilizedtoevaluate(inferred)causalrelationships.
Acknowledgements
ThisprojecthasreceivedfundingfromtheEuropeanResearchCouncil(ERC)undertheEuropeanUnion’s
Horizon 2020 research and innovation programme (grant agreement No 883818). Daniela Schkoda ac-
knowledgessupportbytheDAADprogrammeKonradZuseSchoolsofExcellenceinArtificialIntelligence,
sponsored by the Federal Ministry of Education and Research. Part of this work was done while Philipp
11Cross-validatingcausaldiscovery
M. Faller was an intern at Amazon Research Tu¨bingen. Philipp M. Faller was supported by a doctoral
scholarshipoftheStudienstiftungdesdeutschenVolkes(GermanAcademicScholarshipFoundation).
References
Thomes Cover and Joy Thomas. Elements of Information Theory. Wileys Series in Telecommunications,
NewYork,1991.
Anish Dhir and Ciara´n M. Lee. Integrating overlapping datasets using bivariate causal discovery. In Proc.
34thAAAIConferenceonArtificialIntelligence.AAAIPress,2019.
Philipp M. Faller, Leena Chennuru Vankadara, Atalanti A. Mastakouri, Francesco Locatello, and Dominik
Janzing. Self-compatibility: evaluatingcausaldiscoverywithoutgroundtruth. InProc.27thInternational
ConferenceonArtificialIntelligenceandStatistics,volume238,pages4132–4140.PMLR,2024.
SergioH.GarridoMejia,ElkeKirschbaum,andDominikJanzing. Obtainingcausalinformationbymerging
datasetswithMAXENT. InProc.25thInternationalConferenceonArtificialIntelligenceandStatistics,
volume151,pages581–603.PMLR,2022.
Clark Glymour, Kun Zhang, and Peter Spirtes. Review of causal discovery methods based on graphical
models. FrontiersinGenetics,10,2019.
NicolaGnecco,NicolaiMeinshausen,JonasPeters,andSebastianEngelke.Causaldiscoveryinheavy-tailed
models. TheAnnalsofStatistics,49(3):1755–1778,2021.
Luigi Gresele, Julius Von Ku¨gelgen, Jonas Ku¨bler, Elke Kirschbaum, Bernhard Scho¨lkopf, and Dominik
Janzing. Causal inference through the structural causal marginal problem. In Proc. 39th International
ConferenceonMachineLearning,volume162,pages7793–7824.PMLR,2022.
Peter Gru¨nwald and Philip Dawid. Game theory, maximum entropy, minimum discrepancy and robust
bayesiandecisiontheory. TheAnnalsofStatistics,32(4):1367–433,2004.
Siyuan Guo, Jonas Wildberger, and Bernhard Scho¨lkopf. Out-of-variable generalization for discriminative
models. InProc.12thInternationalConferenceonLearningRepresentations,2024.
Carol M. Hamilton, Mart´ı Aldea, Brian K. Washburn, Paul Babitzke, and Sidney R Kushner. New method
for generating deletions and gene replacements in escherichia coli. Journal of Bacteriology, 171:4617 –
4622,1989.
Patrik Hoyer, Dominik Janzing, Joris M. Mooij, Jonas Peters, and Bernhard Scho¨lkopf. Nonlinear causal
discoverywithadditivenoisemodels.InAdvancesinNeuralInformationProcessingSystems,volume21.
CurranAssociates,Inc.,2008.
Guido W. Imbens. Potential outcome and directed acyclic graph approaches to causality: relevance for
empiricalpracticeineconomics. JournalofEconomicLiterature,58(4):1129–79,2020.
AminJaber,MuratKocaoglu,KarthikeyanShanmugam,andEliasBareinboim. Causaldiscoveryfromsoft
interventions with unknown targets: characterization and learning. In Advances in Neural Information
ProcessingSystems,volume33,pages9551–9561.CurranAssociates,Inc.,2020.
Dominik Janzing. Merging joint distributions via causal model classes with low VC dimension. arXiv
preprint,2018.
EdwinT.Jaynes. ProbabilityTheory: TheLogicofScience. CambridgeUniversityPress,Cambridge,MA,
2003.
YutakaKanoandShoheiShimizu. Causalinferenceusingnonnormality. InProc.InternationalSymposium
onScienceofModeling,the30thAnniversaryoftheInformationCriterion,pages261–270,2003.
12Cross-validatingcausaldiscovery
NanRosemaryKe,SilviaChiappa,JaneX.Wang,JorgBornschein,AnirudhGoyal,MelanieRey,Matthew
Botvinick, Theophane Weber, Michael Curtis Mozer, and Danilo Jimenez Rezende. Learning to induce
causalstructure. InInternationalConferenceonLearningRepresentations,2023.
Murat Kocaoglu, Alexandros G. Dimakis, Sriram Vishwanath, and Babak Hassibi. Entropic causal infer-
ence. InProc.31stAAAIConferenceonArtificialIntelligence,pages1156–1162.AAAIPress,2017.
Se´bastienLachapelle,PhilippeBrouillard,TristanDeleu,andSimonLacoste-Julien. Gradient-basedneural
DAGlearning. InInternationalConferenceonLearningRepresentations,2020.
Kai Lagemann, Christian Lagemann, Bernd Taschler, and Sach Mukherjee. Deep learning of causal struc-
turesinhighdimensionsunderdatalimitations. NatureMachineIntelligence,5(11):1306–1316,2023.
SteffenLauritzen. GraphicalModels. ClarendonPress,1996.
DavidLopez-Paz,KrikamolMuandet,BernhardScho¨lkopf,andIlyaTolstikhin. Towardsalearningtheory
ofcause-effectinference.InProc.32ndInternationalConferenceonInternationalConferenceonMachine
Learning,volume37,pages1452–1461.PMLR,2015.
Takashi Nicholas Maeda and Shohei Shimizu. RCD: repetitive causal discovery of linear non-Gaussian
acyclic models with latent confounders. In International Conference on Artificial Intelligence and
Statistics,pages735–745.PMLR,2020.
Karthika Mohan and Judea Pearl. Graphical models for processing missing data. Journal of the American
StatisticalAssociation,116(534):1023–1037,2021.
Francesco Montagna, Nicoletta Noceti, Lorenzo Rosasco, Kun Zhang, and Francesco Locatello. Scalable
causal discovery with score matching. In Proc. 2nd Conference on Causal Learning and Reasoning,
volume213,pages752–771.PMLR,2023.
JorisM.Mooij,SaraMagliacane,andTomClaassen. Jointcausalinferencefrommultiplecontexts. Journal
ofMachineLearningResearch,21(99):1–108,2020.
MeikeNauta,DoinaBucur,andChristinSeifert. Causaldiscoverywithattention-basedconvolutionalneural
networks. MachineLearningandKnowledgeExtraction,1(1):312–340,2019.
JudeaPearl. Causality. CambridgeUniversityPress,2000.
JonasPeters,JorisM.Mooij,DominikJanzing,andBernhardScho¨lkopf. Identifiabilityofcausalgraphsus-
ingfunctionalmodels. InProc.27thConferenceonUncertaintyinArtificialIntelligence,page589–598.
AUAIPress,2011.
Jonas Peters, Peter Bu¨hlmann, and Nicolai Meinshausen. Causal inference by using invariant prediction:
identification and confidence intervals. Journal of the Royal Statistical Society Series B: Statistical
Methodology,78(5):947–1012,2016.
JonasPeters, DominikJanzing, andBernhardScho¨lkopf. ElementsofCausalInference–Foundationsand
LearningAlgorithms. MITPress,2017.
Alexander G. Reisach, Christof Seiler, and Sebastian Weichwald. Beware of the simulated DAG! Causal
discovery benchmarks may be easy to game. In Advances in Neural Information Processing Systems,
volume34,pages27772–27784.CurranAssociates,Inc.,2021.
Thomas Richardson. Markov properties for acyclic directed mixed graphs. Scandinavian Journal of
Statistics,30(1):145–157,2003.
PaulRolland,VolkanCevher,Mattha¨usKleindessner,ChrisRussell,DominikJanzing,BernhardScho¨lkopf,
andFrancescoLocatello. Scorematchingenablescausaldiscoveryofnonlinearadditivenoisemodels. In
Proc.39thInternationalConferenceonMachineLearning,volume162.PMLR,2022.
13Cross-validatingcausaldiscovery
DominikRothenha¨usler,NicolaiMeinshausen,PeterBu¨hlmann,andJonasPeters. Anchorregression: het-
erogeneousdatameetscausality. JournalRoyalStatisticalSocietySeriesB,83:215–246,2021.
Karen Sachs, Omar Perez, Dana Pe’er, Douglas A. Lauffenburger, and Garry P. Nolan. Causal protein-
signalingnetworksderivedfrommultiparametersingle-celldata. Science,308(5721):523–529,2005.
SaberSalehkaleybar,AmirEmadGhassami,NegarKiyavash,andKunZhang.Learninglinearnon-Gaussian
causal models in the presence of latent variables. Journal of Machine Learning Research, 21(39):1–24,
2020.
DanielaSchkoda,ElinaRobeva,andMathiasDrton. Causaldiscoveryoflinearnon-Gaussiancausalmodels
withunobservedconfounding,2024. arXivpreprint.
BernhardScho¨lkopf, DominikJanzing, JonasPeters, EleniSgouritsa, KunZhang, andJorisM.Mooij. On
causal and anticausal learning. In Proc. 29th International Coference on International Conference on
MachineLearning,page459–466.Omnipress,2012.
XinpengShen, SisiMa, PrashanthiVemuri, GyorgySimon, MichaelWeiner, PaulAisen, RonaldPetersen,
Clifford Jack, Andrew Saykin, William Jagust, John Trojanowki, Arthur Toga, Laurel Beckett, Robert
Green,JohnMorris,LeslieShaw,ZavenKhachaturian,GregSorensen,MariaCarroll,andKristinFargher.
Challengesandopportunitieswithcausaldiscoveryalgorithms: applicationtoAlzheimer’spathophysiol-
ogy. ScientificReports,10:2975,022020.
Shohei Shimizu, Patrik O. Hoyer, Aapo Hyva¨rinen, and Antti Kerminen. A linear non-Gaussian acyclic
modelforcausaldiscovery. JournalofMachineLearningResearch,7:2003–2030,2006.
Shohei Shimizu, Takanori Inazumi, Yasuhiro Sogawa, Aapo Hyva¨rinen, Yoshinobu Kawahara, Takashi
Washio, Patrik O. Hoyer, and Kenneth Bollen. DirectLiNGAM: A direct method for learning a linear
non-Gaussianstructuralequationmodel. JournalofMachineLearningResearch,12:1225–1248,2011.
Peter Spirtes, Clark Glymour, and Richard Scheines. Causation, Prediction, and Search. Springer-Verlag,
NewYork,NY,1993.
Mervyn Stone. Cross-validatory choice and assessment of statistical predictions. Journal of the Royal
StatisticalSociety: SeriesB(Methodological),36(2):111–133,2018.
Xiaohai Sun, Dominik Janzing, and Bernhard Scho¨lkopf. Causal inference by choosing graphs with
most plausible Markov kernels. In Proc. 9th International Symposium on Artificial Intelligence and
Mathematics,pages1–11,2006.
Jin Tian and Judea Pearl. Causal discovery from changes. In Proc. 17th Conference on Uncertainty in
ArtificialIntelligence.AUAIPress,2001.
Ioannis Tsamardinos, Sofia Triantafillou, and Vincenzo Lagani. Towards integrative causal analysis of
heterogeneousdatasetsandstudies. JournalofMachineLearningResearch,13:1097–1157,2012.
JiaqiZhang,KristjanGreenewald,ChandlerSquires,AkashSrivastava,KarthikeyanShanmugam,andCar-
olineUhler. Identifiabilityguaranteesforcausaldisentanglementfromsoftinterventions. InAdvancesin
NeuralInformationProcessingSystems,volume36,pages50254–50292.CurranAssociates,Inc.,2023.
Kun Zhang and Aapo Hyva¨rinen. On the identifiability of the post-nonlinear causal model. In Proc. 25th
ConferenceonUncertaintyinArtificialIntelligence,2009.
Kun Zhang, Biwei Huang, Jiji Zhang, Clark Glymour, and Bernhard Scho¨lkopf. Causal discovery from
nonstationary/heterogeneous data: Skeleton estimation and orientation determination. In Proc. 26th
InternationalJointConferenceonArtificialIntelligence,pages1347–1353,2017.
XunZheng,ChenDan,BryonAragam,PradeepRavikumar,andEricXing. Learningsparsenonparametric
DAGs. InProc.23rdInternationalConferenceonArtificialIntelligenceandStatistics,volume108,pages
3414–3425.PMLR,2020.
14Cross-validatingcausaldiscovery
A LOVOpredictorsforDAGswithtwoarrows
Asmentionedearlier,thecaseofthreenodesisparticularlychallenging,andourapproachpresentedinSec-
tion3.2maynotalwayssucceed. Therefore,wepresentalternativeLOVOpredictorsthatcanbebeneficial
in these cases. Specifically, we consider the ”promise”-scenario of three variable (X,Y,Z), where we are
giventheinformationthatthejointdistributionP(X,Y,Z)hasbeengeneratedbyacausaldirectedacyclic
graph(DAG)withtwoarrowsonly. InTable1,wegroupthe12possibleDAGsaccordingtothe3possible
skeletonsX−Z−Y,X−Y −Z,Y −X−Z,witheachskeletonallowingfor4differentDAGs. Wewill
see that in each of these groups, the collider is special, but the three other Markov equivalent DAGs entail
thesameLOVOpredictor.
No. 1-3: DAGs with X ⊥⊥ Y |Z This is the simplest case where the conditional independence directly
entailsthesolution
P(X,Z,Y) = P(X,Z)P(Y|Z), (5)
withoutanyparametricassumptions. ThesolutionismostintuitivefortheDAGsX → Z → Y (No.1)and
X ← Z → Y (No.2),wherethealgebraicstructureof(5)resemblesthedatageneratingprocessbyapplying
thestochasticmapP(Y|Z)tothejointdistributionofX,Z. While5iscertainlyalsovalidforX ← Z ← Y,
nowP(Y|Z)turnsintoan”anticausal”(Scho¨lkopfetal.,2012)conditional. Whenparametricassumptions
are imposed for causal conditionals (e.g. linear non-Gaussian models (Kano and Shimizu, 2003) or non-
linear additive noise models (Hoyer et al., 2008)), P(Y|Z) now results from Bayesian inversion of those
models. Forlinearmodels,thePearsoncorrelationbetweenX andY iseasilyobtainedvia5
ρ = ρ ·ρ . (6)
XY XZ ZY
IfX,Y havezeromeanandunitvariance,thebestlinearpredictorforY fromX thenreadsE[Y|X = x] =
ρ ·x. By slight abuse of terminology, we will therefore call ρ the ”linear LOVO predictor,” which
XY XY
implicitlyreferstothisconvention.
Note that this LOVO predictor coincides with the MaxEnt baseline predictor, and therefore, the cases
where(6)doesnotholdaretheinterestingonesforus.
No. 4: variableZ ascollider DuetoX ⊥⊥ Y,weignoreX andtakeP(Y)asthecorrectLOVOpredictor
forY. Wewilllatersee,however,thatthiscaseishardtorecognizefromthebivariatedistributionsbecause
thebivariatecausalmodelsX → Z andY → Z canalsooriginatefromthejointmodelsX → Y → Z and
Y → X → Z.
No. 5-7: DAGs with X ⊥⊥ Z|Y Now, the conditional distribution of X given Z is a concatenation of
Markovkernels
P(Z|X) = P(Z|Y)·P(Y|X). (7)
In linear models we conclude ρ ·ρ = ρ , from which we can directly construct the linear LOVO
XY YZ XZ
predictor.
InthecasewhereX,Y,Z arevariableswithfiniterangesX,Y,Z,weintroducethestochasticmatrices
P := (p(x|z)) andobtainthematrixequationP = P ·P . WheneverthematrixP
X|Z x∈X,z∈Z Z|X Z|Y Y|X Z|Y
isinvertible6,wethusobtain
PmediatorY := P−1 ·P . (8)
Y|X Z|Y Z|X
No. 8: variable Y as collider This is a negative case: as explained in Section 3.1, it is unclear how to
constructagoodLOVOpredictor.
5Thisfollowsfromzeropartialcorrelation,whichisdefinedbyρ = ρXY−ρX,ZρZ,Y .
X,Y|Z (cid:113) 1−ρ2 (cid:113) 1−ρ2
X,Z Y,Z
6Notethattheinverseisnotastochasticmatrixexceptforthetrivialcaseofdeterminism.
15Cross-validatingcausaldiscovery
No. DAG linearpredictor stochasticmatrix bivariate necessary
predictor causality conditions
1 X → Z → Y ρ = ρ ·ρ P = P P X → Z
XY XZ YZ Y|X Y|Z Z|X
Y ← Z
2 X ← Z → Y ρ = ρ ·ρ P = P P X ← Z
XY XZ YZ Y|X Y|Z Z|X
Y ← Z
3 X ← Z ← Y ρ = ρ ·ρ P = P P X ← Z
XY XZ YZ Y|X Y|Z Z|X
Y → Z
4 X → Z ← Y ρ = 0 P = P X → Z ρ2 +ρ2 ≤ 1
XY Y|X Y XZ YZ
Y → Z
5 X → Y → Z ρ = ρ /ρ P = P−1 P X → Z I(X : Z)
XY XZ YZ Y|X Z|Y Z|X
Y → Z ≤ I(Y : Z)
6 X ← Y → Z ρ = ρ /ρ P = P−1 P X ↔ Z I(X : Z)
XY XZ YZ Y|X Z|Y Z|X
Y → Z ≤ I(Y : Z)
7 X ← Y ← Z ρ = ρ /ρ P = P−1 P X ← Z I(X : Z)
XY XZ YZ Y|X Z|Y Z|X
Y ← Z ≤ I(Y : Z)
8 X → Y ← Z ? ? X ̸− Z
Y ← Z
9 Y → X → Z ρ = ρ /ρ P = P P−1 X → Z I(X : Z)
XY YZ XZ Y|X Y|Z X|Z
Y → Z ≥ I(Y : Z)
10 Y ← X → Z ρ = ρ /ρ P = P P−1 X → Z I(X : Z)
XY YZ XZ Y|X Y|Z X|Z
Y ↔ Z ≥ I(Y : Z)
11 Y ← X ← Z ρ = ρ /ρ P = P P−1 X ← Z I(X : Z)
XY YZ XZ Y|X Y|Z X|Z
Y ← Z ≥ I(Y : Z)
12 Y → X ← Z ? ? X ← Z
Y ̸− Z
Table1: AllpossibleDAGsonX,Y,Z withtwoarrows,togetherwiththeirLOVOpredictors.
No. 9-11: DAGswithY ⊥⊥ Z|X Hereweobtain
P = P P , (9)
Y|Z Y|X X|Z
whichimpliesthepredictor
PmediatorX := P P−1 , (10)
Y|X Y|Z X|Z
ifP isinvertible.
X|Z
No. 12: variableX ascollider AsforNo. 8,wecannotinferthestrengthoftheinfluenceofY onX and
abstainfromconstructingaLOVOpredictor.
Inspecting the column with the bivariate causal graphs in Table 1, we find only 7 cases where the joint
DAG can be uniquely identified from the bivariate graphs, namely the numbers No. 1, 2, 3, 6, 8, 10,
12. Unfortunately, recognizing 8 and 12 is not helpful for our purpose because we cannot offer a LOVO
predictor there. Moreover, the predictor of No. 1-3 coincides with our baseline. Overall, we are left with
onlytwocaseswhereaLOVOpredictorisrealizableandsimultaneouslybeatsthebaseline. Wecanimprove
upon that by considering additional conditions on the marginals from which the respective DAG can be
excluded, as listed in the last column of the table. For instance, X ⊥⊥ Z|Y implies that the dependence
16Cross-validatingcausaldiscovery
betweenX andZ cannotbelargerthanthedependencebetweenY andZ. Inthenon-parametriccase,this
can be formalized via the Shannon mutual information, for which we have the data processing inequality
I(X : Z) ≤ I(Y : Z). Forlinearmodels,wehaveρ ≤ ρ . Further,thecolliderX → Z ← Y isonly
XZ YZ
possibleifρ2 +ρ1 ≤ 1,otherwisethecorrelationmatrix
XZ YZ
 
1 ρ 0
XZ
 ρ XZ 1 ρ YZ ,
ρ 0 1
YZ
wouldnotbepositivesemi-definite.
B WhenisMaxEntLOVOcorrect?
ThefollowingsimplecriteriontellsuswhentheMaxEntpredictorisright:
Lemma 8 (MaxEnt baseline). Let G be a causal DAG connecting Z,X,Y and Gm be the corresponding
moralgraph.7 IfG doesnotcontaintheedgeX −Y,thentheMaxEntpredictoriscorrect.
m
Proof. DuetotheMarkovconditionforundirectedgraphs(Lauritzen,1996),X ⊥⊥ Y |Z ifthereisnolink
X −Y inGm,whichimpliesP(y|x,z) = P(y|z). □
IfZ consists of justone variableZ, the number ofDAGs for which Lemma 8holds can be counted as
follows: obviously,itonlyholdsforDAGswithlessthan3arrows. Fortheonewith2arrows,theskeleton
mustreadX−Z−Y. ToensurethatGmdoesnotcontainX−Y,therecannotbeacollideratZ,thusonly
the Markov equivalence class of X → Z → Y is remaining (with 3 elements). For the 6 DAGs with one
arrow, X ⊥⊥ Y |Z is always satisfied. Hence, we obtain 9 DAGs for which our MaxEnt LOVO is optimal,
andthetotalnumberofDAGswith3nodesreads25(Petersetal.,2017).
C Proofs
C.1 ProofofLemma1
Define the conditional cumulative distribution functions F (x|z) := P(X ≤ x|Z = z) and F (y|z) :=
X Y
P(Y ≤ y|Z = z). We then define structural equation models for P(Y|Z = z) P(Y|Z = z) with
uniformly distributed noise variables: X = f (Z,N ) and Y = f (Z,N ), where f (z,N ) =
X X Y Y X X
F−1(N |z) and f (z,N ) = F−1(N |z). Whenever we generate z-values with distribution P(Z),
X X Y Y Y Y
we obtain the right marginal distributions P(X,Z) and P(Y,Z). Note that this holds even for de-
pendent noise with arbitrary P(N ,N ) with the only constraint that their marginals need to be uni-
X Y
form (in other words, P(N ,N ) is a copula) since the dependences between N and N do not af-
X Y X Y
fect the marginals. When we choose P(N ,N ) = P(N )P(N ), we obtain the MaxEnt solution
X Y X Y
PMaxEnt(X,Y,Z) = P(X,Z)P(Y|Z). However, when we choose N = N , the variables X and Y
Y X
arepositivelycorrelatedwhenconditionedonZ. WhenwechooseN = (1−N )instead,X andY are
Y X
negatively correlated when conditioned on Z. Let CovMaxEnt(X,Y), Covpos(X,Y), and Covneg(X,Y)
denote the covariances of X,Y with respect to the three different choices of the dependences of N ,N .
X Y
Wethenhave
Covpos(X,Y) > CovMaxEnt(X,Y) > Covneg(X,Y).
Thisfollowsbecause
Covpos(X,Y|Z = z) > CovMaxEnt(X,Y|Z = z) > Covneg(X,Y|Z = z),
7FollowingLauritzen(1996),page7,themoralgraphcorrespondingtoaDAGGistheundirectedgraphthatcontainsanedge
a−bifandonlyifaandbaredirectlyconnectedinGoriftheyhaveacommonchild.
17Cross-validatingcausaldiscovery
holdsforanyz fromthelawoftotalcovariance:
Cov(X,Y) = E[Cov(X,Y|Z)]+Cov(E[X|Z],E[Y|Z]),
since the conditional expectations E[X|Z] the E[Y|Z] are both functions of Z, which only depend on the
respectivemarginaldistributionandarethereforeunaffectedbythedependencesofthenoisevariables. □
C.2 ProofofLemma2
Goingslightlybeyondthestatementinthelemma,weshowthefollowingcriteriaforexcludingeachpossible
edgetype.
1) WecanexcludetheexistenceofadirectedgeX → Y inGif
a) X occurs causally after Y in the sense that in G there exists an ancestor of X that is at the
X
sametimeadescendantofY inG ;or
Y
b) atleastoneofthefollowingimplicationsisviolated
i) P → X ∈ G =⇒ P → Y ∈ G ,
X Y
ii) X → C ∈ G =⇒ Y → C ∈ G orY ↔ C ∈ G ,
X Y Y
iii) X ↔ S ∈ G =⇒ Y ↔ S ∈ G ,
X Y
iv) Y → C ∈ G =⇒ X → C ∈ G .
Y X
2) Likewise,thearrowY → X canbeexcludedbyswappingtherolesofX andY.
3) ThebidirectededgeX ↔ Y canbeexcludedifoneofthefollowingimplicationsisviolated
a) X → C ∈ G =⇒ Y → C ∈ G orY ↔ C ∈ G ,
X Y Y
b) Y → C ∈ G =⇒ X → C orX ↔ C ∈ G .
Y X
Since the criteria in point 3)already entail the criteriain 1) and 2), allthree points taken togetheryield the
Lemma.
We prove statement 1), and the rest works similarly. Condition a) excludes X → Y because we had a
causalcycleotherwise.
Weshowb)byshowingitscontrapositive,thatis,ifX → Y ∈ G,thenallfourimplicationshold.
i) If P → X ∈ G , then P → X ∈ G since all directed edges in G \G are of the form P → C for
X X
P ∈ pa(Y),C ∈ ch(Y)butX isnotachildofY. CombinedwithX → Y thisyieldsP → Y ∈ E .
Y
ii) If X → C ∈ G , then X → Y → C ∈ G or X → C ∈ G. In the first case, it directly follows that
X
Y → C ∈ G . Inthesecondcase,Y ↔ C isaddedinthemarginalizationG sinceX → Y ∈ G.
Y Y
iii)IfX ↔ S ∈ G ,thenX ↔ S ∈ G,whichcombinedwithX → Y ∈ GimpliesY ↔ S ∈ G .
X Y
iv) If Y → C ∈ G , then X → C ∈ G or Y → C ∈ G, which both imply X → C ∈ G since
Y X
X → Y ∈ G.□
C.3 ProofofLemma3
Thelemmaisadirectconsequenceofconditions1a),1b)i),and1b)iv)intheproofintheprevioussubsection,
astheypertainonlytothedirectedpartsofG ,G .□
X Y
C.4 ProofofLemma4
The criterion for X having at least two children or not, as well as condition (1), can be derived from the
marginalizationrulethatC ← X → C ∈ GturnsintoanedgeC ↔ C inG andthisistheonlyway
1 2 1 2 Y
thatbidirectededgescanarise.
Conditions (2) and (3a) follow from rule 1b)iv) specified in the proof of Lemma 2, and (3b) follows
fromrule1b)i).
Turning to conditions (3c) and (3d), if X and Y have the same child C in G , G , the structure of
X Y
X,Y,C ∈ Gcaninprinciplebeeitherofthefollowing:
i)X → Y → C, ii)Y → X → C, or iii)X → C ← Y.
18Cross-validatingcausaldiscovery
Todifferentiatebetweenthem,weincludetheparentsinourconsideration. DenotingP = paG(X)∩Z,Q =
paG(Y)∩Z andR = paG(C)∩Z,weobtainthefollowingdifferencesinthemarginalgraphs.
i) ii) iii)
P QR P Q R P QR
G
X
X C X C X C
P Q R Q P R Q P R
G
Y
Y C Y C Y C
Thesedifferencesfurtherimplydifferentrelationsforthesetsofparentsinthemarginalgraphs:
i) paGX(X) ⊆ paGY(Y) ⊆ paGX(X)∪paGX(C),
ii) paGY(Y) ⊆ paGX(X) ⊆ paGY(Y)∪paGY(C),
iii) paGX(X) ⊆ paGY(C), paGY(Y) ⊆ paGX(C).
Combiningthemyieldstheconditionsinthelemma.
Finally, we prove that if neither of the conditions in the lemma apply, we can not identify the edge
type. First, note that if no condition in the lemma is satisfied, then either chGX(X) = chGY(Y) = {C}
and two of the conditions i) - iii) apply at the same time, or both have no child in the marginal graphs and
paGX(X) ⊆ paGY(Y)orviceversa. Forallthesecases,weneedtofindtwoDAGsG 1,G
2
ontheentireset
ofnodeswithdifferentedgetypesbetweenX andY butwiththesamemarginalizations. Inthecaseofone
commonchildandi)aswellasii),wecandefineG ,G by
1 2
paGi(X) = paGX(X),paGi(W) = paGY(W)forallW ∈ W \{X},i = 1,2.
Additionally,inG weincludeX → Y → C,andinG weaddY → X → C. Ifi)andiii)hold,wedefine
1 2
G ,G by
1 2
paGi(X) = paGX(X),paGi(W) = paGY(W)forallW ∈ W \{X},i = 1,2,
andincludeX → Y → C inG ,aswellasX → C ← Y inG . Allothercasesworksimilarly. □
1 2
C.5 ProofofTheorem6
(1) For a matrix M ∈ Rl×m, and an index i ∈ [l], M denotes the submatrix of all rows starting from
i:,:
the ith one. Similarly, for a subset A ⊆ [l], M is the submatrix consisting of all rows with indices in
A,:
A, and M the submatrix that arises by omitting all A rows. Throughout the proof we assume that W
A(cid:98),:
is enumerated as W = (X,Y,Z ,...,Z ). Following Salehkaleybar et al. (2020), we can rewrite (2) as
1 k
W = MN withthe”mixingmatrix”M := (I−Λ)−1 whichlinearlycombinestheindependent”sources”
N ,...,N . NotethatI −ΛisinvertiblesinceΛisstrictlylowertriangularafterapplyingsimultaneous
1 k+2
rowandcolumnpermutations,andtheentriesofM coincidewiththetotalcausaleffectsdefinedvia(3). By
observingonlythevariables(X,Z),wehavea(slightly)over-completeICAwithk+1observedvariables
andk+2sources,namely
(cid:18) (cid:19)
X
= M N (11)
Z (cid:98)2,:
whereM isthesubmatrixwiththerowforW = Y missing,and,similarly
(cid:98)2,: 2
(cid:18) (cid:19)
Y
= M N. (12)
Z (cid:98)1,:
ThemainideaoftheproofistoidentifyM fromP(X,Z),aswellasM from(Y,Z)andthencombine
(cid:98)2,: (cid:98)1,:
themtoreconstructM. FromTheorem15inSalehkaleybaretal.(2020),ifY hasatleasttwochildren,then,
19Cross-validatingcausaldiscovery
a) if Y has a unique (with respect to the topological order) oldest child W , then M can be iden-
j (cid:98)2,:
tified up to swapping the columns corresponding to Y and W and up to rescaling of the column
j
correspondingtoY.
b) Otherwise,M canbeidentifieduniquelyuptorescalingtheY column.
(cid:98)2,:
IfY hasexactlyonechildW ,thenthecolumninM correspondingtoY isamultipleofthecolumnfor
j (cid:98)2,:
W ,informulas,
j
M = λ M .
(cid:98)2,2 j,2 (cid:98)2,j
IfY hasnochildrenatall,M = 0. Inbothcases,obtainingN′ fromN byomittingN ,and,inthecase
(cid:98)2,2 2
ofonechild,additionallyreplacingN byN′ = N +λ N ,thevector(X,Z)fulfillsthecomplete,and
j j j j,2 2
thereforeidentifiable,ICAmodel
(cid:18) (cid:19)
X
= M N′.
Z (cid:98)2,(cid:98)2
Thus,
c) ifY hasatmostonechild,thesubmatrixM canbeidentifieduniquely.
(cid:98)2,(cid:98)2
RelatingbacktoM,inallcases,P(X,Z)uniquelydeterminesM orM ,withW beingtheoldest
(cid:98)2,(cid:98)2 (cid:98)2,{(cid:92) 2,j} j
child of Y. In scenario a), additionally, we know two candidate columns A,B, where either M ,M =
(cid:98)2,2 (cid:98)2,j
A,B up to rescaling or vice versa. To find the correct assignment, we use the information obtained from
P(Y,Z); that is, we identified all columns of M except for column 1, and at most one other column. In
(cid:98)1,:
particular, we determined M or M . Therefore, comparing whether A or B coincides with M
3:,2 3:,j 2: 2: 3:,2
or M , yields correct assignment as well as correct scale. However, this fails in one exceptional case,
3:,j
specifically when A = B up to scaling. Writing out the entries in M M in terms of the λ , and
2: 2: 2:,1 2:,j ij
using faithfulness, we obtain that this can occur only if ch(Y) = {X,Z } and ch(X) = {Z }, which is
j j
excludedintheassumptionsofthetheorem.
In case b), the only ambiguity in M concerns the scaling of the Y column, which again can derived
(cid:98)2,:
fromtheinformationwehaveonM .
(cid:98)1,:
The same holds for reversed roles. So, if for both (11), and (12) identifiability cases a) or b) apply, we
caninferM ,andM andcombinethemtoM.
(cid:98)2,: (cid:98)1,:
Ifcasec)appliesinoneoftheICAs,assumetheonerelatedto(X,Z),wearestillmissingthevalueof
m . Since Y has multiple, and X at most one child, according to Lemma 4, X → Y ∈ G if and only if
21
X has multiple children in G . If so, we can choose one of these children C. Employing that in the joint
X
model,alldirectedpathsfromX toC gothroughY,weobtain
m = m /m .
21 C1 C2
IfX ↛ Y,λ = 0,whichdeterminesm viaΛ = I −M−1.
21 21
Ifcasec)appliesinbothICAs,thenbothX,Y haveatmostonechild,whichcorrespondstoconditions
(3a)-(3d) in Lemma 4. In cases (3a)-(3c), we know X ̸− Y, and therefore λ = λ = 0, which gives
21 12
m ,m via Λ = I −M−1. In case (3d), we know that X and Y have the same child C in the marginal
12 21
graphs,whereasinthejointgraph,X → Y → C withoutadirectconnectionbetweenX andC,sincethis
wouldcontradictthefactthatX hasonlyonechild. Therefore,asabove
m = m /m .
21 C1 C2
Moreover,wethatfindm = 0duetoacyclicity. Again,thesameholdsforreversedroles. Finally,wecan
12
computeΛasΛ = I −M−1.
(2) To prove the identifiability of P(X,Y,Z), we use the fact that once the projected mixing matrices
M , M in (11), (12) are known, under the genericity assumption on the moments, all cumulants of
(cid:98)1,: (cid:98)2,:
the exogenous sources N can be identified (Schkoda et al., 2024, Lemma 5). These cumulants uniquely
determineP(N),which,combinedwiththeoverallmixingmatrixM,yieldsP(X,Y,Z). □
20Cross-validatingcausaldiscovery
Figure 7: For Lemma 4, the average number of detected absent edges (blue) is close to the true number of
absentedges(grey),whereastheothertwoLemmasdonotfindallabsentedges.
D Additionaldetailsfortheexperiments
D.1 HowoftendoLemmas2-4succeedinexcludingedges?
While the measurements depicted in Figure 2 give insight into how often the Lemmas find at least one
pair without edge per graph, which is the crucial factor for the realizability of LOVO, another interesting
questioniswhatproportionofunlinkedpairsarerecognizedbythelemmasassuch. Toaddressthis,Figure
7comparestheaveragenumberofdetectedunlinkedpairs(blue)tothenumberofpairsinthegraphthatare
actuallynotconnected,whichisexpectedtobe(1−p)·(cid:0)10(cid:1)
inanErdo˝s–Re´nyiDAGwithedgeprobability
2
p,and(1−p)(1−q)·(cid:0)10(cid:1)
forADMGswithdirectededgeprobabilitypandbidirectededgeprobabilityq
2
(grey).
D.2 LOVOforADMGswithoutconfoundedcausallinks
ThealternativeADMGdefinitionprohibitingconfoundedcausallinksusessinglebidirectededgesinplace
↔ ↔
ofourconfoundedcausallinks→or←. Therefore,whenprojectingaDAGtothemarginalADMGwithout
node X, one can follow the same procedure used with our definition and, in the end, substitute all con-
founded links with bidirected edges. This substitution leads to a loss of information, as highlighted in the
followingexample. Considerthethreegraphs
Y X Z , Y X Z , and Y X Z.
Removing X, according to the ADMG definition used so far, one obtains three distinct marginal graphs,
namely
↔ ↔
Y ↔ Z, Y → Z, andY ← Z.
In contrast, with the other definition, one always obtains Y ↔ Z. Despite these differences, Lemma 4
remains valid. The reason is that G is still a DAG, implying that each bidirected edge W ↔ W in G
1 2 Y
can be attributed to W ,W being children of X. Therefore, points 1 and 2 hold. Condition 3 addresses
1 2
the case that X,Y both have at most one child. Thus, in the marginal graphs, no bidirected edges occur
and the difference between the two ADMG definitions remains inconsequential. Similarly, one can show
that also Lemma 2 still holds. However, the next step, that is reading off the parents of X,Y from the
marginal graphs, becomes more involved. Specifically, as illustrated in the example, a bidirected edge
between Y ↔ Z ∈ G precludes their parent-child relationship: In G, we could have Y → Z ,Y ← Z ,
i Y i i
or Y ̸− Z . So, each sibling of Y in G could be a parent in G or not, and therefore, we can not infer
i Y
the parents whenever Y has siblings in G . Note that, when Lemma 4 is employed, we anyways exclude
Y
all pairs where sibGX(X) ̸= ∅ or sibGY(Y) ̸= ∅ since an edge might exist according to condition (1).
However,thisdoesnotapplywhenLemma2isused. Accordingly,inthiscase,werefrainfromaprediction
forallpairswithsiblings,inadditiontothepairsexcludedbythelemma.
21Cross-validatingcausaldiscovery
Figure8: EvolutionofLOVOpredictionlosswhenlearningsamplesizesincreases,andbythat,theaccuracy
ofthelearnedgraphsincreases.
D.3 LOVOappliedtoDirectLiNGAMandRCDwithvaryingsamplesize
This section provides more details on the experiment described in 5.2. Specifically, we analyze the LOVO
prediction loss for varying sample sizes, which is expected to relate to the accuracy of the learned graphs.
Indeed, the mean SHD of a marginal graph learned with DirectLiNGAM is 12.3,6.9,6.5,6.5 for n =
learn
100,500,1000,5000,respectively,and4.0,2.4,1.5,0.3forRCD.Figure8showsthattheLOVOprediction
loss tends to decrease with increased learning sample size. A notable observation is that, for RCD with
smaller sample sizes, the LOVO loss is very close to the baseline loss and often abstains from making
predictions, doing so in about 52% of the replications. This is due to RCD almost always only learning
bidirectedandnodirectededges,meaningitdoesnotcommittoanycausaldirections,whichmakesitharder
tochallengeitsoutput;compareSectionA11inFalleretal.(2024). Inthecaseswherepredictionsaremade,
thelearnedunionofparents, whichistheadjustmentsetinLOVO,isalmostalwaysempty. Consequently,
ρˆLOVO = 0, and both prediction errors are close to the absolute value of true correlation ρ . Moreover,
XY
the scatter plot reveals two clusters cluster characterized by error values below and above 0.3. The cluster
withlowererrorscorrespondstopairscorrectlyidentifiedasunlinked,whiletheotherclustercontainspairs
whereanedgeexists. Notethatwecanrelatethepointsinthescatterplottopairs, eventhougheachpoint
representsthecross-validationerroraveragedoverallpairs, sincethecross-validationerrorwasoftenonly
computedusingonepairandtheLOVOpredictorabstainedforallotherpairs. Theclusterrelatedtohigher
values gets smaller for increased sample size and eventually disappears. Also the number of how often
LOVOabstainsdecreases; to37%,29%,3%forn = 500,1000,5000. Incontrast,forDirectLiNGAM,
learn
LOVO abstained more rarely for lower sample sizes, specifically in 0.3%,2%,1%,23% of the replications
forn = 100,500,1000,5000.
learn
D.4 FurtherdetailsforSubsection5.3
Architecture Themaincomponentofthedeeplearningmodelusedinsection5.3istheencoderfromKe
etal.(2023). ThisencodergetsasinputadatamatrixW ∈ Rn×d,whereN ∈ Nisthenumberofsamples
and d ∈ N is the number of variables (in our case d = 2). Initially, each entry of this matrix is embedded
into Rh/2 with a linear transformation. We also embed the column index (i.e. the node identity) of every
entryintoRh/2. UnlikeKeetal.(2023),weuseanotherlineartransformationforthis. Weconcatenatethis
22Cross-validatingcausaldiscovery
Figure 9: Prediction error for recovering the causal structure from the learned representation of the deep
learning model on unseen examples (as described in Section 5.3). Our prediction model has a lower loss
thanthebaselineinallexamples.
inputembeddingandidentityembeddingtogetanh-dimensionalrepresentationofeveryentry. Eventually,
weaddarowofzerostotheinitialdatamatrixW,whichwillbeusedlaterfortheencodersummary. This
givesusaninitialembeddinge0 ∈ Rn+1×d×h.
Ke et al. (2023) propose to alternatingly use attention blocks that calculate attention weights between
samplesforeverynodeandbetweennodesforeverysample. Atattentionlayeriweexpectaninputtensor
ei−1 ∈ RN+1×d×h and start by applying a classical self-attention block to all matrices ei−1 ∈ Rd×h,j =
j
1,...,N that result from indexing the sample dimension in ei−1. Implementation-wise, this amounts to
passing ei−1 to a standard attention layer and considering the first dimension as batch dimension. We then
apply a feed-forward layer. This results in a tensor eˆi ∈ Rn+1×d×h. The second attention block is then
applied to the matrices eˆi ,j = 1,...,d, that result from indexing the nodes, i.e. we reshape the tensor to
:,j
have the second dimension as batch-dimension (and reshape it back after the attention block). After every
attentionblock,weaddafeed-forwardlayer,andweaddapre-layernormandaresidualconnectiontoevery
attentionblockandfeed-forwardlayer.
The final encoder summary is obtained by another attention block, where we consider the column di-
mensionthebatchdimensionagain,andweonlyusethe(n+1)-throwaskeyandallotherrowsasqueries.
Thisgivesusafinalembeddinge ∈ Rd×h,whichweflattentobeinRd·h.
InsteadofthedecoderproposedbyKeetal.(2023),weaddanotherfeed-forwardlayerthatreceivesthe
concatenatedembeddingsofeachmarginaldatasetasinputandoutputsascalar.
Forthesecondmodelthatpredictsthecausalstructurefromthehiddenrepresentationse ofthefirst
X,Z
model,wesimplyusedafeedforwardslayerwithfouroutputdimensionsandasoftmaxlayertoencodethe
fourpossiblecausalstructures{→,←,↔,noedge}ascategories.
Training NotethatKeetal.(2023)proposetotrainthemodelonadataset, whereeach”datapoint”con-
sistsofasyntheticallygeneratedadjacencymatrixastargetwithamatrixcontainingmultiplesamplesdrawn
from this graph as input features. For every such datapoint, we generate one of the 12 DAGs consisting of
two nodes and three edges (compare Table 1) with equal probability, and data with sample size 3n as in
Section5.1. Again,wesplitthedataintothreeequal-sizedsubsets. Letm ∈ Nbethenumberofdatapoints
ofthetrainingsetanddenotethesamplematricesforthej-thadjacencymatrixwith
Mj := {xj,zj}
X,Z i i i=1,...,n
Mj := {yj,zj}
Y,Z i i i=n+1,...,2n
Mj := {xj,yj} .
X,Y i i i=2n+1,...,3n
23Cross-validatingcausaldiscovery
Wethensolvetheminimizationproblem
m
argmin(cid:88)(cid:16)
f(Mj ,Mj )−ρˆj
(cid:17)2
,
X,Z Y,Z X,Y
f∈F
j=1
where F is the function class defined by the model architecture and ρˆj is the correlation coefficient
X,Y
computedfromthethirdpartofthesamplesMj
.
X,Y
TheparametersettingsofthemainmodeltrainingaresummarizedinTable2. Mostnotably,wegener-
ated100000pairsofmarginaldatamatricesandrespectivegroundtruthcorrelationcoefficientρ .
X,Y
Thesecondmodel(thatistrainedtopredictthecausalstructure)issimplyafeed-forwardnetworkwith
asinglehiddenlayer. Totrainit,wegeneratek ∈ Nmoremarginalsamples{xj,zj} andapplythe
i i i=1,...,M
encoderfromthepre-trainedmodelabovetogetadataset
{(ej ,s )} ,
X,Z j j=1,...,k
wheres ∈ {→,←,↔,̸−}denotesthetrueunderlyingcausalstructurethatgeneratedthej-datapoint. We
j
useacross-entropylosstotrainthismodel. Thesecondmodelistrainedon1000pairsofembeddingse
X,Z
and (one-hot encoded) underlying structure. We trained the LOVO prediction model using a squared loss
and the second model using the cross-entropy loss. Unless stated otherwise, we used the same parameters
forthemainmodelandthesecondmodel.
Computational resources The main deeplearning model from Section 5.3was trained on an AWSEC2
instance of type p3.2xlarge. These machines contain Intel Xeon E5-2686-v4 processors with 2.3 GHz
and 8 virtual cores as well as 61 GB RAM. The training ran less than an hour. All inference and further
experimentswererunonaMacBookProwithAppleM1processorand32GBRAMandcanberuninless
thananhour.
Hyperparameter Value
Batchsize 1
Learningrate 1e-4
Gradientclippingvalue 10
Epochs 2
Encoderlayers 3
Feedforwardhiddenlayers 1
Feedforwardwidening 4
Activation GELU
Attentionheads 8
Hiddendimensionh 64
Samplesperdataset 3000
Testexamples 100
LossTransitivePrediction MSE
TrainingexamplesTransitivePrediction 100000
LossStructurefromEmbedding Cross-entropy
TrainingexamplesStructurefromEmbedding 1000
Table2: Hyperparametersfortrainingthedeeplearningmodels.
24