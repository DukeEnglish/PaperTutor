Foundations for the psychological safety of human and autonomous vehicles
interaction
Yandika Sirgabsoua*, Benjamin Hardinb*, François Leblanca, Efi Railic, Pericle
Salvinib, David Jacksonc Marina Jirotkab and Lars Kunzed
aCapgemini Engineering, Toulouse, France;
bDept. of Computer Science, University of Oxford, UK;
cCapgemini Engineering, UK;
dBristol Robotics Laboratory, University of the West of England, UK;
*Yandika Sirgabsou @ yandika.sirgabsou@capgemini.com; Capgemini Engineering, 4 Avenue
Didier Daurat, 31700 Blagnac, France
*Benjamin Hardin @ benjamin.hardin@cs.ox.ac.uk, Dept. of Computer Science, University of
Oxford, UKFoundations for the psychological safety of human and autonomous vehicles
interaction
This paper addresses the critical issue of psychological safety in the design and operation of
autonomous vehicles, which are increasingly integrated with artificial intelligence
technologies. While traditional safety standards focus primarily on physical safety, this paper
emphasizes the psychological implications that arise from human interactions with
autonomous vehicles, highlighting the importance of trust and perceived risk as significant
factors influencing user acceptance. Through a review of existing safety techniques, the
paper defines psychological safety in the context of autonomous vehicles, proposes a risk
model to identify and assess psychological risks, and adopts a system-theoretic analysis
method. The paper illustrates the potential psychological hazards using a scenario involving
a family's experience with an autonomous vehicle, aiming to systematically evaluate
situations that could lead to psychological harm. By establishing a framework that
incorporates psychological safety alongside physical safety, the paper contributes to the
broader discourse on the safe deployment of autonomous vehicle and aims to guide future
developments in user-cantered design and regulatory practices.
Keywords: autonomous vehicles, systems safety, psychological safety, artificial intelligence
1 Introduction have been proposed in recent standards such as
Safety related systems1 such as aircraft, the SotIF (ISO 21448:2022 - Road Vehicles —
trains, and cars are becoming increasingly Safety of the Intended Functionality, n.d.),
complex, intelligent, and autonomous thanks to ANSI/UL 4600 (Standard for Safety for the
the application of new technologies such as Evaluation of Autonomous Products)
Artificial Intelligence (AI). However, the (Koopman et al., 2019) and ISO/DPAS 8800
current public acceptability of these systems is (Road Vehicles – Safety and artificial
still uncertain as it is not guaranteed to be intelligence) to encompass non-deterministic
universally welcomed (Xu et al., 2018; Liu et behaviour that is common of AI-enabled
al., 2019). The question arises of how well users systems such as AVs. However, these standards
feel supported by these systems and how to only address safety from a physical perspective
design systems in a way that not only ensures (preventing physical harms or property
physical safety but also promotes trust and damage, referred thereafter as physical safety).
acceptance. In particular, this paper focuses on Nevertheless, the biggest roadblocks to
this question in the context of autonomous AV mass adoption may be psychological, not
vehicles (AVs). technological (Shariff et al., 2017;Xu et al.,
To tackle emerging safety concerns in 2018). Trust and perceived risk have been
AVs, new solutions to guide systems design widely shown to be two major concerns of AVs
damage or environmental harm. Such systems
1 A safety related system is a system whose failure
can be found in various industries, e.g.
or malfunction may result in death or serious
aerospace, medical, nuclear, automotive.
injury to people and/or significant property(Kenesei et al., 2022; Thomas et al., 2020; Li et highlighting system design issues that lead to
al., 2019; Brell et al, 2019). Individuals often these hazards.
fear autonomous vehicles, but they may choose The remainder of the paper is structured
to use the technology despite this because of its as follows. The next section (2) outlines the
benefits (Cugurullo & Acheampong, 2023). current systems safety techniques and their
However, this potentially creates the situation concepts. It is followed by a definition of
where many people will be using a safety- psychological safety and a proposal of a risk
related system in their day-to-day life which assessment framework for psychological safety
they fear. We argue that regarding AVs, the in AVs, illustrated through a simple example
inherent relationship that one develops with a scenario in section 3. Section 4 is a discussion
vehicle is one of vulnerability. The safety of on the proposal and its limitations. Section 5
one’s life is placed in control of the automation, concludes the paper.
and this high-risk relationship leads to high
psychological stake. This relationship means it 2 State of the art
will be important for future AV developers to
2.1 Physical safety
consider how psychological hazards introduced
Safety (in a physical sense), commonly
by the autonomy can be mitigated, whether
defined as “freedom from unacceptable risk”
singular catastrophic events or repeated
where risk is a combination of likelihood of a
negative exposure.
harm occurring and its severity if it occurs, is a
To systematically evaluate
concept well-mastered in systems engineering.
psychological hazards, the foundations must be
Systems safety applies specialized knowledge,
laid for the analysis. From a systems safety
engineering and management principles,
perspective, this paper aims to:
criteria, analysis methods and techniques to
achieve acceptable mishap risk within the
(1) define the notion and scope of
constraints of operational effectiveness,
psychological safety in the context of
suitability, time, and cost throughout all phases
human interaction with AVs,
of the system life cycle (DoD, 2012). Such
(2) develop foundations for the assessment of
techniques include hazard analysis and risk
psychological risks and the integration of
assessment activities that are well defined by
psychological safety into AVs systems
standards, planned, and conducted as an
design context through the proposal of a
integral part of systems lifecycle trough
psychological safety risk model,
classical safety analysis methods such as Fault
(3) propose the adoption of a system-theoretic
Tree Analysis (FTA) (Roberts & Haasl, n.d.)
analysis method for psychological safety in
and Failure Modes and Effects Analysis
this context.
We introduce an example scenario in
which a family takes their new autonomous
vehicle on holiday that allows us to identify
potential psychological hazards and
demonstrate the use of our methodology forFigure 1. Overview of the basic STPA Method (N. Leveson & Thomas, 2018)
(FMEA) (SAE International, 2021). Each of the thinking and control theory (Leveson, 2004).
safety analysis technique that are used in STAMP extends the notion of safety (that it
traditional safety, deductive (e.g. FTA) or defines as “freedom from unacceptable losses
inductive (e.g. ETA) in nature, have strengths as determined by the system stakeholders”)
and weaknesses. The choice of method beyond physical. The System-Theoretic
generally depends on what is being addressed, Process Analysis (STPA) is an approach to
the type of system, and the stage of lifecycle. hazard analysis based on STAMP (N. Leveson
Usually, safety engineering will use a mixture & Thomas, 2018). It assists safety analysts and
of deductive and inductive techniques (or system designers in identifying hazards and the
otherwise called top down and bottom-up set of scenarios that can lead to an accident
techniques) to ensure completeness of coverage through 4 basic steps as shown in Figure 1. The
of identified issues. Complementary to classical approach provides benefits when the system
safety techniques, human factors related safety can be seen as a control system and/or it is
analyses include the Human Reliability applied at the systems level including the
Assessment (HRA) (NASA Chang, 2006) and human element.
the human factor Safety Critical Task Analysis
(SCTA) (Energy Institute, 2020). The focus of 2.2 Psychological safety and
these techniques is to assess how human errors limitations of the current methods
may contribute to system hazards. Therefore,
Psychological safety has been defined
the human aspects considered through HRA
in the occupational (workplace-related) health
and SCTA are from the perspective of ensuring
context as “a shared belief held by members of
physical safety.
a team that the team is safe for interpersonal
Emerging human factors and increased
risk taking” (Edmondson, 1999). Thus,
reliance on embedded software safety concerns
psychological safety considers not only direct
have driven the development of additional
psychological harms caused by another party,
safety techniques such as STAMP (System-
but also beliefs about the other party’s
Theoretic Accident Model and Processes), an
trustworthiness to do what they say and perform
accident causation model based on systemto a high standard. Psychological safety these psychological considerations are
"involves but goes beyond trust,” being a consequential, with trust being a significant
distinct concept apart from trust which affects predictor of intention to use AVs, perceived
various behavioural and organizational results usefulness being a strong predictor of
(Edmondson, 1999; Edmondson et al., 2004). willingness to re-ride, well-being affecting
The notion of psychological safety has intention to use an AV, and perceived risk,
also been discussed in the context of physical social trust, and perceived benefit being
human-robot interactions (Bauer et al., 2008; predictors of willingness to pay for an AV (Xu
Lorenzini et al., 2023; Akalin et al., 2022; et al., 2018; Meyer-Waarden & Cloarec, 2022;
Kamide et al, 2012). Psychological safety Liu et al., 2019). Despite these concerns,
around autonomous systems can be understood psychological safety has not been formally
as "the expectation that humans perceive defined for AVs, leading us to propose a
interaction with autonomous systems as being theoretical definition in this paper.
safe, and that the interaction does not result in
any unacceptable psychological harm such as 3 Proposal of a risk assessment
stress or trauma" (Lasota et al., 2017). This framework for psychological
motivates the question of what acceptable and safety in AVs
unacceptable psychological risk is, particularly
We propose to define the key
compared to the expected psychological risk
components of psychological safety in the
from interaction with a non-automated version
context of AVs and adopt a systems-theoretic
of a system.
analysis method relying on STAMP and STPA.
Psychological safety of autonomous
To consider this approach, let’s imagine a
systems is closely related to the notions of
family of 3, including two parents and an infant,
perceived safety which is believed to be a
on their first major journey with a SAE Level 42
determining factor for trust (Akalin et al., 2022;
autonomous vehicle. The parents are stressed
Kamide et al, 2012) and is influenced by a
from work and parental obligations, and to
user’s mental model of system functionality,
avoid adding any unnecessary stress, they have
testing, and reliability (Othman, 2023). In
opted to go on this holiday with their new AV.
contrast to the objective and deterministic
The vehicle is on the highway with the
nature of physical safety (where risk of harm is
autonomous mode engaged, and it is slightly
quantified as a product of severity and
raining. As the vehicle approaches an
likelihood), psychological safety derives from
interchange, another vehicle from the right
perception and feelings resulting from physical
engages in a manoeuvre to merge onto the same
interactions. Furthermore, risk of
lane as the AV. In such an event, the human
psychologically negative interactions may
driver would activate the left turn light, slightly
increase if the user perceives the system as
decelerate/brake and then gently move to the
another being or as having cognitive abilities
left. Once the third-party vehicle had entered
(Winkle et al., 2021). Prior work has shown that
2 A Level 4 vehicles is capable of driving autonomously under However, depending on the design of the vehicle, the human
certain designated geographic, environmental or operational driver may retain optional manual controls out of the specified
conditions without the intervention of the human driver. designated conditions (SAE MOBILUS, n.d.).the lane, our driver would check the traffic behaviour occurs again, as illustrated in Figure
coming from behind, activate the right turn 2. One might intuitively recognise several
signal, and pull back into the original lane. factors in this scenario that puts this family at
However, as the 3rd party vehicle enters the psychological risk, but how can we be sure
merge, the AV engages in an unexpected these factors are systematically identified and
behaviour by alternatively swerving left to right addressed during vehicle design? This
while pulling to the left, as illustrated in Figure illustrative example will be used throughout the
2. Realizing that something might be wrong, the paper as we work through how these situations
human requests takeover. His request is denied, can be identified and analysed.
as the Automated Driving System (ADS)
manages to complete the manoeuvre while
simply alerting the user of a braking system 3.1 Theoretical definition of AV
malfunction and stating that the ADS is psychological safety and the scope
attempting to correct the issues. However, the of our analysis
message is so generic that the human driver is
Definition. We define autonomous vehicle
unable to understand it in the limited amount of
psychological safety as the absence of
time that the driver must process the message.
unacceptable psychological risk resulting from
After the third-party vehicle had entered the
automation use or misuse, where psychological
lane, the ADS attempts to pull back to the initial
risk is a combination of the probability of
lane. As it does so, the same unexpected
occurrence of psychological harm and the
severity of that harm. This definition draws
heavily from the concepts of robotic
psychological safety (Lasota et al., 2017) and
occupational psychological safety
(Edmondson, 1999) introduced previously.
Components of psychological safety. We
characterize psychological safety as being
comprised of several components including
trust, perceived control, perceived risk,
psychological empowerment, responsibility
and liability, comfort, predictability, perceived
support, effort-reward, and low demands (Hoff
& Bashir, 2015, Brell et al., 2019, Wang et al.,
2020, Guettas et al., 2019; Harvey et al., 2017).
Finally, (Harvey et al., 2017) consider the stress
related to role ambiguity and role conflict,
Figure 2. Illustrative example of an AV's tactical
which may play a role in AV teamwork
manoeuvre malfunction. Ego vehicle (red)
experiences a braking malfunction during a lane concerns. Psychological empowerment and
change related to another vehicle (blue) merging emotional exhaustion have also been shown to
onto the motorway.be influencing components of psychological system. One example state is situational
safety (Zhou & Chen, 2021). anxiety. During one interaction, the individual
Timeframe. This analysis is most may begin with a state of low situational
suited to immediate psychological risks; anxiety, while during another interaction, they
however, it is intended to be adaptable to long- may begin with a state of high situational
term risks. Immediate risks are easier to identify anxiety. Because humans perceive threats
and measure, such as the psychological differently based on their psychological state,
response to a sudden braking manoeuvre. different individuals can have varying reactions
Nevertheless, it has been shown in other to the same negative incident (Muris, 2003;
domains that psychological effects can also Kavcıoğlu et al., 2021; Udwin et al., 2000,
compound over many minor negative Bennett et al., 2019). For this reason, we
situations, leading to effects that develop much assume that the presented risk analysis applies
later and can be highly individual (McHugh et to individuals within an average range of
al., 2018; Udwin et al., 2000). psychological state (e.g. low to moderate
Stakeholders. The scope of this situational anxiety, no prior trauma from AV
analysis is aimed primarily at the driver or interaction, average confidence level in the
overseer of an AV but is valid for other possible AV). It may be impossible to ensure system
stakeholder interactions with the vehicle. Most safety for individuals of all backgrounds, and
notably, the analysis should be practicable for thus we must limit the scope of stakeholders to
passengers of an AV who are not responsible the above conditions.
for overseeing the operation of the autonomy. Mitigating psychological risks vs.
The analysis can also extend to other road users improving psychological state. In theory, the
and vulnerable road users that will have their psychological state of an individual could be
own mental model of the vehicle’s automation improved such that risks are not only mitigated,
and maintain perceptions of the vehicle, albeit but the user is given a more positive state than
from a different perspective. their baseline. This is certainly an important
Level of autonomy. The scope of this area of future research but is outside the scope
analysis is aimed at SAE Level 2 to Level 5, of this work. Here, we aim to assess risk and
which are 4 of the 6 levels of driving mitigation for risk.
automation ranging from no driving automation
(Level 0) to full driving automation (Level 5) 3.2 Proposed psychological safety risk
(SAE MOBILUS, n.d.). Each level of model
autonomy will be accompanied by different
Adapting concepts from classical safety
mental models and expectations; however, our
in the automotive domain and the STAMP
analysis still applies across autonomy levels
accident causation model, we define
where the system is expected to perform some
psychological safety concepts and develop the
or all the functions of driving.
risk model as shown in Figure 3. Important to
Baseline psychological state. Each
stakeholder will have a different psychological
state before interacting with the autonomousFigure 3. Proposed psychological safety concepts.
note that this model is not aimed to diagnose from toward the AV. Consequently, defining a
psychological outcomes from events, but rather psychological loss is based on the identification
highlight unsafe control actions that could lead of the stakeholders and the elicitation of their
to potential psychological risk. psychological stakes.
The first concept of our model is Another concept related to a
“psychological stake” defined as "a psychological loss is the concept of
psychological need or well-being that has a “psychological hazard”, defined as “a potential
certain value for the stakeholder(s)" such as source of psychological harm leading to a
feeling safe. As illustrated by ① in Figure 3, psychological loss”. As illustrated in Figure 3,
stakeholders have psychological stakes whose a psychological hazard (see ③) is associated
violation leads to psychological losses (see ② with the concept of “psychological state” (④)
in Figure 3). We define “psychological loss” as and a particular state of a system that together
“any unacceptable psychological harm to with a set of external conditions can potentially
stakeholders resulting from the interaction with lead to a psychological loss. According to
a system”. (Martin, 1990), a psychological state is simply
In our scenario, the driver has the defined as “the mental or emotional condition
psychological stakes of feeling safe, feeling of an individual.”
protected, or ensuring protection of passengers In our scenario, the driver would be in a
under their responsibility, and trusting the psychological state of high stress considering
automated vehicle. The violation of those the work situation he leaves, the responsibility
stakes would lead to psychological losses that for protecting his family as passengers, and the
can be the fear of having their passengers or stress of driving under poor weather conditions.
themselves being harmed or the loss of trustSeverity (S) Controllability (C)
Exposure
Psychological Effect Severity (E) C1 (Simple) C2 (Normal) C3 (Difficult)
Class
E1 (Very
Low)
Short Term
E2 (Low)
(e.g. Increased heart rate, S1
Increase in blood pressure, (Marginal) E3 PsySIL A
Adrenaline release) (Medium)
E4 (High) PsySIL A PsySIL B
E1 (Very
Low)
Medium Term
S2 E2 (Low) PsySIL A
(e.g. Psychological strain, Psychosomatic health
(Moderate) E3
symptoms) PsySIL A PsySIL B
(Medium)
E4 (High) PsySIL A PsySIL B PsySIL C
E1 (Very
PsySIL A
Low)
Long Term
S3 E2 (Low) PsySIL A PsySIL B
(e.g. depression, anxiety,
(Critical) E3
cardiovascular disease) PsySIL A PsySIL B PsySIL C
(Medium)
E4 (High) PsySIL B PsySIL C PsySIL D
Table 1. PsySIL determination based on a psychological Integrity Levels (SIL) in classical safety (IEC
severity classification and ISO 26262.
61508-1:2010 | IEC, n.d.). PsySIL is “a
With the vehicle operating under an SAE criterion for defining the stringency of the
Level 4 autonomous mode, examples of applicable psychological safety goal
psychological hazards could include the commensurately with the magnitude of a
following: 1) vehicle performs apparently psychological loss”, with PsySIL A being the
unsafe unexpected driving manoeuvre without least stringent and PsySIL D the most stringent.
warning the passenger when responding to an Like in Automotive Safety Integrity Levels
external situation (solicitation), 2) vehicle does (ASIL), PsySIL is defined by a combination of
not maintain safe distance from nearby objects, three parameters (exposure, controllability, and
or 3) vehicle performs an action that causes severity) as illustrated in Figure 3 and Table 1
discomfort due to unnatural motion (see ⑥), with the particularity of PsySIL’s
characteristics. severity parameter being based on a
To enable the control of psychological psychological harm scale described in (Taibi et
hazards, we propose to introduce the notion of al., 2022). The latter classifies the severity of a
Psychological Safety Goal inspired from ISO psychological harm according to the projection
26262, which is defined as "a high-level of their effect over time (short, medium, and
requirement specifying the conditions and long term), and consequently defines 3 classes
reinforcement of the system to be met in order of severity (marginal, moderate, and critical).
to prevent psychological hazards and mitigate As it relates to our scenario, let’s consider
psychological losses” as shown by ⑤ in Figure induced high stress being the psychological
3. Next, we must determine the acceptability of harm. The exposure ranks quite high (E4, cf.
a psychological loss. To this end, we define the Table 1) as this simple lane change during a
notion of Psychological Safety Integrity Level merge is a common scenario in highway
(PsySIL), a new concept inspired from Safety driving. The controllability of the situation wasFigure 4. Overview of adaptation of STPA to psychological safety.
C1 (simple) as it is a situation with multiple illustrated in Error! Reference source not
manoeuvre options which the vehicle should be found.. Like the classical STPA, the
expected to navigate regularly. The severity is methodology is divided in 4 steps: 1) define the
S2 (moderate) but not critical since even such a purpose of the analysis, 2) model the control
high stress situation would not lead to serious structure, 3) identify Unsafe Control Actions
psychological outcomes (keeping in mind our (UCA), and 4) identify loss scenarios.
previous considerations about an average
stakeholder’s background) such as depression, 3.3.1 Define the purpose of the analysis
anxiety, or cardiovascular disease which are (step 1)
examples of psychological harm that would be
Defining the purpose of the analysis
classified as critical. Moreover, no accident
necessitates establishing the system boundary,
occurred, and there was significant space
which distinguishes what is internal to the
around all vehicles. Referring to the Table 1,
system under analysis and what is considered
this combination of a high exposure, simple
part of the environment. Defining the purpose
controllability and moderate severity results in
of the analysis also includes identifying the
a PsySIL A level. However, one can imagine
psychological losses we want to prevent as well
how controllability must be reassessed if the
as identifying psychological hazards potentially
situation involved high traffic density with little
leading to those losses. The step completes with
margin for error in relation to other vehicles.
the specification of psychological safety goals
intended to prevent or mitigate psychological
3.3 Proposed psychological safety hazards and losses.
analysis method
Our adaptation of STPA to Define the system boundary. In our
psychological safety analysis in the context of example, the system is comprised of the human
human-autonomous systems interaction is driver, the Automated Driving System (ADS),and the longitudinal and lateral control sub- following examples of system level
systems, as well as interfaces (datalink and psychological hazards as listed below.
physical) between these elements. To simplify
our scenario analysis, we exclude the other • H1 - ADS Controller performs sudden
vehicle passengers and consider only the driver tactical driving manoeuvre without
to be within the system boundary. External informing human driver (L2)
elements such as the road, traffic signs, and • H2 - Vehicle deviates from expected
other road users (other vehicles, pedestrians, behaviour when performing Dynamic
animals) are excluded from the system and Driving Task (DDT) (L2)
considered to be part of the environment as • H3 - ADS Controller ignores human driver
illustrated in Figure 5. With the system requests (e.g. takeover, emergency stop)
boundary defined, psychological safety hazards (L1, L2, L3)
and losses can be identified. • H4 - Vehicle performs DDT while out of
Operational Design Domain (ODD) (L3)
• H5 - Human driver misinterprets
information from ADS Controller (L1, L2)
As the list demonstrates, one hazard can
be related to more than one loss. In our
scenario, H2 occurs when the vehicle
experiences an unexpected behaviour during
Figure 5. System boundary delimitation.
the DDT (uneven and erratic braking). This
Identify psychological losses and hazards.
raises the likelihood for both L1 and L2
Losses must be derived from the violation of
psychological losses, as the human driver may
stakeholder stakes. In our simple example, the
both lose trust in the braking system (L1) and
identified stakeholder under consideration is
be shocked at the behaviour, elevating their
the human driver. Their stakes may include
stress (L2).
trusting the automated vehicle, feeling safe (i.e.
psychologically), being safe, ensuring the
Specify psychological safety goals.
physical safety of the passengers. Hence, the 3
Psychological safety goals express what the
top level psychological losses deriving from the
system must do to prevent psychological
violation of these stakes are listed below:
hazards or mitigate associated psychological
losses, and we derive these goals from the
• L1 - Loss of trust
previous hazard identification. In the context of
• L2 - Stress, fear, shock, or trauma
our example, the following psychological
• L3 - Loss of life, injury, or property
safety goals are specified:
damage
• SG1 - ADS Controller must properly
The identification of the psychological
inform the human driver when performing
losses enables the identification of the
a sudden emergency DDT manoeuvre (H1)• SG2 - The vehicle must perform DDT controller will respond to the human driver’s
manoeuvre in a manner that causes least request by performing the required strategic
stress to the human driver (H1, H2, H5) planning, tactical manoeuvring, and basic
• SG3 - ADS Controller must comply to operational vehicle motion control. The ADS
human driver request (takeover, controller in return provides feedback
emergency stop) (H3) information such as warnings, system state, or
• SG4 - Vehicle must comply with ODD certain events to the human driver. This
specification (H4) information enables the human driver to form
their belief about the current state of the system
• SG5 - ADS user must monitor and
based on their level of situation awareness. The
understand the state of DDT performance
vehicle, which sits at the lowest level of the
(H5)
hierarchical control structure executes the
As an example of one psychological control commands from the ADS controller or
loss mitigation in our scenario, the vehicle human driver. Through sensing, the ADS
could have provided more informative controller collects necessary data related to
warnings of vehicle behaviour, possibly a post- external objects and vehicle state and uses this
hoc explanation, and a description of if information to form the environment models
maintenance is needed, similar to common and the controlled process used by its control
explainability methods in autonomous driving algorithm.
research (Omeiza et al., 2021; Kim et al., 2023). Assign responsibilities (allocate
psychological safety goals). The next step of
3.3.2 Model the psychological safety
control structure (step 2)
The second step focuses on constructing
the psychological safety control structure that
will be used for the psychological safety
analysis. In STAMP, a control structure is an
abstract model of the system that captures
functional relationships and interactions
between system components modelled as a set
of feedback control loops (N. G. Leveson,
2012b). We propose a control structure
arranged in three levels of hierarchy: the
human, the ADS controller, and the controlled
process, as shown in Figure 6.
Build the control structure. In the
proposed psychological control structure, the
human driver will make a decision and transmit
their intention to the ADS controller. The ADS Figure 6. Proposed psychological safety control structure.the STPA analysis is to assign responsibilities (1) Not providing the control action,
to elements of the system. The following lists (2) Providing the control action (for instance
examples of responsibilities assigned to the unexpectedly, in excess, or too quickly),
human driver and the ADS controller in the (3) Providing a potentially safe control action
context of our illustrative scenario. but too early, too late, or in the wrong
order,
Human driver responsibilities:
(4) Providing a control action that lasts too
long or is stopped too soon.
• R1 - Understand the state of the DDT
performance (SG1, SG2) Utilizing these categories, we formulate
• R2 - Decide and request DDT takeover if the following example UCAs in the context of
needed (SG3) our scenario:
ADS controller responsibilities: • UCA1 - While experiencing a brake
malfunction (unbalanced braking torque)
• R3 - Respond to ADS Human driver
during a lane change manoeuvre, ADS
requests (SG1, SG2)
controller commands cause vehicle to
• R4 - Command DDT control (SG3)
alternatively swerve left to right (H1).
• R5 - Inform the human driver (SG3)
• UCA2 - While mitigating a brake
• R5 - Avoid unexpected behaviour (SG2) malfunction (unbalanced braking torque)
• R6 - Monitor driver psychological state during a lane change manoeuvre, ADS
(SG2) controller does not provide the human
driver with enough clear information about
3.3.3 Identify Unsafe Control Actions the underlying issue (H5).
(step 3) • UCA3 - While experiencing an unexpected
The goal of the third step is to use the behaviour (vehicle swerving left to right
psychological safety control structure built while changing lane), the ADS controller
previously to perform an analysis to identify does not respond to the human driver’s
psychologically Unsafe Control Actions (UCA) request to take over (H2, H3) and increases
that could lead to psychological hazards. the human driver’s stress level.
STAMP defines an UCA as “a control action
that, in a particular context and worst-case 3.3.4 Step 4: Identify Psychological
environment, will lead to a hazard” (N. G. loss scenarios (step 4)
Leveson, 2012a). The focus of our analysis The last step identifies psychological
being psychological safety, our UCAs are to be loss scenarios. A psychological loss scenario
understood as control actions (including aspects describes the causal factors within our
of the control algorithm, interactions, control previously defined system boundary that can
commands, and feedback) that will negatively lead to psychological hazards. STPA
impact the human driver’s psychological state. recommends considering two types of loss
STPA provides the following 4 generic scenarios: 1) scenarios leading to unsafe control
categories of UCAs: actions occurrence and, 2) scenarios leading toTable 2. Examples of psychological loss scenarios related to the illustrative example
control actions not being executed or being
improperly executed. Furthermore, STPA 3.4 Psychological safety
provides the following four general categories interpretation
for causal factors explaining why a controller
A new interpretation of psychological
might provide (or not provide) a control action
safety adapted for AVs was proposed,
that is unsafe:
representing a new opportunity for autonomous
1. failures involving the controller,
systems safety research. By defining
2. inadequate control algorithm,
psychological safety, we can examine new
3. unsafe control input,
impacts of interactions between AVs and
4. inadequate process model.
passengers. In our example case, the vehicle
performed in a physically safe manner
4. Discussion
(avoiding collision), but we find that its erratic
Three main contributions identified in behaviour limits driver belief that the system is
this paper include the definition of performing safely and that their safety is
psychological safety in the context of AVs, its ensured. This allows us to uncover a set of
risk model, and analysis method based on Unsafe Control Actions that can be avoided
STPA. through psychologically safe system design.
While not investigated as the most threatening
psychological losses in our example scenario,
we also see how the psychological loss
scenarios of fear and stress can relate toenvironment conditions, baseline psychological concepts defined earlier in the psychological
state, and system familiarity. Rainy weather and safety risk model. The 2nd step proposed a
poor visibility may lead to automation distrust, psychological control structure that enabled us
the passengers may enter the vehicle in a state to 1) capture the psychological dimension in the
of high stress and susceptibility to stress interaction between systems entities
response, and the driver’s unfamiliarity with the considering the driver’s psychological state,
system means that they are unable to recognize situation awareness, and beliefs and 2) assign
and mitigate malfunction of the braking system. responsibilities to systems elements to prevent
or mitigate the identified psychological hazard
3.5 Psychological safety risk model and losses. The responsibility assignment can
be viewed as the formal allocation of
The risk model conceptualized the
psychological safety requirements to system
psychological harm impact mechanism in the
entities, which is not currently done in current
context of human-AV interaction showing how
systems design practices. The 3rd and 4th steps
the combination of system behaviour,
of the methodology identified the potential
environment factors and the human related
causes of the psychological hazards through
individual factors such as psychological state or
UCAs, and psychological loss scenarios
stake affect their appreciation of the system’s
leading to these UCAs respectively. From these
performance. Furthermore, the risk model
scenarios, it is possible to further refine
introduces a safety construct of psychological
psychological safety requirements for the
safety goals for mitigating psychological
system. For UCA2.SC1 for instance, such
hazards and losses from which requirements
further refinement could state the need for “the
specific to psychological safety can be derived.
ADS controller to ensure explainability of
Such goals are allocated accordingly to system
messages sent to the human driver.” Similarly,
entities (ADS controller for instance) thanks to
for UCA3.SC2, one could define the following
the responsibility assignment as described in
additional high-level requirement: “The ADS
the methodology. Finally, the risk model
controller must respect human agency and
introduced the notion of PsySIL, adapted from
oversight.” However, these new requirements
SILs and integrating a psychological severity
can conflict with other requirements such as
scale as a way of appreciating the integrity
physical safety or cybersecurity. To address this
required to avoid psychological losses. These
challenge, one possibility would be to perform
concepts allow us to methodology integrate
a trade-off analysis.
psychological safety notions into the AV
system design concept.
3.7 Limitations and future work
3.6 Psychological safety analysis The paper aimed to address a general
method problem posed by the emergence of
psychological safety as a concern in humans
The 1st step of the proposed method
and autonomous systems interaction. However,
introduced a new category of hazards and
as specified throughout the paper, the scope of
requirements for system design, showing the
the definitions, proposals and analysis was
relevance and usefulness of the theoreticalmainly focused on AVs. Therefore, more estimate the risk for specific subpopulations
evidence is required to extend the proposals to that might be system stakeholders.
autonomous systems in general, which opens
avenues for future research. Moreover, we 4 Conclusion
acknowledge that even in the context of AVs,
This paper expressed the need to not
the proposal does not aim to replace the existing
only ensure physical safety of autonomous
safety approaches (ISO 26262, SotIF); instead,
vehicles but also psychological safety. Existing
it complements them by addressing potential
systems safety standards and methods are
psychological harm. Another limitation lies in
limited in their current form for addressing
the textual description of the different artefacts
psychological safety of human-autonomous
resulting from the analysis (goals, losses,
system interaction but can be modified to tackle
hazards…) and a need for a more formal
this limitation. The paper defined the notion of
traceability between these elements. To address
psychological safety and proposed a method of
this limitation, a potential solution could consist
analysis to integrate it into the development
of an integration of the defined concepts in a
process of autonomous systems. In this context,
systems engineering tool through a semi formal
the paper proposed to address psychological
meta model. We also acknowledge that the
safety through an adaptation of STAMP and
scope of this analysis is within a western
STPA. First, a risk model is defined for this
context and that psychological estimations of
need. The risk model defined and introduced
vehicle behaviour may vary somewhat across
the notion of psychological safety analysis in
cultures.
the autonomous systems interaction. Then a
Psychological safety is an inherently
methodology is proposed and applied step by
broad concept that encompasses many of the
step on the illustrative example to showcase its
previously investigated aspects of AV ridership
relevance and usefulness. We provide this
such as trust and perceived control. However,
proposal as a method that developers of
future work should continue formalizing the
autonomous systems can use to address
components to enable psychological safety to
psychological risk during the design,
be dissected into easier-to-address sub-
integration, and test phases of their
concepts. Additionally, our risk analysis falls
development. Future work will attempt to
far short of being able to estimate how
address the identified limitation by generalizing
individual experience can heighten the
the risk model to other autonomous systems,
psychological risk of an event. For instance, it
ensuring traceability of the analysis artefacts
has been shown extensively that certain
and ensuring the cohabitation of psychological
subpopulations tend to have higher levels of
safety with physical safety.
fear towards AVs (Cugurullo & Acheampong,
2023; Thomas et al., 2020). To this end, our
Acknowledgement
analysis is aimed at high level risk estimation,
The authors would like to thank Dr
and we encourage developers when using this
Josimar Mendes from the University of Oxford
analysis to work directly with psychologists to
for his review and feedback during the writing
of this paper.References
Bauer, A., Wollherr, D., & Buss, M. (2008). Human-robot collaboration: A survey. International
Journal of Humanoid Robotics, 5(1), 47–66. https://doi.org/10.1142/S0219843608001303.
Bennett, R., Vijaygopal, R., & Kottasz, R. (2019). Attitudes towards autonomous vehicles
among people with physical disabilities. Transportation research part A: policy and practice,
127, 1-17.
Cugurullo, F., & Acheampong, R. A. (2023). Fear of AI: an inquiry into the adoption of
autonomous cars in spite of fear, and a theoretical framework for the study of artificial
intelligence technology acceptance. AI & SOCIETY, 1-16.
DoD. (2012). MIL-STD-882E. https://assist.dla.mil.
Edmondson, A. (1999). Psychological Safety and Learning Behavior in Work Teams.
Source: Administrative Science Quarterly, 44(2), 350–383.
Edmondson, A. C., Kramer, R. M., & Cook, K. S. (2004). Psychological safety, trust, and
learning in organizations: A group-level lens. Trust and distrust in organizations: Dilemmas
and approaches, 12(2004), 239-272.
Energy Institute. (2020). Guidance on Human Factors Safety Critical Task Analysis. 1–82.
IEC 61508-1:2010 | IEC. (n.d.). Retrieved September 20, 2024, from
https://webstore.iec.ch/en/publication/5515
ISO 21448:2022 - Road vehicles — Safety of the intended functionality. (n.d.). Retrieved
March 5, 2024, from https://www.iso.org/standard/77490.html#lifecycle
Kavcıoğlu, F. C., Bublatzky, F., Pittig, A., & Alpers, G. W. (2021). Instructed threat enhances
threat perception in faces. Emotion, 21(2), 419.
Kenesei, Z., Ásványi, K., Kökény, L., Jászberényi, M., Miskolczi, M., Gyulavári, T., &
Syahrivar, J. (2022). Trust and perceived risk: How different manifestations affect the
adoption of autonomous vehicles. Transportation research part A: policy and practice, 164,
379-393.
Kim, G., Yeo, D., Jo, T., Rus, D., & Kim, S. (2023). What and When to Explain? On-road
Evaluation of Explanations in Highly Automated Vehicles. Proceedings of the ACM on
Interactive, Mobile, Wearable and Ubiquitous Technologies, 7(3), 1-26.Koopman, P., Ferrell, U., Fratrik, F., & Wagner, M. (2019). A Safety Standard Approach for
Fully Autonomous Vehicles.
Lasota, P. A., Fong, T., Shah, J. A., & -Delft, B. (2017). A Survey of Methods for Safe Human-
Robot Interaction. Foundations and Trends® in Robotics, 5(4), 261–349.
https://doi.org/10.1561/2300000052.
Leveson, N. (2004). A new accident model for engineering safer systems. Safety Science, 42(4),
237–270. https://doi.org/10.1016/S0925-7535(03)00047-X.
Leveson, N. G. (2012a). Engineering a Safer World. Engineering a Safer World.
https://doi.org/10.7551/MITPRESS/8179.001.0001.
Leveson, N. G. (2012b). Engineering a Safer World: Systems Thinking Applied to Safety.
Engineering a Safer World. https://doi.org/10.7551/MITPRESS/8179.001.000.
Leveson, N., & Thomas, J. (2018). STPA Handbook.
https://psas.scripts.mit.edu/home/get_file.php?name=STPA_handbook.pdf
Li, M., Holthausen, B. E., Stuck, R. E., & Walker, B. N. (2019, September). No risk no trust:
Investigating perceived risk in highly automated driving. In Proceedings of the 11th
international conference on automotive user interfaces and interactive vehicular
applications (pp. 177-185).
Liu, P., Yang, R., & Xu, Z. (2019). Public acceptance of fully automated driving: Effects of
social trust and risk/benefit perceptions. Risk Analysis, 39(2), 326-341.
Lorenzini, M., Lagomarsino, M., Fortini, L., Gholami, S., & Ajoudani, A. (2023).
Ergonomic human-robot collaboration in industry: A review. Frontiers in Robotics and
AI, 9, 813907. https://doi.org/10.3389/FROBT.2022.813907/BIBTEX
Martin, D. C. (1990). The Mental Status Examination. Clinical Methods: The History, Physical,
and Laboratory Examinations, 924–929. https://www.ncbi.nlm.nih.gov/books/NBK320/
Meyer-Waarden, L., & Cloarec, J. (2022). “Baby, you can drive my car”: Psychological
antecedents that drive consumers’ adoption of AI-powered autonomous vehicles.
Technovation, 109, 102348.
Muris, P., Merckelbach, H., Schepers, S., & Meesters, C. (2003). Anxiety, threat perception
abnormalities, and emotional reasoning in nonclinical Dutch children. Journal of Clinical
Child and Adolescent Psychology, 32(3), 453-459.
NASA Chang, T. (2006). Human Reliability Analysis Methods Selection Guidance for NASA.Omeiza, D., Webb, H., Jirotka, M., & Kunze, L. (2021). Explanations in autonomous driving: A
survey. IEEE Transactions on Intelligent Transportation Systems, 23(8), 10142-10162.
Othman, K. (2023). Exploring the evolution of public acceptance towards autonomous vehicles
with the level of knowledge. Innovative Infrastructure Solutions, 8(8), 208.
Roberts, N. H., & Haasl, D. F. (n.d.). NUREG-0492, “Fault Tree Handbook”.
Rubagotti, M., Tusseyeva, I., Baltabayeva, S., Summers, D., & Sandygulova, A. (n.d.).
Perceived Safety in Physical Human Robot Interaction-A Survey.
Rubagotti, M., Tusseyeva, I., Baltabayeva, S., Summers, D., & Sandygulova, A. (2021).
Perceived Safety in Physical Human Robot Interaction -- A Survey. Robotics and
Autonomous Systems, 151. https://doi.org/10.1016/j.robot.2022.104047.
SAE International. (2021). Potential Failure Mode and Effects Analysis (FMEA) Including
Design FMEA, Supplemental FMEA-MSR, and Process FMEA. SAE International.
https://doi.org/10.4271/J1739_202101.
SAE MOBILUS. (n.d.). Taxonomy and Definitions for Terms Related to Driving Automation
Systems for On-Road Motor Vehicles. Retrieved August 18, 2023, from
https://saemobilus.sae.org/content/j3016_202104.
Shariff, A., Bonnefon, J. F., & Rahwan, I. (2017). Psychological roadblocks to the adoption of
self-driving vehicles. Nature Human Behaviour, 1(10), 694–696.
https://doi.org/10.1038/S41562-017-0202-6
Thomas, E., McCrudden, C., Wharton, Z., & Behera, A. (2020). Perception of autonomous
vehicles by the modern society: A survey. IET Intelligent Transport Systems, 14(10), 1228-
1239.
Udwin, O., Boyle, S., Yule, W., Bolton, D., & O'Ryan, D. (2000). Risk factors for long-term
psychological effects of a disaster experienced in adolescence: Predictors of post traumatic
stress disorder. The Journal of Child Psychology and Psychiatry and Allied Disciplines,
41(8), 969-979.
Xu, Z., Zhang, K., Min, H., Wang, Z., Zhao, X., & Liu, P. (2018). What drives people to accept
automated vehicles? Findings from a field experiment. Transportation Research Part C:
Emerging Technologies, 95, 320–334. https://doi.org/10.1016/j.trc.2018.07.024