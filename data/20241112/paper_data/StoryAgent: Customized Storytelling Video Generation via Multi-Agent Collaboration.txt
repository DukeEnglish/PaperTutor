Print
STORYAGENT: CUSTOMIZED STORYTELLING VIDEO
GENERATION VIA MULTI-AGENT COLLABORATION
PanwenHu1∗JinJiang1∗JianqiChen3MingfeiHan1ShengcaiLiao2XiaojunChang1XiaodanLiang1†
1MohamedbinZayedUniversityofArtificialIntelligence2UnitedArabEmiratesUniversity
3KingAbdullahUniversityofScienceandTechnology
{panwen.hu, jin.jiang, mingfei.han}@mbzuai.ac.ae
{xiaojun.chang, xiaodan.liang}@mbzuai.ac.ae
jianqi.chen@kaust.edu.sa, scliao@uaeu.ac.ae
Shot 1: Miffy wakes up one bright morning, ready to embark on a day filled with adventure.
Shot 2: First stop is the bustling town square, where Miffy greets friends.
Shot 3: Miffy explores the enchanting forest, admiring nature's beauty.
Shot 4: As the sun sets, Miffy relaxes on the beach, watching the golden hues of twilight.
Reference Videos TI-AnimateDiff DreamVideo Magic-Me Ours
Figure1: Comparisonresultsofcustomizedstorytellingvideos. Existingmethodsfailtopreservethe
subjectconsistencyacrossshots,whileourmethodsuccessfullymaintainsinter-shotandintra-shot
consistencyofthecustomizedsubject.
ABSTRACT
TheadventofAI-GeneratedContent(AIGC)hasspurredresearchintoautomated
video generation to streamline conventional processes. However, automating
storytellingvideoproduction,particularlyforcustomizednarratives,remainschal-
lenging due to the complexity of maintaining subject consistency across shots.
WhileexistingapproacheslikeMoraandAesopAgentintegratemultipleagents
for Story-to-Video (S2V) generation, they fall short in preserving protagonist
consistencyandsupportingCustomizedStorytellingVideoGeneration(CSVG).
To address these limitations, we propose StoryAgent, a multi-agent framework
designedforCSVG.StoryAgentdecomposesCSVGintodistinctsubtasksassigned
tospecializedagents,mirroringtheprofessionalproductionprocess. Notably,our
frameworkincludesagentsforstorydesign,storyboardgeneration,videocreation,
agentcoordination, andresultevaluation. Leveragingthestrengthsofdifferent
models, StoryAgentenhancescontroloverthegenerationprocess, significantly
improvingcharacterconsistency. Specifically,weintroduceacustomizedImage-
to-Video (I2V) method, LoRA-BE, to enhance intra-shot temporal consistency,
whileanovelstoryboardgenerationpipelineisproposedtomaintainsubjectcon-
sistencyacrossshots. Extensiveexperimentsdemonstratetheeffectivenessofour
approachinsynthesizinghighlyconsistentstorytellingvideos,outperformingstate-
of-the-artmethods. OurcontributionsincludetheintroductionofStoryAgent,a
versatileframeworkforvideogenerationtasks,andnoveltechniquesforpreserving
protagonistconsistency.
∗Equaltechnicalcontribution,†thecorrespondingauthor
1
4202
voN
11
]VC.sc[
2v52940.1142:viXraPrint
1 INTRODUCTION
Storytellingvideos,typicallymulti-shotsequencesdepictingaconsistentsubjectsuchasahuman,
animal, or cartoon character, are extensively used in advertising, education, and entertainment.
Producingthesevideostraditionallyisbothtime-consumingandexpensive, requiringsignificant
technicalexpertise. However,withadvancementsinAI-GeneratedContent(AIGC),automatedvideo
generation is becoming an increasingly researched area, offering the potential to streamline and
enhancetraditionalvideoproductionprocesses. TechniquessuchasText-to-Video(T2V)generation
models (He et al., 2022; Ho et al., 2022; Singer et al., 2022; Zhou et al., 2022; Blattmann et al.,
2023a;Chenetal.,2023a)andImage-to-Video(I2V)methods(Zhangetal.,2023a;Daietal.,2023;
Wangetal.,2024a;Zhangetal.,2023b)enableuserstogeneratecorrespondingvideooutputssimply
byinputtingtextorimages.
Whilesignificantadvancementshavebeenmadeinvideogenerationresearch,automatingstorytelling
video production remains challenging. Current models struggle to preserve subject consistency
throughoutthecomplexprocessofstorytellingvideogeneration. Recentagent-drivensystems,such
asMora(Yuanetal.,2024)andAesopAgent(Wangetal.,2024b),havebeenproposedtoaddress
Story-to-Video(S2V)generationbyintegratingmultiplespecializedagents,suchasT2IandI2V
generationagents. However,thesemethodsfallshortinallowinguserstogeneratestorytellingvideos
featuringtheirdesignatedsubjects,i.e.,CustomizedStorytellingVideoGeneration(CSVG).The
protagonists generated from story descriptions often exhibit inconsistency across multiple shots.
Anotherlineofresearchfocusingoncustomizedtext-to-videogenerationlikeDreamVideo(Wei
etal.,2023)andMagic-Me(Maetal.,2024)canalsobeemployedtosynthesizestorytellingvideos.
Theyfirstfine-tunethemodelsusingthedataaboutthegivenreferenceprotagonists,thengeneratethe
videosfromthestorydescriptions. Despitetheseefforts,maintainingfidelitytothereferencesubjects
remainsasignificantchallenge. AsshowninFigure1,theresultsofTI-AnimateDiff,DreamVideo,
andMagic-Mefailtopreservetheappearanceofthereferencesubjectinthevideo. Inthesemethods,
thelearnedconceptembeddingscannotfullycaptureandexpressthesubjectindifferentscenes.
Consideringthelimitationsofexistingstorytellingvideogenerationmodels,weexplorethepotential
ofmulti-agentcollaborationtosynthesizecustomizedstorytellingvideos. Inthispaper,weintroduce
amulti-agentframeworkcalledStoryAgent,whichconsistsofmultipleagentswithdistinctrolesthat
worktogethertoperformCSVG.OurframeworkdecomposesCSVGintoseveralsubtasks,witheach
agentresponsibleforaspecificrole: 1)Storydesigner,writingdetailedstorylinesanddescriptions
foreachscene.2)Storyboardgenerator,generatingstoryboardsbasedonthestorydescriptionsand
the reference subject. 3) Video creator, creating videos from the storyboard. 4) Agent manager,
coordinatingtheagentstoensureorderlyworkflow. 5)Observer,reviewingtheresultsandproviding
feedbacktothecorrespondingagenttoimproveoutcomes.Byleveragingthegenerativecapabilitiesof
differentmodels,StoryAgentenhancescontroloverthegenerationprocess,resultinginsignificantly
improvedcharacterconsistency. Thecorefunctionalityoftheagentsinourframeworkcanbeflexibly
replaced,enablingtheframeworktocompleteawiderangeofvideo-generationtasks. Thispaper
primarilyfocusesontheaccomplishmentofCSVG.
However,simplyequippingthestoryboardgeneratorwithexistingT2Imodels,suchasSDXL(Podell
etal.,2023)asusedbyMoraandAesopAgent,oftenfailstopreserveinter-shotconsistency,i.e.,
maintaining the same appearance of customized protagonists across different storyboard images.
Similarly,directlyemployingexistingI2VmethodssuchasSVD(Blattmannetal.,2023b)andGen-
2(Esseretal.,2023)leadstoissueswithintra-shotconsistency,failingtokeepthecharacter’sfidelity
withinasingleshot. InspiredbytheimagecustomizationmethodAnyDoor(Chenetal.,2023b),we
developanewpipelinecomprisingthreemainsteps—generation,removal,andredrawing—asthe
corefunctionalityofthestoryboardgeneratoragenttoproducehighlyconsistentstoryboards. To
furtherimproveintra-shotconsistency,weproposeacustomizedI2Vmethod. Thisinvolvesintegrat-
ingabackground-agnosticdataaugmentationmoduleandaLow-RankAdaptationwithBlock-wise
Embeddings(LoRA-BE)intoanexistingI2Vmodel(Xingetal.,2023)toenhancethepreservation
ofprotagonistconsistency. Extensiveexperimentsonbothcustomizedandpublicdatasetsdemon-
stratethesuperiorityofourmethodingeneratinghighlyconsistentcustomizedstorytellingvideos
comparedtostate-of-the-artcustomizedvideogenerationapproaches. Readerscanviewthedynamic
2Print
demo videos available at this anonymous link: https://github.com/storyagent123/
Comparison-of-storytelling-video-results/blob/main/demo/readme.md1
The main contributions of this work are as follows: 1) We propose StoryAgent, a multi-agent
frameworkforstorytellingvideoproduction. Thisframeworkstandsoutforitsstructuredyetflexible
systemsofagents,allowinguserstoperformawiderangeofvideogenerationtasks. Thesefeatures
alsoenableStoryAgenttobeaprimeinstrumentforpushingforwardtheboundariesofCSVG.2)
WeintroduceacustomizedImage-to-Video(I2V)method,LoRA-BE(Low-RankAdaptationwith
Block-wiseEmbeddings),toenhanceintra-shottemporalconsistency,therebyimprovingtheoverall
visualqualityofstorytellingvideos. 3)Intheexperimentalsection,wepresentanevaluationprotocol
onpublicdatasetsforCSVGandalsocollectnewsubjectsfromtheinternetfortesting. Extensive
experimentshavebeencarriedouttoprovethebenefitoftheproposedmethod.
2 RELATED WORK
StoryVisulization. OurStoryAgentframeworkdecomposesCSVGintothreesubtasks,including
generatingastoryboardfromstorydescriptions,akintostoryvisualization. Recentadvancementsin
DiffusionModels(DMs)haveshiftedfocusfromGAN-based(Lietal.,2019;Maharanaetal.,2021)
andVAE-basedframeworks(Chenetal.,2022;Maharanaetal.,2022)toDM-basedapproaches.
AR-LDM(Panetal.,2024)usesaDMframeworktogeneratethecurrentframeinanautoregressive
manner,conditionedonhistoricalcaptionsandgeneratedimages. However,thesemethodsstruggle
withdiversecharactersandscenesduetostory-specifictrainingondatasetslikePororoSV(Lietal.,
2019)andFlintstonesSV(MaharanaandBansal,2021). Forgeneralstoryvisualization,StoryGen
(Chang Liu, 2024) iteratively synthesizes coherent image sequences using current captions and
previous visual-language contexts. AutoStory (Wang et al., 2023) generates story images based
onlayoutconditionsbycombininglargelanguagemodelsandDMs. StoryDiffusion(Zhouetal.,
2024)introducesatraining-freeConsistentSelf-Attentionmoduletoenhanceconsistencyamong
generatedimagesinazero-shotmanner.Additionally,methodslikeT2I-Adapter(Mouetal.,2024),
IP-Adapter(Yeetal.,2023),andMix-of-Show(Guetal.,2023),designedtoenhancecustomizable
subjectgeneration,canalsobeusedforstoryboards. However,theseoftenfailtomaintaindetail
consistencyacrosssequences. Toaddressthis,ourstoryboardgenerator,inspiredbyAnyDoor(Chen
etal.,2023b),employsapipelineofremovalandredrawingtoensurehighcharacterconsistency.
ImageAnimation. Animatingasingleimage,acrucialaspectofstoryboardanimation,hasgarnered
considerableattention. Previousstudieshaveendeavoredtoanimatevariousscenarios,including
humanfaces(Gengetal.,2018;Wangetal.,2020;2022),bodies(Blattmannetal.,2021;Karrasetal.,
2023;Siarohinetal.,2021;Wengetal.,2019),andnaturaldynamics(Holynskietal.,2021;Lietal.,
2023;MahapatraandKulkarni,2022). Somemethodshaveemployedopticalflowtomodelmotion
and utilized warping techniques to generate future frames. However, this approach often yields
distortedandunnaturalresults. Recentresearchinimageanimationhasshiftedtowardsdiffusion
models(Hoetal.,2020;Songetal.,2020;Rombachetal.,2022;Blattmannetal.,2023b)dueto
theirpotentialtoproducehigh-qualityoutcomes. Severalapproaches(Daietal.,2023;Xingetal.,
2023;Zhangetal.,2023c;Wangetal.,2024a;Zhangetal.,2023a)havebeenproposedtotackle
open-domainimageanimationchallenges,achievingremarkableperformanceforin-domainsubjects.
However,animatingout-domaincustomizedsubjectsremainschallenging,oftenresultingindistorted
video subjects. To address this issue, we propose LoRA-BE, aimed at enhancing customization
generationcapabilities.
AIAgent.NumeroussophisticatedAIagents,rootedinlargelanguagemodels(LLMs),haveemerged,
showcasingremarkableabilitiesintaskplanningandutilityusage. Forinstance,GenerativeAgents
(Parketal.,2023)introducesanarchitecturethatsimulatesbelievablehumanbehavior, enabling
agentstoremember,retrieve,reflect,andinteract. MetaGPT(Hongetal.,2024)modelsasoftware
companywithagroupofagents,incorporatinganexecutivefeedbackmechanismtoenhancecode
generationquality. AutoGPT(Yangetal.,2023)andAutoGen(Wuetal.,2023)focusoninteraction
andcooperationamongmultipleagentsforcomplexdecision-makingtasks. Inspiredbytheseagent
techniques, AesopAgent (Wang et al., 2024b) proposes an agent-driven evolutionary system for
story-to-videoproduction,involvingscriptgeneration,imagegeneration,andvideoassembly. While
1Thecodeswillbereleasedupontheacceptanceofthepaper
3Print
User Prompt & User Prompt Answer & Story Results &
Reference Videos Story Results Reference Videos
User Agent Manager Story Designer Agent Manager
U Ws re ir
te
P r ao m stp ot r:
y about
U Ws re itr
e
P ar o sm top ryt: S St ho or
t
y
1
R
:
e Msu ifl fts y:
wakes up
k ceh
C
wsnA S St ho or
t
y
1
R
:
Mes iu fflt ys :
w akes up one
M Reif ff ey r’ es
n
o cn ee
V
d ia dy e.
os:
a ob no eu dt ayM .iffy’s o
S
bn uhe
so
t
t
lb ir n2i g:g h
tF
ot
i
wrm
s
nto r …sn toin pg i…
s the
rof
ksA
doog“
re b
S
Sr
h
hi
o
og
t
th
2
…t :m Fo ir rn sti n stg
o
…
p is the …
Shot 3: Miffy explores ” Reference Videos:
the enchanting forest…
Shot 4: As the sun sets,
Miffy relaxes on the… Observer
Answer &
Answer & Story Results & Storyboard Results Subject Masks
Video Results & Reference Videos & Subject Masks & Storyboard Results
Agent Manager Video Creator Agent Manager Storyboard Generator
k ceh
C
wsnA Video Results: S Ru eb fej re ec nt M
ce
a Vsk ids
e
o of
s :
k ceh
C
wsnA Storyboard Results:
rof
ksA ”doog“
re
Video 1 Video 2
S RRt eo
e
fr
s
euy rb
l
eto nsa
:
cr ed S
S
S
St
hh
ho
oo
or
tt
t
y
21
3
:
::R
MM
Fe is
r
iiu
fsf ff
tl yt
y
ss
t
e:
w
o
xpa
p
k lie oss
r
e……
s…
rof
ksA ”doog“
re
SS uh bo jt
e
1
ct
MSh ao st
k
2
s
Shot 3 Shot 4
Videos: Shot 4: As the sun … of Reference
Videos:
Observer Video 3 Video 4 Observer
Figure2: Ourmulti-agentframework’svideocreationprocess. Yellowblocksrepresentthenext
agent’sinput,whileblueblocksindicatethecurrentagent’soutput. Forexample,theStoryboardGen-
erator(SG)’sinputincludesstoryresultsandreferencevideos,anditsoutputconsistsofstoryboard
resultsandthesubjectmaskofthereferencevideos. TheAgentManager(AM)automaticallyselects
thenextagenttoexecuteuponreceivingsignalsfromdifferentagentsandmayrequesttheObserver
toevaluatetheresultswhenotheragentscompletetheirtasks.
this method achieves consistent image generation, generating storytelling videos for customized
subjectsremainsachallengeforAesopAgent.
3 STORYAGENT
As depicted in Figure 2, StoryAgent takes as inputs a prompt and a few videos of the reference
subjects, and employs the collaborative efforts of five agents: the agent manager, story designer,
storyboardgenerator,videocreator,andobserver,tocreatehighlyconsistentmulti-shotstorytelling
videos. Theworkflowissegmentedintothreedistinctsteps: storylinegeneration,storyboardcreation,
andvideogeneration.
During storyline generation, the agent manager forwards the user-provided prompt to the story
designer, who crafts a suitable storyline and detailed descriptions p = {p ,··· ,p } (where N
1 N
represents the number of shots in the final storytelling video) outlining background scenes and
protagonistactions. Theseresultsarethenreviewedbytheobserveroruserviatheagentmanager,
andtheprocessadvancestothenextsteponcetheobserversignalsapprovalorthemaximumchat
roundsarereached.
ThesecondstepfocusesongeneratingthestoryboardI={I ,··· ,I }. Here,theagentmanager
1 N
providesthestorydescriptionspandprotagonistvideosV tothestoryboardgenerator,which
ref
producesaseriesofimagesalignedwithpandV . Similartothepreviousstep,thestoryboard
ref
resultsundergouserorobserverevaluationuntiltheymeetthedesiredcriteria. Finally, thestory
descriptionsp,storyboardV ,andprotagonistvideosV arehandedovertothevideocreator
ref ref
forsynthesizingmulti-shotstorytellingvideos.Insteadofdirectlyemployingexistingmodels,asdone
byMora,thestoryboardgeneratorandthevideocreatoragentsutilizeanovelstoryboardgeneration
pipelineandtheproposedLoRA-BEcustomizedgenerationmethodrespectivelytoenhanceboth
inter-shotandintra-shotconsistency. Inthesubsequentsection,wewilldelveintothedefinitionsand
implementationsoftheagentswithinourframework.
4Print
Initial Initial Storyboards & Initial Storyboards &
Story Results & StoryDiffusion Storyboards LangSAM Reference Videos New No Reference Frames StoryAnyDoor
Reference Videos & Reference & Segmentation Subject & Segmentation
Storyboard Generator Videos Masks Masks
Yes
Story Results: Reference InitialStoryboards: Segmentation Masks: Storyboard Results:
Shot 1: Miffy Videos:
wakes up … Train StoryAnyDoor
Shot 2: First … with Reference Videos
Shot … Shot 1Shot 2Shot 3Shot 4 TargetMasks on BG Subject Masks & Segmentation Masks Shot 1 Shot 2 Shot 3 Shot 4
Figure 3: The workflow diagrams of Storyboard Generator, along with the corresponding inputs
(yellowblocks)andtheoutputsoftheirsubmodules(blueblocks).
3.1 LLM-BASEDAGENTS
Agent Manager. Customized Storytelling Video Generation (CSVG) is a multifaceted task that
necessitatestheorchestrationofseveralsubtasks,eachrequiringthecooperationofmultipleagentsto
ensuretheirsuccessfulcompletioninapredefinedsequence. Tofacilitatethiscoordination,weintro-
duceanagentmanagertaskedwithoverseeingtheagents’activitiesandfacilitatingcommunication
betweenthem.LeveragingthecapabilitiesofLargeLanguageModels(LLM)suchasGPT-4(Achiam
etal.,2023)andLlama(Touvronetal.,2023),theagentmanagerselectsthenextagentinline. This
processinvolvespresentingaprompttotheLLM,requestingtheselectionofthesubsequentagent
fromapredeterminedlistofavailableagentswithintheagentmanager. Theprompt,referredtoasthe
rolemessage,isaccompaniedbycontextualinformationdetailingwhichagentshavecompletedtheir
tasks. EmpoweredbytheLLM’sdecision-makingprowess,theagentmanagerensurestheorderly
executionoftasksacrossvariousagents,thusstreamliningtheCSVGprocess.
StoryDesigner. Inordertocraftcaptivatingstoryboardsandstorytellingvideos,craftingdetailed,
immersive,andnarrative-richstorydescriptionsiscrucial. Toaccomplishthis,weintroduceastory
designeragent,whichharnessesthecapabilitiesofLargeLanguageModels(LLM).Thisagentoffers
flexibilityinLLMselection, accommodatingmodelslikeGPT-4, Claude(Anthropic,2024), and
Gemini(Teametal.,2023).BypromptingtheLLMwitharolemessagetailoredtothestorydesigner’s
specifications,includingparameterssuchasthenumberofshots(N),backgrounddescriptions,and
protagonistactions,thestorydesignergeneratesascriptcomprisingnshotswithcorrespondingstory
descriptionsp={p ,··· ,p },ensuringtheinclusionofdesirednarrativeelements.
1 n
Observer. Theobserverisanoptionalagentwithintheframework,anditactsasacriticalevaluator,
taskedwithassessingtheoutputsofotheragents,suchasthestoryboardgenerator,andsignalingthe
agentmanagertoproceedorprovidefeedbackforoptimizingtheresults. Atitscore,thisagentcan
utilizeAestheticQualityAssessment(AQA)methods(Dengetal.,2017)orthegeneralMultimodal
LargeLanguageModels(MLLMs),suchasGPT-4(Achiametal.,2023)orLLaVA(Linetal.,2023),
capableofprocessingvisualelementstoscoreanddeterminetheirquality.However,existingMLLMs
stillhavelimitedcapabilityinevaluatingimagesorvideos. Asdemonstratedinourexperimentsin
AppendixA.5, thesemodelscannotdistinguishbetweenground-truthandgeneratedstoryboards.
Therefore,weimplementedtheLAIONaestheticpredictor(Prabhudesaietal.,2024)asthecoreof
thisagent,whichcaneffectivelyassessthequalityofstoryboardsincertaincasesandfilteroutsome
low-qualityresults. Nevertheless,currentAQAmethodsremainunreliable. Inpracticalapplications,
usershavetheoptiontoreplacethisagent’sfunctionwithhumanevaluationoromititaltogetherto
generatestorytellingvideos. Sincedesigningarobustqualityassessmentmodelisbeyondthescope
ofthispaper,wewillleaveitforfuturework.
3.2 VISUALAGENTS
StoryboardGenerator. Storyboardgenerationrequiresmaintainingthesubject’sconsistencyacross
shots. Itisstillachallengingtaskdespiteadvancementsincoherentimagegenerationforstorytelling
(Wangetal.,2023;Zhouetal.,2024;Wangetal.,2024c)havebeenmade. Toaddressthis,inspired
byAnyDoor(Chenetal.,2023b),weproposeanovelpipelineforstoryboardgenerationthatensures
subject consistency through removal and redrawing, as shown in Fig. 3. Initially, given detailed
descriptions p = {p ,··· ,p }, we employ text-to-image diffusion models like StoryDiffusion
1 N
(Zhouetal.,2024)togenerateaninitialstoryboardsequenceS={s ,··· ,s }. Duringremoval,
1 N
eachstoryboards undergoessubjectsegmentationusingalgorithmslikeLangSAM,resultingin
n
the subject mask M = {m ,··· ,m }. For redrawing, a user-provided subject image with its
1 N
5Print
Block-wise embeddings
a video of V 1 Text Token embedding E D VAE Enc./Dec.
Self-attn./cross-attn./temporal attn. lora
encoder
a video of V n
“A video of V ” ℕ
… D
E
Augment E ℕ Gaussian noise
video Cond. Trainable
encoder Cross-attention maps
Frozen
Localization V 1 …V n Training only
loss
Figure4: TheillustrationofourcustomizedI2Vgenerationmethod. OnlytheLoRAparameters
insideeachattentionblockandtheblock-wisetokenembeddingsaretrainedtorememberthesubject.
Alocalizationlossisappliedtoenforcethetokens’cross-attentionmapstofocusonthesubject.
backgroundremovedisselected,andStoryAnyDoor,fine-tunedbasedonAnyDoorwithV ,fills
ref
themasklocationsMwiththecustomizedsubject. Experimentsinthefollowingsectionprovethat
thisstrategycaneffectivelypreservetheconsistencyofcharacterdetails.
VideoCreator:LoRA-BEforCustomizedImageAnimation. GiventhereferencevideosV ,the
ref
storyboardI,andthestorydescriptionsp,thegoalofthevideocreatoristoanimatethestoryboard
followingthestorydescriptionsptoformthestorytellingvideoswithconsistentsubjectsofinV .
ref
Theoretically,existingI2Vmethods,suchasSVD(Blattmannetal.,2023b),andSparseCtrl(Guo
etal.,2023a),canequiptheagenttoperformthistask. However,thesemethodsstillfacesignificant
challengesinmaintainingprotagonistconsistency,especiallywhenthegivensubjectisacartoon
characterlikeMiffy. Inspiredbythecustomizedgenerationconceptinimagedomain,weproposea
conceptlearningmethod,namedLoRA-BE,toachievecustomizedI2Vgeneration.
OurmethodisbuiltuponaLatentDiffusionModel(LDM)(Hoetal.,2022)-basedI2Vgeneration
model,DynamiCrafter(DC)(Xingetal.,2023). ThemodulesinthismethodincludeaVAEencoder
E anddecoderD ,atextencoderE ,animageconditionencoderE ,anda3DU-Netarchitecture
i i T c
U withself-attention,temporalattention,andcross-attentionblockswithin. Wefirstintroducethe
inference process of the valina DC. As shown in Figure 4, a noisy video z ∈ RF×C×h×w is
T
sampledfromGaussiandistributionN,whereF isthenumberofframes,andC,h,wrepresentthe
channeldimension,height,andwidthoftheframelatentcodes. ThentheconditionimageI ,i.e.,the
n
storyboardinourtask,isencodedbyE andcontactedwithz astheinputofU-NetU. Additionally,
T
theconditionimageisalsoprojectedbytheconditionencoderE toextractimageembedding.Similar
c
tothetextembeddingextractedbythetextencoderfromthetextpromptp ,theimageembeddingis
n
injectedintothevideothroughthecross-attentionblockinsidetheU-Net. Theoutputϵ ofU-Net
T
willbeusedtodenoisethenoisyvideoz followingthebackwardprocessBofLDM.Thedenoising
T
processforthen-thshotatsteptcanbewrittenas:
zn =B(U([zn;E (I )],E (p ),E (I )),zn,t) (1)
t−1 t i n T n c n t
where[·;·]meanstheconcatenationoperationalongthechanneldimension. Wewilldropoffthe
subscriptninthefollowingcontentforsimplicity.
Althoughthereferenceimageisencodedtoprovidethevisualinformationofthereferenceprotagonist,
theexistingpre-trainedDCmodelstillfailstopreservetheconsistencyoftheout-domainsubject.
Hence,weproposetoenhanceitscustomizationabilityofanimatingout-domainsubjectsbyfine-
tuning. InspiredbytheconclusionsofMix-of-Show(Guetal.,2023)thatfine-tuningtheembedding
of the new token, e.g., <Miffy>, helps to capture the in-domain subject, and fine-tuning to shift
thepre-trainedmodel,i.e.,LoRA(Ryu,2023),helpstocaptureout-domainidentity,weenhance
DC’scustomizationabilityfrombothaspects. Specifically,foreachlinearprojectionL(x)=Wx
intheself-attention,cross-attention,andtemporalattentionmodule,weaddafewextratrainable
parametersAandBtoadjusttheoriginalprojectiontoL(x)=Wx+∆Wx=Wx+BAx,thereby
thegenerationdomainofDCisshiftedtothecorrespondingnewsubjectaftertraining. Moreover,we
alsotraintokenembeddingsforthenewsubjecttokens. UnliketheTextInversion(TI)method(Gal
etal.,2022)whichtrainsanembeddingandinjectsthesameembeddinginallthecross-attention
6
… …Print
modules,wetraindifferentblock-wisetokenembeddings. Asthereare16cross-attentionmodulesin
theU-Net,weadd16newtokenembeddingse∈R16×d,wheredrepresentsthedimensionoftoken
embedding,foreachnewsubjecttoken,andeachembeddingisinjectedinonlyonecross-attention
module.Consequently,toanimateanewsubject,onlytheLoRAparametersand16tokenembeddings
aretunedtoenhancethecustomizedanimationability,whereweusethegivenreferencevideoV
ref
tofine-tuningthemodel.
Duringtraining,thetrainingsamplev∈V isfirstprojectedintolatentspacebytheAVEencoder
ref
z =E(v),thenanoisyvideoisobtainedbyapplyingtheforwardprocessF ofLDMonz with
0 0
thesampledtimesteptandGaussiannoisesϵ∼N(0,1),z =F(z ,t,ϵ). TheU-Netistrainedto
t 0
predictthenoiseϵˆappliedonz ,sothatz canberecoveredtoz throughthebackwardprocess.
0 t 0
Toreducetheinterferenceofbackgroundinformationandmakethetrainableparametersfocuson
learningtheidentityofthenewsubject,wefurtherintroducealocalizationlossL appliedonthe
loc
across-attentionmaps. Specifically,thesimilaritymapD ∈RF×h×w betweentheencodedsubject
tokenembeddingandthelatentvideosiscalculatedforeachcross-attentionmodule,andthesubject
maskmisleveragedtomaximizethevaluesofDinsidethesubjectlocations. Hence,theoverall
trainingobjectivefortheI2Vgenerationcanbeformulatedasfollows:
F
1 (cid:88)
L=L +L =∥ϵ−U([z ;z [1]],E (p),E (v[1]))∥− mean(D[f,m[f]=1]) (2)
ldm loc t 0 T c F
f
Asaresult,thetrainablesubjectembeddingsandLoRAparameterscanfocusmoreonthesubject.
4 EXPERIMENTS
ImplementationDetails. Forstoryboardgeneration,weemployedAnyDoorastheredrawerand
fine-tunedittoaccommodatethenewsubjectusingtheAdamoptimizerwithaninitiallearningrate
of 1e-5. We selected 4-5 videos, each lasting 1-2 seconds, for every subject as reference videos,
and conducted 20,000 fine-tuning steps. Regarding the training of the I2V model, we utilized
DynamiCrafter(DC)(Xingetal.,2023)asthefoundationalmodel. Wetrainedonlytheparameters
ofLoRAandblock-wisetokenembeddings(LoRA-BE)usingtheAdamoptimizerwithalearning
rateof1e-4for400epochs. AllexperimentswereexecutedonanNVIDIAV100GPU.
Datasets and Metrics. We employed two publicly available storytelling datasets, PororoSV (Li
et al., 2019) and FlintstonesSV (Maharana and Bansal, 2021), which include both story scripts
and correspondingvideos, for evaluatingour method. From PororoSV, we selected5 characters,
and from FlintstonesSV, we chose 4 characters as the customized subjects. For the training set,
we selected reference videos for each subject from one episode, simulating practical application
scenarios. Forthetestingset,wecurated10samplesforeachsubject,eachconsistingof4shots
highlyrelevanttothesubject. Toevaluateourmethodonthesedatasets,weutilizedreference-based
metricssuchasFVD(Unterthineretal.,2018),PSNR,SSIM(Wangetal.,2004),andLPIPS(Zhang
etal.,2018). Additionally,toassessthegeneralizationability,wecollected8othersubjectsfrom
YouTubeandopen-sourceonlinewebsitestoformanopen-domainset. Storydescriptionsforthis
set were generated using ChatGPT. Since there is no ground truth for this set, we reported the
results on non-reference metrics as outlined in Liu et al. (2023), including Inception Score (IS),
text-videoconsistency(Clip-score),semanticconsistency(Clip-temp),Warpingerror,andAverage
flow(Flow-score). Arrowsnexttothemetricnamesindicatewhetherhigher(↑)orlower(↓)values
arebetterforthatparticularmetric. ForFlow-Score,thearrowisreplacedwitharightwardsarrow
(→)asitisaneutralmetric.
4.1 EVALUATIONONPUBLICDATASETS
QuantitativeResults. ThePororoSV(Lietal.,2019)andFlintstonesSV(MaharanaandBansal,
2021)datasetscomprisestorydescriptionsandcorrespondingvideos,servingasgroundtruthfor
evaluatingstorytellingvideogenerationmethods. Duringtesting,wegenerateastoryboardwitha
consistentbackgroundalignedwiththeground-truthvideo. Toachievethis,weusethefirstframeof
eachvideowiththesubjectremovedastheinitialstoryboard. Subsequently,ourstoryboardgenerator
redraws this initial storyboard to produce the final version. Finally, the generated storyboard is
animatedbythevideocreatoragenttocreateavideoofthesubject.
7Print
TI-SparseCtrl
SVD
StoryAgent
(Ours)
GT
Loopy turns her head to Loopy shows no interest to her Pororo and Crong are apologizing Loopy remains still after her
something that grabbed her friends Pororo and Crong who to Loopy for what they have done friends Pororo and Crong
attention. are standing outside of her house. before. apologized.
Figure5: TheResultvisualizationofthreemethodsandthegroundtruth. Thetextsatthebottomare
thestorydescriptions. Theothertwomethods(thefirst2rows)failtocaptureinter-andintra-shot
consistency,ourresults(the3 row)aremoreapproachingthegroundtruth(the4 row).
rd th
Table1: ComparisonresultsofstorytellingvideogenerationonPororoSVandFlintstonesSVdatasets.
Dataset Method FVD↓ SSIM↑ PSNR↑ LPIPS↓
SVD 2634.01 0.5584 14.2813 0.3737
TI-Sparsectrl 4209.80 0.5042 12.2749 0.5646
PororoSV
StoryAgent(ours) 2070.56 0.6995 17.5104 0.2535
SVD 1864.91 0.4460 14.5968 0.4023
TI-Sparsectrl 3277.96 0.5571 14.7053 0.4958
FlintstonesSV
StoryAgent(ours) 991.37 0.6700 18.1169 0.2490
Inthisevaluationframework,employingone-stagemethodsthatdirectlygeneratestorytellingvideos
fromstorydescriptionsyieldssignificantdiscrepanciesinthebackgroundcomparedtoground-truth
videos. Toensurefaircomparisons,weemploytwoI2Vmethodsinconjunctionwithourstoryboard
generation as benchmarks: 1) SVD (Blattmann et al., 2023b), an open-source tool endorsed by
recent work (Yuan et al., 2024) for image animation; 2) TI-SparseCtrl, wherein we augment the
customizationgenerationabilityofSparseCtrl(Guoetal.,2023a)byintegratingtheTextInversion
(TI)(Galetal.,2022)technique. Table1presentsresultscomputedagainstground-truthvideos. Our
methodconsistentlyoutperformsothersbyanotablemarginacrossbothvideoqualityandhuman
perception metrics, as evidenced by the FVD and LPIPS scores. Moreover, the improvement in
theSSIMmetricindicatescloseralignmentofourresultswithground-truthvideos,affirmingthe
enhancedconsistencyofcharactersinourgeneratedresults.
QualitativeResults. Tofurtherelucidatetheeffectivenessofourapproach,wequalitativelycompare
it with alternative methods in Figure 5. Our model demonstrates superior consistency compared
to TI-SparseCtrl and SVD, closely resembling the ground truth. While TI-SparseCtrl, reliant on
TextInversion,struggleswithmaintainingconsistencyacrossshots,resultinginnoticeablecharacter
variations,SVDmanagestomaintaininter-shotconsistencybutexhibitssignificantchangeswithin
shots,particularlyevidentinthe2 and3 shots. Conversely,ourmethodadeptlypreservesboth
nd rd
inter-shot and intra-shot consistency, thus affirming its effectiveness. Supplementary qualitative
resultsareavailableintheAppendix.
4.2 EVALUATIONONOPEN-DOMAINSUBJECTS
Open-domain Dataset Results. In this experiment, we also qualitatively compare our method
with other CSVG methods, the video generation performance is shown in Figure 1. Due to the
recentwork,StoryDiffusion(Zhouetal.,2024),didnotreleasethecodesforvideogeneration,we
compareitsstoryboardgenerationperformanceinFigure6. ForotherT2Vmethods,TI-AnimateDiff
8Print
Kitty strolls through
the tranquil
countryside.
Kitty reaches rocky
cliffs overlooking
the ocean.
Kitty explores a
charming flower
garden.
Kitty watches the
sunset from a
hilltop.
Reference Videos TI-AnimateDiff DreamVideo Magic-Me StoryDiffusionStoryAgent (Ours)
Figure6:Storyboardgenerationvisualizationonopen-domainsubject(Kitty).Theotherfourmethods
failtopreservetheconsistencyofthereferencesubjectacrossshots,whileourmethodeffectively
improvestheconsistencybetweenthereferencedimageandthegeneratedimage.
Table2: Comparisonresultsofstorytellingvideogenerationontheopen-domaindataset.
Method Ours TI-SparseCtrl SVD TI-AnimateDiff DreamVideo Magic-Me
IS↑ 2.6346 2.4184 2.3831 2.4539 3.4421 2.3989
CLIP-score↑ 0.2053 0.1963 0.2013 0.2023 0.1843 0.2003
CLIP-temp↑ 0.9985 0.9969 0.9959 0.9990 0.9963 0.9992
Warpingerror↓ 0.0184 0.0189 0.0264 0.0043 0.0208 0.0048
Flow-score→ 2.4332 2.6334 5.2117 1.8184 5.1140 1.4092
(Guoetal.,2023b),DreamVideo(Weietal.,2023),andMagic-Me(Maetal.,2024),weusethe
firstframesofthegeneratedvideosasthestoryboardforcomparison. AsshowninFigure 1and
Figure6,allthesemethodsfailtocaptureinter-shotconsistency. FortheresultsofTI-AnimateDiff
inFigure6,thesubjectinthe3 shotisdifferentfromthesubjectinthe4 shot. StoryDiffusion
rd th
alsocannotmaintainthesubjectconsistencyacrossallshots. DreamVideoisunstableandproduces
unnaturalcontent. Magic-Meevenfailstomaintainintra-shotsubjectconsistency,asshowninthe
4 shotofFigure1. Moreimportantly,allthesemethodscannotpreservethereferencesubjectinthe
th
generatedvideos. Incontrast,ourstoryboardgenerator,basedonthestoryboardofStoryDiffusion,
replacedthesubjectswiththereferencesubjectsthroughtheproposedremovalandredrawingstrategy.
Comparedwithothermethods,theproposedstoryboardgenerationpipelineeffectivelypreservesthe
consistencybetweenthereferencedimageandthegeneratedimageindetail,suchastheclothesof
thesubject,therebyenhancingtheinter-shotconsistencyofthestorytellingvideo. Besides,asproved
byFigure1,thevideocreatorstoringthesubjectinformationinafewtrainableparametersfurther
helpstomaintainintra-shotconsistency.
Inadditiontothesubjectconsistency,wealsoreportthequantitativeresultsofallrelevantmethods,
includingTI-SparseCtrlandSVDusingthestoryboardsfromouragent, inTable2. Ourmethod
outperformsothermethodsontext-videoalignmentwhileachievingcomparableperformanceson
otheraspectslikeISandsemanticconsistency(Clip-temp).Theseresultsindicatethatourmethodcan
achievehighconsistencywhileensuringcomparablevideoqualitytootherstate-of-the-artmethods.
Therefore,thecollaborationofmulti-agentsisapromisingdirectionforachievingbetterresults.
4.3 USERSTUDIES
We conducted a user study on the results of different methods on the open-domain dataset and
thePororodataset. Wepresentedtheresultsofdifferentmethodstotheparticipants(Theydonot
knowwhichmethodeachvideocomesfrom)andaskedthemtoratefiveaspectsonascaleof1-5:
InteR-shotsubjectConsistency(IRC),IntrA-shotsubjectConsistency(IAC),Subject-Background
Harmony(SBH),TextAlignment(TA)andOverallQuality(OQ).Moredetailsoftheuserstudescan
beseeninAppendixA.6.
9Print
Table3: Userstudiesofstorytellingvideogenerationontheopen-domaindataset.
Method IRC↑ IAC↑ SBH↑ TA↑ OQ↑
TI-AnimateDiff 2.9 3.8 3.4 2.7 3.0
DreamVideo 1.4 2.6 2.3 2.0 1.7
Magic-me 2.9 3.6 3.7 3.0 3.3
TI-SparseCtrl 2.6 2.4 2.9 2.8 2.5
SVD 3.4 3.0 3.4 2.8 2.8
StoryAgent 4.6 4.8 4.3 3.9 3.8
Table4: UserstudiesofstorytellingvideogenerationonthePororodataset.
Method IRC↑ IAC↑ SBH↑ TA↑ OQ↑
SVD 3.5 2.9 3.4 3.4 3.1
TI-SparseCtrl 1.7 1.7 2.0 1.9 1.5
LoRA-SparseCtrl 2.5 2.1 2.0 2.0 1.9
DC 2.0 1.9 1.7 2.1 1.8
LoRA-DC 3.9 3.8 3.9 3.6 3.4
StoryAgent 4.8 4.8 4.5 4.3 4.4
Fortheopen-domaintest,themethodsevaluatedincludedTI-AnimateDiff,DreamVideo,Magic-Me,
TI-SparseCtrl,SVD,andourmethodStoryAgent. ItisworthnotingthatSVDandTI-SparseCtrlare
onlyvideocreators,sotheyusedthestoryboardsgeneratedbyourStoryboardGeneratorasinput.
ForthePororodataset,weusedtheground-truthstoryboardasinputtoevaluatethedifferentVideo
CreatormethodsincludingSVD,TI-SparseCtrl,LoRA-SparseCtrl,OriginalDynamiCrafter(DC),
LoRA-DC,OurStoryAgent. Wehavereceived14validresponses,andtheaveragescoresforeach
aspectarepresentedinTable3andTable4. Fromtheuserstudiesconductedonthetwodatasets,
itisevidentthatourmethodreceivedthehighestscoresinallfiveevaluatedaspects,especiallythe
inter-shotconsistencyandtheintra-shotconsistency. Thisindicatesthatuserspreferourmethodover
others,demonstratingthesuperiorityofourapproachcomparedtoexistingmethods.
4.4 ABLATIONSTUDIES
Table5: AblationstudiesofvideogenerationonPororoSVandFlintstonesSVdatasets.
Dataset Method FVD↓ SSIM↑ PSNR↑ LPIPS↓
DC-fintuening 2251.47 0.4479 13.5322 0.4878
PororoSV StoryAgent(ours) 2070.56 0.6995 17.5104 0.2535
DC-finetuning 3753.91 0.3357 10.4159 0.6042
FlintstonesSV StoryAgent(ours) 991.37 0.6700 18.1169 0.2490
EffectivenessofRoLA-BE.OnecorecontributionofthispaperisthecustomizedI2Vgeneration.
Inthissection,wewillassesstheresultswithandwithoutthiscomponent. Wefinetunedtheimage
injectionmoduleofDynamiCrafter(DC)(Xingetal.,2023)withthereferencevideostoimprove
thecustomizationabilityasthebaseline. AsshowninTable5,withouttheproposedRoLA-BE,DC
failstopreserveintra-shotconsistency,andthescoreperformancemeasuringthevideoqualityand
humanperceptiondecreases. ThevisualizationresultscanbefoundinAppendix.A.4Incontrast,our
methodachievesbetterinter-shotandintra-shotconsistency,whileobtaininghigh-qualityvideos.
Theseresultssuggestthattheproposedmethodiseffectiveinanimatingcustomizedsubjects.
5 CONCLUSION
WeintroduceStoryAgent,amulti-agentframeworktailoredforcustomizedstorytellingvideogenera-
tion. Recognizingtheintricatenatureofthistask,weemploymultipleagentstoensuretheproduction
ofhighlyconsistentvideooutputs. Unlikeapproachesthatdirectlygeneratestorytellingvideosfrom
storydescriptions,StoryAgentdividesthetaskintothreedistinctsubtasks: storydescriptiongenera-
tion,storyboardcreation,andanimation. Ourstoryboardgenerationmethodfortifiestheinter-shot
consistencyofthereferencesubject,whiletheRoLA-BEstrategyenhancesintra-shotconsistency
10Print
duringanimation. Bothqualitativeandquantitativeassessmentsaffirmthesuperiorconsistencyof
theresultsgeneratedbyourStoryAgentframework.
Limitations. Althoughourmethodexcelsinmaintainingconsistencyacrosscharactersequences,
it faces challenges in generating customized human videos due to constraints in the underlying
videogenerationmodel. Additionally,thedurationofeachshotremainsrelativelyshort. Moreover,
limitationsinherentinthepre-trainedstablediffusionmodelconstrainourabilitytofullycapture
alltext-specifieddetails. Onepotentialavenueforimprovementinvolvestrainingmoregeneralized
basemodelsonlargerdatasets. Furthermore,enhancingourmethodtogeneratecustomizedvideos
featuring multiple coherent subjects across multiple shots will be a primary focus of our future
research. FurtherinsightsintothesocialimpactoftheproposedsystemaredetailedintheAppendix.
REFERENCES
YingqingHe,TianyuYang,YongZhang,YingShan,andQifengChen. Latentvideodiffusionmodels
forhigh-fidelityvideogenerationwitharbitrarylengths. arXivpreprintarXiv:2211.13221,2022.
JonathanHo,WilliamChan,ChitwanSaharia,JayWhang,RuiqiGao,AlexeyGritsenko,DiederikP
Kingma,BenPoole,MohammadNorouzi,DavidJFleet,etal. Imagenvideo: Highdefinition
videogenerationwithdiffusionmodels. arXivpreprintarXiv:2210.02303,2022.
UrielSinger,AdamPolyak,ThomasHayes,XiYin,JieAn,SongyangZhang,QiyuanHu,Harry
Yang,OronAshual,OranGafni,etal. Make-a-video: Text-to-videogenerationwithouttext-video
data. arXivpreprintarXiv:2209.14792,2022.
DaquanZhou,WeiminWang,HanshuYan,WeiweiLv,YizheZhu,andJiashiFeng. Magicvideo:
Efficientvideogenerationwithlatentdiffusionmodels. arXivpreprintarXiv:2211.11018,2022.
AndreasBlattmann,RobinRombach,HuanLing,TimDockhorn,SeungWookKim,SanjaFidler,and
KarstenKreis. Alignyourlatents: High-resolutionvideosynthesiswithlatentdiffusionmodels. In
ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages
22563–22575,2023a.
Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo
Xing,YaofangLiu,QifengChen,XintaoWang,etal. Videocrafter1: Opendiffusionmodelsfor
high-qualityvideogeneration. arXivpreprintarXiv:2310.19512,2023a.
Yiming Zhang, Zhening Xing, Yanhong Zeng, Youqing Fang, and Kai Chen. Pia: Your per-
sonalized image animator via plug-and-play modules in text-to-image models. arXiv preprint
arXiv:2312.13964,2023a.
ZuozhuoDai,ZhenghaoZhang,YaoYao,BingxueQiu,SiyuZhu,LongQin,andWeizhiWang. Fine-
grainedopendomainimageanimationwithmotionguidance. arXivpreprintarXiv:2311.12886,
2023.
XiangWang,HangjieYuan,ShiweiZhang,DayouChen,JiuniuWang,YingyaZhang,YujunShen,
Deli Zhao, and Jingren Zhou. Videocomposer: Compositional video synthesis with motion
controllability. AdvancesinNeuralInformationProcessingSystems,36,2024a.
ShiweiZhang,JiayuWang,YingyaZhang,KangZhao,HangjieYuan,ZhiwuQin,XiangWang,Deli
Zhao,andJingrenZhou. I2vgen-xl: High-qualityimage-to-videosynthesisviacascadeddiffusion
models. arXivpreprintarXiv:2311.04145,2023b.
Zhengqing Yuan, Ruoxi Chen, Zhaoxu Li, Haolong Jia, Lifang He, Chi Wang, and Lichao Sun.
Mora: Enabling generalist video generation via a multi-agent framework. arXiv preprint
arXiv:2403.13248,2024.
JiuniuWang,ZehuaDu,YuyuanZhao,BoYuan,KexiangWang,JianLiang,YaxiZhao,YihenLu,
GengliangLi,JunlongGao,etal. Aesopagent:Agent-drivenevolutionarysystemonstory-to-video
production. arXivpreprintarXiv:2403.07952,2024b.
YujieWei,ShiweiZhang,ZhiwuQing,HangjieYuan,ZhihengLiu,YuLiu,YingyaZhang,Jingren
Zhou,andHongmingShan. Dreamvideo: Composingyourdreamvideoswithcustomizedsubject
andmotion. arXivpreprintarXiv:2312.04433,2023.
11Print
ZeMa,DaquanZhou,Chun-HsiaoYeh,Xue-SheWang,XiuyuLi,HuanruiYang,ZhenDong,Kurt
Keutzer,andJiashiFeng. Magic-me: Identity-specificvideocustomizeddiffusion. arXivpreprint
arXiv:2402.09368,2024.
Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe
Penna,andRobinRombach. Sdxl: Improvinglatentdiffusionmodelsforhigh-resolutionimage
synthesis. arXivpreprintarXiv:2307.01952,2023.
AndreasBlattmann,TimDockhorn,SumithKulal,DanielMendelevitch,MaciejKilian,Dominik
Lorenz,YamLevi,ZionEnglish,VikramVoleti,AdamLetts,etal. Stablevideodiffusion: Scaling
latentvideodiffusionmodelstolargedatasets. arXivpreprintarXiv:2311.15127,2023b.
PatrickEsser,JohnathanChiu,ParmidaAtighehchian,JonathanGranskog,andAnastasisGermanidis.
Structure and content-guided video synthesis with diffusion models. In Proceedings of the
IEEE/CVFInternationalConferenceonComputerVision,pages7346–7356,2023.
Xi Chen, Lianghua Huang, Yu Liu, Yujun Shen, Deli Zhao, and Hengshuang Zhao. Anydoor:
Zero-shotobject-levelimagecustomization. arXivpreprintarXiv:2307.09481,2023b.
JinboXing,MenghanXia,YongZhang,HaoxinChen,XintaoWang,Tien-TsinWong,andYing
Shan. Dynamicrafter: Animatingopen-domainimageswithvideodiffusionpriors. arXivpreprint
arXiv:2310.12190,2023.
Yitong Li, Zhe Gan, Yelong Shen, Jingjing Liu, Yu Cheng, Yuexin Wu, Lawrence Carin, David
Carlson,andJianfengGao. Storygan: Asequentialconditionalganforstoryvisualization. In2019
IEEE/CVFConferenceonComputerVisionandPatternRecognition(CVPR),pages6322–6331,
2019.
AdyashaMaharana,DarrylHannan,andMohitBansal.Improvinggenerationandevaluationofvisual
storiesviasemanticconsistency. InProceedingsofthe2021ConferenceoftheNorthAmerican
ChapteroftheAssociationforComputationalLinguistics: HumanLanguageTechnologies,pages
2427–2442,2021.
HongChen,RujunHan,Te-LinWu,HidekiNakayama,andNanyunPeng. Character-centricstory
visualizationviavisualplanningandtokenalignment. InProceedingsofthe2022Conferenceon
EmpiricalMethodsinNaturalLanguageProcessing,pages8259–8272,2022.
AdyashaMaharana,DarrylHannan,andMohitBansal. Storydall-e: Adaptingpretrainedtext-to-
imagetransformersforstorycontinuation. InEuropeanConferenceonComputerVision,pages
70–87,2022.
XichenPan,PengdaQin,YuhongLi,HuiXue,andWenhuChen. Synthesizingcoherentstorywith
auto-regressivelatentdiffusionmodels. InProceedingsoftheIEEE/CVFWinterConferenceon
ApplicationsofComputerVision(WACV),pages2920–2930,2024.
AdyashaMaharanaandMohitBansal.Integratingvisuospatial,linguistic,andcommonsensestructure
intostoryvisualization. InProceedingsofthe2021ConferenceonEmpiricalMethodsinNatural
LanguageProcessing,pages6772–6786,2021.
YujieZhongXiaoyunZhangYanfengWangWeidiXieChangLiu,HaoningWu. Intelligentgrimm
–open-endedvisualstorytellingvialatentdiffusionmodels. InTheIEEE/CVFConferenceon
ComputerVisionandPatternRecognition(CVPR),2024.
WenWang,CanyuZhao,HaoChen,ZhekaiChen,KechengZheng,andChunhuaShen. Autostory:
Generatingdiversestorytellingimageswithminimalhumaneffort. 2023.
YupengZhou,DaquanZhou,Ming-MingCheng,JiashiFeng,andQibinHou. Storydiffusion: Con-
sistentself-attentionforlong-rangeimageandvideogeneration. arXivpreprintarXiv:2405.01434,
2024.
Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, and Ying Shan.
T2i-adapter: Learningadapterstodigoutmorecontrollableabilityfortext-to-imagediffusion
models. volume38,pages4296–4304,2024.
12Print
HuYe,JunZhang,SiboLiu,XiaoHan,andWeiYang. Ip-adapter: Textcompatibleimageprompt
adapterfortext-to-imagediffusionmodels. arXivpreprintarXiv:2308.06721,2023.
YuchaoGu,XintaoWang,JayZhangjieWu,YujunShi,YunpengChen,ZihanFan,WUYOUXIAO,
RuiZhao,ShuningChang,WeijiaWu,YixiaoGe,YingShan,andMikeZhengShou. Mix-of-
show: Decentralizedlow-rankadaptationformulti-conceptcustomizationofdiffusionmodels. In
AdvancesinNeuralInformationProcessingSystems,volume36,pages15890–15902,2023.
Jiahao Geng, Tianjia Shao, Youyi Zheng, Yanlin Weng, and Kun Zhou. Warp-guided gans for
single-photofacialanimation. ACMTransactionsonGraphics(ToG),37(6):1–12,2018.
YaohuiWang,PiotrBilinski,FrancoisBremond,andAntitzaDantcheva. Imaginator: Conditional
spatio-temporalganforvideogeneration. InProceedingsoftheIEEE/CVFWinterConferenceon
ApplicationsofComputerVision,pages1160–1169,2020.
YaohuiWang,DiYang,FrancoisBremond,andAntitzaDantcheva. Latentimageanimator: Learning
toanimateimagesvialatentspacenavigation. arXivpreprintarXiv:2203.09043,2022.
AndreasBlattmann,TimoMilbich,MichaelDorkenwald,andBjornOmmer. Understandingobject
dynamicsforinteractiveimage-to-videosynthesis. InProceedingsoftheIEEE/CVFConference
onComputerVisionandPatternRecognition,pages5171–5181,2021.
JohannaKarras,AleksanderHolynski,Ting-ChunWang,andIraKemelmacher-Shlizerman. Dream-
pose: Fashionimage-to-videosynthesisviastablediffusion. arXivpreprintarXiv:2304.06025,
2023.
AliaksandrSiarohin,OliverJWoodford,JianRen,MengleiChai,andSergeyTulyakov. Motionrep-
resentationsforarticulatedanimation. InProceedingsoftheIEEE/CVFConferenceonComputer
VisionandPatternRecognition,pages13653–13662,2021.
Chung-YiWeng,BrianCurless,andIraKemelmacher-Shlizerman. Photowake-up: 3dcharacter
animationfromasinglephoto. InProceedingsoftheIEEE/CVFconferenceoncomputervision
andpatternrecognition,pages5908–5917,2019.
AleksanderHolynski,BrianLCurless,StevenMSeitz,andRichardSzeliski. Animatingpictures
witheulerianmotionfields. InProceedingsoftheIEEE/CVFConferenceonComputerVisionand
PatternRecognition,pages5810–5819,2021.
XingyiLi,ZhiguoCao,HuiqiangSun,JianmingZhang,KeXian,andGuoshengLin. 3dcinemagra-
phyfromasingleimage. InProceedingsoftheIEEE/CVFConferenceonComputerVisionand
PatternRecognition,pages4595–4605,2023.
AniruddhaMahapatraandKuldeepKulkarni. Controllableanimationoffluidelementsinstillimages.
InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages
3667–3676,2022.
JonathanHo,AjayJain,andPieterAbbeel. Denoisingdiffusionprobabilisticmodels. Advancesin
neuralinformationprocessingsystems,33:6840–6851,2020.
Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv
preprintarXiv:2010.02502,2020.
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-
resolutionimagesynthesiswithlatentdiffusionmodels. InProceedingsoftheIEEE/CVFconfer-
enceoncomputervisionandpatternrecognition,pages10684–10695,2022.
Yi Zhang, Dasong Li, Xiaoyu Shi, Dailan He, Kangning Song, Xiaogang Wang, Hongwei Qin,
and Hongsheng Li. Kbnet: Kernel basis network for image restoration. arXiv preprint
arXiv:2303.02881,2023c.
JoonSungPark,JosephO’Brien,CarrieJunCai,MeredithRingelMorris,PercyLiang,andMichaelS.
Bernstein. Generativeagents: Interactivesimulacraofhumanbehavior. InProceedingsofthe36th
AnnualACMSymposiumonUserInterfaceSoftwareandTechnology,2023.
13Print
SiruiHong,MingchenZhuge,JonathanChen,XiawuZheng,YuhengCheng,JinlinWang,Ceyao
Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng
Xiao,ChenglinWu,andJürgenSchmidhuber. MetaGPT:Metaprogrammingforamulti-agent
collaborativeframework. InTheTwelfthInternationalConferenceonLearningRepresentations,
2024.
Hui Yang, Sifu Yue, and Yunzhong He. Auto-gpt for online decision making: Benchmarks and
additionalopinions. arXivpreprintarXiv:2306.02224,2023.
Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun
Zhang,ShaokunZhang,JialeLiu,AhmedHassanAwadallah,RyenWWhite,DougBurger,and
ChiWang. Autogen: Enablingnext-genllmapplicationsviamulti-agentconversationframework.
arXivpreprintarXiv:2308.08155,2023.
JoshAchiam,StevenAdler,SandhiniAgarwal,LamaAhmad,IlgeAkkaya,FlorenciaLeoniAleman,
DiogoAlmeida,JankoAltenschmidt,SamAltman,ShyamalAnadkat,etal. Gpt-4technicalreport.
arXivpreprintarXiv:2303.08774,2023.
HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,Timothée
Lacroix, BaptisteRozière, NamanGoyal, EricHambro, FaisalAzhar, etal. Llama: Openand
efficientfoundationlanguagemodels. arXivpreprintarXiv:2302.13971,2023.
AIAnthropic. Theclaude3modelfamily: Opus,sonnet,haiku. Claude-3ModelCard,2024.
GeminiTeam,RohanAnil,SebastianBorgeaud,YonghuiWu,Jean-BaptisteAlayrac,JiahuiYu,Radu
Soricut,JohanSchalkwyk,AndrewMDai,AnjaHauth,etal. Gemini: afamilyofhighlycapable
multimodalmodels. arXivpreprintarXiv:2312.11805,2023.
YubinDeng,ChenChangeLoy,andXiaoouTang. Imageaestheticassessment: Anexperimental
survey. IEEESignalProcessingMagazine,34(4):80–106,2017.
BinLin,BinZhu,YangYe,MunanNing,PengJin,andLiYuan. Video-llava: Learningunitedvisual
representationbyalignmentbeforeprojection. arXivpreprintarXiv:2311.10122,2023.
Mihir Prabhudesai, Russell Mendonca, Zheyang Qin, Katerina Fragkiadaki, and Deepak Pathak.
Videodiffusionalignmentviarewardgradients. arXivpreprintarXiv:2407.08737,2024.
JiuniuWang,ZehuaDu,YuyuanZhao,BoYuan,KexiangWang,JianLiang,YaxiZhao,YihenLu,
GengliangLi,JunlongGao,XinTu,andZhenyuGuo. Aesopagent: Agent-drivenevolutionary
systemonstory-to-videoproduction. arXivpreprintarXiv:2403.07952,2024c.
YuweiGuo,CeyuanYang,AnyiRao,ManeeshAgrawala,DahuaLin,andBoDai.Sparsectrl:Adding
sparsecontrolstotext-to-videodiffusionmodels. arXivpreprintarXiv:2311.16933,2023a.
SimoRyu. Low-rankadaptationforfasttext-to-imagediffusionfine-tuning,2023.
RinonGal,YuvalAlaluf,YuvalAtzmon,OrPatashnik,AmitHBermano,GalChechik,andDaniel
Cohen-Or. Animageisworthoneword: Personalizingtext-to-imagegenerationusingtextual
inversion. arXivpreprintarXiv:2208.01618,2022.
ThomasUnterthiner,SjoerdVanSteenkiste,KarolKurach,RaphaelMarinier,MarcinMichalski,and
SylvainGelly. Towardsaccurategenerativemodelsofvideo: Anewmetric&challenges. arXiv
preprintarXiv:1812.01717,2018.
ZhouWang,AlanCBovik,HamidRSheikh,andEeroPSimoncelli. Imagequalityassessment: from
errorvisibilitytostructuralsimilarity. IEEEtransactionsonimageprocessing,13(4):600–612,
2004.
RichardZhang,PhillipIsola,AlexeiAEfros,EliShechtman,andOliverWang. Theunreasonable
effectivenessofdeepfeaturesasaperceptualmetric. InProceedingsoftheIEEEconferenceon
computervisionandpatternrecognition,pages586–595,2018.
Yaofang Liu, Xiaodong Cun, Xuebo Liu, Xintao Wang, Yong Zhang, Haoxin Chen, Yang Liu,
TieyongZeng,RaymondChan,andYingShan. Evalcrafter: Benchmarkingandevaluatinglarge
videogenerationmodels. arXivpreprintarXiv:2310.11440,2023.
14Print
YuweiGuo,CeyuanYang,AnyiRao,YaohuiWang,YuQiao,DahuaLin,andBoDai. Animatediff:
Animateyourpersonalizedtext-to-imagediffusionmodelswithoutspecifictuning. arXivpreprint
arXiv:2307.04725,2023b.
A APPENDIX
TheoutlineoftheAppendixisasfollows:
• MoredetailsoftheagentschedulingprocessinAagentManager(AM).
• Moreevaluationsonpublicdatasets;
– Morestorytellingvideogenerationresultsonpublicdatasets;
• Moreevaluationsonopen-domainsubjects;
– Morestorytellingvideogenerationresultsonopen-domainsubjects;
• Moreablationstudies;
– Morestorytellingvideogenerationablationonpublicdatasets;
• TheperformanceofObserveragent;
• Thedetailsofuserstudies.
• Socialimpact.
A.1 MOREDETAILSOFTHEAGENTSCHEDULINGPROCESSINAM
Agent Manager Schedulingwith Observer
Input to
Agent Manager a story about … finished bad finished good finished bad finished good finished bad finished good
Selection from Story Designer Observer Story Designer Observer Storyboard Generator Observer Storyboard Generator Observer Video Creator Observer Video Creator Observer
Agent Manager
Agent Manager Scheduling without Observer
Input to
Agent Manager a story about … finished finished finished
Selection from Story Designer Storyboard Generator Video Creator
Agent Manager
Story Designer Prompt
You play the role of a storytelling video director and receive the user’s story requirement. You will first write a complete story based on the given
story hints. Then you decompose the completed story into 4 shots or storyboards and give the narrative storyline and detailed descriptions of each
shot. The descriptions should describe the content to be shown in the shot as detailed as possible, containing: 1. characters are shown, and action
descriptions; 2. character regions in the shot;3. background descriptions; 4. shot type; 5. shot motion.
Agent Manager Prompt
You are a video production manager, selecting one speaker name each round from multiple agents {“Story Designer”, “Storyboard Generator”,
“Video Creator”, Observer} based on the chat context to jointly complete the video production task. The response from functional agents, “Story
Designer”, “Storyboard Generator”, “Video Creator” needs to be passed to the Observer agent for evaluation. If the response from the Observer
agent is good, then select the next functional agent, otherwise select the last agent to re-generate the results. Only the selected agent name is needed.
Figure7: TheagentschedulingprocessinAM.ThesolidarrowsindicateAM’sselectionofanagent
uponreceivingasignal,whilethedashedarrowsrepresentthesignalsproducedbytheselectedagent.
Additionally,thisfigureshowsthepromptsusedbytheStoryDesignerandAM.
A.2 MOREEVALUATIONSONPUBLICDATASETS
MoreStorytellingVideoGenerationResultsonPublicDatasets.
Asmentionedbefore,existingI2Vmethods,suchasSVD(Blattmannetal.,2023b),andSparseC-
trl(Guoetal.,2023a),alsocanbeusedbyourvideocreatortoanimatethestoryboardIfollowingthe
storydescriptionsptoformthestorytellingvideos. Tofurtherindicatethebenefitsoftheproposed
StoryAgent,wealsovisualizethestorytellingvideosgenerationresultsonFlintstonesSVdataset. As
showninFigure8,ourStoryAgentwiththeproposedLoRA-BEcannotonlygenerateresultscloser
tothegroundtruthbutalsomaintainthetemporalconsistencyofsubjectsbetter,comparedwiththe
resultsgeneratedbyothermethods.
15Print
TI-SparseCtrl
SVD
StoryAgent
(Ours)
GT
Wilma and Betty are in the living Wilma is sitting in a room Wilma is standing in a room. She is Wilma is standing in a room. She is
room. Wilma turns her head to conversing with someone with a looking up and down while talking. talking to someone and then looks
talk to Betty. look of shock on her face as she like she is sad.
holds it with her hand.
Figure8: StorytellingvideosgenerationvisualizationonFlintstonesSVdataset.
TI-SparseCtrl
(realistic)
TI-SparseCtrl
(cartoon)
SVD
StoryAgent (Ours)
The adventure begins, Kitty Kitty reaches the rocky cliffs Kitty explores a charming Kitty watches the sunset from
strolls through the tranquil overlooking the ocean, flower garden, where vibrant a hilltop, its heart filled with
countryside, enjoying the marveling at the beauty of blooms sway in the breeze. gratitude for the adventures.
Reference Videos peaceful scenery. the waves.
Figure9: Storytellingvideogenerationvisualizationonopen-domainsubject(Kitty).
A.3 MOREEVALUATIONSONOPEN-DOMAINSUBJECTS
MoreStorytellingVideoGenerationResultsonOpen-domainSubjects.
ComparingourmethodwithSVD (Blattmannetal.,2023b)andTI-SparseCtrl (Guoetal.,2023a),
wealsovisualizemoregeneratedstorytellingvideosfromstoryscriptsonopen-domainsubjects,
wherethestorydescriptionsaregeneratedbyourstorydesigneragent. AsshowninFigure9and
Figure10,TI-SparseCtrlfailstomaintainconsistencythroughoutalltheshotswherethesubjects
change significantly in subsequent shots, such as the last shots on both of the two subjects. The
proposedStoryAgenteffectivelymaintainthetemporalconsistencybetweenthereferencedsubjects
throughoutthestorysequencesindetails,suchastheclothesofcartoonsubjectslikeKittyandthe
appearanceofreal-worldsubjectslikethebird. AlthoughSVDalsoperformswellinmaintaining
temporalconsistencyofthereal-worldbirdinFigure10,themovementsofthebirdarelessableto
followthetext,whileourmethodcanproducemorevividvideosofthesubject.
16Print
TI-SparseCtrl
(realistic)
TI-SparseCtrl
(cartoon)
SVD
StoryAgent (Ours)
The bird perches on a moss- The bird hops from branch The bird settles into its cozy The bird goes out from its
covered branch in the heart to branch, observing the nest as the sun sets. nest, the entire forest bathed
Reference Videos of a dense forest. forest. in moonlight.
Figure10: Storytellingvideogenerationvisualizationonopen-domainsubject(Thebird).
TI-AnimateDiff
DreamVideo
Magic-Me
StoryAgent (Ours)
The elephant discovers his The elephant stumbles upon The elephant joins in, The elephant realizes that
love for music in the heart a group of playful monkeys creating beautiful melodies. music has the power to unite
of the savannah. drumming on hollow logs. even the most unlikely of
Reference Videos friends.
Figure11: Storytellingvideogenerationvisualizationonopen-domainsubject(Theelephant). The
otherthreemethods(the1-3rows)failtogenerateaconsistentsubjectwiththereferencevideos,
whileourmethod(the4 row)achieveshighconsistency.
th
Furthermore, a comparison of an open-domain subject, a cartoon elephant, with state-of-the-art
customizationT2VmethodsisshowninFigure11. ItcanbeobservedthatTI-AnimatedDifffails
tocaptureinter-shotconsistency,thesubjectinthe4 shotisdifferentfromthesubjectinthe2
th nd
shot. DreamVideooccasionallyfallsshortofgeneratingthesubjectinthevideo. Magic-Mealsofails
tomaintaininter-shotsubjectconsistency. Incontrast,ourmethodcanpreservetheidentityofthe
referencesubjectinallshots. Theseresultsfurtherindicatethatthestoryboardgeneratoragentin
ourframeworkhelpstoimprovetheinter-shotconsistency,andthevideocreatorstoringthesubject
informationhelpstomaintainintra-shotconsistency.
A.4 MOREABLATIONSTUDIES
MoreStorytellingVideoGenerationAblationonPublicDatasets.
17Print
DC
StoryAgent
(Ours)
GT
Eddy and Loopy are scared and At Poby's house Eddy and Loopy Poby wonders if the monster is real. Poby recalls Pororo and Crong
they explain to Poby that there is explains about the monsters. and Poby decides to go.
a monster. Poby is thinking.
Figure12: StorytellingvideosgenerationablationonPororoSVdataset.
DC
StoryAgent
(Ours)
GT
Betty is standing in a room Betty is talking to someone who Betty and Wilma talk to each other Wilma and Betty are standing in
talking to someone. As she talks, is not shown inside of the living in a room. the room. they both look sad.
she moves her head and blinks. room.
Figure13: StorytellingvideosgenerationablationonFlintstonesSVdataset. Simplyfine-tuningstill
resultsininconsistency(the1 row),whileourmethod(the2 row)usingtheRoLA-BEstrategy
st nd
achievesmoreconsistentresultswiththegroundtruth(the3 row).
rd
The storytelling videos generation visualization on PororoSV dataset is also presented to further
indicatetheeffectivenessoftheproposedRoLA-BE.SameastheexperimentalsettingsinSection
4.4,wechoosethefinetunedDynamiCrafter(DC)(Xingetal.,2023)onthereferencevideosasthe
baseline,whileourmethodconsistsofDCandtheproposedRoLA-BE.AsshowninFigure12,DC
stillfailstogeneratecustomlizedsubjectsevenwiththefine-tuningonthereferencedata,whileour
methodgeneratesresultsclosertothegroundtruthandfitsthescriptwell. Similarly,inFigure13,
withouttheproposedRoLA-BE,DCfailstopreserveintra-shotconsistency(the1 row). Incontrast,
st
ourmethodachievesbetterinter-shotandintra-shotconsistency,whileobtaininghigh-qualityvideos.
Table6: ThescorecomparisonofdifferentobserverfunctionsonthePororodataset.
Scoremodel Method Case1 Case2 Case3 Case4 Case5
AnyDoor 8.0 8.0 8.0 7.0 5.0
Gemini OurStoryAgent 7.0 8.0 8.0 7.0 5.0
GT 5.0 4.0 8.0 9.0 9.0
AnyDoor 6.0 4.0 4.0 4.5 3.5
GPT-4o OurStoryAgent 6.0 4.5 3.5 3.5 3.5
GT 6.0 4.0 3.5 3.5 3.5
AnyDoor 3.78 4.03 3.28 4.03 3.58
Aestheticpredictor OurStoryAgent 3.88 4.17 3.59 3.47 3.90
GT 3.95 4.10 3.94 3.73 4.02
18Print
AnyDoor
Our
StoryAgent
GT
Case 1 Case 2 Case 3 Case 4 Case 5
Figure14: ThecasecomparisonoftheObserveronthePororodataset.
A.5 THEPERFORMANCEOFOBSERVER
Inthisexperiment,weusedifferentaestheticqualityassessmentmethods,includingtwoMultiModal
LargeLanguageModels,GeminiandGPT-4o,andtheLAIONaestheticpredictorV2(Prabhudesai
etal.,2024),toscorethegeneratedstoryboardsbythebaselinemethodsAnydoorandourStory-
boardGenerator,andtheground-truthstoryboard. ThestoryboardisshowninFigure14,andthe
correspondingscoresintherangeof1-10arelistedinTable6.
WeobservedthatMLLMsarenoteffectiveatdistinguishingbetweenstoryboardsofvaryingquality.
Forexample,incase4,GPT-4oassignsahighscoretoalow-qualityresultgeneratedbyAnyDoor,
whilegivingtheground-truthimagealowerscore. Similarly,incase2,Geminiexhibitsthesame
behavior. Instead,theaestheticpredictorisrelativelybetteratdistinguishinglower-qualityimages,
althoughitisstillfarfromperfect. Therefore,inourexperiments,wedecidedtobypasstheobserver
agenttoavoidwastingtimeonrepeatedgeneration. Furtherresearchonimprovingaestheticquality
assessmentmethodswillbeleftforfuturework.
A.6 THEDETAILSOFUSERSTUDIES
We conduct user evaluations by designing a comprehensive questionnaire to gather qualitative
feedback. Thisquestionnaireassessesfivekeyindicatorsdesignedforpersonalizedstorytellingimage
andvideogeneration:
(1)InteR-shotsubjectConsistency(IRC):Measureswhetherthefeaturesofthesamesubjectare
consistentamongdifferentshots(Thisindicatorrequirestoconsidertheconsistencyofthesubject
amongshotsbasedontheprovidedsubjectreferenceimages).
(2)IntrA-shotsubjectConsistency(IAC):Measureswhetherthefeaturesofthesamesubjectare
consistentinthesameshot(Thisindicatoronlyrequirestoconsidertheconsistencyofthesubjectin
thesameshot,withoutconsideringthesubjectreferenceimages).
(3)Subject-BackgroundHarmony(SBH):Measureswhethertheinteractionbetweenthesubjectand
thebackgroundisnaturalandharmonious.
(4)TextAlignment(TA):Measurewhetherthevideoresultsmatchthetextualdescriptionofthe
story.
(5)OverallQuality(OQ):Measurestheoverallqualityofthegeneratedstorytellingvideos.
Thefeedbackcollectedwillprovidevaluableinsightstofurtherrefineourmethodsandensurethey
meettheexpectationsofdiverseaudiences.
19Print
A.7 SOCIALIMPACT
Althoughstorytellingvideosynthesiscanbeusefulinapplicationssuchaseducation, andadver-
tisement. Similar to general video synthesis techniques, these models are susceptible to misuse,
exemplifiedbytheirpotentialforcreatingdeepfakes. Besides,questionsaboutownershipandcopy-
rightinfringementmayalsoarise. Nevertheless,employingforensicanalysisandothermanipulation
detectionmethodscouldeffectivelyalleviatesuchnegativeeffects.
20