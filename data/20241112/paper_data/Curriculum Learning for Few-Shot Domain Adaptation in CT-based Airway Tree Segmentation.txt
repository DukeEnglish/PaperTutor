CURRICULUMLEARNINGFORFEW-SHOTDOMAINADAPTATIONINCT-BASED
AIRWAYTREESEGMENTATION
MaximeJacovella†,∗,AliKeshavarzi‡,∗,ElsaAngelini‡,⊛
† DepartmentofComputing,FacultyofEngineering,ImperialCollegeLondon,UK
‡ LTCI,TelecomParis,InstitutPolytechniquedeParis,France
⊛ DivisionofSystemsMedicine,DepartmentofMetabolism,Digestion&Reproduction,
FacultyofMedicine,ImperialCollegeLondon,UK
ABSTRACT In recent years, deep learning–particularly convolutional
neuralnetworks(CNNs)–hasconsiderablyadvancedthefield
Despiteadvanceswithdeeplearning(DL),automatedairway
ofautomatedairwaysegmentation. Adaptationsofthe3DU-
segmentation from chest CT scans continues to face chal-
Net architecture like AttentionUNet [7] and AirwayNet [8],
lenges in segmentation quality and generalization across co-
in particular, have set new standards in airway segmentation
horts. To address these, we propose integrating Curriculum
performance. However, trained DL networks often struggle
Learning(CL)intoairwaysegmentationnetworks, distribut-
to generalize to new cohorts, acquired on different scanners
ingthetrainingsetintobatchesaccordingtoad-hoccomplex-
or from patients with airway changes related to pulmonary
ityscoresderivedfromCTscansandcorrespondingground-
diseases[9,10,11].
truth tree features. We specifically investigate few-shot do-
Toaddressthisissue,weproposeincorporatingprinciples
mainadaptation,targetingscenarioswheremanualannotation
from Curriculum Learning (CL) into a supervised DL seg-
ofafullfine-tuningdatasetisprohibitivelyexpensive.Results
mentationtrainingprocess. OriginallyformalizedbyBengio
arereportedontwolargeopen-cohorts(ATM22andAIIB23)
etal. in2009[12],CLisabout“startingsmall”,byforcinga
withhighperformanceusingCLforfulltraining(Sourcedo-
modeltofirstfocusonsimplerpartsofthelearningtask,ora
main)andfew-shotfine-tuning(Targetdomain),butwithalso
simplifiedsub-task,beforeprogressivelyincreasingdifficulty.
someinsightsonpotentialdetrimentaleffectsifusingaclas-
This allows the trained model to first leverage information
sicBootstrappingscoringfunctionorifnotusingproperscan
from easier examples and then improve capacities on more
sequencing.
complexones. NumerousvariantsofCLhavebeenexplored
IndexTerms— LungCT,Airwaysegmentation,Curricu- in the literature, generally split into data- [13, 14], model-
lumlearning,Few-shotdomainadaptation [15, 16, 17] and task-level [18], depending on the focus of
the curriculum. Applications of CL to medical imaging, in
1. INTRODUCTION particularmedicalimagesegmentation,remainscarce.
Inthisstudy,weexploredata-levelCLasin[14],where
Airway tree segmentation from chest CT scans is crucial thetrainingdataissplitintobatchesaccordingtoacomplex-
for diagnosing and monitoring pulmonary diseases such as ityscoringfunction. WetestsuchCLfortwotasks: (1)full
bronchiectasis, airway wall thickening and chronic obstruc- segmentation network training on Cohort 1 where we com-
tivepulmonarydisease(COPD)[1]. Yet,theintricatebranch- pare different CL batch compositions, (2) few-shot domain
ingstructureandhighvariabilityofairwaytreesmakemanual adaptationtoaCohort2ofscanswithalargepopulationshift
segmentationbothlabor-intensiveandpronetoinconsistency from Cohort 1. We also introduce two complexity scoring
[2],spurringinterestinautomatedsegmentationtools. functionsspecifictotheimagingdataandsegmentationtask
Early attempts, in the 1990s, primarily relied on region- ofinterest.
growing techniques [3, 4] which, while effective for detect- Thisstudymakesthreekeycontributions:
ing large and contrasted airway structures like the trachea
1. Introduceanovelknowledge-transfercomplexityscoring
andmainbronchi,oftenfailedtocapturethenumerous,thin-
metricforassessinglungCTscancomplexity,incorporat-
ner peripheral bronchi. Classical machine learning classi-
ing ground-truth segmentation and visual image proper-
fierssuchask-NearestNeighbors[5]andRandomforest[6]
ties,andcompareittoaself-taughtbootstrappingscoring
subsequentlyimprovedperformance,butreliedonpredefined
function.
features.
2. Propose a CL supervised training framework to enhance
∗Theseauthorscontributedequally. the performance of a DL baseline segmentation network
4202
voN
8
]VC.sc[
1v97750.1142:viXraonasourceCohort1.
3. Propose a CL-based few-shot incremental domain adap-
tationapproachtofine-tuneonatargetCohort2thebest
networktrainedonCohort1.Domainadaptationiscrucial
inthiscontextasCohort2consistsoffibroticlungdisease
cases, which causes airway changes that models trained
onhealthycohortsstruggletogeneralizeto[9,10,11].
2. METHODOLOGY
Data-levelcurriculumlearning(CL)organizesthesupervised
training process by arranging training samples in a strategic
orderbasedonachosencomplexityscoringfunction,instead
of random sampling. In our case, this motivated us to first
investigate two strategies to define an ad-hoc scoring func-
tion, specific to lung CT scans and the airway segmentation
Fig.1. ProposedCTscoringfunctions.
task. WethenexploitedthesescoringfunctionsinseveralCL
scenarioswithdifferentbatchcompositionsorlearningtasks Random-ForestMLclassifieronlabeleddatasamples(x,y),
(fulltrainingorfew-shotdomainadaptation). where:
• xconsistsoffeaturesderivedonlyfromtheCTscansand
their ground-truth segmentations. These features include
2.1. Ad-hocComplexityScoringFunctions
topological properties such as branch count and average
2.1.1. Self-taughtBootstrappingscoringfunction branchlengthanddiameter(seebelow).
GTfeatures: Treelength, Voxelcount, Volume(mm3), Volume
InaclassicCLapproach, thecomplexityscoringfunctionis
ratio=GTvolume/(Lungvolume)2/3,Branchcount,Avgbranch
lengthanddiameter;CTfeatures:Voxelsize,Lungvolume(mm3),
defined via a self-taught bootstrapping approach [14] where
Intensityhistograms(lungwindow,100bins).
thecomplexityofeachtrainingsampleisbasedontheperfor- • y istheperformanceofapre-trainedSEG2segmentation
manceofabaselinenetwork,trainedeitheronthefullavail- thattheMLmodelistrainedtopredict.
abletrainingcohortvisitedinarandomorderoronadifferent
Inthisstudy, wedefinedthetestcomplexityscorey asa
cohort(transferlearning[13]). Bootstrappingisthenrealized
weightedaverageofseveralsegmentationqualitymetrics(see
byre-trainingthenetwork,usingthecomplexityscorestoor-
below) from a naive SEG2 segmentation tool, trained with-
dersamplesinbatchesaccordingtotheircomplexity.
outCL.Theweightsweredeterminedbyprincipalcomponent
Inourcaseweuseabaselinesegmentationnetworkarchi-
analysis (PCA) to capture the contribution of each metric to
tecture (SEG1) trained on the whole Cohort 1 and the com-
theoveralldatasetvariance.
plexity score of each sample is then defined as CS = 1−
B
Segmentation quality metrics: Mean squared error in air-
IoUofSEG1forthisexample.
waysize,Dicecoefficient,TrueandFalsepositive/negative
Thisapproachislimitedinessencebythecapacityofthe (TP/TN/FP/FN) rates, Airway completeness, Airway vol-
chosenSEG1architecturetohandleinafairmannerdifferent umeleakage,Airwaycenterlineleakage,Airwaytreelength,
Airwaycenterlinedistance.
cohorts with large domain shifts, without requiring a com-
pletenetworkretrainingonanynewdataset. 2.2. CLSetup
Given the proposed complexity scores, we proceeded to CL
2.1.2. Knowledge-transferML-basedscoringfunction trainingusingthefollowingsettings:
• Full training on a source dataset, exploring different CL
To overcome the limitations of bootstrapping and transfer batchcompositions.
learning, we propose using a classic ML approach to in- • Domainadaptationtoatargetfibroticlungdiseasedataset,
fer segmentation quality directly from observational data, usingCLtoprioritizescansandpotentiallylimitthenum-
including CT scan visual properties and ground truth seg- berofmanualannotationsneeded.
mentations. This approach learns to infer the segmentation
quality achieved by a reference SEG2 network architecture.
2.2.1. FullTrainingonSourceDataset
The ML inference tool then serves as a scoring function for
evaluating new samples in unseen cohorts. This method is Inordertoinvestigatehowtheorderingoftrainingdataaffects
particularly valuable when a new cohort is too small for full performance,weexploremultiplecurriculumlearningstrate-
baseline training with SEG1, or when SEG1’s suitability for gies for initial network training. In each case, the CL con-
reliabletransferlearningisuncertain. Specifically,wetraina figuration is kept similar, but training examples are ordereddifferently. The common configuration is organized around stepsize Secondwindow
3CLBatchesofincreasingsize(expressedbelowasaper-
...
centageofthefulltrainingset): S S S T T T T T
• CLBatchn°1ofsize15%andtrainedonfor20epochs.
Initialwindow windowsize
• CLBatchn°2ofsize40%andtrainedonfor70epochs.
• CLBatchn°3ofsize45%andtrainedonfor110epochs.
Fig.2. Source2TargetstrategytoformCLBatches.
Batchesarenotnestedbuteachincludesasmallportionofex-
amplesfrompreviousones(15%ofinitialsize). Thisoverlap
e thn esu nr ee ts wa ors km fo roo mthe fr ortr ga en tts ii nti gon prb evet iw oue se ln yb sea etc nh ee xs, amdi psc leo su .raging EXPERIMENT I (%OU
)
D (%LR
)
D (%BR
)
P (R %EC ). 1-L (%EA )K.
WeexplorethreeCLorderings: ATM22
• Vanilla CL:Trainingdataisintroducedinorderofin- SAMMED3D 43.86 9.88 3.93 78.66 87.88
creasing complexity, from an Easy to a Hard batch, fol- NOCL 80.43 71.38 63.27 93.81 94.01
lowingatraditionalcurriculum-basedapproach.
BSVANILLACL 55.85 24.04 16.62 86.26 98.1
• Mixed CL: Data is still ordered from easy to hard, but
REVERSECL 56.74 24.1 16.57 92.71 98.01
eachbatchincludesanadditional15%ofharderexamples.
• Reverse CL:Theorderisreversedcomparedtovanilla. VANILLACL 82.18 72.99 63.16 93.74 94.06
WecompareCLagainstacontrolNo CLapproachinwhich MIXEDCL 82.93 75.15 65.72 94.02 94.16
trainingsamplesareusedinarandomorder. AIIB23
SAMMED3D 41.58 8.14 3.40 78.05 91.10
2.2.2. Incrementaldomainadaptation NOCL 80.82 57.23 47.65 93.98 97.67
We fine-tune the best-performing model from above on the
TARGET 82.17 59.1 49.5 96.74 97.14
targetCohort2,usingonly20labeledscansasatrainingset SOURCE2TARGET 83.22 63.08 53.69 96.39 96.69
(comparedto254before).Thus,wenotonlyexploredifferent
Table 1. Segmentation performance of the proposed CL-
orderingsoftrainingdata,butalsodifferentchoicesofthe20
basedapproachescomparedtobaselinemodels.
trainingscansamongthe90available1.
Specifically, we investigate incremental domain adapta-
intoN=300scansfortraining, 50forvalidation, and150for
tionviaCL,wherewegraduallyintroducetargetdomainex-
testing. Notably, ground-truth segmentations are available
amples. Thisprogressiveapproachaimstohelpthenetwork
only for the training set. For our study, we extracted from
retain knowledge from the source domain while adapting to
theN=300trainingscansaN=45scansforourtestset(15%
the target domain, reducing the risk of catastrophic forget-
of the total size), leaving N=254 for training2. For consis-
ting. Weevaluatetwoincrementalschemes,eachbasedona
tency and comparability, all experiments on ATM22 employ
slidingwindowthatmovesthroughtheindicesoftheselected
thesametestset.
scans. Parameters include the window size (i.e. CLBatch
AIIB23 dataset [9] (Target): consists of N=120 scans for
size), thestepsizeandthestepduration(inepochs), aswell
training, 45 for validation and 140 for testing, with ground
asthesubsetsofsamplestoselectfromthesourceandtarget
truthsonlypublishedforthetrainingset. Inourexperiments,
domains.
we focus on a subset (from the training set) of N=20 scans
• Target: Usingonlytargetscans.
fortraining,whichvariesbetweenapproaches,andN=30test
• Source2Target: Startingwithsourcescansbeforein-
scans,keptconstantforallexperiments.
crementallyintroducingtargetscans(seeFig. 2),toallow
forasmoothertransition,asexplainedinSection3.3. Pre-processing. Ourpre-processingconsistsofclippingCT
intensityvaluesto[-1000,+600]HUbeforescalingthembe-
We also run a No CL fine-tuning, trained on randomly se- tween 0 and 1. This clipping corresponds to a typical ”lung
lectedscanswithoutCL. window”ofHUvalues.CTscansarecroppedaroundthelung
usingmasksextractedwiththelungmasktool[20].Weadded
extraaxialslicesattheapextoencompasstheuppertrachea
3. EXPERIMENTSANDRESULTS
thatmightextendbeyondthe3Dlungboundingbox.Random
croppingisappliedtoextractpatchesof256×256×256vox-
3.1. DatasetsandPre-processing
els,accommodatinglungsizevariabilitywhileworkingwith
ATM22 dataset [19] (Source) : comprises a total of 500 afixedinputsize.
chest CT scans collected from various sources, partitioned
1Cohort 2 comprises 120 publicly annotated scans, among which 30 2TheATM1640000scanwasremovedbyorganizersduetoashiftin
wereusedathevalidationset,commontoallexperiments. itsgroundtruth,andwethusexcludeitfromouranalysis.3.2. TrainingandImplementationDetails
AIIB23_143 AIIB23_116 AIIB23_036
For consistency across experiments, we keep the same net-
work architecture and configuration for SEG1 and SEG2: a
3DAttentionUNetwith5layersfeaturingchannelsizes16,
32,64,128and256respectively,stride2and3×3×3kernels.
Forthetraininglossfunction,weutilizetheDiceloss,op-
timizedusingAdamwithaninitiallearningrateof1×10−3
and an initial weight decay of 1 × 10−5. A ReduceLROn-
Plateau scheduler, featuring a reduction factor of 0.1 and a
patience of 20, is implemented to dynamically modify the
learningrate. OurexperimentswereconductedonaNVIDIA
A100with40GBofmemory,usingabatchsizeof1.
3.3. Results
Fig.4. Qualitativeresultsusing2CLstrategiesontheTarget
ComplexityScoresperCohort. Acleardistributionshiftis domain.
observedinthepredictedcomplexityscoresbetweenATM22
Mixed CLoutperformthecontrolNo CLinallmetrics,val-
andAIIB23(Figure3).Forthetwoscoringfunctions,AIIB23
idating that CL enhances the performance of full network
exhibitsahighercomplexitymeanvaluesandstandarddevi-
training. Conversely, Reverse CL performs significantly
ations, suggesting some challenges in source to target fine-
worse,consistentwithliteraturefindingsinvariouscontexts.
tuning. Theorderingofthescansdiffersgreatlybetweenthe
Bs Vanilla CL also performs poorly, indicating the criti-
2scoringfunctions,motivatingseparateCLexperiments.
calimportanceofthescoringfunction.
Few-Shot Domain Adaptation Results. In the lower sec-
11 0246802
0.1 0.2 0.3 0.4 0.5
01234567 0000000 1122 050505
0.1 0.2 0.3 0.4 0.5
02468111 0000024 000 t
t
ci
a
oo
t
nin
o
trno of lsT
t
Nra
a
ob tl ee
Cg
Li1 e,
s
fith n(e
d
e-ep
tt
ue
a
nr ilf ieo ndr gm
i
ana pn psc ree
oc
ato
i
cf
o
hnt ,h we
2.
ht 2w
i.
c2o h)i wn isc ar
c
se
o
tm
rm
ae ipn nat era del
d
oad ntoa 2p 0a-
(a)Boostrappingscoringfunction (b)ML-basedscoringfunction
randomly selected scans without CL. All experiments used
a window size of 5, a step size of 1, and a step duration of
0.6 Train ML-based
T Tera si tn M B Lo -o bt as st era dpping 5 epochs. The selected target scans are those with the low-
0.5 Test Bootstrapping
estML-basedscores,whereasweselectthe19hardestscans
0.4
fromthesourceset.
0.3
Both the Target and Source2Target strategies
0.2
demonstrate a notable improvement over the No CL base-
0.1
line,withSource2Targetachievingsuperiorresults. We
0 50 100 150 200 250 300
ML-based ordering also measured the forgetting rate as the difference of met-
(c)ComplexityscoresonATM22
rics before and after fine tuning on the test source scans
Fig.3.ComplexityscoresonATM22andAIIB23:Bootstrap- (ATM22). Weobtainedanaverageforgettingrateof13.52%
pingandourML-basedscoringfunctionsarecomparedwith for Target and 9.37% for Source2Target. This mea-
histograms(top)andfororderingofCTscans(bottom). sures the degradation of the model performance on source
cohort, underscoring Source2Target’s advantage in re-
Table1reportssegmentationqualitymetricsmeasuredon taining source knowledge. Additionally, the forgetting rate
the largest connected component of all segmentation results ofthetwoproposeddomainadaptationstrategieswassignif-
from the test scans (see Fig 3 for complexity scores of the icantly lower than the baseline No CL approach (26.22%),
2 test subsets). Detailed explanations of these metrics are indicating that CL effectively mitigates catastrophic forget-
available in the AIIB23 official paper [9]. Metrics are also ting of previously learned cohorts and supports continuous
reportedforSAM-Med3D-turbo,ageneral-purposesegmen- cross-datasetvalidation.
tation model for 3D medical images [21]. In our study, its
predictions were of poor quality, limited to the trachea and 4. CONCLUSION
mainbronchi.
We investigated CL strategies to enhance airway segmenta-
FullTrainingResults. ExceptforBs Vanilla CL,which tion performance using a standard segmentation DL archi-
utilizedCS ,thetrainingsetorderforeachapproachisbased tecture across multiple performance metrics. We proposed
B
on the ML-based scoring function. Both Vanilla CL and anovelcomplexityscoringfunctionforlungCTscanswhich
ycneuqerF
32BIIA
erocs
ytixelpmoC
ycneuqerF
22MTA
ycneuqerF
32BIIA
ycneuqerF
22MTA
tegraT
tegraT_ot_ecruoSleads to high performance in CL training setups. For a full [8] Y. Qin, M. Chen, and H. et al. Zheng, “AirwayNet: A
trainingonasourcedomain(ATM22),weshowedthatbatch voxel-connectivity aware approach for accurate airway
compositionandorderingiscriticalforCLperformance. We segmentation using convolutional neural networks,” in
alsoshowedthataMixedCLapproachworksbestthanclassic Conference Medical Image Computing and Computer
CLordering[14]andoutperformsanoCLapproachwithour AssistedIntervention(MICCAI),2019,pp.212–220.
proposed scoring function. For few-shot domain adaptation
toatargetdomain(AIIB23)weobservedthataCLapproach [9] N.Yang,X.Xing,andS.etal.Wang,“Huntingimaging
with a mixture of samples from Source and Target domains biomarkers in pulmonary fibrosis: Benchmarks of the
outperformsagainanoCLapproach. AIIB23 challenge,” Medical Image Analysis, vol. 97,
pp.103253,2024.
5. ACKNOWLEDGMENTS [10] M.Zhang, X.Yu, H.Zhang, H.Zheng, W.Yu, H.Pan,
X. Cai, and Y. Gu, “FDA: Feature decomposition and
This research work is funded by the Doctoral School of IP-
aggregation for robust airway segmentation,” in MIC-
Paris and Hi!Paris and was performed using HPC resources
CAIWorkshop,DART.2021,p.25–34,Springer-Verlag.
fromGENCI-IDRIS(Grant2024-AD011013999R1).
[11] Minghui Zhang, Hanxiao Zhang, Guang-Zhong Yang,
and Yun Gu, “CFDA: Collaborative feature disentan-
6. REFERENCES
glement and augmentation for pulmonary airway tree
modeling of COVID-19 CTs,” in Conference Medical
[1] Antonio Garcia-Uceda Juarez, H. A. W. M. Tiddens,
Image Computing and Computer Assisted Intervention
andM.deBruijne, “Automaticairwaysegmentationin
(MICCAI), Cham, 2022, pp. 506–516, Springer Nature
chest CT using convolutional neural networks,” in Im-
Switzerland.
age Analysis for Moving Organ, Breast, and Thoracic
Images.2018,pp.238–250,SpringerInternationalPub-
[12] Yoshua Bengio, Je´roˆme Louradour, Ronan Collobert,
lishing.
andJasonWeston, “Curriculumlearning,” inProceed-
ingsoftheAnnualInternationalConferenceonMachine
[2] W. Kuo, M. de Bruijne, J. Petersen, K. Nasserinejad,
and H. et al. Ozturk, “Diagnosis of bronchiectasis and
Learning,NewYork,USA,2009,p.41–48.
airway wall thickening in children with cystic fibrosis:
[13] Daphna Weinshall, Gad Cohen, and Dan Amir, “Cur-
Objective airway-artery quantification,” European Ra-
riculum learning by transfer learning: Theory and ex-
diology,vol.27,pp.4680–4689,2017.
periments with deep networks,” in Proceedings of the
[3] Kensaku Mori and Jun-ichi et al. Hasegawa, “Auto- International Conference on Machine Learning. 10–15
matedextractionandvisualizationofbronchusfrom3D Jul2018,vol.80,pp.5238–5246,PMLR.
CTimagesoflung,” inComputerVision,VirtualReality
[14] GuyHacohenandDaphnaWeinshall, “Onthepowerof
andRoboticsinMedicine.1995,pp.542–548,Springer
curriculumlearningintrainingdeepnetworks,” inPro-
BerlinHeidelberg.
ceedings of the International Conference on Machine
[4] M. Sonka, Wonkyu Park, and E.A. Hoffman, “Rule- Learning. 09–15 Jun 2019, vol. 97, pp. 2535–2544,
based detection of intrathoracic airway trees,” IEEE PMLR.
Transactions on Medical Imaging, vol. 15, no. 3, pp.
[15] Tero Karras, Timo Aila, Samuli Laine, and Jaakko
314–326,1996.
Lehtinen, “ProgressivegrowingofGANsforimproved
[5] P. Lo, J. Sporring, H. Ashraf, J. J. H. Pedersen, and quality, stability, and variation,” in Proceedings of
M. de Bruijne, “Vessel-guided airway tree segmenta- International Conference on Learning Representations
tion: A voxel classification approach,” Medical Image (ICLR),2018.
Analysis,vol.144,pp.527–38,2010.
[16] Samarth Sinha, Animesh Garg, and Hugo Larochelle,
[6] Zijiang Bian and Jean-Paul Charbonnier et al., “Small “Curriculum by smoothing,” in Advances in Neu-
airway segmentation in thoracic computed tomogra- ral Information Processing Systems. 2020, vol. 33, pp.
phy scans: a machine learning approach,” Physics in 21653–21664,CurranAssociates,Inc.
Medicine&Biology,vol.63,2018.
[17] P. Morerio, J. Cavazza, R. Volpi, R. Vidal, and
[7] “Attention U-Net: Learning where to look for the pan- V.Murino, “Curriculumdropout,” IEEEInternational
creas,” Workingpaper,ArXiv,Apr.2018, arXivpreprint Conference on Computer Vision (ICCV), pp. 3564–
arXiv:1804.03999. 3572,2017.[18] SanmitNarvekarandPeterStone,“Learningcurriculum
policies for reinforcement learning,” in International
ConferenceonAutonomousAgentsandMultiagentSys-
tems(AAMAS),Montreal,Canada,May2019.
[19] Minghui Zhang, Yangqian Wu, and Hanxiao Zhang
etal., “Multi-site,multi-domainairwaytreemodeling,”
MedicalImageAnalysis,vol.90,pp.102957,2023.
[20] JohannesHofmanningerandF.Prayeretal.,“Automatic
lungsegmentationinroutineimagingisprimarilyadata
diversityproblem, notamethodologyproblem,” Euro-
peanRadiologyExperimental,vol.4,2020.
[21] HaoyuWang,SizhengGuo,JinYe,ZhongyiDeng,and
JunlongChengetal., “SAM-Med3D:Towardsgeneral-
purpose segmentation models for volumetric medical
images,” 2023.