Quantitative Assessment of Intersectional
Empathetic Bias and Understanding
Vojtěch Formánek1,2 and Ondřej Sotolář1
1 Masaryk University, Faculty of Informatics
2 Masaryk University, Department of Psychology, Faculty of Arts
xforman@mail.muni.cz
Abstract. A growing amount of literature critiques the current oper-
ationalizations of empathy based on loose definitions of the construct.
Suchdefinitionsnegativelyaffectdatasetquality,modelrobustness,and
evaluationreliability.Weproposeanempathyevaluationframeworkthat
operationalizesempathyclosetoitspsychologicalorigins.Theframework
measures the variance in responses of LLMs to prompts using existing
metrics for empathy and emotional valence. The variance is introduced
throughthecontrolledgenerationofthepromptsbyvaryingsocialbiases
affectingcontextunderstanding,thusimpactingempatheticunderstand-
ing. The control over generation ensures high theoretical validity of the
constructs in the prompt dataset. Also, it makes high-quality transla-
tion, especially into languages that currently have little-to-no way of
evaluating empathy or bias, such as the Slavonic family, more manage-
able.UsingchosenLLMsandvariousprompttypes,wedemonstratethe
empathy evaluation with the framework, including multiple-choice an-
swersandfreegeneration.Thevarianceinourinitialevaluationsampleis
smallandwewereunabletomeasureconvincingdifferencesbetweenthe
empathetic understanding in contexts given by different social groups.
However, the results are promising because the models showed signifi-
cant alterations their reasoning chains needed to capture the relatively
subtlechangesintheprompts.Thisprovidesthebasisforfutureresearch
into the construction of the evaluation sample and statistical methods
for measuring the results.
Keywords: Intersectional Bias · Empathy · LLM Evaluation
1 Introduction
Whiletherehasbeenavastamountofliteratureonempathy,ithascomeunder
increased scrutiny due to the unclear way of operationalizing empathy [13, 12].
Loosely defining empathy as the ability to understand another person’s feelings
and respond appropriately [12] was shown to cause problems across different
tasks, such as dataset creation [5], training [30], evaluation [12]. This ambiguity
ledtoanarrowfocusonemotionrecognitionandprediction.Weargue,alongside
previous work, that this effort misunderstands empathy’s psychological origins.
4202
voN
8
]LC.sc[
1v77750.1142:viXra2 V. Formánek, O. Sotolář
Toimproveuponthecurrentoperationalizationsofempathy,weproposeaframe-
work with (i) a disambiguation of empathy, (ii) measurement operationalization
specifically for computational models and (iii) an evaluatory procedure.
Some of the problems stem from the disagreement on the definition of em-
pathy within psychology itself [4]. It was originally used to describe human
ability to understand others [15, 11]. Current research agrees that it has two
components to achieve this: affective empathy, sometimes also named emotional
empathy, and cognitive empathy [31]. Affective empathy refers to the capacity
to feel emotions for others as a result of our belief, perception or imagination of
theirsituation[16].Cognitiveempathyinvolvestheorizingaboutandsimulation
ofothersmentalstates[31],thatisto:(i)retrodictivelysimulateamentalstate,
to explain observed behavior, (ii) take that mental state and run it through
our cognitive mechanisms, and (iii) attribute the conclusion to the target for
explanation and prediction.
Since cognitive empathy is dependent on one’s cognitive mechanisms, it is
alsodependentonexperience.Becauseofthat,apersoncanhavedifferentlevels
of understanding based on the similarity of his experience and the state he is
observingandfeeldifferentlevelsofempathytowarddifferentsocialgroups[37].
ThiseffectiscarriedovertoLLMs[25,22,32,3]whicharereinforcedonhuman
preference data. Observing if models exhibit this type of bias toward some of
the groups thus can be leveraged to indirectly study empathy.
We propose an empathy evaluation framework for conversational agents,
suchasLLMs,whichfocusesonempatheticunderstanding.Theframeworkuses
maskedtemplatestogenerateanevaluationdatasetofpromptsdesignedforthe
agents to respond to. The templates include masked sections into which differ-
ent information is inserted. The information is biased towards different social
groups; the selection of the type of information, the values, and social groups
are inspired by current research such as reviewed in Gallegos et al. [8].
Filling the masked section with varying values results in the evaluation sam-
ple. This enables measuring the variance in responses across a single template,
which assumesthe invariance inempatheticunderstanding,affect,and respond-
ingbothinsideandattheintersectionofdifferentsocialgroups.Thisinvariance
alsomeansthatwithintheframework,biasmanifestsasadeviationfromagiven
social group’s central tendency.
This might or might not be preferable; thus, we give fine-grained tools for
interpreting the scores within a given intersectional group individually. Taking
into account Blodgett et al.’s [1] criticism of the study of biases, within the
framework we only study the tendencies in model outputs and make no claims
about the potential harmfulness or possible impacts of the biases. We make all
of the data and code publicly available3.
3 https://github.com/xforman/JaEm_stJailbreaking Empathy 3
2 Related Work
Several metrics have been proposed for evaluating bias in generated outputs [8].
They are either based on a difference in the distribution of tokens in the gener-
ated outputs between distinct groups [23], a classifier, typically used to detect
toxicity [28, 29], or a lexicon [6]. The datasets used for evaluating bias deal
with various social groups and issues (see Table 4. in [8] for an overview) and
are sometimes created from existing datasets. Sample construction involves re-
placing the relevant social group identifiers (gender, race, etc.) with a mask,
thuscreatingmaskedsamples.Whenevaluating,themasksaresubstitutedwith
examples from the same group. The shift in the responses is measured [19, 35],
assumingthattheoutputshouldbeinvariantofthesocialgroup.Thistechnique
issometimescalledbiasmitigationviacontacthypothesis[22],atermborrowed
from psychology referring to direct contact with other social groups [20].
Datasets used to train empathetic agents are typically single or multi-turn,
with emotional labels [2, 24]. However, other datasets, by their nature, contain
empathy as well, such as transcripts of everyday conversations [14], simulation
of other’s personas [36] or transcripts of therapies [17, 21], labeled with conver-
sational behaviors [3]. Retroactively categorizing existing empathy metrics into
the two dimensions is difficult, especially since they likely overlap.
Since cognitive empathy involves understanding, the accuracy of emotion
prediction can be considered a case of it, but a broader understanding has also
been measured. Zhu et al. [38] collected user comments about products and
their do-, motor- and be-goals [10], then instructed human or LLM designers to
predict those goals and measured the token similarity between them.
The problems with measuring affective empathy are caused mainly by its
dependence on an inner state. Lee et al. [13] uses metrics based on the model
responses, thus might include mechanisms outside empathy. However, since the
metrics measure the qualities indirectly, we mention them here. Representative
ofthisgroupisspecificity,whichisbasedonanormalizedvariantofinversedoc-
ument frequency (NIDF) [26] and measures the similarity in the vocabularies of
the model and user. They also introduce valence, arousal, and dominance based
on the NRC Emotion Intensity Lexicon [18]. With the focus on the similarity
of the input and output texts, closer results are preferred. All of the affective
metrics assume that empathy in this context manifests in the similarity.
Lee et al.’s metrics also fall into the category of empathetic responding,
whichisthefocusofmanycurrentlyexistingmeasures.EpitomeisaRoBERTa-
basedmodelthatpredictsthreedimensionsonascale(0-2;none,weak,strong):
Empathetic Responses (ER), Explanations (EX) and Interpretations (IP) [27].
Chiu et al. [3] evaluate differences between human and LLM therapists and
define several dimensions whose quality is in part dependent on the empathetic
capacity of the therapist – Reflections, Questions, and Solutions.
All of the metrics depend on the true state of the evaluated sample (for
example, the emotion). This makes both dimensions of empathy dependent on
thisstate,meaningthatmisunderstandingleadstoaninaccurateeffect.Thus,we
cannot separate affective from cognitive empathy. For this reason, Coll et al. [4]4 V. Formánek, O. Sotolář
operationalizes the measurement of affective as the similarity of that affective
state to the one in the understood state.
3 JaEm-ST: Framework for the Quantitative Assessment
of Empathetic Behavior of LLMs
Weproposetheevaluationframework,JaEm-ST,whereempathyhastwodimen-
sions, Cognitive and Affective, which follow the definitions introduced in Sec-
tion 1. However, we operationalize the measurement into three metrics. Cogni-
tiveempathy(CE),thedegreetowhichtheempathizerunderstoodtheobserved
state correctly. Affective empathy (AE), the degree to which the empathizer’s
state matches that of the understood state (inspired by Coll et al. [4]). Lastly,
we define Empathic Response Appropriateness (ERA) to the understood state,
which is the result of the empathic process (and several others) but not its di-
mension. In our context, we define "state" as a person’s momentary mental and
physical circumstances.
3.1 Theoretical Basis of the Framework
Dependenceofcognitive empathyonexperiencemeansthatdifferentempathizers
might come to different conclusions about an observed state, even when self-
reporting [9]. This impacts the evaluation on two sides: (i) LM empathizers
might interpret the situation differently, which does not mean that it is false,
and(ii)the"truestates"constructedbythecreatormightnotevenbereflective
of their empathetic understanding. Thus, it is difficult, if not impossible, to
determinewhetheragiveninterpretationisgenuinelyfalse,butitisnonetheless
representativeofahumaninterpretationgroundedinexperience.Forthisreason,
AEandERAdependonthemodel’sunderstandingofthestate,soitispossible
to evaluate an output even when it does not interpret the context in the same
way as the creator of that template. But if we concede that the interpretation
of states is subjective, then we need another standard for the evaluation.
Inourcaseweassumethatempatheticunderstandingisinvariantacrosssim-
ilar situations; the implications of this assumption are discussed later. Which is
why JaEm-ST focuses on finding systemic differences in model output when re-
sponding to similar situations and uses the similarity between the "true state"
and the one predicted by the model as a guiding principle. Given that experi-
ence can lead to biases, we create the evaluation examples by inserting values
sampled from the representatives of different social groups (such as specific gen-
der, education etc.) into the masked templates (see Fig 1). Thus two examples
created from the same template differ only in the specific intersectional groups
that were inserted into them.
3.2 Evaluation Sample
Wecreatetheevaluationsampledynamicallyfromasmallseeddatasetofprede-
finedtemplates.ThetemplatessimulateconversationsbetweenahumanspeakerJailbreaking Empathy 5
and an empathizer, which is assumed to be a conversational agent and is to be
evaluated. It consists of four parts: instruction, context, conversation, and an-
swers. The templates are then filled with different combinations of identifying
information for commonly examined social groups: gender, race, education, reli-
gion, age [19] and socio-economic status [35], sexuality and pronouns.
The templates are based on causal tuples, which provide the reasoning for
what caused a person’s current observed state. It is partly set up by the con-
text, which defines the social groups the observed person is a member of. The
conversation withinthe templatesimplicitlyor explicitlydescribesthe situation
groundedonthecausaltuple.Theconversationcanalsoincludethesamemasks
as the context.
Giventhetemplate,whosemasksarefilledinwithacombinationofthepos-
siblevalues,theevaluatedmodelisthenpromptedtocontinuetheconversation.
If evaluating AE and ERA, it simply continues generating the response to the
observed person’s last utterance. When evaluating CE, it is given a choice be-
tween5answersandispromptedtopicktheonewiththedeepestunderstanding
of how the person reached the observed state in the last utterance.
Even though we assume that empathetic understanding is invariant in the
templates, there may be cases when this assumption does not hold. If certain
types of responses are preferable for a given group, and this type of response is
measured by a metric, it is possible to set thresholds for this metric and group
separately, independently of the overall scores. This is possible because we have
stratified information about the social groups which the observed person in the
evaluated context is a part of.
3.3 Experimental setup
Fortheevaluation,weconstructedtwocausaltuplesandtemplates,asshownin
Figure1.Toproducetheevaluationsample,thetemplatesaredynamicallyfilled
with the context-dependent information resulting from the Cartesian product
of all of the predefined replacement values of the studied social groups. Since
the product would be too large given any non-minimal value vectors, for this
experiment, we focus only on two of the dimensions: gender and pronoun. We
take their Cartesian product four times, then randomly sample from the rest
of the dimensions. We get 486 samples for generation or multi-choice, so each
model is evaluated with 972 samples, that is 1944 inputs in total.
Because we aim to maintain an understandable, controllable, high quality
datasetwithhighecologicalvalidity,wekeepthenumberoftemplatesrelatively
low. This also means that it is relatively simple to translate the templates to
other languages. This could be especially beneficial for low resource languages,
that currently have little to no way of evaluating bias or empathy.
The samples are given as input to a model in a single-turn fashion, which
is most typical for evaluating empathy. The sample is used as is, in the or-
der: (i) prompt ,context,conversation,answers for CE multiple choice, (ii)
CE
context,conversation for AE and ERA. And we pass each to the model sepa-
rately. Because of the small amount of templates, the variability of the input is6 V. Formánek, O. Sotolář
*Causal tuple (not part of the model input in any case):
(adores her parrot, happiness from a gift for her parrot, less critical towards
disliked food, complimented the cook)
*Prompt:
When given a choice of answers A-E choose the one that shows the deepest
understanding of Jane’s situation, not necessarily the most empathetic. Carl
has a full understanding of the context below:
Context:
Generally positive and optimistic person. [PRONOUN] is a [SEXUALITY]
and a firm [RELIGION]. [PRONOUN] always wanted to go to a university,
[PRONOUN] has [EDUCATION] degree. Being a proud cooking connoisseur
and a stern critic, [PRONOUN] rarely compliments other’s food, but today
[PRONOUN] complimented pasta ... The following is part of a conversation
with [PRONOUN] yoga instructor Carl.
Conversation:
Carl: Thank you, I also liked the classes today, seems like a happy day for
everyone. I wanted to make it a little harder for you. I noticed some exhausted
faces, but not any annoyed ones. How were the exercises in the middle?
Jane: I didn’t see any annoyed faces either ...
...
Jane: ... something weird happened to me today, we went for a typical lunch
with my coworkers, but I think I lost my integrity, and complimented a food I
did not actually like!
Carl:
*Answers:
A: I think gift for Poppy made you think of her and made you so happy, that
you complimented the chef even though you didn’t actually like the food.
B: You? Did you actually complimented a food? you’re always so strict and
sternaboutcritiquingotherpeople’sfood.Maybetheday,youknow,theyoga,
Lucy’s traveling and Poppy made you actually enjoy the food. What was it?
C: That’s surprising, maybe it’s because you were feeling so good ... Anyhow,
did Lucy tell you how her backpacking was in Texas?
D: Jane, I think it’s understandable to feel like this. But your integrity isn’t
lost on an occasion like this. ... How do you feel about the situation now?
E: Don’t be silly, it does not make you less of a critic. ...
Fig.1.Anexampleoftheframework’stemplates.[SOCIAL_GROUP]indicateswhere
a given social group is inserted. Answers and the prompt are only given to the model
if cognitive empathy is being evaluated. Otherwise, it generates Carl’s last response.
Option (A) shows the most profound understanding since it is closest to the causal
tuple – three dots in the text mark parts excluded for brevity.Jailbreaking Empathy 7
limited, thus the main purpose of the results is to showcase the evaluation. The
resulting CE is computed as the accuracy of choosing the most understanding
of the choices, across all samples. Valence, Arousal and Dominance are used for
AE, although their results should be interpreted with caution. They were also
be used to measure ERA, alongside Epitome.
We evaluate Llama-3.1-8B [7] and Zephyr-gemma-v.1 [34, 33]. A 80 GB
Nvidia A100 graphics card was used to process the evaluation.
4 Results
We measured the largest differences in all three dimensions: Cognitive Empa-
thy (CE), Affective Empathy (AE) and Empathetic Response Appropriateness
(ERA) – accuracy of multiple-choice answers (CE), VAD (AE) and Epitome
scores (ERA) between the two models and the two templates, as shown in Ta-
ble 1. The differences between the intersectional groups are relatively stable.
Generally,zephyr-gemma performedmuchworseinCE.Theresultsshowthatit
Cognitive Affective Response
Empathy Empathy Appropriateness
Model TemplateCount MC↑ V↓ A↓ D↓ IP↑ EX↑ER↑
Llama-3.1 0 243 0.29 0.16 0.11 0.13 0.12 1.56 0.54
1 243 0.35 0.15 0.11 0.13 0.19 0.09 0.91
Zephyr-gemma 0 243 0.01 0.22 0.14 0.18 0.08 0.19 0.79
1 243 0.10 0.18 0.13 0.16 0.07 0.01 1.60
Table 1. Evaluation of the generated responses on the framework’s three dimensions
of empathy. The metric for Cognitive empathy is the accuracy of finding the most
understanding results of the multi-choice answers (example in Fig. 1). Llama-3.1 has
higheraccuracyinthistask.Llama-3.1 ismorestableacrosstheVADmetrics,andalso
EmpatheticExplorations(EX)ononlyoneofthetemplates,whichwerelowotherwise.
Zephyr-gemma hadhigherscoresinEmpatheticResponding(ER),thismaybecaused
by the fact that it tends to role-play less.
was easiest for both of the models to find the answer with the most understand-
ing to the sample shown in Figure 1. The differences in AE scores between the
models are smaller and Llama-3.1 achieves lower scores and smaller differences
between the two samples, ERA scores are similar.
There are no obvious differences between the pronouns. We found outliers
in the interaction of these groups, such as Lesbian/She, which has an Interpre-
tations (IP) score significantly above the overall average (σ = 3.36), but were
unable to find any significant differences in the generated output.
Both models followed the multi-choice prompt well. For the free generation,
manual inspection of a small subset the outputs suggests that Llama-3.1 might
follow role-playing better, zephyr-gemma tends respond from the third perspec-8 V. Formánek, O. Sotolář
tive (10-15 %), or set the situation up in a couple of sentences (10 %), instead
of directly responding.
5 Conclusion
We provided a disambiguation of empathy for computational models to help
future work define the construct closer to its psychological origins as opposed
to the loose definitions that are currently widespread. As a main result, we
proposed a new empathy evaluation framework for the responses generated by
conversationalagentsthatacknowledgestheinherentsubjectivityofempathetic
understanding. The framework focuses on how it is influenced by intersectional
bias. It provides methods to generate evaluation samples from templates by in-
serting the intersectional contexts into them. The framework uses a new three-
dimensional measurement operationalization of empathy to measure the con-
struct. We demonstrated the usage of the framework on a small synthetic sam-
ple. In all three framework dimensions, we measured significant differences be-
tween the Llama-3.1-8B and Zephyr-gemma-v.1 models. Lastly, we identified
differences in empathetic understanding across the evaluated metrics in some
intersectional groups. More importantly, we showed the framework’s strength in
providing the ability to stratify scores across a wide range of social contexts,
giving a more fine-grained insight into model behavior and potential harms.
Limitations
We view the modest number of templates as a limitation. Even though we can
produce many examples by substituting into the masks, most of their structure
stays the same. Further, the contexts and conversations do not reflect the varia-
tionacrossmultipledifferentgroups;futureworkshouldalsofocusonincreasing
thediversityofthesamplecreators.Thetemplatestructureitself,especiallythe
inclusionofcontextasanexplanationofthespeaker’sbackground,doeshurtthe
naturalness and ecological validity of the framework. Since the models generate
morethanjusttheexpectedresponse,futureworksshouldexploredifferentways
to insert the context into the model. Furthermore, while the number of empa-
thy metrics in this work is limited, future works can use the outlined criteria to
include other metrics.
References
[1] Su Lin Blodgett et al. “Language (technology) is power: A critical survey
of" bias" in nlp”. In: arXiv preprint arXiv:2005.14050 (2020).
[2] Sven Buechel et al. “Modeling Empathy and Distress in Reaction to News
Stories”. In: Proceedings of the 2018 Conference on Empirical Methods in
Natural Language Processing. Ed. by Ellen Riloff et al. Brussels, Belgium:
Association for Computational Linguistics, 2018, pp. 4758–4765. doi: 10.
18653/v1/D18-1507. url: https://aclanthology.org/D18-1507.Jailbreaking Empathy 9
[3] YuYingChiuetal.“Acomputationalframeworkforbehavioralassessment
of llm therapists”. In: arXiv preprint arXiv:2401.00820 (2024). doi: 10.
48550/arXiv.2401.00820.
[4] Michel-PierreColletal.“Arewereallymeasuringempathy?Proposalfora
new measurement framework”. In: Neuroscience & Biobehavioral Reviews
83 (2017), pp. 132–139.
[5] Alok Debnath and Owen Conlan. “A Critical Analysis of EmpatheticDia-
loguesasaCorpusforEmpatheticEngagement”.In:Proceedingsofthe2nd
Empathy-Centric Design Workshop. EMPATHICH ’23. , Hamburg, Ger-
many, Association for Computing Machinery, 2023. isbn: 9798400707490.
doi: 10.1145/3588967.3588973. url: https://doi.org/10.1145/
3588967.3588973.
[6] Jwala Dhamala et al. “Bold: Dataset and metrics for measuring biases in
open-ended language generation”. In: Proceedings of the 2021 ACM con-
ference on fairness, accountability, and transparency. 2021, pp. 862–872.
[7] Abhimanyu Dubey et al. “The llama 3 herd of models”. In: arXiv preprint
arXiv:2407.21783 (2024).
[8] Isabel O Gallegos et al. “Bias and fairness in large language models: A
survey”. In: Computational Linguistics (2024), pp. 1–79.
[9] Sarah A Grainger et al. “Measuring empathy across the adult lifespan:
A comparison of three assessment types”. In: Assessment 30.6 (2023),
pp. 1870–1883.
[10] Marc Hassenzahl. Experience design: Technology for all the right reasons.
Morgan & Claypool Publishers, 2010.
[11] Gustav Jahoda. “Theodor Lipps and the shift from “sympathy” to “empa-
thy””. In: Journal of the History of the Behavioral Sciences 41.2 (2005),
pp. 151–163.
[12] AllisonLahnalaetal.“Acriticalreflectionandforwardperspectiveonem-
pathyandnaturallanguageprocessing”.In:arXivpreprintarXiv:2210.16604
(2022).
[13] Andrew Lee et al. “A Comparative Multidimensional Analysis of Empa-
thetic Systems”. In: Proceedings of the 18th Conference of the European
ChapteroftheAssociationforComputationalLinguistics(Volume1:Long
Papers). 2024, pp. 179–189.
[14] Yanran Li et al. “DailyDialog: A Manually Labelled Multi-turn Dialogue
Dataset”. In: arXiv [cs.CL] (Oct. 2017).
[15] Theodor Lipps. Leitfaden der psychologie. W. Engelmann, 1909.
[16] Heidi L Maibom. “Affective empathy”. In: The Routledge handbook of phi-
losophy of empathy. Routledge, 2017, pp. 22–32.
[17] GaneshanMalhotraetal.“Speakerandtime-awarejointcontextuallearn-
ingfordialogue-actclassificationincounsellingconversations”.In:Proceed-
ings of the fifteenth ACM international conference on web search and data
mining. 2022, pp. 735–745.
[18] SaifMMohammad.“Wordaffectintensities”.In:arXivpreprintarXiv:1704.08798
(2017).10 V. Formánek, O. Sotolář
[19] SergioMorales,RobertClarisó,andJordiCabot.“ADSLforTestingLLMs
for Fairness and Bias”. In: Proceedings of the ACM/IEEE 27th Interna-
tional Conference on Model Driven Engineering Languages and Systems.
2024, pp. 203–213.
[20] Elizabeth Levy Paluck, Seth A Green, and Donald P Green. “The contact
hypothesisre-evaluated”.In:BehaviouralPublicPolicy 3.2(2019),pp.129–
158.
[21] VerónicaPérez-Rosasetal.“Whatmakesagoodcounselor?learningtodis-
tinguish between high-quality and low-quality counseling conversations”.
In: Proceedings of the 57th Annual Meeting of the Association for Compu-
tational Linguistics. 2019, pp. 926–935.
[22] Chahat Raj et al. “Breaking bias, building bridges: Evaluation and miti-
gationofsocialbiasesinLLMsviaContactHypothesis”.In:arXiv [cs.CL]
(July 2024). doi: 10.48550/arXiv.2407.02030.
[23] Pranav Rajpurkar et al. “SQuAD: 100,000+ Questions for Machine Com-
prehension of Text”. In: Proceedings of the 2016 Conference on Empirical
Methods in Natural Language Processing. Ed. by Jian Su, Kevin Duh,
and Xavier Carreras. Austin, Texas: Association for Computational Lin-
guistics, Nov. 2016, pp. 2383–2392. doi: 10.18653/v1/D16-1264. url:
https://aclanthology.org/D16-1264.
[24] Hannah Rashkin et al. “Towards Empathetic Open-domain Conversation
Models: A New Benchmark and Dataset”. In: Proceedings of the 57th An-
nual Meeting of the Association for Computational Linguistics. Ed. by
Anna Korhonen, David Traum, and Lluís Màrquez. Florence, Italy: As-
sociation for Computational Linguistics, July 2019, pp. 5370–5381. doi:
10.18653/v1/P19-1534. url: https://aclanthology.org/P19-1534.
[25] Shibani Santurkar et al. “Whose opinions do language models reflect?” In:
International Conference on Machine Learning. PMLR. 2023, pp. 29971–
30004.
[26] Abigail See et al. “What makes a good conversation? how controllable
attributes affect human judgments”. In: arXiv preprint arXiv:1902.08654
(2019).
[27] Ashish Sharma et al. “A Computational Approach to Understanding Em-
pathy Expressed in Text-Based Mental Health Support”. In: Proceedings
of the 2020 Conference on Empirical Methods in Natural Language Pro-
cessing (EMNLP). Ed. by Bonnie Webber et al. Online: Association for
ComputationalLinguistics,Nov.2020,pp.5263–5276.doi:10.18653/v1/
2020.emnlp-main.425. url: https://aclanthology.org/2020.emnlp-
main.425.
[28] Emily Sheng et al. “The woman worked as a babysitter: On biases in
language generation”. In: arXiv preprint arXiv:1909.01326 (2019).
[29] AnthonySiciliaandMaliheAlikhani.“Learningtogenerateequitabletext
indialoguefrombiasedtrainingdata”.In:arXivpreprintarXiv:2307.04303
(2023).Jailbreaking Empathy 11
[30] OndrejSotolaretal.EmPO:EmotionGroundingforEmpatheticResponse
Generation through Preference Optimization. 2024. arXiv: 2406.19071
[cs.CL]. url: https://arxiv.org/abs/2406.19071.
[31] Shannon Spaulding. “Cognitive empathy”. In: The Routledge handbook of
philosophy of empathy. Routledge, 2017, pp. 13–21.
[32] Elizabeth Stade et al. “Artificial intelligence will change the future of psy-
chotherapy: A proposal for responsible, psychologist-led development”. In:
(2023).
[33] LewisTunstallandPhilippSchmid.Zephyr7BGemma.https://huggingface.
co/HuggingFaceH4/zephyr-7b-gemma-v0.1. 2024.
[34] Lewis Tunstall et al. Zephyr: Direct Distillation of LM Alignment. 2023.
arXiv: 2310.16944 [cs.LG].
[35] Yuxuan Wan et al. “Biasasker: Measuring the bias in conversational ai
system”. In: Proceedings of the 31st ACM Joint European Software Engi-
neering Conference and Symposium on the Foundations of Software Engi-
neering. 2023, pp. 515–527.
[36] Saizheng Zhang et al. “Personalizing Dialogue Agents: I have a dog, do
you have pets too?” In: arXiv [cs.AI] (Jan. 2018).
[37] HaiyanZhaoetal.“ExplainabilityforLargeLanguageModels:ASurvey”.
In: arXiv (Sept. 2023).
[38] Qihao Zhu et al. “Reading Users’ Minds from What They Say: An Inves-
tigation into LLM-based Empathic Mental Inference”. In: arXiv preprint
arXiv:2403.13301 (2024).