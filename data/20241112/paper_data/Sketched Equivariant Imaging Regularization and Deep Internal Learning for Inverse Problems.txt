Sketched Equivariant Imaging Regularization and Deep Internal Learning
for Inverse Problems
GuixianXu,JinglaiLi,JunqiTang* J.TANG.2@BHAM.AC.UK
SchoolofMathematics,UniversityofBirmingham
Abstract
Equivariant Imaging (EI) regularization has become the de-facto technique for unsupervised training
of deep imaging networks, without any need of ground-truth data. Observing that the EI-based unsuper-
visedtrainingparadigmcurrentlyhassignificantcomputationalredundancyleadingtoinefficiencyinhigh-
dimensionalapplications,weproposeasketchedEIregularizationwhichleveragestherandomizedsketching
techniquesforacceleration. WethenextendoursketchedEIregularizationtodevelopanaccelerateddeep
internallearningframework–SketchedEquivariantDeepImagePrior(Sk.EI-DIP),whichcanbeefficiently
appliedforsingle-imageandtask-adaptedreconstruction. OurnumericalstudyonX-rayCTimagerecon-
struction tasks demonstrate that our approach can achieve order-of-magnitude computational acceleration
overstandardEI-basedcounterpartinsingle-inputsetting,andnetworkadaptationattesttime.
1. Introduction
Unsupervised training has become a vital research direction for imaging inverse problems (Carioni et al.,
2024; Tirer et al., 2024). For the most popular and widely applied medical imaging applications such as
CT/MRI/PET,suchstrategiesseektotraindeepreconstructionnetworkfromonlythenoisyandincomplete
measurementdata:
y = Ax†+ε, (1)
wherewedenoteherex† ∈ Rd astheground-truthimage(tobeestimated),A ∈ Rn×d asthemeasurement
operator,ε ∈ Rdasthemeasurementnoise,whiley ∈ Rnasthemeasurementdata. Tobemoreprecise,such
unsupervised training schemes typically seek to learn a reconstruction network (set of network parameters
denotedasθ here)
F (A†y) → x (2)
θ
with only the knowledge of the measurement data y and the measurement operator A. Here we denote A†
as the pseudo-inverse of A, or a stable approximation of it (such as the FBP for X-ray CT). This line of
research initialized on the Deep Internal Learning schemes (see recent review paper by Tirer et al. (2024))
such as the Deep Image Prior (DIP) approach (Ulyanov et al., 2018; Tachella et al., 2021; Mataev et al.,
2019;Liuetal.,2019)forlearningimagereconstructionfromsingleinput,andmorerecentlytheNoise2X-
type approaches (Lehtinen et al., 2018; Batson and Royer, 2019), and its extension Artifact2Artifact (Liu
etal.,2020). ThemostsuccessfulunsupervisedtrainingparadigmiscurrentlytheEquivariantImaging(EI)
regularization proposed by (Chen et al., 2021, 2022; Tachella et al., 2023), which is able to enforce the
networktolearnbeyondtherangespaceofthemeasurementoperatorAandfullyachieveground-truth-free
training under incomplete and noisy measurements. Comparing to the standard supervised training which
requires the ground-truth and measurement pairs, the unsupervised training in general, is computationally
1
4202
voN
8
]VI.ssee[
1v17750.1142:viXramuch more costly. This is due to the fact that the forward/adjoint/pseudo-inverse operators are computed
in each iteration of the training optimizer, leading to significant computational overhead. In this work, we
takeaninitialyetcrucialstepformitigatingthecomputationallimitationofunsupervisedtrainingschemes
fordeepimagingnetworks,byacceleratingEIregularizationviadimensionalityreduction(sketching)tech-
niques rooted in stochastic optimization. We apply the sketched EI regularizer first in the internal learning
setting,whichcanbeviewasanEI-regularizedDIP.OurnumericalstudyinX-rayCTimagereconstruction
demonstratesthatourproposedSketchedEI-DIPcanachieveorder-of-magnitudeaccelerationoverstandard
EI-regularized DIP under this single-input internal learning setting, as well as the network-adaptation task
intesttime.
1.1 Background
Deep Internal Learning. The most typical and fundamental deep internal learning scheme is the deep
imaging prior (DIP) approach proposed by (Ulyanov et al., 2018). The DIP can be described as approxi-
matelyminimizingthefollowinglossbyfirst-orderoptimizerssuchasSGDorAdam:
θ⋆ ≈ argmin∥y−AF (z)∥2,
θ 2
θ (3)
x⋆ = F (z),
θ⋆
wheretheinputz canbechosenasrandomvectororz = A†y forwarm-starting(Tachellaetal.,2021). In
itsvanillaform,ittakesanrandomlyinitializeddeepconvolutionalnetworkandtrainitdirectlyonthegiven
single measurement data. Perhaps surprisingly, the vanilla version of DIP can already provide excellent
reconstruction performance on many inverse problems such as image denoising/superresolution/deblurring
andtomographicimagereconstructiontaskssuchasCT/MRI/PET(Singhetal.,2023;Mayoetal.,2024).
Apartfromlearningreconstructionfromasingleinput,DIPcanalsobeeffectivelyappliedfornetwork
adaptation at test time against distribution shifts (Barbano et al., 2022). Suppose given a pretrained recon-
struction network on a certain set of measurements y and operator A, if we apply directly the network to
a different inverse problem with a new A′ which have fewer measurements and noisier y′, the pretrained
network can perform badly. However, one can mitigate this distribution shift by adapting the network via
theDIPframeworkaboveorregularizedversionsofDIPsuchastheDIP-GSUREapproachbyAbu-Hussein
etal.(2022).
EquivariantImagingRegularization. Amongalltheunsupervisedapproachesfortrainingdeepimaging
networks, EI framework proposed by Chen et al. (2021, 2022) was the first to address explicitly the issue
oflearningbeyondtherangespaceoftheforwardoperatorAutilizingtheinherentsymmetricstructuresof
the imaging systems. Given a collection of measurements Y, and group transformation T tailored for the
g
imagingsystems(forexample,rotationforCT),thevanillaEIframeworkcanbewrittenas:
θ⋆ ≈ argminf (θ) := E {∥y−A(F (A†y))∥2+λ∥T F (A†y)−F (A†AT F (A†y)∥2},
θ EI g∼G,y∼Y (cid:124) (cid:123)θ (cid:122) (cid:125)2 (cid:124) g θ θ (cid:123)(cid:122) g θ (cid:125)2 (4)
MCloss EIregularization
wherethefirsttermisthemeasurement-consistencyloss,whilethesecondtermistheEIregularizerwhich
enable the training program to learn in the null-space of A. After this first pivoting work, variants of EI
hasbeendevelopedforenhancingrobustnesstomeasurementnoise,usingadditionalG-SURE(Chenetal.,
2022) or UNSURE (Tachella et al., 2024) regularizer alongside with EI. Meanwhile, extensions of EI on
new group actions tailored for different inverse problems have been proposed (Wang and Davies, 2024b,a;
Scanvic et al., 2023). In our work we focus on sketching the vanilla form of EI regularizer but the same
principlecanbeeasilyextendedforalltheseenhancedversionsofEI.
2RandomizedSketchingandStochasticOptimization. Operatorsketchingandstochasticgradient-based
optimization techniques has been widely applied for machine learning (Kingma and Ba, 2015; Johnson
and Zhang, 2013; Pilanci and Wainwright, 2015, 2017; Tang et al., 2017) and more recently in imaging
inverse problems (Sun et al., 2019; Ehrhardt et al., 2024). In the context of imaging inverse problems,
given a measurement consistency loss argmin ∥Ax − y∥2, one can split the objective function in to N
x 2
partitionofminibatches: ∥Ax−y∥2 = (cid:80)N ∥M Ax−M y∥2,whereM arethesetofsketchingoperators
2 i=1 i i 2 i
(typically sub-sampling for imaging applications). After splitting the objective function into minibatches,
one can use stochastic gradient methods as the optimizer for efficiency. In a pioneering work of Tang
etal.(2020),theydemonstratethatthesuccessofstochasticoptimizationandoperatorsketchingtechniques
depends on the spectral structure of the forward operator A. If A has a fast decay on the singlar-value
spectrum, then we can expect order-of-magnitude acceleration in terms of computational complexity over
deterministicmethodssuchasproximalgradientdescentorFISTA(BeckandTeboulle,2009). Mostofthe
computationallyintensiveimaginginverseproblemsfallintothiscategory,forexample,X-rayCT,multi-coil
MRIandPositronEmissionTomographyalladmitefficientapplicationsofoperatorsketchingandstochastic
optimization. Inourworkhere,weleverageoperatorsketchingfortheaccelerationofEI-regularizationand
jointlywiththeDIPmeasurementconsistency.
1.2 Contributions
Thecontributionofourworkisthree-fold:
• Sketched EI regularization – We propose an efficient variant of the EI regularizer, mitigating the
computationalinefficiencyoftheoriginalapproachby(Chenetal.,2021).
• Theoretical analysis of sketched EI – We provide a motivational theoretical analysis on the ap-
proximation bound of our sketched EI regularizer, demonstrating that our approach admits a nice
mathematicalinterpretation.
• SketchedEI-DIPandapplicationinX-rayCT–BasedonourSketchedEIregularizer,jointlywith
sketching on measurement consistency term, we propose an efficient deep internal learning frame-
work, namely the Sk.EI-DIP, for single-input and task-adapted image reconstruction. We apply our
Sk.EI-DIP approach in sparse-view X-ray CT imaging tasks and demonstrate significant computa-
tionalaccelerationoverstandardDIPandEI-regularizedDIP.
2. SketchedEquivariantImagingRegularizationandDeepInternalLearning
In this section, we present our sketching scheme for the EI regularizer, and the resulting Sketched EI-DIP
approach for single-input deep internal learning. Meanwhile, we provide a theorectical analysis revealing
themotivationofthisschemeforadditionalinsights.
2.1 AlgorithmicFramework
TheEI-regularizedunsupervisedlearningismuchmorecomputationallyinefficientcomparingtosupervised
training,duetotheneedofcomputingthemeasurementoperatormultipletimesineverytrainingiterations.
Observing that there is significant computational redundancy in the EI-regularizer, we propose a sketched
EIregularizer,whichreplaceAandA† withsketchedminibatchesA := MAandA† := (MA)†,where
M M
3M ∈ Rm×d isarandomsketchingoperatorsatisfyingE(MTM) = I. Inimaginginverseproblems,wecan
simplychooseM tobeasub-samplingoperator:
∥T F (A†y)−F (A† A T F (A†y)∥2 ≈ ∥T F (A†y)−F (A†AT F (A†y)∥2. (5)
g θ θ M M g θ 2 g θ θ g θ 2
JointlyperformingthesketchintheDIPmeasurementconsistencytermwithy := My,wecanderiveour
M
SketchedEI-DIPasfollows:
SketchedMCloss
(cid:122) (cid:125)(cid:124) (cid:123)
θ⋆ ≈ argminf (θ) := E {∥y −A (F (A†y))∥2
Sk.EI−DIP g∼G,M M M θ 2
θ (6)
+λ∥T F (A†y)−F (A† A T F (A†y)∥2},
g θ θ M M g θ 2
(cid:124) (cid:123)(cid:122) (cid:125)
SketchedEIregularization
Then taking x⋆ = F (A†y) we obtain the desired output. Here A† and A† are stable approximations
θ⋆ M
of pseudo-inverses, for example, the filtered-backprojection (FBP) for X-ray CT imaging. In practice, we
can always precompute A†y, and also predefine a set of N sketching operators M ,M ,...M . In each
1 2 N
of training iteration of SGD or Adam, we randomly sample a M where i ∈ [1,N] and compute an effi-
i
cient approximate gradient. For structured inverse problems such as X-ray CT, we can observe significant
computationalaccelerationviathisformofsketching.
2.2 TheoreticalAnalysis
Foraproof-of-concept,weprovidefollowingboundfortheproposedsketchingscheme,demonstratingthat
oursketchedEIregularizerisaneffectiveapproximationoftheoriginalEI.
Theorem1 (ApproximationboundforSketchedEIregularization)SupposethenetworkF isL-Lipschitz,
θ
while∥v∥ ≤ r,wehave:
2
∥v−F (A†Av)∥ −Lrδ ≤ ∥v−F (A† A v)∥ ≤ ∥v−F (A†Av)∥ +Lrδ (7)
θ 2 θ M M 2 θ 2
almostsurely,whereδisaconstantonlydependingonthesketchsizemandthechoiceofsketchingoperator,
andwedenoteherev := T F (A†y).
g θ
Weprovidetheproofofthistheoremintheappendix. TheL-Lipschitzcontinuityassumptionoftheform:
∥F (p)−F (q)∥ ≤ L∥p−q∥ , ∀p,q ∈ X (8)
θ θ 2 2
onthereonstructionnetworkF isstandardforthetheoreticalanalysisofdeepnetworksinimaginginverse
θ
problems. For example, in the convergence analysis of plug-and-play algorithms (Ryu et al., 2019; Tan
etal.,2024)anddiffusion-basedMCMC(Caietal.,2024),suchtypeofassumptionshavebeenusedonthe
pretrained denoisers or generative image priors based on deep networks for the convergence proofs. The
theorem above provides a upper bound and lower bound sandwiching the sketched EI regularization with
√
originalEIregularization,withadeviationδ whichscalesapproximatelyasO(1/ m).
This theoretical result, although preliminary and motivational, justifies that the proposed sketching
scheme provides a good approximation for original EI regularizer statistically, and admits a nice mathe-
maticalinterpretation.
43. NumericalExperiments
Inthissection,weshownumericallytheperformanceoftheproposedmethodforthesparse-viewCTimage
reconstructionproblem.
3.1 SetupandImplementation
We evaluate the proposed approach on sparse-view CT image reconstruction problem, where the forward
operator A is underdetermined with a nontrivial null-space (m < n). Specifically, the imaging physics
model of X-ray computed tomography (CT) is the discrete radon transform, and the physics model A is
theradontransformationwhere50views(angles)areuniformlysubsampledtogeneratedthesparse-view
sinograms(observations)y. TheFilteredbackprojection(FBP)function,i.e. iradon,isusedtoperforma
stableapproximationofA†.
Theaimofthistaskistorecoveragroundtruthimagex∗fromasingleobservationy. Weincorporatethe
invariance of the CT images to rotations into the deep image prior scheme, and G is the group of rotations
from 1 to 360 degrees (|G| = 360). We use a single data sampled from the CT100 dataset (Clark et al.,
2013), and resize it to 256×256 pixels, then we apply the radon function on it to generate the 50-views
sinogram.
Throughouttheexperiments,weuseaU-Net(Ronnebergeretal.,2015)tobuildF assuggestedinChen
θ
et al. (2021). We compare our method (Sketched EI-DIP) with two different learning strategies: unsuper-
vised deep image prior approach with the loss function (3); unsupervised deep image prior term with the
equivariance regularized term (EI-DIP) as showed in (4). For a fair comparison with sketched EI-DIP, we
use the residual U-Net architecture for all the counterpart learning methods to ensure all methods have the
sameinductivebiasfromtheneuralnetworkarchitecture.
We demonstrate that the sketched operation is straightforward and can be easily extended to existing
deep models without modifying the architectures. All our experiments were performed using a NVIDIA
RTX 3050ti GPU, alongside with DeepInv toolbox1. All the compared methods are implemented in Py-
TorchandoptimizedbyAdam(KingmaandBa,2015),forwhichwesetthelearningrateto5×10−4. We
trainallofthecomparingmethodsover5,000iterations.
3.2 NumericalResults
We conduct a comparative numerical study of our proposed Sketched EI-DIP method against traditional
method such as the DIP, as well as the deep image prior with state-of-the-art equivariant regularizer (Chen
et al., 2021), EI-DIP. To ensure a fair comparison, all parameters involved in the deep image prior and EI
method are either manually tuned to optimality or automatically selected as described in the referenced
papers. For our Sketched EI-DIP, we choose subsampling sketch as our M, which split the measurement
operator into N minibatches, A ,A ...A from interleaved angles. We test on the choices N =
M1 M2 MN
2,3,4,5respectivelyhere. Wepresentourresultswithdetaileddescriptionsinthefollowingfigureswhich
demonstratetheeffectivenessofoursketchedEIscheme.
1.https://deepinv.github.io/deepinv/
5Figure 1: Images reconstructed by DIP, EI-DIP and our Sketched EI-DIP. We can observe that the DIP
approach performs badly in our experiments (DIP-1 uses UNet while DIP-2 employs an Auto-Encoder
networkachitecture). WiththehelpofEI-regularizer,theEI-DIPapproachcanprovidemuchmoreimproved
reconstruction. OurSketchedEI-DIPperformsasgoodasEI-DIP.
Figure2: ImagesreconstructedbySketchedEI-DIP,withdifferentsketchsizes. Thesketchingisperformed
increasingly more aggressive from left to right, and we can observe that the results for Sketched EI-DIP is
nearly the same (sometimes even better) than full EI-DIP, for the number of minibatch split chosen to be
either N = 2, 3 or 4. The result would deteriorate if we choose to sketch over aggressively, say N = 5,
indicatingaphase-transition.
6Figure 3: Reconstruction accuracy curves for all the compared schemes. We can observe that, for the case
wheretheminibatchsplitnumbertobe2,3or4,theSketchedEI-DIPconvergestothesameaccuracycom-
paringtofullEI-DIP,withthesameconvergencespeed(yetthecomputationaltimeismassivelyreduced).
Figure 4: Execution time comparision between EI-DIP and Sketched EI-DIP at each iteration. We can
observethatourschemecanachievesignificantcomputationalaccelerationoverfullEI-DIP.
4. Conclusion
InthisworkweproposeasketchedEIregularizerwhichcanbeefficientlyappliedforunsupervisedtraining
of deep imaging networks, especially in the deep internal learning setting with single input. We provide
a motivational theoretical analysis of the proposed sketching scheme demonstrating that it is an effective
approximation of the original EI regularization proposed by Chen et al. (2021). In single-input setting, we
7compare our Sketched EI-DIP approach with standard DIP and EI-DIP for sparse-view X-ray CT imaging
tasks, and observe a substantial acceleration. Our work has focused on sketching the vanilla form of EI,
nevertheless, it is obvious that this scheme can be trivially applied to its enhanced versions such as REI
(Chenetal.,2022;Tachellaetal.,2024).
5. Appendix
In this appendix we provide the proof for the motivational bound presented in Theorem 1 for the approxi-
mationofEIregularizer. Westartbyprovingtheupperbound:
∥v−F (A† A v)∥ = ∥v−F (A† A v)+F (A†Av)−F (A†Av)∥
θ M M 2 θ M M θ θ 2
(9)
≤ ∥v−F (A†Av)∥ +∥F (A† A v)−F (A†Av)∥ .
θ 2 θ M M θ 2
DuetotheassumptionontheL-Lipschitzcontinuityofthenetwork,wehave
∥F (A† A v)−F (A†Av)∥
θ M M θ 2
≤ L∥A† A v−A†Av∥
M M 2
≤ L∥A†M†MA−A†A∥ ∥v∥ (10)
2 2
≤ L∥A†(MTM −I)A∥ ∥v∥
2 2
≤ Lr∥AT(MTM −I)A∥
2
Theremainingchallengeistoboundtheterm∥AT(MTM −I)A∥ ≤ δ almostsurely,whichactualvalue
2
is dependent on the choice of sketching operator M. Here we illustrate with the choice of sub-Gaussian
sketches,whilesubsamplingandrandomizedorthogonalsketchesalsosatisfytheboundwithdifferentvalue
ofδ andprobability. Accordingto(PilanciandWainwright,2015,Proposition1),ifM isaσ-sub-Gaussian
sketchwithsketchsizem,wehave:
Lr∥AT(MTM −I)A∥ ≤ Lrδ, (11)
2
with:
(cid:114)
d
δ = c +δ (12)
0 0
m
mδ2
with probability at least 1−exp(−c 0) where c ,c are universal constants. We hence finish the proof
1 σ4 0 1
oftheupperbound. Forthelowerbound,weusethesamereasoning:
∥v−F (A†Av)∥ = ∥v−F (A† A v)+F (A† A v)−F (A†Av)∥
θ 2 θ M M θ M M θ 2
≤ ∥v−F (A† A v)∥ +∥F (A† A v)−F (A†Av)∥ (13)
θ M M 2 θ M M θ 2
≤ ∥v−F (A† A v)∥ +Lrδ
θ M M 2
Thenimmediatelywehave∥v−F (A† A v)∥ ≥ ∥v−F (A†Av)∥ −Lrδ. Thusfinishestheproof.
θ M M 2 θ 2
8References
Shady Abu-Hussein, Tom Tirer, Se Young Chun, Yonina C Eldar, and Raja Giryes. Image restoration by
deepprojectedgsure. InProceedingsoftheIEEE/CVFWinterConferenceonApplicationsofComputer
Vision,pages3602–3611,2022.
RiccardoBarbano,JohannesLeuschner,MaximilianSchmidt,AlexanderDenker,AndreasHauptmann,Pe-
ter Maass, and Bangti Jin. An educated warm start for deep image prior-based micro ct reconstruction.
IEEETransactionsonComputationalImaging,8:1210–1222,2022.
JoshuaBatsonandLoicRoyer.Noise2self: Blinddenoisingbyself-supervision.InInternationalConference
onMachineLearning,pages524–533,2019.
A. Beck and M. Teboulle. Fast gradient-based algorithms for constrained total variation image denoising
anddeblurringproblems. IEEETransactionsonImageProcessing,18(11):2419–2434,2009.
ZiruoCai,JunqiTang,SubhadipMukherjee,JinglaiLi,Carola-BibianeScho¨nlieb,andXiaoqunZhang. Nf-
ula: Normalizingflow-basedunadjustedlangevinalgorithmforimaginginverseproblems. SIAMJournal
onImagingSciences,17(2):820–860,2024.
MarcelloCarioni,SubhadipMukherjee,HongYeTan,andJunqiTang. Unsupervisedapproachesbasedon
optimaltransportandconvexanalysisforinverseproblemsinimaging. RadonSeriesonComputational
andAppliedMathematics,2024.
Dongdong Chen, Julia´n Tachella, and Mike E Davies. Equivariant imaging: Learning beyond the range
space. InProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision,pages4379–4388,
2021.
Dongdong Chen, Julia´n Tachella, and Mike E Davies. Robust equivariant imaging: a fully unsupervised
frameworkforlearningtoimagefromnoisyandpartialmeasurements. InProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition,pages5647–5656,2022.
KennethClark,BruceVendt,KirkSmith,JohnFreymann,JustinKirby,PaulKoppel,StephenMoore,Stan-
ley Phillips, David Maffitt, Michael Pringle, et al. The cancer imaging archive (tcia): maintaining and
operatingapublicinformationrepository. Journalofdigitalimaging,26:1045–1057,2013.
Matthias J Ehrhardt, Zeljko Kereta, Jingwei Liang, and Junqi Tang. A guide to stochastic optimisation for
large-scaleinverseproblems. arXivpreprintarXiv:2406.06342,2024.
RieJohnsonandTongZhang. Acceleratingstochasticgradientdescentusingpredictivevariancereduction.
InAdvancesinneuralinformationprocessingsystems,pages315–323,2013.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. Proceedings of 3rd
InternationalConferenceonLearningRepresentations,2015.
Jaakko Lehtinen, Jacob Munkberg, Jon Hasselgren, Samuli Laine, Tero Karras, Miika Aittala, and Timo
Aila. Noise2noise: Learning image restoration without clean data. arXiv preprint arXiv:1803.04189,
2018.
9JiamingLiu,YuSun,XiaojianXu,andUlugbekSKamilov. Imagerestorationusingtotalvariationregular-
ized deep image prior. In ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and
SignalProcessing(ICASSP),pages7715–7719.Ieee,2019.
Jiaming Liu, Yu Sun, Cihat Eldeniz, Weijie Gan, Hongyu An, and Ulugbek S Kamilov. Rare: Image
reconstructionusingdeeppriorslearnedwithoutgroundtruth. IEEEJournalofSelectedTopicsinSignal
Processing,14(6):1088–1099,2020.
Gary Mataev, Peyman Milanfar, and Michael Elad. Deepred: Deep image prior powered by red. In Pro-
ceedingsoftheIEEE/CVFInternationalConferenceonComputerVisionWorkshops,pages0–0,2019.
PerlaMayo,MatteoCencini,CarolinMPirkl,MarionIMenzel,MichelaTosetti,BjoernHMenze,andMo-
hammadGolbabaee. Stodip: Efficient3dmrfimagereconstructionwithdeepimagepriorsandstochastic
iterations.InInternationalWorkshoponMachineLearninginMedicalImaging,pages128–137.Springer,
2024.
M.Pilanci andM. J.Wainwright. Randomizedsketchesof convexprograms withsharp guarantees. Infor-
mationTheory,IEEETransactionson,61(9):5096–5115,2015.
Mert Pilanci and Martin J Wainwright. Newton sketch: A near linear-time optimization algorithm with
linear-quadraticconvergence. SIAMJournalonOptimization,27(1):205–245,2017.
OlafRonneberger,PhilippFischer,andThomasBrox. U-net: Convolutionalnetworksforbiomedicalimage
segmentation. InNassirNavab,JoachimHornegger,WilliamM.Wells,andAlejandroF.Frangi,editors,
MedicalImageComputingandComputer-AssistedIntervention–MICCAI2015,pages234–241,Cham,
2015.SpringerInternationalPublishing. ISBN978-3-319-24574-4.
Ernest Ryu, Jialin Liu, Sicheng Wang, Xiaohan Chen, Zhangyang Wang, and Wotao Yin. Plug-and-play
methods provably converge with properly trained denoisers. In International Conference on Machine
Learning,pages5546–5557,2019.
Je´re´myScanvic,MikeDavies,PatriceAbry,andJulia´nTachella. Self-supervisedlearningforimagesuper-
resolutionanddeblurring. arXivpreprintarXiv:2312.11232,2023.
ImrajSingh,RiccardoBarbano,ZeljkoKereta,BangtiJin,KrisThielemans,andSimonArridge. 3dpet-dip
reconstructionwithrelativedifferencepriorusingasirf-basedobjective. Fully3D,2023.
Yu Sun, Brendt Wohlberg, and Ulugbek S Kamilov. An online plug-and-play algorithm for regularized
imagereconstruction. IEEETransactionsonComputationalImaging,2019.
Julia´nTachella,JunqiTang,andMikeDavies. Theneuraltangentlinkbetweencnndenoisersandnon-local
filters. IEEE/CVFConferenceonComputerVisionandPatternRecognition,2021.
Julia´n Tachella, Dongdong Chen, and Mike Davies. Sensing theorems for unsupervised learning in linear
inverseproblems. JournalofMachineLearningResearch,24(39):1–45,2023.
Julia´n Tachella, Mike Davies, and Laurent Jacques. Unsure: Unknown noise level stein’s unbiased risk
estimator. arXivpreprintarXiv:2409.01985,2024.
HongYeTan,SubhadipMukherjee,JunqiTang,andCarola-BibianeScho¨nlieb. Provablyconvergentplug-
and-playquasi-newtonmethods. SIAMJournalonImagingSciences,17(2):785–819,2024.
10JunqiTang,MohammadGolbabaee,andMikeEDavies. Gradientprojectioniterativesketchforlarge-scale
constrainedleast-squares. InInternationalConferenceonMachineLearning,pages3377–3386.PMLR,
2017.
Junqi Tang, Karen Egiazarian, Mohammad Golbabaee, and Mike Davies. The practicality of stochastic
optimizationinimaginginverseproblems. IEEETransactionsonComputationalImaging,6:1471–1485,
2020.
TomTirer,RajaGiryes,SeYoungChun,andYoninaCEldar. Deepinternallearning: Deeplearningfroma
singleinput. IEEESignalProcessingMagazine,41(4):40–57,2024.
Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Deep image prior. In Proceedings of the IEEE
ConferenceonComputerVisionandPatternRecognition,pages9446–9454,2018.
AndrewWangandMikeDavies. Fullyunsuperviseddynamicmrireconstructionviadiffeo-temporalequiv-
ariance. arXivpreprintarXiv:2410.08646,2024a.
Andrew Wang and Mike Davies. Perspective-equivariant imaging: an unsupervised framework for multi-
spectralpansharpening. arXivpreprintarXiv:2403.09327,2024b.
11