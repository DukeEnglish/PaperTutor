End-to-End Navigation with Vision-Language
Models: Transforming Spatial Reasoning into
Question-Answering
DylanGoetting
UniversityofCaliforniaBerkeley
dylangoetting@berkeley.edu
HimanshuGauravSingh AntonioLoquercio
UniversityofCaliforniaBerkeley UniversityofPennsylvania
himanshu singh@berkeley.edu aloque@seas.upenn.edu
Abstract: We present VLMnav, an embodied framework to transform a Vision-
LanguageModel(VLM)intoanend-to-endnavigationpolicy. Incontrasttoprior
work, wedonotrelyonaseparationbetweenperception, planning, andcontrol;
instead, we use a VLM to directly select actions in one step. Surprisingly, we
findthataVLMcanbeusedasanend-to-endpolicyzero-shot, i.e., withoutany
fine-tuningorexposuretonavigationdata. Thismakesourapproachopen-ended
andgeneralizabletoanydownstreamnavigationtask. Werunanextensivestudy
toevaluatetheperformanceofourapproachincomparisontobaselineprompting
methods. In addition, we perform a design analysis to understand the most im-
pactfuldesigndecisions. Visualexamplesandcodeforourprojectcanbefound
atjirl-upenn.github.io/VLMnav/.
Keywords: Navigation,VLM,EmbodiedAI,Exploration
1 Introduction
Theabilitytonavigateeffectivelywithinanenvironmenttoachieveagoalisahallmarkofphysical
intelligence. Spatial memory, along with more advanced forms of spatial cognition, is believed to
havebegunevolvingearlyinthehistoryoflandanimalsandadvancedvertebrates, likelybetween
400 and 200 million years ago [1]. Because this ability has evolved over such a long period, it
feelsalmostinstinctualandtrivialtohumans. However,navigationis,inreality,ahighlycomplex
problem. Itrequiresthecoordinationoflow-levelplanningtoavoidobstaclesalongsidehigh-level
reasoningtointerprettheenvironment’ssemanticsandexplorethedirectionsthataremostlikelyto
gettheagenttoachievetheirgoals.
A significant portion of the navigation problem appears to involve cognitive processes similar to
thoserequiredforansweringlong-contextimageandvideoquestions,anareawherecontemporary
vision-language models (VLMs) excel [2, 3]. However, when naively applied to navigation tasks,
these models face clear limitations. Specifically, when given a task description concatenated with
anobservation-actionhistory,VLMsoftenstruggletoproducefine-grainedspatialoutputstoavoid
obstaclesandfailtoeffectivelyutilizetheirlong-contextreasoningcapabilitiestosupporteffective
navigation[4,5,6].
4202
voN
8
]OR.sc[
1v55750.1142:viXraFigure 1: ThefullactionpromptforVLMnavconsistsofthreeparts: Asystemprompttodescribetheem-
bodiment,anactionprompttodescribethetask,thepotentialactions,andtheoutputinstruction,andanimage
promptshowingthecurrentobservationalongwiththeannotatedactions
Toaddressthesechallenges,previousworkhasincludedVLMsasacomponentofamodularsystem
toperformhigh-levelreasoningandrecognitiontasks. Thesystemsgenerallycontainanexplicit3D
mappingmoduleandaplannertodealwiththemoreembodiedpartofthetask,e.g.,motionandex-
ploration[7,8,9,10,11]. Whilemodularityhastheadvantageofutilizingeachcomponentonlyfor
thesub-taskitexcelsat,itcomesatthedisadvantageofsystemcomplexityandtaskspecialization.
Inthiswork,weshowthatanoff-the-shelfVLMcanbeusedasazero-shotandend-to-endlanguage-
conditioned navigation policy. The key idea to achieve this goal is transforming the navigation
problemintosomethingVLMsexcelat: answeringaquestionaboutanimage.
Todoso,wedevelopanovelpromptingstrategythatenablesVLMstoexplicitlyconsidertheprob-
lem of exploration and obstacle avoidance. This prompting is general, in the sense that it can be
usedforanyvision-basednavigationtask.
Compared to prior approaches, we do not employ modality-specific experts [12, 10, 13], do not
trainanydomain-specificmodels[14,15]anddonotassumeaccesstoprobabilitiesfromthemodels
[12,10].
Weevaluateourapproachonestablishedbenchmarksforembodiednavigation[16,17], wherere-
sultsconfirmthatourmethodsignificantlyimprovesnavigationperformancecomparedtoexisting
promptingmethods. Finally,wedrawdesigninsightsfromablationexperimentsoverseveralcom-
ponentsofourembodiedVLMframework.
2 RelatedWork
Themostcommonapproachforlearninganend-to-endnavigationpolicyinvolvestrainingamodel
fromscratchusingofflinedatasets[18,19,20,21,22]. However,collectinglarge-scalenavigation
dataischallenging,andasaresult,thesemodelsoftenstruggletogeneralizetonoveltasksorout-
of-distributionenvironments.
An alternative approach to enhance generalization is fine-tuning existing vision-language models
(VLMs)withrobot-specificdata[23,24,7,14]. Althoughthismethodcanleadtomorerobustend-
to-end policies, fine-tuning may destroy features not present in the fine-tuning dataset, ultimately
limitingthemodel’sgeneralizationability.
An alternate line of work focuses on using these models zero-shot [11, 25, 10, 13, 12, 9, 5], by
prompting them such that the responses align with task specifications. For instance, [9, 20] use
CLIP or DETIC features to align visual observations to language goals, build a semantic map of
2Figure 2: Approach: Ourmethodismadeupoffourkeycomponents: (i)Navigability, whichdetermines
locationstheagentcanactuallymoveto,andupdatesthevoxelmapaccordingly. Anexampleupdatestepto
themapshowsthemarkingofnewareaasexplored(gray)orunexplored(green). (ii)ActionProposer,which
refinesasetoffinalactionsaccordingtospacingandexploration.(iii)Projection,whichvisuallyannotatesthe
imagewithactions.(iv)Prompting,whichconstructsadetailedchain-of-thoughtprompttoselectanaction.
the environment, and use traditional methods for planning. Other works design specific modules
to handle the task of exploration [13, 12, 11, 26]. These systems often require an estimation of
confidence to know when to stop exploring, which is commonly done using token or object prob-
abilities [12, 10]. In addition, many of these approaches also use low-level navigation modules,
whichabstractawaytheactionchoicestoapre-trainedpoint-to-pointpolicysuchastheFastMarch-
ingMethod[20,9,13,11,10].
VisualPromptingMethods: Toenhancethetask-specificperformanceofVLMs,recentworkhas
involved physically modifying images before passing them to the VLM. Examples include [27],
which annotates images to help recognize spatial concepts. [28] introduces set-of-mark, which
assigns unique labels to objects in an image and references these labels in the textual prompt to
the VLM. This visual enhancement significantly improves performance on tasks requiring visual
grounding. Building on this, [29, 30] apply similar visual prompting methods to the task of web
navigationandshowVLMsareabletocompletesuchtaskszeroshot.
Prompting VLMs for Embodied Navigation: CoNVOI [31] overlays numerical markers on an
image and prompts the VLM to output a sequence of these markers in alignment with contextual
cues (e.g., stay on the pavement), which is used as a navigation path. Unlike our work, they (i)
relyonalow-levelplannerforobstacleavoidanceratherthanusingtheVLM’soutputsdirectlyas
navigational actions, and (ii) do not leverage the VLM to guide the agent toward a specific goal
location. PIVOT [5], introduces a visual prompting method that is most similar to ours. They
approach the navigation problem by representing one-step actions as arrows pointing to labeled
circles on an image. At each step, actions are sampled from an isotropic Gaussian distribution,
withthemeanandvarianceiterativelyupdatedbasedonfeedbackfromtheVLM.Thefinalaction
is selected after refining the distribution. While PIVOT is capable of handling various real-world
navigationandmanipulationtasks,ithastwosignificantdrawbacks:(i)itdoesnotincorporatedepth
informationtoassessthefeasibilityofactionproposals,leadingtolessefficientmovement;and(ii)
it requires many VLM calls to select a single action, resulting in higher computational costs and
latency.
33 Overview
We present VLMnav, designed as a navigation system that takes as input goal G, which can be
specifiedinlanguageoranimage,RGB-DimageI,poseξ,andsubsequentlyoutputsactiona. The
action spaceconsists ofrotation aboutthe yawaxis anddisplacement along thefrontal axisin the
robotframe,whichallowsallactionstobeexpressedinpolarcoordinates.AsitisknownthatVLMs
struggle to reason about continuous coordinates [6], we instead transform the navigation problem
intotheselectionofanactionfromadiscretesetofoptions[28]. Ourcoreideaistochoosethese
actionoptionsinawaythatavoidsobstaclecollisionsandpromotesexploration.
Figure2summarizesourapproach. Westartbydeterminingthenavigabilityofthelocalregionby
estimatingthedistancetoobstaclesusingadepthimage(Sec.3.1). Similarto[20,13,12,31,9,10,
26]weusethedepthimageandposeinformationtomaintainatop-downvoxelmapofthescene,
andnotablymarkvoxelsasexploredorunexplored. SuchamapisusedbyanActionProposer(Sec.
3.2)todetermineasetofactionsthatavoidobstaclesandpromoteexploration. Wethenprojectthis
setofpossibleactionstothefirst-person-viewRGBimagewiththeProjection(Sec.3.3)component.
Finally,theVLMtakesasinputthisimageandacarefullycraftedprompt,describedinSec.3.4,to
select an action, which the agent executes. To determine episode termination, we use a separate
VLMcall,detailedinSec. 3.5.
3.1 Navigability
Usingadepthimage,wecomputeanavigabil-
itymaskthatcontainsthesetofpixelsthatcan
be reached by the robot without crashing into
anyobstacles.
Next, for all directions θ ∈ fov, we use
the navigability mask to calculate the farthest
straight-linedistancerthattheagentcantravel
withoutcolliding. Thiscreatesasetofactions
A that are collision-free. Figure 3 illus-
initial
trates an example calculation of the mask and
navigableactions.
At the same time, we use the depth image and
theposeinformationtobuilda2Dvoxelmapof
Figure3: AnexamplestepoftheNavigabilitysubrou-
theenvironment. Allobservableareaswithin2 tine. Thenavigabilitymaskisshowninblueandpolar
metersoftheagentaremarkedasexplored,and actionsmakingupA areingreen
initial
theonesbeyondasunexplored.
3.2 ActionProposer
WedesigntheActionProposer routinetorefineA → A , anactionsetthatisinterpretable
initial final
for the VLM and promotes exploration. Taking advantage of the information accumulated in our
voxelmap,welookateachactionanddefineanexplorationindicatorvariablee as
i
(cid:26)
1 ifregion(θ ,r )isunexplored
e = i i
i 0 ifregion(θ ,r )isexplored
i i
To build A , we need to prioritize unexplored actions, and also ensure there is enough visual
final
spacingbetweenactionsfortheVLMtodiscern. WestartbyaddingunexploredactionstoA if
final
anangularspacingofθ ismaintained.
δ
A ←A ∪{(θ ,r )|e =1and|θ −θ |≥θ ,∀(θ ,r )∈A }
final final i i i i j δ j j final
4To sufficiently cover all directions but still maintain an exploration bias, we supplement A by
final
addingexploredactionssubjecttoalargerangularspacingofθ >θ :
∆ δ
A ←A ∪{(θ ,r )|e =0and|θ −θ |≥θ ,∀(θ ,r )∈A }
final final i i i i j ∆ j j final
Lastly,wewanttoensuretheseactionsdon’tmovetheagenttooclosetoobstacles,soweclip
2
r ←min( ·r , r ) ∀(θ ,r )∈A
i 3 i max i i final
Occasionally,theagentcangetstuckinacornerwheretherearenonavigableactions(A =∅).
initial
To address this, we add a special action (π,0), which rotates the agent by 180°. This also allows
efficiententry/exitofroomswheretheagentquicklyidentifiesthatthegoalisnotinthatroom.
TheproposedsetA nowhasthreeimportantproperties:(i)actionscorrespondtonavigablepaths,
final
(ii) there is sufficient visual spacing between actions, and (iii) there is an engineered bias towards
exploration. Wecallthisapproachtoexplorationexplorebias.
3.3 Projection
Visually grounding these actions in a space the VLM can understand and reason about is the next
step. TheProjectioncomponenttakesinA from3.2andRGBimageI, andoutputsannotated
final
imageIˆ.Similarlyto[5],eachactionisassignedanumberandoverlayedontotheimage.Weassign
thespecialrotationactionwith0andannotateitontothesideoftheimagealongwithalabelTurn
Around. Wefindthatvisuallyannotatingit,insteadofjustdescribingitinthetextualprompt,helps
grounditsprobabilityofbeingchosentothatoftheotheractions.
3.4 Prompting
Toelicitafinalaction,wecraftadetailedtextualpromptT,whichisfedintotheVLMalongwithIˆ.
Thispromptprimarilydescribesthedetailsofthetask,thenavigationgoal,andhowtointerpretthe
visualannotations. Additionally, weaskthemodeltodescribethespatiallayoutoftheimageand
tomakeahigh-levelplanbeforechoosingtheaction,whichservestoimprovereasoningqualityas
foundby[32,33]. Forimage-basednavigationgoals,thegoalimageissimplypassedintotheVLM
inadditiontoT andIˆ. ThefullpromptcanbefoundinFigure1.
TheactionchosenbytheVLM,P (a∗|Iˆ,T)∈A isthendirectlyexecutedintheenvironment.
vlm final
Notably,thisdoesnotinvolveanylow-levelobstacleavoidancepolicyasinotherworks[20,13,9,
10,11].
3.5 Termination
Figure4: Theseparatepromptfordeterminingepisodetermination
To complete a navigation task, the agent must terminate the episode by calling special action stop
withina thresholddistanceof thegoalobject. Compared tootherapproaches thatleveragea low-
levelnavigationpolicy[20,13,9,10,11],ourmethoddoesnotexplicitlychooseatargetcoordinate
5locationtonavigateto,andthereforewefaceanadditionalchallengeofdeterminingwhentostop.
OursolutionistouseaseparateVLMpromptthatexplicitlyaskswhetherornottostop,whichis
showninFigure4. Wedothisfortworeasons:
1. Annotations:ThearrowsandcirclesfromSec.3.3introducenoiseandcluttertotheimage,
makingitmoredifficulttounderstand.
2. Separationoftasks. Toavoidanytaskinterference,theactioncallisonlyconcernedwith
navigatingandthestoppingcallisonlyconcernedwithstopping.
To avoid terminating the episode too far away from the object, we terminate the episode when
the VLM calls stop two times in a row. After the VLM calls stop the first time, we turn off the
navigability and explore bias components to ensure the agent doesn’t move away from the goal
object.
4 Experiments
We evaluate our approach on two popular embodied navigation benchmarks, ObjectNav [34] and
GoatBench [17], which use scenes from the Habitat-Matterport 3D dataset [35, 36]. Further, we
analyzehowtheperformanceofanend-to-endVLMagentchangeswithvariationsindesignparam-
eterssuchasfield-of-view,lengthofthecontextualhistoryusedtopromptthemodel,andqualityof
depthperception.
Setup: Similar to [16], the agent adopts a cylindrical body of radius 0.17m and height 1.5m. We
equip the agent with an egocentric RGB-D sensor with resolution (1080, 1920) and a horizontal
field-of-view(FOV)of131◦. Thecameraistilteddownwithapitchof25◦ similarto[12], which
helps determine navigability. We use Gemini Flash as the VLM for all our experiments, given its
lowcostandhigheffectiveness.
Metrics:Asinpriorwork[17,16,37],weusethefollowingmetrics:(i)SuccessRate(SR):fraction
episodesthataresuccessfullycompleted(ii)SuccessRateWeightedbyInversePathLength(SPL):
ameasureofpathefficiency.
Baselines: WeusePIVOT[5]asabaselineasitismostsimilartoours. Toinvestigatetheimpactof
ouractionselectionmethod,weablateitbyevaluatingOursw/onav: thesameasoursbutwithout
theNavigabilityandActionProposer components. Theactionchoicesforthisbaselineareastatic
set of evenly-spaced action choices, including the turn around action. Notably, these actions do
not consider navigability or exploration. To further evaluate the impact of visual annotation, we
also evaluate a baseline Prompt Only, which sees actions described in text (“turn around”, “turn
right”,“moveforward”,...) butnotannotatedvisually. Thesedifferentpromptingbaselinescanbe
visualizedinFig5.
Figure 5: Baselines: Comparingthefourdifferentmethodsonasampleimage. Ourscontainsarrowsthat
point to navigable locations, PIVOT has arrows sampled from a random 2-D Gaussian, Ours w/o nav sees
uniformlyspacedarrows(notearrows3and5pointintoawall),andPromptOnlyseesjusttherawRGBimage
Wenotethatinourexperimentsandbaselines,weturntheallow slideparameteron,whichallows
the agent to slide against obstacles in the simulator. Our experiments show that removing this
assumptionleadstolargedropsinperformance.
64.1 ObjectNav
The Habitat ObjectNav benchmark requires navigation to an object instance from one of six cate-
gories[Sofa,Toilet,TV,Plant,Chair,Bed]. Asin[16],togettheoptimalpathlength,wetakethe
minimumoftheshortestpathstoallinstancesoftheobject. Theseexperimentsareevaluatedwitha
successthresholdof1.2meters[13].
Run SR SPL
Ours 50.4% 0.210 Table 1: ObjectNav Results. We evaluate four different
prompting strategies on the ObjectNav benchmark, and see
Oursw/onav 33.2% 0.136
our method achieves highest performance in both accuracy
PromptOnly 29.8% 0.107
(SR)andefficiency(SPL).Ablatingtheallow slideparame-
PIVOT[5] 24.6% 0.106
tershowsourmethodisdependentonslidingpastobstacles
Oursw/osliding 12.9% 0.063
Table1summarizesourresults. OurmethodoutperformsPIVOTbyover25%,andnearlydoubles
itsnavigationefficiencyintermsofSPL.Weseethatouractionselectionmethodishighlyeffective
as shows a 17% improvement over Ours w/o nav. Removing visual annotations leads to a slight
decrease in success rate but a significant reduction in SPL, indicating that visual grounding is im-
portantfornavigationefficiency. Interestingly,wefindthatPIVOTperformsworsethanbothofour
ablations. We attribute this to limited expressivity in its action space, which prevents it from exe-
cutinglargerotationsorturningaroundfully. Thisoftenleadstotheagentgettingstuckincorners,
hinderingitsabilitytorecoverandnavigateeffectively.
Wenotethatdisablingslidingresultsinalargedropinperformance,signalingthatwhileeffective
insimulation,ourmethodwouldlikelyleadtocollisionswithobstaclesintherealworld. Whileour
Navigabilitymodulecanidentifynavigablelocations,itdoesnotconsiderthespecificsizeandshape
oftherobotinthiscalculation,leadingtooccasionalcollisionswheretheagentgetsstucksincewe
lackanexplicitactiontobacktrackpreviousmotions.
4.2 GoToAnythingBenchmark(GOAT)
GOAT Bench [17] is a recent benchmark that establishes a higher level of navigation difficulty.
Eachepisodecontains5-10sub-tasksacrossthreedifferentgoalmodalities: (i)Objectnames,such
asrefrigerator,(ii)Objectimages,and(iii)DetailedtextdescriptionssuchasGreycouchlocatedon
theleftsideoftheroom,nexttothepictureandthepillow. Table2showsourresults,evaluatedon
thevalunseensplit.
Run SR SPL ImageSR ObjectSR DescriptionSR
Ours 16.3% 0.066 14.3% 20.5% 13.4%
Oursw/onav 11.8% 0.054 7.8% 16.5% 10.2%
PromptOnly 11.3% 0.037 7.7% 15.6% 10.1%
PIVOT[5] 8.3% 0.038 7.0% 11.3% 5.9%
Table 2: GOAT Results. Comparison of prompting strategies on GOAT Bench, a more
challenging navigation task. Across three different goal modalities, our method strongly
outperformsbaselinemethods
Across all goal modalities, our model achieves significant improvements over baselines. These
improvements are especially evident in image goals, where our model achieves nearly twice the
successrateofallbaselinemethods. Thishighlightstherobustnessandgeneralnatureofoursys-
tem. AswiththeObjectNavresults,Oursw/onavandPromptonlyperformcomparable,andboth
outperform PIVOT. For all prompting methods, the image and description modalities prove more
challengingthantheobjectmodality,similarlytowhatwasfoundby[17].
Comparisontostate-of-the-art:Weturntheallow slideparameteroffandcomparetotwostate-of-
the-artspecializedapproaches:(i)SenseAct-NN[17]isapolicytrainedwithreinforcementlearning,
using learned submodules for different skills; and (ii) Modular GOAT [20] is a compound system
that builds a semantic memory map of the environment and uses a low-level policy to navigate to
7objects within this map. Unlike SenseAct-NN, our work is zero-shot, and unlike Modular GOAT,
wedonotrelyonalow-levelpolicyoraseparateobject-detectionmodule.
Table 3: Directlycomparingtootherworks,we
Run SR SPL
seethatspecializedsystemsstillproducesuperior
SenseAct-NNSkillChain 29.5% 0.113
performance. Wealsonotetheseotherworksuse
ModularGOAT 24.9% 0.172
a narrower FOV, lower image resolution, and a
Oursw/sliding 16.3% 0.066 differentactionspace,whichcouldexplainsome
Ours 6.9% 0.049 ofthedifferences
We compare the results of our approach to these baselines in Table 3. Interestingly, these meth-
ods have different strengths: a reinforcement learning approach leads to the highest success rate.
Conversely,themodularnavigationsystemachievesthehighestnavigationefficiency.
Ourmethodshowslowerperformancecomparedtothesespecializedbaselinesacrossbothmetrics,
evenwhenpermittedtoslideoverobstacles.Notably,weobservethatin13.9%oftheruns,theVLM
prematurely calls stop when it is between 1 to 1.5 meters from the target object. These instances
areclassifiedasfailures,asthebenchmarkdefinesarunassuccessfulonlyiftheagentiswithin1
meter of the object. This finding suggests that our VLM lacks the fine-grained spatial awareness
necessary to accurately assess distances to objects. However, it also indicates that in over 30% of
the runs, our VLM agent is able to approach the goal object closely, highlighting its capability to
reachnear-targetpositions.
Asshowninpreviousexperiments, whennotallowedtoslideoverobjects, ourapproach’sperfor-
mance drastically decreases, as it gets frequently blocked between obstacles and does not have a
waytobacktrackitsactions.
4.3 ExploringthedesignspaceofVLMagentsfornavigation
In this section, we look at major design choices that impact the navigation ability of VLM-based
agentsinoursetup,allevaluatedontheObjectNavdataset.
4.3.1 HowimportantiscameraFOVfornavigation?
Figure6:ImpactofsensorFOVs.WeevaluatetheperformanceoffourdifferentsensorFOVs,and
findthatawiderFOVinvariablyleadstohigherperformance
Anagent’snavigationabilitieslargelydependonhowfine-graineditsvisionis. Inthissection,we
study whether our VLM agent can benefit from high-resolution images. Specifically, we run our
methodusingfourdifferentFOVs: 82◦ [16],100◦,115◦ and131◦ (iPhone0.5camera). Theresults
ofthisexperiment,showninFig.6,indicatepositivescalingbehaviorsonbothnavigationaccuracy
andefficiency.
84.3.2 Dolongerobservation-actionhistorieshelp?
Inthissection,westudywhetheraVLMnavigationagentcaneffectivelyuseahistoryofobserva-
tions. We create a prompt containing the observation history in a naive way, i.e., we concatenate
observationsandactionsfromtheK mostrecentenvironmentstepsandfeedthisintotheVLMas
context. For all these experiments, we remove our exploration bias (see Sec. 3.2) to specifically
isolatethecontributionofalongerhistory.
HistoryLength SR SPL
Table 4: Impact of adding context history. We compare
Nohistory 46.8% 0.193
ourmethodtoalternativesofkeepingthepast0,5,10,and15
5 42.7% 0.180 observationsandactions. Weseethataddingcontexthistory
10 45.4% 0.196 doesnotimprovetheperformanceofourmethod
15 40.4% 0.170
The results of these experiments are shown in Table 4. We find that when naively concatenating
past observations and actions, our prompt strategy is unable to use a longer context. Indeed, the
performanceremainsthesameordecreaseswhenincreasingthehistorylength.
4.3.3 Howimportantisperfectdepthperception?
Within the simulator, the depth sensor provides accurate pixel-wise depth information, which is
important for determining the navigability mask. To investigate the importance of quasi-perfect
depth perception, we evaluate two alternate approaches that only use RGB: (i) Segformer, which
uses [38] to semantically segment pixels belonging to the floor region. We use this region as the
navigability mask and bypass the need for any depth information. We estimate the distances to
obstacles by multiplying the number of pixels with a constant factor. (ii) ZoeDepth, which uses
[39] to estimate metric depth values. We use such predicted values instead of the ground-truth
distancesfromthesimulatorandcomputenavigabilityintheoriginalway.
Run SR SPL Table 5: DepthAblation. Weevaluatetwoalternate
approachesthatonlyrequireRGB.Wefindthatseman-
Depthsensor 50.4% 0.210
tic segmentation performs close to using ground truth
Segformer[38] 47.2% 0.183
depth, whereasestimatingdepthvaluesleadstoasig-
ZoeDepth[39] 39.1% 0.161
nificantperformancedrop
TheresultsofthisstudyarepresentedinTable5. Wefindthatdepthestimationfrom[39]isnotac-
curateenoughtoidentifynavigableareas. Indeed,depthnoiseleadstoa10%dropinSR.However,
usingasegmentationmaskinsteadofrelyingondepthinformationsurprisinglyprovestobequite
effective, with only a decrease of 3% with respect to using perfect depth perception. Overall, our
experimentsshowthataVLM-basednavigationagentcanperformwellwithonlyRGBinformation.
5 Conclusion
Inthiswork,wepresentVLMnav,anovelvisualprompt-engineeringapproachthatenablesanoff-
the-shelfVLMtoactasanend-to-endnavigationpolicy. Themainideabehindthisapproachisto
carefullyselectactionproposalsandprojectthemonanimage,effectivelytransformingtheproblem
of navigation into one of question-answering. Through evaluations on the ObjectNav and GOAT
benchmarks, we see significant performance gains over the iterative baseline PIVOT, which was
thepreviousstate-of-the-artinpromptengineeringforvisualnavigation. Ourdesignstudyfurther
highlightstheimportanceofawidefieldofviewandthepossibilityofdeployingourapproachwith
minimalsensing,i.e.,onlyanRGBimage.
Our method has a few limitations. The drastic decrease in performance from disabling the al-
low slideparameterindicatesthatthereareseveralcollisionswithobstacles,whichcouldbeprob-
lematic in a real-world deployment. In addition, we find that specialized systems such as [17]
outperformourwork. However, asthecapabilitiesofVLMscontinuetoimprove, wehypothesize
thatourapproachcouldhelpfutureVLMsreachorsurpasstheperformanceofspecializedsystems
forembodiedtasks.
9References
[1] R.N.MuzioandV.P.Bingman.BrainandSpatialCognitioninAmphibians:StemAdaptations
intheEvolutionofTetrapodCognition,page105–124. CambridgeUniversityPress,2022.
[2] OpenAI,J.Achiam, S.Adler, S.Agarwal, L.Ahmad, andI.A.etal. Gpt-4technicalreport,
2024. URLhttps://arxiv.org/abs/2303.08774.
[3] G. Team, P. Georgiev, V. I. Lei, R. Burnell, L. Bai, and A. G. et al. Gemini 1.5: Unlocking
multimodalunderstandingacrossmillionsoftokensofcontext,2024. URLhttps://arxiv.
org/abs/2403.05530.
[4] S. K. Ramakrishnan, E. Wijmans, P. Kraehenbuehl, and V. Koltun. Does spatial cognition
emergeinfrontiermodels?,2024. URLhttps://arxiv.org/abs/2410.06468.
[5] S.Nasiriany,F.Xia,W.Yu,T.Xiao,J.Liang,I.Dasgupta,A.Xie,D.Driess,A.Wahid,Z.Xu,
Q.Vuong,T.Zhang,T.-W.E.Lee,K.-H.Lee,P.Xu,S.Kirmani,Y.Zhu,A.Zeng,K.Hausman,
N.Heess,C.Finn,S.Levine,andB.Ichter. Pivot: Iterativevisualpromptingelicitsactionable
knowledgeforvlms,2024.
[6] P. Rahmanzadehgervi, L. Bolton, M. R. Taesiri, and A. T. Nguyen. Vision language models
areblind,2024. URLhttps://arxiv.org/abs/2407.06581.
[7] M.J.Kim,K.Pertsch,S.Karamcheti,T.Xiao,A.Balakrishna,S.Nair,R.Rafailov,E.Foster,
G. Lam, P. Sanketi, et al. Openvla: An open-source vision-language-action model. arXiv
preprintarXiv:2406.09246,2024.
[8] A. Majumdar, G.Aggarwal, B.Devnani, J.Hoffman, and D.Batra. Zson: Zero-shot object-
goalnavigationusingmultimodalgoalembeddings. AdvancesinNeuralInformationProcess-
ingSystems,35:32340–32352,2022.
[9] S. Y. Gadre, M. Wortsman, G. Ilharco, L. Schmidt, and S. Song. Cows on pasture: Base-
lines and benchmarks for language-driven zero-shot object navigation. In Proceedings of
theIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages23171–23181,
2023.
[10] B. Yu, H. Kasaei, and M. Cao. L3mvn: Leveraging large language models for visual target
navigation.InInternationalConferenceonIntelligentRobotsandSystems(IROS).IEEE,2023.
[11] Y.Kuang,H.Lin,andM.Jiang.Openfmnav:Towardsopen-setzero-shotobjectnavigationvia
vision-languagefoundationmodels,2024. URLhttps://arxiv.org/abs/2402.10670.
[12] A.Z.Ren,J.Clark,A.Dixit,M.Itkina,A.Majumdar,andD.Sadigh. Exploreuntilconfident:
Efficientexplorationforembodiedquestionanswering. InarXivpreprintarXiv:2403.15941,
2024.
[13] D.Shah,M.Equi,B.Osinski,F.Xia,B.Ichter,andS.Levine. Navigationwithlargelanguage
models: Semanticguessworkasaheuristicforplanning. In7thAnnualConferenceonRobot
Learning,2023. URLhttps://openreview.net/forum?id=PsV65r0itpo.
[14] J. Zhang, K. Wang, R. Xu, G. Zhou, Y. Hong, X. Fang, Q. Wu, Z. Zhang, and H. Wang.
Navid:Video-basedvlmplansthenextstepforvision-and-languagenavigation.arXivpreprint
arXiv:2402.15852,2024.
[15] K.Ehsani,T.Gupta,R.Hendrix,J.Salvador,K.-H.Z.LucaWeihs,Y.K.KunalPratapSingh,
W. Han, A. Herrasti, R. Krishna, D. Schwenk, E. VanderBilt, and A. Kembhavi. Imitating
shortest paths in simulation enables effective navigation and manipulation in the real world.
arXiv,2023.
10[16] K.Yadav,S.K.Ramakrishnan,J.Turner,A.Gokaslan,O.Maksymets,R.Jain,R.Ramrakhya,
A. X. Chang, A. Clegg, M. Savva, E. Undersander, D. S. Chaplot, and D. Batra. Habitat
challenge2022. https://aihabitat.org/challenge/2022/,2022.
[17] M. Khanna*, R. Ramrakhya*, G. Chhablani, S. Yenamandra, T. Gervet, M. Chang, Z. Kira,
D.S.Chaplot,D.Batra,andR.Mottaghi. Goat-bench: Abenchmarkformulti-modallifelong
navigation,2024.
[18] D. Shah, B. Eysenbach, G. Kahn, N. Rhinehart, and S. Levine. Ving: Learning open-world
navigationwithvisualgoals.In2021IEEEInternationalConferenceonRoboticsandAutoma-
tion(ICRA),pages13215–13222,2021. doi:10.1109/ICRA48506.2021.9561936.
[19] D. Shah, B. Eysenbach, G. Kahn, N. Rhinehart, and S. Levine. Rapid exploration for open-
world navigation with latent goal models, 2023. URL https://arxiv.org/abs/2104.
05859.
[20] M. Chang, T. Gervet, M. Khanna, S. Yenamandra, D. Shah, S. Y. Min, K. Shah, C. Paxton,
S.Gupta,D.Batra,etal. Goat: Gotoanything. arXivpreprintarXiv:2311.06430,2023.
[21] D.Shah,A.Sridhar,N.Dashora,K.Stachowicz,K.Black,N.Hirose,andS.Levine. Vint: A
foundationmodelforvisualnavigation,2023.URLhttps://arxiv.org/abs/2306.14846.
[22] D.Shah,A.Sridhar,A.Bhorkar,N.Hirose,andS.Levine. Gnm: Ageneralnavigationmodel
todriveanyrobot,2023. URLhttps://arxiv.org/abs/2210.03370.
[23] A.Brohan,N.Brown,J.Carbajal,Y.Chebotar,J.Dabis,C.Finn,K.Gopalakrishnan,K.Haus-
man,A.Herzog,J.Hsu,etal. Rt-1:Roboticstransformerforreal-worldcontrolatscale. arXiv
preprintarXiv:2212.06817,2022.
[24] A.Brohan,N.Brown,J.Carbajal,Y.Chebotar,X.Chen,K.Choromanski,T.Ding,D.Driess,
A. Dubey, C. Finn, et al. Rt-2: Vision-language-action models transfer web knowledge to
roboticcontrol. arXivpreprintarXiv:2307.15818,2023.
[25] G.Zhou,Y.Hong,andQ.Wu. Navgpt: Explicitreasoninginvision-and-languagenavigation
withlargelanguagemodels,2023. URLhttps://arxiv.org/abs/2305.16986.
[26] A.Topiwala,P.Inani,andA.Kathpal. Frontierbasedexplorationforautonomousrobot,2018.
URLhttps://arxiv.org/abs/1806.03581.
[27] A.Shtedritski,C.Rupprecht,andA.Vedaldi. Whatdoesclipknowaboutaredcircle? visual
promptengineeringforvlms,2023. URLhttps://arxiv.org/abs/2304.06712.
[28] J.Yang,H.Zhang,F.Li,X.Zou,C.Li,andJ.Gao. Set-of-markpromptingunleashesextraor-
dinaryvisualgroundingingpt-4v. arXivpreprintarXiv:2310.11441,2023.
[29] J.Y.Koh,R.Lo,L.Jang,V.Duvvur,M.C.Lim,P.-Y.Huang,G.Neubig,S.Zhou,R.Salakhut-
dinov, and D. Fried. Visualwebarena: Evaluating multimodal agents on realistic visual web
tasks,2024. URLhttps://arxiv.org/abs/2401.13649.
[30] A.Yan,Z.Yang,W.Zhu,K.Lin,L.Li,J.Wang,J.Yang,Y.Zhong,J.McAuley,J.Gao,Z.Liu,
andL.Wang. Gpt-4vinwonderland: Largemultimodalmodelsforzero-shotsmartphonegui
navigation,2023. URLhttps://arxiv.org/abs/2311.07562.
[31] A.J.Sathyamoorthy,K.Weerakoon,M.Elnoor,A.Zore,B.Ichter,F.Xia,J.Tan,W.Yu,and
D.Manocha. Convoi: Context-awarenavigationusingvisionlanguagemodelsinoutdoorand
indoorenvironments,2024. URLhttps://arxiv.org/abs/2403.15637.
[32] J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. Chi, Q. Le, and D. Zhou.
Chain-of-thought prompting elicits reasoning in large language models, 2023. URL https:
//arxiv.org/abs/2201.11903.
11[33] T.Kojima,S.S.Gu,M.Reid,Y.Matsuo,andY.Iwasawa.Largelanguagemodelsarezero-shot
reasoners. Advancesinneuralinformationprocessingsystems,35:22199–22213,2022.
[34] D.Batra,A.Gokaslan,A.Kembhavi,O.Maksymets,R.Mottaghi,M.Savva,A.Toshev,and
E. Wijmans. Objectnav revisited: On evaluation of embodied agents navigating to objects,
2020. URLhttps://arxiv.org/abs/2006.13171.
[35] K.Yadav,R.Ramrakhya,S.K.Ramakrishnan,T.Gervet,J.Turner,A.Gokaslan,N.Maestre,
A. X. Chang, D. Batra, M. Savva, A. W. Clegg, and D. S. Chaplot. Habitat-matterport 3d
semanticsdataset,2023. URLhttps://arxiv.org/abs/2210.05633.
[36] M.Savva,A.Kadian,O.Maksymets,Y.Zhao,E.Wijmans,B.Jain,J.Straub,J.Liu,V.Koltun,
J.Malik,D.Parikh,andD.Batra. Habitat: Aplatformforembodiedairesearch,2019. URL
https://arxiv.org/abs/1904.01201.
[37] P.Anderson,A.Chang,D.S.Chaplot,A.Dosovitskiy,S.Gupta,V.Koltun,J.Kosecka,J.Ma-
lik, R. Mottaghi, M. Savva, and A. R. Zamir. On evaluation of embodied navigation agents,
2018. URLhttps://arxiv.org/abs/1807.06757.
[38] E.Xie,W.Wang,Z.Yu,A.Anandkumar,J.M.Alvarez,andP.Luo. Segformer: Simpleand
efficient design for semantic segmentation with transformers, 2021. URL https://arxiv.
org/abs/2105.15203.
[39] S. F. Bhat, R. Birkl, D. Wofk, P. Wonka, and M. Mu¨ller. Zoedepth: Zero-shot transfer by
combiningrelativeandmetricdepth,2023. URLhttps://arxiv.org/abs/2302.12288.
12