The 2024 IEEE International Conference on Big Data (IEEE BigData 2024)
FisherMask: Enhancing Neural Network Labeling
Efficiency in Image Classification Using Fisher Information
Shreen Gul†, Mohamed Elmahallawy‡, Sanjay Madria†, Ardhendu Tripathy†
†Computer Science Department, Missouri University of Science and Technology, Rolla, MO 65401, USA
‡ School of Engineering & Applied Sciences, Washington State University, Richland, WA 99354, USA
Emails: sgchr@mst.edu, mohamed.elmahallawy@wsu.edu, madrias@mst.edu, astripathy@mst.edu
Abstract—Deeplearning(DL)modelsarepopularacrossvari- onFisherinformation,whichmeasureshowmuchinformation
ousdomainsduetotheirremarkableperformanceandefficiency. an observable random variable reveals about an unknown
However, their effectiveness relies heavily on large amounts of
parameter in a distribution. Fisher information is an effective
labeleddata,whichareoftentime-consumingandlabor-intensive
method due to its independence from ground truth values
togeneratemanually.Toovercomethischallenge,itisessentialto
develop strategies that reduce reliance on extensive labeled data and its always semi-definite nature. These properties make it
while preserving model performance. In this paper, we propose advantageous in various applications, including optimization,
FisherMask,aFisherinformation-basedactivelearning(AL)ap- control theory, and machine learning [9].
proach that identifies key network parameters by masking them
Different query strategies use this Fisher information mea-
based on their Fisher information values. FisherMask enhances
batch AL by using Fisher information to select the most critical sure in various ways: some focus on selecting uncertain
parameters, allowing the identification of the most impactful samples, while others prioritize diversity in sample selec-
samples during AL training. Moreover, Fisher information pos- tion. Some approaches approximate Fisher information values
sesses favorable statistical properties, offering valuable insights
through trace operations. For instance, the method introduced
into model behavior and providing a better understanding of
in[6],knownasBAIT(BatchActivelearningviaInformation
the performance characteristics within the AL pipeline. Our
extensiveexperimentsdemonstratethatFisherMasksignificantly maTrices), employs a Fisher-based greedy approach. This
outperformsstate-of-the-artmethodsondiversedatasets,includ- method selects samples by minimizing an objective func-
ing CIFAR-10 and FashionMNIST, especially under imbalanced tion that incorporates approximations of Fisher information
settings.Theseimprovementsleadtosubstantialgainsinlabeling
matrices and their inverses. Another work presented in [10]
efficiency. Hence serving as an effective tool to measure the
proposes training the network by updating only a subset of
sensitivity of model parameters to data samples. Our code is
available on https://github.com/sgchr273/FisherMask. parameters rather than all of them. They reported an approxi-
Index Terms—Data labeling, active learning, information ma- mationofparameterimportancebased ontheaveragesquared
trix, Fisher information gradients of the model’s output. This approximation helps
quantify the significance of each parameter. In another work,
I. INTRODUCTION the authors of [11] noted that higher layers of deep networks
Deep learning (DL) networks are increasingly integrated are better at generating discriminative features compared to
into numerous fields due to their remarkable performance lower layers. Furthermore, [12] showed that deeper layers
and accuracy. However, their efficacy heavily relies on la- capture more complex aspects of the target function. These
beled/annotateddata.Manualannotationofdataisoftencostly, insights motivate our proposed approach, which leverages
promptingagrowingdemandfortechniquescapableofachiev- the discriminative power of upper layers to enhance feature
ing high performance with limited labeled data [1]. Active learning from the dataset. Incorporating these layers into our
learning (AL) emerges as one such strategy, exploiting infor- process could potentially lead to better and more informative
mative samples to train models and thereby diminishing the samples.
necessity for additional data, thus mitigating the demand for Contributions. Motivated by the work of [10], we develop
more annotated data [2]. Scenarios such as medical imaging, a method called FisherMask for constructing a sparse net-
speech recognition, and anomaly detection tasks can greatly work mask. FisherMask aims to leverage Fisher information
benefit from AL [3]–[5]. to capture crucial details about unlabeled data samples. As
AL approaches, such as those proposed by [6]–[8], have illustratedinFig.1,wecomputetheFisherinformationmatrix
been developed to gauge the informativeness of data samples for the entire network and use it to create this mask, which
and select them for model training. These methods utilize is why we refer to it as FisherMask. This mask is formed
information-theoretic measures like Fisher information, en- by selecting k weights with the highest Fisher information
tropy, and Kullback-Leibler divergence to assess the signifi- values.Tospeedupcomputations,weapproximatetheupdates
cance of samples within a dataset [7]. This work will center to the Fisher information matrices and their inverses using
1
4202
voN
8
]GL.sc[
1v25750.1142:viXrathe Woodbury identity and trace rotation techniques, similar example, [16] introduces a hybrid approach called BADGE
to those used in BAIT [6]. FisherMask specifically utilizes (Batch Active learning by Diverse Gradient Embeddings).
the information from the network’s middle layers to identify This method evaluates uncertainty by measuring the gradient
influential samples. To sum up, our contributions are: length concerning the network’s last-layer parameters while
(1) We propose FisherMask, a novel method for constructing ensuring diversity through k-means++ clustering. BADGE
a sparse network mask based on Fisher information. This effectively leverages data embeddings, which is advantageous
method leverages important weights to capture critical whenfeaturelearningisakeybenefitofdeepneuralnetworks
details about unlabeled data samples by selecting the [19].
k weights with the highest Fisher information values,
III. PRELIMINARIES
specifically for large datasets with limited labels.
(2) To enhance computational efficiency, we approximate up- A. AL via information matrices
datestotheFisherinformationmatricesandtheirinverses The process selects the next optimal sample to include in
usingtheWoodburyidentityandtracerotationtechniques. the labeled set by optimizing the objective function such as
This approach leverages information from the network’s theonein[6].Specifically,thisfunctionaimstomaximizethe
middle layers to effectively identify influential samples. potential information by informativeness of the newly labeled
(3) Our performance evaluations on a range of diverse and sample, considering both the current model uncertainty and
publicly available datasets highlight the effectiveness and the diversity of the data. For reference, the objective function
model-agnostic nature of FisherMask. Additionally, the can be given by:
results show that FisherMask achieves performance that
O(x)=argmaxtr(cid:0) VTM−1FLM−1V A−1(cid:1) , (1)
is comparable to or exceeds that of existing methods, x i i x
x
particularly in scenarios with imbalanced datasets.
whereV isthematrixofgradientsofthemodel’spredictions
x
with respect to the parameters, and M is the Fisher informa-
II. RELATEDWORK i
tion matrix of the labeled samples, given by:
ALencompassesarangeoftechniquesaimedatmakingthe
1 (cid:88)
training of machine learning (ML) models more efficient by M =λF + F(x;θL), (2)
i |C|
strategically selecting which data points to label. These tech-
x∈C
niques generally fall into two primary categories: uncertainty
whereC isthesetofselectedsamples,andλisaregularization
samplinganddiversitysampling.Belowisadetailedoverview
parameter. A is an adjustment to the equation and reflects
of each approach.
not only the general Fisher information but also how the
Uncertainty-basedapproaches.Theyseektoreducelabel-
sample-specific gradients impact the model’s uncertainty or
ingeffortsbyfocusingonsampleswherethemodelismostun-
informationdensityintheparameterspace,whichcanbegiven
certain. Key techniques in this area include entropy sampling,
as:
marginsampling,andmutualinformation[13].However,each
A=F +VTM−1V , (3)
of these methods has its limitations. For example, entropy x i x
samplingcanoverlooktheinterrelationshipsbetweensamples, where F is the Fisher information matrix and VTM−1V
x i x
leading to the selection of redundant data. Similarly, mutual represents the contribution of the gradients. FL denotes the
information, while theoretically informative, often involves Fisher Information matrix of the last layer of the network,
high computational complexity, making it less practical for which can be expressed as:
high-dimensional datasets [14].
FL =E (cid:2) ∇2l(x,y;θL)(cid:3) (4)
Diversity-based approaches They focus on selecting sam- y∼p(x|θL)
ples that effectively represent the overall distribution/diversity Here θL denotes the weights of the last layer of the network
oftheentiredataset.Somepopularapproachesinthiscategory and l(x,y;θL) is the loss function.
include k-means sampling [15], k-means++ [16], and k-center
greedy (also known as coreset) [17]. However, each of these B. Entropy sampling
approaches comes with its own limitations. For instance, the
InAL,entropysamplingisusedtoselectthemostuncertain
k-center greedy method, while useful for identifying diverse
data points for labeling. The uncertainty is quantified by the
samples, often faces significant computational challenges and
entropy of the model’s predicted probability distribution. The
ends up taking a lot more computational time than any other
data points with the highest entropy are considered the most
AL strategy. The process involves constructing a distance informative and thus prioritized for labeling. The entropy H
matrix for each unlabeled sample, which can be resource-
of a data point is calculated using the formula:
intensive [18].
N
Some studies combine both model uncertainty and dataset (cid:88)
H =− p log (p ), (5)
diversity to select the most informative samples for AL. For i 2 i
i=1Fig. 1: Illustration of important weights sampling. Hollow circles represent the set of unlabeled samples S fed into the neural
network. Colored arrows depict the process of identifying important weights while pruning the remaining ones. Based on the
selectedweights,asubsetofunlabeledinstancesC (coloredcircles)ischosenforlabeling.Thissubsetisthensenttoanoracle
for labeling, after which the model will be trained on this newly labeled data, shown in the lower-left portion of the figure,
completing one AL round.
where N is the number of classes in the dataset, and p is the point x and its nearest center x [20]. Mathematically, the
i i j
probability of the data sample belonging to class i. Higher problem can be formulated as:
entropy values indicate greater uncertainty in the model’s (cid:18) (cid:19)
predictions,makingsuchsamplesmorevaluableforimproving min max min∆(x i,x j) (7)
S1:|S1|≤b i j
the model’s performance through AL.
where S1 is the set of selected center points with at most b
elements, x represents a data point in the set, x represents a
i j
C. Margin Sampling
center point from the selected set S1, and ∆(x ,x ) denotes
i j
Margin Sampling is a method used to select data samples the distance between data point x i and center point x j.
where the model’s prediction is the least confident. This is
IV. METHODOLOGY
achieved by focusing on the margin between the probabilities
A. Problem Statement
of the top two predicted classes.
In this work, we aim to optimize the training process of
M(x)=|p (x)−p (x)| (6) an ML model f using a dataset S = {x 1,x 2,...x T}. The
max second
objectives are twofold:
where M(x) denotes the margin of the model’s prediction for 1) Identify the most important network parameters: We
sample x, which quantify how close the model’s prediction is calculate the Fisher information matrix (FIM) for the
tobeinguncertain,withasmallermarginindicatinghigherun- model’s parameters θ. The FIM is defined as:
certainty. p max(x) is the probability score of the top predicted FIM = 1 (cid:80)T E (∇ logp (y|x ))2, (8)
class for sample x, and p (x) is the probability score of i T i=1 y∼pθ(y|xi) θ θ i
second
the second most probable class for sample x. whereFIM i measuresthesensitivityofthelog-likelihood
withrespecttothemodelparametersθ .Byanalyzingthe
i
FIM, we identify which parameters have the greatest im-
D. k-center greedy
pact on the model’s performance and learning dynamics.
Inthisalgorithm,bpointsareselectedfromasetS ascenter 2) Determine the Most Influential Samples: We evaluate
points to minimize the maximum distance between any data which samples in S are most influential for the model.This involves assessing the contribution of each sample Algorithm 1: FisherMask Training Process
to the overall learning process, which can be guided by Input Model f(x;θ), pool of unlabeled examples S, AL
metrics such as gradient norms, influence functions, or rounds R, sparsity parameter k
other techniques that measure the impact of individual Output Learned model θ
R
samples on the model’s parameters. 1: Initialize set C of points by selecting N o labeled
samples from S and fit the model on C:
B. Notations
θ =argmin E [l(x,y;θ)]
initial θ S
We focus on the standard batch AL scenario involving
2: for r = 1, 2, ..., R do
Dthe ins (t xa )nc oe fsp tha ece laX b, et lhe spl aa cb eel gs ip va ec ne aY n, a in nd put the x.dis Wtr eibu ht aio vn
e
3: Calculate F θr = |S1 |(cid:80) x∈SF(x;θ r)
wac hY c ie| cX s hs wto ea cs ae nt o sef lu ecn tl ia vb ee lyled red qa ut ea stS a= b{ ax
tc1
h,x
o2
f,. N.., dx
aT
ta}, pofr io nm
ts
4 5:
:
F Ini il tt ie ar liF zeθr Mby
o
={θ λ|F Fθr +≥ |C1s |or (cid:80)t( xF ∈θ Cr) Fk}
(x;θ r)
6: for n = 1, 2, 3, ..., N do
f Cor =labe {li xn (g p. )}I Nn the ofp- Nth sA aL mpc ly escle a, nw de res qe ule ec st
t
a thec io rlle lac bti eo ln
s
7: x˜=argmintr(cid:0) (M i+F(x;θ r))−1F θr(cid:1)
k k=1 x∈S
y(p) ∼ D (x(p)) from the oracle. Our primary objective 8: M i+1 ←M i+F(x˜;θ r),C ←x˜
Y|X k
is to minimize the following loss function as: 9: end for
10: Train model on C :θ r =argmin θE S[l(x,y;θ)]
L (θ)=E [l(x,y;θ∗)], (9)
S x∼S,y∼DY|X(x) 11: end for
where S is the set of unlabeled data, θ∗ denotes the learned
model parameters, and l(x,y;θ∗) is the loss function associ-
ated with the model’s prediction for input x and true label y. After applying the linear algebra techniques, an approximate
Thegoalistoachievethiswiththefewestpossibledatapoints. solution to the optimization problem defined in (10), through
Inthiscontext,wetreattheunlabeleddataS asrepresentative which we select the next best sample, is given by:
of the entire distribution and utilize the FIM to perform AL
argmaxtr(cid:0) VTM−1F M−1V A−1(cid:1) (12)
on the given unlabeled set S. x i θ i x
x
From (8), it is evident that a particular element in the FIM
whereF representstheFisherinformationofcrucialparame-
θ
represents the average of the squared gradients of the net-
ters {θ|F ≥sort(F ) } with k signifies the level of sparsity
work’spredictionsy concerningitsparametersθ.Specifically, θ θ k
for the selection of important weights. We tested multiple
if a parameter significantly influences the model’s output, its
sparsitylevelsforconstructingtheFisherMask,including0.01,
corresponding element in the FIM will be large. Therefore,
0.005, 0.002, and 0.001. Our experiments demonstrated that a
Fisher information can be effectively used to measure the
sparsity level of 0.002 was optimal, yielding the best model
importance of the network’s parameters.
performance. These sparsity levels were chosen relative to
Motivating by [21], we pose our objective function as
the total parameter count in our model architecture, specif-
follows:
ically 11 million parameters in the case of ResNet-18. To
x˜=argmintr(cid:0) (M +F(x;θ ))−1F (cid:1) (10) derive the solution to Eqn. (10), we employed the substitution
i r θr
x∈S F(x;θ )=V VT,whichreducestheneedtostoreallupdates
r x x
whereθ r representssparselyselectedweightsinr thALround. of F(x;θ) and improves the computation cost. The invertible
F exθ pr reis ssF ei dsh ae sr i Fn θfo rrm =ati T1on (cid:80)o
T
if =u 1n Ela yb ∼e pl θe (d y|s xa im )(p ∇le θs r, low gh pic θh (yc |a xn i)b )2e Hm ea rt eri ,x
V
xA is=
a
mF a+ trixV x oT fM sii z− e1V Rx mnis ×nof cod nim tae inn is nio gn grA adi∈ entR sn w× in th.
with M i represents the i-th labeled sample to be in- each column scaled by the square root of the corresponding
cluded in the collection C and is given by M i = λF + prediction.
|C1 |(cid:80) x∈CF(x;θ r). InFig.2,weillustratetheproportionofparametersdeemed
We employ the Woodbury identity and trace rotation for significant across the 61 layers of the ResNet18 model. The
inverse updates, a technique analogous to that used in [6], to analysis reveals that weights from the initial 10 layers are
approximate the expression in (10). The algebraic expressions consistently identified as significant. However, between layers
for these updates can be provided as 15 and 35, fewer parameters are deemed important by the
x˜=argmintr(cid:0) (M +V VT)−1F (cid:1) (11) algorithm. Notably, there are spikes in selection frequency
x
i x x θr
in the later stages of the model, particularly from layer 35
=argmintr(cid:0) (M−1−M−1V A−1VTM−1)F (cid:1) onwards. The substantial spike at the 61st layer indicates that
i i x i θr
x asignificantfractionofweightsfromthefinallayerhavebeen
=argmintr(cid:0) M−1F (cid:1) −tr(cid:0) M−1V A−1VTM−1F (cid:1)
i θr i x x i θr chosen for constructing the FisherMask.
x
=argmintr(cid:0) M−1F (cid:1) −tr(cid:0) VTM−1F M−1V A−1(cid:1) OurapproachdiffersfromBAIT[6]inhowweutilizeFisher
x
i θr x i θr i x
informationmatricestochoosetheoptimaldatapoints.UnlikeFig. 2: Profile of important weights across Resnet-18
Fig. 3: Overview of FisherMask’s framework.
BAIT,whichreliessolelyonFisherinformationmatricesfrom
the last layer of the network, we consider weights across
intermediate layers of the network, as illustrated in Fig. 2. the tenth class has 5,000 samples. These settings are designed
The pseudocode for our proposed strategy is provided in to simulate different class distributions and assess algorithm
Algorithm 1. The Algorithm involves utilizing a classifier performanceundervaryinglevelsofclassimbalance,reflecting
f along with an unlabeled set S. Initially, the model is real-world scenarios where such imbalances are common.
trained on randomly selected samples C as indicated in line TrainingModel.WeemploytheResNet-18architecturefor
1. Subsequently, the AL process begins, where each cycle our experiments, implemented using the PyTorch framework.
includes the calculation of Fisher Information values for both The ResNet-18 model consists of four residual blocks, each
thechosensamplesM andtheremainingunlabeledpoolF,as containing two convolutional layers followed by Batch Nor-
outlinedinlines3and4.ThesecomputedvaluesM andF are malizationlayers.Specifically,eachblockincludesasequence
then used in Equation 10 to determine the optimal samples. of four layers: Conv2d, BatchNorm2d, ReLU, and Conv2d,
InFig.3,wepresenttheoverallmethodologyforselectinga repeated consistently across all layers. After these blocks, a
singledatasample.Adatapointx fromthesamplespaceS is Fully Connected layer is applied at the end of the network to
i
fedtothemodeltoobtainaprobabilityvectorviathesoftmax perform classification. This structure ensures that the model
layer. The FIM is calculated from this probability vector, and benefits from deep residual learning while maintaining a
thetopkparameterswiththelargestFisherinformationvalues manageable level of complexity.
are selected. Using the objective function (Equation 12), the We use the Adam optimizer with a learning rate of 0.001.
next most influential sample is chosen to be added to the Additionally, we apply image preprocessing techniques such
labeleddatasetC.Themodelisthentrainedonthesetofthese asRandomCrop,HorizontalFlip,andNormalizationtotheraw
queried samples (x ,y ). This process of selecting a batch of images.
i i
queried points is repeated a fixed number of times, and the Baselines. We consider four baselines to compare with our
cycle continues until the stopping criteria, such as the label proposed approach, FisherMask, as described below:
budget, are met. • Random Sampling [16]. Certain points are chosen in a
naive manner and added to the unlabeled dataset.
V. RESULTSANDDISCUSSION
• Entropy Sampling [24]. A traditional AL approach that
Datasets. We utilize two datasets in our experiments: selects unlabeled instances with the highest entropy.
CIFAR-10 [22] and FashionMNIST [23]. CIFAR-10 consists • BAIT Sampling [6]. Fisher-based active selection
ofRGBimagesofsize32×32with50,000trainingand10,000 method that selects batches of samples by optimizing a
test images, while FashionMNIST contains grayscale images bound on the MLE (Maximum Likelihood Estimation)
of 24×24 with 60,000 training and 10,000 test images. Two error in terms of the last layer of Fisher information
experimental settings are used to evaluate the algorithms. In matrices.
Setting 1, the first four classes from each dataset are selected • Margin Sampling [13]. A technique used to select
withsamplesizesof250,5,000,250,and250,respectively.In samplesbasedontheminimaldifferencebetweenthetop
Setting 2, the first nine classes each have 250 samples, while two predictions for each class.its superior performance1.
In Fig. 4b, we observe the performance in a high-data
regime, where the entire unlabeled dataset is utilized by the
end of the AL rounds. The process starts with 250 randomly
chosen samples and incrementally adds 250 more until all
5,750 samples from the imbalanced CIFAR-10 dataset are
used. The graph shows that all algorithms begin with an aver-
ageaccuracyofapproximately45%andimprovetoabout70%
bytheendofthecycles.FisherMaskconsistentlyoutperforms
margin sampling, k-center greedy, and BAIT. Entropy sam-
pling, however, maintains performance comparable to Fisher-
Mask throughout the AL process. Random sampling shows
initially lower performance but steadily improves, eventually
(a)Low.
surpassing its initial accuracy by the end of the cycles.
We also examined both the low and high data regimes on
the FashionMNIST dataset, which exhibited similar trends to
those observed with the CIFAR-10 dataset.
2) Setting #2. In this scenario, samples are selected from a
different set of classes. Specifically, 250 samples are chosen
from each of the first nine classes, while the tenth class
contains5,000samplesforbothdatasets.Thissetupsimulates
real-world situations where one class significantly outweighs
the others, leading to potential bias in an ML model towards
thepredominantclassandresultinginskeweddecision-making
due to its overrepresentation.
Fig. 5a displays the average accuracy curves for the imbal-
(b)High.
ancedCIFAR-10datasetunderthissetting.Abatchsizeof500
Fig. 4: Data regime for imbalanced CIFAR10.
is used until the budget of 6,500 is exhausted. All algorithms
start with an accuracy of approximately 30% and achieve
around60%meanaccuracybytheendoftheALcycles.Fish-
• K-center Greedy [20]. An approach that chooses k erMaskconsistentlyoutperformstheothermethodsthroughout
samplesbysolvingak-centerproblemonz wherez is the experiment. BAIT shows a notable improvement around
x x
the embedding of x derived from the penultimate layer. the midpoint but underperforms in the latter stages. Entropy
and Margin sampling exhibit alternating performance relative
Experimental Results. to each other. K-center Greedy initially lags but ultimately
1) Setting #1. In a scenario with limited/low data avail- surpasses Random sampling and BAIT by the end of the
ability, as depicted in Fig. 4a, we evaluate various approaches cycles. Random sampling remains the least effective strategy
using a subset of 575 points from a pool of 5750 samples to throughout the experiment.
address two primary challenges. The first challenge involves Similarly,inFig.5b,wepresenttheresultsfortheFashion-
animbalanceddataset,characterizedbyunevendistributionof MNISTdatasetunderthisimbalanceddatasetting.Aswiththe
samples across different classes. The second challenge relates previous scenario, 500 samples are added to the training data
to the limited sample size, where only 10% of the entire ineachALrounduntilthebudgetof6,500isreached.Alltech-
imbalanced set is utilized. While entropy generally remains niques start with an average accuracy of approximately 52%
below BAIT for a significant portion of the graph, it shows a andachievearound83%accuracybytheendofthecycles.The
slightincreasetowardstheendoftheALrounds.TheRandom increase in accuracy across all strategies can be attributed to
Sampling approach consistently performs poorly due to its FashionMNIST’sgrayscaleimages,whicharegenerallyeasier
naive approach of randomly selecting samples. The k-center for models to learn. Consequently, the differences in learning
Greedyapproachinitiallyperformssimilarlytootherstrategies curves among the strategies are minimal.
but steadily improves throughout the graph. Margin Sampling
initially performs worse than Random Sampling but gradually
improves.Notably,FisherMaskconsistentlyoutperformsbase-
1Theshadedregionsonthelearningcurvesrepresentthevariance,providing
linesacrossasubstantialpartoftheplotteddata,highlighting anindicationoftheuncertaintyorspreadintheperformancemeasurements.D-optimality maximizes the determinant of the FIM, thereby
improvingtheprecisionofestimatedparameters.Additionally,
weareconsideringintegratingFIMintothelossfunctionused
during AL, allowing us to reward or penalize the selection of
samples based on their informativeness.
REFERENCES
[1] D. Yuan, X. Chang, Q. Liu, Y. Yang, D. Wang, M. Shu, Z. He, and
G. Shi, “Active learning for deep visual tracking,” IEEE Transactions
onNeuralNetworksandLearningSystems,2023.
[2] G.NémethandT.Matuszka,“Compute-efficientactivelearning,”arXiv
preprintarXiv:2401.07639,2024.
[3] N. Li, Y. Qi, R. Xin, and Z. Zhao, “Ocean data quality assessment
through outlier detection-enhanced active learning,” in 2023 IEEE In-
ternationalConferenceonBigData(BigData),2023,pp.102–107.
[4] Z. Chen, J. Duan, L. Kang, and G. Qiu, “Supervised anomaly de-
(a)CIFAR10.
tection via conditional generative adversarial network and ensemble
active learning,” IEEE Transactions on Pattern Analysis and Machine
Intelligence,vol.45,no.6,pp.7781–7798,2022.
[5] X. Tang, Y. S. Astle, and C. Freeman, “Deep anomaly detection with
ensemble-basedactivelearning,”in2020IEEEInternationalConference
onBigData(BigData). IEEE,2020,pp.1663–1670.
[6] J.Ashetal.,“Gonefishing:Neuralactivelearningwithfisherembed-
dings,”AdvancesinNeuralInformationProcessingSystems,2021.
[7] H. Hino et al., “Active learning by query by committee with robust
divergences,”InformationGeometry,vol.6,no.1,pp.81–106,2023.
[8] X. Li et al., “Unlabeled data selection for active learning in image
classification,”ScientificReports,vol.14,no.1,p.424,2024.
[9] A.Lyetal.,“Atutorialonfisherinformation,”JournalofMathematical
Psychology,vol.80,pp.40–55,2017.
[10] Y.-L.Sungetal.,“Trainingneuralnetworkswithfixedsparsemasks,”
AdvancesinNeuralInformationProcessingSystems,vol.34,2021.
[11] M. D. Zeiler and R. Fergus, “Visualizing and understanding convo-
lutional networks,” in Computer Vision–ECCV 2014: 13th European
(b)FashionMNIST.
Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings,
Fig. 5: Result for Imbalanced datasets. PartI13. Springer,2014,pp.818–833.
[12] Y.Chenetal.,“Whichlayerislearningfaster?asystematicexploration
of layer-wise convergence rate for deep neural networks,” in The
EleventhInternationalConferenceonLearningRepresentations,2022.
VI. CONCLUSIONANDFUTUREWORK [13] D. Wang and Y. Shang, “A new active labeling method for deep
learning,” in 2014 International joint conference on neural networks
In this paper, we proposed FisherMask, a novel technique (IJCNN). IEEE,2014,pp.112–119.
[14] J. Sourati et al., “Asymptotic analysis of objectives based on fisher
that uses a sparse mask of weights to identify the most
information in active learning,” The Journal of Machine Learning
impactful samples based on their Fisher Information values. Research,vol.18,no.1,pp.1123–1163,2017.
By selecting the top parameters across the entire network, [15] F. Zhdanov, “Diverse mini-batch active learning,” arXiv preprint
arXiv:1901.05954,2019.
FisherMask determines which samples to update and use for
[16] J. T. Ash, C. Zhang, A. Krishnamurthy, J. Langford, and A. Agar-
thenextbatchinanALround.Experimentalresultsshowthat wal, “Deep batch active learning by diverse, uncertain gradient lower
FisherMaskperformswellunderlabelsparsityandchallenging bounds,”arXivpreprintarXiv:1906.03671,2019.
[17] O. Sener and S. Savarese, “Active learning for convolutional neural
class imbalances. Both experimental and theoretical analyses
networks:Acore-setapproach,”arXivpreprintarXiv:1708.00489,2017.
demonstrate that our approach outperforms existing baselines, [18] C. Shui and othes, “Deep active learning: Unified and principled
particularly in low-data regimes. For future work, we aim to methodforqueryandtraining,”inInternationalConferenceonArtificial
IntelligenceandStatistics. PMLR,2020,pp.1308–1318.
explore the effectiveness of FisherMask on additional datasets
[19] A.KirschandY.Gal,“Unifyingapproachesinactivelearningandactive
that closely mimic real-world scenarios. sampling via fisher information and information-theoretic quantities,”
In our future work, we plan to leverage the Fisher Infor- TransactionsonMachineLearningResearch,2022.
[20] B.Settles,“Activelearningliteraturesurvey,”2009.
mation Matrix (FIM) to enhance sample selection, as FIM
[21] K. Chaudhuri, S. M. Kakade, P. Netrapalli, and S. Sanghavi, “Con-
captures critical information about model parameters, leading vergence rates of active learning for maximum likelihood estimation,”
tomoreinformedchoices.Byselectingsamplesthatmaximize AdvancesinNeuralInformationProcessingSystems,vol.28,2015.
[22] A. Krizhevsky et al., “Cifar-10 (canadian institute for advanced re-
the expected information gain about these parameters, FIM
search),”URLhttp://www.cs.toronto.edu/kriz/cifar.html,vol.5,2010.
can help identify those samples that provide the most insight [23] H. Xiao, “Fashion-mnist: a novel image dataset for benchmarking
into parameter estimation. This approach aligns with opti- machinelearningalgorithms,”arXivpreprintarXiv:1708.07747,2017.
[24] B.Safaeiandothes,“Entropicopen-setactivelearning,”inProceedings
mal experimental design criteria: for instance, A-optimality
oftheAAAIConferenceonArtificialIntelligence,vol.38,no.5,2024,
minimizes the average variance of parameter estimates, while pp.4686–4694.