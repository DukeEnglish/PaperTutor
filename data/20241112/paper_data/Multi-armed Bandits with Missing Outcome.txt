Multi-armed Bandits with Missing Outcome
Ilia Mahrooghi†1, Mahshad Moradi†1, Sina Akbari2, and Negar Kiyavash2
2EPFL, Switzerland
1
{ilia.mahrooghi2003,mahshadmoradix}@gmail.com
2{sina.akbari,negar.kiyavash}@epfl.ch
Abstract
While significant progress has been made in designing algorithms that minimize regret in online
decision-making, real-world scenarios often introduce additional complexities, perhaps the most chal-
lenging of which is missing outcomes. Overlooking this aspect or simply assuming random missingness
invariably leads to biased estimates of the rewards and may result in linear regret. Despite the practical
relevanceofthischallenge,norigorousmethodologycurrentlyexistsforsystematicallyhandlingmissing-
ness, especially when the missingness mechanism is not random. In this paper, we address this gap in
the context of multi-armed bandits (MAB) with missing outcomes by analyzing the impact of different
missingness mechanisms on achievable regret bounds. We introduce algorithms that account for miss-
ingness under both missing at random (MAR) and missing not at random (MNAR) models. Through
both analytical and simulation studies, we demonstrate the drastic improvements in decision-making by
accounting for missingness in these settings.
1 Introduction
Multi-armed bandit (MAB) algorithms have emerged as indispensable tools for decision-making under un-
certainty, balancing the trade-off between exploring different options and exploiting the best known action.
These algorithms have achieved success in various domains ranging from personalized online advertisement
and recommender systems [22, 37, 3] to clinical trials [36, 2, 33] and adaptive routing in communication
systems [24, 21]. For instance, in online advertising, advertisers need to continuously select which ad to
show to a user to maximize click-through rates. Similarly, in clinical trials, researchers must decide which
treatment to administer to patients to optimize recovery rates. MAB algorithms guide decision-makers in
such scenarios to learn actions that minimize regret.
SignificantprogresshasbeenmadeindevelopingMABalgorithmsthatminimizeregretinvarioussettings
[18, 1, 7, 20, 30]. However, the real world often introduces challenges that deviate from the assumptions of
the classical MAB framework or its current extensions. One of the most critical challenges is that of missing
outcomes–situationswheretheresultsofcertainactionsarenotalwaysobserved. Thischallengearisesmore
oftenthannotinpracticeandcanfundamentallyunderminethedecision-makingprocessifleftunaddressed.
To illustrate this, consider an example of a large-scale clinical trial for a new cancer treatment. Patients
are randomly assigned to different treatment arms, and their health outcomes are monitored over time. In
practice, not all patients will complete the trial. Some may drop out early due to side effects, while others
†Thisworkwascompletedduringtheauthors’tenureasresearchinternsatEPFL.
1
4202
voN
8
]LM.tats[
1v16650.1142:viXramay stop reporting outcomes for personal reasons, and some could pass away during the trial due to reasons
not related to the treatment (competing events). Crucially, the missingness of the outcome may not be
random. Patients experiencing severe side effects or poor health are more likely to drop out, meaning that
themissingnessmechanismiscorrelatedwiththeunobservedoutcomeitself. Thisintroducessystematicbias
into the estimation of the rewards, and if not accounted for, would lead to poor decision-making.
Theissueofmissingnessisnotconfinedtohealthcare. Inarecommendationsystemthatsuggestsarticles
tousersonanonlineplatform,ifuserswhofindthecontentirrelevantarelesslikelytoprovidefeedback(e.g.,
theyleavethesitewithoutinteracting),thesystemcouldoverestimatethevalueoftherecommendedarticles,
assuming that the missing feedback is independent of user satisfaction. Here too, missingness is correlated
with the unobserved outcome, leading to biased reward estimates and sub-optimal recommendations.
Theproblemofmissingdataisafundamentalchallengeincausalinference. Thisissuehasbeenextensively
studied over the past decades, with seminal works such as [28, 23, 4] laying the foundation for dealing with
biased estimations in the presence of missing data. These methods, along with more recent developments in
graphical models for handling missing data [25, 26], have become standard approaches in causal inference.
Missingdatahasalsobeenextensivelyexploredinspecificcontextssuchasinstrumentalvariables,[32,31,16]
andmediationanalysis[38,39,17],amongothers. Bycontrast,thechallengeofmissingoutcomeshasreceived
relatively little attention in multi-armed bandit problems, although some progress has been made in related
areas. For instance, the problem of delayed feedback in bandits bears some similarity to our setting, as
both involve incomplete information at decision time. Several works have addressed stochastic bandits with
unrestricted delays [14, 35], and delays dependent on stochastic rewards [27, 19]. In contextual bandits, [5]
studied linear contextual bandits with missing (restricted) contexts. While this work addresses missing data
in bandits, it focuses on missing contexts rather than outcomes and assumes a linear reward model. Others
have explored bandit problems with variable costs or restricted observations. For example, [10, 29] studied
MAB problems with variable costs, where the outcome is observable only after paying the associated cost.
The closest to our paper are the works [8] and [6] which consider the problem of MAB with missing out-
comes. [8] settles for some empirical considerations and suggestions, without formally studying the problem
or providing tailored algorithms. [6] utilizes ideas from unsupervised learning to impute the missing rewards
in a contextual bandit setting. Both of these work under the assumption that the missingness mechanism
is random, possibly after conditioning on the context. In this paper, we do a thorough study from a formal
perspective,characterizingthebestachievableregretboundsundermultiplescenarioswithmissingoutcomes
at random (MAR) as well as not at random (MNAR) models. We present novel regret lower bounds and
provide algorithms that are guaranteed to achieve optimal regret. Another closely related line of work in the
MAB research involves leveraging auxiliary information to enhance decision-making. Recent studies have
explored the use of correlated auxiliary feedback to improve reward estimates [34], introduced Thompson
Sampling algorithms for contextual bandits with auxiliary safety constraints [9], and investigated adaptive
sequentialexperimentswithdynamicauxiliaryinformationtoimprovetherobustnessofdecision-making[11].
In our work, we will take advantage of auxiliary information to overcome the challenge of missing outcomes.
Addressingtheproblemofmissingoutcomesisbothpracticallyrelevantandtheoreticallychallenging. In
applicationssuchashealthcare, education, ande-commerce, accountingformissingdatacouldleadtobetter
treatment policies, more personalized learning experiences, and more effective product recommendations,
potentially affecting millions of individuals. In this paper, we undertake the first formal study of multi-
armed bandits with missing outcomes and provide tailored algorithms that explicitly handle different types
of missingness. Our main contributions are two-fold. First, we provide an analysis of the impact of missing
outcomesonachievableregret(thelossofoptimality). Second, weintroduceprovablygoodupperconfidence
bound (UCB) algorithms that are tailored to handle both missing at random and missing not at random
mechanisms. Our algorithms are designed to adjust reward estimates based on the observed data and the
missingness mechanism, ensuring unbiased estimation. Finally, we extend our analysis to settings where
2not only outcomes but also mediators (e.g., users providing feedback) are prone to missingness, to further
broaden the applicability of our approach.
Theremainderofthispaperisstructuredasfollows. InSection2,wereviewtherelevantbackgroundand
formalizetheproblemofmulti-armedbanditswithmissingoutcomes. InSection3wepresentouralgorithms
in the settings of MCAR, MAR, and MNAR, respectively. Additionally, we provide the corresponding
achievable regret lower bounds. The technical proofs are postponed to Appendix B. In Section 4, we extend
ourapproachandpresentalgorithmsforthecasewhenthemediatorisalsopronetomissingness. Adiscussion
of the limitations of our work and our concluding remarks appear in Section 6.
2 Formalization and Problem Setup
Webeginbyreviewingtheclassicmulti-armedbandit(MAB)setupandthenextendittoincorporatemissing
outcomes. The MAB problem involves an agent (decision-maker) who interacts with an environment over
a sequence of T time steps. At each time step t ∈ {1,...,T}, the agent pulls an arm a from a set of n
t
available actions indexed by A = {1,...,n}. Upon pulling this arm, the agent receives a stochastic reward
Y ∈ Y drawn from a fixed (but unknown) probability distribution associated with arm a . The goal of
t t
the agent is to minimize the cumulative regret over the time horizon T, which is defined as the cumulative
difference between the rewards of the optimal arm and the chosen arms. Specifically, let µ :=E[Y |A=a]
a
for every a ∈ A. The optimal arm, denoted by a∗, is the arm that maximizes the expected reward, i.e.,
a∗ :=argmax µ . The regret at time t is defined as R :=µ −E[Y |A=a ], and the cumulative regret
a∈A a t a∗ t
over T rounds, denoted by R , is the sum of the latter instantaneous regrets over the horizon T:
T
T T
(cid:88) (cid:88)
R := (µ −E[Y |A=a ])=Tµ − E[Y |A=a ]. (1)
T a∗ t a∗ t
t=1 t=1
In the classical setting, it is assumed that after pulling an arm a , the agent always observes the true reward
t
Y without any missingness.
t
WeextendtheclassicMABmodeltoaccommodatemissingness. Weassumethatpullingeacharma ∈A,
t
drawsastochastictuple(cid:0) Y ,OY,M ,OM(cid:1) fromafixedbutunknownprobabilitydistributionassociatedwith
t t t t
arm a . In this tuple, Y ∈ Y represents the true reward (as before), whereas OY ∈ {0,1} is an indicator
t t t
denoting whether this reward is observed. M ∈ M is a possible mediator or an auxiliary variable1, with
t
OM ∈{0,1} indicating whether this auxiliary variable is observed. For example in online recommendations,
t
auxiliary information could include metrics such as the time a user spends on a webpage before navigating
away, or other data points gathered from browser cookies, such as past browsing behavior, device type, or
location. The agent has access to the ‘observed’ tuple (cid:0) Yo,OY,Mo,OM(cid:1) , where the observed values Yo and
t t t t t
Mo are defined as follows:
t
(cid:40) (cid:40)
Y ; if OY =1, M ; if OM =1,
Yo = t t ,Mo = t t , (2)
t ?; o.w. t ?; o.w.
where ? denotes a missing value. We define µ as the expected value of Y given A =a as before, with the
a t t
crucial difference that samples of Y are missing when OY =0.
t t
Clearly, without imposing further structure, it is not possible to construct unbiased estimators for the
expected rewards of each arm. In fact, these expectations are not ‘identifiable,’ meaning that they are not
1Theintroductionofthisauxiliaryvariableiswithoutlossofgenerality,asitcanbesimplysettoM ≡0,i.e.,adegenerate
variablethatcarriesnoinformation.
3OY OY OY OY
M M Y M
A Y A Y A M A Y
(a) MCAR (b) MAR (i) (c) MAR (ii) (d) MNAR
Figure 1: Graphical representations of the missing data mechanisms considered in this paper.
uniquely determinable functionals of the probability measure over observable variables. In what follows, we
begin with the case where the mediator is fully observed (OM =1 with probability 1). We first consider the
t
case where the missingness mechanism of the outcome is independent of everything else. Subsequently, we
analyze the more realistic cases where this missingness is correlated with the missing outcome Y . Later in
t
Section 4 we extend our findings further to the case where even the mediator is prone to missingness.
3 MAB with Missing Outcome
Throughout, we assume that the outcomes are not ‘always missing.’
Assumption 1 (Positivity). For every action a∈A, P(cid:0) OY =1|M ,A =a(cid:1) >0. Moreover, P(M |A ) is
t t t t t
positive everywhere2.
Assumption 1 is reasonable as otherwise there exists an arm for which the agent observes no reward
samples. For the rest of this section, we assume that the auxiliary variable M is always observed.
t
3.1 Missing Completely At Random (MCAR)
We begin with the case where the outcome missingness mechanism is independent of the other variables
(including the outcome itself).
Assumption 2 (MCAR). The outcome is missing completely at random. That is, OY ⊥⊥ (A ,Y ,M ) for
t t t t
t∈{1,...,T}.
This assumption holds, for instance, when data gets erased by say an independent mechanism such as
a power outage. The graph of Figure 1a represents this missingness mechanism, whereby the missingness
indicator OY is an isolated node. As there is no information conveyed by the missingness indicator, the
missing chunk of the data can be discarded without any need for extra care. As such, the classic upper
confidence bound (UCB) algorithms are expected to achieve (near-) optimal regret. We formalize these
claims through the next two propositions. For the sake of completeness, we have included the adapted UCB
algorithm (Alg. 1) for this scenario in Appendix A.
Theorem 1. (MCAR regret guarantee) Under Assumption 1, for every α > 1, the cumulative regret of the
adapted UCB (Alg. 1) is bounded as follows:
(cid:32)(cid:115) (cid:33)
αnT log(T)
E[R ]=O .
T γ
2With sufficient caution, the second part of this assumption could be omitted. However, we include it here for the sake of
simplicityinthepresentation.
4The proof of Theorem 1, which provides a regret bound similar to that of the classic UCB algorithm,
but adapted to our setting, is included in Appendix B provided in the supplementary. The following result
indicates that this regret bound is (near-)optimal.
Theorem 2. (Minimax lower bound for MCAR) For any policy π, there exists an MCAR instance ν s.t.
(cid:32)(cid:115) (cid:33)
nT
E[R (π,ν)]=Ω ,
T γ
where E[R (π,ν)] represents the expected regret of policy π in instance ν.
T
See Appendix B for the proof of Theorem 2 as well as the rest of the results of this paper.
3.2 Missing At Random (MAR)
We now focus on more realistic settings where the missingness mechanism provides information about the
missing outcomes. This is the case for instance when the unsatisfied customers are more likely to leave
comments on an online platform, or in health applications, the patients with severe side effects are more
likely to drop out of the study. We first consider the case when missingness is at random, i.e., independent
of Y given M and A. The graphs of Figure 1b and Figure 1c illustrate two possible representations of the
MAR mechanism, under which Assumption 3 holds.
Assumption 3 (MAR). OY ⊥⊥Y |(A ,M ) for t∈{1,...,T}.
t t t t
Under assumption 3, the expected reward is identifiable as follows:
µ =E[Y |A =a]=E(cid:2)E[Y |M,a]|A =a(cid:3)
a t t t t
( =a)E(cid:2)E[Y |M,a,OY =1]|A =a(cid:3) (3)
t t t
( =b)E(cid:2)E[Yo |M,a,OY =1]|A =a(cid:3) ,
t t t
where (a) and (b) follow from Assumption 3 and Equation (2), respectively. Accordingly, we will use the
following estimator for µ :
a
µˆ = 1 (cid:88) (cid:0) (cid:88) 1{M t =m} (cid:88) Yo(cid:1) , (4)
a |T | |T | t′
a m,a,o
t∈Ta m∈M t′∈Tm,a,o
where T ,T ⊆ {1,...,T} are the set of iterations where A = a, and iterations where A = a, M = m
a m,a,o t t t
and OY = 1, respectively. In what follows, for brevity, we define p := P(M = m | A = a). We first
t m,a t t
present an algorithm for minimizing regret when the conditional probabilities p are known. We then
m,a
adaptouralgorithmtothecase wheretheseprobabilitiesareunknown. Recallthat n=|A|isthenumberof
arms. We assume that E[Y |m,a]∈[0,1] for all arms and that the reward Y is sub-Gaussian. Algorithm 2
t t
presents the pseudo-code for the first case. The algorithm is based on UCB, with the difference that at the
beginning, the agent pulls every arm log(T)2 times uniformly. At the subsequent rounds, both the expected
rewards and the associated confidence bands are estimated based on Equation (4). In order to present the
regret bounds, we need the following definitions. Let P = (cid:80) pm,a where γ = P(OY = 1 | m,a).
a m∈M γm,a m,a
Further, define S and H as the arithmetic mean, and the harmonic mean of the P values, respectively:
a
1 (cid:88) |A|
S := P , H := .
|A| a (cid:80) 1
a∈A a∈A Pa
5OY
A Y
Figure 2: Special case of MAR.
Theorem 3. (Regret guarantee for Algorithm 2) For every α>1, the following regret bound holds for suffi-
ciently large T:
(cid:16)(cid:112) (cid:17)
E[R ]=O αT log(T)nS .
T
Next, we show that Algorithm 2 can be adapted to the case where the conditional probabilities p
m,a
are not known and must be estimated – see Algorithm 3. The following theorem shows that this algorithm
achieves the same regret bound as Algorithm 2, i.e., the estimation of p does not affect the cumulative
m,a
regret.
Theorem 4. (Regret guarantee for Algorithm 3) For every α>1, the following regret bound holds for suffi-
ciently large T:
(cid:16)(cid:112) (cid:17)
E[R ]=O αT log(T)nS .
T
It is noteworthy that these regret bounds do not depend on the cardinality of the mediator (M). The
following theorem provides the minimax lower bound, demonstrating near-optimality of Algorithms 2 and 3.
Theorem 5. (Minimax lower bound for MAR) For any policy π, there exists a MAR instance ν such that:
(cid:16)√ (cid:17)
E[R (π,ν)]=Ω TnH .
T
Notethatwhenγ valuesareidentical(andequaltoγ)thenS andH coincide. Further,theupperand
m,a
lower bounds in this case match those of MCAR.
A special case of the MAR environment (depicted in Figure 2) pertains to when there is no mediator. In
this case, Assumption 3 reduces to the following:
Assumption 4. OY ⊥⊥Y |A for all t∈{1,...,T}.
t t t
Theorems 3 and 5 with a degenerate mediator (|M|=1) imply the following corollary.
Corollary 1. Under Assumption 4 Algorithm 3 induces cumulative regret E[R
]=O(cid:16)(cid:112)
αT
log(T)nS(cid:17)
and
T
the cumulative regret of any policy is lower bounded by E[R
]=Ω(cid:16)(cid:112)
αT
log(T)nH(cid:17)
, where S =
(cid:80)
a
γ1
a and
T n
H = n .
(cid:80)γa
a
Discussion 1. We used estimators that explicitly use the mediator values in this section. As we pointed
outearlier,thesizeofM(thealphabetofM)doesnotaffecttheregretbounds. However,onemightwonder
whether the use of the mediator can be avoided, resulting in simpler algorithms and/or estimation schemes.
We show next that any such algorithm can induce linear regret in the worst case. As a corollary, this result
implies that na¨ıvely employing the classical UCB algorithm also induces linear regret.
Theorem 6. For any mediator-agnostic policy π (a policy that does not have access to mediator values),
there exists a MAR instance ν which satisfies Assumption 3 and its regret grows linearly
E[R (π,ν)]=Ω(T).
T
6Discussion 2. The expected reward µ can also be estimated using a Horvitz-Thompson (HT) type esti-
a
mator [13]. Specifically, the conditional expectation terms in Equation (3) can be expressed as follows:
Yo1{M =m,OY =1}
E[Yo |m,a,OY =1]=E[ t t t |A =a],
t t p γ t
m,a m,a
and after plugging it into Eq. (3),
µ =E[ (cid:88) Y to1{M t =m,O tY =1} |A =a]. (5)
a γ t
m,a
m∈M
An estimator based on the latter does not require estimating the conditional outcome means (in contrast
to Eq. 3), but it rather requires modeling the missingness probabilities γ . Using such an estimator is
m,a
particularly beneficial when the missingness probabilities are known in advance, or a parametric model can
be justified. However, if the missingness probabilities are small or estimated imprecisely, the HT estimator
canexhibithighvariance,leadingtoinstability. Onecantakeastepfurtherandconstructaugmentedinverse
propensity weighted (AIPW) estimators for µ :
a
µ a =E(cid:2) (cid:88) 1{M γt =m}(cid:0) Y to1{O tY =1}−(1{O tY =1}−γ m,a)E[Y to |m,a,O tY =1](cid:1) |A t =a(cid:3) , (6)
m,a
m∈M
which is doubly robust (DR) in the sense that it is consistent as long as either the missingness probabilities
γ ortheconditionaloutcomemeansE[Yo |m,a,OY =1](butnotnecessarilyboth)arecorrectlyspecified.
m,a t t
WeprovethisclaimformallyinAppendixBforthesakeofcompleteness. Inthispaper,weconsiderdiscrete-
valuedmediators,andweestimateallthequantitiesofinterestthroughempiricalmeans. Therefore,allthree
estimators(outcome-based,HT,andDR)coincide. However,theHTandDRestimatorscanprovebeneficial
forextendingourapproachtoincorporatecontinuousmediators,orinproblemswithhigh-dimensionalactions
and/or mediators where (semi)parametric models can help improve estimation efficiency.
3.3 Missing Not At Random (MNAR)
Finally, we consider the case where the missingness mechanism directly depends on the outcome value Y.
Here, we follow the identification strategy of [40] for MNAR. However, we are interested only in identifying
the expected rewards, rather than conducting mediation analysis. We begin with the following assumption.
Assumption 5 (MNAR). OY ⊥⊥M |(A ,Y ) for t∈{1,...,T}.
t t t t
In other words, the missingness is independent of the mediator when conditioned on the action and
the actual outcome. Figure 1d graphically represents this scenario. This situation commonly arises in
environments where the reward is missing due to its value. For example, if the outcome of interest is the
income of an individual, they may not be inclined to report it if the value is too high or too low.
We further make the following assumption.
Assumption 6 (Completeness). The distribution P(M,Y,OY = 1 | a) is complete in M, that is, for any
a∈A, and for any function g :Y →R,
(cid:90)
P(M,Y =y,OY =1|a)g(y)dy =0
y∈Y
implies that g(Y)=0 with probability one.
7Below we show how µ is identified under these assumptions. The identification strategy outlined here is
a
akin to [40].
(cid:90)
P(m,OY =0|a)= P(m,y,OY =0|a)dy
y∈Y
( =a)(cid:90) P(m,y,OY =1|a)P(OY =0|y,a,m) dy
P(OY =1|y,a,m)
y∈Y
( =b)(cid:90) P(m,y,OY =1|a)P(OY =0|y,a) dy,
P(OY =1|y,a)
y∈Y
where (a) and (b) follow from Bayes’ rule and Assumption 5, respectively. Since P(M,Y,OY = 1 | a) is
completeinM,solvingthisintegralequationuniquelydeterminestheinverseoddsratioOR =
P(OY=0|y,a),
y,a P(OY=1|y,a)
allowing us to identify P(y |a) as follows:
(cid:88) (cid:88) P(y,m|OY =1,a)
P(y |a)= P(y,m|a)=
P(OY =1|y,a)
m∈M m∈M (7)
(cid:88)
= (1+OR )P(y,m|OY =1,a).
y,a
m∈M
Finally, µ =E[Y |A =a] is identified as µ =(cid:82) yP(y |a)dy.
a t t a y∈Y
In the remainder of this section, we assume Y is discrete with |Y|=L, and the outcomes are normalized
so that (cid:80) |y| = 1. Define K = |M|, and Θ = [P(m,y,OY = 1 | a)] . Additionally, we assume that
y∈Y a K×L
these matrices are not ill-conditioned.
Assumption 7. [Bounded condition number] For each arm a ∈ A, the condition number of the matrix Θ
a
is bounded by:
κ(Θ )≤C ,
a a
where κ(Θ ) denotes the condition number of Θ with respect to ∞-norm, defined as κ(Θ )=∥Θ ∥ ∥Θ†∥ ,
a a a a ∞ a ∞
with Θ† being the pseudo-inverse of Θ .
a a
We present Algorithm 4 for minimizing cumulative regret under the MNAR assumptions. The key intu-
ition behind this algorithm is to construct an estimator based on Eq. (7) and build upper confidence bounds
under Assumption 7. In order to present the regret bound of this algorithm, define p =P(Y =y |A=a),
y,a
and γ =P(OY =1|Y =y,A=a).
y,a
Theorem 7. (Regret guarantee for Algorithm 4) For every α>1, the following regret bound holds for suffi-
ciently large T:
 
(cid:115)
(cid:88)
E[R T]=O αT log(T) S a2,
a
with S a=max{ γa∥L ΘC aa ∥∞, γa(cid:114)(cid:80)K py,aγy,a},γ a=m yinγ y,a.
y∈Y
Remark 1. With γ min =m y,i anγ y,a and κ max =m aax ∥ΘC aa ∥∞, Theorem 7 implies the following bound:
(cid:32)(cid:115) (cid:33)
Lκ K
E[R ]=O αT log(T)Nmax{ max, }2 .
T γ
min
γ3/2
min
8OM OY OM OY
M
M
A Y A Y
(a) MAR (b) MNAR
Figure 3: Graphical representations of the missing data mechanisms with missing outcome and mediator.
4 MAB with Missing Outcome and Mediator
So far we considered cases where the mediator was fully observable. We now discuss how our results extend
to scenarios involving missing data in both Y and M. In this section, we assume that the outcome is MAR,
and discuss the cases where the mediator is MAR and MNAR separately. For the case where both outcome
and mediator are MNAR, refer to Appendix C. We begin by outlining each scenario, providing identification
schemes and estimators for µ . The corresponding algorithms, theoretical results, and proofs are postponed
a
to Appendix C.
Throughout this section, we work under Assumption 3.
4.1 MAR
Since the mediator values are missing, neither the conditional outcome means nor the probabilities p
m,a
are identifiable, and we require further structure to make progress. One such structure arises when the
missingness in the mediator (OM) can be assumed to be at random, i.e., OM ⊥⊥ (M ,Y ,OY) | A . This
t t t t t
assumptionisvalidforinstancewhenthemissingnessmechanismforthemediatordependsonlyontheaction.
A less stringent alternative can be formalized as:
Assumption 8. OM ⊥⊥M |A , and OM ⊥⊥Y |(A ,M ,OY) for all t∈{1,...,T}.
t t t t t t t t
See Fig. 3a for a graph representation satisfying Assumptions 3 and 8. Under these two assumptions,
analogous to Eq. (3), µ can be identified as follows.
a
µ =E(cid:2)E[Y |M,a,OY =1]|A =a(cid:3)
a t t t
=E(cid:2)E[Yo |M,a,OY =1,OM =1]|A =a,OM =1(cid:3) ,
t t t t t
where the second equation is due to Assumption 8.
4.2 MNAR
When the mediator is missing not at random, stronger assumptions are necessary to identify the expected
rewards. Analogous to Section 3.3, we will use a completeness assumption. Here too, the identification
strategy follows the approach of [40].
Assumption 9. Y , OY and OM are mutually independent conditioned on (A ,M ) for all t∈{1,...,T}.
t t t t t
9Assumption 10. For every a ∈ A,m ∈ M, P(M = m,Y,OM = 1,OY = 1 | a) is complete in Y. That is,
for any function g :Y →R,
(cid:90)
P(M =m,Y =y,OM =1,OY =1|a)g(y)dy =0
Y
implies g(Y)=0 with probability one.
Under Assumption 9, µ can be expressed as:
a
(cid:88)
µ = E[Y |a,m]p
a m,a
m∈M
(cid:88)
= E[Y |a,m,OY =1,OM =1]p .
m,a
m∈M
To proceed, we need to identify p =P(M =m|A=a). This is achieved through Assumption 10:
m,a
P(Y =y,OM =0,OY =1|a)
(cid:88)
= P(M =m,Y =y,OM =0,OY =1|a)
m∈M
(cid:88)
= P(M =m,Y =y,OM =1,OY =1|a)
m∈M
P(OM =0|M =m,A=a)
× ,
P(OM =1|M =m,A=a)
where we used Assumption 9 in the last equation. By assumption 10, the inverse odds ratios OR =
m,a
P(OM=0|m,a)
can be uniquely determined. Finally,
P(OM=1|m,a)
P(M =m,OM =1|A=a)
p = ,
m,a P(OM =1|A=a,M =m)
=(1+OR )P(M =m,OM =1|A=a).
m,a
We use a two-step estimation process, whereby in the first step, pˆ is estimated, and in the second step,
m,a
the expected reward is estimated as
(cid:88)
µˆ = pˆ µˆ
a m,a m,a
m∈M
where µˆ is the empirical mean of the samples Y , obtained after pulling arm a, conditioned on M = m
m,a t t
with both OM =1 and OY =1. Here, we require Y to be finite-valued, analogous to Section 3.
t
5 Empirical Evaluation
Here, we provide an empirical evaluation of our MAB algorithms across different missing data scenarios –
MCAR, MAR(i,ii), and MNAR. All our simulations were run on Google Colab3 with Intel Xeon CPUs. The
python codes for reproducing our experimental results are publicly available at the repository4. We model
the MAB environment in all the aforementioned settings with n=10 arms. More comprehensive simulation
results are provided in Appendix D.
3https://colab.google
4https://github.com/ilia-mahrooghi/Multi-armed-Bandits-with-Missing-Outcome
10(a) MCAR algorithm on MCAR (b) MAR algorithms with known (c) MAR algorithm with different p
bandit with various γ values. and unknown p for comparison. initializationsonMARenvironment.
(d)MARandUCBalgorithmsinthe (e) Performance of the MNAR algo-
MAR bandit environment. rithm in the described environment.
Figure 4: Results corresponding to MCAR, MAR, and MNAR settings. The shaded regions represent the
error bars, showing one standard deviation across multiple runs of the simulations.
5.1 Experiment Setup of MCAR
Each arm a ∈ {1,...,n} has an associated mean reward µ , sampled independently from a uniform distri-
a
bution over the interval [0,1]. The observation probability γ is randomly drawn from a uniform distribution
over [0.5,1.0]. At each time t, when arm a is pulled, the reward Y is generated from a normal distribution
t
N(µ ,1). Algorithm1’sperformanceisreportedacross20independentrunsintheMCARenvironmentover
a
a time horizon of T = 10,000 iterations, with a fixed parameter α = 2. Figure 4a depicts the cumulative
regret for different γ values. As expected, when γ decreases, the regret grows more rapidly as a consequence
of lower observation likelihood.
5.2 Experiment Setup of MAR
The MAB environment is again modeled with n = 10 arms but with K = 5 possible mediator values.
The expected reward for all arms is determined by {µ } ∈ Rn×K, where µ represents the mean
m,a m,a m,a
reward for arm a when the mediator takes value m. The latter reward matrix is chosen by sampling each
µ independently from a uniform distribution over [0,0.4]. To ensure the first arm is the optimal one,
m,a
an additional 0.6 is added to its corresponding value. In addition to the reward structure, the observation
mechanism is defined by a matrix {γ } ∈ Rn×K, where each γ is sampled independently from a
m,a m,a m,a
uniform distribution over [0.8,1].
For each arm a, a categorical probability distribution {p } ∈RK is defined over the K values of M.
m,a m
This distribution is drawn from a Dirichlet distribution, i.e., {p } ∼Dirichlet(1 ). Upon pulling arm a
m,a m K
and the mediator taking value m, reward Y is drawn from a normal distribution N(µ ,1), where µ is
t m,a m,a
the mean reward for arm a when mediator takes value m. The reward is observed with probability γ . We
m,a
ran Algorithms 3 and 4 over a time horizon of T = 100,000. Their cumulative regret was averaged across
1110 independent runs. As shown in Fig. 4b., knowing conditional probabilities p in advance improves the
m,a
cumulative regret, as expected.
Fig. 4c demonstrates the average cumulative regret of the MAR algorithm with different probability
distributionsoverthemediator. Inparticular,twomediatorvalueselectionstrategiesweretested: (i)uniform,
where each mediator value has an equal probability, and (ii) a peaked distribution, where one mediator per
arm has a higher probability, using a Dirichlet distribution biased by α = 5 for the chosen mediator. The
peaked distribution results in a higher cumulative regret, which aligns with the result from Theorem 5, since
S is maximized when the probability distribution is concentrated on the largest γ .
m,a
InFigure4d, wecomparetheperformanceoftheUCBandMARalgorithmsintheMARbanditenviron-
ment. TheresultsillustratethatthecumulativeregretoftheUCBalgorithmisconsistentlyhigherthanthat
oftheMARalgorithm. Notably,theregretoftheUCBalgorithmexhibitsanear-lineargrowth,asaresultof
the bias in its estimation of the reward. This bias is due to the failure to account for the mediator structure.
In contrast, the MAR algorithm, which explicitly utilizes mediators to handle missingness, achieves accurate
reward estimation and a significantly lower regret.
5.3 Experiment Setup of MNAR
The MNAR algorithm was evaluated in an environment with n = 10 arms, K = 5 mediators, and |Y| = 5
possible outcomes, over a horizon of T = 100,000, repeated 10 times. For each arm a and mediator m, the
reward function followed a categorical distribution sampled from a Dirichlet distribution, except for one arm
which was sampled from a biased Dirichlet distribution. The bias was applied to the largest y ∈Y, ensuring
that this arm had a higher expected reward. The observation probabilities γ were drawn from a uniform
y,a
distributionover[0.5,1.0],whilethemediatorprobabilitiesweresampledfromaDirichletdistribution. Fig.4e
shows that the algorithm successfully adapts to the MNAR setup, effectively minimizing the cumulative
regret.
6 Limitations and Concluding Remarks
We studied multi-armed bandits with missing outcomes and adapted UCB algorithms to incorporate miss-
ingness. Our approaches extend the applicability of MAB algorithms to a wider range of real-world online
decision-making problems. We expect that the insights given by this paper will help researchers to develop
and adapt other existing decision-making algorithms to take missingness into account. We assumed that the
auxiliary(mediator)M takesvaluesinafiniteset. Parametric(orsemiparametric)modelscanbeadoptedto
relaxthisassumptioninthefuture. Wefurtheracknowledgethatestimatingtheoddsratiosthroughintegral
equations in the MNAR setting presents significant challenges, both in terms of computational complexity
and sample efficiency. Hence, we have postponed the problem of MAB with continuous outcomes missing
not at random to future work.
References
[1] Peter Auer, Nicol`o Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit
problem. Machine Learning, 47(2):235–256, 2002. doi: 10.1023/A:1013689704352. URL https://doi.
org/10.1023/A:1013689704352.
[2] Maryam Aziz, Emilie Kaufmann, and Marie-Karelle Riviere. On multi-armed bandit designs for dose-
finding trials. Journal of Machine Learning Research, 22(14):1–38, 2021.
12[3] YikunBan, YunzheQi, andJingruiHe. Neuralcontextualbanditsforpersonalizedrecommendation. In
Companion Proceedings of the ACM on Web Conference 2024, pages 1246–1249, 2024.
[4] Heejung Bang and James M Robins. Doubly robust estimation in missing data and causal inference
models. Biometrics, 61(4):962–973, 2005.
[5] Djallel Bouneffouf, Irina Rish, Guillermo A Cecchi, and Rapha¨el F´eraud. Context attentive bandits:
Contextual bandit with restricted context. arXiv preprint arXiv:1705.03821, 2017.
[6] Djallel Bouneffouf, Sohini Upadhyay, and Yasaman Khazaeni. Contextual bandit with missing rewards.
arXiv preprint arXiv:2007.06368, 2020.
[7] S´ebastien Bubeck, Nicolo Cesa-Bianchi, et al. Regret analysis of stochastic and nonstochastic multi-
armed bandit problems. Foundations and Trends® in Machine Learning, 5(1):1–122, 2012.
[8] Xijin Chen, Kim May Lee, Sofia S Villar, and David S Robertson. Some performance considerations
when using multi-armed bandit algorithms in the presence of missing data. Plos one, 17(9):e0274272,
2022.
[9] SamuelDaulton,ShaunSingh,VashistAvadhanula,DrewDimmery,andEytanBakshy. Thompsonsam-
plingforcontextualbanditproblemswithauxiliarysafetyconstraints. arXiv preprint arXiv:1911.00638,
2019.
[10] Wenkui Ding, Tao Qin, Xu-Dong Zhang, and Tie-Yan Liu. Multi-armed bandit with budget constraint
and variable costs. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 27, pages
232–238, 2013.
[11] Yonatan Gur and Ahmadreza Momeni. Adaptive sequential experiments with unknown information
arrival processes. Manufacturing & Service Operations Management, 24(5):2666–2684, 2022.
[12] NicholasJHigham. A survey of componentwise perturbation theory,volume48. AmericanMathematical
Society, 1994.
[13] Daniel G. Horvitz and Donovan J. Thompson. A generalization of sampling without replacement
from a finite universe. Journal of the American Statistical Association, 47(260):663–685, 1952. doi:
10.1080/01621459.1952.10483446. URLhttps://www.tandfonline.com/doi/abs/10.1080/01621459.
1952.10483446.
[14] Pooria Joulani, Andras Gyorgy, and Csaba Szepesv´ari. Online learning under delayed feedback. In
International conference on machine learning, pages 1453–1461. PMLR, 2013.
[15] Sudeep Kamath, Alon Orlitsky, Dheeraj Pichapati, and Ananda Theertha Suresh. On learning distri-
butions from their samples. In Conference on Learning Theory, pages 1066–1100. PMLR, 2015.
[16] EdwardHKennedy,JacquelineAMauro,MichaelJDaniels,NatalieBurns,andDylanSSmall.Handling
missing data in instrumental variable methods for causal inference. Annual review of statistics and its
application, 6(1):125–148, 2019.
[17] John Kidd, Chelsea K Raulerson, Karen L Mohlke, and Dan-Yu Lin. Mediation analysis of multiple
mediators with incomplete omics data. Genetic epidemiology, 47(1):61–77, 2023.
[18] Tze Leung Lai and Herbert Robbins. Asymptotically efficient adaptive allocation rules. Advances in
applied mathematics, 6(1):4–22, 1985.
13[19] TalLancewicki,ShaharSegal,TomerKoren,andYishayMansour. Stochasticmulti-armedbanditswith
unrestricted delay distributions. In International Conference on Machine Learning, pages 5969–5978.
PMLR, 2021.
[20] Tor Lattimore and Csaba Szepesv´ari. Bandit algorithms. Cambridge University Press, 2020.
[21] Feng Li, Dongxiao Yu, Huan Yang, Jiguo Yu, Holger Karl, and Xiuzhen Cheng. Multi-armed-bandit-
based spectrum scheduling algorithms in wireless networks: A survey. IEEE Wireless Communications,
27(1):24–30, 2020.
[22] Lihong Li, Wei Chu, John Langford, and Robert E Schapire. A contextual-bandit approach to person-
alized news article recommendation. In Proceedings of the 19th international conference on World wide
web, pages 661–670, 2010.
[23] RoderickJALittleandDonaldBRubin. Statistical analysis with missing data,volume793. JohnWiley
& Sons, 2019.
[24] Setareh Maghsudi and Ekram Hossain. Multi-armed bandits with application to 5g small cells. IEEE
Wireless Communications, 23(3):64–73, 2016.
[25] KarthikaMohanandJudeaPearl.Graphicalmodelsforprocessingmissingdata.JournaloftheAmerican
Statistical Association, 116(534):1023–1037, 2021.
[26] Razieh Nabi, Rohit Bhattacharya, and Ilya Shpitser. Full law identification in graphical models of
missing data: Completeness results. In International conference on machine learning, pages 7153–7163.
PMLR, 2020.
[27] Ciara Pike-Burke, Shipra Agrawal, Csaba Szepesvari, and Steffen Grunewalder. Bandits with delayed,
aggregated anonymous feedback. In International Conference on Machine Learning, pages 4105–4113.
PMLR, 2018.
[28] Donald B Rubin. Inference and missing data. Biometrika, 63(3):581–592, 1976.
[29] Yevgeny Seldin, Peter Bartlett, Koby Crammer, and Yasin Abbasi-Yadkori. Prediction with limited ad-
viceandmultiarmedbanditswithpaidobservations. InInternational Conference on Machine Learning,
pages 280–287. PMLR, 2014.
[30] AleksandrsSlivkins. Introductiontomulti-armedbandits. FoundationsandTrends®inMachineLearn-
ing, 12(1-2):1–286, 2019. doi: 10.1561/2200000068.
[31] BaoLuo Sun, Lan Liu, Wang Miao, Kathleen Wirth, James Robins, and Eric J Tchetgen Tchetgen.
Semiparametric estimation with data missing not at random using an instrumental variable. Statistica
Sinica, 28(4):1965, 2018.
[32] EricJTchetgenTchetgenandKathleenEWirth. Ageneralinstrumentalvariableframeworkforregres-
sion analysis with outcome missing not at random. Biometrics, 73(4):1123–1131, 2017.
[33] YogatheesanVaratharajahandBrentBerry. Acontextual-bandit-basedapproachforinformeddecision-
making in clinical trials. Life, 12(8):1277, 2022.
[34] Arun Verma, Zhongxiang Dai, Yao Shu, and Bryan Kian Hsiang Low. Exploiting correlated auxiliary
feedback in parameterized bandits. Advances in Neural Information Processing Systems, 36, 2024.
14[35] Claire Vernade, Olivier Capp´e, and Vianney Perchet. Stochastic bandit models for delayed conversions.
arXiv preprint arXiv:1706.09186, 2017.
[36] Sof´ıa S Villar, Jack Bowden, and James Wason. Multi-armed bandit models for the optimal design of
clinicaltrials: benefitsandchallenges. Statistical science: a review journal of the Institute of Mathemat-
ical Statistics, 30(2):199, 2015.
[37] Xiao Xu, Fang Dong, Yanghua Li, Shaojian He, and Xin Li. Contextual-bandit based personalized
recommendation with time-varying user interests. In Proceedings of the AAAI Conference on Artificial
Intelligence, volume 34, pages 6518–6525, 2020.
[38] Zhiyong Zhang and Lijuan Wang. Methods for mediation analysis with missing data. Psychometrika,
78:154–184, 2013.
[39] Zhiyong Zhang, Lijuan Wang, and Xin Tong. Mediation analysis with missing data through multiple
imputation and bootstrap. In Quantitative Psychology Research: The 79th Annual Meeting of the
Psychometric Society, Madison, Wisconsin, 2014, pages 341–355. Springer, 2015.
[40] Shuozhi Zuo, Debashis Ghosh, Peng Ding, and Fan Yang. Mediation analysis with the mediator and
outcome missing not at random. Journal of the American Statistical Association, pages 1–21, 2024.
15Appendix
This appendix is organized as follows. Section A includes the omitted algorithms referred to in the main
text. Section B includes the technical proofs of our results. Section C provides our algorithm for the case
whereboththeoutcomeandthemediatoraremissingnotatrandom(MNAR)alongwiththeregretanalysis
and proofs. Finally, Section D includes additional empirical evaluation results.
A Main Algorithms
Algorithm 1 MCAR algorithm
1: Input: Number of arms n, time horizon T, α≥1
2: Initialize: µˆ a =0 for all arms a=1,2,...,n ▷ initial mean reward estimate for each arm
3: Set: T a,o =0 for all arms a=1,2,...,n ▷ number of times each arm is pulled and reward is observed
4: for each round t=1,2,...,T do
5: for each arm a=1,2,...,n do
(cid:113)
6: UCB a(t)=µˆ a+ α 2lo Tg a( ,oT)
7: end for
8: Select arm a t =argmax aUCB a(t)
9: Pull arm a t and observe reward r t
10: if reward is observed then
11: Update n at and µˆ at
12: end if
13: end for
16Algorithm 2 MAR Algorithm with known p
m,a
1: Input: Number of arms n, time horizon T, exploration parameter α
2: Initialize:
3: for each arm a∈[n] and m∈M do
4: µˆ m,a =0 ▷ estimated mean reward for arm a when M =m
5: T m,a,o =0 ▷ number of times arm a is pulled with M =m and reward observed
6: s m,a =0 ▷ number of times M =m was observed for arm a
7: end for
8: for each arm a∈[n] do
9: for log(T)2 rounds do
10: Pull arm a, observe m and reward r
11: Update s m,a for observed M =m
12: if reward is observed then
13: Update T m,a,o and µˆ m,a
14: end if
15: end for
16: end for
17: T 1 =nlog(T)2, T 2 =T −T 1
18: for each round t=1,...,T 2 do
19: for each arm a∈[n] do
(cid:80)
20: Compute µˆ a = p m,aµˆ m,a ▷ estimated mean reward for arm a
m∈[K]
(cid:114)
21: Compute UCB a(t)=µˆ a+ αlo 2g(T) m(cid:80) ∈MTp m2 m ,a,a
,o
▷ Upper Confidence Bound for arm a
22: end for
23: Select arm a t =argmax aUCB a(t)
24: Pull arm a t, observe m and reward r t
25: Update s m,at and, if reward is observed, update n at,m and µˆ at,m
26: end for
17Algorithm 3 MAR Algorithm with unknown p
m,a
1: Input: Number of arms n, time horizon T, exploration parameter α
2: Initialize:
3: for each arm a∈[n] and m∈M do
4: µˆ m,a =0 ▷ estimated mean reward for arm a when M =m
5: T m,a,o =0 ▷ number of times arm a is pulled with M =m and reward observed
6: s m,a =0 ▷ number of times M =m was observed for arm a
7: end for
8: for each arm a∈[n] do
9: for log(T)2 rounds do
10: Pull arm a, observe m and reward r
11: Update s m,a for observed M =m
12: if reward is observed then
13: Update T m,a,o and µˆ m,a
14: end if
15: end for
16: end for
17: T 1 =nlog(T)2, T 2 =T −T 1
18: for each round t=1,...,T 2 do
19: for each arm a∈[n] do
2 20 1:
:
E Cs ot mim pa ut te epˆ µˆm a,a ==
T1
as Tm (cid:80)a,a t(cid:80)for me ∈a Mc (cid:114)h µˆm m,a∈ 1M
{M t =m}
22: Compute UCB a(t)=µˆ a+8 αlo 2g(T) m(cid:80) ∈MTpˆ m2 m ,a,a
,o
23: end for
24: Select arm a t =argmax aUCB a(t)
25: Pull arm a t, observe m and reward r t
26: Update s m,at and, if reward is observed, update n m,at and µˆ m,at
27: end for
18Algorithm 4 MNAR Algorithm
1: Input: Number of arms n, time horizon T, exploration parameter α
2: Initialize:
3: for each arm a∈[n] and m∈M do
4: Set b m,0|a =0 ▷ Estimation of P(M =m,OY =0|a)
5: Set θˆ a =[0] k×L ▷ Estimation of matrix θ a[m,y]=P(m,y,OY =1|a)
6: Set q m,y|1,a =0 ▷ Estimation of P(M =m,Y =y |a,OY =1)
7: Set T a =0 ▷ Count of pulls of arm a
8: Set T a,o =0 ▷ Count of pulls of arm a with observed reward
9: end for
10: for each arm a∈[n] do
11: for log(T)2 rounds do
12: Pull arm a, observe mediator m and reward y
13: Update T a
14: if reward is observed then
15: Update T a,o, θˆ a[m,y], and q m,y|1,a
16: else
17: Update b m,0|a
18: end if
19: end for
20: end for
21: Set T 1 =nlog(T)2 and T 2 =T −T 1
22: for each round t=1,...,T 2 do
23: for each arm a∈[n] do
24: Solve x a =θˆ a† b a
25: Update x a =x a+[1] L×1
26: Compute pˆ(m,y)=x a[y]×q m,y|1,a
(cid:80)
27: Compute pˆ(y)= pˆ(m,y)
m∈M
(cid:80)
28: Compute µˆ a = y×pˆ(y)
y∈Y
29: Compute γˆ a = m y∈a Yx(1 xa[y])
(cid:113) (cid:113)
30: Compute UCB a =µˆ a+8 ∥θˆ aL ∥C ∞a γˆa αlo Tg a(T) + γˆK a αl To ag ,( oT)
31: end for
32: Select arm a t =argmax aUCB a(t)
33: Pull arm a t, observe m and reward y t
34: Update T at
35: if reward is observed then
36: Update n at, θˆ at[m,y], and q m,y|1,at
37: else
38: Update b m,0|at
39: end if
40: end for
19B Technical Proofs
Double Robustness of AIPW estimator. Following Discussion 2 in Section 3.2, let γˆ and µˆ be
m,a m,a
models for γ and E[Yo |m,a,OY =1], respectively. Define
m,a t t
µˆ a =E(cid:2) (cid:88) 1{M γˆt =m}(cid:0) Y to1{O tY =1}−(1{O tY =1}−γˆ m,a)µˆ m,a(cid:1) |A t =a(cid:3) , (8)
m,a
m∈M
as an estimator for µ of Eq. (6). Herein, we prove that µˆ is doubly robust, in the sense that if either of the
a a
missingnessprobabilitymodels(γˆ )ortheoutcomeregressionmodels(µˆ ),butnotnecessarilyboth,are
m,a m,a
correctly specified, then µˆ of Eq. (8) is consistent for µ of Eq. (6). We discuss the two cases separately:
a a
Case (i): the missingness probabilities are correctly specified; i.e., γˆ =γ . In this case,
m,a m,a
(cid:88) 1{OY =1}
E[ ( t −1)µˆ 1{M =m}|A =a]
γˆ m,a t t
m,a
m∈M
( =a) (cid:88) E[(1{O tY =1} −1)µˆ 1{M =m}|A =a]
γ m,a t t
m,a
m∈M
( =b) (cid:88) E[(1{O tY =1}
−1)|A =a,M =m]µˆ p
γ t t m,a m,a
m,a
m∈M
( =c) (cid:88) (γ m,a −1)µˆ p
γ m,a m,a
m,a
m∈M
=0,
where (a) is due to γˆ being correctly specified, (b) is an application of the law of total expectation, and
m,a
(c) is by definition of γ =E[OY |A =a,M =m]. As a result, we get
m,a t t t
µˆ =E(cid:2) (cid:88) 1{M t =m} Yo1{OY =1}|A =a(cid:3) ,
a γ t t t
m,a
m∈M
which matches Eq. (5), and therefore µˆ is consistent for µ .
a a
Case (ii): the outcome regression models are correctly specified; i.e., µˆ =E[Yo |m,a,OY =1]. Then,
m,a t t
E(cid:2) (cid:88) 1{M t =m} (Yo−µˆ )1{OY =1}|A =a(cid:3)
γˆ t m,a t t
m,a
m∈M
( =a) (cid:88) E(cid:2)1{O tY =1} (Yo−µˆ )|A =a,M =m(cid:3) p
γˆ t m,a t t m,a
m,a
m∈M
( =b) (cid:88) γ m,aE(cid:2) Yo−µˆ |A =a,M =m,OY =1(cid:3) p
γˆ t m,a t t t m,a
m,a
m∈M
( =c) (cid:88) γ m,a(cid:0)E(cid:2) Yo |A =a,M =m,OY =1(cid:3) −µˆ (cid:1) p
γˆ t t t t m,a m,a
m,a
m∈M
(d)
= 0,
20where (a) and (b) are due to the law of total expectations, (c) is by linearity of expectation, and (d) follows
from the correctness of µˆ . From Eq. (8),
m,a
(cid:88)
µˆ =E[ 1{M =m}µˆ |A =a]
m,a t m,a t
m∈M
(cid:88)
=µˆ =E[ 1{M =m}E[Yo |m,a,OY =1]|A =a]
m,a t t t t
m∈M
=µˆ =E[E[Yo |M,a,OY =1]|A =a],
m,a t t t
which matches Eq. (3), and therefore µˆ is consistent for µ .
m,a m,a
Theorem 1. (MCAR regret guarantee) Under Assumption 1, for every α > 1, the cumulative regret of the
adapted UCB (Alg. 1) is bounded as follows:
(cid:32)(cid:115) (cid:33)
αnT log(T)
E[R ]=O .
T γ
Proof. Let a∗ = argmaxµ be the optimal arm. Using Hoeffding’s inequality, we can derive the following
a
a
bounds for any time step 1≤t≤T:
- If a=a =argmax(UCB ), we have:
t a
a
(cid:115)
αlog(t)
µˆ −µ ≤ ,
a a 2T
a,o
with probability 1−t−α. Name this “good event” A .
t
- If a=a∗, we have:
(cid:115)
αlog(t)
µ −µˆ ≤ ,
a a 2T
a,o
with probability 1−t−α. Name this “good event” B .
t
(cid:113)
Now, define ϵ = αlog(t). For a=a =argmax(UCB ), we get the following inequality:
a 2Ta,o t a a
∆
µ +2ϵ ≥µˆ +ϵ =UCB ≥UCB =µˆ +ϵ ≥µ ⇒ ϵ ≥ a, (9)
a a a a a a∗ a∗ a∗ a∗ a 2
where ∆ =µ −µ .
a a∗ a
(cid:84)
Now, if E represents the “good events” at time step t, then under E = E , using (9) we obtain:
t t
t
T ≤4αlog(T)∆−2.
a,o a
21Thus, we have:
T
(cid:88)
E[T ]= E[I(I =a,OY =1)]
a,o t t
t=1
T
(cid:88)
≤4αlog(T)∆−2+ E[I(Ec)]
a t
t=1
T
(cid:88)
=4αlog(T)∆−2+ E[I((Ac∪Bc))]
a t t
t=1
T
(cid:88)
≤4αlog(T)∆−2+ 2t−α
a
t=1
2α
≤4αlog(T)∆−2+ . (10)
a α−1
Since we observe the reward with probability γ, and OY ⊥⊥(A,Y), we have E[T ]=γE[T ]. Therefore:
a,o a
4αlog(T)∆−2+ 2α
E[T ]≤ a α−1.
a γ
(cid:113)
Let x= 4αnlog(T). Then, we have:
Tγ
(cid:88)
E[R ]= ∆ E[T ]
T a a
a
(cid:88) (cid:88)
= ∆ E[T ]+ ∆ E[T ]
a a a a
∆a<x ∆a≥x
≤Tx+
(cid:88)
∆
4αlog(T)∆− a2+ α2 −α
1
a γ
∆a≥x
4nαlog(T) 2nα
=Tx+ +
xγ (α−1)γ
(cid:115)
4nαT log(T) 2nα
=2 + (11)
γ γ(α−1)
(cid:32)(cid:115) (cid:33)
nαTlog(T)
=O (12)
γ
Theorem 2. (Minimax lower bound for MCAR) For any policy π, there exists an MCAR instance ν s.t.
(cid:32)(cid:115) (cid:33)
nT
E[R (π,ν)]=Ω ,
T γ
where E[R (π,ν)] represents the expected regret of policy π in instance ν.
T
22Proof. Consider the following n+1 bandit instances, with n arms labeled a ,a ,...,a .
1 2 n
Bandit instance 0:
• E[Y(a)]=0 for all a=a ,...,a .
1 n
Bandit instance k for k =1,...,n:
• E[Y(a )]=∆ for a=a .
k k
• E[Y(a)]=0 for a̸=a .
k
Next, we present key lemmas adapted from [20] to complete our analysis.
DivergenceDecomposition: Letν =(P(1),...,P(k))andν′ =(P′(1),...,P′(k))representthereward
distributions for two k-armed bandits. For a fixed policy π, let P =P and P =P be the probability
ν ν,π ν′ ν′,π
measures induced by the n-round interaction with ν and ν′. Then:
k
(cid:88)
KL(P ,P )= E [T (n)]KL(P(i),P′(i)).
ν ν′ ν i
i=1
Pinsker’s Inequality: For measures P and Q on the same probability space (Ω,F), the total variation
distance is bounded by:
(cid:114)
1
d (P,Q)= sup |P(A)−Q(A)|≤ KL(P,Q).
TV 2
A∈F
Total Variation Bound: Let (Ω,F) be a measurable space, and let P and Q be probability measures
on F. For any F-measurable random variable X :Ω→[a,b], we have:
(cid:12)(cid:90) (cid:90) (cid:12)
(cid:12) (cid:12)
(cid:12) X(ω)dP(ω)− X(ω)dQ(ω)(cid:12)≤(b−a)d TV(P,Q).
(cid:12) (cid:12)
Ω Ω
Now,inoursetupwithmissingobservations,so(OY,Y)representtheobservationtuple. Hence,wehave:
γ∆2
KL(P ,P )=E [T ]KL(P (i),P (i))=E [T ] .
0 i 0 i 0 i 0 i 2
From this, we can bound E [T (T)] as follows:
i i
E [T (T)]≤E [T (T)]+Td (P (i),P (i))
i i 0 i TV 0 i
(cid:114)
1
≤E [T (T)]+T KL(P (i),P (i))
0 i 2 0 i
(cid:114)
1 γ∆2
=E [T (T)]+T · E [T (T)]
0 i 2 2 0 i
T(cid:112)
=E [T (T)]+ γ∆2E [T (T)].
0 i 2 0 i
Let R =R (π;i) denote the regret of applying policy π on the i-th bandit instance up to time T, where
i T
i refers to the i-th bandit instance.
23Summing over all bandit instances, we have:
n n
(cid:88) (cid:88)
E[R ]= ∆(T −E [T (T)])
i i i
i=1 i=1
n (cid:18) (cid:19)
(cid:88) T(cid:112)
≥∆Tn−∆ E [T (T)]+ γ∆2E [T (T)]
0 i 2 0 i
i=1
∆2T(cid:112)
≥∆Tn−∆T − γTn
2
∆Tn ∆2T(cid:112) n
≥ − γTn using ∆= √
2 2 2 γTn
(cid:115)
Tn2 n Tn
≥ √ = .
8 γTn 8 γ
(cid:16)(cid:113) (cid:17)
Thus, there exists an instance where E[R ]≥Ω Tn .
i γ
Theorem 3. (Regret guarantee for Algorithm 2) For every α>1, the following regret bound holds for suffi-
ciently large T:
(cid:16)(cid:112) (cid:17)
E[R ]=O αT log(T)nS .
T
Proof. As before, let a∗ = argmaxµ denote the optimal arm, and define T = (cid:80) T as the total number
a 1 1,a
a a
of times the agent samples each arm during the initial rounds. After the first T rounds, we can derive the
1
following bounds at any time step 1≤t≤T .
2
For each arm a, let the reward samples observed when M = i be denoted by Y (1),...,Y (n ).
m,a m,a m,a
Applying Hoeffding’s inequality, we obtain:
(cid:12) (cid:12)   
P (cid:12) (cid:12) (cid:12)
(cid:12)
(cid:12)m(cid:88) ∈[K]p m,a(cid:80)n j=m 1 n,a mY ,m a,a(j) −µ a(cid:12) (cid:12) (cid:12)
(cid:12)
(cid:12)≥ϵ a≤2exp−
(cid:80)
m∈2 [Kϵ2 a
]
np2 m m, ,a
a
This result holds because the sub-Gaussian norm of the random variable p Ym,a(j) is pm,a.
(cid:113)
m,a nm,a nm,a
By setting ϵ =
αlog(t)(cid:80) p2
m,a, we obtain the following inequality, which holds with probability
a 2 m∈[K] nm,a
at least 1−2t−α:
(cid:118)
|µˆ −µ |≤(cid:117) (cid:117)αlog(t) (cid:88) p2 m,a
a a (cid:116) 2 n
m,a
m∈[K]
Name the above “good event” A .
t,a
Also, like before for a=a =argmax(UCB ), we get the following inequality:
t a
a
∆
µ +2ϵ ≥µˆ +ϵ =UCB ≥UCB =µˆ +ϵ ≥µ ⇒ ϵ ≥ a, (13)
a a a a a a∗ a∗ a∗ a∗ a 2
where ∆ =µ −µ .
a a∗ a
24Next, let s represent the number of times arm a is pulled and M =i is observed, and let T represent
m,a a
thetotalnumberof timesarmaispulled. UsingHoeffding’sinequality, wecanboundthedeviationbetween
p (the probability of observing M =i) and the empirical ratio sm,a as follows:
m,a Ta
(cid:115) (cid:115)
s αlog(t) αlog(T)
p − m,a ≤ ≤ ,
m,a T 2T 2T
a a a
with probability at least 1−t−α. Name this “good event” B .
t,m,a
Similarly, we bound the deviation between γ and nm,a, where n is the number of times reward is
m,a sm,a m,a
observed for arm a and M =i:
(cid:115) (cid:115)
n αlog(t) αlog(T)
γ − m,a ≤ ≤ ,
m,a s 2s 2s
m,a m,a m,a
again with probability 1−t−α. Name this “good event” C .
t,m,a
For sufficiently large T, we have:
1
log(T)2 ≥2αlog(T) ,
p2
(cid:113)
which implies T ≥ log(T)2 ≥ 2αlog(T) 1 . This allows us to use inequality αlog(T) ≤ pm,a to derive a
a p2 2Ta 2
lower bound for s :
m,a
T p
s ≥ a m,a.
m,a 2
Furthermore, since s ≥ Tapm,a and For sufficiently large T, we know that T ≥ T = log(T)2 ≥
m,a 2 a 1,a
(cid:113)
2αlog(T) 2 . This gives us αlog(T) ≤ γm,a, which allows us to establish a lower bound for n :
γ m2 ,ap 2sm,a 2 m,a
γ
n ≥ m,as .
m,a 2 m,a
Combining this with s ≥ Tapm,a, we derive:
m,a 2
T p γ
n ≥ a m,a m,a.
m,a 4
(cid:84)
Let E represent the intersection of ”good events” at time step t. Under E = E , we obtain:
t t
t
(cid:118)
ϵ =(cid:117) (cid:117)αlog(t) (cid:88) p2 m,a
a (cid:116) 2 n
m,a
m∈[K]
(cid:118)
≤(cid:117) (cid:117)αlog(T) (cid:88) 4p2 m,a
(cid:116)
2 T p γ
a m,a m,a
m∈[K]
(cid:118)
=(cid:117) (cid:117)2αlog(T) (cid:88) p m,a
(cid:116)
T γ
a m,a
m∈[K]
(cid:115)
2αlog(T)
= P
T a
a
25Using inequality (13), we have:
8αlog(T)P
T ≤ a
a ∆2
a
Thus, we get:
T
(cid:88)
E[T ]= E[I(I =a)]
a t
t=1
T
≤ 8αlog(T)P a +(cid:88) E[I(Ec)]
∆2 t
a t=1
T
≤ 8αlog(T)P a +(cid:88) E[I((cid:91)(cid:0) Bc ∪Cc (cid:1) ∪A )]
∆2 t,m,a t,m,a t,a
a t=1 m
T
≤ 8αlog(T)P a +(cid:88) 4Kt−α
∆2
a t=1
8αlog(T)P 4Kα
≤ a + . (14)
∆2 α−1
a
To conclude, note that the regret of second part of algorithm is E[R ] = (cid:80) ∆ E[T ]. We now split the
2 a a
a
(cid:113) (cid:113)
arms into two groups: ∆ ≤ 8αlog(T)S and ∆ ≥ 8αlog(T)S. Let R be the regret for the second part,
a T a T 2
(cid:113)
and let x= 8αlog(T)S. To conclude, note that E[R]=(cid:80) ∆ E[T ]. Since S =(cid:80) P Then:
T a a a
a a
(cid:88) (cid:88) (cid:88)
E[R ]= ∆ E[T ]= ∆ E[T ]+ ∆ E[T ]
2 a a a a a a
a ∆a<x ∆a≥x
8αlog(T) 4Kαn (cid:112) 4Kαn
≤Tx+ S+ =2 8αT log(T)S+ .
x α−1 α−1
Finally, for the total regret R=R +R , we have:
1 2
(cid:112) 4Kαn (cid:88)
E[R]≤2 8αT log(T)S+ + T
α−1 1,a
a
(cid:112) 4Kαn
≤2 8αT log(T)S+ +nlog(T)2
α−1
(cid:16)(cid:112) (cid:17)
=O αT log(T)S . (15)
Theorem 4. (Regret guarantee for Algorithm 3) For every α>1, the following regret bound holds for suffi-
ciently large T:
(cid:16)(cid:112) (cid:17)
E[R ]=O αT log(T)nS .
T
Proof. We follow the same approach as the previous proof. From the previous result, we know the following
inequality holds with probability 1−2t−α:
(cid:12) (cid:12) (cid:118)
(cid:12) (cid:12) (cid:12)(cid:88) p m,aµˆ m,a−µ a(cid:12) (cid:12) (cid:12)≤(cid:117) (cid:117) (cid:116)αlo 2g(t) (cid:88) Tp2 m,a .
(cid:12) m∈M (cid:12) m∈M m,a,o
26Let this “good event” be denoted as A .
t,a
Now, we consider:
(cid:12) (cid:12) (cid:12) (cid:12)
(cid:12)(cid:88) (cid:12) (cid:12)(cid:88) (cid:88) (cid:12)
(cid:12) pˆ µˆ −µ (cid:12)=(cid:12) (pˆ −p )µˆ + p µˆ −µ (cid:12) (16)
(cid:12) m,a m,a a(cid:12) (cid:12) m,a m,a m,a m,a m,a a(cid:12)
(cid:12) m∈M (cid:12) (cid:12) m∈M m∈M (cid:12)
(cid:12) (cid:12)
(cid:88) (cid:12)(cid:88) (cid:12)
≤ |pˆ −p |µˆ +(cid:12) p µˆ −µ (cid:12). (17)
m,a m,a m,a (cid:12) m,a m,a a(cid:12)
m∈M (cid:12) m∈M (cid:12)
From the previous proof, since pˆ = sm,a, we have:
m,a Ta
(cid:115)
αlog(t)
|p −pˆ |≤ ,
m,a m,a 2T
a
with probability at least 1−2t−α. Denote this ”good event” as B . Under this event for T ≥ log(T)2
t,i,a a
and sufficient big T we will have pm,a ≤pˆ ≤2p
2 m,a m,a
Additionally, we have:
(cid:115) (cid:115)
(cid:12) (cid:12)
(cid:12) (cid:12) (cid:12)γ i,a− sn i,a (cid:12) (cid:12) (cid:12)≤ α 2l sog(t) ≤ α 2lo sg(T) ,
m,a m,a m,a
again with probability 1−2t−α, denoted as ”good event” C .
t,i,a
If these “good events” hold, we know:
γ p T
T ≥ m,a m,a a.
m,a,o 4
We also know:
(cid:115)
αlog(t)
|µ −µˆ |≤ ,
i,a m,a 2T
m,a,o
which leads to:
(cid:115) (cid:115)
αlog(t) αlog(t)
µˆ ≤µ + ≤1+ ,
m,a m,a 2T 2T
m,a,o m,a,o
with probability at least 1−2t−α, denoted as ”good event” D .
t,i,a
Under all these “good events,” we have:
(cid:115)
(cid:88) (cid:88) (cid:88) αlog(t)
|pˆ −p |µˆ ≤ |pˆ −p |+ |pˆ −p |×
m,a m,a m,a m,a m,a m,a m,a 2T
m∈M m∈M m∈M m,a,o
(cid:115) (cid:115)
(cid:88) (cid:88) αlog(T) 2αlog(T)
≤ |pˆ −p |+ × .
m,a m,a 2T p γ T
m∈M m∈M a m,a m,a a
Using Lemma 7 from [15], we get:
(cid:115)
(cid:88) 2(k−1) 4k1 2(k−1)41
|pˆ −p |≤ + .
m,a m,a πT 3
m∈M a T a4
27Now, combining everything with the initial inequality (16), we have:
(cid:12) (cid:12) (cid:115)
(cid:12) (cid:12)(cid:88) pˆ µˆ −µ (cid:12) (cid:12)≤ 2(k−1) + 4k1 2(k−1)1 4
(cid:12) m,a m,a a(cid:12) πT 3
(cid:12) m∈M (cid:12) a T a4
(cid:115) (cid:115) (cid:118)
+ (cid:88) αlog(T) × 2αlog(T) +(cid:117) (cid:117) (cid:116)αlog(T) (cid:88) p2 m,a .
2T p γ T 2 T
m∈M a m,a m,a a m∈M m,a,o
For sufficiently large T, since T >log(T)2, we have:
a
(cid:118) (cid:115) (cid:115)
(cid:117) (cid:117) (cid:116)αlog(T) (cid:88) p2 m,a ≥ (cid:88) αlog(T) × 2αlog(T) .
2 T 2T p γ T
m∈M m,a,o m∈M a m,a m,a a
and
(cid:118) (cid:115)
(cid:117) (cid:117) (cid:116)αlog(T) (cid:88) p2 m,a ≥ 2(k−1) and 4k21(k−1)41
2 T πT 3
m∈M m,a,o a T a4
Thus, we conclude:
(cid:12) (cid:12) (cid:118)
(cid:12) (cid:12) (cid:12)(cid:88) pˆ m,aµˆ m,a−µ a(cid:12) (cid:12) (cid:12)≤4(cid:117) (cid:117) (cid:116)αlo 2g(T) (cid:88) Tp2 m,a .
(cid:12) m∈M (cid:12) m∈M m,a,o
Finally, since pm,a ≤pˆ , we have:
2 m,a
(cid:12) (cid:12) (cid:118)
(cid:12) (cid:12) (cid:12)(cid:88) pˆ m,aµˆ m,a−µ a(cid:12) (cid:12) (cid:12)≤8(cid:117) (cid:117) (cid:116)αlo 2g(T) (cid:88) Tpˆ2 m,a .
(cid:12) m∈M (cid:12) m∈M m,a,o
FollowingtheexactreasoningintheproofofTheorem3,andusingthefactthatpˆ ≤2p weconclude:
m,a m,a
(cid:16) (cid:112) (cid:17)
E[R]=O α T log(T)S .
Theorem 5. (Minimax lower bound for MAR) For any policy π, there exists a MAR instance ν such that:
(cid:16)√ (cid:17)
E[R (π,ν)]=Ω TnH .
T
Proof. Consider the following n+1 bandit instances, with n arms labeled a ,a ,...,a .
1 2 n
Bandit instance 0:
• µ =0 for all arms a=a ,...,a and for all m=1,...,K.
m,a 1 n
Bandit instance j for j =1,...,n:
• µ = ∆ for arm a=a , and for all m=1,...,K.
• µm,a =0Pa fγ om r,a all arms a̸=a ,j and for all m=1,...,K.
m,a j
For each instance j ∈{1,...,n}:
• If a=j: µ =(cid:80) p µ =∆.
a m∈[K] m,a m,a
28• If a̸=j: µ =0.
a
For instance 0:
• For all a∈[1,...,n]: µ =0.
a
Now like the previous we use the mentioned lemmas from [20] to complete our proof.
Like before for every a=1,...,n we have:
KL(P ,P )=E [T ]KL(P (a),P (a))
0 a 0 a 0 a
(cid:88) ∆2
=E [T ] p γ
0 a m,a m,a2P2γ2
a m,a
m∈[K]
=E [T ]∆2 (cid:88) p m,a
0 a P2 γ
a m,a
m∈[K]
∆2
=E [T ]
0 a P
a
From this, we can bound E [T (T)] as follows:
a a
E [T (T)]≤E [T (T)]+Td (P (a),P (a))
a a 0 a TV 0 a
(cid:114)
1
≤E [T (T)]+T KL(P (a),P (a))
0 a 2 0 a
(cid:115)
T ∆2
=E [T (T)]+ E [T ]
0 a 2 0 a P
a
LetR =R (π;i)denotetheregretofapplyingpolicyπ onthei-thbanditinstanceuptotimeT,where
m T
i refers to the i-th bandit instance.
Summing over all bandit instances 1,...,n, we have:
n
(cid:88) (cid:88)
E[R ]= ∆(T −E [T (T)])
i a a
i=1 a∈[n]
 (cid:115) 
(cid:88) T ∆2
≥∆Tn−∆ E 0[T a(T)]+
2
E 0[T a]
P

a
a∈[n]
(cid:115)
≥∆Tn−∆T −
∆2T (cid:88) E 0[T a(T)]
2 P
a
a∈[n]
(cid:118)
∆2T(cid:117) (cid:88) 1
≥∆Tn−∆T − (cid:117)T
2 (cid:116) P
a
a∈[n]
(cid:118)
∆Tn ∆2T(cid:117) (cid:88) 1 n
≥ − (cid:117)T using ∆=
2 2 (cid:116) P a 2(cid:114) T (cid:80) 1
a∈[n] Pa
a∈[n]
(cid:118)
n(cid:117) Tn2
≥ (cid:117)
8(cid:116) (cid:80) 1
Pa
a∈[n]
29(cid:32) (cid:33)
(cid:114) (cid:16)√ (cid:17)
Thus, there exists an instance where E[R ]≥Ω Tn2 =Ω TnH .
i (cid:80) 1
Pa
a∈[n]
Theorem 6. For any mediator-agnostic policy π (a policy that does not have access to mediator values),
there exists a MAR instance ν which satisfies Assumption 3 and its regret grows linearly
E[R (π,ν)]=Ω(T).
T
Proof. We construct a bandit with two arms such that the KL divergence of the outputs observed by the
agents, when there is no mediator, is zero. In this scenario, we have:
(cid:112)
E[T ]≤E[T ]+T E[T KL(P ,P )]=E[T ],
1 2 1 1 2 2
and
(cid:112)
E[T ]≤E[T ]+T E[T KL(P ,P )]=E[T ],
2 1 2 2 1 1
which implies:
T
E[T ]=E[T ]= .
1 2 2
Thus, if the actual means of the arms differ, with µ −µ =∆, then:
1 2
(cid:18) (cid:19)
T∆
E[R ]=Ω =Ω(T).
T 2
Now, let f (y) represent the probability mass function of Y. We have:
Y
(cid:88) (cid:88)
f (y |a,OY =1)= P(m|a,OY =1)f (y |a,m,OY =1)= P(m|a,OY =1)f (y |a,m),
Y Y Y
m m
and
P(m,a,OY =1) γ p
P(m|a,OY =1)= = m,a m,a .
(cid:80)P(m,a,OY
=1)
(cid:80)
γ p
m,a m,a
m m
1
Now, if we let γ = pm,a , we have:
m,a (cid:80) 1
m
pm,a
(cid:80)
f (y |a,m)
Y
f (y |a,OY =1)= m ,
Y K
This expression is independent of p . Additionally, we have:
m,a
(cid:88) K
P(OY =1|a)= p γ = ,
m,a m,a (cid:80) 1
m
m
pm,a
whichleadstoKL(P ,P )=KL(P ,P )=0foranychoiceofp suchthatthesetP ={p |∀m∈M}
1 2 2 1 m,a a m,a
is the same for both arms.
30(cid:80)
Sinceµ = p µ , weletallµ bezeroexceptforone, whichwesetto1. Now, forarma=1, let
a m,a m,a m,a
m
p =1−ϵ for the m such that µ =1, and set the others equal to ϵ . For arm a=2, let p =1−ϵ
m,a m,a K−1 m,a
for the m such that µ ̸=1, and set the others equal to ϵ .
m,a K−1
In this way, µ −µ =1−ϵ− ϵ , completing the construction for small ϵ.
1 2 K−1
Theorem 7. (Regret guarantee for Algorithm 4) For every α>1, the following regret bound holds for suffi-
ciently large T:
 
(cid:115)
(cid:88)
E[R T]=O αT log(T) S a2,
a
with S a=max{ γa∥L ΘC aa ∥∞, γa(cid:114)(cid:80)K py,aγy,a},γ a=m yinγ y,a.
y∈Y
Proof. We follow the same approach used in previous upper bound proofs to derive a lower bound on the
estimation error of µ .
a
At each time step 1≤t≤T , define:
2
b =[P(m,OY =0|a)]
a K×1
Θ =[P(m,y,OY =1|a)]
a K×L
P(OY =0|y,a)
x =[ ]
a P(OY =1|y,a) L×1
Since we know that Θ x =b , we now invoke Theorem 2.2 from [12], which states:
a a a
Theorem 2.2. LetAx=band(A+∆A)y =b+∆b,where∥∆A∥≤ϵ∥E∥and∥∆b∥≤ϵ∥f∥,andassume
that ϵ∥A−1∥∥E∥<1. Then:
∥x−y∥ ϵ (cid:18) ∥A−1∥∥f∥ (cid:19)
≤ +∥A−1∥∥E∥ , (18)
∥x∥ 1−ϵ∥A−1∥∥E∥ ∥x∥
and this bound is attainable to first order in ϵ.
For each entry of b or Θ , we have T samples. By applying Hoeffding’s inequality and following the
a a a
(cid:113)
same approach as in the proofs of previous theorems, we set ϵ = αlog(T). Consequently, we obtain the
2Ta
following bounds (all norms are ∥.∥ ):
∞
∥ˆb −b ∥≤ϵ,
a a
∥Θˆ −Θ ∥≤Lϵ,
a a
with probability at least 1−2K×(L+1)t−α.
Now, under the event described above and using (18), we have:
∥x −xˆ ∥ ϵ (cid:18) ∥Θ−1∥ (cid:19)
a a ≤ a +L∥Θ−1∥ .
∥x ∥ 1−ϵL∥Θ−1∥ ∥x ∥ a
a a a
31(cid:104) (cid:105)
We have x = 1−γy,a , and therefore ∥x ∥= 1−γa. For sufficiently large T and for T ≥log(T)2, it
a γy,a
L×1
a γa a
follows that ϵL∥Θ−1∥≤ 1, leading to:
a 2
(cid:18) (cid:19) (cid:18) (cid:19)
1−γ L L
∥x −xˆ ∥≤2ϵ ∥Θ−1∥+L∥Θ−1∥ a =2ϵ∥Θ−1∥ −(L−1) ≤2ϵ ∥Θ−1∥.
a a a a γ a γ γ a
a a a
Now, since ∥Θˆ −Θ ∥≤Lϵ, for sufficiently large T and T ≥log(T)2, we have Lϵ≤ ∥Θa∥. Hence:
a a a 2
∥Θ ∥
a ≤∥Θˆ ∥≤2∥Θ ∥,
2 a a
which implies
κ(Θ ) 2κ(Θ ) 2C
∥Θ−1∥= a ≤ a ≤ a .
a ∥Θ a∥ ∥Θˆ a∥ ∥Θˆ a∥
Thus:
LC
∥x −xˆ ∥≤4ϵ a . (19)
a a γ ∥Θˆ ∥
a a
For sufficiently large T and T ≥log(T)2, we will have:
a
1
∥x −xˆ ∥≤
a a 2γ
a
so:
1 1 1 1 1
=∥xˆ +[1] ∥≥∥x +[1] ∥− =∥[ ] ∥− = .
γˆ a L×1 a L×1 2γ γ L×1 2γ 2γ
a a y,a a a
Using (19), we have:
LC
∥x −xˆ ∥≤8ϵ a .
a a γˆ ∥Θˆ ∥
a a
Since x +[1] =[ 1 ] , for every y, we have:
a L×1 γy,a L×1
(cid:12) (cid:12)
(cid:12) (cid:12) 1 − 1 (cid:12) (cid:12)≤8ϵ LC a .
(cid:12)γ y,a γˆ y,a(cid:12) γˆ a∥Θˆ a∥
Now let p = P(m,y | Oy = 1,a). By applying Hoeffding’s inequality, we have the following
m,y|1,a
inequality for all m,y:
(cid:115)
αlog(T)
|q −p |≤
m,y|1,a m,y|1,a 2T
a,o
32with probability at least 1−2KLt−α. Using the fact that pm,y|1,a =P(m,y |a)=p , we have:
γy,a m,y|a
(cid:12)p q (cid:12)
(cid:12) (cid:12) (cid:12) m,y|a m,y|a(cid:12)
(cid:12)p m,y|a−pˆ m,y|a(cid:12)=(cid:12)
(cid:12) γ
−
γˆ
(cid:12)
(cid:12)
y,a y,a
(cid:12) (cid:12)
(cid:12) 1 1 (cid:12) 1 (cid:12) (cid:12)
≤p m,y|a(cid:12)
(cid:12)γ
−
γˆ
(cid:12) (cid:12)+
γˆ
(cid:12)q m,y|1,a−p m,y|1,a(cid:12)
y,a y,a a
(cid:115)
LC 1 αlog(T)
≤8p ϵ a + .
m,y|a γˆ a∥Θˆ a∥ γˆ a T a,o
Summing up over m, we have:
(cid:115)
(cid:12) (cid:12)p y|a−pˆ y|a(cid:12) (cid:12)≤8p y|aϵ
γˆ
aL ∥C Θˆa
a∥
+ γˆK
a
αl To ag ,( oT)
(cid:115)
LC K αlog(T)
≤8ϵ a + .
γˆ a∥Θˆ a∥ γˆ a T a,o
(cid:80)
Thus, using |y|=1:
y
(cid:32) (cid:115) (cid:33) (cid:115)
|µ a−µˆ a|≤(cid:88)
y
|y|(cid:12) (cid:12)p y|a−pˆ y|a(cid:12) (cid:12)≤(cid:88)
y
|y| 8ϵ
γˆ
aL ∥C Θˆa
a∥
+ γˆK
a
αl To ag ,( oT) =8ϵ
γˆ
aL ∥C Θˆa
a∥
+ γˆK
a
αl To ag ,( oT) =ϵ a.
Hence, UCB(a)=µˆ a+ϵ a, and using previous proofs, we conclude that ϵ
a
≥ ∆ 2a. To finalize the proof:
∥Θ ∥ γ
∥Θˆ ∥≥ a ,γˆ ≥ a,
a 2 a 2
and we have P(OY =1|a)=(cid:80) p γ . Applying Hoeffding’s inequality gives:
y,a y,a
y
(cid:12) (cid:12)
(cid:12) (cid:12)T a,o −(cid:88) p γ (cid:12) (cid:12)≤ϵ,
(cid:12) T y,a y,a(cid:12)
(cid:12) a (cid:12)
y
which for sufficiently large T and T ≥log(T)2, states:
a
(cid:32) (cid:33)
(cid:88)
T ≥T p γ .
a,o a y,a y,a
y
Finally, we have:
 
(cid:115) (cid:118) (cid:115) (cid:118)
αlog(T) LC K(cid:117) 1 αlog(T) LC K(cid:117) 1
ϵ ≤ 8 a +2 (cid:117) ≤8 max a , (cid:117) ,
a T γ ∥Θ ∥ γ (cid:116)(cid:80) p γ T γ ∥Θ ∥ γ (cid:116)(cid:80) p γ 
a a a a y,a y,a a a a a y,a y,a
y y
which, following the exact steps of previous theorem proofs, leads to:
 
(cid:115)
(cid:88)
E[R T]=O αT log(T) S a2.
a
33C Theoretical results on Missing Outcome and Missing Mediator
Inthissection,wepresentourtheoreticalresultsonMissingatRandom(MAR)andMissingNotatRandom
(MNAR) environments for both Missing Outcome and Missing Mediator cases.
C.1 Missing at Random (MAR)
As discussed earlier, the identification of µ =E[Y |a] is given by:
a
(cid:88)
µ = P(M =m|a,OM =1)E[Y |M =m,a,OY =1,OM =1].
a
m∈M
Using this identification, we will prove the following theorem. We define p = P(M = m,a),γ =
m,a m,a
P(OY = 1 | M = m,a),λ = P(OM = 1 | a). Our algorithm is exactly like 3 where if M is missed we don’t
a
update anything.
Theorem 8. (Regret bound for MAR with missing mediator and outcome) For every α > 1, the following
regret bound holds for sufficiently large T:
(cid:16)(cid:112) (cid:17)
E[R ]=O αT log(T)nS ,
T
where P =(cid:80) pm,a and S := 1 (cid:80) P .
a m∈M γm,aλa |A|
a∈A
a
Proof. The proof follows a similar approach to the proof of Theorem 4. Using the same reasoning, we have
(where T is the number of times M =m and the reward are observed when pulling arm a):
m,a,oY
(cid:12) (cid:12) (cid:118)
(cid:12) (cid:12) (cid:12)(cid:88) pˆ m,aµˆ m,a−µ a(cid:12) (cid:12) (cid:12)≤8(cid:117) (cid:117) (cid:116)αlo 2g(T) (cid:88) Tpˆ2 m,a .
(cid:12) m∈M (cid:12) m∈M m,a,oY
Similarly,wealsohavethefollowinginequality(whereT isthenumberoftimesM =misobservedwhen
a,oM
pulling arm a):
(cid:115)
αlog(t)
|p −pˆ |≤ .
m,a m,a 2T
a,oM
Additionally, we have:
(cid:115)
(cid:12) (cid:12)
(cid:12) (cid:12) (cid:12)T a T,oM −λ a(cid:12) (cid:12) (cid:12)≤ αl 2o Tg(t) .
a a
Under this event, for T ≥log(T)2 and sufficiently large T, we have pm,a ≤pˆ ≤2p . Following similar
a 2 m,a m,a
steps, we get:
p γ λ T
T ≥ m,a m,a a a.
m,a,oY 4
Therefore, using the same definition of ϵ , we obtain:
a
(cid:118)
ϵ
a
≤8(cid:117) (cid:117) (cid:116)8αlo 2g(T) (cid:88)
p
γp2 m,a
λ T
=8(cid:115) 8αlo 2g(T) (cid:88)
γ
p m λ,a
T
.
m∈M m,a m,a a a m∈M m,a a a
34Finally, following the same steps as in previous proofs, we conclude:
(cid:16)(cid:112) (cid:17)
E[R ]=O αT log(T)nS .
T
C.2 Missing Not at Random (MNAR)
In this section, we use the identification formula discussed earlier to develop an algorithm and establish an
upper bound for this environment. Assume that P(m | a) = p , P(OM = 1 | m,a) = λ , P(OY = 1 |
m,a m,a
m,a)=γ . Alsodefineλ =minλ . WeassumeasimilarconditiontoAssumption7, butforadifferent
m,a a m,a
m
matrix. Let Θ =[P(m,y,OM =1,OY =1|a)] :
a K×L
Assumption 11 (Boundedconditionnumber). For each arm a∈A, the condition number of the matrix Θ
a
is bounded by:
κ(Θ )≤C ,
a a
where κ(Θ ) denotes the condition number of Θ with respect to the ∞-norm, defined as
a a
κ(Θ )=∥Θ ∥ ∥Θ†∥ ,
a a ∞ a ∞
with Θ† being the pseudo-inverse of Θ .
a a
In our algorithm we use the the given identification formula and
(cid:115) (cid:32) (cid:33) (cid:118)
UCB(a)=µˆ a+2 αl 2o Tg a(T) 8 ∥ΘC ˆa a∥λˆK
a
+ λˆ1
a
+(cid:117) (cid:117) (cid:116)αlo 2g(T) m(cid:88) ∈MT4 apˆ ,m2 m ,, oa
Y
.
we will prove the following theorem.
Theorem 9. (Regret bound for MNAR with missing mediator and outcome) For every α > 1, under As-
sumption 11, the following regret bound holds for sufficiently large T:
 
(cid:115)
(cid:88)
E[R T]=O αT log(T) S a2.
a
(cid:32) (cid:33)
(cid:16) (cid:17) (cid:114)
where S
a
=max 32 ∥ΘCa ∥aλK
a
+ λ2
a
, m(cid:80) ∈Mλm32 ,p am γm,a
,a
.
Proof. The proof closely follows the reasoning from Theorem 7. Let:
b =[P(y,OM =0,OY =1|a)] ,
a L×1
Θ =[P(m,y,OM =1,OY =1|a)] ,
a K×L
(cid:20)P(OM =0|m,a)(cid:21)
x = .
a P(OM =1|m,a)
K×1
35We know that Θ x =b . Using the same approach as in Theorem 7, we derive the following inequality for
a a a
(cid:113)
ϵ= αlog(T):
2Ta
(cid:18) (cid:19)
1−λ K C K C K
∥x−xˆ∥≤2ϵ∥Θ−1∥ 1+K a ≤2ϵ∥Θ−1∥ ≤4ϵ a ≤8ϵ a .
a λ
a
a λ
a
∥Θˆ a∥λ
a
∥Θˆ a∥λˆ
a
(cid:104) (cid:105)
Additionally, since x= 1−λm,a , we have:
λm,a
K×1
(cid:12) (cid:12)
(cid:12) 1 1 (cid:12) C K
(cid:12) − (cid:12)≤8ϵ a .
(cid:12) (cid:12)λ
m,a
λˆ m,a(cid:12)
(cid:12)
∥Θˆ a∥λˆ
a
Furthermore, for p =P(M =m,OM =1|a), we have:
m,1|a
(cid:12) (cid:12)
(cid:12)p m,1|a−pˆ m,1|a(cid:12)≤ϵ.
Using a similar approach to the proof of Theorem 7, we obtain:
(cid:12) (cid:12) (cid:32) (cid:33) (cid:32) (cid:33)
(cid:12)p pˆ (cid:12) C K 1 C K 1
(cid:12) m,1|a − m,1|a(cid:12)≤p 8ϵ a + ϵ=ϵ 8p a + .
(cid:12)
(cid:12)
λ
m,a
λˆ
m,a
(cid:12)
(cid:12)
m,1|a ∥Θˆ a∥λˆ
a
λˆ
a
m,1|a ∥Θˆ a∥λˆ
a
λˆ
a
Therefore, we can conclude:
(cid:32) (cid:33)
C K 1
|pˆ −p |≤ϵ 8p a + .
m,a m,a m,1|a ∥Θˆ ∥λˆ λˆ
a a a
Additionally,wehavethefollowingboundforT (thenumberoftimesbothM andY areobserved):
a,oM,oY
(cid:115)
αlog(T)
|µˆ −µ |≤ .
m,a m,a T
a,oM,oY
Using P(OY =1,OM =1|a)= (cid:80) p γ λ , we have:
m,a m,a m,a
m∈M
(cid:12) (cid:12) (cid:115)
(cid:12) (cid:12)T a,oM,oY − (cid:88) p γ λ (cid:12) (cid:12)≤ αlog(T) ,
(cid:12) T m,a m,a m,a(cid:12) T
(cid:12) a m∈M (cid:12) a
(cid:18) (cid:19)
which gives T a,oM,oY ≥ T 2a (cid:80) p m,aγ m,aλ m,a . Thus, for sufficiently large T and T a ≥ log(T)2, we have
m∈M
µˆ ≤2µ ≤2.
m,a m,a
36Therefore:
(cid:12) (cid:12) (cid:12) (cid:12)
(cid:12)(cid:88) (cid:12) (cid:12)(cid:88) (cid:88) (cid:12)
(cid:12) pˆ µˆ −µ (cid:12)=(cid:12) (pˆ −p )µˆ + p µˆ −µ (cid:12) (20)
(cid:12) m,a m,a a(cid:12) (cid:12) m,a m,a m,a m,a m,a a(cid:12)
(cid:12) m∈M (cid:12) (cid:12) m∈M m∈M (cid:12)
(cid:12) (cid:12)
(cid:88) (cid:12)(cid:88) (cid:12)
≤ |pˆ −p |µˆ +(cid:12) p µˆ −µ (cid:12) (21)
m,a m,a m,a (cid:12) m,a m,a a(cid:12)
m∈M (cid:12) m∈M (cid:12)
(cid:32) (cid:33) (cid:12) (cid:12)
≤2 (cid:88) ϵ 8p C a K + 1 +(cid:12) (cid:12)(cid:88) p µˆ −µ (cid:12) (cid:12) (using (cid:88) p ≤1)
m∈M
m,1|a ∥Θˆ a∥λˆ
a
λˆ
a
(cid:12)
(cid:12) m∈M
m,a m,a a(cid:12)
(cid:12) m∈M
m,1|a
(22)
(cid:32) (cid:33) (cid:12) (cid:12)
≤2ϵ 8 C a K + 1 +(cid:12) (cid:12)(cid:88) p µˆ −µ (cid:12) (cid:12). (23)
∥Θˆ a∥λˆ
a
λˆ
a
(cid:12)
(cid:12) m∈M
m,a m,a a(cid:12)
(cid:12)
Using the same technique as before, we have the following inequality for T (the number of times
a,m,oY
M =m and the reward are observed when pulling arm a):
(cid:12) (cid:12) (cid:118)
(cid:12) (cid:12) (cid:12)(cid:88) p m,aµˆ m,a−µ a(cid:12) (cid:12) (cid:12)≤(cid:117) (cid:117) (cid:116)αlo 2g(T) (cid:88) T4pˆ2 m,a .
(cid:12) m∈M (cid:12) m∈M a,m,oY
Therefore:
(cid:12) (cid:12) (cid:32) (cid:33) (cid:118)
(cid:12) (cid:12)
(cid:12)
(cid:12)
m(cid:88) ∈Mpˆ m,aµˆ m,a−µ a(cid:12) (cid:12)
(cid:12)
(cid:12)≤2ϵ 8 ∥ΘC ˆa a∥λˆK
a
+ λˆ1
a
+(cid:117) (cid:117) (cid:116)αlo 2g(T) m(cid:88) ∈MT4 apˆ ,m2 m ,, oa
Y
which proves our UCB upper bound.
Similarly, we know that T
a,m,oY
≥ T 2ap m,aλ m,aγ m,a, which gives:
(cid:32) (cid:33) (cid:118)
|µ a−µˆ a|≤2ϵ 8 ∥ΘC ˆa a∥λˆK
a
+ λˆ1
a
+(cid:117) (cid:117) (cid:116)αlo 2g(T) m(cid:88)
∈MT ap
m3 ,2 aλp2 m m, ,a
aγ
m,a
(cid:32) (cid:33) (cid:115)
=2ϵ 8 C a K + 1 + (cid:88) 32p m,a
∥Θˆ a∥λˆ
a
λˆ
a
m∈Mλ m,aγ
m,a
 
(cid:18) (cid:19) (cid:115)
≤2ϵmax 32 C a K + 2 , (cid:88) 32p m,a .
∥Θ∥ λ λ λ γ
a a a m∈M m,a m,a
Following the same reasoning as in previous proofs, we conclude:
 
(cid:115)
(cid:88)
E[R T]=O αT log(T) S a2.
a
37D Additional Empirical Evaluation
(a) MNAR and UCB algorithms in the (b) MAR and UCB algorithms in a real-
MNAR bandit environment. world MAR bandit environment.
Figure 5: Complementary evaluation results for our proposed algorithms.
In Figure 5a, we compare the performance of the UCB and MNAR algorithms in the MNAR bandit
environment. TheresultsclearlydemonstratethatthecumulativeregretoftheUCBalgorithmisconsistently
higherthanthatoftheMNARalgorithm. Additionally,they-axisisdisplayedonalogarithmicscale,further
highlightingtheconsiderabledifferenceintheperformanceofouralgorithmcomparedtotheUCBalgorithm.
The environment is generated as before, with a horizon of T = 100,000, and the experiment is repeated 10
times.
D.1 Real-World Simulation
The dataset used in this study is the Primary Biliary Cirrhosis (PBC) dataset from the Mayo Clinic 5,
containing 418 observations and 19 variables. Collected over a 10-year span (1974–1984), it focuses on a
randomized,placebo-controlledtrialofD-penicillaminefortreatingPBC,andincludesbothtrialparticipants
and observational data from non-participants.
Tosimulateareal-worldsetting,westructuredthedataasfollows: theZ1variable(1forD-penicillamine,
2forplacebo)wastreatedasthearmsofthebandit,representingtreatmentgroups. TheXvariable,denoting
the time in days from registration to death, liver transplantation, or censoring, was used as the outcome.
TheDvariable,indicatingwhetherXmeasurestimeuntildeath(1)orcensoring(0),servedasthemediator.
The D mediator captures whether the time interval X is associated with death or censoring, offering key
insights into the progression of the disease and the effect of treatment. This setup allows us to model the
pathwaysfromtreatmenttooutcome,whereZ1representstheactiontaken,Xisthereward(dayssurvived),
and D explains the intermediate state between treatment and survival or death.
ApplyingtheMARalgorithmtothisMARbanditenvironmentyieldedresultsconsistentwiththoseseen
in synthetic data, as shown in Figure 5b.
5https://www.openml.org/d/200
38