FINDVER: Explainable Claim Verification over Long and
Hybrid-Content Financial Documents
YilunZhao YitaoLong YuruJiang ChengyeWang WeiyuanChen
HongjunLiu YimingZhang XiangruTang ChenZhao ArmanCohan
YaleNLP- FINDVER Team
Abstract
We introduce FINDVER, a comprehensive
benchmark specifically designed to evaluate
theexplainableclaimverificationcapabilities
ofLLMsinthecontextofunderstandingand
analyzing long, hybrid-content financial doc- Claim to verify:
uments. FINDVER contains 2,400 expert-
The number of customers with accounts over
annotatedexamples,dividedintothreesubsets: 120 days past due increased by 65,300 from
March 31, 2023 to March 31, 2024.
information extraction, numerical reasoning,
andknowledge-intensivereasoning—eachad-
dressingcommonscenariosencounteredinreal-
world financial contexts. We assess a broad
spectrum of LLMs under long-context and Figure 1: An example from the numerical reasoning
RAGsettings. Ourresultsshowthateventhe subsetoftheFINDVERbenchmark. Toverifytheclaim,
currentbest-performingsystem,GPT-4o,still theLLMisrequiredtofirstlocateclaim-relevantdata
lagsbehindhumanexperts. Wefurtherprovide points within long and hybrid-content financial docu-
in-depthanalysisonlong-contextandRAGset- ments, and then apply numerical reasoning over the
ting,Chain-of-Thoughtreasoning,andmodel extracteddatapointsforclaimverification.
reasoningerrors,offeringinsightstodrivefu-
tureadvancements. WebelievethatFINDVER
canserveasavaluablebenchmarkforevaluat-
First,financialdocumentsaretypicallylong,in-
ingLLMsinclaimverificationovercomplex,
tricateanddense, andtheyincludebothquantita-
expert-domaindocuments.
tivetablesandqualitativetext(Chenetal.,2021;
§ github.com/yilunzhao/FinDVer
Zhuetal.,2021;Zhaoetal.,2022;Lietal.,2022;
Zhaoetal.,2023d;Koncel-Kedziorskietal.,2024).
1 Introduction
Extractingandanalyzingclaim-relevantdatafrom
In today’s information explosion era, the respon- these documents requires complicated document
sibilityofverifyingthetruthfulnessoftheitemis comprehension abilities and professional knowl-
oftenpassedontotheaudience.unverifiedclaims edgeinfinancialdomains. Moreover, thetypeof
about a company’s financial performance fre- reasoninginvolvedencompassesvariousuniqueas-
quentlycirculateinonlinemedia,potentiallymis- pectsthatarelessstudied,necessitatingadedicated
leadinginvestors. Therefore,itiscrucialtoverify approachtoevaluationandapplication.
theseclaimsusingthecompanies’originalfinancial Second,inthefinancialdomain,wheredecisions
documents(i.e.,earningsreportsandregulatoryfil- ofteninvolvesignificantstakes,itisoftencritical
ings). Recent advancements in Large Language toprovideclearandcomprehensiblerationalesfor
Models(LLMs)haveattractedsignificantattention anyclaimverificationdecisions(Atanasovaetal.,
due to their capabilities in solving a broad range 2020,2023). However,existingcontext-grounded
oftasks(Touvronetal.,2023b;Jiangetal.,2023b; claimverificationbenchmarks(Chenetal.,2020;
OpenAI,2023). However, itremainsparticularly Kamoietal.,2023;Luetal.,2023;Glockneretal.,
difficultforapplyingthemtodocument-grounded 2024) primarily focus on the task of entailment
claimverificationinreal-worldfinancialdomains classification and do not evaluate the reasoning
duetothefollowingtworeasons: process. Thishindersthepracticalapplicationand
4202
voN
8
]LC.sc[
1v46750.1142:viXraw.Expla- Reason-
Dataset InputContext Annotation/DataCreation #Label
nation? Intensive?
PubHealthTab(Akhtaretal.,2022) Wikipediatable Crowd-sourced 4 ✗ ✗
TABFACT(Chenetal.,2020) Wikipediatable Crowd-sourced 2 ✗ ✓
INFOTABS(Guptaetal.,2020) Wikipediatable Crowd-sourced 3 ✗ ✓
SCITAB(Luetal.,2023) Scientifictable Expert&InstructGPT 3 ✗ ✓
HOVER(Jiangetal.,2020) Wikipediaarticles Crowd-sourced 2 ✗ ✗
DOCNLI(Yinetal.,2021) Newsarticle Fromsummrizationdatasets 2 ✗ ✗
ContractNLI(Koreedaetal.,2021) Contract Expert&Crowd-sourced 2 ✗ ✗
LLM-AGGREFACT(Tangetal.,2024) Docfromvariousdomains Fromexistingbenchmarks 2 ✗ ✗
WICE(Kamoietal.,2023) Wikipediaarticle Crowd-sourced 3 ✗ ✗
AMBIFC(Glockneretal.,2024) Wikipediaarticle Crowd-sourced 3 ✗ ✗
CLAIMDECOMP(Chenetal.,2022a) Politicalarticle Expert 6 ✗ ✗
SCIFACT(Waddenetal.,2020) Scientificpaperabstracts Expert 2 ✗ ✓
LIAR++(Russoetal.,2023) Politicalarticle Fromfact-checkwebsite 2 ✓ ✗
FullFact(Russoetal.,2023) Webpage Fromfact-checkwebsite 2 ✓ ✗
PUBHEALTH(Akhtaretal.,2022) Healthwebpage Fromfactcheckwebsite 4 ✓ ✗
FINDVER(ours) Longfinancialdocwithtables Expert 2 ✓ ✓
Table1: ComparisonbetweenFINDVERandexistingtable-ordocument-groundedclaimverificationbenchmarks.
FINDVERisdistinguishedbyfouruniquecharacteristics: (1)ExpertAnnotation: Itisannotatedbyfinancialexperts
toensurehighdataquality;(2)ComplexDocumentComprehension: Itrequiresinterpretingamixoftextualandtab-
ulardatawithinalong-contextfinancialdocument;(3)ExaminationonReasoning-ProcessExplanation:Itenhances
claimverificationswithdetailedexplanationsaboutthereasoningprocess,increasingthebenchmark’spractical
value;and(4)DiverseReasoningforReal-worldScenarios: Itincorporatesvariousreasoningchallenges,suchas
extractingcomplicatedinformation,performingnumericalcalculations,andapplyingprofessionalknowledge.
evaluationofLLMsinreal-worldscenarios. long-context—inthisstudy. Theexperimentalre-
sultsindicatethateventheexistingbest-performing
Inresponsetotheaforementionedpressingneed,
LLM(i.e.,GPT-4o)stillsignificantlylagsbehind
we present FINDVER, a comprehensive and do-
humanexperts(76.2%versus93.3%),demonstrat-
mainexpert-annotatedexplainableclaimverifica-
ingthechallengesofourproposedbenchmark. Our
tionbenchmarkthatfirstexploresinthecontextof
contributionsaresummarizedbelow:
financial documents. The LLMs are tasked with
generatingexplanationsoftheirreasoningtoverify • Weintroduce FINDVER,thefirstcomprehensive
claimslabeledasentailedorrefutedand,basedon context-groundedclaimverificationbenchmark
theinformationintheprovideddocument,which forfinancialdomains,presentingnewchallenges
contains both textual and tabular data. To iden- forstate-of-the-artLLMs.
tifythecommonreasoning-intensivescenariosin
• Weconductanextensiveevaluationthatencom-
claim verification based on financial documents,
passesawiderangeofLLMs,comprehensively
we engage with domain experts and conducted a
assessingthecapabilitiesandlimitationsofexist-
preliminarystudy. Thishelpedusdeterminethree
ingLLMsinourtask.
keytypesofscenariosthatfrequentlyariseinreal-
• We provide in-depth analysis on Long-context
worldsettings: informationextraction,numerical
setting,RAGsetting,Chain-of-Thoughtreason-
reasoning,andknowledge-intensivereasoning. For
ing, and model reasoning errors, offering valu-
eachscenario,weconstructanevaluationset. Each
ableinsightstodrivefutureadvancements.
exampleinourdatasetisannotatedwithdetailed
supporting evidence and step-by-step reasoning-
processexplanations. 2 RelatedWork
We evaluate a wide spectrum of open- and ClaimVerificationBenchmark Claimverifica-
closed-sourceLLMs,specifically,16modelsfrom tion is a well-established research area with two
9 organizations. The documents in our bench- mainsettings. Thefirstistheopen-domainsetting,
markareexceedinglylong;therefore,weemploy which involves using an external retriever to find
two widely adopted real-world application set- themostrelevantinformationfromalargecorpus
tings—retrievalaugmentedgeneration(RAG)and toverifyclaims(VlachosandRiedel,2014;ThornePipeline Fig
Annotator A
Tables
Annotator B Validator C
1 * Financial Report
Paragraphs
Supporting Evidence & Quality
Source Document Collection Claim Annotation
Explanation Annotation Validation
Figure2: AnoverviewoftheFINDVERconstructionpipeline. Wecollectandprocessquarterlyandannualreports
from companies, which contain both tables and text, as source financial documents (§3.3). For each financial
document,expertannotatorsarefirsttaskedwithannotatingthe“entailed”claims. Next,theyareaskedtoperturb
these“entailed”claimstointroducefactualerrors,makingtheoriginalclaimsinto“refuted”claimsforthepurpose
of“refuted”claimannotation(§3.4). Foreachclaim,theannotatorsarerequiredtoprovidesupportingevidenceand
anexplanationoftheirreasoningprocess(§3.5). Finally,eachannotatedexampleundergoesqualityvalidationbya
separateexpertannotator(§3.6). ThisdesigneddataconstructionpipelineensuresthehighqualityofFINDVER.
etal.,2018;Alyetal.,2021;Waddenetal.,2022; documents(e.g.,annualreportsandregulatoryfil-
Rangapur et al., 2024; Ma et al., 2024). The sec- ings),whicharecrucialforprovidinginsightsinto
ondsettingiscontext-groundedclaimverification, acompany’sperformanceandstrategies. Several
which requires models to verify claims based on QA benchmarks have been proposed to evaluate
theprovideddocumentcontext(Chenetal.,2020; models’performanceinansweringquestionsbased
Kamoietal.,2023;Luetal.,2023;Glockneretal., onfinancialdocuments,withaparticularfocuson
2024). Thisworkfocusesonthesecondsetting,as numericalreasoning(Chenetal.,2021;Zhuetal.,
itallowsustoeliminatevariabilityanddependency 2021;Zhaoetal.,2022;Chenetal.,2022b;Koncel-
on the retriever’s performance, thereby focusing Kedziorskietal.,2024;Zhaoetal.,2024a,b). De-
on the evaluation of LLM performance on on ac- spite these advancements, there remains a signif-
curately verifying claims within a given context. icant gap in the exploration of claim verification
However,asillustratedinTable1,existingcontext- tasks within the financial domain. While the re-
groundedclaimverificationbenchmarkshavethree centFIN-FACTbenchmark(Rangapuretal.,2024)
notablelimitations: theytypically1)focusongen- addresses explainable multimodal financial fact-
eraldomains,overlookingthespecificchallenges checking,itprimarilyfocusesonopen-domainsce-
andintricaciespresentinspecializedfields,2)fo- narios. Verifyingclaimsderivedfromfinancialdoc-
cussolelyonentailmentclassificationanddonot umentsiscrucial,asinaccuraciescansignificantly
evaluatethereasoningprocessesofmodels,3)do influenceinvestmentdecisionsandmarketpercep-
notinvolveclaimsthatrequireintensivereasoning tions. Tobridgethisgap,weintroduce FINDVER,
andcomplicateddocumentcomprehension. These thefirstcontext-groundedclaimverificationbench-
limitationshindertheireffectivenessforevaluating mark,specificallydesignedforreal-worldfinancial
LLMsinreal-worldpractice. documentcomprehension.
Financial Evaluation Benchmark NLP tech- 3 FINDVER Benchmark
niques have been applied to various financial
tasks (Xie et al., 2023b, 2024a), such as named FINDVERprovidesarobustevaluationbenchmark
entity recognition (Salinas Alvarado et al., 2015; forreasoning-intensiveandexplainableclaimveri-
Shahetal.,2023),sentimentanalysis(Maloetal., ficationoverlongandhybrid-contentfinancialdoc-
2013;Maiaetal.,2018),stockmovementpredic- uments. Figure 4 in appendix presents a detailed
tion(XuandCohen,2018;Wuetal.,2018;Soun datasetexampleofourbenchmark. Inthefollowing
et al., 2022; Xie et al., 2023a), and summariza- subsections,wepresentanoverviewoftheFIND-
tion (Zhou et al., 2021; Mukherjee et al., 2022; VER constructionpipelineinFigure2;anddetail
Liu et al., 2022). More recently, there has been thetaskformulation,dataconstruction,andquality
an increasing focus on tasks involving financial validationprocess.3.1 TaskFormulation (1) FDV-IE (information extraction), which in-
volvesextractinginformationfrombothtextualand
We formally define the task of FINDVER within
tabularcontentwithinalong-contextdocument.
thecontextofLLMsasfollows: Considerasingle
financial document d, containing textual data P
(2)FDV-MATH(numericalreasoning),whichne-
cessitates performing calculations or statistical
andtabulardataT,associatedwithaclaimcthat
analysisbasedondatawithinthedocument.
needs verification. The expert-annotated data we
collectsupportsthefollowingtwotasks: (3) FDV-KNOW (knowledge-intensivereasoning),
whichrequiresintegratingexternaldomain-specific
Entailment Classification The LLM is re- knowledgeorregulationsforclaimverification.
quired to determine the entailment label ℓ ∈
L = {“entailed”,“refuted”}, based on the given 3.3 SourceDocumentCollection
hybrid-contentfinancialdocument: SimilartoZhaoetal.(2023a),weusethequarterly
(Form 10-Q) and annual reports (Form 10-K) of
ℓ = argmaxP (ℓ|P,T,c) (1) companiesasthesourcedocuments,whicharepub-
LLM
ℓ∈L liclyavailableintheopen-sourcedatabase1 ofthe
U.S.SecuritiesandExchangeCommission. Wecol-
Reasoning-process Explanation Generation
lectatotalof523documentsthatwerefirstreleased
The model is required to generate a natural lan-
betweenJanuary1toApril30,2024,whichisaf-
guageexplanatione:
terthecutoffdateofmostpretrainingcorporafor
trainingfoundationmodels. Thishelpstoalleviate
e = argmaxP (e|P,T,c) (2)
e LLM issuesrelatedtodatamemorizationtosomeextent.
AftercollectingtherawHTML-formatdocuments,
whicharticulatesthereasoningprocessbehindthe
we utilize the SEC API2, a commercial platform
validity of the provided claim c, based solely on
APIforextractingfinancialdocumentcontent,to
theprovidedtextualcontentP andtabularcontent
processthecollecteddocuments,obtainingdocu-
T withinthefinancialdocument.
mentswithbothtextualandtabulardata.
Notably,someclaimverificationsystems,partic-
3.4 ClaimAnnotation
ularlythosedevelopedpriortotheeraofLLMsand
EntailedClaimAnnotation Toaddressthepo-
forpreviousdatasetsthatdidnotrequireexplana-
tential bias concerning the position of evidence
tiongeneration(Chenetal.,2020;Yinetal.,2021;
within the documents, we initiate the process by
Koreedaetal.,2021),mightnotexplicitlyperform
randomly sampling multiple document contexts
explanationgeneration. Instead,theydirectlyout-
from the given document. Annotators are then
put the final label. For such systems, FINDVER
tasked with creating “entailed” claims based on
canalsobeusedforevaluationbyfocusingonthe
thetextualandtabulardatawithinthesecontexts.
entailmentclassificationtask.
Theannotatorsareinstructedtosimulatereal-world
3.2 FINDVERSubsetDesign documentcomprehensionscenarios,ensuringthe
annotatedclaimsarerepresentativeofpracticalfi-
FINDVER is designed to mirror the real-world
nancial analysis and align with the scenarios de-
challenges encountered in the financial domain.
finedbythecorrespondingsubsets. Annotatorsare
Therefore,weensurethattheincludedannotators
thentaskedwithcarefullylocatingallevidence(i.e.,
arefinancialexpertswithprofessionalexperience
indices of relevant paragraphs and tables) within
incomprehendingandprocessingfinancialdocu-
theentiredocumentthatsupporttheclaims,which
ments. Table 7 in appendix presents the detailed
areusedforthesubsequentdatavalidation.
annotatorbiographiesfor FINDVERannotation.
Toidentifythecommonreasoning-intensivesce- Refuted Claim Annotation Following estab-
nariosinclaimverificationbasedonfinancialdoc- lished practices in the field (Wadden et al., 2020;
uments,weengagedwithdomainexpertsandcon- Chen et al., 2020; Lu et al., 2023), and since di-
ducted a preliminary study. This helped us deter- rectly obtaining “refuted” types is difficult, we
mine three key types of scenarios that frequently insteadperturbtheoriginal“entailed”claimsinto
ariseinreal-worldsettings. Accordingly,wehave 1https://www.sec.gov/edgar/search/
createdthreecorrespondingsubsetsof FINDVER. 2https://sec-api.io/Property FDV-IE FDV-MATH FDV-KNOW
Real-worldscenariosinfinancialdomains information numerical knowledge-
extraction reasoning intensivereasoning
#Document 322 300 314
DocLength(Median/Avg/Max) 39K/40K/69K 38K/40K/68K 38K/40K/70K
#Tablesperdocument(Median/Avg) 41.5/42.6 38.0/40.7 41.0/41.9
Claimlength(Median/Avg) 45.0/45.9 24.0/24.5 46.0/46.4
#Textevidenceperclaim(Avg) 2.2 1.0 2.9
#Tableevidenceperclaim(Avg) 0.7 0.9 0.8
Explanationlength(Median/Avg) 70.0/73.1 74.0/76.2 96.0/100.7
Benchmarksize(#Claims)
testminisize 250 250 200
testsize 600 600 500
Table2: BasicstatisticsoftheFINDVERbenchmark.
“refuted”claimthroughexpertannotation. Specif- tracted information segment in a step-by-step
ically,expertannotatorsfirstcreatean“entailed” manner. For each step, they should elucidate the
claimusingthesameproceduredetailedinthe“En- associated reasoning. Finally, they annotate the
tailed Claim Annotation” paragraph. The anno- entailmentlabelfeature.
tatorsaretheninstructedtoperturbthe“entailed”
3.6 DataQualityValidation
claim to introduce factual errors that are directly
contradictedbytheannotatedevidence,andrewrite Toensurethehighqualityofourannotateddata,for
theannotatedreasoning-processexplanation. everyannotatedexample,wefirstusetheGPT-4o
modeltoproofreadandrefineboththeannotated
3.5 ExplanationAnnotation
statementandexplanation. Thenaqualifiedanno-
Afterfinishingtheclaimannotation,wepassitto tatorisassignedtovalidateseveralkeyaspects: (1)
anotherannotatorforexplanationannotation. The theclaimandreasoning-processexplanationshould
annotatorsarerequiredtofirstreadtheclaimcare- begrammaticallycorrectandfreeofspellingerrors;
fully and annotate a detailed explanation of the (2)theclaimshouldbecloselyrelatedtofinancial
reasoningprocess. Suchreasoning-processexpla- domains and meaningful in real-world scenarios;
nationsallowforagranularandinformativeevalua- (3) the annotated evidence should be relevant to
tionofmodeloutputs,helpingfutureworkidentify theclaimandcompleteenoughtoverifyit;(4)the
reasoningerrorsandprovidemoreaccurateerror entailmentlabeloftheclaimshouldbesupported
feedback. Wecomparetheentailmentlabelanno- bytheannotatedevidence;and(5)thereasoning-
tatedinthisstepwiththoseintheclaimannotation processexplanationshouldcorrectlyinterpretthe
step. A third annotator is introduced if the two extractedevidenceandapplyappropriatereasoning
annotation versions are different. In practice, we stepstocorrectlyverifytheclaim. Thevalidators
achieveaninter-annotatoragreementof90.3%for areaskedtoreviseexamplesthatdonotmeetthese
entailmentlabelannotation. standards. Inpractice,347outof2,400initialex-
Duringourpilotannotationphase,weobserved ampleswererevisedbythevalidators. Wealsore-
variability in the format of reasoning-process ex- portthehumanevaluationscoresover100sampled
planationannotatedbydifferentannotators,which examples. As illustrated in Table 6 in appendix,
madethedatasetlessstandardized. Toensurecon- FINDVERhasahighannotationquality.
sistency and clarity in our benchmark, we devel-
3.7 DatasetPreparationandRelease
opedapredefinedtemplateforannotatorstofollow.
Specifically,annotatorsarerequiredtocommence Table2presentsanoverviewoftheprimarystatis-
withtheextractionofrelevantinformationphase, ticsforourdataset. FINDVERisdividedintotwo
wheretheyneedtolistallclaim-relevantinforma- subsets: testminiandtest. Thetestminisubsetisde-
tion in a numbered list. Subsequently, they are signedspecificallyformodeldevelopmentandval-
required to annotate the reasoning over the ex- idation,whilethetestsubsetisreservedforformalevaluation. Tomitigatetheriskofdatacontamina- best-performingembeddingmodels(i.e.,OpenAI’s
tion(Jacovietal.,2023;Dengetal.,2024),wedo text-embedding-3-large) to retrieve the
notreleaseground-truth-relatedannotationfeatures top-10paragraphsortablesthataremostrelevantto
forthetest setpublicly. Instead, wewilldevelop theclaims. Theseelementsarethenconcatenated
andmaintainanonlineevaluationplatformwhere in their original order as found in the document
researcherscantesttheirmodelsandparticipatein beforebeingfedintothemodel.
apublicleaderboard.
4.3 ImplementationDetails
4 ExperimentSetup
InputTabularDataSerialization Buildingon
Wenextpresenttheexperimentalsetup,covering previous research that assessed LLMs on tasks
theevaluatedLLMs,long-contextandRAGsetups, involving tabular data (Chen, 2023; Zhao et al.,
implementation details, and the measurement of 2023b,c), weintroduceourmethodologyforpro-
human-levelperformance. cessing tables within documents. Our approach
encodes table structures by delineating column
4.1 ExperimentedLLMs headers and cell values with vertical bars (|) and
We examine the performance of LLMs across separating rows with line breaks. This flattened
twodistinctcategorieson FINDVER: (1)Propri- formatenablesthedirectinputoftabulardatainto
etaryLLMs,includingGPT-4o(OpenAI,2024), LLMs. Initial experiments with models such as
Gemini-1.5-Pro (Gemini, 2024), and Claude-3.5- Qwen-2.5andLlama-3.1demonstratetheirability
Sonnet (Anthropic, 2024); and (2) Open-source toeffectivelyinterpretthissimplifiedtableencod-
LLMs, including Llama-3.1&3.2 (Meta, 2024), ing. Nonetheless, we encourage future research
Qwen-2&2.5 (qwe, 2024; Team, 2024b), Mis- toexploremoreadvancedencodingtechniquesfor
tral & Mixtral & Mathstral & Ministral (Jiang tabular data to further improve LLM comprehen-
etal.,2023a,2024),InternLM2.5(Team,2024a), sionandperformanceoncomplextables.
DeepSeek-V2-Lite (DeepSeek-AI, 2024), and
Model Response Processing Following previ-
GLM (Du et al., 2022). Table 8 in Appendix
ous work (Lu et al., 2024), we adopt LLM for
presents the details of evaluated models (i.e., or-
processingmodelresponse. Specifically,weutilize
ganizations,releasetime,maxcontextlength,and
GPT-4o-minitoextractlabelsfromtheLLMout-
modelversion). Theexperimentswithopen-source
put,whichcanbeeither“entailed”,“refuted”or
LLMs use the vLLM framework (Kwon et al.,
“none”. The“none”labeltypicallyindicatesthat
2023). Forallexperiments,thetemperatureissetto
theLLMoutputcontainsnonsensicalsymbolsor
1.0,andthemaximumoutputlengthis512tokens.
unintelligibletextratherthanmeaningfulcontent.
We employ Chain-of-Thought (CoT) prompting
Incaseswheretheoutputislabeledas“none”,we
methods (Wei et al., 2022) for the main experi-
assignthefinallabelbymakingarandomguess.
ments. Inthisapproach,themodelfirstgenerates
a detailed reasoning process to verify each claim 4.4 Human-levelPerformanceMeasurement
andthenprovidesanentailmentlabelbasedonthis
To provide a rough but informative estimate of
reasoning. ThespecificpromptsusedforthisCoT
human-level performance by non-experts and ex-
methodologyareshowninFigure5inappendix.
pertson FINDVER,werandomlysampled5docu-
4.2 Long-ContextandRAGSettings ments×4claims/document=20claimsfromeach
validationsubset,totaling60claims. Weenrolltwo
AspresentedinTable2,thedocumentswithinour
experts(i.e.,professionalswithCFAlicense)and
benchmarkarenotablylengthy. Toeffectivelyhan-
twonon-experts(i.e.,undergraduatestudentsma-
dlethis,wehaveimplementedtworeal-worldap-
jored in computer science) to individually verify
plication settings that are widely recognized for
theclaimsbyprovidingtheNLexplanations. Ta-
their utility in dealing with extensive texts. For
ble3presentsthehuman-levelperformance.
Long-context Setting, we input the entire finan-
cial document into the model. We limit our eval-
5 ExperimentResults
uation to those models that have a context win-
dowlargerthan100,000tokens,whichexeedsthe This section discusses the experiment results on
maximum length of the included financial docu- FINDVER, including our main findings, ablation
ment. For RAG Setting, we leverage the current studies,anderroranalysis.FDV-IE FDV-MATH FDV-KNOW Average
Model Size
Testmini Test Testmini Test Testmini Test Testmini Test
HumanNon-Expert 90.0 85.0 85.0 86.7
HumanExpert 95.0 90.0 95.0 93.3
DeepSeek-V2-Lite 16B 60.4 57.7 64.0 58.5 56.0 58.8 60.1 58.3
Llama-3.2 3B 65.6 60.3 55.6 57.0 53.0 57.8 58.4 58.4
Mathstral 7B 66.4 61.2 58.0 59.8 59.0 62.0 61.3 60.9
Mistral-v0.3 7B 71.2 67.3 61.2 59.3 72.5 65.6 68.0 64.0
InternLM2.5 7B 71.2 70.2 61.6 56.8 65.0 66.2 66.0 64.3
Llama-3.1 8B 70.8 72.2 64.0 58.3 64.0 64.2 66.4 64.9
Qwen2 7B 72.8 69.3 63.6 61.2 65.5 69.4 67.4 66.5
Ministral 8B 74.4 70.5 64.8 62.8 63.5 66.8 67.8 66.7
GLM-4 9B 75.6 72.3 68.0 64.2 70.5 70.6 71.4 68.9
Qwen2.5 7B 77.2 71.5 68.4 68.2 71.5 71.2 72.4 70.2
Claude-3.5-Sonnet – 74.8 76.3 77.2 69.5 66.0 64.4 73.1 70.4
Gemini-1.5-Pro – 75.2 77.5 69.6 70.8 69.0 70.8 71.4 73.2
Llama-3.1 70B 78.8 78.8 66.8 66.2 80.5 79.2 75.0 74.5
Qwen2.5 72B 80.8 77.2 72.8 71.0 73.0 77.0 75.7 74.9
Mistral-Large 123B 78.4 78.3 72.4 73.5 73.5 75.6 74.8 75.8
GPT-4o – 78.0 80.7 74.0 71.3 73.5 76.8 75.3 76.2
Table3: AccuracyofentailmentclassificationontheFINDVERtestminiandtestsplits. Resultsarereportedfor
LLMsusingCoT promptingundertheRAGsetting. Theaverageaccuracyonthetestsetisusedasrankingindicator.
5.1 MainFindings RAG CoT Long-context CoT
Table 3 display the results for FINDVER under GPT-4o
the RAG setting. We observe a significant accu-
Qwen2.5-72B
racy gap between human experts and the evalu-
Qwen2.5-7B
atedLLMs. Notably,GPT-4o,thebest-performing
LLM,achievesanaverageaccuracyofonly76.2%, GLM-4-9B
whichismarkedlylowerthanthe93.3%accuracy Ministral-8B
achievedbyhumanexperts. Thisgapunderscores
Qwen2-7B
thecomplexityandchallengespresentedbyFIND-
Llama-3.1-8B
VER. Foropen-sourceLLMs,frontiermodelslike
Llama-3.1, Qwen2.5, and Mistral have achieved Llama-3.2-3B
performancelevelsonparwithproprietarycounter- 0 25 50 75 100
parts. Forexample,theLlama-3.1-70Bmodelsur-
Figure3: ComparisonofLLMperformanceonthetest-
passesGPT-4oonthe FDV-KNOW dataset. This
minisplitinlong-contextversusRAGsettingsusingthe
progress underscores the promising potential of
CoTpromptingmethod.
open-sourcemodelsforreasoning-intensivetasks
withinspecializeddomains.
theearlierQwen2-7Bmodelshowsapronounced
5.2 Long-ContextSettingAnalysis performance gap between the long-context and
RAGsettings;whilethelatestQwen2.5-7Bmodel
Wenextprovideananalysisofmodelperformance
significantly narrows this performance gap. This
in the long-context setting. Figure 3 presents a
suggeststhatrecentdevelopmentsinmodelarchi-
comparisonofmodelperformanceinlong-context
tectureandtrainingtechniquesareenhancingmod-
versusRAGsettings. FrontierLLMs,suchasGPT-
els’performanceinlong-contextscenarios.
4oandQwen2.5-72B,performbetterinthelong-
contextsetting,highlightingtheircapabilitytoef-
5.3 RAGSettingAnalysis
fectivelyhandleextendedinputlengths. Anencour-
agingtrendisalsoobservedintheprogressofmod- This subsection explores the impact of evidence
elsadaptedforlong-contextsettings. Forinstance, retrieval accuracy on the overall performance ofSetting n Recall Llama Mistral Qwen GPT-4o w/oCoT w/CoT
Model
Contriever 3 11.64 60.1 60.1 59.9 58.1 LongC RAG LongC RAG
BM25 3 31.96 67.7 67.3 69.6 65.7
GPT-4o 78.1(-4.3) 69.4(-5.9) 82.4 75.3
OAI-Large 3 34.92 66.0 66.9 65.9 64.3
Qwen2.5-7B 51.1(-17.9) 61.6(-10.8) 69.0 72.4
Contriever 5 17.99 61.9 62.6 63.4 60.4 Ministral-8B 49.6(-3.1) 59.4(-8.5) 52.7 67.8
BM25 5 38.62 71.9 73.1 73.1 71.6 Qwen2-7B 49.7(-3.9) 62.1(-5.3) 53.6 67.4
OAI-Large 5 41.79 70.6 69.4 71.6 70.9 Llama-3.1-8B 51.7(-15.2) 61.6(-4.8) 66.9 66.4
Llama-3.2-3B 48.1(-6.0) 50.6(-7.8) 54.1 58.4
Contriever 10 28.21 67.6 67.6 70.1 65.6
BM25 10 46.42 73.4 75.7 78.1 73.9
OAI-Large 10 51.65 75.0 74.8 75.7 75.3 Table 5: Comparison of LLM performance with and
Oracle — – 81.6 83.7 83.6 83.0 withoutCoTPromptingmethodsonthetestminiset.
Table 4: Performance comparison of various LLMs
across different RAG settings on the testmini set. roranalysiswithhumanevaluators. Werandomly
ThemodelsevaluatedincludeLlama-3.1-70B,Mistral-
select25instancesfromeachofthethreesubsets
Large,Qwen2.5-72B,andGPT-4o.
wheretheGPT-4omodelfailstoperformcorrectly.
Ouranalysishasidentifiedfourprimarycategories
LLMs. We assess LLM performance using three of errors: (1) Extraction error: The model fails
different retrieval methods, i.e., BM25 (Robert- to correctly locate the relevant context; addition-
sonetal.,1995),Contriever(Izacardetal.,2022), ally,itisalsolikelytoextractdataincorrectlyfrom
andOpenAI’stext-embedding-3-*models, thetableevenwhenitidentifiestherelevanttable.
across three retrieval sizes (k = 3,5,10). As Bothsituationsresultininaccurateverification. (2)
shown in Table 4, providing higher-quality evi- Numerical reasoning error: The model encoun-
dence,indicatedbyincreasedretrievalrecall,gen- tersdifficultieswithcorrectmathematicalreason-
erallyenhancesLLMperformanceintheRAGset- ing. (3)Domainknowledgedeficiency: Themodel
ting. Notably, the BM25 retriever achieves per- lackssufficientknowledgeinfinance-relatedareas,
formance levels comparable to OpenAI’s embed- whichhampersitsabilitytoreasonaccurately. (4)
dingmodel,whereastheContrievermodelsignif- Computationerror: Whilethemodel’sreasoning
icantlyunderperformsrelativetobothBM25and iscorrect,itmakescomputationalmistakesduring
OpenAI’s embedding models. Interestingly, de- intermediate or final steps, resulting in incorrect
spite BM25’s lower retrieval recall compared to claimverification.
OpenAI’s embedding model, the LLM generally While our analysis is restricted to small-scale
achieves higher overall performance with BM25. human evaluation, we believe that future work
This result highlights the robustness of effective- couldexploreadvancedautomatedevaluationmeth-
nessofterm-basedretrievalmethods. ods (Liu et al., 2023; Zheng et al., 2023; Kamoi
etal.,2024)fordetectingreasoningerrorswithin
5.4 Chain-of-ThoughtAnalysis thegeneratedexplanations.
To better understand the effectiveness of CoT
promptingmethodsforourtasks,weselectseveral 6 Conclusion
proprietaryandopen-sourceLLMsforexperiments.
Inthew/oCoTsetting,weinstructtheLLMstodi- This paper presents FINDVER, a comprehensive
rectlyoutputtheentailmentlabeloftheclaimusing benchmark designed to evaluate LLMs in claim
theprovideddocumentcontext(Thepromptused verificationoverlongandhybrid-contentfinancial
isprovidedinFigure6inappendix). Asillustrated documents. Throughextensiveexperimentsinvolv-
in Table 5, both LLMs’ performance degrades in ing16LLMsunderlong-contextandRAGsettings,
the w/o CoT setting. These results highlight the wehavedemonstratedthateventhetop-performing
importance of CoTreasoning in enhancing LLM modelsexhibitasignificantperformancegapcom-
performanceforourtask. pared to financial experts. Our detailed findings
andinsightsrevealthestrengthsandlimitationsof
5.5 ErrorAnalysisofReasoningProcess
current LLMs in this new task. We believe that
The GPT-4o model achieves a top accuracy of FINDVER providesavaluablebenchmarkforfu-
76.2%undertheRAGsetting. Tobetterunderstand tureresearchonLLMs’abilitytohandlecomplex
themodel’slimitations,weperformadetaileder- claimverificationtaskswithintheexpertdomain.Limitations Pepa Atanasova, Jakob Grue Simonsen, Christina Li-
oma,andIsabelleAugenstein.2020. Generatingfact
First, ourevaluationdoesnotincluderecentlyre- checkingexplanations. InProceedingsofthe58th
leased finance-specific LLMs (Wu et al., 2023; AnnualMeetingoftheAssociationforComputational
Linguistics,pages7352–7364.AssociationforCom-
Yang et al., 2023; Xie et al., 2023b, 2024b), as
putationalLinguistics.
thesemodelsarenotyetcompatiblewiththevLLM
frameworkusedforinference. Additionally,dueto JifanChen,AniruddhSriram,EunsolChoi,andGreg
computationalresourceconstraints,wedidnotper- Durrett.2022a. Generatingliteralandimpliedsub-
questionstofact-checkcomplexclaims. InProceed-
formlarge-scalefine-tuningofLLMsonfinance-
ingsofthe2022ConferenceonEmpiricalMethods
domain data. However, we believe that training inNaturalLanguageProcessing,pages3495–3516.
on such domain-specific data could significantly AssociationforComputationalLinguistics.
enhance LLM performance in FINDVER, partic-
WenhuChen.2023. Largelanguagemodelsarefew(1)-
ularly in terms of accuracy on the FDV-KNOW.
shottablereasoners. InFindingsoftheAssociation
Moreover,weonlyconducthumanerroranalysis forComputationalLinguistics: EACL2023, pages
onthegeneratedreasoningprocessofmodels. We 1120–1130.AssociationforComputationalLinguis-
believefutureworkcouldexplorethedevelopment tics.
ofLLM-basedautomatedevaluationsystems(Liu
WenhuChen, HongminWang, JianshuChen, Yunkai
etal.,2023;Zhengetal.,2023;Kamoietal.,2024) Zhang, HongWang, ShiyangLi, XiyouZhou, and
forautomaticallydetectingreasoningerrorswithin WilliamYangWang.2020. Tabfact: Alarge-scale
datasetfortable-basedfactverification. InInterna-
thegeneratedexplanation.
tionalConferenceonLearningRepresentations.
Acknowledgements
ZhiyuChen,WenhuChen,ChareseSmiley,Sameena
Shah,IanaBorova,DylanLangdon,ReemaMoussa,
Wearegratefulforthecomputesupportprovided MattBeane,Ting-HaoHuang,BryanRoutledge,and
bytheMicrosoftResearch’sAFMRprogramand William Yang Wang. 2021. FinQA: A dataset of
TogetherAI3. numericalreasoningoverfinancialdata. InProceed-
ingsofthe2021ConferenceonEmpiricalMethods
inNaturalLanguageProcessing,pages3697–3711.
AssociationforComputationalLinguistics.
References
Zhiyu Chen, Shiyang Li, Charese Smiley, Zhiqiang
2024. Qwen2technicalreport.
Ma,SameenaShah,andWilliamYangWang.2022b.
ConvFinQA:Exploringthechainofnumericalrea-
MubasharaAkhtar,OanaCocarascu,andElenaSimperl.
soninginconversationalfinancequestionanswering.
2022. PubHealthTab: A public health table-based
In Proceedings of the 2022 Conference on Empiri-
dataset for evidence-based fact checking. In Find-
calMethodsinNaturalLanguageProcessing,pages
ingsoftheAssociationforComputationalLinguistics:
6279–6292.AssociationforComputationalLinguis-
NAACL2022,pages1–16.AssociationforComputa-
tics.
tionalLinguistics.
DeepSeek-AI.2024. Deepseek-v2: Astrong,economi-
RamiAly,ZhijiangGuo,MichaelSchlichtkrull,James cal,andefficientmixture-of-expertslanguagemodel.
Thorne,AndreasVlachos,ChristosChristodoulopou-
los,OanaCocarascu,andArpitMittal.2021. Fever- ChunyuanDeng,YilunZhao,YuzhaoHeng,YitongLi,
ous: Fact extraction and verification over unstruc- JiannanCao,XiangruTang,andArmanCohan.2024.
turedandstructuredinformation. InProceedingsof Unveilingthespectrumofdatacontaminationinlan-
theNeuralInformationProcessingSystemsTrackon guagemodel:Asurveyfromdetectiontoremediation.
DatasetsandBenchmarks,volume1. InFindingsoftheAssociationforComputationalLin-
guisticsACL2024,pages16078–16092.Association
Anthropic. 2024. Introducing the next generation of forComputationalLinguistics.
claude.
Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding,
Pepa Atanasova, Oana-Maria Camburu, Christina Li- JiezhongQiu,ZhilinYang,andJieTang.2022. GLM:
oma, Thomas Lukasiewicz, Jakob Grue Simonsen, Generallanguagemodelpretrainingwithautoregres-
andIsabelleAugenstein.2023. Faithfulnesstestsfor siveblankinfilling. InProceedingsofthe60thAn-
naturallanguageexplanations. InProceedingsofthe nualMeetingoftheAssociationforComputational
61stAnnualMeetingoftheAssociationforCompu- Linguistics(Volume1: LongPapers),pages320–335.
tationalLinguistics(Volume2: ShortPapers),pages AssociationforComputationalLinguistics.
283–294.AssociationforComputationalLinguistics.
Gemini.2024. Gemini1.5: Unlockingmultimodalun-
3https://www.together.ai/ derstandingacrossmillionsoftokensofcontext.MaxGlockner,IevaStaliu¯naite˙,JamesThorne,Gisela Qin, ArmanCohan, WenpengYin, andRuiZhang.
Vallejo,AndreasVlachos,andIrynaGurevych.2024. 2024. EvaluatingLLMsatdetectingerrorsinLLM
AmbiFC:Fact-checkingambiguousclaimswithevi- responses. InFirstConferenceonLanguageModel-
dence. TransactionsoftheAssociationforComputa- ing.
tionalLinguistics,12:1–18.
RyoKamoi,TanyaGoyal,JuanDiegoRodriguez,and
VivekGupta,MaitreyMehta,PegahNokhiz,andVivek GregDurrett.2023. WiCE:Real-worldentailment
Srikumar.2020. INFOTABS:Inferenceontablesas forclaimsinWikipedia. InProceedingsofthe2023
semi-structureddata. InProceedingsofthe58thAn- Conference on Empirical Methods in Natural Lan-
nualMeetingoftheAssociationforComputational guageProcessing,pages7561–7583.Associationfor
Linguistics,pages2309–2324.AssociationforCom- ComputationalLinguistics.
putationalLinguistics.
Rik Koncel-Kedziorski, Michael Krumdick, Viet Lai,
GautierIzacard,MathildeCaron,LucasHosseini,Sebas- VarshiniReddy,CharlesLovering,andChrisTanner.
tianRiedel,PiotrBojanowski,ArmandJoulin,and 2024. Bizbench: A quantitative reasoning bench-
EdouardGrave.2022. Unsuperviseddenseinforma- markforbusinessandfinance.
tionretrievalwithcontrastivelearning. Transactions
onMachineLearningResearch. YutaKoreeda,Manning,andChristopher.2021. Con-
tractNLI:Adatasetfordocument-levelnaturallan-
AlonJacovi,AviCaciularu,OmerGoldman,andYoav guage inference for contracts. In Findings of the
Goldberg. 2023. Stop uploading test data in plain AssociationforComputationalLinguistics: EMNLP
text: Practicalstrategiesformitigatingdatacontam- 2021,pages1907–1919,PuntaCana,DominicanRe-
inationbyevaluationbenchmarks. InProceedings public.AssociationforComputationalLinguistics.
of the 2023 Conference on Empirical Methods in
NaturalLanguageProcessing,pages5075–5084.As- Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying
sociationforComputationalLinguistics. Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E.
Gonzalez, Hao Zhang, and Ion Stoica. 2023. Effi-
Albert Q Jiang, Alexandre Sablayrolles, Arthur Men- cientmemorymanagementforlargelanguagemodel
sch,ChrisBamford,DevendraSinghChaplot,Diego servingwithpagedattention. InProceedingsofthe
delasCasas,FlorianBressand,GiannaLengyel,Guil- ACMSIGOPS29thSymposiumonOperatingSystems
laumeLample,LucileSaulnier,etal.2023a. Mistral Principles.
7b. arXivpreprintarXiv:2310.06825.
Chenying Li, Wenbo Ye, and Yilun Zhao. 2022. Fin-
Albert Q. Jiang, Alexandre Sablayrolles, Antoine Math: Injectingatree-structuredsolverforquestion
Roux, Arthur Mensch, Blanche Savary, Chris answeringoverfinancialreports. InProceedingsof
Bamford, Devendra Singh Chaplot, Diego de las theThirteenthLanguageResourcesandEvaluation
Casas, Emma Bou Hanna, Florian Bressand, Gi- Conference,pages6147–6152.EuropeanLanguage
anna Lengyel, Guillaume Bour, Guillaume Lam- ResourcesAssociation.
ple, Lélio Renard Lavaud, Lucile Saulnier, Marie-
AnneLachaux,PierreStock,SandeepSubramanian, ShuaiqiLiu,JiannongCao,RuosongYang,andZhiyuan
Sophia Yang, Szymon Antoniak, Teven Le Scao, Wen.2022. Longtextandmulti-tablesummarization:
Théophile Gervet, Thibaut Lavril, Thomas Wang, Datasetandmethod. InFindingsoftheAssociation
TimothéeLacroix,andWilliamElSayed.2024. Mix- forComputationalLinguistics: EMNLP2022,pages
tralofexperts. 1995–2010.AssociationforComputationalLinguis-
tics.
AlbertQiaochuJiang,AlexandreSablayrolles,Arthur
Mensch, Chris Bamford, Devendra Singh Chap- Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang,
lot, Diego de Las Casas, Florian Bressand, Gi- Ruochen Xu, and Chenguang Zhu. 2023. G-eval:
annaLengyel, GuillaumeLample, LucileSaulnier, NLGevaluationusinggpt-4withbetterhumanalign-
L’elioRenardLavaud,Marie-AnneLachaux,Pierre ment. In Proceedings of the 2023 Conference on
Stock,TevenLeScao,ThibautLavril,ThomasWang, EmpiricalMethodsinNaturalLanguageProcessing,
Timothée Lacroix, and William El Sayed. 2023b. pages 2511–2522. Association for Computational
Mistral7b. ArXiv,abs/2310.06825. Linguistics.
Yichen Jiang, Shikha Bordia, Zheng Zhong, Charles PanLu,HritikBansal,TonyXia,JiachengLiu,Chun-
Dognin, Maneesh Singh, and Mohit Bansal. 2020. yuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-
HoVer: Adatasetformany-hopfactextractionand WeiChang,MichelGalley,andJianfengGao.2024.
claim verification. In Findings of the Association Mathvista: Evaluating mathematical reasoning of
forComputationalLinguistics: EMNLP2020,pages foundation models in visual contexts. In Inter-
3441–3460.AssociationforComputationalLinguis- national Conference on Learning Representations
tics. (ICLR).
RyoKamoi, SarkarSnigdhaSarathiDas, RenzeLou, XinyuanLu,LiangmingPan,QianLiu,PreslavNakov,
Jihyun Janice Ahn, Yilun Zhao, Xiaoxin Lu, Nan and Min-Yen Kan. 2023. SCITAB: A challenging
Zhang, Yusen Zhang, Haoran Ranran Zhang, Su- benchmark for compositional reasoning and claim
jeeth Reddy Vummanthala, Salika Dave, Shaobo verificationonscientifictables. InProceedingsofthe2023ConferenceonEmpiricalMethodsinNatural YejunSoun,JaeminYoo,MinyongCho,JihyeongJeon,
LanguageProcessing,pages7787–7813.Association andUKang.2022. Accuratestockmovementpredic-
forComputationalLinguistics. tionwithself-supervisedlearningfromsparsenoisy
tweets. In2022IEEEInternationalConferenceon
HuanhuanMa,WeizhiXu,YifanWei,LiujiChen,Liang BigData(BigData),pages1691–1700.
Wang,QiangLiu,ShuWu,andLiangWang.2024.
Ex-fever: A dataset for multi-hop explainable fact Liyan Tang, Philippe Laban, and Greg Durrett. 2024.
verification. Minicheck:Efficientfact-checkingofllmsonground-
ingdocuments.
Macedo Maia, Siegfried Handschuh, André Freitas,
InternLMTeam.2024a. Internlm2technicalreport.
BrianDavis,RossMcDermott,ManelZarrouk,and
AlexandraBalahur.2018. Www’18openchallenge:
QwenTeam.2024b. Qwen2.5: Apartyoffoundation
Financialopinionminingandquestionanswering. In
models.
CompanionProceedingsoftheTheWebConference
2018, WWW ’18, page 1941–1942, Republic and
James Thorne, Andreas Vlachos, Christos
CantonofGeneva,CHE.InternationalWorldWide
Christodoulopoulos, and Arpit Mittal. 2018.
WebConferencesSteeringCommittee.
FEVER: a large-scale dataset for fact extraction
and VERification. In Proceedings of the 2018
PekkaMalo,AnkurSinha,PyryTakala,PekkaKorho-
Conference of the North American Chapter of the
nen, andJyrkiWallenius.2013. Gooddebtorbad
AssociationforComputationalLinguistics: Human
debt: Detectingsemanticorientationsineconomic
Language Technologies, Volume 1 (Long Papers),
texts.
pages 809–819. Association for Computational
Linguistics.
AI Meta. 2024. Introducing meta llama 3: The most
capableopenlyavailablellmtodate. MetaAI. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
RajdeepMukherjee,AbhinavBohra,AkashBanerjee, Bashlykov,SoumyaBatra,PrajjwalBhargava,Shruti
SoumyaSharma,ManjunathHegde,AfreenShaikh, Bhosale,DanBikel,LukasBlecher,CristianCanton
ShivaniShrivastava,KoustuvDasgupta,NiloyGan- Ferrer,MoyaChen,GuillemCucurull,DavidEsiobu,
guly,SaptarshiGhosh,andPawanGoyal.2022. ECT- JudeFernandes,JeremyFu,WenyinFu,BrianFuller,
Sum: Anewbenchmarkdatasetforbulletpointsum- CynthiaGao,VedanujGoswami,NamanGoyal,An-
marizationoflongearningscalltranscripts. InPro- thonyHartshorn,SagharHosseini,RuiHou,Hakan
ceedingsofthe2022ConferenceonEmpiricalMeth- Inan,MarcinKardas,ViktorKerkez,MadianKhabsa,
odsinNaturalLanguageProcessing,pages10893– IsabelKloumann,ArtemKorenev,PunitSinghKoura,
10906.AssociationforComputationalLinguistics. Marie-AnneLachaux,ThibautLavril,JenyaLee,Di-
anaLiskovich,YinghaiLu,YuningMao,XavierMar-
OpenAI. 2023. Gpt-4 technical report. ArXiv, tinet,TodorMihaylov,PushkarMishra,IgorMoly-
abs/2303.08774. bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-
stein,RashiRungta,KalyanSaladi,AlanSchelten,
OpenAI.2024. Hellogpt-4o. Ruan Silva, Eric Michael Smith, Ranjan Subrama-
nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-
AmanRangapur,HaoranWang,LingJian,andKaiShu. lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
2024. Fin-fact: Abenchmarkdatasetformultimodal ZhengYan,IliyanZarov,YuchenZhang,AngelaFan,
financialfactcheckingandexplanationgeneration. Melanie Kambadur, Sharan Narang, Aurelien Ro-
driguez,RobertStojnic,SergeyEdunov,andThomas
Stephen E Robertson, Steve Walker, Susan Jones, Scialom.2023a. Llama2: Openfoundationandfine-
MichelineMHancock-Beaulieu,MikeGatford,etal. tunedchatmodels.
1995. Okapiattrec-3. NistSpecialPublicationSp,
109:109. Hugo Touvron, Louis Martin, Kevin R. Stone, Peter
Albert, Amjad Almahairi, Yasmine Babaei, Niko-
Daniel Russo, Serra Sinem Tekirog˘lu, and Marco lay Bashlykov, Soumya Batra, Prajjwal Bhargava,
Guerini.2023. Benchmarkingthegenerationoffact ShrutiBhosale,DanielM.Bikel,LukasBlecher,Cris-
checkingexplanations. TransactionsoftheAssocia- tianCantonFerrer, MoyaChen, GuillemCucurull,
tionforComputationalLinguistics,11:1250–1264. DavidEsiobu,JudeFernandes,JeremyFu,Wenyin
Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami,
JulioCesarSalinasAlvarado,KarinVerspoor,andTimo- NamanGoyal, AnthonyS.Hartshorn, SagharHos-
thyBaldwin.2015. Domainadaptionofnamedentity seini,RuiHou,HakanInan,MarcinKardas,Viktor
recognitiontosupportcreditriskassessment. InPro- Kerkez,MadianKhabsa,IsabelM.Kloumann,A.V.
ceedingsoftheAustralasianLanguageTechnology Korenev,PunitSinghKoura,Marie-AnneLachaux,
AssociationWorkshop2015,pages84–90. ThibautLavril,JenyaLee,DianaLiskovich,Yinghai
Lu,YuningMao,XavierMartinet,TodorMihaylov,
Agam Shah, RuchitVithani, Abhinav Gullapalli, and PushkarMishra,IgorMolybog,YixinNie,Andrew
SudheerChava.2023. Finer: Financialnamedentity Poulton,JeremyReizenstein,RashiRungta,Kalyan
recognitiondatasetandweak-supervisionmodel. Saladi, Alan Schelten, Ruan Silva, Eric MichaelSmith,R.Subramanian,XiaTan,BinhTang,Ross zero-shotanalysisofchatgptovermultimodalstock
Taylor, Adina Williams, Jian Xiang Kuan, Puxin movementpredictionchallenges.
Xu,ZhengxuYan,IliyanZarov,YuchenZhang,An-
gelaFan,MelanieKambadur,SharanNarang,Aure- Qianqian Xie, Weiguang Han, Xiao Zhang, Yanzhao
lienRodriguez,RobertStojnic,SergeyEdunov,and Lai, Min Peng, Alejandro Lopez-Lira, and Jimin
ThomasScialom.2023b. Llama2: Openfoundation Huang.2023b. Pixiu: Alargelanguagemodel, in-
andfine-tunedchatmodels. structiondataandevaluationbenchmarkforfinance.
Andreas Vlachos and Sebastian Riedel. 2014. Fact Qianqian Xie, Dong Li, Mengxi Xiao, Zihao Jiang,
checking: Taskdefinitionanddatasetconstruction. Ruoyu Xiang, Xiao Zhang, Zhengyu Chen, Yueru
InProceedingsoftheACL2014WorkshoponLan- He,WeiguangHan,YuzheYang,ShunianChen,Yifei
guageTechnologiesandComputationalSocialSci- Zhang,LihangShen,DanielKim,ZhiweiLiu,Zhe-
ence, pages18–22.AssociationforComputational hengLuo,YangyangYu,YupengCao,ZhiyangDeng,
Linguistics. Zhiyuan Yao, Haohang Li, Duanyu Feng, Yongfu
Dai,VijayaSaiSomasundaram,PengLu,YilunZhao,
David Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu YitaoLong,GuojunXiong,KalebSmith,Honghai
Wang, Madeleine van Zuylen, Arman Cohan, and Yu,YanzhaoLai,MinPeng,JianyunNie,JordanW.
HannanehHajishirzi.2020. Factorfiction:Verifying Suchow,Xiao-YangLiu,BenyouWang,Alejandro
scientific claims. In Proceedings of the 2020 Con- Lopez-Lira, Jimin Huang, and Sophia Ananiadou.
ferenceonEmpiricalMethodsinNaturalLanguage 2024b. Open-finllms: Open multimodal large lan-
Processing(EMNLP),pages7534–7550.Association guagemodelsforfinancialapplications.
forComputationalLinguistics.
YumoXuandShayB.Cohen.2018. Stockmovement
DavidWadden,KyleLo,BaileyKuehl,ArmanCohan, predictionfromtweetsandhistoricalprices. InPro-
IzBeltagy,LucyLuWang,andHannanehHajishirzi. ceedingsofthe56thAnnualMeetingoftheAssocia-
2022. SciFact-open: Towardsopen-domainscientific tionforComputationalLinguistics(Volume1: Long
claim verification. In Findings of the Association Papers),pages1970–1979.AssociationforCompu-
forComputationalLinguistics: EMNLP2022,pages tationalLinguistics.
4719–4734.AssociationforComputationalLinguis-
tics. Hongyang Yang, Xiao-Yang Liu, and Christina Dan
Wang.2023. Fingpt: Open-sourcefinanciallargelan-
JasonWei,XuezhiWang,DaleSchuurmans,Maarten guagemodels. FinLLMSymposiumatIJCAI2023.
Bosma,brianichter,FeiXia,EdH.Chi,QuocVLe,
and Denny Zhou. 2022. Chain of thought prompt- Wenpeng Yin, Dragomir Radev, and Caiming Xiong.
ing elicits reasoning in large language models. In 2021. DocNLI:Alarge-scaledatasetfordocument-
AdvancesinNeuralInformationProcessingSystems. levelnaturallanguageinference. InFindingsofthe
Association for Computational Linguistics: ACL-
HuizheWu,WeiZhang,WeiweiShen,andJunWang. IJCNLP 2021, pages 4913–4922. Association for
2018. Hybrid deep sequential modeling for so- ComputationalLinguistics.
cialtext-drivenstockprediction. InProceedingsof
the27thACMInternationalConferenceonInforma- YilunZhao,YunxiangLi,ChenyingLi,andRuiZhang.
tionandKnowledgeManagement,CIKM’18,page 2022. MultiHiertt: Numericalreasoningovermulti
1627–1630, New York, NY, USA. Association for hierarchicaltabularandtextualdata. InProceedings
ComputingMachinery. of the 60th Annual Meeting of the Association for
ComputationalLinguistics(Volume1: LongPapers),
ShijieWu,OzanIrsoy,StevenLu,VadimDabravolski, pages 6588–6600. Association for Computational
MarkDredze,SebastianGehrmann,PrabhanjanKam- Linguistics.
badur, David Rosenberg, and Gideon Mann. 2023.
Bloomberggpt: Alargelanguagemodelforfinance. YilunZhao,HongjunLiu,YitaoLong,RuiZhang,Chen
ArXiv,abs/2303.17564. Zhao,andArmanCohan.2024a. KnowledgeFMath:
A knowledge-intensive math reasoning dataset in
QianqianXie,WeiguangHan,ZhengyuChen,Ruoyu finance domains. In Proceedings of the 62nd An-
Xiang,XiaoZhang,YueruHe,MengxiXiao,Dong nualMeetingoftheAssociationforComputational
Li,YongfuDai,DuanyuFeng,YijingXu,Haoqiang Linguistics(Volume1: LongPapers),pages12841–
Kang, Ziyan Kuang, Chenhan Yuan, Kailai Yang, 12858.AssociationforComputationalLinguistics.
Zheheng Luo, Tianlin Zhang, Zhiwei Liu, Guojun
Xiong,ZhiyangDeng,YuechenJiang,ZhiyuanYao, Yilun Zhao, Yitao Long, Hongjun Liu, Ryo Kamoi,
HaohangLi,YangyangYu,GangHu,JiajiaHuang, Linyong Nan, Lyuhao Chen, Yixin Liu, Xian-
Xiao-YangLiu,AlejandroLopez-Lira,BenyouWang, gru Tang, Rui Zhang, and Arman Cohan. 2024b.
YanzhaoLai,HaoWang,MinPeng,SophiaAnani- DocMath-eval: Evaluatingmathreasoningcapabili-
adou,andJiminHuang.2024a. Finben: Aholistic tiesofLLMsinunderstandingfinancialdocuments.
financialbenchmarkforlargelanguagemodels. In Proceedings of the 62nd Annual Meeting of the
AssociationforComputationalLinguistics(Volume
QianqianXie,WeiguangHan,YanzhaoLai,MinPeng, 1: LongPapers),pages16103–16120.Association
andJiminHuang.2023a. Thewallstreetneophyte:A forComputationalLinguistics.Yilun Zhao, Yitao Long, Hongjun Liu, Linyong Nan,
LyuhaoChen,RyoKamoi,YixinLiu,XiangruTang,
RuiZhang,andArmanCohan.2023a. Docmath-eval:
Evaluatingnumericalreasoningcapabilitiesofllms
inunderstandinglongdocumentswithtabulardata.
YilunZhao,ZhentingQi,LinyongNan,BoyuMi,Yixin
Liu,WeijinZou,SimengHan,RuizheChen,Xian-
gru Tang, Yumo Xu, Dragomir Radev, and Arman
Cohan. 2023b. QTSumm: Query-focused summa-
rizationovertabulardata. InProceedingsofthe2023
Conference on Empirical Methods in Natural Lan-
guageProcessing,pages1157–1172.Associationfor
ComputationalLinguistics.
YilunZhao,HaoweiZhang,ShengyunSi,LinyongNan,
Xiangru Tang, and Arman Cohan. 2023c. Investi-
gatingtable-to-textgenerationcapabilitiesoflarge
languagemodelsinreal-worldinformationseeking
scenarios. In Proceedings of the 2023 Conference
onEmpiricalMethodsinNaturalLanguageProcess-
ing: IndustryTrack,pages160–175.Associationfor
ComputationalLinguistics.
Yilun Zhao, Chen Zhao, Linyong Nan, Zhenting
Qi, Wenlin Zhang, Xiangru Tang, Boyu Mi, and
DragomirRadev.2023d. RobuT:Asystematicstudy
oftableQArobustnessagainsthuman-annotatedad-
versarial perturbations. In Proceedings of the 61st
AnnualMeetingoftheAssociationforComputational
Linguistics (Volume 1: Long Papers), pages 6064–
6081.AssociationforComputationalLinguistics.
LianminZheng,Wei-LinChiang,YingSheng,Siyuan
Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,
Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang,
JosephE.Gonzalez,andIonStoica.2023. Judging
LLM-as-a-judgewithMT-benchandchatbotarena.
InThirty-seventhConferenceonNeuralInformation
ProcessingSystemsDatasetsandBenchmarksTrack.
Zhihan Zhou, Liqian Ma, and Han Liu. 2021. Trade
theevent: Corporateeventsdetectionfornews-based
event-driventrading. InFindingsoftheAssociation
forComputationalLinguistics: ACL-IJCNLP2021,
pages 2114–2124. Association for Computational
Linguistics.
FengbinZhu,WenqiangLei,YouchengHuang,Chao
Wang,ShuoZhang,JianchengLv,FuliFeng,andTat-
SengChua.2021. TAT-QA:Aquestionanswering
benchmark on a hybrid of tabular and textual con-
tentinfinance. InProceedingsofthe59thAnnual
Meeting of the Association for Computational Lin-
guisticsandthe11thInternationalJointConference
onNaturalLanguageProcessing(Volume1: Long
Papers),pages3277–3287.AssociationforCompu-
tationalLinguistics.A Appendix
AnexamplewithinFINDVERtestminiset AdoptedChain-of-ThoughtPrompt
[Claim] [SystemInput]
Thenetinterestexpensein2023,consideringdebt As a financial expert, your task is to assess the
interestandcapitalizedinterest,was$183,479,000. truthfulness of the given claim by determining
whetheritisentailedorrefutedbasedontheprovided
[SupportingEvidence(ContextIndex)] financialdocument.Followthesesteps:
107 1.Carefullyreadthegivencontextandtheclaim.
2. Analyzethedocument,focusingontherelevant
[ExplanationofReasoningProcess] financialdataorfactsthatrelatedtotheclaim.
1. Fromtable107,thedebtinterestpaidin2023is 3.Documenteachstepofyourreasoningprocessto
given as $183,479,000 and the capitalized interest ensureyourassessmentisclearandthorough.
amountis$-2,483,000). 4.Concludeyouranalysiswithafinaldetermination.
2. We can calculate the net interest expense as Inyourlastsentence,clearlystateyourconclusion
183479-2483=180996. in the following format: "Therefore, the claim is
3.Therefore,thestatementisrefuted. {entailment_label}." Replace {entailment_label}
witheither’entailed’(iftheclaimissupportedbythe
document) or ’refuted’ (if the claim contradicts or
partiallycontradictsthedocument).
Figure4: AnexamplewithinFINDVERtestminiset
[UserInput]
FinancialReport:
AnnotationQuality %S≥4
{FinancialReport}
Claim
Claimtoverify:
Fluency 92
{Claim}
Meaningfulness 90
Alignmentwithreal-worldscenarios 94
Follow the instructions and think step by step
Evidence toverifytheclaim.
Relevancy 89
Completeness 85
Figure5: TheChain-of-Thoughtpromptused.
Reasoning-processExplanation
Fluency 95
Correctness 92
Comprehensiveness 90
EntailmentLabel
Correctness 94
Table6: Humanevaluationover100samplesfromthe AdoptedChain-of-ThoughtPrompt
FINDVER testmini set. Two internal evaluators were
[SystemInput]
askedtoratethesamplesonascaleof1to5individually.
As a financial expert, your task is to assess the
Wereportpercentofsamplesthathaveanaveragescore truthfulnessofthegivenstatementbydetermining
≥4toindicatetheannotationqualityofFINDVER. whetheritisentailedorrefutedbasedontheprovided
financialdocument. Youshoulddirectlyoutputthe
entailmentlabel(‘entailed’or‘refuted’)withoutany
intermediatesteps.
[UserInput]
FinancialReport:
{FinancialReport}
Claimtoverify:
{Claim}
Directly output the entailment label (‘entailed’ or
‘refuted’)oftheclaim.
Figure6: TheDirectOutputpromptused.ID FinanceIndustryExperience EnglishProficiency AnnotationSets Evaluator?
1 1workingand1internshipatUS Nativespeaker FDV-KNOW ✓
2 >=2internshipatUS >15years FDV-MATH ✓
3 1workingatSingaporeand2internshipatUS Nativespeaker FDV-KNOW ✓
4 2workingand>=1internshipatUS Nativespeaker FDV-KNOW ✗
5 1internshipatUS,2internshipatChina 10years FDV-IE ✗
6 1internshipsatHK,China 15years FDV-IE,FDV-MATH ✓
7 1internshipsatChina 10years FDV-IE,FDV-MATH ✗
Table7: Detailsofannotatorsinvolvedindatasetconstruction. FINDVERisannotatedbyfinancialprofessionals
withextensiveexperienceincomprehendingfinancialdocuments, ensuringitaccuratelyreflectsthereal-world
challengesinthefinancialdomain.
Model Organization ReleaseTime MaxLength Source
GPT-4o(OpenAI,2023) OpenAI 2024-03 128k gpt-4o-2024-08-06
Gemini-1.5-Pro(Gemini,2024) Google 2024-02 128k
Claude-3.5-Sonnet (Anthropic, Anthropic 2024-03 200k claude-3-5-sonnet-20241022
2024)
Llama-3.1(Touvronetal.,2023a) Meta 2023-06 128k meta-llama/Llama-3.1-*B-Instruct
Llama-3.2(Meta,2024) Meta 2024-09 128k meta-llama/Llama-3.2-3B-Instruct
Qwen-2(qwe,2024) Qwen 2024-06 128k Qwen/Qwen2-*-Instruct
Qwen-2.5(Team,2024b) Qwen 2024-09 128k Qwen/Qwen2.5-*B-Instruct
Mistral(Jiangetal.,2023a,2024) MistralAI 2024-05 32k mistralai/Mistral-7B-Instruct-v0.3
Mathstral MistralAI 2024-08 32k mistralai/Mathstral-7B-v0.1
Ministral MistralAI 2024-10 128k mistralai/Ministral-8B-Instruct-2410
InternLM2.5(Team,2024a) internlm 2024-08 200k internlm/internlm2_5-7b-chat
DeepSeek-V2(DeepSeek-AI,2024) deepseekAI 2024-05 128k deepseek-ai/DeepSeek-V2-Lite-Chat
GLM(Duetal.,2022) THUDM 2024-06 128k THUDM/glm-4-9b-chat
Table8:Detailsoftheorganization,releasetime,maximumcontextlength,andmodelsource(i.e.,urlforproprietary
modelsandHuggingfacemodelnameforopen-sourcemodels)fortheLLMsevaluatedinFINDVER.