Graph-Dictionary Signal Model for Sparse Representations of
Multivariate Data
William Cappelletti Pascal Frossard
LTS4, EPFL, Lausanne, Switzerland
Abstract that can provide an explanation of the relational infor-
mation between different sample variables, to identify
the role of each source and describe the distribution
Representing and exploiting multivariate sig-
of their outcomes. Dictionary learning (Rubinstein
nals require capturing complex relations be-
et al., 2010) is a representation learning framework
tween variables. We define a novel Graph-
which aims to recover a finite set of atoms, called a
Dictionary signal model, where a finite set
dictionary, that can encode data through sparse coef-
of graphs characterizes relationships in data
ficients. On the other hand, graphs provide a natural
distribution through a weighted sum of their
representation of structured data with pairwise rela-
Laplacians. We propose a framework to in-
tionships, described by edges between nodes. However,
fer the graph dictionary representation from
these relationships are often hidden, and we must infer
observed data, along with a bilinear general-
them from data.
ization of the primal-dual splitting algorithm
to solve the learning problem. Our new for- In this work, we introduce a novel Graph-Dictionary
mulation allows to include a priori knowledge signalmodel,GraphDict inshort,tojointlyaddressthe
on signal properties, as well as on underly- representationandthestructurelearningproblems. Fol-
ing graphs and their coefficients. We show lowing the Graph Structure Learning literature (Dong
the capability of our method to reconstruct et al., 2019; Mateos et al., 2019), we use signal models
graphs from signals in multiple synthetic set- to link each N-variate signal sample x ∈ RN to an
t
tings, where our model outperforms previous underlying graph G . Our novel approach, illustrated
t
baselines. Then, we exploit graph-dictionary in Fig. 1, uses a dictionary of graphs to characterize
representations in a motor imagery decoding each G as the linear combination of K graph atoms
t
task on brain activity data, where we clas- with respective coefficients δ ∈ [0,1]K. The coeffi-
t
sify imagined motion better than standard cient vector corresponding to each sample provides
methods relying on many more features. an embedding which is inherently explainable. More
precisely, each entry describes the contribution of its
relative graph atom, with edges defining instantaneous
1 INTRODUCTION relationships that generated the observed signal. We
define a novel generalization of the Primal-Dual Split-
ting algorithm, which leverages the adjoint functions
Multivariatesignalsrepresentjointmeasurementsfrom
of a bilinear operator (Arens, 1951), to jointly learn
multiple sources that commonly arise in different fields.
atoms and coefficients.
Theymightrepresentelectricalpotentialsgeneratedby
brain activity, as well as stock values in the financial Then, we present a series of experiments to test the ca-
market, temperatures across weather stations, or even pabilitiesofourmodel. Intwoexperimentsonsynthetic
trafficdatainroadnetworks. Generally,westudythese data, we show that GraphDict outperforms literature
signalstocharacterizethesystemonwhichwemeasure baselines in reconstructing instantaneous graphs from
them, for instance to classify brain activity for motor signals. Finally, we exploit the sparse representations
imagery analysis, or to identify congestions on road given by GraphDict coefficients in a motor imagery
networks. Weareinterestedineffectiverepresentations decoding task on brain activity data. We show that
withfewfeaturesonthreeconnectivityatomsidentified
by our model, we classify imagined motion better than
brain-state models from the neuroscience literature.
Preliminary work under review.
4202
voN
8
]GL.sc[
1v92750.1142:viXraGraph-Dictionary Signal Model
Atoms 2 RELATED WORK
Using dictionaries to characterize graph signals was
proposedbyThanouetal.(2013),whosuggestlearning
dictionariesofstructuredgraphkernelsfromsignals. In
that setting, the graph over which they observe signals
G G G G
1 2 3 4 is known, and the atoms of the dictionary represent
different graph filters on its edges. In our method,
Instantaneous graphs
atomsaredistinctgraphs,andwelearnedgesexplicitly.
Still, with suitable priors we can restrict ourselves to
the settings of Thanou et al. (2013), as we explain in
··· Section 5.
⊙
Another line of work that we encompass with our
G G G Graph-Dictionary signal model consists in signal clus-
1 2 3
teringandgraphlearning. Infact,wecanseeeachclus-
Coefficients Signals ter as an atom of a dictionary, with signal coefficients
providing a vector encoding of cluster assignments.
Maretic et al. (2018), and Maretic and Frossard (2020)
··· Signal ··· propose Graph Laplacian Mixture Models (GLMM)
∼
model in which precision matrices of the signals’ distribu-
tion depend on some function of the graph Laplacians.
δ δ δ
1 2 3
x 1 x 2 x 3 Similarly, Araghi et al. (2019) design an algorithm for
signals clustering and graph learning called K-Graphs,
which refines assignments and graph estimation by
Figure 1: Representation of graph-dictionary signal
expectation-maximisation. Both methods rely on a
model. Coefficients mix atoms from the dictionary
Gaussian assumption on the signal distribution, but
to give instantaneous graphs, which characterize the
while the latter model enforces hard assignments, like
distribution from which signals are sampled.
the K-means algorithm, GLMM performs soft clus-
tering, by estimating the probability that a sample
belongs to each cluster. Still, clustering methods need
Our main contributions are the following:
a significant separation between the underlying distri-
butions, which in graph signal models means that they
might struggle to discriminate samples if the underly-
• Graph signal model. We introduce GraphDict,
ing networks share multiple edges. Furthermore, they
a novel probabilistic model that defines signals
cannot capture continuous changes which can arise in
distribution with graph dictionaries. It further in-
more structured data such as multivariate time series.
troduces domain-specific priors on both the graph
structuresandthecontributionsofeachdictionary
atom so that we can formulate signal representa- 3 GRAPH-DICTIONARY SIGNAL
tion as a maximum a posteriori estimate. MODEL
• Optimization algorithm. We propose a novel In this Section, we introduce GraphDict, our novel
Graph-Dictionary signal model, and we describe the
bilinear generalization of the Primal-Dual Split-
corresponding representation learning problem.
ting algorithm to jointly learn edge weights of
dictionary atoms and their mixing coefficients.
3.1 Graphs And Signals
• Performance Evaluation. We present three Our data X ∈ RT×N represent a set of T signals of
experiments to investigate GraphDict’s ability to dimensionN. Foreacht∈T weobserveamultivariate
reconstructinstantaneousgraphsandprovidehelp- signal x ∈RN on the nodes of a graph G =(N,w )
t t t
ful and explainable representations. Our model whose edges E ⊂ N ×N and corresponding weights
can reliably reconstruct instantaneous graphs on w ∈RE areunknown. Eachedge(i,j)describesarela-
t +
synthetic data, and its sparse coefficients’ repre- tionbetweensignaloutcomesx andx onnodesiand
ti tj
sentation outperforms established methods in a j, with a strength proportional to its weight w . We
ij
motor imagery decoding task. focus on undirected graphs with nonnegative weights,William Cappelletti, Pascal Frossard
which we represent as vectors w ∈RN(N−1)/2 =RE. by δ and W. In this work, we assume that atom
+ + t
weights and coefficients are independent, meaning that
Graph Signal Processing (Ortega, 2022) provides us
structure and appearance do not influence each-other.
tools to understand the link between signals and the
In this way, we can split the joint PDF in
networks on which we observe them. One of the most
important tools is the combinatorial Laplacian L of a P (X,W,∆)=P (W)P (∆)P (X |W,∆), (5)
w c x
graph, which is defined as the difference between the
diagonal matrix of node degrees and the adjacency ma- where P and P are the prior distributions of weights
w c
trix. Expressingitasafunctionofvectorizedweightsw, and coefficients respectively, and we gather all coeffi-
the Laplacian is a linear operator from RE →RN×N: cients δ in the rows of the matrix ∆∈[0,1]T×K.
+ t

−w i̸=j,(i,j)∈E,
 (i,j) 3.3 Representation Learning Problem
(cid:80)
[L ] = w i=j, (1)
w ij k:(i,k)∈E (i,k)
0
otherwise.
With the characterization of signal distribution with
the graph dictionary and coefficients from Eq. (5), we
can define the representation learning problem as a
Of particular interest is the eigendecompostion of the
maximum a posteriori (MAP) objective. Observing
Laplacian L=UΛU⊤, with Λ the diagonal matrix of data X ∈ RT×N, we aim to find graph-dictionary
eigenvalues λ = 0 ≤ ... ≤ λ . We define the Graph
1 N weights W and coefficients ∆ as the minimizers of the
Fourier Transform of a signal x as the projection on
corresponding negative log-likelihood
the eigenbasis xˆ =Ux, and noting that eigenvectors
are invariant signals, we introduce graph frequencies as argmin ℓ (W)+ℓ (∆)+ℓ (X,W,∆)), (6)
w c x
the corresponding eigenvalues. We can finally define W,∆
graphfiltersasfunctionsactinginthefrequencydomain
where ℓ , ℓ , and ℓ are the logarithms of P , P ,
(Sandryhaila and Moura, 2013), and represent them as w c x w c
and P respectively.
x
G(L)=UG(Λ)U⊤. (2)
This formulation leaves a great flexibility on ℓ , ℓ ,
w c
andℓ whichcanenforcestructureanddomain-specific
x
3.2 Probabilistic Signal Model properties on atoms weights, coefficients, and signals
respectively. Such priors are driven by the application,
We propose a model for multivariate data where the
butinSection4wepresentageneralalgorithmtosolve
state of the system at each sample t is described by an
Eq. (6) for W and ∆. In Section 5, we illustrate the
underlying instantaneous graph G , given by a linear
t specific terms that we use in our experiments.
combination of K of networks, which constitute the
atoms of a dictionary of graphs.
4 BILINEAR PRIMAL-DUAL
Each N-variate sample signal x t ∈ RN arises from SPLITTING ALGORITHM
a graph filter G(L ) defined on the instantaneous
t
graph Laplacian L , applied to iid multivariate white
t TofindasolutionoftheMAPobjective(6),wepropose
noise η ∼N(0,I ), so that
N a generalization of the Primal-Dual Splitting (PDS)
x =G(L )η ∼N (cid:0) 0,G2(L )(cid:1) . (3) algorithm by Condat (2013), which is a first order
t t t
algorithm to solve minimization problems of the form
This formulation allows much freedom to model com-
plex systems, since we fix a graph filter, but the actual argmin f(x)+g(x)+h(Ax), (7)
x∈RN
graph is specific to each sample.
wherethefunctionf isconvexandβ-Lipschitzdifferen-
We define instantaneous graphs G through coefficients
δ ∈ [0,1]K and edge weights of et ach atom w ∈ RE. tiable, h and g are proximable, and A:RN →RM is a
t k + linearoperator. Supposingthatwecanrearrangeℓ ,ℓ
We jointly represent the graph dictionary as the ma- w c
trix W =[w⊤,...,w⊤]∈RK×E, whose rows identify fromEq.(6)intoasumofproximableandLipschitzdif-
1 K + ferentiable functions, denoted by g and f respectively,
atoms, and we express instantaneous Laplacians as a
we can rewrite our optimization problem in the form
bilinearformofcoefficientsδ andweightsW asfollows
t
of Eq. (7):
(cid:88)
L =L(δ ,W)= δ L ∈RN×N. (4)
t t tk wk argmin g(W,∆)+f(W,∆)+h(G(L(∆,W))), (8)
k
W,∆
Conditionally to this graph-dictionary formulation, the where G is the graph filter from Eq. (3), which we
signal distribution P from Eq. (3) is characterized apply to the tensor L(∆,W)∈RT×N×N that stacks
xGraph-Dictionary Signal Model
Algorithm1: BilinearPrimal-DualSplitting Splitting in Algorithm 1. We note that the update
of the dual variable Y in Line 4 corresponds to the
Input: Step parameters τ ,τ ,σ ≥0, initial n
w c
one in Eq. (9b), with operator A substituted by the
estimates (W ,∆ ,Y ), maximum
0 0 0
bilinear one L.
number of iterations N
max
Output: Estimated solution of Eq. (8)
1 for n=0 to N max−1 do 5 EXPERIMENTS
2 W n+1 ←
prox(W −τ (L∗∗(∆ ,Y )+∇f (W ))); We present three experiments to illustrate the capa-
n w n n w n
τwgw bilities of GraphDict in reconstructing instantaneous
3 ∆ n+1 ← graphs from signals and in providing useful representa-
prox(∆ n−τ c(L∗(Y n,W n)+∇f c(∆ n))); tions of the data through atoms’ coefficients.
τcgc
4 Y n+1 ← Recalling the negative log-likelihood of the genereal
prox(Y n+σL(2∆ n+1−∆ n,2W n+1−W n)); model from Eq. (6), we design a loss function for our
σh∗ experimental setup that leverages the most common
5 if converged then hypotheses from graph structure learning and sparse
6 break representation literatures. We define the following
objective, with regularization hyperparameters α’s,
7 return W n+1,∆ n+1
ℓ (W)=χ +α ∥W∥ + (12a)
w W≥0 wL1 1
K
instantaneous Laplacians. From Eq. (4), we see that L (cid:88)
+α ⟨w ,w ⟩,
is on its own a bilinear operator on ∆ and W. ⊥ k k′
k′̸=k
TheoriginalPDSalgorithmlooksforsolutionstobotha ℓ (∆)=χ +α ∥∆∥ , (12b)
c ∆∈[0,1] cL1 1
primalandadualproblem,givenrespectivelybyxand
T
Ax, and iteratively brings them closer by combining ℓ (∆,W)=(cid:88)(cid:0) x⊤L(δ ,W)x +h(L(δ ,W))(cid:1) .
gradientandproximaldescent(ParikhandBoyd,2013). x t t t t
t=1
Moreprecisely,givenproximalparametersτ,σ >0,and (12c)
initial estimate (x ,y ) ∈ RN ×RM, it performs the
0 0
following iteration for every n≥0, until convergence:
The first terms in both ℓ and ℓ include the domain
w c
x ←prox (x −τ(∇f(x )+A∗y )), (9a) constraints on W and ∆, expressed as characteristic
n+1 τg n n n
functions χ and χ respectively, with
y ←prox (y +σA(2x −x )), (9b) RK×E [0,1]K×T
n+1 σh∗ n n+1 n +
where A∗ is the adjoint of A, and h∗ is the Fenchel (cid:40) 0 x∈A;
conjugate of the function h. χ A(x)= (13)
+∞ x∈/ A.
The main challenge of our setting comes from optimiz-
ing jointly for W and ∆ the function h, composed The second terms of Eqs. (12a) and (12b) are weighted
to the graph filter G. To generalize PDS, we define L 1-norms that enforce sparsity in the atoms weights
thedualvariablefromthebilinearformY =L(∆,W). and in their coefficients respectively. The third term
Then, we observe that the primal update in Eq. (9) inℓ w penalizesoverdotproductsacrossatomsweights,
relies on the adjoint of the linear operator A∗, which which enforces orthogonality and, since we also require
maps the dual variable to the space of the primal one. the matrix to have nonnegative entries, this regular-
In Eq. (8), supposing that g and f are separable in the ization enforces edges to be zero if they are already
primal variables W and ∆, we can define two update present in another atom.
steps,eachusingthepartialadjointsofL(Arens,1951),
Finally, the first term in the sum in Eq. (12c) enforces
that we show in Appendix A to be
signalstobesmoothontheLaplaciansofinstantaneous
L∗(Y,W)=WdY⊤, graphs; whilethefunctionhdescribessomehypotheses
(10) on instantaneous Laplacians which give rises to two
L∗∗(∆,Y)=∆dY,
differentmodels: GraphDictLog andGraphDictSpectral.
where dY is the tensor whose entries are given by The GraphDictLog model enforces node degrees to be
positive for each instantaneous graph G using a log
(cid:2) (cid:3) t
dY =Y +Y −Y −Y . (11)
tenm tnn tmm tnm tmn barrier on the diagonal of the Laplacians, as done by
Kalofolias (2016) and derivative works:
With the separation and parallel update of the primal
variables W and ∆ , we define Bilinear Primal-Dual h(L )=1⊤log(diag(L )). (14)
n n t N tWilliam Cappelletti, Pascal Frossard
Alternatively, we can implement the model from
Thanou et al. (2013) within our framework, which 1
we call GraphDictSpectral. This model introduces a
constraint that enforces all Laplacians to share the
2
sameeigenvectorsU,whichweinitializetothoseofthe
empirical covariance matrix (Navarro et al., 2022), so
that the optimization is carried over on their eigenval- 3
ues. Thespecifichtermenforcesthisconstraintwitha
characteristic function χ, and includes a log barrier on 4
the Laplacians pseudo-determinants (Holbrook, 2018):
5
h(L )=χ (L )+logdet (L ). (15)
t UΛtU⊤ t + t
Sample Id
Equations (12a) to (12c) are all sums of proximable
and differentiable functions, and thus are compatible
Figure 2: Samples of coefficient matrices for different
with Eq. (8). Therefore, we can optimize for W and ∆
superposition values Each block represent in black the
withAlgorithm1. InAppendixBwederivethespecific
positive coefficients of five atoms over 50 samples.
update steps.
5.1 Superposing Graphs We gather multiple independent samples from this
process as columns of two matrices of discrete coef-
5.1.1 Description ficients ∆
tr
∈{0,1}5×Ttr,∆
te
∈{0,1}5×Tte, and from
the corresponding instantaneous graphs ∆ W we
tr|te
In this experiment we generate data according to the
sample two sets of signals X and X following a
tr te
Graph-DictionarySignalModelfromSection3, andwe
Laplacian-constrainedGaussianMarkovRandomField,
focus on recovering instantaneous graphs. In particu-
or LGMRF (Egilmez et al., 2016). In this model the
lar, weinvestigatetheevolutionofmodelperformances
combinatorial Laplacian corresponds the precision ma-
with respect to the maximum number of atoms that
trix of a Gaussian distribution, so that signals follow
√
can contribute to each signal, which we call super-
Eq. (3) for the filter G(Λ)= Λ†, with † denoting the
position. With a superposition of one, each signal
Moore-Penrose pseudo-inverse. We use X and X
tr te
arises from a single network and the problem is equiv-
respectively for training and testing the models. Each
alent to signal clustering and graph reconstruction.
atomcontributetomultipleinstantaneousgraphsaswe
Therefore, we compare GraphDictLog and GraphDict-
allowformoresuperposition,sowebalancethetasksby
Spectral to Gaussian Mixtures, where we estimate the
setting T to allow for each atom to contribute to 500
tr
graphasthepseudo-inverseoftheempiricalcorrelation,
samples on average. More precisely, for s=1,...,5 we
KGraphs (Araghi et al., 2019), and GLMM (Maretic
have T = 2500,1666,1250,1000,833; while the test
tr
and Frossard, 2020). We re-implement the two lat-
set has a fixed size of T =500.
te
ter, and we use Gaussian Mixtures from Scikit-learn
(Pedregosa et al., 2011).
5.1.3 Results
5.1.2 Data We focus on the ability of models to correctly recover
edges, and we measure their performances with the
We sample a dictionary of K = 5 graphs from an
Matthews correlation coefficient (MCC) score:
Erd¨os-Renyi distribution with 30 nodes and p = 0.2
edge probability (ER(30,0.2)), which will be shared by
tp×tn−fp×fn
all tasks. Then, we define the superposition parameter MCC = (cid:112) ,
(tp+fp)(tp+fn)(tn+fp)(tn+fn)
s ∈ {1,...,5}, that indicates the maximum number
(16)
of atom coefficients that can be nonzero at the same
which takes into account true (t) and false (p) positive
time for each experiment run. Subsequently, for each
(p) and negative (n) edges. This metric, which takes
t∈N, we build a random coefficient vector δ by first
t valuesinthe[−1,1]interval,iswellsuitedtocomparing
sampling the number of positive coefficients uniformly
edges presence or absence in sparse graphs, thanks to
from 1 to s, and then by randomly selecting the atoms.
being balanced even with classes of very different sizes.
Fig. 2 shows multiple sampled coefficients for each
superposition value. We observe that, by definition, at Fig. 3 shows the evolution of the MCC score on the
most s atoms have nonzero coefficients, but some δ test set for increasingly superposed coefficients, aver-
t
have less positive values. aged on five different random seeds. For every graph-
noitisoprepuSGraph-Dictionary Signal Model
and assign learnable graphs to this hierarchy. The sig-
0.35
nal distribution within each window is characterized
0.30 by an instantaneous network given by the sum of all
graphs from the root the corresponding leave, so that
0.25
the hierarchical graphs can be estimated with a maxi-
0.20 mumlikelihoodformulation. Duetoitsadditivenature,
this model falls within our dictionary framework, with
0.15
GaussianMixture graphs in the hierarchy corresponding to atoms, but
KGraphs
0.10 withasubstantialdifferenceinthecoefficients. Intheir
GLMM
0.05 GraphDictSpectral case, these are fixed and describe the tree structure,
GraphDictLog (ours) while in the dictionary learning setting they are pa-
0.00 rameters of the model. We implement this hierarchical
1 2 3 4 5
graph-learning model as GraphDictHier, by fixing the
Superposition
atoms coefficients and only learning atom edge weights.
Figure3: Testperformanceonedgerecoveryofinstan- For this experiment, we include a temporal prior in
taneous graphs from signals, measured by Matthews GraphDictLog and GraphDictSpectral, similarly to
correlation coefficient (MCC). Results are averaged TGFA, by introducing an L regularization on the dif-
1
over five random seeds. ferenceincoefficientsofadjacentwindowstoEq.(12b):
T−1
(cid:88)
learning method, we perform a hyperparameter grid α ∥δ −δ ∥ . (17)
diff t+1 t 1
search by training on X tr and scoring on the instan- t=1
taneous graphs of the same training set. For a super-
We also allow for coefficients to be fixed over windows,
position value of 1, which correspond to signal cluster-
withthesizebecomingahyperparameterwiththesame
ing, all graph learning methods perform equally, while
search space as benchmarks.
Gaussian Mixtures lag behind. With increasingly su-
perposed graphs, GraphDictLog consistently provides
5.2.2 Data
the best results.
Inthissetting, withthelowedgeprobabilityintheER We generate sequential graphs from two time-varying
graphs, we observe that the most useful prior on atom distributions, which we use in separate runs:
weights, from Eq. (12a) is sparsity, controlled by α .
wL1
• The Edge-Markovian evolving graph (EMEG)
5.2 Time-Varying Graphs model starts with a sample of ER(36,0.1), and
at each graph change we add new edges or re-
5.2.1 Description move existing ones from the current network with
probabilities of 0.001 and 0.01 respectively.
In this experiment, we observe signals sequentially, as
time series, and we aim to recover the underlying in- • The Switching behavior graph (SBG) model con-
stantaneousgraphs,whichhavetemporaldependencies. sists of six independent ER(36,0.05) networks,
Multiple models have been proposed for these settings, that define states in a Markov Process. At each
introducing priors on the evolution of networks over stepweeitherkeepthesamegraphastheprevious
time, and we can have a fair comparison with three one, with probability of 0.98, or change uniformly
different benchmarks. First, WindowLogModel is a to another one.
temporal version of the static graph-learning method
from Kalofolias and Perraudin (2018), which learns
In both models, weights for new edges are sampled
independent graphs over sliding windows. TGFA is a
uniformly on the [0.1,3] interval, and do not change
dynamic graph-learning paradigm encompassing the
untiltheedgedisappears. Withthesemodelsweobtain
models from Kalofolias et al. (2017) and Yamada et al. sequences of T weights W ∈RTG×E, that allow us to
G +
(2020), which adds an L1 or L2 regularization on edge
samplesignalsfromtheLGMRFdistributionpresented
changes between adjacent windows to the WindowLog-
in Section 5.1.2. For each time-varying graph process,
Model objective.
we study two signal generation settings:
Yamada and Tanaka (2021) take a different approach
and suppose that the time-varying graphs can be rep- 1. Graphs are stable over windows: we set T =32,
G
resented in a hierarchical manner. They organize win- and we sample W =20 consecutive signals X ∈
w
dows as a binary tree, with neighbors sharing parents, RW×N, for a total of 640 measurements;
erocs
CCMWilliam Cappelletti, Pascal Frossard
T ava eb rl ae ge1: MCE Cdg se cor re ec ao cv re or sy sp ae llr if no sr tm aa nn tac ne es o, um se ga rs au pr he sd
,
fb oy
r M e a
ns-3
K
the time series experiments. Each stochastic model is
M e a
ns-1 6
r aa vn erd ao gm el sy coin ri et .ialized multiple times, and we report its K Mixture-3
Distribution EMEG SBG
GG
a
ua su ss is ai na n Mi Gxt Lu Mre M-1 -6
3
graphs · win. size 32·20 512·1 32·20 512·1 M-1 6
M
G L
W
T
Gi
G
rn
F
ad
A
po hw DL io cg tM Ho id ee rl 3
2
41
0
0.
.
.9
7
9
3
1
36
7
9.
.
.0
3
9
3
1
34
4
6.
.
.2
7
4
2
1
38
6
1.
.
.5
2
8 Gra p h
DictL o
g-3
0.4 0.5 AU0 R.6
OC
0.7 0.8
GraphDictSpectral 59.8 28.8 34.0 20.7
GraphDictLog 61.1 44.1 47.2 38.6
Figure4: Distributionoftestscoresformotorimagery
classification from brain state features, computed by
leave-one-subject-out cross-validation. On the y-axis
2. The system is continually evolving: we sample
we find the name of the model used for learning brain
T =512 graphs each of which produces a single
G
signal x ∈RN. states, together with the number of clusters, or atoms,
t
defining such states.
5.2.3 Results
a downstream classification task, in particular in its
Table 1 shows the performances in edge recovery of
GraphDictLog form. We analyze brain activity mea-
benchmarks and the different GraphDict models on all
surements with the objective to disambiguate left and
time series tasks, averaged over five random seeds. We
right-hand motor imagery. Neurologists often resort to
select hyperparameters of every model by grid search
microstates (Michel and Koenig, 2018) to characterize
and evaluation on training reconstruction. Including
brain signals, and they obtain such states by clustering
the additional regularization from Eq. (17) in the loss
EEG signals over time. This is an interesting setting
term proved to be an effective prior for the EMEG
for our model, where instead of single microstates we
process; while enforcing coefficients to change over win-
can reconstruct a dictionary of states (graph atoms)
dows instead of over samples improved reconstruction
that can provide insights on brain connectivity.
performances for the SGB process.
We infer brain states as the clusters, or atoms, learned
We see that our GraphDictLog greatly outperforms
byGraphDictLogandthreebaselinemethods: KMeans,
other methods in all settings, showing that it can cap-
which is the standard method for microstate inference;
turethegradualevolutionoftheEMEGdistribution,as
Gaussian Mixtures, which allow for soft assignments
wellastherecurrentgraphsofSBG.Weseethatscores
(Mishra et al., 2020); GLMM, that include network
are consistently higher in the stable 32×20 setting, in
properties on states (Ricchi et al., 2022). We perform
particular thanks to the hypothesis of window-based
the classificationtask onseparate timewindows, called
modelsbeingcorrect. Still,ourGraphDictLogperforms
events. For each event we compute three features for
better on the continuous 512×1 setting than other
each state, based on the evolution of attribution prob-
methods. We suppose that the advantage over the
abilities, for clustering methods, and coefficients, for
hierarchical model from Yamada and Tanaka (2021),
graph dictionaries:
which they presented as state of the art, comes from
the greater flexibility of ours in learning on its own
• Numberofoccurrences (OCC):countofcontinuous
when atoms contribute to a sample, or window. In
intervals covered;
particular, for the SBG setting, in which states are
recurrent, our model can “reuse” atoms across time, • Coverage (COV): total time over the window;
while the hierarchical model has less memory across
windows far apart. • Average duration: average length of continuous
occurrences in seconds, i.e. OCC/COV.
5.3 Brain States For Motor Imagery
5.3.2 Data
5.3.1 Description
The data for this experiment consists in a set of an-
In this final experiment we study the capability of notated Electroencephalography (EEG) signals from
GraphDict to learn an explainable representation for the EEG BCI dataset (Schalk et al., 2004), providedGraph-Dictionary Signal Model
6 CONCLUSION
We propose GraphDict, a Graph-Dictionary signal
model for multivariate data, together with an opti-
mization algorithm to jointly learn atoms and their
coefficients. This framework can characterize systems
with complex dynamics, by reconstructing a dictionary
Atom 0 Atom 1 Atom 2 of graphs that contribute to the observed phenomena,
andprovidesparserepresentationsintheformofatoms
Figure 5: Atoms learned by GraphDictLog with or-
coefficient that we can use in downstream tasks.
thogonality prior on EEG signals from motor imagery
data, sorted by frequency of appearance. The graph reconstruction experiments show that our
GraphDictionarylearningmodelisabletoconsistently
retrieve instantaneous Laplacians in multiple settings.
In particular, it shows consistent performances with
increasingatomco-occurrenceinSection5.1,whereit’s
by the MNE-Python library Gramfort et al. (2013).
hypothesis is correct; and it outperforms other graph
EEGs are multivariate time series of electrical poten-
learningbenchmarkswhenmodellingmultivariatetime
tials captured by electrodes placed over the subjects
series in Section 5.2, in spite of the graph generation
scalp. This dataset collects measurements from 15
process not satisfying the dictionary assumptions. Fur-
subjects, each with 45 events spanning the onset of
thermore, in Section 5.3 we show the utility of the
imagined motion of either hand, with 23 for the left
sparse representation given by the atom coefficients of
and 22 for the right hand. We preprocess the data
ourGraphDictmodeltodescribebrainstatesformotor
following microstates’ literature to obtain clean signals
imagery classification. With few features on only three
for the 64 EEG channels, and we detail the steps in
atoms, our model performs at least as well as methods
Appendix C.
relying on five times more states.
In all applications, we observe that choosing well-
thought priors significantly improves the model capa-
5.3.3 Results bilities. More precisely, weight sparsity is very helpful
when it reflects the ground truth graphs, as in our syn-
thetic settings. For temporal data regularizing changes
Fig. 4 shows the results’ distribution of random forest
between activations over time helps capture the se-
classifiers on 15-fold cross validation, with each fold
quential aspect of the underlying process. Still, some
corresponding to a unique subject. For each method
open directions remain to fully leverage the potential
we test different numbers of states, or clusters, and
of our model. Characterizing the properties of the
select other eventual hyperparameters to obtain the
learned graphs and coefficient depending on chosen
highest average training performance in classification.
priors would allow to directly choose hyperparameters,
We see that GraphDictLog with three atoms obtains
instead of requiring expensive grid searches.
consistently the highest AUROC, with the only com-
parable model being KMeans with 16 clusters. Still, ThankstotheflexibilityofGraphDictanditsoptimiza-
KMeans-16 requires 48 features, against the nine fea- tion algorithm to accommodate for different regular-
tures arising from the three states of GraphDictLog-3. ization terms and priors, this framework can leverage
Interestingly,ofallregularizationtermsfromEqs.(12a) domain knowledge and provide insightful representa-
to (12c), only α ⊥, which enforces edge orthogonality, tions of multivariate data from different domains.
is positive.
Acknowledgments
In this setting, we observe a strong influence of the
physics of the problem on the data, which translates in
This work was supported by the SNSF Sinergia
electrodes being close in space to be highly correlated
project “PEDESITE: Personalized Detection of Epilep-
due to the skull acting as a diffuser for brain activity.
tic Seizure in the Internet of Things (IoT) Era” under
To avoid this strong correlation to take over, enforcing
grant agreement CRSII5 193813.
atom orthogonality proves to be an effective solution.
Figure 5 shows the reconstructed atoms corresponding
References
to the best classification model. We see many edges
between electrodes close in space, but the atoms seems H. Araghi, M. Sabbaqi, and M. Babaie–Zadeh. $K$-
to capture functional areas, as we recognize the frontal Graphs: An Algorithm for Graph Signal Clustering
lobe in Atom 1. andMultipleGraphLearning. IEEESignalProcessingWilliam Cappelletti, Pascal Frossard
Letters, 26(10), Oct. 2019. doi: 10.1109/LSP.2019. G.Mateos,S.Segarra,A.G.Marques,andA.Ribeiro.
2936665. Connecting the Dots: Identifying Network Structure
via Graph Signal Processing. IEEE Signal Processing
R. Arens. The Adjoint of a Bilinear Operation. Pro-
Magazine, 36(3), May 2019. doi: 10.1109/MSP.2018.
ceedings of the American Mathematical Society, 2(6),
2890143.
1951. doi: 10.2307/2031695.
C. M. Michel and T. Koenig. EEG microstates as
L. Condat. A Primal–Dual Splitting Method for Con- a tool for studying the temporal dynamics of whole-
vex Optimization Involving Lipschitzian, Proximable brain neuronal networks: A review. NeuroImage, 180,
and Linear Composite Terms. Journal of Optimiza- Oct. 2018. doi: 10.1016/j.neuroimage.2017.11.062.
tion Theory and Applications, 158(2), Aug. 2013. doi:
A. Mishra, B. Englitz, and M. X. Cohen. EEG mi-
10.1007/s10957-012-0245-9.
crostates as a continuous phenomenon. NeuroIm-
X. Dong, D. Thanou, M. Rabbat, and P. Frossard. age, 208, Mar. 2020. doi: 10.1016/j.neuroimage.2019.
LearningGraphsFromData: ASignalRepresentation 116454.
Perspective. IEEE Signal Processing Magazine, 36(3),
M. Navarro, Y. Wang, A. G. Marques, C. Uhler, and
May 2019. doi: 10.1109/MSP.2018.2887284.
S. Segarra. Joint Inference of Multiple Graphs from
Matrix Polynomials. Journal of Machine Learning
H. E. Egilmez, E. Pavez, and A. Ortega. Graph learn-
Research, 23(76), 2022.
ing with Laplacian constraints: Modeling attractive
Gaussian Markov random fields. In 2016 50th Asilo-
A. Ortega. Introduction to Graph Signal Processing.
mar Conference on Signals, Systems and Computers,
Cambridge University Press, Cambridge, 2022. doi:
Nov. 2016. doi: 10.1109/ACSSC.2016.7869621.
10.1017/9781108552349.
A. Gramfort, M. Luessi, E. Larson, D. A. Enge- N. Parikh and S. Boyd. Proximal Algorithms. Now
mann, D. Strohmeier, C. Brodbeck, R. Goj, M. Jas, Publishers, Nov. 2013.
T. Brooks, L. Parkkonen, and M. Ha¨ma¨la¨inen. MEG
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
and EEG data analysis with MNE-Python. Frontiers
B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,
in Neuroscience, 7, Dec. 2013. doi: 10.3389/fnins.
R. Weiss, V. Dubourg, J. Vanderplas, A. Passos,
2013.00267.
D. Cournapeau, M. Brucher, M. Perrot, and E´. Duch-
A. Holbrook. Differentiating the pseudo determinant. esnay. Scikit-learn: Machine Learning in Python.
Linear Algebra and its Applications, 548, July 2018. Journal of Machine Learning Research, 12(85), 2011.
doi: 10.1016/j.laa.2018.03.018.
I. Ricchi, A. Tarun, H. P. Maretic, P. Frossard, and
D. Van De Ville. Dynamics of functional network
V. Kalofolias. How to Learn a Graph from Smooth
organization through graph mixture learning. Neu-
Signals. In Proceedings of the 19th International Con-
roImage, 252, May 2022. doi: 10.1016/j.neuroimage.
ferenceonArtificialIntelligenceandStatistics.PMLR,
2022.119037.
May 2016. doi: 10.48550/arXiv.1601.02513.
R. Rubinstein, A. M. Bruckstein, and M. Elad. Dic-
V. Kalofolias and N. Perraudin. Large Scale Graph
tionaries for Sparse Representation Modeling. Pro-
LearningFromSmoothSignals. InInternational Con-
ceedings of the IEEE, 98(6), June 2010. doi: 10.1109/
ference on Learning Representations, Sept. 2018.
JPROC.2010.2040551.
V. Kalofolias, A. Loukas, D. Thanou, and P. Frossard.
A. Sandryhaila and J. M. F. Moura. Discrete Signal
Learning time varying graphs. In 2017 IEEE Inter-
Processing on Graphs. IEEE Transactions on Signal
national Conference on Acoustics, Speech and Sig-
Processing, 61(7), Apr. 2013. doi: 10.1109/TSP.2013.
nal Processing (ICASSP), Mar. 2017. doi: 10.1109/
2238935.
ICASSP.2017.7952672.
G. Schalk, D. McFarland, T. Hinterberger, N. Bir-
H. P. Maretic and P. Frossard. Graph Laplacian baumer, andJ.Wolpaw. BCI2000: Ageneral-purpose
Mixture Model. IEEE Transactions on Signal and brain-computer interface (BCI) system. IEEE Trans-
Information Processing over Networks, 6, 2020. doi: actions on Biomedical Engineering, 51(6), June 2004.
10.1109/TSIPN.2020.2983139. doi: 10.1109/TBME.2004.827072.
H. P. Maretic, M. E. Gheche, and P. Frossard. Graph D. Thanou, D. I. Shuman, and P. Frossard. Paramet-
HeatMixtureModelLearning. In201852ndAsilomar ricdictionarylearningforgraphsignals. In2013IEEE
Conference on Signals, Systems, and Computers, Oct. Global Conference on Signal and Information Process-
2018. doi: 10.1109/ACSSC.2018.8645150. ing, Dec. 2013. doi: 10.1109/GlobalSIP.2013.6736921.Graph-Dictionary Signal Model
K.YamadaandY.Tanaka. TemporalMultiresolution
Graph Learning. IEEE Access, 9, 2021. doi: 10.1109/
ACCESS.2021.3120994.
K.Yamada,Y.Tanaka,andA.Ortega. Time-Varying
GraphLearningwithConstraintsonGraphTemporal
Variation, Jan. 2020.William Cappelletti, Pascal Frossard
Graph-Dictionary Signal Model for Sparse Representations of
Multivariate Data: Supplementary Materials
A Bilinear operators
In this section we derive the operators of Eq. (10). In the notation from Arens (1951), we let A,B,C be normed
linear spaces on R, and we denote their dual and double dual spaces by .∗ and .∗∗ respectively. Given a bilinear
operator L:A×B →C, its adjoints should satisfy
L∗ :C∗×A→B∗, L∗(f,x)(y)=f(L(x,y)) (A.1)
L∗∗ :B∗∗×C∗ →A∗, L∗∗(y,f)(x)=f(L(x,y)). (A.2)
In our case, the bilinear operator L:A×W →L is defined as
T
[L(∆,W)] =[L ]
tnm ∆⊤W tnm
(cid:40) −(cid:80) δ w n̸=m, (A.3)
= k kt k(n,m)
(cid:80) (cid:80)
δ w n=m,
k kt l k(n,l)
and it maps from the spaces of atom coefficients A = [0,1]K×T and graph weights W = RK×E to that of
+
instantaneous Laplacians L ⊂RT×N×N.
T
We obtain the adjoint operators by solving the identities A.1, A.2 for the standard dot products of the given
spaces.
We start from the adjoint operator with respect to weighs, L∗ :RT×N×N ×[0,1]K×T →RK×E, we observe
+
(cid:88)
⟨Y,L(∆,W)⟩= Y [L ] (A.4a)
tnm ∆⊤W tnm
tnm
(cid:88) (cid:88) (cid:88) (cid:88) (cid:88)
= Y δ (−w )+ Y δ w (A.4b)
tnm kt k(n,m) tnn kt k(n,l)
tnm,n̸=m k tn k l
(cid:32) (cid:33)
(cid:88) (cid:88) (cid:88) (cid:88)
= w − δ (Y +Y ) + w δ Y (A.4c)
k(n,m) kt tnm tmn k(n,l) kt tnn
k(n,m) t k(n,l) t
n<m
(cid:88) (cid:88)
= w δ (Y +Y −Y −Y ) (A.4d)
k(n,m) kt tnn tmm tnm tmn
k(n,m) t
n<m
(cid:42) (cid:43)
(cid:2) (cid:3)
= W,∆ Y +Y −Y −Y (A.4e)
tnn tmm tnm tmn k(n,m)
(cid:124) (cid:123)(cid:122) (cid:125)
=dY
=⟨W,L∗(Y,∆)⟩, (A.4f)
(cid:80)
where between Eq. (A.4c) and Eq. (A.4d) we split the second sum into two sums over directed edges
k(n,l)
(cid:80) (cid:80)
+ .
k(n,l),n<l k(n,l),l<n
On the other hand, for the adjoint operator with respect to coefficients, L∗∗ :RK×E ×RT×N×N →[0,1]K×T, we
+
can follow the previous derivation until Eq. (A.4d), then isolate the coefficient matrix and obtain
(cid:68) (cid:69)
⟨∆,L∗∗(W,Y)⟩= ∆,WdY⊤ . (A.5)Graph-Dictionary Signal Model: Supplementary Materials
B Implementation
In this section we present the proximal operators and gradients corresponding to the GraphDict objective used in
the experiments, namely in Eqs. (12a) to (12c), (14) and (15). In short, signals and dictionaries are bound by the
smoothness term in Eq. (12c), then we enforce sparsity by L1 regularization for both weights and coefficients. For
weights, we additionally have an orthogonality constraint. We regularize instantaneous Laplacians with either the
log barrier on their diagonal from Eq. (14), or with log pseudo-determinant from Eq. (15)
We start by providing an alternative formulation for the smoothness term in Eq. (12c)
T T K
(cid:88) (cid:88)(cid:88)
x⊤L(δ ,W)x = δ x⊤L x (B.1)
t t t tk t wk t
t=1 t=1k=1
T K N N
(cid:88)(cid:88)(cid:88) (cid:88)
= δ x x [L ] (B.2)
tk tn tm wk nm
t=1k=1n=1m=1
 
T K N N N
(cid:88)(cid:88)(cid:88) (cid:88) (cid:88)
= δ tk x2 tnw k(n,l)− x tnx nmw k(n,m) (B.3)
t=1k=1n=1 l̸=n m̸=n
T K
=(cid:88)(cid:88) (cid:88) δ w (x −x )2 (B.4)
tk k(n,m) tn tm
t=1k=1(n,m)∈E
=∥∆W ⊙Z∥ , (B.5)
1
where in the last line we introduce the matrix Z ∈RT×E of squared pairwise differences over edges, with entries
[Z] =(x −x )2. For Eq. (B.4), we split the sum over pairs of nodes to two sums over directed edges. We
t(n,m) tn tm
remark that this formulation is equivalent to the one proposed by Kalofolias (2016) to express signal smoothness
over a single graph through pairwise node distances.
PuttingtogetherEqs.(12a)to(12c)withthesmoothnessformulationabove,ourfullobjectivefortheexperimental
setup is
K
(cid:88)
χ +α ∥W∥ +χ +α ∥∆∥ +∥∆W ⊙Z∥ +α ⟨w ,w ⟩+h(L(∆,W)), (B.6)
W≥0 wL1 1 ∆∈[0,1] cL1 1 1 ⊥ k k′
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) k′̸=k
gw(W) gc(∆)
(cid:124) (cid:123)(cid:122) (cid:125)
f(W,∆)
where g and g are proximable and f is sub-differentiable. The gradients of f with respect to our variables
w c
W ∈RK×E and ∆∈RT×K are respectively
+
∇ f(W,∆)=∆⊤Z+α (11⊤−I )W, (B.7)
W ⊥ K
∇ f(W,∆)=ZW⊤. (B.8)
∆
The proximal operators of g and g are
w c
prox (W)=(W −τ α ) , (B.9)
τwgw w wL1 +
prox (∆)=(∆−τ α ) , (B.10)
τcgc c c [0,1]
where in Eq. (B.9) (x) =max(x,0) is applied element-wise, as in Eq. (B.10) for (x) =max(min(x,1),0).
+ [0,1]
For the function h, we have two different settings, for which we should compute the Fenchel conjugate h∗. In
the GraphDictLog case, we use the function from Eq. (14), which sums the logarithms of node degrees for each
instantaneous graph. For this setting, we do not need to pass through the Laplacian, as we only need the degree
vectors of instantaneous graphs, which we can compute with the linear operator D ∈RN×E defined as
(cid:88)
[Dw] = w (B.11)
n (n,m)
m̸=nWilliam Cappelletti, Pascal Frossard
The dual term is therefore Y =D(∆,W)=D(∆W)⊤ ∈RN×T, for which we have the two partial adjoints
D∗(Y,W)=Y⊤DW⊤, (B.12)
D∗∗(∆,Y)=∆⊤YD. (B.13)
Finally, the proximal operator of the Fenchel conjugate of h is
√
Y − Y2+4σ
prox (Y)= . (B.14)
σh∗ 2
On the other hand, for GraphDictSpectral, we use the main formulation for Y =L(∆,W) and the function h
from Eq. (15). Fixing the eigenvectors for all Laplacians to be U ∈RN×N, the proximal operator of h∗ is
(cid:32) (cid:112) (cid:33)
Λ − Λ2+4γI
[prox (Y)] =U t t N U⊤, (B.15)
σh∗ t 2
where, Λ are the eigenvalues each “Laplacian” Y in the tensor Y; the square root is applied component-wise on
t t
the diagonal and the identity is broadcasted as needed.
C Experimental details
InthissectionweprovidefurtherdetailsontheexperimentsdescribedinSection5,tocompletetheinformationof
themaintextandenhancereproducibility. Tothisend,wealsoprovidethesourcecodeofmodels’implementation
and experimental pipelines.
C.1 Brain activity for motor imagery
FortheexperimentinSection5.3,thefullpipelineconsistsinthefollowingfivesteps,whichweperformseparately
for every benchmark clustering model and our graph-dictionary learning one, with multiple hyperparameters each:
1. Data preprocessing according to Microstates literature (Michel and Koenig, 2018; Mishra et al., 2020):
(a) We re-reference EEG signals and center each sample to the average over all electrodes
N
1 (cid:88)
x ←x − x ; (C.1)
t t N tn
n=1
(b) We apply a band-pass filter between 0.5 and 30Hz;
2. State embedding: we identify the “cleanest samples of brain activity”, which according to neurologists are
peaks of the Global Field Power, which we use to identify brain states. More precisely:
(a) We compute Global Field Power, which is the standard deviation across channels of each sample;
(b) We identify the peaks of this series;
(c) We train the given clustering or GraphDict model and retrieve cluster, or atom, assignments of peaks;
(d) We assign to each samples in EEG events the same coefficients as its closest peak in time;
3. Feature extraction: we compute the pre-cited state features for each EEG event;
4. Classification: wetrainandtestmultiplerandomforestclassifierswithleave-one-patient-outcrossvalidation;
Finally, we gather the results for each state embedding model. We report the distribution of test scores on motor
imagery classification corresponding to the best hyperparameters with the best downstream classifier.