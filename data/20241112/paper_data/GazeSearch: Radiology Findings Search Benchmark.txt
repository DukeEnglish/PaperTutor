GazeSearch: Radiology Findings Search Benchmark
TrongThangPham∗,Tien-PhatNguyen†,YukiIkebe∗,AkashAwasthi‡,ZhigangDeng‡,
CarolC.Wu§,HienNguyen‡,andNganLe∗
∗UniversityofArkansas,Fayetteville,AR,USA
†UniversityofScience,VNU-HCM,HoChiMinhCity,Vietnam
‡UniversityofHouston,Houston,TX,USA
§MDAndersonCancerCenter,Houston,TX,USA
Abstract
A single freeview
(a) Input fixations.
No specific fixations
s po
reuM
trc
me ed ei
f
dc
o
ia
r
cl
au
le ny ide me- at rr
s
ga
t
eac sk
n
.i dn
i
Tg
n
hgd isa ht
o
ia
nw
fi os
rra
ma dn
ai
toi im
l oo
np go
i
nsr ott sa tn
v
ot
i
nsi lun yafo
il
mlr ym
pi
ra
n
ot ti veo ern
s-
(b)
Existing
Datasets for any findings.
t bh ue
t
a ac lsc our ta hc ey iro if ntd ee re pp rel te aa br in lii tn y,g em nho ad nel cs inf gor trX a- nr sa py ara en na cl yys ii ns
Problem:
(c)
GazeSearch
decision-making. However,thecurrenteye-trackingdatais W fixh ai tc ioh n o sf ath ree sra pd ei co iflo icg ai ls lyt's
associated with their
dispersed,unprocessed,andambiguous,makingitdifficult search for pneumonia (or
lung opacity) in this CXR?
toderivemeaningfulinsights. Therefore,thereisaneedto
(c.1) Lung opacity (c.2) Pneumonia
create a new dataset with more focus and purposeful eye-
tracking data, improving its utility for diagnostic applica- Figure 1. (a) Given a CXR image, we are interested in radiolo-
tions.Inthiswork,weproposearefinementmethodinspired gist’seyemovementofradiologistwhentheysearchforafinding.
bythetarget-presentvisualsearchchallenge:thereisaspe- (b)But,theexistingeyegazedatasetsarerecordedinafree-view
cific finding and fixations are guided to locate it. After re- form,wherefixationsaredistributedacrosstheentireCXRimage
finingtheexistingeye-trackingdatasets,wetransformthem andmakingitunclearwhichfixationscorrespondtospecificfind-
ings. (c)OurnewGazeSearchdataset,wherefixationsequenceis
into a curated visual search dataset, called GazeSearch,
focusedforaspecificfinding. Forexample,thegazesequencein
specifically for radiology findings, where each fixation se-
(c.1)targetslungopacity,while(c.2)focusesonpneumonia.Each
quenceispurposefullyalignedtothetaskoflocatingapar-
circledepictsafixation,withthenumberandradiusindicatingits
ticularfinding. Subsequently,weintroduceascanpathpre-
orderandduration,respectively.
diction baseline, called ChestSearch, specifically tailored
to GazeSearch. Finally, we employ the newly introduced or adjust [64]. As a result, the collaborative approach be-
GazeSearch as a benchmark to evaluate the performance tweenAIandprofessionalshassuccessfullyimprovedradi-
of current state-of-the-art methods, offering a comprehen- ological diagnosis in many cases compared to radiologists
sive assessment for visual search in the medical imaging orthesystemalone[57].However,akeychallengeisbuild-
domain. ingtrustinAI,especiallywithblack-boxmodelsinhealth-
care,suchasCXRanalysis. Thishasincreasedthedemand
formodelsthatmimicradiologists’behaviortoimprovein-
1.Introduction terpretability. For instance, aligning AI systems with radi-
ologists’visualattentionpatternsisessential[47,50]. This
ArtificialIntelligence(AI)hasbeengrowingrapidlyand has opened a new domain of research focused on model-
become an important part of daily life [3,27,36,49,57, ingtheradiologists’seyemovementstoimprovethetrans-
64], including important workers like clinical experts and parencyandreliabilityofAIsystemsinclinicalpractice[6].
healthcare providers. Even though human experts remain Recognizing the importance of understanding how ra-
the ultimate authority in decision-making, researchers are diologists’ eye movements impact diagnosis, datasets like
focusing on improving AI-assisted systems to reduce the EGD [35] and REFLACX [4] have been introduced. But,
burdenfortheexperts. Forexample,wecanuseAItopro- theseeye-trackingdatasetspresenttwomajorchallenges:
ducepreliminaryresultsandtheexpertscaneitherconfirm Challenge #1: Free-view format - Existing eye-tracking
1
4202
voN
8
]VC.sc[
1v08750.1142:viXradatasets are collected in a free-view format, where fixa- Ourmaincontributionsare:
tionsaredistributedacrosstheentireCXRimage,makingit • GazeSearch: Weproposeaprocessingtechniquethat
unclear which fixations correspond to specific findings (as convertsfree-vieweyegazedataintofinding-awarera-
showninFigure1(b)). Moreover,thesedatasetsoftencon- diologist’s visual search data. This curated dataset is
tain ambiguity and suffer from misalignment between the the first target-present visual search dataset for chest
recordedfixationsandthefindingsinthereport, rendering X-ray, making possible deep learning modeling of
themunsuitableforaccuratescanpathprediction. medicalvisualsearchprediction.
Challenge #2: Lack of finding-aware radiologist’s scan- • ChestSearch: Weproposeatransformer-basedmodel
path models - Most existing scanpath prediction mod- that utilizes a radiology pretrained feature extractor
els[43,66,68]aredesignedforgeneralapplicationsandlack andquerymechanismtochooseonlyrelevantfixations
the domain-specific expertise needed for radiology. Fur- topredictsubsequentfixationsbasedonpreviousones.
thermore, current models trained on medical eye-tracking Additionally,weevaluateChestSearchagainstseveral
data are not tailored to the challenges of finding-aware vi- leading generic scanpath prediction models using our
sual search in radiology. For instance, I-AI [47] only as- GazeSearch to showcase the current progress in the
sociates diseases with abnormalities in specific anatomical medicalvisualsearchtask.
areas. While RGRG [55] uses anatomical bounding boxes
2.Relatedworks
withoutconsideringgazeforreportgeneration.
Toaddressthechallenge#1,weproposeafinding-aware Visual Search Datasets. Search datasets have been rising
radiologist’s visual search dataset, named GazeSearch. recentlyduetotheinterestinunderstandinghumanbehav-
Ourobjectiveistominimizethemisalignmentbetweenthe ior[8,21,22,25,32,46,59,65,66,70].Thisisparticularlyevi-
findingsextractedfromtheradiologyreportsandtheircor- dentinthegeneralvisualdomain,wherenumerousdatasets
respondingfixations.Inspriredbythevisualsearchdatasets have been created across diverse settings. These datasets
likeCOCO-Search18[66]orAir-D[8],wefurtherprocess coverawiderangeof scenarios, from searching formulti-
GazeSearchbyreducingthefixationlengthusingaradius- ple targets simultaneously [25] to focusing on a single or
based filtering heuristic, ensuring that the direction of fix- two target categories [22,70]. Some datasets, like COCO-
ationsremainsclearandmanageable. Additionally,forev- Search18 [66], feature a large number of target objects, or
eryfinding, weensurethatthedurationoffixationswithin adopt a Visual Question Answering approach [8]. In con-
the location of the given finding is maximized. To create trast,themedicaldomainhaslaggedbehindintermsofded-
GazeSearch dataset, we utilize the existing free-view eye icatedvisualsearchdatasets. Existingmedicaldatasetspri-
gazedatasetsEGD[35]andREFLACX[4](Figure1(b))to marily focus on multi-target search tasks, as demonstrated
conduct a finding-aware radiologist’s visual search dataset by datasets like EGD [35] and REFLACX [4]. However,
(Figure 1(c)), which produces two scanpaths for particular thereisasignificantlackofsearchdatasetstailoredforthe
findingse.g.,“lungopacity”(Figure1(c.1))and“pneumo- medicaldomain. Thispapermakesanovelcontributionby
nia”(Figure 1 (c.2)) in this example. The goal of releas- addressingthisresearchgap. Weintroducethefirsttarget-
ing this dataset is to foster the development of algorithms present visual search dataset specifically designed for the
thatbettermimicradiologists,especiallyfocusingonunder- medical field. This dataset opens up new avenues for re-
standing observation sequences, attention (duration), fre- searchanddevelopmentinthiscriticalarea.
quencyonkeyregions,andexpertknowledge[45,63]. Visual Search Baselines. Parallel to the growth of visual
To address challenge #2, we introduce ChestSearch, search datasets, significant advancements have been made
a scanpath prediction architecture that surpasses existing in scan path prediction accuracy [1,2,7,9–11,15,17,23,
models. ChestSearch builds on a standard meta architec- 28,31,38,43,48,53,54,61,66–69,71]. Early scanpath
ture[14]featuringafeatureextractor[26,40]andaTrans- models mostly rely on sampling fixations from saliency
formerdecoder[56],withtwokeyenhancements. First,we maps [30,42,60,62]. Recent advancements, including the
trainthefeatureextractorusingtheself-supervisedMGCA integrationofdeepneuralnetworks[10,17,33,37,43,48,54,
method[58]onthelargeMIMIC-CXR[34]dataset,provid- 66,68,69], reinforcement learning techniques [10,66,68],
ingastronginitializationfortraining.Second,weutilizethe and transformer-based architectures [11,43,48,67], have
modifiedcrossattentionfrom[13]withaquerymechanism significantly deepened our understanding of the temporal
to select only relevant fixations for predicting the next fix- dynamics of human attention. However, generic models
ation. Then, the model’s three heads handle distinct tasks: are designed for broad application, so the performance of
predicting 2D coordinates, duration, and stopping points. generic visual search models on CXR is uncertain and po-
Finally, we benchmark ChestSearch against current state- tentiallysubpar. Thisworkintroducesatransformer-based
of-the-artvisualsearchmodelsonGazeSearch,showcasing methodthatcanworkwellwithouttheserestrictiveassump-
thecurrentadvancementsinradiologyvisualsearch. tions. Additionally, we further conduct a comparative ex-
2Algorithm1Radius-basedFilteringProcedure
Raw Fixations Final Fixations Input: Image width W, image height H, bounding
boxes B, max length M, radius r, fixations F =
{(x ,y ,d ),(x ,y ,d ),...,(x ,y ,d )}
1 1 1 2 2 2 n n n
Output: FilteredfixationsFˆ
Initialize: Fˆ =(W/2,H/2,0.3)
//ThelastpointmustbeinsideB.
j ←max{i|(x ,y )∈B,(x ,y ,d )∈F,1≤i≤n}
i i i i i
Reports "CaF ri dn id oi mng e ga ly" //Applyradiusheuristicwithloopingbackward.
c←{(x ,y )},where(x ,y ,d )∈F
j j j j j
Na Miv ae p F pi in nd ging V Ci Is mou n pa s ol t sS r iae ti ia n or t nc sh foreachpoint(x i,y i,d i)∈F fromj−1to1do
if(x ,y )iswithinradiusrof(x ,y )then
i i i+1 i+1
c←c∪{(x ,y )}
i i
else
x← 1 (cid:80) x ,y ← 1 (cid:80) y ,d←(cid:80) d ,
|c| k k |c| k k k k
Finding-Anatomy where(x ,y ,d )∈c
Relation Matrix k k k
c←{(x ,y )}
i i
Fˆ ←Fˆ∪{(x,y,d)}
if|Fˆ|=M then
Finding-aware Fixations Anatomy Bounding Boxes break
endif
endif
Figure2. PipelineofGazeSearchcreation,whichprocessesfree-
vieweyegazedataasinputandoutputsafinding-awarescanpath. endfor
ifc̸={}and|Fˆ|<M then
x← 1 (cid:80) x ,y ← 1 (cid:80) y ,d←(cid:80) d ,
|c| k k |c| k k k k
perimentbetweenstate-of-the-artmethodsfromthegeneral where(x ,y ,d )∈c
k k k
visualdomainandourproposedmethod,providingacom- Fˆ =Fˆ∪{(x,y,d)}
prehensive evaluation of their performance in the medical endif
domain.
Algorithm2Time-spentConstrainingProcedure
3.GazeSearchDataset
Input:Fˆ ={(x ,y ,d ),(x ,y ,d ),...,(x ,y ,d )},
1 1 1 2 2 2 n n n
When studying free-view eye-tracking datasets from boundingboxesB
sources like REFLACX [4] and EGD [35], we notice that Output: ConstrainedfixationsF′
theeye-trackingdata(includingbothgazeandfixations)is dout ← {(cid:80)n i=k,(xi,yi)∈/Bd i|(x k,y k,d k) ∈ Fˆ,1 ≤ k ≤
often ambiguous and lacks clarity. This ambiguity comes n}.
from the data collection settings, where radiologists look din ← {(cid:80)n i=k,(xi,yi)∈Bd i|(x k,y k,d k) ∈ Fˆ,1 ≤ k ≤
formultiplefindingssimultaneously. Asaresult, eachfix- n}.
ationcapturesvisualinformationrelevanttomultiplefind- D ←{i|din ≥dout,1≤i≤n}.
i i
ingsratherthanaspecificfinding. Therefore, thefixations if1∈/ Dthen
fromtheseeye-trackingdatasetsareunsuitableforstudying t←minD
theirrelationshiptospecificfindings,i.e. addressingthevi- F′ ←{(x ,y ,d )|i≥t,(x ,y ,d )∈Fˆ}
i i i i i i
sual search problem. Additionally, when visualizing these endif
gaze points or fixations over an image, they often cover
more than 80% of the lung area, even though the actual
anomalyareamightbemuchsmaller. Wecalculatethefix- collect data by having radiologists examine each of the 14
ation coverage distribution in Supplementary. This raises standard findings in CheXpert [29], would be costly and
aconcernthatusingthefree-viewfixationsfromthegiven time-consuming. Therefore, this paper will propose an al-
datasets may not be effective and could even pose risks in ternativetechniquethatleverageseye-trackingdatadirectly
sensitive sectors like healthcare, particularly for tasks re- from the free-view setting to convert to the finding-aware
quiringpreciselocalizationofspecificfindings. visualsearchsetting.
To solve this issue, one way is to collect eye-tracking Inspired by visual search, we studied the COCO-
data under the visual search setting directly. However, to Search18[66],Air-D[8],andCOCO-Freeview[12,69],and
3identified two key properties that are required in a visual over15yearsofexperiencethoroughlyreviewsandrefines
searchdataset: thematrix. ThefinalizedmatrixisincludedintheSupple-
Property#1: Latefixationstendtoconvergetomoredeci- mentary Material. Once the relation matrix is completed,
siveregionsofinterest(ROIs)[8]. And,Shietal.[8]have wereferencethegivenfindingc′toidentifythecorrespond-
concludedthelatefixationsareforsearching. inganatomiesandutilizethegroundtruthanatomybound-
Property#2: Thefixationswithintheobjectofinteresttend ingboxesprovidedbyChestImaGenomeasourB forthe
tohavelongerdurations,whilethoseoutsidetheobjectare subsequentsteps.
typicallyshorter.
3.3.VisualSearchConstraintImposition
Based on those two facts, we propose an approach to
convertfromfree-viewdataintoavisualsearchformat,en- AfterSection3.1,themaximumfixationsequencelength
suringthefilteredfixationsretainproperties#1and#2with- canbeover340fixationsforafinding. Therefore, another
out sacrificing too many fixations. Figure 2 illustrates the taskwemustsolveisreducingthislengthtoaninterpretable
overview of our data processing pipeline, including Naive levelforhumans.
FindingMapping(Section3.1)tocleanirrelevantfixations Utilizingbothproperties(1)and(2)asourguidancefor
foragivenfinding,Finding-AnatomyRelationMatrix(Sec- this process, we perform two main steps: radius-based fil-
tion 3.2) to extract key regions, and finally Visual Search tering(toenforceproperty#1)andtime-spentconstraining
ConstraintImposition(Section3.3)toproducethefixations (to enforce property #2). Besides property #1, we observe
thathavebothvisualsearchproperties. thatthecapturedfixationsfromEGDandREFLACXcover
one-degree visual angle [4,35,39]. Based on that fact, we
3.1.NaiveFindingMapping
use the Algorithm 1 to cluster the finding-aware fixations
Thefirstproblemwemustsolveisthemismatchbetween F tocreateanotherfixationsetFˆ,withalargerradiusrof
thefixationsandthecorrespondingradiologists’reportsen- two-degreeofvisualangleandM isthemaxlengthoffix-
tences. Themainreasonisradiologistsobservetheimages ation sequence. Property #1 is enforced by iterating back-
firstandthendescribetheirfindings,meaningthefixations wardfromtheendtothebeginningofthefixationsequence
within the time frame of a sentence may not fully capture F. Then,weusetheAlgorithm2tomakesurethelatefixa-
the findings reported. Inspired by I-AI [47], we start by tionsmustspendthemosttimeintheanatomiesofinterest,
completelyremovingfixationsafterthecurrentspokensen- whichsatisfiesproperty#2.
tence. Let S = {s ,s ,...,s } be the sequence of sen- InAlgorithms1and2,wedefineapoint(x,y)tobein
1 2 |S|
tences in the transcript. Let C = {c ,c ,...,c } be the theboundingboxsetsBfornotationconvenience:
1 2 m
set of possible findings (e.g., CheXpert labels). We define
(x,y)∈B ⇐⇒xleft ≤x≤xright,ytop ≤y ≤ybottom,
a function ϕ : S → C where c = ϕ(s ) if sentence s
j i i
corresponds to finding c . In our implementation, ϕ(·) is ∀(xleft,ytop,xright,ybottom)∈B (2)
j
the Chexbert model [51]. For a target finding c′ ∈ C, let
ToalignwiththeCOCO-Search18dataset,wesetthemax-
u = max{i|ϕ(s ) = c′,1 ≤ i ≤ |S|}. Then, the new
i imum fixation length to M = 7 and add a default center
finding-awarefixationsF forc′is
as the start fixation. This choice is based on the observa-
F ={(x ,y ,t ,d )|(x ,y ,t ,d )∈F,0≤t ≤e } (1) tionthat95%ofthesamplesinCOCO-Search18havefix-
i i i i i i i i i u
ation lengths under 7. For the first fixation’s duration, we
where F = {(x ,y ,t ,d ),..,(x ,y ,t ,d )} is assign0.3secondstoit,whichreflectsthedurationof91%
1 1 1 1 |F| |F| |F| |F|
thefree-viewfixations,with(x ,y )asspatialcoordinates, of first fixations in COCO-Search18. In total, GazeSearch
i i
t as captured timestamp, and d as duration, and e is has 2,081 images with 413 samples from EGD and 1,668
i i u
the ending time of the sentence s . From this point on- samplesfromREFLACX.Thereareatotalof13findings.
u
wards, we only use the triplet (x ,y ,d ) and ignore the Eachsamplehasfixationsfor1to6findingsandhasamax
i i i
captured timestamp t for our fixation sequence: F = lengthof7,includingthedefaultmiddlefixation. Fortrain-
i
{(x ,y ,d ),...,(x ,y ,d )},wheren=|F|isthefixa- ingandevaluation, wesplitthedatasetinto1,456samples
1 1 1 n n n
tionsequencelength. for training (70%), 208 samples for validation (10%), and
417samplesfortesting(20%).
3.2.Finding-AnatomyRelationMatrix
3.4.UsageValidation
Toaddressthis,weleveragetheChestImaGenome[63]
dataset,whichofferspairsoffindingsandtheircorrespond- Filteringfixationsrequiresdiscarding information, soit
inganatomies, alongwithanatomyboundingboxeslinked is essential to test and ensure that the new data remains
to each finding. For precision, we rely on the gold subset valuable. TovalidatethatGazeSearch’sfixationscanbeas
ofChestImaGenometoconstructarelationmatrixbetween useful as the free-view fixation maps from EGD and RE-
findings and anatomies. As a final step, a radiologist with FLACX, we follow Karargyris et al. [35] to perform the
4Table1. UsagevalidationexperimentsonourGazeSearch. mHC cess begins by applying a Feature Extractor (Section 4.1)
(meanHeatmapCoverage)istheaverageratiooftheheatmaparea to process I to extract both detailed and high-level vi-
tothelungareaacrossallimagesinGazeSearch.
sual features. Following this, a Spatiotemporal Embed-
ding (Section 4.2) embeds previous fixations, combined
Method FixationType AUC mHC
with multi-resolution features, to capture contextual rela-
NaiveClassifier ✗ 76% ✗
tionships within the sequence. These features are passed
Freeview 81% 91%
TemporalClassifier
Finding-aware(GazeSearch) 81% 44% through a transformer decoder with cross-attention, self-
attention, feedforward layers, and normalization (Sec-
tion 4.3) to create a decoded latent feature. Finally, the
Spatiotemporal Embedding
decoded feature is fed into three heads to predict the next
...
K V Q
fixation: termination prediction (Section 4.4), fixation du-
Cross Attention
Self Attention Add & Norm ration (Section 4.5), and distribution for the next fixation
Self Attention
Add & Norm (Section4.6)
... FFN
Add & Norm
4.1.FeatureExtractor
Fixation Decoder
1D Temporal Embedding
Usingfeaturesfromonlythelastlayerisinadequatefor
2D Positional Encoding MLP predicting scanpaths [68]. Therefore, we employ ResNet-
Feature Extractor
50 FPN [40] as our Feature Extractor module (FE). Be-
... sides, using the ImageNet [18] checkpoint may not be op-
Yes
timal for the medical domain, so we train the FE using a
2D Spatial Indexing
self-supervised approach based on MGCA [58] with the
Sample 0.76
... MIMIC-CXR dataset [34]. From the CXR image I with
size H × W, FE produces four multi-scale feature maps
P = {P1,...,P4}. Then we need to mimic how human
...
seeanimage: atfirstweonlyseetheimageatahighlevel
understanding,withnocleardetails,andthenwelookcare-
fullytosearchforwhatweneed[66].Soweuseonefeature
map with low resolution Pl = P1 ∈ RC× 3H 2×W 32, where
Figure 3. The figure provides a detailed view of Chest-
C is the channel dimension, to represent high-level visual
Search. It begins by processing the previous fixations, denoted
as {(x ,y ,d )}t−1, along with the input chest X-ray image I, feature, and one high-resolution feature map Ph = P4 ∈
throughi ai Fei atui r= e1 Extractor and Spatiotemporal Embedding to RC×H 4×W 4 torepresentdetailedvisualinformation.
generatesthespatiotemporalembeddedfeatureE. Next,theFix-
4.2.SpatiotemporalEmbedding
ationDecoderusesalearnablequeryq andtheembeddedfeature
c
E todecodeitintoafeatureE¯. Fromhere,threeheadsuseE¯ to Giventhepreviouspredictedfixations{(x ,y )}t−1,Pl,
predictthenextfixationcoordinates(xˆ t,yˆ t,dˆ t). Here, atstept, andPh,wethenembedthepreviousfixationi stoi cri e= a1
tethe
theterminationheadoutputs“Yes,”indicatingthatthisisthefinal
featurelistastheinputforthedecoderinSection4.3.
fixationfortheimageI.
2DSpatialIndexing. Every(x ,y ), where0 ≤ x ≤ W
i i i
and0≤y ≤H,isscaleddowntothesameresolutionasof
TemporalHeatmapexperiment. Thisexperimentevaluates i
Ph,whichresultinthenew0 ≤ x′ ≤ W and0 ≤ y′ ≤ H
whether eye gaze data can enhance classifier performance i 4 i 4
inourcase.Then,weindexthefeaturecellatthecoordinate
when usingground truthfixations astemporal inputs. The
(x′,y′) in Ph, called Ph. We will have the list of feature
results,Table1,indicatethatdespiteusingonlyhalfthearea i i i
{Ph}t−1.
compared to the free-view setting, performance remains i i=1
2D Positional Embedding. For every Ph, we encode the
comparable. Detailedimplementationofthisexperimentis i
spatialinformationbyusingpositionalencodingtwice,first
providedintheSupplementary.
inthex-axis,theninthey-axis. Asthe2Dorderisimpor-
tant,weenforcethesinusoidversionofpositionalencoding.
4.ChestSearch
1DTemporalEmbedding. Wealso needtoletthe model
GivenaCXRimageIofdimensionsH×W andatarget knowtheorderofeachfixations. However,theroleoffixa-
findingc′,ourobjectiveistogenerateascan-pathcomprises tionorderindiagnosingCXRinpracticeiscomplicated,so
offixationsy ={f }n ,wherenrepresentsthenumberof weletthemodeldecidetheembeddingbyapplyingalearn-
i i=1
fixations,andf = (x ,y ,d )isthefixationat2Dcoordi- ablepositionembeddinghere. Thisresultsinthe{P¯h}t−1
i i i i i i=1
nate(x ,y )withadurationofd . sequenceofembeddedfeature.
i i i
Figure3providesanoverviewofourmethod. Thepro- SelfAttention. Finally,wefeed{P¯h}t−1 intoseverallay-
i i=1
5ersofself-attentiontoaaggregateinformationsothateach Formally,wecompute:
positionisinfluencedbytherelevantfixations. Intheself-
attention layers, we also provide the model with a low-
E¯′ =MLP(E¯)
resolutionfeaturemapPltosupplyhigh-levelfeatureinfor- hˆ =sigmoid(Matmul(E¯′,Ph)) (4)
t
mation.Thisintuitionisalsoproveneffectedempirically,as
itwillbeshownlaterinSection5.4. Thefinalembeddings whereMatmul(·,·)isthematrixmultiplicationbetweentwo
are E = {El} ∪ {E ih}t i=− 11, where El ∈ RD× 3H 2∗W 32 and input tensors, and E¯′ ∈ R|q|×D is the latent embedding
Eh ∈RD. preparedforheatmapgeneration. Atinference, wesample
i
the next 2D coordinate fˆ = (xˆ ,yˆ) from the distribution
t t t
4.3.FixationDecoder
maphˆ foreverygiventimestampt.
t
Atthislayer,wehavethefindinglistq = {q }|q| which
c c 4.7.ObjectiveFunctions
serves as the set of queries. The number of queries is the
numberoffindingsinourdataset|q|=13withq c ∈RD is ChestSearchhasthreeobjectives,eachcorrespondingto
alearnableembeddingforthecurrentfindingqueryc. The oneofitsheads: thelossbetweenthegroundtruthandpre-
previous module (Section 4.2) gives us the embeddings of dicteddistributions,thelossfortermination,andthelossfor
previousfixationsE. duration.
The Fixation Decoder module is the modified trans- The termination loss is just a standard binary cross-
formerdecoder[13]includingtheblocksasshowninFig- entropybetweenthepredictedterminationvalueτˆ andthe
t
ure3. Thecross-attentionblockusesthequeryembedding correspondinggroundtruthτ .
t
q as the query input Q, with E serving as both key (K)
and value (V). This allows the model to capture the corre- L =−τ log(τˆ)−(1−τ )log(1−τˆ), (5)
τ t t t t
lationsamongpreviousfixationsandaccuratelypredictthe
Thedistributionlossisdefinedasfocalpixel-wiseloss:
nextfixation.Theresultingfeaturethenpassesthroughself-
a fett ee dn -t fi oo rn wl aa ry der ns e, tr we osi rd ku .a Tl hc io sn pn re oc ct eio sn ss r, en po er am tsa fl oiz ra Ltio ln ay, ea rn sd ina L h=− N1 (cid:80) ij(cid:26) (( 11 −− hhˆ ij )) αγ (lo hˆg( )hˆ γij lo)
g(1−hˆ )
i of thh eij rw= ise1 ,, (6)
thedecoder. ThefinaloutputE¯ ∈R|q|×D isthenprocessed ij ij ij
bythreedifferentheads. where0 ≤ i ≤ H, 0 ≤ j ≤ W arethe2Dindexes, N =
4 4
H ∗ W is the number of values, α and γ are the hyper-
4.4.TerminationHead 4 4
parameters indicating the importance of each pixel. The
A fixation sequence’s length can vary, so our model durationlossisdefinedastheL1loss,i.e.,L d =|dˆ t−d t|.
needs to learn when to stop. To achieve this, we use a Finally,wetrainallthreelossesjointly.
head consisting of a fully connected (FC) layer followed
byasigmoidfunctionthatmapsE¯toterminationvaluei.e., L=L
τ
+L h+L
d
(7)
τˆc ∈R=sigmoid(FC (E¯)).
t τ
4.5.DurationHead 5.Experiments
The duration can be considered as a Gaussian distribu- 5.1.ImplementationandMetrics
tion. We use E¯, then regress it into a mean value µ =
FC (E¯)andalog-varianceλ =FC (E¯):
dt
Implementation details. All images are scaled down to
µ dt λ 224×224forcomputingefficiency. TheFixationDecoder
dˆ =µ +ϵ ·exp(0.5λ ), hasL = 6layerswithahiddendimensionD = 384. The
t dt dt dt (3) MLP of Fixation Distribution Head consists of 384 units
ϵ ∼N(0,1)
dt with3layersandReLUactivation.Eq.(6)hasα=4γ =2
basedonthebestvalidationresults.TheFeatureExtractor’s
whereϵ noisegivesourpredictionaprobabilisticcharac-
dt backboneisResNet-50[26], andweobtaintheResNet-50
teristic,anddˆ ∈R|q| isthedurationprediction. Theinspi-
t checkpoint using MGCA [58] for 50 epochs with a batch
ration comes from using the reparameterization trick [20],
size of 144. We then finetune this checkpoint jointly with
whichallowsustobackpropagatefromthelabelbacktothe
thefullpipeline. Wetrainthefullpipelinefor30,000iter-
normaldistribution. ations with a learning rate of 1 × 10−5 and a batch size
of 32. The entire training process was conducted using
4.6.DistributionHead
AdamW[41],onasingleA6000GPUwith48GBofRAM.
Because fixation is random in nature, we predict a 2D Metrics. We evaluate fixation scanpath prediction accu-
distributionintheformofaheatmaphˆ t ∈[0,1]|q|×(H 4∗W 4 ). racyusingvariousmetrics: ScanMatch[16,52]appliesthe
6Table2.PerformancecomparisonbetweenourChestSearchandSOTAvisualsearchmethods.
ScanMatch↑ MultiMatch↑
Method SED↓ STDE↑
w/oDur. w/Dur. Vector Direction Length Position Duration
IRL[66] 0.1495 - 0.8248 0.6402 0.7688 0.6998 - 6.6250 0.7043
FFMs[68] 0.2766 - 0.8914 0.6567 0.8785 0.8140 - 5.9221 0.8055
ChenLSTM[10] 0.2751 0.2153 0.8825 0.6222 0.8731 0.7940 0.6384 5.3468 0.7841
ChenLSTM-ISP[11] 0.2863 0.2205 0.8847 0.6430 0.8721 0.7980 0.6504 5.2895 0.7865
Gazeformer[43] 0.2971 0.2042 0.9080 0.6506 0.9035 0.8147 0.5901 5.1024 0.8030
Gazeformer-ISP[11] 0.2736 0.2146 0.9038 0.6181 0.8892 0.8031 0.6755 5.1905 0.7875
HAT[67] 0.3120 - 0.9064 0.6443 0.9065 0.8138 - 5.0613 0.8006
OurChestSearch 0.3321 0.2232 0.9173 0.6790 0.9174 0.8293 0.6951 4.8831 0.8089
Image Radiologists Ours ChenLSTM-ISP Gazeformer Gazeformer-ISP HAT
Figure 4. Qualitative results between our ChestSearch compared with ChenLSTM-ISP, Gazeformer, Gazeformer-ISP, and HAT. Four
different findings (rows) including Atelectasis, Cardiomegaly, Edema, and Lung lesion are shown from the top to bottom. Each circle
representsafixation,withthenumberandradiusindicatingitsorderandduration,respectively.AsHATonlypredicts2Dcoordinates,we
letallpredictedfixationsofHAThavethesameradius.
Needleman-Wunschalgorithm[44]tocomparefixationlo- moredetails,pleaserefertotheSupplementary.
cations and durations; MultiMatch [19] assesses similar-
ity across five dimensions; String-Edit Distance (SED) [5, 5.2.Quantitativeresults
24]comparescharacterstringsrepresentingimageregions;
and Scaled Time-Delay Embedding (STDE) [60] mea- Table 2 demonstrates the proposed method’s superior
sures mean minimum Euclidean distances between sub- performance, surpassing SOTA approaches. Note that
sequencesofpredictedandgroundtruthscanpaths. IRL, FFMs, and HAT do not predict fixation duration,
so their evaluation on this metric is excluded. IRL and
Compared Methods. We evaluate several state-of-the- FFMs face challenges with sample efficiency due to re-
art (SOTA) visual search methods on our GazeSearch: inforcement learning pipelines, while ChenLSTM variants
IRL [66], FFMs [68], ChenLSTM [10], Gazeformer [43], and ISP methods are limited by their specialized mod-
ChenLSTM-ISP[11],Gazeformer-ISP[11],andHAT[67]. ules—ChenLSTMreliesonpretrainedobjectdetectorsand
Note that Gazeformer and Gazeformer-ISP require a pre- ISPs on Observer-Centric modules. HAT and Gazeformer
trained CLIP component to encode the finding names, so overgeneralizeandfailtofullyleveragedomain-specificin-
wereplaceitsdefaultCLIPwithBiomedCLIP[72]. Wead- formationbydesign, withHATignoringdurationdataand
heretotheoriginaltrainingpracticesforallbaselines. For Gazeformer relying heavily on CLIP for zero-shot visual
7
sisatceletA
ylagemoidraC
amedE
noisel
gnuLsearch.Ourmethodavoidstheselimitations.Highscoresin Table3.Theroleoflow-andhigh-resolutionfeaturemaps.
metrics such as ScanMatch, MultiMatch, SED, and STDE
demonstrate our method’s capability to effectively capture Reference Indexing ScanMatch↑ MultiMatch↑ SED↓ STDE↑
w/oDur. w/Dur.
complexscanpathdynamics,settinganewstandardinchest Pl ✗ 0.1848 0.2029 0.7070 6.3636 0.7066
X-raytarget-presentvisualsearch. Ph ✗ 0.1939 0.1925 0.7058 6.1424 0.7184
✗ Pl 0.3077 0.2177 0.7927 5.0180 0.8027
✗ Ph 0.3176 0.2204 0.7985 4.9078 0.8035
5.3.Qualitativeresults Pl Pl 0.3129 0.2228 0.7989 4.9100 0.8060
Ph Ph 0.3221 0.2229 0.8015 5.0277 0.8058
Ph Pl 0.3184 0.2210 0.8022 5.0224 0.8057
Figure 4 presents a qualitative comparison of scan- Pl Ph 0.3321 0.2232 0.8076 4.8831 0.8089
path patterns across different radiology findings and mod-
Table4.Ablationstudyofchoosinginitialweight.
els, including radiologists and several state-of-the-art ap-
proaches. Generally, ChestSearch predicts more con-
ScanMatch↑
sistent and radiologist-like fixations than other methods. InitalWeight MultiMatch↑ SED↓ STDE↑
w/oDur. w/Dur.
ChenLSTM-ISP often exhibits scattered, less focused pat- RandomInit. 0.3130 0.2205 0.79224 5.0331 0.8058
ImageNet 0.3238 0.2224 0.79942 4.9723 0.8081
terns,whileGazeformer-ISPmayoverlookkeyareasorfo-
Ours(Self-supervised) 0.3321 0.2232 0.80762 4.8831 0.8089
cusonfewerlocations. AlthoughGazeformeralignsbetter
withgroundtruththanitsISPvariant,itoccasionallymisses 6.Conclusion
criticalregions,suchaslunglesions.HATperformsreason-
ably well but frequently covers the entire lung, even when Thispaperaddressestwokeychallenges:ambiguousfix-
attentionshouldbelimitedtosmallerareas,suchasincar- ationsinexistingeye-trackingdatasetsandtheabsenceofa
diomegaly. Incontrast,ourChestSearchshowsfixationpat- finding-awareradiologist’sscanpathmodel. Drawinginspi-
ternsmorecloselyresemblingthoseofradiologists,outper- ration from visual search datasets in general domains, we
forming other state-of-the-art methods. Overall, Figure 4 alignfindingswithfixations,managefixationdurationsus-
underscorestheeffectivenessofourapproachinmimicking ingaradius-basedheuristic,andconstrainfixationsondura-
expert gaze patterns across different findings. Additional tiontoproducethefirstfinding-awarevisualsearchdataset,
comparisonwillbeincludedintheSupplementary. GazeSearch. Our dataset reflects two key properties of vi-
sualsearchbehavior: #1latefixationstendtoconvergeon
5.4.Ablationstudy decisiveregionsofinterest,and#2fixationswithinobjects
ofinterestaretypicallylongerindurationcomparedtothose
Tostudythedesignchoiceofourproposedarchitecture, outside. We then propose ChestSearch that utilizes self-
weablateourmethodunderseveralaspects. Moreablation supervised training to obtain a medical pretrained feature
studies are provided in the Supplementary Materials. All extractorandaquerymechanismtoselectrelevantfixations
experiments are performed on our dataset, with the same for predicting subsequent ones. The extensive benchmark
trainingsettings. Here,wereporttheaverageofallaspects shows ChestSearch ’s ability to generate radiologist-like
fortheMultiMatchscore scanpaths,servingasastrongbaselineforfutureresearch.
The importance of low- and high-resolution feature Discussion: Our work impacts the behavioral vision
maps. InSection4.2, guidedbyourintuition, weusetwo literature in the medical domain, where (i) modeling and
feature maps: a low-resolution map for high-level visual replicating radiologists’ behavior has not been explored,
understandingandahigh-resolutionmapfordetailedvisual (ii)understandingtheunderstandingoffinding-awarevisual
understanding. Theseareconcatenatedintoasingletensor search and their integration with Deep Learning remains
fortheSelf-Attentionlayer,withthelow-resolutionfeature poorly understood [45]. These are critical for advancing
serving as a reference and the high-resolution feature in- diagnostics in radiology, enhancing decision-making pro-
dexed using2DSpatialIndexingtogeneratetemporalfea- cesseswithaccuracyandtrustworthiness,andenablingthe
tures. Ablation results in Table 3 show that omitting 2D future development of collaborative interactions between
Spatial Indexing results in a significant performance drop radiologistsandAIsystems. Morebroadly,ourbenchmark
duetothelossoftemporalinformation. Conversely,notus- serves as the first dataset specifically designed for medi-
ingthereferencefeaturebeforeSelf-Attentionhasalesser calfindingsearchprediction, settingthefoundationforre-
impact. searchacrossawiderangeofmedicalapplications,includ-
InitialFeatureExtractor’sweightcontribution. Thisab- inghuman-computerinteraction,errorcorrection,workflow
lationstudiestheeffectoftheinitialweightfortheFeature optimization, and expert analysis. Future work will focus
Extractor(Section4.1),showninTable4.Inconclusion,us- on expanding the dataset to include target-absent samples,
ing ImageNet checkpoint can give a decent performance. furtherenhancingthemodel’sabilitytohandlecomplexdi-
But with a better checkpoint, the performance is higher. agnosticscenariosbyidentifyingboththepresenceandab-
Thisshowstherobustnessofourarchitecture. senceoffindings.
8References transformerforuniversalimagesegmentation. InProceed-
ings of the IEEE/CVF conference on computer vision and
[1] HosseinAdeliandGregoryZelinsky. Deep-bcn: Deepnet-
patternrecognition,pages1290–1299,2022. 2,6
works meet biased competition to create a brain-inspired
[14] BowenCheng,AlexSchwing,andAlexanderKirillov. Per-
modelofattentioncontrol. InCVPRWorkshops,2018. 2
pixelclassificationisnotallyouneedforsemanticsegmen-
[2] Bahar Aydemir, Ludo Hoffstetter, Tong Zhang, Mathieu
tation. Advancesinneuralinformationprocessingsystems,
Salzmann, and Sabine Susstrunk. TempSAL - uncovering
34:17864–17875,2021. 2
temporalinformationfordeepsaliencyprediction. InPro-
[15] MarcellaCornia,LorenzoBaraldi,GiuseppeSerra,andRita
ceedings of the IEEE Conference on Computer Vision and
Cucchiara.Predictinghumaneyefixationsviaanlstm-based
PatternRecognition(CVPR),2023. 2
saliencyattentivemodel. IEEETransactionsonImagePro-
[3] SebastienBenzekry. Artificialintelligenceandmechanistic
cessing(IEEETIP),2018. 2
modelingforclinicaldecisionmakinginoncology. Clinical
[16] Filipe Cristino, Sebastiaan Mathoˆt, Jan Theeuwes, and
Pharmacology&Therapeutics,108(3):471–486,2020. 1
IainDGilchrist. Scanmatch:Anovelmethodforcomparing
[4] RicardoBigolinLanfredi,MingyuanZhang,WilliamFAuf-
fixationsequences.Behaviorresearchmethods,42:692–700,
fermann,JessicaChan,Phuong-AnhTDuong,VivekSriku-
2010. 6
mar,TraftonDrew,JoyceDSchroeder,andTolgaTasdizen.
[17] RyanAnthonyJalovadeBelen,TomaszBednarz,andArcot
Reflacx, a dataset of reports and eye-tracking data for lo-
Sowmya. Scanpathnet:Arecurrentmixturedensitynetwork
calization of abnormalities in chest x-rays. Scientific data,
forscanpathprediction. InProceedingsoftheIEEEConfer-
9(1):350,2022. 1,2,3,4
enceonComputerVisionandPatternRecognitionWorkshop
[5] StephanABrandtandLawrenceWStark. Spontaneouseye
(CVPRW),2022. 2
movementsduringvisualimageryreflectthecontentofthe
[18] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
visualscene. Journalofcognitiveneuroscience,9(1):27–38,
andLiFei-Fei. Imagenet: Alarge-scalehierarchicalimage
1997. 7
database. In2009IEEEconferenceoncomputervisionand
[6] NoraCastner,LubainaArsiwala-Scheppach,SarahMertens,
patternrecognition,pages248–255.Ieee,2009. 5
Joachim Krois, Enkeleda Thaqi, Enkelejda Kasneci,
[19] RichardDewhurst,MarcusNystro¨m,HalszkaJarodzka,Tom
Siegfried Wahl, and Falk Schwendicke. Expert gaze as a
Foulsham,RogerJohansson,andKennethHolmqvist. Itde-
usabilityindicatorofmedicalaidecisionsupportsystems: a
pendsonhowyoulookatit: Scanpathcomparisoninmul-
preliminarystudy. NPJDigitalMedicine,7(1):199,2024. 1
tipledimensionswithmultimatch,avector-basedapproach.
[7] Souradeep Chakraborty, Zijun Wei, Conor Kelton, Seoy-
Behaviorresearchmethods,44:1079–1100,2012. 7
oungAhn,ArunaBalasubramanian,GregoryJ.Zelinsky,and
DimitrisSamaras. Predictingvisualattentioningraphicde- [20] Carl Doersch. Tutorial on variational autoencoders. arXiv
signdocuments. IEEETransactionsonMultimedia(TMM), preprintarXiv:1606.05908,2016. 6
2022. 2 [21] HuiyuDuan,GuangtaoZhai,XiongkuoMin,ZhaohuiChe,
[8] ShiChen,MingJiang,JinhuiYang,andQiZhao. AiR:At- Yi Fang, Xiaokang Yang, Jesu´s Gutie´rrez, and Patrick Le
tentionwithreasoningcapability. InProceedingsoftheEu- Callet. A dataset of eye movements for the children with
ropeanConferenceonComputerVision(ECCV),2020. 2,3, autismspectrumdisorder.InACMMultimediaSystemsCon-
4 ference(MMSys),2019. 2
[9] ShiChen,NachiappanValliappan,ShaoleiShen,XinyuYe, [22] KristaAEhinger,BarbaraHidalgo-Sotelo,AntonioTorralba,
KaiKohlhoff, andJunfengHe. Learningfromuniqueper- andAudeOliva. Modellingsearchforpeoplein900scenes:
spectives: User-aware saliency modeling. In Proceedings Acombinedsourcemodelofeyeguidance.Visualcognition,
of the IEEE Conference on Computer Vision and Pattern 17(6-7):945–978,2009. 2
Recognition(CVPR),2023. 2 [23] Camilo Fosco, Vincent Casser, Amish Kumar Bedi, Peter
[10] Xianyu Chen, Ming Jiang, and Qi Zhao. Predicting hu- O’Donovan, Aaron Hertzmann, and Zoya Bylinskii. Pre-
man scanpaths in visual question answering. In Proceed- dicting visual importance across graphic design types. In
ingsoftheIEEEConferenceonComputerVisionandPattern ACM Symposium on User Interface Software and Technol-
Recognition(CVPR),2021. 2,7 ogy,2020. 2
[11] Xianyu Chen, Ming Jiang, and Qi Zhao. Beyond average: [24] TomFoulshamandGeoffreyUnderwood.Whatcansaliency
Individualizedvisualscanpathprediction. InProceedingsof modelspredictabouteyemovements?spatialandsequential
theIEEE/CVFConferenceonComputerVisionandPattern aspectsoffixationsduringencodingandrecognition. Jour-
Recognition,pages25420–25431,2024. 2,7 nalofvision,8(2):6–6,2008. 7
[12] Yupei Chen, Zhibo Yang, Souradeep Chakraborty, Sounak [25] Syed Omer Gilani, Ramanathan Subramanian, Yan Yan,
Mondal,SeoyoungAhn,DimitrisSamaras,MinhHoai,and David Melcher, Nicu Sebe, and Stefan Winkler. Pet: An
Gregory Zelinsky. Characterizing target-absent human at- eye-trackingdatasetforanimal-centricpascalobjectclasses.
tention. In Proceedings of the IEEE/CVF Conference on In2015IEEEInternationalConferenceonMultimediaand
ComputerVisionandPatternRecognitionWorkshops,pages Expo(ICME),pages1–6.IEEE,2015. 2
5031–5040,2022. 3 [26] KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun.
[13] BowenCheng,IshanMisra,AlexanderGSchwing,Alexan- Deepresiduallearningforimagerecognition.arxive-prints.
der Kirillov, and Rohit Girdhar. Masked-attention mask arXivpreprintarXiv:1512.03385,10,2015. 2,6
9[27] Nehmat Houssami, Georgia Kirkpatrick-Jones, Naomi [39] OlivierLeMeurandThierryBaccino. Methodsforcompar-
Noguchi,andChristophILee. Artificialintelligence(ai)for ingscanpathsandsaliencymaps:Strengthsandweaknesses.
theearlydetectionofbreastcancer: ascopingreviewtoas- BehaviorResearchMethods,45(1),2013. 4
sessai’spotentialinbreastscreeningpractice.Expertreview [40] Tsung-Yi Lin, Piotr Dolla´r, Ross Girshick, Kaiming He,
ofmedicaldevices,16(5):351–362,2019. 1 Bharath Hariharan, and Serge Belongie. Feature pyra-
[28] Xun Huang, Chengyao Shen, Xavier Boix, and Qi Zhao. mid networks for object detection. In Proceedings of the
SALICON: Reducing the semantic gap in saliency predic- IEEE conference on computer vision and pattern recogni-
tionbyadaptingdeepneuralnetworks.InProceedingsofthe tion,pages2117–2125,2017. 2,5
IEEEInternationalConferenceonComputerVision(ICCV), [41] ILoshchilov. Decoupledweightdecayregularization. arXiv
2015. 2 preprintarXiv:1711.05101,2017. 6
[29] JeremyIrvin,PranavRajpurkar,MichaelKo,YifanYu,Sil- [42] OlivierLeMeurandZhiLiu. Saccadicmodelofeyemove-
vianaCiurea-Ilcus, ChrisChute, HenrikMarklund, Behzad ments for free-viewing condition. Vision Research (VR),
Haghgoo, Robyn Ball, Katie Shpanskaya, et al. Chexpert: 2015. 2
Alargechestradiographdatasetwithuncertaintylabelsand [43] Sounak Mondal, Zhibo Yang, Seoyoung Ahn, Gregory
expertcomparison. InProceedingsoftheAAAIconference Zelinsky, Dimitris Samaras, and Minh Hoai. Gazeformer:
onartificialintelligence,volume33,pages590–597,2019. Scalable, effective and fast prediction of goal-directed hu-
3 man attention. In Proceedings of the IEEE Conference on
[30] Laurent Itti, Christof Koch, and Ernst Niebur. A model ComputerVisionandPatternRecognition(CVPR),2023. 2,
of saliency-based visual attention for rapid scene analysis. 7
IEEETransactionsonPatternAnalysisandMachineIntelli- [44] Saul B Needleman and Christian D Wunsch. A general
gence(IEEETPAMI),1998. 2 methodapplicabletothesearchforsimilaritiesintheamino
acidsequenceoftwoproteins.Journalofmolecularbiology,
[31] Sen Jia and Neil D. B. Bruce. EML-NET:an expandable
48(3):443–453,1970. 7
multi-layernetworkforsaliencyprediction. ImageandVi-
sionComputing,2020. 2 [45] Jose´ Neves, Chihcheng Hsieh, Isabel Blanco Nobre, San-
draCostaSousa,ChunOuyang,AndersonMaciel,Andrew
[32] Ming Jiang, Shengsheng Huang, Juanyong Duan, and Qi
Duchowski,JoaquimJorge,andCatarinaMoreira.Shedding
Zhao. Salicon: Saliency in context. In The IEEE Confer-
lightonaiinradiology: Asystematicreviewandtaxonomy
enceonComputerVisionandPatternRecognition(CVPR),
of eye gaze-driven interpretability in deep learning. Euro-
June2015. 2
peanJournalofRadiology,page111341,2024. 2,8
[33] Yue Jiang, Luis A. Leiva, Hamed R. Tavakoli, Paul R. B.
[46] DimPPapadopoulos,AlasdairDFClarke,FrankKeller,and
Houssel, JuliaKylma¨la¨, andAnttiOulasvirta. UEyes: Un-
Vittorio Ferrari. Training object class detectors from eye
derstanding visual saliency across user interface types. In
trackingdata. InEuropeanconferenceoncomputervision,
ACMCHIConferenceonHumanFactorsinComputingSys-
pages361–376.Springer,2014. 2
tems(CHI),2023. 2
[47] Trong Thang Pham, Jacob Brecheisen, Anh Nguyen, Hien
[34] Alistair Johnson, Matt Lungren, Yifan Peng, Zhiyong Lu,
Nguyen,andNganLe. I-ai: Acontrollable&interpretable
Roger Mark, Seth Berkowitz, and Steven Horng. Mimic-
aisystemfordecodingradiologists’intensefocusforaccu-
cxr-jpg-chestradiographswithstructuredlabels. PhysioNet,
rate cxr diagnoses. In Proceedings of the IEEE/CVF Win-
2019. 2,5
ter Conference on Applications of Computer Vision, pages
[35] Alexandros Karargyris, Satyananda Kashyap, Ismini 7850–7859,2024. 1,2,4
Lourentzou, Joy T Wu, Arjun Sharma, Matthew Tong, [48] Mengyu Qiu, Yi Guo, Mingguang Zhang, Jingwei Zhang,
Shafiq Abedin, David Beymer, Vandana Mukherjee, Eliza- Tian Lan, and Zhilin Liu. Simulating human visual sys-
bethAKrupinski, etal. Creationandvalidationofachest tembasedonvisiontransformer. InProceedingsofthe2023
x-ray dataset with eye-tracking and report dictation for ai ACMSymposiumonSpatialUserInteraction,2023. 2
development. ScientificData,8(1):1–18,2021. 1,2,3,4
[49] DanielLRubin. Artificialintelligenceinimaging: theradi-
[36] Enkelejda Kasneci, Kathrin Seßler, Stefan Ku¨chemann, ologist’srole.JournaloftheAmericanCollegeofRadiology,
Maria Bannert, Daryna Dementieva, Frank Fischer, 16(9):1309–1317,2019. 1
Urs Gasser, Georg Groh, Stephan Gu¨nnemann, Eyke [50] Cynthia Rudin. Stop explaining black box machine learn-
Hu¨llermeier,etal. Chatgptforgood? onopportunitiesand ing models for high stakes decisions and use interpretable
challengesoflargelanguagemodelsforeducation.Learning modelsinstead. Naturemachineintelligence,1(5):206–215,
andindividualdifferences,103:102274,2023. 1 2019. 1
[37] Matthias Ku¨mmerer, Matthias Bethge, and Thomas S. A. [51] Akshay Smit, Saahil Jain, Pranav Rajpurkar, Anuj Pareek,
Wallis. DeepGazeIII:Modelingfree-viewinghumanscan- AndrewY.Ng, andMatthewP.Lungren. Chexbert: Com-
pathswithdeeplearning. JournalofVision(JoV),2022. 2 biningautomaticlabelersandexpertannotationsforaccurate
[38] Matthias Ku¨mmerer, Thomas S. A. Wallis, and Matthias radiologyreportlabelingusingbert,2020. 4
Bethge. DeepGaze II: Reading fixations from deep [52] HiroyukiSogo. Gazeparser: anopen-sourceandmultiplat-
features trained on object recognition. arXiv preprint formlibraryforlow-costeyetrackingandanalysis.Behavior
arXiv:1610.01563,2016. 2 researchmethods,45:684–695,2013. 6
10[53] XiangjieSui, YumingFang, HanweiZhu, ShiqiWang, and [66] Zhibo Yang, Lihan Huang, Yupei Chen, Zijun Wei, Seoy-
Zhou Wang. ScanDMM: A deep markov model of scan- oung Ahn, Gregory Zelinsky, Dimitris Samaras, and Minh
path prediction for 360° images. In Proceedings of the Hoai.Predictinggoal-directedhumanattentionusinginverse
IEEE Conference on Computer Vision and Pattern Recog- reinforcementlearning. InProceedingsoftheIEEEConfer-
nition(CVPR),2023. 2 enceonComputerVisionandPatternRecognition(CVPR),
[54] WanjieSun, ZhenzhongChen, andFengWu. Visualscan- 2020. 2,3,5,7
path prediction using IOR-ROI recurrent mixture density [67] Zhibo Yang, Sounak Mondal, Seoyoung Ahn, Ruoyu Xue,
network. IEEE Transactions on Pattern Analysis and Ma- GregoryZelinsky,MinhHoai,andDimitrisSamaras.Unify-
chineIntelligence(IEEETPAMI),2019. 2 ingtop-downandbottom-upscanpathpredictionusingtrans-
[55] Tim Tanida, Philip Mu¨ller, Georgios Kaissis, and Daniel formers. InTheIEEEConferenceonComputerVisionand
Rueckert. Interactiveandexplainableregion-guidedradiol- PatternRecognition(CVPR),June2024. 2,7
ogyreportgeneration. InCVPR,2023. 2 [68] Zhibo Yang, Sounak Mondal, Seoyoung Ahn, Gregory
[56] AVaswani. Attentionisallyouneed. AdvancesinNeural Zelinsky, MinhHoai, andDimitrisSamaras. Target-absent
InformationProcessingSystems,2017. 2 human attention. In Proceedings of the European Confer-
enceonComputerVision(ECCV),2022. 2,5,7
[57] Stephen Waite, Zerwa Farooq, Arkadij Grigorian, Christo-
pher Sistrom, Srinivas Kolla, Anthony Mancuso, Susana [69] Zhibo Yang, Sounak Mondal, Seoyoung Ahn, Gregory
Martinez-Conde, Robert G Alexander, Alan Kantor, and Zelinsky,MinhHoai,andDimitrisSamaras. Predictinghu-
Stephen L Macknik. A review of perceptual expertise in manattentionusingcomputationalattention. arXivpreprint
radiology-howitdevelops,howwecantestit,andwhyhu- arXiv:2303.09383,2023. 2,3
mans still matter in the era of artificial intelligence. Aca- [70] Gregory Zelinsky, Zhibo Yang, Lihan Huang, Yupei Chen,
demicRadiology,27(1):26–38,2020. 1 SeoyoungAhn,ZijunWei,HosseinAdeli,DimitrisSamaras,
[58] FuyingWang,YuyinZhou,ShujunWang,VarutVardhanab- andMinhHoai. Benchmarkinggazepredictionforcategori-
huti, and Lequan Yu. Multi-granularity cross-modal align- calvisualsearch. InProceedingsoftheIEEEConferenceon
ment for generalized medical visual representation learn- ComputerVisionandPatternRecognitionWorkshops,pages
ing. Advances in Neural Information Processing Systems, 0–0,2019. 2
35:33536–33549,2022. 2,5,6 [71] MengmiZhang,JiashiFeng,KengTeckMa,JooHweeLim,
[59] Shuo Wang, Ming Jiang, Xavier Morin, Duchesne, Eliza- QiZhao,andGabrielKreiman.Findinganywaldowithzero-
bethA.Laugeson, DanielP.Kennedy, RalphAdolphs, and shotinvariantandefficientvisualsearch. Naturecommuni-
Qi Zhao. Atypical visual saliency in autism spectrum dis- cations,9(1):3730,2018. 2
orderquantifiedthroughmodel-basedeyetracking. Neuron, [72] Sheng Zhang, Yanbo Xu, Naoto Usuyama, Hanwen Xu,
2015. 2 Jaspreet Bagga, Robert Tinn, Sam Preston, Rajesh Rao,
[60] WeiWang,ChengChen,YizhouWang,TingtingJiang,Fang Mu Wei, Naveen Valluri, et al. Biomedclip: a multimodal
Fang,andYuanYao. Simulatinghumansaccadicscanpaths biomedicalfoundationmodelpretrainedfromfifteenmillion
onnaturalimages. InProceedingsoftheIEEEConference scientificimage-textpairs.arXivpreprintarXiv:2303.00915,
onComputerVisionandPatternRecognition(CVPR),2011. 2023. 7
2,7
[61] ZijunWei,HosseinAdeli,MinhHoai,GregoryZelinsky,and
DimitrisSamaras.Learnedregionsparsityanddiversityalso
predictvisualattention. InNeurIPS,2016. 2
[62] CaldenWloka,IuliiaKotseruba,andJohnK.Tsotsos. Ac-
tive fixation control to predict saccade sequences. In Pro-
ceedings of the IEEE Conference on Computer Vision and
PatternRecognition(CVPR),2018. 2
[63] Joy T Wu, Nkechinyere N Agu, Ismini Lourentzou, Arjun
Sharma, Joseph A Paguio, Jasper S Yao, Edward C Dee,
WilliamMitchell,SatyanandaKashyap,AndreaGiovannini,
etal. Chestimagenomedatasetforclinicalreasoning. arXiv
preprintarXiv:2108.00316,2021. 2,4
[64] Nan Wu, Jason Phang, Jungkyu Park, Yiqiu Shen, Zhe
Huang,MashaZorin,StanisławJastrzebski,ThibaultFe´vry,
JoeKatsnelson,EricKim,etal. Deepneuralnetworksim-
proveradiologists’performanceinbreastcancerscreening.
IEEE transactions on medical imaging, 39(4):1184–1194,
2019. 1
[65] Juan Xu, Ming Jiang, Shuo Wang, Mohan S. Kankanhalli,
andQiZhao.Predictinghumangazebeyondpixels.Journal
ofVision(JoV),2014. 2
11