On Differentially Private String Distances
Jerry Yao-Chieh Hu∗ Erzhi Liu† Han Liu‡ Zhao Song§ Lichen Zhang¶
Abstract
Given a database of bit strings A ,...,A 0,1 n, a fundamental data structure task is
1 m
∈ { }
toestimatethedistancesbetweenagivenqueryB 0,1 nwithallthestringsinthedatabase.
∈{ }
In addition, one might further want to ensure the integrity of the database by releasing these
distance statistics in a securemanner. Inthis work,we proposedifferentially private (DP) data
structures for this type of tasks, with a focus on Hamming and edit distance. On top of the
strong privacy guarantees, our data structures are also time- and space-efficient. In particular,
ourdatastructureisǫ-DPagainstanysequenceofqueriesofarbitrarylength,andforanyquery
B such that the maximum distance to any string in the database is at most k, we output m
distance estimates. Moreover,
• ForHammingdistance,ourdatastructureanswersanyqueryinO(mk+n)time andeach
estimate deviates from the true distance by at most O(k/eǫ/logk);
e
• For edit distance, our data structure answers any qeuery in O(mk2 +n) time and each
estimate deviates from the true distance by at most O(k/eǫ/(logklogn)).
e
For moderate k, both data structures support sublinear equery operations. We obtain these
results via anoveladaptationofthe randomizedresponsetechnique as a bitflipping procedure,
applied to the sketched strings.
∗jhu@ensemblecore.ai;jhu@u.northwestern.edu. EnsembleAI,SanFrancisco,CA,USA;CenterforFoundation
Models and Generative AI & Department of Computer Science, Northwestern University,Evanston, IL,USA. Work
doneduring JH’s internship at Ensemble AI.
†erzhiliu@u.northwestern.edu. Center for Foundation Models and Generative AI & Department of Computer
Science, Northwestern University,Evanston, IL, USA.
‡hanliu@northwestern.edu. CenterforFoundationModelsandGenerativeAI&DepartmentofComputerScience
& Department of Statistics and Data Science, Northwestern University, Evanston, IL, USA. Supported in part by
NIHR01LM1372201, AbbVieand Dolby.
§magic.linuxkde@gmail.com. Simons Institutefor theTheory of Computing, UCBerkeley, Berkeley, CA, USA.
¶lichenz@mit.edu. DepartmentofMathematics&ComputerScienceandArtificialIntelligenceLaboratory,MIT,
Cambridge, MA, USA.Supportedin part by NSFCCF-1955217 and DMS-2022448.
4202
voN
8
]SD.sc[
1v05750.1142:viXraContents
1 Introduction 2
2 Related Work 4
3 Preliminary 4
3.1 Concentration Bounds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
3.2 Differential Privacy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
4 Differentially Private Hamming Distance Data Structure 5
4.1 Time Complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
4.2 Privacy Guarantee . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
4.3 Utility Guarantee . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
5 Differentially Private Edit Distance Data Structure 9
5.1 Time Complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
5.2 Privacy Guarantee . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
5.3 Utility Guarantee . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
6 Conclusion 12
A Proofs for Hamming Distance Data Structure 19
A.1 Proof of Lemma 4.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
A.2 Proof of Lemma 4.4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
A.3 Proof of Lemma 4.5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
A.4 Proof of Lemma 4.6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
B Differentiall Private Longest Common Prefix 20
B.1 Time Complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
B.2 Privacy Guarantee . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
B.3 Utility Guarantee . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
11 Introduction
Estimating string distances is one of the most fundamental problems in computer science and
information theory, with rich applications in high-dimensional geometry, computational biology
andmachine learning. Theproblemcould begenerically formulated as follows: given acollection of
stringsA ,...,A Σn whereΣisthealphabet,thegoalistodesignadatastructuretopreprocess
1 m
∈
these strings such that when a query B Σn is given, the data structure needs to quickly output
∈
estimatesof A B foralli [m],where isthedistanceofinterest. AssumingthesymbolsinΣ
i
k − k ∈ k·k
can allow constant timeaccess andoperations, ana¨ıve implementation wouldbeto simplycompute
all the distances between A ’s and B, which would require O(mn) time. Designing data structures
i
with o(mn) query time has been the driving research direction in string distance estimations. To
make the discussion concrete, in this work we will focus on binary alphabet (Σ = 0,1 ) and for
{ }
distance, we will study Hamming and edit distance. Hamming distance [Ham50] is one of the
most natural distance measurements for binary strings, with its deep root in error detecting and
correction for codes. It finds large array of applications in database similarity searches [IM98,
Cha02, NPF12] and clustering algorithms [Hua97, HN99].
Compared to Hamming distance, edit distance or the Levenshtein distance [Lev66] could be
viewed as a more robust distance measurement for strings: it counts the minimum number of
operations (including insertion, deletion and substitution) to transform from A to B. To see the
i
robustness compared to Hamming distance, consider A = (01)n and B = (10)n, the Hamming
i
distance between these two strings is n, but A could be easily transformed to B by deleting the
i
first bit and adding a 0 to the end, yielding an edit distance of 2. Due to its flexibility, edit
distance is particularly useful for sequence alignment in computational biology [WHZ+15, YFA21,
BWY21], measuring text similarity [Nav01, SGAM+15] and natural language processing, speech
recognition [FARL06, DA10] and time series analysis [Mar09, GS18].
Inadditiontodatastructureswithfastquerytimes,anotherimportantconsiderationistoensure
thedatabase is secure. Considerthescenario wherethedatabase consists of private medical data of
m patients, where each of the A is the characteristic vector of n different symptoms. A malicious
i
adversary might attempt to count the number of symptoms each patient has by querying 0 , or
n
detecting whether patient i has symptom j by querying e and 0 where e is the j-th standard
j n j
basis in Rn. It is hence crucial to curate a private scheme so that the adversary cannot distinguish
the case whether the patient has symptom j or not. This notion of privacy has been precisely
captured by differential privacy [Dwo06, DKM+06], which states that for neighboring databases1,
the output distribution of the data structure query should be close with high probability, hence
any adversary cannot distinguishable between the two cases.
Motivated by both privacy and efficiency concerns, we ask the following natural question:
Is it possible to design a data structures to estimate Hamming and edit distance, that are both
differentially private, and time/space-efficient?
We provide an affirmative answer to the above question, with the main results summarized in
the following two theorems. We will use D (A,B) to denote the Hamming distance between A
ham
and B, and D (A,B) to denote the edit distance between A and B. We also say a data structure
edit
is ǫ-DP if it provides ǫ-DP outputs against any sequence of queries, of arbitrary length.
Theorem 1.1. Let A ,...,A 0,1 n be a database, k [n] and ǫ > 0,β (0,1), then there
1 m
∈ { } ∈ ∈
exists a randomized algorithm with the following guarantees:
1In our case, we say two database D 1 and D 2 are neighboring if there exists one i ∈ [n] such that D 1(Ai) and
D 2(Ai) differs by one bit.
2• The data structure is ǫ-DP;
• It perprocesses A ,...,A in time O(mn) time2;
1 m
• It consumes O(mk) space; e
• Given any queery B 0,1 n such that max D (A ,B) k, it outputs m estimates
i∈[m] ham i
∈ { } ≤
z ,...,z with z D (A ,B) O(k/eǫ/logk) for all i [m] in time O(mk+n), and it
1 m i ham i
| − | ≤ ∈
succeeds with probability at least 1 β.
− e e
Theorem 1.2. Let A ,...,A 0,1 n be a database, k [n] and ǫ > 0,β (0,1), then there
1 m
∈ { } ∈ ∈
exists a randomized algorithm with the following guarantees:
• The data structure is ǫ-DP;
• It perprocesses A ,...,A in time O(mn) time;
1 m
• It consumes O(mn) space; e
• Given any queery B 0,1 n such that max D (A ,B) k, it outputs m estimates
i∈[m] edit i
∈ { } ≤
z ,...,z with z D (A ,B) O(k/eǫ/(logklogn)) for all i [m] in time O(mk2 +n),
1 m i edit i
| − | ≤ ∈
and it succeeds with probability at least 1 β.
e − e
Before diving into the details, we would like to make several remarks regarding our data
structure results. Note that instead of solving the exact Hamming distance and edit distance
problem, we impose the assumption that the query B has the property that for any i [m],
∈
A B k. Such an assumption might seem restrictive at its first glance, but under the
i
k − k ≤
standard complexity assumption Strong Exponential Time Hypothesis (SETH) [IP01, IPZ01], it is
knownthat thereis noO(n2−o(1))time algorithm exists for exact or even approximate editdistance
[BZ16,CGK16b,CGK16a,NSS17,RSSS19,RS20,GRS20,JNW21,BEG+21,KPS21,BK23,KS24].
It is therefore natural to impose assumptions that the query is “near” to the database in pursuit
of faster algorithms [Ukk85, Mye86, LV88, GKS19, KS20, GKKS23]. In fact, assuming SETH,
O(n+k2)runtimefor edit distancewhen m = 1is optimal up tosub-polynomialfactors [GKKS23].
Thus, in this paper, we consider the setting where max A B k for both Hamming and
i∈[m] i
k − k ≤
edit distance and show how to craft private and efficient mechanisms for this class of distance
problems.
Regardingprivacyguarantees, onemightconsiderthefollowingsimpleaugmentation toanyfast
data structure for Hamming distance: compute the distance estimate via the data structure, and
add Laplace noise to it. Since changing one coordinate of the database would lead to the Hamming
distance change by at most 1, Laplace mechanism would properly handle this case. However, our
goal is to release a differentially private data structure that is robust against potentially infinitely
many queries, and a simple output perturbation won’t be sufficient as an adversary could simply
query with the same B, average them to reduce the variance and obtain a relatively accurate
estimate of the de-noised output. To address this issue, we consider the differentially private
function release communication model [HRW13], where the curator releases an ǫ-DP description of
a function e() that is ǫ-DP without seeing any query in advance. The client can then use e() to
· ·
compute e(B) for any query B. This strong guarantee ensures that the client could feed infinitely
b b
many queries to e() without compromising the privacy of the database.
b ·
2Throughout thebpaper, we will use Oe (·) to suppress polylogarithmic factors in m,n,k and 1/β.
32 Related Work
Differential Privacy. Differential privacy is a ubiquitous notion for protecting the privacy
of database. [DKM+06] first introduced this concept, which characterizes a class of algorithms
such that when inputs are two neighboring datasets, with high probability the output distribu-
tions are similar. Differential privacy has a wide range of applications in general machine learn-
ing [CM08, WM10, JE19, TF20], training deep neural networks [ACG+16, BPS19], computer vi-
sion [ZYCW20, LWAL21, TKP19], natural language processing [YDW+21, WK18], large language
models [GSY23, YNB+22], label protect [YSY+22], multiple data release [WYY+22], federated
learning [SYY+23, SWYZ23] and peer review [DKWS22]. In recent years, differential privacy
has been playing an important role for data structure design, both in making these data structures
robustagainst adaptive adversary [BKM+22,HKM+22, SYYZ23, CSW+23] andin the function re-
lease communication model [HRW13, HR14, WJF+16, AR17, CS21, WNM23, BLM+24, LHR+24].
Hamming Distance and Edit Distance. Given bit strings A and B, many distance measure-
mentshavebeenproposedthatcapturevariouscharacteristics ofbitstrings. Hammingdistancewas
firststudiedbyHamming[Ham50]inthecontext of errorcorrection for codes. From analgorithmic
perspective, Hamming distance is mostly studied in the context of approximate nearest-neighbor
search and locality-sensitive hashing [IM98, Cha02]. When it is known that the query B has the
property D (A,B) k, [PL07] shows how to construct a sketch of size O(k) in O(n) time,
ham
≤
and with high probability, these sketches preserve Hamming distance. Edit distance, proposed by
e e
Levenshtein [Lev66], is a more robust notion of distance between bit strings. It has applications in
computational biology [WHZ+15, YFA21, BWY21], text similarity [Nav01, SGAM+15] and speech
recognition [FARL06, DA10]. From a computational perspective, itis known that undertheStrong
Exponential Time Hypothesis (SETH), no algorithm can solve edit distance in O(n2−o(1)) time,
even its approximate variants [BZ16, CGK16b, CGK16a, NSS17, RSSS19, RS20, GRS20, JNW21,
BEG+21, KPS21, BK23, KS24]. Hence, various assumptions have been imposed to enable more
efficient algorithm design. The most related assumption to us is that D (A,B) k, and in this
edit
≤
regime various algorithms have been proposed [Ukk85, Mye86, LV88, GKS19, KS20, GKKS23].
Under SETH, it has been shown that the optimal dependence on n and k is O(n + k2), up to
sub-polynomial factors [GKKS23].
3 Preliminary
Let E be an event, we use 1[E] to denote the indicator variable if E is true. Given two length-n
bit strings A and B, we use D (A,B) to denote n 1[A = B ]. We use D (A,B) to denote
ham i=1 i i edit
the edit distance between A and B, i.e., the minimPum number of operations to transform A to B
wherethe allowed operations areinsertion, deletion and substitution. We use to denote theXOR
⊕
operation. For any positive integer n, we use [n] to denote the set 1,2, ,n . We use Pr[], E[]
{ ··· } · ·
and Var[] to denote probability, expectation and variance respectively.
·
3.1 Concentration Bounds
We will mainly use two concentration inequalities in this paper.
Lemma 3.1 (Chebyshev’s Inequality). Let X be a random variable with 0 < Var[X] < . For
∞
any real number t > 0,
Var[X]
Pr[X E[X] > t] .
| − | ≤ t2
4Lemma 3.2 (Hoeffding’s Inequality). Let X ,...,X with a X b almost surely. Let S =
1 n i i i n
n X , then for any real number t > 0, ≤ ≤
i=1 i
P
2t2
Pr[S E[S ] > t] 2exp( ).
| n − n | ≤ − n (b a )2
i=1 i − i
P
3.2 Differential Privacy
Differential privacy (DP) is the key privacy measure we will be trying to craft our algorithm to
possess it. In this paper, we will solely focus on pure DP (ǫ-DP).
Definition 3.3 (ǫ-Differential Privacy). We say an algorithm is ǫ-differentially private (ǫ-DP)
A
if for any two neighboring databases and and any subsets of possible outputs S, we have
1 2
D D
Pr[ ( ) S] eǫ Pr[ ( ) S],
1 2
A D ∈ ≤ · A D ∈
where the probability is taken over the randomness of .
A
Sincewewillbedesigningdatastructures,wewillworkwiththefunctionrelease communication
model [HRW13] where the goal is to release a function that is ǫ-DP against any sequence of queries
of arbitrary length.
Definition 3.4(ǫ-DPDataStructure). Wesayadata structure isǫ-DP,if isǫ-DPagainstany
A A
sequence of queries of arbitrary length. In other words, the curator will release an ǫ-DP description
of a function e() without seeing any query in advance.
·
Finally, webwill be utilizing the post-processing property of ǫ-DP.
Lemma 3.5 (Post-Processing). Let be ǫ-DP, then for any deterministic or randomized function
A
g that only depends on the output of , g is also ǫ-DP.
A ◦A
4 Differentially Private Hamming Distance Data Structure
To start off, we introduce our data structure for differentially private Hamming distance. In par-
ticular, we will adapt a data structure due to [PL07]: this data structure computes a sketch of
length O(k) bit string to both the database and query, then with high probability, one could re-
trieve the Hamming distance from these sketches. Since the resulting sketch is also a bit string, a
e
natural idea is to inject Laplace noise on each coordinate of the sketch. Since for two neighboring
databases, only one coordinate would change, we could add Laplace noise of scale 1/ǫ to achieve
ǫ-DP. However, this approach has a critical issue: one could show that with high probability, the
magnitude of each noise is roughly O(ǫ−1logk), aggregating the k coordinates of the sketch, this
leads to a total error of O(ǫ−1klogk). To decrease this error to O(1), one would have to choose
ǫ = klogk, which is too large for most applications.
InsteadofLaplacenoises, wepresentanovelschemethatflipseach bitofthesketch withcertain
probability. Our main contribution is to show that this simple scheme, while produces a biased
estimator, the error is only O(e−ǫ/logkk). Let t = logk/ǫ, we see that the Laplace mechanism has
an error of O(t−1k) and our error is only O(e−tk), which is exponentially small! In what follows,
we will describe a data structure when the database is only one string A and with constant success
probability, and we will discuss how to extend it to m bit strings, and how to boost the success
probability to 1 β for any β > 0. We summarize the main result below.
−
5Theorem 4.1. Given a string A of length n. There exists an ǫ-DP data structure DPHam-
mingDistance (Algorithm 1), with the following operations
• Init(A 0,1 n): It takes a string A as input. This procedure takes O(nlogk +klog3k)
∈ { }
time.
• Query(B 0,1 n): for any B with z := D (A,B) k, Query(B) outputs a value z
ham
∈ { } ≤
such that z z = O(k/eǫ/logk) with probability 0.99, and the result is ǫ-DP. This procedure
takes O(n| log− k+| klog3k) time. e
e e
Algorithm 1 Differential Private Hamming Distance Query
1: data structure DPHammingDistance ⊲ Theorem 4.1
2: members
3: M 1,M 2,M 3 N +
∈
4: h(x) :[2n] [M 2] ⊲ h and g are public random hash function
→
5: g(x,i) :[2n] [M 1] [M 3]
× →
6: S i,j,c 0,1 M1×M2×M3 for all i [M 1],j [M 2],c [M 3] ⊲ S represents the sketch
∈ { } ∈ ∈ ∈
7: end members
8:
9: procedure Encode(A 0,1 n,n) ⊲ Lemma 4.2
∈{ }
10: S∗ 0 for all i,j,c
i,j,c ←
11: for p [n] do
∈
12: for i [M 1] do
∈
13: j h(2(p 1)+A p)
← −
14: c g(2(p 1)+A p,i)
← −
15: S∗ S∗ 1
i,j,c ← i,j,c⊕
16: end for
17: end for
18: return S∗
19: end procedure
20:
21: procedure Init(A 0,1 n,n N +,k N +,ǫ′ R +) ⊲ Lemma 4.3
∈ { } ∈ ∈ ∈
22: M 1 10logk
←
23: M 2 2k
←
24: M 3 400log2k
←
25: S Encode(A,n)
26:
Fli←
p each S i,j,c with independent probability
1/(1+eǫ′/(2M1))
27: end procedure
28:
29: procedure Query(B 0,1 n) ⊲ Lemma 4.7
∈ { }
30: SB Encode(B,n)
31: retu← rn 0.5
·
M j=2 1max i∈[M1]( M c=3 1|S i,j,c −S iB ,j,c|)
32: end procedureP P
33: end data structure
To achieve the results above, we set parameters M = O(logk),M = O(k),M = O(log2k) in
1 2 3
Algorithm 1.
We divide the proof of Theorem 4.1 into the following subsections:
64.1 Time Complexity
Note that both the initializing and query run Encode (Algorithm 1) exactly once, we show that
the running time of Encode is O(nlogk).
Lemma 4.2. Given M = O(logk), the running time of Encode (Algorithm 1) is O(nlogk).
1
Proof. InEncode,foreachcharacterintheinputstring,thealgorithmiterateM times. Therefore
1
the total time complexity is O(n M ) = O(nlogk).
1
·
4.2 Privacy Guarantee
Next we prove our data structure is ǫ-DP.
Lemma 4.3. Let A and A′ be two strings that differ on only one position. Let (A) and (A′) be
A A
the output of Init (Algorithm 1) given A and A′. For any output S, we have:
Pr[ (A) = S] eǫ Pr[ (A′)= S].
A ≤ · A
We defer the proof to Appendix A.
4.3 Utility Guarantee
The utility analysis is much more involved than privacy and runtime analysis. We defer the proofs
to the appendix, while stating key lemmas.
Wefirstconsiderthedistancebetweensketches ofAandB withouttherandomflippingprocess.
Let E(A),E(B) be Encode(A) and Encode(B). We prove with probability 0.99, D (A,B) =
ham
0.5 M2 max ( M3 E(A) E(B) ). Before we present the error guarantee, we will
· j=1 i∈[M1] c=1| i,j,c − i,j,c |
first Pintroduce two tePchnical lemmas. If we let T = p [n] A
p
= B
p
denote the set of “bad”
{ ⊆ | 6 }
coordinates, then for each coordinate in the sketch, it only contains a few bad coordinates.
Lemma 4.4. Define set T := p [n] A = B . Define set T := p T h(p) = j . When
p p j
{ ∈ | 6 } { ⊆ | }
M = 2k, with probability 0.99, for all j [M ], we have T 10logk, i.e.,
2 2 j
∈ | |≤
Pr[ j [M ], T 10logk] 0.99.
2 j
∀ ∈ | | | ≤ ≥
The next lemma shows that with high probability, the second level hashing g will hash bad
coordinates to distinct buckets.
Lemma4.5. WhenM = 10logk,M = 2k,M = 400log2k, withprobability 0.98, forallj [M ],
1 2 3 2
∈
there is at least one i [M ], such that all values in g(2(p 1)+A ,i) p T g(2(p 1)+
1 p j
∈ { − | ∈ } { −
B p,i) p T j are distinct. S
| ∈ }
With these two lemmas in hand, we are in the position to prove the error bound before the
random bit flipping process.
Lemma 4.6. Let E(A),E(B) be the output of Encode(A) and Encode(B). With probability
0.98, D (A,B) = 0.5 M2 max ( M3 E(A) E(B) ).
ham · j=1 i∈[M1] c=1| i,j,c − i,j,c |
P P
Our final result provides utility guarantees for Algorithm 1.
Lemma 4.7. Let z be Dham(A,B), z be the output of Query(B)(Algorithm 1). With probability
0.98, z z O(klog3k/eǫ/logk).
| − |≤ e
e
7Proof. From Lemma 4.6, we know with probability 0.98, when ǫ (i.e. without the random
→ ∞
flip process), the output of Query(B) (Algorithm 1) equals the exact hamming distance.
We view the random flip process as random variables. Let random variables R be 1 with
i,j,c
probability 1/(1+eǫ/M1), or 0 with probability 1 1/(1+eǫ/M1). So we have
−
M2 M3
z z = max( R )
i,j,c
| − | Xj=1i∈[M1] Xc=1
e
M2 M1 M3
( R ),
i,j,c
≤
Xj=1Xi=1 Xc=1
where the second step follows from max when all the summands are non-negative.
i ≤ i
Therefore, the expectation of z z is: P
−
E[ez z ] = M M M E[R ]
1 2 3 i,j,c
| − | ·
1
e = klog3k
· (1+eǫ/logk)
klog3k
O( ),
≤ eǫ/logk
where the last step follows from simple algebra. The variance of z z is:
−
Var[z z ] = M M M Var[R ] e
1 2 3 i,j,c
| − | ·
1 1
e = klog3k (1 ).
· (1+eǫ/logk) · − (1+eǫ/logk)
Using Chebyshev’s inequality (Lemma 3.1), we have
klog3k
Pr[z z O( )] 0.01.
| − |≥ eǫ/logk ≤
e
Thus we complete the proof.
Remark 4.8. We will show how to generalize Theorem 4.1 to m bit strings, and how to boost the
success probability to 1 β. To boost the success probability, we note that individual data structure
−
succeeds with probability 0.99, we could take log(1/β) independent copies of the data structure, and
query all of them. By a standard Chernoff bound argument, with probability at least 1 β, at least
−
3/4 fraction of these data structures would output the correct answer, hence what we could do is
to take the median of these answers. These operations blow up both Init and Query by a factor
of log(1/β) in its runtime. Generalizing for a database of m strings is relatively straightforward:
we will run the Init procedure to A ,...,A , this would take O(mnlogk +mklog3k) time. For
1 m
each query, note we only need to Encode the query once, and we can subsequently compute the
Hamming distance from the sketch for m sketched database strings, therefore the total time for
query is O(nlogk + mklog3k). It is important to note that as long as klog3k < n, the query
time is sublinear. Finally, we could use the success probability boosting technique described before,
that uses log(m/β) data structures to account for a union bound over the success of all distance
estimates.
85 Differentially Private Edit Distance Data Structure
Our algorithm for edit distance follows from the dynamic programming method introduced by
[Ukk85, LMS98, LV88, Mye86]. We note that a key procedure in these algorithms is a subroutine
to estimate longest common prefix (LCP) between two strings A and B and their substrings. We
designanǫ-DPdatastructureforLCPbasedonourǫ-DPHammingdistancedatastructure. Dueto
spacelimitation, wedeferthedetails oftheDP-LCPdatastructuretoAppendixB.Inthefollowing
discussion, we will assume access to a DP-LCP data structure with the following guarantees:
Theorem 5.1. Given a string A of length n. There exists an ǫ-DP data structure DPLCP (Al-
gorithm 3 and Algorithm 4) supporting the following operations
• Init(A 0,1 n): It preprocesses an input string A. This procedure takes O(n(logk +
∈ { }
loglogn)) time.
• InitQuery(B 0,1 n): It preprocesses an input query string B. This procedure take
∈ { }
O(n(logk+loglogn)) time.
• Query(i,j): Letw bethe longestcommon prefix of A[i : n]and B[j : n]and w be the output of
Query(i,j), Withprobability 1 1/(300k2), we have: 1). w w; 2). E[D (A[i : i+w],B[j :
ham
j+w])] O((logk+loglogn)/− eǫ/(logklogn)). This procedure≥ takes O(log2n(e logk+loglogn))
≤ e e
time.
e
We will be basing our edit distance data structure on the following result, which achieves the
optimal dependence on n and k assuming SETH:
Lemma 5.2 ([LMS98]). Given two strings A and B of length n. If the edit distance between A
and B is no more than k, there is an algorithm which computes the edit distance between A and B
in time O(k2+n).
We start from a na¨ıve dynamic programming approach. Define D(i,j) to be the edit distance
between string A[1 : i] and B[1: j]. We could try to match A[i] and B[j] by inserting, deleting and
substituting, which yields the following recurrence:
D(i 1,j)+1 if i> 0;
−
D(i,j) = min D(i 1,j 1)+1 if j > 0;
 D(i− 1,j − 1)+1[A[i] = B[j]] if i,j > 0.
− − 6

The edit distance between A and B is then captured by D(n,n). When k < n, for all D(i,j)
such that i j > k, because the length difference between A[1 : i] and B[1 : j] is greater than
| − |
k, D(i,j) > k. Since the final answer D(n,n) k, those positions with i j > k won’t affect
≤ | − |
D(n,n). Therefore, we only need to consider the set D(i,j) : i j k .
{ | − | ≤ }
For d [ k,k],r [0,k], we define F(r,d) = max i : D(i,i + d) = r and let LCP(i,j)
i
∈ − ∈ { }
denote the length of the longest common prefix of A[i : n] and B[j : n]. The algorithm of [LMS98]
defines Extend(r,d) := F(r,d)+LCP(F(r,d),F(r,d)+d). We have
Extend(r 1,d)+1 if r 1 0;
− − ≥
F(r,d) = max Extend(r 1,d 1) if d 1 k,r 1 0;
 Extend(r− 1,d+−
1)+1 if
d+− 1,≥ r+−
1
−
k.
≥
− ≤

The edit distance between A and B equals min r : F(r,0) = n .
r
{ }
To implement LCP, [LMS98] uses a suffix tree data structure with initialization time O(n) and
query time O(1), thus the total time complexity is O(k2 +n). In place of their suffix tree data
structure, we use our DP-LCP data structure (Theorem 5.1). This leads to Algorithm 2.
9Theorem 5.3. Given a string A of length n. There exists an ǫ-DP data structure DPEditDis-
tance (Algorithm 2) supporting the following operations:
• Init(A 0,1 n): It preprocesses an input string A. This procedure takes O(n(logk +
∈ { }
loglogn)) time.
• Query(B 0,1 n): For any query string B with w := D (A,B) k, Query outputs
edit
∈ { } ≤
a value w such that w w O(k/eǫ/(logklogn)) with probability 0.99. This procedure takes
| − | ≤
O(n(logk+loglogn)+k2log2n(logk+loglogn)) = O(k2+n) time.
e e e
e
Algorithm 2 Differential Private Edit Distance
1: data structure DPEditDistance ⊲ Theorem 5.3
2: procedure Init(A 0,1 n,n N +,k N +,ǫ R) ⊲ Lemma 5.4
∈ { } ∈ ∈ ∈
3: DPLCP.Init(A,n,k,ǫ) ⊲ Algorithm 3
4: end procedure
5:
6: procedure Extend(F,i,j)
7: return F(i,j)+DPLCP.Query(F(i,j),F(i,j)+j) ⊲ Algorithm 4
8: end procedure
9:
10: procedure Query(B,n,k) ⊲ Lemma 5.5 and 5.8
11: DPLCP.QueryInit(B,n,k) ⊲ Algorithm 3
12: F 0,0 0
←
13: for i from 1 to k do
14: for j [ k,k] do
∈ −
15: F i,j max(F i,j,Extend(i 1,j)) ⊲ Algorithm 4
← −
16: if j 1 k then
− ≥ −
17: F i,j max(F i,j,Extend(i 1,j 1)) ⊲ Algorithm 4
← − −
18: end if
19: if j +1 k then
≤
20: F i,j max(F i,j,Extend(i 1,j +1)) ⊲ Algorithm 4
← −
21: end if
22: end for
23: if F i,0 = n then
24: return i
25: end if
26: end for
27: end procedure
28: end data structure
Again, we divide the proof into runtime, privacy and utility.
5.1 Time Complexity
We prove the time complexity of Init and Query respectively.
Lemma 5.4. The running time of Init (Algorithm 2) is O(n(logk+loglogn)).
Proof. TheInit runsDPLCP.Init. From Theorem5.1, theinittime is O(n(logk+loglogn)).
10Lemma 5.5. Query (Algorithm 2) runs in time O((n+k2logn)(logk+loglogn)).
Proof. The Query runs DPLCP.QueryInit once and DPLCP.Query k2 times. From Theo-
rem 5.1, the query time is O(n(logk+loglogn)+k2log2n(logk+loglogn)).
5.2 Privacy Guarantee
Lemma 5.6. The data structure DPEditDistance (Algorithm 2) is ǫ-DP.
Proof. The data structure only stores a DPLCP(Algorithm 3, 4). From Theorem 5.1 and the
post-processing property (Lemma 3.5), it is ǫ-DP.
5.3 Utility Guarantee
Before analyzing the error of the output of Query (Algorithm 2), we first introduce a lemma:
Lemma 5.7. Let A,B be two strings. Let LCP(i,d) be the length of the true longest common prefix
of A[i :n] and B[i+d : n]. For i i ,d [ k,k], we have i +LCP(i ,d) i +LCP(i ,d).
1 2 1 1 2 2
≤ ∈ − ≤
Proof. Let w = LCP(i ,d),w = LCP(i ,d). Then for j [i ,i +w 1], A[j] = B[j +d]. On
1 1 2 2 1 1 1
∈ −
the other side, w is the length of the longest common prefix for A[i : n] and B[i +d : n]. So
2 2 2
A[i +w ] = B[i +w +d]. Therefore, (i +w ) / [i ,i +w 1]. Since i +w i i , we
2 2 2 2 2 2 1 1 1 2 2 2 1
6 ∈ − ≥ ≥
have i +w i +w .
2 2 1 1
≥
Lemma 5.8. Let r be the output of Query (Algorithm 2), r be the true edit distance D (A,B).
edit
With probability 0.99, we have r r O(k(logk+loglogn)/(1+eǫ/(logklogn))).
e | − | ≤
Proof. We divide the proof into twoeparts. In part one, we prove that with probability 0.99, r r.
≤
In part two, we prove that with probability 0.99, r r O(k(logk+loglogn)/(1+eǫ/(logklogn))).
In Theorem 5.1, with probability 1 1/(300k2), D≥ PL− CP.Query satisfies two conditions.e Our
following discussion supposes all DPL− CP.Querye satisfies the two conditions. There are 3k2 LCP
queries, by union bound, the probability is at least 0.99.
Part I. Suppose without differential privacy guarantee(using original LCP function instead of
ourDPLCPdatastructure),thedynamicprogrammingmethodoutputsthetrueeditdistance. We
define F′ as the dynamic programming array F without privacy guarantee, Extend’(i,j) be the
i,j
result of Extend(i,j) without privacy guarantee. Then we prove that for all i [0,k],j [ k,k],
∈ ∈ −
F F′ holds true.
i,j ≥ i,j
We prove the statement above by math induction on i. For i = 0, F(0,0) = F′(0,0) = 0.
Suppose for i 1, F(i 1,j) F′(i 1,j), then for i,
− − ≥ −
Extend(i 1,j)+1 if i 1 0;
− − ≥
F(i,j) = max Extend(i 1,j 1) if j 1 k,i 1 0;
 Extend(i−
1,j
−
+1)+1 if j
− +1,≥ i+−
1
− k,i≥
1 0.
− ≤ − ≥

For Extend(i 1,j), we have
−
Extend(i 1,j) =F(i 1,j)+DPLCP.Query(F(i 1,j),F(i 1,j)+j)
− − − −
F(i 1,j)+LCP(F(i 1,j),F(i 1,j)+j)
≥ − − −
F′(i 1,j)+LCP(F′(i 1,j),F′(i 1,j)+j)
≥ − − −
= Extend’(i 1,j)
−
11The second step is because in Query (Theorem 5.1), w w. The third step follows from
≥
F(i 1,j) F′(i 1,j) and Lemma 5.7. Thus, F(i,j) = max Extend(i,j )
max− ≥ Ex− tend’(i,j ) = F′(i,j). Since r = mine r
:F(r,j 02∈ )[ =j,j− n1,j ,+ r1 =]
{ min r :
F′(r2
,0} ) ≥ =
j2∈[j,j−1,j+1]
{
2
} { } {
n , we have F(r,0) F′(r,0) = n. Therefore r r.
} ≥ ≤ e e e
e
Part II. Let G(L,R,j) := D (A[L : R],B[L+j,R+j]). In this part, we prove that the edit
edit
distance G(1,F ,j) i (1+O((logk+loglogn)/(1+eǫ/(logklogn)))) by induction on i.
i,j
≤ ·
For i= 0, F = 0. The statement holds true. Suppose for i 1, G(1,F ,j) (i 1) (1+
0,0 i−1,j
− ≤ − ·
O((logk+loglogn)/(1+eǫ/(logklogn)))), then we prove this holds for i.
Because F(i,j) = max Extend(i,j ) , there is some j j,j 1,j+1 such that
j2∈[j,j−1,j+1]
{
2
}
2
∈ { − }
F = F +DPLCP.Query(F ,F +j ). Let Q := DPLCP.Query(F ,F +
i,j i−1,j2 i−1,j2 i−1,j2 2 i−1,j2 i−1,j2
j ). Therefore
2
G(1,F ,j) G(1,F +Q,j )+1
i,j
≤
i−1,j2 2
G(1,F ,j )+G(F ,F +Q,j )+1
≤
i−1,j2 2 i−1,j2 i−1,j2 2
G(1,F ,j )+1+O((logk+loglogn)/(1+eǫ/(logklogn)))
≤
i−1,j2 2
i (1+O((logk+loglogn)/(1+eǫ/(logklogn))))
≤ ·
Thethirdstepfollows fromTheorem5.1, andthefourthstep follows fromtheinductionhypothesis.
Therefore, r = G(1,F re,0,0) r (1 + O((logk + loglogn)/(1 + eǫ/(logklogn)))) and the proof is
≤ ·
completed.
e
Remark 5.9. To the best of our knowledge, this is the first edit distance algorithm, based on
noisy LCP implementations. In particular, we prove a structural result: if the LCP has query
(additive) error δ, then we could implement an edit distance data structure with (additive) error
O(kδ). Compared to standard relative error approximation, additive error approximation for edit
distance is relatively less explored (see e.g., [BCFN22] for using additive approximation to solve
gap edit distance problem). We hope this structural result sheds light on additive error edit distance
algorithms.
6 Conclusion
We study the differentially private Hamming distance and edit distance data structure problem
in the function release communication model. This type of data structures are ǫ-DP against any
sequence of queries of arbitrary length. For Hamming distance, our data structure has query time
O(mk+n)anderrorO(k/eǫ/logk). For editdistance, ourdatastructurehasquerytimeO(mk2+n)
and error O(k/eǫ/(logklogn)). While the runtime of our data structures (especially edit distance) is
e e e
nearly-optimal, it is interesting to design data structures with better utility in this model.
e
12References
[ACG+16] MartinAbadi,AndyChu,IanGoodfellow,HBrendanMcMahan,IlyaMironov, Kunal
Talwar, and Li Zhang. Deep learning with differential privacy. In Proceedings of the
2016 ACM SIGSAC conference on computer and communications security, pages308–
318, 2016.
[AR17] Francesco Ald`a and Benjamin I.P. Rubinstein. The bernstein mechanism: function
release under differential privacy. In Proceedings of the Thirty-First AAAI Conference
on Artificial Intelligence, AAAI’17, page 1705–1711. AAAI Press, 2017.
[BCFN22] Karl Bringmann, Alejandro Cassis, Nick Fischer, and Vasileios Nakos. Improved
Sublinear-Time Edit Distance for Preprocessed Strings. In Miko laj Bojan´czyk,
Emanuela Merelli, and David P. Woodruff, editors, 49th International Colloquium on
Automata, Languages, and Programming (ICALP 2022), Leibniz International Pro-
ceedings in Informatics (LIPIcs), pages 32:1–32:20, Dagstuhl, Germany, 2022. Schloss
Dagstuhl – Leibniz-Zentrum fu¨r Informatik.
[BEG+21] Mahdi Boroujeni, Soheil Ehsani, Mohammad Ghodsi, MohammadTaghi HajiAghayi,
and Saeed Seddighin. Approximating edit distance in truly subquadratictime: Quan-
tum and mapreduce. Journal of the ACM (JACM), 68(3):1–41, 2021.
[BK23] SudattaBhattacharyaandMichalKoucky`. Locallyconsistentdecompositionofstrings
with applications to edit distance sketching. In Proceedings of the 55th Annual ACM
Symposium on Theory of Computing, pages 219–232, 2023.
[BKM+22] Amos Beimel, Haim Kaplan, Yishay Mansour, Kobbi Nissim, Thatchaphol Saranu-
rak, and Uri Stemmer. Dynamic algorithms against an adaptive adversary: Generic
constructions and lower bounds. In Proceedings of the 54th Annual ACM SIGACT
Symposium on Theory of Computing, pages 1671–1684, 2022.
[BLM+24] ArtursBackurs, ZinanLin, SepidehMahabadi, SandeepSilwal, andJakubTarnawski.
Efficiently computing similarities to private datasets. In The Twelfth International
Conference on Learning Representations, 2024.
[BPS19] Eugene Bagdasaryan, Omid Poursaeed, and Vitaly Shmatikov. Differential privacy
has disparate impact on model accuracy. Advances in Neural Information Processing
Systems (NeurIPS), 32:15479–15488, 2019.
[BWY21] Bonnie Berger, Michael S. Waterman, and Yun William Yu. Levenshtein distance, se-
quencecomparisonandbiologicaldatabasesearch. IEEE Transactions on Information
Theory, 2021.
[BZ16] Djamal Belazzougui and Qin Zhang. Edit distance: Sketching, streaming, and docu-
ment exchange. In 2016 IEEE 57th Annual Symposium on Foundations of Computer
Science (FOCS), pages 51–60. IEEE, 2016.
[CGK16a] Diptarka Chakraborty, Elazar Goldenberg, and Michal Koucky`. Streaming algo-
rithms for computing edit distance without exploiting suffix trees. arXiv preprint
arXiv:1607.03718, 2016.
13[CGK16b] Diptarka Chakraborty, Elazar Goldenberg, and Michal Koucky`. Streamingalgorithms
for embeddingand computing edit distance in the low distance regime. In Proceedings
of the forty-eighth annual ACM symposium on Theory of Computing, pages 712–725,
2016.
[Cha02] Moses Charikar. Similarity estimation techniques from rounding algorithms. In Pro-
ceedings of the thiry-fourth annual ACM symposium on Theory of computing, pages
380–388, 2002.
[CM08] Kamalika Chaudhuriand Claire Monteleoni. Privacy-preserving logistic regression. In
NIPS, volume 8, pages 289–296. Citeseer, 2008.
[CS21] Benjamin Coleman and Anshumali Shrivastava. A one-pass distributed and private
sketch for kernel sums with applications to machine learning at scale. In Proceedings
of the 2021 ACM SIGSAC Conference on Computer and Communications Security,
CCS ’21, page 3252–3265, New York, NY, USA, 2021. Association for Computing
Machinery.
[CSW+23] Yeshwanth Cherapanamjeri, Sandeep Silwal, David Woodruff, Fred Zhang, Qiuyi
Zhang, and Samson Zhou. Robust algorithms on adaptive inputs from bounded ad-
versaries. In The Eleventh International Conference on Learning Representations,
2023.
[DA10] Jasha Droppo and Alex Acero. Context dependent phonetic string edit distance for
automatic speech recognition. In 2010 IEEE International Conference on Acoustics,
Speech and Signal Processing, 2010.
[DKM+06] Cynthia Dwork, Krishnaram Kenthapadi, Frank McSherry, Ilya Mironov, and Moni
Naor. Our data, ourselves: Privacy via distributed noise generation. In Annual
International ConferenceontheTheory andApplications ofCryptographic Techniques,
pages 486–503. Springer, 2006.
[DKWS22] Wenxin Ding, Gautam Kamath, Weina Wang, and Nihar B. Shah. Calibration with
privacyinpeerreview. In2022 IEEEInternational Symposium onInformation Theory
(ISIT), 2022.
[Dwo06] Cynthia Dwork. Differential privacy. In International Colloquium on Automata, Lan-
guages, and Programming (ICALP), pages 1–12, 2006.
[FARL06] JonathanG.Fiscus,JeromeAjot,NicolasRadde,andChristopheLaprun.Multipledi-
mension Levenshtein edit distance calculations for evaluating automatic speech recog-
nition systems during simultaneous speech. In Nicoletta Calzolari, Khalid Choukri,
Aldo Gangemi, Bente Maegaard, Joseph Mariani, Jan Odijk, and Daniel Tapias, ed-
itors, Proceedings of the Fifth International Conference on Language Resources and
Evaluation (LREC’06),Genoa,Italy, 2006.EuropeanLanguageResourcesAssociation
(ELRA).
[GKKS23] Elazar Goldenberg, Tomasz Kociumaka, Robert Krauthgamer, and Barna Saha. An
Algorithmic Bridge Between Hamming and Levenshtein Distances. In Yael Tau-
man Kalai, editor, 14th Innovations in Theoretical Computer Science Conference
(ITCS 2023), Leibniz International Proceedings in Informatics (LIPIcs), pages 58:1–
58:23, Dagstuhl, Germany, 2023. Schloss Dagstuhl – Leibniz-Zentrum fu¨r Informatik.
14[GKS19] Elazar Goldenberg, Robert Krauthgamer, and Barna Saha. Sublinear algorithms for
gapeditdistance. In2019 IEEE60th AnnualSymposium on Foundations of Computer
Science (FOCS), 2019.
[GRS20] ElazarGoldenberg,AviadRubinstein,andBarnaSaha.Doespreprocessinghelpinfast
sequence comparisons? In Proceedings of the 52nd Annual ACM SIGACT Symposium
on Theory of Computing (STOC), pages 657–670, 2020.
[GS18] Omer Gold and Micha Sharir. Dynamic time warping and geometric edit distance:
Breaking the quadratic barrier. ACM Trans. Algorithms, 2018.
[GSY23] Yeqi Gao, Zhao Song, and Xin Yang. Differentially private attention computation.
arXiv preprint arXiv:2305.04701, 2023.
[Ham50] Richard W Hamming. Error detecting and error correcting codes. The Bell System
Technical Journal, 29(2):147–160, 1950.
[HKM+22] Avinatan Hassidim, Haim Kaplan, Yishay Mansour, Yossi Matias, and Uri Stemmer.
Adversarially robust streaming algorithms via differential privacy. J. ACM, 69(6),
2022.
[HN99] Zhexue Huang and Mingkui Ng. A fuzzy k-modes algorithm for clustering categorical
data. IEEE Transactions on Fuzzy Systems, 7(4):446–452, 1999.
[HR14] Zhiyi Huang and Aaron Roth. Exploiting metric structure for efficient private query
release. In Proceedings of the Twenty-Fifth Annual ACM-SIAM Symposium on Dis-
crete Algorithms, SODA ’14, page 523–534, 2014.
[HRW13] RobHall,AlessandroRinaldo,andLarryWasserman. Differentialprivacyforfunctions
and functional data. J. Mach. Learn. Res., 2013.
[Hua97] Zhexue Huang. Extensions to the k-means algorithm for clustering large data sets
with categorical values. Data Mining and Knowledge Discovery, 2(3):283–304, 1997.
[IM98] Piotr Indyk and Rajeev Motwani. Approximate nearest neighbors: Towards removing
the curse of dimensionality. In Proceedings of the Thirtieth Annual ACM Symposium
on Theory of Computing, pages 604–613, 1998.
[IP01] Russell Impagliazzo and Ramamohan Paturi. On the complexity of k-sat. Journal of
Computer and System Sciences, 62(2):367–375, 2001.
[IPZ01] Russell Impagliazzo, Ramamohan Paturi, and Francis Zane. Which problems have
strongly exponential complexity? Journal of Computer and System Sciences,
63(4):512–530, 2001.
[JE19] Bargav Jayaraman and David Evans. Evaluating differentially private machine learn-
ing in practice. In 28th USENIX Security Symposium (USENIX Security 19), pages
1895–1912, 2019.
[JNW21] CeJin,JelaniNelson,andKewenWu. AnImprovedSketchingAlgorithmforEditDis-
tance. In 38th International Symposium on Theoretical Aspects of Computer Science
(STACS), pages 45:1–45:16, 2021.
15[KPS21] TomaszKociumaka,ElyPorat,andTatianaStarikovskaya. Small-spaceandstreaming
patternmatchingwithkedits. In2021 IEEE62nd AnnualSymposium on Foundations
of Computer Science (FOCS), pages 885–896. IEEE, 2021.
[KS20] Tomasz Kociumaka and Barna Saha. Sublinear-time algorithms for computing &
embedding gap edit distance. In 2020 IEEE 61st Annual Symposium on Foundations
of Computer Science (FOCS), 2020.
[KS24] Michal Koucky` and Michael E Saks. Almost linear size edit distance sketch. In
Proceedings of the 56th Annual ACM Symposium on Theory of Computing, pages
956–967, 2024.
[Lev66] Vladimir I Levenshtein. Binary codes capable of correcting deletions, insertions and
reversals. Soviet Physics Doklady, 10:707–710, 1966.
[LHR+24] Erzhi Liu, Jerry Yao-Chieh Hu, Alex Reneau, Zhao Song, and Han Liu. Differentially
private kernel density estimation. arXiv preprint arXiv:2409.01688, 2024.
[LMS98] Gad M. Landau, Eugene Wimberly Myers, and Jeanette P. Schmidt. Incremental
string comparison. SIAM J. Comput., 27:557–582, 1998.
[LV88] Gad M. Landau and Uzi Vishkin. Fast string matching with k differences. Journal of
Computer and System Sciences, 37, 1988.
[LWAL21] ZelunLuo,DanielJWu,EhsanAdeli,andFei-Fei Li. Scalabledifferentialprivacywith
sparse network finetuning. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR), pages 5059–5068, 2021.
[Mar09] Pierre-Franc¸ois Marteau. Time warp edit distance with stiffness adjustment for time
series matching. IEEE Transactions on Pattern Analysis and Machine Intelligence,
2009.
[Mye86] Eugene W. Myers. An O(ND) difference algorithm and its variations. Algorithmica,
1986.
[Nav01] Gonzalo Navarro. A guided tour to approximate string matching. ACM Comput.
Surv., 2001.
[NPF12] Mohammad Norouzi, Ali Punjani, and David J Fleet. Fast search in hamming space
withmulti-indexhashing. InProceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pages 3108–3115. IEEE, 2012.
[NSS17] Timothy Naumovitz, Michael Saks, and C Seshadhri. Accurate and nearly optimal
sublinear approximations to ulam distance. In Proceedings of the Twenty-Eighth An-
nual ACM-SIAM Symposium on Discrete Algorithms, pages 2012–2031. SIAM, 2017.
[PL07] Ely Porat and Ohad Lipsky. Improved sketching of hamming distance with error
correcting. In Combinatorial Pattern Matching, pages 173–182, Berlin, Heidelberg,
2007. Springer Berlin Heidelberg.
[RS20] AviadRubinsteinandZhaoSong. Reducingapproximatelongestcommonsubsequence
to approximate edit distance. In Proceedings of the Fourteenth Annual ACM-SIAM
Symposium on Discrete Algorithms, pages 1591–1600. SIAM, 2020.
16[RSSS19] Aviad Rubinstein, Saeed Seddighin, Zhao Song, and Xiaorui Sun. Approximation
algorithms for lcs and lis with truly improved running times. FOCS, 2019.
[SGAM+15] Grigori Sidorov, Helena Go´mez-Adorno, Ilia Markov, David Pinto, and Nahun Loya.
Computing text similarity using tree edit distance. In 2015 Annual Conference of the
North American Fuzzy Information Processing Society (NAFIPS) held jointly with
2015 5th World Conference on Soft Computing (WConSC), 2015.
[SWYZ23] Zhao Song, Yitan Wang, Zheng Yu, and Lichen Zhang. Sketching for first order
method: efficient algorithm for low-bandwidth channel and vulnerability. In Interna-
tional Conference on Machine Learning (ICML), pages 32365–32417. PMLR, 2023.
[SYY+23] JiankaiSun,XinYang,YuanshunYao,JunyuanXie,DiWu,andChongWang. Dpauc:
differentially private auc computation in federated learning. In Proceedings of the
Thirty-Seventh AAAI Conference on Artificial Intelligence and Thirty-Fifth Confer-
ence on Innovative Applications of Artificial Intelligence and Thirteenth Symposium
on Educational Advances in ArtificialIntelligence,AAAI’23/IAAI’23/EAAI’23. AAAI
Press, 2023.
[SYYZ23] ZhaoSong,XinYang,YuanyuanYang,andLichenZhang. Sketchingmeetsdifferential
privacy: fast algorithm for dynamic kronecker projection maintenance. In Interna-
tional Conference on Machine Learning (ICML), pages 32418–32462. PMLR, 2023.
[TF20] Aleksei Triastcyn and Boi Faltings. Bayesian differential privacy for machine learning.
In International Conference on Machine Learning, pages 9583–9592. PMLR, 2020.
[TKP19] Reihaneh Torkzadehmahani, Peter Kairouz, and Benedict Paten. Dp-cgan: Differen-
tially private synthetic data and label generation. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition Workshops (CVPR Work-
shop), 2019.
[Ukk85] Esko Ukkonen. Finding approximate patterns in strings. Journal of Algorithms,
6(1):132–137, 1985.
[WHZ+15] XiaoShaunWang,YanHuang,YonganZhao,HaixuTang,XiaoFengWang,andDiyue
Bu. Efficient genome-wide, privacy-preserving similar patient query based on private
edit distance. In Proceedings of the 22nd ACM SIGSAC Conference on Computer
and Communications Security, CCS ’15, page 492–503, New York, NY, USA, 2015.
Association for Computing Machinery.
[WJF+16] ZitengWang,ChiJin,KaiFan,JiaqiZhang,JunliangHuang,YiqiaoZhong,andLiwei
Wang. Differentially private data releasing for smooth queries. Journal of Machine
Learning Research, 2016.
[WK18] Benjamin Weggenmann and Florian Kerschbaum. Syntf: Synthetic and differentially
privatetermfrequencyvectorsforprivacy-preservingtextmining.InThe41stInterna-
tional ACM SIGIR Conference on Research & Development in Information Retrieval,
pages 305–314, 2018.
[WM10] Oliver Williams and Frank McSherry. Probabilistic inference and differential privacy.
Advances in Neural Information Processing Systems (NeurIPS), 23:2451–2459, 2010.
17[WNM23] Tal Wagner, Yonatan Naamad, and Nina Mishra. Fast private kernel density esti-
mation via locality sensitive quantization. In Proceedings of the 40th International
Conference on Machine Learning, ICML’23. JMLR.org, 2023.
[WYY+22] Ruihan Wu, Xin Yang, Yuanshun Yao, Jiankai Sun, Tianyi Liu, Kilian Q Weinberger,
and Chong Wang. Differentially private multi-party data release for linear regression.
In The 38th Conference on Uncertainty in Artificial Intelligence, 2022.
[YDW+21] Xiang Yue, Minxin Du, Tianhao Wang, Yaliang Li, Huan Sun, and Sherman S. M.
Chow. Differential privacyfortextanalytics vianaturaltextsanitization. InFindings,
ACL-IJCNLP 2021, 2021.
[YFA21] Brian Young, Tom Faris, and Luigi Armogida. Levenshtein distance as a measure of
accuracy and precision in forensic pcr-mps methods. Forensic Science International:
Genetics, 55:102594, 2021.
[YNB+22] Da Yu, Saurabh Naik, Arturs Backurs, Sivakanth Gopi, Huseyin A. Inan, Gautam
Kamath, Janardhan Kulkarni, Yin Tat Lee, Andre Manoel, Lukas Wutschitz, Sergey
Yekhanin, and Huishuai Zhang. Differentially private fine-tuning of language mod-
els. In The Tenth International Conference on Learning Representations, ICLR 2022,
2022.
[YSY+22] Xin Yang, Jiankai Sun, Yuanshun Yao, Junyuan Xie, and Chong Wang. Differentially
private label protection in split learning. arXiv preprint arXiv:2203.02073, 2022.
[ZYCW20] Yuqing Zhu, Xiang Yu, Manmohan Chandraker, and Yu-Xiang Wang. Private-knn:
Practical differential privacy for computer vision. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR), pages 11854–
11862, 2020.
18A Proofs for Hamming Distance Data Structure
In this section, we include all proof details in Section 4.
A.1 Proof of Lemma 4.3
Proof of Lemma 4.3. Let E(A),E(A′) be Encode(A) and Encode(A′). Let #(E(A) = S) be the
number of the same bits between E(A) and S, #(E(A) = S) be the number of the different bits
6
between E(A) and S. Then the probability that the random flip process transforms E(A) into S
is:
1 eǫ/(2M1)
Pr[ (A) = S] = ( )#(E(A)6=S)( )#(E(A)=S)
A 1+eǫ/(2M1) 1+eǫ/(2M1)
(eǫ/(2M1))#(E(A)=S)
=
(1+eǫ/(2M1))n
Since for each position, Encode changes at most M bits, and A and A′ only have one different
1
position. Therefore there are at most 2M different bits between E(A) and E(A′). So we have
1
Pr[ A(A) = S] (eǫ/(2M1))|#(E(A)=S)−#(E(A′)=S)|
Pr[ (A′)= S] ≤
A
(eǫ/(2M1))2M1
≤
= eǫ
Thus we complete the proof.
A.2 Proof of Lemma 4.4
Proof of Lemma 4.4. h is a hash function randomly drawn from all functions [2n] [M ]. For
2
→
certainj,h(p) = j forallpareindependentrandomvariables,eachofthemequals1withprobability
1/M , or 0 with probability 1 1/M . So we have
2 2
−
Pr[T 10logk]= Pr[ [h(p) = j] 10logk]
j
| |≥ ≥
pX∈T
|T|
T 1 1
= | | ( )d(1 )|T|−d
(cid:18) d (cid:19) M − M
d=1X0logk 2 2
|T|
T ! 1
| | ( )d
≤ d!(T d)! M
d=1X0logk | |− 2
|T|
T d 1
| | ( )d
≤ d! M
d=1X0logk 2
|T|
1 1
( )d
≤ d! 2
d=1X0logk
|T|
1 1
( )d
≤ (10logk)! 2
d=X10logk
191
≤ 200k
The fifth step follows from that fact that T k,M = 2k.
2
| | ≤
Therefore, by union bound over all j [M ], we can show
2
∈
1
Pr[ j [M ], T < 10logk] 1 2k ( )
2 j
∀ ∈ | | ≥ − · 200k
= 0.99.
Thus, we complete the proof.
A.3 Proof of Lemma 4.5
Proof of Lemma 4.5. g is a hash function randomly drawn from all functions [2n] [M ] [M ].
1 3
× →
Foreverysinglei [M ],defineeventE astheeventthatthe2T valuesin g(2(p 1)+A ,i) p
1 i j p
∈ | | { − | ∈
T g(2(p 1)+B ,i) p T are mapped into distinct positions.
j p j
} { − | ∈ }
S
2|Tj|
c
Pr[E ]= (1 )
i
− M
cY=1 3
2|Tj|
c
1
≥ − M
Xc=1 3
2T (T +1)
j j
= 1 | | | |
− M
3
2(10log2k)
> 1
− 400logk
= 0.5
The fourth step follows from Lemma 4.4. It holds true with probability 0.99.
For different i [M ], E are independent. Therefore, the probability that all E are false is
1 i i
∈
(0.5)M1 < 1/(1000k). By union bound, the probability that for every j [M ] there exists at least
2
∈
one i such that E is true is at least
i
1 M 0.5M1 1 M /(1000k) 0.98.
2 2
− · ≥ − ≥
A.4 Proof of Lemma 4.6
Proof of Lemma 4.6. From Lemma 4.5, for all j, there is at least one i, such that the set g(2(p
{ −
1)+A ,i)p T g(2(p 1)+B ,i)p T contains 2T distinct values. Therefore, for that i,
p j p j j
| ∈ } { − | ∈ } | |
E(A)
i,j,1∼M3
andES(B)
i,j,1∼M3
haveexactly 2 |T
j
|differentbits. Fortherestofi,thedifferentbitsof
E(A) andE(B) isnomorethan2T . Sowehave0.5 M2 max ( M3 E(A)
i,j,1∼M3 i,j,1∼M3 | j | · j=1 i∈[M1] c=1| i,j,c −
E(B) ) = 0.5 M2 2T = T = D (A,B). P P
i,j,c | · j=1 | j | | | ham
P
B Differentiall Private Longest Common Prefix
We design an efficient, ǫ-DP longest common prefix (LCP) data structure in this section. Specif-
ically, for two positions i and j in A and B respectively, we need to calculate the maximum l, so
that A[i : i+l] = B[j : j +l]. For this problem, we build a differentially private data structure
(Algorithm 3 and Algorithm 4). The main contribution is a novel utility analysis that accounts for
the error incurred by differentially private bit flipping.
20Algorithm 3 Differential Private Longest Common Prefix, Part 1
1: data structure DPLCP ⊲ Theorem 5.1
2: members
3: TA,TB for all i [logn],j [2i]
i,j i,j ∈ ∈
4: ⊲ T i,j represents the hamming sketch (Algorithm 1) of the interval [i n/2j,(i+1) n/2j]
· ·
5: end members
6:
7: procedure BuildTree(A 0,1 n,n N +,k N +,ǫ R) ⊲ Lemma B.3
∈ { } ∈ ∈ ∈
8: M 1 logk+loglogn+10, M 2 1, M 3 10, ǫ′ ǫ/logn
← ← ← ←
9: for i from 0 to logn do
10: for j from 0 to 2i 1 do
−
11: T i∗
,j ←
DPHammingDistance.Init(A[j ·n/2i : (j +1) ·n/2i],M 1,M 2,M 3,ǫ′)
12: ⊲ Algorithm 1
13: end for
14: end for
15: return T∗
16: end procedure
17:
18: procedure Init(A 0,1 n,n N +,k N +,ǫ R) ⊲ Lemma B.1
∈ { } ∈ ∈ ∈
19: TA BuildTree(A,n,k,ǫ)
←
20: end procedure
21:
22: procedure QueryInit(B 0,1 n,n N +,k N +) ⊲ Lemma B.1
∈ { } ∈ ∈
23: TB BuildTree(B,n,k,0)
←
24: end procedure
25:
26: procedure IntervalSketch(T,p l [n],p r [n])
∈ ∈
27: Divide the interval [p l,p r] into O(logn) intervals. Each of them is stored on a node of the
tree T.
28: Retrieve the Hamming distance sketches of these nodes as S 1,S 2,...,S t.
29: Initialize a new sketch S 0 with the same size of the sketches above.
←
30: for every position w in the sketch S do
31: S[w] S 1[w] S 2[w] S 3[w] ... S t[w]
← ⊕ ⊕ ⊕ ⊕
32: end for
33: return S
34: end procedure
35:
36: procedure SketchHammingDistance(SA,SB RM1×M2×M3) ⊲ Lemma B.4 and B.5
∈
37: Let M 1,M 2,M 3 be the size of dimensions of the sketches SA and SB.
38: return 0.5 M2 max ( M3 SA SB )
· j=1 i∈[M1] c=1| i,j,c− i,j,c|
39: end procedureP P
40: end data structure
B.1 Time Complexity
We prove the running time of the three operations above.
Lemma B.1. The running time of Init and InitQuery (Algorithm 3) are O(nlogn(logk +
21Algorithm 4 Differential Private Longest Common Prefix, Part 2
1: data structure DPCLP ⊲ Theorem 5.1
2: procedure Query(i [n],j [n]) ⊲ Lemma B.2 and B.6
∈ ∈
3: L 0, R n
← ←
4: while L = R do
6
5: mid L+R
← ⌈ 2 ⌉
6: SA IntervalSketch(TA,i,i+mid) ⊲ Algorithm 3
←
7: SB IntervalSketch(TB,j,j +mid) ⊲ Algorithm 3
←
8: threshold 1.5M 1M 3/(1+eǫ/(logklogn))
←
9: if SketchHammingDistance(SA,SB) threshold then ⊲ Algorithm 3
≤
10: L mid
←
11: else
12: R mid 1
← −
13: end if
14: end while
15: return L
16: end procedure
17: end data structure
loglogn))
Proof. From Lemma 4.2, the running time of building node T is O((n/2i)M ). Therefore the
i,j 1
total building time of all nodes is
logn2i−1 logn
(n/2i)M = 2i (n/2i)M = O(nlogn(logk+loglogn)).
1 1
·
Xi=0 Xj=0 Xi=0
Thus, we complete the proof.
Lemma B.2. The running time of Query (Algorithm 4) is O(log2n(logk+loglogn)).
Proof. In Query (Algorithm 4), we use binary search. There are totally logn checks. In each
check, we need to divide the interval into logn intervals and merge their sketches of size M M M .
1 2 3
So the time complexity is O(log2n(logk+loglogn)).
B.2 Privacy Guarantee
Lemma B.3. The data structure DPLCP (Algorithm 3 and Algorithm 4) is ǫ-DP.
Proof. On each node, we build a hamming distance data structure DPHammingDistance that
is (ǫ/logn)-DP. For two strings A and A′ that differ on only one bit, since every position is in at
most logn nodes on the tree, for any output S, the probability
Pr[BuildTree(A) = S]
(eǫ/logn)logn = eǫ
Pr[BuildTree(A′)= S] ≤
Thus we complete the proof.
22B.3 Utility Guarantee
Before analyzing the error of the query, we first bound the error of SketchHammingDistance
(Algorithm 3).
Lemma B.4. We select M = logk+loglogn+10,M = 1,M = 10 for DPHammingDistance
1 2 3
data structure in BuildTree(Algorithm 3). Let z be the true hamming distance of the two strings
A[i : i+mid] and B[i:i+mid]. Let z be the output of SketchHammingDistance(Algorithm 3).
When ǫ = + (without the random flip process), then we have
∞ e
• if z = 0, then with probability 1, z = 0.
• if z = 0, then with probability 1 e1/(300k2logn), z = 0.
6 − 6
Proof. Our proof follows from the proof of Lemma 4.6.eWe prove the case of z = 0 and z = 0
6
respectively.
When z = 0, it means the string A[i : i+mid] and B[i : i+mid] are identical. Therefore, the
output of the hash function is also the same. Therefore, the output SA and SB from IntervalS-
ketch(Algorithm 3) are identical. Then z = 0.
When z = 0, define set Q := p [mid] A[i+p 1] = B[j +p 1] as the positions where
6 { ∈ e | − 6 − }
string A and B are different. Q = z. Note that M = logk +loglogn+10,M = 1,M = 10,
1 2 3
| |
SA,SB 0,1 M1×M2×M3. For every i′ [M ], the probability that SA and SB are identical is
∈ { } ∈
1 i′ i′
the probability that all c [M ] is mapped exactly even times from the position set Q. Formally,
3
∈
define event E as [ j′, p Q g(p) = j′ mod2 = 0]. Define another event E′ as there is only
∀ |{ ∈ | }|
one position mapped odd times from set Q . Then the probability equals
1∼z−1
Pr[E] =Pr[E′] Pr[E E′]
· |
Pr[E E′]
≤ |
= 1/M
3
ThelaststepisbecausePr[E E′]istheprobabilitythatg(Q )equalstheonlypositionthatmapped
z
|
odd times. There are totally M positions and the hash function g is uniformly distributed, so the
3
probability is 1/M .
3
For different i′ [M ], the event E are independent. So the total probability that z′ = 0 is
1
∈ 6
the probability that for at least one i′, event E holds true. So the probability is 1 (1/M )M1 =
3
−
1 (1/10)logk+loglogn+10 > 1 1/(300k2logn).
− −
Lemma B.5. Let M = logk+loglogn+10,M = 1,M = 10. Let z be the true hamming distance
1 2 3
of the two strings A[i : i+mid] and B[i : i+mid]. Let z be the output of SketchHammingDis-
tance(Algorithm 3). With the random flip process with DP parameter ǫ, we have:
e
• When z =0, with probability 1 1/(300k2logn), z < (1+o(1))M M /(1+eǫ/(logklogn)).
1 3
−
• Whenz > 3M M /(1+eǫ/(logklogn)), withprobabiliety 1 1/(300k2logn), z > (2 o(1))M M /(1+
1 3 1 3
− −
eǫ/(logklogn)).
e
Proof. In the random flip process in DPHammingDistance (Algorithm 1,3), the privacy param-
eter ǫ′ = ǫ/logn. We flip each bit of the sketch with independent probability 1/(1+eǫ/lognlogk).
Then we prove the case of z = 0 and z > 3M M /(1+eǫ/(logklogn)) respectively.
1 3
23When z = 0, similar to the proof of Lemma 4.7, we view the flipping operation as random vari-
ables. Let random variable R be 1 if the sketch SA is flipped, otherwise 0. From Lemma B.4,
i,j,c i,j,c
SA and SB are identical. Then we have
M3
z z = max R
i,j,c
| − | i∈[M1]Xc=1
e
M1 M3
R
i,j,c
≤
Xi=1Xc=1
Since R are independent Bernoulli random variables, using Hoeffding’s inequality (Lemma 3.2),
i,j,c
we have
M1×M3
Pr[ R M M E[R ] > L] e−2L2/(M1×M3)
i,j,c 1 3 i,j,c
| − | ≤
Xi=1
When L = M √M ,
1 3
M1×M3
Pr[ R M M E[R ] > L] e−2M12M3/(M1M3)
i,j,c 1 3 i,j,c
| − | ≤
Xi=1
e−2(logk+loglogn)
≤
1/(300k2logn)
≤
Thus we complete the z = 0 case.
When z > 3M M /(1 + eǫ/(logklogn)), the proof is similar to z = 0. With probability 1
1 3
−
1/(300k2logn),wehave z z < (1+o(1))M M /(1+eǫ/(logklogn)). Thusz > (2 o(1))M M /(1+
1 3 1 3
| − | −
eǫ/(logklogn)).
e e
Lemma B.6. Let w be the output of Query(i,j) (Algorithm 4), w be the longest common prefix of
A[i : n] and B[j : n]. With probability 1 1/(300k2), we have: 1.w w. 2. D (A[i :i+w],B[j :
ham
j +w]) 3M M /(e 1+eǫ/(logklogn)). − ≥
1 3
≤ e e
Proof. In Query(i,j) (Algorithm 4), we use a binary search to find the optimal w. In binary
e
search, there are totally logn calculations of SketchHammingDistance. Define threshold :=
1.5M M /(1+eǫ/(logklogn)). Define a return value of SketchHammingDistance is good if: 1).
1 3
when z = 0, z < threshold. 2). when z > 2 threshold, z < threshold. z and z are defined in
·
Lemma B.5.
e e e
Therefore, by Lemma B.5, each SketchHammingDistance is good with probability at least
1 1/(k2logn). TherearelognSketchHammingDistanceinthebinarysearch, byunionbound,
−
the probability that all of them are good is at least 1 1/(300k2).
−
WhenallanswersforSketchHammingDistancearegood,fromthedefinitionofbinarysearch,
for any two positions L,R such that D (A[i : i+L],B[j,j +L]) = 0, D (A[i : i+R],B[j,j +
ham ham
R]) 2 threshold, we have L w R. Next, we prove w w and D (A[i : i + w],B[j :
ham
≥ · ≤ ≤ ≤
j +w]) 3M M /(1+eǫ/(logklogn)) respectively.
1 3
≤ e e e
w isthetruelongestcommonprefixofA[i : n]andB[j :n],sowehaveD (A[i :i+L],B[j,j+
ham
e
L]) = 0. Let L = w, we have w = L w.
≤
Let R be the minimum value that D (A[i : i+ R],B[j : j +R]) 2 threshold. Because
ham
e ≥ ·
D (A[i : i+R],B[j,j +R]) is monotone for R, and w R, we have D (A[i : i+w],B[j :
ham ham
≤
j +w]) D (A[i : i+R],B[j : j +R]) = 2 threshold = 3M M /(1+eǫ/(logklogn)).
ham 1 3
≤ · e e
Thus we complete the proof.
e
24