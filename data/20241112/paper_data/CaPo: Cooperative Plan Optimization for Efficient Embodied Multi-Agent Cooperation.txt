Preprint
CAPO: COOPERATIVE PLAN OPTIMIZATION FOR EFFI-
CIENT EMBODIED MULTI-AGENT COOPERATION
JieLiu1 PanZhou2∗ YingjunDu1 Ah-HweeTan2 CeesG.M.Snoek1
Jan-JakobSonke3 EfstratiosGavves1
1UniversityofAmsterdam 2SingaporeManagementUniversity
3TheNetherlandsCancerInstitute
j.liu5@uva.nl panzhou3@gmail.com E.Gavves@uva.nl
ABSTRACT
In this work, we address the cooperation problem among large language model
(LLM)basedembodiedagents,whereagentsmustcooperatetoachieveacommon
goal.Previousmethodsoftenexecuteactionsextemporaneouslyandincoherently,
withoutlong-termstrategicandcooperativeplanning,leadingtoredundantsteps,
failures, and even serious repercussions in complex tasks like search-and-rescue
missionswherediscussionandcooperativeplanarecrucial.Tosolvethisissue,we
propose Cooperative Plan Optimization (CaPo) to enhance the cooperation effi-
ciencyofLLM-basedembodiedagents. Inspiredbyhumancooperationschemes,
CaPoimprovescooperationefficiencywithtwophases: 1)meta-plangeneration,
and 2) progress-adaptive meta-plan and execution. In the first phase, all agents
analyze the task, discuss, and cooperatively create a meta-plan that decomposes
the task into subtasks with detailed steps, ensuring a long-term strategic and co-
herent plan for efficient coordination. In the second phase, agents execute tasks
accordingtothemeta-plananddynamicallyadjustitbasedontheirlatestprogress
(e.g., discovering a target object) through multi-turn discussions. This progress-
basedadaptationeliminatesredundantactions,improvingtheoverallcooperation
efficiencyofagents.ExperimentalresultsontheThreeDworldMulti-AgentTrans-
portandCommunicativeWatch-And-HelptasksdemonstrateCaPo’smuchhigher
taskcompletionrateandefficiencycomparedwithstate-of-the-arts.
1 INTRODUCTION
Large Language Models (LLMs) have demonstrated remarkable capabilities in understanding
and generating human language, complex reasoning, and planning, achieving impressive perfor-
mance(OpenAI,2024;Touvronetal.,2023). TheseadvancementsempowerLLM-basedembodied
agents to autonomously make plans (Li et al., 2023a; Padmakumar et al., 2022; Zhu et al., 2023;
Wangetal.,2023;Wuetal.,2023b;Huangetal.,2022b)andperformreasoning(Duetal.,2023;
Haoetal.,2023;Zhouetal.,2024;Huangetal.,2022a)byusinghumanlanguagetoassistpeoplein
dailyactivities,suchashouseworkanddailychores. Thenextmilestoneforagentsistocooperate
withotherstoachievejointtasks. Thisiscrucialnotonlyforefficientlyperformingsimpletasksbut
alsofortacklingcomplexonesthatcannotbecompletedinisolationduetotheirinherentcomplexity
orthedynamicnatureoftheenvironment(Zhangetal.,2023b;Guoetal.,2024;Mandietal.,2023;
Zhangetal.,2023a).
Notably, the cooperation among LLM-based embodied agents is rarely investigated despite being
highlydesired. Conventionalworksoftenfocusonadoptingreinforcementlearning(RL)(Jiang&
Lu,2018;Liuetal.,2021;Wangetal.,2021)toexplorethedynamicsofcooperativebehavioramong
non-LLM-based agents. In spite of their promising performance in certain scenarios, RL-based
cooperationmethodsexhibitlimitedadaptabilityacrossdifferenttasks(Dittadietal.,2021;Cobbe
et al., 2019), since they are often not trained on large-scale data and lack sufficient generalization
ability. To solve this issue, in this work, we are particularly interested in the problem of “how to
develop an effective collaboration framework for LLM-based agents”, since LLMs have revealed
∗Correspondingauthor
1
4202
voN
7
]IA.sc[
1v97640.1142:viXraPreprint
TaskDescription: Transport 1 mouses, 1 pen, 1 iPhone, and1 purse to the bed.
Existing AgentInformation:Alice knows the location of mouse, pen; Bob knows the location of purse. They don’t know the location of iPhone.
Ours (CaPo)
CoELA
Meta Plan Generation
Step1:Alicegraspsmouse,pen,andputthemintothewoodbasket.
Communication& Execution Bob Bobgraspspurse.Step2:Alicetransportsmouseandpenusing
Alice grasps purse; Bob grasps mouse. woodbaskettothebed;Bob transportspursetothebed;andtry to
find iPhone and transport it to the bed.
Alice Progress-adaptive MetaPlan 1
AC lico em trm anu spn oi rc ta
s
t pi uo rn se&
to
E thx ee bc eu dt ;i on mousewood_basket purse S bate sp ke1 t.: BA ol bice grg ar sa ps sp ps um rso eu .s Se t, ep pen 2, :a An ld icp eu tt rath ne sm porin tsto mt oh ue sw eo ao nd
d
Bob transports mouse tothebed. penusingwoodbaskettothebed;Bob transportspursetothe
bed;andtry to find iPhone and transport it to the bed.
pen
Communication& Execution PrN oge rw essAlice f go ru an spd
i
niP gh po en newhen update
Alice graspspen;Bob grasps pen. iPhone Progress-adaptive MetaPlan 2
Step1:Alice grasps iPhone and puts it into wood basket,
. Bobtransportspursetothebed.Step2:Alice transports
.. wood basket (containing pen, mouse, iPhone) to the bed.
(Thetaskiscompleted.)
Figure1: ProcedureexampleoftaskaccomplishmentofCoELA(Zhangetal.,2023b)andour
CaPo. In CoELA, after each action execution, Alice and Bob communicate to decide next action
whichisagreedysingle-stepplanandsuboptimal. Forexample,theydonotusewoodbasketwhich
cancontainseverobjects,andbothextemporaneouslymoveasingleitemtothetargetbedwithout
a long-term strategic and collaborative plan. Differently, in CaPo, Alice and Bob first discuss to
makealong-termmeta-planforstrategicalcooperationinwhichAliceisarrangedtomoveseveral
target items into a wood basket, and Bob moves the remaining target items and also searches the
unknownobjects. Thenduringexecutionphase,bothfollowthemeta-plantoaccomplishtask,and
dynamicallyadaptthemeta-planthelatesttaskprogress,ensuringitseffectivenessandefficiencyin
coordinatingagents.
strongreasoning,planning,andcommunicationabilityacrossdifferenttasksandthusareregarded
asgoodagents’brains.
Among the limited related works, CoELA (Zhang et al., 2023b) proposes an LLM-based multi-
agent cooperation framework in which after each action execution, agents communicate to devise
a single-step plan for the next action. Despite its significant advancements, CoELA’s short-term,
single-step planning, which lacks consideration for long-term strategic collaboration, often results
inextemporaneousandincoherentactionsamongagents,leadingtoseveralpotentialissues. Firstly,
withoutalong-termcoherentcollaborationplan,itleadstonumerousredundantactionstepsandin-
creasedcosts,sinceagents’movementisnoteasyandisindeedexpensiveinthephysicalworld.For
instance,asshowninFig.1,fortheobjecttransporttask,agentAliceandBobdonotusethewood
basket which can contain several objects, and extemporaneously move their nearest target objects
onebyone,leadingtoinferiorefficiency. Moreover,complextasksaredifficulttoaccomplishwith-
outthoroughdiscussionandlong-termcollaboration,especiallyin(embodied)environmentswhere
each agent has only partial observations. Finally, without a long-term cooperative plan, agents’
extemporaneous actions can result in mistakes with severe consequences. For instance, in search-
and-rescue missions, poor coordination can have dire outcomes, such as endangering human lives
duetothecomplexnatureoftheseoperations.
Contributions. To address the above issues, we propose a novel and effective Cooperative Plan
Optimization (CaPo) framework that uses LLMs’ strong reasoning and planning ability to en-
hance the cooperation efficiency of LLM-based embodied agents. Inspired by human cooperation
schemes(Tuomela,1998;Thu¨rmeretal.,2017), CaPoengagesagentsinmulti-turndiscussionsto
createandupdatealong-termstrategicandcoherentmeta-plan,providingstep-by-stepguidanceto
coordinateagentsandefficientlycompletetasks.
Specifically,toaccomplishatask,CaPoconsistsoftwophases: 1)meta-plangeneration,providing
long-termstrategicalandcoherentguidanceforcoordinatingagents,and2)progress-adaptivemeta-
planandexecution,dynamicallyadaptingthemeta-plantoagents’latestprogress. Inthefirstphase,
agentsanalyzethetaskanddiscusswithotheragentsforcollectingrelevantinformation. Next,one
agentisresponsibleformakingameta-planwhichdecomposesthetaskintosubtaskswithdetailed
accomplishment steps like agent allocations, and then collects the feedback from other agents for
furthermeta-planrefinement. Thestepsofmeta-plangenerationandrefinementwillcontinueuntil
all agents reach a consensus or the communication cost is exhausted. This approach ensures the
thoroughdiscussionandanalysisofallagents,helpingtomakealong-termstrategicalandcoherent
2
nalP
ateM
evitpada-ssergorP
noitucexE&Preprint
meta-planforefficientlycoordinatingallagents. Forexample,asillustratedinFig.1,intheobject
transporttask,agentsAliceandBobarestrategicallyassignedtodifferentsubtasks.
In the second phase, as shown in Fig. 1, agents follow the meta-plan from the first phase, and
focus on their assigned subtasks. As progress is made, agents may complete subtasks or make
important observations, such as Alice in Fig. 1 discovering the object “iPhone” which is Bob’s
target. Accordingly, agentsdynamicallyadaptmeta-plantothelatesttaskprogressthroughmulti-
turndiscussions,allowingAlicetohandletheobject“iPhone”andcompletethetaskefficiently.This
progress-adaptiveapproachensuresthatthemeta-planremainseffectiveincoordinatingallagents,
therebyenhancingcooperationefficiency.
Finally, experimental results demonstrate that CaPo significantly improves task completion rates
andefficiencycomparedtostate-of-the-art(SoTA)methodsonthewidelyusedThreeDworldMulti-
Agent Transport task (Zhang et al., 2023b) (object transport task) and the Communicative Watch-
And-Help task (Zhang et al., 2023b) (household chore task). For instance, on the ThreeDworld
Multi-Agent Transport task, CaPo surpasses the SoTA CoELA by 16.7% and 4.7% in completion
ratewithGPT-3.5andGPT-4basedagents,respectively
2 RELATED WORK
LLM-based Agents. LLM-based agents (Hong et al., 2023; Wang et al., 2024; Shen et al., 2024;
Liuetal.,2023a)aredesignedtoautonomouslyperceiveenvironments,executeactions,accumulate
knowledge,andevolvethemselves,withrichrealworldknowledgeandcomplexreasoningcapabil-
ityinheritedfromLLMs. NotableagentslikeAutoGPT(Richards&etal,2021),BabyAGI(Naka-
jima,2023),andAgentGPT(Reworkd,2023)showcaseremarkableproficiencyindecision-making
andcomplexreasoning. Intheembodiedenvironment,LLM-basedagentshaveshownsuperiorca-
pacityinstrategicplanning(Lietal.,2023a;Padmakumaretal.,2022;Wuetal.,2023b;Huangetal.,
2022b). Specifically, LLM-planner (Song et al., 2023) harness LLMs to do few-shot planning for
embodiedagents. PET(Wuetal.,2023a)translatesataskdescriptionwithLLMsintoalistofhigh-
levelsub-tasks. TaPAwu2023embodiedenablestheagenttogenerateexecutableplansbyaligning
LLMswithvisualperceptionmodels. AnotherlineofresearchfocusesonharnessingLLMs’srea-
soning capabilities in embodied tasks (Zhou et al., 2024; Huang et al., 2022a). ELLM (Du et al.,
2023)utilizesLLMstosetpretraininggoalsinRL,guidingagentstowardsthegoalwithouthuman
involvement.
Multi-Agent Cooperation. Multi-agent cooperation and communication have been studied for
decades to improve communication efficiency (Jiang & Lu, 2018; Li et al., 2023b) and plan-
ning (Torreno et al., 2017; Zhang et al., 2023a). Within the domain of embodied intelligence,
ProAgent(Zhangetal.,2023a)harnessesLLMstodevelopproactiveagentsthatdynamicallyadjust
their behavior to foster better cooperation with teammates. RoCo (Mandi et al., 2023) introduce
amulti-robotcollaborationframeworkthatemploysLLMsforbothhigh-levelcommunicationand
low-levelpathplanning. (Guoetal.,2024)proposedaprompt-basedorganizationalframeworkfor
LLMagentstoreducecommunicationcostsandboostteamefficiency.CoELA(Zhangetal.,2023b)
enables agents to plan, communicate, and collaborate effectively, but its plan is one-step plan and
isshort-term. Despitetheseadvancements,thesemethodsfocusonshort-termplanninganddonot
involvesufficientagentdiscussion,whileoursseekstoalong-termstrategicalandcoherentplanvia
agent’sthoughtfuldiscussionsforefficientmulti-agentcooperation.
OptimizationwithLLMs. Withtheadvancementofpromptingtechniques,LLMshaveshownre-
markable performance across various domains (Wei et al., 2022; Kojima et al., 2022; Wang et al.,
2022;Zhouetal.,2022;Madaanetal.,2024). Theirabilitytounderstandnaturallanguagelaysout
anewpossibilityforoptimization. (Yangetal.,2023)firstproposedtoleverageLLMsasoptimizer,
wheretheoptimizationtaskisdescribedinnaturallanguage.OPT2I(Man˜asetal.,2024)aimstoen-
hanceprompt-imageconsistencyintext-to-imagemodelsbyiterativelygeneratingrevisedprompts
withLLMstomaximizetheconsistencyscore. VislingInstruct(Zhuetal.,2024)proposesoptimiz-
ingmulti-modalinstructionformulti-modallanguagemodelsinazero-shotmanner. DyLAN(Liu
et al., 2023b) is particularly relevant to our work. DyLAN (Liu et al., 2023b) enables agents to
interactformultipleroundsinadynamicarchitecturetooptimizetheselectionofagent. Incontrast,
ourworkinvestigatescooperativeplanoptimizationviamulti-turndiscussionbetweenagents.
3Preprint
Memory Module Task Description Semantic Map Meta-plan History Dialogue History Agent Progress Teammate Progress Action History
Meta-Plan Generation Phase
Environment
Communication Module Communication Module
Message
Hi, Bob. Thanks for your advice, I will update Hi, Alice. I should explore the kitchen, please
Alice the meta plan accordingly. Bob update the meta-plan. Observation
Feedback Meta-plan Meta-plan Feedback
Perception Module
Cooperative Planning Module Cooperative Planning Module
Step 1: Alice explores <living_room>. Step 1: Alice explores <living_room>.
M Ple at na St...
ep N:
AB lo icb
e
e [x sp ul bo -r pe ls
a
n< ]k .i t Bch oe bn [> s.
ub-plan].
M Ple at na Ste...
p N:
AB lo icb
e
e [x sp ul bo -r pe ls
a
n< ]o .f f Bic oe b> [.
sub-plan].
Memory Module
No
New Progress?
Progress-adaptive Meta Plan & Execution Phase Previous Meta-plan
Yes
Progress-adaptive Planning Module
Environment Execution Module Plan Parsing Progress- Step 1: Alice goes grasp <basket> in <office>, put <pen>
Module -adaptive ... into it; Bob continues exploring <kitchen>.
MetaPlan Step N: Alice[sub-plan]. Bob [sub-plan].
Figure 2: Overview of the CooperAtive Plan Optimization (CaPo) framework for embodied
multi-agentcooperation. CaPoconsistsoftwokeyphases: 1)meta-planGeneration: Allagents
collaborativelyformulateameta-planbeforetakinganyactionsthroughmulti-turndiscussions.One
agent serves as meta-plan designer, responsible for creating the meta-plan, while all other agents
serveasmeta-planevaluators,providingcriticalfeedbackaboutmeta-plan.2)Progressive-adaptive
meta-plan and Execution: As new progress is made, agents adopt a progress-adaptive planning
moduletoadaptthemeta-plantothelatesttaskprogress,ensuringtheeffectivenessofmeta-plan.
3 PRELIMINARIES
Wefollowpreviouswork(Zhangetal.,2023b;Gongetal.,2023)andformulatetheembodiedmulti-
agent cooperation task as an decentralized partially observable Markov decision process (DEC-
POMDP) (Bernsteinet al., 2002;Spaan et al.,2006), which isdefined as < n,S,O,A,P,r,γ >.
Here,nrepresentsthenumberofagents;Sisthefinitestatespace;Odenotestheobservationspace;
Aisafinitejointactionspaceofallagents;P :S×A×S →[0,1]denotesthetransitionprobability
function;r =S×A→Rdenotestherewardfunction;γ ∈[0,1]denotesthediscountfactor.Inthis
framework,attimestept ∈ N,eachagentiobservestheenvironment’sstates ∈ S,andreceives
t
anobservationsetO . O consistsofaworldobservationOw,whichtheagentgathersthroughits
i i i
sensors, or a communication message observation Oc from other teammate agents. Agent i takes
i
actions from its action space A , which includes a finite set of world action Aw, e.g., grasping a
i i
target object, or a finite set of messaging action Ac. Then agents receive a shared reward r =
i t
r(s ,a ), where a ∈ A denotes the joint actions of agents, and observe a new state s with
t t t t+1
probabilityP(s |s ,a ). Weformulatetheproblemwithtwodecentralizedintelligentembodied
t+1 t t
agentsworkingtogethertocompletealong-horizonrearrangementtask(Zhangetal.,2023b;Batra
etal.,2020)inamulti-roomindoorenvironment.Duringthetask,agentscanexecutemultiplekinds
ofactions,suchasnavigation,interaction,andcommunicationbysendingmessages.
4 COOPERATIVE PLAN OPTIMIZATION
WefirstintroducetheoverallframeworkofCooperAtivePlanOptimization(CaPo)forLLM-based
embodied agents in Sec. 4.1. We then respectively elaborate on the two key phases of CaPo, i.e.,
meta-plangenerationandprogress-adaptivemeta-planandexecution,inSec.4.2andSec.4.3.
4.1 OVERALLFRAMEWORKOFCAPO
CaPo aims to enhance cooperation efficiency of LLM-based embodied agents. Its key idea is to
create a long-term meta-plan for strategically and coherently coordinating agents to complete a
rearragement task. Accordingly, agents follow the meta-plan to complete task step by step, and
dynamicallyadaptthemeta-plantotheirlatestprogress,therebyavoidingredundantworkallocation
andimprovingoverallcooperationefficiency.
4
.. . .. .Preprint
The overall pipeline of CaPo is in Fig. 2. Each agent contains several modules, including 1) a
perception module, 2) a memory module, 3) a communication module, 4) a cooperative planning
module, 5) a progress-adaptive planning module, 6) a plan parsing module, and 7) an execution
module. For each agent, the perception module gathers observations from environment, including
messagesfromotheragentsandrelevantsceneinformationfromtheRGB-Dimage. Thememory
moduledynamicallystoresthesharedtask,dialoguehistorybetweenagents,agentprogress,team-
mateprogress,andactionhistory,allformattedastextdescriptions. Additionally,thesemanticmap
fromtheperceptionmoduleisalsostored. Thecommunicationmoduleretrievesrelevantinforma-
tionfromthememorymoduleandusesanLLMtogeneratemessagesthataresenttootheragents.
Thecooperativeplanningmoduleeithergeneratesthemeta-planorprovidesfeedbackonthemeta-
plan, as seen with Alice and Bob in Fig. 2, respectively. The progress-adaptive planning module
adaptsthemeta-plantoagents’latesttaskprogress. Theplanparsingmodule,poweredbytheLLM,
determineswhichsub-plantoexecutebasedonrelevantinformationfromthememorymoduleand
availableactionsforthecurrentstate. Finally, theexecutionmoduleconvertshigh-levelsub-plans
fromtheplanparsingmoduleintoprimitiveactionsforexecution.
Tocompleteataskcooperativelyandefficiently,inspiredbyhumanscollaboration(Tuomela,1998;
Thu¨rmer et al., 2017), CaPo first analyzes the task at hand to create a long-term meta-plan before
agentstakeanyactions. Allagentsparticipateinthisplan-makingprocess,eithergeneratingmeta-
plan or providing feedback. The meta-plan is then dynamically refined based on the latest agent
progresstoensureitseffectivenessincoordinatingagents. Tothisend,itcontainstwokeyphases,
including1)meta-plangeneration,and2)progress-adaptivemeta-planandexecution. Inthemeta-
plangenerationphase,givenatask,multipleembodiedagentsfirstgatherrelevantinformationsuch
asobjectlocations. Then,theydiscusstogethertocreateameta-planthatdecomposesthetaskinto
subtasks and consider agent situation (e.g., agent and object locations) to assign agents to differ-
ent subtasks with accomplishment steps. In the progress-adaptive meta-plan and execution phase,
agents dynamically align the meta-plan with their latest progress. This is achieved through multi-
turn discussion triggered by clear task progress, such as discovering target objects or successfully
completingsubtasks. Inthefollowing,wewillelaborateonthesetwophasesinturn.
4.2 META-PLANGENERATION
To generate the long-term meta-plan which coordinates all agents to accomplish tasks efficiently,
CaPo introduces two key steps, including 1) meta-plan initialization where one agent initializes a
meta-plan according to the task description and existing information, and 2) meta-plan evaluation
andoptimizationwhereallagentsevaluatethemeta-planandprovidefeedbacktoimprovetheplan.
Meta-plan Initialization. At the beginning of a task, the task description is provided to all
agents, e.g, Transport 2 apples and 3 bananas to the bed. One agent, e.g., Al-
ice in Fig. 2, is randomly selected as the meta-plan designer, and creates the meta-plan through a
cooperativeplanningmodule. Notethatthemeta-planhere,asillustratedinFig.3,differsfromthe
short-termorunorganizedplansusedinpreviouswork(Zhangetal.,2023b;a;Mandietal.,2023).
Specifically,thecooperativeplanningmoduleisequippedwithapre-trainedLLM,andleveragethe
LLMtogeneratethemeta-plan. ThepromptingfortheLLMisorganizedasfollows:
Prompt: <Task Desc> + <Instruct Head> \n. LLM: <Meta-plan>.
Here, <Task Desc>, <Instruct Head>,and <Meta-plan> arethreeplaceholdersfor
thetaskdescription,instructionhead,andgeneratedmeta-plan. Thetaskdescriptionprovidesback-
grounddescriptionsaboutthetask,whiletheinstructionheadintroducesadditionalconstraintsinto
thegenerationofmeta-plan,suchastheformatofmeta-planandavailableactionstogenerateaclear
andexecutableplan. DetailedpromptdesignisshowninFig.9ofAppendix.
Meta-plan Evaluation and Optimization. The meta-plan generated by a single agent is often
biased by that agent’s partial observations, resulting in a suboptimal plan that fails to coordinate
all agents effectively. To address this issue, CaPo involves all agents in a multi-turn discussion
to optimize the meta-plan. Specifically, the meta-plan designer (e.g., Alice in Fig. 3) broadcasts
the meta-plan to all teammate agents, while teammate agents (e.g., Bob in Fig. 3) serve as meta-
plan evaluators, providing feedback about the meta-plan. Since teammate agents have different
partialobservationsoftheenvironment,theyprovidethemeta-plandesignerwithbettersituational
5Preprint
Meta Plan Generation
Step 1:Alice go grasp <purse> (931981), put <purse> (931981) into container; Bob exploreliving room.Step 2: Alice put <pen>
(14280289), <mouse> (3660150) into a container; Bob go grasp found objects and transport them to the bed.
Message to Bob
Alice Hi Bob, I have explored the Office and found a purse. I am currently holding a pen. I willcontinue exploring the Office to
gather theremaining target objectsefficiently. Do you have any suggestions for our next steps in the meta plan?
Message to Alice
Hi Alice, considering thatyou are currently holding one object and have found a container, you can start putting the objectsinto
thecontainer. I will grasp the mousein living room. Please update the action plan accordingly. Satisfied with the meta plan : No
Bob
Meta Plan Generation
Step 1:Alice go grasp a container, put holding <pen>(1480289) into container; Bob go grasp <iPhone> (11176165). Step 2: Alice
go grasp <purse> (931981 , <mouse> (3660150), and put into container; Bob put <iPhone> (11176165) into a container.
Message to Bob
Alice Hi Bob, I have updated the metaplan according to your suggestions. I think you could grasp the iPhone in the living room as you
are already there. Do you have any other suggestions to improve our meta plan?
Message to Alice
Hi Alice, current plan looks good. But I recommend you grasp the <basket>(601123) and then put object into it to transport more
objects efficiently. I will grasp the iPhonein the living room. . Satisfied with the meta plan : No Bob
Meta Plan Generation
Step 1:Alice go grasp the <basket>(601123), put holding <pen>(1480289) into <basket>(601123); Bob go grasp <iPhone>
(11176165). Step 2: Alice go grasp <purse> (931981) , <mouse> (3660150), and put them into <basket>(601123); Bob continue
exploring living room.
Message to Bob Alice
Hi Bob, I have updated the metaplan according to your suggestions. I am satisfied with this meta plan; how do you think about it?
Message to Alice
Hi Alice, I think the meta plan is on the right track, let’s work together to transport as many objects as possible to the bed. Good
progress so far!. Satisfied with the meta plan : Yes Bob
Figure3:Examplesoftheevaluationandoptimizationprocessofmeta-planviamulti-turndis-
cussionbetweenagents. Thediscussionistriggeredbynewprogress,i.e.,Alicefoundsnewobject
’purse’. Here, Alice acts as the meta-plan designer, while Bob serves as the meta-plan evaluator.
TheexampleisderivedfromthetransportingtaskofTDW-MAT.
awareness to help generate a more efficient and effective meta-plan. This optimization process
continues until all agents reach a consensus, i.e., the evaluator agents are satisfied with the meta-
plan,orthecommunicationbudget(e.g.,maximumdiscussionround)isexhausted. Indeed,Fig.8
inSec.5.2analyzestheconvergenceanalysisofthemeta-planoptimizationprocess,andshowsthat
typicallyagentswouldreachaconsensuswithinthreeroundsofdiscussion.
AsshowninFig.2,eachagentisequippedwithacommunicationmodulepoweredbyapretrained
LLM to facilitate multi-turn discussions. Specifically, the communication module first retrieves
relevant information from the memory, e.g., meta-plan, agent state, and previous dialogue history
amongagents,thenpromptstheLLMtogeneratethemessagetosendviathefollowingprompt:
Prompt: <Task Desc> + <Instruct Head> + <Meta-plan> + <Agent State> +
<Dialog History>\n. LLM: <Messages>.
Thetags <Meta-plan>, <Agent State>,and <Dialog History> actasplaceholders
for inserting the meta-plan, the agent’s state, and the dialogue history between agents. The tag
<Instruct Head> differs for the meta-plan designer and evaluator: the former instructs the
LLMtogeneratemessagesaskingteammatesfortheiropinions,whilethelatterfocusesonproviding
feedback on the meta-plan. After receiving feedback from the teammate agents, the meta-plan
creator reinitiates the process to generate a new meta-plan. Fig. 3 illustrates the evaluation and
optimization process of a meta-plan through multi-turn discussions among agents. It is evident
that the optimized meta-plan effectively integrates partially observed information from all agents,
resultinginimprovedcoordinationandefficiency. Detailedpromptdesignsforthecommunication
modulecanbefoundinFig.10and11intheAppendix.
4.3 PROGRESS-ADAPTIVEMETA-PLAN&EXECUTION
Theoptimizedmeta-planactsasahigh-levelguide,assigningsubtaskstoeachagentandcoordinat-
ing them for efficient task completion. However, due to dynamic environmental changes and task
progress updates, the meta-plan can become outdated during execution. As illustrated in Fig. 4,
agents may encounter significant progress, such as discovering target objects or completing sub-
6
1 dnuoR
noissucsiD
2
dnuoR
noissucsiD
3
dnuoR
noissucsiDPreprint
tasks, necessitating adjustments to the meta-plan. In such cases, the previous plan becomes less
effectiveorinvalidforcoordinatingtheagents.
Toaddressthis,wedesignaprogress-adaptiveplanningmoduleforCaPoforadaptingthemeta-plan
totheagents’latestprogress. ThismodulefollowsasimilarprocessasdescribedinSec.4.2—meta-
plan initialization, evaluation, and optimization—but with modified prompting strategies for the
LLMs. Whenever an agent makes new progress, the meta-plan designer promptly generates an
updatedmeta-plan,followedbyamulti-turndiscussionamongallagentstofurtheroptimizeit. The
LLMpromptingstrategiesfortheprogress-adaptiveplanningmodulearestructuredasfollows:
Prompt: <Task Desc> + <Instruct Head> + <Meta-plan> + <Agent Progress>
+ <Teammate Progress> + <Dialog History>\n.
LLM: <meta-plan> or <Messages>.
Here we introduce two placeholders, <Agent Progress> and <Teammate Progress>,
to capture the task progress of agents and enable the LLM to generate progress-aware responses,
suchasmeta-plansorcommunicationmessages.Agentsengageindiscussionstooptimizethemeta-
planuntilaconsensusisreachedorcommunicationresourcesareexhausted(e.g.,afterthreediscus-
sionrounds). DetailedpromptdesignsfortheLLMs—responsibleforgeneratingthemeta-planand
facilitatingmessagesforboththemeta-plandesignerandevaluator—areprovidedinFig.11∼12.
Once the meta-plan or progress-adaptive meta-plan
Subtask: Put poundcakein fridge.
is established, each agent autonomously transforms
the plan into executable actions via a plan pars-
ing module and an execution module. The plan
parsing module generates the latest sub-plan by
retrieving relevant information from the memory
module and converting it into text descriptions,
and then compiles an Action List of all avail-
able high-level sub-plans. We implement the plan
(a) Discover a target object (b) Complete a subtask
parsing module as a pretrained LLM, and prompt
it with a concatenation of Instruct Head, Figure 4: Two types of new progress dur-
Task Description, meta-plan, Action ing task execution. (a) Discover a new ob-
History, Agent Progress, and Action jectpoundcake. (b)completeasubtask.
List to choose the most suitable sub-plan. See
Fig. 13 in Appendix for more prompt details. Given the sub-plan, we adopt a similar execution
moduleasin(Zhangetal.,2023b)togenerateprimitiveactionsforexecutingthesub-plan.
5 EXPERIMENTS
Benchmarks. WefollowCoELA,andadopttheThreeDworldMulti-AgentTransport(TDW-MAT)
task (Zhang et al., 2023b), and the Communicative Watch-And-Help (C-WAH) task (Zhang et al.,
2023b)totestourCaPo. TDW-MATisbuiltonthegeneralpurposevirtualworldsimulationplat-
formTDWplatform(Ganetal.,2020),andrequiresagentstomoveobjectsbytheirhandsorcon-
tainerswhichcancontainseveralobjectsforefficientmovingtothedestination. Moreover,agents
canreceiveego-centric512×512RGB-Dimagesasobservationandcancommunicatewithothers.
ThetestsetofTDW-MATconsists24episodes,whichevenlydividedintofoodandstufftasks. In
C-WAH,agentsarerequestedtocompletefivetypesofhouseholdactivities,representedasvarious
predicates with specific counts that must be satisfied. The test set contains 10 episodes, including
bothsymbolicandvisualobservationsettings. MoredetailsaboutTDW-MATandC-WAHenviron-
mentsareprovidedinAppendixB.1andB.2,respectively.
Metrics. OnTDW-MAT,weadoptTransportRate, i.e., thefractionofsubtaskscompletedwithin
3000timesteps(a.k.a. frames),asperformancemetric. Note,oneactionstepmaylastmultipletime
steps,e.g.,resettingarms. OnC-WAH,AverageStepstocompletealltasksisusedasthemetricto
evaluatecooperationefficiency.
Implementation. FollowingCoELA,wetesttwosettingsonTDW-MATtask: 1)areal-worldset-
tingwheretheperceptionmoduleisinstantiatedasMask-RCNN(Heetal.,2017)thatistrainedus-
7Preprint
ClassicAgents GPT-3.5Agents LLAMA-2Agents
RHP∗ RHP CoELA ProAgent RoCo CaPo CoELA ProAgent RoCo CaPo
(ours) (ours)
w/oOraclePerception
Food(↑) 49 67 67 68 64 70 57 60 59 66
+1.5% −4.5% +4.5% +5.3% +3.5% +15.8%
Stuff(↑) 36 54 39 44 47 45 48 55 51 56
+12.8% +20.1% +15.4% +14.6% +6.3% +16.7%
Avg.(↑) 43 61 52 56 55 57 53 57 55 61
+7.7% +5.6% +9.6% +7.5% +3.8% +15.1%
w/OraclePerception
Food(↑) 52 76 72 80 74 85 60 64 62 66
+11.1% +2.8% +18.1% +6.7% +3.3% +10.0%
Stuff(↑) 49 74 73 76 80 84 63 62 69 76
+4.1% +9.6% +15.1% −1.6% +9.5% +20.6%
Avg.(↑) 50 75 72 78 77 84 62 63 65 71
+8.3% +6.9% +16.7% +1.6% +4.8% +14.5%
Table1:ComparisonofaverageTransportRate(TR,%)ofallbaselinesontheTDW-MATw/o
andw/OraclePerceptiontask. Eachtaskrequiresagentstomovetwokindsofitems, including
Food and Stuff. RHP∗ uses a single agent while all others adopt two agents. The subscript value
like+9.6%in57 denotestherelativeimprovementwhencomparingwithbaselineCoELA.
+9.6%
ClassicAgents GPT-4Agents
MHP∗ MHP CoELA ProAgent RoCo CaPo
(ours)
SymbolicObs(↓) 111 75 57 62 57 51
−8.8% +0.0% +10.5%
VisualObs(↓) 141 103 92 90 89 83
+2.2% +3.2% +9.8%
Table2:ComparisonofAverageStep(AS)ofallcomparedmethodsontheC-WAHtask. Sym-
bolicobsandvisualobsdenotesymbolicobservationandvisualobservationsettings,respectively.
MHP∗usesasingleagentwhileallothersadopttwoagents.
ingcollectedsceneimages(Zhangetal.,2023b),and2)anoraclesettingwithsegmentationground-
truth. We use GPT-3.5-turbo and GPT-4 from the OpenAI API (OpenAI, 2024), and LLAMA-2-
13B-CHAT (Touvron et al., 2023), as LLMs in embodied agents. We set default parameters for
LLMs: temperatureof0.7,amaximumof256outputtokens,andtop-psamplingwithp = 1. Our
codewillbemadepubliclyavailable.
Baselines. We adopt two types of methods as our baseline: 1) classical agents, including
MCTS-based Hierarchical Planner (MHP) (Puig et al., 2020) and Rule-based Hierarchical Plan-
ner (RHP) (Gan et al., 2022). 2) LLM-driven agents, including CoELA Zhang et al. (2023b),
ProAgent(Zhangetal.,2023a),andRoCo(Mandietal.,2023). CoELAZhangetal.(2023b)fea-
tures a modular framework for multi-agent planning, communication, and complete long-horizon
tasks,butgenerateindependentshort-termplanforeachagent. ProAgent(Zhangetal.,2023a),and
RoCo (Mandi et al., 2023) generate joint plans for cooperative agents, and introduce a reflection
looporenvironmentfeedbackforplanvalidation. SeemoredetailsinAppendixA.1.
5.1 MAINRESULTS
Performancecomparison. WefollowCoELAtotesttwo-agentcooperationsetting,andcompare
withclassicalmethodslikeMHPandRHP,andLLM-drivenmethodsCoELA,ProAgent,andRoCo.
Table1summarizestheperformanceofallcomparedmethodsunderthetwosettingsoftheTDW-
MATtask,andshowsseveralobservations. 1)Comparedwiththesingle-agentbaselineRHP,CaPo
andalltwo-agentbaselinesconsistentlymakesignificantimprovements,showingtheeffectiveness
of multi-agent cooperation in embodied tasks. 2) In multi-agent comparisons, our CaPo outper-
forms LLM-driven methods by a remarkable margin, e.g., respectively making 16.7% and 8.4%
improvement over CoELA and RoCo under the oracle perception setting. 3) CaPo with different
LLMs as agent brain exhibits consistent superior performance across all settings. Indeed, CaPo
with LLAMA-2 achieves comparable performance with CoELA with GPT-3.5-turbo under oracle
perceptionsetting. TheimprovementofCaPoisderivedfromitsmeta-planandprogress-adaptive
meta-plan,whichbothprovidestrategicalandcoherentguidanceforagentcooperation,therebyim-
provingcooperationperformance.
Table2reportstheperformanceofallmethodsontheC-WAHtask,andshowssimilarandconsistent
observationstothoseontheTDW-MATtask.Specifically,withGPT-4agents,ourCaPorespectively
8Preprint
CoELA-SoTA ×1.2 84 Previous Meta Plan: S St te ep p 1 2: : A Al li ic ce e e ax np dl Bor oe bs gli rv ai sn pg fr oo uo nm d; o B bo jeb c te sx ap nlo dr e p uo tf f thic ee m. into fridge.
CaPo-Ours
Currentprogress: Next action for Alice: Next action for Bob:
72 Alicefound1wine and2 cupcakes.Grasp 1cupcakeand1wine. Grasp 1cupcake.
59
×1.9
31
23
×2.3
10
1000time steps2000times steps 3000time steps P Mr eo tg ar Pes ls a- na :daptive S St te ep
p
1 2:
:
A Al li ic ce
e
g pr ua ts sp ws i1
n
ew ai nn de a cn ud
p
c1
a
kc eu p inca tok e f;
r
iB dgo eb
;
g Br oas bp ps u1
t
sc cu up pc ca ak ke e.
into fridge.
Figure 5: Comparison of Transport Figure 6: Example of progress-adaptive meta-plan
Rate (%) of CoELA and CaPo using adaptation. Newprogress: Alicefoundtargetobjects,
GPT-3.5underdifferenttimesteps. 1wineand2cupcakes.
Meta Plan Step 1: Alice goes grasp pudding and wine; Bobgoes grasp cupcake. Step 3. Aliceand Bobput holding objects on thetable.
Step 2: Aliceand Bobtransport holding objects to the table.
Alice Bob Alice Bob Alice Bob
Go grasp pudding and Go grasp cupcake. Sub Transport pudding and Transport cupcake. Sub Put pudding and wine Put cupcake on dinner-
wine. Plan wine. Plan on table. -table.
Figure 7: Examples of cooperative behaviors introduced by meta-plan. Guided by meta-plan,
agentsshowclearworkandtaskallocation,therebyimprovingcooperativeefficiency.
makes 9.8%, 7.6% and 6.6% relative improvement on CoELA, ProAgent and RoCo, consistently
highlightingthesuperiorityofCaPoinenhancingmulti-agentcooperation.
Efficiencycomparison. Todemonstratethecooperationefficiency,wecomparethetransportrates
ofCaPoandCoELAwithdifferenttimestepsonTDW-MAT.AsshowninFig.5,withthesametime
stepbudgetandGPT-3.5agents,CaPoconsistentlyoutperformsCoELAacrossvarioustimesteps,
indicatingitssuperioritytocoordinateagentseffectively. Theimprovementisparticularlyclearin
scenariosofsmalltimesteps. Forexample,given1,000timesteps,CaPodoublesthetransportrate
ofCoELAbyimproving10%to23%. Thisshowsthatwithlimitedtimeorresources,acooperative
meta-plancansignificantlyimprovecooperationefficiency.
Qualitative Analysis. Here we investigate the agents’ behavior in CaPo with GPT-4 on the C-
WAH task. In the meta-plan generation phase, as shown in Fig. 3, agents ask questions, provide
feedback, and collaboratively refine the initial meta-plan. Moreover, in this phase, Fig. 7 shows
that with meta-plan as guidance, two agents, Alice and Bob, have clear work/labor allocation to
complete tasks, thereby avoiding redundant steps and improving cooperation efficiency. For the
progress-adaptivemeta-planandexecutionphase,Fig.6alsoshowsthatwhenagentAliceachieves
progress, e.g., discovering three target objects, both agents will accordingly discuss to adapt the
meta-plan,e.g.,grasping1wineand1cupcakebyAlice. Thisongoingadaptationofthemeta-plan
provides strategic, coherent, and timely guidance, facilitating efficient coordination among agents
andultimatelyenhancingmulti-agentcooperation.
5.2 ABLATIONSTUDY
Effects of each component in CaPo. Here we examine the effects of two key components: 1)
meta-plangeneration,whichincludesmeta-planinitialization,evaluation,andoptimization,and2)
the progress-adaptive meta-plan. To evaluate their impact, we first remove both components from
CaPo, resulting in CaPo . As shown in Table 3, CaPo , which includes meta-plan initialization
1 2
but freezes the meta-plan during subsequent procedures, improves upon CaPo by approximately
1
1% across three metrics, demonstrating the value of meta-plan initialization. Similarly, CaPo ,
3
whichincorporatesthefullmeta-plangenerationprocess,outperformsCaPo byasignificantmar-
2
gin,highlightingthebenefitsofmeta-planevaluationandoptimization.Finally,CaPoachievesa7%
improvementoverCaPo ,showcasingtheeffectivenessoftheprogress-adaptivemeta-plan. These
3
resultsunderscoretheimportanceofeachcomponentintheCaPoframework.
9
...
...
...Preprint
Method Food(↑) Stuff(↑) Avg.(↑) Method SymbolicObs(↓) VisualObs(↓)
CaPo 1(NoMP+NoPro.MP) 72 75 73 CaPo×1 93 106
CaPo 2(MPInitialization+NoPro.MP) 73 76 74 CaPo×2 51 83
CaPo 3(MPGeneration+NoPro.MP) 74 80 77 CaPo×3 46 72
CaPo(MPGeneration+Pro.MP) 85 84 84 CaPo×4 45 74
Table 4: Benefits of increasing
Table 3: Effects of the components in CaPo using agent numbers in our CaPo using
GPT-3.5 on TDW-MAT task. We report the transport GPT-4 on the C-WAH task. Aver-
Rate (TR, %). MP” denote ‘Meta Plan” and Progress- age steps required to complete task
AdaptiveMetaPlan”,respectively. arereported.
Effects of agent number. Table 4 investigates the effects of agent number in CaPo using GPT-
4 on the C-WAH task, where “CaPo × C” denotes using C GPT-4 agents. We can observe that
increasingagentnumberto3significantlyreducestheoveralltimestepnumberrequiredtocomplete
tasks. This improvement also shows the effectiveness of our CaPo on multiple agent cooperation.
However,increasingthenumberofagentstofourresultsinonlyminorordegradedimprovements.
Thisisbecauseforsimpletasks,agentsaretoomuchandsufferfromhighly-frequentagentdispatch,
leadingtoinferiorcollaborationefficiency. Forinstance,settingupadiningtabledoesnotrequire
fourwaiters,asamaximumoftwoagentsissufficient.
Progress in meta-plan adaptation. Fig. 4 in
Sec. 4.3 shows two clear task progress examples: Discussion 1 2 3 >3
1) discovering a target object and 2) completing a Rounds
subtask, both of which can trigger agents to adapt
the meta-plan to their latest task progress. Such 36.4
progressiscrucial,asagentsneedtocontinuallyre- 31.2 32.4
fine the meta-plan to complete tasks efficiently and
25.6
maximizecooperation. Conversely, actionswithout
significant progress, such as entering a new room, 20.1 20.9 21.1
donotpromptagentstoadjustthecurrent(progress-
12.3
adaptive)meta-plan. Thisisbecauseitwouldbeun-
necessary,andupdatingthemeta-planinvolvescom-
munication,whichincursadditionaltimeoverhead.
Food tasks Stuff tasks
Convergence analysis of agent discussion. Here Figure 8: Percentage (%) of discussion
weinvestigatetheconvergenceofagentdiscussions, roundsneededforagentstoreachconsen-
specifically focusing on how many rounds are re- sus on a meta-plan, based on results from
quiredforagentstoreachaconsensusonthemeta- TDW-MAT.
plan. In the TDW-MAT environment, we set the
maximumnumberofdiscussionrounds—referredtoasthediscussionbudget—atthree. Asshown
in Fig. 8, agents reach consensus on the new meta-plan within three rounds in most cases, with
78.9%achievingconsensusinthe“Stuff”tasks. Thismeta-plan,whichincorporatesthestatesofall
agents,enablesmoreefficientcooperationintaskcompletion. Furthermore,bylimitingthenumber
ofdiscussionrounds,CaPostrikesabalancebetweendiscussioneffectivenessandbudget,prevent-
ingunnecessaryorprolongeddiscussions.
6 CONCLUSION
Inthiswork,weintroduceCooperativePlanOptimization(CaPo)toenhancecooperationefficiency
ofLLM-drivenembodiedagents. CaPofirstproposestocreateastrategicandcoherentmeta-plan
through multi-turn agents discussion before executing any actions. CaPo first proposes creating a
strategicandcoherentmeta-planthroughmulti-turndiscussionsamongagentsbeforeexecutingany
actions. Thismeta-planservesasanactionguidetoefficientlycoordinatemultipleagentsincom-
pletingtasks.Duringtheexecutionphase,agentsdynamicallyadaptthemeta-plantotheirlatesttask
progress,maintainingtheeffectivenessofthemeta-planincoordinatingagentstocompletetasksef-
ficiently. Experimental results on TDW-MAT and C-WAH tasks show the higher task completion
ratesandefficiencyofCaPocomparedtostate-of-the-arts.
WhileCaPosignificantlyimprovesmulti-agentcooperationefficiency,ithaslimitations,specifically
itsheavyrelianceonLLMsforreasoningandplanningduringmeta-plangenerationandadaptation.
10Preprint
AsshowninTable1,agentsusingstrongerLLMslikeGPT-3.5outperformthoseusingweakerones
likeLLAMA-2. ThisdependencyisacommonchallengeforLLM-basedframeworkslikeCoELA.
REPRODUCIBILITY STATEMENT
WeprovidedetaileddescriptionsofthetwoaforementionedembodiedenvironmentsinSec.B,cov-
ering task settings, as well as the observation and action spaces of the agents. Additionally, we
presentthedetailedpromptdesignsusedinourLLMsinSec.CoftheAppendix. Furthermore,we
includeasectioninAppendixSec.A.3todemonstratethereproducibilityofourexperimentalresults
ontheTDW-MATenvironments.
REFERENCES
Dhruv Batra, Angel X. Chang, Sonia Chernova, Andrew J. Davison, Jia Deng, Vladlen Koltun,
Sergey Levine, Jitendra Malik, Igor Mordatch, Roozbeh Mottaghi, Manolis Savva, and Hao
Su. Rearrangement: Achallengeforembodiedai,2020. URLhttps://arxiv.org/abs/
2011.01975.
Daniel S Bernstein, Robert Givan, Neil Immerman, and Shlomo Zilberstein. The complexity of
decentralizedcontrolofmarkovdecisionprocesses. Mathematicsofoperationsresearch,27(4):
819–840,2002.
KarlCobbe,OlegKlimov,ChrisHesse,TaehoonKim,andJohnSchulman. Quantifyinggeneraliza-
tioninreinforcementlearning. InInternationalconferenceonmachinelearning,pp.1282–1289.
PMLR,2019.
Andrea Dittadi, Frederik Tra¨uble, Manuel Wu¨thrich, Felix Widmaier, Peter Gehler, Ole Winther,
Francesco Locatello, Olivier Bachem, Bernhard Scho¨lkopf, and Stefan Bauer. The role of pre-
trainedrepresentationsfortheoodgeneralizationofreinforcementlearningagents.arXivpreprint
arXiv:2107.05686,2021.
Yuqing Du, Olivia Watkins, Zihan Wang, Ce´dric Colas, Trevor Darrell, Pieter Abbeel, Abhishek
Gupta, and Jacob Andreas. Guiding pretraining in reinforcement learning with large language
models. InInternationalConferenceonMachineLearning,pp.8657–8677.PMLR,2023.
ChuangGan,JeremySchwartz,SethAlter,DamianMrowca,MartinSchrimpf,JamesTraer,Julian
DeFreitas,JonasKubilius,AbhishekBhandwaldar,NickHaber,etal. Threedworld: Aplatform
forinteractivemulti-modalphysicalsimulation. arXivpreprintarXiv:2007.04954,2020.
ChuangGan, SiyuanZhou, JeremySchwartz, SethAlter, AbhishekBhandwaldar, DanGutfreund,
DanielLKYamins,JamesJDiCarlo,JoshMcDermott,AntonioTorralba,etal. Thethreedworld
transportchallenge: Avisuallyguidedtask-and-motionplanningbenchmarktowardsphysically
realisticembodiedai. In2022Internationalconferenceonroboticsandautomation(ICRA),pp.
8847–8854.IEEE,2022.
RanGong,QiuyuanHuang,XiaojianMa,HoiVo,ZaneDurante,YusukeNoda,ZilongZheng,Song-
Chun Zhu, Demetri Terzopoulos, Li Fei-Fei, et al. Mindagent: Emergent gaming interaction.
arXivpreprintarXiv:2309.09971,2023.
XudongGuo,KaixuanHuang,JialeLiu,WenhuiFan,NataliaVe´lez,QingyunWu,HuazhengWang,
Thomas L Griffiths, and Mengdi Wang. Embodied llm agents learn to cooperate in organized
teams. arXivpreprintarXiv:2403.12482,2024.
ShiboHao,YiGu,HaodiMa,JoshuaJiahuaHong,ZhenWang,DaisyZheWang,andZhitingHu.
Reasoningwithlanguagemodelisplanningwithworldmodel. arXivpreprintarXiv:2305.14992,
2023.
KaimingHe,GeorgiaGkioxari,PiotrDolla´r,andRossGirshick. Maskr-cnn. InProceedingsofthe
IEEEinternationalconferenceoncomputervision,pp.2961–2969,2017.
11Preprint
SiruiHong, XiawuZheng, JonathanChen, YuhengCheng, JinlinWang, CeyaoZhang, ZiliWang,
Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, et al. Metagpt: Meta programming for multi-
agentcollaborativeframework. arXivpreprintarXiv:2308.00352,2023.
WenlongHuang,PieterAbbeel,DeepakPathak,andIgorMordatch. Languagemodelsaszero-shot
planners: Extractingactionableknowledgeforembodiedagents. InInternationalConferenceon
MachineLearning,pp.9118–9147.PMLR,2022a.
WenlongHuang,FeiXia,TedXiao,HarrisChan,JackyLiang,PeteFlorence,AndyZeng,Jonathan
Tompson,IgorMordatch,YevgenChebotar,etal.Innermonologue:Embodiedreasoningthrough
planningwithlanguagemodels. arXivpreprintarXiv:2207.05608,2022b.
JiechuanJiangandZongqingLu. Learningattentionalcommunicationformulti-agentcooperation.
Advancesinneuralinformationprocessingsystems,31,2018.
Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large
language models are zero-shot reasoners. Advances in neural information processing systems,
35:22199–22213,2022.
Chengshu Li, Ruohan Zhang, Josiah Wong, Cem Gokmen, Sanjana Srivastava, Roberto Mart´ın-
Mart´ın, Chen Wang, Gabrael Levine, Michael Lingelbach, Jiankai Sun, et al. Behavior-1k: A
benchmarkforembodiedaiwith1,000everydayactivitiesandrealisticsimulation.InConference
onRobotLearning,pp.80–93.PMLR,2023a.
HuaoLi,YuQuanChong,SimonStepputtis,JosephCampbell,DanaHughes,MichaelLewis,and
Katia Sycara. Theory of mind for multi-agent collaboration via large language models. arXiv
preprintarXiv:2310.10701,2023b.
Iou-Jen Liu, Unnat Jain, Raymond A Yeh, and Alexander Schwing. Cooperative exploration for
multi-agent deep reinforcement learning. In International conference on machine learning, pp.
6826–6836.PMLR,2021.
Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding,
Kaiwen Men, Kejuan Yang, et al. Agentbench: Evaluating llms as agents. arXiv preprint
arXiv:2308.03688,2023a.
ZijunLiu,YanzheZhang,PengLi,YangLiu,andDiyiYang. Dynamicllm-agentnetwork: Anllm-
agentcollaborationframeworkwithagentteamoptimization. arXivpreprintarXiv:2310.02170,
2023b.
Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri
Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement
withself-feedback. AdvancesinNeuralInformationProcessingSystems,36,2024.
Oscar Man˜as, Pietro Astolfi, Melissa Hall, Candace Ross, Jack Urbanek, Adina Williams, Aish-
waryaAgrawal, AdrianaRomero-Soriano, and MichalDrozdzal. Improvingtext-to-image con-
sistencyviaautomaticpromptoptimization. arXivpreprintarXiv:2403.17804,2024.
ZhaoMandi,ShreeyaJain,andShuranSong. Roco: Dialecticmulti-robotcollaborationwithlarge
languagemodels. arXivpreprintarXiv:2307.04738,2023.
YoheiNakajima. Babyaig. https://github.com/yoheinakajima/babyagi,2023.
OpenAI. Gpt-4technicalreport,2024.
AishwaryaPadmakumar,JesseThomason,AyushShrivastava,PatrickLange,AnjaliNarayan-Chen,
SpandanaGella,RobinsonPiramuthu,GokhanTur,andDilekHakkani-Tur. Teach: Task-driven
embodied agents that chat. In Proceedings of the AAAI Conference on Artificial Intelligence,
volume36,pp.2017–2025,2022.
XavierPuig, TianminShu, ShuangLi, ZilinWang, Yuan-HongLiao, JoshuaBTenenbaum, Sanja
Fidler, and Antonio Torralba. Watch-and-help: A challenge for social perception and human-ai
collaboration. arXivpreprintarXiv:2010.09890,2020.
12Preprint
XavierPuig, TianminShu, ShuangLi, ZilinWang, Yuan-HongLiao, JoshuaBTenenbaum, Sanja
Fidler, and Antonio Torralba. Watch-and-help: A challenge for social perception and human-ai
collaboration. InInternationalConferenceonLearningRepresentations,2021.
Reworkd. Agentgpt. https://github.com/reworkd/AgentGPT,2023.
ToranBruceRichardsandetal.Auto-gpt:Anautonomousgpt-4experiment.https://github.
com/Significant-Gravitas/AutoGPT,2021.
YongliangShen,KaitaoSong,XuTan,DongshengLi,WeimingLu,andYuetingZhuang. Hugging-
gpt:Solvingaitaskswithchatgptanditsfriendsinhuggingface.AdvancesinNeuralInformation
ProcessingSystems,36,2024.
Chan Hee Song, Jiaman Wu, Clayton Washington, Brian M Sadler, Wei-Lun Chao, and Yu Su.
Llm-planner: Few-shotgroundedplanningforembodiedagentswithlargelanguagemodels. In
Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 2998–3009,
2023.
MatthijsTJSpaan,GeoffreyJGordon,andNikosVlassis.Decentralizedplanningunderuncertainty
forteamsofcommunicatingagents. InProceedingsofthefifthinternationaljointconferenceon
Autonomousagentsandmultiagentsystems,pp.249–256,2006.
JLukasThu¨rmer,FrankWieber,andPeterMGollwitzer.Planningandperformanceinsmallgroups:
Collectiveimplementationintentionsenhancegroupgoalstriving.FrontiersinPsychology,8:603,
2017.
AlejandroTorreno,EvaOnaindia,Anton´ınKomenda,andMichalSˇtolba. Cooperativemulti-agent
planning: Asurvey. ACMComputingSurveys(CSUR),50(6):1–32,2017.
HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,Niko-
layBashlykov,SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,etal. Llama2: Openfounda-
tionandfine-tunedchatmodels. arXivpreprintarXiv:2307.09288,2023.
RTuomela. Collectivegoalsandcooperation. InDiscourse,InteractionandCommunication: Pro-
ceedingsoftheFourthInternationalColloquiumonCognitiveScience(ICCS-95), pp.121–139.
Springer,1998.
Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan,
andAnimaAnandkumar. Voyager: Anopen-endedembodiedagentwithlargelanguagemodels.
arXivpreprintarXiv:2305.16291,2023.
LeiWang,ChenMa,XueyangFeng,ZeyuZhang,HaoYang,JingsenZhang,ZhiyuanChen,Jiakai
Tang,XuChen,YankaiLin,etal. Asurveyonlargelanguagemodelbasedautonomousagents.
FrontiersofComputerScience,18(6):1–26,2024.
XuezhiWang,JasonWei,DaleSchuurmans,QuocLe,EdChi,SharanNarang,AakankshaChowdh-
ery,andDennyZhou. Self-consistencyimproveschainofthoughtreasoninginlanguagemodels.
arXivpreprintarXiv:2203.11171,2022.
Yuanfei Wang, Fangwei Zhong, Jing Xu, and Yizhou Wang. Tom2c: Target-oriented multi-agent
communicationandcooperationwiththeoryofmind. arXivpreprintarXiv:2111.09189,2021.
JasonWei,XuezhiWang,DaleSchuurmans,MaartenBosma,FeiXia,EdChi,QuocVLe,Denny
Zhou,etal. Chain-of-thoughtpromptingelicitsreasoninginlargelanguagemodels. Advancesin
neuralinformationprocessingsystems,35:24824–24837,2022.
Yue Wu, So Yeon Min, Yonatan Bisk, Ruslan Salakhutdinov, Amos Azaria, Yuanzhi Li, Tom
Mitchell,andShrimaiPrabhumoye.Plan,eliminate,andtrack–languagemodelsaregoodteachers
forembodiedagents. arXivpreprintarXiv:2305.02412,2023a.
Zhenyu Wu, Ziwei Wang, Xiuwei Xu, Jiwen Lu, and Haibin Yan. Embodied task planning with
largelanguagemodels. arXivpreprintarXiv:2307.01848,2023b.
13Preprint
Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou, and Xinyun
Chen. Largelanguagemodelsasoptimizers. arXivpreprintarXiv:2309.03409,2023.
CeyaoZhang,KaijieYang,SiyiHu,ZihaoWang,GuangheLi,YihangSun,ChengZhang,Zhaowei
Zhang, Anji Liu, Song-ChunZhu, etal. Proagent: Buildingproactive cooperativeai with large
languagemodels. arXivpreprintarXiv:2308.11339,2023a.
HongxinZhang,WeihuaDu,JiamingShan,QinhongZhou,YilunDu,JoshuaBTenenbaum,Tian-
minShu,andChuangGan. Buildingcooperativeembodiedagentsmodularlywithlargelanguage
models. arXivpreprintarXiv:2307.02485,2023b.
Denny Zhou, Nathanael Scha¨rli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuur-
mans, Claire Cui, Olivier Bousquet, Quoc Le, et al. Least-to-most prompting enables complex
reasoninginlargelanguagemodels. arXivpreprintarXiv:2205.10625,2022.
GengzeZhou,YicongHong,andQiWu.Navgpt:Explicitreasoninginvision-and-languagenaviga-
tionwithlargelanguagemodels.InProceedingsoftheAAAIConferenceonArtificialIntelligence,
volume38,pp.7641–7649,2024.
Dongsheng Zhu, Xunzhu Tang, Weidong Han, Jinghui Lu, Yukun Zhao, Guoliang Xing, Junfeng
Wang, and Dawei Yin. Vislinginstruct: Elevating zero-shot learning in multi-modal language
modelswithautonomousinstructionoptimization. arXivpreprintarXiv:2402.07398,2024.
XizhouZhu, YuntaoChen, HaoTian, ChenxinTao, WeijieSu, ChenyuYang, GaoHuang, BinLi,
LeweiLu,XiaogangWang,etal.Ghostintheminecraft:Generallycapableagentsforopen-world
enviromentsvialargelanguagemodelswithtext-basedknowledgeandmemory. arXivpreprint
arXiv:2305.17144,2023.
14Preprint
Appendix
A ADDITIONAL EXPERIMENTS AND DISCUSSION
A.1 BASELINEMODELS
Weadopttwotypesofmethodsasourbaseline, includingclassicalagentsandLLM-drivenmulti-
agents. (1) The classical agents include MCTS-based Hierarchical Planner (MHP) (Puig et al.,
2020)whichisahierarchicalplanneroriginatingfromtheoriginalWatch-And-HelpChalleng,and
Rule-based Hierarchical Planner (RHP) (Gan et al., 2022) derived from a strong baseline in the
ThreeDWorldTransportChallenge. (2)LLM-drivenagentsconsistofCoELAZhangetal.(2023b),
ProAgent(Zhangetal.,2023a),andRoCo(Mandietal.,2023). CooperativeEmbodiedLanguage
Agent(CoELA)(Zhangetal.,2023b)canplan,communicate,andcollaboratewithotheragentsto
completelong-horizontasks,butgenerateindependentshort-termplanforeachagent. Inaddition,
we also introduce two more baselines – ProAgent (Zhang et al., 2023a) and RoCo (Mandi et al.,
2023), and implement them on TDW-MAT and C-WAH using source codes. These two baselines
generatejointplansforcooperativeagents,andintroduceareflectionlooporenvironmentfeedback
forplanvalidation.
A.2 RESULTSWITHGPT-4
HerewefurtherprovideresultsonTDW-MATtaskusingGPT-4inTable5. Wecanobservethatour
proposed method CaPo achieves consistently better performance that baseline methods across all
tasks,includingfoodandstufftransportation.Thisalsodemonstratetheeffectivenessofourmethod
onimprovingmulti-agentcooperation.
Method Food(↑) Stuff(↑) Avg. (↑)
RHP∗ 52 49 50
RHP 76 74 75
CoELA 87 83 85
CaPo 90 87 89
Table5: TransportRate(TR)comparisonontheTDW-MATtaskusingGPT-4. RHP∗ usesa
singleagentwhileallothersadopttwoagents. Resultswithoracleperceptionarereported.
A.3 REPRODUCIBILITYOFRESULTS
LLM-drivenreasoningandplanningtendtobestochastic,requiringmultiplerunstoassessstability.
To verify the stability and reproducibility of our method, we conducted three runs on TDW-MAT
withoracleperceptionandGPT-3.5agents. AsshowninTable6,theresultsexhibitminorvariance
acrossruns,demonstratingthestabilityandreproducibilityofourmethod.
Runs Food(↑) Stuff(↑) Avg. (↑)
1 0.85 0.84 0.84
2 0.84 0.81 0.82
3 0.85 0.83 0.84
Average 0.84(0.006) 0.82(0.015) 0.83(0.012)
Table 6: Transport Rate (TR) comparison on the TDW-MAT task using GPT-3.5 and oracle
perception. Weperform3runswithrandomseedsandreportmeanandvariance.
15Preprint
B ADDITIONAL ENVIRONMENT DETAILS
We evaluate our method and all baseline methods in two simulated environments: ThreeDWorld
Multi-Agent Transport (TDW-MAT) and Communicative Watch-And-Help (C-WAH). We follow
CoELAZhangetal.(2023b)andlistdetailedintroductionstotheseenvironmentsbelow.
B.1 THREEDWORLDMULTI-AGENTTRANSPORT
Tasks. TDW-MAT consists of two types of tasks, food-transporting task and stuff-transporting
task. The food-transporting task has 6 types of targets (apple, banana, orange, bread, loaf bread,
and burger) and 3 containers (bowl, plate, and tea tray). In contrast, the stuff-transporting task
has6differenttypesoftargets(calculator,mouse,pen,lighter,purse,andiPhone)and3containers
(plastic basket, wood basket, and wicker basket). In each task, there are 10 target objects and 2
to5containersintotal. Additionally, thereare4typesofrooms: livingroom, office, kitchen, and
bedroom, and objects are placed in these rooms consistent with common sense. The agents are
taskedwithtransportingasmanytargetobjectsaspossibletothegoalpositionusingcontainersas
tools. Eachcontainercancarryuptothreeobjects,whilewithoutacontainer,anagentcantransport
onlytwoobjectsatatime. Theagentsmusttransportasmanytargetobjectsaspossiblewithin3000
frames.
ObservationSpace Theembodiedagentreceivesavarietyofobservations,withtheprimaryones
beinganegocentricRGBimageandadepthimage. Additionally,thereareseveralauxiliaryobser-
vations. Theobservationspaceincludes:
• RGB image: This is an egocentric image captured by a forward-facing camera, with a
resolutionof512×512andafieldofviewof90degrees.
• Depthimage: ThisimagesharesthesamecameraintrinsicparametersastheRGBimage.
• OraclePerception(optional):AnimagewhereeachobjectIDisrepresentedbyadistinct
color,usingthesamecameraintrinsicparametersastheRGBimage.
• Agentpositionandrotation: Thepositionandrotationoftheagentwithinthesimulation
environment.
• Messages: Communicationssentbyallagents.
• Heldobjects: Informationabouttheobjectscurrentlyheldbytheagent.
• Opponent held objects: Information about objects held by another agent, if the agent is
withinview.
Action Space In TDW-MAT, agents can perform 7 distinct types of actions to interact with the
environmentorcommunicatewitheachother. Eachactionspansmultipleframes,andthedetailed
actionspaceisoutlinedbelow:
• Moveforward: Theagentadvancesby0.5m.
• Turnleft: Theagentrotatesleftby15degrees.
• Turnright: Theagentrotatesrightby15degrees.
• Grasp: Theagentgraspsanobject,successfullyperformingthisactiononlywheninclose
proximitytotheobject. Theobjectcanbeeitheratargetoracontainer.
• PutIn: Theagentplacesatargetintoacontainer,anactionthatispossibleonlywhenthe
agentisholdingatargetinonehandandacontainerintheother.
• Drop: Theagentreleasestheobjectsheldinhand.
• Sendmessage: Theagentsendsamessagetootheragents,withalimitof500characters
perframe.
B.2 COMMUNICATIVEWATCH-AND-HELP
CommunicativeWatch-And-Help(C-WAH)buildsupontheWatch-And-HelpchallengePuigetal.
(2021)byincorporatingtheabilityforagentstosendmessagestooneanother. Sendingmessages,
likeotheractions,consumesonetimestepandissubjecttoamaximumlengthconstraint.
16Preprint
TaskName PredicateSet
Prepareafternoontea ON(cupcake,coffeetable),ON(pudding,coffeetable),
ON(apple,coffeetable),ON(juice,coffeetable),
ON(wine,coffeetable)
Washdishes IN(plate,dishwasher),IN(fork,dishwasher)
Prepareameal ON(coffeepot,dinnertable),ON(cupcake,dinnertable),
ON(pancake,dinnertable),ON(poundcake,dinnertable),
ON(pudding,dinnertable),ON(apple,dinnertable),
ON(juice,dinnertable),ON(wine,dinnertable)
Putgroceries IN(cupcake,fridge),IN(pancake,fridge),
IN(poundcake,fridge),IN(pudding,fridge),
IN(apple,fridge),IN(juice,fridge),
IN(wine,fridge)
Setupadinnertable ON(plate,dinnertable),ON(fork,dinnertable)
Table7:TaskdescriptioninC-WAH.Thetasksaredividedintofivetypes,eachcontainingseveral
predicates.
Tasks The Communicative Watch-And-Help (C-WAH) framework includes five types of tasks:
Prepare afternoon tea, Wash dishes, Prepare a meal, Put groceries, and Set up a dinner table.
These tasks encompass various household activities, each consisting of several subtasks described
by predicates in the ”ON/IN(x, y)” format, such as ”Put x ON/IN y”. Detailed descriptions of the
tasksareprovidedinTable7.
The primary objective is to complete all given subtasks within 250 timesteps, with each task con-
tainingbetween3to5subtasks.
ObservationSpace C-WAHofferstwomodesofobservation: SymbolicObservationandVisual
Observation.
In Symbolic Observation, following the original Watch-And-Help challenge, an agent can access
comprehensiveobjectinformationwithinthesameroom,includinglocation,status,name,andrela-
tionships.
InVisualObservation,agentsreceiveegocentricRGBanddepthimagesalongwithauxiliaryobser-
vations. Thedetailedobservationspaceincludes:
• RGB image: An egocentric image from a forward-facing camera, with a resolution of
256×512andafieldofviewof60degrees.
• Depthimage: AnimagewiththesamecameraintrinsicparametersastheRGBimage.
• OraclePerception:AnimagewhereeachobjectIDismappedtoacolor,sharingthesame
cameraintrinsicparametersastheRGBimage.
• Agentposition: Theagent’spositionwithinthesimulationworld.
• Messages: Communicationssentbyallagents.
• Heldobjects: Informationabouttheobjectscurrentlyheldbytheagent.
• Opponentheldobjects: Informationaboutobjectsheldbyanotheragent,ifvisible.
ActionSpace TheactionspaceinC-WAHcloselyresemblesthatoftheoriginalWatch-And-Help
Challenge,withtheadditionofthesendmessageaction. Thedetailedactionspaceincludes:
• Walktowards: Movetowardsanobjectinthesameroomortowardsaspecificroom.
• Turnleft: Rotateleftby30degrees.
• Turnright: Rotaterightby30degrees.
17Preprint
• Grasp:Graspanobject,whichcanbesuccessfullyperformedonlywhentheagentisclose
totheobject.
• Open: Openaclosedcontainer,performableonlywhentheagentisnearthecontainer.
• Close: Closeanopencontainer,performableonlywhentheagentisnearthecontainer.
• Put: Place held objects into an open container or onto a surface, performable only when
theagentisnearthetargetposition.
• Sendmessage:Communicatewithotheragents,withalimitof500characterspermessage.
18Preprint
C PROMPT TEMPLATE
Welistthepromptstemplateformetaplaninitialization,communicationmoduleofAlice,commu-
nicationmoduleofBob,cooperativeplanningmodule,andtheplanparsingmoduleasfollows.
I am Alice. My teammate Bob and I want to transport as many target objects as
possibletothebedwiththehelpofcontainerswithin3000steps.Icanholdtwothings
at a time, and they can be objects or containers. I can grasp containers and put
objectsintothemtoholdmoreobjectsatatime.
Assumethatyouareanexpertplanoutlinedesigner.Givenoursharedgoal,pleasehelpme
generate a globalmeta planfor me andBobduringtaskexecution,guidingme andBobto
achievethegoalcollaborativelyassoonaspossible.Notethatacontainercancontainthree
objects,andwillbelostoncetransportedtothebed.IcanonlyputobjectsintothecontainerI
holdaftergraspingit.Allobjectsaredenotedas<name>(id),suchas<table>(712).Actions
takeseveralstepstofinish.Itmaybecostlytogotoanotherroomortransporttothebed,use
theseactionssparingly.
Thegeneratedmetaplanmustmeetfollowingrequirements:
1.Thereare5allowedactionsyoucanusetoconstructthemetaplan.1)‘goto’:movetoa
specifiedroom.2)’explore’:explorearoomforunderlyingtargetobjects.3)‘gograsp’:goto
grasp a specified target object. 4) ‘put’: Place an object into a specified container. 5)
’transport’:Transportholdingobjectsorcontainerstothebedanddropthemonthebed.
2.Themetaplanshouldbeconcise,brief,andreliable.
3.The meta plan must be structured strictly in the three-step format: {Action Plan: Step 1:
Alice xxx, Bobxxx;Step 2:Alice xxx, Bobxxx;Step 3:Alice xxx, Bob xxx}. Here, 'xxx'
representsoneormultipleallowedactions.TheactionsinStep1areofthehighestexecution
priority,whilethoseinStep2andStep3areofmediumandlowestexecutionpriority.
4.ThemetaplanshouldreasonablyarrangethedivisionofactionbetweenAliceandBobin
ordertoachievethegoalassoonaspossible.
Hereisanexampleforyou:
{Goal:[Transport3pens,1lighter,and3iPodstothebed.]
Metaplan:[Step1:Aliceexploresthecurrentroom.Bobexploresthecurrentroom.
Step 2: If any target objects are found, Alice and Bob go grasp objects, put them into
containers,andtransportthemtothebed.
Step 3: Alice goes to one of the remaining rooms and explores it. Bob goes to one of the
remainingroomsandexploresit]}
Goal:$GOAL$
Giventheabovegoal,thinkstepbystep,andgeneratethemetaplan:
Figure9: PromptsforLLMingeneratingmetaplan.
19Preprint
IamAlice.MyteammateBobandIwanttotransportasmanytargetobjectsaspossibleto
thebedwiththehelpofcontainerswithin3000steps.Icanholdtwothingsatatime,andthey
canbeobjectsorcontainers.Icangraspcontainersandputobjectsintothemtoholdmore
objectsatatime.
Assume that you are an excellent leader for coordinating the task. Given our shared goal,
meta plan, dialogue history, latest progress, and my previous actions,, please help me
generateamessagesenttoBob,inordertosharemyprogressandinquiretheopinionofBob
about the meta plan. Note that a container can contain three objects, and will be lost once
transportedtothebed.IcanonlyputobjectsintothecontainerIholdaftergraspingit.All
objects are denoted as <name> (id), such as <table> (712). Actions take several steps to
finish. It may be costly to go to another room or transport to the bed, use these actions
sparingly.
Thegeneratedmessageshouldstrictlymeetfollowingrequirements:
The message has to be concise, reliable, and helpful for assisting Bob and me to make an
efficient and consistent action plan, and transport as many objects to the bed as possible.
Don'tgeneraterepetitivemessages.
Hereisanexampleofgeneratedmassageforyou:
Example:
{Message:HiBob,Iamexploringthe<livingroom>(2000)andIfound<apple>(1242543)
there.Imadeanactionplantoguideustocomplete thetaskefficiently.Doyouhaveany
suggestionsfortheactionplanaccordingtoyourlatestprogress?}
Followingareprovidedinformationforyou:
Goal:\$GOAL\$
Previousmetaplan:\$PREVIOUS\_PLAN\$
Dialoguehistory: \$DIALOGUE\_HISTORY\$
Progress:\$PROGRESS\$
Teammateprogress:\$OPP\_PROGRESS\$
Figure10: PromptsforLLMinthecommunicationmoduleofthementoragent,e.g.,Alice.
20Preprint
IamBob.MyteammateAliceandIwanttotransportasmanytargetobjectsaspossibleto
thebedwiththehelpofcontainerswithin3000steps.Icanholdtwothingsatatime,andthey
canbeobjectsorcontainers.Icangraspcontainersandputobjectsintothemtoholdmore
objectsatatime.
Assume that you are an excellent leader for coordinating the task. Given our shared goal,
actionplan,dialoguehistory,progress,andmypreviousactions,pleasehelpmeanalyzethe
feasibilityoftheactionplanproposedbyAliceandgenerateamessagetosendtoAlice.Note
thatacontainercancontainthreeobjects,andwillbelostoncetransportedtothebed.Ican
onlyputobjectsintothecontainerIholdaftergraspingit.Allobjectsaredenotedas<name>
(id), such as <table> (712). Actions take several steps to finish. It may be costly to go to
anotherroomortransporttothebed,usetheseactionssparingly.
Thegeneratedmetaplanshouldstrictlymeetfollowingrequirements:
1.Themessagehastobeconcise,reliable,andhelpfulforassistingBobandmetomakean
efficient and consistent action plan, and transport as many objects to the bed as possible.
Don'tgeneraterepetitivemessages.
2.The message muststrictlybe inthe followingformat:{Mainmessage:a detailedopinion
andsuggestionsofBob abouttheactionplan.Satisfactionlevel:YesorNo,decidewhether
youaresatisfiedwiththecurrentactionplan.}YoushouldconsidertheprogressofbothAlice
andBobindeterminingthesatisfactionlevelandprovidingsuggestionsfortheactionplan.
3.If you are notsatisfied with the currentaction plan, please pointoutthe reason and your
suggestiononhowtomodifytheactionplaninthemessage.Youcansuggestthenextaction
forAliceinthemessagetoachievethegoalassoonaspossible.
Hereisanexampleofgeneratedmassageforyou:
Example:
Message: { Main message: Hi Alice, your proposed action plan looks great. However,
considering that I found an <wood\_basket> (1870213) and <plastic\_basket>(1843721) in
the<office>(2000),IthinkIshouldgrasp<wood\_basket>(1870213),Isuggestyoutogoto
<office>(2000) to grasp <plastic\_basket>(1843721). Please update the meta plan.
Satisfactionlevel:No}
Followingareprovidedinformationforyou:
Goal:\$GOAL\$
Previousmetaplan:\$PREVIOUS\_PLAN\$
Dialoguehistory: \$DIALOGUE\_HISTORY\$
Progress:\$PROGRESS\$
Teammateprogress:\$OPP\_PROGRESS\$
Figure11: PromptsforLLMinthecommunicationmoduleoftheteammateagent,e.g.,Bob.
21Preprint
IamAlice.MyteammateBobandIwanttotransportasmanytargetobjectsaspossibleto
thebedwiththehelpofcontainerswithin3000steps.Icanholdtwothingsatatime,andthey
canbeobjectsorcontainers.Icangraspcontainersandputobjectsintothemtoholdmore
objectsatatime.
Assumethatyouareanexpertplanoutlinedesigner.Givenoursharedgoal,previousmeta
plan, dialogue history, latest progress, please help me refine the meta plan into a more
comprehensiveandefficientplanforBobandme,inordertoachievethegoalcollaboratively
as soon as possible. Note that a container can contain three objects, and will be lost once
transportedtothebed.IcanonlyputobjectsintothecontainerIholdaftergraspingit.All
objects are denoted as <name> (id), such as <table> (712). Actions take several steps to
finish. It may be costly to go to another room or transport to the bed, use these actions
sparingly.
Thegeneratedmetaplanshouldstrictlymeetfollowingrequirements:
1.Themetaplanshouldbebrief,reliable,authentic,andconsistentwiththelatestprogressof
AliceandBob.Don’tmakerandomandmeaninglessplans.
2.Thereare5allowedactionsyoucanusetoconstructthemetaplan.1)‘goto’:movetoa
specifiedroom.2)’explore’:explorearoomforunderlyingtargetobjects.3)‘gograsp’:go
to grasp a specified target object. 4) ‘put’: Place an object into a specified container. 5)
’transport’:Transportholdingobjectsorcontainerstothebedanddropthemonthebed.
3.Themetaplanmustbestructuredstrictlyinathree-stepformat:{ActionPlan:Step1:Alice
xxx, Bob xxx; Step 2: Alice xxx, Bob xxx; Step 3: Alice xxx, Bob xxx}. Here, 'xxx'
representsoneormultipleallowedactions.TheactionsinStep1areofthehighestpriority,
whilethoseinStep2andStep3areofmediumandlowestpriority,respectively.
4.ThemetaplanshouldreasonablyarrangethedivisionofactionbetweenAliceandBobin
ordertoachievethegoalassoonaspossible.
Followingareprovidedinformationforyou:
Goal:\$GOAL\$
Previousmetaplan:\$PREVIOUS\_PLAN\$
Dialoguehistory: \$DIALOGUE\_HISTORY\$
Progress:\$PROGRESS\$
Teammateprogress:\$OPP\_PROGRESS\$
Figure 12: Prompts for LLM in cooperative planning module to generate progress-adaptive meta
plan.
22Preprint
Iam\$AGENT\_NAME\$.Myteammate\$OPP\_NAME\$andIwanttotransportasmany
targetobjectsaspossibletothebedwiththehelpofcontainerswithin3000steps.Icanhold
two things at a time, and they can be objects or containers. I can grasp containers and put
objectsintothemtoholdmoreobjectsatatime.
Assume that you are an expert decision maker. Given our shared goal, action plan, my
progress,andpreviousactions,pleasehelpmechoosethebestavailableactiontoachievethe
goalassoonaspossible. Notethatacontainercancontainthreeobjects,andwillbelostonce
transportedtothebed.IcanonlyputobjectsintothecontainerIholdaftergraspingit.All
objects are denoted as <name> (id), such as <table> (712). Actions take several steps to
finish. It may be costly to go to another room or transport to the bed, use these actions
sparingly.
Goal:\$GOAL\$
Metaplan:\$META\_PLAN\$
Dialoguehistory: \$DIALOGUE\_HISTORY\$
Progress:\$PROGRESS\$
Previousaction:\$PREVIOUS\_ACTIONS\$
Actionlist:\$ACTION\_LIST\$
Figure13: PromptsforLLMintheplanpraisingmodule.
23