An Interactive Agent Foundation Model
ZaneDurante*12§ ,BidiptaSarkar*12§ ,RanGong*23§ ,RohanTaori12§ ,YusukeNoda2 ,
PaulTang1 ,EhsanAdeli1 ,ShrinidhiKowshikaLakshmikanth1 ,KevinSchulman1 ,ArnoldMilstein1 ,
DemetriTerzopoulos3 ,AdeFamoti2 ,NoboruKuno2 ,AshleyLlorens2 ,HoiVo2† ,
KatsuIkeuchi2† ,LiFei-Fei1† ,JianfengGao2† ,NaokiWake*2▶ ,QiuyuanHuang*2▶
Figure1.OverviewofanAgentAIsystemthatcanperceiveandactindifferentdomainsandapplications.AgentAIisemergingasa
promisingavenuetowardArtificialGeneralIntelligence(AGI).Ourmodelrepresentsaninitialstepinthedevelopmentofamodelthatis
highlycapableofhuman-levelreasoningacrossmanytasksandlevelsofgranularity.
Abstract 1.Introduction
Thedevelopmentofartificialintelligencesystems The development of AI systems that can not only gather
istransitioningfromcreatingstatic,task-specific usefulsensoryinformation,butalsointeractwiththeiren-
models to dynamic, agent-based systems capa- vironmentsinmeaningfulwayshasbeenalong-timegoal
ble of performing well in a wide range of ap- forAIresearchers. Onekeyadvantageofdevelopinggen-
plications. We propose an Interactive Agent eralistAIsystemsisthatoftrainingasingleneuralmodel
FoundationModelthatusesanovelmulti-task acrossmanytasksanddatamodalities,anapproachwhich
agent training paradigm for training AI agents ishighlyscalableviadata,compute,andmodelparameters
across a wide range of domains, datasets, and (Reed et al., 2022). With recent significant advances sur-
tasks. Ourtrainingparadigmunifiesdiversepre- roundinggeneral-purposefoundationmodels(Bommasani
trainingstrategies,includingvisualmaskedauto- etal.,2021),theAIcommunityhasanewsetoftoolsfor
encoders, language modeling, and next-action developing generalist, action-taking AI systems en route
prediction,enablingaversatileandadaptableAI toartificialgeneralintelligence. Despitetheirimpressive
framework. Wedemonstratetheperformanceof resultsacrossvariousAIbenchmarks,largefoundationmod-
ourframeworkacrossthreeseparatedomains— elsfrequentlyhallucinatethepresenceofobjectsandactions
Robotics,GamingAI,andHealthcare. Ourmodel inscenesandinferfactuallyincorrectinformation(Rawte
demonstrates its ability to generate meaningful etal.,2023;Pengetal.,2023). Wepositthatoneofthekey
and contextually relevant outputs in each area. reasonswhythesefoundationmodelshallucinateisdueto
Thestrengthofourapproachliesinitsgeneral- theirlackofgroundingintheenvironmentsinwhichthey
ity, leveragingavarietyofdatasourcessuchas aretrained(e.g.,large-scaleinternetdatainsteadofphys-
robotics sequences, gameplay data, large-scale icalorvirtualenvironments). Furthermore, thedominant
videodatasets,andtextualinformationforeffec- approach for building multimodal systems is to leverage
tivemultimodalandmulti-tasklearning. Ourap- frozenpre-trainedfoundationmodelsforeachmodalityand
proachprovidesapromisingavenuefordevelop- totrainsmallerlayersthatallowforcross-modalinforma-
inggeneralist,action-taking,multimodalsystems. tionpassing(Alayracetal.,2022;Lietal.,2022;2023d;
Dai et al., 2023; Liu et al., 2023). Since the visual- and
language-specificsubmodulesarenottunedduringmulti-
∗EqualContribution.▶ProjectLead.†EqualAdvisor.
modaltraining,anyhallucinationerrorsinthesubmodules
§Workdonewhileinterningorresearchingpart-timeatMicrosoft
will likely be present in the resulting multimodal system.
Research,Redmond.1StanfordUniversity;2MicrosoftResearch,
Additionally,lackofcross-modalpre-trainingcouldmake
Redmond;3UniversityofCalifornia,LosAngeles.
1
4202
beF
8
]IA.sc[
1v92950.2042:viXraAnInteractiveAgentFoundationModel
groundinginformationacrossmodalitieschallenging. modeldecoders(thataregenerallyfrozen)withrepresen-
tative models including Flamingo (Alayrac et al., 2022),
Towardssuchageneralistmodelthatisgroundedandpre-
the BLIP-series (Liet al., 2022; 2023d;Dai et al., 2023),
trainedwithinphysicalorvirtualenvironments,wepropose
andLLaVA(Liuetal.,2023). Thesemodelsaregenerally
aunifiedpre-trainingframeworkforhandlingtext,visual
trainedusingthestandardlanguagemodelingcross-entropy
data, and actions as input. We treat each input type as
loss on large-scale internet data consisting of visual-text
separatetokensandpre-trainourmodeltopredictmasked
pairs, using a source of data similar to that used to train
tokensacrossallthreemodalities. Ourapproachusespre-
contrastivedualencodermodels(Radfordetal.,2021;Bain
trained language models and pre-trained visual-language
etal.,2021;Sunetal.,2023b). Unlikemostpreviouswork,
modelstoeffectivelyinitializeourmodelwithpre-trained
weexploretrainingmodelstopredictvisualtokensandac-
submodules,whichwejointlytraininourunifiedframework.
tion tokens in addition to language tokens and explicitly
WecallourapproachandresultingmodelanInteractive
trainourmodelforagentictasks.
AgentFoundationModel,duetoitsabilitytointeractwith
humansanditsenvironment,aswellasitsvisual-language
2.3.Agent-BasedAI
understandingabilityasshowninFigure1.
Inthispaper,weshowthata277Mparametermodel1thatis Agent-basedAIisdistinguishedfromtraditionalAIbyits
needtogeneratedynamicbehaviorsthataregroundedinan
jointlypre-trainedacross13.4Mvideoframesfromseveral
understandingofenvironmentalcontexts. Recentresearch
distinctdomainsanddatasourcescaneffectivelyengagein
hasfocusedonemployingadvancedlargefoundationmod-
interactivemulti-modalsettingsusingtext,video,images,
elstocreateAgent-basedAIsystems,asshownin(Durante
dialogue,captioning,visualquestionanswering,andembod-
et al., 2024). In the field of robotics, for instance, recent
iedactionswithinfourdisparatevirtualenvironments. In
studies have highlighted the potential of LLM/VLMs in
ordertoeffectivelyevaluatethebroadrangeofcapabilities
enhancing multimodal interactions between robots, envi-
andgeneralizationabilitiesofourmodel,weshowresults
ronments,andhumans. Thisappliestobothmanipulation
acrossdistinctdomains: (1)Robotics,(2)GamingAI,and
(Jiangetal.,2022;Brohanetal.,2023;2022;Lietal.,2023e;
(3)Healthcare. Despiteusingdomain-specificvisualinputs,
Ahnetal.,2022;Shahetal.,2023b;Lietal.,2023c;Wake
textdescriptions,andaction-spaces,ourmodeliseffectively
et al., 2023a; Gong et al., 2023a) and navigation (Gadre
able to generalize across all three domains. To facilitate
et al., 2023; Dorbala et al., 2023; Cai et al., 2023; Shah
researchinthisdiscipline,weplantoreleaseourcodeand
etal.,2023a;Zhouetal.,2023;Dorbalaetal.,2022;Liang
modelspublicly.
etal.,2023;Huangetal.,2023). Additionally,significant
advancesinreinforcementlearninghaveimprovedagentpol-
2.RelatedWork
icytrainingontopofVLM/LLMs. Keyadvancementshave
beenmadeinareassuchasrewarddesign(Yuetal.,2023;
2.1.FoundationModels
Kataraetal.,2023;Maetal.,2023),efficientdatacollection
Alargenumberofworkshavesoughttodevelopgeneral- (Kumaretal.,2023;Duetal.,2023),andthemanagement
purposefoundationmodelsbasedonlarge-scalepre-training oflong-horizonsteps(Xuetal.,2023;Sunetal.,2023a;Li
onbroad-scaleinternetdatafromavarietyofsources(Bom- etal.,2023a;Parakhetal.,2023;Wakeetal.,2023b). Simi-
masanietal.,2021). WithinthefieldofNaturalLanguage larlytorobotics,gamingagentsrequireanunderstandingof
Processing, this generally consists of larger proprietary visualscenesandtextualinstructions/feedback(Puigetal.,
LLMs(Wangetal.,2022)suchastheGPT-series(Brown 2023; Li et al., 2021; Srivastava et al., 2022; Gong et al.,
etal.,2020;Minetal.,2022),orsmalleropen-sourcemod- 2023b). Agent-AIinthecontextofhealthcarehasfocused
els such as the LLaMA series (Touvron et al., 2023), or onthetext-basedinteractionbetweenhumansbyutilizing
instruction-tunedvariantssuchasAlpaca(Taorietal.,2023) thecapabilitiesofLLM/VLMs. Representativeapplications
andVicuna(Zhengetal.,2023). Withinthefieldofcom- include diagnostic assistance (Lee et al., 2023; Li et al.,
putervision,strategiessuchasmaskedauto-encoders(He 2023b),knowledgeretrieval(Pengetal.,2023;Guuetal.,
etal.,2022)andcontrastivelearning(Radfordetal.,2021) 2020),andremotemonitoring(Amjadetal.,2023).
aretwopopularmethodsforself-supervisedlearning.
3.AgentParadigm
2.2.MultimodalUnderstanding
RecentadvancementsinAItechnologyhavebeenremark-
Recently, manymultimodalmodelshavebeendeveloped
able,enablingareasonableunderstandingoflinguisticand
thatseektolearnarelativelysmallnumberofparameters
visual information acquired in open-world environments.
toconnectlargepre-trainedvisualencodersandlanguage
Atthispivotalhistoricaljuncture,publicinterestinembod-
1Wearecurrentlydevelopinganevenlargermodel. iedagenttechnologyisshiftingfromresearchconfinedto
2AnInteractiveAgentFoundationModel
Figure2.WeproposeanAgentAIparadigmforsupportinginteractivemulti-modalgeneralistagentsystems.Thereare5mainmodules
asshown:(1)AgentinEnvironmentandPerceptionwithtask-planningandobservation,(2)Agentlearning,(3)Memory,(4)Action,and
(5)CognitionandConsciousness(weuse“consciousness”toimplyadegreeofawarenessofanagent’sstateandsurroundings).Akey
differencebetweenourapproachandsomepreviousinteractivestrategiesisthat,aftertraining,theagent’sactionwilldirectlyimpacttask
planning,astheagentdoesnotneedtoreceivefeedbackfromtheenvironmenttoplanitsnextactions.
simulations and controlled environments to practical ap- 3. Interaction with humans and environments. Many
plicationsinhighlyuncertainenvironments. Forexample, tasksrequiremultipleroundsofinteractionsbetween
considerascenariowherearobot,uponbeingunboxed,can AI and humans or the environment. Enabling fluent
instantlystartcommunicatingwithnon-experthumansand interactions between them would improve the effec-
swiftly adapt to performing household tasks in the home tivenessandefficiencyofcompletingtasksforAI.
environment. Inthissection,wedefineanewparadigmfor
embodiedagentstopositionourproposedInteractiveAgent
FoundationModelwithinthecontextofthisnewparadigm. Inlightoftheseprinciples,ourproposedInteractiveAgent
Foundation Model represents preliminary research that
Wedefinetheembodiedagentparadigmas“anyintelligent
focusesonthesecriticalaspects,aimingtodevelopanem-
agentcapableofautonomouslytakingsuitableandseamless
bodiedagentthatfunctionsasapracticalassistancesystem.
actionbasedonsensoryinput,whetherinthephysicalworld
Foranoverviewofourgoalsfordevelopinganembodied
or in a virtual or mixed-reality environment representing
agent,seeFigure2.
thephysicalworld”(Figure2). Importantly,anembodied
agent is conceptualized as a member of a collaborative Achievinganembodiedagentisnoteasy,especiallyconsid-
system,whereitcommunicateswithhumanswithitsvision- eringthecomplexdynamicsofsystemswithmulti-modal
languagecapabilitiesandemploysavastsetofactionsbased observationsinthephysicalworld.Despitetheadvancement
onthehumans’needs. Inthismanner,embodiedagentsare ofrecentLLM/VLMs,manychallengesmustbeaddressed,
expectedtomitigatecumbersometasksinvirtualrealityand includingbutnotlimitedto: 1)unstructuredenvironments,
thephysicalworld. wherecurrentvisualinputsaffectbothhigh-levelandlow-
levelactionsoftheembodiedagentgiventhesamegoalin-
Webelievesuchasystemofembodiedagentsrequiresat
struction;2)opensetsofobjects,whichrequiretheagent’s
leastthreekeycomponents:
decision-makingmoduletousecommonsenseknowledge
thatishardtoencodemanually;3)naturallanguageinterac-
1. Perceptionthatismulti-sensorywithfinegranularity. tions,whichrequiretheagenttounderstandandoperateon
Likehumans,multi-sensoryperceptioniscrucialfor morethanjusttemplate-basedcommands,butalsoacontext
agentstounderstandtheirenvironment,suchasgaming ofgoals,constraints,andpartialplansexpressedinevery-
environments,toaccomplishvarioustasks. Inparticu- daylanguage. Toenableamorecomprehensiveapproachto
lar,visualperceptionisusefulforagentsthatcanparse thesecomplexchallenges,theinclusionofresearchersand
thevisualworld(e.g.,images,videos,gameplay). practitionersfromabroaderrangeoffieldsiscritical.
2. Planningfornavigationandmanipulation. Planning 4.AgentFoundationModel
isimportantforlong-rangetasks,suchasnavigating
in a robotics environment and conducting sophisti- OurproposedframeworkisshowninFigure3. Bysyner-
catedtasks. Meanwhile,planningshouldbegrounded gistically combining visual perception with linguistic un-
ongoodperceptionandinteractionabilitiestoensure derstanding,ourmodelsofferthepotentialtoendowrobots
planscanberealizedinanenvironment. withamoreintuitiveunderstandingoftheirsurroundings
3AnInteractiveAgentFoundationModel
Robotics Gaming Healthcare
The clinician is helping
Predictions : Turn Left the patient out of bed.
… …
TASKS What is the clinician doing? +1: Restless driT nh ke in p ga st oie mn et ti hs il ny gin fg ro in m b ae cd u, p.
Action Prediction Action Prediction Visual Question Answering Action Recognition Visual Captioning
TASK-
SPECIFIC
OUTPUTS
Agent Pretraining
AGENT
FOUNDATION
Action Encoder Visual Encoder Language Encoder
MODEL
(UNIFIED))
T DR AA TAIN ING Action + Cognition Video + F (wra /m o e As N/I Nm Oa Tg Ae TION) Language+ Knowledge
Low-level Agent Prediction High-level Agent Instruction
Figure3.OverviewofourInteractiveAgentframework. Ourfoundationmodelisdesignedtoprocessmulti-modalinformationthat
conveysvariouslevelsofabstraction. Thisapproachfacilitatesacomprehensiveunderstandingofthecontextandenvironment,thus
ensuringthatactionsarecoherent.Bytrainingonavarietyoftaskdomainsandapplications,wedevelopaversatilefoundationmodelthat
canbefine-tunedforexecutingoptimalactionsinavarietyofcontexts,pavingthewaytowardsgenerallyintelligentagents.
andbettercontextualreasoning. Ourcurrentworkfocuses kenoractiontokenpredictionviaAˆ=F (W,ℓ(E (V ))).
ϕ θ i
ondevelopingajointimageandvideoencoderandalign- Toincorporatepriortimestepsintoourmodel,wealsoin-
ingthisjointencodertoexistingfoundationmodels. This cludethepreviousactionsandvisualframesasinputduring
has several notable benefits: firstly, it allows for the use pre-training. Foragiventimestept,wepredictAˆ as
t
of both action, image, and video with language datasets
forpre-training. Secondly, itincreasesthecapabilitiesof Aˆ =F (W,ℓ(E (V )),A ,ℓ(E (V )),A ,
t ϕ θ 1 1 θ 2 2
themodelacrossavarietyofdownstreamtasks(e.g.,video
...,ℓ(E (V )),A ,ℓ((E (V ))). (1)
θ t−1 t−1 θ t
understanding, temporal reasoning, action prediction, in-
teractionwithhumanfeedback, etc.). Finally, byusinga In practice, due to memory constraints, we only handle
jointencoder,wecanreducetheoverallmodelsize(instead the previous M actions and frames, and update the pre-
of using two separate encoders), which can be useful for vious V and A as a sliding window. In order to more
i i
edgedeploymentsorinlimitedcomputingscenariossuch effectively train our visual encoder to predict masked vi-
asrobotics,gaming,andinteractivehealthcaretasks. sual tokens, we use sinusoidal positional embeddings, as
in(Heetal.,2022)insteadofthepositionalembeddingsof
4.1.ModelArchitecture CLIP.Sinceweareusingrelativelysmallcheckpoints,we
areabletojointlytrainourentiremodelduringpre-training,
Toeffectivelyinitializeourmodeltohandletext,visual,and
unlike previous visual-language models that largely rely
agent tokens as input, we initialize our architecture with
upon frozen submodules and seek to learn an adaptation
twopre-trainedsubmodules. First,weuseCLIPViT-B16
networkforcross-modalalignment(Alayracetal.,2022;Li
from(Radfordetal.,2021)toinitializeourvisualencoder,
etal.,2022;Liuetal.,2023). Weshowourgeneralprocess
denotedE ,andinitializeouractionandlanguagemodel,
θ forformattingourinputtokensinFigure4,anddescribeour
F ,fromOPT-125M(Zhangetal.,2022). Weencodeeach
ϕ pre-trainingstrategyinSection4.2. Foradditionaldetails,
frame in a video V as visual features Z = E (V ). We
i i θ i seeAppendixA.
enablecross-modalinformationsharingbytraininganad-
ditional linear layer ℓ that transforms the embeddings of
4.2.Pre-TrainingStrategy
ourvisualencoderE intothetokenembeddingspaceof
θ
ourtransformermodelF . Thus, givenatextpromptW Wepre-trainourmodelonawiderangeofroboticsandgam-
ϕ
and a single video frame V , we can obtain Aˆ, a text to- ing tasks, with each input sample containing text instruc-
i
tions,videos,andactiontokens. Wenotateeachsampleasa
4AnInteractiveAgentFoundationModel
where M randomly masks 75% of the image patches, U
only selects the previously masked out features, and E
θ
andD aretheencoderanddecoderforthevisionmodule,
θ
respectively.
Finally, the action modeling loss minimizes the negative
log-likelihoodofeachactiontokenconditionedonallprior
information,includingalltexttokens,priorvisualtokens,
and prior action tokens. The action modeling loss for a
particularsampleS is:
(cid:88)T (cid:88)|At|
L (S)=− logp ((a ) |W,V ,A ,(a ) ).
act θ t i ≤t ≤t t <i
t=1 i=1
(4)
Thefulllossfunctionforeachsamplecombinestheabove
components:
Figure4.Our Unified Tokenization Framework. We propose a
L(S)=
L lang(S)+L mae(S)+L act(S)
. (5)
generalpre-trainingstrategyforpredictinginputtokens.Fortext |W|+(cid:80)T (|V |+|A |)
t=0 t t
tokens, we use the standard language modeling task with next
tokenprediction. Foractions,weexpandthevocabularyofthe
On robotics data, we only use T = 4 frames of video as
languagemodeltoincludespecial“agent”tokensthatrepresent
inputsincethetasksareMarkovianandthereforedonotre-
eachoftheactionsavailabletothelanguagemodel. Finally,we
incorporatevisualtokensintoourframeworkbytrainingavisual quirelonghistoriestoaccuratelypredictthenextaction.Our
encodertopredictmaskedvisualtokens. gamingdatasamplesuseT = 9framesofvideoasinput
sinceanobservationhistoryisnecessaryforthepartially-
observablegamingtasks.
sequenceS =(W,V ,A ,V ,A ,...,V ,A ),whereW
1 1 2 2 T T
isthesequenceoftokenscorrespondingtothetextinstruc-
5.Tasks
tion,V isthesequenceofimagepatchescorrespondingto
i
framei,andA iisthesequenceofactiontokenscorrespond- We believe that a foundational model, trained in visual,
ing to the frame i of a video sequence of T frames. We language, and agent capabilities, leads to a powerful and
denotew
j
asthetokensofthetextpromptW,anddenote
general-purpose tool that significantly impacts a variety
theparametersofourmodelasθ.Foreachsample,thereare
of interactive tasks. To evaluate the effectiveness of our
threecomponentstothelossfunction: languagemodeling, approach, we applied the model to three major agent-AI
maskedimageauto-encoding,andactionmodeling. scenarios,encompassingrepresentativedownstreamtasks:
1)Robotics: human-machinemanipulationinthephysical
Thelanguagemodelinglossisastandardcausallanguage
world;2)Gaming: human-machineembodimentinvirtual
modeling loss to minimize the negative log likelihood of
reality;3)Healthcare: augmentedhuman-machineinterac-
eachtokenintheinstructionconditionedonpriortokens.
ThelanguagemodelinglossforaparticularsampleS is tion in traditional multimodal tasks. For these tasks, the
pre-trainedmodelwasfine-tunedwithspecificdatasets. As
|W| aresult,themodeldemonstratedreasonableandcompetitive
(cid:88)
L (S)=− logp (w |w ). (2) performanceintermsofactionprediction,visualunderstand-
lang θ j <j
j=1 ing,naturallanguage-drivenhuman-machineinteractions,
gaming,andhospitalsceneunderstanding. Weoutlinethe
taskdefinitionsandspecificdatasetsusedbelow.
Themaskedimageautoencodinglossisgeneratedbyran-
domlymasking75%oftheimagepatchesandcalculating
the mean-squared error between the reconstructed image 5.1.RoboticsTasks
and original image in pixel space for the masked image
Fortheroboticsscenario,wetestedthemodelonlanguage-
patches. The masked auto-encoder loss for a particular
guided manipulation tasks. To this end, we selected two
sample,S is:
distinct robotics manipulation datasets: Language-Table
(Lynchetal.,2023)andCALVIN(Meesetal.,2022). In
T
L mae(S)=(cid:88) ||U(V t)−U(D θ(E θ(M(V t))))||2 2, (3) t th oe pL oa bn jeg cu ta sg fe o- lt la ob wle ind gat la as ne gt, ua agro eb co ot mgr mip ap ne dr sr .e Tar hra en dg ae td at wab el re e-
t=1
5AnInteractiveAgentFoundationModel
Action ΔX: 0.0135 ΔX: -0.0045 ΔX: -0.0105 ΔX: -0.0105
Predictions: ΔY: 0.0255 ΔY: 0.0135 ΔY: -0.0075 ΔY: 0.0015
Action R…ecognition Video Captioning Vis Au na sl wQ eu re inst gion
Â 1 Â 2 Â t Â T +2: Agitated Q: Where is
+1: Restless The clinician is the patient?
0: Alert & calm helping the A: The patient
FΦ(W,Z1) FΦ(W,Z1,A1,Z2) … FΦ(W,Zt-M,At-M,...,Zt) … FΦ(W,ZT-M,AT-M...,ZT) -1: D…rowsy patient out of bed. is o a ft t t hh ee b e ed dg .e
ℓ(Eθ(V1)) ℓ(Eθ(V2)) ℓ(Eθ(Vt)) ℓ(Eθ(VT))
Text Instruction: Transformer Model
s ce up ba er fa rt oe
m
th te
h
eb lue Training FΦ(W,Eθ(V1),Eθ(V2),...,Eθ(VT)) Training
green star … … Source Source
Figure5.Ourroboticsandgamingpre-trainingpipeline.Forsim- Nurse Labeled PHI-safe GPT-4
plicity,weusethesamenotationasinSections4.1and4.2;we Annotations Generated
Training Data
representourtextinstructionasW,inputframesasV t,ourvisual Video Input
encoderandlinearprojectionlayerasE θandℓ,respectively,our Figure6.A High-level Overview of our Healthcare Tasks. We
actionandlanguagetransformermodelasF ϕ,andthepredicted leveragednurse-labeledannotationstotrainourmultimodalagent
actionsattimesteptasAˆ t. onhealthcaredata.Toadaptourmodelforvisualquestionanswer-
ing,wegeneratedadditionaltrainingdatawithGPT-4usingthe
PHI-safeprocessshowninAppendixB.
collected through teleoperation in a simulation, totaling
4.93millionframes. IntheCalvindataset,a7-DOFrobot ExperiencedICUnursesgeneratedcaptionsofextracted5-
manipulatorperformedmanipulationtasksfollowingrela- 10secondvideoclipsdepictingcommonnursingactivities
tivelyabstractinstructionslinkedwithaseriesoflanguage intheICU.Wealsoincludedroutinenursingdocumentation
commands. Weutilizedonlythedatacontaininglanguage ofimportantobservationsbasedonlonger5-30minutewin-
instructions,whichamountedto1.44millionframes. We dows,whichincludedcommonclinicalmeasuresthatassist
chosethesetwodatasetstogaininsightsintothemodel’s with assessment and treatment of the patient’s condition.
performanceacrosstwodimensions: language-instruction Fortheanalysisdescribedinthispaper,wefocusedonthe
abstractionandtask-steplength. RASS(RichmondAgitation-SedationScale)scoreusedto
assessthepatient’sstateofagitationandsedation(Sessler
5.2.GamingTasks etal.,2002)andthebedpositiontoconfirmthatthehead
ofthebedisattheproperangletodecreasethechanceof
Our primary gaming dataset consists of the Minecraft
acquiringaventilator-associatedpneumonia(Keeley,2007).
demonstrations collected by contractors in (Baker et al.,
Both assessments are recorded frequently in the medical
2022). Intheoriginaldataset,contractorsweresimplyin-
record and automated documentation has the potential to
structed to play Minecraft with no specific goal, and the
optimizecaretakertime.
datasetprovidedvideogameplaysynchronizedwithplayer
actionsandinventorymetadata. However,sinceourarchi- Inordertofine-tuneourmodelforhumaninteractionsinour
tecture can leverage text instructions, we use GPT-4V to ICUusecase,weleveragedthenurse-providedvideo-clip
labelvideoswithmorespecificinstructions. Ourprompt captions and clinical documentation to have GPT-4 gen-
toGPT-4Valsoincludeschangesintheplayer’sinventory erate a synthetic video question-answer dataset that was
overthevideo,whichwefoundhelpedtoreducemisclas- usedtoexpandthecapabilitiesofourmodelafterhealthcare
sificationsofobjectsandactionsinthevideo. Intotal,the fine-tuning. AdefiniteadvantageoftheGPT-4generated
Minecraftportionofourpre-trainingdatasetconsistsof4.7 derivativedatasetisthatitdidnotuseanyconfidentialpa-
millionframes. tientdataandconsequentlycanbemadepubliclyavailable
to train any language-grounded clinical model. Figure 6
InadditiontoMinecraft,wealsousedadatasetofgameplay
providesanoverviewofthehealthcaretasksweevaluated:
fromBleedingEdge,ateam-basemultiplayergame,which
(1)videocaptioning,(2)videoquestionanswering,and(3)
consistsofvideoandsynchronizedplayeractions.Similarly,
RASS score prediction (which we formulate as an activ-
therearenospecificinstructionsprovidedwiththevideo,
ityrecognitionproblem). Formoreinformationaboutour
soweuseGPT-4Vtolabelthevideosinourdataset. The
GPT-4 based question-answer generation procedure, see
BleedingEdgeportionofourpre-trainingdatasetconsists
AppendixB.
of2.3millionframesacross7differentsettingsinthegame.
5.3.HealthcareTasks
6.Experiments
Inthehealthcaredomainweexplored,ourmaindatasetcon-
sistedofreal-worldrecordedscenesfromhospitalICU(in- Fromatechnicalperspective,wearedevelopingageneric
tensivecareunit)roomsusingwall-mountedRGBcameras. artificialintelligenceagentfoundationmodelthatcanun-
6AnInteractiveAgentFoundationModel
derstandawidearrayofinputmodalitiesandcanproduce
coherent outputs and actions within a wide range of di-
verseinteractiveenvironments. Inadditiontoevaluatingour
frameworkinthesemorespecificdomains,weevaluatedthe
capabilitiesofourpre-trainingmodelonroboticsmanipu-
lation,gameplaying,andinteractivehealthcaretasks. The
detailsoftheexperimentalsettingandourmainresultsare
describedinthefollowingsub-sections.
6.1.Pre-trainingExperiments
To pre-train our model, we used the full training sets of
LanguageTable,CALVIN,Minecraft,andBleedingEdge,
Figure7. Plotoftotalpre-traininglossover100epochs.
andtrainedfor100epochs. Weusedalinearwarmupco-
sinelearningratescheduler,withaninitiallearningrateof
0.0001. Weinitializedthevisioncomponentofourmodel
withtheCLIPbasemodelwithpatchsize16,andinitialized tween70%and140%,andrandomlyshiftingthehuebyat
thelanguageandactioncomponentswithOPT-125M.We most0.05. Weplotourpre-traininglossinFigure7.
used12nodesof16V100GPUsfor175hoursforallofour
pre-training. 6.2.RoboticsExperiments
Weaddednewactiontokenscorrespondingtotheactions The pre-trained model was fine-tuned for the Language-
usedinourtrainingset. Alltasksincludeatokentoindicate TableandCALVINdatasetsandevaluatedseparately. For
startingactionsandatokentoindicateendingactions. For fine-tuning, we used the same pipeline as in pre-training,
Minecraft,thereareadditionally23buttonactions,andwe maintainingtheoriginalMAEandlanguage-modelingloss
discretizedmouseactionsto100binsalongthexaxisand functions, and the original vocabulary size. During fine-
100 bins along the y axis. For Bleeding Edge, there are tuning,50%oftheimagepatchesweremasked,whileno
11buttonactions, and2joysticks. Eachjoystickhas256 maskingwasinvolvedintheevaluation.
possible values for rotation and 4 values for magnitude,
resultinginatotalof520joystickactiontokens. 6.2.1.LANGUAGE-TABLE
For robotics, we added new action tokens corresponding IntheLanguage-tabledataset,weuseddatafromasetup
tovalidactionsintheenvironment,alongwithagentstate involvingatotalof8blocks, outofwhich6blockswere
tokensforproprioception. Forallroboticsdata,weincluded non-manipulatedandunrelatedtothetasks. Thissetupre-
aspecialactiontokentoindicatetheendofatrajectory. In sultedin181,020trajectories. Wespliteachtrajectoryinto
LanguageTable,weincluded21binnedactionsforeachof aseriesof4framestofitourmodelarchitecture,resulting
thexandydirections,representingtheendeffectortransla- in 1,233,659 samples for fine-tuning. To investigate per-
tiontarget. Wealsoincluded21binnedstatetokensrepre- formanceagainstdifferenttaskcharacteristics,themodel
sentingthecurrentendeffectortranslationforeachofthex wasevaluatedon5differentsubtasks: 1)movingablockto
andydirections,andanequalnumberofstatetokensrepre- anotherblock;2)movingablockrelativetoanotherblock;
sentingthepreviousrobotaction. InCALVIN,weincluded 3)movingablocktoanabsoluteposition;4)movingablock
twoactionsforthegripper,indicatingopeningandclosing, to arelativeposition; 5)separating twoblocks. Foreach
alongwith21actionsforeachofthesixdegreesoffreedom task,50trajectorieswererandomlysampledandevaluated
of the end effector in the relative Cartesian displacement three times, and the average success rate was computed.
actionspace. Wealsoincluded21binnedstatesforeachof Whilethepre-trainedmodelperformedbetterthantraining
the14attributesoftheproprioceptivestate,excludingthe from scratch (Table 1), our model was outperformed by
gripperactionwhichhastwostates. othermodelssuchas(Brohanetal.,2023),whichcouldbe
attributedtothefactthatweusedlessdataforpre-training,
Ourgamingdatasethas525,309trajectoriesforMinecraft
only using the human-teleoperated data in the Language-
and256,867forBleedingEdge,eachconsistingof9frames.
Table,CALVIN,andgamingdatasets.
Ourroboticsdatasetconsistsof1,233,659trajectoriesfor
Language-Table and 360,566 for CALVIN, each consist-
6.2.2.CALVIN
ing of 4 frames. Therefore, our total dataset consists of
13,416,484frames. Whensamplingtrajectoriestotrainour IntheCALVINdataset,eachlong-steptrajectorywassplit
model, we additionally added color jitter to each of the intoaseriesof4frames,resultingin360,566samplesacross
images,randomlyscalingthebrightnessandsaturationbe- 34tasksforfine-tuning. Tobettercapturetheentirescene,
7AnInteractiveAgentFoundationModel
Table1.Resultsforroboticsfine-tuningacrosstasksonCALVINandLanguage-Table,alongwiththeircorrespondingevaluationmetrics.
CALVIN LANGUAGETABLE
MODEL 1STEP 2STEP 3STEP 4STEP 5STEP AVGLENS SUCCESSRATE
MCIL 37.3 2.7 0.2 0.0 0.0 0.4 —
OURS(FROMSCRATCH) 20.6 0.8 0.0 0.0 0.0 0.214 40.0
OURS 64.8 29.0 12.3 4.7 1.9 1.127 42.0
6.4.HealthcareExperiments
Table2.Performancemetricsforgamingdata.WereportBLEU-4
scoresforactionpredictioninMinecraft(abbreviatedasMC),and Forourexperimentsonourhealthcaredataset,weevaluated
BleedingEdge(abbreviatedasBE).Wechoosethelastepochfor our model’s ability on three separate downstream tasks:
thepre-trainedmodelandtheepochswiththebestvalidationscore video captioning, visual question answering, and activity
fortheothermodels.
recognitionintheformofRASSscoreprediction. Weused
thefinalcheckpointfromourpre-trainingrunasdescribed
MODEL MC(BLEU-4)↑ BE(BLEU-4)↑
inSection6.1.
OURS(FROMSCRATCH) 0.174 0.238
OURS(PRE-TRAINONLY) 0.170 0.249
Healthcare Setting For visual question-answering, we
OURS(PRE-TRAINANDFINE-TUNED) 0.272 0.411
use the question as the text prompt W, and use the fixed
textprompt“Avideoof”forvideocaptioning. Wetrainour
modeltothecorrespondingtexttokensofthecaptionoran-
thethird-personviewRGBcamerawaschosenasthesource swerandreporttheaverageperplexityacrossbothsettings.
ofimageinputfromtheavailablecameraresources.Forfine- WeframeRASSscorepredictionasa10-wayactivityclas-
tuning, weincorporatedallavailableappearancesettings, sificationproblem,andtrainaseparateclassificationhead
including the one used for testing, to enlarge the dataset, forourmodel. Weusethevideo-levelsettingforourvisual
following the standard ABCD → D task definition. To encoderwith9framesasinput,asdescribedinAppendix
evaluate the model performance with multiple steps, we A.Toevaluatetheeffectivenessofourpre-trainingframe-
computed the averaged success rate at each step, follow- work,wecomparedtheperformanceofourmodelagainst
ing the methodology described in the original CALVIN threebaselinesthatleverageCLIPandOPTforinitializa-
paper(Meesetal.,2022). ComparedtoMulti-contextIm- tion. First,wecomparedagainstafrozenbaselinethatuses
itation Learning (MCIL) (Lynch & Sermanet, 2021), our thesamepre-trainedmodels,keptfrozen,whilefine-tuning
modelshowsbetterperformancewhileonlyusing1%ofthe asinglelinearlayerforcrossmodalinformationpassing,
data(Table1). similarto(Liuetal.,2023). Second,wecomparedagainst
ajointbaselinethatusesthesamepre-trainedmodelsbut
6.3.GamingExperiments fine-tunesthemjointlyalongwiththelinearlayer. Forboth
ofthesebaselines,weencodeframeswithCLIPindividu-
ForbothgamingsettingsofMinecraftandBleedingEdge, allyandconcatenatetheframe-levelembeddings. Third,we
we evaluated our model’s ability to predict actions given comparedagainstabaselineofoursamearchitecture,that
video frames and high-level instructions, along with its makesuseofourvideo-levelencoderandisinitializedfrom
MAEreconstructionquality. Specifically,weusedaheld- CLIPandOPT,butdoesnotuseanylarge-scaleagentpre-
outtestdatasetof100videoseach,formattedinthesame training. Weshowourperformanceagainsttheproposed
mannerasourtrainingdata. baselinesinTable4. Forallresults,wetrainfor20epochs
on416GBV100GPUswithafixedlearningrateof4e-5
WereporttheBLEU-4scoresofactionsinTable2.Wecom-
andreportresultsonaheld-outevaluationset. Forfaircom-
pareourpre-trainedbaselinetofine-tuningontask-specific
parison,wedonotperformanyadditionalhyperparameter
data initialized from our pre-trained model and a version
search.
initializedfromCLIPandOPT.Wefindthatbothfine-tuned
modelsover-fittothetrainingdatawithin5epochs,sowe
report theBLEU-4 testscores fromthe checkpointswith 7.AblationsandAnalysis
thehighestvalidationscore. Wefindthatfine-tuningour
Pretraining Loss Curves: We plot our combined pre-
pre-trainedmodelissignificantlymoreeffectivethantrain-
traininglossacross100epochsinFigure7,andshowindi-
ing from scratch for both gaming domains, highlighting
vidualcomponentsofthelossfunctioninAppendixC.
theimportanceofourdiversepre-trainingmixture. Wealso
showavisualizationofpredictedactionsfromourfine-tuned ComparisonswithGPT-4V: InFigure10,weshowhow
modelcomparedtothevalidationground-truthinTable3 ourmodelhastheabilitytooutputlow-levelactionpredic-
andAppendixE. tions,whileGPT-4Visunabletoconsistentlyoutputlow-
8AnInteractiveAgentFoundationModel
Task Textinstruction Startframe PredictedAction GroundTruthAction
the player is using an
[STARTACTION] [STARTACTION]
iron sword to attack
Minecraft [attack] [ENDOFAC- [attack] [ENDOFAC-
and kill pigs in a for-
TION] TION]
est...
[STARTACTION] [STARTACTION]
theplayeriscontrolling
Bleeding [lockon][meleeattack] [lockon][meleeattack]
a red robot ... fighting
Edge [lrot162] [lmag4] [lrot160] [lmag4]
othercharacters
[ENDOFACTION] [ENDOFACTION]
Table3.Examplesofactionspredictedbyourfine-tunedmodelsforMinecraft(above)andBleedingEdge(below).Moreexamplesare
presentedinAppendixE.
9.ImpactStatement
Table4.Performance on healthcare text generation and RASS
scoreactionrecognition,alongwiththecorrespondingevaluation Thispaperpresentstheinitialstepsonmakinginteractive
metrics.Agentpre-trainingonroboticsandgamingdataimproves agents possible through an Interactive Agent Foundation
performance for action recognition, but does not improve text
Model. Wedonotforeseenegativesocietalconsequences
generationabilities.
from presenting and open-sourcing our current work. In
particular,themainoutputofourmodelisdomain-specific
MODEL PERPLEXITY↓ RASSACC↑
actions,suchasbuttoninputsforgamingdata,makingthe
CLIP+OPT(FROZEN) 93.3 55.4 downstreamapplicationsofourmodeldifferentfromthose
CLIP+OPT(UNFROZEN) 102.7 92.6
OURS(FROMSCRATCH) 100.0 70.3 ofstandardLLMsandVLMs.
OURS(AGENTPRE-TRAINED) 106.3 95.7
Inthedomainofrobotics,wewishtoemphasizethatour
level controls. While our model is able to output precise modelshouldnotbedeployedonrealrobotswithoutmore
movements and actions, GPT-4V only outputs high-level trainingandadditionalsafetyfilters.
instruction.
Inthedomainofgaming,downstreamapplicationsofour
EffectsofAgentPre-Training: InTable2andTable4,
foundation model may have some societal consequences.
wedemonstratetheeffectivenessofouragentpre-training
Smarter, more realistic AI characters could lead to more
strategy compared to training from scratch and training
immersiveworlds,whichcanincreaseplayers’enjoyment
againstanequivalentvisual-languagebaseline. Inparticular,
ingames,butmayalsoleadtosocialwithdrawalifnotused
we show that a commonly used approach for fine-tuning
appropriately. Specifically, more realistic AI characters
visual-language models by using frozen visual encoders,
couldpotentiallyleadtovideogameaddictionandplayers
similar to LLaVA (Liu et al., 2023) or Mini-GPT-4 (Zhu
anthropomorphisingartificialplayers. Weencouragegame
etal.,2023),performsworsethanjointfine-tuningforaction
developerswhobuildAIagentsusingourmodelstomitigate
recognition on our healthcare dataset. Furthermore, our
these potential harms by encouraging social interactions
agentpre-trainingboostsperformanceforactionprediction
betweenhumanplayersandapplyingappropriatecontent
acrossallgamingandroboticsdatasets.
filterstoAIagents.
Inthedomainofhealthcare,weemphasizethatourmodels
8.Conclusion
arenotofficialmedicaldevicesandhavenotgonethrough
WeintroducedanInteractiveAgentFoundationModelde- rigorous testing in live settings. We strongly discourage
signedtotaketext,action,andvisualinputs. Wefoundthat usingourmodelsforself-prescription. Evenasourmodels
bypre-trainingonamixtureofroboticsandgamingdata, improveinfutureiterations,westronglyencouragekeeping
ourmodeliseffectiveinmodelingactionsacrossavariety amedicalpractitionerinthelooptoensurethatunsafeac-
ofdomains,evenshowingpositivetransferwhenfine-tuning tionsareavoided. Asourmodelscontinuetodevelop,we
in unseen domains such as healthcare. The generality of believethattheywillbeusefultocaretakers,especiallyby
our framework allows it to be broadly applicable across automaticallyformingdraftsofdocumentationandnotify-
decision-makingsettings, unlockingnewpossibilitiesfor ingcaretakerswhenpatientsmayneedurgentattention.
generalistagentsinmultimodalsystems.
9AnInteractiveAgentFoundationModel
Finally, we note that the capabilities of agent AI models Systems,35:23716–23736,2022.
maysignificantlychangeatscale. Aswescaleourmodel
Amjad,A.,Kordel,P.,andFernandes,G. Areviewoninno-
in terms of architecture, compute, and training data, we
vationinhealthcaresector(telehealth)throughartificial
willactivelymonitoritscapabilitiesbeforereleasingnew
intelligence. Sustainability,15(8):6655,2023.
versionspublicly.
Bain,M.,Nagrani,A.,Varol,G.,andZisserman,A. Frozen
Acknowledgements intime: Ajointvideoandimageencoderforend-to-end
retrieval. InProceedingsoftheIEEE/CVFInternational
WeareespeciallygratefultoDesneyTan,PeterLee,Doug ConferenceonComputerVision,pp.1728–1738,2021.
Burger,RyenWhite,EceKamar,JohnLangford,Jonathan
Baker, B., Akkaya, I., Zhokov, P., Huizinga, J., Tang, J.,
CarlsonandMicrosoft’sOfficeoftheCTO(OCTO)fortheir
Ecoffet, A., Houghton, B., Sampedro, R., and Clune,
advice,enormoussupport,andencouragement. Weappre-
J. Videopretraining(vpt): Learningtoactbywatching
ciate the Microsoft gaming team, Microsoft X-box team,
unlabeledonlinevideos. AdvancesinNeuralInformation
Microsoft 343 team, Kareem Choudhry, Haiyan Zhang,
ProcessingSystems,35:24639–24654,2022.
SpencerPerreault,DaveBignell,KatjaHofmann,SamDe-
vlin,ShanzhengTan,andRalucaGeorgescuforthegaming
Bommasani, R., Hudson, D. A., Adeli, E., Altman, R.,
datacollectionandsharing. WethankBillDolan,Nebojsa
Arora,S.,vonArx,S.,Bernstein,M.S.,Bohg,J.,Bosse-
Jojic,SudhaRao,AdrianBrown,AndrzejBanburski-Fahey,
lut,A.,Brunskill,E.,etal. Ontheopportunitiesandrisks
andJianweiYangfortheirearlyinsightfuldiscussionsand
offoundationmodels. arXivpreprintarXiv:2108.07258,
helpwiththegamingaspectsofourproject. Weappreciate
2021.
KiranMuthabatullaandtheMSRCentralEngineering(CE)
teamfortheirdiscussionandfeedbackfortheproject. The Brohan,A.,Brown,N.,Carbajal,J.,Chebotar,Y.,Dabis,J.,
authors gratefully acknowledge the Microsoft HoloLens Finn,C.,Gopalakrishnan,K.,Hausman,K.,Herzog,A.,
team,MicrosoftMeshteam,andAntonioCriminisifortheir Hsu,J.,etal. Rt-1: Roboticstransformerforreal-world
generousprovisionofequipmentandprojectdiscussions. controlatscale. arXivpreprintarXiv:2212.06817,2022.
Finally,wewouldliketoexpressourgenuineappreciation
Brohan, A., Brown, N., Carbajal, J., Chebotar, Y., Chen,
forJimJernigan,BenHuntley,OlegLosinets,theMicrosoft
X., Choromanski, K., Ding, T., Driess, D., Dubey, A.,
AOAI team, and the GCR team for their Azure-OpenAI
Finn, C., et al. Rt-2: Vision-language-action models
endpointsupportandtheirpointerstotheliterature.
transferwebknowledgetoroboticcontrol. arXivpreprint
WewouldalsoliketothankourcolleaguesfromStanford’s arXiv:2307.15818,2023.
Partnership in AI-assisted Care, who helped inform the
Brown,T.,Mann,B.,Ryder,N.,Subbiah,M.,Kaplan,J.D.,
medicalapplicationsexploredinthiswork. Inparticular,we
Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,
wouldliketothankAmitKaushalandRogerBohnfortheir
Askell,A.,etal. Languagemodelsarefew-shotlearners.
clinical expertise and guidance. Additionally, we greatly
Advancesinneuralinformationprocessingsystems,33:
appreciateZelunLuo,DavidDai,andDevDashfortheir
1877–1901,2020.
participationasactorsforourhospitaldataset.
Cai,W.,Huang,S.,Cheng,G.,Long,Y.,Gao,P.,Sun,C.,
ThisresearchwassupportedbyMicrosoftResearchProject
andDong,H. Bridgingzero-shotobjectnavigationand
Green2024,MicrosoftResearchProjectFair2023,Stanford
foundationmodelsthroughpixel-guidednavigationskill.
University,UniversityofCaliforniaatLosAngeles,MSR
arXivpreprintarXiv:2309.10309,2023.
Acceleratorteam,andtheMicrosoftOCTOteam.
Dai, W., Li, J., Li, D., Tiong, A. M. H., Zhao, J., Wang,
References W.,Li,B.,Fung,P.,andHoi,S. Instructblip: Towards
general-purposevision-languagemodelswithinstruction
Ahn,M.,Brohan,A.,Brown,N.,Chebotar,Y.,Cortes,O.,
tuning,2023.
David,B.,Finn,C.,Gopalakrishnan,K.,Hausman,K.,
Herzog, A., et al. Do as i can, not as i say: Ground- Dorbala, V. S., Sigurdsson, G., Piramuthu, R., Thoma-
ing language in robotic affordances. arXiv preprint son, J., and Sukhatme, G. S. Clip-nav: Using clip for
arXiv:2204.01691,2022. zero-shotvision-and-languagenavigation. arXivpreprint
arXiv:2211.16649,2022.
Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Dorbala, V. S., Mullen Jr, J. F., and Manocha, D. Can
Hasson,Y.,Lenc,K.,Mensch,A.,Millican,K.,Reynolds, an embodied agent find your” cat-shaped mug”? llm-
M.,etal.Flamingo:avisuallanguagemodelforfew-shot based zero-shot object navigation. arXiv preprint
learning. Advances in Neural Information Processing arXiv:2303.03480,2023.
10AnInteractiveAgentFoundationModel
Du, Y., Yang, M., Florence, P., Xia, F., Wahid, A., Lee,P.,Bubeck,S.,andPetro,J. Benefits,limits,andrisks
Ichter,B.,Sermanet,P.,Yu,T.,Abbeel,P.,Tenenbaum, of gpt-4 as an ai chatbot for medicine. New England
J. B., et al. Video language planning. arXiv preprint JournalofMedicine,388(13):1233–1239,2023.
arXiv:2310.10625,2023.
Li, B., Wu, P., Abbeel, P., and Malik, J. Interactive
Durante, Z., Huang, Q., Wake, N., Gong, R., Park, J. S., task planning with language models. arXiv preprint
Sarkar,B.,Taori,R.,Noda,Y.,Terzopoulos,D.,Choi,Y., arXiv:2310.10645,2023a.
et al. Agent ai: Surveying the horizons of multimodal
interaction. arXivpreprintarXiv:2401.03568,2024. Li,C.,Xia,F.,Mart´ın-Mart´ın,R.,Lingelbach,M.,Srivas-
tava,S.,Shen,B.,Vainio,K.,Gokmen,C.,Dharan,G.,
Gadre,S.Y.,Wortsman,M.,Ilharco,G.,Schmidt,L.,and Jain, T., et al. igibson 2.0: Object-centric simulation
Song,S. Cowsonpasture: Baselinesandbenchmarksfor for robot learning of everyday household tasks. arXiv
language-drivenzero-shotobjectnavigation. InProceed- preprintarXiv:2108.03272,2021.
ingsoftheIEEE/CVFConferenceonComputerVision
andPatternRecognition,pp.23171–23181,2023. Li,C.,Wong,C.,Zhang,S.,Usuyama,N.,Liu,H.,Yang,J.,
Naumann,T.,Poon,H.,andGao,J. Llava-med: Training
Gong,R.,Gao,X.,Gao,Q.,Shakiah,S.,Thattai,G.,and alargelanguage-and-visionassistantforbiomedicinein
Sukhatme,G.S. Lemma:Learninglanguage-conditioned oneday. arXivpreprintarXiv:2306.00890,2023b.
multi-robotmanipulation.IEEERoboticsandAutomation
Letters,2023a. Li, J., Li, D., Xiong, C., and Hoi, S. Blip: Boot-
strappinglanguage-imagepre-trainingforunifiedvision-
Gong,R.,Huang,Q.,Ma,X.,Vo,H.,Durante,Z.,Noda,Y., languageunderstandingandgeneration. arXivpreprint
Zheng,Z.,Zhu,S.-C.,Terzopoulos,D.,Fei-Fei,L.,etal. arXiv:2201.12086,2022.
Mindagent: Emergentgaminginteraction. arXivpreprint
arXiv:2309.09971,2023b. Li, J., Gao, Q., Johnston, M., Gao, X., He, X., Shakiah,
S.,Shi,H.,Ghanadan,R.,andWang,W.Y. Mastering
Guu, K., Lee, K., Tung, Z., Pasupat, P., and Chang, M. robot manipulation with multimodal prompts through
Retrieval augmented language model pre-training. In pretraining and multi-task fine-tuning. arXiv preprint
Internationalconferenceonmachinelearning,pp.3929– arXiv:2310.09676,2023c.
3938.PMLR,2020.
Li, J., Li, D., Savarese, S., and Hoi, S. Blip-2: Boot-
He,K.,Chen,X.,Xie,S.,Li,Y.,Dolla´r,P.,andGirshick,R. strapping language-image pre-training with frozen im-
Maskedautoencodersarescalablevisionlearners. CVPR, ageencodersandlargelanguagemodels. arXivpreprint
2022. arXiv:2301.12597,2023d.
Huang, C., Mees, O., Zeng, A., and Burgard, W. Visual Li,X.,Liu,M.,Zhang,H.,Yu,C.,Xu,J.,Wu,H.,Cheang,
languagemapsforrobotnavigation. In2023IEEEInter- C.,Jing,Y.,Zhang,W.,Liu,H.,etal. Vision-language
nationalConferenceonRoboticsandAutomation(ICRA), foundation models as effective robot imitators. arXiv
pp.10608–10615.IEEE,2023. preprintarXiv:2311.01378,2023e.
Jiang,Y.,Gupta,A.,Zhang,Z.,Wang,G.,Dou,Y.,Chen,Y., Liang, X., Ma, L., Guo, S., Han, J., Xu, H., Ma, S., and
Fei-Fei,L.,Anandkumar,A.,Zhu,Y.,andFan,L. Vima: Liang,X. Mo-vln: Amulti-taskbenchmarkforopen-set
General robot manipulation with multimodal prompts. zero-shotvision-and-languagenavigation. arXivpreprint
arXiv,2022. arXiv:2306.10322,2023.
Katara,P.,Xian,Z.,andFragkiadaki,K. Gen2sim: Scaling Liu, H., Li, C., Wu, Q., andLee, Y.J. Visualinstruction
uprobotlearninginsimulationwithgenerativemodels. tuning,2023.
arXivpreprintarXiv:2310.18308,2023.
Lynch,C.andSermanet,P. Languageconditionedimitation
Keeley,L. Reducingtheriskofventilator-acquiredpneu- learningoverunstructureddata. Robotics: Scienceand
moniathroughheadofbedelevation. Nursingincritical Systems, 2021. URL https://arxiv.org/abs/
care,12(6):287–294,2007. 2005.07648.
Kumar, K. N., Essa, I., and Ha, S. Words into action: Lynch, C., Wahid, A., Tompson, J., Ding, T., Betker, J.,
Learning diverse humanoid robot behaviors using lan- Baruch,R.,Armstrong,T.,andFlorence,P. Interactive
guageguidediterativemotionrefinement. arXivpreprint language: Talkingtorobotsinrealtime. IEEERobotics
arXiv:2310.06226,2023. andAutomationLetters,2023.
11AnInteractiveAgentFoundationModel
Ma,Y.J.,Liang,W.,Wang,G.,Huang,D.-A.,Bastani,O., Shah, R., Mart´ın-Mart´ın, R., and Zhu, Y. Mutex: Learn-
Jayaraman, D., Zhu, Y., Fan, L., and Anandkumar, A. ingunifiedpoliciesfrommultimodaltaskspecifications.
Eureka: Human-levelrewarddesignviacodinglargelan- arXivpreprintarXiv:2309.14320,2023b.
guagemodels. arXivpreprintarXiv:2310.12931,2023.
Srivastava, S., Li, C., Lingelbach, M., Mart´ın-Mart´ın, R.,
Mees,O.,Hermann,L.,Rosete-Beas,E.,andBurgard,W. Xia,F.,Vainio,K.E.,Lian,Z.,Gokmen,C.,Buch,S.,Liu,
Calvin: A benchmark for language-conditioned policy K.,etal. Behavior: Benchmarkforeverydayhousehold
learningforlong-horizonrobotmanipulationtasks. IEEE activitiesinvirtual,interactive,andecologicalenviron-
RoboticsandAutomationLetters,7(3):7327–7334,2022. ments. InConferenceonRobotLearning,pp.477–490.
PMLR,2022.
Min, S., Lyu, X., Holtzman, A., Artetxe, M., Lewis, M.,
Hajishirzi,H.,andZettlemoyer,L. Rethinkingtheroleof Sun, J., Zhang, Q., Duan, Y., Jiang, X., Cheng, C., and
demonstrations: Whatmakesin-contextlearningwork? Xu, R. Prompt, plan, perform: Llm-based humanoid
arXivpreprintarXiv:2202.12837,2022. controlviaquantizedimitationlearning. arXivpreprint
arXiv:2309.11359,2023a.
Parakh, M., Fong, A., Simeonov, A., Gupta, A., Chen,
T., and Agrawal, P. Human-assisted continual robot
Sun, Q., Fang, Y., Wu, L., Wang, X., and Cao, Y. Eva-
learning with foundation models. arXiv preprint
clip: Improvedtrainingtechniquesforclipatscale. arXiv
arXiv:2309.14321,2023.
preprintarXiv:2303.15389,2023b.
Peng, B., Galley, M., He, P., Cheng, H., Xie, Y., Hu, Y.,
Taori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li,
Huang,Q.,Liden,L.,Yu,Z.,Chen,W.,etal. Checkyour
X., Guestrin, C., Liang, P., and Hashimoto, T. B.
facts and try again: Improving large language models
Stanford alpaca: An instruction-following llama
withexternalknowledgeandautomatedfeedback. arXiv
model. https://github.com/tatsu-lab/
preprintarXiv:2302.12813,2023.
stanford_alpaca,2023.
Puig,X.,Undersander,E.,Szot,A.,Cote,M.D.,Yang,T.-
Touvron,H.,Lavril,T.,Izacard,G.,Martinet,X.,Lachaux,
Y.,Partsey,R.,Desai,R.,Clegg,A.W.,Hlavac,M.,Min,
M.-A.,Lacroix,T.,Rozie`re,B.,Goyal,N.,Hambro,E.,
S.Y.,etal. Habitat3.0: Aco-habitatforhumans,avatars
Azhar,F.,etal. Llama:Openandefficientfoundationlan-
androbots. arXivpreprintarXiv:2310.13724,2023.
guagemodels. arXivpreprintarXiv:2302.13971,2023.
Radford,A.,Kim,J.W.,Hallacy,C.,Ramesh,A.,Goh,G.,
Wake, N., Kanehira, A., Sasabuchi, K., Takamatsu, J.,
Agarwal,S.,Sastry,G.,Askell,A.,Mishkin,P.,Clark,J.,
and Ikeuchi, K. Gpt models meet robotic applica-
etal. Learningtransferablevisualmodelsfromnatural
tions: Co-speechgesturingchatsystem. arXivpreprint
language supervision. In International conference on
arXiv:2306.01741,2023a.
machinelearning,pp.8748–8763.PMLR,2021.
Wake,N.,Kanehira,A.,Sasabuchi,K.,Takamatsu,J.,and
Rawte, V., Sheth, A., and Das, A. A survey of hallu-
Ikeuchi,K.Chatgptempoweredlong-steprobotcontrolin
cination in large foundation models. arXiv preprint
variousenvironments: Acaseapplication. IEEEAccess,
arXiv:2309.05922,2023.
11:95060–95078, 2023b. doi: 10.1109/ACCESS.2023.
Reed, S., Zolna, K., Parisotto, E., Colmenarejo, S. G., 3310935.
Novikov,A.,Barth-maron,G.,Gime´nez,M.,Sulsky,Y.,
Kay, J., Springenberg, J. T., et al. A generalist agent. Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N. A.,
TransactionsonMachineLearningResearch,2022. Khashabi,D.,andHajishirzi,H. Self-instruct: Aligning
languagemodelwithselfgeneratedinstructions. arXiv
Sessler,C.N.,Gosnell,M.S.,Grap,M.J.,Brophy,G.M., preprintarXiv:2212.10560,2022.
O’Neal,P.V.,Keane,K.A.,Tesoro,E.P.,andElswick,
R. Therichmondagitation–sedationscale: validityand Xu, M., Huang, P., Yu, W., Liu, S., Zhang, X., Niu, Y.,
reliabilityinadultintensivecareunitpatients. American Zhang,T.,Xia,F.,Tan,J.,andZhao,D. Creativerobot
journal of respiratory and critical care medicine, 166 tool use with large language models. arXiv preprint
(10):1338–1344,2002. arXiv:2310.13065,2023.
Shah,D.,Osin´ski,B.,Levine,S.,etal. Lm-nav: Robotic Yu,W.,Gileadi,N.,Fu,C.,Kirmani,S.,Lee,K.-H.,Are-
navigation with large pre-trained models of language, nas,M.G.,Chiang,H.-T.L.,Erez,T.,Hasenclever,L.,
vision,andaction. InConferenceonRobotLearning,pp. Humplik,J.,etal. Languagetorewardsforroboticskill
492–504.PMLR,2023a. synthesis. arXivpreprintarXiv:2306.08647,2023.
12AnInteractiveAgentFoundationModel
Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M.,
Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V.,
etal.Opt:Openpre-trainedtransformerlanguagemodels.
arXivpreprintarXiv:2205.01068,2022.
Zheng,L.,Chiang,W.-L.,Sheng,Y.,Zhuang,S.,Wu,Z.,
Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E. P., Zhang,
H.,Gonzalez,J.E.,andStoica,I. Judgingllm-as-a-judge
withmt-benchandchatbotarena,2023.
Zhou,G.,Hong,Y.,andWu,Q. Navgpt: Explicitreasoning
in vision-and-language navigation with large language
models. arXivpreprintarXiv:2305.16986,2023.
Zhu, D., Chen, J., Shen, X., Li, X., and Elhoseiny, M.
Minigpt-4: Enhancing vision-language understanding
withadvancedlargelanguagemodels,2023.
13AnInteractiveAgentFoundationModel
Appendix
A.ArchitectureDetails
Toeffectivelyhandleimagesandvideoinputsjointly,weuseadividedspace-timeattentionsimilarto(Bainetal.,2021).
WeinitializeourvisualencoderfromCLIPViT-B16(Radfordetal.,2021),andlearntemporalattentionlayersaftereach
spatialattentionlayer. Wefurthermask75%oftheimagepatches(usingtubeletmaskingforvideos)duringtraining,and
useaMAE-decodersimilarto(Heetal.,2022). Gamingandroboticsuseaframe-levelvisualencodersothattheagentis
abletoobserveacontinuousstreamoftokensandactaftereveryframe. Forhealthcare,weleveragethevideounderstanding
capabilitiesofourvisualencodersincethetasksarevideo-level.
B.GPT-4Prompting
GPT-4 Prompt for Healthcare
System prompt: You are a helpful hospital assistant that will be creating questions and answers for
clinical training.
User prompt: I will give you a caption, a bed angle, and an associated RASS score for a patient. The
caption describes what is happening during the local segment of a video clip (5-10 seconds).
The bed angle describes the position of the bed during the video clip. The RASS score describes the level of
sedation of the patient over a larger 5 minute window.
The RASS score is an integer between -5 and +4. Negative numbers indicate sedation, positive numbers
indicate agitation, and zero indicates a calm, alert patient.
Your task is to create a question and answer pair that is relevant to the caption, the bed angle, and/or the
RASS score. The question should be answerable given the live video feed of the patient. To generate the
question/answer pairs, you must use the caption, bed angle, and RASS score. Please generate your
questions and answers in json format from the RASS score and captions as follows. It is preferable to NOT
ask questions directly related to the bed angle. Do not add any additional text, only the part starting with {
and ending with }.
RASS score: 0 - Alert and calm
Caption: Someone begins to walk into an empty hospital room
Bed angle: > 45°
Response:
{ "question": "What is the person doing?", Corresponding Video
"answer": "The person is walking into the room." (for reference)
}
RASS score: 0 - Alert and calm
Caption: The nurse is bringing the patient into the room
Bed angle: > 45°
Response:
{
"question": "Who is the nurse bringing into the room?",
"answer": "The nurse is bringing a patient into the room."
}
RASS score: 0 - Alert and calm
Caption: The clinician is helping the patient up from the
bed and then helping them walk across the room.
Bed angle: > 45°
Response:
Output:
{
"question": "What is the clinician doing with the patient?",
"answer": "The clinician is helping the patient up from the bed
and assisting them in walking across the room."
}
Figure8.OurPHI-safeGPT-4PromptforGeneratingHealthcareQAExamples.Byensuringtheusageofnon-identifyingvideo
captionsanddocumentationdata,wepreventanyidentifiablepatientdataleakagetoGPT-4whilesimultaneouslygeneratingadditional
visual-languagetrainingdata.Fortheparticularexampleshown,weuseaRASSscoreof“0-Alertandcalm”,acaptionof“Theclinician
ishelpingthepatientupfromthebedandthenhelpingthemwalkacrosstheroom.”,andabedangleof“>45°”.
WeshowourGPT-4PromptforHealthcareVisualQuestionAnsweringgenerationinFigure8,andourGPT-4VPromptfor
gaminginstructiongenerationinFigure9.
C.Pre-trainingLossCurves
WeshowallcomponentsofthelossfunctioninFigure11andplotourcombinedpre-traininglossacross100epochsin
Figure7.
14AnInteractiveAgentFoundationModel
GPT-4-Vision
Prompt:
These are frames of a video of a Bleeding Edge player ordered from
left to right and top to bottom as a grid. Give a simple, but precise
description of what the player is doing in 1 sentence. Be specific
about important items, entities, and actions. In your description do
not mention specific frame numbers or the name of the game.
Video input:
Output:
The player begins by running around the
map, passing through different
checkpoints and interacting with several
capture points, then fights against an
enemy player, and finally captures an
objective while being attacked by another
enemy.
Figure9.OurGPT-4VpromptforgameslikeBleedingEdgethathave3rdpersonviewpointsandvisuallycomplexscenes.Inorderto
inputalargenumberofframes(48)toGPT-4V,weinputtheframesasagridwithframenumbersoverlaidoneachframe(asshown
above).
D.GamingTaskPipeline
WeprovideanexampleofourpipelineforagamingtaskinFigure12. NotethesimilaritiestotheroboticstaskinFigure5
sincebothtasksrequirepredictinganactiongivenatextinstructionandsequenceofprioractions.
E.ExampleOutputs
Weshowexamplesofourmodelpredictingactionsonunseen,roboticssimulationdatainTable5and6. Weshowexample
outputsforhealthcareinTable7,andshowexampleoutputsforgaminginTable8and9.
15AnInteractiveAgentFoundationModel
Figure10.WhenusingGPT-4Vtochooseactionsgivenahistoryofframes,wefindthatitgivesreasonablehigh-levelactionsbutdoes
notchoosepreciselow-levelactions,highlightingtheimportanceofourpre-trainedmodel.
Figure11. Plotofallcomponentsofthetraininglossoverthe100epochsofpre-training.
16AnInteractiveAgentFoundationModel
Figure12.Ourgamingpre-trainingpipeline.Forsimplicity,weusethesamenotationasinSections4.1and4.2;werepresentourtext
instructionasW,inputframesasV ,ourvisualencoderandlinearprojectionlayerasE andℓ,respectively,ouractionandlanguage
t θ
transformermodelasF andthepredictedactionsattimesteptasAˆ .
ϕ t
17AnInteractiveAgentFoundationModel
Textinstruction Startframe Middleframe Endframe
Pull the red moon
apart from the blue → →
moon.
Pushtheyellowstart
→ →
nexttotheredmoon.
Move the red pen-
tagon away from the → →
bluecube.
Movetheredmoonto
thebottomoftheyel- → →
lowpentagon.
Pull the red moon to
→ →
thebottomleft.
Table5.Weshow5uniquedemonstrationsfromLanguageTable,whereourmodelsuccessfullyfollowsthetextinstruction.Inadditionto
thehighlevelinstruction,wealsoshowthelow-levelpredictedactionsofouragentaboveeachframe.
18AnInteractiveAgentFoundationModel
Textinstruction Startframe Middleframe Endframe
Push the handle to
→ →
closethedrawer.
Lift the red block
fromtheslidingcabi- → →
net.
Pull the handle to
→ →
openthedrawer.
Pushtheslidingdoor
→ →
totheleftside.
Pushtheslidingdoor
→ →
totherightside.
Table6.Weshow5uniquedemonstrationsfromCALVIN,whereourmodelsuccessfullyfollowsthetextinstruction.Inadditiontothe
highlevelinstruction,wealsoshowthelow-levelpredictedactionsofouragentaboveeachframe.
19AnInteractiveAgentFoundationModel
Task Startframe Endframe ModelOutput
The patient is awake and
VideoCaptioning calm. Thepatientiscoopera-
tive. Thepatientisalert
Q:Whereisthepatient? A:
Video Question An- patient is in deep sedation.
swering Thepatientlikelyrequiresas-
sistance.
Action Recognition
0-Alertandcalm
(RASS)
The patient is awake and
VideoCaptioning calm. Theyarespeakingon
thephone.
Table7.Weshow4demonstrationsofouragentmodel’soutputsonaheld-outHealthcaredatasetthatusesactorsinsteadofactualpatients.
Wedemonstrateourmodel’soutputsacross3differenttasks:videocaptioning,visualquestionanswering,andRASSscoreprediction
(actionrecognition).Duetothenatureofouractor-collectedexamplevideos,themodelpredictsthatthepatientisawakeandcalm(RASS
scoreof0)formostvideoclips,despiteonly 60%ofthetrainingdatacontainingRASSscoreof0.
20AnInteractiveAgentFoundationModel
Textinstruction Startframe PredictedAction GroundTruthAction
the player is dig-
gingandplacingdirt [STARTACTION] [attack]
[STARTACTION] [attack]
blocks to terraform [CAMERAX0][CAMERAY-1]
[ENDOFACTION]
the terrain around [ENDOFACTION]
theirhouse...
the player is min-
ing underground us- [STARTACTION] [attack] [STARTACTION] [attack]
ing a diamond pick- [CAMERAX-3][CAMERAY0] [CAMERAX-3][CAMERAY0]
axe,gatheringcobble- [ENDOFACTION] [ENDOFACTION]
stone,coal,ironore...
the minecraft player
[STARTACTION] [forward] [STARTACTION] [forward]
is moving around a
[sprint][ENDOFACTION] [sprint][ENDOFACTION]
village...
the player is using a [STARTACTION][sneak][use] [STARTACTION] [sneak]
brewingstand... [ENDOFACTION] [ENDOFACTION]
the player is ... ter-
[STARTACTION] [attack] [STARTACTION] [attack]
raformingbydigging
[ENDOFACTION] [ENDOFACTION]
...
Table8.Weshow5demonstrationsfromaheld-outMinecraftdataset.Inadditiontothehighlevelinstruction,weshowthelow-level
predictedactionsandgroundtruthactions.Wetruncatetheinstructionstoshowonlythepartsrelevanttothecurrentframes.Themost
commonerrorsareslightdifferencesincameramovementsandoccasionallyperformingunnecessaryactions.Notethatsometimesthe
groundtruthvaluesarenottheonlyvalidactions;forinstance,thefourthexamplepredictsthattheplayerwillclickthebottle,which
happensafewframeslaterinthegroundtruthtrajectory.
21AnInteractiveAgentFoundationModel
Textinstruction Startframe PredictedAction GroundTruthAction
the player is using
a character with [STARTACTION] [STARTACTION]
a sword to fight [lockon][meleeattack][lrot214] [lockon][meleeattack][lrot213]
enemies and collect [lmag4][ENDOFACTION] [lmag4][ENDOFACTION]
powercells...
theplayerisridinga
hoverboard-likevehi- [STARTACTION] [STARTACTION]
cle... avoidingorat- [lockon][meleeattack][lrot204] [lockon][meleeattack][lrot201]
tacking enemy play- [lmag4][ENDOFACTION] [lmag4][ENDOFACTION]
ers...
the player starts by
descending some
[STARTACTION] [jump]
stairs towards an [STARTACTION] [jump]
[lockon][specialability1]
openareawherethey [lockon][meleeattack][lrot201]
[lrot199] [lmag4] [ENDOFAC-
engage in combat [lmag4][ENDOFACTION]
TION]
withanenemyplayer
...
theplayer... captures
[STARTACTION] [STARTACTION]
an objective point
[lockon][meleeattack] [lrot63] [lockon][meleeattack] [lrot63]
while fighting off
[lmag4][ENDOFACTION] [lmag4][ENDOFACTION]
multipleopponents...
a bleeding edge
playeriscontrollinga
[STARTACTION] [evade] [STARTACTION] [evade]
robotcharacterwitha
[lrot236] [lmag4] [ENDOFAC- [lrot236] [lmag4] [ENDOFAC-
sword... engagingin
TION] TION]
combat with enemy
players...
Table9.Weshow5uniquedemonstrationsfromaheld-outBleedingEdgedataset.Inadditiontothehighlevelinstruction,weshowthe
low-levelpredictedactionsandgroundtruthactions.Wetruncatetheinstructionstoshowonlythepartsrelevanttothecurrentframes.
Themostcommonerrorsareslightdeviationsfromtheprecisevalueofthejoysticks,whicharenaturallynoisy.Someothererrorsinclude
predictingthewrongtypeofattack,thoughthistypicallyhappensinsituationswheremultipleattacksarestillvalid.
22