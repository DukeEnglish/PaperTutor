On the Convergence of Zeroth-Order Federated Tuning in Large
Language Models
ZhenqingLing1,DaoyuanChen2,LiuyiYao2,YaliangLi2,YingShen1
1SunYat-SenUniversity,2AlibabaGroup
ABSTRACT memorycosts,frequentlysurpassingthepracticalcapabilitiesof
TheconfluenceofFederatedLearning(FL)andLargeLanguage theseclients[36].
Models(LLMs)isusheringinanewerainprivacy-preservingnat- Addressingthischallenge,weturnourattentiontoZeroth-Order
urallanguageprocessing.However,theintensivememoryrequire- Optimization(ZOO),analgorithmthatcomputesgradientapproxi-
mentsforfine-tuningLLMsposesignificantchallenges,especially mationswithoutexplicitgradientinformation,thussignificantly
whendeployingonedgedeviceswithlimitedcomputationalre- reducingmemoryconsumption[23].However,thecombinationof
sources.Tocircumventthis,weexplorethenovelintegrationof ZOOandFL—aresearchdirectionwerefertoasZOO-FL—remains
Memory-efficientZeroth-OrderOptimizationwithinafederated unexploredintheliteratureinthecontextofLLMs.Ourworkin-
setting,asynergywedenoteasFedMeZO.Ourstudyisthefirstto tendstobridgethisgapbyharnessingthememoryefficiencyof
examinethetheoreticalunderpinningsofFedMeZOinthecontext ZOOwithinthecontextoffederatedfine-tuningofLLMs,especially
ofLLMs,tacklingkeyquestionsregardingtheinfluenceoflarge onthefollowingtheoreticalfoundations:
parameterspacesonoptimizationbehavior,theestablishmentof (Q1) How does the vast parameter space of LLMs influence
convergenceproperties,andtheidentificationofcriticalparame- thebehaviorofZOO-FL?(Q2)Canweestablishtheconvergence
tersforconvergencetoinformpersonalizedfederatedstrategies. propertiesofZOO-FLforLLMs?(Q3)Whichmodelparametersare
Ourextensiveempiricalevidencesupportsthetheory,showing criticalforconvergence,andhowcanweleveragethemtooptimize
thatFedMeZOnotonlyconvergesfasterthantraditionalfirst-order FLperformance,suchasviapersonalization?
methodssuchasSGDbutalsosignificantlyreducesGPUmem- Inthispaper,wefocusonincorporatingamemory-efficientZOO
ory usage during training to levels comparable to those during method,MeZO[36]intoFL,asynergywedenoteasFedMeZO,and
inference.Moreover,theproposedpersonalizedFLstrategythat establishesitsconvergencepropertiesunderthelarge-scaleparam-
isbuiltuponthetheoreticalinsightstocustomizetheclient-wise eterspaceofLLMs.Wepresentrefinedconvergenceratesconsider-
learningratecaneffectivelyacceleratelossreduction.Wehope ingtheloweffectiverankofthemodels’Hessianmatrices[2,30].
ourworkcanhelptobridgetheoreticalandpracticalaspectsof Ourempiricalfindingscorroboratethetheoreticalpredictions,vali-
federatedfine-tuningforLLMsandfacilitatefurtherdevelopment datingconvergenceevenwhenscalinguptomodelswithbillions
andresearch. of parameters. In comparative studies with first-order methods
suchasFedAvg,FedMeZOconvergesfastermeanwhileremarkably
reducingGPUmemoryrequirements.
1 INTRODUCTION Moreover,werevealthelearningratetobeavariableofcrucial
importanceforconvergence.Buildingonourtheoreticalinsights,
Federated Learning (FL) has become an important approach in
we further tailor the learning rate to each client’s specific data
modern machine learning, particularly in scenarios where data
characteristics.Ourexperimentalresultsshowamorerapidlossre-
decentralizationandprivacy-preservingarecrucial[25,39,40,50,
ductionwhenimplementingthispersonalizedstrategy,asopposed
52].Centraltothislearningparadigmisthetrainingofacoherent
togenericorrandomlearningrateassignments.
globalmodelthroughtheaggregationofupdatesfrommultiple
Insummary,ourtheoreticalandempiricalexplorationvalidates
clients,facilitatedbyacentralserver,withouttheneedtoshare
FedMeZOinthefine-tuningprocessofLLMs,providingarigorous
rawdata[28,38].
frameworkandpracticalinsightsforfutureapplications.Ourkey
Inparallel,LargeLanguageModels(LLMs)haveradicallyad-
contributionsarethreefold:
vanced the field of natural language processing.[4, 14, 47] The
•WeadvancetheunderstandingofFedMeZOforLLMs,extending
fine-tuningofthesemodels,alreadypre-trainedonvastcorpora,
thetwo-pointgradientestimationtofederatedtuningandestab-
hasproventobeahighlyeffectivestrategyforamultitudeoflan- (cid:16) (cid:17)
lishing theoretical convergence rate as O 𝑟3/2(𝑁𝐻𝑇)−1/2 the
guagetasks,yieldingmodelsthatarebothversatileandcapableof
(cid:16) (cid:17) (cid:16) (cid:17)
adaptingtospecificdomainnarrativesoraligningwithnuanced i.i.d. setting and O 𝑟3/2( (cid:101)𝑐 ℎ𝑁𝐻𝑇)−1/2 −O 𝜎 ℎ2(𝑐 ℎ𝑁)−1 for the
humanfeedback[4,36].
non-i.i.d.setting.
ThetuningofLLMsrequiressuitablealignmentdata,whichare
•WeanalyzetheimpactofvarioushyperparametersofFedMeZO
oftencostlytoacquire[9,13].Duetotheabundanceofprivatedata
andexploreatheory-informedstrategyforpersonalizedlearning
thatremainslargelyisolatedandunderutilized,theintersectionof
rateadjustmentstrategies.
FLandLLMshassparkedincreasinginterestamongresearchers
•ThroughexperimentalvalidationwithLLMs,wedemonstratethat
[17,29,55].Notably,thisintegrationpresentssignificantcompu-
FedMeZOyieldseffectiveconvergencewithsubstantiallyreduced
tationalchallenges,especiallyforclientswithlimitedresources
memoryoverheadcomparedtoSGD.Additionally,weshowexten-
[8,56].ThescalingupofLLMsfurthercompoundsthisissue,asthe
siveempiricalevidencetosupporttheproposedtheoreticalresults.
computationofgradientsforbackpropagationincurssubstantial
4202
beF
8
]GL.sc[
1v62950.2042:viXraOurcodesareanonymouslyreleasedatOurcodesarepubliclyavail- clientsareassumedwithequalimportance[48],andthedatais
ableathttps://github.com/alibaba/FederatedScope/tree/FedMeZO. randomlysampledforefficiency[32].𝐹 𝑖(𝜃,B𝑖)representsthelocal
lossfunctionw.r.taspecificmini-batchB𝑖 drawnfromD𝑖.
2 PRELIMINARIES
Zeroth-OrderOptimization. Zeroth-orderoptimization(ZOO)
2.1 BackgroundandRelatedWorks isaprominenttechniqueinscenarioswheregradientsaredifficultto
FederatedFine-TuningofLargeLanguageModels. LargeLan- obtain,whichestimatesgradientsbyforwardpropagations.Given
guageModels(LLMs)havedemonstratedremarkablecapabilities
arandomvector𝑧andasmoothingconstant𝜇,atypicalone-point
thatenableavarietyofreal-worldapplications[47,58,59].Thefed- gradientestimator[15]isdefinedas:
𝑧
e or nat ae dd afi pn tie n- gtu tn hi en sg eo mf oL dL eM lss th oas dore mce an intl -y spa ett cr ia fic cte td asa kt ste wnt hio iln e, pfo rec su es re vd
-
(cid:101)∇𝐹(𝜃,𝑧,B,𝜇)= 2𝜇(cid:0)𝐹(𝜃+𝜇𝑧,B)−𝐹(𝜃,B)(cid:1), (2)
ingtheprivacyofthetrainingdata.Chenetal.[6]investigated However,Eq.(2)providesabiasedgradientestimation,leadingto
theintegrationofLLMswithinfederatedsettings,highlightingthe acertaindegreeofinformationloss[35].Henceourworkemploys
inherentchallengesandpotentialopportunities.Zhangetal.[56] theZOOparadigmwithatwo-pointgradientestimatorproposed
furtheredthisresearchbyexamininginstructiontuningofLLMs by[36]inafederatedsetting:
inafederatedcontext,markingprogressinapplyingFLtothespe-
Definition 2.1. (Two-point gradient estimator) Given a set of
cializedtrainingofLLMs.NotableframeworkssuchasFATE-LLM
byFanetal.[18]andFederatedScope-LLMbyKuangetal.[29]
parameters𝜃 ∈R𝑑 foranLLMandamini-batchB𝑖,thetwo-point
zeroth-ordergradientestimatorisformulatedas:
offerindustrial-gradeandcomprehensivesolutionsforfederated
𝑧
fi On rde- et ru Oni pn tg im.O izu ar tiw onor (k Z, Oin Oc )o wnt ir ta hs Ft, Lin fove rs tt hig ea fite ns et -h tue nf iu ns gio on fo Lf LZ Mer so ,t ah n- (cid:101)∇𝐹 𝑖(𝜃,𝑧 𝑖,B𝑖,𝜇)= 2𝜇𝑖 (cid:0)𝐹 𝑖(𝜃+𝜇𝑧 𝑖,B𝑖)−𝐹 𝑖(𝜃−𝜇𝑧 𝑖,B𝑖)(cid:1), (3)
areathathasyettobefullyinvestigated,therebyaddressingagap where𝑧 𝑖 ∼ N(0,𝐼 𝑑) isaGaussianrandomvariableand𝜇 isthe
intheliteratureandprovidingfundamentaltheoreticalinsights. perturbation scale. The two-point gradient estimator in Eq. (3)
requiresonlytwoforwardpassesthroughthemodeltocompute
Zeroth-OrderOptimizationinFederatedLearning. ZOOhas
the estimation of gradient, which serves as a memory-efficient
emergedasaviablemethodtoaddressthedifficultiesofcomputing
alternativetobackpropagation(BP).
gradients in FL, especially in settings limited by computational
resources.Zhangetal.[57]proposedaZOOalgorithmtailoredfor TheFedMeZOAlgorithm. Inthispaper,westudyandana-
verticalFL,focusingonprivacypreservation.Yietal.[54]andLi lyzetheproprietiesofapracticalsynergyofMeZO[36]andFe-
etal.[31]studiedZOO-FLalgorithms,withdiscussionsonconver- dAvg [38], which is designed to fine-tune LLMs in an efficient,
gencepropertieswithsingle-pointperturbationandlocalupdates privacy-preservingandpersonalizedmanner.WetermthisZOO-FL
indecentralizedFL,respectively.Theconvergenceanalysisisa approachasFedMeZO,depictedwithfollowingprocesses:
criticalaspectofFL,asillustratedbyLietal.[34]fortheFedAvg Inasinglecommunicationround,thecentralserverfirstbroad-
algorithmandfurtherdevelopedbyFangetal.[19]formini-batch caststheglobalmodelparameterstoavailableclients.Oncethe
stochasticZOO-FLinwirelessnetworks.Moreover,Shuetal.[45] clientshavecompletedtheirlocalupdatesanduploadedtheirmod-
proposedenhancementstoqueryefficiencyforZOOwithintheFL els,theserveraggregatestheupdatesaccordingtoEquation(1),
framework.Ourresearchsetsitselfapartbyformulatingtheoret- formingthebasisforthesubsequentround.
icalconvergenceboundsforZOO-FL,specificallytailoredtothe Uponreceivingtheglobalmodelparameters,clientsperformthe
large-scaleparameterspaceofLLMs.Thisbuildsonthepreliminary followingsteps,distinguishingFedMeZOfromtraditionalBP-based
workbyMalladietal.[36],whichconfirmedthefeasibilityofZOO FedAvgalgorithmsintwofold:
forLLMsinacentralizedsetting.
(1)TrainingMemoryReduction:Clientsupdatetheirmodelsusing
thetwo-pointZOOgradientestimatordefinedinEquation(3)as:
2.2 ProblemFormulation 𝑒 𝑖(𝑡,𝑘) =(cid:101)∇𝐹 𝑖(𝜃 𝑖(𝑡,𝑘),𝑧 𝑖(𝑡,𝑘),B 𝑖(𝑡,𝑘),𝜇), (4)
Wesummarizethefulllistofnotationsandpresentdetailedproof
where(𝑡,𝑘)denotesthe𝑘𝑡ℎ iterationwithinthe𝑡𝑡ℎ
communication
ofallthetheoreticalresultsintheAppendix.
round.UnlikestandardZO-SGDalgorithmsthatrequirestoring
FederatedLearning. WeconsiderthegeneralFLsettingasof theperturbationvector𝑧 ateachiteration,FedMeZOresamples
FedAvg[38],withacentralserverandacollectionof𝑁 clients, 𝑧usingrandomseedsinin-placeimplementation,thusreducing
indexedby1,2,...,𝑁.Thecentralservercoordinatesthetrainingofa
memoryusagetoalevelequivalenttoinference[36].
globalmodelthroughthecollaborativeeffortsoftheseclients,each (2)CommunicationCostReduction:Tomitigatethehighcom-
holdinglocaldatasamplesdrawnfromtheirrespectivedistributions municationoverheadassociatedwithLLMs,FedMeZOleverages
D𝑖.Theoptimizationproblemcanbeformulatedas:
Low-RankAdaptation(LoRA)[26],whichintroducesreparametriza-
𝑁
𝜃m ∈i Rn 𝑑𝑓(𝜃)=△ 𝑁1 ∑︁ 𝑖=1𝑓 𝑖(𝜃 𝑖), 𝑓 𝑖(𝜃)=△E B𝑖∼D𝑖(cid:2)𝐹 𝑖(𝜃,B𝑖)(cid:3), (1) t
t
tri ho aen inwt eo
h
dot Lu
l
Len Me LLt pw
M
oo
sw
ss em
e si
sa gl ahl
t
ld ose
,
wl bta
a
“ism
ne
tda rt ior ni
n
sx icto hn
de
it mah
s
ee
s
nul si mn ioe
p
na
t
”r
io
wla
n
hy
t
ee
h
nr as ati dn
w
as pete
l
tla ed dpro
te
of
-
where𝜃 ∈ R𝑑 denotesthe𝑑-dimensionparameterofmodel,and newtasks.Introducingitcanhelpusfurtherreducethenumberof
𝑓(𝜃)and𝑓 𝑖(𝜃)denotethegloballossfunctiononthecentralserver parameterstobeupdatedanduploaded,therebyaligningwiththe
andlocallossfunctionon𝑖𝑡ℎ
client,respectively.Typically,the practicalconstraintsoffederatedsettings.
22.3 LemmasandAssumptions Theorem3.1. (StepwiseLossDescentini.i.d.Setting)Under
Lemma2.2. (UnbiasedGradientEstimator)Thetwo-pointzeroth- Assumptions1-5andwithalearningrate𝜂satisfying
ordergradientestimatordescribedinEq.(3)isanunbiasedestimator (cid:110) 1 𝑁 1 (cid:111)
ofthetruegradient,thatis, 𝜂 ≤min 3𝐻𝐿√︁𝑐 𝑔𝑑, 3𝐻𝐿𝑐 𝑔, 𝐻2 , (7)
E[(cid:101)∇𝐹 𝑖(𝜃,𝑧 𝑖,B𝑖,𝜇)] =∇𝑓 𝑖(𝜃). (5)
theexpecteddecreaseinlossateachstepforFedMeZOunderthei.i.d.
TheHessianmatrix,whichisthesquarematrixofsecond-order scenarioisboundedas
p iza er sti ta hl ede cr ui rv va ati tv ue rs eo of ft th he elo loss ssw s. ur. rt ft ah ce em [2o 2d ]e .l Ap la thra om ue gt her ts h, ech sia zr eac ot fer a-
E 𝑡 (cid:2)𝑓(𝜃𝑡+1)(cid:3) ≤ 𝑓(𝜃𝑡
)−(cid:18)
2 −
2𝜁(cid:19)
𝜂E 𝑡(cid:12) (cid:12)∇𝑓(𝜃𝑡 )(cid:12) (cid:12)2
𝛾 𝑑
model’slossHessianisoftenassociatedwiththerateoffine-tuning,
studies suggest that the large-scale parameters of LLMs do not 2𝜎 𝑔2𝜁𝜂𝐿 𝜁𝜂𝜇2𝐿3
+ + , (8)
necessarilyimpedeconvergence[1,27].Thisparadoxisaddressed 𝑁𝐻𝑑 2𝑁𝐻
byrecognizingthatthelossHessianoftenexhibitsasmalllocal where𝛾 and𝜁 quantifytheeffectivelow-rankpropertiesofthegradi-
effectiverank[36],whichwecaptureinthefollowingassumption: entanditsestimator,respectively.
As •su ∇m 2𝑓p (t 𝜃io )n ⪯1 H. T (𝜃h 𝑡e )re foe rx ais lt l𝜃a sH ue cs hsi ta hn atm ∥a 𝜃tr −ix 𝜃𝑡H ∥( ≤𝜃𝑡 𝜂) 𝑑s 𝐺at (i 𝜃s 𝑡fy ),in wg h:
ere
InTheorem3.1,theterm(−2𝜂/𝛾)E 𝑡|∇𝑓(𝜃𝑡)|2servesasacritical
𝐺(𝜃 •𝑡 T) he= em ffea cx tB iv∼ eD ra∥ n∇ k𝑓 o( f𝜃 H𝑡,B (𝜃) 𝑡∥ ).
,denotedastr(H(𝜃𝑡))/∥H(𝜃𝑡)∥op,
f na ec gt ao tr ivth ea ct od nr ti rv ibe us tt oh re inde Ec qre .a (8s )e si un ct hhe thl ao ts Es 𝑡fu (cid:2)n 𝑓c (𝜃ti 𝑡o +n 1, )a −si 𝑓t (i 𝜃s 𝑡t )h (cid:1)e (cid:3)s ≤ol 0e
.
isatmost𝑟.Heretrdenotesthetraceofthematrix,and∥·∥opdenotes Notethatthepresenceofthefactor𝛾−1 = Θ(𝑟−1) underscores
theoperatornorm. the impact of the low effective rank𝑟 on the convergence rate
(underAssumption1),revealingthatareductionin𝑟canaccelerate
Assumption1characterizesaloweffectiverank𝑟 intheHessian convergence independently of the high-dimensional parameter
matrix,whichdemonstratesthatLLMfine-tuningcanoccurina space𝑑.Consequently,evenforLLMswithexpansiveparameter
lowdimensionalsubspace(≤ 200parameters)[2,30].Withthis spaces,FedMeZOcanattainconvergence.Thisaddressesourfirst
insight,[36]identifiedtheboundoflossdescentateachstepof foundationalquestionQ1:“Howdoesthevastparameterspaceof
centralizedZOO,whichispartiallyinfluencedby𝑟: LLMsinfluencethebehaviorofFedMeZO”.
Lemma2.3. (BoundedCentralizedDescent)Assume𝑓(𝜃)is𝐿- Besides,theterms(2𝜂𝜁/𝑑)E 𝑡(cid:13) (cid:13)∇𝑓(𝜃𝑡)(cid:13) (cid:13)2 and(2𝜎 𝑔2𝜁𝜂𝐿/𝑁𝐻𝑑)both
smoothandlet (cid:101)∇𝐹(𝜃,𝑧,B,𝜇)betheunbiasedzeroth-ordergradient arescaledby𝜁/𝑑,i.e.,inΘ(1/𝑟𝑑2),contributingarelativelysmaller
estimatorfromEquation(3).IftheHessianmatrixH(𝜃)exhibitsa effectontheconvergencespeedcomparedtonegativeterm.This
localeffectiverankof𝑟,andconstants𝛾 =Θ(𝑟/𝑛)and𝜁 =Θ(1/𝑟𝑑) demonstratesthattheinfluenceonconvergencespeedfromthe
exist,thentheexpecteddecreaseinlosscanbeboundedasfollows: zeroth-ordergradientestimationismoderatedbythemodel’seffec-
E(cid:2)𝑓(𝜃𝑡+1)(cid:3) ≤ 𝑓(𝜃𝑡 )− 1 ·𝜂∥∇𝑓(𝜃𝑡 )∥2 tivelowrankanddimensionality.Asforthelastterm,(𝜁𝜂𝜇2𝐿3/2𝑁𝐻),
𝛾 itactsasafactorslowingdowntheconvergencerate,andwecan
+1 𝜂2𝐿·𝜁 ·E(cid:2) ∥(cid:101)∇𝐹(𝜃,𝑧,B,𝜇)∥2(cid:3) (6) observethatwhen𝑁 and𝐻 arelarger,thistermbecomessmaller.
2 Thissuggeststhattheeffectofslowingdowntheconvergencerate
𝑑𝑟+𝑑−2 (𝑑+2)𝑛2 isnotaspronounced,andsimultaneously,theperturbationstep𝜇
where 𝛾 = ,𝜁 =
𝑛(𝑑+2) (𝑑𝑟+𝑑−2)(𝑑+𝑛−1) shouldnotbeexcessivelylarge.Specifically,thistermindicatesthat
increasingthenumberofclientsandthenumberoflocalrounds
where𝑛denotesthenumberofrandomizations.
canenhanceconvergence,whilealsoemphasizingtheimportance
FromEq.(6),weobservethattherateofdescentatasinglestep ofkeepingtheperturbationstep𝜇moderate.
dependsonthegradientrelatedto𝛾 andthegradientestimation Aftergainingintuitiveinsightsineachroundoftrainingthrough
relatedto𝜁.Following[36],weset𝑛to1inthispaper. theanalysisofTheorem3.1,itisnecessarytoassesstheconvergence
Besides,tofacilitatetheanalysisinFLsetting,weintroducefour performanceofFedMeZOfromaglobalperspective.Weutilizethe
assumptions,includingBoundedLoss(Assumption2),𝐿-smoothness squaredmagnitudeofthegradientE 𝑡∥∇𝑓(𝜃𝑡)∥2asameasureto
(Assumption3),mini-batchgradienterrorbound (Assumption4, assessthesuboptimalityofeachiterate.Therapiditywithwhich
global-localdisparitiesini.i.d.andnon-i.i.d.settings(Assumptions5
the algorithmapproaches astationary pointserves asa crucial
and6respectively).Theseassumptionsarestandardandfounda- metricfordeterminingitsefficacyinthecontextofnon-convex
tionalinoptimizationandFLliterature[3,33,34,49],whichwe optimizationproblems[41].
detailinAppendixB.
Corollary3.2. (GlobalConvergenceini.i.d.Setting)Assum-
3 MAINRESULTS ingtheconditionsofTheorem3.1hold,theglobalconvergencefor
FedMeZOinthei.i.d.case,characterizedbyΓ= 𝑑−𝜁𝛾,isgivenby
3.1 ConvergenceAnalysisini.i.d.Case 𝑑𝛾
I Mn et Zh Oiss wu ib thse incti to hn e, iw .i.e d.e dx aa tm ai dn ie stt rh ie buc to in ov ne sr eg te tn inc ge .p Wro ep ee sr tt aie bs lio sf hF te hd e- 𝑡m ∈[i 𝑇n ]E 𝑡(cid:13) (cid:13) (cid:13)∇𝑓(𝜃𝑡 )(cid:13) (cid:13) (cid:13)2 ≤ 𝑓(𝜃 20 𝜂) 𝑇− Γ𝑓∗ + 𝑁𝜎 𝑔 𝐻2𝜁 𝑑𝐿
Γ
+ 4𝜁 𝑁𝜇2 𝐻𝐿 Γ3 (9)
conditionsunderwhichthealgorithmguaranteeslossreductionat
eachiterationandprovideaglobalconvergencerate. where𝑓∗denotestheoptimallossvalue.
3Theupperboundontheminimumsquaredgradientnormacross However,i.i.d.dataistypicallyencounteredinidealizedenvi-
iterationsiscomposedofthreetermsinCorollary3.2.Thefirstterm ronments.Inreal-worldapplications,non-i.i.d.conditionsaremore
indicatesthatthedistancetotheoptimallossreliesontheinitial commonandchallenging.Next,wefurtherdiscussandanalyzethe
stateofoptimality,whilethesecondandthirdtermselucidatethe convergenceofFedMeZOundernon-i.i.d.settings.
influencesofstochasticmini-batcherrorsandtheperturbationscale
𝜇inherenttoZOO,respectively.Specifically,theybothreflectthe 3.2 ConvergenceAnalysisinnon-i.i.d.Case
impactofthemodelparameters𝑑andtheloweffectiverank𝑟 on Analyzingconvergenceinthecontextofnon-i.i.d.datadistributions
theoptimalloss.Aspointedoutin[41],bychoosinganappropriate iscrucialforunderstandingthebehaviorofFLalgorithmsinreal-
stepsize,wecanobtainthedesiredaccuracy. worldscenarios.Inthissection,weextendourconvergenceanalysis
Giventhat𝛾 = Θ(𝑟) and𝜁 = Θ(cid:16) 1 (cid:17) ,wehave𝜁𝛾 = Θ(cid:16) 𝑟 (cid:17) = tothecasewhereclientdatadistributionsareheterogeneous.
𝑟𝑑 𝑟𝑑
(cid:16) (cid:17) (cid:16) (cid:17)
Θ 1 .Astheparameter𝑑 islarge,𝑑 −𝜁𝛾 = 𝑑 −Θ 1 isdomi- Theorem3.4. (StepwiseLossDescentinnon-i.i.d.Setting)Let
𝑑 𝑑
natedby𝑑.Consequently,thedominanttermofΓis 𝑑𝛾 ,which Assumptions1-4andAssumption6holdandlearningrate𝜂satisfy
simplifiesto𝛾 =Θ(𝑟).Therefore,Γ=Θ(cid:16) 𝛾1(cid:17) =Θ(cid:16) 𝑟1(cid:17) .𝑑 B− u𝜁 i𝛾 ldingon 𝜂 ≤min(cid:110) 3𝐻𝐿1 √︁𝑐 𝑔𝑑, 3𝐻𝑁 𝐿𝑐 𝑔, 𝐻1 2(cid:111) . (11)
thisrelationship,wehavethefollowingcorollarythatarticulates
Then,theexpectedlossateachstepforFedMeZOinthenon-i.i.d.
theconvergencerateofFedMeZO.
settingisboundedas
C coo nr do itl il oa nr sy o3 f. C3 o. ro(C llao rn yv 3er .2g hen olc de aR na dt ge ii vn eni. 𝜂i.d =.S (𝑁ett 𝐻in )1g /) 2A (𝑟s 𝑇su )m −1i /n 2g at nh de E 𝑡 (cid:2)𝑓(𝜃𝑡+1)(cid:3) ≤ 𝑓(𝜃𝑡 )−(cid:18) 𝛾2
𝑁
− 2𝜁 𝑑(cid:101)𝑐 ℎ(cid:19) 𝜂E 𝑡(cid:13) (cid:13)∇𝑓(𝜃𝑡 )(cid:13) (cid:13)2
𝜇=(𝑁𝐻)1/4𝑟−1/2,wehave
2𝜎2𝜁𝐿𝜂 𝜁𝜂𝜇2𝐿3 2
+ (cid:101) + − 𝜂𝜎2, (12)
𝑡m ∈[i 𝑇n ]E 𝑡(cid:13) (cid:13) (cid:13)∇𝑓(𝜃𝑡 )(cid:13) (cid:13) (cid:13)2 ≤O(cid:16) 𝑟3/2(𝑁𝐻𝑇)−1/2(cid:17) +O(cid:16) 𝑑−1(𝑟𝑁𝐻)−1/2(cid:17) .
where (cid:101)𝑐
ℎ
=𝑐 ℎ+𝑁 and
(cid:101)𝜎2𝑁 =𝐻 3𝑑
𝑐 𝑔𝜎
ℎ2+2 𝜎𝑁 𝑔2.𝐻 𝛾𝑁 ℎ
(10)
ComparingEq.(12)withitsi.i.d.counterpartEq.(8),thenon-i.i.d.
Theexpressionontheright-handsideofEq.(10)isdominated settingintroducesadditionaltermsreflectingdataheterogeneity.
byO(cid:16) 𝑟3/2(𝑁𝐻𝑇)−1/2(cid:17) .Consequently,wehavederivedthecon- Firstly,anadditionalterm (cid:101)𝑐 ℎappearsbeforeE 𝑡∥∇𝑓(𝜃𝑡)∥2;secondly,
vergencerateforFedMeZO.Theloweffectiverank𝑟 significantly theoriginal𝜎 𝑔2 changeinto (cid:101)𝜎2;thirdly,anewtermrelatedto𝜎 ℎ2
contributestoloweringtheconvergencerate,whichisalsoinflu- isaddedattheend.Theterm (cid:101)𝑐 ℎ amplifiestheeffectofthegradi-
encedbythenumberofclients𝑁,thenumberoflocaliterations entnorm,while (cid:101)𝜎2 encapsulatesboththeintrinsicstochasticity
𝐻,andthetotalnumberofcommunicationrounds𝑇.Moreover, anddataheterogeneity.Thepresenceof𝜎 ℎ2 indicatestheimpact
tosatisfythelearningrateconditioninEq.(7),thevaluesof𝑁,𝐻, ofclientdatadivergenceontheconvergencebehavior,inadegree
and𝑇 mustbesuitablylarge.
dependentonΘ(1/𝑟𝑑2).Giventhatthecontributionofthenegative
termacceleratestherateofdeclineineachround,itcanbecon-
ItisimportanttonotethatFedMeZOdoesnotprimarilyaimto
cludedthatheterogeneityispositivelycorrelatedwithconvergence.
accelerateconvergencespeedbutrathertoidentifytheconvergence
Consideringalltheabovechanges,appropriateheterogeneitycan
rate under assumptions pertinent to LLMs. This is intended to
aidinthemodelconvergence.
demonstratethatFedMeZOcanachieveconvergenceevenwithin
ExperimentalresultsinSection??confirmthatamorerandom-
avastparameterspace.InaseriesofstudiesonfederatedZOO,
izeddatasetdistributionleadstoimprovedconvergence,supporting
FederatedZeroth-OrderOptimization(FedZO)presentsthemost
ourtheoreticalinsights.Next,wepresenttheglobalconvergence
comprehensiveandcompleteanalysiswithaconvergencerateof
O(cid:16)√︁𝑑/(𝑁𝐻𝑇𝑏
1𝑏
2)(cid:17)
[19],whichexhibitsalowerratecompared
resultforthenon-i.i.d.settingbuildinguponTheorem3.4.
to O(cid:0)𝑑3/𝑇(cid:1) of ZONE-S [24] and accounts for the impact of 𝐻 Corollary3.5. (GlobalConvergenceinNon-i.i.d.Setting)As-
comparedtoO(cid:16)√︁𝑑/𝑁𝑇(cid:17)
ofDZOPA[54].IncontrasttoFedZO,our
sumingtheconditionsofTheorem3.4hold,denote (cid:101)Γ= 𝑑 𝑑− 𝛾𝑁 𝑁𝛾𝜁,Fed-
MeZOsatisfies:
method,FedMeZO,theoreticallysupportsafasterconvergenceby
replacing𝑑1/2with𝑟3/2andsetting𝑏 1=𝑏 2=1.Thesecomparisons min E 𝑡(cid:13) (cid:13)∇𝑓(𝜃𝑡 )(cid:13) (cid:13)2 ≤ 𝑓(𝜃0)−𝑓∗ + (cid:101)𝜎2𝜁𝐿
showthatFedMeZOaddressesthechallengesposedbylargemodels 𝑡∈[𝑇] 2(cid:101)Γ (cid:101)𝑐 ℎ𝜂𝑇 (cid:101)Γ (cid:101)𝑐 ℎ𝑁𝐻𝑑
andtheirnumerousparameters,offeringanefficientconvergence 𝜁𝜇2𝐿3 𝜎2
ratethatreliesontheloweffectiverank. + − ℎ . (13)
This advancement signifies progress in optimizing federated
4(cid:101)Γ (cid:101)𝑐 ℎ𝑁𝐻 (cid:101)Γ (cid:101)𝑐 ℎ𝛾𝑁
learningalgorithms,particularlyforLLMs,wherethescalability (cid:16) (cid:17)
Given𝛾 = Θ(𝑟) and𝜁 = Θ 1 ,theexpression𝑑 −𝑁𝜁𝛾 sim-
of parameters and data heterogeneity are major challenges. By 𝑟𝑑
emphasizingtheloweffectiverank,ourapproachenhancesboth plifies to𝑑
−Θ(cid:16)𝑁
𝑑
(cid:17)
which is dominated by𝑑. Consequently,(cid:101)Γ
thetheoreticalunderstandingofconvergencebehaviorincomplex (cid:16) (cid:17)
settingsandtheguidanceinsightsintothesettingsoflearningrates simplifiestoΘ 𝑟1 asinthei.i.d.case.ComparedtoCorollary3.2,
andotherparameterstoachieveefficientconvergenceoutcomes. Corollary3.5introducestwochanges:first,alltermsontheright
4sideoftheinequalityincludeadenominator𝑐 𝑔,andsecond,thereis thannon-personalizedFL:
wan ita hd Θdi (t 𝜎io ℎ2n /a 𝑐l ℎt 𝑁er )m .Ta hs is soc fuia rt te hd erw dit eh mn oo nn s- ti r. ai. td e. sh te hte er co og nen ste ri aty in, is nc gale ed
f-
𝜂 𝑖 =𝜂 0(1+𝛼·Φ 𝑖), (15)
fectofdataheterogeneityinFedMeZO.SimilartoCorollary3.3, where𝜂 0 represents a default learning rate applicable in a i.i.d.
bysettingappropriatevaluesfor𝜂and𝜇,weobtainthefollowing setting,𝛼 isascalingfactorthatdeterminesthesensitivityofthe
convergencerate. learningrateandΦ 𝑖 istheheterogeneityindex,representingthe
extentof𝑐 𝑔and𝜎 ℎ2.Thispropositionunderscorestheimportance
Corollary3.6. (ConvergenceRateinNon-i.i.d.Setting)Assum- ofconsideringdataheterogeneityinthedesignofthelearningrate
ingtheconditionsofCorollary3.5hold,with𝜂 =(𝑁𝐻)1/2(𝑟 (cid:101)𝑐 ℎ𝑇)−1/2
strategy withinpFL, offeringa structuredapproachto enhance
and𝜇=( (cid:101)𝑐ℎ𝑁𝐻)1/4𝑟−1/2,FedMeZOhasconvergencerateasfollows: learningoutcomesacrossdiverseclientdatasets.
InSection5.4,weempiricallyconfirmthataparticularimplemen-
𝑡m ∈[i 𝑇n ]E 𝑡(cid:13) (cid:13) (cid:13)∇𝑓(𝜃𝑡 )(cid:13) (cid:13) (cid:13)2 ≤O(cid:16) 𝑟3/2( (cid:101)𝑐 ℎ𝑁𝐻𝑇)−1/2(cid:17) t ta ot nio on teo tf ht ah tis thst era dt ae tg ay hf ea tc ei rl oit ga ete ns eif ta yst ie nr dc eo xn Φv 𝑖er cg ae nn nc oe t. bIt eis deim tep rmor it na en dt
(cid:16) (cid:17) (cid:16) (cid:17)
+O 𝑑−1(𝑟 (cid:101)𝑐 ℎ𝑁𝐻)−1/2 −O 𝜎 ℎ2(𝑐 ℎ𝑁)−1 . (14) apriori;therefore,weutilizeseveralproxymeasuresduringthe
trainingprocesstoestimateit.Ourgoalisnottoprescribeanexact
solutiontothisstrategy,butrather,throughanalysisandempirical
TheconvergencerateinEq.(14)isprimarilydrivenbytheterm
(cid:16) (cid:17) investigation,toenlightenfurtherresearchanddevelopmentof
O 𝑟3/2( (cid:101)𝑐 ℎ𝑁𝐻𝑇)−1/2 ,indicatingthatoptimizingthebalancebe-
personalizedFedMeZOformoreeffectivetrainingofLLMs.
tween (cid:101)𝑐 ℎ,𝑐 ℎ,and𝑁 iscrucial,whichreflectsacomplexinterplay Thediscussionandcorrespondingempiricalsupportaddress
ofheterogeneity.Specifically,weobservethattoachievebetter theQ3:"Whichmodelparametersarecriticalforconvergence,and
convergence,asmallerO(cid:16) 𝑟3 2( (cid:101)𝑐 ℎ𝑁𝐻𝑇)−1 2(cid:17) ispreferredwhilethe howcanweleveragethemtooptimizeFLperformance,suchasvia
(cid:16) (cid:17) personalization?".
O 𝜎 ℎ2(𝑐 ℎ𝑁)−1 term need to increase at the same time. Conse-
Besides,recallthatweadoptLoRAtomitigatethecommuni-
quently,thebalancebetween𝑐 𝑔,𝑐 ℎ,and𝑁 becomesadynamic cation burden associated with LLMs for practical FL scenarios.
trade-offprocess,i.e.,theheterogeneityamongdifferentclients
Nonetheless,theinfluenceofLoRAonthemodel’sloweffective
directlyinfluencestheoverallconvergenceperformance. rank,especiallywhenamalgamatedwiththeMeZOmethod,re-
Fornow,wehaveansweredthequestion(Q2)“Canweestablish
mainsanopenquestion.Wethusadvancethefollowingconjecture,
theconvergencepropertiesofZOO-FLforLLMs?”viatheoremsand
predicatedonexistingliterature[43,53],tofacilitatefurthervalida-
corollariesmentionedinthissection.Wealsovalidatedthenatureof tions:
convergenceunderdifferentscenariosandtasksthroughempirical
experimentsinSection5.2. Conjecture3.8(RankCorrelation). Theoptimalreparametriza-
tionrank𝑟 usedinLow-RankAdaptation(LoRA)ispositively
LoRA
proportionaltotheeffectiverank𝑟 oftheHessianmatrixH(𝜃𝑡)of
3.3 Implications
thetunedLLM.The𝑟 islower-boundedby𝑟,andcanserveasan
LoRA
Theaforementionedtheoreticalresultsoffernumerousinsightsinto empiricalproxyfor𝑟.
parametertuning.Acriticalrevelationfromouranalysispertains
totheconstraintsimposedonthelearningrate,asdelineatedin 4 PROOFOUTLINE
Eq.(7)andEq.(11),whichsuggeststhatanoptimallearningrate
√ Thissectionprovidesanoutlineofthederivationspresentedin
magnitudeisanchoredat1/ 𝑑.Largerlearningratesarenotonly
Section3,emphasizingthekeyanalyticaltechniquesandconcepts
ineffectualbutalsoposeariskofdestabilizingthetrainingdynamic.
employed.DetailedproofsareavailableinAppendixD.
IntheAppendixF.6,ourempiricalexperimentscorroboratethis
WebeginbytakingexpectationsonbothsidesofEq.(6),consid-
hypothesis,demonstratingthatexcessivelearningratesprecipitate
eringafederatedlearningsetting.Theequationissplitintotwo
abruptincreasesinloss.
mainpartsforfurtheranalysis:
Furthermore,ourinsightsregardingthelearningrateopenup
prospects for personalized FL, a compelling approach that uses
𝑁
client-specificconfigurationstoaddressheterogeneityandhasat- E 𝑡(cid:2)𝑓(𝜃𝑡+1)(cid:3) ≤ 𝑓(𝜃𝑡 )− 1 ·𝜂E 𝑡(cid:13) (cid:13)1 ∑︁ ∇𝑓 𝑖(𝜃𝑡 )(cid:13) (cid:13)2
tractedincreasinginterest[7,10,16,37,44].Specifically,weinves- 𝛾 (cid:13)𝑁 (cid:13)
𝑖=1
tigatetheory-guidedpersonalizedstrategiesbydynamicallyadjust-
𝑁 𝐻
i on fg dath tae hle ea tern roin gg enr ea it te y𝜂 a𝑖 mi on np gr co lp ieo nrt ti so .n Into liga hq tu oa fn Tt hifi ea ob rele mm 3e .4as tu hr ae
t
+ 21 𝜂2𝐿·𝜁 ·E 𝑡(cid:13) (cid:13) (cid:13)𝑁1 ∑︁∑︁ 𝑒 𝑖(𝑡,𝑘)(cid:13) (cid:13) (cid:13)2 . (16)
𝑖=1𝑘=1
alargerheterogeneityismoreconducivetomodelconvergence,
henceitisfeasibletoappropriatelyincreasethelearningrateto
Forsimplicity,wedenotethetwoexpectationtermsas𝑇 1and𝑇 2,
whichpertaintotheexpectedsquarednormsofthegradientand
allowthisclienttocontributemoretotheoverallconvergence,we
thezeroth-ordergradientestimator,respectively.
proposethefollowingtailoredadjustmentstrategy:
Proposition3.7. (AdaptiveLearningRateAdjustment)LetAs- 4.1 ProofofTheorem3.1
sumption6holds,thelearningrate𝜂 𝑖 canbeadjustedaccordingto Forterm𝑇 1inEq.(16),weutilizetheCauchy-Schwarzinequality
theformulatobetteraccommodatethevariedlearninglandscapes ∥𝑎+𝑏∥2 ≤ 2∥𝑎∥2 +2∥𝑏∥2 todecomposeitintotwoparts,with
5the first representing the discrepancy between local and global In Eq. (21), E 𝑡(cid:13) (cid:13) (cid:13)𝜃 𝑖(𝑡,𝑘) −𝜃𝑡(cid:13) (cid:13) (cid:13)2 remains unknown and we need
gradients.ByinvokingAssumption5,weestablish:
toconstrainitfurther.Thekeyideaistotransformthisexpecta-
𝑇
1
≤ 𝑁2 ∑︁𝑁 E 𝑡(cid:13) (cid:13) (cid:13)∇𝑓 𝑖(𝜃𝑡 )−∇𝑓(𝜃𝑡 )(cid:13) (cid:13) (cid:13)2 +2E 𝑡(cid:13) (cid:13)∇𝑓(𝜃𝑡 )(cid:13) (cid:13)2. t ci oo nn clt uer sm ionin ot fo Ea q.fo (1rm 8)are nl dat Ee qd .t (o 19E )𝑘 𝑡 fo∥𝑒 r𝑖( c𝑡, o𝑘 m) ∥ p2 ua tan td iot nh .e Tn hu eti dli ez te ait lh ede
𝑖=1
derivationprocessisprovidedintheAppendixD.1andwecanhave
Forterm𝑇 2,Jensen’sinequalityallowsustoboundtheexpected theboundedresult:
squarednormofthezeroth-ordergradientestimatorasfollows: 𝑁 𝐻
𝑇 2 ≤
𝑁1
∑︁𝑁 ∑︁𝐻
E
𝑡(cid:13)
(cid:13) (cid:13)𝑒
𝑖(𝑡,𝑘)(cid:13)
(cid:13)
(cid:13)2
. (17)
𝑁1 ∑︁ 𝑖=1𝑘∑︁ =1E 𝑡(cid:13) (cid:13) (cid:13)𝜃 𝑖(𝑡,𝑘) −𝜃𝑡(cid:13) (cid:13) (cid:13)2 ≤ 𝐶 𝐶1 0, (22)
𝑖=1𝑘=1 where𝐶 0 = 1 − 3𝑐 𝑔𝑑𝜂2𝐻2𝐿2 and𝐶 1 = 2𝑐 𝑔𝑑𝐻3𝜂2E 𝑡(cid:13) (cid:13)∇𝑓(𝜃𝑡)(cid:13) (cid:13)2
CauB cy hysu -Sb cs hti wtu at ri zng ineE qq u. a( l3 i) tyin toto deE cq o. m(1 p7 o) s, ew the isp gro race de ied ntto esu tis me at th oe
r
+ 32𝑑𝜎 𝑔2𝐻3𝜂2+ 𝜇2𝐿2𝑑 62𝐻3𝜂2 .
Finally,bysubstitutingEq.(22)intoEq.(21),weobtainthefi-
intotwoparts,eachofwhichisabiasedestimator.Recallthatinour
gradientestimator,𝑧 𝑖 followsaGaussiandistribution.Therefore, nalresultofthestepwisedescent.Aftersimplifyingandappropri-
atelysettingthelearningrate,wearriveattheresultpresentedin
theimpactonthenormcausedbyaforwardstepandabackward
Theorem3.1.Thedetailedderivationofthisresultisprovidedin
stepoftheestimatorisidentical.Consequently,weascertainthat
theterm𝑇 2 isboundedbyasingle-pointgradientestimationas AppendixD.1.
(cid:13) (cid:13)2
E 𝑡(cid:13) (cid:13) (cid:13)𝑧 𝑖(𝑡 𝜇,𝑘)(cid:16) 𝐹 𝑖(𝜃 𝑖(𝑡,𝑘) +𝜇𝑧 𝑖(𝑡,𝑘),B 𝑖(𝑡,𝑘) )−𝐹 𝑖(𝜃 𝑖(𝑡,𝑘),B 𝑖(𝑡,𝑘) )(cid:17)(cid:13) (cid:13)
(cid:13)
. 4.2 ProofofTheorem3.4
(cid:13) (cid:13) Theproofforthenon-i.i.d.casefollowsasimilarstructuretothatof
FollowingLemma4.1in[21],wecanboundtheexpectationterm
thei.i.d.case,withadjustmentsmadefortheheterogeneitybetween
as:
(cid:34) (cid:35)
localandglobalmodelsascapturedby𝑐
ℎ
and𝜎 ℎ2(Assumption6).
E 𝑡(cid:13) (cid:13) (cid:13)𝑒 𝑖(𝑡,𝑘)(cid:13) (cid:13) (cid:13)2 ≤ 𝑑1
2
2𝑑·E 𝑡(cid:13) (cid:13) (cid:13)∇𝐹 𝑖(𝜃 𝑖(𝑡,𝑘),B 𝑖(𝑡,𝑘) )(cid:13) (cid:13) (cid:13)2 + 𝜇 22 𝐿2𝑑2 vIn arp ia ar nt cic eu dla ur e,w toe nr oed ne -ifi .in .de .𝑇 d1 ai tn a:Eq.(16)as𝑇 (cid:101)1toreflecttheincreased
(cid:34) (cid:35) 𝑁
≤ 𝑑1
2
2𝑐 𝑔𝑑·E 𝑡(cid:13) (cid:13) (cid:13)∇𝑓 𝑖(𝜃 𝑖(𝑡,𝑘) )(cid:13) (cid:13) (cid:13)2 +2𝑑𝜎 𝑔2+ 𝜇 22 𝐿2𝑑2 , (18) 𝑇 (cid:101)1 ≤ 𝑁2 ∑︁ E 𝑡(cid:13) (cid:13) (cid:13)∇𝑓 𝑖(𝜃𝑡 )−∇𝑓(𝜃𝑡 )(cid:13) (cid:13) (cid:13)2 +2E 𝑡(cid:13) (cid:13)∇𝑓(𝜃𝑡 )(cid:13) (cid:13)2
𝑖=1
wherethesecondinequalityisderivedbasedonAssumption4.Sub- ≤2(1+𝑐 ℎ)E 𝑡(cid:13) (cid:13)∇𝑓(𝜃𝑡 )(cid:13) (cid:13)2+2𝜎 ℎ2, (23)
sequently,weboundtheexpectationtermbyapplyingtheCauchy-
wherethesecondinequalityfollowsAssumption6.
Schwartzinequalitytodivideitintothreeparts:
Subsequently,𝑇 (cid:101)2iscomputedsimilarly,withtheheterogeneity
E 𝑡(cid:13) (cid:13) (cid:13)∇𝑓 𝑖(𝜃 𝑖(𝑡,𝑘) )(cid:13) (cid:13) (cid:13)2 =E 𝑡(cid:13) (cid:13) (cid:13)∇𝑓 𝑖(𝜃 𝑖(𝑡,𝑘) )∓∇𝑓 𝑖(𝜃𝑡 )∓∇𝑓(𝜃 𝑖𝑡 )(cid:13) (cid:13) (cid:13)2 termsincorporated.Thedifferencefromthei.i.d.caseliesinthe
boundingofexpectationterminEq.(19):
≤3𝐿2E 𝑡(cid:13) (cid:13) (cid:13)𝜃 𝑖(𝑡,𝑘) −𝜃𝑡(cid:13) (cid:13) (cid:13)2 +3E 𝑡(cid:13) (cid:13) (cid:13)∇𝑓(𝜃𝑡 )(cid:13) (cid:13) (cid:13)2 . (19) E 𝑡(cid:13) (cid:13) (cid:13)∇𝑓 𝑖(𝜃 𝑖(𝑡,𝑘) )(cid:13) (cid:13) (cid:13)2 ≤3𝐿2E 𝑡(cid:13) (cid:13) (cid:13)𝜃 𝑖(𝑡,𝑘) −𝜃𝑡(cid:13) (cid:13) (cid:13)2
Thefirstpartrepresentsthegradientdifferencebetweenstages
(𝑡,𝑘)and(𝑡,0),whichcanbecomputedusingAssumption3,i.e., +3(𝑐 ℎ+1)E 𝑡(cid:13) (cid:13) (cid:13)∇𝑓(𝜃𝑡 )(cid:13) (cid:13) (cid:13)2 +3𝜎 ℎ2, (24)
the𝐿-smoothcondition.Thesecondpartsignifiesthedisparity
wheretheinequalityemploysAssumption6.
betweenlocalandglobalaspects,calculatedusingAssumption5.
Thethirdpartisretainedasis.
CombiningEquations(17),(18)and(24),webound𝑇 (cid:101)2asfollow:
Comb 𝑇in 2i ≤ng 6E
𝑐
𝑁q
𝑔
𝑑u 𝐿a 2ti ∑︁o 𝑁ns ∑︁𝐻(17 E), 𝑡(
(cid:13)
(cid:13)
(cid:13)1 𝜃8 𝑖() 𝑡,a 𝑘n )d −(1 𝜃9 𝑡)
(cid:13)
(cid:13)
(cid:13), 2webound𝑇 2asfollow: 𝑇 (cid:101)2 ≤ 6𝑐 𝑁𝑔 𝑑𝐿2 ∑︁ 𝑖𝑁 =1𝑘∑︁𝐻 =1E 𝑡(cid:13) (cid:13) (cid:13)𝜃 𝑖(𝑡,𝑘) −𝜃𝑡(cid:13) (cid:13) (cid:13)2 + 6𝑐 𝑔(𝑐 ℎ 𝑑+1)𝐻 E 𝑡(cid:13) (cid:13) (cid:13)∇𝑓(𝜃𝑡 )(cid:13) (cid:13) (cid:13)2
𝑖=1𝑘=1
+
6𝑐 𝑔𝜎 ℎ2𝐻
+
2𝐻𝜎 𝑔2
+
𝜇2𝐻𝐿2
. (25)
+6𝑐 𝑑𝑔𝐻 E 𝑡(cid:13) (cid:13) (cid:13)∇𝑓(𝜃𝑡 )(cid:13) (cid:13) (cid:13)2 + 2𝐻 𝑑𝜎 𝑔2 + 𝜇2𝐻 2𝐿2 . (20)
Bysubstituting𝑇
(cid:101)1inEq𝑑 .(23)and𝑑
𝑇
(cid:101)2inEq.2
(25)intoEq.(16),we
Next,combiningEquations(16),(17)and(20),wehave: gettheresultunderthenon-i.i.d.conditionasfollow:
E 𝑡(cid:2)𝑓(𝜃𝑡+1)(cid:3) −𝑓(𝜃𝑡 ) ≤ (cid:32) 3𝑐 𝑔𝜁 𝑑𝜂2𝐻𝐿 − 2 𝛾𝜂(cid:33) E 𝑡(cid:13) (cid:13) (cid:13)∇𝑓(𝜃𝑡 )(cid:13) (cid:13) (cid:13)2 E 𝑡(cid:2)𝑓(𝜃𝑡+1)(cid:3) −𝑓(𝜃𝑡 ) ≤ (cid:32) 3𝑐 𝑔(cid:101)𝑐 ℎ 𝑑𝜁𝜂2𝐻𝐿 − 2 (cid:101)𝑐 𝛾ℎ𝜂(cid:33) E 𝑡(cid:13) (cid:13) (cid:13)∇𝑓(𝜃𝑡 )(cid:13) (cid:13) (cid:13)2
+ 3𝑐 𝑔 𝑁𝜁𝜂 𝑑2𝐿3 ∑︁𝑁 ∑︁𝐻 E 𝑡(cid:13) (cid:13) (cid:13)𝜃 𝑖(𝑡,𝑘) −𝜃𝑡(cid:13) (cid:13) (cid:13)2 + 3𝑐 𝑔 𝑁𝜁𝜂 𝑑2𝐿3 ∑︁𝑁 ∑︁𝐻 E 𝑡(cid:13) (cid:13) (cid:13)𝜃 𝑖(𝑡,𝑘) −𝜃𝑡(cid:13) (cid:13) (cid:13)2
𝑖=1𝑘=1 𝑖=1𝑘=1
+𝜎 𝑔2𝜁𝜂 𝑑2𝐻𝐿 +𝜁𝜂2𝜇 42𝐻𝐿3
. (21)
+(cid:101)𝜎2𝜁𝜂 𝑑2𝐻𝐿 +𝜁𝜂2𝜇 42𝐻𝐿3
−
𝛾2
𝜎 ℎ2𝜂, (26)
6Dolly-Meta Code-LDA GSM-IID Dolly-Meta Code-LDA GSM-IID
BP-based SGD BP-based SGD 1.25 BP-based SGD =1e 3 =1e 3 1.25 =1e 3
1.8 FedMeZO 0.92 FedMeZO 1.20 FedMeZO 1.8 = =5 2e e 5 4 0.92 = =5 2e e 5 4 1.20 = =5 2e e 5 4
1.15 1.15
1.7 0.90 1.10 1.7 0.90 1.10
1.6 1.05 1.6 0.88 1.05
0.88 1.00 1.00
1.5 0.95 1.5 0.86 0.95
0.86 0.90 0.90
1.4 0.85 1.4 0.84 0.85
0 100 200 300 400 500 0 100 200 300 400 500 0 100 200 300 400 500 0 100 200 300 400 500 0 100 200 300 400 500 0 100 200 300 400 500
Rounds Rounds Rounds Rounds Rounds Rounds
Figure1:ConvergencecomparisonofFedMeZOandBP-based Figure2:Effectsofdifferentperturbationscales𝜇.
SGDoptimization.
1,whileallthecomprehensiveresultsareavailableinAppendix
where (cid:101)𝑐 ℎ denotes(𝑐 ℎ+1)and (cid:101)𝜎2denotes(3𝑐 𝑔𝜎 ℎ2+𝜎 𝑔2).Wethen F.5.
needtomaketheexpectationtermbounded.UnlikeEq.(22),dueto
thevariationsintroducedbyEq.(24),twoadditionaltermsrelated Table1:TheGPUMemoryofBP-basedSGDandFedMeZO.
to (cid:101)𝑐 ℎ and (cid:101)𝜎2emerge,yieldingthefollowingresult:
𝑁1 ∑︁ 𝑖𝑁 =1𝑘∑︁𝐻 =1E 𝑡(cid:13) (cid:13) (cid:13)𝜃 𝑖(𝑡,𝑘) −𝜃𝑡(cid:13) (cid:13) (cid:13)2 ≤ 𝐶 𝐶2 0, (27) DolT lya -s Mk
eta
BP-base 2d 65S 7G 1D(MiB) FedM 1e 0Z 0O 61(MiB)
GSM8K-IID 17771 9733
where𝐶
1
= 1−3𝑐 𝑔𝑑𝜂2𝐻2𝐿2 and𝐶
2
= 2𝑐 𝑔𝑑 (cid:101)𝑐 ℎ𝐻3𝜂2E 𝑡(cid:13) (cid:13)∇𝑓(𝜃𝑡)(cid:13) (cid:13)2 CodeAlpaca-LDA 15287 9569
+2𝜎2𝑑𝐻3𝜂2+ 𝜇2𝐿2𝑑2𝐻3𝜂2 .
3(cid:101) 6
Finally, we can substitute Eq. (27) into Eq. (26) and have the
Two conclusions emerge from the convergence experiments:
resultofTheorem3.4.Thedetailedderivationsaboutthesestepsof
First,whenthelearningratecomplieswiththerequirementsdis-
Theorem3.4areprovidedinAppendixD.3.
cussedinSection3.3,asstipulatedbyTheorems3.1and3.4,Fed-
MeZOconsistentlydiminisheslosswitheachstep,ultimatelyachiev-
5 EMPIRICALSUPPORT
ing stable convergence. Second, under equivalent learning rate
Thissectionaimstoempiricallyvalidateourtheoreticalfindings configurations, FedMeZO decreases loss more rapidly than BP-
throughaseriesofexperiments. basedSGD,indicatingaswifterconvergencerate.Forinstance,in
theDolly-Metafigure,FedMeZOstabilizesandconvergesaround
5.1 ExperimentalSetup
300rounds,whereasBP-basedSGD’slossisstilldecliningatthis
WeutilizeLLaMA-3B[47]asthefoundationalmodelandemploy juncture.Notably,fromTable1,weobservethattheGPUmem-
fourdatasetscoveringarangeoftasksanddatadistributiontypes orydemandforFedMeZOisroughlyone-thirdofthatrequiredby
toprovidecomprehensivevalidationofourtheoreticalresults[29]. BP-basedSGD,suggestingthatFedMeZOcanachieveaspeedier
Given that our theory centers on the loss function, we primar- convergencewithfewerresources.
ilyfocusonanalyzinglossdescentinourexperiments.Thebasic
informationofourexperimentalsetupisdetailedinTable3. 5.3 Hyper-parametersStudy
Wesetthetotalnumberofcommunicationroundsto500.By Inthissubsection,weperformaseriesofexperimentstoascertain
default,BP-basedbaselinesundertakelocaltrainingforoneepoch, the influence of various hyper-parameters, as intimated by our
whereasourproposedmethod,referredtoasFedMeZO,conductslo- theoreticalfindings.
caltrainingfor30steps.Formoredetailedimplementationspecifics,
pleaserefertoAppendixE. 5.3.1 ImpactofPerturbationScale𝜇. Tocorroboratethetheoretical
impactsoftheperturbationsteponconvergence,weexamine𝜇
5.2 ConvergenceStudy values of 5×10−3 and 2×10−4, in addition tothe default 𝜇 =
1×10−3.WeleveragethesamedatasetsandsplittersasinSection
ToassesstheconvergenceofFedMeZO,weperformedexperiments
5.2forrobustness.Figure2showsrepresentativeoutcomes,with
onthreedatasetsusingdifferentdatasplitters,asspecifiedinTable3,
comprehensiveresultsinAppendixF.1.
withtestlossservingastheconvergencemetric.Ourobjectiveisto
Theresultsconfirmthat,consistentwithEquations(10)and(14),
evaluatethegeneralizationandstabilityofFedMeZOacrossdiverse
asmaller𝜇marginallyexpeditesmodelconvergence.Figure2exem-
datasetsandheterogeneityscenarios.Forbenchmarkingpurposes,
plifiesthatthetrainingtrajectorywith𝜇=2×10−4descendsmore
wealsomeasuredtheperformanceofBP-basedSGDonthesame
rapidlythantheothers.However,giventhat𝜇appearsasasecond-
datasets.Additionally,wedocumenttheGPUmemoryusageduring
𝜁𝜇2𝐿3
traininginTable1.RepresentativefindingsareillustratedinFigure ordertermin 4(cid:101)Γ (cid:101)𝑐ℎ𝑁𝐻 anditsabsolutevalueisrelativelysmall,its
7
ssoL ssoL ssoL ssoL ssoL ssoLDolly-Meta Code-LDA GSM-IID Round-wise Loss Five-round Loss Model Update Difference
1.9 H=30 H=30 H=30 Default Default Default
H=10 H=10 H=10 2.2 Random 2.2 Random 2.2 Random
1.8 H=50 0.92 H=50 1.2 H=50 Strategy Strategy Strategy
1.7 0.90 2.0 2.0 2.0
1.1
1.6 0.88 1.8 1.8 1.8
1.0
1.5 0.86 1.6 1.6 1.6
0.9
1.4 0.84
0 100 Ro200 und300
s
400 500 0 100 Ro200 und300
s
400 500 0 100 Ro200 und300
s
400 500 1.4 0 50 1 R00 ou15 n0 d2 s00 250 300 1.4 0 50 1 R00 ou15 n0 d2 s00 250 300 1.4 0 50 1 R00 ou15 n0 d2 s00 250 300
Figure3:Effectsofdifferentlocaliterations𝐻. Figure4:Comparisonofpersonalizedlearningratewithde-
faultandrandomsettings.
overallinfluenceismodest.ThisisevidentinFigure2,wheremod-
ificationsto𝜇withinaspecificrangeyieldonlyslightvariations.
Thus,asmaller𝜇provesadvantageousformodelconvergence. themaximallearningrate,potentiallyleadingtosurgesasperthe
learningratesearchnetwork.Symmetrically,weposittheminimal
5.3.2 ImpactofLocalIterations𝐻. Tovalidatethetheoreticalim- valueat5×10−6,anchoredonthedefaultlearningrateof1×10−5,
pactoflocaliterationsonconvergence,wecontrast𝐻 = 10and therebyassigning𝛼 avalueof5×10−6.
𝐻 =50withthestandard𝐻 =30.Utilizingidenticaldatasetsand
Ascounterpoints,wefurnishtwoconfigurationstrategiesforthe
splittersfromSection5.2,wepresenttypicalfindingsinFigure3, learningrateadjustment:oneuniformlyappliesadefaultlearning
withallthedetailedresultsforthcominginAppendixF.2. rate of 1×10−5, while the other randomly selects within (5×
Theseexperimentalresultssuggestthatalower𝐻 engendersa 10−6,1.5×10−5)foreachround.Theconclusiveresultsaredepicted
moresluggishconvergencepace,whereasahigher𝐻 somewhat inFigure4.
propelsconvergence,mirroringtheimpactof𝐻asadenominatorin
Basedontheexperimentalresults,weobservedthatourmethod
thetheoreticalconvergencerateanalysis.Nonetheless,anexcessive achievedfasterlossconvergencewiththesecondandthirdtypes
𝐻 mayleadtoinstability,asdepictedbythe𝐻 = 50line,which ofsignalquantitiescomparedtothedefaultandrandomsettings,
exhibitsasurgeendwiseinthegraph.Hence,anappropriatechoice withthethirdtypeyieldingthemostimpressiveperformance.In
of𝐻 facilitatesefficientmodelconvergence. contrast,thefirsttypeofsignalquantityhadanegligibleimpact.
Theseresultssuggestthatwhileindividualroundtrainlossesex-
5.3.3 Analysis of Other Hyper-parameters. We also explore the
hibitsomedegreeofrandomness,aggregatinglossesovermultiple
ramificationsofdatasplittersandthenumberofclientsonthe
roundscanapproximateheterogeneitytoameaningfulextent,thus
convergencerate.Detailspertainingtotheseexperimentalsettings
servingasanindicatortoexpeditemodelconvergence.Itisalso
aredocumentedinAppendicesF.3andF.4,respectively.Here,we
noteworthythatthethirdtypeofsignalquantityalignswiththe
merelysummarizetheexperimentaloutcomes.
Dissimilarsplitterssymbolizevaryingextentsofdatahetero- expression ∥∇𝑓(𝜃𝑡) −(cid:205) 𝑘𝐻 =0𝜂 𝑖∇𝑓 𝑖(𝜃(𝑡,𝑘))∥2, which most closely
geneity.Fromtheseresults,wehaveobservationsrevealingthat reflectsAssumption6.Consequently,itdemonstratedthemostef-
augmentedheterogeneityculminatesinlowerstabilizedlossvalues, fectiveperformanceintheexperiments,notonlyachievingthe
intimatingthatamoderatedegreeofdataheterogeneitycanelevate fastestconvergencebutalsotheloweststableloss.Thiscasestudy
themodel’sconvergenceproficiency. experimentsubstantiatestheefficacyofProposition3.7,offering
valuableinsightsforparametertuninginpersonalizedFederated
5.4 PersonalizationStudy Learning(pFL).
ToreconcileProposition3.7articulatedinSection3.3withpractical
6 CONCLUSION
scenarios,weconductsubsequentexperiments.Toaccountforeach
client’sheterogeneityduringmodelupdatesineachround,wede- ThisstudyinvisigatestheconvergenceofFedMeZO,apractical
rivethreesignalquantities:(1)Round-wiseTrainLossDifference: approachintegratingZeroth-OrderOptimizationwithinafeder-
Thediscrepancybetweeneachclient’slossintheprecedingtrain- atedlearningframeworkforLargeLanguageModels.Extensive
ingroundandthegloballoss.(2)Five-roundAverageTrainLoss empiricalresultsverifiedouranalysesandindicatethatFedMeZO
Difference:Theaveragelossdeviationforeachclientrelativetothe achievesfastconvergencewithreducedGPUmemoryrequirements,
globallossovertheantecedentfiverounds.(3)ModelParameter offeringanpromosingalternativetotraditionaloptimizationmeth-
UpdateDifference:Thedisparitybetweeneachclient’sprevious ods.Theincorporationofapersonalizedlearningrateadjustment,
roundparameterupdatesandtheglobalupdatemagnitude.We derivedfromtheoreticalanalysis,hasshowntoeffectivelyenhance
normalizethesesignalquantitiestotherangeof(−1,1),servingas lossreduction.Wehopeourworkcanenlightmoreresearchand
proxiesforΦ. developmentofmemory-efficientoptimizationtechniquestoad-
Forthesettingofthescalingfactor𝛼,followingtheguidance dresspracticalchallengesassociatedwiththefine-tuningofLLMs,
ofthelearningrateinAppendixF.6,wedesignate1.5×10−5 as particularlyinresource-constrainedenvironments.
8
ssoL ssoL ssoL ssoL ssoL ssoLREFERENCES
[25] JunyuanHong,ZhuangdiZhu,ShuyangYu,ZhangyangWang,HirokoDodge,
[1] AlekhAgarwal,MartinJWainwright,PeterBartlett,andPradeepRavikumar. andJiayuZhou.2021.FederatedAdversarialDebiasingforFairandTransferable
2009.Information-theoreticlowerboundsontheoraclecomplexityofconvex
Representations.InKDD.
optimization.AdvancesinNeuralInformationProcessingSystems22(2009). [26] EdwardJ.Hu,YelongShen,PhillipWallis,ZeyuanAllen-Zhu,YuanzhiLi,Shean
[2] ArmenAghajanyan,SonalGupta,andLukeZettlemoyer.2021. IntrinsicDi- Wang,LuWang,andWeizhuChen.2022.LoRA:Low-RankAdaptationofLarge
mensionalityExplainstheEffectivenessofLanguageModelFine-Tuning.In
LanguageModels.InTheTenthInternationalConferenceonLearningRepresenta-
Proceedingsofthe59thAnnualMeetingoftheAssociationforComputationalLin- tions,ICLR.
guisticsandthe11thInternationalJointConferenceonNaturalLanguageProcessing [27] KevinGJamieson,RobertNowak,andBenRecht.2012. Querycomplexityof
(Volume1:LongPapers).7319–7328. derivative-freeoptimization.AdvancesinNeuralInformationProcessingSystems
[3] LéonBottou,FrankECurtis,andJorgeNocedal.2018.Optimizationmethodsfor 25(2012).
large-scalemachinelearning.SIAMreview60,2(2018),223–311. [28] PeterKairouz,H.BrendanMcMahan,BrendanAvent,AurélienBellet,MehdiBen-
[4] TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan, nis,ArjunNitinBhagoji,KallistaBonawitz,ZacharyCharles,GrahamCormode,
PrafullaDhariwal,ArvindNeelakantan,PranavShyam,GirishSastry,Amanda RachelCummings,RafaelG.L.D’Oliveira,HubertEichner,SalimElRouayheb,
Askell,etal.2020.Languagemodelsarefew-shotlearners.Advancesinneural DavidEvans,JoshGardner,ZacharyGarrett,AdriàGascón,BadihGhazi,PhillipB.
informationprocessingsystems33(2020),1877–1901. Gibbons,MarcoGruteser,ZaidHarchaoui,ChaoyangHe,LieHe,ZhouyuanHuo,
[5] SahilChaudhary.2023.Codealpaca:Aninstruction-followingllamamodelfor BenHutchinson,JustinHsu,MartinJaggi,TaraJavidi,GauriJoshi,MikhailKho-
codegeneration. dak,JakubKonecný,AleksandraKorolova,FarinazKoushanfar,SanmiKoyejo,
[6] ChaochaoChen,XiaohuaFeng,JunZhou,JianweiYin,andXiaolinZheng.2023. TancrèdeLepoint,YangLiu,PrateekMittal,MehryarMohri,RichardNock,Ayfer
Federatedlargelanguagemodel:Apositionpaper.arXivpreprintarXiv:2307.08925 Özgür,RasmusPagh,HangQi,DanielRamage,RameshRaskar,MarianaRaykova,
(2023). DawnSong,WeikangSong,SebastianU.Stich,ZitengSun,AnandaTheertha
[7] DaoyuanChen,DaweiGao,WeiruiKuang,YaliangLi,andBolinDing.2022. Suresh,FlorianTramèr,PraneethVepakomma,JianyuWang,LiXiong,Zheng
pFL-Bench:AComprehensiveBenchmarkforPersonalizedFederatedLearning. Xu,QiangYang,FelixX.Yu,HanYu,andSenZhao.2021.AdvancesandOpen
InNeurIPS. ProblemsinFederatedLearning.FoundationsandTrends®inMachineLearning
[8] DaoyuanChen,DaweiGao,YuexiangXie,XuchenPan,ZitaoLi,YaliangLi,Bolin 14,1–2(2021),1–210.
Ding,andJingrenZhou.2023.FS-REAL:TowardsReal-WorldCross-DeviceFed- [29] WeiruiKuang,BingchenQian,ZitaoLi,DaoyuanChen,DaweiGao,XuchenPan,
eratedLearning.InProceedingsofthe29thACMSIGKDDConferenceonKnowledge YuexiangXie,YaliangLi,BolinDing,andJingrenZhou.2023.Federatedscope-
DiscoveryandDataMining.3829–3841. LLM:Acomprehensivepackageforfine-tuninglargelanguagemodelsinfeder-
[9] DaoyuanChen,YilunHuang,ZhijianMa,HesenChen,XuchenPan,CeGe,
atedlearning.arXivpreprintarXiv:2309.00363(2023).
DaweiGao,YuexiangXie,ZhaoyangLiu,JinyangGao,YaliangLi,BolinDing, [30] ChunyuanLi,HeeradFarkhoor,RosanneLiu,andJasonYosinski.2018.Measuring
andJingrenZhou.2023.Data-Juicer:AOne-StopDataProcessingSystemfor
theIntrinsicDimensionofObjectiveLandscapes.InInternationalConferenceon
LargeLanguageModels.arXivpreprintarXiv:2309.02033(2023). LearningRepresentations.
[10] DaoyuanChen,LiuyiYao,DaweiGao,BolinDing,andYaliangLi.2023.Efficient [31] LeiLaiLi,JianzongWang,XiaoyangQu,andJingXiao.2021.Communication-
PersonalizedFederatedLearningviaSparseModel-Adaptation.arXivpreprint memory-efficientdecentralizedlearningforaudiorepresentation.In2021Inter-
arXiv:2305.02776(2023). nationalJointConferenceonNeuralNetworks(IJCNN).IEEE,1–8.
[11] KarlCobbe,VineetKosaraju,MohammadBavarian,MarkChen,HeewooJun, [32] MuLi,TongZhang,YuqiangChen,andAlexanderJSmola.2014.Efficientmini-
LukaszKaiser,MatthiasPlappert,JerryTworek,JacobHilton,ReiichiroNakano,
batchtrainingforstochasticoptimization.InProceedingsofthe20thACMSIGKDD
etal.2021. Trainingverifierstosolvemathwordproblems. arXivpreprint internationalconferenceonKnowledgediscoveryanddatamining.661–670.
arXiv:2110.14168(2021). [33] TianLi,AnitKumarSahu,ManzilZaheer,MaziarSanjabi,AmeetTalwalkar,
[12] MikeConover,MattHayes,AnkitMathur,XiangruiMeng,JianweiXie,JunWan, andVirginiaSmith.2020.Federatedoptimizationinheterogeneousnetworks.
SamShah,AliGhodsi,PatrickWendell,MateiZaharia,etal.2023. Freedolly:
ProceedingsofMachinelearningandsystems2(2020),429–450.
Introducingtheworld’sfirsttrulyopeninstruction-tunedllm. [34] XiangLi,KaixuanHuang,WenhaoYang,ShusenWang,andZhihuaZhang.2019.
[13] MikeConover,MattHayes,AnkitMathur,JianweiXie,JunWan,SamShah,
OntheConvergenceofFedAvgonNon-IIDData.InICLR.
AliGhodsi,PatrickWendell,MateiZaharia,andReynoldXin.2023.FreeDolly: [35] SijiaLiu,Pin-YuChen,BhavyaKailkhura,GaoyuanZhang,AlfredOHeroIII,
IntroducingtheWorld’sFirstTrulyOpenInstruction-TunedLLM. andPramodKVarshney.2020.Aprimeronzeroth-orderoptimizationinsignal
[14] ZhengxiaoDu,YujieQian,XiaoLiu,MingDing,JiezhongQiu,ZhilinYang,and processingandmachinelearning:Principals,recentadvances,andapplications.
JieTang.2021.Glm:Generallanguagemodelpretrainingwithautoregressive
IEEESignalProcessingMagazine37,5(2020),43–54.
blankinfilling.arXivpreprintarXiv:2103.10360(2021). [36] SadhikaMalladi,TianyuGao,EshaanNichani,AlexDamian,JasonDLee,Danqi
[15] JohnCDuchi,MichaelIJordan,MartinJWainwright,andAndreWibisono.2015. Chen,andSanjeevArora.2023.Fine-TuningLanguageModelswithJustForward
Optimalratesforzero-orderconvexoptimization:Thepoweroftwofunction
Passes.arXivpreprintarXiv:2305.17333(2023).
evaluations.IEEETransactionsonInformationTheory61,5(2015),2788–2806. [37] Othmane Marfoq, Chuan Xu, Giovanni Neglia, and Richard Vidal. 2020.
[16] AlirezaFallah,AryanMokhtari,andAsumanOzdaglar.2020.Personalizedfeder- Throughput-OptimalTopologyDesignforCross-SiloFederatedLearning.In
atedlearning:Ameta-learningapproach.InNeurIPS2020. NeurIPS,H.Larochelle,M.Ranzato,R.Hadsell,M.F.Balcan,andH.Lin(Eds.),
[17] TaoFan,YanKang,GuoqiangMa,WeijingChen,WenbinWei,LixinFan,and Vol.33.CurranAssociates,Inc.,19478–19487. https://proceedings.neurips.cc/
QiangYang.2023.FATE-LLM:AIndustrialGradeFederatedLearningFramework paper/2020/file/e29b722e35040b88678e25a1ec032a21-Paper.pdf
forLargeLanguageModels.CoRRabs/2310.10049(2023). [38] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and
[18] TaoFan,YanKang,GuoqiangMa,WeijingChen,WenbinWei,LixinFan,and BlaiseAguerayArcas.2017. Communication-efficientlearningofdeepnet-
QiangYang.2023.Fate-llm:Aindustrialgradefederatedlearningframeworkfor
worksfromdecentralizeddata.InAISTATS.PMLR,1273–1282.
largelanguagemodels.arXivpreprintarXiv:2310.10049(2023). [39] ChuizhengMeng,SirishaRambhatla,andYanLiu.2021.Cross-nodefederated
[19] WenzhiFang,ZiyiYu,YuningJiang,YuanmingShi,ColinNJones,andYong
graphneuralnetworkforspatio-temporaldatamodeling.InKDD.1202–1211.
Zhou.2022.Communication-efficientstochasticzeroth-orderoptimizationfor [40] KhalilMuhammad,QinqinWang,DiarmuidO’Reilly-Morgan,EliasTragos,Barry
federatedlearning.IEEETransactionsonSignalProcessing70(2022),5058–5073. Smyth,NeilHurley,JamesGeraci,andAonghusLawlor.2020. Fedfast:Going
[20] HaozheFeng,TianyuPang,ChaoDu,WeiChen,ShuichengYan,andMinLin.
beyondaverageforfastertrainingoffederatedrecommendersystems.InKDD.
2023.DoesFederatedLearningReallyNeedBackpropagation?arXivpreprint 1234–1242.
arXiv:2301.12195(2023). [41] YuriiNesterovandVladimirSpokoiny.2017.Randomgradient-freeminimization
[21] XiangGao,BoJiang,andShuzhongZhang.2018.Ontheinformation-adaptive
ofconvexfunctions.FoundationsofComputationalMathematics17(2017),527–
variantsoftheADMM:aniterationcomplexityperspective.JournalofScientific 566.
Computing76(2018),327–363. [42] AdamPaszke,SamGross,FranciscoMassa,AdamLerer,JamesBradbury,Gregory
[22] BehroozGhorbani,ShankarKrishnan,andYingXiao.2019. Aninvestigation Chanan,TrevorKilleen,ZemingLin,NataliaGimelshein,LucaAntiga,etal.2019.
intoneuralnetoptimizationviahessianeigenvaluedensity.InInternational Pytorch:Animperativestyle,high-performancedeeplearninglibrary.Advances
ConferenceonMachineLearning.PMLR,2232–2241. inneuralinformationprocessingsystems32(2019).
[23] JiaqiGu,ChenghaoFeng,ZhengZhao,ZhoufengYing,RayTChen,andDavidZ [43] LeventSagun,UtkuEvci,VUgurGuney,YannDauphin,andLeonBottou.2017.
Pan.2021.Efficienton-chiplearningforopticalneuralnetworksthroughpower-
Empiricalanalysisofthehessianofover-parametrizedneuralnetworks.arXiv
awaresparsezeroth-orderoptimization.InProceedingsoftheAAAIconferenceon preprintarXiv:1706.04454(2017).
artificialintelligence,Vol.35.7583–7591. [44] FelixSattler,Klaus-RobertMüller,andWojciechSamek.2020.ClusteredFederated
[24] DavoodHajinezhad,MingyiHong,andAlfredoGarcia.2019. ZONE:Zeroth- Learning:Model-AgnosticDistributedMultitaskOptimizationUnderPrivacy
ordernonconvexmultiagentoptimizationovernetworks.IEEETrans.Automat. Constraints.TNNLS(2020).
Control64,10(2019),3995–4010.
9[45] YaoShu,XiaoqiangLin,ZhongxiangDai,andBryanKianHsiangLow.2023.
FederatedZeroth-OrderOptimizationusingTrajectory-InformedSurrogateGra-
dients.arXivpreprintarXiv:2308.04077(2023).
[46] RohanTaori,IshaanGulrajani,TianyiZhang,YannDubois,XuechenLi,Carlos
Guestrin,PercyLiang,andTatsunoriBHashimoto.2023.Stanfordalpaca:An
instruction-followingllamamodel.
[47] HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-Anne
Lachaux,TimothéeLacroix,BaptisteRozière,NamanGoyal,EricHambro,Faisal
Azhar,etal.2023.Llama:Openandefficientfoundationlanguagemodels.arXiv
preprintarXiv:2302.13971(2023).
[48] JianyuWang,ZacharyCharles,ZhengXu,GauriJoshi,HBrendanMcMa-
han,MaruanAl-Shedivat,GalenAndrew,SalmanAvestimehr,KatharineDaly,
DeepeshData,etal.2021.Afieldguidetofederatedoptimization.arXivpreprint
arXiv:2107.06917(2021).
[49] JianyuWang,QinghuaLiu,HaoLiang,GauriJoshi,andHVincentPoor.2021.A
novelframeworkfortheanalysisanddesignofheterogeneousfederatedlearning.
IEEETransactionsonSignalProcessing69(2021),5234–5249.
[50] ZhenWang,WeiruiKuang,YuexiangXie,LiuyiYao,YaliangLi,BolinDing,and
JingrenZhou.2022.FederatedScope-GNN:TowardsaUnified,Comprehensive
andEfficientPackageforFederatedGraphLearning.InProceedingsofthe28th
ACMSIGKDDConferenceonKnowledgeDiscoveryandDataMining.4110–4120.
[51] ThomasWolf,LysandreDebut,VictorSanh,JulienChaumond,ClementDelangue,
AnthonyMoi,PierricCistac,TimRault,RémiLouf,MorganFuntowicz,etal.
2020.Transformers:State-of-the-artnaturallanguageprocessing.InProceedings
ofthe2020conferenceonempiricalmethodsinnaturallanguageprocessing:system
demonstrations.38–45.
[52] YuexiangXie,ZhenWang,DaweiGao,DaoyuanChen,LiuyiYao,WeiruiKuang,
YaliangLi,BolinDing,andJingrenZhou.2023.FederatedScope:AFlexibleFed-
eratedLearningPlatformforHeterogeneity.ProceedingsoftheVLDBEndowment
16,5(2023),1059–1072.
[53] ZheweiYao,AmirGholami,KurtKeutzer,andMichaelWMahoney.2020.Pyhes-
sian:Neuralnetworksthroughthelensofthehessian.In2020IEEEinternational
conferenceonbigdata(Bigdata).IEEE,581–590.
[54] XinleiYi,ShengjunZhang,TaoYang,andKarlHJohansson.2022.Zeroth-order
algorithmsforstochasticdistributednonconvexoptimization.Automatica142
(2022),110353.
[55] JianyiZhang,SaeedVahidian,MartinKuo,ChunyuanLi,RuiyiZhang,Guoyin
Wang,andYiranChen.2023.TowardsBuildingtheFederatedGPT:Federated
InstructionTuning.arXivpreprintarXiv:2305.05644(2023).
[56] JianyiZhang,SaeedVahidian,MartinKuo,ChunyuanLi,RuiyiZhang,Guoyin
Wang,andYiranChen.2023.TowardsBuildingtheFederatedGPT:Federated
InstructionTuning.arXivpreprintarXiv:2305.05644(2023).
[57] QingsongZhang,BinGu,ZhiyuanDang,ChengDeng,andHengHuang.2021.
Desirablecompanionforverticalfederatedlearning:NewZeroth-ordergradient
basedalgorithm.InProceedingsofthe30thACMInternationalConferenceon
Information&KnowledgeManagement.2598–2607.
[58] SusanZhang,StephenRoller,NamanGoyal,MikelArtetxe,MoyaChen,Shuohui
Chen,ChristopherDewan,MonaDiab,XianLi,XiVictoriaLin,etal.2022.Opt:
Openpre-trainedtransformerlanguagemodels.arXivpreprintarXiv:2205.01068
(2022).
[59] WayneXinZhao,KunZhou,JunyiLi,TianyiTang,XiaoleiWang,YupengHou,
YingqianMin,BeichenZhang,JunjieZhang,ZicanDong,etal.2023.Asurvey
oflargelanguagemodels.arXivpreprintarXiv:2303.18223(2023).
10APPENDIX Bysymmetry,wechange𝛿to−𝛿andobtain
A NOTATION
For ease of reading and reference, we present all mathematical ∇𝐹 𝑖(𝜃,B𝑖)=−E 𝜇𝑧𝑖∼N(0,𝜇2𝐼𝑑)(cid:104)𝑧 𝜇𝑖 ·𝐹 𝑖(𝜃−𝜇𝑧 𝑖,B𝑖)(cid:105) . (29)
symbolsusedinthispaperinTable2.
B ASSUMPTIONS Furtherweprovethatwith
Assumption2. (BoundedLoss)Thegloballossfunction𝑓(𝜃)is
boundedbelowbyascalar𝑓∗,i.e.,𝑓∗ ≥ 𝑓(𝜃) >−∞forall𝜃.
∇𝐹 𝑖(𝜃,B𝑖)=
1 E(cid:104)𝑧 𝑖
·𝐹 𝑖(𝜃+𝜇𝑧
𝑖,B𝑖)(cid:105)
−
1 E(cid:104)𝑧 𝑖
·𝐹 𝑖(𝜃−𝜇𝑧
𝑖,B𝑖)(cid:105)
Assumption3. (𝐿-smoothness)Thelocalandgloballossfunctions 2 𝜇 2 𝜇
𝜃𝐹 𝑖 ,(𝜃 𝜃,B ∈𝑖) R, 𝑑𝑓 𝑖 ,( i𝜃 t) h, oa ldn sd th𝑓 a( t𝜃) are L-smooth. Mathematically, for any =E(cid:104) 2𝑧 𝜇𝑖 (cid:101)∇𝐹 𝑖(𝜃,𝑧 𝑖,B𝑖,𝜇)(cid:105) . (30)
1 2
Finally,weadoptEq.(1)andgetLemma2.2,i.e.,
∥∇𝑓 𝑖(𝜃 2)−∇𝑓 𝑖(𝜃 1)∥ ≤𝐿∥𝜃 2−𝜃 1∥,
𝐿
𝑓 𝑖(𝜃 2) ≤ 𝑓 𝑖(𝜃 1)+⟨∇𝑓 𝑖(𝜃),𝜃 2−𝜃 1⟩+ ∥𝜃 2−𝜃 1∥2.
2 ∇𝑓 𝑖(𝜃)=E[∇𝐹 𝑖(𝜃,B𝑖)] =E[(cid:101)∇𝐹 𝑖(𝜃,𝑧 𝑖,B𝑖,𝜇)]. (31)
Assumption4. (Mini-batchGradientErrorBound)Forany
𝜃 ∈R𝑑,thesecond-ordermomentofthestochasticgradientisbounded
□
byE B𝑖∥∇𝐹 𝑖(𝜃,B𝑖)∥2 ≤𝑐 𝑔∥∇𝑓 𝑖(𝜃)∥2+𝜎 𝑔2,where𝑐
𝑔
≥1.
Assumption5. (Global-LocalDisparitiesini.i.d.Setting)For C.2 ProofofLemma2.3
any𝜃 ∈R𝑑,thediscrepancybetweenthelocalandglobalgradientis
negligible,i.e.,E 𝑖∥∇𝑓(𝜃)−∇𝑓 𝑖(𝜃)∥2=0. Malladietal.[36]presentastep-wiselearningratedecaycorollary
thatisindependentofthedimensionalityparameter𝑑andissolely
Assumption6. (Global-LocalDisparitiesinnon-i.i.d.Setting) related to the low effective rank 𝑟. We formalize this result as
Forany𝜃 ∈R𝑑,thediscrepancybetweenthelocalandglobalgradient follows:
isboundedby∥∇𝑓(𝜃)−∇𝑓 𝑖(𝜃)∥2 ≤𝑐 ℎ∥∇𝑓(𝜃)∥2+𝜎 ℎ2,where𝑐
ℎ
is
apositiveconstant. LemmaC.1(Step-WiseLearningRateDecay). AssumingtheHes-
sianmatrixintermsof𝜃 exhibitsalocaleffectiverankof𝑟,andthere
Assumptions2-4arewell-establishedintheliteratureonlarge- existsaconstant𝛾 = 𝑑𝑟+𝑑−2 =Θ(𝑟/𝑛),theexpecteddecreaseinthe
scalestochasticoptimization[3].Assumption5describesanideal
𝑛(𝑑+2)
losscanbeboundedasfollows:
i.i.d.settingwhereeachclient’sgradientisalignedwiththeglobal
gradient.Assumption6accountsfortheheterogeneityofclient
datadistributionsthatistypicalinnon-i.i.d.settings[33,34,49]. E(cid:2)𝑓(𝜃𝑡+1)(cid:3) ≤ 𝑓(𝜃𝑡 )− 1 𝜂∥∇𝑓(𝜃𝑡 )∥2+ 1 𝜂2𝐿1 E(cid:2) ∥∇𝐹(𝜃,B)∥2(cid:3),
𝛾 2 𝛾
C FULLPROOFOFLEMMA (32)
C.1 ProofofLemma2.2
WerecaptheproofofStein’sidentityfollowing[20],notingthat𝛿 where𝑛denotesthenumberofrandomizations.
isarandomvariablesampledfromN(0,𝐼 𝑑),andwereplaceitwith
𝜇𝑧 𝑖 inDefinition2.1. ItisimportanttonotethatthelastterminEq.(32)represents
thesquarednormofthetruegradient,whichisinconsistentwith
(cid:104) (cid:105) thezeroth-orderestimationmethodusedinourapproach.There-
∇𝐹 𝑖(𝜃,B𝑖)=E
𝛿∼N(0,𝜎2𝐼𝑑)
∇𝐹 𝑖(𝜃+𝛿,B𝑖)
fore,atransformationisnecessarytoalignitwiththeFedMeZO
=(2𝜋)−𝑑 2∇∫
𝐹
𝑖(𝜃+𝛿,B𝑖)·exp(cid:16) −∥𝛿∥2 2(cid:17)
𝑑𝛿
a trl ugo er git rh am di. eB ne ts nid oe rs m,M anal dla td hi ee zt ea rl o.d the -t oai rl dt eh re gr re ala dt ii eo nn ts eh si tp imbe at tw oree nn ot rh me
,
2𝜎2
whichwerestateasfollows:
=(2𝜋)−𝑑
2
∫
𝐹
𝑖(𝜃+𝛿,B𝑖)·∇exp(cid:16) −∥𝜃+𝛿−𝜃∥2 2(cid:17)
𝑑𝜃
2𝜎2 LemmaC.2(GradientEstimatorNormRelationship). Thesquared
=(2𝜋)−𝑑
2
∫
𝐹 𝑖(𝜃+𝛿,B𝑖)·
𝛿 ·exp(cid:16) −∥𝛿∥2 2(cid:17)
𝑑𝛿
normofthegradientestimatedbytheMeZOisgivenby
𝜎2 2𝜎2
=E 𝛿∼N(0,𝜎2𝐼𝑑)(cid:104) 𝜎𝛿
2
·𝐹 𝑖(𝜃+𝛿,B𝑖)(cid:105) E(cid:104) ∥∇𝐹(𝜃,B)∥2(cid:105) = 𝑑+𝑛𝑛 −1E(cid:104) ∥(cid:101)∇𝐹(𝜃,𝑧,B,𝜇)∥2(cid:105) . (33)
=E 𝜇𝑧𝑖∼N(0,𝜇2𝐼𝑑)(cid:104)𝑧 𝜇𝑖 ·𝐹 𝑖(𝜃+𝜇𝑧 𝑖,B𝑖)(cid:105) . (28)
BysubstitutingEq.(33)intoEq.(32),weobtainLemma2.3. □
11Symbol Description
𝑓(𝜃) Globallossfunctionovertheparameter𝜃
∇𝑓(𝜃) Gradientofthelossfunctionwithrespecttoparameter𝜃
𝐹 𝑖(𝜃,B𝑖) Locallossfunctiononthe𝑖𝑡ℎ clientwithmini-batchB𝑖
∇𝐹(𝜃,B) Thegradientofparameter𝜃 withmini-batches
(cid:101)∇𝐹 𝑖(𝜃,𝑧,B𝑖,𝜇) Zeroth-ordergradientestimatorfor𝐹
𝑖
withperturbation𝜇
𝑧 GaussianrandomvariablesampledfromN(0,𝐼 𝑑)
B𝑖 Mini-batchofdatasampledfromthelocaldistributionD𝑖
𝜇 Perturbationscaleforzeroth-ordergradientestimation
𝜂 Learningrateformodelupdates
𝑛 Numberofperturbationsin𝑛-SPSAzeroth-orderoptimization
(𝑡,𝑘) 𝑘𝑡ℎ iterationwithinthe𝑡𝑡ℎ communicationround
H Hessianmatrixofthelossfunction
𝑟 TheloweffectiverankofHessianmatrix
𝛾 Factorquantifyingtheeffectivelowrankpropertyofthegradient
𝜁 Factorquantifyingtheeffectivelowrankpropertyofthegradientestimator
𝑑 Dimensionofthemodelparameter𝜃
𝐿 Smoothnessconstantofthelossfunction
𝑁 NumberofclientsparticipatinginFL
𝑖 Indexidentifyingthe𝑖𝑡ℎ client
𝑇 NumberofcommunicationroundsinFL
𝑒 𝑖(𝑡,𝑘) Zeroth-ordergradientestimatorofthe𝑖𝑡ℎ clientiniteration(𝑡,𝑘)
𝐻 Totalnumberoflocaliterationswithinacommunicationround
𝑐 𝑔,𝜎 𝑔 Constantsrelatedtothegradientestimationgapcausedbymini-batchstochasticity
𝑐 ℎ,𝜎 ℎ Constantsrelatedtotheheterogeneityofclientdataandtheglobalmodel
E 𝑡 Expectationtakenovertherandomnessinthe𝑡𝑡ℎ round
E𝑘
𝑡
Expectationtakenovertherandomnessinthe𝑘𝑡ℎ iterationofthe𝑡𝑡ℎ
round
Table2:Descriptionofsymbolsusedinthepaper.
D FULLPROOFOFTHEOREMS where𝑒 𝑖(𝑡,𝑘) representsthegradientestimatordefinedinEq.(6).
Substitutingtheestimatorintotheaboveexpressionandsimplify-
D.1 ProofofTheorem3.1
Fortheterm𝑇 1:
𝜃in 𝑖(g 𝑡,, 𝑘w
)
,e
𝑧
𝑖′o dbt ea ni on teth 𝑧e 𝑖(𝑡f ,o 𝑘l )l ,o aw ni dng Bi 𝑖n ′e dq eu na ol ti ety B.F 𝑖o (𝑡r ,𝑘b )r :evity,let𝜃 𝑖′ denote
(cid:13) 𝑁 (cid:13)2
𝑇 1 ===
≤
≤
2EE
2
𝑁
EE𝑡𝑡 2(cid:13)
(cid:13) (cid:13) (cid:13)
(cid:13)(cid:13) (cid:13) (cid:13) (cid:13)
𝑡2𝑡
(cid:13)
(cid:13)(cid:13)
(cid:13) (cid:13) (cid:13)
(cid:13)𝑁 𝑁11
∑︁
𝑖
∇𝑁𝑁 =1
1
𝑓∑︁ ∑︁𝑖 𝑖𝑁=
=
(∑︁
E𝑖1
1
𝜃𝑁
=
𝑡
𝑡(cid:104)
1∇
(cid:13) (cid:13)
(cid:13)∇
)(cid:104)
∇
(cid:13)
(cid:13)𝑓 ∇𝑖
𝑓
2𝑓𝑖( ,𝑖𝑓(𝜃
𝑖
(𝜃
(𝑡
𝜃𝑡
𝜃) 𝑡)(cid:13) (cid:13) (cid:13) (cid:13)
𝑡
))− −−∇ ∇∇𝑓 𝑓𝑓( (𝜃
(
𝜃𝑡
𝜃
𝑡)
𝑡
))+
(cid:13) (cid:13)
(cid:13)(cid:105)
2∇
(cid:13)
(cid:13) (cid:13) (cid:13) (cid:13)
+2𝑓
+
2( E𝜃 2𝑡 𝑡E)
(cid:13)
(cid:13)𝑡(cid:105)
∇(cid:13)
(cid:13) (cid:13) (cid:13)
(cid:13)(cid:13)
(cid:13) (cid:13) (cid:13)
(cid:13)
𝑓2
𝑁1 (𝜃∑︁ 𝑖
𝑡𝑁
= )1
(cid:13)
(cid:13)2∇𝑓(𝜃𝑡
)(cid:13)
(cid:13) (cid:13) (cid:13) (cid:13)
(2
34)
𝑇 2 =≤
≤
𝑁𝑁 𝑁11 222
2∑︁
𝑖∑︁
∑︁𝑖 𝑖𝑁
=𝑁
𝑁= =11 1𝑘∑︁𝑘
𝑘∑︁ ∑︁𝐻𝐻
𝐻== =11
1EE E𝑡𝑡
𝑡(cid:13)
(cid:13) (cid:13)
(cid:13)
(cid:13)(cid:13) (cid:13) (cid:13) (cid:13)
(cid:13)
(cid:13)
(cid:13) (cid:13)
(cid:13)
(cid:13)2𝑧2 2𝑧 𝑧𝜇𝑖′𝜇 𝜇𝑖 𝑖′ ′(cid:16)( (cid:16)𝐹𝐹 𝐹𝑖𝑖 𝑖(( (𝜃𝜃 𝜃𝑖𝑖
′
𝑖′
′+
++ +𝜇𝜇
(cid:16)
𝜇𝐹𝑧𝑧
𝑧𝑖
𝑖′𝑖 𝑖′ ′,,
(
,B
𝜃B B𝑖′𝑖𝑖
′
𝑖′
,
′)) )B−−
−𝑖′
)𝐹𝐹 𝐹−𝑖𝑖 𝑖((
(𝜃
𝐹𝜃
𝜃𝑖
𝑖𝑖
′
𝑖′
′,
(
,−
B
𝜃
B𝑖′𝑖′
𝑖𝜇
′)
−
)𝑧
(cid:17)
(cid:17)𝑖′ 𝜇,
(cid:13)
(cid:13) (cid:13)
(cid:13)
(cid:13)B
𝑧
2𝑖′𝑖′ ,) B(cid:13) (cid:13) (cid:13) (cid:13)
(cid:13)
𝑖2
′ )(cid:17)(cid:13) (cid:13) (cid:13)
(cid:13)
(cid:13)2
w eqh ue ar le ityth ,e thfi er ss et ci on neq du ia nl eit qy uafo lil tl yow fos llf or wom sft rh oe mC Ja eu nc sh eny ’- sSc inh ew qa ur az lii tn y-
,
+ 𝑁2
2
∑︁𝑁 ∑︁𝐻 E 𝑡(cid:13) (cid:13) (cid:13) (cid:13)2𝑧 𝜇𝑖′ (cid:16) 𝐹 𝑖(𝜃 𝑖′,B 𝑖′ )−𝐹 𝑖(𝜃 𝑖′ −𝜇𝑧 𝑖′,B 𝑖′ )(cid:17)(cid:13) (cid:13) (cid:13) (cid:13)2 , (36)
andthethirdequalityisderivedfromAssumption5.
𝑖=1𝑘=1 (cid:13) (cid:13)
Fortheterm𝑇 2,byapplyingJensen’sinequality,weobtain: wheretheinequalityfollowsfromthefactthat(∥𝑎+𝑏∥)2 ≤2(∥𝑎∥)2+
𝑁 𝐻 2(∥𝑏∥)2.Duetothesymmetryofthefunction𝐹 𝑖 whenperturbed
𝑇 2 ≤ 𝑁1 2 ∑︁ 𝑖=1𝑘∑︁ =1E 𝑡(cid:13) (cid:13) (cid:13)𝑒 𝑖(𝑡,𝑘)(cid:13) (cid:13) (cid:13)2 , (35) w eqi uth ivG aa leu ns ts .ia Hn e- nd cis etr 𝑇i 2bu ct ae nd b𝑧 e𝑖′ , tb rao nth sft oe rr mm es do in ntt oheright-handsideare
12𝑇
2
≤ 𝑁1
2
∑︁ 𝑖𝑁 =1𝑘∑︁𝐻 =1E 𝑡(cid:13) (cid:13) (cid:13)
(cid:13)
(cid:13)𝑧 𝜇𝑖′ (cid:16) 𝐹 𝑖(𝜃 𝑖(𝑡,𝑘) +𝜇𝑧 𝑖′,B 𝑖′ )−𝐹 𝑖(𝜃 𝑖′,B 𝑖′ )(cid:17)(cid:13) (cid:13) (cid:13)
(cid:13)
(cid:13)2
𝑠(𝑡,𝜏) =𝜂2 𝑁1
∑︁𝑁
E𝜏
𝑡−1(cid:13)
(cid:13) (cid:13)
(cid:13)∑︁𝜏
𝑒
𝑖(𝑡,𝑘)(cid:13)
(cid:13) (cid:13)
(cid:13)2
= 𝑁21 ·𝑑2 ∑︁ 𝑖𝑁 =1𝑘∑︁𝐻 =1E 𝑡(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)𝑑· 𝜇𝑧 𝑖′ (cid:16) 𝐹 𝑖(𝜃 𝑖′ +𝜇𝑧 𝑖′,B 𝑖′ )−𝐹 𝑖(𝜃 𝑖′,B 𝑖′ )(cid:17)(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)2 ≤𝜏𝜂2∑︁𝜏𝑖=1 𝑁1 ∑︁𝑁 (cid:13) E𝑘 𝑘 𝑡= (cid:13) (cid:13) (cid:13)1 𝑒 𝑖(𝑡,𝑘)(cid:13) (cid:13) (cid:13)(cid:13) 2 . (41)
≤ 𝑁21
·𝑑2
∑︁𝑁 ∑︁𝐻 (cid:34)
2𝑑·E 𝑡(cid:13) (cid:13) (cid:13)∇𝐹 𝑖(𝜃 𝑖′,B 𝑖′ )(cid:13) (cid:13) (cid:13)2 + 𝜇 22
𝐿2𝑑2(cid:35) 𝑘=1 𝑖=1
𝑖=1𝑘=1
𝑁 𝐻 (cid:34) (cid:35) BycombingEq.(37),Eq.(38)andEq.(41),wehave
≤ 𝑁21
·𝑑2
∑︁∑︁ 2𝑐 𝑔𝑑·E 𝑡(cid:13) (cid:13) (cid:13)∇𝑓 𝑖(𝜃 𝑖′ )(cid:13) (cid:13) (cid:13)2 +2𝑑𝜎 𝑔2+ 𝜇 22 𝐿2𝑑2 ,
𝑖=1𝑘=1
(37)
𝜏
𝑠(𝑡,𝜏) ≤6𝑐 𝑔𝑑𝐿2𝜏𝜂2∑︁ 𝑠(𝑡,𝑘) +6𝑐 𝑔𝑑𝜏2𝜂2E 𝑡(cid:13) (cid:13)∇𝑓(𝜃𝑡 )(cid:13) (cid:13)2
wherethefirstinequalityfollowsLemma4.1from[21],andthe
𝑘=1
secondinequalityfollowstheAssumption4.Fortheexpectation
𝜇2𝐿2𝑑2𝜏2𝜂2
term,wehave +2𝑑𝜎 𝑔2𝜏2𝜂2+ . (42)
2
E 𝑡(cid:13) (cid:13) (cid:13)∇𝑓 𝑖(𝜃 𝑖(𝑡,𝑘) )(cid:13) (cid:13) (cid:13)2 =E 𝑡(cid:13) (cid:13) (cid:13)∇𝑓 𝑖(𝜃 𝑖(𝑡,𝑘) )∓∇𝑓 𝑖(𝜃𝑡 )∓∇𝑓(𝜃 𝑖𝑡 )(cid:13) (cid:13) (cid:13)2 Bytakingsummationover𝜏 from2to𝐻,andutilizingtheprop-
ertyofarithmeticsequence,weobtain
≤3𝐿2E 𝑡(cid:13) (cid:13) (cid:13)𝜃 𝑖(𝑡,𝑘) −𝜃𝑡(cid:13) (cid:13) (cid:13)2 +3E 𝑡(cid:13) (cid:13) (cid:13)∇𝑓(𝜃𝑡 )(cid:13) (cid:13) (cid:13)2 , (38)
where the inequality is due to the Cauchy-Schwartz inequality,
𝐻 𝐻 𝜏
(𝐿 3- 7s )m ,fio no ath llypr wo epe cr at ny ba on ud nA ds 𝑇s 2u am s:ption5.BytakingEq.(38)intoEq. 𝜏∑︁ =2𝑠(𝑡,𝜏) ≤6𝑐 𝑔𝑑𝐿2𝜂2 𝜏∑︁ =2𝜏 𝑘∑︁ =1𝑠(𝑡,𝑘) +𝐶
1
𝐻
𝑇
2
≤ 6 𝑁𝑐 𝑔 2𝐿 𝑑2 ∑︁ 𝑖𝑁 =1𝑘∑︁𝐻 =1E 𝑡(cid:13) (cid:13) (cid:13)𝜃 𝑖(𝑡,𝑘) −𝜃𝑡(cid:13) (cid:13) (cid:13)2
where𝐶 1=2𝑐
𝑔𝑑𝐻3𝜂≤ 2E3 𝑡𝑐
(cid:13)
(cid:13)𝑔 ∇𝑑𝐻 𝑓(2 𝜃𝐿 𝑡2 )𝜂
(cid:13)
(cid:13)2 2𝑘∑︁ +=1 2𝑠 𝑑( 𝜎𝑡, 𝑔2𝑘 𝐻) + 3𝜂𝐶 21 +, 𝜇2𝐿2𝑑2𝐻3𝜂( 243 .)
+ 6 𝑁𝑐 𝑔 𝑑𝐻 E 𝑡(cid:13) (cid:13) (cid:13)∇𝑓(𝜃𝑡 )(cid:13) (cid:13) (cid:13)2 + 2𝐻 𝑁𝜎 𝑑𝑔2 + 𝜇2 2𝐻 𝑁𝐿2 . (39) 3 6
As𝑠(𝑡,1)
=0,afterrearrangingEq.(43),wehave
AftercombiningEq.(16),Eq.(34)andEq.(39),wehave
𝐻
E 𝑡(cid:2)𝑓(𝜃𝑡+1)(cid:3) ≤ 𝑓(𝜃𝑡 )− 𝛾2 𝜂E 𝑡(cid:13) (cid:13) (cid:13)∇𝑓(𝜃𝑡 )(cid:13) (cid:13) (cid:13)2 + 3𝑐 𝑔𝜁 𝑁𝜂 𝑑2𝐻𝐿 E 𝑡(cid:13) (cid:13) (cid:13)∇𝑓(𝜃𝑡 )(cid:13) (cid:13) (cid:13)2 (1−3𝑐 𝑔𝑑𝐻2𝐿2𝜂2)∑︁ 𝑠(𝑡,𝑘) ≤𝐶 1. (44)
𝑘=1
+ 3𝑐 𝑔 𝑁𝜁𝜂 2𝑑2𝐿3 ∑︁𝑁 ∑︁𝐻 E 𝑡(cid:13) (cid:13) (cid:13)𝜃 𝑖(𝑡,𝑘) −𝜃𝑡(cid:13) (cid:13) (cid:13)2 +𝜎 𝑔2𝜁 𝑁𝜂 𝑑2𝐻𝐿 +𝜁𝜂2 4𝜇 𝑁2𝐻𝐿3
𝑖=1𝑘=1 Forsimplification,herewedenote(1−3𝑐 𝑔𝑑𝐻2𝐿2𝜂2)as𝐶 0.When
=𝑓(𝜃𝑡 )+(cid:32) 3𝑐 𝑔𝜁 𝑁𝜂 𝑑2𝐻𝐿 − 𝛾2 ·𝜂(cid:33) E 𝑡(cid:13) (cid:13) (cid:13)∇𝑓(𝜃𝑡 )(cid:13) (cid:13) (cid:13)2 𝜂 ≤ 3𝐻𝐿√1 𝑐𝑔𝑑,𝐶 0 ≥ 32.Underthiscondition,wehave:
+ 3𝑐 𝑔 𝑁𝜁𝜂 2𝑑2𝐿3 ∑︁ 𝑖𝑁 =1𝑘∑︁𝐻 =1E 𝑡(cid:13) (cid:13) (cid:13)𝜃 𝑖(𝑡,𝑘) −𝜃𝑡(cid:13) (cid:13) (cid:13)2 +𝜎 𝑔2𝜁 𝑁𝜂 𝑑2𝐻𝐿 +𝜁𝜂2 4𝜇 𝑁2𝐻𝐿3 .
𝑁1
∑︁𝑁 ∑︁𝐻
E 𝑡(cid:13) (cid:13) (cid:13)𝜃 𝑖(𝑡,𝑘) −𝜃𝑡(cid:13) (cid:13) (cid:13)2
=∑︁𝐻
𝑠(𝑡,𝑘) ≤ 𝐶 𝐶1
(40) 𝑖=1𝑘=1 𝑘=1 0
(cid:32) (cid:33)
NextweneedtoboundE 𝑡(cid:13) (cid:13) (cid:13)𝜃 𝑖(𝑡,𝑘) −𝜃𝑡(cid:13) (cid:13) (cid:13)2 andsimplifyEq.(40). = 𝐶1 0 2𝑐 𝑔𝑑𝐻3𝜂2E 𝑡(cid:13) (cid:13)∇𝑓(𝜃𝑡 )(cid:13) (cid:13)2+ 2 3𝑑𝜎 𝑔2𝐻3𝜂2+ 𝜇2𝐿2𝑑 62𝐻3𝜂2
Specifically,bydenoting 𝑁1 (cid:205) 𝑖𝑁 =1E𝑘 𝑡−1∥𝜃 𝑖(𝑡,𝑘) −𝜃𝑡∥2as𝑠(𝑡,𝑘),we ≤3𝑐 𝑔𝑑𝐻3𝜂2E 𝑡(cid:13) (cid:13)∇𝑓(𝜃𝑡 )(cid:13) (cid:13)2+𝑑𝜎 𝑔2𝐻3𝜂2+ 𝜇2𝐿2𝑑2𝐻3𝜂2 . (45)
have 4
13TakingEq.(45)intoEq.(40),weobtainthefinalresultofloss □
descentofeachstep:
E 𝑡(cid:2)𝑓(𝜃𝑡+1)(cid:3) ≤ 𝑓(𝜃𝑡 )+(cid:32) 3𝑐 𝑔𝜁 𝑁𝜂 𝑑2𝐻𝐿 − 𝛾2 ·𝜂(cid:33) E 𝑡(cid:13) (cid:13) (cid:13)∇𝑓(𝜃𝑡 )(cid:13) (cid:13) (cid:13)2 D.3 ProofofTheorem3.4
StartingfromEq.(16),undertheassumptionofglobal-localdissim-
+
3𝑐 𝑔𝜁𝜂2𝐿3 ·𝐶
1
+𝜎 𝑔2𝜁𝜂2𝐻𝐿 +𝜁𝜂2𝜇2𝐻𝐿3 ilarity(Assumption6),𝑇 1becomes:
𝑁𝑑 𝐶
0
𝑁𝑑 4𝑁
≤ 𝑓(𝜃𝑡 )+(cid:32) 3𝑐 𝑔𝜁 𝑁𝜂 𝑑2𝐻𝐿 − 𝛾2 ·𝜂+ 9𝑐 𝑔2𝜁𝜂 𝑁4𝐻3𝐿3(cid:33) E 𝑡(cid:13) (cid:13) (cid:13)∇𝑓(𝜃𝑡 )(cid:13) (cid:13) (cid:13)2
𝑇 1=E
𝑡(cid:13)
(cid:13) (cid:13) (cid:13)𝑁1
∑︁𝑁
∇𝑓 𝑖(𝜃𝑡
)(cid:13)
(cid:13) (cid:13)
(cid:13)2
3𝑐 𝑔𝜎 𝑔2𝜁𝜂4𝐻3𝐿3 3𝑐 𝑔𝑑𝜁𝜂4𝜇2𝐻3𝐿5 𝜎 𝑔2𝜁𝜂2𝐻𝐿 𝜁𝜂2𝜇2𝐻𝐿3 (cid:13) 𝑖=1 (cid:13)
≤+ 𝑓(𝜃𝑡 )+(cid:32)𝑁 3𝑐 𝑔𝜁 𝑁𝜂 𝑑2𝐻+ 𝐿 +3𝑐 𝑔𝑑4 𝜂𝑁 2𝐻2𝐿23𝑐+ 𝑔𝜁 𝑁𝜂 𝑑2𝐻𝑁 𝐿𝑑 (cid:33) E 𝑡(cid:13) (cid:13) (cid:13)+ ∇𝑓(𝜃4 𝑡𝑁 )(cid:13) (cid:13) (cid:13)2 =E 𝑡(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)𝑁1 ∑︁ 𝑖𝑁 =1(cid:104) ∇𝑓 𝑖(𝜃𝑡 )−∇𝑓(𝜃𝑡 )+∇𝑓(𝜃𝑡 )(cid:105)(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)2
(cid:13) 𝑁 (cid:13)2 (cid:13) 𝑁 (cid:13)2
− 2 𝛾𝜂 E 𝑡(cid:13) (cid:13) (cid:13)∇𝑓(𝜃𝑡 )(cid:13) (cid:13) (cid:13)2 +3𝑐 𝑔𝑑𝜂2𝐻2𝐿2· 𝜎 𝑔2𝜁 𝑁𝜂 𝑑2𝐻𝐿 ≤2E 𝑡(cid:13) (cid:13) (cid:13) (cid:13)𝑁1 ∑︁ 𝑖=1(cid:104) ∇𝑓 𝑖(𝜃𝑡 )−∇𝑓(𝜃𝑡 )(cid:105)(cid:13) (cid:13) (cid:13) (cid:13) +2E 𝑡(cid:13) (cid:13) (cid:13) (cid:13)𝑁1 ∑︁ 𝑖=1∇𝑓(𝜃𝑡 )(cid:13) (cid:13) (cid:13) (cid:13)
𝑁
+3𝑐 𝑔𝑑𝜂2𝐻2𝐿2· 𝜁𝜂2𝜇2𝐻𝐿3 +𝜎 𝑔2𝜁𝜂2𝐻𝐿 +𝜁𝜂2𝜇2𝐻𝐿3 ≤ 𝑁2
2
∑︁ E 𝑡(cid:13) (cid:13) (cid:13)∇𝑓 𝑖(𝜃𝑡 )−∇𝑓(𝜃𝑡 )(cid:13) (cid:13) (cid:13)2 +2E 𝑡(cid:13) (cid:13)∇𝑓(𝜃𝑡 )(cid:13) (cid:13)2
4𝑁 𝑁𝑑 4𝑁 𝑖=1
≤ 𝑓(𝜃𝑡 )+(cid:32) (1+3𝑐 𝑔𝑑𝐻2𝐿2𝜂2)3𝑐 𝑔𝜁 𝑁𝜂 𝑑2𝐻𝐿 − 𝛾2 ·𝜂(cid:33) E 𝑡(cid:13) (cid:13) (cid:13)∇𝑓(𝜃𝑡 )(cid:13) (cid:13) (cid:13)2 ≤ 𝑁2
2
∑︁𝑁 E 𝑡(cid:104) 𝑐 ℎ(cid:13) (cid:13)∇𝑓(𝜃𝑡 )(cid:13) (cid:13)+𝜎 ℎ2(cid:105) +2E 𝑡(cid:13) (cid:13)∇𝑓(𝜃𝑡 )(cid:13) (cid:13)2
𝑖=1
+(1+3𝑐 𝑔𝑑𝐻2𝐿2𝜂2)𝜎 𝑔2𝜁 𝑁𝜂 𝑑2𝐻𝐿 +(1+3𝑐 𝑔𝑑𝐻2𝐿2𝜂2)𝜁𝜂2 4𝜇 𝑁2𝐻𝐿3 . = 2(𝑁 𝑁+𝑐 ℎ) E 𝑡(cid:13) (cid:13)∇𝑓(𝜃𝑡 )(cid:13) (cid:13)2+ 2 𝑁𝜎 ℎ2 , (50)
(46)
Withthecondition𝜂 ≤ √1 ,wehave(1+3𝑐 𝑔𝑑𝐻2𝐿2𝜂2) ≤2.
3𝐻𝐿 𝑐𝑔𝑑 wherethefirstinequalityfollowsCauchy-Schwartz,thesecondin-
Thentakingthecondition𝜂 ≤ 3𝑐𝑔𝑁 𝐻𝐿 and𝜂 ≤ 𝐻1 2,wecantransform equalityfollowsJensen’sinequality,andthethirdinequalityfollows
Assumption6.
Eq.(46)intotheresultofTheorem3.1as:
(cid:32) (cid:33)
Thenwebegintoboundtheterm𝑇 2.TakingtheresultfromEq.
E 𝑡(cid:2)𝑓(𝜃𝑡+1)(cid:3) ≤ 𝑓(𝜃𝑡 )+2 𝑑𝜁 𝜂− 𝛾1 𝜂 E 𝑡(cid:13) (cid:13) (cid:13)∇𝑓(𝜃𝑡 )(cid:13) (cid:13) (cid:13)2 (37),wehave:
+
2𝜎 𝑁𝑔2 𝐻𝜁𝜂 𝑑𝐿 +𝜁 2𝜂 𝑁𝜇2 𝐻𝐿3
. (47) 𝑇
2
≤ 𝑁21
·𝑑2
∑︁𝑁 ∑︁𝐻 (cid:34)
2𝑐 𝑔𝑑·E 𝑡(cid:13) (cid:13) (cid:13)∇𝑓 𝑖(𝜃 𝑖(𝑡,𝑘) )(cid:13) (cid:13) (cid:13)2 +2𝑑𝜎 𝑔2+ 𝜇 22
𝐿2𝑑2(cid:35)
,
□ 𝑖=1𝑘=1
(51)
D.2 ProofofCorollary(3.2) wherethefirstinequalityfollowsLemma4.1from[21],andthesec-
Togettheresultofglobalconvergence,werearrangeEq.(47)and ondinequalityfollowstheAssumption4.ThetermE 𝑡(cid:13) (cid:13) (cid:13)∇𝑓 𝑖(𝜃 𝑖(𝑡,𝑘) )(cid:13) (cid:13) (cid:13)2
similarlybound𝜂as𝜂 ≤ 1 .Forsimplicity,wedenotes 𝑑−𝜁𝛾 asΓ inEq.(51)canbeboundedas:
𝐻2 𝑑𝛾
andhave
2𝜂ΓE 𝑡(cid:13) (cid:13) (cid:13)∇𝑓(𝜃𝑡 )(cid:13) (cid:13) (cid:13)2 ≤ 𝑓(𝜃𝑡 )−E 𝑡(cid:2)𝑓(𝜃𝑡+1)(cid:3) + 2 𝑁𝜎 𝑔 𝐻2𝜁 𝑑𝐿 𝜂+𝜁 2𝜇 𝑁2𝐿 𝐻3 𝜂 E 𝑡(cid:13) (cid:13) (cid:13)∇𝑓 𝑖(𝜃 𝑖(𝑡,𝑘) )(cid:13) (cid:13) (cid:13)2 =E 𝑡(cid:13) (cid:13) (cid:13)∇𝑓 𝑖(𝜃 𝑖(𝑡,𝑘) )∓∇𝑓 𝑖(𝜃𝑡 )∓∇𝑓(𝜃 𝑖𝑡 )(cid:13) (cid:13) (cid:13)2
E 𝑡(cid:13) (cid:13) (cid:13)∇𝑓(𝜃𝑡 )(cid:13) (cid:13) (cid:13)2 ≤ 𝑓(𝜃𝑡)−E 2𝜂𝑡 Γ(cid:2)𝑓(𝜃𝑡+1)(cid:3) + Γ𝜎 𝑁𝑔2𝜁 𝐻𝐿
𝑑
+ 4𝜁 Γ𝜇 𝑁2𝐿 𝐻3 . ≤3𝐿2E 𝑡(cid:13) (cid:13) (cid:13)𝜃 𝑖(𝑡,𝑘) −𝜃𝑡(cid:13) (cid:13) (cid:13)2 +3(𝑐 ℎ+1)E 𝑡(cid:13) (cid:13) (cid:13)∇𝑓(𝜃𝑡 )(cid:13) (cid:13) (cid:13)2 +3𝜎 ℎ2, (52)
(48)
Noticethat𝛾 =Θ(𝑟)and𝜁 =Θ( 1 ),sinceparameter𝑑isalarge
wheretheinequalityfollowstheCauchy-Schwartz,𝐿-smoothand
𝑟𝑑
Assumption6.ByapplyingEq.(52)intoEq.(51),weobtain:
number,𝑑−𝜁𝛾 =𝑑−Θ(1) =𝑑,hencethedominanttermofΓis
𝑑
𝛾1,whichfollowsΓ=Θ( 𝑟1).Thensimultaneouslysummingover𝑇
rou 𝑇1n ∑︁ 𝑡d 𝑇 =s 0o En 𝑡(cid:13)
(cid:13)
(cid:13)b ∇ot 𝑓h (𝜃si 𝑡d )e (cid:13)
(cid:13)
(cid:13)s 2a ≤nd 𝑓t (a 𝜃k 0i )n −g 2𝜂t Eh 𝑇𝑡e Γ(cid:2)a 𝑓v (𝜃er 𝑇a )g (cid:3)e:
+
Γ𝜎 𝑁𝑔2𝜁 𝐻𝐿
𝑑
+
4𝜁 Γ𝜇 𝑁2𝐿 𝐻3
(4.
9)
𝑇 2 ≤ 6 𝑁𝑐 𝑔 2𝐿 𝑑2 ∑︁ 𝑖𝑁 =1𝑘∑︁𝐻 =1E 𝑡(cid:13) (cid:13) (cid:13)𝜃 𝑖(𝑡, +𝑘) 6−
𝑐 𝑔
𝑁𝜃 𝜎𝑡 𝑑ℎ2(cid:13) (cid:13) (cid:13) 𝐻2 + +6 2𝑐
𝐻
𝑁𝑔(
𝜎
𝑑𝑐 𝑔ℎ 2𝑁+ 𝑑 +𝑁 𝜇2) 2𝐻𝐻 𝑁𝐿E 2𝑡 .(cid:13) (cid:13) (cid:13)∇𝑓(𝜃𝑡 () 5(cid:13) (cid:13) (cid:13) 32
)
14CombiningEq.(16),Eq.(50)andEq.(53),wehave: Let𝐶 0be(1−3𝑐 𝑔𝑑𝐻2𝐿2𝜂2).When𝜂 ≤ 3𝐻𝐿√1 𝑐𝑔𝑑,𝐶 0 ≥ 2 3.Under
thiscondition,wehave:
E 𝑡(cid:2) + +𝑓( 3 3𝜃 𝑐 𝑐𝑡 𝑔
𝑔
𝑁+ 𝜁(1 𝑐
𝜂
2) ℎ 𝑑2(cid:3) 𝐿+ 𝑁≤ 3𝑁 𝑑 ∑︁𝑓 𝑁)( 𝜁𝜃 𝜂 ∑︁𝑡 𝐻2) 𝐻− E𝐿𝛾 𝑡E
(cid:13)
(cid:13)
(cid:13)2 𝑁 𝑡 𝜃(cid:13) (cid:13) (cid:13) 𝑖(( ∇ 𝑡𝑁 ,𝑘𝑓 )(+ 𝜃 −𝑐 𝑡ℎ ) 𝜃(cid:13) (cid:13) (cid:13)) 𝑡𝜂 2
(cid:13)
(cid:13)
(cid:13)E 2+𝑡 +(cid:13) (cid:13) (cid:13) 3∇ 𝑐 𝜎𝑔 𝑔𝑓 2𝜎 𝜁( ℎ 𝑁2 𝑁𝜃 𝜂𝜁𝑡 𝑑2𝑑𝜂) 𝐻2(cid:13) (cid:13) (cid:13) 𝐻2 𝐿− 𝐿 +𝛾 𝜁2 𝑁 𝜂2𝜎 4𝜇ℎ2 𝑁2𝜂
𝐻𝐿3
𝑁1 ∑︁ ≤𝑖𝑁 =1 3𝑐𝑘∑︁𝐻 𝑔= (1 𝑐E ℎ𝑡 +(cid:13) (cid:13) (cid:13)𝜃 1𝑖( )𝑡 𝑑,𝑘 𝜂) 2𝐻− 3𝜃 E𝑡(cid:13) (cid:13) (cid:13) 𝑡(cid:13) (cid:13)2 ∇ += 𝑓 𝜎𝑘∑︁ (𝐻 𝑔2= 𝜃 𝑑1 𝑡 𝜂)𝑠 (cid:13) (cid:13) 2(𝑡 2 𝐻,𝑘 + 3) 3 +≤ 𝑐 𝑔 𝑑𝐶 𝐶 𝜎 2ℎ 𝜂22 0 𝑑 2𝜂 𝜇2 2𝐻 𝐻3
3𝐿2
. (59)
𝑖=1𝑘=1 4
=𝑓(𝜃𝑡 )+(cid:32) 3𝑐 𝑔(𝑐 ℎ+ 𝑁𝑁 𝑑)𝜁𝜂2𝐻𝐿 − 𝛾2
𝑁
·(𝑁 +𝑐 ℎ)𝜂(cid:33) E 𝑡(cid:13) (cid:13) (cid:13)∇𝑓(𝜃𝑡 )(cid:13) (cid:13) (cid:13)2 Eq.L (e 5t 4(cid:101)𝑐 )ℎ ,wbe e( c𝑐 aℎ n+ o𝑁 bt) aia nnd th(cid:101)𝜎 e2 fib ne al(3 r𝑐 e𝑔 s𝜎 uℎ l2 t+ o𝜎 f𝑔 s2 t) e. pA wp ip sl eyi ln og ssE dq e. s( c5 e9 n) ti :nto
+ 3𝑐 𝑔 𝑁𝜁𝜂 2𝑑2𝐿3 ∑︁𝑁 ∑︁𝐻 E 𝑡(cid:13) (cid:13) (cid:13)𝜃 𝑖(𝑡,𝑘) −𝜃𝑡(cid:13) (cid:13) (cid:13)2 + 3𝑐 𝑔𝜎 ℎ2 𝑁𝜁 𝑑𝜂2𝐻𝐿 E 𝑡(cid:2)𝑓(𝜃𝑡+1)(cid:3) ≤ 𝑓(𝜃𝑡 )+ 3𝜁𝜂2𝐿𝐻 𝑁𝑐 𝑔 𝑑(𝑐 ℎ+𝑁) E 𝑡(cid:13) (cid:13) (cid:13)∇𝑓(𝜃𝑡 )(cid:13) (cid:13) (cid:13)2
𝑖=1𝑘=1
+𝜎 𝑔2𝜁 𝑁𝜂 𝑑2𝐻𝐿 +𝜁𝜂2 4𝜇 𝑁2𝐻𝐿3
−
𝛾2
𝑁
·𝜎 ℎ2𝜂. (54)
− 𝛾2
𝑁
·𝜂(𝑁 +𝑐 ℎ)E 𝑡(cid:13) (cid:13) (cid:13)∇𝑓(𝜃𝑡 )(cid:13) (cid:13) (cid:13)2 + 3𝜁𝜂 𝑁2𝐿 𝑑3𝑐 𝑔 · 𝐶𝐶 02
+
3𝜁𝜂2𝑐 𝑔𝜎 ℎ2𝐻𝐿 +𝜁𝜂2𝐻𝜎 𝑔2𝐿 +𝜁𝜂2𝜇2𝐻𝐿3
−
2
·𝜂𝜎2
SimilartoSectionD.1,nowweboundE 𝑡(cid:13) (cid:13) (cid:13)𝜃 𝑖(𝑡,𝑘) −𝜃𝑡(cid:13) (cid:13) (cid:13)2 asfollows: ≤ 𝑓(𝜃𝑡 )+(cid:32)𝑁 3𝜁𝑑 𝜂2 𝑁𝐿𝐻 𝑑𝑐 𝑔(cid:101)𝑐 ℎ −𝑁 2𝑑 𝛾𝜂 𝑁(cid:101)𝑐 ℎ + 9𝜁𝑐 𝑔4 2 (cid:101)𝑐𝑁 ℎ 𝑁𝐻3𝐿3𝜂𝛾 4𝑁 (cid:33) E 𝑡(cid:13) (cid:13) (cid:13)∇ℎ 𝑓(𝜃𝑡 )(cid:13) (cid:13) (cid:13)2
𝑁 (cid:13) 𝜏 (cid:13)2
𝑠(𝑡,𝜏) =𝜂2 𝑁1 ∑︁ 𝑖=1E𝜏 𝑡−1(cid:13) (cid:13)
(cid:13)
(cid:13)𝑘∑︁ =1𝑒 𝑖(𝑡,𝑘)(cid:13) (cid:13)
(cid:13) (cid:13) +
3𝑐 𝑔(cid:101)𝜎2𝜁 𝑁𝐻3𝐿3𝜂4
+
3𝑐 𝑔𝜁𝑑𝜇 42 𝑁𝐻3𝐿5𝜂4 +(cid:101)𝜎2𝜁 𝑁𝐻 𝑑𝐿𝜂2
≤𝜏𝜂2∑︁𝜏 𝑁1 ∑︁𝑁 E𝑘 𝑡(cid:13) (cid:13) (cid:13)𝑒 𝑖(𝑡,𝑘)(cid:13) (cid:13) (cid:13)2 , (55) +𝜁𝜂2 4𝜇 𝑁2𝐻𝐿3 − 𝛾2 𝑁 ·𝜂𝜎 ℎ2
where𝑠(𝑡,𝑘) denotes 𝑁1 (cid:205) 𝑖𝑁
=𝑘 1= E1
𝑘
𝑡−1∥𝑖= 𝜃1
𝑖(𝑡,𝑘) −𝜃𝑡∥2.
≤ 𝑓(𝜃𝑡 )+(cid:32) (1+3𝑐 𝑔𝑑𝐻2𝐿2𝜂2)3𝜁𝜂2 𝑁𝐿𝐻 𝑑𝑐 𝑔(cid:101)𝑐 ℎ(cid:33) E 𝑡(cid:13) (cid:13) (cid:13)∇𝑓(𝜃𝑡 )(cid:13) (cid:13) (cid:13)2
BycombingEq.(51),Eq.(52)andEq.(55),wehave − 𝛾2 𝑁𝜂 (cid:101)𝑐 ℎE 𝑡(cid:13) (cid:13) (cid:13)∇𝑓(𝜃𝑡 )(cid:13) (cid:13) (cid:13)2 +3𝑐 𝑔𝑑𝐻2𝐿2𝜂2 𝑁(cid:101)𝜎2 𝑑𝜁𝐻𝐿𝜂2− 𝛾2 𝑁𝜂𝜎 ℎ2
𝜏
𝑠(𝑡,𝜏) ≤6𝑐 𝑔𝑑𝐿2𝜏𝜂2∑︁ 𝑠(𝑡,𝑘) +6𝑐 𝑔𝑑(𝑐 ℎ+1)𝜏2𝜂2E 𝑡(cid:13) (cid:13)∇𝑓(𝜃𝑡 )(cid:13) (cid:13)2 +3𝑐 𝑔𝑑𝐻2𝐿2𝜂2𝜁𝜂2𝜇2𝐻𝐿3 + (cid:101)𝜎2 𝜁𝐻𝐿𝜂2+𝜁𝜂2𝜇2𝐻𝐿3
𝑘=1
4𝑁 𝑁𝑑 4𝑁
+6𝑐 𝑔𝑑𝜎 ℎ2𝜏2𝜂2+2𝑑𝜎 𝑔2𝜏2𝜂2+ 𝜇2𝐿2𝑑2𝜏2𝜂2 . (56) ≤ 𝑓(𝜃𝑡 )+(cid:32) (1+3𝑐 𝑔𝑑𝐻2𝐿2𝜂2)3𝜁𝜂2 𝑁𝐿𝐻 𝑑𝑐 𝑔(cid:101)𝑐 ℎ − 2 𝛾𝜂 𝑁(cid:101)𝑐 ℎ(cid:33) E 𝑡(cid:13) (cid:13) (cid:13)∇𝑓(𝜃𝑡 )(cid:13) (cid:13) (cid:13)2
2
𝜎2
Bytakingsummationover𝜏 from2to𝐻,andutilizingtheprop- +(1+3𝑐 𝑔𝑑𝐻2𝐿2𝜂2)· (cid:101) 𝜁𝐻𝐿𝜂2
𝑁𝑑
ertyofarithmeticsequence,weobtain
𝜁𝜂2𝜇2𝐻𝐿3 2
𝐻 𝐻 𝜏
+(1+3𝑐 𝑔𝑑𝐻2𝐿2𝜂2)·
4𝑁
−
𝛾𝑁
·𝜂𝜎 ℎ2. (60)
∑︁ 𝑠(𝑡,𝜏) ≤6𝑐 𝑔𝑑𝐿2𝜂2∑︁ 𝜏∑︁ 𝑠(𝑡,𝑘) +𝐶
2 Withthecondition𝜂 ≤ √1 ,wehave(1+3𝑐 𝑔𝑑𝐻2𝐿2𝜂2) ≤2.
𝜏=2 𝜏=2 𝑘=1 3𝐻𝐿 𝑐𝑔𝑑
≤3𝑐
𝑔𝑑𝐻2𝐿2𝜂2∑︁𝐻
𝑠(𝑡,𝑘) +𝐶 2 (57)
Taking𝜂 ≤ 3𝑐𝑔𝑁 𝐻𝐿,𝜂 ≤ 𝐻1 2,Eq.(60)becomes:
(cid:32) (cid:33)
where 𝐶 2=2𝑐 𝑔𝑑(𝑐 ℎ+1)𝐻3𝜂2E𝑘 𝑡= (cid:13) (cid:13)1 ∇𝑓(𝜃𝑡 )(cid:13) (cid:13)2 E 𝑡(cid:2)𝑓(𝜃𝑡+1)(cid:3) ≤ 𝑓(𝜃𝑡 )+2 𝑑𝜁 − 𝛾1 𝑁 (cid:101)𝑐 ℎ𝜂E 𝑡(cid:13) (cid:13) (cid:13)∇𝑓(𝜃𝑡 )(cid:13) (cid:13) (cid:13)2
+2𝑐 𝑔𝑑𝜎 ℎ2𝐻3𝜂2+ 2 3𝑑𝜎 𝑔2𝐻3𝜂2+ 𝜇2𝐿2𝑑 62𝐻3𝜂2 . + 2 (cid:101)𝜎 𝑁2 𝐻𝜁𝐿 𝑑𝜂 +𝜁 2𝜂 𝑁𝜇2 𝐻𝐿3 − 𝛾2
𝑁
·𝜂𝜎 ℎ2. (61)
□
RearrangingEq.(57)andusing𝑠(𝑡,1)
=0,wehave
D.4 ProofofCorollary(3.2)
𝐻
(1−3𝑐 𝑔𝑑𝐻2𝐿2𝜂2)∑︁ 𝑠(𝑡,𝑘) ≤𝐶 2. (58) Denote(cid:101)Γ= 𝑑 𝑑− 𝛾𝑁 𝑁𝛾𝜁 .RearrangingEq.(61),simultaneouslysumming
𝑘=1 over𝑇 roundsonbothsidesandtakingtheaverage,wegetthe
15result: Fed-Alpaca:TheAlpacadataset[46]isdesignedforLLMfine-
2(cid:101)Γ (cid:101)𝑐 ℎ𝜂E 𝑡(cid:13) (cid:13) (cid:13)∇𝑓(𝜃𝑡 )(cid:13) (cid:13) (cid:13)2 ≤ 𝑓(𝜃𝑡 )−E 𝑡(cid:2)𝑓(𝜃𝑡+1)(cid:3) vtu an rii en tg ya on fd Nf Le Pat tu ar se ks sn ,sa utu cr ha al sla tn eg xu ta gg ee neq ru ae ts ioti no ,n ts raa nn sd lar te is op no ,n as ne ds ofo pr ena
2𝜎2𝜁𝐿𝜂 𝜁𝜂𝜇2𝐿3 2 QA.Itspansvariousdomainslikemath,textprocessing,andcode
+ (cid:101) + − ·𝜂𝜎2
𝑁𝐻𝑑 2𝑁𝐻 𝛾𝑁 ℎ generation.
E 𝑡(cid:13) (cid:13) (cid:13)∇𝑓(𝜃𝑡 )(cid:13) (cid:13) (cid:13)2 ≤ 𝑓(𝜃𝑡)− 2E
(cid:101)Γ
(cid:101)𝑐𝑡 ℎ(cid:2) 𝜂𝑓(𝜃𝑡+1)(cid:3) WE. e2 impE lex mp ene tr oim ure apn pt ra ol acP hel sat uf so inr gm Pys
Torch[42]v1.10.1,coupled
𝜎2𝜁𝐿 𝜁𝜇2𝐿3 𝜎2 with PEFT v0.3.0 and the Transformers library [51] v4.29.2. Ex-
+ (cid:101) + − ℎ
(cid:101)Γ (cid:101)𝑐 ℎ𝑁𝐻𝑑 (cid:101)Γ (cid:101)𝑐 ℎ4𝑁𝐻 (cid:101)Γ (cid:101)𝑐 ℎ𝛾𝑁 p eqe uri im ppe en dts ww iti hth foL uL raM NVA I- D3B IAar Ae 1c 0o 0n Gdu Pc Ut sed (4o 0n Ga B)c ,o wm itp hu pte rep -l ta rt af io nr em
d
𝑇1 ∑︁ 𝑡𝑇 =0E 𝑡(cid:13) (cid:13) (cid:13)∇𝑓(𝜃𝑡 )(cid:13) (cid:13) (cid:13)2 ≤ 𝑓( 2𝜃 (cid:101)Γ0 (cid:101)𝑐) ℎ− 𝜂𝑇𝑓∗ +
(cid:101)Γ
(cid:101)𝑐(cid:101)𝜎 ℎ2 𝑁𝜁𝐿
𝐻𝑑
L EL .3Mslo Dad ee fd auas l1 t6 I- mbit pfl lo ea mtin eg n-p to ain tt ion num Sb ee trs t.
ings
𝜁𝜇2𝐿3 𝜎2
+ − ℎ . (62) Followingtheguidelinesin[29,36],allapproachesperformlocal
(cid:101)Γ (cid:101)𝑐 ℎ4𝑁𝐻 (cid:101)Γ (cid:101)𝑐 ℎ𝛾𝑁
trainingwithabatchsizeof1tominimizememoryusage.Inan
□ efforttostandardizetheexperimentalconditions,bothbackprop-
agation(BP)-basedmethodsandourproposedmethodFedMeZO,
E IMPLEMENTATIONDETAILS trainlocallywithspecificlearningrates:𝜂 =1×10−5fortheFed-
Inthissection,weprovidethedetailedimplementationsofourex-
DollyandFed-Alpacadatasets,𝜂 =2×10−5fortheFed-CodeAlpaca
periments.Someexperimentalsettingshavealreadybeendiscussed dataset,and𝜂 =2.5×10−5fortheFed-GSM8K dataset.Therank
inSection5.1andwillnotbereiteratedhere. andalphaparametersforLow-RankAdaptation(LoRA)adapters
usedbybothBP-basedoptimizationandFedMeZOaresetto128
E.1 DatasetsandEvaluationMetrics and256,respectively.Asper[36],theperturbationscale𝜇forFed-
MeZOissetto1×10−3.Inourtrainingprocess,weimplemented
earlystoppingtopreventoverfittingandreducetrainingtime.The
Name #Sample Domain
trainingwasstoppediftherewasnoimprovementinthevalidation
Fed-Alpaca 52.0k GenericLanguage lossforapre-definednumberofconsecutiveepochs,knownasthe
Fed-Dolly 15.0k GenericLanguage patienceparameter.Wechoseapatienceof30epochsbasedon
Fed-GSM8K 7.5k CoT empiricalevidenceorpriorstudies.Thebestmodelwasselected
Fed-CodeAlpaca 8.0k CodeGeneration fromtheepochwiththelowestvalidationloss.
Theinfluenceofdifferenthyper-parametersforFedMeZOhas
Table3:DatasetsandBasicInformation
beenanalyzedinSection5.
WeadoptseveralfederatedtuningdatasetstailoredforLLMs F SUPPLEMENTARYEXPERIMENTS
from[29],withdifferentsplittingstrategiestosimulatethehet-
F.1 TheImpactofPerturbationScale𝜇 on
erogeneitytypicalofdifferentfederatedlearning(FL)scenarios,
includingauniformdistributionofdata,aDirichletdistributionof convergence
dataandasplitterbasedonmeta-information. Wepresentallresultofimpacton𝜇inFigure5.
Fed-Dolly:Thisfederatedcorpusdataset,derivedfromDatabricks-
dolly-15k[12],compriseseightcategoriesofNLPtasks:brainstorm- F.2 TheImpactofLocalIterations
ing,classification,closedQA,creativewriting,generalQA,infor- Wepresentallresultofimpacton𝐻 inFigure6.
mationextraction,openQA,andsummarization.Wedividethe
trainingsetintothreesubsetsusingathree-waysplitandassign F.3 TheImpactofHeterogeneityon
eachsubsettoadistinctclient.
Convergence
Fed-GSM8K:ConstructedfromtheGSM8K dataset[11],this
WepresentallresultofimpactonHeterogeneityinFigure7.
collection is aimed at mathematical fine-tuning and consists of
7.5Ktrainingproblemsalongside1Ktestproblems.Bydefault,we
F.4 TheImpactofClientNumber
partitionthetrainingsetuniformlyintothreesubsetsandallocate
eachtoaseparateclient. WepresentallresultofimpactonHeterogeneityinFigure7.
Fed-CodeAlpaca:ThisfederatedversionofCodeAlpaca[5]en-
compassescodesamplesintenprogramminglanguages,including
C,C#,C++,Go,Java,PHP,Pascal,Python,Scala,andX86-64Assem-
bly.DuetothescarcityofX86-64Assemblysamplesintheoriginal
corpus,weexcludethem.Theremainingsamplesarethendivided
intothreesubsetsusingadefaultthree-waysplit,withonesubset
assignedtoeachclient.
16Dolly-IID Dolly-LDA Dolly-Meta Dolly-IID Dolly-LDA Dolly-Meta
=1e 3 =1e 3 =1e 3 H=30 2.05 H=30 1.9 H=30
=5e 3 =5e 3 =5e 3 H=10 H=10 H=10
2.00 =2e 4 2.00 =2e 4 1.8 =2e 4 2.0 H=50 2.00 H=50 H=50
1.8
1.95 1.95 1.95
1.90 1.90 1.7 1.9 1.90 1.7
1.85 1.85
1.85 1.6 1.6
1.80 1.8 1.80
1.80
1.75 1.5 1.75 1.5
1.75 1.7 1.70
1.70
1.4
1.70 1.4 1.65
1.65 0 100 R2o00und300s 400 500 0 100 R2o00und300s 400 500 0 100 R2o00und300s 400 500
0 50 100R15o0u20n0d250s300350400 0 100 R2o00und300s 400 500 0 100 R2o00und300s 400 500
Code-IID Code-LDA Code-Meta
Code-IID Code-LDA Code-Meta 1.16 H=30 H=30 H=30
1.16 =1e 3 =1e 3 =1e 3 H H= =1 50 0 H H= =1 50 0 0.89 H H= =1 50 0
=5e 3 =5e 3 0.89 =5e 3 1.15 0.92
1.15 =2e 4 =2e 4 =2e 4
0.92 1.14
0.88
1.14 0.88 0.90
1.13
1.13 0.90 0.87
0.87 1.12 0.88
1.12
0.88 1.11 0.86
1.11 0.86 1.10 0.86
1.10 0.86 1.09 0.85
0.84
0.85
1.09 1.08
0.84 0 100 R2o00und300s 400 500 0 100 R2o00und300s 400 500 0 100 R2o00und300s 400 500
1.08
0 100 R2o00und300s 400 500 0 100 R2o00und300s 400 500 0 100 R2o00und300s 400 500 Alpaca-IID GSM-IID
1.0
H=30 H=30
Alpaca-IID GSM-IID
1.0
H H= =1 50
0
H H= =1 50
0
= =1 5e e 3 3 1.25 = =1 5e e 3 3 2.2 1.2 0.8
2.2 =2e 4 1.20 =2e 4
0.8 2.0
1.15 1.1 0.6
2.0
1.10
0.6 1.8
0.4 1.05 1.0
1.8
1.00 0.4 1.6
0.2
0.9
0.95
1.6
0.90 0.2 1.4 0 100 R2o00und300s 400 500 0 100 R2o00und300s 400 500 0.00.0 0.2 0.4 0.6 0.8 1.0
1.4 0.85
0 50 100Ro150un20d0s250 300 350 0 100 R2o00und300s 400 500 0.00.0 0.2 0.4 0.6 0.8 1.0 Figure6:DifferentH performancedraft
Figure5:Different𝜇performancedraft
F.5 ConvergenceAnalysisofallDatasetsand
Splitter
WepresentalltheexperimentalresultsfromSection5.2,asshown
inFigure9. Figure7:Differentsplitterperformancedraft
F.6 RequirementsfortheLearningRate(May
thelargerthelearningrate,theearlierthissharpincreaseoccurs.
bejudged,canputintoappendix)
Thisindicatesthatinourstudiedmethod,excessivelylargelearning
To verify that an appropriate learning rate is a prerequisite for ratesareinappropriate.
personalizedlearningrates,weattemptedtoconductexperiments
withavarietyoflargerlearningrates:
Figure10showsthatwhenthelearningrateexceedstherange
supportedbytheory,thelossfunctionexhibitsasharpincrease,and
17
ssoL
ssoL
ssoL
ssoL
ssoL
ssoL
ssoL
ssoL
ssoL
ssoL
ssoL
ssoL
ssoL
ssoL
ssoL
ssoLDolly-IID Dolly-LDA Dolly-Meta
2.05 2.05
BP-based SGD BP-based SGD BP-based SGD
FedMeZO FedMeZO FedMeZO
2.00 2.00
1.8
1.95 1.95
1.90 1.90 1.7
1.85 1.85
1.6
1.80 1.80
1.75 1.75 1.5
1.70 1.70
1.4
1.65 1.65
0 100 200 300 400 500 0 100 200 300 400 500 0 100 200 300 400 500
Rounds Rounds Rounds Figure10:Phenomenonoflosssurgeduetolargerlearning
Code-IID Code-LDA Code-Meta ratesdraft
1.16
BP-based SGD BP-based SGD BP-based SGD
FedMeZO FedMeZO 0.89 FedMeZO
1.15
0.92
1.14
0.88
1.13 0.90
0.87
1.12
1.11 0.88
0.86
1.10
0.86
0.85
1.09
1.08
0 100 200 300 400 500 0 100 200 300 400 500 0 100 200 300 400 500
Rounds Rounds Rounds
Alpaca-IID GSM-IID
1.0
BP-based SGD 1.25 BP-based SGD
FedMeZO FedMeZO
2.2
1.20
0.8
1.15
2.0
1.10 0.6
1.05
1.8
1.00 0.4
1.6 0.95
0.2
0.90
1.4
0.85
0 100 200 300 400 500 0 100 200 300 400 500 0.00.0 0.2 0.4 0.6 0.8 1.0
Rounds Rounds
Figure9:Mainresultoflossdraft
18
ssoL
ssoL
ssoL
ssoL
ssoL
ssoL
ssoL
ssoL