Sharp Rates in Dependent Learning Theory:
Avoiding Sample Size Deflation for the Square Loss
Ingvar Ziemann∗1, Stephen Tu2, George J. Pappas1, and Nikolai Matni1
1University of Pennsylvania
2University of Southern California
Abstract
In this work, we study statistical learning with dependent (β-mixing) data and square loss
in a hypothesis class F ⊂ LΨ p where Ψ p is the norm kf kΨ p , sup m≥1m−1/p kf kLm for some
p [2, ]. Our inquiry is motivated by the search for a sharp noise interaction term, or
∈ ∞
variance proxy, in learning with dependent data. Absent any realizability assumption, typical
non-asymptotic results exhibit variance proxies that are deflated multiplicatively by the mixing
2
time of the underlying covariates process. We show that whenever the topologies of L and
Ψ are comparable on our hypothesis class F—that is, F is a weakly sub-Gaussian class:
p
kf kΨ p . kf kη L2 for some η ∈ (0,1]—the empirical risk minimizer achieves a rate that only
depends on the complexity of the class and second order statistics in its leading term. Our
result holds whether the problem is realizable or not and we refer to this as a near mixing-
free rate, since direct dependence on mixing is relegated to an additive higher order term. We
arrive at our result by combining the above notion of a weakly sub-Gaussian class with mixed
tail generic chaining. This combination allows us to compute sharp, instance-optimal rates for
a wide range of problems. Examples that satisfy our framework include sub-Gaussian linear
regression,more generalsmoothly parameterizedfunction classes, finite hypothesis classes,and
bounded smoothness classes.
∗Corresponding author: ingvarz@seas.upenn.edu
1
4202
beF
8
]GL.sc[
1v82950.2042:viXraContents
1 Introduction 3
1.1 Contribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
1.2 Proof Outline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
1.3 Further Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
2 Ψ -Norms, Bernstein’s Inequality and Empirical Processes 8
p
2.1 The Multiplier Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
2.2 The Quadratic Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
2.3 β-Mixing Processes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
3 The Main Result 11
3.1 Further Comparison to Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
4 Examples of Weakly sub-Gaussian Classes 13
5 Summary 15
References 15
A Properties of Ψ - and Lp-Norms 18
p
A.1 Proof of Lemma 2.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
B Controlling the Multiplier Process 19
C Controlling the Quadratic Process 22
D Results for Mixing Empirical Processes 26
D.1 Blocking . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
D.2 Controlling Empirical Processes for β-Mixing Data . . . . . . . . . . . . . . . . . . . 26
E Proof of Theorem 3.1 27
E.1 Proof of Corollary 3.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
21 Introduction
Whileasignificantportionthedatausedinmodernlearningalgorithmsexhibitstemporaldependen-
cies, we still lack a sharp theory of supervised learning from dependent data. Examples exhibiting
such dependencies are far ranging and abundant, and include forecasting applications and data
from controls/robotics systems. Over the last several decades, an order-wise rather sharp theory
of learning with independent data has emerged. An entirely incomplete list of these advances in-
cludes the introduction of local Rademacher compleixities by Bartlett et al. [2005], sharp rates in
misspecified linear regression by Hsu et al. [2012], and culminates in the learning without concen-
tration framework by Mendelson [2014], which enables an instance-optimal understanding of many
standard learning problems through a critical radius that is sensitive to both the noise scale and
the (local) geometry of the hypothesis class.
In principle, one expects these results to be carried over to the dependent (β-mixing) setting
1
through blocking [Bernstein, 1927, Yu, 1994]. At a high level, the blocking technique involves
splitting the original data (of length n N) into consecutive blocks, each of length k N, with the
∈ ∈
length chosen such that the starting points of each block are approximately independent. Indeed,
several prior works pursue this route [Mohri and Rostamizadeh, 2008, Kuznetsov and Mohri, 2017,
Roy et al., 2021]. However, the drawback with this approach is that it typically deflates the original
sample size by the block length factor k. If such a deflation were to appear in the final rate of
convergence, this would clearly constitute worst-case behavior; it corresponds to every data point
beingrevealedrepeatedly, k times andwithperfectdependence, withinasequenceofnobservations.
In the context of the square loss function, the typical approach to sidestep this sample size
deflation relies on the “noise” (residual term) forming a martingale difference sequence. This ap-
proach has been carried out for parametric inference in (generalized) linear dynamical systems by
Simchowitz et al. [2018] and Kowshik et al. [2021] and also for more general hypothesis classes and
supervised learning with square loss by Ziemann and Tu [2022]. For the square loss function the
martingale approach requires that the problem is strongly realizable: the best predictor in the
hypothesis class should coincide with the regression function (conditional expectation of targets
given past inputs). Put differently, one requires that the hypothesis class is rich enough such that
conditional expectation function (of the targets and given past inputs) can be realized by it.
In this paper, we instead show how the blocking approach can be salvaged for a wide range of
hypotheses classes and the square loss function. In contrast to the just-mentioned references, our
analysis does not require a realizability assumption. Instead, we show how to extend the analysis of
Ziemann et al. [2023b] for linear regression to more general hypothesis classes. At a high level, this
analysisinvolves combining theabove-mentioned blocking technique withBernstein’s inequality. To
motivate this approach, let us consider what happens in Bernstein’s inequality when we are given
V b-bounded random variables that are k-wise independent, where k divides n, and with identical
1:n
2
marginals (for simplicity alone). By applying Bernstein’s inequality to the bk-bounded variables
V¯ ,V¯ , ik V we find that with probability at least 1 δ:
i:n/k i j=ik k 1 j −
− −
P 1 n k 1E(V¯ )2ln(1/δ) 4bklog(1/δ)
V 2 − 1 + . (1.1)
i
n ≤ n 3n
r
i=1
X
1See AppendixD.1 for a description of thistechnique.
2We say that a sequence Z is k-wise independent if each of the blocks Z (j = 0,1,...,n/k−1) are
1:n jk+1:(j+1)k
independentof each other.
3If the data instead were completely independent, then in the small and moderate deviations regime
δ & exp( nEV2/b2k), (1.1) is just as sharp as directly applying Bernstein’s inequality to the
− 1
independent sum. In this regime for this problem, nothing is lost by blocking, even if the data
happens to be iid and we use the blocked version of Bernstein’s inequality. By contrast, if one were
to carry out the same computation using Hoeffding’s inequality (for bounded random variables)
instead of Bernstein’s, we would incur an irreducible factor k in the leading term in all regimes—
even if the dependent bound is instantiated for independent variables. This suggests that the
variance interacts much more gracefully with blocking arguments than higher order moments.
The difficulty in combining blocking with Bernstein’s inequality lies in making Bernstein’s in-
equality uniformacrossthecorrectportionofthehypothesisclassF. Namely, instatisticallearning
it typically does not suffice to control sums of a single sequence of random variables V but rather
1:n
we need to uniformly control sums of an indexed family V (f) : f F . To obtain fast rates,
1:n
{ ∈ }
this uniform control needs to combined with a localization argument, so that one does “pay” for hy-
potheses too far away from the ground truth but only those within a certain critical radius. Naïvely
union-bounding (or chaining) over such a family unfortunately again reintroduces a sample-size
deflation by the block-length factor k. This happens because the variance term in (1.1) starts to
balance the boundedness term at the above-mentioned critical radius without further assumption.
Ziemann et al.[2023b]showhowtoovercome theissueofuniformity whenF isalinearclassviathe
Fuk-Nagaev inequality [Einmahl and Li, 2008]. Unfortunately, this inequality cannot be applied be-
yondthelinearsetting. Here, weintroduce machinery based onarefinement ofsub-Gaussianclasses
[Lecué and Mendelson, 2013], and a refinement of Bernstein’s inequality (due to Maurer and Pontil
[2021]), that we combine with mixed-tail generic chaining (as introduced by Dirksen [2015]). Our
approach allows us to overcome this issuewith blocking and Bernstein’s inequality fora surprisingly
widerangeoffunctionclasses,thereby relegating anydependence onmixingtoadditive higherorder
terms, instead of the typical multiplicative deflation term.
1.1 Contribution
Let us now make our contribution more precise. We are given stationary β-mixing data (X,Y)
1:n
where the X (resp. Y ) assume values in a subset of a normed space denoted (X, ) (resp. a
i i X
k ·k
Hilbert space (Y, , , )). We assume that (X,Y) is stationary and denote for any i [n]
1:n
h· ·i k ·k ∈
the joint distribution of (X ,Y ) by P , and the corresponding marginals are denoted P and P .
i i X,Y X Y
We study empirical risk minimization over a hypothesis class F, containing functions f : X Y,
→
and with the square loss function. In this scenario, we study the performance of the (any) empirical
risk minimizer
n
1
f argmin f(X ) Y 2. (1.2)
i i
∈ f F n k − k
∈ Xi=1
b
Our main contribution is to characterize the rate of convergence of (1.2) to the best possible
predictor f in the class F defined as:
⋆
f argminE f(X) Y 2, (X,Y) P . (1.3)
⋆ X,Y
∈ f F k − k ∼
∈
Letus alsodenoteby F thestar-hull of F around f . That is, F , ρ(f f ) :f F,ρ [0,1] ,
⋆ ⋆ ⋆ ⋆
{ − ∈ ∈ }
which, for a convex class F coincides with the shifted class F f . We further equip F
⋆ ⋆
− { }
with the L2-norm: f 2 , E f(X) 2,f F ,X P . Let us also define the “noise” W by
k kL2 k k ∈ ⋆ ∼ X 1:n
4W , Y f (X ),i [n]. We focus on the case when F is either (1) convex or (2) realizable (i.e.,
i i ⋆ i
− ∈
E[W X ] = 0 for i [n]). Note that this restriction is due to a known shortcoming of ERM which
i i
| ∈
holds even in iid settings, and can be removed by modifying the estimator itself; we will discuss this
issue in more detail shortly.
As is typical in the learning theory literature, we characterize the rate of convergence of (1.2)
through a fixed point, or critical radius. This critical radius takes the form as a solution to:
1 n g(X ) complexity(F r S )
r sup V W , i ⋆ ∩ ⋆ L2 , (1.4)
⋆ i
≍ g ∈F ⋆ ∩r⋆S L2 √n Xi=1(cid:28) kg kL2(cid:29)!× r ⋆√n
where for r R,r > 0, rS is the unit sphere of radius r in L2 (and the corresponding unit
L2
∈
ball is denoted rB ). This critical radius is akin to the one in Bartlett et al. [2005], but also
L2
resembles the noise interaction term of Mendelson [2014, introduced following Equation 2.2] in that
our radius depends on the weak variance, sup V 1 n W , g(Xi) .3 To aid in the
g ∈F ⋆ ∩r⋆S L2 √n i=1 i kg kL2
interpretation of r ⋆, we will instantiate our main result, T(cid:16)heore Pm 3.1D, for paraEm(cid:17)etric classes and
show that this radius exhibits the desired “dimension counting” scaled with noise-to-signal behavior,
see Corollary 3.1. Moreover, the weak variance term takes into account how targets Y interact
1:n
withthefunctionclassF throughW ,locallyatradiusr neartheminimizerf ,viaasecond-order
1:n ⋆ ⋆
statistic. In particular, this variance term is always sharper than the corresponding iid variance
term deflated by a factor of the mixing-time (or block-length).
With these preliminaries in place we are ready to state an informal version of our main result.
Informal version of Theorem 3.1. Given data that mixes sufficiently fast, for a wide range of
(1) convex or (2) realizable hypothesis classes, any empirical risk minimizer f over such a class F
converges at least as fast a rate characterized by the critical radius r given by the solution to (1.4)
⋆
depending on the variance of the noise-class interaction and local scale of thebclass F. That is with
probability 1 δ:
−
(weak variance) log(1/δ)
f f 2 . r2+ ×
k − ⋆ kL2 ⋆ n
+terms of higher order(r ,n 1,mixing,log(1/δ)). (1.5)
b ⋆ −
d+log(1/δ)
Moreover, for d-dimensional parametric classes the leading term is (weak variance) .
× n
The crux of this result is that past a burn-in, the ERM excess risk does not directly depend on
mixing times, but only on the relevant second order statistics. Put differently, the effect of slow
mixing has been relegated to a small additive term with higher order dependence on 1/n. Indeed,
both r and the variance term in (1.5) do not directly depend on slow mixing (i.e., are not deflated
⋆
by the block-length k) but only on relevant second order statistics. Slow mixing only affects higher
order additive terms that can be pushed into the burn-in.
The qualifier “wide range” above refers to the requirement that the class F satisfies a certain
topological condition. Recall that for a random variable Z the Ψ -norm is the norm Z =
p
k
kΨp
sup m 1/p Z . We will ask that for some η (0,1] and L > 0, every f F satisfies the
m ≥1 − k kLm η ∈ ∈ ⋆
inequality f L f . We will say that such classes are weakly sub-Gaussian and will verify
k kΨp ≤ k kL2
that such an inequality indeed holds true for a range of examples in Section 4:
3The terminology weak variance comes from the empirical processes literature in that the supremum in Defini-
tion 2.2 is on the outside of theexpectation.
5• bounded smoothness classes, see Proposition 4.1;
• parametric classes that are Lipschitz in their parameterization, see Proposition 4.2;
• sub-Gaussian linear regression, see Proposition 4.3;
• finite hypothesis classes, see Proposition 4.4.
Finally, therequirement that F beeither(1)convex or(2)realizable caneasilyberemoved with
a few modifications if one replaces the empirical risk minimizer by the star estimator of Audibert
[2007]. In this case (but with the L2-error replaced with the no-longer directly comparable excess
risk functional) the geometric inequality by Liang et al. [2015, Lemma 1] takes a similar role to the
basic inequality we use below. The necessity of imposing (1) or (2) is due to a known shortcoming
of empirical risk minimization outside of convex (or realizable) classes, and not an issue directly
related to dependent data [see e.g. the discussion in Mendelson, 2019].
1.2 Proof Outline
From a more technical standpoint, our contribution is a novel analysis of two empirical processes
that arise in (but are not restricted to) empirical risk minimization, and which are sharp even for
dependent data. Following the language of Mendelson [2014], we refer to these as the quadratic
and multiplier empirical processes. The first of these, the quadratic process, controls a one-sided
discrepancy between the empirical and population L2-norms:
n n
1 (1+ε)
Q (f), E f(X ) f (X ) 2 f(X ) f (X ) 2, (1.6)
n i ⋆ i i ⋆ i
n k − k − n k − k
i=1 i=1
X X
for some ε (0,1). Under our assumptions, we will show that the process Q (f) is eventually
n
∈
nonpositive uniformly for all sufficiently large f, implying that the empirical L2-norm dominates
the population L2-norm.
Now conditionally on the event Q (f) 0 , for every r 0 we also have the following
n
{ ≤ } ≥
deterministic inequality:
n r f(X ) f (X )
1+ε i ⋆ i
f f r+ 2(1 E) W , − , (1.7)
k − ⋆ kL2 ≤ rn Xi=1 − ′ "* i h bkf −f ⋆ kL2 i +#
b
where E denotes expectation with respect to a fresh copy of randbomness independent of f.
′
Hence, we also need to control the multiplier process:
b
n
1+ε
M (f) , 2(1 E) W ,f(X ) . (1.8)
n ′ i i
n − h i
i=1
X
Itisthe uniform control ofM (f)over theclass F intersected withthe radius r ballrS balanced
n ⋆ L2
with the firstterm of (1.7) that gives rise to the critical radius (1.4). This argument is formalized in
Lemma E.1. Just as in Mendelson [2014], it is the multiplier process (1.8) that yields the dominant
contribution to the error (1.5) (after a burn-in). This is important as it allows us to use blocking
to control (1.6) without affecting the leading term of the final rate.
6Wereiteratethatouranalysisoftheabovetwoempiricalprocesses((1.6)and(1.8))restscrucially
on the assumption that F is a weakly sub-Gaussian class. Let us also point out that we first make
⋆
a simplifying assumption, namely that our model is k-wise independent. We later port all results
to the β-mixing setting by blocking, cf. Section 2.3. A sketch of the analysis of M (f)—found in
n
Section 2.1 with proofs relegated to Appendix B—now goes as follows:
• We invoke a refinement of Bernstein’s inequality (Lemma 2.1) to gain pointwise control of
M (f). The benefit of this over the standard version is that we do not require boundedness,
n
but rather finite Ψ -norm suffices. Unless p = (boundedness), the price we pay for this is
p
∞
that the variance proxy is degraded to a moment of order 2q,q > 1 instead of order 2.
• We make this refinement of Bernstein’s inequality uniform over the class F intersected with
⋆
the radius r ball rS by invoking mixed-tail generic chaining [Dirksen, 2015]. This splits the
L2
tail into an L2q-component and a Ψ -component.
p
• Our assumption that F is a weakly sub-Gaussian class now comes into play by ensuring
⋆
that, past a burn-in, the Ψ -component of the mixed tail is of lesser magnitude than the
p
L2q-part of the tail. Just as in our introductory example with Bernstein’s inequality (1.1),
any dependence on mixing is relegated to this smaller Ψ -component (which now assumes the
p
role of boundedness).
• Combining these steps with (1.7) yields control of the multiplier process and is summarized
in Theorem 2.1.
The analysis of Q (f)is relatively standard and amounts to showing that the norm-bound f
η
n
k
kΨp
≤
L f is sufficient to modify a standard truncation argument [see e.g. Wainwright, 2019, The-
k kL2
orem 14.12]. We then proceed to control the remainder of said truncation argument completely
analogously to our above approach for M (f). We detail these arguments in Section 2.2 and prove
n
them in Appendix C. Finally, we combine our control of the multiplier and quadratic processes
(Theorem 2.1 and Theorem 2.2) with blocking to arrive at our main result, Theorem 3.1.
1.3 Further Preliminaries
Notation. Expectation (resp. probability) with respect to all the randomness of the underlying
probability space is denoted by E (resp. P). For q [1, ) the 2q-variance of a random variable Z
∈ ∞
is defined as V (Z) ,(E(Z EZ)2q)1/q with V = V being the standard variance. For p [1, ),
2q 2
− ∈ ∞
we also introduce the Ψ -norm Z , sup m 1/p Z and also set Z , Z . Two
p k kΨp m ≥1 − k kLm k kΨ∞ k kL∞
extended real numbers q,q [1, ] are said to be Hölder conjugates if 1/q +1/q = 1, where, as
′ ′
∈ ∞
we do throughout, 1/ is interpreted as 0. For two probability measures P and Q defined on the
∞
same probability space, their total variation is denoted P Q . Maxima (resp. minima) of two
TV
k − k
numbers a,b R are denoted by a b = max(a,b) (resp. a b = min(a,b)). For an integer n N,
∈ ∨ ∧ ∈
we also define the shorthand [n] , 1,...,n .
{ }
Talagrand’s functionals. The complexity(F r S ) term in (1.4) is made precise through
⋆ ⋆ L2
∩
Talagrand’s γ -functional (with α = 2 being the dominant term in our result). Let be (H ,d) a
α
metric space. We denote the diameter of H with respect to d by
∆ (H ) , sup d(h,h).
d ′
h,h′ H
∈
7A mseq 1u .en Fc oe rH
α
= (( 0H
,
m) )m
,
∈ thZ e+ γof -s fu ub ns ce tit os no af lH
of
(i Hs c ,a dll )ed isa dd em finis es dib ble yif |H 0 |= 1 and |H m |≤ 22m for all
α
≥ ∈ ∞
∞
γ (H ,d) , inf sup 2m/αd(h,H ), (1.9)
α n
h H
H ∈ m X=0
wheretheinfimumistakenoveralladmissiblesequences(wewrited(h,H) = inf d(h,s)whenever
s H
H is a set). For η (0,1), we slightly abuse notation and write γ (H ,dη) f∈ or d replaced with
α
∈
dη in (1.9) (while being mindful of that fact that dη is not a metric in general). Finally, since
entropy integrals upper-bound γ -functionals, itwillalsobeusefultointroducethecoveringnumber
α
(H ,s), which denotes the minimal number of L2-balls of radius s required to cover H .
L2
N
2 Ψ -Norms, Bernstein’s Inequality and Empirical Processes
p
In this section we establish a few preliminary technical lemmas that will be useful for controlling
the multiplier and quadratic processes ((1.8) and (1.6)). We begin with a version of Bernstein’s
inequality that controls the Laplace transform of Z in terms of its L2q-norm (q 1) and some
≥
Ψ -norm. The lemma comes from Maurer and Pontil [2021].
p
Lemma2.1 (Ψ -BernsteinMGFBound). Fixarandomvariable Z andp [1, ]such thatEZ 0
p
∈ ∞ ≤
and Z < . Let q and q be Hölder conjugates and suppose that λ [0,1/(q e)1/p Z ]. We
k
kΨp
∞
′
∈
′
k
kΨp
have that:
λ2 E(Z)2q 1/q
Eexp(λZ) exp 2 . (2.1)
≤ 1 −λ (cid:0)(q ′e)1/p k(cid:1)Z kΨp!
Ourintention is touseLemma 2.1toaffordus—pointwise in g—control ofthemultiplier process
introducedin(1.8). Indeed,noticethatintheregimeλ (0,(2(q e)1/p Z ) 1]thedominantterm
∈
′
k
kΨp −
in (2.1)is 2q-variance of Z. Since (1.7) can belocalized to aball of radius r in L2 itsuffices that the
L2-norm provides some weak control of the Ψ -norm for any constant choice of λ to be admissible
p
oncethelocalization radiusr ischosensmallenough. Thisinturnmotivates thefollowing definition.
Definition 2.1 (Weakly sub-Gaussian Class). Fix η (0,1] and L [1, ). We say that a class
∈ ∈ ∞
G is (L,η)-Ψ if for every g G we have that:
p
∈
g(X) L g(X) η , X P . (2.2)
k kΨp ≤ k kL2 ∼ X
If (2.2) holds for G with η (0,1) and some L we will call G a weak Ψ -class. If (2.2) instead
p
∈
holds for η = 1 it is simply a Ψ -class. This generalizes the notion of a sub-Gaussian class from
p
Lecué and Mendelson [2013], which corresponds to η = 1 and p = 2. Let us further point out that
by homogeneity, if η (0,1) in (2.2), then one should expect L to depend polynomially on some
∈
other norm (or homogenous functional) of g. Indeed, by the Gagliardo-Nirenberg interpolation
inequality, the above relaxation (η < 1) covers smoothness classes (Proposition 4.1), whereas the
strict sub-Gaussian class assumption (η = 1) of Lecué and Mendelson [2013] is difficult to verify
beyond linear functionals.
As we have pointed out above, our intention is to apply Lemma 2.1 pointwise to the multiplier
process (1.8). However, this yields a different variance term for each index point of the empirical
process. The solution to this is simply to define a uniform variance term, as is done below.
8Definition 2.2 (Noise Level). The 2q-noise-class-interaction between F, the model P , and
(X,Y)1:n
the shifted target W = (Y f (X)) at resolution G is given by
1:n ⋆ 1:n
−
n
1 g(X )
V F,G,P , supV W , i . (2.3)
2q
(cid:0)
(X,Y)1:n
(cid:1)
g ∈G 2q √n Xi=1(cid:28) i kg kL2(cid:29)!
Westressthat,eventhoughDefinition 2.2measuresnoiseuniformly overafunctionclass,itdoes
not generally grow with the complexity of the class. For instance, under the additional hypotheses
that P is drawn iid and that W is independent of X for i [n], it is easy to see that
(X,Y)1:n i i
∈
V F,G,P = V (W) for every such well-specified class G. Rather, Definition 2.2 is a
2 (X,Y)1:n 2
measure of how well the targets Y align with a given class G.
1:n
(cid:0) (cid:1)
2.1 The Multiplier Process
We will not directly control the multiplier process for β-mixing variables. Instead we first suppose
thatthemodelP isk-wiseindependent(wherek dividesn). Wethenporttheseresultstothe
(X,Y)1:n
β-mixingsettingbyblocking(seeAppendixD.1). Weusethefollowingshorthandnotationregarding
V F,G,P : we take the class F and the probability model P as fixed and thus
2q (X,Y)1:k (X,Y)1:k
omit the dependence on F (via f ) and P and write V (G) = V F,G,P . With
(cid:0) (cid:1) ⋆ (X,Y)1:k 2q 2q (X,Y)1:k
these remarks in place, we now turn to establishing pointwise control of (1.8) using Lemma 2.1.
(cid:0) (cid:1)
Lemma 2.2 (Pointwise Control). Fix two Hölder conjugates q and q . Suppose that the model
′
P is stationary and k-wise independent where k divides n. For every g,g L and u
(X,Y)1:n ′
∈
Ψp
∈
(0, ) we have that:
∞
n
P (1 E) W ,g(X ) g (X )
i i ′ i
− h − i
i=1
X
> 4n g g 2 V ( g g )u
k − ′ kL2 2q { }−{ ′ }
q
+4(q e)2/pk W g g u 2e u. (2.4)
′
k
kΨpk
−
′ kΨp
≤
−
!
In the main development we will instantiate Lemma 2.2 with r = g g 2 decaying to 0
k − ′ kL2
(which should be thought of as a fixed point upper-bounding the rate of convergence of ERM) at
a polynomial rate in n. If furthermore G is (L,η)-Ψ , then the second term (linear in u) of (2.4)
p
can be rendered negligible at every scale r, which allows us to invoke mixed-tail generic chaining
[Dirksen, 2015] to show that the weak variance V (F rS ) dominates the noise level in the
2q ⋆ L2
∩
small-to-moderate deviations regime.
Put differently, at the scale of localization considered here, the noise level of the empirical
process is almost entirely dictated by the weak variance V (F rS ). Now, since q is the
2q ⋆ L2 ′
∩
Hölder conjugate of q this further implies that we may choose q = 1+o(1) so that we might expect
V (F rS ) = V(F rS ) + o(1). Moreover if p = this always the case and we may
2q ⋆ L2 ⋆ L2
∩ ∩ ∞
choose q = 1. In principle no better variance proxy is possible, since already for a single function
g as n , by the central limit theorem under mild ergodicity assumptions on P [e.g. for
→ ∞
(X,Y)1:∞
the Markovian situation cf. Meyn and Tweedie, 1993, Theorem 17.3.6]:
n
1 g(X )
(1 E) W , i N(0,V(F , g ,P )), (2.5)
√n −
i
g
⋆
{ }
(X,Y)1:∞
i=1 (cid:28) k kL2(cid:29)
X
9where the variance term on the right is:
V(F , g ,P ) , lim V(F , g ,P ).
⋆
{ }
(X,Y)1:∞
n
⋆
{ }
(X,Y)1:n
→∞
Now, since r = o(1) in all practical situations one expects V(F rS ) V( f ) as long as the
⋆ L2 ⋆
∩ ≈ { }
map f V(f/ f ) is sufficiently regular near f .
L2 ⋆
7→ k k
We arrive at our main result for the multiplier process by making uniform the pointwise control
afforded to useby Lemma 2.2 viaan instantiation of mixed-tail generic chaining [Dirksen,2015] (for
ease of reference, we restate a corollary of his result as Lemma B.1 in the appendix). This yields
the following result.
Theorem 2.1. Fixafailureprobability δ (0,1), apositivescalar r (0, ), twoHölderconjugates
∈ ∈ ∞
q and q , and aclass F. Suppose that F F is (L,η)-Ψ . Suppose further that the model P
′ ⋆
−
⋆ p (X,Y)1:n
isstationaryandk-wiseindependent where k divides n. Thereexist universalpositiveconstants c ,c
1 2
such that for any r (0,1] we have that with probability at least 1 δ:
∈ −
n
1
sup (1 E) W ,f
i
f ∈F ⋆ ∩rS L2 rn Xi=1 − h i
1 log(1/δ)
c V (F rS ) γ (F rS ,d )+
≤
2 2q ⋆
∩
L2
r√n
2 ⋆
∩
L2 L2
n
r !
q
1 rη 1
+c (q e)2/pLk W γ (F rS ,d )+ − log(1/δ) . (2.6)
1 ′
k
kΨp
rn
η ⋆
∩
L2 L2
n
(cid:18) (cid:19)
In the sequel, we will see that the first term on the right of (2.6) is typically dominant.
2.2 The Quadratic Process
A slight modification of theargument leading toTheorem 2.2 combined with atruncation argument
detailed in Lemma C.1 also yields control of the quadratic process.
Theorem 2.2 (Lower Uniform Law). Fix a failure probability δ (0,1), a tolerance ε > 0, a
∈
localization radius r (0,1], and two Hölder conjugates q and q . Suppose that F F is (L,η)-
′ ⋆ ⋆
∈ −
Ψ . Suppose further that the model P is stationary and k-wise independent where k divides
p (X,Y)1:n
n. There exist a universal positive constant c such that uniformly for all f F rB we have
⋆ L2
∈ \{ }
that with probability at least 1 δ:
−
n
1
f(X ) 2 r2(1 ε2)
i
n k k ≥ −
i=1
X
1/p
−c (n −1/2√kL1+3/4rη log
42 ε/ rpL
!!
γ
2+
46η(F
⋆
∩rS L2,d L2)+r1+ 43η log(1/δ)
(cid:16) p (cid:17)
1/p
42/pL
+n 1(q )1/pkrη log L2(γ (F rS ,d )+rηlog(1/δ)) . (2.7)
− ′
εr
η ⋆
∩
L2 L2
!! )
102.3 β-Mixing Processes
We extend the empirical process results of the preceding two sections to β-mixing processes in
Appendix D.2. We do so by a simple blocking argument that we review in Appendix D.1, and for
which we have already set the stage by establishing our results for k-wise independent processes.
Here, we state the definition of dependence we rely on in the sequel.
Definition 2.3. Let Z be a stochastic process. The β-mixing coefficients of Z, denoted β (i),
1:n Z
are for i [n]:
∈
β (i) , sup E P ( Z ) P . (2.8)
Z
k
Zi+t
· |
1:t
−
Zi+tkTV
t [n]:t+i n
∈ ≤
3 The Main Result
Before we state our main result, we will need to establish one more preliminary matter. Let us
define the burn-in times n ,n ,k which together dictate the minimial sample size necessary
quad mult mix
for our result to be sharp. The first of these, n , is required for the population L2 error to
quad
be dominated by the empirical L2 error: i.e., the quadratic process Q (f) is nonpositive on our
n
class of interest. The second of these, n , is required for the multiplier process, M (f), to have
mult n
a dominant variance term (informally—when the CLT-like rate becomes accurate). Finally, k
mix
is the minimal block-size it takes for the β-mixing model P to be well-approximated by a
(X,Y)1:n
corresponding k-wise independent model. These are given as follows below:
n (r)= inf n N n 1/2√kL1+3/4rη
quad −
( ∈ (cid:12)"
(cid:12)
(cid:12) 1/p
×
log
42/p+(cid:12)
(cid:12)
r1/2L
!!
γ
2+
46η(F
⋆
∩rS L2,d L2)+r1+ 43η log(1/δ)
(cid:16) p (cid:17)
1/p
42/p+1/2L
+n 1L2(q )1/pkrη log (γ (F rS ,d )+rηlog(1/δ)) r2 ,
− ′
r
η ⋆
∩
L2 L2
≤
!! # )
1 rη 1
n (r)= inf n N (q e)2/pLk W γ (F rS ,d )+ − log(1/δ) r ,
mult
∈
′
k
kΨp
rn
η ⋆
∩
L2 L2
n ≤
n (cid:12) (cid:18) (cid:19) o
k = inf k [n](cid:12)kβ 1(k) nδ 1 .
mix { ∈ (cid:12)| − ≥ − }
(3.1)
The first two of these are calculated by requiring the remainder terms in Proposition D.2 and
Proposition D.3 to be of negligible order. The last term is obtained by requiring that failure term,
δ, dominates themixingterm, nβ(k), inthefailureprobability ofthesepropositions. Notethatβ(k)
k
in the above expression refers to the β-mixing coefficients (Definition 2.3) of the process (X,Y) .
1:n
With these burn-in times in place, we are now ready to state the main result of our paper.
Theorem 3.1. Fix a failure probability δ (0,1), two Hölder conjugates q and q , and a class F
′
∈
that is either (1) convex or (2) realizable. Suppose that F F is (L,η)-Ψ . Suppose further that
⋆ ⋆ p
−
the model P is stationary and that k divides n/2. There exist universal positive constants
(X,Y)1:n
c ,c ,c such that the following holds. If r solves
1 2 3 ⋆
1
r c V (F rS ) γ (F rS ,d ), (3.2)
≥
1 2q ⋆
∩
L2
× r√n
2 ⋆
∩
L2 L2
q
11we have that with probability 1 4δ that:
−
log(1/δ)
f f 2 c r2+V (F r S ) (3.3)
k − ⋆ kL2 ≤ 2 ⋆ 2q ⋆ ∩ ⋆ L2 n
(cid:18) (cid:19)
b
as long as n c max n (r ),n (r ) and k k (given in (3.1)).
3 quad ⋆ mult ⋆ mix
≥ { } ≥
Theorem 3.1 informs us that past a burn-in, the rate of convergence of empirical risk minimiza-
tion is dictated by the critical radius r given in (3.2). This radius depends on local complexity
⋆
of the class F measured in L2 distance as per the γ -functional and through the weak variance
2
V (F r S ). We point out that we may choose q = 1 if p = , so that the variance term in
2q ⋆ ⋆ L2
∩ ∞
(3.5) is the actual variance V . As indicated at the discussion following (2.5), this variance term
2
cannot be improved in general. Otherwise we can typically let q approach 1 as the sample size
becomes sufficiently large. Moreover, unless the class exhibits large nonparametric behavior, the
dependency on the complexity is also the best possible even in the iid case [Lecué and Mendelson,
2013]. We will now help parse Theorem 3.1 by specializing it to parametric classes.
Corollary 3.1 (Parametric Classes). Fix a failure probability δ (0,1), two Hölder conjugates q,q ,
′
∈
and a class F that is either (1) convex or (2) realizable. Suppose that F F is (L,η)-Ψ . Suppose
⋆ ⋆ p
−
further that the model P is stationary and that k divides n/2.
(X,Y)1:n
There exists a universal positive constant c and a polynomial function φ such that the following
η
holds true. Suppose that there exists dF R
+
such that for s > 0:
∈
1
log NL2(F ⋆,s)
≤
dF log
s
(3.4)
(cid:18) (cid:19)
We have with probability 1 4δ that:
−
f f 2 c V F
dFk kW k2
L2S
dF +log(1/δ)
(3.5)
k − ⋆ kL2 ≤ 1 2q  ⋆ ∩s n L2  n
(cid:18) (cid:19)
b  
as long as kβ 1(k) nδ 1 and
− −
≥
dFk W 2
n
≥
φ
η
dF,k, kW kΨp,L,q,q ′,V −1 F
⋆
∩s
k
n
kL2S
L2
,log(1/δ) . (3.6)
!
 
Consequently, after a polynomial burn-in and up to a universal positive constant, we are able
to recover the optimal parametric rate n −1 dF +log(1/δ) scaled by the appropriate noise term.
(cid:0) (cid:1)
3.1 Further Comparison to Related Work
Intermsoftechnicaldevelopment, thisworkismostcloselyrelatedtotheworkoniidlearninginsub-
GaussianclassesbyLecué and Mendelson[2013]andtheresultformisspecified(agnostic)dependent
linear regression by Ziemann et al. [2023b]—which we generalize to more general function classes
at the cost of more stringent moment assumptions. Returning to Lecué and Mendelson [2013], and
beside the fact that they work with independent data, the biggest difference is in how we deal with
the multiplier process. We employ chaining with a mixed tail [Dirksen, 2015], instead of a single
12tail. On a practical level, the advantage of the mixed tail result is that it allows us to push the
dependence on, mixing, L (the norm equivalence parameter in Theorem 3.1) and any higher order
norms into the burn-in. For example and by contrast, the parameter L appears multiplicatively
instead of additively in the bound by Lecué and Mendelson [2013]. This is important for us as
there are typically no obvious bounds on these parameters other than in terms of the block-length
k, thereby re-introducing the sample-size deflation. Crucially, our proof approach also allows us
to work with weaker norm relations (η < 1 in Definition 2.1), which hold in significantly wider
generality than the sub-Gaussian class assumption. In particular we are able to handle smoothness
classes in Proposition 4.1, which cannot be covered in the baseline sub-Gaussian class framework.
Another closely related line of work studies parameter identification in auto-regressive models
[for an overview, see Tsiamis et al., 2023, Ziemann et al., 2023a]. When the noise model is strictly
realizable—the variables W form a martingale difference sequence with respect to the filtration
1:n
generated by X —parameter identification is possibleat the iid rate even in the absence of mixing
1:n
[Simchowitz et al., 2018, Faradonbeh et al., 2018, Sarkar and Rakhlin, 2019, Kowshik et al., 2021].
Our results do not cover the mixing-free regime as we consider the agnostic setting in which self-
normalized martingale arguments [Peña et al., 2009, Abbasi-Yadkori et al., 2011] are not available.
We consider providing a unified analysis of the martingale and mixing situations an interesting
future direction.
Moregenerally, severalauthorshaveconsideredlearningundervariousweakdependencynotions.
Kuznetsov and Mohri [2017] give generalization bounds in a more general setting using the same
blocking technique—due to Yu [1994]—used here. Statements similar in spirit can also be found
in e.g., Steinwart and Christmann [2009], Duchi et al. [2012] and most recently Roy et al. [2021].
However,theyallsufferthedependencydeflationdiscussedaboveandinourintroduction(Section1).
WealsonotethatZiemann and Tu[2022]andMaurer[2023]obtainrates—similartoourshere—that
relegate mixing times into additive burn-in factors. On the one hand, the work of Ziemann and Tu
[2022] operates at a similar level of generality when it comes hypothesis classes and also relies on
thesquarelossfunction butrequiresastringent realizability assumption tobeapplicable. Moreover,
both our noise term and our complexity parameter are sharper than theirs. On the other hand, the
work of Maurer [2023] operates at a higher level of generality than us, but does not seem to be able
to reproduce sharp rates when specialized to our situation.
4 Examples of Weakly sub-Gaussian Classes
We conclude by collecting a few examples of weakly sub-Gaussian classes (Definition 2.1). Arguably
the most compelling example identified in the present manuscript are smoothness classes, which are
not covered even in the iid setting by Lecué and Mendelson [2013].
Proposition 4.1 (Smoothness Classes). Let X be a measurable, open, connected and bounded subset
of Rd with Lipschitz boundary and let F be a set of uniformly bounded functions f : X R. Fix
an integer s
∈
N and suppose that there exists a constant CF such that
α
skDαf kL∞→
≤
CF.4
Suppose further that the distribution of the covariates P has density µ with|r|e≤spect to the Lebesque
X X
P
measure and that there exists µ,µ R such that µ µ µ. Under the above hypotheses
+ X
∈ ≤ ≤
4SummationoverDα usesmulti-indexnotation—thesumisoverallpartialderivativeoperatorsoforderlessthan
or equal tos.
13there exists a positive constant c only depending on X, d and s such that F is (L,η)-Ψ with
L = cµµ−2s2 +s dC F2sd +d and η = 2s2 +s d. ∞
Proof. The result for P equal to the (normalized) Lebesque measure is immediate by the main
X
result of Nirenberg [1959] instantiated to the correct smoothness class. The general case follows by
our hypothesis that P is equivalent to the Lebesque measure. (cid:4)
X
We stress that the quantities L and η only appear in the burn-in of Theorem 3.1. In other
words, Theorem 3.1 provides sharp rates almost universally, or at least as long as the hypothesis
class is sufficiently smooth and bounded (although the latter can be relaxed). However, one caveat
is thatthis burn-in can beexponentially large(curse ofdimensionality) unless the classis sufficently
smooth: s is proportional to d above.
Our next example relies on smoothness in parameter space instead of smoothness in terms of
inputs.
Proposition 4.2. Fix an open parameter set M RdF equipped with the Euclidean norm .
⊂ k · k
Consider a function φ :X M R that generates a parametric class of functions F = φ(;θ) θ
× → { · ∈
M . Define M , argmin E(φ(X,θ) Y)2 to be the set of population risk minimizers. Suppose
} ⋆ θ ∈M −
that:
(i) for a,b R , the estimation error functional of the model F is (a,b)-sharp, that is: θ M
+
∈ ∀ ∈
there exists θ M such that ab 1 θ θ E(φ(X,θ) φ(X,θ ))2 b ;
⋆ ⋆ − ⋆ ⋆
∈ k − k ≤ −
(ii) the partial gradient φ(x,θ) exists and is uni(cid:0)formly norm-bounded by(cid:1)C > 0 for all (x,θ)
θ
∇ ∈
X M.
×
Then F f is (Cba 1,2b)-Ψ .
⋆ −
−{ } ∞
Thesharpnesscondition(i)inProposition4.2isstandardinoptimization[seee.gRoulet and d’Aspremont,
2017]. This condition holds somewhat generically [Łojasiewicz,1993], but the exactconstants a and
b are typically difficult to obtain. Fortunately, downstream use of Proposition 4.2 only relies on
these constants in the burn-in.
Proof. By the mean value form of Taylor’s Theorem and Cauchy-Schwarz we write for fixed x X:
∈
φ(x;θ) φ(x;θ ) = φ(x,θ˜),θ θ φ(x,θ˜) θ θ C θ θ (4.1)
⋆ θ ⋆ θ ⋆ ⋆
| − | |h∇ − i| ≤ k∇ kk − k ≤ k − k
for some θ˜ [θ,θ ]. Consequently by our sharpness hypothesis and by optimizing over the left hand
⋆
∈
side of (4.1) we have that for some θ M and every θ M:
⋆ ⋆
∈ ∈
Cb
sup φ(x,θ) φ(x,θ ) E(φ(X,θ) φ(X,θ ))2 b (4.2)
⋆ ⋆
k − k ≤ a −
x X
∈
(cid:0) (cid:1)
Equivalently, f Cba 1 f 2b for every f F f as per requirement. (cid:4)
k kL∞ ≤ − k kL2 ∈ −{ ⋆ }
There is also a more direct argument that easily covers linear functionals on Rd.
Proposition 4.3. Let X be a sub-Gaussian random variable taking values in Rd and let F be
the class of linear functionals on Rd. Suppose that λ EXXT > 0. Then F is (L,1)-Ψ with
min 2
L = sup v ∈Rd: kv k=1 k kh hv v, ,X Xi ik kΨ L22. (cid:0) (cid:1)
14Proof. The only observation we need to make is that it suffices to prove the result for v = 1 by
k k
homogeneity. The result is then immediate by construction. (cid:4)
Analogously, finite hypothesis classes are also covered.
Proposition 4.4. Let F be a finite subset of L Ψ2. Then F is (L,1)-Ψ 2 with L = max f ∈F k kf fk kΨ L22.
Proof. The result is immediate since the maximum in the quantity L above is achieved since F <
| |
. (cid:4)
∞
5 Summary
Inthis work, weobtain instance-optimal convergence rates forlearning withthesquarelossfunction
anddependent data. Weovercome thetypicaldeflation, by the mixingtime, ofthesamplesize. The
main technical step to arrive at this result is a refined analysis of the multiplier process (1.8) via
mixed tail generic chaining that is suitable for dependent, β-mixing, random variables. Indeed, the
leading order term of our main result, Theorem 3.1, does not directly depend on any mixing-time
type quantities. It mimics the correct asymptotic rate and scales solely in terms of the statistics
of order 2q of the process at hand (where typically q = 1+o(1)). Finally, our result also allows
us to evaluate said multiplier process for a wider range of hypothesis classes. Typically, sharp
closed form expressions for this process are only available for linear functionals, covered in the iid
setting by Lecué and Mendelson [2013] and Oliveira [2016], and extended to the β-mixing setting
by Ziemann et al. [2023b]. By contrast, since our result relies on a weaker notion of topological
equivalence, it is applicable to more general classes, such as smoothness classes (Proposition 4.1)
and parametric classes with sufficiently regular parameterization (Proposition 4.2).
Acknowledgements
Ingvar Ziemann is supported by a Swedish Research Council international postdoc grant. Nikolai
Matni is supported in part by NSF award CPS-2038873, NSF award SLES-2331880 and NSF CA-
REER award ECCS-2045834. George J. Pappas acknowledges support from NSF award EnCORE-
2217062.
References
Yasin Abbasi-Yadkori, Dávid Pál, and Csaba Szepesvári. Improved algorithms for linear stochastic
bandits. Advances in neural information processing systems, 24, 2011.
Jean-Yves Audibert. Progressive mixture rules are deviation suboptimal. Advances in Neural
Information Processing Systems, 20, 2007.
Peter L Bartlett, Olivier Bousquet, and Shahar Mendelson. Local rademacher complexities. The
Annals of Statistics, 33(4):1497–1537, 2005.
Serge Bernstein. Sur l’extension du théorème limite du calcul des probabilités aux sommes de
quantités dépendantes. Mathematische Annalen, 97:1–59, 1927.
15Sjoerd Dirksen. Tail bounds via generic chaining. Electronic Journal of Probability, 20:1 – 29, 2015.
doi: 10.1214/EJP.v20-3760. URL https://doi.org/10.1214/EJP.v20-3760.
John C. Duchi, Alekh Agarwal, Mikael Johansson, and Michael I. Jordan. Ergodic mirror descent.
SIAM Journal on Optimization, 22(4):1549–1578, 2012.
Uwe Einmahl and Deli Li. Characterization of lil behavior in banach space. Transactions of the
American Mathematical Society, 360(12):6677–6693, 2008.
Mohamad Kazem Shirani Faradonbeh, Ambuj Tewari, and George Michailidis. Finite time identifi-
cation in unstable linear systems. Automatica, 96:342–353, 2018.
Daniel Hsu, Sham M Kakade, and Tong Zhang. Random design analysis of ridge regression. In
Conference on learning theory, pages 9–1. JMLR Workshop and Conference Proceedings, 2012.
Suhas Kowshik, Dheeraj Nagaraj, Prateek Jain, and Praneeth Netrapalli. Near-optimal offline
and streaming algorithms for learning non-linear dynamical systems. In Advances in Neural
Information Processing Systems, volume 34, pages 8518–8531, 2021.
Vitaly Kuznetsov and Mehryar Mohri. Generalization bounds for non-stationary mixing processes.
Machine Learning, 106(1):93–117, 2017.
GuillaumeLecuéandShaharMendelson. Learningsubgaussianclasses: Upperandminimaxbounds.
arXiv preprint arXiv:1305.4825, 2013.
Tengyuan Liang, Alexander Rakhlin, and Karthik Sridharan. Learning with square loss: Localiza-
tion through offset rademacher complexity. In Conference on Learning Theory, pages 1260–1285.
PMLR, 2015.
Stanislas Łojasiewicz. Sur la géométrie semi-et sous-analytique. In Annales de l’institut Fourier,
volume 43, pages 1575–1595, 1993.
Andreas Maurer. Generalization for slowly mixing processes. arXiv preprint arXiv:2305.00977,
2023.
Andreas Maurer and Massimiliano Pontil. Concentration inequalities under sub-gaussian and sub-
exponential conditions. Advances in Neural Information Processing Systems, 34:7588–7597, 2021.
Shahar Mendelson. Learning without concentration. In Conference on Learning Theory, pages
25–39. PMLR, 2014.
Shahar Mendelson. An unrestricted learning procedure. J. ACM, 66(6), nov 2019. ISSN 0004-5411.
doi: 10.1145/3361699. URL https://doi.org/10.1145/3361699.
Sean P. Meyn and Richard L. Tweedie. Markov Chains and Stochastic Stability. Springer-Verlag,
1993.
Mehryar Mohri and Afshin Rostamizadeh. Rademacher complexity bounds for non-i.i.d. processes.
Advances in Neural Information Processing Systems, 21, 2008.
Louis Nirenberg. On elliptic partial differential equations. Annali della Scuola Normale Superiore
di Pisa-Scienze Fisiche e Matematiche, 13(2):115–162, 1959.
16RobertoImbuzeiro Oliveira. The lower tail of random quadratic forms with applications toordinary
least squares. Probability Theory and Related Fields, 166(3):1175–1194, 2016.
Victor H Peña, Tze Leung Lai, and Qi-Man Shao. Self-normalized processes: Limit theory and
Statistical Applications. Springer, 2009.
Vincent Roulet and Alexandre d’Aspremont. Sharpness, restart and acceleration. Advances in
Neural Information Processing Systems, 30, 2017.
Abhishek Roy, Krishnakumar Balasubramanian, and Murat A Erdogdu. On empirical risk mini-
mization with dependent and heavy-tailed data. In Advances in Neural Information Processing
Systems, volume 34, 2021.
Tuhin Sarkar and Alexander Rakhlin. Near Optimal Finite Time Identification of Arbitrary Linear
Dynamical Systems. In International Conference on Machine Learning, pages 5610–5618, 2019.
Max Simchowitz, Horia Mania, Stephen Tu, Michael I. Jordan, and Benjamin Recht. Learning
without mixing: Towards a sharp analysis of linear system identification. In Conference On
Learning Theory, pages 439–473. PMLR, 2018.
Ingo Steinwart and Andreas Christmann. Fast learning from non-i.i.d. observations. Advances in
Neural Information Processing Systems, 22, 2009.
Anastasios Tsiamis, Ingvar Ziemann, Nikolai Matni, and George J Pappas. Statistical learning
theory for control: A finite-sample perspective. IEEE Control Systems Magazine, 43(6):67–97,
2023.
Martin J. Wainwright. High-dimensional statistics: A non-asymptotic viewpoint, volume 48. Cam-
bridge University Press, 2019.
Bin Yu. Rates of convergence for empirical processes of stationary mixing sequences. The Annals
of Probability, 22(1):94–116, 1994.
Ingvar Ziemann and Stephen Tu. Learning with little mixing. In Advances in Neural Information
Processing Systems, volume 35, pages 4626–4637, 2022.
Ingvar Ziemann, Anastasios Tsiamis, Bruce Lee, Yassir Jedra, Nikolai Matni, and George J Pappas.
A tutorial on the non-asymptotic theory of system identification. In 2023 62nd IEEE Conference
on Decision and Control (CDC), pages 8921–8939. IEEE, 2023a.
Ingvar Ziemann, Stephen Tu, George J Pappas, and Nikolai Matni. The noise level in linear re-
gression with dependent data. to appear in the proceedings of Advances in Neural Information
Processing Systems and arXiv:2305.11165, 2023b.
17A Properties of Ψ - and Lp-Norms
p
We begin with an elementary property.
Lemma A.1. For every two random variables Z,Z L we have that:
′
∈
Ψp
Z,Z 22/p Z Z . (A.1)
kh
′ ikΨ
p/2 ≤ k
kΨpk ′ kΨp
Proof. We compute:
Z,Z
′ Lm
Z,Z ′ Ψ = sup kh ik
kh ik p/2 m2/p
m 1
≥
(E Z,Z m)1/m
′
= sup |h i|
m2/p
m 1
≥
(E Z m Z m)1/m
′
sup k k k k ( , -Cauchy-Schwarz)
≤ m 1 m2/p h· ·i (A.2)
≥
(E Z 2mE Z 2m)1/2m
sup k k k ′ k (L2-Cauchy-Schwarz)
≤ m2/p
m 1
≥
(E Z 2m)1/2m (E Z 2m)1/2m
22/p sup k k sup k ′ k
≤ (2m)1/p (2m)1/p
m 1 m 1
≥ ≥
22/p Z Z , ( m 1 2m 1 )
≤ k
kΨpk ′ kΨp
{ ≥ } ⊂ { ≥ }
as was required. (cid:4)
A.1 Proof of Lemma 2.1
Lemma2.1 (Ψ -BernsteinMGFBound). Fixarandomvariable Z andp [1, ]such thatEZ 0
p
∈ ∞ ≤
and Z < . Let q and q be Hölder conjugates and suppose that λ [0,1/(q e)1/p Z ]. We
k
kΨp
∞
′
∈
′
k
kΨp
have that:
λ2 E(Z)2q 1/q
Eexp(λZ) exp 2 . (2.1)
≤ 1 −λ (cid:0)(q ′e)1/p k(cid:1)Z kΨp!
Proof. The idea of the proof is very much the same as that of the standard Bernstein MGF bound
but with the modification made in Maurer and Pontil [2021] by which the L norm is replaced by
∞
a Ψ -norm. We begin by expanding the exponential function:
p
∞
(λZ)m
Eexp(λZ) = E
m!
" #
m=0
X
∞
E (λZ)2(λZ)m
1+ (EZ 0)
(A.3)
≤ (m+2)! ≤
m=0 (cid:2) (cid:3)
X
1/q′
E λZ mq′
1+λ2 E(Z)2q 1/q ∞ | | . (Hölder’s Ineq.)
≤ (cid:16) h(m+2)i!(cid:17)
m=0
(cid:0) (cid:1) X
18We next have:
1/q′
E Z mq′ = Z m
| | k
kLmq′
(cid:16) h i(cid:17) (mq )m/p Z m (df. of Ψ ) (A.4)
≤ ′ k kΨp p
(m!)1/p(q e)m/p Z m . (Stirling’s Approximation)
≤ ′ k kΨp
Upon combining (A.3) with (A.4) we arrive at
Eexp(λZ)
(m!)1/pλm(q e)m/p Z m
1+λ2 E(Z)2q 1/q ∞ ′ k kΨp
≤ (m+2)!
m=0
(cid:0) (cid:1) X
λ2 E(Z)2q 1/q ∞ m (m!)1/p 1
1+ λ(q e)1/p Z p [1, ],m N
≤ 2
′
k
kΨp
∈ ∞ ∈ ⇒ (m+2)! ≤ 2
(cid:0) (cid:1) m X=0(cid:16) (cid:17) !
λ2 E(Z)2q 1/q
∞ 1
= 1+ 2 x [0,1) xm =
1 λ(q e)1/p Z ∈ ⇒ 1 x
− (cid:0) ′ k(cid:1) kΨp m=0 − !
X
λ2 E(Z)2q 1/q
exp 2 , (x R 1+x ex)
≤ 1 −λ (cid:0)(q ′e)1/p k(cid:1)Z kΨp! ∈ ⇒ ≤
(A.5)
as per requirement. (cid:4)
B Controlling the Multiplier Process
Lemma 2.2 (Pointwise Control). Fix two Hölder conjugates q and q . Suppose that the model
′
P is stationary and k-wise independent where k divides n. For every g,g L and u
(X,Y)1:n ′
∈
Ψp
∈
(0, ) we have that:
∞
n
P (1 E) W ,g(X ) g (X )
i i ′ i
− h − i
i=1
X
> 4n g g 2 V ( g g )u
k − ′ kL2 2q { }−{ ′ }
q
+4(q e)2/pk W g g u 2e u. (2.4)
′
k
kΨpk
−
′ kΨp
≤
−
!
Proof. First, note that we may assume throughout the proof that g g > 0, for otherwise the
′ L2
k − k
result is trivial. We now begin by applying Lemma 2.1:
k
Eexp λ (1 E) W ,g(X ) g (X )
i i ′ i
− h − i
!
i=1
X
λ2k g g 2 V 1 k W ,g(X ) g (X ) (B.1)
exp
k − ′ kL2 2q (cid:18)√k kg −g′ kL2 i=1h i i − ′ i i
(cid:19)
≤ 2 1 λ(q e)2/p k (1 E)P W ,g(X ) g (X )
 − ′ k i=1 − h i i − ′ i ikΨ p/2 
 (cid:16) (cid:17) 
 P 
191
as long as λ < (q e)2/p k (1 E) W ,g(X ) g (X ) − . Now, by triangle inequality
′ k i=1 − h i i − ′ i ikΨ p/2
and Lemma A.1,(cid:16) (cid:17)
P
k
λ(q ′e)2/p
(cid:13)
(1 −E) hW i,g(X i) −g ′(X i)
i(cid:13) ≤
λ(2q ′e)2/pk kW kΨpkg(X i) −g ′(X i) kΨp. (B.2)
(cid:13)Xi=1 (cid:13)Ψ
p/2
(cid:13) (cid:13)
(cid:13) (cid:13)
Consequent(cid:13)ly: (cid:13)
k
Eexp λ (1 E) W ,g(X ) g (X )
i i ′ i
− h − i
!
Xi=1 (B.3)
λ2k g g 2 V ( g g )
exp k − ′ kL2 2q { }−{ ′ } .
≤ 2 1 −λ(2q ′e)2/pk kW kΨpkg −g ′ kΨp !
Since the process is k-wise independ(cid:0)ent and mean zero we thus have t(cid:1)hat:
n
Eexp λ (1 E) W ,g(X ) g (X )
i i ′ i
− h − i
!
i=1
X (B.4)
λ2n g g 2 V ( g g )
exp k − ′ kL2 2q { }−{ ′ } .
≤ 2 1 −λ(2q ′e)2/pk kW kΨpkg −g ′ kΨp !
(cid:0) (cid:1)
Hence for every λ
∈
0, 2(2q ′e)2/pk kW kΨpkg −g
′ kΨp
−1 , Λ we have:
h (cid:17)
(cid:0) (cid:1)
n
Eexp λ (1 E) W ,g(X ) g (X )
i i ′ i
− h − i ! (B.5)
i=1
X
exp λ2n g g 2 V g g .
≤ k − ′ kL2 2q { }−{ ′ }
(cid:0) (cid:0) (cid:1)(cid:1)
Taking the above exponential inequality as a starting point, for a fixed u (0, ), a Chernoff
∈ ∞
argument now yields:
n
P (1 E) W ,g(X ) g (X ) > u
i i ′ i
− h − i
!
i=1
X
n
inf Eexp uλ+λ (1 E) W ,g(X ) g (X )
i i ′ i
≤ λ>0 − − h − i !
i=1
X
inf λu+
λ2n kg −g ′ k2 L2V 2q( {g }−{g ′ }) (B.6)
≤ λ ∈Λ − 2 1 −λ(2q ′e)2/pk kW kΨpkg −g ′ kΨp !
exp
4n (cid:16)kg
−g(cid:0)
′ k2
L2− Vu 22
q( {g }−{g′ }) (cid:17)!
u
≤
(cid:16) 2(n 2k qg ′− e)g 2′ /k p2 L(cid:1) k2 kV W2 kq Ψ( { pg k} g− −{ gg ′′ k} Ψ) p(cid:17),
≤  
 exp
(cid:18)4(2q′e)2/pk
kW−u
kΨpkg −g′ kΨp
(cid:19)
otherwise.




20Rescaling and summing the failure probabilities in either case yields:
n
P (1 E) W ,g(X ) g (X )
i i ′ i
− h − i
i=1
X
> 4n g g 2 V ( g g )u
k − ′ kL2 2q { }−{ ′ }
q
+4(2q e)2/pk W g g u 2e u, (B.7)
′
k
kΨpk
−
′ kΨp
≤
−
!
as was required. (cid:4)
Let us turn to making Lemma 2.2 uniform. By instantiating Theorem 3.5 of Dirksen [2015]
combined with the pointwise control of Lemma 2.2, we immediately have the following result.
Lemma B.1 (Corollary of Theorem 3.5 in Dirksen [2015]). Fix δ (0,1),r > 0 and consider the
∈
space F rS endowed with the metrics
⋆ L2
∩
d (g,g )= 4(q e)2/pk W g g ,
1 ′ ′
k
kΨpk
−
′ kΨp
(B.8)
d (g,g )= 4n(V (F rS )) g g ,
2 ′ 2q ⋆ L2 ′ L2
∩ k − k
q
and denote the corresponding diameters by ∆ i(F ⋆ ∩rS L2) , sup g,g′ ∈F ⋆ ∩rS L2 d i(g,g ′),i ∈ [2]. There
exist universal positive constants c and c such that with probability at least 1 δ:
1 2
−
n
sup (1 E) W ,f c (γ (F rS ,d )+γ (F rS ,d ))
i 1 2 ⋆ L2 2 1 ⋆ L2 1
f ∈F ⋆ ∩rS L2 Xi=1 − h i≤ ∩ ∩
+c ∆ (F rS ) log(1/δ)+∆ (F rS )log(1/δ) .
2 2 ⋆ L2 1 ⋆ L2
∩ ∩
(cid:16) p (cid:17)(B.9)
We now restate and prove the result for the multiplier process.
Theorem 2.1. Fixafailureprobability δ (0,1), apositivescalar r (0, ), twoHölderconjugates
∈ ∈ ∞
q and q , and aclass F. Suppose that F F is (L,η)-Ψ . Suppose further that the model P
′ ⋆
−
⋆ p (X,Y)1:n
isstationaryandk-wiseindependent where k divides n. Thereexist universalpositiveconstants c ,c
1 2
such that for any r (0,1] we have that with probability at least 1 δ:
∈ −
n
1
sup (1 E) W ,f
i
f ∈F ⋆ ∩rS L2 rn Xi=1 − h i
1 log(1/δ)
c V (F rS ) γ (F rS ,d )+
≤
2 2q ⋆
∩
L2
r√n
2 ⋆
∩
L2 L2
n
r !
q
1 rη 1
+c (q e)2/pLk W γ (F rS ,d )+ − log(1/δ) . (2.6)
1 ′
k
kΨp
rn
η ⋆
∩
L2 L2
n
(cid:18) (cid:19)
21Proof. We need to translate the metrics (appearing in Lemma B.1) d ,d and their diameters into
1 2
the standard L2-metric using that the class is (L,η)-Ψ . We begin with d , which is just a dilated
p 2
L2-metric:
∞
γ (F rS ,d ) = inf sup 2m/2d (f,F )
2 ⋆ L2 2 2 m
∩ {Fm }f ∈F ⋆ ∩rS L2m X=0 (B.10)
= 4n(V (F rS ))γ (F rS ,d ),
2q ⋆ L2 2 ⋆ L2 L2
∩ ∩
q
and
∆ (F rS ) r 4n(V (F rS )). (B.11)
2 ⋆ L2 2q ⋆ L2
∩ ≤ ∩
q
Turning to the d , we have:
1
∞
γ (F rS ,d )= inf sup 2md (f,F )
1 ⋆ L2 1 1 m
∩ {Fm }f ∈F ⋆ ∩rS L2m X=0
∞
= inf sup 2m2(q e)2/pk W d (f,F )
{Fm }f ∈F ⋆ ∩rS L2m X=0
′
k
kΨp Ψp m
2(q e)2/pLk W γ (F rS ,dη )
≤ ′ k kΨp 1 ⋆ ∩ L2 L2
2(q e)2/pLk W γ (F rS ,d ),
≤
′
k
kΨp η ⋆
∩
L2 L2
where the first inequality uses that G is (L,η)-Ψ and the last inequality uses that γ (F
p 1 ⋆
rS ,dη ) γ (F rS ,d ) as long as r 1. Similarly: ∩
L2 L2 ≤ η ⋆ ∩ L2 L2 ≤
∆ (F rS ) 2(q e)2/pLk W rη. (B.12)
1 ⋆
∩
L2
≤
′
k
kΨp
The result follows by substituting the above expressions into the result of Dirksen [2015] captured
as Lemma B.1. (cid:4)
C Controlling the Quadratic Process
Lemma C.1 (Truncation Accuracy). Fix ε,r > 0, let G be (L,η)-Ψ , and let g G be such that
p
∈
g = r. For τ R , define g , g1 . There exists a truncation level τ and a universal
L2 + τ g τ
k k ∈ k k≤
positive constant c> 0 such that:
g 2 g 2 r2ε (C.1)
k kL2 −k τ kL2 ≤
and
1/p
42/pL1/2r2η
τ Lrη c 1log . (C.2)
−
≤ εr4
!!
Proof. Fix a level τ > 0 to be determined later. For any such level we have that:
g 2 g 2 E[ g 21 ] E g 4P( g > τ) E g 4exp cτp/ g p . (C.3)
k kL2 −k τ kL2 ≤ k k kg k>τ ≤ k k k k ≤ k k − k kΨp
p p (cid:16) (cid:17)
Hence if we choose τp = c −1 kg kp
Ψp
log ε√ r2E Ekg gk4
2
we have:
(cid:18) k k (cid:19)
g 2 g 2 ε2. (C.4)
k kL2 −k τ kL2 ≤
22It remains to derive an upper bound on τ. Since G is (L,η)-Ψ and g = r we have that
p L2
k k
g L g η = Lrη, and
k kΨp ≤ k k2 (C.5)
E g 4 44/p g 4 44/pLr4η.
k k ≤ k kΨp ≤
Hence our choice of τ satisfies:
1/p
42/pL1/2r2η
τ Lrη c 1log (C.6)
−
≤ εr4
!!
and so the result has been established. (cid:4)
Theorem 2.2 (Lower Uniform Law). Fix a failure probability δ (0,1), a tolerance ε > 0, a
∈
localization radius r (0,1], and two Hölder conjugates q and q . Suppose that F F is (L,η)-
′ ⋆ ⋆
∈ −
Ψ . Suppose further that the model P is stationary and k-wise independent where k divides
p (X,Y)1:n
n. There exist a universal positive constant c such that uniformly for all f F rB we have
⋆ L2
∈ \{ }
that with probability at least 1 δ:
−
n
1
f(X ) 2 r2(1 ε2)
i
n k k ≥ −
i=1
X
1/p
−c (n −1/2√kL1+3/4rη log
42 ε/ rpL
!!
γ
2+
46η(F
⋆
∩rS L2,d L2)+r1+ 43η log(1/δ)
(cid:16) p (cid:17)
1/p
42/pL
+n 1(q )1/pkrη log L2(γ (F rS ,d )+rηlog(1/δ)) . (2.7)
− ′
εr
η ⋆
∩
L2 L2
!! )
Proof. By star-shapedness, we may assume without loss of generality that f F rS . Fix τ
⋆ L2
∈ ∩
such that for all such f
f 2 f 2 ε2, (C.7)
k kL2 −k τ kL2 ≤
and note that the existence of such a level is guaranteed by Lemma C.1.
It is then clear that:
n n
1 1
f(X ) 2 r2(1 ε2) sup f (X ) 2 f 2 (C.8)
n Xi=1k i k ≥ − − f ∈F ⋆ ∩rB L2(n Xi=1k τ i k −k τ kL2 )
and, for well-chosen ε,r, it therefore suffices to control the supremum of empirical process to the
right of (C.8) and we will use Dirksen’s theorem again to do so [Dirksen, 2015, Theorem 3.5]. A
preliminary estimate using k-wise independence, stationarity and Lemma 2.1 gives that for every
f,g and admissible λ:
n
Eexp λ f (X ) 2 f 2 g (X ) 2+ g 2
k τ i k −k τ kL2 −k τ i k k τ kL2
" #!
i=1
X
λ2nV 1 k f (X ) 2 f 2 g (X ) 2+ g 2
4 √k i=1k τ i k −k τ kL2 −k τ i k k τ kL2
exp . (C.9)
≤  2 1 λ(q(cid:16) e)1/pPk f (X ) 2 f 2 g (X ) 2+ g 2 (cid:17) 
 − ′ k τ i k −k τ kL2 −k τ i k k τ kL2 Ψ p/2 
 (cid:16) (cid:13) (cid:13) (cid:17)
(cid:13) (cid:13)
23This is almost the exponential inequality we need, but we will want increment conditions for the
above empirical process in terms Ψ and L2.
p
The increment condition in Ψ is simple. We observe that for any two f,g and any x in their
p
domain:
f (x) 2 g (x) 2 = (f +g )(x),(f +g )(x) . (C.10)
τ τ τ τ τ τ
k k −k k h i
Consequently by Lemma A.1 and τ-truncation:
f (X) 2 g (X) 2 22/p f +g f g 21+2/pτ f g . (C.11)
kk
τ
k −k
τ
k
kΨ
p/2 ≤ k
τ τ kΨpk τ
−
τ kΨp
≤ k −
kΨp
A centering argument thus gives:
f (X) 2 f 2 g (X) 2 + g 2 22+2/pτ f g (C.12)
kk τ k −k τ kL2 −k τ k k τ kL2kΨ p/2 ≤ k − kΨp
wherefore we set
d (f,g) , (q e)1/pk22+2/pτ f g . (C.13)
1 ′
k −
kΨp
Let us next address the variance term:
k
1
V f (X ) 2 g (X ) 2
4 τ i τ i
√k k k −k k
!
i=1
X
k
1
=V f (X )+g (X ),f (X ) g (X ) (use (C.10))
4 τ i τ i τ i τ i
√k h − i
!
i=1
X
4
k
1
E f (X )+g (X ),f (X ) g (X ) (V [] E[ 4])
≤v
u
√k h τ i τ i τ i − τ i i
!
4 · ≤ |·|
u Xi=1 p
t
k E( f (X)+g (X),f (X) g (X) )4 (Cauchy-Schwarz)
τ τ τ τ
≤ h − i
4kq τ2 f g 2 ,d2(f,g). (τ-boundedness and Cauchy-Schwarz)
≤ k − kL4 2
(C.14)
With d ,d as in (C.13) and (C.14), we can now estimate (C.9) as:
1 2
n λ2nd2(f,g)
Eexp λ f (X ) 2 f 2 g (X ) 2+ g 2 exp 2 . (C.15)
k τ i k −k τ kL2 −k τ i k k τ kL2 ≤ 2(1 λd (f,g))
" i=1 #! (cid:18) − 1 (cid:19)
X
We thus obtain the probability estimate (u > 0):
n
1
P f (X ) 2 f 2 g (X ) 2+ g 2 > c u/nd (f,g)+c(u/n)d (f,g) 2e u
n k τ i k −k τ kL2 −k τ i k k τ kL2 ′ 2 1 ≤ −
!
i=1
X p
(C.16)
for two universal positive constants c,c. After defining (normalizing) for some universal positive
′
constants c ,c :
1 2
d˜ (f,g) , cn 1d (f,g) = c n 1(q e)1/pk22+2/pτ f g , (C.17)
1 − 1 1 − ′
k −
kΨp
d˜ (f,g) , cn 1/2d (f,g) = c n 1/2 kτ2 f g 2 , (C.18)
2 ′ − 2 2 − k − kL4
(cid:0) (cid:1)
24we notice that (C.16) is consistent with the mixed tail generic chaining condition in Dirksen [2015,
Equation 12] for metrics d˜ ,d˜ . Consequently, by Theorem 3.5 in Dirksen [2015] we have that:
1 2
n
1
sup f (X ) 2 f 2 c (γ (F rB ,d˜ )+√u∆ (F rB ))
f ∈F ⋆ ∩rB L2(n Xi=1k τ i k −k τ kL2 )≤ 3 2 ⋆ ∩ L2 2 d˜ 2 ⋆ ∩ L2
+c (γ (F rB ,d˜ )+u∆ (F rB )) (C.19)
4 1 ⋆
∩
L2 1 d˜
1
⋆
∩
L2
for two universal positive constants c ,c .
3 4
To finish the proof, we turn to relating the quantities γ and ∆ in terms of problem data. We
have (recalling (C.17)):
∆ (F rB ) = cn 1(q e)1/pk22+2/pτ∆ (F rB ) cn 1(q e)1/pk22+2/pτLrη (C.20)
d˜
1
⋆
∩
L2 − ′ Ψp ⋆
∩
L2
≤
− ′
and also (recalling (C.18)):
∆ (F rB ) cn 1/22√kτ∆ (F rB )
d˜ 2 ⋆ ∩ L2 ≤ ′ − d L2 ⋆ ∩ L4 (C.21)
c ′n
−1/22√kτL3/4r1+ 43η
,
≤
where we used Cauchy-Schwarz and the class assumption in the last step to control the L4 norm by
the L2 norm.
As for γ-functionals, we have:
γ (F rS ,d˜ ) cn 1(q e)1/pk22+2/pτLγ (F rS ,d ) (C.22)
1 ⋆ L2 1 − ′ η ⋆ L2 L2
∩ ≤ ∩
and
γ (F rS ,d˜ ) cn 1/22√kτL3/4γ (F rS ,d ). (C.23)
2 ⋆ L2 2 ′ − 2+6η ⋆ L2 L2
∩ ≤ 4 ∩
Putting everything together we thus obtain that:
n
1
sup f (X ) 2 f 2
f ∈F ⋆ ∩rB L2(n Xi=1k τ i k −k τ kL2 )
CL3/4n −1/2√kτ γ 2+6η(F
⋆
rS L2,d L2)+r1+ 43η log(1/δ)
≤ 4 ∩
+C 2C′′/pn 1((cid:16) q )1/pkτL(γ (F rS ,d )+prηlog(1/δ(cid:17) ))
′ − ′ η ⋆ L2 L2
∩
1/p
= Cn −1/2√kL1+3/4rη c −1log
42 ε/ rpL
!!
γ
2+
46η(F
⋆
∩rS L2,d L2)+r1+ 43η log(1/δ)
(cid:16) p (cid:17)
1/p
42/pL
+C 2C′′/pn 1(q )1/pkrη c 1log L2(γ (F rS ,d )+rηlog(1/δ)),
′ − ′ −
εr
η ⋆
∩
L2 L2
!!
for universal positive constants C,C ,C . Since p 1 we may replace all the terms containing
′ ′′
≥
upper-case universal constants by a single universal constant as in the theorem statement. (cid:4)
25D Results for Mixing Empirical Processes
D.1 Blocking
Recall that we partition [n] into 2m consecutive intervals, denoted a for j [2m], so that
j
2m a = n. Denote further by O (resp. by E) the union of the oddly (resp.∈ evenly) indexed
j=1| j |
subsets of [n]. We further abuse notation by writing β (a )= β (a ) in the sequel.
Z i Z i
P | |
We split the process Z as:
1:n
Zo , (Z ,...,Z ), Ze , (Z ,...,Z ). (D.1)
1:O a1 a2m−1 1:E a2 a2m
| | | |
Let Z˜o and Z˜e be blockwise decoupled versions of (D.1). That is we positthat Z˜o P
1: |O | 1: |E | 1: |O | ∼ Z˜ 1o :|O|
and Z˜e P , where:
1: |E | ∼ Z˜ 1e :|E|
P , P P P and P , P P P . (D.2)
Z˜ 1o
:|O|
Za1
⊗
Za3
⊗···⊗
Za2m−1 Z˜ 1e
:|E|
Za2
⊗
Za4
⊗···⊗
Za2m
The process Z˜ with the same marginals as Z˜o and Z˜e is said to be the decoupled version
1:n 1:O 1:E
of Z . To be clear: P , P P | P| , so | th| at Z˜o and Z˜e are alternatingly
1:n Z˜ 1:n Za1 ⊗ Za2 ⊗···⊗ Za2m 1:O 1:E
embedded in Z˜ . The following result is key—by skipping every oth| e| r block, Z|˜| may be used in
1:n 1:n
place of Z for evaluating bounded scalar functionals, such as probabilities of measurable events,
1:n
at the cost of an additive mixing-related term.
Proposition D.1 (Lemma 2.6 in Yu [1994]; Proposition 1 in Kuznetsov and Mohri [2017]). Fix a
β-mixing process Z and let Z˜ be its decoupled version. For any measurable function f of Zo
1:n 1:n 1:O
(resp. g of Ze ) with joint range [0,1] we have that: | |
1:E
| |
E(f(Zo )) E(f(Z˜o )) β (a ),
| 1:O − 1:O | ≤ Z i
| | | |
i E 2m
∈ X\{ } (D.3)
E(g(Ze )) E(g(Z˜e )) β (a ).
| 1:E − 1:E | ≤ Z i
| | | |
i O 1
∈X\{ }
D.2 Controlling Empirical Processes for β-Mixing Data
ApplyingPropositionD.1toTheorem2.1andTheorem2.2yieldsthedesiredcontrolofthemultiplier
and quadratic processes also for β-mixing data.
Proposition D.2. Fix a failure probability δ (0,1), a positive scalar r (0, ), two Hölder
∈ ∈ ∞
conjugates q and q , and a class F. Suppose that F F is (L,η)-Ψ . Suppose further that the
′ ⋆ ⋆ p
−
model P is stationary and β-mixing and suppose further that k N divides n/2. There exist
(X,Y)1:n
∈
universal positive constants c ,c such that for any r (0,1] we have that with probability at least
1 2
∈
261 δ nβ(k):
− − k
n
1
sup (1 E) W ,f
i
f ∈F ⋆ ∩rS L2 rn Xi=1 − h i
1 log(1/δ)
c V (F rS ) γ (F rS ,d )+
≤
2 2q ⋆
∩
L2
r√n
2 ⋆
∩
L2 L2
n
r !
q
1 rη 1
+c (q e)2/pLk W γ (F rS ,d )+ − log(1/δ) (D.4)
1 ′
k
kΨp
rn
η ⋆
∩
L2 L2
n
(cid:18) (cid:19)
Proposition D.3. Fix a failure probability δ (0,1), a tolerance ε > 0, a localization radius
∈
r (0,1], and two Hölder conjugates q and q . Suppose that F F is (L,η)-Ψ . Suppose further
′ ⋆ ⋆ p
∈ −
that the model P is stationary and β-mixing and suppose further that k N divides n/2.
(X,Y)1:n
∈
There exists a universal positive constant c such that uniformly for all f F rS we have that
⋆ L2
∈ ∩
with probability at least 1 δ nβ(k):
− − k
n
1
f(X ) 2 r2(1 ε2)
i
n k k ≥ −
i=1
X
1/p
−c (n −1/2√kL1+3/4rη log
42 ε/ rpL
!!
γ
2+
46η(F
⋆
∩rS L2,d L2)+r1+ 43η log(1/δ)
(cid:16) p (cid:17)
1/p
42/pL
+n 1(q )1/pkrη log L2(γ (F rS ,d )+rηlog(1/δ)) . (D.5)
− ′
εr
η ⋆
∩
L2 L2
!! )
E Proof of Theorem 3.1
Before we finish the proof of the main result, let us first make formal the justification for the
introduction of quadratic and multiplier processes in Section 1.1. The following lemma bounds the
excess risk of empirical risk minimizer in terms of these.
LemmaE.1(LocalizedBasicInequality). Supposethateither(1)F isconvex or(2)F isrealizable.
For every r > 0 we have that:
2
f f 2 r2+ sup M (g) + sup Q (g). (E.1)
k − ⋆ kL2 ≤ g ∈F ⋆ ∩rS L2 n ! g ∈F ⋆ n
b
Proof. We begin by observing that the optimality of f to (1.2) yields the basic inequality:
n n
1 2
f(X ) f (X ) 2 b W ,(f f )(X ) . (E.2)
i ⋆ i i ⋆ i
n k − k ≤ n h − i
i=1 i=1
X X
If F is convex, we have thb at E W ,(f f )(X ) 0 forb every f (by optimality of f to the
i ⋆ i ⋆
h − i ≤
population objective). If instead F is realizable the same holds true but with equality. Hence, in
either case:
n n
1 2
f(X ) f (X ) 2 (1 E) W ,(f f )(X ) (E.3)
i ⋆ i ′ i ⋆ i
n k − k ≤ n − h − i
i=1 i=1
X X
b b
27where E denotes expectation with respect to a fresh copy of randomness (independent of the data
′
used to construct f).
Consequently we also have that:
b
n n
(1+ε) (1+ε))
f f 2 = f(X ) f (X ) 2 + f f 2 f(X ) f (X ) 2
k − ⋆ kL2 n k i − ⋆ i k k − ⋆ kL2 − n k i − ⋆ i k
i=1 i=1
X X
b 2(1+ε) n b b (1+ε)) n b
(1 E) W ,(f f )(X ) + f f 2 f(X ) f (X ) 2 (E.4)
≤ n − ′ h i − ⋆ i i k − ⋆ kL2 − n k i − ⋆ i k
i=1 i=1
X X
b b b
Fix now a radius r and set g = r (f f ). If f f r, dividing both sides above
⋆ ⋆ L2
kf −f⋆ kL2 − k − k ≥
by kf −f
⋆ kL2
yields for the first termbabove i bn (E.4):
b
n
b 2(1+ε)
(1 E) W ,(f f )(X )
′ i ⋆ i
n f f − h − i
k − ⋆ kL2 Xi=1
1 n b
= b 2(1+ε)(1 E) W ,r 1g(X ) (df. of g and divide) (E.5)
′ i − i
n − h i
i=1
X(cid:8) (cid:9)
sup M (g).
n
≤ g ∈F ⋆ ∩rS L2
Either the above inequality holds or f f r. For every r > 0 it is thus true that:
⋆ L2
k − k ≤
2
b
f f 2 r2+ sup M (g) + sup Q (g) (E.6)
k − ⋆ kL2 ≤ g ∈F ⋆ ∩rS L2 n ! g ∈F ⋆ n
b
This proves the claim. (cid:4)
Finishing the proof of Theorem 3.1. We apply Lemma E.1 with r = r , ε = 1/2 and
⋆
note that n c max n (r ),n (r ) implies: (1) in combination with Proposition D.3 that
3 quad ⋆ mult ⋆
≥ { }
2
sup Q (g) . r2; and (2) in combination with Proposition D.2 that sup M (g)
g ∈F ⋆ n ⋆ g ∈F ⋆ ∩r⋆S L2 n
scales at most like the RHS of (3.5). The result follows by a union boun(cid:16)d over the failure even(cid:17)ts
of Proposition D.2 and Proposition D.3, all the while taking into account the fact that we posit
k k . (cid:4)
mix
≥
E.1 Proof of Corollary 3.1
Corollary 3.1 (Parametric Classes). Fix a failure probability δ (0,1), two Hölder conjugates q,q ,
′
∈
and a class F that is either (1) convex or (2) realizable. Suppose that F F is (L,η)-Ψ . Suppose
⋆ ⋆ p
−
further that the model P is stationary and that k divides n/2.
(X,Y)1:n
There exists a universal positive constant c and a polynomial function φ such that the following
η
holds true. Suppose that there exists dF R
+
such that for s > 0:
∈
1
log NL2(F ⋆,s)
≤
dF log
s
(3.4)
(cid:18) (cid:19)
28We have with probability 1 4δ that:
−
f f 2 c V F
dFk kW k2
L2S
dF +log(1/δ)
(3.5)
k − ⋆ kL2 ≤ 1 2q  ⋆ ∩s n L2  n
(cid:18) (cid:19)
b  
as long as kβ 1(k) nδ 1 and
− −
≥
dFk W 2
n
≥
φ
η
dF,k, kW kΨp,L,q,q ′,V −1 F
⋆
∩s
k
n
kL2S
L2
,log(1/δ) . (3.6)
!
 
Proof. Let us begin by observing that for some constant c only depending on η we have that:
η
r 1 1/η
γ η(F
⋆
∩rS L2,d L2)
≤
c
η
dF log
s
ds
Z0 (cid:18) (cid:18) (cid:19)(cid:19)
1 r 1/η (E.7)
1/η
= c d r log ds
η F
s
Z0
(cid:16) (cid:16) (cid:17)(cid:17)
1/η
c d rΓ(1/η+1).
η F
≤
Hence for some universal positive constant c:
1 1
V(F rS ) γ (F rS ,d ) c V(F rS ) d1/2 . (E.8)
⋆
∩
L2
× r√n
2 ⋆
∩
L2 L2
≤
⋆
∩
L2
× √n
F
p p
A few applications of the Cauchy-Schwarz inequality now yields for any r:
V(F rS ) k W 2 . (E.9)
⋆ ∩ L2 ≤ k kL2
dFVF ⋆ ∩rdFkk nWk2 L2S L2
v
A candidate choice is therefore r ⋆ = u u  n . A straighforward but tedious
calculation now reveals that the inequalitty n max n quad(r ⋆),n mult(r ⋆) has a solution depending
≥ { }
polynomially on problem data as long as η > 1/4. (cid:4)
29