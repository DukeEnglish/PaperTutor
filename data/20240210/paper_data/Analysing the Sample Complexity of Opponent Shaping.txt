Analysing the Sample Complexity of Opponent Shaping
KittyFung‚àó QizhenZhang‚àó ChrisLu
UniversityofOxford UniversityofOxford UniversityofOxford
Oxford,UnitedKingdom Oxford,UnitedKingdom Oxford,UnitedKingdom
kittyfung01@gmail.com qizhen.zhang@eng.ox.ac.uk christopher.lu@exeter.ox.ac.uk
JiaWan TimonWilli JakobFoerster
MassachusettsInstituteofTechnology UniversityofOxford UniversityofOxford
Cambridge,UnitedStates Oxford,UnitedKingdom Oxford,UnitedKingdom
jiawan@mit.edu timon.willi@eng.ox.ac.uk jakob.foerster@eng.ox.ac.uk
Abstract Keywords
Learningingeneral-sumgamesoftenyieldscollectivelysub-optimal OpponentShaping;Multi-Agent;ReinforcementLearning;Meta
results.Addressingthis,opponentshaping(OS)methodsactively ReinforcementLearning;SampleComplexity
guidethelearningprocessesofotheragents,empiricallyleading
toimprovedindividualandgroupperformancesinmanysettings. ACMReferenceFormat:
EarlyOSmethodsusehigher-orderderivativestoshapethelearning KittyFung,QizhenZhang,ChrisLu,JiaWan,TimonWilli,andJakobFo-
ofco-players,makingthemunsuitabletoshapemultiplelearning erster.2024.AnalysingtheSampleComplexityofOpponentShaping.In
steps. Follow-up work, Model-free Opponent Shaping (M-FOS), Proc.ofthe23rdInternationalConferenceonAutonomousAgentsandMul-
addressesthesebyreframingtheOSproblemasameta-game.In tiagentSystems(AAMAS2024),Auckland,NewZealand,May6‚Äì10,2024,
contrasttoearlyOSmethods,thereislittletheoreticalunderstand- IFAAMAS,18pages.
ingoftheM-FOSframework.Providingtheoreticalguaranteesfor
M-FOSishardbecauseA)thereislittleliteratureontheoretical 1 Introduction
samplecomplexityboundsformeta-reinforcementlearningB)M-
Learningingeneral-sumgamescommonlyleadstocollectively
FOSoperatesincontinuousstateandactionspaces,sotheoretical
worst-caseoutcomes[6].Toaddressthis,opponentshaping(OS)
analysisischallenging.Inthiswork,wepresentR-FOS,atabular
methodsaccountforopponents‚Äôlearningstepsandinfluenceother
versionofM-FOSthatismoresuitablefortheoreticalanalysis.R-
agents‚Äôlearningprocesses.Empirically,thiscanimproveindividual
FOSdiscretisesthecontinuousmeta-gameMDPintoatabularMDP.
andgroupperformances.
WithinthisdiscretisedMDP,weadapttheùëÖ algorithm,most
ùëöùëéùë•
prominentlyusedtoderivePAC-boundsforMDPs,asthemeta- EarlyOSmethods[6,10,12]relyonhigher-orderderivatives,which
learnerintheR-FOSalgorithm.Wederiveasamplecomplexity arehigh-varianceandresultinunstablelearning.Theyarealso
boundthatisexponentialinthecardinalityoftheinnerstateand myopic,focusingonlyontheopponent‚Äôsimmediatefuturelearning
actionspaceandthenumberofagents.Ourboundguaranteesthat, stepsratherthantheirlong-termdevelopment[14].Recentwork,
withhighprobability,thefinalpolicylearnedbyanR-FOSagent Model-freeOpponentShaping(M-FOS)[14],solvestheabovechal-
isclosetotheoptimalpolicy,apartfromaconstantfactor.Finally, lenges.M-FOSintroducesameta-gamestructure,eachmeta-step
weinvestigatehowR-FOS‚Äôssamplecomplexityscalesinthesizeof representinganepisodeoftheembedded‚Äúinner‚Äùgame.Themeta-
state-actionspace.Ourtheoreticalresultsonscalingaresupported stateconsistsof‚Äúinner‚Äùpolicies,andthemeta-policygeneratesan
empiricallyintheMatchingPenniesenvironment. innerpolicyateachmeta-step.M-FOSusesmodel-freeoptimisation
techniquestotrainthemeta-policy,eliminatingtheneedforhigher-
orderderivativestoaccomplishlong-horizonopponentshaping.
‚àóEqualContribution TheM-FOSframeworkhasshownpromisinglong-termshaping
WorkdoneattheFoersterLabForAIResearch(FLAIR),UniversityofOxford resultsinsocial-dilemmagames[9,14].
CorrespondingAuthors:kittyfung01@gmail.com,qizhen.zhang@eng.ox.ac.uk
TheoriginalM-FOSpaperpresentstwocasesoftheM-FOSalgo-
rithm.Forsimpler,low-dimensionalgames,M-FOSlearnspolicy
updatesdirectlybytakingpoliciesasinputandoutputtingthenext
policyasanaction.Inputtingandoutputtingentirepoliciesdoes
ThisworkislicensedunderaCreativeCommonsAttribution
International4.0License. notextendwelltomorecomplex,higher-dimensionalgames,e.g.
whenpoliciesarerepresentedasneuralnetworks.Theoriginal
Proc.ofthe23rdInternationalConferenceonAutonomousAgentsandMultiagentSystems M-FOSpaperalsoproposesavariantwhichusestrajectoriesas
(AAMAS2024),N.Alechina,V.Dignum,M.Dastani,J.S.Sichman(eds.),May6‚Äì10,2024, inputsinsteadoftheexactpolicyrepresentations.Inthisworkwe
Auckland,NewZealand.¬©2024InternationalFoundationforAutonomousAgentsand
MultiagentSystems(www.ifaamas.org). derivethesamplecomplexityforbothcases.
4202
beF
8
]GL.sc[
1v28750.2042:viXraWhereassomepreviousOSalgorithmsenjoystrongtheoretical betweentheoryandexperimentsbydemonstratingthatinboth
foundationsthankstotheDifferentiableGamesframework[1],the realms,samplecomplexityscalesexponentiallywiththeinner-
M-FOSframeworkhasnotbeeninvestigatedtheoretically.Under- game‚Äôsstate-actionspacesize.
standingthesamplecomplexityofanalgorithmishelpfulinmany
ways,suchasevaluatingitsefficiencyorpredictingthelearning 2 RelatedWork
time.However,providingtheoreticalguaranteesforM-FOSischal- TheoreticalAnalysisofDifferentiableGames:Muchpastwork
lengingbecauseA)thereisverylittleliteratureontheoreticalsam- assumesthatthegamebeingoptimisedisdifferentiable[1].This
plecomplexityboundsforevensingle-agentmeta-reinforcement assumptionenablesfareasiertheoreticalanalysisbecauseonecan
learning(RL),letalonemulti-agentandB)M-FOSoperatesincon- directlyuseend-to-endgradient-basedmethodsratherthanrein-
tinuousstateandactionspace(themeta-game). forcementlearninginthosesettings.Severalworksinthisarea
Inthiswork,wepresentR-FOS,atabularalgorithmapproximation investigatetheconvergencepropertiesofvariousalgorithmsBal-
ofM-FOS.UnlikeM-FOS,whichoperatesinacontinuousmeta- duzzietal.[1],Letcher[11],Sch√§ferandAnandkumar[17].
MDP,R-FOSoperatesinadiscreteapproximationoftheoriginal
OpponentShaping:Morecloselyrelatedtoourworkaremethods
meta-MDP.TheresultingdiscreteMDPallowsustoperformrigor-
thatspecificallyanalyseOS.SOS[12]andCOLA[21]bothanalyse
oustheoreticalanalysis.WeadaptR-FOSfromM-FOSsuchthatit
opponent-shapingmethodsthatoperateinthedifferentiablegames
stillmaintainsallthekeypropertiesofM-FOS.Withinthisdiscrete,
framework.Theseworksprovidetheoreticalconvergenceanalysis
approximateMDP,R-FOSappliestheùëÖ algorithm[8,20]tothe
MAX foropponent-shapingalgorithms;however,neitherworkanalyzes
M-FOSmeta-game.ùëÖ isamodel-basedreinforcementlearning
MAX samplecomplexity.POLA[23]theoreticallyanalysesanOSmethod
(MBRL)algorithmtypicallyusedforthesamplecomplexityanalysis
thatisinvarianttopolicyparameterization.M-FOSdoesnotoperate
oftabularMDPs.UsingexistingresultsdevelopedforùëÖ [3],
MAX inthedifferentiablegamesframework.WhilethisenablesM-FOS
wederiveanexponentialsamplecomplexityPAC-bound,which
toscaletomorechallengingenvironments,suchasCoinGame[14],
guaranteeswithhighprobability(1-ùõø)thattheoptimalpolicyinthe
itcomesatthecostofconvenienttheoreticalanalysis.Khanetal.
discretisedmeta-MDPisveryclose(<ùúñaway)tothepolicylearned
[9]empiricallyscalesM-FOStomorechallengingenvironments
byR-FOS.Wethenderiveseveralboundswhichguaranteepolicies
withlargerstatespaces,whileLuetal.[15]empiricallyinvestigates
betweentheoriginalmeta-MDPandthediscretisedmeta-MDPare
applying M-FOS to a state-based adversary. To the best of our
closetoeachotheruptoaconstantdistance.Lastly,combiningall
knowledge,ourworkisthefirsttotheoreticallyanalyseOSoutside
ofthepreviousboundswederived,weobtainthefinalexponential
ofthedifferentiablegamesframework.Furthermore,ourworkis
samplecomplexityresult.
thefirsttoanalysethesamplecomplexityofanOSmethod.
Fornotationalsimplicity,wemostlyomitthe‚Äúmeta‚Äùprefixinthe
TheoreticalAnalysisofSampleComplexityinRL:Thereare
restofthepaper.Forexample,theterms‚ÄúMDP‚Äù,‚Äútransitionfunc-
severalworksthatusetheùëÖ [3]frameworktoderivethesample
MAX
tion‚Äù,and‚Äúpolicy‚Äù,refertothemeta-MDP,meta-transitionfunction,
complexityofRLalgorithmsacrossavarietyofsettings.Closely
andmeta-policyrespectively.Weusetheprefix‚Äúinner‚Äùwhenever
related to our work is Zhang et al. [22], which uses the ùëÖ
MAX
werefertotheinnergame.Furthermore,ouranalysisofM-FOS
algorithmtoderivesamplecomplexityboundsforlearninginfully-
islimitedtotheasymmetricshapingcase(i.e.themeta-gameof
cooperativemulti-agentRL.
shapinganaiveinner-learner‚àó)andweleavetheextensiontometa-
selfplayforfuturework. Ourworkisalsorelatedtomethodsthatanalysesamplecomplexity
oncontinuous-spaceRL.Analyzingthesamplecomplexityofalgo-
Ourcontributionsarethree-fold:
rithmsincontinuous-spaceRLisparticularlychallengingbecause
(1) WepresentR-FOS(seeAlgorithm1),atabularapproximationof thereareaninfinitenumberofpotentialstates.Toaddressthis,
M-FOS.Insteadoflearningameta-policyinsidethecontinuous numeroustechniqueshavebeensuggestedthateachmakespecific
meta-MDPùëÄ,R-FOSlearnsameta-policyinsideadiscretised assumptions:LiuandBrunskill[13]assumesastationaryasymp-
meta-MDPwhichapproximatesùëÄ.InsidethisdiscretisedMeta- toticoccupancydistributionunderarandomwalkintheMDP.
MDP,R-FOSusesùëÖ asthemeta-agent.NotethatR-FOS Maliketal.[16]usesaneffectiveplanningwindowtohandleMDPs
MAX
stillmaintainskeypropertiesoftheoriginalM-FOSalgorithm, withnon-lineartransitions.However,neitheroftheseassumptions
suchasbeingabletoexploitnaivelearners. appliestoM-FOS.
(2) Wepresentanexponentialsamplecomplexityboundforboth Instead,thisworkfocusesondiscretisingthecontinuousspace
casesdescribedinM-FOS(SeeTheoremsG.2andG.3).Specif- andexpressesthecomplexityboundsintermsofthediscretisation
ically,weprovethat,withhighprobability,thefinalR-FOS gridsize.Thisisrelatedtotheconceptofstateaggregation[2,19],
policyisclosetotheoptimalpolicyintheoriginalmeta-MDP which groups states into clusters and treats the clusters as the
uptoaconstantdistance. statesofanewMDP.Thesepreviousworksonlyformulatedthe
aggregationsettinginMDPsanddidnotprovidetheoreticalor
(3) WeimplementR-FOS‚Ä†andanalysetheempiricalsamplecom-
empiricalsamplecomplexityproofs.
plexityintheMatchingPenniesenvironment.Weestablishlinks
Furthermore,priorstudiesonPAC-MDPdidnotempiricallyverify
‚àóNaivelearnersareplayerswhoupdatetheirpolicyassumingotherlearningagents
theconnectionbetweenthesamplecomplexityandsizeofthestate-
aresimplyapartoftheenvironment.
‚Ä†Theprojectcodeisavailableonhttps://github.com/FLAIROx/rfos actionspace.Inthiswork,weempiricallyverifytherelationshipbetweenthesamplecomplexityandthecardinalityofthe 3.2 MarkovDecisionProcess
inner-state-action-spaceintheMatchingPenniesgame. AMarkovdecisionprocess(MDP)isaspecialcaseofstochastic
gameandcanbedescribedasM = ‚ü®ùëÜ,ùê¥,ùëá,ùëÖ,ùõæ‚ü©,whereùëÜ isthe
Algorithm1TheR-FOSAlgorithm s futa nt ce tis op na ,c ùëÖe, (ùê¥
ùë†
ùë°i ,s
ùëé
ùë°th )e isac thti eon res wpa ac rde, fùëá un(cid:0)ùë† cùë° t+ io1 n| ,ùë† ùë° a, nùëé dùë°(cid:1) ùõæis isth the etr da in scsi ot uio nn
t
Meta-gameInputs:Discretisedmeta-MDP‚ü®ùëÜÀÜ ùëë,ùê¥ÀÜ ùëë,ùëá ùëë,ùëÖ ùëë,ùõæ‚ü© factor.Ateachtimestepùë°,theagenttakesanactionùëé
ùë°
‚ààùê¥froma
ùëö-known,meta-gamehorizon‚Ñémeta stateùë†
ùë°
‚ààùëÜandmovestoanextstateùë† ùë°+1‚àºùëá (cid:0)¬∑|ùë† ùë°,ùëé ùë°(cid:1).Then,the
Inner-gameInputs:InnergameG=‚ü®ùëÜ,ùê¥,ùëá inner,ùëÖ inner‚ü© agentreceivesarewardùëü ùë° =ùëÖ(ùë† ùë°,ùëé ùë°).
inner-gamehorizon‚Ñé
inner
Initialisation:‚àÄùë†ÀÜ ùëë ‚ààùëÜÀÜ ùëë,ùëéÀÜ ùëë ‚ààùëÜÀÜ ùëë,ùë†ÀÜ ùëë‚Ä≤ ‚ààùëÜÀÜ ùëë 3.3 Model-FreeOpponentShaping
ùëÑÀÜ (ùë†ÀÜ ùëë,ùëéÀÜ ùëë)‚Üê0, ùëü(ùë†ÀÜ ùëë,ùëéÀÜ ùëë)‚Üê0, ùëõ(ùë†ÀÜ ùëë,ùëéÀÜ ùëë)‚Üê0, ùëõ(ùë†ÀÜ ùëë,ùëéÀÜ,ùë†ÀÜ‚Ä≤)‚Üê0 Model-freeOpponentShaping(M-FOS)[14]framestheOSproblem
1: formeta-episode=0,1,..do asameta-reinforcement-learningproblem,inwhichtheopponent
2: Resetenvironment shaperplaysameta-game.Themeta-gameisanMDP(sometimes
3: formeta-timestep=1,2,...,‚Ñémetado wealsorefertoitasmeta-MDP),inwhichthemeta-agentcontrols
4: ChooseùëéÀÜ ùëë :=argmax ùëéÀÜ ùëë‚ààùê¥ÀÜ ùëëùëÑÀÜ (ùë†ÀÜ ùëë,ùëéÀÜ ùëë‚Ä≤) oneoftheinneragentsintheinnergame.
5: Roll-outùêæ innergamesoflength‚Ñé innerusingùëéÀÜ ùëë =ùúô ùë° Theinnergameistheactualenvironmentthatouragentsareplay-
6: Inner-game opponents each update their own inner- ing,whichisanSG.TheoriginalM-FOSdescribestwocasesfor
policiesnaively
themeta-state:
7: LetùëÖbeouragent‚Äôsùêæ inner-games‚Äôdiscountedreturn
8: Letùë†ÀÜ‚Ä≤ bethenextmeta-stateafterexecutingmeta-action (1) Inthemeta-gameattimestepùë°,theM-FOSagentisatthemeta-
ùëë
ùëéÀÜ ùëë frommeta-stateùë†ÀÜ ùëë stateùë†ÀÜ ùë° = [ùúô ùë°ùëñ ‚àí1,ùùì ùíï‚àí ‚àíùíä 1],whichcontainsallinner-agents‚Äôpolicy
9: ifùëõ(ùë†ÀÜ ùëë,ùëéÀÜ ùëë) <ùëöthen parametersfortheunderlyingSG.Inthiswork,weassu,eall
10: ùëü(ùë†ÀÜ ùëë,ùëéÀÜ ùëë)‚Üêùëü(ùë†ÀÜ ùëë,ùëéÀÜ ùë°)+ùëÖ inner-agentsareparameterisedbytheirQ-valuetable.
1 11 2: : ùëõ ùëõ( (ùë† ùë†ÀÜ ÀÜùëë ùëë, ,ùëé ùëéÀÜ ÀÜùëë ùëë,) ùë†ÀÜ‚Üê ‚Ä≤ ùëë)ùëõ ‚Üê(ùë†ÀÜ ùëë ùëõ,ùëé (ÀÜ ùë†ÀÜùë° ùëë) ,+ ùëéÀÜ ùëë1 ,ùë†ÀÜ ùëë‚Ä≤)+1 (2) A inl nte er rn -ga ati mve ely re, pùë†ÀÜ ùë° res= enùùâ ttin hec pa ose lis ciw esh se ure ffip cia es nt tt lyra .jectoriesofthe
13: ifùëõ(ùë†ÀÜ ùëë,ùëéÀÜ ùëë)=ùëöthen
ùëôùëõ( 1 ) Weprovidetheoreticalsamplecomplexityresultsforbothofthese
14: forùëñ =1,2,3,¬∑¬∑¬∑,‚åà 1ùúÄ ‚àí(1 ùõæ‚àíùõæ) ‚åâdo
twocases
15: forall(ùë†ÀÜ,ùëéÀÜ)do
16: ifùëõ(ùë†ÀÜ ùëë,ùëéÀÜ ùëë) ‚â•ùëöthen Themeta-agenttakesameta-actionùëéÀÜ ùë° =ùúô ùë°ùëñ ‚àºùúã ùúÉ(¬∑|ùë†ÀÜ ùë°),whichisthe
17: ùëÑÀÜ (ùë†ÀÜ ùëë,ùëéÀÜ ùëë) ‚Üê ùëÖÀÜ ùëë(ùë†ÀÜ ùëë,ùëéÀÜ ùëë) + M-FOS‚Äôinneragent‚Äôspolicyparameters.Theactionischosenfrom
ùõæ(cid:205) ùë† ùëë‚Ä≤ ùëáÀÜ ùëë(ùë†ÀÜ‚Ä≤|ùë†ÀÜ ùëë,ùëéÀÜ ùëë)max ùëéÀÜ ùëë‚Ä≤ ùëÑÀÜ (ùë†ÀÜ ùëë‚Ä≤,ùëéÀÜ ùëë‚Ä≤) themeta-policyùúã parameterizedbyparameterùúÉ.Inthiswork,we
18: ùë†ÀÜ‚Üêùë†ÀÜ‚Ä≤ onlylookatthecasewherethemeta-policyisaQ-valuefunction
table,andisdenotedasùëÑÀÜinstead.TheM-FOSagentreceivesreward
ùëü ùë° = (cid:205) ùëòùêæ =0ùëü ùëòùëñ(ùúô ùë°ùëñ,ùùì ùíï‚àíùíä),whereùêæ isthenumberofinnerepisodes.
Anewmeta-stateissampledfromastochastictransitionfunction
3 Background
ùë†ÀÜ ùë°+1‚àºùëá(¬∑|ùë†ÀÜ ùë°,ùëéÀÜ ùë°).
3.1 StochasticGame
Notethattheoriginalpaperintroducestwodifferentalgorithms:
Astochasticgame(SG)‚Ä°isgivenbyatupleùê∫ = ‚ü®I,ùëÜ,ùë®,ùëá,ùëπ,ùõæ‚ü©. Thefirstmeta-trainsM-FOSagainstnaivelearnerscommonlyre-
I = {1,¬∑¬∑¬∑,ùëõ}isthesetofagents,ùëÜ isthestatespace,ùë®isthe sultinginexploitingthem.Thesecondinsteadconsidersmeta-self-
cross-productoftheactionspaceforeachagentsuchthatthejoint play,wherebytwoM-FOSagentsaretrainedtoshapeeachother,
actionspaceùë® =ùê¥1√ó¬∑¬∑¬∑√óùê¥ùëõ,ùëá :ùëÜ √óùë® ‚Ü¶‚ÜíùëÜ isthetransition resultinginreciprocity.Inthisworkweonlyconsiderthefirst,
function,ùëπisthecross-productofrewardfunctionsforallagents asymmetriccase.
suchthatthejointrewardspaceùëπ=ùëÖ1√ó¬∑¬∑¬∑√óùëÖùëõ,andùõæ ‚àà [0,1)
isthediscountfactor. 3.4 TheùëÖ Algorithm
MAX
ùëÖ [3]isanMBRLalgorithmproposedforanalysingthesample
In an SG, agents simultaneously choose an action according to MAX
theirstochasticpolicyateachtimestepùë°,ùëéùëñ ‚àºùúãùëñ (¬∑|ùë†ùëñ).Thejoint complexityfortabularMDPs.GivenanyMDPùëÄ,ùëÖ MAXconstructs
ùë° ùúôùëñ ùë° anempiricalMDPùëÄÀÜ thatapproximatesùëÄ.Theapproximationis
actionattimestepùë° isùíÇùíï = {ùëéùëñ ùë°,ùíÇ ùíï‚àíùíä},wherethesuperscript‚àíùíä donebyestimatingtherewardfunctionùëÖandtransitionùëá using
indicatesallagentsexceptagentùëñandùúôùëñ isthepolicyparameter
empiricalsamples.Theresultingapproximaterewardandtransition
of agentùëñ. The agents then receive rewardùëü ùë°ùëñ = ùëÖùëñ(ùë† ùë°,ùíÇùíï) and modelsaredenotedbyùëÖÀÜandùëáÀÜrespectively.
observethenextstateùë† ùë°+1 ‚àºùëá(¬∑|ùë† ùë°,ùíÇùíï),resultinginatrajectory
ùúèùëñ =(ùë†0,ùíÇ0,ùëü 0ùëñ,...,ùë† ùëá,ùíÇ ùëª,ùëü ùëáùëñ),whereùëá istheepisodelength. ùëÖ MAXencouragesexplorationbydividingthestate-actionpairsinto
twogroups-thosethathavebeenvisitedatleastùëö-times,andthose
thathaven‚Äôt.Thesetofstate-actionpairsthathavebeenvisitedat
‚Ä°Weusetheboldnotationtoindicatevectorsoverùëõagents. leastùëö-timesiscalledthe‚Äúùëö-knownset‚Äù.UsingtheempiricalMDPùëÄÀÜ,theùëÖ MAXalgorithmconstructsanùëö-knownempiricalMDP.This meta-MDPùëÄ
ùëë
=‚ü®ùëÜÀÜ ùëë,ùê¥ÀÜ ùëë,ùëá ùëë,ùëÖ ùëë,ùõæ‚ü©.Wefirstdiscretisethecon-
ùëö-knownempiricalMDPbehavesalmostexactlyastheempirical tinuousmeta-statespaceandmeta-actionspaceusingepsilon-
MDP,exceptwhentheagentisatastate-actionpairoutsidethe nets[5,7].Basedonthisdiscretisedmeta-stateandmeta-action
ùëö-known set. When the agent is outside theùëö-known set, the space,wedefinethediscretisedtransitionandrewardfunction.
transitionfunctionisself-absorbing(i.e.thetransitionfunction SeeSection4.2fordetails.
onlytransitionsbacktothecurrentstate)andtherewardfunction
(2) Wethenconstructaùëö-knowndiscretisedMDPùëÄ ,ùëë,asde-
isthemaximum(SeeTable3intheappendix).Theconsequence ùëö
scribedbytheR-MAXalgorithm[20].SeeSection4.3fordetails.
oftheùëö-knownsetupistheagentisencouragedtoexplorestate-
actionpairsthathavehighuncertainty(i.e.thathasbeenvisited (3) Then, we deploy the ùëÖ algorithm in ùëÄ ,ùëë. ùëÖ both
MAX ùëö MAX
underùëötimes).Specifically,thevaluefunctionfortheunder-visited estimatestheempiricalùëö-knowndiscretisedMDP,ùëÄÀÜ ,ùëë,using
ùëö
statesisthemaximumpossibleexpectedreturn,whichgivesthe
amaximumlikelihoodestimatefromempiricalsamplesand
algorithm its name. This is in line with optimism in the face of learnsanoptimalpolicyinùëÄÀÜ ,ùëë.Forexample,toestimatethe
ùëö
uncertainty.
meta-reward,ouralgorithm,R-FOSevaluatestheinner-game
policyoutputtedbythemeta-policyusingepisodicrollouts.The
3.5 ùúÄ-Nets estimatesarethenusedtoupdatethemeta-policyaccording
Definition3.1. (ùúÄ-Net[5,7])ForùúÄ >0,NùúÄ isanùúÄ-netovertheset totheR-FOSalgorithm.OurR-FOSalgorithmoptimistically
Œò‚äÜRùê∑ifforallùúÉ ‚ààŒò,thereexistsùúÉ‚Ä≤ ‚ààNùúÄsuchthat(cid:13) (cid:13)ùúÉ‚àíùúÉ‚Ä≤(cid:13)
(cid:13)2
‚â§ùúÄ. assignsrewardsforallunder-visiteddiscretised(meta-state,
meta-action)pairstoencourageexploration(likeùëÖ ).See
MAX
Section4.4fordetails.
To discretise aùê∑-dimensional sphere of radiusùëÖ, we can use a
ùúÄ-netcontainingùê∑-dimensionalcubesofsidesùúÜ.Thisresultsin (4) WenextproveaPAC-boundwhichguaranteeswithlargeprob-
(cid:16)2ùëÖ +1(cid:17)ùê∑ points. Within each ùê∑-dimensional cube, the largest ability,thattheoptimalpolicylearntinùëÄÀÜ ùëö,ùëëissimilartothe
ùúÜ
optimalpolicyinùëÄ,ùëë.Thisstepusesresultsfrom[20].See
distancebetweentheverticesandtheinteriorpointscomesfrom
‚àö Section4.5fordetails.
thecenterofthecube,whichis ùúÜ ùëë.Therefore,toguaranteeafull
2
coverofallthepointsinthesphere,thelargestcubesizethatwe (5) Wealsoproveastrictboundthatguaranteestheoptimalpolicies
‚àö
canhaveshouldsatisfyùúÄ = ùúÜ ùëë.Fromhereon,wewillreplacethe learntinùëÄ,ùëëandùëÄaresimilaruptoaconstant.Thisstepuses
2
ùúÄinùúÄ-netwithùõº toavoidnotationoverloading. resultsfrom[4].SeeSection4.8fordetails.
(6) Usingthetwoboundsfromabove,weprovethefinalsample
4 SampleComplexityAnalysiswithùëÖ MAX as complexityguaranteewhichquantifiesthat,withlargeproba-
Meta-Agent bility,theoptimalpolicylearntinùëÄÀÜ ùëö,ùëëissimilartotheoptimal
As introduced in Section 3, ùëÖ [3] is a MBRL algorithm for policyinùëÄuptoaconstant.SeeSection4.9fordetails.
MAX
learningintabularMDPs.WeadapttheoriginalM-FOSalgorithm
touseùëÖ asthemeta-agent(seeAlgorithm1)andrefertothis 4.1 Assumptions
MAX
adapted algorithm as R-FOS from here on. We use a tabular Q- Wefirstoutlineallassumptionsmadeinderivingthesamplecom-
learnerasthenaivelearnerforallinner-gameopponents.While plexityoftheR-FOSalgorithm.
theoriginalM-FOSpaperusesPPO[18],wechoosetheQ-learner
fortheeaseofsamplecomplexityanalysis.
Assumption4.1. Bothmeta-gameandinner-gamearefinitehori-
WeprovidetheoreticalresultsforthetwocasesofM-FOS‚Äômeta-
zon.Weuse‚Ñémetatodenotethemeta-gamehorizon,and‚Ñé innerto
denotetheinner-gamehorizon.
agentproposedbytheoriginalpaper[14].CaseI usesallagents‚Äô
innerpolicyparametersfromtheprevioustimestepasthemeta-
state.CaseII insteadusesthemostrecentinner-gametrajectories Assumption4.2. Weassumetheinner-gamerewardisbounded.
asthemeta-state.Inbothcases,themeta-actiondeterminesthe Forsimplicityoftheproofandwithoutlossofgenerality,weset
inneragent‚Äôspolicyparametersforthenextinnerepisode. thisboundas ‚Ñéin1 ner,where‚Ñé inneristhehorizonoftheinnergame.
Atahighlevel,wefirstdiscretisethemeta-MDP,whichallowsus
Formally,forall(ùë†,ùëé),0 ‚â§ ùëÖ inner(ùë†,ùëé) ‚â§ ‚Ñéin1 ner.Thisallowsusto
tousethetheoreticalboundsfromùëÖ (onlysuitablefortabular
introducethenotionofmaximuminnerrewardandmaximuminner
MDPs),thenwedeveloptheoryforM boA uX ndingthediscrepancybe- valuefunctionasùëÖ max,inner= ‚Ñéin1
ner
andùëâ max,inner=1respectively.
Thisimpliesthattherewardandvaluefunctioninthemeta-game
tweenthecontinuousanddiscretemeta-MDP,andlastly,weuse
allofthistoboundthefinaldiscrepancy.Specifically,thesample arealsobounded,i.e.,ùëÖmax =1andùëâmax = 1‚àí1 ùõæ (thelatterbeing
complexityanalysisconsistsofsixsteps¬ß: anupperbound).
(1) TouseùëÖ astheM-FOSmeta-agent,wefirstdiscretisethe
continuoM uA sX meta-MDP ùëÄ = ‚ü®ùëÜÀÜ,ùê¥ÀÜ,ùëá,ùëÖ,ùõæ‚ü© into a discretised A simss pu lm icip tyti oo fn th4 e.3 p. rT ooh fe ,tm he et ia n-g na em r-ge au mse es ua sed sis aco du isn ct of ua nc tto fr aco tf oùõæ r. oF fo 1r .
ThisassumptioncanbeeasilydeletedbyadaptingùëÖ (see
max,inner
¬ßseedetailedproofintheappendix above)intheoriginalproofin[20].Assumption4.4. Forsimplicity,theinnergameisassumedtobe 4.2.2 DiscretisingtheStateandActionSpace:CaseII
discrete.
InCaseII,themeta-stateùë†ÀÜ isallinneragents‚Äôpasttrajectories.
ùë°
Assumption4.5. Themeta-rewardfunctionisLipschitz-continuous: Formally,ùë†ÀÜ ùë° := ùùâùë°., whereùúè ùë°ùëñ = {ùë†0,ùëé0,ùë†1,ùëé1,...,ùë† ùë°,ùëé ùë°}. Because
Forallùë†ÀÜ 1,ùë†ÀÜ
2
‚ààùëÜÀÜandùëéÀÜ 1,ùëéÀÜ
2
‚ààùê¥ÀÜ, we assume the Inner-Game is discrete (i.e. the state and action
spacearebothdiscrete),themeta-stateinthiscasedoesnotneed
(cid:12) (cid:12)ùëÖ(ùë†ÀÜ 1,ùëéÀÜ 1)‚àíùëÖ(ùë†ÀÜ 2,ùëéÀÜ 2)(cid:12) (cid:12)‚â§LùëÖ(cid:13) (cid:13)(ùë†ÀÜ 1,ùëéÀÜ 1)‚àí(ùë†ÀÜ 2,ùëéÀÜ 2)(cid:13) (cid:13)
‚àû
discretisation.Let‚Ñébethemaximumlengthofthepasttrajectories
whereLùëÖ isthemeta-rewardfunction‚ÄôsLipschitz-constant. combined,i.e.‚Ñé=‚Ñé inner¬∑‚Ñémeta.Thesizeofthemeta-statespaceis
Assumption4.6. Themeta-transitionfunctionisLipschitz-continuous:
|ùëÜÀÜ ùë°|=(|ùëÜ||ùê¥|)ùëõ‚Ñé. (4)
Forallùë†ÀÜ 1,ùë†ÀÜ 1‚Ä≤,ùë†ÀÜ 2 ‚ààùëÜÀÜandùëéÀÜ 1,ùëéÀÜ 1‚Ä≤,ùëéÀÜ 2 ‚ààùê¥ÀÜ, Themeta-actionremainsthesameasCaseI.
(cid:12) (cid:12)ùëá(ùë†ÀÜ 1‚Ä≤ |ùë†ÀÜ 1,ùëéÀÜ 1)‚àíùëá(ùë†ÀÜ 2‚Ä≤ |ùë†ÀÜ 2,ùëéÀÜ 2)(cid:12) (cid:12)‚â§Lùëá(cid:13) (cid:13)(ùë†ÀÜ 1‚Ä≤,ùë†ÀÜ 1,ùëéÀÜ 1)‚àí(ùë†ÀÜ 2‚Ä≤,ùë†ÀÜ 2,ùëéÀÜ 2)(cid:13)
(cid:13)
‚àû
whereLùëá isthemeta-transitionfunction‚ÄôsLipschitz-constant. 4.2.3 DiscretisingtheTransitionandRewardFunction
Undertheabovediscretisationprocedure,wedefinethediscretised
A pis ns gu bm etp wti eo en n4 m.7 e. taT -sh taer tee‚Äô ss pa aL ceip as nch dit mz- ec to an -at cin tiu oo nu ss pp ao ci en st u-t co h-s te ht am tfa op r- MDPùëÄ
ùëë
=(ùëÜÀÜ,ùê¥ÀÜ ùëë,ùëá ùëë,ùëÖ ùëë,ùõæ),wherethestatespaceremainscontin-
anyùë†ÀÜ,ùë†ÀÜ‚Ä≤ ‚àà ùëÜÀÜandùëéÀÜ,ùëéÀÜ‚Ä≤ ‚àà ùê¥ÀÜ,thereexistssomeùëéÀÜ ‚ààùëà(ùë†ÀÜ) suchthat
uousandtheactionspaceisrestrictedtodiscretised actions.We
definethetransitionfunctionandrewardfunctionforùëÄ as:
(cid:13) (cid:13)ùëéÀÜ‚àíùëéÀÜ‚Ä≤(cid:13)
(cid:13)
<L(cid:13) (cid:13)ùë†ÀÜ‚àíùë†ÀÜ‚Ä≤(cid:13)
(cid:13)
ùëë
‚àû ‚àû
A iss asu pm rop bt ai bo in lit4 y.8 d. enT sh ite ym fue nta c- tg ioa nme sut cra hn ts hit ai ton 0fu ‚â§nc ùëáti (o ùë†ÀÜn
‚Ä≤
ùëá |( ùë†¬∑ ÀÜ,| ùëéÀÜùë† )ÀÜ,ùëéÀÜ ‚â§) ùëá ùëë(ùë†ÀÜ‚Ä≤ |ùë†ÀÜ,ùëéÀÜ ùëë)= ‚à´
ùëÜÀÜùëá ùëá( (ùë† ùëßÀÜ ÀÜùëë‚Ä≤
ùëë
| |ùë† ùë†ÀÜ ÀÜùëë ùëë, ,ùëé ùëéÀÜ ÀÜùëë ùëë)
)ùëëùëßÀÜ (5)
Land ‚à´ ùëÜÀÜùëá(ùë†ÀÜ‚Ä≤ |ùë†ÀÜ,ùëéÀÜ)ùëëùë†ÀÜ‚Ä≤ =1, ‚àÄùë†ÀÜ,ùë†ÀÜ‚Ä≤ ‚ààùëÜÀÜandùëéÀÜ‚ààùê¥ÀÜ Intuitively,ùëá ùëë(ùë†ÀÜ‚Ä≤ |ùë†ÀÜ ùëë,ùëéÀÜ ùëë)isanormalizedsampleofùëá ùëë(¬∑|¬∑,ùëéÀÜ ùëë)at
ùë†ÀÜ‚Ä≤,ùë†ÀÜ ùëë,andthetransitionprobabilityùëá ùëë(¬∑ |ùë†ÀÜ,ùëéÀÜ ùëë)takesaconstant
ThefirstfourassumptionsarerequiredtobeabletousetheR-MAX valuewithineachgridinthestatespace.Thismeansthatinstead
algorithm,whilethelatterassumptionsareneededforboundingthe oftreatingthetransitionfunctionasadiscretiseddistributionofall
discrepancybetweenthecontinuousmeta-MDPandthediscretised possiblevaluesofùëÜÀÜ ,wetreatitasacontinuousdistributionover
ùëë
meta-MDP. theoriginalcontinuousstatespace,butnormalizeeachgridfrom
theùúÄ-netintoastepfunction.
4.2 Step1:DiscretisingtheMeta-MDP
TouseùëÖ astheM-FOSmeta-agent,wediscretisethecontinuous
ùëÖ ùëë(ùë†ÀÜ,ùëéÀÜ ùëë)=ùëÖ(ùë†ÀÜ ùëë,ùëéÀÜ ùëë) (6)
MAX
meta-MDPùëÄ = ‚ü®ùëÜÀÜ,ùê¥ÀÜ,ùëá,ùëÖ,ùõæ‚ü©intoadiscretisedmeta-MDPùëÄ ùëë = Similarly,therewardfunctioniscontinuousoverthestatespace,
‚ü®ùëÜÀÜ ùëë,ùê¥ÀÜ ùëë,ùëá ùëë,ùëÖ ùëë,ùõæ‚ü©.Wediscretisethecontinuousstateandaction butnormalizedeachgridfromtheùúÄ-netintoastepfunction.
spaceusingùúÄ-netswithspacingùõº.
4.3 Step2:Theùëö-knownDiscretisedMDP
4.2.1 DiscretisingtheStateandActionSpace:CaseI Inthepreviousstep,weconvertedthemeta-MDPùëÄintoadiscre-
In Case I, the meta-stateùë†ÀÜ ùë° is all inner agents‚Äô policies parame-
tisedmeta-MDPùëÄ ùëë.FromùëÄ ùëë,R-FOSbuildsanùëö-knowndiscre-
tisedMDPùëÄ (seeTable3intheappendix).
tersfromtheprevioustimestep.Eachoftheinneragentùëñ‚Äôspolicy ùëö,ùëë
is a Q-table, denoted asùúô ùëñ ‚àà R|ùëÜ|√ó|ùê¥|. Formally,ùë†ÀÜ ùë° := ùùìùë°‚àí1 =
[ùúô ùë°ùëñ ‚àí1,ùùì ùíï‚àí ‚àíùíä 1].Themeta-actionùëéÀÜ ùë° istheinneragent‚Äôscurrentpolicy Definition4.9(m-KnownMDP). LetùëÄ ùëë = ‚ü®ùëÜÀÜ ùëë,ùê¥ÀÜ ùëë,ùëá ùëë,ùëÖ ùëë,ùõæ‚ü©be
parametersùúôùëñ. anMDP.WedefineùëÄ ùëö,ùëëtobetheùëö-knownMDP.Asisstandard
ùë°
practice,ùëö-knownreferstothesetofstate-actionpairsthathave
Forthemeta-actionspaceùê¥ÀÜandachosendiscretisationerrorùõº >0, beenvisitedatleastùëötimes.Forallstate-actionpairsinùëö-known,
w
ùëéÀÜ
ùëëe ‚ààob ùê¥ÀÜt ùëëai wn hth ere eùúÄ-netùê¥ÀÜ ùëë ‚äÇ ùê¥ÀÜsuchthatforallùëéÀÜ ‚àà ùê¥ÀÜ,thereexist t ph ae iri sn od uu tc se idd eM ofD ùëöP -ùëÄ knùëö o, wùëë nb ,e th ha ev se ts atid e-e an ct ti ic oa nl pto aiùëÄ rsùëë a. rF eo sr els ft -a at be s- oa rc bti io nn
g
(cid:13) (cid:13)ùëéÀÜ‚àíùëéÀÜ ùëë(cid:13) (cid:13)‚â§ùõº. (1) (i.e.onlyself-transitions)andmaximallyrewardingwithùëÖùëÄùê¥ùëã.
DividingthespacewithgridsizeùúÜresultsinthesizeofdiscretised
meta-actionspaceupperboundedby 4.4 Step3:TheEmpiricalDiscretisedMDP
(cid:32) 2‚àöÔ∏Å |ùëÜ||ùê¥| (cid:33)|ùëÜ||ùê¥| Fromtheùëö-knowndiscretisedMDPùëÄ ùëö,ùëë,wethenlearnanempir-
|ùê¥ÀÜ ùëë| ‚â§
ùúÜ
+1 . (2) icalùëö-knowndiscretisedMDPùëÄ ùëö,ùëë bycalculatingthemaximum
likelihoodfromempiricalsamples(seeTable3intheappendix).As
showninAlgorithm1,R-FOSlearnsanoptimalpolicywithinthis
Similarly, the size of the discretised meta-state space is upper
empiricalùëö-knowndiscretisedMDP.
boundedby
|ùëÜÀÜ ùëë| ‚â§
(cid:32) 2‚àöÔ∏Åùëõ ùúÜ|ùëÜ||ùê¥| +1(cid:33)ùëõ|ùëÜ||ùê¥|
. (3) D the efi en xpit ei co tn ed4 v.1 e0 rs( iE om np oi fr ùëÄic ÀÜalm w-K hn eo rew :ndiscretisedMDP). ùëÄ ùëö,ùëë is
ùëö,ùëë1‚àíùõø,ùëâ ùëÄ‚àó
ùëë
(ùë†ÀÜ ùë°)‚àíùëâ ùëÄA ùëëùë° (ùë†ÀÜ ùë°) ‚â§ùúÄistrueforallbut
ùëá ùëö,ùëë(ùë†ÀÜ ùëë‚Ä≤ |ùë†ÀÜ ùëë,ùëéÀÜ ùëë):=(cid:40) ùëá 1ùëë [ùë†( ÀÜùë† ùëë‚Ä≤ÀÜ ùëë‚Ä≤ =| ùë†ùë† ÀÜÀÜ ùëëùëë, ]ùëé ,ÀÜ ùëë) i of th(ùë†ÀÜ eùëë r, wùëéÀÜ ùëë is) e‚ààm-known
ùëÇÀú(cid:169)
(cid:173)
(cid:173)(cid:18) 2‚àö ùëõ ùúÜ|ùëÜ||ùê¥| +1(cid:19)2ùëõ|ùëÜ||ùê¥| (cid:18) 2‚àö |ùëÜ ùúÜ||ùê¥| +1(cid:19)|ùëÜ||ùê¥|
(cid:170)
(cid:174)
(cid:174)
ùëáÀÜ ùëö,ùëë(ùë†ÀÜ ùëë‚Ä≤ |ùë†ÀÜ ùëë,ùëéÀÜ ùëë):=Ô£±Ô£¥Ô£¥Ô£≤ Ô£¥Ô£¥1ùëõ ùëõ( [ùë† ùë†ÀÜ ÀÜ(ùëë ùëëùë† ‚Ä≤ÀÜ, ùëëùëéÀÜ =,ùëë ùëéÀÜ, ùëëùë† ùë†ÀÜ ÀÜùëë )‚Ä≤ ùëë) ],
,
i of th(ùë†ÀÜ eùëë r, wùëéÀÜ ùëë is) e‚ààm-known (cid:173) (cid:173) (cid:173)
(cid:171)
ùúÄ3(1‚àíùõæ)6 (cid:174) (cid:174) (cid:174)
(cid:172)
Ô£≥ timesteps.
(cid:40)
ùëÖ ùëö,ùëë(ùë†ÀÜ ùëë,ùëéÀÜ ùëë):= ùëÖ ùëÖùëë m( aùë†ÀÜ xùëë,ùëéÀÜ ùëë), i of th(ùë†ÀÜ eùëë r, wùëéÀÜ ùëë is) e‚ààm-known
4.7 CaseII
ùëÖÀÜ ùëö,ùëë(ùë†ÀÜ ùëë,ùëéÀÜ ùëë)=Ô£±Ô£¥Ô£¥Ô£≤(cid:205)ùëõ ùëñ(ùë†ÀÜùëë ùëõ,ùëé (ÀÜ ùë†ùëë ÀÜ ùëë) ,ùëéùëü ÀÜ ùëë(ùë†ÀÜ )ùëë,ùëéÀÜ ùëë), if(ùë†ÀÜ ùëë,ùëéÀÜ ùëë) ‚ààm-known D obir tae ic ntl ty hepl fu og llg inin gg PAin CE -bq ou ua nti do .ns 11 and 9 into Theorem G.1, we
Ô£¥Ô£¥ Ô£≥ùëÖmax, otherwise
(7) Theorem4.13. Supposethat0 ‚â§ ùúÄ < 1‚àí1 ùõæ and0 ‚â§ ùõø < 1are
two real numbers. Let M be any continuous meta-MDP with in-
4.5 Step4:TheBoundBetweenùëÄ ùëë andùëÄÀÜ ùëö,ùëë n ‚ü®ùëÜe ÀÜ ùëër ,s ùê¥t ÀÜo ùëëc ,h ùëá ùëëas ,t ùëÖi ùëëc ,g ùõæa ‚ü©m ae sùê∫ disc= re‚ü® tùëÜ is, eùê¥ d,ùëá vi en rn se ir o, nùëÖ i on fne ùëÄr‚ü©. (aL set deu ss crd ie bn eo dte inùëÄ Cùëë as= e
W pre obfi ar bs it lip tyr ,o tv he et oh pe tP imA aC lpb oo lu icn id esw leh ai rc nh tg inua tr ha en dte ise cs reth tia st e, dw Mit Dh Phi ùëÄgh I) using grid size of ùúÜ. There exists inputsùëö = ùëö(cid:16) ùúÄ1, ùõø1(cid:17) andùúÄ1,
ùëë
andempiricalùëö‚àíùëòùëõùëúùë§ùëõdiscretisedMDPareveryclose.Weprove satisfying
theboundusingresultsfrom[20].
ùëö(cid:18)1 ,1(cid:19)
=ùëÇÀú
(cid:32) (|ùëÜ||ùê¥|)ùëõ‚Ñé(cid:33)
ùúÄ ùõø ùúÄ2(1‚àíùõæ)4
T a Mnh Dde Po 0 .r T‚â§e hm eùõø re4 <. e1 x11 is. a tsr(ùëÖ e inM t pwA uoX tsrM ùëöeaD l =P n ùëöB uo m (cid:16)u ùúÄ1bn ,ed r ùõø1[ s2 (cid:17)a0 an] n) d dS ùëÄ ùúÄu 1p ,=p so a‚ü®s tùëÜe is,t fùê¥h ya , inùëát g0 ,ùëÖ ùëö‚â§ ,ùõæ (cid:16)ùúÄ ‚ü© ùúÄ1< i ,s ùõø1a1 (cid:17)‚àí n1 ùõæ =y ùëöa atn ad tin mùúÄ d1 1 eùúÄ= ùë°1, aùëÇ t nh d(cid:16) enùúÄ1
ùë†ÀÜ
ùë°(cid:17) t, h ds eeu nfc ooh tl elt oh tw ha i et n si gf taùëÖ h to e-ùëÄ l ad tsùê¥ . tùëã L imei t es A ùë°e .x ùë° Wec du ie tt n he od pte ro on ùëÖ b- aùëÄ ùëÄ biùê¥w liùëã ti yt ‚Äôh s ai p tn o lp elu i act sys
t
ùëÇ(cid:18) (ùëÜ+ln ùúÄ2(ùëÜ (1ùê¥ ‚àí/ ùõæùõø )) 2)ùëâ m2 ax(cid:19) and ùúÄ1
1
=ùëÇ(cid:16) ùúÄ1(cid:17) ,suchthatifùëÖ MAXisexecuted 1‚àíùõø,ùëâ ùëÄ‚àó ùëë (ùë†ÀÜ ùë°)‚àíùëâ ùëÄA ùëëùë° (ùë†ÀÜ ùë°) ‚â§ùúÄ (cid:18)is ‚àötrueforall (cid:19)b |ùëÜu |t
|ùê¥|
onùëÄwithinputsùëöandùúÄ1,thefollowingholds.Letùê¥ ùë° denoteùëÖ MAX‚Äôs (cid:169)(|ùëÜ||ùê¥|)ùëõ‚Ñé 2 |ùëÜ||ùê¥| +1 (cid:170)
policyattimeùë° andùë† ùë° denotethestateattimeùë°.Withprobabilityat ùëÇÀú(cid:173)
(cid:173)
ùúÜ (cid:174)
(cid:174)
least1‚àíùõø,ùëâ ùëÄùê¥ùë° (ùë† ùë°) ‚â•ùëâ ùëÄ‚àó (ùë† ùë°)‚àíùúÄistrueforallbut (cid:173)
(cid:173)
ùúÄ3(1‚àíùõæ)6 (cid:174)
(cid:174)
(cid:173) (cid:174)
ùëÇÀú (cid:18) ùëÜ2ùê¥/(cid:16) ùúÄ3 (1‚àíùõæ)6(cid:17)(cid:19) (cid:171) (cid:172)
timesteps.
timesteps(finalsamplecomplexitybound). 4.8 Step5:TheBoundbetweenùëÄ andùëÄ ùëë
Next,wegiveaguaranteethattheoptimalpolicieslearntinthe
4.6 CaseI original meta-MDP ùëÄ and the discretised MDP ùëÄ are similar
ùëë
Directly plugging in Equations 10 and 9 into Theorem G.1, we enoughwithadistanceuptoaconstantfactor.Usingtheresults
obtainthefollowingPAC-bound. from[4],weobtainthefollowingproperty.
Theorem4.12. Supposethat0 ‚â§ ùúÄ < 1‚àí1 ùõæ and0 ‚â§ ùõø < 1are T sth ane to Krem (th4 a. t1 s4 d. ep(M enD dP soD nis lycr oet niz ta ht eio Ln ipB so cu hn itd z[ c7 o] n) sT tah ner te Lex )i ss uts cha tc ho an t-
two real numbers. Let M be any continuous meta-MDP with in-
nerstochasticgameùê∫ = ‚ü®ùëÜ,ùê¥,ùëá inner,ùëÖ inner‚ü©.LetusdenoteùëÄ ùëë = forsomediscretisationcoarsenessùúÜ‚àà (0, L 2]
I‚ü® )ùëÜÀÜ ùëë u, sùê¥ iÀÜ nùëë g,ùëá gùëë r, iùëÖ dùëë s, iùõæ z‚ü© ea os fd ùúÜi .sc Tr he eti rs eed exv ie str ssio inn po uf tsùëÄ ùëö(a =sd ùëöesc (cid:16)r ùúÄ1i ,b ùõøe 1d (cid:17)i an nC da ùúÄs 1e
,
(cid:13) (cid:13) (cid:13)ùëâ ùëÄ‚àó ‚àíùëâ ùëÄ‚àó ùëë(cid:13) (cid:13)
(cid:13)
‚àû
‚â§ (1K ‚àíùúÜ ùõæÀÜ)2.
satisfying
4.9 Step6:Addingittogether
(cid:169)(cid:18) 2‚àö ùëõ|ùëÜ||ùê¥| +1(cid:19)ùëõ|ùëÜ||ùê¥|
(cid:170)
To combine the bounds we obtained in Step 4 and 5, we need
ùëö(cid:18)1 ,1(cid:19) =ùëÇÀú(cid:173)
(cid:173)
ùúÜ (cid:174)
(cid:174)
an additional bound that bounds the policy value between the
ùúÄ ùõø (cid:173) ùúÄ2(1‚àíùõæ)4 (cid:174) continuousanddiscretisedMDP.
(cid:173) (cid:174)
(cid:173) (cid:174)
(cid:171) (cid:172)
Lemma4.15(SimulationLemmaforContinuousMDPs). LetùëÄ
ùëöan ad nùúÄ d1 1 ùúÄ= 1,ùëÇ th(cid:16) enùúÄ1(cid:17) t, hs eu fc oh llt oh wa it ni gfùëÖ ho-ùëÄ ldsùê¥ .ùëã Lei ts Aex ùë°ec du et ne od teon ùëÖ-ùëÄ ùëÄùê¥w ùëãit ‚Äôh si pn op lu ict ys a Ln etd ùúñùëÄ ùëÖÀÜ ‚â•be mtw axo ùë†,M ùëéD |ùëÖÀÜP (s ùë†,t ùëéh )a ‚àíto ùëÖn (l ùë†y ,ùëéd )i |ff ae nr din
ùúÄ
ùëù(ùëá ‚â•,ùëÖ m) aa xn ùë†,d ùëé( ‚à•ùëá ùëáÀÜ ÀÜ, (ùëÖÀÜ ¬∑) |.
ùë†,ùëé)‚àíùëá(¬∑|
attimeùë° andùë†ÀÜ
ùë°
denotethestateattimeùë°.Withprobabilityatleast ùë†,ùëé)||1.Then‚àÄùúã :SÀÜ ‚Üíùëé,istrueforallbut
(cid:13) (cid:13)ùëâùúã ‚àíùëâùúã(cid:13) (cid:13) ‚â§ ùúÄ ùëÖ +ùõæùúñ ùëÉùëâmax . (cid:18) ‚àö (cid:19)2ùëõ|ùëÜ||ùê¥| (cid:18) ‚àö (cid:19)|ùëÜ||ùê¥|
(cid:13) ùëÄ ùëÄÀÜ(cid:13) ‚àû 1‚àíùõæ 2(1‚àíùõæ) (cid:169) 2 ùëõ|ùëÜ||ùê¥| +1 2 |ùëÜ||ùê¥| +1 (cid:170)
ùëÇÀú(cid:173)
(cid:173)
ùúÜ ùúÜ (cid:174)
(cid:174)
Underdiscretisation,[4]showedthat,withasmallenoughgrid (cid:173) (cid:173) ùúÄ3(1‚àíùõæ)6 (cid:174) (cid:174)
size,andrestrictingtothediscretisedactionspace,thedifference (cid:173) (cid:174)
intransitionprobabilityofthecontinuousMDPùëÄanddiscretised (cid:171) (cid:172)
timesteps.I.e.theaboveisthefinalsamplecomplexity.
MDPùëÄ isupperboundedbyaconstant.
ùëë
4.11 CaseII
Lemma4.16. [4]Thereexistsaconstantùêæ ùëÉ (dependingonlyon
Similarly,summinguptheboundsinLemmaI.3,TheoremsG.3and
constantL)suchthat
H.1,weobtainthefinalboundforCaseII.InSection5,wealso
|ùëá ùëë(ùë†ÀÜ‚Ä≤|ùë†ÀÜ,ùëéÀÜ ùëë)‚àíùëá(ùë†ÀÜ‚Ä≤|ùë†ÀÜ,ùëéÀÜ ùëë)| ‚â§ùêæ ùëùùõº showempiricallythatthenumberofsamplesneededindeedscales
forallùë†ÀÜ‚Ä≤,ùë†ÀÜ‚ààùëÜÀÜ,ùëéÀÜ ùëë ‚ààùê¥ÀÜ ùëë andallùõº ‚â§ (0, 21 L]
byafactorof|ùëÜ||ùê¥|2ùëõ‚Ñé,asseeninTheorem4.19.
WenowapplytheLemmaI.1toboundthedifferenceinvaluefor Theorem4.19. Supposethat0 ‚â§ ùúÄ < 1‚àí1 ùõæ and0 ‚â§ ùõø < 1are
anydiscretisedpolicy(i.e.restrictingactionspacetoùê¥ÀÜ )inthe two real numbers. Let M be any continuous meta-MDP with in-
continuousMDPùëÄanddiscretisedMDPùëÄ ùëë. ùëë n ‚ü®ùëÜe ÀÜ ùëër ,s ùê¥t ÀÜo ùëëc ,h ùëá ùëëas ,t ùëÖi ùëëc ,g ùõæa ‚ü©m ae sùê∫ disc= re‚ü® tùëÜ is, eùê¥ d,ùëá vi en rn se ir o, nùëÖ i on fne ùëÄr‚ü©. (aL set deu ss crd ie bn eo dte inùëÄ Cùëë as= e
Lemma4.17. LetùëÄ ùê¥ÀÜ
ùëë
=(ùëÜÀÜ,ùê¥ÀÜ ùëë,ùëá,ùëÖ,ùõæ)bethecontinuousMDPùëÄ I s) atu iss fi yn ig nggrid size of ùúÜ. There exists inputsùëö = ùëö(cid:16) ùúÄ1, ùõø1(cid:17) andùúÄ1,
ùëÄre ùëëstr =ict (e ùëÜÀÜd ,ùê¥t ÀÜo ùëë,t ùëáh ùëëe ,d ùëÖi ùëësc ,ùõære )t .is Te hd enac ft oio rn ansp ya dc ie s. cR ree tc isa el dlt ph oe lid ci ysc ùúãre :ti ùëÜs ÀÜe ‚ÜídM ùê¥ÀÜD ùëëP
,
ùëö(cid:18)1 ,1(cid:19)
=ùëÇÀú
(cid:32) (|ùëÜ||ùê¥|)ùëõ‚Ñé(cid:33)
ùúÄ ùõø ùúÄ2(1‚àíùõæ)4
‚à•ùëâ ùëÄùúã ùê¥ÀÜ
ùëë
‚àíùëâ ùëÄùúã ùëë‚à•‚àû=‚à•ùëâ ùëÄùúã ‚àíùëâ ùëÄùúã ùëë‚à•‚àû ‚â§ L 1‚àíRùõº ùõæ + (1ùõæùêæ ‚àíùëù ùõæùõº )2 and ùúÄ1
1
=ùëÇ(cid:16) ùúÄ1(cid:17) ,suchthatifùëÖ-ùëÄùê¥ùëã isexecutedonùëÄ withinputs
ùëöandùúÄ1,thenthefollowingholds.LetAùë° denoteùëÖ-ùëÄùê¥ùëã‚Äôspolicy
Note that, restricted to discretised policies ùúã which only picks attimeùë° andùë†ÀÜ ùë° denotethestateattimeùë°.Withprobabilityatleast
actionsinùê¥ÀÜ ùëë,thevalueofùúã intheoriginalMDPùëÄ,ùëâ ùëÄùúã,equalsto 1‚àíùõø,
itsvaluethesameMDPrestrictedtodiscretisedactionspace,ùëâ ùëÄùúã ùê¥ÀÜ ùëë. ùëâ ùëÄ‚àó (ùë†ÀÜ ùë°)‚àíùëâ ùëÄAùë° (ùë†ÀÜ ùë°) ‚â§ùúÄ+ (1K ‚àíùúÜ
ùõæÀÜ)2
+ L 1‚àíRùõº
ùõæ
+ 2(ùõæ 1ùêæ ‚àíùëù ùõæùõº
)2
4.10 CaseI istrueforallbut
SumminguptheboundsinLemmaI.3,TheoremsG.2andH.1,we (cid:18) ‚àö (cid:19)|ùëÜ||ùê¥|
obtainthefinalboundforCaseI.Thefinalboundguarantees,with (cid:169)(|ùëÜ||ùê¥|)2ùëõ‚Ñé 2 |ùëÜ||ùê¥| +1 (cid:170)
highprobability,thatthepolicyweobtainfromR-FOSiscloseto
ùëÇÀú(cid:173)
(cid:173)
ùúÜ (cid:174)
(cid:174) timesteps
(cid:173) ùúÄ3(1‚àíùõæ)6 (cid:174)
theoptimalpolicyinùëÄapartfromaconstantfactor. (cid:173) (cid:174)
(cid:173) (cid:174)
(cid:171) (cid:172)
Theorem4.18. Supposethat0 ‚â§ ùúÄ < 1‚àí1 ùõæ and0 ‚â§ ùõø < 1are
5 Experiments
two real numbers. Let M be any continuous meta-MDP with in-
nerstochasticgameùê∫ = ‚ü®ùëÜ,ùê¥,ùëá inner,ùëÖ inner‚ü©.LetusdenoteùëÄ ùëë = Wenowvalidateourtheoreticalfindingsempirically.
‚ü®ùëÜÀÜ ùëë,ùê¥ÀÜ ùëë,ùëá ùëë,ùëÖ ùëë,ùõæ‚ü© asdiscretisedversionofùëÄ (asdescribedinCase
I) using grid size of ùúÜ. There exists inputsùëö = ùëö(cid:16) ùúÄ1, ùõø1(cid:17) andùúÄ1, 5.1 TheMatchingPenniesEnvironment
satisfying
MatchingPenniesisatwo-player,zero-sumgamewithapayoff
matrixshowninSection5.1.EachagenteitherpickHeads(H)or
ùëö(cid:18)1
ùúÄ,
ùõø1(cid:19) =ùëÇÀú(cid:169) (cid:173)
(cid:173)
(cid:173)(cid:18) 2‚àö ùëõ ùúÜ|ùëÜ ùúÄ2|| (ùê¥ 1| ‚àí+ ùõæ1 )(cid:19) 4ùëõ|ùëÜ||ùê¥| (cid:170) (cid:174)
(cid:174)
(cid:174)
T t gh aa e mils p er( iT o sb) n, aùëé obùëñ til i‚àà i tt ey{ rùêª ao tf, eùëá dp .} la Ta y hn e id sr mùëñùëéùëñ p e‚àº i ac nkùúã siùúô n tùëñ hg( a¬∑ H t| a.{ nN} i) o n, t nw e eh t rh -e ear pe t ii sùúô n oùëñ dtc eho i hr sr awe ss o ap ro lk en n,d gth tt ho e
(cid:173) (cid:174)
(cid:173) (cid:174) of1andtheinner-episodicreturncorrespondstothepayoffafter
(cid:171) (cid:172)
oneinteractionùëü =PayoffTable(ùëé1,ùëé2).ForR-FOS,thismeansthat
and ùúÄ1
1
=ùëÇ(cid:16) ùúÄ1(cid:17) ,suchthatifùëÖ-ùëÄùê¥ùëã isexecutedonùëÄ withinputs a gam me et .a T-s ht eep mc eo tr ar -e rs ep tuo rn nds cot ro reo sn pe oi nte dr sa tt oio tn heof dt ish ce oM una tt ec dh ,i cn ug mP ue ln an tii ve es
ùëöandùúÄ1,thenthefollowingholds.LetAùë° denoteùëÖ-ùëÄùê¥ùëã‚Äôspolicy
meta-rewardafterplayingtheMatchingPenniesùêætimes.Whilethe
attimeùë° andùë†ÀÜ
ùë°
denotethestateattimeùë°.Withprobabilityatleast
originalM-FOSwasevaluatedonamorecomplex,iteratedversion
1‚àíùõø,
oftheMatchingPenniesgame,thissimplesettingwithabinary
ùëâ ùëÄ‚àó (ùë†ÀÜ ùë°)‚àíùëâ ùëÄAùë° (ùë†ÀÜ ùë°) ‚â§ùúÄ+ (1K ‚àíùúÜ
ùõæÀÜ)2
+ L 1‚àíRùõº
ùõæ
+ (1ùõæùêæ ‚àíùëù ùõæùõº
)2
a ac lst oio mn os rp ea pce rai cs ts icu affi lfc oie rn imtf po lr emou er ne tam tip oi nri bca el cav ua sli ed ta hti eo ùëÖn M.O Au Xr as le gt ot rin itg hmismemoryusagegrowsexponentiallywiththesizeofthestateand
actionspace.Thus,foranyofthemorecomplexenvironmentsfrom
theM-FOSpaperwewerenotabledoanyempiricalanalysisofR-
FOSatall,duetotheexponentialsampleandmemoryrequirements.
Player1\Player2 Head Tail
Head (+1,-1) (-1,+1)
Tail (-1,+1) (+1,-1)
Table1:PayoffMatrixforMP
5.2 ExperimentSetup
WeimplementanempiricalversionofourR-FOSalgorithm.Be-
causetheR-FOSalgorithmusesQ-valueiterationtosolvethemeta-
game, the algorithm needs to keep a copy of the meta-Q-value
table.Therefore,memoryusagegrowsexponentiallywithrespect
Figure1:Empiricalsamplecomplexitywhilevaryingthetra-
totheinner-game‚Äôsstate-actionspacesize.WefoundthatCaseIof
jectorywindow‚Ñé.Weplotthemeta-rewardpermeta-episode.
thealgorithmwasintractabletoimplementevenwithacompact
Tobettervisualisetheconnectionwiththetheoryresults,
environmentlikeMP.ThemetaQ-tableofsize|ùëÜÀÜ |√ó|ùê¥ÀÜ |wassim-
weplotthex-axisinlog 16scale.Thereportedresultsarethe
plytoolargetofitinmemory.Therefore,wefocusonempirically
meanover3seedswithstandarderror.
validatingasimplifiedcaseofCaseII.Wemaketwosimplifications,
(1) Themeta-stateusesapartialhistoryofpastactions.Onlythe
7 Conclusion
mostrecent‚Ñéactionsareused,where‚Ñéisahyper-parameter
Wepresentedthreemaincontributionsinourwork.Firstofall,we
wepick.Thewindowsizeallowsustocontrolthesizeofthe
presentedR-FOS,atabularalgorithmadaptedfromM-FOS.Unlike
meta-gamestate,i.e.,ùëÜÀÜ ‚ààR2‚Ñé.BecausetheMPgameonlyhas
M-FOS,whichlearnsapolicyinacontinuousmeta-MDP,R-FOS
onestate,itisnotnecessarytoincludethestate.
insteadlearnsapolicyinadiscreteapproximationoftheorigi-
nalmeta-MDPwhichallowsustomoreeasilyperformtheoretical
(2) Tofurtherdecreasetheproblemsizefortractability,wedefine
themeta-agentactiontobetheinner-agent‚Äôsgreedyaction,
analysis.Withinthisdiscretisedmeta-MDP,R-FOSusestheùëÖ
MAX
algorithmasthemeta-agent.WeadaptedR-FOSfromM-FOSsuch
insteadoftheQ-table.Thisresultsinamuchsmallmeta-action
sizeof|ùê¥ÀÜ |=2 thatitstillmaintainsallkeyattributesofM-FOS.Secondofall,we
derivedanexponentialsamplecomplexityboundforbothcases
describedinM-FOS(thetwocasesbeingeitherinner-gamepolicies
6 ResultsandDiscussion
orinner-gametrajectoryhistoryasmeta-state).Specifically,we
Wedrawconnectionsbetweenoursamplecomplexitytheoryresults
provedthatwithhighprobability,thepolicylearntbyR-FOSis
andexperimentalresultsintheMPenvironment.Ourgoalisto
closetotheoptimalpolicyfromtheoriginalmeta-MDPuptoa
analysethescalinglawofR-FOS.Specifically,weinvestigatehow
constantdistance.Finally,weimplementedR-FOSandinvestigated
thesamplecomplexitychangeswhenwevarythewindow-size‚Ñé.
theempiricalsamplecomplexityintheMatchingPenniesenviron-
UndertheMPenvironmentsettings,theinner-gamestate-action
ment.Wedrawconnectionsbetweentheoryandexperimentsby
spacesizeis|ùëÜ||ùê¥|=2andthenumberofplayersisùëõ=2.Following
showingbothresultsscalesexponentiallyaccordingtothesizeof
theboundinTheorem4.19,weseethattheonlytermthatdepends
theinner-game‚Äôsstate-action-space.
onhisthe16‚Ñé term:
(cid:18) ‚àö (cid:19)|ùëÜ||ùê¥| (cid:18) ‚àö (cid:19)2 Acknowledgments
(cid:169)(|ùëÜ||ùê¥|)2ùëõ‚Ñé 2 |ùëÜ||ùê¥| +1 (cid:170) (cid:169)16‚Ñé 2 2 +1 (cid:170)
ùëÇÀú(cid:173)
(cid:173)
(cid:173)
ùúÄ3(1‚àíùúÜ
ùõæ)6
(cid:174)
(cid:174)
(cid:174)‚àºùëÇÀú(cid:173)
(cid:173)
(cid:173)
ùúÄ3(1ùúÜ
‚àíùõæ)6
(cid:174)
(cid:174)
(cid:174)
K QF Zw ia ss ss uu pp pp oo rr tt ee ddb by yt Ah re maD s. uH i. ssC ehe an ndFo Cu on hd ea rt ei .onScholarship.
(cid:173) (cid:174) (cid:173) (cid:174)
(cid:173) (cid:174) (cid:173) (cid:174)
(cid:171) (cid:172) (cid:171) (cid:172)
Hence,ourtheoryresultssaysthatwheneverthegamehorizonis
increasedby1,weexpecttoseethesamplecomplexitytoincrease
byafactorof16intheMPenvironment.Figure1showsthereward
across the meta episodes on a log 16 scale. The graph contains
threerewardcurvesformeta-trajectorylength‚Ñé =2,3,4,which
convergesapproximatelyat163,164,ùëéùëõùëë165episodes.Indeed,this
isconsistentwithourtheoreticalresultsinTheoremG.3.References the7thInternationalConferenceonNeuralInformationProcessingSys-
[1] DavidBalduzzi,SebastienRacaniere,JamesMartens,JakobFoerster,
tems(Denver,Colorado)(NIPS‚Äô94).MITPress,Cambridge,MA,USA,
361‚Äì368.
KarlTuyls,andThoreGraepel.2018. Themechanicsofn-player
[20] AlexanderL.Strehl,LihongLi,andMichaelL.Littman.2009. Rein-
differentiablegames.InInternationalConferenceonMachineLearning.
PMLR,354‚Äì363.
forcementLearninginFiniteMDPs:PACAnalysis.J.Mach.Learn.Res.
10(dec2009),2413‚Äì2444.
[2] Craig Boutilier, Thomas Dean, and Steve Hanks. 1999. Decision-
[21] TimonWilli,AlistairHpLetcher,JohannesTreutlein,andJakobFoer-
TheoreticPlanning:StructuralAssumptionsandComputationalLever-
ster.2022.COLA:consistentlearningwithopponent-learningaware-
age.J.Artif.Int.Res.11,1(jul1999),1‚Äì94.
[3] RonenBrafmanandMosheTennenholtz.2001.R-MAX-AGeneral
ness.InInternationalConferenceonMachineLearning.PMLR,23804‚Äì
23831.
PolynomialTimeAlgorithmforNear-OptimalReinforcementLearning.
[22] QizhenZhang,ChrisLu,AnimeshGarg,andJakobFoerster.2022.
TheJournalofMachineLearningResearch3,953‚Äì958. https://doi.org/
10.1162/153244303765208377
CentralizedModelandExplorationPolicyforMulti-AgentRL.InIn-
[4] CHEE-SChowandJohnNTsitsiklis.1991.Anoptimalone-waymulti- ternationalConferenceonAutonomousAgentsandMulti-AgentSystems
gridalgorithmfordiscrete-timestochasticcontrol.IEEEtransactions
(AAMAS). https://arxiv.org/abs/2107.06434v2
[23] StephenZhao,ChrisLu,RogerBGrosse,andJakobFoerster.2022.
onautomaticcontrol36,8(1991),898‚Äì914.
[5] MuratA.Erdogdu.2022.Coveringwithepsilon-nets. https://erdogdu.
ProximalLearningWithOpponent-LearningAwareness.Advancesin
github.io/csc2532/lectures/lecture05.pdf
NeuralInformationProcessingSystems35(2022),26324‚Äì26336.
[6] Jakob N. Foerster, Richard Y. Chen, Maruan Al-Shedivat, Shimon
Whiteson,PieterAbbeel,andIgorMordatch.2018. Learningwith
Opponent-LearningAwareness. arXiv:1709.04326[cs.AI]
[7] DHausslerandEWelzl.1986.Epsilon-NetsandSimplexRangeQueries.
InProceedingsoftheSecondAnnualSymposiumonComputational
Geometry(YorktownHeights,NewYork,USA)(SCG‚Äô86).Association
forComputingMachinery,NewYork,NY,USA,61‚Äì71. https://doi.
org/10.1145/10515.10522
[8] ShamKakade.2003.OnthesamplecomplexityofReinforcementLearn-
ing.Ph.D.Dissertation.UniversityofLondon.
[9] AkbirKhan,NewtonKwan,TimonWilli,ChrisLu,AndreaTacchetti,
andJakobNicolausFoerster.[n.d.].ContextandHistoryAwareOther-
Shaping.([n.d.]).
[10] Dong-Ki Kim, Miao Liu, Matthew D Riemer, Chuangchuang Sun,
MarwaAbdulhai,GolnazHabibi,SebastianLopez-Cot,GeraldTesauro,
andJONATHANPHOW.2021. APolicyGradientAlgorithmfor
LearningtoLearninMultiagentReinforcementLearning. https:
//openreview.net/forum?id=zdrls6LIX4W
[11] AlistairLetcher.2020.Ontheimpossibilityofglobalconvergencein
multi-lossoptimization.arXivpreprintarXiv:2005.12649(2020).
[12] AlistairLetcher,JakobFoerster,DavidBalduzzi,TimRockt√§schel,and
ShimonWhiteson.2018. Stableopponentshapingindifferentiable
games.arXivpreprintarXiv:1811.08469(2018).
[13] YaoLiuandEmmaBrunskill.2018.WhenSimpleExplorationisSample
Efficient:IdentifyingSufficientConditionsforRandomExplorationto
YieldPACRLAlgorithms.
[14] ChrisLu,TimonWilli,ChristianA.SchroederdeWitt,andJakobN.
Foerster.2022.Model-FreeOpponentShaping.InInternationalCon-
ferenceonMachineLearning,ICML2022,17-23July2022,Baltimore,
Maryland,USA(ProceedingsofMachineLearningResearch,Vol.162).
PMLR,14398‚Äì14411.
[15] ChrisLu,TimonWilli,AlistairLetcher,andJakobFoerster.2022.Ad-
versarialCheapTalk.arXivpreprintarXiv:2211.11030(2022).
[16] DhruvMalik,AldoPacchiano,VishwakSrinivasan,andYuanzhiLi.
2021.SampleEfficientReinforcementLearningInContinuousState
Spaces:APerspectiveBeyondLinearity.InInternationalConferenceon
MachineLearning.
[17] FlorianSch√§ferandAnimaAnandkumar.2019.Competitivegradient
descent.AdvancesinNeuralInformationProcessingSystems32(2019).
[18] JohnSchulman,FilipWolski,PrafullaDhariwal,AlecRadford,and
OlegKlimov.2017. Proximalpolicyoptimizationalgorithms. arXiv
preprintarXiv:1707.06347(2017).
[19] SatinderP.Singh,TommiJaakkola,andMichaelI.Jordan.1994. Re-
inforcementLearningwithSoftStateAggregation.InProceedingsofA Overview
A.1 ProblemSetup
Themeta-gameisdefinedbyacontinuousMDPùëÄ =‚ü®ùëÜÀÜ,ùê¥ÀÜ,ùëá,ùëÖ,ùõæ‚ü©withfinitehorizon‚Ñé.
Fortheremainingoftheproof,weconsidertwowaystoformulatethemeta-statespaceandmeta-actionspace:
‚Ä¢ InCaseI,themeta-stateisallinneragents‚Äôpolicies‚Äôparametersfromtheprevioustimestep,andthemeta-actionistheinneragent‚Äôs
currentpolicyparameters.
‚Ä¢ InCaseII,theonlydifferencewithCaseIisthemeta-stateisinsteadallinneragents‚Äôtrajectories.
SeeTable2forasummaryforthetwocases.
Table2:Twocasesofrepresentingthemeta-statespaceandmeta-actionspace.
ùë†ÀÜ ùë° = ùëéÀÜ ùë° =
CaseI ùùìùë°‚àí1= [ùúô ùë°ùëñ ‚àí1,ùùì ùíï‚àí ‚àíùíä 1] ùúô ùë°ùëñ
CaseII ùùâùë° ùúô ùë°ùëñ
Theinner-gameisann-playerfully-observablediscretestochasticgameùê∫ =‚ü®ùëÜ,ùê¥,ùëá inner,ùëÖ inner‚ü©withfinitehorizon‚Ñé.
A.2 TheoryOverview
WederivethesamplecomplexityofourR-FOSalgorithm.Onahigh-level,theproofconsistsofsixsteps.
(1) WediscretiseourMDPùëÄintoùëÄ .Wefirstdescretisethecontinuousmeta-statespaceandmeta-actionspaceusingepsilon-nets[4].
ùëë
Basedonthedescretisedmeta-stateandmeta-actionspace,wethendefinethediscretisedtransitionandrewardfunction.
(2) Wethenconstructaùëö-knowndiscretisedMDPùëÄ ,asdescribedbytheR-MAXalgorithm[20].
ùëö,ùëë
(3) Then,weestimatetheempiricalùëö-knowndiscretisedMDPùëÄÀÜ usingmaximumlikelihoodestimate.Thisisthesameprocedure
ùëö,ùëë
describedbytheR-MAXalgorithm[20].Ouralgorithm,R-FOS,learnsanoptimalpolicyinùëÄÀÜ .
ùëö,ùëë
(4) WefirstproveaPAC-boundbetweentheoptimalpolicieslearntinùëÄÀÜ andùëÄ .Thisstepusesresultsfrom[20].
ùëö,ùëë ùëë
(5) WethenproveaboundbetweentheoptimalpolicieslearntinùëÄ andùëÄ.Thisstepusesresultsfrom[4].
ùëë
(6) WeobtainthefinalPAC-boundbuildingfromthetwoboundsfromabove.
B Nomenclature
Symbol Definition
ùë†ÀÜ
ùë°
=ùë†ÀÜ Meta-stateattimeùë°,timesubscriptùë° isomittedforconvenience
ùëéÀÜ
ùë°
=ùëéÀÜ Meta-actionattimeùë°,timesubscriptùë° isomittedforconvenience
(ùë†ÀÜ ùëë,ùëéÀÜ ùëë) discretisedstate-actionpairinmeta-game
ùëüÀÜ ùëë =ùëüÀÜ(ùë†ÀÜ ùëë,ùëéÀÜ ùëë) Meta-rewardfunctionparameterisedbydiscretisedmeta-state-actionpair
ùúôùëñ =ùúôùëñ Thesetofinner-gamepolicyparametersofouragentattimeùë°,timesubscriptùë° isomittedforconvenience
ùë°
ùúôùëñ =ùúôùëñ Thesetofdiscretisedinner-gamepolicyparametersofouragentattimeùë°,timesubscriptùë° isomittedforconvenience
ùë°,ùëë ùëë
ùùì‚àíùëñ The set of inner-game policy parameters of all agents except our agent at timeùë°, time subscriptùë° is omitted for
convenience
ùùì‚àíùíä Thesetofdiscretisedinner-gamepolicyparametersofallagentsexceptouragentattimeùë°,timesubscriptùë° isomitted
ùíÖ
forconvenience
ùëÖÀÜ (ùë†ÀÜ,ùëéÀÜ),ùëáÀÜ (ùë†ÀÜ,ùëéÀÜ) Empiricalestimateofrewardandtransitiondistribution
ùëÖ(ùë†ÀÜ,ùëéÀÜ),ùëá(ùë†ÀÜ,ùëéÀÜ) TruerewardandtransitiondistributionC Assumptions
WefirstoutlineallassumptionsmadeinderivingthesamplecomplexityoftheR-FOSalgorithm.
Toestablishtheboundinstep5,wemakethefollowingassumptions.
AssumptionC.1. Bothmeta-gameandinner-gamearefinitehorizon.Weuse‚Ñétodenotethemeta-gamehorizon,and‚Ñé innertodenotethe
inner-gamehorizon.
AssumptionC.2. Themeta-gameusesadiscountfactorofùõæ.Forsimplicityoftheproof,assumetheinner-gameusesadiscountfactorof1.
AlthoughthisassumptioncanbeeasilyomittedbysubstitutingùëÖ intheoriginalproofin[20].
MAX
AssumptionC.3. Weassumetheinner-gamerewardisbounded.Forsimplicityoftheproof,wesetthisboundas 1 ,where‚Ñéisthe
‚Ñéinner
horizonoftheinnergame.Formally,forall(ùë†,ùëé),0 ‚â§ ùëÖ inner(ùë†,ùëé) ‚â§ ‚Ñé1.Thisallowsustointroducethenotionofmaximumrewardand
maximumvaluefunctionasùëÖ max,inner = ‚Ñéin1
ner
andùëâ max,inner =1respectively.Thisimpliestherewardandvaluefunctioninthemeta-game
arealsobounded,i.e.,ùëÖmax=1andùëâmax= 1‚àí1 ùõæ.
AssumptionC.4. Theinnergameisassumedtobediscrete.
Toestablishtheboundinstep6,wemakethefollowingassumptions.
AssumptionC.5. Themeta-rewardfunctionisLipschitz-continuous:Forallùë†ÀÜ 1,ùë†ÀÜ 2 ‚ààùëÜÀÜandùëéÀÜ 1,ùëéÀÜ 2 ‚ààùê¥ÀÜ,
(cid:12) (cid:12)ùëÖ(ùë†ÀÜ 1,ùëéÀÜ 1)‚àíùëÖ(ùë†ÀÜ 2,ùëéÀÜ 2)(cid:12) (cid:12)‚â§LùëÖ(cid:13) (cid:13)(ùë†ÀÜ 1,ùëéÀÜ 1)‚àí(ùë†ÀÜ 2,ùëéÀÜ 2)(cid:13) (cid:13) ‚àû, ‚àÄùë†ÀÜ 1,ùë†ÀÜ 2 ‚ààùëÜÀÜandùëéÀÜ 1,ùëéÀÜ 2 ‚ààùê¥ÀÜ
whereLùëÖ isthemeta-rewardfunction‚ÄôsLipschitz-constant.
AssumptionC.6. Themeta-transitionfunctionisLipschitz-continuous:Forallùë†ÀÜ 1,ùë†ÀÜ 1‚Ä≤,ùë†ÀÜ 2 ‚ààùëÜÀÜandùëéÀÜ 1,ùëéÀÜ 1‚Ä≤,ùëéÀÜ 2 ‚ààùê¥ÀÜ,
(cid:12) (cid:12)ùëá(ùë†ÀÜ 1‚Ä≤ |ùë†ÀÜ 1,ùëéÀÜ 1)‚àíùëá(ùë†ÀÜ 2‚Ä≤ |ùë†ÀÜ 2,ùëéÀÜ 2)(cid:12) (cid:12)‚â§Lùëá(cid:13) (cid:13)(ùë†ÀÜ 1‚Ä≤,ùë†ÀÜ 1,ùëéÀÜ 1)‚àí(ùë†ÀÜ 2‚Ä≤,ùë†ÀÜ 2,ùëéÀÜ 2)(cid:13)
(cid:13)
‚àû
whereLùëá isthemeta-transitionfunction‚ÄôsLipschitz-constant.
AssumptionC.7. There‚ÄôsaLipschitz-continuouspoint-to-setmappingbetweenmeta-statespaceandmeta-actionspacesuchthatforany
ùë†ÀÜ,ùë†ÀÜ‚Ä≤ ‚ààùëÜÀÜandùëéÀÜ,ùëéÀÜ‚Ä≤ ‚ààùê¥ÀÜ,thereexistssomeùëéÀÜ‚ààùëà(ùë†ÀÜ)suchthat(cid:13) (cid:13)ùëéÀÜ‚àíùëéÀÜ‚Ä≤(cid:13)
(cid:13)
<L(cid:13) (cid:13)ùë†ÀÜ‚àíùë†ÀÜ‚Ä≤(cid:13)
(cid:13)
‚àû ‚àû
AssumptionC.8. Themeta-gametransitionfunctionùëá(¬∑|ùë†ÀÜ,ùëéÀÜ)isaprobabilitydensityfunctionsuchthat0‚â§ùëá(ùë†ÀÜ‚Ä≤ |ùë†ÀÜ,ùëéÀÜ) ‚â§Land ‚à´ ùëÜÀÜùëá(ùë†ÀÜ‚Ä≤ |
ùë†ÀÜ,ùëéÀÜ)ùëëùë†ÀÜ‚Ä≤ =1, ‚àÄùë†ÀÜ,ùë†ÀÜ‚Ä≤ ‚ààùëÜÀÜandùëéÀÜ‚ààùê¥ÀÜ
D Step1:DiscretisationwithùúÄ-Net
Figure2:ùúÄ-NetforŒò={ùúÉ ‚ààR2:‚à•ùúÉ‚à• ‚â§ùëÖ}[5]
ToapplytheR-MAXalgorithm,wefirstconverttheMDPùëÄinM-FOSintoatabularMDPùëÄ .
ùëëDefinitionD.1. (ùúÄ-Net[5])ForùúÄ >0,NùúÄ isanùúÄ-netoverthesetŒò‚äÜRùê∑ ifforallùúÉ ‚ààŒò,thereexistsùúÉ‚Ä≤ ‚ààNùúÄ suchthat(cid:13) (cid:13)ùúÉ‚àíùúÉ‚Ä≤(cid:13) (cid:13)2 ‚â§ùúÄ.
Todiscretiseaùê∑-dimensionalsphereofradiusùëÖ,weuseaùúÄ-netcontainingùê∑-dimensionalcubesofsidesùúÜ.Thisresultsina
(cid:16)2ùëÖ +1(cid:17)ùê∑
ùúÜ
points.Withineachùê∑-dimensionalcube,thelargestdistancebetweentheverticesandtheinteriorpointscomesfromthecenterofthe
‚àö
cube,whichis ùúÜ ùëë.Therefore,toguaranteeafullcoverofallthepointsinthesphere,thelargestcubesizethatwecanhaveshouldsatisfy
‚àö 2
ùúÄ = ùúÜ ùëë.Figure2illustratesanexampleofusingùúÄ-netstodiscretisetheinputspaceŒò={ùúÉ ‚ààR2:‚à•ùúÉ‚à• ‚â§ùëÖ}.Fromhereon,wewillreplace
2
theùúÄinùúÄ-netwithùõº toavoidnotationoverloading.
D.1 DiscretisationoftheStateandActionSpace:CaseI
InCaseI,themeta-stateùë†ÀÜ
ùë°
isallinneragents‚Äôpoliciesparametersfromtheprevioustimestep.Eachoftheinneragentùëñ‚ÄôspolicyisaQ-table,
denotedasùúô ùëñ ‚ààR|ùëÜ|√ó|ùê¥|.Formally,ùë†ÀÜ ùë° :=ùùìùë°‚àí1= [ùúô ùë°ùëñ ‚àí1,ùùì ùíï‚àí ‚àíùíä 1].Themeta-actionùëéÀÜ ùë° istheinneragent‚Äôscurrentpolicyparametersùúô ùë°ùëñ.
Forthemeta-actionspaceùê¥ÀÜandachosendiscretisationerrorùõº >0,weobtaintheùúÄ-netùê¥ÀÜ
ùëë
‚äÇùê¥ÀÜsuchthatforallùëéÀÜ‚ààùê¥ÀÜ,thereexistùëéÀÜ
ùëë
‚ààùê¥ÀÜ
ùëë
where
(cid:13) (cid:13)ùëéÀÜ‚àíùëéÀÜ ùëë(cid:13) (cid:13)‚â§ùõº. (8)
Wecaninferùê¥ÀÜhasdimensionùê∑ =|ùëÜ||ùê¥|andRadiusùëÖ=‚àöÔ∏Å1¬∑|ùëÜ||ùê¥|=‚àöÔ∏Å |ùëÜ||ùê¥|(AssumptionC.3).DividingthespacewithgridsizeùúÜresults
inthesizeofdiscretisedmeta-actionspaceuppertobeboundedby
(cid:32) 2‚àöÔ∏Å |ùëÜ||ùê¥| (cid:33)|ùëÜ||ùê¥|
|ùê¥ÀÜ ùëë| ‚â§
ùúÜ
+1 . (9)
Similarity,wecanalsoinferùëÜÀÜhasdimensionùê∑ =ùëõ|ùëÜ||ùê¥|andRadiusùëÖ=‚àöÔ∏Å1¬∑ùëõ|ùëÜ||ùê¥|=‚àöÔ∏Åùëõ|ùëÜ||ùê¥|.DividingthespacewithgridsizeùúÜresults
inthesizeofdiscretisedmeta-statespaceuppertobeboundedby
(cid:32) 2‚àöÔ∏Åùëõ|ùëÜ||ùê¥| (cid:33)ùëõ|ùëÜ||ùê¥|
|ùëÜÀÜ ùëë| ‚â§
ùúÜ
+1 . (10)
D.1.1 DiscretisationoftheStateandActionSpace:CaseII
InCaseII,themeta-stateùë†ÀÜ
ùë°
isallinneragents‚Äôtrajectories.Formally,ùë†ÀÜ
ùë°
:=ùùâùë°.,whereùúè ùë°ùëñ ={ùë†0,ùëé0,ùë†1,ùëé1,...,ùë† ùë°,ùëé ùë°}.Becauseweassumethe
Inner-Gameisdiscrete(i.e.thestateandactionspacearebothdiscrete),themeta-stateinthiscasedoesnotneeddiscretisation.Thus,we
candirectlyobtainthemeta-statespace,whichis,
|ùëÜÀÜ ùë°|=(|ùëÜ||ùê¥|)ùëõ‚Ñé. (11)
Themeta-actionremainssameasCaseI.
D.2 DiscretisationoftheTransitionandRewardFunction
Undertheabovediscretisationprocedure,wedefinethediscretisedMDPùëÄ
ùëë
=(ùëÜÀÜ,ùê¥ÀÜ ùëë,ùëá ùëë,ùëÖ ùëë,ùõæ),wherethestatespaceremainscontinuous,
theactionspaceisrestrictedtodiscretisedactions.WedefinethetransitionfunctionandrewardfunctionforùëÄ as:
ùëë
ùëá ùëë(ùë†ÀÜ‚Ä≤|ùë†ÀÜ,ùëéÀÜ ùëë)=
‚à´
ùëÜÀÜùëáùëá (( ùëßùë† ÀÜÀÜ ùëë‚Ä≤| |ùë†ÀÜ ùë†ÀÜùëë ùëë, ,ùëé ùëéÀÜ ÀÜùëë ùëë)
)ùëëùëßÀÜ
(12)
Wecanviewùëá ùëë(ùë†ÀÜ‚Ä≤|ùë†ÀÜ ùëë,ùëéÀÜ ùëë)asanormalizedsampleofùëá ùëë(¬∑|¬∑,ùëéÀÜ ùëë)atùë†ÀÜ‚Ä≤,ùë†ÀÜ ùëë.
ùëÖ ùëë(ùë†ÀÜ,ùëéÀÜ ùëë)=ùëÖ(ùë†ÀÜ ùëë,ùëéÀÜ ùëë) (13)
E Step2:Theùëö-knownDiscretisedMDP
Inthelaststep,weconvertedthemeta-MDPùëÄintoadiscretisedmeta-MDPùëÄ .FromùëÄ ,R-FOSbuildsaùëö-knowndiscretisedMDPùëÄ .
ùëë ùëë ùëö,ùëë
DefinitionE.1(m-KnownMDP). LetùëÄ
ùëë
=‚ü®ùëÜÀÜ ùëë,ùê¥ÀÜ ùëë,ùëá ùëë,ùëÖ ùëë,ùõæ‚ü©beaMDP.WedefineùëÄ ùëö,ùëëtobetheùëö-knownMDP(SeeTable3),where
ùëö-knownisthesetofstate-actionpairsthathasbeenvisitedatleastùëötimes.Forallstate-actionpairsinùëö-known,theinducedMDPùëÄ
ùëö,ùëë
behavesidenticaltoùëÄ .Forstate-actionpairsoutsideofùëö-known,thestate-actionpairsareself-absorbingandmaximallyrewarding.
ùëëGroundTruth Discretised ùëö-known Empirical
MDPùëÄ MDPùëÄ DiscretisedMDP ùëö-known
ùëë
ùëÄÀÜ DiscretisedMDP
ùëö,ùëë
ùëÄÀÜ
ùëö,ùëë
Known =ùëÄ =ùëÄ
ùëë
=ùëÄ
ùëë
‚âàùëÄ
ùëë
Unknown =ùëÄ =ùëÄ self-loopwithmaximumreward
ùëë
Table3:RelationshipbetweenùëÄ,ùëÄ ùëë,ùëÄ ùëö,ùëë,ùëÄÀÜ
ùëö,ùëë
F Step3:TheEmpiricalDiscretisedMDP
DefinitionF.1(Empiricalm-KnownMDP). ùëÄ
ùëö
istheexpectedversionofùëÄÀÜ
ùëö
where:
(cid:40)
ùëá ùëö,ùëë(ùë†ÀÜ ùëë‚Ä≤ |ùë†ÀÜ ùëë,ùëéÀÜ ùëë):= ùëá 1ùëë [ùë†( ÀÜùë† ùëë‚Ä≤ÀÜ ùëë‚Ä≤ =| ùë†ùë† ÀÜÀÜ ùëëùëë, ]ùëé ,ÀÜ ùëë) i of th(ùë†ÀÜ eùëë r, wùëéÀÜ ùëë is) e‚ààm-known
ùëáÀÜ ùëö,ùëë(ùë†ÀÜ ùëë‚Ä≤ |ùë†ÀÜ ùëë,ùëéÀÜ ùëë):=Ô£±Ô£¥Ô£¥Ô£≤ Ô£¥Ô£¥1ùëõ ùëõ( [ùë† ùë†ÀÜ ÀÜ(ùëë ùëëùë† ‚Ä≤ÀÜ, ùëëùëéÀÜ =,ùëë ùëéÀÜ, ùëëùë† ùë†ÀÜ ÀÜùëë )‚Ä≤ ùëë) ],
,
i of th(ùë†ÀÜ eùëë r, wùëéÀÜ ùëë is) e‚ààm-known
Ô£≥ (14)
(cid:40)
ùëÖ ùëö,ùëë(ùë†ÀÜ ùëë,ùëéÀÜ ùëë):= ùëÖùëÖ ùëë m( aùë†ÀÜ xùëë,ùëéÀÜ ùëë), i of th(ùë†ÀÜ eùëë r, wùëéÀÜ ùëë is) e‚ààm-known
ùëÖÀÜ ùëö,ùëë(ùë†ÀÜ ùëë,ùëéÀÜ ùëë)=Ô£±Ô£¥Ô£¥Ô£≤(cid:205) ùëñùëõ(ùë†ÀÜùëë ùëõ,ùëé (ÀÜ ùë†ùëë ÀÜ ùëë) ,ùëéùëü ÀÜ ùëë(ùë†ÀÜ )ùëë,ùëéÀÜ ùëë), if(ùë†ÀÜ ùëë,ùëéÀÜ ùëë) ‚ààm-known
Ô£¥Ô£¥ùëÖmax, otherwise
Ô£≥
ùëÖÀÜ ùëö,ùëë(ùë†ÀÜ ùëë,ùëéÀÜ ùëë) andùëáÀÜ ùëö,ùëë (cid:16) ùë†ÀÜ ùëë‚Ä≤ |ùë†ÀÜ ùëë,ùëéÀÜ ùëë(cid:17) arethemaximum-likelihoodestimatesfortherewardandtransitiondistributionofstate-actionpair
(ùë† ùëë,ùëé ùëë)withùëõ(ùë† ùëë,ùëé ùëë) ‚â•ùëöobservationsof(ùë† ùëë,ùëé ùëë).
G Step4:TheBoundBetweenùëÄ ùëë andùëÄÀÜ ùëö,ùëë
TheoremG.1. (R-MAXMDPBound[20]) Supposethat0‚â§ùúÄ < 1‚àí1 ùõæ and0‚â§ùõø <1aretworealnumbersandùëÄ =‚ü®ùëÜ,ùê¥,ùëá,ùëÖ,ùõæ‚ü©isanyMDP.
Thereexistsinputsùëö=ùëö(cid:16) ùúÄ1, ùõø1(cid:17) andùúÄ1,satisfyingùëö(cid:16) ùúÄ1, ùõø1(cid:17) =ùëÇ(cid:18) (|ùëÜ|+ln ùúÄ( 2|ùëÜ (1|| ‚àíùê¥ ùõæ| )/ 2ùõø))ùëâ m2 ax(cid:19) and ùúÄ1
1
=ùëÇ(cid:16) ùúÄ1(cid:17) ,suchthatifùëÖ-ùëÄùê¥ùëã isexecutedon
ùëÄwithinputsùëöandùúÄ1,thenthefollowingholds.Letùê¥
ùë°
denoteùëÖ-ùëÄùê¥ùëã‚Äôspolicyattimeùë° andùë†
ùë°
denotethestateattimeùë°.Withprobabilityat
least1‚àíùõø,ùëâ ùëÄùê¥ùë° (ùë† ùë°) ‚â•ùëâ ùëÄ‚àó (ùë† ùë°)‚àíùúÄistrueforallbut
ùëÇÀú (cid:18) |ùëÜ|2 |ùê¥|/(cid:16) ùúÄ3 (1‚àíùõæ)6(cid:17)(cid:19)
timesteps.
G.1 CaseI
TheoremG.2. Supposethat0‚â§ùúÄ < 1‚àí1 ùõæ and0‚â§ùõø <1aretworealnumbers.LetMbeanycontinuousmeta-MDPwithinnerstochasticgame
ùê∫ = ‚ü®ùëÜ,ùê¥,ùëá inner,ùëÖ inner‚ü©.LetusdenoteùëÄ ùëë = ‚ü®ùëÜÀÜ ùëë,ùê¥ÀÜ ùëë,ùëá ùëë,ùëÖ ùëë,ùõæ‚ü©asdiscretisedversionofùëÄ (asdescribedinCaseI)usinggridsizeofùúÜ.There
existsinputsùëö=ùëö(cid:16) ùúÄ1, ùõø1(cid:17) andùúÄ1,satisfying
(cid:18) ‚àö (cid:19)ùëõ|ùëÜ||ùê¥|
(cid:169)
2 ùëõ|ùëÜ||ùê¥|
+1 (cid:170)
ùëö(cid:18)1 ,1(cid:19) =ùëÇÀú(cid:173)
(cid:173)
ùúÜ (cid:174)
(cid:174)
ùúÄ ùõø (cid:173) ùúÄ2(1‚àíùõæ)4 (cid:174)
(cid:173) (cid:174)
(cid:173) (cid:174)
(cid:171) (cid:172)and ùúÄ1
1
=ùëÇ(cid:16) ùúÄ1(cid:17) ,suchthatifùëÖ-ùëÄùê¥ùëã isexecutedonùëÄwithinputsùëöandùúÄ1,thenthefollowingholds.LetAùë° denoteùëÖ-ùëÄùê¥ùëã‚Äôspolicyattimeùë°
andùë†ÀÜ ùë° denotethestateattimeùë°.Withprobabilityatleast1‚àíùõø,ùëâ ùëÄ‚àó
ùëë
(ùë†ÀÜ ùë°)‚àíùëâ ùëÄA ùëëùë° (ùë†ÀÜ ùë°) ‚â§ùúÄistrueforallbut
(cid:18) ‚àö (cid:19)2ùëõ|ùëÜ||ùê¥| (cid:18) ‚àö (cid:19)|ùëÜ||ùê¥|
(cid:169)
2 ùëõ|ùëÜ||ùê¥|
+1
2 |ùëÜ||ùê¥|
+1 (cid:170)
ùëÇÀú(cid:173)
(cid:173)
ùúÜ ùúÜ (cid:174)
(cid:174)
(cid:173) ùúÄ3(1‚àíùõæ)6 (cid:174)
(cid:173) (cid:174)
(cid:173) (cid:174)
(cid:171) (cid:172)
timesteps.
Proof. WefirstpluginEquations10and9intoTheoremG.1.Droppingthelogarithmtermsandplugginginùëâmax= 1‚àí1 ùõæ,weobtainthe
results. ‚ñ°
G.2 CaseII
TheoremG.3. Supposethat0‚â§ùúÄ < 1‚àí1 ùõæ and0‚â§ùõø <1aretworealnumbers.LetMbeanycontinuousmeta-MDPwithinnerstochasticgame
ùê∫ = ‚ü®ùëÜ,ùê¥,ùëá inner,ùëÖ inner‚ü©.LetusdenoteùëÄ ùëë = ‚ü®ùëÜÀÜ ùëë,ùê¥ÀÜ ùëë,ùëá ùëë,ùëÖ ùëë,ùõæ‚ü©asdiscretisedversionofùëÄ (asdescribedinCaseI)usinggridsizeofùúÜ.There
existsinputsùëö=ùëö(cid:16) ùúÄ1, ùõø1(cid:17) andùúÄ1,satisfying
ùëö(cid:18)1 ,1(cid:19)
=ùëÇÀú
(cid:32) (|ùëÜ||ùê¥|)ùëõ‚Ñé(cid:33)
ùúÄ ùõø ùúÄ2(1‚àíùõæ)4
and ùúÄ1
1
=ùëÇ(cid:16) ùúÄ1(cid:17) ,suchthatifùëÖ-ùëÄùê¥ùëã isexecutedonùëÄwithinputsùëöandùúÄ1,thenthefollowingholds.LetAùë° denoteùëÖ-ùëÄùê¥ùëã‚Äôspolicyattimeùë°
andùë†ÀÜ ùë° denotethestateattimeùë°.Withprobabilityatleast1‚àíùõø,ùëâ ùëÄ‚àó
ùëë
(ùë†ÀÜ ùë°)‚àíùëâ ùëÄA ùëëùë° (ùë†ÀÜ ùë°) ‚â§ùúÄistrueforallbut
(cid:18) ‚àö (cid:19)|ùëÜ||ùê¥|
(cid:169)(|ùëÜ||ùê¥|)2ùëõ‚Ñé 2 |ùëÜ||ùê¥| +1 (cid:170)
ùëÇÀú(cid:173)
(cid:173)
ùúÜ (cid:174)
(cid:174)
(cid:173) ùúÄ3(1‚àíùõæ)6 (cid:174)
(cid:173) (cid:174)
(cid:173) (cid:174)
(cid:171) (cid:172)
timesteps.
Proof. WefirstpluginEquations11and9intoTheoremG.1.Droppingthelogarithmtermsandplugginginùëâmax= 1‚àí1 ùõæ,weobtainthe
results. ‚ñ°
H Step5:TheBoundbetweenùëÄ andùëÄ ùëë
TheoremH.1. (MDPDiscretizationBound[7])ThereexistsaconstantK(thatsdependsonlyontheLipschitzconstantL)suchthatforsome
discretisationcoarsenessùúÜ‚àà (0, L 2]suchthat
(cid:13) (cid:13) KùúÜ
(cid:13)ùëâ‚àó ‚àíùëâ‚àó (cid:13) ‚â§ .
(cid:13) ùëÄ ùëÄùëë(cid:13)
‚àû
(1‚àíùõæÀÜ)2
H.1 SampleComplexityAnalysis
I Step6:Addingittogether
LemmaI.1(SimulationLemmaforContinuousMDP). LetùëÄ andùëÄÀÜ betwoMDPsthatonlydifferin (ùëá,ùëÖ) and (ùëáÀÜ,ùëÖÀÜ ).Andsupposethe
commonstatespaceùëÜÀÜ ofùëÄandùëÄÀÜ iscontinuous,anddenotethecommonactionspaceasùê¥ÀÜ
.
Letùúñ ùëÖ ‚â•max ùë†,ùëé|ùëÖÀÜ (ùë†,ùëé)‚àíùëÖ(ùë†,ùëé)|andùúÄ ùëù ‚â•max ùë†,ùëé‚à•ùëáÀÜ (¬∑|ùë†,ùëé)‚àíùëá(¬∑|ùë†,ùëé)||1¬∂ .Then‚àÄùúã :ùëÜÀÜ ‚Üíùê¥ÀÜ ,
(cid:13) (cid:13)ùëâùúã ‚àíùëâùúã(cid:13) (cid:13) ‚â§ ùúÄ ùëÖ +ùõæùúñ ùëÉùëâmax .
(cid:13) ùëÄ ùëÄÀÜ(cid:13) ‚àû 1‚àíùõæ 2(1‚àíùõæ)
¬∂Notethatgivenùëá(¬∑|ùë†,ùëé),ùëáÀÜ(¬∑|ùë†,ùëé)arefunctionsoncontinuousspaceùëÜÀÜ,thisistheùêø1normdefinedby‚à•ùëì‚à•1=‚à´ ùëÜÀÜ|ùëì|ùëëùúá.Similarly,throughouttheproof,wedenoteas‚à•‚à•ùëùthe
ùêøùëùnorm,definedby‚à•ùëì‚à•ùëù=(‚à´ ùëÜÀÜ|ùëì|ùëùùëëùúá)1/ùëù;andtheinnerproduct‚ü®ùëì,ùëî‚ü©=‚à´ ùëÜÀÜùëìùëîùëëùúá.Proof. Forallùë† ‚ààùëÜÀÜ,
(cid:12) (cid:12) (cid:12)ùëâ ùëÄùúã ÀÜ(ùë†)‚àíùëâ ùëÄùúã (ùë†)(cid:12) (cid:12) (cid:12)=(cid:12) (cid:12) (cid:12) (cid:12)ùëÖÀÜ (ùë†,ùúã)+ùõæ(cid:68) ùëáÀÜ (¬∑,ùúã),ùëâ ùëÄùúã ÀÜ(cid:69) ‚àíùëÖ(ùë†,ùúã)‚àíùõæ(cid:68) ùëá(¬∑,ùúã),ùëâ ùëÄùúã(cid:69)(cid:12) (cid:12) (cid:12)
(cid:12)
(cid:12) (cid:12)
‚â§ |ùëÖÀÜ (ùë†,ùúã)‚àíùëÖ(ùë†,ùúã)|+ùõæ(cid:12) (cid:12) (cid:12)(cid:68) ùëáÀÜ (¬∑,ùúã),ùëâ ùëÄùúã ÀÜ(cid:69) ‚àí(cid:68) ùëá(¬∑,ùúã),ùëâ ùëÄùúã(cid:69)(cid:12) (cid:12)
(cid:12)
(triangularinequality)
‚â§ùúÄ ùëÖ+ùõæ (cid:18)(cid:12) (cid:12) (cid:12) (cid:12)(cid:68) ùëáÀÜ (¬∑,ùúã),ùëâ ùëÄùúã ÀÜ(cid:69) ‚àí(cid:68) ùëá(¬∑,ùúã),ùëâ ùëÄùúã ÀÜ(cid:69) +(cid:68) ùëá(¬∑,ùúã),ùëâ ùëÄùúã ÀÜ(cid:69) ‚àí(cid:68) ùëá(¬∑,ùúã),ùëâ ùëÄùúã(cid:69)(cid:12) (cid:12) (cid:12) (cid:12)(cid:19) (add&subtract) (15)
(cid:12) (cid:12) (cid:12) (cid:12)
‚â§ùúÄ ùëÖ+ùõæ(cid:12)
(cid:12)
(cid:12)(cid:68) ùëáÀÜ (¬∑,ùúã)‚àíùëá(¬∑,ùúã),ùëâ ùëÄùúã ÀÜ(cid:69)(cid:12)
(cid:12)
(cid:12)+ùõæ(cid:12)
(cid:12)
(cid:12)(cid:68) ùëá(¬∑,ùúã),ùëâ ùëÄùúã
ÀÜ
‚àíùëâ ùëÄùúã(cid:69)(cid:12)
(cid:12)
(cid:12)
‚â§ùúÄ ùëÖ+ùõæ(cid:12) (cid:12) (cid:12) (cid:12)(cid:68) ùëáÀÜ (¬∑,ùúã)‚àíùëá(¬∑,ùúã),ùëâ ùëÄùúã ÀÜ(cid:69)(cid:12) (cid:12) (cid:12) (cid:12)+ùõæ(cid:12) (cid:12) (cid:12) (cid:12)(cid:12) (cid:12) (cid:12)ùëâ ùëÄùúã ÀÜ ‚àíùëâ ùëÄùúã(cid:12) (cid:12) (cid:12)(cid:12) (cid:12) (cid:12)
(cid:12)
.
‚àû
SinceEquation15holdsforallùë†ÀÜ‚ààùëÜÀÜ,wecantaketheinfinite-normonthelefthandside:
(cid:12) (cid:12) (cid:12) (cid:12)(cid:12) (cid:12) (cid:12)ùëâ ùëÄùúã ÀÜ ‚àíùëâ ùëÄùúã(cid:12) (cid:12) (cid:12)(cid:12) (cid:12) (cid:12)
(cid:12)
‚â§ùúÄ ùëÖ+ùõæ(cid:12) (cid:12) (cid:12) (cid:12)(cid:68) ùëáÀÜ (¬∑,ùúã)‚àíùëá(¬∑,ùúã),ùëâ ùëÄùúã ÀÜ(cid:69)(cid:12) (cid:12) (cid:12) (cid:12)+ùõæ(cid:12) (cid:12) (cid:12) (cid:12)(cid:12) (cid:12) (cid:12)ùëâ ùëÄùúã ÀÜ ‚àíùëâ ùëÄùúã(cid:12) (cid:12) (cid:12)(cid:12) (cid:12) (cid:12)
(cid:12)
. (16)
‚àû ‚àû
Wethenexpandthemiddletermasfollows:
(cid:12) (cid:12) (cid:12) (cid:12)(cid:68) ùëáÀÜ (¬∑,ùúã)‚àíùëá(¬∑,ùúã),ùëâ ùëÄùúã ÀÜ(cid:69)(cid:12) (cid:12) (cid:12) (cid:12)=(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)(cid:28) ùëáÀÜ (¬∑,ùúã)‚àíùëá(¬∑,ùúã),ùëâ ùëÄùúã ÀÜ ‚àí1¬∑ 2(ùëÖ 1m ‚àíax ùõæ)(cid:29)(cid:12) (cid:12) (cid:12) (cid:12)
(cid:12)
(where1isavectorofones‚ààR|ùëÜ|)
‚â§ ‚à•ùëáÀÜ (¬∑,ùúã)‚àíùëá(¬∑,ùúã)‚à•1¬∑(cid:13) (cid:13) (cid:13) (cid:13)ùëâ ùëÄùúã ÀÜ ‚àí1¬∑ 2(ùëÖ 1m ‚àíax ùõæ)(cid:13) (cid:13) (cid:13) (cid:13) (Holder‚Äôsinequality) (17)
‚àû
ùëÖmax
‚â§ùúñ ùëÉ ¬∑ 2(1‚àíùõæ)
ùëâmax
=ùúñ ùëÉ ¬∑ 2 .
InEquation17,thefirststepshiftstherangeofùëâ from[0, ùëÖmax ]to[‚àí ùëÖmax , ùëÖmax ]toobtainatighterboundbyafactorof2.Theequality
(1‚àíùõæ) 2(1‚àíùõæ) 2(1‚àíùõæ)
inline1holdsbecauseofthefollowing,whereùê∂isanyconstant:
‚ü®ùëáÀÜ ‚àíùëá,ùê∂¬∑1‚ü©=ùê∂‚ü®ùëáÀÜ ‚àíùëá,1‚ü©
=ùê∂(cid:16) ‚ü®ùëÉÀÜ,1‚ü©‚àí‚ü®ùëÉ,1‚ü©(cid:17)
(18)
=ùê∂(1‚àí1) becauseùëÉ andùëÉÀÜareprobabilitydistributions
=0
Fromequation18,weobservetheequalityinline1holds:
‚ü®ùëáÀÜ ‚àíùëá,ùëâ ‚àíùê∂¬∑1‚ü©=‚ü®ùëáÀÜ ‚àíùëá,ùëâ‚ü©‚àí‚ü®ùëáÀÜ ‚àíùëá,ùê∂¬∑1‚ü©
=‚ü®ùëáÀÜ ‚àíùëá,ùëâ‚ü©‚àí0 (19)
=‚ü®ùëáÀÜ ‚àíùëá,ùëâ‚ü©.
Finally,weplugequation17intoequation16toobtainthebound.
(cid:12) (cid:12) (cid:12) (cid:12)(cid:12) (cid:12) (cid:12)ùëâ ùëÄùúã ÀÜ ‚àíùëâ ùëÄùúã(cid:12) (cid:12) (cid:12)(cid:12) (cid:12) (cid:12)
(cid:12)
‚â§ùúñ ùëÖ+ùõæùúñ ùëÉ ¬∑ùëâm 2ax +ùõæ(cid:12) (cid:12) (cid:12) (cid:12)(cid:12) (cid:12) (cid:12)ùëâ ùëÄùúã ÀÜ ‚àíùëâ ùëÄùúã(cid:12) (cid:12) (cid:12)(cid:12) (cid:12) (cid:12)
(cid:12)
‚àû ‚àû (20)
=
ùúñ
ùëÖ
+ùõæùúñ ùëÉùëâmax
.
1‚àíùõæ 2(1‚àíùõæ)
‚ñ°
Underdiscretisation,[4]showedthat,withasmallenoughgridsize,andrestrictingtothediscretisedactionspace,thedifferenceintransition
probabilityofthecontinuousMDPùëÄanddiscretisedMDPùëÄ isupperboundedbyaconstant.
ùëëLemmaI.2. [4]Thereexistsaconstantùêæ ùëÉ (dependingonlyonconstantL)suchthat
|ùëá ùëë(ùë†ÀÜ‚Ä≤|ùë†ÀÜ,ùëéÀÜ ùëë)‚àíùëá(ùë†ÀÜ‚Ä≤|ùë†ÀÜ,ùëéÀÜ ùëë)| ‚â§ùêæ ùëùùõº
forallùë†ÀÜ‚Ä≤,ùë†ÀÜ‚ààùëÜÀÜ,ùëéÀÜ ùëë ‚ààùê¥ÀÜ ùëë andallùõº ‚â§ (0, 21 L]
Wenowapplysimulationlemmaboundthedifferenceinvalueforanydiscretisedpolicy(i.e.restrictingactionspacetoùê¥ÀÜ )inthecontinuous
ùëë
MDPùëÄanddiscretisedMDPùëÄ .
ùëë
LemmaI.3. LetùëÄ ùê¥ÀÜ
ùëë
=(ùëÜÀÜ,ùê¥ÀÜ ùëë,ùëá,ùëÖ,ùõæ),thatis,thecontinousMDPùëÄrestrictedtothediscretisedactionspace.AndrecallthatdiscretisedMDP
ùëÄ ùëë =(ùëÜÀÜ,ùê¥ÀÜ ùëë,ùëá ùëë,ùëÖ ùëë,ùõæ).Thenforanydiscretisedpolicyùúã :ùëÜÀÜ ‚Üíùê¥ÀÜ ùëë,
‚à•ùëâ ùëÄùúã
ùê¥ÀÜ
ùëë
‚àíùëâ ùëÄùúã ùëë‚à•‚àû=‚à•ùëâ ùëÄùúã ‚àíùëâ ùëÄùúã ùëë‚à•‚àû ‚â§ L 1‚àíRùõº
ùõæ
+ (1ùõæùêæ ‚àíùëù ùõæùõº
)2
Proof. LemmaI.2givestheboundfordifferenceintransitionprobability
ùúñ ùëÉ =max‚à•ùëá ùëë(ùë†ÀÜ‚Ä≤|ùë†ÀÜ,ùëéÀÜ ùëë)‚àíùëá(ùë†ÀÜ‚Ä≤|ùë†ÀÜ,ùëéÀÜ ùëë)‚à•1=max2ùëáùëâ(ùëá ùëë(¬∑|ùë†ÀÜ,ùëéÀÜ ùëë),ùëá(¬∑|ùë†ÀÜ,ùëéÀÜ ùëë)) ‚â§2max|ùëá ùëë(ùë†ÀÜ‚Ä≤|ùë†ÀÜ,ùëéÀÜ ùëë)‚àíùëá(ùë†ÀÜ‚Ä≤|ùë†ÀÜ,ùëéÀÜ ùëë)| ‚â§2ùêæ ùëùùõº
ùë†ÀÜ,ùëéÀÜ
ùëë
ùë†ÀÜ,ùëéÀÜ
ùëë
ùë†ÀÜ‚Ä≤
WeupperboundthedifferenceinrewardusingourLipschitzassumption:
ùúñ ùëÖ =max|ùëÖ ùëë(ùë†ÀÜ,ùëéÀÜ ùëë)‚àíùëÖ(ùë†ÀÜ,ùëéÀÜ ùëë)|
ùë†ÀÜ,ùëéÀÜ
ùëë
=max|ùëÖ(ùë†ÀÜ ùëë,ùëéÀÜ ùëë)‚àíùëÖ(ùë†ÀÜ,ùëéÀÜ ùëë)|
ùë†ÀÜ,ùëéÀÜ
ùëë
‚â§L R‚à•ùë†ÀÜ ùëë ‚àíùë†ÀÜ‚à•‚àû
=L Rùõº
TheboundholdsbyapplyingsimulationlemmatoùëÄ andùëÄ withùúñ andùúñ above:
ùê¥ÀÜ
ùëë
ùëë ùëÉ ùëÖ
‚à•ùëâ ùëÄùúã
ùê¥ÀÜ
ùëë
‚àíùëâ ùëÄùúã ùëë‚à•‚àû=‚à•ùëâ ùëÄùúã ‚àíùëâ ùëÄùúã ùëë‚à•‚àû ‚â§ L 1‚àíRùõº
ùõæ
+ (1ùõæùêæ ‚àíùëù ùõæùõº
)2
‚ñ°
I.1 CaseI
TheoremI.4. Supposethat0‚â§ùúÄ < 1‚àí1 ùõæ and0‚â§ùõø <1aretworealnumbers.LetMbeanycontinuousmeta-MDPwithinnerstochasticgame
ùê∫ = ‚ü®ùëÜ,ùê¥,ùëá inner,ùëÖ inner‚ü©.LetusdenoteùëÄ ùëë = ‚ü®ùëÜÀÜ ùëë,ùê¥ÀÜ ùëë,ùëá ùëë,ùëÖ ùëë,ùõæ‚ü©asdiscretisedversionofùëÄ (asdescribedinCaseI)usinggridsizeofùúÜ.There
existsinputsùëö=ùëö(cid:16) ùúÄ1, ùõø1(cid:17) andùúÄ1,satisfying
(cid:18) ‚àö (cid:19)ùëõ|ùëÜ||ùê¥|
(cid:169)
2 ùëõ|ùëÜ||ùê¥|
+1 (cid:170)
ùëö(cid:18)1 ,1(cid:19) =ùëÇÀú(cid:173)
(cid:173)
ùúÜ (cid:174)
(cid:174)
ùúÄ ùõø (cid:173) ùúÄ2(1‚àíùõæ)4 (cid:174)
(cid:173) (cid:174)
(cid:173) (cid:174)
(cid:171) (cid:172)
and ùúÄ1
1
=ùëÇ(cid:16) ùúÄ1(cid:17) ,suchthatifùëÖ-ùëÄùê¥ùëã isexecutedonùëÄwithinputsùëöandùúÄ1,thenthefollowingholds.LetAùë° denoteùëÖ-ùëÄùê¥ùëã‚Äôspolicyattimeùë°
andùë†ÀÜ
ùë°
denotethestateattimeùë°.Withprobabilityatleast1‚àíùõø,
ùëâ ùëÄ‚àó (ùë†ÀÜ ùë°)‚àíùëâ ùëÄAùë° (ùë†ÀÜ ùë°) ‚â§ùúÄ+ (1K ‚àíùúÜ
ùõæÀÜ)2
+ L 1‚àíRùõº
ùõæ
+ (1ùõæùêæ ‚àíùëù ùõæùõº
)2
istrueforallbut
(cid:18) ‚àö (cid:19)2ùëõ|ùëÜ||ùê¥| (cid:18) ‚àö (cid:19)|ùëÜ||ùê¥|
(cid:169)
2 ùëõ|ùëÜ||ùê¥|
+1
2 |ùëÜ||ùê¥|
+1 (cid:170)
ùëÇÀú(cid:173)
(cid:173)
ùúÜ ùúÜ (cid:174)
(cid:174)
(cid:173) ùúÄ3(1‚àíùõæ)6 (cid:174)
(cid:173) (cid:174)
(cid:173) (cid:174)
(cid:171) (cid:172)
timesteps.Proof. TheresultfollowsfromaddingTheoremG.2,I.2,andLemmaI.3. ‚ñ°
I.2 CaseII
TheoremI.5. Supposethat0‚â§ùúÄ < 1‚àí1 ùõæ and0‚â§ùõø <1aretworealnumbers.LetMbeanycontinuousmeta-MDPwithinnerstochasticgame
ùê∫ = ‚ü®ùëÜ,ùê¥,ùëá inner,ùëÖ inner‚ü©.LetusdenoteùëÄ ùëë = ‚ü®ùëÜÀÜ ùëë,ùê¥ÀÜ ùëë,ùëá ùëë,ùëÖ ùëë,ùõæ‚ü©asdiscretisedversionofùëÄ (asdescribedinCaseI)usinggridsizeofùúÜ.There
existsinputsùëö=ùëö(cid:16) ùúÄ1, ùõø1(cid:17) andùúÄ1,satisfying
ùëö(cid:18)1 ,1(cid:19)
=ùëÇÀú
(cid:32) (|ùëÜ||ùê¥|)ùëõ‚Ñé(cid:33)
ùúÄ ùõø ùúÄ2(1‚àíùõæ)4
and ùúÄ1
1
=ùëÇ(cid:16) ùúÄ1(cid:17) ,suchthatifùëÖ-ùëÄùê¥ùëã isexecutedonùëÄwithinputsùëöandùúÄ1,thenthefollowingholds.LetAùë° denoteùëÖ-ùëÄùê¥ùëã‚Äôspolicyattimeùë°
andùë†ÀÜ
ùë°
denotethestateattimeùë°.Withprobabilityatleast1‚àíùõø,
ùëâ ùëÄ‚àó (ùë†ÀÜ ùë°)‚àíùëâ ùëÄAùë° (ùë†ÀÜ ùë°) ‚â§ùúÄ+ (1K ‚àíùúÜ
ùõæÀÜ)2
+ L 1‚àíRùõº
ùõæ
+ 2(ùõæ 1ùêæ ‚àíùëù ùõæùõº
)2
istrueforallbut
(cid:18) ‚àö (cid:19)|ùëÜ||ùê¥|
(cid:169)(|ùëÜ||ùê¥|)2ùëõ‚Ñé 2 |ùëÜ||ùê¥| +1 (cid:170)
ùëÇÀú(cid:173)
(cid:173)
ùúÜ (cid:174)
(cid:174)
(cid:173) ùúÄ3(1‚àíùõæ)6 (cid:174)
(cid:173) (cid:174)
(cid:173) (cid:174)
(cid:171) (cid:172)
timesteps.
Proof. ThestepsaresimilartoCaseI. ‚ñ°J ExperimentDetails
J.1 MatchingPenniesTableSummary
Player1\Player2 Head Tail
Head (+1,-1) (-1,+1)
Tail (-1,+1) (+1,-1)
Table4:PayoffMatrixforMP
J.2 ImplementationDetails
TheopponentisastandardQ-learningagentwhoupdatestheQ-valuesateverymeta-stepandselectsanactionthatcorrespondstothe
maximumQ-valueatagivenstate.Toenablebetterempiricalperformance,themeta-agentusesBoltzmannsamplinginsteadofgreedy
samplingtosamplethenextactionfromtheQ-valuetable.Weuseadiscountfactorof0.8.Foreachrun,werunatotalof10√óùëö√ó|SÀÜ
|
R-FOSiterations,where10isourchosenhyper-parameter.