Efficient Stagewise Pretraining via Progressive Subnetworks
AbhishekPanigrahi*1I NikunjSaunshi*2 KaifengLyu1I SobhanMiryoosefi2 SashankReddi2 SatyenKale2
SanjivKumar2
Abstract languagemodelshavedemonstratedremarkableemergent
abilities that only begin to manifest at large scales (Wei
Recent developments in large language models
et al., 2022; Schaeffer et al., 2023). However, training
havesparkedinterestinefficientpretrainingmeth-
such large models is usually very slow and resource
ods. A recent effective paradigm is to perform
intensive(Brownetal.,2020;Touvronetal.,2023;Chowd-
stagewise training, where the size of the model
hery et al., 2022). This has sparked interest in efficient
isgraduallyincreasedoverthecourseoftraining
training of large models, necessitating the development
(e.g. gradual stacking (J. Reddi et al., 2023)).
ofnewalgorithmsandparadigmsforefficientpretraining.
Whiletheresourceandwall-timesavingsareap-
Traditionally, this was accomplished by designing better
pealing,ithaslimitations,particularlytheinabil-
optimizationalgorithms(e.g.,Liuetal.2023a;Chenetal.
itytoevaluatethefullmodelduringearlierstages,
2023; Gupta et al. 2018; Shazeer and Stern 2018) that
anddegradationinmodelqualityduetosmaller require fewer training steps to reduce the loss. Recently,
modelcapacityintheinitialstages. Inthiswork,
otherparadigmssuchasstagewisetraininghavegarnered
weproposeanalternativeframework,progressive
interest. These approaches train large models in stages,
subnetworktraining,thatmaintainsthefullmodel
starting with cheaper training in the early stages, which
throughouttraining,butonlytrainssubnetworks
canfindagoodinitializerforexpensivelaterstages. This
withinthemodelineachstep. Wefocusonasim-
powerful framework has shown to drastically reduce the
ple instantiation of this framework — Random
trainingiterationsandFLOPsneededtotrainlargemodels.
Path Training (RAPTR) — that only trains a
sub-path of layers in each step, progressively Progressivestacking(Gongetal.,2019)andgradualstack-
increasing the path lengths in stages. RAPTR ing (J. Reddi et al., 2023) are instantiations of stagewise
achieves better pre-training loss for BERT and training. Progressivestackingstartswithasmallnetwork
UL2 language models while requiring 20-33% anddoublesthenumberoflayersineachstagebystacking
fewerFLOPscomparedtostandardtraining,and the parameters of the previous stage onto itself, whereas
iscompetitiveorbetterthanotherefficienttrain- gradual stacking increases the network depth in a slower
ingmethods. Furthermore,RAPTRshowsbetter manner, until the desired depth is reached. While these
downstreamperformanceonUL2,improvingQA stackingapproacheshaveshowntobeeffectiveinreducing
tasksandSuperGLUEby1-5%comparedtostan- theFLOPsandwall-clocktime,thetrainingperformance
dard training and stacking. Finally, we provide isfoundtobesensitivetothe(stacking)schedules,thusre-
theoretical basis for RAPTR to justify (a) the quiringacarefulgridsearch. Furthermore,sincethemodel
increasingcomplexityofsubnetworksinstages, growsgradually,itisnotpossibletoassessthefullmodel
and(b)thestabilityinlossacrossstagetransitions performanceduringearlierstages(i.e.,itisnotananytime
duetoresidualconnectionsandlayernorm. algorithm). Additionally,usingsmallmodelwithwayfewer
parametersforpartofthetrainingcanhampermodelquality
(e.g. longtermmemory(Gevaetal.,2021)),especiallyfor
singleepochtrainingwhereeachdatasampleisseenonce
1.Introduction
during training. It is indeed observed in our experiments
Largenetworkbasedlanguagemodels(e.g. Transformers) (§4.2)thatthepretraininglossanddownstreamperformance
have revolutionized the field of NLP. Intriguingly, these maybeworsethanbaselinetrainingatequalFLOPs.
*Equalcontribution,IWorkdoneduringinternship 1Department In this paper, we propose a novel stagewise training
ofComputerScience,PrincetonUniversity2GoogleNewYork. approach called progressive subnetwork training that
Correspondenceto:AbhishekPanigrahi<ap34@princeton.edu>, addressessomeoftheaboveissues. Unlikestacking,where
NikunjSaunshi<nsaunshi@google.com>.
differentstagestrainmodelsofdifferentsizes,wemaintain
a common base model of interest and train subnetworks
Preprint.Donotdistribute.
1
4202
beF
8
]LC.sc[
1v31950.2042:viXraEfficientStagewisePretrainingviaProgressiveSubnetworks
of varying complexities within this model. Motivated
by the phenomenon that gradient-based training learns
functions of increasing complexity over time (Kalimeris
etal.,2019;Abbeetal.,2022),weprogressivelyincrease
thecomplexityofsubnetworksineachstagetoexplicitly
impose this simple-to-complex behavior. As a concrete
instantiationofthisframework,wechoosesubnetworksto
bepathscomprisedofasubsetoflayers,andineachstage
we perform forward and backward passes over a random
subset of layers. This can be achieved using a simple
recipe:(a)dropoutlayersrandomlywithcertainprobability
duringforwardandbackwardpasses,and(b)progressively
decreasethedropoutprobabilitytozeroovertimeaccording Figure1.PictorialdescriptionofstagewiseRAPTRwherethenum-
beroflayersbeingskippedprogressivelydecreasesoverstages.
toafixedschedule. WenamethisapproachRandomPath
Training(RAPTR). Ourapproachreducesthetotalnumber
of FLOPs in the initial stages, but unlike stacking, every
stacking, progressive layer dropping and RAPTR on
parameterofthefullmodelcontributestothetraininglossat
BERT(Devlinetal.,2018)andUL2languagemodels.On
allstages,anditallowsustotrackthefullmodelthroughout
BERT-Base,RAPTRdemonstratesnotableimprovements
training. Thissimplestrategyturnsouttobeveryeffective
inthepretraininglosswhencomparedtogradualstacking
fortrainingefficiencyandimprovingmodelquality.
at similar FLOPs, while being better than baseline
It is worth noting that the idea of dropping some layers trainingwith33%fewerFLOPs. ThebenefitsofRAPTR
duringtraininghasbeenexploredintheliteraturebutmainly are especially significant in the short horizon setting
as a regularization strategy or for improving inference (few epoch training). For UL2-1.6B language model
efficiency(Fanetal.,2019;Zhangetal.,2019;Liuetal., (Tay et al., 2022), RAPTR can match the pretraining
2023c). Theclosestworkfortrainingefficiencyisprogres- perplexity of baseline training with 20% fewer FLOPs.
sivelayerdropping(ZhangandHe,2020),whereFLOPs Interestingly, at the same perplexity we observe signif-
aresavedbyincreasinglydroppingmorelayersastraining icantimprovementsindownstreamevaluations,e.g. we
proceeds–exactlytheoppositeofprogressivesubnetworks. see3-5%absoluteincreasein1-shotexactmatchscores
In fact, a subsequent work (Kaddour et al., 2023) shows onTydiQA(Clarketal.,2020)andSQuADv2(Rajpurkar
that such a schedule can hurt pre-training performance etal.,2018)comparedtobaselineandgradualstacking.
after careful evaluations. In this paper, we confirm this
observationbyshowingtheoryandexperimentsthatthere
• Weprovideatheoreticalstudyforconditionsunderwhich
is a fundamental difference between this strategy and
RAPTR yields smooth transition in loss across stage
RAPTR: RAPTRcaneffectivelylearncomplexcorrelations
transitions (§5), by characterizing a stability condition
inthedatainstages,butdroppingmorelayerslaterduring
withrespecttodroppingoflayers. Illustrativeexamples
training can be detrimental to learning those complex
withlinearnetworksarepresentedtoshowhowlayernorm
correlations. Ourmaincontributionsareasfollows:
andresidualconnectionscanhelpwiththisstability.
• Weintroduceanovelstagewisetrainingcalledprogerssive
subnetworksin§2thatenablestrackingtheperformance 2.Progressivesubnetworktraining
ofthefinalmodelinallstages. Specificallyweexplore
RandomPathtraining(RAPTR)thattrainsapathcom- Notation. [n]denotestheset{1,2,...,n}. a
1:k
isusedto
prised of a random subset of layers, with the average denoteasequenceofkscalarsa ,...,a ∈R. xdenotes
1 k
lengthofpathprogressivelyincreasinginstages. a vector in Rd . x denotes a sequence of k vectors
1:k
x ,...,x ∈ Rd. WeuseX ∈ Rn×d todenoteamatrix
1 k
• Throughanalysisonpolynomialdatain§3,westudythe containingasequencex asrows. Weconsiderthesetting
1:n
roleofprogressivetrainingandshow(withexperiments wherethegoalistolearnanL-layersequence-to-sequence
and theory), that progressive RAPTR can effectively neuralnetwork. (e.g.,standardTransformernetwork).
learnhigherdegreecomponentsinastage-wisemanner,
whereas progressive layer dropping can be detrimental Definition2.1(L-layersequence-to-sequencemodel). Let
forhigherorderterms. Thisprovidesajustificationfor f 1,··· ,f L : RN×d → RN×d denoteasetofLsequence-
progressivelyincreasingcomplexityinstages. to-sequencefunctions. Then, F : RN×d ×RL → RN×d
takes an input sequence X ∈ RN×d, scalars α , and
1:L
• We conduct extensive experiments to compare gradual outputsasequenceY definedrecursivelywithintermediate
2EfficientStagewisePretrainingviaProgressiveSubnetworks
Algorithm1k-stageRAPTR randomsubsetoflayersfortheforwardpassandbypassthe
restofthelayersusingtheresidualconnections. Wecall
Require: Blocks f ,··· ,f , Schedule T ×
(p ,I
),totalsteθ p1 sT,datθ aL
distributionD.
1:k thisapproachRandomPathTraining(RAPTR).
1:k 1:k
Initializeθ 1,··· ,θ L,T k+1 =T. Moreconcretely,letpdenotetheprobabilityofrandomlyse-
fors=1→kdo lectingalayerandIdenoteasubsetoflayersthatarealways
fort=T s →T s+1do included,i.e. theyareneverbypassed. Wedefinearandom
Samplebatch(X,Y)∼D. subnewtorkbeforedescribingRAPTRinmoredetail.
Sampleζ ∼B(p)fori̸∈I ,setζ =1fori∈I .
i s i s
SetY(0) ←X
forj =1→Ldo
Y(j) ←Y(j−1) Definition2.2((p,I)-subnetwork). Letζ 1:LbeBernoulli
if ζ j: Y(j) ←Y(j)+ζ jf θj(Y(j−1)) samples,withζ i =1foralli∈I,whileζ i ∼B(p)forall
endfor i ̸∈ I. Thesetofalllayersforwhichζ i = 1representsa
ComputelosswithY(L)andtakeaoptimizerstep. randomsubnetworkwhilelayerswithζ
i
=0arebypassed.
endfor The output of the selected subnetwork on a sequence in-
endfor put X is equivalent to the output of the model given by
F(X,ζ )asdefinedinEquation(1).
1:L
outputsY(1),··· ,Y(L−1),Y(L)as
Y(ℓ) =Y(ℓ−1)+α ℓf ℓ(Y(ℓ−1))forall1≤ℓ≤L, Stagewise progressive RAPTR. The pseudo-code for
F(X;α)=Y(L), (1) RAPTRisprovidedinAlgorithm1.Atahighlevel,thetotal
training is split into k stages. Each stage s uses (p ,I )-
s s
whereY(0)denotesX,Y(ℓ)istheoutputofℓ-thlayer. 1 subnetwork between steps T s and T s+1. We denote this
schedule by T ×(p ,I ). Importantly, we progres-
1:k 1:k 1:k
Standard model training uses scalars α = 1 for i ∈ [L]. sively increase the random subnetwork selection pattern
i
For simplicity, we use F(X) to denote the output and acrossstagesi.e., theexpectedlengthoftheselectedran-
Y(1),Y(2),··· ,Y(L)astheintermediatelayeroutputsof dom subnetworks is increased progressively towards full
F understandardmodeltraining,unlessspecifiedotherwise. modeltraining. Thiscanbeachievedbyeitherincreasing
theprobabilityofincludingeachlayerinrandomsubnet-
Progressivesubnetworktrainingisastagewisetraining
workorfixingmorelayersinI orboth. Moreformally,we
·
frameworkthatconsistsoftwocrucialcomponents:
useschedulesT ×(p ,I )thatsatisfies: (a)p ≤p ,
1:k 1:k 1:k s s˜
and(b)I ⊆I forall1≤s≤s˜≤k.
1. Subnetwork selection: At each iteration, we select a s s˜
(possiblyrandom)subnetworkoftheL-layerneuralnet- Trainingefficiency. TrainingstepinRAPTRinvolvescom-
work. Theforwardandbackwardpassesarecomputed putingforwardandbackwardpassesonlyonrandomsubnet-
basedonthissubnetwork. work. Inexpectation,traininga(p,I)randomsubnetwork
willrequire |I|+(L−|I|)p FLOPsrelativetostandardtrain-
2. Progressive subnetwork sizes: The size of the sub- L
ingthatusesallLlayers. Byusingp ≪ 1formajorityof
networks being trained is progressively increased in a
training,RAPTRcanimprovetrainingefficiency.
stagewisemanner,startingfromsmallsubnetworksand
endingwiththefullnetworkinthefinalstage. Connection to progressive layer dropping (PLD). We
notethatRAPTRbearsresemblancewithprogressivelayer
dropping(ZhangandHe,2020),whereideaistodropmore
Notethatthisframeworkisquitegeneralsincesubnetworks
layersastrainingproceedsforFLOPsefficiency(see§A.1
couldbeselectedalonganydimensionofthemodel,e.g.,
fordetails). Infact,thisisinsharpcontrasttoprogressive
attention heads, subset of hidden neurons in MLP layers.
RAPTR,wherewedropfewerlayersastrainingproceeds.
Inthiswork,weprimarilyfocusondepth.
While this difference may seem superficial at first, it has
2.1.RandomPathTraining: RAPTR deepimplicationsfrombothconceptualandperformance
standpoints. In §3, we argue through theoretical and em-
Onesimplestrategytoselectasubnetworkisbychoosinga
piricalanalysisonpolynomialdata,thatthiscanhavevery
pathformedbyasubsetoflayerswithinthemodel. More
significantimpactonlearningcomplexcorrelationsindata.
concretely, given a network F to be trained, we pick a
IntheBERTandUL2experimentsin§4,wesimilarlyob-
1IfN =1,werepresenttheinputandtheintermediateoutputs servethatprogressivelayerdroppingperformssignificantly
intheirvectorrepresentation,x,{y(i)}L . worsethanRAPTR.
i=1
3EfficientStagewisePretrainingviaProgressiveSubnetworks
Evaluation Loss PLD RaPTr Baseline
PLD Degree Degree Degree
102 Baseline 101 1 6 1 6 1 6
RaPTr 2 7 2 7 2 7
3 8 3 8 3 8
101 100 4 9 4 9 4 9
5 10 5 10 5 10
100 101
102
101
0 50000100000150000200000250000300000350000400000 0 10 20 30 40 50 60 70 80 0 10 20 30 40 50 60 70 80 0 10 20 30 40 50 60 70 80
Steps Steps (x5000) Steps (x5000) Steps (x5000)
Figure2.Evaluationloss(left)andcomponenterror(3ontheright)onbasispolynomialsofdifferentdegreesfora20layerresidual
networktrainedwithdifferentmethods.Labelsaregeneratedfromacompositionofpolynomialsofdegrees1to10(Eq.2).Theschedules
forRAPTRandPLDareselectedtohave20%fewerFLOPscomparedtobaseline(phasetransitionsforRAPTRhavebeenmarkedwith
darkverticallines). (moredetailsin§A.2). Observations: (a)RAPTRreachessameevaluationlossofbaseline,whilePLDperforms
muchworse.(b)RAPTRlearnslowerordertermsfasterandpicksuphigherordertermsinthelaterstages.PLDisworseatcapturing
higherdegreetermsowingtoitsreducedexpressivitytowardstheend.
3.Illustration: RAPTR inpolynomialsetting • RAPTRiscompetitivetobaselineintermsofevaluation
loss;infact,RAPTRlearnseverycomponenteffectively.
Inthissection,wediscusstheimportanceoftheprogressive
Furthermore, RAPTR quicklylearnslowerorderterms
nature of training in RAPTR and sharply contrast it
andthenpicksuphigherordertermsduringlaterstages.
with PLD. While both these strategies improve training
efficiency, we ask an important question: what schedule • PLD,thatdropsmorelayersastrainingproceeds,failsto
of subnetwork size works better? We demonstrate the effectivelylearnhigherordercomplexcorrelationsand,
importance of progressively increasing the complexity ultimately,hurtsperformance.
of subnetworks in RAPTR (as opposed to progressively
decreasingitPLD)throughasimple,yetinstructive,setting:
The learning pattern of RAPTR is reminiscent of the im-
plicit bias of standard SGD training to learn functions of
learningneuralnetworksonpolynomialdata.Forsimplicity,
werestrictthediscussiontosequencesoflengthN =1. increasing complexity (Kalimeris et al., 2019; Xu et al.,
2019;Rahamanetal.,2019;Caoetal.,2019;Abbeetal.,
Polynomiallearningsetting. Supposexaresampledfrom 2022). RAPTRnaturallyimposesthissimple-to-complex
auniformdistributionover{±1}d. Theground-truthlabel behavior through size of subnetworks and also provides
isapolynomialf⋆ofdegreekdefinedas
trainingefficiencyintheprocess.
k m
(cid:88)(cid:88) (cid:89) Theoretical Analysis. To further illustrate our point, we
F⋆(x)= c x , (2)
l,j i characterize the behavior of RAPTR for a simple 2-layer
l=1j=1 i∈Sl,j
residualnetwork,whereeachblockconsistsofsingleneuron
withanon-linearsineactivation. Weconsidersimplelabel
wherec ∼N(0,1)andS arerandomsubsetsof[d]of √ √
l,j l,j functionF⋆(x)= 3 + 3x −x x forthisanalysis.
cardinalityℓ. Suchpolynomialshavebeenstudiedingreat 2 2 1 1 2
detail (Abbe et al., 2022; 2023) where the higher degree Lemma3.1(Informal,cfTheoremE.1). Forasmallenough
termscapturemorecomplexcorrelationsandareharderto learning rate, 2-phase RAPTR first learns lower degree
learn.Weconsidertrainingof20-layerresidualnetwork(He componentandthenthehigherdegreecomponent.
etal.,2016)F onsuchdatausingsquareloss: E(F(x)−
Inconstrast,onelayerisnotexpressiveenoughtorepresent
F⋆(x))2. Here each residual block consists of a single
thetruelabelfunction(E.2). Thus,progressivelydropping
4d-hiddendimensionMLPlayerwithReLUactivation.
morelayers(e.g.,PLD)willreduceitsexpressivity.
EmpiricalObservations. Weareinterestedinmeasuring
howwelleachmethodcapturesthehigherdegreeterms. To 4.Experiments
thisend,weestimatethecomponentofalearnedfunction
F ontoeachbasispolynomials. Thiscanbedonesimply Inthissection,wepresentcomprehensiveexperimentsthat
(cid:104) (cid:105)
using cˆ = E F(x)(cid:81) x , since basis compare RAPTR withstandardbaselinetraining,gradual
l,j x∼{±1}d i∈Sl,j i
stackingandprogressivelayerdropping(PLD)forBERT
polynomialsareorthogonalundertheuniformdistribution
andUL2. Weprimarilychoosestackingforcomparisons
of the boolean data (O’Donnell, 2014). For each degree
l ≤ k,wedefinetheerroras
(cid:80)m
j=
(cid:80)1( mcl,j c− 2cˆl,j)2
. InFigure2,
g sti av ce kn inth ge tr oec be en tt hfi en bd ei sn tg (s anin dK oa nd lyd )o eu fr fie ct iea nl. t( t2 r0 ai2 n3 i) ngth mat es th ho ow
d
j=1 l,j
weobservethefollowing: thatcanbeatbaselineoverdifferenttraininghorizons.
4
rorrE
tnenopmoCEfficientStagewisePretrainingviaProgressiveSubnetworks
Table1.Lossandfine-tuningresultsforBERT-Baseafter675K Table2.EqualFLOPscomparisonsforBERT-BASEandBERT-
steps and using equal peak learning rate (10−4). Key observa- LARGEwithextensivepeakLRtuning.ForBERT-BASEweuse
tions:(a)RAPTRachievessimilarlossasbaselineat75%reduced thebestperformingschedules6-8-10-12forRAPTRand6-9-12
FLOPs.(b)RAPTRachieves0.02betterevaluationlossthangrad- forstackingfromTable1.ForBERT-LARGE,weuse6-12-18-24
ualstackingforsameschedules.(c)RAPTRhasaslightedgeon forbothRAPTRandstacking.FLOPsdenotesthenumberofsteps
downstreamfine-tuningtask. involvedinbaselinetraining.Keyobservations:forBERT-BASE
Rel. Eval (a)RAPTRachievesbetterlosscomparedtobaselineatallFLOP
Method FLOPs Loss MNLI QNLI SST-2 Avg. measures,withlargerdifferencesforfewerFLOPs. (b)RAPTR
has0.02betterlossthangradualstackingatallFLOPlevels.(c)
Baseline 1.33 1.76 81.5 90.5 91.4 87.8
PLD 1. 1.8 81.7 89.0 90.8 87.2
ForBERT-LARGE,stackingandRAPTRarecompetitivetoeach
other at all FLOP counts. Both methods have better loss than
Stacking
baselineatlowerFLOPmeasures.
6-8-10-12 1 1.78 - - - - Model FLOPs Baseline Stacking RAPTR
6-9-12 1 1.77 80.9 89.8 91.1 87.3
75k 2.09 2.02 2.01
RAPTR BERT-BASE 170k 1.90 1.88 1.86
510k 1.74 1.75 1.73
6-8-10-12 1 1.75 82.1 89.8 92.4 88.1
6-9-12 1 1.75 82.3 89.2 91.0 87.5 62.5k 1.84 1.78 1.80
BERT-LARGE 140k 1.63 1.60 1.61
625k 1.40 1.41 1.41
Few additions to RAPTR: We make few additions to
RAPTRdescribedinSection2.1. WekeepI equaltothe
achievessimilarorbetterevaluationlosstobaselinetrain-
firstandlastlayer(I = {1,L})acrossallstages. Further-
ing,despitebaselinetrainingusing33%moreFLOPsthan
more,weobservedthatappropriatelyscalinglayeroutputs
RAPTR for BERT-BASE. Comparedtogradualstacking
fortheselectedlayersduringinitialstageshelpstrackthe
atsimilarFLOPs,RAPTRagainhasbettervalidationloss.
validation loss better (Fig. 6 in appendix); we defer the
Additionally,allthreemethodsexhibitsimilarperformance
detailstotheappendix(AppendixF)sincethefinaleffectof
in downstream fine-tuning tasks (see Table 1). However,
thisscalingwasminimal. Finally,followinggradualstack-
PLDperformssignificantlyworsecomparedtoothers.
ing(J.Reddietal.,2023),wetrainwithaconstantlearning
rate in all phases except in the final phase of full model ResultsatequalFLOPs.InspiredbyKaddouretal.(2023),
trainingwherewedecaythelearningrate. Thischangein wefurthercompareRAPTRandgradualstackingtobase-
learningrateprimarilyhelpedinBERTpre-training. line training by adjusting the number of training steps of
baselinetrainingtomatchitsFLOPstoRAPTR. InTable2,
Notations for RAPTR and stacking schedules. We
we compare these methods at three different FLOPs bud-
succinctlydenotethestageschedulesbyasetofsubnetwork
gets. For BERT-BASE, weobservethatatshorterFLOP
lengthsseparatedbyhyphens. Forgradualstacking,6-12-
experiments,RAPTRachievesmuchbettervalidationloss
18-24refersto4stageswitha6,12,18and24layermodel
comparedtobaselinetrainingandgradualstacking. This
trainingrespectively. ForRAPTRtrainingwitha24-layer
differencegetssmalleraswemovetolargerhorizonsettings.
model,6-12-18-24refersto4stages,withprobabilitypset
SimilarfindingsforBERT-LARGE.
ineachstagesuchthattheaveragelengthofsubnetworks
is6,12,18,and24respectively. Thelengthofeachstageis AblationswithRAPTRparameters: Weobserverobust-
chosenbasedonthereductioninFLOPstoachieveduring nessofthefinaltrainedmodeltodifferentRAPTRschedules
training(see§B).Unlessspecifiedotherwise,wemaintain (Tab. 6inappendix). Furthermore,weobservethatfixing
anequalnumberoftrainingstepsforeachstage. the first and last layers at all steps during training helps
RAPTR(Tab. 7inappendix).
4.1.ExperimentswithBERT
4.2.ExperimentswithUL2-1.6B
Experimentsetup.WepretrainBERTmodels(Devlinetal.,
2018)onWikipedia+BooksdatasetwithAdamWoptimizer ExperimentsonBERT(Section4.1)showthatRAPTRand
(Loshchilov and Hutter, 2019). For all the experiments stackingoutperformbaselinetrainingatequalFLOPcom-
withBERT-BASEandBERT-LARGE,thetotalnumberof parisonsforshorthorizonsettings, equivalentlywithfew
trainingstepsissetto675Kand1Msteps,respectively. We trainingepochs. Thispromptsustoconsiderwhethersimi-
reporttheevaluationlossandfine-tuningperformanceof lardisparitiesemergeinscenarioswhereonlyoneorafew
the trained models on 3 GLUE tasks (Wang et al., 2018). trainingepochsarefeasible,e.g. trainingbillion-parameter
Pleasesee§Aformoreexperimentaldetails. languagemodelsonlargetextcorpora.
Resultsatequaltrainingsteps. WeobservethatRAPTR We pretrain a 1.6B decoder-only UL2 model (Tay et al.,
5EfficientStagewisePretrainingviaProgressiveSubnetworks
Table3.Validationlossand1-shotdownstreamevalsonUL2-1.6B.Keyobservations: (a)RAPTRwith30kinitialtrainingimproves
performancebyatleast2−4%onTriviaQAandSquADv2. RAPTRisatleast5%betterthangradualstackingonvariousdownstream
tasks,(b)Comparedtobaseline,RAPTRis1−2%betteronalldownstreamtasksonaverage.Decayingthelearningratefromthestart
stillretainsthebenefitofRAPTR.(c)Stagewisetrainingachieves(1−2%)lowervarianceonTydiQAandSquADv2,implyingimplicit
benefitsofstagewisetraining.
Method Rel.FLOPs EvalLoss TriviaQA TydiQA SQuADv2 SuperGLUE Avg.Downstream
Baseline 1.2 2.06(0.01) 25.0(0.2) 34.4(3.1) 42.1(2.9) 60.0(0.4) 40.4
PLD 1 2.09(0.00) 21.3(0.3) 32.4(2.1) 40.2(0.9) 59.9(0.2) 38.5
12-16-20-24Stacking 1 2.08(0.00) 20.1(1.3) 28.6(2.4) 36.0(1.9) 60.4(0.9) 36.3
12-16-20-24RAPTR 1 2.08(—-) 22.2(—) 38.2(—) 40.6(—) 60.1(—) 40.3
(+30kinitialfull-modeltrain) 1 2.06(0.00) 25.8(0.2) 36.7(1.0) 44.1(0.5) 60.9(0.2) 41.9
(+UnmodifiedLR) 1 2.07(0.00) 23.7(0.4) 35.2(0.7) 43.8(0.2) 60.5(0.9) 40.8
Table4.WeextensivelycomparealltrainedmodelsfromTable3onmultipledownstreamtasksandfew-shotin-contextsettings. We
followpromptdesignfromChowdheryetal.(2022)ineachsetting.Fortasksmarkedwith†and⋆,wereportExactmatchandF1scores
respectively.Fortherest,weuseaccuracy.Thetaskshavebeensub-dividedinto4majorgroups,memorizationQA,QAwithcontext,
completion,andSuperGLUE.Onaverage,RAPTRis1-2%betterthanbaselineandstacking.
1-shot 5-shot
Baseline RAPTR Stacking RAPTR Baseline RAPTR Stacking RAPTR
(+30kinitialfull-modeltrain) (+30kinitialfull-modeltrain)
TriviaQA† 25.0(0.2) 22.2(—) 20.1(1.3) 25.8(0.2) 26.5(1.1) 23.4(—) 21.1(1.3) 25.1(0.5)
WebQA† 6.4(0.4) 5.9(—) 5.8(0.6) 7.6(0.5) 10.6(0.4) 8.2(—) 9.2(0.5) 11.2(0.2)
NaturalQA† 4.2(0.5) 3.6(—) 3.4(0.4) 4.4(0.1) 5.7(0.1) 4.6(—) 4.6(0.3) 6.0(0.3)
TydiQA† 34.4(3.1) 38.2(—) 28.6(2.7) 36.7(1.0) - - - -
SQuaDv2† 42.1(2.0) 40.6(—) 36.0(0.9) 44.1(0.5) 43.2(3.0) 40.0(—) 36.2(2.6) 44.5(1.3)
DROP† 21.4(0.8) 23.5(—) 19.5(0.6) 23.0(0.4) - - - -
CoQA⋆ 49.2(0.7) 51.6(—) 43.9(0.8) 52.4(0.7) - - - -
QuAC⋆ 18.1(0.5) 18.8(—) 16.8(0.6) 18.1(0.4) - - - -
LAMBADA 13.7(2.9) 12.6(—) 12.0(1.1) 18.7(3.1) 30.5(3.4) 32.0(—) 29.5(2.1) 38.7(2.2)
Storycloze 72.9(0.4) 73.5(—) 71.0(0.4) 73.3(0.4) 75.1(0.1) 75.5(—) 72.6(0.8) 75.3(0.6)
Hellaswag 58.3(0.2) 57.5(—) 56.1(0.1) 58.5(0.3) 58.3(0.2) 57.3(—) 56.1(0.1) 58.4(0.3)
SuperGLUE 60.0(0.4) 60.1(—) 60.4(0.9) 60.9(0.2) 60.7(0.3) 60.6(—) 58.8(0.5) 62.1(0.2)
Average 33.8 34.1 31.1 35.3 38.8 37.7 36.0 40.2
2022)with24layers. WeuseAdafactor(ShazeerandStern, 30Kstepsfromthefinalphase.
2018) optimizer and train with a batch size 512 for 400k
Results. At 20% FLOPs benefit, we find in Table 3 that
stepsonamixtureofArxiv,C4(Raffeletal.,2020),Github,
RAPTRwith30Kstepsinitialfullmodeltrainingachieves
and Wikipedia (Foundation) datasets, with mixing ratios
similarevaluationperplexitiestobaselinetraining. Stacking
9%,57%,17%,17%respectively.Thisroughlycorresponds
ontheotherhandisslightlyworseatthesameFLOPsas
to0.8epochsofC4. Table3reportsthevalidationlossand
RAPTR. AdditionallyRAPTRhasthehighestdownstream
downstream1-shotperformancewithoutfine-tuning. Please
task average among all comparisons. We find a better
seeAppendixAformoreexperimentaldetails.
schedule for stacking and report results in the Appendix,
SchedulesforRAPTRandgradualstacking. InTable3, but overall RAPTR continues to be better and is more
wereportforaschedulewith4stages,denotedby12-16- robusttothescheduleselection(seeTable5).
20-24. Thelengthofeachstagehasbeenadjustedappro-
Wealsofindthatincluding30Kstepsoffull-modeltraining
priately to use 17% lower FLOPs (equivalent to training
helpswithallmetrics, despitenotchangingFLOPs. The
withaveragesubnetworklengthof20outof24)compared
abilitytoincludefullmodeltrainingisananotherbenefit
tothebaselinetraining(seeAppendixB).Weevaluatean
ofsubnetworktrainingoverstacking. Intriguingly,wefind
alternativesubnetworkconfigurationthatinvolvesaninitial
that despite having similar validation perplexity, RAPTR
full-modeltrainingphasefor30Kstepsbeforetransitioning
has much better downstream metrics compared to both
tothe RAPTR schedule. ForparityinFLOPs,wereduce
6EfficientStagewisePretrainingviaProgressiveSubnetworks
baseline and stacking (see Table 3). The improvements acrossstagetransitions. Oneintriguingobservationabout
areparticularlylargeforTydiQA(3.8%)SQuADv2(2.0%). RAPTR isthatthetraininglossdecreasesquitesmoothly
Furthemore RAPTR performs 2% better on average in whentransitioningbetweentwostages(seeFigure4inap-
extensive downstream evaluations (refer to Table 4 in pendix).Thisisapriorisurprising,becausethediscreteshift
appendix). ThisperhapssuggestsRAPTRhasadesirable inhowthemodelistrainedinthetwostagescould,inprinci-
implicitbias,aphenomenonthathasbeenrecentlystudied ple,leadtoanarbitrarilylargespikeinthelossatthetransi-
forotherpretrainingsettings(Saunshietal.,2022;Liuetal., tion. Thisphenomenonsuggeststhatthemodeltrainedwith
2023b). Another notable observation is that RAPTR has smallersubnetworksinanearlierstageisagoodinitializer
lower variance on all metrics and is thus more stable to forthenextstage. Inthissectionwestudycertaingeneral
trainingruns. Webelievetheseinductivebiasbenefitsof conditionsunderwhichtrainingshouldproceedsmoothly
RAPTRdeservesfurtherexploration. acrossstages,byintroducingtheideaoflayerstability. The
stabilityanalysis,wheninstantiatedforlinearnetworkshigh-
4.3.Learningsandtakeaways lightstheimportanceoftwopopulararchitecturalchoices
in Transformer models – residual connections and layer
We summarize the key takeaways and recommendations
normalization. Wenowdefinethisideaoflayerstability.
fromourexperimentalfindingsonBERTandUL2below.
OverallRAPTRcanspeeduptrainingby20-33%andalso 5.1.Layerstability
getbetterdownstreamperformance,withoutneedingexten- Inthissection,werestrictourdiscussiontosequencesof
sivetuningofthestageschedules. lengthN = 1;generalizationtoN > 1isfairlyeasy. We
representL:Rd →Rasthetraininglossofthebackbone
Schedule selection. Overall we find RAPTR to be fairly
F. For a transformer, the output of the backbone F is
robust to schedule selections (see Table 6). For practical
passed through a classification head H : Rd → RV that
purposes,werecommendtorestrictthenumberofRAPTR
returns the logits for the cross entropy loss. A common
phases to 4. In the initial phase of the RAPTR, random
practice is to include a layer normalization layer that
subnetworks of size L/2 are selected, with the size of
normalizestheoutputofF beforeprojectingtologits. This
thesesubnetworksincrementallyincreasingateachstage
makes the loss L scale-invariant to its input (Ioffe and
transitionuntilthefullmodelistrainedinthefinalphase.
Szegedy,2015;Baetal.,2016),whichgivesusafavorable
ForUL2training,wefindthataninitialphaseoffullmodel
stabilitycondition,asanyperturbationintheoutputofF
traininghelpsRAPTR. Weattributethistothefastdropin
resultsinalossperturbationthatscaleswithnormofthe
lossthatlargemodelscanachieveinitially,butthisdeserves
outputperturbationrelativetothenormoftheoutputofF.
more exploration. Furthermore, fixing the first and last
Thus,wemakethefollowingassumption.
layerfixedduringtraininghelps.
Assumption5.1(Relative-Lipschitzness). Thereexistsa
Learningrateschedule. Keepingthelearningrateconstant
constantµ >0suchthatforanyinputx∈Rd,labelyand
inallphasesexceptthefinalphase,anddecayingthelearn- L
perturbationη ∈Rd,LsatisfiesL(x+η,y)−L(x,y)≤
ingrateinthefullmodeltrainingphasehelpsRAPTRon
µ (∥η∥ /∥x∥ ).
BERT.However,forUL2,learningratedecayfromstartis L 2 2
alsoeffectiveandoutperformsbaselinetraining.
To understand loss changes across stage boundaries of
Few-epochsettings. RAPTR andotherefficienttraining RAPTR, we consider two stage RAPTR: the first stage
methods especially help in few-epoch/pass settings (e.g. trainswithsubnetworkoflengthL−1bydroppingasingle
LLMtraining),butthegapdiminisheswithmoreepochs, layeratrandom,andthesecondstagetrainstheentiremodel.
whichwasalsoreportedin(Kaddouretal.,2023). SupposeL andL denotetheeffectivelossfunctionsbeing
1 2
minimized in the two stages. The aim is to show that F
Inductivebias. WeobserveinductivebenefitsofRAPTR
learnedinthefirststagebyminimizingL shouldalsoresult
overbaselinetraining,whereitachievesbetterdownstream 1
insmallvalueforL .
performanceatsameevaluationlossandlowervariance.We 2
believethisdeservesmoreexploration. Definition5.2. LetF denotethesubnetworkafterskip-
−ℓ
pinglayerℓ. Thestabilityofnetworkoutputwithrespectto
5.TheoreticalAnalysisusinglayerstability droppinglayerℓisdefinedasΨ ℓ(x)=∥F −ℓ(x)−F(x)∥ 2.
Given the empirical success of RAPTR, we aim to get a Our result in Theorem 5.3 shows that L (F)−L (F) is
2 1
deeperunderstandingofsomeofitspropertiesinthissec- smallifthestabilityscalesslowerthantheoutputnorm.
tion.While§3studiesthebenefitofprogressivelyincreasing
Theorem5.3(Informal,cfTheoremD.1). Underassump-
complexityintrainingratherthandecreasing,inthissection
tion5.1oflossL,|L (F)−L (F)|isupperboundedby
westudyanorthogonalquestionofthestabilityoftraining 2 1
(µ /L)E ((cid:80)L Ψ (x))/∥F(x)∥ .
L x∈D ℓ=1 ℓ 2
7EfficientStagewisePretrainingviaProgressiveSubnetworks
||y()||/||y(L)|| / 1 (F) (F )
1.4 1.8 1.0
Best Fit: 0.94(/L)1.0 Best Fit:1.16(1/)0.12
1.4 1.5
1.0
1.0 2.0
0.6
0.6
2.5
Mean gap
0.2 0.2
3 6 9 12 15 18 21 24 3 6 9 12 15 18 21 3 6 9 12 15 18 21
Layer index ( ) Layer index ( ) Layer index ( )
Figure3.StabilitystudyonBERT-LARGEtrainedfor50kstepsbyRAPTRwithsubnetworksoflengthL−1.Lefttoright:Behaviorof
(a)normsofintermediateactivationsy(ℓ),(b)Ψ /Ψ (Definition5.2),and(c)LossgapbetweendifferentrandomsubnetworkF and
ℓ 1 −ℓ
modelF,givenbyL(F)− 1 (cid:80)L L(F ).Keyobservations:(a)Normsoftheintermediateactivationsgrowlinearlywithℓ,(b)Ψ
L ℓ=1 −ℓ ℓ
changesslowlywithℓas(L)0.12,suggestingaworse-caseboundofO(L−0.88)onL(F)− 1 (cid:80)L L(F )basedonTheorem5.3(c)
ℓ L ℓ=1 −ℓ
Interestingly,L(F)≤ 1 (cid:80)L L(F ),evenwhenmodelistrainedwithL−1randomsubnetworks.
L ℓ=1 −ℓ
(cid:13) (cid:13)
101
Train Loss Eval Loss connection; so yℓ = W ℓy(ℓ−1)/(cid:13)y(ℓ−1)(cid:13) 2. For this set-
Train 6×100 Eval ting,thefollowingresultshowsthescaleofΨ ℓandoutput
6×100 norm.
4×100
4×100 Lemma 5.4. At random initialization where W ∼
3×100 3×100 N(0,d−1/2I),forarandomlysampledx∈Sd−1,wei
have
2×100 2×100
20000 40000 60000 80000 100000 20000 40000 60000 80000 100000
Training steps Training steps
(a) With residual connection & layernorm, Ψ (x) =
√ ℓ
(cid:112)
O( L/ℓ) & ∥F(x)∥ = Ω( L). Then the gap in
2 √
lossesbetweenstagesisO(1/ L).
Figure4.TrainandEvaluationLossbehaviorforaBERT-BASE
modeltrainedwithRAPTRfor100ksteps.Wehave4stageswith
(b) Without residual connection, Ψ (x) = Ω(1) &
ℓ
6-8-10-12schedule(seeSection4fordetails). Theboundaries
∥F(x)∥ = O(1). Thus the gap in losses between
areat25k,50k,and75k.Keyobservation:themodel’strainand 2
stagesisΩ(1).
evaluationlosschangesmoothlyacrossstagetransitions,indicating
stabilityofthemodeltoRAPTR. (c) Without layernorm, we have Ψ (x) = Ω(2(L−1)/2)
ℓ
and ∥F(x)∥ = O(2L/2) . Thus the gap in losses
2
betweenstagesisΩ(1).
The proof follows by observing that L (F) =
1
1 (cid:80) L(F ) and L (F) = L(F). Thus, the losses In the Appendix, we show similar results for perfectly
L ℓ∈[L] −ℓ 2 aligned layers, where layer parameters are equal to A
arecloseifF iscloseenoughtoF forarandomℓ. So,
−ℓ
(Lemma D.3). These examples capture the cases of per-
therelativestabilityofthenetworkdeterminesthesuccess
fectlycorrelatedanduncorrelatedlayers. Wecanconsider
oftheapproach. InFig. 3,wereportthestabilityofBERT-
evenmoregeneralscenarios,wherethelayersparameters
LARGE duringtraining. Interestingly,wefindthatL 2(F)
areexpressedascombinationofaGaussianandashared
isbelowL (F),evenwhenwetrainwithsubnetworksof
1
matrix(Fig. 5). Werunsimulationsand observethatfor
lengthL−1. However,thequestionstillremains,whenisa
each τ, Ψ = O(L/ℓ)0.5 , while the output norm grows
networkstableacrossthestagesofRAPTR. Forsimplicity, ℓ
fasterasΩ(L0.5). Then,thegapbetweenaL−1random
we consider the special case of linear residual networks.
subnetworkandthefullmodelscalesasO(L−0.4).
We show that layer normalization layers and the residual
connectionhelpsmaintainthestabilityofRAPTR.
6.Relatedworks
5.2.Illustrativeexample: linearnetworks
Theliteratureontrainingdeepnetworksisvast; thus, we
WepresentamoreconcreteinstantiationofTheorem5.3for mainlyfocusonthemostrelevantworks.
residualnetworkwherethelayersf arelinearwithparam-
1:L Stagewiseefficientpretraining. Stackinghasbeenfurther
etersW . Thelayeroutputisyℓ = y(ℓ−1)+W y(ℓ−1)
1:L (cid:13) (cid:13) ℓ extended to other dimensions (e.g. width) by (Gu et al.,
or yℓ = y(ℓ−1) + W ℓy(ℓ−1)/(cid:13)y(ℓ−1)(cid:13)
2
depending on
2020), (Shenetal.,2022), (Wangetal.,2023), and(Ges-
whether layernorm 2 is enabled or not respectively . We
mundoandMaile,2023)intheformofgrowthoperators
alsostudyanothersettingwithlayernormbutnoresidual
ensuringthelossispreservedwhilegrowingthenetwork.
2Forsimplicity,weonlyfocusonnormalization,andignorethe Whilewemainlyfocusedonmodeldepthexpansion,other
√
meanandanadditional dcomponent,whichcanbeintegrated growth operators also have an analog in our subnetwork
withadditionalcomputation. trainingframeworkandareleftforfuturework.
8
201×EfficientStagewisePretrainingviaProgressiveSubnetworks
1000
Inte =r 0m .0,e Fd
it
ia t 1e
.2
3no 0.4r 6ms ||y()||2
=0. 0, Fit 2.72(L/)0.46
1
L
=L
1
(x)/||F(x)||2
68 00 00 = = = =0 0 0 1. . . .72 5 055 ,
,
,, F
F
Fi iF t tii tt 0
1
10 . .9 1..8 05 44 30 0. .0 09 9. .4 58 98 5 11 00 12 = = = =0 0 0 1. . . .72 5 055 ,
,
,, F
F
Fi iF t tii tt 2
0
23 . .7 9..2 4 3 94 ( 6 (L L( (/L /L/ /) )0) ).0 0 05. . .46 4 03 9 000 ... 345 = = = =0 0 0 0. . . .720 5 55, , ,,F F Fi iFt t ii tt 1 1 11. .2 1 .. 227 9 L3L LL 00 0 .. .0 94 9. 83 694
400 0.2 =1.0, Fit 1.32L 1.03
200 0.1
100
0 0.0
0 200 400 600 800 1000 0 200 400 600 800 1000 101 102 103
Layer Index Layer Index Number of layers (L)
Figure5.Behavioronalinearresidualnetworkwithnormalizationlayerswith100randomsamplesfromSd−1,anddimensiond=100.
√ √
Theparametersofeachlayerℓisrepresentedas τA+ 1−τG(ℓ) forasharedmatrixA ∈ Rd×d with∥A∥ ≤ 1andG(ℓ) ∼
(cid:16) (cid:17) 2
N 0,d−1/2I .Lefttoright:Behaviorof(a)thenormsofintermediateactivationy(ℓ)withindexℓ,(b)Ψ (Definition5.2)foreach
ℓ
stackoflayersF ,and(c) 1 (cid:80)L Ψ (x)/∥F(x)∥ thatappearsinourboundsinTheorem5.3.
ℓ:L L ℓ=1 ℓ 2
Stochastic depth. RAPTR bears some similarity to thelearnablescalesundervariousconstraintsforfastertrain-
stochastic depth —structured dropout that drops layers ing,whichisverydifferentfromtheroleofscalinginour
at random with fixed probability during training to train algorithm.
extremely deep residual networks without increasing the
trainingcost(Huangetal.,2016). Akeydistinctionisthat
theprobabilityofdroppinglayersisfixedduringtraining
and can thus be viewed as a regularization (Pham et al., Earlyexitforefficientinference Alotofrecentworks
2019;Steineretal.,2021;Tolstikhinetal.,2021;Liuetal., havefocusedonimprovinginferenceefficiencyforlargelan-
2023c),whichisdifferentfromtheroledropoutplaysinour guagemodels. (Leietal.,2023;Tayetal.,2022;DelCorro
work. Additionally(Fanetal.,2019)usefixedstochastic etal.,2023;Xinetal.,2020;Zhouetal.,2020;Houetal.,
depth to select lower depth subnetworks for inference 2020). However, none of these works focus on efficient
efficiencyratherthantrainingefficiency. fine-tuning. (Leietal.,2023)modifiedpre-trainingbysub-
stitutingresource-intensiveMLPcomputationswithstraight-
Subnetwork training. Training of random paths and
forwardclassifiersforapredeterminedfractionoftokens
subnetworkshasbeenusedinothercontexts,e.g. parameter
within the sequence. It’s worth noting that their primary
efficient fine-tuning (Houlsby et al., 2019; Pfeiffer et al.,
focuswasonacceleratinginferencetimeratherthanenhanc-
2021;Huetal.,2022;Liuetal.,2022)toreducememory
ingtheefficiencyofthepre-trainingprocess.
footprint, distributed and federated training (Dun et al.,
2022)usingmultiplemachines, andincrementallearning
7.ConclusionandFutureWork
(Jathushanetal.,2019)toavoidforgettingincontinuallearn-
ing. Theseideasarenotmotivatedbyorusedforreducing
This work proposes a stagewise training framework of
FLOPsduringpretraining,tothebestofourknowledge.
progressive subnetworks for efficient pretraining, and
Residualnetworksasensembles. Trainingwithrandom evaluatesanaturalinstantiationofthisframework(RAPTR)
pathscanbeweaklyviewedasensemblesoverpaths,and basedontrainingrandompaths/subsetsoflayers. Overall
theconnectionbetweenResNets(Heetal.,2016)anden- RAPTRyieldsbetterqualitylanguagemodelsthanbaseline
semblesofshallownetworkswasfirstpointoutinVeitetal. training,whilereducingthetotalFLOPsbyatleast20-30%.
(2016)forvisionmodels. (Dongetal.,2021)showedthe At the same speedup, it is also better than prior layer
samephenomenonforself-attentionmodels,wherelonger droppingandstackingbasedapproaches. Thereareother
paths lead to rank-collapse in the self-attention module. stackingapproacheswithdifferentgrowthfunctionsinthe
(Changetal.,2023)studyvisiontransformers(Dosovitskiy literature(see§6forreferences),andexploringanalogsof
etal.,2020)asacascadeofmultiplepathsandproposeprun- subnetwork training for these growth functions could be
ingandself-distillationtoremovelongpathsandimprove a fruitful direction. From theory perspective, the current
performance. Alltheseworksmainlyprovideanovelper- analysisprovidesinsightsintowhythelossisstableatstage
spectiveorinferenceefficiencybutdonotfocusontraining boundaries,butitdoesnotexplainthestrongerempirical
efficiency. findingthatthelossdecreasesattransitions. Additionally,
there is evidence of certain desirable inductive biases of
Learnablescalingofresidualblocks. (Bachlechneretal.,
RAPTRstyletraining,e.g.,betterdownstreamperformance,
2021), (Zhang et al., 2019), (Touvron et al., 2021) con-
smallervariance,whicharenotexplainedbyouranalysis.
siderlearnablescalesontheoutputoftheresidualblocks.
Abetterunderstandingofthesephenomenacouldinspire
Theseworksaimtounderstandfavorableinitializationof
otherefficienttrainingalgorithms.
9EfficientStagewisePretrainingviaProgressiveSubnetworks
BroaderImpact YuanCao,ZhiyingFang,YueWu,Ding-XuanZhou,and
QuanquanGu. Towardsunderstandingthespectralbias
TrainingLLMsdemandsextensivecomputationalresources
ofdeeplearning. arXivpreprintarXiv:1912.01198,2019.
and infrastructure. Our paper proposes an algorithm that
aimstoacceleratetheirpre-training. Acceleratingthetrain- Shuning Chang, Pichao Wang, Hao Luo, Fan Wang, and
ingoflargelanguagemodels(LLMs)cansignificantlyre- MikeZhengShou.Revisitingvisiontransformerfromthe
ducetheirenvironmentalimpactbyloweringenergycon- viewofpathensemble. arXivpreprintarXiv:2308.06548,
sumptionandminimizingcarbonfootprints. Optimizations 2023.
inalgorithmsandinfrastructurewillleadtomoreefficient
use of computational resources and will promote sustain- Xiangning Chen, Chen Liang, Da Huang, Esteban Real,
abilityofthefieldofAI. Kaiyuan Wang, Yao Liu, Hieu Pham, Xuanyi Dong,
Thang Luong, Cho-Jui Hsieh, et al. Symbolic dis-
Furthermore,weobservebenefitsofstructuredpre-training
covery of optimization algorithms. arXiv preprint
(simple-to-complex)ofthesemodelsondownstreamtasks.
arXiv:2302.06675,2023.
Hopefully,theseideascanleadtobetterpre-trainingstrate-
giesforLLMs.
EunsolChoi,HeHe,MohitIyyer,MarkYatskar,Wen-tau
Yih, Yejin Choi, Percy Liang, and Luke Zettlemoyer.
References Quac: Question answering in context. arXiv preprint
arXiv:1808.07036,2018.
Emmanuel Abbe, Enric Boix Adsera, and Theodor Misi-
akiewicz. The merged-staircase property: a necessary
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
andnearlysufficientconditionforsgdlearningofsparse
Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul
functionsontwo-layerneuralnetworks. InConference
Barham,HyungWonChung,CharlesSutton,Sebastian
onLearningTheory,pages4782–4887.PMLR,2022.
Gehrmann,etal. Palm: Scalinglanguagemodelingwith
pathways. arXivpreprintarXiv:2204.02311,2022.
Emmanuel Abbe, Enric Boix Adsera, and Theodor Misi-
akiewicz.Sgdlearningonneuralnetworks:leapcomplex- JonathanHClark,EunsolChoi,MichaelCollins,DanGar-
ityandsaddle-to-saddledynamics.InTheThirtySixthAn- rette,TomKwiatkowski,VitalyNikolaev,andJennimaria
nualConferenceonLearningTheory,pages2552–2623. Palomaki.Tydiqa:Abenchmarkforinformation-seeking
PMLR,2023. questionansweringintypologicallydiverselanguages.
TransactionsoftheAssociationforComputationalLin-
ZeyuanAllen-Zhu,YuanzhiLi,andZhaoSong. Aconver-
guistics,8:454–470,2020.
gencetheoryfordeeplearningviaover-parameterization.
InInternationalconferenceonmachinelearning,pages Luciano Del Corro, Allie Del Giorno, Sahaj Agarwal,
242–252.PMLR,2019. BinYu,AhmedAwadallah,andSubhabrataMukherjee.
Skipdecode: Autoregressiveskipdecodingwithbatching
JimmyLeiBa,JamieRyanKiros,andGeoffreyEHinton. and caching for efficient llm inference. arXiv preprint
Layernormalization. arXivpreprintarXiv:1607.06450, arXiv:2307.02628,2023.
2016.
JacobDevlin,Ming-WeiChang,KentonLee,andKristina
ThomasBachlechner,BodhisattwaPrasadMajumder,Henry Toutanova. Bert: Pre-trainingofdeepbidirectionaltrans-
Mao,GaryCottrell,andJulianMcAuley. Rezeroisall formers for language understanding. arXiv preprint
youneed:Fastconvergenceatlargedepth.InUncertainty arXiv:1810.04805,2018.
inArtificialIntelligence,pages1352–1361.PMLR,2021.
YiheDong,Jean-BaptisteCordonnier,andAndreasLoukas.
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Attentionisnotallyouneed: Pureattentionlosesrank
Liang. Semantic parsing on freebase from question- doublyexponentiallywithdepth. InInternationalCon-
answer pairs. In Proceedings of the 2013 conference ferenceonMachineLearning,pages2793–2803.PMLR,
on empirical methods in natural language processing, 2021.
pages1533–1544,2013.
AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,
TomBrown,BenjaminMann,NickRyder,MelanieSubbiah, DirkWeissenborn,XiaohuaZhai,ThomasUnterthiner,
JaredDKaplan,PrafullaDhariwal,ArvindNeelakantan, MostafaDehghani,MatthiasMinderer,GeorgHeigold,
PranavShyam,GirishSastry,AmandaAskell,etal. Lan- Sylvain Gelly, et al. An image is worth 16x16 words:
guagemodelsarefew-shotlearners. Advancesinneural Transformers for image recognition at scale. arXiv
informationprocessingsystems,33:1877–1901,2020. preprintarXiv:2010.11929,2020.
10EfficientStagewisePretrainingviaProgressiveSubnetworks
Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel NeilHoulsby,AndreiGiurgiu,StanislawJastrzebski,Bruna
Stanovsky, Sameer Singh, and Matt Gardner. Drop: Morrone, Quentin De Laroussilhe, Andrea Gesmundo,
A reading comprehension benchmark requiring dis- MonaAttariyan,andSylvainGelly. Parameter-efficient
crete reasoning over paragraphs. arXiv preprint transferlearningfornlp. InInternationalConferenceon
arXiv:1903.00161,2019. MachineLearning,pages2790–2799.PMLR,2019.
Chen Dun, Cameron R Wolfe, Christopher M Jermaine, Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-
andAnastasiosKyrillidis. Resist: Layer-wisedecomposi- Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu
tionofresnetsfordistributedtraining. InUncertaintyin Chen. LoRA: Low-rank adaptation of large language
ArtificialIntelligence,pages610–620.PMLR,2022. models. InInternationalConferenceonLearningRep-
resentations, 2022. URL https://openreview.
AngelaFan,EdouardGrave,andArmandJoulin. Reducing net/forum?id=nZeVKeeFYf9.
transformer depth on demand with structured dropout.
arXivpreprintarXiv:1909.11556,2019. Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kil-
ianQWeinberger. Deepnetworkswithstochasticdepth.
Wikimedia Foundation. Wikimedia downloads. URL InComputerVision–ECCV2016: 14thEuropeanConfer-
https://dumps.wikimedia.org. ence,Amsterdam,TheNetherlands,October11–14,2016,
Proceedings,PartIV14,pages646–661.Springer,2016.
AndreaGesmundoandKaitlinMaile.Composablefunction-
preserving expansions for transformer architectures. SergeyIoffeandChristianSzegedy. Batchnormalization:
arXivpreprintarXiv:2308.06103,2023. Acceleratingdeepnetworktrainingbyreducinginternal
covariateshift. InFrancisBachandDavidBlei,editors,
Mor Geva, Roei Schuster, Jonathan Berant, and Omer Proceedings of the 32nd International Conference on
Levy. Transformer feed-forward layers are key-value MachineLearning,volume37ofProceedingsofMachine
memories. In Proceedings of the 2021 Conference on Learning Research, pages 448–456, Lille, France, 07–
Empirical Methods in Natural Language Processing, 09Jul2015.PMLR. URLhttps://proceedings.
pages 5484–5495, Online and Punta Cana, Dominican mlr.press/v37/ioffe15.html.
Republic, November 2021. Association for Computa-
tionalLinguistics. doi: 10.18653/v1/2021.emnlp-main. Sashank J. Reddi, Sobhan Miryoosefi, Stefani Karp,
446. URL https://aclanthology.org/2021. Shankar Krishnan, Satyen Kale, Seungyeon Kim, and
emnlp-main.446. Sanjiv Kumar. Efficient training of language models
using few-shot learning. In Andreas Krause, Emma
LinyuanGong,DiHe,ZhuohanLi,TaoQin,LiweiWang, Brunskill,KyunghyunCho,BarbaraEngelhardt,Sivan
and Tieyan Liu. Efficient training of bert by progres- Sabato, and Jonathan Scarlett, editors, Proceedings of
sivelystacking. InInternationalconferenceonmachine the 40th International Conference on Machine Learn-
learning,pages2337–2346.PMLR,2019. ing, volume 202 of Proceedings of Machine Learn-
ing Research, pages 14553–14568. PMLR, 23–29 Jul
XiaotaoGu,LiyuanLiu,HongkunYu,JingLi,ChenChen,
2023.URLhttps://proceedings.mlr.press/
and Jiawei Han. On the transformer growth for pro-
v202/j-reddi23a.html.
gressiveberttraining. arXivpreprintarXiv:2010.12562,
2020. Rajasegaran Jathushan, Hayat Munawar, H Salman,
Khan Fahad Shahbaz, and Shao Ling. Random path
Vineet Gupta, Tomer Koren, and Yoram Singer. Sham-
selectionforincrementallearning. arXivpreprint,2019.
poo: Preconditionedstochastictensoroptimization. In
International Conference on Machine Learning, pages MandarJoshi,EunsolChoi,DanielSWeld,andLukeZettle-
1842–1850.PMLR,2018. moyer. Triviaqa: Alargescaledistantlysupervisedchal-
lengedatasetforreadingcomprehension. arXivpreprint
KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun. arXiv:1705.03551,2017.
Deep residual learning for image recognition. In Pro-
ceedingsoftheIEEEconferenceoncomputervisionand JeanKaddour,OscarKey,PiotrNawrot,PasqualeMinervini,
patternrecognition,pages770–778,2016. andMattJKusner. Notrainnogain: Revisitingefficient
trainingalgorithmsfortransformer-basedlanguagemod-
LuHou,ZhiqiHuang,LifengShang,XinJiang,XiaoChen, els. arXivpreprintarXiv:2307.06440,2023.
and Qun Liu. Dynabert: Dynamic bert with adaptive
widthanddepth. AdvancesinNeuralInformationPro- DimitrisKalimeris, GalKaplun, PreetumNakkiran, Ben-
cessingSystems,33:9782–9793,2020. jaminEdelman,TristanYang,BoazBarak,andHaofeng
11EfficientStagewisePretrainingviaProgressiveSubnetworks
Zhang. Sgd on neural networks learns functions of in- DenisPaperno,Germa´nKruszewski,AngelikiLazaridou,
creasing complexity. Advances in neural information QuanNgocPham,RaffaellaBernardi,SandroPezzelle,
processingsystems,32,2019. MarcoBaroni,GemmaBoleda,andRaquelFerna´ndez.
Thelambadadataset: Wordpredictionrequiringabroad
TomKwiatkowski,JennimariaPalomaki,OliviaRedfield, discourse context. arXiv preprint arXiv:1606.06031,
MichaelCollins,AnkurParikh,ChrisAlberti,Danielle 2016.
Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee,
Jonas Pfeiffer, Aishwarya Kamath, Andreas Ru¨ckle´,
KristinaToutanova,LlionJones,MatthewKelcey,Ming-
KyunghyunCho,andIrynaGurevych. AdapterFusion:
Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc
Non-destructive task composition for transfer learning.
Le, and Slav Petrov. Natural questions: A bench-
markforquestionansweringresearch. Transactionsof
InProceedingsofthe16thConferenceoftheEuropean
ChapteroftheAssociationforComputationalLinguistics:
the Association for Computational Linguistics, 7:452–
466, 2019. doi: 10.1162/tacl a 00276. URL https: MainVolume,pages487–503,Online,April2021.Asso-
//aclanthology.org/Q19-1026. ciationforComputationalLinguistics. doi: 10.18653/v1/
2021.eacl-main.39. URLhttps://aclanthology.
TaoLei,JunwenBai,SiddharthaBrahma,JoshuaAinslie, org/2021.eacl-main.39.
Kenton Lee, Yanqi Zhou, Nan Du, Vincent Y Zhao,
Ngoc-QuanPham,Thai-SonNguyen,JanNiehues,Markus
YuexinWu,BoLi,etal. Conditionaladapters:Parameter-
Mu¨ller, SebastianStu¨ker, andAlexanderWaibel. Very
efficient transfer learning with fast inference. arXiv
deepself-attentionnetworksforend-to-endspeechrecog-
preprintarXiv:2304.04947,2023.
nition. arXivpreprintarXiv:1904.13377,2019.
HaokunLiu,DerekTam,MohammedMuqeeth,JayMohta, Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
TenghaoHuang,MohitBansal,andColinARaffel. Few- Lee,SharanNarang,MichaelMatena,YanqiZhou,Wei
shotparameter-efficientfine-tuningisbetterandcheaper Li, and Peter J. Liu. Exploring the limits of transfer
thanin-contextlearning. AdvancesinNeuralInformation learning with a unified text-to-text transformer. Jour-
ProcessingSystems,35:1950–1965,2022. nalofMachineLearningResearch,21(140):1–67,2020.
URL http://jmlr.org/papers/v21/20-074.
HongLiu,ZhiyuanLi,DavidHall,PercyLiang,andTengyu html.
Ma. Sophia: A scalable stochastic second-order opti-
mizer for language model pre-training. arXiv preprint Nasim Rahaman, Aristide Baratin, Devansh Arpit, Felix
arXiv:2305.14342,2023a. Draxler,MinLin,FredHamprecht,YoshuaBengio,and
AaronCourville. Onthespectralbiasofneuralnetworks.
HongLiu,SangMichaelXie,ZhiyuanLi,andTengyuMa. InInternationalConferenceonMachineLearning,pages
Samepre-trainingloss,betterdownstream: Implicitbias 5301–5310.PMLR,2019.
matters for language models. In International Confer-
Pranav Rajpurkar, Robin Jia, and Percy Liang. Know
enceonMachineLearning,pages22188–22214.PMLR,
whatyoudon’tknow: Unanswerablequestionsforsquad.
2023b.
arXivpreprintarXiv:1806.03822,2018.
Zhuang Liu, Zhiqiu Xu, Joseph Jin, Zhiqiang Shen, and
SivaReddy,DanqiChen,andChristopherDManning.Coqa:
Trevor Darrell. Dropout reduces underfitting. arXiv
Aconversationalquestionansweringchallenge. Transac-
preprintarXiv:2303.01500,2023c.
tionsoftheAssociationforComputationalLinguistics,7:
249–266,2019.
IlyaLoshchilovandFrankHutter. Decoupledweightdecay
regularization. InInternationalConferenceonLearning NikunjSaunshi,JordanAsh,SurbhiGoel,DipendraMisra,
Representations,2019. URLhttps://openreview. CyrilZhang,SanjeevArora,ShamKakade,andAkshay
net/forum?id=Bkg6RiCqY7. Krishnamurthy. Understandingcontrastivelearningre-
quiresincorporatinginductivebiases. InProceedingsof
Nasrin Mostafazadeh, Michael Roth, Annie Louis, the39thInternationalConferenceonMachineLearning,
Nathanael Chambers, and James Allen. Lsdsem 2017
2022.
sharedtask: Thestoryclozetest. InProceedingsofthe
2ndWorkshoponLinkingModelsofLexical,Sentential RylanSchaeffer,BrandoMiranda,andSanmiKoyejo. Are
andDiscourse-levelSemantics,pages46–51,2017. emergent abilities of large language models a mirage?
InThirty-seventhConferenceonNeuralInformationPro-
RyanO’Donnell. Analysisofbooleanfunctions. Cambridge cessingSystems,2023. URLhttps://openreview.
UniversityPress,2014. net/forum?id=ITw9edRDlD.
12EfficientStagewisePretrainingviaProgressiveSubnetworks
Noam Shazeer and Mitchell Stern. Adafactor: Adaptive Peihao Wang, Rameswar Panda, Lucas Torroba Henni-
learning rates with sublinear memory cost. In Interna- gen,PhilipGreengard,LeonidKarlinsky,RogerioFeris,
tional Conference on Machine Learning, pages 4596– David Daniel Cox, Zhangyang Wang, and Yoon Kim.
4604.PMLR,2018. Learning to grow pretrained models for efficient trans-
formertraining. arXivpreprintarXiv:2303.00980,2023.
Sheng Shen, Pete Walsh, Kurt Keutzer, Jesse Dodge,
MatthewPeters,andIzBeltagy. Stagedtrainingfortrans- Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Bar-
formerlanguagemodels. InInternationalConferenceon retZoph,SebastianBorgeaud,DaniYogatama,Maarten
MachineLearning,pages19893–19908.PMLR,2022. Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tat-
sunoriHashimoto,OriolVinyals,PercyLiang,JeffDean,
AndreasSteiner,AlexanderKolesnikov,XiaohuaZhai,Ross and William Fedus. Emergent abilities of large lan-
Wightman,JakobUszkoreit,andLucasBeyer. Howto guagemodels. Transactionson MachineLearningRe-
trainyourvit? data,augmentation,andregularizationin search, 2022. ISSN 2835-8856. URL https://
vision transformers. arXiv preprint arXiv:2106.10270, openreview.net/forum?id=yzkSU5zdwD. Sur-
2021. veyCertification.
YiTay,MostafaDehghani,VinhQTran,XavierGarcia,Ja- JiXin,RaphaelTang,JaejunLee,YaoliangYu,andJimmy
sonWei,XuezhiWang,HyungWonChung,DaraBahri, Lin. Deebert: Dynamicearlyexitingforacceleratingbert
Tal Schuster, Steven Zheng, et al. Ul2: Unifying lan- inference. arXivpreprintarXiv:2004.12993,2020.
guagelearningparadigms. InTheEleventhInternational
Zhi-QinJohnXu,YaoyuZhang,TaoLuo,YanyangXiao,
ConferenceonLearningRepresentations,2022.
and Zheng Ma. Frequency principle: Fourier analysis
sheds light on deep neural networks. arXiv preprint
Ilya O Tolstikhin, Neil Houlsby, Alexander Kolesnikov,
arXiv:1901.06523,2019.
LucasBeyer,XiaohuaZhai,ThomasUnterthiner,Jessica
Yung,AndreasSteiner,DanielKeysers,JakobUszkoreit,
RowanZellers,AriHoltzman,YonatanBisk,AliFarhadi,
et al. Mlp-mixer: An all-mlp architecture for vision.
andYejinChoi. Hellaswag: Canamachinereallyfinish
Advancesinneuralinformationprocessingsystems,34:
yoursentence? arXivpreprintarXiv:1905.07830,2019.
24261–24272,2021.
HongyiZhang,YannNDauphin,andTengyuMa. Fixup
Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles, initialization: Residuallearningwithoutnormalization.
GabrielSynnaeve,andHerve´ Je´gou. Goingdeeperwith arXivpreprintarXiv:1901.09321,2019.
imagetransformers. InProceedingsoftheIEEE/CVFin-
ternationalconferenceoncomputervision,pages32–42, Minjia Zhang and Yuxiong He. Accelerating training
2021. oftransformer-basedlanguagemodelswithprogressive
layerdropping. AdvancesinNeuralInformationProcess-
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, ingSystems,33:14011–14023,2020.
AmjadAlmahairi,YasmineBabaei,NikolayBashlykov,
SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,etal. WangchunshuZhou,CanwenXu,TaoGe,JulianMcAuley,
Llama2: Openfoundationandfine-tunedchatmodels. KeXu,andFuruWei.Bertlosespatience:Fastandrobust
arXivpreprintarXiv:2307.09288,2023. inferencewithearlyexit.AdvancesinNeuralInformation
ProcessingSystems,33:18330–18341,2020.
AndreasVeit,MichaelJWilber,andSergeBelongie. Resid-
ualnetworksbehavelikeensemblesofrelativelyshallow
networks. Advances in neural information processing
systems,29,2016.
AlexWang,AmanpreetSingh,JulianMichael,FelixHill,
OmerLevy,andSamuelRBowman. Glue: Amulti-task
benchmark and analysis platform for natural language
understanding. arXivpreprintarXiv:1804.07461,2018.
AlexWang,YadaPruksachatkun,NikitaNangia,Amanpreet
Singh,JulianMichael,FelixHill,OmerLevy,andSamuel
Bowman. Superglue: Astickierbenchmarkforgeneral-
purposelanguageunderstandingsystems. Advancesin
neuralinformationprocessingsystems,32,2019.
13EfficientStagewisePretrainingviaProgressiveSubnetworks
Contents
1 Introduction 1
2 Progressivesubnetworktraining 2
2.1 RandomPathTraining: RAPTR . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
3 Illustration: RAPTRinpolynomialsetting 4
4 Experiments 4
4.1 ExperimentswithBERT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
4.2 ExperimentswithUL2-1.6B . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
4.3 Learningsandtakeaways . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
5 TheoreticalAnalysisusinglayerstability 7
5.1 Layerstability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
5.2 Illustrativeexample: linearnetworks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
6 Relatedworks 8
7 ConclusionandFutureWork 9
A AdditionalExperimentdetails 16
A.1 DetailsonPLD . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
A.2 Detailsforpolynomialtraining . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
A.3 DetailsforBERT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
A.4 DetailsforUL2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
B Scheduleselection 18
B.1 AblationondifferentschedulesforBERT-BASE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
B.2 AblationonfixedlayersetI forBERT-BASE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
C Systemspeedofdifferentsubnetworks 18
D Noisestabilityfortransformers 19
D.1 Noisestabilityoflinearresidualnetworks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
E RAPTRmotivationwithbooleandata: Theory 23
E.1 Analysisofthedifferentphases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
E.1.1 ProofofLemmaE.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
E.1.2 ProofofLemmaE.4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
14EfficientStagewisePretrainingviaProgressiveSubnetworks
E.1.3 ProofofLemmaE.5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
E.2 UsefulLemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
F Scaling 37
F.1 Proofs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
15EfficientStagewisePretrainingviaProgressiveSubnetworks
Table5.AdditionalrowsfromTable3:wecomparebaselinerunsattwolearningrateschedules,andruncompareRAPTRandGradual
stackingwithanew6-12-18-24schedule. KeyObservations: (a)Cosinedecaylearningworksbestforthebaseline,comparedto
Square-rootscheduleproposedby(Tayetal.,2022),(b)Theperformanceofgradualstackingimproveswiththenewschedule,implyinga
performancedependencewithappropriatescheduling,and(c)RAPTR’sperformancedoesn’tchangemuchwiththenewschedule.
Rel.FLOPs EvalLoss TriviaQA TydiQA SQuADv2 SGLUE Average
Baseline(Square-rootLRdecay) 1.2 2.07 23.4 31.9 44.3 60.0 39.9
Baseline(CosineLRdecay) 1.2 2.06 25.0 34.4 42.1 60.0 40.4
EquiflopBaseline(Square-rootLRdecay) 1 - 32.1 22.1 - 60.9 38.4
EquiflopBaseline(CosineLRdecay) 1 2.07 24.3 35.7 42.37 60.6 40.7
6-12-18-24Stacking 1 2.06 22.1 34.6 38.0 60.5 38.8
12-16-20-24Stacking 1 2.08 20.1 28.6 36.0 60.4 36.3
12-16-20-24RAPTR 1 2.08 22.2 38.2 40.6 60.1 40.3
(+30kinitialfull-modeltrain) 1 2.06 25.8 36.7 44.1 60.9 41.9
6-12-18-24RAPTR
(+30kinitialfull-modeltrain) 1 2.06 24.2 37.3 42.3 61.1 41.2
A.AdditionalExperimentdetails
A.1.DetailsonPLD
Theprogressivelayerdroppingalgorithm(ZhangandHe,2020)isgiveninAlgorithm2. PLDemploysatimeanddepth
schedule that determines the probability of dropping each block. The time schedule begins with a zero probability of
dropping each block. Then it increases this probability throughout the training process until it reaches a maximum of
(1−α¯),wherethehyperparameterα¯ =0.5. Wevaryα¯dependingontheaverageFLOPperplexitywewanttoachievein
theend. Withα¯ =p,theFLOPsreduceby1−(p/2).
Thedepthscheduleensuresthatblockslocatedearlierinthemodelaredroppedwithalowerprobabilitythanthoselocated
later/deeper. An important hyperparameter of the depth schedule in PLD is γ , which controls the rate at which the
f
probabilityofdroppinglayersincreases. Ahighervalueofγ leadstoaquickerincreaseintheprobability. ZhangandHe
f
(2020)setγ to100intheirexperiments.
f
A.2.Detailsforpolynomialtraining
WepresentinFigure2theresultsfroma20-layerresidualnetworktrainedon100-dimensionaldata. Ourf⋆hasmaxdegree
k = 10restrictedtothefirstt = 20variables(x )intheinput. Wepickm = 20randombasispolynomialsforeach
1:20
degreeℓ≤10. WesettheaveragedroprateofPLDtoα¯ =0.6,suchthattheaverageFLOPsis80%lowertoabaselinerun.
ForRAPTR,weuse4-stagetraining,withthestagesrepresentedbyits(p,I)as(0.4,{}),(0.6,{}),(0.8,{}),and(1.0,{})
respectively. Thelengthsofeachstagehavebeensetas16.5k,16.5k,16.5k,and48.5krespectively,suchthattheaverage
FLOPsis80%lowertoabaselinerun(pleaseseeAppendixB).
A.3.DetailsforBERT
EqualstepPretraining Thebatch-sizeissetto256and512forBERT-BASEandBERT-LARGErespectively. Following
J.Reddietal.(2023),weusealearningratewarmupof10kstepsandkeeplearningrateconstantinallphases,followed
byalinearlearningratedecayto0inthefinalfullmodeltrainingphase. Wesetthepeaklearningrateas10−4inallour
experiments.
EquivalentflopPretraining Tomakeafaircomparisonacrossallmethods,wetunethepeakthelearningrateinthegrid
{1,1.6,2.5,4}×10−4forBERT-BASEandgrid{1,1.6,2.5}×10−4forBERT-LARGEinallsettings.
Finetuning Wefine-tuneBERTmodelsfor3epochswithbatch-size32,usingalearningrategridsearchover{1,2,3,5}×
10−5.Weusealinearwarmupscheduleoverinitial6%trainingsteps,followedbyconstantlearningratescheduleforthe
restofthetraining.
16EfficientStagewisePretrainingviaProgressiveSubnetworks
Algorithm2Progressivelayerdropping(PLD)(ZhangandHe,2020)
1: Input:iterationsT,layerkeepprobabilityα¯,temperaturebudgetγ f >0,layersL,functions(self-attention,layer-norm,
feed-forward)f ,f ,f ,lossfunctionL,data(x ,y),outputlayerf
ATTN LN FFN 0 O
2: γ ← γf
T
3: fort←1toT do
4: p←1{Keepprobability.}
5: α t ←(1−α¯)exp(−γ·t)+α¯
6: p d ← 1− Lαt {Layerdecay.}
7: fori←0toL−1do
8: s∼Bernoulli(p){Keepordrop.}
9: ifs==0then
10: x i+1 ←x i{Drop.}
11: else
12: x′
i
←x i+ fATTN( pfLN(xi))
13: x i+1 ←x′ i+ fFFN(f pLN(x′ i))
14: endif
15: p←p−p d{Decayprob.}
16: endfor
17: ℓ←L(f O(x L),y)
18: f ATTN,f LN,f FFN,f O ←Update(ℓ)
19: endfor
A.4.DetailsforUL2
Pretraining SimilartoourexperimentsonBERT,weusealinearwarmupof10Ksteps,followedbyconstantlearning
rateinallstages,andacosinedecayto0.1×thepeaklearningrateinthefinalfullmodeltrainingstage. Thepeaklearning
rateissetto10−2.
ReporteddatasetsinTable3includeTriviaQA(Joshietal.,2017),TydiQA(Clarketal.,2020),SQuADV2(Rajpurkar
etal.,2018),andSuperGLUE(Wangetal.,2019). ForQAtasks,wereportExactmatchscores,andreportaverageaccuracy
forSuperGLUEtasks. Toreducevariance,wereportaverageperformanceacross3runsforthemostrepresentativesetting
foreachmethod. WeconductextensivedownstreamevaluationofthetrainedmodelsfromTable3onadditionaldownstream
tasksandunderhighershotin-contextsetting. AdditionaltasksincludeQAtaskslikenaturalQA(Kwiatkowskietal.,2019),
webQA(Berantetal.,2013),DROP(Duaetal.,2019),CoQA(Reddyetal.,2019),andQuAC(Choietal.,2018);and
completiontaskslikeStoryclose(Mostafazadehetal.,2017),Hellaswag(Zellersetal.,2019),andLAMBADA(Paperno
etal.,2016).
Boundariesfor12-16-20-24 RAPTR andGradualStackingare40k, 120k, and240k(decidedusingAppendixB).The
averagesubnetworklengthusedduringtrainingturnsouttobe20withthisschedule. Whenweintroduceaninitial30k
full-modeltrainingforRAPTR,weshifttheboundariesby30ksothattheaveragesubnetworklengthstays20.
AdditionalPretrainingexperiments InTable5,wehavethefollowingadditionalrunscomparedtoTable3inthemain
paper.
Baselinelearningrateschedule: Wefirstcomparestandardtrainingwithcosinedecaylearningratetoastandardtraining
runwithSquare-rootlearningratedecaythatwasusedintheoriginalpaper(Tayetal.,2022). Weobservethatthemodel
withsquare-rootlearningrateperformssignificantlyworsecomparedtothemodelwithcosinedecaylearningrate. Hence,
weusestandardtrainingwithcosinedecaylearningrateasourbaseline.
Stackingdependenceonschedule: Secondly,wetryanotherscheduleforGradualStacking,whichcontains4stagesandis
representedby6-12-18-24. Thestagelengthshavebeenadjustedsuchthattheaveragenumberoflayersusedovertrainingis
20. Wesetthestageboundariesas17k,74k,and172krespectively. Weobservethat6-12-18-24GradualStackingperforms
muchbetterthan12-16-20-24GradualStackingonvariousdownstreamtasks. Thissignalstowardsthedependencyof
GradualStackingonproperscheduleselection.
17EfficientStagewisePretrainingviaProgressiveSubnetworks
Table6.PerformanceofRAPTRonBERT-BASEwithdifferentschedules.Theaveragesubnetworksizeforeachscheduleis9,which
makeseachschedule1.33xfasterthanthebaselinemodel(providedforreference).WeusethesamehyperparametersasusedinTable1.
Weobserveminordifferencesbetweendifferentschedules,indicatingrobustnessofRAPTR.
RAPTRschedule Rel. flops Evalloss MNLI QNLI SST-2 Avg.
3-6-9-12 1 1.76 82.0 89.6 92.0 87.9
4-8-12 1 1.76 81.9 89.3 91.5 87.6
6-9-12 1 1.75 82.3 89.2 91.0 88.0
6-8-10-12 1 1.75 82.1 89.8 92.4 88.1
Baseline 1.33 1.76 81.5 90.5 91.4 87.8
OntheotherhandRAPTRdoesn’tshowabigdifferencewith6-12-18-24schedule,whencomparedto12-16-20-24schedule.
Aproperanalysisofboththemethodsonscheduledependenciesiskeptforfuturework.
B.Scheduleselection
Wefollowtwoschedulesfrom(J.Reddietal.,2023),EqualandProportional,tobuildthelengthsofeachstageinRAPTR
andStacking. ForEqualscheduling,usedinTables1and2,wesplittrainingintokequalstages. ForProportionalscheduling
inTable3,weincreasethelengthofastageinproportiontoindexofthestage. Forexample,for12-16-20-24RAPTRand
Stackingwithproportionalschedulingfor400ktrainingsteps,thestagelengthsaresetas40k,80k,and120krespectively.
The flop counts are decided on the basis of average subnetwork length during training. For example, if the average
subnetworklengthis18foramodelwith24layers,weconsiderrelativeflopsofRAPTRcomparedtobaselineas0.75.
For12-16-20-24 RAPTR withEqualschedule,theaveragesubnetworklengthduringtrainingisgivenby18,whilefor
Proportionalschedule, theaveragesubnetworklengthduringtrainingisgivenby20. However, wheneitherEqualand
Proportionalschedulesdon’treturnthetargetaveragesubnetworklength,were-considerthestagelengthsbytakingthe
closestcandidateamongEqualandProportionalschedulesandremoveequalstepsfromeachofthefirstk−1stagesand
addtothefinalfullmodeltrainingstage. Forexample,inTable5,with6-12-18-24RAPTRandtargetaveragesubnetwork
length20,Proportionalschedulereturnsanaveragesubnetworklengthof18andstagelengths40k,80k,120kand160k
respectively. Wethenaimtofindxsuchthatstagelengthsof40k−x,80k−x,120k−xand160k+3xreturnsthedesired
averagesubnetworklength. Onsolvingforx,wegetx=22k.
B.1.AblationondifferentschedulesforBERT-BASE
We conduct experiments on RAPTR with different stage schedules that have same relative flop counts in Table 1. We
observeonBERT-BASEthatdifferentschedulesdifferbyatmost0.01inpre-traininglossand0.1−0.5%indownstream
performanceonaverage. Thus,weconcludethatRAPTRisrobusttothestagescheduleselection.
ForUL2,weusethefollowingstageschedulestominimizecomputationaloverhead. Werestrictthenumberofphases
in RAPTR to 4. The first stage selects a sub-network with half of the layers at random at each step. We increase the
sub-networksizebyanequalquantityineachstagetransition,suchthatwetrainthefullmodelinthefinalphase.
B.2.AblationonfixedlayersetI forBERT-BASE
Weconductexperimentson6-8-10-12RAPTRonBERT-BASEwithdifferentfixedlayersetI (Table7). Weobservethat
fixedsetselectionof{1,12}performsthebestamongothercandidates. Thisselectionhowevercanbeexpensivetoverify
forothersettingslikeUL2. Hence,inallsubsequentexperiments,wecontinuewiththefirstandlastlayerfixedinallstages
ofRAPTR.
C.Systemspeedofdifferentsubnetworks
Wereportthespeedoftraininginsteps/secforUL2modelswithdifferentsubnetworksizesonTPU16x16chipswith
percorebatchsizeof2inTable8. Supposetisthetotaltimefor400kstepswitha24-layermodelinUL2training. In
Table3,weuse12-16-20-24RAPTRthathas20%”theoretical”flopcountreduction,withthestagelengthsgivenby40k,
18EfficientStagewisePretrainingviaProgressiveSubnetworks
Table7.Performanceof RAPTR on BERT-BASE withdifferentI setfor6-8-10-12 RAPTR runfor100k steps. Weusethesame
hyperparametersasusedinTable1,exceptthetrainingstepsreducedto100ksteps.Weobservethatfixingthefirstandthelastlayeratall
timesleadtoslightlybetterperformance,comparedtoothercandidates.
I set Evalloss MNLI QNLI SST-2 Avg.
{} 2.12 - - - -
{1} 2.13 - - - -
{1,12} 2.11 78.6 86.77 88.69 84.7
{1,2,12} 2.13 - - - -
{1,11,12} 2.12 77.4 87.0 89.4 84.6
{1,2,11,12} 2.14 78.0 86.1 88.8 84.3
{1,2,3,11,12} 2.18 77.4 86.5 88.4 84.1
{1,2,10,11,12} 2.16 - - - -
Table8. TrainingSteps/secatdifferentstageson24-layerUL2
subnetworksize steps/sec
12 2.0
16 1.6
20 1.4
24 1.1
80k,120k,and240k respectively(AppendixB).Theoretically,thenecessarytimerequiredshouldbe0.83t. Usingthe
trainingspeednumbersofTable8,ittakesapproximately0.84ttocomplete400kstepswithRAPTRonourmachine,close
tothetheoreticaltrainingtime.
D.Noisestabilityfortransformers
TheoremD.1. SupposeLsatisfiesthefollowingconditionLsatisfiesthefollowingconditionforanyinputx˜,η ∈Rdand
labelv ∈{0,1}V forsomeconstantµ :
loss
∥η∥
L(x˜+η,v)−L(x˜,v)≤µ 2. (3)
L∥x˜∥
2
The difference between expected loss of (L−1)-RAPTR and L-RAPTR, i.e. |L 2(F)−L 1(F)|, is upper bounded by
CE ((cid:80)L Ψ (x))/∥F(x)∥ forsomeconstantC thatdependsonregularitypropertiesoftheheadH ontopofthe
L x∈D ℓ=1 ℓ 2
backboneF.
ProofofTheorem5.3. DenotebyF −ℓtheLcandidatesthatwecanrandomlychoosefromduringL−1RAPTRtraining.
TheoutputofF onanyinputxisequivalenttotheoutputofthemodelF onxandα ,withα =0andtherestsetto1.
−ℓ 1:L ℓ
Then,theaveragelossofL−1randomtrainingisgivenby
L
1 (cid:88)
L (F)= E L(F (x),v).
2 L x,y∼D −ℓ
i=1
Ontheotherhand,thelossofthefullmodelisgivenby
L (F)=E L(F(x),v).
1 x,v∼D
Hence,theexpecteddifferencebetweenlossofL−1randomtrainingandfullmodelisgivenby
19EfficientStagewisePretrainingviaProgressiveSubnetworks
L
1 (cid:88)
L (F)−L (F)= E (L(F (x),v)−L(F(x),v))
2 1 L x,v∼D −ℓ
i=1
L
≤
1 (cid:88)
E µ
∥F −ℓ(x)−F(x)∥
2
L x,v∼D L ∥F(x)∥
i=1 2
L
≤ 1 (cid:88) E µ ψ ℓ .
L x,v∼D L∥F(x)∥
i=1 2
Here the pre-final step uses the definition of µ from Equation (3), and the final step uses the definition of Ψ from
L ℓ
Definition5.2.
Discussion An example of a loss function with bounded µ is a transformer model, that uses a layer normalization
L
layerbeforeusinganembeddingmatrixΦtocomputethelogits. Thelogitsarethencomparedwiththetruelabelwitha
cross-entropylosswithasoftmaxfunctiononlogits. µ hencedependsontheℓ normofΦandtheweightparametersof
L 2
thelayernormalizationlayer.
D.1.Noisestabilityoflinearresidualnetworks
Wefirstreportarandommatrixlemmathatwewillrepeatedlyuseinthissection.
LemmaD.2(Normoftheoutputafterarandomlinearlayer, lemma7.1in(Allen-Zhuetal.,2019)withoutactivation
function). For any ϵ > 0, a random matrix A ∼ N(0,d−1/2) and a randomly sampled input x ∈ Sd−1, w.p. atleast
1−e−Ω(dϵ2),
(1−O(ϵ))≤∥Ax∥ ≤(1+O(ϵ)).
2
WerepeatLemma5.4fromthemainpaper.
Lemma5.4. AtrandominitializationwhereW ∼N(0,d−1/2I),forarandomlysampledx∈Sd−1,wehave
i
√
(cid:112)
(a) Withresidualconnection&layernorm,Ψ (x)=O( L/ℓ)&∥F(x)∥ =Ω( L). Thenthegapinlossesbetween
√ ℓ 2
stagesisO(1/ L).
(b) Withoutresidualconnection,Ψ (x)=Ω(1)&∥F(x)∥ =O(1). ThusthegapinlossesbetweenstagesisΩ(1).
ℓ 2
(c) Withoutlayernorm,wehaveΨ (x)=Ω(2(L−1)/2)and∥F(x)∥ =O(2L/2). Thusthegapinlossesbetweenstages
ℓ 2
isΩ(1).
ProofofLemma5.4. Weoutlinetheproofforeachcase.
1. Withresidualconnectionandlayernormalization,thefunctionF computestheintermediateactivationsy(1),··· ,y(L)
onaninputxasfollows.
(cid:32) (cid:33)
W
y(ℓ) = I+ (cid:13) ℓ (cid:13) y(ℓ−1).
(cid:13)y(ℓ−1)(cid:13)
√
SimilartoLemmaD.2,wecanshoww.h.p. withrespecttotherandomnessofW ,··· ,W ,forallℓ≤L, ℓ(1−
√ 1 L
(cid:13) (cid:13)
O(1))≤(cid:13)y(ℓ)(cid:13) ≤ ℓ(1+O(1)).
2
Ignoringtheerrorterms,wecansimplywrite
(cid:18) (cid:19)
W
y(ℓ) = I+ √ℓ y(ℓ−1).
ℓ
20EfficientStagewisePretrainingviaProgressiveSubnetworks
Pickagenerallayerℓtobedropped. Then,
L (cid:18) (cid:19)
F (x)−F(x)= (cid:89) 1+ W √ℓ′ W √ℓy(ℓ−1)+err,
−ℓ
ℓ′ ℓ
ℓ′=ℓ+1
wheretheerrtermappearsbecauseofthechangeinscalesofactivationsy(ℓ+1),··· ,y(L)withthedroppingoflayer
√
ℓ. ThiserrorcanbeboundedasO( L/ℓ)usingthesameprocedurefollowedbelowtoboundthefirsttermonR.H.S..
Similartoboundingthenormsofy(ℓ),w.h.p. wecanshowthat
(cid:13) (cid:13)
(cid:13)
(cid:89)L (cid:18)
1+
W √ℓ′(cid:19) W √ℓy(ℓ−1)(cid:13) (cid:13)
(cid:13)
≤(cid:114) L
.
(cid:13) (cid:13) ℓ′ ℓ (cid:13) (cid:13) ℓ
ℓ′=ℓ+1 2
Hence,
(cid:112)
ψ :=∥F (x)−F (x)∥ ≤O( L/ℓ).
ℓ −ℓ ℓ 2
Thisimplies
1 (cid:88) 1 (cid:88) (cid:112)
ψ (x)= O( L/ℓ)=O(1).
L ℓ L
ℓ ℓ
Since the gap in L and L is bounded by O(E 1 (cid:80) ψ (x)/||F(x)||) from Theorem 5.3, we have the gap as
2 1 xL ℓ ℓ
(cid:16) (cid:17)
O √1 .
L
2. Withnonormalization,thefunctionF looksasfollows.
y(ℓ) =(I+W )y(ℓ−1).
ℓ
SimilartoLemmaD.2,wecanshoww.h.p. withrespecttotherandomnessofW ,··· ,W ,forallℓ≤L,2ℓ/2(1−
1 L
(cid:13) (cid:13)
O(1))≤(cid:13)y(ℓ)(cid:13) ≤2ℓ/2(1+O(1)).
2
Withadropinlayer,weget
L ℓ−1
F (x)−F(x)=
(cid:89) (cid:16) I+Wℓ′(cid:17)
W
(cid:89) (cid:16) I+Wℓ′(cid:17)
x.
−ℓ ℓ
ℓ′=ℓ+1 ℓ′=1
Similar to Lemma D.2, we can show w.h.p. with respect to the randomness of W ,··· ,W ,
1 L
(cid:13) (cid:16) (cid:17) (cid:16) (cid:17) (cid:13)
(cid:13)(cid:81)L I+Wℓ′ W (cid:81)ℓ−1 I+Wℓ′ x(cid:13) ≥O(2(ℓ−1)/2).
(cid:13) ℓ′=ℓ+1 ℓ ℓ′=1 (cid:13)
2
Thisimplies∥F (x)−F(x)∥ =Ω(2(ℓ−1)/2).
−ℓ 2
TheargumentcannowbecompletedbyusingTheorem5.3.
3. Withoutresidualconnection,thefunctionF looksasfollows.
y(ℓ−1)
y(ℓ) =W ℓ(cid:13) (cid:13)y(ℓ−1)(cid:13)
(cid:13)
.
2
SimilartoLemmaD.2,wecanshoww.h.p.withrespecttotherandomnessofW ,··· ,W ,forallℓ≤L,(1−O(1))≤
1 L
(cid:13) (cid:13)
(cid:13)y(ℓ)(cid:13) ≤(1+O(1)).Thus,ignoringerrorterms,thenetworkF roughlylookslike
2
y(ℓ) =W y(ℓ−1).
ℓ
21EfficientStagewisePretrainingviaProgressiveSubnetworks
Withadropinlayer,weget
L ℓ−1
(cid:89) (cid:89)
F (x)−F(x)= W (W(ℓ)−I) W x.
−ℓ ℓ′ ℓ′
ℓ′=ℓ+1 ℓ′=1
UsingrandomnessofW(ℓ) thiscanbeshowntobeofnormΩ(1). Theargumentcannowbecompletedbyusing
Theorem5.3.
WeshowasimilarlemmaforthecasewhereallthematricesW areidenticaltoamatrixA.
i
LemmaD.3. Whenthelayersareperfectlyaligned,i.e. allweightsW =AforsomematrixAwith∥A∥ =1andfor
i 2
simplicity,assumesecondeigenvalueλ (A)=1−δforsomeδ >0,wehaveforarandomlysampledx∈Sd−1
2
(a) Withresidualconnection&layernorm,wehaveΨ (x)=O(1)and∥F(x)∥=Ω(L). Thus,thegapinlossesbetween
ℓ
stagesisO(1/L).
(b) Withoutresidualconnection,wehavebothΨ (x)=O(δ−1e−ℓ)and∥F(x)∥=O(1). Thusthegapinlossesbetween
ℓ
stagesisO(1).
L
(c) Withoutlayernorm,wehaveΨ (x)=Ω(2L−1)and∥F(x)∥=O(2L). ThusthegapinlossesbetweenstagesisΩ(1).
ℓ
ProofofLemmaD.3. Weoutlinetheproofforeachcase.
1. Withresidualconnection&layernorm,thefunctionF computestheintermediateactivationsy(1),··· ,y(L) onan
inputxasfollows.
(cid:32) (cid:33)
A
y(ℓ) = I+ (cid:13) (cid:13) y(ℓ−1).
(cid:13)y(ℓ−1)(cid:13)
Weshowthattheabovemethodwillbehavelikepowermethod,i.e. y(ℓ)willbeofmagnitudeΩ(ℓ)andwillbeϵ-close
inangletothetopeigenvectorofA,denotedasv (A),providedℓ≥Ω((1/δ)log(1/ϵ)).
1
Wegiveaninductiveargumentasfollows. Consideranylayerℓ. Sayθ denotestheanglebetweeny(ℓ) andv (A).
ℓ 1
Also,sayΠ⊥ denotesanorthogonalprojectiontosubspacespannedbytherestoftheeigenvectorsofA. Then,
v1
(cid:12) (cid:12)
(cid:12)⟨v 1(A),y(ℓ+1)⟩(cid:12)
|tanθ ℓ+1|= (cid:13)
(cid:13)Π⊥
(y(ℓ+1))(cid:13)
(cid:13)
v1 2
(cid:12) (cid:12)
(cid:12)⟨v 1(A),y(ℓ)⟩+⟨v 1(A),Ay(ℓ)⟩(cid:12)
= (cid:13) (cid:13)
(cid:13)Π⊥ (y(ℓ))+Π⊥ Ay(ℓ)(cid:13)
v1 v1 2
(cid:12) (cid:12)
2(cid:12)⟨v 1(A),y(ℓ)⟩(cid:12)
= (cid:13) (cid:13)
(cid:13)Π⊥ (y(ℓ))+Π⊥ Ay(ℓ)(cid:13)
v1 v1 2
2⟨v (A),y(ℓ)⟩
≤ (cid:13) 1 (cid:13)
(cid:13)Π⊥ v1(y(ℓ))+λ 2(A)Π⊥ v1y(ℓ)(cid:13)
2
2
= |tanθ |.
1+λ (A) ℓ
2
Thisimplies,underλ (A)<1,|tanθ |decreaseswithℓ. Undertheassumptionthatxisn’torthogonaltov (A),the
2 ℓ 1
aboveconditionsimplyshowsthatifℓ≥O((1/δ)log(1/ϵ)),than|tanθ |<ϵ.
ℓ
(cid:13) (cid:13)
Furthermore,oncealigned(orcloselyaligned)tov 1(A),thenormofy(ℓ) growslinearly. Hence,(cid:13)y(ℓ)(cid:13) = Ω(L).
Furthermore,foranyℓ,thegapbetweenF andF simplybecomesequaltothegapbetweenLandL−1stepsof
−ℓ ℓ
themodifiedpowermethod,whichcanbeboundedasO(1).
22EfficientStagewisePretrainingviaProgressiveSubnetworks
2. Withnoresidualconnection,thefunctionF computestheintermediateactivationsy(1),··· ,y(L) onaninputxas
follows.
A
y(ℓ) = (cid:13) (cid:13)y(ℓ−1).
(cid:13)y(ℓ−1)(cid:13)
Fromtheupdate,it’strivialtocometotheconclusionthaty(ℓ)willstayunitnormforanylayerℓ.
Thisupdateisexactlypower-method,wherey(ℓ)getsϵ-closetothetopeigenvectordirectionofAinO((1/δ)log(1/ϵ))
steps. TheresultthenfollowsfromboundingthegapbetweenLandL−1stepsofpower-method.
3. TheproofisverysimilartotheproofofLemma5.4(3),wherethemajordifferencecomesfromtheblowupinthe
normsoftheintermediateactivations.
E. RAPTR motivationwithbooleandata: Theory
Dataandlabels: Weuseuniformlysampledbooleandatax∼U({±1}d). Thetruelabelfunctionf∗ :Rd →Rforeach
dataisgivenas
√ √
3 3
f∗(x)= + x −x x .
2 2 1 1 2
Networktotrain: Wetraina2-layerresidualnetworkf :Rd →Rcomprisedoftwosingleneuronresidual
p0,w1,w2,b1,b2
blocks,wherep behavesasapositionbiasthatweaddtotheoutputofthefirstblock. Onaninputx,thenetworkcomputes
0
outputy ∈Rwithintermediateoutputy(1),y(2) ∈Rasfollows.
y(1) =p +sin(⟨w ,x⟩+b ),
0 1 1
y(2) =y(1)+sin(⟨w ,x⟩+y(1)+b ). (4)
2 2
Herey :=y(2)isreturnedasthefinaloutput. Thenetworkistrainedwithmeansquarederror,givenby
L=E (y−f∗(x))2. (5)
x∼U({±1}d)
Weconsiderpopulationgradientsforthesakeofcleanexpositionoftheimportantideas. Interestedreaderscanmodifythe
prooftoworkforfinitesampleregimes.
A note: The position bias p has been introduced to simplify our proof. From experiments, we observe that p isn’t
0 0
necessarytoobservethestagewiseconvergenceofRAPTR.
Initializationofweightsandbiases: Theelementsoftheweightsandbiasesw ,w ,p ,b ,b havebeeninitialized
1 2 0 1 2
fromall0s.
RAPTRInitialpositionbiastraining: Intheinitialbiasphasetraining,wesimplytrainthebiasp 0,i.e. ateachstep,we
trainwiththefollowingmeansquaredloss,
L=E (p −f∗(x))2. (6)
x∼U({±1}d) 0
RAPTRFirstphasetraining: Inthefirstphase,boththelayersaretrainedindependently. Thatis,ateachstep,wepicka
randomlayerℓ∈{1,2}andtrainwiththemeansquaredloss,givenby
L=E (p +sin(⟨w ,x⟩+b )−f∗(x))2. (7)
x∼U({±1}d) 0 ℓ ℓ
23EfficientStagewisePretrainingviaProgressiveSubnetworks
RAPTRSecondphasetraining: Inthesecondphaseoftraining,wetrainthefullmodel(Equation(4))withthemean
squaredloss(Equation(5)). Forsimplicity,wefixtheparametersofthefirstlayerandsimplytraintheparametersw ,b .
2 2
Wefirstrecallthetheoremcorrespondingtothestage-wisetrainingofRAPTR.
TheoremE.1. Forasmallenoughlearningrateη < 1 ,underparameterinitializationfromall0s,aninitialposition
poly(d)
biasonlytrainingfollowedbya2-stageRAPTRonthenetworkf
p0,w1,w2,b1,b2
(Equation(4))withmeansquarederror
showsthefollowingbehavior.
• AfterΘ(1/η)stepsofpositionbiasonlytrainingandΘ(1/η)stepsoffirstphase,forbothlayersℓ∈{1,2},
(cid:12) √ √ (cid:12)
(cid:12) 3 3 (cid:12)
(cid:12)p +sin(⟨w ,x⟩+b )− − x (cid:12)≤O(η).
(cid:12) 0 ℓ ℓ 2 2 1(cid:12)
(cid:12) (cid:12)
• AfterΘ(1/η)stepsofsecondphase,theouputofthesecondlayerisgivenas
(cid:12) (cid:12) (cid:12) (cid:12)
(cid:12)y(2)−y(1)−(−x x )(cid:12):=(cid:12)sin(⟨w ,x⟩+y(1)+p +b )−(−x x )(cid:12)≤O(η).
(cid:12) 1 2 (cid:12) (cid:12) 2 0 2 1 2 (cid:12)
Hence,afterΘ(1/η)stepsofinitialpositionbiasand2-phaseRAPTRtraining,lossL<O(η)(5).
Proof. Theproofproceedsbyanalyzingthebehavioroftheweightsandthebiasesinthedifferentphases.
• Intheinitialpositionbiasonlytraining,weshowthatinΘ(1/η)steps,
(cid:12) √ (cid:12)
(cid:12)p − 3/2(cid:12)≤O(η).
(cid:12) 0 (cid:12)
• Inthefirstphase,afterΘ(ηlog(d/η))steps,weshowinLemmaE.4thatforanylayerℓ∈{1,2},theweightsandthe
biasessatisfythefollowingconditions.
(cid:12) π(cid:12)
(cid:12)w − (cid:12)≤O(η),
(cid:12) ℓ,1 3(cid:12)
|w |≤O(η), forall2≤j ≤d,
ℓ,j
|b |≤O(η).
ℓ
Hence,afterfirstphaseoftraining,thenetwork’sintermediateoutputsy(1)andy(2)(fromEquation(4))aregivenas
√ √
3 3
y(1) =p +sin(⟨w ,x⟩+b )= + x +O(η),
0 1 1 2 2 1
y(2) =y(1)+sin(⟨w ,x⟩+y(1)+p +b )
2 0 2
√ √
3 3 √ √
= + x +sin(⟨w ,x⟩+ 3/2x + 3/2+b )+O(η).
2 2 1 2 1 2
• Inthesecondphase,afterΘ(1/η)steps,weshowinLemmaE.5thattheweightsandthebiasesw ,b satisfythe
2 2
followingconditions.
(cid:12) √ π(cid:12)
(cid:12)w + 3/2− (cid:12)≤O(η),
(cid:12) 2,1 2(cid:12)
(cid:12) π(cid:12)
(cid:12)w − (cid:12)≤O(η),
(cid:12) 2,2 2(cid:12)
(cid:12) π(cid:12)
(cid:12)b +p − (cid:12)≤O(η),
(cid:12) 2 0 2(cid:12)
|w |≤O(η), for3≤j ≤d.
2,j
24EfficientStagewisePretrainingviaProgressiveSubnetworks
Thus,attheendofthesecondphase,thenetwork’soutputy :=y(2)(fromEquation(4))isgivenas
y(2) =y(1)+sin(⟨w ,x⟩+y(1)+p +b )
2 0 2
√ √
3 3 √ √
= + x +sin(⟨w ,x⟩+ 3/2x + 3/2+b )+O(η)
2 2 1 2 1 2
√ √ √ √
3 3 π π π 3 3
= + x +sin( x + x + )+O(η)= + x −x x +O(η).
2 2 1 2 1 2 2 2 2 2 1 1 2
Furthermore,wecanshowthateachlayerinitselfisn’texpressiveenoughtorepresentthetruelabelfunction.
TheoremE.2. Foreachlayerwhentrainedindependently,thebestw ,b parameterswillhaveanΩ(1)error.
ℓ ℓ
Proof. Theproofofthetheoremissimple. Weargueforthefirstlayer(sameargumentholdsforthesecondlayer). We
simplycalculatetheprojectionofthefunctionlearnedbyw ,b intothecomponentsofthetruelabelfunction. Supposethe
1 1
functiondefinedbythemisgivenbyαx +βx x
1 1 2
Then,usingorthogonalityofthebasispolynomialsx andx x ,wehave
1 1 2
α=E sin(⟨w ,x⟩+b)x
x∈{±1}d 1 1
=E sin(π/2−⟨w ,x⟩−b)x
x∈{±1}d 1 1
(cid:89)
=sin(b)( cosw )sin(w ). UsingCorollaryE.9
i 1
i̸=1
β =E sin(⟨w ,x⟩+b)x x
x∈{±1}d 1 1 2
=E sin(π/2−⟨w ,x⟩−b)x x
x∈{±1}d 1 1 2
(cid:89)
=−cos(b)( cosw )sin(w )sin(w ). UsingCorollaryE.9
i 1 2
i̸=1,2
Adding/Subtractingtheseterms,weget
(cid:89)
α−β =sin(b+w )( cosw ),
2 i
i̸=1,2
(cid:89)
α+β =sin(b−w )( cosw ),
2 i
i̸=1,2
(cid:89)
−α+β =sin(−b+w )( cosw ),
2 i
i̸=1,2
(cid:89)
−α−β =sin(−b−w )( cosw )
2 i
i̸=1,2
Thus,wemustberestrictedbytheset|α|,|β|<1,|α|+|β|<1. Asthecoefficientsofthetruelabelfunctionforx and
1
x x don’tlieinthisset,theprooffollows.
1 2
E.1.Analysisofthedifferentphases
√
Initialpositionbiasonlytraininglinearlyincreasesp to 3/2.
0
LemmaE.3. [Positionbiasonlytraining]Foranylearningrateη >0,afterΘ(1/η)steps,p satisfies
0
(cid:12) √ (cid:12)
(cid:12)p − 3/2(cid:12)≤O(η).
(cid:12) 0 (cid:12)
25EfficientStagewisePretrainingviaProgressiveSubnetworks
Thefirstlemmaanalysesthebehavioroftheweightsduringphase1.
√
LemmaE.4. [Phase1of RAPTR]Forasmallenoughlearningrateη < poly1 (d),withpositionbiasp 0at 3/2+O(η),
and under parameter from all 0s, after Θ(1/η) steps, for any layer ℓ ∈ {1,2}, the weights and the biases satisfy the
followingconditions.
(cid:12) π(cid:12)
(cid:12)w − (cid:12)≤O(η),
(cid:12) ℓ,1 3(cid:12)
|w |≤O(η), forall2≤j ≤d,
ℓ,j
|b |≤O(η).
ℓ
Thesecondlemmaanalysesthebehaviorofthesecondlayerweightsinphase2.
LemmaE.5. [Phase2of RAPTR]Forasmallenoughlearningrateη < 1 ,startingfromtheweightsandbiases
poly(d)
reachedbyΘ(1/η)stepsofpositionbiasonlyandfirstphaseof RAPTRtraining(LemmasE.3andE.4),afterΘ(1/η)steps
ofsecondphaseof RAPTRtraining,theweightsandbiasesw 2andb 2satisfythefollowingconditions.
(cid:12) √ π(cid:12)
(cid:12)w + 3/2− (cid:12)≤O(η),
(cid:12) 2,1 2(cid:12)
(cid:12) π(cid:12)
(cid:12)w − (cid:12)≤O(η),
(cid:12) 2,2 2(cid:12)
(cid:12) √ π(cid:12)
(cid:12)b + 3/2− (cid:12)≤O(η),
(cid:12) 2 2(cid:12)
|w |≤O(η), for3≤j ≤d.
2,j
E.1.1.PROOFOFLEMMAE.3
LemmaE.3. [Positionbiasonlytraining]Foranylearningrateη >0,afterΘ(1/η)steps,p satisfies
0
(cid:12) √ (cid:12)
(cid:12)p − 3/2(cid:12)≤O(η).
(cid:12) 0 (cid:12)
Proof. Underall0sinitialization,theoutputofbothofthelayersis0. Thus,thegradientofp isgivenby
0
(cid:32) √ √ (cid:33) √
3 3 3
E p − − x +x x =p − .
x∼{−1,+1}d 0 2 2 1 1 2 0 2
√
Hence,p increasesupuntilitreaches 3/2+O(η).
0
E.1.2.PROOFOFLEMMAE.4
√
LemmaE.4. [Phase1of RAPTR]Forasmallenoughlearningrateη < poly1 (d),withpositionbiasp 0at 3/2+O(η),
and under parameter from all 0s, after Θ(1/η) steps, for any layer ℓ ∈ {1,2}, the weights and the biases satisfy the
followingconditions.
(cid:12) π(cid:12)
(cid:12)w − (cid:12)≤O(η),
(cid:12) ℓ,1 3(cid:12)
|w |≤O(η), forall2≤j ≤d,
ℓ,j
|b |≤O(η).
ℓ
√
Proof. SimilartotheproofofLemmaE.3, wecanshowthatthepositionbiasp staysO(η)closeto 3/2. Thus, for
0
simplicityofexposition,wesimplyassumeitwithoutformallyprovingso.
WiththefirststageofRAPTR,weonlykeeponeofthetwolayersduringtraining. Hence,boththelayerparameterstrain
similarlyduringtraining. Withoutlossofgenerality,wediscussforasinglelayerweightsℓ∈{1,2}.
26EfficientStagewisePretrainingviaProgressiveSubnetworks
Generalformulationforgradientw.r.tweights: Thepopulationgradientsofw isgivenby
ℓ
(cid:32) √ √ (cid:33)
3 3
E p +sin(⟨w ,x⟩+b )− − x +x x ·cos(⟨w ,x⟩+b )·x
x∼{−1,+1}d 0 ℓ ℓ 2 2 1 1 2 ℓ ℓ
(cid:32)√ (cid:33)
3
=E sin(⟨w ,x⟩+b )·cos(⟨w ,x⟩+b )·x−E x −x x ·cos(⟨w ,x⟩+b )·x
x∼{−1,+1}d ℓ ℓ ℓ ℓ x∼{−1,+1}d 2 1 1 2 ℓ ℓ
(cid:32) √ (cid:33)
3
+E p − ·cos(⟨w ,x⟩+b )·x.
x∼{−1,+1}d 0 2 ℓ ℓ
Wewillargueaboutthecontributionsofthethreetermsseparately.
LemmaE.6(ContributionofTerm1). Foranycoordinatej ∈[d],
E sin(⟨w ,x⟩+b )·cos(⟨w ,x⟩+b )x
x∼{−1,+1}d ℓ ℓ ℓ ℓ j
 
1 (cid:89)
= 2cos(2b ℓ) cos(2w ℓ,i)sin(2w ℓ,j).
i=1→d,i̸=j
Proof. Term1inthegradientsisgivenas
E sin(⟨w ,x⟩+b )·cos(⟨w ,x⟩+b )·x
x∼{−1,+1}d ℓ ℓ ℓ ℓ
1
= E sin(2⟨w ,x⟩+2b )·x.
2 x∼{−1,+1}d ℓ ℓ
Consideracoordinatej ∈[d]. Thegradientconcerningthecoordinateisgivenas
1
E sin(2⟨w ,x⟩+2b )x
2 x∼{−1,+1}d ℓ ℓ j
 
1 (cid:88)
= 2E
x∼{−1,+1}d
sin2 w ℓ,ix i+2w ℓ,jx
j
+2b ℓx
j
i=1→d,i̸=j
 
1 (cid:89)
= 2cos(2b ℓ) cos(2w ℓ,i)sin(2w ℓ,j),
i=1→d,i̸=j
usingLemmaE.7.
ContributionofTerm2: Term2inthegradientsisgivenas
(cid:32)√ (cid:33)
3
E x −x x ·cos(⟨w ,x⟩+b )·x.
x∼{−1,+1}d 2 1 1 2 ℓ ℓ
27EfficientStagewisePretrainingviaProgressiveSubnetworks
Considerthecoordinatei=1. Thegradientconcerningthecoordinateisgivenas
(cid:32)√ (cid:33)
3
E x −x x ·cos(⟨w ,x⟩+b )x
x∼{−1,+1}d 2 1 1 2 ℓ ℓ 1
√
3
= E cos(⟨w ,x⟩+b )−E x cos(⟨w ,x⟩+b ) Replacex2by1
2 x∼{−1,+1}d ℓ ℓ x∼{−1,+1}d 2 ℓ ℓ 1
√  
3 (cid:89)
=
2
E x∼{−1,+1}dcos(⟨w ℓ,x⟩+b ℓ)+sin(b ℓ) cos(w ℓ,i)sin(w ℓ,2)
i=1→d,i̸=2
SimplifiedsecondtermwithLemmaE.8
√ (cid:32) (cid:33)  
3 (cid:89) (cid:89)
=
2
cos(b ℓ) cos(w ℓ,i) +sin(b ℓ) cos(w ℓ,i)sin(w ℓ,2).
i=1→d i=1→d,i̸=2
SimplifiedfirsttermwithCorollaryE.9
Considerthecoordinatei=2. Thegradientconcerningthecoordinateisgivenas
(cid:32)√ (cid:33)
3
E x −x x ·cos(⟨w ,x⟩+b )x
x∼{−1,+1}d 2 1 1 2 ℓ ℓ 2
√
3
= E cos(⟨w ,x⟩+b )x x −E x cos(⟨w ,x⟩+b ) Replacex2by1
2 x∼{−1,+1}d ℓ ℓ 1 2 x∼{−1,+1}d 1 ℓ ℓ 2
√     
3 (cid:89) (cid:89) (cid:89)
=−
2
cos(b ℓ) cos(w ℓ,i) sin(w ℓ,j)+sin(b ℓ) cos(w ℓ,i)sin(w ℓ,1).
i=1→d,i∈/{1,2} j∈{1,2} i=1→d,i̸=1
UsingCorollaryE.9
Foranyothercoordinatet∈/ {1,2},wehave
(cid:32)√ (cid:33)
3
E x −x x ·cos(⟨w ,x⟩+b )x
x∼{−1,+1}d 2 1 1 2 ℓ ℓ t
√
3
= E cos(⟨w ,x⟩+b )x x −E x x x cos(⟨w ,x⟩+b )
2 x∼{−1,+1}d ℓ ℓ 1 t x∼{−1,+1}d 1 2 t ℓ ℓ
√   
3 (cid:89) (cid:89)
=−
2
cos(b ℓ) cos(w ℓ,i) sin(w ℓ,j)
i=1→d,i∈/{1,t} j∈{1,t}
  
(cid:89) (cid:89)
−sin(b ℓ) cos(w ℓ,i) sin(w ℓ,j). UsingCorollaryE.9
i=1→d,i∈/{1,2,t} j∈{1,2,t}
ContributionofTerm3: Foranycoordinatej ∈[d],
(cid:32) √ (cid:33)
3
E p − ·cos(⟨w ,x⟩+b )x
x∼{−1,+1}d 0 2 ℓ ℓ j
(cid:32) √ (cid:33)  
3 (cid:89)
=− p 0−
2
sin(b ℓ) cos(w ℓ,i)sin(w ℓ,j)
i=1→d,i̸=j
=O(η),
(cid:12) √ (cid:12)
wherethepre-finalstepfollowsfromusingCorollaryE.9andthefinalstepfollowssince(cid:12)p − 3(cid:12)=O(η)fromtheinitial
(cid:12) 0 2 (cid:12)
biastrainingphase.
28EfficientStagewisePretrainingviaProgressiveSubnetworks
Thus,thecombinationofthe3termsgives
√ (cid:32) (cid:33)  
3 (cid:89) (cid:89)
∇w
ℓ,1
=−
2
cos(b ℓ) cos(w ℓ,i) −sin(b ℓ) cos(w ℓ,i)sin(w ℓ,2)
i=1→d i=1→d,i̸=2
 
1 (cid:89)
+ 2cos(2b ℓ) cos(2w ℓ,i)sin(2w ℓ,1)+O(η) (8)
i=1→d,i̸=1
√     
3 (cid:89) (cid:89) (cid:89)
∇w
ℓ,2
=
2
cos(b ℓ) cos(w ℓ,i) sin(w ℓ,j)−sin(b ℓ) cos(w ℓ,i)sin(w ℓ,1)
i=1→d,i∈/{1,2} j∈{1,2} i=1→d,i̸=1
 
1 (cid:89)
+ 2cos(2b ℓ) cos(2w ℓ,i)sin(2w ℓ,2)+O(η) (9)
i=1→d,i̸=2
Fort∈/ {1,2},
√   
3 (cid:89) (cid:89)
∇w
ℓ,t
=
2
cos(b ℓ) cos(w ℓ,i) sin(w ℓ,j)
i=1→d,i∈/{1,t} j∈{1,t}
  
(cid:89) (cid:89)
+sin(b ℓ) cos(w ℓ,i) sin(w ℓ,j)
i=1→d,i∈/{1,2,t} j∈{1,2,t}
 
1 (cid:89)
+ 2cos(2b ℓ) cos(2w ℓ,i)sin(2w ℓ,t)+O(η) (10)
i=1→d,i̸=t
Generalformulationforgradientw.r.tbias: Followingasimilarapproachastheweights,thepopulationgradientsofb
ℓ
isgivenby
(cid:32) √ √ (cid:33)
3 3
E sin(⟨w ,x⟩+b )− − x +x x ·cos(⟨w ,x⟩+b )
x∼{−1,+1}d ℓ ℓ 2 2 1 1 2 ℓ ℓ
=E sin(⟨w ,x⟩+b )cos(⟨w ,x⟩+b )
x∼{−1,+1}d ℓ ℓ ℓ ℓ
(cid:32)√ (cid:33)
3
−E x −x x cos(⟨w ,x⟩+b )
x∼{−1,+1}d 2 1 1 2 ℓ ℓ
(cid:32) √ (cid:33)
3
+E p − cos(⟨w ,x⟩+b )
x∼{−1,+1}d 0 2 ℓ ℓ
Followingsimilarapproachasabove,thetermscanbesimplifiedas
(cid:32) (cid:33)
1 (cid:89)
sin(2b ) cos(2w ) (11)
2 ℓ ℓ,i
i=1→d
√  
3 (cid:89)
+
2
sin(b ℓ) cos(w ℓ,i)sin(w ℓ,1)
i=1→d,i̸=1
  
(cid:89) (cid:89)
−cos(b ℓ) cos(w ℓ,i) sin(w ℓ,j)+O(η). (12)
i=1→d,i∈/{1,2} j∈{1,2}
29EfficientStagewisePretrainingviaProgressiveSubnetworks
Behaviorofgradientsatinitialization: Sincethecoordinatesofw andbiasesb areinitializedfrom0s,wehaveforall
ℓ ℓ
j,
sin(b )=0, sin(w )=0, cos(w )=1, cos(b )=1. (13)
ℓ ℓ,j ℓ,j ℓ
√
Furthermore,recallthatp = 3/2+O(η)aftertheinitialbiastrainingphase.
0
Usingtheabove,wecansimplifythegradientsfromEquations(8)to(10)and(12)
√
3
∇w =− +O(η), (14)
ℓ,1 2
∇w =O(η), t̸=1 (15)
ℓ,t
∇b =O(η), (16)
ℓ
Hence,weobservethreekindsofbehavioratinitialization.
1. Firstcoordinateoftheweightgrowsbyatleast
√
η 3
w ←w + +O(η2).
ℓ,1 ℓ,1 2
2. OthercoordinatesoftheweightgetonlyO(η2)updates.
w ←w +O(η2).
ℓ,j ℓ,j
3. Themagnitudeofthebiasdropsaswell.
b ←b +O(η2).
ℓ ℓ
BeyondInitialization: Duetoincreasingmagnitudeofw ,thebehaviorofthegradientschangesslightlyfromEqua-
ℓ,1
tions(14)to(16). However,assumingthattheweightcoordinates[2,d]andthebiasesaresmallerthanO(η),wecanstill
givesimilarformulationsasbeforeusingEquation(13)usingthefollowinginequalitiesforallj ̸=1.
sin(b )=O(η), sin(w )=1−O(η).
ℓ ℓ,j
Westatethemdirectly.
√ (cid:32) (cid:33)  
3 (cid:89) (cid:89)
∇w
ℓ,1
=−
2
cos(b ℓ) cos(w ℓ,i) −sin(b ℓ) cos(w ℓ,i)sin(w ℓ,2)
i=1→d i=1→d,i̸=2
 
1 (cid:89)
+ 2cos(2b ℓ) cos(2w ℓ,i)sin(2w ℓ,1)+O(η)
i=1→d,i̸=1
(cid:32) √ (cid:33) (cid:32) √ (cid:33)
3 1 3
=(1−O(η))d sin(w )− +O(η)≤ sin(w )− +O(η), (17)
ℓ,1 2 2 ℓ,1 2
forη ≤1/poly(d).
√     
3 (cid:89) (cid:89) (cid:89)
∇w
ℓ,2
=
2
cos(b ℓ) cos(w ℓ,i) sin(w ℓ,j)−sin(b ℓ) cos(w ℓ,i)sin(w ℓ,1)
i=1→d,i∈/{1,2} j∈{1,2} i=1→d,i̸=1
 
1 (cid:89)
+ 2cos(2b ℓ) cos(2w ℓ,i)sin(2w ℓ,2)+O(η)
i=1→d,i̸=2
=O(η). (18)
30EfficientStagewisePretrainingviaProgressiveSubnetworks
Foranyt∈/ {1,2},
√   
3 (cid:89) (cid:89)
∇w
ℓ,j
=
2
cos(b ℓ) cos(w ℓ,i) sin(w ℓ,j)
i=1→d,i∈/{1,t} j∈{1,t}
  
(cid:89) (cid:89)
+sin(b ℓ) cos(w ℓ,i) sin(w ℓ,j)
i=1→d,i∈/{1,2,t} j∈{1,2,t}
 
1 (cid:89)
+ 2cos(2b ℓ) cos(2w ℓ,i)sin(2w ℓ,t)+O(η),
i=1→d,i̸=t
=O(η). (19)
Andfinallyforthebias,
(cid:32) (cid:33) √  
1 (cid:89) 3 (cid:89)
∇b
ℓ
= 2sin(2b ℓ) cos(2w ℓ,i) +
2
sin(b ℓ) cos(w ℓ,i)sin(w ℓ,1)
i=1→d i=1→d,i̸=1
  
(cid:89) (cid:89)
−cos(b ℓ) cos(w ℓ,i) sin(w ℓ,j)+O(η) (20)
i=1→d,i∈/{1,2} j∈{1,2}
=O(η). (21)
Thus,weobservethreeproperties.
√
• Firstcoordinateoftheweightgrowsuntilsin(w )= 3 orw reachesπ/3,
ℓ,1 2 ℓ,1
(cid:32)√ (cid:33)
η 3
w ←w + −sin(w ) +O(η2).
ℓ,1 ℓ,1 2 2 ℓ,1
Thus,inΘ(1/η)steps,w canreacharbitrarilyclosetoπ/3.
ℓ,1
• Anyothercoordinatet̸=1stillreceivesO(η2)updatesas
w ←w +O(η2).
ℓ,t ℓ,t
Hence,inΘ(1/η)steps,w canonlyreachO(η)magnitude.
ℓ,t
• b alsoreceivesO(η2)updatesas
ℓ
b ←b +O(η2).
ℓ ℓ
Hence,inΘ(1/η)steps,b canonlyreachO(η)magnitude.
ℓ
E.1.3.PROOFOFLEMMAE.5
LemmaE.5. [Phase2of RAPTR]Forasmallenoughlearningrateη < 1 ,startingfromtheweightsandbiases
poly(d)
reachedbyΘ(1/η)stepsofpositionbiasonlyandfirstphaseof RAPTRtraining(LemmasE.3andE.4),afterΘ(1/η)steps
ofsecondphaseof RAPTRtraining,theweightsandbiasesw 2andb 2satisfythefollowingconditions.
(cid:12) √ π(cid:12)
(cid:12)w + 3/2− (cid:12)≤O(η),
(cid:12) 2,1 2(cid:12)
(cid:12) π(cid:12)
(cid:12)w − (cid:12)≤O(η),
(cid:12) 2,2 2(cid:12)
(cid:12) √ π(cid:12)
(cid:12)b + 3/2− (cid:12)≤O(η),
(cid:12) 2 2(cid:12)
|w |≤O(η), for3≤j ≤d.
2,j
31EfficientStagewisePretrainingviaProgressiveSubnetworks
Proof. TheprooffollowsalongsimilarlinesasLemmaE.4.
Firstlayeroutput: Attheendofthefirstphasetraining,theweightsw ,w andthebiaseslookasfollows.Forℓ∈{1,2},
1 2
(cid:12) π(cid:12)
(cid:12)w − (cid:12)≤O(η),|w |≤O(η), forallj ≥2,|b |≤O(η),
(cid:12) ℓ,1 3(cid:12) ℓ,j ℓ
(cid:12) √ (cid:12)
and(cid:12)p − 3(cid:12)≤O(η).
(cid:12) 0 2 (cid:12)
Hence,usingaTaylorexpansion,theoutputofthefirstlayerisgivenas(forsimplicity,wedenoteitaso(1))
√
3
o(1) :=sin(⟨w ,x⟩+b )= x +O(η). (22)
1 1 2 1
Secondlayeroutput: Theoutputofthesecondlayerisnowgivenby(forsimplicity,wedenoteitaso(2))
(cid:16) (cid:17)
o(2) :=sin ⟨w ,x⟩+o(1)+p +b
2 0 2
=sin(⟨w ,x⟩+sin(⟨w ,x⟩+b )+p +b )
2 1 1 0 2
(cid:32) (cid:32)√ (cid:33) (cid:33)
3 √
=sin ⟨w ,x⟩+ x +O(η) +( 3/2+O(η))+b
2 2 1 2
(cid:32) √ (cid:33)
3 √
=sin ⟨w ,x⟩+ x + 3/2+b +O(η) WithTaylorexpansion
2 2 1 2
 
√ d √
(cid:88)
=sin(w 2,1+ 3/2)x 1+ w 2,jx
j
+ 3/2+b 2+O(η).
j=2
Forbrevity,wewillusenewnotationstorepresentthecoefficientsofx ,··· ,x intheaboveformulation.
1 d
√
w˜ =w + 3/2,
1 2,1
w˜ =w , forj ≥2,
j 2,j
√
˜b= 3/2+b .
2
Atinitialization,theircorrespondingvaluesare
π √
w˜ = + 3/2+O(η),
1 3
w˜ =O(η), forallj ≥2
j
√
3
˜b= +O(η). (23)
2
Then,theaboveformulationisgivenas
(cid:16) (cid:17)
o(2) =sin ⟨w˜,x⟩+˜b +O(η). (24)
Generalformulationforpopulationgradients: Thegradientsw.r.t. theweightw andb aregivenas
2 2
(cid:32) √ (cid:33)
3 (cid:16) (cid:17)
∇w =E o(1)+o(2)− x +x x cos ⟨w˜,x⟩+˜b x+O(η).
2 x∼{−1,+1}d 2 1 1 2
(cid:32) √ (cid:33)
3 (cid:16) (cid:17)
∇b =E o(1)+o(2)− x +x x cos ⟨w˜,x⟩+˜b +O(η).
2 x∼{−1,+1}d 2 1 1 2
32EfficientStagewisePretrainingviaProgressiveSubnetworks
Wewillconsiderthefirsttwotermsintheabovegradientformulationsandaddtheerrortermlater.
Firstofall,weobservethat
√
3
o(1)+o(2)− x +x x
2 1 1 2
(cid:16) (cid:17)
=sin ⟨w˜,x⟩+˜b +x x +O(η).
1 2
Wehavethegradientsw.r.t. w as
2
(cid:16) (cid:17) (cid:16) (cid:17)
∇w =E sin ⟨w˜,x⟩+˜b cos ⟨w˜,x⟩+˜b x
2 x∼{−1,+1}d
(cid:16) (cid:17)
+E x x cos ⟨w˜,x⟩+˜b x
x∼{−1,+1}d 1 2
+O(η).
Wetreatthetwotermsseparately.
ContributionofTerm1 : UsingasimilarstrategyasLemmaE.6,foranycoordinatet∈[d],
(cid:16) (cid:17) (cid:16) (cid:17)
E sin ⟨w˜,x⟩+˜b cos ⟨w˜,x⟩+˜b x
x∼{−1,+1}d t
 
= 21 cos(2˜b) (cid:89) cos(2w˜ i)sin(2w˜ t).
i=1→d,i̸=t
ContributionofTerm2: Forcoordinatet=2,
(cid:16) (cid:17)
E x x cos ⟨w˜,x⟩+˜b x
x∼{−1,+1}d 1 2 2
(cid:16) (cid:17)
=E cos ⟨w˜,x⟩+˜b x Replacex2by1
x∼{−1,+1}d 1 2
 
=−sin(˜b) (cid:89) cos(w˜ i)sin(w˜ 1). UsingCorollaryE.9
i=1→d,i̸=1
Forcoordinatet=1,
(cid:16) (cid:17)
E x x cos ⟨w˜,x⟩+˜b x
x∼{−1,+1}d 1 2 1
(cid:16) (cid:17)
=E cos ⟨w˜,x⟩+˜b x Replacex2by1
x∼{−1,+1}d 2 1
 
=−sin(˜b) (cid:89) cos(w˜ i)sin(w˜ 2). UsingCorollaryE.9
i=1→d,i̸=2
Foranyothercoordinatet∈/ {1,2},
(cid:16) (cid:17)
E cos ⟨w˜,x⟩+˜b x x x
x∼{−1,+1}d 1 2 t
  
=sin(˜b) (cid:89) cos(w˜ i) (cid:89) sin(w˜ t). UsingCorollaryE.9
i=1→d,i∈/{1,2,t} i∈/{1,2,t}
33EfficientStagewisePretrainingviaProgressiveSubnetworks
Combinationofallterms: Forcoordinatet=1,
   
∇w
2,1
=1 2cos(2˜b) (cid:89) cos(2w˜ i)sin(2w˜ 1)−sin(˜b) (cid:89) cos(w˜ i)sin(w˜ 2)+O(η). (25)
i=1→d,i̸=1 i=1→d,i̸=2
Forcoordinatet=2,
   
∇w
2,2
= 21 cos(2˜b) (cid:89) cos(2w˜ i)sin(2w˜ 2)−sin(˜b) (cid:89) cos(w˜ i)sin(w˜ 1)+O(η). (26)
i=1→d,i̸=2 i=1→d,i̸=1
Foranyothercoordinatet∈/ {1,2},
    
∇w
2,t
= 1 2cos(2˜b) (cid:89) cos(2w˜ i)sin(2w˜ t)−sin(˜b) (cid:89) cos(w˜ i) (cid:89) sin(w˜ i)+O(η).
i=1→d,i̸=t i=1→d,i∈/{1,2,t} i∈{1,2,t}
(27)
Wehavesimilarcomputationforthegradientofthebiasb ,whichwereportdirectly.
2
(cid:32) √ (cid:33)
3 (cid:16) (cid:17)
∇b =E o(1)+o(2)− x +x x cos ⟨w˜,x⟩+˜b +O(η)
2 x∼{−1,+1}d 2 1 1 2
(cid:32) (cid:33)  
= 21 sin(2˜b) (cid:89) cos(2w˜ i) −cos(˜b) (cid:89) cos(w˜ i)sin(w˜ 1)sin(w˜ 2)+O(η). (28)
i=1→d i=1→d,i∈/{1,2}
√
Atinitialization: With˜bbeinginitialziedat 3 +O(η)(fromEquation(23)),
2
√
cos(˜b)=cos( 3/2)+O(η)>0
√
cos(2˜b)=cos( 3)+O(η)<0
√
sin(˜b)=sin( 3/2)+O(η)>0.
√
sin(2˜b)=sin( 3)+O(η)>0.
√
Theinitialvalueofw˜ is 3 + π +O(η)(fromEquation(23))andhencethevaluesof
1 2 3
√
cos(2w˜ )=cos( 3+2π/3)+O(η)<0
1
√
sin(2w˜ )=sin( 3+2π/3)+O(η)<0
1
√
cos(w˜ )=cos( 3/2+π/3)+O(η)<0
1
√
sin(w˜ )=sin( 3/2+π/3)+O(η)>0.
1
Theothercoordinatesw˜ areoforderO(η). Thus,thebehavioroftheweightsandbiasesatinitializationcanbesummarized
j
asfollows.
• w getsadecreasingupdateasboththetermsinvolvedinEquation(25)arepositive.
2,1
(cid:12)   (cid:12) (cid:12)   (cid:12)
(cid:12) (cid:12) (cid:12) (cid:12)
−η∇w 2,1 =−η(cid:12) (cid:12) (cid:12)1 2cos(2˜b) (cid:89) cos(2w˜ i)sin(2w˜ 1)(cid:12) (cid:12) (cid:12)−η(cid:12) (cid:12) (cid:12)sin(˜b) (cid:89) cos(w˜ i)sin(w˜ 2)(cid:12) (cid:12) (cid:12)+O(η2)
(cid:12) i=1→d,i̸=1 (cid:12) (cid:12) i=1→d,i̸=2 (cid:12)
η (cid:12) √ (cid:12) (cid:12) √ (cid:12) η √ √
=− (cid:12)cos( 3)(cid:12)(1−O(2η))d−1(cid:12)sin( 3+2π/3)(cid:12)+O(η2)≤− cos( 3)sin( 3+2π/3)+O(η2),
2 (cid:12) (cid:12) (cid:12) (cid:12) 4
asη =O(1/poly(d))small.
34EfficientStagewisePretrainingviaProgressiveSubnetworks
• w getsanincreasingupdateasboththetermsinvolvedinEquation(26)arenegative.
2,2
(cid:12) (cid:12) (cid:12) (cid:12)
−η∇w 2,2 = η 2 (cid:12) (cid:12) (cid:12)cos(2˜b)(cid:12) (cid:12) (cid:12)(cid:12) (cid:12) (cid:12) (cid:12) (cid:89) cos(2w˜ i)(cid:12) (cid:12) (cid:12) (cid:12)|sin(2w˜ 2)|+(cid:12) (cid:12) (cid:12)sin(˜b)(cid:12) (cid:12) (cid:12)(cid:12) (cid:12) (cid:12) (cid:12) (cid:89) cos(w˜ i)(cid:12) (cid:12) (cid:12) (cid:12)|sin(w˜ 1)|+O(η2)
(cid:12) i=1→d,i̸=2 (cid:12) (cid:12) i=1→d,i̸=1 (cid:12)
√ √ η √ √
=ηsin( 3)sin( 3/2+π/3)(1−O(η))d−1+O(η2)≥ sin( 3)sin( 3/2+π/3)+O(η2),
2
asη =O(1/poly(d))small.
• Othercoordinatesw fort∈/ {1,2}getsmallgradientsasboththeinvolvedtermsinEquation(27)dependonhow
2,t
largew˜ :=w =O(η)is.
t 2,t
 
−η∇w
2,t
=−η
2
cos(2˜b) (cid:89) cos(2w˜ i)sin(2w˜ t)
i=1→d,i̸=t
  
+ηsin(˜b) (cid:89) cos(w˜ i) (cid:89) sin(w˜ i)+O(η2)=O(η2).
i=1→d,i∈/{1,2,t} i∈{1,2,t}
• Biasb getanincreasingupdateasboththeinvolvedtermsinEquation(28)arenegative.
2
−η∇b 2 = η 2 (cid:12) (cid:12) (cid:12)sin(2˜b)(cid:12) (cid:12) (cid:12)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)(cid:32) i=(cid:89) 1→dcos(2w˜ i)(cid:33)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)+η(cid:12) (cid:12) (cid:12)cos(˜b)(cid:12) (cid:12) (cid:12)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)  i=1→(cid:89) d,i∈/{1,2}cos(w˜ i) sin(w˜ 1)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)|sin(w˜ 2)|+O(η)
η √ (cid:12) √ (cid:12) η √ (cid:12) √ (cid:12)
= sin( 3)(cid:12)cos( 3+2π/3)(cid:12)(1−O(η))d−1+O(η)≥ sin( 3)(cid:12)cos( 3+2π/3)(cid:12).
2 (cid:12) (cid:12) 4 (cid:12) (cid:12)
Beyondinitialization: Infact,wecanextendtheaboveargumentbeyondinitialization,wherew decreasesandw ,b
2,1 2,2 2
increasebyΘ(η)updates,upuntilwereachtheconditions
(cid:12) π(cid:12) (cid:12) π √ (cid:12)
(cid:12)w˜ − (cid:12)≤O(η), (or) (cid:12)w − + 3/2(cid:12)≤O(η),
(cid:12) 1 2(cid:12) (cid:12) 2,1 2 (cid:12)
(cid:12) π(cid:12) (cid:12) π(cid:12)
(cid:12)w˜ − (cid:12)≤O(η), (or) (cid:12)w − (cid:12)≤O(η)
(cid:12) 2 2(cid:12) (cid:12) 2,2 2(cid:12)
(cid:12) π(cid:12) (cid:12) π √ (cid:12)
(cid:12)˜b− (cid:12)≤O(η), (or) (cid:12)b − + 3/2(cid:12)≤O(η).
(cid:12) 2(cid:12) (cid:12) 2 2 (cid:12)
Since,wegetΘ(η)updatesineachstep,thisconditioncanbereachedinΘ(1/η)steps.
Theargumentisasfollows. Foranyw˜ ,w˜ ,˜bsatisfyingthefollowingranges,
1 2
√
π 3 π
w˜ ∈( , + ),
1 2 2 3
π
w˜ ∈(0, ),
2 2
√ π
˜b∈( 3/2, ),
2
thefollowingconditionsholdtrue.
cos(˜b), sin(˜b), sin(2˜b)>0, cos(2˜b)<0
cos(2w˜ ), sin(2w˜ ), cos(w˜ )<0, sin(w˜ )>0.
1 1 1 1
sin(w˜ ), cos(w˜ ), sin(2w˜ )>0, cos(2w˜ )<0.
2 2 2 2
Wecanthenshowthatthetermsinvolvedintheupdateruleofw (Equation(25))arepositive,implying−η∇w <−Ω(η)
2,1 2,1
upuntilw˜ reachesO(η)closeto π. Similarargumentcanbegivenfortheupdatesofb andw respectively.
1 2 2 2,2
Furthermore, the coordinates w for t ∈/ {1,2} get O(η2) updates per step and stay O(η) small for Θ(1/η) steps of
2,t
training.
35EfficientStagewisePretrainingviaProgressiveSubnetworks
E.2.UsefulLemmas
LemmaE.7.
 
(cid:89)
E
x∼{−1,+1}d
sin(⟨w,x⟩+b)x
j
=cos(b) cos(w i)sin(w j).
i=1→d,i̸=j
Proof.
E sin(⟨w,x⟩+b)x
x∼{−1,+1}d j
 
(cid:88)
=E
x∼{−1,+1}d
sin w ix i+w jx
j
+bx
j
i=1→d,i̸=j
   
(cid:88)
=cos(b)E x∼{−1,+1}dsin w ix i·E xjcos(w jx j)x j Equals0asanoddfunction
i=1→d,i̸=j
   
(cid:88)
+cos(b)E x∼{−1,+1}dcos w ix i·E xjsin(w jx j)x j Equals0asanoddfunction
i=1→d,i̸=j
   
(cid:88)
+sin(b)E x∼{−1,+1}dcos w ix i·E xjcos(w jx j)x j
i=1→d,i̸=j
   
(cid:88)
−sin(b)E x∼{−1,+1}dsin w ix i·E xjsin(w jx j)x j Equals0asanoddfunction
i=1→d,i̸=j
   
(cid:88)
=cos(b)E x∼{−1,+1}dcos w ix i·E xjsin(w jx j)x j
i=1→d,i̸=j
 
(cid:89)
=cos(b) cos(w i)sin(w j). Uselemmas....
i=1→d,i̸=j
LemmaE.8.
 
(cid:89)
E
x∼{−1,+1}d
cos(⟨w,x⟩+b)x
j
=−sin(b) cos(w i)sin(w j).
i=1→d,i̸=j
Proof.
E cos(⟨w,x⟩+b)x
x∼{−1,+1}d j
(cid:16)π (cid:17)
=E cos −⟨w,x⟩−b x
x∼{−1,+1}d 2 j
 
(cid:89)
=cos(π/2−b) cos(−w i)sin(−w j) UsingLemmaE.7
i=1→d,i̸=j
 
(cid:89)
=−sin(b) cos(w i)sin(w j).
i=1→d,i̸=j
36EfficientStagewisePretrainingviaProgressiveSubnetworks
Eval Loss Eval Loss
5×100
5×100 No Scale ={}
Linear Scale 4×100 ={1}
4×100
Sqrt Scale ={1,12}
3×100 3×100
20000 40000 20000 40000
Training steps Training steps
Figure6.AblationstudyonchoicesforRAPTRalgorithm.WetrainBERT-basewithRAPTRfor100kstepswitha6-8-10-12schedule
(seeSection4).Lefttoright:(a)Square-rootscaling(h )hasbettervalidationlosscomparedwithlinearscalingandnoscalingat
sqrt
training,especiallyintheearlierstages.Withnoscale,wescalelayer’soutputby1/p duringinference(following(Huangetal.,2016)).
i
(b)DifferentcandidatesforthefixedsetIarecomparedforRAPTR. Wefindthattrainingwithfirstandlastlayersfixedhelpsfaster
training.
CorollaryE.9. ConsiderasetS ⊆[d].
  
(cid:89) (cid:89) (cid:89)
E x∼{−1,+1}d cos(⟨w,x⟩+b) x i =c b cos(w i) sin(w j),
j∈S i=1→d,i∈/S j∈S
where
 −sin(b), if|S|=4t+1forsomet∈N
−cos(b),
if|S|=4t+2forsomet∈N
c =
b s ci on s( (b b) ),
,
i if f| |S S| |= =4 4t t+ for3 sf oo mr eso tm ∈e Nt .∈N
F.Scaling
Whileworkingonsubnetworks,itisimportanttoappropriatelyrescalethenetwork. Inparticular,bypassinglayer(s)in
RAPTRcanshifttheinputdistributionforlayerscomparedtothefullmodel. Tomitigatethisshift,wescaletheoutputof
thelayerstomaintainthenormsoftheinputtoeachlayerthroughouttraining. Theideaofscalinglayeroutputshasalso
beenexploredinpriorwork(Huangetal.,2016;Fanetal.,2019).Weuseadifferentsquare-rootscalingmechanismthatis
motivatedbyanalysisonarandomlyinitializedTransformerarchitecture. Weshowthatatinitialization,thenormofthe
√
intermediateoutputsy(ℓ)scaleas ℓ.
TheoremF.1(Square-rootscalingofinputnorms). Atinitialization,givenanyinputsequencex andscalarsα ,w.h.p.
1:N 1:L
theintermediatesequencesofF (Definition2.1)satisfy
(cid:13) (cid:13)y(ℓ)(cid:13) (cid:13)2
=∥x
∥2+Θ((cid:88)ℓ
α2d), forall1≤i≤N,1≤ℓ≤L.
(cid:13) i (cid:13) i 2 j
2
j=1
Inspiredbythis,wedefinethefunctionh thattakesinasequenceofbinaryvaluesζ ∈{0,1}Landreturnsscalars
sqrt 1:L
ζ whereζ isthescalingforoutputoflayeri. Wefindascalingthatsatisfiesthefollowingtwoconditionshold(a)
1:L i
(cid:80)j
ζ
2
=j forallj ≤L(maintainingthenorm),and(b)ζ =0ifζ =0foralli≤L(maintaininglayersinrandom
i=1 i i i
subnetwork). Formally,foreachindexj withζ
j
=1,itfindstheindexminimum¯j >j withζ¯j =1andsetsζ
j
=(cid:112)¯j−j.
TheRAPTRalgorithmwithsquarerootscalingispresentedinAlgorithm1. InFigure6wecomparesquarerootscaling
withsomeotherscalingmethodsandfindittobeslightlybetter.
37EfficientStagewisePretrainingviaProgressiveSubnetworks
F.1.Proofs
Forsimplicity,wepresenttheresultsfortransformerswithsingleheadintheself-attentionlayer. Furthermore,forsimplicity,
weuseσ activationintheMLPs.
relu
Algorithm3TransformerLayer
Require: 2layernormalizationlayersfLN,fLN (F.2),anMLPlayerfmlp(F.4),andasoftmaxself-attentionlayerfattn
attn mlp
(F.3),inputsequence{x ∈Rd}N .
n n=1
1: AttentionLayernormalization: returns{y nattnln}N n=1withy nattnln =f aL tN tn(x n)foralln≤N.
2: Softmaxself-attention: returns{yattn}N =fattn({yattnln}N ).
n n=1 n n=1
3: Residualconnection: returns{y nattnblock}N n=1,withy nattnblock =x n+y nattnforalln≤N.
4: MLPLayernormalization: returnsymlpln =fLN(yattnblock)foralln≤N.
n mlp n
5: MLPfunction: returns{ymlp}N withymlp =fmlp(ymlpln)foralln≤N.
n n=1 n n
6: Computey n =y nmlp+y nattnblock foralln≤N.
7: Return{y n}N
n=1
DefinitionF.2. [LayerNormalization]Defineanormalizationfunctionf : Rd → Rd thatperformsf(x) = (x−µ)/σ,
whereµandσ arethemeanandstandarddeviationofx,respectively. Then,layernormalizationfLN : Rd → Rd with
parametersγ,b∈Rdtakesasinputx∈Rdandoutputsy ∈Rd,whichiscomputedasz =f(x),y =γ⊙z+b.
Definition F.3 (Softmax self-attention). A self-attention layer fattn : RN×d → RN×d with parameters
{W ,W ,W ,Cattn ∈Rd×d}takesasequence{x } andoutputsasequence{y } ,suchthat
Q K V n n≤N n n≤N
N
(cid:88)
y =Cattn a v ,
n n,n n
n=1
witha =softmax(Kq ) , q =W x , k =W x , v =W x ,
n,n n n n Q n n K n n V n
foralln≤N,andK ∈RN×N definedwithrows{k }N .
n n=1
DefinitionF.4(MLP). AnMLPlayerfmlp : Rd → Rd withparameters{W ∈ Rm×d,Cmlp ∈ Rm×d}andactivation
σ ,takesaninputx∈Rdandoutputsy ∈Rd,suchthat
relu
y =Cmlpσ (Wx).
relu
DefinitionF.5(Transformerlayer). Apre-layernormtransformerlayerwiththreesub-layers;2layernormalizationlayers
fLN,fLN (F.2),anMLPlayerfmlp (F.4),andasoftmaxself-attentionlayerfattn (F.3);takesasequence{x and
attn mlp n≤N
outputsasequence{y } infoursteps.
n n≤N
1. Firstcomputes{yattnln}N usingalayernormalization,i.e. yattnln =fLN(x )foralln≤N.
n n=1 n attn n
2. Thenitrunssoftmaxself-attentiontoget{yattn}N =fattn({yattnln}N ).
n n=1 n n=1
3. Thenetoutputoftheself-attentionblockisgivenby{yattnblock}N ,withyattnblock =x +yattnforalln≤N.
n n=1 n n n
4. BeforepassingtotheMLPfunction,itispassedthroughanotherlayernormalization,i.e. ymlpln =fLN(yattnblock)
n mlp n
foralln≤N.
5. MLPfunctionthenreturns{ymlp}N withymlp =fmlp(ymlpln)foralln≤N.
n n=1 n n
6. y =ymlp+yattnblock foralln≤N.
n n n
DefinitionF.6(Initializationoftheweightsinthetransformerlayer). Theweightsareinitializedasfollows:
1
Cmlp,Cattn,W ,W ,W ∼N(0,√ I),
Q K V
d
√
2
W ∼N(0,√ I).
m
Theparametersγ,bofthefunctionsfLN,fLN havebeeninitializedas1and0respectively.
attn mlp
38EfficientStagewisePretrainingviaProgressiveSubnetworks
Lemma F.7 (Norm of the output of the MLP function, modification of lemma 7.1 in (Allen-Zhu et al., 2019)). For a
giveninputsequence{ymlpln} ,ifε∈(0,1],withprobabilityatleast1−O(N)·e−Ω(mε2) overtherandomnessof
n n≤N
Cmlp,W,wehave
∀i∈[N]:
(cid:13) (cid:13)ymlp(cid:13) (cid:13)/(cid:13) (cid:13)ymlpln(cid:13)
(cid:13)∈[1−ε,1+ε].
n n
Lemma F.8 (Norm of the output of the layer normalization layers). Given any input sequence {x }N , under the
n n=1
assumptionforalln≤N,x
−(cid:80)d
x isn’tidentically0,wehave
n i=1 n,i
√
(cid:13) (cid:13)yattnln(cid:13)
(cid:13)= d
n
foralln≤N.
Similarresultholdsfor{ymlpln}N .
n n=1
LemmaF.9(NormoftheoutputoftheMLPblock). Foragiveninputsequence{yattnblock} ,withprobabilityatleast
n n≤N
1−O(1)overtherandomnessofCmlp,W,wehave
∥y n∥2 =(cid:13) (cid:13)y nmlp+y nattnblock(cid:13) (cid:13)2 =(cid:13) (cid:13)y nattnblock(cid:13) (cid:13)2 +d
(cid:32) (cid:33)
+O
(cid:13) (cid:13)yattnblock(cid:13)
(cid:13)logN
+(d+(cid:13) (cid:13)yattnblock(cid:13) (cid:13))log √3/2N
,
n n m
foralln≤N.
Proof. CombiningLemmasF.7andF.8,wehaveforthesequence{yattnblock} ,
n n≤N
√
(cid:12) (cid:12) (cid:12)(cid:13) (cid:13)y nmlp(cid:13) (cid:13)/√ d−1(cid:12) (cid:12) (cid:12)≤O(cid:18) √lo mgN(cid:19) ,
w.p. atleast1−O(1).
Furthermore,duetotherandomnessofCmlp,wecanfurthershowthatw.p. atleast1−O(1),
(cid:12) (cid:12)⟨yattnblock,ymlp⟩(cid:12) (cid:12)≤O((cid:13) (cid:13)yattnblock(cid:13)
(cid:13)logN),
n n n
foralln≤N. Combiningthetworesults,wehave
∥y n∥2 =(cid:13) (cid:13)y nmlp+y nattnblock(cid:13) (cid:13)2 =(cid:13) (cid:13)y nattnblock(cid:13) (cid:13)2 +d
(cid:32) (cid:33)
+O
(cid:13) (cid:13)yattnblock(cid:13)
(cid:13)logN
+(d+(cid:13) (cid:13)yattnblock(cid:13) (cid:13))log √3/2N
.
n n m
Lemma F.10 (Norm of the output of the attention block). For a given input sequence {x } , if ε ∈ (0,1], with
n n≤N
probabilityatleast1−O(1)overtherandomnessofCattn,W ,wehave
V
√
∀i∈[N]: ∥x n∥2 ≤(cid:13) (cid:13)y nattnblock(cid:13) (cid:13)2 ≤∥x n∥2+∥x n∥+d+O( l √ogN (d+∥x n∥)).
d
Proof. Fromthedefinitionsof{yattnln}and{yattn},wehave
n n
(cid:88)
yattn =CattnW a yattnln.
n V n,j j
j≤N
Thus,wecanuseaproofsimilartotheproofofLemmaF.9toarguethatwiththerandomnessofCattnandW ,
V
(cid:12) (cid:12) √
(cid:12) (cid:12)
(cid:12)(cid:13)
∥y nattn∥
(cid:13)
−1(cid:12) (cid:12) (cid:12)≤O(cid:18) l √ogN(cid:19)
,
(cid:12)(cid:13)(cid:80) a yattnln(cid:13) (cid:12) d
(cid:12)(cid:13) j≤N n,j j (cid:13) (cid:12)
39EfficientStagewisePretrainingviaProgressiveSubnetworks
foralln≤N w.p. atleast1−O(1).
Furthermore,duetotherandomnessofCattn,wecanfurthershowthatw.p. atleast1−O(1),
(cid:12) (cid:12)⟨y nattn,x n⟩(cid:12) (cid:12)≤O(∥x n∥logN),
foralln≤N.
UsingCauchy-Schwarzinequality,wemusthave
(cid:13) (cid:13)
(cid:13) (cid:13) √
(cid:13)
(cid:13)
(cid:13)(cid:88)
a n,jy
jattnln(cid:13)
(cid:13) (cid:13)≤m j≤a
Nx(cid:13)
(cid:13)y
jattnln(cid:13)
(cid:13)= d.
(cid:13)j≤N (cid:13)
Thus,combiningtheresults,wehave
(cid:13) (cid:13)y nattnblock(cid:13) (cid:13)2 =(cid:13) (cid:13)y nattn+x n(cid:13) (cid:13)2 ≤∥x n∥2+d
(cid:32) (cid:33)
log3/2N
+O ∥x ∥logN +(d+∥x ∥) √ .
n n
d
Lemma F.11 (Norm of the output of the transformer layer). For a given input sequence {x } , if ε ∈ (0,1], with
n n≤N
probabilityatleast1−O(1)overtherandomnessofCattn,W ,Cmlp,wehave
V
∀i∈[N]: ∥x ∥2+d+O(err)≤∥y ∥2 ≤∥x ∥2+2d+O(err),
n n n
(cid:16) (cid:17)
whereerr =O ∥x n∥logN +(d+∥x n∥)log √3/ m2N .
TheoremF.1(Square-rootscalingofinputnorms). Atinitialization,givenanyinputsequencex andscalarsα ,w.h.p.
1:N 1:L
theintermediatesequencesofF (Definition2.1)satisfy
(cid:13) (cid:13)y(ℓ)(cid:13) (cid:13)2
=∥x
∥2+Θ((cid:88)ℓ
α2d), forall1≤i≤N,1≤ℓ≤L.
(cid:13) i (cid:13) i 2 j
2
j=1
Proof. ThisfollowsfromthefactthatthetransformerarchitectureisastackofstruturallyidenticalLtransformerlayers,
andeachtransformerlayer’soutputnormincreasesbyΘ(d)comparedtoitsinputnorm,asgivenbyLemmaF.11.
40