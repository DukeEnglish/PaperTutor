Prior-Dependent Allocations for Bayesian Fixed-Budget Best-Arm
Identification in Structured Bandits
Nicolas Nguyen∗1, Imad Aouali∗2,3, András György4, and Claire Vernade1
1University of Tübingen
2Criteo AI Lab
3ENSAE CREST, IP Paris
4Google Deepmind
February 9, 2024
Abstract
We study the problem of Bayesian fixed-budget best-arm identification (BAI) in structured bandits.
We propose an algorithm that uses fixed allocations based on the prior information and the structure of
the environment. We provide theoretical bounds on its performance across diverse models, including the
first prior-dependent upper bounds for linear and hierarchical BAI. Our key contribution is introducing
new proof methods that result in tighter bounds for multi-armed BAI compared to existing methods. We
extensively compare our approach to other fixed-budget BAI methods, demonstrating its consistent and
robust performance in various settings. Our work improves our understanding of Bayesian fixed-budget
BAI in structured bandits and highlights the effectiveness of our approach in practical scenarios.
1 Introduction
Best arm identification (BAI) addresses the challenge of finding the optimal arm in a bandit environment
(Lattimore and Szepesvári, 2020), with wide-ranging applications in online advertising, drug discovery or
hyperparameter tuning. BAI is commonly approached through two primary paradigms: fixed-confidence and
fixed-budget. In the fixed-confidence setting (Even-Dar et al., 2006; Kaufmann et al., 2016), the objective is
to find the optimal arm with a pre-specified confidence level. Conversely, fixed-budget BAI (Audibert et al.,
2010; Karnin et al., 2013; Carpentier and Locatelli, 2016) involves identifying the optimal arm within a fixed
number of observations. Within this fixed-budget context, two main metrics are used: the probability of
error (PoE) (Audibert et al., 2010; Karnin et al., 2013; Carpentier and Locatelli, 2016)—the likelihood of
incorrectly identifying the optimal arm—and the simple regret (Bubeck et al., 2009; Russo, 2016; Komiyama
et al., 2023)—the expected performance disparity between the chosen and the optimal arm. We focus on PoE
minimization in fixed-budget BAI.
Existing algorithms for PoE minimization in fixed-budget BAI are largely frequentist and often employ
elimination strategies. Bayesian approaches have predominantly focused on the minimization of the simple
regret (Komiyama et al., 2023; Azizi et al., 2023), or were studied under a frequentist lens (Hoffman et al.,
2014),whichdonotcapturetheadvantagesofknowinginformativepriors. ItwasonlyrecentlythatAtsidakou
et al. (2022) introduced a Bayesian version of the well-known Sequential Halving (SH) algorithm (Karnin
et al., 2013), offering a prior-dependent bound on the probability of error in multi-armed bandits (MAB),
albeit under certain limiting assumptions on the prior. Their proofs are still largely influenced by frequentist
approaches and come with strong constraints.
Severalrecentworkshaveshednewlightonadaptivemethodsforfrequentistfixed-budgetBAI.Forinstance,
Qin (2022); Degenne (2023); Wang et al. (2023) examined whether adaptive algorithms can consistently
surpass the best static algorithm for any bandit instance. Remarkably, Degenne (2023) established the
∗Equalcontribution. Correspondenceto: NicolasNguyen<nicolas.nguyen@uni-tuebingen.de>.
1
4202
beF
8
]LM.tats[
1v87850.2042:viXraabsence of such universally superior adaptive algorithms in several BAI problems, including Gaussian BAI.
Wang et al. (2023) demonstrated that in Bernoulli BAI with two arms, no algorithm consistently outperforms
uniform sampling. Inspired by these recent results, we develop a method based on fixed and non-adaptive
allocations in the Bayesian setting. These allocations leverage both prior information and the structure of
the environment. As demonstrated in our experiments in Section 5, our static but prior-informed algorithm
can outperform adaptive baselines. Moreover, our proofs incorporate fully Bayesian techniques, diverging
from existing works. This novel technical approach not only produces a tighter upper bound but also applies
under milder assumptions.
As a motivating example, consider a scenario with three arms (K =3), where the information a priori
strongly suggests that either of the first two arms is more likely to be optimal than the third one. A pivotal
questionarises: whatstrategicapproachshouldbeemployedtoallocateresources(orbudget)totheseemingly
suboptimal third arm? Furthermore, if greater confidence is placed on the first arm compared to the second,
what is the optimal budget distribution between them? This situation underlines a fundamental challenge
not directly addressed by the frequentist approach, which lacks knowledge about the bandit instance prior to
interaction.
Relation to prior works. While Atsidakou et al. (2022) considers prior information, their method does
not exploit it to its full potential. To maintain adaptivity, they impose restrictive assumptions on the prior.
Moreover, their results are only valid for a specific budget allocation, while ours are applicable for any fixed
and non-adaptive allocation rule. This facilitates the creation of ad-hoc allocation strategies, informed and
guided by our theoretical results. Additionally, a particularly relevant application of prior information is
found in structured bandit problems, such as linear bandits (Abbasi-Yadkori et al., 2011; Hoffman et al., 2014;
Katz-Samuels et al., 2020; Azizi et al., 2021) and hierarchical bandits (Hong et al., 2022b; Aouali et al., 2023),
where arm rewards are determined by underlying latent parameters. Our approach captures the structure of
these problems as reflected in the prior, leading to more efficient exploration thanks to arm correlations. This
aspect of our work also extends beyond the scope of Atsidakou et al. (2022), which primarily addressed MAB.
Contributions. 1) We present and analyze Prior-Informed BAI, PI-BAI, a fixed-budget BAI algorithm
that leverages prior information for efficient exploration. Our main contributions include establishing upper
prior-dependent bounds on its expected PoE in multi-armed, linear, and hierarchical bandits. Specifically, in
theMABsetting,ourupperboundissmallerandisvalidundermilderassumptionsontheprior. 2)Theproof
techniques developed for PI-BAI provide a fully Bayesian perspective, significantly diverging from existing
methodologiesthatrelyonfrequentistproofs. Thisallowsamorecomprehensiveframeworkforunderstanding
and analyzing Bayesian BAI algorithms, while also enabling us to relax previously held assumptions. 3)
Our algorithms and proof techniques are naturally applicable to structured problems, such as linear and
hierarchical bandits, leading to the first Bayesian BAI algorithm with a prior-dependent PoE bound in these
settings. 4) We empirically evaluate PI-BAI and its variants in various numerical setups. Our experiments
on synthetic and real-world data show the generality of PI-BAI and highlight its good performance.
2 Background
Notation. Let ∆ be the K-simplex and ∆+ ={ω ∈∆ ; ω >0, ∀i∈[K]}. For any positive-definite
K K √ K i
matrix A∈Rd×d and vector v ∈Rd, we define ∥v∥ = v⊤Av. Also, λ (A) and λ (A) denote the maximum
A 1 d
and minimum eigenvalues of A, respectively. We denote by e the i-th vector of the canonical basis.
i
We consider a scenario involving K arms. In each round t∈[n], the agent selects an arm A ∈[K], and
t
then receives a stochastic reward Y ∼P(·;θ,A ), where θ is the unknown parameter vector and P(·;θ,i) is
t t
the known reward distribution of arm i, given θ. We denote by r(i;θ) = E [Y] the mean reward
Y∼P(·;θ,i)
of arm i, given θ. We adopt the Bayesian view where θ is assumed to be sampled from a known prior
distribution P . Given a bandit instance characterized by θ ∼P , the goal is to find the (unique) optimal
0 0
arm i (θ)=argmax r(i;θ) by interacting with the bandit instance for a fixed-budget of n rounds. These
∗ i∈[K]
interactions are summarized by the history H ={(A ,Y )} , and we let J ∈[K] be the arm selected by
n t t t∈[n] n
2the agent after n rounds. In this Bayesian setting, Atsidakou et al. (2022) introduced the expected PoE as
P =E(cid:2)P(cid:0) J ̸=i (θ)|θ(cid:1)(cid:3) , (1)
n n ∗
a Bayesian metric that corresponds to the average PoE across all bandit instances sampled from the prior,
θ ∼ P . This is different from the frequentist counterpart where the performance is assessed for a single
0
instance θ.
2.1 Multi-Armed Bandit
In this setting, each component of θ =(θ ) is sampled independently from the prior distribution. We
i i∈[K]
focus on the Gaussian case where θ ∼N(µ ,σ2 ), with µ and σ2 being the known prior reward mean
i 0,i 0,i 0,i 0,i
and variance for arm i. Then, given θ, the reward distribution of arm i is N(θ ,σ2) where σ2 is the (known)
i
observation noise variance1,
θ ∼N(µ ,σ2 ) ∀i∈[K]
i 0,i 0,i
Y |θ,A ∼N(θ ,σ2) ∀t∈[n]. (2)
t t At
Under (2), the posterior distribution of θ given H is a Gaussian N(µˆ ,σ2 ) (Bishop, 2006), where
i n n,i n,i
1 n (cid:16)µ B (cid:17)
σˆ−2 = + i , µˆ =σˆ2 0,i + n,i , (3)
n,i σ2 σ2 n,i n,i σ2 σ2
0,i 0,i
where T = {t ∈ [n], A = i} is the set of rounds when arm i is chosen, n = |T | is the number of times
i t i i
arm i is chosen and B = (cid:80) Y is the sum of rewards of arm i. Here the mean posterior reward is
E[r(i;θ)|H ]=µˆ .
n,i t∈Ti t
n n,i
2.2 Linear Bandit
One major drawback of model (2) is that is is not able to model situations where arms are dependent, thus
leading to suboptimal exploration.
In linear bandits (Abbasi-Yadkori et al., 2011), arms share a common low-dimensional representation
through parameter θ ∈Rd. We denote X ={x ,...x } the set of arms where each x ∈Rd. We focus on the
1 K i
Gaussian case where θ is sampled from a Gaussian distribution with mean µ ∈Rd and covariance matrix
0
Σ ∈Rd×d. Given θ, the reward distribution of arm i is Gaussian with mean r(i;θ)=x⊤θ and variance σ2,
0 i
θ ∼N(µ ,Σ )
0 0
Y |θ,A ∼N(θ⊤x ,σ2) ∀t∈[n]. (4)
t t At
Similarly to (2), this model offers closed-form formulas, where the posterior of θ given the history H
n
containing n samples from arm i is a Gaussian N(µˆ ,Σ ):
i n n
Σˆ−1 =Σ−1+σ−2 (cid:88) n x x⊤, µˆ =Σˆ (cid:0) Σ−1µ +σ−2B (cid:1) , (5)
n 0 i i i n n 0 0 n
i∈[K]
where B =(cid:80) Y x . The mean posterior reward of arm i is given by E[r(i;θ)|H ]=µˆ⊤x . Note that
n t∈[n] t At n n i
the MAB (2) can be recovered from (4) when x =e ∈RK and Σ =diag(σ2 ) .
i i 0 0,i i∈[K]
2.3 Hierarchical Bandit
Another practical model that captures arm correlations is the hierarchical (or mixed-effect) model (Bishop,
2006; Wainwright et al., 2008; Hong et al., 2022b; Aouali et al., 2023), defined in the Gaussian case as
µ∼N(ν,Σ)
θ ∼N(b⊤µ,σ2 ) ∀i∈[K]
i i 0,i
Y |µ,θ,A ∼N(θ ,σ2) ∀t∈[n]. (6)
t t At
1Arm-dependentobservationnoisevariancescouldbeusedbutwechoosetokeepthenotationsimple.
3This generative model reads as follows. First, µ=(µ ) is an unknown latent vector composed of L effects
ℓ ℓ∈[L]
µ and it is sampled from a multivariate Gaussian with mean ν ∈RL and covariance Σ∈RL×L. Then, given
ℓ
µ, the mean rewards θ are independently sampled as θ ∼ N(b⊤µ,σ2 ), where (b ) represent known
i i i 0,i i i∈[K]
mixing weights. In particular, b⊤µ creates a linear mixture of the L effects, with b indicating a known score
i i,ℓ
that quantifies the association between arm i and the effect ℓ. Concrete examples of b are provided in
i,ℓ
Appendix A.1. Note that arm correlations arise because θ are derived from the same effect parameter µ.
i
Finally, given µ and θ, the reward distribution of arm i is similar to the MAB (2) and writes N(θ ,σ2).
i
With abuse of notation, the effect posterior distribution Q (µ)=P(µ|H ) induces a conditional arm
n n
posterior distribution for each arm i, P (θ |µ)=P(θ |H ,µ). Then, the marginal arm posterior density
n,i i i n
can be computed by marginalizing over Q such as P(θ |H )=E [P (θ |µ)]. Therefore, despite the
n i n µ∼Qn n,i i
hierarchical structure, these distributions can be derived in closed-form2. First, the effect posterior writes
Q (µ)=N(µ˘ ,Σ˘ ) with
n n n
Σ˘−1 =Σ−1+ (cid:88) n i b b⊤, µ˘ =Σ˘ (cid:16) Σ−1ν+ (cid:88) B n,i b (cid:17) . (7)
n n σ2 +σ2 i i n n n σ2 +σ2 i
i∈[K] i 0,i i∈[K] i 0,i
Then, given µ∼Q , the conditional arm posteriors are P (θ |µ)=N(µ˜ ,σ˜2 ), where
n n,i i n,i n,i
1 n (cid:16)µ⊤b B (cid:17)
σ˜−2 = + i , µ˜ =σ˜2 i + n,i . (8)
n,i σ2 σ2 n,i n,i σ2 σ2
0,i 0,i
Finally, combining (7) and (8) leads to the marginal arm posterior P(θ |H )=N(µˆ ,σˆ2 ) where
i n n,i n,i
σ˜4 (cid:16)µ˘⊤b B (cid:17)
σˆ2 =σ˜2 + n,ib⊤Σ˘ b , µˆ =σ˜2 n i + n,i , (9)
n,i n,i σ4 i n i n,i n,i σ2 σ2
0,i 0,i
and we have that E[r(i;θ)|H ]=µˆ .
n n,i
Link to linear bandit. (6) is a special case of a linear bandit (4), as can be seen by realizing that
θ =b⊤µ+η where η ∼N(0,σ2 ) and theη are independentof µ. Hence, (6) can berewrittenbyreplacing
i i i i 0,i i
ν with ν¯∈RL+K defined as ν¯⊤ =(ν⊤,0,...,0) and Σ with a block-diagonal matrix Σ¯ =⟨Σ,σ2 ,...,σ2 ⟩
0,1 0,K
with new actions¯b ∈RL+K defined as¯b⊤ =(b⊤,e⊤), leading to a linear bandit
i i i i
µ¯ ∼N(ν¯,Σ¯)
Y |µ¯,A ∼N(¯b⊤ µ¯,σ2) ∀t∈[n]. (10)
t t At
Note that reinterpreting hierarchical bandits in this way does not lead to practical benefits. In contrast,
adhering to the initial formulation in (6) and the subsequent derivations in (9) is more computationally
efficient. Indeed, this is one of the motivations behind the concept of hierarchical bandits.
3 Algorithm and Error Bounds
PI-BAI takes as input a budget n and an arbitrary vector of allocation weights ω =(ω ) ∈∆ . Then, it
i i∈[K] K
collectsn =⌊ω n⌋samplesforeacharmi,andfinallyreturnsthearmwiththehighestposteriormean,defined
i i
as J =argmax E[r(i;θ)|H ]. PI-BAI is described in Algorithm 1 where we make its dependence on
n i∈[K] n
the allocation weights ω explicit and call it PI-BAI(ω).
Our algorithm consists in coupling PI-BAI to a well-chosen allocation vector ω that depends on the prior.
We will discuss the allocation strategies further below, but we first give theoretical guarantees that hold for
any choice of fixed ω. This idea has the benefit of its versatility, as it naturally generalizes to structured
bandit settings such as the linear or hierarchical problems defined above. The structure is a direct prior
information and is taken into account in the computation of the allocation weights as well as in the posterior
updates. Similarly and despite additional technicality, our novel proof scheme (see Section 4) is preserved
across all settings and allows us to state our main theorems below.
2FullderivationsareinAppendixC.
4Algorithm 1 Prior-Informed BAI: PI-BAI(ω)
Input: Budget n, allocation weights ω ∈∆ .
K
for i=1,...,K do
Get n =⌊ω n⌋ samples of arm i
i i
Compute mean posterior reward E[r(i;θ)|H ]
n
Set J =argmax E[r(i;θ)|H ].
n i∈[K] n
3.1 PoE Bounds for Multi-Armed Bandits
Theorem 3.1 presents an upper bound on the expected PoE of PI-BAI(ω) for MAB (2). The bound depends
on the prior and allocation weights3 ω ∈∆+.
K
Theorem 3.1 (Upper bound for multi-armed bandit). For all ω ∈ ∆+, the expected PoE of PI-BAI(ω)
K
under the MAB problem (2) is upper bounded as
−(µ0,i−µ0,j)2
(cid:88) e 2(σ02 ,i+σ02 ,j)
P ≤ ,
n (cid:112)
1+nϕ
i,j
i,j∈[K]
i̸=j
√
whereϕ =Ω(1),anditdependsonthepriorparametersandallocationweights. Inparticular,P =O(1/ n).
i,j n
Full expressions and proofs are given in Appendix C.2.
√
The O(1/ n) bound contrasts with frequentist results where PoE is typically O(e−n/f(θ)), where f(θ) is
a complexity measure that depends on the fixed bandit instance θ (Audibert et al., 2010; Carpentier and
√
Locatelli, 2016). However, this O(1/ n) rate is not surprising for the expected PoE. For instance, when
K =2, Atsidakou et al. (2022) states the existence of a prior distribution for which the expected PoE is lower
√
boundedbyO(1/ n), andourresultasymptoticallymatchesthislowerbound. Togivemoreintuition, notice
that averaging the frequentist PoE bound O(e−n/f(θ)) under a Gaussian prior θ ∼N(µ ,σ2 ) leads to a
√ i 0,i 0,i
O(1/ n) rate for the expected PoE. Indeed, this idea was employed by Atsidakou et al. (2022) to achieve
√
their O(1/ n) rate. However, while we achieve similar asymptotic rates, our proof differs significantly as we
do not average the frequentist bound. Beyond asymptotic behavior, our bound also captures the structure of
the prior. In particular, if the prior is informative, either with small prior variances σ2 →0 or large prior
0,i
gaps |µ −µ |→∞, then P →0 for any fixed allocation weights ω ∈∆+. After interpreting our bound,
0,i 0,j n K
we now compare it to that of another Bayesian algorithm, BayesElim (Atsidakou et al., 2022), in the simpler
setting where their results are valid, that is, when the prior variances are homogeneous, σ2 = σ2. Their
0,i 0
bound reads
−(µ0,i−µ0,j)2
PBayesElim ≤
(cid:88)
log (K)
e 4σ02
.
n i,j∈[K] 2 (cid:113) 1+ Klognσ (0 K2 )σ2
i̸=j 2
Theorem 3.1 in this setting even with the simplest allocations ω = 1 simplifies to the similar expression
i K
−(µ0,i−µ0,j)2
PPI-BAI ≤
(cid:88) e 4σ02
.
n (cid:113)
i,j∈[K] 1+ nσ 02
Kσ2
i̸=j
By omitting elimination phases, we gain roughly a factor log3/2(K) over the bound of Atsidakou et al. (2022)
2
(highlighted in blue). This difference makes our bound smaller even in their setting with homogeneous prior
variances and choosing uniform allocation weights for PI-BAI.
3Fortechnicalreasonsweonlyallowpositiveallocationweightsω∈∆+.
K
5Homogeneous setting Heterogeneous setting
1.0 1.0
0.9 0.9
0.8
0.8
0.7
0.7
0.6
0.6
0.5
0.5 0.4
0.4 0.3
50 100 150 200 50 100 150 200
Budget n Budget n
Optimal Random (Atsidakou et al., 2022)
Uniform Heuristic
Figure 1: Upper bound value of our method instantiated with various weights compared to the upper bound
of BayesElim (Atsidakou et al., 2022) in two settings.
We numerically compare these two bounds on the 3-armed bandit example described in Section 1, where
one arm is a priori suboptimal, and one of the other two is a priori optimal, µ =(1,1.9,2). We consider two
0
scenarios: one with homogeneous prior variances (σ =0.3 for all i∈[3]) and another with heterogeneous
0,i
prior variances, (σ ,σ ,σ ) = (0.1,0.5,0.5). Since BayesElim’s bound does not handle heterogeneous
0,1 0,2 0,3
prior variances, we use an average prior variance σ2 = 1 (cid:80) σ2 for comparison. As predicted, Figure 1
0 K i∈[K] 0,i
shows that the value of the upper bound is much lower for PI-BAI for optimized, uniform and random
allocation weights. We also plot the upper bound of PI-BAI instantiated with a heuristic weight that favors
higher prior means and higher prior variance arms, that is, ωh = µ0,iσ0,i .
i (cid:80) k∈[K]µ0,kσ0,k
3.2 PoE Bounds for Structured Bandits
Importantly, our analysis extends to the linear and hierarchical bandits in (4) and (6). Theorems 3.2 and 3.3
provide upper bounds on the PoE of PI-BAI(ω) in these settings. To the best of our knowledge, these are the
first prior-dependent bounds for fixed-budget Bayesian BAI in these settings.
Theorem 3.2 (Upper bound for linear bandit). Assume that x ̸=x for any i̸=j, and that there exists
i j
S >0 such that ∥x∥2 ≤S for any x∈X. Then, for all ω ∈∆+, the expected PoE of PI-BAI(ω) under the
2 K
linear bandit problem (4) is upper bounded as
−(µ⊤ 0xi−µ⊤ 0xj)2
(cid:88) e 2∥xi−xj∥2 Σ0
P ≤ ,
n (cid:113) 1+ ci,j
i,j i∈ ̸=[ jK] ∥xi−xj∥2
Σˆn
where c =Ω(1), λ (Σˆ )=O(1/n), and they depend on both prior parameters and allocation weights. In
i,j 1 √n
particular, P =O(1/ n) and we recover Theorem 3.1 when x =e ∈RK and Σ =diag(σ2 ) . Full
n i i 0 0,i i∈[K]
expressions and proofs are given in Appendix C.3.
√
Similarly to our results in multi-armed bandits, P =O(1/ n) since λ (Σˆ )=O(1/n). Also, this bound
n 1 n
captures the benefit of using informative priors. Indeed, P → 0 when the prior variances are small, i.e.
n
Σ →0 , or when the prior gaps are large, |µ⊤x −µ⊤x |→∞.
0 L×L 0 i 0 j
Theorem 3.3 (Upperboundforhierarchicalbandit). For all ω ∈∆+, the expected PoE of PI-BAI(ω) under
K
the hierarchical bandit problem (6) is upper bounded as
−
(ν⊤bi−ν⊤bj)2
(cid:88) e 2(∥bi−bj∥2 Σ+σ02 ,i+σ02 ,j)
P ≤ ,
n (cid:113)
1+ ci,j
i,j∈[K] σˆn,i+σˆn,j
i̸=j
6
eulav
dnuob
reppUwhere c =Ω(1) and σˆ2 =Ω(1/n) and they all depend on both the prior parameters and allocation weights.
ij n,i√
In particular, P =O(1/ n). Full expressions and proofs are given in Appendix C.4.
n
The term ∥b −b ∥2 +σ2 +σ2 accounts for the prior uncertainty of both arms and effects. If the effects
i j Σ 0,i 0,j
aredeterministic(Σ→0 )thenourboundrecoverstheupperboundofMABwithpriormeanµ =ν⊤b .
L×L 0,i i
On the other hand, if the arms are deterministic given the effects (σ2 → 0), the bound only depends on
0,i
the effect covariance. Finally, if the prior is informative by its gap (|ν⊤b −ν⊤b |→∞) or by its variance
i j
(Σ→0 and σ2 →0), then P →0.
L×L 0,i n
3.3 Allocation Strategies
Instantiating our algorithm requires choosing the potentially prior-dependent allocation weights. Though our
bound holds for any such choice, different principles can be used to find empirically satisfying allocations.
Allocation by optimization. Our upper bounds on the PoE in Theorems 3.1 to 3.3 are of the form
P ≤C (ω,n),
n prior
√
whereC (ω,n)=O(1/ n)dependsonthepriorandallocationweightsω. Sinceitisvalidforanyω ∈∆+,
prior K
we define the optimized allocation weights as
ωopt =argminC (ω,n). (11)
prior
w∈∆+
K
We denote this variant PI-BAI(ωopt). To fix ideas and give intuition, we give the explicit solution for MAB
with K =2. By Theorem 3.1,
−(µ0,1−µ0,2)2 −(µ0,1−µ0,2)2
e 2(σ02 ,1+σ02 ,2) +e 2(σ02 ,1+σ02 ,2)
P ≤ ,
n (cid:112)
1+nϕ (ω ,ω )
1,2 1 2
which can be optimized to obtain
(cid:16)1 (σ2 −σ2 )σ2(cid:17)
ωopt =Π − 0,2 0,1 , ωopt =1−ωopt, (12)
1 [0,1] 2 2nσ2 σ2 2 1
0,1 0,2
where Π (·) is the projection on [a,b]. This expression gives much insight into the allocation strategy.
[a,b]
First, in the case of equal prior confidence σ2 =σ2 , allocating the same amount of samples for each arm is
0,1 0,2
optimal. On the other hand, if for example σ2 ≪σ2 for small budget n, we would have ωopt ≫ωopt, and
0,1 0,2 2 1
hence most of the budget would be allocated to the arm with high prior variance (low initial confidence).
This discussion is only valid for small budgets: as n → +∞, the optimal choice is to divide the budget
equallybetweenbotharms, sincethepriorrelevancevanishesasymptotically. Ontheotherhand, asaturation
phenomenon happens for ‘too small’ budgets: if n<2σ2(cid:12) (cid:12)σ 02 ,2 − σ 02 ,1(cid:12) (cid:12), the weight of the arm with larger prior
(cid:12)σ2 σ2 (cid:12)
0,1 0,2
variance is equal to 1 due to the projection. Note that (12) does not depend on the prior gap ∆ , which is
0
coherent since identifying the optimal arm is strictly equivalent to identifying the worst arm when dealing
with 2 arms. However, this is not necessarily the case when K is larger, as discussed in Appendix D.8.
Since the objective function in (11) is non-convex for K >2, we use numerical optimization to solve it
(e.g. L-BFGS-B (Virtanen et al., 2020)). Thankfully, this optimization is done just once before interacting
with the environment. These optimized weights remain non-guaranteed to be good, because (11) is only
optimal with respect to the bound we derived. We found it useful in practice to mix them with the heuristic
weight ωh defined in Section 3.1. This allocation is motivated by having a small probability of error when
plugged in the bound (Figure 1), and our theoretical guarantees (Theorem 3.1) still hold because the weights
are a function of the prior. Hence, we define the new optimized weight as αωopt+(1−α) µ0,iσ0,i . For
i (cid:80) k∈[K]µ0,kσ0,k
simplicity, we also refer this as ωopt. We tested the value of the parameter α in various settings, and found
that it is generally around 0.5 (Appendix D.7).
7Allocation by optimal design. Inthelinearbanditsetting,wegeneralizeideasfromoptimalexperimental
design (Lattimore and Szepesvári, 2020, Chapter 21) to Bayesian MAB, linear and hierarchical bandits. To
thebestofourknowledge,ourworkisthefirsttoexploitthisideaforsuchsettings. Findingan(approximate)
BayesianG-optimaldesign(seee.g. López-Fidalgo(2023,Chapter4)forareview)isequivalenttomaximizing
the log-determinant of the regularized information matrix defined as
ωG(cid:57)opt =argmaxlogdet(cid:0) nσ−2 (cid:88) ω x x⊤+Σ−1(cid:1) .
k k k 0
ω∈∆+
K k∈[K]
This leads to budget allocations that minimize the worst-case posterior variance in all directions. We quantify
the effects of using such an allocation in the upper bound:
Corollary 3.4 (Upper bound of PI-BAI(ωG(cid:57)opt)). Assume that x ̸=x for any i̸=j, and that there exists
i j
S >0 such that ∥x∥2 ≤S for any x∈X. The expected PoE of PI-BAI(ωG(cid:57)opt) in the linear bandit problem
2
(4) with diagonal covariance matrix Σ is upper bounded as
0
−(µ⊤ 0xi−µ⊤ 0xj)2
(cid:88) e 2∥xi−xj∥2 Σ0
P ≤ ,
n (cid:112) 1+ n c
i,j∈[K] 2dσ2 i,j
i̸=j
where c =Ω(1). The full expressions and proofs are given in Appendix C.3. The result also holds for the
i,j
MAB setting when x =e ∈RK and Σ =diag(σ2 ) , and for the hierarchical setting with the equivalent
i i 0 0,i i∈[K]
model (10) when Σ is diagonal.
G-optimal design can be applied for MAB, by using x =e for any i∈[K] and Σ =diag(σ2 ) , and
i i 0 0,i i∈[K]
also for hierarchical bandits by using its connection to the linear model (10).
Note that both allocation weights ωopt and ωG(cid:57)opt are prior-dependent but not instance-dependent. In
particular, they enjoy the theoretical guarantees we derived in Section 4.
Allocation by warm-up. Finally, we present an adaptive allocation rule, for which our theoretical results
do not apply directly, but performs well in practice. Here we use a warm-up policy π to interact with the
w
bandit environment for n rounds (the warm-up phase), and then choose some allocation weights ω based
w πw
on the prior and the data collected through the interaction. The warm-up policy can be any decision-making
policy that (preferably) takes as input the prior distribution. Motivated by its good performance in BAI (Lee
et al., 2023), we choose π to be Thompson sampling, and select the allocation weights to be proportional to
w
the number of pulls to each arm during the warm-up phase:
(cid:80) 1{A =i}
ωTS = t∈[nw] t , ∀i∈[K]. (13)
i n
w
To illustrate the differences between optimized weights (PI-BAI(ωopt)) and learned weights with Thompson
sampling as a warm-up policy (PI-BAI(ωTS)), we return to our motivating example in Section 3.1, where
K =3, µ =(1,1.9,2) and σ =0.3 for all i∈[3]. We set the budget as n=100. We repeat 104 times the
0 0,i
following experiment: we sample a bandit instance from the prior and run Thompson sampling for n =20
w
rounds, then construct the allocation weights ωTS. Computing the weights ωopt by numerical optimization of
(11) is done once at the beginning of these experiments.
Figure 2 shows an empirical comparison of the weights on 2 problem instances and on average over 104
runs. We see that, in this example, both allocation strategies assign high weights to arms 2 and 3, while
allocating a small weight to arm 1. This is because, based on the prior information, arm 1 is highly unlikely
to be the optimal arm. Then, the primary objective revolves around selecting the optimal arm among arms
2 and 3. Also, while the allocation weights ωts vary with each bandit instance, their average values in all
i
instances are similar to those of ωopt. Thus PI-BAI(ωTS) is more adaptive than PI-BAI(ωopt), while both
i
have similar average behavior.
8Allocations for instance 1 Allocations for instance 2 Average allocations
1.0
0.8
!1 !3
!2
0.6
0.4
0.2
0.0
!opt !TS !opt !TS !opt !TS
Figure 2: Allocations weights ωopt and ωTS.
i i
4 General Proof Scheme
We outline the key technical insights to derive our Bayesian proofs. The idea is general and can be applied to
all our settings. Specific proofs for these settings are in Appendix C.
From Frequentist to Bayesian proof. To analyze their algorithm in the MAB setting, Atsidakou et al.
(2022)relyonthestrongrestrictionthatσ =σ forallarmsi∈[K]andtunetheirallocationsasafunction
0,i 0
of the noise variance σ2 such that in the Gaussian setting, the posterior variances σˆ2 in (3) are equal for all
n,i
arms i∈[K]. This assumption is needed to directly leverage results from Karnin et al. (2013), thus allowing
them to bound the frequentist PoE P(cid:0) J ̸=i (θ)|θ(cid:1) ≤B(θ) for a fixed instance θ. Then, the expected PoE,
n ∗
P =E(cid:2)P(cid:0) J ̸=i (θ)|θ(cid:1)(cid:3), is bounded by computing E [B(θ)]. We believe it is not possible to extend
n n ∗ θ∼P0
such technique for general choices of allocations n and prior variances σ2 . Thus, we pursue an alternative
i 0,i
approach, establishing the result in a fully Bayesian fashion. We start with a key observation.
Key reformulation of the expected PoE. We observe that the expected PoE can be reformulated as
follows
P
=E(cid:2)P(cid:0)
J ̸=i
(θ)|θ(cid:1)(cid:3) =E(cid:2)P(cid:0)
J ̸=i (θ)|H
(cid:1)(cid:3)
.
n n ∗ n ∗ n
This swap of measures means that to bound P , we no longer bound the probability of
n
J =argmaxE[r(i;θ)|H ]̸=argmaxr(i;θ)=i (θ)
n n ∗
i∈[K] i∈[K]
foranyfixedθ. Instead,weonlyneedtoboundthatprobabilitywhenθ isdrawnfromtheposteriordistribution.
Precisely, we bound the probability that the arm i maximizing the posterior mean E[r(i;θ)|H ] differs from
n
the arm i maximizing the posterior sample r(i;θ) | H . This is achieved by first noticing that P can be
n n
rewritten as
(cid:88)
P = E[P(i (θ)=i|J =j,H )1{J =j}].
n ∗ n n n
i,j∈[K]
i̸=j
The rest of the proof consists of bounding the above conditional probabilities for distincts i and j, and
this depends on the setting (MAB, linear or hierarchical). Roughly speaking, this is achieved as follows.
P(i (θ)=i|H ,J =j) is the probability that arm i maximizes the posterior sample r(·;θ), given that
∗ n n
arm j maximizes the posterior mean E[r(·;θ)|H ]. We show that this probability decays exponentially
n
with the squared difference (E[r(i;θ)|H ]−E[r(j;θ)|H ])2. Taking the expectation of this term under
√n n
the history H gives the desired O(1/ n) rate. All technical details and full explanations can be found
n
in Appendix C. This proof introduces a novel perspective for Bayesian BAI, distinguished by its tighter
prior-dependent bounds on the expected PoE and ease of extension to more complex settings like linear and
hierarchical bandits. However, its application to adaptive algorithms could be challenging, particularly due
to the complexity of taking expectations under the history H in that case. Also, extending this proof to
n
non-Gaussian distributions is an interesting direction for future work. Note that PI-BAI is applicable beyond
the Gaussian case, as done in Appendices A.2 and D.2 featuring an approximate approach to logistic bandits
(Chapelle and Li, 2012).
9
sthgiew
noitacollAMulti-armed, K=10 Linear, K=30, d=4 Hierarchical, K=60, L=10
0.5 0.7
0.9
0.6
0.4 0.8
0.5 0.7
0.3 0.4 0.6
0.2 0.3 0.5
0.4
50 100 150 200 250 300 100 300 500 700 900 150 250 350 450 550
0.5 0.6 0.9
0.5 0.8
0.4
0.4 0.7
0.3 0.3 0.6
0.2 0.5
0.2
50 100 150 200 250 300 100 300 500 700 900 150 250 350 450 550
Budget n Budget n Budget n
PI-BAI(!uni) PI-BAI(!opt) PI-BAI(!G ¡opt) PI-BAI(!TS) BayesGap
BayesElim TTTS SR SH GSE
Figure 3: Averaged PoE with varying budgets for the Fixed and Random settings. We compare PI-BAI
instantiated with different allocations strategies to the baselines BayesElim, TTTS, SR, SH in multi-armed and
hierarchical bandits, and to BayesGap and GSE in linear bandits.
5 Experiments
We conduct several experiments to evaluate the performance of PI-BAI. In all experiments, we set the
observation noise to σ = 1 and run algorithms 104 times and display the (narrow) standard error. The
code is available in the supplementary material, and additional experiments and details are presented in
Appendix D.1. We consider four variants of PI-BAI(ω) varying in allocation weights: PI-BAI(ωuni) (uniform
weights),PI-BAI(ωopt)(optimizedweightswithmixing),PI-BAI(ωG(cid:57)opt)(G-optimaldesign)andPI-BAI(ωTS)
(warmed-up with Thompson Sampling). The question of tuning the warm-up length n is discussed in
w
Appendix D.7 and we set n =K.
w
Multi-armed bandit. We consider two settings, Fixed and Random. For both, we set K =10 and σ
0,i
evenly spaced between 0.1 and 0.5. In the Fixed setting, µ is evenly spaced between 0 and 1 whereas
0,i
in the Random one, µ ∼ U([0,1]). We compare PI-BAI variants to state-of-the-art Bayesian methods,
0,i
top-two Thompson sampling (TTTS) (Russo, 2016; Jourdan et al., 2022) and BayesElim (Atsidakou et al.,
2022), as well as to frequentist elimination algorithms: successive rejects (SR) (Audibert et al., 2010) and
sequential halving (SH) (Karnin et al., 2013). Note that TTTS does not come with theoretical guarantees in
the fixed-budget setting, but we include it given its good empirical performance.
Linear bandit. We let d=4 and K =30. Then we construct the arm set X by sampling arms x from a
i
multivariate Gaussian distribution with mean 0 and covariance I . In the Fixed setting, the prior mean
d d
is flat µ =(1,...,1) whereas in the Random setting, the prior means are sampled uniformly from [0,1] as
0
µ ∼U([0,1]). For both settings, Σ =diag(σ2 ) where the σ ’s are evenly spaced between 0.1 and
0,i 0 0,i i∈[K] 0,i
0.5. We compare our methods with two algorithms that were designed for linear bandits; BayesGap (Hoffman
et al., 2014) and GSE (Azizi et al., 2021), the current (tractable4) state-of-the-art that leverages G-optimal
design to perform successive elimination. Other methods that leverage the same elimination idea and have
lower performances on these settings (Alieva et al., 2021; Yang and Tan, 2022) are not tested.
Hierarchical bandit. Here, each mixing weight b is chosen uniformly between 0 and 1, and then
i
normalized to form a probability vector. In the Fixed setting, The ν ’s are evenly spaced in [−1,1], whereas
i
each ν ∼ U([−1,1]) in the Random setting. In both settings, Σ is diagonal with entries evenly spaced
i
in [0.12,0.52], and Σ is also diagonal, where the σ ’s are evenly spaced between 0.1 and 0.5. The prior
0 0,i
4Katz-Samuelsetal.(2020)hasanalgorithmwithtighterboundsbutitisnottractable.
10
EoP
degarevA
EoP
degarevA
)gnittes
dexiF(
)gnittes
modnaR(MovieLens, K=100, d=5
0.4
0.3
0.2
0.1
200 600 1000 1400 1800
Budget n
PI-BAI(!uni) PI-BAI(!G ¡opt) BayesGap
PI-BAI(!opt) PI-BAI(!TS) GSE
Figure 4: Averaged PoE on MovieLens.
distribution for all Bayesian algorithms (except PI-BAI) is simply obtained by marginalizing out the effects.
This allows obtaining µ and σ , even for algorithms that are not suitable for hierarchical priors. Though
0,i 0,i
there is no explicit baseline for this setting, we implement TS based on meTS (Aouali et al., 2023) and
we compare with frequentist and Bayesian elimination strategies agnostic to the structure. Despite the
connection to linear bandits, we do not include such baselines as they do not perform well due to their
inefficient representation of the structure.
Results on simulated data (Figure 3). Overall, despite setting-dependent variations, PI-BAI(ωG(cid:57)opt) is
the best-performing version, closely followed by ωopt. In the hierarchical experiments PI-BAI(ωTS) surpasses
all baselines, highlighting the effectiveness of this method in capturing the underlying problem structure.
These observations reaffirm that leveraging prior information is a powerful and practical tool to scale the
applicability of BAI to cases with a large number of arms in limited data regimes.
MovieLens data experiment (Figure 4). The MovieLens (Lam and Herlocker, 2016) dataset is a large
sparse matrix of ratings from 6040 users (rows) on movies (columns, we subsampled K = 100). We first
perform a low-rank matrix factorization to obtain d=5 dimensional vectors representing users (context) and
movies (actions) as well as estimated corresponding values. We then simulate an online interaction setting:
at each round t ∈ [n], a user vector is picked uniformly at random and a movie is chosen by the policy,
leading to a reward Y ∼N(x⊤θ ,σ2). This semi-synthetic problem allows us to assess PI-BAI’s robustness
t t t
to prior misspecification since the bandit instances are no longer sampled from the prior. More details in
Appendix D.1.
6 Conclusion
We revisit the Bayesian fixed-budget BAI for PoE minimization (Atsidakou et al., 2022) and propose a simple
yet efficient algorithm for MAB, linear and hierarchical bandits. Our new proof technique reveals a flexible
and smaller upper bound on the expected PoE. In particular, this allows us to derive the first prior-dependent
Bayesian PoE upper bounds for linear and hierarchical bandits. We believe that our work sheds a new light
on the adaptativity-vs-generality trade-off in BAI algorithm design while opening several avenues for future
research. Our work relies on the assumption that the algorithm has access to the true instance-generating
distribution, i.e. the prior. Though this assumption is very common in the Bayesian bandits literature, it is
unrealistic in many scenarios. An interesting question for future work is to explore methods to learn the
generative distribution and the related consequences of prior misspecification on the expected PoE of PI-BAI
(Kveton et al., 2021; Simchowitz et al., 2021; Nguyen and Vernade, 2023).
11
EoP
degarevABroader Impact Statement
Inthisworkwehavedevelopedandanalyzedgenericalgorithmsforcertainoptimizationproblems. Employing
our methods may lead to savings in computation and energy. Since our problem setting and our algorithms
are generic, the broader (social) impact is unforeseeable and depends on the area where the methods are
applied.
References
Yasin Abbasi-Yadkori, David Pal, and Csaba Szepesvari. Improved algorithms for linear stochastic bandits.
In Advances in Neural Information Processing Systems 24, pages 2312–2320, 2011.
Ayya Alieva, Ashok Cutkosky, and Abhimanyu Das. Robust pure exploration in linear bandits with limited
budget. In International Conference on Machine Learning, pages 187–195. PMLR, 2021.
Imad Aouali. Linear diffusion models meet contextual bandits with large action spaces. In NeurIPS 2023
Foundation Models for Decision Making Workshop, 2023.
Imad Aouali, Branislav Kveton, and Sumeet Katariya. Mixed-effect thompson sampling. In International
Conference on Artificial Intelligence and Statistics, pages 2087–2115. PMLR, 2023.
Alexia Atsidakou, Sumeet Katariya, Sujay Sanghavi, and Branislav Kveton. Bayesian fixed-budget best-arm
identification. arXiv preprint arXiv:2211.08572, 2022.
Jean-Yves Audibert, Sébastien Bubeck, and Rémi Munos. Best arm identification in multi-armed bandits. In
COLT, pages 41–53, 2010.
Javad Azizi, Branislav Kveton, Mohammad Ghavamzadeh, and Sumeet Katariya. Meta-learning for simple
regret minimization. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages
6709–6717, 2023.
Mohammad Javad Azizi, Branislav Kveton, and Mohammad Ghavamzadeh. Fixed-budget best-arm identifi-
cation in structured bandits. arXiv preprint arXiv:2106.04763, 2021.
Soumya Basu, Branislav Kveton, Manzil Zaheer, and Csaba Szepesvari. No regrets for learning the prior in
bandits. In Advances in Neural Information Processing Systems 34, 2021.
Christopher M Bishop. Pattern Recognition and Machine Learning, volume 4 of Information science and
statistics. Springer, 2006.
Sébastien Bubeck, Rémi Munos, and Gilles Stoltz. Pure exploration in multi-armed bandits problems. In
Algorithmic Learning Theory: 20th International Conference, ALT 2009, Porto, Portugal, October 3-5,
2009. Proceedings 20, pages 23–37. Springer, 2009.
Alexandra Carpentier andAndreaLocatelli. Tight (lower) boundsfor the fixed budget best arm identification
bandit problem. In Conference on Learning Theory, pages 590–604. PMLR, 2016.
Olivier Chapelle and Lihong Li. An empirical evaluation of Thompson sampling. In Advances in Neural
Information Processing Systems 24, pages 2249–2257, 2012.
Rémy Degenne. On the existence of a complexity in fixed budget bandit identification. arXiv preprint
arXiv:2303.09468, 2023.
Eyal Even-Dar, Shie Mannor, Yishay Mansour, and Sridhar Mahadevan. Action elimination and stopping
conditions for the multi-armed bandit and reinforcement learning problems. Journal of machine learning
research, 7(6), 2006.
Sarah Filippi, Olivier Cappe, Aurelien Garivier, and Csaba Szepesvari. Parametric bandits: The generalized
linear case. In Advances in Neural Information Processing Systems 23, pages 586–594, 2010.
12MatthewHoffman,BobakShahriari,andNandoFreitas. Oncorrelationandbudgetconstraintsinmodel-based
banditoptimizationwithapplicationtoautomaticmachinelearning. InArtificial Intelligence and Statistics,
pages 365–374. PMLR, 2014.
Joey Hong, Branislav Kveton, Manzil Zaheer, Yinlam Chow, Amr Ahmed, and Craig Boutilier. Latent
bandits revisited. In Advances in Neural Information Processing Systems 33, 2020.
Joey Hong, Branislav Kveton, Sumeet Katariya, Manzil Zaheer, and Mohammad Ghavamzadeh. Deep
hierarchy in bandits. arXiv preprint arXiv:2202.01454, 2022a.
Joey Hong, Branislav Kveton, Manzil Zaheer, and Mohammad Ghavamzadeh. Hierarchical Bayesian bandits.
In Proceedings of the 25th International Conference on Artificial Intelligence and Statistics, 2022b.
Joey Hong, Branislav Kveton, Manzil Zaheer, Sumeet Katariya, and Mohammad Ghavamzadeh. Multi-
task off-policy learning from bandit feedback. In International Conference on Machine Learning, pages
13157–13173. PMLR, 2023.
Marc Jourdan, Rémy Degenne, Dorian Baudry, Rianne de Heide, and Emilie Kaufmann. Top two algorithms
revisited. Advances in Neural Information Processing Systems, 35:26791–26803, 2022.
Zohar Karnin, Tomer Koren, and Oren Somekh. Almost optimal exploration in multi-armed bandits. In
International conference on machine learning, pages 1238–1246. PMLR, 2013.
Julian Katz-Samuels, Lalit Jain, Kevin G Jamieson, et al. An empirical process approach to the union bound:
Practical algorithms for combinatorial and linear bandits. Advances in Neural Information Processing
Systems, 33:10371–10382, 2020.
Emilie Kaufmann, Olivier Cappé, and Aurélien Garivier. On the complexity of best arm identification in
multi-armed bandit models. Journal of Machine Learning Research, 17:1–42, 2016.
Junpei Komiyama, Kaito Ariu, Masahiro Kato, and Chao Qin. Rate-optimal bayesian simple regret in best
arm identification. Mathematics of Operations Research, 2023.
BranislavKveton,ManzilZaheer,CsabaSzepesvari,LihongLi,MohammadGhavamzadeh,andCraigBoutilier.
Randomized exploration in generalized linear bandits. In International Conference on Artificial Intelligence
and Statistics, pages 2066–2076. PMLR, 2020.
Branislav Kveton, Mikhail Konobeev, Manzil Zaheer, Chih-Wei Hsu, Martin Mladenov, Craig Boutilier, and
Csaba Szepesvari. Meta-Thompson sampling. In Proceedings of the 38th International Conference on
Machine Learning, 2021.
Shyong Lam and Jon Herlocker. MovieLens Dataset. http://grouplens.org/datasets/movielens/, 2016.
Tor Lattimore and Csaba Szepesvári. Bandit algorithms. Cambridge University Press, 2020.
Jongyeong Lee, Junya Honda, and Masashi Sugiyama. Thompson exploration with best challenger rule in
best arm identification. arXiv preprint arXiv:2310.00539, 2023.
Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar. Hyperband: A novel
bandit-based approach to hyperparameter optimization. The journal of machine learning research, 18(1):
6765–6816, 2017.
Jesús López-Fidalgo. Optimal Experimental Design: A Concise Introduction for Researchers, volume 226.
Springer Nature, 2023.
Nicolas Nguyen and Claire Vernade. Lifelong best-arm identification with misspecified priors. In Sixteenth
European Workshop on Reinforcement Learning, 2023.
Amit Peleg, Naama Pearl, and Ron Meir. Metalearning linear bandits by prior update. In International
Conference on Artificial Intelligence and Statistics, pages 2885–2926. PMLR, 2022.
13MyPhan,YasinAbbasiYadkori,andJustinDomke. Thompsonsamplingandapproximateinference. Advances
in Neural Information Processing Systems, 32, 2019.
Chao Qin. Open problem: Optimal best arm identification with fixed-budget. In Conference on Learning
Theory, pages 5650–5654. PMLR, 2022.
Daniel Russo. Simple bayesian algorithms for best arm identification. In Conference on Learning Theory,
pages 1417–1418. PMLR, 2016.
Max Simchowitz, Christopher Tosh, Akshay Krishnamurthy, Daniel Hsu, Thodoris Lykouris, Miro Dudik, and
Robert Schapire. Bayesian decision-making under misspecified priors with applications to meta-learning.
In Advances in Neural Information Processing Systems 34, 2021.
Inigo Urteaga and Chris Wiggins. Variational inference for the multi-armed contextual bandit. In Proceedings
of the 21st International Conference on Artificial Intelligence and Statistics, pages 698–706, 2018.
PauliVirtanen,RalfGommers,TravisE.Oliphant,MattHaberland,TylerReddy,DavidCournapeau,Evgeni
Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, Stéfan J. van der Walt, Matthew Brett,
Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew R. J. Nelson, Eric Jones, Robert Kern, Eric
Larson, C J Carey, İlhan Polat, Yu Feng, Eric W. Moore, Jake VanderPlas, Denis Laxalde, Josef Perktold,
RobertCimrman,IanHenriksen,E.A.Quintero,CharlesR.Harris,AnneM.Archibald,AntônioH.Ribeiro,
Fabian Pedregosa, Paul van Mulbregt, and SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithms for
Scientific Computing in Python. Nature Methods, 17:261–272, 2020. doi: 10.1038/s41592-019-0686-2.
Martin J Wainwright, Michael I Jordan, et al. Graphical models, exponential families, and variational
inference. Foundations and Trends® in Machine Learning, 1(1–2):1–305, 2008.
Po-AnWang,KaitoAriu,andAlexandreProutiere.Onuniformlyoptimalalgorithmsforbestarmidentification
in two-armed bandits with fixed budget. arXiv preprint arXiv:2308.12000, 2023.
R. Wolke and H. Schwetlick. Iteratively reweighted least squares: Algorithms, convergence analysis, and
numerical comparisons. SIAM Journal on Scientific and Statistical Computing, 9(5):907–921, 1988.
Junwen Yang and Vincent Tan. Minimax optimal fixed-budget best arm identification in linear bandits.
Advances in Neural Information Processing Systems, 35:12253–12266, 2022.
14Organization of the Appendix
The supplementary material is organized as follows. In Appendix A, we provide additional general additional
remarks. In Appendix B, we mention additional existing works relevant to our work. In Appendix C, we give
complete statements and proofs of our theoretical results. In Appendix D, we provide additional numerical
experiments.
A Additional Discussions
A.1 Motivating examples for hierarchical bandits
In this section, we discuss motivating examples for using hierarchical models in pure exploration settings.
Hyper-parameter tuning. Here the goal is to find the best configuration for a neural network using n
epochs (Li et al., 2017). A configuration i is represented by a scalar θ ∈R which quantifies the expected
i
performance of a neural network with such configuration. Again, it is intuitive to learn all θ individually.
i
Roughlyspeaking,thismeansrunningeachconfigurationfor⌊n⌋epochsandselectingtheonewiththehighest
K
performance. This is statistically inefficient since the number of configurations can be combinatorially large.
Fortunately, we can leverage the fact that configurations often share the values of many hyper-parameters.
Precisely, a configuration is a combination of multiple hyper-parameters, each set to a specific value. Then
we represent each hyper-parameter ℓ∈[L] by a scalar parameter µ ∈R, and the configuration parameter θ
ℓ i
is a mixture of its hyper-parameters µ weighted by their values. That is, θ =(cid:80) b µ +ϵ , where b is
ℓ i ℓ∈[L] i,ℓ ℓ i i,ℓ
the value of hyper-parameter ℓ in configuration i and ϵ is a random noise to incorporate uncertainty due to
i
model misspecification.
Drug design. In clinical trials, K drugs are administrated to n subjects, with the goal of finding the
optimaldrugdesign. Eachdrugisparameterizedbyitsexpectedefficiencyθ ∈R. Asinthepreviousexample,
i
it is intuitive to learn each θ individually by assigning a drug to ⌈n⌉ subjects. However, this is inefficient
i K
when K is large. We leverage the idea that drugs often share the same components: each drug parameter
θ is a combination of component parameters µ , each accounting for a specific dosage. More precisely, the
i l
parameter of drug i can be modeled as θ =(cid:80) b µ +ϵ where ϵ accounts for uncertainty due to model
i l∈[L] i,l l i i
misspecification. Similarly to the hyper-parameter tuning example, this models correlations between drugs
and it can be leveraged for more efficient use of the whole budget of n epochs.
A.2 Beyond Gaussian distributions
The standard linear model (4) can be generalized beyond linear mean rewards. The Generalized Linear
Bandit (GLB) model with prior P writes (Filippi et al., 2010; Kveton et al., 2020)
0
θ ∼P (14)
0
Y |θ,A ∼P(.;θ,A ) ∀t∈[n],
t t t
where the reward distribution P(.;θ,A ) belongs to some exponential family with mean reward r(A ;θ)=
t t
ϕ(θ⊤x ). ϕ is called the link function. The log-likelihood of such reward distribution can be written
At
n n
(cid:88) (cid:88)
L (θ)= logP(Y ;θ,A )= Y θ⊤x −A(θ⊤x )+h(Y ),
n t i t At At t
t=1 t=1
where A is a log-partition function and h another function. Importantly, (14) encompasses the logistic bandit
model with the particular link function ϕ(z)= 1 .
1+e−z
The main challenge of (14) is that closed-form posterior generally does not exist. One method is to
approximate the posterior distribution of θ given H with Laplace approximation: P(θ |H ) is approximated
n n
15with a multivariate Gaussian distribution with mean θˆ and covariance Σˆ , where θˆ is the maximum
MAP Lap MAP
a posteriori,
θˆ =argmaxL (θ)P (θ)
MAP n 0
θ
n
Σˆ−1 =(cid:88) ϕ˙(θˆ⊤ x )x x⊤ ,
Lap MAP At At At
t=1
where ϕ is assumed continuously differentiable and increasing. Note that θˆ can be computed efficiently
MAP
by iteratively reweighted least squares (Wolke and Schwetlick, 1988).
Logistic Bandit. In the particular case where the reward distribution is Bernoulli, the model writes
θ ∼N(µ ,Σ )
0 0
Y |θ,A ∼B(ϕ(θ⊤x )) ∀t∈[n], (15)
t t At
where ϕ is the logistic function. Then the mean posterior reward can be approximated as
E (cid:2) ϕ(θ⊤x )(cid:3) ≈ ϕ(θˆ M⊤ APx i) ,
θ∼N(θˆ MAP,Σˆ Lap) i (cid:113) 1+ π∥x ∥
8 i Σˆ Lap
and the decision after n rounds is J n =argmax i∈[K] (cid:113) 1ϕ +(θˆ π 8M⊤ ∥A xP i∥x Σˆi)
Lap
.
Proving an upper bound on the expected PoE of this algorithm is challenging. Particularly, upper
bounding the expectation with respect to H is hard because one needs to show that θˆ concentrates
n MAP
(cid:104) (cid:105)
in norm towards its expectation E θˆ . We leave this study for future work. However, we provide
Hn MAP
numerical experiments for this setting in Appendix D.2.
A.3 Additional Remarks on Hierarchical Models
The two-level prior we consider has a shared latent parameter µ = (µ ) ∈ RL representing L effects
ℓ ℓ∈[L]
impacting each of the K arm means:
µ∼Q
θ ∼P (·;µ) ∀i∈[K]
i 0,i
Y |µ,θ,A ∼P(·;θ ) ∀t∈[n],
t t At
where Q is the latent prior on µ∈RL.
In the Gaussian setting (6), the maximum likelihood estimate of the reward mean of action i, B /n ,
n,i i
contributes to (7) proportionally to its precision n /(n σ2 +σ2) and is weighted by its mixing weight b . (8)
i i 0,i i
is a standard Gaussian posterior, and (9) takes into account the information of the conditional posterior.
Finally, (9) takes into account the arm correlation through its dependence on σˆ and µˆ . While the
n,i n,i
properties of conjugate priors are useful for inference, other models could be considered with approximate
inference techniques (Urteaga and Wiggins, 2018; Phan et al., 2019).
Link with linear bandit (cont.). The slightly unusual characteristic of (10) is that the prior distribution
hascorrelatedcomponents. Thiscanbeaddressedbythewhiteningtrick(Bishop,2006),definingµ˜ =Σ¯−1/2µ¯
and˜b =Σ¯1/2¯b , giving
i i
µ˜ ∼N(Σ¯−1/2ν¯,I )
L+K
Y |µ˜,A ∼N(˜b⊤ µ˜,σ2) ∀t∈[n], (16)
t t At
whereI isthe(L+K)-dimensionalidentitymatrix. Then,(16)correspondstoalinearbanditmodelwith
L+K
K arms and d=K+L features. However, this model comes with some limitations. First, when computing
16posteriors under (16), the time and space complexities are O((K +L)3) and O((K +L)2) respectively,
compared to the O(K+L3) and O(K+L2) for our model (9). The feature dimension d=K+L can be
reduced to d=K through the following QR decomposition: B˜ =(cid:0)˜b ,...,˜b (cid:1) ∈R(K+L)×K can be expressed
1 K
as B˜ = VR, where V ∈ R(K+L)×K is an orthogonal matrix and R ∈ RK×K. This leads to the following
model µˇ ∼N(V⊤Σ¯−1/2ν¯,V⊤V) and Y |µˇ,A ∼N(R⊤ µˇ,σ2), yet the feature dimension d remains at the
order of K, and computational efficienct y is not t improvA edt with respect to K.
From hierarchical bandit to MAB. Marginilizing the hyper-prior in (6) leads to a MAB model,
θ ∼N(b⊤ν,σ2 +b⊤Σb ) ∀i∈[K] (17)
i i 0,i i i
Y |µ,θ,A ∼N(θ ,σ2) ∀t∈[n]. (18)
t t At
In this marginalized model, the agent does not know µ and he doesn’t want to model it. Therefore, only θ is
learned. The marginalized prior variance σ2 +b⊤Σb accounts for the uncertainty of the not-modeled effects.
0,i i i
B Extended Related Work
In this section, we provide additional references relevant to our work.
Bayesian bandits in structured environments. Bayesian bandits algorithms under hierarchical models
havebeenheavilystudied(Hongetal.,2020;Kvetonetal.,2021;Basuetal.,2021;Hongetal.,2022a,b;Peleg
et al., 2022; Aouali et al., 2023; Aouali, 2023). All the aforementioned papers propose methods to explore
efficiently in the structured environment to minimize the (Bayesian) regret. The hierarchical model we use is
derived from Aouali et al. (2023). Beyond regret minimization, Bayesian structured models also found success
in simple regret minimization (Azizi et al., 2023) and off-policy learning in bandits (Hong et al., 2023).
Bayesian simple regret minimization. Azizi et al. (2023) considers the problem of simple regret
minimization in a Bayesian hierarchical setting. Their algorithm is based on Thompson sampling, and choose
√
an arm at the last round by sampling according to the number of pulls. This leads to a O(1/ n) rate on
the Bayesian simple regret. Recently, Komiyama et al. (2023) derived a method for Bayesian simple regret
minimization that asymptotically matches their proposed lower bound scaling in O(1/n). Their result does
not contradict our analysis because as mentioned in their work, the difference between the simple regret and
PoE matters in terms of rate when considering a Bayesian objective, unlike in the frequentist case. Moreover,
their method is designed for Bernoulli rewards, and their algorithm does not use the prior distribution.
C Proofs
In this section, we give complete proof of our theoretical results. In Appendix C.1, we give proofs for the
Bayesian posterior derivations and we provide technical results. In Appendix C.2, we prove Theorem 3.1.
The proofs for linear bandits (Theorem 3.2 and Corollary 3.4) are given in Appendix C.3. In Appendix C.4,
we provide proofs and technical remarks for hierarchical bandits (Theorem 3.3).
C.1 Technical Proofs and Posteriors Derivations
Bayesian computations. We focus on the hierarchical Gaussian case (6) and detail the computations of
posterior distributions. We first recall the model,
µ∼N(ν,Σ)
θ ∼N(b⊤µ,σ2 ) ∀i∈[K]
i i 0,i
Y |µ,θ,A ∼N(θ ,σ2) ∀t∈[n],
t t At
where we recall that B =(cid:80) Y and T ={t∈[n], A =i}.
n,i t∈Ti t i t
17Lemma C.1 (Gaussian posterior update). For any ρ∈R,µ∈RL,b∈RL and σ,σ >0,m∈N, we have
0
(cid:90)
(cid:89)
N
(cid:0)
Y
;ρ,σ2(cid:1)
N
(cid:0) ρ;b⊤µ,σ2(cid:1)
dρ∝N (µ;µ ,Σ ) ,
t 0 m m
ρ
t∈[m]
with
(cid:80)
Σ =
m
bb⊤; µ =Σ−1
t∈[m]Y t
b.
m mσ2+σ2 m m mσ2+σ2
0 0
Proof of Lemma C.1. By keeping only terms that depend on µ,
(cid:90)
f(µ)=
(cid:89)
N
(cid:0)
Y
;ρ,σ2(cid:1)
N
(cid:0) ρ;b⊤µ,σ2(cid:1)
dρ
t 0
ρ
t∈[m]
 
(cid:90)  1 (cid:88) 1 
∝ exp − (Y −ρ)2− (ρ−b⊤µ)2 dρ
2σ2 t 2σ2
ρ  t∈[m] 0 
(cid:90) (cid:40) 1 (cid:18) 1 m(cid:19) (cid:32)(cid:80) b⊤µ(cid:33)(cid:41) (cid:26) 1 (cid:27)
∝ exp − ρ2 + −2ρ t∈[m] + dρexp − µTbb⊤µ
2 σ2 σ2 σ2 σ2 2σ2
ρ 0 0 0
 
 2
1 1 (cid:88) b⊤µ σ2σ2 1 
∝exp 2
σ2
Y t+
σ2
 σ2+0
mσ2
− 2σ2µTbb⊤µ
 t∈[m] 0 0 0 
∝exp(cid:40)(cid:80)
t∈[m]Y tb⊤µ
+
σ2
µ⊤bb⊤µ−
1
bb⊤(cid:41)
σ2+mσ2 2(σ2+mσ2) σ2
0 0 0
(cid:40) (cid:32) (cid:80) (cid:33)(cid:41)
∝exp
−1
µ⊤
m
bb⊤µ−2µ⊤
t∈[m]Y t
b
2 σ2+mσ2 σ2+mσ2
0 0
∝N (µ;µ ,Σ ) .
m m
Lemma C.2 (Joint effect posterior). For any n∈[N], the joint effect posterior is a multivariate Gaussian
(cid:16) (cid:17)
Q (µ)=N µ˘,Σ˘ , where
n n
Σ˘−1 =Σ−1+ (cid:88) n i b b⊤, µ˘ =Σ˘ (cid:16) Σ−1ν+ (cid:88) B n,i b (cid:17) .
n n σ2 +σ2 i i n n n σ2 +σ2 i
i∈[K] i 0,i i∈[K] i 0,i
Proof of Lemma C.2. The joint effect posterior can be written as
(cid:90)
Q (µ)∝ L (Y ,...,Y )P (θ |µ)dθQ(µ)
n θ A1 An 0
θ
(cid:90)
= (cid:89) (cid:89) N (cid:0) Y ;θ ,σ2(cid:1) N (cid:0) θ ;b⊤µ,σ2 (cid:1) dθ N (µ;ν,Σ) .
t i i i 0,i i
i∈[K]
θit∈Ti
Applying Lemma C.1 gives
(cid:90)
(cid:89) N (cid:0) Y ;θ ,σ2(cid:1) N (cid:0) θ ;b⊤µ,σ2 (cid:1) dθ ∝N (cid:0) µ;µ¯ ,Σ¯ (cid:1) ,
t i i i 0,i i n,i n,i
θit∈Ti
with
n n B
Σ˘−1 = i b b⊤, µ˘ =Σ˘ b i n,i .
n,i σ2 n +σ2 i i n,i n,i iσ2 n +σ2 n
0,i i 0,i i i
18Therefore, the joint effect posterior is a product of Gaussian distributions,
Q (µ)∝ (cid:89) N (cid:16) µ;µ˘ ,Σ˘ (cid:17) N (µ;ν,Σ)∝N (cid:16) µ;µ˘ ,Σ˘ (cid:17) ,
n n,i n,i n n
i∈[K]
where
Σ˘ =Σ−1+ (cid:88) Σ˘ =Σ−1+ (cid:88) n i b b⊤
n n,i σ2 n +σ2 i i
i∈[K] i∈[K] 0,i i
   
µ˘ n =Σ˘− n1 Σ−1ν+ (cid:88) Σ˘− n,1 iµ˘ n,i=Σ˘− n1 Σ−1ν+ (cid:88) n σB 2n +,i σ2b i .
i∈[K] i∈[K] i 0,i
Lemma C.3 (Conditional arm posteriors). For any n∈[n] and any arm i∈[K], the conditional posterior
distribution of arm i is a Gaussian distribution P (θ |µ)=N (cid:0) µ˜ ,σ˜2 (cid:1), where
n,i i n,i n,i
1 n (cid:16)µ⊤b B (cid:17)
σ˜−2 = + i , µ˜ =σ˜2 i + n,i .
n,i σ2 σ2 n,i n,i σ2 σ2
0,i 0,i
Proof of Lemma C.3. The conditional posterior of arm i can be written as
P (θ |µ)∝L (Y ,...,Y )P (θ |µ)
n,i i θi A1 An 0,i i
∝ (cid:89) N (cid:0) Y ;θ ,σ2(cid:1) N (cid:0) θ ;bTµ,σ2 (cid:1)
t i i i 0,i
t∈Ti
(cid:40) (cid:41)
1 (cid:88) 1
∝exp − (Y −θ )2− (θ −b⊤µ)2
2σ2 t i 2σ2 i i
t∈Ti 0,i
(cid:40) (cid:41)
∝exp − 1 (cid:88)(cid:0) −2Y θ +θ2(cid:1) − 1 (cid:0) θ2−2θ b⊤µ(cid:1)
2σ2 t i i 2σ2 i i i
t∈Ti 0,i
(cid:40) (cid:32) (cid:32) (cid:33) (cid:32) (cid:33)(cid:33)(cid:41)
∝exp −1 θ2 n i + 1 −2θ 1 (cid:88) Y + 1 b⊤µ
2 i σ2 σ2 i σ2 t σ2 i
0,i t∈Ti 0,i
∝N (cid:0) θ ;µ˜ ,σ˜2 (cid:1)
i n,i n,i
Lemma C.4 (Marginal arm posterior). For any n ∈ [n] and any arm i ∈ [K], the marginal posterior
distribution of arm i is a Gaussian distribution P(θ |H )=N (cid:0) µˆ ,σˆ2 (cid:1), where
i n n,i n,i
σ˜4 (cid:16)µ˘⊤b B (cid:17)
σˆ2 =σ˜2 + n,ib⊤Σ˘ b , µˆ =σ˜2 n i + n,i .
n,i n,i σ4 i n i n,i n,i σ2 σ2
0,i 0,i i
Proof of Lemma C.4. The marginal distribution of arm i can be written as
(cid:90) P (θ |µ)Q (µ)dµ=(cid:90) N (cid:0) θ ;µ˜ ,σ˜2 (cid:1) N (cid:16) µ;µ˘ ,Σ˘ (cid:17) dµ
n,i i n i n,i n,i n n
µ µ
(cid:32) (cid:32) (cid:33) (cid:33)
(cid:90) µ⊤b B (cid:16) (cid:17)
∝ N θ ;σ˜2 i + n,i ,σ˜2 N µ;µ˘ ,Σ˘ dµ.
i n,i σ2 σ2 n,i n n
µ 0,i
The line above is a convolution of Gaussian measures, and can be written as (Bishop, 2006),
(cid:32) (cid:32) (cid:33) (cid:33)
(cid:90) µ˘⊤b B σ˜2 σ˜2
P (θ |µ)Q (µ)dµ∝N θ ;σ˜2 n i + n,i ,σ˜2 + n,ib⊤Σ˘ n,ib dµ
n,i i n i n,i σ2 σ2 n,i σ2 i nσ2 i
µ 0,i i 0,i 0,i
=N (cid:0) θ ;µˆ ,σˆ2 (cid:1) .
i n,i n,i
19Lemma C.5 (Technical lemma). Let a>0 and X ∼N(µ,σ2). Then E X(cid:104) e− 2X a2 2(cid:105) =
(cid:113)
1+1 σ2e− 2(a2µ +2 σ2).
a2
C.2 Proofs for MAB
From now, we consider that ⌊ω n⌋=ω n∈N for sake of simplicity.
k k
Theorem C.6 (Complete statement of Theorem 3.1). For all ω ∈ ∆+, the expected PoE of PI-BAI(ω)
K
under the MAB problem (2) is upper bounded as
−(µ0,i−µ0,j)2 −(µ0,i−µ0,j)2
(cid:88) e 2(σ02 ,i+σ02 ,j) (cid:88) e 2(σ02 ,i+σ02 ,j)
P ≤ := ,
n (cid:115) (cid:112)
i,j∈[K] 1+nσ 04 ,iωi(cid:16) σ n2+ωjσ 02 ,j(cid:17) +σ 04 ,jωj(cid:16) σ n2+ωiσ 02 ,i(cid:17) i,j∈[K] 1+nϕ i,j
i̸=j σ2σ 02 ,i(cid:16) σ n2+ωjσ 02 ,j(cid:17) +σ2σ 02 ,j(cid:16) σ n2+ωiσ 02 ,i(cid:17) i̸=j
Remark C.7. When σ 02
,i
= σ 02 ,j,lim n→+∞ϕ
i,j
= lim
n→∞
σσ 204 , σi 02ω ,i i(cid:16) (cid:16)σ σn n2 2+ +ω ωj jσ σ02 02, ,j j(cid:17) (cid:17)+ +σ σ04 2, σj 02ω ,j j(cid:16) (cid:16)σ σn n2 2+ +ω ωi iσ σ02 02, ,i i(cid:17)
(cid:17)
= 2 σσ 202 ωω ii +ω ωj
j
=
O(1).
Proof of Theorem 3.1. We first write P as a double sum over all possible distinct arms,
n
E(cid:2)P(cid:0) J ̸=i (θ)|H (cid:1)(cid:3) =E[1{J ̸=i (θ)}]
n ∗ n n ∗
  
K K
(cid:88)(cid:88)
=E E  1{i̸=j}1{i ∗(θ)=i}1{J n =j}|H n
i=1j=1
(cid:88)
= E[E[1{i (θ)=i}1{J =j}|H ]]
∗ n n
i,j∈[K]
i̸=j
(cid:88)
= E[P(i (θ)=i∩J =j |H )]
∗ n n
i,j∈[K]
i̸=j
Since J :H →[K], P(J =j |H )=1{J =j}. Considering both events {J =j} or {J ̸=j} under H ,
n n n n n n n n
(cid:88) (cid:88)
E[P(i (θ)=i∩J =j |H )]= E[P(i (θ)=i∩J =j |H )(1{J =j}+1{J ̸=j})]
∗ n n ∗ n n n n
i,j∈[K] i,j∈[K]
i̸=j i̸=j
(cid:88)
= E[P(i (θ)=i|J =j,H )|J =j]P(J =j).
∗ n n n n
i,j∈[K]
i̸=j
(cid:88)
= E[P(i (θ)=i|J =j,H )1{J =j}] .
∗ n n n
i,j∈[K]
i̸=j
Overall,
(cid:88)
P = E[P(i (θ)=i|J =j,H )1{J =j}] .
n ∗ n n n
i,j∈[K]
i̸=j
20By definition of i (θ) in the MAB setting and applying Hoeffding inequality for sub-Gaussian random
∗
variables,
(cid:32) (cid:33)
P argmaxθ =i|H ,J =j ≤P(θ ≥θ |H ,J =j)
k n n i j n n
k∈[K]
=P((θ −θ )−(µˆ −µˆ )≥−(µˆ −µˆ )|H ,J =j)
i j n,i n,j n,i n,j n n
(cid:32) (cid:33)
(µˆ −µˆ )2
≤exp − n,i n,j . (19)
2(σˆ2 +σˆ2 )
n,i n,j
Therefore,
(cid:34) (cid:32) (cid:33)(cid:35)
(µˆ −µˆ )2
E[P(i (θ)=i|J =j,H )1{J =j}]≤E exp − n,i n,j (20)
∗ n n n 2(σˆ2 +σˆ2 )
n,i n,j
We now want to compute this above expectation with respect to H .
n
First, we remark that because the scheduling of arms (A ,...,A ) is deterministic, the law of H =
1 n n
(A ,Y ,...,A ,Y ) is the law of (Y ,...,Y ). Denoting π the marginal distribution of H ,
1 A1 n An A1 An Hn n
(cid:90)
π (H )=π (Y ,...,Y )= L (Y ,...Y )P (θ)dθ,
Hn n Hn A1 An θ A1 An 0
θ
where L (Y ,...Y ) denotes the likelihood of (Y ,...Y ) given parameter θ and P (θ)=(cid:81) P (θ )
θ A1 An A1 An 0 i∈[K] 0,i i
since each mean reward θ is drawn independently from P in the MAB setting. Since rewards given
i 0,i
parameter θ are independent and identically distributed,
(cid:90)
(cid:89) (cid:0) (cid:1)
π (H )= L (Y ) P (θ )dθ
Hn n θi t t∈Ti 0,i i i
θ
i∈[K]
(cid:90)
= (cid:89) N(cid:0) (Y ) ;θ 1 ,σ2I (cid:1) N(cid:0) θ ;µ ,σ2 (cid:1) dθ , (21)
t t∈Ti i ωin ωin i 0,i 0,i i
θ
i∈[K]
where 1 denotes the vector of size q whose all components are 1s.
q
(21) is a convolution of Gaussians and can be computed easily (Bishop, 2006),
N(cid:0) (Y ) ;θ 1 ,σ2I (cid:1) N(cid:0) θ ;µ ,σ2 (cid:1) =N(cid:0) (Y ) ;µ 1 ,σ2I +σ2 1 1⊤ (cid:1) .
t t∈Ti i ωin ωin i 0,i 0,i t t∈Ai 0,i ωin ωin 0,i ωin ωin
The above covariance matrix exhibits σ2+σ2 on the diagonal and σ2 out of diagonal.
0,i 0,i
We are now ready to compute some useful statistics : for any i∈[K],
(cid:34) (cid:35)
E[µˆ ]=E σ2 µ + σ 02 ,i (cid:88) Y =µ (22)
n,i σ2+σ2 ω n 0,i σ2+σ2 ω n t 0,i
0,i i 0,i i t∈Ti
(cid:32) (cid:33)
V(µˆ )= σ 04 ,i V (cid:88) Y = σ 04 ,i ω n (23)
n,i (cid:0) σ2+σ 02 ,iω in(cid:1)2
t∈Ti
t (cid:0) σ2+σ 02 ,iω in(cid:1) i
(cid:34) (cid:35)
1 (cid:88)
E Y =µ (24)
ω n t 0,i
i
t∈Ti
(cid:32) (cid:33)
V 1 (cid:88) Y = 1 (cid:0) ω n(cid:0) σ2+σ2 (cid:1) +(cid:0) ω2n2−ω n(cid:1) σ2 (cid:1) = σ2 +σ2 (25)
ω n t ω2n2 i 0,i i i 0,i ω n 0,i
i t∈Ti i i
Applying Lemma C.5 on (20) and simplifying terms gives
(cid:34) (cid:32) (cid:33)(cid:35)
−(µ0,i−µ0,j)2
E[P(i (θ)=i|J =j,H )1{J =j}]≤E exp
−(µˆ n,i−µˆ n,j)2
=
e 2(σ02 ,i+σ02 ,j)
.
∗ n n n 2(σˆ2 +σˆ2 ) (cid:112) 1+nϕ
n,i n,j i,j
21C.3 Proofs for Linear Bandits
Theorem C.8 (Complete statement of Theorem 3.2). Assume that x ̸=x for any i̸=j, and that there
i j
exists S >0 such that ∥x∥2 ≤S for any x∈X. Then, for all ω ∈∆+, the expected PoE of PI-BAI(ω) under
2 K
the linear bandit problem (4) is upper bounded as
P
n
≤
(cid:88)
(cid:113) 1+
1
ci,j
e−(µ 2⊤ 0 ∥xx ii −− xµ j⊤ 0 ∥x 2 Σˆj 0)2
,
i,j i∈ ̸=[ jK] ∥xi−xj∥2
Σˆn
where:
 
 
c i,j =∥x i−x j∥2 Cov(µˆn), Cov(µˆ n)= σ1 4Σˆ n  (cid:88) (cid:0) ω in(σ2+x⊤ i Σ 0x i)(cid:1) x ix⊤ i + (cid:88) (cid:88) x⊤ i Σ 0x jω iω jn2x ix⊤ j   Σˆ n.
i∈[K] i∈[K]j∈[K]\{j} 
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
varianceterms covariancebetweenarms
Proof of Theorem 3.2. The proof for the linear model follows the same steps as the MAB model by
rewriting P as
n
(cid:88)
P = E[P(i (θ)=i|J =j,H )1{J =j}] .
n ∗ n n n
i,j∈[K]
i̸=j
By definition of i (θ) and J in the linear bandit setting,
∗ n
P(i (θ)=i|H ,J =j)=P(cid:0) ∀k ∈[K], θ⊤x ≥θ⊤x |H ,J =j(cid:1)
∗ n n i k n n
≤P(cid:0) θ⊤x ≥θ⊤x |J =j,H (cid:1)
i j n n
(cid:32) ∥µˆ ∥2 (cid:33)
≤exp −
n (xi−xj)(xi−xj)⊤
,
2∥x −x ∥2
i j Σˆ
n
where the last inequality follows from Hoeffding inequality for sub-Gaussian random variables. Taking the
expectation with respect to H ,
n
(cid:34) (cid:32) ∥µˆ ∥2 (cid:33)(cid:35)
E[P(i (θ)=i|H ,J =j)1{J =j}]≤E exp − n (xi−xj)(xi−xj)⊤ . (26)
∗ n n n 2∥x −x ∥2
i j Σˆ
n
Then we remark that the expectation of µˆ with respect to H is
n n
     
E[µˆ n]=E Σˆ nΣ− 01µ 0+ σ1
2
(cid:88) Y tx At=Σˆ nΣ− 01µ 0+ σ1 2E (cid:88) Y tx At ,
t∈[n] t∈[n]
since the scheduling (A ,...A ) is known beforehand. Now,
1 n
 
(cid:88) (cid:88) (cid:88)
E  Y tx At= E[Y t]x At = µ⊤ 0x Atx At,
t∈[n] t∈[n] t∈[n]
where E[Y ] was obtained by marginalizing the likelihood over the prior distribution as in (21).
t
Rearranging the terms permits to conclude that E[µˆ ]=µ . Then we can compute the expectation in
n 0
(26) by applying Lemma C.5, Sylvester identity, and some simplifications:
E(cid:34) exp(cid:32) −∥µˆ n 2∥ ∥2 ( xxi i− −xj x)( jx ∥i
2
Σ−
ˆ
nxj)⊤(cid:33)(cid:35) =
(cid:114)
1+
∥x ∥i−
x1
ix −j∥ x2
C j∥ov
2
Σˆ(
nµˆn)e−1 2∥µ0∥2 Λij . (27)
22where from Lemma C.5,
(cid:32) (cid:33)−1
(A −A )(A −A )
Λ =Cov(µˆ )−1−Cov(µˆ )−1 Cov(µˆ )−1+ i j i j Cov(µˆ )−1
i,j n n n ∥A −A ∥2 n
i j Σˆ
n
(cid:32) (cid:33)−1
(x −x )(x −x )
=Cov(µˆ )−1−Cov(µˆ )−1 I +Cov(µˆ ) i j i j
n n d n ∥x −x ∥2
i j Σˆ
n
(x −x )(x −x )⊤
= i j i j .
∥x −x ∥2 +∥x −x ∥2
i j Σˆ
n
i j Cov(µˆn)
The last equality follows from an application of Sherman-Morrison identity. Applying the law of total
expectation,
Cov(θ)=E[Cov(θ |H )]+Cov(E[θ |H ])=Σˆ +Cov(µˆ ).
n n n n
Therefore,
∥x −x ∥2 +∥x −x ∥2 =∥x −x ∥2 =∥x −x ∥2 .
i j Σˆ n i j Cov(µˆn) i j Σˆ n+Cov(µˆn) i j Σ0
Plugging these into (27), we obtain
−∥µ0∥2
(xi−xj)(xi−xj)⊤ −(µ⊤ 0xi−µ⊤ 0xj)2
E[P(i (θ)=i|H ,J =j)]≤
e
2∥xi−xj∥2
Σˆ0
=
e
2∥xi−xj∥2
Σˆ0
.
∗ n n (cid:114) (cid:114)
1+
∥xi−xj∥2
Cov(µˆn) 1+
∥xi−xj∥2
Cov(µˆn)
∥xi−xj∥2
Σˆn
∥xi−xj∥2
Σˆn
Computation of Cov(µˆ ). By definition of Gaussian posteriors in linear bandit in (5),
n
(cid:18) (cid:18) (cid:19)(cid:19) (cid:18) (cid:19)
1 1 1
Cov(µˆ )=Cov Σˆ Σ−1µ + B =Σˆ Cov Σ−1µ + B Σˆ = Σˆ Cov(B )Σˆ ,
n n 0 0 σ2 n n 0 0 σ2 n n σ4 n n n
and
(cid:88) (cid:88) (cid:88)
Cov(B )= V(Y x )+ Cov(Y x ,Y x )
n t At t At t′ A t′
t∈[n] t∈[n]t′∈[n],t̸=t′
(cid:88) (cid:88) (cid:88)
= V(Y )x x⊤ + Cov(Y ,Y )x x⊤
t At At t t′ At A t′
t∈[n] t∈[n]t′∈[n],t̸=t′
(cid:88) (cid:88)
= ω nV(Y )x x⊤+ ω ω n2Cov(Y ,Y )x x⊤
k xk k k i j xi xj i j
k∈[K] i,j∈[K]
i̸=j
(cid:88) (cid:88)
= ω n(σ2+x⊤Σ x ))x x⊤+ ω ω n2(x⊤Σ x )x x⊤.
k k 0 k k k i j i 0 j i j
k∈[K] i,j∈[K]
i̸=j
Proof of Corollary 3.4. We first prove a useful lemma that holds for Bayesian G-optimal design.
Lemma C.9. Let X a finite set such that |X|=K, ξ :X →[0,1] a distribution on X so that (cid:80) ξ(x)=1,
x∈X
V (ξ) = (cid:80) ξ(x)xx⊤ + σ2Σ−1, Σ ∈ Rd×d a diagonal matrix, and f(ξ) = logdet(cid:0) V (ξ)(cid:1). If ξ∗ =
n x∈X n 0 0 n
argmin f(ξ), then max ∥x∥2 ≤d.
ξ∈∆K x∈X Vn(ξ)−1
Proof of Lemma C.9. By concavity of ξ (cid:55)→f(ξ), we have for any ξ that
0≥⟨∇f(ξ∗),ξ−ξ∗⟩= (cid:88) ξ(x)(cid:2) ∇f(ξ∗)(cid:3) −⟨ξ∗,∇f(ξ∗)⟩,
x
x∈X
23and since this holds for any pdf ξ, choosing ξ =δ for an arbitrary action x′ yields
x′
(cid:2) ∇f(ξ∗)(cid:3) ≤⟨ξ∗,∇f(ξ∗)⟩ for any x′ ∈X .
x′
Since r.h.s. does not depend on x′,
max(cid:2) ∇f(ξ∗)(cid:3) ≤⟨ξ∗,∇f(ξ∗)⟩. (28)
x∈X x
By the property of the gradient of log-determinant, (cid:2) ∇f(ξ∗)(cid:3) =∥x∥ . Therefore, for any ξ,
x Vn(ξ∗)−1
(cid:88)
⟨ξ,∇f(ξ)⟩= ξ(x)∥x∥2
V−1(ξ)
x∈X
(cid:88)
= ξ(x)x⊤V (ξ)−1x
n
x∈X
(cid:32) (cid:33)
(cid:88)
=Tr ξ(x)x⊤V (ξ)−1x
n
x∈X
 (cid:32) (cid:33)−1
(cid:88) (cid:88) σ2
=Tr ξ(x)xx⊤ ξ(x′)x′x′⊤+ Σ−1 
n 0
x∈X x′∈X
(cid:32) (cid:18) σ2 (cid:19)−1(cid:33) (cid:88)
=Tr A A+ Σ−1 where E = ξ(x)xx⊤
n 0
x∈X
(cid:32)(cid:18)
σ2
(cid:19)−1(cid:33)
=Tr I + Σ−1E−1
d n 0
(cid:32) σ2 (cid:18) σ2 (cid:19)−1 (cid:33)
=Tr I − Σ−1 I + E−1Σ−1 E−1 (Woodburry)
d n 0 d n 0
σ2
(cid:32)(cid:18)
σ2
(cid:19)−1(cid:33)
=Tr(I )− Tr EΣ + I
d n 0 n d
≤Tr(I )=d.
d
All putting together in (28) with ξ∗ =argmin f(ξ) implies max ∥x∥ ≤d.
ξ∈∆K x∈X V(ξ∗)−1
A direct implication of Lemma C.9 is that max ∥x∥ ≤ dσ2. Therefore,
x∈X Σˆ n n
−(µ⊤ 0xi−µ⊤ 0xj)2 −(µ⊤ 0xi−µ⊤ 0xj)2 −(µ⊤ 0xi−µ⊤ 0xj)2
(cid:88) e
2∥xi−xj∥2
Σˆ0 (cid:88) e
2∥xi−xj∥2
Σˆ0 (cid:88) e
2∥xi−xj∥2
Σˆ0
P ≤ ≤ ≤ .
n i,j i∈ ̸=[ jK](cid:113) 1+ ∥xi−ci x, jj ∥2 Σˆn i,j i∈ ̸=[ jK](cid:113) 1+ 2maxxc ∈i X,j ∥x∥2 Σˆn i,j i∈ ̸=[ jK] (cid:113) 1+n 2c di σ,j 2
C.4 Proofs for Hierarchical Bandits
We begin by stating the complete proof.
Theorem C.10 (Complete statement of Theorem 3.3). For all ω ∈ ∆+, the expected PoE of PI-BAI(ω)
K
under the hierarchical bandit problem (6) is upper bounded as
(cid:88) 1 −
(ν⊤bi−ν⊤bj)2
P
n
≤
(cid:113)
e 2(∥bi−bj∥2 Σ+σ02 ,i+σ02 ,j) ,
1+ ci,j
i,j∈[K] σˆn,i+σˆn,j
i̸=j
24where
σ4 σ4
c =V(µˆ −µˆ )= 0,i V(B )+ 0,j V(B )
i,j n,i n,j (σ2 ω n+σ2)2 n,i (σ2 ω n+σ2)2 n,j
0,i i 0,j j
+
σ4 (cid:88) (b⊤ kΣ˘ nb i)2
V(B )+
σ4 (cid:88) (b⊤ kΣ˘ nb j)2
V(B )
(σ2 ω n+σ2)2 (σ2+ω nσ2 )2 n,k (σ2 ω n+σ2)2 (σ2+ω nσ2 )2 n,k
0,i i k∈[K] k 0,k 0,j j k∈[K] k 0,k
+
(σ 02 ,iω
iσ n4
+σ2)2
k(cid:88) ∈[K]k′∈[(cid:88)
K]\{k}(σ 02 ,kω
k(b n⊤ k +Σ˘ n σb 2i )) .. (( σb 02⊤ k ,′ kΣ˘ ′ωn kb ′i n) +σ2)cov(B n,k,B n,k′)
+
(σ 02 ,jω
jσ n4
+σ2)2
k(cid:88) ∈[K]k′∈[(cid:88)
K]\{k}(σ 02
,kω( kb n⊤ k +Σ˘ n σb 2j )) .. (( σb 02⊤ k ,′ kΣ˘ ′ωn kb ′j n) +σ2)cov(B n,k,B n,k′)
 
−
(σ 02 ,iω
in+σ22 )σ (σ4
02 ,jω
jn+σ2) k(cid:88) ∈[K](b (⊤ k σΣ 2˘ +nb ωi) k( nb⊤ k σΣ 02˘ ,kn )b 2j) V(B n,k)+ k(cid:88) ∈[K]k′∈[(cid:88)
K]\{k}(σ 02 ,kω
k(b n⊤ k +Σ˘ n σb 2i )) .. (( σb 02⊤ k ,′ kΣ˘ ′ωn kb ′j n) +σ2)cov(B n,k,B n,k′)
2σ2 σ2
− 0,i 0,j cov(B ,B )
(σ2 ω n+σ2)(σ2 ω n+σ2) n,i n,j
0,i i 0,j j
 
+
(σ2
2 ωσ2 nσ +02 ,i
σ2)2
 (cid:88) (σ2b⊤ k ωΣ˘ nn +b i σ2)cov(B n,k,B n,i)+ (σ2b⊤ i ωΣ˘ nn +b i σ2)V(B n,i)
0,i i k∈[K]\{i} 0,k k 0,i i
 
+
(σ2
2 ωσ2 nσ +02 ,j
σ2)2
 (cid:88) (σ2b⊤ k ωΣ˘ nnb +j σ2)cov(B n,k,B n,j)+ (σ2b⊤ j ωΣ˘ nn +b i σ2)V(B n,j)
0,j j k∈[K]\{j} 0,k k 0,j j
 
−
(σ2 ω
n+2 σσ 22 )(σ σ02 2,j
ω
n+σ2) (cid:88) σ2b +⊤ k ωΣ˘ n nb σi
2
cov(B n,k,B n,i)+ σ2b +⊤ j ωΣ˘ n nb σi
2
V(B n,j)
0,i i 0,j j k∈[K]\{j} k 0,k j 0,j
 
−
(σ2 ω
n+2 σσ 22 )σ (σ02 , 2i
ω
n+σ2) (cid:88) σ2b +⊤ k ωΣ˘ n nb σj
2
cov(B n,k,B n,j)+ σ2b +⊤ i Σ ω˘ n nb σj
2
V(B n,i) ,
0,j j 0,i i k∈[K]\{i} k 0,k i 0,i
where we defined Σ˘ from (7),
n
Σ˘−1 =Σ−1+ (cid:88) ω kn b b⊤ ; V(B )=ω nσ2+ω2n2(σ2 +b⊤Σb ) ; cov(B ,B )=ω ω n2b⊤Σb .
n σ2+ω nσ2 k k n,k k k 0,k k k n,k n,k′ k k′ k k′
k∈[K] k 0,k
Proof of Theorem C.10. This proof follows the same idea of the proof of Theorem 3.1. We first write P as
n
(cid:88)
P = E[P(i (θ)=i|J =j,H )1{J =j}] .
n ∗ n n n
i,j∈[K]
i̸=j
Following (19), by applying Hoeffding inequality for sub-Gaussian random variables,
(cid:32) (cid:33)
P argmaxθ =i|H ,J =j ≤P(θ ≥θ |H ,J =j)
k n n i j n n
k∈[K]
=P((θ −θ )−(µˆ −µˆ )≥−(µˆ −µˆ )|H ,J =j)
i j n,i n,j n,i n,j n n
(cid:32) (cid:33)
(µˆ −µˆ )2
≤exp − n,i n,j ,
2(σˆ2 +σˆ2 )
n,i n,j
where µˆ and σˆ2 are given by (9). Taking the expectation with respect to the history H ,
n,i n,i n
(cid:34) −(µˆn,j−µˆn,i)2(cid:35)
E[P(i ∗(θ)=i|J
n
=j,H n)1{J
n
=j}]≤E e 2(σˆn2 ,i+σˆn2 ,j) .
25Since µˆ −µˆ ∼N(cid:0)E[µˆ ]−E[µˆ ],V(µˆ −µˆ )(cid:1), applying Lemma C.5 gives
n,i n,j n,i n,j n,i n,j
(cid:34) −(µˆn,j−µˆn,i)2(cid:35)
1

(E[µˆ ]−E[µˆ ])2 1

E e 2(σˆn2 ,i+σˆn2 ,j) = (cid:114)
1+
V(µˆn,i−µˆn,j) exp− 2(n σˆ,i n2 ,i+σˆ n2 ,n j),j 1+ V( σˆµˆ 2n,i +− σˆµˆ 2n,j)
σˆ2 +σˆ2 n,i n,j
n,i n,j
Therefore,
1 −
(E[µˆn,i]−E[µˆn,j])2
E[P(i ∗(θ)=i|J
n
=j,H n)]≤
(cid:114)
e 2(σˆn2 ,i+σˆn2 ,j+V(µˆn,i−µˆn,j)) .
1+
V(µˆn,i−µˆn,j)
σˆ2 +σˆ2
n,i n,j
Now we want to simplify σˆ2 +σˆ2 +V(µˆ −µˆ ). on one hand, by the law of total variance,
n,i n,j n,i n,j
V(θ −θ )=E[V(θ −θ |H )]+V(E[θ −θ |H ])=σˆ2 +σˆ2 +V(µˆ −µˆ ),
i j i j n i j n n,i n,j n,i n,j
On the other hand,
V(θ −θ )=E[V(θ −θ |µ)]+V(E[θ −θ |µ])=σ2 +σ2 +V((b −b )⊤µ)
i j i j i j 0,i 0,j i j
=σ2 +σ2 +∥b −b ∥2 .
0,i 0,j i j Σ
Combining these two last equations gives σˆ2 +σˆ2 +V(µˆ −µˆ )=σ2 +σ2 +∥b −b ∥2 .
n,i n,j n,i n,j 0,i 0,j i j Σ
Therefore,
1 −
(E[µˆn,i]−E[µˆn,j])2
E[P(i ∗(θ)=i|J
n
=j,H n)]≤
(cid:114)
e 2(σ02 ,i+σ02 ,j+∥bi−bj∥2 Σ) . (29)
1+
V(µˆn,i−µˆn,j)
σˆ2 +σˆ2
n,i n,j
Computing V(µˆ −µˆ ).
n,i n,j
The rest of the proof consists to compute E[µˆ ] and V(µˆ −µˆ ) for (i,j). Denoting Q the latent
n,i n,i n,j
prior distribution µ∼Q and π the law of H ,
Hn n
π (H )=π (Y ,...,Y )
Hn n Hn A1 An
(cid:90)(cid:90)
= L (Y ,...,Y )P (θ |µ)Q(µ)dθdµ
θ A1 An 0
(θ,µ)
(cid:90)(cid:90)
(cid:89)
= L ((Y ) )P (θ |µ)Q(µ)dθ dµ
θi t t∈Ti 0,i i i
(θ,µ)
i∈[K]
 
(cid:90) (cid:90)
=  (cid:89) N (cid:0) (Y t) t∈Ti);θ i1 ωin,σ2I ωin(cid:1) N(θ i;b⊤ i µ,σ 02 ,i)dθ iQ(µ)dµ.
µ
i∈[K]
θi
From properties of Gaussian convolutions (Bishop, 2006),
(cid:90)
N (cid:0) (Y ) );θ 1 ,σ2I (cid:1) N(θ ;b⊤µ,σ2 )dθ =N (cid:0) (Y ) );(b⊤µ)1 ,σ2.I +1 1⊤ σ2 (cid:1) .
t t∈Ti i ωin ωin i i 0,i i t t∈Ti i ωin ωin ωin ωin 0,i
θi
Therefore,
(cid:90)
(cid:89) N (cid:0) (Y ) );θ 1 ,σ2I (cid:1) N(θ ;b⊤µ,σ2 )dθ
t t∈Ti i ωin ωin i i 0,i i
i∈[K]
θi
 
=N(H n; (cid:88) e i(cid:0)RK(cid:1) ⊗ (cid:88) e t(Rωin)⊗b⊤
i
⊗µ, I
K
⊗(σ2I ωin+1 ωin1⊤ ωinσ 02 ,i)),
i∈[K] t∈[ωin]
26where we define explicitly e (cid:0)RK(cid:1) as the ith base vector of RK.
i
Therefore,
(cid:32) (cid:33)
(cid:90)
π(H )= N(H ; (cid:88) e (cid:0)RK(cid:1) ⊗ (cid:88) e (Rωin)⊗b⊤ ⊗µ, I ⊗(σ2I +1 1⊤ σ2 ))N(µ;ν,Σ)dµ
n n i t i K ωin ωin ωin 0,i
µ i∈[K] t∈ωin
=N(H ;˚µ,˚Σ),
n
where˚µ∈Rn, ˚Σ∈Rn×n with
(cid:32) (cid:33)
˚µ= (cid:88) e (cid:0)RK(cid:1) ⊗ (cid:88) e (Rωin)⊗b⊤ ⊗ν
i t i
i∈[K] t∈ωin
˚Σ=I ⊗(σ2I +1 1⊤ σ2 )
K ωin ωin ωin 0,i
     ⊤
+(cid:88) e(cid:0)RK(cid:1) ⊗ (cid:88) e t(Rωin)⊗b⊤ i Σ(cid:88) e i(cid:0)RK(cid:1) ⊗ (cid:88) e t(Rωin)⊗b⊤ i  . (30)
i∈[K] t∈[ωin] i∈[K] t∈[ωin]
Thecovariancematrix˚Σseemscomplexbuthasasimplestructure. ThefirsttermI ⊗(σ2I +1 1⊤ σ2 )
is the same as in the standard model. The remaining term accounts for the
correlaK
tion
betwωi en
en
dω isi tn incω tin ar0 m,i
s
(i,j), and this correlation is of the form b⊤Σb .
i j
Now we are ready to compute E[µˆ ] for any arm k ∈[K]: from (8) and (9),
n,k
(cid:34) (cid:32) (cid:33)(cid:35)
µ˘⊤b B
E[µˆ ]=E σ˜2 n k + n,k
n,k n,k σ2 σ2
0,k
(cid:34) (cid:32) (cid:33)(cid:35)
σ2σ2 µ˘⊤b B
=E 0,i n k + n,k
σ2 ω n+σ2 σ2 σ2
0,k k 0,k
=
σ2
E(cid:2) µ˘⊤b (cid:3) +
σ 02
,k E[B ] .
σ2 ω n+σ2 n k σ2 ω n+σ2 n,k
0,k k 0,k k
From (7),
 
µ˘⊤ nb
k
=ν⊤Σ−1+ (cid:88) σ2+B ωn, ni
σ2
b⊤
i
Σ˘ nb
k
=ν⊤Σ−1Σ˘ nb k+ (cid:88) σ2+B ωn, ni
σ2
b⊤
i
Σ˘ nb k.
i∈[K] i 0,i i∈[K] i 0,i
By linearity,
 
E[µˆ n,k]=
σ2
ωσ n2
+σ2
ν⊤Σ−1Σ˘ nb k+ (cid:88) σ2E +[B ωn n,i σ]
2
b⊤
i
Σ˘ nb k+
σ2
ωσ 02 n,k +σ2E[B n,k] .
0,k k i∈[K] i 0,i 0,k k
From Equation (30), E[B ]=ω nν⊤b . Therefore,
n,i i i
 
E[µˆ n,k]=
σ2
ωσ n2
+σ2
ν⊤Σ−1Σ˘ nb k+ (cid:88) σ2ω +in ων⊤ nb σi
2
b⊤
i
Σ˘ nb k+
σ2
ωσ 02 n,k +σ2ω knν⊤b
k
=ν⊤b
k
0,k k i∈[K] i 0,i 0,k k
(31)
27Now we are ready to compute V(µˆ −µˆ ) for any (i,j). From (31),
n,i n,j
σ2 σ2 σ2 σ2
µˆ −µˆ = µ˘⊤b + 0,i B − µ˘⊤b − 0,j B
n,i n,j σ2 ω n+σ2 n i σ2 ω n+σ2 n,i σ2 ω n+σ2 n j σ2 ω n+σ2 n,j
0,i i 0,i i 0,j j 0,i j
σ2 σ2
= ν⊤Σ−1Σ˘ b − ν⊤Σ−1Σ˘ b
σ2 ω n+σ2 n i σ2 ω n+σ2 n j
0,i i 0,j j
(cid:124) (cid:123)(cid:122) (cid:125)
doesnotdependonobservations
+ σ2 . (cid:88) B n,k b⊤Σ˘ b + −σ2 . (cid:88) B n,k b⊤Σ˘ b
σ2 ω n+σ2 ω nσ2 +σ2 k n i σ2 ω n+σ2 ω nσ2 +σ2 k n j
0,i i k∈[K] k 0,k 0,j j k∈[K] k 0,k
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
(1) (2)
σ2 −σ2
+ 0,j B + 0,j B .
σ2 ω n+σ2 n,i σ2 ω n+σ2 n,j
0,i i 0,j j
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
(3) (4)
Since (1), (2),(3) and (4) are correlated,
4 4 4
(cid:88) (cid:88) (cid:88)
V(µˆ +µˆ )=V((1)+(2)+(3)+(4))= V((i))+ 2cov((i),(j)). (32)
n,i n,j
i=1 i=1j=1,j̸=i
We now compute each term of (32):
 
V((1))=
(σ2
ωσ n4 +σ2)2V  (cid:88)
ω
nσB 2n,k +σ2b⊤ kΣ˘ nb i
0,i i k∈[K] k 0,k
 (cid:32) (cid:33)
= (σ2 ωσ n4 +σ2)2  (cid:88) (σ2( +b⊤ k ωΣ˘ n nb σi) 22 )2V(B n,k)+ (cid:88) cov σ2+B ωn,k nσ2 b⊤ kΣ˘ nb i, σ2+B ωn,k n′ σ2 b⊤ k′Σ˘ nb i 
0,i i k∈[K] k 0,k (k,k′),k̸=k′ k 0,k k′ 0,k′
 
=
(σ2
ωσ n4
+σ2)2
 (cid:88) (σ2( +b⊤ k ωΣ˘ n nb σi) 22 )2V(B n,k)+ (cid:88) (σ2+ω(b⊤ k nΣ σ˘ 2nb i )) (( σb 2⊤ k′ +Σ˘ n ωb i)
nσ2
)cov(B n,k,B n,k′)
0,i i k∈[K] k 0,k (k,k′),k̸=k′ k 0,k k′ 0,k′
 
V((2))=
(σ2
ωσ n4
+σ2)2
 (cid:88) (σ2(b +⊤ k ωΣ˘ n nb σj) 22 )2V(B n,k)+ (cid:88) (σ2+ω(b⊤ k nΣ˘ σn 2b i )) (( σb 2⊤ k′ +Σ˘ n ωb j)
nσ2
)cov(B n,k,B n,k′) ,
0,j j k∈[K] k 0,k (k,k′),k̸=k′ k 0,k k′ 0,k′
σ4
V((3))= 0,i V(B )
(σ2 ω n+σ2)2 n,i
0,i i
σ4
V((4))= 0,j V(B ),
(σ2 ω n+σ2)2 n,j
0,j j
28 
cov((1),(2))=−cov
σ2
ωσ n2 +σ2. (cid:88)
ω
nσB 2n,k +σ2b⊤ kΣ˘ nb i,
σ2
ωσ n2 +σ2. (cid:88)
ω
nσB 2n,k +σ2b⊤ kΣ˘ nb j
0,i i k∈[K] k 0,k 0,j j k∈[K] k 0,k
 
=−
(σ2 ω
n+σ2σ )(4
σ2 ω
n+σ2)cov (cid:88)
ω
nσB 2n,k +σ2b⊤ kΣ˘ nb i, (cid:88)
ω
nσB 2n,k +σ2b⊤ kΣ˘ nb j
0,i i 0,j j k∈[K] k 0,k k∈[K] k 0,k
=−
σ4 (cid:20) (cid:88) (b⊤ kΣ˘ nb i)(b⊤ kΣ˘ nb j)
V(B )
(σ2 ω n+σ2)(σ2 ω n+σ2) (σ2+ω nσ2 ) n,k
0,i i 0,j j k∈[K] k 0,k
+
(cid:88) (b⊤ kΣ˘ nb i)(b⊤ k′Σ˘ nb j)
cov(B ,B
)(cid:21)
(σ2+ω nσ2 )(σ2+ω nσ2 ) n,k n,k′
(k,k′),k̸=k′ k 0,k k′ 0,k′
(cid:32) (cid:33)
σ2 σ2
cov((3),(4))=−cov 0,i B , 0,j B
σ2 ω n+σ2 n,i σ2 ω n+σ2 n,j
0,i i 0,j j
σ2 σ2
=− 0,i 0,j cov(B ,B )
(σ2 ω n+σ2)(σ2 ω n+σ2) n,i n,j
0,i i 0,j j
 
cov((1),(4))=−cov
σ2
ωσ n2 +σ2. (cid:88)
ω
nσB 2n,k +σ2b⊤ kΣ˘ nb i,
σ2
ωσ 02 n,j +σ2B n,j
0,i i k∈[K] k 0,k 0,j j
 
=−
(σ2 ω
n+σσ 22 )σ (0 σ2 , 2j
ω
n+σ2)cov (cid:88) σ2+B ωn,k
nσ2
b⊤ kΣ˘ nb i,B n,j
0,i i 0,j j k∈[K] k 0,k
 
=−
(σ2 ω
n+σσ 22 )σ (0 σ2 , 2j
ω
n+σ2) (cid:88) σ2b ω⊤ kΣ˘ nn +b i
σ2
+ σ2b ω⊤ j Σ˘ nn +b i σ2V(B n,j) .
0,i i 0,j j k∈[K]\{j} 0,k k 0,j j
The remaining terms are obtained by symmetry. Finally, for any (i,j) :
(cid:32) (cid:33)
(cid:88)
V(B )=V Y =ω nσ2+ω2n2(σ2 +b⊤Σb )
n,i t i i 0,i i i
t∈Ai
 
(cid:88) (cid:88) (cid:88) (cid:88)
cov(B n,i,B n,j)=cov Y t, Y t= cov(Y t,Y t)=ω iω jn2b⊤
i
Σb j.
t∈Ai t∈Aj t∈Ait∈Aj
Remark C.11 (ComputingtheupperboundforhierarchicalbanditwithTheorem3.2). Thereadercanwonder
whytransformingthehierarchicalmodelintoalinearmodelthanksto(10), andplugdirectlythetransformed
prior and actions to the linear upper bound (Theorem 3.2). While this is what we do to optimize numerically
the bound, it is challenging to give explicit terms with this method. In fact, it would yield to the following
upper bound,
(cid:88) 1
−(ν¯⊤¯bi−ν¯⊤¯bj)2
(cid:88) 1 −
(ν⊤bi−ν⊤bj)2
P n ≤ (cid:114) e 2(∥¯bi−¯bj∥2 Σ¯ = (cid:114) e 2(∥bi−bj∥2 Σ+σ02 ,i+σ02 ,j) ,
i,j∈[K] 1+
∥¯bi−¯bj∥2
C¯ovn i,j∈[K] 1+
∥¯bi−¯bj∥2
C¯ovn
i̸=j
∥¯bi−¯bj∥2
Σ¯n i̸=j
∥¯bi−¯bj∥2
Σ¯n
where
 
 −1
Σ¯ n =Σ¯ + σ1 2 (cid:88) ω in¯b i¯b⊤ i  , C¯ov n = σ1 4Σ¯ n  (cid:88) ω in(σ2+¯b iΣ¯¯b i)¯b i¯b⊤ i + (cid:88) ¯b iΣ¯¯b jω iω jn2¯b i¯b⊤ j   Σ¯ n.
i∈[K] i∈[K] i,j∈[K]
i̸=j
However, computing ∥¯b −¯b ∥2 and ∥¯b −¯b ∥2 is computationally challenging because it requires first to
compute Σ¯ with block-mi atrij xC in¯o vvn ersion, thi en tj o rΣ¯ en cover the marginal and posterior covariances σ˜2 , σ˘2 and
n,i n,i
σˆ2 from (7), (8) and (9).
n,i
29D Additional Experiments
We provide additional numerical experiments on synthetic data.
• Appendix D.1 provides additional details for the MovieLens experiments.
• Appendix D.2 provides experiments for the logistic bandit model.
• Appendix D.3 provides justifications for the choice of baselines. In particular, we discuss the choice of
the warm-up policy and the influence of adding elimination on top of our method.
• Appendix D.4 gives experiments when focusing as the simple regret as a metric.
• Appendix D.5 provides another type of confidence intervals on the experiments of Section 5.
• Appendix D.6 explains in which setting the hierarchical model benefits from model structure.
• Appendix D.7 tackles the problem of tuning the warm-up length n and the influence of the choice of
w
π on the PoE.
w
• Appendix D.8 provides toy example when deriving ωopt.
D.1 MovieLens Experiments
We provide more information on our MovieLens experiments in Figure 4. The MovieLens dataset contains
ratings given by 6040 users to 3952 movies. We use a subset of K = 100 randomly picked movies for our
experiments. The prior used for inference in PI-BAI and BayesGap is set to be Gaussian with mean µ and
0
Σ . These parameters are estimated by taking the empirical mean and empirical covariance over the wall
0
dataset. All results are averaged over 104 rounds.
D.2 Logistic Bandits
We consider two main settings as in Section 5. In the Fixed setting, µ is flat, µ =(1,...,1) whereas in the
0 0
Random setting, the prior means are uniformly sampled from [0,1]. For both settings, Σ =diag(σ2 )
0 0,i i∈[K]
where the σ ’s are evenly spaced between 0.1 and 0.5. We ran experiments for K =30 arms and d∈{3,4}.
0,i
Figure5showsthat thegeneralizationof PI-BAIwithG-optimal designallocationson hasgood performances
beyond linear settings.
K=30, d=3 (Fixed setting) K=30, d=3 (Random setting) K=30, d=4 (Fixed setting) K=30, d=4 (Random setting)
0.85 0.85 0.95 0.9
0.80 0.80 0.90 0.8 0.75
0.75 0.70 0.85 0.7
0.70 0.65 0.6
0.65 0.60 0.80 0.5
0.55
0.60 0.50 0.75 0.4
0.55 0.45 0.70 0.3
100 300 500 700 900 100 300 500 700 900 100 300 500 700 900 100 300 500 700 900
Budget n Budget n Budget n Budget n
PI-GLBAI(!Uni) PI-GLBAI(!G ¡Opt) PI-GLBAI(!TS) GSE
Figure 5: Average PoE with varying budgets for fixed and randomized settings in the GLB framework.
D.3 Choice of Baselines
A remark on TTTS. Top two sampling algorithms is a family of algorithms that is known to have good
performances in BAI. In Section 5, we used TS-TCI with β =0.5 from Jourdan et al. (2022) and denoted it
as TTTS for sake of notation simplicity.
30
EoP
degarevAChoice of warm-up policy. We evaluate different warm-up policies,TS and two Top-Two algorithms,
TSTCI and T3C from Jourdan et al. (2022). The experiments shown in Figure 6 are run in the same setting as
in Section 5, with K =10 arms in the MAB setting, and with K =60 and d=4 in the hierarchical setting.
Figure 6 suggests to pick TS as a warm-up policy for the MAB setting and meTS for the hierarchical setting.
MAB (Fixed setting) MAB (Random setting) Hierarchical (Fixed setting) Hierarchical (Random setting)
0.7 0.60
0.6 0.55 0.5 0.5
0.50
0.5 0.45 0.4 0.4
0.4 0.40
0.3 0.3
0.3 0.35
0.30 0.2 0.2
0.2 0.25
0.1 0.20 0.1 0.1
50 150 250 50 150 250 150 250 350 450 550 150 250 350 450 550
Budget n Budget n Budget n Budget n
PI-BAI(!TS) PI-BAI(!T3C) PI-BAI(!TSTCI)
Figure 6: Average PoE of PI-BAI instantiated with different warm-up policies.
Influence of elimination. Weempiricallycomparetheinfluenceofusingeliminationontopofourmethods.
TheeliminationprocedureisthesameastheoneusedinAtsidakouetal.(2022). Thereare⌊log (K)⌋rounds,
2
and each lasts ⌊n⌋ steps. At each round, we pull each remaining arm i ⌊ωin⌋ times. At the end of the round,
R R
half of arms are eliminated. These correspond to the arms that have the least posterior mean reward (so µˆ
n,i
in the MAB setting). The allocation ω is then normalized to allocate more budget to remaining arms. Note
that we draft all observations at the end of each round, as it is the case in (Karnin et al., 2013; Azizi et al.,
2021; Atsidakou et al., 2022). Figure 7 shows that using elimination does not give better performances, and
hence we chose to not add these baselines in Section 5.
MAB (Fixed setting) MAB (Random setting)
0.7 0.60
0.55
0.6
0.50
0.5
0.45
0.4 0.40
0.35
0.3
0.30
0.2
0.25
0.1 0.20
50 150 250 50 150 250
Budget n Budget n
PI-BAI(!Opt) PI-BAI(!G ¡Opt) PI-BAI(!Uni) PI-BAI(!TS)
PI-BAI(!Opt(+Elim)) PI-BAI(!G ¡Opt(+Elim)) PI-BAI(!Uni(+Elim)) PI-BAI(!TS(+Elim))
Figure 7: Average PoE of PI-BAI instantiated with different weights with or without elimination in the MAB
setting.
D.4 Simple Regret
Figure8comparestheperformancesofourmethodsbasedontheBayesiansimpleregretE(cid:2)
max θ−θ
(cid:3),
i∈[K] Jn
where the expectation is taken with respect to the prior distribution over instances θ. Overall, it shows that
our methods also have low simple regret in these settings.
31
EoP
degarevA
EoP
degarevAK-armed, K=10 Linear, K=30, d=4 Hierarchical, K=60, L=10
0.35 0.10 0.7
0.30 0.6 0.08
0.25 0.5
0.20 0.06 0.4
0.15 0.04 0.3
0.10 0.2
0.02
0.05 0.1
0.00 0.00 0.0
50 100 150 200 250 300 100 300 500 700 900 150 250 350 450 550
0.35 0.12
0.30 0.10 0.5
0.25
0.08 0.4
0.20
0.06 0.15 0.3
0.04
0.10
0.2
0.05 0.02
0.00 0.00 0.1
50 100 150 200 250 300 100 300 500 700 900 150 250 350 450 550
Budget n Budget n Budget n
PI-BAI(!uni) PI-BAI(!opt) PI-BAI(!G ¡opt) PI-BAI(!TS) BayesGap
BayesElim TTTS SR SH GSE
Figure 8: Average simple regret with varying budgets for fixed and randomized settings.
D.5 Confidence Intervals on Sampled Instances
We provide additional plots in the same settings of Section 5. The first row of Figure 9 shows the PoE of the
methods averaged over 1000 different instances sampled from the prior distribution. For each instance, we
repeat the experiments 100 times to get an estimate of the probability. We show one standard deviation
around the averaged mean of PoE over instances. In the second row of the same figure, we plot the PoE of
each method subtracted by the PoE of the method having the least PoE in each setting, that is, PI-BAI(ωopt)
in MAB, PI-BAI(ωG(cid:57)opt) in linear bandits and PI-BAI(ωTS) in hierarchical bandits.
0.7 Multi-armed, K=10 (fixed setting) 0.7Multi-armed, K=10 (random setting) 0.9 Linear, K=30, d=4 (fixed setting) 0.8Linear, K=30, d=4 (random setting) 1.0Hierarchical, K=60, L=10 (fixed setting) 1H.0ierarchical, K=60, L=10 (random setting)
0.6 0.6 0.8 0.7 0.9 0.9
0.5 0.7 0.6 0.8 0.8
0.5 0.6 0.5 0.4 0.7 0.7
0.4 0.5 0.4
0.3 0.4 0.3 0.6 0.6
0.2 0.3 0.3 0.2 0.5 0.5
0.1 50 100 150 200 250 300 0.2 50 100 150 200 250 300 0.2100 300 500 700 900 0.1100 300 500 700 900 0.4150 350 550 0.4150 350 550
0.25 0.25 0.35 0.35 0.5 0.40
0.20 0.20 00 .. 23 50 00 .. 23 50 0.4 00 .. 33 05
0.15 0.15 0.20 0.20 0.3 0.25
0.10 0.10 0.15 0.15 0.2 00 .. 12 50
0.05 0.05 0.10 0.10 0.1 0.10
0.00 0.00 00 .. 00 05 00 .. 00 05 0.0 00 .. 00 05
−0.05 50 100 150 200 250 300 −0.05 50 100 150 200 250 300 −0.05100 300 500 700 900 −0.05100 300 500 700 900 −0.1150 350 550 −0.05150 350 550
PI-BAI(!uni) BayesElim PI-BAI(!opt) TTTS PI-BAI(!G¡opt) SR PI-BAI(!TS) SH BayesGap GSE
Figure 9: PoE of several methods (first row) and PoE of each method substracted by the PoE of the most
performing method in each setting (second row). For each instance, we repeat the experiments 100 times and
we average the results over 1000 instances. The confidence intervals show one standard deviation.
D.6 Benefits of Hierarchical Models
To illustrate the benefits of using a hierarchical structure (6), we compare the posterior variances σˆ2 under a
n,i
standard model and a hierarchical model. The standard model is obtained by marginalizing over the effects µ,
θ ∼N(b⊤ν,σ2 +b⊤Σb ) ∀i∈[K]
i i 0,i i i
Y |µ,θ,A ∼N(θ ,σ2) ∀t∈[n]. (33)
t t At
32
terger
degarevA
EoP
degarevA
terger
elpmiS
terger
elpmiS
)gnittes
dexiF(
)gnittes
modnaR(
terger
degarevAFrom (3), the corresponding posterior covariance of an arm i∈[K] is
1 ω n
σˆ−2 = + i .
n,i σ2 +bTΣb σ2
0,i i i
For the first setting, we uniformly draw a vector u∈[0,1] and set σ =0.1u and Σ=2I . For the second
0 L
setting, we set σ = u and Σ = 10−3I . In both settings, we consider K = 50 arms, and L = 10 effects.
0 L
We draw uniformly ν,b ∈[−1,1]L and the allocation vector is set to uniform allocation ωuni = 1 for any
i ∈ [K]. In Figure 10 wi e plot the average posterior covariance 1 (cid:80) σ2 across all armi s forK both the
K i∈[K] n,i
(marginalized) standard model (33) and the hierarchical model (6). The goal of this experiment is to show
for which setting the benefits of the hierarchy are pronounced. The results show that this difference is more
pronounced when the initial uncertainty of the effects Σ is greater than the initial uncertainty of the mean
rewards (σ2 ) .
0,i i∈[K]
Setting 1, K=50, L=10 Setting 2, K=50, L=10
0.26
0.8 0.24
0.22
0.6
0.20
0.4 0.18
0.16
0.2 0.14
0.12
0.0
0.10
50 100 150 200 50 100 150 200
Budget n Budget n
standard hierarchical
Figure 10: Average posterior covariance across all arms for standard and hierarchical model for two settings.
D.7 Hyperparameters
Warm-up length n . We try different values of warm-up length n for our warm-up policies. We
w w
emphasize that methods based on TTTS require n >K because each arm has to be pulled at the beginning.
w
Figure 11 suggests picking n = 2K for the warm)up with T3C and TSTCI, and n = K for the warm-up
w w
with TS.
K=10, n=100 K=10, n=200 K=20, n=200 K=20, n=200
0.50 0.45 0.44
0.55
0.42
0.45 0.40
0.40
0.50
0.40 0.35 0.38
0.35 0.30 0.45 0.36
0.34
0.30 0.25
0.40 0.32
0.25 0.20 0.30
10 20 30 40 50 60 70 80 90 10 40 70 100130160190 20 30 40 50 60 70 80 90 20 50 80 110 140 170
0.50 0.50 0.50
0.55
0.48 0.45
0.45 0.46
0.40 0.50 0.44
0.42
0.40 0.35 0.40
0.45
0.30 0.38
0.35 0.36
0.25
0.40 0.34
0.30 0.20 0.32
10 20 30 40 50 60 70 80 90 10 40 70 100130160190 20 30 40 50 60 70 80 90 20 50 80 110 140 170
Warm-up length Warm-up length Warm-up length Warm-up length
PI-BAI(!TS) PI-BAI(!T3C) PI-BAI(!TSTCI)
Figure 11: Average PoE of PI-BAI instantiated with different warm-up policies for different warm-up lengths
n .
w
33
EoP
degarevA
EoP
degarevA
)gnittes
dexiF(
)gnittes
modnaR(
2^¾
1
i;n
K
]KX[
2iMixture parameter α. We discuss the choice of the mixture parameter α. We recall that we use the
heuristic αωopt+(1−α) µ0,iσ0,i in our experiments. Figure 12 shows that for the fixed setting, adding
i (cid:80) k∈[K]µ0,kσ0,k
the vector µ0,iσ0,i helps improve the performances. This is not necessarily the case in the random
setting.
(cid:80) k∈[K]µ0,kσ0,k
K=5, n=25 K=5, n=50 K=10, n=75 K=10, n=100
0.235 0.395 0.40
0.27 0.230 0.390 0.39
0.225 0.385 0.38
0.26 0.220 0.380 0.37
0.215 0.375
0.25 0.36 0.210 0.370
0.24 0.205 0.365 0.35
0.200 0.360 0.34
0.23 0.195 0.355 0.33
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
0.58 0.39 0.60 0.50
0.56 0.38 0.59 0.49 0.54 0.37 0.48
0.36 0.58 0.47 0.52
0.35 0.57 0.46 0.50
0.34 0.56 0.45
0.48 0.33 0.44
0.46 0.32 0.55 0.43
0.44 0.31 0.54 0.42
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
® ® ® ®
Figure 12: Average PoE of PI-BAI(ωopt) for different mixture parameter α.
D.8 Toy Experiments for ωopt
We provide additional experiments to evaluate the optimized weights ωopt in different settings. In Figure 13,
we set K =3, µ =(2,1.9,0), (σ ,σ ,σ )=(10−2,0.5,0.5). This corresponds to the motivating setting
0 0,1 0,2 0,3
depicted in the introduction (Section 1). We provide a comprehensive illustration of the prior bandit instance
µ ±2σ (left plot). Then we let the budget vary, n∈[10,200], and for each n we (numerically) optimize
0,i 0,i
(11) to get ωopt (middle plot). On the right plot, we let the prior mean of arm 2 vary, µ ∈[0,2], and get
n 0,2
ωopt as a function of µ . In Figure 14, we do the same experiments for 6 arms with 2 good arms a priori,
0,2
µ = (2,1.9,1,0.6,0.3,0) and σ uniformly spaced in [0.5,0.1]. As n increases, ωopt suggests distributing
0 0
roughly one third of the budget for each arm 1 and 2, and the remaining to the rest of the arms.
These results give insight on the behavior of ωopt. In Figure 13, for small budgets, ωopt suggests pulling a
lot the second arm because of its wide prior confidence σ2 ≫σ2 . It does not give much allocation to the
0,2 0,1
last arm since it is statistically unlikely to become optimal. As the budget n grows, most of the allocations
are almost equal to arm 1 and arm 2. Since σ2 ̸=σ2 , ωopt ̸=ωopt even for a large budget n=200. The
0,1 0,2 1 2
right plot of Figure 13 shows that ωopt depends on prior gaps. Interestingly, this was not the case for the
very special example K =2 depicted in Section 3.3.
34
EoP
degarevA
EoP
degarevA
)gnittes
dexiF(
)gnittes
modnaR(4
¹0§2¾0
0.9
!¤ when varying budget n
0.50
!¤ when varying ¹0;2, n=200
0.8 0.45
3
0.7 0.40
2 0.6 0.35
0.5 0.30
1
0.4 0.25
0 0.3 0.20
0.2 0.15
−1
0.1 0.10
−2 0.0 0.05
50 100 150 200 0.0 0.5 1.0 1.5 2.0
Budget n Prior mean ¹0;2
arm 1 arm 2 arm 3
Figure 13: Illustration of the prior bandit instance (left), and optimized allocation ωopt when varying budget
n (middle) or one coordinate of µ (right).
0
¹ 2¾ ! when varying budget n
3.5
0§ 0
0.35
¤
3.0 0.30
2.5
0.25
2.0
0.20
1.5
0.15
1.0
0.10
0.5
0.0 0.05
−0.5 0.00
50 100 150 200
Budget n
arm 1 arm 2 arm 3 arm 4 arm 5 arm 6
Figure 14: Illustration of the prior bandit instance (left), and optimized allocation ωopt when varying budget
n (right).
35