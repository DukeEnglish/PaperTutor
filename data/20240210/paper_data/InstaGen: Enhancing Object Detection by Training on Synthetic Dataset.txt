InstaGen: Enhancing Object Detection by Training on Synthetic Dataset
ChengjianFeng1 YujieZhong1 ZequnJie1,† WeidiXie2,† LinMa1
1 MeituanInc. 2 ShanghaiJiaoTongUniversity
https://fcjian.github.io/InstaGen
Figure1. (a)ThesyntheticimagesgeneratedfromStableDiffusionandourproposedInstaGen,whichcanserveasadatasetsynthesizer
forsourcingphoto-realisticimagesandinstanceboundingboxesatscale. (b)Onopen-vocabularydetection,trainingonsyntheticimages
demonstratessignificantimprovementoverCLIP-basedmethodsonnovelcategories.(c)Trainingonthesyntheticimagesgeneratedfrom
InstaGenalsoenhancesthedetectionperformanceinclose-setscenario,particularlyindata-sparsecircumstances.
commonpractiseistotrainthedetectorsonlarge-scaleim-
Abstract age datasets, such as MS-COCO [20] and Object365 [30],
where objects are exhaustively annotated with bounding
In this paper, we present a novel paradigm to enhance
boxesandcorrespondingcategorylabels.However,thepro-
the ability of object detector, e.g., expanding categories
cedureforcollectingimagesandannotationsisoftenlabo-
or improving detection performance, by training on syn-
riousandtime-consuming,limitingthedatasets’scalability.
theticdatasetgeneratedfromdiffusionmodels.Specifically,
we integrate an instance-level grounding head into a pre- In the recent literature, text-to-image diffusion models
trained, generative diffusion model, to augment it with the have demonstrated remarkable success in generating high-
abilityoflocalisingarbitraryinstancesinthegeneratedim- qualityimages[28,29],thatunlocksthepossibilityoftrain-
ages.Thegroundingheadistrainedtoalignthetextembed- ing vision systems with synthetic images. In general, ex-
ding of category names with the regional visual feature of isting text-to-image diffusion models are capable of syn-
thediffusionmodel,usingsupervisionfromanoff-the-shelf thesizing images based on some free-form text prompt, as
objectdetector,andanovelself-trainingschemeon(novel) shown in the first row of Figure 1a. Despite being photo-
categoriesnotcoveredbythedetector.Weconductthorough realistic, such synthesized images can not support training
experimentstoshowthat,thisenhancedversionofdiffusion sophisticated systems, that normally requires the inclusion
model, termed as InstaGen, can serve as a data synthe- of instance-level annotations, e.g., bounding boxes for ob-
sizer, to enhance object detectors by training on its gen- ject detection in our case. In this paper, we investigate a
erated samples, demonstrating superior performance over novel paradigmof dataset synthesis for trainingobject de-
existingstate-of-the-artmethodsinopen-vocabulary(+4.5 tector,i.e.,augmentingthetext-to-imagediffusionmodelto
AP)anddata-sparse(+1.2∼5.2AP)scenarios. generateinstance-levelboundingboxesalongwithimages.
To begin with, we build an image synthesizer by fine-
1.Introduction tuning the diffusion model on existing detection dataset.
This is driven by the observation that off-the-shelf diffu-
Object detection has been extensively studied in the field
sionmodelsoftengenerateimageswithonlyoneortwoob-
of computer vision, focusing on the localization and cate-
jects on simplistic background, training detectors on such
gorizationofobjectswithinimages[3,5,12,26,27]. The
images may thus lead to reduced robustness in complex
†:correspondingauthor. real-world scenarios. Specifically, we exploit the existing
1
4202
beF
8
]VC.sc[
1v73950.2042:viXradetection dataset, and subsequently fine-tune the diffusion training data. In the recent literature, to further expand
model with the image-caption pairs, constructed by taking theabilityofobjectdetector,open-vocabularyobjectdetec-
randomimagecrops,andcomposingthecategorynameof tion(OVD)hasbeenwidelyresearched,forexample,OVR-
theobjectsinthecrop. Asillustratedinthesecondrowof CNN [37] introduces the concept of OVD and pre-trains a
the Figure 1a, once finetuned, the image synthesizer now vision-languagemodelwithimage-captionpairs. Thesub-
enables to produce images with multiple objects and intri- sequent works make use of the robust multi-modal repre-
catecontexts,therebyprovidingamoreaccuratesimulation sentationofCLIP[24],andtransferitsknowledgetoobject
ofreal-worlddetectionscenarios. detectorsthroughknowledgedistillation[9,36],exploiting
Togenerateboundingboxesforobjectswithinsynthetic extradata[5,41]andtextprompttuning[2,5].Inthispaper,
images,weproposeaninstancegroundingmodulethates- we propose to expand the ability of object detectors, e.g.,
tablishes the correlation between the regional visual fea- expanding categories or improving detection performance,
tures from diffusion model and the text embedding of cat- bytrainingonsyntheticdataset.
egory names, and infers the coordinates for the objects’
Generative Models. Image generation has been consid-
boundingboxes. Specifically,weadoptatwo-steptraining
eredasataskofinterestincomputervisionfordecades. In
strategies, firstly, we train the grounding module on syn-
therecentliterature,significantprogresshasbeenmade,for
thetic images, with the supervision from an off-the-shelf
example, the generative adversarial networks (GANs) [8],
objectdetector,whichhasbeentrainedonasetofbasecat-
variational autoencoders (VAEs) [15], flow-based mod-
egories; secondly, weutilizethetrainedgroundingheadto
els [14], and autoregressive models (ARMs) [32]. More
generatepseudolabelsforalargersetofcategories,includ-
recently,therehasbeenagrowingresearchinterestindiffu-
ing those not seen in existing detection dataset, and self-
sionprobabilisticmodels(DPMs),whichhaveshowngreat
train the grounding module. Once finished training, the
promise in generating high-quality images across diverse
groundingmodulewillbeabletoidentifytheobjectsofar-
datasets. For examples, GLIDE [23] utilizes a pre-trained
bitrary category and their bounding boxes in the synthetic
languagemodelandacascadeddiffusionstructurefortext-
image,bysimplyprovidingthenameinfree-formlanguage.
to-image generation. DALL-E 2 [25] is trained to gener-
Tosummarize, weexploreanovelapproachtoenhance
ate images by inverting the CLIP image space, while Ima-
object detection capabilities, such as expanding detectable
gen [29] explores the advantages of using pre-trained lan-
categories and improving overall detection performance,
guagemodels. StableDiffusion[28]proposesthediffusion
by training on synthetic dataset generated from diffusion
processinVAElatentspacesratherthanpixelspaces,effec-
model.Wemakethefollowingcontribution:(i)Wedevelop
tivelyreducingresourceconsumption. Ingeneral,therapid
an image synthesizer by fine-tuning the diffusion model,
developmentofgenerativemodelsopensthepossibilityfor
with image-caption pairs derived from existing object de-
traininglargemodelswithsyntheticdataset.
tection datasets, our synthesizer can generate images with
multipleobjectsandcomplexcontexts,offeringamorere-
3.Methodology
alisticsimulationforreal-worlddetectionscenarios.(ii)We
introduceadatasynthesisframeworkfordetection,termed Inthissection,wepresentdetailsforconstructingadataset
as InstaGen. This is achieved through a novel ground- synthesizer,thatenablestogeneratephoto-realisticimages
ing module that enables to generate labels and bounding withboundingboxesforeachobjectinstance, andtrainan
boxesforobjectsinsyntheticimages.(iii)Wetrainstandard objectdetectoronthecombinedrealandsyntheticdatasets.
object detectors on the combination of real and synthetic
dataset, and demonstrate superior performance over exist- 3.1.ProblemFormulation
ingstate-of-the-artdetectorsacrossvariousbenchmarks,in-
Given a detection dataset of real images with manual an-
cludingopen-vocabularydetection(increasingAveragePre-
notations, i.e., D = {(x ,B ,Y ),...,(x ,B ,Y )},
cision[AP]by+4.5), data-sparsedetection(enhancingAP real 1 1 1 N N N
where B = {b ,...,b |b ∈ R2×2} denotes the set of
by+1.2to+5.2),andcross-datasettransfer(boostingAPby i 1 m j
box coordinates for the annotated instances in one image,
+0.5to+1.1).
andY
i
= {y 1,...,y m|y
j
∈ RCbase}referstothecategories
of the instances. Our goal is thus to exploit the given real
2.RelatedWork
dataset (D ), to steer a generative diffusion model into
real
Object Detection. Object detection aims to simultane- datasetsynthesizer,thatenablestoaugmenttheexistingde-
ouslypredictthecategoryandcorrespondingboundingbox tectiondataset,i.e.,D =D +D . Asaresult,detec-
final real syn
for the objects in the images. Generally, object detec- torstrainedonthecombineddatasetdemonstrateenhanced
tors[3,4,6,26,27]aretrainedonasubstantialamountof ability,i.e.,extendingthedetectioncategoriesorimproving
training data with bounding box annotations and can only thedetectionperformance.
recognize a predetermined set of categories present in the Inthefollowingsections,wefirstdescribetheprocedure
2(a)Fine-tuningdiffusionmodelondetectiondataset. (b)Supervisedtrainingandself-trainingforgroundinghead(i.e.student).
Figure2.Illustrationoftheprocessforfinetuningdiffusionmodelandtrainingthegroundinghead:(a)stablediffusionmodelisfine-tuned
on the detection dataset on base categories. (b) The grounding head is trained on synthetic images, with supervised learning on base
categoriesandself-trainingonnovelcategories.
forconstructinganimagesynthesizer,thatcangenerateim- images,andconstructthetextpromptwithcategoriesinthe
ages suitable for training object detector (Section 3.2). To imagecrops,asshowninFigure2a. Ifanimagecropcon-
simultaneously generate the images and object bounding tainsmultipleobjectsofthesamecategory,weonlyusethis
boxes, we propose a novel instance-level grounding mod- categorynameonceinthetextprompt.
ule,whichalignsthetextembeddingofcategorynamewith Fine-tuningloss. Weusethesampledimagecropandcon-
theregionalvisualfeaturesfromimagesynthesizer,andin-
structedtextprompttofine-tuneSDMwithasquarederror
fersthecoordinatesfortheobjectsinsyntheticimages. To lossonthepredictednoisetermasfollows:
further improve the alignment towards objects of arbitrary (cid:104) (cid:105)
L =E ||ϵ−ϵ (zt,t,y)||2 , (1)
category,weadoptself-trainingtotunethegroundingmod- fine-tune z,ϵ∼N(0,1),t,y θ 2
ule on object categories not existing in D (Section 3.3).
real wherez denotesalatentvectormappedfromtheinputim-
As a result, the proposed model, termed as InstaGen, can
agewithVAE,tdenotesthedenoisingstep,uniformlysam-
automatically generate images along with bounding boxes
pledfrom{1,...,T},T referstothelengthofthediffusion
for object instances, and construct synthetic dataset (D )
syn Markov chain, and ϵ refers to the estimated noise from
atscale,leadingtoimprovedabilitywhentrainingdetectors θ
SDM with parameters θ being updated. We have experi-
onit(Section3.4).
mentally verified the necessity of this fine-tuning step, as
3.2.ImageSynthesizerforObjectDetection showninTable4.
Here, we build our image synthesizer based on an off-the- 3.3.DatasetSynthesizerforObjectDetection
shelfstablediffusionmodel(SDM[28]). Despiteofitsim-
In this section, we present details for steering the image
pressive ability in generating photo-realistic images, it of-
synthesizer into dataset synthesizer for object detection,
ten outputs images with only one or two objects on sim-
which enables to simultaneously generate images and ob-
plistic background with the text prompts, for example, ‘a
jectboundingboxes. Specifically,weproposeaninstance-
photographofa[category1name]anda[category2name]’,
level grounding module that aligns the text embedding of
as demonstrated in Figure 4b. As a result, object detec-
object category, with the regional visual feature of the
torstrainedonsuchimagesmayexhibitreducedrobustness
diffusion model, and infers the coordinates for bounding
whendealingwithcomplexreal-worldscenarios. Tobridge
boxes, effectively augmenting the image synthesizer with
such domain gap, we propose to construct the image syn-
instance grounding, as shown in Figure 3. To further im-
thesizerbyfine-tuningtheSDMwithanexistingreal-world
provethealignmentinlargevisualdiversity, weproposea
detectiondataset(D ).
real self-training scheme that enables the grounding module to
Fine-tuning procedure. To fine-tune the stable diffusion
generalisetowardsarbitrarycategories,includingthosenot
model (SDM), one approach is to na¨ıvely use the sample
exist in real detection dataset (D ). Asa result, our data
real
fromdetectiondataset, forexample, randomlypickanim-
synthesizer, termed as InstaGen, can be used to construct
ageandconstructthetextpromptwithallcategoriesinthe
syntheticdatasetfortrainingobjectdetectors.
image. However, as the image often contains multiple ob-
jects, such approach renders significant difficulty for fine- 3.3.1 InstanceGroundingonBaseCategories
tuning the SDM, especially for small or occluded objects. Tolocalisetheobjectinstancesinsyntheticimages,wein-
Weadoptamildstrategybytakingrandomcropsfromthe troduce an open-vocabulary grounding module, that aims
3Figure3.IllustrationofthedatasetgenerationprocessinInstaGen.Thedatagenerationprocessconsistsoftwosteps:(i)Imagecollection:
givenatextprompt,SDMgeneratesimageswiththeobjectsdescribedinthetextprompt;(ii)Annotationgeneration: theinstance-level
groundingheadalignsthecategoryembeddingwiththevisualfeatureregionofSDM,generatingthecorrespondingobjectbounding-boxes.
to simultaneously generate image (x) and the correspond- an image cross-attention layer for combining image fea-
ing instance-level bounding boxes (B) based on a set of tures, and a text cross-attention layer for combining text
categories (Y), i.e., {x,B,Y} = Φ (ϵ,Y), where features. Finally, we apply the dot product between each
InstaGen
ϵ∼N(0,I)denotesthesamplednoise. queryandthetextfeatures,followedbyaSigmoidfunction
topredicttheclassificationscoresˆforeachcategory. Addi-
Tothisend, weproposeaninstancegroundinghead, as
tionally,theobjectqueriesarepassedthroughaMulti-Layer
shown in Figure 3, it takes the intermediate representation
Perceptron(MLP)topredicttheobjectboundingboxesˆb,as
fromimagesynthesizerandthetextembeddingofcategory
shown in Figure 3. We train the grounding head by align-
as inputs, then predicts the corresponding object bounding
ing the category embedding with the regional visual fea-
boxes, i.e., {B ,Y } = Φ (F ,Φ (g(Y ))), where
i i g-head i t-enc i turesfromdiffusionmodel,asdetailedbelow.Oncetrained,
F = {f1,...,fn}referstothemulti-scaledensefeatures
i i i thegroundingheadisopen-vocabulary,i.e.,givenanycat-
fromtheimagesynthesizerattimestept = 1,g(·)denotes
egories (even beyond the training categories), the ground-
atemplatethatdecorateseachofthevisualcategoriesinthe
ing head can generate the corresponding bounding-boxes
text prompt, e.g., ‘a photograph of [category1 name] and
fortheobjectinstances.
[category2name]’,Φ (·)denotesthetextencoder.
t-enc
Inspired by GroundingDINO [22], our grounding head Training triplets of base categories. Following [18], we
Φ (·) mainly contains four components: (i) a channel- applyanautomaticpipelinetoconstructthe{visualfeature,
g-head
compression layer, implemented with a 3×3 convolution, bounding-box, text prompt} triplets, with an object detec-
forreducingthedimensionalityofthevisualfeatures;(ii)a tor trained on base categories from a given dataset (D ).
real
featureenhancer,consistingofsixfeatureenhancerlayers, In specific, assuming there exists a set of base categories
tofusethevisualandtextfeatures.Eachlayeremploysade- {c1 ,...,cN }, e.g., the classes in MS-COCO [20]. We
base base
formableself-attentiontoenhanceimagefeatures,avanilla firstselectarandomnumberofbasecategoriestoconstruct
self-attention for text feature enhancers, an image-to-text a text prompt, e.g., ‘a photograph of [base category1] and
cross-attention and a text-to-image cross-attention for fea- [basecategory2]’,andgenerateboththevisualfeaturesand
turefusion;(iii)alanguage-guidedqueryselectionmodule images with our image synthesizer. Then we take an off-
forqueryinitialization. Thismodulepredictstop-N anchor the-shelfobjectdetector,forexample,pre-trainedMaskR-
boxesbasedonthesimilaritybetweentextfeaturesandim- CNN [12], to run the inference procedure on the synthetic
agefeatures.FollowingDINO[38],itadoptsamixedquery images, and infer the bounding boxes of the selected cate-
selection where the positional queries are initialized with gories. To acquire the confident bounding-boxes for train-
theanchorboxesandthecontentqueriesremainlearnable; ing, we use a score threshold α to filter out the bounding-
(iv)across-modalitydecoderforclassificationandboxre- boxes with low confidence (an ablation study on the se-
finement. It comprises six decoder layers, with each layer lection of the score threshold has been conducted in Sec-
utilizing a self-attention mechanism for query interaction, tion4.5). Asaresult,aninfinitenumberoftrainingtriplets
4(a)StableDiffusion+Groundingheadw/Super- (b) Stable Diffusion + Grounding head w/ (c)StableDiffusionw/Fine-tuning+Grounding
visedtraining. Supervised-andSelf-training. headw/Supervised-andSelf-training.
Figure 4. Visualization of the synthetic images and bounding-boxes generated from different models. The bounding-boxes with green
denotetheobjectsfrombasecategories,whiletheoneswithreddenotetheobjectsfromnovelcategories.
forthegivenbasecategoriescanbeconstructedbyrepeat- module) with teacher model on the visual features to pro-
ingtheaboveoperation. duce bounding boxes, and then use a score threshold β to
Training loss. We use the constructed training triplets to filter out those with low confidence, and use the remain-
trainthegroundinghead: ing training triplets (F i,ˆb i,y inovel) to train the student, i.e.,
groundinghead.
N
L
base
=(cid:88) [L cls(sˆ i,c i)+1 {ci̸=∅}L box(ˆb i,b i)], (2) Trainingloss. Now, wecanalsotrainthegroundinghead
i=1 ontheminedtripletsofnovelcategories(thatareunseenin
wheretheithprediction(sˆ,ˆb )fromtheN objectqueries theexistingrealdataset)withthetraininglossL noveldefined
i i similartoEq.2. Thus,thetotaltraininglossfortrainingthe
is assigned to a ground-truth (c , b ) or ∅ (no object) with
i i groundingheadcanbe: L =L +L .
bipartite matching. L and L denote the classification grounding base novel
cls box
loss(e.g. Focalloss)andboxregressionloss(e.g. L1loss
3.4.TrainingDetectorwithSyntheticDataset
andGIoUloss),respectively.
Inthissection,weaugmenttherealdataset(D ),withsyn-
real
3.3.2 InstanceGroundingonNovelCategories theticdataset(D syn),andtrainpopularobjectdetectors,for
example,FasterR-CNN[27]withthestandardtrainingloss:
Till here, we have obtained a diffusion model with open-
vocabulary grounding, which has been only trained with L =L +L +L +L , (3)
det rpncls rpnbox detcls detbox
basecategories. Inthissection,weproposetofurtherlever-
whereL ,L aretheclassificationandboxregres-
age the synthetic training triplets from a wider range of rpncls rpnbox
sionlossesofregionproposalnetwork,andL ,L
categories to enhance the alignment for novel/unseen cat- detcls detbox
aretheclassificationandboxregressionlossesofthedetec-
egories. Specifically,asshowninFigure2b,wedescribea
tionhead. Generallyspeaking,thesyntheticdatasetenables
frameworkthatgeneratesthetrainingtripletsfornovelcat-
to improve the detector’s ability from two aspects: (i) ex-
egoriesusingthegroundeddiffusionmodel, andthenself-
pandingtheoriginaldatawithmorecategories,(ii)improve
trainthegroundinghead.
thedetectionperformancebyincreasingdatadiversity.
Training triplets of novel categories. We design the text
Expanding detection categories. The grounding head is
prompts of novel categories, e.g., ‘a photograph of [novel
designedtobeopen-vocabulary,thatenablestogenerateob-
category1] and [novel category2]’, and pass them through
jectboundingboxesfornovelcategories, eventhoughitis
ourproposedimagesynthesizer,togeneratethevisualfea-
trained with a specific set of base categories. This feature
tures. To acquire the corresponding bounding-boxes for
enables InstaGen to construct a detection dataset for any
novel categories, we propose a self-training scheme that
category. Figure 4 demonstrates several synthetic images
takestheabovegroundingheadasthestudent,andapplya
andobjectboundingboxesfornovelcategories,i.e.,theob-
meanteacher(anexponentialmovingaverage(EMA)ofthe
jectwithredboundingbox.Weevaluatetheeffectivenessof
student model) to create pseudo labels for update. In con-
trainingonsyntheticdatasetthroughexperimentsonopen-
trast to the widely adopted self-training scheme that takes
vocabulary detection benchmark. For more details, please
theimageasinput,thestudentandteacherinourcaseonly
refertoFigure1bandSection4.2.
takethevisualfeaturesasinput,thuscannotapplydataaug-
mentationasforimages. Instead,weinsertdropoutmodule Increasing data diversity. The base diffusion model is
after each feature enhancer layer and decoder layer in the trained on a large corpus of image-caption pairs, that en-
student. Duringtraining,weruninference(withoutdropout ablestogeneratediverseimages. Takingadvantageofsuch
5Method Supervision Detector Backbone AP50box AP50box AP50box
all base novel
Detic[41] CLIP FasterR-CNN R50 45.0 47.1 27.8
PromptDet[5] CLIP FasterR-CNN R50 - 50.6 26.6
BARON[34] CLIP FasterR-CNN R50 53.5 60.4 34.0
OADP[33] CLIP FasterR-CNN R50 47.2 53.3 30.0
ViLD[9] CLIP MaskR-CNN R50 51.3 59.5 27.6
F-VLM[16] CLIP MaskR-CNN R50 39.6 - 28.0
RO-ViT[13] CLIP MaskR-CNN ViT-B[1] 41.5 - 30.2
VLDet[19] CLIP CenterNet2[40] R50 45.8 50.6 32.0
CxORA[35] CLIP DAB-DETR[21] R50 35.4 35.5 35.1
DK-DETR[17] CLIP DeformableDETR[42] R50 - 61.1 32.3
EdaDet[31] CLIP DeformableDETR[42] R50 52.5 57.7 37.8
InstaGen StableDiffusion FasterR-CNN R50 52.3 55.8 42.3
Table1. Resultsonopen-vocabularyCOCObenchmark. AP50box isthemainmetricforevaluation. Ourdetector,trainedonsynthetic
novel
datasetfromInstaGen,significantlyoutperformsstate-of-the-artCLIP-basedapproachesonnovelcategories.
capabilities,InstaGeniscapableofgeneratingdatasetwith thetextencoderofCLIPiskeptfrozen,whiletheremaining
diverseimagesandboxannotations,whichcanexpandthe componentsaretrainedfor6epochswithabatchsizeof16
originaldataset,i.e.,increasethedatadiversityandimprove andalearningrateof1e-4.
detection performance, particularly in data-sparse scenar-
ios. We conducted experiments with varying proportions Instancegroundingmodule. Westartbyconstructingthe
of COCO [20] images as available real data, and show the training triplets using base categories i.e., the categories
effectivenessoftrainingonsyntheticdatasetwhenthenum- present in the existing dataset. The text prompt for each
berofreal-worldimagesislimited. Wereferthereadersfor tripletisconstructedbyrandomlyselectingoneortwocat-
moredetailsinSection4.3,andresultsinFigure1c. egories. Theregionalvisualfeaturesaretakenfromtheim-
agesynthesizertimestept=1,andtheoracleground-truth
4.Experiment bounding boxes are obtained using a Mask R-CNN model
trainedonbasecategories,asexplainedinSection3.3.1.
Inthissection,weusetheproposedInstaGentoconstruct
Subsequently, we train the instance grounding module
syntheticdatasetfortrainingobjectdetectors,i.e.,generat-
with these training triplets for 6 epochs, with a batch size
ingimageswiththecorrespondingboundingboxes. Specif-
of 64. In the 6th epoch, we transfer the weights from the
ically,wepresenttheimplementationdetailsinSection4.1.
studentmodeltotheteachermodel,andproceedtotrainthe
To evaluate the effectiveness of the synthetic dataset for
studentforanadditional6epochs. Duringthistraining,the
trainingobjectdetector,weconsiderthreeprotocols: open-
student receives supervised training on the base categories
vocabulary object detection (Section 4.2), data-sparse ob-
and engages in self-training on novel categories, and the
ject detection (Section 4.3) and cross-dataset object detec-
teachermodelisupdatedusingexponentialmovingaverage
tion (Section 4.4). Lastly, we conduct ablation studies on
(EMA)withamomentumof0.999.Theinitiallearningrate
theeffectivenessoftheproposedcomponentsandtheselec-
issetto1e-4andissubsequentlyreducedbyafactorof10
tionofhyper-parameters(Section4.5).
atthe11-thepoch,andthescorethresholdsαandβ areset
4.1.Implementationdetails to0.8and0.4,respectively.
Network architecture. We build image synthesizer from
Trainingobjectdetectoroncombineddataset. Inourex-
thepre-trainedStableDiffusionv1.4[28],andusetheCLIP
periment, we train an object detector (Faster R-CNN [27])
text encoder [24] to get text embedding for the category
withResNet-50[11]asbackbone,onacombinationofthe
name. Thechannelcompressionlayermapsthedimension
existingrealdatasetandthesyntheticdataset. Specifically,
ofvisualfeaturesto256,whichisimplementedwitha3×3
for synthetic dataset, we randomly select one or two cat-
convolution.Forsimplicity,thefeatureenhancer,language-
egories at each iteration, construct the text prompts, and
guidedqueryselectionmoduleandcross-modalitydecoder
feedthemasinputtogeneratesimagesalongwiththecor-
aredesignedtothesamestructureastheonesin[22]. The
responding bounding boxes with β of 0.4. Following the
numberoftheobjectqueriesissetto900.
standardimplementation[27],thedetectoristrainedfor12
Constructing image synthesizer. In our experiments, we epochs(1×learningschedule)unlessspecified. Theinitial
firstfine-tunethestablediffusionmodelonarealdetection learning rate is set to 0.01 and then reduced by a factor of
dataset,e.g.,theimagesofbasecategories.Duringtraining, 10atthe8thandthe11thepochs.
6InstaGen 10% 25% 50% 75% 100% Method Supervision Detector ExtraData Object365 LVIS
✗ 23.3 29.5 34.1 36.1 37.5 Gaoetal.[7] CLIP CenterNet2 ✓ 6.9 8.0
✓ 28.5 32.6 35.8 37.3 38.5 VL-PLM[39] CLIP MaskR-CNN ✓ 10.9 22.2
InstaGen StableDiffusion FasterR-CNN ✗ 11.4 23.3
Table 2. Results on data-sparse object de-
tection. WeemployFasterR-CNNwiththe Table3.ResultsongeneralizingCOCO-basetoObject365andLVIS.Alldetectorsutilize
ResNet-50 backbone as the default object the ResNet-50 backbone. The evaluation protocol follows [7] and reports AP50. Extra
detectorandevaluateitsperformanceusing datareferstoanadditionaldatasetthatencompassesobjectsfromthecategorieswithinthe
the AP metric on MS COCO benchmark. targetdataset. Inbothexperiments,theextradataconsistsofalltheimagesfromCOCO,
Pleaserefertothetextformoredetails. whichhascoveredthemajorityofcategoriesinObject365andLVIS.
G-head ST FT AP50box AP50box AP50box synthesizer,andtrainaMaskR-CNNforgeneratingoracle
all base novel
ground-truthboundingboxesinsyntheticimages. Weem-
✓ 50.6 55.3 37.1
ploy1000syntheticimagespercategorytotrainaFasterR-
✓ ✓ 51.1 55.0 40.3
✓ ✓ ✓ 52.3 55.8 42.3 CNNinconjunctionwiththecorrespondingCOCOsubset.
TheperformanceismeasuredbyAveragePrecision[20].
Table4. Theeffectivenessoftheproposedcomponents. G-head,
Comparisontobaseline. AsshowninTable2, theFaster
STandFTrefertothegroundinghead,self-trainingthegrounding
R-CNN trained with synthetic images achieves consistent
headandfine-tuningSDM,respectively.
improvementacrossvariousrealtrainingdatabudgets. No-
4.2.Open-vocabularyobjectdetection tably, as the availability of real data becomes sparse, syn-
thetic dataset plays even more important role for perfor-
Experimentalsetup.Followingthepreviousworks[5,39], mance improvement, for instance, it improves the detector
we conduct experiments on the open-vocabulary COCO by +5.2 AP (23.3→28.5 AP) when only 10% real COCO
benchmark,where48classesaretreatedasbasecategories, trainingsubsetisavailable.
and17classesasthenovelcategories. Totraintheground-
ing head, we employ 1000 synthetic images per category 4.4.Cross-datasetobjectdetection
per training epoch. While for training the object detector,
Experimental setup. In this section, we assess the ef-
weuse3000syntheticimagespercategory, alongwiththe
fectiveness of synthetic data on a more challenging task,
originalrealdatasetforbasecategories.Theobjectdetector
namely cross-dataset object detection. Following [39], we
istrainedwithinputsizeof800×800andscalejitter. The
evaluate theCOCO-trained modelon twounseen datasets:
performanceismeasuredbyCOCOAveragePrecisionatan
Object365 [30] and LVIS [10]. Specifically, we consider
IntersectionoverUnionof0.5(AP50).
the 48 classes in the open-vocabulary COCO benchmark
Comparison to SOTA. As shown in Table 1, we eval- as the source dataset, while Object365 (with 365 classes)
uate the performance by comparing with existing CLIP- and LVIS (with 1203 classes) serve as the target dataset.
basedopen-vocabularyobjectdetectors. Itisclearthatour When training the instance grounding module, we acquire
detector trained on synthetic dataset from InstaGen out- 1000 synthetic images for base categories from the source
performs existing state-of-the-art approaches significantly, dataset,and100syntheticimagesforthecategoryfromthe
i.e., around +5AP improvement over the second best. In targetdatasetateachtrainingiteration. Inthecaseoftrain-
essence, through the utilization of our proposed open- ingtheobjectdetector,weemploy500syntheticimagesper
vocabulary grounding head, InstaGen is able to generate categoryfromthetargetdatasetforeachtrainingiteration.
detectiondatafornovelcategories,enablingthedetectorto Thedetectoristrainedwithinputsizeof1024×1024and
attain exceptional performance. To the best of our knowl- scalejitter[39].
edge,thisisthefirstworkthatappliesgenerativediffusion
Comparison to SOTA. The results presented in Table 3
model for dataset synthesis, to tackle open-vocabulary ob-
demonstrate that the proposed InstaGen achieves supe-
jectdetection,andshowcaseitssuperiorityinthistask.
riorperformanceingeneralizationfromCOCO-basetoOb-
ject365andLVIS,whencomparedtoCLIP-basedmethods
4.3.Data-sparseobjectdetection
suchas[7,39]. ItisworthnotingthatCLIP-basedmethods
Experimental setup. Here, we evaluate the effectiveness require the generation of pseudo-labels for the categories
of synthetic dataset in data-spare scenario, by varying the fromthetargetdatasetonCOCOimages,andsubsequently
amount of real data. We randomly select subsets compris- trainthedetectorusingtheseimages.Thesemethodsneces-
ing10%,25%,50%,75%and100%oftheCOCOtraining sitate a dataset that includes objects belonging to the cate-
set,thiscoversallCOCOcategories.Thesesubsetsareused goriesofthetargetdataset. Incontrast,InstaGenpossesses
to fine-tune stable diffusion model for constructing image the ability to generate images featuring objects of any cat-
7#Images AP50box AP50box AP50box α AP50box AP50box AP50box β AP50box AP50box AP50box
all base novel all base novel all base novel
1000 51.6 55.9 39.7 0.7 51.3 55.1 40.6 0.3 46.4 53.3 26.9
2000 51.7 55.4 41.1 0.8 52.3 55.8 42.3 0.4 52.3 55.8 42.3
3000 52.3 55.8 42.3 0.9 51.8 55.6 41.1 0.5 51.2 55.4 39.2
Table5.Numberofgeneratedimages. Table6.αforbounding-boxfiltration. Table7.βforbounding-boxfiltration.
egory without the needfor additional datasets, thereby en- categories,showingthescalabilityoftheproposedtraining
hancingitsversatilityacrossvariousscenarios. mechanism.
Score thresholds for bounding box filtration. We com-
4.5.Ablationstudy
paretheperformancewithdifferentscorethresholdsαand
To understand the effectiveness of the proposed compo- βforfilteringboundingboxesonbasecategoriesandnovel
nents, we perform thorough ablation studies on the open- categories,respectively. FromtheexperimentresultsinTa-
vocabulary COCO benchmark [20], investigating the ef- ble6,weobservethattheperformanceisnotsensitivetothe
fectoffine-tuningstablediffusionmodel,traininginstance valueofα, andα = 0.8yieldsthebestperformance. The
groundingmodule,self-trainingonnovelcategories. Addi- experimental results using different β are presented in Ta-
tionally,weinvestigateotherhyper-parametersbycompar- ble7. Withalowscorethreshold(α = 0.3),therearestill
ingtheeffectivenessofsyntheticimagesanddifferentscore numerous inaccurate bounding boxes remaining, resulting
thresholdsforbaseandnovelcategories. in an AP of 26.9 for novel categories. by increasing β to
Fine-tuning diffusion model. We assess the effective- 0.4, numerous inaccurate bounding boxes are filtered out,
ness of fine-tuning stable diffusion model, and its impact resulting in optimal performance. Hence, we set α = 0.8
for synthesizing images for training object detector. Fig- andβ =0.4inourexperiments.
ure 4c illustrates that InstaGen is capable of generating
imageswithmoreintricatecontexts,featuringmultipleob- 5.Limitation
jects,smallobjects,andoccludedobjects.Subsequently,we
employedthesegeneratedimagestotrainFasterR-CNNfor UsingsyntheticorartificiallygenerateddataintrainingAI
objectdetection.TheresultsarepresentedinTable4,show- algorithms is a burgeoning practice with significant poten-
ingthatimagesynthesizerfromfine-tuningstablediffusion tial. It can address data scarcity, privacy, and bias issues.
model delivers improvement detection performance by 2.0 However, there remains two limitations for training object
AP(from40.3to42.3AP). detectors with synthetic data, (i) synthetic datasets com-
monlyfocusonclean,isolatedobjectinstances,whichlim-
Instancegroundingmodule.Todemonstratetheeffective-
itstheexposureofthedetectortothecomplexitiesandcon-
nessofthegroundingheadinopen-vocabularyscenario,we
textual diversity of real-world scenes, such as occlusions,
exclusivelytrainitonbasecategories. Visualizationexam-
clutter, varied environmental factors, deformation, there-
ples of the generated images are presented in Figure 4a.
fore, models trained on synthetic data struggle to adapt to
These examples demonstrate that the trained grounding
real-worldconditions,affectingtheiroverallrobustnessand
head is also capable of predicting bounding boxes for in-
accuracy,(ii)existingdiffusion-basedgenerativemodelalso
stancesfromnovelcategories. Leveragingthesegenerated
suffersfromlong-tailissue,thatmeansthegenerativemodel
images to train the object detector leads to a 37.1 AP on
struggles to generate images for objects of rare categories,
novelcategories,surpassingorrivalingallexistingstate-of-
resultinginimbalancedclassrepresentationduringtraining
the-artmethods,asshowninTable1andTable4.
andreduceddetectorperformanceforlesscommonobjects.
Self-training scheme. We evaluate the performance after
self-trainingthegroundingheadwithnovelcategories. As
6.Conclusion
showninTable4,trainingFasterR-CNNwiththegenerated
images of novel categories, leads to a noticeable enhance- This paper proposes a dataset synthesis pipeline, termed
mentindetectionperformance,increasingfrom37.1to40.3 as InstaGen, that enables to generate images with object
AP. Qualitatively, it also demonstrates enhanced recall for bounding boxes for arbitrary categories, acting as a free
novelobjectsafterself-training,asshowninFigure4b. sourceforconstructinglarge-scalesyntheticdatasettotrain
object detector. We have conducted thorough experiments
Number of synthetic images. We investigate the perfor-
to show the effectiveness of training on synthetic data, on
mancevariationwhileincreasingthenumberofthegener-
improving detection performance, or expanding the num-
atedimagespercategoryfordetectortraining. Asshownin berofdetectioncategories. Significantimprovementshave
Table 5, when increasing the number of generated images beenshowninvariousdetectionscenarios,includingopen-
from 1000 to 3000, the detector’s performance tends to be vocabulary(+4.5AP)anddata-sparse(+1.2∼5.2AP)de-
increasing monotonically, from 39.7 to 42.3 AP on novel tection.
8References [17] LiangqiLi,JiaxuMiao,DahuShi,WenmingTan,YeRen,Yi
Yang,andShiliangPu. Distillingdetrwithvisual-linguistic
[1] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
knowledgeforopen-vocabularyobjectdetection. InICCV,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
pages6501–6510,2023. 6
MostafaDehghani,MatthiasMinderer,GeorgHeigold,Syl-
[18] Ziyi Li, Qinye Zhou, Xiaoyun Zhang, Ya Zhang, Yanfeng
vain Gelly, et al. An image is worth 16x16 words: Trans-
Wang,andWeidiXie.Open-vocabularyobjectsegmentation
formers for image recognition at scale. arXiv preprint
withdiffusionmodels. InCVPR,pages7667–7676,2023. 4
arXiv:2010.11929,2020. 6
[19] ChuangLin,PeizeSun,YiJiang,PingLuo,LizhenQu,Gho-
[2] YuDu,FangyunWei,ZiheZhang,MiaojingShi,YueGao,
lamreza Haffari, Zehuan Yuan, and Jianfei Cai. Learning
andGuoqiLi. Learningtopromptforopen-vocabularyob-
object-language alignments for open-vocabulary object de-
jectdetectionwithvision-languagemodel. InCVPR,pages
tection. 2022. 6
14084–14093,2022. 2
[20] Tsung-YiLin,MichaelMaire,SergeBelongie,JamesHays,
[3] Chengjian Feng, Yujie Zhong, Yu Gao, Matthew R Scott,
PietroPerona,DevaRamanan,PiotrDolla´r,andCLawrence
andWeilinHuang. Tood: Task-alignedone-stageobjectde-
Zitnick. Microsoft coco: Common objects in context. In
tection.InICCV,pages3490–3499.IEEEComputerSociety,
ECCV,pages740–755.Springer,2014. 1,4,6,7,8
2021. 1,2
[21] ShilongLiu,FengLi,HaoZhang,XiaoYang,XianbiaoQi,
[4] ChengjianFeng,YujieZhong,andWeilinHuang. Exploring
Hang Su, Jun Zhu, and Lei Zhang. Dab-detr: Dynamic
classificationequilibriuminlong-tailedobjectdetection. In
anchor boxes are better queries for detr. arXiv preprint
ICCV,pages3417–3426,2021. 2
arXiv:2201.12329,2022. 6
[5] ChengjianFeng, YujieZhong, ZequnJie, XiangxiangChu,
[22] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao
HaibingRen,XiaolinWei,WeidiXie,andLinMa. Prompt-
Zhang,JieYang,ChunyuanLi,JianweiYang,HangSu,Jun
det:Towardsopen-vocabularydetectionusinguncuratedim-
Zhu, etal. Groundingdino: Marryingdinowithgrounded
ages. InECCV,pages701–717.Springer,2022. 1,2,6,7
pre-training for open-set object detection. arXiv preprint
[6] ChengjianFeng, ZequnJie, YujieZhong, XiangxiangChu,
arXiv:2303.05499,2023. 4,6
andLinMa. Aedet:Azimuth-invariantmulti-view3dobject
[23] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav
detection. InCVPR,pages21580–21588,2023. 2
Shyam,PamelaMishkin,BobMcGrew,IlyaSutskever,and
[7] Mingfei Gao, Chen Xing, Juan Carlos Niebles, Junnan Li,
MarkChen. Glide:Towardsphotorealisticimagegeneration
RanXu,WenhaoLiu,andCaimingXiong.Openvocabulary
andeditingwithtext-guideddiffusionmodels.arXivpreprint
objectdetectionwithpseudobounding-boxlabels.InECCV,
arXiv:2112.10741,2021. 2
pages266–282.Springer,2022. 7
[24] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
[8] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Xu,DavidWarde-Farley,SherjilOzair,AaronCourville,and
AmandaAskell,PamelaMishkin,JackClark,etal. Learn-
YoshuaBengio. Generativeadversarialnetworks. Commu-
ingtransferablevisualmodelsfromnaturallanguagesuper-
nicationsoftheACM,63(11):139–144,2020. 2
vision. pages8748–8763.PMLR,2021. 2,6
[9] Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, and Yin Cui.
[25] AdityaRamesh,PrafullaDhariwal,AlexNichol,CaseyChu,
Open-vocabulary object detection via vision and language
andMarkChen. Hierarchicaltext-conditionalimagegener-
knowledge distillation. arXiv preprint arXiv:2104.13921, ationwithcliplatents. arXivpreprintarXiv:2204.06125,1
2021. 2,6
(2):3,2022. 2
[10] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A
[26] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali
datasetforlargevocabularyinstancesegmentation.InCVPR,
Farhadi. Youonlylookonce: Unified,real-timeobjectde-
pages5356–5364,2019. 7 tection. InCVPR,pages779–788,2016. 1,2
[11] KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun. [27] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
Deep residual learning for image recognition. In CVPR, Fasterr-cnn: Towardsreal-timeobjectdetectionwithregion
pages770–778,2016. 6 proposalnetworks.Advancesinneuralinformationprocess-
[12] KaimingHe,GeorgiaGkioxari,PiotrDolla´r,andRossGir- ingsystems,28,2015. 1,2,5,6
shick. Mask r-cnn. In ICCV, pages 2961–2969, 2017. 1, [28] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
4 PatrickEsser,andBjo¨rnOmmer.High-resolutionimagesyn-
[13] DahunKim,AneliaAngelova,andWeichengKuo. Region- thesiswithlatentdiffusionmodels. InCVPR,pages10684–
awarepretrainingforopen-vocabularyobjectdetectionwith 10695,2022. 1,2,3,6
visiontransformers. InCVPR,pages11144–11154,2023. 6 [29] Chitwan Saharia, William Chan, Saurabh Saxena, Lala
[14] Durk P Kingma and Prafulla Dhariwal. Glow: Generative Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour,
flowwithinvertible1x1convolutions. NeurIPS,31,2018. 2 RaphaelGontijoLopes,BurcuKaragolAyan,TimSalimans,
[15] DiederikPKingmaandMaxWelling. Auto-encodingvaria- etal.Photorealistictext-to-imagediffusionmodelswithdeep
tionalbayes. arXivpreprintarXiv:1312.6114,2013. 2 languageunderstanding. NeurIPS,35:36479–36494, 2022.
[16] Weicheng Kuo, Yin Cui, Xiuye Gu, AJ Piergiovanni, and 1,2
AneliaAngelova. F-vlm: Open-vocabularyobjectdetection [30] ShuaiShao,ZemingLi,TianyuanZhang,ChaoPeng,Gang
uponfrozenvisionandlanguagemodels. 2022. 6 Yu, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365:
9A large-scale, high-quality dataset for object detection. In
ICCV,pages8430–8439,2019. 1,7
[31] Cheng Shi and Sibei Yang. Edadet: Open-vocabulary ob-
jectdetectionusingearlydensealignment. InICCV,pages
15724–15734,2023. 6
[32] Aaron Van den Oord, Nal Kalchbrenner, Lasse Espeholt,
Oriol Vinyals, Alex Graves, et al. Conditional image gen-
erationwithpixelcnndecoders. NeurIPS,29,2016. 2
[33] Luting Wang, Yi Liu, Penghui Du, Zihan Ding, Yue Liao,
QiaosongQi,BiaolongChen,andSiLiu. Object-awaredis-
tillation pyramid for open-vocabulary object detection. In
CVPR,pages11186–11196,2023. 6
[34] Size Wu, Wenwei Zhang, Sheng Jin, Wentao Liu, and
Chen Change Loy. Aligning bag of regions for open-
vocabularyobjectdetection. InCVPR,pages15254–15264,
2023. 6
[35] XiaoshiWu,FengZhu,RuiZhao,andHongshengLi. Cora:
Adapting clip for open-vocabulary detection with region
promptingandanchorpre-matching. InCVPR,pages7031–
7040,2023. 6
[36] Johnathan Xie and Shuai Zheng. Zsd-yolo: Zero-shot
yolodetectionusingvision-languageknowledgedistillation.
arXivpreprintarXiv:2109.12066,2(3):4,2021. 2
[37] AlirezaZareian,KevinDelaRosa,DerekHaoHu,andShih-
FuChang.Open-vocabularyobjectdetectionusingcaptions.
InCVPR,pages14393–14402,2021. 2
[38] HaoZhang,FengLi,ShilongLiu,LeiZhang,HangSu,Jun
Zhu, Lionel M Ni, and Heung-Yeung Shum. Dino: Detr
withimproveddenoisinganchorboxesforend-to-endobject
detection. arXivpreprintarXiv:2203.03605,2022. 4
[39] Shiyu Zhao, Zhixing Zhang, Samuel Schulter, Long Zhao,
BGVijayKumar,AnastasisStathopoulos,ManmohanChan-
draker,andDimitrisNMetaxas. Exploitingunlabeleddata
with vision and language models for object detection. In
ECCV,pages159–175.Springer,2022. 7
[40] Xingyi Zhou, Vladlen Koltun, and Philipp Kra¨henbu¨hl.
Probabilistic two-stage detection. arXiv preprint
arXiv:2103.07461,2021. 6
[41] Xingyi Zhou, Rohit Girdhar, Armand Joulin, Philipp
Kra¨henbu¨hl, and Ishan Misra. Detecting twenty-thousand
classesusingimage-levelsupervision.InECCV,pages350–
368.Springer,2022. 2,6
[42] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang
Wang,andJifengDai. Deformabledetr: Deformabletrans-
formers for end-to-end object detection. arXiv preprint
arXiv:2010.04159,2020. 6
10InstaGen: Enhancing Object Detection by Training on Synthetic Dataset
Supplementary Material
7.QualitativeResults ofnovelcategories. InFigure6,wefurthershowthequal-
itative results predicted by the Faster R-CNN trained with
WeshowmorequalitativeresultsgeneratedbyourInstaGen
the synthetic images form InstaGen on COCO validation
inFigure5. Withoutanymanualannotations,InstaGencan
set. Thedetectorcannowaccuratelylocalizeandrecognize
generate high-quality images with object bounding-boxes
theobjectsfromnovelcategories.
Figure5.QualitativeresultsgeneratedbyourInstaGen.Thebounding-boxeswithgreendenotetheobjectsfrombasecategories,whilethe
oneswithreddenotetheobjectsfromnovelcategories.
1Figure 6. Qualitative results from our Faster R-CNN trained with the synthetic images from InstaGen on COCO validation set. The
bounding-boxeswithgreendenotetheobjectsfrombasecategories,whiletheoneswithreddenotetheobjectsfromnovelcategories.
2