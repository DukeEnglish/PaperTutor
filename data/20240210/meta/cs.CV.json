[
    {
        "title": "InstaGen: Enhancing Object Detection by Training on Synthetic Dataset",
        "authors": "Chengjian FengYujie ZhongZequn JieWeidi XieLin Ma",
        "links": "http://arxiv.org/abs/2402.05937v1",
        "entry_id": "http://arxiv.org/abs/2402.05937v1",
        "pdf_url": "http://arxiv.org/pdf/2402.05937v1",
        "summary": "In this paper, we introduce a novel paradigm to enhance the ability of object\ndetector, e.g., expanding categories or improving detection performance, by\ntraining on synthetic dataset generated from diffusion models. Specifically, we\nintegrate an instance-level grounding head into a pre-trained, generative\ndiffusion model, to augment it with the ability of localising arbitrary\ninstances in the generated images. The grounding head is trained to align the\ntext embedding of category names with the regional visual feature of the\ndiffusion model, using supervision from an off-the-shelf object detector, and a\nnovel self-training scheme on (novel) categories not covered by the detector.\nThis enhanced version of diffusion model, termed as InstaGen, can serve as a\ndata synthesizer for object detection. We conduct thorough experiments to show\nthat, object detector can be enhanced while training on the synthetic dataset\nfrom InstaGen, demonstrating superior performance over existing\nstate-of-the-art methods in open-vocabulary (+4.5 AP) and data-sparse (+1.2 to\n5.2 AP) scenarios.",
        "updated": "2024-02-08 18:59:53 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.05937v1"
    },
    {
        "title": "SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large Language Models",
        "authors": "Peng GaoRenrui ZhangChris LiuLongtian QiuSiyuan HuangWeifeng LinShitian ZhaoShijie GengZiyi LinPeng JinKaipeng ZhangWenqi ShaoChao XuConghui HeJunjun HeHao ShaoPan LuHongsheng LiYu Qiao",
        "links": "http://arxiv.org/abs/2402.05935v1",
        "entry_id": "http://arxiv.org/abs/2402.05935v1",
        "pdf_url": "http://arxiv.org/pdf/2402.05935v1",
        "summary": "We propose SPHINX-X, an extensive Multimodality Large Language Model (MLLM)\nseries developed upon SPHINX. To improve the architecture and training\nefficiency, we modify the SPHINX framework by removing redundant visual\nencoders, bypassing fully-padded sub-images with skip tokens, and simplifying\nmulti-stage training into a one-stage all-in-one paradigm. To fully unleash the\npotential of MLLMs, we assemble a comprehensive multi-domain and multimodal\ndataset covering publicly available resources in language, vision, and\nvision-language tasks. We further enrich this collection with our curated OCR\nintensive and Set-of-Mark datasets, extending the diversity and generality. By\ntraining over different base LLMs including TinyLlama1.1B, InternLM2-7B,\nLLaMA2-13B, and Mixtral8x7B, we obtain a spectrum of MLLMs that vary in\nparameter size and multilingual capabilities. Comprehensive benchmarking\nreveals a strong correlation between the multi-modal performance with the data\nand parameter scales. Code and models are released at\nhttps://github.com/Alpha-VLLM/LLaMA2-Accessory",
        "updated": "2024-02-08 18:59:48 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.05935v1"
    },
    {
        "title": "WebLINX: Real-World Website Navigation with Multi-Turn Dialogue",
        "authors": "Xing Han LùZdeněk KasnerSiva Reddy",
        "links": "http://arxiv.org/abs/2402.05930v1",
        "entry_id": "http://arxiv.org/abs/2402.05930v1",
        "pdf_url": "http://arxiv.org/pdf/2402.05930v1",
        "summary": "We propose the problem of conversational web navigation, where a digital\nagent controls a web browser and follows user instructions to solve real-world\ntasks in a multi-turn dialogue fashion. To support this problem, we introduce\nWEBLINX - a large-scale benchmark of 100K interactions across 2300 expert\ndemonstrations of conversational web navigation. Our benchmark covers a broad\nrange of patterns on over 150 real-world websites and can be used to train and\nevaluate agents in diverse scenarios. Due to the magnitude of information\npresent, Large Language Models (LLMs) cannot process entire web pages in\nreal-time. To solve this bottleneck, we design a retrieval-inspired model that\nefficiently prunes HTML pages by ranking relevant elements. We use the selected\nelements, along with screenshots and action history, to assess a variety of\nmodels for their ability to replicate human behavior when navigating the web.\nOur experiments span from small text-only to proprietary multimodal LLMs. We\nfind that smaller finetuned decoders surpass the best zero-shot LLMs (including\nGPT-4V), but also larger finetuned multimodal models which were explicitly\npretrained on screenshots. However, all finetuned models struggle to generalize\nto unseen websites. Our findings highlight the need for large multimodal models\nthat can generalize to novel settings. Our code, data and models are available\nfor research: https://mcgill-nlp.github.io/weblinx",
        "updated": "2024-02-08 18:58:02 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.05930v1"
    },
    {
        "title": "Collaborative Control for Geometry-Conditioned PBR Image Generation",
        "authors": "Shimon VainerMark BossMathias PargerKonstantin KutsyDante De NigrisCiara RowlesNicolas PeronySimon Donné",
        "links": "http://arxiv.org/abs/2402.05919v1",
        "entry_id": "http://arxiv.org/abs/2402.05919v1",
        "pdf_url": "http://arxiv.org/pdf/2402.05919v1",
        "summary": "Current 3D content generation builds on generative models that output RGB\nimages. Modern graphics pipelines, however, require physically-based rendering\n(PBR) material properties. We propose to model the PBR image distribution\ndirectly to avoid photometric inaccuracies in RGB generation and the inherent\nambiguity in extracting PBR from RGB. Existing paradigms for cross-modal\nfinetuning are not suited for PBR generation due to a lack of data and the high\ndimensionality of the output modalities: we overcome both challenges by\nretaining a frozen RGB model and tightly linking a newly trained PBR model\nusing a novel cross-network communication paradigm. As the base RGB model is\nfully frozen, the proposed method does not risk catastrophic forgetting during\nfinetuning and remains compatible with techniques such as IPAdapter pretrained\nfor the base RGB model. We validate our design choices, robustness to data\nsparsity, and compare against existing paradigms with an extensive experimental\nsection.",
        "updated": "2024-02-08 18:53:21 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.05919v1"
    },
    {
        "title": "Point-VOS: Pointing Up Video Object Segmentation",
        "authors": "Idil Esen ZulfikarSabarinath MahadevanPaul VoigtlaenderBastian Leibe",
        "links": "http://arxiv.org/abs/2402.05917v1",
        "entry_id": "http://arxiv.org/abs/2402.05917v1",
        "pdf_url": "http://arxiv.org/pdf/2402.05917v1",
        "summary": "Current state-of-the-art Video Object Segmentation (VOS) methods rely on\ndense per-object mask annotations both during training and testing. This\nrequires time-consuming and costly video annotation mechanisms. We propose a\nnovel Point-VOS task with a spatio-temporally sparse point-wise annotation\nscheme that substantially reduces the annotation effort. We apply our\nannotation scheme to two large-scale video datasets with text descriptions and\nannotate over 19M points across 133K objects in 32K videos. Based on our\nannotations, we propose a new Point-VOS benchmark, and a corresponding\npoint-based training mechanism, which we use to establish strong baseline\nresults. We show that existing VOS methods can easily be adapted to leverage\nour point annotations during training, and can achieve results close to the\nfully-supervised performance when trained on pseudo-masks generated from these\npoints. In addition, we show that our data can be used to improve models that\nconnect vision and language, by evaluating it on the Video Narrative Grounding\n(VNG) task. We will make our code and annotations available at\nhttps://pointvos.github.io.",
        "updated": "2024-02-08 18:52:23 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.05917v1"
    }
]