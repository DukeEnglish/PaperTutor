[
    {
        "title": "SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large Language Models",
        "authors": "Peng GaoRenrui ZhangChris LiuLongtian QiuSiyuan HuangWeifeng LinShitian ZhaoShijie GengZiyi LinPeng JinKaipeng ZhangWenqi ShaoChao XuConghui HeJunjun HeHao ShaoPan LuHongsheng LiYu Qiao",
        "links": "http://arxiv.org/abs/2402.05935v1",
        "entry_id": "http://arxiv.org/abs/2402.05935v1",
        "pdf_url": "http://arxiv.org/pdf/2402.05935v1",
        "summary": "We propose SPHINX-X, an extensive Multimodality Large Language Model (MLLM)\nseries developed upon SPHINX. To improve the architecture and training\nefficiency, we modify the SPHINX framework by removing redundant visual\nencoders, bypassing fully-padded sub-images with skip tokens, and simplifying\nmulti-stage training into a one-stage all-in-one paradigm. To fully unleash the\npotential of MLLMs, we assemble a comprehensive multi-domain and multimodal\ndataset covering publicly available resources in language, vision, and\nvision-language tasks. We further enrich this collection with our curated OCR\nintensive and Set-of-Mark datasets, extending the diversity and generality. By\ntraining over different base LLMs including TinyLlama1.1B, InternLM2-7B,\nLLaMA2-13B, and Mixtral8x7B, we obtain a spectrum of MLLMs that vary in\nparameter size and multilingual capabilities. Comprehensive benchmarking\nreveals a strong correlation between the multi-modal performance with the data\nand parameter scales. Code and models are released at\nhttps://github.com/Alpha-VLLM/LLaMA2-Accessory",
        "updated": "2024-02-08 18:59:48 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.05935v1"
    },
    {
        "title": "Classifying Nodes in Graphs without GNNs",
        "authors": "Daniel WinterNiv CohenYedid Hoshen",
        "links": "http://arxiv.org/abs/2402.05934v1",
        "entry_id": "http://arxiv.org/abs/2402.05934v1",
        "pdf_url": "http://arxiv.org/pdf/2402.05934v1",
        "summary": "Graph neural networks (GNNs) are the dominant paradigm for classifying nodes\nin a graph, but they have several undesirable attributes stemming from their\nmessage passing architecture. Recently, distillation methods succeeded in\neliminating the use of GNNs at test time but they still require them during\ntraining. We perform a careful analysis of the role that GNNs play in\ndistillation methods. This analysis leads us to propose a fully GNN-free\napproach for node classification, not requiring them at train or test time. Our\nmethod consists of three key components: smoothness constraints,\npseudo-labeling iterations and neighborhood-label histograms. Our final\napproach can match the state-of-the-art accuracy on standard popular benchmarks\nsuch as citation and co-purchase networks, without training a GNN.",
        "updated": "2024-02-08 18:59:30 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.05934v1"
    },
    {
        "title": "Time Series Diffusion in the Frequency Domain",
        "authors": "Jonathan CrabbéNicolas HuynhJan StanczukMihaela van der Schaar",
        "links": "http://arxiv.org/abs/2402.05933v1",
        "entry_id": "http://arxiv.org/abs/2402.05933v1",
        "pdf_url": "http://arxiv.org/pdf/2402.05933v1",
        "summary": "Fourier analysis has been an instrumental tool in the development of signal\nprocessing. This leads us to wonder whether this framework could similarly\nbenefit generative modelling. In this paper, we explore this question through\nthe scope of time series diffusion models. More specifically, we analyze\nwhether representing time series in the frequency domain is a useful inductive\nbias for score-based diffusion models. By starting from the canonical SDE\nformulation of diffusion in the time domain, we show that a dual diffusion\nprocess occurs in the frequency domain with an important nuance: Brownian\nmotions are replaced by what we call mirrored Brownian motions, characterized\nby mirror symmetries among their components. Building on this insight, we show\nhow to adapt the denoising score matching approach to implement diffusion\nmodels in the frequency domain. This results in frequency diffusion models,\nwhich we compare to canonical time diffusion models. Our empirical evaluation\non real-world datasets, covering various domains like healthcare and finance,\nshows that frequency diffusion models better capture the training distribution\nthan time diffusion models. We explain this observation by showing that time\nseries from these datasets tend to be more localized in the frequency domain\nthan in the time domain, which makes them easier to model in the former case.\nAll our observations point towards impactful synergies between Fourier analysis\nand diffusion models.",
        "updated": "2024-02-08 18:59:05 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.05933v1"
    },
    {
        "title": "An Interactive Agent Foundation Model",
        "authors": "Zane DuranteBidipta SarkarRan GongRohan TaoriYusuke NodaPaul TangEhsan AdeliShrinidhi Kowshika LakshmikanthKevin SchulmanArnold MilsteinDemetri TerzopoulosAde FamotiNoboru KunoAshley LlorensHoi VoKatsu IkeuchiLi Fei-FeiJianfeng GaoNaoki WakeQiuyuan Huang",
        "links": "http://arxiv.org/abs/2402.05929v1",
        "entry_id": "http://arxiv.org/abs/2402.05929v1",
        "pdf_url": "http://arxiv.org/pdf/2402.05929v1",
        "summary": "The development of artificial intelligence systems is transitioning from\ncreating static, task-specific models to dynamic, agent-based systems capable\nof performing well in a wide range of applications. We propose an Interactive\nAgent Foundation Model that uses a novel multi-task agent training paradigm for\ntraining AI agents across a wide range of domains, datasets, and tasks. Our\ntraining paradigm unifies diverse pre-training strategies, including visual\nmasked auto-encoders, language modeling, and next-action prediction, enabling a\nversatile and adaptable AI framework. We demonstrate the performance of our\nframework across three separate domains -- Robotics, Gaming AI, and Healthcare.\nOur model demonstrates its ability to generate meaningful and contextually\nrelevant outputs in each area. The strength of our approach lies in its\ngenerality, leveraging a variety of data sources such as robotics sequences,\ngameplay data, large-scale video datasets, and textual information for\neffective multimodal and multi-task learning. Our approach provides a promising\navenue for developing generalist, action-taking, multimodal systems.",
        "updated": "2024-02-08 18:58:02 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.05929v1"
    },
    {
        "title": "WebLINX: Real-World Website Navigation with Multi-Turn Dialogue",
        "authors": "Xing Han LùZdeněk KasnerSiva Reddy",
        "links": "http://arxiv.org/abs/2402.05930v1",
        "entry_id": "http://arxiv.org/abs/2402.05930v1",
        "pdf_url": "http://arxiv.org/pdf/2402.05930v1",
        "summary": "We propose the problem of conversational web navigation, where a digital\nagent controls a web browser and follows user instructions to solve real-world\ntasks in a multi-turn dialogue fashion. To support this problem, we introduce\nWEBLINX - a large-scale benchmark of 100K interactions across 2300 expert\ndemonstrations of conversational web navigation. Our benchmark covers a broad\nrange of patterns on over 150 real-world websites and can be used to train and\nevaluate agents in diverse scenarios. Due to the magnitude of information\npresent, Large Language Models (LLMs) cannot process entire web pages in\nreal-time. To solve this bottleneck, we design a retrieval-inspired model that\nefficiently prunes HTML pages by ranking relevant elements. We use the selected\nelements, along with screenshots and action history, to assess a variety of\nmodels for their ability to replicate human behavior when navigating the web.\nOur experiments span from small text-only to proprietary multimodal LLMs. We\nfind that smaller finetuned decoders surpass the best zero-shot LLMs (including\nGPT-4V), but also larger finetuned multimodal models which were explicitly\npretrained on screenshots. However, all finetuned models struggle to generalize\nto unseen websites. Our findings highlight the need for large multimodal models\nthat can generalize to novel settings. Our code, data and models are available\nfor research: https://mcgill-nlp.github.io/weblinx",
        "updated": "2024-02-08 18:58:02 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.05930v1"
    }
]