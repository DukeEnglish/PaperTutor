[
    {
        "title": "SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large Language Models",
        "authors": "Peng GaoRenrui ZhangChris LiuLongtian QiuSiyuan HuangWeifeng LinShitian ZhaoShijie GengZiyi LinPeng JinKaipeng ZhangWenqi ShaoChao XuConghui HeJunjun HeHao ShaoPan LuHongsheng LiYu Qiao",
        "links": "http://arxiv.org/abs/2402.05935v1",
        "entry_id": "http://arxiv.org/abs/2402.05935v1",
        "pdf_url": "http://arxiv.org/pdf/2402.05935v1",
        "summary": "We propose SPHINX-X, an extensive Multimodality Large Language Model (MLLM)\nseries developed upon SPHINX. To improve the architecture and training\nefficiency, we modify the SPHINX framework by removing redundant visual\nencoders, bypassing fully-padded sub-images with skip tokens, and simplifying\nmulti-stage training into a one-stage all-in-one paradigm. To fully unleash the\npotential of MLLMs, we assemble a comprehensive multi-domain and multimodal\ndataset covering publicly available resources in language, vision, and\nvision-language tasks. We further enrich this collection with our curated OCR\nintensive and Set-of-Mark datasets, extending the diversity and generality. By\ntraining over different base LLMs including TinyLlama1.1B, InternLM2-7B,\nLLaMA2-13B, and Mixtral8x7B, we obtain a spectrum of MLLMs that vary in\nparameter size and multilingual capabilities. Comprehensive benchmarking\nreveals a strong correlation between the multi-modal performance with the data\nand parameter scales. Code and models are released at\nhttps://github.com/Alpha-VLLM/LLaMA2-Accessory",
        "updated": "2024-02-08 18:59:48 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.05935v1"
    },
    {
        "title": "Time Series Diffusion in the Frequency Domain",
        "authors": "Jonathan CrabbéNicolas HuynhJan StanczukMihaela van der Schaar",
        "links": "http://arxiv.org/abs/2402.05933v1",
        "entry_id": "http://arxiv.org/abs/2402.05933v1",
        "pdf_url": "http://arxiv.org/pdf/2402.05933v1",
        "summary": "Fourier analysis has been an instrumental tool in the development of signal\nprocessing. This leads us to wonder whether this framework could similarly\nbenefit generative modelling. In this paper, we explore this question through\nthe scope of time series diffusion models. More specifically, we analyze\nwhether representing time series in the frequency domain is a useful inductive\nbias for score-based diffusion models. By starting from the canonical SDE\nformulation of diffusion in the time domain, we show that a dual diffusion\nprocess occurs in the frequency domain with an important nuance: Brownian\nmotions are replaced by what we call mirrored Brownian motions, characterized\nby mirror symmetries among their components. Building on this insight, we show\nhow to adapt the denoising score matching approach to implement diffusion\nmodels in the frequency domain. This results in frequency diffusion models,\nwhich we compare to canonical time diffusion models. Our empirical evaluation\non real-world datasets, covering various domains like healthcare and finance,\nshows that frequency diffusion models better capture the training distribution\nthan time diffusion models. We explain this observation by showing that time\nseries from these datasets tend to be more localized in the frequency domain\nthan in the time domain, which makes them easier to model in the former case.\nAll our observations point towards impactful synergies between Fourier analysis\nand diffusion models.",
        "updated": "2024-02-08 18:59:05 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.05933v1"
    },
    {
        "title": "Driving Everywhere with Large Language Model Policy Adaptation",
        "authors": "Boyi LiYue WangJiageng MaoBoris IvanovicSushant VeerKaren LeungMarco Pavone",
        "links": "http://arxiv.org/abs/2402.05932v1",
        "entry_id": "http://arxiv.org/abs/2402.05932v1",
        "pdf_url": "http://arxiv.org/pdf/2402.05932v1",
        "summary": "Adapting driving behavior to new environments, customs, and laws is a\nlong-standing problem in autonomous driving, precluding the widespread\ndeployment of autonomous vehicles (AVs). In this paper, we present LLaDA, a\nsimple yet powerful tool that enables human drivers and autonomous vehicles\nalike to drive everywhere by adapting their tasks and motion plans to traffic\nrules in new locations. LLaDA achieves this by leveraging the impressive\nzero-shot generalizability of large language models (LLMs) in interpreting the\ntraffic rules in the local driver handbook. Through an extensive user study, we\nshow that LLaDA's instructions are useful in disambiguating in-the-wild\nunexpected situations. We also demonstrate LLaDA's ability to adapt AV motion\nplanning policies in real-world datasets; LLaDA outperforms baseline planning\napproaches on all our metrics. Please check our website for more details:\nhttps://boyiliee.github.io/llada.",
        "updated": "2024-02-08 18:59:03 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.05932v1"
    },
    {
        "title": "An Interactive Agent Foundation Model",
        "authors": "Zane DuranteBidipta SarkarRan GongRohan TaoriYusuke NodaPaul TangEhsan AdeliShrinidhi Kowshika LakshmikanthKevin SchulmanArnold MilsteinDemetri TerzopoulosAde FamotiNoboru KunoAshley LlorensHoi VoKatsu IkeuchiLi Fei-FeiJianfeng GaoNaoki WakeQiuyuan Huang",
        "links": "http://arxiv.org/abs/2402.05929v1",
        "entry_id": "http://arxiv.org/abs/2402.05929v1",
        "pdf_url": "http://arxiv.org/pdf/2402.05929v1",
        "summary": "The development of artificial intelligence systems is transitioning from\ncreating static, task-specific models to dynamic, agent-based systems capable\nof performing well in a wide range of applications. We propose an Interactive\nAgent Foundation Model that uses a novel multi-task agent training paradigm for\ntraining AI agents across a wide range of domains, datasets, and tasks. Our\ntraining paradigm unifies diverse pre-training strategies, including visual\nmasked auto-encoders, language modeling, and next-action prediction, enabling a\nversatile and adaptable AI framework. We demonstrate the performance of our\nframework across three separate domains -- Robotics, Gaming AI, and Healthcare.\nOur model demonstrates its ability to generate meaningful and contextually\nrelevant outputs in each area. The strength of our approach lies in its\ngenerality, leveraging a variety of data sources such as robotics sequences,\ngameplay data, large-scale video datasets, and textual information for\neffective multimodal and multi-task learning. Our approach provides a promising\navenue for developing generalist, action-taking, multimodal systems.",
        "updated": "2024-02-08 18:58:02 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.05929v1"
    },
    {
        "title": "Risk-Sensitive Multi-Agent Reinforcement Learning in Network Aggregative Markov Games",
        "authors": "Hafez GhaemiHamed KebriaeiAlireza Ramezani MoghaddamMajid Nili Ahamdabadi",
        "links": "http://arxiv.org/abs/2402.05906v1",
        "entry_id": "http://arxiv.org/abs/2402.05906v1",
        "pdf_url": "http://arxiv.org/pdf/2402.05906v1",
        "summary": "Classical multi-agent reinforcement learning (MARL) assumes risk neutrality\nand complete objectivity for agents. However, in settings where agents need to\nconsider or model human economic or social preferences, a notion of risk must\nbe incorporated into the RL optimization problem. This will be of greater\nimportance in MARL where other human or non-human agents are involved, possibly\nwith their own risk-sensitive policies. In this work, we consider\nrisk-sensitive and non-cooperative MARL with cumulative prospect theory (CPT),\na non-convex risk measure and a generalization of coherent measures of risk.\nCPT is capable of explaining loss aversion in humans and their tendency to\noverestimate/underestimate small/large probabilities. We propose a distributed\nsampling-based actor-critic (AC) algorithm with CPT risk for network\naggregative Markov games (NAMGs), which we call Distributed Nested CPT-AC.\nUnder a set of assumptions, we prove the convergence of the algorithm to a\nsubjective notion of Markov perfect Nash equilibrium in NAMGs. The experimental\nresults show that subjective CPT policies obtained by our algorithm can be\ndifferent from the risk-neutral ones, and agents with a higher loss aversion\nare more inclined to socially isolate themselves in an NAMG.",
        "updated": "2024-02-08 18:43:27 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.05906v1"
    }
]