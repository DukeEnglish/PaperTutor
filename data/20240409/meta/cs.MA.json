[
    {
        "title": "360°REA: Towards A Reusable Experience Accumulation with 360° Assessment for Multi-Agent System",
        "authors": "Shen GaoHao LiZhengliang ShiChengrui HuangQuan TuZhiliang TianMinlie HuangShuo Shang",
        "links": "http://arxiv.org/abs/2404.05569v1",
        "entry_id": "http://arxiv.org/abs/2404.05569v1",
        "pdf_url": "http://arxiv.org/pdf/2404.05569v1",
        "summary": "Large language model agents have demonstrated remarkable advancements across\nvarious complex tasks. Recent works focus on optimizing the agent team or\nemploying self-reflection to iteratively solve complex tasks. Since these\nagents are all based on the same LLM, only conducting self-evaluation or\nremoving underperforming agents does not substantively enhance the capability\nof the agents. We argue that a comprehensive evaluation and accumulating\nexperience from evaluation feedback is an effective approach to improving\nsystem performance. In this paper, we propose Reusable Experience Accumulation\nwith 360{\\deg} Assessment (360{\\deg}REA), a hierarchical multi-agent framework\ninspired by corporate organizational practices. The framework employs a novel\n360{\\deg} performance assessment method for multi-perspective performance\nevaluation with fine-grained assessment. To enhance the capability of agents in\naddressing complex tasks, we introduce dual-level experience pool for agents to\naccumulate experience through fine-grained assessment. Extensive experiments on\ncomplex task datasets demonstrate the effectiveness of 360{\\deg}REA.",
        "updated": "2024-04-08 14:43:13 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.05569v1"
    },
    {
        "title": "Opinion Dynamics for Utility Maximizing Agents: Exploring the Impact of Resource Penalty",
        "authors": "Prashil WankhedeNirabhra MandalSonia MartínezPavankumar Tallapragada",
        "links": "http://arxiv.org/abs/2404.04912v1",
        "entry_id": "http://arxiv.org/abs/2404.04912v1",
        "pdf_url": "http://arxiv.org/pdf/2404.04912v1",
        "summary": "We propose a continuous-time nonlinear model of opinion dynamics with\nutility-maximizing agents connected via a social influence network. A\ndistinguishing feature of the proposed model is the inclusion of an\nopinion-dependent resource-penalty term in the utilities, which limits the\nagents from holding opinions of large magnitude. The proposed utility functions\nalso account for how the relative resources within the social group affect both\nan agent's stubbornness and social influence. Each agent myopically seeks to\nmaximize its utility by revising its opinion in the gradient ascent direction\nof its utility function, thus leading to the proposed opinion dynamics. We show\nthat, for any arbitrary social influence network, opinions are ultimately\nbounded. For networks with weak antagonistic relations, we show that there\nexists a globally exponentially stable equilibrium using contraction theory. We\nestablish conditions for the existence of consensus equilibrium and analyze the\nrelative dominance of the agents at consensus. We also conduct a game-theoretic\nanalysis of the underlying opinion formation game, including on Nash equilibria\nand on prices of anarchy in terms of satisfaction ratios. Additionally, we also\ninvestigate the oscillatory behavior of opinions in a two-agent scenario.\nFinally, simulations illustrate our findings.",
        "updated": "2024-04-07 10:39:53 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.04912v1"
    },
    {
        "title": "Automatic Gradient Estimation for Calibrating Crowd Models with Discrete Decision Making",
        "authors": "Philipp AndelfingerJustin N. Kreikemeyer",
        "links": "http://arxiv.org/abs/2404.04678v1",
        "entry_id": "http://arxiv.org/abs/2404.04678v1",
        "pdf_url": "http://arxiv.org/pdf/2404.04678v1",
        "summary": "Recently proposed gradient estimators enable gradient descent over stochastic\nprograms with discrete jumps in the response surface, which are not covered by\nautomatic differentiation (AD) alone. Although these estimators' capability to\nguide a swift local search has been shown for certain problems, their\napplicability to models relevant to real-world applications remains largely\nunexplored. As the gradients governing the choice in candidate solutions are\ncalculated from sampled simulation trajectories, the optimization procedure\nbears similarities to metaheuristics such as particle swarm optimization, which\nputs the focus on the different methods' calibration progress per function\nevaluation. Here, we consider the calibration of force-based crowd evacuation\nmodels based on the popular Social Force model augmented by discrete decision\nmaking. After studying the ability of an AD-based estimator for branching\nprograms to capture the simulation's rugged response surface, calibration\nproblems are tackled using gradient descent and two metaheuristics. As our main\ninsights, we find 1) that the estimation's fidelity benefits from disregarding\njumps of large magnitude inherent to the Social Force model, and 2) that the\ncommon problem of calibration by adjusting a simulation input distribution\nobviates the need for AD across the Social Force calculations, allowing\ngradient descent to excel.",
        "updated": "2024-04-06 16:48:12 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.04678v1"
    },
    {
        "title": "Self-organizing Multiagent Target Enclosing under Limited Information and Safety Guarantees",
        "authors": "Praveen Kumar RanjanAbhinav SinhaYongcan Cao",
        "links": "http://arxiv.org/abs/2404.04497v1",
        "entry_id": "http://arxiv.org/abs/2404.04497v1",
        "pdf_url": "http://arxiv.org/pdf/2404.04497v1",
        "summary": "This paper introduces an approach to address the target enclosing problem\nusing non-holonomic multiagent systems, where agents autonomously self-organize\nthemselves in the desired formation around a fixed target. Our approach\ncombines global enclosing behavior and local collision avoidance mechanisms by\ndevising a novel potential function and sliding manifold. In our approach,\nagents independently move toward the desired enclosing geometry when apart and\nactivate the collision avoidance mechanism when a collision is imminent,\nthereby guaranteeing inter-agent safety. We rigorously show that an agent does\nnot need to ensure safety with every other agent and put forth a concept of the\nnearest colliding agent (for any arbitrary agent) with whom ensuring safety is\nsufficient to avoid collisions in the entire swarm. The proposed control\neliminates the need for a fixed or pre-established agent arrangement around the\ntarget and requires only relative information between an agent and the target.\nThis makes our design particularly appealing for scenarios with limited global\ninformation, hence significantly reducing communication requirements. We\nfinally present simulation results to vindicate the efficacy of the proposed\nmethod.",
        "updated": "2024-04-06 04:15:05 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.04497v1"
    },
    {
        "title": "ROMA-iQSS: An Objective Alignment Approach via State-Based Value Learning and ROund-Robin Multi-Agent Scheduling",
        "authors": "Chi-Hui LinJoewie J. KohAlessandro RonconeLijun Chen",
        "links": "http://arxiv.org/abs/2404.03984v1",
        "entry_id": "http://arxiv.org/abs/2404.03984v1",
        "pdf_url": "http://arxiv.org/pdf/2404.03984v1",
        "summary": "Effective multi-agent collaboration is imperative for solving complex,\ndistributed problems. In this context, two key challenges must be addressed:\nfirst, autonomously identifying optimal objectives for collective outcomes;\nsecond, aligning these objectives among agents. Traditional frameworks, often\nreliant on centralized learning, struggle with scalability and efficiency in\nlarge multi-agent systems. To overcome these issues, we introduce a\ndecentralized state-based value learning algorithm that enables agents to\nindependently discover optimal states. Furthermore, we introduce a novel\nmechanism for multi-agent interaction, wherein less proficient agents follow\nand adopt policies from more experienced ones, thereby indirectly guiding their\nlearning process. Our theoretical analysis shows that our approach leads\ndecentralized agents to an optimal collective policy. Empirical experiments\nfurther demonstrate that our method outperforms existing decentralized\nstate-based and action-based value learning strategies by effectively\nidentifying and aligning optimal objectives.",
        "updated": "2024-04-05 09:39:47 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.03984v1"
    }
]