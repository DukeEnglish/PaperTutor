[
    {
        "title": "Finding Visual Task Vectors",
        "authors": "Alberto HojelYutong BaiTrevor DarrellAmir GlobersonAmir Bar",
        "links": "http://arxiv.org/abs/2404.05729v1",
        "entry_id": "http://arxiv.org/abs/2404.05729v1",
        "pdf_url": "http://arxiv.org/pdf/2404.05729v1",
        "summary": "Visual Prompting is a technique for teaching models to perform a visual task\nvia in-context examples, without any additional training. In this work, we\nanalyze the activations of MAE-VQGAN, a recent Visual Prompting model, and find\ntask vectors, activations that encode task-specific information. Equipped with\nthis insight, we demonstrate that it is possible to identify the task vectors\nand use them to guide the network towards performing different tasks without\nproviding any input-output examples. To find task vectors, we compute the\naverage intermediate activations per task and use the REINFORCE algorithm to\nsearch for the subset of task vectors. The resulting task vectors guide the\nmodel towards performing a task better than the original model without the need\nfor input-output examples.",
        "updated": "2024-04-08 17:59:46 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.05729v1"
    },
    {
        "title": "MA-LMM: Memory-Augmented Large Multimodal Model for Long-Term Video Understanding",
        "authors": "Bo HeHengduo LiYoung Kyun JangMenglin JiaXuefei CaoAshish ShahAbhinav ShrivastavaSer-Nam Lim",
        "links": "http://arxiv.org/abs/2404.05726v1",
        "entry_id": "http://arxiv.org/abs/2404.05726v1",
        "pdf_url": "http://arxiv.org/pdf/2404.05726v1",
        "summary": "With the success of large language models (LLMs), integrating the vision\nmodel into LLMs to build vision-language foundation models has gained much more\ninterest recently. However, existing LLM-based large multimodal models (e.g.,\nVideo-LLaMA, VideoChat) can only take in a limited number of frames for short\nvideo understanding. In this study, we mainly focus on designing an efficient\nand effective model for long-term video understanding. Instead of trying to\nprocess more frames simultaneously like most existing work, we propose to\nprocess videos in an online manner and store past video information in a memory\nbank. This allows our model to reference historical video content for long-term\nanalysis without exceeding LLMs' context length constraints or GPU memory\nlimits. Our memory bank can be seamlessly integrated into current multimodal\nLLMs in an off-the-shelf manner. We conduct extensive experiments on various\nvideo understanding tasks, such as long-video understanding, video question\nanswering, and video captioning, and our model can achieve state-of-the-art\nperformances across multiple datasets. Code available at\nhttps://boheumd.github.io/MA-LMM/.",
        "updated": "2024-04-08 17:59:24 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.05726v1"
    },
    {
        "title": "Ferret-UI: Grounded Mobile UI Understanding with Multimodal LLMs",
        "authors": "Keen YouHaotian ZhangEldon SchoopFloris WeersAmanda SwearnginJeffrey NicholsYinfei YangZhe Gan",
        "links": "http://arxiv.org/abs/2404.05719v1",
        "entry_id": "http://arxiv.org/abs/2404.05719v1",
        "pdf_url": "http://arxiv.org/pdf/2404.05719v1",
        "summary": "Recent advancements in multimodal large language models (MLLMs) have been\nnoteworthy, yet, these general-domain MLLMs often fall short in their ability\nto comprehend and interact effectively with user interface (UI) screens. In\nthis paper, we present Ferret-UI, a new MLLM tailored for enhanced\nunderstanding of mobile UI screens, equipped with referring, grounding, and\nreasoning capabilities. Given that UI screens typically exhibit a more\nelongated aspect ratio and contain smaller objects of interest (e.g., icons,\ntexts) than natural images, we incorporate \"any resolution\" on top of Ferret to\nmagnify details and leverage enhanced visual features. Specifically, each\nscreen is divided into 2 sub-images based on the original aspect ratio (i.e.,\nhorizontal division for portrait screens and vertical division for landscape\nscreens). Both sub-images are encoded separately before being sent to LLMs. We\nmeticulously gather training samples from an extensive range of elementary UI\ntasks, such as icon recognition, find text, and widget listing. These samples\nare formatted for instruction-following with region annotations to facilitate\nprecise referring and grounding. To augment the model's reasoning ability, we\nfurther compile a dataset for advanced tasks, including detailed description,\nperception/interaction conversations, and function inference. After training on\nthe curated datasets, Ferret-UI exhibits outstanding comprehension of UI\nscreens and the capability to execute open-ended instructions. For model\nevaluation, we establish a comprehensive benchmark encompassing all the\naforementioned tasks. Ferret-UI excels not only beyond most open-source UI\nMLLMs, but also surpasses GPT-4V on all the elementary UI tasks.",
        "updated": "2024-04-08 17:55:44 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.05719v1"
    },
    {
        "title": "SwapAnything: Enabling Arbitrary Object Swapping in Personalized Visual Editing",
        "authors": "Jing GuYilin WangNanxuan ZhaoWei XiongQing LiuZhifei ZhangHe ZhangJianming ZhangHyunJoon JungXin Eric Wang",
        "links": "http://arxiv.org/abs/2404.05717v1",
        "entry_id": "http://arxiv.org/abs/2404.05717v1",
        "pdf_url": "http://arxiv.org/pdf/2404.05717v1",
        "summary": "Effective editing of personal content holds a pivotal role in enabling\nindividuals to express their creativity, weaving captivating narratives within\ntheir visual stories, and elevate the overall quality and impact of their\nvisual content. Therefore, in this work, we introduce SwapAnything, a novel\nframework that can swap any objects in an image with personalized concepts\ngiven by the reference, while keeping the context unchanged. Compared with\nexisting methods for personalized subject swapping, SwapAnything has three\nunique advantages: (1) precise control of arbitrary objects and parts rather\nthan the main subject, (2) more faithful preservation of context pixels, (3)\nbetter adaptation of the personalized concept to the image. First, we propose\ntargeted variable swapping to apply region control over latent feature maps and\nswap masked variables for faithful context preservation and initial semantic\nconcept swapping. Then, we introduce appearance adaptation, to seamlessly adapt\nthe semantic concept into the original image in terms of target location,\nshape, style, and content during the image generation process. Extensive\nresults on both human and automatic evaluation demonstrate significant\nimprovements of our approach over baseline methods on personalized swapping.\nFurthermore, SwapAnything shows its precise and faithful swapping abilities\nacross single object, multiple objects, partial object, and cross-domain\nswapping tasks. SwapAnything also achieves great performance on text-based\nswapping and tasks beyond swapping such as object insertion.",
        "updated": "2024-04-08 17:52:29 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.05717v1"
    },
    {
        "title": "Learning 3D-Aware GANs from Unposed Images with Template Feature Field",
        "authors": "Xinya ChenHanlei GuoYanrui BinShangzhan ZhangYuanbo YangYue WangYujun ShenYiyi Liao",
        "links": "http://arxiv.org/abs/2404.05705v1",
        "entry_id": "http://arxiv.org/abs/2404.05705v1",
        "pdf_url": "http://arxiv.org/pdf/2404.05705v1",
        "summary": "Collecting accurate camera poses of training images has been shown to well\nserve the learning of 3D-aware generative adversarial networks (GANs) yet can\nbe quite expensive in practice. This work targets learning 3D-aware GANs from\nunposed images, for which we propose to perform on-the-fly pose estimation of\ntraining images with a learned template feature field (TeFF). Concretely, in\naddition to a generative radiance field as in previous approaches, we ask the\ngenerator to also learn a field from 2D semantic features while sharing the\ndensity from the radiance field. Such a framework allows us to acquire a\ncanonical 3D feature template leveraging the dataset mean discovered by the\ngenerative model, and further efficiently estimate the pose parameters on real\ndata. Experimental results on various challenging datasets demonstrate the\nsuperiority of our approach over state-of-the-art alternatives from both the\nqualitative and the quantitative perspectives.",
        "updated": "2024-04-08 17:42:08 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.05705v1"
    }
]