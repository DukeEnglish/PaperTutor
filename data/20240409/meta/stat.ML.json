[
    {
        "title": "Flexible Fairness Learning via Inverse Conditional Permutation",
        "authors": "Yuheng LaiLeying Guan",
        "links": "http://arxiv.org/abs/2404.05678v1",
        "entry_id": "http://arxiv.org/abs/2404.05678v1",
        "pdf_url": "http://arxiv.org/pdf/2404.05678v1",
        "summary": "Equalized odds, as a popular notion of algorithmic fairness, aims to ensure\nthat sensitive variables, such as race and gender, do not unfairly influence\nthe algorithm prediction when conditioning on the true outcome. Despite rapid\nadvancements, most of the current research focuses on the violation of\nequalized odds caused by one sensitive attribute, leaving the challenge of\nsimultaneously accounting for multiple attributes under-addressed. We address\nthis gap by introducing a fairness learning approach that integrates\nadversarial learning with a novel inverse conditional permutation. This\napproach effectively and flexibly handles multiple sensitive attributes,\npotentially of mixed data types. The efficacy and flexibility of our method are\ndemonstrated through both simulation studies and empirical analysis of\nreal-world datasets.",
        "updated": "2024-04-08 16:57:44 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.05678v1"
    },
    {
        "title": "On the Convergence of Continual Learning with Adaptive Methods",
        "authors": "Seungyub HanYeongmo KimTaehyun ChoJungwoo Lee",
        "links": "http://arxiv.org/abs/2404.05555v1",
        "entry_id": "http://arxiv.org/abs/2404.05555v1",
        "pdf_url": "http://arxiv.org/pdf/2404.05555v1",
        "summary": "One of the objectives of continual learning is to prevent catastrophic\nforgetting in learning multiple tasks sequentially, and the existing solutions\nhave been driven by the conceptualization of the plasticity-stability dilemma.\nHowever, the convergence of continual learning for each sequential task is less\nstudied so far. In this paper, we provide a convergence analysis of\nmemory-based continual learning with stochastic gradient descent and empirical\nevidence that training current tasks causes the cumulative degradation of\nprevious tasks. We propose an adaptive method for nonconvex continual learning\n(NCCL), which adjusts step sizes of both previous and current tasks with the\ngradients. The proposed method can achieve the same convergence rate as the SGD\nmethod when the catastrophic forgetting term which we define in the paper is\nsuppressed at each iteration. Further, we demonstrate that the proposed\nalgorithm improves the performance of continual learning over existing methods\nfor several image classification tasks.",
        "updated": "2024-04-08 14:28:27 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.05555v1"
    },
    {
        "title": "Maximally Forward-Looking Core Inflation",
        "authors": "Philippe Goulet CoulombeKarin KlieberChristophe BarretteMaximilian Goebel",
        "links": "http://arxiv.org/abs/2404.05209v1",
        "entry_id": "http://arxiv.org/abs/2404.05209v1",
        "pdf_url": "http://arxiv.org/pdf/2404.05209v1",
        "summary": "Timely monetary policy decision-making requires timely core inflation\nmeasures. We create a new core inflation series that is explicitly designed to\nsucceed at that goal. Precisely, we introduce the Assemblage Regression, a\ngeneralized nonnegative ridge regression problem that optimizes the price\nindex's subcomponent weights such that the aggregate is maximally predictive of\nfuture headline inflation. Ordering subcomponents according to their rank in\neach period switches the algorithm to be learning supervised trimmed inflation\n- or, put differently, the maximally forward-looking summary statistic of the\nrealized price changes distribution. In an extensive out-of-sample forecasting\nexperiment for the US and the euro area, we find substantial improvements for\nsignaling medium-term inflation developments in both the pre- and post-Covid\nyears. Those coming from the supervised trimmed version are particularly\nstriking, and are attributable to a highly asymmetric trimming which contrasts\nwith conventional indicators. We also find that this metric was indicating\nfirst upward pressures on inflation as early as mid-2020 and quickly captured\nthe turning point in 2022. We also consider extensions, like assembling\ninflation from geographical regions, trimmed temporal aggregation, and building\ncore measures specialized for either upside or downside inflation risks.",
        "updated": "2024-04-08 05:39:41 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.05209v1"
    },
    {
        "title": "Convergence analysis of controlled particle systems arising in deep learning: from finite to infinite sample size",
        "authors": "Huafu LiaoAlpár R. MészárosChenchen MouChao Zhou",
        "links": "http://arxiv.org/abs/2404.05185v1",
        "entry_id": "http://arxiv.org/abs/2404.05185v1",
        "pdf_url": "http://arxiv.org/pdf/2404.05185v1",
        "summary": "This paper deals with a class of neural SDEs and studies the limiting\nbehavior of the associated sampled optimal control problems as the sample size\ngrows to infinity. The neural SDEs with N samples can be linked to the\nN-particle systems with centralized control. We analyze the\nHamilton--Jacobi--Bellman equation corresponding to the N-particle system and\nestablish regularity results which are uniform in N. The uniform regularity\nestimates are obtained by the stochastic maximum principle and the analysis of\na backward stochastic Riccati equation. Using these uniform regularity results,\nwe show the convergence of the minima of objective functionals and optimal\nparameters of the neural SDEs as the sample size N tends to infinity. The\nlimiting objects can be identified with suitable functions defined on the\nWasserstein space of Borel probability measures. Furthermore, quantitative\nalgebraic convergence rates are also obtained.",
        "updated": "2024-04-08 04:22:55 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.05185v1"
    },
    {
        "title": "On the price of exact truthfulness in incentive-compatible online learning with bandit feedback: A regret lower bound for WSU-UX",
        "authors": "Ali MortazaviJunhao LinNishant A. Mehta",
        "links": "http://arxiv.org/abs/2404.05155v1",
        "entry_id": "http://arxiv.org/abs/2404.05155v1",
        "pdf_url": "http://arxiv.org/pdf/2404.05155v1",
        "summary": "In one view of the classical game of prediction with expert advice with\nbinary outcomes, in each round, each expert maintains an adversarially chosen\nbelief and honestly reports this belief. We consider a recently introduced,\nstrategic variant of this problem with selfish (reputation-seeking) experts,\nwhere each expert strategically reports in order to maximize their expected\nfuture reputation based on their belief. In this work, our goal is to design an\nalgorithm for the selfish experts problem that is incentive-compatible (IC, or\n\\emph{truthful}), meaning each expert's best strategy is to report truthfully,\nwhile also ensuring the algorithm enjoys sublinear regret with respect to the\nexpert with the best belief. Freeman et al. (2020) recently studied this\nproblem in the full information and bandit settings and obtained truthful,\nno-regret algorithms by leveraging prior work on wagering mechanisms. While\ntheir results under full information match the minimax rate for the classical\n(\"honest experts\") problem, the best-known regret for their bandit algorithm\nWSU-UX is $O(T^{2/3})$, which does not match the minimax rate for the classical\n(\"honest bandits\") setting. It was unclear whether the higher regret was an\nartifact of their analysis or a limitation of WSU-UX. We show, via explicit\nconstruction of loss sequences, that the algorithm suffers a worst-case\n$\\Omega(T^{2/3})$ lower bound. Left open is the possibility that a different IC\nalgorithm obtains $O(\\sqrt{T})$ regret. Yet, WSU-UX was a natural choice for\nsuch an algorithm owing to the limited design room for IC algorithms in this\nsetting.",
        "updated": "2024-04-08 02:41:32 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.05155v1"
    }
]