[
    {
        "title": "Language-Independent Representations Improve Zero-Shot Summarization",
        "authors": "Vladimir SolovyevDanni LiuJan Niehues",
        "links": "http://arxiv.org/abs/2404.05720v1",
        "entry_id": "http://arxiv.org/abs/2404.05720v1",
        "pdf_url": "http://arxiv.org/pdf/2404.05720v1",
        "summary": "Finetuning pretrained models on downstream generation tasks often leads to\ncatastrophic forgetting in zero-shot conditions. In this work, we focus on\nsummarization and tackle the problem through the lens of language-independent\nrepresentations. After training on monolingual summarization, we perform\nzero-shot transfer to new languages or language pairs. We first show naively\nfinetuned models are highly language-specific in both output behavior and\ninternal representations, resulting in poor zero-shot performance. Next, we\npropose query-key (QK) finetuning to decouple task-specific knowledge from the\npretrained language generation abilities. Then, after showing downsides of the\nstandard adversarial language classifier, we propose a balanced variant that\nmore directly enforces language-agnostic representations. Moreover, our\nqualitative analyses show removing source language identity correlates to\nzero-shot summarization performance. Our code is openly available.",
        "updated": "2024-04-08 17:56:43 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.05720v1"
    },
    {
        "title": "Ferret-UI: Grounded Mobile UI Understanding with Multimodal LLMs",
        "authors": "Keen YouHaotian ZhangEldon SchoopFloris WeersAmanda SwearnginJeffrey NicholsYinfei YangZhe Gan",
        "links": "http://arxiv.org/abs/2404.05719v1",
        "entry_id": "http://arxiv.org/abs/2404.05719v1",
        "pdf_url": "http://arxiv.org/pdf/2404.05719v1",
        "summary": "Recent advancements in multimodal large language models (MLLMs) have been\nnoteworthy, yet, these general-domain MLLMs often fall short in their ability\nto comprehend and interact effectively with user interface (UI) screens. In\nthis paper, we present Ferret-UI, a new MLLM tailored for enhanced\nunderstanding of mobile UI screens, equipped with referring, grounding, and\nreasoning capabilities. Given that UI screens typically exhibit a more\nelongated aspect ratio and contain smaller objects of interest (e.g., icons,\ntexts) than natural images, we incorporate \"any resolution\" on top of Ferret to\nmagnify details and leverage enhanced visual features. Specifically, each\nscreen is divided into 2 sub-images based on the original aspect ratio (i.e.,\nhorizontal division for portrait screens and vertical division for landscape\nscreens). Both sub-images are encoded separately before being sent to LLMs. We\nmeticulously gather training samples from an extensive range of elementary UI\ntasks, such as icon recognition, find text, and widget listing. These samples\nare formatted for instruction-following with region annotations to facilitate\nprecise referring and grounding. To augment the model's reasoning ability, we\nfurther compile a dataset for advanced tasks, including detailed description,\nperception/interaction conversations, and function inference. After training on\nthe curated datasets, Ferret-UI exhibits outstanding comprehension of UI\nscreens and the capability to execute open-ended instructions. For model\nevaluation, we establish a comprehensive benchmark encompassing all the\naforementioned tasks. Ferret-UI excels not only beyond most open-source UI\nMLLMs, but also surpasses GPT-4V on all the elementary UI tasks.",
        "updated": "2024-04-08 17:55:44 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.05719v1"
    },
    {
        "title": "Comprehensive Study on German Language Models for Clinical and Biomedical Text Understanding",
        "authors": "Ahmad Idrissi-YaghirAmin DadaHenning SchäferKamyar ArzidehGiulia BaldiniJan TrienesMax HasinJeanette BewersdorffCynthia S. SchmidtMarie BauerKaleb E. SmithJiang BianYonghui WuJörg SchlöttererTorsten ZeschPeter A. HornChristin SeifertFelix NensaJens KleesiekChristoph M. Friedrich",
        "links": "http://arxiv.org/abs/2404.05694v1",
        "entry_id": "http://arxiv.org/abs/2404.05694v1",
        "pdf_url": "http://arxiv.org/pdf/2404.05694v1",
        "summary": "Recent advances in natural language processing (NLP) can be largely\nattributed to the advent of pre-trained language models such as BERT and\nRoBERTa. While these models demonstrate remarkable performance on general\ndatasets, they can struggle in specialized domains such as medicine, where\nunique domain-specific terminologies, domain-specific abbreviations, and\nvarying document structures are common. This paper explores strategies for\nadapting these models to domain-specific requirements, primarily through\ncontinuous pre-training on domain-specific data. We pre-trained several German\nmedical language models on 2.4B tokens derived from translated public English\nmedical data and 3B tokens of German clinical data. The resulting models were\nevaluated on various German downstream tasks, including named entity\nrecognition (NER), multi-label classification, and extractive question\nanswering. Our results suggest that models augmented by clinical and\ntranslation-based pre-training typically outperform general domain models in\nmedical contexts. We conclude that continuous pre-training has demonstrated the\nability to match or even exceed the performance of clinical models trained from\nscratch. Furthermore, pre-training on clinical data or leveraging translated\ntexts have proven to be reliable methods for domain adaptation in medical NLP\ntasks.",
        "updated": "2024-04-08 17:24:04 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.05694v1"
    },
    {
        "title": "Evaluating Mathematical Reasoning Beyond Accuracy",
        "authors": "Shijie XiaXuefeng LiYixin LiuTongshuang WuPengfei Liu",
        "links": "http://arxiv.org/abs/2404.05692v1",
        "entry_id": "http://arxiv.org/abs/2404.05692v1",
        "pdf_url": "http://arxiv.org/pdf/2404.05692v1",
        "summary": "The leaderboard of Large Language Models (LLMs) in mathematical tasks has\nbeen continuously updated. However, the majority of evaluations focus solely on\nthe final results, neglecting the quality of the intermediate steps. This\noversight can mask underlying problems, such as logical errors or unnecessary\nsteps in the reasoning process. To measure reasoning beyond final-answer\naccuracy, we introduce ReasonEval, a new methodology for evaluating the quality\nof reasoning steps. ReasonEval employs $\\textit{validity}$ and\n$\\textit{redundancy}$ to characterize the reasoning quality, as well as\naccompanying LLMs to assess them automatically. Instantiated by base models\nthat possess strong mathematical knowledge and trained with high-quality\nlabeled data, ReasonEval achieves state-of-the-art performance on human-labeled\ndatasets and can accurately detect different types of errors generated by\nperturbation. When applied to evaluate LLMs specialized in math, we find that\nan increase in final-answer accuracy does not necessarily guarantee an\nimprovement in the overall quality of the reasoning steps for challenging\nmathematical problems. Additionally, we observe that ReasonEval can play a\nsignificant role in data selection. We release the best-performing model,\nmeta-evaluation script, and all evaluation results at\nhttps://github.com/GAIR-NLP/ReasonEval.",
        "updated": "2024-04-08 17:18:04 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.05692v1"
    },
    {
        "title": "VietMed: A Dataset and Benchmark for Automatic Speech Recognition of Vietnamese in the Medical Domain",
        "authors": "Khai Le-Duc",
        "links": "http://arxiv.org/abs/2404.05659v1",
        "entry_id": "http://arxiv.org/abs/2404.05659v1",
        "pdf_url": "http://arxiv.org/pdf/2404.05659v1",
        "summary": "Due to privacy restrictions, there's a shortage of publicly available speech\nrecognition datasets in the medical domain. In this work, we present VietMed -\na Vietnamese speech recognition dataset in the medical domain comprising 16h of\nlabeled medical speech, 1000h of unlabeled medical speech and 1200h of\nunlabeled general-domain speech. To our best knowledge, VietMed is by far the\nworld's largest public medical speech recognition dataset in 7 aspects: total\nduration, number of speakers, diseases, recording conditions, speaker roles,\nunique medical terms and accents. VietMed is also by far the largest public\nVietnamese speech dataset in terms of total duration. Additionally, we are the\nfirst to present a medical ASR dataset covering all ICD-10 disease groups and\nall accents within a country. Moreover, we release the first public large-scale\npre-trained models for Vietnamese ASR, w2v2-Viet and XLSR-53-Viet, along with\nthe first public large-scale fine-tuned models for medical ASR. Even without\nany medical data in unsupervised pre-training, our best pre-trained model\nXLSR-53-Viet generalizes very well to the medical domain by outperforming\nstate-of-the-art XLSR-53, from 51.8% to 29.6% WER on test set (a relative\nreduction of more than 40%). All code, data and models are made publicly\navailable here: https://github.com/leduckhai/MultiMed.",
        "updated": "2024-04-08 16:43:52 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.05659v1"
    }
]