MA-LMM: Memory-Augmented Large Multimodal Model
for Long-Term Video Understanding
BoHe1,2* HengduoLi2 YoungKyunJang2 MenglinJia2 XuefeiCao2
AshishShah2 AbhinavShrivastava1 Ser-NamLim3
1UniversityofMaryland,CollegePark 2Meta 3UniversityofCentralFlorida
https://boheumd.github.io/MA-LMM/
Abstract
Text
Withthesuccessoflargelanguagemodels(LLMs),inte- Visual Querying
LLM
gratingthevisionmodelintoLLMstobuildvision-language Encoder Transformer
foundationmodelshasgainedmuchmoreinterestrecently.
However,existingLLM-basedlargemultimodalmodels(e.g.,
Video-LLaMA,VideoChat)canonlytakeinalimitednumber What did the man
hold at the start of
offramesforshortvideounderstanding. Inthisstudy,we the video?
mainlyfocusondesigninganefficientandeffectivemodel Long-Term Memory Bank
forlong-termvideounderstanding. Insteadoftryingtopro-
cessmoreframessimultaneouslylikemostexistingwork,we 45
propose to process videos in an online manner and store InstructBLIP 40
pastvideoinformationinamemorybank. Thisallowsour VideoChat-Embed
35 Video-LLaMA model to reference historical video content for long-term
Ours
analysiswithoutexceedingLLMs’contextlengthconstraints 30
or GPU memory limits. Our memory bank can be seam- 25
lesslyintegratedintocurrentmultimodalLLMsinanoff-the-
20
shelfmanner. Weconductextensiveexperimentsonvarious
15
videounderstandingtasks,suchaslong-videounderstand- 100 101 102 103
ing, videoquestionanswering, andvideocaptioning, and Number of Frames
Figure 1. (a) We propose the long-term memory bank to auto-
ourmodelcanachievestate-of-the-artperformancesacross
regressivelystoreandaccumulatepastvideoinformation,different
multipledatasets.
frompreviousmethodsdirectlyfeedingthevisualencoder’soutputs
intothequeryingtransformer.(b)GPUmemoryconsumptionv.s.
videoframelengthofexistingmultimodalmethodsandMA-LMM
1.Introduction duringinference.Circlesizesrepresentthenumberoftexttokens.
Tohandlevideoinputs,somepriorlargemultimodalmod-
Largelanguagemodels(LLMs)havegainedsignificantpop-
els[7,9]directlyfeedtheconcatenatedqueryembeddings
ularity in the natural language processing field. By pre-
ofeachframealongthetemporalaxisintoLLMs. However,
trainingonlarge-scaledtextualdata,LLMs(e.g.GPT[1–4],
the inherent context length limitation of LLMs and GPU
LLaMA[5,6])havedemonstratedremarkableabilitiesto
memoryconsumptionrestrictthenumberofvideoframes
performbothgenerativeanddiscriminativetaskswithauni-
thatcanbeprocessed. Forexample,LLaMAhasacontext
fiedframework. Recently,therehasbeenagrowinginterest
lengthlimitationof2048whilelargemultimodalmodelslike
inutilizingLLMsonmultimodaltasks.ByintegratingLLMs
LLaVA[8]andBLIP-2[7,9]takein256and32tokensper
with visual encoders, they can take images and videos as
imagerespectively. Therefore, thisdesignisnotpractical
inputandshowincrediblecapabilitiesinvariousvisualun-
andfeasiblewhenvideodurationismuchlonger(e.g.movies
derstandingtasks,suchascaptioning,questionanswering[7–
andTVshows). Toaddresstheseissues,anaivesolutionis
13],classification,detection,andsegmentation[14–20].
toapplyaveragepoolingalongthetemporalaxislikeVideo-
*WorkdoneduringBo’sinternshipatMeta. ChatGPT[21],butthisleadstoinferiorperformancesasit
1
4202
rpA
8
]VC.sc[
1v62750.4042:viXra )BG(
yromeM
UPGlacks explicit temporal modeling. An alternative method cludinglong-termvideounderstanding,videoquestion
involves adding a video modeling component to capture answering,andvideocaptioning.
temporaldynamics,asseeninVideo-LLaMA[12],which
employsanextravideoqueryingtransformer(Q-Former)to 2.RelatedWork
obtainvideo-levelrepresentation. However,thisdesignadds
modelcomplexities,increasesthetrainingparameters,and Image-languagemodels. Inspiredbythesuccessofpow-
isnotsuitableforonlinevideoanalysis. erfullanguagemodels[1–6],recentimage-languagemod-
Withtheseinmind,weintroduceaMemory-Augmented els tend to incorporate pre-trained language models with
Large Multimodal Model (MA-LMM), aiming for effi- image encoders to support the multimodal reasoning abil-
ity [7–10, 22]. Flamingo [22] proposes to connectpower-
cientandeffectivelong-termvideomodeling. MA-LMM
fulpre-trainedvision-onlyandlanguage-onlymodelsand
adoptsastructuresimilartoexistinglargemultimodalmod-
achieve state-of-the-art performance in few-shot learning
els [7, 9, 12], which comprise a visual encoder to extract
tasks. BLIP-2[7]introducesalightweightqueryingtrans-
visualfeatures,aqueryingtransformertoalignthevisualand
formertobridgethemodalitygapbetweenthefrozenpre-
textembeddingspaces,andalargelanguagemodel. Asillus-
trained image encoder and frozen LLMs. Despite having
tratedinFigure1(a),asopposedtodirectlyfeedingvisual
significantly fewer trainable parameters, it performs well
encoderoutputstothequeryingtransformer,weoptforan
onvariousmultimodaltasks. LLaVA[8]employsasimple
onlineprocessingapproachthattakesvideoframessequen-
linearlayertoprojectimagefeaturesintothetextembedding
tiallyandstoresthevideofeaturesintheproposedlong-term
space and efficiently finetunes LLMs [23] for better per-
memorybank.Thisstrategyofsequentiallyprocessingvideo
formance. BuildinguponBLIP-2,MiniGPT-4[10]collects
framesandleveragingamemorybanksignificantlyreduces
a large-scale high-quality dataset of image-text pairs and
theGPUmemoryfootprintforlongvideosequences. Italso
achievesbetterlanguagegenerationability. VisionLLM[15]
effectively addresses the constraints posed by the limited
leveragesthereasoningandparsingcapacitiesofLLMs,pro-
contextlengthinLLMsasdemonstratedinFigure1(b). Our
ducingstrongperformanceonmultiplefine-grainedobject-
designprovidesasolutionforlong-termvideounderstanding
levelandcoarse-grainedreasoningtasks.
with large multimodal models with great advantages over
priorapproaches[7,9,12,13,21]whichconsumehugeGPU Video-languagemodels. Currentimage-languagemodels
memoryandrequirealargenumberofinputtexttokens. suchasFlamingo[22]andBLIP-2[7,9]canalsosupport
Thecorecontributionofourapproachistheintroduction videoinputs. Theysimplyflattenedthespatio-temporalfea-
ofalong-termmemorybankthatcapturesandaggregates turesinto1Dsequencesandthenfedthemintothelanguage
historicalvideoinformation. Specifically,thememorybank modelsforvideoinputs. However,theseapproachescannot
aggregatespastvideofeaturesinanauto-regressivemanner, effectivelycapturethetemporaldynamicsofvideos. Recent
whichcanbereferencedduringsubsequentvideosequence video-languagemodels[11–13]attempttoaddressthisbyin-
processing. Also,ourmemorybankisdesignedtobecom- corporatingmechanismstobetterunderstandthesedynamics.
patiblewiththeQ-Former,whereitactsasthekeyandvalue Forinstance,Video-LLaMA[12]enhancesBLIP-2structure
intheattentionoperationforlong-termtemporalmodeling. byaddinganadditionalvideoqueryingtransformertoexplic-
As a result, it can be seamlessly integrated into existing itlymodelthetemporalrelationship. Similarly,buildingon
largemultimodalmodelsinanoff-the-shelfmannertoen- LLaVA[8],Video-ChatGPT[21]simplyaveragepoolsthe
ablelong-termvideomodelingability. Tofurtherenhance frame-levelfeaturesacrossspatialandtemporaldimensions
efficiency,weproposeamemorybankcompressionmethod togeneratevideo-levelrepresentation. ChatVideo[24]treats
thatmaintainsthelengthofthememorybankconstantrel- trackletsasthebasicvideoelementsandreliesonseveral
ativetotheinputvideolength. Byselectingandaveraging video foundation models to annotate these tracklets with
themostsimilaradjacentframefeatures,itcanpreserveall textualdescriptions. Then,itusesChatGPT[4]toachieve
the temporal information while significantly reducing the variousvideounderstandingtasks. VideoChat[13]utilizes
temporalredundanciesinlongvideos. perceptionmodelstogenerateactionandobjectannotations,
Wesummarizeourmaincontributionsasfollows: which are then forwarded to LLMs for further reasoning.
Despitetheseadvancements,thesemodelsareprimarilyde-
• Weintroduceanovellong-termmemorybankdesignto
signed for short videos, as longer videos pose significant
enhanceexistinglargemultimodalmodels,equipping
challengesduetolanguagemodelcontextlengthandGPU
themwithlong-termvideomodelingcapability.
memoryrestrictions.
• OurmodelsignificantlyreducestheGPUmemoryus-
ageandaddressesLLMs’contextlengthlimitationsby Long-term video models. Long-term video understand-
processingvideosequencesinanonlinefashion. ingmethodsfocusoncapturinglong-rangepatternsinlong
• Our approach has achieved new state-of-the-art per- videos, which typically exceed 30 seconds. To mitigate
formancesonvariousdownstreamingvideotasks,in- the computational demands of processing long videos, a
2Text
LLM
Step 1: Calculate the cosine
Prompt FC similarities of adjacent frames
Visual Visual
Encoder Encoder
0.3 0.5 0.9 0.1
Cross-Attention
Step 2: Select top-1 similarity and
average selected features
Visual Memory Bank
Self-Attention
Query Memory Bank
Learned Queries
Long-Term Memory Bank Q-Former
(a) Framework Overview (b) Memory Bank Compression
Figure2.(a)Frameworkoverview.MA-LMMauto-regressivelyprocessesvideoframesinanonlinemanner.Twolong-termmemorybanks
aredesignedtostoretherawvisualfeaturesandlearnedqueriesateachtimestep,whichareusedforfuturereference. TheQ-Formeris
composedofseveralcascadedblocks,indexedbyl.LLMoutputstextforvariousvideounderstandingdownstreamtasks.Thesnowflake
iconindicatescomponentswithfixedparameters,whiletheflameicondenotespartsofthemodelthatarefine-tuned.(b)Illustrationofthe
memorybankcompressiontechnique,whichisappliedtomaintainthelengthofthememorybankconstant.
prevalent approach involves using pre-extracted features, cognitiveprocesseshumansusetohandlelong-termvisual
sidesteppingtheneedforjointtrainingofbackbonearchi- information. Insteadofconcurrentlyprocessingextensive
tectures[25–29]. Alternatively,someresearchworksaimto duration of signals, humans process them in a sequential
devisesparsevideosamplingmethods[30,31],reducingthe manner,correlatecurrentvisualinputswithpastmemories
numberofinputframesbyonlypreservingsalientvideocon- forcomprehension,andselectivelyretainsalientinformation
tent. OtherworkslikeVis4mer[32]andS5[33]leveragethe forsubsequentreference[38]. Similarly,ourMA-LMMpro-
streamlinedtransformerdecoderstructureofS4[34]toen- cessesvideoframessequentially,dynamicallyassociating
ablelong-rangetemporalmodelingwithlinearcomputation newframeinputwithhistoricaldatastoredinthelong-term
complexity. Inspiredbythememorybankdesign[35–38], memorybank,ensuringthatonlydiscriminativeinformation
we propose to integrate the long-term memory bank with isconservedforlateruse. Thisselectiveretentionfacilitates
large multimodal models to enable efficient and effective a more sustainable and efficient approach to video under-
long-termtemporalmodelingcapabilities. standing,whichfurtherallowsthemodeltoautomatically
supportonlinevideoreasoningtasks.
3.Method
Formally,givenasequenceofT videoframes,wepass
WeintroduceMA-LMM,amemory-augmentedlargemulti- eachvideoframeintoapre-trainedvisualencoderandobtain
modalmodelforlong-termvideounderstanding. Figure2(a) thevisualfeaturesV = [v ,v ,..,v ],v ∈ RP×C,where
1 2 T t
illustratestheoverviewofourMA-LMMframework. Fol- P is the number of patches for each frame and C is the
lowing similar practices of large multimodal models [7– channel dimension for the extracted frame feature. Then
9, 12], the overall model architecture can be divided into weinjecttemporalorderinginformationintotheframe-level
threeparts: (1)visualfeatureextractionwithafrozenvisual featuresbyapositionembeddinglayer(PE)as
encoder(Sec.3.1),(2)long-termtemporalmodelingwith
a trainable querying transformer (Q-Former) to align the f t =v t+PE(t),f t ∈RP×C. (1)
visual and text embedding spaces (Sec. 3.2), and (3) text
3.2.Long-termTemporalModeling
decodingwithafrozenlargelanguagemodel(Sec.3.3).
For aligning the visual embedding to the text embedding
3.1.VisualFeatureExtraction
space,weusethesamearchitectureastheQueryingTrans-
We propose to auto-regressively process video frames in former(Q-Former)inBLIP-2[7,9]. Q-Formertakesinthe
anonlinemanner. Thisdesigndrawsinspirationfromthe learnedqueriesz ∈RN×C tocapturevideotemporalinfor-
3mation,whereN isthenumberoflearnedqueries,andC is MemoryBankCompression.Giventhatourmodeldirectly
thechanneldimension. Inourexperiments,Q-Formerout- storespastvideoinformationinthememorybanks,theGPU
puts32tokensforeachimage,whichismoreefficientthan memorycostandcomputationalcomplexityincreaselinearly
256tokensproducedbyLLaVA[8]. EachQ-Formerblock asthenumberofpastvideoframes. Thisbecomespartic-
consists of two attention submodules: (1) cross-attention ularlychallengingforlongvideos, andthusitisessential
layer, which interacts with the raw visual embedding ex- tofurthercompressthememorybanktoasmallersize. A
tractedfromthefrozenvisualencoder,and(2)self-attention straightforwardsolutionistouseafirst-in-first-outqueueto
layer, which models interactions within the input queries. managethetemporalsequence. Inthisscenario,thefeatures
DifferentfromtheoriginalQ-FormerinBLIP-2thatonly of the earliest timestep are removed if the current length
attendstothecurrentframe’sembedding,wedesignalong- ofthememorybankexceedsapre-definedlimit. However,
term memory bank consisting of the visual memory bank thisapproachinevitablyleadstothelossofearlierhistorical
and the query memory bank, which accumulates the past informationasnewframesareaddedandoldfeaturesare
videoinformationandaugmentstheinputtocross-andself- poppedwhenthememorybankisfull. Toaddressthisis-
attentionlayersforeffectivelong-termvideounderstanding. sue,wetakeadvantageofthetemporalredundancyinlong
videos and introduce a novel memory bank compression
VisualMemoryBank. Thevisualmemorybankstoresthe
(MBC)technique. Thismethodaggregatesandcompresses
rawvisualfeaturesofeachframeextractedfromthefrozen
videoinformationovertimebasedonthesimilarityofadja-
visualencoder. Specifically,forthecurrenttimestept,the
centfeatures,whilepreservingearlyhistoricalinformation.
visualmemorybankcontainstheconcatenatedlistofpast
visual features F = Concat[f ,f ,..,f ],F ∈ RtP×C. Inthismanner,repetitiveinformationisconsolidatedinthe
t 1 2 t t
memorybank,whilediscriminativefeaturesremainintact.
Giventheinputqueryz ,thevisualmemorybankactsasthe
t
We apply the compression algorithm at each auto-
keyandvalueas:
regressive iteration if the current memory bank length is
Q=z tW Q, K =F tW K, V =F tW V. (2) largerthanthepre-definedthresholdM. Formally,giventhe
visualmemorybankcontainingalistof[f ,f ,..,f ],f ∈
Thenweapplythecross-attentionoperationas: 1 2 M t
RP×C,whenanewframefeaturef comesin,weneed
M+1
(cid:18) QKT(cid:19)
to compress the memory bank by reducing the length by
O =Attn(Q,K,V)=Softmax √ V. (3)
C 1. At each spatial location i, we first calculate the cosine
similaritybetweenallthetemporallyadjacenttokensas
Inthisway,itcanexplicitlyattendtopastvisualinforma-
tionthroughthecachedvisualmemorybankwithlong-term si =cos(fi,fi ),t∈[1,M],i∈[1,P]. (5)
t t t+1
context. Sinceallthecross-attentionlayersintheQ-Former
attend to the same visual feature, there is only one visual Thenweselectthehighestsimilarityacrosstime,whichcan
memorybankthatissharedacrossalltheQ-Formerblocks. beinterpretedasthemosttemporallyredundantfeatures:
QueryMemoryBank. Differentfromthefixedvisualmem-
k =argmax (si). (6)
ory bank which stores the raw and static visual features, t t
thequerymemorybankaccumulatesinputqueriesofeach
Next,wesimplyaveragetheselectedtokenfeaturesatall
timestep,representedasZ = Concat[z ,z ,..,z ],Z ∈
t 1 2 t t thespatiallocationstoreducethememorybanklengthby1:
RtN×C. Bystoringthesequeries,wemaintainadynamic
memory of the model’s understanding and processing of fˆi =(fi +fi )/2. (7)
eachframeuptothecurrenttimestepviatheQ-Former. The k k k+1
querymemorybankalsoactsaskeyandvalueas:
In this way, we can still preserve the most discriminative
featureswhilekeepingthetemporalorderingunchangedas
Q=z W , K =Z W , V =Z W . (4)
t Q t K t V depictedinFigure2(b). Thesameprocedureisadoptedto
compressthequerymemorybank.
similartotheEq2. Thenweapplythesameattentionop-
erationasEq.3. Ateachtimestep,z containsthelearned
t 3.3.TextDecoding
important information specifically for each video till the
currenttimestept. Differentfromthestaticvisualmemory Asweprocessvideoframesinanauto-regressivemanner,
bank,theinputqueriesz evolvethroughcascadedQ-Former theQ-Formeroutputatthefinaltimestepcontainsallhistori-
t
blocksduringthemodeltraining,capturingdistinctvideo calinformation,whichisthenfedintotheLLM.Therefore,
conceptsandpatternsatincreasinglevelsofabstraction. As wecansignificantlyreducethenumberofinputtexttokens
aresult,eachself-attentionlayerhasauniquequerymemory fromN ∗T toN,addressingthecontextlengthlimitation
bank,wherethecontainedinputqueriesareupdatedduring ofthecurrentLLMsandsubstantiallyeasingtheGPUmem-
thetrainingtime. oryrequirements. Duringtraining,givenalabeleddataset
4consistingofvideoandtextpairs,ourmodelissupervised 4.2.ImplementationDetails
withthestandardcrossentropylossas:
Forthevisualencoder,weadoptthepre-trainedimageen-
S coderViT-G/14[57]fromEVA-CLIP[58],itcanbefurther
1 (cid:88)
L=− logP(w |w ,V). (8) changed to other clip-based video encoders. We use the
S i <i
i=1 pre-trained Q-Former weights from InstructBLIP [9] and
adoptVicuna-7B[59]astheLLM.Alltheexperimentsare
in which V represents the input video, and w is the i-th
i conductedon4A100GPUs. Moredetailsabouttrainingand
ground-truth text token. During training, we update the
evaluationaredescribedinthesupplementarymaterial.
parametersoftheQ-Formerwhilekeepingtheweightsof
boththevisualencoderandthelanguagemodelfrozen. 4.3.MainResults
4.Experiments Long-termVideoUnderstanding. WecompareMA-LMM
withpreviousstate-of-the-art(SOTA)methodsontheLVU
4.1.TasksandDatasets
benchmark[29]inTable1. Notably,MA-LMMoutperforms
existing long-term video models (S5 [33], ViS4mer [32],
TovalidatetheeffectivenessoftheproposedMA-LMM,we
VideoBERT[41],andObjectTransformer[29])inbothcon-
mainlyfocusonthelong-termvideounderstandingtask. We
tentunderstandingandmetadatapredictiontasks. Thisre-
alsoextendtheevaluationtostandardvideounderstanding
sults in significantimprovement inmost tasks, enhancing
tasks(e.g.,videoquestionanswering,videocaptioning)to
theaveragetop-1accuracyby3.8%comparedtotheS5[33]
furthercomparewithexistingmultimodalmethods.
model. Unlikepreviousvideo-basedmodelswhichprocess
Long-termVideoUnderstanding.Weconductexperiments
all video frames simultaneously in an offline manner and
on three widely used long-term video datasets including
predictprobabilitiesforeachclass,ourMA-LMMprocesses
LVU[29],Breakfast[42],andCOIN[43].Wereportthetop-
videoframesinanonlinefashionanddirectlyoutputsthe
1classificationaccuracyastheevaluationmetric. TheLVU
textlabelforeachclasstype.
datasetcontains∼30Kvideosextractedfrom∼3Kmovies,
We also evaluate our MA-LMM on the Breakfast [42]
witheachvideolasting1to3minutes. Giventhatcurrent
andCOIN[43]datasetsthatposeachallengeforthelong-
largemultimodalmodelsgenerallyperformtextgeneration
termvideoactivityclassificationtask. Weshowtheresults
and lack regression capability, we limit our experiments
in Table 2. Our method improves upon the previous best
to seven classification tasks: relationship, speaking style,
method,S5[33],by2.3%and2.4%respectivelyonthetop-
scene,director,genre,writer,andreleaseyear. TheBreak-
1 accuracy metric. This result further proves the superior
fast[42]datasetincludesvideosrelatedtobreakfastprepara-
long-termvideounderstandingcapabilityofourapproach.
tion,whichconsistsof1712videoswithanaveragelength
ofaround2.7minutes. COIN[43]isalarge-scaledataset VideoQuestionAnswering. Tocomparewithexistingmul-
forcomprehensiveinstructionalvideoanalysis,whichcom- timodalvideounderstandingmethods,weconductexperi-
prises11827instructionalvideosfromYouTube,covering mentsontheopen-endedvideoquestionansweringdatasets
180distincttasksin12domainsrelatedtodailylife. The inTable3todemonstratethegeneralizationabilityofour
averagelengthofavideois2.36minutes. model. Given that these are mostly short videos, it is ex-
pectedthatourmemorybankwillbelesseffective. Interest-
Video Question Answering. We conduct evaluation on
ingly,weobservethatourMA-LMMachievesnewstate-of-
threeopen-endedvideoquestionansweringdatasetsinclud-
the-artperformancesontheMSRVTTandMSVDdatasets
ingMSRVTT-QA[48],MSVD-QA[48],andActivityNet-
whilefallingshortofVideoCoCa’sperformanceontheAc-
QA[49]. ActivityNet-QAcontainslongvideoswithaverage
tivityNet dataset. On the latter, it is not surprising, since
durationsof2minutes,whileMSRVTT-QAandMSVD-QA
VideoCoCa[67]leverageslarge-scalevideo-textdatasetsfor
consistofshortvideoswith10-15secondsduration.
pre-training(e.g.,HowTo100M[70]andVideoCC3M[71])
VideoCaptioning. Wereportthevideocaptioningresults whileourMA-LMMusesmodelweightsonlypre-trained
ofMETEOR[50]andCIDEr[51]metricsonthreepopular ontheimage-textdatasets.
datasets: MSRVTT[52],MSVD[53]andYoucook2[54].
Notably, our MA-LMM significantly outperforms the
OnlineActionPrediction. Wefurtherevaluatetheonline recentLLM-basedmodelVideo-LLaMA[12]onallthree
prediction capability of our model by conducting experi- datasets. Video-LLaMAconcatenatesallthequeryembed-
mentsontheEpicKitchens-100[55]dataset,whichconsists dings from the frozen image Q-Former and trains an ad-
of700longvideosofcookingactivitieswith100totalhours. ditional video Q-Former from scratch to model temporal
Itincludes97verbs,300nouns,and3807actiontypes. Fol- dependencies, consuming too much GPU memory to be
lowingthesameexperimentalsettingin [56],wereportthe feasibleforlongvideoinputs. Incontrast,ourMA-LMM
top-5accuracyandrecallresultsonthevalidationdataset. simplyfine-tunestheweightsfromthepre-trainedimageQ-
5Table1.Comparisonwithstate-of-the-artmethodsontheLVU[29]dataset.Boldandunderline Table2.ComparisonontheBreakfast[42]
representthetop-1andtop-2results. andCOIN[43]datasets. Thetop-1accu-
racyisreportedhere.
Content Metadata
Model Avg
Model Breakfast COIN
Relation Speak Scene Director Genre Writer Year
TSN[44] - 73.4
Obj_T4mer[29] 54.8 33.2 52.9 47.7 52.7 36.3 37.8 45.0
VideoGraph[45] 69.5 -
Performer[39] 50.0 38.8 60.5 58.9 49.5 48.2 41.3 49.6
Timeception[28] 71.3 -
Orthoformer[40] 50.0 38.3 66.3 55.1 55.8 47.0 43.4 50.8
VideoBERT[41] 52.8 37.9 54.9 47.3 51.9 38.5 36.1 45.6 GHRM[46] 75.5 -
LST[32] 52.5 37.3 62.8 56.1 52.7 42.3 39.2 49.0 D-Sprv.[47] 89.9 90.0
VIS4mer[32] 57.1 40.8 67.4 62.6 54.7 48.8 44.8 53.7 ViS4mer[32] 88.2 88.4
S5[33] 67.1 42.1 73.5 67.3 65.4 51.3 48.0 59.2 S5[33] 90.7 90.8
Ours 58.2 44.8 80.3 74.6 61.0 70.4 51.9 63.0 Ours 93.0 93.2
Table3.Comparisonwithstate-of-the-artmethodsonthevideo Table4.Comparisonwithstate-of-the-artmethodsonthevideocaption-
questionansweringtask.Top-1accuracyisreported. ingtask.METEOR(M)andCIDEr(C)resultsarereported.
Model MSRVTT MSVD ActivityNet MSRVTT MSVD YouCook2
Model
JustAsk[60] 41.8 47.5 38.9
M C M C M C
FrozenBiLM[61] 47.0 54.8 43.2
SINGULARITY[62] 43.5 – 44.1 UniVL[68] 28.2 49.9 29.3 52.8 – 127.0
VIOLETv2[63] 44.5 54.7 – SwinBERT[69] 29.9 53.8 41.3 120.6 15.6 109.0
GiT[64] 43.2 56.8 – GIT[64] 32.9 73.9 51.1 180.2 17.3 129.8
mPLUG-2[65] 48.0 58.1 –
mPLUG-2[65] 34.9 80.3 48.4 165.8 – –
UMT-L[66] 47.1 55.2 47.9
VideoCoca[67] – 73.2 – – – 128.0
VideoCoCa[67] 46.3 56.9 56.1
Video-LLaMA 32.9 71.6 49.8 175.3 16.5 123.7
Video-LLaMA[12] 46.5 58.3 45.5
Ours 48.5 60.6 49.8 Ours 33.4 74.6 51.0 179.1 17.6 131.2
Table5.ActionanticipationresultsonEpicKitchens-100.
FormerwithoutintroducinganadditionalvideoQ-Former,
yetisabletoeffectivelycapturetemporalrelationshipsby Accuracy@Top-5 Recall@Top-5
Model
virtueofthelong-termmemorybank. Thisresultstrongly
Verb Noun Act. Verb Noun Act.
justifiesthesuperiorityofourdesignonthegeneralvideo
questionansweringtask,andrevealsthatevenafewframes Video-LLaMA 73.9 47.5 29.7 26.3 27.3 11.7
andqueriescapturedinthememorybankscanhavesignifi- Ours 74.5 50.7 32.7 25.9 29.9 12.2
cantbeneficialeffects.
100[55]datasettoinvestigatetheonlineactionprediction
VideoCaptioning. Tofurtherevaluatethecapabilitiesof
capability. InTable5, ourMA-LMMoutperformsVideo-
our MA-LMM in generating free-form text, we conduct
LLaMA,achievingmoreaccurateresultsinbothtop-5ac-
experiments on the standard video captioning datasets in-
curacy and recall measures. This highlights our model’s
cluding MSRVTT [52], MSVD [53] and YouCook2 [54]
superiorcapacitytoanticipateactionsinanonlinemanner,
inTable4. Althoughthesedatasetsonlyconsistofvideos
showcasing its effectiveness for applications that require
with short duration and our model is initially pre-trained
real-timeanalyticalcapabilities.
merelyonimage-textdatasetpairs,ourMA-LMMexhibits
outstandingperformancesacrossallthemetrics. Itconsis- 4.4.AblationStudies
tentlyranksamongthetop-2positionscomparedtocurrent
Contributionofeachcomponent.Tofurtherinvestigatethe
leadingmethods. Remarkably,ourresultsalsosurpassthe
contributionofthevisualmemorybankandquerymemory
recent Video-LLaMA [12] on these datasets, highlighting
bank,weconductablationstudiesinTable6. Initially,we
thesignificantimprovementsourmodeloffersinbothvideo
observethatwithoutanymemorybankmodule,theperfor-
captioningandquestion-answeringtasks.
mancesacrossallthreedatasetsarenotablyworse,duetothe
OnlineActionPrediction. Sinceourmodelcannaturally lackoftemporalcontext. Theintroductionofeithermemory
support the online video understanding task, we compare bankresultsinsubstantialimprovements,confirmingtheir
our MA-LMM with Video-LLaMA on the EpicKitchens- rolesinenhancingthemodel’sabilitytounderstandtempo-
6Table6.Contributionofvisualandquerymemorybanks. Table8.Ablationofdifferenttemporalmodelingmethods.
Visual Query LVU Breakfast COIN Method #Frame #Token GPU LVU Breakfast COIN
✗ ✗ 48.3 74.6 72.3 Concat 60 1920 49.2 62.6 90.4 93.0
AvgPool 100 32 21.2 57.6 80.6 87.6
✓ ✗ 61.5 91.8 92.4
ToMe 100 200 22.2 61.5 91.3 91.5
✗ ✓ 58.0 81.4 88.5
FIFO 100 32 19.1 61.3 88.5 90.4
✓ ✓ 63.0 93.0 93.2
MBC 100 32 19.1 63.0 93.0 93.2
Table 7. Contribution of the long-term memory bank (MB)
underoff-the-shelfevaluationwithouttraining. Table9.ThecomparisonofusingdifferentLLMs.
MB MSRVTT MSVD ActivityNet LVU LLM MSRVTT MSVD ActivityNet LVU
✗ 19.5 38.8 29.9 23.6 FlanT5-XL 46.5 57.6 48.2 62.0
✓ 20.3 40.0 37.2 32.8 Vicuna-7B 48.5 60.6 49.8 63.0
ral sequences. We also find that the visual memory bank
achievesbetterperformancethanthequerymemorybank. 90
Wehypothesizethattheexplicitmethodofstoringhistorical
rawvideofeaturesinthevisualmemorybankismoreeffec- 80 LVU
tivethanthequerymemorybankwhichimplicitlycaptures Breakfast
videoinformationthroughtheinputlearnedqueries. And 70 COIN
twomemorybanksarecomplementarytoeachother. When
incorporatingtwomemorybankstogether,ourapproachcan 60
boostthefinalperformanceby14.7%,18.4%,and20.9%on
theLVU,Breakfast,andCOIN,respectively. 1 5 10 20 40 60 80 100
Memory Bank Length
Long-termtemporalmodelingablation. Wecomparedif-
Figure3.Impactofdifferentmemorybanklengths.
ferent temporal modeling approaches in Table 8. In our
setup,theQ-Formeroutputs32texttokensperframe. The
is that our long-term memory bank can be inserted into
moststraightforwardapproachfortemporalfeatureintegra-
existinglargemultimodalmodelsinanoff-the-shelfmanner,
tioniseitherconcatenatingoraveragingframe-levelfeatures.
thereby endowing them with effective temporal modeling
However,theyresultedininferiorperformances. Notably,
capabilitieswithoutretraining. AspresentedinTable7,MA-
concatenationrequiresasignificantlyhighernumberoftext
LMM can consistently boost the final performance when
tokensandcomputationalcostcomparedtoothervariants,
incorporating the long-term memory bank to the baseline
which also introduces higher GPU memory consumption
method[9]. Particularly, onlong-termvideodatasetslike
sincetheyneedtotakesinallthevideoframessimultane-
ActivityNetandLVU,MA-LMMcanlargelyimprovethe
ously. Inaddition,weconductexperimentsusingToMe[72]
resultsby7.3%and9.2%. Thishighlightstherobustnessof
to reduce the number of text tokens per frame from 32 to
long-termmemorybanksintemporalmodelingunderthe
2. However,withoutourauto-regressivestrategy,itstillre-
off-the-shelfsetting.
quires200texttokensfor100-frameinput. Thesecondpart
ofthistablepresentstheperformancesofdifferentmemory Differentlanguagemodelarchitectures. OurMA-LMM
bankcompressionapproaches. Thefirst-in-first-out(FIFO) canutilizedifferentlanguagemodelarchitecturesincluding
techniqueremovestheoldestfeaturestomainthelengthof butnotlimitedtoencoder-decodermodelsanddecoder-only
thememorybankfixed,whilethememorybankcompres- models. WeexperimentedwithtwopopularmodelsFlanT5-
sion(MBC)strategymergestemporallyconsecutivefeatures XL[73]andVicuna-7B[59],andshowtheresultsinTable9
with the highest similarity, effectively reducing the most thattheVicuna-7BmarginallyoutperformstheFlanT5-XL
redundantinformationwhilekeepingthetemporalordering onthesevideotasks.
unchanged. Withthisdesignthattheoreticallykeepsallhis-
Memorybanklengthablation. InFigure3, weconduct
toricalinformation,MBCoutperformsFIFOby1.7%,4.5%,
experiments to evaluate the effect of varying the memory
and2.8%accuracyacrossthreedatasets. Thisexperimental
banklength. Givenaninputof100videoframes,thetop-1
resultvalidatesthesuperiorefficiencyandeffectivenessof
accuracyfirstincreasesasthefeaturebanklengthbecomes
ourapproachinmodelinglong-termtemporalinformation.
larger. Thisrisecanbeattributedtotheaugmentedstorage
Off-the-shelfevaluation. AkeyadvantageofMA-LMM capacity of the memory bank, which can preserve more
7
)%(
ycaruccA
1-poTVideo-LLaMA Ours
1.What are people doing in the ground in video? play football play football
2.What color is the man with No.7 in the video? blue red
3.How many goalkeepers are there in the video? 1 2
4.Why is the yellow team celebrating? win goal
(a)VideoquestionansweringTask(ActivityNet-QA)
Q: What happened in the last 5 seconds? Q:What willhappenforthenext5seconds? Q:What is therecipeofthisvideo?
Video-LLaMA:A glass of water is poured Video-LLaMA:Apersoniscooking food in a Video-LLaMA:This video shows the
into a glass stainless steel pan with an orange on the table preparation of eggs in a glass dish
Ours:Eggs were poured into bowl Ours:Eggwillbecooked Ours:Scrambledeggs
(b)Onlineoff-the-shelfsettingwithcustomquestions
Figure4.Visualizationresultsonthevideoquestionansweringtaskandtheonlineoff-the-shelfsetting.
cut potato into strips drystrips putintheoiltofry lift strips from the pan placefriesontotheplate
Figure5.Visualizationofthecompressedvisualmemorybank.
historicaldataandconsequentlyboostthefinalperformance. memorybank. Wesetthememorybanklengthto5forthis
However, we observe that performances begin to saturate illustration. Thecompressedvisualmemorybankappears
when the memory bank length is around 10 to 20. This togroupconsecutiveframeswithsimilarvisualcontent. For
supports our hypothesis that there are prevalent temporal instance,inthepresentedvideo,thevideoframesareeffec-
redundanciesinlongvideos,andwecansignificantlyreduce tively grouped into five clusters, each capturing a distinct
theframelengthwithoutsacrificingtheperformance. yetsemanticallyconsistentactivity,whichissimilartothe
effectoftemporalsegmentation.
4.5.Visualization
5.Conclusion
InFigure4,weprovideacomprehensivevisualcomparison
In this paper, we introduce a long-term memory bank de-
betweenMA-LMMandVideo-LLaMA[12]. Inthevideo
signedtoaugmentcurrentlargemultimodalmodels,equip-
question answering task, our MA-LMM exhibits superior
pingthemwiththecapabilitiestoeffectivelyandefficiently
memorizationandrecognitioncapabilities. Specifically,it
modellong-termvideosequences. Ourapproachprocesses
canaccuratelymemorizehistoricalinformationandrecog-
videoframessequentiallyandstoreshistoricaldatainthe
nizefine-grainedinformation,suchasthecoloroftheman
memorybank,addressingLLMs’contextlengthlimitation
withNo.7,andpreciselycountthenumberofgoalkeepers
andGPUmemoryconstraintsposedbythelongvideoinputs.
whoappearedinthevideo. Withtheauto-regressivedesign,
Ourlong-termmemorybankisaplug-and-playmodulethat
ourmodelsupportsonlinereasoningdirectly.Thiscapability
canbeeasilyintegratedintoexistinglargemultimodalmod-
is further exemplified in our experiments on off-the-shelf
elsinanoff-the-shelfmanner. Experimentsonvarioustasks
evaluations using custom questions. We can see that, our
havedemonstratedthesuperioradvantagesofourmethod.
model can correctly anticipate the next step of the video
WebelieveourMA-LMMoffersvaluableinsightsforfuture
("eggwillbecooked")andpredictthecorrectrecipeofthe
researchinthelong-termvideounderstandingarea.
video("scrambledegg"). Morevisualizationexamplesare
showninthesupplementarymaterial. Acknowledgements. Thisprojectwaspartiallyfundedby
Figure5providesavisualizationofthecompressedvisual NSFCAREERAward(#2238769)toAS.
8References [15] Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu,
Xizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou,
[1] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya
YuQiao,etal. Visionllm: Largelanguagemodelisalsoan
Sutskever,etal. Improvinglanguageunderstandingbygener-
open-endeddecoderforvision-centrictasks. arXivpreprint
ativepre-training. OpenAI,2018. 1,2
arXiv:2305.11175,2023. 2
[2] AlecRadford,JeffreyWu,RewonChild,DavidLuan,Dario
[16] Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechu
Amodei,IlyaSutskever,etal. Languagemodelsareunsuper-
Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi,
visedmultitasklearners. OpenAIblog,1(8):9,2019.
VikasChandra, YunyangXiong, andMohamedElhoseiny.
[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
Minigpt-v2: Large language model as a unified interface
biah,JaredDKaplan,PrafullaDhariwal,ArvindNeelakantan,
for vision-language multi-task learning. arXiv preprint
PranavShyam,GirishSastry,AmandaAskell,etal.Language
arXiv:2310.09478,2023.
modelsarefew-shotlearners.Advancesinneuralinformation
[17] JunkeWang,DongdongChen,ZuxuanWu,ChongLuo,Lu-
processingsystems,33:1877–1901,2020.
oweiZhou,YuchengZhao,YujiaXie,CeLiu,Yu-GangJiang,
[4] OpenAI. Chatgpt. https://openai.com/blog/chatgpt,2023. 1,
and Lu Yuan. Omnivl: One foundation model for image-
2
languageandvideo-languagetasks. InNeurIPS,2022.
[5] HugoTouvron,ThibautLavril,GautierIzacard,XavierMar-
[18] JunkeWang,XitongYang,HengduoLi,LiLiu,ZuxuanWu,
tinet,Marie-AnneLachaux,TimothéeLacroix,BaptisteRoz-
andYu-GangJiang. Efficientvideotransformerswithspatial-
ière,NamanGoyal,EricHambro,FaisalAzhar,etal. Llama:
temporaltokenselection. InECCV,2022.
Open and efficient foundation language models. arXiv
[19] Junke Wang, Dongdong Chen, Zuxuan Wu, Chong Luo,
preprintarXiv:2302.13971,2023. 1
Xiyang Dai, Lu Yuan, and Yu-Gang Jiang. Omnitracker:
[6] HugoTouvron,LouisMartin,KevinStone,PeterAlbert,Am-
Unifyingobjecttrackingbytracking-with-detection. arXiv
jadAlmahairi,YasmineBabaei,NikolayBashlykov,Soumya
preprintarXiv:2303.12079,2023.
Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2:
[20] JunkeWang,DongdongChen,ChongLuo,BoHe,LuYuan,
Openfoundationandfine-tunedchatmodels. arXivpreprint
Zuxuan Wu, and Yu-Gang Jiang. Omnivid: A generative
arXiv:2307.09288,2023. 1,2
frameworkforuniversalvideounderstanding.InCVPR,2024.
[7] JunnanLi,DongxuLi,SilvioSavarese,andStevenHoi. Blip-
1
2: Bootstrappinglanguage-imagepre-trainingwithfrozen
imageencodersandlargelanguagemodels. arXivpreprint [21] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and
arXiv:2301.12597,2023. 1,2,3 FahadShahbazKhan. Video-chatgpt:Towardsdetailedvideo
understandingvialargevisionandlanguagemodels. arXiv
[8] HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee.
preprintarXiv:2306.05424,2023. 1,2
Visualinstructiontuning. arXivpreprintarXiv:2304.08485,
2023. 1,2,4 [22] Jean-BaptisteAlayrac,JeffDonahue,PaulineLuc,Antoine
Miech,IainBarr,YanaHasson,KarelLenc,ArthurMensch,
[9] WenliangDai,JunnanLi,DongxuLi,AnthonyMengHuat
KatherineMillican,MalcolmReynolds,etal. Flamingo: a
Tiong,JunqiZhao,WeishengWang,BoyangLi,PascaleFung,
visuallanguagemodelforfew-shotlearning. Advancesin
andStevenHoi.Instructblip:Towardsgeneral-purposevision-
Neural Information Processing Systems, 35:23716–23736,
languagemodelswithinstructiontuning, 2023. 1, 2, 3, 5,
2022. 2
7
[10] DeyaoZhu, JunChen, XiaoqianShen, XiangLi, andMo- [23] EdwardJHu,YelongShen,PhillipWallis,ZeyuanAllen-Zhu,
hamed Elhoseiny. Minigpt-4: Enhancing vision-language YuanzhiLi,SheanWang,LuWang,andWeizhuChen. Lora:
understandingwithadvancedlargelanguagemodels. arXiv Low-rankadaptationoflargelanguagemodels.arXivpreprint
preprintarXiv:2304.10592,2023. 2 arXiv:2106.09685,2021. 2
[11] QinghaoYe,HaiyangXu,GuohaiXu,JiaboYe,MingYan, [24] Junke Wang, Dongdong Chen, Chong Luo, Xiyang Dai,
Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, LuYuan, ZuxuanWu, andYu-GangJiang. Chatvideo: A
Yaya Shi, et al. mplug-owl: Modularization empowers tracklet-centricmultimodalandversatilevideounderstanding
largelanguagemodelswithmultimodality. arXivpreprint system. arXivpreprintarXiv:2304.14407,2023. 2
arXiv:2304.14178,2023. 2 [25] JeffreyDonahue,LisaAnneHendricks,SergioGuadarrama,
[12] HangZhang, XinLi, andLidongBing. Video-llama: An MarcusRohrbach, SubhashiniVenugopalan, KateSaenko,
instruction-tunedaudio-visuallanguagemodelforvideoun- andTrevorDarrell. Long-termrecurrentconvolutionalnet-
derstanding. arXivpreprintarXiv:2306.02858,2023. 2,3,5, worksforvisualrecognitionanddescription. InProceedings
6,8,12 oftheIEEEconferenceoncomputervisionandpatternrecog-
[13] Kunchang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai nition,pages2625–2634,2015. 3
Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. [26] JoeYue-HeiNg,MatthewHausknecht,SudheendraVijaya-
Videochat:Chat-centricvideounderstanding. arXivpreprint narasimhan,OriolVinyals,RajatMonga,andGeorgeToderici.
arXiv:2305.06355,2023. 1,2 Beyondshortsnippets: Deepnetworksforvideoclassifica-
[14] GuoChen,Yin-DongZheng,JiahaoWang,JilanXu,Yifei tion. InProceedingsoftheIEEEconferenceoncomputer
Huang, JuntingPan, YiWang, YaliWang, YuQiao, Tong visionandpatternrecognition,pages4694–4702,2015.
Lu, etal. Videollm: Modelingvideosequencewithlarge [27] RohitGirdhar,DevaRamanan,AbhinavGupta,JosefSivic,
languagemodels. arXivpreprintarXiv:2305.13292,2023. 1 andBryanRussell. Actionvlad: Learningspatio-temporal
9aggregationforactionclassification. InProceedingsofthe [40] MandelaPatrick,DylanCampbell,YukiAsano,IshanMisra,
IEEEconferenceoncomputervisionandpatternrecognition, FlorianMetze,ChristophFeichtenhofer,AndreaVedaldi,and
pages971–980,2017. JoaoFHenriques.Keepingyoureyeontheball:Trajectoryat-
[28] Noureldien Hussein, Efstratios Gavves, and Arnold WM tentioninvideotransformers.Advancesinneuralinformation
Smeulders. Timeceptionforcomplexactionrecognition. In processingsystems,34:12493–12506,2021. 6
ProceedingsoftheIEEE/CVFConferenceonComputerVi- [41] Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy,
sionandPatternRecognition,pages254–263,2019. 6 andCordeliaSchmid. Videobert: Ajointmodelforvideo
[29] Chao-YuanWuandPhilippKrahenbuhl. Towardslong-form and language representation learning. In Proceedings of
videounderstanding. InProceedingsoftheIEEE/CVFCon- theIEEE/CVFinternationalconferenceoncomputervision,
ferenceonComputerVisionandPatternRecognition,pages pages7464–7473,2019. 5,6
1884–1894,2021. 3,5,6,12 [42] HildeKuehne,AliArslan,andThomasSerre.Thelanguageof
[30] BrunoKorbar,DuTran,andLorenzoTorresani. Scsampler: actions:Recoveringthesyntaxandsemanticsofgoal-directed
Samplingsalientclipsfromvideoforefficientactionrecogni- humanactivities. InProceedingsoftheIEEEconferenceon
tion. InProceedingsoftheIEEE/CVFInternationalConfer- computer vision and pattern recognition, pages 780–787,
enceonComputerVision,pages6232–6242,2019. 3 2014. 5,6,12
[31] ZuxuanWu,CaimingXiong,Chih-YaoMa,RichardSocher, [43] Yansong Tang, Dajun Ding, Yongming Rao, Yu Zheng,
and Larry S Davis. Adaframe: Adaptive frame selection DanyangZhang,LiliZhao,JiwenLu,andJieZhou. Coin:
forfastvideorecognition. InProceedingsoftheIEEE/CVF Alarge-scaledatasetforcomprehensiveinstructionalvideo
Conference on Computer Vision and Pattern Recognition, analysis. InProceedingsoftheIEEE/CVFConferenceon
pages1278–1287,2019. 3 ComputerVisionandPatternRecognition,pages1207–1216,
[32] MdMohaiminulIslamandGedasBertasius. Longmovieclip 2019. 5,6,12
classificationwithstate-spacevideomodels. InEuropean [44] LiminWang,YuanjunXiong,ZheWang,YuQiao,DahuaLin,
Conference on Computer Vision, pages 87–104. Springer, XiaoouTang,andLucVanGool.Temporalsegmentnetworks
2022. 3,5,6,12 foractionrecognitioninvideos.IEEEtransactionsonpattern
[33] JueWang,WentaoZhu,PichaoWang,XiangYu,LindaLiu, analysisandmachineintelligence,41(11):2740–2755,2018.
Mohamed Omar, and Raffay Hamid. Selective structured 6
state-spacesforlong-formvideounderstanding. InProceed- [45] Noureldien Hussein, Efstratios Gavves, and Arnold WM
ingsoftheIEEE/CVFConferenceonComputerVisionand Smeulders. Videograph:Recognizingminutes-longhuman
PatternRecognition,pages6387–6397,2023. 3,5,6,12 activitiesinvideos. arXivpreprintarXiv:1905.05143,2019.
[34] AlbertGu,KaranGoel,andChristopherRé. Efficientlymod- 6
eling long sequences with structured state spaces. arXiv [46] JiamingZhou,Kun-YuLin,HaoxinLi,andWei-ShiZheng.
preprintarXiv:2111.00396,2021. 3 Graph-basedhigh-orderrelationmodelingforlong-termac-
[35] YihongChen,YueCao,HanHu,andLiweiWang. Memory tionrecognition.InProceedingsoftheIEEE/CVFConference
enhancedglobal-localaggregationforvideoobjectdetection. onComputerVisionandPatternRecognition,pages8984–
In Proceedings of the IEEE/CVF conference on computer 8993,2021. 6
visionandpatternrecognition,pages10337–10346,2020. 3 [47] Xudong Lin, Fabio Petroni, Gedas Bertasius, Marcus
[36] SanghoLee,JinyoungSung,YoungjaeYu,andGunheeKim. Rohrbach,Shih-FuChang,andLorenzoTorresani. Learning
Amemorynetworkapproachforstory-basedtemporalsum- to recognize procedural activities with distant supervision.
marizationof360videos. InProceedingsoftheIEEEcon- InProceedingsoftheIEEE/CVFConferenceonComputer
ferenceoncomputervisionandpatternrecognition,pages VisionandPatternRecognition,pages13853–13863,2022. 6
1410–1419,2018. [48] DejingXu,ZhouZhao,JunXiao,FeiWu,HanwangZhang,
[37] SangminLee,HakGuKim,DaeHwiChoi,Hyung-IlKim, XiangnanHe,andYuetingZhuang.Videoquestionanswering
andYongManRo. Videopredictionrecallinglong-termmo- viagraduallyrefinedattentionoverappearanceandmotion.
tioncontextviamemoryalignmentlearning. InProceedings InProceedingsofthe25thACMinternationalconferenceon
oftheIEEE/CVFConferenceonComputerVisionandPattern Multimedia,pages1645–1653,2017. 5,12
Recognition,pages3054–3063,2021. [49] ZhouYu,DejingXu,JunYu,TingYu,ZhouZhao,Yueting
[38] Chao-YuanWu,YanghaoLi,KarttikeyaMangalam,Haoqi Zhuang, and Dacheng Tao. Activitynet-qa: A dataset for
Fan,BoXiong,JitendraMalik,andChristophFeichtenhofer. understandingcomplexwebvideosviaquestionanswering.In
Memvit:Memory-augmentedmultiscalevisiontransformer ProceedingsoftheAAAIConferenceonArtificialIntelligence,
forefficientlong-termvideorecognition. InProceedingsof volume33,pages9127–9134,2019. 5,12
theIEEE/CVFConferenceonComputerVisionandPattern [50] SatanjeevBanerjeeandAlonLavie. Meteor:Anautomatic
Recognition,pages13587–13597,2022. 3 metricformtevaluationwithimprovedcorrelationwithhu-
[39] Krzysztof Choromanski, Valerii Likhosherstov, David Do- manjudgments.InProceedingsoftheaclworkshoponintrin-
han, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter sicandextrinsicevaluationmeasuresformachinetranslation
Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, and/orsummarization,pages65–72,2005. 5
etal. Rethinkingattentionwithperformers. arXivpreprint [51] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi
arXiv:2009.14794,2020. 6 Parikh. Cider: Consensus-based image description evalu-
10ation. InProceedingsoftheIEEEconferenceoncomputer ofend-to-endvideo-languagetransformerswithmaskedvi-
visionandpatternrecognition,pages4566–4575,2015. 5 sualmodeling. InProceedingsoftheIEEE/CVFConference
[52] JunXu,TaoMei,TingYao,andYongRui. Msr-vtt:Alarge onComputerVisionandPatternRecognition,pages22898–
videodescriptiondatasetforbridgingvideoandlanguage. In 22909,2023. 6
ProceedingsoftheIEEEconferenceoncomputervisionand [64] Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li,
patternrecognition,pages5288–5296,2016. 5,6,12 KevinLin,ZheGan,ZichengLiu,CeLiu,andLijuanWang.
[53] DavidLChenandWilliamBDolan. Collectinghighlypar- Git: Agenerativeimage-to-texttransformerforvisionand
alleldataforparaphraseevaluation. InProceedingsofthe language. arXivpreprintarXiv:2205.14100,2022. 6
49thAnnualMeetingoftheAssociationforComputational [65] Haiyang Xu, Qinghao Ye, Ming Yan, Yaya Shi, Jiabo
Linguistics:HumanLanguageTechnologies-Volume1,pages Ye, Yuanhong Xu, Chenliang Li, Bin Bi, Qi Qian, Wei
190–200.AssociationforComputationalLinguistics,2011.5, Wang, et al. mplug-2: A modularized multi-modal foun-
6,12 dationmodelacrosstext,imageandvideo. arXivpreprint
[54] LuoweiZhou,ChenliangXu,andJasonCorso. Towardsauto- arXiv:2302.00402,2023. 6
maticlearningofproceduresfromwebinstructionalvideos. [66] KunchangLi, YaliWang, YizhuoLi, YiWang, YinanHe,
InProceedingsoftheAAAIConferenceonArtificialIntelli- Limin Wang, and Yu Qiao. Unmasked teacher: Towards
gence,volume32,2018. 5,6,12 training-efficientvideofoundationmodels,2023. 6
[55] Dima Damen, Hazel Doughty, Giovanni Maria Farinella, [67] ShenYan,TaoZhu,ZiruiWang,YuanCao,MiZhang,Soham
SanjaFidler,AntoninoFurnari,EvangelosKazakos,Davide Ghosh,YonghuiWu,andJiahuiYu.Video-textmodelingwith
Moltisanti,JonathanMunro,TobyPerrett,WillPrice,etal. zero-shottransferfromcontrastivecaptioners. arXivpreprint
Theepic-kitchensdataset: Collection,challengesandbase- arXiv:2212.04979,2022. 5,6
lines. IEEETransactionsonPatternAnalysisandMachine
[68] Huaishao Luo, Lei Ji, Botian Shi, Haoyang Huang, Nan
Intelligence,43(11):4125–4141,2020. 5,6
Duan,TianruiLi,JasonLi,TaroonBharti,andMingZhou.
[56] ZeyunZhong,DavidSchneider,MichaelVoit,RainerStiefel-
Univl:Aunifiedvideoandlanguagepre-trainingmodelfor
hagen,andJürgenBeyerer. Anticipativefeaturefusiontrans- multimodal understanding and generation. arXiv preprint
formerformulti-modalactionanticipation. InProceedings arXiv:2002.06353,2020. 6
oftheIEEE/CVFWinterConferenceonApplicationsofCom-
[69] KevinLin,LinjieLi,Chung-ChingLin,FaisalAhmed,Zhe
puterVision,pages6068–6077,2023. 5
Gan,ZichengLiu,YumaoLu,andLijuanWang. Swinbert:
[57] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
End-to-endtransformerswithsparseattentionforvideocap-
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
tioning.InProceedingsoftheIEEE/CVFConferenceonCom-
MostafaDehghani,MatthiasMinderer,GeorgHeigold,Syl-
puterVisionandPatternRecognition,pages17949–17958,
vainGelly, etal. Animageisworth16x16words: Trans-
2022. 6
formers for image recognition at scale. arXiv preprint
[70] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac,
arXiv:2010.11929,2020. 5
MakarandTapaswi,IvanLaptev,andJosefSivic.Howto100m:
[58] YuxinFang,WenWang,BinhuiXie,QuanSun,LedellWu,
Learningatext-videoembeddingbywatchinghundredmil-
XinggangWang,TiejunHuang,XinlongWang,andYueCao.
lionnarratedvideoclips. InProceedingsoftheIEEE/CVF
Eva: Exploringthelimitsofmaskedvisualrepresentation
international conference on computer vision, pages 2630–
learningatscale.InProceedingsoftheIEEE/CVFConference
2640,2019. 5
onComputerVisionandPatternRecognition,pages19358–
[71] ArshaNagrani, PaulHongsuckSeo, BryanSeybold, Anja
19369,2023. 5
Hauth, Santiago Manen, Chen Sun, and Cordelia Schmid.
[59] Wei-LinChiang,ZhuohanLi,ZiLin,YingSheng,Zhanghao
Learning audio-video modalities from image captions. In
Wu,HaoZhang,LianminZheng,SiyuanZhuang,Yonghao
EuropeanConferenceonComputerVision,pages407–426.
Zhuang,JosephE.Gonzalez,IonStoica,andEricP.Xing.
Springer,2022. 5
Vicuna:Anopen-sourcechatbotimpressinggpt-4with90%*
[72] DanielBolya,Cheng-YangFu,XiaoliangDai,PeizhaoZhang,
chatgptquality,March2023. 5,7
ChristophFeichtenhofer,andJudyHoffman. Tokenmerging:
[60] AntoineYang,AntoineMiech,JosefSivic,IvanLaptev,and
Yourvitbutfaster. arXivpreprintarXiv:2210.09461,2022. 7
CordeliaSchmid.Justask:Learningtoanswerquestionsfrom
[73] HyungWonChung,LeHou,ShayneLongpre,BarretZoph,
millionsofnarratedvideos. InProceedingsoftheIEEE/CVF
YiTay,WilliamFedus,YunxuanLi,XuezhiWang,Mostafa
international conference on computer vision, pages 1686–
Dehghani, Siddhartha Brahma, et al. Scaling instruction-
1697,2021. 6
finetunedlanguagemodels.arXivpreprintarXiv:2210.11416,
[61] AntoineYang,AntoineMiech,JosefSivic,IvanLaptev,and
2022. 7
CordeliaSchmid. Zero-shotvideoquestionansweringvia
frozenbidirectionallanguagemodels. AdvancesinNeural
InformationProcessingSystems,35:124–141,2022. 6
[62] JieLei,TamaraLBerg,andMohitBansal. Revealingsingle
framebiasforvideo-and-languagelearning. arXivpreprint
arXiv:2206.03428,2022. 6
[63] Tsu-JuiFu, LinjieLi, ZheGan, KevinLin, WilliamYang
Wang,LijuanWang,andZichengLiu. Anempiricalstudy
11Appendix Table10.Memorybankcompressionatdifferentspatiallevels.
WereportadditionalablationexperimentsinSec.A.Wealso SpatialLevel LVU Breakfast COIN
presentmorequalitativeresultsonthevideocaptioningtask
Frame-level 61.8 86.5 91.1
in Sec. B. And in Sec. C, we show more dataset-specific
implementationdetailsandhyper-parameters. Finally,we Token-level 63.0 93.0 93.2
discusssomelimitationsandfutureworksinSec.D.
3000
A.AdditionalExperiments 2500 Concat
2000 Ours
Memory bank compression at different spatial levels. 1500
1000
In Table 10, we show comparison results of compressing
500
thememorybankatdifferentspatiallevels(frame-levelvs.
0
token-level)ontheLVU[29],Breakfast[42]andCOIN[43] 0 20 40 60 80 100
Number of Frames
datasets. Fortheframe-levelcompression,wecalculatethe Figure6.Inferencetimevs.inputframelength.
cosine similarity between adjacent frame features and av-
erage the frame-level features with the highest similarity. describingfine-graineddetails.
Forthetoken-levelcompression,thecosinesimilarityiscal-
culatedbetweentokensatthesamespatiallocationacross C.ExperimentDetails
theentiretemporalaxis,giventhateachframe-levelfeature
We show the details of hyper-parameters in the following
containsmultipletokensatdifferentspatiallocations. The
tablefordifferenttasksanddatasets. Foralltheexperiments,
results indicate that token-level compression consistently
we use a cosine learning rate decay. Table 11 shows the
surpassesframe-levelcompressioninperformance. Partic-
hyper-parametersforthelong-termvideounderstandingtask.
ularly, on the Breakfast dataset, the token-level surpasses
FortheLVUdataset,wefollowthesamepracticein[32,33],
theframe-levelby6.5%intop-1accuracy. Thissuperiority
wesample100framesof1fpsforeachvideoclip. Forthe
canbeattributedtotheimportanceofrecognizingtheobject
Breakfast [42] and COIN [43], we uniformly sample 100
typeofbreakfastinvideos.Andtoken-levelcompressioncan
frames from the whole video. Table 12 shows the hyper-
helppreservemuchmorefine-grainedspatialinformation
parametersontheMSRVTT-QA[48],MSVD-QA[48],and
anddetails.
ActivityNet-QA[49]datasetsforthevideoquestionanswer-
InferencetimeofdifferentinputframesInFigure6,the ingtaskwhileTable13presentsthehyperparametersonthe
inferencetimeofMA-LMMincreaseslinearlywithrespect MSRVTT[52],MSVD[53],YouCook2[54]datasetsforthe
to the frame lengths, due to its auto-regressive design of videocaptioningtasks.
processingvideoframessequentially. Incontrast,directly
concatenatingframe-levelfeaturestakesmuchlongertime D.LimitationandFutureWork
and higher GPU memory consumption, since it needs to
processallvideoframessimultaneously. Sinceourmodeltakesinvideoframesinanonlinemanner,
leadingtoreducedGPUmemoryusage,butatthecostof
increased video processing time. This trade-off becomes
B.MoreQualitativeResults
particularlynoticeablewithextremelylongvideos,where
Ourmodel’senhancedcapabilitiesinvideocaptioningare processing times can become significantly prolonged. To
furthershowcasedthroughadditionalvisualizationresults mitigate this issue, we suggest a hierarchical method to
inFigure7. Here,ourMA-LMMsignificantlyoutperforms processextremelylong-termvideosequences. Thisstrategy
Video-LLaMA[12]ingeneratingdetailedandaccuratesen- involves dividing extensive videos into smaller segments
tencedescriptions. Forinstance,inthefirstvideo,ourmodel andthenprocessingeachsegmentsequentiallyinanauto-
precisely describes the action as "remove the onion rings regressive fashion as we present in the main paper. Then
and place them on the paper towel," capturing the entire we can employ additional video modeling techniques to
action steps, while Video-LLaMA’s description lacks this model inter-segment relationships. This method aims to
completeness,notablymissingthecrucialactionofremov- strikeabalancebetweenmemoryefficiencyandprocessing
ingtheonionrings. Inthesecondvideoexample,ourmodel speed, making it a practical solution for long-term video
distinguishesitselfbyaccuratelyidentifyingsubtledetails understanding.
suchasspecificingredients: chilipowder, salt, andgarlic For the future work, there are several potential aspects
powder, which Video-LLaMA overlooks. This highlights to further enhance the model’s capabilities. First, replac-
theenhancedcapabilityofourMA-LMMinrecognizingand ing the existing image-based visual encoder with a video
12
)s(
emiTGround-Truth: remove the onions and place on paper towel
Video-LLaMA: fry the onion rings in oil
Ours: remove the onion rings from the oil and place them on a paper towel
Ground-Truth: add garlic powder chili powder paprika salt cayenne pepper buffalo wing sauce to the wings and mix
Video-LLaMA: coat the chicken wings with the sauce
Ours: add chili powder salt garlic powder onion powder and paprika to the chicken and mix
Figure7.Visualizationresultsonthevideocaptioningtask.
or clip-based encoder can naturally enhance the model’s Table12.Hyperparametersofdifferentdatasetsonthevideoques-
abilitytocaptureshort-termvideodynamics. Thisprovides tionansweringtask.
a better representation of the video’s temporal dynamics.
Dataset MSRVTT MSVD ActivityNet
Second,themodel’soverallperformanceinunderstanding
videoscansubstantiallybenefitfromthepre-trainingstage LLM Vicuna-7B
onlarge-scalevideo-textdatasets. Thisapproachisacom-
Epochs 5
monpracticeinexistingresearchandhasproveneffectivein
Learningrate 1e-4
enhancinggeneralizationcapabilities. Finally,theflexibility
Batchsize 128
inherentinourmodel’sarchitectureallowsfortheincorpora-
tionofamoreadvancedLLMasthelanguagedecoder. This AdamWβ (0.9,0.999)
integrationoffersaclearopportunityforboostingthefinal Weightdecay 0.05
performance,makingourmodelmoreeffectiveininterpret- Imageresolution 224
ingandrespondingtocomplexvideocontent.
Beamsize 5
Framelength 20
Memorybanklength 10
Table11.Hyperparametersofdifferentdatasetsonthelong-term
Prompt “Question:{}ShortAnswer:”
videounderstandingtask.
Dataset LVU Breakfast COIN
Table13.Hyperparametersofdifferentdatasetsonthevideocap-
LLM Vicuna-7B
tioningtask.
Epochs 20
Learningrate 1e-4 Dataset MSRVTT MSVD YouCook2
Batchsize 64
LLM Vicuna-7B
AdamWβ (0.9,0.999)
Epochs 10
Weightdecay 0.05
Learningrate 1e-5 1e-5 1e-4
Imageresolution 224
Batchsize 128
Beamsize 5
Framelength 100 AdamWβ (0.9,0.999)
Memorybanklength 20 Weightdecay 0.05
“Whatisthe “What type “Whatisthe Beamsize 5
{task}ofthe ofbreakfast activity in
Imageresolution 224
Prompt movie?” is shown in thevideo?”
thevideo?” Framelength 80
Memorybanklength 40
Prompt “whatdoesthevideodescribe?”
13