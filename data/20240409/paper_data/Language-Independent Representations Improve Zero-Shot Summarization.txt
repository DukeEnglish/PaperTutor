Language-Independent Representations Improve Zero-Shot
Summarization
VladimirSolovyev∗ DanniLiu JanNiehues
KarlsruheInstituteofTechnology,Germany
vladimir.solovyev.90@gmail.com,{danni.liu, jan.niehues}@kit.edu
Abstract Finetune with summarization data
A→A en en
Finetuningpretrainedmodelsondownstream pretrained
B→B es es
generationtasksoftenleadstocatastrophicfor-
getting in zero-shot conditions. In this work, Intralingual zero-shot Crosslingual zero-shot
wefocusonsummarizationandtackletheprob- C→C A→B (seen)
lemthroughthelensoflanguage-independent gu f i n e t u n e d gu en f i n e t u n e d es
representations. Aftertrainingonmonolingual en gu
A→C (unseen)
summarization, we perform zero-shot trans-
fer to new languages or language pairs. We Figure1: Wefinetuneapretrainedmodel(e.g. mBART)
firstshownaivelyfinetunedmodelsarehighly onintralingualsummarizationdataandtestitonzero-
language-specificinbothoutputbehaviorand shotintralingualandcrosslingualsummarization.
internalrepresentations,resultinginpoorzero-
shot performance. Next, we propose query-
key(QK)finetuningtodecoupletask-specific ConneauandLample,2019;WuandDredze,2019),
knowledgefromthepretrainedlanguagegener- it remains challenging for generation tasks (Rön-
ationabilities. Then,aftershowingdownsides nqvistetal.,2019;Vuetal.,2022;LiandMurray,
ofthestandardadversariallanguageclassifier,
2023)includingsummarizationandtranslation. A
we propose a balanced variant that more di-
main obstacle is catastrophic forgetting (French
rectly enforces language-agnostic representa-
andChater,2002),wherelanguagessupportedby
tions. Moreover,ourqualitativeanalysesshow
removingsourcelanguageidentitycorrelates the pretrained model but not covered in the fine-
tozero-shotsummarizationperformance. Our tuning data are forgotten. In this work, we use
codeisopenlyavailable1. summarization as a testbed for various types of
zero-shotgeneration. AsshowninFigure1,given
1 Introduction
apretrainedmodelandintralingualsummarization
training data in some languages (A→A, B→B),
Pretrained multilingual models (Conneau et al.,
weaimforzero-shotintralingualandcrosslingual
2020;Liuetal.,2020;Xueetal.,2021;Linetal.,
summarizationonnewlanguages(C→C)andlan-
2022)havebeenestablishedaspromisingsources
guagepairs(A→B,A→C)respectively.
oftransferlearning,wheretask-specificfinetuning
benefits from the general knowledge learned on Toalleviatecatastrophicforgetting,onelineof
diverseunsuperviseddata. However,duetodataor worktrainsonadditionalunsuperviseddata(Mau-
computationalconstraints,thetask-specificdataof- rya et al., 2021; Vu et al., 2022; Chronopoulou
tenonlycoveralimitedsubsetofthelanguagesin etal.,2023). Besidesthecomputationaloverhead,
pretraining. Therefore,duringfinetuningitiscru- thisapproachraisesatheoreticalquestion: Asthe
cialtoretaintheknowledgeofthepretrainedmodel pretrainedlanguagemodelhasalreadylearnedex-
andtoenablezero-shottransfer,i.e.,performingthe tensively on unsupervised data, is it necessary to
taskonmorelanguagescoveredbythepretrained re-learnlanguagemodelingintask-specificfinetun-
model. While zero-shot crosslingual transfer has ing? Wethereforeexploreamorechallengingcase
shown very promising results on sequence clas- of only using paired summarization data without
sificationorlabelingproblems(Piresetal.,2019; relyingonanyunsuperviseddata.
We identify two challenges when generalizing
∗WorkdonewhileatKarlsruheInstituteofTechnology
summarization abilities to new languages. First,
1https://github.com/vladsolovyev/fairseq_
summarization/tree/main/summarization_scripts decoupling the task-specific knowledge from the
4202
rpA
8
]LC.sc[
1v02750.4042:viXratrainonen trainonen+es+ru eration. As zero-shot crosslingual generation re-
liesonlanguage-agnosticrepresentations,wetest
es-es ru-ru gu-gu gu-gu es-en ru-en es-ru tr-en
0.2 2.3 13.4 99.6 0.0 0.0 0.0 1.3 forthiswithaprobinganalysis(Adietal.,2017).
Specifically,weassessthedifficultyofrecovering
Table 1: Proportion of generated summaries in the
the source language identity on the encoder out-
correctlanguage(%)underzero-shotconditions.Codes:
put. Givenatrainedmodel,wetrainatoken-level
es(Spanish),ru(Russian),gu(Gujarati),tr(Turkish).
classifier for the input languages on the encoder
outputs.3 AsshowninFigure2,theclassifiercan
direct finetune +adversarial classifier
100
99.2 0.5 0.3 0.1 95.4 2.7 1.8 0.2 almostperfectlyrecoverthesourcelanguage. Even
80
0.6 99.0 0.3 0.1 2.8 95.3 1.8 0.1 60 afterexplicitlyencouraginglanguage-agnosticrep-
0.4 0.4 99.0 0.1 2.1 2.1 95.6 0.2 40 resentationswithanadversariallanguageclassifier
20
0.6 0.4 0.5 98.5 1.5 1.0 0.8 96.8 (Arivazhagan et al., 2019), recovering the source
en es ru tr en es ru tr
True True languageidentityremainseasy.
Figure2: Accuracyofaprobingclassifier. Higheraccu-
3 Approaches
racyindicatesmorelanguage-specificrepresentations.
3.1 DecouplingLanguagefromTask
language generation abilities is essential. In re- Query-Key (QK) Finetuning Prior works on
sponse,weproposeanewfinetuningmethodbased zero-shot generation (Chi et al., 2020; Maurya
on query and keys, which is shown effective for et al., 2021; Li et al., 2021) have highlighted the
bothintralingualandcrosslingualzero-shotsetting. needforselectivefinetuningtomitigateforgetting,
Forcrosslingualzero-shotsettings,itisalsocrucial where the consensus is updating the encoder and
to decouple language from content, i.e., creating cross-attention weights only. However, existing
language-agnosticrepresentations. Thishasbeen methods treat attention weights as a whole. A
shown to facilitate zero-shot crosslingual genera- closer look at the attention module reveals that,
tioningeneral(Phametal.,2019;Wuetal.,2022; only the value projections determine the basis of
Duquenne et al., 2023). There a prominent ap- theupcomingtransformations,whereasthequery
proachisadversarialtraining(Ganinetal.,2016; andkeycontrolhowtheinputsareaggregated. We
Chen et al., 2018), where the model is trained to hypothesize that the value projections should be
deceivealanguageclassifier. Weshowtheexisting kept unchanged to prevent losing pretrained gen-
formulationfailstofullyachievelanguage-agnostic erationcapabilitiesduringfinetuning. Incontrast,
representations, and improves it by explicitly in- queryandkeyareupdatedasadaptationtospecific
centivizingthemodeltodeceivetheclassifierinto tasks. Therefore,weproposeaselectivefinetuning
auniformclassdistribution. approach, which only updates the query and key
projection weights of encoder self-attention and
2 CurrentModelsareHighly
cross-attention.4
Language-Specific
Two-StepFinetuning Forthemorechallenging
Wefirstshowthatnaivefinetuningmakesthemod-
caseofcrosslingualzero-shotsummarization,our
elshighlylanguage-specificintheiroutputbehav-
approachismotivatedbythefactthatthetaskcon-
ior and internal representations. Table 1 shows
sistsoftwosubtasks: translationandsummariza-
the proportion of outputs in the correct language
tion. We first finetune the pretrained model for
after finetuning mBART on intralingual summa- translation5. Then we finetune again on intralin-
rizationdata. FinetuningonEnglishonlyleadsthe
gualsummarizationusingourproposedquery-key
modeltoforgetitsgenerationabilityforotherlan-
3DetailsontheprobinganalysisareinAppendixA.
guages, resulting in off-target generation (Zhang
4It is extendable to the parameter-efficient finetuning
etal.,2020a;Pfeifferetal.,2023),whereawrong
(PEFT) approach LoRA (Hu et al., 2022) by placing the
targetlanguage,oftenonewithsuperviseddata,is adapters on the query and key weights only. In the exper-
imentswedonotcomparetoprominentPEFTapproacheslike
generated. Althoughmultilingualtraininglargely
prompttuning(Lesteretal.,2021)andLoRA,aspriorworks
resolves off-target generation in intralingual set-
haveshowntheyintheirstandardformsstillsufferfromcatas-
tings2, the problem persists for crosslingual gen- trophicforgettinginfinetuning(Vuetal.,2022)orcontinual
pretraining(LiandLee,2024).
2This is consistent with recent or concurrent findings 5WedonotusethefinetunedmBARTontranslation(Tang
(Chirkovaetal.,2023;Pfeifferetal.,2023). etal.,2020)asitcanonlytranslatefromorintoEnglish.
detciderP
ne
se
ur
rtfinetuningtoretainitscrosslingualcapabilitiesac- areinAppendixC.1. Forthetwo-stepfinetuning,
quiredfromtranslationinthefirststep. thetranslationdatadetailsareinAppendixC.2.
3.2 DecouplingLanguagefromContent
Data Conditions Besides the direct zero-shot
Anadversariallanguageclassifierisoftenusedto condition, we compare to the following two data
decouple language from the semantic representa- conditions:
tions of input contents. Most existing works use • PipelineapproachtranslatingintoandfromEn-
the cross-entropy loss (Arivazhagan et al., 2019; glish: learnsummarizationonEnglishonlyand
Mallinsonetal.,2020)andagradientreversallayer translate with NLLB-200 (NLLB Team et al.,
(Ganinetal.,2016)toupdatetheencoderweights 2022), a recent open multilingual translation
intheoppositedirectionoftheclassifieraccuracy.6 model. Here we rely on English-only summa-
Aproblemwiththecross-entropy-basedformula- rizationasEnglishhasthemosttrainingdatain
tion is that it operates on single classes and does bothdatasets,whichpresumablyyieldsthehigh-
notincentivizelanguage-agnosticrepresentations estsummarizationquality. Whilethisapproach
on the output distribution level. The adversarial ensuresthattheoutputsareintherightlanguage,
classifiercouldpotentiallybeshiftallitspredicted thedownsidesareinferencelatencyandtransla-
probabilitymasstoanotherlanguage,achievinga tionerrorpropagation.
low classification accuracy but leaving the repre- • Supervised: trainonsuperviseddataforthezero-
sentationsstilllanguage-specific. Indeedasshown shotdirectionsasperformanceupper-bounds.
inFigure2,evenaftertrainingwiththisobjective,
aprobingclassifiercanstilleasilylearntorecover Baselines WecompareourQKfinetuningto:
thesourcelanguageidentity. • Encoderfinetuning(Chietal.,2020): Itonlyup-
datestheencoderweightstoretainthepretrained
Balanced Adversarial Language Classifier
generationcapability,asthedecoderisexpected
Giventhedrawbackabove,weproposeabalanced
tobemoreresponsibleforgeneration.
adversarialobjective. Specifically,wetraintheen-
• Layernorm and attention (LNA) finetuning (Li
coder such that a language classifier is only able
etal.,2021): Itonlyfinetunes: 1)layernorm,2)
topredictanuniformdistribution. Weachievethis
encoderself-attention,and3)cross-attention.
by a modified adversarial loss based on the KL-
We also compare to the standard formulation
divergence between the classifier output distribu-
oftheadversariallanguageclassifier(Arivazhagan
tionandauniformdistribution:
etal.,2019)basedonthecross-entropyloss.
L = D (P ∥ U), (1)
balanced_adversarial KL θ
classifier
TrainingandEvaluation Weinitializefromthe
whereP istheclassifieroutputdistributiononto- mBART(Liuetal.,2020)model,whichwaspre-
kenlevelandU = (1,..., 1)withN beingthe trainedonmonolingualdataof25languages. Fur-
N N
numberoflanguagestoclassify. thertrainingdetailsareinAppendixD. Toassess
summarizationquality,weuseROUGE(Lin,2004)
ResidualDrop Wefurthercombineourapproach
andBERTScore(Zhangetal.,2020b). Wereport
withresidualdrop(Liuetal.,2021),amethodpro-
ROUGE-L in the main text supply ROUGE-1/2
posedformachinetranslationthatdropstheresid-
in Appendix E. We use BERTScore F (F )
1 BERT
ualconnectionofamiddleencoderlayertoreduce
following the authors’ suggestions (Zhang et al.,
sourcelanguagesignalsintheencoderoutput.
2020b). To measure the percentage of outputs in
thecorrectlanguages,weusealanguageidentifier
4 ExperimentsandResults
(LuiandBaldwin,2012)andreportaccuracy(%).
4.1 ExperimentalSetup
DatasetsWetrainonintralingualsummarization 4.2 ImpactofQuery-KeyFinetuning
datainEnglishor{English,Spanish,Russian}. We
The intralingual zero-shot results are in Table 2
useXL-Sum(Hasanetal.,2021)andWikiLingua
withdetailedscoresinAppendixE. Fullfinetuning
(Ladhak et al., 2020) for experiments in Table 2
(row(1))onEnglish-onlydatacausessevereforget-
and Table 3 and respectively. The dataset details
ting,wheremostoftheoutputareinthewronglan-
6MoredetailsinAppendixB guage,whichfurtherharmssummarizationscores.ID Model es ru gu gu
(trainonen) (trainonen) (trainonen) (trainonen+es+ru)
RG-L F RG-L F RG-L F RG-L F
BERT BERT BERT BERT
(1) Fullft. 5.4 66.0 1.0 64.3 1.2 59.1 15.1 71.8
(2) Encoderft.(Chietal.,2020) 18.4 70.8 22.7 73.2 14.5 71.7 15.3 72.2
(3) “LNA”ft.(Lietal.,2021) 20.9 71.9 21.6 72.7 10.5 68.6 16.0 72.6
(4) Query-keyft.(ours) 21.3 72.3 23.4 73.6 16.6 73.2 16.5 73.1
(5) Pipeline(translateto/fromen) 20.7 72.1 20.2 72.4 13.6 72.1 13.6 72.1
(6) Supervised 25.0 74.0 27.5 75.1 19.3 74.2 19.3 74.2
Table2: Zero-shotintralingualsummarizationresultsonXL-Sum.
IDModel es-en ru-en es-ru avg.seen tr-en en-tr tr-tr avg.unseen
RG-LF RG-LF RG-LF RG-LF RG-LF RG-LF RG-LF RG-LF
BERT BERT BERT BERT BERT BERT BERT BERT
(1)Baselinezero-shot 2.2 67.8 0.7 63.3 0.6 64.6 1.2 65.2 4.6 62.9 2.5 60.9 18.0 71.5 8.4 65.1
(2)Adv.classifier 26.7 76.1 25.3 75.7 14.1 72.5 22.0 74.8 26.1 75.2 2.5 60.9 5.2 62.8 11.3 66.3
(3)Balancedadv.(ours) 27.2 76.4 25.6 75.8 14.3 72.8 22.4 75.0 26.6 75.5 2.6 60.9 3.2 61.1 10.8 65.8
(4)(3)+residualdrop 27.6 76.6 26.3 76.1 14.8 73.1 22.9 75.3 25.7 75.2 2.5 61.0 2.3 60.8 10.2 65.7
(5)Two-step+QKft.(ours) 27.7 76.5 26.3 76.1 14.8 73.4 22.9 75.3 30.7 77.4 16.7 71.3 18.4 72.0 21.9 73.6
(6)Pipeline 31.1 78.1 28.5 77.3 14.4 73.8 24.7 76.4 34.1 78.7 18.7 73.1 18.5 73.2 26.3 75.0
(7)Supervised 31.4 78.1 29.4 77.5 18.0 75.2 26.3 76.9 34.5 78.8 20.7 73.2 26.2 75.4 27.1 75.8
Table3: Zero-shotcrosslingualsummarizationresultsonWikiLinguaaftertrainingon{en,es,ru}intralingualdata,
groupedbyseen(newcombinationsoflanguagesseeninfinetuning)andunseen(languagesnotinfinetuning).
QK finetuning outperforms previous methods balanced adversarial +residual drop 100
59.2 25.2 15.2 0.2 49.6 32.9 17.0 0.5
andpipelineapproach: TheproposedQKfine- 80
22.3 68.9 8.6 0.2 33.8 52.3 13.6 0.4 60
tuninginrow(4)surpassesthetwopreviousmeth-
21.9 13.8 64.1 0.2 27.2 21.3 51.0 0.5 40
ods in row (2) and (3) by 0.4-2.1 ROUGE. It is
20
3.8 3.0 2.7 90.5 12.0 9.6 13.9 64.4
alsotheonlyapproachconsistentlyoutperforming
en es ru tr en es ru tr
True True
the translation-based pipeline in row (5). More-
over, the gap to the pipeline approach magnifies Figure 3: The models from rows (3) and (4) of Ta-
ble3havemorelanguage-agnosticrepresentationsand
fromhigh-tolow-resourcelanguages: Fores,ru,
strongerzero-shotperformancethanthoseinFigure2.
gu,ourQKfinetuningleadsby2.9%,15.8%,and
22.1%ROUGErespectively. Thissuggeststhatthe
two translation steps in the pipeline accumulates 4.3 ImpactofRemovingLanguageSignals
errorthatharmssummarizationquality,especially
Despiteitseffectivenesssofar,QKfinetuningalone
onlower-resourcelanguages. Comparedtotheor-
isnotsufficientincrosslingualzero-shotsettings.
acleconditionwithfullsuperviseddata(row(6)),
Thesummarizationscores7 areverylowingeneral
the strongest zero-shot scores with our approach
as a result of off-target generation. This leads to
lies2.7-4.1ROUGEbehind. Giventhedifficultyof
ournextimprovementsonlanguage-agnosticrep-
creatingsummarizationdata,thisrelativelysmall
resentations with results in Table 3 with detailed
gapshowsthepotentialofthezero-shotapproach.
scoresinAppendixE.
Comparisontomultilingualtraining: Compar-
Removinglanguagesignalsimproveszero-shot
ing the zero-shot results on Gujarati (gu), train-
performance for languages seen in finetuning:
ingmultilinguallyonen+es+ruinsteadofEnglish
Onlanguagepairswhereboththesourceandtar-
aloneclearlypreventsforgetting. Evenfullfinetun-
getareseeninfinetuning(es-en,ru-en,es-ru),our
inginrow(1)almostalwaysgeneratesthecorrect
balancedadversarialclassifierinrow(3)surpasses
targetlanguage. Yet,QKfinetuningstillsurpasses
row(2)by0.4ROUGEonaverage. Combiningit
rows (1)-(3). Moreover, its scores on gu when
withtheresidualdropbringsafurthergainof0.5
trainingonEnglishonlymatchthosewithmultilin-
ROUGE. However, these approaches are less ef-
gualtraining,suggestingitisamoredata-efficient
approachthatdoesnotrelyonmultilingualdata. 7detailsinAppendixE
detciderP
ne
se
ur
rtfectiveonlanguagesabsentinthefinetuningstage, approaches. Thiscouldforinstancebeachievedby
particularlyonnewtargetlanguages,asshownby SingularVectorCanonicalCorrelation(SVCCA)
thepoorscoresonen-trandtr-tr. Particularly,the (Raghu et al., 2017), which has been used to an-
intralingualsummarization(tr-tr)qualitydegrades alyze multilingual representations for translation
belowthebaseline. Thisshowsthat,toworkonun- (Kuduguntaetal.,2019;Liuetal.,2021;Sunetal.,
seenlanguages,language-agnosticrepresentations 2023).
mustbestrengthenedintargetlanguagegeneration.
Limitations
Balancedadversarialclassifiercreatesmorelan-
guage-agnosticrepresentations: Weprobethe Thisworkshasthefollowinglimitations:
sourcelanguageidentityonthemodelstrainedwith
SingleUnderlyingModel Alloutexperiments
our balanced adversarial classifier and its combi-
arebasedonmBART(Liuetal.,2020),specifically
nation with residual drop in Figure 3. Compare
thevariantpretrainedon25languages. Extending
toFigure2,thesetwomodels’representationsare
the current setup to mBART-50 which covers 50
clearly are language-agnostic. The results show
languagescanalreadyprovidewiderlanguagecov-
that language-agnostic representations are corre-
eragefortestingzero-shotinference. Moreover,a
latedtozero-shotcross-lingualsummarizationqual-
further exploration with other pretrained models
ityforlanguagesseeninfinetuning.
suchasmT5(Xueetal.,2021)orrecentdecoder-
only large languages models (Scao et al., 2022;
4.4 ImpactofTwo-StepFinetuning
Touvronetal.,2023)couldfurthervalidatethere-
Row (5) of Table 3 shows our two-step finetun-
sults.
ingachievesstrongzero-shotresultsforlanguages
unseeninsummarizationfinetuning. AsQKfine- RelianceonTranslationData Ourtwo-stepfine-
tuningwithoutthetranslationstepwasnotcapable tuningapproachrequiresmany-to-manytranslation
ofcross-lingualzero-shotgeneration,wehaveevi- dataforthelanguagesofinterest. Inextremelylow-
dencethatthemodelretainedknowledgefromthe resourcecases,wewouldneedtocreatesynthetic
crosslingual (translation) training. Also, the two- data by backtranslation (Sennrich et al., 2016),
stepfinetuningsurpassesthepipelineapproachon whichrequiresmorecomputationalresources.
es-ruandtr-tr,whereneitherthesourcenortarget
Lack of Multiple Experiment Runs Due to
isEnglish,therebyneedingtranslationtwice. This
computationalconstraints,thescoresinourexper-
confirmsthepreviousfindingontranslationerror
iments are reported from single experiment runs.
propagationharmingsummarizationquality.
Asapartialremedy,weusebootstrapresampling
5 Conclusion toderiveconfidenceintervalsofthereportedscores
andreporttheresultsinAppendixE.
In this work, we proposed two methods: 1) QK
finetuning and 2) balanced adversarial language Acknowledgement
classifiertoimproveintralingualandcrosslingual
We thank the anonymous reviewers for helpful
zero-shotsummarization. Wepresentedevidence
feedback. Part of this work was performed on
that language-independent representations facili-
the HoreKa supercomputer funded by the Min-
tatezero-shotsummarization,inbothintralingual
istry of Science, Research and the Arts Baden-
andcrosslingualforms.
Württemberg and by the Federal Ministry of Ed-
We are curious to see the applicability of our
ucation and Research. Part of this work was sup-
methodstoothergenerationtasks. Wearealsocu-
ported by funding from the pilot program Core-
riousaboutadditionalqualitativecomparisonsof
InformaticsoftheHelmholtzAssociation(HGF).
language-specificand-independentrepresentations.
Inthecurrentstudy, weonlyusedprobinganaly-
ses to assess language-specific versus language-
References
independent representations. One way to supple-
menttheseanalysesistodirectlyanalyzethemodel YossiAdi,EinatKermany,YonatanBelinkov,OferLavi,
and Yoav Goldberg. 2017. Fine-grained analysis
hiddenrepresentations,e.g.,comparethesimilarity
of sentence embeddings using auxiliary prediction
betweenmodelhiddenrepresentationsofdifferent
tasks. In5thInternationalConferenceonLearning
languagesbeforeandafterapplyingtheproposed Representations,ICLR2017,Toulon,France,April24-26,2017,ConferenceTrackProceedings.Open- Domain-adversarialtrainingofneuralnetworks. J.
Review.net. Mach.Learn.Res.,17:59:1–59:35.
NaveenArivazhagan,AnkurBapna,OrhanFirat,Roee Tahmid Hasan, Abhik Bhattacharjee, Md. Saiful Is-
Aharoni,MelvinJohnson,andWolfgangMacherey. lam,KaziMubasshir,Yuan-FangLi,Yong-BinKang,
2019. The missing ingredient in zero-shot neural M.SohelRahman,andRifatShahriyar.2021. XL-
machinetranslation. CoRR,abs/1903.07091. sum:Large-scalemultilingualabstractivesummariza-
tionfor44languages. InFindingsoftheAssociation
XilunChen,YuSun,BenAthiwaratkun,ClaireCardie, forComputationalLinguistics: ACL-IJCNLP2021,
andKilianWeinberger.2018. Adversarialdeepaver- pages4693–4703,Online.AssociationforComputa-
agingnetworksforcross-lingualsentimentclassifica- tionalLinguistics.
tion. TransactionsoftheAssociationforComputa-
tionalLinguistics,6:557–570. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan
Allen-Zhu,YuanzhiLi,SheanWang,LuWang,and
ZewenChi,LiDong,FuruWei,WenhuiWang,Xian- WeizhuChen.2022. Lora: Low-rankadaptationof
LingMao,andHeyanHuang.2020. Cross-lingual largelanguagemodels. InTheTenthInternational
naturallanguagegenerationviapre-training. InThe ConferenceonLearningRepresentations,ICLR2022,
Thirty-FourthAAAIConferenceonArtificialIntelli- VirtualEvent,April25-29,2022.OpenReview.net.
gence,AAAI2020,TheThirty-SecondInnovativeAp-
plicationsofArtificialIntelligenceConference,IAAI Diederik P. Kingma and Jimmy Ba. 2015. Adam: A
2020, The Tenth AAAI Symposium on Educational method for stochastic optimization. In 3rd Inter-
AdvancesinArtificialIntelligence,EAAI2020,New national Conference on Learning Representations,
York, NY, USA, February 7-12, 2020, pages 7570– ICLR 2015, San Diego, CA, USA, May 7-9, 2015,
7577.AAAIPress. ConferenceTrackProceedings.
Nadezhda Chirkova, Sheng Liang, and Vassilina Sneha Kudugunta, Ankur Bapna, Isaac Caswell, and
Nikoulina.2023. Empiricalstudyofpretrainedmul- OrhanFirat.2019. InvestigatingmultilingualNMT
tilinguallanguagemodelsforzero-shotcross-lingual representations at scale. In Proceedings of the
generation. CoRR,abs/2310.09917. 2019 Conference on Empirical Methods in Natu-
ralLanguageProcessingandthe9thInternational
Alexandra Chronopoulou, Jonas Pfeiffer, Joshua JointConferenceonNaturalLanguageProcessing
Maynez,XinyiWang,SebastianRuder,andPriyanka (EMNLP-IJCNLP),pages1565–1575,HongKong,
Agrawal.2023. Languageandtaskarithmeticwith China.AssociationforComputationalLinguistics.
parameter-efficientlayersforzero-shotsummariza-
tion. CoRR,abs/2311.09344. FaisalLadhak,EsinDurmus,ClaireCardie,andKath-
leenMcKeown.2020. WikiLingua: Anewbench-
AlexisConneau,KartikayKhandelwal,NamanGoyal, markdatasetforcross-lingualabstractivesummariza-
Vishrav Chaudhary, Guillaume Wenzek, Francisco tion. In Findings of the Association for Computa-
Guzmán, Edouard Grave, Myle Ott, Luke Zettle- tionalLinguistics: EMNLP2020,pages4034–4048,
moyer,andVeselinStoyanov.2020. Unsupervised Online.AssociationforComputationalLinguistics.
cross-lingualrepresentationlearningatscale. InPro-
ceedings of the 58th Annual Meeting of the Asso- BrianLester,RamiAl-Rfou,andNoahConstant.2021.
ciationforComputationalLinguistics,pages8440– The power of scale for parameter-efficient prompt
8451, Online. Association for Computational Lin- tuning. InProceedingsofthe2021Conferenceon
guistics. EmpiricalMethodsinNaturalLanguageProcessing,
pages3045–3059,OnlineandPuntaCana,Domini-
AlexisConneauandGuillaumeLample.2019. Cross- can Republic. Association for Computational Lin-
lingual language model pretraining. In Advances guistics.
in Neural Information Processing Systems 32: An-
nualConferenceonNeuralInformationProcessing Chen-AnLiandHung-YiLee.2024. Examiningfor-
Systems2019,NeurIPS2019,December8-14,2019, gettingincontinualpre-trainingofalignedlargelan-
Vancouver,BC,Canada,pages7057–7067. guagemodels. CoRR,abs/2401.03129.
Paul-AmbroiseDuquenne,HolgerSchwenk,andBenoît TianjianLiandKentonMurray.2023. Whydoeszero-
Sagot. 2023. SONAR: sentence-level multimodal shotcross-lingualgenerationfail?anexplanationand
and language-agnostic representations. CoRR, asolution. InFindingsoftheAssociationforCompu-
abs/2308.11466. tationalLinguistics: ACL2023,pages12461–12476,
Toronto,Canada.AssociationforComputationalLin-
RobertM.FrenchandNickChater.2002. Usingnoise guistics.
tocomputeerrorsurfacesinconnectionistnetworks:
A novel means of reducing catastrophic forgetting. XianLi,ChanghanWang,YunTang,ChauTran,Yuqing
NeuralComput.,14(7):1755–1769. Tang, Juan Pino, Alexei Baevski, Alexis Conneau,
andMichaelAuli.2021. Multilingualspeechtrans-
YaroslavGanin,EvgeniyaUstinova,HanaAjakan,Pas- lationfromefficientfinetuningofpretrainedmodels.
calGermain,HugoLarochelle,FrançoisLaviolette, InProceedingsofthe59thAnnualMeetingoftheAs-
Mario Marchand, and Victor S. Lempitsky. 2016. sociationforComputationalLinguisticsandthe11thInternationalJointConferenceonNaturalLanguage John Hoffman, Semarley Jarrett, Kaushik Ram
Processing(Volume1:LongPapers),pages827–838, Sadagopan, Dirk Rowe, Shannon Spruit, Chau
Online.AssociationforComputationalLinguistics. Tran, Pierre Andrews, Necip Fazil Ayan, Shruti
Bhosale, Sergey Edunov, Angela Fan, Cynthia
Chin-Yew Lin. 2004. ROUGE: A package for auto- Gao,VedanujGoswami,FranciscoGuzmán,Philipp
maticevaluationofsummaries. InTextSummariza- Koehn, AlexandreMourachko, ChristopheRopers,
tionBranchesOut,pages74–81,Barcelona,Spain.
SafiyyahSaleem,HolgerSchwenk,andJeffWang.
AssociationforComputationalLinguistics. 2022. No language left behind: Scaling human-
centeredmachinetranslation.
XiVictoriaLin,TodorMihaylov,MikelArtetxe,Tianlu
Wang,ShuohuiChen,DanielSimig,MyleOtt,Na-
MyleOtt,SergeyEdunov,AlexeiBaevski,AngelaFan,
manGoyal,ShrutiBhosale,JingfeiDu,Ramakanth
SamGross,NathanNg,DavidGrangier,andMichael
Pasunuru,SamShleifer,PunitSinghKoura,Vishrav
Auli. 2019. fairseq: A fast, extensible toolkit for
Chaudhary,BrianO’Horo,JeffWang,LukeZettle-
sequencemodeling. InProceedingsofthe2019Con-
moyer,ZornitsaKozareva,MonaDiab,VeselinStoy-
ferenceoftheNorthAmericanChapteroftheAssocia-
anov, and Xian Li. 2022. Few-shot learning with
tionforComputationalLinguistics(Demonstrations),
multilingualgenerativelanguagemodels. InProceed-
pages48–53,Minneapolis,Minnesota.Association
ingsofthe2022ConferenceonEmpiricalMethods
forComputationalLinguistics.
inNaturalLanguageProcessing,pages9019–9052,
AbuDhabi,UnitedArabEmirates.Associationfor JonasPfeiffer,FrancescoPiccinno,MassimoNicosia,
ComputationalLinguistics. Xinyi Wang, Machel Reid, and Sebastian Ruder.
2023. mmT5: Modular multilingual pre-training
Danni Liu, Jan Niehues, James Cross, Francisco
solvessourcelanguagehallucinations. InFindings
Guzmán, and Xian Li. 2021. Improving zero-shot
of the Association for Computational Linguistics:
translationbydisentanglingpositionalinformation.
EMNLP2023,pages1978–2008,Singapore.Associ-
In Proceedings of the 59th Annual Meeting of the
ationforComputationalLinguistics.
Association for Computational Linguistics and the
11thInternationalJointConferenceonNaturalLan-
Ngoc-Quan Pham, Jan Niehues, Thanh-Le Ha, and
guageProcessing(Volume1: LongPapers),pages
AlexanderWaibel.2019. Improvingzero-shottrans-
1259–1273,Online.AssociationforComputational
lationwithlanguage-independentconstraints. InPro-
Linguistics.
ceedingsoftheFourthConferenceonMachineTrans-
lation (Volume 1: Research Papers), pages 13–23,
YinhanLiu,JiataoGu,NamanGoyal,XianLi,Sergey
Florence,Italy.AssociationforComputationalLin-
Edunov, Marjan Ghazvininejad, Mike Lewis, and
guistics.
LukeZettlemoyer.2020. Multilingualdenoisingpre-
training for neural machine translation. Transac-
Telmo Pires, Eva Schlinger, and Dan Garrette. 2019.
tionsoftheAssociationforComputationalLinguis-
HowmultilingualismultilingualBERT? InProceed-
tics,8:726–742.
ingsofthe57thAnnualMeetingoftheAssociationfor
Computational Linguistics, pages 4996–5001, Flo-
MarcoLuiandTimothyBaldwin.2012. langid.py: An
rence,Italy.AssociationforComputationalLinguis-
off-the-shelflanguageidentificationtool. InProceed-
tics.
ingsoftheACL2012SystemDemonstrations,pages
25–30,JejuIsland,Korea.AssociationforComputa-
Maithra Raghu, Justin Gilmer, Jason Yosinski, and
tionalLinguistics.
JaschaSohl-Dickstein.2017. SVCCA:singularvec-
JonathanMallinson,RicoSennrich,andMirellaLapata. torcanonicalcorrelationanalysisfordeeplearning
2020. Zero-shotcrosslingualsentencesimplification. dynamicsandinterpretability. InAdvancesinNeural
InProceedingsofthe2020ConferenceonEmpirical InformationProcessingSystems30: AnnualConfer-
MethodsinNaturalLanguageProcessing(EMNLP), enceonNeuralInformationProcessingSystems2017,
pages5109–5126,Online.AssociationforComputa- December4-9,2017,LongBeach,CA,USA,pages
tionalLinguistics. 6076–6085.
KaushalKumarMaurya,MaunendraSankarDesarkar, SamuelRönnqvist,JennaKanerva,TapioSalakoski,and
YoshinobuKano,andKumariDeepshikha.2021. Zm- FilipGinter.2019. IsmultilingualBERTfluentinlan-
BART:Anunsupervisedcross-lingualtransferframe- guagegeneration? InProceedingsoftheFirstNLPL
work for language generation. In Findings of the WorkshoponDeepLearningforNaturalLanguage
Association for Computational Linguistics: ACL- Processing,pages29–36,Turku,Finland.Linköping
IJCNLP2021,pages2804–2818,Online.Association UniversityElectronicPress.
forComputationalLinguistics.
Teven Le Scao, Angela Fan, Christopher Akiki, El-
NLLBTeam,MartaR.Costa-jussà,JamesCross,Onur lie Pavlick, Suzana Ilic, Daniel Hesslow, Roman
Çelebi,MahaElbayad,KennethHeafield,KevinHef- Castagné,AlexandraSashaLuccioni,FrançoisYvon,
fernan, Elahe Kalbassi, Janice Lam, Daniel Licht, MatthiasGallé,JonathanTow,AlexanderM.Rush,
JeanMaillard,AnnaSun,SkylerWang,Guillaume StellaBiderman,AlbertWebson,PawanSasankaAm-
Wenzek, Al Youngblood, Bapi Akula, Loic Bar- manamanchi, ThomasWang,BenoîtSagot, Niklas
rault,GabrielMejiaGonzalez,PrangthipHansanti, Muennighoff,AlbertVillanovadelMoral,OlatunjiRuwase, Rachel Bawden, Stas Bekman, Angelina ShijieWuandMarkDredze.2019. Beto,bentz,becas:
McMillan-Major, Iz Beltagy, Huu Nguyen, Lucile Thesurprisingcross-lingualeffectivenessofBERT.
Saulnier, Samson Tan, Pedro Ortiz Suarez, Vic- InProceedingsofthe2019ConferenceonEmpirical
tor Sanh, Hugo Laurençon, Yacine Jernite, Julien MethodsinNaturalLanguageProcessingandthe9th
Launay, Margaret Mitchell, Colin Raffel, Aaron InternationalJointConferenceonNaturalLanguage
Gokaslan, Adi Simhi, Aitor Soroa, Alham Fikri Processing(EMNLP-IJCNLP),pages833–844,Hong
Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg Kong,China.AssociationforComputationalLinguis-
Nitzav,CanwenXu,ChenghaoMou,ChrisEmezue, tics.
ChristopherKlamm,ColinLeong,DanielvanStrien,
DavidIfeoluwaAdelani,andetal.2022. BLOOM: XianzeWu,ZaixiangZheng,HaoZhou,andYongYu.
A176b-parameteropen-accessmultilinguallanguage 2022. LAFT:Cross-lingualtransferfortextgener-
model. CoRR,abs/2211.05100. ationbylanguage-agnosticfinetuning. InProceed-
ingsofthe15thInternationalConferenceonNatural
Rico Sennrich, Barry Haddow, and Alexandra Birch.
Language Generation, pages 260–266, Waterville,
2016. Improvingneuralmachinetranslationmodels
Maine, USA and virtual meeting. Association for
withmonolingualdata. InProceedingsofthe54th
ComputationalLinguistics.
AnnualMeetingoftheAssociationforComputational
Linguistics(Volume1: LongPapers),pages86–96,
LintingXue,NoahConstant,AdamRoberts,MihirKale,
Berlin,Germany.AssociationforComputationalLin-
RamiAl-Rfou,AdityaSiddhant,AdityaBarua,and
guistics.
ColinRaffel.2021. mT5: Amassivelymultilingual
HaoranSun,XiaohuZhao,YikunLei,ShaolinZhu,and pre-trainedtext-to-texttransformer. InProceedings
DeyiXiong.2023. Towardsadeepunderstandingof ofthe2021ConferenceoftheNorthAmericanChap-
multilingualend-to-endspeechtranslation. InFind- teroftheAssociationforComputationalLinguistics:
ingsoftheAssociationforComputationalLinguis- HumanLanguageTechnologies,pages483–498,On-
tics: EMNLP2023,pages14332–14348,Singapore. line.AssociationforComputationalLinguistics.
AssociationforComputationalLinguistics.
BiaoZhang,PhilipWilliams,IvanTitov,andRicoSen-
YuqingTang,ChauTran,XianLi,Peng-JenChen,Na- nrich.2020a. Improvingmassivelymultilingualneu-
manGoyal,VishravChaudhary,JiataoGu,andAn- ralmachinetranslationandzero-shottranslation. In
gelaFan.2020. Multilingualtranslationwithexten- Proceedingsofthe58thAnnualMeetingoftheAsso-
siblemultilingualpretrainingandfinetuning. CoRR, ciationforComputationalLinguistics,pages1628–
abs/2008.00401. 1639,Online.AssociationforComputationalLinguis-
tics.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.
Bashlykov,SoumyaBatra,PrajjwalBhargava,Shruti
Weinberger,andYoavArtzi.2020b. Bertscore: Eval-
Bhosale,DanBikel,LukasBlecher,CristianCanton-
uating text generation with BERT. In 8th Inter-
Ferrer,MoyaChen,GuillemCucurull,DavidEsiobu,
national Conference on Learning Representations,
JudeFernandes,JeremyFu,WenyinFu,BrianFuller,
ICLR 2020, Addis Ababa, Ethiopia, April 26-30,
CynthiaGao,VedanujGoswami,NamanGoyal,An-
2020.OpenReview.net.
thonyHartshorn,SagharHosseini,RuiHou,Hakan
Inan,MarcinKardas,ViktorKerkez,MadianKhabsa,
IsabelKloumann,ArtemKorenev,PunitSinghKoura, A DetailsonProbingAnalysis
Marie-AnneLachaux,ThibautLavril,JenyaLee,Di-
anaLiskovich,YinghaiLu,YuningMao,XavierMar- Theprobingexperimentaimstoanalyzethemodel
tinet,TodorMihaylov,PushkarMishra,IgorMoly-
hidden representations regarding the information
bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-
theycontain. Hereweareinterestedinthesource
stein,RashiRungta,KalyanSaladi,AlanSchelten,
Ruan Silva, Eric Michael Smith, Ranjan Subrama- languagesignalsintheencoderoutputs. Wefreeze
nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay- a trained model on the WikiLingua dataset (Lad-
lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
haketal.,2020),andtrainatoken-levelclassifier
ZhengYan,IliyanZarov,YuchenZhang,AngelaFan,
on the encoder outputs to recover the source lan-
Melanie Kambadur, Sharan Narang, Aurélien Ro-
driguez,RobertStojnic,SergeyEdunov,andThomas guage identity, where higher accuracy indicates
Scialom.2023. Llama2: Openfoundationandfine- more language-specific representations. Specifi-
tunedchatmodels. CoRR,abs/2307.09288.
cally,weusealinearprojectionfromthehiddendi-
Tu Vu, Aditya Barua, Brian Lester, Daniel Cer, Mo- mensiontothenumberofoutputclasses,followed
hit Iyyer, and Noah Constant. 2022. Overcoming by a softmax activation. For the output classes,
catastrophicforgettinginzero-shotcross-lingualgen- weconsiderthefourlanguagesinthecrosslingual
eration. InProceedingsofthe2022Conferenceon
experiments: English, Spanish, Russian, Turkish.
EmpiricalMethodsinNaturalLanguageProcessing,
Theclassifieristrainedonthesamedataasinthe
pages9279–9300,AbuDhabi,UnitedArabEmirates.
AssociationforComputationalLinguistics. summarizationtask.B DetailsonAdversarialLoss Lang. Split #Samples Lang.pair Split #Samples
Withthestandardcross-entropy-basedadversarial Intralingual Crosslingual
en-en Train 95,517 es-en Train 76,295
classifier(Arivazhaganetal.,2019;Mallinsonetal.,
Dev 3,000 Dev 3,000
2020),theclassifieritselfistrainedtooptimize Test 27,489 Test 21,726
es-es Train 76,295 ru-en Train 35,313
(cid:88)L Dev 3,000 Dev 3,000
L classification = − y clog(p c), (2) Test 21,726 Test 9,962
ru-ru Train 35,313 es-ru Train 32,458
c=1
Dev 3,000 Dev 3,000
where L is the number of languages, y is a bi-
c Test 7,780 Test 8,737
naryindicatorbasedonwhetherthetruelanguage tr-tr Train 8,790 tr-en Train 3,052
Dev 1,139 Dev 438
label is c, and p is the predicted probability for
c Test 1,139 Test 874
languagec. Totrainthemodeltodeceivetheclas- en-tr Train 3,052
sifier, the adversarial loss is therefore the inverse Dev 438
Test 874
ofEquation2:
L Table5: DatasetstatisticsonWikiLingua. Trainingis
(cid:88)
L adversarial = − y clog(1−p c). (3) doneonintralinugadatainEnglishor{English,Spanish,
c=1 Russian}.
However,thetermwillonlybeactivatedwheny
c
is true, i.e., when the true label is c. This means
C.2 DetailsonTranslationData
whentheclassifierplacesallitsprobabilitymass
onanotherlanguagethatisnotc(hencestillhighly We use many-to-many data in all four languages
language-specific),theadversariallosswillnotup- evaluatedinthecrosslingualexperiments: English,
datethemodeltochangeitsrepresentations. This Spanish, Russian, and Turkish. To prepare the
leaves the resulting language-specific representa- translationdata,weparsetheWikiLinguadataset
tionsunresolved. bymatchingcommonintputsoroutputsofdifferent
languagepairs. Weiterateoversamplesindiffer-
C DatasetStatistics
ent language pairs and match samples that have
thesameinputtextoroutputsummaryinthesame
C.1 DatasetSplits
language,butthecorrespondingoutputsummary
Fortheintralingualexperiments,wetrainonXL-
orinputtextispresentedindifferentlanguages. By
Sum(Hasanetal.,2021). Table4showsthedataset
performingsuchmatching,wegeneratetranslation
statistics. For the crosslingual experiments, we
datainthesamedomainasusedforsummarization.
Atranslationmodeltrainedwithsuchdataiscapa-
Split #SamplesAvg.inputleng.Avg.outputleng.
bleoftranslatingbothshortandlongsequences.
English Train 302,627 459.9 22.3
Dev 11,535 440.4 21.2
D TrainingandInferenceDetails
Test 11,535 437.3 21.2
SpanishTrain 35,633 723.5 29.4
Dev 4,763 766.5 27.4 WeimplementourapproachesintheFAIRSEQ(Ott
Test 4,763 764.8 27.4 etal.,2019)toolkit.
RussianTrain 60,044 564.0 26.1
Dev 7,780 466.3 24.2
Training We initialized from the pretrained
Test 7,780 465.3 24.2
GujaratiTrain 8,790 769.1 24.0 mBARTmodel8 (Liuetal.,2020). Thewordem-
Dev 1,139 542.6 21.2 beddingsarefrozenduetoinitialfavourableresults
Test 1,139 529.9 21.7
inzero-shotsettings. WeusetheAdamoptimizer
(KingmaandBa,2015)withbetas(0.9,0.999)and
Table4: DatasetstatisticsonXL-Sum. Trainingisdone
eps1e-8. Weuseweightdecayof0.01,startlearn-
onEnglishor{English,Spanish,Russian}.
ingrateof2e-5andendendlearningrateof5e-9.
trainonWikiLingua(Ladhaketal.,2020). Table5 Dropoutissetto0.1. Weusethedevelopmentset
showsthedatasetstatistics. Forbothdatasets,in ofthesamelanguagesasintrainingforearlystop-
trainingweexcludesamplesthathaveveryshortin- ping. All models are trained on an Nvidia Titan
puts(nomorethan20wordsorpunctuationmarks)
8We use the 610M mbart.CC25 model from https:
orsummaries(nomorethan10wordsorpunctua-
//github.com/facebookresearch/fairseq/blob/main/
tionmarks),astheylikelyhavedataqualityissues. examples/mbart/README.md#pre-trained-models.ID Model/Language RG-1 RG-2 RG-L F L-Acc.
BERT
es-es(en-onlytrain)
(1) Fullft. 6.7/6.8/6.9 1.0/1.0/1.1 5.3/5.4/5.5 65.9/66.0/66.1 0.2
(2) Encoderft. 24.4/24.8/25.1 6.6/6.9/7.1 18.1/18.4/18.6 70.6/70.8/70.9 85.2
(3) “LNA”ft. 28.1/28.4/28.8 8.0/8.2/8.5 20.6/20.9/21.1 71.8/71.9/72.1 99.5
(4) Query-keyft. 28.3/28.6/29.0 8.6/8.8/9.1 21.1/21.3/21.7 72.1/72.3/72.4 99.9
(5) Pipeline 27.8/28.1/28.5 8.0/8.2/8.5 20.4/20.7/21.0 72.0/72.1/72.3 100.0
(6) Supervised 32.4/32.8/33.3 12.1/12.5/12.9 24.6/25.0/25.4 73.8/74.0/74.2 100.0
ru-ru(en-onlytrain)
(1) Fullft. 1.0/1.0/1.1 0.2/0.2/0.3 0.9/1.0/1.0 64.2/64.3/64.4 2.3
(2) Encoderft. 28.0/28.3/28.6 10.2/10.4/10.6 22.4/22.7/22.9 73.1/73.2/73.3 100.0
(3) “LNA”ft. 26.9/27.3/27.6 9.4/9.6/9.8 21.3/21.6/21.8 72.5/72.7/72.8 100.0
(4) Query-keyft. 28.8/29.2/29.5 10.9/11.1/11.4 23.1/23.4/23.6 73.4/73.6/73.7 100.0
(5) Pipeline 25.0/25.3/25.6 7.9/8.1/8.3 20.0/20.2/20.5 72.3/72.4/72.5 100.0
(6) Supervised 33.8/34.1/34.5 14.5/14.7/15.0 27.2/27.5/27.8 74.9/75.1/75.2 100.0
gu-gu(en-onlytrain)
(1) Fullft. 1.2/1.3/1.5 0.2/0.3/0.3 1.1/1.2/1.4 59.0/59.1/59.3 13.4
(2) Encoderft. 15.1/15.8/16.6 4.3/4.8/5.3 13.8/14.5/15.2 71.4/71.7/72.0 100.0
(3) “LNA”ft. 11.2/11.7/12.4 2.8/3.2/3.6 9.9/10.5/11.1 68.3/68.6/68.9 99.8
(4) Query-keyft. 17.5/18.3/19.1 5.2/5.8/6.4 15.9/16.6/17.3 72.9/73.2/73.6 100.0
(5) Pipeline 14.6/15.2/15.7 2.8/3.2/3.5 13.0/13.6/14.1 71.9/72.1/72.4 100.0
(6) Supervised 20.8/21.5/22.4 7.1/7.7/8.4 18.5/19.3/20.1 73.9/74.2/74.5 100.0
gu-gu(multi.train)
(1) Fullft. 16.1/16.9/17.6 4.3/4.9/5.4 14.3/15.1/15.8 71.5/71.8/72.0 99.6
(2) Encoderft. 16.0/16.8/17.5 4.4/4.9/5.4 14.6/15.3/15.9 71.9/72.2/72.5 100.0
(3) “LNA”ft. 17.1/17.8/18.5 5.1/5.6/6.1 15.3/16.0/16.8 72.3/72.6/72.8 100.0
(4) Query-keyft. 17.4/18.2/19.0 5.6/6.2/6.8 15.7/16.5/17.3 72.8/73.1/73.4 100.0
(5) Pipeline 14.6/15.2/15.7 2.8/3.2/3.5 13.0/13.6/14.1 71.9/72.1/72.4 100.0
(6) Supervised 20.8/21.5/22.4 7.1/7.7/8.4 18.5/19.3/20.1 73.9/74.2/74.5 100.0
Table6: Fullzero-shotintralingualsummarizationresultsonXL-Sumcalculatedusing95%bootstrapconfidence
intervals(resultsarepresentedas0.025/0.5/0.975percentiles).
es-en ru-en es-ru tr-en en-tr E DetailedExperimentScores
ROUGE-L 2.2 0.7 0.5 2.3 2.5
Detailed Intralingual Results The detailed re-
L-Acc. 0.0 0.0 0.0 0.0 0.0
sultsforTable2withROUGE-1andROUGE-2are
Table7: ResultsofQKfinetuningalone(withouttwo- inTable6withRGstandingforROUGE.
stepfinetuning)underthecrosslingualzero-shotsetup.
DetailedCrosslingualResults Thedetailedre-
sultsforTable3areinTable8.
QK Finetuning in Crosslingual Settings QK
RTXGPUwith24GBmemory.
finetuning alone is not sufficient in crosslingual
zero-shotsettings. ThescoresareinTable7.
Inference Whendecoding,weuseabeamsizeof
5. Thelengthpenaltyis0.6and1.0forintralingual
andcrosslingualexperimentsrespectively. Forthe
translationmodelinthepipelineapproach,weuse
thedistilledNLLB-200model(NLLBTeametal.,
2022)with600Mparameters.
Evaluation OnXL-Sum,wefollowtheoriginal
datasetpaper(Hasanetal.,2021)andusetheMul-
tilingualROUGEScoringfromhere.ID Model/Language RG-1 RG-2 RG-L F L-Acc.
BERT
es-en
(1) Baselinezero-shot 2.3/2.4/2.4 0.1/0.1/0.1 2.1/2.2/2.2 67.8/67.8/67.9 0.0
(2) Adv.classifier 33.2/33.4/33.6 10.4/10.5/10.6 26.5/26.7/26.8 76.1/76.1/76.2 98.5
(3) Balancedadv.classifier 33.9/34.1/34.3 10.8/11.0/11.1 27.1/27.2/27.4 76.3/76.4/76.4 99.4
(4) (3)+residualdrop 34.6/34.8/34.9 11.2/11.3/11.5 27.5/27.6/27.8 76.5/76.6/76.6 99.7
(5) Two-step+QKft. 34.2/34.4/34.5 11.3/11.5/11.7 27.5/27.7/27.9 76.4/76.5/76.6 98.4
(6) Pipeline 37.7/37.9/38.1 14.3/14.5/14.6 31.0/31.1/31.3 78.1/78.1/78.2 99.7
(7) Supervised 38.2/38.4/38.6 14.6/14.8/14.9 31.2/31.4/31.6 78.1/78.1/78.2 99.8
ru-en
(1) Baselinezero-shot 0.7/0.7/0.7 0.1/0.1/0.1 0.6/0.7/0.7 63.3/63.3/63.3 0.0
(2) Adv.classifier 31.6/31.9/32.2 9.7/9.9/10.1 25.1/25.3/25.5 75.6/75.7/75.7 99.6
(3) Balancedadv.classifier 31.9/32.2/32.5 10.0/10.2/10.4 25.4/25.6/25.9 75.7/75.8/75.9 99.8
(4) (3)+residualdrop 32.9/33.1/33.4 10.4/10.6/10.8 26.0/26.3/26.5 76.0/76.1/76.2 99.9
(5) Two-step+QKft. 32.6/32.8/32.9 10.6/10.7/10.8 26.1/26.3/26.4 76.0/76.1/76.3 99.6
(6) Pipeline 34.4/34.6/34.9 12.0/12.2/12.4 28.2/28.5/28.7 77.2/77.3/77.4 99.7
(7) Supervised 35.7/36.0/36.3 12.9/13.2/13.4 29.2/29.4/29.7 77.4/77.5/77.5 99.7
es-ru
(1) Baselinezero-shot 0.5/0.6/0.6 0.1/0.1/0.1 0.5/0.6/0.6 64.6/64.6/64.7 0.0
(2) Adv.classifier 16.6/16.9/17.1 3.9/4.1/4.2 13.8/14.1/14.3 72.4/72.5/72.6 97.6
(3) Balancedadv.classifier 17.1/17.3/17.5 4.1/4.3/4.4 14.1/14.3/14.5 72.7/72.8/72.9 99.9
(4) (3)+residualdrop 17.6/17.8/18.0 4.3/4.5/4.6 14.5/14.8/15.0 73.0/73.1/73.2 100.0
(5) Two-step+QKft. 17.4/17.6/17.8 4.5/4.6/4.6 14.7/14.8/14.9 73.2/73.4/73.6 98.4
(6) Pipeline 16.4/16.7/16.9 3.5/3.7/3.8 14.2/14.4/14.6 73.7/73.8/73.8 100.0
(7) Supervised 20.8/21.0/21.3 6.0/6.1/6.3 17.7/18.0/18.2 75.1/75.2/75.3 100.0
tr-en
(1) Baselinezero-shot 4.4/5.0/5.6 0.9/1.1/1.4 4.1/4.6/5.1 62.6/62.9/63.1 1.6
(2) Adv.classifier 31.0/32.0/33.0 10.0/10.7/11.4 25.3/26.1/26.9 74.9/75.2/75.5 98.9
(3) Balancedadv.classifier 31.7/32.6/33.6 10.3/11.0/11.7 25.7/26.6/27.4 75.2/75.5/75.8 99.8
(4) (3)+residualdrop 31.2/32.1/33.0 9.7/10.3/11.0 24.9/25.7/26.5 74.9/75.2/75.5 99.1
(5) Two-step+QKft. 38.3/38.6/38.8 14.4/14.6/14.7 30.5/30.7/30.9 77.1/77.4/77.6 99.7
(6) Pipeline 39.9/40.9/41.8 16.1/17.0/17.8 33.2/34.1/35.0 78.4/78.7/79.0 99.4
(7) Supervised 40.4/41.4/42.5 17.1/18.1/19.1 33.5/34.5/35.5 78.5/78.8/79.1 99.4
en-tr
(1) Baselinezero-shot 2.4/2.7/3.0 0.4/0.5/0.5 2.3/2.5/2.7 60.8/60.9/61.1 0.0
(2) Adv.classifier 2.5/2.8/3.0 0.4/0.5/0.6 2.3/2.5/2.8 60.7/60.9/61.0 0.0
(3) Balancedadv.classifier 2.5/2.8/3.1 0.4/0.5/0.6 2.4/2.6/2.8 60.8/60.9/61.1 0.0
(4) (3)+residualdrop 2.4/2.7/3.0 0.4/0.5/0.6 2.3/2.5/2.7 60.8/61.0/61.1 0.0
(5) Two-step+QKft. 20.2/20.4/20.6 5.7/5.8/5.8 16.5/16.7/16.9 71.0/71.3/71.5 98.7
(6) Pipeline 20.1/20.9/21.6 5.1/5.5/6.0 18.0/18.7/19.4 72.8/73.1/73.4 99.8
(7) Supervised 22.7/23.7/24.8 7.4/8.0/8.7 19.7/20.7/21.5 72.9/73.2/73.5 100.0
tr-tr
(1) Baselinezero-shot 20.0/20.9/21.7 5.5/6.0/6.5 17.4/18.0/18.8 71.2/71.5/71.8 96.4
(2) Adv.classifier 5.3/5.7/6.2 1.0/1.1/1.3 4.7/5.2/5.6 62.6/62.8/63.0 10.6
(3) Balancedadv.classifier 3.2/3.5/3.8 0.5/0.5/0.7 3.0/3.2/3.5 60.9/61.1/61.3 0.1
(4) (3)+residualdrop 2.2/2.5/2.7 0.3/0.4/0.5 2.1/2.3/2.5 60.6/60.8/60.9 0.0
(5) Two-step+QKft. 22.9/23.1/23.3 6.9/7.0/7.1 18.2/18.4/18.6 71.7/72.0/72.2 100.0
(6) Pipeline 19.9/20.7/21.5 5.4/5.8/6.3 17.7/18.5/19.2 73.0/73.2/73.5 99.8
(7) Supervised 29.2/30.3/31.3 11.4/12.3/13.0 25.3/26.2/27.2 75.2/75.4/75.7 99.7
Table8: Fullzero-shotcrosslingualsummarizationresultsonWikiLinguacalculatedusing95%bootstrapconfidence
intervals(resultsarepresentedas0.025/0.5/0.975percentiles).