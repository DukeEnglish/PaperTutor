Humanoid-Gym: Reinforcement Learning for Humanoid Robot with
Zero-Shot Sim2Real Transfer
Xinyang Gu2∗, Yen-Jen Wang13∗, Jianyu Chen123
Abstract—Humanoid-Gym is an easy-to-use reinforcement
learning (RL) framework based on Nvidia Isaac Gym, de-
signed to train locomotion skills for humanoid robots, em-
phasizing zero-shot transfer from simulation to the real-
world environment. Humanoid-Gym also integrates a sim-to-
sim framework from Isaac Gym to Mujoco that allows users
to verify the trained policies in different physical simulations
to ensure the robustness and generalization of the policies.
This framework is verified by RobotEra’s XBot-S (1.2-meter
tall humanoid robot) and XBot-L (1.65-meter tall humanoid
robot) in a real-world environment with zero-shot sim-to-real
transfer. The project website and source code can be found at:
sites.google.com/view/humanoid-gym.
(a) Different Physical Environments
I. INTRODUCTION
Modern environments are primarily designed for humans.
Therefore, humanoid robots, with their human-like skeletal
structure, are especially suited for tasks in human-centric
environments, offering unique advantages over other types
of robots. Recently, massively parallel deep reinforcement
learning(RL)insimulationhasbecomepopular[1],[2],[3].
However, due to the complex structure of humanoid robots,
the sim-to-real gap [4], [5], [6], [7] exists and is larger than
(b) Zero-Shot Sim-to-Real Transfer
thatofquadrupedalrobots.Therefore,wereleaseHumanoid-
Gym, an easy-to-use RL framework based on Nvidia Isaac
Fig. 1: Humanoid-Gym enables users to train their policies
Gym [8], designed to train locomotion skills for humanoid
withinNvidiaIsaacGymandvalidatetheminMuJoCo.Ad-
robots,emphasizingzero-shottransferfromsimulationtothe
ditionally, we have successfully tested the complete pipeline
real-worldenvironment.Humanoid-Gymfeaturesspecialized
with two humanoid robots. They were trained in Humanoid-
rewardsanddomainrandomizationtechniquesforhumanoid
Gym and transferred to real-world environments in a zero-
robots, simplifying the difficulty of sim-to-real transfer. Fur-
shot manner.
thermore, it also integrates a sim-to-sim framework from
Isaac Gym [8] to MuJoCo [9] that allows users to verify the
trainedpoliciesindifferentphysicalsimulationstoensurethe
robustnessandgeneralizationofthepolicies,showninFig.1. • Ouropen-sourcelibraryfeaturesasim-to-simvalidation
tool, enabling users to test their policies across diverse
Currently, Humanoid-Gym is verified by multiple humanoid
environmental dynamics rigorously.
robots with different sizes in a real-world environment with
zero-shot sim-to-real transfer, including RobotEra’s XBot-S
II. RELATEDWORKS
(1.2-meter tall humanoid robot) and XBot-L (1.65-meter tall
humanoid robot) [10]. The contribution of Humanoid-Gym A. Robot Learning on Locomotion Tasks
can be summarized as follows:
Reinforcement learning (RL) has shown promise in en-
• We launch an open-source reinforcement learning (RL) abling robots to achieve stable locomotion [6], [11], [12].
framework with meticulous system design. Compared to prior RL efforts with quadrupedal robots [1],
• Our framework enables zero-shot transfer from simula- [13] and bipedal robots like Cassie [14], [15], our work
tion to the real world, which has been rigorously tested withhumanoidrobotsintroducesamorechallengingscenario
across humanoid robots of various sizes. for robot control. Recent studies [16], [17] have applied
transformerarchitecturetoimprovethewalkingperformance
∗Equalcontribution.Listedalphabetically.
of humanoid robots on flat surfaces. Beyond lower-body
1ShanghaiQiZhiInstitute,Shanghai,China.
2RobotEraTECHNOLOGYCO.,LTD.,Beijing,China control,someworks[18],[19]havealsoexploredmorecom-
3TsinghuaUniversity,Beijing,China. plexupper-bodyskillsforhumanoidrobotcontrol.However,
4202
rpA
8
]OR.sc[
1v59650.4042:viXraL =∥R −V(s )∥ , (2)
v t t 2
B. System design
The base poses of the robot, denoted as Pb, are six-
dimensional vectors [x,y,z,α,β,γ], representing both the
positioncoordinatesx,y,z andtheorientationanglesα,β,γ
in Euler notation. The joint position for each motor is
represented by θ, and the corresponding joint velocity by θ˙.
Furthermore, we define a gait phase [24], [25], which com-
prisestwodoublesupportphases(DS)andtwosinglesupport
phases (SS) within each gait cycle. The cycle time, denoted
Fig. 2: Pipeline of Humanoid-Gym. Initially, we employ as C , is the duration of one full gait cycle. A sinusoidal
T
massively parallel deep reinforcement learning (RL) within wave is employed to generate reference motion, reflecting
NvidiaIsaacGym,incorporatingdiverseterrainsanddynam- the repetitive nature of the gait cycle involving pitch, knee,
ics randomization. Subsequently, we undertake sim-to-sim and ankle movements. Notably, we also designed a periodic
transfertotestpolicies.Duetoourmeticulouscalibration,the stancemaskI (t)(Fig6)thatindicatesfootcontactpatterns
p
performance in both MuJoCo and real-world settings aligns insynchronizationwiththereferencemotion.Forinstance,if
closely. the reference motion lifts the left foot, the right foot should
be in the single support phase, with the foot contact mask
indicated as [0,1]; during DS phases, it would be [1,1].
the sim-to-real transition for humanoid locomotion remains The chosen action is the target joint position for the
a significant challenge, with a notable lack of open-source Proportional-Derivative (PD) controller. The policy network
resources in the robot learning community. To contribute to integrates proprioceptive sensor data, a periodic clock sig-
this area, we have developed Humanoid-Gym, an accessible nal [sin(2πt/C ),cos(2πt/C )], and velocity commands
T T
framework with full codebase. P˙ . A single frame of input are elaborated in Table I.
x,y,γ
Additionally, the state frame includes feet contact detect
III. METHOD
I (t) and other privileged observations.
d
The workflow of Humanoid-Gym is illustrated in Fig. 2.
Inthissection,wewillintroducetheproblemsetting,system TABLE I: Summary of Observation Space. The table cate-
design, and reward design of our Humanoid-Gym. gorizes the components of the observation space into obser-
vation and state. The table also details their dimensions.
A. Reinforcement Learning For Robot Control
Components Dims Observation State
Our approach employs a reinforcement learning model ClockInput(sin(t),cos(t)) 2 ✓ ✓
M = ⟨S,A,T,O,R,γ⟩, with S and A denoting state and Commands(P˙ x,y,γ) 3 ✓ ✓
action spaces, T(s′|s,a) the transition dynamics, R(s,a) JointPosition(θ) 12 ✓ ✓
the reward function, γ ∈ [0,1] the discount factor, and JointVelocity(θ˙) 12 ✓ ✓
AngularVelocity(P˙b ) 3 ✓ ✓
O the observation space. The framework is designed for αβγ
EulerAngle(Pb ) 3 ✓ ✓
bothsimulatedandreal-worldsettings,transitioningfromfull LastActions(aα t−β 1) 12 ✓ ✓
observability in simulations (s ∈ S) to partial observability Frictions 1 ✓
intherealworld(o∈O).Thisnecessitatesoperatingwithin BodyMass 1 ✓
BaseLinearVelocity 3 ✓
a Partially Observable Markov Decision Process (POMDP)
PushForce 2 ✓
[20], with the policy π(a|o ≤t) mapping observations to PushTorques 3 ✓
action distributions to maximize the expected return J = TrackingDifference 12 ✓
E[R ]=E[(cid:80) γtr ]. PeriodicStanceMask 2 ✓
t t t FeetContactdetection 2 ✓
We leverage Proximal Policy Optimization (PPO) [21]
loss, supplemented by the Asymmetric Actor Critic [22]
Ourcontrolpolicyoperatesatahighfrequencyof100Hz,
method and the integration of privileged information during
providing enhanced granularity and precision beyond stan-
training, shifting to partial observations during deployment.
dard RL locomotion approaches. The internal PD controller
The policy loss is defined as:
runs at an even higher frequency of 1000Hz. For training
(cid:20) π(a |o ) simulations,IsaacGymisutilized[8],whileMuJoCo,known
L
π
=min
π
(at |o≤t )Aπb(o ≤t,a t),
foritsaccuratephysicaldynamics,ischosenforsim2simval-
b t ≤t
(cid:18) (cid:19) (cid:21) (1) idation. This approach combines the benefits of high-speed
π(a |o )
clip π (at |o≤t ),c 1,c 2 Aπb(o ≤t,a t) GPU-basedparallelsimulation,albeitwithlessaccuracy,and
b t ≤t the high accuracy but slower CPU-based simulation.
Advantage estimation utilizes Generalized Advantage Es- The detailed settings for both algorithms and the envi-
timation (GAE) [23], requiring an updated value function: ronment designed are shown in Appendix TABLE II. Weuse multi-frames of observations and privilege observation,
which is crucial for locomotion tasks on uneven terrain.
C. Reward Design
Ourrewardfunctiondirectstherobottoadheretovelocity
commands,sustainastablegait,andachievesmoothcontact.
The reward function is structured into four key components:
(1) velocity tracking, (2) gait reward, and (3) regularization
terms.
The reward function is summarized in Appendix Table
IV. It is important to note that the commands CMD
z,γ,β
(velocitymismatchterm)areintentionallysettozero.Thisis
because we do not control them; rather, we aim to maintain
their values at zero to ensure stable and smooth walking.
In addition, the reward (contact pattern) encourages feet to
align with their contact masks, denoting swing, and stance
phases,asillustratedinAppendixFig.6.Therefore,thetotal
Fig. 3: Sine wave in Both MuJoCo and real-world environ-
rewardatanytimesteptiscomputedastheweightedsumof
ment.Itcanbefoundthatthetrajectoriesofthetwoarevery
(cid:80)
individual reward components, expressed as r = r ·µ ,
t i i i close after calibration.
where µ represents the weighting factor for each reward
i
component r .
i
IV. EXPERIMENTS
In this section, we will illustrate the result of zero-
shot transfer for both sim-to-sim and sim-to-real scenarios.
Additionally,wealsoprovidevisualizationofthecalibration
forMuJoCotoverifytheeffectivenessofsim-to-sim.Forthe
validation,weutilizedRobotEra’shumanoidrobots,XBot-S
andXBot-L,measuring1.2metersand1.65metersinheight,
shown in Appendix Fig. 5, respectively.
Fig.4:PhasePortraitforMuJoCo,Real-WorldEnvironment,
A. Zero-shot Transfer and Isaac Gym.
We carefully design domain randomization terms, as de-
tailed in Appendix TABLE III, to minimize the sim2real
gap, following the approach outlined in [26]. Our agents V. CONCLUSIONS
are capable of transitioning to real-world environments via Humanoid-Gymfacilitateszero-shottransferforhumanoid
zero-shot sim-to-real transfer, which is illustrated in Fig. 1. robots of two distinct sizes, from sim-to-sim and sim-to-
The standard procedure involves training agents on GPUs, real, via a specialized reward function tailored for humanoid
followedbypolicyanalysisinMuJoCo.Foracomprehensive robotics.Ourexperimentaloutcomesrevealthattheadjusted
evaluation, we developed two types of terrains: flat and MuJoCo simulation closely mirrors the dynamics and per-
uneven,asdepictedinAppendixFig.7.Theflatterrainrepli- formance of the real-world environment. This congruence
cates the environment encountered during training in Isaac enables researchers lacking physical robots to validate train-
Gym, while the uneven terrain offers a substantially more ing policies through sim-to-sim, significantly enhancing the
challenginglandscape,differingsignificantlyfromourinitial potential for successful sim-to-real transfers.
training scenarios. Remarkably, our trained policies enable
ACKNOWLEDGMENT
the robots to traverse both types of terrain successfully.
The implementation of Humanoid-Gym relies on re-
B. Calibration for MuJoCo
sources from legged gym and rsl rl projects[1] created by
We meticulously calibrated the MuJoCo environment to theRoboticSystemsLab.Wespecificallyutilizethe‘Legge-
align its dynamics and performance more closely with that dRobot‘ implementation from their research to enhance our
of the real world. By comparing the leg swing sine waves codebase.
generated in both MuJoCo and the real-world environment,
we observed nearly identical trajectories, as depicted in
Fig. 3. Furthermore, we also compare the resulting phase
portrait of the left knee joint and left ankle pitch joint[17]
within 5-second trajectories, as shown in Fig. 4. It is clear
to see that the dynamics in MuJoCo are closer to the real
environment than Isaac Gym.REFERENCES [21] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov,
“Proximal policy optimization algorithms,” arXiv preprint
arXiv:1707.06347,2017.
[1] N. Rudin, D. Hoeller, P. Reist, and M. Hutter, “Learning to walk
[22] L.Pinto,M.Andrychowicz,P.Welinder,W.Zaremba,andP.Abbeel,
in minutes using massively parallel deep reinforcement learning,” in
“Asymmetric actor critic for image-based robot learning,” arXiv
ConferenceonRobotLearning. PMLR,2022,pp.91–100.
preprintarXiv:1710.06542,2017.
[2] A. Kumar, Z. Fu, D. Pathak, and J. Malik, “Rma: Rapid motor [23] J.Schulman,P.Moritz,S.Levine,M.Jordan,andP.Abbeel,“High-
adaptationforleggedrobots,”arXivpreprintarXiv:2107.04034,2021. dimensional continuous control using generalized advantage estima-
[3] Y. Guo, Z. Jiang, Y.-J. Wang, J. Gao, and J. Chen, “Decentralized tion,”arXivpreprintarXiv:1506.02438,2015.
motorskilllearningforcomplexroboticsystems,”IEEERoboticsand [24] J.Siekmann,Y.Godse,A.Fern,andJ.Hurst,“Sim-to-reallearningof
AutomationLetters,2023. all common bipedal gaits via periodic reward composition,” in 2021
[4] X.B.Peng,M.Andrychowicz,W.Zaremba,andP.Abbeel,“Sim-to- IEEEInternationalConferenceonRoboticsandAutomation(ICRA).
realtransferofroboticcontrolwithdynamicsrandomization,”in2018 IEEE,2021,pp.7309–7315.
IEEE international conference on robotics and automation (ICRA). [25] Y. Yang, T. Zhang, E. Coumans, J. Tan, and B. Boots, “Fast and
IEEE,2018,pp.3803–3810. efficient locomotion via learned gait transitions,” in Conference on
[5] W. Zhao, J. P. Queralta, and T. Westerlund, “Sim-to-real transfer in robotlearning. PMLR,2022,pp.773–783.
deep reinforcement learning for robotics: a survey,” in 2020 IEEE [26] J.Tobin,R.Fong,A.Ray,J.Schneider,W.Zaremba,andP.Abbeel,
symposiumseriesoncomputationalintelligence(SSCI). IEEE,2020, “Domain randomization for transferring deep neural networks from
pp.737–744. simulation to the real world,” in 2017 IEEE/RSJ international con-
[6] J. Tan, T. Zhang, E. Coumans, A. Iscen, Y. Bai, D. Hafner, S. Bo- ference on intelligent robots and systems (IROS). IEEE, 2017, pp.
hez, and V. Vanhoucke, “Sim-to-real: Learning agile locomotion for 23–30.
quadrupedrobots,”arXivpreprintarXiv:1804.10332,2018.
[7] A. Kadian, J. Truong, A. Gokaslan, A. Clegg, E. Wijmans, S. Lee,
M. Savva, S. Chernova, and D. Batra, “Sim2real predictivity: Does
evaluation in simulation predict real-world performance?” IEEE
RoboticsandAutomationLetters,vol.5,no.4,pp.6670–6677,2020.
[8] V.Makoviychuk,L.Wawrzyniak,Y.Guo,M.Lu,K.Storey,M.Mack-
lin,D.Hoeller,N.Rudin,A.Allshire,A.Handa,etal.,“Isaacgym:
High performance gpu-based physics simulation for robot learning,”
arXivpreprintarXiv:2108.10470,2021.
[9] E. Todorov, T. Erez, and Y. Tassa, “Mujoco: A physics engine for
model-basedcontrol,”in2012IEEE/RSJinternationalconferenceon
intelligentrobotsandsystems. IEEE,2012,pp.5026–5033.
[10] RobotEra, “Robotera technology co.,ltd.” [Online]. Available: https:
//www.robotera.com/
[11] J. Hwangbo, J. Lee, A. Dosovitskiy, D. Bellicoso, V. Tsounis,
V. Koltun, and M. Hutter, “Learning agile and dynamic motor skills
forleggedrobots,”ScienceRobotics,vol.4,no.26,p.eaau5872,2019.
[12] J. Lee, J. Hwangbo, L. Wellhausen, V. Koltun, and M. Hutter,
“Learningquadrupedallocomotionoverchallengingterrain,”Science
robotics,vol.5,no.47,p.eabc5986,2020.
[13] G.B.Margolis,G.Yang,K.Paigwar,T.Chen,andP.Agrawal,“Rapid
locomotionviareinforcementlearning,”TheInternationalJournalof
RoboticsResearch,p.02783649231224053,2022.
[14] Z. Li, X. Cheng, X. B. Peng, P. Abbeel, S. Levine, G. Berseth,
and K. Sreenath, “Reinforcement learning for robust parameterized
locomotion control of bipedal robots,” in 2021 IEEE International
Conference on Robotics and Automation (ICRA). IEEE, 2021, pp.
2811–2817.
[15] A.Kumar,Z.Li,J.Zeng,D.Pathak,K.Sreenath,andJ.Malik,“Adapt-
ing rapid motor adaptation for bipedal robots,” in 2022 IEEE/RSJ
International Conference on Intelligent Robots and Systems (IROS).
IEEE,2022,pp.1161–1168.
[16] I. Radosavovic, T. Xiao, B. Zhang, T. Darrell, J. Malik, and
K. Sreenath, “Learning humanoid locomotion with transformers,”
arXivpreprintarXiv:2303.03381,2023.
[17] I.Radosavovic,B.Zhang,B.Shi,J.Rajasegaran,S.Kamat,T.Darrell,
K. Sreenath, and J. Malik, “Humanoid locomotion as next token
prediction,”arXivpreprintarXiv:2402.19469,2024.
[18] T. He, Z. Luo, W. Xiao, C. Zhang, K. Kitani, C. Liu, and G. Shi,
“Learning human-to-humanoid real-time whole-body teleoperation,”
arXivpreprintarXiv:2403.04436,2024.
[19] X. Cheng, Y. Ji, J. Chen, R. Yang, G. Yang, and X. Wang, “Ex-
pressive whole-body control for humanoid robots,” arXiv preprint
arXiv:2402.16796,2024.
[20] M. T. Spaan, “Partially observable markov decision processes,” in
Reinforcement learning: State-of-the-art. Springer, 2012, pp. 387–
414.APPENDIX
Flat Plane Uneven Terrains
Fig. 7: Terrains in MuJoCo. Humanoid-Gym provides two
typesofterrainsutilizedforsim-to-simvalidation:flatplanes
and uneven terrains.
Fig. 5: Hardware Platform. Our Humanoid-Gym framework TABLE II: Hyperparameters.
is tested on two distinct sizes of humanoid robots, XBot-S
Parameter Value
and XBot-L, provided by Robot Era.
NumberofEnvironments 8192
NumberTrainingEpochs 2
Batchsize 8192×24
EpisodeLength 2400steps
DiscountFactor 0.994
GAEdiscountfactor 0.95
EntropyRegularizationCoefficient 0.001
c1 0.8
c2 1.2
Learningrate 1e-5
FrameStackofSingleObservation 15
FrameStackofSinglePrivilegedObservation 3
NumberofSingleObservation 47
NumberofSinglePrivilegedObservation 73
TABLE III: Overview of Domain Randomization. Presented
are the domain randomization terms and the associated
Fig. 6: The stance mask is the contact planning for the left parameter ranges. Additive randomization increments the
(L)andright(R)feet,where0indicatestheswingphaseand parameterbyavaluewithinthespecifiedrangewhilescaling
1 indicates the stance phase is expected. randomization adjusts it by a multiplicative factor from the
same range.
Parameter Unit Range Operator Type
JointPosition rad [-0.05,0.05] additive Gaussian(1σ)
JointVelocity rad/s [-0.5,0.5] additive Gaussian(1σ)
AngularVelocity rad/s [-0.1,0.1] additive Gaussian(1σ)
EulerAngle rad [-0.03,0.03] additive Gaussian(1σ)
SystemDelay ms [0,10] - Uniform
Friction - [0.1,2.0] - Uniform
MotorStrength % [95,105] scaling Gaussian(1σ)
Payload kg [-5,5] additive Gaussian(1σ)TABLE IV: In defining the reward function, we use a
tracking error metric denoted by ϕ(e,w). This metric is
expressed as ϕ(e,w):=exp(−w·∥e∥2), where e represents
thetrackingerror,andw istheassociatedweight.Thetarget
base height is set to 0.7m.
Reward Equation(ri) rewardscale(µi)
Lin.velocitytracking ϕ(P˙ xb yz−CMDxyz,5) 1.2
Ang.velocitytracking ϕ(P˙ αb βγ−CMDαβγ,5) 1.0
Orientationtracking ϕ(Pb ,5) 1.0
αβ
Baseheighttracking ϕ(Pb−0.7,100) 0.5
z
Velocitymismatch ϕ(P˙ zb ,γ,β−CMDz,γ,β,5) 0.5
ContactPattern ϕ(Ip(t)−I d(t),∞) 1.0
JointPositionTracking ϕ(θ−θtarget,2) 1.5
DefaultJoint ϕ(θt−θ0,2) 0.2
EnergyCost |τ||θ˙| -0.0001
ActionSmoothness ∥at−2at−1+at−2∥2 -0.01
Largecontact max(FL,R−400,0,100) -0.01