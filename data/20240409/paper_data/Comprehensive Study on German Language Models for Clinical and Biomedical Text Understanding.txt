Comprehensive Study on German Language Models for Clinical and
Biomedical Text Understanding
Ahmad Idrissi-Yaghir1,2∗, Amin Dada3∗, Henning Schäfer1,4∗,
Kamyar Arzideh3,Giulia Baldini3,11,Jan Trienes3,Max Hasin3,
Jeanette Bewersdorff5,Cynthia S. Schmidt3,4,Marie Bauer3,
Kaleb E. Smith6,Jiang Bian7,8,Yonghui Wu7,8,Jörg Schlötterer9,10,
Torsten Zesch5,Peter A. Horn4,Christin Seifert9,
Felix Nensa3,11,Jens Kleesiek3,12,13,14,Christoph M. Friedrich1,2
1DepartmentofComputerScience,UniversityofAppliedSciencesandArtsDortmund,Dortmund,Germany
2InstituteforMedicalInformatics,BiometryandEpidemiology,UniversityHospitalEssen,Essen,Germany
3InstituteforAIinMedicine(IKIM),UniversityHospitalEssen(AöR),Essen,Germany
4InstituteforTransfusionMedicine,UniversityMedicineEssen,Essen,Germany
5ComputationalLinguistics,CATALPAFernUniversitätinHagen,Germany
6NVIDIA,SantaClara,CA,USA
7DepartmentofHealthOutcomesandBiomedicalInformatics,CollegeofMedicine,
UniversityofFlorida,Gainesville,FL,USA
8CancerInformaticsandeHealthcore,UniversityofFloridaHealthCancerCenter,
UniversityofFlorida,Gainesville,FL,USA
9UniversityofMarburg,Marburg,Germany
10UniversityofMannheim,Mannheim,Germany
11UniversityHospitalEssen,InstituteofInterventionalandDiagnosticRadiologyandNeuroradiology,
Essen,Germany
12CancerResearchCenterCologneEssen(CCCE),WestGermanCancerCenterEssen,
UniversityHospitalEssen(AöR),Essen,Germany
13GermanCancerConsortium(DKTK,PartnersiteEssen),Heidelberg,Germany
14DepartmentofPhysics,TUDortmund,Dortmund,Germany
{ahmad.idrissi-yaghir,christoph.friedrich}@fh-dortmund.de
{amin.dada,henning.schaefer,jens.kleesiek}@uk-essen.de
Abstract
Recentadvancesinnaturallanguageprocessing(NLP)canbelargelyattributedtotheadventofpre-trainedlanguage
models such as BERT and RoBERTa. While these models demonstrate remarkable performance on general
datasets,theycanstruggleinspecializeddomainssuchasmedicine,whereuniquedomain-specificterminologies,
domain-specificabbreviations,andvaryingdocumentstructuresarecommon. Thispaperexploresstrategiesfor
adaptingthesemodelstodomain-specificrequirements,primarilythroughcontinuouspre-trainingondomain-specific
data. Wepre-trainedseveralGermanmedicallanguagemodelson2.4BtokensderivedfromtranslatedpublicEnglish
medical data and 3B tokens of German clinical data. The resulting models were evaluated on various German
downstream tasks, including named entity recognition (NER), multi-label classification, and extractive question
answering. Our results suggest that models augmented by clinical and translation-based pre-training typically
outperformgeneraldomainmodelsinmedicalcontexts. Weconcludethatcontinuouspre-traininghasdemonstrated
theabilitytomatchorevenexceedtheperformanceofclinicalmodelstrainedfromscratch. Furthermore,pre-training
onclinicaldataorleveragingtranslatedtextshaveproventobereliablemethodsfordomainadaptationinmedical
NLPtasks.
Keywords:German-centricNLP,ClinicalLanguageModels,DomainAdaptation
1. Introduction ument and token classification, as well as ma-
chine translation and text summarization. The
In recent years, pre-trained language models successofthesemodelsisprimarilyattributedto
(PLMs) such as BERT (Devlin et al., 2019) and theirtransformer-basedarchitecture(Vaswanietal.,
RoBERTa (Liu et al., 2019) have become crucial 2017)andtheirtrainingonlargeamountsofunla-
inthefieldofnaturallanguageprocessing(NLP). beleddata. However,sincethesemodelsareoften
Thesemodelshavesignificantlyenhancedtheper- trainedongeneraldatasourcessuchasWikipedia,
formanceofawiderangeoftasks,includingdoc- newsarticles,andbooks,theireffectivenessmay
be limited in specific domains, such as medicine
∗Theseauthorscontributedequallytothiswork
4202
rpA
8
]LC.sc[
1v49650.4042:viXraorfinance,whichhavedistinctterminologiesand 2. Related Work
writing styles. To achieve better results in these
specialized fields, it is essential to use language
Following the success of transformer-based lan-
modelsthataretailoredtothesespecificdomains.
guagemodels,severallanguagemodelshavebeen
Buildingonthisconcept,languagemodelscanbe
developed for biomedical and clinical domains,
adaptedtospecializeddomainsthroughtwometh-
mainly for English. One of the earliest models is
ods. The first approach involves training models
BioBERT(Leeetal.,2019),whichwasinitialized
from scratch on unlabeled data from the desired
fromageneralBERTmodelandfurtherpre-trained
domain. This method ensures that the model is
usingbiomedicaldatasuchasPubMedabstracts.
groundedintheuniquecharacteristicsofthetarget
Thisapproachdemonstratedtheeffectivenessof
domainfromthebeginning. Thesecondapproach
continuous pre-training on domain-specific data,
reliesoncontinuouspre-training. Insteadofstarting
allowingthemodeltomoreaccuratelycapturethe
fromscratch,existinggeneral-purposepre-trained
challenging nature of the biomedical domain tex-
models can be used and refined through further
tualdata. Severalotherspecializedmodelssoon
pre-trainingonthedomain-specificunlabeleddata
followed. OnesuchspecializedmodelisClinical-
(Gururanganetal.,2020). Thisallowsatransition
BERT(Alsentzeretal.,2019). Whileitusedasimi-
thatshiftsthefocalpointofthemodelfromabroad
larcontinuouspre-trainingapproachtoBioBERT,
scope to one specific to the particularities of the
itdifferedinitsintegrationofclinicaldata,particu-
targetdomain.
larlyfromsourcessuchastheMedicalInformation
Particularlyinthemedicaldomain,suchspecial-
MartforIntensiveCare(MIMIC-III,Johnsonetal.
ized models can potentially improve the practice
2016), a public dataset of de-identified medical
ofmedicinebyprovidingaccurateandrelevantin-
records for over 40,000 patients in the intensive
sights from vast amounts of textual data. These
careunitsofBethIsraelDeaconessMedicalCen-
specialized models are highly valuable given the
terfrom2001–2012. Furthermore,Guetal.(2022)
complexnatureofmedicalterminologyandthecrit-
introducedPubMedBERT,whichwasnotbasedon
icalimportanceofaccurateinformationinhealth-
apreviouslypre-trainedmodelbutwastrainedfrom
care. Forexample,theycanhelpanalyzepatient
scratchonbiomedicaldata. Thetrainingdataset
records, extract critical information from medical
consisted of both PubMed abstracts and the full
literature,andfacilitatereal-timeclinicaldecision-
textfromPMC,resultinginmodelsthatwereable
makingbyunderstandingpatientqueriesormedical
toachieveimprovedperformanceonawiderange
notes.
ofbiomedicaltasks.
However, building these specialized medical
InlanguagesotherthanEnglish,itismorechal-
modelspresentsuniquechallenges. Medicaldata
lenging to build such specialized models due to
ischaracterizednotonlybyspecializedterminology
the lack of available data, as is the case for Ger-
butalsobythesensitivenatureoftheinformation.
man(ZeschandBewersdorff,2022). However,sig-
Patient confidentiality and other ethical consider-
nificantadvancementshaveoccurredinthisfield
ations are paramount, which can complicate the
recently, such as BioGottBERT (Lentzen et al.,
acquisitionoflargedatasetsfortraining.
2022), a model based on the GottBERT model
ParticularfocushasbeensetonGermanmedical
(Scheible et al., 2020). GottBERT is a German
databecauseofitsdatasparsitywhencompared
RoBERTabasemodelthatunderwenttraininguti-
toEnglishdatasets(Névéoletal.,2018;Schneider
lizing general domain information. BioGottBERT
et al., 2020). While resources for languages like
enhanced its medical capabilities by undergoing
French(Labraketal.,2023)andSpanish(Carrino
furtherpre-trainingonpublicGermanmedicaltexts
etal.,2022)havebeenincreasinglymadeavailable,
from Wikipedia and scientific abstracts. This re-
Germanmedicaldatastillremainsnotablyunder-
sultedinasignificantimprovementinperformance
represented and has only recently been pushed
onmedicaltaskswhencomparedtoGottBERT.In
further(Bressemetal.,2024).
addition to BioGottBERT, the authors trained an
Inthiswork,severalnewGermanbiomedicaland
ELECTRA(Clarketal.,2020)smallandbasicmod-
clinical language models are introduced and ex-
elsfromscratchwiththeaimofevaluatingtheef-
tensivelyevaluatedonmultipledownstreamtasks.
fectiveness of training new models using only a
Allmodelvariantsarecontinuouslypre-trainedon
limited amount of biomedical data. However, the
twodifferentdatastreams,resultinginpublicmod-
authorsreportedthatthisstrategywasunsuccess-
elsbenefitingfromtranslationsofbiomedicaland
ful, and the resulting models were inferior to ex-
medicaldatasetsintoGerman,whileprivatemod-
isting general models. Another German medical
els use internal data from a large German hospi-
modelisMedBERTde(Bressemetal.,2024). This
tal. Translation-basedmodelswillbemadepublicly
available.1 BERT-basedmodelwastrainedfromscratchusing
variouspublicGermanmedicaldatasetssuchas
1https://huggingface.co/ikim-uk-essen GGPONC1.0(Borchertetal.,2020),PubMedab-stracts,anddoctoraldissertations. Inaddition,the ourknowledge,thisisthelargestGermanclinical
trainingalsoincorporatedreal-worldclinicaldata, textdatasetcompiledforpre-training.
suchasradiologyreportsfromtheCharitéUniver-
sityHospitalinBerlin. Byintegratingsuchawide 3.2. Public Data
rangeofclinicalandbiomedicaldata,medBERTde
Theseconddatasetisderivedfrompubliclyavail-
aimstoprovideacomprehensiveunderstandingof
ablebiomedicaldata. Itwasinitiatedwithapproxi-
themedicalfieldtailoredtotheGermancontext.
mately16KGermanabstractsfromPubMed. Rec-
Recently,Frenchhasalsoseenasurgeinspe-
ognizingthelimitedsizeofthisdataset,itwasnec-
cialized biomedical and clinical pre-trained lan-
essarytoexpandittoimprovereliabilityandcov-
guage models. Among them is DrBERT (Labrak
erage. To achieve this, approximately 6 million
etal.,2023),whichisbasedontheRoBERTaar-
English PubMed abstracts, along with MIMIC-III
chitecture and has been trained on both public
clinical notes (Johnson et al., 2016), were trans-
webdataandspecializedprivatemedicaldatafrom
lated using the Fairseq WMT’19 English to Ger-
theUniversityHospitalofNantes. Thepublicweb
mantranslationmodel2 (Ngetal.,2019). Although
dataisalargetextcorpuscalledNACHOS(opeN
thetranslationofmedicalcontentcanbecomplex
crAwledfrenChHealthcarecOrpuS),crawledfrom
andpotentiallyleadtoinaccuraciesduetospecial-
severalonlinebiomedicalsources. Bytrainingon
izedterminologies,itprovidesawayofaugment-
thedifferentdatasets,asetofmodelswasobtained,
ing the corpus. The decision to use this particu-
whichwerethencomparedbyevaluatingtheirper-
lartranslationmodelwasbasedonaphysician’s
formance on a wide range of public and private
assessment. They were provided with different
tasks. AnotherdevelopedmodelisAliBERT(Berhe
translationsof10PubMedabstractsamplesand20
etal.,2023),amodeldesignedspecificallyforthe
MIMICnotessamplesgeneratedbyvarioustrans-
French biomedical domain. It was trained using
lationmodels,includingWMT19-en-de,M2M-100
a regularized unigram tokenizer on different sub-
(Fan et al., 2021), NLLB (Team et al., 2022), T5
corporaofFrenchbiomedicaltextualdocuments,
(Raffeletal.,2020),MBart-50(Tangetal.,2021),
suchasbiomedicalarticlesfromScienceDirect,the-
andOPUS-MT-en-de(TiedemannandThottingal,
sis manuscripts, and articles from the Cochrane
2020). Theirevaluationguidedthefinalmodelse-
database. The model excels in F1 and accuracy
lection.
scores, provingitscapabilitiesinthisdomain. In-
Prior to translation, a preprocessing step was
terestingly, despite a smaller amount of training
performed. Alldocumentsweretokenizedintoindi-
data and a shorter pre-training period, AliBERT
vidualsentencesusingtheStanzalibrary(Qietal.,
managestosurpasssomenotablegeneralFrench
2020). These sentences were then grouped into
models,highlightingitscapabilities.
specificsegments,eachlimitedtoamaximumof
128 tokens. Due to this segmentation, the num-
3. Pre-Training Datasets berofdocumentsincreasedsubstantially. Forin-
stance, the initial 6 million English PubMed ab-
This section details the datasets utilized for pre- stractsweredividedintoapproximately21million
training, including clinical data and public medi- segmentsordocumentsfortranslation. Thetoken
cal/biomedicaldata. Whiletheclinicaldatasetcap- limitwaschosenbasedontheobservationthatseg-
turesreal-worldpatientinsights,themedicaldata ments with more than 128 tokens often suffered
encompassesawiderangeofscientificinformation. frompoortranslationquality. Thenumberoftokens
Thesedatasourcesprovidethefoundationforour was determined using the tokenizer of the trans-
models. Anin-depthdescriptionofthesedatasets lationmodel. Thisprocessyieldedapproximately
follows. 45Mdocuments,detailedinTable3.2.
3.1. Clinical Data 4. Base Models
The first dataset was sourced from a major Ger- Inthissection,themodelsincludedinourbench-
manhospital, providingacomprehensiveclinical markaredescribed. Asabaselineforourevalua-
datasetthatspansfrom2002to2023. Itincludes tions,weusemodelsthathavebeenpre-trainedon
variousclinicaldocuments,includingclinicalnotes, extensivedatasetsandhavedemonstratedstrong
differentreports,anddoctor’sletters. Eachtextwas performanceonavarietyofgeneralandmedical
dividedintoparagraphs,whichwerethenfiltered. NLP tasks. The general German language mod-
Paragraphswitharatioofletterstoothercharacters elsGBERT ,GBERT andGELECTRA
base large large
below60%andparagraphswithanaveragenum- (Chanetal.,2020)wereprimarilyconsidered. They
berofwordsperlinebelowthreewerefilteredout.
Theresultingdatasetconsistsof3,060,845,169to- 2https://huggingface.co/facebook/
kensfrom25,023,489documents. Tothebestof wmt19-en-de,lastaccessed: 2023-10-13Pre-training
Downstream Tasks
Private Models Named Entity Recognition
Multi-label Classification
Translation
Public Models Question Answering
Continuous Pre-training
Figure1: WorkflowforContinuousPre-trainingofdifferentPubliclyAvailableGeneralDomainModelswith
PrivateandPublicDatasets: TheprivatedatasetcomesfromamajorGermanhospitalandyields25M
documents. Thepublicdataset,sourcedfromEnglishPubMedabstractsandMIMIC-IIIclinicalnotes,is
preprocessed,tokenizedintosentencesegments,andtranslatedintoGermanviatheFairseqWMT’19
EnglishtoGermanmodel,resultinginapproximately45Mdocuments. Bothmodelsetupsaresubjectto
modelfine-tuningandevaluationacrossvariousbiomedicalandclinicalGermanlanguagedownstream
tasks.
Dataset Tokens Documents etal.,2024)werealsoexaminedtoprovideaddi-
tionalcomparativeinsights.
GermanPubMed 5M 16K
PubMed 1,700M 21M
MIMIC-III 695M 24M
5. Pre-training
Total 2,400M 45M
Continuouspre-trainingofseveralpubliclyavailable
Table 1: Public dataset composition. The in- general domain models was performed with the
crease in the number of documents for PubMed aforementioneddatasets. First,wecontinuedthe
and MIMIC-III compared to the original source is pre-trainingoftheGeBERTa andGeBERTa
base large
duetothesegmentationofthecontentintochunks models with the clinical dataset. The objective
of128tokensorlessforthetranslationprocess. wastoquantifythecontributionofclinicaldatato
the performance of the models. While the orig-
inal model has already been trained on medical
were trained on four different datasets: the Ger- texts obtained mainly from MIMIC-III, translated
manportionoftheOpenSuper-largeCrawledAL- PubMed abstracts, and filtered CC100 (Wenzek
MAnaCH coRpus (OSCAR) (Ortiz Suárez et al., etal.,2020),ithasnotseenanyreal-worldGerman
2020),GermanWikipediadump,TheOpenParal- clinical data. Both models were trained for 200k
lelCorpus(OPUS)(Tiedemann,2012),andOpen AdamW(LoshchilovandHutter,2019)optimization
LegalData(Ostendorffetal.,2020). Anothermodel stepswithabatchsizeof512. The learningrate
is GottBERT (Scheible et al., 2020), a German was set to 3e−5 for the base model and 2e−5 for
RoBERTa base modeltrainedontheGermanpartof thelargemodel. Additionally,theGBERT base and
OSCARdata. Furthermore,twomultilingualmod- GBERT large models were further pre-trained on
elsXLM-R(Conneauetal.,2020)andmDeBERTa theclinicaldatawiththesameparameters. These
V3(Heetal.,2023)werealsoconsidered. Addition- modelswerenotexplicitlypre-trainedonmedical
ally,GeBERTa andGeBERTa (Dadaetal., textsbefore.
base large
2023)wereexplored,withtheseDeBERTav2(He In order to separately quantify the influence
etal.,2021)basedmodelspre-trainedfromscratch of translation-based medical texts and clini-
on a combination of data sources ranging from cal documents, additional pre-training experi-
Wikipedia to medical datasets. Finally, the Ger- ments were conducted with the GBERT and
base
manbiomedicalandclinicalmodelsBioGottBERT GBERT models. Bothmodelsunderwentfur-
large
(Lentzenetal.,2022)andmedBERTde(Bressem ther pre-training on the translated dataset. TheGBERTlargewastrainedfor73kstepswithalearn- tionguidethatclarifiesthedefinitionofeachentity
ing rate of 5e−5 and a batch size of 144. On the class.
other hand, the GBERTbase was trained for 75k
stepsusingabatchsizeof336. Duetohardware
limitations,theinitialexperimentswereconducted GraSCCo
separately,reflectingthedifferencesinbatchsizes
betweentheexperiments. TheAdamWoptimizer Graz Synthetic Clinical text Corpus (GraSCCo)
wasalsoemployedduringthispre-trainingforthe (Modersohn et al., 2022) is a synthetic German
optimizationprocess. corpusconsistingofabout60clinicaldocuments
withmorethan43,000tokens. Itincludesaseries
of alienation steps to hide privacy-sensitive infor-
6. Downstream Datasets & Tasks
mationinrealclinicaldocuments,thetrueoriginof
allGraSCCotexts. Asaresult,thedataispublicly
To evaluate the pre-trained models, the models
availablewithoutanylegalrestrictions. Withinthe
werefine-tunedandsubsequentlyevaluatedacross
medbert.depaper,anadditionalannotationofthe
arangeofdownstreamtasks,aimingtodetermine
GraSCCodataforanNERtaskwasperformedby
theirefficacyandadaptabilityinspecializedbiomed-
the authors of the GGPONC 2.0. We use these
icalandclinicaldomains.
annotationsforourbenchmarkaswell.
BRONCO
CLEFeHealth2019
TheBerlin-Tübingen-OncologyCorpus(BRONCO)
(Kittneretal.,2021)isacomprehensive,freelyac- TheCLEF(ConferenceandLabsoftheEvaluation
cessibleGermancorpusderivedfrom200oncology Forum)(Kellyetal.,2019)eHealthdatasetisacu-
discharge summaries of cancer patients. These ratedcollectionofnon-technicalsummaries(NTS)
summarieshavebeenmanuallyde-identifiedand ofanimalexperimentsfromGermany,whichwas
annotated to highlight key entities such as diag- usedtoorganizetheMultilingualInformationExtrac-
noses, treatments, and medications. This an- tionTask(Task1)intheCLEFeHealthChallenge
notated corpus contains 11,434 sentences and 2019 (Dörendahl et al., 2019). These NTS have
89,942tokens,with11,124annotationsidentifying been made publicly available to increase trans-
medicalentitiessuitablefornamedentityrecogni- parencyinanimalresearch. Fortheidentification
tion(NER).Whiletheauthorshavereleased75% of the primary diseases that are the focus of the
(or150summaries)ofthedatasettothepublic,they experiments,eachNTSinthedatasetismanually
have kept 25% (or 50 summaries) as a held-out annotated with the corresponding ICD-10 codes.
settoensureunbiasedevaluationordatacontami- Reportsarepredominantlyscientificandbiomed-
nation. GiventheunavailabilityoftheBRONCO50 ical, while some clinical jargon could also be ob-
dataset, a 5-fold cross-validation was performed served. In total, the dataset contains over 8,000
totrainandevaluatemodelsontheBRONCO150 NTSsfortrainingandanadditional407NTSsfor
dataset. testing. Theprimaryobjectiveassociatedwiththis
dataset is a multi-label classification, where sys-
temsarechallengedtoassigntherelevantICD-10
GGPONC2.0
codestoeachsummaryautomatically.
TheGermanGuidelinePrograminOncologyNLP
Corpus(GGPONC)2.0(Borchertetal.,2022)rep-
resentsasignificantadvanceinGermanmedical RadQA
languageresourcesandoffersalargecorpusfor
NER applications. Based on the top-level hier- The RadQA dataset is an extractive question-
archies of the SNOMED CT concept model, its answeringdatasetcreatedfrom1,223anonymized
annotationschemedistinguishesbetweenseveral radiology reports of brain CT scans from a large
subclassesofentities. The“Finding”categoryin- hospital in Germany. For its development, three
cludesentitiessuchasdiagnosis,pathology,and medicalstudentassistantsintheirsixthandeighth
otherrelevantfindings. The“Substance”category semesters were assigned to annotate 29,273
delves into clinical drugs, nutrients or body sub- question-answerpairs. Theannotatorswerepro-
stances,andexternalsubstances. Inaddition,the videdwithalistofquestionsdesignedwiththeinput
“Procedure” category houses entities associated of a radiologist. This decision was influenced by
withtherapeuticanddiagnosticprocedures. Rec- theuniquenatureofradiologyqueriesandthechal-
ognizingthecomplexnatureofclinicaltexts,where lenges of annotating sensitive clinical data. The
entityboundariesareoftenambiguous,GGPONC annotatorsgeneratedonecustomquestionforev-
2.0iscomplementedbyacomprehensiveannota- erythirdreporttoensurevariety.LearningRate BatchSize thereareoftenmanyclasses,amongwhichsome
Task Epochs
appearrarelyandothersveryfrequently. Thiscan
base,large base,large
leadtotrainingnotconvergingatallorbeingsen-
BRONCO 3e−5,1e−5 16,16 20
sitivetohyperparameters. Somemodelconfigura-
GGPONC2.0 3e−5,1e−5 16,16 5
GraSCCo 3e−5,1e−5 16,16 20 tionswereunstableforthistask,especiallyforlarge
CLEFeHealth 4e−5,1e−5 16,32 20 models. Usingtheadjustedweightsledtostable
RadQA 3e−5,1e−5 16,16 10 results. Thepositiveclassweights,w ,arecalcu-
j
latedforlabelj andusethefollowinglogarithmic
Table 2: Hyperparameters of the different down- scheme:
streamtasks. (cid:18) (cid:19)
N
w =log
j 1+c
j
7. Fine-tuning
Where N denotes the total number of training
In a comprehensive evaluation, the performance samples and c is the count of each label. The
j
ofthemodelswasassessedonavarietyofdown- logarithmicweightingschemeadjustsweightsin-
streamtasks. TheproblemclassesincludedNER verselytothelabelfrequencyinthetrainingdata,
(BRONCO,GGPONC2.0,GraSCCO),multi-label ensuringbalancedattentionbetweenfrequentand
classification(CLEFeHealth2019),andextractive rarelabelsduringtraining.
questionanswering(RadQA).
8. Results
Hyperparameters
Thefine-tuningresultsforalldownstreamtasksare
Choosingtherighthyperparametersiscrucialfor
splitbetweenTable3andTable4.
optimizing model performance. However, in this
Acrossalltasks,ourclinicallypre-trainedmodels
study,anextensivehyperparametersearchwasin-
achieved the highest F1-Scores, with the excep-
tentionallyavoidedtoreflectaclinicalenvironment
tionofGGPONC2.0,whereoneofthetranslation
with limited computational resources. Instead, a
modelsachievedthehighestscore. Thisisespe-
mixtureofstandardandtask-specificsettingswas
ciallyevidentintheresultsoftheGraSCCodataset,
opted for, starting from the default parameters in
where clinical pre-training improved the perfor-
HuggingFace. Only the learning rate and batch
manceofGBERT by5.4percentagepointsin
sizewereadjustedtoaddresstraininginstabilities, base
F1-ScoreandtheperformanceofGBERT by
andthenumberofepochswassettoensurecon- large
6.9 percentage points. In addition, the GBERT-
vergenceofallmodels,resultinginvariationacross
BioM-translationmodelswereabletooutperform
differentdatasetsizesinthebenchmark.
thegeneralmodels. Forinstance,pre-trainingon
While this approach may not yield the optimal
translatedtextsresultedinanimprovementof2.1
resultsthatcanbeachievedbyanextensivegrid
percentagepointsoverGBERT .
searchwithhundredsofconfigurations,itprovides base
Although BRONCO150 is the only NER task
valuableinsightsintotheperformanceofmodels
withreal-worldclinicaldocuments,thereisnoob-
understandardparameterconditions,whichispar-
servable performance difference between Med-
ticularlyrelevantforclinicalapplicationswherecom-
BERTde,GeBERTa ,andourclinicalbasemod-
putational resources are often limited. By using base
els. However, the GeBERTa-Clinical model
uniformconfigurationsforbasicandlargemodels large
outperformed the second-best model by 0.5 per-
andanappropriatesmallnumberofepochsforfine-
centagepoints. Theminordifferencesbetweenthe
tuning,comparabilityacrossthemodelsandtasks
variousmodelsandthelackofclearadvantagesof
inthebenchmarkisaimedtobeincreased.
clinicalmodelscanbeattributedtothesmallsizeof
Althoughamorein-depthanalysisofthehyper-
thedataset. Interestingly,onGGPONC2.0,various
parameter optimization process could further en-
generaldomainmodelsareonparwithclinicalmod-
hancetherigorofthiswork,themethodologyused
elsorevenbetter. InthecaseofGeBERTa ,the
reflectsapracticalscenarioinclinicalsettingsand base
additionalclinicalpre-trainingdecreaseditsperfor-
offersarealisticassessmentofmodelperformance
mance. Itisworthnotingthatthedatasetconsists
underresourceconstraints. Thespecifichyperpa-
of guidelines for oncologists written in a different
rametersusedforeachdownstreamtaskarelisted
writingstylethanclinicaldocuments. Overall,the
inTable2.
translation-basedmodelsachievedthehighestre-
sults.
TaskSpecifics
In the multi-label classification CLEF eHealth
InthecontextoftheCLEFeHealth2019fine-tuning, 2019task,largemodelsgenerallyperformedbetter.
classimbalancesamongthelabelsareaddressed Forexample,GBERT hasanF1-Scorethatis
large
withlogarithmicweighting. Inmulti-labelscenarios, around1.6percentagepointsbetterthanitsbasevariant. Domain-specific continuous pre-training Thesefindingsopenuppossibilitiestotrainclini-
on the translations reached strong performance callyapplicablemodelsindifferentscenarioswhere
acrossallmetrics. Prioritizingabalancebetween onlylimitedclinicaldataisavailableorcannotbe
precision and recall favors the GELECTRA accessedforprivacyreasons. Thisisthecase,for
large
model,whileGeBERTa-Clinical surpassesin example,withlow-resourcelanguages,wherefar
large
precision. ThegeneraldomainGBERT model fewerclinicaldocumentsarewritten. Additionally,
large
providesthehighestrecall,althoughitsprecision synthetic or translated public data can also help
islowercomparedtoothers. protectpatientprivacybyavoidingpre-trainingon
Looking at the difference between GBERT- patientdata.
Clinical andGBERT-BioM-Translation inTa- Despitebeinglessresource-intensivethantrain-
base base
ble 4, the latter achieves comparable results to ing from scratch, this work was able to achieve
its clinical counterpart on BRONCO, GGPONC good results on various downstream tasks. This
2.0, and RadQA. Only on GraSCCo the results highlightstheeffectivenessoftransferlearningand
oftheGBERTmodelstrainedonthetranslations thevalueofpre-trainedmodels.
areworse. ComparedtothebaselineGBERTgen- Aswediscussthefindingsandmethodologies,
eralmodels,allfurtherpre-trainedmodelshavean itisimportanttoaddresssomeofthechallenges
advantage. Inthiscase,weseenoclearadvantage andlimitations. Determiningtheoptimalhyperpa-
oftrainingonGermanclinicaldataovertranslated rameters can be intricate. Subtle changes, such
texts. asadjustingthebatchsizeorseed,canaffectthe
Insummary,acrossalmostalltasks,clinicalpre- results. Asaresult,directcomparisonswithprevi-
training and translation-based pre-training led to ousworkshouldbemadewithcaution,especially
better performance than general domain models when score variances are minimal. Additionally,
that were not explicitly trained on medical data. althoughmodelsgroundedontranslationscanbe
While the results indicate a slight advantage for madepublic,sharingmodelstrainedonproprietary
models trained on our clinical data, the perfor- clinical data remains prohibited. This restriction
mancedifferencetendstobesmall,andinsome isgroundedindataprotectionmeasuresandcon-
cases,thetranslation-basedmodelsevenoutper- cerns about potential retrieval attacks that might
formtheclinicalones. exposethetrainingdata.
9. Discussion and Limitations 10. Conclusion
Overall, the results show that especially the ad- Inthisstudy,severalnewGermanbiomedicaland
dition of medical pre-training data gives a perfor- clinicallanguagemodelswereintroducedcoming
manceadvantage,butnotnecessarilythequalityof fromtwodatastreams: publictranslationdataand
thedata. Forexample,a6.9percentagepointsim- privateclinicaldatafromalargeGermanhospital,
provementinF1-Scoreperformancewasmeasured with the translation-based models made publicly
on the GraSCCo dataset between GBERT availabletotheresearchcommunity. Thesemod-
large
andGBERT-Clinical . However,thedifference elswereassessedonfivedownstreamtasksand
large
between GBERT-Clinical and GBERT-BioM- comparedtoavarietyofGermanandmultilingual
large
Translation isonly2.4percentagepoints. Simi- modelsfromthegeneral,biomedical,andclinical
large
larly,inRadQAthedifferencebetweenGBERT domains. Itwasdemonstratedthatthetranslation
base
andGBERT-BioM-Translation is1.4percentage ofPubMedisapromisingapproachthatcanavoid
base
points,butthedifferencebetweenthetwofurther data protection concerns. In most downstream
pre-trainedmodelsisonly0.1percentagepoints. tasks,translation-basedmodelsachievedcompara-
We conclude that the presence of medical data bleresultstolarge-scaleclinicalmodels,although
is crucial for pre-training, but not necessarily its only 6 million translated abstracts were utilized.
qualityorproximityforthedownstreamtask. The Inparticular,clinicaldownstreamtasksshowthat
smalldifferencesbetweenourGeBERTa-Clinical domain-specificpre-trainingcanstillbeworthwhile,
andstandardGeBERTamodelsfurthersupportthis butthegeneraldomainGermanmodels,suchas
hypothesis,asthestandardversionofGeBERTA GBERT, can be sufficient in many domains. De-
already contained translations. In this context, it spitetheperformanceandeaseofdistributionfor
is also worth noting that the difference between translation-basedmodels,itisimportanttorecog-
GBERT-ClinicalandGBERT-BioM-Translationmod- nizethatinhalfofthetaskstested,modelsderived
els is particularly evident on the GraSCCo task, fromprivateclinicaldatastillperformedbetter,high-
eventhoughitconsistsofsyntheticallygenerated lightingtheimportanceandeffectivenessoflarge
documentsandnotrealclinicaltexts. Incontrast, specializeddatasources.
smallerdifferencesareevidentinclinicaltaskssuch Future work could look at training models that
asRadQA. useallavailablePubMedabstracts. Inaddition,weCLEFeHealth2019 RadQA GraSCCo
Model
F1 P R F1 EM F1 P R
GBERT (Chanetal.,2020) .816 .818 .815 .794 .707 .642 .617 .676
base
GBERT (Chanetal.,2020) .832 .802 .865 .809 .718 .647 .617 .680
large
GottBERT(Scheibleetal.,2020) .791 .818 .765 .796 .712 .652 .681 .624
XLM-R (Conneauetal.,2020) .804 .781 .829 .813 .731 .674 .655 .694
large
GELECTRA (Chanetal.,2020) .827 .826 .828 .812 .725 .681 .702 .661
large
mDeBERTaV3 (Heetal.,2023) .793 .786 .801 .810 .741 .675 .646 .706
base
GeBERTa (Dadaetal.,2023) .823 .817 .829 .839 .769 .684 .702 .667
base
GeBERTa (Dadaetal.,2023) .837 .848 .826 .834 .757 .669 .700 .640
large
BioGottBERT(Lentzenetal.,2022) .791 .779 .803 .797 .706 .637 .673 .605
GBERT-BioM-Translation † .825 .851 .801 .808 .716 .661 .642 .681
base
GBERT-BioM-Translation † .833 .860 .807 .811 .714 .692 .677 .707
large
MedBERTde(Bressemetal.,2024) .836 .839 .833 .833 .761 .660 .626 .697
GBERT-Clinical † .833 .853 .815 .807 .726 .696 .670 .725 base
GBERT-Clinical † .843 .876 .812 .806 .710 .716 .692 .742
large
GeBERTa-Clinical † .804 .816 .792 .845 .762 .680 .699 .662
base
GeBERTa-Clinical † .833 .874 .796 .846 .768 .703 .677 .730
large
Table3: PerformanceofDifferentModelsonvariousDownstreamTasks: CLEFeHealth2019(Multi-label
classification),RadQA(ExtractiveQuestionsAnswering),andGraSCCo(NER).F1-Scoresreportedare
micro-averaged. PdenotesPrecision,RdenotesRecall,EMdenotesExactMatch,†marksmodelsthat
wepre-trained
BRONCO150 GGPONC2.0
Model
F1 P R F1 P R
GBERT .833±.004 .818±.002 .849±.011 .770 .761 .780
base
GBERT .835±.006 .820±.004 .852±.011 .772 .758 .786
large
GottBERT .840±.008 .827±.010 .854±.012 .756 .744 .768
XLM-R .841±.003 .823±.007 .860±.007 .775 .764 .786
large
GELECTRA .850±.006 .835±.005 .865±.007 .780 .769 .792
large
mDeBERTaV3 .843±.005 .824±.007 .862±.007 .768 .753 .783
base
GeBERTa .848±.007 .830±.010 .866±.007 .772 .761 .783
base
GeBERTa .847±.008 .825±.003 .872±.001 .772 .758 .786
large
BioGottBERT .844±.011 .826±.012 .863±.013 .770 .756 .785
GBERT-BioM-Translation † .842±.006 .824±.007 .861±.007 .780 .766 .794
base
GBERT-BioM-Translation † .844±.007 .825±.007 .864±.009 .786 .779 .793
large
MedBERTde .848±.005 .833±.008 .864±.006 .755 .744 .744
GBERT-Clinical † .847±.009 .828±.009 .867±.011 .772 .763 .781 base
GBERT-Clinical † .844±.007 .825±.007 .864±.009 .785 .778 .792
large
GeBERTa-Clinical † .846±.006 .823±.007 .870±.009 .768 .757 .780
base
GeBERTa-Clinical † .855±.007 .832±.006 .878±.011 .773 .760 .787
large
Table 4: Performance of Different Models on NER Downstream Tasks: BRONCO150 (5-fold cross
validation), GGPONC 2.0. F1-Scores reported are micro-averaged. P denotes Precision, R denotes
Recall,†marksmodelsthatwepre-trained
aimtoexplorethetrainingofGermanmedicallarge 11. Ethical Statement
languagemodelsandleveragetheircapabilities.
This study was conducted in alignment with the
principlesoftheHelsinkiDeclarationandreceived
approvalfromtheInstitutionalReviewBoard(IRB).
Throughout the research, the team maintained
transparency,integrity,andrespectfortheunique
lareneg
lacidem
lacinilc
lareneg
lacidem
lacinilcrequirementsofmedicaldata,emphasizingpatient Tasks,pages223–236,Toronto,Canada.Asso-
welfareanddataprotectionstandards. ciationforComputationalLinguistics.
Theuseoflanguagemodelsinthemedicalfield
FlorianBorchert,ChristinaLohr,LuiseModersohn,
raisesseveralethicalconcerns. Biasesinthetrain-
ThomasLanger,MarkusFollmann,JanPhilipp
ingdatacanleadtopooroutcomesforunderrepre-
Sachs,UdoHahn,andMatthieu-P.Schapranow.
sentedgroups,posingasignificantissueinhealth-
2020. GGPONC:AcorpusofGermanmedical
caredelivery. Thishighlightstheneedforstrategies
text with rich metadata based on clinical prac-
toidentifyandaddresspotentialbiases,ensuring
tice guidelines. In Proceedings of the 11th In-
equitablerepresentationandoutcomesforallpa-
ternationalWorkshoponHealthTextMiningand
tientgroups.
InformationAnalysis,pages38–48,Online.As-
Moreover,the“blackbox”natureofthesemodels
sociationforComputationalLinguistics.
complicatestheiruseinmedicaldecision-making,
wheretransparencyandtrustarecrucial. Thisem-
FlorianBorchert,ChristinaLohr,LuiseModersohn,
phasizestheimportanceofprioritizinginterpretabil-
JonasWitt,ThomasLanger,MarkusFollmann,
ity and explainability in the development and ap-
MatthiasGietzelt,BertArnrich,UdoHahn,and
plication of language models, fostering trust and
Matthieu-P.Schapranow.2022. GGPONC2.0-
enablinginformeddecision-making.
the Germanclinicalguideline corpus foroncol-
Furthermore, the use of patient data presents
ogy: Curationworkflow,annotationpolicy,base-
challenges in informed consent and privacy, es-
line NER taggers. In Proceedings of the Thir-
peciallygiventhedifficultiesinsecuringindividual
teenthLanguageResourcesandEvaluationCon-
consentforlargedatasetsandtheinabilityforpa-
ference, pages 3650–3660, Marseille, France.
tients to opt-out post-training. This underscores
EuropeanLanguageResourcesAssociation.
thenecessityforrobustdataprotectionmeasures,
adherencetorelevantprivacyregulations,anden- Keno K. Bressem, Jens-Michalis Papaioannou,
suringthatpatientdataishandledwiththeutmost Paul Grundmann, Florian Borchert, Lisa C.
careandconfidentiality. Adams, Leonhard Liu, Felix Busch, Lina Xu,
Theseissuesemphasizetheneedforcarefuleth- Jan P. Loyen, Stefan M. Niehues, Moritz Au-
icalconsiderationsinthedeploymentoflanguage gustin, Lennart Grosser, Marcus R. Makowski,
modelsinhealthcare,andthisstudyaimedtoad- HugoJ.W.L.Aerts,andAlexanderLöser.2024.
dress these concerns through a comprehensive medbert.de: A comprehensive german bert
andethicallygroundedapproach. modelforthemedicaldomain. ExpertSystems
withApplications,237:121598.
12. Acknowledgement Casimiro Pio Carrino, Joan Llop, Marc Pàmies,
AsierGutiérrez-Fandiño,JordiArmengol-Estapé,
The work of Ahmad Idrissi-Yaghir and Henning Joaquín Silveira-Ocampo, Alfonso Valencia,
SchäferwasfundedbyaPhDgrantfromtheDFG AitorGonzalez-Agirre,andMartaVillegas.2022.
Research Training Group 2535 Knowledge- and Pretrainedbiomedicallanguagemodelsforclini-
data-basedpersonalisationofmedicineatthepoint calNLPinSpanish. InProceedingsofthe21st
ofcare(WisPerMed). WorkshoponBiomedicalLanguageProcessing,
pages193–199,Dublin,Ireland.Associationfor
13. Bibliographical References ComputationalLinguistics.
BrandenChan,StefanSchweter,andTimoMöller.
2020. German’snextlanguagemodel. InPro-
ceedings of the 28th International Conference
EmilyAlsentzer,JohnMurphy,WilliamBoag,Wei-
onComputationalLinguistics,pages6788–6796,
Hung Weng, Di Jindi, Tristan Naumann, and
Barcelona,Spain(Online).InternationalCommit-
Matthew McDermott. 2019. Publicly available
teeonComputationalLinguistics.
clinicalBERTembeddings.InProceedingsofthe
2ndClinicalNaturalLanguageProcessingWork- KevinClark,Minh-ThangLuong,QuocV.Le,and
shop, pages 72–78, Minneapolis, Minnesota, ChristopherD.Manning.2020. ELECTRA:pre-
USA.AssociationforComputationalLinguistics. training text encoders as discriminators rather
thangenerators. In8thInternationalConference
Aman Berhe, Guillaume Draznieks, Vincent
on Learning Representations, ICLR 2020, Ad-
Martenot, Valentin Masdeu, Lucas Davy, and
disAbaba,Ethiopia,April26-30,2020.OpenRe-
Jean-Daniel Zucker. 2023. AliBERT: A pre-
view.net.
trained language model for French biomedical
text. InThe22ndWorkshoponBiomedicalNat- Alexis Conneau, Kartikay Khandelwal, Naman
uralLanguageProcessingandBioNLPShared Goyal,VishravChaudhary,GuillaumeWenzek,Francisco Guzmán, Edouard Grave, Myle Ott, Pengcheng He, Jianfeng Gao, and Weizhu
LukeZettlemoyer,andVeselinStoyanov.2020. Chen. 2023. Debertav3: Improving deberta
Unsupervisedcross-lingualrepresentationlearn- using electra-style pre-training with gradient-
ingatscale. InProceedingsofthe58thAnnual disentangled embedding sharing. In The
MeetingoftheAssociationforComputationalLin- EleventhInternationalConferenceonLearning
guistics,pages8440–8451,Online.Association Representations, ICLR 2023, Kigali, Rwanda,
forComputationalLinguistics. May1-5,2023.OpenReview.net.
Amin Dada, Aokun Chen, Cheng Peng, Kaleb Pengcheng He, Xiaodong Liu, Jianfeng Gao,
Smith,AhmadIdrissi-Yaghir,ConstantinSeibold, and Weizhu Chen. 2021. Deberta: decoding-
Jianning Li, Lars Heiliger, Christoph Friedrich, enhanced bert with disentangled attention. In
Daniel Truhn, Jan Egger, Jiang Bian, Jens 9thInternationalConferenceonLearningRep-
Kleesiek,andYonghuiWu.2023. Ontheimpact resentations,ICLR2021,VirtualEvent,Austria,
ofcross-domaindataonGermanlanguagemod- May3-7,2021.OpenReview.net.
els. InFindingsoftheAssociationforComputa-
Alistair E.W. Johnson, Tom J. Pollard, Lu Shen,
tionalLinguistics: EMNLP2023,pages13801–
LiweiH.Lehman,MenglingFeng,Mohammad
13813, Singapore. Association for Computa-
Ghassemi, Benjamin Moody, Peter Szolovits,
tionalLinguistics.
Leo Anthony Celi, and Roger G. Mark. 2016.
JacobDevlin,Ming-WeiChang,KentonLee,and MIMIC-III, a freely accessible critical care
KristinaToutanova.2019. BERT:Pre-trainingof database. ScientificData,3(1).
Deep Bidirectional Transformers for Language
LiadhKelly,HannaSuominen,LorraineGoeuriot,
Understanding. InProceedingsofthe2019Con-
Mariana Neves, Evangelos Kanoulas, Dan Li,
ference of the North American Chapter of the
LeifAzzopardi,ReneSpijker,GuidoZuccon,Har-
Association for Computational Linguistics: Hu-
risenScells,andJoãoPalotti.2019. Overviewof
manLanguageTechnologies,NAACL-HLT2019,
theCLEFeHealthevaluationlab2019. InLec-
Minneapolis,MN,USA,June2-7,2019,Volume
tureNotesinComputerScience,pages322–339.
1(LongandShortPapers), pages4171–4186.
SpringerInternationalPublishing.
AssociationforComputationalLinguistics.
MadeleineKittner,MarioLamping,DamianTRieke,
Antje Dörendahl, Nora Leich, Benedikt Hummel,
Julian Götze, Bariya Bajwa, Ivan Jelas, Gina
GilbertSchönfelder,andBarbaraGrune.2019.
Rüter, Hanjo Hautow, Mario Sänger, Maryam
Overviewoftheclefehealth2019multilingualin-
Habibi, Marit Zettwitz, Till de Bortoli, Leonie
formationextraction. InWorkingNotesofCLEF
Ostermann, Jurica Ševa, Johannes Starlinger,
2019-ConferenceandLabsoftheEvaluationFo-
OliverKohlbacher,NisarPMalek,UlrichKeilholz,
rum.
andUlfLeser.2021. Annotationandinitialevalu-
ationofalargeannotatedGermanoncological
AngelaFan,ShrutiBhosale,HolgerSchwenk,Zhiyi
corpus. JAMIAOpen,4(2):ooab025.
Ma, Ahmed El-Kishky, Siddharth Goyal, Man-
deepBaines,OnurCelebi,GuillaumeWenzek, YanisLabrak,AdrienBazoge,RichardDufour,Mick-
Vishrav Chaudhary, Naman Goyal, Tom Birch, ael Rouvier, Emmanuel Morin, Béatrice Daille,
VitaliyLiptchinsky,SergeyEdunov,MichaelAuli, and Pierre-Antoine Gourraud. 2023. Drbert: A
and Armand Joulin. 2021. Beyond english- robust pre-trained model in french for biomedi-
centricmultilingualmachinetranslation. Journal calandclinicaldomains. InProceedingsofthe
ofMachineLearningResearch,22(107):1–48. 61stAnnualMeetingoftheAssociationforCom-
putationalLinguistics(Volume1: LongPapers),
Yu Gu, Robert Tinn, Hao Cheng, Michael Lucas,
ACL 2023, Toronto, Canada, July 9-14, 2023,
Naoto Usuyama, Xiaodong Liu, Tristan Nau-
pages16207–16221.AssociationforComputa-
mann, JianfengGao, andHoifungPoon.2022.
tionalLinguistics.
Domain-specificlanguagemodelpretrainingfor
biomedicalnaturallanguageprocessing. ACM Jinhyuk Lee, Wonjin Yoon, Sungdong Kim,
Trans.Comput.Heal.,3(1):2:1–2:23. Donghyeon Kim, Sunkyu Kim, Chan Ho So,
and Jaewoo Kang. 2019. BioBERT: a pre-
Suchin Gururangan, Ana Marasovic, Swabha
trained biomedical language representation
Swayamdipta,KyleLo,IzBeltagy,DougDowney,
modelforbiomedicaltextmining. Bioinformatics,
andNoahA.Smith.2020. Don’tstoppretraining:
36(4):1234–1240.
Adapt language models to domains and tasks.
InProceedingsofthe58thAnnualMeetingofthe Manuel Lentzen, Sumit Madan, Vanessa Lage-
AssociationforComputationalLinguistics,ACL Rupprecht,LisaKühnel,JulianeFluck,MarcJa-
2020,Online,July5-10,2020,pages8342–8360. cobs, Mirja Mittermaier, Martin Witzenrath, Pe-
AssociationforComputationalLinguistics. terBrunecker,MartinHofmann-Apitius,JoachimWeber, and Holger Fröhlich. 2022. Critical as- Colin Raffel, Noam Shazeer, Adam Roberts,
sessment of transformer-based AI models for KatherineLee,SharanNarang,MichaelMatena,
germanclinicalnotes. JAMIAOpen,5(4). YanqiZhou,WeiLi,andPeterJ.Liu.2020. Ex-
ploringthelimitsoftransferlearningwithauni-
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, fiedtext-to-texttransformer. JournalofMachine
Mandar Joshi, Danqi Chen, Omer Levy, Mike LearningResearch,21(140):1–67.
Lewis,LukeZettlemoyer,andVeselinStoyanov.
2019. RoBERTa: ARobustlyOptimizedBERT RaphaelScheible,FabianThomczyk,PatricTipp-
PretrainingApproach. CoRR,abs/1907.11692. mann,VictorJaravine,andMartinBoeker.2020.
Gottbert: apuregermanlanguagemodel.CoRR,
IlyaLoshchilovandFrankHutter.2019. Decoupled abs/2012.02110.
weightdecayregularization. In7thInternational
ElisaTerumiRubelSchneider,JoãoVitorAndrioli
ConferenceonLearningRepresentations,ICLR
deSouza,JulienKnafou,LucasEmanuelSilvae
2019, New Orleans, LA, USA, May 6-9, 2019.
Oliveira,JennyCopara,YohanBonesckiGumiel,
OpenReview.net.
LucasFerroAntunesdeOliveira,EmersonCabr-
era Paraiso, Douglas Teodoro, and Cláudia
Luise Modersohn, Stefan Schulz, Christina Lohr,
Maria Cabral Moro Barra. 2020. BioBERTpt -
andUdoHahn.2022.GRASCCO—thefirstpub-
a Portuguese neural language model for clini-
liclyshareable,multiply-alienatedgermanclinical
cal named entity recognition. In Proceedings
textcorpus. InStudiesinHealthTechnologyand
ofthe3rdClinicalNaturalLanguageProcessing
Informatics.IOSPress.
Workshop,pages65–72,Online.Associationfor
Nathan Ng, Kyra Yee, Alexei Baevski, Myle Ott, ComputationalLinguistics.
MichaelAuli,andSergeyEdunov.2019. Face-
YuqingTang,ChauTran,XianLi,Peng-JenChen,
bookFAIR’sWMT19newstranslationtasksub-
Naman Goyal, Vishrav Chaudhary, Jiatao Gu,
mission. In Proceedings of the Fourth Confer-
and AngelaFan.2021. Multilingual translation
enceonMachineTranslation(Volume2: Shared
from denoising pre-training. In Findings of the
TaskPapers,Day1),pages314–319,Florence,
AssociationforComputationalLinguistics: ACL-
Italy.AssociationforComputationalLinguistics.
IJCNLP2021,pages3450–3466,Online.Asso-
ciationforComputationalLinguistics.
AurélieNévéol,HerculesDalianis,SumithraVelupil-
lai,GuerganaSavova,andPierreZweigenbaum.
NLLBTeam,MartaR.Costa-jussà,JamesCross,
2018. Clinical natural language processing in
OnurÇelebi,MahaElbayad,KennethHeafield,
languagesotherthanenglish: opportunitiesand
Kevin Heffernan, Elahe Kalbassi, Janice Lam,
challenges. Journal of biomedical semantics,
Daniel Licht, Jean Maillard, Anna Sun, Skyler
9(1):12.
Wang,GuillaumeWenzek,AlYoungblood,Bapi
Akula, Loic Barrault, Gabriel Mejia Gonza-
Pedro Javier Ortiz Suárez, Laurent Romary, and
lez, Prangthip Hansanti, John Hoffman, Se-
Benoît Sagot. 2020. A Monolingual Approach
marley Jarrett, Kaushik Ram Sadagopan, Dirk
to Contextualized Word Embeddings for Mid-
Rowe, Shannon Spruit, Chau Tran, Pierre
Resource Languages. In Proceedings of the
Andrews, Necip Fazil Ayan, Shruti Bhosale,
58thAnnualMeetingoftheAssociationforCom-
Sergey Edunov, Angela Fan, Cynthia Gao,
putationalLinguistics,pages1703–1714,Online.
VedanujGoswami,FranciscoGuzmán,Philipp
AssociationforComputationalLinguistics.
Koehn,AlexandreMourachko,ChristopheRop-
ers,SafiyyahSaleem,HolgerSchwenk,andJeff
Malte Ostendorff, Till Blume, and Saskia Osten-
Wang.2022. Nolanguageleftbehind: Scaling
dorff.2020. TowardsanOpenPlatformforLegal
human-centeredmachinetranslation.
Information. In Proceedings of the ACM/IEEE
Joint Conference on Digital Libraries in 2020, Jörg Tiedemann. 2012. Parallel Data, Tools and
JCDL’20, page 385–388, NewYork, NY,USA. Interfaces in OPUS. In Proceedings of the
AssociationforComputingMachinery. Eighth International Conference on Language
Resources and Evaluation (LREC’12), pages
PengQi,YuhaoZhang,YuhuiZhang,JasonBolton,
2214–2218, Istanbul, Turkey. European Lan-
andChristopherD.Manning.2020. Stanza: A
guageResourcesAssociation(ELRA).
python natural language processing toolkit for
manyhumanlanguages. InProceedingsofthe Jörg Tiedemann and Santhosh Thottingal. 2020.
58thAnnualMeetingoftheAssociationforCom- OPUS-MT—Buildingopentranslationservices
putationalLinguistics: SystemDemonstrations, for the World. In Proceedings of the 22nd An-
pages101–108,Online.AssociationforCompu- nualConferenecoftheEuropeanAssociationfor
tationalLinguistics. MachineTranslation(EAMT),Lisbon,Portugal.Ashish Vaswani, Noam Shazeer, Niki Parmar,
JakobUszkoreit,LlionJones,AidanN.Gomez,
Lukasz Kaiser, and Illia Polosukhin. 2017. At-
tention is All you Need. In Advances in Neural
InformationProcessingSystems30: AnnualCon-
ferenceonNeuralInformationProcessingSys-
tems 2017, December 4-9, 2017, Long Beach,
CA,USA,pages5998–6008.
Guillaume Wenzek, Marie-Anne Lachaux, Alexis
Conneau, Vishrav Chaudhary, Francisco
Guzmán, Armand Joulin, and Edouard Grave.
2020. CCNet: Extracting high quality mono-
lingual datasets from web crawl data. In
Proceedings of the 12th Language Resources
and Evaluation Conference, pages 4003–
4012, Marseille, France. European Language
ResourcesAssociation.
Torsten Zesch and Jeanette Bewersdorff. 2022.
Germanmedicalnaturallanguageprocessing–a
data-centricsurvey. InTheUpper-RhineArtificial
IntelligenceSymposiumUR-AI2022: AIApplica-
tionsinMedicineandManufacturing,19October
2022,Villingen-Schwenningen,Germany,pages
137–145.FurtwangenUniversity.