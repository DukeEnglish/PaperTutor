Preprint
A Large-Scale Exploration of µ-Transfer
LucasDaxLingle
lucasdaxlingle@gmail.com
Abstract
Largeneuralnetworkmodelshavebecomeamainstayofnaturallanguage
processingandcomputervision,yettheirinitializationandlearningrates
aresetinalargelyheuristicfashion,potentiallyvaryingfrompapertopa-
per and one model size to the next. The µ-Parameterization (µP) offers
a potential solution to these challenges, yielding scaling rules for model
initializationandlearningrates,andreportedlyenablingzero-shothyper-
parametertransferfromsmalltolargemodelsinavarietyofcases.
Despite the evident promise, the µP scaling rules are not yet widely
adopted, perhaps due to higher implementation complexity, many vari-
ations,orcomplextheoreticalbackground. ThisworkinvestigatesµPem-
pirically, focusing on the ubiquitous transformer architecture, and aims
toanswerasimplequestion: doesµ-Transferyieldoptimallearningrates
in practice? From models with 2M to 10B parameters, we show that µ-
Transfer works as intended for the majority of important cases, but also
identifysomesurprisingcaseswhereitmaynot.
1 Introduction
Despite emergence of transformers as the dominant architecture for language and vi-
sion (OpenAIetal., 2024; Anthropic, 2024; GeminiTeametal., 2023; Reidetal., 2024;
Touvronetal., 2023a;b; Jiangetal., 2024; Parmaretal., 2024; Dehghanietal., 2023;
Liuetal., 2023), there is still no universal method for setting their initialization, learning
rate, or architectural hyperparameters. Further, the hyperparameters selected for large
models might be far from optimal due to the expense of conducting hyperparameter
sweepsatscale.
The µ-Paramerization (µP) (Yang&Hu, 2021; Yangetal., 2022; 2023b) offers a general
method for scaling initializations and learning rates, based on a Gaussian Process inter-
pretation of deep neural networks. Empirically, µP is also reported to enable zero-shot
hyperparametertransferfromsmallproxymodelstolargetargetmodels(Yangetal.,2022;
2023b),usingwidthasthedirectionofscaling. This‘µ-transfer’techniqueoffersapromise
ofstabletrainingandoptimalhyperparametersatscalewithlowexpense.
However,whiletheinitialreportonµ-transferdemonstratedapproximatepreservationof
hyperparameter optima, it was only shown at a relatively small scale (Yangetal., 2022),
with the sole large-scaleexperiment being oriented as a benchmark. As a result, there is
alackofconvincingempiricalevidencethathyperparameteroptimaarepreservedunder
µ-transferwhentargetmodelisverylarge. Inthisabsence,itseemspossibletheoptimum
coulddriftorjump,astransfercouldbedisruptedbyemergentoutliers(Dettmers,2022).
Asecondopenquestionisifµ-transferiscompatiblewiththetechniquesusedinpractice,
suchasdecoupledweightdecay(Loshchilov&Hutter,2017)ormultiplicativenonlineari-
ties (Shazeer, 2020; Soetal., 2021). While the initial reportaims to delineate the compati-
bletechniques, thereisaneedforfurtherexplorationandempiricalverification. Perhaps
awaitingverification,manyrecentlargemodelshavenotreportedlyusedµ-transfer.
A few recentworks have adopted µP (Deyetal., 2023; Huetal., 2024; XAI, 2024), but do
notsettletheopenquestionsabove;suchaninvestigationwouldrequireextensivehyper-
parametersweepsatscale. Inspiredbythese works, thispaperaimstoshedfurtherlight
onµ-transfer,studyingitsreliabilityontransformermodelsfrom2Mto10Bparametersin
avarietyofsettings.
1
4202
rpA
8
]GL.sc[
1v82750.4042:viXraPreprint
ExperimentGroup Width BaseLR Transfer
2 10 2 8 2 6 2 4 2 2
− − − − −
128 3.846 3.743 3.695 3.884 4.143
BaselineµP 512 3.114 2.993 2.953 3.221 3.506 ✔
2048 2.711 2.553 2.511 2.563 3.244
128 3.838 3.735 3.705 3.911 4.269
ProjectionBiases 512 3.108 2.986 2.947 2.970 3.557 ✔
2048 2.710 2.552 2.529 2.672 3.418
128 3.836 3.743 3.694 3.877 4.167
ZeroQueryInit 512 3.115 2.992 2.949 3.135 3.532 ✔
2048 2.711 2.553 2.510 2.551 3.272
128 3.861 3.765 3.699 3.896 4.161
SPUnembeddingInit 512 3.119 2.990 2.951 3.265 3.582 ✔
2048 2.716 2.554 2.509 2.564 7.471
128 3.846 3.743 3.695 3.906 4.143
CosineSchedule 512 3.114 2.995 2.955 3.225 3.506 ✔
2048 2.712 2.558 2.518 2.572 3.244
128 3.834 3.743 3.693 4.012 4.120
EmbeddingNormalization 512 3.115 2.993 2.954 3.028 3.506 ✔
2048 2.710 2.553 2.512 2.564 7.316
128 3.800 3.740 3.715 4.090 7.024
SwiGLUNonlinearity 512 3.070 2.975 2.953 3.175 6.863 ✔
2048 2.677 2.536 2.505 2.553 4.571
128 3.808 3.735 3.686 3.999 4.484
SquaredReLUNonlinearity 512 3.071 2.964 2.929 3.184 7.299 ✔
2048 2.666 2.516 2.482 2.532 3.259
128 3.811 3.708 3.667 3.881 4.121
Multi-QueryAttention 512 3.101 2.979 2.940 3.187 3.518 ✔
2048 2.715 2.564 2.521 2.546 3.257
128 3.844 3.735 3.697 3.716 10.380
4xLargerBatch 512 3.141 2.990 2.965 3.305 10.373 ✔
2048 2.745 2.556 2.541 2.697 7.197
128 3.855 3.774 3.736 3.945 4.104
4xSmallerBatch 512 3.120 3.011 2.977 3.024 3.521 ✔
2048 2.714 2.568 2.527 2.549 3.223
128 3.842 3.744 3.689 3.670 3.681
RMSNormGains(Vector) 512 3.101 2.992 2.951 2.950 3.412 ✗
2048 2.692 2.553 2.609 2.605 3.169
128 3.843 3.749 3.692 3.670 4.471
RMSNormGains(Scalar) 512 3.106 3.000 2.961 2.959 3.515 ✗
2048 2.704 2.570 2.525 2.542 3.334
128 3.836 3.758 3.905 4.140 4.597
SPAttentionScale 512 3.104 2.993 2.962 3.449 4.184 ✗
2048 2.706 2.555 2.525 3.306 7.280
128 3.760 3.679 3.694 3.741 4.011
DecoupledWeightDecay 512 3.057 2.963 2.957 3.139 3.373 ✗
2048 2.686 2.535 2.502 3.123 6.594
128 3.708 3.736 4.057 4.344 10.380
LionOptimizer 512 2.952 2.947 3.416 3.961 10.285 ✗
2048 2.519 2.511 3.151 10.377 10.377
Table1: µ-transferresultsfortransformers. Thelargestmodelsizehereis1.2Bparameters
andallmodelstrainfor33Btokens. SeeSection4fordetails.
2Preprint
2 Background andNotation
This paper focuses on decoder-only transformer models, which process sequences of to-
kens z 0,...,V 1 C, whereV iscalledthevocabularysizeand C thecontextlength.
∈ { − }
This architecture has three components: embeddings, transformer blocks, and unembed-
dings. Wedescribeapre-normtransformerdecoder(Radfordetal.,2019)ofdepthL.
2.1 Embeddings
Thetokensequencez 0,...,V 1 C isusedtoindexintoanembeddingmatrixWE
∈ { − } ∈
RV M, where M iscalledthe modelwidth. Theresultingreal-valuedvectorsarewritten
×
asrowstoanactivationmatrixX0 RC M accordingtotheformulaX0 =WE.
∈ × i zi
2.2 TransformerBlocks
A transformer block consists of two residual blocks (Heetal., 2016), denoted MHA and
MLP,whichareaddedtoa‘residualstream’accordingtotheformula
Xℓ =Xℓ −1+MHA(Xℓ −1)+MLP(Xℓ −1+MHA(Xℓ −1)). (1)
TheMHAresidualblockperformsmulti-headself-attention,definedbyaheadwidthD
N and number of heads H N. For each head, MHA uses a distinct set of projection∈ s
∈
WAQ,WAK,WAV RM D toperformthefollowingcomputationsgiveninputX RC M:
× ×
∈ ∈
Y =LayerNorm(X) (2)
Q=YWAQ (3)
K =YWAK (4)
V =YWAV (5)
S =τ−1QK⊤+M (6)
P =Softmax(S) (7)
O=PV (8)
LayerNorm and Softmax are applied row-wise, τ 1 > 0 is an scalar constant commonly
−
setto1/√D,andMisacausalmaskgivenbyM = ∞ifi < jandM = 0otherwise.
i,j i,j
−
The heads’outputs Oareconcatenatedtogether, andthenprojectedusing oneadditional
matrix WAO RHD M toformthe residualMHA(X). Thisresidualissummed onto the
×
∈
residualstreamasinEquation1,andthesumisprocessedbytheMLPresidualblock.
The MLP residual block applies a multi-layer perceptron to each row individually. It is
definedviahiddenwidthFandelement-wiseactivationφ.Itusestwotrainableprojections
WFI RM FandWFO RF M. GivenaninputtensorX,itdefinestheresidual:
× ×
∈ ∈
Y =LayerNorm(X) (9)
O=φ(YWFI)WFO (10)
Thisresidualislikewisesummedontotheresidualstream,followingEquation1.
2.3 Unembedding
The unembeddinglayerusesamatrix WU RM V toproducethe probabilitiesfornext-
×
∈
tokenprediction. Thelayer’sinputistheresidualstreamoutputXL,anditsoutputis
Y=LayerNorm(XL) (11)
U=Softmax(YWU) (12)
Due to the softmax, each row of U RC V defines a valid probability mass func-
×
∈
tion over tokens in the vocabulary. The model is trained on the cross-entropy loss
−C1 ∑C i=−01logU i,zi+1.
3Preprint
3 µ-Transfer
Theµ-Parameterization(µP)(Yang&Hu,2021;Yangetal.,2022;2023b)referstoaspecific
familyof initializations and learningratesthatreportedlyallow hyperparametertransfer
from small to large models. This paper investigates µP for transformers with respect to
width. WedonotconsiderdepthwiseµP(Yangetal.,2023b)becauseitrequiresonelinear
layerperresidualblock,whiletransformersrequireatleasttwo.
The generalformulation of µP when training with Adam(Kingma&Ba,2014)and using
an i.i.d. Gaussian initialization is given by Yangetal. (2022). The first three columns of
Table 2 display these rules for transformers. Note these columns use big-theta notation.
Formally, f(x) = Θ(g(x))ifthereexistsx Randc,C >0s.t. cg(x) f(x) Cg(x)for
0
allx > x . ∈ ≤ ≤
0
Param InitVariance(Θ) AdamLR(Θ) InitVariance(Exact) AdamLR(Exact)
WE 1 1 1 α
WAQ 1/M 1/M 1/M αP/M
WAK 1/M 1/M 1/M αP/M
WAV 1/M 1/M 1/M αP/M
WAO 1/(HD) 1/(HD) 1/M αP/M
WFI 1/M 1/M 1/M αP/M
WFO 1/F 1/F 0.25/M αP/M
WU 1/M2 1/M 1/M2 αP/M
Table2: µPscalingrulesfortransformers.
Intheremainderofthispaper,weassume HD = Mand F = 4M. Inourexperimentswe
fixaproxymodelwidthP = 128andheadwidthD = 128,andfollowthespecificscaling
rulesinthelasttwocolumnsofTable2,whereαdenotesthebaselearningrate,sonamed
becauseitisthelearningrateforallparameterswhen M= P.
Inaddition,µPusesanattentionscaleofτ 1 = Θ(1/D)insteadoftheusualτ 1 =1/√D.
− −
For simplicity, we use τ 1 = 1/D, since in preliminary experiments we observed only
−
a small improvement from using smaller multiples of 1/D. Note that for D fixed across
model widths M, any constant τ 1 = 0 technically complies with µP (Yangetal., 2022);
−
6
however, in the experiments, τ 1 will be shown to have a major impact on performance
−
andtransfer.
It is also possible to add scalar multipliers throughout the network as hyperparameters.
Includingthesemultiplierswouldbesimilartoalteringthelearningratesbyconstantsfor
particularweighttensors. Forsimplicity, wefocuson µ-Transferof thebaselearningrate
α.
4 Experiments
4.1 ExperimentalSetup
4.1.1 Implementation
Our experiments are implemented using Jax/Flax. Training is performed on TPU pod
slices, using the fully-shardeddata parallelism(FSDP) strategy fromXuetal. (2021) (‘2D
finalized’) to reduce memory overhead if necessary. Models train on the Colossal Clean
CommonCrawl(C4)dataset,usingtheT5tokenizer(Raffeletal.,2019)withcontextlength
C =256.
4Preprint
The experiments use a bitwise-deterministic training pipeline, with shards of data writ-
ten to disk in a random-access format similar to Nvidia Megatron (Shoeybietal., 2020).
Distributed model checkpoints aresaved periodically, and the reportedvalidation loss is
computed onthe best-performingcheckpoint. The same seedisused forallexperiments;
duetotimeconstraints,eachexperimentisrunonce.
4.1.2 Configuration
Weusethefollowingdefaultconfiguration,deviatingfromitonlyifspecificallymentioned.
ThedepthisfixedatL =24,andweconsidermodelwidthsM 128,512,2048 ,yielding
∈ { }
threemodelsizesrangingfrom4.7Mto1.2Bnon-embeddingparameters. Theheadwidth
is fixed at D = 128, the number of heads is H = M/D, and the MLP hidden width is
F = 4M. Weuse RMSLayerNormwithout gains(Zhang&Sennrich,2019), linear projec-
tionswithoutbiases(Raffeletal.,2019),rotarypositionencodingonthequeriesandkeys
(Suetal.,2021),andrelufortheMLPnonlinearity(Vaswanietal.,2017;Raffeletal.,2019).
Bydefault,weuse218tokensperbatch,float32parameters,bfloat16activations/gradients1.
TheoptimizerisAdamW(Loshchilov&Hutter,2017)with β = 0.9,β = 0.98,ǫ = 10 9,
1 2 −
withdefaultweightdecay0.0andgradientclip1.0.Modelstrainfor125Kstepstotal,with
10Kstepsoflearningratewarmupfollowedbylineardecaytozero.
4.2 PrimaryExperiments
Inourprimaryexperiments,wesweepoverbaselearningrateα 2 2j : j N,1 j
−
∈ { ∈ ≤ ≤
5 foreachmodelsizeandexperimentsetting,andwereportallresults. Thisallowsusto
}
investigateimpactofvariousexperimentalconditionsonmodelqualityandtransferability
oflearningratesunderthe µPscalingrules. We focusonlearningratetransferbecauseit
isthemainhyperparameterofinterestfortransformerLLMs.
4.2.1 Baseline
In our first experiment group, we establish baselines for model quality to compare the
other experiment groups to. In addition, we verify that µ-Transfer works reliably even
withmixed-precisiontraining. Forthispurpose,weutilizetheGoogleBrainfloatingpoint
format, bfloat16, for the activations and gradients. This format is supported by Google
TPUsandrecentNvidiaGPUs,andwasusedwithµPinacontemporarywork(Deyetal.,
2023).
Asshown in Table 1, the learningratestransfer reliablyacrossmodelsizes under µP. De-
spiteeachmodelbeing4xwider(and16xlarger)thanthelast,thesmallestmodel’soptimal
baselearningrateαdirectlypredictstheoptimuminoursweepsforthelargermodels.
4.2.2 ProjectionBiases
Itisnotaprioricleariftrainablebiasvectorsinlinearlayersarebeneficialformodelqual-
ity,andseveralpriorworksomitthem(Raffeletal.,2019;Shazeer,2020;Chowdheryetal.,
2023). Here,wetesttheirbenefitandimpactonlearningratetransferabilityunderµP.
As shown in Table 1, the learning rates appear to transfer across model sizes under µP.
However,forthesmallestandlargestmodel,biasesdonotappeartoimprovequalityver-
susthebaselinewhenthelearningrateisoptimal.
4.2.3 RMSNormGains
It is not a priori clear if trainable scale vectors (‘gains’) in RMSNorm (Zhang&Sennrich,
2019)arebeneficialformodelqualityandmanyframeworksoffertheoptiontoomitthem.
This ablation tests their benefit and impact on learning rate transferability under µP. We
1Duringevaluation,outputlogitsarecomputedinfloat32.
5Preprint
alsotestavariantwherethetrainablegainvectorisreplacedwithatrainablescalarmulti-
plier,similartoElhageetal.(2023).
AsshowninTable1,optimallearningratesforthesemodelsdonotreliablytransferwhen
usingΘ(1)learningratescalingforthegains,despitethefactthatthe‘coordinatesize’of
thefeaturesbeforeandafterRMSNormalizationisΘ(1)w.r.t.widthbydesign.Inaddition
tothelackoftransferintheseexperiments,wefindtrainablegainsharmthequalityofthe
largestµPmodelswhenthebaselearningrateαisoptimal.
4.2.4 QueryInitialization
TheusualµPinitializationforqueryprojectionsWAQ isGaussianwithvarianceΘ(1/M).
One alternative is to use zero-initialized query projections, which yield equal attention
weights over all past timesteps at initialization. This change was recommended by
Yangetal.(2022)toimprovetransfer,soweinvestigateitseffectsaswell.
As shown in Table 1, the learning rates transfer across model sizes when using µP with
zero-initializedqueryprojections. Thereisalsoaslightyetconsistentimprovementinloss.
4.2.5 SPAttentionScale
Theusualattentionscaleτ 1 =1/√DwasfirstproposedbyVaswanietal.(2017)andgen-
−
erallyusedsince. However,µPproposesτ 1 = Θ(1/D),andweuseτ 1 =1/D. Notably
− −
inour experimentswe scalethe modelwidth M andkeepthe headwidth D fixed across
modelsizes,sotheattentionscaleshouldnotactuallymatterforpurposesoftransfer;any
differencebetween 1/√D and 1/D can be treated as a constant multiplier. Nonetheless,
weinvestigatetheeffectofusingthestandard1/√Dattentionscale.
AsshowninTable1,theSPattentionscale1/√Dappearsquitesuboptimal,harmingper-
formance relative to the baselines across all model sizes. Interestingly, this 11.3 larger
×
attention scale actually prevented transfer of the optimal learning rate, despite actually
usingaconstantattentionheadwidthD =128acrossmodels. Giventhisresult,ourrecom-
mendationistouseτ 1 1/Difapplyingµ-transferwithsmallproxymodels.
−
≤
4.2.6 SPUnembeddingInitialization
TheµPinitializationforunembeddingmatrixWU isaGaussiandistributionwithvariance
Θ(1/M2),whiletheso-calledstandardparameterization(SP)uses1/M(Yangetal.,2022).
Wethusablatetheimpactofusingthestandardinitializationonperformanceandtransfer.
As shown in Table 1, despite using SP initialization for the unembedding projection, the
learningratesempiricallytransferacrossmodelsizes. Therealsoappearstobeasmallbut
consistentimprovementfromusingtheSPinitializationfortheunembeddingprojection.
4.2.7 CosineSchedule
Itisnotimmediatelyclearwhetherthelinearlearningratescheduleinthebaselinesettings
istheoptimalchoice. Inthisablation,weinsteaduseacosineschedule,decayingtozero.
As shown in Table 1, the learning rates transfer across model sizes, showing that µP is
compatiblewithacosinescheduleaswellasthelinearscheduleusedbythebaseline. Our
tentativeconclusionisthatthescheduleisunlikelytointerferewithlearningratetransfer.
4.2.8 WeightDecay
DecoupledweightdecayforAdam(Loshchilov&Hutter,2017)isoftenusedtotraintrans-
formers. IncommonlibrariessuchasPytorchandOptax,itsdecayrateλismultipliedby
thecurrentlearningrateinsteadofjusttheschedulemultiplier,althoughthelatterwasre-
centlyinvestigatedbyWortsmanetal.(2023)underthename‘independentweightdecay’.
We focus on the version used by these libraries, as we found the ‘independent’ variant
6Preprint
inevitablyledtoinstabilityinthelargermodelsnotseeninthesmallerones–sincetheopti-
mallearningratetendedtodecreasewithmodelsize,evenwhileλstayedconstant.
AsshowninTable1,whenusingdecoupledweightdecay,theoptimalbaselearningrate
α maynotalwaystransferfromthesmallestmodeltothelargerones. Ontheotherhand,
theseexperimentsusethestrongesttypicalsettingatλ=0.1,andyettheoptimumforαis
notvisiblychangedbetweenthetwolargermodels. Furthermore,thisoptimumforαalso
matchestheoptimumforthebaselines,whichisthesameacrossmodelsizes.
Basedontheseobservations,decoupledweightdecayappearsunlikelytoaltertheoptimal
learningrateforthetargetmodelsignificantlywhenλ 0.1and M P,soonestrategy
≤ ≫
wouldbetoomititwhentrainingtheproxymodel,andapplyittothetargetmodelonly.
4.2.9 EmbeddingNormalization
Weconsiderusingnormalizedembeddingsfollowing(Nguyen&Salazar,2019;Pengetal.,
2023; Gemma, 2024), using RMSNorm without trainable gains (Zhang&Sennrich, 2019).
WedonotchangethelearningratefromthesettinginTable2noradjusttheinitialization.
As shown in Table 1, the optimal learning rate transfers across models using µP; how-
ever, the improvement in model quality over the baseline is negligible. We briefly in-
vestigated using lower initialization variances, but found this harmed stability with the
width-constant embedding learning rate of µP Adam, possibly due to similar effects
(vanLaarhoven,2017).
4.2.10 MultiplicativeNonlinearities
Multiplicative nonlinearities such as SwiGLU (Shazeer, 2020) and Squared ReLU
(Soetal., 2021) are increasingly used in the MLP blocks to improve transformer quality
(Touvronetal., 2023a;b; Elsenetal., 2023; Pengetal., 2023; Parmaretal., 2024). In this
experiment, we investigate both of the aforementioned nonlinearities, which are notably
‘superlinear’ and thus may create outliers that interfere with µ-Transfer, as discussed by
Yangetal.(2022). ForSwiGLU,weuseF =5M,sotheMLPhas7.5M2parameters.
AsshowninTable1,theSwiGLUandSquaredReLUnonlinearitiesbothallowµ-transfer
ofthelearningrateacrossmodelsizes. TheoutcomesherecontrastnicelywithRMSNorm
gainsexperiments,sincetransferoccursdespitethemultiplicativeinteractions.
4.3 LionOptimizer
We empirically investigate if the Lion optimizer (Chenetal., 2023b;a) is compatible with
µ-Transfer.Thisoptimizerisatleasttwiceasmemory-efficientastheAdamoptimizer,and
wasreportedtoyieldmodelsofsimilarquality,includingtransformers(Chenetal.,2023b).
A notable property of this optimizer is that its updates are constrained to 1,+1 per
coordinate,yieldingacoordinatesizeofΘ(1)perstep. Asaresult,aΘ(1/M){− transfer} rule
forweightlearningrates,similartoµPAdam,mightbeappropriate(Yangetal.,2022).
As shown in Table 1, the Lion optimizer did not admit transfer of the base learning rate
fromthe smallest modelsize. The scaling rulesdo appearto preservethe optimum α be-
tweenlargermodels,butfurtherinvestigationsbeyondthescopeofthisprojectareneeded.
4.4 Multi-QueryAttention
Multi-QueryAttention(Shazeer,2019)anditsgroupedgeneralization(Ainslieetal.,2023)
areincreasinglyused intransformer LLMs(Chowdheryetal.,2023; Touvronetal., 2023b;
Almazroueietal.,2023;GeminiTeametal.,2023;Jiangetal.,2024). Thesetechniquesaim
to improve the inference speed of transformers by sharing keys/values across multiple
heads.ThisablationinvestigatestheimpactofthesharedK/Vcacheonµ-Transfer.Similar
to Shazeer(2019), we approximatelycorrectfor the change in parametersby setting F =
5M.
7Preprint
AsshowninTable1,multi-queryattentioniscompatibleµ-Transfer.
4.4.1 4xLargerBatch
Large-batch training can reduce wall time, but may also have a considerable influence
on the training dynamics (McCandlishetal., 2018; Youetal., 2019). In this section, we
consider scaling up the batch size by 4 while keeping the number of training tokens
×
the same. For thisablation, we adoptthe learningratescalingrule fromYouetal.(2019);
Malladietal.(2022),sothateachformulainTable2isscaledby2 .
×
AsshowninTable1,the4 largerbatchsizeadmitstransferofthelearningrateviaµP.
×
4.4.2 4xSmallerBatch
Animportantquestion isif µ-Transferrequiresaminimum batchsizetowork(Deyetal.,
2023). In this section, we consider scaling the batch size down by 4 , while keeping the
×
number of training tokens the same. For this ablation, we again adopt the learning rate
scaling rule from Youetal. (2019); Malladietal. (2022), so that eachformula in Table 2 is
scaledby0.5 .
×
AsshowninTable1,the4 smallerbatchsizeadmitstransferofthelearningrateviaµP.
×
4.5 Large-ScaleTransferExperiment
Params Width BaseLR
2 8 2 6 2 4
− − −
2M 128 3.791 3.766 3.814
40M 512 3.016 2.983 3.004
600M 2048 2.513 2.459 2.466
10B 8192 2.238 2.167 2.169
Table3: Ourlargest-scaleµ-transferresults.
In this experiment, we combine the architectural choices that both transferred and im-
proved performance. Then, we investigate if µ-Transfer continues to work as desired at
scale.
We use depth L = 12 and consider widths M 128,512,2048,8192 , yielding models
∈ { }
with approximately 2M, 40M, 600M, and 10B non-embedding parameters, respectively.
Weusezero-initializedqueries(Yangetal.,2022)andSquaredReLUnonlinearity(Soetal.,
2021). We use 221 tokens per batch, training for 90K steps. We use the Adam optimizer
(Kingma&Ba,2014)with β
1
= 0.9,β
2
= 0.95,ǫ = 10−8,decoupledweightdecay0.1,gra-
dientclip1.0.
As shown in Table 3, the optimal learning rate transfers directly from a model 5000
×
smaller. Thisshowsthatµ-Transfercontinuestopredicttheoptimallearningrateatscale.
Moreover,theoutcomesuggeststhat‘emergentoutliers’arenotasourceofinterferencein
µ-Transfer,giventhatthesereportedlyappearataround7Bparameters(Dettmers,2022).
5 Related Works
Theµ-Parameterization(µP)isproposedaspartoftheTensorProgramsseries(Yang,2019;
2020;2021;Yang&Hu,2021;Yangetal.,2022;Littwin&Yang,2023;Yangetal.,2023b;a).
Theempiricaldemonstrationofzero-shothyperparametertransferunderµPwasgivenin
Yangetal. (2022). Notably, the largest model trained in that report had 6.7B parameters.
It used FP32 computation for numerical stability, aswell asusing a differentposition en-
codingmechanismandadifferentlearningrateschedulethantheFP16baseline. Thisled
8Preprint
toanopenquestionwhether µ-Parameterizedtransformerscouldbemadestableatlarge
scale. Moreover,thelarge-scaleexperimentdidnotdemonstratethatµ-Transferpredicted
thehyperparameteroptimatothe6.7Btargetmodel.
SomerecentworkshaveadoptedµPforhyperparametertuning(Deyetal.,2023;Huetal.,
2024; XAI, 2024), but did not provide any evidence that the hyperparameter optimum is
preserved under µ-Transfer when the target model is very large. Furthermore, Deyetal.
(2023)trainedasuiteofmodels,butonlyusedµPtransformerswithupto2.7Bparameters,
whiletheirlargestmodelusingastandardparameterizationhad13Bparameters. Thisleft
openthequestionofwhetherµ-Transferworkedreliablyonlarger-scaletargetmodels.
Some recent works suggest to use µ-Transfer to avoid large-scale experiments entirely
(Yao&Wang, 2023; Fanetal., 2024). These works use power laws to predict the loss for
largermodelsfromsmallerones(Kaplanetal.,2020;Hoffmannetal.,2022;Soetal.,2021),
and use µP for hyperparameter tuning when fitting the smaller reference models. How-
ever, similar to the other works, these papers do not study whether µ-Transfer correctly
predictstheoptimalhyperparametersthemselveswhenthetargetmodelisverylarge.
Apotential alternativeto µP wasrecentlyproposed byDeepSeek-AIetal. (2024), namely
a scaling law for the optimal learning rate solely in terms of compute budget. However,
empirically-derived scaling laws are strongly affected by the choice of independent vari-
ablesand the datathey werefitted on, so the fitted scaling lawmay not transfer toother
setups.
Otherrecentworkhasinvestigatedtransformertraininginstabilitiesviasmallproxymod-
els(Wortsmanetal.,2023),proposingarchitecturaladjustmentsthatreducethelosssensi-
tivitytothelearningrate,ratherthanpredicttheoptimalone;theirmethodnotablyapplies
to depthwise scaling of transformermodels. Automatic GradientDescent andHypergra-
dients(Bernsteinetal.,2023;Baydinetal.,2017;Chandraetal.,2019)aremethodstotune
thelearningrateofoptimizersduringtraining.Thesemethodshaveahighimplementation
complexityandmightalsoincuraperformancehitversusvanillahyperparametertuning,
whichµPmakesaffordableviaproxymodels.
6 Conclusion
Thispaperstudiedthereliabilityofµ-Transferofthelearningrate,focusingontransform-
ers. Inourexperiments,µ-Transferworkedasdesiredinmostofourexperiments,includ-
ing with multiplicative nonlinearities, multi-query attention, and large and small batch
training. However, µ-Transfer did not work when using trainable scale parameters, or
when using too large an attention scale (even when the attention head width was held
constant).
Inaddition,wealsofoundthatµ-Transferfroma2Mparametermodelpredictedtheopti-
mallearningratefromasweepatthescaleof10Bparameters.Tothebestofourknowledge,
thisisthelargesttargetmodelwherethispropertyhasbeenverified. Wehopethesefind-
ings arehelpfulto the researchcommunity, and inspire further work on hyperparameter
transfer.
Acknowledgments
TheauthorthanksOlegFilatov,StellaBiderman,LucasNestler,andHaileySchoelkopffor
helpfulremarksduringthe projectandOleg forfeedbackonthe manuscript. The author
is grateful to Google’s TPU Research Cloud for supporting the experiments with Cloud
TPUs.
References
Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebro´n,
and Sumit Sanghai. GQA: Training generalized multi-query transformer models from
multi-headcheckpoints,2023. URLhttps://arxiv.org/abs/2305.13245.
9Preprint
Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli,
Ruxandra Cojocaru, Me´rouane Debbah, E´tienne Goffinet, Daniel Hesslow, Julien
Launay, Quentin Malartic, Daniele Mazzotta, Badreddine Noune, Baptiste Pannier,
and Guilherme Penedo. The falcon series of open language models, 2023. URL
https://arxiv.org/abs/2311.16867.
Anthropic. The claude 3 model family: Opus, sonnet, haiku.
https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf,
2024. Accessed:2024-03-13.
Atilim Gunes Baydin, Robert Cornish, David Martinez Rubio, Mark Schmidt, and Frank
Wood. Online learning rate adaptation with hypergradient descent, 2017. URL
https://arxiv.org/abs/1703.04782.
Jeremy Bernstein, Chris Mingard, Kevin Huang, Navid Azizan, and Yisong Yue. Au-
tomatic gradient descent: Deep learning without hyperparameters, 2023. URL
https://arxiv.org/abs/2304.05187.
Kartik Chandra, Audrey Xie, Jonathan Ragan-Kelley, and Erik Meijer. Gradient descent:
Theultimateoptimizer,2019. URLhttps://arxiv.org/abs/1909.13371.
Lizhang Chen, Bo Liu, Kaizhao Liang, and Qiang Liu. Lion secretly solves constrained
optimization: AsLyapunovpredicts,2023a. URLhttps://arxiv.org/abs/2310.05898.
Xiangning Chen, Chen Liang, Da Huang, Esteban Real, Kaiyuan Wang, Yao
Liu, Hieu Pham, Xuanyi Dong, Thang Luong, Cho-Jui Hsieh, Yifeng Lu, and
Quoc V. Le. Symbolic discovery of optimization algorithms, 2023b. URL
https://arxiv.org/abs/2302.06675.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,
AdamRoberts,PaulBarham,HyungWonChung,CharlesSutton,SebastianGehrmann,
ParkerSchuh,KensenShi,SashaTsvyashchenko,JoshuaMaynez,AbhishekRao,Parker
Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben
Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari,
Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Hen-
ryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny
Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiri-
donov,RyanSepassi,DavidDohan,ShivaniAgrawal,MarkOmernick,AndrewM.Dai,
Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,
Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Bren-
nan Saeta, MarkDiaz, OrhanFirat, Michele Catasta, JasonWei, Kathy Meier-Hellstern,
Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. PaLM: Scaling language mod-
eling with pathways. Journal of Machine Learning Research, 24(240):1–113, 2023. URL
http://jmlr.org/papers/v24/22-1144.html.
DeepSeek-AI,XiaoBi,DeliChen,GuantingChen,ShanhuangChen,DamaiDai,Chengqi
Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, HuazuoGao, Kaige Gao, Wenjun
Gao, Ruiqi Ge, Kang Guan, Daya Guo, Jianzhong Guo, Guangbo Hao, Zhewen Hao,
Ying He, Wenjie Hu, Panpan Huang, Erhang Li, Guowei Li, Jiashi Li, Yao Li, Y. K. Li,
WenfengLiang,FangyunLin,A.X.Liu,BoLiu,WenLiu,XiaodongLiu,XinLiu,Yiyuan
Liu, Haoyu Lu, Shanghao Lu, Fuli Luo, Shirong Ma, Xiaotao Nie, Tian Pei, Yishi Piao,
Junjie Qiu, Hui Qu, Tongzheng Ren, Zehui Ren, Chong Ruan, Zhangli Sha, Zhihong
Shao, Junxiao Song, Xuecheng Su, Jingxiang Sun, Yaofeng Sun, Minghui Tang, Bingx-
uanWang, PeiyiWang, ShiyuWang, Yaohui Wang, Yongji Wang, TongWu, Y. Wu, Xin
Xie, Zhenda Xie, Ziwei Xie, Yiliang Xiong, Hanwei Xu, R. X. Xu, Yanhong Xu, Dejian
Yang, YuxiangYou, ShuipingYu, XingkaiYu,B.Zhang, HaoweiZhang, LecongZhang,
LiyueZhang,MingchuanZhang,MinghuaZhang,WentaoZhang,YichaoZhang,Cheng-
gang Zhao, Yao Zhao, Shangyan Zhou, Shunfeng Zhou, Qihao Zhu, and Yuheng Zou.
Deepseek llm: Scaling open-source language models with longtermism, 2024. URL
https://arxiv.org/abs/2401.02954.
10Preprint
MostafaDehghani, Josip Djolonga, BasilMustafa,Piotr Padlewski, JonathanHeek, Justin
Gilmer, Andreas Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin,
Rodolphe Jenatton, Lucas Beyer, Michael Tschannen, Anurag Arnab, Xiao Wang, Car-
losRiquelme,MatthiasMinderer,JoanPuigcerver,UtkuEvci,ManojKumar,Sjoerdvan
Steenkiste, GamaleldinF. Elsayed, AravindhMahendran, Fisher Yu, AvitalOliver, Fan-
tine Huot, Jasmijn Bastings, Mark Patrick Collier, Alexey Gritsenko, Vighnesh Birod-
kar,CristinaVasconcelos,YiTay,ThomasMensink,AlexanderKolesnikov,FilipPavetic´,
DustinTran,ThomasKipf,MarioLucˇic´,XiaohuaZhai,DanielKeysers,JeremiahHarm-
sen,andNeilHoulsby. Scalingvisiontransformersto22billionparameters,2023. URL
https://arxiv.org/abs/2302.05442.
TimDettmers.LLM.int8()andemergentfeatures.https://timdettmers.com/2022/08/17/llm-int8-and-emergent-
2022. Accessed:2024-03-09.
Nolan Dey, Gurpreet Gosal, Zhiming Chen, Hemant Khachane, William Marshall,
Ribhu Pathria, Marvin Tom, and Joel Hestness. Cerebras-GPT: Open compute-
optimal language models trained on the cerebras wafer-scale cluster, 2023. URL
https://arxiv.org/abs/2304.03208.
Nelson Elhage, Robert Lasenby, and Christopher Olah.
Privileged bases in the transformer residual stream.
https://transformer-circuits.pub/2023/privileged-basis/index.html, 2023.
Accessed:2024-03-09.
ErichElsen, AugustusOdena, MaxwellNye, Sag˘nakTas¸ırlar,TriDao, CurtisHawthorne,
Deepak Moparthi, and Arushi Somani. Releasing persimmon-8b, 2023. URL
https://www.adept.ai/blog/persimmon-8b.
Siqi Fan, Xiusheng Huang, Xuezhi Fang, Yiqun Yao, Xiang Li, Ziyi Ni, Xin Jiang, Xuying
Meng, Peng Han, Shuo Shang, Kang Liu, Aixin Sun, and Yequan Wang. NanoLM: An
affordableLLMstudybenchmarkviaaccurateloss predictionacrossscales, 2024. URL
https://openreview.net/forum?id=mao3y822aM.
Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Ji-
ahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M. Dai, Anja Hauth, Katie Millican,
David Silver, Slav Petrov, Melvin Johnson, Ioannis Antonoglou, Julian Schrittwieser,
Amelia Glaese, Jilin Chen, Emily Pitler, Timothy Lillicrap, Angeliki Lazaridou, Orhan
Firat,JamesMolloy,MichaelIsard,PaulR.Barham,TomHennigan,BenjaminLee,Fabio
Viola, Malcolm Reynolds, Yuanzhong Xu, Ryan Doherty, Eli Collins, Clemens Meyer,
ElizaRutherford,EricaMoreira,KareemAyoub,MeghaGoel,GeorgeTucker,EnriquePi-
queras,MaximKrikun,IainBarr,NikolaySavinov,IvoDanihelka,BeccaRoelofs, Ana¨ıs
White, AndersAndreassen,TamaravonGlehn, LakshmanYagati, MehranKazemi, Lu-
casGonzalez,MishaKhalman,JakubSygnowski,AlexandreFrechette,CharlotteSmith,
LauraCulp,LevProleev,YiLuan,XiChen,JamesLottes,NathanSchucher,FedericoLe-
bron,AlbanRrustemi,NatalieClay,PhilCrone,TomasKocisky,JeffreyZhao,BartekPerz,
Dian Yu, HeidiHoward, AdamBloniarz, JackW. Rae, HanLu, LaurentSifre, Marcello
Maggioni,FredAlcober,DanGarrette,MeganBarnes,ShantanuThakoor,JacobAustin,
Gabriel Barth-Maron, William Wong, Rishabh Joshi, Rahma Chaabouni, Deeni Fatiha,
ArunAhuja, Ruibo Liu, YunxuanLi, SarahCogan, JeremyChen, Chao Jia, Chenjie Gu,
Qiao Zhang, Jordan Grimstad, Ale Jakse Hartman, Martin Chadwick, Gaurav Singh
Tomar, Xavier Garcia, Evan Senter, Emanuel Taropa, Thanumalayan Sankaranarayana
Pillai, Jacob Devlin, Michael Laskin, Diego de Las Casas, Dasha Valter, Connie Tao,
Lorenzo Blanco, Adria` Puigdome`nech Badia, David Reitter, Mianna Chen, Jenny Bren-
nan, Clara Rivera, Sergey Brin, Shariq Iqbal, Gabriela Surita, Jane Labanowski, Abhi
Rao, Stephanie Winkler, Emilio Parisotto, Yiming Gu, Kate Olszewska, Yujing Zhang,
Ravi Addanki, Antoine Miech, Annie Louis, Laurent El Shafey, Denis Teplyashin, Ge-
off Brown, Elliot Catt, Nithya Attaluri, Jan Balaguer, Jackie Xiang, Pidong Wang, Zoe
Ashwood, Anton Briukhov, Albert Webson, Sanjay Ganapathy, Smit Sanghavi, Ajay
Kannan, Ming-Wei Chang, Axel Stjerngren, Josip Djolonga, Yuting Sun, Ankur Bapna,
11Preprint
MatthewAitchison,PedramPejman,HenrykMichalewski,TianheYu,CindyWang,Juli-
etteLove,JunwhanAhn,DawnBloxwich,KehangHan,PeterHumphreys,ThibaultSel-
lam, James Bradbury, Varun Godbole, Sina Samangooei, Bogdan Damoc, Alex Kaska-
soli, Se´bastienM.R. Arnold, VijayVasudevan, ShubhamAgrawal, JasonRiesa, Dmitry
Lepikhin, Richard Tanburn, Srivatsan Srinivasan, Hyeontaek Lim, Sarah Hodkinson,
Pranav Shyam, Johan Ferret, Steven Hand, Ankush Garg, Tom Le Paine, Jian Li, Yu-
jia Li, Minh Giang, Alexander Neitz, Zaheer Abbas, Sarah York, Machel Reid, Eliza-
beth Cole, Aakanksha Chowdhery, Dipanjan Das, Dominika Rogozin´ska, Vitaly Niko-
laev, Pablo Sprechmann, Zachary Nado, Lukas Zilka, Flavien Prost, Luheng He, Mar-
ianne Monteiro, Gaurav Mishra, Chris Welty, Josh Newlan, Dawei Jia, Miltiadis Al-
lamanis, Clara Huiyi Hu, Raoul de Liedekerke, Justin Gilmer, Carl Saroufim, Shruti
Rijhwani, Shaobo Hou, Disha Shrivastava, Anirudh Baddepudi, Alex Goldin, Adnan
Ozturel, Albin Cassirer, Yunhan Xu, Daniel Sohn, Devendra Sachan, Reinald Kim
Amplayo, Craig Swanson, Dessie Petrova, Shashi Narayan, Arthur Guez, Siddhartha
Brahma, Jessica Landon, Miteyan Patel, Ruizhe Zhao, Kevin Villela, Luyu Wang, Wen-
haoJia,MatthewRahtz,MaiGime´nez,LeggYeung,HanzhaoLin,JamesKeeling,Petko
Georgiev, Diana Mincu, Boxi Wu, Salem Haykal, Rachel Saputro, Kiran Vodrahalli,
JamesQin,ZeynepCankara,AbhanshuSharma,NickFernando,WillHawkins,Behnam
Neyshabur, Solomon Kim, AdrianHutter, Priyanka Agrawal, Alex Castro-Ros, George
van den Driessche, Tao Wang, Fan Yang, Shuo yiin Chang, Paul Komarek, Ross McIl-
roy, Mario Lucˇic´, Guodong Zhang, Wael Farhan, Michael Sharman, Paul Natsev, Paul
Michel, Yong Cheng, Yamini Bansal, Siyuan Qiao, Kris Cao, SiamakShakeri, Christina
Butterfield, Justin Chung, Paul Kishan Rubenstein, Shivani Agrawal, Arthur Mensch,
KedarSoparkar,KarelLenc,TimothyChung, AedanPope,LorenMaggiore,JackieKay,
Priya Jhakra, Shibo Wang, Joshua Maynez, Mary Phuong, Taylor Tobin, Andrea Tac-
chetti,MajaTrebacz,KevinRobinson, YashKatariya,SebastianRiedel,PaigeBailey,Ke-
fan Xiao, Nimesh Ghelani, Lora Aroyo, Ambrose Slone, Neil Houlsby, Xuehan Xiong,
Zhen Yang, Elena Gribovskaya, Jonas Adler, Mateo Wirth, Lisa Lee, Music Li, Thais
Kagohara, Jay Pavagadhi, Sophie Bridgers, Anna Bortsova, Sanjay Ghemawat, Zafar-
ali Ahmed, Tianqi Liu, Richard Powell, Vijay Bolina, Mariko Iinuma, Polina Zablot-
skaia, James Besley, Da-Woon Chung, Timothy Dozat, Ramona Comanescu, Xiance Si,
JeremyGreer,GuolongSu,MartinPolacek,Raphae¨lLopezKaufman,SimonTokumine,
Hexiang Hu, Elena Buchatskaya, Yingjie Miao, Mohamed Elhawaty, Aditya Siddhant,
Nenad Tomasev, Jinwei Xing, Christina Greer, Helen Miller, Shereen Ashraf, Aurko
Roy, Zizhao Zhang, Ada Ma, Angelos Filos, Milos Besta, Rory Blevins, Ted Klimenko,
Chih-Kuan Yeh, SoravitChangpinyo, JiaqiMu, Oscar Chang, MantasPajarskas, Carrie
Muir,VeredCohen,CharlineLeLan,KrishnaHaridasan,AmitMarathe,StevenHansen,
Sholto Douglas, Rajkumar Samuel, Mingqiu Wang, Sophia Austin, Chang Lan, Jiepu
Jiang, Justin Chiu, Jaime Alonso Lorenzo, Lars Lowe Sjo¨sund, Se´bastien Cevey, Zach
Gleicher, Thi Avrahami, Anudhyan Boral, Hansa Srinivasan, Vittorio Selo, Rhys May,
KonstantinosAisopos,Le´onardHussenot,LivioBaldiniSoares,KateBaumli,MichaelB.
Chang,Adria` Recasens,BenCaine,AlexanderPritzel,FilipPavetic,FabioPardo,Anita
Gergely, Justin Frye, Vinay Ramasesh, Dan Horgan, Kartikeya Badola, Nora Kassner,
SubhrajitRoy,EthanDyer,V´ıctorCampos,AlexTomala,YunhaoTang,DaliaElBadawy,
ElspethWhite,BasilMustafa,OranLang,AbhishekJindal,SharadVikram,ZhitaoGong,
SergiCaelles,RossHemsley, GregoryThornton, FangxiaoyuFeng, WojciechStokowiec,
Ce Zheng, Phoebe Thacker, C¸ag˘lar U¨nlu¨, Zhishuai Zhang, Mohammad Saleh, James
Svensson, Max Bileschi, Piyush Patil, Ankesh Anand, Roman Ring, Katerina Tsihlas,
Arpi Vezer, Marco Selvi, Toby Shevlane, Mikel Rodriguez, Tom Kwiatkowski, Samira
Daruki,KeranRong,AllanDafoe,NicholasFitzGerald,KerenGu-Lemberg,MinaKhan,
LisaAnneHendricks,MariePellat,VladimirFeinberg,JamesCobon-Kerr,TaraSainath,
Maribeth Rauh, Sayed Hadi Hashemi, Richard Ives, Yana Hasson, YaGuang Li, Eric
Noland,YuanCao,NathanByrd,LeHou,QingzeWang,ThibaultSottiaux,MichelaPa-
ganini, Jean-Baptiste Lespiau, Alexandre Moufarek, Samer Hassan, Kaushik Shivaku-
mar, Joost van Amersfoort, Amol Mandhane, Pratik Joshi, Anirudh Goyal, Matthew
Tung, Andrew Brock, Hannah Sheahan, Vedant Misra, Cheng Li, Nemanja Rakic´evic´,
MostafaDehghani, FangyuLiu, SidMittal, JunhyukOh, SebNoury, ErenSezener,Fan-
tineHuot, MatthewLamm, NicolaDeCao, CharlieChen, GamaleldinElsayed,EdChi,
MahdisMahdieh,IanTenney,NanHua,IvanPetrychenko,PatrickKane,DylanScandi-
12Preprint
naro,RishubJain,JonathanUesato,RominaDatta,AdamSadovsky,OskarBunyan,Do-
minikRabiej,ShimuWu,JohnZhang,GautamVasudevan,EdouardLeurent,Mahmoud
Alnahlawi,IonutGeorgescu,NanWei,IvyZheng,BettyChan,PamGRabinovitch,Piotr
Stanczyk, Ye Zhang, David Steiner, Subhajit Naskar, Michael Azzam, Matthew John-
son,AdamPaszke,Chung-ChengChiu,JaumeSanchezElias,AfrozMohiuddin,Faizan
Muhammad, Jin Miao, Andrew Lee, Nino Vieillard, Sahitya Potluri, Jane Park, Elnaz
Davoodi, Jiageng Zhang, Jeff Stanway, Drew Garmon, Abhijit Karmarkar, Zhe Dong,
JongLee,AviralKumar, LuoweiZhou, JonathanEvens, WilliamIsaac,ZheChen, John-
sonJia,AnselmLevskaya,ZhenkaiZhu, ChrisGorgolewski, PeterGrabowski, YuMao,
Alberto Magni, Kaisheng Yao, Javier Snaider, Norman Casagrande, Paul Suganthan,
Evan Palmer, Geoffrey Irving, Edward Loper, Manaal Faruqui, Isha Arkatkar, Nanxin
Chen, IzhakShafran,MichaelFink, Alfonso Castan˜o, IreneGiannoumis, Wooyeol Kim,
MikołajRybin´ski, AshwinSreevatsa,JenniferPrendki,DavidSoergel,AdrianGoedeck-
emeyer,WilliGierke,MohsenJafari,MeenuGaba,JeremyWiesner,DianaGageWright,
YawenWei,HarshaVashisht,YanaKulizhskaya,JayHoover,MaigoLe,LuLi,Chimezie
Iwuanyanwu, Lu Liu, Kevin Ramirez, Andrey Khorlin, Albert Cui, Tian LIN, Marin
Georgiev,MarcusWu,RicardoAguilar,KeithPallo,AbhishekChakladar,AlenaRepina,
XihuiWu,TomvanderWeide,PriyaPonnapalli,CarolineKaplan,JiriSimsa,Shuangfeng
Li, Olivier Dousse, Fan Yang, Jeff Piper, Nathan Ie, Minnie Lui, Rama Pasumarthi,
Nathan Lintz, Anitha Vijayakumar, Lam Nguyen Thiet, Daniel Andor, Pedro Valen-
zuela, Cosmin Paduraru, Daiyi Peng, Katherine Lee, Shuyuan Zhang, Somer Greene,
Duc Dung Nguyen, Paula Kurylowicz, Sarmishta Velury, Sebastian Krause, Cassidy
Hardin,LucasDixon,LiliJanzer,KiamChoo,ZiqiangFeng,BiaoZhang,AchintyaSing-
hal,TejasiLatkar,MingyangZhang,QuocLe,ElenaAllicaAbellan,DayouDu,DanMcK-
innon, NatashaAntropova, Tolga Bolukbasi, OrgadKeller, David Reid, DanielFinchel-
stein, Maria Abi Raad, Remi Crocker, Peter Hawkins, Robert Dadashi, Colin Gaffney,
SidLall,KenFranko,EgorFilonov,AnnaBulanova,Re´miLeblond,VikasYadav,Shirley
Chung,HarryAskham,LuisC.Cobo,KelvinXu,FelixFischer,JunXu,ChristinaSorokin,
ChrisAlberti,Chu-ChengLin,ColinEvans,HaoZhou,AlekDimitriev,HannahForbes,
DylanBanarse,ZoraTung,JeremiahLiu,MarkOmernick,ColtonBishop,ChintuKumar,
RachelSterneck,RyanFoley,RohanJain,SwaroopMishra,JiaweiXia,TaylorBos,Geof-
frey Cideron, Ehsan Amid, Francesco Piccinno, Xingyu Wang, Praseem Banzal, Petru
Gurita, Hila Noga, Premal Shah, Daniel J. Mankowitz, Alex Polozov, Nate Kushman,
VictoriaKrakovna,SashaBrown,MohammadHosseinBateni,DennisDuan,VladFiroiu,
MeghanaThotakuri,TomNatan,AnhadMohananey,MatthieuGeist,SidharthMudgal,
Sertan Girgin, Hui Li, Jiayu Ye, Ofir Roval, Reiko Tojo, Michael Kwong, James Lee-
Thorp, Christopher Yew, Quan Yuan, Sumit Bagri, Danila Sinopalnikov, Sabela Ramos,
John Mellor, Abhishek Sharma, Aliaksei Severyn, Jonathan Lai, Kathy Wu, Heng-Tze
Cheng, David Miller, Nicolas Sonnerat, Denis Vnukov, Rory Greig, Jennifer Beattie,
EmilyCaveness,LibinBai,JulianEisenschlos,AlexKorchemniy,TomyTsai,MimiJasare-
vic, Weize Kong, Phuong Dao, Zeyu Zheng, Frederick Liu, Fan Yang, Rui Zhu, Mark
Geller, Tian Huey Teh, Jason Sanmiya, Evgeny Gladchenko, Nejc Trdin, Andrei Sozan-
schi, Daniel Toyama, Evan Rosen, Sasan Tavakkol, Linting Xue, Chen Elkind, Oliver
Woodman, John Carpenter,George Papamakarios, RupertKemp, SushantKafle, Tanya
Grunina, Rishika Sinha, Alice Talbert, Abhimanyu Goyal, Diane Wu, Denese Owusu-
Afriyie, Cosmo Du, Chloe Thornton, Jordi Pont-Tuset, Pradyumna Narayana, Jing Li,
Sabaer Fatehi, John Wieting, Omar Ajmeri, Benigno Uria, Tao Zhu, Yeongil Ko, Laura
Knight, Ame´lieHe´liou, NingNiu, ShaneGu, ChenxiPang, DustinTran,YeqingLi, Nir
Levine, Ariel Stolovich, Norbert Kalb, Rebeca Santamaria-Fernandez, Sonam Goenka,
Wenny Yustalim, Robin Strudel, Ali Elqursh, Balaji Lakshminarayanan, Charlie Deck,
Shyam Upadhyay, Hyo Lee, Mike Dusenberry, Zonglin Li, Xuezhi Wang, Kyle Levin,
RaphaelHoffmann,DanHoltmann-Rice,OlivierBachem,SummerYue,ShoArora,Eric
Malmi, Daniil Mirylenka, Qijun Tan, Christy Koh, Soheil Hassas Yeganeh, Siim Po˜der,
Steven Zheng, Francesco Pongetti, Mukarram Tariq, Yanhua Sun, Lucian Ionita, Mo-
jtaba Seyedhosseini, Pouya Tafti, Ragha Kotikalapudi, Zhiyu Liu, Anmol Gulati, Jas-
mine Liu, Xinyu Ye, Bart Chrzaszcz, Lily Wang, Nikhil Sethi, Tianrun Li, Ben Brown,
Shreya Singh, Wei Fan, Aaron Parisi, Joe Stanton, Chenkai Kuang, Vinod Koverkathu,
Christopher A. Choquette-Choo, Yunjie Li, TJ Lu, Abe Ittycheriah, Prakash Shroff, Pei
Sun, Mani Varadarajan, Sanaz Bahargam, Rob Willoughby, David Gaddy, Ishita Das-
13Preprint
gupta, Guillaume Desjardins, Marco Cornero, Brona Robenek, Bhavishya Mittal, Ben
Albrecht, Ashish Shenoy, Fedor Moiseev, Henrik Jacobsson, AlirezaGhaffarkhah, Mor-
gane Rivie`re, Alanna Walton, Cle´ment Crepy, AliciaParrish, YuanLiu, Zongwei Zhou,
ClementFarabet,CareyRadebaugh,PraveenSrinivasan,ClaudiavanderSalm,Andreas
Fidjeland, Salvatore Scellato, Eri Latorre-Chimoto, Hanna Klimczak-Plucin´ska, David
Bridson,DariodeCesare,TomHudson,PiermariaMendolicchio,LexiWalker,AlexMor-
ris, IvoPenchev, MatthewMauger, AlexeyGuseynov, Alison Reid, SethOdoom, Lucia
Loher,VictorCotruta,MadhaviYenugula,DominikGrewe,AnastasiaPetrushkina,Tom
Duerig,AntonioSanchez,SteveYadlowsky,AmyShen,AmirGloberson,AdamKurzrok,
Lynette Webb, Sahil Dua, Dong Li, Preethi Lahoti, Surya Bhupatiraju, Dan Hurt, Ha-
roon Qureshi, Ananth Agarwal, Tomer Shani, Matan Eyal, Anuj Khare, Shreyas Ram-
mohan Belle, Lei Wang, Chetan Tekur, Mihir Sanjay Kale, Jinliang Wei, Ruoxin Sang,
BrennanSaeta,TylerLiechty,YiSun,YaoZhao,StephanLee,PanduNayak,DougFritz,
Manish Reddy Vuyyuru, John Aslanides, Nidhi Vyas, Martin Wicke, Xiao Ma, Taylan
Bilal, Evgenii Eltyshev, Daniel Balle, Nina Martin, Hardie Cate, James Manyika, Key-
van Amiri, Yelin Kim, Xi Xiong, Kai Kang, Florian Luisier, Nilesh Tripuraneni, David
Madras,MandyGuo,AustinWaters,OliverWang,JoshuaAinslie,JasonBaldridge,Han
Zhang,GarimaPruthi,JakobBauer,FengYang,RihamMansour,JasonGelman,YangXu,
GeorgePolovets,JiLiu,HonglongCai,WarrenChen,XiangHaiSheng,EmilyXue,Sherjil
Ozair,AdamsYu,ChristofAngermueller,XiaoweiLi,WeirenWang,JuliaWiesinger,Em-
manouil Koukoumidis, Yuan Tian, Anand Iyer, MadhuGurumurthy, MarkGoldenson,
Parashar Shah, MK Blake, Hongkun Yu, Anthony Urbanowicz, Jennimaria Palomaki,
ChrisanthaFernando,KevinBrooks,KenDurden,HarshMehta,NikolaMomchev,Elahe
Rahimtoroghi,MariaGeorgaki,AmitRaul,SebastianRuder,MorganRedshaw,Jinhyuk
Lee,KomalJalan,DinghuaLi,GingerPerng,BlakeHechtman,ParkerSchuh,MiladNasr,
MiaChen,KieranMilan,VladimirMikulik,TrevorStrohman,JulianaFranco,TimGreen,
DemisHassabis,KorayKavukcuoglu,JeffreyDean,andOriolVinyals. Gemini: Afamily
ofhighlycapablemultimodalmodels,2023. URLhttps://arxiv.org/abs/2312.11805.
TeamGemma.Gemma:Openmodelsbasedongeminiresearchandtechnology,2024.URL
https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep
residualnetworks,2016. URLhttps://arxiv.org/abs/1603.05027.
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai,
Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan
Clark, Tom Hennigan, EricNoland, Katie Millican, George vandenDriessche, Bogdan
Damoc,AureliaGuy,SimonOsindero,KarenSimonyan,ErichElsen,JackW.Rae,Oriol
Vinyals,andLaurentSifre.Trainingcompute-optimallargelanguagemodels,2022.URL
https://arxiv.org/abs/2203.15556.
Shengding Hu, Yuge Tu, Xu Han, Ganqu Cui, Chaoqun He, Weilin Zhao, Xi-
ang Long, Zhi Zheng, Yewei Fang, Kaihuo Zhang, Yuxiang Huang, Zhenning
Dai, Baitao Gong, Chongyi Wang, Yuan Yao, Jie Zhou, Jie Cai, Xinrong Zhang,
Zhongwu Zhai, Ning Ding, Chao Jia, Guoyang Zeng, Dahai Li, Zhiyuan Liu, and
Maosong Sun. Minicpm: Unveiling the potential of end-side large language models.
https://shengdinghu.notion.site/MiniCPM-Unveiling-the-Potential-of-End-side-Large-Language-Mod
2024. Accessed:2024-03-09.
Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary,
Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Flo-
rian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Le´lio Renard
Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian,
Sophia Yang, Szymon Antoniak, Teven Le Scao, The´ophile Gervet, Thibaut Lavril,
ThomasWang, Timothe´eLacroix,andWilliamElSayed. Mixtralofexperts,2024. URL
https://arxiv.org/abs/2401.04088.
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon
Child,ScottGray,AlecRadford,JeffreyWu,andDarioAmodei. Scalinglawsforneural
languagemodels,2020. URLhttps://arxiv.org/abs/2001.08361.
14Preprint
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2014.
URLhttps://arxiv.org/abs/1412.6980.
Etai Littwin and Greg Yang. Adaptive optimization in the $ infty$-width limit.
\
In The Eleventh International Conference on Learning Representations, 2023. URL
https://openreview.net/forum?id=zgVDqw9ZUES.
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning,
2023. URLhttps://arxiv.org/abs/2304.08485.
Ilya Loshchilov and Frank Hutter. Fixing weight decay regularization in adam. CoRR,
abs/1711.05101,2017. URLhttp://arxiv.org/abs/1711.05101.
Sadhika Malladi, Kaifeng Lyu, Abhishek Panigrahi, and Sanjeev Arora. On
the SDEs and scaling rules for adaptive gradient algorithms, 2022. URL
https://arxiv.org/abs/2205.10287.
Sam McCandlish, Jared Kaplan, Dario Amodei, and OpenAI Dota Team. An
empirical model of large-batch training. CoRR, abs/1812.06162, 2018. URL
http://arxiv.org/abs/1812.06162.
ToanQ.NguyenandJulianSalazar.Transformerswithouttears:Improvingthenormaliza-
tionofself-attention,2019. URLhttps://arxiv.org/abs/1910.05895.
OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Flo-
renciaLeoniAleman,DiogoAlmeida,JankoAltenschmidt,SamAltman,ShyamalAnad-
kat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming
Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-
Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-
Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor
Cai,RosieCampbell,AndrewCann,BrittanyCarey,ChelseaCarlson,RoryCarmichael,
Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason
Chen,MarkChen,BenChess,ChesterCho,CaseyChu,HyungWonChung,DaveCum-
mings, JeremiahCurrier, Yunxing Dai, Cory Decareaux,Thomas Degry, NoahDeutsch,
DamienDeville,ArkaDhar,DavidDohan,SteveDowling,SheilaDunning,AdrienEcof-
fet,AttyEleti,TynaEloundou,DavidFarhi,LiamFedus,NikoFelix,Simo´nPosadaFish-
man, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, ChristianGibson, Vik Goel,
Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Graf-
stein,ScottGray,RyanGreene,JoshuaGross,ShixiangShaneGu,YufeiGuo,ChrisHal-
lacy, Jesse Han, JeffHarris, Yuchen He, MikeHeaton, JohannesHeidecke, Chris Hesse,
Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli
Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang,
Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer
Kaftan,ŁukaszKaiser,AliKamali,IngmarKanitscheider,NitishShirishKeskar,Tabarak
Khan,LoganKilpatrick,JongWookKim,ChristinaKim,YongjikKim,JanHendrikKirch-
ner,JamieKiros,MattKnight,DanielKokotajlo,ŁukaszKondraciuk,AndrewKondrich,
Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai
Lan, TeddyLee, JanLeike, JadeLeung, Daniel Levy, ChakMingLi, RachelLim, Molly
Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna
Makanju,KimMalfacini,SamManning,TodorMarkov,YanivMarkovski,BiancaMartin,
KatieMayer,AndrewMayne,BobMcGrew,ScottMayerMcKinney,ChristineMcLeavey,
Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz,
Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Moss-
ing, Tong Mu, Mira Murati, Oleg Murk, David Me´ly, Ashvin Nair, Reiichiro Nakano,
Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang,
CullenO’Keefe,JakubPachocki, AlexPaino, JoePalermo,AshleyPantuliano, Giambat-
tista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew
Peng, AdamPerelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde
de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell,
AletheaPower,BorisPower,ElizabethProehl,RaulPuri,AlecRadford,JackRae,Aditya
Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted,
15Preprint
HenriRoussez,NickRyder,MarioSaltarelli,TedSanders,ShibaniSanturkar,GirishSas-
try, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard,
Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler,
Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang
Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie
Tang,NikolasTezak,MadeleineB.Thompson,PhilTillet,AminTootoonchian,Elizabeth
Tseng,PrestonTuggle,NickTurley,JerryTworek,JuanFelipeCero´nUribe,AndreaVal-
lone,ArunVijayvergiya,ChelseaVoss,CarrollWainwright,JustinJayWang,AlvinWang,
BenWang,JonathanWard,JasonWei,CJWeinmann,AkilaWelihinda,PeterWelinder,Ji-
ayiWeng, Lilian Weng, MattWiethoff, Dave Willner, Clemens Winter, SamuelWolrich,
HannahWong, LaurenWorkman, SherwinWu,JeffWu,MichaelWu,KaiXiao, TaoXu,
Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang,
MarvinZhang,ShengjiaZhao,TianhaoZheng,JuntangZhuang,WilliamZhuk,andBar-
retZoph. Gpt-4technicalreport,2024. URLhttps://arxiv.org/abs/2303.08774.
Jupinder Parmar, Shrimai Prabhumoye, Joseph Jennings, Mostofa Patwary, Sandeep
Subramanian, Dan Su, Chen Zhu, Deepak Narayanan, Aastha Jhunjhunwala, Ayush
Dattagupta, Vibhu Jawa, Jiwei Liu, Ameya Mahabaleshwarkar, Osvald Nitski, Annika
Brundyn, James Maki, Miguel Martinez, Jiaxuan You, John Kamalu, Patrick LeGresley,
DenysFridman, JaredCasper,AshwathAithal, OleksiiKuchaiev,MohammadShoeybi,
Jonathan Cohen, and Bryan Catanzaro. Nemotron-4 15b technical report, 2024. URL
https://arxiv.org/abs/2402.16819.
Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella Bi-
derman, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV,
XuzhengHe,HaowenHou, JiajuLin, PrzemyslawKazienko,JanKocon, JiamingKong,
Bartlomiej Koptyra, Hayden Lau, Krishna Sri Ipsit Mantri, Ferdinand Mom, Atsushi
Saito, Guangyu Song, Xiangru Tang, Bolun Wang, Johan S. Wind, Stanislaw Wozniak,
Ruichong Zhang, Zhenyuan Zhang, Qihang Zhao, Peng Zhou, Qinghua Zhou, Jian
Zhu, and Rui-Jie Zhu. Rwkv: Reinventing rnns for the transformer era, 2023. URL
https://arxiv.org/abs/2305.13048.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and
Ilya Sutskever. Language models are unsupervised multitask learners, 2019.
https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners
Lastvisitedon2023/09/07.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning
withaunifiedtext-to-texttransformer,2019.URLhttps://arxiv.org/abs/1910.10683.
Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap,
Jean baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrit-
twieser, Ioannis Antonoglou, Rohan Anil, SebastianBorgeaud, AndrewDai, Katie Mil-
lican, Ethan Dyer, Mia Glaese, Thibault Sottiaux, Benjamin Lee, Fabio Viola, Malcolm
Reynolds, Yuanzhong Xu, James Molloy, Jilin Chen, Michael Isard, Paul Barham, Tom
Hennigan, Ross McIlroy, Melvin Johnson, Johan Schalkwyk, Eli Collins, Eliza Ruther-
ford, Erica Moreira, Kareem Ayoub, Megha Goel, Clemens Meyer, Gregory Thornton,
Zhen Yang, Henryk Michalewski, Zaheer Abbas, Nathan Schucher, Ankesh Anand,
RichardIves,JamesKeeling,KarelLenc,SalemHaykal,SiamakShakeri,PranavShyam,
AakankshaChowdhery,RomanRing,StephenSpencer,ErenSezener,LukeVilnis,Oscar
Chang,NobuyukiMorioka,GeorgeTucker,CeZheng,OliverWoodman,NithyaAttaluri,
Tomas Kocisky, Evgenii Eltyshev, Xi Chen, Timothy Chung, Vittorio Selo, Siddhartha
Brahma,PetkoGeorgiev,AmbroseSlone,ZhenkaiZhu, JamesLottes,SiyuanQiao,Ben
Caine, SebastianRiedel, Alex Tomala, MartinChadwick, Juliette Love, Peter Choy, Sid
Mittal,NeilHoulsby,YunhaoTang,MatthewLamm,LibinBai,QiaoZhang,LuhengHe,
YongCheng,PeterHumphreys,YujiaLi,SergeyBrin,AlbinCassirer,YingjieMiao,Lukas
Zilka,TaylorTobin,KelvinXu,LevProleev,DanielSohn,AlbertoMagni,LisaAnneHen-
dricks, IsabelGao, SantiagoOntan˜o´n, Oskar Bunyan,NathanByrd,AbhanshuSharma,
BiaoZhang, MarioPinto, Rishika Sinha, HarshMehta, Dawei Jia, SergiCaelles, Albert
16Preprint
Webson, AlexMorris, BeccaRoelofs, YifanDing, RobinStrudel,XuehanXiong, Marvin
Ritter, MostafaDehghani, Rahma Chaabouni, Abhijit Karmarkar, Guangda Lai, Fabian
Mentzer, Bibo Xu, YaGuang Li, Yujing Zhang, Tom Le Paine, Alex Goldin, Behnam
Neyshabur, Kate Baumli, Anselm Levskaya, Michael Laskin, Wenhao Jia, Jack W. Rae,
KefanXiao, Antoine He,SkyeGiordano, LakshmanYagati, Jean-BaptisteLespiau,Paul
Natsev, Sanjay Ganapathy, Fangyu Liu, Danilo Martins, Nanxin Chen, Yunhan Xu,
MeganBarnes,RhysMay,ArpiVezer,JunhyukOh,KenFranko,SophieBridgers,Ruizhe
Zhao, BoxiWu, BasilMustafa,SeanSechrist, Emilio Parisotto, ThanumalayanSankara-
narayana Pillai, Chris Larkin, Chenjie Gu, Christina Sorokin, Maxim Krikun, Alexey
Guseynov,JessicaLandon,RominaDatta,AlexanderPritzel,PhoebeThacker,FanYang,
KevinHui,AnjaHauth,Chih-KuanYeh,DavidBarker,JustinMao-Jones,SophiaAustin,
Hannah Sheahan, Parker Schuh, James Svensson, Rohan Jain, Vinay Ramasesh, An-
ton Briukhov, Da-Woon Chung, Tamara von Glehn, Christina Butterfield, Priya Jhakra,
Matthew Wiethoff, Justin Frye, Jordan Grimstad, Beer Changpinyo, Charline Le Lan,
Anna Bortsova, Yonghui Wu, Paul Voigtlaender, Tara Sainath, Charlotte Smith, Will
Hawkins, KrisCao, JamesBesley,SrivatsanSrinivasan,MarkOmernick, ColinGaffney,
GabrielaSurita,RyanBurnell,BogdanDamoc,JunwhanAhn,AndrewBrock,MantasPa-
jarskas,AnastasiaPetrushkina,SebNoury,LorenzoBlanco,KevinSwersky,ArunAhuja,
ThiAvrahami,VedantMisra,RaouldeLiedekerke,MarikoIinuma,AlexPolozov,Sarah
York, GeorgevandenDriessche,PaulMichel,JustinChiu,RoryBlevins, ZachGleicher,
Adria` Recasens, Alban Rrustemi, Elena Gribovskaya, Aurko Roy, Wiktor Gworek, Se´b
Arnold, Lisa Lee, James Lee-Thorp, Marcello Maggioni, Enrique Piqueras, Kartikeya
Badola,SharadVikram,LucasGonzalez,AnirudhBaddepudi,EvanSenter,JacobDevlin,
JamesQin,MichaelAzzam,MajaTrebacz,MartinPolacek,KashyapKrishnakumar,Shuo
yiin Chang, Matthew Tung, Ivo Penchev, Rishabh Joshi, Kate Olszewska, Carrie Muir,
Mateo Wirth, Ale Jakse Hartman, Josh Newlan, Sheleem Kashem, Vijay Bolina, Elahe
Dabir, Joost van Amersfoort, Zafarali Ahmed, James Cobon-Kerr, Aishwarya Kamath,
ArnarMarHrafnkelsson,LeHou,IanMackinnon,AlexandreFrechette,EricNoland,Xi-
ance Si, Emanuel Taropa, Dong Li, Phil Crone, Anmol Gulati, Se´bastien Cevey, Jonas
Adler, Ada Ma, David Silver, Simon Tokumine, Richard Powell, Stephan Lee, Michael
Chang,SamerHassan,DianaMincu,AntoineYang,NirLevine,JennyBrennan,Mingqiu
Wang,SarahHodkinson, JeffreyZhao,JoshLipschultz, AedanPope,MichaelB.Chang,
Cheng Li, Laurent El Shafey, Michela Paganini, Sholto Douglas, Bernd Bohnet, Fabio
Pardo,SethOdoom,MihaelaRosca,CiceroNogueiradosSantos,KedarSoparkar,Arthur
Guez,TomHudson,StevenHansen,ChulayuthAsawaroengchai,RaviAddanki,Tianhe
Yu,WojciechStokowiec,MinaKhan,JustinGilmer,JaehoonLee,CarrieGrimesBostock,
KeranRong,JonathanCaton,PedramPejman,FilipPavetic,GeoffBrown,VivekSharma,
Mario Lucˇic´, Rajkumar Samuel, Josip Djolonga, Amol Mandhane, Lars Lowe Sjo¨sund,
ElenaBuchatskaya,ElspethWhite,NatalieClay,JiepuJiang,HyeontaekLim,RossHem-
sley,JaneLabanowski,NicolaDeCao,DavidSteiner,SayedHadiHashemi,JacobAustin,
Anita Gergely, Tim Blyth, Joe Stanton, Kaushik Shivakumar, Aditya Siddhant, Anders
Andreassen,CarlosAraya,NikhilSethi, RakeshShivanna,StevenHand, Ankur Bapna,
AliKhodaei, Antoine Miech, GarrettTanzer, AndySwing, ShantanuThakoor, Zhufeng
Pan, Zachary Nado, Stephanie Winkler, Dian Yu, Mohammad Saleh, Loren Maggiore,
Iain Barr, Minh Giang, Thais Kagohara, Ivo Danihelka, Amit Marathe, Vladimir Fein-
berg, Mohamed Elhawaty, Nimesh Ghelani, Dan Horgan, Helen Miller, Lexi Walker,
Richard Tanburn, Mukarram Tariq, Disha Shrivastava, Fei Xia, Chung-Cheng Chiu,
Zoe Ashwood, Khuslen Baatarsukh, Sina Samangooei, Fred Alcober, Axel Stjerngren,
Paul Komarek, Katerina Tsihlas, Anudhyan Boral, Ramona Comanescu, Jeremy Chen,
Ruibo Liu, Dawn Bloxwich, Charlie Chen, Yanhua Sun, Fangxiaoyu Feng, Matthew
Mauger, Xerxes Dotiwalla, Vincent Hellendoorn, Michael Sharman, Ivy Zheng, Kr-
ishna Haridasan, Gabe Barth-Maron, Craig Swanson, Dominika Rogozin´ska, Alek An-
dreev, Paul Kishan Rubenstein, Ruoxin Sang, Dan Hurt, Gamaleldin Elsayed, Renshen
Wang,DaveLacey,AnastasijaIlic´,YaoZhao,LoraAroyo,ChimezieIwuanyanwu,Vitaly
Nikolaev, Balaji Lakshminarayanan, Sadegh Jazayeri, Raphae¨l Lopez Kaufman, Mani
Varadarajan, Chetan Tekur, Doug Fritz, Misha Khalman, David Reitter, Kingshuk Das-
gupta, Shourya Sarcar, Tina Ornduff, Javier Snaider, Fantine Huot, Johnson Jia, Ru-
pertKemp, Nejc Trdin, Anitha Vijayakumar, LucyKim, Christof Angermueller, LiLao,
Tianqi Liu, Haibin Zhang, David Engel, Somer Greene, Ana¨ıs White, Jessica Austin,
17Preprint
LillyTaylor,ShereenAshraf,DangyiLiu,MariaGeorgaki,IreneCai,YanaKulizhskaya,
Sonam Goenka, Brennan Saeta, Kiran Vodrahalli, Christian Frank, Dario de Cesare,
Brona Robenek, HarryRichardson, Mahmoud Alnahlawi, Christopher Yew, Priya Pon-
napalli, MarcoTagliasacchi, AlexKorchemniy, YelinKim, Dinghua Li, BillRosgen, Zoe
Ashwood, Kyle Levin, JeremyWiesner, PraseemBanzal, PraveenSrinivasan, Hongkun
Yu, C¸ag˘lar U¨nlu¨, David Reid, Zora Tung, Daniel Finchelstein, Ravin Kumar, Andre
Elisseeff, Jin Huang, Ming Zhang, Rui Zhu, Ricardo Aguilar, MaiGime´nez, Jiawei Xia,
OlivierDousse,WilliGierke,SoheilHassasYeganeh,DamionYates,KomalJalan,LuLi,
Eri Latorre-Chimoto, Duc Dung Nguyen, Ken Durden, Praveen Kallakuri, Yaxin Liu,
MatthewJohnson,TomyTsai,AliceTalbert,JasmineLiu,AlexanderNeitz,ChenElkind,
MarcoSelvi,MimiJasarevic,LivioBaldiniSoares,AlbertCui,PidongWang,AlekWen-
jiao Wang, Xinyu Ye, Krystal Kallarackal, Lucia Loher, Hoi Lam, Josef Broder, Dan
Holtmann-Rice, Nina Martin, Bramandia Ramadhana, Daniel Toyama, Mrinal Shukla,
Sujoy Basu, Abhi Mohan, Nick Fernando, Noah Fiedel, Kim Paterson, Hui Li, Ankush
Garg, Jane Park, DongHyun Choi, Diane Wu, Sankalp Singh, Zhishuai Zhang, Amir
Globerson,LilyYu,JohnCarpenter,Fe´lixdeChaumontQuitry,CareyRadebaugh,Chu-
ChengLin,AlexTudor,PrakashShroff,DrewGarmon,DayouDu,NeeraVats,HanLu,
Shariq Iqbal, Alex Yakubovich, Nilesh Tripuraneni, James Manyika, Haroon Qureshi,
Nan Hua, Christel Ngani, Maria Abi Raad, Hannah Forbes, Anna Bulanova, Jeff Stan-
way, Mukund Sundararajan, Victor Ungureanu, Colton Bishop, Yunjie Li, BalajiVenka-
traman, Bo Li, Chloe Thornton, Salvatore Scellato, Nishesh Gupta, Yicheng Wang, Ian
Tenney, Xihui Wu, Ashish Shenoy, Gabriel Carvajal, Diana Gage Wright, Ben Bariach,
Zhuyun Xiao, Peter Hawkins, Sid Dalmia, Clement Farabet, Pedro Valenzuela, Quan
Yuan, Chris Welty, Ananth Agarwal, Mia Chen, Wooyeol Kim, Brice Hulse, Nandita
Dukkipati, Adam Paszke, Andrew Bolt, Elnaz Davoodi, Kiam Choo, Jennifer Beattie,
Jennifer Prendki, Harsha Vashisht, Rebeca Santamaria-Fernandez, Luis C. Cobo, Jarek
Wilkiewicz, David Madras, Ali Elqursh, Grant Uy, Kevin Ramirez, Matt Harvey, Tyler
Liechty, Heiga Zen, JeffSeibert, Clara Huiyi Hu, Mohamed Elhawaty, AndreyKhorlin,
Maigo Le, Asaf Aharoni, MeganLi, Lily Wang, SandeepKumar, AlejandroLince, Nor-
manCasagrande,JayHoover,DaliaElBadawy,DavidSoergel,DenisVnukov,MattMiec-
nikowski,JiriSimsa,AnnaKoop,PraveenKumar,ThibaultSellam,DanielVlasic,Samira
Daruki,NirShabat,JohnZhang,GuolongSu,JiagengZhang,JeremiahLiu,YiSun,Evan
Palmer,AlirezaGhaffarkhah,XiXiong,VictorCotruta,MichaelFink,LucasDixon,Ash-
win Sreevatsa, Adrian Goedeckemeyer, Alek Dimitriev, Mohsen Jafari, Remi Crocker,
NicholasFitzGerald,AviralKumar,SanjayGhemawat,IvanPhilips,FrederickLiu,Yan-
nie Liang, Rachel Sterneck, Alena Repina, Marcus Wu, Laura Knight, Marin Georgiev,
Hyo Lee, Harry Askham, Abhishek Chakladar, Annie Louis, Carl Crous, Hardie Cate,
Dessie Petrova, Michael Quinn, Denese Owusu-Afriyie, Achintya Singhal, Nan Wei,
SolomonKim,DamienVincent,MiladNasr,ChristopherA.Choquette-Choo,ReikoTojo,
ShawnLu,DiegodeLasCasas,YuchungCheng,TolgaBolukbasi,KatherineLee,Saaber
Fatehi, RajagopalAnanthanarayanan, MiteyanPatel, CharbelKaed, Jing Li, JakubSyg-
nowski, Shreyas Rammohan Belle, Zhe Chen, Jaclyn Konzelmann, Siim Po˜der, Roopal
Garg,VinodKoverkathu,AdamBrown,ChrisDyer,RosanneLiu,AzadeNova,JunXu,
SlavPetrov,DemisHassabis,KorayKavukcuoglu,JeffreyDean,andOriolVinyals. Gem-
ini1.5: Unlockingmultimodalunderstandingacrossmillionsoftokensofcontext,2024.
URLhttps://arxiv.org/abs/2403.05530.
Noam Shazeer. Fast transformer decoding: One write-head is all you need, 2019. URL
https://arxiv.org/abs/1911.02150.
Noam Shazeer. GLU variants improve transformer, 2020. URL
https://arxiv.org/abs/2002.05202.
Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and
BryanCatanzaro. Megatron-LM:Trainingmulti-billion parameterlanguagemodelsus-
ingmodelparallelism,2020. URLhttps://arxiv.org/abs/1909.08053.
David So, Wojciech Man´ke, Hanxiao Liu, Zihang Dai, Noam Shazeer, and Quoc V Le.
Searchingfor efficienttransformersfor languagemodeling. InM.Ranzato, A.Beygelz-
imer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Infor-
18Preprint
mationProcessingSystems,volume34,pp.6010–6022.CurranAssociates,Inc.,2021. URL
https://proceedings.neurips.cc/paper_files/paper/2021/file/2f3c6a4cd8af177f6456e7e51a916ff3-Pape
Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. RoFormer: Enhanced
transformer with rotary position embedding. CoRR, abs/2104.09864, 2021. URL
https://arxiv.org/abs/2104.09864.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
Lachaux, Timothe´e Lacroix, Baptiste Rozie`re, Naman Goyal, Eric Hambro, Faisal
Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lam-
ple. Llama: Open and efficient foundation language models, 2023a. URL
https://arxiv.org/abs/2302.13971.
HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,
NikolayBashlykov,SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,DanBikel,Lukas
Blecher,CristianCantonFerrer,MoyaChen,GuillemCucurull,DavidEsiobu,JudeFer-
nandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman
Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas,
Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura,
Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning
Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, An-
drew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan
Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross
Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen
Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert
Stojnic,SergeyEdunov,andThomasScialom. Llama2:Openfoundationandfine-tuned
chatmodels,2023b. URLhttps://arxiv.org/abs/2307.09288.
TwanvanLaarhoven.L2regularizationversusbatchandweightnormalization,2017.URL
https://arxiv.org/abs/1706.05350.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N
Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in
Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL
https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Pape
Mitchell Wortsman, Peter J. Liu, Lechao Xiao, Katie Everett, Alex Alemi, Ben Adlam,
John D. Co-Reyes, Izzeddin Gur, Abhishek Kumar, Roman Novak, Jeffrey Penning-
ton, Jascha Sohl-dickstein, Kelvin Xu, Jaehoon Lee, Justin Gilmer, and Simon Korn-
blith. Small-scale proxies for large-scale transformer training instabilities, 2023. URL
https://arxiv.org/abs/2309.14322.
XAI. Grok-1,2024. URLhttps://github.com/xai-org/grok-1.
Yuanzhong Xu, HyoukJoong Lee, Dehao Chen, Blake Hechtman, Yanping Huang,
Rahul Joshi, Maxim Krikun, Dmitry Lepikhin, Andy Ly, Marcello Maggioni, Ruom-
ing Pang, Noam Shazeer, Shibo Wang, Tao Wang, Yonghui Wu, and Zhifeng Chen.
GSPMD: General and scalable parallelization for ML computation graphs, 2021. URL
https://arxiv.org/abs/2105.04663.
Greg Yang. Tensor Programs I: Wide feedforward or recurrent neural networks of any
architecturearegaussianprocesses,2019. URLhttps://arxiv.org/abs/1910.12478.
Greg Yang. Tensor Programs II: Neural tangent kernel for any architecture, 2020. URL
https://arxiv.org/abs/2006.14548.
Greg Yang. Tensor Programs III: Neural matrix laws, 2021. URL
https://arxiv.org/abs/2009.10685.
Greg Yang and Edward J. Hu. Tensor Programs IV: Feature learning in infinite-
width neural networks. In Marina Meila and Tong Zhang (eds.), Proceedings
of the 38th International Conference on Machine Learning, volume 139 of Proceed-
ings of Machine Learning Research, pp. 11727–11737. PMLR, 18–24 Jul 2021. URL
https://proceedings.mlr.press/v139/yang21c.html.
19Preprint
Greg Yang, Edward J. Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi,
Nick Ryder, Jakub Pachocki, Weizhu Chen, and Jianfeng Gao. Tensor Programs
V: Tuning large neural networks via zero-shot hyperparameter transfer, 2022. URL
https://arxiv.org/abs/2203.03466.
GregYang,JamesB.Simon,andJeremyBernstein.Aspectralconditionforfeaturelearning,
2023a. URLhttps://arxiv.org/abs/2310.17813.
Greg Yang, Dingli Yu, Chen Zhu, and Soufiane Hayou. Tensor Programs
VI: Feature learning in infinite-depth neural networks, 2023b. URL
https://arxiv.org/abs/2310.02244.
Yiqun Yao and Yequan Wang. Research without re-search: Maximal update
parametrization yields accurate loss prediction across scales, 2023. URL
https://arxiv.org/abs/2304.06875.
YangYou, JingLi, JonathanHseu, XiaodanSong, JamesDemmel, andCho-JuiHsieh. Re-
ducingBERTpre-trainingtimefrom3daysto76minutes. CoRR,abs/1904.00962,2019.
URLhttp://arxiv.org/abs/1904.00962.
Biao Zhang and Rico Sennrich. Root mean square layer normalization. CoRR,
abs/1910.07467,2019. URLhttp://arxiv.org/abs/1910.07467.
20