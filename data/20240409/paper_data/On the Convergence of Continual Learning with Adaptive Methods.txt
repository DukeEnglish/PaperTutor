On the Convergence of Continual Learning with Adaptive Methods
SeungyubHan1 YeongmoKim1 TaehyunCho1 JungwooLee1
1SeoulNationalUniversity,Seoul,RepublicofKorea
Abstract
Inthislineofresearch,nonconvexstochasticoptimization
problemshavebeenwellstudiedonasingletasktotrain
deepneuralnetworksandprovetheoreticalguaranteesof
One of the objectives of continual learning is to goodconvergence.
preventcatastrophicforgettinginlearningmulti-
Previous continual learning algorithms have introduced
ple tasks sequentially, and the existing solutions
novelmethodssuchasareplaymemorytostoreandreplay
havebeendrivenbytheconceptualizationofthe
thepreviouslylearnedexamples[Lopez-PazandRanzato,
plasticity-stabilitydilemma.However,theconver-
2017,Aljundietal.,2019b,Chaudhryetal.,2019a],regular-
genceofcontinuallearningforeachsequentialtask
izationmethodsthatpenalizeneuralnetworks[Kirkpatrick
islessstudiedsofar.Inthispaper,weprovidea
etal.,2017,Zenkeetal.,2017],Bayesianmethodsthatuti-
convergenceanalysisofmemory-basedcontinual
lizetheuncertaintyofparametersordatapoints[Nguyen
learningwithstochasticgradientdescentandem-
et al., 2018, Ebrahimi et al., 2020], and other recent ap-
piricalevidencethattrainingcurrenttaskscauses
proaches [Yoon et al., 2018, Lee et al., 2019]. The study
thecumulativedegradationofprevioustasks.We
ofcontinuallearninginBayesianframeworksformulatea
propose an adaptive method for nonconvex con-
trainedmodelforprevioustasksparameterintoanapprox-
tinuallearning(NCCL),whichadjustsstepsizes
imateposteriortolearnaprobabilisticmodelwhichhave
ofbothpreviousandcurrenttaskswiththegradi-
empirically good performance on entire tasks. However,
ents.Theproposedmethodcanachievethesame
Bayesianapproachescanfailinpracticeanditcanbehard
convergence rate as the SGD method when the
toanalyzetherigorousconvergenceduetotheapproxima-
catastrophicforgettingtermwhichwedefineinthe
tion.Thememory-basedmethodsaremorestraightforward
paperissuppressedateachiteration.Further,we
approaches,wherethelearnerstoresasmallsubsetofthe
demonstratethattheproposedalgorithmimproves
dataforprevioustasksintoamemoryandutilizesthemem-
theperformanceofcontinuallearningoverexisting
orybyreplayingsamplestokeepamodelstayinginafeasi-
methodsforseveralimageclassificationtasks.
bleregionwithoutlosingtheperformanceontheprevious
tasks.Gradientepisodicmemory(GEM)[Lopez-Pazand
Ranzato,2017]firstformulatedthereplaybasedcontinual
1 INTRODUCTION
learning as a constrained optimization problem. This for-
mulationallowsustorephrasetheconstraintsonobjectives
Learningnewtaskswithoutforgettingpreviouslylearned
forprevioustasksasinequalitiesbasedontheinnerproduct
tasksisakeyaspectofartificialintelligencetobeasversa-
of loss gradient vectors for previous tasks and a current
tileashumans.Unliketheconventionaldeeplearningthat
task.However,thegradientupdatebyGEMvariantscannot
observestasksfromani.i.d.distribution,continuallearning
guaranteeboththeoreticalandempiricalconvergenceofits
train sequentially a model on a non-stationary stream of
constrainedoptimizationproblem.Themodifiedgradient
data[Ring,1995,Thrun,1994].ThecontinuallearningAI
updates do not always satisfy the loss constraint theoreti-
systemsstrugglewithcatastrophicforgettingwhenthedata
cally,andwecanalsoobservetheforgettingphenomenon
accessofpreviouslylearnedtasksisrestricted[Frenchand
occursempirically.Italsoimpliesthatthisintuitiverefor-
Chater,2002].Althoughnovelcontinuallearningmethods
mulationviolatestheconstrainedoptimizationproblemand
successfully learn the non-stationary stream sequentially,
cannotprovidetheoreticalguaranteetopreventcatastrophic
studiesonthetheoreticalconvergenceanalysisofbothpre-
forgettingwithoutarigorousconvergenceanalysis.
vioustasksandacurrenttaskhavenotyetbeenaddressed.
Acceptedforthe39thConferenceonUncertaintyinArtificialIntelligence (UAI2023).
4202
rpA
8
]GL.sc[
1v55550.4042:viXraInthiswork,weexplainthecauseofcatastrophicforgetting methodanditsextensionwhichadjuststepsizesbe-
bydescribingcontinuallearningwithasmoothnonconvex tweentasksateachstepwiththeoreticalground,and
finite-sumoptimizationproblem.Inthestandardsingletask demonstratethatbothmethodsshowremarkableper-
case,SGD[GhadimiandLan,2013],ADAM[Reddietal., formanceonimageclassificationtasks.
2018], YOGI [Zaheer et al., 2018], SVRG [Reddi et al.,
2016a],andSCSG[Leietal.,2017]arethealgorithmsfor
2 RELATEDWORK
solving nonconvex problems that arise in deep learning.
Toanalyzetheconvergenceofthosealgorithms,previous
Memory-based methods. Early memory-based methods
worksstudythefollowingnonconvexfinite-sumproblem
utilizememorybythedistillation[Rebuffietal.,2017,Li
1 (cid:88)n and Hoiem, 2017] or the optimization constraint [Lopez-
min f(x)= f (x), (1)
x∈Rd n i PazandRanzato,2017,Chaudhryetal.,2019a].Especially,
i=1 A-GEM[Chaudhryetal.,2019a]simplifiestheapproach
whereweassumethateachobjectivef (x)withamodelx forconstraintviolatedupdatestepsastheprojectedgradi-
i
andadatapointindexi∈[n]foradatasetwithsizen(by entonareferencegradientwhichensuresthattheaverage
theconventionfornotationsinnonconvexoptimizationliter- memorylossoverprevioustasksdoesnotincrease.Recent
ature[Reddietal.,2016a])isnonconvexwithL-smoothness works[Chaudhryetal.,2019b,2020a,Riemeretal.,2018]
assumption.Ingeneral,wedenotef (x)asf(x;d )where haveshownthatupdatingthegradientsonmemorydirectly,
i i
d iisadatapointtuple(INPUT,OUTPUT)withindexi.We whichiscalledexperiencereplay,isalightandprominent
expect that a stochastic gradient descent based algorithm approach.Wefocusonconvergenceofcontinuallearning,
reachesastationarypointinsteadoftheglobalminimumin but the above methods focus on increasing the empirical
nonconvexoptimization.Unliketheconvexcase,thecon- performance without theoretical guarantee. Our analysis
vergence is generally measured by the expectation of the providesalegitimatetheoreticalconvergenceanalysisun-
squared norm of a gradient E∥∇f(x)∥2. The theoretical derthestandardsmoothnonconvexfinite-sumoptimization
computational complexity is derived from the ϵ-accurate problem setting. Further, [Knoblauch et al., 2020] shows
solution, which is also known as a stationary point with theperfectmemoryforoptimalcontinuallearningisNP-
E∥∇f(x)∥2 ≤ ϵ.Thegeneralnonconvexfinite-sumprob- hard by using set-theory, but the quantitative analysis of
lems assume that all data points can be sampled during performancedegradationislessstudied.
trainingiterations.Thisfactisanobstacletodirectlyapply
Adaptivestepsizesinnonconvexsetting.Adaptivestep
(1)forcontinuallearningproblem.
sizesundersmoothnonconvexfinite-sumoptimizationprob-
We provide a solution of the above issue by leveraging lemhavebeenstudiedongeneralsingletaskcases[Reddi
memory-based methods, which allow models to access a etal.,2018,Zhangetal.,2020,Zaheeretal.,2018]recently.
partialaccesstothedatasetofprevioustasks.Inthissetting, [Simseklietal.,2019,Zhangetal.,2020,Simseklietal.,
wecananalyzenonconvexstochasticoptimizationproblems 2020]haverevealedthatthereexistsaheavy-tailednoisein
ontheconvergenceofprevioustaskswithlimitedaccess. someoptimizationproblemsforneuralnetworks,suchasat-
Similarwithadaptivemethodsfornoncovexoptimization, tentionmodels,and[Zhangetal.,2020]showsthatadaptive
weapplyadaptivestepsizesduringoptimizationtomini- methodsarehelpfultoachievethefasterconvergenceunder
mizeforgettingwiththeoreticalguarantee.Specifically,we theheavy-taileddistributionwherestochasticgradientsare
makethefollowingcontributions: poorlyconcentratedaroundthemean.Inthiswork,wetreat
thecontinuallearningproblemwherestochasticgradients
• Wedecomposethefinite-sumproblemofentiretasks ofprevioustasksareconsideredastheout-of-distribution
intotwosummationtermsforprevioustasksandacur- samplesin regardto acurrent task,anddevelopadaptive
renttask,respectively.Wetheoreticallyshowthatsmall methodswhicharewell-performedincontinuallearning.
randomsubsetsofprevioustasksleadtoanalyzingthe
expectedconvergencerateofbothtaskswhilelearning
3 PRELIMINARIES
acurrenttask.
• Westudytheconvergenceofgradientmethodsunder
Supposethatweobservethelearningprocedureonadata
asmallmemorywherethebackwardtransferperfor-
streamofcontinuallearningatsomearbitraryobservation
mance degrades, and propose a new formulation of
point.Letusconsidertimestept=0asgivenobservation
continual learning problem with the forgetting term.
point.WedefinetheprevioustaskP fort<0asallvisited
Wethenshowwhycatastrophicforgettingoccurstheo-
datapointsandthecurrenttaskCfort≥0asalldatapoints
reticallyandempirically.
whichwillfaceinthefuture.Then,P andC canbedefined
• Though memory-based methods mitigate forgetting, as the sets of data points in P and C at time step t = 0,
previousworksdoesnotfullyexploitthegradientin- respectively.Notethattheabovetaskdescriptionisbased
formationofmemory.Weintroduceanoveladaptive onnotasequenceofmultipletasks,buttwoseparatesetstoanalyzetheconvergenceofeachofP andCwhenstartingto WederiveEquation5inAppendixC.Assumption3.1isa
updatethegivenbatchatthecurrenttaskCatsomearbitrary well-knownandusefulstatementinnonconvexfinite-sum
observationpoint.Weconsideracontinuallearningproblem optimization problem [Reddi et al., 2016a, 2018, Zhang
as a smooth nonconvex finite-sum optimization problem et al., 2020, Zaheer et al., 2018], and also helps us to de-
withtwodecomposedobjectives scribe the convergence of continual learning. We also as-
sumethesupremumoflossgapbetweenaninitialpointx0
andaglobaloptimumx∗as∆ ,andtheupperboundonthe
f
1 (cid:88) varianceofthestochasticgradientsasσ f inthefollowing.
min h(x)= h (x), (2)
x∈Rd n f +n g i∈P∪C i ∆ =supf(x0)−f(x∗),
f
x0
wheren andn arethenumbersofelementsforP andC,
andh(x)f canbeg
decomposedintoasfollows: σ2 =sup
1
(cid:88)nf
∥∇f (x)−∇f(x)∥2.
f n i
x f
n n i=1
h(x)= f f(x)+ g g(x).
n +n n +n
f g f g
It should be noted that g (x),∇g (x), which denote the
j j
lossandthegradientforacurrenttask,alsosatisfyallthree
Forclarity,weusef(x)=h(x)| andg(x)=h(x)| for
P C aboveassumptionsandthefollowingstatement.
therestrictionofhtoeachdatasetP andC,respectively.
f (x)andg (x)alsodenotestheobjectivetermsinduced Tomeasuretheefficiencyofastochasticgradientalgorithm,
i j
from data where each index is i ∈ P and j ∈ C, respec- wedefinetheIncrementalFirst-orderOracle(IFO)frame-
tively. work [Ghadimi and Lan, 2013]. IFO call is defined as a
unitofcomputationalcostbytakinganindexiwhichgets
SupposethatthereplaymemoriesM fortimestep∈[0,T]
t the pair (∇f (x),f (x)), and IFO complexity of an algo-
i i
arerandomvariableswhicharethesubsetsofP∪Ctocover
rithmisdefinedasthesummationofIFOcallsduringopti-
priormemory-basedapproaches[Chaudhryetal.,2019b,a].
mization.Forexample,avanillastochasticgradientdescent
Toformulateanalgorithmformemory-basedapproaches,
(SGD)algorithmrequirescomputationalcostasmuchas
wedefinemini-batchesI whicharesampledfromamem-
t the batch size b at each step, and the IFO complexity is
t
ory M
t
at step t. We now define the stochastic update of thesumofbatchsizes(cid:80)T
b .LetT(ϵ)betheminimum
t=1 t
memory-basedmethod
numberofiterationstoguaranteeϵ-accuratesolutions.The
averageboundofIFOcomplexityislessthanorequalto
xt+1 =xt−α Ht∇f It(xt)−β Ht∇g Jt(xt), (3) (cid:80)T(ϵ)b =O(1/ϵ2)[Reddietal.,2016a].
t=1 t
whereI ⊂M andJ ⊂C denotethemini-batchesfrom
t t t
thereplaymemoryandthecurrentdatastream,respectively. 4 CONTINUALLEARNINGAS
Here,H istheunionofI andJ .Inaddition,foragivenset
t t t NONCONVEXOPTIMIZATION
S,∇f (xt),∇g (xt)denotethelossgradientofamodel
S S
xt withthemini-batchS attimestept.Theadaptivestep
We first present a theoretical convergence analysis of
sizes(learningrates)of∇f (xt)and∇g (xt)aredenoted
It Jt memory-basedcontinuallearninginnonconvexsetting.We
byα andβ whicharethefunctionsofH .
Ht Ht t aim to understand why catastrophic forgetting occurs in
Itshouldbenotedthemini-batchI fromM mightcontain termsoftheconvergencerate,andreformulatetheoptimiza-
t t
adatapointj ∈C forsomecases,suchasER-Reservoir. tionproblemofcontinuallearningintoanonconvexsetting
withtheoreticalguarantee.Forcompletenesswepresentall
Throughout the paper, we assume L-smoothness and the
proofsinAppendixC.
followingstatements.
Assumption3.1. f isL-smooththatthereexistsaconstant
i 4.1 MEMORY-BASEDNONCONVEXCONTINUAL
L>0suchthatforanyx,y ∈Rd,
LEARNING
∥∇f (x)−∇f (y)∥≤L∥x−y∥ (4)
i i Unlikeconventionalsmoothnonconvexfinite-sumoptimiza-
tionproblemswhereeachmini-batchisi.i.d-sampledfrom
where∥·∥denotestheEuclideannorm.Thenthefollowing
thewholedatasetP ∪C,thereplaymemorybasedcontin-
inequalitydirectlyholdsthat
uallearningencountersanon-i.i.dstreamofdataC with
L accesstoasmallsizedmemoryM t.Algorithm1provides
− 2∥x−y∥2 ≤f i(x)−f i(y)−⟨∇f i(y),x−y⟩ thepseudocodeformemory-basedapproachwiththeitera-
tiveupdaterule3.Now,wecananalyzetheconvergenceon
L
≤ ∥x−y∥2. (5) P andC duringalearningprocedureonanarbitrarydata
2𝑡𝑡 𝑡𝑡
∇𝑓𝑓𝐼𝐼𝑡𝑡(𝑥𝑥 ) ∗ ∇𝑓𝑓𝐼𝐼𝑡𝑡(𝑥𝑥 ) ∗
𝑥𝑥𝑃𝑃∪𝐶𝐶 𝑥𝑥𝑃𝑃∪𝐶𝐶
𝑡𝑡 𝑡𝑡
𝛼𝛼𝐻𝐻𝑡𝑡∇𝑓𝑓𝐼𝐼𝑡𝑡 𝑥𝑥 +𝛽𝛽𝐻𝐻𝑡𝑡∇𝑔𝑔𝐽𝐽𝑡𝑡,𝑝𝑝𝑝𝑝𝑝𝑝(𝑥𝑥 )
𝑡𝑡
𝑥𝑥𝑃𝑃∗ =𝑥𝑥0 ∇𝑔𝑔𝐽𝐽𝑡𝑡,𝑝𝑝𝑝𝑝𝑝𝑝(𝑥𝑥 ) 𝑥𝑥𝑃𝑃∗ =𝑥𝑥0 𝛼𝛼𝐻𝐻𝑡𝑡∇𝑓𝑓𝐼𝐼𝑡𝑡 𝑥𝑥𝑡𝑡 +𝛽𝛽𝐻𝐻𝑡𝑡∇𝑔𝑔𝐽𝐽𝑡𝑡,𝑛𝑛𝑛𝑛𝑛𝑛(𝑥𝑥𝑡𝑡 )
𝑡𝑡
∇𝑔𝑔𝐽𝐽𝑡𝑡,𝑛𝑛𝑛𝑛𝑛𝑛(𝑥𝑥 )
Figure1:GeometricillustrationofNon-ConvexContinualLearning(NCCL).Incontinuallearning,amodelparameterxt
startsfromalocaloptimalpointforthepreviouslylearnedtasksx∗.OverT iterations,weexpecttoreachanewoptimal
P
pointx∗ whichhasagoodperformanceonbothP andC.Inthet-thiteration,xt encounterseither∇g (xt)or
P∪C Jt,pos
∇g (xt).Thesetwocasesindicatewhether⟨f (xt),∇g (xt)⟩ispositiveornot.Topreventxt fromescapingthe
Jt,neg It Jt
feasibleregion,i.e.,catastrophicforgetting,weimposeatheoreticalconditiononlearningratesforf andg.
Algorithm1NonconvexContinualLearning(NCCL) Lemma4.1. IfM isuniformlysampledfromP,thenboth
0
episodicmemoryandER-reservoirsatisfies
Require: Previous task set P, current task set C, initial
modelx0. E (cid:2) ∇f (xt)(cid:3) =∇f(xt) and E [e ]=0.
SampleainitialmemoryM ⊂P ▷Byreplayschemes,
M[0:t] Mt M[0:t] Mt
0
theselectiondist.ofM aredifferent.
0
Notethattakingexpectationiterativelywithrespecttothe
fort=0toT −1do
historyM isneededtocomputetheexpectedvalueof
Sampleamini-batchI ⊂M [0:t]
t t gradients for M . Surprisingly, taking the expectation of
Sampleamini-batchJ ⊂C t
t
overfittingerrorovermemoryselectiongetszero.However,
Computestepsizesα ,β by∇f (xt),∇g (xt)
Ht Ht It Jt itdoesnotimplye = 0foreachlearningtrialwithsome
xt+1 ←xt−α ∇f (xt)−β ∇g (xt) t
Ht It Ht Jt M .
UpdateM bytheruleofreplayschemewithJ . [0:t]
t+1 t
endfor
4.2 THEORETICALCONVERGENCEANALYSIS
stream from two consecutive sets P and C for continual Wenowproposetwotermsofinterestinagradientupdate
learning[Chaudhryetal.,2019a,b,2020b]. of nonconvex continual learning (NCCL). We define the
overfittingtermB andthecatastrophicforgettingtermΓ
BylimitedaccesstoP,theexpectationofgradientupdate t t
E [∇f (xt)]inEquation3forf(x)isabiasedesti- asfollows:
It⊂Mt It
mateofthegradient∇f(xt).Atthetimestept,wehave B =(Lα2 −α )⟨∇f(xt),e ⟩+β ⟨∇g (xt),e ⟩,
t Ht Ht t Ht Jt t
∇f Mt(xt)=E It(cid:2) ∇f It(xt)|M t(cid:3) =E It(cid:2) ∇f(xt)+e t|M t(cid:3) Γ
t
= β H2 2tL ∥∇g Jt(xt)∥2−β Ht(1−α HtL)⟨∇f It(xt),∇g Jt(xt)⟩.
=∇f(xt)+e ,
Mt
Theamountofeffectonconvergencebyasingleupdatecan
wheree ande denotetheerrorterms,∇f (xt)−∇f(xt)
t Mt It bemeasuredbyusingEquation5asfollows:
andtheexpectationoverI givenM ,respectively.Itshould
t t
benotedthatagivenreplaymemoryM twithsmallsizeat f(xt+1)≤f(xt)−⟨∇f(xt),α Ht∇f It(xt)+β Ht∇g Jt(xt)⟩
timesteptintroducesaninevitableoverfittingbias.
L
+ ∥α ∇f (xt)+β ∇g (xt)∥2 (6)
For example, there exist two popular memory schemes, 2 Ht It Ht Jt
episodic memory and ER-reservoir. The episodic mem-
by letting x ← xt+1 and y ← xt. Note that the above
ory M = M for all t is uniformly sampled once from
t 0 inequalitycanberewrittenas
arandomsequenceofP,andER-reservoiriterativelysam-
(cid:18) (cid:19)
ples the replay memory M by the selection rule M ⊂ L
t t f(xt+1)≤f(xt)− α − α2 ∥∇f(xt)∥2+Γ +B
M t−1∪J t.Here,wedenotethehistoryofM tasM [0:t] = Ht 2 Ht t t
(M ,··· ,M ).Tocomputetheexpectationoverallstochas-
0 t L
ticities of NCCL, we need to derive the expectation of + 2α H2 t∥e t∥2.
∇f (xt) over the randomness of M . We formalize the
Mt t
expectation over all learning trials with the selection ran- A NCCL algorithm update its model with two additional
domnessasfollows. termsB ,Γ comparedtoconventionalSGD.Anoverfitting
t ttermB andacatastrophicforgettingtermΓ areobtained Thus, the convergence of a current task C is guaranteed,
t t
by grouping terms that contain e and ∇g (xt), respec- sinceitssupersetM ∪C isconverged.Otherwise,thecon-
t Jt
tively.Thesetwotermsinevitablydegradetheperformance vergenceratemightdifferfromtheconventionalSGDforC
ofNCCLwithrespecttotime.ItshouldbenotedthatΓ has bythegiven∆ ,σ attime0,buttheasymptotic
t h|M∪C h|M∪C
⟨∇f (xt),∇g (xt)⟩,whichisakeyfactortodetermine convergencerateisstillidentical.
It Jt
interferenceandtransfer[Riemeretal.,2018].Ontheother
OnekeyobservationisthatE[Γ ]arecumulativelyadded
hand,B includese ,whichisanerrorgradientbetweenthe t
t t ontheupperboundofE∥∇f(x)∥2,whichisaconstantin
batchfromM andtheentiredatasetP.
t conventionalSGD.Thelossgap∆ andthevarianceofgra-
f
SincetakingtheexpectationoverallstochasticitiesofNCCL dientsσ arefixedvalues.Inpractice,tightening(cid:80) E[Γ ]
f t t
impliesthetotalexpectation,wedefinetheoperatoroftotal appearstobecriticalfortheperformanceofNCCL.How-
√
expectationwithrespectto0≤t<T foreaseofexposition
ever,(cid:80)T−1E[Γ
]/ T isnotguaranteedtoconvergeto0.
t=0 t
asfollows: Thisfactgivesrisetocatastrophicforgettingintermsofa
E t =E M[0:t](cid:2)E It[E Jt[·|I t]]|M [0:t](cid:3) . n oo fn thd eec cr oe na vsi en rg geu np cp eer ofbo (cid:80)un T t=d −. 01W Ee [n Γo tw ]/√sh Tow . thekeycondition
Inaddition,wedenoteE =E.Wefirststatethestepwise Lemma4.5. Letanupperboundβ >β >0.Consider
T−1 Ht
changeofupperbound. twocases,β <αandβ ≥αforαinTheorem4.3.Wehave
thefollowingbound
Lemma4.2. SupposethatAssumption3.1holdsand0<
α ≤ 2.ForxtupdatedbyAlgorithm1,wehave
Ht L
(cid:34) (cid:35)
T (cid:88)−1E √[Γ t] <O(cid:16) 1/T3/2+1/T(cid:17)
whenβ <α,
f(xt)−f(xt+1)+B +Γ T
E ∥∇f(xt)∥2 ≤E t t t=0
t t
(cid:34)
α Ht(1− L 2α Ht (cid:35)) T (cid:88)−1E √[Γ t] <O(cid:16)√
T
+1/√ T(cid:17)
, whenβ ≥α.
+E
α HtL
σ2 . (7) t=0
T
t 2(1− Lα ) f
2 Ht
With the following theorem, we show that f(x) can con-
Surprisingly, we observe E t[B t] = 0 by Lemma 4.1. It vergeevenifwehavelimitedaccesstoP.
shouldbealsonotedthattheindividualtrialwitharandomly
givenM 0cannotcanceltheeffectofB t.Wediscussmore Theorem4.6. Letβ Ht <α= √c
T
forallt.Thenwehave
detailsofoverfittingtomemoryinAppendixE. theconvergencerate
We now describe a convergence analysis of Algorithm 1. (cid:18) 1 (cid:19)
minE∥∇f(xt)∥2 ≤O √ . (9)
We telescope over training iterations for the current task,
t T
whichleadstoobtainthefollowingtheorem.
√ Otherwise,f(x)isnotguaranteedtoconvergewhenβ ≥α
Theorem4.3. Letα
Ht
=α= √c
T
forsome0<c≤ 2 LT andmightdivergeattherateO(√
T).
and t ∈ {0,··· ,T − 1}. By Lemma 4.2, the iterates of
NCCLsatisfy Corollary 4.7. For β
Ht
< α = √c
T
for all t, the IFO
complexityofAlgorithm1toobtainanϵ-accuratesolution
is:
A (cid:32) 1(cid:32) T (cid:88)−1 (cid:33) Lc (cid:33) IFOcalls=O(1/ϵ2). (10)
minE∥∇f(xt)∥2 ≤ √ ∆ + E[Γ ] + σ2
t T c f t 2 f
t=0
Webuildintuituionsabouttheconvergenceconditionofthe
whereA=1/(1−Lα/2). previoustasksP inTheorem4.6.Asempiricallyshownin
stable A-GEM and stable ER-Reservoir [Mirzadeh et al.,
We also prove the convergence rate of a current task C 2020],theconditionofβ Ht <αtheoreticallyimpliesthat
with the gradient udpates from the replay-memory M in decaying step size is a key solution to continual learning
continuallearining. consideringwhenwepickanyarbitraryobservationpoints.
Lemma4.4. SupposethatI ∩J =∅,Takingexpectation Remark4.8. Topreventcatastrophicforgetting,thestep
t t
overI ⊂M andJ ⊂C,wehave sizeofg(x),β shouldbelowerthanthestepsizeoff(x),
t t t Ht
(cid:114)
2∆ L
α Ht.ItshouldalsobenotedthatE M[1:t][B t|M 0]isnotal-
m tinE∥∇h| M∪C(xt)∥2 ≤ h| TM∪C σ h|M∪C, (8) w tria ay ls w0 itf hor da ifn fy erM en0 t. gT ivh eis ni Mmpli ae ls soth ha at, sf tr ho em nt oim n-e zes rte op c0 u, me uac lah
-
0
where∆ andσ istheversionoflossgapand
tivesum(cid:80)E
[B |M ],whichoccursoverestimating
h|M∪C h|M∪C M[1:T] t 0
thevarianceforhonM ∪C,respectively. biastheoretically.The convergence rate with respect to the marginalization
onM 0inTheorem4.6exactlymatchtheusualnonconvex (cid:28) ∇f (xt) (cid:29) ∇f (xt)
SGDrates.TheselectionrulesforM 0withvariousmemory ∇g Jt(xt)− ∥∇fIt (xt)∥,∇g Jt(xt) ∥∇fIt (xt)∥.
schemesareimportanttoreducethevarianceofconvergence It It
rate with having the mean convergence rate as Equation
Letβ bethestepsizeforg(x)whentheconstraintisnot
9 among trials. This is why memory schemes matters in
violated.Thenwecaninterpretthesurrogateasanadaptive
c do etn at ii ln su ia nl Ale pa pr en nin dg ixi En .terms of variance. Please see more learningrateα Ht,whichisα(1−⟨∇fI ∥t ∇(x ft I) t, (∇ xtg )J ∥t 2(xt)⟩ )tocan-
celoutthenegativecomponentof∇f (xt)on∇g (xt).
It Jt
4.3 REFORMULATEDPROBLEMOFCONTINUAL For the transfer case Λ Ht > 0, A-GEM use α Ht = 0.
LEARNING Afterapplyingthesurrogate,E[Γ t]isreducedasshownin
AppendixD.ItisnotedthatA-GEMtheoreticallyviolates
The previous section showed the essential factors in con- theconstraintsof(11)topreventcatastrophicforgettingby
tinuallearningtoobservethetheoreticalconvergencerate. lettingα Ht =0anddoesnotutilizethebettertransfereffect.
TheoverfittingbiastermB hasastrongdependenceonthe Then, A-GEM is an adaptive method without theoretical
t
memoryselectionruleandcanbecomputedexactlyonly guarantee.
ifwecanaccesstheentiredatasetP duringlearningonC.
Intermsofexpectation,wehaveshownthattheeffectof
5.2 NCCL
B isnegligible.Wealsoshowthatitsempiricaleffectis
t
lessimportantthanΓ t inFigure2.Thenwefocusonthe Asdiscussedabove,wenotethatE[Γ ]isaquadraticpoly-
t
performancedegradationbythecatastrophicforgettingterm
nomial of β . For the interference case Λ ≤ 0, the
Γ t.Foreverytrial,theworst-caseconvergenceisdependent minimum poH int t of polynomial, β∗ has a neH gt ative value
o pen r∆ bof u+ nd(cid:80) anT t= d− 0 k1 eE ep[Γ tht] eb my oT dh ee lo tore bm e4 c. o3 n. vT eo rgt eig dh ,t wen esth he ouu lp d- whichviolatestheconstraintβ Ht H >t 0,andE[Γ t]ismono-
tonically increasing on β > 0. Then, we instead adapt
minimizethecumulativesumofΓ t. Wenowreformulate α toreducethevalueoH ft E[Γ ]attimetbyadoptingthe
thecontinuallearningproblem2asfollows.
Ht t
scheme of A-GEM. The minimum of the polynomial on
E[Γ ]canbeobtainedwhenthecaseoftransfer,Λ > 0
T−1 t Ht
minimize (cid:88) E[Γ t] bydifferentiatingonβ Ht.ThentheminimumE[Γ∗ t]andthe
αHt,βHt
t=0
optimalstepsizeβ H∗
t
canbeobtainedas
subjectto 0<β <α ≤2/Lforallt<T (11)
Ht Ht
β∗ =
(1−α HtL)Λ
Ht,
E[Γ∗]=−(1−α HtL)Λ
Ht.
Ht L∥∇g (xt)∥2 t 2L∥∇g (xt)∥2
Jt Jt
Itisnotedthattheabovereformulationpresentsatheoreti-
callyguaranteedcontinuallearningframeworkformemory-
Tosatisfytheconstraintsof(11),weshouldupdate∇f (xt)
basedapproachesinnonconvexsettingandtheconstraintis
It
withnon-zerostepsizeandβ <α forallt.Thenthe
toguaranteetheconvergenceofbothf(x)andg(x). Ht Ht
proposedadaptivemethodformemory-basedapproachesis
givenby
5 ADAPTIVEMETHODSFOR
CONTINUALLEARNING α
Ht
=(cid:40) αα( ,1− ∥∇fΛ ItH (xt t)∥2), ΛΛ Ht >≤ 00
,
Ht
AsdiscussedintheaboveSection,wecansolveamemory- (cid:40)
α, Λ ≤0
b tia vs ee mdc eo thn oti dn sua al rele va arn rii an ng tsby ofm Si Gni Dm ,iz win hg ic(cid:80) haT t= u− t0 o1 mE a[Γ tit c] a. lA lyd aa dp -- β Ht = min(cid:16) α(1−δ), L( ∥1 ∇− gα JL t() xΛ tH )∥t 2(cid:17) , ΛH Ht
t
>0
justthestepsize(learningrate)onaper-featurebasis.Inthis √
section,wereviewA-GEMintermsofadaptivemethods, whereα=c/ T andδissomeconstant0<δ ≪1.Note
and also propose a new algorithm (NCCL) for achieving thatourtwoadaptivelearningratesareastepwisegreedy
adaptivityincontinuallearning.Forbrevity,wedenotethe perspectivechoiceofmemory-basedcontinuallearning.
innerproduct⟨∇f (xt),∇g (xt)⟩asΛ .
It Jt Ht
6 EXPERIMENTS
5.1 A-GEM
We use two following metrics to evaluate algorithms. (1)
A-GEM [Chaudhry et al., 2019a] propose a surrogate of Averageaccuracyisdefinedas 1 (cid:80)T a ,wherea
T j=1 T,j i,j
∇g (xt)asthefollowingequationtoavoidviolatingthe denotesthetestaccuracyontaskj aftertrainingontaski.
Jt
constraintwhenthecaseofinterference,Λ ≤0: (2)Forgettingistheaveragemaximumforgettingisdefined
Ht0.20 A-GEM A ER-G -rE inM g 1000 2.5
ER-reservoir 800
0.15 N NC CC CL L- -r ri en sg ervoir 600 2.0
0.10 ER-ring ER-reservoir 400
1.5
0.05 NCCL-reservoir 200
0.00 NCCL-ring 0 1.0
0 2000 4000 6000 8000 012345678910111213141516171819 5 10 15 20
Cumulative gamma Tasks Gradient norm
(a) (b) (c)
4 bias
20
15
2
15 10 0
10
2
5
5 4
0.0 0.5 1.0 1.5 2.0 0 1000 2000 3000 4000 5000 0123456789101112131415161718
Cumulative gamma Cumulative gamma Tasks
(d) (e) (f)
Figure2:Metricsforcontinuallearning(CL)algorithmstrainedonsplit-CIFAR100withdifferent5seeds.(a)Forgetting
versus(cid:80)E[Γ ]attheendoftraining.(b)Evolutionof(cid:80)E[Γ
]duringcontinuallearning.(c)Empiricalverificationof
t t
therelationbetween∥∇f(x)∥forthefirsttaskandtestlossofthefirsttaskinsplitCIFAR-100.(d)-(e)aretheempirical
verificationof(cid:80)E[Γ
]versus∥∇f(x)∥forthefirsttaskinCLalgorithms.Theredhorizontallineindicatestheempirical
t
∥∇f(x)∥rightaftertrainingthefirsttask.(f)IllustrationofempiricalB attheendofeachtask.
t
as 1 (cid:80)T−1 max (a −a ). Due to limited space, iteration for experience replay. ER-Reservoir [Chaudhry
T−1 j=1 l,j T,j
l∈[T−1] etal.,2019b]showsapowerfulperformanceincontinual
wereportthedetailsofarchitectureandlearningprocedure
learningscenario.GEMandA-GEM[Lopez-PazandRan-
andmissingresultswithadditionaldatasetsinAppendixB.
zato,2017,Chaudhryetal.,2019a]usegradientepisodic
memory to overcome forgetting. The key idea of GEM
isgradientprojectionwithquadraticprogrammingandA-
6.1 EXPERIMENTALSETUP
GEMsimplifiesthisprocedure.WealsocomparewithiCarl,
MER,ORTHOG-SUBSPACE[Chaudhryetal.,2020b],sta-
Datasets.Wedemonstratetheexperimentalresultsonstan-
bleSGD[Mirzadehetal.,2020],andMC-SGD[Mirzadeh
dard continual learning benckmarks: Permuted-MNIST
etal.,2021].
[Kirkpatricketal.,2017]isaMNIST[LeCunetal.,1998]
baseddataset,whereeachtaskhasafixedpermutationof
pixelsandtransformdatapointsbythepermutationtomake 6.2 EXPERIMENTRESULTS
eachtaskdistributionunrelated.Split-MNIST[Zenkeetal.,
2017]splitsMNISTdatasetintofivetasks.Eachtaskcon- Thefollowingtablesshowourmainexperimentalresults,
sistsoftwoclasses,forexample(1,7),(3,4),andhasap- whichisaveragedover5runs.Wedenotethenumberofex-
proximately12Kimages.Split-CIFAR10,100,andMini- amplesperclasspertaskatthetopofeachcolumn.Overall,
ImagenetalsosplitversionsofCIFAR-10,100[Krizhevsky NCCL + memory schemes outperform baseline methods
etal.,2009],andMiniImagenet[Vinyalsetal.,2016]into especially in the forgetting metric. Our goal is to demon-
fivetasksand20tasks. stratetheusefulnessoftheadaptivemethodstoreducethe
catastrophicforgetting,andtoshowempiricalevidencefor
Baselines. We report the experimental evaluation on the
ourconvergenceanalysis.WeremarkthatNCCLsuccess-
onlinecontinualsettingwhich impliesamodelistrained
fullysuppressforgettingbyalargemargincomparedto
withasingleepoch.Wecomparewiththefollowingcon-
baselines.ItisnotedthatNCCLalsooutperformsA-GEM,
tinual learning baselines. Fine-tune is a simple method
whichdoesnotmaximizetransferwhenΛ >0andvio-
thatamodeltrainsobserveddatanaivelywithoutanysup- Ht
latestheproposedconstraintsin(11).
port,suchasreplaymemory.Elasticweightconsolidation
(EWC) is a regularization based method by Fisher Infor- We now investigate the proposed terms with regard to
mation[Kirkpatricketal.,2017].ER-Reservoirchooses
memory-basedcontinuallearning,(cid:80)E[Γ
]andB .Tover-
t t
samplestostorefromadatastreamwithaprobabilitypro- ifyourtheoreticalanalysis,inFigure2weshowthecumula-
portional to the number of observed data points. The re- tivecatastrophicforgettingterm(cid:80) E[Γ ]isthekeyfactor
t t
playmemoryreturnsarandomsubsetofsamplesateach oftheconvergenceofthefirsttaskinsplit-CIFAR100.Dur-
gnittegroF
mron
tneidarG
ammag
evitalumuC
mron
tneidarG
ssol
tseT
saiBTable1:AccuaryandForgetitngresultsbetweentheproposedmethods(NCCL+Ring,NCCL+Reservoir)andotherbaselines
intask-incrementallearning.Whenthereplay-memoryisused,wedenotethememorysizeasthenumberofexamplesper
classpertask.TheadditionalresultsandthedetailedsettingwithdifferentmemorysizeisinAppendixB
dataset Permuted-MNIST split-CIFAR100 split-MiniImagenet
Method
memorysize 5 5 1
memory accuracy forgetting accuracy forgetting accuracy forgetting
Fine-tune ✗ 47.9 0.29(0.01) 40.4(2.83) 0.31(0.02) 36.1(1.31) 0.24(0.03)
EWC ✗ 63.1(1.40) 0.18(0.01) 42.7(1.89) 0.28(0.03) 34.8(2.34) 0.24(0.04)
stableSGD ✗ 80.1(0.51) 0.09(0.01) 59.9(1.81) 0.08(0.01) - -
MC-SGD ✗ 85.3(0.61) 0.06(0.01) 63.3(2.21) 0.06(0.03) - -
A-GEM ✓ 64.1(0.74) 0.19(0.01) 59.9(2.64) 0.10(0.02) 42.3(1.42) 0.17(0.01)
ER-Ring ✓ 75.8(0.24) 0.07(0.01) 62.6(1.77) 0.08(0.02) 49.8(2.92) 0.12(0.01)
ER-Reservoir ✓ 76.2(0.38) 0.07(0.01) 65.5(1.99) 0.09(0.02) 44.4(3.22) 0.17(0.02)
ORHOG-subspace ✓ 84.32(1.1) 0.11(0.01) 64.38(0.95) 0.055(0.007) 51.4(1.44) 0.10(0.01)
NCCL+Ring ✓ 84.41(0.32) 0.053(0.002) 61.09(1.47) 0.02(0.01) 45.5(0.245) 0.041(0.01)
NCCL+Reservoir ✓ 88.22(0.26) 0.028(0.003) 63.68(0.18) 0.028(0.009) 41.0(1.02) 0.09(0.01)
Multi-task 91.3 0 71 0 65.1 0
ingcontinuallearning,(cid:80) E[Γ ]increasesinallmethodsof betweenforgettingonP andlearningonC.InAppendix
t t
Figure2b.Figrure2a,2d,2eshowthatthelarger(cid:80) E[Γ ] B,weaddmoreresultswithlargersizesofmemory,which
t t
causesthelargerforgettingand∥∇f(x)∥forthefirsttask. showsthatNCCLoutperformsintermsofaverageaccuracy.
Wecanobservethat∥∇f(x)∥getslargerthan4,whichis Itmeansthatestimatingtransferandinterferenceintermsof
fortheredline,when(cid:80) E[Γ ]becomeslargerthan2.We Λ toalleviateforgettingbythesmallmemoryforNCCL
t t Ht
alsoverifythatthetheoreticalresultE [B ]=0isvalidin islesseffective.
t t
Figure2f.ItimpliesthattheempiricalresultsofLemma4.1,
Table2:Resultsofclassincrementalsplit-CIFAR100with
whichshowtheeffectofB onEquation7.Furthermore,the
t
Memorysize=10,000.
memorybiashelpstotightentheconvergencerateofP by
havingnegativevaluesinpractice.Evenwithtinymemory,
theestimatedB hasmuchsmallervaluethanE[Γ ]aswe Methods accuracy
t t
canobserveinFigure2.Forexperiencereplay,weneednot Finetune 3.06(0.2)
toworryaboutthedegradationbymemorybiasandwould A-GEM 2.40(0.2)
liketoemphasizethattinymemorycanslightlyhelptokeep GSS-Greedy[Aljundietal.,2019b] 19.53(1.3)
the convergence on P empirically. We conclude that the MIR[Aljundietal.,2019a] 20.02(1.7)
overfittingbiastermmightnotbeamajorfactorindegrad- ER+GMED[Jinetal.,2020] 20.93(1.6)
ingtheperformanceofcontinuallearningagentwhenitis MIR+GMED[Jinetal.,2020] 21.22(1.0)
comparedtothecatastrophicforgettingtermΓ t.Next,we NCCL-Reservoir(ours) 21.95(0.3)
modify the clipping bound of β in Section of adaptive
Ht
methodstoresolvethelowerperformanceintermsofav-
erageaccuracy.InTable1,NCCL+Ringdoesnothavethe
7 CONCLUSION
bestaverageaccuracyscore,eventhoughithasthelowest
valueof(cid:80)E[Γ
].Aswediscussedearlier,itisbecausethe
t
We have presented a theoretical convergence analysis of
convergencerateofC isslowerthanvanillaER-Ringwith
continuallearning.Ourproofshowsthatatrainingmodel
thefixedstepsizes.Now,weremovetherestrictionofβ ,
(cid:16) (cid:17)
Ht
cancircumventcatastrophicforgettingbysuppressingcatas-
min α(1−δ), L( ∥1 ∇− gα JL t() xΛ tH )∥t 2 for Λ Ht > 0, and instead trophicforgettingtermintermsoftheconvergenceonpre-
applythemaximumclippingboundβ tomaximizethe
max vious task. We demonstrate theoretically and empirically
transfereffect,whichoccursifΛ >0,bygettingE[Γ∗].
Ht t thatadaptivemethodswithmemoryschemesshowthebet-
In the original version, we force β < α to reduce the-
Ht terperformanceintermsofforgetting.Itisalsonotedthat
oreticalcatastrophicforgettingtermcompletely.However,
thereexisttwofactorsontheconvergenceofprevioustask:
replacingwithβ ishelpfulintermsofaverageaccuracy
max catastrophicforgettingandoverfittingtomemory.Finally,it
as shown in Appendix B. It means that β is a hyper-
max isexpectedtheproposednonconvexframeworkishelpful
parameter to increase the average accuracy by balancing
toanalyzetheconvergencerateofCLalgorithms.AuthorContributions SaynaEbrahimi,MohamedElhoseiny,TrevorDarrell,and
MarcusRohrbach. Uncertainty-guidedcontinuallearn-
SeungyubHaninitatedandledthiswork,implementedthe ingwithbayesianneuralnetworks. In8thInternational
proposedmethod,andwrotethepaper.YeongmoKimset ConferenceonLearningRepresentations,ICLR2020,Ad-
up and ran baseline tests. Taehyun Cho helped to write disAbaba,Ethiopia,April26-30,2020.OpenReview.net,
thepaperandverifiedmathematicalproofs.JungwooLee 2020.
advisedonthiswork.Correspondingauthor:JungwooLee
RobertM.FrenchandNickChater. Usingnoisetocompute
(e-mail:junglee@snu.ac.kr)
errorsurfacesinconnectionistnetworks:Anovelmeans
ofreducingcatastrophicforgetting. NeuralComputation,
Acknowledgements 14(7):1755–1769,2002.
Saeed Ghadimi and Guanghui Lan. Stochastic first-and
This work was supported in part by National Research
zeroth-ordermethodsfornonconvexstochasticprogram-
Foundation of Korea(NRF) grants funded by the Korea
ming. SIAMJournalonOptimization,23(4):2341–2368,
Government(MSIT) (No. 2021R1A2C2014504 and No.
2013.
2021M3F3A2A02037893),inpartbyInstituteofInforma-
tion & communications Technology Planning & Evalua- KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun.
tion(IITP)grantsfundedbytheKoreaGovernment(MSIT) Deep residual learning for image recognition. In Pro-
(No.2021-0-00106(20%),No.2021-0-02068(20%),and ceedingsoftheIEEEconferenceoncomputervisionand
No.2021-0-01059(20%)),andinpartbyINMACandBK21 patternrecognition,pages770–778,2016.
FOURprogram.
XisenJin,ArkaSadhu,JunyiDu,andXiangRen. Gradient-
basededitingofmemoryexamplesforonlinetask-free
continual learning. arXiv preprint arXiv:2006.15294,
References
2020.
Rahaf Aljundi, Lucas Caccia, Eugene Belilovsky, Mas- XisenJin,ArkaSadhu,JunyiDu,andXiangRen. Gradient-
simoCaccia,MinLin,LaurentCharlin,andTinneTuyte- basededitingofmemoryexamplesforonlinetask-free
laars. Online continual learning with maximally inter- continuallearning. AdvancesinNeuralInformationPro-
fered retrieval. CoRR, abs/1908.04742, 2019a. URL cessingSystems,34:29193–29205,2021.
http://arxiv.org/abs/1908.04742.
JamesKirkpatrick,RazvanPascanu,NeilRabinowitz,Joel
RahafAljundi,MinLin,BaptisteGoujaud,andYoshuaBen- Veness, Guillaume Desjardins, Andrei A Rusu, Kieran
gio. Gradientbasedsampleselectionforonlinecontinual Milan,JohnQuan,TiagoRamalho,AgnieszkaGrabska-
learning. InAdvancesinNeuralInformationProcessing Barwinska,etal. Overcomingcatastrophicforgettingin
Systems,pages11816–11825,2019b. neuralnetworks. Proceedingsofthenationalacademyof
sciences,114(13):3521–3526,2017.
ArslanChaudhry,Marc’AurelioRanzato,MarcusRohrbach,
JeremiasKnoblauch,HishamHusain,andTomDiethe.Opti-
andMohamedElhoseiny. Efficientlifelonglearningwith
malcontinuallearninghasperfectmemoryandisnp-hard.
A-GEM. In7thInternationalConferenceonLearning
arXivpreprintarXiv:2006.05188,2020.
Representations,ICLR2019,NewOrleans,LA,USA,May
6-9,2019.OpenReview.net,2019a. AlexKrizhevsky,GeoffreyHinton,etal. Learningmultiple
layersoffeaturesfromtinyimages. 2009.
Arslan Chaudhry, Marcus Rohrbach, Mohamed Elho-
Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick
seiny, Thalaiyasingam Ajanthan, Puneet K Dokania,
Haffner. Gradient-based learning applied to document
Philip HS Torr, and Marc’Aurelio Ranzato. On tiny
recognition.ProceedingsoftheIEEE,86(11):2278–2324,
episodicmemoriesincontinuallearning. arXivpreprint
1998.
arXiv:1902.10486,2019b.
Kibok Lee, Kimin Lee, Jinwoo Shin, and Honglak Lee.
ArslanChaudhry,AlbertGordo,PuneetKDokania,Philip
Overcomingcatastrophicforgettingwithunlabeleddata
Torr,andDavidLopez-Paz. Usinghindsighttoanchor
in the wild. In Proceedings of the IEEE International
past knowledge in continual learning. arXiv preprint
ConferenceonComputerVision,pages312–321,2019.
arXiv:2002.08165,3,2020a.
SoochanLee,JunsooHa,DongsuZhang,andGunheeKim.
ArslanChaudhry,NaeemullahKhan,PuneetDokania,and A neural dirichlet process mixture model for task-free
PhilipTorr. Continuallearninginlow-rankorthogonal continual learning. In 8th International Conference
subspaces. AdvancesinNeuralInformationProcessing onLearningRepresentations,ICLR2020,AddisAbaba,
Systems,33,2020b. Ethiopia,April26-30,2020.OpenReview.net,2020.LihuaLei,ChengJu,JianboChen,andMichaelIJordan. minimizinginterference. InInternationalConferenceon
Non-convex finite-sum optimization via scsg methods. LearningRepresentations,2018.
InAdvancesinNeuralInformationProcessingSystems,
Mark B. Ring. Continual learning in reinforcement
pages2348–2358,2017.
environments. PhD thesis, University of Texas at
ZhizhongLiandDerekHoiem. Learningwithoutforget- Austin,TX,USA,1995. URLhttp://d-nb.info/
ting. IEEEtransactionsonpatternanalysisandmachine 945690320.
intelligence,40(12):2935–2947,2017.
UmutSimsekli,LeventSagun,andMertGurbuzbalaban. A
David Lopez-Paz and Marc’Aurelio Ranzato. Gradient tail-index analysis of stochastic gradient noise in deep
episodic memory for continual learning. In Advances neuralnetworks.InInternationalConferenceonMachine
inNeuralInformationProcessingSystems,pages6467– Learning,pages5827–5837.PMLR,2019.
6476,2017.
UmutSimsekli,LingjiongZhu,YeeWhyeTeh,andMert
Seyed Iman Mirzadeh, Mehrdad Farajtabar, Razvan Pas- Gurbuzbalaban. Fractional underdamped langevin dy-
canu,andHassanGhasemzadeh. Understandingtherole namics:Retargetingsgdwithmomentumunderheavy-
oftrainingregimesincontinuallearning. InAdvances tailed gradient noise. In International Conference on
in Neural Information Processing Systems 33: Annual MachineLearning,pages8970–8980.PMLR,2020.
ConferenceonNeuralInformationProcessingSystems
Sebastian Thrun. A lifelong learning perspective
2020,2020.
for mobile robot control. In Intelligent Robots
Seyed Iman Mirzadeh, Mehrdad Farajtabar, Dilan Gorur, and Systems, Selections of the International Confer-
RazvanPascanu,andHassanGhasemzadeh.Linearmode ence on Intelligent Robots and Systems 1994, IROS
connectivityinmultitaskandcontinuallearning. InInter- 94, Munich, Germany, 12-16 September 1994, pages
nationalConferenceonLearningRepresentations,2021. 201–214, 1994. doi: 10.1016/b978-044482250-5/
50015-3. URL https://doi.org/10.1016/
Cuong V. Nguyen, Yingzhen Li, Thang D. Bui, and b978-044482250-5/50015-3.
RichardE.Turner. Variationalcontinuallearning. In6th
InternationalConferenceonLearningRepresentations, OriolVinyals,CharlesBlundell,TimothyLillicrap,koray
ICLR2018,Vancouver,BC,Canada,April30-May3, kavukcuoglu,andDaanWierstra. Matchingnetworksfor
2018,ConferenceTrackProceedings.OpenReview.net, oneshotlearning. InD.Lee,M.Sugiyama,U.Luxburg,
2018. I. Guyon, and R. Garnett, editors, Advances in Neural
InformationProcessingSystems,volume29.CurranAs-
Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg sociates,Inc.,2016. URLhttps://proceedings.
Sperl,andChristophHLampert. icarl:Incrementalclas- neurips.cc/paper/2016/file/
sifierandrepresentationlearning. InProceedingsofthe 90e1357833654983612fb05e3ec9148c-Paper.
IEEEconferenceonComputerVisionandPatternRecog- pdf.
nition,pages2001–2010,2017.
Mitchell Wortsman, Vivek Ramanujan, Rosanne Liu,
SashankJReddi,AhmedHefny,SuvritSra,BarnabásPóc- Aniruddha Kembhavi, Mohammad Rastegari, Jason
zos,andAlexSmola. Stochasticvariancereductionfor Yosinski,andAliFarhadi. Supermasksinsuperposition.
nonconvexoptimization. InInternationalconferenceon AdvancesinNeuralInformationProcessingSystems,33:
machinelearning,pages314–323,2016a. 15173–15184,2020.
SashankJ.Reddi,AhmedHefny,SuvritSra,BarnabásPóc- Jaehong Yoon, Eunho Yang, Jeongtae Lee, and Sung Ju
zos, and Alexander J. Smola. Stochastic variance re- Hwang. Lifelonglearningwithdynamicallyexpandable
ductionfornonconvexoptimization. InProceedingsof networks. In6thInternationalConferenceonLearning
the33ndInternationalConferenceonMachineLearning, Representations, ICLR 2018, Vancouver, BC, Canada,
ICML2016,NewYorkCity,NY,USA,June19-24,2016, April30-May3,2018,ConferenceTrackProceedings.
pages314–323,2016b.URLhttp://proceedings. OpenReview.net,2018.
mlr.press/v48/reddi16.html.
ManzilZaheer,SashankReddi,DevendraSachan,Satyen
SashankJReddi,SatyenKale,andSanjivKumar. Onthe Kale,andSanjivKumar. Adaptivemethodsfornoncon-
convergenceofadamandbeyond. InInternationalCon- vex optimization. In Advances in neural information
ferenceonLearningRepresentations,2018. processingsystems,pages9793–9803,2018.
Matthew Riemer, Ignacio Cases, Robert Ajemian, Miao FriedemannZenke,BenPoole,andSuryaGanguli. Contin-
Liu,IrinaRish,YuhaiTu,andGeraldTesauro. Learning uallearningthroughsynapticintelligence. Proceedings
tolearnwithoutforgettingbymaximizingtransferand ofmachinelearningresearch,70:3987,2017.JingzhaoZhang,SaiPraneethKarimireddy,AndreasVeit,
SeungyeonKim,SashankReddi,SanjivKumar,andSu-
vritSra. Whyareadaptivemethodsgoodforattention
models? Advances in Neural Information Processing
Systems,33,2020.A ADDITIONALBACKGROUNDSANDEXTENDEDDISCUSSION
A.1 SUMMARYOFNOTATIONS
Notations Definitions Notations Definitions
x modelparameter H theunionofI andJ
t t t
P previoustask n thenumberofdatapointsinP
f
C currenttask n thenumberofdatapointsinC
g
P datasetofP ⟨·,·⟩ innerproduct
C datasetofC L L-smoothnessconstant
h(x) meanlossofxonentiredatasets α adaptivestepsizeforf withH
Ht t
f(x) meanlossofxonP β adaptivestepsizeforgwithH
Ht t
g(x) meanlossofxonC M memoryattimet
t
f (x) lossofxonadatapointi∈P e errorofestimatef attimet
i t
g (x) lossofxonadatapointj ∈C e errorofestimatef withM
j Mt t
f (x) mini-batchlossofxonabatchI f meanlossofxwithM
It t Mt t
g (x) mini-batchlossofxonabatchJ M thehistoryofmemoryfromt1tot2
Jt t [t1:t2]
I minibatchsampledfromP B memorybiastermatt
t t
J minibatchsampledfromC Γ forgettingtermatt
t t
E totalexpectationfrom0totimet Λ innerproductbetween∇f and∇g
t Ht It Jt
A.2 REVIEWOFTERMINOLOGY
(Restrictionoff)Iff :A→BandifA isasubsetofA,thentherestrictionoff toA isthefunction
0 0
f| :A →B
A0 0
givenbyf| (x)=f(x)forx∈A .
A0 0
A.3 ADDITIONALRELATEDWORK
Regularizationbasedmethods.EWChasanadditionalpenalizationlossthatpreventtheupdateofparametersfromlosing
theinformationofprevioustasks.WhenweupdateamodelwithEWC,wehavetwogradientcomponentsfromthecurrent
taskandthepenalizationloss.
task-specificmodelcomponents.SupSuplearnsaseparatesubnetworkforeachtasktopredictagivendatabysuperimposing
allsupermasks.Itisanovelmethodtosolvecatastrophicforgettingwithtakingadvantageofneuralnetworks.
SGDmethodswithoutexpereincereplay.stableSGD[Mirzadehetal.,2020]andMC-SGD[Jinetal.,2021]showoverall
higher performancein terms of average accuracy than theproposed algorithm. For averageforgetting,our method has
the lowest value, which means that NCCL prevents catastrophic forgetting successfully with achieving the reasonable
performanceonthecurrenttask.Wethinkthatourmethodisfocusedonreducingcatastrophicforgettingaswedefined
inthereformulatedcontinuallearningproblem(12),soourmethodshowsthebetterperformanceonaverageforgetting.
Otherwise,MC-SGDfindsalow-losspathswithmode-connectivitybyupdatingwiththeproposedregularizationloss.This
procedureimpliesthatacontinuallearningmodelmightfindabetterlocalminimumpointforthenew(current)taskthan
NCCL.
Fornon-memorybasedmethods,thetheoreticalmeasuretoobserveforgettingandconvergenceduringtrainingdoesnot
exist.Ourtheoreticalresultsarethefirstattempttoanalyzetheconvergenceofprevioustasksduringcontinuallearning
procedure.Infuturework,wecanapproximatethevalueofwithfisherinformationforEWCandintroduceBayesiandeep
learningtoanalyzetheconvergenceofeachsubnetworksforeachtaskinthecaseofSupSup[Wortsmanetal.,2020].B ADDITIONALEXPERIMENTALRESULTSANDIMPLEMENTATIONDETAILS
WeimplementthebaselinesandtheproposedmethodonTensorflow1.Forevaluation,weuseanNVIDIA2080tiGPU
alongwith3.60GHzInteli9-9900KCPUand64GBRAM.
B.1 ARCHITECTUREANDTRAININGDETAIL
Forfaircomparison,wefollowthecommonlyusedmodelarchitectureandhyperparametersof[Leeetal.,2020,Chaudhry
etal.,2020b].ForPermuted-MNISTandSplit-MNIST,weusefully-connectedneuralnetworkswithtwohiddenlayersof
[400,400]or[256,256]andReLUactivation.ResNet-18withthenumberoffiltersn =64,20[Heetal.,2016]isapplied
f
forSplitCIFAR-10and100.Allexperimentsconductasingle-passoverthedatastream.Itisalsocalled1epochor0.2
epoch(inthecaseofsplittasks).Wedealbothcaseswithandwithoutthetaskidentifiersintheresultsofsplit-tasksto
comparefairlywithbaselines.Batchsizesofdatastreamandmemoryareboth10.Allreportedvaluesaretheaverage
valuesof5runswithdiffrentseeds,andwealsoprovidestandarddeviation.Othermiscellaneoussettingsarethesameasin
[Chaudhryetal.,2020b].
B.2 HYPERPARAMETERGRIDS
Wereportthehyper-paramtersgridweusedinourexperimentsbelow.Exceptfortheproposedalgorithm,weadoptedthe
hyper-paramtersthatarereportedintheoriginalpapers.Weusedgridsearchtofindtheoptimalparametersforeachmodel.
• finetune-learningrate[0.003,0.01,0.03(CIFAR),0.1(MNIST),0.3,1.0]
• EWC-learningrate:[0.003,0.01,0.03(CIFAR),0.1(MNIST),0.3,1.0]-regularization:[0.1,1,10(MNIST,CIFAR),
100,1000]
• A-GEM-learningrate:[0.003,0.01,0.03(CIFAR),0.1(MNIST),0.3,1.0]
• ER-Ring-learningrate:[0.003,0.01,0.03(CIFAR),0.1(MNIST),0.3,1.0]
• ORTHOG-SUBSPACE-learningrate:[0.003,0.01,0.03,0.1(MNIST),0.2,0.4(CIFAR),1.0]
• MER-learningrate:[0.003,0.01,0.03(MNIST,CIFAR),0.1,0.3,1.0]-withinbatchmeta-learningrate:[0.01,0.03,
0.1(MNIST,CIFAR),0.3,1.0]-currentbatchlearningratemultiplier:[1,2,5(CIFAR),10(MNIST)]
• iid-offlineandiid-online-learningrate[0.003,0.01,0.03(CIFAR),0.1(MNIST),0.3,1.0]
• ER-Reservoir-learningrate:[0.003,0.01,0.03,0.1(MNIST,CIFAR),0.3,1.0]
• NCCL-Ring(default)-learningrateα:[0.003,0.001(CIFAR),0.01,0.03,0.1,0.3,1.0]
• NCCL-Reservoir-learningrateα:[0.003(CIFAR),0.001,0.01,0.03,0.1,0.3,1.0]
B.3 HYPERPARAMETERSEARCHONβ ANDTRAININGTIME
max
Table3:Permuted-MNIST(23tasks10000examplespertask),FC-[256,256]andMulti-headedsplit-CIFAR100,fullsize
Resnet-18.AccuracieswithdifferentclippingrateonNCCL+Ring.
β Permuted-MNIST Split-CIFAR100
max
0.001 72.52(0.59) 49.43(0.65)
0.01 72.93(1.38) 56.95(1.02)
0.05 72.18(0.77) 56.35(1.42)
0.1 72.29(1.34) 58.20(0.155)
0.2 74.38(0.89) 57.60(0.36)
0.5 72.95(0.50) 59.06(1.02)
1 72.92(1.07) 57.43(1.33)
5 72.31(1.79) 57.75(0.24)Table4:Permuted-MNIST(23tasks10000examplespertask),FC-[256,256]andMulti-headedsplit-CIFAR100,fullsize
Resnet-18.Trainingtime.
Trainingtime[s]
Methods
Permuted-MNIST Split-CIFAR100
fine-tune 91 92
EWC 95 159
A-GEM 180 760
ER-Ring 109 129
ER-Reservoir 95 113
ORTHOG-SUBSPACE 90 581
NCCL+Ring 167 248
NCCL+Reservoir 168 242B.4 ADDITIONALEXPERIMENTRESULTS
Table5:Permuted-MNIST(23tasks60000examplespertask),FC-[256,256].
memorysize 1 5
Method
memory accuracy forgetting accuracy forgetting
multi-task ✗ 83 - 83 -
Fine-tune ✗ 53.5(1.46) 0.29(0.01) 47.9 0.29(0.01)
EWC ✗ 63.1(1.40) 0.18(0.01) 63.1(1.40) 0.18(0.01)
stableSGD ✗ 80.1(0.51) 0.09(0.01) 80.1(0.51) 0.09(0.01)
MC-SGD ✗ 85.3(0.61) 0.06(0.01) 85.3(0.61) 0.06(0.01)
MER ✓ 69.9(0.40) 0.14(0.01) 78.3(0.19) 0.06(0.01)
A-GEM ✓ 62.1(1.39) 0.21(0.01) 64.1(0.74) 0.19(0.01)
ER-Ring ✓ 70.2(0.56) 0.12(0.01) 75.8(0.24) 0.07(0.01)
ER-Reservoir ✓ 68.9(0.89) 0.15(0.01) 76.2(0.38) 0.07(0.01)
ORHOG-subspace ✓ 84.32(1.10) 0.12(0.01) 84.32(1.1) 0.11(0.01)
NCCL+Ring ✓ 74.22(0.75) 0.13(0.007) 84.41(0.32) 0.053(0.002)
NCCL+Reservoir ✓ 79.36(0.73) 0.12(0.007) 88.22(0.26) 0.028(0.003)
Table6:Multi-headedsplit-CIFAR100,reducedsizeResnet-18n =20.
f
memorysize 1 5
Method
memory accuracy forgetting accuracy forgetting
EWC ✗ 42.7(1.89) 0.28(0.03) 42.7(1.89) 0.28(0.03)
Fintune ✗ 40.4(2.83) 0.31(0.02) 40.4(2.83) 0.31(0.02)
StableSGD ✗ 59.9(1.81) 0.08(0.01) 59.9(1.81) 0.08(0.01)
MC-SGD ✗ 63.3(2.21) 0.06(0.03) 63.3(2.21) 0.06(0.03)
A-GEM ✓ 50.7(2.32) 0.19(0.04) 59.9(2.64) 0.10(0.02)
ER-Ring ✓ 56.2(1.93) 0.13(0.01) 62.6(1.77) 0.08(0.02)
ER-Reservoir ✓ 46.9(0.76) 0.21(0.03) 65.5(1.99) 0.09(0.02)
ORTHOG-subspace ✓ 58.81(1.88) 0.12(0.02) 64.38(0.95) 0.055(0.007)
NCCL+Ring ✓ 54.63(0.65) 0.059(0.01) 61.09(1.47) 0.02(0.01)
NCCL+Reservoir ✓ 52.18(0.48) 0.118(0.01) 63.68(0.18) 0.028(0.009)
Table7:Multi-headedsplit-MiniImagenet,fullsizeResnet-18n =64.Accuracyandforgettingresults.
f
memorysize 1
Method
memory accuracy forgetting
Fintune ✗ 36.1(1.31) 0.24(0.03)
EWC ✗ 34.8(2.34) 0.24(0.04)
A-GEM ✓ 42.3(1.42) 0.17(0.01)
MER ✓ 45.5(1.49) 0.15(0.01)
ER-Ring ✓ 49.8(2.92) 0.12(0.01)
ER-Reservoir ✓ 44.4(3.22) 0.17(0.02)
ORTHOG-subspace ✓ 51.4(1.44) 0.10(0.01)
NCCL+Ring ✓ 45.5(0.245) 0.041(0.01)
NCCL+Reservoir ✓ 41.0(1.02) 0.09(0.01)Table8:Multi-headedsplit-CIFAR100,fullsizeResnet-18n =64.Accuracyandforgettingresults.
f
memorysize 1 5
Method
memory accuracy forgetting accuracy forgetting
Fintune ✗ 42.6(2.72) 0.27(0.02) 42.6(2.72) 0.27(0.02)
EWC ✗ 43.2(2.77) 0.26(0.02) 43.2(2.77) 0.26(0.02)
ICRAL ✓ 46.4(1.21) 0.16(0.01) - -
A-GEM ✓ 51.3(3.49) 0.18(0.03) 60.9(2.5) 0.11(0.01)
MER ✓ 49.7(2.97) 0.19(0.03) - -
ER-Ring ✓ 59.6(1.19) 0.14(0.01) 67.2(1.72) 0.06(0.01)
ER-Reservoir ✓ 51.5(2.15) 0.14(0.09) 62.68(0.91) 0.06(0.01)
ORTHOG-subspace ✓ 64.3(0.59) 0.07(0.01) 67.3(0.98) 0.05(0.01)
NCCL+Ring ✓ 59.06(1.02) 0.03(0.02) 66.58(0.12) 0.004(0.003)
NCCL+Reservoir ✓ 54.7(0.91) 0.083(0.01) 66.37(0.19) 0.004(0.001)
Table9:permuted-MNIST(23tasks10000examplespertask),FC-[256,256].Accuracyandforgettingresults.
memorysize 1 5
Method
memory accuracy forgetting accuracy forgetting
multi-task ✗ 91.3 - 83 -
Fine-tune ✗ 50.6(2.57) 0.29(0.01) 47.9 0.29(0.01)
EWC ✗ 68.4(0.76) 0.18(0.01) 63.1(1.40) 0.18(0.01)
MER ✓ 78.6(0.84) 0.15(0.01) 88.34(0.26) 0.049(0.003)
A-GEM ✓ 78.3(0.42) 0.21(0.01) 64.1(0.74) 0.19(0.01)
ER-Ring ✓ 79.5(0.31) 0.12(0.01) 75.8(0.24) 0.07(0.01)
ER-Reservoir ✓ 68.9(0.89) 0.15(0.01) 76.2(0.38) 0.07(0.01)
ORHOG-subspace ✓ 86.6(0.91) 0.04(0.01) 87.04(0.43) 0.04(0.003)
NCCL+Ring ✓ 74.38(0.89) 0.05(0.009) 83.76(0.21) 0.014(0.001)
NCCL+Reservoir ✓ 76.48(0.29) 0.1(0.002) 86.02(0.06) 0.013(0.002)
Table10:Single-headedsplit-MNIST,FC-[256,256].Accuracyandforgettingresults.
memorysize 1 5 50
Method
memory accuracy forgetting accuracy forgetting accuracy forgetting
multi-task ✗ 95.2 - - - - -
Fine-tune ✗ 52.52(5.24) 0.41(0.06) - - - -
EWC ✗ 56.48(6.46) 0.31(0.05) - - - -
A-GEM ✓ 34.04(7.10) 0.23(0.11) 33.57(6.32) 0.18(0.03) 33.35(4.52) 0.12(0.04)
ER-Reservoir ✓ 34.63(6.03) 0.79(0.07) 63.60(3.11) 0.42(0.05) 86.17(0.99) 0.13(0.016)
NCCL+Ring ✓ 34.64(3.27) 0.55(0.03) 61.02(6.21) 0.207(0.07) 81.35(8.24) -0.03(0.1)
NCCL+Reservoir ✓ 37.02(0.34) 0.509(0.009) 65.4(0.7) 0.16(0.006) 88.9(0.28) -0.125(0.004)Table11:Single-headedsplit-MNIST,FC-[400,400]andmem.size=500(50/cls.).Accuracyandforgettingresults.
Method accuracy
multi-task 96.18
Fine-tune 50.9(5.53)
EWC 55.40(6.29)
A-GEM 26.49(5.62)
ER-Reservoir 85.1(1.02)
CN-DPM 93.23
Gdumb 91.9(0.5)
NCCL+Reservoir 95.15(0.91)
Table12:Single-headedsplit-CIFAR10,fullsizeResnet-18andmem.size=500(50/cls.).Accuracyandforgettingresults.
Method accuracy
iid-offline 93.17
iid-online 36.65
Fine-tune 12.68
EWC 53.49(0.72)
A-GEM 54.28(3.48)
GSS 33.56
ReservoirSampling 37.09
CN-DPM 41.78
NCCL+Ring 54.63(0.76)
NCCL+Reservoir 55.43(0.32)
Table13:Single-headedsplit-CIFAR100,Resnet18withn =20.Memorysize=10,000.Weconducttheexperimentwith
f
thesamesettingofGMED[Jinetal.,2021].
Methods accuracy
Finetune 3.06(0.2)
iidonline 18.13(0.8)
iidoffline 42.00(0.9)
A-GEM 2.40(0.2)
GSS-Greedy 19.53(1.3)
BGD 3.11(0.2)
ER-Reservoir 20.11(1.2)
ER-Reservoir+GMED 20.93(1.6)
MIR 20.02(1.7)
MIR+GMED 21.22(1.0)
NCCL-Reservoir 21.95(0.3)C THEORETICALANALYSIS
Inthissection,weprovidetheproofsoftheresultsfornonconvexcontinuallearning.Wefirststartwiththederivationof
Equation5inAssumption3.1.
C.1 ASSUMPTIONANDADDITIONALLEMMA
DerivationofEquation5. Recallthat
L
|f (x)−f (y)−⟨∇f (y),x−y⟩|≤ ∥x−y∥2. (12)
i i i 2
Notethatf isdifferentiableandnonconvex.Wedefineafunctiong(t)=f (y+t(x−y))fort∈[0,1]andanobjective
i i
functionf .Bythefundamentaltheoremofcalculus,
i
(cid:90) 1
g′(t)dt=f(x)−f(y). (13)
0
Bytheproperty,wehave
|f (x)−f (y)−⟨∇f (y),x−y⟩|
i i i
(cid:12)(cid:90) 1 (cid:12)
(cid:12) (cid:12)
=(cid:12) ⟨∇f i(y+t(x−y)),x−y⟩dt−⟨∇f i(y),x−y⟩(cid:12)
(cid:12) (cid:12)
0
(cid:12)(cid:90) 1 (cid:12)
(cid:12) (cid:12)
=(cid:12) ⟨∇f i(y+t(x−y))−∇f i(y),x−y⟩dt(cid:12).
(cid:12) (cid:12)
0
UsingtheCauchy-Schwartzinequality,
(cid:12)(cid:90) 1 (cid:12)
(cid:12) (cid:12)
(cid:12) ⟨∇f i(y+t(x−y))−∇f i(y),x−y⟩dt(cid:12)
(cid:12) (cid:12)
0
(cid:12)(cid:90) 1 (cid:12)
(cid:12) (cid:12)
≤(cid:12) ∥∇f i(y+t(x−y))−∇f i(y)∥·∥x−y∥dt(cid:12).
(cid:12) (cid:12)
0
Sincef satisfiesEquation4,thenwehave
i
|f (x)−f (y)−⟨∇f (y),x−y⟩|
i i i
(cid:12)(cid:90) 1 (cid:12)
(cid:12) (cid:12)
≤(cid:12) L∥y+t(x−y)−y∥·∥x−y∥dt(cid:12)
(cid:12) (cid:12)
0
(cid:12)(cid:90) 1 (cid:12)
=L∥x−y∥2(cid:12)
(cid:12)
tdt(cid:12)
(cid:12)
(cid:12) (cid:12)
0
L
= ∥x−y∥2.
2
LemmaC.1. Letp=[p ,···p ], q =[q ,··· ,q ]betwostatisticallyindependentrandomvectorswithdimensionD.
1 D 1 D
ThentheexpectationoftheinnerproductoftworandomvectorsE[⟨p,q⟩]is(cid:80)D E[p ]E[q ].
d=1 d d
Proof. Bythepropertyofexpectation,
D
(cid:88)
E[⟨p,q⟩]=E[ p q ]
d d
d=1
D
(cid:88)
= E[p q ]
d d
d=1
D
(cid:88)
= E[p ]E[q ].
d d
d=1C.2 PROOFOFMAINRESULTS
Wenowshowthemainresultsofourwork.
ProofofLemma4.1. ToclarifytheissueofE [E [e |M ]]=0,letusexplainthedetailsofconstructingreplay-memory
Mt It t t
asfollows.Wehaveconsideredepisodicmemoryandreservoirsamplinginthepaper.Wewillfirstshowthecaseofepisodic
memorybydescribingthesamplingmethodforreplaymemory.Wecanalsoderivethecaseofreservoirsamplingbysimply
applyingtheresultofepisodicmemory.
Episodicmemory(ringbuffer).WedividetheentiredatasetofcontinuallearningintotheprevioustaskP andthecurrent
task C on the time step t = 0. For the previous task P, the data stream of P is i.i.d., and its sequence is random on
everytrial(episode).Thetrial(episode)impliesthatacontinuallearningagentlearnsfromanonlinedatastreamwith
two consecutive data sequences of P and C. Episodic memory takes the last data points of the given memory size m
bytheFirstInFirstOut(FIFO)rule,andholdstheentiredatapointsuntillearningonC isfinished.Then,wenotethat
M = M forallt ≥ 0andM isuniformlysampledfromthei.i.d.sequenceofP.Bythelawoftotalexpectation,we
t 0 0
deriveE [E [∇f (xt)|M ]]foranyxt, ∀t≥0.
M0⊂P It It 0
E (cid:2)E (cid:2) ∇f (xt)|M (cid:3)(cid:3) =E (cid:2) ∇f (xt)(cid:3) .
M0⊂P It It 0 M0⊂P M0
ItisknownthatM wasuniformlysampledfromP oneachtrialbeforetrainingonthecurrenttaskC.Then,wetake
0
expectationwithrespecttoeverytrialthatimpliestheexpectedvalueoverthememorydistributionM .Wehave
0
E (cid:2) ∇f (xt)(cid:3) =∇f(xt)
M0⊂P M0
for any xt, ∀t. We can consider ∇f (xt) as a sample mean of P on every trial for any xt, ∀t ≥ 0. Although xt is
Mt
constructediteratively,theexpectedvalueofthesamplemeanforanyxt,E [∇f (xt)]isalsoderivedas∇f(xt).
M0⊂P M0
Reservoirsampling.Toclarifythenotationforreservoirsamplingfirst,wedenotetheexpectationwithrespecttothe
historyofreplaymemoryM = (M ,··· ,M )asE .ThisistherevisedversionofE .Reservoirsamplingis
[0:t] 0 t M[0:t] Mt
atrickiercasethanepisodicmemory,butE [E [e |M ]] = 0stillholds.SupposethatM isfullofthedatapoints
M[0:t] It t t 0
fromP astheepisodicmemoryissampledandthemini-batchsizefromC is1forsimplicity.Thereservoirsampling
algorithmdropsadatapointinM andreplacesthedroppeddatapointwithadatapointinthecurrentmini-batchfrom
t−1
C withprobabilityp = m/n,wheremisthememorysizeandnisthenumberofvisiteddatapointssofar.Theexact
pseudo-codeforreservoirsamplingisdescribedin[1].Thereplacementprocedureuniformlychoosesthedatapointwhich
willbedropped.Wecanalsoconsiderthereplacementprocedureasfollows.ThememoryM forP isreducedinsize1from
t
M ,andthereplaceddatapointd fromC contributesintermsof∇g (xt)ifd issampledfromthereplaymemory.
t−1 C dC C
LetM =[d ,··· ,d ]where|·|denotesthecardinalityofthememory.ThesamplemeanofM isgivenas
t−1 1 |Mt−1| t−1
1 (cid:88)
∇f (xt−1)= ∇f (xt−1). (14)
Mt−1 |M | di
t−1
di
Bytheruleofreservoirsampling,weassumethatthereplacementprocedurereducesthememoryfromM toM with
t−1 t
size|M |−1andthesetofremainedupcomingdatapointsC ∈C fromthecurrentdatastreamforonlinecontinual
t−1 t
learningisreformulatedintoC ∪[d ].Then,d canberesampledfromC ∪[d ]tobecomposedoftheminibatch
t−1 C C t−1 C
ofreservoirsamplingwiththedfferentprobability.However,weignoretheprobabilityissuenowtofocusontheeffectof
replay-memoryon∇f.Now,wesampleM fromM ,thenwegettherandomvector∇f (xt)as
t t−1 Mt
1
|M (cid:88)t−1|
∇f (xt)= W ∇f (xt), (15)
Mt |M | ij dj
t
j=1
wheretheindexiisuniformlysampledfromi∼[1,··· ,|M |],andW istheindicatorfunctionthatW is0ifi=j
t−1 ij ij
else1.
Theabovedescriptionimpliesthedroppingrule,andM canbeconsideredasanuniformlysampledsetwithsize|M |from
t t
M .TherecouldalsobeM =M withprobability1−p=1−m/n.Thentheexpectationof∇f (xt)givenM
t−1 t t−1 Mt t−1isderivedas
 
E Mt[∇f Mt(xt)|M t−1]=p |M1
||M (cid:88)t−1|
|M1
||M (cid:88)t−1|
W ij∇f dj(xt)+(1−p)(cid:0) ∇f Mt−1(xt)(cid:1)
t−1 t
i j=1
=∇f (xt).
Mt−1
Whenweconsiderthemini-batchsampling,wecanformallyreformulatetheaboveequationas
E (cid:2)E (cid:2) ∇f (xt)|M (cid:3) |M (cid:3) =∇f (xt). (16)
Mt∼p(Mt|Mt−1) It⊂Mt It t t−1 Mt−1
Now,weapplytheaboveequationrecursively.Then,
E (cid:2) ···E (cid:2)E (cid:2) ∇f (xt)|M (cid:3) |M (cid:3) ···|M (cid:3) =∇f (xt). (17)
M1∼p(M1|M0) Mt∼p(Mt|Mt−1) It⊂Mt It t t−1 0 M0
Similartoepisodicmemory,M isuniformlysampledfromP.Therefore,weconcludethat
0
E [∇f (xt)]=∇f(xt) (18)
M0,···,Mt Mt
bytakingexpectationoverthehistoryM =(M ,M ,··· ,M ).
[0:t] 1 2 t
NotethattakingexpectationiterativelywithrespecttothehistoryM isneededtocomputetheexpectedvalueofgradients
[t]
forM .However,theresultE [E [e |M ]]=0stillholdsintermsofexpectation.
t M0,···,Mt It t t
Furthermore,wealsodiscussthattheeffectofreservoirsamplingontheconvergenceofC.Unlikewesimplyupdateg(x)
bythestochasticgradientdescentonC,thedatapointsd ∈ M ∩C havealittlelargersamplingprobabilitythanother
datapointsd ∈C−M.TheexpectationofgradientnormontheaveragedlossE∥∇g(xt)∥2isbasedontheuniform
C−M
andequiprobablesamplingoverC,butthenatureofreservoirsamplingdistortthismeasureslightly.Inthispaper,wefocus
ontheconvergenceoftheprevioustaskC whiletrainingonthecurrenttaskC withseveralexistingmemory-basedmethods.
Therefore,analyzingtheconvergenceofreservoirsamplingmethodwillbeafuturework.
ProofofLemma4.2. Weanalyzetheconvergenceofnonconvexcontinuallearningwithreplaymemoryhere.Recallthat
thegradientupdateisthefollowing
xt+1 =xt−α ∇f (xt)−β ∇g (xt)
Ht It Ht Jt
forallt ∈ {1,2,··· ,T}.Lete = ∇f (xt)−∇f(xt).Sinceweassumethatf, g isL-smooth,wehavethefollowing
t It
inequalitybyapplyingEquation5:
L
f(xt+1)≤f(xt)+⟨∇f(xt),xt+1−xt⟩+ ∥xt+1−xt∥2
2
L
=f(xt)−⟨∇f(xt),α ∇f (xt)+β ∇g (xt)⟩+ ∥α ∇f (xt)+β ∇g (xt)∥2
Ht It Ht Jt 2 Ht It Ht Jt
=f(xt)−α ⟨∇f(xt),∇f (xt)⟩−β ⟨∇f(xt),∇g (xt)⟩
Ht It Ht Jt
L L
+ α2 ∥∇f (xt)∥2+ β2 ∥∇g (xt)∥2+Lα β ⟨∇f (xt),∇g (xt)⟩
2 Ht It 2 Ht Jt Ht Ht It Jt
=f(xt)−α ⟨∇f(xt),∇f(xt)⟩−α ⟨∇f(xt),e ⟩−β ⟨∇f (xt),∇g (xt)⟩+β ⟨∇g (xt),e ⟩
Ht Ht t Ht It Jt Ht Jt t
Lα2 Lα2 Lβ2
+ Ht∥∇f(xt)∥2+Lα2 ⟨∇f(xt),e ⟩+ Ht∥e ∥2+ Ht∥∇g (xt)∥2+Lα β ⟨∇f (xt),∇g (xt)⟩
2 Ht t 2 t 2 Jt Ht Ht It Jt
(cid:18) (cid:19)
L L
=f(xt)− α − α2 ∥∇f(xt)∥2+ β2 ∥∇g (xt)∥2−β (1−α L)⟨∇f (xt),∇g (xt)⟩
Ht 2 Ht 2 Ht Jt Ht Ht It Jt
+(cid:0) Lα2 −α (cid:1) ⟨∇f(xt),e ⟩+β ⟨∇g (xt),e ⟩+ L α2 ∥e ∥2. (19)
Ht Ht t Ht Jt t 2 Ht tToshowtheproposedtheoreticalconvergenceanalysisofnonconvexcontinuallearning,wedefinethecatastrophicforgetting
termΓ andtheoverfittingtermB asfollows:
t t
B =(Lα2 −α )⟨∇f(xt),e ⟩+β ⟨∇g (xt),e ⟩,
t Ht Ht t Ht Jt t
β2 L
Γ = Ht ∥∇g (xt)∥2−β (1−α L)⟨∇f (xt),∇g (xt)⟩.
t 2 Jt Ht Ht It Jt
Then,wecanrewriteEquation19as
(cid:18) (cid:19)
L L
f(xt+1)≤f(xt)− α − α2 ∥∇f(xt)∥2+Γ +B + α2 ∥e ∥2. (20)
Ht 2 Ht t t 2 Ht t
WefirstnotethatB isdependentoftheerrorterme withthebatchI .Inthecontinuallearningstep,antrainingagentcannot
t t t
access∇f(xt),thenwecannotgettheexactvalueofe .Furthermore,Γ isdependentofthegradients∇f (xt),∇g (xt)
t t It It
andthelearningratesα ,β .
Ht Ht
TakingexpectationswithrespecttoI onbothsidesgivenJ ,wehave
t t
E (cid:2) f(xt+1)(cid:3) ≤E (cid:20) f(xt)−(cid:18) α − L α2 (cid:19) ∥∇f(xt)∥2+Γ +B + L α2 ∥e ∥2(cid:12) (cid:12)J (cid:21)
It It Ht 2 Ht t t 2 Ht t (cid:12) t
(cid:20) (cid:18) L (cid:19) L (cid:21) (cid:104) (cid:12) (cid:105)
≤E f(xt)− α − α2 ∥∇f(xt)∥2+ α2 ∥e ∥2 +E Γ +B (cid:12)J .
It Ht 2 Ht 2 Ht t It t t(cid:12) t
Now,takingexpectationsoverthewholestochasticityweobtain
(cid:20) (cid:18) (cid:19) (cid:21)
E(cid:2) f(xt+1)(cid:3) ≤E f(xt)− α − L α2 ∥∇f(xt)∥2+Γ +B + L α2 ∥e ∥2 .
Ht 2 Ht t t 2 Ht t
Rearrangingthetermsandassumethat 1 >0,wehave
1−LαHt/2
(cid:18) (cid:19) (cid:20) (cid:21)
L L
α − α2 E∥∇f(xt)∥2 ≤E f(xt)−f(xt+1)+Γ +B + α2 ∥e ∥2
Ht 2 Ht t t 2 Ht t
and
(cid:34) (cid:35)
E∥∇f(xt)∥2 ≤E 1 (cid:0) f(xt)−f(xt+1)+Γ +B (cid:1) + α HtL ∥e ∥2
α (1− Lα ) t t 2(1− Lα ) t
Ht 2 Ht 2 Ht
(cid:34) (cid:35)
≤E 1 (cid:0) f(xt)−f(xt+1)+Γ +B (cid:1) + α HtL σ2 .
α (1− Lα ) t t 2(1− Lα ) f
Ht 2 Ht 2 Ht
√
ProofofTheorem4.3. Supposethatthelearningrateα isaconstantα=c/ T,forc>0,1− Lα= 1 >0.Then,by
Ht 2 A
summingEquation7fromt=0toT −1,wehaveT−1
1 (cid:88)
minE∥∇f(xt)∥2 ≤ E∥∇f(xt)∥2
t T
t=0
(cid:32) (cid:32) T−1 (cid:33) (cid:33)
1 1 (cid:88) L
≤ f(x0)−f(xT)+ (E[B +Γ ]) + ασ2
1− Lα αT t t 2 f
2 t=0
(cid:32) (cid:32) T−1 (cid:33) (cid:33)
1 1 (cid:88) Lc
= √ ∆ + (E[B +Γ ]) + √ σ2
1− Lα c T f t t 2 T f
2 t=0
(cid:32) (cid:32) T−1 (cid:33) (cid:33)
A 1 (cid:88) Lc
= √ ∆ + E[B +Γ ] + σ2 . (21)
T c f t t 2 f
t=0
We note that a batch I is sampled from a memory M ⊂ M which is a random vector whose element is a datapoint
t t
d∈P ∪C.Then,takingexpectationoverI ⊂M ⊂P ∪C impliesthatE[B ]=0.Therefore,wegettheminimumof
t t t
expectedsquareofthenormofgradients
(cid:32) (cid:32) T−1 (cid:33) (cid:33)
A 1 (cid:88) Lc
minE∥∇f(xt)∥2 ≤ √ ∆ + E[Γ ] + σ2 .
t T c f t 2 f
t=0
√
ProofofLemma4.4. Tosimplifytheproof,weassumethatlearningratesα ,β areasamefixedvalueβ =c′/ T.
Ht Ht
The assumption is reasonable, because it is observed that the RHS of Equation 7 is not perturbed drastically by small
(cid:83)
learningratesin0<α ,β ≤2/L≪1.LetusdenotetheunionofM overtime0≤t≤T −1asM = M .By
Ht Ht t t t
theassumption,itisequivalenttoupdateonM ∪C.Then,thenon-convexfinitesumoptimizationisgivenas
1 (cid:88)
min h| (x)= h (x), (22)
x∈Rd M∪C n g+|M| i
i∈M∪C
where|M|isthenumberofelementsinM.ThisproblemcanbesolvedbyasimpleSGDalgorithm[Reddietal.,2016b].
Thus,wehave
minE∥∇h| (xt)∥2 ≤
1 (cid:88)T
E∥∇h| (xt)∥2
≤(cid:114) 2∆ h|M∪CL
σ . (23)
t M∪C T M∪C T h|M∪C
t=0
LemmaC.2. ForanyC ⊂D ⊂M ∪C,defineω2 as
h|D
ω2 =supE ∥∇h (xt)−∇h| (xt)∥2].
h|D
x
j∈D j M∪C
Then,wehave
E∥∇g (xt)∥2 ≤E∥∇h| (xt)∥2+ sup ω2 . (24)
Jt M∪C h|D
C⊂D⊂M∪C
ProofofLemmaC.2. WearriveatthefollowingresultbyJensen’sinequality
supE ∥∇g (xt)−∇h| (xt)∥2 =supE (cid:2) ∥E [∇h (xt)]−∇h| (xt)∥2(cid:3) (25)
Jt⊂C Jt M∪C Jt⊂C j∈Jt j M∪C
x x
≤ sup supE (cid:2) ∥E [∇h (xt)]−∇h| (xt)∥2(cid:3) (26)
Jt⊂D j∈Jt j M∪C
C⊂D⊂M∪C x
(cid:20) (cid:21)
≤ sup supE [∥∇h (xt)−∇h| (xt)∥2] (27)
j∈D j M∪C
C⊂D⊂M∪C x
= sup ω2 . (28)
h|D
C⊂D⊂M∪CBythetriangularinequality,weget
E∥∇g (xt)∥2 ≤E∥∇g (xt)−∇h| (xt)∥2+E∥∇h| (xt)∥2 (29)
Jt Jt M∪C M∪C
≤E∥∇h| (xt)∥2+ sup ω2 . (30)
M∪C h|D
C⊂D⊂M∪C
Forcontinuallearning,themodelx0 reachestoanϵ-stationarypointoff(x)whenwehavefinishedtolearnP andstart
tolearnC.Now,wediscussthefrequencyoftransferandinterferenceduringcontinuallearningbeforeshowingLemma
4.5.Itiswellknownthatthefrequenciesbetweeninterferenceandtransferhavesimilarvalues(thefrequencyofconstraint
violationisapproximately0.5forAGEM)asshowninAppendixDof[Chaudhryetal.,2019a].Evenifmemory-based
continuallearninghasasmallmemorybufferwhichcontainsasubsetofP,randomsamplingfromthebufferallowstohave
similarfrequenciesbetweeninterferenceandtransfer.
Inthispaper,weconsidertwocasesfortheupperboundofE[Γ ],themoderatecaseandtheworstcase.Forthemoderate
t
case, which covers most continual learning scenarios, we assume that the inner product term ⟨∇f (xt),∇g (xt)⟩
It Jt
has the same probabilities of being positive (transfer) and negative (interference). Then, we can approximate
E[⟨∇f (xt),∇g (xt)⟩] ≈ 0 over all randomness. For the worst case, we assume that all ⟨∇f (xt),∇g (xt)⟩ has
It Jt It Jt
negativevalues.
ProofofLemma4.5. Forthemoderatecase,wederivetheroughupperboundofE[Γ ]:
t
(cid:20)β2 L (cid:21)
E[Γ ]=E Ht ∥∇g (xt)∥2−β (1−α L)⟨∇f (xt),∇g (xt)⟩ (31)
t 2 Jt Ht Ht It Jt
(cid:20)β2 L (cid:21)
≈E Ht ∥∇g (xt)∥2 (32)
2 Jt
(cid:18) (cid:20) β2L (cid:21)(cid:19)
=O E ∥∇g (xt)∥2 (33)
2 Jt
BypluggingLemmaC.2intoE[Γ ],weobtainthat
t
(cid:18) (cid:20) β2L (cid:21)(cid:19)
E[Γ ]≤O E ∥∇g (xt)∥2 (34)
t 2 Jt
(cid:18) (cid:20) β2L β2L (cid:21)(cid:19)
=O E ∥∇h| (xt)∥2+ sup ω2 . (35)
2 M∪C 2 h|D
C⊂D⊂M∪C
WeusethetechniqueforsummingupintheproofofTheorem1,thenthecumulativesumofcatastrophicforgettingtermis
derivedas
T (cid:88)−1
E[Γ
]≤T (cid:88)−1 β2L O(cid:18)
E(cid:2) ∥h| (xt)∥2(cid:3) + sup ω2
(cid:19)
(36)
t 2 M∪C h|D
C⊂D⊂M∪C
t=0 t=0
≤
β2LT (cid:88)−1 O(cid:18)
1 (cid:2) h| (xt)−h| (xt+1)(cid:3) + Lβ σ2 + sup ω2
(cid:19)
(37)
2 β M∪C M∪C 2 h|M∪C h|D
C⊂D⊂M∪C
t=0
β2L (cid:18) 1 TLβ (cid:19)
≤ O ∆ + σ2 +T sup ω2 (38)
2 β h|M∪C 2 h|M∪C h|D
C⊂D⊂M∪C
(cid:18) TLβ3 (cid:19)
=O β∆ + σ2 +Tβ2 sup ω2 . (39)
h|M∪C 2 h|M∪C h|D
C⊂D⊂M∪C
Now,weconsidertherandomnessofmemorychoice.LetD∗beasfollows:
TLβ3
D∗ = argmax β∆ + σ2 . (40)
h|D 2 h|D
C⊂D⊂P∪CThen,weobtainthefollowinginequality,
T (cid:88)−1 (cid:18) TLβ3 (cid:19)
E[Γ ]≤O β∆ + σ2 +Tβ2 sup ω2 (41)
t h|D∗ 2 h|D∗
C⊂D⊂M∪C
h|D
t=0
(cid:18) TLβ3 (cid:19)
≤O β∆ + σ2 +Tβ2 sup ω2 . (42)
h|D∗ 2 h|D∗
C⊂D⊂P∪C
h|D
Rearrangingtheaboveequation,weget
T (cid:88)−1 (cid:18) (cid:18) Lβ3 (cid:19) (cid:19)
E[Γ ]≤O T σ2 +β2 sup ω2 +β∆ . (43)
t 2 h|D∗
C⊂D⊂P∪C
h|D h|D∗
t=0
Forthemoderatecase,weprovidethederivationsoftheconvergenceratefortwocasesofβ asfollows.
√
Whenβ <α=c/ T,theupperboundalwayssatisfies
T (cid:88)−1E
√[Γ t]
≤
√1
O(cid:18)
1
(cid:18)
Lβ
σ2 +
√1
sup ω2
(cid:19)
+
√1
∆
(cid:19) <O(cid:18)
1
+
1(cid:19)
.
T T T 2 h|D∗ TC⊂D⊂P∪C h|D T h|D∗ T3/2 T
t=0
√
Forβ ≥α=c/ T,wecannotderiveatighterbound,sowestillhave
T (cid:88)−1E √[Γ t]
≤
√1 O(cid:18)
T
(cid:18) Lβ3
σ2 +β2 sup ω2
(cid:19)
+β∆
(cid:19) =O(cid:18)√
T +
√1 (cid:19)
.
T T 2 h|D∗ C⊂D⊂P∪C h|D h|D∗ T
t=0
Fortheworstcase,weassumethatthereexistsaconstantc whichsatisfiesc ∥∇g (xt)∥≥∥∇f (xt)∥.
f,g f,g Jt It
(cid:20)β2 L (cid:21)
E[Γ ]=E Ht ∥∇g (xt)∥2−β (1−α L)⟨∇f (xt),∇g (xt)⟩ (44)
t 2 Jt Ht Ht It Jt
(cid:20)β2 L (cid:21)
≤E Ht ∥∇g (xt)∥2+β (1−α L)∥∇f (xt)∥∥∇g (xt)∥ (45)
2 Jt Ht Ht It Jt
(cid:20) β2L (cid:21)
≤E ∥∇g (xt)∥2+βc ∥∇g (xt)∥2 (46)
2 Jt f,g Jt
=O(cid:0)E(cid:2)(cid:0) β2+β(cid:1)
∥∇g
(xt)∥2(cid:3)(cid:1)
. (47)
Jt
BypluggingLemmaC.2intoE[Γ ],weobtainthat
t
E[Γ ]≤O(cid:0)E(cid:2)(cid:0) β2+β(cid:1) ∥∇g (xt)∥2(cid:3)(cid:1) (48)
t Jt
(cid:18) (cid:20) (cid:21)(cid:19)
=O (cid:0) β2+β(cid:1)E ∥∇h| (xt)∥2+ sup ω2 . (49)
M∪C h|D
C⊂D⊂M∪C
WeusethetechniqueforsummingupintheproofofTheorem1,thenthecumulativesumofcatastrophicforgettingtermis
derivedas
T−1 T−1 (cid:18) (cid:19)
(cid:88) E[Γ ]≤ (cid:88)(cid:0) β2+β(cid:1) O E(cid:2) ∥h| (xt)∥2(cid:3) + sup ω2 (50)
t M∪C h|D
C⊂D⊂M∪C
t=0 t=0
T−1 (cid:18) (cid:19)
≤(cid:0) β2+β(cid:1)(cid:88) O 1 (cid:2) h| (xt)−h| (xt+1)(cid:3) + Lβ σ2 + sup ω2 (51)
β M∪C M∪C 2 h|M∪C h|D
C⊂D⊂M∪C
t=0
(cid:18) (cid:19)
≤(cid:0) β2+β(cid:1) O 1 ∆ + TLβ σ2 +T sup ω2 (52)
β h|M∪C 2 h|M∪C h|D
C⊂D⊂M∪C
(cid:18) TLβ2(β+1) (cid:19)
=O (β+1)∆ + σ2 +Tβ(β+1) sup ω2 . (53)
h|M∪C 2 h|M∪C h|D
C⊂D⊂M∪CFortheworstcase,weprovidethederivationsoftheconvergenceratefortwocasesofβ asfollows.
√
Whenβ <α=c/ T,theupperboundalwayssatisfies
T (cid:88)−1E √[Γ t]
≤
√1 O(cid:32) Lc √+√ T
σ2
+(√
T +c) sup ω2 +
√ √T +c
∆
(cid:33) <O(cid:18) 1
+
√1 +1(cid:19)
.
T T T h|D∗ C⊂D⊂P∪C h|D T h|D∗ T T
t=0
√
Forβ ≥α=c/ T,wecannotderiveatighterbound,sowestillhave
T (cid:88)−1E √[Γ t]
≤
√1 O(cid:18)
T
(cid:18) Lβ2(β+1)
σ2 +β(β+1) sup ω2
(cid:19)
+(β+1)∆
(cid:19) =O(cid:18)√
T +
√1 (cid:19)
.
T T 2 h|D∗ C⊂D⊂P∪C h|D h|D∗ T
t=0
Evenifweconsidertheworstcase,westillhaveO(1)forthecumulativeforgettingE[Γ ]whenβ <α.Thisimpliesthat
t
wehavethetheoreticalconditionforcontroltheforgettingonf(x)whileevolvingonC.Inthemaintext,weonlydiscuss
themoderatecasetoemphasizef(x)canbeconvergedbytheeffectoftransferduringcontinuallearning,butwehavealso
consideredtheworstcasecanbewelltreatedbyourtheoreticalconditionbykeepingtheconvergenceoff(x)overtimeas
follows.
ProofofCorollary4.6. ByLemma4.5,wehave
T (cid:88)−1E
√[Γ t]
<O(cid:18)
1
+
1(cid:19)
T T3/2 T
t=0
forβ <αforthemoderatecase.Then,wecanapplytheresultintoRHSoftheinequalityinTheorem4.3asfollows.
(cid:32) (cid:32) T−1 (cid:33) (cid:33)
A 1 (cid:88) Lc
minE∥∇f(xt)∥2 ≤ √ ∆ + E[Γ ] + σ2
t T c f t 2 f
t=0
A/c(cid:18) Lc2 (cid:19) A/cT (cid:88)−1
= √ ∆ + σ2 + √ E[Γ ]
T f 2 f T t
t=0
(cid:18) (cid:19) (cid:18) (cid:19)
1 1 1 1
=O + + =O √ .
T3/2 T T1/2 T
Inaddition,wehavetheconvergencerateoff(x)fortheworstcaseasfollows:
minE∥∇f(xt)∥2 =O(1), (54)
t
whichimpliesthatf(x)cankeeptheconvergencewhileevolvingonC.
ProofofCorollary4.7. ToformulatetheIFOcalls,RecallthatT(ϵ)
T(ϵ)=min{T : min E∥∇f(xt)∥2 ≤ϵ}.
AsingleIFOcallisinvestedincalculatingeachstep,andwenowcomputeIFOcallstoreachanϵ-accuratesolution.
(cid:32) (cid:32) T−1 (cid:33) (cid:33)
A 1 (cid:88) Lc
√ ∆ + E[Γ ] + σ2 →ϵ.
T c f t 2 f
t=0
Whenβ <α,weget
(cid:18) (cid:19)
1
IFOcalls=O .
ϵ2
Otherwise,whenβ ≥α,wecannotguaranteetheupperboundofstationarydecreasesovertime.Then,wecannotcompute
IFOcallsforthiscase.D DERIVATIONOFEQUATIONSINADAPTIVEMETHODSINCONTINUALLEARNING
DerivationforA-GEM Letthesurrogate∇g˜ (xt)as
Jt
(cid:28) ∇f (xt) (cid:29) ∇f (xt)
∇g˜ (xt)=∇g (xt)− It ,∇g (xt) It , (55)
Jt Jt ∥∇f (xt)∥ Jt ∥∇f (xt)∥
It It
whereα Ht =α(1− ⟨∇fI ∥t ∇(x ft I) t, (∇ xtg )J ∥t 2(xt)⟩ )andβ Ht =αforEquation3.
Then,wehave
(cid:20)β2 L (cid:21)
E[Γ ]=E Ht ∥∇g˜ (xt)∥2−β ⟨∇f (xt),∇g˜ (xt)⟩
t 2 Jt Ht It Jt
(cid:20)β2 L(cid:18) ⟨∇f (xt),∇g (xt)⟩2 ⟨∇f (xt),∇g (xt)⟩2(cid:19) (cid:21)
=E Ht ∥∇g (xt)∥2−2 It Jt + It Jt −β ⟨∇f (xt),∇g˜ (xt)⟩
2 Jt ∥∇f (xt)∥2 ∥∇f (xt)∥2 Ht It Jt
It It
=E(cid:20)β H2 tL(cid:18) ∥∇g (xt)∥2− ⟨∇f It(xt),∇g Jt(xt)⟩2(cid:19) −β (cid:0) ⟨∇f (xt),∇g (xt)⟩−⟨∇f (xt),∇g (xt)⟩(cid:1)(cid:21)
2 Jt ∥∇f (xt)∥2 Ht It Jt It Jt
It
(cid:20)β2 L(cid:18) ⟨∇f (xt),∇g (xt)⟩2(cid:19)(cid:21)
=E Ht ∥∇g (xt)∥2− It Jt . (56)
2 Jt ∥∇f (xt)∥2
It
Now,wecomparethecatastrophicforgettingtermbetweentheoriginalvaluewith∇g (xt)andtheabovesurrogate.
Jt
(cid:20)β2 L(cid:18) ⟨∇f (xt),∇g (xt)⟩2(cid:19)(cid:21) (cid:20)β2 L (cid:21)
E Ht ∥∇g (xt)∥2− It Jt <E Ht ∥∇g (xt)∥2−β ⟨∇f (xt),∇g (xt)⟩ .
2 Jt ∥∇f (xt)∥2 2 Jt Ht It Jt
It
Then,wecanconcludethatE[Γ ]withthesurrogateofA-GEMissmallerthantheoriginalE[Γ ].
t t
DerivationofoptimalΓ∗andβ∗ Forafixedlearningrateα,wehave
t Ht
∂E[Γ ] (cid:20) ∂Γ (cid:21)
0= t =E t
∂β ∂β
Ht Ht
=E(cid:2) β L∥∇g (xt)∥−(1−αL)⟨∇f (xt),∇g (xt)⟩(cid:3) .
Ht Jt It Jt
Thus,weobtain
(1−α L)⟨∇f (xt),∇g (xt)⟩ (1−α L)Λ
β∗ = Ht It Jt = Ht Ht,
Ht L∥∇g (xt)∥2 L∥∇g (xt)∥2
Jt Jt
(1−α L)⟨∇f (xt),∇g (xt)⟩ (1−α L)Λ
Γ∗ =− Ht It Jt =− Ht Ht.
t 2L∥∇g (xt)∥2 2L∥∇g (xt)∥2
Jt Jt
E OVERFITTINGTOREPLAYMEMORY
InLemma4.2,weshowtheexpectationofstepwisechangeofupperbound.Now,wediscussthedistributionoftheupper
boundbyanalyzingtherandomvariableB .AsB iscomputedbygetting
t t
B =(Lα2 −α )⟨∇f(xt),e ⟩+β ⟨∇g (xt),e ⟩.
t Ht Ht t Ht Jt t
ThepurposeofourconvergenceanalysisistocomputetheupperboundofEquation7,thenwecomputetheupperboundof
B .
t
B ≤(Lα2 −α )∥∇f(xt)∥∥e ∥+β ∥∇g (xt)∥∥e ∥.
t Ht Ht t Ht Jt t
Itisnotedthattheupperboundisrelatedtothedistributionofthenormofe .WehavealreadyknowthatE[e ]=0,sowe
t t
consideritsvariance,Var(∥e ∥)inthissection.LetusdenotethenumberofdatapointsofP inamemoryM asm .We
t 0 P
assumethatM isuniformlysampledfromP.Thenthesamplevariance,Var(∥e ∥)iscomputedas
0 t
n −m
Var(∥e ∥)= f P σ2
t (n −1)m f
f P
bythesimilarderivationwithEquation25.TheaboveresultdirectlycanbeappliedtothevarianceofB .Thisimplies
t
m isakeyfeaturewhichhasaneffectontheconvergencerate.Itisnotedthatthelargerm hasthesmallervariance
t P
byapplyingschemes,suchaslargermemory.Inaddition,thedistributionsofe and∇f (xt)aredifferentwithvarious
t It
memoryschemes.Therefore,wecanobservethatmemoryschemesdiffertheperformanceevenifweapplysamestepsizes.