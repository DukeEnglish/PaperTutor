On the price of exact truthfulness in incentive-compatible
online learning with bandit feedback:
A regret lower bound for WSU-UX
AliMortazavi JunhaoLin*
UniversityofVictoria UniversityofWaterloo
alithemorty@gmail.com linjunhao9385@gmail.com
NishantA.Mehta
UniversityofVictoria
nmehta@uvic.ca
Abstract
Inoneviewoftheclassicalgameofpredictionwithexpertadvicewithbinaryoutcomes,ineachround,
eachexpertmaintainsanadversariallychosenbeliefandhonestlyreportsthisbelief.Weconsiderarecently
introduced, strategic variant of this problem with selfish (reputation-seeking) experts, where each expert
strategicallyreportsinordertomaximizetheirexpectedfuturereputationbasedontheirbelief.Inthiswork,
ourgoalistodesignanalgorithmfortheselfishexpertsproblemthatisincentive-compatible(IC,ortruthful),
meaningeachexpert’sbeststrategyistoreporttruthfully,whilealsoensuringthealgorithmenjoyssublinear
regretwithrespecttotheexpertwiththebestbelief. Freemanetal.(2020)recentlystudiedthisproblemin
thefullinformationandbanditsettingsandobtainedtruthful,no-regretalgorithmsbyleveragingpriorwork
onwageringmechanisms.Whiletheirresultsunderfullinformationmatchtheminimaxratefortheclassical
("honestexperts")problem,thebest-knownregretfortheirbanditalgorithmWSU-UXisO(T2/3),which
doesnotmatchtheminimaxratefortheclassical("honestbandits")setting.Itwasunclearwhetherthehigher
regretwasanartifactoftheiranalysisoralimitationofWSU-UX.Weshow,viaexplicitconstructionofloss
sequences, thatthealgorithmsuffersaworst-caseΩ(T2/3)lowerbound. Leftopenisthepossibilitythat
√
adifferentICalgorithmobtainsO( T)regret. Yet,WSU-UXwasanaturalchoiceforsuchanalgorithm
owingtothelimiteddesignroomforICalgorithmsinthissetting.
1 Introduction
Intheproblemofpredictionwithexpertadvice(Vovk,1995),wehaveK expertsandT daysandafixedloss
functionℓ(x ,y ). Eachdayt∈[T],thelearnerhasaccesstotheadviceb ofeachexperti∈[K]aboutthe
t t i,t
outcomey . Usingtheexpertadvice,thelearnerpredictsx andthenoutcomey isrevealed. Theerrorofthe
t t t
learnerismeasuredbyℓ(x ,y )atroundt. Thegoalofthelearneristoachievelowregretwithrespecttothe
t t
cumulativelossofthebestexpertinhindsight. Onecommonapproachtothisproblemintheliteratureisto,
foranyroundt,maintainasetofweightsoverexpertsw foralli ∈ [K],selecttheadviceofexpertiwith
t,i
probability wt,i ,andupdatetheweightsappropriatelyoncetheoutcomey isrevealed. Seeforexamplethe
∥wt∥1 t
multiplicativeweightupdate(MWU)method(Aroraetal.,2012)andHedge(FreundandSchapire,1997).
RoughgardenandSchrijvers(2017)consideredthisproblemforbinaryoutcomes(i.e.y ∈ {0,1})where
t
theexpertsarestrategic: ineachroundt,eachexpertformsabeliefb ∈[0,1]aboutthebinaryoutcomey ,
i,t t
(i.e.b =Pr(y =1)∈[0,1])andreportsr ∈[0,1]insuchawayastomaximizeitsownfuturereputation
i,t t i,t
amongthepoolofexperts. SeetheprotocolinAlgorithm1whichshowsthisframework.
Moreover,consideringtheclassoflearningalgorithmsthatmaintainweightsoverexpertsw fori∈[K],
t,i
RoughgardenandSchrijvers(2017)assumedthateachexpertiatroundtassociatesitsowncurrentreputation
*WorkcompletedwhileatUniversityofVictoria
1
4202
rpA
8
]GL.sc[
1v55150.4042:viXraAlgorithm1:ProtocolforPredictionWithSelfish(ReputationSeeking)Expertsforbinaryoutcomes
Input: T,K,ℓ(X,Y):[0,1]×{0,1}→R
fort=1,...,T do
Thelearnerchoosesadistributionπ ∈∆ overexpertsanddrawsanexpertI .
t K t
Eachexperti∈[K]formsabeliefb ∈[0,1]aboutthedistributionofoutcomey .
i,t t
Eachexpertireportsapredictionr ∈[0,1]withthegoalof
i,t
maximizingtheirownfuturereputation.
Naturerevealstheoutcomey ∈{0,1}.
t
LearnerincurslossofE[ℓ(r ,y )]=(cid:80) π ℓ(r ,y )
It,t t j∈[K] t,i j,t t
withtheweightw anditsfuturereputationsimplyasitsweightinthenextroundw . Thistypeofexpert
t,i t+1,i
iscalledamyopicexpertasitdoesnotconsidertheimpactofthedecisiononitslong-termreputation.
Given this notion of future reputation, the design of the algorithm would impact how each expert would
report. Considerroundtandexpertiandafixedweight-basedlearningalgorithm;dependingonthelearning
algorithm,thereisafunctionf thatdeterminestheweightw as
t+1,i
w =f(r ,y ,r ,h ), (1)
t+1,i i,t t −i,t t−1
wherer denotesthereportsoftheexpertsotherthanexpertiandh isalltheinformationrevealedby
−i,t t−1
theendofroundt−1.
Moreover, assume that expert i has perfect information about r and h . Assuming expert i has a
−i,t t−1
beliefb aboutthedistributionofy ,thenitreportsr tomaximizeitsexpectedreputation:
i,t t i,t
r =argmaxE [w ]
i,t bi,t t+1,i
r∈[0,1]
=argmaxE [f(r,y ,r ,h )],
bi,t t −i,t t−1
r∈[0,1]
wheretheexpectationisovertherandomnessoftheoutcomey asifitweredrawnbasedontheexpert’sbelief
t
b .Observethatdependingonfunctionf,thevaluetheexpertireportsr canbedifferentthanitsbeliefb .
i,t i,t i,t
Iftruth-tellingisalwaysadominatingstrategy,meaningthatnomattertheotherexperts’reportsr ,thebest
−i,t
responseisr = b ,thenthealgorithmiscalledincentive-compatible. Thedesignofincentive-compatible
i,t i,t
algorithmsisdesirablefortworeasons:
• Quality of prediction: The regret guarantee for an incentive-compatible online learning algorithm holds
notonlyfortheexpertwiththebestreportsbutalsoholdsfortheexpertwiththebestbeliefs, whichthe
algorithmdoesnothavedirectaccessto. Thisguaranteeiscalledbeliefregret.1
• Naturalstrategy:Anexpertdoesnotneedtotakeintoconsiderationthereportsofotherexperts.Moreover,
when a simple strategy (truth-telling) is strictly dominating, it is reasonable to expect that agents will
choosethatstrategy.
As observed by Roughgarden and Schrijvers (2017), the design of incentive-compatible online learning
algorithmsisintimatelyconnectedtotheproblemofdesigningproperscoringrules(seeDefinition3inSec-
tion2.3). Thisimpliesthatwhenthelossfunctionisproper,theproblemiseasy. Forinstance,foranyproper
lossfunction,suchassquaredloss,MWUwhichusestheupdaterule
w =w (1−ηℓ(r ,y )),
t+1,i t,i i,t t
isincentive-compatible.
However,forabsoluteloss,whichisnotaproperlossfunction,RoughgardenandSchrijvers(2017,Corol-
lary31)showedthat, undersomemildrestrictions2, noweight-basedrandomizedalgorithmcanachieveno-
regret.
1Frongilloetal.(2021)usedtheterm“regretwithrespecttothetruebeliefs”.
2TheclassofalgorithmsthatareconsideredbyRoughgardenandSchrijvers(2017)arethosethatareanaturalextensionofdetermin-
isticweightedmajorityalgorithms,wheretheweightupdatehassomemildrestrictions.
2Yet,asobservedbyFreemanetal.(2020),evenforproperlossfunctionssuchassquaredloss,foranother
natural variation of incentive for experts who, in any round t, want to maximize their expected normalized
weight(i.e.theprobabilityofbeingselected)inthenextroundbasedontheirprivatebeliefabouttheoutcome
y ,theclassicalmultiplicativeweightalgorithm(MWU)failstobetruthful3. Freemanetal.(2020)designed
t √
the Weighted-Score Update rule which is truthful and also achieves O( T) regret in the full-information
setting. However, in their extension to the multi-armed bandit setting (which they refer to as the partial
informationsetting),theiralgorithmWSU-UXachievesO(T2/3)regret,whichdoesnotmatchtheminimax
√
optimalrateO( T)intheclassical“honestexperts”problem.
Although the experimental results by Freeman et al. (2020) suggested that WSU-UX performs similarly
√
to EXP3 (Auer et al., 2002) which has minimax optimal regret of O( T) in the classical “honest experts”
problem, it remained an open problem whether the O(T2/3) regret of WSU-UX is due to an artifact in the
analysisorifinsteadthealgorithmcannotachievelowerregret.
Main question The main question that we are interested in understanding is: in the setting of Freeman
et al. (2020), whether learning with reputation-seeking experts under bandit feedback is strictly harder than
theclassicalbanditproblem.
Contribution WetakeonesteptowardansweringthisquestionbyshowingthatWSU-UX,whichisavery
naturalchoiceforthisproblem,cannotachieveregretbetterthanΩ(T2/3)intheworstcase. Inparticular,we
showthatforanychoiceofhyperparametersforWSU-UX,forlargeenoughT, thereexistsalosssequence
where the belief regret is Ω(T2/3). The construction of the loss sequence is fairly simple but, for the set of
non-trivialhyperparameters(seeSection3.2foradescriptionofthisset)requiresahighlyintricateanalysis.In
particular,forthenon-trivialcase,wedesignalosssequencesuchthat(i)thebestexpert4hasanestimatedloss
withlargevarianceforaconstantfractionofT rounds,andatthesametime(ii)thebestexpertoutperformsthe
otherexpertsattheend. Thecoretechnicaldifficultyisshowingthatboth(i)and(ii)happensimultaneously.
2 Model and preliminaries
2.1 Problemsetting
We first describe the learning protocol. In the setting of Freeman et al. (2020), the protocol is the same as
Protocol1,wherethelossfunctionissquaredloss(whichisaproperloss). Inthefull-informationversionof
the problem, all experts offer reports, while under bandit feedback, only the selected expert offers a report.
The future reputation of each expert is defined as the probability of being selected in the next round. More
concretely, each expert i at round t with belief b ∈ [0,1] about binary outcome y strategically reports
i,t t
r ∈[0,1]tomaximizetheirprobabilityofbeingselectedbythealgorithminthenextroundt+1. Thegoal
i,t
ofthelearneristominimizeitsbeliefregret,theregretwithrespecttotheexpertwiththebestbelief.
Definition1. Letπ ∈[K]bethelearner’sprobabilitydistributioninroundt. Thenthelearner’sbeliefregret
t
E[R ]afterT roundsisdefinedas
T
 
(cid:88) (cid:88) (cid:88)
E  π t,jℓ(r j,t,y t)− min ℓ(b i,t,y t).
i∈[K]
t∈[T]j∈[K] t∈[T]
Note that the learner incurs loss according to reports r whereas the performance of the best expert is
i,t
measuredwithrespecttob .Ingeneral,r neednotequalb .However,ifalearningalgorithmisincentive-
i,t i,t i,t
compatible, meaning that truth-telling is the only strictly dominant strategy, it is reasonable to assume that
r = b . In this case, low classical regret implies low belief regret. Next, we restate the definition of
i,t i,t
incentive-compatibilityfromFreemanetal.(2020).
3ItiseasytoshowthatHedgeandMWUarenottruthful. However,notethatFrongilloetal.(2021)showedthatHedgeisapproxi-
matelytruthful.Undersomeassumptions,approximatelytruthfulnessisenoughtogetgoodbeliefregret,butinthispaper,weonlyfocus
onexactly-truthful/incentive-compatiblealgorithms:thealgorithmswheretruth-tellingistheonlydominantstrategy.
4ThebestexperthereistheexpertwhosebeliefhasthelowestcumulativelossoverT rounds.
3Definition2(Freemanetal.(2020)). Anonlinelearningalgorithmisincentive-compatibleifforeverytimestep
t∈[T],everyexpertiwithbeliefb ,everyreportr ,everyvectorofreportsoftheotherexpertsr ,and
i,t i,t −i,t
everyhistoryofreports(r ) andoutcomes(y ) ,
t′ t′<t t′ t′<t
E [π |(b ,r ),y ,(y ) ,(r ) ]
yt∼Bern(bi,t) t+1,i i,t −i,t t t′ t′<t t′ t′<t
≥E [π |(r ,r ),y ,(y ) ,(r ) ],
yt∼Bern(bi,t) t+1,i i,t −i,t t t′ t′<t t′ t′<t
wherey ∼Bern(b)denotesarandomvariabletakingvalue1withprobabilityband0otherwise.
2.2 Motivationforbanditsetting
Tomotivatethebanditversionoftheproblem,considerthefollowingexample.Aforecastingagencywishesto
forecastaneventandhasachoiceofwhichforecastertoemploy. Theselectedforecasterwillbegivenafixed
payment(say$1000)fromtheagencytoresearchtheevent,willthendeveloptheirbeliefaboutthelikelihood
oftheeventoccurring,andwillfinallydecidewhatprobabilityforecast(report)togivetheforecastingagency.
Anyforecasterthatisnotselectedwillnotreceiveapaymentandwillneverprovideareporttotheforecasting
agency. Naturally,theagencydesiresaccuratereports,andsoitsgoalistoselecttheforecasterwhosebelief
(whichcanbedevelopedonlyaftertheforecasterisfundedandhencewasselected)isthemostaccurate. To
thisend,theagencyshouldensurethatthefutureexpectedpaymentgiventoaselectedforecaster(forthenext
event)incentivizestheforecastertoreportitsbeliefhonestlyforthecurrentevent.Theforecaster’sincentiveis
exactlyequaltothefutureprobabilityofbeingselected(aquantificationoftheforecaster’sreputation)since,
if selected in the future, the selected forecaster will again receive a fixed payment. The agency should thus
ensure that a forecaster’s future probability of being selected is directly proportional to the accuracy of the
forecaster’sreport,anaccuracywhichisknownoncetheoutcomehasbeenrealized.
2.3 Preliminaries
Inthissubsection, weoverviewfundamentalconceptsrelevanttoincentive-compatibility. Wefirstrecallthe
notionofproperscoringrules,whichcanbeusedtoelicitinformationfromanexpert(GneitingandRaftery,
2007),(Bujaetal.,2005).
Definition 3. Let Y denote the outcome space and R ⊆ ∆(Y) denote the distributional report space. A
scoringrules:R×Y →Risproperifforanyb,r ∈R,wehave
E [s(b,Y)]≥E [s(r,Y)],
Y∼b Y∼b
andstrictlyproperiftheinequalitybecomestightonlywhenr =b.
Thisimpliesthatwhenascoringruleis(strictly)proper,anexpertwithbeliefbaboutthedistributionof
outcomeY ∈Y would(uniquely)maximizetheirexpectedscorebyreportingr =b.
A(strictly)properlossfunctionℓisdefinedsimilarlywheretruthfulreportingofthebelief(strictly)mini-
mizestheexpectedloss. WeassumeR=[0,1]andY ={0,1}inthispaper.
Next,letusviewanyonlinealgorithmfortheproblemofpredictionwithexpertadviceasfollows.
Definition4(Probability-basedupdateclassofanonlinelearningalgorithm). AnonlinelearnerM maintains
adistributionπ overK experts. Inroundt,thelearnerdrawsanexpertI = iwithprobabilityπ . Then,
t t t,i
theoutcomey isrevealed,andthelearnerincurslossℓ =ℓ(r ,y )andupdatesπ onlyasafunction
t It,t It,t t t+1
ofr =(r ,...,r ),π =(π ,...,π ),andoutcomey .
t t,1 t,K t t,1 t,K t
Note that many algorithms can be described as probability-based update algorithms, such as MWU and
Hedge. In order to make a distinction, we describe the precise definition of Hedge and MWU using the
probability-basedupdatedescription.
Definition5. Hedgeinitializestheweightsw = 1 foralli∈[K],updatestheweightsineachroundbased
1,i K
ontheupdate
w =w ·exp(−ηℓ(r ,y )), (2)
t+1,i t,i i,t t
4andchoosesπ = wt,i .
t,i (cid:80) jwt,j
MWUdoesthesameexceptitusestheupdate
w =w ·(1−ηℓ(r ,y )).
t+1,i t,i i,t t
Notethat1−ηℓ(r ,y )isalinearapproximationofexp(−ηℓ(r ,y ))around0.
i,t t i,t t
UnlikethesettingofRoughgardenandSchrijvers(2017)whereusingMWUwithaproperlossfunctionℓ
impliesincentive-compatibility,inthissettingwedonotachieveincentive-compatibilitysincethenormaliza-
tionwouldimpacttheincentive.5 Indeed, intheMWUalgorithm, inroundt, dependingontheoutcomey ,
t
thesumoftheweightsofallexperts(thenormalizationfactor)canbedifferent. Thiswillskewtheincentive
ofanexpertwhowantstoreporttomaximizetheexpectednormalizedweight.
2.3.1 Connectiontowageringmechanism
Towardgettinganupdaterulethatisincentivecompatible, Freemanetal.(2020)observedaconnectionbe-
tween online learning algorithms and wagering mechanisms. In a wagering mechanism, each player reports
theirpredictionaboutarandomoutcomeandatthesametimewagers(bets)anon-negativeamountofmoney
ontheirprediction. Oncetheoutcomeisrealized,themechanismwillpayeachplayerapaymentbasedonthe
qualityoftheirpredictionandtheamounttheywagered.
More concretely, consider the specific setting of wagering mechanisms where there are K fixed players
calledexperts,andthereisanunknownBernoullioutcomey ∈ {0,1}. Eachexperti ∈ [K]withbeliefb ∈
i
[0,1]abouttheprobabilitythaty = 1wagersm > 0andreportsr ∈ [0,1]withthegoalofmaximizingthe
i i
expectedpaymentfromthemechanism.Notethatr mayormaynotbeequaltob .Oncetherandomoutcome
i i
y isrevealed,themechanismtakesthevectorofreportsr = (r ,...,r ),wagersm = (m ,...,m ),and
1 K 1 K
the realization y and outputs a K dimensional vector of Γ(r,m,y) ∈ RK where Γ (r,m,y) ∈ R is the
i
paymentthatthemechanismwillpaytoexperti.
Amechanismiscalledincentive-compatibleifanyexpertiwithbeliefb strictlymaximizestheirexpected
i
paymentbyreportingr =b ,i.e.,
i i
b =argmaxE [Γ ((r ,r ),m,y)]
i y∼Bern(bi) i i −i
r∈R
foranyfixedvectorr ofreportsoftheotherexpertsandvectorofwagersm.
−i
The Weighted-Score Wagering Mechanism (WSWM) is an incentive-compatible wagering mechanism
definedasfollows.
Definition6(Lambertetal.(2008)). TheWeighted-ScoreWageringMechanismisawageringmechanismthat
mapsanyvectorsrandmandoutcomeytopaymentΓ=(Γ ,...,Γ ),where
1 K
 
(cid:88)
ΓW
i
SWM(r,m,y)=m i1−ℓ(r i,y)+ m jℓ(r j,y)
j∈[K]
isthepaymentforexpertiandℓisastrictlyproperlossfunction.
This mechanism has several essential properties (Lambert et al., 2008). First, the mechanism is budget-
(cid:80) (cid:80)
balancedmeaning Γ (r,m,y)= m ,andmoreover,thepaymentisnon-negative,i.e.,Γ (r,m,y)≥
j j j j i
0. Observethatdesigninganincentive-compatibleprobability-basedonlinelearningalgorithmcanbeseenas
designing an incentive-compatible wagering mechanism that is budget balanced with non-negative payment
asfollows.
Consider the probability-based description of any online learning algorithm. This algorithm wants to
reallocate π to π . To do that, the algorithm at round t asks for the reports of the experts. Once the
t t+1
5NotethatHedgeisnotincentive-compatibleeveninthesettingofRoughgardenandSchrijvers(2017).
5outcomey isrevealed,thealgorithmusesthereportsoftheexpertsr = (r ,...,r ),thewagersvector
t t t,1 t,K
π =(π ,...,π ),andoutcomey tosettheprobabilityinthenextroundas
t t,1 t,K t
π =Γ (r ,π ,y ).
t+1,i i t t t
SincethepaymentΓ (r ,π ,y )isincentive-compatible,expertswillreporttruthfully.
i t t t
Therefore,amongthealgorithmsintheclassofonlineprobability-basedupdateonlinelearningalgorithms
definedinDefinition4,theonlyincentive-compatibleonesaretheoneswheretheirupdatecanbedescribed
asawageringmechanismupdatethatisnon-negativeandbudget-balanced.
Usingawageringmechanismitselfdoesnotimplyano-regretguarantee;however,Freemanetal.(2020)
designed a wagering mechanism called Weighted-Score Update (WSU)6 in which the mechanism uses the
update
π =ΓWSU(r ,ηπ ,y )
t+1,i i t t t
=ΓWSWM(r ,ηπ ,y )+ΓConst(r ,(1−η)π ,y )
i t t t i t t t
=ΓWSWM(r ,ηπ ,y )+(1−η)π
i t t t t,i
foranyi ∈ [K], whereη ∈ (0,0.5)andΓConst issimplyamechanismthatreturnstheinputwagers. Using
i
thedefinitionofWSWM,thisupdatemaybewrittenas
(cid:32) (cid:32) (cid:33)(cid:33)
(cid:88)
π =π 1−η ℓ − π ℓ . (3)
t+1,i t,i t,i t,j t,j
j
√
Freeman et al. (2020) showed that WSU with the update of form (3) can achieve O( T lnK) regret in the
full-informationsetting.
Interestingly,anapparentlyunnoticedconnectionisthattheupdateofform(3)recoversthesameupdate
astheML-ProdupdateofGaillardetal.(2014)ifinallroundsallexpertsusethesamelearningrateη.
2.4 Existingbanditresults
Freeman et al. (2020) extended their result to the bandit case by designing the Weighted-Score Update with
UniformExploration(WSU-UX)algorithmdescribedinAlgorithm2,andtheyshowedaO(T2/3(KlnK)1/3)
upperboundontheregretofthisalgorithm. Inparticular,intheiralgorithm,theyusedthecommontechnique
of constructing unbiased importance-weighted loss estimates. The algorithm then applies the WSU update
on the estimated losses to update the probability distribution. For some technical reasons, they additionally
neededtomixtheprobabilitydistributionoverarms(π )withauniformdistributionwithweightγ ∈ [0,1]
t
togettheprobabilitydistribution(π˜ )fromwhichanarmisselected,i.e.,π˜ = (1−γ)π +γ1. Thetwo
t t,i t,i k
technicalreasonsforusingγ areasfollows:
1. Tomakesurethataftereachupdate,π isstillavalidprobabilitydistribution.
t,i
2. Theirregretupperboundcanbeextremelylargeincasetheydonotmix(i.e.γ =0).
Note that mixing with uniform distribution is not for the purpose of getting high probability bounds, as
theyboundpseudo-regret.7
Indeed,theyshowedforWSU-UXwithlearningrateηandmixingweightγ,
ηKT lnK
E[R ]≤γT + + +2ηKT.
T γ η
Thebestchoiceofηandγ attainstheregret
E[R ]≤2(4T)2/3(KlnK)1/3,
T
whichisO(T2/3)intermsofT.
6Theydonotcalltheirupdateruleasawageringmechanism,butitcanbeviewedasawageringmechanism.
7“Regret”inthisworkisactuallypseudo-regret,whichequalsexpectedregretunderobliviousbeliefsandoutcomes.
6Algorithm2:WSU-UX(Freemanetal.,2020)
Input: η,γ ∈(0,1/2)suchthat ηK ≤1/2,andlosssequenceℓ(x,y).
γ
Setπ = 1,∀i∈[K]
1,i K
fort∈[T]do
ThelearnerchoosesexpertI accordingtodistr. π˜ =(1−γ)π + γ,∀i∈[K].
t t,i t,i K
Armi=I formsabeliefb ∈[0,1].
t i,t
Armi=I reportsareportr ∈[0,1]withthegoalofmaximizingE [π ].
t i,t yt∼Bern(bi,t) t+1,i
Naturerevealstheoutcomey ∈{0,1}
t
Thelearnercomputesℓˆ = ℓ(ri,t,yt) fori=I andℓˆ =0,∀j ̸=I .
i,t π˜i,(cid:16)t
(cid:16)
t j,t
(cid:17)(cid:17)
t
Thelearnerupdatesπ =π 1−η ℓˆ −(cid:80)K π ℓˆ .
t+1,i t,i t,i j=1 t,j t,j
It was unclear whether the higher regret was an artifact of their analysis or a limitation of WSU-UX. If
thereisatighteranalysisofWSU-UX’sregret, thenforsomevalidγ,η, wewouldhaveE[R ] = o(T2/3).
T
However,weshowthatforanyvalidγ,η,(seeSection3.2foradescriptionofvalidhyperparamters)forlarge
enoughT,thereexistsalosssequenceforwhichE[R ] = Ω(T2/3),implyingthatWSU-UXcannotachieve
T
regretbetterthanO(T2/3).
3 Regret lower bound for WSU-UX
3.1 PotentialanalysisviewcomparisionbetweenEXP3andWSU-UX
A potential(-based) analysis is a common way to analyze the regret of online learning algorithms (Cesa-
Bianchi and Lugosi, 2003). We compare the potential analysis of WSU-UX and EXP3, beginning with the
full-informationvariationofeachalgorithmandthenturningtotheimplicationsinthebanditsetting.
InthepotentialanalysisofHedge,foranyi ∈ [K]andt ∈ [T],wedefineΦHEDGE := w withw as
t,i t,i t,i
inDefinition5. WedefineΦHEDGE := (cid:80) w . Bynon-negativityofw ,wehave
t j∈[K] t,j t,i
1 1
ln(ΦHEDGE)≤ ln(ΦHEDGE), (4)
η T+1,i η T+1
whereη isthelearningrateofthealgorithm. FromtheLHSandRHSof(4), wecanextractthecumulative
lossofexpertiandthecumulativelossofthelearningalgorithmrespectively. However,wemightnotbeable
toexactlyextractthesetwoquantitiesfromthepotentialsastheremightbesomeerrortermsinvolvedinthe
extractionprocess. Indeed,forHedge,theLHSisexactlythecumulativelossofexperti; however,theRHS
canonlybeupperboundedbythecumulativelossofthelearnerplussomeextraterms:
1 (cid:88) (cid:88)
ln(ΦHEDGE)≤ π ℓ
η T+1 t,j t,j
t∈[T] j
 
+ ln ηK +η (cid:88) (cid:88) π t,j(ℓ t,j)2 . (5)
(cid:124)(cid:123)(cid:122)(cid:125) t∈[T] j
explorationterm (cid:124) (cid:123)(cid:122) (cid:125)
Secondordererror
Thesetwotermswillappearintheregretanalysis.
On the other hand, note that the WSU update can be written as a linear approximation of the Hedge
update at the point ℓ¯ := (cid:80) π ℓ (see Appendix C for details). This means that WSU just uses a linear
t j t,j t,j
approximationofHedgewhenupdatingthepotential.Thischangeinpotentialfunctionwillimpacttheprocess
ofextractingtheregretfromthepotential.
ForWSU,thepotentialisdefinedasΦWSU := π andΦWSU := (cid:80) π =1. Bynon-negativity
t,i t,i t j∈[K] t,j
7ofπ wehave
t,i
1 1
ln(ΦWSU )≤ ln(ΦWSU)=0. (6)
η T+1,i η T+1
Now, the RHS of (6) (which is 0) does not involve any second-order error term. In fact, since WSU is
normalized, the RHS does not give us information about the regret. However, we can extract the difference
between the cumulative loss of the algorithm and expert i from the LHS of (6), and this extraction process
wouldleadtoasecond-ordererrorterm. Indeedwehave
 
1 (cid:88) (cid:88)
η ln(ΦW T+S 1U ,i)≥  π t,jℓ t,j −ℓ t,i
t∈[T] j
 2
lnK (cid:88) (cid:88)
− η −η  π t,jℓ t,j −ℓ t,i . (7)
(cid:124)(cid:123)(cid:122)(cid:125) t∈[T] j
exploration (cid:124) (cid:123)(cid:122) (cid:125)
term Second-ordererror
Thesetwotermsin(7)willappearintheregret.
(cid:104) (cid:105)
(cid:80)
Comparing(7)and(5): Notethattheerrortermin(7)isasecond-orderversionof π ℓ −ℓ for
j t,j t,j t,i
(cid:104) (cid:105)
a fixed i whereas the second-order term in (5), which is (cid:80) π (ℓ )2 , is a weighted average of (ℓ )2
j t,j t,j t,j
weightedbyπ .
t
Implicationforbanditcase Now,inthebanditcasewhereweuseℓˆ tobeanunbiasedestimatedlossfor
t,i
(cid:20) (cid:16) (cid:17)2(cid:21)
quantityℓ ,theexpectationofthesecond-ordertermin(5)isE (cid:80) π ℓˆ =O(K),whereastheex-
t,i j t,j t,j
(cid:20)(cid:16) (cid:17)2(cid:21) (cid:20)(cid:16) (cid:17)2(cid:21) (cid:20)(cid:16) (cid:17)2(cid:21)
pectationofthesecond-ordertermin(7)isE (cid:80) π ℓˆ −ℓˆ ≤E (cid:80) π ℓˆ +E ℓˆ =
j t,j t,j t,i j t,j t,j t,i
(cid:104) (cid:105)
2K+E 1 =O(K). ThisdifferencemakestheregretboundforWSU-UXlarger.
π˜t,i γ
However, it is not clear whether the potential-based analysis is tight or if there might be a way to get a
betterregretupperbound. Next,weshowourmainresult,alowerbounddemonstratingthatitisnotpossible
togetabetterupperbound.
3.2 Lowerboundproof
WeshowthatWSU-UXcannotachieveregretbetterthanΩ(T2/3). Thefollowingtheoremrestrictsthefocus
tovalidsettingsofthehyperparameters,whichwedefineafterthetheorem.
Theorem7. Foranyvalidsetofhyperparameters(η,γ)thereexistsT suchthatforanyT ≥T ,
0 0
E[R ]=Ω(T2/3).
T
ThenotionofvalidhyperparametersistakenfromtherestrictionsimposedbyFreemanetal.(2020). In
particular,therestrictionsare ηK ≤ 1/2andη,γ ∈ (0,1/2). SeethebeginningoftheAppendixAformore
γ
informationabouttherestrictions.
Now,wefurtherpartitionthesetofvalidhyperparameters(η,γ)intotwocases:
• thetrivialcase: η <T−2/3orγ >T−1/3;
• thenon-trivialcase: η ≥T−2/3andγ ≤T−1/3.
8Inthetrivialcase,eitherthelearningrateηistoosmall,causingthealgorithmtotakealongtimetoconcentrate
ontheoptimalexpertandincurringalargeregretoforderΩ(T2/3),orγissolargethattheuniformexploration
would cause the algorithm to incur a large regret of Ω(T2/3). The proof for the trivial case, along with all
otherresultsinthispaper,canbefoundintheappendix. Forthenon-trivialcase,weshowthefollowing.
Theorem8. ForK =2andforanyvalidsetof(η,γ)inthenon-trivialcase,thereexistsT suchthatforany
0
T ≥T ,wehavealosssequence{ℓ }T suchthat
0 t t=1
E[R ]=Ω(T2/3). (8)
T
3.3 High-levelprooffornon-trivialcase
Inthissubsection,wegiveahigh-levelproofofTheorem8. Wefirstintroducethefollowinglosssequence.
Definition9. ForanyT,wedefine
(cid:40)
ℓ =1,ℓ =0 for1≤t≤ T
{ℓ }T = t,1 t,2 100 .
t t=1 ℓ =0,ℓ =1 for T <t≤T
t,1 t,2 100
Moreover,wecallthesetofrounds{t : 1 ≤ t ≤ T }Phase1,whereonlyarm1incursloss,andthesetof
100
rounds{t: T <t≤T}Phase2,whereonlyarm2incursloss.
100
From now on, by {ℓ }T we mean the loss sequence defined in Definition 9. Note that in this loss
t t=1
sequence,thebestarmisarm1. Ourgoalistoshowthatthisparticularlosssequenceforcesthealgorithmto
incurlargeregret. Wedothisbydecomposingtheregretintothreeterms,asdescribedinTheorem10.
Theorem 10. When running WSU-UX for any valid choice of (η,γ) in the non-trivial case, there exists T
0
suchthatforanyT ≥T ,forlosssequence{ℓ }T ,wehaveforsomeconstantsc ,c ,c >0,
0 t t=1 1 2 3
1 ηTK
E[R ]≥c +c +c γT. (9)
T 1η 2 γ 3
NotethatTheorem10impliesE[R ]=Ω(T2/3)astheRHSof(9)canbelowerboundedbyΩ(T2/3).
T
3.3.1 ProofofTheorem10
It remains to show (9). To do that, we first introduce the following key lemma that follows similar steps as
Lemma 4.3 of Freeman et al. (2020) but with all the inequalities in the reverse direction, which implies a
second-orderlowerbound.
Lemma 11 (Second-order lower bound). For any valid choice of (η,γ) when running WSU-UX on loss se-
quence{ℓ }T ,weget
t t=1
T T T
(cid:88) (cid:88) π ℓˆ −(cid:88) ℓˆ ≥ lnπ T+1,1+lnK + η (cid:88) (ℓˆ − (cid:88) π ℓˆ )2. (10)
t,j t,j t,1 η 4 t,1 t,j t,j
t=1j∈[K] t=1 t=1 j∈[K]
Next,toconvert(10)tothelowerboundin(9),welistthreeclaimsthatholdwhenrunningWSU-UXon
thelosssequenceinDefinition9givenhyperparametersfallinginthenon-trivialcase. Notethateachclaim
correspondstoatermontheright-handsideof(9).
Claim1(Concentrationonbestarmattheend). ForlargeenoughT,thereexistsc >0suchthat
1
E[lnπ +lnK]≥c . (11)
T+1,1 1
9Claim2(Secondmomentlowerbound). ForlargeenoughT,thereexistsc suchthat
2
  2
T
E (cid:88) ℓˆ t,1− (cid:88) π t,jℓˆ t,j  ≥4c 2T γK .
t=1 j∈[1,2]
Claim3(Biasinducedbyuniformexploration). ForlargeenoughT,thereexistsc suchthat
3
 T (cid:32) (cid:33)
E (cid:88) (cid:88) π˜ t,jℓˆ
t,j
− (cid:88) π t,jℓˆ
t,j
≥c 3γT.
t=1 j∈[1,2] j∈[1,2]
Given all three claims and Lemma 11, the proof of Theorem 10 is straightforward. In loss sequence
{ℓ }T ,thebestarmisarm1;therefore,Claim3gives
t t=1
 
T T
E[R T]=E (cid:88) (cid:88) π˜ t,jℓˆ t,j −(cid:88) ℓˆ t,1
t=1j∈[K] t=1
 
T T
≥E (cid:88) (cid:88) π t,jℓˆ
t,j
−(cid:88) ℓˆ t,1+c 3γT. (12)
t=1j∈[K] t=1
Tofurtherlowerbound(12),wetaketheexpectationofbothsidesof(10)anduseClaims1and2toget
1 ηTK
E[R ]≥c +c +c γT.
T 1η 2 γ 3
3.3.2 Subtletyofshowingtheclaims
The proof of the claims is not straightforward and requires subtle work. At a high level, we designed loss
sequence {ℓ }T such that for a constant fraction of rounds T ≤ t ≤ T , arm 1 (the best arm) has a
t t=1 200 100
cumulativelosslinearlyworsethanarm2. ThiscanbeusedtoshowClaim2. However,duringtheserounds
T ≤ t ≤ T , arm 1 keeps getting small probability and hence very large estimated loss. Yet, because
200 100
thealgorithmhassomeuniformexploration, itpicksarm1frequently. Therefore, atthebeginningofround
t≥ T ,theprobabilityupdateisveryslow. Therefore,itisnotobviouswhetherthealgorithmcanallowπ
100 T,1
torecoverattheendornot.
The next section gives a technical overview of how we prove the claims. We perform a careful analysis
oftheprobabilityupdates,leveragingarecentlyshownmultiplicativeformofAzuma’sinequality(Kuszmaul
andQi,2021)insomekeystepstoshowthatindeedwithprobabilityatleast1−O( 1 ),π ≥1/4.
T2 T1+T2+1,1
Wethenshowthatwithhighprobabilityπ ≥3/4. ThisimpliesClaim1andalsoClaim3.
T+1,1
4 Showing the claims
TheproofofClaim2isfairlystraightforward. Theproofrequiresthefollowinglemma.
Lemma12. InWSU-UXwithtwoarms(i,¯i),
E[π |F ]=(1−C )π +C π2 , (13)
t+1,i t−1 t,i t,i t,i t,i
(cid:0) (cid:1)
whereC t,i :=η ℓ t,i−ℓ t,¯i andF t−1isthehistoryupuntiltheendofroundt−1.
10ThislemmacanbeusedtoshowthatfortinPhase1,i.e.,t≤ T ,π decreasesinamultiplicativeway
100 t,1
andwehave
η
E[π ]≤(1− )E[π ].
t+1,1 2 t,1
Theaboveinequalitycanbeusedtoshowthatforaconstantfractionofrounds T ≤t≤ T wehave
200 100
1
E[π ]≤ . (14)
t,1 KT
Theabovefactcanbefurtherutilizedtoshowthatthesummationofthesecondmomentsoftheestimatedloss
differencesinClaim2islarge,whichprovesClaim2.
Claims 1 and 3 require more sophisticated techniques. To demonstrate them, we need to analyze the
behaviorofπ fort∈[T]whenrunningthealgorithmonthelosssequence{ℓ }T . Notethatsinceweare
t,1 t t=1
inthebanditcase,π isarandomvariable.
t,1
RecallPhases1and2fromDefinition9,thedefinitionofthelosssequence.Weneedtofurtherdecompose
thesephasesintomultiplesub-phasesasdefinedbelow.
Definition13. DefineT = 1 T,T = 2 T,T = 1 T,T = 69 T,andthendefine(sub-)phasesasfollows:
1 100 2 10 3 10 4 100
• Phase1: T ={t:1≤t≤T },
1 1
• Phase2.1: T ={t:T +1≤t≤T +T },
2 1 1 2
• Phase2.2: T ={t:T +T +1≤t≤T +T +T },
3 1 2 1 2 3
• Phase2.3: T ={t:T +T +T +1≤t≤T +T +T +T }.
4 1 2 3 1 2 3 4
Moreover,wedefineT′ andM,whichareintermediatenumbersthataregoingtobeusedinouranalysis
regardinghighprobabilitystatementsinthissection.
Definition14. WedefineM andT′asfollows
 
1  (cid:18) 2K(cid:19) ηK 
M := ln +2(1+ε )(1+ )ηT  (15)
ln2 γ 1 γ 1
 
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
∝(lnT) ∝(ηT1)
1 4 2
T′ := ( ) M (16)
1−ε 3−γ η
2
(cid:113) (cid:113)
whereε = 6lnT andε = 4lnT .
1 2 KγT1 2 3− 4γT2
Note that since we are in the non-trivial case (η ≥ T−2/3 and γ ≤ T−1/3), as T goes to ∞, we have
ε ,ε →0andmoreoverM ≈cηT andT′ = c′M ≈ cc′T . NotethatforlargeenoughT,wehave
1 2 1 η 1
T′ ≤T . (17)
2
Wenowgiveahigh-levelpictureofhowweproveClaims1and3,whichalsowillgivemoreintuitionabout
M andT′.
ProofSketchofClaims1and3. LetE = {π ≥ 2−M}betheeventthatarm1’sprobabilityattheend
1 T1+1,1
ofPhase1isnottoosmall,whereM isdefinedin(15). LetE ={π ≥ 1}betheeventthatarm1’s
2 T1+T2+1,1 4
probabilityattheendofPhase2.2hasrecoveredto 1.
4
First,weshowthatwithhighprobability,thealgorithmdoesnotpullarm1toomanytimesinPhase1. In
particular,weshowthatattheendofPhase1,theprobabilityπ ofselectingarm1islowerboundedby
T1+1,1
2−M (hence,E happens).Toprovethisresult,weleveragearecentmultiplicativeformofAzuma’sinequality
1
formartingales(KuszmaulandQi,2021).
11ThenextkeystepistoshowthatifE happens,thenwithhighprobabilityE happens. Sincewealready
1 2
showedthatE happenswithhighprobability,itfollowsthatwithhighprobabilitywehavethatπ ≥
1 T1+T2+1,1
1/4. Now,toshowthiskeystep,weproceedasfollows. First,weobservethatinPhase2.1,itisonlyviapulls
ofarm2thattheprobabilityofarm1canincrease. Moreover,initially,therateofupdateofarm1is πt+1,1,
πt,1
whichisverycloseto1. Therefore,wefirstanalyzehowmanypullsofarm2sufficeforarm1’sprobability
todouble, andwethenanalyzehowmanydoublingsareneededtosatisfyeventE . Finally, weagainusea
2
martingaleanalysis(multiplicativeAzuma)toshowthatwithintheroundsofPhase2.1,thesufficientnumber
ofpullsofarm2occurwithhighprobabilityandhenceeventE happens.
2
WethenshowthatconditionaloneventE ,fromPhase2.2onwards,theprobabilityπ goestozeroexpo-
2 t,2
nentiallyquicklyastincrementsbeyondT +T . Therefore,theprobabilityπ convergesto1exponentially
1 2 t,1
quickly. This,combinedwithLemma12,isessentiallywhatisneededtoproveClaim3. ForClaim1,weuse
acarefulanalysisbasedonChebyshev’sinequalitytoshowthatwithprobabilityexponentiallycloseto1,π
T,1
isatleast3/4. ThisisessentiallywhatallowsustocontroltheexpectedlogprobabilityterminClaim1and
hencewhatallowsClaim1togothrough.
Acknowledgements
AM and NM were supported by NSERC Discovery Grant RGPIN- 2018-03942 and a JP Morgan Faculty
ResearchAward.
References
Sanjeev Arora, Elad Hazan, and Satyen Kale. The multiplicative weights update method: a meta-algorithm
andapplications. TheoryofComputing,8(1):121–164,2012.
PeterAuer,NicoloCesa-Bianchi,YoavFreund,andRobertESchapire. Thenonstochasticmultiarmedbandit
problem. SIAMJournalonComputing,32(1):48–77,2002.
Andreas Buja, Werner Stuetzle, and Yi Shen. Loss functions for binary class probability estimation and
classification: Structureandapplications. Workingdraft,November,3:13,2005.
Nicolo Cesa-Bianchi and Gábor Lugosi. Potential-based algorithms in on-line prediction and game theory.
MachineLearning,51:239–261,2003.
RupertFreeman,DavidPennock,CharaPodimata,andJenniferWortmanVaughan. No-regretandincentive-
compatibleonlinelearning. InInternationalConferenceonMachineLearning,pages3270–3279.PMLR,
2020.
YoavFreundandRobertESchapire.Adecision-theoreticgeneralizationofon-linelearningandanapplication
toboosting. JournalofComputerandSystemSciences,55(1):119–139,1997.
RafaelFrongillo,RobertGomez,AnishThilagar,andBoWaggoner.Efficientcompetitionsandonlinelearning
with strategic forecasters. In Proceedings of the 22nd ACM Conference on Economics and Computation,
pages479–496,2021.
PierreGaillard,GillesStoltz, andTimVanErven. Asecond-orderboundwithexcesslosses. InConference
onLearningTheory,pages176–196.PMLR,2014.
TilmannGneitingandAdrianERaftery. Strictlyproperscoringrules,prediction,andestimation. Journalof
theAmericanStatisticalAssociation,102(477):359–378,2007.
Elad Hazan et al. Introduction to online convex optimization. Foundations and Trends® in Optimization, 2
(3-4):157–325,2016.
JyrkiKivinenandManfredKWarmuth. Exponentiatedgradientversusgradientdescentforlinearpredictors.
InformationandComputation,132(1):1–63,1997.
12WilliamKuszmaulandQiQi. ThemultiplicativeversionofAzuma’sinequality,withanapplicationtocon-
tentionanalysis. arXivpreprintarXiv:2102.05077,2021.
Nicolas S Lambert, John Langford, Jennifer Wortman, Yiling Chen, Daniel Reeves, Yoav Shoham, and
David M Pennock. Self-financed wagering mechanisms for forecasting. In Proceedings of the 9th ACM
ConferenceonElectronicCommerce,pages170–179,2008.
TimRoughgardenandOkkeSchrijvers. Onlinepredictionwithselfishexperts. AdvancesinNeuralInforma-
tionProcessingSystems,30,2017.
VladimirGVovk. Agameofpredictionwithexpertadvice. InProceedingsoftheEighthAnnualConference
onComputationalLearningTheory,pages51–60,1995.
A Main result and high-level proof
We give a high-level proof of the main theorem. For the convenience of the reader, we re-state the lemmas
alongwiththeirproofs.
Theorem7. Foranyvalidsetofhyperparameters(η,γ)thereexistsT suchthatforanyT ≥T ,
0 0
E[R ]=Ω(T2/3).
T
We remind the reader that we restrict our attention to the set of valid hyperparameters: ηK ≤ 1/2 and
γ
η,γ ∈(0,1/2). ThenotionofvalidhyperparametersistakenfromtherestrictionsimposedbyFreemanetal.
(2020). Webrieflyexplainsomeoftherestrictions.
Wenotethatweneed ηK ≤1tomakesurethatwegetavalidprobabilitydistributionπ afterupdating
γ t+1
ineachroundt. Moreover,togetsublinearregret,weneedγ ≤1/2;otherwise,uniformexplorationwillpick
thesuboptimalarmfrequently, whichcancauselinearregret. SinceK ≥ 1, theinequality ηK ≤ 1implies
γ
η ≤ γ ≤γ ≤1/2andthereforeη ≤1/2.
K
Our analysis focuses on the restriction ηK ≤ c for c = 1/2. Thus far, for technical reasons related to
γ
certain inequalities, we are not sure whether our particular analysis can be made to go through for a larger
c (that is still less than 1). The main concern arises as c gets closer to 1. However, using advanced Taylor
approximation-basedinequalities,thismightbepossible.
Now,wefurtherpartitionthesetofvalidhyperparameters(η,γ)intotwocases:
• Thetrivialcase: η <T−2/3orγ >T−1/3
• Thenon-trivialcase: η ≥T−2/3andγ ≤T−1/3.
The proof strategy for this theorem is that for any valid hyperparameters (η,γ), we show there exists a loss
sequencesuchthatthealgorithmwillincuranexpectedregretofΩ(T2/3).
A.1 Trivialcase
WeshowthatinthetrivialcaseE[R ]=Ω(T2/3).
T
Lemma15. ForK =2andforanysetting(η,γ)inthetrivialcasewhereη <T−2/3orγ >T1/3,
E[R ]=Ω(T2/3).
T
WeprovethisbyprovingthefollowingLemma16forthecasewhereη < T−2/3 andLemma17forthe
casewhereγ >T−1/3.
13Lemma16. ForWSU-UXrunwithanyη < T−2/3 andanyγ ≥ 0thereexistsalosssequence{ℓ }T and
t t=1
c= 1 >0suchthat
200
E[R ]≥cT2/3.
T
BeforeprovingLemma16,letusrestateandproveLemma12. Wewillusethislemmanumeroustimesin
thecourseofprovingthemainresult.
Lemma12. InWSU-UXwithtwoarms(i,¯i),
E[π |F ]=(1−C )π +C π2 , (13)
t+1,i t−1 t,i t,i t,i t,i
(cid:0) (cid:1)
whereC t,i :=η ℓ t,i−ℓ t,¯i andF t−1isthehistoryupuntiltheendofroundt−1.
ProofofLemma12. Whenwehavetwoarms,theupdateruleforarmicanbeexpressedas
  
π t+1,i =π t,i1−ηℓˆ t,i− (cid:88) π t,jℓˆ t,j
j∈{i,¯i}
 
=π t,i−ηπ t,iℓˆ t,i+ηπ t,i (cid:88) π t,jℓˆ t,j.
j∈{i,ˆi}
Now,takingtheexpectationofbothsidesconditionalonthepast,weget
E[π |F ]=E [π ]
t+1,i t−1 t−1 t+1,i
 
=E t−1π t,i−ηπ t,iℓˆ t,i+ηπ t,i (cid:88) π t,jℓˆ t,j
j∈{i,¯i}
(cid:104) (cid:105) (cid:104) (cid:105)
=π t,i−ηπ t,iE t−1[ℓˆ t,i]+ηπ t2 ,iE t−1 ℓˆ t,i +η(π t,i)(π t,¯i)E t−1 ℓˆ t,¯i
=π t,i−ηπ t,iℓ t,i+ηπ t2 ,iℓ t,i+η(π t,i)(1−π t,i)ℓ t,¯i.
Now,rearrangingtheterms,weget
E t[π t+1,i]=(cid:0) 1−ηℓ t,i+ηℓ t,¯i(cid:1) π t,i+η(cid:0) ℓ t,i−ℓ t,¯i(cid:1) π t2
,i
=(1−C )π +(C )π2 ,
t,i t,i t,i t,i
(cid:0) (cid:1)
wherewerecallthatC t,i =η ℓ t,i−ℓ t,¯i .
Now,wearereadytoproveLemma16.
ProofofLemma16. Considerlosssequencewithtwoarms,{ℓ }T whereℓ = 0,ℓ = 1for1 ≤ t ≤ T.
t t=1 t,1 t,2
Inthiscase,weshowthatregretsimplifiestothenumberoftimeswepullarm2. Inparticular,
 
T 2 T
(cid:88)(cid:88) (cid:88)
E[R T]=E  π˜ t,jℓ t,j − ℓ t,1
t=1j=1 t=1
T
(cid:88)
= E[π˜ ]ℓ (ℓ =0,∀t∈[T])
t,2 t,2 t,1
t=1
T
(cid:88)
= E[π˜ ]. (ℓ =1,∀t∈[T])
t,2 t,2
t=1
14Rememberthat
γ
π˜ =(1−γ)π + . (18)
t,i t,i 2
Now,takingtheexpectationofbothsidesof(18)fori=2,weget
T T
(cid:88) (cid:88) γ
E[π˜ ]= E[(1−γ)π + ]
t,2 t,2 2
t=1 t=1
T
(cid:88) γ
=(1−γ) E[π ]+ T. (19)
t,2 2
t=1
Next,welowerboundthefirstterm. ObservethatbyapplyingLemma12onanyroundt>1,weget
E[π |F ]=(1−η)π +ηπ2
t,2 t−2 t−1,2 t−1,2
η
≥(1− )π , (20)
2 t−1,2
wherethelastinequalitycomesfromthefactthatπ ≤1/2. Thisfactistruebecauseweknowtheinitial
t−1,2
value for π = 1 in the first round, as this quantity can only decrease, E[π2 ] ≤ 1E[π ]. Now, taking
1,2 2 t,2 2 t,2
expectationofbothsidesof(20),weget
η
E[π ]≥(1− )E[π ]. (21)
t,2 2 t−1,2
Now,using(21)recursively,weget
η
E[π ]≥π (1− )t−1
t,2 1,2 2
1 η
≥ e−η(t−1). (1− ≥e−η,∀η :0<η ≤0.5)
2 2
SettingT′ =min{⌊ln100⌋+1,T}≤T ,wehave
η
T T T′
(cid:88) (cid:88)1 (cid:88)1
E[π ]≥ e−η(t−1) ≥ e−η(t−1)
t,2 2 2
t=1 t=1 t=1
(cid:88)T′
1 T′ 1 ln100
≥ = ≥ min{ ,T}. (22)
200 200 200 η
t=1
Now,byusing(22),wecanfurtherlowerbound(19)toget
T T
(cid:88) (cid:88) γ
E[π˜ ]=(1−γ) E[π ]+ T
t,2 t,2 2
t=1 t=1
(cid:18) (cid:26) (cid:27)(cid:19)
1 ln100 T
≥(1−γ) min ,T +γ (from(22))
200 η 2
(cid:26) (cid:26) (cid:27) (cid:27)
1 ln100 T
≥min min ,T , (γα+(1−γ)β ≥min{α,β})
200 η 2
(cid:26) (cid:27)
1 ln100
= min ,T
200 η
(cid:26) (cid:27)
1 1
≥ min ,T
200 η
1
≥ T2/3 (η >T−2/3)
200
=cT2/3.
15Wenowpresentthenextlemma.
Lemma17. ForWSU-UXrunwithanyγ > T−1/3 andanyη ≥ 0thereexistsalosssequence{ℓ }T and
t t=1
c= 1 >0suchthat
2
E[R ]≥cT2/3.
T
Proof. ConsiderthesamelosssequenceasLemma16withtwoarms: {ℓ }T whereℓ = 0,ℓ = 1for
t t=1 t,1 t,2
1 ≤ t ≤ T. Thebestarmisarm1. Sincewehaveauniformexplorationofγ,inanyroundt ∈ [T],wepick
arm2withprobabilityπ˜ = (1−γ)π +γ1 ≥ γ1. Therefore, thealgorithmincursatleastγ1 lossfor
t,2 t,2 2 2 2
eachround. Hence
T T
(cid:88) (cid:88)γ γT
E[R ]= E[π˜ ]≥ = >cT2/3,
T t,2 2 2
t=1 t=1
wherethelastinequalityholdsbecauseγ >T−1/3.
A.2 Non-trivialcase
Forthenon-trivialcase,wehavethefollowingtheoremasstatedinthemaintext.
Theorem8. ForK =2andforanyvalidsetof(η,γ)inthenon-trivialcase,thereexistsT suchthatforany
0
T ≥T ,wehavealosssequence{ℓ }T suchthat
0 t t=1
E[R ]=Ω(T2/3). (8)
T
Note that in the non-trivial case, we always consider the loss sequence defined in Definition 9. For the
convenienceofthereader,werecallthatforanyT,wedefine{ℓ }T as
t t=1
(cid:40)
ℓ =1,ℓ =0 for1≤t≤ T
{ℓ }T = t,1 t,2 100 .
t t=1 ℓ =0,ℓ =1 for T <t≤T
t,1 t,2 100
ToproveTheorem8,byLemma18wefirstobservethatanyboundoftheformc 1 +c ηKT +c γT can
1η 2 γ 3
belowerboundedbyc 4K31T2 3.
Lemma18. Foranychoiceofc ,c ,c >0,andanyvalidchoiceofγ,η >0,thereexistsc >0suchthat
1 2 3 4
1 ηKT
c 1η +c 2 γ +c 3γT ≥c 4K31T2 3 =Ω(T2/3).
(cid:113) (cid:113)
Proof. Observethatc 1η1+c 2ηK γT ≥2 c η1 c2η γKT. Therefore,c 1η1+c 2ηK γT +c 3γT ≥2 c1c2 γKT +c 3γT.
(cid:113)
Definef(γ) := 2 c1c2 γKT +c 3γT forγ > 0. Notethatsincef isconvex, theminimumisattainedatγ∗
wheref′(γ∗)=0. Itiseasytoseethatγ∗ =c 3−2 3 ·(c1c T2K)1 3 andtherefore,weget
f(γ)≥f(γ∗)
=f(c−
3
2 3 ·(c 1c T2K )1 3)
(cid:115)
=2
c 3−
32c 1 ·c (2 cK
1c
T2T
K)1 3
+c 31 3(c 1c 2K)1 3T2 3
(cid:113)
=2 (c 1c 2c 3K)2 3T4 3 +(c 1c 2c 3K)1 3T2 3
=2(c 1c 2c 3K)31T32 +(c 1c 2c 3K)1 3T2 3
=3(c 1c 2c 3K)31T32.
Therefore,thereexistsc 4 =3(c 1c 2c 3)1 3 >0suchthatc 1η1 +c 2ηK γT +c 3γT ≥c 4K31T2 3.
16WewillthenshowthatforlargeenoughT,theregretonthisparticularlosssequencecanbelowerbounded
asfollows.
Theorem 10. When running WSU-UX for any valid choice of (η,γ) in the non-trivial case, there exists T
0
suchthatforanyT ≥T ,forlosssequence{ℓ }T ,wehaveforsomeconstantsc ,c ,c >0,
0 t t=1 1 2 3
1 ηTK
E[R ]≥c +c +c γT. (9)
T 1η 2 γ 3
Theorem10alongwithLemma18provesTheorem8.
A.2.1 ProvingTheorem10usingclaims
In this section, we show Theorem 10 given Claim 1, 2, and 3. We first restate and prove the second-order
lowerboundlemma.
Lemma 11 (Second-order lower bound). For any valid choice of (η,γ) when running WSU-UX on loss se-
quence{ℓ }T ,weget
t t=1
T T T
(cid:88) (cid:88) π ℓˆ −(cid:88) ℓˆ ≥ lnπ T+1,1+lnK + η (cid:88) (ℓˆ − (cid:88) π ℓˆ )2. (10)
t,j t,j t,1 η 4 t,1 t,j t,j
t=1j∈[K] t=1 t=1 j∈[K]
ProofofLemma11. Wecanexpressπ asfollows.
T+1,1
T
π = 1 (cid:89) (1−η(ℓˆ − (cid:88) π ℓˆ )).
T+1,1 K t,1 t,j t,j
t=1 j∈[K]
Takingthelogarithmofbothsides,weget
 
T
ln(π T+1,1)=−lnK+(cid:88) ln1−η(ℓˆ t,1− (cid:88) π t,jℓˆ t,j). (23)
t=1 j∈[K]
Next,weobservethatforanyfixedt ∈ [T],wehave−1 ≤ η(ℓˆ −(cid:80) π ℓˆ ) ≤ 1/2(byLemma20
t,1 j∈[K] t,j t,j
below). Notethatfor−1≤x≤1/2wehaveln(1−x)≤−x−x2/4(byLemma19below). Therefore,we
canshow
 
ln1−η(ℓˆ t,1− (cid:88) π t,jℓˆ t,j)≤−η(ℓˆ t,1− (cid:88) π t,jℓˆ t,j)− η 42 (ℓˆ t,1− (cid:88) π t,jℓˆ t,j)2
j∈[K] j∈[K] j∈[K]
TakingthesummationoverT roundsandcombiningwith(23),weget
lnπ ≤−lnK−η(cid:88)T (ℓˆ − (cid:88) π ℓˆ )− η2 (cid:88)T (ℓˆ − (cid:88) π ℓˆ )2.
T+1,1 t,1 t,j t,j 4 t,1 t,j t,j
t=1 j∈[K] t=1 j∈[K]
Rearranginganddividingbyη,weget
T T
(cid:88) (cid:88) π ℓˆ −ℓˆ ≥ lnπ T+1,1+lnK + η (cid:88) (ℓˆ − (cid:88) π ℓˆ )2.
t,j t,j t,1 η 4 t,1 t,j t,j
t=1j∈[K] t=1 j∈[K]
Thepreviousproofusedthefollowingtwosimplelemmas.
Lemma19. ln(1−x)≤−x−x2/4when−1≤x≤1/2.
17Proof. Let f(x) = ln(1−x)+x+x2/4. Then f′(x) = −1/(1−x)+1+x/2 = −x(1+x). Therefore
2−2x
f′(x)≥0for−1≤x≤0,andf′(x)<0for0<x≤1/2. Themaximumisattainedatx=0. Asaresult,
f(x)≤f(0)=0when−1<x≤1/2.
Lemma 20. For 0 ≤ η,γ ≤ 1/2 where ηK ≤ 1/2, and loss sequence {ℓ }T , for any round t ∈ [T], we
γ t t=1
have
 
−1≤ηℓˆ t,1− (cid:88) π t,jℓˆ t,j≤1/2.
j∈{1,2}
Proof. For1≤t≤ T ,wehaveℓ =0,andhenceℓˆ =0. Therefore,wehave
100 t,2 t,2
 
ηℓˆ t,1− (cid:88) π t,jℓˆ t,j=η(cid:16) ℓˆ t,1−π t,1ℓˆ t,1(cid:17)
j∈{1,2}
1−π
=ηℓ t,11[I =1].
t,1 π˜ t
t,1
Observethat
1−π
−1<0≤ηℓ t,11[I =1]
t,1 π˜ t
t,1
1
≤η
π˜
t,1
K γ
≤η (π˜ ≥ )
γ t,1 K
≤1/2.
For T <t≤T,wehaveℓ =0,andhenceℓˆ =0. Therefore
100 t,1 t,1
η(ℓˆ − (cid:88) π ℓˆ )=η(−π ℓˆ )
t,1 t,j t,j t,2 t,2
j∈{1,2}
π
=−ηℓ t,21[I =2]
t,2π˜ t
t,2
π
=−η t,2 ℓ 1[I =2].
π (1−γ)+ γ t,2 t
t,2 K
Now,wecansimplyshow
π
1/2≥0≥−η t,2 ℓ 1[I =2]
π (1−γ)+ γ t,2 t
t,2 K
π
≥−η t,2 ℓ 1[I =2]
π (1−γ) t,2 t
t,2
η
≥−
1−γ
η
≥− (γ ≤1/2)
1/2
≥−1. (η ≤1/2)
Then,asmentionedinthemainpartofthepaper,wecanuseClaims1,2,and3toconverttheRHSof(10)
tothelowerboundin(9).
18B Proof of claims
Inthispart,weprovetheclaims.
B.1 ProofofClaim2
WefirstrestateClaim2.
Claim2(Secondmomentlowerbound). ForlargeenoughT,thereexistsc suchthat
2
  2
T
E (cid:88) ℓˆ t,1− (cid:88) π t,jℓˆ t,j  ≥4c 2T γK .
t=1 j∈[1,2]
InordertoproveClaim2,weneedtointroducethefollowinglemma.
Lemma21. WhenrunningWSU-UXonlosssequence{ℓ }T andhyperparameterdefinedinthenon-trivial
t t=1
caseandforlargeenoughT,for T 21 ≤t≤T 1,wehave
1
E[π ]≤ .
t,1 KT
Proof. According to Lemma 12, which can be found in Appendix A.1, we have E[π |F ] = (1−
t+1,1 t−1
C t)π t,1 +C tπ t2 ,1 where C t = η(ℓ t,1 −ℓ t,2). For t in T 21 ≤ t ≤ T 1, we have π t,1 ≤ 1/2 and C t = η.
Therefore,
E[π |F ]=(1−η)π +ηπ2
t+1,1 t−1 t,1 t,1
η
≤(1−η)π + π
t,1 2 t,1
η
=(1− )π .
2 t,1
TakingtheexpectationoverallpossibleF ,weget
t−1
η
E[π ]≤(1− )E[π ].
t+1,1 2 t,1
Therefore,wehave
E(cid:104) π 1,T1(cid:105) =E[π
1,1]⌈T 2(cid:89)1⌉−1E
E[π [πs+1, ]1] ≤ 1 2(1− η 2)T 21
2 s,1
s=1
1 T
≤ 2(e−η 2)2T 00 (1+x≤ex,T 1 = 100)
(cid:18) (cid:19)
1 1
≤ exp − T1 3 . (η ≥T−2/3)
2 400
Since we are in the non-trivial case and η ≥ T−2/3, we have ηT ≥ T1/3. Since 1e−T 41 0/ 03 converges to
2
zero exponentially, whereas 1 convergence to zero at a slower rate, we can say for large enough T that
KT
1e− 4η 0T 0 ≤ 1e−T 41 0/ 03 ≤ 1 .
2 2 KT
Now,wearereadytoproveClaim2.
19ProofofClaim2. For T 21 ≤ t ≤ T 1, we have ℓ t,2 = 0, therefore ℓˆ t,2 = 0. Now we can lower bound
(cid:20) (cid:16) (cid:17)2(cid:21)
E (cid:80)T ℓˆ −(cid:80) π ℓˆ asfollows:
t=1 t,1 j∈[1,2] t,j t,j
  2   2
T T/200
E (cid:88) ℓˆ t,1− (cid:88) π t,jℓˆ t,j  ≥E  (cid:88) ℓˆ t,1− (cid:88) π t,jℓˆ t,j  
t=1 j∈[1,2] t=T/100 j∈[1,2]
 
T/100
=E  (cid:88) (cid:16) ℓˆ t,1−π t,1ℓˆ t,1(cid:17)2 
t=T/200
 
T/100
=E  (cid:88) (1−π t,1)2ℓˆ2 t,1
t=T/200
 
T/100
≥E  (cid:88) (1−1/2)2ℓˆ2 t,1 (∀t, 2T 00 ≤t≤ 1T 00 :π t,1 ≤1/2)
t=T/200
=
1
T (cid:88)/100 E(cid:34)(cid:18)
ℓ
t,11[I
=1](cid:19)2(cid:35)
4 π˜ t
t,1
t=T/200
=
1
T (cid:88)/100 E(cid:34)
E
(cid:34)(cid:18)
ℓ
t,1(cid:19)2
(1[I
=1])2(cid:35)(cid:35)
4 t−1 π˜ t
t,1
t=T/200
=
1 T (cid:88)/100 E(cid:34)(cid:18) 1 (cid:19)2
E
(cid:104)
(1[I
=1])2(cid:105)(cid:35)
(∀t,
T
≤t≤
T
:ℓ =1)
4 π˜ t−1 t 200 100 t,1
t,1
t=T/200
T/100 (cid:20) (cid:21)
1 (cid:88) 1
= E
4 π˜
t,1
t=T/200
T/100
1 (cid:88) 1
≥ ,
4 E[π˜ ]
t,1
t=T/200
wherethelastinequalitycomesfromapplyingJensen’sinequalityE(cid:2)1(cid:3) ≥ 1 . Notethatforlargeenough
X E[X]
T,for T ≤T ≤ T ,wehave
200 100
(cid:104) γ (cid:105)
E[π˜ ]=E π (1−γ)+
1,t 1,t K
γ
≤E[π ]+
1,t K
1 γ
≤ + (Lemma 21)
KT K
2γ 1
≤ , (since ≤γ)
K T
wherethelastinequalitycomesfromthefactthatwehave ηK ≤1/2forWSU-UX.Thisimpliesγ ≥2ηK ≥
γ
202KT−2/3 ≥ 1,whereweusethefactthatη ≥T−2/3. Asaresult,weget
T
  2
T T/100
E (cid:88) ℓˆ t,1− (cid:88) π t,jℓˆ t,j  ≥ 1
4
(cid:88) E[π˜1
]
t,1
t=1 j∈[1,2] t=T/200
T/100
1 (cid:88) K
≥
4 2γ
t=T/200
1 T T K 1 TK
≥ ( − ) = .
4 100 200 2γ 1600 γ
Therefore,forc = 1 ,Claim2holds.
2 4·1600
B.2 ProofsketchofClaims1and3
In this subsection, we prove Claims 1 and 3 using several technical lemmas without stating their proof. We
willproveallthetechnicallemmasinthenextsubsection.
Wefirstrecallthenotionofphaseshere.
Definition13. DefineT = 1 T,T = 2 T,T = 1 T,T = 69 T,andthendefine(sub-)phasesasfollows:
1 100 2 10 3 10 4 100
• Phase1: T ={t:1≤t≤T },
1 1
• Phase2.1: T ={t:T +1≤t≤T +T },
2 1 1 2
• Phase2.2: T ={t:T +T +1≤t≤T +T +T },
3 1 2 1 2 3
• Phase2.3: T ={t:T +T +T +1≤t≤T +T +T +T }.
4 1 2 3 1 2 3 4
WerecallthedefinitionsofM andT′aswell.
Definition14. WedefineM andT′asfollows
 
1  (cid:18) 2K(cid:19) ηK 
M := ln +2(1+ε )(1+ )ηT  (15)
ln2 γ 1 γ 1
 
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
∝(lnT) ∝(ηT1)
1 4 2
T′ := ( ) M (16)
1−ε 3−γ η
2
(cid:113) (cid:113)
whereε = 6lnT andε = 4lnT .
1 2 KγT1 2 3− 4γT2
Next,werestatetwoeventsE andE .
1 2
Definition22. Let
E ={π ≥2−M}
1 T1+1,1
betheeventthatarm1’sprobabilityattheendofPhase1isnottoosmall,whereM isdefinedin(15).
Definition23. LetE = {π ≥ 1}betheeventthatarm1’sprobabilityattheendofPhase2.2has
2 T1+T2+1,1 4
recoveredto 1.
4
Next,wehavethefollowinglemmastatingthat,withhighprobability,π isnottoosmall.
T1+1,1
Lemma24. WhenwerunWSU-UXwithanyvalidparametersη,γonspecificlosssequence{ℓ }T ,forany
t t=1
(cid:114)
3ln1
(ε,δ),whereε= δ ∈(0,1],attheendofphase1,wehavethatwithprobabilityatleast1−δ,
2 KγT1
π ≥2−M,
T1+1,1
21(cid:16) (cid:17)
whereM = 1 2(1+ε)(1+ ηK)ηT +ln2K . Inparticular, forlargeenoughT, bychoosingδ = 1 ,
ln2 γ 1 γ T2
wegetπ ≥2−M whereM isdefinedin(15).
T1+1,1
We useda recentlydeveloped multiplicative formof Azuma’s inequalityfor martingales(Kuszmaul and
Qi,2021)toshowLemma24. ThislemmashowsthatwhenT islargeenough,withhighprobability,π
T1+1,1
doesnotbecometoosmall, i.e., eventE happens. Next, wewillshowthatπ recoversto1/4with
1 T1+T2+1,1
high probability. To show this, observe that Phase 2.1 is the phase where π can start to recover. By each
t,1
pull of arm 2 in Phase 2.1, the probability π increases, whereas by pulls of arm 1, π does not change.
t,1 t,1
Hence,wefirstfindanupperboundonthenumberofpullsofarm2neededsothatπ recoversto1/4. Atthe
t,1
beginningofPhase2.1,aftereachpullofarm2,therateofupdate πt+1,1 ≈1+εisverycloseto1. Therefore,
πt,1
wefirstfocusonfindinganupperboundonthenumberofpullsofarm2neededforπ todouble.
t,1
Lemma25. Consideraroundt >T ,where0<π ≤ 1. Ifarm2ispulledmtimesinroundst ,...,t
0 1 t0,1 4 1 m
wheret ≤t <t <...<t ≤T andm≥ 2 · 1 ,thenwehaveπ ≥2π .
0 1 2 m η 1−2πt0,1 tm+1,1 t0,1
Next,Lemma26isacumulativeversionofLemma25whereweshowanupperboundonthetotalnumber
ofpullsofarm2neededsothatπ doublesforM −2times.
t,1
Lemma26. Consideraroundt>T wherewehave2−m ≤π ≤2−(m−1),wherem∈Z . Thenifarm2
1 t,1 +
ispulledk = 2 mtimesinroundst,t+1,...,T ,androundt′denotestheroundjustafterthekthpull,then
η 1
wehaveπ ≥ 1.
t′,1 4
Lemma26indicatesthatgiveneventE happened,k = 2 M pullsofarm2sufficetoensurethatπ ≥
1 η t,1
1/4. The next lemma shows that given event E happened, then with high probability, arm 2 is going to be
1
pickedinT roundsatleastktimes. ThisalongwithLemma26impliesthatπ recovers.
2 T1+T2+1,1
Lemma27. WhenT islargeenough,withprobabilityatleast1−2 1 ,eventE happens,i.e.,π ≥
T2 2 T1+T2+1,1
1/4.
WenextshowthatgivenE ,theexpectationofπ goesto0exponentiallyquicklyastincrementsbeyond
2 t,2
T +T .
1 2
Lemma 28. Assume event E happens. Define t = T +T +1, and 1 ≤ τ ≤ T +T , and time step
2 0 1 2 3 4
t=t +τ. Thenwehave
0
3 η
E[π |E ]≤ exp(− τ).
t,2 2 4 4
Now, conditional on event E , by using Chebyshev’s inequality and Lemma 28, we show that it is very
2
unlikelythatπ isconstantlysmallerthan1.
T,1
Lemma29. ConditionaloneventE definedinDefinition23,wehave
2
(cid:18) (cid:12) (cid:19)
Pr π T+1,1 ≤
43(cid:12)
(cid:12) (cid:12)E 2 ≤
7 45
e−cηT.
UsingconditionalexpectationandLemma29,weproveClaim1inthenextsubsection.
Next,weintroducethefollowinglemma.
Lemma30. ForlargeenoughT,wehaveE[π ]≤ 1 forT +T +T <t≤T.
t,2 4 1 2 3
WeproveClaim3usingLemma30inAppendixB.3.
22B.3 CompleteproofofClaims1and3
B.3.1 HighprobabilitylemmaforeventE
1
ProofofLemma24. Wedefineτ tobethelargesttin1≤t≤T suchthatπ > γ. Sinceπ arerandom
1 t,1 K t,1
variablesthatdependoninternalrandombitsofthealgorithm,τ isalsoarandomvariable. Observethatfor
thelosssequence{ℓ }T weareconsidering,forall1≤t≤T +1wehaveπ ≥π . Thisimpliesthat
t t=1 1 t,1 t+1,1
γ
π > ∀t:1≤t≤τ (24)
t,1 K
γ
π ≤ ∀t:τ +1≤t≤T . (25)
t,1 K 1
Usingτ,wecanexpressπ asfollows8:
T1+1,1
π =π
(cid:89)T1
π
s+1,1
T1+1,1 1,1 π
s,1
s=1
=(cid:32)
π
τ (cid:89)−1
π
s+1,1(cid:33)(cid:18)
π
τ+1,1(cid:19)(cid:32) (cid:89)T1
π
s+1,1(cid:33)
. (26)
1,1 π π π
s,1 τ,1 s,1
s=1 s=τ+1
(cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
firstterm secondterm thirdterm
Clearly,thefirsttermin(26)canbelowerboundedas
τ−1
π (cid:89) π s+1,1 =π > γ , (27)
1,1 π τ−1,1 K
s,1
s=1
where we used (24). Next, observe that for rounds 1 ≤ t ≤ T , in phase one, we can simply lower bound
1
πs+1,1 asfollows:
πs,1
π s+1,1 =1−η(cid:16) ℓˆ − (cid:88) π ℓˆ (cid:17)
π s,1 s,j s,j
s,1
j∈[2]
1−π
=1−η s,1 1[I =1]
(1−γ)π + γ s
s,1 K
1−0
≥1−η 1[I =1] (π ≥0)
0+ γ s s,1
K
ηK
=1− 1[I =1]
γ s
ηK
=(1−
)1[Is=1].
(28)
γ
Therefore,thesecondtermcanbelowerboundedby
π ηK
τ+1,1 ≥(1− )1[Iτ=1] ≥1/2, (29)
π γ
τ,1
(cid:124) (cid:123)(cid:122) (cid:125)
secondterm
since ηK ≤1/2inWSU-UX.Moreover,wehave
γ
(cid:89)T1
π
s+1,1 ≥
(cid:89)T1
(1−
ηK
)1[Is=1] (by28)
π γ
s,1
s=τ+1 s=τ+1
(cid:124) (cid:123)(cid:122) (cid:125)
thirdterm
=(1− ηK )(cid:80)T s=1 τ+11[Is=1]. (30)
γ
8Forconvention,theproductoveranemptysetisassumedtobe1. e.g. ifτ = 1,thenthefirsttermintheright-handsideof(26)
whichisaproductovertheemptysetisassumedtobe1.Similarly,ifτ =T1,thethirdtermintheright-handsideof(26)isassumedto
be1.
23Therefore,byplugging(27),(29),and(30)intotherighthandsideof(26)weobtain
π
T1+1,1
≥ 2γ K(1− η γK )(cid:80)T s=1 τ+11[Is=1], (31)
where the right-hand side is a random variable that depends on τ. Note that 1[τ =t′|F ] is measurable,
t′
meaningthatinroundt′,givenaccesstothepasthistory,wecandeterministicallytellwhetherτ = t′ ornot.
Next,wewillshowthatforanypossiblevaluet′ thatτ cantake,andparticularhistoryF upuntiltheendof
t′
roundt′forwhichτ =t′,9thereisasuitableupperboundonthefollowingterm
(cid:88)T1
1[I =1|F ] (32)
s t′
s=t′+1
thatholdswithprobabilityatleast1−δ. Thisimpliesauniformhighprobabilitylowerboundon(31),which
completestheproof.
It remains to show this uniform upper bound for (32). In order to show this, we set up a martingale. In
particular,wedefineI := 1[I = i]. Consideranyfixedhistoryupuntilroundt′ denotedbyF suchthat
t,i t t′
τ =t′. Thenforanytwheret′+1≤t≤T ,wehave
1
(cid:104) γ (cid:12) (cid:105)
E[I |F ]=E (1−γ)π + (cid:12)F (UniformExplorationbyWSU-UX)
t,1 t−1 t,1 K (cid:12) t−1
(cid:104) γ (cid:12) (cid:105)
≤E π + (cid:12)F
t,1 K (cid:12) t−1
2γ
≤ =:q, (from25) (33)
K
where the last inequality comes from (24). Define Z := 0 and for any t where t′ + 1 ≤ t ≤ T , let
t′ 1
W :=I −E[I |F ]andZ :=(cid:80)t W . Observethatfortint′+1≤t≤T ,wehave
t t,1 t,1 t−1 t s=t′+1 s 1
E[Z |F ]=Z +E[W |F ]=Z ;
t t−1 t−1 t t−1 t−1
therefore(Z ) isamartingale. Moreover, fortint′ +1 ≤ t ≤ T , wehaveZ −Z = W ∈
t t∈{t′,...,T1} 1 t t−1 t
[−A ,B ]forA =E[I |F ]andB =1−A ,simplybecauseI ∈{0,1}. Observethatwehave
t t t t,1 t−1 t t t,1
(cid:88)T1 (cid:88)T1 (cid:88)T1 (cid:88)T1
2γ
A = E[I |F ]≤ q ≤ q =qT = T =:µ,
t t,1 t−1 1 K 1
t=t′+1 t=t′+1 t=t′+1 t=1
wherethefirstinequalitycomesfrom(33). Wedefinec:=A +B =1andapplyTheorem10ofKuszmaul
t t
andQi(2021)togetthatforallε>0,
(cid:18) ε2µ (cid:19)
Pr(Z −Z ≥εµ|F )≤exp − .
T1 t′ t′ (2+ε)c
Usingourdefinitionofthemartingalesequence,andnotingthatc=1,weget
(cid:32) (cid:88)T1 (cid:88)T1 (cid:12) (cid:12) (cid:33) (cid:18) µε2 (cid:19)
Pr I ≥ E[I |F ]+εµ(cid:12)F ≤exp − .
t,1 t,1 t−1 (cid:12) t′ 2+ε
(cid:12)
t=t′+1 t=t′+1
Using(cid:80)T1 E[I |F ]≤µandbyimposingtherestrictionε≤1,wehaveforε∈(0,1],
t=t′+1 t,1 t−1
(cid:32) (cid:88)T1 (cid:12) (cid:12) (cid:33) (cid:18) µε2(cid:19)
Pr I ≥(1+ε)µ(cid:12)F ≤exp − .
t,1 (cid:12) t′ 3
(cid:12)
t=t′+1
9Observethatthist′iswelldefinedsinceF t′−1,thehistoryupuntilroundt′−1,isenoughtodeterminewhetherτ =t′orτ ̸=t′.
24(cid:114)
3ln1
Equivalently,conditionalonF t′,withprobabilityatleast1−δ,whereε=
2
KγTδ
1
∈(0,1],wehave
(cid:88)T1
I ≤(1+ε)µ
t,1
t=t′+1
2γ
=(1+ε) T . (34)
K 1
Thisisthesuitableupperboundwewantedforthequantityin(32). Inparticular,wehave
(1−
ηK )(cid:104)(cid:80)T t=1 t′+1It(cid:105) ≥(cid:16) e(−η γK)(1+η
γK)(cid:17)(cid:104)(cid:80)T
t=1
t′+1It(cid:105)
(1−x≥e(−x−x2)for0<x≤ 1
)
γ 2
≥(cid:16)
e(−η γK)(1+η
γK)(cid:17)(1+ε)2 KγT1
from(34)
(cid:32) (cid:33)
ηK
=exp −2(1+ε)(1+ )ηT . (35)
γ 1
Combiningthislowerboundwith(31),wegetthefollowingstatement.
(cid:114)
For any possible value for τ and any fixed F
t′
satisying τ = t′, for any ε = 3
2
Kγln Tδ1
1
∈ (0,1], with
probabilityatleast1−δ,wehave
(cid:32) (cid:33)
π =
γ
(1−
ηK )(cid:104)(cid:80)T s=1 τ+11[Is=1](cid:105)
≥
γ
exp −2(1+ε)(1+
ηK
)ηT
T1+1,1 2K γ 2K γ 1
(cid:32) (cid:18) (cid:19)(cid:33)
ηK 2K
=exp − 2(1+ε)(1+ )ηT +ln
γ 1 γ
=2−M. (36)
Since(36)holdstrueforanypossiblevalueofτ andanyfixedF whereτ =t′,itholdstrueingeneral.
t′
(cid:113)
Moreover,notethatwhenT islargeenough,wecanchooseδ = 1 sinceε = 6lnT ∈ (0,1]forlarge
T2 2 KγT1
enoughT. Asaresult,weget
π ≥2−M,
T1+1,1
whereM isdefinedin(15).
25B.3.2 Upperboundonthenumberofpullsofarm2
ProofofLemma25. Wecanlowerboundπ as
tm+1,1
π =1−π =1−π
(cid:89)tm
π
s+1,1
tm+1,1 tm+1,2 t0,2 π
s,1
s=t0
=1−π
(cid:89)tm
(cid:0) 1−η 1−π s,2 1[I =2](cid:1)
t0,2 (1−γ)π + γ s
s=t0 s,2 K
≥1−π
(cid:89)tm
(cid:0) 1−ηπ s,11[I =2](cid:1) (cid:0) (1−γ)π + γ ≤1(cid:1)
t0,2 1 s s,2 K
s=t0
≥1−π
(cid:89)tm
(cid:0) 1−ηπ 1[I =2](cid:1) (cid:0) π ≥π (cid:1)
t0,2 t0,1 s s,1 t0,1
s=t0
=1−π
(cid:89)tm
(cid:0)
1−ηπ
(cid:1)1[Is=2]
t0,2 t0,1
s=t0
(cid:0) (cid:1)m
=1−π 1−ηπ
t0,2 t0,1
(cid:0) (cid:1)m
≥1− 1−ηπ . (π ≤1)
t0,1 t0,2
Moreover,
(1−ηπ )m ≤exp(−ηπ m) (1−x≤exp(x),∀x∈R)
t0,1 t0,1
(cid:18) (cid:19)
−2π 2
≤exp t0,1 (m≥ )
1−2π η(1−2π )
t0,1 t0,1
1
−2x
≤1−2π t0,1, (e1−2x ≤1−2x,∀x∈(0, 2])
whichmeans
(cid:0) (cid:1)m
π ≥1− 1−ηπ ≥2π .
tm+1,1 t0,1 t0,1
ProofofLemma26. Notethatifm≤2,thenwealreadyhaveπ ≥ 1,andsincet≥T ,anypullsofarm2
t,1 4 1
onlyincreaseπ . Hence,afterkpullswehaveπ ≥π ≥ 1.
t,1 t′,1 t,1 4
Considerthecasewherem≥3. Wehave2−m ≤π ≤2m−1. Nowwewanttoupperboundthenumber
t,1
ofpullsittakessothat1/4≤π ≤1/2. Supposewerequirek pullsforthefirstdoublingofπ,k forthe
t′,1 1 2
seconddoubling, andsoforth. Thismeansweneedk =
(cid:80)m−2k
pullsbeforeweget1/4 ≤ π ≤ 1/2.
i=1 i t′,1
Next,weupperboundeachk . Todothis,wedenotealltheroundsaftert,inwhichwepullarm2asfollows
i
t(1),t(1),...,t(1) t(2),...,t(2) ... t(i),t(i),...t(i) ... t(m2−2),...,t(m2−2) , t
(cid:124)1 2
(cid:123)(cid:122)
k1
(cid:125)
(cid:124)1
(cid:123)(cid:122)
k2
(cid:125)
(cid:124)1 2
(cid:123)(cid:122)
ki
(cid:125)
(cid:124)1
(cid:123)(cid:122)
km2−2
(cid:125)
(cid:124)(cid:123)k
(cid:122)(cid:125)
roundsbefore1stdoublingroundsbefore2nddoubling roundsbeforeithdoubling roundsbefore(m−2)doubling 1/4≤πtk,1
Nowwecanupperboundeachk asfollows:
i
2 1
k ≤ (Lemma25)
i η1−2π(i)
t1
2 1
≤ . (π(i) ≤2−m+i)
η1−2−m+i+1 t1
Toseewhythefirstinequalityholds,observethatwestartfromroundt (i),andLemma25hasanupperbound
1
onthenumberofpullsneededtogetdoubled.
26Therefore,weget
m−2 m−2 m−2
(cid:88) (cid:88) 2 1 2 (cid:88) 1
k ≤ = . (37)
i η1−2−m+i+1 η 1−2−i
i=1 i=1 i=1
Now,observethat
1 1
=1+
1−2−i 2i−1
1
≤1+ . (2i−1≥2i−1fori≥1)
2i−1
Therefore,weget
m−2 m−2(cid:18) (cid:19)
(cid:88) 2 (cid:88) 1
k ≤ 1+
i η 2i−1
i=1 i=1
(cid:34) m−2 (cid:35)
2 (cid:88) 1
≤ m−2+
η 2i
i=0
2
≤ [m−2+2] (geometricseries)
η
2
= m.
η
B.3.3 HighprobabilitylemmaforeventE
2
ProofofLemma27. RecallthedefinitionofE fromDefinition22andE fromDefinition23asfollows:
1 2
E
=(cid:8)
π
≥2−M(cid:9)
1 T1+1,1
(cid:26) (cid:27)
1
E = π ≥ ,
2 T1+T2+1,1 4
(cid:16) (cid:17)
whereM = 1 2(1+ε )(1+ ηK)ηT +ln2K . Wethenshowthefollowingtwostatements.
ln2 1 γ 1 γ
(a) Forδ = 1 ,weprovethatwithprobabilityatleast1−δ ,E happens,i.e.,Pr(E )≥1−δ
T2 1 1 1 1
(b) Given E happened, for δ = 1 , we prove that with probability at least 1 − δ , E happens, i.e.,
1 2 T2 2 2
Pr(E |E )≥1−δ .
2 1 2
Havingboth(a)and(b)impliesthatthelemmaholdstrue,as
Pr(E )≥Pr(E andE )=Pr(E ) Pr(E |E )=(1−δ )(1−δ )
2 1 2 1 2 1 1 2
2
≥1−δ −δ =1− .
1 2 T2
Now,weprove(a)and(b).
Proofof(a) ByLemma24,withprobabilityatleast1− 1 ,wehave
T2
π ≥2−M,
T1+1,1
(cid:16) (cid:17)
forM = 1 2(1+ε )(1+ ηK)ηT +ln2K .
ln2 1 γ 1 γ
27Proofof(b) WeshowthatforanyhistoryF suchthatE happened,wehave
T1 1
Pr(E |F )≥1−δ . (38)
2 T1 2
ThisimpliesthatPr(E |E )≥1−δ .
2 1 2
ConsiderafixedhistoryF suchthatE happened. EventE impliesthatforsomeM′ ≤M,wehave
T1 1 1
2−M′
≤π
≤2−(M′−1).
T1+1,1
Now,Lemma26statesthatΓ= 2M′pullsissufficienttoget
η
1
π ≥ , (39)
t′,1
4
wheret′isroundnumberafterΓ-thpull. Wedefine
(cid:20) (cid:21)
1
X :=1 I =2orπ ≥ .
t t t,1 4
Next,observethatif
(cid:88)
T (cid:88)1+T2
2
X = X ≥ M′, (40)
t t η
t∈phase2.1 t=T1+1
then this implies that π ≥ 1/4, (i.e. E happens.) To see why, note that if for any for t in
T1+T2+1 2
T +1≤t≤T +T ,wehaveπ ≥1/4,thisimpliesπ ≥1/4sinceπ canonlyincrease
1 1 2 t,1 T1+T2+1 t,1
inphase2.1. IfforalltinT +1≤t≤T +T ,wehaveπ <1/4,then(40)impliesthat
1 1 2 t,1
T (cid:88)1+T2 T (cid:88)1+T2
2
X = 1[I =2]≥ M′.
t t γ
t=T1+1 t=T1+1
Therefore,Lemma26impliesthatπ ≥1/4.
T1+T2+1
Now it remains to show that with probability at least 1 − 1 , (40) happens. We use a martingale
T2
concentrationargumenttoshowthis. Indeed,wedefineZ =0andforanytinT +1≤t≤T +T ,
wedefineW :=X −E[X |F ]andZ :=(cid:80)t
T1
W . Observethat
1 1 2
t t t t−1 t s=T1+1 s
E[Z |F ]=Z +E[W |F ]=Z ,
t t−1 t−1 t t−1 t−1
andhence(Z ) isamartingale. SinceX ∈ {0,1},wehaveZ −Z = W ∈ [−A ,B ]
t t∈{T1,...,T} t t t−1 t t t
for A = E[X |F ] and B = 1−A . Consequently, we have A +B = 1 := c for all t ≥ t′.
t t t−1 t t t t
Defineq := 3−γ.
4
Observe that we have E[X |F ] ≥ q. It is because for any F such that π ≥ 1/4, we have
t t−1 t−1 t,1
E[X |F ] = E(cid:2)1(cid:2) I =2orπ ≥ 1(cid:3) |F (cid:3) = 1 > q. Moreover,foranyF suchthatπ <
t t−1 t t,1 4 t−1 t−1 t,1
1/4,wehaveπ ≥3/4andhenceE[1[I =2]|F ]=π˜ =(1−γ)π + γ ≥ 3−γ ,therefore
t,2 t t−1 t,2 t,2 2 4
(cid:20) (cid:12) (cid:21)
E[X t |F t−1]=E 1[I t =2] or1[π t,1 ≥ 1 4](cid:12) (cid:12) (cid:12)F t−1 =E[1[I t =2]|F t−1]≥ 3− 4 γ .
Notethatwehave
T (cid:88)1+T2 T (cid:88)1+T2 T (cid:88)1+T2
A = E[X |F ]≤ q =qT =:µ.
t t t−1 2
t=T1+1 t=T1+1 t=T1+1
28WenowapplyTheorem15fromKuszmaulandQi(2021)togetforanyε>0,
(cid:18) ε2µ(cid:19)
Pr(Z −Z ≤−εµ|F )≤exp −
T1+T2 T1 T1 2c
forF whereE holds.
T1 1
Plugginginoursettingofcandusingourdefinitionofthemartingalesequencegives
(cid:32)T (cid:88)1+T2 T (cid:88)1+T2 (cid:12) (cid:12) (cid:33) (cid:18) µε2(cid:19)
Pr X ≤ E[X |F ]−εqT (cid:12)F ≤exp − .
t t t−1 2(cid:12) T1 2
(cid:12)
t=T1+1 t=T1+1
Using(cid:80)T1+T2 E[X |F ]≤µ,wehaveallε>0,
t=T1+1 t t−1
(cid:32)T (cid:88)1+T2 (cid:12) (cid:12) (cid:33) (cid:18) µε2(cid:19)
Pr X ≤(1−ε)µ(cid:12)F ≤exp − .
t (cid:12) T1 2
(cid:12)
t=T1+1
Thisimpliesthat,foranygivenF suchthatE holds,withprobabilityatleast1− 1 ,wehave
T1 1 T2
T (cid:88)1+T2
3−γ
X ≥µ= (1−ε )T ,
t 4 2 2
t=T1+1
(cid:113)
where ε = 4lnT . Now, recall T′ from Definition 14. For large enough T, by (17), we have
2 3− 4γT2
T ≥T′. Therefore,
2
3−γ 3−γ
( )(1−ε )T ≥( )(1−ε )T′.
4 2 2 4 2
AlsobydefinitionofT′,weget
3−γ 2
(1−ε )T′ =M .
4 2 η
FinallybydefinitionofM′,wehave
2 2
M ≥M′ ,
η η
whichmeanswithprobabilityatleast1− 1 ,(40)happens.
T2
B.3.4 ProofofLemmas28and29
WefirstproveLemma28.
ProofofLemma28. Consider any round t where t ≥ t + 1. Consider any history F where event E
0 t−1 2
happened. ByapplyingLemma12fori=2,weget
E[π |F ]=(1−η)π +ηπ2 .
t+1,2 t−1 t,2 t,2
Now, note that since E happened we have π ≥ 1/4. Since π = π can only increase, we have
2 t0,1 t,1 t0+τ
π ≥1/4. Thisimpliesπ ≤3/4,therefore
t,1 t,2
3
π2 ≤ π .
t,2 4 t,2
29Therefore,weget
3
E[π |F ]≤(1−η)π + ηπ
t+1,2 t−1 t,2 4 t,2
η
≤(1− )π . (41)
4 t,2
WenowcanshowanupperboundonE[π |E ]bynotingthat
t+1,2 2
E[π |E ]=E[E[π |E ,F ]|E ]
t+1,2 2 t+1,2 2 t−1 2
=E[E[π |F ]|E ].
t+1,2 t−1 2
Thismeansthatwecantaketheconditionalexpectationonbothsidesof(41)toget
η
E[π |E ]≤(1− )E[π |E ]. (42)
t+1,2 2 4 t,2 2
Moreover,bydefinitionofE wehaveE[π |E ]=π ≤3/4. Therefore,weget
2 t0,2 2 t0,2
E[π |E ]=E[π |E
]t0+ (cid:89)τ−1E[π
s+1,2
|E 2]
t,2 2 t0,2 2 E[π |E ]
s,2 2
s=t0
t0+ (cid:89)τ−1
η
≤E[π |E ] (1− ) by(42)
t0,2 2 4
s=t0
3 η
≤ (1− )τ
4 4
3 η
≤ exp(− τ). (1−x≤e−x)
4 4
WenowproveLemma29.
ProofofLemma29. Letτ =T +T =cT forc>0. DefinerandomvariableX :=π ∈[0,1]. Clearly
3 4 T+1,1
Lemma 28 implies that E[X|E 2] = E[π T+1,1|E 2] = 1−E[π T+1,2|E 2] ≥ 1− 3 4e−η 4cT. Therefore, using
30Chebyshev’sinequality,weget
(cid:18) (cid:12) (cid:19) (cid:18) (cid:12) (cid:19)
Pr X ≤ 3 4(cid:12) (cid:12) (cid:12)E 2 =Pr X−E[X|E 2]+E[X|E 2]≤ 43(cid:12) (cid:12) (cid:12)E 2
(cid:18) (cid:12) (cid:19)
=Pr X−E[X|E 2]≤ 3 4 −E[X|E 2](cid:12) (cid:12) (cid:12)E 2
(cid:18) (cid:18) (cid:19)(cid:12) (cid:19)
≤Pr X−E[X|E 2]≤ 43 − 1− 43 e−η 4cT (cid:12) (cid:12) (cid:12)E 2 (E[X|E 2]≥1− 43 e−η 4cT)
(cid:18) (cid:12) (cid:19)
=Pr X−E[X|E 2]≤ 3 4e−η 4cT − 1 4(cid:12) (cid:12) (cid:12)E 2
(cid:18) (cid:12) (cid:19)
≤Pr X−E[X|E 2]≤−1 5(cid:12) (cid:12) (cid:12)E 2 (forlargeT wehave3 4e− 4cηT ≤ 21 0)
(cid:18) (cid:12) (cid:19)
≤Pr
(cid:12) (cid:12)X−E(cid:2)
X|E
2(cid:3)(cid:12)
(cid:12)≥
1 5(cid:12)
(cid:12) (cid:12)E 2
≤25Var(X|E ) (Chebyshevinequality)
2
=25(cid:16) E(cid:2) X2|E (cid:3) −E[X|E ]2(cid:17)
2 2
(cid:16) (cid:17)
≤25 E[X|E ]−E[X|E ]2 (E[X]≥E[X2]forX ∈[0,1])
2 2
=25E[X|E ](1−E[X|E ])
2 2
≤25(cid:0) 1−E(cid:2)
X|E
(cid:3)(cid:1) (E(cid:2)
X|E
(cid:3)
≤1)
2 2
75
≤ e−cη 4T. (fromLemma28)
4
B.3.5 ProofofClaim1
Now,wearereadytoproveClaim1. WerecallClaim1.
Claim1(Concentrationonbestarmattheend). ForlargeenoughT,thereexistsc >0suchthat
1
E[lnπ +lnK]≥c . (11)
T+1,1 1
First,wehavethefollowingsimpleobservation.
Observation31. WhenrunningWSU-UXwithanyvalidhyperparameterη,γ onthelosssequence{ℓ }T
t t=1
definedinDefinition9,wehavewithprobability1,that
(cid:18) 1(cid:19) 1T 00+1
π ≥ .
T+1,1 2
ProofofObservation31. Theprobabilityofπ canonlydecreaseinthefirst T roundsandonlyifarm1is
t,1 100
pulledinthoserounds. Itiseasytoseethatthevaluedropsbyatmostafactorof2eachtimeitispulledas
for1≤s≤T wehave
1
π s+1,1 =1−η(cid:16) ℓˆ − (cid:88) π ℓˆ (cid:17) ≥1− ηK 1[I =1]≥1/2.
π s,1 s,j s,j γ s
s,1
j∈[2]
31ProofofClaim1. DefineeventA:=1[π ≥3/4]. Usingconditionalexpectation,wehave
T+1,1
E[lnπ |E ]=Pr(A|E )E[lnπ |E ,A]+Pr(Ac |E )E[lnπ |E ,Ac]
T+1,1 2 2 T+1,1 2 2 T+1,1 2
(cid:18) (cid:19) (cid:18) (cid:19)
75 75
≥ 1−
4
e−c 4ηT E[lnπ T+1,1|E 2,A]+
4
e−c 4ηT E[lnπ T+1,1|E 2,Ac],
wheretheinequalitycomesfromLemma29. Thiscanbefurtherlowerboundedby
(cid:18) (cid:19) (cid:18) (cid:19)
75 3 75
1−
4
e−c 4ηT ln
4
+
4
e−c 4ηT E[lnπ T+1,1|E 2,Ac]
(cid:18) (cid:19) (cid:18) (cid:19)
75 3 75
≥ 1−
4
e−c 4ηT
ln
4
+
4
e−c 4ηT
min[lnπ T+1,1]
(cid:18) (cid:19) (cid:18) (cid:19)(cid:18) (cid:19)
75 3 75 T 1
≥ 1−
e−c 4ηT
ln +
e−c 4ηT
+1 ln (Observation31)
4 4 4 100 2
(cid:124) (cid:123)(cid:122) (cid:125)
secondterm
11
≥ln . secondterm →0asT→∞
16
Therefore,wehave
11
E[lnπ |E ]≥ln . (43)
T+1,1 2 16
Now,wecanlowerboundE[lnπ ]usingconditionalexpectation:
T+1,1
E[lnπ ]=Pr(E )E[lnπ |E ]+Pr(Ec)E[lnπ |Ec]
T+1,1 2 T+1,1 2 2 T+1,1 2
≥(1−2δ) E[lnπ |E ]+(2δ)E[lnπ |Ec]
T+1,1 2 T+1,1 2
≥(1−2δ)E[lnπ |E ]+(2δ) [minlnπ ]
T+1,1 2 T+1,1
11
≥(1−2δ)(ln )+(2δ)[minlnπ ] (Byinequality43)
16 T+1,1
11 T 1
≥(1−2δ)(ln )+(2δ)( +1)ln (Observation31)
16 100 2
11 T 1
≥(ln )+(2δ)( +1)ln
16 100 2
(cid:124) (cid:123)(cid:122) (cid:125)
secondterm
5 1
≥(ln ). δ = ,thereforesecondterm→0asT→∞
8 T2
Therefore,weget
5
E[lnπ +lnK]≥(ln )+lnK
T+1,1 8
5
=ln . (K =2)
4
Therefore,forc =ln5 >0,
1 4
E[lnπ +lnK]≥c .
T+1,1 1
B.3.6 ProofofClaim3
WefirstproveLemma30andthenweproveClaim3.
32ProofofLemma30. Sett = 31T = T +T +T +1. Wecanuseconditionalexpectationforπ onevent
100 1 2 3 2,t
E definedinDefinition23toget
2
E[π ]=Pr(E )E[π |E ]+Pr(Ec)E[π |Ec]
t,2 2 t,2 2 2 t,2 2
≤E[π |E ]+Pr(Ec)·1. (π ≤1)
t,2 2 2 t,2
Now, by setting τ = T in Lemma 28, one would get E[π |E ] ≤ 3exp(−ηT ) = 3exp(−cηT) for
3 t,2 2 4 4 3 4 4
c = 1 > 0. Moreover, by Lemma 27 we have that Pr(Ec) ≤ 2 . Therefore, for large enough T, we can
10 2 T2
furtherupperboundE[π ]by
t,2
3 2 1
E[π
t,2
|E 2]+Pr(E 2c)·1≤ 4e−c 4ηT +
T2
≤ 4.
ProofofClaim3. Byexpandingπ˜ =π (1−γ)+ γ,weget
t,j t,j 2
   
2 2 2 2 2
(cid:88) π˜ t,jℓˆ
t,j
−ℓˆ t,1=(cid:88)(cid:16) π t,j(1−γ)+ γ 2(cid:17) ℓˆ
t,j
−ℓˆ
t,1
=(cid:88) π t,jℓˆ
t,j
−ℓˆ t,1−(cid:88) γπ t,jℓˆ
t,j
+(cid:88)γ 2ℓˆ
t,j
j=1 j=1 j=1 j=1 j=1
Takingtheexpectationfrombothsides,weget
     
2 2 2 2
E (cid:88) π˜ t,jℓˆ t,j −ℓˆ t,1=E (cid:88) π t,jℓˆ t,j −ℓˆ t,1+−(cid:88) E(cid:104) γπ t,jℓˆ t,j(cid:105) +(cid:88) E(cid:104)γ 2ℓˆ t,j(cid:105) 
j=1 j=1 j=1 j=1
 
2 2 2
=E (cid:88) π t,jℓˆ
t,j
−ℓˆ t,1−(cid:88) E(cid:104) γπ t,jE t−1(cid:104) ℓˆ t,j(cid:105)(cid:105) +(cid:88) E(cid:104)γ
2
E t−1(cid:104) ℓˆ t,j(cid:105)(cid:105)
j=1 j=1 j=1
   
2 2 2
=E (cid:88) π t,jℓˆ
t,j
−ℓˆ t,1+−(cid:88) E[γπ t,jℓ t,j]+(cid:88)γ 2ℓ t,j.
j=1 j=1 j=1
SummingoverT rounds,weget
       
T 2 T 2 T 2 T 2
E (cid:88) (cid:88) π˜ t,jℓˆ
t,j
−ℓˆ t,1=E (cid:88) (cid:88) π t,jℓˆ
t,j
−ℓˆ t,1+−(cid:88)(cid:88) E[γπ t,jℓ t,j]+(cid:88)(cid:88)γ 2ℓ t,j,
t=1 j=1 t=1 j=1 t=1j=1 t=1j=1
(cid:124) (cid:123)(cid:122) (cid:125)
∆
wherewedefine
T 2 T 2
(cid:88)(cid:88) (cid:88)(cid:88)γ
∆:=− E[γπ ℓ ]+ ℓ . (44)
t,j t,j 2 t,j
t=1j=1 t=1j=1
NotethattoproveClaim3,weneedtoshowthatforlargeenoughT,
∆≥c γT (45)
3
holdstrue.
Notethatinlosssequence{ℓ }T forallroundst,wehaveℓ +ℓ =1;therefore
t t=1 t,1 t,2
T 2
(cid:88)(cid:88)γ γT
ℓ = . (46)
2 t,j 2
t=1j=1
33Moreover,forlargeenoughT wehave
T 2
(cid:88)(cid:88) (cid:88) (cid:88) (cid:88)
E[γπ ℓ ]= E[γπ ]+ E[γπ ]+ E[γπ ]
t,j t,j t,1 t,2 t,2
t=1j=1 t∈T1 t∈T2∪T3 t∈T4
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
phase1 phase2.1andphase2.2 phase2.3
γ (cid:88) (cid:88)
≤T + E[γπ ]+ E[γπ ] (π ≤1/2∈T )
12 t,2 t,2 t,1 1
t∈T2∪T3 t∈T4
γ (cid:88)
≤T +γ(T +T )+ E[γπ ] (π ≤1)
12 2 3 t,2 t,2
t∈T4
γ γ
≤T +γ(T +T )+T , (∀t∈T ,E[π ]≤1/4whenT islarge)
12 2 3 4 4 4 t,2
(47)
wherethefirstinequalitycomesfromthefactthatπ ≤ 1/2when1 ≤ t ≤ T . Thethirdinequalitycomes
t,1 1
fromthefactthatbyLemma30,forlargeenoughT,wehaveE[π ]≤ 1 whent∈T .
t,2 4 4
Byusing(46)and(47),wehave
T 2 T 2
(cid:88)(cid:88) (cid:88)(cid:88)γ (cid:16) γ γ(cid:17) γ
∆=− E[γπ ℓ ]+ ℓ ≥− T +(T +T )γ+T +T
t,j t,j 2 t,j 12 2 3 4 4 2
t=1j=1 t=1j=1
1 γ 3 69 γ γ
=−T ( + γ+ )+T
1002 10 1004 2
9
= γT =c γT,
400 3
forlargeenoughT,i.e.,(45)holds. Thisprovestheclaim.
C Potential analysis
C.1 WSUasalinearapproximationofHedgeupdate
Asmentionedinthemaintext,theWSUupdatecanbeviewedasalinearapproximationtotheHedgeupdate.
Inthissection,webrieflyshowthisapproximationargument.
Observethatbyapplyingthelinearapproximationf(x)≈f(x )+f′(x )(x−x )forf(x)=exp(−x)
0 0 0
andforx=ηℓ andx =ηℓ¯,whereℓ¯ :=(cid:80) π ℓ ,weget
t,i 0 t t j t,j t,j
exp(−ηℓ
)≈exp(cid:0) −ηℓ¯(cid:1) ·(cid:0) 1−η(cid:0)
ℓ
−ℓ¯(cid:1)(cid:1)
. (48)
t,i t t,i t
NotethatHedgeupdatesweightsbytheLHSof(48). Now,ifweinsteadupdatetheweightsbyRHSof(48),
weget
w =w
·exp(cid:0) −ηℓ¯(cid:1) ·(cid:0) 1−η(cid:0)
ℓ
−ℓ¯(cid:1)(cid:1)
.
t+1,i t,i t t,i t
Bydefiningπ := wt,i ,weget
t,i (cid:80) j∈[K]wt,j
w
exp(cid:0) −ηℓ¯(cid:1)(cid:2)
w
·(cid:0) 1−η(cid:0)
ℓ
−ℓ¯(cid:1)(cid:1)(cid:3)
π = t+1,i = t t,i t,i t
t+1,i (cid:80) jw t+1,j exp(cid:0) −ηℓ¯ t(cid:1)(cid:104) (cid:80) j∈[K]w
t,j
·(cid:0) 1−η(cid:0) ℓ
t,j
−ℓ¯ t(cid:1)(cid:1)(cid:105)
w
·(cid:0) 1−η(cid:0)
ℓ
−ℓ¯(cid:1)(cid:1)
t,i t,i t
=
(cid:80)
w
·(cid:0) 1−η(cid:0)
ℓ
−ℓ¯(cid:1)(cid:1)
j∈[K] t,j t,j t
w
·(cid:0) 1−η(cid:0)
ℓ
−ℓ¯(cid:1)(cid:1)
t,i t,i t
=
(cid:80)
w
j∈[K] t,j
=π
(cid:0) 1−η(cid:0)
ℓ
−ℓ¯(cid:1)(cid:1)
.
t,i t,i t
NotethatthisrecoverstheWSUupdate.10
10TheideaoflinearapproximationofhedgewasnotedbyKivinenandWarmuth(1997)foraslightlydifferentsetting.
34C.2 CompletedversionofpotentialargumentofSection3.1
Inthissubsection,fortheconvenienceofthereader,wegiveacomprehensiveexplanationofthederivationof
(7)and(5).
InthepotentialanalysisofHedgewhichcanbefoundinHazanetal.(2016),foranyi∈[K]andt∈[T],
wedefineΦHEDGE := w withw andπ asinDefinition5. Moreover,assumethatw =1.11 Wealso
t,i t,i t,i t,i 1,i
definedefineΦHEDGE := (cid:80) w . Bynon-negativityofw ,wehave
t j∈[K] t,j t,i
1 1
ln(ΦHEDGE)≤ ln(ΦHEDGE). (49)
η T+1,i η T+1
Itiseasytoseethatforanyt∈[T]wecanwrite
 
(cid:88)
ΦH t+E 1DGE =ΦH tEDGE  π t,jexp(−ηℓ t,j).
j∈[K]
Notethatwehave
(cid:88) π exp(−ηℓ )≤1−η (cid:88) π ℓ +η2 (cid:88) π (ℓ )2 (exp(−x)≤1−x+x2forx≥0)
t,j t,j t,j t,j t,j t,j
j∈[K] j∈[K] j∈[K]
 
≤exp−η (cid:88) π t,jℓ
t,j
+η2 (cid:88) π t,j(ℓ t,j)2 . (exp(x)≤1+x)
j∈[K] j∈[K]
Therefore,wehave
 
ΦH t+E 1DGE ≤ΦH tEDGEexp−η (cid:88) π t,jℓ
t,j
+η2 (cid:88) (cid:88) π t,j(ℓ t,j)2 .
j∈[K] t∈[T]j∈[K]
Byapplying(50)recursively,weget
 
ΦH T+ED 1GE ≤ΦH 1EDGEexp−η (cid:88) (cid:88) π t,jℓ t,j +η2 (cid:88) (cid:88) π t,j(ℓ t,j)2 
t∈[T]j∈[K] t∈[T]j∈[K]
 
=explnK−η (cid:88) (cid:88) π t,jℓ
t,j
+η2 (cid:88) (cid:88) π t,j(ℓ t,j)2 , (50)
t∈[T]j∈[K] t∈[T]j∈[K]
sinceΦHEDGE =(cid:80) 1 =K.
1 j∈[K] K
Ontheotherhand,wehave
 
(cid:88) (cid:88)
ΦH T+ED 1,G iE =ΦH T,E iDGEexp(−ηℓ T,i)=ΦH 1,E iDGEexp(−η ℓ t,i)=exp−η ℓ t,i. (51)
t∈[T] t∈[T]
WecanupperboundtheRHSof(49)by(50)andlowerboundtheLHSof(49)by(51)toget
 
− (cid:88) ℓ t,i ≤ η1 ln(ΦH T+ED 1,G iE)≤ η1 ln(ΦH T+ED 1GE)≤− (cid:88) (cid:88) π t,jℓ t,j + ln ηK +η (cid:88)  (cid:88) π t,j(ℓ t,j)2 .
t∈[T] t∈[T]j∈[K] t∈[T] j∈[K]
11ThisisslightlydifferentthanDefinition5wherew1,i = 1/K. Wecanviewitasdividingallweightsbythesameconstant. This
doesnotimpactthebehaviourofHedgeatall.
35Notethattheaboveisthefullversionof(5). Rearranging,weget12
 
(cid:88) (cid:88) π t,jℓ
t,j
− (cid:88) ℓ
t,i
≤ ln ηK +η (cid:88) (cid:88) π t,j(ℓ t,j)2 .
t∈[T]j∈[K] t∈[T] (cid:124)(cid:123)(cid:122)(cid:125) t∈[T] j
explorationterm (cid:124) (cid:123)(cid:122) (cid:125)
Secondordererror
ForWSU,thepotentialisdefinedasΦWSU := π andΦWSU := (cid:80) π =1. Bynon-negativity
t,i t,i t j∈[K] t,i
ofπ wehave
t,i
1 1
ln(ΦWSU )≤ ln(ΦWSU)=0. (52)
η T+1,i η T+1
Now, the RHS of (6) (which is 0) does not involve any second-order error term. In fact, since WSU is
normalized, the RHS does not give us information about the regret. However, we can extract the difference
betweenthecumulativelossofthealgorithmandexpertifromtheLHSof(6).
Indeed,wehave
  
(cid:88)
ΦW T+S 1U ,i =ΦW T,iSU 1−ηℓ T,i− π T,jℓ T,j
j
  
(cid:89) (cid:88)
=ΦW 1,iSU 1−ηℓ t,i− π t,jℓ t,j
t∈[T] j
    2
1 (cid:89) (cid:88) (cid:88)
≥ K exp −ηℓ t,i− π t,jℓ t,j−η2 ℓ t,i− π t,jℓ t,j  
t∈[T] j j
    2
1 (cid:88) (cid:88) (cid:88) (cid:88)
=
K
exp −η ℓ t,i− π t,jℓ t,j−η2 ℓ t,i− π t,jℓ t,j  , (53)
t∈[T] j t∈[T] j
wheretheinequalitycomesfrom1−x≥exp(−x−x2)for0≤x≤1/2.
Using(53),wecanlowerboundtheLHSof(52)as
   2
(cid:88) (cid:88) lnK (cid:88) (cid:88) 1 1
 π t,jℓ t,j −ℓ t,i− η − η  π t,jℓ t,j −ℓ t,i ≤ η ln(ΦW T+S 1U ,i)≤ η ln(ΦW T+S 1U)=0.
t∈[T] j t∈[T] j
Notethattheaboveisthefullversionof(7). Rearranging,weget
 2
(cid:88) (cid:88) (cid:88) lnK (cid:88) (cid:88)
π t,jℓ t,j − ℓ t,i ≤ η +η  π t,jℓ t,j −ℓ t,i .
t∈[T]j∈[K] t∈[T] (cid:124)(cid:123)(cid:122)(cid:125) t∈[T] j
explorationterm (cid:124) (cid:123)(cid:122) (cid:125)
Secondordererror
12NotethattheexplorationtermisaninevitableerrorincurredbybothHedgeandWSUwhentheymovetowardtheoptimalpointin
thesimplex∆K bythelearningrateη.Wecallitexplorationtermasthealgorithmistryingtoexploreandfindtheoptimalpointinthe
domainofthesimplex.
36Implication for bandit case In the bandit setting, when we use WSU-UX, we can show that we get a
second-ordertermin(7)whichisupperboundedby
 2  2 
E (cid:88) π t,jℓˆ t,j −ℓˆ t,i  ≤E (cid:88) π t,jℓˆ t,j +(cid:16) ℓˆ t,i(cid:17)2  
j j
 
≤E (cid:88) π t,j(cid:16) ℓˆ t,j(cid:17)2 +(cid:16) ℓˆ t,i(cid:17)2 . (Jensen’sinequalityforf(x)=x2)
j
Notethat
   
E  (cid:88) π t,j(cid:16) ℓˆ t,j(cid:17)2 =E  (cid:88) π t,j(cid:18) ℓ t,j1 π˜[I t =j](cid:19)2 
t,j
j∈[K] j∈[K]
 
=E  (cid:88) π
t,j(cid:18)
π˜ℓ
t,j(cid:19)2
E t−1(cid:2)1[I t =j]2(cid:3) 
t,j
j∈[K]
 
=E  (cid:88) ππ ˜t,j  (ℓ t,j ≤1) (54)
t,j
j∈[K]
≤2K, (55)
wherethelastinequalityholdssince πt,j ≤2aswehave
π˜t,j
(cid:16) γ (cid:17) 1 1
2π˜ −π =2 (1−γ)π + −π =(1−2γ)π +2γ ≥min{π , }≥0.
t,i t,i t,i K t,i t,i K t,i K
Moreover,
(cid:20)(cid:16) (cid:17)2(cid:21) (cid:34)(cid:18) ℓ 1[I =i](cid:19)2(cid:35)
E ℓˆ =E t,i t
t,i π˜
t,i
=E(cid:34)(cid:18)
ℓ
t,i(cid:19)2
E (cid:2)1[I
=i]2(cid:3)(cid:35)
π˜ t−1 t
t,i
(cid:34) (cid:35)
ℓ2
=E t,i
π˜
t,i
(cid:20) (cid:21)
1
≤E . (ℓ ≤1)
π˜ t,i
t,i
Therefore,wehave
 
E  (cid:88) π t,j(cid:16) ℓˆ t,j(cid:17)2 ≤2K+E(cid:20) π˜1 (cid:21) ≤2K+ K
γ
=O(K
γ
),
t,i
j∈[K]
wherethelastinequalityholdsbecausewehaveπ˜ =(1−γ)π + γ ≥ γ.
t,i t,i K K
37