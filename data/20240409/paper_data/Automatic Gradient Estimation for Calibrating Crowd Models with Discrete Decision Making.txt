Automatic Gradient Estimation for Calibrating
Crowd Models with Discrete Decision Making
Philipp Andelfinger, Justin N. Kreikemeyer∗
Institute for Visual and Analytic Computing,
University of Rostock, Germany
Abstract
Recentlyproposedgradientestimatorsenablegradientdescentoverstochas-
tic programs with discrete jumps in the response surface, which are not
covered by automatic differentiation (AD) alone. Although these estima-
tors’ capability to guide a swift local search has been shown for certain
problems,theirapplicabilitytomodelsrelevanttoreal-worldapplications
remains largely unexplored. As the gradients governing the choice in
candidate solutions are calculated from sampled simulation trajectories,
the optimization procedure bears similarities to metaheuristics such as
particle swarm optimization, which puts the focus on the different meth-
ods’ calibration progress per function evaluation. Here, we consider the
calibration of force-based crowd evacuation models based on the popular
SocialForcemodelaugmentedbydiscretedecisionmaking. Afterstudying
the ability of an AD-based estimator for branching programs to capture
thesimulation’sruggedresponsesurface,calibrationproblemsaretackled
using gradient descent and two metaheuristics. As our main insights, we
find 1) that the estimation’s fidelity benefits from disregarding jumps of
largemagnitudeinherenttotheSocialForcemodel,and2)thatthecom-
mon problem of calibration by adjusting a simulation input distribution
obviates the need for AD across the Social Force calculations, allowing
gradient descent to excel.
1 Introduction
Agent-based crowd models are widely used in urban planning [1,2], to study
disease spread [3] or to optimize strategies for emergency evacuations [4–6]. In
contrast to coarse-grained models that consider groups of people in aggregate,
agent-based models operate on the microscopic level, giving each agent its own
state, perception of its environment, and decision making. To generate mean-
ingful simulation results, the calibration of the model parameters to empirical
data, e.g., gathered from video footage, is a crucial prerequisite.
Often, metaheuristics such as genetic algorithms or particle swarm opti-
mization are applied [7,8], which permit a straightforward parallelization to
tacklethecomputationalcostofevaluatingmicroscopicsimulationsacrosshigh-
dimensional parameter spaces. Surrogate models generated by sampling the
simulation response can support a faster calibration but may require enormous
numbers of samples to capture an original model’s dynamics [9].
∗Email: {philipp.andelfinger,justin.kreikemeyer}@uni-rostock.de
1
4202
rpA
6
]GL.sc[
1v87640.4042:viXraHowever, all these methods typically operate on black-box observations of
the simulation output. If it is possible to also determine its partial deriva-
tives wrt. the parameters, local search strategies based on gradient descent can
steer the calibration toward a local optimum. Calibration using Bayesian in-
ference [10,11], which also offers uncertainty information instead of sheer point
estimates, could particularly benefit from the resulting increase in sampling
efficiency [12]. Unfortunately, in the presence of models of discrete decision
making, agent-based simulations form stochastic functions with discrete jumps.
For these functions, simple averaging over derivatives determined using the es-
tablished methods and tools for automatic differentiation (AD) [13,14] yields
biased estimates [15].
Recently,therehasbeenrenewedinterestingradientestimationoverstochas-
ticfunctionswithdiscretejumps. Rootedininfinitesimalperturbationanalysis,
this line of research has produced AD-based estimators tailored to specific do-
mains[16–18],forprogramsinvolvingrandomsamplingfromdiscreteprobability
distributions [19], and for general imperative programs [20]. The recent generic
estimators combine pathwise derivatives with the contributions of jumps, the
former being determined using traditional AD, and the latter based on a priori
knowledge of the distributions or density estimations. As an AD-free alter-
native, modern finite differences estimators compute gradient estimates from
series of function evaluations on stochastically perturbed inputs [21]. These
recentestimators’relianceonsamplingraisesthequestionwhetherthefunction
evaluations permitted by a time budget are better spent on obtaining gradi-
ent estimates, or to directly evaluate a set of candidate solutions as part of a
metaheuristic.
Here, we explore the suitability of gradient descent for calibrating crowd
evacuationmodelsbasedonTreiber’spopularSocialForcemodel[22]augmented
bydiscretedecisions. WeassessanAD-basedandastochasticfinitedifferences-
based estimator [20,21] compared to a genetic algorithm and particle swarm
optimization. Our main contributions are threefold:
• We present an alternative derivation of our gradient estimator DiscoGrad
Gradient Oracle [20] starting from the concept of stratified derivatives.
• We study the fidelity of sampling-based gradient estimates over an evac-
uation scenario with continuous or discrete objective.
• The calibration progress is evaluated for three problems, one being a dis-
tribution fitting problem over a 20-dimensional parameter space, showing
that this problem class permits fast convergence via gradient descent.
The remainder of the paper is structured as follows: In Section 2, we briefly
introduce methods for (automatic) differentiation across discrete jumps. In
Section 3, we introduce the existing AD-based gradient estimator DGO. Sec-
tion4describestheconsideredsimulationmodelandscenarios. InSection5,we
presentourexperimentresultsanddiscusstheirimplications. Section6provides
an interpretation of our results and concludes the paper.
22 Background and Related Work
Simulation models of crowd dynamics typically combine low-level models of
pedestrian movement with 1) models of discrete decisions, e.g., for path plan-
ning,and2)stochasticcomponentsaccountingforuncertaintyandvariabilityin
initial conditions and pedestrian behaviors. In effect, the models thus take the
form of stochastic functions : Rn R involving discrete jumps, which poses
challenges to traditional graPdient e→stimation methods. In the following, we
brieflyintroducetheexistingworkongradientestimationacrossdiscontinuities
and concrete estimators for this purpose.
To begin, we briefly recapitulate the widely employed concept of automatic
differentiation (AD) [13,14]. This method views the execution of as a com-
position of operators . By repeatedly applying thePchain rule,
1 2
the partial derivativesPwrt◦. tPhe◦inp··u·ts can be determined from the intermediate
derivativesandvaluesattheoperators . Implementationscanbegroupedinto
i
reverse and forward modes. WhereasPthe former implements AD as a second
(reverse) pass over previously stored intermediate values retrieving one row of
the Jacobian per pass, the latter propagates derivatives through the forward
execution retrieving one column per pass. The partial derivatives obtained this
way are pathwise in the sense that they only capture the operation sequence of
a single program execution, disregarding alternative branches.
When estimating gradients of stochastic programs, we are typically inter-
ested in the partial derivatives of the expected value of wrt. a parameter
vector θ: P
∂
E[ (θ)] (1)
∂θ P
Asiscommonintheliterature[15,23],weintroduceanadditionalparameterω.
This allows the explicit consideration of the stochasticity of , so that (ω;θ)
refers to a specific realization of the stochastic function, e.g.,Pas determiPned by
a pseudo-random generator’s seed. Without loss of generality, we sometimes
consideronlyn=1,whilen>1followsdirectlyfromseparatelycalculatingthe
partial derivative for each dimension. One important case occurs if exhibits
discontinuities (jumps), whose positions depend on θ and/or ω. IPn the last
decades, several methods have been developed to deal with this situation, an
overview of which is given in [24].
Among the earliest is the infinitesimal perturbation analysis (IPA) estima-
tor [25]. Relying on an interchange of the differentiation and expectation op-
erators in Eq. 1 it can be computed by averaging over pathwise derivatives.
However, this estimator is biased for discontinuous , as then the requirements
for the interchange of operators are not satisfied. PTo still account for jumps,
smoothed perturbation analysis (SPA) [15] employs a method inspired by Con-
ditional Monte Carlo. Based on the law of total expectation, E[ ] is calculated
asE[E[ z]]forsometailoredcharacterizationz of ’sexecutionP. Ifz ischosen
correctlPy,|this allows the use of pathwise derivativesPas in IPA.
Recently,inspiredbythesuccessofADondeterministicprograms,automatic
methodstocalculateEq.1gainednewinterest. TheStochasticAD estimator[19]
3builds on SPA and AD to allow the automatic differentiation of programs sam-
pling from discrete parametric distributions. Other recent publications employ
interpolation, replacing discontinuous operators in with continuous ap-
i
proximations[18,26], andabstract interpretation [P27], syPmbolicallypropagating
distributions through to smooth over discontinuities.
Another approachPto the calculation of Eq. 1 are black-box estimators like
REINFORCE [28] and randomized finite-differences schemes. A notable candi-
date from the latter category is proposed in [21] building on [29, Chapter 3.4],
whichweadopthereasfollowsunderthenamePolyakGradientOracle(PGO):
(θ) (cid:80)S ( (θ+σu,ω ) (θ,ω))σ−1u/S, (2)
∇P ≈ s=1 P s −P
whereuisavectorofi.i.d. standardnormalvariatesandσa“smoothingfactor”.
By introducing random perturbations on θ, this estimator can provide a full
gradient estimate from one sample. Note that introducing such perturbations
is possible (or even required) with many estimators, allowing their application
to deterministic programs with discontinuities.
3 DiscoGrad Gradient Oracle
In this publication, we evaluate practical applications of the recently proposed
DiscoGrad Gradient Oracle (DGO) [20]. The following provides an alterna-
tive derivation starting from the abstract concept of the “stratified derivative”
from [23]. The latter is constructed around the concept of a critical event A,
which occurs if (ω;θ+ϵ/2) (ω;θ ϵ/2) > B ϵ for some bound B > 0,
i.e., when a jum|pPis observed −inPan ϵ-n−eighbo|rhood|a|round θ. Then, it holds
that
∂E[ (θ)] (cid:20) ∂ (cid:21)
P =E (θ) +E[∆ ]p′. (3)
∂θ ∂θP P θ
Here, ∆ denotes the distribution of the jump’s magnitudes conditioned on A,
P
and p′ is the critical rate, defined as lim 1P(A). The DGO estimates the
θ ϵ↓0 ϵ
above for the special case of imperative programs with conditional branches.
Letusconsideraprogram withscalarinputandoutput,includingasingle
branchoftheform“ifG(θ)<dP“,withdaconstantandthevalueof depending
on the path taken. Defining C(θ) := G(θ) d, the branching condPition can be
rewritten as C(θ)<0. A realization of C’s−sign indicates the chosen branch.
The term E(cid:2) ∂ (θ)(cid:3) in Eq. 3 can be trivially estimated by sampling path-
wise derivatives u∂ sθ inPg AD. Considering p′, we first note that P( (ω;θ+ϵ/2)
(ω;θ ϵ/2) >B ϵ)=P(C(ω;θ+ϵ/2)θ C(ω;θ ϵ/2)<0). T|hPis is the prob−-
Pability−of a si|gn ch|an|ge in an ϵ-neighborh·ood aro−und θ. We assume all jumps
to originate from branches of the above form, which still allows many other
discontinuous functions like the minimum or absolute value to be expressed.
For ϵ 0, we arrive at p′ = f (0)E(cid:2) ∂ C(θ)(cid:3), where f is the prob-
ability de→nsity function of C(θ θ). DC G(θ O) estim∂ aθ tes p′ by gatheC ri( nθ) g realizations
θ
C(ω ;θ) and calculating a density estimation, which is evaluated at the origin.
s
E(cid:2) ∂ C(θ)(cid:3)aswellasE[∆ ]areestimatedusingrealizationsofC(θ)closeto0.
∂θ P
4Estimating gradients across programs with several branches requires addi-
tional considerations. In the presence of sequential branches, a branch condi-
tion’s distribution can depend on whether previous branches have been taken.
Hence,thedensityestimationmustdistinguishthecontrolflowpathalongwhich
abranchisreached. Weuniquelyidentifyeachbranchbbythepathalongwhich
itisencountered,correspondingtothesequenceofconditionsignsatallprevious
branches. Now, we can express DGO for programs with B N branches:
∈
∂E[ P(θ)]
= lim
1 (cid:88)S ∂ P(ω s;θ) +(cid:88)B
( (ω+;θ) (ω-;θ))λ fˆ
(0)∂C bϵ
∂θ S→∞S ∂θ P b −P b b Cb(θ) ∂θ
s=1 b=1
(4)
whereω+ andω- selectthesamplesscorrespondingtothepositiveandnegative
b b
realizations of C (θ) closest to the branching point, λ is the proportion of
b b
samplesthatencounteredthebranch,fˆ isanestimateofC (θ)’sprobability
Cb(θ) b
density function, and ∂ Cϵ is the partial derivative of the condition near the
∂θ b
branching point.
Wenotethat,incontrasttoPGO(cf.Section2),whosecalculationofdirec-
tional derivatives relies on perturbations of the parameters, DGO can operate
on an original program without introducing external stochasticity. However,
by reducing the ruggedness of the objective function, smoothing via external
perturbations can contribute to faster convergence of gradient descent.
An implementation of DGO exists as part of the DiscoGrad tool [20], which
permits the differentiation across a subset of C++ programs with conditional
branches. The implementation is available publicly1.
4 Crowd Model and Scenarios
Ourexperimentsbuildonthetypicalconstituentsofevacuationstudies: aforce-
based model of crowd mobility in a two-dimensional continuous space and per-
pedestrian discrete decision making. The mobility is modeled using Treiber’s
popularSocialForcemodel[22],inwhichaperson’seffectiveaccelerationvector
is calculated as a sum of three forces. The internal force reflects a person i’s
intention to move in a straight line towards its goal location in direction e0
i
with desired velocity v0, v (t) being the current velocity. The adaptation time
i i
is scaled according to a characteristic time τ . Interaction forces f between
i ij
person i and each other person j in his or her vicinity exert a repellent effect,
reflecting avoidance maneuvers and maintenance of personal distance. Finally,
obstacle forces f repel the person from any nearby wall W, leading to the
iW
overall force equation for a person i with mass m and scaling coefficients w ,
i 1
w , w :
2 3
m dv i =w m v i0e0 i(t) −v i(t) +w (cid:88) f +w (cid:88) f
i dt 1 i τ 2 ij 3 iW
i
j̸=i W
1https://github.com/philipp-andelfinger/DiscoGrad
5Figure 1: Scenario for calibrating exit selection coefficients. The crowd en-
ters from the left-hand side, aiming to evacuate by reaching the circular way-
points(dashedcircles),eachagentperiodicallyreconsideringthetargetedexitby
weighing its distance against the number of agents in its vicinity (gray circles).
We consider two scenarios, the first representing a bottleneck in an evac-
uation situation, similar to [11]. The simulation space is a 30 30m square
separated in the center by a single wall with a door 4m in widt×h. As in [11],
we calibrate the weight coefficients that determine the strengths of the forces
experienced by pedestrians aiming to pass through the door. The output of the
simulationtobecalibratediseitherthepedestrians’averagehorizontalposition
or the number of pedestrians evacuated after 20s of simulation time.
Thesecondscenario(cf.Figure1)combineslow-levelmobilityviatheSocial
Force model with discrete exit selections as in existing work such as [30]. A
crowd comprised of 1000 pedestrians gradually enters the scenario from the
left-hand side, aiming to exit the building via any of the four available doors,
each 3m wide. Each pedestrian selects its target door by weighing its distance
against the congestion level at the door as measured by the number of agents
nearby. To be able to react to changing circumstances, each agent reconsiders
thepreviousdecisionevery15s. Theweightcoefficientunderlyingthedecisionis
drawnfromaprobabilitydistributionsuppliedasmodelparametersintheform
of a histogram ranging in 20 steps from 0.1, where the decision is dominated
by the congestion level, to 1.0, where the decision is made solely based on
distance. The simulation output to be minimized is the Wasserstein distance
of the histogram of observed evacuation times to a reference histogram after a
warm-up time of 100s, spanning 20 steps from 10s to 75s.
The scenarios differ fundamentally in their implications for gradient estima-
tion. While the first scenario involves explicit conditional branches only in the
counting of evacuations, discrete jumps are created by the Social Force model
itself,makingthisachallengingscenarioforDGO.Incontrast,thecalibrationof
the second scenario leads to gradients entirely defined by conditional branches,
which prevents their estimation via AD alone but is well-suited for DGO.
65 Experiments
The goal of our experiments is to determine whether gradient descent using
sampling-basedgradientestimatorscanoutperformgeneticalgorithmsandpar-
ticle swarm optimization in the calibration of evacuation models. We approach
this objective by first studying the degree to which the estimators are able to
capture Social Force’s dynamics. We then turn to the calibration problems
and carry out hyperparameter sweeps in order to shed light on the relative
performance of the different optimization methods.
The simulation models were implemented in C++ within DiscoGrad [20],
closely following PEDSIM2 for the Social Force model and its parametrization.
As the genetic algorithm implementation, we used pyeasyga 3, and for the par-
ticle swarm optimization we employ the pyswarms library [31]. All simulations
use Leapfrog integration with a time step of 0.1s. The calibration experiments
werecarriedoutontwoidenticalmachines,eachequippedwithanAMDEPYC
9754 processor with 256 threads and 768GB RAM, running Ubuntu 22.04.4
LTS, each machine executing at most 256 calibration runs in parallel.
5.1 Automatically Differentiating the Social Force Model
WhiletheSocialForcemodelitselfdoesnotcallforexplicitconditionalbranches,
slight perturbations in the parameters can cause large discrete jumps in ac-
celeration. For instance, the interaction force increases exponentially as the
distance between two agents decreases, and its direction depends discretely on
the distance and angle difference between two agents. If the distance between
two agents is small, minor changes in parameters can cause a change in direc-
tion and thus extreme changes in forces. While AD alone correctly determines
the gradient at a given point of the parameter space, it cannot capture such
jumps. DGO can treat these jumps as explicit conditional branches. However,
its estimation relies on sampled derivatives of intermediate branch conditions
(cf. Section 3), which may suffer from high variance when pairs of agents come
in close mutual proximity.
To assess the differences in the AD-based and black-box estimators’ ability
to estimate gradients of crowd simulations, we carried out a parameter sweep
acrosstheweightcoefficientsw ,w ,w thatgoverntheintensityoftheinternal,
0 1 2
interaction,andobstaclesforcesforthesingle-exitevacuationscenario. Forcom-
petitiveness with optimization procedures other than gradient descent, we are
particularly interested in the gradients’ fidelity with small numbers of samples.
Thus, we study the estimation error when varying the number of samples in
comparison to reference gradients calculated from 100000 function evaluations
via PGO, which delivers unbiased estimates of the smoothed gradient.
Figure 2 shows the partial derivatives wrt. each of the coefficients being
variedseparatelyin300stepswhilekeepingtheothersfixedatvaluesof0.6,5.5,
and 5.5, respectively. The grey curves show the simulation output, which is the
2https://github.com/chgloor/pedsim
3https://github.com/remiomosowon/pyeasyga
7−
−1 10
0
1111
−
−
0000
01
10101 I
D
PP GGA OO, ,1 ,0 110
00
00s ,a 0sm
0a
0mpl
p
se als
e ms
ples
511 05
−
−1 10
0
11111
−
−
00000
01
101012 1234
−−102
0.5 0.6 0.7 0.8 0.9 1.0
0 −−102
0.5 0.6 0.7 0.8 0.9 1.0
0
w0 w0
(a) 3 agents, internal weight w . (d) 10 agents, internal weight w .
0 0
1.00 10.0 4 2.0
0.75 7.5 2 1.5
0.50 5.0 0 1.0
0.25 2.5 0.5
2
0.0 − 0.0
0 5 10 15 20 25 30 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0
w1 w1
(b) 3 agents, interaction weight w . (e) 10 agents, interaction weight w .
1 1
11 00 01 60 11 00 12 60
10− 01
40
101 −0 010
40
− −−−10 111− 000 2101 020 − −−−10 1111− 0000 32101 020
0 5 10 15 20 25 30 − 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0
w2 w2
(c) 3 agents, obstacle weight w . (f) 10 agents, obstacle weight w .
2 2
Figure 2: Derivative estimates with respect to the weight of the three types
of forces in scenarios populated with 3 agents (a-c) and 10 agents (d-f) with
the fit in the agents’ final coordinates’ as output and σ = 0.001. PGO with a
large number of samples is used as the reference. DGO captures most of the
derivatives’spikes,whereastheIPAestimateonlyreflectsthegeneralcurvature.
squared error in the average final agent positions compared to the pre-defined
reference. For comparability to PGO, the input is perturbed by Gaussian noise
with σ = 0.001. For the partial derivative estimates, we evaluate averaging
across plain AD gradients (IPA) and DGO against the reference produced by
PGO. With 3 agents, both AD-based estimators closely track the derivatives’
curvature. In (b), we can see that the jump in the simulation output at about
24.5 is not visible in the derivative at the chosen resolution along the w axis.
1
Importantly,weobservethattheIPAcurvedoesnotfollowthesharpdownward
spikes in ∂y/∂w , while they are accurately captured by DGO. The results
2
with 10 agents follow a similar trend, but due the increased number of force
calculations,thesimulationoutputbecomessubstantiallymorerugged,resulting
in noisier estimates using DGO.
Figure3assessesthesamescenariofocusingonw afterchangingtheoutput
0
tothesquarederrorinthenumberofevacuationscomparedtoareferencevalue.
This entirely discrete objective is smoothed only by the parameter’s perturba-
tions, whose standard deviation we set to 0.01 and 0.1, observing the expected
8
0w∂/y∂
1w∂/y∂
2w∂/y∂
y
y
y
0w∂/y∂
2w∂/y∂
1w∂/y∂
y
y
y1.5 103
101 IPA/DGO,100samples 102
−
−−1 10
0
111 −
−
000 01 1010 0.6D PGG OO ,, 11 00 00 ,0s 0a 0m 0p s .al 7e ms ples
0.8 0.9 1.0
001 ... 050
−
−−1 10
0
1111 −
−
0000 01 10101
0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25
012 00
w0 w0
(a) 3 agents, σ=0.01. (c) 10 agents, σ=0.01.
5.0 0.8 103
102
2.5 0.6 11 00 01 20
−0 2. .0 5 00 .. 24 − −1 10 0 11− − 0001 101 10
−102
−5.0
0.5 0.6 0.7 0.8 0.9 1.0
−−103
0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25
0
w0 w0
(b) 3 agents, σ=0.1. (d) 10 agents, σ=0.1.
Figure 3: Derivative estimates with respect to the weight of the internal force,
the simulation output being the fit in the number of evacuations. In the larger
scenario,DGO’sestimatessufferfromsubstantialnoiseasjumpsinthemobility
derivatives translate to biased and high-variance derivative estimates of the
simulationoutput. WhenignoringjumpsintheSocialForcemodel(IPA/DGO),
estimates observe some bias but capture the trends well.
increase in smoothness with the larger value. In this problem, the simulation
output is gathered by counting the number of agents that have passed the exit,
which in the model’s source code translates to a series of conditional branches
on the agent positions. Considering DGO, the estimation of the branches being
takenandtheireffectsontheoverallderivativesarenowsubjecttoanynoisein
thepositions’partialderivatives,leadingtoextremelynoisyandofteninaccurate
derivative estimates particularly with 10 agents.
In the previous scenario, we have seen that the IPA estimates reflect the
main curvature of the derivatives. However, since IPA alone yields zero-valued
derivatives with this discrete objective, it cannot be applied here. Instead,
we combine IPA and DGO by disregarding any jumps in Social Force while
still accounting for the effects of branches using DGO. In this combination,
while some noise and slight deviations from the reference are observed, the
tendenciesofthereferencearereflectedmuchmoreaccuratelywith100function
evaluations.
To quantify the gradient estimates’ fidelity, we consider the mean absolute
error compared to PGO with 100000 function evaluations for w . Figure 4
0
supports the previous observations: As the source of IPA’s deviation from the
reference is its inability to capture jumps, its estimates do not improve with
additional samples, while the estimates with 10 samples observe a similar er-
ror compared to the other estimators. In contrast, DGO improves somewhat
with more evaluations but consistently outperforms PGO only in the smaller
9
0w∂/y∂
0w∂/y∂
y
y
0w∂/y∂
0w∂/y∂
y
y2.4 0.7 0.25 0.21 6.7 3.2 2.4 2.4
0.22 0.17 0.17 0.17 3.0 3.0 3.0 3.0
8.2 2.6 0.95 0.28 5.5 1.6 0.54 0.19
10 100 1000 10000 10 100 1000 10000
Number of Samples Number of Samples
(a) 3 agents. (b) 10 agents.
Figure 4: MAE of gradients wrt. w fitting the agent coordinates, σ =0.001.
0
1.8 0.82 0.33 0.12 9.1 3.9 4.7 4.0
2.1 0.57 0.24 0.078 5.7 4.6 2.5 2.4
2.9 0.78 0.4 0.061 16. 4.2 1.3 0.41
10 100 1000 10000 10 100 1000 10000
Number of Samples Number of Samples
(a) 3 agents. (b) 10 agents.
Figure 5: MAE of gradients wrt. w fitting the evacuation count, σ =0.001.
0
scenario. PGO reliably approaches the reference when increasing the sample
count. However, we note that at low sample counts, the AD-based estimators
are highly competitive.
The results for the same scenario with the discrete objective are shown in
Figure5. Here,similarerrorlevelsareobservedforallestimatorswith3agents,
whilewith10agents,PGOissuperiorat1000functionevaluationsandbeyond.
Again, the AD-based estimators are competitive up to 100 evaluations.
5.2 Calibrating Force Coefficients
We now compare the practical capabilities of the different gradient estimators,
PSO and GA. To achieve a reasonably fair comparison among the calibration
progress, we carried out a sweep across a range of sensible hyperparameters,
with 10 microreplications of the program and 20 macroreplications of each hy-
perparameter configuration (cf. Table 1).
Foreachestimator,PSO,andGA,weselecttheconfigurationthatproduced
the best average solution at the end of the time budget of 5 minutes wall time
and report the mean over macroreplications of the crisp simulation output, i.e.,
DGO, PGO, and IPA PSO GA
samples 1,10,50 particles 10,50 population 10,50
σ 0,0.01,0.1,0.5,1.0 c1,c2,w 10LHCsamples elitism yes,no
lr 0.01,0.1,0.5,1.0 neighbors 3,6,all mutation x=U(0,10),x+=N(0,0.1)
Table 1: Hyperparameter sampling ranges. Legend: σ sample size, lr learning
rate, (c ,c ,w)social, cognitive, andinertiaparameters, LHClatinhypercube.
1 2
10
rotamitsE
rotamitsE
OGD
API
OGP
OGD
OGD/API
OGP
rotamitsE
rotamitsE
OGD
API
OGP
OGD
OGD/API
OGPPSO
101 101 G PGA O
IPA
DGO
0 25 50 75 100 125 150 0 2500 5000 7500 10000 12500 15000
WallTime[s] FunctionEvaluations
(a) Progress over wall time. (b) Progress over function evaluations.
Figure 6: Calibration progress over steps, wall time and function evaluations
during calibration of the final agent coordinates via the Social Force weights.
101 101
PSO
GA
PGO
DGO
IPA/DGO
100 100
0 50 100 150 200 250 300 0 5000 10000 15000 20000 25000 30000
WallTime[s] FunctionEvaluations
(a) Progress over wall time. (b) Progress over function evaluations.
Figure 7: Calibration progress over steps and wall time during calibration of
the number of evacuations via the Social Force weights.
without any perturbations, which we evaluated in a post-processing step over
1000 microreplications. In addition to the progress over wall time, we also
show the progress over function evaluations, each of which reflects one simula-
tion trajectory executed by a sample (gradient estimators), particle (PSO), or
population member (GA).
Figure 6 shows the solution quality over function evaluations and wall time
for the first problem. As expected, we observe that all approaches converge
quicklyforthisproblem. ThefastestprogressovertimeisachievedbyGA,which
converged after only a few seconds. Considering the gradient estimators, DGO
and the finite differences-based PGO exhibit comparatively high variance and
aretheslowesttoconverge. DGO’sprogressperfunctionevaluationissimilarto
GA,butisslowedinwalltimebythegradientestimationoverhead. Remarkably,
IPA, which fared worst in terms of gradient fidelity (cf. Figure 4), yields the
fastest convergence, which suggests that capturing the general curvature of the
objective function suffices to quickly identify a local minimum.
In the second calibration problem shown in Figure 7, GA outperforms the
other methods both in terms of function evaluations and time. Of the gradient
estimators, PGO makes the fastest progress in wall time, with a plateau be-
tween about 10s to 100s stemming from a lack of initial progress in one of the
20 macroreplications. Again, the AD-based gradient estimation benefits from
disregardingjumpsintheforcecalculations,whichallowsIPA/DGOtoovertake
PGO over function evaluations, albeit encumbered by the AD overhead.
11
ESM
ESM
ESM
ESM6 ×10−2 6 ×10−2
34 ×× 11 00 −− 22 34 ×× 11 00 −− 22 P G PS GAO
O
2 ×10−2 2 ×10−2 DGO
0 1000 2000 3000 4000 5000 6000 7000 0 100 200 300 400
WallTime[s] OptimizationStep
(a) Progress over wall time. (b) Progress over steps.
Figure8: Calibrationprogressoveroptimizationsteps,functionevaluations,and
wall time for the exit selection scenario. Gradient descent using the sampling-
based estimators PGO and DGO achieved the best fit to the reference.
5.3 Calibrating Decision-Making Parameters
Next, we turn to the higher-dimensional problem of adjusting a distribution of
weight coefficients that govern the agents’ discrete exit selection decisions. An
important property regarding the gradient estimation and a key difference to
thepreviousproblemsliesindrawingtheindividualagents’coefficientsfromthe
input distribution as part of a simulation run. Each coefficient is determined
by inverse transform sampling on the distribution specified in discretized form
by the parameters. After drawing u U(0,1), we iterate over the histogram’s
normalized per-bin probabilities b a∼nd select the coefficient according to the
i
lowest bin index i with u (cid:80)20 b . Drawing each pedestrian’s coefficient thus
involvesasequenceofcond≥itioni= al1 bri
anchesonthecumulativesumofnormalized
bin weights. The branch conditions are functions of the input parameters and
are thus taken into account as part of DGO’s gradient estimation. However,
the remainder of each simulation trajectory, including all force calculations,
exit selection decisions, and the calculation of the distance to the reference
outputdistribution,isadirectconsequenceofthebranchestakenwhiledrawing
coefficients. Since the pathwise derivatives wrt. parameters thus extend only to
the initial coefficient calculation, DGO’s gradient estimates are based solely
on the critical events generated by these initial branches. Hence, AD-based
derivatives across the force calculations do not contribute to DGO’s estimates,
which eliminates the main source of noise observed in the previous problems.
Figure8showsthecalibrationprogressforthisproblembasedonasubsetof
the hyperparameters from Table 1, and setting the number of microreplications
to 1 or 10. As one function evaluation corresponds to about 1s of wall time, we
omit the progress over function evaluations and instead show the progress over
optimization steps, each of which can cover several evaluations depending on
the method’s hyperparameters. Here, gradient descent using PGO and DGO
identified the best solutions within the time budget. We note the steepness of
DGO’sprogressoverstepsinthebest-performingconfigurationof10simulation
replications and 25 samples per step, in contrast to PGO’s 1 replication and 25
samples. DGO overtakes PGO at around 5000 steps, whereas PGO stagnates.
12
ecnatsiDnietsressaW ecnatsiDnietsressaW0.20 Reference Reference
0.15 Fitted 0.2 Fitted
0.10
0.1
0.05
0.00 0.0
0.10 0.19 0.29 0.38 0.48 0.57 0.67 0.76 0.86 0.95 10 17 24 31 37 44 51 58 65 72
DistanceWeightCoefficient EvacuationTime[s]
(a)InputDistributionoverExitSelec- (b) Output Distribution over Evacua-
tion Coefficients tion Times
Figure9: BestcalibrationresultsgeneratedbyDGO.Whiletheoutputmatches
thereferencedistributionwell,theidentifiedinputdistributionissimilarinmean
to the reference but differs in shape.
PSOandGAshowsimilarprogress,andneitherreachesconvergencebytheend
of the time budget, with GA achieving a similar solution quality as PGO.
Finally, we consider the solution identified by DGO in its best-performing
hyperparameter combination and macroreplication. Figure 9 shows the cali-
bratedinputhistogramoverexitselectioncoefficients,andtheoutputhistogram
over evacuation times. As expected, a good fit is achieved with respect to the
distribution of evacuation times. In contrast, the input distribution still some-
what deviates from the reference in shape, although the tendency is captured.
This result shows that for this problem, high-quality solutions can be achieved
via differently shaped input distributions. Hence, the calibration for real-world
purposes would likely benefit from additional criteria to increase identifiability.
6 Conclusions
Our study of the crowd model calibration via gradient descent using sampling-
based estimators shows both the challenges and the promise of the approach.
Firstly, even in the absence of explicit conditional branching, crowd simula-
tions based on Social Force can observe large jumps in the simulation output.
WhileourAD-basedgradientestimatoriscapableofaccountingforsuchjumps,
scenarios of non-trivial size can generate sufficiently rugged output so that cap-
turing the jumps based on only few samples becomes challenging. The general
function curvature seems to be largely independent of the jumps, which sug-
gests using either simple pathwise derivatives for the Social Force portion of a
simulation, or resorting to black-box gradient estimators.
Our largest calibration problem involved adjusting an input distribution
acrossper-agentcoefficients,fromwhichwedrawbyinversetransformsampling
using a series of conditional branches. A key insight is that in this formulation
ofsimulation-basedinference,ourAD-basedmethodreducestoestimatingthese
branches’ effects. This can positively affect both performance, by reducing the
ADoverheadtoaminimum,andfidelity,byreducingthedependenceonpoten-
tially noisy intermediate pathwise gradients. We consider this an encouraging
result demonstrating a gainful integration of AD into inference workflows [12].
13
seicneuqerFdezilamroN seicneuqerFdezilamroNIn a calibration problem across 20 input dimensions, gradient descent using
the sampling-based estimators outperformed the gradient-free methods. The
general tendency observed is that the AD-based gradient estimators are benefi-
cial where large jumps in the underlying force calculations can be disregarded
andonlyexplicitconditionalbranchesmustbeaccountedfor. Thus,apromising
direction for future work lies in further facilitating gradient-based calibration
and optimization by model refinements that reduce jumps in acceleration [32]
while maintaining a realistic representation of real-world crowd behavior.
Acknowledgements
FundedbytheDeutscheForschungsgemeinschaft(DFG,GermanResearchFoun-
dation), grant no. 497901036 (PA) and 320435134 (JK).
References
[1] HamidMotieyanandMohammadSaadiMesgari. Anagent-basedmodeling
approach for sustainable urban planning from land use and public transit
perspectives. Cities, 81:91–100, 2018.
[2] Mauricio González-Méndez, Camilo Olaya, Isidoro Fasolino, Michele
Grimaldi, and Nelson Obregón. Agent-based modeling for urban devel-
opment planning based on human needs. conceptual basis and model for-
mulation. Land Use Policy, 101:105110, 2021.
[3] Jürgen Hackl and Thibaut Dubernet. Epidemic spreading in urban areas
using agent-based transportation models. Future internet, 11(4):92, 2019.
[4] XuweiChenandFBenjaminZhan. Agent-basedmodellingandsimulation
ofurbanevacuation: relativeeffectivenessofsimultaneousandstagedevac-
uationstrategies. JournaloftheOperationalResearchSociety,59(1):25–33,
2008.
[5] WeihaoYin,PamelaMurray-Tuite,SatishVUkkusuri,andHughGladwin.
Anagent-basedmodelingsystemfortraveldemandsimulationforhurricane
evacuation. Transportation research part C: emerging technologies, 42:44–
59, 2014.
[6] Selain Kasereka, Nathanaël Kasoro, Kyandoghere Kyamakya, Emile-
Franc Doungmo Goufo, Abiola P Chokki, and Maurice V Yengo. Agent-
basedmodellingandsimulationforevacuationofpeoplefromabuildingin
case of fire. Procedia Computer Science, 130:10–17, 2018.
[7] David Wolinski, S J. Guy, A-H Olivier, Ming Lin, Dinesh Manocha, and
Julien Pettré. Parameter estimation and comparative evaluation of crowd
simulations. In Computer Graphics Forum, volume 33, pages 303–312.
Wiley Online Library, 2014.
14[8] Daniil Voloshin, Dmitriy Rybokonenko, and Vladislav Karbovskii.
Optimization-based calibration for micro-level agent-based simulation of
pedestrian behavior in public spaces. Procedia Computer Science, 66:372–
381, 2015.
[9] BrunoPietzsch,SebastianFiedler,KaiGMertens,MarkusRichter,Cédric
Scherer, Kirana Widyastuti, Marie-Christin Wimmler, Liubov Zakharova,
andUtaBerger.Metamodelsforevaluating,calibratingandapplyingagent-
based models: a review. The Journal of academic social science studies,
23(2), 2020.
[10] Nikolai Bode. Parameter calibration in crowd simulation models using ap-
proximate bayesian computation. arXiv preprint arXiv:2001.10330, 2020.
[11] Marion Gödel, Nikolai Bode, Gerta Köster, and Hans-Joachim Bungartz.
Bayesian inference methods to calibrate crowd dynamics models for safety
applications. Safety science, 147:105586, 2022.
[12] Kyle Cranmer, Johann Brehmer, and Gilles Louppe. The frontier of
simulation-based inference. Proceedings of the National Academy of Sci-
ences, 117(48):30055–30062, 2020.
[13] AndreasGriewankandAndreaWalther. Evaluating derivatives: principles
and techniques of algorithmic differentiation. SIAM, 2008.
[14] Charles C Margossian. A review of automatic differentiation and its effi-
cient implementation. Wiley Interdisciplinary Reviews: Data Mining and
Knowledge Discovery, 9(4):e1305, 2019.
[15] Wei-BoGongandYu-ChiHo. Smoothed(conditional)perturbationanaly-
sis of discrete event dynamical systems. IEEE Transactions on Automatic
Control, 32(10):858–866, 1987.
[16] Ayush Chopra, Ramesh Raskar, Jayakumar Subramanian, Balaji Krishna-
murthy, Esma S Gel, Santiago Romero-Brufau, Kalyan S Pasupathy, and
Thomas C Kingsley. Deepabm: scalable and efficient agent-based simula-
tions via geometric learning frameworks - a case study for covid-19 spread
and interventions. In Winter Simulation Conference (WSC), pages 1–12.
IEEE, 2021.
[17] SanghyunSon,Yi-LingQiao,JasonSewall,andMingCLin. Differentiable
hybridtrafficsimulation. ACM Transactions on Graphics (TOG),41(6):1–
10, 2022.
[18] Philipp Andelfinger. Towards differentiable agent-based simulation. ACM
Transactions on Modeling and Computer Simulation, 32(4):1–26, 2023.
[19] Gaurav Arya, Moritz Schauer, Frank Schäfer, and Christopher Rack-
auckas. Automatic differentiation of programs with discrete randomness.
In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh,
15editors, Advances in Neural Information Processing Systems, volume 35,
pages 10435–10447. Curran Associates, Inc., 2022.
[20] Justin N. Kreikemeyer and Philipp Andelfinger. Smoothing methods
for automatic differentiation across conditional branches. IEEE Access,
11:143190–143211, 2023.
[21] YuriiNesterovandVladimirSpokoiny. Randomgradient-freeminimization
of convex functions. Foundations of Computational Mathematics, 17:527–
566, 2017.
[22] DirkHelbingandPeterMolnar.Socialforcemodelforpedestriandynamics.
Physical review E, 51(5):4282, 1995.
[23] RubenSeyer. Differentiablemontecarlosamplerswithpiecewisedetermin-
isticmarkovprocesses. Master’sthesis,ChalmersUniversityofTechnology,
2023.
[24] Michael C. Fu. Chapter 19: Gradient Estimation. In Shane G. Hender-
son and Barry L. Nelson, editors, Simulation, volume 13 of Handbooks in
Operations Research and Management Science, pages 575–616. Elsevier,
2006.
[25] Yu-Chi Ho and Christos Cassandras. A new approach to the analysis of
discrete event dynamic systems. Automatica, 19(2):149–167, 1983.
[26] Sebastian Christodoulou and Uwe Naumann. Differentiable programming:
Efficient smoothing of control-flow-induced discontinuities. arXiv preprint
arXiv:2305.06692, 2023.
[27] Swarat Chaudhuri and Armando Solar-Lezama. Smooth interpretation.
ACM Sigplan Notices, 45(6):279–291, 2010.
[28] Ronald J Williams. Simple statistical gradient-following algorithms for
connectionist reinforcement learning. Machine learning, 8:229–256, 1992.
[29] B.T. Polyak. Introduction to Optimization. Optimization Software, New
York, 1987.
[30] Xiang Wang, Chraibi Mohcine, Juan Chen, Ruoyu Li, and Jian Ma. Mod-
elingboundedlyrationalroutechoiceincrowdevacuationprocesses. Safety
Science, 147:105590, 2022.
[31] Lester James Miranda. Pyswarms: a research toolkit for particle swarm
optimization in python. Journal of Open Source Software, 3(21):433, 2018.
[32] Sven Kreiss. Deep social force. arXiv preprint arXiv:2109.12081, 2021.
16