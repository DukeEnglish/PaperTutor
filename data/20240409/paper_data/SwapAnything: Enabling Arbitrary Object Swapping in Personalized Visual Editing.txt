SWAPANYTHING: Enabling Arbitrary Object
Swapping in Personalized Visual Editing
JingGu1∗ YilinWang2 NanxuanZhao2 WeiXiong2 QingLiu2
ZhifeiZhang2 HeZhang2 JianmingZhang2 HyunJoonJung2 XinEricWang1
1UniversityofCalifornia,SantaCruz
2Adobe
https://swap-anything.github.io/
Concept Source Image Ours Concept Source Image Ours
(a) Single Object Swapping
(b) Partial Object Swapping
(c) Multi-Object Swapping (d) Cross Domain Swapping
Figure1: SwapAnythingresultsonvariouspersonalizedimageswappingtasks. SwapAnythingis
adeptatprecise,arbitraryobjectreplacementinasourceimagewithapersonalizedreference,and
achieveshigh-fidelityswappingresultswithoutinfluencinganycontextpixels. Wedemonstrateits
generaleffectinsingle-object,multi-object,partial-object,andcross-domainswappingtasks.
Abstract
Effectiveeditingofpersonalcontentholdsapivotalroleinenablingindividualsto
expresstheircreativity,weavingcaptivatingnarrativeswithintheirvisualstories,
andelevatetheoverallqualityandimpactoftheirvisualcontent. Therefore,inthis
work,weintroduceSwapAnything,anovelframeworkthatcanswapanyobjects
in an image with personalized concepts given by the reference, while keeping
the context unchanged. Compared with existing methods for personalized sub-
jectswapping,SwapAnythinghasthreeuniqueadvantages: (1)precisecontrolof
arbitraryobjectsandpartsratherthanthemainsubject,(2)morefaithfulpreser-
vationofcontextpixels,(3)betteradaptationofthepersonalizedconcepttothe
image. First,weproposetargetedvariableswappingtoapplyregioncontrolover
latentfeaturemapsandswapmaskedvariablesforfaithfulcontextpreservation
andinitialsemanticconceptswapping. Then,weintroduceappearanceadapta-
tion, to seamlessly adapt the semantic concept into the original image in terms
oftargetlocation,shape,style,andcontentduringtheimagegenerationprocess.
Extensiveresultsonbothhumanandautomaticevaluationdemonstratesignificant
improvementsofourapproachoverbaselinemethodsonpersonalizedswapping.
∗ThisworkwaspartlyperformedwhenthefirstauthorinternedatAdobe.
Preprint.
4202
rpA
8
]VC.sc[
1v71750.4042:viXraFurthermore,SwapAnythingshowsitspreciseandfaithfulswappingabilitiesacross
singleobject,multipleobjects,partialobject,andcross-domainswappingtasks.
SwapAnythingalsoachievesgreatperformanceontext-basedswappingandtasks
beyondswappingsuchasobjectinsertion.
1 Introduction
Intoday’sdigitalagemarkedbytheprolificcreationandwidespreadsharingofpersonalcontent,
generativemodels(Rombachetal.,2022;Crowsonetal.,2022;Dingetal.,2021;Blattmannetal.,
2022)haverisenasapotenttoolforself-expression,storytelling,andamplifyingtheimpactofvisual
narratives. Amongexistingimageeditingmethods(Mengetal.,2022;Hertzetal.,2022;Chenetal.,
2023b) for content creation that empowered individuals to convey their creative instincts, weave
captivatingnarrativesintotheirvisualstories,andenhancethequalityoftheirvisualrepresentations,
personalizecontentswapping(Guetal.,2023a;Lietal.,2023b;Guetal.,2023b),whichtargetsat
creatingandcompositingnewimageswithuser-specifiedvisualconcept,attractedsignificantinterest
duetoitswide-rangingapplicationsinE-commerce,entertainment,andprofessionalediting.
Achievingarbitrarypersonalizedcontentswappingnecessitatesadeepunderstandingofthevisual
conceptinherenttoboththeoriginalandreplacementsubjects. Thereareseveralcrucialchallenges.
Firstly, arbitraryswappingdemandssignificantlygreaterflexibilityfromtheswappingtechnique
comparedwithswappingthemainsubject,duetothevariednatureofthecontentbeingexchanged.
Secondly,anidealswaprequiresflawlesspreservationofthesurroundingcontextpixels,ensuring
thatonlythedesignatedtargetareaundergoesmodification. Thethirdchallengeliesinaccurately
integratingthepersonalizedcontentintothetargetimageinaharmoniousmannerwhilepreserving
theoriginalposesandstyles.
Existing works often fall short of addressing these challenges. Most of existing research (Ruiz
etal.,2023;Shietal.,2023;Chenetal.,2023b;Jiaetal.,2023;Wangetal.,2024)arefocusedon
personalizedimagesynthesis,whichseekstocreatenewimageswithpersonalizedcontent. Although
theseapproachescansynthesizehigh-fidelitycontent,theycannoteditorreplacethevisualcontent
inanexistingimage. Lietal.(2023b);Caoetal.(2023);Mengetal.(2022);Avrahamietal.(2023)
havetriedtoremoveandregeneratetheobjectviamasks,theyoftenstruggletoadaptthenewconcept
intotheimage. Recentlystudies(Patashniketal.,2023;Guetal.,2023a;Tumanyanetal.,2023)
focusonobjectswappingandreplacementbytweakingintermediatevariablesaffectingtheobject’s
features. However,thisapproachlackstheprecisionneededforlocalizedobjectswapping,resulting
inlimitedvisualqualities. Besides,thosemethodsmainlyworkonmainsubjectswapping,andthey
areincapableofarbitraryobjectswapping.
To address these challenges, we introduce SwapAnything, a framework that utilizes pre-trained
diffusionmodelstostreamlinepersonalizedarbitraryobjectswapping. Unlikepreviouswork,our
workisdesignedforarbitraryswappingtaskswithperfectcontextpixelpreservationandharmonious
objecttransition. Ourmethodbeginsbyexploringaninformativerepresentationofthesourceimage
on a diffusion model. We found that various variables in the diffusion process especially latent
featuresfromU-nethaveacorrespondentrelationwiththeimage. Soweproposetokeepthecontext
pixelsinthesourceimagebypreservingthecorrespondentpartinthosevariablesintheswapping
process. Thisprocessistailoredtopreciselyswapspecificareas,ensuringthepreservationofother
objectsandthebackground’sintegrity. Theobjectinformationinthesourceimageisalsoselected
forappearanceadaptation. Morespecifically,locationadaptationcontrolsthelocationwherethe
newconceptshouldbeswapped. Styleadaptationensuresstylisticharmonybetweentheconcept
object and the original image, fostering a natural and cohesive visual presentation. Additionally,
scaleadaptationisintroducedtomodulatethetargetobject’sshape,ensuringitscongruencewith
the spatial and dimensional aspects of the source image. Last, content adaptation is crucial for
smoothly generating the new concept, enabling a seamless blend that mitigates any artifacts or
unnaturaltransitions. Withthesespecializedadaptations,SwapAnythingprovidesaheightenedlevel
ofprecisionandrefinementintherealmofobject-drivenimagecontentswappingasshowninFig.1.
Ourmaincontributionsare: 1)WeproposeSwapAnything,ageneralframeworkforbothpersonalized
swappingandtext-basedswappingonsingleobject,multipleobjects,partialobject,andcross-domain
object. 2)Weidentifiedkeyvariablesforcontentpreservationandproposedtargetedswappingfor
perfect background preservation. 3) We designed a sophisticated appearance adaptation process
toadapttheconceptimageintothesourceobject. 4)Ourapproachhasdemonstratedexceptional
performancethroughcomprehensivequalitativeassessmentsandquantitativeanalysesonswapping
tasksandtasksbeyondswappingsuchasinsertion.
22 RelatedWork
2.1 Text-guidedImageEditing
Withrecentprogressindiffusionmodel,text-guidedImageeditinghasbeenlargelyexplored(Avra-
hamietal.,2023). Imageeditingdrivenbytextaimstoalteranexistingimagefollowingthetextual
guidelinesprovided,whileensuringcertainelementsorfeaturesoftheoriginalimageremainun-
changed. InitialeffortsusingGANmodels(Karrasetal.,2019)wererestrictedtospecificobject
domains. However, diffusion-oriented techniques (Zhang et al., 2023; Nichol et al., 2022; Feng
etal.,2023)havesurpassedthislimitation,enablingtext-drivenimagemodifications. Whilethese
techniquesproducecaptivatingoutcomes,manygrapplewithlocalizededits. Asaresult,manual
masks(Mengetal.,2022;Zengetal.,2023;Mengetal.,2022)areoftenhelptodelineatetheediting
zones. Thoughemployingcross-attention(Hertzetal.,2022)orspatialattributes(Tumanyanetal.,
2023)canachievelocalizededits,theyfacechallengeswithflexiblechanges(likealteringposes)and
preservingtheinitialimagecomposition.
2.2 ObjectDrivenImageEditing
Inobject-drivenimagegeneration,theobjectcouldbeinvertedintothetexualspacesotorepresented
by a new token such as “∗” (Gal et al., 2023; Ruiz et al., 2023; Chen et al., 2023a; Shi et al.,
2023; Tewel et al., 2023). Image editing guided by exemplars spans a vast array of uses. Much
oftheresearchinthisarea(Wangetal.,2019;Huangetal.,2018;Zhouetal.,2021)fallsunder
theumbrellaofexample-basedimagetransformationtasks. Thesetasksleveragevarioussources
ofinformation,fromstylizedimages(Liuetal.,2021;Dengetal.,2022;Zhangetal.,2022)and
layouts(Yangetal.,2023b;Lietal.,2023c;Jahnetal.,2021),toskeletons(Lietal.,2023c)and
sketches or outlines (Seo et al., 2023). Given the versatility of stylized images, the art of image
style adaptation (Liao et al., 2017; Zhang et al., 2020) has garnered significant interest. These
methodsprimarilyfocusonestablishingadetailedmatchbetweentheinputandreferencevisualsbut
falterwhenitcomestolocalizedalterations. Tofacilitatelocalizedchanges,especiallywithflexible
transformations,toolslikeboundingboxesandskeletonshavebeenintroduced. However,thesetools
often demand manual input, which can be challenging for users. A novel approach (Yang et al.,
2023a)framesexemplar-drivenimageeditingasataskoffillingingaps,maintainingthecontext
whiletransferringsemanticelementsfromthereferencetotheoriginalimage. WhileDreamEdit(Li
etal.,2023b)employsastep-by-stepgap-fillingmethodforobjectsubstitution,itdoesn’teffectively
bridge the connection between the original and desired objects. In contrast, our method ensures
that crucial features, like body movements and facial expressions, stay consistent. Meanwhile,
Photoswap(Guetal.,2023a),CustomEdit(Choietal.,2023)andBLIP-Diffusion(Lietal.,2023a)
achievedpersonalizedobjectswapping. However,itdoesnotkeepthenon-objectpixelbackground
intact. Whileourmethoddirectlyfocusesontheeditingareawithoutinfluencingotherobjectsand
background.
3 Preliminary
DiffusionModelsbelongtothefamilyofgenerativemodelsthatarebasedonstochasticprocesses.
Theygenerateanimagebyiterativelyreducingnoisefromaninitialdistribution.Startingfromapoint
ofrandomnoisedenotedasz ,whichfollowsanormaldistributionz ∼ N(0,I),thediffusion
T T
modeldenoiseseachinstancez ,thusproducingz . Diffusionmodelspredictandreversethe
t t−1
noiseateachstepinthediffusionsequencetoarriveatthefinaldenoisedimage.
Inourresearch,weemploythepre-trainedtext-to-imagediffusionframeworkStableDiffusionRom-
bachetal.(2022). Thismodelencodesimagesintoalatentspaceandincrementallydenoisesthe
encodedlatentrepresentation. TheStableDiffusionmodeloperatesonaU-Netarchitecture(Ron-
nebergeretal.,2015),wherethelatentrepresentationz atanygivenstepisderivedfromthetext
t−1
promptP andthepreviouslatentstatez ,asindicatedbythefollowingequation:
t
z =ϵθ(z ,P,t) (1)
t−1 t
This U-Net includes sequence of layers that repeatedly apply self-attention and cross-attention
mechanisms. Inself-attention,thelatentimagefeaturez ,isfirstprojectedintoqueryQ ,K ,
t self self
3Isrc V : U-Net Variables
Source text prompt: “A photo of a turtle” Msrc: Source Image Mask
Mconcept: Con Mce ap st
k
I m a g e
Source
Msrc
Inversion U-Net Variab S×l tye
l
TV
e
′
s
A
r c
daptationBa( c1 k-
g
rM
o
s ur c
n
d)
*V
src Appearance Adaptation
content
U-G Ne en
t
e Vr aa rt ie ad
b le
Msrc *Vc′
oncept
U-NT eta Vrg ae rt
i a ble
adaption
Vconcept Location Adaptation Vtarget
×T′
Itarget
Mconcept
Msrc Mconcept Scale Adaptation
Concept: <*> Target text prompt: “A photo of <*>”
Figure2: OverviewofSwapAnythingonswappingaobjectfromasourceimage(I )intoa
src
personalizedconcept(<∗>)togetthetargetimage(I ). Thepersonalizedconceptisfirst
target
convertedintotextualspacetobetreatedasconceptappearance. Meanwhile,thesourceimageis
firstinvertedintoinitialnoisetoobtainU-Netvariables(includinglatentfeature,attentionmap,and
attentionoutput). Targetedvariableswappingpreservesthecontextpixelsinthesourceimage. The
appearanceadaptationprocessthenutilizestheseinformativevariablestointegratetheconceptinto
thetargetimage.
V ,whichwillbeusedtogetself-attentionmapAandself-attentionoutputϕ.
self
(cid:32) (cid:33)
Q ·KT
M =softmax self √ self
d (2)
ϕ=M ·V
self
Meanwhile,forcross-attentionlayer,thefeatureoutofpreviousself-attentionlayerisprojectedinto
Q ,whilefeatureembeddingoftextualpromptisprojectedintoK andV .
cross cross cross
(cid:18) Q ·KT (cid:19)
A=softmax cross√ cross (3)
d
whereAisthecross-attentionmap. Inthiswork,westudytheswappingofA,M,ϕandz.
4 SwapAnything
Inthissection,weintroducetheSwapAnythingframeworkthatusesadiffusionmodeltoswapon
targetingareafaithfullywhilekeepingthecontextpixelsunchanged.Fig.2illustratesourframework’s
overallstructure. ForsourceimageI ,wefirstinvertittoalatentnoiseandthenobtainthefeature
src
representationsV ,whichwillbeusedduringthetargetimageI generationprocess. InSec.4.1,
src src
wediscusshowtopreservethenon-targetpixelsinthesourceimageperfectly,andhowtoselect
andtransferkeyinformationaboutthesourceimage. Followingthis,inSec.4.2,weintroducethe
appearanceadaptationpipelinethatusesthekeyinformationtointegratethenewconceptintothe
sourceimageseamlessly.
4.1 TargetedVariableSwapping
IntermediatevariablesintheU-Netofadiffusionmodelhavebeenproveninformativeaboutthe
contentofthegeneratedimage(Guetal.,2023a;Hertzetal.,2022;Caoetal.,2023;Tumanyan
etal.,2023). TheyusuallyfocusonthestudyofvariablesinsideofU-netstructuresuchasattention
map,andattentionoutput,whiletheoutputofU-Netateachdiffusionstep,i.e.,latentimagefeature
4𝑧 !% "# Targeted Attention 𝑧!%
Variables Swap Targeted Latent
Feature Swap
Latent Feature 𝑧 Visualization Generated Image
𝑧∗ 𝑧∗
!"# !
Figure3: SwappingprocessinSwapAnything. Theleftpartshowsthecorrespondencebetween
latentfeaturez andtheGeneratedimage. Therightpartshowstheprocedureoftargetedvariable
manipulationintheU-Netdiffusionprocess.
zisnotwidelyexploredbefore. Wearguethatthelatentimagefeaturezcontainsmoreinformation
onimagecontentcontrol. Theimagegenerationprocessforthelatentdiffusionmodelisachievedby
denoisingtheztoarriveataclearrepresentationofahigh-qualityimage,whereasallothervariables
insideofU-netindirectlyaffecttheimagebyimpactingz. Incontrasttosimplyswappingzlikeother
variables,whichwoulderasethenewimage’suniquedetailsandresultinamereduplicationofthe
originalimage,ourinvestigationrevealedasignificantcorrelationbetweenthelatentfeaturezand
theproducedimage,includingapixel-levelcorrespondence. IntheleftpartofFig.3,wevisualize
themaincomponentoftheaveragedlatentfeaturezacrossalldiffusionsteps. ThefindingthatIthas
apart-to-partcorrespondencewiththegeneratedimagesindicatesthepotentialoflocalizedediting
bymanipulatingthelatentfeature.
Consequently,weconsiderastrategywhereonlythecontextpixelswithinzarealtered,affecting
solelytheintendedpixel. Althoughthismethodyieldssuboptimaloutcomes,akintomergingtwo
unrelatedimages,itpromptsustodevisetworemedialmeasures. Firstly,wesuggestlimitingthe
exchangeofthelatentfeaturetotheinitialstagesofdiffusion,allowingsubsequentstepstosmooth
outanydiscordanceinthelatentspace. Furthermore,ourexplorationintoUnet’scross-attentionmap
M,self-attentionmapA,andself-attentionoutputψ revealtheirpotentialinmitigatingartifacts.
Swappingthosefacilitatesthealignmentofthelatentfeaturesbetweenthesourceandtargetimages
beforethepartial-swappingbetweenthem. Inshort,allthevariablesmentionedaboveinboththe
sourceimageandtargetimagegenerationprocesscouldberesizedintotheshapeofthemask,where
themaskcanbeutilizedfortheswappingprocess.
V =g(f(V )∗(1−M )+f(V )∗M ) (4)
target src src target src
HereV includeslatentfeaturez,andotherassistantvariablecross-attentionmapM,self-attention
mapA,andself-attentionoutputϕ. f(·)meansthetransformationprocesstotheshapeofthemask,
whileg(·)meansthetransformationbacktotheoriginalspace. Forsimplicity,weignoreallf(·)
andg(·)inthefollowingtext. Thecontentinthelatentfeatureofthesourceimageischangingas
thediffusionprocesscontinues. Therefore,thelocationofthecorrespondentpixelinlatentspace
maychangeoverdiffusionsteps. Adirectsolutionistodecodethelatentfeaturez intoanimage
at each step and extract the mask dynamically according to the object location in the generated
image. However,wefindthatachangingmaskusuallyconfusesthemodelandleadstoalessoptimal
performance. Therefore,whileusingthesamehigh-qualitymaskthroughthediffusionprocess. We
findthatthemaskcouldeitherbeextractedfromthesourceimagedirectlyusinganoff-the-shelf
modelorfromthegenerationprocessasinSimsaretal.(2023);Patashniketal.(2023). Pleasecheck
theAppendixformoredetails.
4.2 AppearanceAdaptation
Inthissection,weintroducetheappearanceadaptationprocessthatadaptstheconceptintothesource
image,whichnecessitatesmeticulousadjustmentsacrossseveraldimensions: location,style,scale,
and content. Our framework enhances realism and coherence in image manipulation, marking a
significantadvancementinthefield.
54.2.1 LocationAdaptation
Variousintermediatevariableshavebeenproventocorrelatewiththefinalgeneratedimage. Although
withgreatperformance,itdidnotachievelocalswapandthusthebackgroundismodifiedinevitably.
AsshowninFig.2,foreachstep,insteadofdirectlyswappingthewholevariable,weconductlocal
swappingtoonlyswappingthenon-objectposition. Also,toenhancetheswappingresults,wefurther
proposetoconductlocalswappingonthelatentrepresentationz directly. M isa2-dimension
src
variablecontaining0and1. Itisthesamesizeasofthesourceimageandvalue1markstheswapping
location. Tosimplifytheexpression,herewedenotethreeU-Netvariablesattentionmap,attention
output,andlatentrepresentationfortheoriginalimagerecoveryprocessasV ,denotetheones
src
generatedviatargettextpromptasV ,thenwedefinethetargetvariableusedasVbg asthe
concept target
backgroundinformationofthetargetvariable,whichcanbeobtainedfromEq.(5).
Vbg =V ∗(1−M ) (5)
target src src
Thenon-maskedareaistheswappingtargetarea,wherethevariablewillbegeneratedviathetarget
textprompttoinjecttheconceptappearance.Locationadaptationextendsbeyondtheobjectswapping
tasks;wealsodiscovereditsprofoundcapabilityforobjectinsertion. Forfurtherdetailsandresults,
pleaserefertotheAppendix.
4.2.2 StyleAdaptation
Anidealobjectswappingshouldkeepthestyleunchanged. Theobjectinformationinthegenerated
variables is injected via the new concept token. Some style attributes could be already bound
withthetoken. Therefore,solelygeneratingtheforegroundinformationviathetextpromptmight
lead to style inconsistency. Recently, Karras et al. (2019); Park et al. (2019) found that adding
suchnormalizationlayerscanhelpimprovetheconditionalimagegenerationqualitybecausesuch
activationfunctionsmodulation. Unlikefromthem,inourwork,weemploytheAdaIN(Adaptive
InstanceNormalization)tomodulatetheswappingfeatureswithspatialconstraints. WefollowEq.(6)
andEq.(7)todenormalizetheV withthemeanandvariancefromV ineachtimestepfor
concept src
V duringtheimagegenerationprocess. Asaresult,wefindthatbymodulatingtheconcept
target
feature,thegeneratedcontentcanadaptivelyfollowtheoriginalstyleinthesourceimage.
′
V =MaskedAdaIN(V ,V ,M ) (6)
concept scr concept src
Vfg =V′ ∗M (7)
target concept src
MaskedAdaINutilizesthemeanandvariancefromthemaskedregionintheAdaINcalculation. Then
wehavetheblendedfeaturerepresentationsforV :
target
V =Vfg +Vbg (8)
target target target
4.2.3 ScaleAdaptation
Theproportionofanobjectcomparedtoitsenvironmentandotherelementsintheimageiscrucial
forcoherence. Aswappingresultwithimproperscalingcandisturbtheaestheticbalance,resultingin
adisjointappearanceoftheimage.
GuidancefromanexternalclassifierintheinferenceprocessofDiffusionmodelscouldinfluencethe
diffusionnoisetocontrolthegeneratedimage. Epsteinetal.(2023)showsthattheguidancecanalso
beusedontheattentionmaptocontrolthegeneration. Similarly,weadaptthemaskguidanceEq.(9)
tobetteraligntheshapebetweenthesourceobjectandthetargetobject.
ϵˆ =(1+s)ϵ (z ;t,y)−sϵ (z ;t,∅)
t θ t θ t
+vσ ∇ ∥M −Shape(M )(k)∥ (9)
t zt src src 1
wheresistheclassifier-freeguidancestrengthandvisanadditionalguidanceweightforg. Aswith
classifierguidance,wescalebyσ toconvertthescorefunctiontoapredictionofϵ . Shape(M )
t t src
denotes the object shape as identified in the attention layer. Here the energy function g is set as
∥M −Shape(M )∥ tocalculatetheshapedifferencebetweentheoriginalobjectmaskand
src src 1
theextractedshapeintheattentionlayer,whichindicatesthedeviationbetweentheidealshapeand
shapeduringthediffusionprocess.
64.2.4 ContentAdaptation
Abinarymaskwithoutsmoothinghasahigh-frequencytransitionattheedge—itjumpsabruptly
from0to1. Whenusedtomergetwointermediatevariablesfromtwodifferentdiffusionprocesses,
this can result in high-frequency artifacts at the boundary, such as jagged edges or a halo effect.
Smoothing the mask transitions these high frequencies into lower frequencies, which blends the
imagesmorenaturallyandeliminatessuchartifacts. Asmoothmaskcreatesafeatheringeffectat
theedgesofthetransition. Thismakesthemergedareaappearmorecoherentasifthetwoimages
naturallyblendintoeachotherratherthanbeingcutoffabruptly. Therefore,forthediffusionprocess,
wespecificallydesigntwomasksaccordingtothefeatureofdiffusionmodels.
Withoutthissmoothing,theboundarybetweentheimageswouldbesharplydefined,leadingtoa
jarringandunnaturalappearance. TheGaussianBlursoftenstheedges,blendingtheimagesmore
seamlessly. Toaugmentthisimprovement,weintroducetwosmoothingtechniquesforbinarymasks,
applied across both spatial dimensions and temporal steps. These techniques serve to refine the
swapping process, mitigating artifacts and ensuring a smoother, more natural integration of the
swappedregions. Thisresultsinanenrichedvisualoutput,seamlesslyblendingtheinsertedobjects
orobjectpartsintotheoverallimagecomposition.
LinearBoundaryInterpolation: Thisisaprocesswherethesharptransitionbetweentheareawith
1sandtheareawith0sinyourbinaryarrayismadegradual. Onewaytoachievethisisbyusing
aconvolutionwithasmoothingkernel(likeaGaussiankernel)thatwillaveragethevaluesinthe
vicinityofeachpoint,effectivelycreatingagradientattheboundary.
δ(M )=M ⊕K
src src
S =δ(M )∗G
src
(cid:26)
1 ifM[i,j]=1
S′[i,j]= (10)
S[i,j] otherwise
thedilationofthemaskM usingthestructuringelementK,wheredenotesthedilationoperation
src
andGistheGaussiankernel. Theasterisk∗denotestheconvolutionoperation. S′isthefinalsoft
mask.
GradualBoundaryTransition: Thisinvolvesgeneratingasequenceofarrayswherethevalueof
1doesnotappearimmediatelybutincreasesincrementallyfrom0to1. Thiscanbeachievedby
interpolatingbetween0and1acrossthesequenceofarrays.
(cid:26) M (x,y)· t , ift≤K andM (x,y)=1
M (x,y)= src K src (11)
src M (x,y), otherwise
src
Inthisequation,thevalueofM (x,y)isassumedtobe1inthecenterareaand0elsewhere. For
src
thecentralregion,thevaluelinearlyincreasesfrom0to1overthefirstK steps. Fortherestofthe
mask,theoriginalvalueM (x,y)remainsunchanged.
src
Severalprevalentbackbonediffusionmodels,includingStableDiffusion2.1,arerestrictedtopro-
cessingimagesinasquareformat. Resizingimagestofitasquaredimensioncanleadtosubstantial
contentdistortion,adverselyaffectingtheeditingoutcomes. Nevertheless,ourfindingsdemonstrate
thatourmethodexhibitsaremarkablecapacityforadaptation,allowingittoprocessimagesofany
aspectratiowithoutcompromise. Asdocumentedinthispaper,wepresentallimagesinvarious
ratios.
5 Evaluation
5.1 ImplementationDetails
Hereweintroduceimplementationdetails. Inourpaper,weusedStableDiffusion2.1asthepre-
trainedtext-to-imagediffusionmodel. DreamBooth(Ruizetal.,2023)isusedtoconverttheconcept
intotextualspace. Weusednull-textinversion(Mokadyetal.,2023)basedonDDIMinversion(Song
etal.,2020)tobooststheaccuracyandreliabilityoftheinversion.
Forobjectmask,wefirstdetecttheobjectwithGroundingDINO(Liuetal.,2023)andthenextract
themaskusingSegmentAnything(Kirillovetal.,2023). Forthetargetingvariableswappingprocess,
wedo30forlatentimagefeaturez,20stepsforcross-attentionmap,25fortheself-attentionmap,
and10fortheself-attentionoutput,WeconductswappinginallU-Netlayer. Thereisnoadditional
7Concept Image Source Image Mask Ours Photoswap P2P PnP MasaCtrl BlipDiffusion DreamEdit
Figure4: Qualitativecomparisonwithdifferentbaselines. Notethatthosebaselinemethodswere
alreadyequippedwithsomecomponentsofSwapAnythingforprecisecontroloftheswappingregion.
PleasecheckSec.5.1fordetails.
Table 1: Human evaluation results. We show the human preference between results generated
by our method and the baseline methods. SS means Object Swapping, BP means Background
Preservation,andOQmeansOverallQuality. SGmeansObjectGesture. Forthebaselinemethods,
PSmeansPhotoswap;MCmeansMasaCtrl;BPmeansBlipDiffusion;DEmeansDreamEdit;CP
meansCopyPaste.
Ours PS Tie Ours P2P Tie Ours PnP Tie Ours MC Tie Ours BP Tie Ours DE Tie Ours CP Tie
SS 52.3 12.5 35.2 55.3 17.9 26.8 52.3 29.9 11.5 64.3 20.0 15.7 55.3 16.5 28.2 55.1 20.1 24.8 60.1 17.3 22.6
SG 44.5 34.0 21.5 49.5 32.0 18.5 55.5 33.0 11.5 65.3 18.8 15.9 41.5 39.6 18.9 44.3 18.8 36.9 54.3 20.5 25.2
BP 41.5 32.0 26.5 44.5 28.9 26.6 50.2 20.9 28.9 55.2 23.1 21.7 40.0 27.5 32.5 44.0 18.9 37.1 54.1 13.1 32.8
OQ 49.3 27.8 22.9 55.0 27.2 17.8 52.5 22.9 24.6 59.4 20.1 20.5 48.1 25.3 26.6 53.1 19.9 27.0 71.1 10.5 18.4
operationforsingle-object,partialobject,crossdomainswapping.Multi-objectswappingisachieved
byconductingswappingoperationonthepreviousswappedimage.
Dataset.Weconductedexperimentsonbothhumanandnon-humanobjects.Forhumanswapping,we
collectcelebritiesfrominternetsearches. Weemployedthesearchprompt: “aphotoof<target>”,
where < target > is the celebrity name. We collected images of 15 celebrities for the concept
learningprocess. Wealsocollected500imagescontaining1ormorepeopleasthesourceimages.
Fornon-humanobject,weincludeDreamEdit(Lietal.,2023b)datasetandmoreconceptsandits
correspondingsourceimagesfromGooglesearch. Intotal,weaggregated1,000images. Wewill
alsoreleasethecollecteddataset.
Baselines setting. Photoswap(Gu et al., 2023a), P2P(Hertz et al., 2022), PnP(Tumanyan et al.,
2023),MasaCtrl(Caoetal.,2023)areattentionvariablebasedimageeditingmethods,whichare
alsocompatiblewithourproposedMaskedLatentBlendinginSec.4.1andLocationAdaptationin
Sec.4.2. Thereforeweboosttheirperformancewiththoseadditionalcomponents. Withoutthose,
theirperformancewouldbemuchworse. Pleaseseeappendixforcomparisonswiththeiroriginal
methods.Besides,weuseexternalmasktohelptheinpaintingprocessinDreamEdit.SwapAnythingis
alsocomparedwithBlipDiffusion(Lietal.,2023a). Photoswap,P2P,PnP,andMasaCtrl,DreamEdit
wereequippedwiththesameDreamBoothmodeltograspthenewconcept. Notethatthiswouldalso
indirectlyincludecomparisonwithCustomEdit(Choietal.,2023),sinceitalsoachievedpersonalized
objectswappingviaequipingP2Pwithconceptlearning. CopyPasteinvolvesdirectlytransplanting
theconceptobjectintheconceptimageintothesourceobject’sposition.
5.2 Single-objectSwapping
Weconsiderhumanevaluationtobethemainquantitativeperformancemeasurement. Asuccessful
swapshouldkeepthenon-objectareaunchanged,changetheobjectidentitytotarget,andkeepthe
gesturethesameasthesourceobject. AsinTab.1,ourmodelconsistentlyoutperformsbaselines
8Table2: Automaticevaluationresults. SwapAnythingoutperformsallothermethodsacrossall
metrics.
Ours Photoswap P2P PnP MasaCtrl BlipDiffusion DreamEdit CopyPaste
DINO fore 0.61 0.55 0.47 0.49 0.29 0.44 0.52 0.56
CLIP fore 0.79 0.53 0.71 0.73 0.46 0.54 0.61 0.75
DINO back 0.79 0.75 0.68 0.64 0.71 0.71 0.76 0.77
CLIP back 0.89 0.86 0.75 0.70 0.67 0.76 0.82 0.79
Concept Source Image Ours Concept Source Image Ours
Figure 5: Multi-object swapping results of SwapAnything. Given different source images, we
swaptheidentityfromthesameconceptobject. Notethattheredcirclemeansthetargetobjecttobe
replaced. Thesamecolormeansapairofconceptandtargetforobjectswapping.
acrossallmetrics. Fig.4showsthequalitativecomparisonforbothhumanandnon-humanimages.
Thanks to the addition of our targeting variable swapping and location adaptation, all attention-
manipulation based baselines also achieved perfect background preservation and some level of
localized swapping result. SwapAnything yield a much better appearance adaptation result. For
example,inthefirstrow,thehumanfacialexpressionandthedirectionoftheeyesarefullyaligned
withthesourceimage.
Wealsoconductedautomaticevaluation. FollowingRuizetal.(2023);Lietal.(2023b);Guetal.
(2023a),weemploybothDINOandCLIP-Iastoolstoevaluatethequalityoftheimagesgenerated.
Thesetwometricsserveascomplementaryindicatorstotheresultsobtainedfromhumanevaluations.
AsinTable2,SwapAnythingoutperformsallotherbaselinesintermsofbothsubjectidentityswapping
andbackgroundpreservation,whichisconsistentwiththeresultsofhumanevaluation.
5.3 Multi-objectSwapping
AsisshowninFig.5,multi-objectswappingissimplyachievedviarepeatingsingle-objectswapping,
whichfurtherhighlightsitsversatilityandefficiency. Multi-objectswappingisanaturaloutcomeof
ourtargetedvariableswapping,whereaspreviousmethodsstruggletoachievesatisfactoryresults.
Withoutperfectcontextpixelpreservation,theunwantedimagemodificationwouldaccumulateas
theswappingcontinues. Pleasechecktheappendixformoreresultsandcomparisonwithbaselines.
5.4 PartialObjectSwapping
AsisshowninFig.6,SwapAnythingachievedagreatperformanceonswapapartofawholeobject,
evenwhenthetargetingareaisverysmall. Meanwhile,allotherbaselinesfailedtoachievesuch
results. Pleasecheckappendixformoreresults.
Concept Source Image Ours Concept Source Image Ours Concept Source Image Ours
Figure6: Resultsonpartialobjectswapping. SwapAnythingcanswappartialobjectthatistightly
connectedwithotherpartsandadaptseamlesslytothesourceimage. Thesecondrowisthezoom-in
imagesoftheswappingpartinthefirstrow.
9Concept Source Image Ours Concept Source Image Ours Concept Source Image Ours
Figure7: Resultsoncross-domainobjectswapping. Withavarietyofsourceimages,including
graphic,black-whitephotos,andoilpaintings,theframeworkseamlesslyintegratesconceptobjects
takenfromregularinternetimagesintothesediversesourceimages.
5.5 Cross-domainSwapping
Fig.7demonstratesthatSwapAnythingcanadeptlyhandlearangeofstylizedsourceimages,success-
fullyadaptingconceptobjectstomatchthedesiredstylewithinthesourceimagewhileseamlessly
transferringidentityintothegeneratedimages. Forinstance, inthefourthrow, thesourceimage
is an oil painting photo, yet SwapAnything skillfully generates the same painting style featuring
personalities like “Justin Bieber” and “Jackie Chan”. Notably, all the concept images of “Justin
Bieber”and“JackieChan”areregular,unstyledphotos,underscoringthemodel’sabilitytoblend
differentstylesandidentitieseffectively. BesidesswappingtaskslistedinSec.5, SwapAnything
isalsocapableofothertaskssuchasswappingbackground. Pleaserefertotheappendixformore
results.
5.6 Text-basedSwapping
AsshowninFig.8,besidespersonalizedswapping,ourmethodcanalsodotext-basedswapping,
swapping an object in the source image with another described in text. This can be achieved by
simplyreplacingthepersonalizedconcepttoken∗withatextprompt,e.g.,“Aphotoofnew_obj”.
Source Image Lion Tiger Cat Puppy
Figure8: Resultsontext-basedswapping. Besidesswappingfromconceptimage,SwapAnythingis
alsocapableofswappinganobjectdescribedintextintotheimage.
5.7 ObjectInsertion
SwapAnythingisageneralframeworkandisalsocapableofobjectinsertion.Withthesameprocessas
single-objectswapping,wecouldinsertandadaptaconceptintobackgroundpixels,whilepreserving
thecompositionandstyleofthesourceimage. InFig.9,weinsertapuppyandabutterflyintoThe
StarryNightfromVincentvanGogh.
Concept Concept
Source Image Ours Ours
Figure9: Resultsonobjectinsertion. SwapAnythingcaninsertandadaptanobjectintoacertain
locationofanimage.
10Concept
Source Image Ours N Feo a L tua rt ee n St wap N Vao r iA at bt le en Sti wo an p No Any Swap N Ado a S pt ty ale ti on N Ado a S pc ta al te io n N Ado a C po tan tt ie on nt N Wo it A h n My aA sd kaptation;N No o A Mn ay s kAdaptation;
Figure10: Ablationstudy. Theleftpartshowstheeffectofswapping,andtherightpartshowsthe
effectofadaptationandmask. Pleaserefertotheappendixformoreresults.
5.8 AblationStudy
Fig.10showtheeffectofthecomponentsinSwapAnything. Fromtheleftpart,weseethatwithout
latentfeatureswap,evenwithamaskandattentionvariableswap,thecontextpixelsuchasclothes
is still changed. Both latent feature and attention variable has effect of information preservation
whencomparedwiththeresultofnoswap. Theeffectofadaptationandmaskispresentedonthe
right part. With style adaptation, the visual texture is closer to the source image. Without scale
adaptation,thefaceshapeisnotwellalignedandartifactsappearintheneckpart. Withoutcontent
adaptation,artifactssuchasahandtouchingthechinappearontheneckintheimage. Whenwithout
anyadaptation,thegeneratedimageismuchlessconnectedthesourceimageregardingtheswapping
area. Whenwithoutmask,bothbackgroundandtargetingareaarechanged,whichleadstoadifferent
image. Also,withouttheswappingandtheadaptationproposedinSwapAnything,theeditedimage
wouldhavestongvisualdistortionafterreshapetothesizeofsourceimage,sowepresentthosetwo
imageswithmodel’soutputsize.
5.9 EthicsDiscussion
Existingtext-to-imagediffusionmodels(Ruizetal.,2023;Rombachetal.,2022)canpotentially
exhibitbiasesreflectiveofthoseinherentinthetrainingdata. Giventhatthesemodelsaretrained
on vast text and image datasets, they might inadvertently learn and perpetuate biases. However,
SwapAnythingaimstomitigatesuchbiasesbyincorporatingstyleandcontentadaptations,asdetailed
inSec.4.2. Nevertheless,toensureauthenticity,werecommendusingSwapAnythingwithobjectsof
thesamegenderandsimilarracialbackground. Moreover,ourintentionforSwapAnythingistoserve
humanitarianpurposes,suchascreatinggroupphotoswithdeceasedfamilymembers. Westrictly
prohibitthecreationofharmfulcontent.
6 Conclusion
Inconclusion, SwapAnythingrepresentsanotablebreakthroughintherealmofobjectswapping.
Swapping latent features and attention variables in the diffusion model ensures the retention of
crucial information within the generated image. Through a targeted manipulation, we achieved
a perfect background preservation. Additionally, we have introduced a sophisticated appearance
adaptationprocessdesignedtoseamlesslyintegratetheconceptintothecontextofthesourceimage.
Consequently,SwapAnythingisequippedtohandleadiversearrayofobjectswappingchallenges. In
thefuture,weplantoextendourframeworkto3D/videopersonalizedobjectswappingtasks.
References
Avrahami,O.,Fried,O.,andLischinski,D.(2023). Blendedlatentdiffusion. ACMTransactionsonGraphics
(TOG),42(4),1–11.
Blattmann,A.,Rombach,R.,Oktay,K.,Müller,J.,andOmmer,B.(2022). Retrieval-AugmentedDiffusion
Models. InNIPS.
Cao,M.,Wang,X.,Qi,Z.,Shan,Y.,Qie,X.,andZheng,Y.(2023). Masactrl:Tuning-freemutualself-attention
controlforconsistentimagesynthesisandediting. InICCV.
Chen,H.,Zhang,Y.,Wang,X.,Duan,X.,Zhou,Y.,andZhu,W.(2023a). Disenbooth: Identity-preserving
disentangledtuningforsubject-driventext-to-imagegeneration.
11Chen,W.,Hu,H.,Li,Y.,Rui,N.,Jia,X.,Chang,M.-W.,andCohen,W.W.(2023b).Subject-driventext-to-image
generationviaapprenticeshiplearning. arXiv.
Choi, J., Choi, Y., Kim, Y., Kim, J., and Yoon, S. (2023). Custom-edit: Text-guided image editing with
customizeddiffusionmodels. arXivpreprintarXiv:2305.15779.
Crowson,K.,Biderman,S.,Kornis,D.,Stander,D.,Hallahan,E.,Castricato,L.,andRaff,E.(2022).Vqgan-clip:
Opendomainimagegenerationandeditingwithnaturallanguageguidance. InECCV.
Deng,Y.,Tang,F.,Dong,W.,Ma,C.,Pan,X.,Wang,L.,andXu,C.(2022). Stytr2:Imagestyletransferwith
transformers. InCVPR.
Ding,M.,Yang,Z.,Hong,W.,Zheng,W.,Zhou,C.,Yin,D.,Lin,J.,Zou,X.,Shao,Z.,Yang,H.,etal.(2021).
Cogview:Masteringtext-to-imagegenerationviatransformers. NIPS.
Epstein,D.,Jabri,A.,Poole,B.,Efros,A.A.,andHolynski,A.(2023). Diffusionself-guidanceforcontrollable
imagegeneration. AdvancesinNeuralInformationProcessingSystems.
Feng,W.,He,X.,Fu,T.-J.,Jampani,V.,Akula,A.,Narayana,P.,Basu,S.,Wang,X.E.,andWang,W.Y.(2023).
Training-FreeStructuredDiffusionGuidanceforCompositionalText-to-ImageSynthesis. InICLR.
Gal,R.,Alaluf,Y.,Atzmon,Y.,Patashnik,O.,Bermano,A.H.,Chechik,G.,andCohen-Or,D.(2023). An
ImageisWorthOneWord:PersonalizingText-to-ImageGenerationusingTextualInversion. InICLR.
Gu,J.,Wang,Y.,Zhao,N.,Fu,T.-J.,Xiong,W.,Liu,Q.,Zhang,Z.,Zhang,H.,Zhang,J.,Jung,H.,andWang,
X.E.(2023a). Photoswap:Personalizedsubjectswappinginimages.
Gu,Y.,Zhou,Y.,Wu,B.,Yu,L.,Liu,J.-W.,Zhao,R.,Wu,J.Z.,Zhang,D.J.,Shou,M.Z.,andTang,K.
(2023b). Videoswap:Customizedvideosubjectswappingwithinteractivesemanticpointcorrespondence.
arXivpreprintarXiv:2312.02087.
Hertz,A.,Mokady,R.,Tenenbaum,J.,Aberman,K.,Pritch,Y.,andCohen-or,D.(2022). Prompt-to-prompt
imageeditingwithcross-attentioncontrol. InTheEleventhInternationalConferenceonLearningRepresen-
tations.
Huang,X.,Liu,M.-Y.,Belongie,S.,andKautz,J.(2018). Multimodalunsupervisedimage-to-imagetranslation.
InECCV.
Jahn,M.,Rombach,R.,andOmmer,B.(2021). High-resolutioncomplexscenesynthesiswithtransformers.
arXiv.
Jia,X.,Zhao,Y.,Chan,K.C.,Li,Y.,Zhang,H.,Gong,B.,Hou,T.,Wang,H.,andSu,Y.-C.(2023). Taming
encoder for zero fine-tuning image customization with text-to-image diffusion models. arXiv preprint
arXiv:2304.02642.
Karras, T., Laine, S., and Aila, T. (2019). A style-based generator architecture for generative adversarial
networks. InCVPR.
Kirillov,A.,Mintun,E.,Ravi,N.,Mao,H.,Rolland,C.,Gustafson,L.,Xiao,T.,Whitehead,S.,Berg,A.C.,Lo,
W.-Y.,Dollar,P.,andGirshick,R.(2023). Segmentanything. InProceedingsoftheIEEE/CVFInternational
ConferenceonComputerVision(ICCV),pages4015–4026.
Li, D., Li, J., and Hoi, S. C. (2023a). Blip-diffusion: Pre-trained subject representation for controllable
text-to-imagegenerationandediting. AdvancesinNeuralInformationProcessingSystems.
Li, T., Ku, M., Wei, C., andChen, W.(2023b). Dreamedit: Subject-drivenimageediting. arXivpreprint
arXiv:2306.12624.
Li,Y.,Liu,H.,Wu,Q.,Mu,F.,Yang,J.,Gao,J.,Li,C.,andLee,Y.J.(2023c). Gligen: Open-setgrounded
text-to-imagegeneration. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition,pages22511–22521.
Liao,J.,Yao,Y.,Yuan,L.,Hua,G.,andKang,S.B.(2017). Visualatributetransferthroughdeepimageanalogy.
ACMTransactionsonGraphics.
Liu,S.,Lin,T.,He,D.,Li,F.,Wang,M.,Li,X.,Sun,Z.,Li,Q.,andDing,E.(2021). Adaattn:Revisitattention
mechanisminarbitraryneuralstyletransfer. InICCV.
Liu,S.,Zeng,Z.,Ren,T.,Li,F.,Zhang,H.,Yang,J.,Li,C.,Yang,J.,Su,H.,Zhu,J.,etal.(2023).Groundingdino:
Marryingdinowithgroundedpre-trainingforopen-setobjectdetection. arXivpreprintarXiv:2303.05499.
Meng,C.,He,Y.,Song,Y.,Song,J.,Wu,J.,Zhu,J.-Y.,andErmon,S.(2022). SDEdit:Guidedimagesynthesis
andeditingwithstochasticdifferentialequations. InInternationalConferenceonLearningRepresentations.
12Mokady,R.,Hertz,A.,Aberman,K.,Pritch,Y.,andCohen-Or,D.(2023). Null-textinversionforeditingreal
imagesusingguideddiffusionmodels. InProceedingsoftheIEEE/CVFConferenceonComputerVisionand
PatternRecognition,pages6038–6047.
Nichol, A.Q., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., Mcgrew, B., Sutskever, I., andChen, M.
(2022). Glide:Towardsphotorealisticimagegenerationandeditingwithtext-guideddiffusionmodels. In
InternationalConferenceonMachineLearning,pages16784–16804.PMLR.
Park,T.,Liu,M.-Y.,Wang,T.-C.,andZhu,J.-Y.(2019). Semanticimagesynthesiswithspatially-adaptive
normalization. InProceedingsoftheIEEE/CVFconferenceoncomputervisionandpatternrecognition,
pages2337–2346.
Patashnik,O.,Garibi,D.,Azuri,I.,Averbuch-Elor,H.,andCohen-Or,D.(2023). Localizingobject-levelshape
variationswithtext-to-imagediffusionmodels. InProceedingsoftheIEEE/CVFInternationalConferenceon
ComputerVision(ICCV).
Rombach,R.,Blattmann,A.,Lorenz,D.,Esser,P.,andOmmer,B.(2022). High-resolutionimagesynthesiswith
latentdiffusionmodels. InCVPR.
Ronneberger, O., Fischer, P., and Brox, T. (2015). U-net: Convolutional networks for biomedical image
segmentation. InMICCAI.Springer.
Ruiz,N.,Li,Y.,Jampani,V.,Pritch,Y.,Rubinstein,M.,andAberman,K.(2023). DreamBooth:FineTuning
Text-to-ImageDiffusionModelsforSubject-DrivenGeneration. InCVPR.
Seo, J., Lee, G., Cho, S., Lee, J., and Kim, S. (2023). Midms: Matching interleaved diffusion models
forexemplar-basedimagetranslation. InProceedingsoftheAAAIConferenceonArtificialIntelligence,
volume37,pages2191–2199.
Shi,J.,Xiong,W.,Lin,Z.,andJung,H.J.(2023). Instantbooth:Personalizedtext-to-imagegenerationwithout
test-timefinetuning.
Simsar,E.,Tonioni,A.,Xian,Y.,Hofmann,T.,andTombari,F.(2023). Lime: Localizedimageeditingvia
attentionregularizationindiffusionmodels. arXivpreprintarXiv:2312.09256.
Song,J.,Meng,C.,andErmon,S.(2020). Denoisingdiffusionimplicitmodels. InInternationalConferenceon
LearningRepresentations.
Tewel, Y., Gal, R., Chechik, G., and Atzmon, Y. (2023). Key-locked rank one editing for text-to-image
personalization. InACMSIGGRAPH2023ConferenceProceedings,SIGGRAPH’23.
Tumanyan,N.,Geyer,M.,Bagon,S.,andDekel,T.(2023). Plug-and-playdiffusionfeaturesfortext-driven
image-to-imagetranslation. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition,pages1921–1930.
Wang,M.,Yang,G.-Y.,Li,R.,Liang,R.-Z.,Zhang,S.-H.,Hall,P.M.,andHu,S.-M.(2019). Example-guided
style-consistentimagesynthesisfromsemanticlabeling. InCVPR.
Wang,Q.,Bai,X.,Wang,H.,Qin,Z.,andChen,A.(2024). Instantid:Zero-shotidentity-preservinggeneration
inseconds. arXivpreprintarXiv:2401.07519.
Yang,B.,Gu,S.,Zhang,B.,Zhang,T.,Chen,X.,Sun,X.,Chen,D.,andWen,F.(2023a). Paintbyexample:
Exemplar-based image editing with diffusion models. In Proceedings of the IEEE/CVF Conference on
ComputerVisionandPatternRecognition,pages18381–18391.
Yang,Z.,Wang,J.,Gan,Z.,Li,L.,Lin,K.,Wu,C.,Duan,N.,Liu,Z.,Liu,C.,Zeng,M.,etal.(2023b). Reco:
Region-controlledtext-to-imagegeneration.InProceedingsoftheIEEE/CVFConferenceonComputerVision
andPatternRecognition,pages14246–14255.
Zeng,Y.,Lin,Z.,Zhang,J.,Liu,Q.,Collomosse,J.,Kuen,J.,andPatel,V.M.(2023).Scenecomposer:Any-level
semanticimagesynthesis. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition,pages22468–22478.
Zhang,L.,Rao,A.,andAgrawala,M.(2023). Addingconditionalcontroltotext-to-imagediffusionmodels.
Zhang, P., Zhang, B., Chen, D., Yuan, L., andWen, F.(2020). Cross-domaincorrespondencelearningfor
exemplar-basedimagetranslation. InCVPR,pages5143–5153.
Zhang,Y.,Huang,N.,Tang,F.,Huang,H.,Ma,C.,Dong,W.,andXu,C.(2022). Inversion-basedcreativity
transferwithdiffusionmodels. arXiv.
Zhou,X.,Zhang,B.,Zhang,T.,Zhang,P.,Bao,J.,Chen,D.,Zhang,Z.,andWen,F.(2021). Cocosnetv2:
Full-resolutioncorrespondencelearningforimagetranslation. InCVPR.
13Inthesupplementarymaterial,wedescribethemethodindetailandpresentmorequantitativeand
qualitativeresultsonvariousobjectswappingtasks. Weadditionallyshowthecapacityoftext-based
swappingandobjectinsertion. Finally,weintroducethefailurecasesandhumanevaluationinterface.
A VariableSwappingDetails
WeuseStableDiffusion2.1asthepre-trainedtext-to-imagediffusionmodel. DreamBooth(Ruiz
etal.,2023)isusedtoconverttheconceptintotextualspace. Thelearningrateforthisprocessisset
at1e-6,andweusetheAdawmoptimizerfor800steps. TheU-netandthetextencoderarefine-tuned
duringthisprocess, typicallytakingabout2minutesonamachineequippedwith8A100GPUs.
Thetargetpromptisessentiallythesourcepromptwithaswapinobjecttokenstointroduceanew
concept.
Forareamasksmooth,wefirstenlargethemaskedareasusingadilationoperationwithanelliptical
kernel,whichcanbeadjustedinsize. Afterdilation,themaskedgesaresmoothedusingaGaussian
blur, creatingagradienteffectattheboundaries. Forthesmoothoverdiffusionstep, welinearly
increasethemaskratefrom0to1duringthefirst30steps. Forabetterunderstanding,wemarkthe
maskedareausingacircleinmostfiguresinthispaper.
B AdaptationDetails
StyleAdaptation. Thisoperationadjuststhemeanandvarianceofcontentimagefeaturestomatch
thoseofthestylefeatures,facilitatingthetransferofartisticstylesontocontentimages. TheAdaIN
techniqueisrenownedforitsefficiencyandflexibility,makingitago-tochoiceforreal-timestyle
transfer and artistic image manipulation. Building on this, we introduce Masked-AdaIN. Unlike
traditionalAdaINwhichappliesstylealignmentacrosstheentireimage,Masked-AdaINfocuses
thisalignmentonlyonaspecifictargetarea. Inthisapproach,meanandvariancecalculationsare
exclusivelyperformedonthedesignatedmaskedarea,allowingformorepreciseandlocalizedstyle
transfers.
ScaleAdaptation. Weadaptthescaleoftheobjectinlatentspacetotheshapeofthemask. The
objectshapeisindicatedinthecross-attentionmapateachdiffusionstep(Guetal.,2023a;Hertz
etal.,2022). Shape(M )(k)meanstheattentionmapforobjecttexttokenk,whichisobtained
src
through binary-like transformation to the attention map. We apply a threshold of 0.4 after using
sigmoidtonormalizetheattentionvaluebetween0and1.
ContentAdaptation. IntheLinearBoundaryInterpolationprocess,thestructuringelementK isa
predefinedshapeusedinthedilationprocesstocreatethedilatedimage. Thestructuringelement
K slidesoverthebinarymaskM andateachposition. IfatleastonepixelunderK is1, the
src
pixelintheoutputimageunderthecenterofK issetto1. Thisoperationtypicallyresultsinthe
enlargementoftheregionswith1sinthebinarymask,effectivelysmoothingtheboundaryandfilling
smallholesandgaps.ThesubsequentconvolutionwithaGaussiankernelGfurthersmoothsthemask
byaveragingvaluesinthevicinityofeachpoint,therebycreatingagradienteffect. Thecombination
ofdilationandGaussiansmoothingpreparesthemaskS′forlinearboundaryinterpolation,where
thesharptransitionsaremadegradual,andthefinalsoftmaskS′isobtainedbyselectivelysetting
pixelsto1basedontheoriginalmaskandthesmoothedvalues. InGradualBoundaryTransition,we
setthetransitionstepparameteras30toannealM from0tothesetvalue.
src
C MoreQualitativeResults
Herewefirstshowthecomparisonwithbaselinesintheiroriginalsettingonsingle-objectswapping.
Onothermorechallengingtasks,wealsoshowtheresultsofPhotoswapsinceitisthestate-of-the-art
methodofsubjectswapping.
Single-objectSwapping. Fig.11showscomparisonsbetweenSwapAnythingandbaselines. Swa-
pAnything consistently outperforms other models in terms of background preservation, identity
swapping,andoverallquality. Notethatthereisalsoahugeperformancegapbetweensomebaselines
andtheircounterpartinFig. 4inthemainpaper, whichfurthervalidatestheefficacyoftargeted
variableswappingandlocationadaptation,whichwasappliedtoPhotoswap,P2P,PnP,andMasaCtrl
inFig. 4inthemainpaper.
14Concept Source Image Mask Ours Photoswap P2P PnP MasaCtrl BlipDiffusion DreamEdit
Figure11: Comparisononsingle-objectswappingwithbaselinesintheiroriginalcomponents.
SSmeansObjectSwapping,BPmeansBackgroundPreservation,andOQmeansOverallQuality.
SGmeansObjectGesture. Pleasezoominforaclearvisualresult.
Concept Source Image Ours Photoswap Photoswap+Mask
Concept Source Image Ours Photoswap Photoswap+Mask
Figure12: ComparisonwithPhotoswaponpartialobjectswappingandcross-domainswapping.
TheupperpartshowsSwapAnythingcouldlocalizetheswappingareawhilePhotoswapinevitably
modifiedthebackground. Inthelowerpart, SwapAnythingadaptsabearintothestyleofalogo,
whilePhotoswapfailedonthiscross-domainswappingtask.
PartialObjectSwapping. AsinFig.12,ourmethodpreciselyswapsthecatheadwitharaccoon
headharmoniouslywithoutinfluencingotherpixels. Meanwhile,Photoswapswapsthewholebody
andmodifiedthecontextpixels. Whenourproposedmaskedvariableswappingisadded,Photoswap
achievesabetterbackgroundpreservationperformance.
Cross-domainSwapping. SwapAnythingiscapableofswappingbetweenstylesandtextual. In
Fig.12,abearisadaptedintoalogowhilekeepingthegestureofthesourceobjecthorse. Meanwhile,
Photoswapfailstocompletethechallengingtask. Also,whenmaskedvariableswappingisadded,
Photoswapachievesabetteradaptationperformance.
Multi-objectSwapping. Multi-objectswappingisabigstepaftersingle-objectswapping. Herein
Fig.13,weshowtwomainobstaclesweresolved. First,previousmethodsusuallyhaveabackground
modificationsuchthatcontinuouseditingwouldaccumulateunwanteddistortion,whichleadstoa
totallydifferentimageandfailsthetaskofswapping. Thesecondissueisthatpreviousmethodsare
15Issue 1:
Context Distortion
Photoswap
Ours
Issue 2:
Object Disappearance
Photoswap
Ours
Figure 13: Comparison between SwapAnything and Photoswap on multi-object swapping.
Photoswapusuallyhasalargemodificationonthecontextpixelandcannotpreciselykeepallobjects
soitfailstoconductcontinuousswapping.
usuallydesignedformainsubjectswappinganddonotpayattentiontootherobjects. Inthiscase,the
objectsinthefollowingswappingstepscoulddisappearinthepreviousswappingprocess.
AdditionalAblationStudyWepresentonemoreexampletoshowtheeffectofvariouscomponents
inSwapAnything. Thiscross-domainswappingexampleismorechallengingthantheoneinthemain
paper,whichfurthershowstheimportanceofeachmodule.
D MoreQualitativeResults
Tab.3showsresultsonhumanevaluationonbothhumanandnon-humanimagesontopbaselines.
PSmeansPhotoswap(Guetal.,2023a); P2PmeansPrompt-to-Prompt(Hertzetal.,2022); PnP
meansPlug-and-Play(Tumanyanetal.,2023);DEmeansDreamEdit(Lietal.,2023b). Wealso
conductcomparisonswithanotherbaselinePbE,Paint-by-Example(Yangetal.,2023a).
16Concept Source Image Ours No Latent No Attention No Style No Scale No Content No Any Adaptation; No Any Adaptation;
Feature Swap Variable Swap Adaptation Adaptation Adaptation With Mask No Mask
Figure14: Additionalablationstudy. Theswappingareaisthecompasslogointhemiddleofthe
bag.
Table3: Userstudyresults. The2ndto5throwsand6thto9throwsshowresultsonhumanobjects
andnon-humanobjects.
Ours PS Tie Ours P2P Tie Ours PnP Tie Ours DE Tie Ours PbE Tie
SS 59.0 10.0 31.0 52.7 18.2 24.1 58.8 29.2 12.0 53.4 16.5 30.1 62.1 12.0 28.0
SG 44.0 33.7 22.3 54.5 29.1 16.4 61.6 33.3 5.1 42.4 17.2 40.4 73.1 15.8 12.0
BP 45.4 32.2 22.4 49.9 26.9 23.2 49.7 22.0 28.3 43.8 18.0 38.2 42.1 31.3 27.0
OQ 47.3 24.3 28.4 58.4 23.3 18.3 51.6 31.1 17.3 47.5 26.5 26.0 52.1 21.9 30.0
SS 45.6 15.0 39.4 52.9 17.6 29.5 45.8 30.6 23.6 56.8 23.7 19.5 58.1 12.0 27.8
SG 45.0 34.3 20.7 44.5 34.9 20.6 49.4 32.7 17.9 46.2 20.4 33.4 69.3 15.0 14.8
BP 37.6 31.8 30.6 39.1 30.9 30.0 50.7 19.8 29.5 44.2 19.8 36.0 40.5 31.5 27.6
OQ 51.3 31.3 17.4 51.6 31.1 17.3 47.5 26.5 26.0 48.1 21.2 30.7 48.1 18.3 29.6
E FailureCases
Wehighlighttwocommonfailurescenariosencounteredinourexperiments. Firstly,althoughour
model adeptly preserves contextual pixels, it struggles with accurately reconstructing non-target
pixelswithinthemaskarea. ThisissueisillustratedinthefirstrowofFigure15,thepositionofring
inthesourceimageischangedinthetargetimage. Thesecondchallengeariseswhendealingwith
subjectsthatexhibitahighdegreeofvariabilityorfreedomofmovement. Insuchcases,asshownin
thebottomrowofthefigure,accuratelyreplicatingtheconceptsubjectbecomesdifficult. Toaddress
this,weareconsideringtheimplementationofexplicitalignment,whichweaimtoexploreinour
futurework.
F HumanEvaluationInterface
AmazonTurkerwaspresentedwithonereferenceimagemainlycontainingtheconceptsubject,one
sourceimagetobeswapped,andtwogeneratedimagesfromSwapAnythingandabaseline.
17Concept Source Image Ours
Figure15: Examplesoffailurecases. Themodelsometimesstrugglestokeepthedetailsinsidethe
maskareaandcouldfailiftheobjecthasahighdegreeoffreedom.
Figure16: Theillustrationoftheuserstudyinterface.
18