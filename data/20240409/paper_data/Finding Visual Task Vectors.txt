Finding Visual Task Vectors
AlbertoHojel1 YutongBai1 TrevorDarrell1 AmirGloberson2 AmirBar1,2
1UCBerkeley 2TelAvivUniversity
Figure 1: Visual Prompting models like MAE-VQGAN [4] require an input-output example to
describethedesiredtaskintheirforwardpass. Weanalyzethemodelactivationsandfindtaskvectors,
activationsthatencodetaskinformationthatcanbereusedtocontrolthetaskthemodelperforms(see
Figurea). Specifically,wetapintoactivationsofindividualattentionheadsandreplacetheiroutputs
withtaskvectorstoguidethemodeltothedesiredtask(seeFigureb). Surprisingly,theresulting
modelsperformbetterthantheoriginalmodelwhilereducingtheneedforinput-outputexamples.
Thisconfirmsthattaskvectorsexistinthenetworkactivationspaceandtheycanguidethemodelto
performthedesiredtask.
Abstract
VisualPromptingisatechniqueforteachingmodelstoperformavisualtaskvia
in-context examples, without any additional training. In this work, we analyze
theactivationsofMAE-VQGAN,arecentVisualPromptingmodel[4],andfind
taskvectors,activationsthatencodetask-specificinformation. Equippedwiththis
insight,wedemonstratethatitispossibletoidentifythetaskvectorsandusethem
to guide the network towards performing different tasks without providing any
input-outputexamples. Tofindtaskvectors,wecomputetheaverageintermediate
activationspertaskandusetheREINFORCE[43]algorithmtosearchforthesubset
oftaskvectors. Theresultingtaskvectorsguidethemodeltowardsperforminga
taskbetterthantheoriginalmodelwithouttheneedforinput-outputexamples.1
1 Introduction
In-context learning (ICL) is an emergent capability of large neural networks, first discovered in
GPT-3[6]. ICLallowsmodelstoadapttonoveldownstreamtasksspecifiedintheuser’sprompt. In
computervision,VisualICL(knownasVisualPrompting[4,2,3,51])isstillatitsinfancybutitis
increasinglybecomingmorepopularduetotheappealofusingasinglemodeltoperformvarious
downstreamtaskswithoutspecificfinetuningorchangeinthemodelweights.
1Forcodeandmodelsseewww.github.com/alhojel/visual_task_vectors
4202
rpA
8
]VC.sc[
1v92750.4042:viXraInthiswork,weaskhowin-contextlearningworksincomputervision. Whilethisquestionhasyet
tobeexplored,therehasbeenasignificantbodyofresearchinNaturalLanguageProcessing(NLP)
tryingtoexplainthisphenomenon[1,7,13,14,15]. Mostrecently,Hendeletal.[17]suggestedthat
LLMsencodeTaskVectors,thesearevectorsthatcanbepatchedintothenetworkactivationspace
andreplacetheICLexampleswhileresultinginasimilarfunctionality. Concurrently,Toddetal.[38]
discoveredFunctionVectors,activationsoftransformerattentionheadsthatcarrytaskrepresentations.
OurworkisinspiredbytheseobservationsandaimstostudyhowICLworksincomputervision.
Cantaskvectorsexistincomputervisionmodelstoo? Tobuildintuition,westartbyexploringthe
activation space of MAE-VQGAN [4]. Intuitively, we look for intermediate activations that are
invarianttochangeswithinatask,buthavehighvarianceacrossdifferenttasks. Weusethissimple
metrictorankactivationsaccordingtotheir“taskness”. Estimatingthisscoreonlyrequiresapplying
theforwardpassofamodeloverminibatchesofdataandthusitisveryfasttocompute. Usingthis
score,wefindthattheactivationspaceofcertainattentionheadscanperfectlyclusterthedataby
tasks,hintingtheexistenceofvisualtaskvectors.
Basedonthisinsight,wehypothesizethatVisualICLmodelscreatetaskvectorstoo,andaimto
findthem. However,findingvisualtaskvectorsbyrelyingonpreviousapproachesischallenging.
Forexample,both[17,38]restrictedtheirsearchspacetotheoutputactivationsofthelasttokenin
thepromptsequence(coloredinredinthefollowingexample: ‘‘Banana:B, Apple:’’). Thisis
naturalfortextasitisprocessedsequentiallybyautoregressivemodelslikeLLaMA[39],butwith
images(seeFigure1,“VisualPrompting”inputimage),itisnotobviouswhattokenactivationshold
taskinformation,andarchitectureslikeMAE-VQGAN[4]donotprocessimagetokenssequentially.
Thisaloneincreasesthesearchspacesignificantlybecausemultipletokensmightholdtaskvectors.
Tofindthesetoftaskvectors,weconditionthemodelonqueryonly(seeFigure1a,bottomrow)
andsearchforasubsetofmeantaskactivationsthatcanbepatchedinselfattentionheadstoguide
the model towards performing the desired task using REINFORCE [43]. Patching the identified
taskvectors(seeexampleinFigure1b)leadstocompetitiveperformancewiththeoriginalmodel
onarangeoftasks,whichconfirmsthatthesetofresultingactivationsareindeedtaskvectors. Our
approachdirectlymodelssearchingforasubsetofactivationscomparedtothepreviouslyproposed
CasualMediationAnalysis[32](CMA)thatsearcheseachactivationindependently.
Our contributions are as follows. We show evidence for the existence of visual task vectors and
proposeapracticalwaytoidentifythem. Moreover,wefindthatpatchingtheresultingtaskvectors
guidesthemodeltowardsthedesiredtaskwithsimilarperformancecomparedtotheoriginalmodel
whilereducingthenumberofFLOPSby22.5%.
2 RelatedWork
2.1 VisualPrompting
VisualPrompting[4,2,18,46,51,3]isaclassofapproachestoadaptcomputervisionmodelsto
downstreamtasks,inspiredbythesuccessofpromptinginNLP[6]. Approacheslike[2,18]seekto
improvetask-specificperformancebyaddingtrainablepromptvectorstothemodel. OtherVisual
Promptingapproachesallowamodeltohandlevariousvisiontasks[4,46,3,51]byintroducing
visualexamplesortextatthetimeofinference. Suchpromptingisrelatedtothewayin-context
learning [45, 42, 22, 25] operates in language models [33, 6, 41]. In fact, trainable prompts and
in-contextlearningcanbeviewedastwocomplementaryapproachesfor“describing”atasktoa
model[21]. OurgoalhereistobetterunderstandtheunderlyingmechanismofVisualICL,andwe
analyzetheMAE-VQGANmodelpresentedin[4].
2.2 Explainability
Inthecontextofenhancingmodelinterpretability[50,29,37]withincomputervision,theintegration
of Causal Interventions [5, 31] and Activation Patching [48] has become a cornerstone in the
elucidationofcomplexneuralnetworks’decision-makingprocesses. Thesemethodologiesenable
asystematicexaminationofhowmodelsencodeandutilizehigh-levelconcepts,offeringprofound
insights into their internal mechanisms[49, 44, 24]. Causal Interventions [32, 28], facilitate the
explorationofthecausalstructuresofmodelsbymanipulatingtheirinternalstatesorinputs[12]
and observing the impact on outputs, thus uncovering the direct and indirect effects that drive
2modelpredictions. HereweuseActivationPatching[48]toshowthroughtargetedinterventionsthe
significanceofTaskVectorstowardsguidingVisualPromptingmodelstoperformdifferentcomputer
visiontasks.
2.3 TaskVectors
In[17,38,11],aTaskVectororFunctionVectorisalatentactivationderivedfromaparticularlayer
ofthetransformer[40].Thisvectorsubsequentlysubstitutestheoriginallatentstatesatthesamelayer
duringtheforwardpassofaquerytoguidethemodeltoperformthedesiredtask. Theinvestigation
ofTaskVectorsalignswithbroadereffortsinthefieldtomakeneuralnetworksmoreadaptableand
tailoredtospecifictasks[23,26]aswellasboostingtheperformance[30,47,19]bygainingadeeper
understandingofhowdifferentlayerswithinamodelcontributetoitsoverallfunction. Ourworkis
thefirsttoexploretaskvectorsincomputervision.
3 Methods
Our goal is to understand in-context learning for computer vision, and how existing models can
be adapted to different downstream task in inference time. Specifically, we focus on the MAE-
VQGAN[4]model,avariantofMAE[16]withaVisionTransformer[9]encoder-decoderarchitec-
ture.
Givenaaninput-outputexample(x ,y )andanewqueryx ,tosucceedinthisICLtask,themodel
s s q
F hastoimplicitlyapplythetransformationfromx toy overx toproducey :
s s q q
y =F(x ,y ,x ) (1)
q s s q
BasedonpastobservationsfromNLP[17,38],wehypothesizethatcomputervisionmodelsencode
latenttaskvectorsintheiractivationspaceduringtheforwardpassaswell. Thisrequiresthemodel
toimplicitlymaptheinputexampleintoasetoflatenttaskvectorsz ={z }k . Then,theoriginal
i i=1
functionF canbedecomposedintoextractingthetaskvectorsbyafunctionGthenapplyingF on
thequerywhilefixingthecomputedtaskactivationsz:
z =G(x ,y ) y =F(x |z) (2)
s s q q
3.1 ScoringActivations
Intuitively,everytaskvectorisanactivationthatchangesacrossdifferenttasksbutremainsinvariant
tochangeswithinaspecifictask. Specifically,denotei=(l,m,k)asthelth attentionblock,mth
attentionhead,andkthtokenanddenoteitbyF thefunctionthatoutputsthecorrespondingactivation
i
intheattentionblockresidualstream. Wesampleaninput-outputexampleandaquerytripletfrom
individualtasksaswellasacrosstasksandcomputethelatentactivationscorrespondingtoi:
(x ,y ,x )∼D (x′,y′,x′)∼D
s s q all_tasks s s q task_j
zi =F (x ,y ,x ) zi =F (x′,y′,x′)
all i s s q taskj i s s q
Then,wedefinethescoreandthemeanactivationsforpositioniandtaskj:
var(zi )
ρ (i)= all µ =E[zi ] (3)
token 1 (cid:80)n var(zi ) i,j taskj
n j=1 taskj
Notethatsinceourendgoalistooperateina“zero-shot”setting(e.g,noinput-outputexamples),we
onlyconsiderthescoreandtaskvectorsthatcorrespondtotheCLStoken,theinputquery,andthe
output(presentinthedecoderonly). Also,computingthetokenscoreandmeanactivationsisvery
efficientasitsimplyrequiresapplyingtheforwardpassofaneuralnetworkacrossabatchwithout
anyaccesstoaheld-outvalidationset.
Importantly, while ρ (·) assigns high values to task vectors, it might also assign high values
token
for other potential activations, for example, an activation that captures the color histogram can
alsovaryacrosstasks(segmentationmapscomparedtonaturalimages)butremainsimilarwithin
eachindividualtask. Therefore,weusethisscoringmechanismmainlytobuildintuitionaboutthe
existenceoftaskvectors.
3Table 1: Task Clustering Quality. Clustering Scores of Different Attention Heads, ranked by
ourActivationScoringmetric(seeSection3). ThisindicatesthathigherActivationScoresindeed
correlatewithbetterclusteringbytasks.
(Layer,Head) OurScore SilhouetteScore Davies-BouldinScore
↑ ↑ ↓
High1 (26,3) 2.1663 0.3583 1.2744
High2 (11,3) 1.0827 0.2692 1.5567
High3 (11,13) 0.9849 0.2246 1.8084
High4 (22,3) 0.9448 0.2031 2.6842
Random1 (4,3) 0.2329 0.0708 4.1062
Random2 (18,16) 0.1259 0.0369 4.3256
Random3 (32,6) 0.3253 0.0827 2.9670
Random4 (11,14) 0.1196 0.0409 5.3884
Random5 (18,16) 0.1259 0.0369 4.3256
Low1 (2,16) 0.0221 -0.0518 21.8265
Low2 (2,12) 0.0264 -0.0334 13.5982
Low3 (27,5) 0.0281 -0.0280 15.4219
Finally,wecomputetheaggregatedper-layerscore:
(cid:88)
ρ (l)= ρ (i) (4)
layer token
i=(l,m,k)
Using this scoring function, we find that some activations capture “taskness” (see Figure 2) and
cluster the data well with respect to tasks (see Table 1). We further elaborate on this analysis in
Section4andSection5. Tofindvisualtaskvectors,weleverageamoregeneralapproachoutlinedin
thefollowingsection.
3.2 FindingVisualTaskVectorsviaREINFORCE
Howcantaskvectorsbeidentified? Anexhaustivesearchoverallsubsetsofactivationsisintractable.
Forexample,MAE-VQGAN[4]whichweutilizeherehas32attentionblocks,eachwith16heads,
therefore,ifweconsiderthesetobethepotentialsetofactivationsthenweneedtosearchover2512
options,evaluatingeachoptionoveraheld-outvalidationset.
Foreverytaskj wecanapplythefollowingproceduretofindthetaskvectors. Recallthat{µ }
i,j
denote the mean activations and F(·) is a pretrained visual prompting model. Denote α ∼
i,j
Bernoulli(σ(θ ))asrandomvariablesthatsignifywhetherthemeantaskactivationµ isatask
ij i,j
vectorandθ isalearnedweightfollowedbythesigmoidfunctiontoensureitisin[0,1].
ij
Denotez asthesetoftaskvectorsfortaskj: z = {µ |α = 1}. Givenpairsofinput-output
j j i,j i,j
taskdemonstrationsx ,y ∼D ,wewanttofindthesetoftaskvectorsthatminimizestheloss
q q task_j
function:
L(θ)=E L(y ,F(x |z )) (5)
zj∼pθ q q j
Where p denotes the sampling distribution of z and L denotes the loss function that suits the
θ j
particulartaskj. NotethatdifferentlythaninEquation1,ifwefindagoodsetoftaskvectorsz ,
j
thenwenolongerneedtoconditionF overadditionalinput-outputexamples.
Tofindasetoftaskvectors,weneedtoestimatetheparametersθ. Sincethesamplingdistributionp
θ
dependsonθ,itisnaturaltousetheREINFORCEalgorithm[43]. ThekeyideainREINFORCEis
theobservationthat:
∇L(θ)=E L(y ,F(x |z ))∇logp (z )
zj∼pθ q q j θ j
Thus,wecanapproximate∇L(θ)bysamplingz andaveragingtheaboveequation. Afteriteratively
j
optimizingwithgradientdescent,weselectthefinaltaskvectorpositionsbysamplingasetofz .
j
Thisprocedureisoutlinedforfindingthetaskvectorsplacementsforindividualtasks,howeveritis
possibletofindplacementsthatgeneralizeformultipletasksbyoptimizingoverexamplesacross
datasets and we refer this as “multi-task” patching. Additionally, while above we search for all
4Figure2: ActivationScoringAnalysis. Individualscores(ρ (i))aggregatedperAttentionHead
token
(left). Individualtokenscoresofspecificheadsarefurthervisualized, alongwiththet-SNE[27]
clusteringofheadactivationsofdifferenttasks(right).
5tokenactivations,itispossibletoapplythesearchindifferentlevelsofgranularity. Forexample,by
patchinggroupsoftokensfromthesamequadrant,patchingallthetokensinanattentionhead,or
patchingtheentirelayer. Wediscussthesedesignchoicesinthenextsection.
4 Experiments
Ourexperimentsaimtoexplorewhetheractivationpatchingoftaskvectorsinzero-shotenvironments
cancausethemodeltoexecutethedesiredvisualtask,hopefullyperformingsimilarly–orbetter–to
theoriginalmodel’sone-shotin-context-learningperformance. Inthissection,wedescribetheimple-
mentationdetailsofMAE-VQGAN,thepromptingschemesused,thedetailsofourimplementation,
thebaselinesusedforcomparison,thevisualtasksofinterest,andtheexperimentsconducted.
4.1 ImplementationDetails
MAE-VQGAN[4]. AnMAE [16]withaViT-L[9]backbone,whichhas24encoderblocksand8
decoderblocks,with16attentionheadsineverylayerandapatchsizeof16x16andinputresolution
of224x224. ThedecoderpredictsadistributionoveraVQGAN[10]codebooktooutputimages
withbettervisualquality. Weusedthepretrainedcheckpointfrom[4]trainedovertheComputer
VisionFigures[4]datasetandImageNet[35].
One-shotPrompting.Wefollowthebasicone-shotsetupin[4].Specifically,weconstructagrid-like
imagestructurewithaninput-outputdemonstration,aquery,andamaskedoutputregionwhichare
embeddedintoa2x2grid. Wefeedthisgridimagetothemodeltoobtaintheoutputpredictionwhich
weuseforevaluationpurposes.
Zero-shotPrompting. Similarlytoone-shotwiththekeydifferencethatonlythequeryisembedded
intothegridimage,inthesamebottomleftquadrantpositionasintheone-shotsetting. Themodelis
thenusedtoreconstructonlythepartthatcorrespondstotheoutput. Specifically,wepatchifythe
queryimageataresolutionof112x112andapplytoitthepositionalencodingsofthebottomleft
quadrant. Thepatchesarethenfedintotheencoderandareprocessedbythedecodertogetherwith
themasktokensthatcorrespondtothebottomrightquadranttoobtaintheresult. Notethatinthe
zero-shotsettingwealsopatchintermediateactivationsbytaskvectors.
CausalMediationAnalysis. WecompareourmethodologywiththeCausalMediationAnalysis
methodologyaspresentedin[38]asabaseline. Weselectthetop25%ofactivationswiththehighest
causalscoreacross10images.
Greedy Random Search. We compare our methodology to an iterative greedy random search
algorithm (GRS) used to select task vectors based on the activation scoring metric proposed in
Section3.1. ThisservesasabaselineandisoutlinedintheSupplementaryMaterials(Section10.1).
FindingTaskVectors. WeuseREINFORCE [43]toselectwheretopatch. Weinitializeθ with
ij
−1. EachpatchpositionmodeledasarandomvariablecorrespondstoeithertheCLStoken,ora
groupingoftheimagepatchtokensbasedontheirspatialpositioning: thebottomleftquadrant(the
imagepatchtokensthatcorrespondtothequery),andbottomrightquadrant(theMASKtokens). In
otherwords,werestrictthegranularityofthepatchingprocedure;insteadofhavinganindividual
randomvariableparameterforeachtokenpositioniwereducethedimensionalityofthesearchspace
bygroupingthetokensintothethreecategoriesmentioned. UponeachiterationofREINFORCE,we
sample32timesfromtheBernoullidistributionforeachimage(fixedat10forallexperiments)and
performapatchingprocedureintothepositionssampledwithavalueequalto1. Thisresultsina
totalof320executionsofzero-shotMAE-VQGANperiteration. Then,weoptimizetheBernoulli
parametersasoutlinedin3.2withAdam[20]usingalearningrateof0.1. Weexecutethealgorithm
foratotalof600stepsandselectthecheckpointatintervalsof50stepswiththebestevaluationona
held-outtestset.
4.2 ActivationScoringAnalysis
Here our goal is to evaluate in isolation the Activation Scoring step (outlined in Section 3.1),
specificallywhetherhighscoringactivationsindeedcorrespondtoTaskVectors.
6CollectingActivations. AsoutlinedinSection3.1,ourfirststeptowardscomputingactivationscores
istoruntheforwardpassofthemodelinaone-shotsettingtocollectactivationsacrossdifferent
tasks. Specifically,weuse100promptsandqueriesfromPascal5i[36]trainingset,ensuringthatthe
one-shotperformanceacrossalltasksworksreasonablywell. Duringtheforwardpass,wesavethe
activationsforeverytaskj andi=(l,m,k),whichcorrespondstotheintermediateactivationofthe
kthtokeninthelthblockafterthemthattentionhead. Afterwards,wecomputethemeanactivation
µ andscoreρ (i).
i,j token
Evaluation via Clustering. Next, we wish to analyze if ρ (i) indeed captures “taskness”.
token
Intuitively,weexpectlayersthatcapturetaskinformationtosucceedinclusteringactivationsbytask.
Toassessthis,weanalyzetheclusteringperformanceofvectorswithhigh-rankingactivationsversus
thosemarkedwithlowscores. Wemeasuretheclusteringperformanceusingcommonclustering
metricsliketheSilhouetteScore[34]andtheDavies-BouldinScore[8]. Finally,wealsoperforma
qualitativeanalysisbyvisualizingtherepresentationsonat-SNE[27]plot,coloringeachdatapoint
byit’stasklabel.
4.3 DownstreamTasks
Weevaluatetheperformanceonstandardimage-to-imagetaskslikeForegroundSegmentation,Low
LightEnhancement,In-painting,andColorization.
Dataset. WeusethePascal-5i[36]dataset,whichiscomprisedof4differentimagesplitswhere
everysplitcontainsbetween346and725imagesandassociatedsegmentationmasks. Forevaluation,
wesample1000prompt-querypairsfromeachsplitinthevalidationset. ForActivationScoringand
thevariousmethodstofindTaskVectorsweonlyuseexamplesfromthetrainingset.
ForegroundSegmentation. Formodelevaluations,weusethesegmentationmasksincludedinthe
Pascal-5i[36]dataset,andreportthemeanIOU(mIOU)metricacrossthefoursplits.
LowLightEnhancement. Toobtaintheinput-outputdata,givenaPascal-5i[36]image,wemultiply
thecolorchannelsbyafactorof0.5anddefinetheresultastheinputandtheoriginalimageasthe
output. WereporttheMeanSquaredError(MSE)metricofthepredictionwiththelabel.
Inpainting. Toobtaininput-outputpairs,werandomlymaskasquareregioninheight/widthequalto
25%ofanimage(resultinginablacksquareof1/8thearea)anddefinethisastheinput,whileusing
theoriginalimageastheoutput. Forevaluation,wereporttheMSEmetric.
Colorization. Toobtaininput-outputpairs,weconvertanimagetograyscaleanddenoteitasthe
input,andhavetheoutputbetheoriginalimage. Forevaluation,wereporttheMSEmetric.
4.4 Ablations
Inthissectionwedescribethesetofexperimentsconductedtoascertaintheparticularimplementation
detailsofourREINFORCE[43]method,validatingourdesignchoices.
TaskVectorsLocationinEncodervs. Decoder. Wehypothesizethattaskimplementationoccurs
inadistributedfashionacrossmultiplepartsofthenetwork,necessitatinginterventionsinboththe
encoderanddecodertoinducetaskimplementationinthezero-shotscenario. Hence,wetestthis
by executing our method by restricting interventions to the encoder only, the decoder only, and
allowingforinterventionsthroughoutthewholenetwork. WereportthemIoUonthefoursplitsfor
theSegmentationtask,seekingtofindifinterventionsinbothpartsofthemodelarerequiredfor
appropriatetaskimplementation.
Patching Granularity. We explore different intervention granularities by mapping each token
positiontoagroupthusdecreasingthedimensionalityatwhichweperforminterventions. Withthis
inmind,wecanfurtherreducethesizeofthesearchspaceoftheoptimizationbygroupingpotential
patchingpositionsi=(l,m,k)intospatialquadrantsorindividualattentionheads. Weseektofind
theoptimalgranularityatwhichwemaintainpreciseinterventionsbutmanagetoreducethesearch
spacenonetheless. Weexecuteourmethodwiththreegranularitylevels(individualtokens,quadrants,
attentionheads)andreporttheperformanceonthefourtasks.
74.5 Zero-shotTaskVectorPatching
We seek to explore whether zero-shot task vector patching can provide task implementation per-
formance similar to one-shot prompting. We perform the following experiments to investigate
this:
Task-specific. Foreachofthefourtasksweexecuteourtask-specificmethod,andthebaselinesof
CausalMediationAnalysis(CMA)andGreedyRandomSearch(GRS)with10imagesandreportthe
resultsoncorrespondingtasks.
Multi-task. Wefollowthesameprocedurebutinamulti-taskscenariowhereweselecttwoimages
andperformourmethodalongsidetheCMAandGRSbaselines,evaluatingthelossacrossalltasks
jointly. Wenormalizethelosspertaskinordertoaccountfortask-specificdifferencestoensure
nosingletaskdominatesthesearch,andalltasksareweightedequally. Finally,weintroducethe
identity-copytasktokeepthebatchsizeat10forequivalentcomparisons.
Wealsoprovidethefollowingbaselinestobenchmarktheperformanceofthethreealgorithms,and
tovalidatethenecessityoftopkscorerankingthelayersandperformingtheiterativesearchforthe
GRSmethod:
MAE-VQGAN.Wecomparetotheoriginalmodel’sone-shotperformance.
RandomQuadrants. Wepatchintorandomlysampledpositionsacrossthewholenetwork(same
totalamountofpatchesastask-specificandmulti-task)
GRSacrossRandomKLayers. Weexecutethesearchalgorithmconstrainingittoarandomset
ofklayersinsteadofrankingthembytheirscore. Thisservestovalidatetheneedoftheproposed
scoringmethod.
PatchingintoTopQuadrants. Wepatchintothetopquadrantsbasedontheirscoring(sametotal
amountofpatchesastask-specificandmulti-task). Thatis,wedirectlypatchintoareaswithhigh
scoresnaivelywithouttheiterativerefiningstepsoftheGRS,tovalidateitsutility.
Wereporttheresultsacrossthefoursplitsforthefourtasks,andcompareallvariantstotheoriginal
MAE-VQGANmodelandtherandombaselines.
5 Results
5.1 ActivationScoringAnalysis
Wecomputeρ (i)anddisplayitaggregatedperheadonaheatmapwherethey-axisisthelayer
token
andthex-axisistheattentionheadindex(seeFigure2). Thisheatmapshowcasescertainheadsthat
standout,suggestingthattheseheadsmayholdtaskvectors. Fromthisheatmapwethenselectthe
toptwoheadsrankedbyscorewhichareatposition(26,3)and(11,3);wealsoselectalower-ranked
headatposition(27,5). Foreachofthesethreeheads,wevisualizetheclusteringofitsactivations,
andtheindividualActivationScorepertoken. Bothareplacedtotherightsideoftheheatmap.
ClusteringVisualization. Weobserveaclearclusteringbasedontaskinheadsrankedhighly–head
(26,3)forinstance–byourscoringmethodology,andwhatappearstobemanysmallclustersforthe
low-rankedheadwhichwehypothesizeareclusteringbythesemanticsoftheinputprompt-query
pair–head(27,5). Toperformthedimensionalityreductionweverticallystacktheactivationsofa
particularheadacrosstokenpositionsandprojectthemonto2Dwitht-SNE[27]coloringbytask.
Score-per-token Heatmap. We display the un-aggregated ρ (i) values for each token re-
token
arrangedtoconveyspatialpositioning(equivalenttothe2x2promptinggrid). Thatis,fortheheads
ofinterest,wereporttheρ (i)valuesthattheparticularindividualheadencapsulates–displayed
token
onaheatmap. WeplacetheCLStokenasthelonesquareonthetopleftcorner;followingthiswe
havethevaluescorrespondingtothefourdifferentquadrants(x ,y ,x ,y ). Itisimportanttonote
s s q q
thatintheencodertherearenotokensthatcorrespondtothey (bottomright)quadrantastheseare
q
incorporatedasmasktokensaftertheencoder;hence,forvisualizationpurposeswedisplaytheir
valuetobemarkedasX.Weobservethatwithinasingleheaddifferenttokenscantakeawiderange
8ofvalues. Interestingly,thereappearstobeaconsistencyinvaluesacrossthetokensinparticular
quadrantswhichmotivatedthedecisiontogrouptokenpatchingbyquadrants.
Quantitative Clustering Analysis. To further support the observation of high and low-quality
clusteringbasedoffofthescoringvalue,wereporttheSilhouetteScoreandDavies-Bouldinscorefor
thefourhighest-rankedheads,5randomlysampledheads,andthethreelowest-rankedheads(see
Table1). Weobservethatattentionheadsscoredhighlybyourmethodologydisplayhighquality
clusteringscoresfromtheSilhouetteScoreandDevies-BouldinScoreandvice-versaforthosescored
lowly. Finally,therandomlyselectedattentionheadsreceivedintermediatescores.
5.2 Zero-shotTaskVectorPatching
Wenowsharetheperformanceofourmethod,bothtask-specificandmulti-task,incomparisonto
theoriginalmodelandthefollowingbaselines: CausalMediationAnalysis,GreedyRandomSearch,
Random Quadrants, Random K Layers, and Top Quadrants. Surprisingly, by performing these
interventionswithtaskvectorsweareabletoimplementvisualICLtaskswithbetterperformance
than the original model. We observe in the following Table 2 (which displays the mean and
varianceacrossthe4splits,orTable5intheSupplementaryMaterialswhichcontainstheindividual
split evaluations), that our task-specific models beat the original MAE-VQGAN across all tasks.
Furthermore,theGRStask-specificmodelsbeattheoriginalMAE-VQGANinsometasks. Wealso
observethatourmulti-taskmodelsbeattheoriginalMAE-VQGANinalltasksexceptSegmentation
whereperformanceismet,andGRSmulti-taskperformscomparablytotheoriginal. CMAperforms
betterthantherandombaselinesbutdoesnotreachthehighperformanceofourmethod,ortheGRS
methodnonetheless.
Furthermore,weprovidevalidationthatrankinglayersbytheirρ (l)andselectingthetop-kis
layer
instrumentaltosupportingtheperformanceofthegreedyrandomsearchbycomparingtorandomly
selectingklayers. WeseethatrandomKlayersdoesnotperformwell,exceptforcolorizationwhere
itissimilartoGRS.Moreover,ifwesimplypatchintothetopquadrantswithouttheGRSweget
poorperformance.
Table2: QuantitativeAnalysis. Resultscomparisonacrossdifferenttasksandsplits,indicatingthe
effectivenessofourtask-specificmodel.
Segmentation↑ LowlightEnhance↓ Colorization↓ In-painting↓
Model (Mean±STD) (Mean±STD) (Mean±STD) (Mean±STD)
OriginalMAE-VQGAN 0.338±0.033 0.685±0.032 0.618±0.027 0.550±0.042
RandomQuadrants 0.170±0.061 3.000±0.967 3.025±1.190 2.350±0.955
RandomKLayers 0.090±0.007 1.825±0.043 0.568±0.022 0.875±0.100
TopQuadrants 0.150±0.023 4.875±0.228 4.250±0.269 3.900±0.141
CMA(Task-specific) 0.230±0.012 0.825±0.043 0.895±0.063 1.750±0.112
CMA(Multi-task) 0.150±0.017 1.400±0.122 1.130±0.075 1.225±0.083
GRS(Task-specific) 0.320±0.021 0.600±0.025 0.555±0.025 0.580±0.047
GRS(Multi-task) 0.323±0.019 0.515±0.032 0.568±0.029 0.605±0.036
Ours(Multi-task) 0.325±0.026 0.492±0.025 0.502±0.036 0.558±0.022
Ours(Task-specific) 0.353±0.028 0.458±0.032 0.453±0.036 0.480±0.022
WenowsharethequalitativeresultsforSegmentation,LowlightEnhancement,andIn-painting. First,
wevisualizethetask-specificmodelsofourmethodologyalongsidetheoriginalMAE-VQGAN,and
theCMAandGRSbaselines(seeFigure3). Secondly,wevisualizethetask-specificandmulti-task
variantsofourmethodologyincomparisontoCMAandGRS(seeFigure4)
9Figure3: QualitativeExamples. Wequalitativelycomparethetask-specificvariantsofourmethod-
ology’sresultswiththeoriginalmodelandtheCMAandGRSbaselines. Ourpatchingmethodology
performsbetterthantheoriginalMAE-VQGANmodel.
10Figure4: QualitativeExamples. Wequalitativelycomparethetask-specificandmulti-taskvariants
ofourmethodologywiththeCMAandGRSbaselines. Ourpatchingmethodologyperformsbetter
thantheoriginalMAE-VQGANmodel.
115.3 Ablations
TaskVectorsLocationinEncodervs. Decoder.Wereportourresultsonisolatingthesetofpossible
interventionstotheencoderonly,decoderonly,incontrasttoallowinginterventionsthroughoutthe
wholenetwork. Wecanobservethatin-contexttasklearningbuildsuponbothmodelcomponents.
Thedecoder,however,ismoreimportant. Itisclearthatinterveninginbothcomponentsiscrucialfor
taskimplementationaswehypothesizethatitiscomputedinadistributedfashionwithcascading
higher-ordereffectsthroughthenetworkwhereearlyinterventionshavestrongdownstreameffects
(seeTable3).
Table3: IsolatingTaskLocations. PatchingintoEncoderonly,Decoderonly,andboth.
Segmentation↑
Model Split0 Split1 Split2 Split3
Encoder(Task-specific) 0.09 0.14 0.14 0.13
Decoder(Task-specific) 0.32 0.34 0.29 0.29
Both(Task-specific) 0.35 0.35 0.31 0.29
PatchingGranularity. Motivatedbytheemergenceofquadrantsintheper-tokenscoringvisualiza-
tion,weexploretheoptimalgranularityatwhichtogroupthetokenstoreducethedimensionalityof
thesearchspace. InthetasksofSegmentationandColorization,groupingbyquadrantsresultsin
betterperformance;whereasinthetasksofLowlightEnhancementandIn-painting,maintainingfull
patchinggranularityatthetokenlevelresultsinbetterperformance(seeTable4whichdisplaysthe
meanandvarianceacrossthe4splits,orTable6intheSupplementaryMaterialswhichcontainsthe
individualsplitevaluations).
Table4: OptimalPatchingGranularity. PatchingintoTokens(T),Quadrants(Q),orHeads(H)
Segmentation↑ LowlightEnhance↓ Colorization↓ In-painting↓
Model (Mean±STD) (Mean±STD) (Mean±STD) (Mean±STD)
OursT(Task-specific) 0.350±0.025 0.495±0.036 0.453±0.036 0.485±0.018
OursQ(Task-specific) 0.338±0.023 0.458±0.032 0.465±0.036 0.480±0.022
OursH(Task-specific) 0.245±0.015 0.942±0.070 0.857±0.069 0.885±0.067
OursT(Multi-task) 0.318±0.023 0.510±0.028 0.510±0.039 0.565±0.025
OursQ(Multi-task) 0.325±0.026 0.492±0.025 0.502±0.036 0.558±0.022
OursH(Multi-task) 0.253±0.013 1.105±0.064 0.998±0.069 0.980±0.082
6 Limitations
OurfocuswasinidentifyingTaskVectors,butwehypothesizethattheremightexistotherimportant
vector-typessuchasimagestructureactivationsthatcapturelocations,input-outputplacements,and
left-to-rightordering. DuringtheoptimizationwithREINFORCE[43]weevaluatetheperformance
using the MSE metric for all tasks (except Segmentation which uses mIoU) after decoding the
predictedVQGANtokens. Instead,itmaybepossibletodirectlyevaluatethemodelinVQGAN
token space using cross entropy loss which may lead to more accurate results. We leave these
extensionsforfutureworks.
7 Conclusion
Inthisworkweexploretheinternalmechanismsofvisualin-contextlearninganddeviseanalgorithm
toidentifyTaskVectors,activationspresentintransformersthatcanreplacethein-contextexamplesto
guidethemodelintoperformingaspecifictask.WeconfirmourapproachbyadaptingMAE-VQGAN
toperformspecifictasksinazero-shotfashionbypatchingtheTaskVectoridentified. Wefindthat
different than in NLP, in computer vision Task Vectors are distributed throughout the network’s
encoderanddecoder.
12Acknowledgements: ThisprojecthasreceivedfundingfromtheEuropeanResearchCouncil(ERC)
undertheEuropeanUnionsHorizon2020researchandinnovationprogramme(grantERCHOLI
819080). Prof. Darrell’sgroupwassupportedinpartbyDoDincludingDARPA’sLwLLand/or
SemaForprograms,aswellasBAIR’sindustrialallianceprograms.
References
[1] Akyürek, E., Schuurmans, D., Andreas, J., Ma, T., Zhou, D.: What learning algorithm is
in-contextlearning? investigationswithlinearmodels.arXivpreprintarXiv:2211.15661(2022)
[2] Bahng,H.,Jahanian,A.,Sankaranarayanan,S.,Isola,P.: Exploringvisualpromptsforadapting
large-scalemodels.arXivpreprintarXiv:2203.17274(2022)
[3] Bai,Y.,Geng,X.,Mangalam,K.,Bar,A.,Yuille,A.,Darrell,T.,Malik,J.,Efros,A.A.: Sequen-
tialmodelingenablesscalablelearningforlargevisionmodels.arXivpreprintarXiv:2312.00785
(2023)
[4] Bar,A.,Gandelsman,Y.,Darrell,T.,Globerson,A.,Efros,A.: Visualpromptingviaimage
inpainting.AdvancesinNeuralInformationProcessingSystems35,25005–25017(2022)
[5] Bau, D., Zhu, J.Y., Strobelt, H., Zhou, B., Tenenbaum, J.B., Freeman, W.T., Torralba, A.:
Gandissection: Visualizingandunderstandinggenerativeadversarialnetworks.arXivpreprint
arXiv:1811.10597(2018)
[6] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Neelakantan, A.,
Shyam,P.,Sastry,G.,Askell,A.,etal.: Languagemodelsarefew-shotlearners.Advancesin
neuralinformationprocessingsystems33,1877–1901(2020)
[7] Dai,D.,Sun,Y.,Dong,L.,Hao,Y.,Sui,Z.,Wei,F.: Whycangptlearnin-context? language
modelssecretlyperformgradientdescentasmetaoptimizers.arXivpreprintarXiv:2212.10559
(2022)
[8] Davies, D., Bouldin, D.: A cluster separation measure. Pattern Analysis and
Machine Intelligence, IEEE Transactions on PAMI-1, 224 – 227 (05 1979).
https://doi.org/10.1109/TPAMI.1979.4766909
[9] Dosovitskiy,A.,Beyer,L.,Kolesnikov,A.,Weissenborn,D.,Zhai,X.,Unterthiner,T.,Dehghani,
M.,Minderer,M.,Heigold,G.,Gelly,S.,Uszkoreit,J.,Houlsby,N.: Animageisworth16x16
words: Transformersforimagerecognitionatscale(2021)
[10] Esser,P.,Rombach,R.,Ommer,B.: Tamingtransformersforhigh-resolutionimagesynthesis.
In: ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition
(CVPR).pp.12873–12883(June2021)
[11] Ferry, Q.R., Ching, J., Kawai, T.: Emergence and function of abstract representations in
self-supervisedtransformers.arXivpreprintarXiv:2312.05361(2023)
[12] Gandelsman,Y.,Efros,A.A.,Steinhardt,J.: Interpretingclip’simagerepresentationviatext-
baseddecomposition.arXivpreprintarXiv:2310.05916(2023)
[13] Garg, S., Tsipras, D., Liang, P.S., Valiant, G.: What can transformers learn in-context? a
casestudyofsimplefunctionclasses.AdvancesinNeuralInformationProcessingSystems35,
30583–30598(2022)
[14] Hahn,M.,Goyal,N.: Atheoryofemergentin-contextlearningasimplicitstructureinduction.
arXivpreprintarXiv:2303.07971(2023)
[15] Han,C.,Wang,Z.,Zhao,H.,Ji,H.: In-contextlearningoflargelanguagemodelsexplainedas
kernelregression.arXivpreprintarXiv:2305.12766(2023)
[16] He,K.,Chen,X.,Xie,S.,Li,Y.,Dollár,P.,Girshick,R.B.: Maskedautoencodersarescalable
visionlearners.CoRRabs/2111.06377(2021),https://arxiv.org/abs/2111.06377
[17] Hendel,R.,Geva,M.,Globerson,A.: In-contextlearningcreatestaskvectors.arXivpreprint
arXiv:2310.15916(2023)
[18] Jia,M.,Tang,L.,Chen,B.C.,Cardie,C.,Belongie,S.,Hariharan,B.,Lim,S.N.: Visualprompt
tuning.In: EuropeanConferenceonComputerVision.pp.709–727.Springer(2022)
13[19] Jin,Z.,Cao,P.,Yuan,H.,Chen,Y.,Xu,J.,Li,H.,Jiang,X.,Liu,K.,Zhao,J.: Cuttingoffthe
headendstheconflict: Amechanismforinterpretingandmitigatingknowledgeconflictsin
languagemodels.arXivpreprintarXiv:2402.18154(2024)
[20] Kingma,D.P.,Ba,J.: Adam: Amethodforstochasticoptimization(2017)
[21] Li,X.L.,Liang,P.:Prefix-tuning:Optimizingcontinuouspromptsforgeneration.arXivpreprint
arXiv:2101.00190(2021)
[22] Liu, J., Shen, D., Zhang, Y., Dolan, B., Carin, L., Chen, W.: What makes good in-context
examplesforgpt-3? arXivpreprintarXiv:2101.06804(2021)
[23] Liu,S.,Xing,L.,Zou,J.: In-contextvectors: Makingincontextlearningmoreeffectiveand
controllablethroughlatentspacesteering.arXivpreprintarXiv:2311.06668(2023)
[24] Lu, S., Schuff, H., Gurevych, I.: How are prompts different in terms of sensitivity? arXiv
preprintarXiv:2311.07230(2023)
[25] Lu, Y., Bartolo, M., Moore, A., Riedel, S., Stenetorp, P.: Fantastically ordered prompts
and where to find them: Overcoming few-shot prompt order sensitivity. arXiv preprint
arXiv:2104.08786(2021)
[26] Luo,H.,Specia,L.: Fromunderstandingtoutilization: Asurveyonexplainabilityforlarge
languagemodels.arXivpreprintarXiv:2401.12874(2024)
[27] vanderMaaten,L.,Hinton,G.: Visualizingdatausingt-sne.JournalofMachineLearningRe-
search9(86),2579–2605(2008),http://jmlr.org/papers/v9/vandermaaten08a.html
[28] Meng,K.,Bau,D.,Andonian,A.,Belinkov,Y.: Locatingandeditingfactualassociationsingpt.
AdvancesinNeuralInformationProcessingSystems35,17359–17372(2022)
[29] Moraffah,R.,Karami,M.,Guo,R.,Raglin,A.,Liu,H.: Causalinterpretabilityformachine
learning-problems, methodsandevaluation.ACMSIGKDDExplorationsNewsletter22(1),
18–33(2020)
[30] Palit,V.,Pandey,R.,Arora,A.,Liang,P.P.: Towardsvision-languagemechanisticinterpretabil-
ity: Acausaltracingtoolforblip.In: ProceedingsoftheIEEE/CVFInternationalConference
onComputerVision.pp.2856–2861(2023)
[31] Park,K.,Choe,Y.J.,Veitch,V.: Thelinearrepresentationhypothesisandthegeometryoflarge
languagemodels.arXivpreprintarXiv:2311.03658(2023)
[32] Pearl,J.: Directandindirecteffects.In: Probabilisticandcausalinference: theworksofJudea
Pearl,pp.373–392(2022)
[33] Radford,A.,Wu,J.,Child,R.,Luan,D.,Amodei,D.,Sutskever,I.,etal.: Languagemodelsare
unsupervisedmultitasklearners.OpenAIblog1(8), 9(2019)
[34] Rousseeuw, P.J.: Silhouettes: A graphical aid to the interpretation and valida-
tion of cluster analysis. Journal of Computational and Applied Mathematics 20, 53–
65 (1987). https://doi.org/https://doi.org/10.1016/0377-0427(87)90125-7, https://www.
sciencedirect.com/science/article/pii/0377042787901257
[35] Russakovsky,O.,Deng,J.,Su,H.,Krause,J.,Satheesh,S.,Ma,S.,Huang,Z.,Karpathy,A.,
Khosla, A., Bernstein, M., Berg, A.C., Fei-Fei, L.: Imagenet large scale visual recognition
challenge(2015)
[36] Shaban,A.,Bansal,S.,Liu,Z.,Essa,I.,Boots,B.:One-shotlearningforsemanticsegmentation.
arXivpreprintarXiv:1709.03410(2017)
[37] Singh,C.,Inala,J.P.,Galley,M.,Caruana,R.,Gao,J.: Rethinkinginterpretabilityintheeraof
largelanguagemodels.arXivpreprintarXiv:2402.01761(2024)
[38] Todd,E.,Li,M.L.,Sharma,A.S.,Mueller,A.,Wallace,B.C.,Bau,D.: Functionvectorsinlarge
languagemodels.arXivpreprintarXiv:2310.15213(2023)
[39] Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.A., Lacroix, T., Rozière, B.,
Goyal,N.,Hambro,E.,Azhar,F.,etal.: Llama: Openandefficientfoundationlanguagemodels.
arXivpreprintarXiv:2302.13971(2023)
[40] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L.,
Polosukhin,I.: Attentionisallyouneed.CoRRabs/1706.03762(2017),http://arxiv.org/
abs/1706.03762
14[41] Wang,B.,Komatsuzaki,A.: Gpt-j-6b: A6billionparameterautoregressivelanguagemodel
(2021)
[42] Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q.V., Zhou, D., et al.:
Chain-of-thoughtpromptingelicitsreasoninginlargelanguagemodels.AdvancesinNeural
InformationProcessingSystems35,24824–24837(2022)
[43] Williams,R.J.: Simplestatisticalgradient-followingalgorithmsforconnectionistreinforcement
learning.Machinelearning8,229–256(1992)
[44] Wu,X.,Varshney,L.R.: Transformer-basedcausallanguagemodelsperformclustering.arXiv
preprintarXiv:2402.12151(2024)
[45] Xie,S.M.,Raghunathan,A.,Liang,P.,Ma,T.: Anexplanationofin-contextlearningasimplicit
bayesianinference.arXivpreprintarXiv:2111.02080(2021)
[46] Xu,J.,Gandelsman,Y.,Bar,A.,Yang,J.,Gao,J.,Darrell,T.,Wang,X.: Improv: Inpainting-
basedmultimodalpromptingforcomputervisiontasks.arXivpreprintarXiv:2312.01771(2023)
[47] Xu,S.,Dong,W.,Guo,Z.,Wu,X.,Xiong,D.: Exploringmultilingualhumanvalueconcepts
inlargelanguagemodels: Isvaluealignmentconsistent,transferableandcontrollableacross
languages? arXivpreprintarXiv:2402.18120(2024)
[48] Zhang,F.,Nanda,N.:Towardsbestpracticesofactivationpatchinginlanguagemodels:Metrics
andmethods.arXivpreprintarXiv:2309.16042(2023)
[49] Zhang, K., Lv, A., Chen, Y., Ha, H., Xu, T., Yan, R.: Batch-icl: Effective, efficient, and
order-agnosticin-contextlearning.arXivpreprintarXiv:2401.06469(2024)
[50] Zhang,Y.,Tinˇo,P.,Leonardis,A.,Tang,K.: Asurveyonneuralnetworkinterpretability.IEEE
TransactionsonEmergingTopicsinComputationalIntelligence5(5),726–742(2021)
[51] Zhang, Y., Zhou, K., Liu, Z.: What makes good examples for visual in-context learning?
AdvancesinNeuralInformationProcessingSystems36(2024)
15SupplementaryMaterial
8 FullResults
Assupplementarymaterial,weprovidethetablesdisplayingthefullevaluationsacrossthe4splitsof
ourmethodalongsidethebaselines,andthedifferentpatchinggranularities.
Table5: QuantitativeAnalysis. Resultscomparisonacrossdifferenttasksandsplits,indicatingthe
effectivenessofourtask-specificmodel.
Segmentation↑ LowlightEnhancement↓
Model Split0 Split1 Split2 Split3 Split0 Split1 Split2 Split3
OriginalMAE-VQGAN 0.35 0.38 0.33 0.29 0.70 0.66 0.73 0.65
RandomQuadrants 0.08 0.25 0.16 0.19 4.30 2.40 3.50 1.80
RandomKLayers 0.09 0.10 0.09 0.08 1.80 1.80 1.90 1.80
TopQuadrants 0.11 0.17 0.16 0.16 4.50 5.00 5.10 4.90
CMA(Task-specific) 0.23 0.25 0.22 0.22 0.76 0.83 0.88 0.83
CMA(Multi-task) 0.18 0.14 0.14 0.14 1.2 1.5 1.5 1.4
GRS(Task-specific) 0.33 0.35 0.30 0.30 0.56 0.61 0.63 0.60
GRS(Multi-task) 0.33 0.35 0.31 0.30 0.47 0.52 0.56 0.51
Ours(Multi-task) 0.35 0.35 0.31 0.29 0.46 0.49 0.53 0.49
Ours(Task-specific) 0.38 0.38 0.33 0.32 0.41 0.46 0.50 0.46
Colorization↓ In-painting↓
Model Split0 Split1 Split2 Split3 Split0 Split1 Split2 Split3
OriginalMAE-VQGAN 0.59 0.62 0.66 0.60 0.49 0.55 0.61 0.55
RandomQuadrants 2.10 4.10 4.30 1.60 3.80 1.20 1.90 2.50
RandomKLayers 0.54 0.57 0.60 0.56 0.72 0.89 1.00 0.89
TopQuadrants 3.80 4.40 4.30 4.50 3.70 3.90 4.10 3.90
CMA(Task-specific) 0.79 0.92 0.96 0.91 1.6 1.8 1.9 1.7
CMA(Multi-task) 1.02 1.2 1.2 1.1 1.1 1.3 1.3 1.2
GRS(Task-specific) 0.52 0.56 0.59 0.55 0.52 0.56 0.65 0.59
GRS(Multi-task) 0.53 0.57 0.61 0.56 0.55 0.61 0.65 0.61
Ours(Multi-task) 0.45 0.51 0.55 0.50 0.53 0.56 0.59 0.55
Ours(Task-specific) 0.40 0.46 0.50 0.45 0.45 0.49 0.51 0.47
Table6: OptimalPatchingGranularity. PatchingintoTokens(T),Quadrants(Q),orHeads(H)
Segmentation↑ LowlightEnhancement↓
Model Split0 Split1 Split2 Split3 Split0 Split1 Split2 Split3
T(Task-specific) 0.38 0.37 0.33 0.32 0.44 0.50 0.54 0.50
Q(Task-specific) 0.36 0.36 0.32 0.31 0.41 0.46 0.50 0.46
H(Task-specific) 0.24 0.27 0.23 0.24 0.83 0.97 1.02 0.95
T(Multi-task) 0.34 0.34 0.30 0.29 0.47 0.51 0.55 0.51
Q(Multi-task) 0.35 0.35 0.31 0.29 0.46 0.49 0.53 0.49
H(Multi-task) 0.26 0.27 0.24 0.24 1.01 1.12 1.19 1.10
Colorization↓ In-painting↓
Model Split0 Split1 Split2 Split3 Split0 Split1 Split2 Split3
T(Task-specific) 0.40 0.46 0.50 0.45 0.46 0.49 0.51 0.48
Q(Task-specific) 0.41 0.47 0.51 0.47 0.45 0.49 0.51 0.47
H(Task-specific) 0.75 0.88 0.94 0.86 0.78 0.92 0.96 0.88
T(Multi-task) 0.45 0.51 0.56 0.52 0.53 0.57 0.60 0.56
Q(Multi-task) 0.45 0.51 0.55 0.50 0.53 0.56 0.59 0.55
H(Multi-task) 0.89 1.02 1.08 1.0 0.85 1.02 1.07 0.98
169 MoreQualitativeExamples
Weprovideawiderselectionofexamplescomparingourzero-shottaskvectorpatchingmethodology
incomparisonto1)theoriginalone-shotMAE-VQGAN,CMA,andGRS,and2)ourablations.
Alongsideeachfigure,weaccompanyitwithanaccordinganalysis.
Figure 5: Qualitative Examples. We qualitatively compare our the task-specific variants of our
methodologywiththeoriginalmodelandtheCMAandGRSbaselines.
17First,wevisualizethetask-specificmodelsofourmethodologyalongsidetheoriginalMAE-VQGAN,
and the CMA and GRS baselines (see Figure 5). Secondly, we visualize the task-specific and
multi-taskvariantsofourmethodologyincomparisontoCMA(seeFigure6).
Figure6: QualitativeExamples. Wequalitativelycomparethetask-specificandmulti-taskvariants
ofourmethodologywiththeCMAandGRSbaselines.
18Figure7: OurResultsonSegmentationTask
Qualitative Analysis for Segmentation. In Figure 7, we compare our methodology and GRS
task-specificandmulti-taskmethodstotheoriginalone-shotMAE-VQGANperformanceonthe
taskofSegmentation. Itappearsthatourmethodisgoodatsegmentingthecoarseandfinedetails
oftheobjectoffocus. Inmanycases,thesegmentationsgeneratedbytheoriginalMAE-VQGAN
suffer from holes or incomplete masks. In contrast, our method outputs consistent and coherent
masks. Ontheotherhand,theGRSmethodsufferswithparticulardetailsespeciallyobservablewhen
attemptingtosegmentananimal’searorleg. However,inmanysuchcasesitperformsbetterthan
MAE-VQGANatgettingthegeneralshapeofobjects.
QualitativeAnalysisforLowlightEnhancement. InFigure8,wecompareourmethodologyand
GRStask-specificandmulti-taskmethodstotheoriginalone-shotMAE-VQGANperformanceon
thetaskofLowlightEnhancement. ItappearsthattheGRSmethodsuffersinmaintainingthevisual
qualitiesofthequeryimage. HowevertherearemanycaseswhereMAE-VQGANassignsbright
colorswhichislikelyduetotheparticularpromptinuseandtheinherentambiguitiesofthetask.
Ontheotherhand,ourmethod–particularlythemulti-taskvariant–ouputsconsistentlybetterresults
withaccuratevisualqualities. Insomecasesourmethodproducessomewhatmutedorblurryresults
whichmaybeaconsequenceofusingMSEinthepixelspaceassupervision,butnonethelessreports
betterquantitativeperformance.
QualitativeAnalysisforIn-painting. InFigure9,wecompareourmethodologyandGRStask-
specificandmulti-taskmethodstotheoriginalone-shotMAE-VQGANperformanceonthetask
ofIn-painting. Weobservethatourmethodconsistentlyoutperformstheoriginalmodel. However,
itappearsthattheGRStask-vectorpatchingmethod–onceagain–suffersinmaintainingthehigher
frequency components of the query image; it appears to reduce the contrast of the image and
reducesaturation. However,therearemanysuchcaseswheretheoriginalMAE-VQGANone-shot
techniquefailstoappropriatelyimplementthetaskwhileourmethodsucceeds. Theoriginalmodel’s
performancedependsheavilyonthespecificpromptusedwhichmaybetherootcauseoffailures
whiletask-vectorpatchingsucceeds.
19Figure8: OurResultsonLowlightEnhancementTask
Figure9: OurResultsonIn-paintingTask
20Figure10: REINFORCEAblationsforSegmentationTask
Qualitative Analysis of Ablations. Finally, we present the qualitative analysis of the different
ablationsfortheSegmentationtaskinFigure10. Thebenefitsofobservingthevisualfeaturesofthe
differentablationsinadditiontoquantitativeanalysisbecomesclearwhencomparingtheDecoder
OnlyandEncoderOnlycolumns. Hereitisclearthatpatchingintodecoderisofutmostimportance
inrelationtopatchingintotheencoder;thedistinctionisclearwhenobservingqualitativefeatures.
Intheend,itisbothpartsinsynchronywhichallowfortheimplementationofin-contextlearning.
2110 GreedyRandomSearchbaseline
10.1 SelectingTaskVectorsviaGreedyRandomSearch
Foreverytaskj weapplythefollowingalgorithmtoobtainatask-specificmodel.
Input. Themeanactivations{µ },scoringfunctionρ (·),pretrainedvisualpromptingmodel
i,j layer
F(·),andanevaluationset.
Initialization. Initialize a set of binary indicators {α } for all i, where α ∈ {0,1} signifies
i,j i,j
whetherthemeantaskactivationµ isataskvector. Nextwedescribethealgorithmtochoosethe
i,j
valuesofα .
i,j
Foreveryactivationi = (l,m,k)inthetopk scoringlayersw.r.tρ (l),randomlychoosethe
layer
valueofα bysamplingfromaBernoullirandomvariablewithparametervaluep. Settheactivation
i
vectorstobez ={µ |∀ijifα =1}. Evaluatethenowtask-specificmodelF(·|z )onaheld-out
j i,j i,j j
validationset. RunforT trialsandforeveryi,j setthevaluesofα tobethevaluesfromthemost
i,j
successfultrial.
GreedySearch. Iterateoverlinthetopkscoringlayerssortedbyρ (l)fromhightolow,pick
layer
activationi=(l,m,k),flipthevalueα andevaluatethevalidationscoreforF(·|z ). Afterhaving
i,j j
evaluatedeachflipoftheα ontheparticularlayerl wekeeptheα whichperformedbestor
i,j i,j
continuetothenextlayeriftheperformancedidnotimprove.
Termination. Whenonesearchloopacrosstheklayersresultsinnochanges-orafter10kiterations,
thesearchhasthusconvergedandwereturnthesingle-taskmodelF(·|z ).
j
Thisprocedureisoutlinedforeverytoken,attentionhead,andlayer. However,itispossibletoapply
itindifferentlevelsofgranularity. Forexample,bypatchinggroupoftokensfromthesamequadrant,
patchingallthetokensinanattentionhead,orpatchingtheentirelayer. Wediscussthesedesign
choicesinthenextsection.
10.2 GreedyRandomSearchImplementationExperiments
Inthissectionwedescribethesetofexperimentsconductedtoascertaintheparticularimplementation
detailsofthegreedyrandomsearch,validatingthedesignchoices.
ImplementationDetailsWesearchthroughthetopk = 17layersrankedbyActivationScoring.
Duringtheinitializationphasewesampleα ∈{0,1}fromaBernoullidistributionwithaparameter
i,j
ofp=0.3(probabilityofselecting1)andevaluateperformance. WerepeatthisforT =100trials
andcontinuewiththebestperformingα . Furthermore,weperformagroupingoftokenpositions
i,j
iineachindividualattentionheadinto3groups: theCLStoken,bottomleftquadrant,andbottom
rightquadrant. Thisservestofurtherreducethesearchspace. Weuseasetof10trainingimagesto
supervisethesearch. Thesedesigndecisionsarefurthervalidatedthroughablationexperiments.
SelectingInitializationParameters. FortheinitializationoftheGreedyRandomSearchthereare
twoparameters,kwhichdictateshowmanylayerstosearchacrossandtheBernoullirandomvariable
parameterpwhichdictatestheprobabilityatwhichwesetα tobe1duringtheinitializationphase.
i,j
Thequestionis,whichk valueisbestatnarrowingdownthesearchspacewithoutrestrictingour
abilitytoinducetaskimplementation,andwhatisthebestaccordingpvalue? Weascertainthisby
searchingfortheoptimalconfigurationtoinitializetheGreedyRandomSearch. Weperformagrid
searchforkvaluesfrom14to20,andpvaluesfrom0.1to0.6andreporttheevaluationmetricfor
theSegmentationtaskonthebatchof10images. Ourgoalistofindthe(k,p)pairwithhighest
performingrandominitialization.
TaskVectorsLocationinEncodervs. Decoder. Similartotheablationconductedforourmain
methodology(viaREINFORCE[43])implementation,weexecutetheGreedyRandomSearchfor
theSegmentationtaskbyrestrictinginterventionstotheencoderonly,thedecoderonly,andallowing
forinterventionsthroughoutthewholenetwork. Itiskeytonotethatinordertorestrictinterventions
tothedecoderonly,whichhas8layers,thekvaluemustbesetto8,whereasforisolatingtheencoder
wecankeeptheoriginalk =17value. WereportthemIoUonthefoursplitsfortheSegmentation
22Figure11: SelectingInitializationParameters. WeevaluateForegroundSegmentationmIoUon
Pascal5iusing10imagesfordifferentrandominitializationparameterizedbyK andp.
task seeking to find if interventions in both parts of the model are required for appropriate task
implementation.
Patching Granularity. We execute our Greedy Random Search with three granularity levels,
groupingbyQuadrants,groupingbyHeads,andgroupingbyLayers,andreportthemIoUperformance
onthefoursplitsfortheSegmentationtask.
10.3 GRSImplementationExperimentsResults
SelectingK.Weexploretheoptimalparametersforarandominitialization. Wefindthebestsetupto
betoconstrainthesearchacrossthetopk =17layers,samplingquadrantstopatchwithaprobability
ofp=0.3(seeFigure11).
TaskVectorsLocationinEncodervs. Decoder. Wereporttheresultsonisolatingthesetofpossible
interventionstotheencoderonly,decoderonly,incontrasttoallowinginterventionsthroughoutthe
wholenetwork. Wecanobservethatin-contexttasklearningbuildsuponbothmodelcomponents.
Thedecoder,however,ismoreimportant. Itisclearthatinterveninginbothcomponentsiscrucialfor
taskimplementationaswehypothesizethatitiscomputedinadistributedfashionwithcascading
higher-ordereffectsthroughthenetworkwhereearlyinterventionshavestrongdownstreameffects
(seeTable7).
Table7: IsolatingTaskLocations. Patching Table8: OptimalPatchingGranularity. Patch-
intoEncoderonly,Decoderonly,andBoth ingintoFullLayers,FullHeads,orQuadrants
Segmentation↑ only
Model Split0 Split1 Split2 Split3 Segmentation↑
GRSBoth(Task-specific) 0.33 0.35 0.30 0.30 Model Split0 Split1 Split2 Split3
GRSEncoder(Task-specific) 0.13 0.22 0.20 0.20
GRSQuadrants(Task-specific) 0.33 0.35 0.30 0.30
GRSDecoder(Task-specific) 0.26 0.28 0.25 0.25
GRSHeads(Task-specific) 0.15 0.15 0.14 0.13
GRSLayers(Task-specific) 0.28 0.31 0.26 0.27
PatchingGranularity. Weexploretheoptimalgranularityatwhichtogroupthetokenstoreduce
thedimensionalityofthesearchspace. Motivatedbytheemergenceofquadrantsintheper-token
scoringvisualization,andvalidatedbyattemptingtogroupbywholeattentionheads(patchinginto
allthetokensintheattentionhead)andgroupbywholelayers(patchingintoalltheattentionheads
ofalayer),itisclearthatquadrantsprovidethebesttrade-offbetweenreducingthedimensionalityof
thesearchspaceandperformance(seeTable8). Itisinterestingtonotethatpatchingintofulllayers
23reducesthesearchspacetoasizeof232whereasattentionheadsis2512andquadrantsis2768forthe
encoderand2384inthedecoder(disregardingtop-klayerselection).
10.4 GRSBaselineQualitativeComparisons
We provide a wider selection of examples comparing our GRS zero-shot task vector patching
methodologyincomparisonto1)aselectionofbaselines,and2)ourablations. Alongsideeach
figure,weaccompanyitwithanaccordinganalysis.
Figure12: GRSBaselineComparisonforSegmentation
Figure13: GRSBaselineComparisonforLowlightEnhancement
IntheFigures12,13,and14wecompareourGRStask-specificmethodwithahandfulofbaselines
definedinsection4.5. WehaveabbreviatedTopQuadrantsasTopQ,RandomKLayersasRandom
Layers, and Random Quadrants as Random Q. It is clear that Top Quadrants struggles to output
24Figure14: GRSBaselineComparisonforIn-painting
coherentcompletions. Webelievethistobebecauseoftheneedofpatchingintopositionsofdifferent
purposesotherthantaskimplementationsuchaspositionsthatencodetheinput-outputstructurethat
aone-shotpromptprovides. Furtheropportunitiesforexplorationcouldincludeotherscoringterms
thattakeintoaccountstructuralinformationprovidedbydifferentpromptorientations. RandomK
LayersperformssurprisinglywellduetotheefficiencyoftheGreedyRandomSearchbutnonetheless
doesnotreachtheperformanceofusingourscoringmechanismtoselectthetopK layers. Finally
RandomQuadrantsstrugglestocompletecoherentresults.
Finally,wepresentthequalitativeanalysisofthedifferentablationsfortheSegmentationtaskin
Figure15.Similarlytoourmainmethod(viaREINFORCE[43]),itisclearthatpatchingintodecoder
ismoreimportantthanpatchingintotheencoderbutintheend,itisbothpartsinsynchronywhich
reportthebestperformance. Furthermore,itisclearthattheoptimalgranularityforpatchingisata
quadrantlevel. Wefinditcounterintuitivethatlayer-levelpatchingperformsbetterthanhead-level
patching–as one would assume that a finer granularity provides better accuracies. However, we
believethatbygroupingper-layerwesignificantlyreducethesearchspace(byafactorof16)which
reducestheprobabilityoffallingintoalocaloptimum;whereasgroupingbyhead,wesufferfroma
reducedprecisionbutdonotgainthebenefitsofareducedsearchspacemagnitude(factorof2for
encoderandfactorof3fordecoderwhengroupingbyheadinsteadof16whengroupingbylayer).
Furtherexplorationinthisdirectionisofinterest.
25Figure15: GRSAblationforSegmentationTask
26