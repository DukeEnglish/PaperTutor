Convergence analysis of controlled particle systems arising in deep
learning: from finite to infinite sample size
Huafu Liao ∗ Alpa´r R. M´esza´ros † Chenchen Mou ‡ Chao Zhou§
Abstract
This paper deals with a class of neural SDEs and studies the limiting behavior of the associated
sampledoptimalcontrolproblemsasthesamplesizegrowstoinfinity. TheneuralSDEswithN samples
can be linked to the N-particle systems with centralized control. We analyze the Hamilton–Jacobi–
Bellman equation corresponding to the N-particle system and establish regularity results which are
uniform in N. The uniform regularity estimates are obtained by the stochastic maximum principle and
the analysis of a backward stochastic Riccati equation. Using these uniform regularity results, we show
theconvergenceoftheminimaofobjectivefunctionalsandoptimalparametersoftheneuralSDEsasthe
samplesizeN tendstoinfinity. Thelimitingobjectscanbeidentifiedwithsuitablefunctionsdefinedon
the Wasserstein space of Borel probability measures. Furthermore, quantitative algebraic convergence
rates are also obtained.
2020 AMS Mathematics subject classification: 49N80; 65C35; 49L12; 62M45.
Keywords: Mean field optimization; Neural networks; Mean field control; Propagation of chaos..
1 Introduction
In recent years, neural networks have been shown to be very effective modelling complicated data sets. For
the situations where large amounts of samples are observed, it is important to ensure the convergence of
optimal parameters as the number of samples goes to infinity, i.e., the generality of the neural network.
Such problems are studied in Chizat et. al. (2018); E et. al. (2019), and Mei et. al. (2018). Motivated by
these studies, we investigate a mathematical model concerning so-called neural SDEs with N samples and
establish quantitative results on the convergence of the minima of the corresponding objective functionals
and the optimal parameters to suitable limit objects.
Our research concerns with the following neural SDEs

(cid:18) N (cid:19)
dX Nθ,i(t)=f t,θ(t),X Nθ,i(t), N1 (cid:88) δ
X Nθ,j(t)
dt+σdW0(t),
(1.1)
j=1
Xθ,i(0)=x
, i=1,...,N,
N i
where θ : [0,T] → Θ is a stochastic process that represents the trainable parameters (valued in a given
control set Θ). Here f : [0,T]×Θ×R×P (R) → R is a nonlinear function that governs the feed forward
2
dynamics. T > 0 is a given time horizon, (W0(t)) is a given Brownian motion on R with intensity
t∈[0,T]
σ ∈R, and the neural SDEs are initiated with samples x ∈R, i=1,...,N. The standing assumptions on
i
the data and the set up for the specific sampled optimal control problems, including the description of the
objective function is given in Section 2 (see (2.1) and (2.4)).
∗Email: mathuaf@outlook.com,DepartmentofMathematics,CityUniversityofHongKong.
†Email: alpar.r.meszaros@durham.ac.uk,DepartmentofMathematicalSciences,UniversityofDurham,Durham,UK.
‡Email: chencmou@cityu.edu.hk,DepartmentofMathematics,CityUniversityofHongKong.
§Email: matzc@nus.edu.sg,DepartmentofMathematicsandRiskManagementInstitute,NationalUniversityofSingapore.
1
4202
rpA
8
]CO.htam[
1v58150.4042:viXraThe neural SDE (1.1) describes the deep learning from a dynamical systems viewpoint and relying on
this, our results make it possible to analyze the convergence of trainable parameters obtained from samples
with size N. The dynamical system approach to deep learning was proposed in E et. al. (2017); Haber et.
al. (2017), and studied later in Bo and Capponi et. al. (2022); Cuchiero et. al. (2020); E et. al. (2019);
Jabir et. al. (2019); Thorpe et. al. (2018), etc. See also, for instance, Baudelet et. al. (2023); Carmona et.
al. (2021); Chen et. al. (2018); Ruthotto et. al. (2020), for the application of such approach. The intuition
of the dynamical system approach to deep learning is as follows. For models such as residual networks,
recurrent neural networks and normalizing flows, the typical feed-forward propagation with T layers can be
presented as
(cid:0) (cid:1)
x(t+1)=x(t)+f x(t),θ(t) , t=0,1,...,T −1,
where x(0), x(T)∈Rd are the input and output, respectively, and θ(t) is the weight matrix. Given multiple
samples of the input x(0), the goal of learning is to tune the trainable parameters θ(t), t = 0,1,...,T −1,
so that the outputs x(T) minimize a given objective function. As the layer number T tends to infinity, after
an appropriate rescaling, the above discrete system is then turned into an ODE
(cid:0) (cid:1)
x˙(t)=f x(t),θ(t) , t∈[0,T].
In a more general setting, the feed-forward propagation might depend on the distribution of the input (see
for instance Baudelet et. al. (2023); Carmona et. al. (2021); Elie et. al. (2020)) and systemic noise, the
aforementioned continuous idealization is then naturally generalized to the neural SDE below:
dx(t)=f(cid:0) x(t),L(x(t)),θ(t)(cid:1) dt+σdW0, t∈[0,T],
t
where L(x(t)) stands for the law of x(t) and (W0) is a given Brownian motion. So the goal is to tune
t t∈[0,T]
the stochastic control θ(t) so that a given objective function is minimized. Another situation where the
distributionofsamplesentersthefeed-forwardpropagationistheso-calledbatchnormalization(seeIoffeet.
al. (2015)), for example
 
(cid:82)
x− yµ(dy)
f(x,µ,θ)=f˜ (cid:113) ,θ,
(cid:82)
y2µ(dy)+ϵ
for some function f˜, where the variable µ in the above corresponds to the distribution of samples, and ϵ>0
is a given parameter.
Since the distribution L(x(t)) is practically hard to observe, this is usually replaced with the empirical
measure of the samples. As a result, we obtain (1.1) as well as the empirical risk minimization Problem 2.2
(see the details in Section 2).
InspiredbyEet. al.(2017,2019);Haberet. al.(2017),inthismanuscriptwetreat(1.1)fromanoptimal
control point of view. The dynamics in (1.1) can be viewed as the N-particle systems with centralized
control. Indeed, we may view each Xθ,i(t) as the process driving particle i and θ(t) the control process.
N
However, we note here that the controlled particle system (1.1) is different from the usual mean field type,
typically studied in the literature, in the sense that every particle Xθ,i(t) in the system shares the same
N
control θ(t) rather than having their own θi(t). In summary, our problem relates to the convergence of
the value functions and optimal controls of the controlled particle systems, i.e., the propagation of chaos
or the law of large numbers. As the number of particles grows to infinity, we explore sufficient conditions
that ensure the aforementioned convergence. Such convergences are possible thanks to the presence of an
L2-regularizerintheobjectivefunctional. Furthermore,quantitativeresultsontheconvergenceratearealso
obtained.
Similar convergence problems for mean field control have been extensively studied recently. To name
a few, we refer to Cardaliaguet et. al. (2022, 2019); Carmona et. al. (2022); Daudin et. al. (2023); Djete
(2022); Gangbo et. al. (2021, 2022); Lacker (2017); Li et. al. (2023); Mayorga et. al. (2023); Tangpi (2022);
Tangpi et. al. (2023), see also Chassagneux et. al. (2022); Ren et. al. (2023, 2022); Coghi et. al. (2016);
2Germain et. al. (2022), as well as the references therein for the ones with uncontrolled particle systems.
The limit of the value functions in the aforementioned convergence is a function whose state variable is a
probability measure. For literature on such limit, see for instance Gangbo et. al. (2021, 2022); Mayorga et.
al. (2023); Pham et. al. (2017); Wu et. al. (2020).
As mentioned before, our model (1.1) is significantly different from the ones above in terms of the form
of control, which thus results in a very different Hamilton–Jacobi–Bellman (HJB) system. Although similar
models are studied in E et. al. (2016, 2017); Labord`ere (2017); Hur´e et. al. (2021, 2022); Kou et. al.
(2016), their emphasis is on the analysis of the corresponding algorithm. The models and results in Bo and
Capponi et. al. (2022); Bo et. al. (2022); E et. al. (2019) are the closest ones to the present paper, where
the convergence of both value functions and optimal controls are investigated. In Bo and Capponi et. al.
(2022);Boet. al.(2022)thelawoflargenumbersisobtainedwherethereisnoquantitativeresults. InEet.
al. (2019) on the other hand, the authors focus on the deterministic control and obtain quantitative results
on large deviations, but the state dynamics f therein is required to be independent of the distribution of
particles. Here, we study models with more general state dynamics f that could depend on the distribution
of particles and obtain quantitative results. More specifically, besides the law of large numbers, we further
showthecorrespondingconvergencerate: asthesamplesizeN growstoinfinity,theminimaoftheobjective
functional, i.e. V and the optimal feedback function θ∗ converge, at certain algebraic rates, to a value
N N
functionandafeedbackfunctionwhosestatevariableistheempiricalmeasureofthesamples. Asaresult,we
showthattheoptimalparametersalsoconvergeatcertainalgebraicrate. Weobtaintwokindsofconvergence
results: theshorttimeconvergenceandtheglobalconvergence,bothaccompaniedwithapreciseconvergence
rates.
TheHJBequationwrittenforthevaluefunctionV associatedtoourmaincontrolproblem,i.e. Problem
N
2.2, formally reads as

∂ tV
N
+ σ 22 i(cid:88) ,jN =1∂ x2 ixjV
N
+ θi ∈n Θf (cid:26) λ 2(cid:12) (cid:12)θ(cid:12) (cid:12)2 +(cid:88) i=N 1f(cid:18) t,θ,x i, N1 (cid:88) jN =1δ xj(cid:19) ∂ xiV
N
+ N1 (cid:88) i=N 1L(x i)(cid:27) =0,
(t,x)∈[0,T)×RN, (1.2)
V
N(T,x 1,...,x N)=
N1
(cid:88)N
U(x i), x∈RN,
i=1
where L and U are the suitably chosen loss function and final cost function, respectively, and the control
functions are valued in some control set Θ and σ ∈R and λ>0 are given further parameters.
To study the convergence of the value functions and optimal feedback functions of Problem 2.2, the
main ingredients are the regularity results on the HJB equations which are uniform and decay suitably with
respect to N – the dimension of the input variables (Theorem 3.17 and Theorem 3.18). This idea is in the
spirit of Gangbo et. al. (2022) where the control is of the mean field type. However, in contrast to this, in
our problem we are faced with the same common control for each particle and the dynamics of each particle
could be nonlinear with respect to the control variable. Hence the resulting Hamiltonian is significantly
different in structure. Moreover, in our problem there is a common Brownian motion in the dynamics of
eachparticle. ThereforethemethodinGangboet. al.(2022)isnolongerapplicabledirectlyinoursituation.
Instead,herewerelymoreonaprobabilisticapproachtoanalyzetheHJBequationandtoobtainthedesired
regularity results.
Thefirst main contributionofthispaperistheuniform(inN)estimatesonthedegeneratePDEsystems
describingV , aswellas∇ V , ∇2 V . SuchuniformestimateswillimplytheconvergencerateofV (t,x)
N x N xx N N
and the corresponding feedback functions θ∗ (t,x). In order to obtain the desired uniform estimates, we
N
apply the nonlinear Feymann–Kac representation and focus on the stochastic processes corresponding to
V , ∇ V and ∇2 V , respectively. Becauseof the degenerate natureof the problem, we needto introduce
N x N xx N
regularizations at several levels: these will be via involving non-degenerate idiosyncratic noise as well as
some suitable cut-off procedures to handle the growth properties of the data. Our estimates will turn out to
be independent of these regularization parameters. It is well known that ∇ V corresponds to the adjoint
x N
processinthestochasticmaximumprinciple. Asaresultofthis,weapplythestochasticmaximumprinciple
3and obtain that each entry of ∇ V decays at the rate of O(N−1). However, the analysis of the systems
x N
involving ∇2 V is more subtle. It turns out that the suitable approximations of ∇2 V introduced above,
xx N xx N
are related to matrix-valued processes (Y in (3.60) and (3.59)) that satisfy backward stochastic Riccati
t
equations. We first analyze the processes Y using the contraction mapping principle and obtain short time
t
estimatesforeachentryofY : the(i,j)−entryofY hasadecayrateofO(δ N−1+N−2). Asfortheglobal
t t ij
estimates, we make further suitable convexity assumptions on the data and analyze the eigenvalues of Y
t
utilizing the Riccati (i.e. quadratic) feature of the corresponding BSDE. Under these extra assumptions,
each eigenvalue of Y decays at the rate of O(N−1) for arbitrary long time horizon T. These convexity
t
assumptions are similar in spirit to displacement convexity (used in Bensoussan et. al. (2019); Carmona
et. al. (2015); Gangbo et. al. (2022)), however, they are not covered by the existing literature (not even
by the displacement monotonicity conditions introduced in Bansil et. al. (2023); Gangbo et. al. (2022)),
because the state dynamics given by f is allowed to have a measure dependence. We note here that such
measure dependence of f has been investigated in Djete (2022); Lacker (2017); Mou et. al. (2022) within
the framework of standard mean field games and control.
Oursecond main contributionistheconvergenceanalysisonV (t,x)andθ (t,x)onaquantitativelevel.
N N
We use a variational approach to show that V and θ∗ are both finite dimensional projection of certain
N N
functions V and θ∗ whose state variables are probability measures. Furthermore, thanks to the previous
uniformestimates,weshowthat,bothV andθ∗ areLipschitzcontinuouswithrespecttotheirstatevariables.
Under our two sets of different assumptions, the previous results hold for a short time horizon or global in
time, respectively. Such convergence of V (t,x) and θ (t,x) has two major implications on neural SDE.
N N
First, the convergence V (t,x) translates to the convergence of minima of objective functionals. Second,
N
the convergence of θ (t,x) would yield pathwise convergence results that translate to the convergence of
N
optimal parameters obtained via neural SDEs (see Proposition 4.7 and Proposition 4.12).
Someconcludingremarks. ThelimitfunctionV isformallyassociatedtoasecondorderHJBequation
set on the Wasserstein space P (R). This formally read as
2
 σ2 (cid:26)(cid:90) (cid:90) (cid:27)
 ∂
Vt
(V T( ,t µ, +µ ))
=θi
∈+
n
Θ
(cid:90)f
(cid:26)2
Uλ
2
((cid:12) (cid:12) yθ )(cid:12)
(cid:12)R
µ2∂
(+
dy yµ
(cid:90)
)V
R
,( ft,
(cid:18)
µµ t) ∈,( θy P,) yµ
2,
((
µ
Rd (cid:19)y )) .∂+
µV(R t,2
µ∂
)µ (µ
yV )µ(t (, dµ y) )( +y, (cid:90)y′ R)µ L( (d yy )) µµ (( dd yy )′ (cid:27))
=0, (t,µ)∈[0,T)×P 2(R),
R
(1.3)
WewouldliketounderlineatthisstagethatstudyingthequantitativedecayestimateswithrespecttoN of
secondorderspacialderivativesofV (thatweperforminthispaper)resultsinthefactthat∂ V existsand
N µ
itisLipschitzcontinuousinasuitablesense. Theverysameanalysisthatweperformontheseobjectscould
bepushedfurther, tostudyquantitativethirdorderderivativeestimatesforV , whichwouldresultintwice
N
differentiability of V, and hence in the fact that V is a classical solution to the HJB equation (1.3). This
would be very much in the flavor of the C2,1,w(P (R)) type estimates from Gangbo et. al. (2022). However,
2
because of the technical burden behind such estimates, we do not pursue the question of classical solutions
to (1.3) in this paper.
The specific choice for L,U and f in the above setting is motivated by the concrete applications in deep
neural networks we have described above. In our analysis, in fact one would be able to allow more general
measure dependent functions in (1.3).
We would like to emphasize once more that connections between equations of type (1.2) and (1.3), and
the corresponding quantitative rates of convergence as N →+∞ have received a great attention in the past
2-3 years in the works Cardaliaguet et. al. (2022, 2023); Daudin et. al. (2023). However, these works were
seeking relationship and convergence rates for viscosity solutions to the corresponding HJB equations. The
results of these papers differ significantly from ours, as their motivation is quite different. In particular, in
thoseworkstheauthorshavealwaysconsiderednon-degenerateidiosyncraticnoiseandnocommonnoise. In
4our models, we consider purely common noise coming from centralized control problems. Also, our analysis
is based on finite dimensional approximations and a careful combination of parabolic PDE techniques and
stochastic analysis of FBSDE systems, while the mentioned papers relied on viscosity solutions techniques
andregularizationproceduresforsemi-concaveandLipschitzcontinuousfunctionsdefinedontheWasserstein
space.
The remainder of the paper is organized as follows. In Section 2 we describe the the model and the main
problem of interest. In Section 3 we first introduce the auxiliary problems and study the regularity of the
correspondingvaluefunctions. Thenweestablishtheestimateonthederivativesofthevaluefunctionaswell
as the verification results associated to the original problem. In Section 4 we show that the value function
V in Problem 2.2 is the finite dimensional projection of a function V whose state variable is in the space of
N
probability measure, and establish the results on the convergence rate. In Section 5, as a concrete example,
we consider a linear quadratic model which falls into our framework and for which closed form solutions are
available.
2 The model problem and standing assumptions
Let T > 0 be a given time horizon. Let (Ω,P,F,F) be an augmented filtered probability space satisfying
the usual conditions, where F=(F ) is the natural filtration generated by a sequence of independent
t t∈[0,T]
Brownian motions {Wi}∞ .
i=0
The[0,T]∋t(cid:55)→Xθ,i(t), i=1,...,N,in(1.1)isasequenceofcontrolleddiffusionprocessescoupledwith
N
the common noise W0(t) and the mean field term 1 (cid:80)N δ . The control [0,T] ∋ t (cid:55)→ θ(t) in (1.1),
N j=1 Xθ,j(t)
N
which is understood as the weight process in deep learning, is shared among the dynamics of all Xθ,i(t).
N
Given x ,x ,..., we consider the optimization problem over the admissible set Uad, consisting of the
1 2
tuple (Ω,P,F,F,{Wi}+∞,θ) such that
i=0
• θ is predictable, θ(t)∈Θ, t∈[0,T];
• for each N ≥1, (cid:0) {x }N ,{Wi}N ,θ(cid:1) is a weak solution to (1.1).
i i=1 i=0
When there is no ambiguity, we use θ to denote the admissible control.
Given a control θ ∈ Uad and N inputs (x ,...,x ) ∈ RN, we can further define the objective function
1 N
J :Uad×[0,T]×RN →R as follow
N
J (θ,t,x ,...,x )
N 1 N
:=E(cid:20) 1 (cid:88)N (cid:90) T L(cid:0) Xθ,i(t)(cid:1)
dt+
1 (cid:88)N U(cid:0) Xθ,i(T)(cid:1)
+
λ(cid:90) T |θ(t)|2dt(cid:21)
, (2.1)
N N N N 2
i=1 0 i=1 0
The third term on the right hand side of (2.1) is the regularizer. It is straightforward but notationally
cumbersome to generalize our results to the case where Θ=Rd and x ∈Rm. For the ease of notations and
i
convenience in this paper we choose d=m=1.
In our analysis we consider the space of Borel probability measures, supported in Euclidean spaces Rm.
We work on the specific subset of these measures, which have finite second moment, and denote this by
P (Rm). We equip this subset with the classical 2-Wasserstein distance W .
2 2
Here we make the following technical assumptions on parameters.
Assumption 2.1. Assume that
1. The function [0,T]×Θ∋(t,θ)(cid:55)→f(t,θ,0,δ ) is continuous, where Θ=R;
{0}
52. the function f : [0,T]×Θ×R×P(R) → R is such that ∂ f is bounded and has bounded derivatives
t
with respect to (θ,x,µ) up to the second order;
3. For φ∈{L,U}, φ≥0, and there exist constants Cφ, Cφ, Cφ such that
11 10 20
|φ′(x)|≤Cφ|x|+Cφ, |φ′′(x)|≤Cφ. (2.2)
11 10 20
InAssumption2.1,byderivativewithrespecttoµwemeantheintrinsicderivative,theso-calledWasser-
stein derivative (see for instance (Cardaliaguet et. al., 2019, Definition 2.2.2) or (Carmona et. al., 2018,
Chapter 5) and the discussion therein). In particular, when we say differentiability with respect to the mea-
sure variable, we always mean the so-called fully C1, C2, etc. classes (see (Carmona et. al., 2018, Chapter
5)). In what follows we use the notation ∂ to denote this intrinsic Wasserstein derivative. We denote by x˜
µ
the new variable arising after applying ∂ , and we display this after the measure variable as ∂ g(µ,x˜), for
µ µ
any g ∈C1(P (RM)).
2
We note here that the optimization of J under the constraint (1.1) can be understood as a learning
N
process with neural SDEs. Suppose we are to determine a system with dynamics
dX(t)=g(cid:0) t,X(t),L(X(t))(cid:1) dt+σdW˜0(t), (2.3)
where the diffusion coefficient σ has been observed but the drift g(t,x,µ) is unknown. The motivation of
determining such g(t,x,µ) could be mimicking the genuine dynamics of certain processes (e.g. Chen et.
al. (2018)) or finding an optimal feedback function (e.g. Baudelet et. al. (2023); Carmona et. al. (2021);
Ruthottoet. al.(2020)). Thelogicbehindthelearningprocessistoapproximateg(t,x,µ)withthecandidate
function chosen from the family f(t,θ(t),x,µ) where θ(t) is the parameter to be determined. Given inputs
x ,x ,...,x ,wemayusethedynamicsin(1.1)toapproximate(2.3)accordingtoappropriateperformance
1 2 N
functionals. Abstractly speaking, the training process is equivalent to obtaining the optimal control θ∗ of
the following optimization problem:
Problem 2.2. Minimizing (2.1) over Uad.
Denote the value function to Problem 2.2 by
V (t,x ,...,x ):= inf J (θ,t,x ,...,x ), (2.4)
N 1 N N 1 N
θ∈Uad
and θ∗ (t,x ,...,x ) one of the optimal feedback functions (if exists). Suppose that
N 1 N
(cid:18) N (cid:19)
1 (cid:88)
W δ ,µ −→ 0 as N →+∞,
2 N xk
k=1
where µ ∈ P (R) is a given probability measure. We are interested in the convergence as well as the
2
convergence rate of both V (t,x ,...,x ) and θ∗ (t,x ,...,x ) to their corresponding limits. To the
N 1 N N 1 N
questions above, we give our positive answers in Section 4.
3 The auxiliary problems and corresponding uniform estimates
In order to study the aforementioned convergence as well as the convergence rate, we establish uniform
derivative estimates on V as the number of variables increases, which is different from the usual PDE
N
estimates. Our results include the uniform estimates on the first and the second order derivatives of V .
N
These estimates are used in Section 4. It turns out (as we will see in the next section) that the estimates on
the first order derivatives yield the convergence rate of V (t,x ,...,x ), while the estimates on the second
N 1 N
order derivatives yield the convergence rate of θ∗ (t,x ,...,x ).
N 1 N
63.1 The auxiliary problems and the estimates on the first order derivatives
To solve (2.4), the dynamic programming principle yields the HJB equation

∂ tV
N
+ σ 22 i(cid:88) ,jN =1∂ x2 ixjV
N
+ θi ∈n Θf (cid:26) λ 2(cid:12) (cid:12)θ(cid:12) (cid:12)2 +(cid:88) i=N 1f(cid:18) t,θ,x i, N1 (cid:88) jN =1δ xj(cid:19) ∂ xiV
N
+ N1 (cid:88) i=N 1L(x i)(cid:27) =0,
(t,x)∈[0,T)×RN, (3.1)
V
N(T,x 1,...,x N)=
N1
(cid:88)N
U(x i), x∈RN.
i=1
The equation (3.1) is degenerate parabolic, as the Fourier symbol of the second order differential operator
is given by
σ2 (cid:88)N σ2 (cid:32) (cid:88)N (cid:33)2
ξ ξ = ξ .
2 i j 2 i
i,j=1 i=1
Hence the classical solution to (3.1) is not guaranteed by standard results.
In order to study (3.1), we introduce the following auxiliary equation with parameters R=(R ,R ) and
1 2
ε:

 ∂
Vt
NεV ,RNε, (+R T,+
θ
x∈in
1Θσ
2
,f
R2
.2
.i(cid:88)
(cid:26),
.jN
,=
λ
2
x1
N(cid:12)
(cid:12)∂
θ
)x2
(cid:12)
(cid:12)2i =x
+jV
N1(cid:88)
iN
=Nε, (cid:88)1R Nf+
(cid:18)
Utε
R2
,2
θ
1(cid:88) (,i=N
xx1
ii
),∂
,Nx
12
ix
(cid:88)
ji
N
=V 1Nε δ,R
xj(cid:19)
∂ xiV Nε,R N1 (cid:88)
i=N
1L R1(x
i)(cid:27)
=0, (3.2)
i=1
where Θ :=Θ∩B (0) and for φ∈{L,U} we have defined the smooth truncated version φ satisfying
R2 R2 R1
1. φ (x)=φ(x) on x∈B (x), |φ (x)|≤|φ(x)|;
R1 R1 R1
2. φ , ∇ φ , ∇2φ are bounded;
R1 x R1 x R1
3. The derivatives satisfy
|φ′ (x)|≤Cφ|x|+Cφ, |φ′′ (x)|≤Cφ. (3.3)
R1 11 10 R1 20
Thesederivativeboundsandgrowthratesonthetruncatedfunctionscanbeguaranteedbecauseofthemain
assumptions on L,U, which we imposed in Assumption 2.1.
The equation above corresponds to the auxiliary optimization problem with he underlying training pro-
cesses
(cid:18) N (cid:19)
1 (cid:88)
dXε,θ,i(t)=f t,θ(t),Xε,θ,i(t), δ dt+εdWi(t)+σdW0(t), i=1,...,N, (3.4)
N N N X Nε,θ,j(t)
j=1
and the admissible set Uad consists of θ ∈Uad with |θ(t)|≤R , t∈[0,T], as well as the objective function
R2 2
Jε,R1 :Uad ×[0,T]×RN →R is defined as
N R2
Jε,R1(θ,t,x ,...,x )
N 1 N
7:=E(cid:20) 1 (cid:88)N (cid:90) T
L
(cid:0) Xθ,i(t)(cid:1)
dt+
1 (cid:88)N
U
(cid:0) Xθ,i(T)(cid:1)
+
λ(cid:90) T |θ(t)|2dt(cid:21)
.
N R1 N N R1 N 2
i=1 0 i=1 0
Suppose that Assumption 2.1 takes place. Then we have
Vε,R(t,x ,...,x )= inf Jε,R1(θ,t,x ,...,x ). (3.5)
N 1 N N 1 N
θ∈Uad
R2
Usingthecorrespondingvariationalrepresentations, itisstraightforwardtoshowthefollowingconvergences
lim Vε,R(t,x ,...,x )= inf Jε,R1(θ,t,x ,...,x )=:Vε,R1(t,x ,...,x ), (3.6)
N 1 N N 1 N N 1 N
R2→+∞ θ∈Uad
lim Vε,R1(t,x ,...,x )= inf Jε(θ,t,x ,...,x )=:Vε(t,x ,...,x ), (3.7)
N 1 N N 1 N N 1 N
R1→+∞ θ∈Uad
lim Vε,R(t,x ,...,x )= lim Vε,R1(t,x ,...,x )=V (t,x ,...,x ), (3.8)
N 1 N N 1 N N 1 N
R1,R2→+∞ R1→+∞
ε→0 ε→0
whereforthetrainingprocessesin(3.4)wehaveintroducedyetanotherobjectivefunctionJε :Uad×[0,T]×
N
RN →R is defined as
Jε(θ,t,x ,...,x )
N 1 N
:=E(cid:20) 1 (cid:88)N (cid:90) T L(cid:0) Xε,θ,i(t)(cid:1)
dt+
1 (cid:88)N U(cid:0) Xε,θ,i(T)(cid:1)
+
λ(cid:90) T |θ(t)|2dt(cid:21)
.
N N N N 2
i=1 0 i=1 0
After some modification of standard results on parabolic PDEs (that we detail below), we can show that
the HJB equation (3.2) admits a solution Vε,R ∈C1+γ 2,2+γ(cid:0) [0,T)×RN(cid:1) ∩C(cid:0) [0,T]×RN(cid:1) . In this section,
N loc
we establish uniform estimates on Vε,R and its first order derivatives, especially uniform in (ε,N). Different
N
from the usual PDE estimates, the estimates here are focused more on the dimension of variables since the
dimension, which corresponds to the number of samples, is now changing. We begin with the existence and
uniqueness of classical solution to (3.2).
Lemma 3.1. Suppose that Assumption 2.1 takes place. Then the HJB equation (3.2) admits a unique
bounded solution Vε,R ∈ C1+γ 2,2+γ(cid:0) [0,T)×RN(cid:1) ∩C(cid:0) [0,T]×RN(cid:1) where 0 < γ < 1 and ∂ Vε,R, ∂ Vε,R,
N loc t N xi N
∂2 Vε,R, 1≤i,j ≤N are bounded.
xixj N
Proof. Notice that L and U as well as their derivatives are all bounded. According to Theorem 4.4.3,
R1 R1
Theorem 4.7.2 and Theorem 4.7.4 in Krylov (1980), the value function Vε,R defined in (3.5) is the weak
N
solution (in the distributional sense) to (3.2), furthermore, Vε,R and its weak derivatives ∂ Vε,R, ∂ Vε,R,
N t N xi N
∂2 Vε,R, 1≤i,j ≤N are all bounded. Note also that for (t,x)∈(0,T)×RN
xixj N
σ2 (cid:88)N ε2 (cid:88)N
∂ Vε,R(t,x)+ ∂2 Vε,R(t,x)+ ∂2 Vε,R(t,x)=g(t,x), (3.9)
t N 2 xixj N 2 xixi N
i,j=1 i=1
where
(cid:26) N (cid:18) N (cid:19) N (cid:27)
g(t,x):=− θ∈in Θf
R2
λ 2(cid:12) (cid:12)θ(cid:12) (cid:12)2 +(cid:88) i=1f t,θ,x i, N1 (cid:88) j=1δ
xj
∂ xiV Nε,R+ N1 (cid:88) i=1L R1(x i) .
As is shown above, ∂2 Vε,R is bounded. Moreover, we have from Corollary 4.7.8 of Krylov (1980) that for
xixj N
0 < γ < 1, ∇ Vε,R(t,x) is γ-H¨older with respect to t (uniformly in x). Hence g(t,x) is locally Lipschitz
x N 2
continuous with respect to x and H¨older continuous with respect to t. Let us view Vε,R as the solution to
N
PDE (3.9) with constant coefficients, where the terminal conditions are the same as (3.2). Standard results
then yield that ∂ Vε,R, ∂ Vε,R, ∂2 Vε,R ∈Cγ 2,γ(cid:0) [0,T)×RN(cid:1) , 1≤i,j ≤N.
t N xi N xixj N loc
Asfortheuniqueness,wecanusethestochasticcontrolinterpretationto(3.2)andshowthatanysolution
Vε,R equals the value function in (3.5) by the standard verification results.
N
8Notice that at the moment the bound on Vε,R, ∂ Vε,R, ∂2 Vε,R, 1≤i,j ≤N might depend on ε, R
N xi N xixj N
and N. Before establishing uniform estimates with respect to ε, R and N, we need a refined analysis on the
sample path.
Lemma 3.2. Let x ∈R, 1≤i≤N, θ(t) be an admissible control, and X (t) be the associated sample path
i N
in (3.4). ThenthereexistsaconstantC˜ =C˜ (f,T)(dependingonlyonf,T, independentofN,ε,σ,R ,R ),
1 1 1 2
increasing in T, such that
E|X Nε,θ,i(t)|2 ≤C˜ 1(cid:18) 1+|x i|2+E(cid:90) t (cid:12) (cid:12)f(cid:0) s,θ(s),0,δ {0}(cid:1)(cid:12) (cid:12)2 ds+ N1 (cid:88)N |x j|2(cid:19) . (3.10)
0 j=1
Proof. For x ,...,x ∈R, denote by
1 N
(cid:18) N (cid:19)
f˜(cid:0)
s,θ,x ,...,x
(cid:1)
:=f s,θ,x ,
1 (cid:88)
δ ,
i 1 N i N xk
k=1
then
(cid:18) N (cid:19) (cid:18) N (cid:19)
∂
f˜(cid:0)
s,θ,x ,...,x
(cid:1)
=δ f s,θ,x ,
1 (cid:88)
δ +
1
∂ f s,θ,x ,
1 (cid:88)
δ ,x ,
xj i 1 N ij x i N xk N µ i N xk j
k=1 k=1
where δ stands for the Kronecker symbol. We represent the dynamics of Xε,θ,i(t) in (3.4) in such a way
ij N
that
(cid:90) t N (cid:90) t
Xε,θ,i(t)=x + f˜(cid:0) s,θ(s),0,...,0(cid:1) ds+(cid:88) ∆ f˜(cid:0) s,θ(s),,Xε,θ,1(s),...,Xε,θ,N(s)(cid:1) ds
N i i j i N N
0 j=1 0
+σW0(t)+εWi(t),
where for j =1,...,N,
∆
f˜(cid:0) s,θ(s),Xε,θ,1(s),...,Xε,θ,N(s)(cid:1)
j i N N
:=f˜(cid:0) s,θ(s),0,0,...,0,Xε,θ,j(s),...,Xε,θ,N(s)(cid:1) −f˜(cid:0) s,θ(s),0,0,...,0,Xε,θ,j+1(s),...,XN(s)(cid:1)
.
i N N i N N
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
(j−1)-times j -times
According to the Lipschitz continuity, we can deduce
(cid:12) (cid:12)∆ jf˜ i(cid:0) s,θ(s),X Nε,θ,1(s),...,X Nε,θ,N(s)(cid:1)(cid:12) (cid:12)≤(cid:0) δ ij∥∂ xf∥ ∞+N−1∥∂ µf∥ ∞(cid:1)(cid:12) (cid:12)X Nj (s)(cid:12) (cid:12).
Therefore there exist constant C = C (f) and the corresponding matrix valued process A (s) satisfying
1 1 N
A (s)∈M (C ) such that
N N 1
(cid:90) t (cid:90) t
Xε,θ(t)=x+ f(cid:0) s,θ(s),0,δ (cid:1) 1ds+ A (s)Xε,θ(s)ds
N {0} N N
0 0
+εWN(t)+σ1W0(t),
where
WN(t):=(W1(t),...,WN(t))⊤,1:=(1,...,1)⊤.
Here for the brevity of expression we have introduced the subset M (C)⊂RN×N such that
N
A∈M (C) if and only if |A |≤C(δ +N−1), 1≤i,j ≤N. (3.11)
N ij ij
Solving the linear SDE above, we have
(cid:90) t
Xi (t)=(cid:0) Φ+(t)x(cid:1) + f(cid:0) s,θ(s),0,δ (cid:1)(cid:0) Φ+(t)Φ−(s)1(cid:1) ds
N N i {0} N N i
0
9(cid:18)(cid:90) t (cid:19) (cid:90) t
+ε Φ+(t)Φ−(s)dWN(s) +σ (cid:0) Φ+(t)Φ−(s)1(cid:1) dW0(s), (3.12)
N N N N i
0 i 0
where the matrix valued processes Φ±(s) solve
N
(cid:90) t (cid:90) t
Φ+(t)=I + A (s)Φ+(s)ds, Φ−(t)=I − Φ−(s)A (s)ds.
N N N N N N N N
0 0
Note that
d (cid:2) Φ−(t)Φ+(t)(cid:3) =0 and Φ−(0)Φ+(0)=I ,
dt N N N N N
thus Φ−(t)Φ+(t)=Φ−(0)Φ+(0)=I .
N N N N N
According to Lemma A.2, Φ±(s) ∈ M (C ) for some C = C (f,T) because A (s) ∈ M (C ). More-
N N 2 2 2 N N 1
over, Φ+(t)Φ−(s) ∈ M (C ) due to Lemma A.1. An application of Burkholder–Davis–Gundy inequality
N N N 2
(see e.g. Yong et. al. (1999)) to the i-th component in (3.12) gives the estimate (3.10).
Remark 3.3. Take θ(t)≡0 (which is admissible since 0∈Θ), then (3.10) can be rephrased as
(cid:18) N (cid:19)
E|Xε,0,i(t)|2 ≤C˜ 1+|x |2+ 1 (cid:88) |x |2 . (3.13)
N 1 i N j
j=1
Based on Lemma 3.2, we can go on with the estimates on the first derivatives. In the context below, the
values of constants C , C˜ , k ≥ 1, might vary, but their dependence on the model parameters remains the
k k
same.
For (t,p,q,z,θ)∈[0,T]×RN ×RN ×RN×N ×RN, define the following Hamiltonian
N (cid:18) N (cid:19) N
H
NR1(t,x,p,θ):=λ(cid:12) (cid:12)θ(cid:12) (cid:12)2 +(cid:88)
f t,θ,x i,
N1 (cid:88)
δ
xj
p i+
N1 (cid:88)
L R1(x i), (3.14)
i=1 j=1 i=1
as well as, for later use,
N (cid:18) N (cid:19) N
(cid:12) (cid:12)2 (cid:88) 1 (cid:88) 1 (cid:88)
H N(t,x,p,θ):=λ(cid:12)θ(cid:12) + f t,θ,x i,
N
δ
xj
p i+
N
L(x i). (3.15)
i=1 j=1 i=1
With the preparation above, we show the following estimates on Vε,R in (3.2).
N
Lemma 3.4. Let x ∈ R, 1 ≤ i ≤ N and Vε,R be the solution to (3.2). Then there exists a constant
i N
C˜
2
=C˜ 2(f,λ−1 2,T), increasing in T,λ−1 2, independent of N, σ, ε and R such that for 1≤i≤N,
(cid:12) (cid:12)∂ xiV Nε,R(t,x)(cid:12) (cid:12)≤ C˜ 2(C 1L 1 N+C 1U 1)(cid:18) 1+|x i|2+ N1 (cid:88)N |x j|2(cid:19)1 2 + C˜ 2(C 1L 0 N+C 1U 0) . (3.16)
j=1
Proof. We begin by showing the existence of a constant Cˆ =Cˆ (f,T) such that
2 2
(cid:12) (cid:12)∂ Vε,R(t,x)(cid:12) (cid:12)≤ Cˆ 2(C 1L 1+C 1U 1)(cid:18) 1+|x |2+E(cid:20)(cid:90) T |f(cid:0) s,θ∗(s),0,δ (cid:1) |ds(cid:21) + 1 (cid:88)N |x |2(cid:19)1 2
(cid:12) xi N (cid:12) N i {0} N j
0 j=1
Cˆ (CL +CU)
+ 2 10 10 , (3.17)
N
where θ∗ is the optimal control process.
10ItsufficestoshowtheexistenceofsuchCˆ fort=0. Forothert∈[0,T]theproofandCˆ canbededuced
2 2
in the same way.
In view of Lemma 3.1, the HJB equation (3.2) admits a classical solution. Following the standard
verification procedure (see e.g. Fleming et. al. (2006)), one can show the existence of an optimal control (in
the weak sense) and the corresponding optimal path. Hence we may denote by θ∗(t) and (X∗(t),Y∗(t)) the
N N
optimalcontrol,optimalpathaswellastheadjointprocess. Accordingtothestochasticmaximumprinciple,
we have the adjoint equation (in the weak sense) as follow

N
dY N∗,i(t)=−∂ xiH NR1(t,X N∗(t),Y N∗(t),θ∗(t))dt+(cid:88) j=0Z Nij(t)dWj(t),
(3.18)
dX∗,i(t)=∂ HR1(t,X∗(t),Y∗(t),θ∗(t))dt+εdWi(t)+σdW0(t),
 XN ∗(0)=xp ,i YN ∗,i(T)N
=
1
UN
′
(cid:16) X∗,i(T)(cid:17)
.
N N N R1 N
Here Y∗, X∗ ∈ RN, Z ∈ RN×N, and recall that HR1(t,x,p,θ) is given in (3.14). Rewrite (3.18) in the
N N N N
following manner:
 (cid:20) N (cid:21) N
dY N∗,i(t)=− (cid:88) j=1AN ij(t)Y N∗,j(t)+ N1 L′ R1(X N∗,i(t)) dt+(cid:88) j=0Z Nij(t)dWj(t)
(cid:18) N (cid:19)
1 (cid:88) (3.19)
dX∗,i(t)=f t,θ∗(t),X∗,i(t), δ dt+εdWi(t)+σdW0(t),
 XN
∗(0)=x,
Y∗,i(T)N
=
1
UN
′
j (cid:0)= X1
∗,iX (N∗ T,j )( (cid:1)t ,)
N N N R1 N
where for 1≤i,j ≤N,
(cid:18) N (cid:19) (cid:18) N (cid:19)
1 (cid:88) 1 1 (cid:88)
AN(t):=δ f t,θ∗(t),X∗,j(t), δ + ∂ f t,θ∗(t),X∗,j(t), δ ,X∗,i(t) . (3.20)
ij ij x N N X N∗,k(t) N µ N N X N∗,k(t) N
k=1 k=1
Consider the matrix valued processes Φ±(t)∈RN×N solving
N
(cid:90) t
Φ+(t)=I − AN(s)Φ+(s)ds, (3.21)
N N N
0
(cid:90) t
Φ−(t)=I + Φ−(s)AN(s)ds. (3.22)
N N N
0
Then
Φ−(t)Y∗(t)= 1 E (cid:2) Φ−(T)U′ (cid:0) X∗(T)(cid:1)(cid:3) + 1 E (cid:20)(cid:90) T Φ−(s)L′ (cid:0) X∗(s)(cid:1) ds(cid:21) , (3.23)
N N N t N R1 N N t N R1 N
t
where
(cid:16) (cid:17)⊤
U′ (X∗(T)):= U′ (X∗,1(T)),...,U′ (X∗,N(T)) ,
R1 N R1 N R1 N
(cid:16) (cid:17)⊤
L′ (X∗(s)):= L′ (X∗,1(s)),...,L′ (X∗,N(s)) .
R1 N R1 N R1 N
In particular,
(cid:34) (cid:35)
Y∗(0)= 1 E(cid:2) Φ−(T)U′(X∗(T))(cid:3) + 1 E (cid:90) T Φ−(s)L′ (X∗(s))ds ,
N N N N N N R1 N
0
11and thus
Y∗,i(0)2 ≤ 2 (cid:12) (cid:12)E(cid:0) Φ−(T)U′ (X∗(T))(cid:1) (cid:12) (cid:12)2 + 2T (cid:90) T (cid:12) (cid:12)E(cid:0) Φ−(s)L′ (X∗(s))(cid:1) (cid:12) (cid:12)2 ds (3.24)
N N2 N R1 N i N2 N R1 N i
0
According to (3.20), A ∈ M (C ) with C = C (f). In view of Lemma A.2 and (3.22), for A (t) ∈
N N 1 1 1 N
M (C ), it follows that
N 1
EΦ−(s)∈M (C ), C =C (f,T), s∈[0,T].
N N 2 2 2
Note (2.2) and (3.3), for the i-th entry of Φ−(T)U′ (X∗(T)):
N R1 N
(cid:12) (cid:12)E(cid:0) Φ−(T)U′ (X∗(T))(cid:1) (cid:12) (cid:12)2
N R1 N i
(cid:20) (cid:21) (cid:20) (cid:21) (cid:20) N (cid:21) (cid:20) N (cid:21)
≤2E (cid:0) Φ−(T)(cid:1)2 E U′ (cid:0) X∗,i(T)(cid:1)2 +2E (cid:88)(cid:0) Φ−(T)(cid:1)2 E (cid:88) U′ (cid:0) X∗,j(T)(cid:1)2
N ii R1 N N ij R1 N
j=1 j=1
j̸=i j̸=i
(cid:20) (cid:21) (cid:20) N (cid:21)
≤C E U′ (cid:0) X∗,i(T)(cid:1)2 + C 2E (cid:88) U′ (cid:0) X∗,j(T)(cid:1)2
2 R1 N N R1 N
j=1
j̸=i
(cid:18) N (cid:19)
≤C (CU)2 1+E(cid:2) |X∗,i(T)|2(cid:3) + 1 (cid:88) E(cid:2) |X∗,j(T)|2(cid:3) +C (CU +1)2. (3.25)
2 11 N N N 2 10
j=1
Similarly,
(cid:12) (cid:12)E(cid:0) Φ−(s)L′ (X∗(s))(cid:1) (cid:12) (cid:12)2
N R1 N i
(cid:18) N (cid:19)
≤C (CU)2 1+E(cid:2) |X∗,i(s)|2(cid:3) + 1 (cid:88) E(cid:2) |X∗,j(s)|2(cid:3) +C (CU +1)2. (3.26)
2 11 N N N 2 10
j=1
In view of Lemma 3.2,
E(cid:2)(cid:12) (cid:12)X N∗,i(t)(cid:12) (cid:12)(cid:3) ≤C˜ 1(cid:18) |x i|2+E(cid:90) t |f(cid:0) s,θ∗(s),0,δ {0}(cid:1) |ds+ N1 (cid:88)N |x j|2(cid:19) . (3.27)
0 j=1
Plugging (3.25), (3.26) and (3.27) into (3.24), we obtain (3.17).
To further prove (3.16), it suffices to prove that there exist constant Cˇ = Cˇ(f,λ− 21,T) (increasing in
λ− 21) such that
E(cid:90) t |f(cid:0) s,θ∗(s),0,δ (cid:1) |ds≤Cˇ(cid:18) 1+ 1 (cid:88)N |x |2(cid:19) 21 .
{0} N i
0 i=1
In fact,
(cid:90) t (cid:90) t (cid:90) t
E |f(cid:0) s,θ∗(s),0,δ (cid:1) |ds≤ |f(cid:0) s,0,0,δ (cid:1) |ds+∥f ∥ E |θ∗(s)|ds.
{0} {0} θ ∞
0 0 0
And we notice that
(cid:90) t (cid:18)(cid:90) T (cid:19)1
E |θ∗(s)|ds≤T1 2E |θ∗(s)|2ds 2 ≤(2T)21λ− 21J N(θ∗,0,x 1,...,x N)1 2
0 0
(cid:18) N (cid:19)1
≤(2T)1 2λ− 21J N(0,0,x 1,...,x N)1 2 ≤Cˇ 1+ N1 (cid:88) |x i|2 2 , (3.28)
i=1
wherethelastinequalityholdsbecauseof (3.13). Hencewemaydeduce(3.16)fromtheestimatesabove.
12The next lemma shows that, thanks to Lemma 3.4, we may drop the parameter R in (3.2) and consider
2

∂ tV Nε,R1 + σ 22 (cid:88)N ∂ x2 ixjV Nε,R1 + ε 22 (cid:88)N ∂ x2 ixiV Nε,R1 +H˜ NR1(t,x,∇ xV Nε,R1)=0,
i,j=1 i=1
(3.29)
V
Nε,R1(T,x 1,...,x N)=
N1 (cid:88)N
U R1(x i),
i=1
where for (t,x,p)∈[0,T]×RN ×RN,
(cid:26) N (cid:18) N (cid:19) N (cid:27)
H˜ NR1(t,x,p):= θin ∈Rf λ 2(cid:12) (cid:12)θ(cid:12) (cid:12)2 +(cid:88) f t,θ,x i, N1 (cid:88) δ
xj
p i+ N1 (cid:88) L R1(x i) . (3.30)
i=1 j=1 i=1
Lemma 3.5. The equation (3.29) admits a unique classical solution Vε,R1 ∈ C1+γ 2,2+γ(cid:0) [0,T)×RN(cid:1) ∩
N loc
C(cid:0) [0,T]×RN(cid:1) where for 0 < γ < 1 and Vε,R1, ∂ Vε,R1, ∂ Vε,R1, ∂2 Vε,R1, 1 ≤ i,j ≤ N are bounded.
N t N xi N xixj N
Moreover, the derivatives ∂ Vε,R1, 1≤i≤N, satisfy
xi N
(cid:12) (cid:12)∂ xiV Nε,R1(t,x)(cid:12) (cid:12)≤ C˜ 2(C 1L 1 N+C 1U 1)(cid:18) 1+|x i|2+ N1 (cid:88)N |x j|2(cid:19) 21 + C˜ 2(C 1L 0 N+C 1U 0) , (3.31)
j=1
where the constant C˜ is from Lemma 3.4.
2
Proof. We recall Lemma 3.1 saying that (3.2) admits classical solutions Vε,R. Moreover, since L and U
both have bounded derivatives, in view of Lemma 3.4,
(cid:12)
(cid:12)∇ xV
Nε,R(cid:12)
(cid:12) is
bounN
ded by a constant
indeR p1 endentR o1
f
R . Therefore, for sufficiently large R , we have
2 2
(cid:26) N (cid:18) N (cid:19) N (cid:27)
θ∈in Θf
R2
λ 2(cid:12) (cid:12)θ(cid:12) (cid:12)2 +(cid:88) i=1f t,θ,x i, N1 (cid:88) j=1δ
xj
∂ xiV Nε,R(t,x)+ N1 (cid:88) i=1L R1(x i)
(cid:26) (cid:27)
= inf ···
=H˜R1(cid:0)
t,x,∇
Vε,R(t,x)(cid:1)
. (3.32)
θ∈R N x
In other words, for those R satisfying (3.32), Vε,R solves (3.29). Choose an arbitrary R such that Vε,R
2 N 2 N
satisfies (3.32) and denote it by Vε,R1. We thus have by Lemma 3.1 that Vε,R ∈C1+γ 2,2+γ(cid:0) [0,T)×RN(cid:1) ∩
N N loc
C(cid:0) [0,T]×RN(cid:1) and Vε,R,∂ Vε,R, ∂ Vε,R, ∂2 Vε,R are bounded. We can show the uniqueness via the
N t N xi N xixj N
variationalargumentsdescribedinLemma3.1. Wecanalsoobtain(3.31)fromLemma3.4sinceitissatisfied
by any Vε,R.
N
3.2 The estimates on the second order derivatives
In this section we establish uniform estimates on the second order derivatives ∂2 Vε,R1, 1 ≤ i,j ≤ N for
xixj N
the solution to (3.29) where the parameter R has been dropped. To do so, our idea is to formally take the
2
derivatives with respect to x and x in (3.29) and obtain the PDE system on ∂2 Vε,R1, 1≤i,j ≤N. The
i j xixj N
above differentiation requires further analysis on the differentiability of Hamiltonian in (3.29). It then turns
out that the aforementioned analysis involves the uniform estimates on the first order derivatives in (3.31).
We can see from (3.31) that the first order derivatives therein are only locally bounded in general. In our
forthcominganalysis,weproposesometechnicalassumptionssoastodealwiththisnon-globalboundedness.
Denote by A the set consisting of real numbers p , ..., p , x , ..., x satisfying
N 1 N 1 N
|p |<
C˜ 2(C 1L 1+C 1U 1)(cid:18)
1+|x |2+
1 (cid:88)N
|x
|2(cid:19)1 2
+
C˜ 2(C 1L 0+C 1U 0)
. (3.33)
i N i N j N
j=1
13In other words,
A :=(cid:8) (x,p)∈RN ×RN :(x,p) satisfies (3.33)(cid:9) . (3.34)
N
We assume that the following hold in the remaining of the paper.
Hypothesis (R) Suppose the following
1. There exists λ >0, such that for any (θ,x,p)∈Θ×A , N ≥1
0 N
(cid:18) N (cid:19)
λ 0 ≥∂2 f t,θ,x , 1 (cid:88) δ p . (3.35)
N θθ i N xj i
j=1
2. There exists CQ > 0 such that for any (θ,x,p) ∈ Θ×A , N ≥ 1 and for φ ∈ {|∂2 f|,|∂2 f|} and
N xθ xx
ϕ∈{|∂2 f|, |∂2 f|}
xµ θµ
(cid:18) N (cid:19) (cid:18) N (cid:19)
1 (cid:88) 1 (cid:88)
CQ >φ t,θ,x , δ ·Np +ϕ t,θ,x , δ ,x ·Np . (3.36)
i N xj i i N xj i i
j=1 j=1
3. The coefficient λ is taken such that λ>λ .
0
Remark 3.6. In terms of Hypothesis (R), we have the following examples.
1. For an LQ model with uncontrolled diffusion, ∂2 f,∂2 f,∂2 f,∂2 f = 0 and (3.35), (3.36) holds
xθ xx xµ θµ
trivially.
2. For f, L, U with bounded derivatives, CL +CU =0 and (3.35), (3.36) holds trivially.
11 11
Given (3.35), for (x,p) ∈ A , the corresponding HR1(t,x,p,θ) in (3.14) is strictly convex in θ. Hence
N N
the unique minimizer θR1 ∈Θ can be defined as a function of (t,x,p) in such a way that
N
θR1(t,x,p):=argminHR1(t,x,p,θ). (3.37)
N N
θ∈Θ
In light of the definition above, an optimal control θ∗(t) in (3.18) can be represented as
θ∗(t)=θR1(t,X∗(t),Y∗(t)).
N N N
ThankstoHypothesis (R),wecannowshowtheLipschitzcontinuityofthefeedbackfunctionθR1(t,x,p).
N
Lemma 3.7. Suppose Hypothesis (R). Then θR1(t,x,p) is smooth with respect to (x,p) ∈ A with
N N
derivatives
(cid:12) (cid:12)∂ xkθ NR1(t,x,p)(cid:12) (cid:12)≤ (λ−λ N0)−1CQ , (cid:12) (cid:12)∂ pkθ NR1(t,x,p)(cid:12) (cid:12)≤(λ−λ 0)−1∥f θ∥ ∞, k =1,...,N. (3.38)
Proof. We postpone the proof of this result to Appendix A.
We have the following estimates on the coefficients based on Lemma 3.7.
Lemma 3.8. Suppose Hypothesis (R), then there exists a constant C˜
3
=C˜ 3(f,λ1 2,T,L,(λ−λ 0)−1), in-
creasing in T, λ− 21, (λ−λ 0)−1, such that for (x,p)∈A N,
(cid:12) (cid:12)∂ xiH˜ NR1(t,x,p)(cid:12) (cid:12),(cid:12) (cid:12)∂ x2 ipjH˜ NR1(t,x,p)(cid:12) (cid:12)≤C˜ 3N−1, (cid:12) (cid:12)∂ x2 ixjH˜ NR1(t,x,p)(cid:12) (cid:12)≤C˜ 3N−1(δ
ij
+N−1),
(cid:12) (cid:12)∂ piH˜ NR1(t,x,p)(cid:12) (cid:12),(cid:12) (cid:12)∂ p2 ipjH˜ NR1(t,x,p)(cid:12) (cid:12)≤C˜ 3, 1≤i,j ≤N. (3.39)
Proof. Recall (3.30) and (3.37),
H˜R1(t,x,p)=HR1(cid:0) t,x,p,θR1(t,x,p)(cid:1)
, (x,p)∈A .
N N N N
Hence we can obtain the above estimates via (3.38).
143.2.1 Short time estimates
As is mentioned before, with the preparation above, we may take partial derivatives in (3.29) and derive the
equation satisfied by Vε,kl :=∂2 Vε,R1. We begin with a regularity results which validates the differenti-
N xkxl N
ation.
Lemma 3.9. Suppose Hypothesis (R). The equation (3.29) admits a unique classical solution Vε,R1 ∈
N
C(cid:0) [0,T]×RN(cid:1) where Vε,R1,∂ Vε,R1,∂2 Vε,R1,1 ≤ i,j ≤ N are bounded. Moreover for 0 < γ < 1,
N xi N xixj N
Vε,R1,∂ Vε,R1,∂2 Vε,R1 ∈C1+γ 2,2+γ(cid:0) [0,T)×RN(cid:1) ,1≤i,j ≤N.
N xi N xixj N loc
Proof. In Lemma 3.5 we have shown that the solution to (3.29) Vε,R1 ∈ C([0,T] × RN) has bounded
N
derivatives Vε,R1,∂ Vε,R1,∂2 Vε,R1,1≤i,j ≤N. In order to show the higher regularity of Vε,R1, let R
N xi N xixj N N 2
besufficientlylargeandtake∂ (1≤i≤N)in(3.9)toobtainthelinearPDEsatisfiedby∂ Vε,R1. Notice
xi xi N
that when R is sufficiently large,
2
∂ g(t,x)=∂ (cid:0) H˜ (x,∇ Vε,R1)(cid:1) ∈Cγ 2,γ(cid:0) [0,T)×RN(cid:1) .
xi xi N x N loc
So we have by the standard results on linear PDE that ∂ Vε,R1 ∈ C1+γ 2,2+γ(cid:0) [0,T)×RN(cid:1) ,1 ≤ i ≤ N. In
xi N loc
view of Lemma 3.8, we may repeat the previous procedure once more, i.e., take ∂2 (1≤i,j ≤N) in (3.9)
xixj
and show that ∂2 Vε,R1 ∈C1+γ 2,2+γ(cid:0) [0,T)×RN(cid:1) ,1≤i,j ≤N.
xixj N loc
Denote by Vε,R1,kl = ∂2 Vε,R1, 1 ≤ k,l ≤ N. By direct calculation, applying ∂2 to the equation
N xkxl N xkxl
(3.29), one obtains

∂ tV Nε,R1,kl ++
(cid:88)
iσ =N22
1i
∂(cid:88) ,jN
p= iH1
˜∂ NRx2
1ix (tj
,V xNε ,, ∇kl x+
V
Nε
ε2
,2 R(cid:88) 1i=N
)∂1
x∂ ix2
Vi Nx εi
,RV 1Nε ,k,k ll ++
i(cid:88)
,∂ jNx2
=k 1x
∂lH p2˜ ipNR j1 H˜(t NR,x 1(, t∇
,xx
,V ∇Nε x,R V1 Nε)
,R1)V Nε,R1,kiV Nε,R1,jl
(3.40)
N N
 V+
=
Nε,(cid:88)
i
0
R=
,
11
,k∂
lx
(2
Tlp
,iH x˜ 1NR ,.1 .(t ., ,x x, N∇
)x
=V Nε
δ
N, kR l1 U)V
R′′N
1ε (,R x1 k, )k ,i+ 1(cid:88)
i= ≤1
k∂ ,x2
lkp
≤iH˜ NNR .1(t,x,∇ xV Nε,R1)V Nε,R1,li
TheequationaboveenablesustoarrivetotheresultsonthesecondorderderivativesvianonlinearFeynman–
Kac representation. In the current subsection we present the estimates on the second order derivatives for
short time.
Proposition 3.10. Suppose Hypothesis (R). There exists a constant c˜=c˜(f,λ− 21,L,U,(λ−λ 0)−1) and
C˜ =C˜ (f,L,U,(λ−λ )−1), C˜ increasing in (λ−λ )−1, such that for T <c˜, PDE (3.40) admits a unique
4 4 0 4 0
bounded solution satisfying for 1≤i,j ≤N,
(cid:12) (cid:12)V Nε,R1,ij(t,x)(cid:12) (cid:12)≤C˜ 4N−1(δ
ij
+N−1), (t,x)∈[0,T]×RN. (3.41)
Proof. For x=(x ,...,x )∈RN, consider
1 N
dXi =∂ H˜R1(cid:0) t,X ,∇ Vε,R1(t,X )(cid:1) dt+σdWi+εdW0,Xi =x , (3.42)
t pi N t x N t t t 0 i
as well as
Ykl =Vε,R1,kl(t,X ), 1≤k,l≤N. (3.43)
t N t
15According to Lemma 3.5, we can deduce the existence of a constant C depending on R such that
1
(cid:12) (cid:12)∇ xV Nε,R1(t,X t)(cid:12) (cid:12)≤C.
The estimate above and the first order condition associated to (3.30) yields
(cid:12) (cid:12)∂ piH˜ NR1(t,X t,∇ xV Nε,R1(t,X t))(cid:12) (cid:12)≤C(1+|X t|). (3.44)
Hence SDE (3.42) admits a weak solution satisying
(cid:20) (cid:21)
E max |X |m ≤C(1+|x|m), m≥1.
t
0≤t≤T
In view of Lemma 3.5,
|Ykl|≤C, 1≤k,l≤N,
t
where the constant C might depend on ε and R .
1
In view of (3.40) and the estimates above, we can infer from the nonlinear Feynman–Kac representation
that the matrix process Y(t) satisfies the backward stochastic Riccati equation
Y =E (cid:26) 1 U˜(T)+(cid:90) T (cid:20) ∇2 H˜R1(cid:0) s,X ,∇ Vε,R1(s,X )(cid:1) +Y ∇2 H˜R1(cid:0) s,X ,∇ Vε,R1(s,X )(cid:1)
t t N xx N s x N s s xp N s x N s
t
(cid:21) (cid:27)
+∇2 H˜R1(cid:0) s,X ,∇ Vε,R1(s,X )(cid:1) Y +Y ∇2 H˜R1(cid:0) s,X ,∇ Vε,R1(s,X )(cid:1) Y ds , (3.45)
xp N s x N s s s pp N s x N s s
where the matrix U˜(T) is given by
U˜ij(T)=δ U′′(Xi), 1≤i,j ≤N. (3.46)
ij T
Next, define the mapping from the set of adapted matrix processes to itself
Φ: L∞(cid:0) Ω;C([0,T];RN×N)(cid:1) −→L∞(cid:0) Ω;C([0,T];RN×N)(cid:1) , Φ(Y)=Y˜,
such that for t∈[0,T],
Y˜ =E (cid:20) 1 U˜(T)+(cid:90) T (cid:2) ∇2 H˜R1(s,X ,∇ Vε,R1(s,X ))+Y ∇2 H˜R1(s,X ,∇ Vε,R1(s,X ))
t t N xx N s s N s s xp N s s N s
t
(cid:21)
+∇2 H˜R1(s,X ,∇ Vε,R1(s,X ))Y +Y ∇2 H˜R1(s,X ,∇ Vε,R1(s,X ))Y (cid:3) ds .
xp N s s N s s s pp N s s N s s
We can see that Y in (3.43) is a fixed point of Φ. Next we show that such fixed point is unique. In fact,
t
let Y∗ and Y∗∗ be two bounded fixed points. And consider their norm of the following form
t t
N
max
(cid:13) (cid:13)Y∗(cid:13)
(cid:13) := max max
(cid:88)(cid:12) (cid:12)Y∗,ij(cid:12)
(cid:12)≤C, max
(cid:13) (cid:13)Y∗∗(cid:13)
(cid:13) ≤C.
0≤t≤T t ∞ 0≤t≤T1≤i≤N t 0≤t≤T t ∞
j=1
Then for t∈[T −δ,T] and C˜ =C˜ depending only on C˜ from (3.39),
3 3
(cid:13) (cid:13)Y∗−Y∗∗(cid:13)
(cid:13)
t t ∞
(cid:20)(cid:90) T (cid:21)
≤E t (cid:0)(cid:13) (cid:13)Y s∗(cid:13) (cid:13) ∞+(cid:13) (cid:13)Y s∗∗(cid:13) (cid:13) ∞(cid:1)(cid:13) (cid:13)∇2 ppH˜ NR1(s,X s,∇ sV Nε(s,X s))(cid:13) (cid:13) ∞(cid:13) (cid:13)Y s∗−Y s∗∗(cid:13) (cid:13) ∞ds
t
(cid:20)(cid:90) T (cid:21)
+2E t (cid:13) (cid:13)∇2 xpH˜ NR1(s,X s,∇ sV Nε(s,X s))(cid:13) (cid:13) ∞(cid:13) (cid:13)Y s∗−Y s∗∗(cid:13) (cid:13) ∞ds
t
16(cid:20)(cid:90) T (cid:21)
≤2(C+1)C˜E t
t
(cid:13) (cid:13)Y s∗−Y s∗∗(cid:13) (cid:13) ∞ds ≤2(C+1)C˜δ T−m δ≤a sx ≤T(cid:13) (cid:13)Y s∗−Y s∗∗(cid:13) (cid:13) ∞ a.s..
Choose2(C+1)C˜δ <1,thentheinequalityaboveimpliesthatY∗ =Y∗∗ ont∈[T−δ,T]. Repeattheabove
t t
procedure, we can show that Y∗ =Y∗∗ on t∈[T −δ,T], [T −2δ,T −δ] and after finite times repetitions we
t t
obtain Y∗ = Y∗∗ on t ∈ [0,T]. The uniqueness above thus tells that Y in (3.43) is the only bounded fixed
t t t
point of Φ.
Tocontinue,definetheclosedsubsetB(N,K)ofadaptedmatrixprocessesinsuchawaythatY ∈B(N,K)
if and only if
max |Yij|≤KN−1(δ +N−1) a.s., (3.47)
t ij
t∈[0,T]
where the constant K >0 is to be determined.
We claim that for appropriate K and c˜ (independent of N), Φ is invariant on B(N,K), and Φ is a
contraction mapping on B(N,K) with T <c˜.
Let Y(1) and Y(2) be two inputs from B(N,K) and Y˜(1) and Y˜(2) be the associated outputs.
t t t t
N
(cid:13) (cid:13)Y˜(1)−Y˜(2)(cid:13) (cid:13) = max (cid:88)(cid:12) (cid:12)Y˜(1),ij −Y˜(2),ij(cid:12) (cid:12)
t t ∞ 1≤i≤N t t
j=1
(cid:20)(cid:90) T (cid:21)
≤E t (cid:0)(cid:13) (cid:13)Y s(1)(cid:13) (cid:13) ∞+(cid:13) (cid:13)Y s(2)(cid:13) (cid:13) ∞(cid:1)(cid:13) (cid:13)∇2 ppH˜ NR1(s,X s,∇ sV Nε(s,X s))(cid:13) (cid:13) ∞(cid:13) (cid:13)Y s(1)−Y s(2)(cid:13) (cid:13) ∞ds
t
(cid:20)(cid:90) T (cid:21)
+2E t (cid:13) (cid:13)∇2 xpH˜ NR1(s,X s,∇ sV Nε(s,X s))(cid:13) (cid:13) ∞(cid:13) (cid:13)Y s(1)−Y s(2)(cid:13) (cid:13) ∞ds
t
(cid:20)(cid:90) T (cid:21)
≤(2K+1)C˜E t
t
(cid:13) (cid:13)Y s(1)−Y s(2)(cid:13) (cid:13) ∞ds ≤(2K+1)C˜T 0m ≤sa ≤x T(cid:13) (cid:13)Y s(1)−Y s(2)(cid:13) (cid:13) ∞ a.s.,
where C˜ is increasing in T because by Lemma 3.8 the constant C˜ is increasing in T. Let’s further fix the
3
parameter T in C˜ to be T =1 and obtain C˜ =C˜(f,λ− 21,L,(λ−λ 0)−1). Hence for T <1,
max (cid:13) (cid:13)Y˜(1)−Y˜(2)(cid:13) (cid:13) ≤(2K+1)C˜T max (cid:13) (cid:13)Y(1)−Y(2)(cid:13) (cid:13) a.s..
0≤t≤T t t ∞ 0≤s≤T s s ∞
We thus have that if we choose K, c˜satisfying
(2K+1)C˜c˜<1, c˜<1,
then Φ is a contraction mapping on B(N,K) with T < c˜. Next we show that B(N,K) is invariant for
appropriate K and c˜. Denote by Y ∈ B(N,K) the input and Y˜ the output, then Lemma 3.8 and direct
t t
calculation yield
|Y˜ tij|≤E t(cid:20) N1 |U˜ij(T)|+(cid:90) T (cid:12) (cid:12) (cid:12) (cid:12)∂ x2 ixjH˜ NR1(s,X s,∇ sV Nε,R1(s,X s))+(cid:88)N Y sik∂ p2 kxjH˜ NR1(s,X s,∇ sV Nε(s,X s))
t k=1
N N (cid:12) (cid:21)
(cid:88) ∂ x2 ipkH˜ NR1(s,X s,∇ sV Nε(s,X s))Y skj + (cid:88) Y sik∂ p2 kplH˜ NR1(s,X s,∇ sV Nε(s,X s))Y slj(cid:12) (cid:12) (cid:12)ds
k=1 k,l=1
N N
≤δ C˜N−1+C˜c˜N−1(δ +N−1)+C˜c˜KN−2(cid:88) (δ +N−1)+C˜c˜KN−2(cid:88) (δ +N−1)
ij ij ik kj
k=1 k=1
N
+C˜c˜K2N−2 (cid:88) (δ +N−1)(δ +N−1)
ik lj
k,l=1
17=δ C˜N−1+C˜c˜N−1(δ +N−1)+4C˜c˜KN−2+4C˜c˜K2N−2.
ij ij
It is easy to see that we can choose K, c˜such that
K =K(cid:0) f,λ− 21,L,(λ−λ 0)−1(cid:1) , c˜=c˜(cid:0) f,λ− 21,L,(λ−λ 0)−1(cid:1) ,
and
KN−1(δ +N−1)>δ C˜N−1+C˜c˜N−1(δ +N−1)+4C˜c˜KN−2+4C˜c˜K2N−2.
ij ij ij
Then we have for such K, c˜that B(N,K) is invariant.
Since Φ is contractive and invariant on (B(N,K),∥·∥ ), which is a Banach space, it follows that Φ
∞
admitsafixedpointinB(N,K)whenT <c˜. NotethatprocessesinB(N,K)areallbounded. Thereforethe
aforementionedfixedpointinB(N,K)isnothingbutthematrixprocessin(3.43)andwemaytakeC˜ =K.
4
Consider t=0 in (3.43), then we have (3.41) from (3.47).
We can see from the proof above that C˜ actually depends on U′,U′′ rather than U.
4
3.2.2 Global in time estimates
InthissubsectionwefocusontheglobalestimatesforanygivenT >0withsufficientlysmoothdata. Aswill
beseen,theglobalestimatesrelyheavilyontheconvexityassumption(withrespecttox)ontheHamiltonian
H˜ (t,x,p) in (3.49). However, the truncation of L, U might break the convexity of H˜ (t,x,p). Therefore,
N N
we need to pass R to infinity in (3.29) and consider
1

∂ tV Nε + σ 22 (cid:88)N ∂ x2 ixjV Nε + ε 22 (cid:88)N ∂ x2 ixiV Nε +H˜ N(t,x,∇ xV Nε)=0,
i,j=1 i=1
(3.48)
V
Nε(T,x 1,...,x N)=
N1 (cid:88)N
U(x i),
i=1
Here
(cid:26) N (cid:18) N (cid:19) N (cid:27)
H˜ N(t,x,p):= θin ∈Rf λ 2(cid:12) (cid:12)θ(cid:12) (cid:12)2 +(cid:88) f t,θ,x i, N1 (cid:88) δ
xj
p i+ N1 (cid:88) L(x i) . (3.49)
i=1 j=1 i=1
Similarly to (3.40), we would like to take ∂2 in (3.48) and analysis the resulting system. To do so, we
xkxl
show the validity of taking derivatives in the next proposition.
Proposition 3.11. Suppose Hypothesis (R). The PDE (3.48) admits a unique classical solution Vε,R1 ∈
N
C(cid:0) [0,T]×RN(cid:1) where Vε,R1,∂ Vε,R1,∂ Vε,R1,∂2 Vε,R1,1 ≤ i,j ≤ N are bounded. For 0 < γ < 1 and
N t N xi N xixj N
1 ≤ i,j ≤ N, Vε,∂ Vε,∂2 Vε ∈ C1+γ 2,2+γ(cid:0) [0,T)×RN(cid:1) . And for φ ∈ {Vε,∂ Vε,∂ Vε,∂2 Vε},
N xi N xixj N loc N t N xi N xixj N
1≤i,j ≤N, φ has polynomial growth in x:
|φ(t,x)|≤C˘(1+|x|)7, (t,x)∈[0,T)×RN. (3.50)
Here the constant C˘ depends only on f, L, U, σ, ε. Moreover, the solution Vε,R1 to (3.29) satisfies
N
lim (Vε,R1,∂ Vε,R1,∂ Vε,R1,∂2 Vε,R1)(t,x)=(Vε,∂ Vε,∂ Vε,∂2 Vε)(t,x),
R1→+∞
N t N xi N xixj N N t N xi N xixj N
where the convergence is locally uniform on [0,T]×RN.
As a result, for the first order derivatives of Vε,R1, we also have
N
(cid:12) (cid:12)∂ xiV Nε(t,x)(cid:12) (cid:12)≤ C˜ 2(C 1L 1 N+C 1U 1)(cid:18) 1+|x i|2+ N1 (cid:88)N |x j|2(cid:19) 21 + C˜ 2(C 1L 0 N+C 1U 0) , (3.51)
j=1
where the constant C˜ is from Lemma 3.4.
2
18Proof. Let Vε,R1 be the solution to (3.29) in Lemma 3.5. According to Theorem 4.7.2 and Theorem 4.7.4 in
N
Krylov(1980)aswellasthegrowthcondition(3.3),wehavethatforφ∈{Vε,R1,∂ Vε,R1,∂ Vε,R1,∂2 Vε,R1},
N t N xi N xixj N
1≤i,j ≤N, φ has polynomial growth in x:
|φ(t,x)|≤C˘(1+|x|)7, (t,x)∈[0,T)×RN,
where the constant C˘ depends only on f, L, U, σ, ε and is independent of R 1
1
Similar to (3.9), we may view the solution of (3.29) as the solution of the constant coefficients PDE
σ2 (cid:88)N ε2 (cid:88)N
∂ Vε,R1(t,x)+ ∂2 Vε,R1(t,x)+ ∂2 Vε,R1(t,x)=gR1(t,x), (3.52)
t N 2 xixj N 2 xixi N
i,j=1 i=1
where
gR1(t,x):=−H˜R1(t,x,∇ Vε,R1).
N x N
InviewofCorollary4.7.8inKrylov(1980)aswellasLemma3.5andLemma3.8,gR1(t,x)islocallyLipschitz
continuous with respect to x with Lipschitz constant independent of R
1
while gR1(t,x) is locally γ 2−H¨older
continuous (0 < γ < 1) with respect to t with H¨older constant independent of R . It then follows that
1
∂ Vε,R1(t,x) and ∂2 Vε,R1(t,x), 1≤i,j ≤N are locally H¨older continuous in (t,x) with H¨older constant
t N xixj N
independent of R . According to Arzel`a–Ascoli Theorem, we may pass R to infinity in (3.29) and obtain
1 1
the limit of Vε,R1 as the solution Vε ∈C1+γ 2,2+γ(cid:0) [0,T)×RN(cid:1) ∩C(cid:0) [0,T]×RN(cid:1) of (3.48). We remark that
N N loc
becauseoftheuniquenessofsolutionstothislastproblem, thereisnoneedtoconsidersub-sequentiallimits
in the Arzel`a–Ascoli theorem. Moreover, we have (3.51) and (3.50) for φ ∈ {Vε,∂ Vε,∂ Vε,∂2 Vε},
N t N xi N xixj N
1≤i,j ≤N.
In order to show higher regularity of Vε, we may take ∂ , (1 ≤ i ≤ N) in (3.48) and obtain the PDE
N xi
satisfies by ∂ Vε. Notice that ∂ (cid:0) H˜ (x,∇ Vε)(cid:1) ∈ Cγ 2,γ(cid:0) [0,T)×RN(cid:1) , then it follows that ∂ Vε ∈
xi N xi N x N loc xi N
C1+γ 2,2+γ(cid:0) [0,T)×RN(cid:1) ∩C(cid:0) [0,T]×RN(cid:1)
. Thanks to Lemma 3.8 we may let R go to infinity in (3.39) to
loc 1
obtain that ∂2 (cid:0) H˜ (x,∇ Vε)(cid:1) is bounded and ∂2 (cid:0) H˜ (x,∇ Vε)(cid:1) ∈C1+γ 2,2+γ(cid:0) [0,T)×RN(cid:1) . Thus we
xixj N x N xixj N x N loc
can further take ∂2 , (1 ≤ i,j ≤ N) in (3.48) and repeat the previous procedure once more to show that
xixj
∂2 Vε ∈C1+γ 2,2+γ(cid:0) [0,T)×RN(cid:1) ∩C(cid:0) [0,T]×RN(cid:1) .
xixj N loc
Now we make the following assumption on convexity of the data and set out for the global in time
estimates.
Hypothesis (R1) Suppose Hypothesis (R) and the following:
1. U is convex;
2. H˜ in (3.49) is convex in x∈RN, for all (t,p).
N
Remark 3.12. The second assumption in Hypothesis (R1) on the convexity of the Hamiltonian is quite
common in mean field control problems. In our model, since the control is centralized and the dynamics of
the particles is more complicated, this assumption could no longer be guaranteed in a simple way. According
to direct calculation, with the notation µ= 1 (cid:80)N δ , we find
N j=1 xj
1 1
∂2 H˜ (t,x,p)=δ L′′(x )+∂2 f(t,θ∗,x ,µ)p ∂ θ∗+δ ∂2 f(t,θ∗,x ,µ)p + ∂2 f(t,θ∗,x ,µ,x )p
xkxl N klN k θx k k xl kl xx k k N xµ k l k
N
1 1 (cid:88)
+ ∂2 f(t,θ∗,x ,µ,x )p + ∂2 f(t,θ∗,x ,µ,x )p ∂ θ∗
N xµ l k l N µθ i k i xl
i=1
1According to Theorem 4.7.2 and Theorem 4.7.4 in Krylov (1980) for L, U with growth rate (1+|x|m), the estimates on
thesecondorderderivativesareofgrowthrate(1+|x|3m+1). Hereinourcase,sinceLandU growatmostquadratically,we
havem=2.
19N N
1 (cid:88) 1 (cid:88)
+δ ∂2 f(t,θ∗,x ,µ,x )p + ∂2 f(t,θ∗,x ,µ,x ,x )p .
klN µx˜ i k i N2 µµ i k l i
i=1 i=1
We can see from the above that one possible way to ensure the convexity of H˜ (t,x,p) in x is to assume an
N
affine structure on f.
Indeed, set the parameters in (1.1) and (2.1) as follows
(cid:90)
f(t,θ,x,µ)=θ+x+ yµ(dy), L(x)=U(x)=x2, σ =λ=1.
R
Then Hypothesis (R1) can be easily verified.
More generally, if we suppose that f(t,θ,x,µ) is jointly convex in (x,µ) in the sense that
(cid:0) (cid:1) (cid:0) (cid:1) (cid:0) (cid:1)
f t,θ,sx +(1−s)x ,Law(sξ +(1−s)ξ ) ≤sf t,θ,x ,Law(ξ ) +(1−s)f t,θ,x ,Law(ξ ) ,
1 2 1 2 1 1 2 2
for all s∈[0,1] and ξ ,ξ , random variables, we can argue as follows. In this case,
1 2
(cid:18) N (cid:19)
1 (cid:88)
fi(t,θ,x):=f t,θ,x , δ ,
i N xj
j=1
is convex in x and for y ∈RN there exists fˆi(t,θ,y) convex in y such that
(cid:26) (cid:27)
fˆi(t,θ,y)= sup −y·x−fi(t,θ,x) ,
x∈RN
as well as
(cid:26) (cid:27)
fi(t,θ,x)= sup −y·x−fˆi(t,θ,y) .
y∈RN
Hence (3.30) yields
(cid:26) N N N (cid:27)
H˜ N(t,x,p)= θin ∈Rf y1,...s ,yu Np
∈RN
λ(cid:12) (cid:12)θ(cid:12) (cid:12)2 −(cid:88) i=1(x·y)p i−(cid:88) i=1fˆi(t,θ,y)p i+ N1 (cid:88) i=1L(x i) .
Suppose that p ≥ 0 and for all (t,p,x) the optimal θ∗ is attained in a compact set, then according to the
i
minimax theorem (see e.g. Sion (1958)),
(cid:26) N N N (cid:27)
H˜ N(t,x,p)= y1,...s ,yu Np ∈RNθin ∈Rf λ(cid:12) (cid:12)θ(cid:12) (cid:12)2 −(cid:88) i=1(x·y)p i−(cid:88) i=1fˆi(t,θ,y)p i+ N1 (cid:88) i=1L(x i) , (3.53)
and thus H (t,x,p) is convex. However, the constraint p ≥ 0 requires that the value function Vε is
N i N
increasing in every component. Roughly speaking, one way to achieve this is to assume ∂ f,L′,U′ ≥0 then
µ
apply the theory on monotone dynamical systems (see e.g. Smith (1995)).
Similar to the last subsection, the key estimate in this subsection is from the BSDE of Riccati type (3.59)
below. The following lemma is devoted to estimating the terms appearing in (3.59).
Lemma 3.13. Suppose Hypothesis (R1), then for H˜ in (3.49) and (x,p)∈A , t∈[0,T],
N N
(cid:12) (cid:12)∂ xiH˜ N(t,x,p)(cid:12) (cid:12),(cid:12) (cid:12)∂ x2 ipjH˜ N(t,x,p)(cid:12) (cid:12)≤C˜ 3N−1, (cid:12) (cid:12)∂ x2 ixjH˜ N(t,x,p)(cid:12) (cid:12)≤C˜ 3N−1(δ
ij
+N−1),
(cid:12) (cid:12)∂ piH˜ N(t,x,p)(cid:12) (cid:12),(cid:12) (cid:12)∂ p2 ipjH˜ N(t,x,p)(cid:12) (cid:12)≤C˜ 3, 1≤i,j ≤N. (3.54)
As a result, there exists a constant C˜
5
=C˜ 5(f,λ− 21,T,L,U,(λ−λ 0)−1) such that
C˜
0≤∇2H˜ (t,x,p)≤ 5I , (x,p)∈A , t∈[0,T].
x N N N N
20Proof. In view of Lemma 3.8, the constant C˜ in (3.39) is independent of R . Therefore we can let R go to
3 1 1
infinity and obtain (3.54) according to definitions in (3.30) and (3.49). Furthermore, in view of the second
inequality in (3.54), we can deduce the existence of C˜ such that for any ξ ∈RN, (x,p)∈A , t∈[0,T],
5 N
(cid:88)N
∂2 H˜ (t,x,p)ξ ξ ≤
1 (cid:88)N
∂2 H˜ (t,x,p)(ξ2+ξ2)≤
C˜
5|ξ|2.
xixj N i j 2 xixj N i j N
i,j=1 i,j=1
Combining the above with Hypothesis (R1) we have the last inequality.
With the preparation above, if we further assume that ∂2 Vε(t,x) (1 ≤ i,j ≤ N) are bounded, we
xixj N
wouldthenobtainarefinedestimateontheboundof∂2 Vε(t,x)(1≤i,j ≤N)in(3.55). Thisisobtained
xixj N
via the BSDE of Riccati type in (3.59) below where the convexity assumption plays a key role.
Lemma 3.14. Suppose that there exist positive constants δ and C˘ (which could depend on N and ε) such
that for (t,x)∈[T −δ,T]×RN, it holds that ∂2 Vε(t,x) (1≤i,j ≤N) are bounded by constant C˘. Then
xixj N
there exists a constant C˜
6
=C˜ 6(f,λ− 21,T,L,U,(λ−λ 0)−1) (independent of C˘ and δ) such that for ξ ∈RN
and (t,x)∈[T −δ,T]×RN,
0≤
(cid:88)N
Vε,ij(t,x)ξ ξ ≤
C˜
6|ξ|2, (3.55)
N i j N
i,j=1
In particular, ∂2 Vε(t,x) (1≤i,j ≤N) are bounded by C˜ 6 for (t,x)∈[T −δ,T]×RN.
xixj N N
Proof. Without the loss of generality, we show (3.55) when t=T −δ. For x=(x ,...,x )∈RN, consider
1 N
dXi =∂ H(t,X ,∇ Vε(t,X ))dt+σdWi+εdW0,Xi =x , (3.56)
t pi t x N t t t 0 i
as well as
Ykl =Vε,kl(t,X ), (t,x)∈[T −δ,T]×RN. (3.57)
t N t
According to (3.51) and Hypothesis (R1), it is easy to show that
(cid:12) (cid:12)∂ piH(t,X t,∇ xV Nε(t,X t))(cid:12) (cid:12)≤C(1+|X t|). (3.58)
for some constant C. Hence (3.56) admits a weak solution satisfying
(cid:20) (cid:21)
E max |X |κ ≤C(1+|x|κ), ∀κ≥1.
t
0≤t≤T
Moreover, since t∈[T −δ,T], we have by assumption that
|Ykl|≤C˘, 1≤k,l≤N.
t
Given the estimates above and Proposition 3.11, we may differentiate (3.48) with respect to x , x
i j
(1≤i,j ≤N) and obtain an analog of (3.40), then we can deduce from the assumption on the boundedness
of the matrix process Y(t) that it satisfies the Riccati type equation
Y =E (cid:20) 1 U˜(T)+(cid:90) T (cid:2) ∇2 H˜ (cid:0) X ,∇ Vε(s,X )(cid:1) +Y ∇2 H˜ (cid:0) s,X ,∇ Vε(s,X )(cid:1)
t t N xx N s x N s s xp N s x N s
t
(cid:21)
+∇2 H˜ (cid:0) s,X ,∇ Vε(s,X )(cid:1) Y +Y ∇2 H˜ (cid:0) X ,∇ Vε(s,X )(cid:1) Y (cid:3) ds . (3.59)
px N s x N s s s pp N s x N s s
Herewerecall that the firstterm 1U˜(T)ontherighthandsideisdefinedsimilarlytothatin(3.46). Define
N
Φ satisfying
s
21Φ =I −(cid:90) t Φ (cid:20) 1 Y ∇2 H˜ (s,X ,∇ Vε(s,X ))+∇2 H˜ (cid:0) s,X ,∇ Vε(s,X )(cid:1)(cid:21) ds, t∈[T −δ,T].
t N s 2 s pp N s x N s px N s x N s
T−δ
(3.60)
Note here that Φ , t ∈ [T −δ,T] is bounded because Y , ∇2 H˜ and ∇2 H˜ in the right hand side above
t t pp N px N
are bounded. According to the above and (3.59), we may write the dynamics of Y , Φ as
t t
(cid:20)
dY =− ∇2 H˜ (cid:0) X ,∇ Vε(t,X )(cid:1) +Y ∇2 H˜ (cid:0) t,X ,∇ Vε(t,X )(cid:1)
t xx N t x N t t xp N t x N t
(cid:21) N
+∇2 H˜ (cid:0) t,X ,∇ Vε(t,X )(cid:1) Y +Y ∇2 H˜ (cid:0) X ,∇ Vε(t,X )(cid:1) Y dt+(cid:88) ZidWi(t),
px N t x N t t t pp N t x N t t t
i=0
(cid:20) (cid:21)
dΦ =−Φ 1 Y ∇2 H˜ (t,X ,∇ Vε(t,X ))+∇2 H˜ (cid:0) t,X ,∇ Vε(t,X )(cid:1) dt,
t t 2 t pp N t x N t px N t x N t
where (cid:82)t ZidWi(s) (0≤i≤N) are BMO martingales. Then Itˆo’s formula gives
T−δ s
d(Φ Y Φ⊤)=(dΦ )Y Φ⊤+Φ (dY )Φ⊤+Φ Y (dΦ⊤)
t t t t t t t t t t t t
N
=−Φ ∇2 H˜ (cid:0) t,X ,∇ Vε(t,X )(cid:1) Φ⊤dt+(cid:88) Φ ZiΦ⊤dWi(t).
t xx N t x N t t t t t
i=0
Since Φ is bounded, we may present the above as
t
Φ Y Φ⊤ =E (cid:20) 1 Φ U˜(T)Φ⊤+(cid:90) T Φ ∇2 H˜ (cid:0) s,X ,∇ Vε(s,X )(cid:1) Φ⊤ds(cid:21) . (3.61)
t t t t N T T s xx N s x N s s
t
According to Hypothesis (R1), we have
U˜(T)≥0, ∇2 H˜ (cid:0) s,X ,∇ Vε(s,X )(cid:1) ≥0.
xx N s x N s
Here the ordering relation ≥ is used in the sense of positive semi-definite matrices. Hence
1 Φ U˜(T)Φ⊤ ≥0, Φ ∇2 H˜ (cid:0) s,X ,∇ Vε(s,X )(cid:1) Φ⊤ ≥0, s∈[t,T].
N T T s xx N s x N s s
Moreover,
E (cid:20) 1 Φ U˜(T)Φ⊤+(cid:90) T Φ ∇2 H˜ (cid:0) s,X ,∇ Vε(s,X )(cid:1) Φ⊤ds(cid:21) ≥0. (3.62)
t N T T s xx N s x N s s
t
In other words, the right hand side of (3.61) is a (random) positive semi-definite matrix. Now we may take
t = T −δ in (3.61) and combine (3.60), (3.62) to obtain Y ≥ 0. In view of (3.57), we have ∇2 Vε ≥ 0
T−δ xx N
and hence
N
(cid:88)
Vε,ij(T −δ,x)ξ ξ ≥0.
N i j
i,j=1
One the other hand, according to Hypothesis (R1) and (3.49), we have
∇2 H˜ (cid:0) s,X ,∇ Vε(s,X )(cid:1) , −∇2 H˜ (cid:0) s,X ,∇ Vε(s,X )(cid:1) ≥0.
xx N s x N s pp N s x N s
Hence for any α∈RN satisfying |α|=1,
0≤α⊤Y α≤E (cid:20) 1 α⊤U˜(T)α+(cid:90) T (cid:104) α⊤Y ∇2 H˜ (cid:0) s,X ,∇ Vε(s,X )(cid:1) α
t t N s xp N s x N s
t
22+α⊤∇2 H˜ (cid:0) s,X ,∇ Vε(s,X )(cid:1) Y α+α⊤∇2 H˜ (cid:0) s,X ,∇ Vε(s,X )(cid:1) α(cid:105) ds(cid:21) .
px N s x N s s xx N s x N s
Moreover, in view of Lemma 3.13 and Y ≥0, we have
s
α⊤Y s∇2 xpH˜ N(cid:0) s,X s,∇ xV Nε(s,X s)(cid:1) α≤(cid:12) (cid:12)Y sα(cid:12) (cid:12)·(cid:12) (cid:12)∇2 xpH˜ N(cid:0) s,X s,∇ xV Nε(s,X s)(cid:1) α(cid:12) (cid:12)
≤C˜ 3(cid:12) (cid:12)Y sα(cid:12) (cid:12)·(cid:12) (cid:12)α(cid:12) (cid:12)≤C˜
3
sup β⊤Y sβ,
|β|=1
as well as
α⊤∇2 H˜ (cid:0) s,X ,∇ Vε(s,X )(cid:1) α≤ C˜ 5|α|2 = C˜ 5, 1 α⊤U˜(T)α≤ C 2U 0,
xx N s x N s N N N N
where we recall that CU is from (2.2). Hence
20
CU C˜ T (cid:20)(cid:90) T (cid:18) (cid:19) (cid:21)
sup β⊤Y β ≤ 20 + 5 +2C˜ E sup β⊤Y β ds .
t N N 3 t s
|β|=1 t |β|=1
Therefore we may deduce the existence of C˜
6
=C˜ 6(f,λ−1 2,T,L,U,(λ−λ 0)−1) such that
(cid:20) (cid:21) C˜
E sup β⊤Y β ≤ 6, t∈[T −δ,T].
t N
|β|=1
The inequality above implies that
(cid:88)N
Vε,ij(t,x)ξ ξ ≤
C˜
6|ξ|2, ξ ∈RN, (t,x)∈[T −δ,T]×RN,
N i j N
i,j=1
and we have completed the proof.
Remark 3.15. 1. To see why we confine ourselves to the case where ∂2 Vε(t,x) (1 ≤ i,j ≤ N) are
xixj N
bounded, one might turn to the definition of the matrix valued process Φ . If we do not assume that Y
t t
is bounded, then we cannot ensure the integrability of Φ . Without the integrability of Φ , we can not
t t
do the calculations in (3.59) and below, since they all involve taking conditional expectation.
2. For now, in this Lemma 3.14, the the existence of the constants δ and C˘ is merely an assumption. But
we know from Proposition 3.10 and Proposition 3.11 that δ indeed exists and is at least c˜, so does C˘.
In the next Proposition 3.16, we will use the refined estimate (3.55) to show that δ = T. Moreover,
showing that δ =T will then in turn gives us the refined estimate (3.55) on [0,T].
We finish this section with the next proposition where the extra assumption on boundedness in Lemma
3.14isremoved. Themainideaistotakeadvantageoftherefinedestimatein(3.55)whileutilizingasuitable
‘continuity’ method.
Proposition 3.16. Suppose Hypothesis (R1) and λ is sufficiently large. There exists a constant
C˜
6
=C˜ 6(f,λ−1 2,T,L,U,(λ−λ 0)−1) such that for 1≤i,j ≤N,
0≤
(cid:88)N
Vε,ij(t,x)ξ ξ ≤
C˜
6|ξ|2, ξ ∈RN, (t,x)∈[0,T]×RN.
N i j N
i,j=1
Proof. We have from Proposition 3.10 that, for 0 ≤ T −t ≤ c˜and x ∈ RN, ∂2 Vε,R1(t,x), 1 ≤ i,j ≤ N
xixj N
are uniformly bounded by C˜ + 1 independent of R . In view of the convergence of ∂2 Vε,R1(t,x) to
4 1 xixj N
∂2 Vε(t,x), as R →+∞ in Proposition 3.11, we obtain that ∂2 Vε(t,x), 1≤i,j ≤N are bounded on
xixj N 1 xixj N
(t,x)∈[T−c˜,T]×RN. Thereforewehaveboth(3.51)and(3.55)on(t,x)∈[T−c˜,T]×RN fromProposition
3.11 and Lemma 3.14.
23(cid:18) (cid:19)
Next we replace 1 (cid:80)N U (x ) with ρ |x| Vε(T −c˜,x) in (3.29), (3.40) (we impose some specific
N i=1 R1 i 2R1 N
properties on ρ below) and consider the following coupled PDE system on a time interval (T −c˜−c,T −c˜),
where c>0 is a small number which will be specified later (written for Vˆε,R1)
N

∂ tVˆ Nε,R1 + σ 22 (cid:88)N ∂ x2 ixjVˆ Nε,R1 + ε 22 (cid:88)N ∂ x2 ixiVˆ Nε,R1 +H˜ NR1(t,x,∇ xVˆ Nε,R1)=0,
i,j=1 i=1 (3.63)
Vˆ
Nε,R1(T −c˜,x 1,...,x
N)=ρ(cid:18) 2| Rx| (cid:19)
V Nε(T −c˜,x),
1
as well as
 ∂ +tVˆ
(cid:88)
iN
=Nε, 1R ∂1, pk il H˜+ NRσ
12
(2
xi
,(cid:88)
,j
∇N
= x1
Vˆ∂ Nx2
εi ,Rxj
1V )ˆ ∂Nε x,k iVl ˆ+ Nε,Rε
2
12 ,k(cid:88)
i
l=N +1∂
i(cid:88)
,x2
jNi =x
1iV ∂ˆ
N
p2ε i, pk jl H+
˜
NR∂ 1x2
(k xx ,l
∇H˜ xNR Vˆ1 N( εx ,R, 1∇
)Vx
ˆ
NVˆ
εN
,ε R,R 1,1 k)
iVˆ Nε,R1,jl
(3.64)
N N
Vˆ+
=
Nε,(cid:88)
i
0
R=
,
11
,k∂
lx
(2
Tlpi
−H˜
N
c˜R ,1 x( )x, =∇
∂x
x2Vˆ
kN
xε, lR (cid:20)1 ρ) (cid:18)Vˆ
N
2ε
|
Rx,R |1 (cid:19),ki V+ Nε((cid:88)
i T=1
−∂
x
c2
˜k ,p xi
)H (cid:21)˜ ,NR1( 1x, ≤∇
kx
,Vˆ lNε ≤,R N1) .Vˆ Nε,R1,li
1
Here ρ is any twice continuously differentiable function on [0,+∞) such that ρ(x) = 1 if x ∈ [0,1],
ρ(x)=0 on [η−1,+∞), as well as |ρ′′(x)|+|ρ′(x)|≤e−ηx2 for some 0<η <1. As a result,
(cid:18) (cid:19) (cid:18) (cid:19) (cid:20) (cid:18) (cid:19) (cid:21)
|x| |x| |x|
ρ Vε(T −c˜,x), ∂ ρ Vε(T −c˜,x), ∂2 ρ Vε(T −c˜,x) , 1≤k,l≤N,
2R N xk 2R N xkxl 2R N
1 1 1
(cid:18) (cid:19)
areallbounded. Moreover, theterminalconditionρ |x| Vε(T−c˜,x)admitstheuniformgrowthestimate
2R1 N
(3.16) which is the counterpart to (2.2). We can also establish the first order estimate analogous to Lemma
3.4, which is uniform in (R ,N) and possible with different coefficients. Note also that C˜ in Lemma 3.4 is
1 2
decreasinginλ,thenforsufficientlylargeλ(independentof(R ,N)),wemayshowthat∂ H˜R1(x,∇ Vˆε,R1),
1 xi N x N
∂2 H˜R1(x,∇ Vˆε,R1), ∂2 H˜R1(x,∇ Vˆε,R1), ∂2 H˜R1(x,∇ Vˆε,R1) are well-defined and admit the same
xixj N x N pi N x N pipj N x N
uniform estimates as Lemma 3.8 with possibly different coefficients.
Next, we may use a contraction method similar to the one in the proof of Proposition 3.10 to show the
existence of c > 0, depending on C˜ (CL +CU) in (3.51), C˜ in (3.55) as well as N, such that the solution
2 10 10 6
Vˆε,R1,kl to (3.64) is unique and bounded on (t,x) ∈ [T −c˜−c,T −c˜]×RN uniformly in R . We may also
N 1
argue similarly to Proposition 3.11 to obtain that
lim ∂2 Vε,R1,kl(t,x)=∂2 Vε,kl(t,x),
R1→+∞
xkxl N xkxl N
where (t,x) ∈ [T −c˜−c,T −c˜]×RN, 1 ≤ k,l ≤ N. In particular, we have shown that ∂2 Vε(t,x),
xixj N
1 ≤ i,j ≤ N are bounded on t ∈ [T −c˜−c,T]. Then Proposition 3.11 and Lemma 3.14 again yield both
(3.51) and (3.55) on (t,x)∈[T −c˜−c,T −c˜]×RN.
(cid:18) (cid:19)
It is important to notice that ρ |x| Vε(T −c˜,x), as the terminal condition of (3.64), is only used to
2R1 N
show the boundedness of ∂2 Vε(t,x) but not (3.55). We are relying the convexity of the final datum only
xixj N
after passing to the imit R →+∞.
1
Now we can replace U (x) with Vε(T −c˜−c,x) in (3.29), (3.40) and repeat the procedure above to
R1 N
prove (3.51) and (3.55) on (t,x) ∈ [T −c˜−2c,T −c˜−c]×RN. After finite such repetition we can show
(3.55) on (t,x)∈[0,T]×RN.
243.3 Convergence of auxiliary problems
In this section, we study the original problem associated to the HJB equation (3.1). Thanks to the uniform
estimatesintheprevioussections,wemayobtainthedesiredsolutionbyextractingconvergencesubsequence
from the families (Vε,R1) and (Vε) which solve (3.29) and (3.48), respectively. More importantly, the
N ε,R1 N ε
resulting limits inherit the estimates (uniform in N) satisfied by Vε,R1 and Vε.
N N
For short time, we have the following result on the well-posedness of (3.1) as well as the corresponding
estimates.
Theorem3.17. SupposeHypothesis(R).Letc˜>0giveninProposition3.10. ForT <c˜,theoriginalHJB
equation (3.1) admits a solution V ∈W1,2,∞([0,T]×RN), satisfying for 1≤i,j ≤N, (t,x)∈[0,T]×RN,
N loc
|∂ V (t,x)|≤
C˜ 2(C 1L 1+C 1U 1)(cid:18)
1+|x |2+
1 (cid:88)N
|x
|2(cid:19)1 2
+
C˜ 2(C 1L 0+C 1U 0)
, (3.65)
xi N N i N k N
k=1
and
(cid:12) (cid:12)
(cid:12)∂2 V (t,x)(cid:12)≤C˜ N−1(δ +N−1) a.e. (3.66)
(cid:12) xixj N (cid:12) 4 ij
Moreover, such solution V is characterized by the value function in (2.4) and thus it is unique. The unique
N
optimal feedback function is
θ∗ (cid:0) t,x(cid:1) := lim θR1(cid:0) t,x,∇ V (t,x)(cid:1)
N N x N
R1→+∞
(cid:26) N (cid:18) N (cid:19) (cid:27)
λ(cid:12) (cid:12)2 (cid:88) 1 (cid:88)
∈argm θ∈i Θn 2(cid:12)θ(cid:12) + f t,θ,x i,
N
δ xj) ∂ xiV N(t,x) , (3.67)
i=1 j=1
where θR1(t,p,q) is defined in (3.37).
N
Proof. Rewrite (3.29) as follow
ε2 (cid:88)N σ2 (cid:88)N
− ∂2 Vε,R1 =∂ Vε,R1 + ∂2 Vε,R
2 xixi N t N 2 xixj N
i=1 i,j=1
(cid:26) N (cid:18) N (cid:19) (cid:27) N
+ θi ∈n Θf λ 2(cid:12) (cid:12)θ(cid:12) (cid:12)2 +(cid:88) f t,θ,x i, N1 (cid:88) δ
xj
∂ xiV Nε,R1 + N1 (cid:88) L R1(x i).
i=1 j=1 i=1
In view of the uniform estimates in Lemma 3.5 and Proposition 3.10 let
ε→0+, R →+∞,
1
we immediately have the existence of V ∈ W1,2,∞([0,T) × RN) such that on any compact subset of
N loc
[0,T)×RN,Vε,R1 and∇ Vε,R1 converge(uptoasubsequence)uniformlytoV andDV whereas∂ Vε,R1,
N x N N N t N
∇2Vε,R1 converges weakly to ∂ V , ∇2V . Moreover, V also satisfies the corresponding local estimates
x N t N x N N
(3.51), (3.41) of Vε,R1, hence (3.65) and (3.66) is valid.
N
According to (3.41),
(cid:12) (cid:12)
(cid:12) (cid:12)ε2 (cid:88)N
∂2
Vε,R1(cid:12)
(cid:12)≤
ε2
C˜ .
(cid:12) 2 xixi N (cid:12) 2 4
(cid:12) (cid:12)
i=1
Sending ε to 0+, R to +∞ in (3.29), we get
1
∂ tV
N
+ σ 22 (cid:88)N ∂ x2 ixjV
N
+ θi ∈n Θf (cid:26) λ 2(cid:12) (cid:12)θ(cid:12) (cid:12)2 +(cid:88)N f(cid:18) t,θ,x i, N1 (cid:88)N δ xj(cid:19) ∂ xiV N(cid:27) + N1 (cid:88)N L(x i)=0, (3.68)
i,j=1 i=1 j=1 i=1
25in the distributional sense.
Toshowtheuniqueness,itsufficestoestablishtheverificationresultthatanysolutionV ∈W1,2,∞([0,T]×
N loc
RN) satisfying (3.65) and (3.66) equals the value function in (2.4). Consider any θ ∈ Uad, as well as the
corresponding Xθ,i(t) in (1.1) and Xε,θ,i(t) in (3.4). The generalized Itˆo’s formula (see e.g. Krylov (1980))
gives that, for any bounded domain D ⊂RN, denoting by τ the corresponding exit time of Xε,θ(t),
D N
V (T ∧τ ,Xε,θ(T ∧τ ))
N D N D
(cid:90) T∧τD (cid:88)N (cid:90) T∧τD
=V (0,Xε,θ(0))+ LεV (t)dt+σ ∂ V (t,Xε,θ(t))dW0(t)
N N t N xi N N
0 i=1 0
(cid:88)N (cid:90) T∧τD
+ε ∂ V (t,Xε,θ(t))dWi(t)
xi N N
i=1 0
For the ease of notation, we have adopted the notation
σ2 (cid:88)N ε2 (cid:88)N
LεV (t)=∂ V (t,Xε,θ(t))+ ∂2 V (t,Xε,θ(t))+ ∂2 V (t,Xε,θ(t))
t N t N N 2 xixj N N 2 xixi N N
i,j=1 i=1
N (cid:18) N (cid:19)
(cid:88) 1 (cid:88)
+ f t,θ(t),Xε,θ,i(t), ρ(Xε,θ,j(t)) ∂ V (t,Xε,θ(t)).
N xi N N
i=1 j=1
According to Proposition 3.10 as well as the convergence of ∇ Vε,R1(t,x) to ∇ V (t,x), ∇ V (t,x) is
x N x N x N
continuous and uniformly bounded on D. Hence
(cid:104) (cid:105) (cid:20)(cid:90) T∧τD (cid:21)
E V (T ∧τ ,Xε,θ(T ∧τ )) =V (0,Xε,θ(0))+E LεV (t)dt . (3.69)
N D N D N N t N
0
According to Theorem 2.10.2 in Krylov (1980) and (3.66), (3.68),
(cid:20)(cid:90) T∧τD (cid:21) ε2 (cid:34) (cid:90) T∧τD(cid:88)N (cid:35)
E LεV (t)dt ≤ E ∂2 V (t,Xε,θ(t))dt
t N 2 xixi N N
0 0 i=1
−E(cid:20) λ(cid:90) T∧τD(cid:12) (cid:12)θ(t)(cid:12) (cid:12)2
dt+
1 (cid:88)N (cid:90) T∧τD L(Xε,θ,i(t))dt(cid:21)
2 N
0 i=1 0
≤ ε 22 C˜ 4−E(cid:20) λ
2
(cid:90) T∧τD(cid:12) (cid:12)θ(t)(cid:12) (cid:12)2 dt+ N1 (cid:88)N (cid:90) T∧τD L(Xε,θ,i(t))dt(cid:21) . (3.70)
0 i=1 0
Plug (3.70) into (3.69), and let D extend to RN, the monotone convergence theorem yields that
E(cid:34) N1 (cid:88)N U(Xε,θ,i(T))+ N1 (cid:88)N (cid:90) T L(Xε,θ,i(t))dt+ λ
2
(cid:90) T (cid:12) (cid:12)θ(t)(cid:12) (cid:12)2 dt(cid:35) ≤V N(0,Xε N,θ(0))+ ε 22 C˜ 4. (3.71)
i=1 i=1 0 0
Sending ε to 0+ and noticing the convergence of Xε,θ,i to Xθ,i, we have
E(cid:34) N1 (cid:88)N U(Xθ,i(T))+ N1 (cid:88)N (cid:90) T L(Xθ,i(t))dt+ λ
2
(cid:90) T (cid:12) (cid:12)θ(t)(cid:12) (cid:12)2 dt(cid:35) ≤V N(0,Xθ N(0)). (3.72)
i=1 i=1 0 0
On the other hand, consider the candidate optimal feedback control θ∗ (cid:0) t,x,∇ V (t,x)(cid:1) . We first claim
N x N
that the corresponding system
(cid:18) N (cid:19)
1 (cid:88)
dX∗,i(t)=f t,θ∗ (t,X∗),X∗,i(t), δ dt+σdW0(t), i=1,...,N. (3.73)
N N N N N X N∗,j(t)
j=1
26admits a unique solution for any initial data x ,...,x , N ≥ 1. In fact, it is easy to see from (3.67) and
1 N
Lemma 3.7 that θ∗ (t,p,q) is locally Lipschitz continuous with respect to (p,q) ∈ A . In the same time,
N N
(x,∇ V (t,x))∈A and V ∈W1,2,∞([0,T]×RN). Therefore, after composition,
x N N N loc
(cid:18) N (cid:19)
1 (cid:88)
x(cid:55)→f t,θ∗ (t,x,∇ V (t,x)),x , δ , i=1,...,N,
N x N i N xj
j=1
islocallyLipschitzcontinuous. ThelocalLipschitzcontinuitythengivesthestronguniquenessofthesolution.
Notice that we have got (3.65), the weak existence can be deduced from (3.67) and from the linear growth
property that
(cid:12) (cid:18) N (cid:19)(cid:12)
(cid:12) (cid:12) (cid:12)f t,θ N∗ (t,x,∇ xV N(t,x)),x 1, N1 (cid:88) ρ(x j) (cid:12) (cid:12) (cid:12)≤C N(1+|x|),
j=1
for some constant C .
N
Having shown the well-posedness of (3.73), the first “≤” in (3.70) becomes “=”. The estimates in (3.66)
then enable us to replace the “≤” in (3.72) with “≥”, implying that θ∗ is optimal and V is the value
N N
function.
Using the same method as in Theorem 3.17 and combining with the uniform estimates in Proposition
3.11, Proposition 3.16, we can prove the following result for long time.
Theorem 3.18. Suppose Hypothesis (R1) and λ is sufficiently large. The original HJB equation (3.1)
admits a solution V ∈W1,2,∞([0,T]×RN), satisfying for 1≤i,j ≤N, (t,x)∈[0,T]×RN,
N loc
|∂ V (t,x)|≤
C˜ 2(C 1L 1+C 1U 1)(cid:18)
1+|x |2+
1 (cid:88)N
|x
|2(cid:19)1 2
+
C˜ 2(C 1L 0+C 1U 0)
, (3.74)
xi N N i N k N
k=1
and
0≤
(cid:88)N
∂2 V (t,x)ξ ξ ≤
C˜
6|ξ|2 a.e.. (3.75)
xixj N i j N
i,j=1
Moreover, such solution V is characterized by the value function in (2.4) and thus unique. An optimal
N
feedback function is
(cid:26) N (cid:18) N (cid:19) (cid:27)
θ N∗ (cid:0) t,x(cid:1) ∈argm θ∈i Θn λ 2(cid:12) (cid:12)θ(cid:12) (cid:12)2 +(cid:88) f t,θ,x i, N1 (cid:88) δ xj) ∂ xiV N(t,x) .
i=1 j=1
4 Discussion on the convergence rate
In this section we discuss the convergence rate of convergence for the value functions V as well as the
N
minimizer θ∗ where the number of samples N goes to infinity. In terms of neural SDEs, the convergence of
N
V above is instantly interpreted as the convergence of minima of objective functionals, while we may use
N
the convergence of θ∗ above to yield pathwise convergence results that imply the convergence of optimal
N
parameters obtained via neural SDE with N samples (see Proposition 4.7 and Proposition 4.12 below).
We recall that for sufficiently large N, the conclusion in Theorem 3.17 holds as long as T < c˜, while the
conclusion in Theorem 3.18 holds for any T >0.
We first show the interesting fact that the value function V of Problem 2.2 is actually the finite dimen-
N
sional projection of a function V with probability measure as state variable.
27Lemma 4.1. Suppose Hypothesis (R). Let V be the value function in (2.4). For samples x ,...,x ∈R
N 1 N
and y ,...,y ∈R, (for M,N ∈N) suppose that
1 M
N M
1 (cid:88) 1 (cid:88)
δ = δ ,
N xi M yi
i=1 i=1
then for t∈[0,T], T >0,
V (t,x ,...,x )=V (t,y ,...,y ). (4.1)
N 1 N M 1 M
Proof. In view of (3.6) and (3.8), it suffices to show
V0,R(t,x ,...,x )=V0,R(t,y ,...,y ),
N 1 N M 1 M
for any R ,R >0. Here we have defined the value function
1 2
V0,R(t,x ,...,x ):= inf J0,R1(θ,t,x ,...,x ). (4.2)
N 1 N N 1 N
θ∈Uad
R2
Note that
N N M M
1 (cid:88) 1 (cid:88) 1 (cid:88) 1 (cid:88)
Mδ = δ = δ = Nδ .
NM xi N xi M yi NM yi
i=1 i=1 i=1 i=1
Since the left hand side and the right hand side have the same sample size, it holds that
{x ,...,x ,x ,...,x ,...,x ,...,x }={y ,...,y ,y ,...,y ,...,y ,...,y }.
1 N 1 N 1 N 1 M 1 M 1 M
Here the left hand side above consists of M duplicates of {x ,...,x }, while the right hand side above
1 N
consists of N duplicates of {y ,...,y }. According to the variational definition of V0,R, it is easy to check
1 M MN
the symmetric feature that
V0,R(t,x 1⊤,...,x 1⊤)=V0,R(t,y ,...,y ,y ,...,y ,...,y ,...,y ),
NM 1 M N M NM 1 M 1 M 1 M
where
1 :=(1,...,1)⊤.
M
(cid:124) (cid:123)(cid:122) (cid:125)
M − times
Therefore it suffices to show that
V0,R(t,x ,...,x )=V0,R(t,x 1⊤,...,x 1⊤). (4.3)
N 1 N NM 1 M N M
For any continuous θ ∈Uad, define the following particle systems
R2

(cid:18) NM (cid:19)
dX˜ N(k M−1)M+l(t)=f t,θ(t),X˜ N(k M−1)M+l(t), N1
M
(cid:88) δ
X˜ Ni M(t)
dt+σdW0(t),
i=1
X˜(k−1)M+l(s)=x
, 1≤k ≤N, 1≤l≤M.
NM k
Nowthatθ isaboundedprocess, thesolutionadmitsstronguniqueness. Takingadvantageofthesymmetry
and the strong uniqueness, it is easy to verify that for t∈[s,T], the only solution to the above SDE satisfies
X˜(k−1)M+l1(t)=X˜(k−1)M+l2(t), 1≤k ≤N, 1≤l ,l ≤M. (4.4)
NM NM 1 2
Denote by
Xk(t):=X˜(k−1)M+1(t), 1≤k ≤N.
N NM
28In view of (4.4), we have
NM N
1 (cid:88) 1 (cid:88)
δ = δ .
MN X˜ Ni M(t) N X Ni (t)
i=1 i=1
Moreover, (X1(t),...,XN(t)) uniquely solves
N N

(cid:18) N (cid:19)
dXi
(t)=f t,θ(t),Xi (t),
1 (cid:88)
δ dt+σdW0(t),
N N N X Ni (t)
i=1
Xi
(s)=x , 1≤i≤N.
N i
Therefore
J0,R1(θ,t,x ,...,x )=E(cid:20) 1 (cid:88)N (cid:90) T L (cid:0) Xi (t)(cid:1) dt+ 1 (cid:88)N U (cid:0) Xi (T)(cid:1) + λ(cid:90) T |θ(t)|2dt(cid:21)
N 1 N N R1 N N R1 N 2
i=1 0 i=1 0
=E(cid:20) 1 N (cid:88)M(cid:90) T L (cid:0) X˜i (t)(cid:1) dt+ 1 N (cid:88)M U (cid:0) X˜i (T)(cid:1) + λ(cid:90) T |θ(t)|2dt(cid:21)
NM R1 NM NM R1 NM 2
i=1 0 i=1 0
=J0,R1(cid:0) θ,t,x 1⊤,...,x 1⊤(cid:1) .
NM 1 M N M
Since θ is taken arbitrarily from Uad, we have (4.3).
R2
In view of Lemma 4.1, it is easy to see the following definition is meaningful.
Definition4.1. Forsamplesx ,...,x ,denotebyµN thecorrespondingempiricalmeasureµN = 1 (cid:80)N δ .
1 N N i=1 xi
Define
V(t,µN):=V (t,x ,...,x ). (4.5)
N 1 N
In view of the estimates in (3.51), we may now show the Lipschitz continuity of V defined above.
Theorem 4.2. Suppose Hypothesis (R). Let µ and µ be two empirical measures, then
1 2
(cid:20) (cid:18)(cid:90) (cid:90) (cid:19) (cid:21)
|V(t,µ )−V(t,µ )|≤C˜ W (µ ,µ )+C˜ W2(µ ,µ )+ y2µ (dy)+ y2µ (dy) W (µ ,µ ) ,
1 2 71 2 1 2 72 2 1 2 1 2 2 1 2
R R
where
C˜ (CL +CU)
C˜ =C˜ (CL +CU +CL +CU), C˜ = 2 11 11 . (4.6)
71 2 11 11 10 10 72 2
As a result, V(t,·) can be uniquely extended to a local Lipschitz function on P (R).
2
Proof. Up to a duplication, we may assume µ and µ admit the following representation
1 2
N N
1 (cid:88) 1 (cid:88)
µ = δ , µ = δ .
1 N xi 2 N yi
i=1 i=1
Then
(cid:18) N (cid:19)1
1 (cid:88) 2
W (µ ,µ )=min |x −y |2 ,
2 1 2 σ N i σ(i)
i=1
29where the minimum is taken over all permutation on {1,...,N}. Up to a permutation, we may further
assume that
(cid:18) N (cid:19)1
1 (cid:88) 2
W (µ ,µ )= |x −y |2 .
2 1 2 N i i
i=1
Denote by
x :=(x ,...,x ), y :=(y ,...,y ),
N 1 N N 1 N
then
|V(t,µ )−V(t,µ )|=|V (x )−V (y )|.
1 2 N N N N
Let g :[0,1]→R be defined as
g(γ):=V (t,γx +(1−γ)y ).
N N N
Then according to (3.65),
(cid:90) 1 N (cid:90) 1
(cid:88)
|V (t,x )−V (t,y )|≤ |g′(γ)|dγ ≤ |∂ V (t,γx +(1−γ)y )|·|x −y |dγ, (4.7)
N N N N xi N N N i i
0 i=1 0
where
|∂ V (t,γx +(1−γ)y )|
xi N N N
≤
C˜ 2(C 1L 1+C 1U 1)(cid:18)
1+|γx +(1−γ)y |2+
1 (cid:88)N
|γx +(1−γ)y
|2(cid:19)1 2
+
C˜ 2(C 1L 0+C 1U 0)
N i i N j j N
j=1
≤
C˜ 2(C 1L 1+C 1U 1)(cid:20)
1+|γx +(1−γ)y
|+(cid:18) 1 (cid:88)N
|γx +(1−γ)y
|2(cid:19) 21(cid:21)
+
C˜ 2(C 1L 0+C 1U 0)
.
N i i N j j N
j=1
Direct calculation gives
(cid:90) 1 |x −y | 1
|x −y | |γx +(1−γ)y |dγ = i i (|x |x −|y |y )≤ |x −y |2,
i i i i 2(x −y ) i i i i 2 i i
0 i i
(cid:90) 1(cid:18) 1 (cid:88)N (cid:19)1 2 (cid:18) 1 (cid:88)N (cid:90) 1 (cid:19)1 2
|x −y | |γx +(1−γ)y |2 dγ ≤|x −y | |γx +(1−γ)y |2dγ
i i N j j i i N j j
0 j=1 j=1 0
(cid:18) N N (cid:19)1
1 (cid:88) 1 (cid:88) 2
≤|x −y | |x |2+ |y |2 .
i i 2N j 2N j
j=1 j=1
We notice that in the previous computations we have assumed that x ̸= y , otherwise the inequalities are
j j
trivially true. Plug the above inequalities into (4.7),
|V(t,µ )−V(t,µ )|≤
C˜ 2(C 1L 1+C 1U 1+C 1L 0+C 1U 0)(cid:88)N
|x −y |+
C˜ 2(C 1L 1+C 1U 1)(cid:88)N
|x −y |2
1 2 N i i 2N i i
i=1 i=1
+
C˜ 2(C 1L 1+C 1U 1)(cid:18) (cid:88)N
|x −y
|(cid:19)(cid:18) 1 (cid:88)N
|x |2+
1 (cid:88)N
|y
|2(cid:19)1 2
N i i 2N j 2N j
i=1 j=1 j=1
C˜ (CL +CU)
≤C˜ (CL +CU +CL +CU)W (µ ,µ )+ 2 11 11 W2(µ ,µ )
2 11 11 10 10 2 1 2 2 2 1 2
C˜ (CL +CU) (cid:18)(cid:90) (cid:90) (cid:19)
+ 2 11 11 W (µ ,µ )· y2µ (dy)+ y2µ (dy) .
2 2 1 2 1 2
R R
30Corollary4.3. SupposeHypothesis(R)orHypothesis(R1). LetµN := 1 (cid:80)N δ →µin(P (R),W ),
N i=1 xi 2 2
as N →+∞. Then
lim V (t,x ,...,x )=V(t,µ),
N 1 N
N→+∞
at a rate
|V (t,x ,...,x )−V(t,µ)|≤C˜ W (µN,µ)
N 1 N 71 2
(cid:20) (cid:18)(cid:90) (cid:90) (cid:19) (cid:21)
+C˜ W2(µN,µ)+ x2µN(dx)+ x2µ(dx) W (µ ,µ ) .
72 2 2 1 2
R R
In view of Theorem 4.2 and Corollary 4.3 above, the definition domain of V can be extended to P (R).
2
Moreover,theyrevealtheconvergence(ataspecificrate)ofV (t,x ,...,x )toV(t,µ)whenever 1 (cid:80)N δ
N 1 N N i=1 xi
converges in P (R). It is thus natural to further consider the convergence of feedback control function
2
θ∗(t,x ,...,x ), as well as the corresponding convergence rate.
1 N
Consider empirical measure
N
1 (cid:88)
µN = δ ,
N xi
i=1
and we introduce the notation
D(N)V(t,µN,x ):=N∂ V (t,x ,...,x ), i=1,...,N. (4.8)
µ i xi N 1 N
In view of the symmetric property of V , D(N)V(t,µN,x ) above is well-defined.
N µ i
Next we show that V is differentiable in the measure variable at (t,µN) and
∂ V(t,µN,x )=D(N)V(t,µN,x ). (4.9)
µ i µ i
Lemma 4.4. Suppose Hypothesis (R). Let V be the value function in (2.4). For samples x ,...,x ∈R
N 1 N
and y ,...,y ∈R, suppose that
1 M
N M
1 (cid:88) 1 (cid:88)
µN := δ = δ =:νM,
N xi M yi
i=1 i=1
then for t∈[0,T], T >0 and any bounded continuous function φ: R → R, we have
(cid:90) (cid:90)
φ(y)D(N)V(t,µN,y)µN(dy)= φ(y)D(M)V(t,νM,y)νM(dy), (4.10)
µ µ
R R
as well as
(cid:18) (cid:19) (cid:90)
lim ϵ−1 V(cid:0) t,(I+ϵφ)♯µN(cid:1) −V(t,µN) = φ(y)D(N)V(t,µN,y)µN(dy). (4.11)
µ
ϵ→0+ R
As a result, (4.9) is valid.
Proof. Similarly to the comments before (4.3), it suffices to show that
N MN
(cid:88) (cid:88)
φ(x )∂ V (t,x ,...,x )= φ(y )∂ V (t,y ,...,y ), (4.12)
i xi N 1 N i yi NM 1 NM
i=1 i=1
where we have adopted the notation in (4.8) and
(y ,...,y )=(x 1⊤,...,x 1⊤),
1 NM 1 M N M
31in other words, y =x , 1≤i≤N. According to Lemma 4.1,
(i−1)+k i
V (t,x ,...,x )=V (t,x 1⊤,...,x 1⊤)=V (t,y ,...,y ).
N 1 N NM 1 M N M NM 1 NM
Take the derivative with respect to x and obtain
i
M
(cid:88)
∂ V (t,x ,...,x )= ∂ V (t,y ,...,y ).
xi N 1 N y(i−1)+k NM 1 NM
k=1
Using the equality above and noticing that y =x , 1≤i≤N, we can show that (4.12) is true.
(i−1)+k i
To show (4.11), we may plug in (4.5) and (4.8). Then (4.11) and (4.9) follows.
According to Lemma 4.4, we may present the optimal feedback function θ∗ in such a way that
N
θ∗(t,µN):=θ∗ (cid:0) t,x,∇ V (t,x)(cid:1)
N x N
(cid:26) λ (cid:90) (cid:27)
=argmin |θ|2+ f(t,θ,y,µN)∂ V(t,µN,y)µN(dy) . (4.13)
2 µ
θ∈Θ R
Similar to Theorem 4.2, we can show the Lipschitz continuity of θ∗(t,µN) in (4.13), which implies the
convergence rate of the optimal feedback function.
Theorem 4.5. Suppose Hypothesis (R). Let µ , µ be two empirical measures and θ∗(t,µ) be defined as
1 2
in (4.13). Then for T <c˜, where c˜is from Proposition 3.10,
|θ∗(t,µ )−θ∗(t,µ )|≤C˜ W (µ ,µ ). (4.14)
1 2 8 1 1 2
Here
C˜ :=(λ−λ )−1CQ+(λ−λ )−1∥f ∥ C˜ .
8 0 0 θ ∞ 6
As a result, θ∗(t,·) can be uniquely extended to a Lipschitz continuous mapping on (P (R),W ).
2 1
Proof. Up to a duplication, we may assume that µ and µ have the same sample size. Denote by
1 2
N N
1 (cid:88) 1 (cid:88)
µ = δ , µ = δ .
1 N xi 1 N yi
i=1 i=1
It is easy to see from (4.13) that θ∗(t,x ,...,x ) on the right hand side remains unchanged after a permu-
1 N
tation of the input {x ,...,x }. Hence up to a permutation, we may assume that
1 N
N
1 (cid:88)
W (µ ,µ )= |x −y |.
1 1 2 N i i
i=1
Define
g(γ):=θ∗(t,γx +(1−γ)y ), γ ∈[0,1].
N N
According to (3.38) and (3.66),
|θ∗(t,µ )−θ∗(t,µ )|=|θ∗(t,x ,...,x )−θ∗(t,y ,...,y )|
1 2 1 N 1 N
(cid:90) 1
=|g(1)−g(0)|≤ |g′(γ)|dγ
0
≤(cid:88)N |x l−y l|(cid:90) 1(cid:12) (cid:12) (cid:12) (cid:12)∂∂ p θ∗+(cid:88)N ∂∂ q θ∗·∂ k2 lV N(cid:12) (cid:12) (cid:12) (cid:12)(t,γx N +(1−γ)y N)dγ
l=1 0 l k=1 k
32≤
(λ−λ 0)−1CQ+(λ−λ 0)−1∥f θ∥ ∞C˜
6
(cid:88)N
|x −y |
N l l
l=1
=((λ−λ )−1CQ+(λ−λ )−1∥f ∥ C˜ )W (µ ,µ ).
0 0 θ ∞ 6 1 1 2
Now we have the convergence rate of feedback function as the sample size grows to infinity.
Corollary4.6. SupposethattheassumptionsofTheorem4.5takeplaceandsupposethatµN := 1 (cid:80)N δ →
N i=1 xi
µ in (P (R),W ), as N →+∞. Then for T <c˜where c˜is from Proposition 3.10,
2 2
lim θ∗ (cid:0) t,x,∇ V (t,x)(cid:1) =θ∗(t,µ),
N x N
N→+∞
at a rate
|θ∗ (cid:0) t,x,∇ V (t,x)(cid:1) −θ∗(t,µ)|≤C˜ W (µN,µ).
N x N 8 1
Proof. This is directly from (4.13) and Theorem 4.5.
Another consequence of Theorem 4.5 is the pathwise convergence with algebraic rate.
Proposition 4.7. Let X∗ = (X1,∗(t),...,XN,∗(t)) , N ≥ 1 be the optimal path of Problem 2.2, with
N N N t∈[0,T]
initial values x(1),x(2),...,x(N). Suppose that the assumptions of Theorem 4.5 take place and suppose that
N N N
1 (cid:80)N δ → µ in (P (R),W ), as N → +∞. Then for T < c˜ where c˜ is from Proposition 3.10, there
N i=1 x(i) 2 2
exists an aN dapted limit process (θ∗,µ∗), where θ∗(t)∈Θ and µ∗(t)∈P (R), 0≤t≤T, such that µ∗(0)=µ
1
and
max W (µ∗ (s),µ∗(s))≤Cˆ W (cid:0) µ∗ (0),µ(0)(cid:1) ,
1 N 8 1 N
s∈[0,T]
max |θ∗ (cid:0) s,X∗(s),∇ V (t,X∗(s))(cid:1) −θ∗(s)|≤Cˆ W (µ∗ (0),µ(0)). (4.15)
N N x N N 8 1 N
s∈[0,T]
Here µ∗ (t):= 1 (cid:80)N δ and Cˆ >0 is a constant independent of N.
N N i=1 Xi,∗(t) 8
N
Proof. In order to show the first inequality in (4.15), it suffices to first show that, for the sample paths X∗
N
and X∗ , which corresponds to sample number N and M respctively, it holds that
M
max W (µ∗ (s),µ∗ (s))≤Cˆ W (cid:0) µ∗ (0),µ∗ (0)(cid:1) , (4.16)
1 N M 8 1 N M
s∈[0,T]
and then pass M to infinity. Here we have used the assumption that µ∗ (0) = 1 (cid:80)N δ , N ≥ 1, is a
N N i=1 x(i)
Cauchy sequence in (P (R),W ). N
2 2
According to Lemma 4.4 and (4.13), define
f∗(t,x,µ):=f(t,θ∗(t,µ),x,µ), (t,x,µ)∈[0,T]×R×P(R),
then the optimal path X∗ satisfies
N
dXi,∗(t)=f∗(t,Xi,∗(t),µ∗ (t))dt+σdW(t), 1≤i≤N. (4.17)
N N N
Moreover, according to Theorem 4.5, for x ,...,x ,x˜ ,...,x˜ ∈R, it holds that
1 N 1 N
(cid:12) (cid:12) (cid:12) (cid:12)f∗(cid:18) t,x i, N1 (cid:88)N δ xj(cid:19) −f∗(cid:18) t,x˜ i, N1 (cid:88)N δ x˜j(cid:19)(cid:12) (cid:12) (cid:12) (cid:12)≤Cˆ 8|x i−x˜ i|+ C Nˆ 8 (cid:88)N |x j −x˜ j|, 1≤i≤N. (4.18)
ij=1 j=1 j=1
33Given the dynamics in (4.17), we may proceed in a similar fashion to the proof in Lemma 4.1 and show that
µ∗ (t)=µ˜∗ (t),
N NM
where
NM N
1 (cid:88) 1 (cid:88)
µ˜∗ (t):= δ , µ˜∗ (0)= Mδ ,
NM NM X˜ Ni,∗ M(t) NM NM x( Ni)
i=1 i=1
with
dX˜i,∗ (t)=f∗(t,X˜i,∗ (t),µ˜∗ (t))dt+σdW(t), 1≤i≤NM.
NM NM NM
Therefore, up to a duplication, showing (4.16) is equivalent to showing that
max W (µ∗ (s),µ˜∗ (s))≤Cˆ W (cid:0) µ∗ (0),µ˜∗ (0)(cid:1) , N ≥1, (4.19)
1 N N 8 1 N N
s∈[0,T]
where
N N
1 (cid:88) 1 (cid:88)
µ˜∗ (t):= δ , µ˜∗ (0)= δ ,
N N X˜ Ni,∗(t) N N x˜( Ni)
i=1 i=1
with
dX˜i,∗(t)=f∗(t,X˜i,∗(t),µ˜∗ (t))dt+σdW(t), 1≤i≤N.
N N N
Herex˜(1),...,x˜(N) areN arbitrarynumbersfromR. Butinviewof (4.18),subtractingtheaboveand(4.17)
N N
as well as the standard Gr¨onwall’s inequality then yields (4.19), which further implies (4.16).
Having obtained (4.16), we may use Theorem 4.5 to further show that for any t∈[0,T],
|θ∗ (cid:0) t,X (t),∇ V (t,X (t))(cid:1) −θ∗ (cid:0) t,X (t),∇ V (t,X (t))(cid:1) |
N N x N N M M x M M
=|θ∗(t,µ∗ (t))−θ∗(t,µ∗ (t))|≤Cˆ W (µ∗ (0),µ (0)).
N M 8 1 N M
Passing M to infinity in the above yields the second inequality in (4.15).
Thepathθ∗ (t,X (t),∇ V (t,X (t))), t∈[0,T]inProposition4.7actuallycorrespondstotheoptimal
N N x N N
parameters obtained via the neural SDE with N samples. Hence we may interpret Proposition 4.7 in such a
way that the aforementioned parameters converge, at certain rate, as long as the empirical distributions of
inputs converge as N tends to infinity.
In addition to the above convergence for short time, we can also obtain the global convergence under
assumptions on convexity. We first do some preparation in Lemma 4.8 then present the main results in
Theorem 4.10. Denote by
N N
1 (cid:88) 1 (cid:88)
µN := δ , νN := δ .
N xi N yi
i=1 i=1
Lemma 4.8. Suppose Hypothesis (R1) and λ is sufficiently large. Then
N1 (cid:88)N (cid:12) (cid:12)∂ µV(t,µN,x i)−∂ µV(t,νN,y i)(cid:12) (cid:12)2 ≤ C N˜ 6 (cid:88)N |x i−y i|2. (4.20)
i=1 i=1
Proof. In view of Theorem 3.18, for (t,x)∈[0,T]×RN we have
(cid:18) (cid:19)⊤
∇2 xV N(t,x)= ∇2 xV N(t,x)1 2 ∇2 xV N(t,x)21,
34for some matrix ∇2 xV N(t,x)1 2 ∈RN×N such that for any α∈RN with |α|=1,
(cid:115)
C˜
|∇2 xV N(t,x)21α|≤ N6|α|.
Therefore for any α,x,y ∈RN,
⟨α,∇ V (t,x)−∇ V (t,y)⟩
x N x N
(cid:90) 1(cid:28) (cid:29)
= ∇2 xV N(t,y+s(x−y))1 2α,∇2 xV N(t,y+s(x−y))21(x−y) ds
0
C˜
≤ 6|α|·|x−y|.
N
The inequality above implies that
C˜
|∇ V (t,x)−∇ V (t,y)|≤ 6|x−y|,
x N x N N
which is (4.20) according to (4.8).
Lemma 4.8 tells that we can extend the domain of ∂ V(t,ν,·) from the set of all empirical measures to
µ
ν ∈P (R) in some weak sense, which is formalized as follows.
2
Corollary 4.9. For each t ∈ [0,T], there exists a Lipschitz continuous mapping Φ that maps empirical
t
measures on R to P (R) in such a way that for any µ= 1 (cid:80)N δ ,
2 N i=1 xi
N
1 (cid:88)
Φ (µ)= δ .
t N ∂µV(t,µ,xi)
i=1
Therefore, Φ can be uniquely extended to a Lipschitz continuous map Φ :P (R)→P (R).
t t 2 2
Proof. Consider the following empirical measures
N N
1 (cid:88) 1 (cid:88)
µN = δ , νN = δ .
N xi N yi
i=1 i=1
In view of the symmetric property, there is no loss of generality in assuming
(cid:18) N (cid:19)1
1 (cid:88) 2
W (µN,νN)= |x −y |2 .
2 N i i
i=1
Then we have from Lemma 4.8 that
(cid:113)
W (cid:0) Φ (µN),Φ (νN)(cid:1) ≤ C˜ W (µN,νN).
2 t t 6 2
Hence Φ is Lipschitz continuous.
t
As a result of the distributional difference estimate in Lemma 4.8, we deduce the Lipschitz continuity of
θ∗(t,µN) for long time T >0.
Theorem 4.10. Suppose Hypothesis (R1). Let µN = 1 (cid:80)N δ , νN = 1 (cid:80)N δ . Then
N i=1 xi N i=1 yi
|θ∗(t,µN)−θ∗(t,νN)|≤C˜ W (µN,νN), (4.21)
9 2
for some C˜
9
=C˜ 9(f,λ21,T,L,U,(λ−λ 0)−1 2).
35Proof. Let θ∗, θˆ∗ denote the optimal feedback function corresponding to µN and νN. According to the first
order condition,
N
1 (cid:88)
λθ∗+ f (t,θ∗,x ,µN)∂ V(t,µN,x )=0,
N θ i µ i
i=1
N
λθˆ∗+ 1 (cid:88) f (t,θˆ∗,y ,νN)∂ V(t,νN,y )=0.
N θ i µ i
i=1
Subtracting the above and utilizing (3.35), (3.36), (3.74) as well as (4.6), we have
(λ−λ )|θ∗−θˆ∗|≤
∥f θx∥ ∞CQ (cid:88)N
|x −y |+
∥f µθ∥ ∞C˜
71
(cid:88)N
|x −y |
0 N i i N i i
i=1 i=1
N
+ ∥f θ∥ ∞ (cid:88) |∂ V(t,µN,x )−∂ V(t,µˆN,y )|.
N µ i µ i
i=1
InviewofLemma4.8,bychoosingappropriate(x ,y ),i=1,...,N,wecandeduce(4.21)fromtheabove.
i i
Parallel to Corollary 4.6 and Proposition 4.7, we estimate the convergence rate of feedback function and
the optimal parameters for long time T >0 as follows.
Corollary4.11. SupposethattheassumptionsofTheorem4.10takeplaceandsupposethatµN := 1 (cid:80)N δ →
N i=1 xi
µ in (P (R),W ), as N →+∞. Then for T >0,
2 2
lim θ∗ (cid:0) t,x,∇ V (t,x)(cid:1) =θ∗(t,µ), t∈[0,T],
N x N
N→+∞
at a rate
|θ∗ (cid:0) t,x,∇ V (t,x)(cid:1) −θ∗(t,µ)|≤C˜ W (µN,µ).
N x N 9 2
Proposition 4.12. Let X∗ =(X1,∗(t),...,XN,∗(t)) , N ≥1 be the optimal path of Problem 2.2, with
N N N t∈[0,T]
initial values x(1),x(2),...,x(N). Suppose that the assumptions of Theorem 4.10 take place and suppose that
N N N
1 (cid:80)N δ → µ in (P (R),W ), as N → +∞. Then for T > 0, there exists an adapted limit process
N i=1 x(i) 2 2
(θ∗,µ∗), whN ere θ∗(t)∈Θ and µ∗(t)∈P (R), 0≤t≤T, such that µ∗(0)=µ and
1
max W (µ∗ (s),µ∗(s))≤Cˆ W (cid:0) µ∗ (0),µ(0)(cid:1) ,
1 N 9 1 N
s∈[0,T]
|θ∗ (cid:0) t,X∗,∇ V (t,X∗)(cid:1) −θ∗(t)|≤Cˆ W (µ∗ (0),µ(0)). (4.22)
N N x N N 9 1 N
Here µ∗ (t):= 1 (cid:80)N δ and Cˆ >0 is a constant independent of N.
N N i=1 Xi,∗(t) 9
N
5 A Linear-Quadratic example
As an example, covered by our main results, we study a linear-quadratic model in this section. See also Li
et. al. (2023) and Li et. al. (2023) for other propagation of chaos results under linear-quadratic model.
Set the parameters in (1.1) and (2.1) as follows
(cid:90)
f(t,θ,x,µ)=θ+x+ yµ(dy), L(x)=U(x)=x2−x, σ =λ=1, T =2.
R
36Then Hypothesis (R1) can be easily verified. Moreover (3.1) reduces to

∂ tV
N
+ σ 22 (cid:88)N ∂ i2 jV
N
+ θin ∈Rf (cid:26) (cid:12) (cid:12)θ(cid:12) (cid:12)2 +(cid:88)N (cid:18) θ+x i+ N1 (cid:88)N x j(cid:19) ∂ iV
N
+ N1 (cid:88)N (x2
i
−x i)(cid:27) =0,
i,j=1 i=1 j=1 i=1
(5.1)

V N(T,x 1,...,x N)=
N1 (cid:88)N
(x2
i
−x i).
i=1
In view of the quadratic structure as well as the symmetry of the value function, we make the ansatz
(cid:18)(cid:90) (cid:19)2 (cid:18)(cid:90) (cid:19) (cid:18)(cid:90) (cid:19)
V (t,x ,...,x )=a (t) yµN(dy) +b (t) yµN(dy) +c (t) y2µN(dy) +d (t), (5.2)
N 1 N N N N N
R R R
for some a ,b ,c ,d :[0,T]→R, where
N N N N
N
1 (cid:88)
µN := δ .
N xi
i=1
Suppose the previous ansatz, then according to Lemma 4.1 and (4.5),
V(t,µN)=V (t,x ,...,x ),
N 1 N
and we may further reduce the ansatz to
(cid:18)(cid:90) (cid:19)2 (cid:18)(cid:90) (cid:19) (cid:18)(cid:90) (cid:19)
V (t,x ,...,x )=a(t) yµN(dy) +b(t) yµN(dy) +c(t) y2µN(dy) +d(t), (5.3)
N 1 N
R R R
for some a,b,c,d:[0,T]→R.
According to the first order condition, we can write the optimal control as
θ∗ (t,x ,...,x )=−(cid:0)
a(t)+c(t)(cid:1)(cid:18)(cid:90) yµN(dy)(cid:19)
− 1 b(t). (5.4)
N 1 N 2
R
Plugging (5.3), (5.4) into (5.1), we have that
 a˙(t)+4a(t)+2c(t)−(a(t)+c(t))2 =0,

c˙(t)+2c(t)+1=0,
b˙(t)+(cid:0) 2b(t)−(a(t)+c(t))b(t)(cid:1)
−1=0,
(5.5)

d˙(t)− 1 b2(t)+(cid:0) a(t)+c(t)(cid:1) =0,
4
with the terminal condition a(T) = d(T) = 0, −b(T) = c(T) = 1. The ansatz (5.3) and (5.4) are verified
once we show the well-posedness of (5.5).
In fact, we may first solve c(t) according to the second equation in (5.5). Then we can add up the first
and the second equation in (5.5) and obtain
g˙(t)+4g(t)−g2(t)+1=0, g(T)=1,
where g(t):=a(t)+c(t). After a reverse of time, we find that the above is a Riccati equation whose global
solution is guaranteed. Therefore we may solve a(t) after obtaining g(t). Given a(t) and c(t), we can solve
b(t) and d(t) accordingly.
37The following plots illustrate the convergence of the value functions and optimal parameters in (5.3) and
(5.4) as N →+∞.
94 0.36
Sample set 1 Sample set 1
93 S Sa am mp pl le e s se et t 2 3 0.34 S Sa am mp pl le e s se et t 2 3
92
0.32
91
90 0.3
89 0.28
88 0.26
87
0.24
86
85 0.22
84 0.2
0 0.5 1 1.5 2 0 0.5 1 1.5 2
105 105
Figure 1: V (0,x ) and θ (0,x ) versus N
N N N N
Here we generate the first N samples x :=(x ,...,x )∈RN in such a way that 1 (cid:80)N δ −d →N(0,1)
N N N N i=1 xN
(standardGaussiandistribution). Theabovegenerationofsamplesetisrepeated3timesandtheresultsare
reported in Figure 1. It is suggested in the plots that, as the number N tends to infinity, both the minima
of objective functionals and the optimal parameters converge to certain value.
A Some technical results
First we recall the notation introduced in (3.11): M (C)⊂RN×N and
N
A∈M (C) if and only if |A |≤C(δ +N−1), 1≤i,j ≤N.
N ij ij
Next, for a matrix valued function A˜, where A˜(t) ∈ RN×N, t ∈ [0,T], we define the matrix |A˜| ∈ RN×N in
such a way that
|A˜| = max |A˜ (t)|, 1≤i,j ≤N. (A.1)
ij ij
t∈[0,T]
Further, define the norm ∥A˜∥ by
N
∥A˜∥ = max Nδij−1|A˜| . (A.2)
N ij
1≤i,j≤N
Lemma A.1. For N ≥1, let A ∈M (C ) and B ∈M (C ), then A B ∈M (3C C ).
N N 1 N N 2 N N N 1 2
Proof. If i̸=j, then
N
(cid:88)(cid:12) (cid:12)aN ikbN kj(cid:12) (cid:12)≤C 1· C N2 + C N1 ·C 2+ C N1 · C N2 ·(N −2)< 3C N1C 2.
k=1
If i=j, then
N
(cid:88)(cid:12) (cid:12)aN ikbN kj(cid:12) (cid:12)≤C 1C 2+ C N1 · C N2 ·(N −1)<3C 1C 2.
k=1
Hence we have A B ∈M (3C C ).
N N N 1 2
38Lemma A.2. For each N ≥ 1, let (W(t)) be a real valued standard Brownian motion. Let the
t∈(0,T)
RN×N-valued processes (X(t)) , (A(t)) , (B(t)) , satisfy
t∈(0,T) t∈(0,T) t∈(0,T)
(cid:90) t (cid:90) t
X(t)=X(0)+ X(s)A(s)ds+ X(s)B(s)dW , t∈[0,T]. (A.3)
s
0 0
Suppose that X(0) satisfies for k =1,2,...

C , i=j,
 0,k
E(cid:2)
|X
ij(0)|2k(cid:3)
≤ C
0,k, i̸=j,
N2k
and |A|,|B|∈M (C) for some constant C. Then
N
 C˜ , i=j,
(cid:20) (cid:21)  k
E max |X ij(s)|2k ≤ C˜
0≤s≤T  k , i̸=j,
N2k
where C˜ =C˜ (C ,C,T) is increasing in T (but independent of N).
k k 0,k
Proof. We show that (A.3) admits a unique solution X, with the required estimates, which is also the fixed
point of the mapping Φ: X (cid:55)→X˜ defined as follow:
(cid:90) t (cid:90) t
X˜(t)=X + X(s)A(s)ds+ X(s)B(s)dW(s), t∈[0,δ], (A.4)
0
0 0
where δ >0 is a small enough positive number. Consider to inputs X(1) and X(2), for 1≤p,q ≤N,
Φ
(cid:0) X(1)(cid:1)
(t)−Φ
(cid:0) X(2)(cid:1)
(t)
pq pq
=(cid:90) t (cid:88)N (cid:104) X(1)(s)−X(2)(s)(cid:105)
A
(s)ds+(cid:90) t (cid:88)N (cid:104) X(1)(s)−X(2)(s)(cid:105)
B (s)dW(s),
pk pk kq pk pk kq
0 k=1 0 k=1
then according to Burkholder–Davis–Gundy inequality,
(cid:20) (cid:21)
E max |Φ (cid:0) X(1)(cid:1) (t)−Φ (cid:0) X(2)(cid:1) (t)|2k
pq pq
t∈[0,δ]
 
(cid:90) δ(cid:12) N (cid:20) (cid:21) (cid:12)2k
≤C kE

(cid:12) (cid:12) (cid:12)(cid:88) X p(1 i)(s)−X p(2 i)(s) A iq(s)(cid:12) (cid:12)
(cid:12)
ds

0 i=1
i̸=q
(cid:34) (cid:90) δ(cid:12)(cid:20) (cid:21) (cid:12)2k (cid:35)
+C kE (cid:12) (cid:12)
(cid:12)
X p(1 q)(s)−X p(2 q)(s) A qq(s)(cid:12) (cid:12)
(cid:12)
ds
0
 
(cid:90) δ(cid:12) N (cid:20) (cid:21) (cid:12)2k
+C kE

(cid:12) (cid:12) (cid:12)(cid:88) X p(1 i)(s)−X p(2 i)(s) B iq(s)(cid:12) (cid:12)
(cid:12)
ds

0 i=1
i̸=q
(cid:34) (cid:90) δ(cid:12)(cid:20) (cid:21) (cid:12)2k (cid:35)
+C kE (cid:12) (cid:12)
(cid:12)
X p(1 q)(s)−X p(2 q)(s) B qq(s)(cid:12) (cid:12)
(cid:12)
ds .
0
According to Jensen’s inequality,
(cid:12) (cid:12) (cid:12) (cid:12)(cid:88)N (cid:20) X p(1 i)(s)−X p(2 i)(s)(cid:21) A iq(s)(cid:12) (cid:12) (cid:12) (cid:12)2k ≤ NC2 2k
k
·(N −1)2k·(cid:12) (cid:12) (cid:12) (cid:12)N1 −1(cid:88)N |X p(1 i)(s)−X p(2 i)(s)|(cid:12) (cid:12) (cid:12) (cid:12)2k
i=1 i=1
i̸=q i̸=q
39N
≤C2k· 1 (cid:88)(cid:12) (cid:12)X(1)(s)−X(2)(s)(cid:12) (cid:12)2k . (A.5)
N −1 pi pi
i=1
i̸=q
Here we have used
(cid:12) (cid:12)(cid:2) X p(1 i)(s)−X p(2 i)(s)(cid:3) A iq(s)(cid:12) (cid:12)≤  CC
(cid:12)
(cid:12)(cid:12) (cid:12) XX p (( 11 i )) (( ss )) −− XX p (( 22 i )) (( ss ))
(cid:12)
(cid:12)(cid:12) (cid:12) ,, ii ̸== qq .,
N pi pi
Therefore
(cid:20)(cid:90) δ(cid:12) N (cid:20) (cid:21) (cid:12)2k (cid:21)
E (cid:12) (cid:12) (cid:12)(cid:88) X p(1 i)(s)−X p(2 i)(s) A iq(s)(cid:12) (cid:12)
(cid:12)
ds
0 i=1
i̸=q
≤
C2k (cid:88)N E(cid:34) (cid:90) δ
(cid:12) (cid:12)X(1)(s)−X(2)(s)(cid:12) (cid:12)2k
ds(cid:35)
N −1 pi pi
i=1 0
i̸=q
≤
C2kδ (cid:88)N E(cid:20)
max
(cid:12) (cid:12)X(1)(s)−X(2)(s)(cid:12)
(cid:12)2k(cid:21)
N −1 0≤s≤δ pi pi
i=1
i̸=q
(cid:20) (cid:21)
≤C2kδ max E max |X(1)(t)−X(2)(t)|2k . (A.6)
ij ij
1≤i,j≤N 0≤t≤δ
Similar estimates to (A.5) yields
(cid:20) (cid:21) (cid:20) (cid:21)
E max |Φ (cid:0) X(1)(cid:1) (t)−Φ (cid:0) X(2)(cid:1) (t)|2k ≤4δC C2k max E max |X(1)(t)−X(2)(t)|2k ,
pq pq k
t∈[0,δ] 1≤i,j≤N 0≤t≤δ
and thus
(cid:20) (cid:21)
max E max |Φ (cid:0) X(1)(cid:1) (t)−Φ (cid:0) X(2)(cid:1) (t)|2k
ij ij
1≤i,j≤N t∈[0,δ]
(cid:20) (cid:21)
≤4δC C2k max E max |X(1)(t)−X(2)(t)|2k ,
k ij ij
1≤i,j≤N 0≤t≤δ
Consider
1
δ < . (A.7)
8C C2k
k
For the sake of later iterations, we note here that the choice of δ in (A.7) is independent of the bound C of
0
initial data.
In view of (A.7), Φ is a contraction mapping. Next, we claim that Φ maps the following set
(cid:26) (cid:20) (cid:21) (cid:27)
X := X : X is matrix valued process and E max |X (s)|2k ≤M (N−2k+δ ) . (A.8)
ij k ij
0≤s≤δ
into itself for some M .
k
To see the claim, consider 1≤p,q ≤N and p̸=q,
(cid:20) (cid:21)
E max |Φ (cid:0) X(cid:1) (t)|2k
pq
t∈[0,δ]
(cid:20)(cid:90) δ(cid:12) N (cid:12)2k (cid:21) (cid:20)(cid:90) δ (cid:21)
≤C kE(cid:2)(cid:12) (cid:12)X pq(0)(cid:12) (cid:12)2k(cid:3) +C kE (cid:12) (cid:12) (cid:88) X pi(s)A iq(s)(cid:12) (cid:12) ds +C kE (cid:12) (cid:12)X pp(s)A pq(s)(cid:12) (cid:12)2k ds
(cid:12) (cid:12)
0 i=1 0
i̸=p,q
40(cid:20)(cid:90) δ (cid:21) (cid:20)(cid:90) δ(cid:12) N (cid:12)2k (cid:21)
+C kE (cid:12) (cid:12)X pq(s)A qq(s)(cid:12) (cid:12)2k ds +C kE (cid:12) (cid:12) (cid:88) X pi(s)B iq(s)(cid:12) (cid:12) ds
(cid:12) (cid:12)
0 0 i=1
i̸=p,q
(cid:20)(cid:90) δ (cid:21) (cid:20)(cid:90) δ (cid:21)
+C kE (cid:12) (cid:12)X pp(s)B pq(s)(cid:12) (cid:12)2k ds +C kE (cid:12) (cid:12)X pq(s)B qq(s)(cid:12) (cid:12)2k ds .
0 0
In view of (A.5) and (A.8),
E(cid:20)(cid:90) δ(cid:12) (cid:12)
(cid:12)
(cid:12)
(cid:88)N
X pi(s)A
iq(s)(cid:12) (cid:12)
(cid:12)
(cid:12)2k ds(cid:21)
≤
NC −2k 2E(cid:20) (cid:88)N (cid:90) δ (cid:12)
(cid:12)X
pi(s)(cid:12) (cid:12)2k ds(cid:21)
0 i=1 i=1 0
i̸=p,q i̸=p,q
≤ NδC −2k
2
(cid:88)N E(cid:20) 0m ≤sa ≤x δ(cid:12) (cid:12)X pi(s)(cid:12) (cid:12)2k(cid:21) ≤ δC N2k 2M
k
k.
i=1
i̸=p,q
Combining the two inequalities above together, we arrive at
E(cid:20)
max |Φ
(cid:0) X(cid:1) (t)|2k(cid:21)
≤
C kC 02k
+
6δC kM kC2k
.
t∈[0,δ] pq N2k N2k
Similarly,
(cid:20) (cid:21)
E max |Φ (cid:0) X(cid:1) (t)|2k
pp
t∈[0,δ]
(cid:20)(cid:90) δ(cid:12) N (cid:12)2k (cid:21) (cid:20)(cid:90) δ (cid:21)
≤C kE(cid:2)(cid:12) (cid:12)X pp(0)(cid:12) (cid:12)2k(cid:3) +C kE (cid:12) (cid:12)(cid:88) X pi(s)A ip(s)(cid:12) (cid:12) ds +C kE (cid:12) (cid:12)X pp(s)A pp(s)(cid:12) (cid:12)2k ds
(cid:12) (cid:12)
0 i=1 0
i̸=p
(cid:20)(cid:90) δ(cid:12) N (cid:12)2k (cid:21) (cid:20)(cid:90) δ (cid:21)
+C kE (cid:12) (cid:12)(cid:88) X pi(s)B ip(s)(cid:12) (cid:12) ds +C kE (cid:12) (cid:12)X pp(s)B pp(s)(cid:12) (cid:12)2k ds
(cid:12) (cid:12)
0 i=1 0
i̸=p
2δC M C2k
≤C C2k+ k k +2δC M C2k.
k 0 N2k k k
Let δ and M satisfy
k
1
δ < , M >3C2k.
12C C2k k 0
k
Note again that the choice of δ is still independent of C . Then estimate above implies that
0
E(cid:2) max |Φ(X) (s)|2k(cid:3) ≤M (N−2k+δ ).
ij k ij
0≤s≤δ
In other words, contraction mapping Φ maps X into itself. Hence the only fixed point of Φ lies in X.
To conclude the lemma, notice that the choice of δ is independent of C , therefore we can separate [0,T]
0
into[0,δ],[δ,2δ],[2δ,3δ],...,thengoovertheprocedureaboverepeatedlyandobtainthedesiredresults.
Proof of Lemma 3.7: Note that for each (t,x,p) ∈ [0,T] × A , θ∗ := θR1(t,x,p) minimizes the strictly
N N
convex function HR1(t,x,p,θ) with respect to θ ∈Θ, hence
N
⟨∂ HR1(t,x,p,θ∗),θ−θ∗⟩≥0, θ ∈Θ.
θ N
Similarly, for another pair of (xˆ,pˆ)∈A and θˆ∗ :=θR1(t,xˆ,pˆ),
N N
⟨∂ HR1(t,xˆ,pˆ,θˆ∗),θ−θˆ∗⟩≥0, θ ∈Θ.
θ N
41Therefore we have by taking θ =θˆ∗, θ∗ that
0≥(cid:10) ∂ HR1(t,x,p,θ∗)−∂ HR1(t,xˆ,pˆ,θˆ∗),θ∗−θˆ∗(cid:11) . (A.9)
θ N θ N
on the other hand,
∂ H (t,x,p,θ∗)−∂ H (t,xˆ,pˆ,θˆ∗)
θ N θ N
=I+(θ∗−θˆ∗)·(λ+II), (A.10)
where
N (cid:18) N (cid:19) N (cid:18) N (cid:19)
(cid:88) 1 (cid:88) (cid:88) 1 (cid:88)
I := f t,θ∗,x , δ p − f t,θ∗,xˆ , δ pˆ, (A.11)
θ i N xj i θ i N xˆj i
i=1 j=1 i=1 j=1
N (cid:20) (cid:18) N (cid:19) (cid:18) N (cid:19)(cid:21)
II :=(cid:88) f t,θ∗,xˆ , 1 (cid:88) δ −f t,θˆ∗,xˆ , 1 (cid:88) δ pˆ i .
θ i N xˆj θ i N xˆj θ∗−θˆ∗
i=1 j=1 j=1
According to (3.35) in the assumption, it holds for some constant λ >0 that
0
λ ≥|II|. (A.12)
0
Plugging (A.10), (A.12) into (A.9), and using the Cauchy-Schwartz inequality, we have that
|θ∗−θˆ∗|≤(λ−λ )−1|I|. (A.13)
0
According to (A.11), I is the difference of the following function (w.r.t. (x,p)∈RN ×RN)
N (cid:18) N (cid:19)
(cid:88) f t,θˆ∗,x , 1 (cid:88) δ p ,
θ i N xj i
i=1 j=1
which implies the local Lipschitz continuity of θR1(t,x,p) with respect to (x,p)∈A .
N N
In view of the local Lipschitz continuity, θR1(t,x,p) is differentiable almost everywhere. Furthermore, it
N
follows from (A.13) that
(cid:12)
(cid:12)∂ xkθ
NR1(t,x,p)(cid:12)
(cid:12)
(cid:12) (cid:18) N (cid:19) N (cid:18) N (cid:19) (cid:12)
≤(λ−λ 0)−1(cid:12) (cid:12) (cid:12)f θx t,θˆ∗,x k, N1 (cid:88) δ xj p k+ N1 (cid:88) ∂ µf θ t,θˆ∗,x i, N1 (cid:88) δ xj (x k)p i(cid:12) (cid:12) (cid:12).
j=1 i=1 j=1
In view of (3.36),
(cid:12)
(cid:12)∂ xkθ
NR1(t,x,p)(cid:12)
(cid:12)≤
2(λ−λ N0)−1CQ
.
Similarly we also have
(cid:12) (cid:12)∂ pkθ NR1(t,x,p)(cid:12) (cid:12)≤(λ−λ 0)−1∥f θ∥ ∞.
Acknowledgements. HLacknowledgesthesupportprovidedbyHongKongRGCGrantECS21302521
and by Singapore MOE AcRF Grants R-146-000-271-112. ARM acknowledges the support provided by the
EPSRC via the NIA with grant number EP/X020320/1 and by the King Abdullah University of Science
and Technology Research Funding (KRF) under Award No. ORA-2021-CRG10-4674.2. CM acknowledges
the support provided by Hong Kong RGC Grant ECS 21302521, Hong Kong RGC Grant GRF 11311422
and by Hong Kong RGC Grant GRF 11303223. CZ acknowledges the support provided by Singapore MOE
(Ministry of Education) AcRF Grant A-8000453-00-00, IoTex Foundation Industry Grant A-8001180-00-00
and NSFC Grant No. 11871364.
42References
M. Bansil, A.R. M´esz´aros, and C. Mou. Global well-posedness of displacement monotone degenerate mean
field games master equations. arXiv:2308.16167, 2023.
S. Baudelet, B. Fr´enais, M. Lauri`ere, A. Machtalay, and Y. Zhu. Deep learning for mean field optimal
transport. arXiv:2302.14739, 2023.
A. Bensoussan, P. J. Graber, and S. C. P. Yam. Stochastic control on space of random variables.
arXiv:1903.1260.
L.Bo, A.Capponi, andH.Liao.Largesamplemean–fieldstochasticoptimization.SIAM J. Control Optim.,
60(4):2538–2573, 2022.
L. Bo, T. Li, and X. Yu. Centralized systemic risk control in the interbank system: Weak formulation and
Gamma–convergence. Stochastic Process Appl., 150:622–654, 2022.
P. Cardaliaguet, S. Daudin, J. Jackson, and P. Souganidis. An algebraic convergence rate for the optimal
control of Mckean–Vlasov dynamics. SIAM J. Control Optim., 61(6):3341–3369, 2023.
P.Cardaliaguet,F.Delarue,J.-M.Lasry,andP.-L.Lions.TheMasterEquationandtheConvergenceProblem
inMeanFieldGames,volume201ofAnn.ofMath.Stud.PrincetonUniversityPress,Princeton,NJ,2019.
P. Cardaliaguet, J. Jackson, N. Mimikos-Stamatopoulos, and P.E. Souganidis. Sharp convergence rates for
mean field control in the region of strong regularity. arXiv:2312.11373, 2023.
R. Carmona and F. Delarue. Forward–backward stochastic differential equations and controlled Mck-
ean–Vlasov dynamics. Ann. Probab., 43(5):2647–2700, 2015.
R. Carmona and F. Delarue. Probabilistic theory of mean field games with applications. I, volume 83 of
Probability Theory and Stochastic Modelling. Springer, Cham, 2018. Mean field FBSDEs, control, and
games.
R.CarmonaandM.Laur`ere.Convergenceanalysisofmachinelearningalgorithmsforthenumericalsolution
of mean field control and games: II – the finite horizon case. Ann. Appl. Probab., 32(6):4065–4105, 2022.
R. Carmona and M. Lauri`ere. Deep learning for mean field games and mean field control with applications
to finance. arXiv:2107.04568, 2021.
J.-F. Chassagneux, L. Szpruch, and Alvin Tse. Weak quantitative propagation of chaos via differential
calculus on the space of measures. Ann. Appl. Probab., 32(3):1929–1969, 2022.
F.Chen,Y.Lin,Z.Ren,andS.Wang.Uniform-in-timepropagationofchaosforkineticmeanfieldLangevin
dynamics. Electron. J. Probab., 29(17):1–43, 2024.
F. Chen, Z. Ren, and S. Wang. Uniform-in-time propagation of chaos for mean field Langevin dynamics.
arXiv:2212.03050v3, 2022.
R. T. Q. Chen, Y. Rubanova, J. Bettencourt, and D. K. Duvenaud. Neural ordinary differential equations.
32nd Conference on Neural Information Processing Systems (NeurIPS 2018), 2018.
L. Chizat and F. Bach. On the global convergence of gradient descent for over-parameterized models using
optimal transport. 32nd Conference on Neural Information Processing Systems (NeurIPS 2018), 2018.
M. Coghi and F. Flandoli. Propagation of chaos for interacting particles subject to environmental noise.
Ann. Appl. Probab., 26(3):1407–1442, 2016.
C. Cuchiero, M. Larsson, and J. Teichmann. Deep neural networks, generic universal interpolation, and
controlled odes. SIAM J. Math. Data Sci., 2:901–919, 2020.
43S.Daudin,F.Delarue,andJ.Jackson.Ontheoptimalratefortheconvergenceprobleminmeanfieldcontrol.
arXiv:2305.08423v1, 2023.
M. F. Djete. Extended mean field control problem: a propagation of chaos result. Electron. J. Probab.,
27:1–53, 2022.
W. E. A proposal on machine learning via dynamical systems. Commun. Math. Stat., 5(1):1–11, 2017.
W.EandJ.Han.Deeplearningapproximationforstochasticcontrolproblems.NIPS 2016, Deep Reinforce-
ment Learning Workshop, 2016.
W.E,J.Han,andA.Jentzen.Deeplearning-basednumericalmethodsforhigh-dimensionalparabolicpartial
differential equations and backward stochastic differential equations. Commun. Math. Stat, 5:349–380,
2017.
W. E, J. Han, and Q. Li. A mean–field optimal control formulation of deep leanring. Res. Math. Sci., 6(10),
2019.
R. Elie, J. P´erolat, M. Lauri`ere, M. Geist, and O. Pietquin. On the convergence of model free learning in
mean field. The Thirty-Fourth AAAI Conference on Artificial Intelligence, 2020.
W. H. Fleming and H. M. Soner. Controlled Markov Processes and Viscosity Solutions. Springer New York,
NY, 2006.
W. Gangbo, S. Mayorga, and A. S´wie.ch. Finite dimensional approximation of Hamilton–Jacobi–Bellman
equations in spaces of probability measures. SIAM J. Math. Anal., 53(2):1320–1356, 2021.
W. Gangbo and A. R. M´esz´aros. Global well-posedness of master equations for deterministic displacement
convex potential mean field games. Comm. Pure Appl. Math., 75:2685–2801, 2022.
W. Gangbo, A.R. M´esz´aros, C. Mou, and J. Zhang. Mean field games master equations with nonseparable
Hamiltonians and displacement monotonicity. Ann. Probab., 50(6):2178–2217, 2022.
M.Germain,H.Pham,andX.Warin.RateofconvergenceforparticleapproximationofpdesinWasserstein
space. J. Appl. Probab., 59(4):992–1008, 2022.
E.HaberandL.Ruthotto.Stablearchitecturesfordeepneuralnetworks.Inverse Problems,5(1):1–11,2017.
P. Henry-Labord`ere. Deep primal-dual algorithm for bsdes: Applications of machine learning to cva and im.
SSRN: 3071506, 2017.
C. Hur´e, H. Pham, A. Bachouch, and N. Langren´e. Deep neural networks algorithms for stochastic control
problems on finite horizon: Convergence analysis. SIAM J. Numer. Anal., 59(1):525–557, 2021.
C. Hur´e, H. Pham, A. Bachouch, and N. Langren´e. Deep neural networks algorithms for stochastic control
problems on finite horizon: Numerical applications. Methodol. Comput. Appl. Probab., 24:143–178, 2022.
S.IoffeandC.Szegedy.Batchnormalization: acceleratingdeepnetworktrainingbyreducinginternalcovari-
ate shift. ICML’15 Proceedings of the 32nd International Conference on Machine Learning, 37:448–456,
2015.
J-F. Jabir, D. Siska, and L. Szpruch. Mean-field neural ODEs via relaxed optimal control.
arXiv:1912.05475v3, 2019.
S. Kou, X. Peng, and X. Xu. EM algorithm and stochastic control in economics. SSRN: 2865124, 2016.
N. V. Krylov. Controlled Diffusion Processes. Springer Berlin, Heidelberg, 1980.
D.Lacker.LimittheoryforcontrolledMcKean–Vlasovdynamics.SIAMJ.ControlOptim.,55(3):1641–1672,
2017.
44M. Li, C. Mou, Z. Wu, and C. Zhou. Linear-quadratic mean field control with non-convex data.
arXiv:2311.18292v1, 2023.
M.Li,C.Mou,Z.Wu,andC.Zhou.Linear-quadraticmeanfieldgamesofcontrolswithnon-monotonedata.
Trans. Amer. Math. Soc., 376(6):4105–4143, 2023.
S. Mayorga and A. S´wie.ch. Finite dimensional approximations of Hamilton–Jacobi–Bellman equations for
stochastic particle systems with common noise. SIAM J. Control Optim., 61(2):820–851, 2023.
S. Mei, A. Montanari, and P. Nguyen. A mean field view of the landscape of two–layer neural networks.
PNAS, 115(33):E7665–E7671, 2018.
C. Mou and J. Zhang. Mean field games of controls: Propagation of monotonicities. Probab. Uncertain.
Quant. Risk, 7(3):247–274, 2022.
H. Pham and X. Wei. Dynamic programming for optimal control of stochastic Mckean–Vlasov dynamics.
SIAM J. Control Optim., 55(2):1069–1101, 2017.
L. Ruthotto, S. J. Osher, W. Li, L. Nurbekyan, and S. W. Fung. A machine learning framework for solving
high-dimensional mean field game and mean field control problems. PNAS, 117(17):9183–9193, 2020.
M. Sion. On general minimax theorems. Pac. J. Math., 8:171–176, 1958.
H. L. Smith. Monotone Dynamical Systems: An Introduction to Competitive and Cooperative Systems, vol-
ume 41 of AMS Math. Surveys and Monographs. Amer. Math. Soc., Providence RI, 1995.
L. Tangpi. A probabilistic approach to vanishing viscosity for pdes on the Wasserstein space. Indiana Univ.
Math. J., to appear, 2022.
L. Tangpi and J. Jackson. Quantitative convergence for displacement monotone mean field games with
controlled volatility. Math. Oper. Res., to appear, 2023.
M. Thorpe and Y. van Gennip. Deep limit of residual neural networks. Res. Math. Sci., 10(6), 2023.
C.WuandJ.Zhang.ViscositysolutionstoparabolicmasterequationsandMcKean-Vlasovsdeswithclosed-
loop controls. Ann. Appl. Probab., 30(2):936–986, 2020.
J.YongandX.Y.Zhou.Stochastic Controls: Hamiltonian Systems and HJB Equations.SpringerNewYork,
NY, 1999.
45