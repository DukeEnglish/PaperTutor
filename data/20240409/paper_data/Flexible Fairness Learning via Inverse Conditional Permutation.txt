Flexible Fairness Learning via Inverse Conditional
Permutation
Yuheng Lai , Leying Guan∗
Abstract
Equalized odds, as a popular notion of algorithmic fairness, aims to
ensure that sensitive variables, such as race and gender, do not unfairly
influence the algorithm prediction when conditioning on the true out-
come. Despiterapidadvancements,mostofthecurrentresearchfocuseson
the violation of equalized odds caused by one sensitive attribute, leaving
the challenge of simultaneously accounting for multiple attributes under-
addressed. Weaddressthisgapbyintroducingafairnesslearningapproach
that integrates adversarial learning with a novel inverse conditional per-
mutation. Thisapproacheffectivelyandflexiblyhandlesmultiplesensitive
attributes, potentially of mixed data types. The efficacy and flexibility
of our method are demonstrated through both simulation studies and
empirical analysis of real-world datasets.
1 Introduction
Machinelearningmodelshavebecomeimportanttoolsforaidingdecision-making
in various applications. One of the challenges in applying machine learning is
ensuringthatthemodelsarefair,i.e.,theydonotdiscriminateagainstminorities
or other protected groups (Mehrabi et al., 2021). Several fairness concepts have
been developed in the literature to address different practical needs (Mehrabi
et al., 2021; Castelnovo et al., 2022). In this work, we consider the equalized
odds criterion (Hardt et al., 2016), defined as
Yˆ ⊥⊥A|Y. (1)
Here, Y is the response variable, A is the sensitive attribute(s) that we care to
protect(e.g. gender/race/income),andYˆ isthepredictiongivenbyanymodel.
Notice that, when dropping the conditional term, (1) becomes unconditional
independence as Yˆ ⊥⊥A to accommodate the need of demographic parity. The
requirements for a learned model to satisfy certain independence relations are
not limited to the realm of fairness: in this broader context, statisticians have
long been working on robust inference techniques based on the concept of a
∗Dept. ofBiostatistics,YaleUniversity,leying.guan@yale.edu
1
4202
rpA
8
]LM.tats[
1v87650.4042:viXrapivot – a quantity whose distribution is invariant with respect to the nuisance
parameters (see, e.g., Keener (2010)).
Despite the exciting progress, most existing algorithms aiming for equalized
odds can only handle one protected attribute. However, in fields including
clinical research, there is a growing need to mitigate biases related to multiple
sensitiveattributes(Yangetal.,2022). Ithasalsobeenpointedoutthatfairness
gerrymandering can occur when algorithmic decision-making considers only a
single sensitive attribute at a time (Kearns et al., 2018). Additionally, the
equalized-odds problem in the context of continuous sensitive attributes is much
less explored.
Here, we alleviate these two limitations by proposing a versatile equalized
odds training scheme, FairICP, as illustrated by Figure 1: Building on the
sensitive attribute resampling framework (Romano et al., 2020), we generate
A˜ using a novel inverse conditional permutation (ICP) strategy, conditional
permutationsofAgivenY,andconstructamorefairmodelthroughregularizing
the distribution of (Yˆ,A,Y) toward the distribution of (Yˆ,A˜,Y) (see Figure 1).
Our contributions are summarized as below.
• Weproposeanovelinverseconditionalpermutation(ICP)strategytogenerate
A˜, conditional permutations of A, without estimating the multi-dimensional
conditional density of A|Y.
• We show theoretically that the equalized odds condition holds asymptotically
for (Yˆ,A˜,Y) when the A˜ is generated according to ICP.
• We propose examining the fairness level with a recently developed non-
parametric conditional dependence measure.
• We demonstrate experimentally that FairICP enjoys improved efficacy and
flexibility.
Related work Existing fairness concepts can be divided into different cat-
egories, including statistical/group fairness (Hardt et al., 2016; Zafar et al.,
2017),whichaimstoensuresimilarpredictionsacrossdifferentgroups;individual
fairness (Dwork et al., 2012), which targets similar predictions for similar indi-
viduals; and causality-based fairness (Kusner et al., 2017), which tries to reveal
causal relationships. More comprehensive discussions can be found in (Mehrabi
et al., 2021; Castelnovo et al., 2022). Prominent statistical fairness measures
include demographic parity (Zafar et al., 2019), equal opportunity (Hardt et al.,
2016), and equalized odds (Hardt et al., 2016), which can all be articulated
as (conditional) independence relations from a statistical perspective. Given
the fairness concept, the associated procedures can be generally categorized
into three types: (1) pre-processing, (2) post-processing, and (3) in-processing.
Pre-processing aims to correct potentially biased data before any model fitting
procedures (Zemel et al., 2013; Feldman et al., 2015), while post-processing
modifies the classifier’s output at the test phase, leaving the model unchanged
(Hardt et al., 2016; Kim et al., 2018; Hebert-Johnson et al., 2018).
2Figure 1: Illustration of the FairICP framework. A, X, and Y denote the sensitive
attributes, other features, and labels.
FairICP is an in-processing method that encourages equalized-odds fairness
for multiple complex sensitive attributes during model training. Several in-
processing methods have been previously introduced to address the violation
of equalized odds. For example, Agarwal et al. (2018) describes a procedure
for handling categorical sensitive attributes for binary classification. Mary et al.
(2019) trains a model that penalizes the violation of equalized-odds, measured
by the Hirschfeld-Gebelein-R´enyi (HGR) Maximum Correlation Coefficient, and
is designed to reduce equalized-odds violations in the presence of one sensitive
attribute, whether categorical or continuous. Closely related to FairICP, another
line of in-processing algorithms encourages fairness using an adversarial loss
designed for different fairness metrics (Zhang et al., 2018). Particularly, Romano
etal.(2020)proposesanoveladversariallearninglossthatutilizestheresampled
syntheticvariableA˜fromtheconditionaldistributionofapotentiallycontinuous
A conditional on Y. Although the joint consideration of multiple sensitive
attributes has been explored for demographic parity under this framework
(Creager et al., 2019), jointly modeling multiple sensitive attributes, especially
continuous ones, remains an unresolved challenge. This challenge is largely due
to the difficulty of estimating the conditional density of A|Y. Our approach
shares similar loss designs with that of Romano et al. (2020) but employs a
novelpermutationtechniquecapableofhandlingmultipleandcomplexprotected
variables.
32 Method
We propose a general adversarial learning procedure to obtain models with
improved equalized odds guarantee through utilizing a novel Inverse Conditional
Permutation (ICP). The proposed procedure FairICP enables efficient fairness
learning with multi-dimensional sensitive attributes with either categorical or
continuous response Y. Before describing our proposal, we first define some
notationsusedthroughoutthispaper. Wewillalsoreviewtheframeworkofmodel
training with equalized odds penalty based on sensitive attribute re-sampling
Romanoetal.(2020)andthechallengeinapplyingsensitiveattributere-sampling
and existing methods for multidimensional attributes, which motivates our
proposal.
Let (X ,A ,Y ) for i = 1,...,n be i.i.d. generated triples of (feature,
i i i tr
sensitive attribute, response). Let f (.) be a prediction function with model
θf
parameter θ . Although f (.) can be any prediction that is differentiable in
f θf
θ , we will consider f (.) as the neural network throughout this work. Let
f θf
Yˆ =f (X) be the prediction for Y given X. For a regression problem, Yˆ is the
θf
predicted value of the continuous response Y; for a classification problem, the
last layer of f (.) is a softmax layer and Yˆ is the predicted probability vector
θf
for being in each class. We also denote X=(X ,...,X ),A=(A ,...,A ),
1 ntr 1 ntr
Y =(Y ,...,Y ) and Yˆ =(Yˆ ,...,Yˆ ).
1 ntr 1 ntr
2.1 Fairness-learning via sensitive attribute re-sampling
We first present the framework (Romano et al., 2020) denoted by Fair Dummies
Learning (FDL) to achieve equalized odds for one sensitive attribute. Our
terminology will differ somewhat from the terminology used in this reference, to
help us introduce the new perspectives and frameworks in this paper later on.
To evaluate the potential violation of equalized odds (1) in prediction Yˆ,
FDL construct a resampled version of the original sensitive attribute as A˜ to
be a contrast and sample A˜ according to A˜ ∼Qntr(·|Y), where Qntr(·|Y):=
(cid:81)
Q(·|Y ), and Q(·|y) denotes the conditional distribution of A given
1≤i≤ntr i
Y = y. Since we generate A˜ without looking at Yˆ, the following equalized
odds property holds: Yˆ ⊥⊥ A˜ | Y . Hence, we can measure the degree of
i i i
violation to the equalized odds condition by measuring the discrepancy between
the distribution of (Yˆ,A,Y) and the distribution of (Yˆ,A˜,Y). Following this
intuition, FDL utilizes GAN (Goodfellow et al., 2014), to iteratively learn how
to separate the two distributions and optimize a fairness-regularized prediction
loss. More specifically, define
L (θ )=E (cid:2) −logp (Y |X)(cid:3) (2)
f f XY θf
L (θ ,θ )=E [−logD (Yˆ,A,Y)]+E [−log(1−D (Yˆ,A˜,Y))] (3)
d f d YˆAY θd YˆA˜Y θd
V (θ ,θ )=(1−µ)L (θ )−µL (θ ,θ ) (4)
µ f d f f d f d
as the expected negative log-likelihood loss, the discriminator loss, and value
function respectively, where D (.) is the classifier which separates (Yˆ,A,Y)
θd
4and (Yˆ,A˜,Y), and µ∈[0,1] is a tuning parameter that controls the prediction-
fairness trade-off. Then, FDL learns θ ,θ by finding the minimax solution
f d
θˆ ,θˆ =argminmaxV (θ ,θ ). (5)
f d µ f d
θf θd
FDL generates A˜ through Conditional Randomization (CR) (Cand`es et al.,
2018), which is done by re-sampling it from its (estimated) conditional distribu-
tion given other variables that we want to control for. However, the effectiveness
of conditional randomization requires estimation of Q(A|Y), which is challeng-
ing when A is multi-dimensional (Scott, 1991). This challenge is not unique to
FDL and needs to be addressed for other non-resampling-based approaches such
as Holdout Randomization Test (HRT) (Tansey et al., 2022) as well. In addition,
the sensitive attributes A can also potentially be both discrete and continuous,
which adds another layer of the challenge of estimating Q(A|Y). An approach
allows A to have flexible types and scales well with the dimension of A to help
the promotion of fairness learning in many social and medical applications.
2.2 Fairness learning via ICP
To circumvent the challenge in learning the conditional density of A given Y,
we pivot to estimate Y given A and leverage Conditional Permutation (CP)
(Berrett et al., 2020) to generate a permuted version of A˜ which also has the
property of equalized odds (1) asymptotically.
CP in fairness learning. To begin with, we first introduce the vanilla CP
strategy to generate permutation copies in Berrett et al. (2020) in our setting.
LetS denotethesetofpermutationsontheindices{1,...,n}. Givenanyvec-
n
(cid:0) (cid:1)
torx=(x ,...,x )andanypermutationπ ∈S , definex = x ,...,x
1 n n π π(1) π(n)
as permuted version of x with its entries reordered according to the permutation
π. InsteadofdrawingapermutationΠuniformlyatrandom,CPassignsunequal
sampling probability to permutations based on the conditional probability of
observing A given Y:
Π
qn(A |Y)
P{Π=π|A,Y}= (cid:80) π′∈Snqnπ
(A π′
|Y). (6)
Here we let q(· | y) be the density of the distribution Q(· | y) (i.e., q(· |
y) is the conditional density of A given Y = y ). We write qn(· | Y) :=
q(·|Y )···q(·|Y ) to denote the product density. This leads to the synthetic
1 n
A˜ =A , which, intuitively, should have low dependence on Yˆ given Y, and can
Π
thus be utilized to encourage equalized odds as described in (1).
ICP circumvents density estimation of A|Y. Unfortunately, conducting
conditional permutation with multivariate A relies on conditional density esti-
mation of A given Y and does not alleviate the issue arising from multivariate
density estimation as we mentioned earlier. To circumvent this problem, we
5proposeasimpleICP (inverseconditionalpermutation)strategywhichisindirect
yet scales better with the dimensionality of A and can adapt easily to various
data types of A.
ICP begins with the observation that the distribution of (A ,Y) is identical
Π
as the distribution of (A,Y ). Hence, intuitively, instead of determining Π
Π−1
based on the conditional law of A given Y, we first consider the conditional
permutation of Y given A, which is one dimensional and can be estimated
conveniently using standard regression or generalized regression techniques
regardless of the complexity in A. We then generate Π by applying an inverse
operator to the distribution of these permutations. Specifically, we generate
A˜ =A with the following probabilities:
Π
qn(Y |A)
P{Π=π|A,Y}= (cid:80) qnπ (− Y1 |A). (7)
π′∈Sn π′−1
Indeed, this intuition helps us to A˜ which can be used to monitor the violation
of the equalized odds condition.
Theorem 2.1. For any n i.i.d observations (X,A,Y), let A˜ be generated by
the ICP sampling scheme (7). Let S(A) denote the unordered set of rows in A,
and let p be the dimension of A. We have
(1) If Yˆ ⊥⊥A|Y, then (Yˆ,A,Y)=d (Yˆ,A˜,Y).
(2) If (Yˆ,A,Y)=d (Yˆ,A˜,Y), then Yˆ ⊥⊥A|(Y and S(A)). Further, when
logp →0, the asymptotic equalized odds condition holds: for any constant vectors
n
t and t ,
1 2
(cid:104) (cid:105) (cid:104) (cid:105)
P Yˆ ≤t ,A≤t |Y −P Yˆ ≤t |Y P[A≤t |Y]n→ →∞ 0.
1 2 1 2
Remark 2.2. InFDL,theavailabilityofaccurateconditionaldensityA|Y enables
the equivalence between (Yˆ,A,Y)=d (Yˆ,A˜,Y) and Yˆ ⊥⊥ A | Y, ICP pays
an almost negligible price and offers a fast-rate asymptotic equivalence but
circumvents the density estimation of A|Y.
Motivated by this, we propose an adversarial learning procedure utilizing the
permutedsensitiveattributesA˜fromtheICPsamplingscheme(7),whichisbuilt
under the same formulation of the loss function shown previously in Section 2.1.
Let Lˆ (θ ) and Lˆ (θ ,θ ) be the empirical realizations of the losses L (θ ),
f f d f d f f
and L (θ ,θ ) defined in (2) and (3) respectively. Algorithm 1 presents the
d f d
details. We also detailed the sampling algorithm, Parallelized pairwise sampler,
in Appendix B for the sake of completeness, which is adapted from Berrett et al.
(2020).
Theorem 2.3. If there exists a minimax solution (θˆ ,θˆ ) for V (.,.) defined
f d µ
in (5) such that V (θˆ ,θˆ ) = (1−µ)H(Y | X)−µlog(4), where H(Y | X) =
µ f r
E [−logp(Y |X)] denotes the conditional entropy, then fˆ (·) is both an
XY θˆ
f
optimal and fair predictor, which simultaneously minimizes L (θ ) and satisfies
f f
equalized odds simultaneously.
6Algorithm 1 Fairness learning via ICP
Input: Data (X,A,Y)={(X ,A ,Y )}
i i i i∈Itr
Parameters: penalty weight µ, step size α, number of gradient steps N , and
g
iterations T.
Output: predictive model fˆ (·) and discriminator Dˆ (·).
θˆ
f
θˆ
d
1: for t=1,...,T do
2: Generate permuted copy A˜ by (7) (using the sampler described in
Appendix B)
3: Update the discriminator parameters θ d by repeating the following for
N gradient steps:
g
θ ←θ −α∇ Lˆ (θ ,θ ).
d d θd d f d
4: Update the predictive model parameters θ f by repeating the following
for N gradient steps:
g
(cid:104) (cid:105)
θ ←θ −α∇ (1−µ)Lˆ (θ )−µLˆ (θ ,θ ) .
f f θf f f d f d
5: end for
Output: Predictive model fˆ (·).
θˆ
f
In practice, the assumption of the existence of an optimal and fair predictor
(in terms of equalized odds) may not hold (Tang and Zhang, 2022). Setting µ
to a large value will preferably enforce f to satisfy equalized odds while setting
µ close to 0 will push f to be optimal: an increase in accuracy would often be
accompanied by a decrease in fairness and vice-versa.
2.3 Density Estimation
The estimation of conditional densities is a crucial part of both our method
and previous work (Romano et al., 2020; Mary et al., 2019). However, unlike
the previous work which requires the estimation of A | Y, our proposal looks
into the inverse relationship of Y | A. In practice, our proposed method can
easily leverage the state-of-the-art density estimator and is less disturbed by the
increased complexity in A, due to either dimension or data types.
In this manuscript, we applied Masked Autoregressive Flow (MAF) (Papa-
makarios et al., 2017) to estimate the conditional density of Y|A when Y is
continuous and A ,...,A can take arbitrary data types (discrete or continuous)
1 k
1. In classification scenario when Y ∈{0,1,...,L}, one can always fit a classifier
to model Y|A. To this end, FairICP is more feasible to handle more complex
sensitiveattributesandissuitableforbothregressionandclassificationtasks. To
provide more theoretical and empirical insights into how the quality of density
1InMAFpaper(Papamakariosetal.,2017),toestimatep(U |V),U isassumedtobecon-
tinuouswhileV cantakearbitraryform,butthere’snorequirementsaboutthedimensionality
ofU andV
7estimation affects CP and ICP, we have additional analysis in Appendix C.
3 Measuring the violation of equalized odds
To gain a reliable understanding of the potential violation of equalized odds
using the trained model fˆ, we carry out a disciplined evaluation utilizing an
untouched test set (Xte,Ate,Yte)={(X ,A ,Y )} and a recently proposed
i i i 1∈Ite
conditional independence measure.
3.1 Measure of Conditional Dependence
From a statistical point of view, we note that equalized odds (1) is exactly the
notion of conditional independence. Thus, measuring the violation of equalized
odds is equivalent to measuring conditional independence, and there have been
some works trying to bridge these two problems (Mary et al., 2019; Kamishima
et al., 2011; Romano et al., 2020).
In Mary et al. (2019), Hirschfeld-Gebelein-Renyi Maximum Correlation Co-
efficient (HGR) is chosen to measure the conditional dependence for equalized
odds and used as a penalty term to fit a fair model. However, the estimation of
HGR, which is based on kernel density estimation of A, becomes difficult when
A is multivariate. Here, we take advantage of recent developments in conditional
dependence measures and link them to our problem by introducing a flexible
measure proposed by Huang et al. (2022).
Definition 3.1. Kernel Partial Correlation (KPC) coefficient ρ2 ≡ρ2(U,V |W)
is defined as:
E(cid:2) MMD2(cid:0)
P ,P
(cid:1)(cid:3)
ρ2(U,V |W):= U|WV U|W ,
E(cid:2) MMD2(cid:0)
δ ,P
(cid:1)(cid:3)
U U|W
where (U,V,W)∼P and P is supported on a subset of some topological space
U×V×W,MMDisthemaximum mean discrepancy -adistancemetricbetween
two probability distributions depending on the characteristic kernel k(·,·) and
δ denotes the Dirac measure at U.
U
Under mild regularity conditions (see details in Huang et al. (2022)), ρ2
satisfies several good properties for any joint distribution of (U,V,W) in Defi-
nition 3.1: (1) ρ2 ∈ [0,1]; (2) ρ2 = 0 if and only if U ⊥⊥ V | W; (3) ρ2 = 1 if
and only if U is a measurable function of V given W. A consistent estimator
calculated by geometric graph-based methods
ρˆ2
(Section 3 in Huang et al.
(2022)) is also provided in R Package KPC.
With the aid of KPC, we can rigorously quantify the violation of equalized
odds by estimating ρ2(Yˆ,A|Y), where A can take arbitrary form and response
Y can be continuous (regression) or categorical (classification).
83.2 Hypothesis test for equalized odds
To this end, we provide a formal hypothesis test with a statistical guarantee
to detect any violation of equalized odds. Our hypothesis test once again uses
the permuted version of A˜ and implements a conditional independence test.
The idea is that we keep generating fake copies A˜ by (7), and by Theorem 2.1,
(Yˆ,A˜,Y) will have the same distribution as (Yˆ,A,Y) under the assumption
of equalized odds (1). Therefore, we can use any test statistic T to obtain
a valid hypothesis test since any test statistic T(Yˆ,A˜,Y) will also have the
same distribution as T(Yˆ,A,Y) under the assumption of equalized odds. The
procedure of our proposed hypothesis test is in Algorithm 2.
Proposition 3.2. Suppose the test observations (Xte,Ate,Yte)={(X ,Y ,A )
i i i
for 1≤i≤n } are i.i.d.. Yˆte ={fˆ(X ) for 1≤i≤n } for a learned model
te i te
fˆ(not necessarily trained as our proposed method). If H : Yˆte ⊥⊥ Ate | Yte,
0
i.e., equalized odds (1) holds, then the output p-value p of Algorithm 2 is valid,
v
satisfying P{p ≤α}≤α for any desired Type I error rate α∈[0,1] when H
v 0
is true.
Algorithm 2 ICP Test for Equalized Odds
Input: Data (Xte,Ate,Yte)={(Yˆ,A ,Y )}, 1≤i≤n
i i i test
Parameter: the number of synthetic copies K.
1: Compute the test statistic T on the test set: t∗ =T(Yˆte,Ate,Yte).
2: for k =1,...,K do
3: Generate permuted copy A˜ k of Ate by (7) (using the sampler described
in Appendix B)
4: Compute the test statistic T using fake copy on the test set: t(k) =
T(Yˆte,A˜ ,Yte).
k
5: end for
6: Compute the p-value:
(cid:32) K (cid:33)
1 (cid:88) (cid:104) (cid:105)
p = 1+ I t∗ ≥t(k)
v K+1
k=1
Output: A p-value p for the hypothesis that equalized odds (1) holds.
v
We note that a similar hypothesis test for equalized odds is proposed in
Romano et al. (2020) which is done by using a resampled version of A˜ and
choosing T in Algorithm 2 as described in Holdout Permutation Test (Tansey
et al., 2022), which is based on a predictor rˆ(A,Y) aiming to predict Yˆ and
is formulated as the empirical risk (e.g., mean squared error). However, such
T(Yˆ,A,Y) chosen in Tansey et al. (2022) itself cannot serve as an accurate
dependence measure as KPC does.
94 Experiments
In this section, we conduct numerical experiments to examine the effectiveness
of the proposed approach on both synthetic datasets and real datasets.1 All the
details are included in Appendix D.
4.1 Experiments on synthetic datasets
4.1.1 Synthetic data generation
In this section, we explore the performance of FairICP in simulations with a
continuous response Y ∈ R, and potentially multiple sensitive attributes are
differently involved by two mechanisms:
• Simulation 1: The response Y depends on two set of features X∗ ∈RK and
X′ ∈RK:
Y ∼N (cid:0) ΣK X∗+ΣK X′,σ2(cid:1) ,
k=1 k k=1 k
√
X∗ ∼N( wA ,(1−w)I ), (Sim1)
1:K 1:K K
X′ ∼N(0 ,I ),
1:K K K
• Simulation 2: The response Y depends on two features X∗ ∈R and X′ ∈R:
Y ∼N
(cid:0) X∗+X′,σ2(cid:1)
,
√
X∗ ∼N( wA ,1−w), (Sim2)
1
X′ ∼N(0,1),
• Y is influenced by multiple sensitive attributes A in the setting Sim1
1:K
and influenced by a sole sensitive attribute A in the setting Sim2. The
1
parameter w ∈[0,1] controls the dependence of the predictive feature on A,
and we consider w =0.9 as a high dependence scenario and w =0.6 as a low
dependence scenario in our experiments.
• In both settings, all sensitive attributes are generated independently from a
mixture of Gamma distributions to increase the difficulty of estimating A:
1 1
A ∼ Gamma(1,1)+ Gamma(1,10),
k 2 2
where k =1,...,K for setting Sim1 and k =1,...,K+1 for setting Sim2.
We compare the proposed method FairICP to FDL and an oracle version of
FairICP where Y|A is given as the true conditional density. These synthetic
experiments are where we can reliably evaluate the violation of the equalized
odds condition of different methods. We are interested in 1) investigating if
FairICP is more effective than FDL as the number of noisy attributes increases
1Thecodeisavailableathttps://github.com/yuhenglai/FairICP
10(increased K) by considering the easier problem of estimating the density of Y|A
rather than A|Y; and 2) evaluating if KPC is a good measure for conditional
dependence in the sense that it can capture the relative degree of violation of
equalized odds when applying different methods to the same data sets.
4.1.2 Results on synthetic datasets
We compare FairICP with P(Y |A) estimated by MAF (Papamakarios et al.,
2017)),FDLwithP(A|Y)estimatedbyMAF,andtheoracleversionofFairICP
withtruedensity. Forthemeasureoftheviolationofequalizedodds,wecalculate
the empirical KPC
=ρˆ2(Yˆ,A|Y)
as R Package KPC with Gaussian kernel and
default parameters (Huang et al., 2022). Apart from the KPC measure itself, we
also consider a second evaluation metric using a hypothesis test as outlined by
Algorithm 2 with T =KPC, where we consider the power of rejecting the null
hypothesisatlevel0.05asameasureofconditionaldependencewhenutilizingthe
underlying true conditional density. The greater
ρˆ2
or rejection power indicates
stronger conditional dependence between A and Yˆ given Y. Note that, in Sim2
only A influences the Y, so the test will be based on
ρˆ2(Yˆ,A
|Y) to exclude
1 1
the effects of noise (though the training is based on A for all methods to
1:K+1
demonstrate the performance under noise).
Figure 2 and 3 show the trade-off curves between prediction loss and degree
of fairness violations measured by KPC or its associated fairness testing power
by Algorithm 2 with T =KPC under settings Sim1 and Sim2 respectively, with
K ∈ {1,5,10} under the high-dependence scenario w = 0.9 (Results with low
dependence on A are shown Appendix D.1). We implemented f as linear model
and d as neural network, and all methods being compared are trained with
different penalty parameter µ∈[0,1] to show the trade-off. In both simulations,
the trade-off by Pareto fronts is based on 100 independent runs with a sample
size of 500 for the training set and 400 for the test set.
Figure 2 shows the results from the setting Sim1. Models from all three
methodsreducetoaplainlinearregressionwithoutregardtofairnesswhenµ=0,
resultinginlowpredictionlossbutasevereviolationofequalizedodds(evidenced
bylargeKPCandstatisticalpower); asµgoeslarger,modelspaymoreattention
to fairness (lower KPC and power) by sacrificing more prediction loss. FairICP
(proposed) performs very closely to the oracle model while outperforming FDL
as the dimension of K gets larger using both the KPC measure and the power
measure, which fits our expectation and follows from the increased difficulty
of estimating the conditional density of A|Y. FairICP shows a noticeable but
still less performance reduction compared to the oracle model measured by
KPC when the dimension of A is 10, which is already large compared to what
is examined in the current literature. Of note, this slight difference does not
show up when measured by the power, likely due to an information loss when
dichotomies the continuous KPC measure into the 0-1 decision given the p-value
cutoff.
Figure 3 shows the results from setting Sim2 and delivers a similar message
as Figure 2. The gaps between FairICP and FDL are wider compared to the
11results in Figure 2 as K increases, which echos less percent of information about
A needed for estimating Y|A in setting Sim2.
Figure2: Predictionlossandmetricsoffairnessinsimulationover100independentruns
under setting Sim1 and w=0.9. KPC estimate ρˆ2 and statistical power P{p-value<
0.05}areshowninleftcolumnandrightcolumnrespectively. Fromtoptobottomshows
the results on different choices of noisy sensitive attribute dimension of K. The X-axis
represents the metrics of fairness and the Y-axis is the prediction loss. Each graph
shows the proposed method, FDL, and oracle model with different hyperparameters µ.
Note that the power measure depends on how the permutation/sampling is
conductedinpractice,anditsreliabilityhingesonthecorrectnessofthesampling
scheme,andthus,theaccuracyofdensityestimation. Incontrast,thedirectKPC
(Kernel-basedPearsonCorrelation)measureisindependentofdensityestimation.
Therefore, we can trust the power evaluation in our synthetic experiments, as we
have utilized true conditional density estimation. The consistency between KPC
12measures and the power measures in our synthetic experiments suggests that
KPC is a reasonable and density-estimation-free measure in real applications for
comparing different learning methods
Results for when X has a weaker dependence on A with w = 0.6 are in
Appendix D.1, which demonstrated the same message as from Figure 3 and
Figure 2 with larger discrepancies across methods.
Figure3: Predictionlossandmetricsoffairnessinsimulationover100independentruns
under setting Sim2 and w=0.9. Conditional dependence measure ρˆ2 and statistical
power P{p-value<0.05} are shown in the left column and right column respectively.
From top to bottom shows the results on different choices of noisy sensitive attribute
dimension of K. The X-axis represents the metrics of fairness and the Y-axis is the
prediction loss. Each graph shows the proposed method, FDL, and oracle model with
different hyperparameters µ.
134.2 Real-data experiments
We consider real-world cases where we may need to protect more than one
sensitive attribute. For all the experiments, we split the data into a training set
(60%) and a test set (40%), and all the results shown are based on the test set.
4.2.1 Fair regression
In the Communities and Crime dataset 1, each record describes the aggregate
demographic properties of a different U.S. community; the data combines socio-
economicdatafromthe1990USCensus,lawenforcementdatafromthe1990US
LEMAS survey, and crime data from the 1995 FBI UCR. The total number of
recordsis1994,andthenumberoffeaturesis122. Ourtaskhereistopredictthe
number of violent crimes per population for US cities while protecting all race
informationtoavoidbiasingpolicechecksdependingontheethniccharacteristics
of the community. Specifically, we take three minority race information in this
dataset into account (African American, Hispanic, Asian) as sensitive attributes
instead of only one kind of race as done in the previous literature. We also
consider the case where A only includes one race (African American) as in
Romano et al. (2020); Mary et al. (2019) for better comparison. All the sensitive
attributesusedherearecontinuous,representingthepercentageofthepopulation
of certain races.
We compare our proposed methods with FDL (Romano et al., 2020) and
HGR (Mary et al., 2019)2. Note that we don’t include sensitive attributes as
features in our experiments as in Romano et al. (2020); Mary et al. (2019).
We consider neural networks as predictor f in all methods3, and we tune the
hyperparameters as in (Romano et al., 2020) (see details in Appendix D).
We present our results as Pareto front in Fig 4 to show the trade-off curves
of prediction and fairness given by our method and the state-of-the-art methods
where the fairness is measured by both KPC and the power from the statistical
test for fairness as outlined by Algorithm 2 with T chosen as KPC. We see that
both metrics give similar trends: although there are some small discrepancies
between using KPC and the fairness test, we observe that FairICP outperforms
FDL and HGR especially when both three sensitive attributes are considered.
Although the conditional density is now estimated and the fairness test might
suffer from it, KPC is a robust measure regardless of the sampling scheme for A˜.
1AvailableattheUCIrvineDataRepositoryhttp://archive.ics.uci.edu/ml/datasets/
Communities+and+Crime
2InMaryetal.2019,sincetheirimplementationdoesn’tdirectlyapplytomultiplesensitive
attributes,wesetthemeanofthreeHGRcoefficientofeachattributeaspenalty.
3Wealsoconsiderf asalinearmodelinAppendixD.2
14Figure 4: Prediction loss and violation of equalized odds
(ρˆ2
and power
P{p-value < 0.05} by ρˆ2) obtained by 3 different training methods in Crimes
data over 100 random splits. The mean value and one standard deviation of loss
are reported in the bar graph. Each graph shows the results of using different A:
1 dim = (African American) and 3 dim = (African American, Hispanic, Asian).
The Pareto front for each algorithm is obtained by varying the fairness trade-off
parameter µ.
4.2.2 Fair classification
We then turn to a binary classification case that has been well-studied and
considers two categorical sensitive attributes. The dataset we consider is ProP-
ublica’s COMPAS recidivism data (5278 examples) 1. The task is to predict
recidivism from someone’s criminal history, jail and prison time, demographics,
and COMPAS risk scores. We choose two binary protected attributes A: race
1Althoughit’swidelyusedinfairness-relatedliterature,recentlytherehavebeencritiques
aboutthelimitationsofthisdataset(Baoetal.,2022).
15(white vs. non-white) and sex. For this special task (binary classification against
multiple binary sensitive attributes), we compare FairICP to two baselines HGR
(Mary et al., 2019) and Exponentiated-gradient reduction (Agarwal et al., 2018),
with the later developed for this particular kind of task. We aim to use this
exampletodemonstratetheabilityofFairICPtohandlecategoricalobservations
and provide comparable performance with regard to the more tailored approach.
Figure 5: Prediction loss and violation of equalized odds (DEO (8), ρˆ2 and statistical
power P{p-value<0.05} by ρˆ2) obtained by 3 different training methods in COMPAS
dataover100randomsplits. TheParetofrontforeachalgorithmisobtainedbyvarying
the hyper-parameter controlling the level of fairness.
16In addition, apart from KPC and the corresponding fairness test, we also
consider another fairness metric based on confusion matrix (Hardt et al., 2016;
Cho et al., 2020) designed for such a binary classification task with categorical
sensitive attributes to quantify equalized odds:
DEO:= (cid:88) (cid:88) |Pr(Y˜ =1|Z =z,Y =y)−Pr(Y˜ =1|Y =y)|,
(8)
y∈{0,1}z∈Z
where Y˜ is the predicted class label.
Similar to the regression case, we train neural network models as classifiers
and discriminators 1 (see details in Appendix D).
Figure 5 shows that all three methods behave similarly overall in this clas-
sification example regarding their prediction-fairness trade-offs, with FairICP
closely matching the performance of the exponential-gradient reduction (referred
to as Reduction) using all three fairness evaluation metrics, and HGR slightly
worse than FairICP and Reduction when measured by DEO.
5 Discussion
We introduced a flexible fairness learning approach FairICP to address the
challenge of algorithmic fairness learning with complex sensitive attributes. Fair-
ICP combines adversarial learning with a novel inverse conditional permutation
strategy and offers a flexible and effective solution for handling sensitive at-
tributes that are potentially of mixed data types and multidimensional in nature.
Theoretical insights into the proposed method were provided, elucidating the
underpinning concepts and the rationale behind the integration of ICP with
adversarial learning. We further conducted numerical experiments on both
synthetic and real data to support our theoretical insights and demonstrated the
efficacyandflexibilityofourproposedmethod. Wealsonoticethepotentialcom-
putational challenges for complex datasets brought by the adversarial learning
framework (also mentioned in Zhang et al. (2018); Romano et al. (2020)), which
should be more carefully dealt with by implementing more efficient techniques,
and we view it as a future direction of improving FairICP.
Acknowledgment
This work is supported by NSF DMS-2310836.
References
Agarwal, A., A. Beygelzimer, M. Dud´ık, J. Langford, and H. Wallach (2018).
A reductions approach to fair classification. In International conference on
machine learning, pp. 60–69. PMLR.
1Wealsoconsiderf asalinearmodelinAppendixD.2
17Bao, M., A. Zhou, S. Zottola, B. Brubach, S. Desmarais, A. Horowitz, K. Lum,
andS.Venkatasubramanian(2022). It’scompaslicated: Themessyrelationship
between rai datasets and algorithmic fairness benchmarks.
Berrett, T. B., Y. Wang, R. F. Barber, and R. J. Samworth (2020). The
conditionalpermutationtestforindependencewhilecontrollingforconfounders.
Journal of the Royal Statistical Society Series B: Statistical Methodology 82(1),
175–197.
Cand`es, E., Y. Fan, L. Janson, and J. Lv (2018). Panning for gold: Model-X
knockoffs for high-dimensional controlled variable selection. Journal of the
Royal Statistical Society: Series B 80(3), 551–577.
Castelnovo, A., R. Crupi, G. Greco, D. Regoli, I. G. Penco, and A. C. Cosen-
tini (2022). A clarification of the nuances in the fairness metrics landscape.
Scientific Reports 12(1), 4209.
Cho, J., G. Hwang, and C. Suh (2020). A fair classifier using kernel density
estimation. Advances in neural information processing systems 33, 15088–
15099.
Creager, E., D. Madras, J.-H. Jacobsen, M. Weis, K. Swersky, T. Pitassi,
and R. Zemel (2019, 09–15 Jun). Flexibly fair representation learning by
disentanglement. In K. Chaudhuri and R. Salakhutdinov (Eds.), Proceedings
of the 36th International Conference on Machine Learning, Volume 97 of
Proceedings of Machine Learning Research, pp. 1436–1445. PMLR.
Dwork, C., M. Hardt, T. Pitassi, O. Reingold, and R. Zemel (2012). Fairness
through awareness. In Proceedings of the 3rd Innovations in Theoretical
Computer Science Conference, pp. 214––226.
Feldman, M., S. A. Friedler, J. Moeller, C. Scheidegger, and S. Venkatasubrama-
nian (2015). Certifying and removing disparate impact. In Proceedings of the
21st ACM SIGKDD International Conference on Knowledge Discovery and
Data Mining, pp. 259–268.
Goodfellow, I., J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
A. Courville, and Y. Bengio (2014). Generative adversarial nets. In Advances
in Neural Information Processing Systems 27, pp. 2672–2680.
Hardt, M., E.Price, andN.Srebro(2016). Equalityofopportunityinsupervised
learning. Advances in neural information processing systems 29.
Hebert-Johnson, U., M. Kim, O. Reingold, and G. Rothblum (2018, 10–15 Jul).
Multicalibration: Calibrationforthe(Computationally-identifiable)masses. In
J. Dy and A. Krause (Eds.), Proceedings of the 35th International Conference
on Machine Learning,Volume80ofProceedings of Machine Learning Research,
pp. 1939–1948. PMLR.
18Huang, Z., N. Deb, and B. Sen (2022). Kernel partial correlation coefficient
— a measure of conditional dependence. Journal of Machine Learning Re-
search 23(216), 1–58.
Kamishima, T., S. Akaho, and J. Sakuma (2011). Fairness-aware learning
through regularization approach. In 2011 IEEE 11th International Conference
on Data Mining Workshops, pp. 643–650. IEEE.
Kearns, M., S. Neel, A. Roth, and Z. S. Wu (2018, 10–15 Jul). Preventing
fairnessgerrymandering: Auditingandlearningforsubgroupfairness. InJ.Dy
and A. Krause (Eds.), Proceedings of the 35th International Conference on
Machine Learning, Volume 80 of Proceedings of Machine Learning Research,
pp. 2564–2572. PMLR.
Keener, R. W. (2010). Theoretical statistics: Topics for a core course. Springer.
Kim, M. P., A. Ghorbani, and J. Zou (2018). Multiaccuracy: Black-box post-
processing for fairness in classification.
Kusner, M.J., J.Loftus, C.Russell, andR.Silva(2017). Counterfactualfairness.
In Advances in Neural Information Processing Systems 30, pp. 4066–4076.
Mary, J., C. Calauzenes, and N. El Karoui (2019). Fairness-aware learning for
continuousattributesandtreatments. InInternational Conference on Machine
Learning, pp. 4382–4391.
Mehrabi, N., F. Morstatter, N. Saxena, K. Lerman, and A. Galstyan (2021).
A survey on bias and fairness in machine learning. ACM computing surveys
(CSUR) 54(6), 1–35.
Naaman, M. (2021). On the tight constant in the multivariate dvoretzky–kiefer–
wolfowitz inequality. Statistics & Probability Letters 173, 109088.
Papamakarios,G.,T.Pavlakou,andI.Murray(2017).Maskedautoregressiveflow
for density estimation. Advances in neural information processing systems 30.
Romano, Y., S. Bates, and E. Candes (2020). Achieving equalized odds by
resampling sensitive attributes. Advances in neural information processing
systems 33, 361–371.
Scott, D. W. (1991). Feasibility of multivariate density estimates.
Biometrika 78(1), 197–205.
Tang, Z. and K. Zhang (2022). Attainability and optimality: The equalized
odds fairness revisited. In Conference on Causal Learning and Reasoning, pp.
754–786. PMLR.
Tansey, W., V. Veitch, H. Zhang, R. Rabadan, and D. M. Blei (2022). The
holdout randomization test for feature selection in black box models. Journal
of Computational and Graphical Statistics 31(1), 151–162.
19Yang, J., A. A. S. Soltan, Y. Yang, and D. A. Clifton (2022). Algorithmic
fairness and bias mitigation for clinical machine learning: Insights from rapid
covid-19 diagnosis by adversarial learning. medRxiv.
Zafar, M. B., I. Valera, M. Gomez Rodriguez, and K. P. Gummadi (2017). Fair-
ness beyond disparate treatment & disparate impact: Learning classification
without disparate mistreatment. In Proceedings of the 26th International
Conference on World Wide Web, pp. 1171––1180.
Zafar, M. B., I. Valera, M. Gomez-Rodriguez, and K. P. Gummadi (2019).
Fairness constraints: A flexible approach for fair classification. Journal of
Machine Learning Research 20(75), 1–42.
Zemel, R., Y. Wu, K. Swersky, T. Pitassi, and C. Dwork (2013). Learning fair
representations. In International conference on machine learning, pp. 325–333.
PMLR.
Zhang, B. H., B. Lemoine, and M. Mitchell (2018). Mitigating unwanted biases
with adversarial learning. In Proceedings of the 2018 AAAI/ACM Conference
on AI, Ethics, and Society, pp. 335–340.
20A Proofs
Proof of Theorem 2.1. Let S(A) = {A ,...,A } denote the row set of the
1 n
observed n realizations of sensitive attributes (unordered and duplicates are
allowed). Let X, Yˆ :=f(X) and Y be the associated n feature, prediction, and
response observations.
1. Taks 1: Show that (Yˆ,A,Y) =d (Yˆ,A˜,Y) given conditional independence
Yˆ ⊥⊥A|Y.
Proof of Task 1. Recall that conditional on S(A) = S for some S =
{a ,...,a }, we have (Berrett et al., 2020):
1 n
qn (a |Y)
P{A=a |S(A)=S,Y}= A|Y π , (9)
π (cid:80) qn (a |Y)
π′∈Sn A|Y π′
where a = (a ,...,a ) is the stacked a values in S. On the other hand,
1 n
conditional on S(A˜)=S, by construction:
P(cid:110) A˜ =a |S(A)=S,Y(cid:111) = q Yn |A(Y π−1 |a) = q An |Y (a π |Y) (10)
π (cid:80) qn (Y |a) (cid:80) qn (a |Y)
π′ Y|A π′−1 π′ A|Y π
where the last equality utilizes the following fact,
qn (Y |a) qn (Y ,a) qn (Y,a ) qn (a |Y)
Y|A π−1 = Y,A π−1 = Y,A π = A|Y π .
(cid:80) qn (Y |a) (cid:80) qn (Y ,a) (cid:80) qn (Y,a ) (cid:80) qn (a |Y)
π′ Y|A π′−1 π′∈Sn Y,A π′−1 π′∈Sn Y,A π′ π′ A|Y π′
Consequently, under the conditional independence assumption, we can write
the joint distribution of (Yˆ,A,Y) as the following (yˆ, y are some stacked
observation values yˆ ,...,yˆ and y ,...,y for Yˆ and Y respectively):
1 n 1 n
P(Yˆ =yˆ,A=a ,Y =y)=P(Yˆ =yˆ,A=a |Y =y)·P(Y =y)
π π
( =b1)P(Yˆ =yˆ|Y =y)·P(A=a |Y =y)·P(Y =y)
π
=P(Yˆ =yˆ|Y =y)·E [P(A=a |Y =y,S(A)=S)]·P(Y =y)
S π
( =b2)P(Yˆ =yˆ |Y =y)·E (cid:104) P(A˜ =a |Y =y,S(A˜)=S)(cid:105) ·P(Y =y)
S π
(cid:104) (cid:105)
=E P(Yˆ =yˆ |Y =y)·P(A˜ =a |Y =y,S(A˜)=S) ·P(Y =y)
S π
( =b3)E (cid:104) P(Yˆ =yˆ,A˜ =a |Y =y,S(A˜)=S)(cid:105) ·P(Y =y)
S π
=P(Yˆ =yˆ,A˜ =a |Y =y)·P(Y =y)
π
=P(Yˆ =yˆ,A˜ =a ,Y =y). (11)
π
Here, step(b )hasusedeq.(9)andeq.(10), whichestablishestheequivalence
2
between the condition law of A and A˜; steps (b ) and (b ) relies on the
1 3
21conditional independence relationships A ⊥⊥ Yˆ|Y and A˜ ⊥⊥ Yˆ|Y. Hence,
conditional independence indicate the distributional equivalence
(Yˆ,A,Y)=d
(Yˆ,A˜,Y).
2. Taks2: ShowthefurtherconditionedconditionalindependenceYˆ ⊥⊥A|Y,S(A)
given (Yˆ,A,Y)=d (Yˆ,A˜,Y).
Proof of Task 2. When P(Yˆ =yˆ,A=a,Y =y)=P(Yˆ =yˆ,A˜ =a,Y =
y), we have
P(Yˆ =yˆ,A=a,Y =y)
P(Yˆ =yˆ,A=a|Y =y,S(A)=S)=
P(Y =y,S(A)=S)
P(Yˆ =yˆ,A˜ =a,Y =y)
=
P(Y =y,S(A)=S)
=P(Yˆ =yˆ,A˜ =a|Y =y,S(A)=S)
( =b1)P(Yˆ =yˆ |Y =y,S(A)=S)P(A˜ =a|Y =y,S(A)=S)
( =b2)P(Yˆ =yˆ|Y =y,S(A)=S)P(A=a|Y =y,S(A)=S)
(12)
Here, step (b ) holds by the construction of A˜, while step (b ) holds as a
1 2
result of eq. (9) and eq. (10).
3. Taks 3: Show the asymptotic equalized odds given (Yˆ,A,Y)=d (Yˆ,A˜,Y).
Proof of Task 3. We prove this statement utilizing the previous statement
and known multi-dimensional c.d.f (cumulative distribution function) estima-
tion bound (Naaman, 2021). Let t1 and t2 be constant vectors of the same
dimensions as Yˆ and A, and t3 be a constant vector of the same dimension
as Y. Construct augmented matrix t1, t2, t3 where t1 = t1, t2 = t2 and
1. 1.
t1 = ∞, t2 = ∞ for i = 2,...,n, and t3 = t the same for all i = 1,...,n.
i. i. i. 3
Let (Yˆ ,A ,Y ) be a from the same distribution as (Yˆ,A,Y). Then,
1 1 1
P(cid:16)
Yˆ ≤t1,A ≤t2|Y
=t3(cid:17)( =b1)P(cid:16)
Yˆ ≤t1,A ≤t2|Y
=t3(cid:17)
1 1 1 1 1
(cid:16) (cid:17)
P Yˆ ≤t1,A ≤t2,Y =t3
1 1
=
P(Y =t3)
(cid:16) (cid:17)
P Yˆ ≤t1,A≤t2,Y =t3
( =b2)
,
P(Y =t3)
wherestep(b )hasusedthefactthat(X ,A ,Y ),fori=1,...,nareindepen-
1 i i i
dentlygenerated,thus,conditioningonadditionalindependentY ,...,Y does
2 n
not change the probability; step (b ) holds because t1 and t2, for i=2,...,n,
2 i. i.
take infinite values and do not modify the event considered. Utilizing eq. (12),
22have further have
(cid:16) (cid:17)
P Yˆ ≤t1,A≤t2,Y =t3
P(Y =t3)
(cid:104) (cid:16) (cid:17)(cid:105)
=E P Yˆ ≤t1,A≤t2|Y =t3,S(A)=S
S|Y
(cid:104) (cid:16) (cid:17) (cid:16) (cid:17)(cid:105)
=E P Yˆ ≤t1|Y =t3,S(A˜)=S P A˜ ≤t2|Y =t3,S(A˜)=S
S|Y
( =b3)E (cid:104) P(cid:16) Yˆ ≤t1|Y =t3,S(A˜)=S(cid:17) P(cid:16) A˜ ≤t2|Y =t3,S(A˜)=S(cid:17)(cid:105)
S|Y 1 1
=E (cid:104) P(cid:16) Yˆ ≤t1|Y =t3,S(A)=S(cid:17) P(cid:0) A ≤t2|Y =t3,S(A)=S(cid:1)(cid:105)
S|Y 1 1 1
=P(cid:16) Yˆ ≤t1|Y =t3(cid:17) P(cid:0) A ≤t2|Y =t3(cid:1) +∆
1 1 1 1
where step (b ) has used again the fact that t1 = ∞ and t2 = ∞, for
3 i. i.
i=2,...,n, and ∆ is defined as
∆=E (cid:104) P(cid:16) Yˆ ≤t1|Y =t3,S(A)=S(cid:17)(cid:0)P(cid:0) A ≤t2|Y =t3,S(A)=S(cid:1) −P(cid:0) A ≤t2|Y =t3(cid:1)(cid:1)(cid:105) ,
S|Y 1 1 1 1
=E (cid:104) P(cid:16) Yˆ ≤t1|Y =t3,S(A)=S(cid:17)(cid:0)P(cid:0) A ≤t2|Y =t3,S(A)=S(cid:1) −P(cid:0) A ≤t2|Y =t3(cid:1)(cid:1)(cid:105)
S|Y 1 1 1 1
Our goal is equivalent to bound |∆|. Notice that since t3 = ... = t3 = t3
1. n.
are the same for all n samples, A , ..., A are exchangeable given S(A)=S.
1 n
Consequently, we obtain that
|∆|≤E (cid:2) |P(cid:0) A ≤t2|Y=t3,S(A)=S(cid:1) −P(cid:0) A ≤t2|Y =t3(cid:1) |(cid:3)
S|Y 1 1 1
=(cid:88) |P(cid:0) A ≤t2 |Y=t3,S(A)=S(cid:1)P(cid:0) S(A)=S|Y=t3(cid:1) −P(cid:0) A ≤t2|Y =t3(cid:1)P(cid:0) S(A)=S|Y=t3(cid:1) |
1 1 1
S
( =b4)(cid:88) |FˆS(t2)−F (t2)|P(cid:0) S(A)=S|Y=t3(cid:1) ,
y y
S
wherestep(b )hasusedtheequivalenceofA ,..,A ,whichleadstoP(cid:0) A ≤t2|Y =t3,S(A)=S(cid:1)
4 1 n 1
the S-induced empirical c.d.f evaluated at t2. Also, S is a set n samples A
generatedconditionalonY =t3,andFˆS(.)denotestheempiricalc.d.finduced
t3
by S and F (.) denote the theoretical c.d.f of A|Y =t3. From Lemma 4.1 in
t3
(Naaman, 2021), which generalizes Dvoretzky–Kiefer–Wolfowitz inequality to
multi-dimensional empirical c.d.f to we know
P(sup|FˆS(t2)−F (t2)|>δ)≤p(n+1)exp(−2nt2).
t3 t3
t2
Combine this equality with the bound for |∆|, we have
logp+logn
P(|∆|>C )→0,
n
for a sufficiently large C as n→∞. We thus reached our conclusion that
lim (cid:104) P(cid:16) Yˆ ≤t1,A ≤t2|Y =t3(cid:17) −P(cid:16) Yˆ ≤t1|Y =t3(cid:17) P(cid:0) A≤t2|Y =t3(cid:1)(cid:105) →0,
1
n→∞
23Proof of Theorem 2.3. For fixed f, the optimal discriminator D∗ is reached at
θˆ∗ =argminL (θ ,θ ),
d d f d
θd
p (·)
in which case, the discriminating classifier is D (·)=
YˆAY
(See
θ d∗ p YˆAY(·)+p YˆA˜Y(·)
Proposition 1 in (Goodfellow et al., 2014)), and L reduces to
d
(cid:0) (cid:1)
L (θ ,θ )=log(4)−2·JSD p ∥ p
d f d YˆAY YˆA˜Y
where JSD is the Jensen-Shannon divergence between the distributions of
(Yˆ,A,Y) and (Yˆ,A˜,Y). Plug this this into V (θ ,θ ), we reach the single-
µ f d
parameter form of the original objective:
V (θ )=minV (θ ,θ )=(1−µ)L (θ )+2µ·JSD(p ∥ p )−µlog(4)≥(1−µ)H(Y |X)−µlog(4),
µ f µ f d f f YˆAY YˆA˜Y
θd
where the equality holds at θ∗ = argmin V (θ ). In summary, the solution
θf f
value (1−µ)H(Y |X)−µlog(4) is achieved when:
• θˆ minimizes the negative log-likelihood of Y | X under f, which happens
f
when θˆ are the solutions of an optimal predictor f. In this case, L reduces
f f
to its minimum value H(Y |X)
• θˆ minimizes the Jensen-Shannon divergence JSD(cid:0) p ∥ p (cid:1) , Since the
f YˆAY YˆA˜Y
Jensen–Shannon divergence between two distributions is always non-negative,
and zero if and only if they are equal.
The second characterization is equivalent to the condition (YˆAY)=d (YˆA˜Y).
Note that this is a population level characterization with E corresponding to the
case where n→∞. As a result, by the asymptotic equalized odds statement in
Theorem 2.1, we have that fˆ also satisfies equalized odds.
θˆ
f
Proof of Proposition 3.2. The proposed test is a special case of the Conditional
Permutation Test (Berrett et al., 2020), so the proof is a direct result from
Theorem 2.1 in our paper and Theorem 1 in (Berrett et al., 2020) .
B Sampling Algorithm
To sample the permutation Π from the probabilities:
qn(Y |A)
P{Π=π |A,Y}= π−1 ,
(cid:80) qn(Y |A)
π′∈Sn π′−1
we use the Parallelized pairwise sampler for the CPT proposed in Berrett et al.
(2020), which is detailed as follows:
24Algorithm 3 Parallelized pairwise sampler for the ICP
Input: Data (A,Y), Initial permutation Π[0], integer S ≥1.
1: for s=1,...,S do
2: Sample uniformly without replacement from {1,...,n} to obtain disjoint
pairs
(cid:0) (cid:1)
(i ,j ),..., i ,j .
s,1 s,1 s,⌊n/2⌋ s,⌊n/2⌋
3: Draw independent Bernoulli variables B s,1,...,B s,⌊n/2⌋ with odds ratios
(cid:16) (cid:17) (cid:16) (cid:17)
P{B
s,k
=1}
=
q Y (Π[s−1](js,k)) |A is,k ·q Y (Π[s−1](is,k) |A js,k
.
P{B =0} (cid:16) (cid:17) (cid:16) (cid:17)
s,k q Y |A ·q Y |A
(Π[s−1](is,k)) is,k (Π[s−1](js,k)) js,k
Define Π[s] by swapping Π[s−1](i ) and Π[s−1](j ) for each k with
s,k s,k
B =1.
s,k
4: end for
Output: Permuted copy A˜ =A .
Π[S]−1
C Density Estimation in CP/ICP
The density estimation quality will depend on both the density estimation
algorithmandthedatadistribution. Whileadeepdiveintothisaspect,especially
from the theoretical aspects, is beyond the scope, some additional numerical or
theoretical insights will improve the understanding of the potential gain of ICP
over CP in practice.
1. An upper bound on the distortion of conditional permutation can be found in
theoriginalworkofBerrettetal.(2020). Letqˇ (.)/qˇ (.)beanestimation
Y|A A|Y
of the true conditional q (.)/q (.) of Y given A/A given Y respectively.
Y|A A|Y
Following the the robustness analysis arguments for CP, we can show that
an equivalent upper bound for ICP can be established, and will be smaller if
qˇ is closer to q compared to qˇ and q on an average sense:
Y|A Y|A A|Y A|Y
1(cid:90) (cid:90)
E [d (qˇ ,q )]= |q (y|a)−qˇ (y|a)|q (a)dyda
A TV Y|A Y|A 2 Y|A Y|A A
a y
1(cid:90) (cid:90)
= |q (y,a)−qˇ (y|a)q (a)|dyda
2 Y,A Y|A A
y y
1(cid:90) (cid:90)
< |q (y,a)−qˇ (a|y)q(y)|dyda=E [d (qˇ ,q )].
2 Y,A A|Y Y TV A|Y A|Y
a y
2. ToillustratethatICPcanprovideresamplingdistributionclosertothatofthe
oracle conditional permutation compared to CP, both utilizing off-the-shelf
toolsfordensityestimationwithvaryingdimensions,weconsiderthefollowing
examples:
25(1)let A=(A 1,...,A K0,A K0+1,...,A K0+K)×Θ1 2 for some randomly gener-
ated covariance matrix, and A be independently generated normal or mixed
j
Gamma samples.
√
(2)letY ∼N(
.5(cid:80)K0
A ,1.5)forK =1,5,K =0,5,10,20(noisysentitive
k=1 j 0
attributes) .
(3) We estimate q / q using (1) lasso regression/graphical lasso, where
Y|A A|Y
weestimatethelineardependenceofY onAandvarianceempiricallyforq
Y|A
and estimate q assuming joint normality of (Y,A). For K =1 and K =0,
A|Y 0
OLS was used for both estimations, and (2) MAF, which was default in our
paper. The sample size for density estimation and evaluating the conditional
permutation distribution are both 200.
Based on the upper bound, naturally, we want to look at the total variation
difference between permutations using ICP and CP with the estimated densi-
ties to that with the true density, which is explicitly known in this example
up to a normalization constant. However, the calculation of the actual total
variation distance is difficult due to the large permutation space. Hence,
we restrict the permutation space to swapping actions: we consider the TV
distance (log10 transformed) restricted to permutations π that swaps i and j
for i≠ j,i,j =1,...,n and the original order, and compare ICP and CP to
the oracle conditional permutations on such n(n+1) permutations only.
2
6 are results using (1) cross-validated lasso regression or graphical lasso , and
(2)MAFrespectively(repeated20timesforeachsetting). WeseethattheTV
distances between ICP and the oracle are smaller than the corresponding ones
for CP using both density estimation approaches. MAF is a default density
estimation approach for general purposes. By design, lasso regression/OLS
is favored over MAF for estimating q in this particular example. There
Y|A
may be better density estimation choices in other applications, but overall,
estimating Y|A can be simpler and allows us to utilize existing tools, e.g.,
those designed for supervised learning.
26Figure 6: Prediction loss and metrics of fairness in simulation over 100 independent
runs for Sim2 under low A-dependence. Conditional dependence measure ρˆ2 and
statistical power P{p-value<0.05} are shown in column 1 and 2 resp. From top to
bottom shows the results on different choices of the dimension of noisy attributes
A′. The X-axis represents the metrics of fairness and the Y-axis is the prediction
loss. Each graph shows the proposed me2th7od, FDL and oracle model with different
hyperparameters µ.D Experiments
In both simulation studies and real-data experiments, we implement the algo-
rithms with the hyperparameters chosen by the tuning procedure as in Romano
et al. (2020). In practice, we tune the hyperparameters only once using 10-fold
cross-validation on the entire data set and then treat the chosen set as fixed for
the rest of the experiments. Then we compare the performance metrics of the
different algorithms on 100 data splits that are different than the ones used to
tune the parameters. This same tuning scheme is used for all methods, ensuring
that the comparisons are meaningful.
D.1 Experiments on synthetic datasets
Forallthemodelsevaluated(FairICP,Oracle,FDL),wesetthehyperparameters
as follows:
• We set f as a linear model and use the Adam optimizer with a mini-batch
size in {16, 32, 64}, learning rate in {1e-4, 1e-3, 1e-2}, and the number of
epochs in {20, 40, 60, 80, 100, 120, 140, 160, 180, 200}. The discriminator is
implemented as a four-layer neural network with a hidden layer of size 64 and
ReLU non-linearities. We use the Adam optimizer, with a fixed learning rate
of 1e-4.
D.1.1 Low sensitive attribute dependence for Sim1
We report the results with A-dependence w =0.6 here:
28Figure 7: Prediction loss and metrics of fairness in simulation over 100 independent
runs for Sim1 under low A-dependence. Conditional dependence measure ρˆ2 and
statistical power P{p-value<0.05} are shown in column 1 and 2 resp. From top to
bottom shows the results on different choices of dimension of A. The X-axis represents
the metrics of fairness and the Y-axis is the prediction loss. Each graph shows the
proposed method, FDL and oracle model with different hyperparameters µ.
D.1.2 Low sensitive attribute dependence for Sim2
We report the results with A-dependence w =0.6 here:
29Figure 8: Prediction loss and metrics of fairness in simulation over 100 independent
runs for Sim2 under low A-dependence. Conditional dependence measure ρˆ2 and
statistical power P{p-value<0.05} are shown in column 1 and 2 resp. From top to
bottom shows the results on different choices of the dimension of noisy attributes
A′. The X-axis represents the metrics of fairness and the Y-axis is the prediction
loss. Each graph shows the proposed method, FDL and oracle model with different
hyperparameters µ.
D.2 Real-data experiments
D.2.1 Regression
For FairICP and FDL, the hyperparameters used for linear model and neural
network are as follows:
• Linear: we set f as a linear model and use the Adam optimizer with a mini-
batch size in {16, 32, 64}, learning rate in {1e-4, 1e-3, 1e-2}, and the number
30of epochs in {20, 40, 60, 80, 100}. The discriminator is implemented as a four-
layer neural network with a hidden layer of size 64 and ReLU non-linearities.
We use the Adam optimizer, with a fixed learning rate of 1e-4. The penalty
parameter µ is set as {0,0.3,0.5,0.7,0.8,0.9}.
• Neural network: we set f as a two-layer neural network with a 64-dimensional
hidden layer and ReLU activation function. The hyperparameters are the
same as the linear ones.
For HGR, the hyperparameters used for the linear model and neural network
are as follows:
• Linear: we set f as a linear model and use the Adam optimizer with a
mini-batch size in {16, 32, 64}, learning rate in {1e-4, 1e-3, 1e-2}, and the
number of epochs in {20, 40, 60, 80, 100}. The penalty parameter λ is set as
{0,0.25,0.5,0.75,1}.
• Neural network: we set f as a two-layer neural network with a 64-dimensional
hidden layer and ReLU activation function. The hyperparameters are the
same as the linear ones.
We report the results with f as a linear model here, which is similar to NN
version:
Figure 9: Prediction loss and violation of equalized odds (ρˆ2 and statistical power
P{p-value<0.05}byρˆ2)obtainedby3differenttrainingmethodsinCrimes dataover
100 random splits using linear models. The mean value and one standard deviation
of loss are reported in the bar graph. Each graph shows the results of using different
A. The Pareto front for each algorithm is obtained by varying the hyper-parameter
controlling the level of fairness.
31D.2.2 Classification
For FairICP, the hyperparameters used for linear model and neural network are
as follows:
• Linear: we set f as a linear model and use the Adam optimizer with a
mini-batch size in {64, 128, 256}, learning rate in {1e-4, 1e-3, 1e-2}, and
the number of epochs in {50, 100, 150, 200, 250, 300}. The discriminator is
implemented as a four-layer neural network with a hidden layer of size 64 and
ReLU non-linearities. We use the Adam optimizer, with a fixed learning rate
in {1e-4, 1e-3}. The penalty parameter µ is set as {0,0.3,0.5,0.7,0.8,0.9}.
• Neural network: we set f as a two-layer neural network with a 64-dimensional
hidden layer and ReLU activation function. The hyperparameters are the
same as the linear ones.
For HGR, the hyperparameters used for the linear model and neural network
are as follows:
• Linear: we set f as a linear model and use the Adam optimizer with a
mini-batch size in {64, 128, 256}, learning rate in {1e-4, 1e-3, 1e-2}, and the
number of epochs in {20, 40, 60, 80, 100}. The penalty parameter λ is set as
{0,0.075,0.125,0.25,0.5,1}.
• Neural network: we set f as a two-layer neural network with a 64-dimensional
hidden layer and ReLU activation function. The hyperparameters are the
same as the linear ones.
We report the results with f as a linear model here:
32Figure10: Predictionlossandviolationofequalizedodds(DEO (8),ρˆ2 andstatistical
power P{p-value<0.05} by ρˆ2) obtained by 3 different training methods in COMPAS
data over 100 random splits using linear models. The Pareto front for each algorithm
is obtained by varying the hyper-parameter controlling the level of fairness.
33