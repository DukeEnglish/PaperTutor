Learning 3D-Aware GANs from Unposed Images
with Template Feature Field
Xinya Chen1, Hanlei Guo1, Yanrui Bin2, Shangzhan Zhang1, Yuanbo Yang1,
Yue Wang1, Yujun Shen3, Yiyi Liao1⋆
1Zhejiang University 2The Hong Kong Polytechnic University 3Ant Group
{hust.xinyachen, guohl77701, binyanrui, shenyujun0302}@gmail.com,
{zhang3z, yybbb, ywang24, yiyi.liao}@zju.edu.cn
3DGP PoF3D Ours
Fig.1: We propose to perform on-the-fly pose estimation of training images with a
learned template feature field. Our method enables learning complete 3D shapes from
challenging datasets, including elephants, planes, and in-the-wild car datasets. While
other methods have half or inaccurate geometry.
Abstract. Collectingaccuratecameraposesoftrainingimageshasbeen
showntowellservethelearningof3D-awaregenerativeadversarialnet-
works(GANs)yetcanbequiteexpensiveinpractice.Thisworktargets
learning 3D-aware GANs from unposed images, for which we propose
to perform on-the-fly pose estimation of training images with a learned
template feature field (TeFF). Concretely, in addition to a generative
radiance field as in previous approaches, we ask the generator to also
learn a field from 2D semantic features while sharing the density from
theradiancefield.Suchaframeworkallowsustoacquireacanonical3D
feature template leveraging the dataset mean discovered by the genera-
tive model, and further efficiently estimate the pose parameters on real
data. Experimental results on various challenging datasets demonstrate
the superiority of our approach over state-of-the-art alternatives from
both the qualitative and the quantitative perspectives. Project page:
https://XDimlab.github.io/TeFF.
Keywords: 3d-aware generation, semantic features, pose estimation
⋆ Corresponding author.
4202
rpA
8
]VC.sc[
1v50750.4042:viXra2 X. Chen et al.
1 Introduction
3D assets are an important part of popular media formats such as video games,
movies, and computer graphics. Compared to designing 3D content manually
which is expensive and time-consuming, learning generative 3D models from
data is a promising alternative to reduce the design effort. It is particularly
attractive to learn 3D generative models from easy-to-obtained 2D images that
generalize to different categories.
Recent 3D-aware GANs have shown great success in this direction [4,5,31].
Most methods are designed for datasets with known camera pose distribution
definedwithrespecttoacanonicalobjectpose.Ithasbeenprovedthatknowing
theexactcameraposesofrealimagesfurtherimprovesthelearnedgeometry[3].
However, estimating camera poses of real images requires domain-specific 3D
knowledgeabouttheobjectcategory,whichisdifficulttoacquireformostin-the-
wild objects. To remove the assumption of known camera poses, there are some
recent attempts to use a generator to jointly learn the camera pose distribution
and the 3D contents. Despite achieving promising performance on faces and
synthetic objects [27,33], these methods struggle in object categories with more
complex pose distributions such as multi-peak distribution. This is mainly due
to the fact that the generated camera poses and object poses are entangled in
the 2D image space, leading to highly ill-posed solution space. For example,
the objects may be generated facing different directions with a fixed camera so
that the synthesized images can match the target distribution, as the result of
3DGP [35] and PoF3D [33] shown in Fig. 2. This leads to incomplete geometry
as shown in Fig. 1 as parts of the geometry are never observed.
In this work, we tackle this challenge by disentangling the estimation of
camera pose distribution from the training of the 3D-aware GANs. Our key
idea is to learn a 3D semantic template feature field along with the generative
model and define the object pose estimation as an auxiliary task taking the
template feature field as the canonical object space. This template feature field
is automatically learned from self-supervised 2D features, e.g., DINO features,
which are semantically aligned across instances of different appearances, thus
enabling pose estimation. Specifically, We augment the generative radiance field
with a semantic feature field while sharing the density. Then we acquire a
template feature field leveraging the dataset mean discovered by the generative
model. With the learned 3D feature template field and a 2D semantic feature of
a real image, we estimate the camera pose of the real image by solving a 3D-
2D pose estimation task. For robust and efficient pose estimation, we propose
to project the 3D template to the 2D image space using a discretized set of
cameraposesandfindtheonewiththebestmatching.Thisallowsforachieving
global optimum camera pose estimation when an ideal template is provided. To
address the slow matching process when the camera pose discretization space
is of high dimensional, we propose to discretize the azimuth and elevation only,
and leverage phase correlation to efficiently solve scale and in-plane rotation.
We demonstrate that our method can generate full geometry even in complexTeFF 3
3DGP PoF3D Ours
Fig.2:QualitativeComparisonof3DGP({cols.1−3}),PoF3D({cols.4−6}),Ours
({cols.7−9}) when rendering in the same camera pose. Note that objects generated
by 3DGP and PoF3D face different directions with the same camera pose while ours
face the same direction.
pose distribution, including real-world cars, planes, and in-the-wild animals like
elephants.
Our contributions can be summarized as follows, 1) We present a novel 3D-
aware generative model that jointly learns a semantic template feature field to
enable pose estimation of real-world images on the fly, thus enabling training
without known pose distribution. 2) We propose to efficiently solve the camera
pose estimation by incorporating phase correlation for estimation scale and in-
plane rotation. 3) Our model learns 3D-aware generative models on multiple
challenging datasets, including real-world cars, planes, and elephants.
2 Related Work
3D-Aware Image Synthesis: Recently, 3D-aware image synthesis has at-
tracted growing attention by lifting the generator to the 3D space. A key to
3D-aware image synthesis is the choice of the underlying 3D representation.
Early methods attempt to learn 3D voxel grid [13,26] and mesh [21], yet these
discretized representations limit the image fidelity. More recent methods exploit
neuralradiancefields[24]for3D-awareimagesynthesis [4,5,8–12,15,16,23,28,29,
31,32,34,39–42,48]. Albeit achieving photorealistic 3D-aware image synthesis,
these methods focus on datasets with known camera pose distribution, e.g.,
human and cat faces or synthetic car datasets.
3D-Aware Image Synthesis from Unposed images: There are a few
attempts to address the task of 3D-aware unposed image synthesis [27,33,35,
46]. CAMPARI [27] designs the camera generator as a residual function to
map a prior pose distribution to the target pose distribution. However, their
performance is highly sensitive to the given pose prior. PoF3D [33] frees the
requirements of 3D pose priors by learning a pose-free generator which map
a Gaussian distribution to the target distribution and learning a pose-aware
discriminator in an adversarial manner. These methods only consider camera
posesoftwodegreesoffreedom(DoF),e.g.,azimuthandelevationangles,based
ontheassumptionthatallcameraslieonasphere.Recently,3DGP [35]extends
theDoFfromtwotosixbyadditionallymodelingthefieldofviewofthecamera4 X. Chen et al.
Semantic Feature Training Pipeline
Camera Pose
Estimation
TeFF
Moving Camera Pose Real
Average
Feature Field
Volume
Rendering
Real Image Feature Radiance Field Fake
Fig.3:MethodOverview.Weaugmentthegenerativeradiancefieldwithasemantic
feature field, enabling estimating camera poses of real images on the fly to facilitate
the 3D-aware GAN training. Specifically, we map a randomly sampled noise vector to
a radiance field and a semantic feature field. By taking the mean shape of the feature
field, we obtain a 3D template feature field. This allows us to perform efficient 2D-3D
pose estimation to estimate camera poses of real images, which are in turn fed into
thegeneratortoperformvolumerendering.Thebluepartistheauxiliarytaskofpose
estimation we introduced.
and the look-at point. [46] model the translation and the rotation of the camera
with6DoF.Allthesemethodsadoptageneratortojointlylearnthecamerapose
distribution and the 3D contents. Despite performing well for data with simple
pose distribution, e.g., camera positions form a Gaussian distribution, these
methods are prone to fail when the pose distribution becomes more complex.
We instead learn a 3D semantic template feature field to solve the camera poses
ofeachrealimage.Ourmethodcangeneratefullgeometryevenincomplexpose
distribution.
Neural Feature Fields: Self-supervisedmodels,likeDINO[2],cansolvevideo
instancesegmentationandtrackingbycalculatingthesimilarityamongfeatures
in adjacent frames. [1] demonstrates that DINO features also work well on co-
segmentation and point correspondence of different instances. Reconstruction
methods have explored incorporating self-supervised features into neural fields
for scene decomposition, editing, and semantic understanding [18,19,37,43]. In
this work, we explore the semantic-aware characteristics of such self-supervised
features to enable camera pose estimation across different instances within the
same category.
3 Method
Fig. 4 gives an overview of our method. We augment a 3D-aware GAN using
our Texture Feature Fields (TeFF) to enable learning from real-world images
withoutcameraposedistributions.Thekeytoourapproachisthejointlearning
ofthegenerativeradiancefieldandasemanticfeaturefield,wherea3Dtemplate
feature field can be extracted to solve the camera poses of the real images on
the fly. In the following sections, we first introduce the offline semantic feature
extraction in Sec. 3.1. Next, we present our generator augmented with TeFF inTeFF 5
Sec. 3.2. We further elaborate on the camera pose estimation of the real images
based on the 2D feature maps and the 3D template feature field in Sec. 3.3,
followedbythediscriminatorandimplementationdetailsinSec.3.4andSec.4.1.
3.1 Semantic Feature Extraction
Inthispaper,werefertosemanticfeaturesasfeaturemapsthataresemantically
aligned across instances of different appearances and poses, as illustrated in
Fig.4(left).Forexample,thecarwheelssharethesamefeaturesacrossdifferent
instances. Existing methods have demonstrated that such semantic features
can be leveraged to establish correspondences between different instances [1,2],
potentially enabling their relative pose estimation. In this work, we propose
to extract semantic features from each real image to enable its camera pose
estimation.
Specifically, we extract semantic features F ∈ RH×W×F from real RGB
images I∈RH×W×3 based on DINO, as we observe that DINO generalizes well
andprovidesmeaningfulsemanticfeaturesforvariouscategories.Inpractice,we
extract foreground masks from the RGB images and feed the masked images to
DINO model to get DINO feature maps. Then we apply the same mask to this
feature maps. This allows us to focus on the semantic features of foreground
instances.
3.2 Generator with TeFF
GenerativeRadianceandFeatureFields: Webuildourgeneratorbasedon
EG3D [3] but with a key difference in that the model jointly generates radiance
fieldsandfeaturefields.Conditionedonanoisez∈RM sampledfromaGaussian
distribution,thegeneratorG generatesaradiancefieldandafeaturefieldthat
ψ
mapsa3DpointxtoacorrespondingRGBvaluec,featurevaluef,anddensity
value σ:
G :R3×RM →R3×RF ×R+ (x,z)(cid:55)→(c,f,σ) (1)
ψ
where F denotes the dimension of the feature vector. In practice, the generator
produces two tri-planes, one for the color and density and the other for the
feature.
The generative radiance field is rendered to a 2D image via the volume
rendering operation. Given an estimated camera pose ξ (discussed in Sec. 3.3),
thegeneratorisqueriedonH×W rayswithN samplingpointsoneachray.Let
{c ,f ,σ } denote the queried color, feature, and density values of one
i i i i=1,...,N
ray, the pixel color c and feature f at the corresponding ray is obtained via
r r
volume rendering:
π :(R3×R+)N →R3 {(c ,f ,σ )}(cid:55)→c ,f
i i i r r
N N i−1
(cid:88) (cid:88) (cid:89)
c = T α c f = T α f T = (1−α ) α =1−exp(−σ δ )
r i i i r i i i i j i i i
i=1 i=1 j=16 X. Chen et al.
MSE Inverse
Discretization Estimation Sampling
TeFF Pose
Distribution
Real Image Feature
Fig.4:CameraPoseEstimation.Weleveragethetemplatefeaturefieldtoestimate
camera poses of 2D real images. We discretize the azimuth θ and elevation ϕ angles
and render the feature field from these discretized camera poses. Then we use phase
correlation to estimate the scale and the in-plane rotation in the 2D image space and
warp each of the rendered templates based on the solution. We calculate the mean
squareerrorbetweenthewarpedrenderingandtherealfeatureandfurtherobtainthe
probability distribution function of the camera pose. Finally, we sample the camera
pose using inverse sampling.
whereT andα denotethetransmittanceandalphavalueatasamplepointx ,
i i i
δ denotes the distance interval between two adjacent sample points. Note that
i
the same density values are shared for rendering the color c and the feature
r
f . By compositing all ray values we obtain a color image ˆI ∈ RH×W×3 and a
r
feature map Fˆ ∈RH×W×F.
Following EG3D [3], we apply a super-resolution module to the rendered
RGB image for upsampling via neural rendering. Note that the super-sampling
module is not shown in Fig. 4 for brevity. We do not perform super-resolution
modules to the rendered feature maps as we observe that low-resolution feature
maps are sufficient for our purpose of pose estimation.
Template Feature Field: We propose to leverage the mean feature fields
automaticallydiscoveredbythegeneratortoenablethesubsequentcamerapose
estimation. This mean feature field can be considered as a template for the full
category. To obtain this template feature field, we first apply the exponential
moving average to the generator G during training to get G . Next, we feed
ψ ψ
the mean noise vector z to the moving averaged generator G to decode the
0 ψ
feature and density for the template.
Background Generator: In order to estimate camera poses focusing on the
objects, we disentangle the foreground and background generation. Following
VoxGRAF [32], we generate the background of the RGB image with a 2D GAN
for efficiency. Specifically, we use the StyleGAN2 generator [17] with reduced
channel size as modeling the background requires less capacity than generat-
ing the full image. We use the same latent code z of the foreground for the
background generator to allow for modeling correlations like lighting between
the two. The final RGB image is obtained by alpha compositing the rendered
foreground image and the 2D background.TeFF 7
3.3 Camera Pose Estimation
We leverage the 3D template feature field to estimate camera poses of 2D real
images, which can be formulated as a 2D-3D camera pose estimation task. This
is possible since the features are semantically aligned across different instances.
Thetaskof2D-3Dmatchingisnon-trivial.Manyexisting2D-3Dposematching
methods require establishing 2D-3D correspondences [30], followed by solving a
Perspective-n-Point(PnP)problemforposeestimation.However,inourprelim-
inarytrials,itisnon-trivialtoestablishcorrect2D-3Dcorrespondences(e.g.,the
left leg of an elephant in the 2D image might be matched to a right leg in the
template), thus the estimated poses are unreliable. To tackle this challenge, we
propose an efficient yet accurate way of solving the camera pose via grid search.
The core idea is to discretize the camera pose space and select the one that
rendersafeaturemapthatbestmatchestherealfeature.Wenowintroducehow
we define the camera model and how the grid search is efficiently implemented.
Camera Model: We assume our camera lies
on a sphere and looks at the sphere center, with
freedom degrees of azimuth, elevation, in-plane
rotation, and sphere radius. More formally, we
parameterize the camera pose ξ = (θ,ϕ,γ,r).
In-plane
Here, r denotes the radius of the sphere. θ ∈ Rotation 𝛾
Elevation ∅
[0,2π] and ϕ ∈ [0,π] denote the azimuth and Radius 𝑟
Azimuth 𝜃
elevationanglesofthecamera’stranslationinthe
spherical coordinate system. By looking at the
sphere center from the direction determined by
θ and ϕ and performing the in-plane rotation γ, Fig.5: Camera Model.
we can determine the camera’s rotation. Note that, r determines the scale of
objects in the image, we use scale to refer it in the following.
Azimuth and Elevation Discretization: With this 4 DoF camera model, a
naïvegridsearchcanbeimplementedbydiscretizingallfourvariables(θ,ϕ,γ,r)
to form a large number of camera poses, and render the feature field into a set
of feature maps, followed by comparing them with a feature map of the real
image and selecting the best-matching one. This exhaustive grid search allows
for finding the best-matching pose among the candidates, yet it is extremely
computationally expensive. Hence, we propose to combine grid search with the
efficient phase correlation approach [20], allowing for efficiently estimating the
camera pose with a high accuracy. Specifically, we discretize the azimuth θ and
elevation ϕ angles into N and N values, respectively. Next, we render the
θ ϕ
template feature fields from these discretized azimuth θ and elevation ϕ angles,
yielding a set of 2D template features {F
}Nθ×Nϕ.
k k=1
Scale and In-plane Rotation Estimation: GivenasemanticfeaturemapF
of a real image I, we may retrieve its best-matching template feature from {F }
k
andassignthecorrespondingposetoI.However,thisdoesnotmodelscalerand
in-plane rotation γ. Therefore, we further estimate the r and γ between the real
feature map F and each 2D template feature F . Considering the efficiency, we
k8 X. Chen et al.
employphasecorrelation(PC)[20],aclassicalfrequency-domain-basedapproach
that allows for estimating the scale and in-plane rotation between two images
efficiently, without relying on any explicit correspondence or initialization. This
is because PC estimates scale and in-plane rotation following the same idea as
grid search, yet the calculations are performed in the frequency domain for fast
computation. Based on the estimated scale r and in-plane rotation γ, we warp
each 2D template image F to get F˜ .
k k
Camera Pose Sampling: We calculate the mean square error (mse) between
each F˜ and the real semantic feature map F as:
k
e (F˜ ,F)=∥F˜ −F∥2 (2)
k k k 2
Ideally, we can assign the camera pose of F˜ corresponding to the lowest e
k k
to the real feature map F. However, the feature template may not exactly
matcheachrealimage,especiallyattheearlytrainingstage,occasionallyleading
to inaccurate pose estimation. Therefore, we form a probability distribution
function (PDF) over all possible N × N poses and sample from it during
θ ϕ
training.Specifically,wesampleacameraposeusinginversesamplingaccording
to the PDF of the camera pose. The pose probability p(k) is calculated by:
p(k)=softmax(−e (F˜ ,F)∗τ) (3)
k k
where τ is the temperature, which controls the sharpness of the probability
distributionfunction.Attheearlyoftraining,weuselowtemperatureT andthen
increase the temperature linearly. Since we discretize the camera pose space, we
furtheraddaslightGaussiannoisetothesampledpose.Thisallowsustosample
continuous camera pose locations from each discretized bin. During inference,
we select the camera pose with the maximum likelihood.
3.4 Discriminator and GAN Training
Discriminator: Inordertotrainourfeaturefield,weaddafeaturediscrimina-
torDF ontopoftheoriginalimagediscriminatorDI.DI isusedtodiscriminate
ζ ζ ψ
real RGB images and fake RGB images, whereas DF learns to discriminate the
ζ
semantic features. Following [3], we adopt the dual discriminator with camera
poseconditionforDI,i.e.,theinputtoDI isanupsampledlow-resolutionRGB
ζ ζ
image, a high-resolution RGB image, and the corresponding camera pose. The
input to DF is the low-resolution RGB image and the corresponding semantic
ζ
feature map, to encourage the learned feature field to match with the corre-
sponding radiance field. We stop the gradient backpropagation from DF into
ζ
the RGB values following [36].
GAN Training Given the latent code z, a real image I and its corresponding
semantic feature F sampled from the real-data distribution p , we train our
D
model using non-saturated GAN loss with R1 regularization [22].
L=E (cid:2) f(DI(G (z))) + f(DF(G (z)))(cid:3)
z∼N(0,1) ζ ψ ζ ψ
+ E (cid:104) f(−DI(I)) + λ∥∇DI(I)∥2 + f(−DF(F)) + λ∥∇DF(F)∥2(cid:105)
I,F∼pD ζ ζ ζ ζTeFF 9
Training Strategy: The pose estimation is done on the fly in the early stage.
We observe that once the model learns a reasonable template, we can use the
camera poses estimated from a fixed template for the remaining training. We
updatethetemplateonceevery16iterationsbefore3kiterationsandthenupdate
the template once epoch.
4 Experiments
Datasets: We evaluate on four datasets, including Shapenet Cars [3,6], Com-
pCars [44], SDIP Elephant [25], and LSUN Plane [47]. Shapenet Cars is a
synthetic dataset that contains car images with ground truth camera poses.
Note that the ground truth poses are only used for evaluation for all methods.
As the camera poses of the original dataset is roughly uniformly distributed, we
consider a more challenging scenario by building a sub-dataset where the pose
distribution has multiple peaks, with around 103K images. CompCars contains
136kunposedimagescapturingtheentirecarswithdifferentstyles.Theoriginal
dataset contains images with different aspect ratios. We preprocess the images
by center cropping, padding to the squared images with the same length, and
resizingthemto256×256.Weusethemasktosetthebackgroundblackandfilter
the data with bad mask estimation and extreme scale, leading to 110k images.
WeusetheSDIPElephantdatasetfollowing[35]andfiltertheelephantswiththe
half body, which contains around 20k unposed images. LSUN Plane is a dataset
that contains unposed images of different planes. We use MMDetection [7] to
detect the plane and filter the plane larger than 226×226 resolution and the
occluded plane, leading to 130k images. We rescale the plane to make the large
side equal to 226 and padding it to 256 resolution. In the experiments, we use
the resolution of 256×256 for SDIP Elephant, CompCars and LSUN Plane, and
128×128 for Shapenet Cars.
Baselines: We consider three baselines: EG3D [3], PoF3D [33] and 3DGP [35].
EG3Disastate-of-the-art3D-awareGANforposedimages,whereasPoF3Dand
3DGP both learn from unposed images while jointly learning the camera pose
distribution. For EG3D we uniformly sample pose with azimuth angle sampled
from 360 degrees and elevation from 85 degrees to 95 degrees. Note that the
original EG3D conditions the generator and discriminator on the camera poses.
We do not use this pose condition for EG3D due to the lack of image-pose pair
data. For 3DGP, we set the prior azimuth angle with a uniform distribution of
360 degrees and set the prior elevation angle with a uniform distribution from
85 degrees to 95 degrees.
Metrics: We use three metrics to evaluate the performance, including Frechet
Inception Distance (FID) [14], Depth Error [3], and Kullback-Leibler (KL) Di-
vergence. FID is measured between 50K generated images and all real images.
Note that, we evaluate the FID using two sets of camera poses. As the camera
pose distributions estimated by different methods are different, we first sample
images using the ground truth pose distribution for evaluation as “FID ”. This
gt10 X. Chen et al.
ensures a fair comparison of different methods. In cases where the ground truth
posedistributionisnotavailableonCompCars,SDIPElephantandLSUNPlane,
we render fake images from a uniform distribution with azimuth angle sampled
from 360 degrees and evaluate their fidelity as “FID ”. This is rationale as
360
those datasets contains images captured from 360 degree azimuth angles. We
further evaluate FID using the estimated camera poses, denoted as “FID ” to
est
reflect the image fidelity at the learned camera distributions. Depth Error is
used to assess the quality of geometry. Following [3], we use a monocular depth
estimation algorithm [45] to get the pseudo depth and use a mask estimation
algorithm[38]toobtaintheforegroundmask.Werandomlygenerate10kimages
and only evaluate the depth error of the foreground objects. We normalize
the depth by subtracting the mean depth of the current sample and dividing
the standard deviation depth of the dataset. Similar to FID we also evaluate
“Depth ”/“Depth ” and“Depth ”.Thestandarddeviationdepthusestheone
gt 360 est
calculated in the estimated camera pose setting since the depth estimation and
mask estimation may fail in the 360 camera pose setting. For pose distribution
estimation, we randomly sample 10k ground truth poses and estimated poses,
andthencalculatetheirKLDivergence.Weadoptthisdistributionlosssincethe
baseline methods do not provide an estimated camera pose for each real image
but learn a distribution.
4.1 Implementation Details
In our implementation, We extract the semantic features from DINO using real
images of 256×256 resolution and get the DINO features of 64×64 resolution.
We apply Principal Component Analysis to the feature maps and keep the first
three principal components of the original DINO features for training efficiency.
This means the feature channel F is set to 3.
The standard deviation of the noise is 1/6 of the discrete interval. We dis-
cretizetheazimuthθ andelevationϕanglesintoN andN values,respectively.
θ ϕ
The azimuth range is 360 degrees and N is 36 for all datasets. The elevation
θ
range is 180 degrees and N is 18 for the ShapeNet Cars dataset. For other
ϕ
datasets, the elevation range is from 85 to 95 degrees and N is 3 since the
ϕ
elevation variation is small.
4.2 Comparison to the Baselines
Qualitative Comparison: Fig. 6, Fig. 7, Fig. 8 shows the rendering com-
parison against the baselines on SDIP Elephant, CompCars, and LSUN Plane.
We render the image from 360 degrees at 60-degree intervals. The bottom right
of the image illustrates the corresponding geometry. EG3D [3], 3DGP [35], and
PoF3D [33] perform well in some specific views but fail to perform 360-degree
rendering with consistency due to the incomplete or inaccurate geometry. In-
stead,ourmethodcanperform360-degreerenderingwithgoodfidelity.Besides,
ourmethodallowsforlearningthecompletegeometrywithhighquality,whereasTeFF 11
Fig.6: Qualitative Comparison on SDIP Elephant. We show each sample from
360 viewing directions.
the baselines fail to reconstruct the geometry correctly or only reconstruct half
of the object.
QuantitativeComparison: Wereportthequantitativeevaluationofbaselines
and our method in Tab. 1 and Tab. 2. The result suggests that EG3D trained
with unknown camera pose distribution can achieve a low FID, yet results in
theworstgeometry.NotethatEG3DachievesthebestFID sincetheazimuth
360
angle is uniformly sampled from 360 degrees during training. Despite having
high fidelity for all surrounding viewpoints, EG3D does not recover the correct
geometry and thus yields bad Depth , e.g., as shown in Fig. 6. 3DGP and
360
PoF3D achieve the best FID when evaluated on their estimated camera poses,
whereas the performance drops significantly when evaluated using camera poses
uniformly sampled on CompCars, SDIP Elephant and LSUN Plane. The reason
isthat3DGPandPoF3Dlearnacollapsedcameraposedistribution,i.e.,itonly
coversasmallrangeoftheunderlyingtrueposedistribution.Ourmethodinstead
retainsasimilarperformancewhenevaluatedusingtheuniformdistribution,i.e.,
our FID and FID are close, thanks to the fact that our method learns to
360 est
recoverafullobjectinthe3Dspaceforreasonablerenderingfromallsurrounding
viewpoints. This is also verified by the fact that our method performs the best
D3GE
PGD3
D3FoP
sruO12 X. Chen et al.
Fig.7: Qualitative Comparison on CompCars. We show each sample from 360
viewing directions.
Multiple-PeakShapenetCars CompCars
Method Depth ↓Depth ↓FID ↓FID ↓ Depth ↓Depth ↓FID ↓FID ↓
gt est gt est 360 est 360 est
EG3D 0.61 0.61 7.25 11.89 0.95 0.95 7.06 7.06
3DGP 4.84 0.46 139.48 4.93 4.02 0.37 187.20 8.34
PoF3D 0.65 0.70 12.72 6.45 10.31 0.70 44.52 6.62
Ours 0.53 0.52 5.95 6.55 0.31 0.29 27.71 20.60
Table 1: Quantitative Comparison on Shapenet Cars and CompCars.
in terms of the Depth . Note that 3DGP achieves the best Depth since
360 est
it uses the depth maps as supervision during training, and we use the same
monocular depth estimation algorithm to provide depth supervision for 3DGP
and for evaluating its depth performance. Despite not relying on any depth
supervision,ourmethodevenachievescomparableDepth comparedto3DGP
est
on the car datasets in Tab. 1.
Pose Distribution Comparison: We use the
Method θ KL ↓ ϕ KL ↓
ShapeNet Cars with ground truth poses to evaluate
3DGP 40.4571 39.3625
the accuracy of the estimated camera pose distri-
PoF3D 4.4829 0.5495
bution. We discretize the azimuth angle into 24
Ours 0.0555 0.0696
intervalsandtheelevationangleinto12intervals,and
calculate the discrete probability distributions over
Table 3: Pose Distribution
these intervals. Fig. 9 demonstrate the comparison
Comparison.
between the GT pose distribution and the estimated
pose distribution of different methods regarding azimuth and elevation angles. It is
shown that the pose distribution of 3DGP is collapsed to a small range. PoF3D
covers a larger range but fails to capture the multi-modal distribution: the learned
distribution remains Gaussian, similar to its initialization. In contrast, our estimated
pose distribution is notably more accurate. This is also verified by the quantitative
D3GE
PGD3
D3FoP
sruOTeFF 13
Fig.8: Qualitative Comparison on LSUN Plane.Weshoweachsamplefrom360
viewing directions with a 60-degree interval.
SDIPElephant LSUNPlane
MethodDepth ↓Depth ↓FID ↓FID ↓Depth ↓Depth ↓FID ↓FID ↓
360 est 360 est 360 est 360 est
EG3D 1.10 1.10 6.03 6.03 1.19 1.19 7.27 7.27
3DGP 3.29 0.30 196.04 2.18 3.84 0.33 196.29 3.79
PoF3D 3.14 1.08 36.32 3.04 1.37 1.07 16.26 6.07
Ours 0.60 0.68 5.51 5.44 0.78 0.78 14.20 16.55
Table 2: Quantitative Comparison on SDIP Elephant and LSUN Plane.
Fig.9: Pose Distribution Comparison on Shapenet Cars.
comparison of the KL divergence in Tab. 3, where our distribution is significantly
better than that of 3DGP and PoF3D. This superior pose distribution contributes to
our ability to better recover the full object geometry.
4.3 Ablation Study
Tempate Feature Field: Wenowablatethedesignchoiceofusingtemplatefeature
fields for pose estimation. Alternatively, one may directly obtain a template radiance
D3GE
PGD3
D3FoP
sruO14 X. Chen et al.
field (TeRF) for pose estimation, without introducing the additional feature branch.
This template radiance field and the real RGB images can be leveraged to solve the
camera pose estimation task using the method introduced in Sec. 3.3. To further en-
hancetherobustnessagainstcolorvariation,onemayfurtherconverttheRGBimages
to gray for pose estimation. We compare our semantic feature-based method to these
two alternatives on the Shapenet Cars dataset with ground truth pose distribution.
As shown in Tab. 4, directly leveraging a template radiance field for pose estimation
leadstoworseposeestimation,whileourTeFFissemanticallyalignedacrossinstances
of different appearances and achieves better pose estimation.
Freedom of Degrees: To demonstrate the effect of leveraging four degrees of free-
dom (DoF), we conduct experiments on the CompCars dataset since this dataset has
a large 2D scale variation. We compare with an alternative that omits the phase
correlationforestimatingin-planerotationγ andscaler.Althoughtrainingwithonly
two DoF (θ,ϕ) achieves lower FID as shown in Tab. 5, we observe that the generated
3D samples have wrong geometry. This observation is also supported by the worse
FID and depth metrics, demonstrating the importance of modeling the scale and
360
in-plane rotation in the camera model.
Method TeRFRGB TeRFGray TeFF(Ours) DoF Depth360 ↓Depthest ↓FID360 ↓FIDest ↓
θKL↓ 0.0663 0.0656 0.0555 θ,ϕ 4.98 1.16 39.66 11.09
ϕKL ↓ 0.1422 0.1490 0.0696 θ,ϕ,γ,r 0.31 0.29 27.31 20.60
Table 4: AblationStudyonShapenetCar. Table 5: AblationStudyonCompCars.
5 Limitation
Our method can not model images with signifi-
cantperspectivedistortion.Tofitthedistribution
of real images, the perspective effect is modeled
by generated geometry and leads to the failure Fig.10: Failure case.
case like Fig. 10. The 2D-3D pose estimation may be influenced by the geometry of
the instance since we use the mse to select the best-matching camera pose. In the
future, we will explore disentangling the geometry information during the matching
process.Besides,wedonotmodelthearticulationoftheinstance.Thegeneratormay
model different articulations in different views and occasionally lead to instances with
multiple legs.
6 Conclusion
We present TeFF, a novel approach to enable learning 3D-aware generative models
from in-the-wild images with unknown camera pose distribution. By estimating the
camera poses of real images on the fly, we demonstrate that our method is capable
of recovering the full 3D geometry from datasets of challenging distributions. Despite
achieving promising results on in-the-wild images, our method learns a single feature
template and hence is only applicable to a specific category. Future work may explore
the usage of multiple templates for learning a single 3D-aware generative model on
images of multiple cateogories.TeFF 15
References
1. Amir, S., Gandelsman, Y., Bagon, S., Dekel, T.: Deep vit features as dense visual
descriptors. ECCVW What is Motion For? (2022) 4, 5
2. Caron, M., Touvron, H., Misra, I., Jégou, H., Mairal, J., Bojanowski, P., Joulin,
A.: Emerging properties in self-supervised vision transformers. In: Proceedings of
the International Conference on Computer Vision (ICCV) (2021) 4, 5
3. Chan, E.R., Lin, C.Z., Chan, M.A., Nagano, K., Pan, B., De Mello, S., Gallo,
O., Guibas, L.J., Tremblay, J., Khamis, S., Karras, T., Wetzstein, G.: Efficient
geometry-aware3dgenerativeadversarialnetworks.In:CVPR(2022) 2,5,6,8,9,
10
4. Chan, E.R., Lin, C.Z., Chan, M.A., Nagano, K., Pan, B., Mello, S.D., Gallo, O.,
Guibas,L.,Tremblay,J.,Khamis,S.,Karras,T.,Wetzstein,G.:Efficientgeometry-
aware 3D generative adversarial networks. In: IEEE Conf. Comput. Vis. Pattern
Recog. (2022) 2, 3
5. Chan, E.R., Monteiro, M., Kellnhofer, P., Wu, J., Wetzstein, G.: Pi-gan: Periodic
implicit generative adversarial networks for 3d-aware image synthesis. In: IEEE
Conf. Comput. Vis. Pattern Recog. (2021) 2, 3
6. Chang, A.X., Funkhouser, T.A., Guibas, L.J., Hanrahan, P., Huang, Q., Li, Z.,
Savarese, S., Savva, M., Song, S., Su, H., Xiao, J., Yi, L., Yu, F.: Shapenet: An
information-rich 3d model repository. arXiv.org 1512.03012 (2015) 9
7. Chen,K.,Wang,J.,Pang,J.,Cao,Y.,Xiong,Y.,Li,X.,Sun,S.,Feng,W.,Liu,Z.,
Xu,J.,Zhang,Z.,Cheng,D.,Zhu,C.,Cheng,T.,Zhao,Q.,Li,B.,Lu,X.,Zhu,R.,
Wu,Y.,Dai,J.,Wang,J.,Shi,J.,Ouyang,W.,Loy,C.C.,Lin,D.:MMDetection:
Open mmlab detection toolbox and benchmark. arXiv preprint arXiv:1906.07155
(2019) 9
8. Chen, X., Deng, Y., Wang, B.: Mimic3d: Thriving 3d-aware gans via 3d-to-
2d imitation. In: Proceedings of the IEEE/CVF International Conference on
Computer Vision (ICCV) (2023) 3
9. Chen, X., Huang, J., Bin, Y., Yu, L., Liao, Y.: Veri3d: Generative vertex-based
radiance fields for 3d controllable human image synthesis. In: Proceedings of the
IEEE/CVFInternationalConferenceonComputerVision(ICCV).pp.8986–8997
(October 2023) 3
10. Deng,K.,Liu,A.,Zhu,J.,Ramanan,D.:Depth-supervisednerf:Fewerviewsand
faster training for free. arXiv.org 2107.02791 (2021) 3
11. Deng, Y., Yang, J., Xiang, J., Tong, X.: Gram: Generative radiance manifolds
for 3d-aware image generation. In: Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition. pp. 10673–10683 (2022) 3
12. Gu,J.,Liu,L.,Wang,P.,Theobalt,C.:Stylenerf:Astyle-based3d-awaregenerator
for high-resolution image synthesis. Int. Conf. Learn. Represent. (2022) 3
13. Henzler, P., Mitra, N.J., Ritschel, T.: Escaping plato’s cave: 3d shape from
adversarial rendering. In: Int. Conf. Comput. Vis. (2019) 3
14. Heusel,M.,Ramsauer,H.,Unterthiner,T.,Nessler,B.,Hochreiter,S.:Ganstrained
by a two time-scale update rule converge to a local nash equilibrium. In: Adv.
Neural Inform. Process. Syst. (2017) 9
15. Jo,K.,Jin,W.,Choo,J.,Lee,H.,Cho,S.:3d-awaregenerativemodelforimproved
side-view image synthesis. In: Proceedings of the IEEE/CVF International
Conference on Computer Vision. pp. 22862–22872 (2023) 3
16. Jo, K., Shim, G., Jung, S., Yang, S., Choo, J.: Cg-nerf: Conditional generative
neural radiance fields. arXiv.org (2021) 316 X. Chen et al.
17. Karras, T., Laine, S., Aittala, M., Hellsten, J., Lehtinen, J., Aila, T.: Analyzing
and improving the image quality of StyleGAN. In: Proc. CVPR (2020) 6
18. Kerr, J., Kim, C.M., Goldberg, K., Kanazawa, A., Tancik, M.: Lerf: Language
embedded radiance fields. In: International Conference on Computer Vision
(ICCV) (2023) 4
19. Kobayashi, S., Matsumoto, E., Sitzmann, V.: Decomposing nerf for editing via
feature field distillation. In: Advances in Neural Information Processing Systems.
vol. 35 (2022), https://arxiv.org/pdf/2205.15585.pdf 4
20. Kuglin, C.D.: The phase correlation image alignment method (1975), https://api.
semanticscholar.org/CorpusID:61133413 7, 8
21. Liao, Y., Schwarz, K., Mescheder, L., Geiger, A.: Towards unsupervised learning
ofgenerativemodelsfor3dcontrollableimagesynthesis.In:IEEEConf.Comput.
Vis. Pattern Recog. (2020) 3
22. Mescheder, L., Geiger, A., Nowozin, S.: Which training methods for gans do
actually converge? In: Int. Conf. Mach. Learn. (2018) 8
23. Michael,N.,Andreas,G.:Giraffe:Representingscenesascompositionalgenerative
neural feature fields. In: IEEE Conference on Computer Vision and Pattern
Recognition, CVPR 2021. pp. 11453–11464. IEEE Computer Society (2021) 3
24. Mildenhall, B., Srinivasan, P.P., Tancik, M., Barron, J.T., Ramamoorthi, R., Ng,
R.:NeRF:Representingscenesasneuralradiancefieldsforviewsynthesis.In:Eur.
Conf. Comput. Vis. (2020) 3
25. Mokady, R., Yarom, M., Tov, O., Lang, O., Daniel Cohen-Or, Tali Dekel, M.I.,
Mosseri,I.:Self-distilledstylegan:Towardsgenerationfrominternetphotos(2022)
9
26. Nguyen-Phuoc,T.,Li,C.,Theis,L.,Richardt,C.,Yang,Y.:Hologan:Unsupervised
learning of 3d representations from natural images. In: Int. Conf. Comput. Vis.
(2019) 3
27. Niemeyer, M., Geiger, A.: Campari: Camera-aware decomposed generative neural
radiance fields (2021) 2, 3
28. Or-El, R., Luo, X., Shan, M., Shechtman, E., Park, J., Kemelmacher, I.: Stylesdf:
High-resolution 3d-consistent image and geometry generation. In: IEEE Conf.
Comput. Vis. Pattern Recog. (2022) 3
29. Pan, X., Xu, X., Loy, C.C., Theobalt, C., Dai, B.: A shading-guided generative
implicit model for shape-accurate 3d-aware image synthesis. In: Adv. Neural
Inform. Process. Syst. (2021) 3
30. Peng,S.,Liu,Y.,Huang,Q.X.,Bao,H.,Zhou,X.:Pvnet:Pixel-wisevotingnetwork
for 6dof pose estimation. 2019 IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR) pp. 4556–4565 (2018), https://api.semanticscholar.
org/CorpusID:57189382 7
31. Schwarz, K., Liao, Y., Niemeyer, M., Geiger, A.: Graf: Generative radiance fields
for 3d-aware image synthesis. In: Adv. Neural Inform. Process. Syst. (2020) 2, 3
32. Schwarz,K.,Sauer,A.,Niemeyer,M.,Liao,Y.,Geiger,A.:Voxgraf:Fast3d-aware
imagesynthesiswithsparsevoxelgrids.Adv.NeuralInform.Process.Syst.(2022)
3, 6
33. Shi, Z., Shen, Y., Xu, Y., Peng, S., Liao, Y., Guo, S., Chen, Q., Yeung, D.Y.:
Learning3d-awareimagesynthesiswithunknownposedistribution(2023) 2,3,9,
10
34. Shin,M.,Seo,Y.,Bae,J.,Choi,Y.S.,Kim,H.,Byun,H.,Uh,Y.:Ballgan:3d-aware
image synthesis with a spherical background. arXiv preprint arXiv:2301.09091
(2023) 3TeFF 17
35. Skorokhodov,I.,Siarohin,A.,Xu,Y.,Ren,J.,Lee,H.Y.,Wonka,P.,Tulyakov,S.:
3d generation on imagenet. In: International Conference on Learning Representa-
tions (2023), https://openreview.net/forum?id=U2WjB9xxZ9q 2, 3, 9, 10
36. Sun, J., Wang, X., Zhang, Y., Li, X., Zhang, Q., Liu, Y., Wang, J.: Fenerf: Face
editinginneuralradiancefields.In:ProceedingsoftheIEEE/CVFConferenceon
Computer Vision and Pattern Recognition. pp. 7672–7682 (2022) 8
37. Tschernezki, V., Laina, I., Larlus, D., Vedaldi, A.: Neural feature fusion fields:
3D distillation of self-supervised 2D image representations. In: Proceedings of the
International Conference on 3D Vision (3DV) (2022) 4
38. Wu,Y.,Kirillov,A.,Massa,F.,Lo,W.Y.,Girshick,R.:Detectron2.https://github.
com/facebookresearch/detectron2 (2019) 10
39. Xiang,J.,Yang,J.,Deng,Y.,Tong,X.:Gram-hd:3d-consistentimagegeneration
at high resolution with generative radiance manifolds. In: Proceedings of the
IEEE/CVF International Conference on Computer Vision. pp. 2195–2205 (2023)
3
40. Xu,X.,Pan,X.,Lin,D.,Dai,B.:Generativeoccupancyfieldsfor3dsurface-aware
image synthesis. In: Adv. Neural Inform. Process. Syst. (2021) 3
41. Xu, Y., Peng, S., Yang, C., Shen, Y., Zhou, B.: 3d-aware image synthesis via
learningstructuralandtexturalrepresentations.IEEEConf.Comput.Vis.Pattern
Recog. (2022) 3
42. Xue, Y., Li, Y., Singh, K.K., Lee, Y.J.: Giraffe hd: A high-resolution 3d-aware
generative model. In: Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition. pp. 18440–18449 (2022) 3
43. Yang, J., Ivanovic, B., Litany, O., Weng, X., Kim, S.W., Li, B., Che, T., Xu,
D., Fidler, S., Pavone, M., Wang, Y.: Emernerf: Emergent spatial-temporal scene
decomposition via self-supervision. arXiv preprint arXiv:2311.02077 (2023) 4
44. Yang, L., Luo, P., Loy, C.C., Tang, X.: A large-scale car dataset for fine-grained
categorization and verification. In: IEEE Conference on Computer Vision and
Pattern Recognition, CVPR 2015, Boston, MA, USA, June 7-12, 2015. pp. 3973–
3981. IEEE Computer Society (2015). https://doi.org/10.1109/CVPR.2015.
7299023, https://doi.org/10.1109/CVPR.2015.7299023 9
45. Yin, W., Zhang, J., Wang, O., Niklaus, S., Mai, L., Chen, S., Shen, C.: Learning
to recover 3d scene shape from a single image. In: Proc. IEEE Conf. Comp. Vis.
Patt. Recogn. (CVPR) (2021) 10
46. Yu, F., Wang, X., Li, Z., Cao, Y., Shan, Y., Dong, C.: GET3D-: learning GET3D
from unconstrained image collections. CoRR abs/2307.14918 (2023). https://
doi.org/10.48550/ARXIV.2307.14918, https://doi.org/10.48550/arXiv.2307.14918
3, 4
47. Yu,F.,Seff,A.,Zhang,Y.,Song,S.,Funkhouser,T.,Xiao,J.:Lsun:Constructionof
alarge-scaleimagedatasetusingdeeplearningwithhumansintheloop.arXiv.org
1506.03365 (2015) 9
48. Zhou, P., Xie, L., Ni, B., Tian, Q.: CIPS-3D: A 3D-Aware Generator of GANs
Based on Conditionally-Independent Pixel Synthesis. arXiv.org (2021) 318 X. Chen et al.
Appendix
A Runtime Analysis
Runtime Breakdown: We report the average runtime of different processes during
training in Tab. 6. The runtime analysis is conducted on an A6000 GPU. The "Tem-
plate Rendering" indicates the time to render the template feature field at discretized
azimuth and elevation angles θ and ϕ to obtain 2D template features
{F}Nθ×Nϕ.
The
k=1
batchsizetorenderthetemplateis32.Weupdatethetemplateandrenderitonceevery
16 iterations before 3k iterations and then once every epoch. The template rendering
time in the early stage is averaged over 16 iterations. Since the iterations of each
epoch are different for different datasets, we report the averaged template rendering
timeinthelatestageusingthedatasetofthesmallestamountofimages.The"Phase
Correlation" refers to the time for estimating the scale r and in-plane rotation γ,
and warping each feature template to yield
{F˜}Nθ×Nϕ.
The "Camera Pose Sampling"
k=1
includesthetimetocalculatemeansquareerrorandtoperforminversesampling(see
Eq. 2 and Eq. 3 of the main paper). "Training" indicates the training time without
camera pose estimation, which includes the data loading time, the network forward
time, and the optimization time. The batch size for training is 4. Note that we use a
fixedcameraposeanddonotperformposeestimationafter500kiterations.Compared
to the overall iterations of 6250k, the increased time is acceptable.
Runtime Comparison with Naïve Grid Search: As mentioned in our main
paper,onecanimplementanaïvegridsearchmethodbydiscretizingallfourvariables
(θ,ϕ,γ,r)weconsiderforthecameraposes.Wedemonstratethatthisnaïveapproach
significantly increases the pose estimation time in Tab. 7. Here, we further discretize
scale r and in-plane rotation γ, each discretized into 256 values, increasing the total
amountof2Dfeaturetemplatesby2562 times.Whilethisomitsthephasecorrelation
module, it significantly increases the template rendering time and is hence intractable
for training the 3D GAN.
TemplateRenderingTemplateRendering Phase CameraPose
Process Training
EarlyStage LateStage Correlation Sampling
Time(s/iter) 0.0992 0.0023 0.3898 0.0156 1.4038
Table 6: Time Analysis of Different Processes.
TemplateRenderingTemplateRendering Phase CameraPose
Process Training
EarlyStage LateStage Correlation Sampling
Time(s/iter) 4423.0519 102.5506 – 74.8994 1.4038
Table 7: Time Analysis of Naïve Grid Search.
B Qualitative Results of Camera Pose Estimation
Fig. 11, Fig. 12, Fig. 13 and Fig. 14 show the estimated poses of real images on
CompCars, SDIP Elephant, Shapenet Cars and LSUN Plane, respectively. The first
row is the real image I and its corresponding DINO feature F, and the second row isTeFF 19
thebest-matchingtemplatefeatureF˜∗.ThecorrespondingcameraposeofF˜∗ indicates
k k
the estimated pose of the real image. Note that we only perform phase correlation on
CompCarsandLSUNPlanesinceShapenetCarsandSDIPElephantdonothavelarge
variationsinscaleandin-planerotation.Theresultsdemonstratethatourmethodcan
performfairlyaccuratecameraposeestimation.Fig.15showsthefailurecasesofpose
estimation. The estimated pose may not be accurate due to the object articulation,
partialobservation,significantgeometrydifference,lackofmodelingobjecttranslation,
etc. This could be solved by introducing multiple templates, modeling more freedom
of degrees of camera pose, and disentangling the geometry information during the
matching process in the future. We remark that despite some instances of inaccurate
pose estimation, the overall distribution of poses is generally precise. This accuracy
enables the trained generator to produce objects with complete geometry and the
capability for comprehensive 360-degree view synthesis.
Fig.11: Qualitative Pose Estimation on CompCars.
C Uncurated Qualitative Results
Fig. 16, Fig. 17, Fig. 18 and Fig. 19 show the uncurated qualitative results on SDIP
Elephant, CompCars, LSUN Plane and Shapenet Cars. Our results demonstrate good
fidelity and diversity.
laeR
esoP
laeR
esoP
laeR
esoP20 X. Chen et al.
Fig.12: Qualitative Camera Pose Estimation on SDIP Elephant.
Fig.13: Qualitative Camera Pose Estimation on Shapenet Cars.
laeR
esoP
laeR
esoP
laeR
esoP
laeR
esoP
laeR
esoP
laeR
esoPTeFF 21
Fig.14: Qualitative Camera Pose Estimation on LSUN Plane.
Elephant CompCars Plane ShapenetCars
Fig.15: Failure Camera Pose Estimation
laeR
esoP
laeR
esoP
laeR
esoP22 X. Chen et al.
Fig.16: Uncurated Result on SDIP Elephant.
Fig.17: Uncurated Result on CompCars.TeFF 23
Fig.18: Uncurated Result on LSUN Plane.
Fig.19: Uncurated Result on Shapenet Cars.