EVALUATINGTHEEFFICACYOFCUT-AND-PASTEDATAAUGMENTATIONIN
SEMANTICSEGMENTATIONFORSATELLITEIMAGERY
IonutM.Motoi,LeonardoSaraceni,DanieleNardi,ThomasA.Ciarfuglia
DepartmentofComputer,ControlandManagementEngineering
SapienzaUniversityofRome
00185Rome,Italy
1. INTRODUCTION
Semanticsegmentationinsatelliteimageryinvolvesclassify-
ABSTRACT
ing each pixel into categories like impervious surfaces, cul-
tivated areas, forests, or water bodies. This process, often
Satelliteimageryiscrucialfortaskslikeenvironmentalmon-
referred to as Land Use Land Cover (LULC) classification,
itoring and urban planning. Typically, it relies on semantic
is crucial for interpreting the Earth’s surface and finds ap-
segmentationorLandUseLandCover(LULC)classification plications in environmental monitoring and urban planning,
to categorize each pixel. Despite the advancements brought
among others. However, it faces several challenges, such as
about by Deep Neural Networks (DNNs), their performance
the limited availability of labeled data [1], the inherent vari-
insegmentationtasksishinderedbychallengessuchaslim-
ability and complexity of satellite imagery, and class imbal-
ited availability of labeled data, class imbalance and the in-
ances within datasets. Generating pixel-level annotations is
herentvariabilityandcomplexityofsatelliteimages.
not only costly and labor-intensive but also prone to biases,
In order to mitigate those issues, our study explores the wherecertainclassesareunderrepresented,leadingtomodels
effectiveness of a Cut-and-Paste augmentation technique for thatstrugglewithrarecategories.
semanticsegmentationinsatelliteimages. Weadaptthisaug- One of the traditional approaches to mitigate these is-
mentation, which usually requires labeled instances, to the sues has been image augmentation, which generates new
caseofsemanticsegmentation. Byleveragingtheconnected trainingsamplesthroughtransformationslikeflips,rotations,
components in the semantic segmentation labels, we extract crops, and color adjustments, thereby improving model per-
instancesthatarethenrandomlypastedduringtraining. formance and generalization. Instance-level augmentations,
Using the DynamicEarthNet dataset and a U-Net model suchasthosebasedontheCut-and-Pasteconcept[2,3,4,5],
forevaluation, wefoundthatthisaugmentationsignificantly have shown promise in object detection and instance seg-
enhances the mIoU score on the test set from 37.9 to 44.1. mentation tasks in conventional camera imagery, as well as
ThisfindinghighlightsthepotentialoftheCut-and-Pasteaug- in construction resource detection using UAV-acquired im-
mentation to improve the generalization capabilities of se- ages [6]. These methods rely on the idea that diverse object
manticsegmentationmodelsinsatelliteimagery. representations can be achieved by manipulating instance
placement[2].
IndexTerms— SemanticSegmentation, LandUseLand Sometechniquesfurtherrefinethisconceptbyplacingob-
Cover, Deep Learning, Data Augmentation, Cut-and-Paste, ject masks in realistic locations to create consistent appear-
Copy-Paste,SatelliteRemoteSensing ances [3, 7, 4, 8], while others paste instances without the
needforcontextualmodeling[5].
In the context of satellite images, generative adversarial
Thisworkreceivedfundingby:
⋆⋆⋆⋆ ⋆⋆ ⋆⋆ ⋆⋆⋆⋆ the European Union’s Horizon 2020 research and innovation pro- networks have been employed to paste buildings into high-
grammeundergrantagreementNo101016906–ProjectCANOPIES, resolutionopticalimagestoaidbinarychangedetectionmod-
⋆⋆⋆⋆ ⋆⋆ ⋆⋆ ⋆⋆⋆⋆ Project AGRITECH Spoke 9 - Codice progetto MUR: AGRITECH els[9]. Anotherapproachinvolvesdirectlypastingbuildings
”National Research Centre for Agricultural Technologies” - CUP
onto diverse backgrounds, enhancing building segmentation
CN00000022, of the National Recovery and Resilience Plan (PNRR) fi-
nancedbytheEuropeanUnion”NextGenerationEU”, inveryhigh-resolutionimages[10]. Despitetheserecentad-
SapienzaUniversityofRomeaspartoftheworkforprojectH&M: vancements, a gap remains in adapting these techniques to
HyperspectralandMultispectralFruitSugarContentEstimationforRobot
the specific challenges of semantic segmentation in satellite
HarvestingOperationsinDifficultEnvironments, Del. SAn.36/2022, and
imagery. Thisismostlybecausethelabelsusedforthistask
projectWeaklyandSemi-SupervisedLearningforSemanticSegmentationus-
ingSatelliteImages,AR123188AA99CE2A. do not differentiate between individual objects or instances,
Copyright2024IEEE.Publishedinthe2024IEEEInternationalGeoscienceandRemoteSensingSymposium(IGARSS2024),scheduledfor7-12July,
2024inAthens,Greece. Personaluseofthismaterialispermitted. However,permissiontoreprint/republishthismaterialforadvertisingorpromotional
purposesorforcreatingnewcollectiveworksforresaleorredistributiontoserversorlists,ortoreuseanycopyrightedcomponentofthisworkinotherworks,
mustbeobtainedfromtheIEEE.Contact: Manager,CopyrightsandPermissions/IEEEServiceCenter/445HoesLane/P.O.Box1331/Piscataway,NJ
08855-1331,USA.Telephone:+Intl.908-562-3966.
4202
rpA
8
]VC.sc[
1v39650.4042:viXrawhicharenecessaryforinstance-levelaugmentations. More-
over, the application of a Cut-and-Paste augmentation tech-
niqueinmedium-resolutionsatelliteimages(1-10meters)has
notyetbeenexplored. For each
element
To bridge those gaps, we explore the use of a Cut-and-
in the
Pasteaugmentationforsemanticsegmentationofsatelliteim-
train set
ages. Theapproachreliesonextractingandsavinginstances
fromthesemanticsegmentationlabelsusingconnectedcom-
ponentsanalysis. Thisstepallowsustogenerateaugmented
images by pasting the extracted instances in various config- Extract connected
urations onto the original training images. Unlike other ap- components
from labels
proachesthatmayrestrictthesourceofinstancestothetarget
imageitself[3,4]ortoanotherimagerandomlyselectedfrom
the dataset [9], our method can leverage the entire training …
…
set, allowing for more diverse compositions. We also allow
instancestooverlap,creatingatypicalyetplausiblescenarios
thatencouragemodelstogeneralizebeyondfrequentpatterns.
This technique can be categorized as model-free, multi- Save the
image, and instance-level, as defined by Xu et al. [11] since image-label Class 1 Class 2 Class 3 … Class C
pairs
it does not rely on additional generative models, uses mul-
tiple images to produce augmented samples, and focuses on
Randomly
manipulating specific instances. Our approach offers a sim-
paste
ple solution for generating new semantic segmentation data instances
ofsatelliteimageswithoutrequiringadditionalmanualanno- during
training
tations. Hence, it addresses the previously mentioned chal-
lengesbyenhancingdatadiversity,whichiscrucialfortrain-
ingrobustsemanticsegmentationmodels.
Our evaluation, using the DynamicEarthNet dataset and
aU-Netmodelbaseline,demonstratesthattheCut-and-Paste
augmentation significantly improves the mean Intersection
over Union (mIoU) score on the test set from 37.9 to 44.1,
confirmingitspracticalbenefits.
Fig. 1: Overview of the Cut-and-Paste augmentation tech-
2. METHODOLOGY
nique. The training set is first processed to extract and save
instances that are then randomly pasted during the training
The entire process involves two steps, as shown in Fig. 1.
phase.
Firstly,weperformanofflinepreprocessingofthedatatoex-
tract the instances of interest. Next, we perform an online
augmentationoftheimagesbyrandomlypastingtheobjects
In order to identify individual objects present in an im-
extractedinthepreviousstep.
age,weexploittheconceptofconnectedcomponents,which
consistofcontiguouspixelsbelongingtothesameclass. For
2.1. InstanceExtraction each image I, we separate its corresponding label L into C
binarymasksL ,oneforeachclass,andapplytheconnected
c
TheinitialstageisnecessarytoadapttheCut-and-Pasteaug- componentsalgorithmtoextractN components. Inpractice,
c
mentation to the problem of semantic segmentation. Unlike this corresponds to partitioning the label into a set of non-
instance segmentation, where labels delineate individual ob- overlapping mask instances L , each belonging to one of
c,i
jects, semantic segmentation in datasets like DynamicEarth- theC classes.
Net [12] provides class information at a pixel level, merged
intoasinglemaskimage. ThekeytoleveragingtheCut-and- (cid:91)C (cid:91)C (cid:91)Nc
L= L = L (1)
Paste technique in this context lies in the separation of the c c,i
composite mask into distinct instance masks. This step en- c=1 c=1i=1
ables the independent manipulation and pasting of instances Theextractedobjectswillfinallyconsistofcroppedmul-
inthesubsequentstage. tispectral images and the relative binary masks (I ,L ).
c,i c,iThosearesavedinordertocreateasetofinstancescontaining 3.2. Evaluationmetric
theextractedobjectsforeachclass.
Toevaluatetheperformanceofthemodels,themeanintersec-
tionoverunion(mIoU)wasemployedastheprimarymetric.
Itisdefinedastheaverageoftheintersectionoverunionval-
2.2. InstancePasting
uesforthepredictedandgroundtruthlabelsacrossallclasses:
The second step is done during training, augmenting each C
image-label pair on the fly with instances from the set gen- mIoU =
1 (cid:88)|A i∩B
i
|
(2)
C |A ∪B |
erated in the first stage. We uniformly sample a category c i=1 i i
andrandomlychooseaninstance(I c,i,L c,i)belongingtothat
whereC isthenumberofclasses, A isthegroundtruth
i
class. Thechoseninstanceisoptionallyaugmentedwithflips maskforclassi,andB isthepredictionmaskforclassi.
i
androtationsandthenpastedontothecurrenttrainingsample
atarandomlocation.
3.3. Training
We repeat the instance pasting process N times for each
trainingimage,whereN isaparameterindicatingthenumber Since we are studying the effects of a generally applicable
of objects to be added. Differently from [9], we allow the augmentationtechnique,welimitourcomparisonstotheuse
instancestooverlap,and,asaresult,ourmethodcanproduce ofabasicU-Netmodel[13],whichiswidelyemployedinthe
unusualscenarios.Althoughsuchscenariosmayberare,they literatureforsemanticsegmentationtasks. TheU-Netmodel
arenotimpossibleinrealityandcanhelpthetrainedmodelto wastrainedfromscratchontheDynamicEarthNetdatasetus-
generalizetoinfrequentreal-worldcases. ing an Adam optimizer with a learning rate of 1e-4 for 200
epochs. We saved the model weights at the epoch with the
Oncethepastingprocessisconcluded,thefinalimageun-
highestmIoUontheinternalvalidationset. Toensureconsis-
dergoessimplenon-destructivetransformations,i.e.,horizon-
tencyandreproducibilityinourexperiments,afteroptimiza-
tal/verticalflipsand90-degreerotations.
tionofthebaseline,thehyperparametersremainedfixed. We
repeated each experiment three times and reported the mean
andstandarddeviationofthemIoU.
3. EXPERIMENTALSETUP
Ourbaselinemodelwastrainedwithstandardaugmenta-
tions, including horizontal and vertical flips and random 90-
3.1. Dataset degree rotations, but did not employ the Cut-and-Paste aug-
mentation.
Forevaluation,weutilizedtheDynamicEarthNetdataset[12], We evaluated the impact of the Cut-and-Paste augmen-
which consists of daily, multispectral satellite images from tation in addition to standard augmentations by varying the
PlanetLabs.Thedatasetcovers75areasofinterestaroundthe number of pasted instances per image (N=10, 100, 1000).
world, with data spanning over a period of 24 months, from Furthermore, we explored the influence of augmenting ev-
January 2018 to December 2019. Each image includes both ery instance with flips and rotations prior to pasting them
RGB and near-infrared bands with a spatial resolution of 3 ontotheimage. Werefertothoseinstanceaugmentationsas
metersperpixel. Thedatasetprovidespixel-wisemonthlyse- pre-pastingaugmentations.
manticsegmentationlabelsof7classes: impervioussurface,
agriculture, forest & other vegetation, wetlands, soil, water,
4. RESULTS
andsnow&ice. Followingthemethodologyoutlinedin[12],
wedidnotconsiderthesnow&iceclassduetoitslimitedrep-
In this section, we present the results of implementing the
resentation. Since we focused on fully supervised semantic
Cut-and-Pasteaugmentationinvariousconfigurations. Inall
segmentation,weselectedtheimagesthathadacorrespond-
the experiments we used Cut-and-Paste in conjunction with
inggroundtruthlabel,meaningthefirstdayofeachmonth.
the standard augmentations of the baseline to underline its
The dataset is natively split into training, validation, and
additive contribution. Our results, shown in Tables 1 and
test sets. However, as the validation and test sets lack la-
2, demonstrate notable improvements in model performance
bels, evaluations require submitting predictions to the Co-
overthebaseline.
dalabplatform1. Tofacilitatethevalidationprocess,wesub-
Table1showstheperformanceofthemodelsontheval-
divided the training set into 1200 images for training and
idationandinternalvalidationsets. WhentheCut-and-Paste
120 images for internal validation, while ensuring that both
augmentationwasappliedwithteninstances,themIoUonthe
splitsincludedalltheclassesandthatanyareaofinterestwas
validationsetimproved,andaslightdecreaseinperformance
presentinonlyoneofthetwosplitstopreventdataleakage.
was seen on the internal validation set. By increasing the
number of pasted instances to 100, the validation improves
1https://codalab.lisn.upsaclay.fr/competitions/2882 substantially on both sets, suggesting that the method scalesTable 1: Validation performance of the Cut-and-Paste aug-
mentationcomparedtothebaseline. Theeffectsofdifferent
numbers of pasted instances and the use of pre-pasting aug-
mentationsareconsidered. Themeanandstandarddeviation
arecomputedoverthreedifferentruns.
Pre-pasting Val Internal
augmentations mIoU ValmIoU
Baseline 34.1(2.1) 43.5(1.0)
C&PN=10 ✓ 34.5(0.8) 43.4(0.9)
C&PN=100 ✓ 35.9(1.1) 47.4(0.7)
(a)Image (b)GT (c)Baseline (d)C&P
C&PN=1000 ✓ 35.6(0.6) 47.0(0.6)
C&PN=10 ✗ 35.5(1.3) 43.2(0.9) Fig. 2: Qualitative visualization of segmentation results on
C&PN=100 ✗ 36.0(1.5) 47.5(0.8) internalvalidationimages. Subfigure(a)displaystheoriginal
multispectralsatelliteimage,convertedtoRGBandenhanced
C&PN=1000 ✗ 34.8(2.0) 47.0(0.7)
for better visualization, (b) shows the corresponding ground
truthsegmentation, (c)isthesegmentationpredictionbythe
Table 2: Test performance of the Cut-and-Paste Augmen-
baseline U-Net model, and (d) shows the improved predic-
tation with N=100 pasted instances, with and without pre-
tions achieved by the U-Net model with Cut-and-Paste aug-
pastingaugmentations,comparedtothebaseline.
mentation(N=100withoutpre-pastingaugmentations). Each
colorin(b-d)indicatesadifferentclass,withimpervioussur-
Pre-pasting Test
facesshowninblue, forestingreen, soilinpink, agriculture
augmentations mIoU inred,andwaterinorange.
Baseline 37.9
C&PN=100 ✓ 42.3
a test mIoU of 44.1, and the one with pre-pasting augmen-
C&PN=100 ✗ 44.1
tationsachieved42.3,bothsignificantlysurpassingthebase-
line’smIoUof37.9. Theseresultshighlightthegeneraleffi-
cacy of the Cut-and-Paste augmentation technique and sug-
well with an increased number of instances to some extent.
gest that the number of instances and the use of pre-pasting
As the number of instances increases to 1000, we observe a
augmentations need to be carefully calibrated to obtain the
drop in mIoU gains, suggesting a point where adding more
greatestimprovement.
instancesdoesnotyieldfurtherimprovementsandmayeven
bedetrimental.
The most effective configuration utilized Cut-and-Paste
with100instanceswithoutpre-pastingaugmentations,achiev- 5. CONCLUSION
ing an average highest validation mIoU of 36.0, with a sig-
nificant improvement over the baseline. The second-best Inthisstudy,wehaveexploredtheapplicationofaCut-and-
configuration, using the same number of instances but with Paste augmentation technique for semantic segmentation of
pre-pasting augmentations, reached a validation mIoU of satelliteimagery. Wehaveadaptedthistechnique,whichusu-
35.9. This indicates that pre-pasting augmentations do not ally requires labeled instances, to the case of semantic seg-
necessarily correlate with better performance and that the mentationbyleveragingtheconnectedcomponentsinthese-
modelcanachievesubstantialgainswithjustplainCut-and- manticlabels. Wehavedemonstratedthatthisaugmentation
Paste. enhancesthedatadiversityandvariability,improvingthegen-
Fig. 2 provides visual comparisons of the segmentation eralizationcapabilitiesofthesegmentationmodel.Ourexper-
results between the baseline and the best-performing model iments, performedusingasimpleU-Netmodel, haveshown
ontwoimagesfromtheinternalvalidationset. Theimprove- that the Cut-and-Paste augmentation provides a significant
ments are particularly notable in the impervious surface and performanceincrease, bringingthemIoUscorefrom37.9to
agricultureclasses,whereouraugmentationapproachsignif- 44.1 on the DynamicEarthNet test set. Our approach offers
icantlyenhancesthemodel’spredictiveaccuracy. a simple and effective solution for generating new semantic
For the test set evaluation (Table 2), only the two best segmentationdataofsatelliteimageswithoutrequiringaddi-
configurationswerefurtherassessed. Here, theperformance tional manual annotations. Future work could further refine
gains were more pronounced: the configuration with 100 this technique and explore its applicability to other remote
pasted instances without pre-pasting augmentations reached sensingtasks,suchasChangeDetection.6. REFERENCES study,” in Proceedings of the IEEE/CVF International
ConferenceonComputerVision,2021,pp.1659–1668.
[1] Jia Song, Shaohua Gao, Yunqiang Zhu, and Chenyan
Ma, “A survey of remote sensing image classification [11] MingleXu,SookYoon,AlvaroFuentes,andDongSun
basedoncnns,” Bigearthdata, vol.3, no.3, pp.232– Park, “Acomprehensivesurveyofimageaugmentation
254,2019. techniques for deep learning,” Pattern Recognition, p.
109347,2023.
[2] Debidatta Dwibedi, Ishan Misra, and Martial Hebert,
“Cut, paste and learn: Surprisingly easy synthesis for [12] Aysim Toker, Lukas Kondmann, Mark Weber, Mar-
instancedetection,”inProceedingsoftheIEEEinterna- vin Eisenberger, Andre´s Camero, Jingliang Hu, Ari-
tional conference on computer vision, 2017, pp. 1301– adnaPregelHoderlein,C¸ag˘larS¸enaras,TimothyDavis,
1310. DanielCremers,etal., “Dynamicearthnet: Dailymulti-
spectral satellite dataset for semantic change segmen-
[3] Tal Remez, Jonathan Huang, and Matthew Brown,
tation,” in Proceedings of the IEEE/CVF Conference
“Learning to segment via cut-and-paste,” in Proceed-
onComputerVisionandPatternRecognition,2022,pp.
ings of the European conference on computer vision
21158–21167.
(ECCV),2018,pp.37–52.
[13] Olaf Ronneberger, Philipp Fischer, and Thomas Brox,
[4] Hao-ShuFang,JianhuaSun,RunzhongWang,Minghao
“U-net: Convolutional networks for biomedical im-
Gou,Yong-LuLi,andCewuLu, “Instaboost: Boosting
age segmentation,” in Medical Image Computing and
instancesegmentationviaprobabilitymapguidedcopy-
Computer-Assisted Intervention–MICCAI 2015: 18th
pasting,” inProceedingsoftheIEEE/CVFinternational
International Conference, Munich, Germany, October
conferenceoncomputervision,2019,pp.682–691.
5-9,2015,Proceedings,PartIII18.Springer,2015,pp.
[5] Golnaz Ghiasi, Yin Cui, Aravind Srinivas, Rui Qian, 234–241.
Tsung-Yi Lin, Ekin D Cubuk, Quoc V Le, and Barret
Zoph,“Simplecopy-pasteisastrongdataaugmentation
method for instance segmentation,” in Proceedings of
the IEEE/CVF conference on computer vision and pat-
ternrecognition,2021,pp.2918–2928.
[6] Seongdeok Bang, Francis Baek, Somin Park, Wontae
Kim, and Hyoungkwan Kim, “Image augmentation
to improve construction resource detection using gen-
erative adversarial networks, cut-and-paste, and image
transformation techniques,” Automation in Construc-
tion,vol.115,pp.103198,2020.
[7] Nikita Dvornik, Julien Mairal, and Cordelia Schmid,
“Modeling visual context is key to augmenting object
detection datasets,” in Proceedings of the European
Conference on Computer Vision (ECCV), 2018, pp.
364–380.
[8] Nikita Dvornik, Julien Mairal, and Cordelia Schmid,
“On the importance of visual context for data augmen-
tation in scene understanding,” IEEE transactions on
pattern analysis and machine intelligence, vol. 43, no.
6,pp.2014–2028,2019.
[9] Hao Chen, Wenyuan Li, and Zhenwei Shi, “Adversar-
ialinstanceaugmentationforbuildingchangedetection
inremotesensingimages,” IEEETransactionsonGeo-
scienceandRemoteSensing,vol.60,pp.1–16,2021.
[10] SvetlanaIllarionova,SergeyNesteruk,DmitriiShadrin,
Vladimir Ignatiev, Mariia Pukalchik, and Ivan Os-
eledets, “Object-based augmentation for building se-
mantic segmentation: Ventura and santa rosa case