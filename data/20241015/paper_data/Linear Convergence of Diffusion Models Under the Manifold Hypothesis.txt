Linear Convergence of Diffusion Models Under the Manifold
Hypothesis
Peter Potaptchik∗, Iskander Azangulov∗, and George Deligiannidis
University of Oxford
{surname}@stats.ox.ac.uk
Abstract
Score-matching generative models have proven successful at sampling from complex high-
dimensional data distributions. In many applications, this distribution is believed to concentrate
on a much lower d-dimensional manifold embedded into D-dimensional space; this is known as
the manifold hypothesis. The current best-known convergence guarantees are either linear in D
or polynomial (superlinear) in d. The latter exploits a novel integration scheme for the backward
SDE. We take the best of both worlds and show that the number of steps diffusion models require
in order to converge in Kullback-Leibler (KL) divergence is linear (up to logarithmic terms) in the
intrinsic dimension d. Moreover, we show that this linear dependency is sharp.
1 Introduction
Score-matching generative models (Ho et al., 2020; Song et al., 2021) such as diffusion models have
become a leading paradigm for generative modeling. They achieve state-of-the-art results in many
domains including audio/image/video synthesis (Evans et al., 2024; Dhariwal and Nichol, 2021; Ho
et al., 2022), molecular modeling (Watson et al., 2023), and recently text generation (Lou et al., 2024).
Informally, diffusion models take samples from a distribution in RD, gradually corrupt them with
Gaussian noise, and then learn to reverse this process. Once trained, a diffusion model can turn noise
into new samples from the data distribution by iteratively applying the denoising procedure.
Due to the empirical success of diffusion models, there has been a push (Oko et al., 2023; Wibisono
et al., 2024; Wu et al., 2024) to better understand their theoretical properties, in particular, their
convergence guarantees.
An important question is to determine the iteration complexity of diffusion models. In this paper,
this refers to the number of steps diffusion models require in order to converge in KL to the original
distribution regularized by a small amount of Gaussian noise. Assuming only the existence of a second
moment, Benton et al. (2024) prove that the iteration complexity is at most linear (up to logarithmic
factors) in D.
While this result is tight in the general case, many real-world distributions actually have a low-
dimensional structure. The assumption that a distribution lives on a d-dimensional manifold is called
the manifold hypothesis. This hypothesis has been supported by empirical evidence in many settings,
e.g. image data, in which diffusion models are particularly successful. Therefore, the study of diffusion
models under this assumption has garnered increased interest (Kadkhodaie et al., 2024; Tang and Yang,
2024; Pidstrigach, 2022).
RecentlyLiandYan(2024a)haveshownthatthereisaspecialdiscretizationdesignguaranteeingan
iterationcomplexityofd4 intheintrinsicdimensiond. Thecurrent(Azangulovetal.,2024)best-known
bound scales as d3.
∗Equalcontribution
1
4202
tcO
11
]LM.tats[
1v64090.0142:viXraOur Contribution
In this work, we improve upon these results and show that the number of steps diffusion models require
to converge in KL divergence is linear (up to logarithmic terms) in the intrinsic dimension d. This is
formalized in Theorem 3. Additionally, we prove that the linear dependency is sharp.
The proof follows the structure of Chen et al. (2023b) and Benton et al. (2024) combined with a
result from Azangulov et al. (2024) providing bounds on the score function depending only on the
intrinsic dimension d. A key insight of our proof exploits the inherent martingale structure in diffusion
processes. As we show, with the right SDE discretization, the corresponding error can be represented
as a sum of easy-to-control martingale increments leading to a very concise argument.
We posit that this scaling is one of the major reasons why diffusion models are able to perform so
well on tasks such as synthetic image generation. While the extrinsic dimension of image datasets is
very large, e.g. ≈1.5·105 for ImageNet, Pope et al. (2021) estimate that the true intrinsic dimension
is much lower, e.g. around 50 for ImageNet. Our result implies that the number of steps diffusion
models need to sample scales as the latter rather than the former. This helps explain why diffusion
models are able to generate crisp image samples with less than 1000 iterations (Ho et al., 2020).
In Section 2, we give an overview of diffusion models, our assumptions and the discretization scheme
that we use. We introduce our main result in Section 3 and give its proof in Section 4. We conclude in
Section 5.
2 Preliminaries
2.1 Diffusion Models
Suppose we want to generate samples from a distribution µ on RD. Diffusion models solve this by
first specifying a forward noising process {X } up to some time T. This process is defined as the
t t∈[0,T]
evolution of data X ∼µ according to an Ornstein-Uhlenbeck (OU) SDE
0
(cid:40) √
dX =−X dt+ 2dB , t∈(0,T]
t t t
X ∼µ,
0
where {B } is a Brownian motion on RD. Letting c :=exp(−t) and σ2 :=1−exp(−2t), we note
t t∈[0,T] t t
dist.
that X = c X +σ Z where Z ∼N(0,Id ). We use p to denote the marginal density of X .
t t 0 t D D D t t
The reverse process {Y } :={X } , under mild assumptions Anderson (1982), satisfies
t t∈[0,T] T−t t∈[0,T]
(cid:40) √
dY =(Y +2∇logp (Y ))dt+ 2dB′, t∈(0,T]
t t T−t t t (1)
Y ∼p ,
0 T
where {B′} is another Brownian motion on RD. By generating samples Y ∼ p and then
t t∈[0,T] 0 T
simulating (1) up to time T, we can obtain samples Y ∼µ from the data distribution. The main idea
T
behind diffusion models is to simulate these dynamics approximately since neither p nor the score
T
function s :=∇logp are known.
t t
In practice, we solve these problems by building an approximate process Yˆ. Due to the exponential
t
convergence of the OU process to a standard normal distribution, for a sufficiently large T, we have
that p ≈ N(0,Id ). So, we initialize as Yˆ ∼ N(0,Id ). Second, we learn a score approximation
T D 0 D
sˆ ≈s which is used instead of the true score function. The approximation sˆ is usually parameterized
t t t
by neural networks and trained via a denoising score-matching objective.
Finally, we note that the SDE (1) cannot be simulated exactly and instead a discretization scheme
must be introduced. More precisely, first, in order to ensure numerical stability, an early stopping time
δ >0ischosen. Next,theinterval[0,T−δ]isdividedintoK timesteps0=t <t <...<t =T−δ.
0 1 K
2The final discretization is given by
(cid:40)
Yˆ =α Yˆ +β sˆ (Yˆ )+η Z , 0≤k <K
tk+1 k tk k T−tk tk k k (2)
Yˆ ∼N(0,Id ),
t0 D
i.i.d.
whereZ ∼ N(0,Id )andα ,β ,η arerealnumbers. Thediscretizationschedulet <t <...<t
k D k k k 0 1 K
and coefficients α ,β ,η are hyper-parameters which we will specify in Section 2.3.
k k k
(a) Hilbertcurvesofordersn=2,4,6 (b) Prematureearlystoppingleadstoover-
smoothingofthemanifoldstructure
2.2 Assumptions and Notation
Throughout the paper, we assume that the distribution µ satisfies the manifold hypothesis which we
state more formally in the following.
Assumption A. µ is supported on a smooth compact d-dimensional β ≥ 2-smooth manifold M
embedded into RD.
Wealsomakethefollowingassumptionforeaseofpresentation. Thegeneralcaseishandledbyrescaling
and shifting.
Assumption B. We assume diamM ≤1 and 0∈M.
Without any restrictions on the manifold M, we cannot hope to obtain bounds independent of the
ambient dimension D. As an intuitive counter-example, consider D-dimensional Hilbert curves M ,
n
see Fig. 1(a). In the limit, these curves cover the entire D dimensional cube M :=[0,1]D. Moreover,
∞
anymeasureonM canbeseenasaweaklimitofmeasuresonM . ThismakessamplingfromM (for
∞ n n
large enough n) as hard as sampling from M , which according to (Benton et al., 2024, Appendix H)
∞
scales as at least D.
Therefore, we should place additional assumptions on the complexity of µ and M in order to avoid
such pathological cases. Informally (see details in Appendix A), the complexity of M depends both
on its global (volume) and local (smoothness) properties. We control its smoothness by introducing a
scale r >0 at which M is locally flat. To control the measure µ, we assume that it has a density p (dy)
0
(w.r.t. the standard volume form dy) bounded from above and below. We assume logarithmic control
over the discussed quantities.
Assumption C. There is a constant C >0 such that VolM ≤eC, r >e−C, and e−C ≤p ≤eC.
0
Remark 1. Informally, on a scale larger than r, the manifold M is not flat anymore. So, if M is
corrupted by noise of magnitude greater than r, over-smoothing may destroy the geometric structure,
√
see Fig. 1(b). Diffusion models stopped at time δ add Gaussian noise proportional to δ. To capture
its shape, M shouldn’t be corrupted by noise of magnitude greater than r. So, the stopping time should
√
be chosen to satisfy δ ≲r. Thus we should expect logδ−1 ≥C.
Recall that δ denotes the early stopping time, and 0 = t < t < ... < t = T −δ are the K
0 1 K
discretization time steps. Let γ :=t −t be the k-th step size. We control the score estimation
k k+1 k
error of sˆ as follows.
t
3Assumption D. The score network sˆ(x) satisfies
t
K−1
(cid:88)
γ E∥s (X )−sˆ (X )∥2 ≤ε2 .
k T−tk T−tk T−tk T−tk score
k=0
2.3 Discretization Scheme
In this section, we recall the construction of a discretization scheme that allows for a polynomial
convergence in the intrinsic dimension d. The main goal of this paper is to show that this convergence
is, in fact, linear. We follow the discretization schedule in Benton et al. (2024). Fix a positive κ<1/4
and choose a partition 0=t <t <...<t =T −δ such that γ =t −t ≤κmin(1,T −t ).
0 1 K k k+1 k k
Remark 2. For example, fixing integers L,K, and choosing T :=κL+1 and δ :=(1+κ)L−K, one can
taketheuniformpartitiont =κk,k <Lof[0,T−1]andtheexponentialpartitiont =T−(1+κ)−m
k L+m
of [T −1,T −δ] for 0≤m≤K−L. We use this explicit schedule in what follows.
The class of discretization schemes of the form (2) contains a unique one, first found by Li and Yan
(2024a), that has an iteration complexity independent of D. It is given by

Yˆ
tk+1
=c− γk1Yˆ
tk
+ σ cγγ2 kksˆ T−tk(Yˆ tk)+σ γkσ σT T− −tk t+ k1Z k, Z
k
i. ∼i.d. N(0,Id D)
(3)
Yˆ ∼N(0,Id ).
t0 D
Let us fix some (t,x),(t′,x′)∈R ×RD. The classic exponential integrator scheme uses sˆ(t′,x′) as an
+
estimate of the true score s(t,x). Azangulov et al. (2024) shows that, in contrast, (3) implicitly applies
a first-order correction of the score. More precisely, they prove that if we introduce
σ2 x−c−1 x′
sˆ(x|t′,x′):=c−1 t′sˆ(t′,x′)− t′−t , (4)
t t′−tσ2 σ2
t t
then the scheme (3) corresponds to the continuous time dynamics given by
(cid:40) (cid:104) (cid:105) √
dYˆ = Yˆ +2sˆ (Yˆ|T −t ,Yˆ ) dt+ 2dB′, t∈[t ,t )
t t T−t t k tk t k k+1 (5)
Yˆ ∼N(0,Id ).
0 D
This can be proven by integrating the linear SDE (5). The properties of the linear correction (4) will
become apparent in the proof of our main result. See (9) and then a discussion in Remark 10. We
analogously define s (x|t′,x′) to be a correction of the true score obtained by substituting s(t′,x′)
t
instead of sˆ(t′,x′) into (4).
3 Main Results
The main result of our paper is the following.
Theorem 3. Let µ be a measure satisfying Assumptions A–C and let sˆ be a score approximation
t
satisfying Assumption D. Then the process Yˆ following (3) and the discretization schedule defined in
t
Section 2.3 satisfies
KL(Yˆ ∥X )≲ε2 +De−2T +κ+dκ2(K−L)(cid:0) logδ−1+C(cid:1) , (6)
T−δ δ score
where we use KL(X∥Y) to denote the KL-divergence between the laws of X and Y.
4This bound consists of four terms: (i) ε2 corresponding to the error in score approximation; (ii)
score
De−2T corresponding to the initialization error; (iii) κ corresponding to the discretization error for
t∈[0,T−1];(iv)dκ2(K−L)(logδ−1+C)correspondingtothediscretizationerrorfort∈[T−1,T−δ].
By choosing K to be sufficiently large, we obtain a linear (up to logarithmic factors) in d bound on the
iteration complexity in the following.
Corollary 4. Under the same assumptions as in Theorem 3, for a given κ<1/4 and tolerance ε>0,
choosing L≃κ−1(cid:0) logD+logε−1(cid:1) and K−L≃κ−1logδ−1, diffusion models with K denoising steps
achieve an error bounded as
d(logδ−1+logε−1+logD)(logδ−1+C)logδ−1
KL(Yˆ ∥X )≲ε2+ε2 + .
T−δ δ score K
Tightness of linear bound. Inthebestcase,adiffusionmodellearnsthescorefunctionexactly. Let
π be a compactly supported distribution on R such that a diffusion model with a given discretization
scheme and perfect score achieves an ε error in KL.
Consider the product measure π⊗d ⊗(δ )⊗(D−d) on Rd ×RD−d. Note that its score function
0
is (s (x ),...,s (x ),−σ−2x ,...,−σ−2x ). So, by the tensorization of KL, applying the same
t 1 t d t d+1 t D
discretization scheme as above with the new exact score has a dε error. Therefore a linear dependence
in d is optimal.
4 Proof of Theorem 3
WedefertheproofsoflemmastoAppendixB.ThemainideaoftheproofofTheorem3istoexploitthe
martingale structure of E[X |X ]. So, we begin by formalizing this before proceeding to the main proof.
0 t
We introduce the functions m (x) := E[X |X = x] and define a stochastic process {m (X )} .
t 0 t t t t∈[0,T]
Note that m (X )=E[X |X ]. The next lemma states that this process is a martingale.
t t 0 t
Lemma 5. Define the filtration F :=σ(X :s≥t). The process {m (X )} is a martingale w.r.t.
t s t t t∈[0,T]
{F } . In particular, for t<t′
t t∈[0,T]
E[m (X )|X ]=m (X ). (7)
t t t′ t′ t′
By Assumption A, X is a.s. bounded. Therefore the process {m (X )} is square integrable. So,
0 t t t∈[0,T]
the following lemma on the orthogonality of martingale increments can be applied.
Lemma 6. Let {M } be a square integrable martingale in RD w.r.t. a filtration {F } . Then for
t t≥0 t t≥0
any t <t <t we have
1 2 3
E∥M −M ∥2 =E∥M −M ∥2+E∥M −M ∥2.
t3 t1 t3 t2 t2 t1
We are now ready to present the proof of Theorem 3. Our first steps coincide with Chen et al. (2023a);
Benton et al. (2024). We begin by decoupling the errors coming from score estimation, approximate
initialization and SDE discretization. This is formalized by the following lemma.
Lemma 7.
KL(Yˆ ∥X )≲ε2 +De−2T
+K (cid:88)−1(cid:90) tk+1
E∥s (X |T −t ,X )−s (X )∥2dt. (8)
T−δ δ score T−t T−t k T−tk T−t T−t
k=0 tk
Note that we get we get the first and second terms of the desired bound in (6) plus the discretization
error of the ideal score approximation. So, it is sufficient to bound this latter sum by κ+dκ2(K −
L)(logδ−1+C).
5The next observation is that Tweedie’s formula (Robbins, 1956) gives
c c
s (x|t′,x′)−s(t,x)= t (E[X |X =x]−E[X |X =x′])= t (m (x)−m (x′)). (9)
t σ2 0 t 0 t′ σ2 t t′
t t
This implies that
c2
E∥s (X |t′,X )−s (X )∥2 = t E∥m (X )−m (X )∥2. (10)
t t t′ t t σ4 t t t′ t′
t
Applying Lemma 6 with t =0,t =t,t =t′ we get
1 w 3
E∥m (X )−m (X )∥2 =E∥m (X )−m (X )∥2+E∥m (X )−m (X )∥2. (11)
0 0 t′ t′ 0 0 t t t t t′ t′
Since m (X )=E[X |X ]=X , substituting (11) into (10) gives
0 0 0 0 0
c2(cid:16) (cid:17)
E∥s (X |t′,X )−s (X )∥2 = t E∥X −m (X )∥2−E∥X −m (X )∥2 . (12)
t t t′ t t σ4 0 t′ t′ 0 t t
t
We next formalize the intuitive statement that the discretization error increases with the time gap.
Lemma 8. For t <t <t′
1 2
E∥s (X |t′,X )−s (X )∥2 ≥E∥s (X |t′,X )−s (X )∥2.
t1 t1 t′ t1 t1 t2 t2 t′ t2 t2
We can now use Lemma 8 and then (12) to bound the sum in (8) as
K (cid:88)−1(cid:90) tk+1
E∥s (X |T −t ,X )−s (X )∥2dt
T−t T−t k T−tk T−t T−t
k=0 tk
K−1
(cid:88)
≤ (t −t )E∥s (X |T −t ,X )−s (X )∥2
k+1 k T−tk+1 T−tk+1 k T−tk T−tk+1 T−tk+1
k=0
=K (cid:88)−1
(t −t
)c2 T−tk+1(cid:16)
E∥X −m (X )∥2−E∥X −m (X
)∥2(cid:17)
. (13)
k+1 k σ4 0 T−tk T−tk 0 T−tk+1 T−tk+1
k=0 T−tk+1
Next, we will split the sum in (13) into two terms: (i) the sum for t ∈[0,T −1] and (ii) the sum for
k
t ∈ [T −1,T −δ]. In particular, the first term will sum over indices k = 0 to k = L−1, and the
k
second term will sum from k = L to k = K −1. The first term (i) can be bounded by a telecoping
argument as follows. We recall that L was chosen in Remark 2 so that t =T −1 and t −t =κ
L k+1 k
for k <L. Therefore for k ≤L, we have that (c2 /σ4 )≤1/σ4 ≤4. So, by telescoping we obtain
T−tk T−tk 1
L (cid:88)−1
(t −t
)c2 T−tk+1(cid:16)
E∥X −m (X )∥2−E∥X −m (X
)∥2(cid:17)
k+1 k σ4 0 T−tk T−tk 0 T−tk+1 T−tk+1
k=0 T−tk+1
≤4κE∥X −m (X )∥2 ≤4κ. (14)
0 T T
The last inequality follows from Assumption A combined with
(cid:90)
∥X −m (X )∥=∥ (X −y)µ (dy|X )∥≤ sup ∥X −y∥≤diamM ≤1,
0 t t 0 0|t t 0
M y∈M
whereµ (dy|x)isthelawof(X |X =x). Next,wedealwiththethesecondterm(ii)whichcorresponds
0|t 0 t
to the terms k =L to k =K−1. This term is bounded using the exponential partitioning of time gaps
6γ in [T −1,T −δ]. By the choices in Remark 2, we have T −t ≤1, and so (t −t )/σ4 ≤
k k k+1 k T−tk+1
4(t −t )/(T −t )2 ≤8κ/(T −t ). Also recalling that c ≤1 for all t, we obtain
k+1 k k+1 k+1 t
K (cid:88)−1
(t −t
)c2 T−tk+1(cid:16)
E∥X −m (X )∥2−E∥X −m (X
)∥2(cid:17)
k+1 k σ4 0 T−tk T−tk 0 T−tk+1 T−tk+1
k=L T−tk+1
K−1
≤8κ (cid:88) 1 (cid:16) E∥X −m (X )∥2−E∥X −m (X )∥2(cid:17)
T −t 0 T−tk T−tk 0 T−tk+1 T−tk+1
k+1
k=L
K−1
≤8κE∥X −m (X )∥2+8κ (cid:88) t k+1−t k E∥X −m (X )∥2
0 1 1 (T −t )(T −t ) 0 T−tk T−tk
k+1 k
k=L+1
K−1
≤8κ+16κ2 (cid:88) 1 E∥X −m (X )∥2. (15)
(T −t ) 0 T−tk T−tk
k
k=L+1
Finally, we bound E∥X −m (X )∥2 via the following lemma.
0 T−tk T−tk
Lemma 9. Let µ satisfy Assumptions A–C. Fix positive δ <1/4. Then for any t>δ
E∥X −m (X )∥2 ≲min(cid:2) 1,dt(cid:0) logδ−1+C(cid:1)(cid:3) .
0 t t
Combining (14) and (15) with Lemma 9, we obtain the following desired inequality
K (cid:88)−1(cid:90) tk+1
E∥s (X |T −t ,X )−s (X )∥2dt≲κ+dκ2(K−L)(cid:0) logδ−1+C(cid:1) .
T−t T−t k T−tk T−t T−t
k=0 tk
Remark 10. We return to explaining the nature of first-order correction in (4). In the proof, we control
the discretization error of the backwards SDE by controlling the difference in drifts s (x|t′,x′)−s (x),
t t
see (8). By construction, the differences are proportional to martingale increments m (x)−m (x) as
t t′
in (9). This allows us to use the orthogonality of martingale increments after that.
Benton et al. (2024) also leverage martingale properties of the score function. They note that M :=
t
e−(T−t)s (X ) is a martingale. However, since they consider a standard exponential integrator
T−t T−t
scheme, the discretization error which is given by the difference in drifts s (X )−s (X ) is
T−t T−t T−tk T−tk
a linear combination of a martingale increment (M −M ) and the score term s (X ). The last
t tk T−t T−t
term scales as the norm of a D-dimensional Gaussian noise vector. We avoid this problem by adjusting
the discretization coefficients to kill the second term. So, the difference is only a scaled martingale
increment. This enables bounds that are independent of D.
5 Conclusion & Future Work
In this work, we studied the iteration complexity of diffusion models under the manifold hypothesis.
Assuming that the data is supported on a d-dimensional manifold, we proved the first linear in d
iteration complexity bound w.r.t. KL divergence. Furthermore, we showed that this dependence is
optimal.
ThisresultisequivalenttoanO(d/K)convergencerateinKLwhereK isthenumberofdiscretization
(cid:112)
steps. A simple application of Pinkser’s inequality gives an O( d/K) bound for the total variation
(TV) distance as well.
Recently, Li and Yan (2024b) obtained an O(D/K) bound w.r.t TV distance in the non-manifold
case. This raises the interesting question of whether such a bound can be extended to the manifold
setting.
7Acknowledgements
GDwassupportedbytheEngineeringandPhysicalSciencesResearchCouncil[grantnumberEP/Y018273/1].
IAwassupportedbytheEngineeringandPhysicalSciencesResearchCouncil[grantnumberEP/T517811/1].
PPissupportedbytheEPSRCCDTinModernStatisticsandStatisticalMachineLearning(EP/S023151/1)
References
EddieAamariandClémentLevrard. Stabilityandminimaxoptimalityoftangentialdelaunaycomplexes
for manifold reconstruction, 2018.
Brian D.O. Anderson. Reverse-time diffusion equation models. Stochastic Processes and their Applica-
tions, 12(3):313–326, 1982. doi: https://doi.org/10.1016/0304-4149(82)90051-5.
IskanderAzangulov, GeorgeDeligiannidis, andJudithRousseau. Convergenceofdiffusionmodelsunder
the manifold hypothesis in high-dimensions, 2024. URL https://arxiv.org/abs/2409.18804.
JoeBenton,ValentinDeBortoli,ArnaudDoucet,andGeorgeDeligiannidis. Nearlyd-linearconvergence
bounds for diffusion models via stochastic localization, 2024.
Hongrui Chen, Holden Lee, and Jianfeng Lu. Improved analysis of score-based generative modeling:
User-friendly bounds under minimal smoothness assumptions, 2023a. URL https://arxiv.org/ab
s/2211.01916.
Sitan Chen, Sinho Chewi, Jerry Li, Yuanzhi Li, Adil Salim, and Anru R. Zhang. Sampling is as easy
as learning the score: theory for diffusion models with minimal data assumptions, 2023b. URL
https://arxiv.org/abs/2209.11215.
Prafulla Dhariwal and Alex Nichol. Diffusion models beat gans on image synthesis, 2021. URL
https://arxiv.org/abs/2105.05233.
Vincent Divol. Measure estimation on manifolds: an optimal transport approach, 2022.
Zach Evans, CJ Carr, Josiah Taylor, Scott H. Hawley, and Jordi Pons. Fast timing-conditioned latent
audio diffusion, 2024. URL https://arxiv.org/abs/2402.04825.
Herbert Federer. Curvature measures. Trans. Amer. Math. Soc., 93, 1959.
Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in
neural information processing systems, 33:6840–6851, 2020.
Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J.
Fleet. Video diffusion models, 2022. URL https://arxiv.org/abs/2204.03458.
Zahra Kadkhodaie, Florentin Guth, Eero P. Simoncelli, and Stéphane Mallat. Generalization in
diffusion models arises from geometry-adaptive harmonic representations, 2024. URL https:
//arxiv.org/abs/2310.02557.
Jean-Francois Le Gall. Brownian Motion, Martingales, and Stochastic Calculus. Springer Publishing
Company, Incorporated, 2018. ISBN 331980961X.
J.M. Lee. Introduction to Smooth Manifolds. Graduate Texts in Mathematics. Springer New York,
2013. ISBN 9780387217529.
Gen Li and Yuling Yan. Adapting to unknown low-dimensional structures in score-based diffusion
models, 2024a. URL https://arxiv.org/abs/2405.14861.
8Gen Li and Yuling Yan. o(d/t) convergence theory for diffusion probabilistic models under minimal
assumptions, 2024b. URL https://arxiv.org/abs/2409.18959.
Aaron Lou, Chenlin Meng, and Stefano Ermon. Discrete diffusion modeling by estimating the ratios
of the data distribution. In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller,
Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp, editors, Proceedings of the 41st International
Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pages
32819–32848.PMLR,21–27Jul2024. URLhttps://proceedings.mlr.press/v235/lou24a.html.
Kazusato Oko, Shunta Akiyama, and Taiji Suzuki. Diffusion models are minimax optimal distribution
estimators, 2023.
Jakiw Pidstrigach. Score-based generative models detect manifolds. In S. Koyejo, S. Mohamed,
A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing
Systems, volume 35, pages 35852–35865. Curran Associates, Inc., 2022.
Phillip Pope, Chen Zhu, Ahmed Abdelkader, Micah Goldblum, and Tom Goldstein. The intrinsic
dimension of images and its impact on learning, 2021. URL https://arxiv.org/abs/2104.08894.
Herbert E Robbins. An empirical bayes approach to statistics. Proceedings of the Third Berkeley
Symposium on Mathematical Statistics and Probability, 1956.
YangSong,JaschaSohl-Dickstein,DiederikPKingma,AbhishekKumar,StefanoErmon,andBenPoole.
Score-basedgenerativemodelingthroughstochasticdifferentialequations. InInternationalConference
on Learning Representations, 2021. URL https://openreview.net/forum?id=PxTIG12RRHS.
Rong Tang and Yun Yang. Adaptivity of diffusion models to manifold structures. In Proceedings of
The 27th International Conference on Artificial Intelligence and Statistics, volume 238 of Proceedings
of Machine Learning Research, pages 1648–1656. PMLR, 2024. URL https://proceedings.mlr.pr
ess/v238/tang24a.html.
Joseph L. Watson, David Juergens, Nathaniel R. Bennett, Brian L. Trippe, Jason Yim, Helen E.
Eisenach, Woody Ahern, Andrew J. Borst, Robert J. Ragotte, Lukas F. Milles, Basile I. M. Wicky,
Nikita Hanikel, Samuel J. Pellock, Alexis Courbet, William Sheffler, Jue Wang, Preetham Venkatesh,
Isaac Sappington, Susana Vázquez Torres, Anna Lauko, Valentin De Bortoli, Emile Mathieu, Sergey
Ovchinnikov, Regina Barzilay, Tommi S. Jaakkola, Frank DiMaio, Minkyung Baek, and David
Baker. De novo design of protein structure and function with rfdiffusion. Nature, 620, 2023. URL
10.1162/NECO_a_00142.
Andre Wibisono, Yihong Wu, and Kaylee Yingxi Yang. Optimal score estimation via empirical bayes
smoothing, 2024. URL https://arxiv.org/abs/2402.07747.
Yuchen Wu, Minshuo Chen, Zihao Li, Mengdi Wang, and Yuting Wei. Theoretical insights for diffusion
guidance: A case study for gaussian mixture models, 2024. URL https://arxiv.org/abs/2403.0
1639.
A Elements of Manifold Learning
We follow (Azangulov et al., 2024, Section 2.2) in defining the class of regular smooth manifolds of
interest. We give only the bare minimum details required to describe it. For a more comprehensive
discussion, see (Divol, 2022).
Werecallthatad-dimensionalmanifoldLee(2013)isatopologicalspaceM thatislocallyisomorphic
to an open subset of Rd. In other words, for each y ∈ M, there is an open set y ∈ U ⊆ M and a
y
9continuous function Φ :U →Rd such that Φ is a homeomorphism onto its image. The smoothness
y y y
of the manifold is defined as the smoothness of the functions Φ .
y
When a manifold M is embedded into RD, a key quantity (Federer, 1959) used to control the
regularity of the embedding is called the reach τ =τ(M) and is defined as
τ :=sup{ε:∀x∈Mε, ∃!y ∈M s.t. dist(x,M)=∥x−y∥},
where Mε ={x∈RD :dist(x,M)<ε} is the ε-neighborhood of M. Equivalently, τ is the supremum
over the radii of neighborhoods of M for which the projection is unique. The reach controls (Divol,
2022) both the global and local properties of the manifold.
In particular, the reach controls the scale at which M admits a natural smooth parameterization
Φ which will be used to control the smoothness of the manifold. More precisely, for a point y ∈M,
y
let π :=π be the orthogonal projection onto the tangent space T M ≃Rd at y. Then (Aamari
y TyM y
and Levrard, 2018) the restriction of π to M ∩ B (y,τ/4) is one-to-one and B (0,τ/8) ⊆
y RD
(cid:12)
TyM
π y(M ∩B D(y,τ/4)). Defining Φ y as the inverse of π y(cid:12) M∩B RD(y,τ/4), we have constructed a local
parameterization Φ :B (0,τ/8)→M of the manifold M at the point y.
y TyM
WeassumethattheΦ areinC2(B (0,τ/8)),anddefineL:=L(M):=sup ∥Φ ∥ .
y TyM y y C2(BTyM(0,τ/8))
From a geometric perspective, this allows us to compare tangent vectors at different points by applying
parallel transport which is defined in terms of a second-order differential operator. Finally, we define r
in Assumption C as r :=min(τ,L)/8.
B Proofs of Lemmas
Lemma 5. Define the filtration F :=σ(X :s≥t). The process {m (X )} is a martingale w.r.t.
t s t t t∈[0,T]
{F } . In particular, for t<t′
t t∈[0,T]
E[m (X )|X ]=m (X ). (7)
t t t′ t′ t′
Proof. Since X ∈L1, {E[X |F ]} is a Doob martingale. Since {X } is a Markov process,
0 0 t t∈[0,T] t t∈[0,T]
we have E[X |F ]=E[X |X ]=m (X ). This completes the proof.
0 t 0 t t t
Lemma 6. Let {M } be a square integrable martingale in RD w.r.t. a filtration {F } . Then for
t t≥0 t t≥0
any t <t <t we have
1 2 3
E∥M −M ∥2 =E∥M −M ∥2+E∥M −M ∥2.
t3 t1 t3 t2 t2 t1
Proof. We follow the proof of (Le Gall, 2018, Proposition 3.14) with minimal modifications for the case
when M takes values in RD.
t
E∥M −M ∥2 =E(cid:2)E(cid:2) ∥M −M ∥2|F (cid:3)(cid:3) =E(cid:2)E(cid:2) ∥M ∥2−2⟨M ,M ⟩+∥M ∥2|F (cid:3)(cid:3)
t2 t1 t2 t1 t1 t2 t2 t1 t1 t1
=E∥M ∥2−E∥M ∥2.
t2 t1
Applying the same calculation to E∥M −M ∥2 and E∥M −M ∥2, then summing up gives the
t3 t1 t3 t2
desired result.
Lemma 7.
KL(Yˆ ∥X )≲ε2 +De−2T
+K (cid:88)−1(cid:90) tk+1
E∥s (X |T −t ,X )−s (X )∥2dt. (8)
T−δ δ score T−t T−t k T−tk T−t T−t
k=0 tk
Proof. Weintroducetheprocess{Yˆ′} thatapproximatesthetruebackwardsprocess{Y }
t t∈[0,T−δ] t t∈[0,T−δ]
and is given by
 (cid:104) (cid:105) √
dYˆ′ = Yˆ′+2sˆ (Yˆ|T −t ,Yˆ ) dt+ 2dB , t∈[t ,t )
t t T−t t k tk t k k+1
Yˆ′ di =st. X .
0 T
10Yˆ′ followsthesamedynamicsastheprocessYˆ,butitisinitializedwiththetruedistributionYˆ′ di =st. X ,
t t 0 T
not by Gaussian noise Yˆ ∼N(0,Id ). By (Benton et al., 2024, Section 3.3) and the data-processing
0 D
inequality
KL(Yˆ ∥X )≤KL(Yˆ∥Y)=KL(Yˆ′∥Y)+KL(Y ∥N(0,Id )),
T−δ δ 0 D
where KL(X∥Y) is the KL-divergence between the path measures of processes X and Y. We bound the
terms separately. Combining Assumption B with (Benton et al., 2024, Proposition 5) we have
KL(Y ∥N(0,Id ))≲(cid:0) D+E∥X ∥2(cid:1) e−2T ≤(D+1)e−2T ≲De−2T. (16)
0 D 0
At the same time, by (Benton et al., 2024, Proposition 3), we have a Girsanov-like bound
KL(Yˆ′∥Y)≤K (cid:88)−1(cid:90) tk+1
E∥sˆ (X |T −t ,X )−s (X )∥2dt.
T−t T−t k T−tk T−t T−t
k=0 tk
By (Azangulov et al., 2024, Appendix F), this can be bounded as
K (cid:88)−1(cid:90) tk+1
E∥sˆ (X |T −t ,X )−s (X )∥2dt
T−t T−t k T−tk T−t T−t
k=0 tk
K (cid:88)−1(cid:90) tk+1
≤8ε2 +2 E∥s (X |T −t ,X )−s (X )∥2dt. (17)
score T−t T−t k T−tk T−t T−t
k=0 tk
Combining (16) and (17) completes the proof.
Lemma 8. For t <t <t′
1 2
E∥s (X |t′,X )−s (X )∥2 ≥E∥s (X |t′,X )−s (X )∥2.
t1 t1 t′ t1 t1 t2 t2 t′ t2 t2
Proof. We recall (10) which represents the error as a product of two terms. Since both terms are
positive, it is enough to prove that both terms are decreasing. First, a simple calculation shows that
for t≥0
d(cid:0) c2/σ4(cid:1)
=
d e−2t =−2e2t(e2t+1)
<0.
dt t t dt(1−e−2t)2 (e2t−1)3
So c2/σ4 is decreasing and c2 /σ4 ≥c2 /σ4 . Second, by Lemma 6, the second term is decreasing since
t t t1 t1 t2 t2
E∥m (X )−m (X )∥2 =E∥m (X )−m (X )∥2+E∥m (X )−m (X )∥2
t1 t1 t′ t′ t1 t1 t2 t2 t2 t2 t′ t′
≥E∥m (X )−m (X )∥2.
t2 t2 t′ t′
Combining these two statements, we get the lemma.
Lemma 9. Let µ satisfy Assumptions A–C. Fix positive δ <1/4. Then for any t>δ
E∥X −m (X )∥2 ≲min(cid:2) 1,dt(cid:0) logδ−1+C(cid:1)(cid:3) .
0 t t
(cid:82)
Proof. First, we note that m (x) = yµ (dy|x) where we recall that µ (dy|x) is the law of
t M 0|t 0|t
(X |X =x) and so
0 t
(cid:90)
X −m (X )= (X −y)µ (dy|X ).
0 t t 0 0|t t
M
Since diamM ≤ 1 and X ∈ M, a.s. we have that ∥X −m (X )∥ ≤ 1. Therefore, it is enough to
0 0 t t
consider the case t≤d−1 ≤1. In this case, (σ /c )2 ≃t. We also note that
t t
(cid:90)
∥X −m (X )∥2 ≤ ∥X −y∥2µ (dy|X ).
0 t t 0 0|t t
M
11As shown in (Azangulov et al., 2024, Theorem 15), if
(cid:113)
(cid:0) (cid:1)
r(t,δ,η)=2(σ /c ) 20d log (σ /c )−1+4C +8logδ−1+logη−1,
t t + t t
then with probability at least 1−δ
(cid:90)
µ (dy|X )≥1−η.
0|t t
y∈M:∥X0−y∥≤r(t,δ,η)
Integrating ∥X −y∥2 w.r.t. µ (dy|X ) we get that with probability at least 1−δ
0 0|t t
(cid:90)
∥X −m (X )∥2 ≤ ∥X −y∥2µ (dy|X )≤r2(t,δ,η)+η.
0 t t 0 0|t t
M
Taking an expectation w.r.t. both X and X we get
0 t
E∥X −m (X )∥2 ≤r2(t,δ,η)+η+δ.
0 t t
Choosing δ =η
=min(cid:0)
1,(σ /c
)2(cid:1)
, we have
t t
E∥X −m (X )∥2 ≤4(σ /c )2(cid:0) 20d(cid:0) log (σ /c )−1+4C(cid:1) +18log (σ /c )−1(cid:1) .
0 t t t t + t t + t t
Therefore
E∥X −m (X )∥2 ≲(σ /c )2d(cid:0) log (σ /c )−1+C(cid:1) .
0 t t t t + t t
As we mentioned, we can limit ourselves to the case t ≤ 1. For t ∈ [δ,1], we have (σ /c )2 ≃ t and
t t
log (σ /c )−1 ≲logδ−1. This shows that
+ t t
E∥X −m (X )∥2 ≲td(cid:0) logδ−1+C(cid:1) .
0 t t
12