Kaleidoscope: Learnable Masks for Heterogeneous
Multi-agent Reinforcement Learning
XinranLi LingPan JunZhang
DepartmentofElectronicandComputerEngineering
TheHongKongUniversityofScienceandTechnology
xinran.li@connect.ust.hk, lingpan@ust.hk, eejzhang@ust.hk
Abstract
Inmulti-agentreinforcementlearning(MARL),parametersharingiscommonly
employedtoenhancesampleefficiency. However, thepopularapproachoffull
parametersharingoftenleadstohomogeneouspoliciesamongagents,potentially
limitingtheperformancebenefitsthatcouldbederivedfrompolicydiversity. To
addressthiscriticallimitation,weintroduceKaleidoscope,anoveladaptivepartial
parametersharingschemethatfosterspolicyheterogeneitywhilestillmaintaining
highsampleefficiency. Specifically,Kaleidoscopemaintainsonesetofcommon
parametersalongsidemultiplesetsofdistinct,learnablemasksfordifferentagents,
dictatingthesharingofparameters.Itpromotesdiversityamongpolicynetworksby
encouragingdiscrepancyamongthesemasks,withoutsacrificingtheefficienciesof
parametersharing. ThisdesignallowsKaleidoscopetodynamicallybalancehigh
sampleefficiencywithabroadpolicyrepresentationalcapacity,effectivelybridging
thegapbetweenfullparametersharingandnon-parametersharingacrossvarious
environments. WefurtherextendKaleidoscopetocriticensemblesinthecontextof
actor-criticalgorithms,whichcouldhelpimprovevalueestimations. Ourempirical
evaluationsacrossextensiveenvironments,includingmulti-agentparticleenviron-
ment,multi-agentMuJoCoandStarCraftmulti-agentchallengev2,demonstratethe
superiorperformanceofKaleidoscopecomparedwithexistingparametersharing
approaches,showcasingitspotentialforperformanceenhancementinMARL.The
codeispubliclyavailableathttps://github.com/LXXXXR/Kaleidoscope.
1 Introduction
Cooperativemulti-agentreinforcementlearning(MARL)hasdemonstratedremarkableeffectiveness
insolvingcomplexreal-worlddecision-makingproblemsacrossvariousdomains,suchasresource
allocation(YingandDayong,2005),packagedelivery(SeukenandZilberstein,2007),autonomous
driving(Zhouetal.,2021),androbotcontrol(Swamyetal.,2020). Tomitigatethechallengesposed
bythenon-stationaryandpartiallyobservableenvironmentstypicalofMARL(Yuanetal.,2023),
thecentralizedtrainingwithdecentralizedexecution(CTDE)paradigm(Foersteretal.,2016)has
becomeprevalent, inspiringmanyinfluentialMARLalgorithmssuchasMADDPG(Loweetal.,
2017),COMA(Foersteretal.,2018),MATD3(Ackermannetal.,2019),QMIX(Rashidetal.,2020),
andMAPPO(Yuetal.,2022).
Under the CTDE paradigm, parameter sharing among agents is a commonly adopted practice to
improve sample efficiency. However, identical network parameters across agents often lead to
homogeneouspolicies,restrictingdiversityinbehaviorsandtheoveralljointpolicyrepresentational
capacity. Thislimitationcanresultinundesiredoutcomesincertainsituations(Christianosetal.,
2021;Fuetal.,2022;KimandSung,2023),asshowninFigure1,impedingfurtherperformance
38thConferenceonNeuralInformationProcessingSystems(NeurIPS2024).
4202
tcO
11
]GL.sc[
1v04580.0142:viXragains. Analternativeapproachisthenon-parametersharingscheme,whereeachagentpossessesits
ownuniqueparameters. Nevertheless,whilethismethodnaturallysupportsheterogeneouspolicies,
itsuffersfromreducedsampleefficiency,leadingtosignificanttrainingcosts. Thisisparticularly
problematicgiventhecurrenttrendtowardsincreasinglylargemodelsizes,withsomescalingto
trillionsofparameters(Zhaoetal.,2023;Achiametal.,2023). Therefore,itisimperativetodevelop
aparametersharingstrategythatenjoysbothhighsampleefficiencyandbroadpolicyrepresentational
capacity,potentiallyachievingsignificantlyenhancedperformance. Whileseveralefforts(Christianos
etal.,2021;KimandSung,2023)haveexploredpartialparametersharinginitiatedatthestartof
training,suchinitializationscanbechallengingtodesignwithoutdetailedknowledgeofagent-specific
environmentaltransitionsorrewardfunctions(Christianosetal.,2021).
In this work, we build upon insights from previous
studies(Christianosetal.,2021;Fuetal.,2022;Kimand Food
Sung,2023)andintroduceKaleidoscope,anoveladaptive
Forest
partial parameter sharing scheme. It maintains a single
setofpolicyparametersandemploysmultiplelearnable Obstacle
maskstodesignatethesharedparameters. Unlikeearlier
Prey
methodsthatdependonfixedinitializations,Kaleidoscope
dynamicallylearnsthesemasksalongsideMARLparam- Predator
eters throughout the training process. This end-to-end
training approach inherently integrates environmental
information,anditsadaptivenatureenablesKaleidoscope Figure 1: Full parameter sharing
todynamicallyadjustthelevelofparametersharingbased confinesthepoliciestobehomogeneous.
on the demands of the environment and the learning Inthisexample,allpredatorspursuethe
progressoftheagents. Thelearnablemasksfacilitatea same prey, neglecting another prey in
dynamicbalancebetweenfullparametersharingandnon- the game World. Further game details
parametersharing, offeringaflexibletrade-offbetween areinAppendixA.2.
sample efficiency and policy representational capacity
throughenhancedheterogeneity. Initially, webuildKaleidoscopeuponagentnetworks, whereit
achievesdiversepolicies. Followingthissuccess,weextendittomulti-agentactor-criticalgorithms
toencourageheterogeneityamongthecentralcriticensemblesforfurtherperformanceenhancement.
Justlikeakaleidoscopeusesthereflectivepropertiesofrotatingmirrorstotransformsimple
shapesintobeautifulpatterns,ourproposedmethodleverageslearnablemaskstomapasingle
setofparametersintodiversepolicies,therebyenhancingtaskperformance.
Wesummarizeourcontributionsasfollows:
• To enable policy heterogeneity among agents for better training flexibility, we adapt the soft
threshold reparameterization (STR) technique to learn distinct masks for different agent net-
workswhileonlymaintainingonesetofcommonparameters,effectivelybalancingbetweenfull
parametersharingandnon-parametersharingmechanisms.
• Toenhancepolicydiversityamongagents,weintroduceanovelregularizationtermthatencour-
agesthepairwisediscrepancybetweenmasks. Additionally,wedesignresettingmechanismsthat
recyclemaskedparameterstopreservetherepresentationalcapacityofthejointnetworks.
• ThroughextensiveexperimentsonMARLbenchmarks,includingmulti-agentparticleenviron-
ment (MPE) (Lowe et al., 2017), multi-agent MuJoCo (MAMuJoCo) (Peng et al., 2021) and
StarCraftmulti-agentchallengev2(SMACv2)(Ellisetal.,2024),wedemonstratethesuperior
performanceofKaleidoscopeoverexistingparametersharingapproaches.
2 Background
Multi-agent reinforcement learning (MARL) In MARL, a fully cooperative partially
observable multi-agent task is typically formulated as a decentralized partially observable
Markov decision process (dec-POMDP) (Oliehoek and Amato, 2016), represented by a tuple
M = ⟨S,A,P,R,Ω,O,N,γ⟩. Here, N denotesthenumberofagents, andγ ∈ (0,1]represents
thediscountfactor. Ateachtimestept,withtheenvironmentstateasst ∈S,agentireceivesalocal
observationot ∈ΩdrawnfromtheobservationfunctionO(st,i)andthenfollowsitslocalpolicy
i
2π toselectanactionat ∈ A. Individualactionsformajointactionat ∈ AN, leadingtoastate
i i
transitiontothenextstatest+1 ∼P(st+1|st,at)andinducingaglobalrewardrt =R(st,at). The
overallteamobjectiveistolearnthejointpoliciesπ = ⟨π ,...,π ⟩suchthattheexpectationof
1 N
discountedaccumulatedrewardGt =(cid:80) γtrtismaximized.
t
Tolearnsuchpoliciesπ ,variousMARLalgorithms(Loweetal.,2017;Foersteretal.,2018;Rashid
θ
etal.,2020;Yuetal.,2022)havebeendeveloped. Forinstance,theoff-policyactor-criticalgorithm
MATD3(Ackermannetal.,2019)servesasanexamplemethod. Specifically,thecriticnetworksare
updatedbyminimizingthetemporaldifference(TD)errorloss
L (ϕ)=E
(cid:104)(cid:0) yt−Q(st,at;ϕ)(cid:1)2(cid:105)
, (1)
c (st,ot,at,rt,st+1,ot+1)∼D
with
yt =rt+γ min Q(st+1,π (ot+1;θ′)+ϵ,...,π (ot+1;θ′ )+ϵ;ϕ ), (2)
1 1 1 N N N j
j=1,2
whereϕaretheparametersforcritics,θaretheparametersforactorpoliciesandθ′aretheparameters
fortargetactorpolicies. AndϵistheclippedGaussiannoise,givenasclip(N(0,σ),−c,c).
Thepolicyisupdatedbythedeterministicpolicygradientalgorithm(Silveretal.,2014)
(cid:104) (cid:105)
∇J(θ )=E ∇ π (ot;θ )∇ Q(st,a ,...,a | ;ϕ ) . (3)
i (st,ot,at,rt,st+1,ot+1)∼D θi i i i ai 1 N ai=πi(ot i;θi) 1
Softthresholdreparameterization(STR) Originallyintroducedinthecontextofmodelsparsifica-
tion,STR(Kusupatietal.,2020)isanunstructuredpruningmethodthatachievesnotableperformance
withoutrequiringapredeterminedsparsitylevel. Specifically,STRappliesatransformationtothe
originalparametersW asfollows
S (W,s)=sign(W)·ReLU(|W|−g(s)), (4)
g
wheresisalearnableparameter,α=g(s)servesasthepruningthreshold,andReLU(·)=max(·,0).
Theoriginalsupervisedlearningproblemmodeledby
minL(W;D) (5)
W
withDasthedataisnowtransferredto
minL(S (W,s);D). (6)
g
W,s
Overall,thisapproachoptimizesthelearnablepruningthresholdalongsidethemodelparameters,
facilitatingdynamicadjustmenttothesparsitylevelduringtraining.
3 LearnableMasksforHeterogenousMARL
Inthissection,weproposeusinglearnablemasksasalow-costmethodtoenablenetworkheterogene-
ityinMARL.Thecoreconcept,illustratedinFigure2,istolearnasinglesetofsharedparameters
complementedbymultiplemasksfordistinctagents,specifyingwhichparameterstoshare.
Specifically,inSection3.1,wefirstadaptSTRintoadynamicpartiallyparametersharingmethod,
unlocking the joint policy network’s capability to represent diverse policies among agents. In
Section3.2,weactivelyfosterpolicyheterogeneitythroughanovelregularizationtermbasedonthe
masks.Giventhatthemaskingtechniquecouldexcessivelysparsifythenetwork,potentiallydiminish-
ingitsrepresentationalcapacity,inSection3.3,weproposeastraightforwardremedytoperiodically
resettheparametersbasedontheoutcomesofmasking,whichadditionallymitigatesprimacybias.
Finally,inSection3.4,weexplorehowtofurtherextendthisapproachwithinthecriticcomponents
ofactor-criticalgorithmstoimprovevalueestimationsinMARLandfurtherboostperformance.
For the sake of clarity, we integrate the proposed Kaleidoscope with the MATD3 (Ackermann
etal.,2019)algorithmtodemonstratetheconceptwithinthissection. Nevertheless,asaversatile
partialparameter-sharingtechnique,ourmethodcanreadilybeadaptedtootherMARLalgorithms.
WedeferitsintegrationwithotherMARLframeworkstoAppendixA.1.2andwillevaluatethem
empiricallyinSection4.
3𝑄(#;𝜙!) Critic Ensembles 𝑄(#;𝜙#)
𝑄(#;𝜙!) … 𝑄(#;𝜙") … 𝑄(#;𝜙#)
⊙ = ⊙ =
𝜙$ ⊙ 𝑴!’ = 𝜙! Mean 𝜙$ ⊙ 𝑴#’ = 𝜙#
Actor 𝟏 𝜋!(#;𝜃!) Actor 𝒊 𝜋%(#;𝜃%) Actor 𝑵 𝜋&(#;𝜃&)
⊙ = … ⊙ = … ⊙ =
𝜃$ ⊙ 𝑴! = 𝜃! 𝜃$ ⊙ 𝑴% = 𝜃% 𝜃$ ⊙ 𝑴& = 𝜃&
Figure2: OverallnetworkarchitectureofKaleidoscope. Itmaintainsonesetofparametersθ with
0
N setsofmasks[M ]N foractornetworks,andonesetofparametersϕ withK setsofmasks
i i=1 0
(cid:2) Mc(cid:3)K
for critic ensemble networks, where N is the number of agents, K is the number of
j j=1
ensembles,and⊙denotestheHadamardproduct.
3.1 Adaptivepartialparametersharing♦
The core idea of this work is to learn distinct binary masks M for different agents to facilitate
i
differentiatedpolicies,ultimatelyaimingtoimproveMARLperformance. Toachievethis,weapply
theSTR(Kusupatietal.,2020)techniquetothepolicyparameterswithdifferentthresholdsdedicated
toeachagent:
θ =θ ⊙M , (7)
i 0 i
where θ parameterizes the policy for agent i, θ is the set of learnable parameters shared by all
i 0
(cid:104) (cid:105)
agents, and M is the learnable mask for agent i. Specifically, assume θ = θ(1),...,θ(Na) ,
i 0 0 0
(cid:104) (cid:105) (cid:104) (cid:105)
θ = θ(1),...,θ(Na) and M = m(1),...,m(Na) with N being the total parameter count
i i i i i i a
of an agent’s network. In line with STR, we compute each element m(k) of M as m(k) =
i i i
(cid:104) (cid:105)
1 |θ(k)|>σ(s(k)) ,whereσ(·)denotestheSigmoidfunction.
0 i
Thebenefitsofsuchacombinationaresummarizedasfollows:
• PreservationoforiginalMARLlearningobjectives: Unlikemostofthemethodsinpruning
literature, which primarily aim to minimize the discrepancies between pruned and unpruned
networksintermsofweights,loss,oractivations(Hoefleretal.,2021;Menghani,2023;Deng
etal.,2020),STRmaintainstheoriginalgoalofminimizingtask-specificloss,aligningdirectly
withourobjectivestoenhanceMARLperformance.
• Flexibilityinsparsity: Manyclassicalpruningmethodsrequirepredefinedper-layersparsity
levels(Evcietal.,2020;Ramanujanetal.,2020). Suchrequirementscancomplicateourdesign,
withthegoalnottogainextremesparsitybutrathertopromoteheterogeneitythroughmasking.
TheSTRtechniqueisidealinourcaseasitdoesnotrequirepredefiningsparsitylevels,allowing
foradaptivelearningofthemasks.
• Enhanced network representational capacity: Utilizing learnable masks for adap-
tive partial parameter sharing enhances the network’s representational capacity beyond
traditional full parameter sharing. In full parameter sharing, agents’ joint policies
are parameterized as πps(·|θ ) = ⟨π (·|θ ),...,π (·|θ )⟩. In contrast, our pro-
0 1 0 N 0
posed adaptive partial parameter sharing mechanism parameterizes the joint policies as
πKaleidoscope(·|θ ,M) = ⟨π (·|θ ⊙M ),...,π (·|θ ⊙M )⟩. Intheextremecasewhereall
0 1 0 1 n 0 N
thevaluesinM are1s,thefunctionsetrepresentedbyπKaleidoscope(·|θ ,M)degradestothat
i 0
ofπps(·|θ ). Inotherscenarios,itisasupersetofthatrepresentedbyπps(·|θ ).
0 0
4Actors Actors Critic Ensembles Critic Ensembles
𝜃! ⊙ 𝑴" = 𝜃" 𝜃! ⊙ 𝑴" = 𝜃" 𝜙! ⊙ 𝑴"# = 𝜙" 𝜙! ⊙ 𝑴"# = 𝜙"
𝜃! ⊙ 𝑴# = 𝜃# 𝜃! ⊙ 𝑴# = 𝜃# 𝜙! ⊙ 𝑴$# = 𝜙$ 𝜙! ⊙ 𝑴$# = 𝜙$
(a)Actors:reinitializetheweightsthataremaskedby (b)Criticensembles:resetonesetofmasksatatime.
allagentswithprobabilityρ.
Figure3: Illustrationonresettingmechanisms.
3.2 Policydiversityregularization♣
Whileindependentlylearnedmasksenableagentstodevelopdistinctpolicies,withoutaspecificin-
centive,thesepoliciesmaystillconvergetobeinghomogeneous. Tothisend,weproposetoexplicitly
encourageagentpolicyheterogeneitybyintroducingadiversityregularizationtermmaximizingthe
weightedpairwisedistancebetweennetworkmasks,whichisdefinedas
(cid:88) (cid:88)
Jdiv(s)= ∥θ ⊙(M −M )∥ . (8)
0 i j 1
i=1,...,nj=1,...,n
j̸=i
Thistermisinherentlynon-differentiableduetotheindicatorfunction1[·]insideM. Toovercome
thisdifficulty,followingestablishedpracticesintheliterature(Bengioetal.,2013;Alizadehetal.,
2018),weutilizeasurrogatefunctionforgradientapproximation:
∂Jdiv (cid:20) ∂Jdiv(cid:21)
=−tanh . (9)
∂g(s ) ∂M
i i
WeformallyprovidetheoveralltrainingobjectiveforactorsinAppendixA.1.1.
3.3 Periodicallyreset♠
As the training with masks proceeds, we observe an increasing sparsity in each agent’s network,
potentiallyreducingtheoverallnetworkcapacity. Toremedytheissue,weproposeasimpleapproach
to periodically reset the parameters that are consistently masked across all M with a certain
i
probabilityρ,whichisillustratedinFigure3a. Atintervalsdefinedbyt mod reset_interval==0,
iftheparameterindexksatisfies∀i,m(k) ==0,weapplythefollowingresettingrule
i
(cid:40)
Reinitialize[θ(k),s(k),...,s(k)] withprobabilityρ
θ(k),s(k),...,s(k) ← 0 1 N . (10)
0 1 N θ(k),s(k),...,s(k) withprobability1−ρ
0 1 N
This resetting mechanism recycles the weights masked as zeros by all the masks, preventing
the networks from becoming overly sparse. A side benefit of this resetting mechanism is the
enhancement of neural plasticity (Lyle et al., 2023; Nikishin et al., 2024), which helps alleviate
theprimacybias(Nikishinetal.,2022)inreinforcementlearning. Unlikemethodsthatreinitialize
entirelayersresultinginabruptperformancedrops(Nikishinetal.,2022),ourresettingapproach
selectivelytargetsweightsasindicatedbythelearnablemasks,thusavoidingsignificantperformance
disruptions,asshowninSection4.
3.4 Criticensembleswithlearnablemasks
Inactor-criticalgorithmframeworks,wefurtherapplyKaleidoscopetocentralcriticsasanefficient
waytoimplementensemble-likecritics. Byfacilitatingdynamicpartialparametersharing,Kalei-
doscopeenablesheterogeneityamongcriticensembles. Furthermore,byregularizingthediversity
5amongcriticfunctions,wecancontrolensemblevariances. Thisapproachiselaboratedinsubsequent
paragraphs.
♦ Adaptive partial parameter sharing for critic ensembles In the standard MATD3 algo-
rithm(Ackermannetal.,2019),twocriticswithindependentparametersaremaintainedtomitigate
overestimationrisks. However,usingseparateparameterstypicallyresultsinalowupdate-to-data
(UTD)ratio(Hiraokaetal.,2022). Toaddressthisissue,weproposetoenhancetheUTDratioby
employingKaleidoscopeparametersharingamongensemblesofcritics. Specifically,wemaintaina
singlesetofparametersϕ andK
masks(cid:2) Mc(cid:3)K
todistinguishthecriticfunctions,resultinginK
0 j j=1
ensembles[Q(·;ϕ )]K withϕ =ϕ ⊙Mc.
j j=1 j 0 j
Tobespecific,weupdatethecriticnetworksbyminimizingthetemporaldifference(TD)errorloss
L (ϕ )=E
(cid:104)(cid:0)
yt−Q(st,at;ϕ
)(cid:1)2(cid:105)
, (11)
c j (st,at,st+1)∼D j
with
yt =rt+γ min Q(st+1,π (ot+1;θ′)+ϵ,...,π (ot+1;θ′ )+ϵ;ϕ ). (12)
1 1 1 n N N j
j=1,...,K
Andthepoliciesareupdatedbythemeanestimationoftheensemblesas
 
K
1 (cid:88)(cid:104) (cid:105)
∇J(θ i)=E st∼D∇ θiπ i(ot i;θ i)∇
aiK
Q(st,a 1,...,a N|
ai=πi(ot
i;θi);ϕ j) . (13)
j=1
♣Criticensemblesdiversityregularization AsinSection3.2,wealsoapplydiversityregular-
izationtocriticmaskstopreventcriticsfunctionsfromcollapsingtoidenticalones. Thediversity
regularizationtomaximizeforthecriticensemblesisexpressedas
(cid:88) (cid:88)
Jdiv(sc)= ∥ϕ ⊙(Mc−Mc)∥ . (14)
c 0 i j 1
i=1,...,Kj=1,...,K
j̸=i
Intuitively,astrainingprogresses,thistermencouragesdivergenceamongthecriticmasks,leadingto
increasedmodelestimationuncertainty. Thisprocessfostersagradualshiftfromoverestimationto
underestimation. Asdiscussedinpriorresearch(Hiraokaetal.,2022;Lanetal.,2020;Chenetal.,
2021;Wangetal.,2021b),overestimationcanencourageexploration,beneficialinearlytraining
stages, whereas underestimation alleviates error accumulation (Fujimoto et al., 2018), which is
preferred in the late training stage. We formally provide the overall training objective for critic
ensemblesinAppendixA.1.1.
♠ Periodically reset To further promote diversity among critic ensembles and counteract the
reductioninnetworkcapacitycausedbymasking,weimplementaresettingmechanismsimilarto
thatdescribedinSection3.3. Inparticular,wesequentiallyreinitializethemasksMc followinga
j
cyclicpattern,asillustratedinFigure3b. Inthisway,eachcriticfunction’smaskistrainedondistinct
datasegments,leadingtodifferentbiases.
In summary, by adopting Kaleidoscope parameter sharing with learnable masks, we establish a
cost-effectiveimplementationforcriticensemblesthatenjoyahighUTDratio. Throughenforcing
distinctivenessamongthemasks,wesubtlycontrolthedifferencesamongcriticfunctions,thereby
improvingthevalueestimationsinMARL.
4 ExperimentalResults
Inthissection,weintegrateKaleidoscopewiththevalue-basedMARLalgorithmQMIXandthe
actor-criticMARLalgorithmMATD3,andevaluatethemacrosselevenscenariosinthreebenchmark
tasks.
6Table 1: Methods compared in the experiments. Here, “adaptive” indicates whether the sharing
schemeevolvesduringtraining.
Methods Paradigm Sharinglevel Adaptive Descriptions
NoPS Nosharing - No Agentshavedistinctparameters
FuPS Fullsharing Networks No Agentssharealltheparameters
FuPS+ID Fullsharing Networks No Agents share all the parameters
withagentIDsininput
SePS Partialsharing Networks No Agents are clustered to share pa-
rameterswithineachcluster
MultiH Partialsharing Layers No Agentssharealltheparametersex-
ceptfordistinctactionheads
SNP Partialsharing Neurons No Agents share specific neurons
basedonfixed,randompruning
Kaleidoscope Partialsharing Weights Yes Agentsshareparametersbasedon
distinct,learnablemasks
4.1 ExperimentalSetups
Environment descriptions We test our proposed Kaleidoscope on three benchmark tasks:
MPE(Loweetal.,2017),MaMuJoCo(Pengetal.,2021)andSMACv2(Ellisetal.,2024). Forthe
discretetasks MPEandSMACv2, weintegrate Kaleidoscopeandbaselines withQMIX(Rashid
et al., 2020) and assess the performance. For the continuous task MaMuJoCo, we employ
MATD3(Ackermannetal.,2019). WeusefiverandomseedsforMPEandMaMuJoCoandthree
randomseedsforSMACv2,reportingaveragedresultsanddisplayingthe95%confidenceinterval
with shaded areas. The chosen benchmark tasks reflect a mix of discrete and continuous action
spacesandbothhomogeneousandheterogeneousagenttypes,detailedfurtherinAppendixA.2.
Baselines Inthefollowing,wecompareourproposedKaleidoscopewithbaselines(Christianos
etal.,2021;KimandSung,2023),aslistedinTable1. ForbothKaleidoscopeandthebaselines,in
scenarioswithfixedagenttypes(MPEandMaMuJoCo),weassignonemaskperagent.ForSMACv2,
where agent types vary, we assign one mask per agent type. We use official implementations of
thebaselineswhereavailable;otherwise,wecloselyfollowthedescriptionsfromtheirrespective
papers,integratingthemintoQMIXorMATD3. Hyperparametersandfurtherdetailsareprovidedin
AppendixA.1.3.
4.2 Results
Performance WepresentthecomparativeperformanceofKaleidoscopeandbaselinesinFigure4
andFigure5. Overall,Kaleidoscopedemonstratessuperiorperformance,attributabletotheflexibility
ofthelearnablemasksandtheeffectivenessofdiversityregularization. Additionally,weobserve
that FuPS + ID generally outperforms NoPS, except for the Ant-v2-4x2 scenario (Figure 4c).
ThisadvantageislargelyduetoFuPS’shighersampleefficiency; asingletransitiondatasample
updatesthemodelparametersN timesinFuPS+ID,onceforeachagent,comparedtojustoncein
NoPS.Consequently,FuPS+IDmodelslearnfasterfromthesamenumberoftransitions. Similarly,
Kaleidoscopebenefitsfromthismechanismasitsharesweightsamongagents,allowingasingle
transition to update the model parameters multiple times. Furthermore, by integrating policy
heterogeneitythroughlearnablemasks,Kaleidoscopeenablesdiverseagentbehaviors,asillustratedin
thevisualizationresultsinFigure8. Ultimately,Kaleidoscopeeffectivelybalancesparametersharing
anddiversity,outperformingbothfullparametersharingandnon-parametersharingapproaches.
Costanalysis Despiteitssuperiorperformance,Kaleidoscopedoesnotincreasecomputationalcom-
plexityattesttimecomparedtothebaselines. WereportthetesttimeaveragedFLOPscomparisonof
KaleidoscopeandbaselinesinTable2. Weseethatduetothemaskingtechnique,Kaleidoscopehas
lowerFLOPscomparedtobaselines,therebyenjoyingafasterinferencespeedwhenbeingdeployed.
7Kaleidoscope SNP MultiH SePS FuPS + ID FuPS NoPS
4000
300 10 6000
3000
200 60000 4000
2000
100 −10 2000 1000
4000
0 −20 0 0
0 1 2 3 0 1 2 3 0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00
Timesteps 1e6 Timesteps 1e6 Timesteps 1e7 Timesteps 1e7
2000
(a)World (b)Push (c)Ant-v2-4x2 (d)Hopper-v2-3x1
8000 500
8000
6000
0 400
6000 0.0 0.2 0.4 06.0600 0.8 1.0
4000 Timesteps 1e7 300
4000 4000
200
2000 2000 2000 100
0 0 0 0
0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00
Timesteps 1e7 Timesteps 1e7 Timesteps 1e7 Timesteps 1e7
(e)Walker2D-v2-2x3 (f)Walker2D-v2-6x1 (g)HalfCheetah-v2-2x3 (h)Swimmer-v2-10x2
Figure4: PerformancecomparisonwithbaselinesonMPEandMaMuJoCobenchmarks.
50
Kaleidoscope
60 60 40 SNP
MultiH
40 40 30
SePS
20 FuPS + ID
20 20 FuPS
10
NoPS
0
0.0 0.2 0.4 0.6 0.8 1.0
0
0.0 0.2 0.4 0.6 0.8 1.0
0
0.0 0.2 0.4
0.650 00 .800
1.0
Timesteps 1e7 Timesteps 1e7 Timesteps 01.0e70 0.25 0.50 0.75 1.00
Timesteps 1e7
(a)Terran_5_vs_5 (b)Protoss_5_vs_5 (c)Zerg_5_vs_5
Figure5: PerformancecomparisonwithbaselinesonSMACv2benchmarks.
Ablationstudies WeconductablationstudiestoassesstheimpactofkeycomponentsinKaleido-
scope,withresultspresentedinFigure6. Specifically,wecompareKaleidoscopewiththreeablations:
1)Kaleidoscopew/oreg,whichlackstheregularizationterminEquation(8)thatencouragesthe
maskstobedistinct. 2)Kaleidoscopew/oreset,whichdoesnotresetparameters. 3)Kaleidoscope
w/oce,whichdoesnotuseKaleidoscopeparametersharingincriticensemblesandinsteadmaintains
twoindependentsetsofparametersforcritics. Fromtheresults,weobservethatdiversityregular-
izationcontributesthemosttotheperformanceofKaleidoscope. Withoutit,maskingdegradesthe
performanceduetothereducednumberofparametersineachpolicynetwork. Resettingprimarily
aidslearninginthelatestagesoftrainingwhenneeded,whichalignswiththeobservationmadeby
Nikishinetal.(2022). Notably,evenwithresetting,theperformancedoesnotexperienceabruptdrops
thankstotheguidanceprovidedbythemasksonwheretoreset. Whenablatingthecriticensembles
withKaleidoscopeparametersharing,weobserveinferiorperformancefromthebeginningofthe
training. ThisisbecausethecriticensembleswithKaleidoscopeparametersharingenableahigher
UTDratioofthecritics,asdiscussedinSection3.4.
Furthermore,weconductexperimentstostudytheimpactofmaskdesigns. Theresultsareshown
in Figure Figure 7. Specifically, we compare original Kaleidoscope with two alternative mask
designchoices: 1)Kaleidoscopew/neuronmasks,whereadaptivemaskingtechniquesareapplied
to neurons rather than weights. 2) Kaleidoscope w/ fixed masks, where the masks are initialized
atthebeginningoftrainingandkeptfixedthroughoutthelearningprocess. Theresultsshowthat
performancedropswitheitheralternativedesignchoice,demonstratingthatKaleidoscope’ssuperior
performanceoriginatesfromtheflexibilityofthelearnablemasksonweights.
MoreresultsonhyperparameteranalysisareincludedinAppendixB.2.
8
nruteR
tseT
nruteR
tseT
%
etaR
niW
tseT
nruteR
tsneTruteR
tseT
nruteR
tseT
%
etaR
niW
tseT
nruteR
tseT
nruteR
tseT
%
etaR
niW
tseT nruteR
tseT
nruteR
tseT
nruteR
tseTTable 2: Averaged FLOPs (with calculation methods detailed in Appendix A.3) across different
methods. ResultsarefirstnormalizedwithrespecttotheFuPS+IDmodelforeachscenarioand
thenaveragedacrossscenarioswithineachenvironment(detailedresultsinAppendixB.1). The
lowestcostsarehighlightedinbold.
Methods NoPS FuPS FuPS+ID SePS MultiH SNP Kaleidoscope
MPE 1.0x 0.992x 1.0x 1.0x 1.0x 0.988x 0.901x
MaMuJoCo 1.0x 0.985x 1.0x 1.0x 1.0x 0.900x 0.680x
SMACv2 1.0x 0.992x 1.0x 1.0x 1.0x 0.988x 0.890x
300 6000 300 6000
200 4000 200 4000
100 Kaleidoscope 2000 K Ka al le ei id do os sc co op pe e w/o reg 100 Kaleidoscope 2000 Kaleidoscope
Kaleidoscope w/o reg Kaleidoscope w/o reset Kaleidoscope w/ neuron masks Kaleidoscope w/ neuron masks
Kaleidoscope w/o reset Kaleidoscope w/o ce Kaleidoscope w/ fixed masks Kaleidoscope w/ fixed masks
0 0 0 0
0 1 2 3 0.00 0.25 0.50 0.75 1.00 0 1 2 3 0.00 0.25 0.50 0.75 1.00
Timesteps 1e6 Timesteps 1e7 Timesteps 1e6 Timesteps 1e7
(a)World (b)Ant-v2-4x2 (a)World (b)Ant-v2-4x2
Figure6: Ablationstudies. Figure7: Comparisononmaskdesigns.
Visualization WevisualizethetrainedpoliciesofKaleidoscopeonWorld,asshowninFigure8a.
Theagentsexhibitcooperativedivide-and-conquerstrategies(fourredagentsdivideintotwoteams
andsurroundthepreys),contrastingwiththehomogeneouspoliciesdepictedin Figure1. Wefurther
examinethedistinctionsintheagents’masksandpresenttheresultsinFigure8b. First,weobserve
thatbytheendofthetraining,eachagenthasdevelopedauniquemask,revealingthatdistinctmasks
facilitatediversepoliciesbyselectivelyactivatingdifferentsegmentsoftheneuralnetworkweights.
Second,throughoutthetrainingprocess,wenotethatthedifferencesamongtheagents’masksevolve
dynamically. ThisobservationconfirmsthatKaleidoscopeeffectivelyenablesdynamicparameter
sharing among the agents based on the learning progress, empowered by the adaptability of the
learnablemasks. MorevisualizationresultsareprovidedinAppendixB.3.
5 RelatedWork
Parametersharing FirstintroducedbyTan(1993),parametersharinghasbeenwidelyadoptedin
MARLalgorithms(Foersteretal.,2018;Rashidetal.,2020;Yuetal.,2022),duetoitssimplicity
andhighsampleefficiency(Grammeletal.,2020). However,schemeswithoutparametersharing
typicallyoffergreaterflexibilityforpolicyrepresentation. Tobalancesampleefficiencywithpolicy
representationalcapacity,someresearcheffortsaimtofindeffectivepartialparametersharingschemes.
Notably,SePS(Christianosetal.,2021)firstclustersagentsbasedontheirtransitionsatthestartof
trainingandrestrictsparametersharingwithintheseclusters. Subsequently,SNP(KimandSung,
2023)enablespartialparametersharingbyutilizingthelotterytickethypothesis(Suetal.,2020)
to initialize heterogeneous network structures. Concurrent to our work, AdaPS (Li et al., 2024)
combinesSNPandSePSbyproposingacluster-basedpartialparametersharingscheme. Whilethese
methods have shown promise in certain domains, their performance potential is often limited by
thestaticnatureoftheparametersharingschemessetearlyintraining. OurproposedKaleidoscope
distinguishesitselfbydynamicallylearningspecificparametersharingconfigurationsalongsidethe
developmentofMARLpolicies,therebyofferingenhancedtrainingflexibility.
AgentheterogeneityinMARL ToincorporateagentheterogeneityinMARLandenablediverse
behaviors among agents, previous methods have explored concepts such as diversity and roles.
Specifically,diversity-basedapproachesaimtoenhancepairwisedistinguishabilityamongagents
basedonidentities(JiangandLu,2021),trajectories(Lietal.,2021),orcreditsassignment(Liu
et al., 2023; Hu et al., 2023) through contrastive learning techniques. Concurrently, role-based
strategies,sometimesreferredtoasskills(Yangetal.,2020)orsubtasks(Yuanetal.,2022),employ
conditionalpoliciestodifferentiateagentsbyassigningthemtovariousconditions. Theseconditions
maybebasedonagentidentities(Yangetal.,2022),localobservations(Yangetal.,2020),local
9
nruteR
tseT
nruteR
tseT
nruteR
tseT
nruteR
tseTTimesteps = 1M Timesteps = 2M Timesteps = 3M
0.08
0.06
0.04
0.02
0.00
1 2 3 4 1 2 3 4 1 2 3 4
Agent Agent Agent
(a)Trainedpolicies. (b)Pairwisemaskdifferencesamongagentsatdifferenttrainingtimesteps.
Figure8: VisualizationonWorld.
histories (Wang et al., 2020, 2021a; Yuan et al., 2022) or joint histories (Liu et al., 2021; Iqbal
etal.,2022;Zengetal.,2023). Thislineofresearchesmainlyfocusonmoduledesignandoperate
separatelyfromparameter-leveladjustments,makingthemorthogonaltoourapproach. Nevertheless,
integratingthesemethodswithourworkcouldpotentiallyenhanceperformancefurther.
Sparsenetworksindeepreinforcementlearning(RL) Althoughrelativelyfew,therearesome
noteworthy recent attempts to find sparse networks for deep RL. In particular, PoPS (Livne and
Cohen,2020)prunesthedensenetworkspost-training,achievingsignificantlyreducedexecution
time complexity. Additionally, (Yu et al., 2020) validate the lottery ticket hypothesis within the
RL domain, producing high-performance models even under extreme pruning rates. Subsequent
efforts,includingDST(Sokaretal.,2022),TE-RL*(Graesseretal.,2022)andRLx2(Tanetal.,
2023)employtopologyevolution(TE)techniquestofurtherdecreasethetrainingcosts. Whilethese
developmentsutilizesparsetrainingtechniques,whicharesimilartothemethodsweemploy,their
primaryfocusisonreducingtrainingandexecutioncostsinsingle-agentsettings. Incontrast,our
workleveragessparsenetworkstrategiesasameanstoenhanceparametersharingtechniques,aiming
toimproveMARLperformance.
6 ConclusionsandFutureWork
Inthiswork,weintroducedKaleidoscope,anoveladaptivepartialparametersharingmechanismfor
MARL.Itleveragesdistinctlearnablemaskstofacilitatenetworkheterogeneity,applicabletoboth
agentpoliciesandcriticensembles. Specifically,Kaleidoscopeisbuiltonthreetechnicalcomponents:
STR-empoweredlearnablemasks,networkdiversityregularization,andaperiodicresettingmecha-
nism. Whenappliedtoagentpolicynetworks,Kaleidoscopebalancessampleefficiencyandnetwork
representationalcapacities. Inthecontextofcriticensembles, itimprovesvalueestimations. By
combiningourproposedKaleidoscopewithQMIXandMATD3,wehaveempiricallydemonstrated
itseffectivenessacrossvariousMARLbenchmarks. Thisstudyshowsgreatpromisesindeveloping
adaptivepartialparametersharingmechanismstoenhancetheperformanceofMARL.Forfuture
work, it is interesting to further extend Kaleidoscope to other domains such as offline MARL or
meta-RL.
Acknowledgements
ThisworkwassupportedbytheHongKongResearchGrantsCouncilundertheNSFC/RGCCollab-
orativeResearchSchemegrantCRS_HKUST603/22. Andwethanktheanonymousreviewersfor
theirvaluablefeedbackandsuggestions.
References
JoshAchiam,StevenAdler,SandhiniAgarwal,LamaAhmad,IlgeAkkaya,FlorenciaLeoniAleman,
DiogoAlmeida,JankoAltenschmidt,SamAltman,ShyamalAnadkat,etal.2023. GPT-4technical
report. arXivpreprintarXiv:2303.08774(2023).
Johannes Ackermann, Volker Gabler, Takayuki Osa, and Masashi Sugiyama. 2019. Reducing
overestimation bias in multi-agent domains using double centralized critics. arXiv preprint
arXiv:1910.01465(2019).
10
tnegA
4
3
2
1
4
3
2
1
4
3
2
1
DifferencesMiladAlizadeh,JavierFernández-Marqués,NicholasDLane,andYarinGal.2018. Anempirical
studyofbinaryneuralnetworks’optimisation.InProceedingsoftheInternationalConferenceon
LearningRepresentations.
YoshuaBengio,NicholasLéonard,andAaronCourville.2013. Estimatingorpropagatinggradients
throughstochasticneuronsforconditionalcomputation. arXivpreprintarXiv:1308.3432(2013).
XinyueChen,CheWang,ZijianZhou,andKeithW.Ross.2021. RandomizedEnsembledDouble
Q-Learning: LearningFastWithoutaModel.InProceedingsofthe9thInternationalConference
onLearningRepresentations.
FilipposChristianos,GeorgiosPapoudakis,ArrasyRahman,andStefanoV.Albrecht.2021. Scaling
Multi-AgentReinforcementLearningwithSelectiveParameterSharing.InProceedingsofthe38th
InternationalConferenceonMachineLearning,Vol.139.PMLR,1989–1998.
LeiDeng,GuoqiLi,SongHan,LupingShi,andYuanXie.2020. Modelcompressionandhardware
accelerationforneuralnetworks: Acomprehensivesurvey. Proc.IEEE108,4(2020),485–532.
BenjaminEllis,JonathanCook,SkanderMoalla,MikayelSamvelyan,MingfeiSun,AnujMahajan,
Jakob Foerster, and Shimon Whiteson. 2024. Smacv2: An improved benchmark for coopera-
tivemulti-agentreinforcementlearning.InAdvancesinNeuralInformationProcessingSystems,
Vol.36.
UtkuEvci,TrevorGale,JacobMenick,PabloSamuelCastro,andErichElsen.2020. Riggingthe
lottery: Makingallticketswinners.InProceedingsoftheInternationalConferenceonMachine
Learning.PMLR,2943–2952.
JakobFoerster,IoannisAlexandrosAssael,NandoDeFreitas,andShimonWhiteson.2016. Learning
tocommunicatewithdeepmulti-agentreinforcementlearning.InAdvancesinNeuralInformation
ProcessingSystems,Vol.29.
JakobFoerster,GregoryFarquhar,TriantafyllosAfouras,NantasNardelli,andShimonWhiteson.
2018. Counterfactualmulti-agentpolicygradients.InProceedingsoftheAAAIConferenceon
ArtificialIntelligence,Vol.32.
WeiFu,ChaoYu,ZelaiXu,JiaqiYang,andYiWu.2022. RevisitingSomeCommonPracticesin
CooperativeMulti-AgentReinforcementLearning.InProceedingsoftheInternationalConference
onMachineLearning.PMLR,6863–6877.
Scott Fujimoto, Herke van Hoof, and David Meger. 2018. Addressing Function Approximation
ErrorinActor-CriticMethods.InProceedingsofthe35thInternationalConferenceonMachine
Learning,Vol.80.PMLR,1582–1591.
LauraGraesser,UtkuEvci,ErichElsen,andPabloSamuelCastro.2022.Thestateofsparsetrainingin
deepreinforcementlearning.InProceedingsoftheInternationalConferenceonMachineLearning.
PMLR,7766–7792.
NathanielGrammel, SanghyunSon, BenjaminBlack, andAakritiAgrawal.2020. Revisitingpa-
rametersharinginmulti-agentdeepreinforcementlearning. arXivpreprintarXiv:2005.13625
(2020).
TakuyaHiraoka,TakahisaImagawa,TaiseiHashimoto,TakashiOnishi,andYoshimasaTsuruoka.
2022. DropoutQ-FunctionsforDoublyEfficientReinforcementLearning.InTheTenthInterna-
tionalConferenceonLearningRepresentations.
TorstenHoefler,DanAlistarh,TalBen-Nun,NikoliDryden,andAlexandraPeste.2021. Sparsityin
deeplearning: Pruningandgrowthforefficientinferenceandtraininginneuralnetworks. Journal
ofMachineLearningResearch22,241(2021),1–124.
JianHu,SiyangJiang,SethAustinHarding,HaibinWu,andShihweiLiao.2021. Rethinkingthe
ImplementationTricksandMonotonicityConstraintinCooperativeMulti-AgentReinforcement
Learning. (2021). arXiv:2102.03479[cs.LG]
11Zican Hu, Zongzhang Zhang, Huaxiong Li, Chunlin Chen, Hongyu Ding, and Zhi Wang. 2023.
Attention-Guided Contrastive Role Representations for Multi-Agent Reinforcement Learning.
arXivpreprintarXiv:2312.04819(2023).
Shariq Iqbal, Robby Costales, and Fei Sha. 2022. ALMA: Hierarchical learning for composite
multi-agenttasks.InAdvancesinNeuralInformationProcessingSystems,Vol.35.7155–7166.
Jiechuan Jiang and Zongqing Lu. 2021. The emergence of individuality. In Proceedings of the
InternationalConferenceonMachineLearning.PMLR,4992–5001.
WoojunKimandYoungchulSung.2023. ParameterSharingwithNetworkPruningforScalable
Multi-AgentDeepReinforcementLearning.InProceedingsofthe2023InternationalConference
onAutonomousAgentsandMultiagentSystems.1942–1950.
Aditya Kusupati, Vivek Ramanujan, Raghav Somani, Mitchell Wortsman, Prateek Jain, Sham
Kakade,andAliFarhadi.2020. Softthresholdweightreparameterizationforlearnablesparsity.In
ProceedingsoftheInternationalConferenceonMachineLearning.PMLR,5544–5555.
QingfengLan,YangchenPan,AlonaFyshe,andMarthaWhite.2020. MaxminQ-learning: Con-
trollingtheEstimationBiasofQ-learning.InProceedingsofthe8thInternationalConferenceon
LearningRepresentations,AddisAbaba,Ethiopia,April26-30,2020.
Chenghao Li, Tonghan Wang, Chengjie Wu, Qianchuan Zhao, Jun Yang, and Chongjie Zhang.
2021. Celebratingdiversityinsharedmulti-agentreinforcementlearning.InAdvancesinNeural
InformationProcessingSystems,Vol.34.3991–4002.
DapengLi,NaLou,BinZhang,ZhiweiXu,andGuoliangFan.2024. Adaptiveparametersharing
formulti-agentreinforcementlearning.InICASSP2024-2024IEEEInternationalConferenceon
Acoustics,SpeechandSignalProcessing(ICASSP).IEEE,6035–6039.
BoLiu,QiangLiu,PeterStone,AnimeshGarg,YukeZhu,andAnimaAnandkumar.2021. Coach-
playermulti-agentreinforcementlearningfordynamicteamcomposition.InProceedingsofthe
InternationalConferenceonMachineLearning.PMLR,6860–6870.
ShunyuLiu,YiheZhou,JieSong,TongyaZheng,KaixuanChen,TongtianZhu,ZunleiFeng,and
MingliSong.2023. Contrastiveidentity-awarelearningformulti-agentvaluedecomposition.In
ProceedingsoftheAAAIConferenceonArtificialIntelligence,Vol.37.11595–11603.
Dor Livne and Kobi Cohen. 2020. Pops: Policy pruning and shrinking for deep reinforcement
learning. IEEEJournalofSelectedTopicsinSignalProcessing14,4(2020),789–801.
RyanLowe,YiWu,AvivTamar,JeanHarb,PieterAbbeel,andIgorMordatch.2017. Multi-agent
actor-criticformixedcooperative-competitiveenvironments.InAdvancesinNeuralInformation
ProcessingSystems,Vol.30.
ClareLyle,ZeyuZheng,EvgeniiNikishin,BernardoAvilaPires,RazvanPascanu,andWillDabney.
2023. Understandingplasticityinneuralnetworks.InProceedingsoftheInternationalConference
onMachineLearning.PMLR,23190–23211.
GauravMenghani.2023. Efficientdeeplearning: Asurveyonmakingdeeplearningmodelssmaller,
faster,andbetter. Comput.Surveys55,12(2023),1–37.
EvgeniiNikishin, JunhyukOh, GeorgOstrovski, ClareLyle, RazvanPascanu, WillDabney, and
AndréBarreto.2024. Deepreinforcementlearningwithplasticityinjection.InAdvancesinNeural
InformationProcessingSystems,Vol.36.
EvgeniiNikishin,MaxSchwarzer,PierlucaD’Oro,Pierre-LucBacon,andAaronCourville.2022.
Theprimacybiasindeepreinforcementlearning.InProceedingsoftheInternationalConference
onMachineLearning.PMLR,16828–16847.
FransAOliehoekandChristopherAmato.2016. AconciseintroductiontodecentralizedPOMDPs.
Springer.
12GeorgiosPapoudakis,FilipposChristianos,LukasSchäfer,andStefanoV.Albrecht.2021. Bench-
markingMulti-AgentDeepReinforcementLearningAlgorithmsinCooperativeTasks.InPro-
ceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks.
http://arxiv.org/abs/2006.07869
Bei Peng, Tabish Rashid, Christian Schroeder de Witt, Pierre-Alexandre Kamienny, Philip Torr,
WendelinBöhmer,andShimonWhiteson.2021. Facmac: Factoredmulti-agentcentralisedpolicy
gradients.InAdvancesinNeuralInformationProcessingSystems,Vol.34.12208–12221.
VivekRamanujan,MitchellWortsman,AniruddhaKembhavi,AliFarhadi,andMohammadRastegari.
2020. What’shiddeninarandomlyweightedneuralnetwork?.InProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition.11893–11902.
TabishRashid,MikayelSamvelyan,ChristianSchroederDeWitt,GregoryFarquhar,JakobFoer-
ster,andShimonWhiteson.2020. Monotonicvaluefunctionfactorisationfordeepmulti-agent
reinforcementlearning. TheJournalofMachineLearningResearch21,1(2020),7234–7284.
SvenSeukenandShlomoZilberstein.2007. Improvedmemory-boundeddynamicprogramming
fordecentralizedPOMDPs.InProceedingsoftheTwenty-ThirdConferenceonUncertaintyin
ArtificialIntelligence.344–351.
David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller.
2014. Deterministicpolicygradientalgorithms.InProceedingsoftheInternationalConferenceon
MachineLearning.Pmlr,387–395.
GhadaSokar,ElenaMocanu,DecebalConstantinMocanu,MykolaPechenizkiy,andPeterStone.
2022. DynamicSparseTrainingforDeepReinforcementLearning.InProceedingsoftheThirty-
FirstInternationalJointConferenceonArtificialIntelligence.ijcai.org,3437–3443.
JingtongSu,YihangChen,TianleCai,TianhaoWu,RuiqiGao,LiweiWang,andJasonDLee.2020.
Sanity-checkingpruningmethods: Randomticketscanwinthejackpot.InAdvancesinNeural
InformationProcessingSystems,Vol.33.20390–20401.
Gokul Swamy, Siddharth Reddy, Sergey Levine, and Anca D Dragan. 2020. Scaled autonomy:
Enabling human operators to control robot fleets. In 2020 IEEE International Conference on
RoboticsandAutomation.IEEE,5942–5948.
MingTan.1993. Multi-agentreinforcementlearning: Independentvs.cooperativeagents.InPro-
ceedingsofthe10thInternationalConferenceonMachineLearning.330–337.
YiqinTan,PiheHu,LingPan,JiataiHuang,andLongboHuang.2023. RLx2: TrainingaSparse
DeepReinforcementLearningModelfromScratch.InProceedingsoftheEleventhInternational
ConferenceonLearningRepresentations.
Hang Wang, Sen Lin, and Junshan Zhang. 2021b. Adaptive Ensemble Q-learning: Minimizing
Estimation Bias via Error Feedback. In Advances in Neural Information Processing Systems.
24778–24790.
TonghanWang,HengDong,VictorLesser,andChongjieZhang.2020. ROMA:multi-agentrein-
forcementlearningwithemergentroles.InProceedingsofthe37thInternationalConferenceon
MachineLearning.9876–9886.
TonghanWang, TarunGupta, AnujMahajan, BeiPeng, ShimonWhiteson, andChongjieZhang.
2021a. RODE: Learning Roles to Decompose Multi-Agent Tasks. In Proceedings of the 9th
InternationalConferenceonLearningRepresentations.
JiachenYang, IgorBorovikov, andHongyuanZha.2020. HierarchicalCooperativeMulti-Agent
ReinforcementLearningwithSkillDiscovery.InProceedingsofthe19thInternationalConference
onAutonomousAgentsandMultiAgentSystems.1566–1574.
MingyuYang, JianZhao, XunhanHu, WengangZhou, JiangchengZhu, andHouqiangLi.2022.
LDSA:Learningdynamicsubtaskassignmentincooperativemulti-agentreinforcementlearning.
InAdvancesinNeuralInformationProcessingSystems,Vol.35.1698–1710.
13WangYingandSangDayong.2005. Multi-agentframeworkforthirdpartylogisticsinE-commerce.
ExpertSystemswithApplications29,2(2005),431–436.
ChaoYu,AkashVelu,EugeneVinitsky,JiaxuanGao,YuWang,AlexandreBayen,andYiWu.2022.
The surprising effectiveness of ppo in cooperative multi-agent games. In Advances in Neural
InformationProcessingSystems,Vol.35.24611–24624.
HaonanYu,SergeyEdunov,YuandongTian,andAriS.Morcos.2020.Playingthelotterywithrewards
andmultiplelanguages: lotteryticketsinRLandNLP.InProceedingsofthe8thInternational
ConferenceonLearningRepresentations.
LeiYuan,ChengheWang,JianhaoWang,FuxiangZhang,FengChen,CongGuan,ZongzhangZhang,
ChongjieZhang,andYangYu.2022. Multi-AgentConcentrativeCoordinationwithDecentralized
TaskRepresentation..InProceedingsoftheThirty-FirstInternationalJointConferenceonArtificial
Intelligence.599–605.
LeiYuan,ZiqianZhang,LiheLi,CongGuan,andYangYu.2023. ASurveyofProgressonCoopera-
tiveMulti-agentReinforcementLearninginOpenEnvironment. arXivpreprintarXiv:2312.01058
(2023).
Xianghua Zeng, Hao Peng, and Angsheng Li. 2023. Effective and stable role-based multi-agent
collaboration by structural information principles. In Proceedings of the AAAI Conference on
ArtificialIntelligence,Vol.37.11772–11780.
Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min,
BeichenZhang,JunjieZhang,ZicanDong,etal.2023. Asurveyoflargelanguagemodels. arXiv
preprintarXiv:2303.18223(2023).
YifanZhong,JakubGrudzienKuba,XidongFeng,SiyiHu,JiamingJi,andYaodongYang.2024.
Heterogeneous-agentreinforcementlearning. JournalofMachineLearningResearch25(2024),
1–67.
Ming Zhou, Jun Luo, Julian Villella, Yaodong Yang, David Rusu, Jiayu Miao, Weinan Zhang,
MontgomeryAlban,ImanFadakar,ZhengChen,etal.2021. Smarts: Anopen-sourcescalable
multi-agentrltrainingschoolforautonomousdriving.InProceedingsoftheConferenceonRobot
Learning.PMLR,264–285.
14A Experimentaldetails
A.1 Implementationdetails
A.1.1 KaleidoscopewithMATD3
Criticensembles WhenincorporatingKaleidoscopeintotheMATD3algorithm,theoveralltraining
lossforthecriticensemblesbecomes:
(cid:88) (cid:88)
Lall(ϕ ,sc)= Lall(ϕ )= L (ϕ ,sc)−αd·Jdiv(sc), (15)
c 0 c j c 0 j c
j=1,...,K j=1,...,K
withL (ϕ ,sc)beingtheoriginalMATD3lossgiveninEquation(11),Jdiv(sc)beingthediversity
c 0 j c
regularization given in Equation (14), and αd being a coefficient balancing the original MARL
objectiveandtheproposeddiversityregularization. NotethatalthoughJdiv(sc)containsparameters
c
ϕ ,westopthegradientsforϕ inJdiv(sc).
0 0 c
Intheimplementation,weapplylayer-wiseweightstothediversityregularizationtermJdiv(sc),
c
whichisdefinedas
(cid:88) (cid:88) (cid:88)
Jdiv(sc)= w · ∥ϕ ⊙(Mc −Mc )∥ , (16)
c l 0 i,l j,l 1
l=1,...,L i=1,...,Kj=1,...,K
j̸=i
where l denotes the layer index of the neuron networks, L represents the total number of layers,
Mc isthemaskforagentiatlayerl,andthelayer-wiseweightsaresetasw =2l. Theintuition
i,l l
behindthischoiceisthatfeaturesclosertotheoutputtendtobemorecompact(Kusupatietal.,2020);
consequently,assigninglargerregularizationweightstotheselayersmayhaveamoresignificant
impactontheoutputactiondecisions. Ourinitialexperimentsempiricallydemonstratethatsetting
w = 2l improvesperformancecomparedtothecasewherew = 1. Basedonthesefindings,we
l l
maintainthisdesignchoicethroughoutallourexperiments.
Inpractice,weadaptivelyadjustαdwhilemaintainingaconstantratiobetweentheMATD3lossand
thediversityloss,whichistreatedasahyperparameter:
|(cid:80) L (ϕ ,sc)|
αd = j=1,...,K c 0 j ·α, (17)
|Jdiv(sc)|
c
whereαisahyperparameter,andthegradientsfor
|(cid:80) j=1,...,KLc(ϕ0,sc j)|
arestopped.
|Jdiv(sc)|
c
Actors Fortheactors,thetrainingobjectiveistomaximizethefollowingterm
(cid:88) (cid:88)
Jall(θ ,s)= Jall(θ )= J(θ ,s )+βd·Jdiv(s), (18)
0 i 0 i
i=1,...,n i=1,...,n
whereJ(θ ,s )istheoriginalactorobjectivedefinedinEquation(3),Jdiv(s)isthediversityregu-
0 i
larizationgiveninEquation(8)withlayer-wiseweightsasinEquation(16),βdistheregularization
coefficient. Thevalueofβdisdeterminedby
(cid:80)
| J(θ ,s )|
βd = i=1,...,n 0 i ·β, (19)
|Jdiv(s)|
whereβ isaconstanthyperparameter,similartotheapproachusedinEquation(17)forthecritic
ensembles.
A.1.2 KaleidoscopewithQMIX
WhenincorporatingKaleidoscopeintotheQMIXalgorithm(Rashidetal.,2020),weapplyKalei-
doscopeparametersharingonlytothelocalQnetworks. Consequently,thetraininglossisdefined
as:
Lall(θ ,s)=L(θ ,s)−βd·Jdiv(s), (20)
0 0
where
L(θ ,s)=E
(cid:104)(cid:0)
ytot−Q (st,ot,at;θ
,s)(cid:1)2(cid:105)
, (21)
0 (st,ot,at,rt,st+1,ot+1)∼D tot 0
with ytot = r +γmax Q (st+1,ot+1,a;θ−) and θ− representing the parameters of a target
a tot
networkasinDQN.
15A.1.3 Networkarchitectureandhyperparameters
Codebase OurimplementationofKaleidoscopeandbaselinealgorithmsarebasedonthefollowing
codebase:
• HARL (Zhong et al., 2024) (MATD3 implementation): https://github.com/PKU-MARL/
HARL
• EPyMARL(Papoudakisetal.,2021)(QMIXimplementationforMPE):
https://github.com/uoe-agents/epymarl
• PyMARL2(Huetal.,2021)(QMIXimplementationforSMACv2):
https://github.com/benellis3/pymarl2
• SePS(Christianosetal.,2021): https://github.com/uoe-agents/seps
ThecodeforKaleidoscopeispubliclyavailableathttps://github.com/LXXXXR/Kaleidoscope.
Networkarchitecture Inlinewithpriorworks(Zhongetal.,2024;Papoudakisetal.,2021;Hu
etal.,2021),weemploydeepneuralnetworksconsistingofmultilayerperceptrons(MLPs)with
rectifiedlinearunit(ReLU)activationfunctionsandgatedrecurrentunits(GRUs)toparameterizethe
actorandcriticnetworks. Moreover,whenthemaskingtechniqueisappliedtocriticensembles,we
incorporatelayernormalizationbetweentheMLPlayersandReLUactivations(Hiraokaetal.,2022).
InKaleidoscope,themaskingtechniqueisappliedtotheMLPlayers,andtheresettingmechanisms
describedinSections3.3and3.4areappliedtothelastthreelayersoftherespectiveneuralnetworks,
followingNikishinetal.(2022).
Hyperparameters Toensureafaircomparison,weimplementourmethodandallthebaselines
using the same codebase with the same set of hyperparameters, with the exception of method-
specificones. ThecommonhyperparametersarelistedinTables3to5. TheKaleidoscope-specific
hyperparametersareprovidedinTable6.
Table3: CommonhyperparametersusedforMATD3intheMaMuJoCodomain.
Hyperparameter Value
Numberoflayers 3
Hiddensizes 256
Discountfactorγ 0.99
Rolloutthreads 10
Criticlr 1×10−3
Actorlr 5×10−4
Explorationnoise 0.1
Batchsize 1000
Replaybuffersize 1×106
Numberofenvironmentsteps 10×106
n_step1 (5,10,20)
1 Hereweadopttheper-scenariofinetuned
valueforthishyperparameterasprovided
byHARL.
A.2 Environmentaldetails
Codebase TheenvironmentsusedinthisworkarelistedbelowwithdescriptionsinTable7.
• MaMuJoCo (Peng et al., 2021): https://github.com/schroederdewitt/multiagent_
mujoco
• MPE (Lowe et al., 2017; Papoudakis et al., 2021): https://github.com/semitable/
multiagent-particle-envs
• SMACv2(Ellisetal.,2024): https://github.com/oxwhirl/smacv2
16Table4: CommonhyperparametersusedforQMIXintheMPEdomain.
Hyperparameter Value
Numberoflayers 5
Hiddensizes 64
Discountfactorγ 0.99
Lr 5×10−4
Initialϵ 1.0
Finalϵ 0.05
Batchsize 32
Replaybuffersize 5000
Numberofenvironmentsteps 3×106
DoubleQ True
Table5: CommonhyperparametersusedforQMIXintheSMACv2domain.
Hyperparameter Value
Numberoflayers 5
Hiddensizes 64
Discountfactorγ 0.99
Lr 1×10−3
Initialϵ 1.0
Finalϵ 0.05
Batchsize 128
Replaybuffersize 5000
Numberofenvironmentsteps 10×106
DoubleQ False
MPE Weextendthescenariosettingsprovidedintheoriginalcodebasetoincreasethecomplexity
andchallengeofthetasks. InWorld,wesetthenumberofpredators(agents)to4,thenumberof
preyto2,thenumberofobstaclesto1,andthenumberofforeststo2. Theobjectiveofthegameis
forthepredatorstoapproachthepreywhileavoidingcollisionswithobstacles. Thepreyisattracted
tothefoodandcanhidefromthepredatorsintheforests. InPush,wesetthenumberofagentsto5,
thenumberofadversariesto2,andthenumberoflandmarksto2. Thegoalofthegameisforthe
agentstopushtheadversariesawayfromthelandmarks. Inbothscenarios,wepretraintheadversary
(prey)policiesusingtheMADDPGalgorithmLoweetal.(2017)andusethesepretrainedpoliciesto
testtheperformanceofdifferentalgorithms
A.3 FLOPscalculation
Tocalculatethenumberoffloating-pointoperations(FLOPs)forasingleforwardpassofasparse
model, we sum the total number of multiplications and additions layer by layer, following the
approachinEvcietal.(2020). Forafully-connectedlayer,theFLOPsarecomputedasfollows:
FLOPs=2×(1−Sparsity)×In×Out. (22)
ForaGRUcell,theFLOPsarecomputedas:
FLOPs=2×(3×Hidden2+3×In×Hidden+13×Hidden). (23)
A.4 ExperimentalInfrastructure
TheexperimentsontheSMACv2benchmarkwereconductedusingNVIDIAGeForceRTX3090
GPUs,whiletheexperimentsonotherbenchmarkswereperformedusingNVIDIAGeForceRTX
3080GPUs. Eachexperimentalrunrequiredlessthan2daystocomplete.
17Table6: HyperparametersusedforKaleidoscope.
Hyperparameter Environment Value
MaMuJoCo 0.1
Actordiversitycoefficientβ MPE 0.5
SMACv2 5.0
MaMuJoCo 0.5
Actorsresetprobabilityρ MPE 0.1
SMACv2 0.2
MaMuJoCo 1×106
Actorresetinterval MPE 200×103
SMACv2 1×106
NumberofcriticensemblesK MaMuJoCo 5
Criticensemblesdiversitycoefficientα MaMuJoCo 0.1
Criticresetinterval MaMuJoCo 800×103
Table7: Environmentsdetails.
Environment Actionspace Agenttypes Scenarios Numberofagents
Ant-v2-4x2 4
Hopper-v2-3x1 3
Walker2D-v2-2x3 2
MaMuJoCo Continuous Heterogeneous,fixed
Walker2D-v2-6x1 6
HalfCheetah-v2-2x3 2
Swimmer-v2-10x2 10
World 4
MPE Discrete Homogeneous,fixed
Push 5
Terran_5_vs_5 5
SMACv2 Discrete Heterogeneous,dynamic Protoss_5_vs_5 5
Zerg_5_vs_5 5
B Moreresultsanddiscussion
B.1 DetailedCosts
Weprovideper-scenarioFLOPsacrossdifferentmethodsinTable8asasupplementforTable2.
Table8: AveragedFLOPsfordifferentmethods. Resultsarenormalizedw.r.t. theFuPS+IDmodel.
Thelowestcostsarehighlightedinbold.
Scenarios NoPS FuPS FuPS+ID SePS MultiH SNP Kaleidoscope
World 1.0x 0.993x 1.0x 1.0x 1.0x 0.988x 0.897x
Push 1.0x 0.991x 1.0x 1.0x 1.0x 0.988x 0.904x
Ant-v2-4x2 1.0x 0.990x 1.0x 1.0x 1.0x 0.900x 0.640x
Hopper-v2-3x1 1.0x 0.989x 1.0x 1.0x 1.0x 0.900x 0.721x
Walker2D-v2-2x3 1.0x 0.992x 1.0x 1.0x 1.0x 0.900x 0.731x
Walker2D-v2-6x1 1.0x 0.979x 1.0x 1.0x 1.0x 0.900x 0.763x
HalfCheetah-v2-2x3 1.0x 0.993x 1.0x 1.0x 1.0x 0.900x 0.614x
Swimmer-v2-10x2 1.0x 0.968x 1.0x 1.0x 1.0x 0.900x 0.611x
Terran_5_vs_5 1.0x 0.992x 1.0x 1.0x 1.0x 0.988x 0.890x
Protoss_5_vs_5 1.0x 0.992x 1.0x 1.0x 1.0x 0.988x 0.895x
Zerg_5_vs_5 1.0x 0.992x 1.0x 1.0x 1.0x 0.988x 0.885x
188000
8000
6000
6000
4000 α=10 4000 β=10
α=1 β=1
2000 α=0.1 2000 β=0.1
α=0.01 β=0.01
0 0
0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00
Timesteps 1e7 Timesteps 1e7
(a)Criticensemblesdiversitycoefficientα. (b)Actorsdiversifycoefficientβ.
Figure9: Hyperparameteranalysis.
B.2 HyperparameterAnalysis
Weconductfurtheranalysisonthehyperparametersαandβ,andpresenttheresultsinFigure9. The
hyperparameterαcontrolsthevarianceofthecriticensembles. AsshowninFigure9a,weobserve
thatanexcessivelysmallαresultsindegradedperformancebecauseitreducesthecriticensemblesto
asinglecriticnetwork,causingthevalueestimationtosufferfromsevereoverestimation. Conversely,
anexcessivelylargeαalsodeterioratesperformance,possiblyduetoincreasedestimationbias. For
β,asillustratedinFigure9b,anoverlysmallβ leadstodegradedperformancebecauseitreducesthe
Kaleidoscopeparametersharingtofullparametersharing,confiningthepoliciestobeidentical. An
overlylargeβ alsonegativelyimpactsperformance,asitmaycausethetrainingobjectivetodeviate
toomuchfromminimizingtheoriginalMARLloss.
Ingeneral,werecommendsettingbothhyperparametersbetween0.1and1. However,theoptimal
hyperparametervaluesmayvaryacrossdifferentscenarios. Forfaircomparisons,wemaintainthe
samesetofhyperparametersacrossallscenariosinourexperiments. Nevertheless,furthertuningof
thesehyperparametershasthepotentialtoenhanceperformance.
B.3 FurtherVisualizationResults
TobetterunderstandhowlearnablemasksinKaleidoscopeaffecttheperformancethroughpolicies,
we visualize the pairwise mask differences among agents and the agent trajectories at different
training stages in Figure 10. As training progresses, the test return increases and diversity loss
decreases,indicatingbetterperformanceandgreaterdiversityamongagentpolicies. Correspondingly,
maskdifferencesamongagentsincrease,andtheagenttrajectorydistributionbecomesmorediverse.
B.4 Limitations
HerewediscusssomelimitationsofKaleidoscope.
First, as suggested by results in Appendix B.2, the optimal hyperparameters vary from scenario
to scenario. Therefore, using the same hyperparameters across all scenarios may not yield the
bestperformanceforKaleidoscope. Developinganautomaticschemethatutilizesenvironmental
informationtodeterminethesehyperparameterswouldbebeneficial.
Second,astheenvironmentsusedinthisworkcontainnomorethan10agents,weassignadistinct
maskforeachagent. However,whentheproblemscalestohundredsofagents,thisvanillaimplemen-
tationmayfail. Insuchcases,apossibleapproachistoclusterN agentsintoK (K <N)groups
andtrainK maskswithKaleidoscope. Thiswouldreducecomputationalcostsandachieveabetter
trade-offbetweensampleefficiencyanddiversity. Withinthesamegroup,agentsshareallparameters,
whileagentsfromdifferentgroupsshareonlypartialparameters. Techniquesforclusteringagents
basedonexperience,asproposedbyChristianosetal.(2021),couldbeuseful.
19
nruteR
tseT
nruteR
tseT(cid:5)(cid:4)(cid:5)(cid:5)(cid:5)
(cid:7)(cid:5)(cid:5) (cid:14)(cid:15)(cid:22)(cid:23)(cid:3)(cid:13)(cid:15)(cid:23)(cid:24)(cid:21)(cid:18) (cid:27)(cid:5)(cid:4)(cid:5)(cid:5)(cid:9)
(cid:11)(cid:16)(cid:25)(cid:15)(cid:21)(cid:22)(cid:16)(cid:23)(cid:26)(cid:3)(cid:12)(cid:19)(cid:22)(cid:22)
(cid:27)(cid:5)(cid:4)(cid:5)(cid:6)(cid:5)
(cid:5)
(cid:5)(cid:4)(cid:5) (cid:5)(cid:4)(cid:9) (cid:6)(cid:4)(cid:5) (cid:6)(cid:4)(cid:9) (cid:7)(cid:4)(cid:5) (cid:7)(cid:4)(cid:9) (cid:8)(cid:4)(cid:5)
(cid:14)(cid:16)(cid:17)(cid:15)(cid:22)(cid:23)(cid:15)(cid:20)(cid:22) (cid:6)(cid:15)(cid:10)
T= 0 T= 1M T= 2M T= 3M
Pairwise Mask Differences
Agent Trajectories t-SNE
Figure10: FurthervisualizationonWorld.
NeurIPSPaperChecklist
1. Claims
Question: Dothemainclaimsmadeintheabstractandintroductionaccuratelyreflectthe
paper’scontributionsandscope?
Answer: [Yes]
Justification: Theabstractandintroductionaccuratelyreflectthepaper’scontributionsby
proposing a novel parameter sharing technique in MARL (Section 3) and claiming its
superiorperformancecomparedtoexistingmethods,whichissupportedbytheexperimental
results(Section4).
Guidelines:
• TheanswerNAmeansthattheabstractandintroductiondonotincludetheclaimsmade
inthepaper.
• The abstract and/or introduction should clearly state the claims made, including the
contributionsmadeinthepaperandimportantassumptionsandlimitations. ANoorNA
answertothisquestionwillnotbeperceivedwellbythereviewers.
20
(cid:18)(cid:21)(cid:24)(cid:23)(cid:15)(cid:13)(cid:3)(cid:23)(cid:22)(cid:15)(cid:14)
1
tnegA
2
tnegA
3
tnegA
4
tnegA
(cid:22)(cid:22)(cid:19)(cid:12)(cid:3)(cid:26)(cid:23)(cid:16)(cid:22)(cid:21)(cid:15)(cid:25)(cid:16)(cid:11)• The claims made should match theoretical and experimental results, and reflect how
muchtheresultscanbeexpectedtogeneralizetoothersettings.
• Itisfinetoincludeaspirationalgoalsasmotivationaslongasitisclearthatthesegoals
arenotattainedbythepaper.
2. Limitations
Question: Doesthepaperdiscussthelimitationsoftheworkperformedbytheauthors?
Answer: [Yes]
Justification: PleaseseeAppendixB.4.
Guidelines:
• TheanswerNAmeansthatthepaperhasnolimitationwhiletheanswerNomeansthat
thepaperhaslimitations,butthosearenotdiscussedinthepaper.
• Theauthorsareencouragedtocreateaseparate"Limitations"sectionintheirpaper.
• Thepapershouldpointoutanystrongassumptionsandhowrobusttheresultsareto
violations of these assumptions (e.g., independence assumptions, noiseless settings,
modelwell-specification,asymptoticapproximationsonlyholdinglocally). Theauthors
should reflect on how these assumptions might be violated in practice and what the
implicationswouldbe.
• Theauthorsshouldreflectonthescopeoftheclaimsmade,e.g.,iftheapproachwasonly
testedonafewdatasetsorwithafewruns. Ingeneral,empiricalresultsoftendependon
implicitassumptions,whichshouldbearticulated.
• Theauthorsshouldreflectonthefactorsthatinfluencetheperformanceoftheapproach.
Forexample,afacialrecognitionalgorithmmayperformpoorlywhenimageresolution
isloworimagesaretakeninlowlighting. Oraspeech-to-textsystemmightnotbeused
reliablytoprovideclosedcaptionsforonlinelecturesbecauseitfailstohandletechnical
jargon.
• Theauthorsshoulddiscussthecomputationalefficiencyoftheproposedalgorithmsand
howtheyscalewithdatasetsize.
• Ifapplicable,theauthorsshoulddiscusspossiblelimitationsoftheirapproachtoaddress
problemsofprivacyandfairness.
• Whiletheauthorsmightfearthatcompletehonestyaboutlimitationsmightbeusedby
reviewersasgroundsforrejection,aworseoutcomemightbethatreviewersdiscover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgmentandrecognizethatindividualactionsinfavoroftransparencyplayanimportant
roleindevelopingnormsthatpreservetheintegrityofthecommunity. Reviewerswill
bespecificallyinstructedtonotpenalizehonestyconcerninglimitations.
3. TheoryAssumptionsandProofs
Question: Foreachtheoreticalresult,doesthepaperprovidethefullsetofassumptionsand
acomplete(andcorrect)proof?
Answer: [NA]
Justification: Thispaperdoesnotincludetheoreticalresults.
Guidelines:
• TheanswerNAmeansthatthepaperdoesnotincludetheoreticalresults.
• All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
• Allassumptionsshouldbeclearlystatedorreferencedinthestatementofanytheorems.
• Theproofscaneitherappearinthemainpaperorthesupplementalmaterial,butifthey
appearinthesupplementalmaterial,theauthorsareencouragedtoprovideashortproof
sketchtoprovideintuition.
• Inversely,anyinformalproofprovidedinthecoreofthepapershouldbecomplemented
byformalproofsprovidedinappendixorsupplementalmaterial.
• TheoremsandLemmasthattheproofreliesuponshouldbeproperlyreferenced.
4. ExperimentalResultReproducibility
21Question: Doesthepaperfullydisclosealltheinformationneededtoreproducethemainex-
perimentalresultsofthepapertotheextentthatitaffectsthemainclaimsand/orconclusions
ofthepaper(regardlessofwhetherthecodeanddataareprovidedornot)?
Answer: [Yes]
Justification: ThemethodandimplementationdetailsareprovidedinSection3andAp-
pendixA.1,andtheexperimentsettingsandenvironmentdetailsaredescribedinSection4
andAppendixA.2.
Guidelines:
• TheanswerNAmeansthatthepaperdoesnotincludeexperiments.
• Ifthepaperincludesexperiments,aNoanswertothisquestionwillnotbeperceived
wellbythereviewers:Makingthepaperreproducibleisimportant,regardlessofwhether
thecodeanddataareprovidedornot.
• Ifthecontributionisadatasetand/ormodel,theauthorsshoulddescribethestepstaken
tomaketheirresultsreproducibleorverifiable.
• Dependingonthecontribution,reproducibilitycanbeaccomplishedinvariousways.
Forexample,ifthecontributionisanovelarchitecture,describingthearchitecturefully
mightsuffice,orifthecontributionisaspecificmodelandempiricalevaluation,itmay
benecessarytoeithermakeitpossibleforotherstoreplicatethemodelwiththesame
dataset, orprovideaccessto themodel. Ingeneral. releasingcodeanddata isoften
onegoodwaytoaccomplishthis,butreproducibilitycanalsobeprovidedviadetailed
instructionsforhowtoreplicatetheresults,accesstoahostedmodel(e.g.,inthecase
ofalargelanguagemodel),releasingofamodelcheckpoint,orothermeansthatare
appropriatetotheresearchperformed.
• WhileNeurIPSdoesnotrequirereleasingcode,theconferencedoesrequireallsubmis-
sionstoprovidesomereasonableavenueforreproducibility,whichmaydependonthe
natureofthecontribution. Forexample
(a) Ifthecontributionisprimarilyanewalgorithm,thepapershouldmakeitclearhow
toreproducethatalgorithm.
(b) Ifthecontributionisprimarilyanewmodelarchitecture,thepapershoulddescribe
thearchitectureclearlyandfully.
(c) Ifthecontributionisanewmodel(e.g.,alargelanguagemodel),thenthereshould
eitherbeawaytoaccessthismodelforreproducingtheresultsorawaytoreproduce
themodel(e.g.,withanopen-sourcedatasetorinstructionsforhowtoconstructthe
dataset).
(d) Werecognizethatreproducibilitymaybetrickyinsomecases,inwhichcaseauthors
arewelcometodescribetheparticularwaytheyprovideforreproducibility. Inthe
caseofclosed-sourcemodels,itmaybethataccesstothemodelislimitedinsome
way(e.g.,toregisteredusers),butitshouldbepossibleforotherresearcherstohave
somepathtoreproducingorverifyingtheresults.
5. Openaccesstodataandcode
Question: Doesthepaperprovideopenaccesstothedataandcode,withsufficientinstruc-
tionstofaithfullyreproducethemainexperimentalresults,asdescribedinsupplemental
material?
Answer: [Yes]
Justification: Thecodeisavailableathttps://github.com/LXXXXR/Kaleidoscope.
Guidelines:
• TheanswerNAmeansthatpaperdoesnotincludeexperimentsrequiringcode.
• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
public/guides/CodeSubmissionPolicy)formoredetails.
• Whileweencouragethereleaseofcodeanddata,weunderstandthatthismightnotbe
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
includingcode, unlessthisiscentraltothecontribution(e.g., foranewopen-source
benchmark).
22• Theinstructionsshouldcontaintheexactcommandandenvironmentneededtorunto
reproducetheresults. SeetheNeurIPScodeanddatasubmissionguidelines(https:
//nips.cc/public/guides/CodeSubmissionPolicy)formoredetails.
• Theauthorsshouldprovideinstructionsondataaccessandpreparation,includinghow
toaccesstherawdata,preprocesseddata,intermediatedata,andgenerateddata,etc.
• The authors should provide scripts to reproduce all experimental results for the new
proposedmethodandbaselines. Ifonlyasubsetofexperimentsarereproducible,they
shouldstatewhichonesareomittedfromthescriptandwhy.
• At submission time, to preserve anonymity, the authors should release anonymized
versions(ifapplicable).
• Providingasmuchinformationaspossibleinsupplementalmaterial(appendedtothe
paper)isrecommended,butincludingURLstodataandcodeispermitted.
6. ExperimentalSetting/Details
Question: Doesthepaperspecifyallthetrainingandtestdetails(e.g.,datasplits,hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: TheexperimentsettingsareprovidedinSection4withdetailselaboratedin
AppendicesA.1andA.2.
Guidelines:
• TheanswerNAmeansthatthepaperdoesnotincludeexperiments.
• Theexperimentalsettingshouldbepresentedinthecoreofthepapertoalevelofdetail
thatisnecessarytoappreciatetheresultsandmakesenseofthem.
• Thefulldetailscanbeprovidedeitherwiththecode,inappendix,orassupplemental
material.
7. ExperimentStatisticalSignificance
Question:Doesthepaperreporterrorbarssuitablyandcorrectlydefinedorotherappropriate
informationaboutthestatisticalsignificanceoftheexperiments?
Answer: [Yes]
Justification: Themainresultsarereportedwith95%confidenceerrorbarsinFigures4to6.
Guidelines:
• TheanswerNAmeansthatthepaperdoesnotincludeexperiments.
• Theauthorsshouldanswer"Yes"iftheresultsareaccompaniedbyerrorbars,confidence
intervals,orstatisticalsignificancetests,atleastfortheexperimentsthatsupportthe
mainclaimsofthepaper.
• Thefactorsofvariabilitythattheerrorbarsarecapturingshouldbeclearlystated(for
example,train/testsplit,initialization,randomdrawingofsomeparameter,oroverall
runwithgivenexperimentalconditions).
• Themethodforcalculatingtheerrorbarsshouldbeexplained(closedformformula,call
toalibraryfunction,bootstrap,etc.)
• Theassumptionsmadeshouldbegiven(e.g.,Normallydistributederrors).
• Itshouldbeclearwhethertheerrorbaristhestandarddeviationorthestandarderrorof
themean.
• It is OK to report 1-sigma error bars, but one should state it. The authors should
preferablyreporta2-sigmaerrorbarthanstatethattheyhavea96%CI,ifthehypothesis
ofNormalityoferrorsisnotverified.
• For asymmetric distributions, the authors should be careful not to show in tables or
figuressymmetricerrorbarsthatwouldyieldresultsthatareoutofrange(e.g. negative
errorrates).
• Iferrorbarsarereportedintablesorplots,Theauthorsshouldexplaininthetexthow
theywerecalculatedandreferencethecorrespondingfiguresortablesinthetext.
8. ExperimentsComputeResources
23Question: Foreachexperiment,doesthepaperprovidesufficientinformationonthecom-
puterresources(typeofcomputeworkers,memory,timeofexecution)neededtoreproduce
theexperiments?
Answer: [Yes]
Justification: PleaseseeAppendixA.4.
Guidelines:
• TheanswerNAmeansthatthepaperdoesnotincludeexperiments.
• ThepapershouldindicatethetypeofcomputeworkersCPUorGPU,internalcluster,or
cloudprovider,includingrelevantmemoryandstorage.
• Thepapershouldprovidetheamountofcomputerequiredforeachoftheindividual
experimentalrunsaswellasestimatethetotalcompute.
• Thepapershoulddisclosewhetherthefullresearchprojectrequiredmorecomputethan
theexperimentsreportedinthepaper(e.g.,preliminaryorfailedexperimentsthatdidn’t
makeitintothepaper).
9. CodeOfEthics
Question: Doestheresearchconductedinthepaperconform, ineveryrespect, withthe
NeurIPSCodeofEthicshttps://neurips.cc/public/EthicsGuidelines?
Answer: [Yes]
Justification: [NA]
Guidelines:
• TheanswerNAmeansthattheauthorshavenotreviewedtheNeurIPSCodeofEthics.
• IftheauthorsanswerNo,theyshouldexplainthespecialcircumstancesthatrequirea
deviationfromtheCodeofEthics.
• Theauthorsshouldmakesuretopreserveanonymity(e.g.,ifthereisaspecialconsidera-
tionduetolawsorregulationsintheirjurisdiction).
10. BroaderImpacts
Question: Does the paper discuss both potential positive societal impacts and negative
societalimpactsoftheworkperformed?
Answer: [NA]
Justification: ThispaperpresentsworkwhosegoalistoadvancethefieldofReinforcement
Learning. Therearemanypotentialsocietalconsequencesofourwork,nonewhichwefeel
mustbespecificallyhighlightedhere.
Guidelines:
• TheanswerNAmeansthatthereisnosocietalimpactoftheworkperformed.
• IftheauthorsanswerNAorNo, theyshouldexplainwhytheirworkhasnosocietal
impactorwhythepaperdoesnotaddresssocietalimpact.
• Examplesofnegativesocietalimpactsincludepotentialmaliciousorunintendeduses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g.,deploymentoftechnologiesthatcouldmakedecisionsthatunfairlyimpactspecific
groups),privacyconsiderations,andsecurityconsiderations.
• The conference expects that many papers will be foundational research and not tied
toparticularapplications,letalonedeployments. However,ifthereisadirectpathto
anynegativeapplications,theauthorsshouldpointitout. Forexample,itislegitimate
topointoutthatanimprovementinthequalityofgenerativemodelscouldbeusedto
generatedeepfakesfordisinformation. Ontheotherhand,itisnotneededtopointout
thatagenericalgorithmforoptimizingneuralnetworkscouldenablepeopletotrain
modelsthatgenerateDeepfakesfaster.
• Theauthorsshouldconsiderpossibleharmsthatcouldarisewhenthetechnologyisbeing
usedasintendedandfunctioningcorrectly,harmsthatcouldarisewhenthetechnologyis
beingusedasintendedbutgivesincorrectresults,andharmsfollowingfrom(intentional
orunintentional)misuseofthetechnology.
24• Iftherearenegativesocietalimpacts,theauthorscouldalsodiscusspossiblemitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanismsformonitoringmisuse,mechanismstomonitorhowasystemlearnsfrom
feedbackovertime,improvingtheefficiencyandaccessibilityofML).
11. Safeguards
Question: Doesthepaperdescribesafeguardsthathavebeenputinplaceforresponsible
releaseofdataormodelsthathaveahighriskformisuse(e.g.,pretrainedlanguagemodels,
imagegenerators,orscrapeddatasets)?
Answer: [NA]
Justification: Theexperimentsinthisworkareconductedinsimulatedgameenvironments,
therebypresentingminimalriskofmisuse.
Guidelines:
• TheanswerNAmeansthatthepaperposesnosuchrisks.
• Releasedmodelsthathaveahighriskformisuseordual-useshouldbereleasedwith
necessarysafeguardstoallowforcontrolleduseofthemodel,forexamplebyrequiring
thatusersadheretousageguidelinesorrestrictionstoaccessthemodelorimplementing
safetyfilters.
• DatasetsthathavebeenscrapedfromtheInternetcouldposesafetyrisks. Theauthors
shoulddescribehowtheyavoidedreleasingunsafeimages.
• Werecognizethatprovidingeffectivesafeguardsischallenging,andmanypapersdonot
requirethis,butweencourageauthorstotakethisintoaccountandmakeabestfaith
effort.
12. Licensesforexistingassets
Question: Arethecreatorsororiginalownersofassets(e.g.,code,data,models),usedin
thepaper,properlycreditedandarethelicenseandtermsofuseexplicitlymentionedand
properlyrespected?
Answer: [Yes]
Justification: ThepaperscorrespondingtotheenvironmentsusedarecitedinSection4,and
thecodebasesutilizedarelistedinAppendicesA.1.3andA.2.
Guidelines:
• TheanswerNAmeansthatthepaperdoesnotuseexistingassets.
• Theauthorsshouldcitetheoriginalpaperthatproducedthecodepackageordataset.
• Theauthorsshouldstatewhichversionoftheassetisusedand,ifpossible,includea
URL.
• Thenameofthelicense(e.g.,CC-BY4.0)shouldbeincludedforeachasset.
• For scraped data from a particular source (e.g., website), the copyright and terms of
serviceofthatsourceshouldbeprovided.
• Ifassetsarereleased,thelicense,copyrightinformation,andtermsofuseinthepack-
ageshouldbeprovided. Forpopulardatasets,paperswithcode.com/datasetshas
curatedlicensesforsomedatasets. Theirlicensingguidecanhelpdeterminethelicense
ofadataset.
• Forexistingdatasetsthatarere-packaged,boththeoriginallicenseandthelicenseofthe
derivedasset(ifithaschanged)shouldbeprovided.
• Ifthisinformationisnotavailableonline,theauthorsareencouragedtoreachouttothe
asset’screators.
13. NewAssets
Question:Arenewassetsintroducedinthepaperwelldocumentedandisthedocumentation
providedalongsidetheassets?
Answer: [NA]
Justification: [NA]
Guidelines:
25• TheanswerNAmeansthatthepaperdoesnotreleasenewassets.
• Researchersshouldcommunicatethedetailsofthedataset/code/modelaspartoftheir
submissions via structured templates. This includes details about training, license,
limitations,etc.
• Thepapershoulddiscusswhetherandhowconsentwasobtainedfrompeoplewhose
assetisused.
• Atsubmissiontime,remembertoanonymizeyourassets(ifapplicable). Youcaneither
createananonymizedURLorincludeananonymizedzipfile.
14. CrowdsourcingandResearchwithHumanSubjects
Question: Forcrowdsourcingexperimentsandresearchwithhumansubjects,doesthepaper
includethefulltextofinstructionsgiventoparticipantsandscreenshots,ifapplicable,as
wellasdetailsaboutcompensation(ifany)?
Answer: [NA]
Justification: [NA]
Guidelines:
• TheanswerNAmeansthatthepaperdoesnotinvolvecrowdsourcingnorresearchwith
humansubjects.
• Includingthisinformationinthesupplementalmaterialisfine,butifthemaincontri-
butionofthepaperinvolveshumansubjects,thenasmuchdetailaspossibleshouldbe
includedinthemainpaper.
• AccordingtotheNeurIPSCodeofEthics,workersinvolvedindatacollection,curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15. InstitutionalReviewBoard(IRB)ApprovalsorEquivalentforResearchwithHuman
Subjects
Question: Doesthepaperdescribepotentialrisksincurredbystudyparticipants,whether
suchrisksweredisclosedtothesubjects,andwhetherInstitutionalReviewBoard(IRB)
approvals(oranequivalentapproval/reviewbasedontherequirementsofyourcountryor
institution)wereobtained?
Answer: [NA]
Justification: [NA]
Guidelines:
• TheanswerNAmeansthatthepaperdoesnotinvolvecrowdsourcingnorresearchwith
humansubjects.
• Dependingonthecountryinwhichresearchisconducted,IRBapproval(orequivalent)
mayberequiredforanyhumansubjectsresearch. IfyouobtainedIRBapproval,you
shouldclearlystatethisinthepaper.
• Werecognizethattheproceduresforthismayvarysignificantlybetweeninstitutions
andlocations,andweexpectauthorstoadheretotheNeurIPSCodeofEthicsandthe
guidelinesfortheirinstitution.
• Forinitialsubmissions,donotincludeanyinformationthatwouldbreakanonymity(if
applicable),suchastheinstitutionconductingthereview.
26