[
    {
        "title": "Unraveling and Mitigating Safety Alignment Degradation of Vision-Language Models",
        "authors": "Qin LiuChao ShangLing LiuNikolaos PappasJie MaNeha Anna JohnSrikanth DossLluis MarquezMiguel BallesterosYassine Benajiba",
        "links": "http://arxiv.org/abs/2410.09047v1",
        "entry_id": "http://arxiv.org/abs/2410.09047v1",
        "pdf_url": "http://arxiv.org/pdf/2410.09047v1",
        "summary": "The safety alignment ability of Vision-Language Models (VLMs) is prone to be\ndegraded by the integration of the vision module compared to its LLM backbone.\nWe investigate this phenomenon, dubbed as ''safety alignment degradation'' in\nthis paper, and show that the challenge arises from the representation gap that\nemerges when introducing vision modality to VLMs. In particular, we show that\nthe representations of multi-modal inputs shift away from that of text-only\ninputs which represent the distribution that the LLM backbone is optimized for.\nAt the same time, the safety alignment capabilities, initially developed within\nthe textual embedding space, do not successfully transfer to this new\nmulti-modal representation space. To reduce safety alignment degradation, we\nintroduce Cross-Modality Representation Manipulation (CMRM), an inference time\nrepresentation intervention method for recovering the safety alignment ability\nthat is inherent in the LLM backbone of VLMs, while simultaneously preserving\nthe functional capabilities of VLMs. The empirical results show that our\nframework significantly recovers the alignment ability that is inherited from\nthe LLM backbone with minimal impact on the fluency and linguistic capabilities\nof pre-trained VLMs even without additional training. Specifically, the unsafe\nrate of LLaVA-7B on multi-modal input can be reduced from 61.53% to as low as\n3.15% with only inference-time intervention.\n  WARNING: This paper contains examples of toxic or harmful language.",
        "updated": "2024-10-11 17:59:31 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.09047v1"
    },
    {
        "title": "Linear Convergence of Diffusion Models Under the Manifold Hypothesis",
        "authors": "Peter PotaptchikIskander AzangulovGeorge Deligiannidis",
        "links": "http://arxiv.org/abs/2410.09046v1",
        "entry_id": "http://arxiv.org/abs/2410.09046v1",
        "pdf_url": "http://arxiv.org/pdf/2410.09046v1",
        "summary": "Score-matching generative models have proven successful at sampling from\ncomplex high-dimensional data distributions. In many applications, this\ndistribution is believed to concentrate on a much lower $d$-dimensional\nmanifold embedded into $D$-dimensional space; this is known as the manifold\nhypothesis. The current best-known convergence guarantees are either linear in\n$D$ or polynomial (superlinear) in $d$. The latter exploits a novel integration\nscheme for the backward SDE. We take the best of both worlds and show that the\nnumber of steps diffusion models require in order to converge in\nKullback-Leibler~(KL) divergence is linear (up to logarithmic terms) in the\nintrinsic dimension $d$. Moreover, we show that this linear dependency is\nsharp.",
        "updated": "2024-10-11 17:58:30 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.09046v1"
    },
    {
        "title": "Alberta Wells Dataset: Pinpointing Oil and Gas Wells from Satellite Imagery",
        "authors": "Pratinav SethMichelle LinBrefo Dwamena YawJade BoutotMary KangDavid Rolnick",
        "links": "http://arxiv.org/abs/2410.09032v1",
        "entry_id": "http://arxiv.org/abs/2410.09032v1",
        "pdf_url": "http://arxiv.org/pdf/2410.09032v1",
        "summary": "Millions of abandoned oil and gas wells are scattered across the world,\nleaching methane into the atmosphere and toxic compounds into the groundwater.\nMany of these locations are unknown, preventing the wells from being plugged\nand their polluting effects averted. Remote sensing is a relatively unexplored\ntool for pinpointing abandoned wells at scale. We introduce the first\nlarge-scale benchmark dataset for this problem, leveraging medium-resolution\nmulti-spectral satellite imagery from Planet Labs. Our curated dataset\ncomprises over 213,000 wells (abandoned, suspended, and active) from Alberta, a\nregion with especially high well density, sourced from the Alberta Energy\nRegulator and verified by domain experts. We evaluate baseline algorithms for\nwell detection and segmentation, showing the promise of computer vision\napproaches but also significant room for improvement.",
        "updated": "2024-10-11 17:49:50 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.09032v1"
    },
    {
        "title": "Variance reduction combining pre-experiment and in-experiment data",
        "authors": "Zhexiao LinPablo Crespo",
        "links": "http://arxiv.org/abs/2410.09027v1",
        "entry_id": "http://arxiv.org/abs/2410.09027v1",
        "pdf_url": "http://arxiv.org/pdf/2410.09027v1",
        "summary": "Online controlled experiments (A/B testing) are essential in data-driven\ndecision-making for many companies. Increasing the sensitivity of these\nexperiments, particularly with a fixed sample size, relies on reducing the\nvariance of the estimator for the average treatment effect (ATE). Existing\nmethods like CUPED and CUPAC use pre-experiment data to reduce variance, but\ntheir effectiveness depends on the correlation between the pre-experiment data\nand the outcome. In contrast, in-experiment data is often more strongly\ncorrelated with the outcome and thus more informative. In this paper, we\nintroduce a novel method that combines both pre-experiment and in-experiment\ndata to achieve greater variance reduction than CUPED and CUPAC, without\nintroducing bias or additional computation complexity. We also establish\nasymptotic theory and provide consistent variance estimators for our method.\nApplying this method to multiple online experiments at Etsy, we reach\nsubstantial variance reduction over CUPAC with the inclusion of only a few\nin-experiment covariates. These results highlight the potential of our approach\nto significantly improve experiment sensitivity and accelerate decision-making.",
        "updated": "2024-10-11 17:45:29 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.09027v1"
    },
    {
        "title": "AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents",
        "authors": "Maksym AndriushchenkoAlexandra SoulyMateusz DziemianDerek DuenasMaxwell LinJustin WangDan HendrycksAndy ZouZico KolterMatt FredriksonEric WinsorJerome WynneYarin GalXander Davies",
        "links": "http://arxiv.org/abs/2410.09024v2",
        "entry_id": "http://arxiv.org/abs/2410.09024v2",
        "pdf_url": "http://arxiv.org/pdf/2410.09024v2",
        "summary": "The robustness of LLMs to jailbreak attacks, where users design prompts to\ncircumvent safety measures and misuse model capabilities, has been studied\nprimarily for LLMs acting as simple chatbots. Meanwhile, LLM agents -- which\nuse external tools and can execute multi-stage tasks -- may pose a greater risk\nif misused, but their robustness remains underexplored. To facilitate research\non LLM agent misuse, we propose a new benchmark called AgentHarm. The benchmark\nincludes a diverse set of 110 explicitly malicious agent tasks (440 with\naugmentations), covering 11 harm categories including fraud, cybercrime, and\nharassment. In addition to measuring whether models refuse harmful agentic\nrequests, scoring well on AgentHarm requires jailbroken agents to maintain\ntheir capabilities following an attack to complete a multi-step task. We\nevaluate a range of leading LLMs, and find (1) leading LLMs are surprisingly\ncompliant with malicious agent requests without jailbreaking, (2) simple\nuniversal jailbreak templates can be adapted to effectively jailbreak agents,\nand (3) these jailbreaks enable coherent and malicious multi-step agent\nbehavior and retain model capabilities. To enable simple and reliable\nevaluation of attacks and defenses for LLM-based agents, we publicly release\nAgentHarm at https://huggingface.co/datasets/ai-safety-institute/AgentHarm.",
        "updated": "2024-10-14 17:28:08 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.09024v2"
    }
]