[
    {
        "title": "PEAR: A Robust and Flexible Automation Framework for Ptychography Enabled by Multiple Large Language Model Agents",
        "authors": "Xiangyu YinChuqiao ShiYimo HanYi Jiang",
        "links": "http://arxiv.org/abs/2410.09034v1",
        "entry_id": "http://arxiv.org/abs/2410.09034v1",
        "pdf_url": "http://arxiv.org/pdf/2410.09034v1",
        "summary": "Ptychography is an advanced computational imaging technique in X-ray and\nelectron microscopy. It has been widely adopted across scientific research\nfields, including physics, chemistry, biology, and materials science, as well\nas in industrial applications such as semiconductor characterization. In\npractice, obtaining high-quality ptychographic images requires simultaneous\noptimization of numerous experimental and algorithmic parameters.\nTraditionally, parameter selection often relies on trial and error, leading to\nlow-throughput workflows and potential human bias. In this work, we develop the\n\"Ptychographic Experiment and Analysis Robot\" (PEAR), a framework that\nleverages large language models (LLMs) to automate data analysis in\nptychography. To ensure high robustness and accuracy, PEAR employs multiple LLM\nagents for tasks including knowledge retrieval, code generation, parameter\nrecommendation, and image reasoning. Our study demonstrates that PEAR's\nmulti-agent design significantly improves the workflow success rate, even with\nsmaller open-weight models such as LLaMA 3.1 8B. PEAR also supports various\nautomation levels and is designed to work with customized local knowledge\nbases, ensuring flexibility and adaptability across different research\nenvironments.",
        "updated": "2024-10-11 17:50:59 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.09034v1"
    },
    {
        "title": "The Dynamics of Social Conventions in LLM populations: Spontaneous Emergence, Collective Biases and Tipping Points",
        "authors": "Ariel Flint AsheryLuca Maria AielloAndrea Baronchelli",
        "links": "http://arxiv.org/abs/2410.08948v1",
        "entry_id": "http://arxiv.org/abs/2410.08948v1",
        "pdf_url": "http://arxiv.org/pdf/2410.08948v1",
        "summary": "Social conventions are the foundation for social and economic life. As\nlegions of AI agents increasingly interact with each other and with humans,\ntheir ability to form shared conventions will determine how effectively they\nwill coordinate behaviors, integrate into society and influence it. Here, we\ninvestigate the dynamics of conventions within populations of Large Language\nModel (LLM) agents using simulated interactions. First, we show that globally\naccepted social conventions can spontaneously arise from local interactions\nbetween communicating LLMs. Second, we demonstrate how strong collective biases\ncan emerge during this process, even when individual agents appear to be\nunbiased. Third, we examine how minority groups of committed LLMs can drive\nsocial change by establishing new social conventions. We show that once these\nminority groups reach a critical size, they can consistently overturn\nestablished behaviors. In all cases, contrasting the experimental results with\npredictions from a minimal multi-agent model allows us to isolate the specific\nrole of LLM agents. Our results clarify how AI systems can autonomously develop\nnorms without explicit programming and have implications for designing AI\nsystems that align with human values and societal goals.",
        "updated": "2024-10-11 16:16:38 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.08948v1"
    },
    {
        "title": "PILLAR: an AI-Powered Privacy Threat Modeling Tool",
        "authors": "Majid MollaeefarAndrea BissoliSilvio Ranise",
        "links": "http://arxiv.org/abs/2410.08755v1",
        "entry_id": "http://arxiv.org/abs/2410.08755v1",
        "pdf_url": "http://arxiv.org/pdf/2410.08755v1",
        "summary": "The rapid evolution of Large Language Models (LLMs) has unlocked new\npossibilities for applying artificial intelligence across a wide range of\nfields, including privacy engineering. As modern applications increasingly\nhandle sensitive user data, safeguarding privacy has become more critical than\never. To protect privacy effectively, potential threats need to be identified\nand addressed early in the system development process. Frameworks like LINDDUN\noffer structured approaches for uncovering these risks, but despite their\nvalue, they often demand substantial manual effort, expert input, and detailed\nsystem knowledge. This makes the process time-consuming and prone to errors.\nCurrent privacy threat modeling methods, such as LINDDUN, typically rely on\ncreating and analyzing complex data flow diagrams (DFDs) and system\ndescriptions to pinpoint potential privacy issues. While these approaches are\nthorough, they can be cumbersome, relying heavily on the precision of the data\nprovided by users. Moreover, they often generate a long list of threats without\nclear guidance on how to prioritize them, leaving developers unsure of where to\nfocus their efforts. In response to these challenges, we introduce PILLAR\n(Privacy risk Identification with LINDDUN and LLM Analysis Report), a new tool\nthat integrates LLMs with the LINDDUN framework to streamline and enhance\nprivacy threat modeling. PILLAR automates key parts of the LINDDUN process,\nsuch as generating DFDs, classifying threats, and prioritizing risks. By\nleveraging the capabilities of LLMs, PILLAR can take natural language\ndescriptions of systems and transform them into comprehensive threat models\nwith minimal input from users, reducing the workload on developers and privacy\nexperts while improving the efficiency and accuracy of the process.",
        "updated": "2024-10-11 12:13:03 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.08755v1"
    },
    {
        "title": "Edge AI Collaborative Learning: Bayesian Approaches to Uncertainty Estimation",
        "authors": "Gleb RadchenkoVictoria Andrea Fill",
        "links": "http://arxiv.org/abs/2410.08651v1",
        "entry_id": "http://arxiv.org/abs/2410.08651v1",
        "pdf_url": "http://arxiv.org/pdf/2410.08651v1",
        "summary": "Recent advancements in edge computing have significantly enhanced the AI\ncapabilities of Internet of Things (IoT) devices. However, these advancements\nintroduce new challenges in knowledge exchange and resource management,\nparticularly addressing the spatiotemporal data locality in edge computing\nenvironments. This study examines algorithms and methods for deploying\ndistributed machine learning within autonomous, network-capable, AI-enabled\nedge devices. We focus on determining confidence levels in learning outcomes\nconsidering the spatial variability of data encountered by independent agents.\nUsing collaborative mapping as a case study, we explore the application of the\nDistributed Neural Network Optimization (DiNNO) algorithm extended with\nBayesian neural networks (BNNs) for uncertainty estimation. We implement a 3D\nenvironment simulation using the Webots platform to simulate collaborative\nmapping tasks, decouple the DiNNO algorithm into independent processes for\nasynchronous network communication in distributed learning, and integrate\ndistributed uncertainty estimation using BNNs. Our experiments demonstrate that\nBNNs can effectively support uncertainty estimation in a distributed learning\ncontext, with precise tuning of learning hyperparameters crucial for effective\nuncertainty assessment. Notably, applying Kullback-Leibler divergence for\nparameter regularization resulted in a 12-30% reduction in validation loss\nduring distributed BNN training compared to other regularization strategies.",
        "updated": "2024-10-11 09:20:16 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.08651v1"
    },
    {
        "title": "Kaleidoscope: Learnable Masks for Heterogeneous Multi-agent Reinforcement Learning",
        "authors": "Xinran LiLing PanJun Zhang",
        "links": "http://arxiv.org/abs/2410.08540v1",
        "entry_id": "http://arxiv.org/abs/2410.08540v1",
        "pdf_url": "http://arxiv.org/pdf/2410.08540v1",
        "summary": "In multi-agent reinforcement learning (MARL), parameter sharing is commonly\nemployed to enhance sample efficiency. However, the popular approach of full\nparameter sharing often leads to homogeneous policies among agents, potentially\nlimiting the performance benefits that could be derived from policy diversity.\nTo address this critical limitation, we introduce \\emph{Kaleidoscope}, a novel\nadaptive partial parameter sharing scheme that fosters policy heterogeneity\nwhile still maintaining high sample efficiency. Specifically, Kaleidoscope\nmaintains one set of common parameters alongside multiple sets of distinct,\nlearnable masks for different agents, dictating the sharing of parameters. It\npromotes diversity among policy networks by encouraging discrepancy among these\nmasks, without sacrificing the efficiencies of parameter sharing. This design\nallows Kaleidoscope to dynamically balance high sample efficiency with a broad\npolicy representational capacity, effectively bridging the gap between full\nparameter sharing and non-parameter sharing across various environments. We\nfurther extend Kaleidoscope to critic ensembles in the context of actor-critic\nalgorithms, which could help improve value estimations.Our empirical\nevaluations across extensive environments, including multi-agent particle\nenvironment, multi-agent MuJoCo and StarCraft multi-agent challenge v2,\ndemonstrate the superior performance of Kaleidoscope compared with existing\nparameter sharing approaches, showcasing its potential for performance\nenhancement in MARL. The code is publicly available at\n\\url{https://github.com/LXXXXR/Kaleidoscope}.",
        "updated": "2024-10-11 05:22:54 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.08540v1"
    }
]