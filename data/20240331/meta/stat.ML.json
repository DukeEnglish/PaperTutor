[
    {
        "title": "Metric Learning from Limited Pairwise Preference Comparisons",
        "authors": "Zhi WangGeelon SoRamya Korlakai Vinayak",
        "links": "http://arxiv.org/abs/2403.19629v1",
        "entry_id": "http://arxiv.org/abs/2403.19629v1",
        "pdf_url": "http://arxiv.org/pdf/2403.19629v1",
        "summary": "We study metric learning from preference comparisons under the ideal point\nmodel, in which a user prefers an item over another if it is closer to their\nlatent ideal item. These items are embedded into $\\mathbb{R}^d$ equipped with\nan unknown Mahalanobis distance shared across users. While recent work shows\nthat it is possible to simultaneously recover the metric and ideal items given\n$\\mathcal{O}(d)$ pairwise comparisons per user, in practice we often have a\nlimited budget of $o(d)$ comparisons. We study whether the metric can still be\nrecovered, even though it is known that learning individual ideal items is now\nno longer possible. We show that in general, $o(d)$ comparisons reveals no\ninformation about the metric, even with infinitely many users. However, when\ncomparisons are made over items that exhibit low-dimensional structure, each\nuser can contribute to learning the metric restricted to a low-dimensional\nsubspace so that the metric can be jointly identified. We present a\ndivide-and-conquer approach that achieves this, and provide theoretical\nrecovery guarantees and empirical validation.",
        "updated": "2024-03-28 17:46:25 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.19629v1"
    },
    {
        "title": "Top-$k$ Classification and Cardinality-Aware Prediction",
        "authors": "Anqi MaoMehryar MohriYutao Zhong",
        "links": "http://arxiv.org/abs/2403.19625v1",
        "entry_id": "http://arxiv.org/abs/2403.19625v1",
        "pdf_url": "http://arxiv.org/pdf/2403.19625v1",
        "summary": "We present a detailed study of top-$k$ classification, the task of predicting\nthe $k$ most probable classes for an input, extending beyond single-class\nprediction. We demonstrate that several prevalent surrogate loss functions in\nmulti-class classification, such as comp-sum and constrained losses, are\nsupported by $H$-consistency bounds with respect to the top-$k$ loss. These\nbounds guarantee consistency in relation to the hypothesis set $H$, providing\nstronger guarantees than Bayes-consistency due to their non-asymptotic and\nhypothesis-set specific nature. To address the trade-off between accuracy and\ncardinality $k$, we further introduce cardinality-aware loss functions through\ninstance-dependent cost-sensitive learning. For these functions, we derive\ncost-sensitive comp-sum and constrained surrogate losses, establishing their\n$H$-consistency bounds and Bayes-consistency. Minimizing these losses leads to\nnew cardinality-aware algorithms for top-$k$ classification. We report the\nresults of extensive experiments on CIFAR-100, ImageNet, CIFAR-10, and SVHN\ndatasets demonstrating the effectiveness and benefit of these algorithms.",
        "updated": "2024-03-28 17:45:03 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.19625v1"
    },
    {
        "title": "Taming the Interactive Particle Langevin Algorithm -- the superlinear case",
        "authors": "Tim JohnstonNikolaos MakrasSotirios Sabanis",
        "links": "http://arxiv.org/abs/2403.19587v1",
        "entry_id": "http://arxiv.org/abs/2403.19587v1",
        "pdf_url": "http://arxiv.org/pdf/2403.19587v1",
        "summary": "Recent advances in stochastic optimization have yielded the interactive\nparticle Langevin algorithm (IPLA), which leverages the notion of interacting\nparticle systems (IPS) to efficiently sample from approximate posterior\ndensities. This becomes particularly crucial within the framework of\nExpectation-Maximization (EM), where the E-step is computationally challenging\nor even intractable. Although prior research has focused on scenarios involving\nconvex cases with gradients of log densities that grow at most linearly, our\nwork extends this framework to include polynomial growth. Taming techniques are\nemployed to produce an explicit discretization scheme that yields a new class\nof stable, under such non-linearities, algorithms which are called tamed\ninteractive particle Langevin algorithms (tIPLA). We obtain non-asymptotic\nconvergence error estimates in Wasserstein-2 distance for the new class under\nan optimal rate.",
        "updated": "2024-03-28 17:11:25 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.19587v1"
    },
    {
        "title": "Maximum Likelihood Estimation on Stochastic Blockmodels for Directed Graph Clustering",
        "authors": "Mihai CucuringuXiaowen DongNing Zhang",
        "links": "http://arxiv.org/abs/2403.19516v1",
        "entry_id": "http://arxiv.org/abs/2403.19516v1",
        "pdf_url": "http://arxiv.org/pdf/2403.19516v1",
        "summary": "This paper studies the directed graph clustering problem through the lens of\nstatistics, where we formulate clustering as estimating underlying communities\nin the directed stochastic block model (DSBM). We conduct the maximum\nlikelihood estimation (MLE) on the DSBM and thereby ascertain the most probable\ncommunity assignment given the observed graph structure. In addition to the\nstatistical point of view, we further establish the equivalence between this\nMLE formulation and a novel flow optimization heuristic, which jointly\nconsiders two important directed graph statistics: edge density and edge\norientation. Building on this new formulation of directed clustering, we\nintroduce two efficient and interpretable directed clustering algorithms, a\nspectral clustering algorithm and a semidefinite programming based clustering\nalgorithm. We provide a theoretical upper bound on the number of misclustered\nvertices of the spectral clustering algorithm using tools from matrix\nperturbation theory. We compare, both quantitatively and qualitatively, our\nproposed algorithms with existing directed clustering methods on both synthetic\nand real-world data, thus providing further ground to our theoretical\ncontributions.",
        "updated": "2024-03-28 15:47:13 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.19516v1"
    },
    {
        "title": "Tensor Network-Constrained Kernel Machines as Gaussian Processes",
        "authors": "Frederiek WeselKim Batselier",
        "links": "http://arxiv.org/abs/2403.19500v1",
        "entry_id": "http://arxiv.org/abs/2403.19500v1",
        "pdf_url": "http://arxiv.org/pdf/2403.19500v1",
        "summary": "Tensor Networks (TNs) have recently been used to speed up kernel machines by\nconstraining the model weights, yielding exponential computational and storage\nsavings. In this paper we prove that the outputs of Canonical Polyadic\nDecomposition (CPD) and Tensor Train (TT)-constrained kernel machines recover a\nGaussian Process (GP), which we fully characterize, when placing i.i.d. priors\nover their parameters. We analyze the convergence of both CPD and\nTT-constrained models, and show how TT yields models exhibiting more GP\nbehavior compared to CPD, for the same number of model parameters. We\nempirically observe this behavior in two numerical experiments where we\nrespectively analyze the convergence to the GP and the performance at\nprediction. We thereby establish a connection between TN-constrained kernel\nmachines and GPs.",
        "updated": "2024-03-28 15:29:30 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.19500v1"
    }
]