[
    {
        "title": "Human-compatible driving partners through data-regularized self-play reinforcement learning",
        "authors": "Daphne CornelisseEugene Vinitsky",
        "links": "http://arxiv.org/abs/2403.19648v1",
        "entry_id": "http://arxiv.org/abs/2403.19648v1",
        "pdf_url": "http://arxiv.org/pdf/2403.19648v1",
        "summary": "A central challenge for autonomous vehicles is coordinating with humans.\nTherefore, incorporating realistic human agents is essential for scalable\ntraining and evaluation of autonomous driving systems in simulation. Simulation\nagents are typically developed by imitating large-scale, high-quality datasets\nof human driving. However, pure imitation learning agents empirically have high\ncollision rates when executed in a multi-agent closed-loop setting. To build\nagents that are realistic and effective in closed-loop settings, we propose\nHuman-Regularized PPO (HR-PPO), a multi-agent algorithm where agents are\ntrained through self-play with a small penalty for deviating from a human\nreference policy. In contrast to prior work, our approach is RL-first and only\nuses 30 minutes of imperfect human demonstrations. We evaluate agents in a\nlarge set of multi-agent traffic scenes. Results show our HR-PPO agents are\nhighly effective in achieving goals, with a success rate of 93%, an off-road\nrate of 3.5%, and a collision rate of 3%. At the same time, the agents drive in\na human-like manner, as measured by their similarity to existing human driving\nlogs. We also find that HR-PPO agents show considerable improvements on proxy\nmeasures for coordination with human driving, particularly in highly\ninteractive scenarios. We open-source our code and trained agents at\nhttps://github.com/Emerge-Lab/nocturne_lab and provide demonstrations of agent\nbehaviors at https://sites.google.com/view/driving-partners.",
        "updated": "2024-03-28 17:56:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.19648v1"
    },
    {
        "title": "Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models",
        "authors": "Samuel MarksCan RagerEric J. MichaudYonatan BelinkovDavid BauAaron Mueller",
        "links": "http://arxiv.org/abs/2403.19647v1",
        "entry_id": "http://arxiv.org/abs/2403.19647v1",
        "pdf_url": "http://arxiv.org/pdf/2403.19647v1",
        "summary": "We introduce methods for discovering and applying sparse feature circuits.\nThese are causally implicated subnetworks of human-interpretable features for\nexplaining language model behaviors. Circuits identified in prior work consist\nof polysemantic and difficult-to-interpret units like attention heads or\nneurons, rendering them unsuitable for many downstream applications. In\ncontrast, sparse feature circuits enable detailed understanding of\nunanticipated mechanisms. Because they are based on fine-grained units, sparse\nfeature circuits are useful for downstream tasks: We introduce SHIFT, where we\nimprove the generalization of a classifier by ablating features that a human\njudges to be task-irrelevant. Finally, we demonstrate an entirely unsupervised\nand scalable interpretability pipeline by discovering thousands of sparse\nfeature circuits for automatically discovered model behaviors.",
        "updated": "2024-03-28 17:56:07 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.19647v1"
    },
    {
        "title": "Retrieval-Enhanced Knowledge Editing for Multi-Hop Question Answering in Language Models",
        "authors": "Yucheng ShiQiaoyu TanXuansheng WuShaochen ZhongKaixiong ZhouNinghao Liu",
        "links": "http://arxiv.org/abs/2403.19631v1",
        "entry_id": "http://arxiv.org/abs/2403.19631v1",
        "pdf_url": "http://arxiv.org/pdf/2403.19631v1",
        "summary": "Large Language Models (LLMs) have shown proficiency in question-answering\ntasks but often struggle to integrate real-time knowledge updates, leading to\npotentially outdated or inaccurate responses. This problem becomes even more\nchallenging when dealing with multi-hop questions since they require LLMs to\nupdate and integrate multiple knowledge pieces relevant to the questions. To\ntackle the problem, we propose the Retrieval-Augmented model Editing (RAE)\nframework tailored for multi-hop question answering. RAE first retrieves edited\nfacts and then refines the language model through in-context learning.\nSpecifically, our retrieval approach, based on mutual information maximization,\nleverages the reasoning abilities of LLMs to identify chain facts that na\\\"ive\nsimilarity-based searches might miss. Additionally, our framework incorporates\na pruning strategy to eliminate redundant information from the retrieved facts,\nwhich enhances the editing accuracy and mitigates the hallucination problem.\nOur framework is supported by theoretical justification for its fact retrieval\nefficacy. Finally, comprehensive evaluation across various LLMs validates RAE's\nability in providing accurate answers with updated knowledge.",
        "updated": "2024-03-28 17:47:19 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.19631v1"
    },
    {
        "title": "Metric Learning from Limited Pairwise Preference Comparisons",
        "authors": "Zhi WangGeelon SoRamya Korlakai Vinayak",
        "links": "http://arxiv.org/abs/2403.19629v1",
        "entry_id": "http://arxiv.org/abs/2403.19629v1",
        "pdf_url": "http://arxiv.org/pdf/2403.19629v1",
        "summary": "We study metric learning from preference comparisons under the ideal point\nmodel, in which a user prefers an item over another if it is closer to their\nlatent ideal item. These items are embedded into $\\mathbb{R}^d$ equipped with\nan unknown Mahalanobis distance shared across users. While recent work shows\nthat it is possible to simultaneously recover the metric and ideal items given\n$\\mathcal{O}(d)$ pairwise comparisons per user, in practice we often have a\nlimited budget of $o(d)$ comparisons. We study whether the metric can still be\nrecovered, even though it is known that learning individual ideal items is now\nno longer possible. We show that in general, $o(d)$ comparisons reveals no\ninformation about the metric, even with infinitely many users. However, when\ncomparisons are made over items that exhibit low-dimensional structure, each\nuser can contribute to learning the metric restricted to a low-dimensional\nsubspace so that the metric can be jointly identified. We present a\ndivide-and-conquer approach that achieves this, and provide theoretical\nrecovery guarantees and empirical validation.",
        "updated": "2024-03-28 17:46:25 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.19629v1"
    },
    {
        "title": "Top-$k$ Classification and Cardinality-Aware Prediction",
        "authors": "Anqi MaoMehryar MohriYutao Zhong",
        "links": "http://arxiv.org/abs/2403.19625v1",
        "entry_id": "http://arxiv.org/abs/2403.19625v1",
        "pdf_url": "http://arxiv.org/pdf/2403.19625v1",
        "summary": "We present a detailed study of top-$k$ classification, the task of predicting\nthe $k$ most probable classes for an input, extending beyond single-class\nprediction. We demonstrate that several prevalent surrogate loss functions in\nmulti-class classification, such as comp-sum and constrained losses, are\nsupported by $H$-consistency bounds with respect to the top-$k$ loss. These\nbounds guarantee consistency in relation to the hypothesis set $H$, providing\nstronger guarantees than Bayes-consistency due to their non-asymptotic and\nhypothesis-set specific nature. To address the trade-off between accuracy and\ncardinality $k$, we further introduce cardinality-aware loss functions through\ninstance-dependent cost-sensitive learning. For these functions, we derive\ncost-sensitive comp-sum and constrained surrogate losses, establishing their\n$H$-consistency bounds and Bayes-consistency. Minimizing these losses leads to\nnew cardinality-aware algorithms for top-$k$ classification. We report the\nresults of extensive experiments on CIFAR-100, ImageNet, CIFAR-10, and SVHN\ndatasets demonstrating the effectiveness and benefit of these algorithms.",
        "updated": "2024-03-28 17:45:03 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.19625v1"
    }
]