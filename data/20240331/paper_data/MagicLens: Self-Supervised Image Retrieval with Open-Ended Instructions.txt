MagicLens : Self-Supervised Image Retrieval with Open-Ended Instructions
KaiZhang2⋆ YiLuan1 HexiangHu1 KentonLee1 SiyuanQiao1 WenhuChen1 YuSu2 Ming-WeiChang1
1GoogleDeepMind 2TheOhioStateUniversity
https://open-vision-language.github.io/MagicLens/
Teaser Figure
Query Image Instruction MagicLens Prior SOTA
Abstract
Find the identical image
Imageretrieval,i.e.,findingdesiredimagesgiven
areferenceimage,inherentlyencompassesrich,
multi-faceted search intents that are difficult to Compare its height to the
world's tallest building
capturesolelyusingimage-basedmeasures. Re-
cent work leverages text instructions to allow
userstomorefreelyexpresstheirsearchintents.
Outside view from the
However,existingworkprimarilyfocusesonim- inside of it
agepairsthatarevisuallysimilarand/orcanbe
characterizedbyasmallsetofpre-definedrela- Find other attractions
in this country
tions. The core thesis of this paper is that text
instructions can enable retrieving images with
Figure1. Top-1retrievedimagesusingMagicLensandtheprior
richerrelationsbeyondvisualsimilarity. Toshow
SOTA (Gu et al., 2024) from a retrieval pool containing 1.4M
this, we introduce MagicLens, a series of self-
images.ThepriorSOTAmethod,whileacceptingtextinstructions,
supervised image retrieval models that support
primarilyretrievesimagesbasedonvisualsimilaritytothequery
open-endedinstructions. MagicLensisbuiltona image,ignoringthenuancesofthetextinstructions. Incontrast,
keynovelinsight:imagepairsthatnaturallyoccur MagicLensexcelsatretrievingbothvisuallysimilarimagesand
onthesamewebpagescontainawiderangeofim- thosethatalignwiththedeepermeaningandcontextofthetext
plicitrelations(e.g.,inside view of),andwe instructions—evenwhentheimagesdonotresemblethequery.
canbringthoseimplicitrelationsexplicitbysyn- Forexample,ifgivenaqueryimageoftheBurjAlArabandthe
thesizinginstructionsvialargemultimodalmod- instruction“findotherattractionsinthiscountry”,itcansuccess-
fullylocateimagesofthePalmIslandsinDubai.
els(LMMs)andlargelanguagemodels(LLMs).
Trainedon36.7M(queryimage,instruction,tar-
rangeofreal-worldapplications,suchasvisualsearch,ob-
get image) triplets with rich semantic relations
jectlocalization,andre-identification. However,sinceits
minedfromtheweb, MagicLensachievescom-
inception,thistaskhassufferedfromambiguousdefinitions
parableorbetterresultsoneightbenchmarksof
duetothecomplexandrichcontentencapsulatedinimages.
variousimageretrievaltasksthanpriorstate-of-
Similarimagesmaydifferinkeyaspects,anddifferentim-
the-art(SOTA)methods. Remarkably,itoutper-
agescansharecommonalities. Inimagesearchscenarios,
forms previous SOTA but with a 50× smaller
users frequently present multiple search intents for a sin-
modelsizeonmultiplebenchmarks. Additional
gle query image, indicating that mere image relevance is
humananalysesona1.4M-imageunseencorpus
insufficientforprecisesearchresults. Forinstance,when
furtherdemonstratethediversityofsearchintents
searchingwithanimageoftheBurj Al Arab hotel
supportedbyMagicLens.
in Dubai(seeFigure1),ausermightseekotherattrac-
tionsinDubaioraninteriorview,eachrelatingdifferentlyto
thequeryimage. Therefore,incorporatingtextinstructions
1.Introduction
thatarticulatesearchintentsisessentialandindispensable
Imageretrievalisalong-establishedproblemincomputer for enhancing retrieval accuracy. Ideally, models should
vision(Dattaetal.,2008;Gordoetal.,2016)withawide accuratelycaptureandinterpretdiversereal-worldsearch
intentsasconveyedbyopen-endedtextinstructions.
⋆Work done at Google DeepMind. Correspondence to:
<zhang.13253@osu.edu>. These open-ended search instructions, span a wide range
1
4202
raM
82
]VC.sc[
1v15691.3042:viXraSelf-SupervisedImageRetrievalwithOpen-EndedInstructions
Withtheconstructeddataset,wetraindual-encodermodels
calledMagicLens,whichretrieveimagesgivenaquerycon-
sistingofanimagewithaninstruction. Ourmodelsachieve
comparableorbetterresultsthanpriorSOTAmethodson
eightbenchmarks,rangingfromvariousmultimodality-to-
imageandimage-to-imageretrievaltasks. Inaddition,Mag-
icLens can retain or even significantly improve the text-
to-image retrieval performance of the underlying single-
modalityencoders. Witha50timessmallermodelsizethan
priorSOTAmethods,MagicLensoutperformsthemonmul-
Figure2.Dataconstructionoverview.Wecollectnaturallyoccur- tiplebenchmarks: CIRCO(Baldratietal.,2023),Domain
ringimagepairsfromthesamewebpagesandusePaLI+PaLM2 TransferImageNet(Saitoetal.,2023),andGeneCIS(Vaze
togenerateinstructionsconnectingthetwoimages.
etal.,2023). Tofurtherexamineourmodels’capabilitiesin
amorerealisticscenario,weconstructthelargestretrieval
pooltodatewith1.4millionunseenimagesandperform
oftopicsandconcepts,andreflectthediversewaysusers
retrievalgivenhuman-writtensearchquerieswithdiverse
interactwithvisualcontent,requiringtheretrievalsystem
instructions. ThehumanevaluationfindsthatMagicLens
tograspnotonlythevisualfeaturesofanimagebutalso
cansuccessfullysatisfycomplexandbeyondvisualsearch
the nuanced semantic relation between the query image
intents,whereasthepriorSOTAfailstodoso.
and desired results as expressed in the instructions. Ex-
isting models, however, are optimized towards one or a Ourcontributionsarethreefold:
few restricted domains (Vo et al., 2019; Wu et al., 2021;
• Webringanovelinsightforimageretrieval: naturallyoc-
Liuetal.,2021;Baldratietal.,2023),wherethetypesof
curringimagepairsfromthesamewebpagesarestrong
visual similarities are manually defined as a prior. They
self-supervisedtrainingsignals. Basedonthis, wepro-
eitheradjustthemodelarchitectureandtrainingrecipeto
poseaneffectivepipeline,backedwithLMMsandLLMs,
utilizeimage-captiondata(Chen&Lai,2023;Saitoetal.,
toconstructtrainingdataconsistingof36.7Mtriplets.
2023; Baldrati et al., 2023; Gu et al., 2024), or rely on
• WeintroduceMagicLens, aseriesoflight-weightdual-
syntheticdataconstructedfrompre-definedinstructiontem-
encodersthatjointlyembedapairofimageandinstruc-
plates (Brooks et al., 2023; Gu et al., 2023). As a result,
tion, trained on the constructed dataset. Across multi-
neitheroftheseresearchdirectionscaneffectivelymodel
plebenchmarks,MagicLensoutperformspreviousSOTA
open-endedinstructions,evidencedbyFigure1.
methodsbutwitha50×smallermodelsize.
In this paper, we present MagicLens, a series of self- • Weconductanin-depthhumanevaluationandanalysis
supervisedimageretrievalmodelstrainedonawiderangeof ona1.4M-scaleretrievalpool,whichisthelargesttodate.
(query image, instruction, target image) triplets RemarkablyhighsuccessratesshowthatMagicLenscan
thatreflectnaturallyoccurringsemanticrelations,mined wellcaptureandsatisfydiversesearchintents,especially
fromwebpagesandcuratedwithstate-of-the-art(SOTA) complexandbeyondvisualones.
foundation models. Specifically, we extract image pairs
that naturally occur on the same web page to form posi-
2.RelatedWork
tivepairsthatcarryabundantbutnaturalsemanticrelations.
Wethenapplybothlargemultimodalmodels(Chenetal.,
Pre-trainingMultimodalEncoders. Multimodalencoder
2023b;a)andlargelanguagemodels(Aniletal.,2023)to
pre-training(Faghrietal.,2017;Chenetal.,2021;Radford
refinethedescriptionofsuchopen-endedsemanticrelation,
etal.,2021;Yuetal.,2022;Lietal.,2021;Kimetal.,2021;
intoopen-endedinstruction. Figure2presentsanoverview
Wangetal.,2023;Lietal.,2022;2023;Chertietal.,2023)
ofthedataconstructionpipeline. Asaconcreteexample,a
has witnessed great success in recent years. Pre-trained
camerareviewwebsite1thatpresentstheimageofaNikon
onweb-scaleimage-captiondata(Zhaietal.,2022;Schuh-
CameraandtheimageofaNikon Chargerwouldoffer
mannetal.,2022),thesemodelsaligntherepresentations
aninterestingandnon-trivialrelation“chargerofaprod-
ofdifferentmodalitiesinajointspace,enablingzero-shot
uct”, which would then be curated by the LMM+LLM
cross-modalityretrieval. However,theseworksfocusonen-
pipeline,andproduceafinalinstructionFind a charger
codingsinglemodalities,withoutconsideringthecomposed
for it. This process produces open-ended instructions
representationofmultiplemodalities.Somelaterefforts(Hu
thatdepictdiversesemanticrelationsbeyondmerevisual
etal.,2023a;Chenetal.,2023c;Weietal.,2023)attemptto
similarity, resulting in a large-scale training dataset with
combinetextandimageembeddingsviafine-tuningasmall
36.7Mhigh-qualitytripletsoverawidedistribution.
numberofparametersontopofpre-trainedsingle-modalen-
1https://amateurphotographer.com/review/nikon-z5-review/ coders,withoutlarge-scalejointpre-training. Consequently,
2Self-SupervisedImageRetrievalwithOpen-EndedInstructions
Web Pages Grouping & Cleaning Metadata Expansion w/ LMM Scoring & Filtering
ICA
Alt-text
Caption
0.83
ICA: animal;playset;metal;...
ICA
Alt-text: bricklink rancor Alt-text
Caption: a lego set with a monster
and two minifigures Caption
Training Data Instruction Generation w/ LLM
Same island when it's (Instruction & 5-Shot Demonstrations) Input
sunset.
Both Images are from a website titled [bricklink+rancor cheap buy online]
Q Image: [bricklink rancor] [meta;playset;...] [a lego set with a monster and two minifigures]
What does grilled it
T Image: [lego rancor] [tooth;brown;roar;...] [lego monster with a chain around its hand]
look like?
Response
Find 2020 of this car,
in blue. … So the instruction can be: Monster like this with a chain around its hand and without minifigures.
Figure3.Dataconstructionpipeline.Wemineimagepairsfromthewebvia(1)groupingimagesfromthesamewebpageandcleaning,
(2)annotatingmetadataforeachimagewithLMMs,and(3)scoringandfilteringunqualifiedimagepairs. Eventually,wegenerate
open-endedinstructionsusingLLMsfortheremainingimagepairs.
Both images feature Lego Rancor monsters based on the alt_text. The query image includes
the monster with minifigures, while th se u t car hget aim nag ae dfo acu pse ts a s to il oel ny on s t th re a m teon gst yer w si hth o a ws inferior resuIn ltp sut o ( n5- ts hh eot dem 3o .ns Mtra atio gn is c a Lnd e nm setadata of the image pair)
taskofourinterest,emphasizingtheimportanceoftheMag-
3.1.DataConstructionforSelf-SupervisedTraining
icLens’self-supervisedtraining.
Webdocumentscontainmultimodalcontexts,featuringin-
Composed Image Retrieval. Composed image retrieval
terleavedtextsandimagesonpertinentsubjects.Imagepairs
(CIR;Voetal.(2019))sharesthesametaskformwithus.
extractedfromthesamewebpagethroughco-occurrence
However, all existing benchmarks (Liu et al., 2021; Bal-
frequentlyimplyassociationsbetweenimagesandspecific
dratietal.,2023;Wuetal.,2021)collectvisuallysimilar
relations. This encompasses a broad spectrum of image
images first and then write instructions for image pairs.
relations,rangingfromvisualsimilaritytomorenuanced
Thislimitstherichnessofimagerelationsonthesebench-
connections(e.g.,Figure2). Consequently,thesenaturally
marks and the models developed upon/for them. Recent
occurring image pairs serve as excellent self-supervised
worksonzero-shotCIReitherdesignlight-weightmodality
trainingsignalsforimageretrievalmodels. Basedonthis
transformationoradjusttrainingandmodeltouseexisting
insight,weproposeasystematicdataconstructionpipeline
image-caption(Saitoetal.,2023;Baldratietal.,2023;Gu
to mine image pairs from web pages and adopt LLMs to
etal.,2024)data. CIReVL(Karthiketal.,2024)usesLLM
generateopen-endedinstructionsthatexplicitlyconveythe
andLMMon-the-flyforCIR,limitingitsefficiency. Please
imagerelationswithineachpair.
refertoAppendixBformoredetailsofthesemethods. In
termsofconstructingtrainingdata,CompoDiff(Guetal., Mining Image Pairs from Web Pages. (1) Grouping &
2023)synthesizes18MtripletswithLLMsandimagegener- Cleaning. WecollectallimageswiththesameURLfrom
ativemodels,followingthesamepipelinewithBrooksetal. CommonCrawl2asagroupofimagesfromthesameweb
(2023). Thekeydifferencebetweentheirdataandourslies pageforpotentialpairing. Duetotheinevitablenoisyim-
intheimagequalityandtheimagerelations. Asshownin agesintroducedbysimplegrouping,weremoveduplicated,
Figure2,ourdatacomesfromnaturalimagepairsfoundon low-resolution, and advertising images, as well as highly
thesamewebpages. Thus,ourdatacoversopen-endedim- overlappedgroups. Thisresultsinalargenumberofgroups
agerelationsoverawidedistribution,includingbothvisual withmoredenselyandintrinsicallyconnectedimages.
andbeyond-visualones.
(2)MetadataExpansion. Toprovidedetailedtextualinfor-
Retrieval with Instruction. Instruction tuning (Ouyang mation of images for later LLMs with massive metadata
etal.,2022)enablesmodelswithstrongcross-domainand expansion,weannotateimageswithAlt-texts3,imagecon-
zero-shotgeneralizationcapabilitiesinretrievingbothtex- tent annotation (ICA) labels4, and captions. We discard
tual (Su et al., 2023; Asai et al., 2023) and multimodal imagesiftheirAlt-textsareunqualified. ForICAlabels,we
content (Wei et al., 2023). However, prior efforts focus annotateentitiesforeachimagesuchasgeneralobjectsand
onunifyingdifferentretrievaltaskswithmanually-written activities. Forimagecaptions,weadoptaSOTALMM—
instructionsastaskprefixesofactualqueries,onahundred-
2https://commoncrawl.org/
scalebasis. Incontrast,ourapproachutilizesmillion-scale
3https://en.wikipedia.org/wiki/Alt attribute
instructionsthatnaturallyexpressuser’ssearchintents.
4https://cloud.google.com/vision/docs/labels
3Self-SupervisedImaMgeoRedtreielv aFlwigithuOrepen-EndedInstructions
PaLI(Chenetal.,2023a)togeneratecaptions. Eachtypeof VL Embedding
metadataprovidestextualinformationaboutimagesfrom Encoder
E
differentperspectives. SeeAppendixAformoredetails.
Attention Pooling
Monster like this
(3)Scoring&Filtering. Afterobtaininggroupsofimages with a chain…
Self Attention x K Shared
along with their extensive metadata, we pair up images
withinthesamegroupandeliminateunqualifiedpairsusing V Embedding , L Embedding
E
a combination of relevance measures. We use the CLIP Vision Language
Encoder Encoder
image-to-image score to assess visual relevance and the “”
text-to-textscorefornon-visualrelevance. Imagepairsthat
Figure4.ModelarchitectureandtrainingofMagicLensEncoder
don’t meet the criteria, such as those with low scores in
(E),whichtakesthevisionandlanguageembeddingsandfeeds
bothaspects,areexcludedfromconsideration. Toavoidthe themasasequencetoself-attentionlayersformodalityintegration.
over-samplingofredundantimagesandduplicatedrelations,
ativeexample.Toscaleupthenumberofnegativeexamples,
wesetamaximumof threepairsforeachgroup, thereby
foreachqueryimage,weuseallquerynegativesandother
ensuringamoreuniformdistributionofimagesandrelations
target negatives in the same batch. Formally, for the i-th
inourtrainingdata(seeFigure5).
trainingexample,thelossfunctionL isdefinedas,
i
Open-EndedInstructionGeneration. Withtheinforma-
tivemetadataofhigh-qualitypairedimages,LLMsareable
esim(r qi,r ti)/τ
−log ,
to well understand the image content (ICA and caption) (cid:80)N (esim(r qi,r tj)/τ +esim(r qi,r tj′ )/τ)
j=1
andtheirbackgroundinformation(Alt-text). Usinginstruc-
where sim(,) indicates a cosine similarity function
tion(Chungetal.,2022),few-shotdemonstrations(Brown
rqTrt
, N refers to the sampled batch size, and τ is
et al., 2020), and chain-of-thought prompting (Wei et al., ||rq||·||rt||
2022)techniques,PaLM2(Aniletal.,2023)generatesopen- atemperaturehyperparameterforlogitscaling. Pleaserefer
endedinstructionsthatpreciselyconnectthepairedimages toAppendixAformoreimplementationdetails.
(image ,image ). Figure3illustratesgeneratedinstructions
q t
andAppendix10showsthedetailedpromptanddemonstra- 4.Experiments
tions. Eventually, we obtain 36.7M triplets (image , text,
q
image )forself-supervisedimageretrievaltraining. 4.1.ExperimentSetup
t
BenchmarksandMetrics. Tocomprehensivelyevaluate
3.2.MagicLensModel
MagicLens’ multimodality-to-image retrieval ability, we
considerthreerelatedtasksinazero-shot,one-checkpoint
Model Design. As shown in Figure 4, we adopt a sim-
setting: (1) composed image retrieval (CIR), (2) domain
pledual-encoderarchitecturewithsharedparametersand
transferretrieval,and(3)conditionalimagesimilarity. Each
initializethebackbonevisionandlanguageencoderswith
taskhasdifferentyetlimitedsetsofimagerelations. Table2
CoCa(Yuetal.,2022)orCLIP(Radfordetal.,2021). To
showsthedetailedstatisticsoffivebenchmarks.
enable deep modality integration, we introduce multiple
self-attentionlayersanddesignasinglemulti-headatten- Composed Image Retrieval. We consider one domain-
tionpooler,compressingthemultimodalinputsintoasingle specific and two open-domain benchmarks to evaluate
embedding r for later matching. Additionally, since the model’sdomainadaptabilityanditscapabilityonreal-world
retrievaltargetcomprisesonlyanimagewithoutaccompa- natural images, respectively. FIQ (Wu et al., 2021) is
nyingtext,weemployanemptytextplaceholder“”totrans-
a fashion-domain benchmark with three disjoint retrieval
formthetargetintoamultimodalinput.Wedenoter qasthe sub-tasks: dress, shirt, and toptee. Following previous
embeddingforthemultimodalquery(image q,text)andr
t work (Saito et al., 2023; Baldrati et al., 2023; Gu et al.,
astheembeddingforthetarget(image t,“”). Considering 2024), we evaluate on its validation set and report recall
theefficiency,weproposeMagicLens-BandMagicLens-L, averagedoversub-tasks. CIRR(Liuetal.,2021)isthefirst
initializedwiththebaseandlargecheckpoints,respectively. dataset constructed on natural images (Suhr et al., 2019)
withninepre-definedrelationsbetweenthequeryandthe
ModelTraining. Weuseasimplecontrastivelosstotrain
targetimages.Italsodesignsasubsetretrievalsettingwhere
MagicLens,ourmodelisupdatedbycontrastingthepaired
modelsretrieveatargetimagefromadedicatedsmallsubset
query-targetagainstothertargetsinonetrainingbatch. In
foreachquery. However,inadditiontothelimitedsizeof
particular,asthequeryimageitself(image )canbeachal-
q theretrievalpool,italsosuffersfromfalsenegativeissues,
lenging hard negative for the multimodal query (image ,
q aspointedoutbyBaldratietal.(2023). Weutilizerecall(R
text),wecombinethequeryimageitselfandanemptytext
andR )toevaluatestandardretrievalandsubsetretrieval.
toencode(image ,“”)togetr′ asanadditionalqueryneg- s
q t Incontrast,tobetteralignwiththereal-worldlarge-scale
4
ContrastiveSelf-SupervisedImageRetrievalwithOpen-EndedInstructions
Table1.Performancecomparisononfivebenchmarksofthreemultimodality-to-imageretrievaltasks.Theresultsofbaselinesarefrom
theoriginalpapers.Wemarkthebestresultsinboldandthesecond-bestresultsunderlined.⋆CIReVLusesmultiplemodelcomponents
includingChatGPTforretrieval,wereport#parametersofcomponentswithknownsizes.†PLIdoesnotreleasecodesoweestimate.
ComposedImageRetrieval DomainTrans CondImSim
#Total
Method Backbone FIQ CIRR CIRCO DTIN GeneCIS
Params
R@10 R@1 R @1 mAP@5 R@10 R @1
s s
PALAVRA(Cohenetal.,2022) CLIP-B 176M 19.8 16.6 41.6 4.6 - -
SEARLE(Baldratietal.,2023) CLIP-B 165M 22.9 24.0 54.9 9.4 - -
CIReVL(Karthiketal.,2024) CLIP-B 12.3B⋆ - 23.9 60.2 14.9 - -
PLI(Chen&Lai,2023) BLIP-B 224M† 35.9 27.2 55.1 7.1 - -
MagicLens-B CLIP-B 166M 26.3 27.0 66.7 23.1 28.3 15.0
MagicLens-B CoCa-B 267M 35.2 31.6 69.3 30.8 46.8 17.4
Pic2Word(Saitoetal.,2023) CLIP-L 429M 24.7 23.9 - 8.7 10.1 11.2
SEARLE(Baldratietal.,2023) CLIP-L 442M 25.6 24.2 53.8 11.7 - 12.3
Context-I2W(Tangetal.,2024)CLIP-L 496M 27.8 25.6 - - 12.9 -
CompoDiff(Guetal.,2023) CLIP-L 568M 36.0 18.2 57.4 12.6 - 14.9
CIReVL(Karthiketal.,2024) CLIP-L 12.5B⋆ - 24.6 59.5 18.6 - -
PLI(Chen&Lai,2023) CLIP-L 428M† 35.4 25.5 55.6 10.4 - -
LinCIR(Guetal.,2024) CLIP-L 442M 26.3 25.0 57.1 12.6 - 12.2
MagicLens-L CLIP-L 465M 30.7 30.1 68.1 29.6 41.5 16.3
MagicLens-L CoCa-L 613M 38.0 33.3 70.9 34.1 48.2 16.7
Table2.Statisticsoffiveevaluationbenchmarks.Weaveragethe query image and keyword, models need to find the most
numberofqueriesoversub-tasks(e.g.,FIQ),ifavailable. The# similarimagestothequeryimage,conditionedonthegiven
Indexrepresentsthesizeoftheretrievalpoolthatissharedamongst keyword,fromadedicatedsmallsubsetwith13.8images
allqueriesandthe#SubsetIndexistheaveragesizeofsubsets, onaverage. Forexample,inthechange-objectsub-taskwith
eachofwhichisdedicatedforaquery. keyword“car”andanimage,modelsneedtofindanother
Open-domain #Subset imagebutwithadditionalcars.
#Query #Index
Image?Instr? Index
FIQ ✗ ✗ 2,005 5,179 - Baselines. We consider several baselines: (1)
CIRR ✓ ✗ 4,148 2,316 8.3 PALARVA (Cohen et al., 2022), (2) Pic2Word (Saito
CIRCO ✓ ✗ 800 123,403 - etal.,2023),(3)SEARLE(Baldratietal.,2023),(4)Con-
DTIN ✓ ✗ 10,000 16,983 -
textI2W(Tangetal.,2024),(5)LinCIR(Guetal.,2024),
GeneCIS ✓ ✗ 2,008 - 13.8
(6)CIReVL(Karthiketal.,2024)(7)CompoDiff(Guetal.,
retrieval,CIRCOannotatesmultiplegroundtruthsforeach 2023) and (8) PLI (Chen & Lai, 2023). Details of these
queryandhasover120Knaturalimages(Linetal.,2014) methodsaredescribedinAppendixB.
astheindexset. Therefore,weregardCIRCOasourmain
benchmark. Aseachqueryhasmultipletargets,weadopt 4.2.Multimodality-to-ImageRetrieval
meanAveragePrecision(mAP)astheevaluationmetric.
Table1showsresultsoverfivebenchmarksfromthreetasks,
DomainTransferRetrieval. DomainTransferImageNet fromwhichwehavethefollowingobservations:
(DTIN;Saitoetal.(2023))aimstoretrieveanimagefrom
First, with the comparable model size, both CLIP- and
anotherdomainwiththesameconceptualobjectshownin
CoCa-basedMagicLensoutperformpreviousstate-of-the-
thequeryimage. Itisconstructedfromnaturalimagesin
artmodelsacrossthefouropen-domainbenchmarksbya
ImageNet(Dengetal.,2009)andimagesinotherdomains
largemargin,especiallyCoCa-basedMagicLens-Lonthe
inImageNet-R(Hendrycksetal.,2021).Forexample,given
challengingCIRCO(mAP@5from12.6to34.1)andDTIN
adomainkeyword“cartoon”andarealhorseimageasa
(R@10from12.9to48.2). Thisshowsthestrongcapability
query,modelsareexpectedtoretrieveacartoonhorsefrom
of MagicLens. We leave full results on Appendix C and
theindexsetwithimagesfrommultipledomains. Itcovers
detailedparameterefficiencyanalysison§5.2.
4domains,10Kobjects,andover16Kimagesastheindex
set. Followingpriorworks(Saitoetal.,2023;Karthiketal., Second,bycomparingMagicLens-LtoMagicLens-B,we
2024),wereportrecallaveragedoversub-tasks. findgenerallyconsistentperformanceimprovementsacross
fivebenchmarks. Thisdemonstrates theconstructeddata
Conditional Image Similarity. GeneCIS (Vaze et al.,
isofhighqualityandcanbenefitlargermodels. Also,this
2023)isakeyword-conditionedimagesimilaritymeasure-
observationshowsthescalabilitythankstothesimpledual-
mentbenchmark. Ithasfoursub-tasksaboutchangingor
encodermodelarchitectureandcontrastiveloss.
focusingtheattributeorobjectinthegivenimage. Foreach
5Self-SupervisedImageRetrievalwithOpen-EndedInstructions
Table3. Zero-shotimage-textretrievalresults. Resultsaremarkedinboldiftheyarebetterthaninitializedcheckpoints. ⋆CoCawe
reproducedandusedforMagicLens.†CoCareportedintheoriginalpaper.
Flickr30K(1Ktestset) MSCOCO(5Ktestset)
Model Image→Text Text→Image Image→Text Text→Image
R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10
CoCa-B† 89.8 98.8 99.8 76.8 93.7 96.8 63.8 84.7 90.7 47.5 72.4 80.9
CoCa-B⋆ 88.6 98.5 99.4 74.5 93.4 96.4 63.4 84.2 90.4 46.4 71.5 80.1
MagicLens-B 87.9 97.7 99.5 76.2 93.7 96.5 64.8 85.5 91.2 48.9 73.9 82.5
CoCa-L† 91.4 99.2 99.9 79.0 95.1 97.4 65.4 85.6 91.4 50.1 73.8 81.8
CoCa-L⋆ 92.1 98.8 99.9 78.4 94.2 96.9 65.1 85.5 91.3 49.3 73.2 81.5
MagicLens-L 89.6 98.7 99.4 79.7 95.0 97.4 67.7 87.6 92.7 53.1 77.4 84.9
Table4.Results on three image-to-image retrieval benchmarks. 4.4.Text-to-ImageRetrieval
TheresultsofbaselinesarefromLinetal.(2023)withseparate
SinceMagicLensmodelsarebuiltuponvisionandlanguage
checkpointsforeachbenchmarkwhileMagicLens(ML)models
encoders,thesebackboneencodersaftertrainingcanstillbe
areevaluatedacrossbenchmarksunderone-checkpointsetting.
reusedforimage→textandtext→imageretrievaltasks.
TU-Berlin Sketchy QuickDraw
Method
Therefore,weevaluateMagicLens’backboneencoderson
mAP P@100 mAP@200 P@200 mAP P@200
Flickr30k(Plummeretal.,2015)andMSCOCO(Chenetal.,
ViT(2021) 36.0 50.3 40.3 51.2 10.1 11.3
SOTA(2023) 56.9 63.7 52.5 62.4 14.5 21.6 2015),usingthesamedatasetsplitsandevaluationmetrics
ML-B-CLIP 45.9 57.9 49.3 60.6 10.1 14.0 aspriorworks(Radfordetal.,2021;Yuetal.,2022).
ML-B-CoCa 61.7 72.1 70.5 77.2 13.9 19.9
ML-L-CLIP 62.9 73.1 68.2 75.8 15.1 20.4 Table3showsthecomparisonbetweentheoriginalencoders
ML-L-CoCa 70.2 79.1 75.7 81.3 19.7 27.4 andtheonesupdatedafterMagicLenstraining. Fortext→
imagetask,wecanobserveconsistentandnon-trivialim-
4.3.Image-to-ImageRetrieval provementsacrossallmetricsonbothdatasets. Forimage
→ text task, we observe marginal drops. These observa-
AlthoughMagicLensmodelsaretrainedfor(image ,text)
q tions show that our training recipe can enhance the back-
→ image task format, they can naturally cover image
t q bone encoders for text-to-image retrieval. We can draw
→ image tasks by providing a fixed text instruction for
t the same conclusion with CLIP, which is detailed in Ta-
allimage . Asacasestudy,weconsiderzero-shotsketch
q ble 17. The improvements might stem from the fact that
basedimageretrieval(ZS-SBIR)taskwheremodelsneedto
ourmultimodality-to-imagetrainingtasknecessitatesdeep
retrieveanaturalimagegivenasketchofit.Bysimplyusing
understandingoftextinstruction,thusimprovingbackbone
“findanaturalimageofit”forallqueryimages,MagicLens
languageencoders. Thesetext-to-imageresultsshowthat
canperformsuchatask.
MagicLens can successfully handle three kinds of image
Following the prior zero-shot SOTA methods (Liu et al., retrievaltaskformswithallstrongresults,consideringthe
2019; Lin et al., 2023) in this domain, we consider performancesonothertasksaforementioned.
threebenchmarks,namelyTU-Berlin(Zhangetal.,2016),
Sketchy (Yelamarthi et al., 2018), and QuickDraw (Dey 5.Analysis
etal.,2019).TU-Berlinhas30classes,2,400sketchqueries,
and27,989naturalimagesasindexset;Sketchyconsistsof 5.1.DataAnalysis
21classesunseeninImageNet-1Kand12,694queriesover
anindexsetwith12,553naturalimages;QuickDrawhas30 Comparison to Existing Training Data. Previous data
classes,92,291queries,anda54,146-sizedindexset. For constructionefforts,includingCompoDiff(Guetal.,2023)
eachdataset,wereportmAPandprecisionmetricsusedin andInstructPix2Pix(IP2P;Brooksetal.(2023)),usesyn-
thepriorSOTAwork(Linetal.,2023). thesized image pairs and essentially template-based in-
structionstotrainimageretrievalmodels. Giventhedata
Notably,unlikepreviouszero-shotmethodsthatusesepa- availabilityandthefactthatCompoDiffadoptsacreation
ratecheckpointstrainedoneachdatasetandevaluatedon pipelinesimilartoIP2P,weuseIP2Pdataasourbaselineto
the above holdout test set, we use the same checkpoints exploretheeffectsofdifferenttrainingdataondownstream
forevaluationonallbenchmarks. Resultsarereportedin models. WecompareaCoCa-basedMagicLens-Bmodel
Table4andwecanfindthatourmodelsoutperformprior trainedonallIP2Pdata(1M)withonetrainedonourdown-
SOTAmethodsbyasignificantmargin,despiteouradher- sampled,same-sizeddata,usingthesametrainingrecipe.
encetoasinglecheckpointsetting. Thisdemonstratesthe Table5showsthatMagicLens+Oursachievesperformance
stronggeneralizationcapabilityofMagicLensmodelsand advantages over its variant trained with IP2P data (Mag-
thediversityoftasksthattheycancover.
6Self-SupervisedImageRetrievalwithOpen-EndedInstructions
50
Table5.PerformancecomparisonofCoCa-basedMagicLens-B FIQ
trainedwith1MIP2Pdataagainst1Mourdata.Wereportaveraged CIRR
R@10&R@50onFIQandaveragedR@1&R @1CIRRfor CIRCO
s
40
faircomparisonswithCompoDiff(Guetal.,2023). DTIN
FIQ CIRR CIRCO DTIN GeneCIS GeneCIS
(RAVG) (RAVG) (mAP@5) (R@10) (R@1) Average
30
CompoDiff+IP2P 27.2 27.4 - - -
MagicLens+IP2P 29.8 33.7 13.6 30.2 14.5
MagicLens+Ours 43.7 48.2 29.7 43.7 15.8
20
0.2 0.5 1 5 10 2537
#OurTrainingData(M)
Figure6.PerformanceofCoCa-basedMagicLens-Bwhentrained
withdifferentsizesofourdata.
Table6.Results of CoCa-based MagicLens-B trained with
template-basedandtemplate-freeinstructions,at1Mscale.
FIQ CIRR CIRCO DTIN GeneCIS
Instruction
R@10 R@1 mAP@5 R@10 R@1
(a) IP2P (b) Ourtrainingdata
Template-based 33.4 23.4 25.1 23.1 14.6
Figure5. WorddistributionsofIP2Pdataandourdata. Template-free 33.5 29.6 29.7 43.7 15.8
icLens + IP2P) on all five benchmarks. This proves that
Impacts of Instructions during Training. Instructions
ourdatawithnaturalimagesandtemplate-freeinstructions
used in previous works (Brooks et al., 2023; Gu et al.,
canenablestrongerimageretrievalmodels. Pleasereferto
2023)arerootedfromtemplateswhileourinstructionsare
AppendixCfordetailedcomparisons.
template-free. Toinvestigatetheeffectsofdifferentinstruc-
Inaddition,wecomparethesetwomodelswithIP2P-trained tionsondownstreammodels,wealsosynthesizetemplate-
CompoDiff,whichisaretrievalmodeldesignedforusing basedinstructionsfornaturallyoccurringimagepairscol-
synthesizedimages. Despiteitsspecificdesign,MagicLens lectedin§3.1. Specifically,duetothemassiveinformative
+ IP2P still outperforms the CompoDiff + IP2P. Also, it metadataofeachimage,weutilizeLLMstodeterminekey
achievesbetterresultsonCIRCO,DTIN,andGeneCISthan metadata to fill pre-defined sentence structures. For our
prior comparable-sized SOTA baselines. These show the template-freeinstructions,LLMsarespecificallyguidedto
advantageofourtrainingrecipe,asourmodelcanachieve generate diverse and coherent instructions without adher-
decentresultsevenwhentrainedonsub-optimaldata. ingtoanyfixedtemplate. Weshowconcreteexamplesof
differentinstructionsinFigure10intheAppendix.
Toprovidemoreinsights,wevisualizethewordsofinstruc-
tions in IP2P and in our data separately in Figure 5. As Table 6 compares the performance of two CoCa-based
we can see, IP2P data has a large number of instruction MagicLens-B models. Both of them are trained on 1M
keywordslike“turn”and“make”duetoitstemplate-based triplets,usingthesameimagepairsbutdifferentinstructions
nature. Also,ithasmanycoarse-grainedkeywordssuchas mentionedabove. Template-freeinstructionsclearlyresult
“photograph”and“painting”. Incontrast,becauseofthe inastrongermodel,asevidencedbyconsistentlybetterre-
controlledsamplingfromonewebpagedescribedin§3.1, sultsonallbenchmarkscomparedtotheothermodel. This
ourdatahasmorediverseandequallydistributedkeywords, demonstratesthatnaturallyexpressedanddiverseinstruc-
coveringfine-grainedlabelslike“brand”. tions can better stimulate the model to understand image
relationsandfollowinstructions.
DataScaling. Toinvestigatetheeffectofourdatascaleon
models,wetrainCoCa-basedMagicLens-Bonrandomly 5.2.ModelAnalysis
sampledsetsof0.2M,0.5M,1M,5M,10M,25M,andthe
entire36.7Mtriplets. Resultsonfivebenchmarksandtheir ModelSizevs.Performance.PreviousSOTAmethods(Gu
averaged performance are illustrated in Figure 6. As the etal.,2024;2023)considerusingbothlargervisionandlan-
datasizeincreases,MagicLensshowsenhancedaverageper- guage encoders (Cherti et al., 2023) or using LMMs and
formance,especiallybeforethe10Mmark. Thisindicates LLMs (Karthik et al., 2024) on-the-fly for performance
theeffectivenessofscalingdata. benefits. However,wearguethatthemodelsizesandthe
7
ecnamrofrePSelf-SupervisedImageRetrievalwithOpen-EndedInstructions
MagicLens Pic2Word SEARLE LinCIR CompoDiff CIReVL
CIRCO(mAP@5) DTIN(R@10) GeneCIS(R@1)
50
34
17
30
40 16
26
15
22
30 14
18
13
14 20
12
10
6 10 11
0.2 2 15 0.2 2 15 0.2 2 15
#Parameters(B) #Parameters(B) #Parameters(B)
Figure7. ModelSizevs.Performance.MagicLens-BoutperformstheSOTACIReVLonthreetasksevenwith50×smaller#Parameters.
Table7.AblationstudyonCoCa-basedMagicLens-Btakingquery Table8. ResultsofMagicLensvariants.CrossAttnindicatesthe
imagesasnegativesamplesduringtraining(QryNeg). modelwithcross-attentioninsteadofself-attentionformodality
FIQ CIRR CIRCO DTIN GeneCIS integration.FrozenEncmeansthemodelwithbackbonevisionand
R@10 R@1 mAP@5 R@10 R@1 languageencodersfrozenduringtraining.
FIQ CIRR CIRCO DTIN GeneCIS
MagicLens 35.2 31.6 30.8 46.8 17.4
R@10 R@1 mAP@5 R@10 R@1
w/oQryNeg 33.2 1.6 11.9 14.1 14.5
MagicLens-B 35.2 31.6 30.8 46.8 17.4
w/CrossAttn 31.0 28.3 27.0 41.4 16.2
correlatedefficienciesshouldalsobetakenintoconsidera- w/FrozenEnc 30.8 25.9 21.7 30.1 15.2
tionforreal-worlddeployments. InFigure7,wevisualize MagicLens-L 38.0 33.3 34.1 48.2 16.7
therelationshipbetweenmodelsizeandperformanceofvar- w/CrossAttn 32.3 29.9 28.5 52.5 16.5
iousmodelsonGeneCIS,CIRCO,andDTINbenchmarks. w/FrozenEnc 32.5 26.5 23.0 29.4 15.5
The results on GeneCIS and CIRCO are from Gu et al.
(2024;2023)andusingCLIP-Large,OpenCLIP-Huge,and
structions. Thisindicatesthatdifferentiatingcloselysimilar
OpenCLIP-Giantbackbones(Radfordetal.,2021;Cherti
imagesiscrucialinimprovingthemodel’sinstructionun-
etal.,2023). ResultsofCIReVL(Karthiketal.,2024)on
derstandingcapabilities. Importantly,althoughusingquery
DTINandGeneCISarenotfullyreportedbytheauthors.
negativesseemstolimitMagicLens’abilitytofindtheiden-
WeomitthesizeofChatGPTusedandonlycountparam-
ticalimage,thefirstexampleinFigure1showsMagicLens
etersofCIReVL’sothermodelcomponents(e.g., BLIP2-
cangeneralizetothisinstructionunseenduringtrainingand
FLANT5-XXL+OpenCLIP-Giant).
successfullyretrievetheidenticalimage.
Despitethe50×smallersizeofCoCa-basedMagicLens-B
AblationonModelArchitecture. Weprovideresultsof
(267M) compared to other baselines (e.g., CIReVL with
othermodelarchitectureswehaveexploredinTable8. In
14.6B),itachievesbetterperformanceonthesebenchmarks,
CrossAttnmodelarch,weexplorevariousformsofcross
withasignificantadvantageontheDTIN.Thisobservation
attention,wereportthebestonewhichusestextembedding
demonstratesthehighparameterefficiencyintroducedby
toattendconcatenatedimageandtextembeddings. How-
theparameter-sharingdesigninourmodelandthestrong
ever, even the best variant of this arch fails to reach the
advantageofourdatainenablingstrongyetsmallmodels.
performanceofselfattentiononmostbenchmarks.
DetailedresultsareinAppendixTable14,15,and16.
We also explore the impact of freezing the backbone en-
AblationonContrastiveLoss. Comparedtostandardcon-
codersinitializedfromCoCa(Yuetal.,2022)duringtrain-
trastiveloss, weintroducequeryimagesashardnegative
ing. TheresultsofFrozenEncareconsistentlyworsethan
examplesduringtraining. Toinvestigatetheimpactofthis
thefully-trainedMagicLens. Thisprovesthatmerelytrain-
design,wetrainCoCa-basedMagicLens-Bwithoutthese
ingadditionallayersonthetopofsingle-modalityencoders
hardnegativesandreporttheresultsinTable7. Aswecan
isnotsufficienttodeliverthestrongestmodel.
see,withoutquerynegatives,theperformanceofMagicLens
dropsacrossallbenchmarks,significantlyonCIRR,CIRCO,
5.3.Retrievalon1.4MOpen-DomainImageCorpus
andDTINbenchmarks. Also,wefindinmanycases,this
model prefers to rank the query image itself higher than To simulate image retrieval in a more realistic scenario,
other images during retrieval, regardless of the given in- we hold out 1.4M unseen images as our index set, mak-
8
ecnamrofrePSelf-SupervisedImageRetrievalwithOpen-EndedInstructions
Case Study Open-domain Retrieval
Same car model as the Bucket bag from the
Table9.One-on-onecomparison(winrate)onaholdoutindexset given image, but a 2013 same brand, in gray,
with1.4Mimages.Eachsettinghas50querieswithmanuallywrit- model, blue in color, and without a person
parked in front of trees. holding the bag.
teninstructions.Theresultsareaveragedoverthreeevaluators. MagicLens LinCIR MagicLens LinCIR
InstructionType MagicLens-LWin LinCIRWin Tie
SimpleVisual 50.7 41.3 8.0
ComplexVisual 61.3 24.0 14.7
BeyondVisual 80.0 4.7 15.3
Baking muffins, but show 3D rendering of human
the process of adding the anatomy of a different
pumpkin pie spice. body part like the head.
ing it the largest retrieval pool to date. We then collect
MagicLens LinCIR MagicLens LinCIR
150imagesanddividethemintothreedisjointgroupswith
different types of manually written instructions: simple,
complex,andbeyondvisual. Bothsimpleandcomplexin-
structionsareusedinsearchingforvisuallysimilarimages,
buttheydifferintermsofcomplexity. Simpleinstructions Figure8.Top-1retrievedimagesofCoCa-basedMagicLens-Land
describeonlyonevisualdifference(e.g. same product LinCIRontheholdoutindexsetwith1.4Mimages.Queriesare
with different color) in the images given, whereas withabluebackground,whilecorrectandincorrectretrievedim-
complexoneshavemultipledifferences(e.g.,carandbag agesaremarkedwithgreenandredoutlines,respectively.LinCIR
examplesinFigure8). Beyondvisualinstructionsaimto failstoretrievecorrectresultsforcar,bag,andmuffinqueries,
findimagesthatsharenovisualsimilaritieswiththequery evenconsideringitstop-5results(SeeFigure11inAppendix).
images(e.g.,find other attractions... inFigure1).
Table 9 compares CoCa-based MagicLens-L with code-
availableprevious-bestmodel(LinCIR;Guetal.(2024)),
both with ViT-L backbones. For each query, one-on-one
humanevaluationisappliedtotheimagesretrievedbythese
twomodelstoselecttheonethatfullymeetstheinstructions.
Ifbothorneitherofthemodelssucceed,theevaluatorswill
mark them as a tie. We can observe LinCIR can handle
Figure9.Top-2retrievalresultsonfourdomainsgiventhesame
simpleinstructionsbutsuffersfromcomplexinstructions
queryimageontheDTINbenchmark.
andalmostcompletelyfailsonbeyondvisualinstructions.
In contrast, ourmethod cansatisfy diverse searchintents
expressed by all kinds of instructions, remarkably on the resultsarecorrect,highlightingtheeffectivenessofMagi-
complex(61.3vs. 24.0)andbeyondvisual(80vs. 4.7)ones. cLensinunderstandingconceptualimagerelations.
5.4.QualitativeStudy
6.Conclusion
Figure 8 illustrates top-1 retrieval results on the holdout
WepresentMagicLens,aseriesofself-supervisedlytrained
indexsetwith1.4Mimages. Evenwithcomplexinstruction
imageretrievalmodelsthatfollowopen-endedtextinstruc-
containing multiple conditions (car and bag examples),
tions. Despitebeing50×smallerthanpriorSOTAmethods,
MagicLens is still able to accurately comprehend search
MagicLensachievesbetterresultsonmultiplebenchmarks
intentsandretrievedesiredimages. Themuffinexample
includingCIRCO,GeneCIS,andDTIN.Humanevaluation
showcasesthatMagicLenscanunderstandthenon-trivial
onthe1.4MretrievalimagepoolshowsthatMagicLenscan
temporal relation between images, thanks to the relation
wellsatisfydiversesearchintentsexpressedbyopen-ended
diversity introduced by naturally occurring image pairs.
instructions, especially complex and beyond visual ones.
However,theimageretrievedbyMagicLensgiventhe3D
ThisindicatesMagicLens’strongcapabilityandpotential
anatomyquerymaynotbegenerallypreferredsincethe
forreal-worldsearchscenarios. Suchretrievalmodelsthat
instructionexemplifiesthehead. Thissuggestsourmodel
supportopen-endedinstructionscanpotentiallybenefitother
mayreturnqualifiedyetnon-perfectexampleswhenthein-
vision-languagetaskssuchasvisualQA(Antoletal.,2015;
structionisnotclearlyexpressed. PleaserefertoFigure11
Chenetal.,2023c),andenhancemultimodalretrievalaug-
inAppendixDformorequalitativestudies.
mentedmodels(Chenetal.,2022;Huetal.,2023b). More
Figure 9 presents a visual case study on domain transfer importantly,wehopeourrecipeforconstructinglarge-scale
retrievalusingtheDTINbenchmark. Thetextinstruction synthetic self-supervised training data can shed light on
presentedineachdomainis“findthisobjectin{domain}”, otherresearchdirections,suchasmultimodalretrieval,mul-
where the same query image is used. All top-2 retrieved timodalrepresentationlearning,andbeyond.
9Self-SupervisedImageRetrievalwithOpen-EndedInstructions
7.ImpactStatements Baldrati,A.,Agnolucci,L.,Bertini,M.,andDelBimbo,A.
Zero-shotcomposedimageretrievalwithtextualinver-
Thisworkprovidesnovelinsightsintoself-supervisedtrain-
sion. InProceedingsofICCV,2023.
ingsignalsbyminingnaturallyoccurringimagepairsand
developsimageretrievalmodelsthatfollowopen-endedin- Brooks,T.,Holynski,A.,andEfros,A.A. Instructpix2pix:
structionstosatisfydiversesearchintents. Itmayenablea Learning to follow image editing instructions. In Pro-
widerangeofsearchscenariosandhavepotentialsforreal- ceedingsofCVPR,2023.
worldapplicationsbyprovidinguserswithmoreaccurate
search results. Therefore, we do not see major ethical or Brown,T.,Mann,B.,Ryder,N.,Subbiah,M.,Kaplan,J.D.,
socialconcernsassociatedwithourwork. Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,
Askell,A.,Agarwal,S.,Herbert-Voss,A.,Krueger,G.,
Acknowledgements Henighan,T.,Child,R.,Ramesh,A.,Ziegler,D.,Wu,J.,
Winter,C.,Hesse,C.,Chen,M.,Sigler,E.,Litwin,M.,
TheauthorswouldliketothankJinhyukLee,WilliamCo- Gray, S., Chess, B., Clark, J., Berner, C., McCandlish,
hen, Jonathan Berant, Kristina Toutanova, Boqing Gong, S.,Radford,A.,Sutskever,I.,andAmodei,D. Language
andothermembersfromtheGoogleDeepMindSeattlefor modelsarefew-shotlearners. InProceedingsofNeurIPS,
theirconstructivefeedback. 2020.
Chen,J.andLai,H. Pretrainlikeyourinference: Masked
References
tuning improves zero-shot composed image retrieval.
Anil,R.,Dai,A.M.,Firat,O.,Johnson,M.,Lepikhin,D., arXivpreprintarXiv:2311.07622,2023.
Passos,A.,Shakeri,S.,Taropa,E.,Bailey,P.,Chen,Z.,
Chu, E., Clark, J. H., Shafey, L. E., Huang, Y., Meier- Chen,J.,Hu,H.,Wu,H.,Jiang,Y.,andWang,C. Learning
Hellstern, K., Mishra, G., Moreira, E., Omernick, M., thebestpoolingstrategyforvisualsemanticembedding.
Robinson,K.,Ruder,S.,Tay,Y.,Xiao,K.,Xu,Y.,Zhang,
InProceedingsofCVPR,2021.
Y.,Abrego,G.H.,Ahn,J.,Austin,J.,Barham,P.,Botha,
Chen,W.,Hu,H.,Chen,X.,Verga,P.,andCohen,W.W.
J., Bradbury, J., Brahma, S., Brooks, K., Catasta, M.,
Murag: Multimodal retrieval-augmented generator for
Cheng,Y.,Cherry,C.,Choquette-Choo,C.A.,Chowd-
openquestionansweringoverimagesandtext. InPro-
hery, A., Crepy, C., Dave, S., Dehghani, M., Dev, S.,
ceedingsofEMNLP,2022.
Devlin,J.,D´ıaz,M.,Du,N.,Dyer,E.,Feinberg,V.,Feng,
F., Fienber, V., Freitag, M., Garcia, X., Gehrmann, S.,
Chen, X., Fang, H., Lin, T.-Y., Vedantam, R., Gupta, S.,
Gonzalez,L.,Gur-Ari,G.,Hand,S.,Hashemi,H.,Hou,
Dollar, P., and Zitnick, C. L. Microsoft coco captions:
L.,Howland,J.,Hu,A.,Hui,J.,Hurwitz,J.,Isard,M.,It-
Data collection and evaluation server. rXiv preprint
tycheriah,A.,Jagielski,M.,Jia,W.,Kenealy,K.,Krikun,
arXiv:1504.00325,2015.
M.,Kudugunta,S.,Lan,C.,Lee,K.,Lee,B.,Li,E.,Li,
M.,Li,W.,Li,Y.,Li,J.,Lim,H.,Lin,H.,Liu,Z.,Liu, Chen,X.,Djolonga,J.,Padlewski,P.,Mustafa,B.,Chang-
F.,Maggioni,M.,Mahendru,A.,Maynez,J.,Misra,V., pinyo,S.,Wu,J.,Ruiz,C.R.,Goodman,S.,Wang,X.,
Moussalem,M.,Nado,Z.,Nham,J.,Ni,E.,Nystrom,A., Tay,Y.,etal. Pali-x: Onscalingupamultilingualvision
Parrish,A.,Pellat,M.,Polacek,M.,Polozov,A.,Pope, andlanguagemodel. arXivpreprintarXiv:2305.18565,
R.,Qiao,S.,Reif,E.,Richter,B.,Riley,P.,Ros,A.C., 2023a.
Roy, A., Saeta, B., Samuel, R., Shelby, R., Slone, A.,
Smilkov,D.,So,D.R.,Sohn,D.,Tokumine,S.,Valter, Chen, X., Wang, X., Changpinyo, S., Piergiovanni, A.,
D., Vasudevan, V., Vodrahalli, K., Wang, X., Wang, P., Padlewski, P., Salz, D., Goodman, S., Grycner, A.,
Wang,Z.,Wang,T.,Wieting,J.,Wu,Y.,Xu,K.,Xu,Y., Mustafa, B., Beyer, L., Kolesnikov, A., Puigcerver, J.,
Xue, L., Yin, P., Yu, J., Zhang, Q., Zheng, S., Zheng, Ding, N., Rong, K., Akbari, H., Mishra, G., Xue, L.,
C.,Zhou,W.,Zhou,D.,Petrov,S.,andWu,Y. Palm2 Thapliyal,A.V.,Bradbury,J.,Kuo,W.,Seyedhosseini,
technicalreport. arXivpreprintarXiv:2305.10403,2023. M.,Jia,C.,Ayan,B.K.,Ruiz,C.R.,Steiner,A.P.,An-
gelova,A.,Zhai,X.,Houlsby,N.,andSoricut,R. PaLI:
Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Ajointly-scaledmultilinguallanguage-imagemodel. In
Zitnick, C. L., and Parikh, D. VQA: visual question ProceedingsofICLR,2023b.
answering. InProceedingsofICCV,2015.
Chen,Y.,Hu,H.,Luan,Y.,Sun,H.,Changpinyo,S.,Ritter,
Asai,A.,Schick,T.,Lewis,P.,Chen,X.,Izacard,G.,Riedel, A.,andChang,M. Canpre-trainedvisionandlanguage
S., Hajishirzi, H., and Yih, W.-t. Task-aware retrieval modelsanswervisualinformation-seekingquestions? In
withinstructions. InFindingsofACL,2023. ProceedingsofEMNLP,2023c.
10Self-SupervisedImageRetrievalwithOpen-EndedInstructions
Cherti, M., Beaumont, R., Wightman, R., Wortsman, M., Hu, H., Luan, Y., Chen, Y., Khandelwal, U., Joshi, M.,
Ilharco, G., Gordon, C., Schuhmann, C., Schmidt, L., Lee,K.,Toutanova,K.,andChang,M.-W. Open-domain
andJitsev,J. Reproduciblescalinglawsforcontrastive visualentityrecognition: Towardsrecognizingmillions
language-imagelearning. InProceedingsofCVPR,2023. ofwikipediaentities. InProceedingsofCVPR,2023a.
Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Hu, Z., Iscen, A., Sun, C., Wang, Z., Chang, K.-W., Sun,
Fedus,W.,Li,Y.,Wang,X.,Dehghani,M.,Brahma,S., Y., Schmid, C., Ross, D. A., and Fathi, A. Reveal:
Webson, A., Gu, S. S., Dai, Z., Suzgun, M., Chen, X., Retrieval-augmentedvisual-languagepre-trainingwith
Chowdhery, A., Castro-Ros, A., Pellat, M., Robinson, multi-source multimodal knowledge memory. In Pro-
K.,Valter,D.,Narang,S.,Mishra,G.,Yu,A.,Zhao,V., ceedingsofCVPR,2023b.
Huang,Y.,Dai,A.,Yu,H.,Petrov,S.,Chi,E.H.,Dean,
J.,Devlin,J.,Roberts,A.,Zhou,D.,Le,Q.V.,andWei, Jia,C.,Yang,Y.,Xia,Y.,Chen,Y.-T.,Parekh,Z.,Pham,H.,
J. Scalinginstruction-finetunedlanguagemodels. arXiv Le, Q., Sung, Y.-H., Li, Z., and Duerig, T. Scaling up
preprintarXiv:2210.11416,2022. visualandvision-languagerepresentationlearningwith
noisytextsupervision. InProceedingsofICML,2021.
Cohen,N.,Gal,R.,Meirom,E.A.,Chechik,G.,andAtz-
mon,Y. “thisismyunicorn,fluffy”:Personalizingfrozen Karthik,S.,Roth,K.,Mancini,M.,andAkata,Z. Vision-by-
vision-languagerepresentations.InProceedingsofECCV, languagefortraining-freecompositionalimageretrieval.
2022. InProceedingsofICLR,2024.
Datta,R.,Joshi,D.,Li,J.,andWang,J.Z. Imageretrieval: Kim,W.,Son,B.,andKim,I. Vilt: Vision-and-language
Ideas,influences,andtrendsofthenewage. ACMCom- transformer without convolution or region supervision.
put.Surv.,2008. InProceedingsofICML,2021.
Deng,J.,Dong,W.,Socher,R.,Li,L.-J.,Li,K.,andFei-Fei, Li,J.,Selvaraju,R.,Gotmare,A.,Joty,S.,Xiong,C.,and
L. Imagenet: Alarge-scalehierarchicalimagedatabase. Hoi, S. C. H. Align before fuse: Vision and language
InProceedingsofCVPR,2009. representationlearningwithmomentumdistillation. In
ProceedingsofNeurIPS,2021.
Dey, S., Riba, P., Dutta, A., Llados, J., and Song, Y.-Z.
Doodletosearch: Practicalzero-shotsketch-basedimage
Li,J.,Li,D.,Xiong,C.,andHoi,S. BLIP:Bootstrapping
retrieval. InProceedingsofCVPR,2019.
language-imagepre-trainingforunifiedvision-language
understandingandgeneration. InProceedingsofICML,
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,
2022.
D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer,
M.,Heigold,G.,Gelly,S.,Uszkoreit,J.,andHoulsby,N.
Li, J., Li, D., Savarese, S., and Hoi, S. BLIP-2: Boot-
Animageisworth16x16words: Transformersforimage
strappinglanguage-imagepre-trainingwithfrozenimage
recognitionatscale. InProceedingsofICLR,2021.
encodersandlargelanguagemodels. InProceedingsof
Faghri,F.,Fleet,D.J.,Kiros,J.R.,andFidler,S.Vse++:Im- ICML,2023.
provingvisual-semanticembeddingswithhardnegatives.
Lin,F.,Li,M.,Li,D.,Hospedales,T.,Song,Y.-Z.,andQi,
arXivpreprintarXiv:1707.05612,2017.
Y. Zero-shoteverythingsketch-basedimageretrieval,and
Gordo,A.,Almaza´n,J.,Revaud,J.,andLarlus,D. Deepim- inexplainablestyle. InProceedingsofCVPR,2023.
ageretrieval: Learningglobalrepresentationsforimage
Lin, T., Maire, M., Belongie, S. J., Hays, J., Perona, P.,
search. InProceedingsofECCV,2016.
Ramanan, D., Dolla´r, P., and Zitnick, C. L. Microsoft
Gu, G., Chun, S., Kim, W., Jun, H., Kang, Y., and Yun, COCO:commonobjectsincontext. InProceedingsof
S. Compodiff: Versatilecomposedimageretrievalwith ECCV,2014.
latentdiffusion. arXivpreprintarXiv:2303.11916,2023.
Liu,Q.,Xie,L.,Wang,H.,andYuille,A. Semantic-aware
Gu, G., Chun, S., Kim, W., , Kang, Y., and Yun, S. knowledgepreservationforzero-shotsketch-basedimage
Language-only training of zero-shot composed image retrieval. InProceedingsofICCV,2019.
retrieval. InProceedingsofCVPR,2024.
Liu, Z., Rodriguez-Opazo, C., Teney, D., and Gould, S.
Hendrycks,D.,Basart,S.,Mu,N.,Kadavath,S.,Wang,F.,
Imageretrievalonreal-lifeimageswithpre-trainedvision-
Dorundo, E., Desai, R., Zhu, T., Parajuli, S., Guo, M.,
and-languagemodels. InProceedingsofICCV,2021.
Song,D.,Steinhardt,J.,andGilmer,J. Themanyfaces
ofrobustness: Acriticalanalysisofout-of-distribution OpenAI. Chatgpt,2022. URLhttps://openai.com/
generalization. InProceedingsofICCV,2021. blog/chatgpt.
11Self-SupervisedImageRetrievalwithOpen-EndedInstructions
Ouyang,L.,Wu,J.,Jiang,X.,Almeida,D.,Wainwright,C., Vo, N., Jiang, L., Sun, C., Murphy, K., Li, L.-J., Fei-Fei,
Mishkin,P.,Zhang,C.,Agarwal,S.,Slama,K.,Ray,A., L., and Hays, J. Composing text and image for image
Schulman,J.,Hilton,J.,Kelton,F.,Miller,L.,Simens,M., retrieval-anempiricalodyssey. InProceedingsofCVPR,
Askell,A.,Welinder,P.,Christiano,P.F.,Leike,J.,and 2019.
Lowe,R.Traininglanguagemodelstofollowinstructions
Wang,W.,Bao,H.,Dong,L.,Bjorck,J.,Peng,Z.,Liu,Q.,
withhumanfeedback. InProceedingsofNeurIPS,2022.
Aggarwal,K.,Mohammed,O.K.,Singhal,S.,Som,S.,
Plummer,B.A.,Wang,L.,Cervantes,C.M.,Caicedo,J.C., andWei,F. Imageasaforeignlanguage: Beitpretraining
Hockenmaier, J., and Lazebnik, S. Flickr30k entities: forvisionandvision-languagetasks. InProceedingsof
Collectingregion-to-phrasecorrespondencesforricher CVPR,2023.
image-to-sentence models. In Proceedings of ICCV,
Wei,C.,Chen,Y.,Chen,H.,Hu,H.,Zhang,G.,Fu,J.,Ritter,
2015.
A.,andChen,W. Uniir: Trainingandbenchmarkinguni-
Radford,A.,Kim,J.W.,Hallacy,C.,Ramesh,A.,Goh,G., versalmultimodalinformationretrievers. arXivpreprint
Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, arXiv:2311.17136,2023.
J.,Krueger,G.,andSutskever,I. Learningtransferable
Wei,J.,Wang,X.,Schuurmans,D.,Bosma,M.,ichter,b.,
visual models from natural language supervision. In
Xia,F.,Chi,E.,Le,Q.V.,andZhou,D. Chain-of-thought
ProceedingsofICML,2021.
promptingelicitsreasoninginlargelanguagemodels. In
Saito,K.,Sohn,K.,Zhang,X.,Li,C.-L.,Lee,C.-Y.,Saenko, ProceedingsofNeurIPS,2022.
K.,andPfister,T. Pic2word: Mappingpicturestowords
forzero-shotcomposedimageretrieval. InProceedings Wu,H.,Gao,Y.,Guo,X.,Al-Halah,Z.,Rennie,S.,Grau-
ofCVPR,2023. man,K.,andFeris,R. Fashioniq: Anewdatasettowards
retrievingimagesbynaturallanguagefeedback. InPro-
Schuhmann,C.,Beaumont,R.,Vencu,R.,Gordon,C.W., ceedingsofCVPR,2021.
Wightman,R.,Cherti,M.,Coombes,T.,Katta,A.,Mullis,
C., Wortsman, M., Schramowski, P., Kundurthy, S. R., Yelamarthi,S.K.,Reddy,S.K.,Mishra,A.,andMittal,A.
Crowson,K.,Schmidt,L.,Kaczmarczyk,R.,andJitsev, Azero-shotframeworkforsketchbasedimageretrieval.
J. LAION-5b: Anopenlarge-scaledatasetfortraining InFerrari,V.,Hebert,M.,Sminchisescu,C.,andWeiss,
next generation image-text models. In Proceedings of Y.(eds.),ProceedingsofECCV,2018.
NeurIPS,2022.
Yu,J.,Wang,Z.,Vasudevan,V.,Yeung,L.,Seyedhosseini,
Sharma,P.,Ding,N.,Goodman,S.,andSoricut,R. Con- M.,andWu,Y. Coca: Contrastivecaptionersareimage-
ceptualcaptions: Acleaned,hypernymed,imagealt-text textfoundationmodels. TransactionsonMachineLearn-
datasetforautomaticimagecaptioning. InProceedings ingResearch,2022.
ofACL,2018.
Zhai,X.,Kolesnikov,A.,Houlsby,N.,andBeyer,L.Scaling
Shazeer, N. and Stern, M. Adafactor: Adaptive learning visiontransformers. InProceedingsofCVPR,2022.
rates with sublinear memory cost. In Proceedings of
Zhang,H.,Liu,S.,Zhang,C.,Ren,W.,Wang,R.,andCao,
ICML,2018.
X. Sketchnet: Sketchclassificationwithwebimages. In
Su, H., Shi, W., Kasai, J., Wang, Y., Hu, Y., Ostendorf, ProceedingsofCVPR,2016.
M., Yih, W.-t., Smith, N. A., Zettlemoyer, L., and Yu,
T. One embedder, any task: Instruction-finetuned text
embeddings. InFindingsofACL,2023.
Suhr, A., Zhou, S., Zhang, A., Zhang, I., Bai, H., and
Artzi,Y. Acorpusforreasoningaboutnaturallanguage
groundedinphotographs. InProceedingsofACL,2019.
Tang, Y., Yu, J., Gai, K., Zhuang, J., Xiong, G., Hu, Y.,
andWu, Q. Context-i2w: Mappingimagestocontext-
dependentwordsforaccuratezero-shotcomposedimage
retrieval. InProceedingsofAAAI,2024.
Vaze,S.,Carion,N.,andMisra,I. Genecis: Abenchmark
forgeneralconditionalimagesimilarity. InProceedings
ofCVPR,2023.
12Self-SupervisedImageRetrievalwithOpen-EndedInstructions
OverviewofAppendix
Oursupplementaryincludesthefollowingsections:
• SectionA:ImplementationDetails.
• SectionB:Baselines.
• SectionC:FullResults.
• SectionD:MoreQualitativeStudy.
A.ImplementationDetails
ImageCleaningandPairing. WeusetheCommonCrawlandgroupimageswithidenticalURLs,consideringthemas
imagesfromthesamewebsites. WeregardtwoimagesasidenticaliftheirCLIPimageembeddingscoresexceed0.98
andremovethem. Iftwogroupsshareahighratioofduplicatedimages(80%),werandomlyremoveoneofthosegroups.
Theminimumresolutionremainedis288x288,whichmatchestheinputsizeofCoCamodelsweused. Fortheconcrete
thresholdsusedforfiltering,wehaveset0.82asthethresholdforCLIPimage-to-imagesimilarityand0.9fortext-to-text
similarityovercaptions. Additionally,toensuretheuniquenessoftheimages,thetargetimagemusthaveadistinctICA
labelwithhightext-imagesimilaritytoitself(0.32)andlowsimilaritytothequeryimage(0.18). Onlyimagepairsthatmeet
theserequirementswillberemainedfortheinstructiongenerationstage.
InstructionGeneration. WeprovideLLMswithmassivemetadataexpansionincludingAlt-texts,imagecontentannotation
(ICA)labels,andimagecaptionsbyusingvarioustoolsandLMMs. Specifically,similartoSharmaetal.(2018),weanalyze
candidateAlt-textswithpart-of-speech,sentiment,andpornographyannotationsofGoogleNaturalLanguageAPIs. We
discardimagesiftheirAlt-textsonlyhaveraretokensoriftheyaretriggeredbysentiment/pornographydetectorsForICA
labels,weutilizeGoogleVisionAPIstoannotateentitiesforeachimagesuchasgeneralobjects,locations,andactivities.
Onaverage,eachimagehas25.2fine-grainedICAlabels. Also,weprovidetheinstructionandtwodetaileddemonstrations
forinstructiongenerationinTable10.
Model. Withtheproposeddataconstructionpipeline,weeventuallycollect36,714,118tripletsforpre-training. Formodel
architecture,wedesign4randomlyinitializedself-attentionlayersonthetopofvisionandlanguageencoders. Further,
we utilize one attention pooling layer (Yu et al., 2022) for the final embedding. Following Jia et al. (2021); Yu et al.
(2022),duringthetrainingofCoCa-basedMagicLens,wesetimageresolutionof288×288andpatchsize18×18. For
CLIP-basedMagicLens,wesetimageresolutionof224×224anduseViT-B16andViT-L14. ForbothCLIPandCoCa,we
usecontrastiveimageembeddingandcontrastivetextembedding,whichwillbeconcatenatedasasequencewithafixed
lengthof2inself-attentionlayers. Thenumberofnewlyaddedself-attentionlayersis4andtheτ islearnableandinitialized
with0.07. Wesetthebatchsizeas2048andtrainedourmodelsforamaximumof50,000stepswithAdafactor(Shazeer
&Stern,2018)andanearlystoppingmechanism. Thelearningratesaresetdifferentlyfornewly-introducedparameters
andre-usedCLIPorCoCaparameters,at2e-5and2e-6,respectively. Wetrainourbaseandlargemodelson64and128
TPUs,respectively. Thetrainingprocesslastssixhoursforbothmodelsandthebestcheckpointsareselectedbasedonthe
performanceonthevalidationsetofCIRRandCIRCO.
B.Baselines
Weconsidervariousbaselinesanddetailthemasfollows: (1)PALARVA(Cohenetal.,2022)(2)Pic2Word(Saitoetal.,
2023),(3)SEARLE(Baldratietal.,2023),(4)ContextI2W(Tangetal.,2024),and(5)LinCIR(Guetal.,2024)trainan
additionalmappingnetworktoencodethegivenreferenceimageasapseudowordtoken. Then,itcanbecombinedwith
theactualquerytextfortext-to-imageretrieval. Thesemethodsrelyonimage-captionpairsformappingnetworktraining
Further,LinCIRintroducestext-onlydataforbettermappingcapability. (6)CIReVL(Karthiketal.,2024)isatraining-free
methodbyusingBLIP-2withFLANT5-XXL(Lietal.,2023)forqueryimagecaptiongeneration,ChatGPT(OpenAI,2022)
fortargetimagecaptiongeneration,andCLIP(Radfordetal.,2021;Chertietal.,2023)forthefinaltext-to-imageretrieval.
Suchacomplexretrievalpipelinemaylimittheirinferencespeedandpotentialpracticalnessinreal-worldscenarios.Inspired
bydiffusionmodels, (7)CompoDiff(Guetal.,2023)regardsquerytextasaconditiontoguidetheimageembedding
generationandtrainthemodelwith18Msynthesizeddata. (8)PLI(Chen&Lai,2023)corruptstheimageinimage-caption
dataandregardstheoriginalimageasatargettosimulatetheCIRtaskduringthepre-trainingstage.
13Self-SupervisedImageRetrievalwithOpen-EndedInstructions
Table10. DetailedpromptforquerygenerationusingPaLM2(Aniletal.,2023).
BasedontheprovidedALT TEXT,TEXT LABEL,andCAPTIONoftwodifferentimages,createaninterestingtext
querywhichcanbeusedwiththesourceimagetoretrievethetargetimage.
NotethattheTEXT LABELandCAPTIONaregeneratedbymodelssotheymaynotbe100%correct,especially
whenit’saboutveryspecificentities(e.g.,aspecificcartypeinsomeyear),soselectivelyusethemostlikelycorrect
informationandgeneratethequery.
Instruction
Thisqueryshouldinclude:
1)onegeneralandunspecificsimilarity(samebrand,similartoy,similarbuilding,etc).
2)alldifferencesthatonlythetargetimagehas.
Rememberthequeryshouldbeconcise,short,andnotbeabletodirectlyretrievethetargetimage.Theretrievalhas
tobedonebycombiningthesourceimageandtextquery.
Bothimagesarefromthewebsite[HOME-1-of-1AutomotiveArtworks]
SourceImage: ALT TEXT[customporschecaymangt4illustrationframed]. TEXT LABEL[Licenceplate].
CAPTION[adrawingofaporschegt4rscoupeinaframe].
Target Image: ALT TEXT [custom illustration of a 1972 porsche 911 blue]. TEXT LABEL [Licence plate,
Turquoise].CAPTION[aframedprintofablueporsche911sallcoupe].
Think: BothimagesarecustomillustrationsofPorschecarsasdescribedinthealt text. Thesourceimageisa
PorscheCaymanGT4whilethetargetimageisa1972Porsche911inblue.Therefore,thequeryshouldfocusonthe
typeofimage(customillustrationofaPorschecar),butspecifythedifferentmodelandyear(1972Porsche911)and
color(blue).
Query:[Porsche911inblueshowninthesameillustrativeway.]
Both Images are from the website [ Rapunzel Worksheet — Printable Worksheets and Activities for Teach-
Demonstrations
ers,Parents,TutorsandHomeschoolFamilies]
SourceImage:ALT TEXT[tangledrapunzelcolorpagesprintable].TEXT LABEL[Coloringbook].CAPTION
[rapunzelinaboatwithlanternsfloatingintheaircoloringpage].
TargetImage: ALT TEXT[coolrapunzelandflynnflowerhaircoloringpage]. TEXT LABEL[Coloringbook,
Floraldesign].CAPTION[ablackandwhitedrawingofrapunzeltangledwithlonghairandflowersinherdress].
Think: BothimagesarecoloringpagesfeaturingRapunzelasdescribedinthealt text. Thesourceimageshows
Rapunzel in a boat with lanterns, while the target image shows Rapunzel with Flynn and flowers in her hair.
Therefore,thequeryshouldfocusonthetypeofimage(Rapunzelcoloringpage),butspecifythedifferentscene
(RapunzelwithFlynnandflowersinherhair,notinaboat,nolanterns).
Query:[SamecoloringpageaboutRapunzelbutnoboatorlantern,withmoreclearflowersinthecharacter’shair]
...(Threemorefew-shotdemonstrations)
Table11.DetailedperformanceofCoCa-basedMagicLens-BtrainedwithIP2Pdataandourconstructeddata,onthesame1Mscale.
FIQ CIRR CIRR CIRCO DTIN GeneCIS
R@10 R@1 R @1 mAP@5 R@10 R@1
s
MagicLens+IP2P(1M) 20.3 12.5 54.9 13.6 30.2 14.5
MagicLens+Ours(1M) 33.5 29.6 66.9 29.7 43.7 15.8
C.FullResults
C.1.ResultsonFiveMultimodality-to-ImageBenchmarks
Table12,13,and14showthefullresultsonthreeCIRbenchmarks(Wuetal.,2021;Liuetal.,2021;Baldratietal.,2023).
WereporttheperformancesofvariousmodelsonDTandGeneCISonTable15andTable16,respectively. Someprior
methodsmayuselargerencoders(Guetal.,2023;2024)anddeveloparetrievalpipeline(Karthiketal.,2024)including
LLMs(OpenAI,2022)andLMMs(Lietal.,2023)forperformancegains. Despitethis,theirresultsarestillworsethanthat
ofMagicLens,supportingtheparameterefficiencyclaimedinFigure7.
C.2.DataTrainingComparison
Table11comparesCoCa-basedMagicLens-BtrainedonIP2Pdataandonourdataindetail,bothata1Mscale.
C.3.Text-to-ImageRetrievalwithCLIP-basedMagicLens
We list the text-to-image retrieval results in Table 17 with the original CLIP and updated backbone CLIP encoders
of MagicLens. The text-to-image retrieval performance is significantly boosted on both base and large models where
image-to-textretrievalabilitymarginallydrops. ThisalignswiththeconclusionwedrawonCoCain§4.4.
14similar {turtle} with {a sword}
same type of soft toy but of a single character instead
of a set of 5, and of the character Donatello instead of
a set of 5.
same plush toy but of Donatello, not a set.
similar {shirt} with {the skull version}.
same {text "Stay Well Lubricated Sleep With A
Mechanic"} but {on a shirt} instead of {a mug}.
Self-SupervisedImageRetrievalwithOpen-EndedInstructions
T-shirt with the same text as the given image.
same {text "Stay Well Lubricated Sleep With A
Mechanic"} but {on a shirt} instead of {a mug}.
T-shirt with the same text as the given image.
Figure10. Examplesoftemplate-basedandtemplate-freeinstructionsforthesameimagepair.
C.4.ExamplesofTemplate-basedInstructions
WeprovideaconcreteexampleofdifferentinstructionsonthesameimagepairinFigure10.
D.MoreQualitativeStudy
Wepresentdetailedtop-5retrievalresultsofCoCa-basedMagicLens-Landthecode-availableSOTALinCIR(Guetal.,
2024)inFigure11. 1)Forthebagquery,MagicLenscanretrievebags(thethirdandfourthimages)fromthesamebrand,
eventhoughtheydon’thavesharedvisualclues(brandlogo)withthequeryimage. 2)Giventhehouse and gavel
query,ourmodelsuccessfullyfindsaninterestingreal-worldsceneandtheperfectexampleinthetop-2results,butLinCIR
failstosatisfythequery. Thismaystemfromthelimitedrepresentationabilitiesofasinglepseudotokenforanimagewith
multipleobjects. 3)ThesuccessonthegazeboexampleshowsthatMagicLenscanunderstandsimplenumericalrelations.
Table12. FullresultsontheFIQbenchmark(Wuetal.,2021).†PLIdoesnotreleasecodesoweestimate.
#Total Backbone Dress Shirt Toptee Overall
Method
Params Network
R@10 R@50 R@10 R@50 R@10 R@50 R@10 R@50
PALAVRA(Cohenetal.,2022) 176M CLIP-B 17.3 35.9 21.5 37.1 20.6 38.8 19.8 37.3
SEARLE(Baldratietal.,2023) 165M CLIP-B 18.5 39.5 24.4 41.6 25.7 46.5 22.9 42.5
PLI(Chen&Lai,2023) 224M BLIP-B 28.6 50.8 38.1 57.8 40.9 62.7 35.9 57.1
MagicLens-B 166M CLIP-B 21.5 41.3 27.3 48.8 30.2 52.3 26.3 47.4
MagicLens-B 267M CoCa-B 29.0 48.9 36.5 55.5 40.2 61.9 35.2 55.4
Pic2Word(Saitoetal.,2023) 429M CLIP-L 20.0 40.2 26.2 43.6 27.9 47.4 24.7 43.7
SEARLE(Baldratietal.,2023) 442M CLIP-L 20.5 43.1 26.9 45.6 29.3 50.0 25.6 46.2
Context-I2W(Tangetal.,2024) 496M CLIP-L 23.1 45.3 29.7 48.6 30.6 52.9 27.8 48.9
CompoDiff(Guetal.,2023) 568M CLIP-L 32.2 46.3 37.7 49.1 38.1 50.6 36.0 48.6
PLI(Chen&Lai,2023) 428M† CLIP-L 28.1 51.1 38.6 58.5 39.4 62.7 35.4 57.4
LinCIR(Guetal.,2024) 442M CLIP-L 20.9 42.4 29.1 46.8 28.8 50.2 26.3 46.5
MagicLens-L 465M CLIP-L 25.5 46.1 32.7 53.8 34.0 57.7 30.7 52.5
MagicLens-L 613M CoCa-L 32.3 52.7 40.5 59.2 41.4 63.0 38.0 58.2
Pic2Word(Saitoetal.,2023) 987M CLIP-H 28.0 51.5 36.9 56.0 40.2 62.0 35.0 56.5
SEARLE(Baldratietal.,2023) 1.0B CLIP-H 28.5 51.1 36.5 55.5 38.8 60.9 34.6 55.8
LinCIR(Guetal.,2024) 1.0B CLIP-H 29.8 52.1 36.9 57.8 42.1 62.5 36.3 57.5
Pic2Word(Saitoetal.,2023) 2.5B CLIP-G 25.4 47.7 33.2 50.4 35.2 57.6 31.3 51.9
SEARLE(Baldratietal.,2023) 2.6B CLIP-G 28.2 50.3 36.5 55.4 39.8 61.5 34.8 55.7
CompoDiff(Guetal.,2023) 2.9B CLIP-G 37.8 49.1 41.3 55.2 44.3 56.4 39.0 51.7
LinCIR(Guetal.,2024) 2.6B CLIP-G 38.1 60.9 46.8 65.1 50.5 71.1 45.1 65.7
15Self-SupervisedImageRetrievalwithOpen-EndedInstructions
Table13.FullresultsontheCIRRbenchmark(Liuetal.,2021).CLIP-HandCLIP-GareOpenCLIP(Chertietal.,2023)checkpoints.
⋆CIReVLusesmultiplemodelcomponents,weomitChatGPTandreport#parametersofothercomponents(e.g.,BLIP2-FLANT5-XXL
+CLIP-G).†PLIdoesnotreleasecodesoweestimate.
#Total Backbone IndexSet Subset
Method
Params Network
R@1 R@5 R@10 R@50 R@1 R@2 R@3
PALAVRA(Cohenetal.,2022) 176M CLIP-B 16.6 43.5 58.5 84.0 41.6 65.3 80.9
SEARLE(Baldratietal.,2023) 165M CLIP-B 24.0 53.4 66.8 89.8 54.9 76.6 88.2
CIReVL(Karthiketal.,2024) 12.3B⋆ CLIP-B 23.9 52.5 66.0 87.0 60.2 80.1 90.2
PLI(Chen&Lai,2023) 224M† BLIP-B 27.2 58.9 71.4 91.3 55.1 77.4 89.1
MagicLens-B 166M CLIP-B 27.0 58.0 70.9 91.1 66.7 83.9 92.4
MagicLens-B 267M CoCa-B 31.6 64.0 76.9 93.8 69.3 86.0 94.0
Pic2Word(Saitoetal.,2023) 429M CLIP-L 23.9 51.7 65.3 87.8 - - -
SEARLE(Baldratietal.,2023) 442M CLIP-L 24.2 52.5 66.3 88.8 53.8 75.0 88.2
Context-I2W(Tangetal.,2024) 496M CLIP-L 25.6 55.1 68.5 89.8 - - -
CompoDiff(Guetal.,2023) 568M CLIP-L 18.2 53.1 70.8 90.3 57.4 77.1 87.9
CIReVL(Karthiketal.,2024) 12.5B⋆ CLIP-L 24.6 52.3 64.9 86.3 59.5 79.9 89.7
PLI(Chen&Lai,2023) 428M† CLIP-L 25.5 54.6 67.6 88.7 55.6 77.5 89.5
LinCIR(Guetal.,2024) 442M CLIP-L 25.0 53.3 66.7 - 57.1 77.4 88.9
MagicLens-L 465M CLIP-L 30.1 61.7 74.4 92.6 68.1 84.8 93.2
MagicLens-L 613M CoCa-L 33.3 67.0 77.9 94.4 70.9 87.3 94.5
Pic2Word(Saitoetal.,2023) 987M CLIP-H 32.9 63.1 73.9 - 62.2 81.4 91.2
SEARLE(Baldratietal.,2023) 1.0B CLIP-H 34.0 64.0 75.3 - 64.6 83.2 92.8
LinCIR(Guetal.,2024) 1.0B CLIP-H 33.8 63.5 73.4 - 62.4 81.5 92.1
Pic2Word(Saitoetal.,2023) 2.5B CLIP-G 30.4 58.1 69.2 - 68.9 85.5 93.0
SEARLE(Baldratietal.,2023) 2.6B CLIP-G 34.8 64.1 75.1 - 68.7 84.7 93.2
CompoDiff(Guetal.,2023) 2.9B CLIP-G 26.7 55.1 74.5 92.0 64.5 82.4 91.8
CIReVL(Karthiketal.,2024) 14.6B⋆ CLIP-G 34.7 64.3 75.1 91.7 68.0 84.9 93.2
LinCIR(Guetal.,2024) 2.6B CLIP-G 35.3 64.7 76.1 - 63.4 82.2 92.0
Table14.Full results on the CIRCO benchmark (Baldrati et al., 2023). CLIP-H and CLIP-G are OpenCLIP (Cherti et al., 2023)
checkpoints.⋆CIReVLusesmultiplemodelcomponents,weomitChatGPTandreport#parametersofothercomponents(e.g.,BLIP2-
FLANT5-XXL+CLIP-G).†PLIdoesnotreleasecodesoweestimate.
#Total Backbone
Method mAP@5 mAP@10 mAP@25 mAP@50
Params Network
PALAVRA(Cohenetal.,2022) 176M CLIP-B 4.6 5.3 6.3 6.8
SEARLE(Baldratietal.,2023) 165M CLIP-B 9.4 9.9 11.1 11.8
CIReVL(Karthiketal.,2024) 12.3B⋆ CLIP-B 14.9 15.4 17.0 17.8
PLI(Chen&Lai,2023) 224M† BLIP-B 7.1 8.0 9.2 9.7
MagicLens-B 166M CLIP-B 23.1 23.8 25.8 26.7
MagicLens-B 267M CoCa-B 30.8 32.0 34.5 35.6
Pic2Word(Saitoetal.,2023) 429M CLIP-L 8.7 9.5 10.6 11.3
SEARLE(Baldratietal.,2023) 442M CLIP-L 11.7 12.7 14.3 15.1
CompoDiff(Guetal.,2023) 568M CLIP-L 12.6 13.4 15.8 16.4
CIReVL(Karthiketal.,2024) 12.5B⋆ CLIP-L 18.6 19.0 20.9 21.8
PLI(Chen&Lai,2023) 428M† CLIP-L 10.4 11.6 13.0 13.7
LinCIR(Guetal.,2024) 442M CLIP-L 12.6 13.6 15.0 15.9
MagicLens-L 465M CLIP-L 29.6 30.8 33.4 34.4
MagicLens-L 613M CoCa-L 34.1 35.4 38.1 39.2
Pic2Word(Saitoetal.,2023) 987M CLIP-H 11.7 12.3 13.7 14.4
SEARLE(Baldratietal.,2023) 1.0B CLIP-H 16.1 16.9 18.8 19.7
LinCIR(Guetal.,2024) 1.0B CLIP-H 17.6 18.5 20.5 21.4
Pic2Word(Saitoetal.,2023) 2.5B CLIP-G 5.5 5.6 6.7 7.1
SEARLE(Baldratietal.,2023) 2.6B CLIP-G 13.2 13.9 15.3 16.0
CompoDiff(Guetal.,2023) 2.9B CLIP-G 15.3 17.7 19.4 -
CIReVL(Karthiketal.,2024) 14.6B⋆ CLIP-G 26.8 27.6 30.0 31.0
LinCIR(Guetal.,2024) 2.6B CLIP-G 19.7 21.0 23.1 24.2
16Self-SupervisedImageRetrievalwithOpen-EndedInstructions
Table15.FullresultsontheDTINbenchmark(Saitoetal.,2023).CLIP-GisaOpenCLIP(Chertietal.,2023)checkpoint.⋆CIReVLuses
multiplemodelcomponents,weomitChatGPTandreport#parametersofothercomponents(e.g.,BLIP2-FLANT5-XXL+CLIP-G).
#Total Backbone Cartoon Origami Toy Sculpture Overall
Method
Params Network
R@10 R@50 R@10 R@50 R@10 R@50 R@10 R@50 R@10 R@50
Image-only(Saitoetal.,2023) 304M CLIP-L 0.3 4.5 0.2 1.8 0.6 5.7 0.3 4.0 0.4 4.0
Text-only(Saitoetal.,2023) 124M CLIP-L 0.2 1.1 0.8 3.7 0.8 2.4 0.4 2.0 0.5 2.3
Image+Text(Saitoetal.,2023) 428M CLIP-L 2.2 13.3 2.0 10.3 1.2 9.7 1.6 11.6 1.7 11.2
Pic2Word(Saitoetal.,2023) 429M CLIP-L 8.0 21.9 13.5 25.6 8.7 21.6 10.0 23.8 10.1 23.2
Context-I2W(Tangetal.,2024) 496M CLIP-L 10.2 26.1 17.5 28.7 11.6 27.4 12.1 28.2 12.9 27.6
CIReVL(Karthiketal.,2024) 14.6B⋆ CLIP-G 19.2 42.8 30.2 41.3 22.2 43.1 23.4 45.0 23.8 43.0
MagicLens-B 166M CLIP-B 49.4 67.0 13.8 26.3 25.8 43.4 24.3 41.3 28.3 44.5
MagicLens-B 267M CoCa-B 65.8 73.3 29.3 38.6 46.7 57.7 45.3 57.1 46.8 56.7
MagicLens-L 465M CLIP-L 62.6 72.2 21.5 33.4 43.8 58.4 38.0 54.2 41.5 54.5
MagicLens-L 613M CoCa-L 60.1 69.6 36.0 44.7 45.2 56.9 51.4 59.6 48.2 57.7
Table16.FullresultsontheGeneCISbenchmark(Vazeetal.,2023).⋆CIReVLusesmultiplemodelcomponents,weomitChatGPTand
report#parametersofothercomponents(e.g.,BLIP2-FLANT5-XXL+CLIP-G).
Backbone FocusAttribute ChangeAttribute FocusObject ChangeObject Avg
Method #Params
Network
R@1 R@2 R@3 R@1 R@2 R@3 R@1 R@2 R@3 R@1 R@2 R@3 R@1
Pic2Word(2023) 429M CLIP-L 15.7 28.2 38.7 13.9 24.7 33.1 8.4 18.0 25.8 6.7 15.1 24.0 11.2
SEARLE(2023) 442M CLIP-L 17.0 29.7 40.7 16.4 25.3 34.1 8.0 16.9 25.6 7.9 16.8 24.8 12.3
CompoDiff(2023) 568M CLIP-L 13.5 24.3 36.1 19.2 28.6 37.2 8.1 16.4 25.1 18.7 31.7 40.6 14.9
LinCIR(2024) 442M CLIP-L 16.9 30.0 41.5 16.2 28.0 36.8 8.3 17.4 26.2 7.4 15.7 25.0 12.2
Pic2Word(2023) 987M CLIP-H 18.6 30.7 42.1 13.2 23.9 33.1 9.2 17.6 27.1 6.6 16.5 25.4 11.9
SEARLE(2023) 1.0B CLIP-H 18.8 31.5 42.3 15.5 26.9 35.9 10.6 18.7 26.5 8.5 17.9 26.2 13.3
LinCIR(2024) 1.0B CLIP-H 19.6 31.5 41.6 16.6 27.6 37.5 9.8 18.8 27.9 9.0 17.6 25.7 13.8
Pic2Word(2023) 2.5B CLIP-G 12.5 23.4 33.7 11.7 21.9 30.9 9.9 19.3 27.4 8.6 18.2 26.1 10.7
SEARLE(2023) 2.6B CLIP-G 16.3 29.4 40.7 16.2 27.3 35.5 10.8 18.2 27.9 8.3 15.6 25.8 12.9
CompoDiff(2023) 2.9B CLIP-G 14.3 26.7 38.4 19.7 28.8 37.4 9.2 19.1 25.8 18.7 31.7 40.2 15.5
CIReVL(2024) 14.6B⋆ CLIP-G 17.9 29.4 40.4 14.8 25.8 35.8 14.6 24.3 33.3 16.1 27.8 37.6 15.9
LinCIR(2024) 2.6B CLIP-G 19.1 33.0 42.3 17.6 30.2 38.1 10.1 19.1 28.1 7.9 16.3 25.7 13.7
MagicLens-B 166M CLIP-B 15.5 28.4 39.1 12.3 23.0 32.1 14.4 26.2 35.5 17.7 28.4 39.2 15.0
MagicLens-B 267M CoCa-B 16.2 27.8 38.6 16.2 27.2 36.6 17.1 27.7 38.2 20.2 32.2 42.9 17.4
MagicLens-L 465M CLIP-L 16.1 28.2 39.0 15.6 27.5 36.3 16.3 26.2 35.5 17.1 29.5 39.7 16.3
MagicLens-L 613M CoCa-L 16.6 28.7 39.3 16.0 27.5 36.5 15.7 27.6 37.3 18.7 31.7 40.2 16.7
Table17. Zero-shotimage-textretrievalresults.Resultsaremarkedinboldiftheyarebetterthaninitializedcheckpoints.⋆CLIPand
CoCawereproducedandusedforMagicLens.†CLIPandCoCareportedintheoriginalpaper.
Flickr30K(1Ktestset) MSCOCO(5Ktestset)
Model Image→Text Text→Image Image→Text Text→Image
R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10
CLIP-B⋆ 81.7 97.1 98.5 61.6 85.6 91.2 51.9 76.3 83.9 32.1 56.7 67.6
MagicLens-B 78.9 94.9 97.5 67.8 88.8 93.4 49.5 74.5 82.5 40.1 65.4 75.1
CoCa-B† 89.8 98.8 99.8 76.8 93.7 96.8 63.8 84.7 90.7 47.5 72.4 80.9
CoCa-B⋆ 88.6 98.5 99.4 74.5 93.4 96.4 63.4 84.2 90.4 46.4 71.5 80.1
MagicLens-B 87.9 97.7 99.5 76.2 93.7 96.5 64.8 85.5 91.2 48.9 73.9 82.5
CLIP-L† 88.0 98.7 99.4 68.7 90.6 95.2 58.4 81.5 88.1 37.8 64.2 72.2
CLIP-L⋆ 84.6 97.9 99.3 65.4 87.6 92.9 56.2 79.3 87.3 34.6 59.4 69.8
MagicLens-L 84.6 96.2 98.8 72.5 91.5 95.2 55.9 78.7 86.3 44.3 69.4 78.3
CoCa-L† 91.4 99.2 99.9 79.0 95.1 97.4 65.4 85.6 91.4 50.1 73.8 81.8
CoCa-L⋆ 92.1 98.8 99.9 78.4 94.2 96.9 65.1 85.5 91.3 49.3 73.2 81.5
MagicLens-L 89.6 98.7 99.4 79.7 95.0 97.4 67.7 87.6 92.7 53.1 77.4 84.9
17Self-SupervisedImageRetrievalwithOpen-EndedInstructions
Same car model as the given Bucket bag from the Baking muffins, but
image, but a 2013 model, same brand, in gray, show the process of
blue in color, and parked in without a person adding the pumpkin
front of trees. holding the bag. pie spice.
MagicLens LinCIR MagicLens LinCIR MagicLens LinCIR
Top1
Top5
Image of a model house
Show me a bamboo
and wooden gavel like this Dinosaur eating leaves from a
gazebo like this but with
one, but with the gavel tree in the forest like this.
two gazebos in a garden
sitting next to the house.
MagicLens LinCIR MagicLens LinCIR MagicLens LinCIR
Top1
Top5
Figure11. Top-5retrievedimagesofCoCa-basedMagicLens-LandLinCIRontheholdoutindexsetwith1.4Mimagesforqueriesshown
inFigure8andmore.Queriesarewithabluebackgroundandonlythemostcorrectretrievedimagesaremarkedwithgreenoutlines.
18