JOURNALOFLATEXCLASSFILES,VOL.X,NO.X,2024 1
Inferring Latent Temporal Sparse Coordination
Graph for Multi-Agent Reinforcement Learning
Wei Duan, Student Member, IEEE, Jie Lu, Fellow, IEEE, and Junyu Xuan, Senior Member, IEEE
Abstract—Effective agent coordination is crucial in coopera-
tive Multi-Agent Reinforcement Learning (MARL). While agent
cooperation can be represented by graph structures, prevailing
graphlearningmethodsinMARLarelimited.Theyrelysolelyon
one-step observations, neglecting crucial historical experiences,
leading to deficient graphs that foster redundant or detri-
(a) (b) (c)
mental information exchanges. Additionally, high computational
demands for action-pair calculations in dense graphs impede Fig. 1: The current methods to infer latent graphs in MARL
scalability. To address these challenges, we propose inferring can be categorized into three types: (a) fully connected un-
a Latent Temporal Sparse Coordination Graph (LTS-CG) for
weightedgraphs,(b)fullyconnectedweightedgraphs,and(c)
MARL. The LTS-CG leverages agents’ historical observations
sparseweightedgraphs.Thesemethodsrelysolelyonone-step
to calculate an agent-pair probability matrix, where a sparse
graphissampledfromandusedforknowledgeexchangebetween observations, leading to deficient graphs that foster redundant
agents,therebysimultaneouslycapturingagentdependenciesand or detrimental information exchanges and suffer from high
relationuncertainty.Thecomputationalcomplexityofthisproce- computational complexity for action-pair calculations.
dureisonlyrelatedtothenumberofagents.Thisgraphlearning
process is further augmented by two innovative characteristics:
Predict-Future, which enables agents to foresee upcoming ob-
servations, and Infer-Present, ensuring a thorough grasp of the the real world, agents should not only consider their own
environmental context from limited data. These features allow observationsbutalsotakeintoaccountthesituationsofothers
LTS-CG to construct temporal graphs from historical and real- when taking action [7]. Effective cooperation among agents
time information, promoting knowledge exchange during policy
emerges as a pivotal factor in achieving specific objectives.
learning and effective collaboration. Graph learning and agent
This cooperation can be assumed to have some latent graph
training occur simultaneously in an end-to-end manner. Our
demonstratedresultsontheStarCraftIIbenchmarkunderscore structures [8]. Since the graph is not explicitly given, the
LTS-CG’s superior performance. inference of meaningful dynamic graph topology has been a
persistent challenge.
IndexTerms—Multi-agentreinforcementlearning,multi-agent
cooperation, coordination graph, graph structure learning. Thecurrentmethodstoaddressthisproblemcanbebroadly
categorized into three types, illustrated in Fig.1. The first
I. INTRODUCTION type involves employing fully connected unweighted graphs,
such as PIC [9] and DCG [10]. The second type incorporates
EffectiveagentcoordinationiscrucialincooperativeMulti-
fully connected weighted graphs, such as GraphMIX [11] and
Agent Reinforcement Learning (MARL), which offers an
DICG[12].Thethirdtypeutilizesweightedsparsegraphs,such
instrumental approach to control multiple intelligent agents
as SOP-CG [13] and CASEC [14]. However, these methods
to fulfil various tasks, including coordinating traffic lights
exhibit the following limitations: (1) They primarily focus
throughoutacity[1],orchestratingmulti-robotformations[2],
on one-step observations and fail to consider the value of
and optimizing the behaviour of unmanned aerial vehicles [3]
historical trajectory data, which more accurately represents
One efficient approach to training multiple agents in dynamic
agents’ behaviours and is more meaningful to help to learn
environments involves decomposing the global value function
policies [15]. This overreliance on one-step data can lead
into manageable segments for each agent. This methodology
to suboptimal graph learning, producing graphs that may
is exemplified by techniques such as VDN employing the
encourage redundant or even counterproductive information
sum of independent agent value functions [4], QMIX utiliz-
exchanges, thereby impeding effective policy learning. (2)
ing a monotonic mixture instead of a simple sum [5], and
The computation-intensive nature of action-pair calculations
QTRAN using a hyper-edge that connects all agents without
in coordination graphs (CG) [8] poses significant scalability
factorization [6]. Within this framework, each agent selects
challenges,especiallyinfully-connectedsettings.Forinstance,
actions to maximize its own value function and contributes to
in a system with N agents, each having A actions, the
maximising the total reward.
computationalcomplexityofthesemethodsisO(A2N2).This
While these methods balance computational efficiency with
complexity becomes increasingly problematic as the number
effective agent interaction and complex decision-making, in
of agents and actions increases.
TheauthorsarewiththeAustralianArtificialIntelligenceInstitute(AAII), In this paper, we address these limitations by proposing a
FacultyofEngineeringandInformationTechnology,UniversityofTechnology
novel approach called Latent Temporal Sparse Coordination
Sydney,Ultimo,NSW2007,Australia.(Email:wei.duan@student.uts.edu.au,
jie.lu@uts.edu.au,junyu.xuan@uts.edu.au) Graph(LTS-CG)forMARL.LTS-CGefficientlyinfersgraphs
4202
raM
82
]GL.sc[
1v35291.3042:viXraJOURNALOFLATEXCLASSFILES,VOL.X,NO.X,2024 2
using agents’ observation trajectories to generate an agent- II. PRELIMINARIES
pair probability matrix, where the probability is absorbed
We focus on cooperative multi-agent tasks modelled as
and trained together with Graph Convolutional Networks
a Decentralized Partially Observable Markov Decision
(GNN) parameters. The computational complexity of this
Process (Dec-POMDP) [16] consisting of a tuple
procedure scales quadratically with the number of agents ⟨I,S,{Ai}n ,P,{Oi}n ,{σi}n ,R,γ⟩, where I is the
i=1 i=1 i=1
N, which renders our approach scalable and suitable for finitesetofnagents,s∈S isthetruestateoftheenvironment.
handling complex MARL scenarios. Subsequently, a sparse At each time step, each agent i observes the state partially by
graph is sampled from this matrix, which simultaneously drawing observation oi ∈ Oi and selects an action ai ∈ Ai
t t
captures agent dependencies underlying the trajectories and according to its own policy σi. Individual actions form a
models the relation-uncertainty between agents. Driven by joint action a = (a ,...,a ), which leads to the next state s′
1 n
the goal of creating meaningful graphs, we enhance agents’ according to the transition function P(s′|s,a) and a reward
understanding of their peers and the environment by em- R(s,a) shared by all agents. Each agent has local action-
bedding two essential characteristics into the graph: Predict- observation history τ = (o ,a ,...,o ,a ,o ) ∈
i,t i,0 i,0 i,t−1 i,t−1 i,t
Future and Infer-Present. Predict-Future empowers agents to (Oi × Ai)t × Oi. This paper considers episodic tasks
predict upcoming observations using current observations and yielding episodes (s ,{oi}n ,a ,r ,...,s ,{oi }n )
0 0 i=1 0 0 T T i=1
the sampled graph, providing valuable insights for immediate of varying finite length T. Agents learn to
decision-making. Infer-Present aids each partially observed collectively maximize the global return Q (s,a) =
tot
agent in comprehending the full environmental context and E (cid:104) (cid:80)T γtR(s ,a )|s =s,a =a(cid:105) , where
deducing the current state with the graph’s information. LTS- s0:T,a0:T t=0 t t 0 0
γ ∈[0,1) is the discount factor.
CGleveragesbothhistoricalandreal-timedataforgraphtrain-
Learning the underlying relation of agents can be seen as
ing, considering local and global perspectives. The temporal
the inference of a meaningful dynamic graph topology. This
structure of the learned graph encapsulates past experiences,
graph is denoted as G = {V,E} where V = I is node/agent
withedgeweightsreflectingongoingobservations.Thisfacili-
set and E is the edge/relation set between agents.
tatesknowledgeexchangeduringpolicylearningandsupports
historical and present insights for effective cooperation. The
computational complexity of our method is O(TN2), where III. RELATEDWORK
T represents the observation length used for graph learning, A. Graph-based MARL
making it more efficient than action-pair-based methods. MARL faces the challenge of dealing with the exponen-
The main insight behind designing our method is to enable tially growing size of joint action spaces among agents [17].
simultaneous graph inference and multi-agent policy learning, The paradigm of CTDE [18, 19] strikes a balance between
facilitating efficient end-to-end training using standard policy computational efficiency and multi-agent interaction but falls
optimization methods. We evaluate LTS-CG on the StarCraft short in handling dependencies between agents. Graph Neural
II benchmark, demonstrating its superior performance. The Networks (GNNs) have demonstrated remarkable capability
ablation results empirically proved that using trajectories for in modelling relational dependencies [20, 21], making graphs
learning the coordination graph is more effective than rely- a compelling tool for graph-based MARL, which can be
ing on one-step observations, and having the Predict-Future generally divided into two types. One type involves using
and Infer-Present characteristics improves the performance of graphs as coordination graphs during policy training, such as
LTS-CG. The contributions of this paper are summarized as DCG [10], SOP-CG [13] and CASEC [14]. In this approach,
follows: the total action-value function is defined as:
• We pioneer the treatment of agent trajectories as data Q (s ,a)= 1 (cid:88) qi(cid:0) ai|s (cid:1) + 1 (cid:88) qij(cid:0) ai,aj|s (cid:1) ,
streams in MARL with LTS-CG. Our method leverages tot t |V| t |E| t
i∈V {i,j}∈E
these trajectories to infer latent temporal sparse graphs, (1)
facilitating knowledge exchange between agents. wherethefirsttermcalculatestheQ-valueofeachaction(also
• By sampling sparse graphs from trajectories-generated knownasutilityfunction),andthesecondtermevaluatesevery
agent probability matrices, LTS-CG captures agent de- action-pair of agents (also known as payoff function). This
pendencies and models the uncertainty of relations be- methodexplicitlyassessesthequalityofjointactionsbetween
tween agents simultaneously, with computational com- different agents. The other type uses graphs to facilitate
plexity only related to the number of agents. information exchange among agents, such as DICG [22] and
• LTS-CG further infers the graph from both local and G2ANet [23]. It is formulated as:
global standpoints to encode Predict-Future and Infer-
n
(cid:88)
Present characteristics. This meaningful graph enables m =AGG (f(o ,a )), Q = Q (o ,a ,m ) (2)
i j∈Ni j j tot i i i i
agents to gain historical and present perspectives to
i=1
achieve effective cooperation.
where N means the neighbours of agent i. f(·) transfers the
i
The rest of the paper is organized as follows. In Sec. II, original observation and action into embedding, and AGG(·)
we give a definition of our task, followed the related work in aggregatestheembeddingbasedongraphtopologytogenerate
Sec. III. In Sec. IV, we described our approach. We report the message m . This message provides additional knowledge
i
experimental studies in Sec. V and conclude in Sec. VI. that aids agents in decision-making and represents an implicitJOURNALOFLATEXCLASSFILES,VOL.X,NO.X,2024 3
Predict-future
Trajectory Sample Recurrent Feature Agent-pair Graph Convolution
Predictor
Extractor
Agent-pair
probability
Learned graph
Observation Trajectory
Infer-present
Agent 1
Attention
Readout
Graph
Attention Convolution Pooling Graph
Convolution
Agent N
Cooperative MARL Inter-Agent Sparse Graph Learning
Fig.2:TheframeworkofLTS-CG.LTS-CGsamplesasparsegraphfromanagent-pairprobabilitymatrixgeneratedbyagents’
observation trajectories. We further learn the graph to encode two essential characteristics: Predict-Future enables agents to
predict future steps effectively, and Infer-Present empowers every partially observed agent to deduce the current state using
graph information. The learned temporal graph structure captures past experiences, while current observations determine edge
weights. This allows agents to exchange knowledge during policy learning and gain historical and present perspectives for
effective cooperation. Graph learning and agent training occur simultaneously in an end-to-end fashion.
coordination between agents. Although these methods do the former, Yu et al. [27] explored pairwise similarities or
not strictly calculate the payoff-utility function based on the connectionsamongthemtoenhanceforecastingaccuracy.Wu
coordinationgraph,theybuilduponthesameideaofreasoning et al. [28] presented a framework for modelling multivariate
about joint actions based on interactions between agents [22]. timeseriesdataandlearninggraphstructuresthatcanbeused
As the graph itself is not explicitly given, inferring graph with or without a pre-defined graph structure. Satorras et al.
topology remains a critical prerequisite for training MARL. [29]proposedanapproachthatbalancesaccuracyandcompu-
From the perspective of graph structure, existing methods for tational efficiency, allowing the flexibility to infer either fully
graphinferencecanbebroadlycategorizedintothreetypes:(a) connectedorbipartitegraphs.Regardingtrajectoryprediction,
creatingfullyconnectedunweightedgraphsbydirectlylinking Kipf et al. [30] proposed NRI, a variational autoencoder that
all nodes/agents explicitly, such as DGN [24], PIC [9] and leverages a latent-variable approach to learn a latent graph.
DCG [10] or implicitly such as MAAC [25], ROMA [26]; (b) On the other hand, LDS [31] and GTS [32] focus on learning
employing attention mechanisms to calculate fully connected probabilistic graph models by optimizing performance over
weighted graphs, such as GraphMIX [11] and DICG [12]; the graph distribution mean. To further adaptively connect
(c) designing drop-edge criteria to generate sparse weighted multiple nodes, Li et al. [33] proposed a group-aware rela-
graphs, such as random drop edges in G2ANet [21], select tional reasoning approach to infer hyperedges. In the context
sparse graph from candidate set in SOP-CG [13] and drop of MARL, the absence of labelled data poses a challenge
edges based on variance of payoff functions in CASEC [14]. for traditional trajectory prediction or multiple time series
Despite this progress, these methods exhibit the following forecasting methods. Borrowing the learning capabilities from
limitations:oneisthattheyprimarilyfocusonone-stepobser- these two directions while fully leveraging the information
vations and fail to consider the value of historical trajectory available in MARL remains an underexplored area.
data,whichmoreaccuratelyrepresentsagents’behavioursand
is more meaningful to help to learn policies [15]; another is
that the computation-intensive nature of action-pair calcula- IV. THEPROPOSEDMETHOD
tions in coordination graphs (CG) [8] poses significant scala-
The framework of LTS-CG is illustrated in Fig. 2. To
bility challenges, which becomes increasingly problematic as
efficiently infer the underlying relation from past experiences,
the number of agents and actions increases (See: V-A).
LTS-CG samples a sparse graph from the agent-pair prob-
ability matrix generated by agents’ observation trajectories.
B. Graph Structure Learning
The core of LTS-CG lies in creating a meaningful graph
Tolearnarelationalgraphbetweenagentsthattakeaseries that enhances agents’ understanding of their peers and the
ofactionswithinspecifictimesteps,twopromisingdirections environment.Thisisachievedthroughtwokeycharacteristics:
areworthconsidering:learningagraphformultipletimeseries Predict-FutureandInfer-Present,whichenableagentstoshare
forecasting and inferring a graph for trajectory prediction. For knowledge and gain both historical and present insights,
. . .
.
.
.
gnixiM krowteN . . .
.
.
.
. . .
.
. .
.
.
.JOURNALOFLATEXCLASSFILES,VOL.X,NO.X,2024 4
fostering effective cooperation. Detailed descriptions of each a Gumbel distribution with a location parameter of 0 and a
component are provided in the subsequent sections. scale parameter of 1. The sampled values from the Gumbel
distributionarethenusedtocomputethelogitsforthesigmoid
A. Latent Temporal Sparse Graphs Learning function in the Bernoulli sampling equation. Specifically, the
logits are calculated as:
1) Sparse graph construction: The accumulated observa-
tion trajectories of all agents encapsulate their experiences A =sigmoid(cid:0)(cid:0) log(θ /(1−θ ))+(cid:0) g1 −g2(cid:1)(cid:1) /s(cid:1) , (6)
ij ij ij ij ij
of interactions with the environment and their cooperation.
To efficiently capture the underlying relationships, instead of where g i1 j,g i2 j ∼ Gumbel(0,1) for all i,j, θ ij represents the
directly learning the structure of the inter-agent sparse graph probability parameter for the Bernoulli distribution, and s is
A, we utilize observation trajectories {Oi}n to generate a temperature parameter that controls the sharpness of the
i=1
the agent-pair probability matrix θ ∈ [0,1]n×n. This matrix sampling process. As the temperature s → 0,A ij = 1 with
parameterizes the element-wise Bernoulli distribution [31], probability θ ij and 0 with remaining probability. By applying
which allows us to sample a graph representing the relevant Eq. (5) and Eq. (6), we convert the observation trajectories
connections between agents. This graph learning objective is O T into an agent-pair probability θ. We subsequently sample
achieved by minimizing the loss of function to obtain the inter-agent graph A for further learning and
utilization in cooperative MARL.
min E [L(A,w,O )]. (3)
A∼Ber(θ(w)) T 2) Meaningful graph learning: Motivated by the idea that
w
the graph should enhance the agents’ understanding of other
Here, O = {Oi }n , and Oi = {oi,...,oi } denotes the
T T i=1 T 0 T agents and the environment, we further learn the graph to
observation trajectory for agent i over the time steps T.
encode the following two essential characteristics.
Each element of A is sampled from a Bernoulli distribution
Predict-Future means by exploiting the graph, we aim to
Ber(θ(w)), with ω denoting the trainable weight. In Eq. (3),
empower agents to predict future steps effectively, enabling
theadjacentprobabilityθ isabsorbedtogetherwiththeGNNs
themtomakebetterdecisionsinthecurrenttimestep.Weuse
parametersw,makingthegradientcomputationmoreefficient
the diffusion convolutional gated recurrent unit introduced in
and having better scalability [32]. In the following, we give
DiffusionConvolutionalRecurrentNeuralNetwork(DCRNN)
the details about how to infer the inter-agent sparse graph A
[37] and leverage the learned graphs A to process the obser-
and how to define the graph learning loss function L.
vations of all agents O ={oi}n as follows
To acquire knowledge about the temporal dependence of t t i=1
each agent and the relationship between agents, we establish R =sigmoid(W ⋆ [O ∥H ]+b ),
t R A t t−1 R
t th he
e
o tb emse prv oa rati lon dee px ep ne dri ee nn cc ee e ox ftr ea ac cto hr f agoe e( n· t) t zo ih be ylp eu ms pc la op yt iu nr ge UC t == st ia gn mh o( iW d(C W⋆ A ⋆[O t [∥ O(R ∥Ht⊙H ]t +−1 b]+ )b ,C) (7)
t U A t t−1 U
convolution along the time dimension, followed by a fully H =U ⊙H +(1−U )⊙C ,
t t t−1 t t
connected layer, defined as
where the graph convolution ⋆ is defined as
A
zi =f (Oi )=FC(CONV(Oi )), (4)
oe T T
K
where FC(·) is a fully connected layer and CONV(·) is the W ⋆ Y =(cid:88)(cid:16) wQ (cid:0) D−1A(cid:1)k +wQ (cid:0) D−1AT(cid:1)k(cid:17) Y, (8)
Q A k,1 O k,2 I
convolution layer performed along the temporal dimension. k=0
This convolutional layer plays a crucial role in capturing each
withD andD beingtheout-degreeandin-degreematrixof
O I
agent’s latent behaviour patterns over time, enhancing the learnedagent-pairmatrixA,respectively.Here,wQ ,wQ ,b
model’s ability to discern dynamic and temporal patterns in k,1 k,2 Q
for Q=R,U,C are model parameters and K is the diffusion
the agents’ interactions. Then the agent-pair predictor f (·)
ap degree. We adopt a 1-layer DCRNN and set K = 3 in our
utilize the temporal dependencies of every agent-pair (zi and
experiments.
zj) to calculate adjacent probability θ as follows
ij To capture both temporal and spatial dependencies between
θ ij =f ap(zi∥zj)=FC(FC(zi∥zj)), (5) agents, we feed a T-step observations {oi t+1:t+T}n i=1 into
Eq.(7), to forecast the future changes in the current T-step
where ∥ denotes concatenation along the feature dimension.
observation.Theoutputofthehiddenstateineverysteprepre-
We adopt multi-layer perceptrons (MLPs) to model and learn
sentsthepredictionofhowthecurrentobservationwillchange
f ap(·),leveragingtheuniversalapproximationtheorem[34]to in the next step, denoted as H = {∆oi }n .
t+1:t+T t+1:t+T i=1
enhance their representational capacity.
Then, the Predict-Future is achieved by calculating the fol-
To enable backpropagation through the Bernoulli sampling,
lowing loss function
we apply the Gumbel parameterization trick [35, 36]. This
t te ioc nhn ti oqu ae ppl re ov xe ir mag ae tes tt hh ee sp ar mop pe lir nti ges pro of cet sh se inGu am db ie ffl erd ei ns tt ir aib bu le-
L pre
=(cid:88)n (cid:88)T
(cid:13) (cid:13)(cid:0) oi t+t′ +∆oi t+t′(cid:1) −oi t+1+t′(cid:13) (cid:13) 2. (9)
manner, allowing gradients to flow through the stochastic i t′=1
operation. In the context of Bernoulli sampling, the Gumbel Since Eq. (9) is calculated by the observation of each agent,
trickinvolvesgeneratingtwoGumbel-distributedrandomvari- Predict-Future is a local-level characteristic of LTS-CG. Em-
ables, denoted as g1 and g2, for each element A in the ploying the message-passing mechanism of GNNs [38], it
ij ij ij
adjacency matrix. These random variables are sampled from enables agents to predict future observations based on theirJOURNALOFLATEXCLASSFILES,VOL.X,NO.X,2024 5
own current observations and the passed information from to start the game with the capability to cooperate and make
neighbouring agents. decisions.Astrainingprogresses,LTS-CGdynamicallylearns
Infer-Present is designed to assist every partially observed and updates the graph’s structure within the buffer. During
agent in gaining the ability to grasp the entire environmen- testing, the graph, retrieved from the buffer, is updated with
tal context and deduce the current state with the informa- edge weights based on current observations. This temporal
tion provided by the graph. Given the current observation sparse graph is then utilized for message calculation and
{oi}n , we first generate the observation embeddings matrix dissemination, ensuring agents have access to the latest in-
t i=1
E = [e1⊤,...,en⊤] using the ongoing observation extractor formation. Such an approach guarantees effective decision-
t t t
ei = f (oi), where f is a MLPs. Then we adopt an making and cooperation throughout the training process.
t obs t obs
attentionmechanismtodynamicallycalculatetheedgeweight Leveraging the learned graph A at every time step and fol-
between every pair of agents resulting in the attention edge- lowingtheEq.(10),thecurrentobservation{oi}n areusedto
t i=1
weight matrix, defined as computetheedgeweightsinA.Theseedgeweightsdetermine
theimportanceofcooperatingwithneighbouringagents.Con-
exp(ej⊤W ei)
µij = t a t , Cij =µij, (10) sequently, we obtain the latent temporal sparse coordination
t (cid:80) exp(ek⊤W ei) t t
k∈Ni t a t graph,encompassinghistoricalinformationwithinitsstructure
where N represents the neighbors of agent i in the graph andongoingagentrelationshipsthroughitsedgeweights.The
i
and W a is trainable parameter of attention mechanism. The exchanged knowledge m i = H tl[i,:] between agents is then
weighted-agent-pair matrix is updated as A′ = C A, and shared on this graph. Using Eq.(11), what information should
t t
the graph convolution [39] is performed using the following be exchanged is calculated during cooperation. This process
equation enhances the agents’ perception, prediction, and decision-
(cid:16) (cid:17)
Hl =ReLU Aˆ H(l−1)W(l−1) , (11) making capabilities. With this knowledge, the local action-
t t t
valuefunctionisdefinedasQ (τ ,a ,m ).Tokeepthebalance
i i i i
wherelistheindexofGNNlayers,Aˆ
t
=D˜− 21A′ tD˜− 21,D˜
ii
= ofcomputationalefficiencywitheffectiveagentinteractionand
(cid:80) A′[i,j], and H0 = E . The current sparse graph A′ not complex decision-making, we build our algorithm on top of
j t t t t
only encapsulates historical information within its structure the QMIX [5] to integrate all the individual Q values. The
but also captures the ongoing agent relationships through the total-action value is monotonic in the per-agent values, which
edgeweights.Themessage-passingmechanismoftheGNNin is formulated as
Eq.(11)enablesagentstoexchangetheirknowledgeeffectively  argmax Q (τ ,a ,m ) 
at every time step. The current feature of the entire graph at
a1 1
.
1 1 1
argmaxQ (τ,a)= . .
the t-step is defined as tot  . 
a
argmax Q (τ ,a ,m )
(cid:88)N an n n n n (15)
g =READOUT( H [i,:]), (12)
t t The entire framework is trained by minimizing the loss func-
i tion
where READOUT(·) is an average function aggregating all L(θ)=L (θ−)+λL (θ ), (16)
TD g g
the agents’ information to obtain the entire graph feature. The
Infer-Present is achieved by where θ includes all parameters in the model, L represents
g
thegraphlossfromEq.(14)andλistheweightofgraphloss.
T
L =(cid:88) ∥g −s ∥ , (13) The TD loss L TD(θ−) in Eq. (16) is defined as
inf t t 2
t=1
L
(θ−)=(cid:104)
r+γmaxQ
(cid:0) s′,a′;θ′(cid:1)
−Q
(s,a;θ−)(cid:105)2
,
where s denotes the actual state of the environment at the TD tot tot
t a′
t step. Infer-Present is a global-level characteristic of LTS- (17)
CG that utilizes graph convolution to facilitate a seamless whereθ′ denotestheparametersofaperiodicallyupdatedtar-
exchange of observations among agents, allowing the en- getnetwork,ascommonlyemployedinDQN.Bytrainingwith
tire graph (comprising all agents/nodes and their relation- theEq.(16),ourmethodenablessimultaneousgraphinference
ships/edges) to represent the current state of the environment and multi-agent policy learning, facilitating efficient end-to-
collectively. end training using standard policy optimization methods.
Withtheabovetwocharacters,thegeneralizedlossfunction
for the graph learning Eq.(3) now can be formalized as V. EXPERIMENTS
L(A,w,O T)=L g =L pre+L inf. (14) Inthissection,wedesignexperimentstoanswerthefollow-
ingquestions:(1)HowdoesLTS-CGcompareinperformance
with state-of-the-art methods on complex cooperative multi-
B. Cooperative MARL with LTS-CG
agenttasks?(See:V-A)(2)Istheutilizationoftrajectoriesfor
In our design, LTS-CG facilitates simultaneous graph in- learning the coordination graph more effective than relying
ference and multi-agent policy learning for efficient end-to- on one-step observations? (See: V-B1) (3) Does having the
end training. Initially, with the buffer’s setup, we store the Predict-Future and Infer-Present characteristics improve the
inter-agent graph with full connectivity. This allows agents performanceofLTS-CG?(See:V-B2)(4)WhataretheeffectsJOURNALOFLATEXCLASSFILES,VOL.X,NO.X,2024 6
Fig. 3: Performance of our method and baselines on six maps of the StarCraft II benchmark [40]. Y-axis is the test winning
rate of the game. X-axis is training steps
of varying the weights for L on the experimental outcomes? Hyperparameter Value
g
(See: V-B3) Batch-size 32
Replaymemorysize 5000
Toanswertheabovequestions,weconductourexperiments discountfactorγ 0.99
on StarCraft II benchmark [40], which consists of different
Optimizer RMSProp
maps with varying numbers of agents. The experiments in- Learningrate 5×10−4
cluded scenarios with a minimum of eight agents, comprising optim alpha 0.99
optim eps 1×10−5
both homogeneous and heterogeneous agent setups. All the
Gradient-norm-clip 10
experimentsarecarriedoutwithdifficulty=7andrepeatedwith
Action-selector ϵ-greedy
5randomseeds.Weemploydistinct2-layerGNNsasspecified
ϵ-start 1.0
in Eq.(11) to facilitate the acquisition of the Infer-present ϵ-finish 0.05
characteristicandtocomputetheknowledgeexchangedduring ϵ-anneal-time 50000steps
agents’ cooperation. The graph loss λ, the character-balance targetupdateinterval 200
weight b and c in Eq (16) are set to 1. The hyperparameters
TABLE II: The hyperparameters used in StarCraft II.
usedforMARLinStarCraftIIaregiveninTab.II.Experiences
arestoredinafirst-in-first-out(FIFO)replaybufferandduring
the training phase. The experiments are finished with Intel(R)
Method Graphtype Edge Datainlearninggraph
Xeon(R) E-2288G CPU and Tesla V100-PCIE-32GB GPU.
The software that we use for experiments is Python 3.7.13, QMIX × × ×
DCG Complete Unweighted One-step
PyTorch1.13.1,PyYAML6.0,numpy1.21.5andCUDA11.6.
DICG Complete Weighted One-step
SOP-CG Sparse Unweighted One-step
CASEC Sparse Weighted One-step
LTS-CG Sparse Weighted Trajectories
Agent Enemy Action Episode
Maps
Num. Num. Num. Limit TABLE III: Comparison of different experiment methods in
8m 8 8 14 120 terms of graph type, edge representation, and data used for
25m 25 25 31 150 learning graph.
3s5z 8 8 14 150
1c3s5z 9 9 15 180
8m vs 9m 8 9 15 120
10m vs 11m 10 11 17 150
27m vs 30m 27 30 36 180 A. Performance comparison on StarCraft II
MMM2 10 12 18 180
1) Details for comprised methods: We utilize several
TABLE I: Detailed information of each map we used in
state-of-the-art baseline algorithms for our experiments. Each
StarCraft II benchmark.
method’s graph type, edge representation, and group utiliza-
tion are summarised in Tab. III. Below, we provide a brief
introductionofeachmethodandthedetailedsettingsweused:JOURNALOFLATEXCLASSFILES,VOL.X,NO.X,2024 7
25m 27m_vs_30m
1.0 0.8
0.7
0.8
0.6
0.5 0.6
0.4
0.4 0.3
QMIX
DCG 0.2 QMIX
0.2 DICG 0.1 DICG
LTS-CG LTS-CG
0.0
0.0
0.5 1.0 1.5 2.0 0.5 1.0 1.5 2.0 2.5
T (mil) T (mil)
Fig. 4: Performance comparison on the 25m and 27m vs 30m maps. Due to the high computational complexity, SOP-CG and
CASEC could not complete the experiments on both maps. DCG could not finish the experiment on the later map.
• QMIX 1 [5] is effective but without cooperation be- thanthecomparedmethodsonallsixmapsintheearlystages
tween agents. We adopt the configuration specified in of training (below 0.6 mil for 8m, 2 mil for MMM2, and 1
the StarCraft Multi-Agent Challenge [40] for the QMIX mil for other maps). This indicates that our approach enables
algorithm. the agents to quickly learn effective cooperative strategies
• DCG 2 [10] directly links all the agents to get an and achieve high-performance levels. Moreover, our method
unweighted fully connected graph. The graph is used to demonstrated a smaller standard deviation in performance
calculate the action-pair values function. For DCG, we compared to the other methods, such as CASEC in 3s5z,
employalow-rankpayoffapproximationwithK =1(as DICG in 8m vs 9m and DCG in 10m vs 11m. The reduced
describedinEq.(5)oftheoriginalpaper)andincorporate variability suggests that our approach consistently produces
privileged information through the action representation reliable and stable cooperative behaviours, resulting in more
learningtechnique.ThiscorrespondstotheDCG-S(rank predictable and robust performance across different maps.
1) setting outlined in the original paper. Notably, our method achieved consistent and competitive
• DICG 3 [12] uses attention mechanisms to calculate performance across all six maps. This indicates that our
weightedfullyconnectedgraph.Thegraphisusedtopass approachgeneralizeswellandiscapableofadaptingtovarious
information between agents. We utilize the DICG algo- environmentalconditionsandagentconfigurations.Theability
rithminthecontextofthecentralisedtrainingcentralised to achieve good results consistently is essential for real-world
execution (CTCE) paradigm. This approach involves applications of multi-agent systems.
using QMIX as the base policy learning framework. Comparing our method to two SOTA approaches, SOP-CG
The graph learning procedure strictly follows the DICG andCASEC,whichaimtolearnsparsegraphsforMARL,we
methodology. observed interesting patterns in their performance on specific
• SOP-CG 4 [13] selects sparse graphs from a pre- maps. In the 3s5z, 1c3s5z, and 10m vs 11m maps, SOP-CG
calculated candidate set. In line with the original paper, outperformedCASEC.However,inthe8m vs 9mandMMM2
we adopt the tree organization G for SOP-CG. In maps, CASEC exhibited superior performance compared to
T
this configuration, the agents are organized in a tree SOP-CG. The varying performance of SOP-CG and CASEC
structure with n−1 edges, ensuring that all agents form indicates the importance of learning the meaningful graph
a connected component. basedontheenvironmentandagentsetup,whichfurtherhigh-
• CASEC 5[14] drops some edges on the weighted fully lightstheadvantagesofourapproachinachievingconstantand
connected graph according to the variance payoff func- competitive performance across diverse scenarios.
tion. We employ the construction q var (Eq.(4) in the Large maps. We further investigated the performance of the
paper) and q var loss (Eq. 8 in the paper) strategies de- proposed method on larger maps: 25m and 27m vs 30m,
scribedintheoriginalpaper.Theweightofthesparseness which are designed to test the scalability and efficiency of the
loss term is set to λ =0.3 in our experiments. algorithms under high computational complexity. Due to the
sparse
2) Results: Fig. 3 presents the results of our method com- high computational demands in representing action-pairs, two
pared to the performance of other algorithms on six different SOTA approaches, SOP-CG and CASEC, could not complete
maps. The experimental results clearly demonstrate the supe- the experiments on both maps, and DCG could not finish
riority of our approach LTS-CG across all scenarios (shown the experiment on the 27 vs 30m map, which is indicative
in orange). Firstly, our method exhibited faster convergence of their computational limitations in this context. In Fig. 4,
the results of our proposed method on these two maps were
1https://github.com/oxwhirl/pymarl
presented.Ourapproachdemonstratedpromisingperformance
2https://github.com/wendelinboehmer/dcg
compared to the other methods, even in these challenging
3https://github.com/sisl/DICG
4https://github.com/yanQval/SOP-CG and computationally intensive scenarios. Notably, the QMIX
5https://github.com/TonghanWang/CASEC-MACO-benchmark algorithm (shown in blue), which operates without explicit
%
niW
tseT
%
niW
tseTJOURNALOFLATEXCLASSFILES,VOL.X,NO.X,2024 8
3s5z 10m_vs_11m
1.0 1.0
0.8 0.8
0.6 0.6
0.4 0.4
OneStepObs-c OneStepObs-c
0.2 0.2
OneStepObs-s OneStepObs-s
LTS-CG(w/o g) LTS-CG(w/o g)
0.0 0.0
0.0 0.5 1.0 1.5 0.0 0.5 1.0 1.5
T (mil) T (mil)
Fig. 5: Performance comparison on the 3s5z and 10m vs 11m. OneStepObs-c and OneStepObs-s utilize a one-step observation
to generate a fully connected graph and a sparse graph separately. LTS-CG(w/oL ) investigates the effectiveness of using
g
trajectories by omitting the Predict-Future and Infer-Present characteristics.
Fig.6:Evaluatetheeffectivenessofthedifferentlatenttemporalsparsegraphlearningstrategiesontwomaps.LTS-CG(w/oLg)
excludesbothPredict-FutureandInfer-Presentcharacteristics.LTS-CG(Lpre)andLTS-CG(L )onlyincorporatethePredict-
inf
Future or Infer-Present characteristic into the learning process separately.
cooperation mechanisms or coordination graphs, surprisingly • LTS-CG(w/oL g)utilizestrajectoriesforgraphgeneration
outperforms DCG and DICG (shown in light green and pink, while excluding Predict-Future and Infer-Present charac-
respectively),whicharegraph-basedlearningalgorithms.This teristics to solely assess the impact of trajectory-based
result indicates that while the graph-based approaches are learning.
designed to foster coordination among agents, the lack of
As depicted in Fig. 5, LTS-CG(w/oL ) surpasses both
a well-constructed coordination graph can be detrimental, g
OneStepObs-c and OneStepObs-s in win percentage over
potentially hindering the policy learning process.
training iterations, demonstrating its superior performance
In summary, the experiments suggest that graph-based co-
in cooperative multi-agent settings. This finding underscores
ordination in multi-agent settings must be carefully crafted
the significant benefit of trajectory-based graph generation in
to ensure that it is conducive to the learning environment.
enhancing MARL performance, independent of other factors.
The results highlight the necessity for well-designed graph
The shaded areas in the figure represent the variance across
structures that enhance rather than impede policy learning, as
multipleruns,withLTS-CG(w/oL )notonlyachievinghigher
evidenced by the success of LTS-CG in complex scenarios g
win rates but also exhibiting less variance, reflecting its
where other graph-based methods struggle.
consistent and reliable performance.
Furthermore, in Fig. 5, the comparison among LTS-
B. Ablation study
CG(w/oL ), OneStepObs-c (a method similar to DICG), and
g
1) Trajectory Graph Learning vs One-Step Observations: OneStepObs-s (a method similar to G2ANet) shows that LTS-
We examined the effect of graph generation methods on CG(w/oL ) demonstrates the most significant performance
g
MARL performance in the 3s5z and 10m vs 11m scenarios. improvement in terms of win percentage across training it-
We considered three settings: erations. This outcome highlights the advantages of using
• OneStepObs-c generates a fully connected graph using trajectory-based information for graph generation, even with-
one-step observations, akin to methods like DICG [12]. out relying on specialized characteristics like Predict-Future
• OneStepObs-s employs one-step observations to create a and Infer-Present.
sparse graph, similar to G2ANet [21]. The shaded regions in the graph represent the variance in
%
niW
tseT
%
niW
tseTJOURNALOFLATEXCLASSFILES,VOL.X,NO.X,2024 9
Fig. 7: Evaluate the effect of the different weights of graph loss.
winpercentagesovermultipleruns,providinginsightsintothe results are obtained after approximately 2 million time steps,
reliability of the methods. Notably, LTS-CG(w/oL ) achieves showcasingtheeffectivenessofleveragingbothcharacteristics
g
higher win rates and maintains tighter confidence intervals, for improved cooperative multi-agent learning. This ablation
suggesting a consistent performance advantage over the other study confirms the significance of two characteristics in LTS-
methods. These experimental results provide strong support CG for learning meaningful graphs to help agents cooperate.
forthehypothesisthattrajectory-basedgraphlearningismore 3) Weight of graph loss: We tested the different weight of
effective and robust than one-step observation-based methods, graphlossL ontwomaps,asshowninFig.7.(w/o L )rep-
g g
contributing significantly to the advancement of cooperative resentsthescenariowheretheMARLtrainingdoesnotinclude
multi-agent learning techniques. the graph loss term, i.e., λ = 0. The results demonstrate the
2) Latent Temporal Sparse Graph Learning strategies: positive impact of incorporating L g in MARL, as compared
We conducted an evaluation to assess the effectiveness of to the case without it. Specifically, when L g = 1,10,50,
differentstrategiesandexaminetheimportanceofthePredict- the addition of L g Consistently improves the performance of
FutureandInfer-Presentcharacteristicsingraphlearning.Our MARL on both maps. As the value of λ increases, the final
investigation focused on the following settings: results during training on both maps first improve and then
starttodecline,whichindicatesthattheweightλofthegraph
• LTS-CG(w/oL g)excludesbothPredict-FutureandInfer-
lossfunctionhasanoticeableinfluenceonthefinalresults.We
Presentcharacteristics.Thissettingimpliesthatwedonot
present empirical evidence related to the parameter λ here.
further refine the learned graph structure after sampling.
Identifying the most appropriate λ value for specific sce-
• LTS-CG(L pre)onlyincorporatesthePredict-Futurechar-
narios is a labour-intensive task that requires additional ex-
acteristic into the learning process.
perimentation. It involves balancing leveraging the benefits
• LTS-CG(L inf) only incorporates the Infer-Present char-
of graph-based learning and avoiding potential overfitting or
acteristic into the learning process.
performance degradation due to excessive emphasis on the
• LTS-CG with both Predict-Future and Infer-Present char-
graph loss term. This process underscores the nuanced nature
acteristics, where we consider both aspects simultane-
of parameter tuning in MARL and highlights the need for
ously.
careful consideration when designing and optimizing such
The final performance is assessed on the 8m vs 9m, and 3s5z systems.
maps and the results are presented in Fig. 6. The ablation
study revealed several important findings. Firstly, regardless
C. Discussion
of whether we include the Predict-Future, the Infer-Present,
or both characteristics, the performance was consistently bet- In this discussion, we underscore the contrasts between our
ter than not having anyone. This highlights the importance proposed approach and CG-based methods (e.g., DCG [10],
of having these characteristics in enhancing the learning of SOP-CG [13], and CASEC [14]), as well as with the method
the inter-agent graph and improving cooperative behaviour. where the graph serves for information exchange (e.g., DICG
Moreover, on the 8m vs 9m map, with the Predict-Future [12], G2ANet [21]).
characteristic outperformed the other settings. One possible 1) Coordinationgraphs(CG)methods: TheCGisdenoted
reason for this observation is that the agents in this map as G = {V,E} where V is agent/node set and E is the edge
arehomogeneous,sharingsimilarcharacteristics.Knowingthe setbetweenagents[8,41].TheseCG-basedmethodsfactorize
next time observation benefits the overall cooperation among the Q-function into utility functions qi and payoff functions
the agents. In contrast, in the 3s5z map agents are hetero- qij as follows:
geneous. Utilizing observations from different agents to infer
thecurrentstateprovesbeneficialforlearninginthisscenario. Q (s ,a)= 1 (cid:88) qi(cid:0) ai|s (cid:1) + 1 (cid:88) qij(cid:0) ai,aj|s (cid:1) .
tot t |V| t |E| t
Although using both characteristics simultaneously introduces
i∈V {i,j}∈E
more parameters and slightly slower convergence, promising (18)JOURNALOFLATEXCLASSFILES,VOL.X,NO.X,2024 10
1mstepstime(h) TimeComplexity methods learn a weighted fully-connected graph, which in-
DCG 11.63±0.64 O(A2N2) evitably transfers redundant or even detrimental information
SOP-CG 19.46±0.80 O(A2N2) betweenagents,asillustratedinFig.8.Thisdrawbackimpedes
CASEC 10.12±0.51 O(A2N2)
individual agents’ capacity to acquire effective policies. In
LTS-CG 8.84±0.49 O(TN2)
contrast, our method generates a sparser graph focusing on
the most relevant relationships among agents, leading to more
TABLE IV: Time consumption on map 10m vs 11m and
efficient and effective cooperation.
complexity analysis with CG-based methods.
Another existing method is G2ANet [21], which uses both
hard and soft attention to generate a sparse graph for MARL.
However, our method offers two distinct advantages over
G2ANet: (1) G2ANet focuses solely on ongoing information,
while we leverage the complete observation trajectories to
capture the latent relationships between agents, shown in
Sec.V-B1. (2) Unlike G2ANet’s arbitrary edge reduction us-
ing the attention mechanism, our approach harnesses agents’
experience and current information to acquire a meaningful
graph. This graph exhibits two critical attributes, the efficacy
of which has been demonstrated through Sec.V-B2.
Fig. 8: Visualization of latent agent-pair matrices learned by
VI. CONCLUSIONSANDFUTUREDIRECTIONS
DICG and LTS-CG on map 8m vs 9m.
ThispaperintroducesLTS-CG,anovelapproachforMARL
that infers a latent temporal sparse graph to enable effective
information exchange among agents. To efficiently infer the
However, due to the large number of action pairs represented
graph from past experiences, LTS-CG uses the agents’ ob-
by the payoff functions qij, these methods face challenges
servation trajectories to generate the agent-pair probability
of high computational complexity. To alleviate this concern,
matrix. Motivated by the idea that the meaningful graph
various approaches have been employed, such as low-rank
should enrich agents’ comprehension of their peers and the
approximation in DCG [10], construction of polynomial-time
environment, we further learn the graph to encode two es-
CGinSOP-CG[13],anddroppingedgesusingvariancepayoff
sential characteristics: Predict-Future and Infer-Present. The
functions in CASEC [14]. Despite these efforts, the inherent
formerisalocal-levelcharacteristicthatgivesagentsvaluable
high complexity of CG-based methods, as high as O(A2N2),
insightsintothefutureenvironment,enhancingtheirdecision-
limits their applicability in large-scale scenarios.
making capabilities in the current time step. The latter is
Bycontrast,thecomplexitiesofLTS-CGisO(TN2),where
a global-level one that enables partially observed agents to
N is the number of agents, and T is the length of trajectories.
deducethecurrentstate,promotingoverallcooperationamong
For instance, consider experiments conducted on the map
agents.Byhavingthem,LTS-CGlearnstemporalgraphsfrom
10m vs 11m, involving 10 agents and 17 actions, resulting
historical and real-time information, facilitating knowledge
in 289 potential action-pair within qij. Table IV illustrates the
exchange during policy learning and effective collaboration.
timerequirementsofvariousmethods,revealingthatCG-based
Graph learning and agent training occur simultaneously in an
approaches demand more time to complete 1m time steps
end-to-end manner. Experimental evaluations on the StarCraft
than our method. Particularly noteworthy is the significantly
II benchmark demonstrate the superior performance of our
higher time consumption of SOP-CG and the remarkably
method over existing ones.
elevated GPU usage of CASEC (25.85 GB), contrasted with
For future directions, it is imperative to extend the scope of
DCG (2.29 GB), SOP-CG (4.00 GB), and our approach (4.13
graph learning beyond agent-pair relationships. Investigating
GB). Moreover, it’s important to highlight that all CG-based
higher-order relationships, such as group dynamics, while
methods failed to conclude the experiment on the 27 vs 30m
inferring cooperation graphs can deepen our understanding of
map, involving the calculation of 362 = 1296 action-pair for
cooperativebehavioursamongagents.Additionally,addressing
every two agents (the complexity now is O(1296 × N2)).
the challenges posed by asynchronous scenarios is crucial.
In contrast, for our proposed LTS-CG method, even though
Developing techniques to effectively learn cooperation graphs
we set the length of trajectory for graph learning to the
insuchscenarioswillenhancetheapplicabilityandrobustness
episodelimit,thecomputationalcomplexityremainsamodest
of methods in real-world environments.
O(180×N2). In the actual experiment, this trajectory length
typicallyfallsbelowtheepisodelimit.Thus,LTS-CGdelivers
competitive performance while maintaining a reasonable level
ACKNOWLEDGMENT
of computational efficiency.
2) Graphs for information exchange: Current methods us- This work is supported by the Australian Research Coun-
ing graphs for information exchange rely on attention mech- cil under Australian Laureate Fellowships FL190100149 and
anisms to calculate edge weight. Like DICG [12], these Discovery Early Career Researcher Award DE200100245.JOURNALOFLATEXCLASSFILES,VOL.X,NO.X,2024 11
REFERENCES Systems (AAMAS 2021), Virtual Event, United Kingdom,
2021, pp. 764–772.
[1] M. Wang, L. Wu, J. Li, and L. He, “Traffic signal [13] Q. Yang, W. Dong, Z. Ren, J. Wang, T. Wang, and
control with reinforcement learning based on region- C. Zhang, “Self-organized polynomial-time coordination
aware cooperative strategy,” IEEE Trans. Intell. Transp. graphs,” in International Conference on Machine Learn-
Syst., vol. 23, no. 7, pp. 6774–6785, 2022. ing (ICML 2022), Baltimore, Maryland, USA, vol. 162,
[2] Y. Rizk, M. Awad, and E. W. Tunstel, “Cooperative 2022, pp. 24963–24979.
heterogeneous multi-robot systems: A survey,” ACM [14] T. Wang, L. Zeng, W. Dong, Q. Yang, Y. Yu, and
Comput. Surv., vol. 52, no. 2, pp. 29:1–29:31, 2019. C. Zhang, “Context-aware sparse deep coordination
[3] J. Cui, Y. Liu, and A. Nallanathan, “Multi-agent rein- graphs,” in the 10th International Conference on Learn-
forcement learning-based resource allocation for UAV ing Representations (ICLR 2022), Virtual Event, 2022.
networks,” IEEE Trans. Wirel. Commun., vol. 19, no. 2, [15] A. Pacchiano, J. Parker-Holder, Y. Tang, K. Choroman-
pp. 729–743, 2020. ski,A.Choromanska,andM.Jordan,“Learningtoscore
[4] P.Sunehag,G.Lever,A.Gruslys,W.M.Czarnecki,V.F. behaviors for guided policy optimization,” in the 37th
Zambaldi, M. Jaderberg, M. Lanctot, N. Sonnerat, J. Z. International Conference on Machine Learning, (ICML
Leibo, K. Tuyls, and T. Graepel, “Value-decomposition 2020), vol. 119, 13–18 Jul 2020, pp. 7445–7454.
networks for cooperative multi-agent learning based on [16] F. A. Oliehoek and C. Amato, A Concise Introduction to
team reward,” in the 17th International Conference on Decentralized POMDPs, ser. Springer Briefs in Intelli-
Autonomous Agents and MultiAgent Systems (AAMAS gent Systems. Springer, 2016.
2018), Stockholm, Sweden, 2018, pp. 2085–2087. [17] A. Oroojlooy and D. Hajinezhad, “A review of coop-
[5] T. Rashid, M. Samvelyan, C. S. de Witt, G. Farquhar, erative multi-agent deep reinforcement learning,” Appl.
J. N. Foerster, and S. Whiteson, “QMIX: monotonic Intell., vol. 53, no. 11, pp. 13677–13722, 2023.
value function factorisation for deep multi-agent rein- [18] R. Lowe, Y. Wu, A. Tamar, J. Harb, P. Abbeel,
forcementlearning,”inthe35thInternationalConference and I. Mordatch, “Multi-agent actor-critic for mixed
on Machine Learning (ICML 2018), Stockholmsma¨ssan, cooperative-competitive environments,” in the 30th An-
Stockholm, Sweden, vol. 80, 2018, pp. 4292–4301. nual Conference on Neural Information Processing Sys-
[6] K. Son, D. Kim, W. J. Kang, D. Hostallero, and Y. Yi, tems (NIPS 2017), Long Beach, CA, USA, 2017, pp.
“QTRAN: learning to factorize with transformation for 6379–6390.
cooperative multi-agent reinforcement learning,” in the [19] J. N. Foerster, G. Farquhar, T. Afouras, N. Nardelli,
36th International Conference on Machine Learning and S. Whiteson, “Counterfactual multi-agent policy
(ICML 2019), Long Beach, California, USA, vol. 97, gradients,” in the 32nd AAAI Conference on Artificial
2019, pp. 5887–5896. Intelligence(AAAI2018),NewOrleans,Louisiana,USA,
[7] Y. Hong, Y. Jin, and Y. Tang, “Rethinking individual 2018, pp. 2974–2982.
global max in cooperative multi-agent reinforcement [20] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and P. S.
learning,” in the 36th Annual Conference on Neural Yu,“Acomprehensivesurveyongraphneuralnetworks,”
Information Processing Systems (NIPS 2022), vol. 35, IEEETrans.NeuralNetworksLearn.Syst.,vol.32,no.1,
2022, pp. 32438–32449. pp. 4–24, 2021.
[8] C. Guestrin, M. G. Lagoudakis, and R. Parr, “Coor- [21] Y. Liu, W. Wang, Y. Hu, J. Hao, X. Chen, and Y. Gao,
dinated reinforcement learning,” in the 19th Interna- “Multi-agent game abstraction via graph attention neural
tionalConference(ICML2002),UniversityofNewSouth network,” in the 34th AAAI Conference on Artificial
Wales, Sydney, Australia, 2002, pp. 227–234. Intelligence (AAAI 2020), New York, NY, USA,, 2020,
[9] I.-J. Liu, R. A. Yeh, and A. G. Schwing, “Pic: Permuta- pp. 7211–7218.
tion invariant critic for multi-agent deep reinforcement [22] T. Wang, J. Wang, C. Zheng, and C. Zhang, “Learning
learning,” in the 3rd Conference on Robot Learning nearlydecomposable valuefunctions viacommunication
(CoRL 2019), Osaka, Japan, vol. 100, 2020, pp. 590– minimization,” in the 8th International Conference on
602. Learning Representations (ICLR 2020), Addis Ababa,
[10] W. Boehmer, V. Kurin, and S. Whiteson, “Deep coordi- Ethiopia, 2020.
nation graphs,” in the 37th International Conference on [23] W. Duan, J. Xuan, M. Qiao, and J. Lu, “Learning from
MachineLearning(ICML2020),VirtualEvent,vol.119, the dark: Boosting graph convolutional neural networks
2020, pp. 980–991. with diverse negative samples,” in the 36th AAAI Con-
[11] N. Naderializadeh, F. H. Hung, S. Soleyman, and ference on Artificial Intelligence (AAAI 2022), Virtual
D. Khosla, “Graph convolutional value decomposi- Event. AAAI Press, 2022, pp. 6550–6558.
tion in multi-agent reinforcement learning,” CoRR, vol. [24] J. Jiang, C. Dun, T. Huang, and Z. Lu, “Graph con-
abs/2010.04740, 2020. volutional reinforcement learning,” in 8th International
[12] S. Li, J. K. Gupta, P. Morales, R. E. Allen, and M. J. Conference on Learning Representations (ICLR 2020),
Kochenderfer, “Deep implicit coordination graphs for Addis Ababa, Ethiopia, 2020.
multi-agent reinforcement learning,” in the 20th Interna- [25] S. Iqbal and F. Sha, “Actor-attention-critic for multi-
tionalConferenceonAutonomousAgentsandMultiagent agent reinforcement learning,” in the 36th InternationalJOURNALOFLATEXCLASSFILES,VOL.X,NO.X,2024 12
Conference on Machine Learning (ICML 2019), Long [39] T. N. Kipf and M. Welling, “Semi-supervised classifi-
Beach, California, USA, vol. 97, 2019, pp. 2961–2970. cation with graph convolutional networks,” in the 5th
[26] T.Wang,H.Dong,V.R.Lesser,andC.Zhang,“ROMA: International Conference on Learning Representations
multi-agentreinforcementlearningwithemergentroles,” (ICLR 2017), Toulon, France, April 24-26, 2017.
in the 37th International Conference on Machine Learn- [40] M. Samvelyan, T. Rashid, C. S. de Witt, G. Farquhar,
ing (ICML 2020), Virtual Event, vol. 119, 2020, pp. N. Nardelli, T. G. J. Rudner, C. Hung, P. H. S. Torr,
9876–9886. J. N. Foerster, and S. Whiteson, “The starcraft multi-
[27] B. Yu, H. Yin, and Z. Zhu, “Spatio-temporal graph agent challenge,” in the 18th International Conference
convolutional networks: A deep learning framework for onAutonomousAgentsandMultiAgentSystems(AAMAS
traffic forecasting,” in the 27th International Joint Con- 2019), Montreal, QC, Canada,, 2019, pp. 2186–2188.
ference on Artificial Intelligence (IJCAI 2018), Stock- [41] C. Guestrin, S. Venkataraman, and D. Koller, “Context-
holm, Sweden, 2018, pp. 3634–3640. specific multiagent coordination and planning with fac-
[28] Z. Wu, S. Pan, G. Long, J. Jiang, X. Chang, and tored mdps,” in the 18th National Conference on Artifi-
C. Zhang, “Connecting the dots: Multivariate time series cial Intelligence, (AAAI 2002), 2002, pp. 253–259.
forecasting with graph neural networks,” in The 26th
ACMSIGKDDConferenceonKnowledgeDiscoveryand
DataMining(KDD2020),VirtualEvent,CA,USA,2020,
pp. 753–763.
[29] V. G. Satorras, S. S. Rangapuram, and T. Januschowski,
“Multivariate time series forecasting with latent graph
inference,” CoRR, vol. abs/2203.03423, 2022.
[30] T. N. Kipf, E. Fetaya, K. Wang, M. Welling, and R. S.
Zemel, “Neural relational inference for interacting sys-
tems,” in the 35th International Conference on Machine
Learning (ICML 2018), Stockholmsma¨ssan, Stockholm,
Sweden, vol. 80. PMLR, 2018, pp. 2693–2702.
[31] L.Franceschi,M.Niepert,M.Pontil,andX.He,“Learn-
ing discrete structures for graph neural networks,” in
the 36th International Conference on Machine Learning
(ICML 2019), Long Beach, California, USA, vol. 97,
2019, pp. 1972–1982.
[32] C. Shang, J. Chen, and J. Bi, “Discrete graph structure
learning for forecasting multiple time series,” in the 9th
International Conference on Learning Representations
(ICLR 2021), Virtual Event, Austria, 2021.
[33] J. Li, C. Hua, J. Park, H. Ma, V. M. Dax, and
M. J. Kochenderfer, “Evolvehypergraph: Group-aware
dynamic relational reasoning for trajectory prediction,”
CoRR, vol. abs/2208.05470, 2022.
[34] K. Hornik, “Approximation capabilities of multilayer
feedforward networks,” Neural Networks, vol. 4, no. 2,
pp. 251–257, 1991.
[35] E. Jang, S. Gu, and B. Poole, “Categorical reparameter-
ization with gumbel-softmax,” in the 5th International
Conference on Learning Representations (ICLR 2017),
Toulon, France, 2017.
[36] C. J. Maddison, A. Mnih, and Y. W. Teh, “The concrete
distribution: A continuous relaxation of discrete random
variables,”inthe5thInternationalConferenceonLearn-
ing Representations (ICLR 2017),Toulon, France, 2017.
[37] Y. Li, R. Yu, C. Shahabi, and Y. Liu, “Diffusion con-
volutional recurrent neural network: Data-driven traffic
forecasting,” in the 6th International Conference on
Learning Representations (ICLR 2018), Vancouver, BC,
Canada, 2018.
[38] W. Duan, J. Lu, Y. G. Wang, and J. Xuan, “Layer-
diverse negative sampling for graph neural networks,”
Transactions on Machine Learning Research, 2024.