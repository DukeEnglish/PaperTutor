Semantic Map-based Generation of Navigation Instructions
Chengzu Li1,2∗, Chao Zhang2, Simone Teufel1,
Rama Sanand Doddipatla2, Svetlana Stoyanchev2
1UniversityofCambridge,2ToshibaEuropeLimited
{cl917,sht25}@cam.ac.uk
{chao.zhang,rama.doddipatla,svetlana.stoyanchev}@toshiba.eu
Abstract
Weareinterestedinthegenerationofnavigationinstructions, eitherintheirownrightorastrainingmaterialfor
roboticnavigationtask. Inthispaper,weproposeanewapproachtonavigationinstructiongenerationbyframing
theproblemasanimagecaptioningtaskusingsemanticmapsasvisualinput. Conventionalapproachesemploy
asequenceofpanoramaimagestogeneratenavigationinstructions. Semanticmapsabstractawayfromvisual
detailsandfusetheinformationinmultiplepanoramaimagesintoasingletop-downrepresentation,therebyreducing
computationalcomplexitytoprocesstheinput. Wepresentabenchmarkdatasetforinstructiongenerationusing
semantic maps, propose an initial model and ask human subjects to manually assess the quality of generated
instructions. Ourinitialinvestigationsshowpromiseinusingsemanticmapsforinstructiongenerationinsteadofa
sequenceofpanoramaimages,butthereisvastscopeforimprovement. Wereleasethecodefordatapreparation
andmodeltrainingathttps://github.com/chengzu-li/VLGen.
Keywords:semanticmap,navigationinstructiongeneration,Room2Room
1. Introduction achievesabetterresult(Wangetal.,2022d).
Vision and Language Navigation (VLN) is a task The existing approach to this task has two short-
thatinvolvesanagentnavigatinginaphysicalen- comings. Fromtheperspectiveofrepresentation,
vironmentinresponsetonaturallanguageinstruc- usingpanoramicimagesisresource-intensiveasit
tions(Wuetal.,2021). Thedataannotationforthe requiresprocessingofmultipleimageinputscorre-
VLNtaskistime-consumingandcostlytoscaleup, spondingtodifferentpointsonthepath. Second,
and the development of models that address the panoramic images contain many details that are
taskisseverelylimitedbytheavailabilityoftraining irrelevant for the task. The model has to learn to
data(Guetal.,2022). Navigationinstructiongen- interprettheenvironmentsfromRGBpanoramas,
eration (VL-GEN) is the reverse of the VLN task suchasobjectrecognition,andgenerateinstruc-
in that it generates natural language instructions tionsatthesametime. Asitisnaturalforhumans
forapathinthevirtual(orphysical)environment, to understand navigation instructions from a top-
whichishelpfulforinteractionswithusersandex- down map (as in Google Maps) (Paz-Argaman
plainability. Previousworkhasalsodemonstrated etal.,2024),weproposetoseparatetheVL-GEN
theeffectivenessofVL-GENinimprovingtheper- taskintotwosteps: 1)environmentinterpretation,
formance of VLN systems such as the Speaker- which is addressed by semantic SLAM in physi-
Followermodel(Friedetal.,2018)andEnvDrop calroboticsystems(Chaplotetal.,2020),and2)
(Tanetal.,2019). ThispaperexplorestheVL-GEN spatial reasoning. In this paper, we focus on the
taskofgeneratingnavigationinstructionframingit second step and explore the feasibility of using
asanimagecaptioningtask. top-downsemanticmapforVL-GEN.
VL-GENrequiresthemodeltogeneratelanguage Our research question is whether it is feasible to
instruction in the context of the physical environ- use the top-down semantic map (a single RGB
ment,groundingobjectsreferencesandactionin- image) as our main source of information. We
structions to the given space. Previous studies alsoexplorewhichotherdatasources,inaddition
usephoto-realisticRGBpanoramicimagesasthe to the semantic map, can further improve perfor-
visualinput;theyframeVL-GENastheend-to-end mance. To address this question, we formalize
taskofgeneratingtextfromasequenceofphoto- theVL-GENtaskasimagecaptioningwiththein-
realisticRGBimages(Friedetal.,2018;Tanetal., put of a semantic map with the path (see Figure
2019;Wangetal.,2022d). WhileZhaoetal.(2021) 1). Weextracttheimagesoftop-downmapsfrom
reportthattheoverallqualityofinstructionsgener- the Habitat simulator (Savva et al., 2019) based
atedwithend-to-endmodelsisonlyslightlybetter onRoom-to-Roomdataset(Andersonetal.,2018)
thanthatoftemplate-basedgeneration,theappli- andVLN-CE(Krantzetal.,2020). Ourkeycontri-
cationofobjectgroundingtothepanoramicimages butionsandfindingsincludethefollowing:
• We extend the R2R dataset with semantic
∗WorkdoneoninternshipatToshibaEuropeLimited.
4202
raM
82
]LC.sc[
1v30691.3042:viXrameetingroom Wealsoexperimentwithprovidingthemodelwith
straight
, hallway
additionalfeaturesofthenavigationpathbeyond
hallway left thesemanticmapsalone,includingactions,names
… …… …… ofregions,andpanoramicimages. Thereisafixed
setofactiontypes(LEFT, RIGHT, STRAIGHT, STOP),
hallway right
which are determined heuristically from the path
meetingroom stop shape at each navigation point. For each navi-
gation point, we use the name of its associated
Top-down semantic map Panoramas Regions Actions
region (e.g., hallway, meeting room). We do not
One of the Three Human Annotated Navigation Instructions:
● Turn left and follow the rope. At the end turn left and follow the red carpet to the end. thinkthatpanoramicimagesconstituteidealinput
At the end, turn right and stop in front of the white and gold table.
tothesystem,butitispossiblethattheymaypro-
Figure 1: An example navigation scenario from videadditionalvisualinformationnotshowninthe
ournewdatasetforinstructiongeneration,withthe map. Therefore,wealsoconductexperimentswith
navigationpathoverlayedonthesemanticmap. panoramicimagesaspartoftheinputinformation
tothemodel.
split size Avg.#points Avg.#regions Avg.#objects We extract semantic maps, region and action in-
train 10623 5.95 3.26 22.64
formation from the Habitat (Savva et al., 2019;
valseen 768 6.07 3.3 22.36
Krantz et al., 2020) simulation environment. In
valunseen 1839 5.87 3.11 22.13
a deployed robot, it may be obtained with a se-
Table1: Statisticsofextractedsemanticmaps. Avg. mantic SLAM component (Chaplot et al., 2020).
#region: averagenumberofdistinctregionsalong Each object type on the map is represented in a
thepath. Avg. #object: averagenumberofobject uniquecolor. Weadoptthenavigationpathsand
typesinthesemanticmap. humanannotationsfromtheR2Rdataset(Ander-
son et al., 2018). Panoramic images in RGB are
obtainedfromtheMatterport3Dsimulator(Chang
maps, providing a new benchmark dataset etal.,2017)ateachdiscretenavigationpoint. An
andabaselinethatdemonstratesthefeasibil- example of the new dataset derived from R2R,
ityofusingsemanticmapsforVL-GENtask. including a semantic map with a path, language
instruction,panoramaimages,actions,andregion
• Wedemonstrateexperimentallywithbothau- names,isshowninFigure1.
tomaticandhumanevaluationsthatincluding
Statisticsaboutthesemanticmapsarepresented
additionalinformation(namely,region,action,
in Table 1. The data splits we use are inherited
and prompt) leads to more accurate and ro-
fromtheoriginalR2Rdataset. Thedifferencebe-
bust navigation instructions than using only
tweenseenvalidationsetandtheunseenvalida-
semanticmaps.
tionsetinR2Riswhethertheroomenvironmentis
• Wealsoconductanintrinsichumanevaluation includedinthetrainset.1
ofthequalityofthegeneratedinstructionswith
fine-grainederroranalysis. 3. Method
Motivated by the success of the multimodal pre-
2. Task Definition and Data
trained models, we construct a multimodal text
generationmodelusingBLIP2 (Lietal.,2022). Fig-
A semantic map M is a top-down view of the
s
ure 2 illustrates the architecture of the proposed
scene s, which contains a path P = {p ,...,p },
1 K
modelwithmodulesthatprocessdifferentinputs;
representedasasequenceofpointsconnectedby
these will be described in Section 3.1. In Sec-
aline,andasetofN objectsO ={o ,...o }.
1 N
tion3.2,wedescribetheaugmentationsappliedto
In light of the success of image captioning mod- theBLIPmodelinourexperiments.
els(Lietal.,2022;Wangetal.,2022b),weframe
theVL-GENtaskasimagecaptioningtask. Given 3.1. Model Input
asemanticmapM ,thetaskistogenerateanat-
s
ural language description D that describes the Top-down semantic map (TD) The semantic
P
pathP shown. Ourtaskdescriptionreplacesthe mapformsthemaininputusedinallexperiments.
photo-realisticRGBimagesusedpreviously,with
asemanticmap. TheprocessingofRGBimages 1FurtherdetailsonthedatasetarepresentedinAp-
isresource-intensive,whileourtaskdefinitionhas
pendixA.1.
theadvantageofabstractingawayfromtheobject 2The implementation is based on the Huggingface
recognitiontask,concentratingontheinstruction transformerslibrary(Wolfetal.,2019): Salesforce/blip-
generationtaskinstead. image-captioning-baseSemantic map methodsinthevideocaptioningtask(Tangetal.,
Semantic Map Encoder (BLIP&tuned) prompt
2021;Luoetal.,2022),wetreatthepanoramasas
LSTM
discrete frames and use the mean average of all
Action Embedding
Action
straight left left straight … Decoder inA sn tn ruo cta tit oe nd s panoramicembeddingstorepresentthepanorama
Meetingroom hallway hallway hallway … (BLIP) informationofthenavigationpath.
Region Text Encoder (BLIP&tuned) Region
LSTM
Cross lE osn stropy Finally, the embedded input representations are
Mean Pooling added together to form the input to the decoder
P Ea nn co ora dm era P Ea nn co ora dm era P Ea nn co ora dm era P Ea nn co ora dm era … inP sr te rd ui cc tt ie od n s thatoutputsnaturallanguageinstructions.
(BLIP) (BLIP) (BLIP) (BLIP)
Panorama
… Contrastive
loss
3.2. Model Augmentation
Figure2: Illustrationoftheoverallmodelarchitec-
Multimodal alignment with contrastive loss
ture. Text input is encoded with pretrained BLIP
Contrastivelearningisaneffectivemethodusedin
text encoder and LSTM, and image input is en-
self-supervisedlearningforvisualrepresentation
codedwiththepretrainedBLIPencoder. Modules
learning(Radfordetal.,2021;Lietal.,2022)and
shown in the same color share the weights. The
multimodal pre-training in BLIP (Li et al., 2022).
weightsofthepanoramaencoderarefixed.
Weinvestigatetheeffectivenessofintroducingcon-
trastive training for navigation instruction genera-
tiontaskasanauxiliaryloss. Wedefinethepos-
It is encoded by the image encoder in the BLIP
itive examples P+(C ,I ) as pairs of the com-
model. Wefirstresizetheimagebynearestsam- gt gt
binedinputembeddingandtheinstructionembed-
pling to 384×384 and then feed it to the vision
ding. The negative examples P−(C ,I ) con-
transformerwithpatchsize16. gt rnd
sist of the pairs of the input embedding and the
embeddingofarandomlysampledinstruction. Fol-
Regions (Reg) and actions (Act) Region lowingCLIP(Radfordetal.,2021),wemultiplythe
names and actions are frequently mentioned in multimodalinputmatrixE andtextualinstruc-
input
humannavigationinstructions. Togivethemodel tionmatrixE toobtainthepredictedcompatible
text
information about the relevant region names, we matrix C between inputs and labels and then
pred
representthemasasequenceofstringsforeach computetheCrossEntropylossonC withthe
pred
navigationpoint. Weuseatextencoderfromthe ground-truthcorrespondenceC .
gt
pre-trained BLIP model to represent the region
names. The region embedding for each point is
Augmentation and grounding with prompt
obtainedbyapplyingameanpoolingoperationto
ThepromptingofLLMshasdemonstrateditseffec-
the word embeddings. For actions, we apply an
tivenessacrossvariousdomainsinpreviousworks
embeddinglayertothediscreteactionvaluesand
(Li and Liang, 2021; Liu et al., 2021; Tang et al.,
getactionembeddingsinthesamedimensionas
2022;Keicheretal.,2022;Songetal.,2022). We
theregionembedding. Weaddtheregionandthe
generate the prompt from a template, which de-
actionembeddingstogetherateachpointanduse
scribes the nearby objects and regions, such as
a 3-layer LSTM model to embed the sequential
Startingfromthedarkyellowpointnearsofacush-
informationalongthenavigationpath.
ion in the living room region. We tune the model
withpromptingandfeedtheprompttemplatetothe
Panoramicimages(Pano) Basedonouranal- decoderduringinference. Wearguethatprompting
ysis, visual object properties such as color and canbenefitthegenerationtaskintwoways. First,
shapearementionedinmorethan25%ofhuman itcanhelpvisual-languagegroundingbecausethe
instructions. Assemanticmapsonlyincludeobject prompting template describes nearby landmarks
typesbutnotthepropertiesofvisualobjects, we and regions. Second, at inference time, the in-
augmentthemodelinputwithpanoramicimages. structions that are generated are conditioned on
This might enable the model to learn the visual theprompttemplateinanauto-regressiveway,re-
properties mentioned in the instructions. We ini- sultinginmorecontrollablegenerationinVL-GEN
tializetheimageencoderbasedonthepre-trained task.
image encoder in BLIP model. We freeze its pa-
rametersduringtrainingbecausethemodelispre- 4. Experiments
trainedonphoto-realisticimages,whichwebelieve
endowsthemodelwithcapabilitiesofrecognizing Weperformtwoevaluationsoverexperiments: an
panoramicimagesinourcase. Inordertoincrease automaticevaluationaccordingtoperformanceon
the flexibility of the visual embedding, we apply thetask(extrinsic)andahumanevaluationofthe
an additional MLP with two linear layers on top quality of the instructions (intrinsic). These eval-
of the panoramic vision encoder. Following the uations can tell us about the influence of region,SPICE HumanScore instructioncandidatesgeneratedbydifferentsys-
Input P C
seen unseen unseen tems.
TD(baseline) - - 20.50 16.19 3.42(5)
✓ - 20.79 15.77 -
4.3. Automatic Evaluation Metrics
✓ ✓ 21.78* 17.10 -
TD+Reg+Act - - 21.00 17.00 4.20(3)
Intheautomaticevaluation,wecomparetheper-
✓ - 21.86* 17.84** 4.29(2)
formanceof9systemvariantsbasedonanauto-
✓ ✓ 19.96 17.09 3.98(4)
maticmetricSPICE(SemanticPropositionalImage
TD+Reg+Act+Pano - - 19.87 17.44* 4.36*(1)
CaptionEvaluation)(Andersonetal.,2016),follow-
✓ - 22.14** 17.79** -
ing Zhao et al. (2021). SPICE is a metric used
✓ ✓ 20.36 17.08 -
to evaluate the quality of image captions, focus-
Table2: Automatic(SPICE)andhumanevaluation ingonthesemanticcontentofcaptions. Itidenti-
results with inputs of different modalities in seen fies semantic propositions within the parse trees
and unseen environments, where P is short for andcomparesthesemanticpropositionsfromthe
promptandCisshortforcontrastiveloss. **and* generated caption with those from the reference
indicatestatisticallysignificantdifferencewiththe captions.
baseline(p≤0.01)and(p≤0.05).
When comparing different systems, we use the
two-sidedpermutationtesttoseeifthearithmetic
meansofthetwosystems’performancesareequal.
actions, prompting, and contrastive loss on the
Ifthep-valueislargerthan0.05,weconsiderthe
quality of the instructions both quantitatively and
performance of the two systems to be not signifi-
qualitatively.
cantlydifferent.
4.1. Experimental setup
4.4. Evaluation Results
WetrainthemodelusingthetrainsplitoftheR2R
Table 2 shows the SPICE and human evaluation
datasetandevaluateitbothonvalidationseenand
scoresinseenandunseenenvironments. Asex-
unseensets. WeusetheBLIP-basemodelforex-
pected,themodelsperformbetterinseenthanin
periments. We setup the baselines with different
unseen setting by 3.88 in SPICE score on aver-
combinations of the input: 1) top-down semantic
age across all 9 systems. For both settings, we
map (TD) 2) + regions (Reg) and actions (Act);
observe that using region and action information
3) + panoramic images (Pano). We also experi-
withthepromptimprovesthemodel’sperformance
mentwithcontrastivelossandprompting,making
withp≤0.05,whilecontrastivelearningdoesnot
9systemvariantsforexperimentsintotal.
seem to help. Adding panoramic images tends
In the intrinsic human evaluation, we use a Latin to improve the performance, but not significantly
Square design of size 5. We therefore compare (p≥0.1). Whencomparingwithpreviousmethods
only a subset of the above system variants with in SPICE score, our systems (17.84/22.14) per-
differentcombinationsofinput(TD,TD+Reg+Act formonparorevenachievehigherSPICEscores
andTD+Reg+Act+Pano),andpromptingandcon- thanSpeakerFol. (Friedetal.,2018)(17.0/18.7)
trastivelossonTD+Reg+Act. andEnvDrop(Tanetal.,2019)(18.1/20.2)onun-
seen/seensettings.
4.2. Human Participants and Procedure In the results for the human evaluation, shown
in Table 2, we observe that using the semantic
Forthehumanexperiment,werecruit5evaluators
mapastheonlyinputresultsinthelowestaverage
who have never contributed to or been involved
scoreacrossallsystems(3.42). Thisrepeatsthe
in the project before under the consent from the
observationsfromtheautomaticevaluation. Using
Ethics Committee. The evaluation workload for
regions,actions,andpanoramasachievesthehigh-
each participant is designed to be within 30 min-
est rating (4.36) which is significantly better than
utesforthemtoconcentrateonthetask. Wealso
the baseline (p=0.05), followed by using regions,
providetwospecificillustrationexamplesaboutthe
actions, and prompts (4.29). However, incorpo-
evaluation task for the human participants. The
ratingPano (4.36)alongsideTD+Reg+Act (4.20)
evaluationmaterialsconsistof15navigationpaths
doesnotshowanoteworthydifference.
in the unseen environments, randomly sampled.
Theexperimentisperformedonlineusinganeval- Inadditiontotheresultsabove,wewerealsocu-
uation interface. The participants are shown the rious about the degree to which our automatic
semanticmapwiththepathaswellaspanorama results in SPICE correlate with the human judg-
images. They are asked to assign a score from ments. We measure a Kendall τ correlation be-
0 (worst) to 10 (best) based on the quality of the tweenSPICEandhumanevaluationresultsof0.6InputInformation P C Incorrect Hallucination Redundancy Linguistic Apart from changing the input information, when
TD - - 15 10 0 0 we train the model with prompting, the resulting
TD+Reg+Act - - 15 10 0 1
instructionsarelesslikelytoincludehallucinations
TD+Reg+Act ✓ - 12 6 1 2
in terms of actions and objects. Yet after intro-
TD+Reg+Act ✓ ✓ 12 6 1 2
TD+Reg+Act+Pano - - 11 6 0 0 ducingthecontrastiveloss,itcausesredundancy
andlinguisticproblemsinthepredictions. Thelan-
Table3: Erroranalysisonrandomlyselectedpre-
guagequalityproblemsmainlyconsistofspelling
dictionsfromthesystemsinunseenenvironments,
mistakesinobjectsandregions,andpunctuation
wherePisshortforpromptandCisshortforcon-
errorswhenintroducingthepromptandcontrastive
trastiveloss.
loss for training. This may be because the con-
trastivelossinfluencestheCrossEntropylossand
thusinterfereswiththelanguagegenerationtask.
andconcludethatthisissatisfactory,justifyingthe
useofSPICEforautomaticevaluation.3
5. Conclusion
Ourfindingsindicatethatincorporatingmoreinfor-
mationindifferentmodalitiestendstoimprovethe Ourlonger-termgoalistobuildmobilerobotswith
performanceforthegenerationtask. Oursemantic spatialawarenessandreasoningcapabilitieswhich
map abstracts information in a way that is useful can follow natural language instructions and ex-
forcurrentsystems,althoughitconsistsofonlya press their intentions in natural language. We
singleimage. Mostofoursystemvariantsthatdo propose to use semantic maps as the interme-
notusepanoramaimagesperformson-parwiththe diate representation for spatial reasoning as it is
existingLSTM-basedend-to-endapproachesthat ahuman-interpretableandlight-weightapproach
use only panoramic images. However, the abso- thatencodesinformationnecessaryforthenaviga-
luteperformanceofallmodelsisstilllow,indicating tioninasingleabstractimage.
thatthereismuchroomforimprovement.
Inthiswork,wecreatethedatasetwithtop-down
semantic maps for R2R corpus and reframe in-
4.5. Error Analysis structiongenerationtaskasimagecaptioning,us-
ingabstracttop-downsemanticmapasmaininput.
Further to human evaluation score, we manually
We set a baseline for the instruction generation
analyze the quality of the instructions generated
fromsemanticmapinput. Ourexperimentalresults
by the same 5 system variants according to the
showthatusingthetop-downsemanticmapper-
followingfouraspects:
formson-parwiththeend-to-endmethodsthatuse
• Incorrectness: Does the prediction contain sequenceofpanoramaimagesasinput.
incorrectinformation?
Limitations
• Hallucination: Does the prediction contain a
descriptionnotcorrespondingtotheinput?
Thecurrentapproachtothesemanticmaprepre-
• Redundancy: Doesthepredictioncontainre- sentation is missing some of the information re-
dundantexpressionsandinformation? quiredtogenerateorinterpretinstructions. Forex-
ample,roomnames,suchasbathroom,bedroom,
• Linguisticproblems: Isthegeneratedinstruc-
orsittingroom,arenaturallyusedinindoornaviga-
tiongrammaticallywrongornotfluent?
tioninstructions. However,thecurrentsingle-layer
Foreachexperimentalsetting,werandomlyselect semanticmaprepresentationdoesnotencodethe
15examples. Thecountsforeacherrortypeare informationaboutsuchregionnames. Toaddress
given in Table 3. We can see that the systems this in our current approach, we provide region
that do not use prompting or panorama images namesforeachnavigationpointasaseparatetex-
containerrorsinallcases. Mostoftheseerrorsare tualinput. Thelimitationofthisapproachisthatit
causedbyhallucinations. Analyzinghallucinations onlyincludestheregionnamesforthenavigation
further, we find that the action descriptions are points. Forexample,aninstruction‘Stopinfrontof
most prone to hallucinations, such as when left thebathroom’,thebathroomwillnotbeincludedin
and right are confused with each other. When the input because the navigation point is outside
regionsandactionsareusedasinput,thenumber of the bathroom region. In future work, we plan
ofhallucinationsinactiondescriptionsgoesdown, tointroduceamulti-layeredsemanticmapwhere,
butremainshighinregions. in addition to encoding objects, a separate layer
encodesinformationaboutregions.
3We also computed BLEU and ROUGE scores, Anotherlimitationisthatcurrentsemanticmapen-
however they show lower correlation with the human- codingdoesnotencodeobjectproperties,suchas
assignedscores,whichareomittedhere. color,material,orshape. Accordingtoouranaly-sis,objectpropertiesarementionedinone-thirdof Savva, Shuran Song, Andy Zeng, and Yinda
theinstructions,butthesewouldnotbecaptured Zhang.2017. Matterport3d: Learningfromrgb-
by the map. To address this limitation, in future d data in indoor environments. arXiv preprint
work, we will encode the object properties in the arXiv:1709.06158.
semanticmap.
DevendraSinghChaplot,DhirajGandhi,Saurabh
Data and Code availability Gupta,AbhinavGupta,andRuslanSalakhutdi-
nov.2020. Learningtoexploreusingactiveneu-
We release the code for data preparation, model ralslam. InInternationalConferenceonLearn-
training and inference, and evaluation at https: ingRepresentations(ICLR).
//github.com/chengzu-li/VLGen, along with the
HowardChen,AlaneSuhr,DipendraMisra,Noah
prompt templates and hyper-parameter settings
Snavely,andYoavArtzi.2019. Touchdown: Nat-
forexperiments. Wealsoreleasethethetop-down
urallanguagenavigationandspatialreasoning
semanticmapsextractedfromHabitatenvironment
invisualstreetenvironments. In2019IEEE/CVF
extendingtheexistingR2Rdataset,whichcanbe
Conference on Computer Vision and Pattern
obtained upon request following the guideline at
https://github.com/chengzu-li/VLGen. Recognition(CVPR),pages12530–12539.
Ta-Chung Chi, Minmin Shen, Mihail Eric,
Bibliographical References
Seokhwan Kim, and Dilek Hakkani-tur. 2020.
Justask: Aninteractivelearningframeworkfor
visionandlanguagenavigation. InProceedings
oftheAAAIConferenceonArtificialIntelligence,
DongAn,YuankaiQi,YangguangLi,YanHuang,
volume34,pages2459–2466.
Liang Wang, Tieniu Tan, and Jing Shao.
2023. Bevbert: Multimodal map pre-training JaeminCho,JieLei,HaoTan,andMohitBansal.
for language-guided navigation. Proceedings 2021. Unifying vision-and-language tasks via
of the IEEE/CVF International Conference on textgeneration. InInternationalConferenceon
ComputerVision. MachineLearning,pages1931–1942.PMLR.
Peter Anderson, Basura Fernando, Mark John- Chenhe Dong, Yinghui Li, Haifan Gong, Miaoxin
son,andStephenGould.2016. SPICE:Seman- Chen,JunxinLi,YingShen,andMinYang.2022.
tic propositional image caption evaluation. In Asurveyofnaturallanguagegeneration. ACM
ECCV. ComputingSurveys,55(8):1–38.
Peter Anderson, Qi Wu, Damien Teney, Jake
Vishnu Sashank Dorbala, Gunnar Sigurdsson,
Bruce, Mark Johnson, Niko Sünderhauf, Ian
Robinson Piramuthu, Jesse Thomason, and
Reid,StephenGould,andAntonvandenHen-
Gaurav S Sukhatme. 2022. Clip-nav: Using
gel.2018. Vision-and-languagenavigation: In-
clip for zero-shot vision-and-language naviga-
terpretingvisually-groundednavigationinstruc-
tion. arXivpreprintarXiv:2211.16649.
tions in real environments. In Proceedings of
the IEEE Conference on Computer Vision and WeixiFeng,Tsu-JuiFu,YujieLu,andWilliamYang
PatternRecognition(CVPR). Wang. 2022. Uln: Towards underspecified
vision-and-languagenavigation. arXivpreprint
Satanjeev Banerjee and Alon Lavie. 2005. ME-
arXiv:2210.10020.
TEOR: An automatic metric for MT evaluation
withimprovedcorrelationwithhumanjudgments. Daniel Fried, Ronghang Hu, Volkan Cirik, Anna
InProceedingsoftheACLWorkshoponIntrinsic Rohrbach, Jacob Andreas, Louis-Philippe
andExtrinsicEvaluationMeasuresforMachine Morency,TaylorBerg-Kirkpatrick,KateSaenko,
Translationand/orSummarization,pages65–72, Dan Klein, and Trevor Darrell. 2018. Speaker-
AnnArbor,Michigan.AssociationforComputa- followermodelsforvision-and-languagenaviga-
tionalLinguistics. tion. AdvancesinNeuralInformationProcessing
Systems,31.
TomBrown,BenjaminMann,NickRyder,Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Albert Gatt and Emiel Krahmer. 2017. Survey of
ArvindNeelakantan,PranavShyam,GirishSas- the state of the art in natural language gener-
try,AmandaAskell,etal.2020. Languagemod- ation: Core tasks, applications and evaluation.
els are few-shot learners. Advances in neural arXivpreprintarXiv:1703.09902.
informationprocessingsystems,33:1877–1901.
Jing Gu, Eliana Stefani, Qi Wu, Jesse Thoma-
Angel Chang, Angela Dai, Thomas Funkhouser, son,andXinWang.2022. Vision-and-language
Maciej Halber, Matthias Niessner, Manolis navigation: A survey of tasks, methods, andfuture directions. In Proceedings of the 60th fidelityinvision-and-languagenavigation. InPro-
Annual Meeting of the Association for Compu- ceedingsofthe57thAnnualMeetingoftheAs-
tational Linguistics (Volume 1: Long Papers), sociation for Computational Linguistics, pages
pages7606–7623,Dublin,Ireland.Association 1862–1872,Florence,Italy.AssociationforCom-
forComputationalLinguistics. putationalLinguistics.
DavidHall,BenTalbot,SumanRajBista,Haoyang Aishwarya Kamath, Peter Anderson, Su Wang,
Zhang, Rohan Smith, Feras Dayoub, and Niko JingYuKoh,AlexanderKu,AustinWaters,Yin-
Sünderhauf.2020. Theroboticvisionsceneun- fei Yang, Jason Baldridge, and Zarana Parekh.
derstandingchallenge. 2022. Anewpath: Scalingvision-and-language
navigationwithsyntheticinstructionsandimita-
XuHan,WeilinZhao,NingDing,ZhiyuanLiu,and
tionlearning. arXivpreprintarXiv:2210.03112.
Maosong Sun. 2022. Ptr: Prompt tuning with
rulesfortextclassification. AIOpen,3:182–192.
Matthias Keicher, Kamilia Mullakaeva, Tobias
Czempiel,KristinaMach,AshkanKhakzar,and
KaimingHe,XiangyuZhang,ShaoqingRen,and
Nassir Navab. 2022. Few-shot structured radi-
JianSun.2016.Deepresiduallearningforimage
ologyreportgenerationusingnaturallanguage
recognition. InProceedingsoftheIEEEconfer-
prompts. arXivpreprintarXiv:2203.15723.
enceoncomputervisionandpatternrecognition,
pages770–778.
MauriceGKendall.1938. Anewmeasureofrank
Karl Moritz Hermann, Mateusz Malinowski, Piotr correlation. Biometrika,30(1/2):81–93.
Mirowski, Andras Banki-Horvath, Keith Ander-
son,andRaiaHadsell.2020. Learningtofollow Diederik P Kingma and Jimmy Ba. 2014. Adam:
directionsinstreetview. InProceedingsofthe A method for stochastic optimization. arXiv
AAAI Conference on Artificial Intelligence, vol- preprintarXiv:1412.6980.
ume34,pages11773–11781.
ThomasKollar,JayantKrishnamurthy,andGrantP
Jack Hessel, Ari Holtzman, Maxwell Forbes, Ro- Strimel.2013. Towardinteractivegroundedlan-
nanLeBras,andYejinChoi.2021. Clipscore: A guage acqusition. In Robotics: Science and
reference-freeevaluationmetricforimagecap- systems,volume1,pages721–732.
tioning. arXivpreprintarXiv:2104.08718.
Jacob Krantz, Erik Wijmans, Arjun Majumdar,
Yicong Hong, Cristian Rodriguez-Opazo, Qi Wu, Dhruv Batra, and Stefan Lee. 2020. Beyond
and Stephen Gould. 2020a. Sub-instruction thenav-graph: Vision-and-languagenavigation
aware vision-and-language navigation. arXiv incontinuousenvironments.InComputerVision–
preprintarXiv:2004.02707. ECCV2020: 16thEuropeanConference,Glas-
gow, UK, August 23–28, 2020, Proceedings,
Yicong Hong, Qi Wu, Yuankai Qi, Cristian
PartXXVIII16,pages104–120.Springer.
Rodriguez-Opazo, and Stephen Gould. 2020b.
Arecurrentvision-and-languagebertfornaviga-
Alexander Ku, Peter Anderson, Roma Patel, Eu-
tion. arXivpreprintarXiv:2011.13922.
gene Ie, and Jason Baldridge. 2020. Room-
across-room: Multilingualvision-and-language
Yicong Hong, Qi Wu, Yuankai Qi, Cristian
navigationwithdensespatiotemporalgrounding.
Rodriguez-Opazo,andStephenGould.2021. A
arXivpreprintarXiv:2010.07954.
recurrent vision-and-language bert for naviga-
tion. In Proceedings of the IEEE/CVF Confer-
JunnanLi,DongxuLi,CaimingXiong,andSteven
enceonComputerVisionandPatternRecogni-
Hoi.2022. Blip: Bootstrappinglanguage-image
tion(CVPR),pages1643–1653.
pre-training for unified vision-language under-
Nikolai Ilinykh, Yasmeen Emampoor, and Simon standingandgeneration. InInternationalConfer-
Dobnik. 2022. Look and answer the question: enceonMachineLearning,pages12888–12900.
On the role of vision in embodied question an- PMLR.
swering. InProceedingsofthe15thInternational
Conference on Natural Language Generation, Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui
pages 236–245, Waterville, Maine, USA and Hsieh,andKai-WeiChang.2019. Visualbert: A
virtual meeting. Association for Computational simple and performant baseline for vision and
Linguistics. language. arXivpreprintarXiv:1908.03557.
Vihan Jain, Gabriel Magalhaes, Alexander Ku, XiangLisaLiandPercyLiang.2021. Prefix-tuning:
Ashish Vaswani, Eugene Ie, and Jason Optimizing continuous prompts for generation.
Baldridge. 2019. Stay on the path: Instruction arXivpreprintarXiv:2101.00190.XiujunLi,XiYin,ChunyuanLi,PengchuanZhang, Problems beyond language grounding. ArXiv,
XiaoweiHu,LeiZhang,LijuanWang,Houdong abs/2110.04441.
Hu, Li Dong, Furu Wei, et al. 2020. Oscar:
TzufPaz-Argaman,JohnPalowitch,SayaliKulka-
Object-semanticsalignedpre-trainingforvision-
rni, Jason Baldridge, and Reut Tsarfaty. 2024.
language tasks. In Computer Vision–ECCV
Where do we go from here? multi-scale allo-
2020: 16thEuropeanConference,Glasgow,UK,
centric relational inferencefrom natural spatial
August23–28,2020,Proceedings,PartXXX16,
descriptions. InProceedingsofthe18thConfer-
pages121–137.Springer.
enceoftheEuropeanChapteroftheAssociation
Chin-YewLin.2004. ROUGE:Apackageforauto- forComputationalLinguistics(Volume1: Long
maticevaluationofsummaries. InTextSumma- Papers),pages1026–1040,St.Julian’s,Malta.
rizationBranchesOut,pages74–81,Barcelona, AssociationforComputationalLinguistics.
Spain.AssociationforComputationalLinguistics.
Alec Radford, Jong Wook Kim, Chris Hallacy,
JunyangLin,AnYang,YichangZhang,JieLiu,Jin- AdityaRamesh,GabrielGoh,SandhiniAgarwal,
grenZhou,andHongxiaYang.2020. Interbert: GirishSastry,AmandaAskell,PamelaMishkin,
Vision-and-languageinteractionformulti-modal Jack Clark, et al. 2021. Learning transferable
pretraining. arXivpreprintarXiv:2003.13198. visualmodelsfromnaturallanguagesupervision.
InInternationalconferenceonmachinelearning,
Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao pages8748–8763.PMLR.
Jiang, Hiroaki Hayashi, and Graham Neubig.
2023. Pre-train, prompt, and predict: A sys- Colin Raffel, Noam Shazeer, Adam Roberts,
tematicsurveyofpromptingmethodsinnatural KatherineLee,SharanNarang,MichaelMatena,
languageprocessing. ACMComputingSurveys, YanqiZhou,WeiLi,andPeterJLiu.2020.Explor-
55(9):1–35. ing the limits of transfer learning with a unified
text-to-texttransformer. TheJournalofMachine
XiaoLiu,KaixuanJi,YichengFu,WengLamTam, LearningResearch,21(1):5485–5551.
ZhengxiaoDu,ZhilinYang,andJieTang.2021.
P-tuning v2: Prompt tuning can be compara- LariaReynoldsandKyleMcDonell.2021. Prompt
bletofine-tuninguniversallyacrossscalesand programming for large language models: Be-
tasks. arXivpreprintarXiv:2110.07602. yond the few-shot paradigm. In Extended Ab-
stractsofthe2021CHIConferenceonHuman
Huaishao Luo, Lei Ji, Ming Zhong, Yang Chen, FactorsinComputingSystems,pages1–7.
Wen Lei, Nan Duan, and Tianrui Li. 2022.
Clip4clip: An empirical study of clip for end to Manolis Savva, Abhishek Kadian, Oleksandr
end video clip retrieval and captioning. Neuro- Maksymets, Yili Zhao, Erik Wijmans, Bhavana
computing,508:293–304. Jain, Julian Straub, Jia Liu, Vladlen Koltun, Ji-
tendra Malik, et al. 2019. Habitat: A platform
Dipendra Misra, Andrew Bennett, Valts Blukis, forembodiedairesearch. InProceedingsofthe
EyvindNiklasson,MaxShatkhin,andYoavArtzi. IEEE/CVFinternationalconferenceoncomputer
2018. Mapping instructions to actions in 3D vision,pages9339–9347.
environmentswithvisualgoalprediction. InPro-
ceedingsofthe2018ConferenceonEmpirical Sheng Shen, Liunian Harold Li, Hao Tan, Mohit
MethodsinNaturalLanguageProcessing,pages Bansal,AnnaRohrbach,Kai-WeiChang,Zhewei
2667–2678,Brussels,Belgium.Associationfor Yao, and Kurt Keutzer. 2021. How much can
ComputationalLinguistics. clip benefit vision-and-language tasks? arXiv
preprintarXiv:2107.06383.
OpenAI. 2023. Gpt-4 technical report. ArXiv,
abs/2303.08774. MohitShridhar,JesseThomason,DanielGordon,
YonatanBisk,WinsonHan,RoozbehMottaghi,
KishorePapineni,SalimRoukos,ToddWard,and Luke Zettlemoyer, and Dieter Fox. 2020. AL-
Wei-Jing Zhu. 2002. BLEU: a method for au- FRED:ABenchmarkforInterpretingGrounded
tomatic evaluation of machine translation. In Instructions for Everyday Tasks. In The IEEE
Proceedingsofthe40thAnnualMeetingofthe Conference on Computer Vision and Pattern
AssociationforComputationalLinguistics,pages Recognition(CVPR).
311–318,Philadelphia,Pennsylvania,USA.As-
Xuemeng Song, Liqiang Jing, Dengtian Lin,
sociationforComputationalLinguistics.
Zhongzhou Zhao, Haiqing Chen, and Liqiang
SethPate,WeiXu,ZiyiYang,MaxwellLove,Sid- Nie. 2022. V2P: Vision-to-prompt based multi-
darth Ganguri, and Lawson L. S. Wong. 2021. modalproductsummarygeneration. InProceed-
Naturallanguageforhuman-robotcollaboration: ingsofthe45thInternationalACMSIGIRCon-ference on Research and Development in In- PengWang,AnYang,RuiMen,JunyangLin,Shuai
formationRetrieval,SIGIR’22,page992–1001, Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jin-
NewYork,NY,USA.AssociationforComputing gren Zhou, and Hongxia Yang. 2022b. Ofa:
Machinery. Unifying architectures, tasks, and modalities
throughasimplesequence-to-sequencelearn-
XiuchaoSui,ShaohuaLi,HongYang,Hongyuan
ing framework. In International Conferenceon
Zhu,andYanWu.2023. Languagemodelscan
MachineLearning,pages23318–23340.PMLR.
dozero-shotvisualreferringexpressioncompre-
hension. InInternationalConferenceonLearn- PengWang,AnYang,RuiMen,JunyangLin,Shuai
ingRepresentations(ICLR). Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jin-
grenZhou,andHongxiaYang.2022c. Unifying
HaoTanandMohitBansal.2019. Lxmert: Learn- architectures, tasks, and modalities through a
ingcross-modalityencoderrepresentationsfrom simple sequence-to-sequence learning frame-
transformers. arXivpreprintarXiv:1908.07490. work. arXivpreprintarXiv:2202.03052.
Hao Tan, Licheng Yu, and Mohit Bansal. 2019. Su Wang, Ceslee Montgomery, Jordi Orbay,
Learning to navigate unseen environments: VighneshBirodkar,AleksandraFaust,Izzeddin
Backtranslationwithenvironmentaldropout. In Gur, Natasha Jaques, Austin Waters, Jason
Proceedings of the 2019 Conference of the Baldridge,andPeterAnderson.2022d. Lessis
NorthAmericanChapteroftheAssociationfor more: Generatinggroundednavigationinstruc-
Computational Linguistics: Human Language tions from landmarks. In Proceedings of the
Technologies, Volume 1 (Long and Short Pa- IEEE/CVFConferenceonComputerVisionand
pers), pages 2610–2621, Minneapolis, Min- PatternRecognition,pages15428–15438.
nesota. Associationfor ComputationalLinguis-
Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang
tics.
Dai, Yulia Tsvetkov, and Yuan Cao. 2021.
Mingkang Tang, Zhanyu Wang, Zhenhua Liu, Simvlm: Simple visual language model pre-
Fengyun Rao, Dian Li, and Xiu Li. 2021. training with weak supervision. arXiv preprint
Clip4caption: Clipforvideocaption. InProceed- arXiv:2108.10904.
ingsofthe29thACMInternationalConference
ThomasWolf,LysandreDebut,VictorSanh,Julien
onMultimedia,pages4858–4862.
Chaumond, Clement Delangue, Anthony Moi,
TianyiTang,JunyiLi,WayneXinZhao,andJi-Rong Pierric Cistac, Tim Rault, Rémi Louf, Morgan
Wen.2022. Context-tuning: Learningcontextu- Funtowicz,etal.2019. Huggingface’stransform-
alizedpromptsfornaturallanguagegeneration. ers: State-of-the-artnaturallanguageprocess-
arXivpreprintarXiv:2201.08670. ing. arXivpreprintarXiv:1910.03771.
Wansen Wu, Tao Chang, and Xinmeng Li. 2021.
YoadTewel,YoavShalev,IdanSchwartz,andLior
Visual-and-language navigation: A survey and
Wolf.2021. Zero-shotimage-to-textgeneration
taxonomy. arXivpreprintarXiv:2108.11544.
for visual-semantic arithmetic. arXiv preprint
arXiv:2111.14447.
Teng Xue, Weiming Wang, Jin Ma, Wenhai Liu,
ZhenyuPan,andMingshuoHan.2020.Progress
Ashish Vaswani, Noam Shazeer, Niki Parmar,
and prospects of multimodal fusion methods
JakobUszkoreit, LlionJones, AidanNGomez,
in physical human–robot interaction: A review.
ŁukaszKaiser,andIlliaPolosukhin.2017. Atten-
IEEESensorsJournal,20(18):10355–10370.
tionisallyouneed. Advancesinneuralinforma-
tionprocessingsystems,30. Jianing Yang, Xuweiyi Chen, Shengyi Qian,
Nikhil Madaan, Madhavan Iyengar, David F.
RamakrishnaVedantam,CLawrenceZitnick,and
Fouhey,andJoyceChai.2023. LLM-Grounder:
Devi Parikh. 2015. Cider: Consensus-based
Open-vocabulary 3d visual grounding with
image description evaluation. In Proceedings
large language model as an agent. ArXiv,
oftheIEEEconferenceoncomputervisionand
abs/2309.12311.
patternrecognition,pages4566–4575.
TianYun,ChenSun,andElliePavlick.2021. Does
Hanqing Wang, Wei Liang, Jianbing Shen, Luc
vision-and-languagepretrainingimprovelexical
VanGool,andWenguanWang.2022a. Counter-
grounding? arXivpreprintarXiv:2109.10246.
factualcycle-consistentlearningforinstruction
followingandgenerationinvision-languagenav- Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jian-
igation. InProceedingsoftheIEEE/CVFconfer- wei Yang, Lei Zhang, Lijuan Wang, Yejin Choi,
enceoncomputervisionandpatternrecognition, and Jianfeng Gao. 2021. Vinvl: Revisiting vi-
pages15471–15481. sualrepresentationsinvision-languagemodels.In Proceedings of the IEEE/CVF Conference For the buildings with multiple floors, we extract
on Computer Vision and Pattern Recognition, a semantic map for each floor. Given the 3D co-
pages5579–5588. ordinatesoftheobject’scenter(x ,y ,h )andthe
i i i
size of the object’s bounding box is (w ,w ,w ),
x y h
TianyiZhang,VarshaKishore,FelixWu,KilianQ
weusetheagent’sverticalpositionh tofilter
agent
Weinberger, and Yoav Artzi. 2019. Bertscore:
theobjectsforagivenfloorbyincludingallobjects
Evaluating text generation with bert. arXiv
thatsatisfyoneofthefollowing:
preprintarXiv:1904.09675.
1 1
MingZhao,PeterAnderson,VihanJain,SuWang, h i− 2w h ≤h agent ≤h i+ 2w h
Alexander Ku, Jason Baldridge, and Eugene
Ie. 2021. On the evaluation of vision-and- |h −h |≤1.6
i agent
languagenavigationinstructions. arXivpreprint
arXiv:2101.10504. Regions For each navigation point, we deter-
mine the corresponding region by calculating
LuoweiZhou,HamidPalangi,LeiZhang,Houdong
whether the agent’s current position is within the
Hu,JasonCorso,andJianfengGao.2020. Uni-
areaoftheregion. Theregion’sareaisdefinedby
fiedvision-languagepre-trainingforimagecap-
thecoordinatesofthecenter(x ,y )andthesizes
tioning and vqa. In Proceedings of the AAAI c c
inwidthandlength(w ,w )asarectangle. Wede-
conferenceonartificialintelligence,volume34, x y
finethatiftheagent’slocation(l ,l )satisfiesthe
pages13041–13049. x y
followingrequirement,theregionwouldbeadded
Shuyan Zhou, Uri Alon, Frank F Xu, Zhengbao intotheinformationofthisnavigationpoint.
JIang, and Graham Neubig. 2022. Doccoder:
Generatingcodebyretrievingandreadingdocs. 1 1
x − w ≤l ≤x + w
arXivpreprintarXiv:2207.05987. c 2 x x c 2 x
1 1
Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, y − w ≤l ≤y + w
c 2 y y c 2 y
andMohamedElhoseiny.2023. Minigpt-4: En-
hancingvision-languageunderstandingwithad-
Actions The actions are a closed set of LEFT,
vanced large language models. arXiv preprint
RIGHT, STRAIGHT, STOP. They are determined
arXiv:2304.10592.
basedonthecoordinationofthenavigationpoints.
Zhuofan Zong, Guanglu Song, and Yu Liu. 2022. Wecalculatethedifferencesinanglesbetweenthe
DETRs with collaborative hybrid assignments previousnavigationpointandthecurrentposition
training. 2023 IEEE/CVF International Confer- and define that if the differences stay within 20
enceonComputerVision(ICCV),pages6725– degrees,theagentisheadingstraight. Otherwise,
6735. the agent makes a turn, with the corresponding
direction depending on whether the difference is
A. Data Extraction positiveornegative.
A.1. Conditions of Data Extraction A.2. More Information about the
Extracted Data
This section describes how the semantic map is
extractedfromtheHabitatenvironment. Toevaluatethequalityofthedataset,werandomly
sample30examplesfromthetrainingsetandlook
Objects Objects on a semantic map are repre- into the instructions regarding the way they de-
sented by the bounding box with a unique color scribethepath. wefindthattheinstructionsmen-
assigned toeach objecttype. We usethe (X,Y) tion 2.2 landmark objects and 1.3 regions on av-
coordinates of the object’s bounding box in Mat- erage. 20 out of 30 sampled instructions can be
terplot3D to represent them in the 2D semantic inferredsimplyfromthetop-downview. Thisfind-
map. ingjustifiestheuseoftop-downsemanticmapsfor
navigationinstructiongenerationtosomedegree.
Thereare40differentobjecttypeslabeledinthe
Fortheother10instructions,7outof10cannotbe
simulationenvironment. Wefilteroutthefollowing
inferredonlyfromthetop-downviewduetothede-
objecttypesfromthesimulatorbecausetheyare
scriptiveexpressionsabouttheenvironment. The
seldommentionedintheinstructionsbuttakeupa
descriptionscanbeobtainedfromthepanoramic
largeareainthesemanticmap:
imagesassupportiveinformation(suchasgoing
['misc', 'ceiling', 'curtain', upstairs or downstairs) and requires the interac-
'objects', 'floor', 'wall', tionsbetweendifferenttypesofinput. Theother3
'void'] instructionsmisstheregionannotationsfromthesimulatorinR2R.Theseobservationsindicatethat B. Experimental Setup
thenewtaskweproposehastheproblemsofweak
supervisionandrequiresthemodeltoconnectdif- B.1. Hyperparameters
ferenttypesofinputswitheachother.
We train our model for a maximum of 25 epochs
using an initial learning rate of 5e-5 with linear lr
scheduler. Thebatchsizeissetto32fortraining
A.3. Alignment between Colors and
and 64 for validation. When balancing the con-
Objects
trastivelossandCrossEntropyloss, weassigna
BelowshowsthemappingbetweenRGBpixelval- weight of 0.1 to the contrastive loss to make the
ues and the object types (textual names) in the modelmorefocusedonthegenerationtask.
semanticmap.
B.2. Data Preprocessing
[31, 119, 180], "void",
[174, 199, 232], "wall", We mainly adopt the original BLIP processor for
[255, 127, 14], "floor", imageandtextinputs,butmakeafewmodifications
[255, 187, 120], "chair", tothetop-downsemanticmap. Becausethemaps
[44, 160, 44], "door", are in different sizes for different rooms, we first
[152, 223, 138], "table", padthemtothesizeof1024×1024withblackpixels
[214, 39, 40], "picture", andapplyamaskingstrategytothetop-downmap
[255, 152, 150], "cabinet", by only selecting the nearby regions of the path.
[148, 103, 189], "cushion", Wekeepthereceptivefieldofthetop-downviewto
[197, 176, 213], "window", acertainvalue(defaultto40pixels)withinthepath
[140, 86, 75], "sofa", area. Then,inordertoavoidintroducingnewpixel
[196, 156, 148], "bed", valueswhenresizing,weresizethemaskedimage
[227, 119, 194], "curtain", withthenearestresamplinginterpolationstrategy
[247, 182, 210], "chest_of_drawers", to386×386,followingthedefaultsettingofBLIP.
[127, 127, 127], "plant",
[199, 199, 199], "sink", C. Prompt Design
[188, 189, 34], "stairs",
prompt : Starting from the dark
[219, 219, 141], "ceiling",
yellow point [objects]
[23, 190, 207], "toilet",
[regions], [instruction]
[158, 218, 229], "stool",
For example:
[57, 59, 121], "towel",
[objects] : near sofa cushion
[82, 84, 163], "mirror",
[regions] : in the living room region
[107, 110, 207], "tv_monitor",
[instruction]: exit the living room, turn
[156, 158, 222], "shower",
left, wait at the bottom
[99, 121, 57], "column",
of the stairs.
[140, 162, 82], "bathtub",
[181, 207, 107], "counter",
D. Experiment Results
[206, 219, 156], "fireplace",
[140, 109, 49], "lighting",
[189, 158, 57], "beam", D.1. Significance Test Results on SPICE
[231, 186, 82], "railing", Scores
[231, 203, 148], "shelving",
We first define the indices for all of our system
[132, 60, 57], "blinds",
variantsfrom1to9followingtheorderofsystems
[173, 73, 74], "gym_equipment",
inTable2. Table4showsthefullsignificancetest
[214, 97, 107], "seating",
onSPICEscoresinunseenenvironments.
[231, 150, 156], "board_panel",
[123, 65, 115], "furniture",
D.2. Human Evaluations
[165, 81, 148], "appliances",
[206, 109, 189], "clothes",
Fortheevaluationpage,itshowsthetop-downse-
[222, 158, 214], "objects",
manticmap,thepanoramicimagesandtheregion
[255, 255, 102], "[POINT]",
information as well. The page provides 5 gener-
[255, 255, 0], "[START]",
ated instructions to describe the navigation path
[255, 255, 204], "[END]",
from5differentgeneratormodels. Theevaluator
[255, 255, 255], "[LINE]",
issupposedtogivethequalityscoresforthese5
[0, 0, 0], "[NONNAVIGABLE]",
instructionsbasedontheguidanceintheinstruc-
[150, 0, 0], "[NAVIGABLE]"
tiondocumentation. Figure3showsascreenshot2(15.77) 3(17.10) 4(17.00) 5(17.84) 6(17.09) 7(17.44) 8(17.79) 9(17.08)
1(16.19) 0.4421 0.7891 0.1554 0.0030 0.1050 0.0269 0.0055 0.1115
2(15.77) 0.2992 0.0310 0.0002 0.0175 0.0033 0.0004 0.0194
3(17.10) 0.2432 0.0072 0.1726 0.0505 0.0113 0.1825
4(17.00) 0.1459 0.8774 0.4536 0.1825 0.8890
5(17.84) 0.1822 0.4809 0.9318 0.1830
6(17.09) 0.5423 0.2256 0.9913
7(17.44) 0.5463 0.5401
8(17.79) 0.2271
Table 4: Two-sided permutation test p-values on
SPICEinvalidationunseenenvironments. Therow
namesandcolumnnamesarethesystemindices
fordifferentsystems,withtheSPICEvaluesinthe
parenthesisbrackets. Thenumbersinboldarethe
p-valuesbelow0.05.
oftheinterfaceforhumanevaluation.
Significance test based on human evaluation
Table 5 presents the two-sided permutation test
resultsbasedonthehumanevaluation.
4(4.20) 5(4.29) 6(3.98) 7(4.36)
1(3.42) 0.10 0.06 0.27 0.05
4(4.20) 0.88 0.68 0.77
5(4.29) 0.55 0.92
6(3.98) 0.47
Table 5: Two-sided permutation test results be-
tween systems based on the human evaluation.
Therowindicesandcolumnindicesarethesystem
indicesfollowingTable4andtheirqualityscoresin
theparenthesis.Figure3: Screenshotoftheevaluationinterfaceforhumanevaluation.