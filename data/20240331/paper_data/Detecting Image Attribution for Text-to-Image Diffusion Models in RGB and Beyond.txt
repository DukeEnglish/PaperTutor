Detecting Image Attribution for Text-to-Image
Diffusion Models in RGB and Beyond
Katherine Xu1, Lingzhi Zhang2, and Jianbo Shi1
1 University of Pennsylvania
2 Adobe Inc.
https://github.com/k8xu/ImageAttribution
Abstract. Modern text-to-image (T2I) diffusion models can generate
imageswithremarkablerealismandcreativity.Theseadvancementshave
sparkedresearchinfakeimagedetectionandattribution,yetpriorstud-
ieshavenotfullyexploredthepracticalandscientificdimensionsofthis
task.Inadditiontoattributingimagesto12state-of-the-artT2Igenera-
tors,weprovideextensiveanalysesonwhatinferencestagehyperparam-
eters and image modifications are discernible. Our experiments reveal
that initialization seeds are highly detectable, along with other subtle
variations in the image generation process to some extent. We further
investigatewhatvisualtracesareleveragedinimageattributionbyper-
turbing high-frequency details and employing mid-level representations
of image style and structure. Notably, altering high-frequency informa-
tion causes only slight reductions in accuracy, and training an attribu-
tor on style representations outperforms training on RGB images. Our
analyses underscore that fake images are detectable and attributable at
various levels of visual granularity than previously explored.
Keywords: Generative Models · Image Attribution · Image Forensics
1 Introduction
In recent years, the emergence of advanced text-to-image (T2I) diffusion mod-
els [7,46,53,55,57,58,61,63] has markedly transformed the landscape of image
generation.Theseadvancementsenablethecreationofhighlyrealisticandimag-
inativevisualcontentdirectlyfromtextualdescriptions,heraldingnewpossibili-
tiesforcreativeexpressionandpracticalapplications.However,thisprogressalso
introducessignificantchallengesindifferentiatingrealimagesfromAI-generated
imagesandaccuratelyidentifyingtheirorigins.Addressingthesechallengesisvi-
tal for copyright enforcement, digital forensics, and maintaining the integrity of
visual content across digital platforms.
Previousstudies[5,8,11,39,68,72,81]haveprimarilyfocusedondifferentiating
AI-generatedimagesfromrealones,withsomeresearchextendingtotheattribu-
tionofimagestotheirsourcegenerators,notablyinGANvariants[10,29,48,76]
and diffusion models [17,32,66]. Yet, these investigations have largely been con-
ducted using generative models that may not reflect the latest advancements in
the field. Moreover, these studies have not fully explored the broader, practical,
and scientific dimensions of these tasks, which we aim to further examine.
4202
raM
82
]VC.sc[
1v35691.3042:viXra2 Katherine Xu, Lingzhi Zhang, Jianbo Shi
Asa firststep,our workunifies "real vs.fake" classificationandimage attri-
butionintoasingletaskbysimplytreatingrealimagesasanadditionalcategory.
Weexpandtheanalysistoincludeacomprehensiverangeofstate-of-the-artT2I
diffusion models, as of March 2024. This includes Stable Diffusion (SD) 1.5 [61],
SD2.0[61],SDXL[55],SDXLTurbo[63],LatentConsistencyModel(LCM)[46],
Stable Cascade [53], Kandinsky 2.1 [58], DALL-E 2 [57], DALL-E 3 [7], along
with Midjourney versions 5.2 and 6 [1]. To encompass a wide range of visual
concepts, we utilize 5,000 captions from MS-COCO [43] for natural scenes and
employ GPT-4 [4] to generate another 5,000 creative and surreal prompts. For
eachprompt,wegeneratemultipleimagesfromeachmodel,amassingnearlyhalf
a million AI-generated image dataset to train our image attributor, where the
details are discussed in Sec. 3. Regarding performance, our top-performing at-
tributorreachesanaccuracyexceeding90%,significantlysurpassingthebaseline
random chance of merely 7.69%, as detailed in Sec. 4.1.
Movingbeyondpreviousresearchthatfocusedonattributingimagestotheir
originating generators, our study probes further into whether nuanced changes
in hyperparameters during the inference phase of the same T2I diffusion model
can be identified. We examine hyperparameters including model checkpoints
at different training iterations, scheduler types, the number of sampling steps,
and initialization seeds. A significant finding from our experiments is the abil-
ity to distinguish between initialization seeds with 98%+ accuracy, employing
ten unique seeds for image generation within a consistent generator framework.
Whiletheaccuracyinidentifyingotherhyperparametersdoesn’treachtheexcep-
tional levels observed with initialization seeds, they all notably exceed random
chance. This suggests that even subtle variations in the generation process can
indeed be discerned to some extent. More details are discussed in Sec. 4.2.
In the workflow involving AI-generated images, users often enhance these
imagesfurtherbyimportingthemintoadditionalsoftwareormodelsforregional
editing via SDXL Inpainting [55] or Photoshop Generative Fill (Ps GenFill)
[2], or employing tools like Magnific AI [3] for texture enhancement at higher
resolutions.Thisraisesanessentialquestion:Canwestilltracethesepost-edited
images back to their original generators, and to what extent is this feasible? In
Sec. 4.3, we mimic user-driven regional editing using SDXL Inpainting and Ps
GenFill, alongside utilizing Magnific AI on a selected group of test images. Our
discussionthoroughlyexaminesandprovidesinsightsintohowthesepost-editing
interventions impact the image attribution performance.
Lastly, while prior research has demonstrated notable success in differentiat-
ing "real vs. fake" images and accurately attributing them to their origins, the
exactnatureofthedetectabletracesrecognizedbyclassifiersandtheirlocations
within the images remain elusive. In Sec. 5, we delve deeper into this scientific
question by introducing perturbations in the high-frequency domain and con-
verting images into various mid-level representations, such as depth maps and
Canny edges, to assess their impact on image attribution accuracy. This strat-
egyaimstounearthdetectabletracesacrossdifferentlevelsofvisualgranularity,
enriching our understanding of how classifiers recognize and attribute images.Detecting Image Attribution for T2I Diffusion Models in RGB and Beyond 3
Notably, our investigations reveal that training the image attributor using style
representations—specifically,theGrammatrix—enhancesaccuracybeyondwhat
is achievable with attributors trained on original RGB images. Furthermore, in-
troducing perturbations to high-frequency signals within images results in only
minor performance decreases in the attributors. When these models are trained
on mid-level representations, they maintain commendable accuracy levels that
significantly surpass random chance. This observation suggests that detectable
traces extend beyond just the high-frequency domain, encompassing mid-level
aspects of texture, structure, and potentially the layout of images.
Overall, our key contributions are as follows:
·
Developed an extensive dataset of nearly half a million AI-generated images
fromcutting-edgeT2Imodelswithavarietyofnaturalandsurrealprompts.
·
Achieved over 90% accuracy in training an image attributor across 12 con-
temporary T2I generators and real images, significantly outperforming ran-
dom chance for the 13-way classification task.
·
Pioneered the exploration of detectability regarding minor hyperparameter
modifications during the inference stage of T2I diffusion models.
·
InnovativelyreplicatedusereditingworkflowsonAI-generatedimagesusing
various tools, thoroughly evaluating their effect on attribution accuracy.
·
Introduced a novel approach for analyzing detectable traces within images
through high-frequency perturbations and conversion to diverse mid-level
representations, yielding significant insights.
2 Related Work
ClassifyingFakevs.RealImages.Theriseofsophisticatedimagegenerators
facilitates creating highly realistic images with diverse artistic styles, which has
spurred research aimed at detecting synthetic images from real images. Wang et
al. [72] introduced a CNN model that identifies images generated by GANs [30]
andlow-levelvisionmodels[12,19].Theyshowedthattrainingdiversityiscrucial
for fake image detectors to achieve good generalization. Additionally, Yu et al.
[76]discoveredthatdifferentGANarchitectures,trainingsets,andinitialization
seeds lead to distinct fingerprints in the generated images.
Various approaches detect synthetic images using visible [49] and invisible
artifacts that can lie in the spatial or frequency domain [31]. Spatial domain
methods often estimate these digital fingerprints using deep learning methods
[8,68] or by averaging their noise residuals [47]. These detection methods may
use local image patches [11], combine local and global image features [39], or
use gradients extracted by a pretrained CNN [70]. Moreover, style and texture
information have been utilized for fake image detection [5,44,81]. Amoroso et
al. revealed that real and fake images are more easily separable using style
featuresratherthansemantics[5],andZhongetal.[81]founditmorechallenging
for generative models to synthesize rich texture regions. Another line of work
suggests that GAN-generated images can be detected by studying artifacts in
the frequency domain [6,8,16,20,22,23,25,47,60,71].
Recently, there is a trend towards identifying images generated by diffu-
sion models [9,24,34,66,73,79,82]. Wang et al. [73] discovered that features of4 Katherine Xu, Lingzhi Zhang, Jianbo Shi
diffusion-generated images are more easily reconstructed by pretrained diffusion
models than real images, and Cozzolino et al. [18] observed that images from
diffusionmodelshavespectralpeaksthatdistinguishthemfromrealimages.Fur-
thermore, the idea of learning classifiers that leverage both visual and language
features to supplement low-level features has gained interest [18,51,74]. Ojha et
al. [51] found that using the feature space of CLIP [56] improves generalization
ability for detecting fake images from GANs and diffusion models.
Detecting Fake Image Attribution. In addition to recognizing synthetic
images, some approaches strive to identify the source of generated images. Yu
et al.[76]discoveredthatdifferentGANarchitectures,trainingsets,andinitial-
ization seeds can lead to fingerprint features for attribution. RepMix [10] traces
GAN images to their generators while being invariant to semantic content and
image perturbations. Girish et al. [29] and Marra et al. [48] developed an al-
gorithm for online detection and attribution of GAN images. Recent work has
also explored fake image detection and attribution from diffusion models [17]
andT2Igenerationmodels[66].Guarneraet al.[32]proposedahierarchicalap-
proach to categorize images into real or fake, GAN or diffusion-generated, and
the specific generator. Guo et al. [33] takes a similar approach, but they also
determine whether the image was entirely synthesized or only partially edited.
Analyzing Images Generated by Diffusion Models. There have been
worksstudyingsceneknowledgewithinpretraineddiffusionmodels[14,21,78],as
well as methods for examining the geometry of diffusion-generated images [62].
Du et al. [21] discovered that generative models contain rich information about
scene intrinsics, and they train a low-rank adapter [36] to produce surface nor-
mals, shading, albedo, and depth. Sarkar et al. [62] revealed that synthetic im-
agescanbedifferentiatedfromrealonesbyanalyzingtheirgeometricproperties.
In contrast, our work extends beyond prior research by delving deeper into
the specific inference stage hyperparameters, image modifications, and levels of
visual granularity that are discernible by an image attributor.
3 Dataset Generation
In this work, our objective is to detect and comprehend image attributions for
contemporarytext-to-image(T2I)models,whilealsoinvestigatingtheextentto
which traces can be detected across different generators and within the nuanced
variations of inference stage controls. To achieve this, we first generate images
using a variety of modern T2I models, employing a wide range of text prompts
to ensure diversity. Subsequently, we maintain a consistent generator while ad-
justing inference time hyperparameters, which include the number of inference
steps, scheduler types, model checkpoints, and random seeds.
3.1 Images from Diverse Generators and Prompts
As of March 2024, we have employed the following state-of-the-art, open-source
T2I models for image generation: SD 1.5 [61], SD 2.0 [61], SDXL [55], SDXL
Turbo[63],LatentConsistencyModel(LCM)[46],StableCascade[53],Kandin-
sky 2 [58], DALL-E 2 [57], DALL-E 3 [7], along with Midjourney versions 5.2
and 6 [1]. To generate images, we use the OpenAI API for DALL-E 2 and 3,Detecting Image Attribution for T2I Diffusion Models in RGB and Beyond 5
an automation bot for Midjourney 5.2 and 6, and the Hugging Face diffusers
GitHub repository [54] for the remaining models.
Fig.1: Adepictionofimagesgeneratedforourdataset,showcasingtwodistincttypes
of prompts: MS-COCO [43] derived captions are displayed at the top, while creative
prompts generated by GPT-4 are featured at the bottom. For both categories, images
were produced using 12 different T2I generators.
To gather a broad spectrum of text prompts, we include both descriptions
of natural scenes and imaginative, surreal prompts. This diversity is achieved
by leveraging around 5,000 captions from the MS-COCO dataset [43], comple-
mented by approximately 5,000 prompts generated by GPT-4 [4]. The GPT-4
generatedpromptsstemfromawide-rangingcollectionofpopularuserprompts
found online, details of which are provided in the supplemental materials. Uti-
lizingthiscomprehensivesetofprompts,wegenerateimagesacrossallthetext-
to-image (T2I) models referenced, as depicted in Fig. 1. For each prompt, we
generated one image for DALL-E 2 and 3 (due to cost considerations), four
images for Midjourney, and five images for the other models, culminating in
a dataset exceeding 450K generated images. It’s important to note that not all
imageswereusedduringtraining;thespecificsareinthesupplementalmaterials.
3.2 Images from Varying Hyperparameters at Inference Stage
In this research, we expand our focus beyond simply identifying the source gen-
eratorsbasedontheirarchitectures,toadeeperanalysisofthecriticalyetsubtle
choicesmadeduringtheinferencestagethathaveaprofoundeffectonthegener-
atedoutputs.Initially,weinvestigatethepossibilityofidentifyingspecificmodel
checkpointswithinthesamearchitecture,specificallyStableDiffusion(SD)[61],
based on different training iterations. To facilitate this, we generated images
using five versions of SD from 1.1 to 1.5. Despite sharing a common architec-
ture,eachversionwastrainedforadistinctnumberofiterations.Next,wedelve
ees
nac
ew
egami
eht
nI:tpmorPOCOC
edam
eltsac
retawrednu
nA:tpmorP4-TPG
dna
nav
a si
erehT
.gnidnats
elpoep
ynam
yks
dna
ssarg
,dum
,dnas
a
si
sihT
.enalpria
fo
elyts
eht
ni
,klofrem
htiw
dellif
,laroc
fo
.roloc
eulb
elap
ni
si
hcihw
.enakA
akanhO
srotareneGI2T
srotareneGI2T6 Katherine Xu, Lingzhi Zhang, Jianbo Shi
into the impact of using different schedulers or samplers [35,40,69,80] during
the inference phase for the same generator. We question whether the generated
images can reveal which scheduler was employed. Furthermore, drawing inspi-
ration from studies indicating that the use of different seeds in GAN-generated
images can be detected [77], we seek to apply this concept to diffusion models
to determine if the choice of seed is detectable in the resulting images. Finally,
we conduct experiments with diffusion steps ranging from 5 to 50 in increments
of 5 to investigate whether the number of sampling steps employed can leave
detectable traces in the images. Selected samples of images generated under
different hyperparameter adjustments are presented in Fig. 2.
Checkpoints(TrainingIterations) Schedulers
Seeds InferenceSteps
Fig.2: An illustration showcasing the diversity in generated images influenced by
varying hyperparameters: different model checkpoints (within the same architecture),
diverseschedulingalgorithms,variedinitializationseeds,andarangeofinferencesteps.
4 Detecting Image Attribution in RGB
In this section, we benchmark the performance of image attribution across 12
modern text-to-image generators, and we examine the impact of various archi-
tectures, training sizes, and croSesesd-s:domain influences on task performance. We
Checkpoints:100723
then delve into the detectability of traces for various hyperparameter adjust-
ments during the inference stage. Finally, inspired by typical user workflows, we
investigate whether AI-generated images can still be attributed to their original
generators after being modified by distinct software or models.
4.1 Training Image Attributors
Problem Setup and Model Performance. Priorresearchhasdemonstrated
deep networks’ ability to distinguish AI-generated images from real ones [11,18,
39,47,49,51,72] and to identify their sources [10,66,76] effectively. Our study
builds on this foundation by merging the tasks of discerning "AI-generated vs.
RealImages"andattributingimagestotheirsourcesintoasingularframework.
Thisisachievedbyincludingrealimagesinourdatasetandtreatingthemasan
additional‘generator’,enablingamoredetailedanalysisofAI-generatedcontent.
Concerningthearchitectureoftheimageattributor,whichfunctionsasanimage
classifier, previous studies [18,51] have demonstrated that a straightforward lin-
ear probe or nearest neighbor search, when applied to a large pretrained model
like CLIP [56], can effectively differentiate AI-generated images from real ones.
Inspired by these findings, we employ three network architectures to tackle the
attributiontaskacross12modernmainstreamtext-to-image(T2I)generators—
such as SDXL Turbo [63], DALL-E 3 [7], and Midjourney 6—plus a real image
dataset.ThesearchitecturesincludeanEfficientFormer[41]trainedfromscratch,Detecting Image Attribution for T2I Diffusion Models in RGB and Beyond 7
aCLIP[56]backboneconnectedwithalinearprobeandMLP,andDINOv2[52]
with a similar configuration. We also analyze the impact of incorporating text
promptsasadditionalinputssimilartoShaetal.[66],whichwefoundtoprovide
slight yet consistent improvements across all architectures, as shown in Tab. 1.
E.F. (scratch) CLIP+LP CLIP+MLP DINOv2+LP DINOv2+MLP
w/o text 90.03% 70.15% 73.09% 67.68% 71.33%
w/ text 90.96% 71.44% 74.19% 69.44% 73.08%
Table 1: Quantitative evaluation (13-way classification accuracy) of various architec-
turesforimageattributionlearningperformedacross12generatorsandacorresponding
setofrealimages,witheachcategorycontaininganequalnumberofimages.Theprob-
abilityofrandomlyguessingthecorrectsourceis 1 ,correspondingto7.69%accuracy.
13
In this context, "E.F." refers to EfficientFormer. The first and second rows in the re-
sults table indicate classifiers trained without and with text prompts, respectively.
Analyzing Classifier Performance Across Generators. To provide a
more granular view of our analysis, we delve into the performance specifics of
eachclassifier,illustratingadetailedaccuracybreakdownthrougharadargraph
andacorrespondingconfusionmatrix,asdepictedinFig.3.Ourfindingsreveala
noticeable challenge in differentiating between generators from the same family,
withnotablepairsincluding"SD1.5vs.SD2.0,""Midjourney5.2vs.Midjourney
6," and "LCM (2 steps) vs. LCM (4 steps)." While Midjourney’s architecture
remains undisclosed to the public, it is reasonable to infer that versions 5.2 and
6 likely share a similar underlying architecture from our analysis. Interestingly,
DALL-E3presentsmoreconfusionwhencomparedtoMidjourneyversions5.2/
6, rather than with DALL-E 2. We attribute this to the significant architectural
differences: DALL-E 2 incorporates pixel diffusion in its decoder stage, whereas
DALL-E3employsmulti-stagelatentdiffusionalongsideadistinctone-stepVAE
decoder, similar to [61], leading to divergent generative characteristics. Finally,
we demonstrate that the accuracy of the attributor consistently improves with
an increase in the number of training images, as shown on the right side of Fig.
3. However, due to budget constraints, fully exploring the dataset expansion up
to the saturation point is deferred to future research endeavors.
Fig.3: Left/Middle:AccuracyandconfusionmatrixofEfficientFormertrainedwith
text prompts, which achieved the highest accuracy in Table 1. Right: Accuracy of
EfficientFormer as we vary the number of training images.8 Katherine Xu, Lingzhi Zhang, Jianbo Shi
Cross-domain Generalization. As highlighted in Sec. 3.1, user prompts
vary significantly, with some describing natural scenes and others depicting cre-
ativeorsurrealconcepts.Thisdiversityledustoexaminehowaclassifier,trained
on images generated using MS-COCO captions, would perform when applied to
images created from GPT-4’s inventive prompts, and conversely. The results,
presentedinTab.2,showanoticeabledeclineinperformancewhentheclassifier
is trained and tested across these differing domains. Since we keep the same set
of generators and only change the style of prompts, this outcome underscores
thatlearningimageattributionusesthevisiblecontentinthegeneratedimages.
Train on MS-COCO Train on GPT-4 Train on Both
Test on MS-COCO 89.04% 71.07% 85.78%
Test on GPT-4 69.24% 79.35% 81.06%
Table 2: Cross-domain generalization accuracy in image attributors. The amount of
training and testing data was kept consistent across trials, and an equal number of
imageswassourcedfromMS-COCOandGPT-4promptsforthe‘TrainonBoth’trial.
4.2 Analyzing the Detectability of Hyperparameter Variations
T2I generators often have several adjustable hyperparameters at the inference
stage that affect the generated image quality. A natural question that arises
is whether images produced using different hyperparameters are distinguish-
able. To investigate this, we target four hyperparameter choices for Stable Dif-
fusion [61]: model checkpoint, scheduler type, number of sampling steps, and
initialization seed. Specifically, we compared Stable Diffusion checkpoints 1.1
to 1.5, each of which are trained using a different number of iterations on the
LAION dataset [64]. We then examined the detectability of images generated
using eight schedulers: DDIM [69], DDPM [35], Euler [40], Euler with ancestral
sampling [40], KDPM 2 [40], LMS [40], PNDM [40], and UniPC [80]. Addition-
ally,wegeneratedimagesusingbothSD2.0andSDXLfortendifferentsampling
stepsrangingfrom5to50,andtendifferentseedsrangingfrom1to10.Foreach
hyperparameter choice, we train a separate EfficientFormer [41] to classify the
generated images, and the results are illustrated in Tab. 3 and Fig. 4. As shown
inTab.3,allsixclassifierscandetectthehyperparameterchoicebetterthanran-
domchance.Interestingly,theinitializationseedachievesnearly100%accuracy,
which aligns with prior work by Yu et al. [76] that found different seeds lead to
attributable GAN fingerprints. Moreover, when looking at the confusion matrix
for different sampling steps using SDXL in Fig. 4, we see that images generated
using fewer steps are more detectable than those generated using more steps,
likely because fewer steps noticeably degrades the generation quality.
4.3 Assessing Detectability of Post-Editing Enhancements
A common workflow for utilizing AI-generated images involves users identifying
unwanted artifacts or distracting areas within these images. They often import
these images into additional models or software for further editing and refine-
ment, such as SDXL Inpainting [55] or Photoshop Generative Fill (Ps Gen-Detecting Image Attribution for T2I Diffusion Models in RGB and Beyond 9
Checkpoints Schedulers Sampling Steps Seeds
Random Chance 20% 12.5% 10%/10% 10%/10%
Accuracy 30.21% 20.18% 25.96%/56.64% 98.80%/99.94%
Table 3: Comparison of accuracy for detecting hyperparameter values based on gen-
eratedimages.Forthe‘SamplingSteps’and‘Seeds’trials,wetrainedandevaluatedon
images from SD 2.0 and SDXL, and the accuracies are formatted as SD 2.0 / SDXL.
Notably, the ‘Seeds’ trial attains near perfect performance.
Fig.4: Confusion matrices for different hyperparameter adjustments, including the
Stable Diffusion version, scheduler type, and number of inference steps. We observe
thatimagesgeneratedwithfewerSDXLsamplingstepsaremoredetectable,likelydue
to visible degradation in the image quality.
Fill) [2], to enhance local regions. Many text-to-image applications are con-
strained to relatively low resolutions, typically around 1K, or produce images
with smooth/blurry texture. Consequently, some professionals opt to upscale or
refine the details of these generated images using advanced tools, such as Mag-
nific AI [3]. This practice leads to a pertinent question: Is it possible to still
detect the original source generator after the images have undergone further
modifications using a variety of software or other AI models? For instance, an
image initially created by Midjourney 6 [1] could subsequently be edited with
SDXL Inpainting, Photoshop GenFill, or Magnific AI, as illustrated in Fig. 5.
SDXLInpainting MagnificAI
Small(0~15%) Medium(15~30%) Large(30~60%) 4XUpsampling
MidjourneyV6
Small(0~15%) Medium(15~30%) Large(30~60%)
PhotoshopGenerativeFill
Fig.5: Left:OriginalimagegeneratedbyMidjourney6.Middle:Localmodifications
utilizing SDXL inpainting and Photoshop Generative Fill across three masks with
small, medium, and large holes. Right: The image upscaled 4X by Magnific AI.
Tosimulatetypicaluseredits,wegeneratedfree-formmasksacrossthreesize
categories—small (0 to 15%), medium (15 to 30%), and large (30 to 60%)—re-10 Katherine Xu, Lingzhi Zhang, Jianbo Shi
SDXL Inpainting Ps Generative Fill Magnific AI
EditRegionRatio 0∼15% 15∼30% 30∼60% 0∼15% 15∼30% 30∼60% 100%
RandomChance 7.69% 7.69% 7.69% 7.69% 7.69% 7.69% 33.33%
OriginalImage 90.96% 90.96% 90.96% 90.96% 90.96% 90.96% 93.33%
Post-Editing 64.96% 61.56% 55.62% 88.21% 85.44% 71.91% 70.00%
Table4:Comparisonofpost-editingdetectionaccuracyacrossdifferentAImodels.We
use the best performing image attributor in Table 1 for evaluation, which is Efficient-
Formertrainedwithtextprompts.Accuracydeclinesatasimilarrateaftermodifying
the image using SDXL Inpainting [55] and Photoshop (Ps) Generative Fill [2].
flectingthecommonrangeofeditsappliedtoimages.Thesemaskswereapplied
to the entire test set for pixel regeneration using SDXL Inpainting [55] and Ps
GenFill [2]. We then assessed the best performing image attributor in Tab. 1,
EfficientFormertrainedwithtextprompts,onthesepost-editedimages.Accord-
ing to Tab. 4, we observed a monotonic decrease in accuracy with respect to
the modified area of the images. Notably, SDXL Inpainting resulted in greater
accuracy loss compared to Ps GenFill for the same images and masks. We hy-
pothesizethisdisparityarisesbecausetheSDXLInpaintingmodelcloselyrelates
totheSDXLtext-to-image(T2I)modelincludedinourtraininggeneratorpool,
potentiallyskewingeditedimagestowardsanSDXL-likeappearance,whereasPs
GenFilldoesnotcloselyresembleanygeneratorinourtrainingset.Thisobserva-
tionisvalidatedinthecorrespondingconfusionmatrix,whichwehavesharedin
the supplemental materials. For texture enhancements via Magnific AI [3], bud-
get constraints limited our examination to 10 examples from each of the three
generators: DALL-E 3, Midjourney 6, and SDXL Turbo. This limitation set a
basic random chance of classification at 33.33%. This analysis, reflected in the
lastcolumnofTab.4,showsapproximately23%degradation,despiteeditingall
pixelsintheimages.Despitethenotedperformancereductions,theaccuracyfor
all post-edited images remains significantly above random chance, establishing
a strong baseline for the task of post-editing image attribution.
5 Detecting Image Attribution Beyond RGB
Previous studies have demonstrated that training a standard deep network can
effectively distinguish between real and generated images, as well as correctly
attribute generated images to their original generators. In Sec. 4.1, we observed
that a lightweight transformer achieves high accuracy for these tasks, mirror-
ing these findings. These prior studies have suggested that the attributor may
leverage middle-to-high frequency information to differentiate images. However,
itremainsunclearwhatexactlyconstitutesthis"middle-to-highfrequencyinfor-
mation"andtowhatextentthenetworkcanstillidentifydetectabletracesinthe
imagesasweincrementallyremovevisualdetails.Therefore,thissectionpresents
an extensive empirical study on the impact of progressively eliminating visual
information at various levels of granularity on image attribution performance.
High-Frequency Perturbation.Priorresearch[6,8,16,20,22,23,25,47,60,
71]hasidentifiedthatgeneratorsleaveuniquefingerprintsinthehigh-frequency
domain, allowing attributors to learn these high-frequency details and achieve
high performance. As an initial step, we investigate the effects of introducing
high-frequency perturbations to images on the attributor’s performance, whichDetecting Image Attribution for T2I Diffusion Models in RGB and Beyond 11
aimstoenforcetheclassifierlearnbeyondhigh-frequencydetails.Forsimplicity,
we train a separate EfficientFormer [41] on each set of perturbed images. Figure
6 illustrates our observations under four types of perturbation: Gaussian blur,
bilateral filtering, adding Gaussian noise, and SDEdit [50]. We note that these
perturbations result in a modest decrease in classification accuracy. Specifically
for SDEdit, the high-frequency traits of SDXL are embedded into every image,
regardless of their source generators, by undergoing processing via the encoder,
diffusion UNet, and decoder of SDXL [55]. Remarkably, this process led to only
aminorreductioninaccuracy,suggestingarobustnessintheattributor’sability
to identify generator-specific fingerprints despite high-frequency modifications.
Gaussian Blur Bilateral Filter
radius = 5 81.78% radius = 10 77.45% diameter = 5 90.30% diameter = 10 87.54%
Original
90.96%
std = 15 83.56% std = 25 81.23% strength: 0.01 86.34% strength: 0.05 84.58%
Add Gaussian Noise SDEdit
Fig.6: Weshowcaseageneratedimagebeforeandafterperturbingitshigh-frequency
details via Gaussian blurring, bilateral filtering, adding Gaussian noise, and SDEdit
[50].WetrainedEfficientFormeronimagesaftereachhigh-frequencyperturbationand
observedamilddeclineintherespectivetestaccuracy,asindicatedbesidetheimages.
Middle-LevelRepresentation.High-frequencyperturbationsresultinonly
minor performance degradation, which suggests that the detectable traces left
by different generators might also reside within the mid-frequency domain. To
delve deeper into the presence of these detectable traces, we convert the im-
ages into various mid-level representations. These include ‘Albedo,’ [21] ‘Shad-
ing,’[21]‘CannyEdge,’‘DepthMap,’[75]‘SurfaceNormal,’[21]and‘Perspective
Fields,’ [37] utilizing readily available models for the transformations. This ap-
proach aims to uncover the extent to which these mid-level frequencies carry
generator-specificinformationthatcanbeleveragedforattribution.Weproceed
bytrainingadistinctEfficientFormer[41]foreachmid-levelrepresentation,and
we show their classification accuracies in Fig. 7 and confusion matrices in Fig.
8. Notably, although the overall accuracy for the attributors trained on Canny
Edge, Depth Map, and Perspective Field images is not high in Fig. 7, they
demonstrateremarkableperformanceatdiscerningrealimagesfromfakeimages
in Fig. 8. This finding aligns with previous work by Sarkar et al. [62] suggesting
that generative models often fail to generate accurate geometry.12 Katherine Xu, Lingzhi Zhang, Jianbo Shi
RGB Albedo Shading CannyEdge DepthMap SurfaceNormal PerspectiveField
90.96% 73.13% 58.34% 53.97% 43.74% 31.09% 39.08%
Fig.7: We present an RGB image and its mid-level representations. We trained Effi-
cientFormeroneachmid-levelrepresentationandincludethecorrespondingtestaccu-
racy under each image. Please keep in mind that the random chance is 1 or 7.69%.
13
Fig.8: Confusion matrices for image attributors trained on mid-level representations.
Remarkably, attributors trained on "Canny Edge," "Depth Map," and "Perspective
Field" images are significantly better at detecting real images than synthetic images.
ImageStyleRepresentation.Giventheperceptibledifferencesinstylesor
tones among image generators, it’s common to observe distinct characteristics
in their outputs. For instance, Midjourney [1] often produces images with a
‘cinematic’ quality, while DALL-E [7,57] sometimes tends to create images with
overly smooth textures and cartoonish appearances, as shown in Fig. 1. This
observation leads to a pertinent question: if we train an attributor solely on the
stylistic representations of images, can we still identify the source generators?
To capture the style representation of images, we adhere to the methodol-
ogy established in prior style transfer literature [27,38], employing a pretrained
VGG network [67] to extract features across multiple layers. Subsequently, we
calculate the Gram matrix [26] for each layer of the network. If we denote the
feature at a specific layer as F ∈RH×W×N, then the Gram matrix is the cosine
similarity between each channel in the feature representation, yielding a matrix
of dimensions G ∈ RN×N. This process aims to distill the stylistic essence of
images, providing a unique fingerprint for each generator’s output. Specifically,
we reshape and concatenate the Gram matrices extracted from multiple layers,
and then train EfficientFormer [41] using these aggregated feature vectors.Detecting Image Attribution for T2I Diffusion Models in RGB and Beyond 13
Remarkably, the image attributor achieves an accuracy of 92.80% when
trained on style representations, surpassing the performance of the attributor
trained on original RGB images by 1.84%. The superior accuracy achieved by
this style-based image attributor highlights the critical role of stylistic features,
such as texture and color patterns, in distinguishing generators more effectively
than traditional RGB data. This suggests that the unique signatures of image
generators might be more intricately tied to their style rather than the direct
visual content. This insight not only advances our understanding of image at-
tribution techniques but also emphasizes the potential of leveraging stylistic
elements for more nuanced AI recognition and analysis tasks.
Furthermore, given the exceptional performance of training on style features
for image attribution, we seek to understand what insights we can extract from
raw values in the Gram matrices without any model training. To achieve this,
we average the Gram matrix from a single layer of VGG across 450 images
per generator, and we visualize the density distribution of its values in Fig. 9.
We observe that LCM (2 steps) and LCM (4 steps) have similar image style
distributions, as does SDXL and SDXL Turbo. Additionally, since we use real
imagesfromtheMS-COCOdataset[42],thegeneratorswithdistributionscloser
to that of real images in Fig. 9 likely generate more natural image styles.
Fig.9:Left:ConfusionmatrixforEfficientFormertrainedonaggregatedstylefeatures
obtained from Gram matrices. Compared to EfficientFormer trained on original RGB
images in Figure 3, we observe that training on image style reduces misclassification
between generators of the same family, such as "Midjourney 5.2 vs. Midjourney 6."
Right: Density distribution of values in the averaged Gram matrix (log-scaled) using
450imagespergenerator.Weincluderealimagesasadistinct‘generator’.Imagestyle
is moderately distinguishable across generators by analyzing Gram matrices alone.
ImageCompositionPattern. Beyondstylisticdifferences,wehypothesize
that various generators might create images with unique composition patterns
or layouts from the same text prompt. For instance, given identical prompts,
some generators may depict humans in portrait-style photos, while others may
place humans further from the camera, treating them as elements within the
largerscene.Thesevariationscouldstemfromeachgenerator’slearningwithits
distinctively ‘curated’ training data distribution and proprietary prompt aug-14 Katherine Xu, Lingzhi Zhang, Jianbo Shi
mentationtechniques,featuresthatareoftenintegraltocommercialmodelslike
DALL-E [7,57] and Midjourney [1]. To test our hypothesis, we analyze 100 im-
ages generated from the same prompt for each generator. We employ Grounded
SAM [59] to compute segmentation masks, serving as a proxy for layout repre-
sentation. For instance, as depicted in Figure 10, by averaging the segmentation
masks for ‘person’ and ‘corgi’ across 100 images from each generator, created
from the prompt ‘a couple, a daughter, and a corgi walking,’ we visualize the
distribution of image composition. This reveals unique layout patterns among
the generators, supporting our hypothesis.
Prompt: “a couple, a daughter and a corgi walking”
Kandinsky LCM LCM Midjourney Midjourney SDXL Stable
DALL-E 2 DALL-E 3 2.1 (2 steps) (4 steps) 5.2 6 SD 1.5 SD 2.0 SDXL Turbo Cascade
glasses 25 tree 29 road 17 road 61 road 70 handbag 58handbag 30 road 38 road 48 road 43 glasses 32 handbag 36 road 12 bench 19 handbag 4 glasses 15 tree 12 artwork 17 road 14 glasses 23 grass 18 handbag 21 road 22 road 35 handbag 9 sky 14 teddy bear 2handbag 11 handbag 11 tree 13 backpack 13 grass 7 sky 8 tree 19 handbag 1 tree 17
Fig.10:Imagecompositionanalysisacrossgeneratorsforasingleprompt.Wevisualize
theaveragedsegmentationmasksforthe‘person’and‘corgi’classes.Somegenerators,
such as Stable Cascade, tend to produce objects at specific locations. We also list the
top three inserted classes and the number of images (out of 100) with these classes.
Given the noticeable variations in the layout of generated images for a spe-
cific prompt, we further investigate whether a classifier can learn to attribute
images based solely on their composition. To this end, we segment 111 semantic
classesusingGroundedSAM[59]andsubsequentlytrainEfficientFormer[41]on
the segmentation maps with their input prompts by concatenating their respec-
tive embeddings. This approach enables the classifier to achieve an accuracy of
17.66%, despite relying on such a coarse representation. Remarkably, this accu-
racyismorethantwicethatexpectedbyrandomchance(7.69%),suggestingthat
distinct patterns in layout generation do indeed exist across these generators.
6 Conclusion
Inthisstudy,wepresentin-depthanalysesonthedetectionandattributionofim-
ages generated by contemporary text-to-image (T2I) diffusion models. Through
rigorous testing, our image attributors, trained to recognize outputs from 12
different T2I diffusion models along with a category for real images, reached an
impressive accuracy of over 90%, significantly surpassing random chance. Our
investigationintotheroleoftextprompts,thechallengeofdistinguishinggenera-
torswithinthesamefamily,andtheabilitytogeneralizeacrossdomainsprovides
comprehensive insights. Pioneeringly, we delved into the detectability of hyper-
parameter adjustments at inference time and assessed the effects of post-editing
on attribution accuracy. Going beyond mere RGB analysis, we introduce a new
framework for identifying detectable traces across various levels of visual detail,
offering profound insights into the underlying mechanics of image attribution.
Theseanalysesprovidefreshperspectivesonimageforensicsaimedatalleviating
the threat of synthetic images on copyright protection and digital forgery.
nosrep
igroc
d se et sr se as ln CIDetecting Image Attribution for T2I Diffusion Models in RGB and Beyond 15
References
1. https://www.midjourney.com/ 2, 4, 9, 12, 14, 20, 23, 24
2. https://www.adobe.com/products/photoshop/generative-fill.html 2, 9, 10,
22
3. https://magnific.ai/ 2, 9, 10, 22
4. Achiam,J.,Adler,S.,Agarwal,S.,Ahmad,L.,Akkaya,I.,Aleman,F.L.,Almeida,
D.,Altenschmidt,J.,Altman,S.,Anadkat,S.,etal.:Gpt-4technicalreport.arXiv
preprint arXiv:2303.08774 (2023) 2, 5, 20
5. Amoroso, R., Morelli, D., Cornia, M., Baraldi, L., Bimbo, A.D., Cucchiara, R.:
Parents and children: Distinguishing multimodal deepfakes from natural images
(2023) 1, 3
6. Bammey,Q.:Synthbuster:Towardsdetectionofdiffusionmodelgeneratedimages.
IEEE Open Journal of Signal Processing PP, 1–9 (01 2023). https://doi.org/
10.1109/OJSP.2023.3337714 3, 10
7. Betker,J.,Goh,G.,Jing,L.,Brooks,T.,Wang,J.,Li,L.,Ouyang,L.,Zhuang,J.,
Lee,J.,Guo,Y.,etal.:Improvingimagegenerationwithbettercaptions.Computer
Science. https://cdn. openai. com/papers/dall-e-3. pdf 2(3), 8 (2023) 1, 2, 4, 6,
12, 14, 20
8. Bi, X., Liu, B., Yang, F., Xiao, B., Li, W., Huang, G., Cosman, P.C.: Detecting
generated images by real images only (2023) 1, 3, 10
9. Bird, J.J., Lotfi, A.: Cifake: Image classification and explainable identification of
ai-generated synthetic images (2023) 3
10. Bui, T., Yu, N., Collomosse, J.: Repmix: Representation mixing for robust attri-
bution of synthesized images (2022) 1, 4, 6
11. Chai, L., Bau, D., Lim, S.N., Isola, P.: What makes fake images detectable? un-
derstanding properties that generalize (2020) 1, 3, 6
12. Chen, C., Chen, Q., Xu, J., Koltun, V.: Learning to see in the dark (2018) 3
13. Chen, J., Yao, J., Niu, L.: A single simple patch is all you need for ai-generated
image detection (2024) 24
14. Chen, Y., Viégas, F., Wattenberg, M.: Beyond surface statistics: Scene represen-
tations in a latent diffusion model (2023) 4
15. Contributors, M.: Openmmlab’s pre-training toolbox and benchmark. https://
github.com/open-mmlab/mmpretrain (2023) 22
16. Corvi,R.,Cozzolino,D.,Poggi,G.,Nagano,K.,Verdoliva,L.:Intriguingproperties
ofsyntheticimages:fromgenerativeadversarialnetworkstodiffusionmodels(2023)
3, 10
17. Corvi, R., Cozzolino, D., Zingarini, G., Poggi, G., Nagano, K., Verdoliva, L.: On
the detection of synthetic images generated by diffusion models (2022) 1, 4
18. Cozzolino, D., Poggi, G., Corvi, R., Nießner, M., Verdoliva, L.: Raising the bar of
ai-generated image detection with clip (2023) 4, 6
19. Dai, T., Cai, J., Zhang, Y., Xia, S.T., Zhang, L.: Second-order attention network
forsingleimagesuper-resolution.In:ProceedingsoftheIEEEConferenceonCom-
puter Vision and Pattern Recognition. pp. 11065–11074 (2019) 3
20. Dong, C., Kumar, A., Liu, E.: Think twice before detecting gan-generated fake
images from their spectral domain imprints. In: 2022 IEEE/CVF Conference on
ComputerVisionandPatternRecognition(CVPR).pp.7855–7864(2022).https:
//doi.org/10.1109/CVPR52688.2022.00771 3, 10
21. Du,X., Kolkin, N., Shakhnarovich,G., Bhattad,A.: Generativemodels:What do
they know? do they know things? let’s find out! (2023) 4, 1116 Katherine Xu, Lingzhi Zhang, Jianbo Shi
22. Durall, R., Keuper, M., Keuper, J.: Watch your up-convolution: Cnn based gen-
erativedeepneuralnetworksarefailingtoreproducespectraldistributions(2020)
3, 10
23. Dzanic, T., Shah, K., Witherden, F.: Fourier spectrum discrepancies in deep net-
work generated images (2020) 3, 10
24. Epstein, D.C., Jain, I., Wang, O., Zhang, R.: Online detection of ai-generated
images. In: ICCV DeepFake Analysis and Detection Workshop (2023) 3
25. Frank, J., Eisenhofer, T., Schönherr, L., Fischer, A., Kolossa, D., Holz, T.: Lever-
aging frequency analysis for deep fake image recognition (2020) 3, 10
26. Gatys,L.A.,Ecker,A.S.,Bethge,M.:Texturesynthesisusingconvolutionalneural
networks (2015) 12
27. Gatys,L.A.,Ecker,A.S.,Bethge,M.:Imagestyletransferusingconvolutionalneu-
ralnetworks.In:2016IEEEConferenceonComputerVisionandPatternRecogni-
tion (CVPR). pp. 2414–2423 (2016). https://doi.org/10.1109/CVPR.2016.265
12
28. Gildenblat, J., contributors: Pytorch library for cam methods. https://github.
com/jacobgil/pytorch-grad-cam (2021) 25, 30
29. Girish, S., Suri, S., Rambhatla, S., Shrivastava, A.: Towards discovery and attri-
bution of open-world gan generated images (2021) 1, 4
30. Goodfellow, I.J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair,
S., Courville, A., Bengio, Y.: Generative adversarial networks (2014) 3
31. Gragnaniello, D., Cozzolino, D., Marra, F., Poggi, G., Verdoliva, L.: Are gan gen-
erated images easy to detect? a critical analysis of the state-of-the-art (2021) 3
32. Guarnera,L.,Giudice,O.,Battiato,S.:Levelupthedeepfakedetection:amethod
to effectively discriminate images generated by gan architectures and diffusion
models (2023) 1, 4
33. Guo, X., Liu, X., Ren, Z., Grosz, S., Masi, I., Liu, X.: Hierarchical fine-grained
image forgery detection and localization (2023) 4
34. Ha, A.Y.J., Passananti, J., Bhaskar, R., Shan, S., Southen, R., Zheng, H., Zhao,
B.Y.:Organicordiffused:Canwedistinguishhumanartfromai-generatedimages?
(2024) 3
35. Ho, J., Jain, A., Abbeel, P.: Denoising diffusion probabilistic models. Advances in
neural information processing systems 33, 6840–6851 (2020) 6, 8
36. Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen,
W.: Lora: Low-rank adaptation of large language models (2021) 4
37. Jin, L., Zhang, J., Hold-Geoffroy, Y., Wang, O., Matzen, K., Sticha, M., Fouhey,
D.F.: Perspective fields for single image camera calibration. CVPR (2023) 11, 22
38. Johnson,J.,Alahi,A.,Fei-Fei,L.:Perceptuallossesforreal-timestyletransferand
super-resolution. In: Computer Vision–ECCV 2016: 14th European Conference,
Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14. pp.
694–711. Springer (2016) 12
39. Ju,Y.,Jia,S.,Ke,L.,Xue,H.,Nagano,K.,Lyu,S.:Fusingglobalandlocalfeatures
for generalized ai-synthesized image detection (2022) 1, 3, 6
40. Karras,T.,Aittala,M.,Aila,T.,Laine,S.:Elucidatingthedesignspaceofdiffusion-
basedgenerativemodels.AdvancesinNeuralInformationProcessingSystems35,
26565–26577 (2022) 6, 8
41. Li, Y., Yuan, G., Wen, Y., Hu, J., Evangelidis, G., Tulyakov, S., Wang, Y., Ren,
J.: Efficientformer: Vision transformers at mobilenet speed. Advances in Neural
Information Processing Systems 35, 12934–12949 (2022) 6, 8, 11, 12, 14, 22, 24,
25Detecting Image Attribution for T2I Diffusion Models in RGB and Beyond 17
42. Lin,T.Y.,Maire,M.,Belongie,S.,Bourdev,L.,Girshick,R.,Hays,J.,Perona,P.,
Ramanan,D.,Zitnick,C.L.,Dollár,P.:Microsoftcoco:Commonobjectsincontext
(2015) 13, 21, 30
43. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P.,
Zitnick, C.L.: Microsoft coco: Common objects in context. In: Computer Vision–
ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12,
2014, Proceedings, Part V 13. pp. 740–755. Springer (2014) 2, 5
44. Liu,Z.,Qi,X.,Torr,P.:Globaltextureenhancementforfakefacedetectioninthe
wild (2020) 3
45. Loshchilov, I., Hutter, F.: Decoupled weight decay regularization (2019) 22
46. Luo, S., Tan, Y., Huang, L., Li, J., Zhao, H.: Latent consistency models: Synthe-
sizing high-resolution images with few-step inference (2023) 1, 2, 4, 20
47. Marra, F., Gragnaniello, D., Verdoliva, L., Poggi, G.: Do gans leave artificial fin-
gerprints? (2018) 3, 6, 10
48. Marra, F., Saltori, C., Boato, G., Verdoliva, L.: Incremental learning for the de-
tection and classification of gan-generated images (2019) 1, 4
49. Matern,F.,Riess,C.,Stamminger,M.:Exploitingvisualartifactstoexposedeep-
fakesandfacemanipulations.In:2019IEEEWinterApplicationsofComputerVi-
sionWorkshops(WACVW).pp.83–92(2019).https://doi.org/10.1109/WACVW.
2019.00020 3, 6
50. Meng,C.,He,Y.,Song,Y.,Song,J.,Wu,J.,Zhu,J.Y.,Ermon,S.:Sdedit:Guided
image synthesis and editing with stochastic differential equations. arXiv preprint
arXiv:2108.01073 (2021) 11
51. Ojha,U.,Li,Y.,Lee,Y.J.:Towardsuniversalfakeimagedetectorsthatgeneralize
across generative models (2023) 4, 6
52. Oquab, M., Darcet, T., Moutakanni, T., Vo, H., Szafraniec, M., Khalidov, V.,
Fernandez,P.,Haziza,D.,Massa,F.,El-Nouby,A.,etal.:Dinov2:Learningrobust
visualfeatureswithoutsupervision.arXivpreprintarXiv:2304.07193(2023) 7,22,
23, 24
53. Pernias,P.,Rampas,D.,Richter,M.L.,Pal,C.J.,Aubreville,M.:Wuerstchen:An
efficient architecture for large-scale text-to-image diffusion models (2023) 1, 2, 4,
20, 23, 24
54. von Platen, P., Patil, S., Lozhkov, A., Cuenca, P., Lambert, N., Rasul, K.,
Davaadorj, M., Wolf, T.: Diffusers: State-of-the-art diffusion models. https://
github.com/huggingface/diffusers (2022) 5
55. Podell,D.,English,Z.,Lacey,K.,Blattmann,A.,Dockhorn,T.,Müller,J.,Penna,
J.,Rombach,R.:Sdxl:Improvinglatentdiffusionmodelsforhigh-resolutionimage
synthesis. arXiv preprint arXiv:2307.01952 (2023) 1, 2, 4, 8, 10, 11, 20, 22
56. Radford,A.,Kim,J.W.,Hallacy,C.,Ramesh,A.,Goh,G.,Agarwal,S.,Sastry,G.,
Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from
naturallanguagesupervision.In:Internationalconferenceonmachinelearning.pp.
8748–8763. PMLR (2021) 4, 6, 7, 22, 23, 24
57. Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., Chen, M.: Hierarchical text-
conditional image generation with clip latents (2022) 1, 2, 4, 12, 14, 20
58. Razzhigaev,A.,Shakhmatov,A.,Maltseva,A.,Arkhipkin,V.,Pavlov,I.,Ryabov,
I.,Kuts,A.,Panchenko,A.,Kuznetsov,A.,Dimitrov,D.:Kandinsky:animproved
text-to-imagesynthesiswithimagepriorandlatentdiffusion(2023) 1,2,4,20,23,
24
59. Ren,T.,Liu,S.,Zeng,A.,Lin,J.,Li,K.,Cao,H.,Chen,J.,Huang,X.,Chen,Y.,
Yan, F., et al.: Grounded sam: Assembling open-world models for diverse visual
tasks. arXiv preprint arXiv:2401.14159 (2024) 1418 Katherine Xu, Lingzhi Zhang, Jianbo Shi
60. Ricker, J., Damm, S., Holz, T., Fischer, A.: Towards the detection of diffusion
model deepfakes (2024) 3, 10
61. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution
image synthesis with latent diffusion models (2022) 1, 2, 4, 5, 7, 8, 20
62. Sarkar, A., Mai, H., Mahapatra, A., Lazebnik, S., Forsyth, D.A., Bhattad, A.:
Shadows don’t lie and lines can’t bend! generative models don’t know projective
geometry...for now (2023) 4, 11
63. Sauer, A., Lorenz, D., Blattmann, A., Rombach, R.: Adversarial diffusion distilla-
tion. arXiv preprint arXiv:2311.17042 (2023) 1, 2, 4, 6, 20, 30
64. Schuhmann,C.,Beaumont,R.,Vencu,R.,Gordon,C.,Wightman,R.,Cherti,M.,
Coombes, T., Katta, A., Mullis, C., Wortsman, M., Schramowski, P., Kundurthy,
S., Crowson, K., Schmidt, L., Kaczmarczyk, R., Jitsev, J.: Laion-5b: An open
large-scale dataset for training next generation image-text models (2022) 8
65. Selvaraju, R.R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., Batra, D.:
Grad-cam: Visual explanations from deep networks via gradient-based localiza-
tion. International Journal of Computer Vision 128(2), 336–359 (Oct 2019).
https://doi.org/10.1007/s11263-019-01228-7, http://dx.doi.org/10.1007/
s11263-019-01228-7 25, 30
66. Sha,Z.,Li,Z.,Yu,N.,Zhang,Y.:De-fake:Detectionandattributionoffakeimages
generated by text-to-image generation models (2023) 1, 3, 4, 6, 7
67. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale
image recognition. arXiv preprint arXiv:1409.1556 (2014) 12
68. Sinitsa,S.,Fried,O.:Deepimagefingerprint:Towardslowbudgetsyntheticimage
detection and model lineage analysis (2023) 1, 3
69. Song,J.,Meng,C.,Ermon,S.:Denoisingdiffusionimplicitmodels.arXivpreprint
arXiv:2010.02502 (2020) 6, 8
70. Tan, C., Zhao, Y., Wei, S., Gu, G., Wei, Y.: Learning on gradients: Generalized
artifactsrepresentationforgan-generatedimagesdetection.In:Proceedingsofthe
IEEE/CVFConferenceonComputerVisionandPatternRecognition.pp.12105–
12114 (2023) 3
71. Tian, C., Luo, Z., Shi, G., Li, S.: Frequency-aware attentional feature fusion for
deepfake detection. In: ICASSP 2023 - 2023 IEEE International Conference on
Acoustics,SpeechandSignalProcessing(ICASSP).pp.1–5(2023).https://doi.
org/10.1109/ICASSP49357.2023.10094654 3, 10
72. Wang, S.Y., Wang, O., Zhang, R., Owens, A., Efros, A.A.: Cnn-generated images
are surprisingly easy to spot... for now (2020) 1, 3, 6
73. Wang,Z.,Bao,J.,Zhou,W.,Wang,W.,Hu,H.,Chen,H.,Li,H.:Direfordiffusion-
generated image detection (2023) 3
74. Wu,H.,Zhou,J.,Zhang,S.:Generalizablesyntheticimagedetectionvialanguage-
guided contrastive learning (2023) 4
75. Yang, L., Kang, B., Huang, Z., Xu, X., Feng, J., Zhao, H.: Depth anything: Un-
leashing the power of large-scale unlabeled data. In: CVPR (2024) 11
76. Yu, N., Davis, L., Fritz, M.: Attributing fake images to gans: Learning and ana-
lyzing gan fingerprints (2019) 1, 3, 4, 6, 8
77. Yu,N.,Skripniuk,V.,Abdelnabi,S.,Fritz,M.:Artificialfingerprintingforgenera-
tive models: Rooting deepfake attribution in training data. In: Proceedings of the
IEEE/CVF International conference on computer vision. pp. 14448–14457 (2021)
6
78. Zhan, G., Zheng, C., Xie, W., Zisserman, A.: What does stable diffusion know
about the 3d scene? (2023) 4Detecting Image Attribution for T2I Diffusion Models in RGB and Beyond 19
79. Zhang, Y., Xu, X.: Diffusion noise feature: Accurate and fast generated image
detection (2023) 3
80. Zhao, W., Bai, L., Rao, Y., Zhou, J., Lu, J.: Unipc: A unified predictor-corrector
framework for fast sampling of diffusion models. Advances in Neural Information
Processing Systems 36 (2024) 6, 8
81. Zhong,N.,Xu,Y.,Li,S.,Qian,Z.,Zhang,X.:Patchcraft:Exploringtexturepatch
for efficient ai-generated image detection (2024) 1, 3, 24
82. Zhu,M.,Chen,H.,Huang,M.,Li,W.,Hu,H.,Hu,J.,Wang,Y.:Gendet:Towards
good generalizations for ai-generated image detection (2023) 320 Katherine Xu, Lingzhi Zhang, Jianbo Shi
A Human Performance
In computer vision and machine learning, human performance is typically seen
as the benchmark for AI models. However, in the case of image attribution,
the scenario reverses—AI significantly outperforms humans. This is highlighted
by an experiment conducted by one of our co-authors, who has extensive expe-
rience with AI-generated images. Tasked with attributing 650 images to their
correct source generators, the co-author achieved only a 37.23% accuracy rate.
This figure, while better than the 7.69% random chance level, is markedly in-
ferior to the accuracy of our top AI classifier, which has 90%+ accuracy. This
outcome underlines the exceptional challenge of image attribution, where even
well-informed individuals struggle. It showcases the necessity of AI in assisting
humans with tasks that are beyond their natural proficiency, emphasizing AI’s
potential to enhance human performance in specialized domains.
Fromtheperspectiveofthehumanevaluator,differentiatingbetweencertain
AIimagegeneratorsandotherscanbenuancedyetdiscernible.TheLatentCon-
sistency Models (LCM) [46], at 2 and 4 steps, are notable for their occasional
oversmooth artifacts, a result of undersampling, making them easier to identify
comparedtoothermodels.DALL-E3[7]isdistinguishedbyitstendencytopro-
duce surreal, cartoonish images, though these often exhibit repetitive patterns.
DALL-E 2 [57], on the other hand, is characterized by a unique ‘sharp’ visual
artifact,likelyaconsequenceofitspixeldiffusionprocessinthedecoder,setting
it apart from other models. Midjourney versions 5.2 and 6 [1] typically deliver
the highest quality images, sometimes with a distinctive cinematic style.
Real images, however, are generally more straightforward to identify. One
can often look at the detailed object regions—like hands and text—where AI-
generated images tend to falter. The naturalistic photo style of real images also
serves as a key differentiation factor from AI-generated content. Other genera-
tors, such as SD 1.5 [61], SD 2.0 [61], SDXL [55], SDXL Turbo [63], Kandinsky
2.1 [58], and Stable Cascade [53], present a greater challenge for human evalua-
tors to distinguish due to the subtlety of their differences.
B Data and Implementation Details
GPT-4 Generated Prompts. Building upon Section 3 of our main paper,
this section delves into the methodology behind generating creative and surreal
prompts using GPT-4 [4]. As illustrated in Fig. 11, our process begins with the
formulation of system-level instructions directing GPT-4 to act as an assistant
for writing text prompts. We then supply a specific context and a collection of
severalhundredexemplaryprompts.ThissetupenablesGPT-4tosynthesizeand
generate new, innovative prompts based on the provided examples and context.
Image Generation.Weemployed12T2IdiffusionmodelstogenerateRGB
images without watermarks, and the generated image sizes are as follows:
·
512 × 512: Kandinsky 2.1, SD 1.1, SD 1.2, SD 1.3, SD 1.4, SD 1.5, SD 2.0,
SDXL Turbo
·
1024 × 1024: DALL-E 2, DALL-E 3, LCM (2 steps), LCM (4 steps), Mid-
journey 5.2, Midjourney 6, SDXL, Stable CascadeDetecting Image Attribution for T2I Diffusion Models in RGB and Beyond 21
Response:
System:You are an AI A surreal depiction of a city skyline turning into a wave, in the style of
assistanttohelpgenerate Salvador Dali
imagetextprompts. A cybernetic eagle soaring over a futuristic cityscape, rendered in the style of
User:I am trying to generate Moebius
thousands of novel prompts for An old oak tree sheltering a boy, a dog and a horse, reflecting Johanna Basford's
AI to generate images using style
tools, such as Midjourney, A steampunk table setting, complete with cogs and gears, in a dingy Victorian
Stable Diffusion, and DALL-E. room, Max Ernst style
You can use your prior knowledge The Milky Way stretched across a desert landscape, in vivid neon colors,
abour popular prompts, and I am reflecting the style of Brandon Woelfel
also providing you a few hundred An armored dinosaur in a dramatic rainforest battle, inspired by classic paleoart
examplar prompts in below: An underwater castle made of coral, filled with merfolk, in the style of Ohnaka
Akane
Cat looking at it and ‘on top of GPT4 A miniaturized New York City, with tiny taxis and pedestrians, mimicking Tatsuya
the world, cyberpunk poster, in Tanaka's miniature calendar
the style of Erik Jones, social A fiery phoenix rising from a cityscape, reflecting the style of Simon Stålenhag’s
media portraiture, sparth, retro futuristic scenes
vibrant color fields, graffiti A regal king standing heroically on a dragon's head, detailed in the style of
and street art, highly detailed Yoshitaka Amano
figures, hyper-detailed A romantic scene of two people kissing under cherry blossom trees, digital oil
…… painting style of Leonid Afremov
A dark space station inhabited with multi-species aliens living harmoniously,
vibrant color tones and intricate details, H.R Giger’s surrealism style
Could you learn from these A demon and an angel playing chess on the edge of a cliff during a storm, in the
prompts and generate 30 more style of Francisco Goya
prompts that are sufficiently An 8k ultra-detailed mural of graffiti covering an entire city block, reflecting
diverse and novel covering the style of Banksy
various content and styles. A hyper-detailed print of a wise owl reading a spellbook, in the style of Brian
Froud’s fantasies
……
Fig.11:AnillustrationofhowweusetheGPT-4APItomassivelygeneratethousands
of creative and surreal prompts.
We also use 5000 real images from the MS-COCO [42] 2017 validation set.
More Visualizations of Hyperparameter Variations. As an extension
of Fig. 2 in the main paper, we show more generations by hyperparameter vari-
ations in Fig. 12.
Checkpoints(TrainingIterations) Sampler
Seeds InferenceSteps
Checkpoints(TrainingIterations) Sampler
Seeds InferenceSteps
Checkpoints(TrainingIterations) Sampler
Seeds InferenceSteps
Fig.12: More examples showcasing the diversity in generated images influenced by
varying hyperparameters: different model checkpoints within the same architecture,
diverseschedulingalgorithms,variedinitializationseeds,andarangeofinferencesteps.
Training Data. For Sec. 4.1 and 5, we view image attribution as a 13-way
classificationtaskwith12text-to-imagediffusionmodelsand1setofrealimages.
An exception is the cross-domain generalization study, where we exclude real22 Katherine Xu, Lingzhi Zhang, Jianbo Shi
imagesasa13th classbecausetherearenorealimagesfortheGPT-4generated
prompts.Weuse3200training,450validation,and450testingimagesperclass.
For Sec. 4.2, we analyze four hyperparameters: Stable Diffusion checkpoint,
scheduler type, number of sampling steps, and initialization seed. When train-
ing classifiers for SD checkpoints, schedulers, and sampling steps, we use 20000
training, 2500 validation, and 2500 testing images per class. For seeds, we use
3200 training, 450 validation, and 450 testing images per class.
For Sec. 4.3, we run inference using the EfficientFormer [41] trained with
textpromptsfromSec.4.1.ForSDXLInpainting[55]andPhotoshopGenerative
Fill [2], we use 450 images from each of the 13 classes. For Magnific AI [3], we
use 10 images from each of DALL-E 3, Midjourney 6, and SDXL Turbo.
Data Augmentation. During training, we first resize each image to have a
shorteredgeofsize224usingbicubicinterpolation,thencentercroptheimageto
size 224×224, and finally randomly flip the image horizontally with probability
0.5. During validation and testing, we only resize and center crop the images.
Image Attributors. We selected three network architectures for the image
attributiontask,andweusethecodeimplementationfromMMPretrain[15].Our
primaryarchitectureisEfficientFormer-L3[41]trainedfromscratchbecauseitis
alightweighttransformer.Moreover,weemployapretrained,frozentransformer
backbone attached to a linear probe (LP) or multilayer perceptron (MLP). The
backboneiseitherCLIPViT-B/16[56]orDINOv2ViT-L/14[52],andtheMLP
consistsofthreelinearlayerswithsigmoidactivationandhiddendimension256.
ForthelinearprobeandMLPclassifierheads,thereare768channelsintheinput
featuremapforCLIP+LPandCLIP+MLP,and1024channelsforDINOv2+LP
and DINOv2+MLP.
To train image attributors with text prompts, we compute text embeddings
using a pretrained CLIP [56] text encoder. Then, we concatenate image embed-
dings from the backbone with text embeddings as input to the classifier head.
Forallimageattributors,wesetabatchsizeof128andtrainfor2000epochs.
Weusethecheckpointwiththebestvalidationaccuracy.Additionally,weutilize
theAdamWoptimizer[45]withlearningrate0.0002andweightdecay0.05.The
learning rate scheduler has a linear warm-up period of 20 epochs, followed by a
cosine annealing schedule with a minimum learning rate of 0.00001.
Perspective Fields.Weusethecodeimplementationfrom[37].Eachinput
to the attributor trained on Perspective Fields has a size of 640×640×3. The
first 640 × 640 channel contains latitude values, and the next two 640 × 640
channels contain gravity values. We adapt the code from [37] to visualize the
Perspective Field on a black image in Fig. 5 of the main paper.
C Additional Experiments
C.1 Color Analysis
In addition to studying image style and image composition pattern, we examine
whetherdifferentgeneratorsproduceimageswithdistinctcolorschemes.Weuse
100imagesgeneratedfromasetoffixedpromptsforouranalysis.InFig.13,we
visualize the density distribution of pixel values in each RGB color channel. WeDetecting Image Attribution for T2I Diffusion Models in RGB and Beyond 23
discover that Kandinsky 2.1 [58], Midjourney 5.2 [1], and Stable Cascade [53]
oftengenerateimageswithawiderrangeofpixelintensityvalues.InFig.14,we
observe that these three generators often create images with glow and shadow
effects, which can lead to higher and lower intensities.
Prompt: “a couple, a daughter and a corgi walking”
Prompt: “a girl dancing”
Prompt: “a person ride a bike”
Prompt: “two cars, a truck, and an airplane in the cityscape”
Fig.13: DensitydistributionofpixelvaluesinRGBcolorchannelsafteraveraging100
images for each prompt and generator. Kandinsky 2.1 [58], Midjourney 5.2 [1], and
Stable Cascade [53] tend to create images covering a wider range of pixel intensities.
C.2 Frozen vs. Fine-tuned CLIP/DINOv2 Backbone
In Sec. 4.1 of the main paper, we evaluated the accuracy of a frozen CLIP
[56] backbone connected with a linear probe and MLP, and a frozen DINOv2
[52] backbone with a similar configuration. In this section, we compare using a
frozen and fine-tuned backbone for the CLIP and DINOv2 linear probes. Table
5 indicates that a CLIP backbone provides marginally better performance than24 Katherine Xu, Lingzhi Zhang, Jianbo Shi
Kandinsky LCM LCM Midjourney Midjourney SDXL Stable
DALL-E 2 DALL-E 3 2.1 (2 steps) (4 steps) 5.2 6 SD 1.5 SD 2.0 SDXL Turbo Cascade
Prompt: “a couple, a daughter and a corgi walking”
Prompt: “a girl dancing”
Prompt: “a person ride a bike”
Prompt: “two cars, a truck, and an airplane in the cityscape”
Fig.14: Visualizationof100imagesaveragedtogetherforeachpromptandgenerator.
ConsistentwithourobservationsinFig.13,weseethatKandinsky2.1[58],Midjourney
5.2 [1], and Stable Cascade [53] often produce images with glow and shadow effects.
a DINOv2 backbone when the backbone is frozen. However, the reverse holds
true when the backbone is fine-tuned.
CLIP + LP DINOv2 + LP
Backbone Frozen Fine-tuned Frozen Fine-tuned
Accuracy 70.15% 95.31% 67.68% 96.67%
Precision 69.95% 95.51% 67.36% 96.71%
Recall 70.15% 95.32% 67.68% 96.67%
F1 70.00% 95.34% 67.45% 96.67%
Table 5: Quantitative comparison of using a frozen or fine-tuned backbone to train
CLIP[56]andDINOv2[52]linearprobes.CLIPachieveshigheraccuracythanDINOv2
whenthebackboneisfrozen,buttheoppositeistruewhenthebackboneisfine-tuned.
C.3 Image Resolutions
The default EfficientFormer [41] takes inputs of size 224×224. We examine the
performance of using five additional image resolutions between 128×128 and
1024×1024 for the image attribution task. As illustrated on the left side of Fig.
15, accuracy tends to increase as image resolution increases.
C.4 Cropped Image Patches
Ourpreviousexperimentsusemost,ifnotall,imagepixelsfortheimageattribu-
tion task. We also explore the opposite: how few pixels are necessary to achieve
goodperformance?Inspiredby[13,81],wecropasinglepatchofeachimageand
then train EfficientFormer [41] on these patches instead of the full-sized images.
Specifically,wefirstresizeeachoriginalimagetohaveashorteredgeofsize512,
then center crop the image to create a patch of size k ×k, and finally resize
the patch to 224×224. We utilized k = [2,4,8,16,32,64,128,256] and resized
images using bicubic interpolation.
OntherightsideofFig.15,weseethataccuracyincreaseswithimagepatch
size. Remarkably, even training an image attributor on 2×2 patches can lead
to 22.29% accuracy, which is well above the random chance accuracy of 7.69%.Detecting Image Attribution for T2I Diffusion Models in RGB and Beyond 25
Fig.15: Left: Accuracy of our EfficientFormer [41] image attributor across six image
resolutions on the 13-way classification task. In general, accuracy increases as image
resolutionincreases.Right:AccuracyofEfficientFormeracrosseightimagepatchsizes.
Interestingly, using 2×2 image patches can achieve 22.29% accuracy, whereas the
probability of randomly guessing the correct generator is 1 , corresponding to 7.69%.
13
D Elaboration on Results in the Main Paper
In this section, we expand upon the results from the experiments performed in
the main paper. Figure 16 and Table 6 showcase the confusion matrices and
evaluation metrics for the image attributors in Sec. 4.1. Furthermore, Figure 17
andTable7presenttheconfusionmatricesandevaluationmetricsforthecross-
domain generalization study in Sec. 4.1. Additionally, Figure 18 illustrates the
confusion matrices for post-editing enhancements in Sec. 4.3. Lastly, Figure 19
visualizes the averaged segmentation masks across generators for two additional
prompts, which is an extension of our image composition analysis in Sec. 5.
E.F. (scratch) CLIP+LP CLIP+MLP DINOv2+LP DINOv2+MLP
Accuracy 90.03/90.96 70.15/71.44 73.09/74.19 67.68/69.44 71.33/73.08
Precision 90.07/90.98 69.95/71.30 73.13/74.12 67.36/69.09 71.20/72.91
Recall 90.03/90.96 70.15/71.44 73.09/74.19 67.68/69.44 71.33/73.08
F1 90.04/90.96 70.00/71.25 73.07/74.12 67.45/69.17 71.23/72.93
Table 6: Additional quantitative evaluation of image attributors for 13-way classifi-
cation, consisting of 12 generators and a set of real images. The values (percentages)
represent training each attributor Without / With text prompts.
E Grad-CAM Visualizations
Figure 20 showcases the Grad-CAM [28,65] heatmaps for image attributors
trainedonvariousimagetypes,includingtheoriginalRGBimages,imagesafter
high-frequency perturbations, and mid-level representations. We observe that
the image attributors trained on RGB images and images after high-frequency
perturbationstendtopayattentiontosmoothimageregions,suchastheskyor
ground.Nonetheless, eventhoughtheattributors focus onvariedimage regions,
it remains difficult to explain how they make their decisions for each image.26 Katherine Xu, Lingzhi Zhang, Jianbo Shi
Train on MS-COCO Train on GPT-4 Train on Both
Accuracy 89.04/69.24 71.07/79.35 85.78/81.06
Precision 89.07/70.38 71.81/79.29 85.88/80.87
Recall 89.04/69.24 71.07/79.35 85.78/81.06
F1 88.99/68.44 71.06/79.21 85.78/80.86
Table 7: Cross-domain generalization in image attributors. The amount of training
andtestingdatawaskeptconsistentacrosstrials,andanequalnumberofimageswas
sourcedfromMS-COCOandGPT-4promptsforthe‘TrainonBoth’trial.Thevalues
(percentages) represent testing on images from MS-COCO / GPT-4 prompts.Detecting Image Attribution for T2I Diffusion Models in RGB and Beyond 27
Fig.16: Confusion matrices for image attributors in Sec. 4.1. It’s important to note
that the backbone for the CLIP and DINOv2 models are frozen.28 Katherine Xu, Lingzhi Zhang, Jianbo Shi
Fig.17: Confusion matrices for cross-domain generalization in Sec. 4.1.Detecting Image Attribution for T2I Diffusion Models in RGB and Beyond 29
Fig.18: Confusion matrices for evaluating on post-edited images in Sec. 4.3.30 Katherine Xu, Lingzhi Zhang, Jianbo Shi
Prompt: “a girl dancing”
Kandinsky LCM LCM Midjourney Midjourney SDXL Stable
DALL-E 2 DALL-E 3 2.1 (2 steps) (4 steps) 5.2 6 SD 1.5 SD 2.0 SDXL Turbo Cascade
sky 9 sky 12 water 6 window 3 road 4 window 3 artwork 20 road 26 window 3 road 3 window 3 window 6 glasses8 lamp11 road5 chair1 glasses 3 road2 water3 glasses8 artwork3 tree2 water2 water2 handbag 1 vase 7 frisbee 1 road 1 car2 jewelry 2 road 2 handbag 3 jewelry 2 window 2 bed 1
Prompt: “a person ride a bike”
Kandinsky LCM LCM Midjourney Midjourney SDXL Stable
DALL-E 2 DALL-E 3 2.1 (2 steps) (4 steps) 5.2 6 SD 1.5 SD 2.0 SDXL Turbo Cascade
sky 15 sky 32 umbrella 10 road 41 road 74 sky 56 sky 20 road 84 road 80 road 59 road 71 road 100 road15 tree32 car6 sky15 sky33 glasses54 backpack18 sky23 glasses20 glasses35 tree19 sky70 tree 5 road 26 road 5 tree 12 glasses 29 backpack 44road 13 glasses 17 backpack 15 sky 28 sky 10 backpack 59
Fig.19: Additional image composition analyses across generators. We show the aver-
agedsegmentationmasksforeachsemanticclassindicatedontheleftside.Wealsolist
thetopthreeinsertedclassesandthenumberofimages(outof100)withtheseclasses.
Fig.20: Grad-CAM[28,65]visualizationsforimageattributorstrainedoneachimage
type, where each column represents a distinct attributor. The first and third rows
illustrate the Grad-CAM heatmaps overlaid on the input images. The second and
fourth rows show the input images without Grad-CAM. The first example on the top
isbasedonarealimagefromMS-COCO[42],whilethesecondexampleonthebottom
isbasedonafakeimagegeneratedbySDXLTurbo[63].Wenoticethattheattributors
trained on RGB images and images after high-frequency perturbations often focus on
relatively smooth image regions, such as the sky or ground.
lrig
d se et sr se as ln CI
nosrep
ekib
d se et sr se as ln CI