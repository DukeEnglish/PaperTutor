Maximum Likelihood Estimation on Stochastic Blockmodels
for Directed Graph Clustering
Mihai Cucuringu mihai.cucuringu@stats.ox.ac.uk
Department of Statistics and Mathematical Institute
University of Oxford, Oxford, UK
The Alan Turing Institute, London, UK
Xiaowen Dong xiaowen.dong@eng.ox.ac.uk
Department of Engineering
University of Oxford, Oxford, UK
The Alan Turing Institute, London, UK
Ning Zhang ning.zhang@stats.ox.ac.uk
∗
Department of Statistics
University of Oxford, Oxford, UK
Abstract
This paper studies the directed graph clustering problem through the lens of statis-
tics, where we formulate clustering as estimating underlying communities in the directed
stochastic block model (DSBM). We conduct the maximum likelihood estimation (MLE)
on the DSBM and thereby ascertain the most probable community assignment given the
observedgraphstructure. In additionto the statisticalpointof view, we further establish
the equivalence between this MLE formulation and a novel flow optimization heuristic,
which jointly considers two important directed graph statistics: edge density and edge
orientation. Buildingonthisnewformulationofdirectedclustering,weintroducetwoeffi-
cient andinterpretable directedclustering algorithms,a spectral clustering algorithmand
a semidefinite programming based clustering algorithm. We provide a theoretical upper
bound on the number of misclustered vertices of the spectral clustering algorithm using
toolsfrommatrixperturbationtheory. Wecompare,bothquantitativelyandqualitatively,
our proposed algorithms with existing directed clustering methods on both synthetic and
real-worlddata, thus providing further ground to our theoretical contributions.
Keywords: graph clustering, directed graphs, maximum likelihood estimation, spectral
methods, matrix perturbation analysis, semidefinite programming.
Authorsare listed in alphabetical order.
∗ This is thecorresponding author.
1
4202
raM
82
]LM.tats[
1v61591.3042:viXraContents
1 Introduction 3
2 Preliminaries and problem formulation 6
2.1 Basic notations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
2.2 Directed Stochastic Block Model . . . . . . . . . . . . . . . . . . . . . . . . 7
2.3 Problem formulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
3 Main results 9
3.1 Maximum likelihood estimation on DSBM . . . . . . . . . . . . . . . . . . . 9
3.2 A regularized flow optimization interpretation . . . . . . . . . . . . . . . . . 10
3.3 Proposed algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
3.4 Error bound of Algorithm 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
4 Perturbation analysis on Algorithm 1 15
4.1 Bounding perturbation on the top eigenvectors . . . . . . . . . . . . . . . . 16
4.2 Bounding the random perturbation R . . . . . . . . . . . . . . . . . . . . . 17
4.3 Proof of Theorem 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
5 Algorithmic implementation and experiments 19
5.1 Implementation details and complexity analysis . . . . . . . . . . . . . . . . 19
5.2 Experiments on DSBM synthetic data . . . . . . . . . . . . . . . . . . . . . 22
5.3 Experiments on real-world data sets . . . . . . . . . . . . . . . . . . . . . . 24
6 Concluding remarks and discussions 25
A Summary of notations 33
B Proof of Theorem 1 35
C Proofs in perturbation analysis 40
C.1 Proof of Lemma 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
C.2 Useful theorems from matrix perturbation analysis . . . . . . . . . . . . . . 42
C.3 Proof of Lemma 5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42
C.4 Proof of Lemma 6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
C.5 Useful theorem in k-means error analysis . . . . . . . . . . . . . . . . . . . . 47
C.6 Proof of Corollary 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47
D Additional experimental details 50
D.1 Experiments on DSBM with known model parameters . . . . . . . . . . . . 50
D.2 Visualization on directed adjacency matrices . . . . . . . . . . . . . . . . . 51
D.3 Additional real-world dataset . . . . . . . . . . . . . . . . . . . . . . . . . . 52
2D.4 Experiments with different initialization strategies . . . . . . . . . . . . . . 54
1. Introduction
Graph clustering, or community detection, aims to partition a graph into disjoint clusters
(or communities) such that vertices within the same cluster are more “similar” to each
other compared to vertices from different clusters. As one of the most fundamental graph
data analysis methods, graph clustering is well motivated by the need to reveal hidden
patterns in real-world networks, such as segmenting images Shi and Malik (2000), finding
communities in social networks Oliveira and Gama (2012), and detecting pairwise lead-lag
communities in financial networks Bennett et al. (2022).
There has been extensive research on clustering undirected graphs. In particular, a large
volume of work Amini and Levina (2018); Hajek et al. (2016); Montanari and Sen (2016);
Abbe et al. (2015); Amini et al. (2013); Decelle et al. (2011); Chen et al. (2014); Li et al.
(2021)formulates graphclusteringas estimating theplanted communities of randomgraph
models, typically the stochastic block model (SBM) Holland et al. (1983) or its variants
Karrer and Newman (2011); Avrachenkov et al. (2020). These lines of work start with
applying statistical estimation methods, including maximum likelihood estimation and
maximum posterior marginal estimation, in order to estimate community labels. This
estimation procedure subsequently yields a combinatorial optimization formulation of the
graphclusteringproblem,suchasminimizingthecross-communityedgesAbbe et al.(2015)
(similartograph cut Shi and Malik(2000)),andmodularitymaximizationNewman(2016).
These combinatorial optimization problems can further be relaxed or approximated, lead-
ing to computationally efficient clustering algorithms using spectral methods Von Luxburg
(2007);Newman(2013),semidefiniteprogramming(SDP)Amini and Levina(2018);Hajek et al.
(2016); Montanari and Sen (2016); Abbe et al. (2015); Li et al. (2021), and belief propa-
gation algorithms Decelle et al. (2011). Such a methodological framework enables the
integration of statistics, optimization, and spectral methods, and leads to a multi-viewed
understanding of the strengths and limitations of the clustering algorithms.
While most of the existing studies on graph clustering are only targeted for undirected
graphs, in a number of important applications, the relationships between vertices are
not symmetric, as seen in causal relationships Pearl and Verma (1987), interbank debt
Acemoglu et al. (2015), and paper citations An et al. (2004). Employing undirected graph
representation in the aforementioned scenarios results in a loss of valuable information
pertaining to edge directionality. This underscores the necessity for studying the directed
graph clustering problem and developing fast and robust algorithms customized to the
specific task at hand. While it is not applicable to naively transfer undirected clustering
analysis to directed graphs due to the lack of symmetry of the adjacency matrix, a number
3ofstudiesattempttocircumventthischallengeindifferentwaysMalliaros and Vazirgiannis
(2013); we survey below two closely related approaches.
Oneline of work suggests a two-step framework, where they firstrestore the graph symme-
try and then apply classical undirected clustering algorithms. For example, some propose
to cluster the bibliographic coupling matrix AAT Kessler (1963), the co-citation matrix
ATASmall(1973),andbibliometricsymmetrizationAAT+ATASatuluri and Parthasarathy
(2011). Performing clustering on such product of graph adjacency matrices implicitly
groups vertices based on some notion of higher-order structure. For instance, constructing
AAT involves counting the number of common “offspring” vertices. In a similar spirit,
Rohe et al. (2016) proposes a spectral algorithm denoted as DI-SIM, which builds on a
dual measure of similarity from both “parent” vertices and “offspring” vertices.
Another more recent line of research adopts complex-valued Hermitian matrices to repre-
sent directed graphs. These studies explore the graph structural information through the
analysisofthespectrumofHermitianmatrices,whichleadstothecreationofavarietyofdi-
rected clustering algorithms Cucuringu et al. (2020); Fanuel et al. (2017); Laenen and Sun
(2020). In Cucuringu et al. (2020), the authors suggest using a skew-symmetric Hermi-
tian matrix, employing i to representdirected edges, and subsequentlyclusteringvertices
±
basedontheirembeddingintheassociatedeigenspace. Laterfollow-upworksHayashi et al.
(2022); Laenen (2019) also point out a connection between the Hermitian clustering algo-
rithm inCucuringu et al. (2020) andan optimization heuristicthat maximizes the netflow
between two clusters. In Laenen and Sun (2020), the authors choose the k-th root of unity
toindicateadirectededge. Theydefineaflowratiooptimization schemeandshowthatthe
optimal partition is given at the bottom eigenspace of their proposed Hermitian matrix.
Other than optimization heuristics, the authors in Fanuel et al. (2017) also relate directed
clustering to quantum physics heuristics. They apply the Magnetic Laplacian for spectral
clustering on directed graphs, wherethis combinatorial Laplacian was initially designed for
studying the quantum dynamics of particles under the influence of magnetic fluxes Shubin
(1994).
Despite thesuccess of the aforementioned methodsin a variety of directed clustering tasks,
there remains a significant lack of insights from a statistical perspective. Such a point of
view is helpful as it allows one to justify why a particular clustering algorithm design is
deemed suitable based on the statistical properties of the data, i.e., the structures and
randomness inherent in random graph models. The rationales behind existing directed
clustering algorithms rely on prespecified favoured patterns or optimization heuristics. For
example, Cucuringu et al. (2020); Hayashi et al. (2022) optimize the between-cluster net
flow, whileLaenen and Sun(2020); Rohe et al. (2016);Satuluri and Parthasarathy (2011);
Small (1973); Kessler (1963) count higher-order patterns. These algorithms, customized
for certain prespecified patterns, have weak connections to the properties of the real-world
data being studied, and there is a lack of rigorous arguments as to why and when those
4methods are desirable or effective. This motivates us to explore a design that comes with
a theoretical justification based on the statistical properties of the data. In this paper, we
studythemaximumlikelihoodestimationontheDirectedStochasticBlockModel(DSBM),
anditsimplicationforthedevelopmentofefficientdirectedclusteringalgorithms. Ourwork
is mainly inspired by the multi-viewed research on the statistical estimation of the SBM
and the Hermitian matrix representations of directed graphs. We summarize our main
contributions as follows.
1. Problem formulation. Under the DSBM setting, we propose to formulate a max-
imum likelihood estimation (MLE) on the community labelling, where we maximize
the log-likelihood function and thereby ascertain the most probable community as-
signment given the observed graph structure. Building on the MLE derivation, we
introduce a novel Hermitian matrix representation for clustering directed graphs.
In contrast to Hermitian matrices proposed in earlier studies Fanuel et al. (2017);
Cucuringu et al. (2020); Laenen and Sun (2020), where the underlying optimization
interpretations are heuristic, our proposed Hermitian matrix is derived from a well-
establishedstatisticalestimation method. TheconnectionbetweenourproposedHer-
mitian matrix and MLE is established through the observation that the quadratic
form of the Hermitian matrix associated with the complex community indicator vec-
tor,correspondstotheoptimizationobjectiveoftheMLE(Theorem1). Furthermore,
we establish the equivalence between the MLE on DSBM and a new (regularized)
flow optimization heuristic, as detailed in Section 3.2. This optimization heuristic
jointly considers both edge density and edge orientation when clustering directed
graphs. Compared with existing optimization heuristics that also take into account
thecross-cluster edgedensity andorientation Fanuel et al. (2017,2018), ourformula-
tion provides a theoretical justification for how we balance the weighting parameters
of the two terms. This flow optimization interpretation also extends the flexibility
of our formulation, as it allows one to go beyond the assumptions on the underlying
statistical model, and customize the weighting parameter between the cost incurred
fromtheedgedensityandtheoneincurredfromtheedgeorientations, basedonone’s
domain knowledge.
2 Efficient algorithms. Based on the above theoretical framework, we introduce two
newdirected clusteringalgorithms, aspectralclusteringalgorithm (Algorithm 1)and
asemidefiniteprogramming(SDP)clusteringalgorithm (Algorithm 2), bothofwhich
are derived through relaxing the combinatorial optimization problem induced by the
MLE. As these MLE-driven algorithms require the DSBM parameters as input, we
adapt the iterative approach from Newman (2016) to learn the model parameters
from the observable graph data. The combination of our proposed algorithms and
the interactive learning method not only enables efficient computation on clusters,
but also ensures a self-adaptive process that operates without the need for any prior
knowledge. We compare our algorithms, both quantitatively and qualitatively, with
5existing directed clustering methods on synthetic data (Section 5.2) and real-world
data(Section 5.3). From theseexperiments, weobservethatourproposedalgorithms
consistently outperform existing methods in most of the cases we have experimented
with, thus providing further ground to our theoretical contributions.
3. Theoreticalguarantee. Wepresentatheoreticalupperboundontherecoveryerror
of the spectral clustering algorithm (Algorithm 1). For graphs generated from the
DSBM, we establish, in Theorem 2, a high-probability upper bound on the number
of misclustered vertices using tools from matrix perturbation analysis. Prior work
Cucuringu et al. (2020) also proves an upper bound on the misclustering error on
DSBM. The main difference between our bound and prior work lies in the DSBM
setting. While Cucuringu et al. (2020) considers multiple equal-sized clusters with
uniformedgedensity,i.e.,p = q,thisstudyfocusesonamoregeneraltwo-clustercase,
where our analysis also covers scenarios with inhomogeneous edge density (p = q)
6
and unequal cluster size (n = n ). Furthermore, the analysis in Cucuringu et al.
1 2
6
(2020) involves adirected application of the Davis-Kahan theorem, whichimposes an
extra condition to ensure the gap between eigenvalues of the population matrix E[H]
and the sample matrix H to be large enough. We circumvent this issue by adapting
a variant of the Davis-Kahan theorem from Vu et al. (2013); Yu et al. (2015) into
our proof, which allows us to derive an error bound that only considers eigenvalues
of the population matrix.
The rest of the paper is organized as follows. In Section 2 we provide notations and
background, and introduce the clustering problem under consideration. In Section 3, we
introduce our statistical estimation formulation and summarize the main results of this
study. In Section 4, we sketch the proof for the error bound on the Algorithm 1, and defer
some of the proof details to the appendix. In Section 5, we explain the implementation
details of our proposed algorithms, and report experimental results on both synthetic and
real-world data sets. In Section 6, we discuss the advantages and limitations of our work,
and highlight potential future improvements and avenues of research.
2. Preliminaries and problem formulation
2.1 Basic notations
Let G( , ) be a directed graph on vertex set and edge set . For a pair of vertices
V E V E
u,v , we denote u v if there is an edge pointing from u to v and we denote u v
∈ V 6∼
if there is no edge between u and v. The edge set is a collection of ordered vertices pairs,
where (u,v) , we have u v. A directed graph can be represented by its adjacency
∀ ∈ E
matrix A 0,1 N N, where A = 1 iff u v. This research studies two-cluster directed
× uv
∈ { }
graphs,andwedenotetheclustersas and ,where , aredisjointsubsetsof .
1 2 1 2
C C C C V
6This study involves both real-valued matrices in Rn n and complex-valued matrices in
×
Cn n. We use i to denote the imaginary unit √ 1. For x C, we denote the conjugate
×
− ∈
of x as x, and use x = to represent the norm of x. Let () be the operation of taking
| | ℜ ·
the real part of a complex number, and () denote taking the imaginary part. When the
ℑ ·
inputs of () and () are matrices, we consider them as elementwise operations on every
ℜ · ℑ ·
entry of the matrix.
For a matrix H, we use HT to denote its transpose and H to denote the conjugate
∗
transposeand H = HT . Let n n betheset of Hermitian matrices of size n, where H
∗ ×
H ∀ ∈
n n we have H = H . For an arbitrary Hermitian matrix H, it has n real eigenvalues,
× ∗
H
and throughout this paper, the eigenvalues are consistently organized in descending order
of magnitude, i.e., λ (H) λ (H) λ (H). We also employ several commonly
1 2 n
| | ≥ | | ≥ ···| |
used matrices: we use I to denote the identity matrix of size n, and J to denote the
n n
square all-one matrix of size n, while 1 is used to represent the all-one vector of length n.
n
When the matrix dimension can be inferred from the context, we may drop the subscript
for the sake of conciseness. This paper makes use of several matrix norms: we use H to
k k
denotethespectralnorm,whichisthelargestmagnitudeofanyeigenvalues λ (H). Welet
1
| |
H denote the Frobenius norm, where H = λ2(H). For two matrices H ,H
k kF k kF j j 1 2
with the same number of rows, we use [H 1,H 2] to dqen Pote a new matrix by concatenating
thecolumnsof H andH . Letdiag(H)denotetheoperator thatcreates adiagonal matrix
1 2
by considering the main diagonal elements of M. We use Tr(H) to denote the trace of the
matrix H.
This paper considers large graphs, where we provide both non-asymptotic and asymptotic
analysis. Whenitcomes toasymptotic analysis, i.e., consideringthegraphsizenconverges
to infinity, we use the big-O notation as conventions: we use g = o(f ) to denote that
n n
g
n
g is asymptotically dominate by f , i.e., lim = 0. We use g = O(f ) when g is
n n n n n
n f n
→∞ g
n
asymptotically bounded above by f , i.e., limsup < . We denote g = Θ(f ) as g
n n n n
n f n ∞
is bounded both above and below by f asym→p∞totically, in other words, g = O(f ) and
n n n
g
n
f = O(g ). We denote g = Ω(f ) as g is bounded below by f , i.e., limsup > 0. We
n n n n n n
f
n n
g n →∞
denote g dominate f asymptotically as g = ω(f ), where limsup = . We refer to
n n n n
n f n ∞
Table 2 in Appendix A for a summary of notions used in this p→ap∞er.
2.2 Directed Stochastic Block Model
TheDSBMisagenerativemodelforrandomdirectedgraphs,initiallyproposedinCucuringu et al.
(2020). In this paper, we specialize their setting to the two-community case, consisting
of the source community of size n and the target community of size n . Here,
1 1 2 2
C C
the community membership labels are considered as fixed parameters that are unknown,
7instead of treated as latent variables. We introduce a vector σ to indicate the community
membership in the DSBM where σ = σ iff vertex u,v belongs to the same community.
u v
The DSBM encodes the community information into the graph topology through the edge
density parameters p,q and the edge orientation parameter η. To be more specific, for a
directed graph sampled from the DSBM (n ,n ,p,q,η), the edges are generated indepen-
1 2
dently conditioning on the community labelling σ as follows
• if a pair of vertices u and v belongs to the same cluster, then with probability (w.p.)
p there exists an edge between them. This within-community edge is unordered in
the sense that the probability of this edge pointing from u to v is the same as the
probability of it pointing from v to u. In other words, we have that
A = 1,A = 0 w.p. p/2,
uv vu
A = 0,A = 1 w.p. p/2,
 uv vu
 A = A = 0 w.p. 1 p.
uv vu
−

• if u and v , the n wehave that theedgeexists withprobability q. Thecross-
1 2
∈ C ∈ C
community edge is oriented in the sense that the probability of this edge pointing
from the vertex to the vertex is 1 η, i.e.,
1 2
C C −
A = 1,A = 0 w.p. (1 η)q ,
uv vu
−
A = 0,A = 1 w.p. ηq,
 uv vu
 A = A = 0 w.p. 1 q.
uv vu
−


Therefore, the conditional probability of a directed graph with adjacency matrix A can be
written as
P(Aσ) = P(A σ ,σ ),
uv u v
| |
(u<v)
Y
where the probability distribution of a vertex pair P(A σ ,σ ) is specified in the above
uv u v
|
discussed six cases.
Weusep = max p,q todenotethemaximumedgeprobabilityinDSBM(n ,n ,p,q,η).
max 1 2
{ }
Throughout the discussion in this paper, we assume that the maximum edge probability
is above the connectivity threshold Frieze and Karon´ski (2016), i.e.,
p = Ω(logN/N). (A-1)
max
This assumption is a necessary condition for us to provide a theoretical upper bound on
the number of misclustered vertices. Otherwise, if p = o(logN/N), then, with high
max
probability, the sampled graphs contain multiple components, and there is no proper way
to determine a cluster membership assignment over different components.
82.3 Problem formulation
This study takes a statistical point of view on the clustering problem, considering a cluster
as a collection of statistically equivalent vertices. Following this line of thought, we assume
graphs are generated from the DSBM (n ,n ,p,q,η), where the edge generating process
1 2
betweenapairofverticesdependsonlyontherespectivecommunitiestowhichtheybelong.
Ourgoalofclusteringistoestimatetheunderlyingcommunitiesonlybasedontheobserved
graph topology, without any additional side information on the underlying community
structure. We use the indicator vector σ to denote the ground truth community labelling
of the DSBM, and we use σˆ to denote the estimated community labelling vector. We
evaluate the efficacy of the community recovery σˆ by counting the number of misclustered
vertices l(σ,σˆ), where
l(σ,σˆ) = 1(σ = σˆ ).
j j
6
j
X∈V
3. Main results
3.1 Maximum likelihood estimation on DSBM
GivenagraphwithadjacencymatrixAgeneratedfromtheDSBM,weinferthecommunity
labels in a way that renders the observed graph topology most probable. This intuition
is formally achieved by applying the maximum likelihood estimation (MLE), where the
desired community labelling maximizes the log-likelihood function. To be more concrete,
we consider a directed graph with adjacency matrix A sampled from the DSBM, and we
also assume that the DSBM parameters p,q,η are known. Then, the MLE on community
assignments can be formally written as
σˆ = argmax (A;σ), (1)
MLE
σ L
where (A;σ) is the log-likelihood function and
L
(A;σ) = log(P(A σ ,σ )). (2)
u,v u v
L |
u<v
X
We present in Theorem 1 the explicit optimization formulation derived from the MLE
on DSBM (1). The key step in the derivation is to describe the optimization objective,
the log-likelihood function (A;σ). We manage this simply by grouping the terms in (2)
L
accordingtothecommunityassignmentofvertexpairsineachterm,whichwesummarizein
Lemma7. Wethenconvertthereal-valuedoptimizationprobleminLemma7toitscomplex
equivalence. The purpose of this conversion is to obtain a more compact expression of the
optimization problem, where the optimization objective is a simple quadratic form. The
detailed proof of Theorem 1 can be found in Appendix B.
9Theorem 1 (MLE on DSBM) Consider a directed graph with adjacency matrix A gen-
erated from the model DSBM(n ,n ,p,q,η). Let x i,1 N be the indicator vector, where
1 2
∈ { }
x = i if u , and x = 1 if u . The maximum likelihood estimation on the
u 1 u 2
∈ C ∈ C
community labels is equivalent to solving the following complex optimization problem
max x Hx (Herm-MLE)
∗
s.t. x 1,i N,
∈e{ }
where H is a Hermitian matrix defined as
1 η p2(1 p)2 1 p
H =eilog − (A AT)+log − (A+AT)+2log − (J I) (3)
η − 4η(1 η)q2(1 q)2 1 q −
− − −
e , w ii(A AT)+w r(A+AT)+w c(J I).
− −
3.2 A regularized flow optimization interpretation
In this section, we present a different view on the (Herm-MLE) problem by relating it to
a flow optimization heuristic. In particular, we explain how the real and imaginary parts
of the Hermitian matrix contribute to the edge density optimization and edge orienta-
tion optimization separately. This alternative view extends the flexibility of our proposed
methodology, as it allows one to consider different matrix constructions that depend on
their own judgment about what a “good” metric for modularity in directed graphs ought
to be.
We start by defininggraph statistics that are of interest in directed clustering tasks. Given
two clusters , in a directed graph, we use TF( , ) to denote the total flow, which
1 2 1 2
C C C C
is the number of cross-cluster edges given by
TF( , )= (A +A ).
1 2 uv vu
C C
u ∈CX1v ∈C2
We use NF( , ) to denote the net flow from to , which is the number of edges
1 2 1 2
C C C C
pointing from to subtracting the number of edges from to
1 2 2 1
C C C C
NF( , ) = (A A ).
1 2 uv vu
C C −
u ∈CX1v ∈C2
Recall thatthe Hermitian matrix we derived is H = w (A+AT)+iw (A AT)+w (J I),
r i c
− −
and the objective in (Herm-MLE) is
e
x Hx= w x (A+AT)x+iw x (A AT)x+w x (J I)x, (4)
∗ r ∗ i ∗ c ∗
− −
where the indicator vector x 1,i N and x = i if u and x = 1 if u .
e u 1 u 2
∈ { } ∈ C ∈ C
10For thefirstterm in(4), onecan easily verify that x (A+AT)xcounts thenumberof edges
∗
within plus the number of edges within , which can be equivalently written as
1 2
C C
1
x (A+AT)x = C TF( , ), (5)
∗ 1 2
2 − C C
where the constant C is the total number of edges in the graph.
For the second term in (4), one can derive that ix (A AT)x is the number of edges
∗
−
pointing from to subtracting the number of edges from to , and thus
1 2 2 1
C C C C
i
x (A AT)x = NF( , ). (6)
∗ 1 2
2 − C C
The last term in (4) is weighted by w = log(1 p) log(1 q) = O(p q), which is a very
c
− − − −
small constant. If we ignore this term for the moment, we then have that (Herm-MLE)
optimizesaweightedcombinationofnetflowandtotalflow: w TF( , )+w NF( , ).
r 1 2 i 1 2
− C C C C
This optimization view agrees with the intuition that we aim to find a proper partition
that considers both the edge density difference and the edge orientation between clusters.
This optimization heuristic also shares a similar intuition with the cut imbalance ratio
|NF( C1, C2)
| from Cucuringu et al. (2020), where the authors proposed it as a measure of
2TF( 1, 2)
edgeCimCbalance between clusters.
Thelasttermin(4)x (J I)xsimplycalculates 1(n2+n2). Bymaximizingit,weimplicitly
∗ − 2 1 2
penalize or encourage imbalanced clusters, depending on the sign of w . Adding an all-
c
one matrix into the graph representation matrix also appears in previous studies, where
this technique is known as regularization. Existing studies such as Amini et al. (2013);
Joseph and Yu (2016); Le et al. (2017) have shown both theoretically and empirically that
adding a regularization term will improve the performance of spectral clustering in the
sparse regime, where the graph edge density is p,q = o(logN), even as low as p,q = Θ(1)
N n
which is the very sparse regime often arising in certain applications involving large-scale
graphs. All the above studies only consider undirected graphs, with a real-valued matrix
representation, and there is no analysis known for directed graphs.
Statistical estimation v.s. combinatorial optimization. Although in this section we
have demonstrated the equivalence between MLE and the regularized flow optimization,
it is still worth mentioning the difference between an optimization view and a statisti-
cal view. From a statistical perspective, clustering aims at recovering the probabilistic
equivalent structure, while from an optimization standpoint, clustering is purely driven by
finding structures that optimize the objectives, which can be irrelevant to the underlying
data-generating model. In cases where the underlying probabilistic models are unknown
or mathematically intractable, an optimization objective can explicitly guide the goal of
clustering and serve as a metric for evaluating cluster results. In this vein, our flow op-
timization framework is ready to be extended to a more general weighted graph setting
11without specifying a generating model. In addition to model independence, an optimiza-
tion framework also naturally provides a way to integrate prior knowledge as constraints,
leading to new constrained optimization formulations on directed clustering.
3.3 Proposed algorithms
In Section 3.1, we derive the optimization formulation for directed clustering (Herm-MLE)
through applying MLE on the DSBM, and in Section 3.2, we discuss the heuristic interpre-
tation for this problem. We now move on to presenting two efficient algorithms for solving
the optimization problem (Herm-MLE). First note that exactly solving the above com-
binatorial optimization problem (Herm-MLE) involves enumerating all 2N combinations
in the worst case and is an NP-hard problem. For computational efficiency, we consider
relaxing the problem (Herm-MLE) to some continuous convex domain and then project-
ing the relaxed solutions back to the indicator vectors. In this study, we introduce the
following two relaxations: spectral relaxation and SDP relaxation, which lead to two new
directed graph clustering algorithms, the spectral clustering algorithm (Algorithm 1) and
SDP clustering algorithm (Algorithm 2).
The spectral relaxation. The general idea here is to relax the integer constraints in
(Herm-MLE) to the continuous complex domain, and we arrive at the following new opti-
mization problem
max x Hx (SC-MLE)
∗
s.t. x CN
∈e
x 2 = N
k k2
Here, thecontinuous optimization problem(SC-MLE)isanalytically solvable, anditsmax-
imum is attained when x is a rescaled (by √N) version of the leading eigenvector of H.
Note that the top eigenvector is not unique in the sense that its multiplication with eiθ
is also a top eigenvector for any θ [0,2π]. For this reason, we use the k-means algeo-
∈
rithm to project the relaxed solution back to a complex indicator vector, which produces
rotation-invariant results. We formally present the directed spectral clustering algorithm
in Algorithm 1. To provide further intuition on this spectral algorithm, we visualize in
Figure 1b the vertex embedding given by the top eigenvector, which is the key step of
Algorithm 1.
The SDP relaxation. Another commonly used relaxation, which relies on semidefinite
programming, considers lifting the vector variables x CN to a matrix. Because the
∈
objective in (Herm-MLE) has x Hx = Tr(Hxx ), through defining X = xx we obtain
∗ ∗ ∗
x Hx = Tr(HX). Correspondingly, the integer constraints x 1,i N amount to con-
∗
∈ { }
e e
e e
12Input : directed graph G( , ), DSBM parameters p,q,η
V E { }
Output: community labels σˆ
1 Compute the Hermitian matrix H according to (3) ;
2 Compute the top eigenvector vˆ of H ;
3 Apply k-means on the matrix [ e(vˆ), (vˆ)] to partition into 2 clusters;
ℜ ℑ V
Algorithm 1: MLE Speectral Clustering Algorithm
straints on X, and we further relax them to obtain the following Hermitian SDP
max Tr(HX) (SDP-MLE)
s.t. X N N
×
∈eH
X 0
(cid:23)
diag(X) = I.
Althoughmoreintricatethanthespectralrelaxation, thisSDPcanstillbesolvedefficiently
with standard optimization tools, which we discuss in more detail in Section 5. To project
the SDP solution X back to an indicator vector, we first compute the leading eigenvector
of X which provides the best rank-1 approximation of it. Then we apply the k-means
algorithm on the embedding space given by this eigenvector, and obtain the two-cluster
partition of the graph. We summarize the implementation steps in Algorithm 2. The
embedding space given by the leading eigenvector of X is visualized in Figure 1c.
Input : directed graph G( , ), DSBM parameters p,q,η
V E { }
Output: community labels σˆ
1 Compute the Hermitian matrix according to (3);
2 Solve (SDP-MLE) and compute the top eigenvector vˆ of the SDP solution;
3 Apply k-means on the matrix [ (vˆ), (vˆ)] to partition into 2 clusters.
ℜ ℑ V
Algorithm 2: MLE SDP Clustering Algorithm
The difference between the algorithms stems from their choice of relaxation. Compared
withthespectralrelaxation,theSDPrelaxationapproachpreservestheconstraintdiag(X) =
I; therefore, we would expect that the SDP algorithm would yield clustering results closer
to the solutions from the original combinatorial optimization problem (Herm-MLE) com-
pared to the spectral clustering. This intuition is validated in experiments by comparing
the two algorithms on the synthetic dataset sampled from DSBMs (see Figure 3, Figure 4
in Section 5.2 and Figure 8 in Appendix D.1), where we observe that Algorithm 2 consis-
tently performs slightly better than Algorithm 1 over all tested datasets. In addition, this
SDP optimization framework can accommodate various linear constraints, thereby offer-
ing flexibility to incorporate additional side information such as cluster sizes. Despite the
aforementioned advantages of the SDP approach, we highlight that the spectral clustering
131 0.05 0.05
100 0.04
c c1
2 0.04
c c1
2
200 0.03 0.03
300 0.02 0.02
400 0.01 0.01
500 0 0 0
600 -0.01 -0.01
700 -0.02 -0.02
800 -0.03 -0.03
900 -0.04 -0.04
1000 100 200 300 400 500 600 700 800 900 1000 -1 -0.05-0.04 -0.02 0 0.02 0.04 0.06 0.08 -0.05 -0.04 -0.02 0 0.02 0.04 0.06
T
(a) Directed graph A A (b) Spectral embedding (c) SDP embedding
−
Figure 1: We sample a directed graph A from the DSBM with n =500,n =500,p = q =
1 2
5%,η = 5%. To highlight the direction of each edge, we visualize A AT in (a), where a
−
bluepointrepresentsadirectededgefromitsrowindextothecolumnindexandaredpixel
means the direction is from the column index to the row index. Given this directed graph,
we compute H using (3) and plot in (b) the top eigenvector of H that gives the embedding
of the vertex that separates the two clusters. The SDP embedding in (c) is obtained by
first solving (eSDP-MLE) and then plotting the top eigenvectoreof the SDP solution. Note
that compared with spectral embedding, vertices in (c) reside on the unit circle, which is
mainly due to the extra linear constraint diag(X) = I in the SDP relaxation.
algorithm is computationally more efficient than the SDP clustering algorithm. There-
fore, the spectral algorithm is particularly more suitable for fast community discovery and
clustering large-scale networks.
3.4 Error bound of Algorithm 1
Inthissection, wepresentanupperboundonthenumberofmisclusteredvertices usingthe
spectral clustering algorithm. We use σ to denote the ground truth community indicating
vectors, and σˆ denote the clustering result of Algorithm 1. Recall that we define l(σ,σˆ) as
the number of misclustered vertices, where l(σ,σˆ)= 1(σ = σˆ ). While some patholog-
j j 6 j
ical inputgraphinstances may resultinrelatively large values of l(σ,σˆ), wedonotfocuson
P
providing guarantees for these worst-case scenarios. Instead, our focus lies on the majority
of instances sampled from the DSBM, and we prove that with high probability, graphs
sampled from the DSBM have bounded misclustering error, under Algorithm 1.
Theorem 2 (Error bound on Algorithm 1) For a graph generated from the DSBM
(n ,n ,p,q,η), let σˆ be the (1 + ǫ)-approximate solution of k-means from Algorithm 1.
1 2
Then there exists C = Θ w2+w2 (see (13)) and an absolute constant ǫ , such that
r i 0
with probability at least 1 (cid:16)qN ǫ0, the(cid:17)error rate is such that
−
−
l(σ,σˆ) 64(2+ǫ)C2p logN
max
. (7)
N ≤ d2∆2
14Here ∆ lower bounds the eigengap λ (E[H]) λ (E[H]) and its expression is given in (11);
1 2
| − |
d is the distance between the two cluster centroids of the population version E[H], with its
expression provided in (28). e e
e
The error bound that we obtain in Theorem 2 is inversely proportional to d2 and ∆2.
Both d and ∆ are determined by the population matrix E[H]. The entries in E[H] are
community-dependent and their values are determined by the model parameters p,q, and
η, so as the centroid distance d and ∆. To establish a moreeexplicit connection beetween
the error bound (7) and the DSBM parameters, we explore a particular case that provides
analytical forms on the error bound, and present in Corollary 3 interpretable error bounds
on DSBM (N/2,N/2,p,p,η). The proof of Corollary 3 can be found in Appendix C.6.
Corollary 3 Consider directed graphs generated from the DSBM (N/2,N/2,p,p,η). As
N , if η 0.5 ǫ with an absolute constant ǫ > 0, then the misclustering error of
→ ∞ ≤ −
Algorithm 1 is such that
l(σ,σˆ) logN
= O (8)
N Np
(cid:18) (cid:19)
If η = 0.5 o(1), we have that
−
l(σ,σˆ) logN
= ω . (9)
N Np
(cid:18) (cid:19)
From (8) and (9), we infer that the error bound is inversely proportional to the average
degreeNp. Thisalignswithourintuition thatDSBMs withalarger average degreeprovide
more observations on edge orientations, rendering the generated graph more informative,
and consequently lead to a smaller clustering error. Furthermore, for the noise level η,
when η 0.5, the cross-cluster edges become nearly disordered and thus boost up the
→
clustering error. This intuition is reflected in the different orders of the upper bounds
provided in (8) and (9).
4. Perturbation analysis on Algorithm 1
The key idea behind the spectral clustering algorithm (Algorithm 1) is that eigenvectors of
the data matrices contain crucial information revealing the underlying community struc-
ture. Here, we rigorously articulate this using matrix perturbation analysis, and derive the
error bound (7) as presented in Theorem 2. Our analysis builds on the following simple
intuition: for directed graphs sampled from the DSBM, the expected value of our proposed
Hermitian matrix E[H], also known as the population version of the matrix H, has a clear
block-wise structure, and its top eigenvector v = v (E[H]) has exactly two distinct values
1
e e
e
15that perfectly indicate the true community labels. In practice, however, this well-behaved
matrix E[H] is unobservable , and we consider the observable Hermitian matrix H as a
corrupted version of E[H]. We know from classical matrix perturbation analysis that, if
H deviatesenot far away from its population version E[H], one would expect that tehe top
eigenvector of vˆ = v (H) might continue to be informative, leading to a relatively good
1
reecovery of the true community labels. Our proof of thee error bounds follows a standard
procedure, with the folloewing main steps
(i) Usingmatrixperturbationtheory,wecharacterizetheeigenvectorperturbation vv
∗
k −
vˆvˆ in Section 4.1;
∗ F
k
(ii) UsingtheMatrix-Bernsteininequalityfromrandommatrixtheory,weprovideahigh-
probability upper bound on the random perturbation H E[H] in Section 4.2;
k − k
(iii) Combining the results from the above two steps, we perform an error analysis on
e e
the k-means clustering step, and present the final spectral clustering error bound in
Section 4.3.
4.1 Bounding perturbation on the top eigenvectors
We first characterize properties on the eigenspace of the population matrix and show that
v = v (E[H]) perfectly recovers the community labels. Then we upper bound how far vˆ
1
deviates from v using the Davis-Kahan Theorem on eigenspace perturbation.
e
Eigenspace of the population version – the ideal case.
For a directed graph generated from DSBM with two communities, entries of the matrix
E[H]havecommunity-dependentvalues. Toexpressthisinacompact way, weuseacluster
membership matrix M 0,1 N 2 to represent the ground truth community labelling,
×
∈ { }
wheere M = 1 indicates that vertex u belongs to the community , and M = 1 denotes
u1 1 u2
C
u . We consider the two communities to have no shared vertices, and thus the two
2
∈ C
column vectors of M are orthogonal. Using the membership matrix, we can write out E[H]
as follows
e
E(H) = MQMT (pw +w )I,
r c
−
0 1 p q 1 1
eQ , iw i(1 2η)q +w
r
+w
c
.
− 1 0 q p 1 1
(cid:20)− (cid:21) (cid:20) (cid:21) (cid:20) (cid:21)
By defining the 2 2 matrix Q, we have (MQMT) = Q where c() : 1,2 is
u,v c(u),c(v)
× · V → { }
a function that maps a vertex to its true community. The matrix MQMT is of rank two,
and its top eigenvector has exactly two distinct values, indicating the community labels.
SinceH andMQMT sharethesame top eigenvector, wecan easily calculate theeigenpairs
ofQandthencombinethemwithM toobtaintheeigenvectors ofE[H]. Wesummarizethe
e
e
16eigenspacepropertiesofE[H]inthefollowingLemma4,andleavethedetailedcomputation
to Appendix C.1
e
Lemma 4 For the DSBM(n ,n ,p,q,η), the population version of the proposed Hermitian
1 2
matrix E[H] has a unique largest eigenvalue. The top eigenvector v has exactly two distinct
values that indicate the community labels where the distance between them d can be easily
computedeusing (28). Moreover, the eigengap
λ (E[H]) λ (E[H]) = min 2∆, 1/2N(w p+w ) +∆ ∆. (10)
1 2 r c
| − | { | | }≥
where e e
1
∆ = N2(w p+w )2 4n n ((w p+w )2 w +w +iw (1 2η)q 2) (11)
r c 1 2 r c r c i
2 − −| − |
p
Perturbation analysis on the top eigenvectors.
For the DSBM(n ,n ,p,q,η), the population matrix E[H] is unobservable, but from the
1 2
generated graphs we are able to construct the matrix H. We consider H as a perturbed
version of E[H] and use R = H E[H] to denote the ranedom perturbation. From matrix
−
perturbationanalysis, theperturbationontheeigenspaceeandeigenvaluesearecharacterized
by two well-kenown theorems: eDavis-Keahan’s theorem (Theorem 9) and Wyle’s inequality
(Theorem 10). Applyingthe above two theorems directly to boundtheeigenspace distance
vv vˆvˆ requires a sufficient gap between eigenvalues of E[H] and H, which may
∗ ∗ F
k − k
impose extra conditions for this bound to be valid. We circumvent this by adapting a
variantofeigenspaceperturbationboundfromVu et al.(2013)intooeurproofe,whichallows
us to derive an upper bound of vv vˆvˆ that involves only comparing eigenvalues of
∗ ∗ F
k − k
E[H]. We summarize our eigenspace perturbation result in Lemma 5 and defer the proof
details to appendix C.3.
e
Lemma 5 Given a directed graph from DSBM(n ,n ,p,q,η) and its Hermitian matrix
1 2
representation H, the projection matrix of the top eigenvector has
R
e vv ∗ vˆvˆ ∗ F 2√2 k k .
k − k ≤ λ (E[H]) λ (E[H])
1 2
−
4.2 Bounding the random perturbation R e e
The goal of this section is to establish a high-probability upper bound on R , where R is
k k
the random perturbation with the following form
R = H E[H]
−
0 i p q
= iew i(A eAT) (1 2η)qM MT +w r(A+AT) w rM M w rpI.
− − − i 0 − q p −
(cid:20)− (cid:21) (cid:20) (cid:21)
17Fromtheexpressionabove,weobservethateachentryofRhasaboundedabsolutevalueas
well as aboundedvariance. Therefore, usingtheMatrix Bernstein inequality on Hermitian
matrices(Lemma12),wecandirectlyobtainahigh-probabilityupperboundon R ,which
k k
wepresentinLemma6. TheproofdetailsofLemma6canbefoundinAppendixC.4.
Lemma 6 (Bound on random perturbation R) Consideradirectedgraph fromDSBM
(n ,n ,p,q,η) and its Hermitian matrix representation H. We use p = max p,q to
1 2 max
{ }
denote the maximum edge probability. Assume that the maximum edge probability is above
the connectivity threshold, i.e., e
Np = Ω(logN). (12)
max
Then there exist an absolute constant ǫ and
logN
C = (2+ǫ) w2+w2 +1 = Θ w2+w2 , (13)
r i Np r i
q (cid:18) max (cid:19) (cid:18)q (cid:19)
such that the random perturbation R = H E[H] has
−
P( R C eNp maxelogN) N −ǫ.
k k ≥ ≤
p
4.3 Proof of Theorem 2
Theorem 2 (Error bound on Algorithm 1) For a graph generated from the DSBM
(n ,n ,p,q,η), let σˆ be the (1 + ǫ)-approximate solution of k-means from Algorithm 1.
1 2
Then there exists C = Θ w2+w2 (see (13)) and an absolute constant ǫ , such that
r i 0
with probability at least 1 (cid:16)qN ǫ0, the(cid:17)error rate is such that
−
−
l(σ,σˆ) 64(2+ǫ)C2p logN
max
. (7)
N ≤ d2∆2
Here ∆ lower bounds the eigengap λ (E[H]) λ (E[H]) and its expression is given in (11);
1 2
| − |
d is the distance between the two cluster centroids of the population version E[H], with its
expression provided in (28). e e
e
Proof Recallthatthekeystepsofourspectralclusteringalgorithm(Algorithm 1)involves:
first compute the top eigenvector of H, and then cluster the vertices using k-means on the
embedding space given by the real and imaginary part of the top eigenvector. We use
Uˆ = [ (vˆ), (vˆ)] to denote the embeedding space given by the top eigenvector of H and
ℜ ℑ
U = [ (v), (v)] for that of E[H], where both U,Uˆ RN 2. For the clustering outcomes,
×
ℜ ℑ ∈
we denote by σˆ the clustering result using H, and use σ to represent the clusteringegiven
by E[H]. First, note that from Leemma 4, we have the leading eigenvector of E[H] perfectly
indicates the true community membership, teherefore σ is the true community membership
e e
18vector. Next, given that the k-means clustering step achieves a (1 + ǫ) approximation,
using the error bound (39) from Lemma 13, we have that
l(σ,σˆ)d2 4(4+2ǫ) Uˆ U 2,
F
≤ k − k
where d is the distance between the two cluster centroids of the population version E[H],
with its expression provided in (28). Given that a rotation of Uˆ does not change the
k-means clustering result, the tightest upper bound we can obtain is e
l(σ,σˆ)d2 4(4+2ǫ) min Uˆ OU 2 =4(4+2ǫ)min v rvˆ 2 4(4+2ǫ) vˆvˆ vv 2,
F F ∗ ∗ F
≤ O 2k − k r C 1k − k ≤ k − k
∈O ∈
(14)
where the first equality follows from the fact that U Uˆ = v vˆ and the last
F F
k − k k − k
inequality follows from Lemma 11.
Recall that combining Lemma 4, Lemma 5 and Lemma 6, we can derive that for a absolute
constant ǫ , with probability at least 1 N ǫ0
0 −
−
2√2C√Np logN 2√2C√Np logN
max max
vˆvˆ vv . (15)
∗ ∗ F
k − k ≤ λ (E[H]) λ (E[H]) ≤ ∆
1 2
−
Combining(14)and(15),weeventuale lyobtainthe atwithprobabilityatleast1 N ǫ0
−
−
l(σ,σˆ) 64(2+ǫ)C2p logN
max
.
N ≤ d2∆2
5. Algorithmic implementation and experiments
5.1 Implementation details and complexity analysis
Thissectionoutlinestheimplementationdetailsofourproposedalgorithms. OurMATLAB
code is available on Github1.
Implementation details of the Spectral Clustering Algorithm. Algorithm 1 re-
quires computing the top eigenvector of the Hermitian matrix, with a compute time of
O(N2) via the power method. For the k-means step, we employ the k-means++ algorithm
Arthur et al.(2007),whichproducessolutionsO(log2)competitive totheoptimalk-means
solution.
1 https://github.com/ningz97/MLE-DSBM
19Implementation details of the SDP Algorithm. One way of solving the (SDP-MLE)
in polynomial time is to use standard interior point methods based tools, such as SDPT3
Toh et al. (2012) and Mosek. However, those solvers are quite memory intensive for large
graphs (in practice, around thousands of vertices). To avoid memory outage, in this pa-
per, we used the Burer-Monteiro approach Burer and Monteiro (2003), which is a rank-
restricted non-convex programming algorithm that allows a much smaller search space.
The Burer-Monteiro method considers the following optimization problem
max Tr(Z HZ) (MLE-BM)
∗
s.t. Z CN r
×
∈ e
diag(ZZ )= I .
∗ N
This non-convex optimization problem (MLE-BM) is guaranteed to map to the global
optimal of the SDP when the rank satisfies r2 > N Boumal et al. (2016). For solving
(MLE-BM), one can use either the Augmented Lagrangian method Boyd et al. (2011) or
manifold optimization tools Boumal (2023). With all these in mind, we summarize the
steps of the clustering algorithm in Algorithm 3.
Input : directed graph G( , ) of size N, DSBM parameters p,q,η
V E { }
Output: community labels σˆ
1 Compute the Hermitian matrix according to (3);
2 Solve (MLE-BM) with r = √N and compute the top eigenvector of the solution
⌈ ⌉
vˆ;
3 Apply k-means on the concatenated matrix [ (vˆ), (vˆ)];
ℜ ℑ
Algorithm 3: Burer-Monteiro for MLE clustering
Learning Model Parameters. Note that our proposed algorithms, Algorithm 1, Algo-
rithm 2 and Algorithm 3, all require knowing the DSBM parameters as inputs so that one
can compute the MLE Hermitian matrix using (3). Such a requirement is often practically
infeasible because it is hard to compute or approximate p,q and η without knowing the
true community label. To circumvent this limitation, we adopt an iterative approach from
Newman(2016),andcombineitwithourproposedclusteringalgorithmstolearnthemodel
parameters. We summarize this iterative clustering approach in Algorithm 4.
We conduct empirical tests on this iterative approach on directed graphs generated from
the DSBM ensemble. In Figure 2, we show how the learned model parameters vary as one
repeats the updating process in Algorithm 4. Through our study on the synthetic data
sets from theDSBM, we observe that in most cases, this iterative algorithm converges near
the truth model parameters very fast (within 10 iterations). We also conduct experiments
to test how different initialization strategies may influence the clustering outcomes. We
present detailed comparisons of different initialization strategies in Appendix D.4. From
20Input : directed graph G( , ), threshold t
V E
Output: estimated DSBM parameters p,q,η , community label σˆ
{ }
1 Randomly initialize p,q, and η;
2 Apply MLE-driven algorithms clustering (Algorithm 1, Algorithm 2 or
Algorithm 3) and get two clusters , ;
1 2
C C
3 Update the DSBM parameters as follows:
p :=
|E|−TF( C1, C2)
, which is the within-community edge frequency;
(|C1|)+(|C2|)
2 2
q :=
TF( C1, C2)
, which is the between-community edge frequency;
1 2
|C |×|C |
η := min |( C1 ×C2) ∩E|,|( C2 ×C1) ∩E| , which computes the ratio of oriented
TF( 1, 2) TF( 1, 2)
C C C C
cross-comnmunity edges; o
4 Repeat step 2 and step 3 until the update is below the convergence threshold t;
5 Finalize the clusters using MLE-driven algorithms with converged DSBM
parameters.
Algorithm 4: Iterative MLE clustering
20.0 %
8.0 % 0.15
15.0 %
6.0 %
0.10
10.0 % 4.0 % ture parameters
SC
0.05 SDP
5.0 %
2.0 %
1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
Figure 2: Illustration on the convergence of iterative learning algorithm. We first sample a
directed graph from DSBM with n = n = 1000,p = 2%,q = 1%,η = 0.1. Then starting
1 2
from a random guess on the model parameters, we apply Algorithm 4 to learn the DSBM
parameters. The lines with circles represent model parameter learning using the spectral
clustering algorithm (Algorithm 1) and those with stars represent learning with the SDP
clustering algorithm (Algorithm 3).
21our empirical studies, we summarize the following three types of strategies to provide a
relatively good initialization, and facilitate the iterative learning of the parameters:
a. When the edge orientation is the main consideration for clustering, we suggest ini-
tializing the Hermitian matrix as H = i(A AT), which, according to our discussion
0
−
in Section 3.2, corresponds to the net flow optimization.
b. When edge density difference is tehe main consideration, we suggest initializing the
Hermitian matrix as H = A+AT as it corresponds to the total flow optimization
0
scheme;
c. When both edge denseity and orientation need to be taken into account, we suggest
initializing with H = i(A AT)+(A+AT), which jointly considers the two factors
0
−
without bias.
e
5.2 Experiments on DSBM synthetic data
In this section, we conduct experiments on directed graphs sampled from the DSBM with
different model parameters p,q,η. To measure the performance of each algorithm, we
calculate the Adjusted Rand Index (ARI) Gates and Ahn (2017), which quantifies how
well the clustering output aligns with the ground truth community labelling. The ARI
takes value in [ 1,1], where an ARI value of 1 indicates a perfect recovery on the ground
−
truth; a nearly 0 ARI implies that the recovery is almost like a random guess; and 1
−
indicates that the recovered clusters are completely different from the ground truth.
In the experiments, we cluster the directed graphs using our proposed spectral clustering
algorithm (MLE-SC) and the SDP clustering algorithm (MLE-SDP), where we assume
no prior knowledge about the model parameters and we employ the iterative learning
approachtolearnthemfromdata. Wecompareourproposedalgorithmswithotherexisting
approaches for clustering directed graphs: the DI-SIM algorithm in Rohe et al. (2012),
the Hermitian clustering algorithms from Cucuringu et al. (2020) (Herm and HermRW),
the bibliometric symmetrization from Satuluri and Parthasarathy (2011) (B-Sym), and
spectral clustering on na¨ıve symmetrization using A+AT.
Experiments on DSBM with p = q. We first conduct experiments on DSBM graphs
with homogeneous edge probability. With p = q, the generated directed graphs have
roughly the same between-cluster and within-cluster edge densities, therefore community
recovery can only rely on the information attached to the edge directions. In each of the
experiments, we independently sample 10 directed graphs with a fixed parameter set, and
we averaged the ARI values over these graph samples. We summarize the ARI compar-
isons in Figure 3. From the quantitative comparisons, we observed that our proposed
MLE-SC (Algorithm 4 with Algorithm 1) and MLE-SDP (Algorithm 4 with Algorithm 2
or Algorithm 3) attain nearly the same performance, and they outperform all other al-
gorithms when the noise value η is small. In the high-noise regime, i.e., η close to 0.5,
DI-SIM is the best-performing algorithm. For a more intuitive illustration, we also visual-
221 1 1
Herm
HermRW
0.8 0.8 0.8 DI-SIM
B-Sym
0.6 0.6 0.6 A+AT
MLE-SC
MLE-SDP
0.4 0.4 0.4
0.2 0.2 0.2
0 0 0
0 0.1 0.2 0.3 0.4 0.5 0 0.1 0.2 0.3 0.4 0.5 0 0.1 0.2 0.3 0.4 0.5
Figure 3: Performance comparison of all algorithms on DSBM with n = n = 1000, p = q
1 2
(for three different values of the edge density), and varying η.
ize in Appendix D.2 the graph adjacency relation before and after applying the clustering
algorithms.
Experiments on DSBM with p = q. We consider experiments for which the edge den-
6
sities are different, and test on graphs from DSBM with p = q. In Figure 4, we summarize
6
the comparisons of the clustering results on directed graphs sampled with different values
of p,q. As before, each reported ARI value is obtained by averaging over 10 independently
sampled directed graphs with fixed DSBM parameters. We observed that overall our pro-
posed algorithms MLE-SC and MLE-SDP have the highest ARIs with MLE-SDP slightly
better than MLE-SC.
It is also worth mentioning that our MLE-based Hermitian clustering algorithms empir-
ically outperform the other two closely related Hermitian clustering algorithms, namely
Herm and HermRW from Cucuringu et al. (2020). The main difference between the latter
two algorithms and our approaches is that our proposed Hermitian matrix contains both
real and imaginary components with a derived weighting parameter. Recall that from
the optimization interpretation in Section 3.2, we derived that (H) corresponds to the
ℜ
TF( , ) optimization, and (H) corresponds to the NF( , ) optimization. There-
1 2 1 2
C C ℑ C C
fore,thiscomparisonbetweenourproposedalgorithmsandthoseinCeucuringu et al.(2020)
suggests the importance of havingea joint analysis of the edge density and edge orientation
when clustering directed graphs.
In addition to the above tests on DSBM, we also conduct experiments following the same
setup but using true DSBM parameters p,q,η as inputs for Algorithm 1 and Algorithm 3
instead of employing the iterative approach to learn the parameters. We reporttest results
with true DSBM parameters in Figure 8 in Appendix D.1. Comparing the performance
between those with and without true model parameters (Figure 8 v.s. Figure 3 and 4), we
observe that given true model parameters as inputs, Algorithm 1 and Algorithm 3 have
nearly the same performance as MLE-SC and MLE-SDP. This comparison indicates that
231 1 1
Herm
HermRW
0.8 0.8 0.8 DI-SIM
B-Sym
0.6 0.6 0.6 A+AT
MLE-SC
MLE-SDP
0.4 0.4 0.4
0.2 0.2 0.2
0 0 0
0 0.1 0.2 0.3 0.4 0.5 0 0.1 0.2 0.3 0.4 0.5 0 0.1 0.2 0.3 0.4 0.5
Figure 4: Performance comparison of all algorithms on DSBM with n = n = 1000, p = q
1 2
6
(for three different combinations of p,q), and varying η.
the iterative approach consistently converges to the true DSBM parameters on our tested
dataset.
5.3 Experiments on real-world data sets
We first perform experiments on the real-world data set email-Eu-core Leskovec et al.
(2007), which is a network containing email exchange data from European research in-
stitutions. In this network, there is an edge (u,v) if a person u sent person v at least one
email. This dataset comes with “ground-truth” community membership labelling, where
each community represent oneof the 42 departments at the research institute. We perform
experiments on two-community subgraphs of the email-Eu-core dataset by selecting pairs
from the three largest communities in the data set 2.
We compare the cluster results with the “ground-truth”community assignment and report
in Table 1 the ARIs of each algorithm, where each ARI is averaged from 10 repeating ex-
periments. Onthetested dataset, weobservethatouralgorithms MLE-SCandMLE-SDP
outperform all other baselines. In particular, we observe that MLE-SC is the best among
all spectral clustering algorithms, and the MLE-SDP outperforms all other algorithms by
a large margin.
Data set Herm HermRW DI-SIM B-Sym A+AT MLE-SC MLE-SDP
email-Eu-core12 0.048 0.003 -0.005 0.359 0.423 0.631 0.957
email-Eu-core23 -0.009 0.0059 -0.005 0.383 0.490 0.578 0.978
Table 1: ARIs from tests on email-Eu-core.
2 email-Eu-core12: pick the1st and 2nd largest communities from email-Eu-core dataset
email-Eu-core23: pick the2nd and 3rd largest communities from email-Eu-core dataset
24ground truth Herm, ARI = 0.03 HermRW, ARI = 0.00 DI-SIM, ARI = -0.01
50 50 50 50
100 100 100 100
150 150 150 150
50 100 150 50 100 150 50 100 150 50 100 150
B-Sym, ARI = 0.36 A+AT, ARI = 0.41 MLE-SC, ARI = 0.63 MLE-SDP, ARI = 0.96
50 50 50 50
100 100 100 100
150 150 150 150
50 100 150 50 100 150 50 100 150 50 100 150
Figure 5: A AT after clustering. In this experiment, we test on a directed graph consist-
−
ing of the 1st and 2nd largest communities from the email-En-core dataset Leskovec et al.
(2007). Below the figures of MLE-SC and MLE-SDP, we report the learned DSBM param-
eters from the iterative algorithm.
We present in Figure 5 and Figure 6 visual representations of the graph adjacency relation
after clustering. From the visualization, we observe distinctive clustering patterns exhib-
ited by the compared algorithms: Herm, HermRW and DI-SIM tend to cluster vertices in
a way that the cross-cluster edges are oriented in the same direction while not account-
ing much for the edge density. In contrast, both the na¨ıve symmetrization and B-Sym
demonstrate an awareness of inhomogeneity between-cluster and within-cluster edge den-
sity. Our proposed approaches strike a balance between the cross-cluster edge orientation
and the inhomogeneity edge densities, where the weights are learned from the iterative
approach.
We also report and visualize in Appendix D.3 the case being studied where our proposed
algorithms attain low ARI values. We comment that in those experiments, our proposed
algorithms still lead to meaningful community discovery where the clusters exhibit block-
wise patterns from the edge density or edge orientation, but the clusters recovered by our
algorithms do not agree with the community labeling provided.
6. Concluding remarks and discussions
This paper studies the directed graph clustering problem through the lens of statistics. In
particular, we formulate the task of directed clustering as a statistical estimation prob-
25ground truth Herm, ARI = -0.01 HermRW, ARI = 0.08 DI-SIM, ARI = -0.01
20 20 20 20
40 40 40 40
60 60 60 60
80 80 80 80
100 100 100 100
120 120 120 120
140 140 140 140
20 40 60 80100120140 20 40 60 80100120140 20 40 60 80100120140 20 40 60 80100120140
B-Sym, ARI = 0.37 A+AT, ARI = 0.49 MLE-SC, ARI = 0.58 MLE-SDP, ARI = 1.00
20 20 20 20
40 40 40 40
60 60 60 60
80 80 80 80
100 100 100 100
120 120 120 120
140 140 140 140
20 40 60 80100120140 20 40 60 80100120140 20 40 60 80100120140 20 40 60 80100120140
Figure 6: A AT after clustering. In this experiment, we test a directed graph consisting
−
of the 2nd and 3rd largest communities from email-En-core Leskovec et al. (2007). Below
the figures of MLE-SC and MLE-SDP, we report the learned DSBM parameters from the
iterative algorithm.
lem on the DSBM, and employ the MLE to infer the underlying community labels. This
statistical formulation gives rise to a novel flow optimization heuristic, which jointly con-
siders the edge density and edge orientation. Building on our formulation, we propose a
newHermitian matrix representation ofdirected graphs, establishingaconnection between
its spectrum and the MLE optimization problem. Based on this Hermitian matrix rep-
resentation, we introduce two efficient and interpretable directed clustering algorithms, a
spectralclusteringalgorithm (Algorithm 1)andanSDPclusteringalgorithm(Algorithm 2,
along with its more scalable version, Algorithm 3). We compare, both quantitatively and
qualitatively, our proposed algorithms with existing directed clustering methods on syn-
thetic data and real-world data. In addition to our experimental evaluation, we conduct a
perturbation analysis using Davis-Kahan’s theorem and establish an upper bound on the
misclustering error of Algorithm 1.
Ourcurrentworkondirectedgraphclusteringdoesnotconsideranypriorknowledgeabout
thecommunityassignment. Therefore,itwouldbenaturaltoextendthisworkbysystemat-
icallyintegratingsideinformationorpriorknowledge. Onewaytoachievethisistoconsider
the community labels as latent random variables and the side information can be modelled
as the prior distribution of community labels. Correspondingly, community recovery can
then beachieved by applyingBayesian methods, suchas maximum-a-posteriori, and Gibbs
sampling. In addition to the statistical view, we also present an equivalent (regularized)
26flow optimization heuristic for directed clustering. From this optimization perspective, one
can directly incorporate prior knowledge about community labels by imposing constraints
on the flow optimization problem. Alternatively, the optimization interpretation also al-
lows using a constrained clustering method from Cucuringu et al. (2016), where one can
merge the data matrix with any available constraints by converting the constraints into a
penalty term of the optimization objective.
This work focuses on studying the two-cluster directed stochastic block model. Extending
the analysis to multi-cluster models is not as easy as the inductive analysis on undirected
graphs. The challenge stems from the asymmetric nature of the problem. To elaborate
on this point, if one additional cluster is added to the current two-cluster model, there
would be multiple approaches to instantiate the problem: e.g., add a new source cluster
to the existing source cluster, add a new sink cluster to the existing source cluster, etc.
Therefore, for a multi-cluster model, one needs to specify a directed meta-graph structure
to determine the relationship between different clusters, which may vary from case to case.
Consequently, the task of inferring the number of clusters, as well as clustering multi-
cluster directed graphs, may require a more elaborated analysis and potentially further
assumptions. The main difficulty of the problem stems from the fact that this underlying
meta-graph structure is not known to the user, a-priori. While we defer such work for
future research, we comment that one may iteratively apply the two-cluster algorithms on
the clustered subgraphs to obtain a hierarchical clustering structure.
Ourstudyfocuses on clustering unweighted simple directed graphs, and itwould bepracti-
cally meaningful and interesting to consider clustering more general and complex directed
graphs. For instance, our flow optimization heuristic can readily be adapted for clustering
weighted directed graphs. In addition, recent work Tian and Lambiotte (2023) adopts a
Hermitian matrix representation for signed graphs. It would be interesting to explore the
connections between signed graphs and directed graphs, which could possibly lead to a
more general Hermitian matrix design that can unify these two types of graphs, as in the
recent work of He et al. (2022) that introduced the Magnetic Signed Laplacian, a Hermi-
tian PSD matrix. Moreover, from a statistical perspective, the DSBM we studied in this
paperhasits limitations indescribingreal-world networks, andthusitwould beinteresting
to extend our analysis to random directed graph models that better capture some of the
real-world network features, such as power-law distribution Michel et al. (2019) or hetero-
geneity of node degrees Rohe et al. (2016). Finally, extending our proposed methodology
to the setting of time-evolving networks Matias and Miele (2017) is another timely avenue
of future research, potentially operating under the assumption that the node cluster labels
vary smoothly over time.
27References
Emmanuel Abbe, Afonso S Bandeira, and Georgina Hall. Exact recovery in the stochastic
block model. IEEE Transactions on information theory, 62(1):471–487, 2015.
DaronAcemoglu,AsumanOzdaglar,andAlirezaTahbaz-Salehi. Systemicriskandstability
in financial networks. American Economic Review, 105(2):564–608, 2015.
Lada A Adamic and Natalie Glance. The political blogosphere and the 2004 us election:
divided they blog. In Proceedings of the 3rd international workshop on Link discovery,
pages 36–43, 2005.
Arash A Amini and Elizaveta Levina. On semidefinite relaxations for the block model.
2018.
Arash A Amini, Aiyou Chen, Peter J Bickel, and Elizaveta Levina. Pseudo-likelihood
methods for community detection in large sparse networks. 2013.
Yuan An, Jeannette Janssen, and Evangelos E Milios. Characterizing and mining the
citation graph of the computer science literature. Knowledge and Information Systems,
6:664–678, 2004.
David Arthur, Sergei Vassilvitskii, et al. k-means++: The advantages of careful seeding.
In Soda, volume 7, pages 1027–1035, 2007.
Konstantin Avrachenkov, Maximilien Dreveton, and Lasse Leskel¨a. Community recovery
in non-binary and temporal stochastic block models. arXiv preprint arXiv:2008.04790,
2020.
Stefanos Bennett, Mihai Cucuringu, and Gesine Reinert. Lead–lag detection and network
clustering for multivariate time series with an application to the us equity market. Ma-
chine Learning, 111(12):4497–4538, 2022.
Nicolas Boumal. An introduction to optimization on smooth manifolds. Cambridge Uni-
versity Press, 2023.
Nicolas Boumal, Vlad Voroninski, and Afonso Bandeira. The non-convex burer-monteiro
approach works on smooth semidefinite programs. Advances in Neural Information Pro-
cessing Systems, 29, 2016.
StephenBoyd, Neal Parikh, EricChu,BorjaPeleato, JonathanEckstein, etal. Distributed
optimization and statistical learning via the alternating direction method of multipliers.
Foundations and Trends® in Machine learning, 3(1):1–122, 2011.
28Samuel Burer and Renato DC Monteiro. A nonlinear programming algorithm for solving
semidefinite programs via low-rank factorization. Mathematical programming, 95(2):
329–357, 2003.
Yudong Chen, Sujay Sanghavi, and Huan Xu. Improved graph clustering. IEEE Transac-
tions on Information Theory, 60(10):6440–6455, 2014.
Yuxin Chen, Yuejie Chi, Jianqing Fan, Cong Ma, et al. Spectral methods for data science:
A statistical perspective. Foundations and Trends® in Machine Learning, 14(5):566–
806, 2021.
Mihai Cucuringu, Ioannis Koutis, Sanjay Chawla, Gary Miller, and Richard Peng. Sim-
ple and scalable constrained clustering: a generalized spectral method. In Artificial
Intelligence and Statistics, pages 445–454. PMLR, 2016.
Mihai Cucuringu, Huan Li, He Sun, and Luca Zanetti. Hermitian matrices for
clustering directed graphs: insights and applications. In International Confer-
ence on Artificial Intelligence and Statistics, pages 983–992. PMLR, 2020. URL
http://128.84.4.34/pdf/1908.02096.
Chandler Davis and William Morton Kahan. The rotation of eigenvectors by a perturba-
tion. iii. SIAM Journal on Numerical Analysis, 7(1):1–46, 1970.
Aurelien Decelle, Florent Krzakala, Cristopher Moore, and Lenka Zdeborov´a. Asymp-
totic analysis of the stochastic block model for modular networks and its algorithmic
applications. Physical review E, 84(6):066106, 2011.
Micha¨el Fanuel, Carlos M Alaiz, and Johan AK Suykens. Magnetic eigenmaps for commu-
nity detection in directed networks. Physical Review E, 95(2):022302, 2017.
Micha¨el Fanuel, Carlos M Ala´ız, A´ngela Ferna´ndez, and Johan AK Suykens. Magnetic
eigenmaps for the visualization of directed networks. Applied and Computational Har-
monic Analysis, 44(1):189–199, 2018.
Alan Frieze and Micha l Karon´ski. Introduction to random graphs. Cambridge University
Press, 2016.
Alexander J Gates and Yong-Yeol Ahn. The impact of random models on clustering
similarity. arXiv preprint arXiv:1701.06508, 2017.
Bruce Hajek, Yihong Wu, and Jiaming Xu. Achieving exact cluster recovery threshold via
semidefinite programming: Extensions. IEEE Transactions on Information Theory, 62
(10):5918–5937, 2016.
29Koby Hayashi, Sinan G Aksoy, and Haesun Park. Skew-symmetric adjacency matri-
ces for clustering directed graphs. arXiv preprint arXiv:2203.01388, 2022. URL
https://arxiv.org/pdf/2203.01388.pdf.
Yixuan He, Michael Perlmutter, Gesine Reinert, and Mihai Cucuringu. MSGNN: A Spec-
tral Graph Neural Network Based on a Novel Magnetic Signed Laplacian. In Bastian
Rieck andRazvan Pascanu, editors, Proceedings of the First Learning on Graphs Confer-
ence,volume198ofProceedings of Machine Learning Research, pages40:1–40:39. PMLR,
09–12 Dec 2022. URL https://proceedings.mlr.press/v198/he22c.html.
Paul W Holland, Kathryn Blackmond Laskey, and Samuel Leinhardt. Stochastic block-
models: First steps. Social networks, 5(2):109–137, 1983.
Wolfram Research, Inc. Mathematica, Version 14.0. URL
https://www.wolfram.com/mathematica. Champaign, IL, 2024.
Antony Joseph and Bin Yu. Impact of regularization on spectral clustering. 2016.
Brian Karrer and Mark EJ Newman. Stochastic blockmodels and community structure in
networks. Physical review E, 83(1):016107, 2011.
Maxwell Mirton Kessler. Bibliographic coupling between scientific papers. American doc-
umentation, 14(1):10–25, 1963.
Steinar Laenen. Directed graph clustering using hermitian laplacians. Master’s thesis,
2019.
Steinar Laenen and He Sun. Higher-order spectral clustering of directed graphs.
Advances in neural information processing systems, 33:941–951, 2020. URL
https://proceedings.neurips.cc/paper/2020/hash/0a5052334511e344f15ae0bfafd47a67-Abstrac
Can M Le, Elizaveta Levina, and Roman Vershynin. Concentration and regularization of
random graphs. Random Structures & Algorithms, 51(3):538–561, 2017.
Jing Lei and Alessandro Rinaldo. Consistency of spectral clustering in stochastic block
models. 2015.
Jure Leskovec, Jon Kleinberg, and Christos Faloutsos. Graph evolution: Densification and
shrinking diameters. ACM transactions on Knowledge Discovery from Data (TKDD), 1
(1):2–es, 2007.
Xiaodong Li, Yudong Chen, and Jiaming Xu. Convex relaxation methods for community
detection. 2021.
Fragkiskos D Malliaros and Michalis Vazirgiannis. Clustering and community detection in
directed networks: A survey. Physics reports, 533(4):95–142, 2013.
30Catherine Matias and Vincent Miele. Statistical clustering of tempo-
ral networks through a dynamic stochastic block model. Journal of
the Royal Statistical Society Series B, 79(4):1119–1141, 2017. URL
https://EconPapers.repec.org/RePEc:bla:jorssb:v:79:y:2017:i:4:p:1119-1141.
Jesse Michel, Sushruth Reddy, Rikhav Shah, Sandeep Silwal, and Ramis Movassagh. Di-
rected random geometric graphs. Journal of Complex Networks, 7(5):792–816, 2019.
Andrea Montanari and Subhabrata Sen. Semidefinite programs on sparse random graphs
and their application to community detection. In Proceedings of the forty-eighth annual
ACM symposium on Theory of Computing, pages 814–827, 2016.
Mark EJ Newman. Community detection and graph partitioning. Europhysics Letters, 103
(2):28003, 2013.
Mark EJNewman. Equivalencebetween modularity optimization andmaximumlikelihood
methods for community detection. Physical Review E, 94(5):052315, 2016.
Marcia Oliveira and Joao Gama. An overview of social network analysis. Wiley Interdis-
ciplinary Reviews: Data Mining and Knowledge Discovery, 2(2):99–115, 2012.
JudeaPearlandThomasVerma. Thelogicofrepresentingdependenciesbydirectedgraphs.
In Proceedings of the sixth National conference on Artificial intelligence-Volume 1, pages
374–379, 1987.
Karl Rohe, Tai Qin, and Bin Yu. Co-clustering for directed graphs: the stochastic co-
blockmodel and spectral algorithm di-sim. arXiv preprint arXiv:1204.2296, 2012. URL
https://arxiv.org/pdf/1204.2296.pdf.
Karl Rohe, Tai Qin, and Bin Yu. Co-clustering directed graphs to
discover asymmetries and directional communities. Proceedings of
the National Academy of Sciences, 113(45):12679–12684, 2016. URL
https://www.pnas.org/doi/full/10.1073/pnas.1525793113.
Venu Satuluri and Srinivasan Parthasarathy. Symmetrizations for clus-
tering directed graphs. In Proceedings of the 14th international con-
ference on extending database technology, pages 343–354, 2011. URL
https://dl.acm.org/doi/pdf/10.1145/1951365.1951407?casa_token=IOameE8wvjAAAAAA:s7sr2Ue_mPhbftd
Jianbo Shi and Jitendra Malik. Normalized cuts and image segmentation. IEEE Transac-
tions on pattern analysis and machine intelligence, 22(8):888–905, 2000.
MA Shubin. Discrete magnetic laplacian. Communications in mathematical physics, 164
(2):259–275, 1994.
31Henry Small. Co-citation in the scientific literature: A new measure of the relationship
between two documents. Journal of the American Society for information Science, 24
(4):265–269, 1973.
Yu Tian and Renaud Lambiotte. Structural balance and random walks on complex net-
works with complex weights. arXiv preprint arXiv:2307.01813, 2023.
Kim-Chuan Toh, Michael J Todd, and Reha H Tu¨tu¨ncu¨. On the implementation and
usageofsdpt3–amatlabsoftwarepackageforsemidefinite-quadratic-linearprogramming,
version 4.0. Handbook on semidefinite, conic and polynomial optimization, pages 715–
754, 2012.
Joel A Tropp et al. An introduction to matrix concentration inequalities. Foundations and
Trends® in Machine Learning, 8(1-2):1–230, 2015.
Ulrike Von Luxburg. A tutorial on spectral clustering. Statistics and computing, 17:395–
416, 2007.
Vincent Q Vu, Juhee Cho, Jing Lei, and Karl Rohe. Fantope projection and selection: A
near-optimal convex relaxation of sparsepca. Advances in neural information processing
systems, 26, 2013.
Hermann Weyl. Das asymptotische verteilungsgesetz der eigenwerte linearer partieller
differentialgleichungen (mit einer anwendung auf die theorie der hohlraumstrahlung).
Mathematische Annalen, 71(4):441–479, 1912.
Yi Yu, Tengyao Wang, and Richard J Samworth. A useful variant of the davis–kahan
theorem for statisticians. Biometrika, 102(2):315–323, 2015.
32Appendix A. Summary of notations
Notation Definition
G( , ) Graph with vertex set and edge set
V E V E
u v There is an edge pointing from vertex u to vertex v
u v There is no edge between vertex u and vertex v
A6∼
Graph adjacency matrix, A 0,1 n n and A = 1 iff u v
× uv
∈{ }
Source community
1
C
Target community
2
C
Set of all vertices, =
1 2
V V C ∪C
c() Community labelling function, c() : 1,2 where c(u) = 1 iff u
1
· · V → { } ∈ C
H Hermitian matrix derived from MLE on DSBM (see (3))
σ A general community indicator vector, σ = σ iff c(u) = c(v)
u v
(Ae;σ) Log-likelihood function
L
TF( , ) Total flow between and , TF( , )= (A +A )
1 2 1 2 1 2 uv vu
C C C C C C
u ∈CX1v ∈C2
NF( , ) Net flow from to , NF( , ) = (A A )
1 2 1 2 1 2 uv vu
C C C C C C −
u ∈CX1v ∈C2
p Within-community edge probability
q Cross-community edge probability
p max p,q
max
{ }
η Probability of a cross-community edge pointing from to
2 1
C C
HT Transpose of H
H Conjugate transpose of H
∗
H The j-th row vector of H
j
∗
[H ,H ] Concatenating columns of H and H
1 2 1 2
H Spectral norm of H, H = λ (H)
1
k k k k | |
H Frobenius norm of H, H = λ2(H)
k kF k kF j j
hH 1,H
2
i
Frobenius inner product, hH 1,Hq2 iP= Tr(H 1∗H 2)
diag(H) Create a diagonal matrix by taking the main diagonal elements of H
(H) Take the real part of the matrix H;
ℜ
(H) Take the imaginary part of the matrix H
ℑ
v (H) The j-th eigenvector of H
j
ARI Adjusted Rand Index
M 0,1 N 2 Membership matrix
×
∈ { }
1() Indicator function, 1(p)= 1 if claim p is true, otherwise 1(p)= 0
·
1 Binary indicator vector for community , 1 = 1 if u otherwise 1 = 0
1C1
Binary indicator vector for community
C1
,
1u
= 1 if
u∈ C1
otherwise
1u
= 0
C2 C2 u g
n
∈ C2 u
g = o(f ) g is asymptotically dominated by f , i.e., lim = 0
n n n n
n f n
→∞
33g
n
g = O(f ) g is asymptotically bounded above by f , i.e., limsup <
n n n n
n f n ∞
g n g n →∞
g = Θ(f ) limsup < and liminf > 0
n n
n f n ∞ n f n
→∞ →∞ g n
g = Ω(f ) g bounded below by f asymptotically, i.e., limsup > 0
n n n n
f
n n
g n →∞
g = ω(f ) g dominate f asymptotically, i.e., limsup =
n n n n
n f n ∞
→∞
Table 2: Summary on notations
34Appendix B. Proof of Theorem 1
We detail the derivation of the optimization problem (MLE) from the maximum likelihood
estimator. To start with, we explicitly express the likelihood function as a matrix, which
simply relies on subdividingthe likelihood function according to which community an edge
belongs to.
Lemma 7 Consider a directed graph with adjacency matrix A sampled from the model
DSBM(n ,n ,p,q,η). Then, applying the maximum likelihood estimation is equivalent to
1 2
solving the following combinatorial optimization problem
1
max M ,1 1T +1 1T + M ,1 1T (MLE)
2 intra C1 C1 C2 C2 inter C1 C2
s.t. 1 (cid:10) 0,1 N (cid:11) (cid:10) (cid:11)
C1 ∈ { }
1 +1 = 1
1 2
C C
where 1 ,1 0,1 N are the indicator vectors for cluster and separately, and
C1 C2 ∈{ } C1 C2
M = log(1/2p)(A+AT)+log(1 p)(J I A AT),
intra
− − − −
M = log(q(1 η))A+log(ηq)AT +log(1 q)(J I A AT),
inter
− − − − −
are derived from the log-likelihood functions for intra-community and inter-community
edges.
ProofLetAbetheadjacencymatrixofadirectedgraphgeneratedfromDSBM(n ,n ,p,q,η).
1 2
For aparticularclusterization ofthegraph,weusectodenoteitscommunity labelingfunc-
tion c : , , and we use the vectors 1 ,1 0,1 N to indicate community
and sV ep→ ara{ tC e1 ly,C2 w} here 1 +1 = 1. The logC1 likeC l2 ih∈ oo{ d fu} nction of A given 1 and 1C1
C2 C1 C2 C1 C2
can be decomposed as follows
(A;σ) = logP(A1 ,1 )= logP(A c(u),c(v))
L | C1 C2 uv |
u<v
X
= logP(A c(u),c(v))+ logP(A c(u),c(v)), (16)
uv uv
| |
u<v u<v
c(uX)=c(v) c(u)=X1,c(v)=2
where the first term in (16) is only summing over intra-community pair, and the second
term handles the inter-community pair.
For an intra-community vertex pair u,v, the log-likelihood function is
log(1/2p) if u v,
logP(A c(u) = c(v)) = log(1/2p) if v u,
uv 
|
 log(1 p) if u v.
− 6∼


35This intra-community log-likelihood function coincides with the matrix
M ,log(1/2p)(A+AT)+log(1 p)(J I A AT), (17)
intra
− − − −
on the entries that represent intra-community pairs, thus allowing us to convert the intra-
community summation in (16) into the following matrix multiplication form
1
logP(A c(u),c(v)) = M ,1 1T +1 1T . (18)
uv | 2 intra C1 C1 C2 C2
u<v
c(uX)=c(v) (cid:10) (cid:11)
For an inter-community vertex u ,v , the log-likelihood function is
1 2
∈ C ∈ C
log((1 η)q) if u v,
−
logP(A c(u) = ,c(v) = ) = log(ηq) if v u,
uv 1 2 
| C C
 log(1 q) if u v.
− 6∼

Similar to the approach followed for the intra-community case, we convert the inter-
community summation in (16) into the matrix multiplication form
logP(A c(u),c(v)) = M ,1 1T . (19)
uv | inter C1 C2
u<v
c(u)=X1,c(v)=2 (cid:10) (cid:11)
where
M , log((1 η)q)A+log(ηq)AT +log(1 q)(J I A AT). (20)
inter
− − − − −
Combining (18), (19) and (16), we have
1
logP(A1 ,1 ) = M ,1 1T +1 1T + M ,1 1T . (21)
| C1 C2 2 intra C1 C1 C2 C2 inter C1 C2
(cid:10) (cid:11) (cid:10) (cid:11)
To arrive at a more compact expression for the optimization formulation, we introduce
an equivalent Hermitian matrix optimization framework. The transformation from the
real-valued matrix optimization to the Hermitian optimization builds on the following
observation.
Lemma 8 Consider an arbitrary Hermitian matrix H = (H)+i (H), where (H)
ℜ ℑ ℜ ∈
Rn n with all 0 diagonal entries is symmetric, and (H) Rn n is skew-symmetric. Let
× ×
ℑ ∈
x i,1 n be the complex community indicator vector, where x = i for u . Then,
u 1
∈ { } ∈ C
36the quadratic form x Hx is the sum of entries in (H) that are in the same community,
∗
ℜ
plus the sum of entries in (H) that belong to different communities, i.e.,
ℑ
x Hx= (H) + (H) = 2 (H) +2 (H) .
∗ uv uv uv uv
ℜ ℑ ℜ ℑ
oru, X uv ,∈v ∈C1 C2 oru u∈ ∈CX C1v 2∈,vC ∈2 C1 oruu ,X uv< ,∈vv C1
2
oru u∈u CX< 1v 2v ∈,, vC2
1
∈C ∈C ∈C
In other words,
x Hx= (H),1 1T +1 1T +2 (H),1 1T . (22)
∗ hℜ C1 C1 C2 C2i hℑ C1 C2i
With the real-valued MLE optimization formula derived in Lemma 7 and the observation
made in Lemma 8, we are ready to prove the Hermitian optimization formulation of the
MLE on DSBM in Theorem 1.
Theorem 1 (MLE on DSBM) Consider a directed graph with adjacency matrix A gen-
erated from the model DSBM(n ,n ,p,q,η). Let x i,1 N be the indicator vector, where
1 2
∈ { }
x = i if u , and x = 1 if u . The maximum likelihood estimation on the
u 1 u 2
∈ C ∈ C
community labels is equivalent to solving the following complex optimization problem
max x Hx (Herm-MLE)
∗
s.t. x 1,i N,
∈e{ }
where H is a Hermitian matrix defined as
1 η p2(1 p)2 1 p
H =eilog − (A AT)+log − (A+AT)+2log − (J I) (3)
η − 4η(1 η)q2(1 q)2 1 q −
− − −
e , w ii(A AT)+w r(A+AT)+w c(J I).
− −
Proof From Lemma 7, we derive the log-likelihood
1
logP(A1 ,1 ) = M ,1 1T +1 1T + M ,1 1T . (23)
| C1 C2 2 intra C1 C1 C2 C2 inter C1 C2
(cid:10) (cid:11) (cid:10) (cid:11)
Compared this equation with the observation (22) in Lemma 8, we need the matrix cor-
responding to the intra-cluster log-likelihood to be symmetric and need the inter-cluster
log-likelihood matrix to be skew-symmetric, so that we can directly apply (22) to convert
the real-valued objective into a more compact complex-valued expression.
From the definition in (17), we have that M = MT . For the inter-cluster log-
intra intra
likelihood matrix, by definition (20), M is not slew-symmetric. To circumvent this, we
inter
decomposed the inter-cluster log-likelihood matrix M into a symmetric matrix plus a
inter
skew-symmetric matrix as follows
1 1
M = (M +MT )+ (M MT ),
inter 2 inter inter 2 inter − inter
37whereM +MT issymmetricandM MT isskew-symmetric. Correspondingly,
inter inter inter − inter
we have
1 1
M ,1 1T = M +MT ,1 1T + M MT ,1 1T , (24)
inter C1 C2 2h inter inter C1 C2i 2h inter − inter C1 C2i
(cid:10) (cid:11)
where the first term sums over the inter-cluster entries of a symmetric matrix and the
second term sums over the inter-cluster entries of a skew-symmetric matrix. Due to the
symmetry in the first term of (24), we can further write it as
1 1
M +MT ,1 1T = M +MT ,J 1 1T 1 1T (25)
2h inter inter C1 C2i 4h inter inter − C1 C1 − C2 C2i
Combining (24), (25) and (23), we can rewrite the log-likelihood objective as
1
logP(A1 ,1 ) = M ,1 1T +1 1T + M ,1 1T
| C1 C2 2 intra C1 C1 C2 C2 inter C1 C2
= 1 M ,1 1T +(cid:10)1 1T + 1 M +M(cid:11)T (cid:10) ,1 1T + 1 (cid:11) M MT ,1 1T
2 intra C1 C1 C2 C2 2h inter inter C1 C2i 2h inter − inter C1 C2i
= 1 (cid:10) M ,1 1T +1 1T (cid:11) + 1 M +MT ,J 1 1T 1 1T
2 intra C1 C1 C2 C2 4h inter inter − C1 C1 − C2 C2i
+(cid:10)1 M MT ,1 1T(cid:11)
2h inter − inter C1 C2i
Because the term M + MT ,J is always a constant and resealing the objective
h inter inter i
function by a constant factor 4 does not affect the optimal solution, therefore solving the
(MLE) in Lemma 7 is equivalent to solve the following
max 2M (M +MT ),1 1T +1 1T +2 M MT ,1 1T
intra − inter inter C1 C1 C2 C2 inter − inter C1 C2
s.t. 1 (cid:10) C1 ∈ {0,1 }N (cid:11) (cid:10) (cid:11)
1 +1 = 1
1 2
C C
Using (22) from Lemma 8, we convert the above real-valued optimization problem to the
following complex-valued equivalence
max x Hx
∗
s.t. x i,1 N
∈e{ }
38where the Hermitian matrix H has
(H) = 2M
intra
(eM
inter
+M iT nter)
ℜ −
p2 1 p
e = log (A+AT)+2log − (J I A AT)
4η(1 η)q2 1 q − − −
− −
p2(1 q)2 1 p
= log − (A+AT)+2log − (J I),
4η(1 η)q2(1 q)2 1 q −
(cid:18) − − (cid:19) (cid:18) − (cid:19)
(H) = M MT
inter inter
ℑ −
1 η
e = log − (A AT).
η −
(cid:18) (cid:19)
39Appendix C. Proofs in perturbation analysis
C.1 Proof of Lemma 4
Lemma 4 For the DSBM(n ,n ,p,q,η), the population version of the proposed Hermitian
1 2
matrix E[H] has a unique largest eigenvalue. The top eigenvector v has exactly two distinct
values that indicate the community labels where the distance between them d can be easily
computedeusing (28). Moreover, the eigengap
λ (E[H]) λ (E[H]) = min 2∆, 1/2N(w p+w ) +∆ ∆. (10)
1 2 r c
| − | { | | }≥
where e e
1
∆ = N2(w p+w )2 4n n ((w p+w )2 w +w +iw (1 2η)q 2) (11)
r c 1 2 r c r c i
2 − −| − |
p
Proof Recall that the population version of H has a block structure and can be written
as
e
E[H] = MQMT (pw +w )I
r c
−
0 1 p q 1 1
eQ = +w
r
+w
c
,
1 0 q p 1 1
(cid:20)− (cid:21) (cid:20) (cid:21) (cid:20) (cid:21)
where M 0,1 N 2 is the community membership matrix, and M = 1 denotes that
× uc
∈ { }
vertex u belongs to community c. We further normalize the columns of M as follows
MQMT = MD 1DQD(MD 1)T
− −
√n 0
D = 1
0 √n .
2
(cid:20) (cid:21)
Here the normalized matrix MD 1 has orthonormal column vectors.
−
Let DQD = UΛU be the eigendecomposition on the 2 2 matrix, Then, the N N
∗
× ×
matrix MQMT can be diagonalized as
MQMT = (MD 1U)Λ(MD 1U) ,
− − ∗
where diag(Λ) contains the eigenvalues of MQMT and the columns of MD 1U RN 2
− ×
∈
are the orthonormal eigenvectors. Therefore, the problem of computing the eigenpairs of
E[H] reduces to compute the eigenpairs of the 2 2 matrix DQD where
×
e
DQD =
n 1(w rp+w c) √n 1n 2(w rq+w c+i(1 −2η)q)
√n 1n 2(w rq+w
c
i(1 2η)q) n 2(w rp+w c)
(cid:20) − − (cid:21)
40For the eigenvalues, via a simple calculation we arrive at
1
λ (DQD)= (N(w p+w )+2∆),
1 r c
2
1
λ (DQD)= (N(w p+w ) 2∆),
2 r c
2 −
where
2∆ = N2(w p+w )2 4n n ((w p+w )2 w q+w +iw q(1 2η)2).
r c 1 2 r c r c i
− −| − |
p
Therefore, we obtain the eigenvalues of E[H] = MQMT
1
λ (E[H]) = (N(w p+w )+2∆) (w p+w )
1 r c r c
2 −
1
λ 2(E[He]) = (N(w rp+w c) 2∆) (w rp+w c)
2 − −
λ (E[H]) = ... = λ (E[H]) = (w p+w ).
3 N e r c
−
The eigenvalue theat obtains the laergest magnitude is unique and it is λ (E[H]) when
1
N(w p + w ) 0, or λ (E[H]) when N(w p + w ) < 0. The gap between the largest
r c 2 r c
≥
and the second largest eigenvalue is e
e
min 2∆, 1/2N(w p+w ) +∆ . (26)
r c
{ | | }
One can easily verify that the eigengap lies in [∆,2∆]. Therefore, the lower bound ∆ is a
good approximation to the spectral gap in the sense that they are of the same order.
Next, we move on to compute the top eigenvector of E[H]. We use x = (x ,x ) C2 to
1 2
∈
denote the top eigenvector of DQD, and we have that x = x . Then, the top eigenvector
1 2
6
of E[H] can be easily computed through v = MD 1x, aned it has two distinct values
−
e x /√n if u ,
1 1 1
v(u) = ∈ C (27)
(x 2/√n
2
if u 2.
∈ C
The distance between the two cluster centroids d is simply
x x
1 2
d = . (28)
√n − √n
(cid:12) 1 2(cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
41C.2 Useful theorems from matrix perturbation analysis
Theorem 9 (Davis-Kahan’s perturbation bound Davis and Kahan (1970)) LetH,R
∈
be two Hermitian matrices. Then, for any a β and δ > 0 it holds that
H ≤
R
P (H) P (H +R) k k.
[α,β] (α δ,β+δ)
− − ≤ δ
(cid:13) (cid:13)
(cid:13) (cid:13)
HereP (H)denotestheprojection matrixonthesubspacespannedbyeigenvectors ofH
[α,β]
with corresponding eigenvalues lie between [α,β], and P (H +R) is the projection
(α δ,β+δ)
−
matrix on the subspace spanned by eigenvectors of H + R with eigenvalues lie between
(α δ,β +δ).
−
Theorem 10 (Weyl’s inequality Weyl (1912)) Let H,R be two Hermitian ma-
∈ H
trices. Then for every 1 j n, the j-th largest eigenvalues of H and H +R obey
≤ ≤
λ (H) λ (H +R) R .
j j
| − |≤ k k
InadditiontotheeigenspaceperturbationboundinThemrem9,wesummarizeinLemma11
comparisons over two different representations of the eigenspace distance, which will be
useful in the error analysis of k-means in Section 4.3.
Lemma 11 [Adapted From Lemma 2.1 in Chen et al. (2021)] For any U,U˜ CN k, we
×
∈
have
min U OU˜ UU U˜U˜
F ∗ ∗ F
O ∈Ok×kk − k ≤ k − k
C.3 Proof of Lemma 5
Lemma 5 Given a directed graph from DSBM(n ,n ,p,q,η) and its Hermitian matrix
1 2
representation H, the projection matrix of the top eigenvector has
R
e vv ∗ vˆvˆ ∗ F 2√2 k k .
k − k ≤ λ (E[H]) λ (E[H])
1 2
−
Proof From the Davis-Kahan’s perturbation bouned, we have e
R
vˆvˆ ∗ vv ∗ k k . (29)
k − k ≤ λ (E[H]) λ (H)
1 2
| − |
Using Wyle’s inequality, we have that
e e
λ (E[H]) λ (H) R
2 2
| − | ≤ k k
e e
42Therefore,thedenominatorin(29)canbefurtherlowerboundedby λ (E[H]) λ (E[H])
1 2
| − |−
R , and we obtain
k k
e e
R
vˆvˆ ∗ vv ∗ k k . (30)
k − k ≤ λ (E[H]) λ (E[H]) R
1 2
| − |−k k
The denominator in (30) involves comparingethe spectrael gap λ (E[H]) λ (E[H]) and
1 2
| − |
R , which further requires an extra condition on the denominator being positive, to allow
k k
the inequality to hold. To circumvent this limitation, we divide the ceomparison einto two
cases
• if R 1 λ (E[H]) λ (E[H]), then we have
k k ≥ 2| 1 − 2 |
e e 2 R
vˆvˆ ∗ vv ∗ 1 k k .
k − k≤ ≤ λ (E[H]) λ (E[H])
1 2
| − |
e e
• if R 1 λ (E[H]) λ (E[H]), then we use the perturbation bound (30) and get
k k ≤ 2| 1 − 2 |
e e R 2 R
vˆvˆ ∗ vv ∗ k k k k .
k − k≤ λ (E[H]) λ (E[H]) R ≤ λ (E[H]) λ (E[H])
1 2 1 2
| − |−k k | − |
e e e e
Combining the two cases, we obtain that for any R , the following upper bound always
k k
holds
2 R
vˆvˆ ∗ vv ∗ k k .
k − k ≤ λ (E[H]) λ (E[H])
1 2
| − |
Since for any rank r Hermitian matrix H, H Fe √r H . eTherefore, we have that
k k ≤ k k
2√2 R
vv ∗ vˆvˆ ∗ F √2 vv ∗ vˆvˆ ∗ k k .
k − k ≤ k − k ≤ λ (E[H]) λ (E[H])
1 2
| − |
e e
C.4 Proof of Lemma 6
Lemma 12 (Matrix Bernstein Tropp et al. (2015)) Consider a finite sequence S
k
{ }
of independent, random matrices with dimension d. Assume that
ES = 0 and S L for each index k.
k k
k k ≤
43For the random matrix Z = S , let v(Z) be the matrix variance statistic of the sum:
k k
P
v(Z) = max E(S S ) , E(S S ) .
k k∗ k∗ k
((cid:13) (cid:13) (cid:13) (cid:13))
(cid:13)Xk (cid:13) (cid:13)Xk (cid:13)
(cid:13) (cid:13) (cid:13) (cid:13)
(cid:13) (cid:13) (cid:13) (cid:13)
Then, for all t 0, (cid:13) (cid:13) (cid:13) (cid:13)
≥
t2/2
P Z t 2dexp − .
{k k ≥ }≤ v(Z)+Lt/3
(cid:18) (cid:19)
Lemma 6 (Bound on random perturbation R) Consideradirectedgraph fromDSBM
(n ,n ,p,q,η) and its Hermitian matrix representation H. We use p = max p,q to
1 2 max
{ }
denote the maximum edge probability. Assume that the maximum edge probability is above
the connectivity threshold, i.e., e
Np = Ω(logN). (12)
max
Then there exist an absolute constant ǫ and
logN
C = (2+ǫ) w2+w2 +1 = Θ w2+w2 , (13)
r i Np r i
q (cid:18) max (cid:19) (cid:18)q (cid:19)
such that the random perturbation R = H E[H] has
−
P( R C eNp maxelogN) N −ǫ.
k k ≥ ≤
p
Proof Recall that by definition random perturbation R = H E[H] is Hermitian. We first
−
decompose it into summation of perturbations on different entries R = Rjl where Rjl
j<l
is also a random Hermitian and only has non-zero entries aet (j,l)eand (l,j). If j,l belongs
P
to the same community σ(j) = σ(l)
w (1 p)+iw w.p. p/2
r i
−
jl
R = w (1 p) iw w.p. p/2 (31)
jl  r − − i

 w p w.p. 1 p.
r
− −

Ifj,lbelongstodifferentcommunitiesσ(j) = σ(l),andwithoutlossofgeneralityweassume
6
j ,l
1 2
∈ C ∈C
w (1 q)+iw (1 (1 2η)q) w.p. q(1 η)
r i
− − − −
jl
R = w (1 q) iw (1+(1 2η)q) w.p. qη (32)
jl  r − − i −
  w q iw (1 2η)q w.p. 1 q
r i
− − − −


44From the Matrix Bernstein’s inequality in Lemma 12, we have for any t 0
≥
t2/2
P( R t) 2N exp − ,
k k ≥ ≤ Var(R)+Lt/3
(cid:18) (cid:19)
where L is an upper upper of Rjl and Var(R) is the variance.
k k
For computing L, recall that by definition
Rjl L, j = l.
k k ≤ ∀ 6
Here the matrix spectral norm can be simplified to be upper bounded by R because
jl
| |
the spectral norm is always upper bounded by the maximum absolute values of each en-
try. Therefore, it suffices to take L as an upper bound on max R . From (31), we
j=l jl
6 | |
have that, if σ(j) = σ(l), then R w2(1 p)2+w2; if σ(j) = σ(l), then R
| jl | ≤ r − i 6 | jl | ≤
w2(1 q)2+w2(1+(1 2η)q)2. Combq ining the two, it suffices for us to take
r − i −
q
L = w2(1 p )2+w2(1+(1 2η)q)2 2 w2+w2. (33)
r − min i − ≤ r i
q q
To compute the variance term Var(R), first recall by definition
Var(R) = max E[Rjl(Rjl) ] , E[(Rjl) Rjl] .
∗ ∗
(cid:13) (cid:13) (cid:13) (cid:13)
(cid:13) (cid:13)Xj<l (cid:13)
(cid:13)
(cid:13) (cid:13)Xj<l (cid:13) (cid:13)
(cid:13) (cid:13) (cid:13) (cid:13)
For each j < l, we have Rjl(Rj(cid:13) (cid:13)l)
∗
= (Rjl) ∗Rjl a(cid:13) (cid:13)nd(cid:13) (cid:13)we use Mjl to(cid:13) (cid:13)denote the product
matrix. In Mjl, the only two non zero entries are Mjl and Mjl and Mjl = Mjl = R R .
jj ll jj ll jl jl
Therefore, E[Rjl(Rjl) ] also only has two non-zero entries at (j,j) and (l,l) for every j < l
∗
andthus,thespectralnormofthematrixsummationissimplythelargestdiagonalelement,
i.e.,
Var(R) = max
E[Mjl
]. (34)
jj
j [N]
∈ l=j
X6
jl
In (34), M is a real random variable whose distribution can be derived from (31) and
jj
(32), and we have that, if σ(j) = σ(l), then
w2(1 p)2+w2 w.p. p
Mjl = r − i
jj (w r2p2 w.p. 1 p,
−
and E[Mjl ]= w2p(1 p)+pw2.
jj r − i
45If σ(j) = σ(l), then
6
w2(1 q)2+w2(1 (1 2η)q)2 w.p. q(1 η)
r − i − − −
Mjl = w2(1 q)2+w2(1+(1 2η)q)2 w.p. qη
jj  r − i −
 w2q2+w2(1 2η)2q2 w.p.1 q,
r i − −
and E[Mjl ] = w2q(1 q )+w2q(1 (1 2η)2q). Since, without loss of generality, we assume
jj r − i − −
p,q 0.5, thus for all j = l we have E[Mjl ] w2p (1 p )+w2p . Therefore, from
≤ 6 jj ≤ r max − max i max
(34), we arrive at
Var(R) N(w2p (1 p )+w2p ) Np (w2+w2). (35)
r max max i max max r i
≤ − ≤
Using the Matrix Bernstein’s inequality, we have for t = C√Np logN,
max
C2Np logN
P( R t) 2exp max +logN
k k ≥ ≤ −2Var(R)+2LC√Np logN/3
(cid:18) max (cid:19)
C2Np logN
max
2exp +logN (36)
≤ −2Np (w2+w2)+2L√Np logN/3
(cid:18) max r i max (cid:19)
C2
= 2exp logN +logN .
−2(w r2+w i2)+2LC/3 logN/Np max !
Here (36) follows from the analysis on Var(R) in (35p). From (37), if there exist an absolute
ǫ, such that
C2
2+ǫ, (37)
(w2 +w2)+LC/3 logN/Np ≥
r i max
then we have P( R t) N −ǫ, whichpconclude the proof. It turns out that we can
k k ≥ ≤
always find an absolute constant C such that (37) holds. To see this, first note that (37)
is equivalent to
logN logN
C (1+ǫ/2)L + (2+ǫ)(w2 +w2)+(1+ǫ/2)2L2 .
≥ sNp max s r i Np max
Since a2+b2 (a+b)2 for a,b > 0, it suffices to let
≤
logN
C = (2+ǫ)L +(2+ǫ) w2 +w2.
Np r i
max
q
Since L 2 w2+w2, we have
≤ r i
q
logN
C (2+ǫ) w2+w2 +1 = Θ w2+w2 , (38)
≤ r i Np r i
q (cid:18) max (cid:19) (cid:18)q (cid:19)
where the last equality is due to the connectivity assumption (12).
46C.5 Useful theorem in k-means error analysis
Lemma 13 (k-means error adapted from Lemma 5.3 in Lei and Rinaldo (2015))
For ǫ > 0 and any two matrices Uˆ,U, such that U = MX with M 0,1 N 2 be the indi-
×
∈ { }
cator matrix and X R2 2 have its row vectors representing the centroids of two clusters,
×
let(Mˆ,Xˆ) be a(1+ǫ∈ )solution to the k-means problem and U¯ = MˆXˆ. For δ = X X ,
1 2
define S = j [N] : U¯ U δ/2 then
k ∗− ∗k
j j
{ ∈ k ∗− ∗k} ≥
S δ2 4(4+2ǫ) Uˆ U 2. (39)
F
| | ≤ k − k
C.6 Proof of Corollary 3
Corollary 14 Consider directed graphs generated from the DSBM (N/2,N/2,p,p,η). As
N , if η 0.5 ǫ with an absolute constant ǫ > 0, then the misclustering error of
→ ∞ ≤ −
Algorithm 1 is such that
l(σ,σˆ) logN
= O (8)
N Np
(cid:18) (cid:19)
If η = 0.5 o(1), we have that
−
l(σ,σˆ) logN
= ω . (9)
N Np
(cid:18) (cid:19)
Proof From Theorem 2, we have the general upper bound on the error bound
l(σ,σˆ) 64(2+ǫ)C2p logN C2p logN
max max
= Θ , (40)
N ≤ d2∆2 d2∆2
(cid:18) (cid:19)
where d and ∆ depends on E[H]. When p = q, we have that
1 1 η
w =log e , w = log − , w = 0.
r i c
4η(1 η) η
(cid:18) − (cid:19)
Moreover, notice that normalizing H does not affect the clustering error. For the rest of
the discussion, we consider 1/w H as the input Hermitian matrix for Algorithm 1, and
i
correspondingly we denote the updaeted coefficient as
e
1 1 η
w˜ = log /log − , w˜ = 1, w˜ = 0.
r i c
4η(1 η) η
(cid:18) − (cid:19) (cid:18) (cid:19)
Because w˜ 1andw˜ = 1, theterm C2 in(40)hasC2 = Θ(w˜ 2+w˜2)= Θ(1). Therefore,
r i r i
≤
we can further simplify (40) as follows
l(σ,σˆ) plogN
Θ .
N ≤ d2∆2
(cid:18) (cid:19)
47Foranalyzingtheasymptoticbehaviouroftheerrorbound,weareonlyleftwithcomputing
the centroid distance d and eigengap bound ∆.
Following from the definition of ∆ in (11), we have
Np
∆ = w˜ 2+w˜2(1 2η)2. (41)
r i
2 −
q
For computing the centroid distance d, recall that the population matrix can be written
as
E[H]= MQMT w˜ pI,
r
−
where M is the community indicatoer matrix and the 2 2 matrix Q has
×
w˜ w˜ +(1 2η)i
r r
Q = − p.
w˜ (1 2η)i w˜
r r
(cid:20) − − (cid:21)
Since n = n = N/2, we have that the two distinct values in v (E[H]) (see (27)) to be
1 2 1
the values of v (Q) divided by N/2. We can easily compute that the top eigenvector
1
has e
p
v (E[H]) =
√Nwr+ wrw +i( w1
−
i(12η) 2i
η)i for u ∈ C1 (42)
1 |− − |
(1/√N for u 2.
∈ C
e
Wedenotec¯ ,c¯ theclustercentroidsof , intheembeddinggivenbythetopeigenvector
1 2 1 2
C C
of E[H]. The locations of two cluster centroids in the complex plane are exactly the
two distinct values in (42). We visualize the two cluster centroids in Figure 7a. Let
θ = aerccos wr be the angle between the two values in the complex plane.
wr+wi(1 2η)i
| − |
Therefore, w(cid:16)e have (cid:17)
1 4sin2θ/2
d2 = (1 cosθ)= . (43)
N − N
Combining (40), (41), and (43) and letting L(η) = w˜ +i(1 2η) sin(θ/2), we have
r
| − |
l(σ,σˆ) logN
Θ . (44)
N ≤ NpL2
(cid:18) (cid:19)
From the above inequality (44), the upper bound of misclustering error is determined by
two independent variables Np and L2. The term Np is the average degree of the graph.
The term L2, by definition, is a function on η. To see how the value L changes as η
varies from 0 to 0.5, we plot L(η) in Figure 7b using Mathematica Inc.. We observe that
48#
!
!"+’()*,
"#$!!%&’()*!"$
"
!
#
"
!!
"#$!!%&’()*!"$ ’- "
(a) Visualization on c¯1,c¯2,d and θ. (b) Plot of L as function of η.
Figure 7: Visualization of important parameters for representing the error bound.
L(η) = Θ(1) when η is bounded away from 0.5. Therefore, if η 0.5 ǫ with an absolute
≤ −
constant ǫ > 0, then the misclustering error of Algorithm 1 is such that
l(σ,σˆ) logN
= O
N Np
(cid:18) (cid:19)
When η converges to 0.5 (when the imbalance structure disappears), L(η) converges to 0.
Therefore, if η = 0.5 o(1), we have that
−
l(σ,σˆ) logN
= ω .
N Np
(cid:18) (cid:19)
The above results on misclustering error bounds agree with the intuition that lower values
of η denote a less noisy problem instance, and thus lead to a lower clustering error.
49Appendix D. Additional experimental details
D.1 Experiments on DSBM with known model parameters
1 1 1
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
0 0 0
0 0.1 0.2 0.3 0.4 0.5 0 0.1 0.2 0.3 0.4 0.5 0 0.1 0.2 0.3 0.4 0.5
1 1 1
Herm
HermRW
0.8 0.8 0.8 DI-SIM
B-Sym
0.6 0.6 0.6 A+AT
MLE-SC
MLE-SDP
0.4 0.4 0.4
0.2 0.2 0.2
0 0 0
0 0.1 0.2 0.3 0.4 0.5 0 0.1 0.2 0.3 0.4 0.5 0 0.1 0.2 0.3 0.4 0.5
Figure 8: Cluster DSBM with n = n = 1000, different p,q and varying η. MLE-SC
1 2
represents Algorithm 1 with true model parameters; MLE-SDP represents Algorithm 3
with true model parameters.
50D.2 Visualization on directed adjacency matrices
InFigure9,wetesttheclusteringalgorithmsonDSBMandvisualizetheadjacencyrelation
before and after clustering. To provide a clear visual demonstration, we generate directed
graphs from DSBM with only 200 vertices.
input graph Herm, ARI = 0.15 HermRW, ARI = 0.05 DI-SIM, ARI = 0.43
50 50 50 50
100 100 100 100
150 150 150 150
200 200 200 200
50 100 150 200 50 100 150 200 50 100 150 200 50 100 150 200
B-Sym, ARI = 0.41 A+AT, ARI = 0.43 MLE-SC, ARI = 0.88 MLE-SDP, ARI = 0.86
50 50 50 50
100 100 100 100
150 150 150 150
200 200 200 200
50 100 150 200 50 100 150 200 50 100 150 200 50 100 150 200
Figure 9: Visualization of A AT before and after clustering where A is sampled from
−
DSBM with n = n =100,p = 0.1,q = 0.05,η = 0.1.
1 2
51input graph Herm, ARI = 0.62 HermRW, ARI = 0.56 DI-SIM, ARI = 0.61
50 50 50 50
100 100 100 100
150 150 150 150
200 200 200 200
50 100 150 200 50 100 150 200 50 100 150 200 50 100 150 200
B-Sym, ARI = 0.04 A+AT, ARI = 0.01 MLE-SC, ARI = 0.64 MLE-SDP, ARI = 0.67
50 50 50 50
100 100 100 100
150 150 150 150
200 200 200 200
50 100 150 200 50 100 150 200 50 100 150 200 50 100 150 200
Figure 10: Visualization of A AT before and after clustering where A is sampled from
−
DSBM with n = n =100,p = 0.05,q = 0.05,η = 0.1.
1 2
D.3 Additional real-world dataset
Data set Herm HermRW DI-SIM B-Sym A+AT MLE-SC MLE-SDP
PolBlog 3 0.008 -0.000 -0.001 0.148 0.177 0.014 0.105
Table 3: ARIs from experiments on the real-world PolBlog data set.
4 Adamicand Glance (2005)
52ground truth Herm HermRW DI-SIM
100 200 200 200
200 400 400 400
300 600 600 600
400 800 800 800
500 1000 1000 1000
600
1200 1200 1200
200 400 600 200 600 1000 200 600 1000 200 600 1000
B-Sym A+AT MLE-SC, ARI = 0.06 MLE-SDP, ARI = 0.00
200 200 200 200
400 400 400 400
600 600 600 600
800 800 800 800
1000 1000 1000 1000
1200 1200 1200 1200
200 600 1000 200 600 1000 200 600 1000 200 600 1000
Figure 11: Visualization of A AT before and after clustering where A is the adjacency
−
matrix of the PloBlog graph.
53D.4 Experiments with different initialization strategies
In this section, we demonstrate by example how different initialization strategies of Algo-
rithm 4 may lead to different learning outcomes. We conduct experiments on the following
three strategies:
Strategy 1. Total flow preferred. We initialize with H = A+AT. We call this total
0
flow preferred initialization as x H x = C TF( , ) according to our optimization
∗ 0 1 2
− C C
interpretation in Section 3.2.
Strategy 2. Net flow preferred. We initialize with H = i(A AT). We call this
0
−
net flow preferred initialization as x H x = NF( , ) according to our discussion in
∗ 0 1 2
C C
Section 3.2.
Strategy 3. Balanced between net flow and total flow. We initialize with H =
0
i(A AT)+(A+AT), where we assign equal weights on TF( , ) and NF( , ).
1 2 1 2
− C C C C
We visualize the clustering results using the above strategies on the DSBM dataset (Fig-
ure 12) and email-Eu-core dataset (Figure 13, Figure 14). From the conducted experi-
ments, we observe that directed graphs may have different types of clusters: one values
max pˆ,qˆ
more on edge density difference and leads to large { }; the other cares more about
min pˆ,qˆ
{ }
between-cluster edge orientation, which produces small ηˆ. When an input graph may ex-
hibit different clustering possibilities, we observe that initializing with H = i(A AT)
0
−
tends to converge to edge orientation favored clustering, while H = A + AT are more
0
likely to converge to clustering based on edge density.
54ground truth input graph MLE-SC, ARI = 0.01 MLE-SDP, ARI = 0.01
50 50 50 50
100 100 100 100
150 150 150 150
200 200 200 200
50 100 150 200 50 100 150 200 50 100 150 200 50 100 150 200
ground truth input graph MLE-SC, ARI = 0.79 MLE-SDP, ARI = 0.76
50 50 50 50
100 100 100 100
150 150 150 150
200 200 200 200
50 100 150 200 50 100 150 200 50 100 150 200 50 100 150 200
ground truth input graph MLE-SC, ARI = 0.79 MLE-SDP, ARI = 0.77
50 50 50 50
100 100 100 100
150 150 150 150
200 200 200 200
50 100 150 200 50 100 150 200 50 100 150 200 50 100 150 200
Figure 12: A AT before and after clustering. We test on a directed graph sampled from
−
DSBM with n = 100,n = 100,p = 0.05,q = 0.05,η = 0.1.
1 2
Results presented on the first row correspond to total flow preferred initialization, where
MLE-SC and MLE-SDP are both initialized with H = A+AT. Results presented on the
0
second row correspond to net flow preferred initialization, where MLE-SC and MLE-SDP
are both initialized with H = i(A AT). Results presented on the third row correspond
0
−
to balanced initialization, where MLE-SC and MLE-SDP are both initialized with H =
0
i(A AT)+A+AT.
−
55ground truth input MLE-SC, ARI = 0.63 MLE-SDP, ARI = 0.96
50 50 50 50
100 100 100 100
150 150 150 150
50 100 150 50 100 150 50 100 150 50 100 150
ground truth input MLE-SC, ARI = 0.00 MLE-SDP, ARI = -0.00
50 50 50 50
100 100 100 100
150 150 150 150
50 100 150 50 100 150 50 100 150 50 100 150
ground truth input MLE-SC, ARI = 0.63 MLE-SDP, ARI = -0.01
50 50 50 50
100 100 100 100
150 150 150 150
50 100 150 50 100 150 50 100 150 50 100 150
Figure 13: A AT before and after clustering. We test on email-Eu-core12. Results
−
presented on the first row correspond to total flow preferred initialization, where MLE-SC
and MLE-SDP are both initialized with H = A+AT. Results presented on the second
0
row correspond to net flow preferred initialization, where MLE-SC and MLE-SDP are
both initialized with H = i(A AT). Results presented on the third row correspond
0
−
to balanced initialization, where MLE-SC and MLE-SDP are both initialized with H =
0
i(A AT)+A+AT.
−
56ground truth input MLE-SC, ARI = 0.58 MLE-SDP, ARI = 1.00
20 20 20 20
40 40 40 40
60 60 60 60
80 80 80 80
100 100 100 100
120 120 120 120
140 140 140 140
20 40 60 80100120140 20 40 60 80100120140 20 40 60 80100120140 20 40 60 80100120140
ground truth input MLE-SC, ARI = -0.01 MLE-SDP, ARI = -0.01
20 20 20 20
40 40 40 40
60 60 60 60
80 80 80 80
100 100 100 100
120 120 120 120
140 140 140 140
20 40 60 80100120140 20 40 60 80100120140 20 40 60 80100120140 20 40 60 80100120140
ground truth input MLE-SC, ARI = 0.58 MLE-SDP, ARI = -0.01
20 20 20 20
40 40 40 40
60 60 60 60
80 80 80 80
100 100 100 100
120 120 120 120
140 140 140 140
20 40 60 80100120140 20 40 60 80100120140 20 40 60 80100120140 20 40 60 80100120140
Figure 14: A AT after clustering. We test on email-Eu-core12. Results presented on the
−
first row correspond to total flow preferred initialization, where MLE-SC and MLE-SDP
are both initialized with H = A+AT. Results presented on the second row correspond to
0
net flow preferred initialization, where MLE-SC and MLE-SDP are both initialized with
H = i(A AT). Results presented on the third row correspond to balanced initialization,
0
−
where MLE-SC and MLE-SDP are both initialized with H = i(A AT)+A+AT.
0
−
57