[
    {
        "title": "Minimax Optimal and Computationally Efficient Algorithms for Distributionally Robust Offline Reinforcement Learning",
        "authors": "Zhishuai LiuPan Xu",
        "links": "http://arxiv.org/abs/2403.09621v1",
        "entry_id": "http://arxiv.org/abs/2403.09621v1",
        "pdf_url": "http://arxiv.org/pdf/2403.09621v1",
        "summary": "Distributionally robust offline reinforcement learning (RL), which seeks\nrobust policy training against environment perturbation by modeling dynamics\nuncertainty, calls for function approximations when facing large state-action\nspaces. However, the consideration of dynamics uncertainty introduces essential\nnonlinearity and computational burden, posing unique challenges for analyzing\nand practically employing function approximation. Focusing on a basic setting\nwhere the nominal model and perturbed models are linearly parameterized, we\npropose minimax optimal and computationally efficient algorithms realizing\nfunction approximation and initiate the study on instance-dependent\nsuboptimality analysis in the context of robust offline RL. Our results uncover\nthat function approximation in robust offline RL is essentially distinct from\nand probably harder than that in standard offline RL. Our algorithms and\ntheoretical results crucially depend on a variety of new techniques, involving\na novel function approximation mechanism incorporating variance information, a\nnew procedure of suboptimality and estimation uncertainty decomposition, a\nquantification of the robust value function shrinkage, and a meticulously\ndesigned family of hard instances, which might be of independent interest.",
        "updated": "2024-03-14 17:55:10 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.09621v1"
    },
    {
        "title": "Extremal graphical modeling with latent variables",
        "authors": "Sebastian EngelkeArmeen Taeb",
        "links": "http://arxiv.org/abs/2403.09604v1",
        "entry_id": "http://arxiv.org/abs/2403.09604v1",
        "pdf_url": "http://arxiv.org/pdf/2403.09604v1",
        "summary": "Extremal graphical models encode the conditional independence structure of\nmultivariate extremes and provide a powerful tool for quantifying the risk of\nrare events. Prior work on learning these graphs from data has focused on the\nsetting where all relevant variables are observed. For the popular class of\nH\\\"usler-Reiss models, we propose the \\texttt{eglatent} method, a tractable\nconvex program for learning extremal graphical models in the presence of latent\nvariables. Our approach decomposes the H\\\"usler-Reiss precision matrix into a\nsparse component encoding the graphical structure among the observed variables\nafter conditioning on the latent variables, and a low-rank component encoding\nthe effect of a few latent variables on the observed variables. We provide\nfinite-sample guarantees of \\texttt{eglatent} and show that it consistently\nrecovers the conditional graph as well as the number of latent variables. We\nhighlight the improved performances of our approach on synthetic and real data.",
        "updated": "2024-03-14 17:45:24 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.09604v1"
    },
    {
        "title": "Variational Inference with Sequential Sample-Average Approximations",
        "authors": "Heiko ZimmermannChristian A. NaessethJan-Willem van de Meent",
        "links": "http://arxiv.org/abs/2403.09429v1",
        "entry_id": "http://arxiv.org/abs/2403.09429v1",
        "pdf_url": "http://arxiv.org/pdf/2403.09429v1",
        "summary": "We present variational inference with sequential sample-average approximation\n(VISA), a method for approximate inference in computationally intensive models,\nsuch as those based on numerical simulations. VISA extends importance-weighted\nforward-KL variational inference by employing a sequence of sample-average\napproximations, which are considered valid inside a trust region. This makes it\npossible to reuse model evaluations across multiple gradient steps, thereby\nreducing computational cost. We perform experiments on high-dimensional\nGaussians, Lotka-Volterra dynamics, and a Pickover attractor, which demonstrate\nthat VISA can achieve comparable approximation accuracy to standard\nimportance-weighted forward-KL variational inference with computational savings\nof a factor two or more for conservatively chosen learning rates.",
        "updated": "2024-03-14 14:20:22 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.09429v1"
    },
    {
        "title": "Scalability of Metropolis-within-Gibbs schemes for high-dimensional Bayesian models",
        "authors": "Filippo AscolaniGareth O. RobertsGiacomo Zanella",
        "links": "http://arxiv.org/abs/2403.09416v1",
        "entry_id": "http://arxiv.org/abs/2403.09416v1",
        "pdf_url": "http://arxiv.org/pdf/2403.09416v1",
        "summary": "We study general coordinate-wise MCMC schemes (such as\nMetropolis-within-Gibbs samplers), which are commonly used to fit Bayesian\nnon-conjugate hierarchical models. We relate their convergence properties to\nthe ones of the corresponding (potentially not implementable) Gibbs sampler\nthrough the notion of conditional conductance. This allows us to study the\nperformances of popular Metropolis-within-Gibbs schemes for non-conjugate\nhierarchical models, in high-dimensional regimes where both number of\ndatapoints and parameters increase. Given random data-generating assumptions,\nwe establish dimension-free convergence results, which are in close accordance\nwith numerical evidences. Applications to Bayesian models for binary regression\nwith unknown hyperparameters and discretely observed diffusions are also\ndiscussed. Motivated by such statistical applications, auxiliary results of\nindependent interest on approximate conductances and perturbation of Markov\noperators are provided.",
        "updated": "2024-03-14 14:04:44 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.09416v1"
    },
    {
        "title": "Pantypes: Diverse Representatives for Self-Explainable Models",
        "authors": "Rune KjærsgaardAhcène BoubekkiLine Clemmensen",
        "links": "http://arxiv.org/abs/2403.09383v1",
        "entry_id": "http://arxiv.org/abs/2403.09383v1",
        "pdf_url": "http://arxiv.org/pdf/2403.09383v1",
        "summary": "Prototypical self-explainable classifiers have emerged to meet the growing\ndemand for interpretable AI systems. These classifiers are designed to\nincorporate high transparency in their decisions by basing inference on\nsimilarity with learned prototypical objects. While these models are designed\nwith diversity in mind, the learned prototypes often do not sufficiently\nrepresent all aspects of the input distribution, particularly those in low\ndensity regions. Such lack of sufficient data representation, known as\nrepresentation bias, has been associated with various detrimental properties\nrelated to machine learning diversity and fairness. In light of this, we\nintroduce pantypes, a new family of prototypical objects designed to capture\nthe full diversity of the input distribution through a sparse set of objects.\nWe show that pantypes can empower prototypical self-explainable models by\noccupying divergent regions of the latent space and thus fostering high\ndiversity, interpretability and fairness.",
        "updated": "2024-03-14 13:34:30 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.09383v1"
    }
]