EXTREMAL GRAPHICAL MODELING WITH LATENT VARIABLES
SEBASTIANENGELKE1 ANDARMEENTAEB2
Abstract. Extremalgraphicalmodelsencodetheconditionalindependencestructureofmultivari-
ate extremes and provide a powerful tool for quantifying the risk of rare events. Prior work on
learningthesegraphsfromdatahasfocusedonthesettingwhereallrelevantvariablesareobserved.
ForthepopularclassofHu¨sler–Reissmodels,weproposetheeglatentmethod,atractableconvex
program for learning extremal graphical models in the presence of latent variables. Our approach
decomposestheHu¨sler–Reissprecisionmatrixintoasparsecomponentencodingthegraphicalstruc-
tureamongtheobservedvariablesafterconditioningonthelatentvariables,andalow-rankcompo-
nentencodingtheeffectofafewlatentvariablesontheobservedvariables. Weprovidefinite-sample
guarantees of eglatentand showthat it consistently recoversthe conditionalgraph aswellas the
number of latent variables. We highlight the improved performances of our approach on synthetic
andrealdata.
1. Introduction
Floods, heat waves, and financial crashes illustrate the environmental and economic hazards pri-
marily influenced by rare, yet significant, events. Such catastrophic scenarios often result from the
simultaneous occurrence of extreme values across multiple variables (Asadi et al., 2015, Zhou, 2009,
Zscheischler and Seneviratne., 2017). To effectively measure and mitigate these disasters, it is es-
sential to understand the dependencies between the various risk factors. From a mathematical per-
spective, this requires examining the tail dependence between the components of the random vector
X =(X ,...,X ). Extremevaluetheoryprovidesthetheoreticalfoundationforextrapolationstothe
1 d
distributionaltailoftherandomvectorX. Withinthemultivariatesetting,therearetwodifferentyet
closely related approaches for modeling extremal data. The first method considers component-wise
maxima of independent copies of X and leads to the notion of max-stable distributions (de Haan
and Resnick, 1977). The second method relies on multivariate Pareto distributions that describe the
random vector X conditioned on the event that there is an extreme in one of the coordinates of X
(Rootz´en and Tajvidi, 2006).
Given the increasing complexity and dimensionality of contemporary data sets, identifying sparse
representationsfordistributionsofextremeeventsiscriticalforaccuratemodelingandriskassessment
(Engelke and Ivanovs, 2021). Graphical models serve as powerful tools in achieving such sparse rep-
resentations, offering clear and interpretable models for understanding dependencies among variables
(Lauritzen, 1996). However, in the framework of max-stable distributions, Papastathopoulos and
Strokorb (2016) highlighted limitations in developing non-trivial graphical models for their densities.
On the other hand, multivariate Pareto distributions do not face these limitations. Indeed, Engelke
andHitz(2020)introducedextremalgraphicalmodelsthatfactorizeaccordingtomultivariatePareto
distributions and encode extremal conditional independence relationships, and Segers (2020) showed
that extremal trees naturally arise as limits of Markov trees. For the popular Hu¨sler–Reiss family
1Research Center for Statistics, GSEM, University of Geneva, Switzerland
2Department of Statistics, University of Washington, U.S.
E-mail addresses: sebastian.engelke@unige.ch, ataeb@uw.edu.
Date:March15,2024.
1
4202
raM
41
]EM.tats[
1v40690.3042:viXra2 S.ENGELKEANDA.TAEB
(Hu¨sler and Reiss, 1989), Hentschel et al. (2022) showed that, similarly to the Gaussian case, the
sparsitypatternofanextremalgraphicalmodelcanbereadofffromapositivesemi-definiteprecision
matrix Θ with the all-ones vector in its null space. This precision matrix Θ is derived from a trans-
formation of the variogram matrix Γ that parameterizes a Hu¨sler–Reiss distribution. Several recent
papers have proposed methods to learn the extremal graphical structure from data (Engelke et al.,
2022c, Engelke and Volgushev, 2022, Hu et al., 2022, Lederer and Oesting, 2023, Wan and Zhou,
2023).
The study and techniques for modeling extremes have so far concentrated on scenarios where all
relevant variables are directly observable. However, in many real-world situations, there exist latent
variables that are not observable due to prohibitive costs or other practical constraints. Mathemati-
cally, the overall system of variables is then given by X =(X ,X ), where X are the observed and
O H O
X the latent variables, with (O,H)={1,2,...,d}. The importance of accounting for latent factors
H
becomes apparent in the example of a single latent variable X ={X }, where the data is generated
H c
through the one-factor model
X =X +ε , j ∈O.
j c j
Here, X is the common (unobserved) factor influencing all observed variables, and ε , j ∈ O, are
c j
independentnoiseterms. SupposethattheexceedancesoftherandomvectorX convergeindistribu-
tion to a multivariate Pareto distribution Y =(Y ,Y ); a concrete example where this is satisfied is
O H
when X is standard exponential and the noise variables are normally distributed, in which case Y
H
hasaHu¨sler–Reissdistribution,butmanyothercombinationsarepossible(Engelkeetal.,2019). The
joint vector Y can be shown to be an extremal graphical model with respect to the star graph on the
left-hand side of Figure 1, where the observed variables Y are conditionally independent given the
O
latentvariableY . However,thesub-modelmodelofY correspondingtotheobservedvariables,that
H
is, the limiting multivariate Pareto distribution arising from threshold exceedances of X , induces,
O
in general, the fully connected extremal graph on the right-hand side of Figure 1, where are all the
variables are conditionally dependent.
This simple example illustrates that ignoring the effect of latent variables induces confounding
dependencies among the observed variables: even for a sparse joint graph of observed and latent
variables, any two observed variables are dependent when conditioning on the remaining observed
variables. This phenomenon also appears in many real-world applications. In such cases, a latent
extremal graphical model with possibly more than one latent variable Y serves multiple purposes:
H
(i) it obtains the number of latent variables h = |H| that summarize the effect of external
phenomena on the observed variables;
(ii) it identifies the residual graph structure among the observed variables after extracting away
the effect of these external factors;
(iii) it often yields a more sparsely represented and accurate statistical model than a model that
ignores the latent variables.
Latent extremal graphical models have only been studied when the graphical structure among the
observedandlatentvariablesisatree,andwherethetreestructureisassumedtobeknown(Asenova
and Segers, 2023, R¨ottger et al., 2023b).
1.1. Our contributions. We introduce a general latent Hu¨sler–Reiss graphical model where the
graphical structure among the observed and latent variables as well as the number of latent variables
maybearbitrary. LettingΘ∈Rd×d betheprecisionmatrix,akeyresultthatweestablishisthatthe
marginal precision matrix Θ˜ ∈ Rp×p over the observed variables can be expressed in terms of blocksEXTREMAL GRAPHICAL MODELING WITH LATENT VARIABLES 3
O
1 O
1
O H O O O
4 2 4 2
O
O 3
3
Figure 1. One-factor graph with one latent variable with four observed variables O1,...,O4 and
onelatentvariableH (left)anditsmarginalizationontheobservedvariables(right).
of Θ as
(cid:18) (cid:19)
Θ Θ
Θ˜ =Θ −Θ Θ−1Θ , where Θ= O OH .
O OH H HO Θ Θ
HO H
TherepresentationofΘresemblestheSchurcomplementinGaussianlatentvariablegraphicalmodels
(Chandrasekaran et al., 2012). However, in the Hu¨sler–Reiss case, the matrices Θ and Θ˜ are not
invertible since they have the all-one vector in their kernel, and the link between our representation
and the Schur complement is therefore non-trivial.
Assuming that the conditional graph among the observed variables is sparse and that there are a
fewlatentvariablesinfluencingtheobservedvariables,themarginalprecisionmatrixΘ˜ isdecomposed
as the sum of a sparse and a low-rank matrix. The sparse component Θ encodes the conditional
O
graphical structure among the observed variables after conditioning on the latent variables and the
low-rank component Θ Θ−1Θ encodes the effect of a few latent variables on the observed vari-
OH H HO
ables. Using this decomposition, we propose a convex optimization procedure named eglatent that
estimates each term in the decomposition without knowledge of the underlying graphical structure
or the number of latent variables. Under some identifiability assumptions, we provide finite-sample
consistency guarantees for our estimator, showing that our procedure recovers the conditional graph
and the number of latent variables. Our identifiability conditions assume that the number of latent
variables is small (compared to the observed variables) and they affect many observed variables.
Figure2highlightstheadvantageofourmethodeglatentovertheexistingextremalgraphlearning
methodeglearn(Engelkeetal.,2022c),whichdoesnotaccountforlatentvariables. Inthissynthetic
example, we generated 5000 approximate observations from an extremal graphical model with h=2
latent variables and a cycle graph among p = 30 observed variables, and fitted both methods for
differentvaluesoftheregularizationparameters; seeSection5.1.1fordetailsonthesetup. Compared
to eglearn, our eglatent produces a better model fit on validation data and more accurate graph
estimates among the observed variables in terms of F-score. Indeed, due to the latent confounding,
the marginal graph among the observed variables, encoded by the zero pattern in Θ˜, is dense, and
thus the sparsity that eglearn exploits is not appropriate: the best validated eglearn model has
283 edges while the true graph has 30 edges. On the other hand, conditional on the latent variables,
the conditional graph among the observed variables, encoded by the zero pattern in Θ , is sparse,
O
and eglatent exploits this structure. Furthermore, eglatent estimates the correct number of latent
variables and a near-perfect graph among the observed variables for regularization parameters with
highvalidationlikelihood. Notethatintheleftplot,thecrossesforeglearnmeanthattheestimated
graphicalmodelisdisconnectedandthereforedoesnotleadtoavalidHu¨sler–Reissmodel. Incontrast,4 S.ENGELKEANDA.TAEB
16 6 5 4 3 2 2 2 2 2 2 2 2 2 2 25120112985 60 41 32 31 31 30 30 30 29 26 24
1.00 −276000
0.75
−276500
0.50
−277000
0.25
0.00 −277500
0 0.01 0.02 0.03 0.04 0.05 0 0.01 0.02 0.03 0.04 0.05
Regularization parameter l n Regularization parameter l n
Figure 2. Left: F-score of our proposed method eglatent (solid line) and eglearn (dashed line)
as function of the regularization parameter with larger F-scores being better; top axis shows the
number of estimated latent variables. Right: the likelihood of the same methods evaluated on a
validationdataset;thetopaxisshowsthenumberofestimatededgesinthelatentmodel.
eglatentalwaysyieldsavalidHu¨sler–Reissmodel. Moresimulationsandanapplicationtolargeflight
delays in the U.S. are presented in Section 5.
1.2. Notation. We denote I as an r×r identity matrix and denote 1 as the all-ones vector with r
r r
coordinates. Thecollectionofr×r symmetricmatricesisdenotedbySr. Thefollowingmatrixnorms
are employed throughout this paper: ∥M∥ denotes the spectral norm, or the largest singular value
2
of M; ∥M∥ denotes the largest entry in the magnitude of M; ∥M∥ denotes the nuclear norm, or
∞ ⋆
the sum of the singular values of M (this reduces to the trace for positive semidefinite matrices); and
∥M∥ denotes the sum of the absolute values of the entries of M. Finally, we will denote σ (M) as
1 min
the largest non-zero singular value of M.
2. Background
2.1. Multivariate extreme value theory. Multivariate extreme value theory studies asymptoti-
cally motivated models for the largest observations of a random vector X =(X :j ∈V) with index
j
setV ={1,...,d}. Sinceweconcentrateonmodelsfortheextremaldependencestructure,weassume
thatthemarginaldistributionsofX havebeenstandardizedtostandardexponentialdistributions. In
practice, this standardization can be achieved by using the marginal empirical distribution functions;
see Section 3.3.1.
A multivariate Pareto distribution models the multivariate tail of the distribution of X. It is
defined as the limit in distribution of the conditional exceedances over a high threshold u, that is,
(1) Y = lim (X−u|max(X ,...,X )>u),
1 d
u→∞
if the limit exists (Rootz´en and Tajvidi, 2006). Here the simple normalization by subtracting u in
each component of X is due to the exponential marginals. The random vector X is said to be in
the domain of attraction of the multivariate Pareto distribution Y, which is supported on the space
L = {y ∈ Rd : max(y ,...,y ) > 0}. Multivariate Pareto distributions are the only possible limits
1 d
of threshold exceedances (Rootz´en et al., 2018) and therefore a canonical model for extremes. If
the convergence in (1) holds, it is easy to see that for any non-empty subset I ⊂ V, the sub-vector
erocs−F
doohilekil−goLEXTREMAL GRAPHICAL MODELING WITH LATENT VARIABLES 5
X =(X :j ∈I) is itself in the domain of attraction of a |I|-dimensional Pareto distribution, which
I j
we call the Ith sub-model of Y.
We now introduce the Hu¨sler–Reiss model, which is the most popular parametric sub-class of
multivariateParetodistributions. ItcanbeseenastheanalogofGaussiandistributionsinmultivariate
extreme value theory, a fact, that will become apparent when studying extremal graphical models in
the next section.
Definition 1. A multivariate Pareto distribution Y =(Y ,...,Y ) is called a Hu¨sler–Reiss distribu-
1 d
tion parameterized by the variogram matrix Γ in the space of conditionally negative definite matrices
(2) Cd ={Γ∈[0,∞)d×d :Γ=Γ⊤, diag(Γ)=0, v⊤Γv<0∀0̸=v⊥1},
if its density has the form
(cid:40) d (cid:41)
1 1(cid:88)
(3) f(y;Γ)=c exp − (y−µ )⊤Θ(y−µ )− y , y ∈L,
Γ 2 Γ Γ d i
i=1
where c > 0 is a normalizing constant, µ = Π(−Γ/2)1 , and Π = I −1 1⊤/d is the projection
Γ Γ d d d d
matrix onto the orthogonal complement of the all-ones vector in d-dimensions. The matrix Θ =
(Π(−Γ/2)Π)+ is the positive semi-definite Hu¨sler–Reiss precision matrix (Hentschel et al., 2022),
where A+ is the Moore–Penrose pseudoinverse of a matrix A.
The Hu¨sler–Reiss distribution is stable under marginalization, in the sense that for I ⊂ V, the
Hu¨sler–Reiss sub-model corresponding to the Ith marginal is again Hu¨sler–Reiss distributed with
parameter matrix Γ . While the density in (3) resembles the density of a multivariate normal dis-
I
tribution, we note that there are important differences. First, this function would not have finite
integral on Rd because of the second term in the exponential, and the restriction to the subset L is
crucial. Second, the precision matrix Θ is of rank d−1, which complicates theoretical and practical
considerations.
An important summary statistic of the dependence structure in multivariate Pareto distributions
is the extremal variogram (Engelke and Volgushev, 2022). It takes a similar role as the covariance
matrix in the non-extremal world.
Definition 2. For a multivariate Pareto distribution Y =(Y :j ∈V) the extremal variogram rooted
j
at node m∈V is defined as the matrix Γ(m) with entries
Γ(m) =Var{Y −Y |Y >1}, i,j ∈V,
ij i j m
whenever the right-hand side is finite.
If Y follows a Hu¨sler–Reiss distribution with parameter matrix Γ, it can be checked that the
extremal variogram matrices coincide for all m∈V, and that they satisfy
(4) Γ=Γ(1) =···=Γ(d).
We use this fact later to combine empirical estimators of the extremal variograms rooted at the
different nodes to obtain a more efficient joint estimator of Γ.
2.2. Extremal graphical models. Conditional independence for multivariate Pareto distributions
Y is non-standard since it is defined on the space L, which is not a product space. Engelke and
Hitz (2020) therefore define a new notion of extremal conditional independence using the auxiliary
vectors Y(m), for m ∈ {1,...,d}, defined as Y conditioned on the event that {Y > 0}. For non-
m
empty sub-sets A,B,C ⊂V, we say that Y is conditionallyindependent of Y given Y , denoted by
A B C6 S.ENGELKEANDA.TAEB
Y ⊥ Y |Y , if for all auxiliary random vectors, we have the corresponding statement in the usual
A e B C
sense, that is,
Y(m) ⊥⊥Y(m) |Y(m) for all m∈V.
A B C
It can be shown that requiring the relation above is equivalent to requiring the existence of a single
m∈V for which Y(m) ⊥⊥Y(m) |Y(m) (Engelke et al., 2022b).
A B C
Let G = (V,E) be an undirected graph with nodes V = {1,...,d} and edge set E ⊂ V ×V.
Usingthenewnotionofconditionalindependence,anextremalgraphicalmodelonG isamultivariate
Pareto distribution Y that satisfies the extremal pairwise Markov property on G, that is,
Y ⊥ Y |Y if (i,j)∈/ E.
i e j V\{i,j}
EngelkeandHitz(2020)showthatthisdefinitionisnaturalinthesensethatitenablesaHammersley–
Clifford theorem showing that densities factorize into lower-dimensional terms on the cliques of the
graph.
For a multivariate Gaussian distribution with covariance matrix Σ, the conditional dependence
relationships, or equivalently the edges in the Gaussian graphical model can be identified from the
nonzeros of the precision matrix Σ−1. A similar property holds for extremal graphical models if Y
follows a Hu¨sler–Reiss distribution, where the matrix Θ in Definition 1 plays a key role.
Proposition 1 (Lemma 1 and Proposition 3 of Engelke and Hitz (2020)). Let Y ∈ Rd follow a
Hu¨sler–Reiss distribution with precision matrix Θ. Then,
(5) Y ⊥ Y |Y ⇔ Θ =0.
i e j V\{i,j} ij
AconsequenceofProposition1isthatforaHu¨sler–Reissgraphicalmodelonanarbitraryconnected
graph G, we can read off the graph structure from the zero pattern of the precision matrix Θ.
Finally, we note that an important property of an extremal graphical model is that if Y possesses
a density that factorizes on the graph G, then G must necessarily be connected (Engelke and Hitz,
2020). The state-of-the-art structure learning methods for extremal data (Engelke et al., 2022c, Wan
and Zhou, 2023) can yield disconnected graphs that thus do not always yield a valid distribution (see
the example in Figure 1). For a detailed review of recent progress on extremal graphical models, we
refer to Engelke et al. (2024). In the next section, we present our approach for structure learning,
which can handle latent variables and always yields a valid distribution.
3. Latent Hu¨sler–Reiss models and the eglatent method
3.1. Latent Hu¨sler–Reiss models. In the illustrative example in the introduction, we presented
a Hu¨sler–Reiss model with a single latent variable and a very simple graphical structure among the
observed variables. We next introduce a latent Hu¨sler–Reiss model with a general extremal graphical
structure and any number of latent variables. In what follows, let X ∈ Rp be the collection of
O
observed variables, X ∈Rh be a collection of latent variables, and put d:=p+h.
H
Definition 3 (Latent Hu¨sler–Reiss models). Suppose that the random vector X = (X ,X ) ∈ Rd,
O H
indexed by V = (O,H), is in the domain of attraction of a Hu¨sler–Reiss distribution Y ∈ Rd in the
sense of (1) with variogram and precision matrices, and corresponding extremal graphical structure
(cid:18) (cid:19) (cid:18) (cid:19)
Γ Γ Θ Θ
Γ= O OH , Θ= O OH , and G =(V,E),
Γ Γ Θ Θ
HO H HO H
respectively. Here Θ = (Π(−Γ/2)Π)+, Γ and Θ are p×p-dimensional symmetric matrices, and
O O
E = {(i,j) : i,j ∈ V,i ̸= j,Θ ̸= 0}. We then say that Y is a latent Hu¨sler–Reiss model, and we
ij
note that the observed variables X are in the domain of attraction of a Hu¨sler–Reiss model with
O
variogram Γ .
OEXTREMAL GRAPHICAL MODELING WITH LATENT VARIABLES 7
Note that Θ and Θ in the above definition are positive definite since Γ ∈ Cd; see Definition 1
O H
and Engelke and Hitz (2020, Appendix B). Latent Hu¨sler–Reiss models have been studied only for
very simple graphs, namely tree structures (Asenova et al., 2021, R¨ottger et al., 2023b) and block
graphs (Asenova and Segers, 2023). All of the above methods assume the underlying graph structure
among the observed and latent variables and the number of latent variables to be known, which is
rarely realistic in practice. To handle more general graphs, we establish the following theorem, which
relates the marginal distribution of the observed variables to components of the precision matrix Θ.
Theorem 2. Let Π˜ = I −1 1T/p be the projection matrix onto the orthogonal complement of the
p p p
all-ones vector in p dimensions. Then, the precision matrix Θ˜ ∈ Rp×p of the observed variables of a
latent Hu¨sler–Reiss model with variogram matrix Γ satisfies
(6) Θ˜ =(Π˜(−Γ /2)Π˜)+ =Θ −Θ Θ−1Θ .
O O OH H HO
While it is not possible to observe the joint precision matrix Θ or any of its components directly,
Theorem3providesausefuldecompositionoftheobservableprecisionmatrixΘ˜ intothedifferenceof
twoterms, eachterminvolvingthecomponentsofΘ. Bythepropertyin(5), wehaveforanyi,j ∈O
that
Y ⊥ Y |Y ,Y ⇔ [Θ ] =0.
i e j H O\{i,j} O i,j
Thus, the first term Θ in decomposition (6) specifies the conditional independencies among the
O
observed variables after conditioning on the latent variables. Moreover, the sparsity pattern of Θ
O
encodes the residual graph G among the observed variables after extracting the influence of the
O
latent variables. Here, G = (O,E ) is a subgraph of G restricted to the observed variables where
O O
E ={(i,j)∈E,i,j ∈O}. ThesecondtermΘ Θ−1Θ indecomposition(6)servesasasummary
O OH H HO
of the marginalization of the latent variables Y and encodes their effect on the observed variables.
H
Therankofthismatrixisequaltothenumberoflatentvariables. TheoveralltermΘ −Θ Θ−1Θ
O OH H HO
is a Schur complement with respect to Θ .
H
As an illustration, consider the extremal graph on the left-hand side of Figure 1. Here, the ma-
trix Θ is diagonal. Furthermore, the matrix Θ Θ−1Θ has rank equal to one with all of its
O OH H HO
entries being nonzero. Note that Θ˜ generally consists of all nonzero entries and hence the marginal
graphicalstructureamongtheobservedvariablesontheright-handsideofFigure1isfullyconnected.
3.2. Sparsepluslow-rankdecomposition. Inthispaper,weconsideralatentHu¨sler–Reissgraph-
ical model where the subgraph G among the observed variables is sparse and the number of latent
O
variablesissmallrelativetothenumberofobservedvariables,thatis, h≪p. Thismodelingassump-
tion is often natural in real-world applications. For example, Chandrasekaran et al. (2012) and Taeb
andChandrasekaran(2016)showedthatalargefractionoftheconditionaldependenciesamongstock
returns can be explained by a small number of latent variables and interpreted these to be correlated
to exchange rate and government expenditures. In a similar spirit, Taeb et al. (2017) demonstrated
that the California reservoir network is sparsely connected after accounting for a few latent factors,
andinterpretedtheselatentfactorstobehighlycorrelatedtoenvironmentalvariablessuchasdrought
level and precipitation.
In the case of extremes, a sparse subgraph G and the presence of only a few latent variables
0
in the model translate to a latent Hu¨sler–Reiss model with matrix Θ being sparse, the matrix
O
Θ Θ−1Θ being low-rank, and thus the observed precision matrix Θ˜ being decomposed as a
OH H HO
sparse plus low-rank matrix having zero row sums. Notice that the matrix Θ˜ will generally be dense
duetotheadditionallow-ranktermΘ Θ−1Θ ,highlightinghowthelatentvariablesinducemany
OH H HO8 S.ENGELKEANDA.TAEB
confounding dependencies among the observed variables (see Figure 1), and how structure learning
procedures that impose sparsity on the precision matrix Θ˜ will generally not perform well.
Insummary,wecancasttheproblemoflearningalatentHu¨sler–Reissgraphicalmodelasobtaining
a sparse plus low-rank decomposition of the precision matrix Θ˜ of the observed variables. The sparse
componentprovidestheresidualgraphicalstructureoftheobservedvariablesafteraccountingforthe
latent variables, the rank of the low-rank component provides the number of latent variables, and
the overall sum provides a compact model of the observed variables that can be used for downstream
tasks. In the following section, we propose a convex optimization procedure to accurately estimate
each of these components from data.
Remark 1. In the setting where the observed and latent variables are jointly Gaussian, Chan-
drasekaran et al. (2012) also models the precision matrix among the observed variables as a sum
of a sparse and a low-rank matrix. Analogous to our setting, the sparse component encodes the sub-
graphoftheobservedvariablesandthelow-rankcomponentencodestheeffectofthelatentvariableson
the observed variables. An important distinguishing feature with our extremal setting however is that
in the Gaussian context, the resulting sum is not constrained to have zero row sum. As we describe in
Section 3.3, the additional subspace constraint in our extremal setting results in a different estimation
procedure and assumptions for statistical consistency.
3.3. Inference for latent Hu¨sler–Reiss graphical models. Let X = (X ,X ) be a collection
O H
of observed and latent variables in the domain of attraction of a latent Hu¨sler–Reiss graphical model
with a sparse subgraph among the observed variables and a small number of latent variables; we will
specify the sparsity level and the number of latent variables in our theoretical results. Let Γ⋆ be the
underlyingpopulationvariogrammatrixandΘ⋆ bethepopulationprecisionmatrixwithcomponents
Θ⋆,Θ⋆ and Θ⋆ . Let Θ˜⋆ be the precision matrix among the observed variables. From Theorem 3,
O OH H
we have that Θ˜⋆ = S⋆ −L⋆ where S⋆ := Θ⋆ is a sparse matrix and L⋆ := Θ⋆ Θ⋆ −1Θ⋆ is a
O OH H HO
low-rankmatrix. Here,thesupportofS⋆ encodesthesubgraphamongtheobservedvariablesandthe
rank of L⋆ encodes the number of latent variables. We will propose a convex optimization procedure
to estimate the matrices (S⋆,L⋆) from data.
3.3.1. Empiricalextremalvariogrammatrix. Animportantingredientofourprocedureisanempirical
estimate for the extremal variogram matrix Γ⋆. To arrive at our estimate, define for any m ∈ O,
O
the population extremal variogram matrix Γ⋆,(m) rooted at the node m; see Definition 2. Suppose
O
we have n independent and identically distributed samples {X(t)}n ⊆Rp of the observed variables
O t=1
X . Then, a natural estimate Γˆ(m) for Γ⋆,(m) is given by
O O O
(cid:16) (cid:17)
Γˆ( ijm) :=V(cid:100)ar log(1−Fˆ i(X i(t)))−log(1−Fˆ j(X j(t))):Fˆ m(X m(t))≥1−k/n , i,j ∈O.
Here, V(cid:100)ar denotes the sample variance, and k is the number of extreme samples considered in the
conditioning event, which can be viewed as the effective sample size. Since in Section 2 we assumed
that X has standard exponential margins, for i∈O, t∈{1,...,n}, inside the variance we normalize
thei-thentryofthet-thobservationempiricallyby−log(1−Fˆ(X(t))),whereFˆ denotestheempirical
i i i
distributionfunctionofX(1),...,X(n). As(4)establishesthattheempiricalvariogrammatrixrooted
i i
at node m coincides with the true variogram matrix Γ⋆ for every m, a natural empirical estimator of
O
this matrix is
p
(7) Γˆ := 1 (cid:88) Γˆ(m).
O p O
m=1EXTREMAL GRAPHICAL MODELING WITH LATENT VARIABLES 9
Under the assumption that k → ∞ and k/n → 0, and mild conditions on the underlying data
generation, this estimator can be shown to be consistent for Γ⋆ (Engelke and Volgushev, 2022).
O
Moreover, Engelke et al. (2022c) derive finite sample concentration bounds for Γˆ that can be used
O
for high-dimensional consistency results. We refer to Appendix B for details on the assumptions and
results.
3.3.2. Parameter estimation and structure learning. For structure learning in Hu¨sler–Reiss models,
formulating optimization problems in the precision domain leads to computationally efficient pro-
cedures. Indeed, the precision matrix estimate obtained from plugging in the empirical extremal
variogram Γˆ in place of Γ⋆ in the expression Θ˜⋆ =(Π˜(−Γ⋆/2)Π˜)+ is the minimizer of the following
O O O
convex optimization problem:
Θˆ =argmin −logdet(cid:0) UTΘU(cid:1) − 1 tr(ΘΓˆ ),
(8) Θ∈Sp 2 O
s.t. Θ⪰0 , Θ1 =0,
p
where the matrix U ∈Rp×(p−1) consists of the first p−1 left singular vectors of Π˜ so that UUT =Π˜;
seeAppendixDforaformalproof. Theconstraint⪰0imposespositivesemi-definiteness, Sp denotes
the space of symmetric p×p matrices, and the constraint Θ1 =0 ensures that Θ has zero row sum.
p
The above optimization problem corresponds to the surrogate maximum likelihood estimator of the
Hu¨sler–Reiss distribution; for more details on this justification we refer to R¨ottger et al. (2023b).
TheformulationintermsoftheprecisionmatrixΘopensthedoortovariousregularizedestimation
methods. R¨ottgeretal.(2023b)solveproblem(8)undertheadditionalconstraintthatΘ ≤0forall
ij
i,j ∈V to ensure a from of positive dependence. For a graph G =(V,E), in order to obtain a graph
structured estimate of Γ, Hentschel et al. (2022) solve a matrix completion problem that corresponds
to (8) under the constraint Θ =0 for (i,j)∈/ E. In the context of structure learning without latent
ij
variables, Engelke et al. (2022c) and Wan and Zhou (2023) consider adding an ℓ penalty to the loss
1
function akin to the graphical lasso.
In the setting with latent variables, we rely on the sparse plus low-rank decomposition described
in Section 3.1. We, therefore, search over the space of precision matrices Θ that can be decomposed
as Θ = S−L to identify a sparse matrix S and a low-rank matrix L, whose difference has zero row
sum and yields a small surrogate negative likelihood. Motivated by the estimator for Gaussian latent
variable graphical modeling (Chandrasekaran et al., 2012), we introduce the eglatent method that
solves the following regularized convex likelihood problem for some λ ,γ ≥0:
n
(Sˆ,Lˆ)= argmin −logdet(UT(S−L)U)−tr((S−L)Γˆ /2)+λ (∥S∥ +γtr(L)),
O n 1
(9) S∈Sp,L∈Sp
s.t. S−L⪰0,L⪰0,(S−L)1 =0.
p
Here, Sˆ and Lˆ are estimates for the population quantities S⋆ and L⋆, respectively. The matrix Sˆ−Lˆ
representsanestimatedprecisionmatrixamongtheobservedvariables. Bytheconstraintsin(9)and
the property of logdet functions, span(1 1⊤) is the null space of Sˆ−Lˆ and Sˆ−Lˆ always specifies a
p p
valid Hu¨sler–Reiss model.
The function ∥·∥ denotes the ℓ norm that promotes sparsity in the matrix S (Friedman et al.,
1 1
2007). The role of the trace penalty on L is to promote low-rank structure (Fazel et al., 2004).
The regularization parameter γ provides a trade-off between the graphical model component and
the latent component. In particular, for very large values of γ, eglatent produces Lˆ =0 so that no
latentvariablesareincludedinthemodel. Asγ decreases,thenumberoflatentvariablesincreasesand
correspondinglythenumberofedgesintheresidualgraphicalstructuredecreases. Theregularization
parameter λ provides overall control of the trade-off between the fidelity of the model to the data
n10 S.ENGELKEANDA.TAEB
and the complexity of the model, and thus naturally depends on the sample size. For λ ,γ ≥ 0,
n
eglatent is a convex program that can be solved efficiently.
Remark 2. A challenge with the optimization in (8), both theoretically and numerically, is the fact
that the matrices range in the space of positive semi-definite matrices with zero row sum. This factor
indeed seems to prohibit direct structure learning without latent variables (i.e., setting L=0 in (9) to
obtain a graphical lasso analog) where the estimated graphical structure can be rather different than
the true graphical structure; see the discussion in Engelke et al. (2022c, Section 7). To circumvent
this issue, Engelke et al. (2022c) and Wan and Zhou (2023) solve slightly different problems to obtain
accurate graph estimation, although their estimated graphs do not always yield valid Hu¨sler–Reiss
models. Remarkably, the addition of the low-rank component L in the eglatent estimator (9) solves
theseissues. Indeed,wewillshowthateglatentconsistentlyrecoversthesubgraphamongtheobserved
variables and the number of latent variables, and matches the performance of existing procedures
(Engelke et al., 2022c, Wan and Zhou, 2023) for learning an accurate model when no latent variables
are present.
4. Consistency guarantees for eglatent
RecallfromSection3.3thatwedenotebyS⋆thepopulationmatrixencodingthegraphicalstructure
among the observed variables conditioned on the latent variables, and by L⋆ the population matrix
encoding the effect of a few latent variables on the observed variables. Further, Θ˜⋆ = S⋆ − L⋆
represents the marginal precision matrix in the Hu¨sler–Reiss model over the observed variables. In
this section, we state a theorem to prove that the estimates of eglatent in (9) provide, with high
probability,thecorrectgraphicalstructureamongtheobservedvariables,thecorrectnumberoflatent
variables, and an accurate extremal model. Stated mathematically, we show with high probability
that (i) the sign-pattern of Sˆ is the same as that of S⋆, i.e., sign(Sˆ) = sign(S⋆), where sign(0) = 0;
(ii) the rank of Lˆ is the same as that of L⋆, i.e., rank(Lˆ)=rank(L⋆); and (iii) the estimated precision
model Sˆ−Lˆ closely approximates the true precision matrix Θ˜⋆, i.e., Sˆ−Lˆ ≈Θ˜⋆.
Our analysis requires assumptions on the population model so that the matrices S⋆ and L⋆ are
identifiable from their sum, and that the number of effective samples k is of order k ≳p2log(p).
4.1. Technicalsetup. Aseglatentissolvedintheprecisionmatrixparameterization,theconditions
for our theorems are naturally stated in terms of the precision matrix S⋆−L⋆. The assumptions are
similar in spirit to convex relaxation methods for Gaussian latent-variable graphical model selection
(Chandrasekaranetal.,2012),althoughsomeconditionsarenewduetothezerorowandcolumnsum
structure of the observed precision matrix S⋆−L⋆.
Toensurecorrectgraphrecoveryandcorrectnumberoflatentvariables,weseekanestimate(Sˆ,Lˆ)
from eglatent such that support(Sˆ) = support(S⋆) and rank(Lˆ) = rank(L⋆). Building on both
classical statistical estimation theory, as well as the recent literature on high-dimensional statistical
inference,anaturalsetofconditionsforaccurateparameterestimation,istoassumethatthecurvature
of S⋆−L⋆ isboundedin certaindirections. Thecurvatureisgovernedby themodifiedHessianof the
surrogate log-likelihood loss at S⋆−L⋆:
(cid:18)
1
(cid:19)−1 (cid:18)
1
(cid:19)−1
I⋆ := S⋆−L⋆+ 1 1⊤ ⊗ S⋆−L⋆+ 1 1⊤ ,
p p p p p p
where ⊗ denotes a Kronecker product between matrices, and I⋆ may be viewed as a map from Sp to
Sp. The matrix I⋆ modifies the Hessian of the surrogate log-likelihood loss (S⋆−L⋆)+⊗(S⋆−L⋆)+,
where the addition of the term 11 1⊤ (a dual parameter of the program (9)) helps to compactify the
p p p
assumptions we place in our population model.EXTREMAL GRAPHICAL MODELING WITH LATENT VARIABLES 11
WeimposeconditionssothatI⋆ iswell-behavedwhenappliedtomatricesoftheformS−S⋆−(L−
L⋆)andS−S⋆−(L−L⋆+t1 1⊤). Here,S isintheneighborhoodofS⋆ restrictedtosparsematrices,
p p
LisintheneighborhoodofL⋆ restrictedtolow-rankmatrices,andt1 1⊤ isadualparameterforsome
p p
t∈R due to the constraint (S−L)1 =0 that appears in the analysis of (9). These local properties
p
ofI⋆ aroundS⋆−L⋆ areconvenientlystatedintermsoftangentspacestoalgebraicvarietiesofsparse
and low-rank matrices. In particular, the tangent space of a matrix M with r non-zero entries with
respect to the algebraic variety of p×p matrices with at most r non-zeros is given by
Ω(M):={N ∈Rp×p :support(N)⊆support(M)}.
Moreover, the tangent space at a rank-r matrix M with respect to the algebraic variety of p×p
matrices with rank less than or equal to r is given by:
T(M):={N +N :N ,N ∈Rp×p,
R C R C
row-space(N )⊆row-space(M),col-space(N )⊆col-space(M)}.
R C
Formorediscussiononthetangentspacesofsparseandlow-rankmatrices, seeChandrasekaranetal.
(2012). In the next section, we describe conditions on the population Hessian I⋆ in terms of tangent
spaces Ω(S⋆) and T(L⋆). Under these conditions, we present a theorem in Section 4.3 showing that
the convex program provides accurate estimates.
For notational simplicity, we let Ω⋆ :=Ω(S⋆) and T⋆ :=T(L⋆).
4.2. Identifiability of (S⋆,L⋆) and hessian conditions. Appealing to the previous literature on
sparse-plus-low rank decompositions, the matrices S⋆ and L⋆ are identifiable from their sum if the
row and columns of the matrix S⋆ are sufficiently sparse and the matrix L⋆ is sufficiently low-rank
withmostofitsentriesnon-zeroandsimilarinmagnitude(Cand`esetal.,2011,Chandrasekaranetal.,
2011, Recht et al., 2010). The structural constraint on L⋆ can be interpreted as the number of latent
variables being small (as compared to the ambient dimension p) with their effects spread across all
the observed variables. We measure the sparsity of S⋆ by the maximal number of non-zeros in any
row/column (this amounts to the degree of the subgraph induced among the observed variables):
(cid:88)
d⋆ :=max I[S⋆ ̸=0].
ij
i
j
Thus, we require d⋆ to be small so that no observed variable is directly connected to “many” other
observed variables. To measure the ”diffuseness” of the latent effects, we consider the following
quantityforanylinearsubspaceZ ⊆Rp(Cand`esetal.,2011,Cand`esandRecht,2012,Chandrasekaran
et al., 2012, 2011):
µ[Z]:=max∥P (e )∥ ,
Z i 2
i
where P is the projection onto the subspace Z and e is a standard coordinate basis. The quantity
Z i
µ[Z] is also known as the “incoherence parameter” (Cand`es and Recht, 2012, Chandrasekaran et al.,
2011). It measures how aligned the subspace Z is with respect to standard basis elements and is
(cid:112)
lower-bounded by dim(Z)/p and upper-bounded by one. In our setting, the relevant subspace is
the row or column space of L⋆ and so we define:
µ⋆ :=µ[col-space(L⋆)].
(cid:112)
Thus, a lower bound for µ⋆ is h/p. A small value of µ⋆ ensures the matrix L⋆ has a small rank and
its singular vectors are reasonably spread out – in other words, not sparse.
Finally, as described earlier, the zero row sum constraint in our estimator introduces a dual pa-
rameter t1 1⊤ for some t ∈ R. Our conditions will rely on how far span(1 1⊤) deviates from T⋆,
p p p p12 S.ENGELKEANDA.TAEB
which is summarized in the parameter
κ⋆ :=∥P (1 1⊤/p)∥ ∈[0,1].
T⋆⊥ p p 2
The smaller the quantity κ⋆, the smaller the deviation of the subspace span(1 1⊤) from T⋆. As a
p p
result,wedonotwantκ⋆tobetoosmallsothatL⋆andthedualparametert1 1⊤canbedistinguished
p p
from one another. We also do not want κ⋆ to be too large so that the size of t can be controlled.
In addition to the quantities (µ⋆,d⋆,κ⋆), we must control the behavior of the Hessian I⋆ restricted
to elements in the direct sums Ω⋆⊕T⋆ as well as Ω⋆⊕(T⋆⊕span(1 1⊤)). A more interpretable set
p p
of assumptions is to measure the behavior of I⋆ restricted to the spaces Ω⋆ and T⋆ separately and
use the bound on the quantities (µ⋆,d⋆,κ⋆) to “couple” them to control the behavior of I⋆ restricted
to the combined spaces. We next present the decoupled conditions and describe in Appendix F how
they combine.
Behavior of I⋆ with respect to Ω⋆. We consider the following functions of I⋆ with respect to Ω⋆:
α := min ∥P I⋆P (N)∥ ,
Ω Ω⋆ Ω⋆ ∞
N∈Ω⋆,∥N∥∞=1
δ := max ∥P I⋆P (N)∥ ,
Ω⊥ Ω⋆⊥ Ω⋆ ∞
N∈Ω⋆,∥N∥∞=1
β := max ∥I⋆(N)∥ .
Ω 2
N∈Ω⋆,∥N∥2=1
Here, α quantifies the minimum gain of I⋆ restricted to subspace Ω⋆ and with respect to the ℓ
Ω ∞
norm (the minimum gain of a matrix M restricted to subspace S and with respect to norm ∥·∥ is
min ∥P MP (x)∥);thequantityδ computestheinner-productbetweenelementsinΩ⋆ and
x∈S,∥x∥=1 S S Ω
Ω⋆⊥ as quantified by the metric induced by I⋆; and finally, β quantifies the behavior of I⋆ restricted
Ω
to Ω⋆ in spectral norm.
Behavior of I⋆ with respect to T⋆. Similar to Ω⋆, we control the behavior of I⋆ associated with the
subspaceT⋆. Asdiscussedearlier,acomplicationthatariseswithtangentspacestolow-rankvarieties
is that they are locally smooth. To account for this curvature, we bound distances of nearby tangent
spaces via the following induced norm:
ρ(T ,T ):= max ∥(P −P )(N)∥ .
1 2
∥N∥2≤1
T1 T2 2
The quantity ρ(T ,T ) measures the sine of the largest angle between T and T . So we control the
1 2 1 2
behavior of I⋆ for tangent spaces T′ close to the tangent space T⋆. In particular, we consider the
following functions:
α := min ∥P I⋆P (N)∥ ,
T T′ T′ 2
N∈T′,ρ(T′,T⋆)≤µ⋆/4,∥N∥2=1
 
 
δ :=max max ∥P I⋆P (N)∥ , max ∥P I⋆P (N)∥ ,
T⊥ T′⊥ T′ 2 T′ T′⊥ 2
N∈T′,ρ(T′,T⋆)≤µ⋆/4, N∈T′⊥,ρ(T′,T⋆)≤µ⋆/4, 
∥N∥2≤1 ∥N∥2≤1
β := max ∥I⋆(N)∥ .
T ∞
N∈T′,ρ(T′,T⋆)≤µ⋆/4,∥N∥∞=1
Here, α quantifies the minimum gain of I⋆ restricted to tangent spaces T′ that are close to T⋆ with
T
respecttothespectralnorm; thequantifyδ computestheinner-productbetweenelementsinT′ and
T
T′⊥ as quantified by the metric induced by I⋆; and finally, β quantifies the behavior of I⋆ restricted
T
to T′ in infinity norm.
With these above quantities defined, the main assumption is the following.EXTREMAL GRAPHICAL MODELING WITH LATENT VARIABLES 13
Assumption 1. Define α = min{α ,α }, δ = max{δ ,δ }, and β = max{β ,β }. Then, there
Ω T Ω⊥ T⊥ Ω T
(cid:16) (cid:17)
exists ν ∈(0,min{1/2,2κ⋆+µ⋆/2}) and ν ∈ 8κ⋆+4µ⋆ ,1/2 such that δ/α≤1−2ν where
1 2 min{1,α}+2κ⋆+µ⋆/2
ν =min{ν ,ν }.
1 2
Assumption 1 can be viewed as the generalization of the irrepresentability condition imposed on
the covariance matrix or the Hessian. In particular, Assumption 1 is akin to the irrepresentability
conditionimposedinMeinshausenandBuhlmann(2006),Ravikumaretal.(2008),Wainwright(2009),
ZhaoandYu(2006),whichgivessufficientconditionsforconsistentrecoveryofgraphicalmodelsusing
ℓ -regularizedmaximumlikelihoodestimator. Similarly,Assumption1isakintotheirrepresentability
1
condition imposed in Cand`es and Recht (2012), Chandrasekaran et al. (2012), which gives sufficient
conditions for low-rank matrix recovery using nuclear norm regularization.
4.3. Theorem statement. Wenowdescribetheperformanceof eglatentundersuitableconditions
on the quantities described in the previous section. We state the theorem based on essential aspects
of the conditions required for the success of our convex relaxation (i.e., the Hessian conditions) and
omit complicated constants. We specify these constants in Appendix G. Our results depend on a
second-order parameter ξ >0 that determines the rate of convergence of the random vector X to its
limiting Hu¨sler–Reiss distribution, with larger values being better (see Appendix B).
Theorem 3. Let α,β,δ,d⋆,µ⋆,κ⋆,ν ,ν be defined as above with κ⋆2 ≥ µ⋆/2 and 2κ⋆ +µ⋆/2 < 1.
1 2
Let ν =min{ν ,ν } and suppose Assumption 1 is satisfied. Suppose that:
1 2
ν2α2
d⋆µ⋆ ≤ .
(1−2κ⋆−µ⋆)18β2(2−ν)2
(cid:104) (cid:105)
Lettheregularizationγ bechosenintheintervalγ ∈
2βd⋆(2−ν),να(1−2κ⋆−µ⋆)
. Letm=max{1,1/γ}
να 36βµ⋆(2−ν)
and m¯ =max{1,γ}. Let the effective sample size k be chosen such that k =nξ/2(1+ξ). Furthermore,
suppose that:
(1) k ≳ m3hd⋆2m2ν2p2log(p)
α6ν2
(cid:113)
(2) λ ∼ m p2log(p)
n ν k
(cid:113)
(3) σ (L⋆)≳ m4m¯h p2log(p)
min να4 k
√ (cid:113)
(4) |S⋆|≳ m3m¯ h p2log(p) for every (i,j) with |S⋆|>0
ij να2 k ij
Then, the estimate (Sˆ,Lˆ) defined as the unique minimizer of eglatent in (8) satisfies
(cid:32) √ (cid:114) (cid:33)
m3 h p2log(p) 1
P sign(Sˆ)=sign(S⋆),rank(Lˆ)=rank(L⋆),∥(Sˆ−Lˆ)−Θ˜⋆∥ ≲ ≥1− .
2 να2 k p
We prove Theorem 3 in Appendix G. The theorem essentially states that if the minimum nonzero
singular value of the low-rank term L⋆ and the minimum nonzero entry of the sparse piece S⋆ are
bounded away from zero, then eglatent provides accurate estimates for the subgraph among the
observed variables, the number of latent variables, and a marginal extremal model.
The quantities (α,β,δ,d⋆,µ⋆,κ⋆,ν) as well as the choices of the parameters λ and γ play a
n
prominent role in the result. Larger values of α,ν and smaller values of µ⋆,d⋆,β lead to a better
conditioned Hessian I⋆ around the tangent spaces Ω⋆ and T⋆. The better conditioning of the Hessian
I⋆ then results in less stringent requirements on the range of values for γ, sample complexity, the
minimum nonzero singular value of L⋆, and the magnitude of the minimum nonzero entry of S⋆.
Notice that the number of latent variables h appears explicitly in the bounds in Theorem 3. We also14 S.ENGELKEANDA.TAEB
notethedependenceonhisimplicitinthedependenceonµ⋆,α,β andν. Indeed,aslargerhincreases
the dimension of the tangent space T⋆, it results in smaller α,ν and larger values of β and µ⋆.
Remark 3. The sample size requirement in Theorem 3 is driven by how fast the empirical variogram
matrix Γˆ converges in spectral norm to the true variogram matrix Γ⋆. Engelke et al. (2022c) carried
O O
out extensive mathematical arguments to analyze the concentration of the empirical variogram matrix
in ℓ norm. In our analysis, we use this result and the equivalence of norms to obtain a convergence
∞
rate in spectral norm. It is of interest to develop tighter convergence results, and we leave this as a
topic for future research; see Section 6 for more discussion.
5. Experimental demonstrations
Inournumericalexperiments,weuseeglatentasamodelselectionprocedureandperformasecond
refitting step on the selected model structure to estimate the model parameters; see Appendix H for
details.
5.1. Synthetic simulations. We illustrate the utility of our method for recovering the subgraph
among the observed variables and the number of latent variables on synthetic data. We compare
the performance of our eglatent method to eglearn by Engelke et al. (2022c) for learning extremal
graphical models. To evaluate the accuracy of the estimated graphs with edges Eˆ relative to the true
subgraph among the observed variables with edges E =E , we use the F-score
O
|E∩Eˆ|
(10) F = .
|E∩Eˆ|+ 1(|Ec∩Eˆ|+|E∩Eˆc|)
2
Larger F-scores thus indicate more accurate graph recovery.
5.1.1. Structure recovery. In order to evaluate the performance of our new method, we generate data
fromarandomvectorX =(X ,X )inthedomainofattractionofalatentHu¨sler–Reissmultivariate
O H
ParetodistributionY withtheprecisionmatrixΘ⋆ ∈Rp+h×p+h,pobservedvariablesO ={1,2,...,p}
and h latent variables H ={p+1,...,p+h}. We choose to simulate X from the Hu¨sler–Reiss max-
stable distribution with the same precision matrix Θ⋆, which is well-known to be in the domain of
attractionofY;seeResnick(2008)fordetails. Thesimulationcanbedoneefficientlywiththemethod
in Dombry et al. (2016).
We specify the sub-graph G =(E ,O) among the observed variables to be a cycle graph and set
0 O
Θ⋆ to −2 for every (i,j) ∈ E and zero otherwise. The latent variables are not connected in the
ij O
joint graph so that Θ⋆ = 0 for every i,j ∈ H,i ̸= j. We connect each latent variable node i ∈ H to
ij
every k ∈ O satisfying k = i−(p+1)+ζh for some positive integer ζ (thus every latent variable is
connected to a distinct set of observed variables in the graph). The corresponding entries Θ⋆ in the
√ √ ik
precision matrix are chosen uniformly at random from the interval [30/ p+h,60/ p+h]. Finally,
we set the diagonal entries of Θ⋆ so that it has the all-ones vector is in its null space. Appendix I
shows results for a setting where the subgraph among the observed variables is generated according
to an Erd˝os–R´enyi graph.
We let p = 30, h ∈ {1,2,3}, and we set the number of marginal exceedances to k = ⌊n0.7⌋. We
then generate n samples from the Hu¨sler–Reiss distribution parameterized by Θ⋆ so that we obtain
k ∈ {100,1000,5000} effective extreme samples. When deploying our eglatent estimator in (9), we
fix γ = 4 to a reasonable default value. Concerning the regularization parameter λ , which also
n
appears in the eglearn method, in both methods, it is chosen either by validation likelihood on a
separate dataset of size n or by an oracle approach maximizing the F-score for the sub-graph among
observed variables.EXTREMAL GRAPHICAL MODELING WITH LATENT VARIABLES 15
h=1 h=2 h=3
1.00
0.75
0.50
0.25
0.00
100 1000 5000 100 1000 5000 100 1000 5000
k
Method: eglatent_cv eglatent_oracle eglearn_cv eglearn_oracle
h=1 h=2 h=3
6
4
2
0
100 1000 5000 100 1000 5000 100 1000 5000
k
Method: rk_cv rk_oracle
h=1 h=2 h=3
600
400
200
0
-200
100 1000 5000 100 1000 5000 100 1000 5000
k
Figure 3. F-score (top row) and estimated number of latent variables (middle row) of eglatent
method with the selection of the tuning parameter based on the oracle and validation on the F-
score for the cycle graph with h = 1,2,3 latent variables and different effective sample sizes k =
100,1000,5000. The bottom row shows the difference between best eglatent and best eglearn
log-likelihoodsonthevalidationset.
erocs-F
knaR
ecnereffid
doohilekil-goL16 S.ENGELKEANDA.TAEB
Figure 3 summarizes the performance of the methods on 50 independent trials for the different
sample sizes and different numbers of latent variables. We observe that our proposed approach
outperforms eglearn in several ways. Indeed, the top row shows that the graph learned by eglearn
only poorly recovers the graphical structure among observed variables. This reveals a limitation of
thismethod,namelythatinthepresenceoflatentvariables,themarginalgraphofobservedvariables
is dense and sparsity cannot be well detected by methods that ignore this fact. Clearly, this problem
becomes more pronounced with a larger number of latent variables. On the other hand, our new
eglatent method exploits the latent structure for learning the sparse graph among the observed
variables conditional on the latent variables. It recovers the graphical structure among the observed
variablesincreasinglywellwithagrowingsamplesize. Infact,theresultsforthetuningparameterλ
n
chosenthroughvalidationlikelihoodarealmostasgoodasthosebasedontheoracle. Themiddlerow
of Figure 3 shows that eglatent is able to identify the correct number of latent variables, especially
for larger sample sizes.
We can also compare the model in terms of their likelihood on the validation data. Again, our
eglatent method generally attains a better validation likelihood and is thus more representative of
thedata. Asanexception,weobservethatiftheeffectivesamplesizeissmall(k =100),theneglearn
performsbetter. Thereasonisthateglatentisamoreflexiblemodelwithmoreparameterstolearn,
and it therefore benefits more from additional data.
5.1.2. Robustness to zero latent variables. Wenowevaluatetheperformanceof eglatentwhenthere
are no latent variables present and compare its performance to eglearn. We first specify a graph
structure using a Barab´asi–Albert model denoted by BA(d,q), which is a preferential attachment
model with d notes and a degree parameter q (Albert and Barab´asi, 2001). We set d=20 and q =2.
WethendefineaHu¨sler–ReissprecisionmatrixΘ⋆ ∈Rd×d withentriessampleduniformlyatrandom
from the interval [−5,−2]. The diagonal entries of Θ⋆ are chosen so that it has the all-ones vector in
its null space. We generate n samples from the max-stable Hu¨sler–Reiss distribution parameterized
by Θ⋆ such that there are k =⌊n0.7⌋=2000 effective marginal extreme samples. We also generate a
separatedatasetofsizenforvalidation. Forthemethodeglatent,foreachvalueoftheregularization
parameter γ = 1,4,8,20 the regularization parameter λ is chosen based on the validation set. The
n
regularization parameter λ in eglearn is chosen similarly.
n
Figure 4 presents the F-scores and validation log-likelihood scores of eglatent as γ varies and
for 50 independent trials. We also display the average numbers of edges and latent variables, as well
as the performance of eglearn. As expected, larger values of γ lead to smaller estimates for the
number of latent variables. We observe that when γ = 4, eglatent obtains an accurate graphical
structure(F-scoreclosetoone)withasimilarvalidationlikelihoodaseglearn. Here, eglearnyields
asparsegraphsince,unliketheprevioussettings,therearenounobservedconfounding. Interestingly,
the average number of estimated latent variables in this case is not close to zero. In particular, we
observe that when γ is chosen so that eglatent yields nearly zero latent variables (i.e., Lˆ ≈ 0), the
F-scores scores obtained by eglatent drop significantly. For such γ, our estimator (9) resembles the
analog of the graphical lasso which is known to yield inaccurate models (Engelke et al., 2022c); see
also Remark 2.
In summary, when the sample size is sufficiently large, eglatent yields a similar model fit and
graph recovery as eglearn even when there are no latent variables. It is worth emphasizing that
eglatent achieves this favorable performance by estimating some latent variables. This shows the
robustness of our method to model misspecification.
5.2. Real data application. WeapplyourlatentHu¨sler–Reissmodeltoanalyzelargeflightdelays.
WeuseadatasetfromtheR-packagegraphicalExtremes(Engelkeetal.,2022a)withp=29airports
in the southern U.S. shown in the left panel of Figure 5. Large flight delays cause huge financialEXTREMAL GRAPHICAL MODELING WITH LATENT VARIABLES 17
11.6 3 1 0 54.34 42.48 42.18 109.7 39.22
1.0 -47000
-48000
0.8
-49000
-50000
0.6
-51000
0.4 -52000
γ=1 γ=4 γ=8 γ=20 eglearn γ=1 γ=4 γ=8 γ=20 eglearn
Figure 4. Left: F-score of eglatent for different regularization parameters γ ∈ {1,4,8,20} and
eglearn; top axis shows the average number of estimated latent variables in eglatent. Right: the
log-likelihoodofthesamemethodsevaluatedonavalidationdataset;thetopaxisshowstheaverage
numberofestimatededgesineachmodel.
40°N 40°N 40°N
35°N 35°N 35°N
30°N 30°N 30°N
25°N 25°N 25°N
110°W 105°W 100°W 95°W 90°W 110°W 105°W 100°W 95°W 90°W 110°W 105°W 100°W 95°W 90°W
Figure 5. Airports in the Southern U.S. (dots) and flight connections, where the thickness of the
nodesindicatestheaveragenumberofdailyflightsattheairports. Left: flightconnectiongraphwith
anedgebetweenanypairofairportswithdailyflights. Center: estimatedgraphofoptimaleglearn
model. Right: estimatedsub-graphcorrespondingtoobservedvariablesofoptimaleglatentmodel.
losses and lead to congestion of critical airport infrastructure. Our method provides an improved
model for the dependence of such excessive delays at different airports, and can eventually be used
for stress testing of the system; see Hentschel et al. (2022) for details on this application. Unless
otherwise noted, we fit the models on the whole dataset consisting of n = 3603 observations from
2005-01-01to2020-12-31. WecompareoureglatentmethodforlatentHu¨sler–Reissmodelswiththe
eglearn algorithm by (Engelke and Volgushev, 2022) that estimates a graphical structure without
latent variables. Throughout this application, we choose the exceedance threshold to be 0.85 (i.e.,
1−k/n = 0.85) resulting in k = 540 marginal exceedances for the computation of the empirical
variogram Γˆ ; see Section 3.3.1. The latter is the input for the different structure learning methods.
O
Theleft-handsideofFigure6showsthenumberofedgesof eglatentandof eglearnasafunction
ofthetuningparameterλ ,wheretheparameterγ relatedtothelatentvariableselectionineglatent
n
is fixed to γ =2; different values of γ give similar results and are omitted here. We see that for both
methods, larger values of λ result in sparser graphs. It is important to note that for eglearn, we
n
erocs-F
doohilekil-gol
noitadilaV18 S.ENGELKEANDA.TAEB
29 14 12 11 11 10 10 9 8 8 7
400
−8375
300
−8400
200
−8425
100
−8450
0
0 0.010.020.030.040.050.060.070.080.09 0.1 0 0.010.020.030.040.050.060.070.080.09 0.1
Regularization parameter l n Regularization parameter l n
Figure6. Left: numberofedgesoftheestimatedgraphof eglearn(dashedline)andtheestimated
sub-graphofobservedvariablesof eglatent(solidline)asfunctionsoftheregularizationparameter
ρ;topaxisshowsthenumberoflatentvariablesineglatent. Right: correspondinglog-likelihoods;
horizontallineisthevalidationlog-likelihoodofthefullyconnectedgraph.
count the edges of the usual estimated graph. For our eglatent method we count the edges of the
residual graph among the observed variables. The latent graphs have generally fewer edges and are
therefore more easily interpretable.
To compare the different model fits and to select the optimal value for the tuning parameter λ ,
n
we must compute the likelihood of the fitted models on an independent validation set. To this end,
we split the data chronologically into five equally large folds and perform cross-validation by leaving
one fold out (validation data) and fitting on the remaining four folds (training data). The results
for model performance on the validation sets are then averaged. The right-hand side of Figure 6
shows the averaged log-likelihood values on the validation sets that were not used for model fitting.
For both methods, we see that for too small values of λ , the graphs are too dense and overfit to
n
the training data. In fact, for λ = 0, both models correspond to the fully connected graph whose
n
performance (horizontal line) is much worse than the models enforcing sparsity. For too large values
of λ , the graph becomes too sparse and the model is not flexible enough. Clearly, the latent model
n
outperforms eglearn, indicating that latent variables are present in this data set. In this particular
application, they can be thought of as confounding factors such as meteorological variables or strikes
in the aviation industry that affect many airports simultaneously.
Figure 5 compares the estimated graphs of eglatent (center) and eglearn (right) fitted on the
wholedataset,wheretheregularizationparameterλ inbothmethodsischosenasthemaximizersof
n
the respective validation likelihoods. We observe that the latent graph is much sparser and therefore
highlights more clearly certain features of the system. For instance, it seems that hubs, such as the
Fort Worth International Airport in Dallas (the thickest point on the map), are more central in the
graph since they have more connections than smaller airports.
6. Future work
Our work on latent variables in the analysis of extremal dependence opens several future research
directions. First,asdescribedinSection4.3,oursamplesizerequirementisdrivenbyaspectralnorm
concentration result on the empirical variogram matrix. This result was derived by translating the
ℓ concentration result of Engelke et al. (2022c) to the spectral norm setting using equivalence of
∞
segde
fo
rebmuN
doohilekil−goLEXTREMAL GRAPHICAL MODELING WITH LATENT VARIABLES 19
norms. To obtain tighter convergence results, one must obtain direct concentration bounds on the
spectral norm; such a result would be of independent interest in the multivariate extremes literature.
Second, solving eglatent can be challenging for large problems. Building on the work of Ma et al.
(2012) in the Gaussian setting, faster solvers can be developed using alternating direction method of
multipliers (Boyd et al., 2011). Moreover, we observed in Section 5.1.2 that eglatent estimates a
few latent variables to accurately recover the underlying graphical structure when there are no latent
variables present. It would be of interest to develop a theoretical justification for this phenomenon.
Finally, additional structure on the dependency structures among the observed and latent variables,
such as multivariate total positivity of order 2 (R¨ottger et al., 2023b) or colored graphs R¨ottger et al.
(2023a), may be exploited to develop more powerful extremal graphical models with latent variables.
References
Albert, R., Barab´asi, A.L., 2001. Statistical mechanics of complex networks. ArXiv cond-
mat/0106096. URL: https://journals.aps.org/rmp/abstract/10.1103/RevModPhys.74.47.
Asadi, P., Davison, A.C., Engelke, S., 2015. Extremesonrivernetworks. AnnalsofAppliedStatistics
9, 2023–2050. URL: https://www.jstor.org/stable/43826454.
Asenova,S.,Mazo,G.,Segers,J.,2021. Inferenceonextremaldependenceinthedomainofattraction
ofastructuredHu¨sler–ReissdistributionmotivatedbyaMarkovtreewithlatentvariables.Extremes
24, 461–500. arXiv:2001.09510.
Asenova, S., Segers, J., 2023. Extremes of Markov random fields on block graphs: max-stable limits
andstructuredHu¨sler–Reissdistributions. Extremes26,433–468. URL:https://link.springer.
com/article/10.1007/s10687-023-00467-9.
Boyd, S.P., Parikh, N., Chu, E., Peleato, B., Eckstein, J., 2011. Distributed optimization and statis-
tical learning via the alternating direction method of multipliers. Foundational Trends of Machine
Learning 3, 1–122. URL: https://dl.acm.org/doi/10.1561/2200000016.
Cand`es, E., Li, X., Ma, Y., Wright, J., 2011. Robust principal component analysis? Journal of the
ACM 58, 1–37. URL: https://dl.acm.org/doi/10.1145/1970392.1970395.
Cand`es, E., Recht, B., 2012. Exact matrix completion via convex optimization. Foundations of
ComputationalMathematics55,111–119. URL:https://link.springer.com/article/10.1007/
s10208-009-9045-5.
Chandrasekaran, V., Parillo, P., Willsky, A., 2012. Latent variable graphical model selection via
convex optimization. Annals of Statistics 40, 1935–1967. URL: https://www.jstor.org/stable/
41806519.
Chandrasekaran,V.,Sanghavi,V.,Parrilo,P.,Willsky,A.,2011.Rank-sparsityincoherenceformatrix
decomposition. SIAMJournalofOptimization21,572–596. URL:https://epubs.siam.org/doi/
10.1137/090761793.
Dombry, C., Engelke, S., Oesting, M., 2016. Exact simulation of max-stable processes. Biometrika
103, 303–317. arXiv:1506.04430.
Engelke, S., Hentschel, M., Lalancette, M., R¨ottger, F., 2024. Graphical models for multivariate
extremes. arXiv:2402.02187.
Engelke,S.,Hitz,A.S.,2020. Graphicalmodelsforextremes(withdiscussion). J.R.Stat.Soc.Ser.B
Stat. Methodol 82, 871–932. URL: https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/
rssb.12355.
Engelke, S., Hitz, A.S., Gnecco, N., Hentschel, M., 2022a. graphicalExtremes: Statistical Method-
ology for Graphical Extreme Value Models. URL: https://github.com/sebastian-engelke/
graphicalExtremes.20 S.ENGELKEANDA.TAEB
Engelke, S., Ivanovs, J., 2021. Sparse structures for multivariate extremes. Annu.
Rev. Stat. Appl. 8, 241–270. URL: https://www.annualreviews.org/doi/abs/10.1146/
annurev-statistics-040620-041554.
Engelke, S., Ivanovs, J., Strokorb, K., 2022b. Graphical models for infinite measures with applica-
tionstoextremesandL´evyprocesses. URL:https://arxiv.org/abs/2211.15769,doi:10.48550/
ARXIV.2211.15769.
Engelke, S., Lalancette, M., Volgushev, S., 2022c. Learning extremal graphical structures in high
dimensions. URL: https://arxiv.org/abs/2111.00840.
Engelke, S., Opitz, T., Wadsworth, J., 2019. Extremal dependence of random scale con-
structions. Extremes 22, 623–666. URL: https://link.springer.com/article/10.1007/
s10687-019-00353-3.
Engelke, S., Volgushev, S., 2022. Structure learning for extremal tree models. Journal of the Royal
Statistical Society Series B: Statistical Methodology 84, 2055–2087. URL: https://doi.org/10.
1111/rssb.12556, doi:10.1111/rssb.12556.
Fazel, M., Hindi, H.A., Boyd, S.P., 2004. Rank minimization and applications in system the-
ory. Proceedings of the 2004 American Control Conference 4, 3273–3278 vol.4. URL: https:
//ieeexplore.ieee.org/document/1384521.
Friedman,J.,Hastie,T.,Tibshirani,R.,2007. Sparseinversecovarianceestimationwiththegraphical
lasso. Biostatistics 9, 432–441. URL: https://doi.org/10.1093/biostatistics/kxm045.
de Haan, L., Resnick, S., 1977. Limit theory for multivariate sample extremes. Z. Wahrschein-
lichkeitstheorie Verw. Gebiete 40, 317–337. URL: https://link.springer.com/article/10.
1007/BF00533086.
Hentschel, M., Engelke, S., Segers, J., 2022. Statistical inference for Hu¨sler–Reiss graphical models
through matrix completions. URL: https://arxiv.org/abs/2210.14292.
Hu, S., Peng, Z., Segers, J., 2022. Modelling multivariate extreme value distributions via markov
trees. URL: https://arxiv.org/abs/2208.02627, arXiv:2208.02627.
Hu¨sler,J.,Reiss,R.D.,1989. Maximaofnormalrandomvectors: Betweenindependenceandcomplete
dependence. Statist. Prob. Letters 7, 283–286. URL: https://ideas.repec.org/a/eee/stapro/
v7y1989i4p283-286.html.
Lauritzen, S.L., 1996. Graphical models. volume 17 of Oxford statistical science series. Clarendon
Press, Oxford. URL: https://www.tib.eu/de/suchen/id/TIBKAT%3A197598226.
Lederer, J., Oesting, M., 2023. Extremes in high dimensions: Methods and scalable algorithms.
arXiv:2303.04258. available from https://arxiv.org/abs/2303.04258.
Ma, S., Xue, L., Zou, H., 2012. Alternating direction methods for latent variable gaussian graphi-
cal model selection. Neural Computation 25, 2172–2198. URL: https://direct.mit.edu/neco/
article/25/8/2172/7900/Alternating-Direction-Methods-for-Latent-Variable.
Meinshausen, N., Buhlmann, P., 2006. High-dimensional graphs and variable se-
lection with the lasso. Annals of Statistics 34, 1436–1462. URL: https:
//projecteuclid.org/journals/annals-of-statistics/volume-34/issue-3/
High-dimensional-graphs-and-variable-selection-with-the-Lasso/10.1214/
009053606000000281.full.
Papastathopoulos,I.,Strokorb,K.,2016.Conditionalindependenceamongmax-stablelaws.Statistics
&ProbabilityLetters108,9–15.URL:https://www.sciencedirect.com/science/article/pii/
S0167715215002874.
Ravikumar, P., Wainwright, M.J., Raskutti, G., Yu, B., 2008. High-dimensional covariance estima-
tion by minimizing ℓ -penalized log-determinant divergence. Electronic Journal of Statistics 5,
1
935–980. URL: https://projecteuclid.org/journals/electronic-journal-of-statistics/
volume-5/issue-none/High-dimensional-covariance-estimation-by-minimizing-%E2%84%EXTREMAL GRAPHICAL MODELING WITH LATENT VARIABLES 21
931-penalized-log-determinant/10.1214/11-EJS631.full.
Recht,B.,Fazel,M.,Parrilo,P.,2010. Guaranteedminimum-ranksolutionsoflinearmatrixequations
via nuclear norm minimization. SIAM Review 52, 471–501. URL: https://epubs.siam.org/doi/
10.1137/070697835.
Resnick, S.I., 2008. Extreme Values, Regular Variation and Point Processes. Springer, New York.
Rootz´en, H., Segers, J., Wadsworth, J.L., 2018. Multivariate generalized Pareto distributions:
Parametrizations, representations, and properties. J. Multivariate Anal. 165, 117–131. URL:
https://www.sciencedirect.com/science/article/pii/S0047259X17303147.
Rootz´en, H., Tajvidi, N., 2006. Multivariate generalized Pareto distributions. Bernoulli
12, 917–930. URL: https://projecteuclid.org/journals/bernoulli/volume-12/issue-5/
Multivariate-generalized-Pareto-distributions/10.3150/bj/1161614952.full.
R¨ottger, F., Coons, J.I., Grosdos, A., 2023a. Parametric and nonparametric symmetries in graphical
models for extremes. arXiv:2306.00703. available from https://arxiv.org/ags/2306.00703.
R¨ottger, F., Engelke, S., Zwiernik, P., 2023b. Total positivity in multivariate extremes. The Annals
of Statistics 51, 962 – 1004. URL: https://doi.org/10.1214/23-AOS2272.
Segers, J., 2020. One- versus multi-component regular variation and extremes of Markov trees. Adv.
in Appl. Probab. 52, 855–878. arXiv:1902.02226.
Taeb,A.,Chandrasekaran,V.,2016.Interpretinglatentvariablesinfactormodelsviaconvexoptimiza-
tion. MathematicalProgramming167,129–154. URL:https://link.springer.com/article/10.
1007/s10107-017-1187-7.
Taeb, A., Reager, J.T., Turmon, M.J., Chandrasekaran, V., 2017. A statistical graphical model
of the California reservoir system. Water Resources Research 53, 9721 – 9739. URL: https:
//agupubs.onlinelibrary.wiley.com/doi/10.1002/2017WR020412.
Wainwright, M.J., 2009. Sharp thresholds for high-dimensional and noisy sparsity recovery using ℓ -
1
constrained quadratic programming (lasso). IEEE Transactions on Information Theory 55, 2183–
2202. URL: https://ieeexplore.ieee.org/document/4839045.
Wan, P., Zhou, C., 2023. Graphical lasso for extremes. arXiv:2307.15004.
Zhao,P.,Yu,B.,2006. Onmodelselectionconsistencyoflasso. JournalofMachineLearningResearch
7, 2541–2563. URL: https://www.jmlr.org/papers/volume7/zhao06a/zhao06a.pdf.
Zhou, C., 2009. Dependence structure of risk factors and diversification effects. Risk and Insurance
/ Measures and Control 2 URL: https://api.semanticscholar.org/CorpusID:15682170.
Zscheischler,J.,Seneviratne.,S.I.,2017. Dependenceofdriversaffectsrisksassociatedwithcompound
events. Science Advances 3. URL: https://www.science.org/doi/10.1126/sciadv.1700263.22 S.ENGELKEANDA.TAEB
supplementary material
Appendix A. notations and definitions
We let H⋆ = Ω⋆ ×T⋆ and H′ = Ω⋆ ×T′ where T′ is close to T⋆. We denote Q′ = Ω⋆ ×(T′ ⊕
span(1 1⊤)). For ω ∈(0,1), we define in the following set:
p p
U(ω):={Ω(S⋆)×T′ :ρ(T(L⋆),T′)≤ω}.
We denote the dual norm of the regularizer ∥S∥ +γ∥L∥ (here, we have replaced tr(L) with ∥L∥
ℓ1 ⋆ ⋆
since L is a positive definite matrix) in (9) by:
(cid:26) (cid:27)
∥L∥
Φ (S,L):=max ∥S∥ , 2 .
γ ∞ γ
Further, we define the linear operators J :Sp×Sp →Sp and J+ :Sp →Sp×Sp as:
J(C,D)=C−D ; J+(C)=(C,C).
Finally, for any subspace H, the projection onto the subspace is denoted by P .
H
Our analysis will depend on the following quantities for any pair of subspaces Ω,T ⊆Rp×p:
θ(Ω):= max ∥N∥ ; ξ(T):= max ∥N∥ .
2 ∞
N∈Ω,∥N∥∞=1 N∈T,∥N∥2=1
When Ω=Ω⋆ and T =T⋆, these quantities are closely connected to the maximal degree d⋆ and the
incoherenceparameterµ⋆ (definedinSection4.2). Inparticular,Chandrasekaranetal.(2012)showed
that µ(Ω⋆)∈[0,d⋆] and ξ(T⋆)∈[µ⋆,2µ⋆].
Appendix B. Finite sample convergence guarantees of the empirical variogram
matrix
Inadditiontoassumptionsforidentifiability,followingEngelkeetal.(2022c),weimposeconditions
to characterize the convergence rate of the empirical variogram matrix to the population variogram
matrix. Throughout,wesupposethattherandomvectorX =(X ,X )isinthedomainofattraction
O H
of the multivariate Pareto distribution Y following a latent Hu¨sler–Reiss distribution with parameter
matrix Γ; for details see Section 2.1 and 3.1.
Assumption 2. The marginal distribution functions F of X , i∈O, are continuous and there exists
i i
constants ξ >0, K <∞ such that for all triples of distinct indices J =(i,j,m)⊂O and q ∈(0,1],
sup (cid:12) (cid:12) (cid:12) (cid:12)q−1P(F J(X J)>1−qx)− P P(Y (J Y> >1 1/ )x)(cid:12) (cid:12) (cid:12) (cid:12)≤Kqξ,
x∈[0,q−1]2×[0,1] 1
where F (x)=(F (x ),F (x ),F (x )).
J i i j j m m
Assumption 2 is a second-order condition that essentially controls the speed of convergence of the
sample variogram matrix to the population variogram matrix.
Corollary 4. (Engelke et al., 2022c, Theorem 3) Let Assumption 2 hold. Let ℓ∈(0,1] be arbitrary.
Suppose that nℓ ≤ k ≤ n/2 where k is the effective sample size in computing the sample variogram
√
matrix (see Section 3.3.1). Let ϑ ≥ 0 be any scalar satisfying ϑ ≤ k/(logn)4. Then, there exists
positive constants c ,C ,C˜ only depending on K,ξ,ℓ,ϵ, and G(z) such that:
5 5 5
(cid:32) (cid:40)(cid:18) k(cid:19)ξ 1+ϑ(cid:41)(cid:33)
P ∥Γˆ −Γ⋆∥ >C (log(n/k))2+ √ ≤C˜ p3e−c5ϑ2 .
O O ∞ 5 n k 5EXTREMAL GRAPHICAL MODELING WITH LATENT VARIABLES 23
Appendix C. Useful lemmas
Our theoretical analysis relies on some lemmas.
Lemma 5. Let A ∈ Sd and B ∈ Sd be two symmetric matrices with A+B being nonsingular and
row/column spaces of A and B being orthogonal to one another. Then, (A+B)−1 =A++B+.
Proof of Lemma 5. LetU D UT andU D UT bethereducedSVDofAandB. Then,sinceA+B
A A A B B B
is non-singular, and the subspaces spanned by the columns of U and U are orthogonal, we have
A B
that (U ,U ) forms an orthogonal matrix. Therefore,
A B
(cid:18) (cid:19)
(A+B)=(cid:0) U U (cid:1) D A 0 (cid:0) U U (cid:1)T ,
A B 0 D A B
B
and thus:
(A+B)−1 =U D−1UT +U D−1UT =A++B+.
A A A B B B
□
Lemma 6. Suppose that UUTMUUT =M. Then, U(UTMU)−1UT =M+.
Proof of Lemma 6. Let UDUT be the reduced-SVD of M. Then, U(UMU)−1UT =UD−1UT, which
is equivalent to M+. □
(cid:18) (cid:19)
Θ Θ
Lemma 7. Let Π˜ = (I −1 1⊤/p). Let Θ = O OH ∈ Rd×d with Θ ∈ Rp×p, Θ ∈ Rh×h
p p p Θ Θ O H
HO H
and d = h+p. Suppose Θ is a positive semi-definite matrix with its null-space being the span of the
all-ones vector. Then:
Π˜(Θ −Θ Θ −1Θ )Π˜ =Θ −Θ Θ −1Θ .
O OH H HO O OH H HO
Proof. Since Θ1 =0, we have
d
(11) Θ 1 +Θ 1 =0
O p OH h
(12) Θ 1 +Θ 1 =0
HO p H h
Consider Θ −Θ Θ −1Θ , we have
O OH H HO
(Θ −Θ Θ −1Θ )1 =Θ 1 −Θ Θ −1Θ 1
O OH H HO p O p OH H HO p
by =(12) Θ 1 +Θ Θ −1(Θ 1 )
O p OH H H h
=Θ 1 +Θ 1
O p OH h
by(11)
= 0
Thus, 1 ∈ker(Θ −Θ Θ −1Θ ).
p O OH H HO
Tocompletetheproof,wewillshowthatdim(ker(Θ −Θ Θ −1Θ ))=1. Supposethereexist
O OH H HO
non-zero vector v ∈ ker(Θ −Θ Θ −1Θ ), and let u = Θ −1Θ v. Since v ̸= 0, u ̸= 0, and
O OH H HO H HO
then it follows that
Θ v−Θ u=0
O OH
Θ v−Θ u=0
HO H
yielding
(cid:18) (cid:19)
v
Θ⋆ =0.
−u24 S.ENGELKEANDA.TAEB
(cid:18) (cid:19)
v
Since ∈ker(Θ),v =α1 forsomeα∈R,whichimpliesthatdim(ker(Θ −Θ Θ −1Θ ))=
−u p O OH H HO
1. □
Lemma 8. Let Π˜ =I −1 1⊤/p and Π=I −1 1T/d with d=p+h. For any matrix M ∈Rd×d,
p p p d d d
Π˜(ΠMΠ) Π˜ =Π˜M Π˜
1:p,1:p 1:p,1:p
Proof. Note that
(cid:18) (cid:19)
(ΠMΠ) =(cid:0) I 0(cid:1) ΠMΠ I p
1:p,1:p p 0
Then it follows that
(cid:18) (cid:19)
Π˜(ΠMΠ) Π˜ =Π˜(cid:0) I 0(cid:1) ΠMΠ I p Π˜
1:p,1:p p 0
(13)
=(cid:0) Π˜ 0(cid:1)
ΠMΠ(cid:18) Π˜(cid:19)
.
0
Notice that:
(cid:0) Π˜ 0(cid:1) Π=(cid:0) (I −1 1⊤/p)(I −1 1⊤/d) (I −1 1⊤/p)1 /d(cid:1)
p p p p p p p p p p
=(cid:0) (I −1 1⊤/p)(I −1 1⊤/p+1 1⊤/(p)−1 1⊤/d) 0(cid:1)
p p p p p p p p p p
(14)
=(cid:0) Π˜(Π˜ +1 1⊤/(p)−1 1⊤/d) 0(cid:1)
p p p p
=(cid:0) Π˜ 0(cid:1) .
Putting (13) and (14) together, we have the desired result.
□
Lemma 9 (Lemma 3.1 of Chandrasekaran et al. (2012)). For any tangent spaces T ,T of same
1 2
dimension with ρ(T ,T )<1, we have that: ξ(T )≤ ξ(T1)+ρ(T1,T2).
1 2 2 1−ρ(T1,T2)
Lemma 10. Consider a tangent space T′ of a symmetric matrix with ρ(T⋆,T′)≤ω with ω <1. Let
C′ and C⋆ be the column spaces that form the tangent spaces T′ and T⋆ respectively. Then, we have
that: ∥P −P ∥ ≤ω.
C′ C⋆ 2
Proof of Lemma 10. Since ω <1, T⋆ and T′ are of the same dimension. Let σ (·) be the s-th largest
s
singular value of the input matrix. Notice that
∥P −P ∥ =∥P −P ∥
C′ C⋆ 2 C′⊥ C⋆⊥ 2
(cid:113)
= 1−σ (P P )2
p−k C′⊥ C⋆⊥
(cid:113)
= 1−σ (P P )
(p−k)2 T′⊥ T⋆⊥
(cid:113)
≤ 1−σ (P P )2
(p−k)2 T′⊥ T⋆⊥
=∥P −P ∥
T′⊥ T⋆⊥ 2
=∥P −P ∥ .
T′ T⋆ 2
□EXTREMAL GRAPHICAL MODELING WITH LATENT VARIABLES 25
Lemma 11. Let C ,C ⊆Rp be a pair of subspaces. Then, for any z ∈Rp:
1 2
(cid:26) (cid:27)
max ⟨z,v⟩≤2min max ⟨z,v⟩, max ⟨z,v⟩
v∈C1⊕C2,∥v∥2=1 v∈C1,∥v∥2=1 v∈C2,∥v∥2=1
(cid:26) (cid:27)
+max max ⟨z,v⟩, max ⟨z,v⟩ .
v∈C1,∥v∥2=1 v∈C2,∥v∥2=1
Proof of Lemma 11. Supposewithoutlossofgeneralitythatmax uTz ≤max uTz.
u1∈C1,∥u1∥2=1 1 u2∈C2,∥u2∥2=1 2
max ⟨z,v⟩= max |vTz|/∥v∥
2
v∈C1⊕C2,∥v∥2=1 u1∈C1,u2∈C2,∥u1∥2=∥u2∥2=1
v=c1u1+c2u2
= max |vTz|/∥v∥
2
u1∈C1,u2∈C2,∥u1∥2=∥u2∥2=1
u3=u2−(uT 2u1)u1
v=c1u1+c2u3
|c | |c |
≤ max 1 |uTz|+ 2 |uTz|
u1∈C1,u2∈C2,∥u1∥2=∥u2∥2=1(cid:112) c2 1+c2
2
1 (cid:112) c2 1+c2
2
3
u3=u2−(uT 2u1)u1
v=c1u1+c2u3
≤ max 2|uTz|+ max |uTz|
1 2
u1∈C1,∥u1∥2=1 u2∈C2,∥u2∥2=1
□
Lemma 12. Let Z ∈ T′ ⊕span(1 1⊤) with ρ(T′,T⋆) ≤ ω and ∥Z∥ = 1. Then, 1+2(κ⋆ +ω) ≥
p p 2
∥P (Z)∥ ≥1−2(κ⋆+ω) and thus ∥P (Z)∥ ≤2(κ⋆+ω).
T′ 2 T′⊥ 2
Proof of Lemma 10. Note that ∥Z∥ +∥P (Z)∥ ≥ ∥P (Z)∥ ≥ ∥Z∥ −∥P (Z)∥ . Let T′ be
2 T′⊥ 2 T′ 2 2 T′⊥ 2
a tangent space with associated row and column spaces C′ and R′. Let C˜ = C′ ⊕ span(1 ) and
p
R˜ =R′⊕span(1 ). SinceZ ∈T′⊕span(1 ),itisstraightforwardtoshowthatZ =P ZP +ZP .
p p C˜ R˜⊥ R˜
Therefore, wehavethatP (Z)=P [P ZP +ZP ]P . Thus, ∥P (Z)∥ ≤∥P P ∥ +
T′⊥ C′⊥ C˜ R˜⊥ R˜ R′⊥ T′⊥ 2 C′⊥ C˜ 2
∥P P ∥ . Letting C =C′ and C =span(1 ), we appeal to Lemma 11 to conclude that:
R˜ R′⊥ 2 1 2 p
max ∥P (v)∥ ≤ max max 2|⟨z,u ⟩|+ max max |⟨z,u ⟩|
C′⊥ 2 1 2
v∈C˜,∥v∥2=1 z∈C′⊥ u1∈C′,∥u1∥2=1 z∈C′⊥ u2∈span(1),∥u2∥2=1
∥z∥2=1
√
∥z∥2=1
=∥P (1 / p)∥ .
C′⊥ p 2
Again, appealing to Lemma 11,
max ∥P (z)∥ ≤ max max 2|⟨z,u ⟩|+ max max |⟨z,u ⟩|
C˜ 2 1 2
v∈C′⊥,∥v∥2=1 z∈C′⊥ u1∈C′,∥u1∥2=1 z∈C′⊥ u2∈span(1),∥u2∥2=1
∥z∥2=1
√
∥z∥2=1
=∥P (1 / p)∥ .
C′⊥ p 2
√
Sowehaveconcludedthat∥P P ∥ ≤∥P (1/ p)∥ . Thus,appealingtoLemma10,weconclude
C′⊥ C˜ 2 C′⊥ 2 √
that: ∥P P ∥ ≤κ⋆+ω. Similarly, we can conclude that: ∥P P ∥ ≤∥P (1/ p)∥ and thus
C′⊥ C˜ 2 R′⊥ R˜ 2 R′⊥ 2
∥P P ∥ ≤κ⋆+ω. Putting things together, we have the desired bound.
R′⊥ R˜ 2
□
Lemma13. LetT′ ⊆Rp×p beatangentspacetoalow-rankvariety. Then,∥P (L∥ ≤
(T′⊕span(1p1⊤ p))⊥ 2
∥P (L)∥ for any matrix L∈Rp×p
T′⊥ 226 S.ENGELKEANDA.TAEB
Proof of Lemma 13. Let R′,C′ be row/column space pair that form the tangent space T′. Let C˜ =
span(C′,1) and R˜ = span(R′,1). Then, it is straightforward to see that T′ ⊕span(1 1⊤) is itself
p p
a tangent space formed by column space C˜ and row space R˜. Thus, ∥P (L⋆)∥ =
(T′⊕span(1p1⊤ p))⊥ 2
∥P L⋆P ∥ . Since C′ ⊆C˜, we have that: ∥P LP ∥ ≤∥P LP ∥ =∥P (L)∥ . □
C˜⊥ R˜⊥ 2 C˜⊥ R˜⊥ 2 C′⊥ R′⊥ 2 T′⊥ 2
Lemma 14. Suppose that κ⋆ > ω. Then, span(1 1⊤)∩(T′⊕T⋆) = {0} for every tangent space T′
p p
with ρ(T′,T⋆)≤ω.
Proof of Lemma 14. It suffices to show that ∥P (1 1⊤/p)∥ >0. Let C′ be the column space
(T′⊕T⋆)⊥ p p 2
associated with the tangent space T′ at a symmetric matrix. Note that T′⊕T⋆ is another tangent
√
space with column space C′ ⊕C⋆. Then, ∥P (1 1⊤/p)∥ = ∥P (1/ p)∥2. So it suf-
√ (T′⊕T⋆)⊥ p p 2 √ (C′⊕C⋆)⊥ 2 √
fices to show that ∥P (1/ p)∥ < 1. Note that: ∥P (1/ p)∥ ≤ ∥P P (1/ p)∥ +
√ C′⊕C⋆ √2 C′⊕C⋆ 2 C′⊕C⋆√C⋆ 2
∥P P (1/ p)∥ ≤ ∥P (1/ p)∥ +∥P P ∥ . We have that: ∥P (1/ p)∥ = 1−κ⋆.
C′⊕C⋆ C⋆⊥ 2 C⋆ 2 C′⊕C⋆ C⋆⊥ 2 C⋆ 2
UsingLemma11,itisstraightforwardtoconcludethat∥P P ∥ ≤∥P P ∥ ≤∥P −P ∥ .
C′⊕C⋆ C⋆⊥ 2 C′ C⋆⊥ 2 C′√ C⋆ 2
Appealing to Lemma 10 , and putting everything together, we conclude that: ∥P (1/ p)∥ ≤
C′⊕C⋆ 2
(1−κ⋆)+ω. As κ⋆ >ω, we have the desired result. □
√
Lemma 15. Let Z =T′⊕span(1 1⊤) with ∥Z∥ =1 and ρ(T′,T⋆)≤ω. Then, assuming κ⋆ > ω,
p p 2
Z can be decomposed uniquely as follows Z = Z + Z where Z ∈ T′, Z ∈ span(1 1⊤) with
1 2 1 2 p p
√
max{∥Z ∥ ,∥Z ∥ }≤ √ 2 5h .
1 2 2 2
1− 1−(κ⋆2−ω)2
Proof of Lemma 15. The unique decomposition follows from Lemma 14. Since ω < 1, we have that
T′ andT⋆ havethesamedimension. SinceZ ∈T′⊕T⋆,thenrank(Z )≤4h(thisfollowsfromnoting
1 1
thateverymatrixinsideT′orT⋆hasrankatmost2handrankofasumofmatricesislessthanthesum
√
of the ranks). Further, rank(Z ) ≤ 1, so that rank(Z) ≤ 5h. Therefore, ∥Z∥ ≤ 5h. Notice that:
2 F
∥Z∥2 =∥Z +P (Z )+P (Z )∥2 =∥Z +P (Z )∥2 +∥P (Z )∥2. Thus, ∥Z +P (Z )∥ ≤
√ F 1 T′ 2 T′⊥ 2 F 1 T′ 2 F T′⊥ √2 F 1 T′ 2 F
5h. Using reverse triangle inequality, we conclude that ∥Z ∥ ≤ 5h+∥P (Z )∥ . Now notice
1 F T′ 2 F
(cid:112)
that: ∥Z ∥2 = ∥P (Z )∥2 +∥P (Z )∥2, so that: ∥Z ∥2 −∥P (Z )∥2 = ∥P (Z )∥ . Since
2 F T′⊥ 2 F T′ 2 F 2 F T′⊥ 2 F T′ 2 F
(cid:113)
Z is rank-1, we have then that: ∥P (Z )∥ = ∥Z ∥ 1−∥P (1 1⊤/p)∥2. Combining things,
2 T′ 2 F 2 2 T′⊥ p p 2
√ (cid:113)
we conclude that ∥Z ∥ ≤ 5h+∥Z ∥ 1−∥P (1 1⊤/p)∥2. Notice that ∥P (1 1⊤/p)∥ ≥
1 F 2 2 T′⊥ p p 2 T′⊥ p p 2
√
∥P (1 1⊤/p)∥ −ω = κ⋆2 −ω. Reverse triangle inequality also gives ∥Z ∥ ≤ ∥Z ∥ + 5h.
T⋆⊥ p p 2 2 F 1 F
√
Putting the last bounds together, we have that: ∥Z ∥ ≤ √ 2 5h . Plugging this into a
2 F
1− 1−(κ⋆2−ω)2
√
previous bound, we also find that ∥Z ∥ ≤ √ 2 5h .
1 F
1− 1−(κ⋆2−ω)2
□
Lemma 16. Let T′ ⊆Rp×p be a tangent space. We have that: max ∥N∥ ≤
N∈T′⊕span(1p1⊤ p),∥N∥2=1 ∞
3ξ(T′).
Proof of Lemma 16. Let (R′,C′) be the row/column space pair associated with T′. Let C˜ = C′ ⊕
span(1 ) and R˜ = R′ ⊕ span(1 ). Since Z ∈ T′ ⊕ span(1), it is straightforward to show that
p p
Z = P ZP +ZP . Therefore, ∥Z∥ ≤ max ∥P (e )∥ +max ∥P (e )∥ . Letting C = C′ and
C˜ R˜⊥ R˜ ∞ i C˜ i 2 i R˜ i 2 1
C =span(1 ), and appealing to Lemma 11, we have that:
2 p
√
max∥P (e )∥ ≤2max max 2|uTe |+max max |uTe |≤2/ p+µ[C′].
C˜ i 2 1 i 2 i
i i u1∈span(1),∥u1∥2=1 i u2∈C′,∥u2∥2=1
Analogously, letting C =R′ and C =span(1), and appealing to Lemma 11, we have that:
1 2
√
max∥P (e )∥ ≤2/ p+µ[R′].
R˜ i 2
iEXTREMAL GRAPHICAL MODELING WITH LATENT VARIABLES 27
Since ξ(T′)≥max{[µ[C′],µ[R′]} and 2ξ(T′)≥ √2 , we conclude the desired result. □
p
Appendix D. Arriving at estimator (9)
Recall that Θ˜⋆ = (Π˜(−Γ⋆/2)Π˜)+, where Π˜ = UUT. Furthermore, the null-space of Θ˜⋆ is the
O
subspace span(1 1⊤). In other words, UUTΘ˜⋆UUT =Θ˜⋆. We arrive at our estimator by noting that
p p
Θ˜⋆ is the unique minimizer of the convex program:
Θˆ =argmin −logdet(cid:0) UTΘU(cid:1) − 1 tr(ΘΓ⋆),
(15) Θ∈Sp 2 O
s.t Θ⪰0 , Θ1 =0.
p
Toseewhythatis,firstnotethattheconstraintΘ⪰0canberemovedsincethelog-detfunctionforces
UTΘU to be positive definite and together with the constraint Θ1 forces Θ ⪰ 0 and additionally
p
UUTΘUUT =Θ. Notethattr(ΘΓ⋆)=tr(UUTΘUUTΓ⋆)=tr(ΘUUTΓ⋆UUT). Thus,anequivalent
O O O
optimization to (15) is
Θˆ =argmin −logdet(cid:0) UTΘU(cid:1) − 1 tr(ΘUUTΓ⋆UUT),
(16) Θ∈Sp 2 O
s.t Θ∈span(1 1⊤)⊥.
p p
Using Lagrangian duality theory, we have that Θˆ must satisfy for some t∈R
1
−U(UTΘˆU−1)UT − UUTΓ⋆UUT +t1 1⊤ =0.
2 O p p
Note that t = 0 since the first two terms live in the space spanned by the columns of U and the
last term lies in the orthogonal subspace. Similarly, −U(UTΘˆU−1)UT − 1UUTΓ⋆UUT = 0. Since
2 O
UUTΘˆUUT = Θˆ, we appeal to Lemma 6 to conclude that Θˆ+ = −11UUTΓ⋆UUT. Some simple
22 O
manipulations allow us to conclude that Θˆ =Θ˜⋆.
Appendix E. Proof of Theorem 2
Proof of Theorem 2. For notational simplicity, we let M = −Γ⋆/2. Let Π = I −1 1T/d. We have
d d d
from Hentschel et al. (2022) that (ΠMΠ)+ = Θ⋆ or equivalently ΠMΠ = (Θ⋆)+. Since Θ⋆ has zero
row/column sums and thus its row/column spaces are orthogonal to the all-ones vector, we have by
Lemma5thatforanyt>0,(Θ⋆+t1 1T)−1 =Θ⋆++(t1 1T)+ =Θ⋆++ 1 (1 1T). AsΠ1 1TΠ=0,
d d d d td2 d d d d
we have that:
ΠMΠ=Π(Θ⋆+t1 1T)−1Π.
d d
The equation above implies
Π˜[ΠMΠ] Π˜ =Π˜[Π(Θ⋆+t1 1T)−1Π] Π˜.
1:p,1:p d d 1:p,1:p
Using Lemma 8, we have that:
(17) Π˜M Π˜ =Π˜[(Θ⋆+t1 1T)−1] Π˜.
1:p,1:p d d 1:p,1:p
We will now analyze the term [(Θ⋆+t1 1T)−1] inside (17). From Schur’s complement, we have
d d 1:p,1:p
that:
[(Θ⋆+t1 1T)−1] =
d d 1:p,1:p
(18)
(cid:2) Θ⋆ +t1 1T −(Θ⋆ +t1 1T)(Θ⋆ +t1 1T)−1(Θ⋆ +t1 1T)(cid:3)−1 .
O p p OH p h H h h HO h p28 S.ENGELKEANDA.TAEB
By the Woodbury inversion lemma, we have that:
(cid:18)
1
(cid:19)−1
(19) (Θ⋆ +t1 1T)−1 =(Θ⋆ )−1−(Θ⋆ )−11 +1T(Θ⋆ )−11 1T(Θ⋆ )−1.
H h h H H h t h H h h H
Plugging the result of (19) into (18), we have that:
[(Θ⋆+t1 1T)−1]
d d 1:p,1:p
=Θ⋆ +t1 1T −(Θ⋆ +t1 1T)(Θ⋆ +t1 1T)−1(Θ⋆ +t1 1T)
O p p OH p h H h h HO h p
=A+B+C
where
A=Θ⋆ −Θ⋆ (Θ⋆ )−1Θ⋆ ,
O OH H HO
B =t1 1T +t21 1T(Θ⋆ +t1 1T)−11 1T,
p p p h H h h h p
C =t1 1T(Θ⋆ +t1 1T)−1Θ⋆ +tΘ⋆ (Θ⋆ +t1 1T)−11 1T
p h H h h HO OH H h h h p
(cid:18)
1
(cid:19)−1
+Θ⋆ (Θ⋆ )−11 +1T(Θ⋆ )−11 1T(Θ⋆ )−1Θ⋆ .
OH H h t h H h h H HO
FromLemma7,wehavethat: Π˜AΠ˜ =A. Furthermore,noticethatB liesintheall-onessubspace,
i.e. Π˜BΠ˜ = 0 and is a positive semi-definite matrix for t > 0. Thus, the matrix A+B is invertible.
Notice
limt1 1T(Θ⋆ +t1 1T)−1Θ⋆ = limt1 1T(Θ⋆ )−1Θ⋆ =0,
p h H h h HO p h H HO
t→0 t→0
limtΘ⋆ (Θ⋆ +t1 1T)−11 1T = limtΘ⋆ (Θ⋆ )−11 1T =0,
OH H h h h p OH H h p
t→0 t→0
(cid:18)
1
(cid:19)−1
limΘ⋆ (Θ⋆ )−11 +1T(Θ⋆ )−11 1T(Θ⋆ )−1Θ⋆ = limtΘ⋆ (Θ⋆ )−11 (Θ⋆ )−1Θ⋆ =0,
t→0 OH H h t h H h h H HO t→0 OH H h H HO
so that lim C =0. Notice on the other hand that lim A+B ̸=0. By the Woodbury inversion
t→0 t→0
lemma, we have that: (A+B +C)−1 = (A+B)−1 −(A+B)−1C(I +(A+B)−1C)−1(A+B)−1.
Thus:
limΠ˜(A+B+C)−1Π˜ =Π˜ lim(A+B)−1Π˜ −limΠ˜(A+B)−1C(I+A−1C)−1A−1Π˜.
t→0 t→0 t→0
Since lim C =0, we have that:
t→0
lim Π˜[(Θ⋆+t1 1T)−1] Π˜ = limΠ˜(A+B+C)−1Π˜ = limΠ˜(A+B)−1Π˜ =Π˜A+Π˜ =A+.
d d 1:p,1:p
t→∞ t→0 t→0
Here, the second equality follows from noting that the row/column spaces of A and B are orthogonal
to one another and so by Lemma 5, (A+B)−1 = A++B+. Furthermore, since B is a multiple of
all-ones matrix, Γ˜B+Γ˜ =0. The last equality follows from Lemma 7. Noting that M =−Γ⋆/2
1:p,1:p O
and plugging in A+ for Π˜[(Θ⋆+t1 1T)−1] Π˜ in (17), we conclude that:
d d 1:p,1:p
(Π˜(−Γ⋆/2)Π˜)+ =Θ⋆ −Θ⋆ (Θ⋆ )−1Θ⋆ .
O O OH H HO
Taking pseudo-inverses of both sides, we have the desired result. In Lemma 7, we also showed that:
Θ⋆ −Θ⋆ (Θ⋆ )−1Θ⋆ =Π˜(Θ⋆ −Θ⋆ (Θ⋆ )−1Θ⋆ )Π˜,
O OH H HO O OH H HO
as desired.
□EXTREMAL GRAPHICAL MODELING WITH LATENT VARIABLES 29
Appendix F. Coupled hessian conditions implied by assumption 1 and the choice γ
Consider the following quantities measuring the behavior of the Hessian for some ω ∈(0,1)
χ= H′m ∈Uin (ω)Z∈H′,m ∥Zin ∥Φγ=1∥PHJ+I⋆JPH(Z)∥ Φγ,
(cid:40) (cid:41)
ε= H′m ∈Uax (ω)max Z∈Hm ,∥Zax ∥Π=1∥P H⊥J+I⋆JPH(Z)∥ Φγ, Z∈H⊥m ,∥a Zx ∥Φγ=1∥PHJ+I⋆JP H⊥(Z)∥
Φγ
,
χ
+
= H′m ∈Uin (ω)Z∈Q′m ,Φi γn (Z)=1∥PH′J+I⋆JPQ′(Z)∥ Φγ,
ε
+
= H′m ∈Uax (ω)Z∈H,m ∥Za ∥x Φγ=1∥P H⊥J+I⋆JPQ′(Z)∥ Φγ.
ThenotationsforthequantitiesU(ω),Φ ,H′,Q′ aredefinedinSectionA.Thequantityχbeinglarge
γ
ensures that the Hessian I⋆ is well-conditioned restricted to the image JH′ ⊆ Sp for all H′ close to
H⋆; the quantity ε being small implies that any element of JH′ and any element of JH′⊥ have a
small inner-product for all H′ close to H⋆. Finally, χ and ε are analogs of χ and ε, accounting for
+ +
the dual parameter that lies in the subspace span(1 1⊤).
p p
Thequantitiesχ,ε,χ ,ε representcoupledHessianquantities. Wenextshowthattheuncoupled
+ +
conditions assumed in Theorem 3 imply control on these quantities.
Proposition 17. Suppose the conditions of Theorem 3 are satisfied. Then, the following properties
are satisfied:
P1 χ≥α,
P2 ε ≤1− 2
ξ ζ1
P3 ε+ ≤1− 2
ξ+ ζ2
where ω =µ⋆/4, α=α, ζ =2/ν and ζ = 2 .
1 1 2 1− 11 −+ 22κ κ⋆ ⋆+ −µ µ⋆ ⋆/ /2 2(1−ν2)
Proof of Proposition 17. Throughout, we will use the notation θ(Ω⋆) and ξ(T⋆) defined in Section A,
as well as the bounds θ(Ω⋆) ≤ d⋆ and ξ(T⋆) ∈ [µ⋆,2µ⋆]. Recall that γ is in the range defined in
Theorem 3.
We begin by proving that Property P1 holds under the conditions of Proposition 17. Let Z =
(Z ,Z ). Suppose that ∥Z ∥ =1. Then, we have using Lemma 9 with ω =µ⋆/4 to conclude that:
1 2 1 ∞
∥P Ω⋆I⋆JPH(Z)∥
∞
≥∥P Ω⋆I⋆P Ω⋆(Z 1)∥ ∞−∥P Ω⋆I⋆(Z 2)∥
∞
≥α−∥I⋆(Z )∥ ≥α−3γβξ(T⋆)≥α/2.
2 ∞
Suppose that ∥Z ∥ =γ. Then, we have that:
2 2
∥P T′I⋆JPH(Z)∥
2
≥∥P T′I⋆P T′(Z 1)∥ 2−∥P T′I⋆(Z 1)∥
2
≥α−2∥I⋆(Z )∥ ≥αγ−2βθ(Ω⋆)≥αγ/2.
1 2
Putting together the last two inequalities proves that Property P1 holds. We next prove that Prop-
erty P2 holds. Note that for Z =(Z ,Z )∈H′ with ∥Z ∥ =1, we have
1 2 1 ∞
∥P Ω⋆⊥I⋆JPH(Z)∥
∞
≤∥P Ω⋆⊥I⋆(Z 1)∥ ∞+∥P Ω⋆⊥I⋆(Z 2)∥
∞
≤δ+3ξ(T⋆)βγ.
Similarly, suppose that ∥Z ∥ =γ. We have that:
2 2
∥P T′⊥I⋆JPH(Z)∥
2
≤∥P T′⊥I⋆(Z 2)∥ 2+∥P T′⊥I⋆(Z 1)∥
2
≤δγ+βµ(Ω⋆).30 S.ENGELKEANDA.TAEB
Combining these last two bounds with the bounds from the first part, we have that:
ε δ+βmax{3ξ(T⋆)γ,2θ(Ω⋆)/γ} δ+ν α/(2−ν ) 2
≤ ≤ 1 1 ≤1−ν =1− .
χ α−βmax{3ξ(T⋆)γ,2θ(Ω⋆)/γ} α−ν α/(2−ν ) 1 ζ
1 1 1
Here, we have used the bound for γ and Assumption 1.
Next,wewillshowthatPropertyP3 holds. Specifically, weconsiderthequantitymin Z∈Q′,Φγ(Z)=1
∥PH′J+I⋆JPQ′(Z)∥ Φγ. Let Z = (Z 1,Z 2) where ∥Z∥
Φγ
= 1, Z ∈ Q′. Suppose ∥Z 1∥
∞
= 1. Then
using Lemmas 9 and 16, we have that:
∥P J+I⋆J(Z)∥ ≥∥P I⋆P (Z )∥ −∥P I⋆(Z )∥
Ω⋆ ∞ Ω⋆ Ω⋆ 1 ∞ Ω⋆ 2 ∞
≥α−∥I⋆(Z )∥ ≥α−γβξ(T′⊕span(1 1⊤))≥α−3γβξ(T′)≥α−9ξ(T⋆)βγ.
2 ∞ p p
Now suppose that ∥Z ∥ =γ. Then, we have using Lemma 12 that:
2 2
∥P I⋆J(Z)∥ ≥∥P I⋆P (Z )∥ −∥P I⋆P (Z )∥ −∥P I⋆(Z )∥
T′ 2 T′ T′ 2 2 T′ T′⊥ 2 2 T′ 1 2
≥αγ(1−2κ⋆−µ⋆/2)−δγ(2κ⋆+µ⋆/2)−βθ(Ω⋆).
Combining the above inequality, we have that:
(20)
Z∈Q′m ,Φi γn (Z)=1∥PH′J+I⋆JPQ′(Z)∥
Φγ
≥
α(1−2κ⋆−µ⋆/2)−δ(2κ⋆+µ⋆/2)−max{9ξ(T⋆)βγ,βθ(Ω⋆)}.
Now we consider the quantity max Z∈Q′,Φγ(Z)=1∥P H′⊥J+I⋆JPQ′(Z)∥ Φγ. Let Z = (Z 1,Z 2) where
∥Z∥ =1, Z ∈Q′. Suppose ∥Z ∥ =1.
Φγ 1 ∞
∥P J+I⋆J(Z)∥ ≤∥P I⋆P (Z )∥ +∥P I⋆(Z )∥
Ω⋆⊥ ∞ Ω⋆⊥ Ω⋆ 1 ∞ Ω⋆⊥ 2 ∞
≤δ+∥I⋆(Z )∥ ≤δ+9ξ(T⋆)βγ.
2 ∞
Now suppose that ∥Z ∥ =γ. Then, we have using Lemma 12:
2 2
∥P I⋆J(Z)∥ ≤∥P I⋆P (Z )∥ +∥P I⋆P (Z )∥ +∥P I⋆(Z )∥
T′⊥ 2 T′⊥ T′ 2 2 T′⊥ T′⊥ 2 2 T′⊥ 1 2
≤δγ(1+2κ⋆+µ⋆/2)+γ(2κ⋆+µ⋆/2)+βθ(Ω⋆).
Combining the above inequality, we have that:
(21)
Z∈Q′m ,Φi γn (Z)=1∥P H′⊥J+I⋆JPQ′(Z)∥
Φγ
≤δ(1+2κ⋆+µ⋆/2)+2κ⋆+µ⋆/2+max{9ξ(T⋆)βγ,βθ(Ω⋆)}.
Putting the bounds (20) and (21) together, we have:
ε δ(1+2κ⋆+µ⋆/2)+2κ⋆+µ⋆/2+βmax{9ξ(T⋆)γ,θ(Ω⋆)}
+ ≤
χ α(1−2κ⋆−µ⋆/2)−δ(2κ⋆+µ⋆/2)−βmax{9ξ(T⋆)γ,θ(Ω⋆)}
+
1+2κ⋆+µ⋆/2 δ+ 2κ⋆+µ⋆/2 + β max{9ξ(T⋆)γ,θ(Ω⋆)}
1+2κ⋆+µ⋆/2 1+2κ⋆+µ⋆/2
=
1−2κ⋆−µ⋆/2α−δ 2κ⋆+µ⋆/2 − β max{9ξ(T⋆)γ,θ(Ω⋆)}
1−2κ⋆−µ⋆/2 1−2κ⋆−µ⋆/2
1+2κ⋆+µ⋆/2 δ+ ν2α
≤ 2−ν2
1−2κ⋆−µ⋆/2α− ν2α
2−ν2
1+2κ⋆+µ⋆/2 2
= (1−ν )=1− .
1−2κ⋆−µ⋆/2 2 ζ
2
□EXTREMAL GRAPHICAL MODELING WITH LATENT VARIABLES 31
Appendix G. Proof of Theorem 3
G.1. Full theoretical statement. Let α,β,δ,d⋆,µ⋆,κ⋆,ν ,ν be the quantities in Theorem 3. Let
1 2
ω =µ⋆/4,andζ =2/ν andζ = 2 (seeProposition17. Letc ,C ,C˜ beconstants
1 1 2 1− 11 −+ 22κ κ⋆ ⋆+ −µ µ⋆ ⋆/ /2 2(1−ν2) 5 5 5
√ (cid:104) (cid:105)
thatensureCorollary4issatisfied. Letψ =max{1,∥(S⋆−L⋆)+∥ },C =8+ √ 32 5h 1+ 1 ,
2 0 α(1− 1−(κ⋆2−ω)2)( 2 −2(κ⋆+ω)) 3ζ2
(cid:16) (cid:17) ζ1
C 1 =ψ(m+d⋆), and C 2 =mmax{ 4C α0 + ψ1 ,1}. We also define,
(cid:40) (cid:40)
8α
min{α,1}(2 −2(κ⋆+ω))(cid:41) α(2 −2(κ⋆+ω))
C =min min , ζ1 ζ1
4 C 16mψC2 4(1+ 1 )
1 2 3ζ2
α(2 −2(κ⋆+ω)) α2(2 −2(κ⋆+ω))2(cid:41)
, ζ1 , ζ1 .
64C (1+ 1 ) 6144ζ (1+ 1 )2
1 3ζ2 2 3ζ2
Theorem 18. Suppose κ⋆2 ≥ µ⋆/2 and 2κ⋆+µ⋆/2 < 1. Let ν = min{ν ,ν } and suppose Assump-
1 2
tion 1 is satisfied. Suppose that:
ν2α2
d⋆µ⋆ ≤ .
(1−2κ⋆−µ⋆)18β2(2−ν)2
(cid:104) (cid:105)
Let the regularization γ be chosen in the interval γ ∈ 2βd⋆(2−ν),να(1−2κ⋆−µ⋆) . Let m=max{1, 1}
να 36βµ⋆(2−ν) γ
and m¯ = max{1,γ}. Let the effect sample size k be chosen as (n/2)ξ/2(1+ξ) ≤ k ≤ (n/2)ξ/(1+ξ).
Furthermore, suppose that:
(cid:40)
C21152m2ζ2p2log(C˜ p) 72m2ζ2 262144(1+ξ)16log(C˜ p)
k ≥max 5 2 5 + 2, 5 ,
C2c2 C2 c ξ16
4 5 4 5
(cid:18) 2−ξ(cid:19)16/7+ξ 2(cid:113) log(C˜ 5p)−8/7−ξ/2 (cid:41)
 √ 
ξ c
5
and
(cid:20) (cid:113) (cid:21)
(1) λ
n
=C
5
24 √m c5ζ2 p2log k(C˜ 5p) + 6 √m kζ2 ,
(cid:26) (cid:18) (cid:19) (cid:27)
(2) σ min(L⋆)≥max 16mm¯ λn ωC2,2ψC C2 02λn, mC 2+ α( ζ2 41(cid:104)− 1+2(κ 1⋆+ (cid:105)ω)) λ n ,
3ζ2
(3) |S⋆|≥12mm¯λ C whenever |S⋆|>0.
ij n 2 ij
Then, the estimate (Sˆ,Lˆ) is the unique minimizer of (9) with
(cid:16) (cid:17) 1
P sign(Sˆ)=sign(S⋆),rank(Lˆ)=rank(L⋆),∥(Sˆ−Lˆ)−Θ˜⋆∥ ≤2mC λ ≥1− .
2 2 n p
To arrive at the scalings provided in Theorem 3, note that, ζ = O(1/ν), ζ = O(1/ν), C =
√ √ 1 √ 2 0
O( hν/α), C = O(md⋆), C = O(m h/α2), C = O(α3ν/(d⋆m3 h). This scaling allows us
1 2 4
(cid:113) (cid:113)
to conclude that: k ≳ m3hd⋆2mνplog(p), λ = m p2log(p), σ (L⋆) ≳ m4hm¯ p2log(p), S⋆ ≳
α6ν2 n ν k min να4 k ij
√ (cid:113) √ (cid:113)
m3m¯ h p2log(p), and finally ∥(Sˆ−Lˆ)−Θ˜⋆∥ ≲ m3 h p2log(p).
να2 k 2 να2 k32 S.ENGELKEANDA.TAEB
G.2. Proof strategy. The high-level proof strategy is similar in spirit to the proofs of consistency
results for sparse graphical model recovery and latent variable graphical model recovery (Chan-
drasekaran et al., 2012), although our convex program and the conditions required for its success
are different from these previous results. Consider the following convex program
(Sˆ,Lˆ)=arg min −logdet(UT(S−L)U)−tr((S−L)Γˆ /2)+λ (∥S∥ +γ∥L∥ ).
O n 1 ⋆
(22)
S,L∈Sp
subject-to S−L∈span(1 1⊤)
p p
Comparing (22) with the convex program (9), the differences are: i) we have removed the positive-
definite constraints, ii) we have replaced tr(L) with ∥L∥ which is valid for positive semi-definite L,
⋆
iii) we have replaced the constraint (S−L)1 =0 with S−L∈span(1 1⊤) which is equivalent since
p p p
thematricesS,Laresymmetric. Regardingitemi),thepositivedefinitenessofSˆ−Lˆ isautomatically
met due to the log-det term. We show with high probability that Lˆ ⪰0.
Notethatduetothelog-detterm,wehavethatUUT(S−L)UUT =S−L. AppealingtoLemma6,
weconcludethatU(UT(S−L)U)−1UT,whichisthegradientofthenegativelog-determinatetermwith
respecttoS isequivalentto(S−L)+. Similarly,sincetr((S−L)Γˆ /2)=tr(UUT(S−L)UUTΓˆ /2)=
0 0
tr((S−L)UUTΓˆ /2UUT), the gradient of the trace term in the objective with respect to S is given
0
by UUTΓˆ /2UUT. Standard convex analysis states that (Sˆ,Lˆ) is the solution of the convex program
0
(22) if there exists a dual variable t∈R with the following conditions being satisfied:
−UUT(Γˆ /2)UUT −(Sˆ−Lˆ)++t1 1⊤ =−λ∂∥Sˆ∥
O p p 1
(23) UUT(Γˆ /2)UUT +(Sˆ−Lˆ)+−t1 1⊤ =−λγ∂∥Lˆ∥
O p p ⋆
Sˆ−Lˆ ∈span(1 1⊤).
p p
RecallthatelementsofthesubdifferentialwithrespecttonuclearnormatamatrixM havethekey
propertythattheydecomposewithrespecttothetangentspaceT(M). Specifically,thesubdifferential
with respect to the nuclear norm at a matrix M with (reduced) SVD given by M = U QUT is as
l r
follows:
N ∈∂∥M∥ ⇔P (N)=U VT,∥P (N)∥ ≤1,
⋆ T(M) l r T(M)⊥ 2
where P denotes a projection operator. Similarly, we have the following for the subdifferential of ℓ
1
norm:
N ∈∂∥M∥ ⇔P (N)=sign(N),∥P (N)∥ ≤1.
1 Ω(M) Ω(M)⊥ ∞
Let SVD of Lˆ be UˆDˆVˆT and let Z = (−λsign(Sˆ),−λγUˆVˆT). Then, letting H = Ω(Sˆ)×T(Lˆ) the
optimality conditions of (22) reduce to:
PHJ+(−UUTΓˆ O/2UUT −(Sˆ−Lˆ)+−t1 p1⊤ p)=Z,
(24) Φ (P J+(−UUTΓˆ /2UUT −(Sˆ−Lˆ)+−t1 1⊤))≤λ ,
γ H⊥ O p p n
Sˆ−Lˆ ∈span(1 1⊤).
p p
To ensure that the estimates (Sˆ,Lˆ) are close to their respective population parameters, the quantity
∆ =Sˆ−S⋆ and ∆ =Lˆ−L⋆ must be small. Since the optimality conditions of (22) are stated in
S L
termsof(Sˆ−Lˆ)+, weboundthedeviationbetween(Sˆ−Lˆ)+ and(S⋆−L⋆)+. Specifically,theTaylor
Series expansion of (Sˆ−Lˆ)+ around (S⋆−L⋆)+ is given by:
(Sˆ−Lˆ)+ =(S⋆−L⋆+J(∆ ,∆ ))+ =(S⋆−L⋆)++(S⋆−L⋆)+J(∆ ,∆ )(S⋆−L⋆)++R J(∆ ,∆ ).
S L S L Γ⋆ S L
0EXTREMAL GRAPHICAL MODELING WITH LATENT VARIABLES 33
where some algebra yields the following representation for the remainder term R (J(∆ ,∆ )):
Γ⋆ S L
0
(cid:34) ∞ (cid:35)
(cid:88)
(25) R (J(∆ ,∆ ))=U(S⋆−L⋆+1 1⊤/p)−1 (−J(∆ ,∆ )(S⋆−L⋆+1 1⊤/p)−1)k UT.
Γ⋆ 0 S L p p S L p p
k=2
From Theorem 2, we have that (S−L)+ =UUT(−Γ⋆/2)UUT. Since UUT(S⋆−L⋆)UUT =S⋆−L⋆,
we appeal to Lemma 6 to conclude that (UT(S⋆ −L⋆)U)−1 = UT(−Γ⋆)U. Let E := UUT(Γˆ −
O n O
Γ⋆)/2UUT. Then,wehavethefollowingequivalentcharacterizationoftheoptimalityconditions(23):
PHJ+((S⋆−L⋆)+J(∆ S,∆ L)(S⋆−L⋆)++R
Γ⋆
0J(∆ S,∆ L)+E n+t1 p1⊤ p)=Z,
(26) Φ γ(P H⊥J+((S⋆−L⋆)+J(∆ S,∆ L)(S⋆−L⋆)++R Γ⋆ 0J(∆ S,∆ L)+E n+t1 p1⊤ p))≤λ n,
Sˆ−Lˆ ∈span(1 1⊤).
p p
Finally, Since (S⋆−L⋆)1 1⊤ =0 and J(∆ ,∆ )1 1⊤ =0, we have the following formulation of the
p p S L p p
optimality condition (26) in terms of the matrix I⋆
PHJ+(I⋆(J(∆ S,∆ L+t1 p1⊤ p))+R
Γ⋆
0J(∆ S,∆ L+t1 p1⊤ p)+E n)=Z,
(27) Φ γ(P H⊥J+(I⋆(J(∆ S,∆ L+t1 p1⊤ p))+R Γ⋆ 0J(∆ S,∆ L+t1 p1⊤ p)+E n))≤λ n,
Sˆ−Lˆ ∈span(1 1⊤).
p p
It is straightforward to show that if for some (Sˆ,Lˆ), the second condition in (27) is satisfied with
strict inequality, that is:
Φ (P J+(I⋆(J(∆ ,∆ +t1 1⊤))+R J(∆ ,∆ +t1 1⊤)+E ))<λ .
γ H⊥ S L p p Γ⋆ 0 S L p p n n
G.3. Constrained optimization problem. We consider the following non-convex optimization
problem:
argmin −logdet(UT(S−L)U)−tr((S−L)Γˆ /2)+λ (∥S∥ +γ∥L∥ ),
O n 1 ⋆
(28) S∈Sp,L∈Sp
subject-to S−L∈span(1 1⊤) ; (S,L)∈M
p p
where:
(cid:40)
M= S,L∈Sp :S ∈Ω⋆,rank(L)≤rank(L⋆)
(cid:41)
C λ
∥P (L−L⋆)∥ ≤ 0 n ; Φ (J+I⋆J(S−S⋆,L−L⋆))≤C λ ,
T⋆⊥ 2 ψ γ 0 n
√ (cid:104) (cid:105)
withC =10+ √ 32 5h 1+ 1 . Theoptimizationprogram(28)isnon-convex
0 α(1− 1−(κ⋆2−ω)2)( 2 −2(κ⋆+ω)) 3ζ2
ζ1
duetotherankconstraintrank(L)≤rank(L⋆)inthesetM. Theseconstraintsensurethatthematrix
L belongs to an appropriate variety. The constraints in M along T⋆⊥ ensure that the tangent space
T(L) is close to T⋆. Finally, the last condition roughly controls the error. We begin by proving the
following useful proposition:
Proposition 19. Let (S,L) be a set of feasible variables of (28). Let ∆= (S−S⋆,L−L⋆). Then,
(cid:16) (cid:17)
Φ γ(∆)≤C 2λ n where C 2 =mmax{ 4C α0 + ψ1 ,1}.34 S.ENGELKEANDA.TAEB
Proof of Proposition 19. Let H⋆ =Ω⋆×T⋆. Then:
Φ γ[J+I⋆JPH⋆(∆)]≤Φ γ[J+I⋆J(∆)]+Φ γ[J+I⋆JP H⋆⊥(∆)]
≤C λ +mC λ ≤2mC λ .
0 n 0 n 0 n
Since Φ γ[PH⋆(·)] ≤ 2Φ γ[·], we have that: Φ γ[PH⋆J+I⋆JPH⋆(∆)] ≤ 4mC 0λ n. Then, appealing to
Property P1, we have that: Φ γ[PH⋆(∆)]≤ 4C α0λn. Moreover, Φ γ(∆)≤Φ γ[PH⋆(∆)]+Φ γ[P H⋆⊥(∆)]≤
(cid:16) (cid:17)
λ nm 4C α0 + ψ1 . □
Proposition 19 leads to powerful implications. In particular, under additional conditions on the
minimum nonzero singular values of L⋆, any feasible set of variables (S,L) of (28) has two key
properties: (a) The variables (S,L) are smooth points of their underlying varieties with L ⪰ 0 and
S −L ⪰ 0, and (b) The constraints in M along T⋆⊥ are locally inactive at L. These properties,
among others, are proved in the following corollary.
Corollary 20. Consider any feasible variables (S,L) of (28). Let T′ =T(L). Let σ be the smallest
nonzerosingularvalueofL⋆ andsbethesmallestinmagnitudenonzerovalueofS⋆. LetH′ =Ω⋆×T′,
C =P (L⋆) and C =P (L⋆). Suppose that the following inequalities
T′ T′⊥
(cid:26)
T′⊕span(1p1⊤ p) (cid:18)(T′⊕span(1p1⊤ p))⊥
(cid:19) (cid:27)
are met: σ ≥max 16mm¯ λn ωC2,2ψC C2 02λn, mC 2+ α( ζ2 41(cid:104)− 1+2(κ 1⋆+ (cid:105)ω)) λ n and s≥12mm¯λ nC 2. Then,
3ζ2
(1) L and S are smooth points of their underlying varieties so that support(Sˆ) = support(S⋆)
and rank(Lˆ)=rank(L⋆). Furthermore, L⪰0, and S−L⪰0
(2) ∥P T⋆⊥(Lˆ−L⋆)∥ 2 ≤ C 20 ψλn,
(3) ρ(T′,T⋆)≤ω,
(4) max{Φ γ(J+I⋆C T′),Φ γ(J+I⋆ (cid:104)C
T′⊕1p1 (cid:105)⊤
p)}≤ 6λ ζn 2,
(5) Φ γ[J+C T′]≤
α( ζ2
1−4 2λ (n
κ⋆+ω))
1+ 31
ζ2
.
Proof of Corollary 20. We appeal to the results regarding the perturbation analysis of the low-rank
matrix variety.
(1) Based on assumptions regarding the minimum nonzero singular value of L⋆ and minimum
nonzero entry in magnitude of S⋆, one can check that since ω ≤1
λ C
σ ≥12mm¯ n 2 ≥12mm¯λ C ≥8∥L−L⋆∥ ,
ω n 2 2
s≥12mm¯λ C ≥12mm¯λ C ≥2∥S−S⋆∥ .
n 2 n 2 2
Combining these results, we conclude that S,L are smooth points of their varieties, namely
that rank(L) = rank(L⋆) and support(S) = support(S⋆). The fact that L ⪰ 0 follows from
σ ≥2∥L−L⋆∥ 2. Furthermore, to check that S−L⪰0, first note that σ min(S⋆−L⋆)≥ √1 ψ.
Then, ∥S −L−(S⋆ −L⋆)∥ ≤ 2mC λ . From the choice of λ and the condition on the
2 2 n n
sample size, we have that 4mC 2λ
n
< √1 ψ. Thus, S−L⪰0.
(2) Since σ ≥ 8∥L−L⋆∥ , we can appeal to Proposition 2.2 of Chandrasekaran et al. (2012) to
2
conclude that the constrain5 in M along P is strictly feasible:
T⋆⊥
∥L−L⋆∥2 C2λ2 C λ
∥P (L−L⋆)∥ ≤ 2 ≤ 2 n < 0 n.
T⋆⊥ 2 σ σ ψEXTREMAL GRAPHICAL MODELING WITH LATENT VARIABLES 35
(3) AppealingtoProposition2.1ofChandrasekaranetal.(2012),weprovethatthetangentspace
T′ is close to T⋆:
2∥L−L⋆∥ 2mm¯λ C ω
ρ(T′,T⋆)≤ 2 ≤ n 2 ≤ω.
σ 12mm¯λ C
n 2
(4) Letting σ′ be the minimum nonzero singular value of L. One can check that:
σ′ ≥σ−∥L−L⋆∥ ≥σ−mC λ ≥10mC λ ≥8∥L−L⋆∥ .
2 2 n 2 n 2
One can also obtain the following lower bounds for σ′:
σ′ ≥σ−∥L−L⋆∥ ≥σ−mC λ ≥6ζ mC2ψλ −mC λ ≥6ζ mψC2λ
2 2 n 2 2 n 2 n 2 2 n
α(2 −2(κ⋆+ω))λ
σ′ ≥σ−∥L−L⋆∥ ≥σ−mC λ ≥ ζ1 n
2 2 n (cid:104) (cid:105)
4 1+ 1
3ζ2
where we have used C ψ ≥ 1. Once again appealing to Proposition 2.2 of Chandrasekaran
2
et al. (2012) and simple algebra, we have:
∥L−L⋆∥2 C2λ2 λ
Φ [J+I⋆C ]≤mψ∥C ∥ ≤mψ 2 ≤mψ 2 n ≤ n.
γ T′ T′ 2 σ′ 6ζ mψC2λ 6ζ
2 2 n 2
From Lemma 13, we have that ∥C ∥ ≤ ∥C ∥ . Following the same logic as above,
T′⊕1p1⊤
p
2 T′ 2
we can then show that: Φ γ[J+I⋆C
T′⊕1p1⊤
p]≤ 6λ ζn 2.
(5) Finally, we show that:
∥L−L⋆∥2 mC2λ2 4λ (cid:20) 1 (cid:21)
Φ [C ]≤m∥P (L−L⋆)∥ ≤m 2 ≤ 2 n ≤ n 1+ .
γ T′ T′⊥ 2 σ′ σ′ α(2 −2(κ⋆+ω)) 3ζ
ζ1 2
□
Consider any optimal solution (SˆM,LˆM) of (28). We will show that (SˆM,LˆM) is the unique
solution of the nonconvex program (28), as well as the unique solution of (22).
G.4. Variety constrained program to tangent space constrained program. Let (SˆM,LˆM)
be any optimal solution of (28). In Corollary 20, we conclude that the variables (SˆM,LˆM) are
smooth points of their respective varieties. As a result, the rank constraint rank(L) ≤ rank(L⋆) can
be linearized to L ∈ T(LˆM). Since all the remaining constraints are convex, the optimum of the
linearized program is also the optimum of (28). Moreover, we once more appeal to Corollary 20
to conclude that the constraints in M along T⋆⊥ are strictly feasible at LˆM. As a result, these
constraints are inactive and can be removed in this “linearized program”. We now argue that the
constraint Φ [J+I⋆J(SˆM−S⋆,LˆM−L⋆)] is inactive. For notational simplicity, we let T′ =T(LˆM)
γ
and H′ =Ω⋆×T′, we consider the following optimization problem:
(S˜,L˜)= argmin −logdet(UT(S−L)U)−tr((S−L)Γˆ /2)+λ (∥S∥ +γ∥L∥ )
O n 1 ⋆
(29) S∈Sp,L∈Sp
subject-to (S,L)∈H′,S−L∈span(1 1⊤).
p p
We prove that under conditions imposed on the regularization parameter λ , the pair of variables
n
(SˆM,LˆM) is the unique optimum of (29). First, note that the optimum of (29) is unique since it is
a strictly convex program convex because the negative log-likelihood terms have a strictly positive-
definite Hessian due to Property P1. To show that (SˆM,LˆM) is the optimum of (29), it suffices to
show strict feasibility of the constraint, that is: Φ [J+I⋆J(S˜−S⋆,L˜−L⋆)]<C λ .
γ 0 n36 S.ENGELKEANDA.TAEB
The optimality conditions of (29) states that there exists Q ∈Ω⋆⊥, Q ∈T′⊥, t∈R such that:
Ω T
−Γˆ /2−(S˜−L˜)++t1 1⊤+Q =−λ∂∥S˜∥ ,
O p p Ω 1
(30) Γˆ /2+(S˜−L˜)+−t1 1⊤+Q =−λγ∂∥L˜∥ ,
O p p T ⋆
S˜−L˜ ∈span(1 1⊤).
p p
Let the reduced SVD of L˜ be given by L˜ =U¯D¯V¯T and Z =(λsign(S˜),λγU¯V¯T). Following a similar
logic as in Section G.2 and restricting the optimality conditions to the space of H, we have the
following equivalent characterization of the optimality conditions:
(31)
PH′J+(I⋆(J(∆ S,∆ L+t1 p1⊤ p))+R
Γ⋆
0J(∆ S,∆ L+t1 p1⊤ p)+E n)=Z,
S˜−L˜ ∈span(1 1⊤).
p p
Here, ∆ = S˜−S⋆, ∆ = L˜ −L⋆. In the remaining, we will denote ∆ = L˜ −L⋆+t1 1⊤. Our
S L L+ p p
result relies on the following propositions to control the remainder term.
Proposition 21. Suppose Φ (∆ ,∆ ) ≤ 1 for C = ψ(m + d⋆) and any ∆ ∈ Ω⋆. Then,
γ S L+ 2C1 1 S
Φ [J+R (J(∆ ,∆ ))]≤2mψC2Φ (∆ ,∆ )2.
γ Γ⋆ 0 S L+ 1 γ S L+
Proof of Proposition 21. We have that:
∥∆ ∥
∥J(∆ ,∆ )∥ ≤∥∆ ∥ +∥∆ ∥ ≤θ(Ω⋆)∥∆ ∥ +γ L+ 2 ≤(γ+θ(Ω⋆))Φ (∆ ,∆ )
S L+ 2 S 2 L+ 2 S ∞ γ γ S L+
1
≤(m+d⋆)Φ (∆ ,∆ )≤ .
γ S L+ 2ψ
Therefore,
∞
(cid:88) 1
∥R (J(∆ ,∆ ))∥ ≤ψ (∥∆ +∆ ∥ ψ)k ≤ψ3∥∆ +∆ ∥2
Γ⋆ 0 S L+ 2 S L+ 2 S L+ 21−∥∆ +∆ ∥ ψ
S L+ 2
k=2
(cid:18)
α
(cid:19)2
≤2ψ3 1+ Φ (∆ ,∆ )2 =2ψC2Φ (∆ ,∆ )2.
6ζ γ S L+ 2 γ S L+
2
Putting everything together, we have the desired result. □
NoticethattheboundontheremaindertermisdependentontheerrortermΦ (∆ ,∆ ). Inthe
γ S L+
following proposition, we bound this error so we can control the remainder term.
Proposition 22. Let S˜,L˜ be the solution of convex program (29). Define
(cid:40) (cid:41)
4
r =max [Φ (J+E )+Φ (J+I⋆C )+λ ],Φ [(0,C )] .
α(2 −2(κ⋆+ω)) γ n γ T′ n γ T′
ζ1
(cid:26) min{α,1}( 2 −2(κ⋆+ω))(cid:27) √
If we have that r ≤min C8α 1, 16mζ1
ψC 22
, then Φ γ(∆ S,∆ L)≤ 1−√ 14 −r (κ5 ⋆h
2−ω)2
√
and Φ (0,t1 1⊤)≤ √4r 5h .
γ p p 1− 1−(κ⋆2−ω)2
The proof of the proposition relies on the following lemma which we state and prove first.
Lemma 23. Consider the following optimization:
(32)
argmin −logdet(UT(S−L)U)−tr((S−L)Γˆ /2)+trace(t1 1⊤(S−L))+λ (∥S∥ +γ∥L∥ ).
O p p n 1 ⋆
S∈Sp,L∈Sp
subject-to (S,L)∈H′EXTREMAL GRAPHICAL MODELING WITH LATENT VARIABLES 37
Then, the solution of (32) is unique and is equal to S˜,L˜ (i.e. the solution of (29)).
Proof of Lemma 23. Note that by Property P1, the estimator (32) is strictly convex. We will denote
the optimal solution of (32) by (S˜,L˜). We are using the same notation as the optimal solution of
(29) as we will show momentarily that these optimal solutions are identical. Specifically, define Z as
is done before Proposition 21. Let ∆ =S˜−S⋆ and ∆ =L˜−L⋆. The optimality condition of (32)
S L
is given by:
(33) PH′J+(I⋆J(∆ S,∆ L+t1 p1⊤ p)+R
Γ⋆
0J(∆ S,∆ L+t1 p1⊤ p)+E n)=Z.
Notice that the optimality condition (33) is identical to the first condition in (31). Since (32) has a
unique solution, then, the optimal solutions of (29) and (32) coincide. □
Proof of Proposition 22. Since T′ is a tangent space such that ρ(T′,T⋆) ≤ ω, we have from Prop-
erty P1 that the operator B = (PH′J+I⋆JPH′)−1 is bijective and is well-defined. Consider the
following function taking as input (δ ,δ )∈Q′ where Q′ =Ω⋆×(T′⊕t1 1⊤):
S L+ p p
F(δ S,δ L+)=(δ S,δ L+)−B(cid:8) PH′J+[I⋆J(δ S,δ L+)+R Γ⋆(J(δ S,δ L++C T′))+I⋆C
T′
+E n−Z(cid:9) .
0
Here,C
T′
=P T′⊥(L⋆). Nowapoint(δ S,δ L+)isafixedpointofF ifandonlyifPH′J+[I⋆J(δ S,δ L+)+
R (J(δ ,δ +C ))+I⋆C +E ] = Z. Further, a fixed point (δ ,δ ) provides certificates of
Γ⋆ S L+ T′ T′ n S L+
0
optimality for (32). Specifically, let S˜ = S⋆ + δ . By Lemma 14, find a unique decomposition
S
of δ = L+t1 1⊤ where L ∈ T′. Then, let L˜ = P (L⋆)+L. By construction, the parameters
L+ p p T′
(S˜,L˜)thensatisfytheoptimalityconditionfor(33)andthusalsotheoptimalityconditionof (31)after
appealingtoLemma23. Inotherwords,thefixedpointofthefunctionF isPH′(∆ S,∆ L)+(0,t1 p1⊤ p).
Next, using Brouwer’s fixed point theorem, we show that F has a fixed point that lies in the ball
B ={(δ ,δ )∈Q′|Φ (δ ,δ )≤r}. Specifically, note that F can be equivalently expressed as:
r S L+ γ S L+
F(δ S,δ L+)=P H′⊥(δ S,δ L+)−B(cid:8) PH′J+[R Γ⋆(J(δ S,δ L++C T′))+I⋆[C
T′
+JP H′⊥(δ S,δ L+)]+E n−Z(cid:9) .
0
First, note that by appealing to Lemma 12, we have that:
Φ [P (δ ,δ )]≤2r(κ⋆+ω).
γ H′⊥ S L+
Similarly, we have from Property P2 that:
(cid:18) (cid:19)
Φ γ(cid:2) B(cid:8) PH′J+IJP H′⊥(δ S,δ L+)(cid:9)(cid:3) ≤r 1− ζ2 .
1
Finally, we note that:
Φ γ(cid:2) B(cid:8) PH′J+[R
Γ⋆
0(J(δ S,δ L++C T′))+I⋆C T′ +E n−Z(cid:9)(cid:3)
≤ 2 (cid:0) Φ [J+R (J(δ ,δ +C ))]+Φ [I⋆C ]+Φ [E ]+λ (cid:1)
α γ Γ⋆ 0 S L+ T′ γ T′ γ n n
≤ r( ζ2 1 −2(κ⋆−ω)) + 2 (cid:0) Φ [J+R (J(δ ,δ +C ))](cid:1)
2 α γ Γ⋆ 0 S L+ T′
wherethelastinequalityisbythedefinitionofr. Bytheassumptiononr,wehavethatΦ ((δ ,δ )+
γ S L+
(0,C ))≤ 1 . And so we can appeal to Proposition 21 to conclude that:
T′ 2C1
2 8mψC2r2 16mψC2r r(2 −2(κ⋆+ω))
Φ [J+R (J((δ ,δ +C )≤ 1 ≤ 2 ζ1 ≤r/2,
α γ Γ⋆ 0 S L+ T′ α α(2 −2(κ⋆+ω)) 2
ζ1
where the last inequality uses the bound on r.38 S.ENGELKEANDA.TAEB
SobyBrouwer’sfixedpointtheorem,weconcludethat: Φ γ[PH′(∆ S,∆ L)+(0,t1 p1⊤ p)]≤r. Finally,
note that: Φ [P (∆ ,∆ )] ≤ r. Thus, Φ [(∆ ,∆ )+(0,t1 1⊤)] ≤ 2r. Finally, appealing to
γ H′⊥ S L γ S L p p
√
Lemma 15 and some manipulations, we have the bound max{Φ (∆ ,∆ ),t1 1⊤}≤ √4r 5h .
γ S L p p 1− 1−(κ⋆2−ω)2
□
We are now ready to state the following proposition.
Proposition 24. Suppose that Φ γ[J+E n]≤ 6λ ζn
2
and suppose that:
(cid:40) (cid:40)
8α
min{α,1}(2 −2(κ⋆+ω))(cid:41) α(2 −2(κ⋆+ω))
λ ≤min min , ζ1 ζ1
n C 16mψC2 4(1+ 1 )
1 2 3ζ2
α(2 −2(κ⋆+ω)) α2(2 −2(κ⋆+ω))2(cid:41)
, ζ1 , ζ1 .
64C (1+ 1 ) 6144ζ (1+ 1 )2
1 3ζ2 2 3ζ2
Then, we have that: S˜=SˆM, L˜ =LˆM.
Proof. From Corollary 20, we have that Φ γ[J+I⋆C T′]≤ 6λ ζn 2. We then have that:
4
[Φ (J+E )+Φ (J+I⋆C )+λ ]
α(2 −2(κ⋆+ω)) γ n γ T′ n
ζ1
4λ
(cid:20)
1
(cid:21) (cid:40)
8α
min{α,1}(2 −2(κ⋆+ω))(cid:41)
≤ n 1+ ≤min , ζ1 .
α(2 −2(κ⋆+ω)) 3ζ C 16mψC2
ζ1 2 1 2
(cid:104) (cid:105) (cid:104) (cid:105)
WealsohavefromCorollary20thatΦ γJ+C T′ ≤ α( ζ2 1−4 2λ (n κ⋆+ω)) 1+ 31 ζ2 . Letr = α( ζ2 1−4 2λ (n κ⋆+ω)) 1+ 31 ζ2 .
We can appeal to Proposition 22 to conclude that:
√
(cid:20) (cid:21)
16λ 5h 1
Φ [∆ ,∆ ]≤ n 1+ .
γ S L α(1−(cid:112) 1−(κ⋆2−ω)2)(2 −2(κ⋆+ω)) 3ζ 2
ζ1
From the bound on λ , we have that: Φ [∆ ,∆ ] ≤ 1 . So we can appeal to Proposition 21 to
n γ S L 2C1
conclude that:
λ
(34) Φ [J+R J(∆ ,∆ )]≤2mψC2Φ [∆ ,∆ ]2 ≤ n,
γ Γ⋆ O S L 1 γ S L 6ζ 2
wherehereagainweusetheboundonλ . Notethat∆ =∆ +t1 1⊤. WehavefromCorollary20
n L+ L p p
that Φ γ[J+I⋆C T′]≤ 6λ ζn 2. From the optimality conditions of (29), we have that:
Φ γ(PH′J+I⋆JPH′(∆ S,∆ L))≤2λ n+2Φ γ(0,t1 p1⊤ p)+Φ γ[J+R
Γ⋆
OJ(∆ S,∆ L)]
+Φ γ[PH′J+I⋆C T′]+Φ γ[J+E n],
√
(cid:20) (cid:21)
λ 16λ 5h 1
≤2λ + n + n 1+ ,
n 2ζ 2 α(1−(cid:112) 1−(κ⋆2−ω)2)(2 −2(κ⋆+ω)) 3ζ 2
ζ1EXTREMAL GRAPHICAL MODELING WITH LATENT VARIABLES 39
where the second inequality follows from bound on Φ ((0,t1 1⊤)) in Proposition 22. Appealing to
γ p p
Property P2, we have that: Φ γ(P H′⊥J+I⋆JPH′(∆ S,∆ L)) ≤ Φ γ(P H′⊥J+I⋆JPH′(∆ S,∆ L)). There-
fore:
Φ γ(J+I⋆J(∆ S,∆ L))≤Φ γ(PH′J+I⋆JPH′(∆ S,∆ L))+Φ γ(P H′⊥J+I⋆JPH′(∆ S,∆ L))
√
(cid:20) (cid:21)
32λ 5h 1
+Φ [J+I⋆C ]≤8λ + n 1+
γ T′ n α(1−(cid:112) 1−(κ⋆2−ω)2)(2 −2(κ⋆+ω)) 3ζ 2
ζ1
<C λ .
0 n
□
G.5. Removing the tangent space constraint. Itremainstoconnecttheestimator(29)with(9).
In particular, we check that S˜ = Sˆ and L˜ = Lˆ where (S˜,L˜) is the solution of (29) and (Sˆ,Lˆ) is the
solution of (9). We formalize this in the following proposition.
Proposition 25. Suppose that Φ γ[J+E n]≤ 6λ ζn 2. Then, S˜=Sˆ and L˜ =Lˆ.
Proof of Proposition 25. We must show that (S˜,L˜) satisfy the optimality conditions of (22) in (27),
namely that there exists a dual variable t such that
PHJ+(I⋆(J(∆ S,∆ L+t1 p1⊤ p))+R
Γ⋆
0J(∆ S,∆ L)+E n)=Z,
(35) Φ γ(P H⊥J+(I⋆(J(∆ S,∆ L+t1 p1⊤ p))+R Γ⋆ 0J(∆ S,∆ L)+E n))<1,
S˜−L˜ ∈span(1 1⊤),
p p
where ∆ =S˜−S⋆ and ∆ =L˜−L⋆. Notice that the first and third optimality conditions are the
S L
same as (31). It remains to show the second inequality where the strict inequality is to ensure that
(S˜,L˜) is the unique solution. It suffices to show that:
Φ γ(P H⊥J+(I⋆PQ′(J(∆ S,∆ L+t1 p1⊤ p))
(36)
<λ −Φ [R J(∆ ,∆ )]−Φ [J+I⋆C ]−Φ [J+E ].
n γ Γ⋆
0
S L γ T′⊕1p1⊤
p
γ n
Manipulating the first optimality condition, we have that:
Φ γ(PHJ+(I⋆PQ′(J(∆ S,∆ L+t1 p1⊤ p))≤λ n+2(Φ γ[R
Γ⋆
0J(∆ S,∆ L)]+Φ γ[J+I⋆C
T′⊕1p1⊤
p]+Φ γ[J+E n])
(cid:18) (cid:19)
λ 1
≤λ + n =λ 1+ ,
n ζ n ζ
2 2
where we have here used the bound Φ γ[J+I⋆C
T′⊕1p1⊤
p] ≤ 6λ ζn
2
from Corollary 20 and the bounds
Φ γ[R Γ⋆ 0J(∆ S,∆ L)]≤ 6λ ζn
2
from (34) and Φ γ[J+E n]≤ 6λ ζn
2
from proposition statement. Appealing to
Property P3, we then have that:
(cid:18) (cid:19)(cid:18) (cid:19) (cid:18) (cid:19)
1 1 1 1
Φ γ(P H⊥J+(I⋆PQ′(J(∆ S,∆ L+t1 p1⊤ p))≤λ
n
1+
ζ
1−
ζ
=λ
n
1−
ζ2
<λ n(1−
2ζ
).
2 2 2 2
Since Φ γ[R Γ⋆ 0J(∆ S,∆ L)]+Φ γ[J+I⋆C T′⊕1p1⊤
p
+Φ γ[J+E n]≤ 2λ ζn 2, we have shown that (36) holds.
□
(cid:20) (cid:113) (cid:21)
G.6. BoundingtheerrortermΦ γ[J+E n]. Letλ
n
=C
5
24 √m c5ζ2 p2log k(C˜ 5p) + 6 √m kζ2 wherec 5,C 5,C˜
5
are defined in Corollary 4.40 S.ENGELKEANDA.TAEB
Lemma 26. Under the conditions of Theorem 3, we have:
(cid:18) (cid:19)
λ
P Φ [J+E ]≤ n ≥1−p−1.
γ n 6ζ
2
Proof. Note that Φ γ[J+E n]≤m∥Γ⋆ O−Γˆ O∥ 2 ≤pm∥Γ⋆ O−Γˆ O∥ ∞. To show that, Φ γ[J+E n]≤ 6λ ζn 2, it
suffices to show that
(cid:115)
4C log(C˜ p) C
(37) ∥Γ⋆ −Γˆ ∥ ≤ √ 5 5 + √5.
O O ∞ c 5 k k
Based on the condition on k, it is straightforward to show that:
(cid:40)(cid:18) k(cid:19)ξ 1+ϑ(cid:41) 4C (cid:115) log(C˜ p) C
C (log(n/k))2+ √ ≤ √ 5 5 + √5.
5 n k c 5 k k
(cid:113) √ √
for ϑ=2 log(C˜ p)/ c . Note that ϑ≤ k/log(n)4. Furthermore, since (n/2)ξ/(1+ξ) ≥k, k ≤n/2.
5 5
Appealing to Corollary 4, we have that with probability greater than 1−C˜ 5p3e−c5ϑ2 =1−p−1 that
the bound in (37) is satisfied. □
G.7. Summary and putting things together. Combining Propositions 24 and 25, we conclude
that under the conditions of Theorem 3, with probability greater than 1−1/p, the optimal solution
(Sˆ,Lˆ) of (22) is unique and equal to an optimal solution (SˆM,LˆM) of (28). From Corollary 20,
we have that Sˆ−Lˆ ⪰ 0, Lˆ ⪰ 0. Thus, (Sˆ,Lˆ) = (SˆM,LˆM) is also the unique minimizer of (9).
The guarantees on the closeness of (Sˆ,Lˆ) to the population parameters (S⋆,L⋆) then follows from
Corollary 20 and Proposition 19.
Appendix H. Refitting for eglatent
Suppose (Sˆ,Lˆ) is the solution of (9) in the first step. We then obtain refitted parameters (S˜,L˜) as
the second step by solving the following convex optimization program:
(S˜,L˜)= argmin −logdet(UT(S−L)U)−tr((S−L)Γˆ /2),
O
S∈Sp,L∈Sp
s.t. S−L⪰0,L⪰0,(S−L)1 =0,
p
support(S)⊆support(Sˆ),col-space(L)⊆col-space(Lˆ).
Here, the constraint support(S) ⊆ support(Sˆ) restricts the graph structure of our refitted solution
to be contained in the graph estimated in the first step. Similarly, the constraint col-space(L) ⊆
col-space(Lˆ) restricts the row/column space of the refitted low-rank term to be contained in the
row/column space estimated in the first step.
Appendix I. Additional experimental results
We consider the exact same setup as in the simulation study in Section 5.1.1. The only difference
is that we specify the sub-graph G = (E ,O) among the observed variables to be an Erd˝os–R¨enyi
0 O
with edge probability 0.08 and set Θ⋆ to −2 for every (i,j) ∈ E and zero otherwise. The latent
ij O
variables are not connected in the joint graph so that Θ⋆ = 0 for every i,j ∈ H,i ̸= j. We connect
ij
each latent variable node i ∈ H to every k ∈ O where k = i+ζh for some positive integer ζ. The
corresponding entries Θ⋆ in the precision matrix are chosen uniformly at random from the interval
√ √ ik
[30/ p+h,60/ p+h]. Finally, we set the diagonal entries of Θ⋆ so that it has the all-ones vector
in its null space.EXTREMAL GRAPHICAL MODELING WITH LATENT VARIABLES 41
The rest of the simulation study is carried out as described in Section 5.1.1. Figure 7 summarizes
the performance of all the methods on 50 independent results. We again observe that our approach
outperforms eglearn, and accurately recovers the graphical structure among the observed variables
as well as the number of latent variables. In terms of validation likelihood, eglatent is a bit weaker
than in the simulation with the cycle graph.42 S.ENGELKEANDA.TAEB
h=1 h=2 h=3
1.00
0.75
0.50
0.25
0.00
100 1000 5000 100 1000 5000 100 1000 5000
k
Method: eglatent_cv eglatent_oracle eglearn_cv eglearn_oracle
h=1 h=2 h=3
6
4
2
0
100 1000 5000 100 1000 5000 100 1000 5000
k
Method: rk_cv rk_oracle
h=1 h=2 h=3
250
0
-250
100 1000 5000 100 1000 5000 100 1000 5000
k
Figure 7. F-score (top row) and estimated number of latent variables (middle row) of eglatent
methodwiththeselectionofthetuningparameterbasedontheoracleandcross-validationonthe
F-score for the random graph with h = 1,2,3 latent variables and different effective sample sizes
k=100,1000,5000. Thebottomrowshowsthedifferencebetweenbesteglatentandbesteglearn
log-likelihoodsonthevalidationset.
erocs-F
knaR
ecnereffid
doohilekil-goL