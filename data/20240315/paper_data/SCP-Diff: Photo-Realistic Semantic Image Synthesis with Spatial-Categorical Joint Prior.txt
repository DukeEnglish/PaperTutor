SCP-Diff: Photo-Realistic Semantic Image
Synthesis with Spatial-Categorical Joint Prior
Huan-ang Gao*1, Mingju Gao*1, Jiaju Li1,2, Wenyi Li1,
Rong Zhi3, Hao Tang4, and Hao Zhao†1
1 Institute for AI Industry Research (AIR), Tsinghua University
2 University of Chinese Academy of Sciences
3 Mercedes-Benz Group China Ltd.
4 Carnegie Mellon University
gha20@mails.tsinghua.edu.cn, gaomingju19@mails.ucas.ac.cn
zhaohao@air.tsinghua.edu.cn
Project Page: https://air-discover.github.io/SCP-Diff/
(a) Semantic Mask (b) Ground Truth Images or SCP-Diff Results (shuffled)
Fig.1: For single-domain Cityscapes [8], our method can generate photo-
realisticimagesfromsemanticmasks(a).In(b),weshufflegroundtruthreal
images and SCP-Diff results and the right answer is in the footnote of the
conclusion. While the state-of-the-art method ECGAN [40] achieves 44.5
FID on Cityscapes, our method achieves 10.5 FID. The quality is credited
to the strong spatial and categorical prior of Cityscapes.
Abstract. Semantic image synthesis (SIS) shows good promises for
sensor simulation. However, current best practices in this field, based
on GANs, have not yet reached the desired level of quality. As latent
1 * Indicates Equal Contribution. † Indicates Corresponding Author.
4202
raM
41
]VC.sc[
1v83690.3042:viXra2 Gao et al.
Semantic Mask OASIS Semantic Mask OASIS Semantic Mask OASIS
ControlNet Ours ControlNet Ours ControlNet Ours
+ Spatial Prior + Categorial Prior + Joint Prior
(a) (b) (c)
Fig.2: We explain the underlying mechanisms of our proposed noise priors
using ADE20K [59]. (a) Introducing the spatial prior aligns the image style with
the dataset’s aesthetic while removing odd substructures in large semantic areas. (b)
Incorporatingtheclasspriorcanenhancealignmentwiththeprovidedsemanticmasks.
(c)Byjointingspatialandclasspriors,theirbeneficialfeaturesarecombined,allowing
our joint prior (SCP-Diff) to achieve state-of-the-art results on ADE20K.
diffusion models make significant strides in image generation, we are
prompted to evaluate ControlNet, a notable method for its dense con-
trolcapabilities.Ourinvestigationuncoveredtwoprimaryissueswithits
results:thepresenceofweirdsub-structureswithinlargesemanticareas
and the misalignment of content with the semantic mask. Through em-
pirical study, we pinpointed the cause of these problems as a mismatch
between the noised training data distribution and the standard normal
priorappliedattheinferencestage.Toaddressthischallenge,wedevel-
opedspecificnoisepriorsforSIS,encompassingspatial,categorical,and
aninnovativespatial-categoricaljointpriorforinference.Thisapproach,
whichwehavenamedSCP-Diff,hasyieldedexceptionalresults,achiev-
inganFIDof10.53onCityscapesand12.66onADE20K.Thecodeand
models can be accessed via the project page.
Keywords: SemanticImageSynthesis·DiffusionModels·NoisePriors
1 Introduction
Semanticimagesynthesisisdedicatedtogeneratinghigh-qualityimagesaligned
with provided semantic maps, offering users the ability to accurately control
the spatial layout of the generated images. The potential of this technology is
particularly notable in fields like autonomous driving and robotics, because it
enables the creation of highly realistic and diverse virtual environments with
semantic layout control, reducing the dependency on extensive real-world data
collection [20,21,25,53,58].
However, current leading techniques [33,40] in this area, which rely on Gen-
erative Adversarial Networks (GANs), have not reached the anticipated quality
levels necessary for the practical application of sensor simulation. As latent dif-
fusionmodelsachieveremarkablesuccessinimagecreation,wearemotivatedto
evaluate ControlNet [56], a method that enables dense control over Stable Dif-
fusion [30]. We finetuned ControlNet with semantic label masks as conditions,SCP-Diff 3
but identified two significant issues in the generated results: (i) the emergence
of weird sub-structures in large semantic regions (see Fig. 2(a), the bathtub is
divided into two parts) and (ii) the misalignment of content placement with the
provided semantic masks (see Fig. 2(b), buildings appear in the sky region).
Why do large-scale pretrained latent diffusion models struggle to complete
this semantic image synthesis task after finetuning? Our empirical analysis (see
Fig.3)revealsthattheprimarysourceofdiscrepancyinqualitybetweenthegen-
erated outcomes and the actual images is not from the score matching learning
(i.e., finetuning) process. Instead, it originates from a mismatch [17] between
the distribution of noised data used during training and the standard normal
distribution typically employed during the inference process.
Devise noise priors for inference. To address the mismatch in the dis-
tributions between training and inference, we implemented inference-time noise
priors tailored for SIS, which can be seamlessly integrated into the finetuned
ControlNet without further training. Initially, we introduced a spatial prior by
estimatingthedistributionofreallatentsthroughaGaussianmodelandaverag-
ingtheoutcomesacrossbatches,similarto[11].Thisapproachnotablyimproved
the organization of scene layouts (for instance, the bathtub is now recognized
as a single entity rather than fragmented parts in Fig. 2(a)) and enriched the
diversity of colors observed in the images. This prior is closer to the training
trajectory; however, discrepancies with the provided label masks remained evi-
dent (such as an unintended lamp appearing on the wall in Fig. 2(a)). We argue
that this issue arises because the spatial noise prior unfits the control branch
of ControlNet, where the former contains a mixture of modes (corresponding to
different categories) during reduction, which may hinder the latter to generate
meaningfulresiduals(addedbacktotheSDbranch)foron-trajectory denoising.
To further refine our approach, we explored a categorical prior by aggregating
real-image latents by class and started denoising from the aggregated statistics.
This strategy helped reduce label alignment issues, though it resulted in the
outputs reverting to a monotonous color scheme, as shown in Fig. 2(b).
To combine the best of both worlds, we introduced the Spatial-Categorical
Joint Prior and coined this diffusion-based synthesis technique SCP-Diff. Our
thorough experiments have confirmed the efficacy of SCP-Diff. Remarkably, de-
spite its simplicity and the fact that it is training-free, SCP-Diff sets state-of-
the-art on single-domain Cityscapes and multi-domain ADE20K simultanously,
synthesizes photo-realistic scenes that are difficult to distinguish from real ones
(seeFig.1,2(c)).Asidefromquality,wealsoquantitativelydemonstratethatin-
corporatingthepriordoesnotnegativelyimpactthediversityofdiffusion-based
generativemodeling.Userstudiesarealsosupportiveforthequalityandfidelity
of the generated images to the provided label masks. The contributions of this
work can be summarized as follows:
– Wepinpointthechallengeposedbythediscrepancyininferencedistribution
in finetuned ControlNets for Semantic Image Synthesis (SIS) and introduce
a solution that utilizes inference noise priors to bridge this gap, notably
without the need for retraining.4 Gao et al.
– We elucidate the design philosophy behind the creation of inference noise
priors tailored for the SIS task, unpacking the mechanics of spatial and
categorical priors within SIS, and finally integrate them into a joint one.
– Demonstrating superior capabilities, our integrated joint prior (SCP-Diff)
achieves state-of-the-art performance on two well-established SIS bench-
marks, Cityscapes and ADE20K, delivering high-quality outputs that are
hard to distinguish from the real-world images.
2 Related Work
2.1 Semantic Image Synthesis
Semanticimagesynthesis(SIS)[19,23,26,42,43,46]aimstocreatephoto-realistic
images from semantic label maps. Previous works in this field are mostly based
on Generative Adversarial Networks (GANs), which are trained using both ad-
versarialloss[13]andreconstructionloss.Akeyadvancementinnetworkdesign
was introduced with AdaIN [16], which aligns the mean and variance of content
features with those of the style features. Building on this modulation concept,
SPADE[26]proposesspatiallyadaptivenormalizationtobetterembedsemantic
layoutsintothegenerator.CLADE[35]furtheroptimizedSPADE’sapproachby
introducing a more efficient normalization layer that adapts to different seman-
tic classes. The state-of-the-art GAN-based approach, ECGAN [40], proposes
to use the edge as an intermediate representation to help modulate the image
generation process and employs contrastive learning to derive embeddings rich
in semantic information.
Theadventofconditionaldenoisingdiffusionprobabilisticmodels[15,32]has
spurred innovations in SIS. FreestyleNet [52] innovates within this space by ad-
justing the cross-attention maps, ensuring that each text token influences only
the pixel regions delineated by the semantic mask. Following the modulation
concept,SDM[47]designsanoveldenoiserarchitecturewithmodulationlayers.
However, these methods directly operate in pixel space and, consequently, pro-
duceoutcomesthatarenotashigh-resolutionasthosegeneratedbyGAN-based
techniques due to VRAM constraints.
2.2 Latent Diffusion-based Controllable Generation
Addressing the challenges of slow inference speeds and exceedingly high train-
ing expenses in pixel space, Stable Diffusion [30] introduced a two-stage image
synthesis strategy. Initially, it employs VQGANs [10] to compress the image
into a discretized latent code. Subsequently, it utilizes powerful diffusion-based
generative modeling to process these latent representations, employing a UNet
denoiser to exploit the inductive biases in images, which supports the text con-
dition via cross attention. Building upon Stable Diffusion [30], numerous efforts
aimtobroadencontroltoadditionalmodalities,suchasincorporatingCLIPfea-
tures[29],ormodifyingspecificregionsthroughtheinpaintingformulation[1,9].SCP-Diff 5
ControlNet [56] offers task-specific control by using a control branch to en-
code conditions. Within SIS, these conditions can be specified by color-coded
semantic masks, and we report that such formulation can achieve state-of-the-
art performance over previous SIS methods. Nevertheless, a more thorough ex-
amination uncovers issues such as the generation of unrealistic sub-structure
and misalignment between the generated images and the corresponding seman-
tic masks. This paper posits that the root cause of these issues lies in the initial
noise setup and suggests the integration of the proposed pre-computed noise
priors.
2.3 Playing Noise Tricks in Diffusion Models
Sampling Inversion.AsDDIMsampling[32]operatesdeterministically,Null-
text Inversion [24] learns to inverse this sampling process to extract noise from
earliertimesteps,whichisidealforeditingthesignalimagebutnotforgenerating
new ones with label masks.
Signal Leakage in the Noised Latents. In video diffusion models, PY-
oCo [12] observed that the noise maps corresponding to frames from the same
video tend to group together. FreeNoise [28] shows that rescheduling a sequence
of noises enables long-range correlation modeling for longer video generation.
These findings point to the assumption that common diffusion noise schedules
inadequately corrupt signals in images, as discussed in [11]. To address this,
FreeInit [50] refines the low-frequency components of the inference initial noise
inaniterativemannerinvideodiffusionmodelsatthecostofinferencetime.[17]
recommendsretrainingwithadifferentnoiseschedule,whichiscomputationally
intensive. Note that none of these works specifically addresses SIS, where the
key challenge is maintaining alignment with labels for controlled generation.
3 Preliminaries and Observations
3.1 Preliminaries
Given a semantic segmentation mask M ∈ NH×W, where H and W represent
height and width, and each element within M corresponds to a semantic label
assigned to a specific pixel, the objective of semantic image synthesis (SIS) is to
devise a function that transforms M into a photorealistic image I ∈RH×W×3.
Thestate-of-the-artmethodforSIS,ControlNet[56],istrainedtoreconstruct
an initial image x by removing noise from a distorted image x in a given
0 t
timestept∈[0,T].Ateachtimestept,thedenoisingmodelε ispresentedwith,
θ
√ (cid:112)
x = α x + (1−α )ε , (1)
t t 0 t t
where x is the image encoded by VQGAN [10] encoder, ε ∼ N(0,I) and α
0 t t
is the cumulative product of scaling at each timestep t. Taking x as input,
t
thedenoisingmodelpredictstheaddednoiseεˆ =ε (x ,t,T,M),whereT isthe
t θ t6 Gao et al.
encodingofthetextconditionandM isthelabelmask.Withinthemodel,anon-
trainable branch retains its configuration from the pretrained weights of Stable
Diffusion [30] and is tasked with processing x ,t, and T. Simultaneously, the
t
modelduplicatesitsencodertoaseparate,trainablebranchtoprocessM,which
is subsequently linked to the non-trainable one using layers of zero convolution.
The loss function used to train the denoiser is defined as,
L=E [∥ε −εˆ∥2]. (2)
x0,t,T,M,εt t t
At inference time, we sample x
T
∼ Nnormal, i.e., N(0,I) and conduct the
reverse denosing process iteratively from t=T until t=1,
√
(cid:18) (cid:19)
x =√ α x t− √1−α t·εˆ t +(cid:112) 1−α ·εˆ. (3)
t−1 t−1 α t−1 t
t
AfterT iterations,weget x anddecodeittoI usingtheVQGANdecoder[10].
0
3.2 Denoising from N ? An Unreliable Inference Assumption
normal
Simply applying ControlNet [56] finetun- FID
ingtotheSemanticImageSynthesis(SIS)
taskresultsinsuboptimaloutcomes,such (a) Error from divergence of noise prior distribution
as monotonous color schemes, weird sub-
stuctures,oraninabilitytoaccuratelyfol-
low semantic label masks, as illustrated
in Figs. 2 and 7. To understand the un-
derlying reasons for these issues, we per- (b) Error from score matching learning
form an empirical analysis by finetuning (c) Error from sampling a subset of images from the whole dataset
on the ADE20K dataset. The results are Diffusion & Denoising Steps (Normalized by )
reported in Fig. 3, where we try to ana- Fig.3:Anempiricalμ studyof𝑇𝑇 denoising
lyze the errors associated with the gener- priors. Brown dotted line denotes the
ated results compared to the images from FID denoising from Nnormal, and the
the ADE20K dataset. polyline illustrates the FID denoising
WeintroduceN x0,µT asameanstoin- from N x0,µT introduced in main text.
vestigatewhetherthescorematchinglearningprocess,specificallytheoptimiza-
tion of Eq. (2), incorporates any errors,
√
N :=N( α x ,(1−α )I), (4)
x0,µT µT 0 µT
Intuitively, for each pair of (x ,M), we supply the denoising model with x
0 µT
(as defined in Eq. (1), µ ∈ [0,1]) and the condition branch with M, followed
by assessing the resultant generation using the FID score. Although an increase
in the FID score is expected with the progression of the denoising steps µT
(Fig. 3(b)), a significant discrepancy (Fig. 3(a)) remains between the denoising
from N
x0,T
(µ=1) and a standard denoising inference process, Nnormal.
The empirical study reveals that the common inference assumption that x
T
should closely resemble Nnormal is unreliable. This argument is also supportedSCP-Diff 7
Pixel Space Latent Space
Spatial Prior
Aggregate by
pixel
V EQ ncG oA deN
r
𝒩𝒩( 𝐻𝐻′×𝑊𝑊…′×4, 𝐻𝐻′×𝑊𝑊…′×4)
Categorial Prior
Aggregate by
class
𝒩𝒩( 𝐶𝐶×4 , 𝐶𝐶×4 )
Joint Prior
Aggregate by
pixel & class
𝑁𝑁×𝐻𝐻×𝑊𝑊×3 𝑁𝑁×𝐻𝐻𝐻×𝑊𝑊𝐻×4 𝒩𝒩( 𝐻𝐻′×𝑊𝑊′×𝐶𝐶×4, 𝐻𝐻′×𝑊𝑊′×𝐶𝐶) ×4
(a) Noise Prior Preparation
Text Prompt T Label MaskM’ Time t
Context Encoder
𝒩𝒩( 𝐻𝐻′×𝑊𝑊…′×4, 𝐻𝐻′×𝑊𝑊…′×4) 𝒩𝒩( 𝐻𝐻′L ×𝑊𝑊at′e ×n 4t Prior, Distrib𝐻𝐻u′×ti𝑊𝑊o′n×4 )
Sample Denoise Denoise
𝒩𝒩( 𝐶𝐶×4 , 𝐶𝐶×4 ) …
Latent
Representation
𝒩𝒩( 𝐻𝐻′×𝑊𝑊′×𝐶𝐶×4, 𝐻𝐻′×𝑊𝑊′×𝐶𝐶)
×4
𝐻𝐻𝐻×𝑊𝑊𝐻×4
𝑥𝑥𝜇𝜇𝑇𝑇 Rev𝑥𝑥 e𝜇𝜇 r𝑇𝑇 s− e1
Denoising
𝑥𝑥0
Forward Diffusion Steps
𝜇𝜇𝑇𝑇
Noise Prior VQGAN
Decoder
Label Map 𝐻𝐻𝐻×𝑊𝑊𝐻×4
𝐻𝐻𝐻×𝑊𝑊𝐻 𝑀𝑀𝐻 (b) Inference with Noise Prior
Fig.4: Overview of our proposed framework. N stands for Gaussian distri-
bution.
by the mathematical analysis presented in [17,55], arguing that current settings
ofα inpracticecanmakethedenoisingmodeldistinguishbetweeninitialization
t
samples (namely x in our study) in training and testing cases. Nevertheless,
T
theirproposedsolution,whichinvolvesretrainingdiffusionmodels,iscostlyand
does not leverage important domain knowledge of semantic image synthesis,
which demands (i) a comprehensive grasp of scene layouts from segmentation
masks and (ii) adherence to pixel-wise dense semantic label maps at the same
time.
4 Method
4.1 Overview
InFig.4,weprovideanoverviewofourproposedframework,whichisstructured
intotwomainstages:thepreparationoflatentpriorsandtheapplicationofthese
pre-computed latent priors during inference.
Noise Prior Preparation. With a set of N reference images, this phase
involvesreducingtheseimagesintolatentpriors,approximatedbyGaussiandis-
tributions. Initially, the images are transformed into the latent space using a
pretrained VQGAN [10] encoder, followed by computing the means and vari-
ances. Depending on the type of prior needed, we apply Eq. (5) for the spatial8 Gao et al.
Denoising Timestep t
12 x 16
L
reyaL
24 x 32 red
o
ControlNet + Spatial Prior (0.85T)
ceD
48 x 64
12 x 16
Semantic Label L
reyaL
24 x 32 red
o
ceD
48 x 64
ControlNet 0.85T 0.65T 0.45T 0.25T 0.05T
Fig.5: Case study on spatial prior. We provided the finetuned ControlNet [56]
with identical semantic label maps (middle on the left, where white pixels represent
’unlabeled’), but introduced different priors, spatial prior (above) and normal prior
(below).Wevisualizeattentionmapsofdecoderblockswherethequeryregionishigh-
lightedwitha‘✕’.Theimagesbeneaththeattentionmapmasksrepresenttheultimate
decoding outcomes and are included solely for visualization purposes.
prior, Eq. (7) for the categorical prior, and Eq. (8) for the joint version of the
two.
InferencewithNoisePrior.Wefirstassemblethelatentpriordistribution
mapalignedwiththeprovideddownsampledlabelmapM′.Forthespatialprior,
we directly replicate it. Otherwise, M′ is used to index the specified prior on a
token-by-tokenbasistoconstructadistributionmap.Fromthisdistribution,we
sample a latent representation and introduce noise for µT steps to create the
noise prior. This noise prior is then processed through the fine-tuned Control-
Net [56], which denoises the last µT steps. The final step involves utilizing the
pretrained VQGAN [10] decoder to reconstruct the generated image.
4.2 Exploring Spatial Prior and Categorical Prior
With N reference latent images and their associated masks {(x(i),M(i))}N
0 i=1
from the dataset, our objective is to reduce them into noise priors N which
xµT
aligns with the training trajectory of noise distributions, facilitating low-error
inference.
Spatial Prior.Giventhatthetrainingprocessisfocusedonminimizingthe
score matching loss (Eq. (2)) for each instance of x in expectation, with the
0
theoretical premise that every x has an equal impact on the ultimately learned
0SCP-Diff 9
training noise distribution, our initial step is to diminish the reference latent
images {x(i)}N across the batch dimension.
0 i=1
More specifically, we define the spatial prior as,
Nspatial :=N
(cid:32) (cid:88)N x N( 0i) ,(cid:34) (cid:88)N N1 (cid:104)
x( 0i)−µ(x(
0i))(cid:105)2(cid:35)
⊙I
H′×W′×4(cid:33)
, (5)
i=1 i=1
where ⊙ denotes the Hadamard product. For the sake of simplification in our
reductionprocess,wechoosenottomodelthecorrelationsbetweenspatialtokens
collectively, treating each spatial location as an individual marginal distribution
instead. For initializing noise values for the inference process started at µT, we
sample from,
(cid:0)√ (cid:1)
Nspatial,µT :=N α
µT
·xspatial,(1−α µT)I , where xspatial ∼Nspatial. (6)
So, what is encoded by the spatial prior? To answer this, we conducted a
case study, illustrated in Fig. 5. From the analysis, we see that the group using
spatial priors exhibits a broader receptive field in constructing scene layouts, in
contrast to the group employing normal priors, which quickly focuses its atten-
tionnarrowlyonlocalfields.Thisdistinctionshedslightonwhythespatialprior
group can generate completed scenes with less weird sub-structures, while the
normal prior group’s output resembles cropping and pasting objects following
similar shape masks. Further insights from Fig. 7 demonstrate that the use of
spatial priors facilitates the production of images that are consistent with the
dataset style and enriched with a wider spectrum of colors and textures.
Categorical Prior. While spatial priors succeed in achieving global atten-
tionacrossthescenetoconstructscenelayoutswithfidelity,theyfallshortinin-
corporating class-specific information. This shortfall becomes apparent through
hallucinatory artifacts, such as sketching buildings in the sky or drawing lamps
on walls, as depicted in Fig. 2(a, b). We believe these hallucinations stem from
theimcompatibilitybetweenthespatialnoiseprior(whichfeaturesamixofclass
modes after reduction) and ControlNet’s control branch (which is only trained
todenoisenoisedtokensofcorrespondingclasseswiththelabelmask).Thismis-
matchconfusesthecontrolbranch,leadingtothegenerationofresiduals(added
back to the SD branch) that are less effective in denoising the current sample.
Such errors accumulate along the denoising trajectory, potentially exacerbating
the denoising process further away from the intended trajectory.
Therefore, we delve into the analysis of running statistics on a class-specific
basis, calculating a categorical prior for each class. Initially, we downscale M to
M′ ∈ NH′×W′ through nearest pixel selection, employing a scale factor of H .
H′
Subsequently,forN referenceimages,weorganizedistinctsetsN foreachclass
c
c in the set of classes C, which comprise encoded tokens of dimension 1×4.
Following this, we compute the mean and standard deviation for each class to
achieve a class-wise statistical reduction.
Ncategorical,c :=N (Mean[N c],Var[N c]). (7)10 Gao et al.
ControlNet Attention Score Map ControlNet Attention Score Map
Semantic Mask Semantic Mask
+ Categorial Prior (0.85T) + Categorial Prior (0.85T)
(a) Sky (b) Tree
ControlNet Attention Score Map ControlNet Attention Score Map
Semantic Mask Semantic Mask
+ Categorial Prior (0.85T) + Categorial Prior (0.85T)
(c) Floor (d) Wall
Fig.6: Case study on categorial prior. We examine the intermediate features
of encoder blocks from the middle of the denoising process where the object shapes
begin to form [2,4,44]. The cross-attention score map is computed for these features
againstthelanguageembeddingscorrespondingtovariouscategories.It’simportantto
notethatthisextracategoricalinformationfromnaturallanguageisutilizedsolelyfor
visualizationanddoesnotusedintheactualdenoisingprocess.Thedenoiserexclusively
identifies categories through the semantic mask and our categorical prior.
The definition of Ncategorical,c,µT is a replication of Eq. (6). To examine the
knowledgeencodedinthecategoricalprior,wealsoconductacasestudyinFig.6.
We find that the latent features denoised from the categorical prior achieve a
multi-modal understanding of natural languages at the stage of object shaping.
This can explain why our categorical prior can generate results that have better
alignment with the given semantic mask. However, simply using the categorical
prior makes the color scheme revert to a monotonous manner.
4.3 Joint Prior
InSec.4.2,weexplorespatialprior,whichaidsinconstructingscenelayoutsbut
falls short in generalizing to class-specific details, and class priors, which excel
in generating localized objects of a certain class, yet lack comprehensive global
attention. In this section, we introduce a combined prior that effectively merges
these two aspects, which we dub as joint prior.
The calculation of the joint prior, Njoint, can be formulated as follows. For
N reference images, we use different sets N to store encoded tokens with
x,y,cSCP-Diff 11
dimensions 1×4. Subsequently, the mean and variance are calculated for each
tuple (x,y,c), where x∈[0,H′),y ∈[0,W′),c∈C,
Njoint,x,y,c :=N (Mean[N x,y,c],Var[N x,y,c]). (8)
In instances where the sample size for a given tuple is excessively small, we
revert to taking the statistics from class prior, assuming that such situations
are rare and indicating a scenario where the local shape formation of an object
should take precedence. The formulation of Njoint,x,y,c,µT is also like Eq. (6). As
evidenced by both quantitative (Tab. 1) and qualitative analyses (Fig. 7), the
joint prior effectively incorporates the strengths of both spatial and categorical
priors.
Discussion of Denoising steps µT. Ideally, the coefficient µ of the de-
noisingtimestepsneedstobecarefullytuned.Asmallerµmeansinjectinglower
levels of noise into the calculated latent priors, thereby reducing the number
of denoising steps required and accelerating the inference process. Nevertheless,
within the framework of the joint prior, as we treat encoded tokens from vary-
ing spatial positions and categories as independent variables and overlook their
correlations, a higher number of denoising steps µ becomes necessary. This re-
quirement is due to the need for more self-attention to transition the marginal
statistics towards joint modeling progressively. On a practical note, employing
DDIMcanexpeditethissearchproblem,significantlynarrowingdownthesearch
space of µ-s to a stride of 0.05. We provide this study in Fig. 8.
5 Experiments
5.1 Setup
We evaluate our proposed noise priors on three challenging datasets: Cityscapes
[8], ADE20K [59] and COCO-Stuff [3]. We use (i) the mean intersection-over-
union (mIoU) and pixel accuracy (Acc) to evaluate the alignment with the pro-
vided label mask (following SPADE [26]), (ii) the Fréchet inception distance
(FID) score [14] to access the quality of generated images, (iii) LPIPS [57] and
MS-SSIM [49] to evaluate the diversity of generated images, and (iv) user study
to see the aesthetic appeal of the outcomes.
We pre-compute the proposed priors on N=10,000 images. These latent pri-
ors are applied to ControlNet models [56] finetuned on three different datasets,
Table 1: Quantitative Comparison of Different Noise Priors.
Cityscapes[8] ADE20K[59]
Method
mIoU↑ Acc↑ FID↓ mIoU↑ Acc↑ FID↓
NormalPrior 65.14 94.14 23.35 20.73 61.14 20.58
(+0.00) (+0.00) (+0.00) (+0.00) (+0.00) (+0.00)
SpatialPrior 66.77 94.29 12.83 20.86 64.46 16.03
(+1.63) (+0.15) (−10.52) (+0.13) (+3.32) (−4.55)
CategoricalPrior 66.86 94.54 11.63 21.86 66.63 16.56
(+1.72) (+0.40) (−11.72) (+1.13) (+5.49) (−4.02)
JointPrior 67.92 94.65 10.53 25.61 71.79 12.66
(+2.78) (+0.51) (−12.82) (+4.88) (+10.65) (−7.92)12 Gao et al.
Label Mask OASIS ControlNet Spatial Prior Categorical Prior Joint Prior
Fig.7: Qualitative Comparison of Different Noise Priors. The resolutions for
generatedresultsonCityscapes(top4rows)[8]andADE20K(bottom3rows)[59]are
1024×512 and 512×512, respectively. Please zoom in for a better view.
respectively. This finetuning process is conducted using a batch size of 16 on
A100 80G GPUs, employing a learning rate of 10−5 across 100,000 steps. The
decoder in the original Stable Diffusion [30] branch is also unlocked for tuning.
5.2 Main Results
Comparison of Noise Priors. In this section, we quantitatively and qualita-
tivelycomparedifferentnoisepriorsweproposedinthispaper,includingnormal
prior (which reduces to standard finetuned ControlNet [56] inference scheme),
spatialprior,categoricalpriorandjointprior.TheresultsarereportedinTab.1
and Fig. 7.
Tab.1showsthatthejointprioroutperformsthestandardinferencepriorofa
normaldistributionusedbyControlNet[56]intermsofimagequality(FID)andSCP-Diff 13
consistencywiththeprovidedlabelmasks(mIoUandAcc).Forinstance,onthe
Cityscapesdataset,ourapproachregistersanimprovementinmIoUof+2.78and
a significant reduction in FID of -12.82. Examining the generated images more
closely reveals that, while ControlNet (or the normal prior) tends to produce
images with softer edges and fewer blur effects compared to OASIS, it struggles
withscenelayoutorganizationandoftenfailstoaligncorrectlywiththeprovided
label masks, such as incorrectly placing buildings in the sky area. Conversely,
our proposed joint prior addresses these issues, significantly enhancing photo-
realism.
Comparison with State- Table2:ComparisonofOurMethodwith
State-of-the-Art Approaches.
of-the-Arts Compared to state-
of-the-art advancements in se- FID
Method
mantic image synthesis, our pro- Cityscapes ADE20K COCO-Stuff
CRN[6] 104.7 73.3 70.4
posed joint prior (or SCP-Diff)
SIMS[27] 49.7 - -
maintains a competitive edge. As
Pix2pixHD[46] 95.0 81.8 111.5
shown in Tab. 2, ControlNet [56] GauGAN[26] 71.8 33.9 22.6
(or normal prior), by shifting DPGAN[38] 53.0 31.7 -
the focus of generative model- DAGAN[36] 60.3 31.9 -
ing from pixel space to latent SelectionGAN[42] 65.2 33.1 -
SelectionGAN++[41] 63.4 32.2 -
space, already significantly sur-
LGGAN[43] 57.7 31.6 -
passes earlier methods, benefit-
LGGAN++[39] 48.1 30.5 -
ing from the ability to synthe- CC-FPSE[18] 54.3 31.7 19.2
size high-resolution images. Our INADE[34] 44.3 35.2 -
approachamplifiesthisadvantage GroupDNet[60] 47.3 41.7 -
SC-GAN[48] - 29.3 18.1
by addressing the issue of the in-
OASIS[33] 47.7 28.3 17.0
herent gap in the inference prior
RESAIL[31] 45.5 30.2 18.3
distribution of diffusion models, SAFM[22] 49.5 32.8 24.6
thus setting an astonishing low ECGAN[37] 44.5 25.8 15.7
FID score for the Cityscapes [8] PITI[45] - 27.9 16.1
FreestyleNet[52] - 25.0 14.4
and ADE20K [59] datasets.
SDM[47] 42.1 27.5 15.9
ControlNet[56] 20.7 20.6 28.4
IntheCOCO-Stuff[3]dataset, SCP-Diff(Jointprior) 10.5 13.0 17.6
our performance is on par with
that of leading methods, despite
achieving a significant improvement over ControlNet. This result can be at-
tributed to the diverse spatial resolutions present within the COCO-Stuff
dataset, requiring images to be resized into distorted shapes for processing by
UNets, due to the requirement of denoising and diffusion processes for data of a
uniform shape. Future research could explore solutions to this challenge.
By applying inference noise priors at µT where µ < 1, we can optimize the
traditionallytime-consumingsamplingmethodoflatentdiffusionmodels(which
typically requires an amortized time of 2.34 seconds per image on A100 GPUs
withoutanyaccelerationtechniques)toafactorofµ.Theselectionofµisablated
in the subsequent section.14 Gao et al.
Table 3: Diversity of Results. Table 4: Results of User Study.
Method ADE20K[59] Method ResultQuality↑ ConditionFidelity↑
LPIPS↑ MS-SSIM↓
OASIS[33] 1.45±0.48 1.65±0.59
OASIS[33] 0.35 0.65
ControlNet[56] 1.93±0.46 1.80±0.52
ControlNet[56] 0.59 0.17
SCP-Diff 2.62±0.36 2.55±0.43
SCP-Diff 0.56 0.21
5.3 Study on Denoising Steps µT
AsdiscussedinSec.4.3,weconductedanexperimenttoseetheeffectofµT over
the quality of the generated results. From Fig. 8, we can see that the optimal
choice of µ for quality lies in [0.8,0.9] for both datasets. The trend patterns ob-
served in both curves are similar, indicating a level of robustness when applying
noise priors.
(a) FID on Cityscapes (b) FID on ADE20K
Diffusion & Denoising Steps 𝝁(Normalized by 𝑇) Diffusion & Denoising Steps 𝝁(Normalized by 𝑇)
Fig.8: Study on the Effects of Denoising Steps µT.
5.4 Study on Diversity of Generated Results
Following OASIS [33], we assess the diversity of the generated images by ana-
lyzing the variation within a set of images created from the same label map, re-
ferredtoasabatch,usingMS-SSIMandLPIPS.Weproduce20imagesperlabel
map, calculate the mean pairwise scores across these images, and then average
thesescoresoverthelabelmaps.Intuitively,thelargerdifferencesamongimages
within the same batch indicate a higher diversity in the generated outcomes.
From the results in Tab. 3, we see that our joint prior registers a marginally re-
duceddiversityscorecomparedtoControlNet[56].Thisoutcomeisanticipated,
considering that introducing priors to the inference process inherently balances
diversity against improved quality.
5.5 User Study
Following ControlNet [56], we conduct a user study and invite participants to
rank 500 groups of images generated by three methods (see Tab. 4) individuallySCP-Diff 15
in terms of “the quality of displayed images” and “the fidelity to the given label
mask”. We employ the Average Human Ranking (AHR) as a metric for user
preference, where participants rank each result on a scale from 1 to 3, with
3 being the best. According to the average rankings shown in Tab. 4, users
significantly favor the outcomes produced by our method over those generated
by ControlNet [56].
6 Conclusion
In this paper, we have addressed the challenge of the inference distribution dis-
crepancy in finetuned ControlNets for Semantic Image Synthesis (SIS) by in-
troducing inference noise priors that effectively bridge this gap, eliminating the
needforretraining.WedevisedinferencenoisepriorstaileredforSISanddelved
into the design philosophy and mechanics behind them, exploring both spatial
and categorical priors before successfully integrating them into a comprehen-
sive joint prior. Our SCP-Diff showcases outstanding performance, setting new
benchmarks in SIS on two prominent datasets, Cityscapes and ADE20K. We
hope our insights and high-quality generated images will inspire future works in
the research community.23
References
1. Avrahami,O.,Lischinski,D.,Fried,O.:Blendeddiffusionfortext-driveneditingof
naturalimages.In:ProceedingsoftheIEEE/CVFConferenceonComputerVision
and Pattern Recognition. pp. 18208–18218 (2022) 4
2. Balaji, Y., Nah, S., Huang, X., Vahdat, A., Song, J., Kreis, K., Aittala, M., Aila,
T., Laine, S., Catanzaro, B., et al.: ediffi: Text-to-image diffusion models with an
ensemble of expert denoisers. arXiv preprint arXiv:2211.01324 (2022) 10
3. Caesar,H.,Uijlings,J.,Ferrari,V.:Coco-stuff:Thingandstuffclassesincontext.
In:ProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition.
pp. 1209–1218 (2018) 11, 13, 19
4. Cao,M.,Wang,X.,Qi,Z.,Shan,Y.,Qie,X.,Zheng,Y.:Masactrl:Tuning-freemu-
tualself-attentioncontrolforconsistentimagesynthesisandediting.arXivpreprint
arXiv:2304.08465 (2023) 10
5. Chen, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L.: Semantic
image segmentation with deep convolutional nets and fully connected crfs. arXiv
preprint arXiv:1412.7062 (2014) 19
6. Chen,Q.,Koltun,V.:Photographicimagesynthesiswithcascadedrefinementnet-
works. In: Proceedings of the IEEE international conference on computer vision.
pp. 1511–1520 (2017) 13
7. Chong, M.J., Forsyth, D.: Effectively unbiased fid and inception score and where
to find them. arXiv preprint arXiv:1911.07023 (2019) 19
2 The real images in Fig. 1 are all in the left.
3 This research is supported by Tsinghua University – Mercedes Benz Institute for
Sustainable Mobility.16 Gao et al.
8. Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R.,
Franke, U., Roth, S., Schiele, B.: The cityscapes dataset for semantic urban scene
understanding. In: Proceedings of the IEEE conference on computer vision and
pattern recognition. pp. 3213–3223 (2016) 1, 11, 12, 13, 19
9. Couairon, G., Verbeek, J., Schwenk, H., Cord, M.: Diffedit: Diffusion-based se-
manticimageeditingwithmaskguidance.arXivpreprintarXiv:2210.11427(2022)
4
10. Esser,P.,Rombach,R.,Ommer,B.:Tamingtransformersforhigh-resolutionimage
synthesis. In: Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition. pp. 12873–12883 (2021) 4, 5, 6, 7, 8
11. Everaert,M.N.,Fitsios,A.,Bocchio,M.,Arpa,S.,Süsstrunk,S.,Achanta,R.:Ex-
ploitingthesignal-leakbiasindiffusionmodels.In:ProceedingsoftheIEEE/CVF
Winter Conference on Applications of Computer Vision. pp. 4025–4034 (2024) 3,
5
12. Ge, S., Nah, S., Liu, G., Poon, T., Tao, A., Catanzaro, B., Jacobs, D., Huang,
J.B., Liu, M.Y., Balaji, Y.: Preserve your own correlation: A noise prior for video
diffusion models. In: Proceedings of the IEEE/CVF International Conference on
Computer Vision. pp. 22930–22941 (2023) 5
13. Goodfellow,I.,Pouget-Abadie,J.,Mirza,M.,Xu,B.,Warde-Farley,D.,Ozair,S.,
Courville,A.,Bengio,Y.:Generativeadversarialnetworks.Communicationsofthe
ACM 63(11), 139–144 (2020) 4
14. Heusel,M.,Ramsauer,H.,Unterthiner,T.,Nessler,B.,Hochreiter,S.:Ganstrained
byatwotime-scaleupdateruleconvergetoalocalnashequilibrium.Advancesin
neural information processing systems 30 (2017) 11
15. Ho, J., Jain, A., Abbeel, P.: Denoising diffusion probabilistic models. Advances in
neural information processing systems 33, 6840–6851 (2020) 4
16. Huang,X.,Belongie,S.:Arbitrarystyletransferinreal-timewithadaptiveinstance
normalization. In: Proceedings of the IEEE international conference on computer
vision. pp. 1501–1510 (2017) 4
17. Lin,S.,Liu,B.,Li,J.,Yang,X.:Commondiffusionnoiseschedulesandsamplesteps
are flawed. In: Proceedings of the IEEE/CVF Winter Conference on Applications
of Computer Vision. pp. 5404–5411 (2024) 3, 5, 7
18. Liu, X., Yin, G., Shao, J., Wang, X., et al.: Learning to predict layout-to-image
conditional convolutions for semantic image synthesis. Advances in Neural Infor-
mation Processing Systems 32 (2019) 13
19. Lu,M., Zhao, H.,Yao,A., Chen, Y.,Xu, F., Zhang,L.:A closed-formsolution to
universalstyletransfer.In:ProceedingsoftheIEEE/CVFInternationalConference
on Computer Vision. pp. 5952–5961 (2019) 4
20. Luo, W., Yang, S., Wang, H., Long, B., Zhang, W.: Context-consistent seman-
tic image editing with style-preserved modulation. In: European Conference on
Computer Vision. pp. 561–578. Springer (2022) 2
21. Luo, W., Yang, S., Zhang, X., Zhang, W.: Siedob: Semantic image editing by dis-
entangling object and background. In: Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition. pp. 1868–1878 (2023) 2
22. Lv, Z., Li, X., Niu, Z., Cao, B., Zuo, W.: Semantic-shape adaptive feature modu-
lationforsemanticimagesynthesis.In:ProceedingsoftheIEEE/CVFConference
on Computer Vision and Pattern Recognition. pp. 11214–11223 (2022) 13
23. Lv, Z., Wei, Y., Zuo, W., Wong, K.Y.K.: Place: Adaptive layout-semantic fusion
for semantic image synthesis (2024) 4SCP-Diff 17
24. Mokady, R., Hertz, A., Aberman, K., Pritch, Y., Cohen-Or, D.: Null-text inver-
sion for editing real images using guided diffusion models. In: Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 6038–
6047 (2023) 5
25. Ntavelis, E., Romero, A., Kastanis, I., Van Gool, L., Timofte, R.: Sesame: Se-
manticeditingofscenesbyadding,manipulatingorerasingobjects.In:Computer
Vision–ECCV2020:16thEuropeanConference,Glasgow,UK,August23–28,2020,
Proceedings, Part XXII 16. pp. 394–411. Springer (2020) 2
26. Park, T., Liu, M.Y., Wang, T.C., Zhu, J.Y.: Semantic image synthesis with
spatially-adaptivenormalization.In:ProceedingsoftheIEEE/CVFconferenceon
computer vision and pattern recognition. pp. 2337–2346 (2019) 4, 11, 13
27. Qi, X., Chen, Q., Jia, J., Koltun, V.: Semi-parametric image synthesis. In: Pro-
ceedings of the IEEE Conference on Computer Vision and Pattern Recognition.
pp. 8808–8816 (2018) 13
28. Qiu,H.,Xia,M.,Zhang,Y.,He,Y.,Wang,X.,Shan,Y.,Liu,Z.:Freenoise:Tuning-
freelongervideodiffusionvianoiserescheduling.arXivpreprintarXiv:2310.15169
(2023) 5
29. Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., Chen, M.: Hierarchical text-
conditional image generation with clip latents. arXiv preprint arXiv:2204.06125
1(2), 3 (2022) 4
30. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution
image synthesis with latent diffusion models. In: Proceedings of the IEEE/CVF
conferenceoncomputervisionandpatternrecognition.pp.10684–10695(2022) 2,
4, 6, 12
31. Shi, Y., Liu, X., Wei, Y., Wu, Z., Zuo, W.: Retrieval-based spatially adaptive
normalization for semantic image synthesis. In: Proceedings of the IEEE/CVF
ConferenceonComputerVisionandPatternRecognition.pp.11224–11233(2022)
13
32. Song,J.,Meng,C.,Ermon,S.:Denoisingdiffusionimplicitmodels.arXivpreprint
arXiv:2010.02502 (2020) 4, 5
33. Sushko, V., Schönfeld, E., Zhang, D., Gall, J., Schiele, B., Khoreva, A.: Oasis:
onlyadversarialsupervisionforsemanticimagesynthesis.InternationalJournalof
Computer Vision 130(12), 2903–2923 (2022) 2, 13, 14, 19
34. Tan, Z., Chai, M., Chen, D., Liao, J., Chu, Q., Liu, B., Hua, G., Yu, N.: Diverse
semantic image synthesis via probability distribution modeling. In: Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp.
7962–7971 (2021) 13
35. Tan,Z.,Chen,D.,Chu,Q.,Chai,M.,Liao,J.,He,M.,Yuan,L.,Hua,G.,Yu,N.:
Efficient semantic image synthesis via class-adaptive normalization. IEEE Trans-
actions on Pattern Analysis and Machine Intelligence 44(9), 4852–4866 (2021) 4
36. Tang, H., Bai, S., Sebe, N.: Dual attention gans for semantic image synthesis.
In: Proceedings of the 28th ACM International Conference on Multimedia. pp.
1994–2002 (2020) 13
37. Tang, H., Qi, X., Sun, G., Xu, D., Sebe, N., Timofte, R., Van Gool, L.: Edge
guidedganswithcontrastivelearningforsemanticimagesynthesis.arXivpreprint
arXiv:2003.13898 (2020) 13
38. Tang,H.,Sebe,N.:Layout-to-imagetranslationwithdoublepoolinggenerativead-
versarialnetworks.IEEETransactionsonImageProcessing 30,7903–7913(2021)
1318 Gao et al.
39. Tang, H., Shao, L., Torr, P.H., Sebe, N.: Local and global gans with semantic-
aware upsampling for image generation. IEEE Transactions on Pattern Analysis
and Machine Intelligence 45(1), 768–784 (2022) 13
40. Tang, H., Sun, G., Sebe, N., Van Gool, L.: Edge guided gans with multi-scale
contrastive learning for semantic image synthesis. IEEE Transactions on Pattern
Analysis and Machine Intelligence (2023) 1, 2, 4
41. Tang, H., Torr, P.H., Sebe, N.: Multi-channel attention selection gans for guided
image-to-image translation. IEEE Transactions on Pattern Analysis and Machine
Intelligence 45(5), 6055–6071 (2022) 13
42. Tang,H.,Xu,D.,Sebe,N.,Wang,Y.,Corso,J.J.,Yan,Y.:Multi-channelattention
selectionganwithcascadedsemanticguidanceforcross-viewimagetranslation.In:
Proceedings of the IEEE/CVF conference on computer vision and pattern recog-
nition. pp. 2417–2426 (2019) 4, 13
43. Tang, H., Xu, D., Yan, Y., Torr, P.H., Sebe, N.: Local class-specific and global
image-level generative adversarial networks for semantic-guided scene generation.
In: Proceedings of the IEEE/CVF conference on computer vision and pattern
recognition. pp. 7870–7879 (2020) 4, 13
44. Voynov, A., Chu, Q., Cohen-Or, D., Aberman, K.: p+: Extended textual condi-
tioning in text-to-image generation. arXiv preprint arXiv:2303.09522 (2023) 10
45. Wang, T., Zhang, T., Zhang, B., Ouyang, H., Chen, D., Chen, Q., Wen,
F.: Pretraining is all you need for image-to-image translation. arXiv preprint
arXiv:2205.12952 (2022) 13
46. Wang, T.C., Liu, M.Y., Zhu, J.Y., Tao, A., Kautz, J., Catanzaro, B.: High-
resolution image synthesis and semantic manipulation with conditional gans. In:
Proceedings of the IEEE conference on computer vision and pattern recognition.
pp. 8798–8807 (2018) 4, 13
47. Wang, W., Bao, J., Zhou, W., Chen, D., Chen, D., Yuan, L., Li, H.: Semantic
imagesynthesisviadiffusionmodels.arXivpreprintarXiv:2207.00050(2022) 4,13
48. Wang,Y.,Qi,L.,Chen,Y.C.,Zhang,X.,Jia,J.:Imagesynthesisviasemanticcom-
position.In:ProceedingsoftheIEEE/CVFInternationalConferenceonComputer
Vision. pp. 13749–13758 (2021) 13
49. Wang, Z., Simoncelli, E.P., Bovik, A.C.: Multiscale structural similarity for im-
age quality assessment. In: The Thrity-Seventh Asilomar Conference on Signals,
Systems & Computers, 2003. vol. 2, pp. 1398–1402. Ieee (2003) 11
50. Wu, T., Si, C., Jiang, Y., Huang, Z., Liu, Z.: Freeinit: Bridging initialization gap
in video diffusion models. arXiv preprint arXiv:2312.07537 (2023) 5
51. Xiao,T.,Liu,Y.,Zhou,B.,Jiang,Y.,Sun,J.:Unifiedperceptualparsingforscene
understanding. In: Proceedings of the European conference on computer vision
(ECCV). pp. 418–434 (2018) 19
52. Xue, H., Huang, Z., Sun, Q., Song, L., Zhang, W.: Freestyle layout-to-image syn-
thesis. In: Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition. pp. 14256–14266 (2023) 4, 13
53. Yang, L., Xu, X., Kang, B., Shi, Y., Zhao, H.: Freemask: Synthetic images with
dense annotations make stronger segmentation models. Advances in Neural Infor-
mation Processing Systems 36 (2024) 2
54. Yu, F., Koltun, V., Funkhouser, T.: Dilated residual networks. In: Proceedings
of the IEEE conference on computer vision and pattern recognition. pp. 472–480
(2017) 19
55. Zhang, J., Chang, S.Y., Li, K., Forsyth, D.: Preserving image properties through
initializationsindiffusionmodels.In:ProceedingsoftheIEEE/CVFWinterCon-
ference on Applications of Computer Vision. pp. 5242–5250 (2024) 7SCP-Diff 19
56. Zhang, L., Rao, A., Agrawala, M.: Adding conditional control to text-to-image
diffusion models. In: Proceedings of the IEEE/CVF International Conference on
Computer Vision. pp. 3836–3847 (2023) 2, 5, 6, 8, 11, 12, 13, 14, 15, 19
57. Zhang, R., Isola, P., Efros, A.A., Shechtman, E., Wang, O.: The unreasonable
effectiveness of deep features as a perceptual metric. In: CVPR (2018) 11
58. Zheng, Y., Zhong, C., Li, P., Gao, H.a., Zheng, Y., Jin, B., Wang, L., Zhao, H.,
Zhou, G., Zhang, Q., et al.: Steps: Joint self-supervised nighttime image enhance-
ment and depth estimation. arXiv preprint arXiv:2302.01334 (2023) 2
59. Zhou, B., Zhao, H., Puig, X., Fidler, S., Barriuso, A., Torralba, A.: Scene parsing
throughade20kdataset.In:ProceedingsoftheIEEEconferenceoncomputervision
and pattern recognition. pp. 633–641 (2017) 2, 11, 12, 13, 14, 19
60. Zhu, Z., Xu, Z., You, A., Bai, X.: Semantically multi-modal image synthesis. In:
Proceedings of the IEEE/CVF conference on computer vision and pattern recog-
nition. pp. 5467–5476 (2020) 13
A Implementation Details
Finetuning ControlNet. We initialize the Stable Diffusion branch of Con-
trolNet [56] with SD 2.1 weights. During training, we set the text prompt to
fixed strings, and those are: City road scenes for Cityscapes [8], Photorealistic
and diverse images depicting various scenes for ADE20K [59], and high quality,
detailed for COCO-Stuff [3]. This aims to ensure that the text prompt remains
devoid of any semantic cues, with the sole source of semantics derived from the
semantic label processed by the control branch and the noise priors.
Evaluation.Duringinferenceofthegeneratedresultsfromdifferentdatasets,
the text prompt was kept the same as the training procedure. For evaluation of
FID, we sampled 50,000 images for each group of experimental setting, noting
that FID score is biased and the bias is depending on the number of images
we use for calculation [7]. For evaluation of mIoU, we follow OASIS [33], using
UperNet101 [51] for ADE20K, multi-scale DRN-D-105 [54] for Cityscapes, and
DeepLabV2 [5] for COCO-Stuff.
B Results
We provide more qualitative results, with Fig. 9 and Fig. 10 for Cityscapes,
Fig. 11 and Fig. 12 for ADE20K, Fig. 13 and Fig. 14 for COCO-Stuff.20 Gao et al.
Ground Truth OASIS ControlNet SCP-Diff (Ours)
Fig.9: Qualitative Results on Cityscapes dataset. (cont.)SCP-Diff 21
Ground Truth OASIS ControlNet SCP-Diff (Ours)
Fig.10: Qualitative Results on Cityscapes dataset. (cont.)22 Gao et al.
Ground Truth OASIS ControlNet SCP-Diff (Ours)
Fig.11: Qualitative Results on ADE20K dataset. (cont.)SCP-Diff 23
Ground Truth OASIS ControlNet SCP-Diff (Ours)
Fig.12: Qualitative Results on ADE20K dataset. (cont.)24 Gao et al.
Ground Truth OASIS ControlNet SCP-Diff (Ours)
Fig.13: Qualitative Results on COCO-Stuff dataset. (cont.)SCP-Diff 25
Ground Truth OASIS ControlNet SCP-Diff (Ours)
Fig.14: Qualitative Results on COCO-Stuff dataset.