1
Multi-Objective Optimization Using Adaptive
Distributed Reinforcement Learning
Jing Tan, Ramin Khalili and Holger Karl
Abstract‚ÄîTheIntelligentTransportationSystem(ITS)environ- The study of multi-agent reinforcement learning (MARL)
mentisknowntobedynamicanddistributed,whereparticipants algorithms for resource allocation decisions in ITS is gaining
(vehicleusers,operators,etc.)havemultiple,changingandpossi-
traction [4]. RL algorithms are known for their ability to
blyconflictingobjectives.AlthoughReinforcementLearning(RL)
learn sequential tasks without supervision and purely based
algorithms are commonly applied to optimize ITS applications
suchasresourcemanagementandoffloading,mostRLalgorithms on feedback from the environment [5]. MARL makes learning
focusonsingleobjectives.Inmanysituations,convertingamulti- strategies more effective, especially in distributed environ-
objective problem into a single-objective one is impossible, in- ments [6]. But most such algorithms assume that all players
tractableorinsufficient,makingsuchRLalgorithmsinapplicable.
have a single objective, although many real-world problems
Weproposeamulti-objective,multi-agentreinforcementlearning
are multi-objective in nature [7]. Converting a multi-objective
(MARL)algorithmwithhighlearningefficiencyandlowcompu-
tationalrequirements,whichautomaticallytriggersadaptivefew- problem into a single-objective one 1) is impossible when
shot learning in a dynamic, distributed and noisy environment utility or user preference over each objective is unknown a
withsparseanddelayedreward.WetestouralgorithminanITS priori, changing fast or incommensurate, or 2) is intractable
environment with edge cloud computing. Empirical results show
with high dimensionality or non-convexity, or 3) performs
that the algorithm is quick to adapt to new environments and
worse because a single-objective learning algorithm cannot
performs better in all individual and system metrics compared
to the state-of-the-art benchmark. Our algorithm also addresses track the development of reward on multiple objectives [8],
variouspracticalconcernswithitsmodularizedandasynchronous [9]. Few studies focus on solving multi-objective problems in
online training method. In addition to the cloud simulation, we ITS, and most of them use scalarization methods to simplify
test our algorithm on a single-board computer and show that it
into a single-objective problem [10]‚Äì[12]. On the other hand,
can make inference in 6 milliseconds.
some multi-objective learning algorithms such as [13] require
IndexTerms‚ÄîV2X,DistributedSystems,ReinforcementLearn- learning and storing many models and extensive retraining
ing, Multi-Objective
whenever user preferences change; some others such as [14]
are computationally expensive. Model-agnostic meta-learning
I. INTRODUCTION
(MAML) [15] addresses these challenges‚Äîit trains a single
Anintelligenttransportationsystem(ITS)comprisesvehicle model that is adaptable to different new tasks with few-shot
users, mobile users, edge and cloud service providers [1]. learning.However,thesealgorithmsareoftenappliedtosingle-
They all have individual objectives and private, changing agents; they are rarely studied in a decentralized, multi-agent
preferences, and they act selfishly to achieve their objectives and non-stationary environment characteristic of ITS.
by competing for limited communication and computation Inourstudy,weseeacentralizedapproachasanunrealistic
resourcesinthenetwork[2].Withadvancesincommunication option for a real-life ITS system design. Hence, we focus
and autonomous technologies, ITS is connecting more peo- our effort on a more practical approach, using independent
ple and devices, and centrally optimizing resource allocation multiagents with partial information. This approach limits
through service providers is no longer practical. information sharing and reduces communication overhead.
Such an environment is well suited to multi-agent systems More specifically, we use a parallel and distributed stochas-
(MAS). They are distributed in nature, use agents to represent tic gradient ascent method [16] for training using Federated
individual interests, and model complex interaction between Learning framework. This is a logically centralized training
players.MASreducesmodelcomplexityanddatarequirements method implemented in a distributed manner. In deployment,
by breaking down a centralized problem into local, individual we execute the algorithm distributedly (i.e., each vehicle user
problems. Such systems are naturally compatible with game- infers its decisions independently), to avoid transferring data
theoretic approaches that have similar assumptions such as from / to a centralized entity that increases overhead and la-
player independence, selfishness, and limited information- tency.Adecentralizedapproachasoursalsoavoidscomplexity
sharing [3]. in modeling and solving a centralized optimization problem
Due to the complexity and dynamicity of ITS, agents in from the operator side.
the MAS need to learn from and adapt to the environment. With our multi-objective design, vehicle users in an ITS
environment can choose and weigh their own offloading ob-
Manuscript accepted by IEEE Transactions on Intelligent Transportation
Systems;dateofcurrentversion15December2023. jectives,without sharing thatinformationto anyotherusersor
J. Tan and R. Khalili are with the Huawei Technology Munich Research theoperator.Thisalsoconformswithprivacyrequirementsthat
CenterinGermany.
might be imposed by the vehicles. In this study, we define six
H. Karl is with the Hasso Plattner Institute, University of Potsdam in
Germany. differentshortandlongterm,individualandsystemobjectives,
4202
raM
31
]GL.sc[
1v97880.3042:viXra2
and periodically sample objective weights for each user to today, vehicle and mobile users are participating in offloading
simulate their changing choice and preference of objectives. and resource allocation decisions in the network, motivated
WithourMARLalgorithm,theuserslearnanoptimaloffload- only by their individual objectives. [21], [22], [23] and [24]
ing strategy and compete for edge-cloud computing resources, assume all users are cooperative with common objectives, and
despite this frequent change. Our contributions are: ifusershavemultipleobjectives,themodelingcomplexitywill
‚Ä¢ To the best of our knowledge, we are the first ones to increase significantly.
address the multi-objective nature of ITS applications Many real-world decision-making problems consider multi-
in its distributed, non-stationary and adversarial envi- ple, sometimes contradicting objectives [7]. This is unlike a
ronment. Our multi-agent, multi-objective algorithm can single-objective problem (SOP) where the objective is scalar
optimize frequently changing combinations of objectives and totally ordered: in a multi-objective problem (MOP), the
and preferences. objectives are only partially ordered [27]. An MOP is formu-
‚Ä¢ Wetrainoneoptimalinitialmodeloffline,thendeploythe lated as finding decision variables that lead to solutions on the
model to each independent agent representing a vehicle Pareto frontier: ùëì(x) = (ùëì 1(x),¬∑¬∑¬∑ , ùëì ùëô(x)) s.t. x ‚àà K ‚äÜ Rùëõ ,
user, who is able to change its private objectives and where ùëì is a vector of ùëô objective functions, x is the decision
update its offloading strategy through online few-shot variable, and K is the feasible region in an ùëõ-dimensional
learning, needing low retraining cost and no prior knowl- decision variable space. Since ùëì can only be partially ordered,
edge for reward shaping. Our solution outperforms the we use the Pareto frontier to represent a set of equivalent
benchmarkingstate-of-the-artalgorithmsonallindividual solutions: for a solution on that frontier, no objective can be
and system metrics. Also, in a heterogeneous environ- improved without at least one other objective being worsened.
ment with different competing algorithms, our algorithm Some approaches to solving MOPs extend equilibria con-
increases bottom-line resource efficiency, such that other cepts to multi-objective settings [28]‚Äì[31]. They all assume
algorithmsintheenvironmentalsobenefitfromimproved some degree of cooperation and communication between
offloading rate and fairness. agents;theythereforedifferfromourcompetitiveenvironment
‚Ä¢ Our algorithm can be modularized and trained asyn- setting where agents do not share information (Sec. III-A).
chronously. We test the runtime inference performance Reference [32] assumes a stationary environment, which is
of our algorithm on a single-board computer with a GPU different from our multi-state MDP and dynamic environment
and show that inference in 6 milliseconds is feasible. setting. Reference [33] assumes complete information that
‚Ä¢ We provide public access to our code and data at [16]. is different from our partial information assumption. Other
approaches try to find discrete solutions on a Pareto frontier
Sec. II reviews existing studies on RL for ITS applications
in stationary environments through objective selection [34] or
such as offloading and resource allocation and multi-objective
decomposition [35]‚Äì[37]. However, in a dynamic environment
RL approaches; Sec. III describes our generic MAS modeled
like ours, they do not meet the challenge of huge state and
as an auction and formulates the multi-objective optimization
action space, unknown state distributions, and MDP with
problemaccordingly;Sec.IVintroducesouralgorithm;Sec.V
continuing tasks.
analyzes simulation results and Sec. VI discusses practical
Reinforcement learning (RL) is commonly used to explore
concerns.
huge state and action space, but most RL algorithms only
solve SOP [5]. The goal of a single-objective RL algorithm
II. PRELIMINARIES&RELATEDWORK is to maximize the return ùêΩ =E[(cid:205)T ùë°=0ùëü ùë°], where T is the time
RL algorithms are increasingly used to optimize perfor- horizon and ùëü ùë° is a scalar-valued reward at time ùë°, expecta-
mance in ITS applications such as offloading [17] [18] and tion is taken over random rewards. In a multi-objective RL
resource allocation [19] [20]. Due to the distributed nature algorithm, we have a vector-valued return for |ùëÇ| objectives:
of the ITS environment, game-theoretic approaches [21] [22] J= {ùêΩ (ùëú)|ùëú = {1,...,ùëÇ}} ‚ààR|ùëÇ|,thereturnforeachobjective
and multi-agent systems [23] [24] combined with RL are also ùëú is ùêΩ (ùëú) =E[(cid:205) ùë°ùëü ùëú,ùë°].Owingtopartialorderofrewards,such
common approaches in the recent years. Some studies such asituationisnotdirectlyamenabletostandardRLtechniques;
as [25], [26] and [24] consider the multi-objective nature of either it needs to be simplified into an SOP of finding only
ITS applications, they either decompose the objectives into one of many equivalent optimal solutions (using a constant
subproblems[25]orconverttheproblemintoasingle-objective weight vector W ‚àà R|ùëÇ| to form a single reward ùêΩ = WùëáJ),
one through scalarization [26] [24]. or the Pareto frontier of all optimal solutions needs to be
All of these studies simplify the ITS environment in some characterized explicitly.
regards: [18] and [20] consider only one single objective; EvenifanMOPcanbesimplifiedintoanSOP,reference[9]
[19], [26] and [20] require complete information to centrally points out that such simplification requires much theoretical
solvetheoptimizationproblem,butinadynamicenvironment, knowledge for reward engineering and manual tuning when
acquiring enough information for a centralized approach is objective preferences change over time; a scalarized reward is
often not feasible or violating user‚Äôs privacy; [17] and [25] alsoinexplainable,i.e.,thescalarizationdoesnotalwaysreflect
consider only system objectives, not user objectives, and it is therealrelationshipbetweendecisionvariablesandobjectives.
assumed that users do not make offloading and resource allo- Such methods are therefore sensitive to preference changes.
cationdecisions‚Äìthisassumptionmaynotapplytothehighly Instead,multi-objectiveRLalgorithmsaimatexplicitlyfind-
individual and customized environment of ITS, where even ingtheParetofrontier.Suchalgorithmscanbecategorizedinto3
Remote computing sites
Bidderm
server server DB with budget B Auctioneer Commodity seller
Commodity
sellers internet Edge computing n foe rw c ore mq mue os dt i ti y k p ofr fe v reio qu us el sy t sbacked
sites server with value Vm,k previously rejected u exp ed ca ute te a rv ea qil ua eb sil ti sty,
ACA & bidp da is nt g e rn ev s. uin ltfo s requests (before
Auctioneer a ad nm
d
ais ss si io gn
n
mco en nt trol
infer from learning
d me oad dl ein l:e) a fuv la fii ll la mb eil nit ty ,
l
ec vo es lt ,,
etc.
a :back off or join the auction
b:bidding price
Bidders
backed off requests submit bid with price b
& backoff cost q
Figure 1: Vehicles request for services from the ACA in determine winner
& pass request to sellers
range through bidding. The ACA is connected to computing short-term feedback: request details
-bidding result z
sites both on the edge and in the cloud. The ACA serves as -cost of losing bid c
-payment info p queue incoming requests
auctioneer, and the computing sites are sellers of resources.
long-term, sparse
reward signal:
multiple-model and single-model methods. Multiple-model new env. & feedback -fairness
info for learning -winning rate, etc.
methods result in multiple, independent models, each aiming
train learning model
to optimize one point on the Pareto frontier, making learning
Figure 2: Bidders join repeated auctions with one auctioneer
and inference inefficient. They often have high computational and multiple commodity sellers. A bidder ùëö decides to join
cost in high-dimensional objective spaces and are inflexible with bid ùëñ or back off with cost ùëû, based on past info and
in a dynamic environment [13]. Single-model methods such
current observations. The auctioneer determines the winner,
as in [14] train only one model for all solutions to the MOP, sends back bidding results ùëß and ùëê, payment ùëù and rewards.
butitiscomputationallyexpensive.MAML[15]combinesthe
Commodity sellers execute requests passed on by the auction-
two methods: multiple models are trained for their specific
eer. Only the bidders can learn.
objectives and combined into a generic model, which can be
quicklyretrainedforanynewobjectivewithonlyafewsample requests on their own onboard units, they try to offload these
data points (‚Äúshots‚Äù). References [38] and [39] extend the servicestoedge-cloudcomputingsitesthroughroad-sideunits
methodin[15]fromsingle-agenttomulti-agent,buttheformer equippedwithmulti-accessedgecomputingdevices.Theroad-
considers a stationary environment, and the latter formulates sideunitsareresponsibleforadmissioncontrolandassignment
a non-stationary SOP as a stationary MOP. To the best of our (ACA) of service requests.
knowledge, there have not been studies of multi-agent non- Among classic decentralized decision-making mechanisms,
stationary environments with multiple objectives. Although we use an auction mechanism because it is most suitable in
[15]providesaframeworkfortwo-phasemulti-tasktraining,it a dynamic and competitive environment, where the number of
doesnotsuggestachoiceofthemodel,asitdoesnotconsider bidders and their preferences vary over time, or the bidders‚Äô
any specific problem or application; due to the complexity of private valuations of the same commodity are very differ-
the approach, real-life implementation of their method in a ent [40]. Such conditions are typical of V2X environments.
dynamicenvironmentsuchasITSisproblematic.Inourstudy, Specifically, we set up the system as an auction with multiple
we design our own multi-agent algorithm for a distributed, vehicles as bidders, one ACA as the auctioneer, and multiple
dynamic environment with sparse and delayed rewards that is computingsitesascommoditysellers.Adiagramofthemech-
capable of automatically triggering adaptive online retraining anism is in Fig. 2. The bidders do not share information with
(Sec. IV). We also apply various performance improvement other bidders or commodity sellers; they only communicate
measures (Sec. VI) to address practical concerns, making our with the auctioneer. Our study focuses on the behavior of the
model more suitable for real-life implementation. independent bidders, conceived of as agents. Each bidder has
multiple objectives to achieve in the auction. As in real life,
III. SYSTEMMODELANDPROBLEMFORMULATION the bidder‚Äôs preference of objectives changes over time, and
it needs to learn the Pareto frontier of the MOP to respond
In Sec. III-A, we introduce our system model for ITS
quickly to changes [41]. We introduce each component of the
computation resource allocation in edge cloud and show how
system below. Table I summarizes the notation.
it can be described as a repeated auction. In Sec. III-B, we
1) Commodities: Commodities are typically products or
formally define the decentralized optimization problem in an
services. In V2X, the commodities are the service slots on
auction mechanism. In Sec. III-C, we describe the multi-agent
the edge cloud for computation offloading. Let ùêæ be the set
system that simulates the mechanism and the interaction.
of commodity types. To be executed, each type ùëò ‚àà ùêæ has
its own specification and resource needs (in terms of material
A. System model and time). Let ùëÄ be the set of bidders. Over time, a bidder
Our system is designed as an abstraction of the classic ùëö ‚àà ùëÄ getsrequestsfor oneormultipletypesof commodities
edge-cloud computing architecture. An example topology is and tries to offload and fulfill the request within the specified
in Fig. 1. Vehicles get service requests such as image segmen- deadline. All instances of commodities of the same type are
tation and motion planning in self-driving applications, each equivalent. At time ùë°, the bidding price for type ùëò is denoted
request with its own quality-of-service (QoS) requirements ùëèùë° . Each type ùëò has a total of ùëõùë° available service slots in
ùëö,ùëò ùëò
such as deadline. When vehicles cannot process all of the computingsites.Maximumcommodityavailabilityisfixedper4
Table I: Problem formulation (e.g. set by the commodity sellers according to some load-
balancing heuristics). Each bid can only be assigned to one
Sym Description Sym Description
seller. If no seller can fulfill the request, the bids are rejected.
ùëò ‚ààùêæ commoditytype ùëõ ùëò ùëò‚Äôsavailability
ùëñ ‚àà ùêº bid/request ùëö‚àà ùëÄ bidder Forarejectedbid,thebiddercanrebidbyanumberoftimes.
ùêµ budget ùë£ valuation The auctioneer sets the maximum permitted rebidding times
ùõº backoffdecision ùëè biddingprice to balance between low bidding failure rate and additional
ùëê costoflosingthebid ùëû backoffcost biddingoverheadtothesystem(e.g. communicationoverhead
ùëù payment ùëß biddingoutcome
in V2X). For simplicity, we allow rebidding once, the same
ùë¢ auctionutility ùõΩ resourceutilization
ùëü reward W preferencevector as in [40]. If the bid is accepted by the auctioneer, but not
ùëú‚ààùëÇ objective ùúã biddingstrategy executedbythecommoditysellerwithinitsdeadline,theseller
drops the bid and informs the auctioneer and the bidder. Both
type and is not interchangeable between types.
rejected and dropped bids are considered failures.
2) Bidders: A bidder (in V2X, a vehicle) ùëö has private, After the auction round, the auctioneer sends the bidders
time-invariant valuation ùë£ ùëö,ùëò (i.e. the benefit it derives from the bidding outcome, payment, and reward signals for system
winningthecommodity,orinV2X,fromsuccessfullyoffload- objective achievements, e.g. resource utilization, fairness, etc.
ing its task) for each type ùëò and an initial wealth of ùêµ0 ùëö. At The bidder also calculates rewards related to its individual
time ùë°, ùëö can submit its bid denoted ùëñùë° ùëò ‚àà ùêº to the auctioneer, objectives, such as bidding failure rate. It can decide whether
it contains the service request and the bidding price
ùëèùë°
ùëö,ùëò. Its to use auctioneer‚Äôs reward signals for learning. For example,
direct payoff from the auction is ùë£ ùëö,ùëò minus its payment to onebiddermayhavelowpreferenceforsystemobjectivesand
the auctioneer
ùëùùë°
ùëò, if it wins the bid, 0 if it loses. Note that ignore the information; another one may find the information
the payment to the auctioneer maybe be the same or lower useful.Theauctioneerhastheobjectiveùëú tomaximizeoverall
than the bidding price (ùëùùë° ùëò ‚â§ ùëèùë° ùëö,ùëò), depending on the auction fairness among bidders. 3
type. The bidder‚Äôs first objective ùëú is to maximize average
1 The commodity sellers dynamically adjust their commodity
utility: the payoff minus additional costs due to losing a bid
prices. Since our study focuses on the behavior of the bidders,
or having to rebid later. In Sec. III-C, we further break down
we make the sellers passive (i.e., not learning-capable) and
ùëú into three sub-objectives.
1 use a simplified pricing heuristic with load-balancing effect:
Instead of bidding at time ùë°, ùëö also has the option to back a seller with higher percentage of unsold resources sells at a
off (i.e. delay its bid), hoping for less competition for the lower price; the requests are assigned to the seller with the
commodity in the future, but, on the other hand, using up lowest price, until all sellers have similar utilization and price.
time towards the fixed deadline and thus making the bid more The commodity sellers have the objective ùëú to maximize
4
urgent. If the bid passes the deadline, it is viewed as lost (i.e. system utilization and minimize variance in utilization. With
0 payoff with cost of losing the bid). Specifically, 1) bidders low variance, commodity sellers can better plan long-term
are incentivized to balance between backoff and immediate resource availability, reaching high utilization especially in
bidding; 2) backoff time and bidding price can be learned high contention, saving cost while keeping the same service
rather than randomly chosen; 3) learning is only based on level.
information visible to the bidder. Wecallùëú andùëú abidder‚Äôsindividualobjectives,ùëú andùëú
1 2 3 4
Besides maximizing utility, the bidder‚Äôs second individual thesystemobjectives.Althoughtheauctioneerandcommodity
objective ùëú 2 is to minimize its long-term offloading failure sellers cannot force the bidders to consider system objectives,
rate. The bidder can also be incentivized to consider system inSec.V-Bweshowthattherewardsignalshelpbidderslearn
objectives,aswouldbedetailizedinthefollowingsubsections. the correlation between individual and system objectives, and
Each bidder‚Äôs preference over all objectives is private and by considering system objectives, the bidders effectively earn
expressed through a non-negative preference vector Wùëö = higher reward on their individual objectives.
{ùëä ùëöùëú|‚àÄùëú ‚ààùëÇ}thatcanchangefrequentlyovertime.Thebidder
To make our system resemble the fast-paced edge-cloud
independentlylearnsabiddingstrategytomaximizeitsreward computing architecture in real life, we add transmission delay
from objective achievements weighted by its preference. We between bidders, the auctioneer and commodity sellers, and
study different learning algorithms in each bidder. randomize resource requirement, queuing time and processing
3) Auctioneer and commodity sellers: In our system, we time for request execution. Each bidder learns its optimal
have one auctioneer that determines the winners of auctions. bidding strategy despite noisy state information in such a
In V2X, the auctioneer is the road-side unit with multi- dynamic environment. Sec. V describes the simulated V2X
access edge computing device (MEC), also called the ACA, environmentanditsexampleself-drivingapplicationsindetail.
becauseitcontrolsadmissionofservicerequestsfromvehicles
and assigns admitted requests to edge-cloud computing sites
B. Problem formulation
(i.e. commodity sellers). It determines the winner through an
auction mechanism: service requests with the highest bidding We now formulate the distributed decision making problem
prices are prioritized. The auction is repeated in each discrete related to the system model in Sec. III-A as an auction for
time step ùë°, if there are active bidders and commodity is multiple commodities. From its bidding strategy ùúã ùëö, bidder ùëö
available (ùëõùë° > 0). The auctioneer passes winning bidders‚Äô draws its actions ùõºùë° = {ùõºùë° ‚àà {0,1}} and bùë° = {ùëèùë° ‚ààR }
ùëò ùëö ùëö,ùëò ùëö ùëö,ùëò +
requeststothecommoditysellerswiththelowestsellingprice for each service type. ùõº is the vector of backoff decisions, b5
is the vector of bidding prices. More specifically, bidder ùëö‚Äôs eveniftheirindividualoutcomemaybehurt.Bidderùëödecides
optionsforeachbidare:1)backoff(ùõº ùëöùë° ,ùëò =0)withabackoff whether it backs off for the current auction round (ùõº ùëö,ùëò = 0)
cost ùëûùë° ùëö,ùëò, or 2) bid (ùõº ùëöùë°
,ùëò
= 1) with price ùëèùë° ùëö,ùëò. To avoid or bids with price ùëè ùëö,ùëò. The auctioneer receives only the
overbidding, at any time ùë°, (cid:205) ùëòùõº ùëöùë° ,ùëòùëèùë°
ùëö,ùëò
‚â§ ùêµùë° ùëö. bidders‚Äô required commodity type ùëò and bidding price; at the
From bidder ùëö‚Äôs perspective, the competing bidders (de- end of each auction round, the auctioneer sends back to ùëö the
noted ‚àíùëö) draw their actions from a joint strategy distribution bidding outcome ùëß ùëö,ùëò, the final price (i.e., required payment
ùúãùë°
‚àíùëö
that is an unknown function of (p1,¬∑¬∑¬∑ ,pùë°‚àí1), where from winning bidders) ùëù ùëò, as well as system rewards such as
pùë° ‚àà R|ùêæ| is the vector of final prices at the end of time ùë°. fairness score and resource utilization ùõΩ if they are available.
+
All bidders get the vector of commodity prices for time ùë°, Among different types of auctions, we choose to use the
denoted pùë° , as feedback from the auctioneer. If bidder ùëö wins second-priceauction.Becausesecond-priceauctionsmaximize
itsbidùëñùë° ùëò indicatedbybiddingoutcome ùëßùë° ùëö,ùëò =1,itpays ùëùùë° ùëò to social welfare (i.e., total utility of all bidders) instead of
the auctioneer. If the bidder loses (i.e. ùëßùë° ùëö,ùëò =0), it pays 0 to auctioneer profit, it is commonly used in auctions for public
the auctioneer, but it also has a cost associated with losing the goods. For ùëõ ùëò =1, the required payment ùëù ùëò is the price of the
bid, denoted by
ùëêùë°
ùëö,ùëò. If rebidding is permitted and
ùëñùë°
ùëò has not second highest bid (hence the name ‚Äúsecond-price auction‚Äù);
reacheditsdeadline, ùëö repeatsthedecision-makingprocessin forùëõ ùëò >1availablecommodities,thiswouldbetheùëõt ùëòh highest
ùë°+1.Ifùëñùë° ùëò passesthedeadlinebeforeitisadmitted,itisviewed bid, denoted ùëè‚àó ùëò. For the winning bidders with the highest ùëõ ùëò
as a lost bid with cost ùëêùë° ùëö,ùëò. bids (ties are randomly broken), ùëß ùëö,ùëò =1 and ùëù ùëò = ùëè‚àó ùëò.
The auction repeats for T rounds, in every auction round, Ineachauctionround,bidderùëö hasobjectiveùëú :maximize
ab did dd ee dr tùëö o‚Äô ts heut wil eit ay lthis pùë¢ oùë° ùëö ol( :ùõº ùêµùëöùë° ùë° ùëö, +b 1ùë° ùëö =,p ùêµùë° ùë° ùëö,z +ùë° ùëö ùë¢, ùë° ùëöcùë° ùëö ., Ifqùë° ùëö ùêµ) ùë° ùëö, +1th ‚â§e 0u ,ti bli it dy deis
r
i dm owm nedi ia nt te
o
a tu hc reti eon suu bti -l oit by jeùëü cùëö tùëú i1 ve= s.ùë¢ 1ùëö ). ùëúObje :ct miv ae xiùëú m1 1 izis
e
b pr ao yk oe fn
f
ùëö Nlo es xe ts ,wal el t fh oe rmu un lfi an teis th he ed pb roid bs lew mit wh ic thos mt uùëê lùë° ùëö ti, pùëò lean od bjeis ctr ie vs ee st. ùëú ‚àà ùëü ùëö(ùëò,ùëú 1‚àí1) = ùõº ùëö,ùëò ¬∑ ùëß ùëö,ùëò ¬∑ (ùë£ ùëö,ùëò ‚àí ùëè‚àó ùëò). 21‚àí )1 ùëú 1‚àí2: minimize the
chanceofbeingrejectedbytheauctioneer.Thecostofbidding
ùëÇ
rùë°
as ‚ààd Res |ùëÇcr |ib ined rain ndS oe mc.I iI nI t- eA rv. aB lsid fd re or mùëö ir tsec oe wiv nes oa bsre ew rva ar td iov nec at no dr and then losing the bid is ùëü ùëö(ùëò,ùëú 1‚àí2) =‚àíùõº ùëö,ùëò¬∑(1‚àíùëß ùëö,ùëò)¬∑ùëê ùëö,ùëò.
ùëö 3) ùëú : minimize backoff time. If backed off, ùëö has cost
1‚àí3
the feedback signals from the auctioneer for its achievement ùëü ùëö(ùëò,ùëú 3) =‚àí(1‚àíùõº ùëö,ùëò)¬∑ùëû ùëö,ùëò.
of these objectives. More details will be provided in the next
s
W
lie fc
eùë°
ùëöt ,io
‚àà
cn hR. anE
|ùëÇ
ga e|c
.
sh
B
ib nidid
d
pd
e
re err f‚Äô
p
es
r re
ep
f
nr
e
ce
r
ef ee nr cce aen nsce
bc
eav ne dc
c
rt iho var
en
no gv
e
be yr ovt lh oee
r
ngto
i
-b
m
teje
e
rmc .t Ii nv se hrs
ie
fai ts
sl
ùë¢ ùëö
ùëü
ùëöùëú,ùëò
1
==ùëü ùë¢ùëö( ùëöùëò,ùëú =1‚àí ‚àëÔ∏Å1) + ùë¢ùëä ùëö,ùëöùëú ùëò1‚àí2ùëü ùëö(ùëò,ùëú 1‚àí2) +ùëä ùëöùëú 1‚àí3ùëü ùëö(ùëò,ùëú 1‚àí3)
(1)
in societal, legal and personal attitudes, or short-term private
ùëò‚ààùêº
prioritization, etc. The bidder‚Äôs goal is to maximize expected The cost terms ùëê ùëö,ùëò and ùëû ùëö,ùëò with preferences ùëä ùëöùëú 1‚àí2 and
vre at ru iarn ntJ aùëö nd= unT1 k(cid:205) noT ùë°= w1 n(W toùë° ùëö t) hùëá e¬∑ br iùë° dùëö d, eT r‚Üí in a‚àû dv, aw nh cee .re Wùë° ùëö is time- ùëä bidùëöùëú d1‚àí i3 ngq .u Ia nnt oif uy r it mra pd le eo mff enb te at tw ioe nen (Sl eo cn .g Vb )a ,c ùëêk ùëöo ,f ùëòf =tim ùë£ ùëöe ,ùëòa ,nd ùëû ùëör ,i ùëòsk iy s
Typical RL techniques learn to maximize reward with a reciprocal to the time-to-deadline, and non-negative weights
constantpreferencevectoroverthemultipleobjectives.Thisis ùëä ùëöùëú 1‚àí2 + ùëä ùëöùëú 1‚àí3 = 1. Sec. V-C shows our algorithm is not
essentially one single point on the Pareto frontier of the MOP. sensitive to changes in the hyperparameters ùë£ ùëö,ùëò and ùëû ùëö,ùëò.
Our approach in Sec. IV finds the shape of the Pareto frontier, Bidderùëö‚Äôslong-termindividualobjectiveùëú istominimize
2
befitting the V2X environment where vehicles have time- ùëü ùëöùëú 2 = OFRùëö ‚àà (0,1) at long intervals with preference ùëä ùëöùëú 2.
variant preference vectors Wùë° ùëö that is unknown in advance. Long-termobjectivesareonlyavailabletoùëö attheendofeach
InthefollowingSec.III-C,webuildaMAStosimulatethe interval. The short-term system objective ùëú of maximizing
3
auction mechanism and bidders with multiple objectives. resourceutilizationùëü ùëöùëú 3 = ùõΩandthelong-termsystemobjective
ùëú
4
of maximizing fairness ùëü ùëöùëú 4 =Fairness are the same for all
bidders and broadcasted to all. Bidders do not have to respect
C. Multi-agent system (MAS) for the auction mechanism
the system objectives; their preferences are reflected in the
We design a MAS where each auctioneer and commodity valuesùëä ùëöùëú 3 andùëä ùëöùëú 4.Apreferenceof0meansthebidderdoes
seller is represented by a passive agent. Each bidder ùëö is one not consider system objectives at all. The definitions of OFR,
active (i.e., learning-capable) agent. Other bidders are denoted ùõΩ and Fairness for our implementation are in Sec. V-A.
‚àíùëö. Thus, the agents in our MAS represent actual entities in
The reward for objective achievement in each time step is:
V2X (Fig. 1).
secF to ior ns .im Fop rlic ei aty c, hw ce omom mi ot dt ih tye n tyo pta etio ùëòn
,
tf ho er t bim ide des rte ip sùë° gii vn et nhi as ùëü ùëí,ùëö =ùëä ùëöùëú 1 ¬∑ùëü ùëöùëú 1 +ùëä ùëöùëú 2 ¬∑ùëü ùëöùëú 2 +ùëä ùëöùëú 3 ¬∑ùëü ùëöùëú 3 +ùëä ùëöùëú 4 ¬∑ùëü ùëöùëú 4 (2)
privatevaluationùë£ ùëö,ùëò thatis1)lineartothebidder‚Äôsestimated The notation ùëü ùëí,ùëö is for the scalarized extrinsic reward
(Sec. IV-A) specific for the bidder ùëö with preference vector
resource needs for the commodity and 2) within its initial
wealth ùêµ0. [40] proves the first condition guarantees Pareto Wùë° ùëö at time ùë°. We let ùëä ùëöùëú 1 +ùëä ùëöùëú 3 = 1 to balance the short-
optimalityùëö
in the case the bidder has a constant preference
term objectives, and ùëä ùëöùëú 2 +ùëä ùëöùëú 4 = 1 to balance the long-term
objectives.
vector over all objectives, and the second condition avoids
overbidding under rationality. We do not consider irrational or Next,weproposeanalgorithmthatlearnstomaximizeùëü ùëí,ùëö
malicious bidders, e.g., whose goal is to reduce social welfare over time, with changing
Wùë°
.
ùëö6
Table II: Proposed solution
Innerloop training phase
Bidder Sym Description Sym Description
Outerloop training phase
Auctioneer
SIn Od Pi v mid ou da el l IndU ivn idif uie ad
l
gp ra ar da im ens
ts
co Gor ed ni en ra icto Mr a Og Pen t ùõæ ùê¥ùúÉ m d acio s ticd ooe nul np ta rr aa tm eeters ùëÜŒì ùõø l T se tD aa tr en ei rn rg orrate
Bidder model ùëâ statevalue ùúã targetpolicy
Individual J scalarizedreturn ùúè inner-looptrainingshots
SOP model ùëüùëí extrinsicreward ùëüùëñ intrinsicreward
ùúÅ bestresponsestrategy ùúì behaviorstrategy
Figure 3: Two-phase offline training ùúÇ bestresponseweight ùêø ùëì statepredictionloss
ùúô statefeatures ùúñ creditweightforactions
IV. PROPOSEDSOLUTION
method [42] using fully distributed, asynchronous federated
Tosolvethelong-term,multi-objectiverewardmaximization learning to increase learning efficiency. 3) We propose an
problem, we propose MOODY: Multi-Objective Optimization adaptive online retraining method that continuously predicts
through Distributed reinforcement learning with delaYed re- long-term reward; a decreasing prediction accuracy triggers a
ward. short, few-shot online retraining cycle. Our model is therefore
Thealgorithm‚Äôsmulti-objectivedesignallowsuserstoweigh more adaptive to changing environment and objectives com-
their own objectives without information sharing with the pared to [15], which only retrains the model at the beginning
operator. of deployment.
In this study, we define six different short and long term, The two-phase training cycle takes place offline with
individual and system objectives, and periodically sample gradient-sharing between the generic model and the local,
objective weights for each user to simulate their changing single models. Otherwise, observation data, hyperparameters
choice and preference of objectives. If the sampled preference for initialization and objective preferences remain private to
weight is 0, the user does not choose that objective. the local agents. Once the training cycle is over, we reset the
Ourtechniquecomprisestwoparts:theofflinetrainingcycle simulationenvironmenttohavealllocalagentsinitializedwith
and the online inference/retrain cycles. In the offline training theextensivelytrainedgenericmodel,thentestthemforonline
cycle, the approach is further split into two phases: the inner- inference and retraining without further parameter sharing, in
loop training phase and the outer-loop training phase. Authors a realistic test environment.
of [15] demonstrated that such a two-phase training method Section IV-A introduces the inner-loop RL algorithm. Sec-
createsainitialgenericMOPmodelthatcanbeeasilyretrained tion IV-B describes the parallel stochastic gradient ascent
online for a different task. method using asynchronous federated learning in the outer-
In the inner-loop training phase, a local agent trains with loop offline training phase. Section IV-C introduces our adap-
a uniform randomly sampled, constant preference vector and tiveretrainingmethodintheonlineretrainingcycle.Notations
tries to find an optimal solution on its Pareto frontier for the are in Table II.
given preference vector.
In the outer-loop training phase, one coordinator agent
A. RL in the inner loop
combines all results from inner-loop trainings. The inner-loop
and outer-loop training happens alternatively (Fig. 3). At the In the inner loop, each bidder (local agent) learns au-
endofthetrainingcycle,wehaveaninitialgenericmodelthat, tonomously to maximize its reward. The inner-loop algorithm
given any new preference vector, can infer an action which is based on our previous work [43], we change it to suit our
leads to a set of rewards that is close to the Pareto frontier. multi-objectiveproblem,suchthatitcannowlearntooptimize
In our case, the individual long-term objective of OFR multiple short and long-term objectives with a preference vec-
and the system long-term objective fairness are positively tor Wùë° ùëö ‚àà (0,1)|ùëÇ| that changes over time (in our simulation,
correlated to each other (see Sec. V-B), therefore all users‚Äô it is drawn from a uniform distribution at random interval).
optimal solutions regardless of their preference vectors would Statevector ùëÜùë° intheinnerloopconsistsof:1)information
ùëö
have similar (maximum) rewards in the two objectives. In of ùëö‚Äôs bids (in our simulated V2X scenario in Sec. III-A,
Sec. V-C, we show with results from the inference/retrain this includes the type of service request, its deadline, resource
cycle how the rewards for these objectives are not sensitive amount required, etc.); 2) limited environment information
to vehicle users‚Äô different and changing preferences. Hence, ùëö gets from the auctioneer, e.g., number of bidders in the
although each user makes independent decisions based on network, system utilization, etc.; 3) other private bidder con-
private objectives and preferences, their collective decision- ditions such as previous wealth ùêµùë°‚àí1; 4) previous competitor
ùëö
making results in a better solution for all. state ùëÉùë°‚àí1, represented by previous payments: ùëÉùë°‚àí1 = pùë°‚àí1 =
‚àíùëö ‚àíùëö
Based on the two-phase training framework of [15], we {ùëùùë°‚àí1|ùëò ‚àà ùêæ}; 5) previous extrinsic reward ùëüùë°‚àí1 as defined
ùëò ùëí,ùëö
makeseveralimprovements:1)wedesignaspecificinner-loop in Eq.2. We specifically limit the model input to information
algorithm for our multi-agent application scenario, it outper- that is easily obtainable by the bidder. This meets real-life
forms classic RL algorithms such as actor-critic in dynamic requirements for limited information-sharing between bidders.
environments with sparse and delayed rewards. 2) In the outer Through feature extraction layers, we get feature vector
ùúôùë°
ùëö
loop, we implement the parallel stochastic gradient ascent from stacked state vectors from the memory as input. Output
‚Ä¶7
Algorithm 1 Offline innerloop training of local agent ùëö
Decoder
Attention 1: Initializeùëá =0, ùêµ0 ùëö, vùëö, Œìùëö, ùõæ ùëö, ùúÇ ùëö, ùúè.
Encoder Attention vectorœµ 2: while true do
Credit Assignment ùúô ùëö ùëü ùëí,ùëö ùëü ùëí,ùëöùëö 3 4: : ùë° R‚Üê eceùëá ive+1 ùúÉ, 0r fe rc oe mive con oe rw dinp ar te of rer ae gn ec ne ts ,W iniùë° ùëö tiai lf iza ev ùúÉai ùëölab =le ùúÉ. 0.
Int. reward ùëü ùëñ,ùëö 5: while ùë° ‚â§ùëá+ùúè do
Ext. reward ùëü ùëí,ùëö 6: Observe and remember:
7: Get new service request and add to pipeline at time ùë°.
Actor-critic: Œ∂ ùëö Œ∑ Action Lossùêø f 8: Observe environment variables and past payments.
ùúô ùëö Sb Le :s
s
t
b t
rr
e
ae
h
ts eap gvo yin os re
al ùúì ùëö1-Œ∑
ùê¥ ùëö ME emnv o.
ry exF te ra at cu tir oe n ùúô ùëöPr med oi dct ei lon 1
19
0
1:
: :
R
C
Ine
r
fet er
a
ri te
e
ùúôv ùë°e
ùëösta
,d
t
ùêøe et
ùë°
ùëìa
v
ùëöi els
c
fto
ro
of
r
ma ùëÜl ùë° ùëöl cure raq
in
ou
d
se ias tyt ds .din toc mur er men ot rp y.ipeline.
Actor-Critic with FF iS gP u r( eB 4en :c Ih nnm ea rr lk o) op RL Curiosity 1 12 3: : I Wnf ie thr ùúñ ùúñùëö ùëöùë° ùë° ,f uro pm datc ere bd ai ct ka wss ai rg dn sm pe an st t. ùëü ùëñ,ùëö‚Äôs in memory.
14: Take action:
from each model is the bidding strategy for time step ùë°, 15: Infer actions ùõº ùëöùë° ,bùë° ùëö from actor-critic RL with FSP.
including backoff decision ùõº ùëöùë° and bidding price bùë° ùëö. 1 16 7:
:
Co Clle ac lct ua ll al tebi bd as cw koit fh
f
cb oa sc tk qof ùë° ùëöf ,d ue pc dis ai to en ùëüùõº ùëíùë° ,ùëö=0 in:
memory.
The local, single models have the same structure as the 18: Add those before deadline to pipeline at ùë°+1.
ùë°
genericmodel.Itconsistsofthreeparts(Fig.4):1)afictitious 19: Drop the rest as lost bids with penalty cùëö.
self-play (FSP) module [44], including an RL with actor-critic 20: Submit bids with ùõº=1, with prices bùë° ùëö.
21: Collect rewards:
andasupervisedlearning(SL)part,2)acuriosity-learning[45] ùë° ùë°
22: Observe bidding results zùëö and payments p .
module, and 3) a credit-assignment module. 23: Collect all lost bids with ùëß=0:
With the FSP method, an RL learns a bidding strategy that 24: Calculate penalty cùë° ùëö, updateùëü ùëíùë° ,ùëö in memory.
is the best response to other bidders‚Äô actions; parallel to the 25: Collect requests before deadline and can rebid:
26: Add those to pipeline at ùë°+1; drop the rest.
RL, the SL learns a behavioral strategy from the bidder‚Äôs own 27: Collect all won bids with ùëß=1:
past bidding behaviors, disregarding current competitor state, 28: Calculate auction utility, updateùëü ùëíùë° ,ùëö in memory.
and the final bidding decision is selected between the best- 29: Update ùêµùë° ùëö.
response and the behavioral strategy with a factor ùúÇ ‚àà (0,1) 30: Get other ext. rewards, updateùëü ùëíùë° ,ùëö in memory.
that increases over time. This stabilizes learning in a dynamic 31: Calculate and addùëü ùëñùë° ,ùëö in memory.
32: Update learning model:
environment and improves the overall convergence property
33: Train actor-critic RL with FSP and curiosity.
[44]. In Sec. V-B, we use a stand-alone actor-critic (AC) with
34: Train credit assignment if long-term reward available.
F [4S 5P ]a es xto rn ace tsof st th ate eb fe en ac tuh rm esar tk haa tlg do ir ri et ch tm lys. inT flh ue ec nu cr eio as git ey ntm ao cd tiu ol ne 3 35 6:
: ùë°
‚ÜêUp ùë°d +a 1te ùúÉ ùëö with gradient ‚àáùúÉ ùëöùë° Jùëö=ùõø ùëö‚àáùúÉùëölnùúã ùëö.
anddisregardslessusefulstateinformation,thusimprovingthe 37: end while
model‚Äôs ability to generalize. It predicts next state and inserts
38: Pass ‚àáùúÉ ùëöùúèJùëö to coordinator agent.
the prediction loss as intrinsic reward
ùëüùë°
between sparse
39: ùëá ‚Üêùëá+ùúè.
ùëñ,ùëö 40: end while
extrinsic rewards
ùëüùë°
to encourage exploration in unfamiliar
ùëí,ùëö
state space. Finally, the credit assignment module predicts and if they are available. ùëö‚Äôs curiosity module predicts next state
breaksdownlong-term,delayedrewardsandattributesthemto with prediction loss
ùêøùë°
ùëìùëö, and the credit assignment module
short-term actions through a weight vector from the attention outputs attention vector ùúñ ùëöùë° . The resulting intrinsic reward is
layer.ItusesanRNNandupdatesparametersatlongintervals ùëü ùëñùë° ,ùëö = ùúñ ùëöùë° ùëü ùëíùë° ,ùëö + ùêøùë° ùëìùëö,ùë° ‚àà {1,¬∑¬∑¬∑ ,ùúè}. The expected return is
(i.e., when long-term reward is available). now Jùëö = T1 (cid:205)T ùë°=1ùëü ùëñùë° ,ùëö,T‚Üí‚àû. In trying to maximize Jùëö, the
In an ablation study in [43], we compared the contribution local agent encourages 1) actions that bring higher extrinsic
of each module to the agent‚Äôs performance. We simulated two reward,2)explorationinlessvisitedstateswithpoorprediction
common types of repeated auctions with a single commodity accuracy (high
ùêøùë°
), and 3) actions that contribute more to
ùëìùëö
and pitched three algorithms against each other: 1) a stand- the accurate prediction of long-term rewards (high ùúñùë° ) . The
ùëö
aloneFSPwiththebasicACalgorithm,2)theACpluscurios- update rule for ùëö‚Äôs individual parameters in the inner-loop
ity learning, and 3) the AC plus both curiosity learning and offline training phase is [5]:
credit assignment. Results showed that with each additional
module, the performance became better, and the combination
(cid:40)
ofthreemodulesoutperformedallothers.Basedonthisresult, ùúÉùë° ùëö ‚ÜêùúÉ ùëöùë°‚àí1+Œìùëöùõøùë° ùëö‚àí1‚àá ùúÉ ùëöùë°‚àí1lnùúã(ùê¥|ùúôùë° ùëö‚àí1,ùúÉ ùëöùë°‚àí1)
we use all three modules in this study. ùúÉ0 =ùúÉ0,‚àÄùë° ‚àà {1,...,ùúè}
ùëö
In the beginning of every inner-loop offline training phase,
all local agents are initialized with the same generic model
that is the outcome of the previous outer-loop phase, with the where ùõøùë° ùëö‚àí1 =r ùëñùë° ,‚àí ùëö1+ùõæ ùëöùëâÀÜ(ùúôùë° ùëö,ùúÉ ùëöùë°‚àí1)‚àíùëâ(ùúôùë° ùëö‚àí1,ùúÉ ùëöùë°‚àí1) is the TD
parameters ùúÉ0.Duringinner-looptraining,localagentùëö trains error, Œìùëö is the learning rate, and ùõæ ùëö is the discount rate. In
its own local, single model and does not share parameters or ourcase,action ùê¥= (ùõºùë° ,bùë° ).Attheendof ùúè shots,thelocal
ùëö ùëö
private observations with other agents. At each time step ùë°, it gradients are passed to the coordinator agent before the next
receivesextrinsicrewardùëüùë°
,includingthelong-termrewards outer-loop phase. The inner-loop algorithm is in Alg. 1.
ùëí,ùëö8
B. Federated learning in the outer loop V. EVALUATION
While independent local agents with the local models learn A. Simulation setup
forùúè shots,thecoordinatoragentwiththegenericmodelwaits
The evaluation has two cycles: offline two-phase training,
with the original parameters ùúÉ0, until the next update in the
and online testing / retraining. The coordinator agent with
outer-loop phase (Fig.3). In the outer-loop phase, the goal of
the generic model is only present in the training cycle, it
the coordinator agent is to maximize all local agents‚Äô sum
collects gradients from all local agents and learns a generic
of returns: J = (cid:205) ùëöJùëö(ùúÉ ùëöùúè). After ùúè shots, at the end of
model. Once deployed in the test environment, all agents are
the previous inner-loop phase, the generic model‚Äôs parameters
initialized with the same generic model, but then diverge from
are ùúÉ0, and it uses the local models‚Äô gradients to update its
it by adapting to the environment through online retraining.
parameters:ùúÉ0‚Ä≤ =ùúÉ0+Œì‚àá ùúÉ0J.Sinceeachindividuallyupdated
All agents are independent bidders with private observations
parameter ùúÉùë° ,‚àÄùë° ‚àà {1,...,ùúè} is a function of ùúÉ0, using chain
ùëö and model parameters that are not shared with any other
rule, the generic model‚Äôs parameter update is:
agent. In both cycles, we consider a V2X system as defined
(cid:16) ùúè‚àí1 (cid:17)
ùúÉ0‚Ä≤ =ùúÉ0+(cid:205)
ùëö
‚àáùúÉ ùëöùúèJùëö(ùúÉ ùëöùúè) ùë°(cid:206)
=1
(cid:0) I‚àíŒìùëö‚àá2
ùúÉ ùëöùë°
Jùëö(ùúÉùë° ùëö)(cid:1) i sn ervS ie cc e. sI (I cI- oA m: mv oe dh ii tc il ee ss );a rr oe adb -id sid de ers uw nih to orreq bu ase est sn tae tt iw onork ai cn tg
s
where I is the identity matrix. Although it is computationally
as auctioneer that controls admission of service requests and
expensive, it can be approximated by a first-order derivative
assigns them to different computing sites (commodity sellers),
with the assumption that both Œì and ùúè are small [15], [46].
which own resources and execute services [47]. Many V2X
ùúÉ0‚Ä≤ =ùúÉ0+(cid:205) ùëöŒìùõø ùëöùúè‚àáùúÉ ùëöùúè lnùúã ùëöùúè(ùúÉ ùëöùúè) isthesimplifiedupdaterule. use cases [48] can be mapped to this setup.
Our setup meets the assumptions with Œì=0.1 and ùúè =3.
We develop a Python discrete-event simulator based on the
We use asynchronous federated learning to implement the
available open-source code [49]. It is a realistic V2X setup
parallelstochasticgradientascentmethod(Sec.VI).Itdoesnot
modeled as a 4-way traffic intersection (Fig. 1), with varying
require all local models to be trained and updated at the same
number of vehicles of infinite lifespan, one MEC system with
time: each model is trained based on the availability of new
ACAandedgecomputingsite,andoneremotecomputingsite
local data. Whenever the local model finishes training for ùúè
withnon-negligibledelayto/fromtheintersectionindatatrans-
shots,thelocalagenttransmitsthegradientstothecoordinator
mission and state information update. The commodity types
agentandgetsupdatedmodelparametersfromit.Thisreduces
in the auction correspond to service request types in V2X.
data rate needed for gradient and parameter communication
We specifically model two self-driving applications: motion
and further increases learning efficiency. planning (ùêπ1) and image segmentation (ùêπ2). The details are
in Table III. The commodity instances being auctioned are
service slots for the different service request types, provided
C. Adaptive online retraining
by the computing sites. All environment parameters are ran-
After the offline training, and once the model is deployed domized to imitate noise in real life. ACA assigns admitted
in a real-world setting, the credit-assignment module continu- requeststocomputingsitesbasedonaload-balancingheuristic
ouslypredictsrewards.Thecurrentrewardpredictionaccuracy named resource-intensity-aware load-balancing (RIAL) [50].
is compared to the moving average of past ùëÅ prediction The method achieves dynamic load-balancing among comput-
accuracies, if it falls below the past average, a short ùëõ-shot ingsitesthroughresourcepricingthatiscorrelatedtothesite‚Äôs
retraining cycle is triggered. In our simulation, we use ùëÅ =10 load,andloadsareshiftedto‚Äúcheaper‚Äùsites.Themobilitydata
and ùëõ=1. The algorithm is described in Alg. 2. is generated from SUMO [51], with varying vehicle speed,
arrival rate, traffic light phases, etc.
Algorithm 2 Online adaptive retraining of local agent ùëö The bidders have the following objectives:
1: Initialize ùë° = 0, ùêµ0 ùëö, vùëö, Œìùëö, ùõæ ùëö, ùúÇ ùëö, ùúè, and moving average ‚Ä¢ Maximize individual short-term (immediate) auction util-
period ùëÅ of credit assignment‚Äôs prediction loss. ity: as defined in Sec. III-C.
2: Initialize with ùúÉ from coordinator agent.
‚Ä¢ Maximize system short-term resource utilization: load-
3: while true do
4: ùë° ‚Üêùë°+1, receive new preferences Wùë° ùëö if available. balancing effect is achieved by encouraging bidding at
5: Observe and remember. time of low system utilization [40]. Resource utilization
6: Take action. is the ratio of resources effectively utilized at computing
7: Collect rewards. sites at the time of ACA admission.
8: if long-term reward is available then
‚Ä¢ Minimize long-term individual offloading failure rate
9: Calculate and store prediction loss of credit assignment.
10: if current prediction loss > past ùëÅ average then (OFR):averageratioofoffloadingrequeststhatcannotbe
11: Update learning model. serviced before deadline. In fact, OFR should include all
12: end if failedserviceexecutionsatcomputingsitesuntildeadline,
13: end if rather than only those dropped by the bidder or rejected
14: end while
by the ACA. However, this means feedback of bidding
result to the bidders is delayed, and the length of delay
SimulationresultsinSec.V-Bshowtheeffectivenessofthis is specific to each service request. To simplify, we use
adaptiveonlineretrainingapproach.Sec.VImentionspractical rejection rate as a proxy to OFR. This is justified by
considerations in online retraining. the fact that our system responsiveness (i.e., the ratio of9
Table III: Setup differences
EnvironmentParameters TrainingSetup TestSetup
F1:80resourceunits(abstractedfromCPUandmemoryusage)neededwithin100timesteps(milliseconds)
servicerequesttype
F2:80resourceunitsneededwithin500timesteps
servicearrivalrate F1:every100timesteps;F2:every500timesteps
datasize uplink:F1:0.4Mbit,F2:4Mbit;downlink:F1:0(negligible),F2:0.4Mbit
latency 802.11ac:65mradius,maximumchannelwidth1.69Gbps,throughput=‚àí26√ódistance+1690Mbps[54]
computingsitecapacity
60(lowcontention) 10(highcontention)
(resourceunitspertimestep)
vehiclearrivalrate 1every2.2seconds 1every1second
vehiclespeed 10km/hwhendriving 30km/h
vehiclecount 22‚àí29andslow-changing 14‚àí30andburstya
aWeregulateburstinessbyadjustingvehiclespeed,arrivalrateandtrafficlightphases
0.25 0.25 3.0
0.9 0.23
0.2
2.5 0.2 0.85
2.0 0.18 0.15
0.8 0.15 1.5 0.1 0.12
0.75 1.0 0.1 0.05
0.08
0.7 0.5
0.0
0 50 100 150 200 0 50 100 150 200 0 50 100 150 200 0 50 100 150 200
step √ó103 step √ó103 step √ó103 step √ó103
(a)AverageRLrewardofallbidders (b)Averagelossincreditassignment (c) Average state prediction loss in (d)Averageactionpredictionlossin
increasesovertime. decreasesovertime. curiositydecreasesovertime. curiositydecreasesovertime.
Figure 5: Training results
successfullyexecutedrequeststoallacceptedrequests)is and trigger a short, adaptive retraining cycle according to
ca. 99%. Sec.IV-C.Besidesthesesetupdifferences,thetestenvironment
‚Ä¢ Maximizelong-termsystemfairness:weuseJ-index[52] also differs significantly from the training environment in
of payments over the last T time steps: Fairness = resource capacity, vehicle arrival rate and speed, and traffic
((cid:205) ùëö(cid:205)ùë° ùë°‚àíT ùëù ùëö)2 ,‚àÄùëö ‚àà ùëÄ. It is commonly used to light phases. Table III summarizes the differences.
|ùëÄ|(cid:205) ùëö((cid:205)ùë°
ùë°‚àíT
ùëù ùëö)2
measure fairness in networking, it is also the reciprocal
B. Performance results
of the normalized Herfindahl‚ÄìHirschman Index [53].
All modules of MOODY converge to a local optimum in
The two short-term rewards on auction utility and resource thetrainingcycle(Fig.5).Inthelow-contentiontrainingsetup,
utilization are available immediately after the auction round. wereachclosetooptimallong-termobjectives(i.e.,OFR‚Üí0,
The two long-term rewards on offloading failure rate (OFR) fairness ‚Üí1).
and fairness are available after a 2000-time-step delay. In the following inference/retrain cycles, we compare the
The same simulator is used for both offline training and performance of 1) MOODY bidders initialized with the
online testing. However, there are significant differences to generic model for multiple objectives, 2) DRACO2 bidders
the environment setups. In the training evironment, besides with the state-of-the-art single-objective algorithm from [43],
bidders and the auctioneer, there is a coordinator agent that pretrained independently with a scalarized objective, 3) AC
is only active during the outer-loop training phase to learn bidders with only the actor-critic module. The tests are run
parametersforagenericmodel(Sec.IV-B).Thegenericmodel separately, each test has only one algorithm for all bidders in
is incrementally updated and used to initialize all local agents thesimulationandrunmultipletimes.Wereporttheconfidence
at the beginning of each inner-loop training phase. During intervalsacrossallruns.Inalltests,bidders‚Äôpreferencevectors
every inner-loop, each bidder randomly selects a preference changerandomlyovertime,drawnfromauniformdistribution.
vector for the objectives and acts independently. During testing, each MOODY bidder decides independently
In the test environment, there is no coordinator agent, the whether to trigger a retraining cycle. In our simulation, once
bidders are initialized with the generic MOODY model in retraining is triggered, the modules learn with 1 shot in each
the beginning of the simulation, and they randomly select a retrainingcycle.TheDRACO2biddersareretrainedforafixed
preference vector at random intervals. Throughout evaluation, 10k time steps at the beginning of the deployment in the test
their credit assignment modules continuously predict rewards environment. The AC bidders are not retrained.
drawer
cisnirtni
dethgiew
ssol
noitnetta
egareva
ssol
egareva
ssol
egareva10
0.6 0.96 MOODY
0.95 5.0 DRACO2
0.5 0.94 0.9 4.0
0.4 3.0 0.92 0.85
0.3 2.0 0.9
MOODY 0.8
DRACO2 1.0 0.88
0.2 AC
0.75 0.0
0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100 0.2 0.3 0.4 0.5
step √ó103 step √ó103 step √ó103 offloading failure rate
(a) MOODY long-term individual (b) MOODY achieves average fair- (c) MOODY bidders‚Äô retrain cycles (d)Achievementofhighsystemfair-
OFR is 16% lower than DRACO2, nessof0.92,comparedtoDRACO2: are12%ofthetime(graylinesare nessandlowindividualOFRiscor-
30%lowerthanAC. 0.89andAC:0.86. retraincycles). related.
Figure 6: Objective achievement in test and retraining cycles
through incentivization.
1.0
All of the evaluations in Fig. 6 are done with the same
0.8 type of algorithms in the test environment (i.e., ‚Äúhomoge-
0.6 neous‚Äù). However, in real life, vehicle users may run different
algorithms (i.e., ‚Äúheterogeneous‚Äù). In Fig. 7, we show the
0.4 MOODY
DRACO2 20% cumulative distribution function (CDF) of each bidder‚Äôs OFR
0.2 DRACO2 50%
DRACO2 80% performance, when the two algorithms compete in the same
0.0 DRACO2 100% environment.
0.2 0.3 off0 lo.4 ading failure0 . r5 ate 0.6 0.7 The blue solid line labeled ‚ÄúMOODY‚Äù shows the perfor-
Figure 7: In a heterogeneous test environment with competing mance of MOODY bidders in either the homogeneous (all-
algorithms, DRACO2 performance improves with MOODY MOODY) or the heterogeneous environments with different
unimpacted. System fairness also improves. percentage of MOODY bidders‚Äìtheir average performance in
all environments are hardly different. In other words, they are
Figures6aand6bcompareperformanceontheachievement
unimpacted by the existence of other algorithms. Hence, to
ofsystemlong-termfairnessandindividuallong-termOFR.In
simplify the figure, we show their performance in one single
fact,inallfourobjectives,MOODYoutperformsotherbidders:
curve. The dotted lines show performance in environments
MOODY‚Äôsfairnessscoreiscloseto1,comparedtoDRACO2‚Äôs
with different mix of MOODY and DRACO2 bidders. The
0.89andAC‚Äôs0.86;MOODYachieves16‚àí30%loweroffload-
rightmostlineshowsaverageperformanceofDRACO2bidders
ing failure rate. Our results also show that MOODY achieves
in a homogeneous, all-DRACO2 environment, which has the
46‚àí77%higherutilityand5‚àí14%lesssystemutilization‚Äî
worst performance of all environments. Interestingly, in all
although the average utilization with MOODY and DRACO2
of the heterogeneous environments, DRACO2 bidders‚Äô OFR
bidders are similar, MOODY lowers load variance by 19%
performance improved, compared to the all-DRACO2 envi-
compared to DRACO2. With low variance, it is easier to plan
ronment, reducing the difference to MOODY bidders by 50%.
long-term resource availability, saving cost while keeping the
Overall system fairness also improved significantly. These
same service level.
improvements do not depend on the percentage of MOODY
Fig. 6c shows an example of how retraining contributes
biddersintheenvironment,indicatingthateventhepresenceof
to the decrease in prediction loss for one of the bidders: as
very few MOODY bidders can enhance overall performance.
explained in Sec. IV-C, the retraining cycles are triggered by
To summarize: we test MOODY‚Äôs transfer learning capabil-
low reward prediction accuracy. The vertical gray lines are
ity by evaluating its performance in more dynamic test envi-
where retraining cycles occur. The bidder triggers a one-shot
ronments and allowing the bidders to change their objective
retraining cycle whenever the prediction accuracy of rewards
preferences. Evaluation results show that 1) bidders initialized
reduces to below the moving average of the past 10 prediction
with MOODY and adaptively retrained outperform bidders
accuracies. As shown in Fig. 6c, the retraining cycles are
withotherstate-of-the-artlearningalgorithmsinallobjectives;
frequently, almost continuously triggered in the beginning of
2) the MOODY bidders demonstrate good generalization and
the deployment in test environment. Overall, the MOODY
transfer learning property, adapting to preference changes and
bidders spend 9‚àí15% of time in retraining cycles.
dynamicity in the environment; 3) the presence of MOODY
Fig. 6d shows correlation between achievements of the bidders in the environment improves the performance of bid-
two long-term objectives: improvement in fairness is corre- ders with other algorithms and system overall fairness.
lated to reduction in failure rate. In fact, we also see such
correlation between other objectives. The reward signals on
C. Sensitivity analysis
the system objectives help bidders learn this correlation, and
by considering system objectives, the bidders effectively earn First, we test the sensitivity of reward achievement to
higher reward on their individual objectives, at the same time changing user preferences, based on data collected in infer-
the auctioneer and commodity sellers achieve their objectives ence/retraining cycle. We show in Fig. 8 the sensitivity of the
etar
eruliaf
gnidaolffo
FDC
ssenriaf
ssol
noitnetta
egareva
ssenriaf11
Table IV: Performance test
Modules Training Inference
Nr.calls Percall(millisec) Nr.calls Percall(millisec)
RL+credit 108 5484 431 29
supervised 108 112 431 0
curiosity 197 3092 431 0
dataprep 1275 10 431 29
Timepershot
max.ofmodules+dataprep 5494 sumofmodules+dataprep 58
(testedwithNano)
Timepershot 1/10ùë°‚Ñé ofNano 550 1/10ùë°‚Ñé ofNano 6
(estim.withAGXOrin)
1.0 0.975 1.0
0000 .... 2468 OFR 0000 .... 8999 7025 5050 fairness 0000 .... 2468 000 ... 899 505 fairness
0.0 0.850 0.0 0.80
0 ut.0 iliz0 a. t2 ion0 . p4 re0 f. e6
re0 nc.8 e1.0
0.00.20 OF. R40 p.
r60 ef. e8 r1 e. n0 ce
0 ut.0 iliz0 a. t2 ion0 . p4 re0 f. e6
re0 nc.8 e1.0
0.00 fa. i2 r0 n. e40 ss. 6
p0 r. e8 f1 e. r0 ence
0.0 0. b2 id0 v.4 alu0 e.6
0.8 1.0 0
1b2 ack3
o-f4
f
c5 ost
0.0 0. b2 id0 v.4 alu0 e.6
0.8 1.0 0
1b2 ack3
o-f4
f
c5 ost
(a)IndividualOFR (b)Systemfairness
(a)IndividualOFR (b)Systemfairnness
Figure 9: Sensitivity analysis shows target achievement is not
Figure 8: Sensitivity analysis shows target achievement is not
sensitive to changes in hyperparameters.
sensitive to changes in preference.
More importantly, asynchronous training reduces the online
twolongtermobjectivesOFRandfairnesstouserpreferences
retraining time after deployment.
of OFR, fairness and resource utilization. As expected (see
After deployment, bidders decide independently when to
Sec. IV), the rewards are not sensitive to different preference
retrain the model to adapt to new objectives and environments
vectors. The user-specific optimal solutions are close to the
(Sec. IV-C). The retraining is easily separated from the main
initial generic model, needing only a few shots of retraining.
program that infers bidding decisions in runtime (i.e., out-
Next, we test the sensitivity of our solution to different
of-critical-path). With retraining off the critical path, we can
hyperparameter inputs. We give two hyperparameters to each
ensure fast decision-making even with retraining.
MOODY bidder at the time of initialization: 1) the bid val-
uation ùë£ ùëö,ùëò that is private to each bidder ùëö, and specific to We test real-life training, retraining and inference speed of
each service request type ùëò, and 2) backoff cost ùëû ùëö,ùëò that is ouralgorithmonanNvidiaJetsonNanosingle-boardcomputer
with GPU. The single-board computer simulates an onboard
private to each bidder and reciprocal to the time-to-deadline
unit of a vehicle, or a bidder in the auction. We run the
(Sec. III-C). We change the value of these hyperparameters
training and inference repeatedly and record the average time
and show in Fig. 9 that MOODY is a robust algorithm that is
for one shot. The results are shown in Table IV. Besides data
insensitive to hyperparameter changes.
preparation (e.g. input data formatting, reshaping, stacking,
etc. that is done once for all modules), time for one-shot
VI. PRACTICALITYCONSIDERATIONS
training is the maximum time among all modules, and time
To speed up training, we train each module asynchronously for inference is the sum of all modules. We provide the
in federated learning (Sec. IV-B). Before each inner loop measuredperformanceonNano,andanestimatedperformance
begins, the local agent is initialized with generic model onthenewerAGXOrin.Althoughthetheoreticalperformance
parameters. Then, the agent joins the auction whenever it difference between the two is > 100 times, multiple bench-
receives a request. Since each local agent receives requests mark tests on various AI applications show a more realistic
randomly and makes independent decisions, they finish the performancedifferenceofca.10times(seeNvidiawebsitefor
inner-loop training phase at different time steps. Furthermore, Jetson modules technical specifications and benchmarks). We
each module trains at different time intervals. Asynchronous thereforeestimatethatwithAGXOrin,trainingoneshottakes
trainingreducespeakdatarate(i.e. themaximumdatavolume ca. 550ms, and inference takes ca. 6ms. Speed can be further
transmittedoveranetworkpersecond).Italsoreducestraining increased through fewer layers and nodes, smaller batch size,
time:thetimefortrainingonce(i.e.,one-shot)isthemaximum shorter input length, etc. The analysis of performance impact
duration among the modules 1) RL with credit assignment is left to future work.
(RL+credit), 2) supervised learning (supervised) and 3) cu- These results show that despite the complexity of the
riositylearning(curiosity).Withouttheasynchronoustraining, proposed solution, bidders can perform runtime inference, on
one-shot training time would be the sum of all modules. current hardware, with a reaction time of 6ms. V2X applica-
RFO12
tions (e.g., segmentation, motion planning) typically run on [7] J.-H. Cho, Y. Wang, R. Chen, K. S. Chan, and A. Swami, ‚ÄúA survey
the time scale of seconds, an inference speed in milliseconds onmodelingandoptimizingmulti-objectivesystems,‚ÄùIEEECommuni-
cationsSurveys&Tutorials,2017.
makes our model a good candidate for real-life deployment.
[8] Q. H. Ansari, E. K√∂bis, and J.-C. Yao, ‚ÄúVector variational inequalities
The retraining cycle is longer, for which we believe that out- andvectoroptimization,‚ÄùSpringer,2018.
of-critical-path few-shot retraining holds great promise. Even [9] C.F.Hayes,R.RaÀòdulescu,E.Bargiacchi,J.K√§llstr√∂m,M.Macfarlane,
M. Reymond, T. Verstraeten, L. M. Zintgraf, R. Dazeley, F. Heintz
without that optimization, the retraining cycle lasts only a
et al., ‚ÄúA practical guide to multi-objective reinforcement learning and
few seconds, well below the frequency of changes in a V2X planning,‚ÄùAAMAS,2022.
environment that may trigger retraining. [10] M.A.KhamisandW.Gomaa,‚ÄúAdaptivemulti-objectivereinforcement
learning with hybrid exploration for traffic signal control based on co-
operativemulti-agentframework,‚ÄùEngineeringApplicationsofArtificial
VII. CONCLUSION&FUTUREWORK
Intelligence,2014.
We combine offline federated learning and online few-shot [11] H. A. Aziz, F. Zhu, and S. V. Ukkusuri, ‚ÄúLearning-based traffic signal
control algorithms with neighborhood information sharing: An appli-
learningto solveanMOPin adynamicenvironment.Through
cation for sustainable mobility,‚Äù Journal of Intelligent Transportation
extensive offline training, we get an optimal initial model that Systems,2018.
learns the best initialization point. From this point, it can [12] V. Pandey, E. Wang, and S. D. Boyles, ‚ÄúDeep reinforcement learning
algorithm for dynamic pricing of express lanes with multiple access
quickly find a solution on the Pareto frontier, even without
locations,‚Äù Transportation Research Part C: Emerging Technologies,
retraining, when the agent‚Äôs objectives change. Only in a 2020.
significantly different environment, we allow each bidding [13] S.Parisi,M.Pirotta,N.Smacchia,L.Bascetta,andM.Restelli,‚ÄúPolicy
gradientapproachesformulti-objectivesequentialdecisionmaking,‚Äùin
agent adaptive, online, few-shot retraining to customize its
IEEEIJCNN,2014.
model, needing very few data points. [14] M. Pirotta, S. Parisi, and M. Restelli, ‚ÄúMulti-objective reinforcement
We show empirically that our new multi-objective algo- learningwithcontinuousparetofrontierapproximation,‚ÄùinAAAI,2015.
[15] C. Finn, P. Abbeel, and S. Levine, ‚ÄúModel-agnostic meta-learning for
rithm outperforms the benchmark algorithms in all objectives.
fastadaptationofdeepnetworks,‚ÄùinICML,2017.
Furthermore, our algorithm increases bottom-line resource
[16] ‚ÄúMoodysource,‚Äùhttps://github.com/moodysourcecode/moody.
efficiency, such that other algorithms in the environment also [17] Z. Ning, K. Zhang, X. Wang, L. Guo, X. Hu, J. Huang, B. Hu, and
benefit from improved offloading success rate and fairness. R.Y.Kwok,‚ÄúIntelligentedgecomputingininternetofvehicles:ajoint
computation offloading and caching solution,‚Äù IEEE Transactions on
Our algorithm can be easily modularized, each module
IntelligentTransportationSystems,2020.
trainedseparatelyandasynchronously.Coupledwiththeadap- [18] G. Ma, X. Wang, M. Hu, W. Ouyang, X. Chen, and Y. Li, ‚ÄúDrl-based
tive few-shot online training method, the algorithm is a very computationoffloadingwithqueuestabilityforvehicular-cloud-assisted
mobile edge computing systems,‚Äù IEEE Transactions on Intelligent
good candidate for real-life deployment.
Vehicles,2022.
Currently,wesimulateagents‚Äôpreferenceofobjectiveswith [19] X. Zhu, Y. Luo, A. Liu, N. N. Xiong, M. Dong, and S. Zhang, ‚ÄúA
uniformrandomlygeneratedweights,andscalarizetherewards deepreinforcementlearning-basedresourcemanagementgameinvehic-
ular edge computing,‚Äù IEEE Transactions on Intelligent Transportation
with a linear objective function, with the assumption that the
Systems,2021.
individual objectives are independent from each other. There [20] L.Liu,J.Feng,X.Mu,Q.Pei,D.Lan,andM.Xiao,‚ÄúAsynchronousdeep
aretwopotentialimprovementstothisapproach:1)themethod reinforcementlearningforcollaborativetaskcomputingandon-demand
resourceallocationinvehicularedgecomputing,‚ÄùIEEETransactionson
for sampling preferences may impact the approximation of
IntelligentTransportationSystems,2023.
the Pareto frontier and the performance of the initial model. [21] R.Bajracharya,R.Shrestha,S.A.Hassan,K.Konstantin,andH.Jung,
Future work should consider different sampling methods such ‚ÄúDynamic pricing for intelligent transportation system in the 6g unli-
censedband,‚ÄùIEEETransactionsonIntelligentTransportationSystems,
as proposed in [55] and [56]. 2) Simulation results show that
2021.
agents learn the correlation between different objectives. In [22] S. Xia, Z. Yao, G. Wu, and Y. Li, ‚ÄúDistributed offloading for coop-
fact, multiple objectives in real-life are typically correlated erative intelligent transportation under heterogeneous networks,‚Äù IEEE
TransactionsonIntelligentTransportationSystems,2022.
to each other. There may exist a hierarchy or network of
[23] D.Wei,J.Zhang,M.Shojafar,S.Kumari,N.Xi,andJ.Ma,‚ÄúPrivacy-
objectives, and we should guide the learning process with this
aware multiagent deep reinforcement learning for task offloading in
knowledge of the objective structure in our future work. vanet,‚ÄùIEEETransactionsonIntelligentTransportationSystems,2022.
[24] Y. Ju, Y. Chen, Z. Cao, L. Liu, Q. Pei, M. Xiao, K. Ota, M. Dong,
and V. C. Leung, ‚ÄúJoint secure offloading and resource allocation for
REFERENCES
vehicular edge computing network: A multi-agent deep reinforcement
[1] P. Arthurs, L. Gillam, P. Krause, N. Wang, K. Halder, and A. Mouza- learning approach,‚Äù IEEE Transactions on Intelligent Transportation
kitis, ‚ÄúA taxonomy and survey of edge cloud computing for intelligent Systems,2023.
transportation systems and connected vehicles,‚Äù IEEE Transactions on [25] X.Xu,C.Yang,M.Bilal,W.Li,andH.Wang,‚ÄúComputationoffloading
IntelligentTransportationSystems,2021. for energy and delay trade-offs with traffic flow prediction in edge
[2] K.Xiong,S.Leng,C.Huang,C.Yuen,andY.L.Guan,‚ÄúIntelligenttask computing-enablediov,‚ÄùIEEETransactionsonIntelligentTransportation
offloading for heterogeneous v2x communications,‚Äù IEEE Transactions Systems,2022.
onIntelligentTransportationSystems,2020. [26] L. Yao, X. Xu, M. Bilal, and H. Wang, ‚ÄúDynamic edge computation
[3] M.BowlingandM.Veloso,‚ÄúAnanalysisofstochasticgametheoryfor offloading for internet of vehicles with deep reinforcement learning,‚Äù
multiagentreinforcementlearning,‚ÄùCarnegie-MellonUnivPittsburghPa IEEETransactionsonIntelligentTransportationSystems,2022.
SchoolofComputerScience,Tech.Rep.,2000. [27] L.S.ShapleyandF.D.Rigby,‚ÄúEquilibriumpointsingameswithvector
[4] A.HaydariandY.Yƒ±lmaz,‚ÄúDeepreinforcementlearningforintelligent payoffs,‚ÄùNavalResearchLogisticsQuarterly,1959.
transportation systems: A survey,‚Äù IEEE Transactions on Intelligent [28] F.Patrone,L.Pusillo,andS.Tijs,‚ÄúMulticriteriagamesandpotentials,‚Äù
TransportationSystems,2020. Top,2007.
[5] R.S.SuttonandA.G.Barto,Reinforcementlearning:Anintroduction. [29] A.-I. Mouaddib, M. Boussard, and M. Bouzid, ‚ÄúTowards a formal
MITpress,2018. frameworkformulti-objectivemultiagentplanning,‚ÄùinAAMAS,2007.
[6] I.Althamary,C.-W.Huang,andP.Lin,‚ÄúAsurveyonmulti-agentrein- [30] P. Perny, P. Weng, J. Goldsmith, and J. Hanna, ‚ÄúApproximation of
forcementlearningmethodsforvehicularnetworks,‚ÄùinIEEEIWCMC, lorenz-optimal solutions in multiobjective markov decision processes,‚Äù
2019. inConferenceonUncertaintyinArtificialIntelligence,2013.13
[31] C.Jonker,R.Aydogan,T.Baarslag,K.Fujita,T.Ito,andK.Hindriks,
‚ÄúAutomatednegotiatingagentscompetition(anac),‚ÄùinAAAI,2017.
[32] G. D. O. Ramos, R. Radulescu, and A. Nowe, ‚ÄúA budged-balanced
tollingschemeforefficientequilibriaunderheterogeneouspreferences,‚Äù
inAAMASALAworkshop,2019.
[33] A.Bousia,E.Kartsakli,A.Antonopoulos,L.Alonso,andC.Verikoukis,
‚ÄúMultiobjective auction-based switching-off scheme in heterogeneous
networks: To bid or not to bid?‚Äù IEEE Transactions on Vehicular
Technology,2016.
[34] H. Gedawy, K. Habak, K. A. Harras, and M. Hamdi, ‚ÄúRamos: A
resource-awaremulti-objectivesystemforedgecomputing,‚ÄùIEEETrans-
actionsonMobileComputing,2021.
[35] Z.LiandZ.Ding,‚ÄúDistributedmultiobjectiveoptimizationfornetwork
resource allocation of multiagent systems,‚Äù IEEE Transactions on Cy-
bernetics,2021.
[36] R. Wang et al., ‚ÄúWang, rui and zhang, qingfu and zhang, tao,‚Äù IEEE
TransactionsonEvolutionaryComputation,2016.
[37] Y.Sun,S.Zhou,andZ.Niu,‚ÄúDistributedtaskreplicationforvehicular
edge computing: Performance analysis and learning-based algorithm,‚Äù
IEEETransactionsonWirelessCommunications,2021.
[38] M. Kayaalp, S. Vlaski, and A. H. Sayed, ‚ÄúDif-maml: Decentralized
multi-agent meta-learning,‚Äù IEEE Open Journal of Signal Processing,
2022.
[39] M. Al-Shedivat, T. Bansal, Y. Burda, I. Sutskever, I. Mordatch, and
P. Abbeel, ‚ÄúContinuous adaptation via meta-learning in nonstationary
andcompetitiveenvironments,‚ÄùinICLR,2018.
[40] J. Tan, R. Khalili, H. Karl, and A. Hecker, ‚ÄúMulti-agent distributed
reinforcement learning for making decentralized offloading decisions,‚Äù
IEEEINFOCOM,2022.
[41] C. Raquel and X. Yao, ‚ÄúDynamic multi-objective optimization: a sur-
vey of the state-of-the-art,‚Äù in Evolutionary computation for dynamic
optimizationproblems,2013.
[42] X.Lian,W.Zhang,C.Zhang,andJ.Liu,‚ÄúAsynchronousdecentralized
parallelstochasticgradientdescent,‚ÄùinICML,2018.
[43] J.Tan,R.Khalili,andH.Karl,‚ÄúLearningtobidlong-term:Multi-agent
reinforcement learning with long-term and sparse reward in repeated
auctiongames,‚ÄùinAAAIRLGWorkshop,2022.
[44] J.Heinrich,M.Lanctot,andD.Silver,‚ÄúFictitiousself-playinextensive-
formgames,‚ÄùinICML,2015.
[45] D. Pathak, P. Agrawal, A. A. Efros, and T. Darrell, ‚ÄúCuriosity-driven
explorationbyself-supervisedprediction,‚ÄùinICML,2017.
[46] A. Nichol, J. Achiam, and J. Schulman, ‚ÄúOn first-order meta-learning
algorithms,‚ÄùarXivpreprintarXiv:1803.02999,2018.
[47] M. Whaiduzzaman, M. Sookhak, A. Gani, and R. Buyya, ‚ÄúA survey
on vehicular cloud computing,‚Äù Journal of Network and Computer
applications,2014.
[48] ‚ÄúC-v2xusecasesvolumeii:Examplesandservicelevelrequirements,‚Äù
5GAAAutomotiveAssociation,2020.
[49] ‚ÄúMalfoysource,‚Äùhttps://github.com/DRACOsource/malfoy.
[50] H.ShenandL.Chen,‚ÄúAresourceusageintensityawareloadbalancing
methodforvirtualmachinemigrationinclouddatacenters,‚ÄùIEEETrans.
onCloudComputing,2020.
[51] M. Behrisch, L. Bieker, J. Erdmann, and D. Krajzewicz, ‚ÄúSumo‚Äì
simulationofurbanmobility:anoverview,‚ÄùinSIMUL,2011.
[52] R.K.Jain,D.-M.W.Chiu,W.R.Haweetal.,‚ÄúAquantitativemeasure
offairnessanddiscrimination,‚ÄùEasternResearchLaboratory,1984.
[53] S.A.Rhoades,‚ÄúTheherfindahl-hirschmanindex,‚ÄùFed.Res.Bull.,1993.
[54] Z.Shah,S.Rau,andA.Baig,‚ÄúThroughputcomparisonofieee802.11
ac and ieee 802.11 n in an indoor environment with interference,‚Äù in
IEEEITNAC,2015.
[55] J.-h.Ryu,S.Kim,andH.Wan,‚ÄúParetofrontapproximationwithadaptive
weighted sum method in multiobjective simulation optimization,‚Äù in
Proceedingsofthe2009WinterSimulationConference(WSC). IEEE,
2009.
[56] E.Khorram,K.Khaledian,andM.Khaledyan,‚ÄúAnumericalmethodfor
constructingtheparetofrontofmulti-objectiveoptimizationproblems,‚Äù
JournalofComputationalandAppliedMathematics,2014.