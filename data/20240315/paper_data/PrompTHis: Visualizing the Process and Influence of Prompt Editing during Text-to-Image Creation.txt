JOURNALOFLATEXCLASSFILES,VOL.XX,NO.XX,XX 1
PrompTHis: Visualizing the Process and Influence
of Prompt Editing during Text-to-Image Creation
Yuhan Guo, Hanning Shao, Can Liu, Kai Xu, and Xiaoru Yuan
Abstract—Generative text-to-image models, which allow users complicated by the inherent randomness such models have,
to create appealing images through a text prompt, have seen e.g., same prompt can lead to different images in different
a dramatic increase in popularity in recent years. However,
runs, and the fact that the mapping from language to images
most users have a limited understanding of how such models
isambiguoussuchthatsmallchangeinthepromptcanleadto
work and it often requires many trials and errors to achieve
satisfactory results. The prompt history contains a wealth of large change in the resulting image. Together with the lack of
information that could provide users with insights into what supportfororganizingandreviewingpreviousattempts,artists
have been explored and how the prompt changes impact the often engage in near-random explorations, easily lose track of
output image, yet little research attention has been paid to the
previous attempts, which often lead to repetitive efforts, stuck
visual analysis of such process to support users. We propose the
in the local convergence, or spending a significant amount of
Image Variant Graph, a novel visual representation designed to
supportcomparingprompt-imagepairsandexploringtheediting time without achieving desired outcomes.
history.TheImageVariantGraphmodelspromptdifferencesas In this work,we aim to address this challenge by designing
edges between corresponding images and presents the distances a novel visual analytics system that can help artists better
between images through projection. Based on the graph, we
make sense of the behavior and characteristics of the gen-
developedthePrompTHissystemthroughco-designwithartists.
erative model, which can in turn lead to a more efficient and
Besides Image Variant Graph, PrompTHis also incorporates a
detailedprompt-imagehistoryandanavigationmini-map.Based effective creative process. We co-designed with artists who
on the review and analysis of the prompting history, users can utilize generative models as part of their creative process,
better understand the impact of prompt changes and have a understand their goals, the current practice and workflow, and
more effective control of image generation. A quantitative user
the challenges they face. Two of the main needs we identified
studywithelevenamateurparticipantsandqualitativeinterviews
are that artists would like to know the image space that has with five professionals and one amateur user were conducted to
evaluatetheeffectivenessofPrompTHis.Theresultsdemonstrate been already explored to avoid repetition and understand how
PrompTHiscanhelpusersreviewtheprompthistory,makesense the changes in prompts influence the generation of images.
of the model, and plan their creative process. Theterm“promptengineering”hasbeencreatedtodescribe
Index Terms—Text visualization, image visualization, text-to- the methods and processes that help users create effective
image generation, editing history, provenance, generative art prompts. Some research efforts have been devoted to creating
visualization tools that can assist users in prompt editing
I. INTRODUCTION or recommendation [3]–[5]. However, these mostly try to
matchuserpromptswithpreviousexamples,ignoringpotential
In recent years, generative text-to-image models, such as
differences in individual’s intention and preference. We took
StableDiffusion[1]andDALL-E2[2],havegainedsignificant
a different approach and focus on the prompt editing process,
popularity. These models can generate exquisite images from
believing the prompt history contains the information that
a text prompt, reducing the barriers for the general public
is key to a solution. Before the prevalence of text-to-image
to engage in visual creation and providing new avenues for
models,theeditingprocessprimarilyreferstorevisingtextual
artisticexpression.Manyartistsalsobeguntoexplorecreative
content [6]–[8]. The arrival of the generative models has
ideas with such models, taking advantage of the sometimes
changed the nature of such editing, as two modalities, text
unexpected results they produce.
and images, are involved. Understanding these two modalities
Despite the enormous potential, it is often challenging to
simultaneously poses great challenges. Moreover, the two are
generate images that match artists’ intentions and creative
connected, i.e., changes in the prompt text cause updates in
preferences. Most users have a limited understanding of such
the resulting images, and such connections are often complex
modelsandstruggletoconveytheirintentionsinawaythatthe
and difficult to understand, if possible at all.
model can understand. There is no guarantee of satisfactory
As a result, we developed the Image Variant Graph, which
outcomes even after many trials and errors. This is further
models the prompt history as a graph with the images as
Yuhan Guo, Hanning Shao, Can Liu, and Xiaoru Yuan are with Key nodes and the differences in text prompts as edges. We assign
Laboratory of Machine Perception (Ministry of Education), School of AI, weights to the edges to reflect how the modifications of a
Peking University, China. E-mail: {yuhan.guo, hanning.shao, can.liu, xi-
specific word impact the generation of images. The Image
aoru.yuan}@pku.edu.cn. Xiaoru Yuan is also with National Engineering
LaboratoryforBigDataAnalysisandApplication,PekingUniversity.Xiaoru Variant Graph positions the image nodes in a 2D space based
Yuanisthecorrespondingauthor. on their visual similarity. This allows users to observe the
Kai Xu is with University of Nottingham, UK. E-mail:
distribution of generated images and help analyze the impact
Kai.Xu@nottingham.ac.uk.
Manuscriptreceivedxx;revisedxx. of prompt change on the generation.
4202
raM
41
]CH.sc[
1v51690.3042:viXraJOURNALOFLATEXCLASSFILES,VOL.XX,NO.XX,XX 2
BasedonImageVariantGraph,PrompTHisisaninteractive can accurately capture their intention and preferences and
visualization system designed to further assist artists with also be understandable by the model. How to create effective
promptengineering.WiththeImageVariantGraphasthemain prompts becomes a craft itself, which is referred to as prompt
view, the PrompTHis system also includes a detailed prompt- engineering. Oppenlaender [13] summarized six prompt mod-
image pair history, a prompt mini-map for navigation, and a ifiers applied by individuals in the online community. Liu et
creation panel to generate images. A formal user study with al. [14] also conducted experiments to analyze the influences
eleven participants was conducted to evaluate effectiveness of prompt keywords and model hyper-parameters on the out-
of Image Variant Graph and PrompTHis in a post-analysis puts.Theseworksprovideexperimentalguidelinesforprompt
setting. We further conducted in-depth interviews with five constructions to help individuals produce better. However,
professional users and one amateur user to understand how these guidelines are usually model-specific, i.e., they do not
thesystemsupportsthecreativeprocess.Allparticipantsfound always apply to different models. Also, each artist would
thesystemhelpfulforreviewingandunderstandingtheprompt have his or her own style and preference, and these nuanced
history. Three of the interview participants were engaged in differences are not always captured by these guidelines.
leveragingImageVariantGraphtorefinepromptsaccordingto
model behavior and plan the creative process. To summarize, B. Auto and Visual Assistance in Prompt Engineering
thecontributionofourmethodcanbesummarizedasfollows:
Giventhechallengesofcreatingeffectiveprompts,research
1) Image Variant Graph, a novel and efficient visual design
has been carried out to help with prompt engineering. In the
for prompt history that reveals the image distribution
context of text-to-text generation, PromptAid [15] helps users
from the existing attempts and how word-level changes
apply perturbations on keywords, paraphrases, and in-context
in prompt influence the generation of images.
examples to test and refine their prompts. PromptIDE [16]
2) PrompTHis, A visual analysis system that helps users
allows prompt testing on small datasets before being applied
explore the prompting history and make sense of the
to the whole dataset. There are also a few methods designed
generative model through analysis of the editing history.
specifically for text-to-image creation. Some works visualize
3) A user study and in-depth interviews to demonstrate the
the details during the process of generation [17] and allow
effectiveness of Image Variant Graph and PrompTHis.
user users to assign different prompts on different areas on
the canvas and stages of the generation process [18]. While
II. RELATEDWORK useful, these methods are more suitable for users with enough
This section begins with the related literature in the field technical knowledge and can benefit from the appreciation of
of text-to-image generation. This is followed by the recent internal process of generative models, which is often not the
work on prompt engineering, particularly the support for the case for the members of the creative community.
prompt editing process, because our work targets the process Other works target non-technical users. Wang et al. [3]
by individual creators to iteratively refine prompts to create explored the emotional expressiveness in prompts from real
artistic paintings. The last part covers the related work in the users and proposed the RePrompt model to automatically
broader field of visualization for the editing process. refineusers’promptswithemotiondescriptions.Promptify[5]
leverages large language models to recommend prompts.
A. Text-to-Image Generation PromptMagician [4] extracts similar prompt-image pairs from
the DiffusionDB [19] according to users’ inputs and provides
Generative AI has attracted a huge amount of interest
multi-view interaction that can help users find interesting
from the general public and professionals, since it demon-
recommendations and refine their prompts.
strated ground breaking capability in image creation. Ever
Shared among these methods is the approach to provide
since OpenAI releases CLIP [9] in DALL-E architecture [10],
recommendationsbasedonthesimilaritybetweenuserprompt
a contrastive language-image pre-training model that aligns
and previous examples stored in a large database. However,
natural languages and images in vector-based representations,
as mentioned earlier, each artist may have his or her own
a number of models are proposed to generate images from
style and preference, and previous examples may not be
text, including VQGAN-CLIP [11] and latent diffusion [1].
a good fit just because the prompts are similar. Our work
Please refer to a recent survey [12] for more details.
focuses on the analysis of the prompt engineering process.
Thesetext-to-imagemodelssignificantlyreducethebarriers
There are two potential benefits of this approach: first, it
of creating images. Artists are also very interested in these
provides users with a more intuitive understanding of how
AI generators, not necessarily using the output as their work
the prompt changes impact the output images, allowing better
but more of an inspiration for creative ideas. The randomness
controlofthegenerationprocesswithoutexposingtheinternal
and uncertainty during the generation process may lead to
workings of the models. Second, it provides a more nuanced
surprising results, some of which the artists we worked with
understanding of user intention and preference, which can be
found inspirational. In addition, such models allow artists to
used to improve similarity-based recommendations.
quicklytestoutdifferentideas.Therefore,manyartistsinclude
these models as part of their creative workflow.
C. Visualization for Editing Process
One of the main challenges faced by the artists when
working with the generative models is how to compose ef- Our work focuses on the prompt editing history, and an
fective text prompts, i.e. how to construct descriptions that important aspect of that is the changes of the prompt text.JOURNALOFLATEXCLASSFILES,VOL.XX,NO.XX,XX 3
There are previous visualization methods designed for text A. Workflow
comparison,thoughnotinthecontextoftext-to-imagegenera-
OneoftheartistsexperimentedwiththeDisco-Diffusionvia
tion.Someofthesemethodsfocusonthecomparisonbetween Colabnotebook1.andtheotherartistusedStableDiffusion[1]
differentversionsofthetext,whichisalsoknownas“parallel
and Midjourney [33]. The artists often do not have a specific
texts”. One of the common technique for visualizing parallel
idea to start with and would adapt the prompt and setting as
texts is juxtaposition [6], [20], [21], often leveraging close
they progress. The iterations stop when the artist is satisfied
and distant reading methods [22]–[24]. The other important
with the results, does not know how to further improve the
aspect of prompt history is the images generated at each step.
prompt, or simply runs out of time. The latter two are far
Research efforts have been made to visualize the changes in a
more common than the first one. We organize the experiences
collection of images. These methods often employ projection
and needs of the artists into the following insights.
methodstomaptheimagestoa2Dor3Dspacetohelpreveal
• I1. Lack of organization in exploration process. Currently
the similarities and differences among the images [25]–[28].
there is no easy way to save the prompt/setting history
In text-to-image generation, prompts and images are tightly
and the generated images in Colab notebook, so the artist
coupled in the editing process and need to be considered
manuallycreatedfilesandfolderstosaveandorganizethem.
together.Thus,existingvisualcomparisonmethodsfortextor
Even though some current apps support saving the attempts
imagesdiscussedearlierarenoteasilyapplicable.Inourwork,
automatically, it is not easy to review the explored settings
prompts and corresponding images are always considered as
andtheextenttowhichtheoutcomesmatchexpectations.As
pairs. The Image Variant Graph visualizes the changes in
aresult,theartistssometimesmakerepetitiveunsatisfactory
both the prompt text and resulting images. Together with
experiments.Inothertimestheymighttemporarilyleavean
the other features, PrompTHis allows users to gain a better
intermediateresultandexploreotherbranches,butforgetor
understanding of the relationships between the two in a way
find it challenging to come back.
that is different from other attempts so far.
• I2. Misalignment between user intention and model output.
The text and images involved in the prompt history can
Themodeldoesnotalwaysunderstanduser’sintentioninthe
be considered as part of the Analytic Provenance [29] of the
prompt. In some cases, the model misinterpret the context
creative process. Analytic provenance includes a wide range
of the prompt due to ambiguity in natural language. For
of contextual information about the analysis [30], from the
example, once there were word “head” and “shoulder” in
data used, user interactions (which include the prompt), the
the prompt and shampoo appeared in the image. This was
analysis performed (such as the running of the generative
not intended and the artist would modify the prompt to
model), intermediate and final results (e.g. images generated),
remove the shampoo. But generating unexpected output is
to the user’s critical thinking and analytic reasoning. One
not always bad. Another time, to the artist’s surprise, the
of the common goals of analytic provenance analysis is to
output image had robotic animals that the artist liked and
identify patterns in user behavior and reveal user’s intention
he then added the term to the next prompt.
and analysis strategies. From this perspective, PrompTHis
• I3.Diverserequirementsforpersonalizedrecommendations.
aims to better understand user’s creative intention through the
Theartistsputforwardvariousdesiresforthemodeltomake
collection and analysis of the prompt history. While both in-
suggestions.Possibleaspectsoftherecommendationinclude
tention and prompt/image are part of the analytic provenance,
automatic exploration of parameters, guidance for refining
the former is much harder to capture and has to be inferred
prompts, and assessment of output images according to
from the latter [31]. However, if solved, even in a specific
user’s taste. The common emphasis is that the suggestions
application context such as text-to-image generation, this
must cater to the preference of the artist.
would enable many exciting features, such as more effective
recommendation and adaptive system [32]. This is the long-
B. Requirements
termgoalofPrompTHis:currentlyitfocusesonthecollection
and visualization of analytic provenance; if successful the We are aware that direct guidance and recommendations
resultswouldallowfutureworktoprovidemoreintelligentand (I3), if effective, will significantly empower generative art
nuancedsupportforartisticcreationswithgenerativemodels. creation. However, these require a thorough understanding of
theusers’requirementsandpreferences,whicharereflectedin
III. DESIGNRATIONALE the exploration history. Therefore, we choose to focus on the
To understand how artists utilize text-to-image models in prompting history first and use the results as the foundation
their creative workflow and their needs during this process, for more active support in the next step. The target user
we conducted in-depth interview with two artists, including of this paper is professional artists who utilize generative
observation of their current practice. We started from learning models to explore creative ideas. The usage scenario is
abouttheartiststhemselves,suchastheirtechnicalbackground twofold: 1) reviewing and planning the creative process (I1),
and creative interests. We then went through their current and 2) making sense of the model’s behavior so as to convey
workflow and observed a few examples. Finally we discussed user’s intention in a way that the model can understand
with the artists about their experiences, comments, and chal- (I2). We believe such support serves as the foundation for
lenges about the generative AI. The interview was recorded
1Disco Diffusion (Colab), accessed Sep 2023. Available at:
and thematic analysis was applied to the interview transcript.
https://colab.research.google.com/github/alembics/disco-diffusion/blob/
We summarize the interview and analysis results below. main/Disco Diffusion.ipynb.JOURNALOFLATEXCLASSFILES,VOL.XX,NO.XX,XX 4
understandinguserintentionandpreference,whichwillenable differentorsimilar.ThePrompTHissystemcentersaroundthe
ustocreatepersonalizedrecommendations(I3).Therefore,we ImageVariantGraphthatmodelsthedifferencesinpromptsas
have summarized the following design requirements. avariantgraphthatiscommonintextanalysis.Thenodesare
• R1. Support organization and review of previous images and the edges are word-level differences in prompts.
promptsandimages.AsisdescribedinI1,theartistsspent Therefore, users can explore how the text modifications affect
a large amount of time in almost random trials and errors. theimagegeneration(R2).ImageVariantGraphalsoprovides
Even after the prompts and images are saved, there is no anoverviewofalltheattempts(R3)andallowseasyinspection
easy way to review previous attempts and understand what ofeachofthemthroughinteraction(R1).Finally,theoverview
hasworkedandwhathasnot.Thereisaneedtosignificantly also allows identification of gaps in the exploration, helping
reduce or eliminate the overhead of saving prompt history user plan the creative process (R4). The details of the Image
andamoreeffectivemeanstoorganizeandreviewprevious VariantGrapharediscussedinSectionIV,whereasadditional
attempts, even when the number of attempts is large. features in PrompTHis are covered in Section V.
• R2. Support comparison between different prompts and
images. Text-to-image models like Disco Diffusion are
guided by CLIP models [9], which aligns texts and images.
The artists find it helpful to “understand how CLIP works”
byexperimentingwithdifferentwordingandcomparingthe
results, so that they can set more accurate prompts and
convey the intent to the model. Currently, it is difficult to
locate relevant text and images and compare them.
• R3. Provide users with a better understanding of model Fig. 1. In Image Variant Graph the nodes are the images and the edges
arethedifferencebetweenprompts,oneedgeforeachdifference.Weighting
behavior. While the ability to compare individual prompt
algorithmsarethenappliedtofilteroutlessimportantedges.Finally,anovel
and image would help artists understand the model’s be- layoutalgorithmandvisualencodingareusedtoenhancescalability.
havior at a micro level, there is also a need to understand
such behaviors at a macro level, such as comparing two IV. IMAGEVARIANTGRAPH
groups of prompts. This would remove the negative impact ImageVariantGraphaimstoenablebetterunderstandingof
the inherent randomness that generative models have, and the behaviors of text-to-image models through efficient com-
wouldalsoallowartistgeneralizetheirunderstandingofthe parison between text-image pairs (R2, R3) and allow easier
underlying model, e.g., which types of prompts work well. navigation oflarge promptinghistories (R1). Fig.1 showsthe
• R4. Help users plan the creative exploration process. conceptual construction pipeline of the Image Variant Graph,
We observed several instances where artists consistently which is explained in more detail in the following sections.
obtainunsatisfactoryresultsregardlessofadjustmentsmade.
While any direct support (such as recommendations) would A. Graph Modeling
deserve a separate paper, we believe there is also the
An important and challenging aspect that artists are con-
possibility of indirect support, such as helping artists build
cerned with is how modifications to prompts affect the gener-
and maintain a mental map of and orient themselves in
ation of images. As is shown in Fig. 1, each image is a node.
the spaces explored, which would help identify the gaps
The word-level differences in prompts between two images
not covered so far and allow them to conduct creative
aremodeledasmultipleedgesconnectingthetwonodes,with
explorations more systematically.
each edge representing one-word insertion or deletion.
C. Overall Design
For a creative session, while prompts and images are
explicitly connected through the prompt-image pairs, their
Fig. 2. An example showing that not all the word modifications have the
distribution in the respective text and image space can be same impact on the image: While “white” causes the color of the vase to
very different. It is difficult to mentally align the semantic change,“besidesacomputer”doesnothaveanobviousimpact.
distribution of prompts and that of the images they generate, As discussed earlier, Image Variant Graph emphasizes the
andwebelievethisisoneofthemaincausesofthedifficulties semantics difference between prompts and not their temporal
artistsface.ForPrompTHis,wechosetobasethevisualization orders. As a result, the Image Variant Graph is a complete
on the image space and include information from the text graph, but in practice, not all the edges are important for
space, since the artists’ goal is the image and not the prompt. explainingtheimagedifferences.Whenthedifferencebetween
The prompt history can be considered from two perspec- two prompts involves multiple words, the impact of these
tives.Thefirstisthetemporalevolutionthatrepresentsartist’s words on the generated images can vary. For instance, in
creative process. The second is the semantic relations among Fig. 2, the word “white” has a more prominent impact on the
various prompt-image pairs. Given that one of the main new image than the phrase “besides a computer”. A negative
goals is to help artists better understand the behaviors of the impact of showing all possible edges is that it can cause
generativemodel(R2,R3),wedecidedtoemphasizesemantic significant visual clutter. Therefore, we designed an algorithm
relationships among prompts and images, e.g., how they are to measure the edge weights, i.e., the significance of theJOURNALOFLATEXCLASSFILES,VOL.XX,NO.XX,XX 5
influenceofedges,anduseittofilteroutlessimportantedges. target images are represented by a glyph before the edge
The details of the algorithm and its usage in the layout are label. Since there might be multiple words changed between
discussed in Section IV-C and Section IV-D respectively. the sources and targets, and only changes with higher weight
We considered a few alternatives when designing the graph (please see Section IV-C for details) are shown, the glyph
model. The first choice is whether to represent the text encodes the change of each word as a slice of the circle. As
differenceandtheimagevariationinseparateviewsorcouple is shown in the bottom right of Fig. 3, the angle of the slice
theminasingleview.Wechosethelatterduetothedifficulty represents the frequency of the change among all the word
in aligning multiple text-image pairs in separate views. The modifications on the bundled edges, and the radius represents
nextdesignchoiceishowtoshowalltherelevantinformation: the weight. Slices considered as less important (with lower
text distribution, image distribution, text variations, image weight)arepresentedinlowopacity.Bluerepresentsaddition,
variations, relations between text and image distribution, and eitherinsertinganewwordortheincreasedweightofaword.
relations between text and image variations. We chose to Redrepresentsthesubtraction,eithertheremovalofawordor
focus on the differences, as these are the most important a reduction in weight. Green encodes reorder. For example,
aspectsforuserstounderstandthepromptchangeimpact(R2, in Fig. 3, the glyph, annotated with “+1girl” and “-1boy”,
R3), using position for image difference and edges for text indicates that the major cause of the image variation from the
difference. This design also provides a good representation of middleclustertothetopclusterischanging“1boy”to“1girl”.
theotherfourtypesofinformation(text/imagedistributionand
C. Edge Derivation
relationship between distribution/variation).
The workflow of deriving edges is shown in Fig. 4. We
first compare the prompts and embed and cluster the images.
The original set of edges is obtained by comparing prompts
and then bundled based on the image clusters. After that, we
calculate the edge weight, which reflects the amount of image
updatethetextchangecauses.Basedontheweights,theedges
are further merged and filtered for visualization.
Text pre-processing. The first step of text pre-processing
is to calculate the Jaccard similarity between every pair of
prompts, resulting in a distance matrix. We treat phrases the
same way as words, i.e., if several words always appear
together in the prompts, they are treated as one. Only prompt
Fig. 3. Visual encoding of Image Variant Graph. Image relationships are pairs that are relatively similar are compared. Prompt pairs
indicatedbybubblesandthewordmodificationsarerepresentedbyglyphs. with a distance higher than the predefined lower bound S
min
B. Visual Encoding are reserved. By default, S is set as 0.6, and users can
min
As is illustrated in Fig. 3, images are shown as thumbnails adjust the threshold to include more edges when the prompts
whose positions indicate the image variation. To reduce vi- varysignificantly,orexcludeedgesifmostpromptsaresimilar.
sual clutter, edges with the same word changes are bundled Foreachpairofprompts,wesplitthepromptsintowordsand
together.Lessimportantimagesarerepresentedbyrectangles. compare the words to identify the modifications. Diffusion
Image nodes. Each image is represented as a thumbnail or models allow users to set the weight of specific words or
small rectangle (if overlapped with more important images) phrases in a prompt following the given syntax. The weights
scaled proportionally to the original one. The gray scale of are parsed when splitting the prompts and each word is
the rectangle encodes the temporal order, and so does the assigned a weight value (1 by default). Fig. 5 illustrates
border of the shown image (top right of Fig. 3). When using the comparison algorithm. First, the Myers algorithm [35] is
text-to-image models, a single prompt often generates a batch applied to align the words, which identifies the insert and
of images, and the images within the same batch can vary remove modification. If a word is identified as removed in
significantly. In Image Variant Graph, the image locations the first prompt and inserted in the second prompt, it is
reflect similarity, based on which the images are clustered considered as reordered. Finally, the weights of the aligned
(more on this in Section IV-C). As a result, images from the wordsarecomparedtoidentifytheincreaseweightandreduce
same prompt could be far apart in the Image Variant Graph. weight operation. Therefore, the edge can be denoted as
If the outputs of the same prompt fall into different clusters, a e = (w,a,I ,I ), where w denotes the modified word,
src tgt
bubblewithdashedborderisadded(e.g.,bottomleftofFig.3). a denotes the modification action, I and I denotes the
src tgt
Bubbleswithfillareusedtoenhancethevisualrepresentation source image node and the target image node.
of images within the same group, i.e., in the same cluster Imagepre-processing.Inordertoshowcasethedifferences
(Fig. 3) or exploration stage (Fig. 7). between images interpreted by the generative model, images
Bundled edges. Text modifications are encoded as tapered are embedded to the two-dimensional space and grouped
edges, which is shown to be the most effective visual repre- into clusters. We take both text information and image in-
sentation of edge direction [34]. Edges share the same text formation into consideration for the embedding. Images are
modifications or the same sources and targets are bundled first encoded by the text encoder and image encoder of
together. The actual word changes between the source and CLIP [9] respectively. Each encoder transform the imagesJOURNALOFLATEXCLASSFILES,VOL.XX,NO.XX,XX 6
Fig. 4. Pipeline of edge derivation. The text pre-processing stage compares the prompts to identify the word modifications and derive the original set of
edges.Imagepre-processinginvolvesembeddingimagesbasedontextandimageencoding,combiningtheembedding,andclusteringimages.Edgesarethen
bundledbasedontheclusters.Theimpactofwordmodificationonimagechangeiscalculatedasedgeweight,whichisusedtofilteroutlow-impactedges.
word A, which is represented by an edge from cluster one to
clustertwo,probablycontributestothedifference.Weassume
thatthesumoftheedgeweightsbetweentwoimagesisalways
one. Initially, for every prompt pair, the edges between them
areassignedequalweights.Specifically,iftherearen images
1
associated with the prompt T , n images associated with the
1 2
prompt T , the weight of each edge between these two image
2
Fig.5. Three-stepcomparisonoftwopromptstoidentifyword-levelmodifi- groupsissetas1/(n 1·n 2·m),wheremisthedistancebetween
cations.First,theMyerscomparisonalgorithm[35]isappliedtocalculatethe T and T . The distance m is the number of different words
1 2
insertedanddeletedwords.Then,thechangedwordsarematchedtoidentify
which is calculated during the text preprocessing stage and
thereorderedwords.Finally,theweightsofthematchedwordsarecompared.
illustrated in Fig. 5. The weight of the bundled edge is the
sum of the weights of its child edges.
into 512-dimensional vectors. The vectors are reduced to two
dimensions through the t-SNE algorithm [36] with the cosine (cid:88)
W(E)= W(e),
distance as the metric parameter, resulting in two groups of
e∈E
image embeddings, one based on the text space and the other
based on the image space. The two spaces are aligned using where W(E) denotes the weight of the bundled edge E and
Procrustes analysis and combined to generate the final em- W(e) denotes the weight of a child edge e. However, not all
beddings. By default, the combined embedding is the average edges between two images contribute equally to the image
of the two, and users can adjust the weight of combination. variation (Fig. 2). Based on the weights of the bundled edges,
Based on the embeddings, the images are clustered using the we redistribute the weights of each individual edge.
hierarchical agglomerative clustering algorithm.
W(E)
Edge bundling. The original edges are bundled according
W(e)= ,
to the results of the image clustering, since images within (cid:80) W(E′)
e′=(w′,a′,Isrc,Itgt)
the same cluster tend to share more common features than
images between clusters. The edges representing the same where e′ denotes any edge between I and I , E (E′)
src tgt
word modification with the starting node and ending node in denotes the bundled edge that e (e′) belongs to. The weights
thesameclusterarebundledtogether.Formally,twoedgesare of the bundled edges are updated accordingly.
bundled together if and only if w, a, C(I src), and C(I tgt) are Merging and filtering. When the weights are updated,
the same for them, where C(I) denotes the ID of the cluster some multi-edges located between two images may still have
which image I belongs to. Thus, the bundled edge can be the same weight, indicating that the algorithm cannot distin-
denoted as E =(w,a,C src,C tgt). guish the difference in their impact on the image through
Weight measuring. Weight is designed to quantify the the prompt history. To reduce the abundance, we merge the
impact a text modification has on the image change. First, the multipleedgeswiththesameweightintoasingleedge.Forthe
more the changed words between two images are, the smaller merged edge, there will be multiple word modifications and a
the average impact of each word change would be. Second, glyphsummarizestheedits(SectionIV-B).Thebundlededges
between two clusters of images, edges that better align with whose weight is lower than the threshold W will not be
min
the common differences in prompts between the two groups renderedwithoutuserdemand.Bydefault,W iscalculated
min
aremorelikelytocauseimagevariations.Thatis,forexample, subjecttotheconstraintthatthereareatmostN (wesetN
E E
if word A appears frequently in prompts of cluster one and as 12) edge bundles, and users can adjust the value W to
min
is not included in cluster two, the modification of removing show fewer or more bundled edges.JOURNALOFLATEXCLASSFILES,VOL.XX,NO.XX,XX 7
D. Layout and Drawing is taken over by robotic flesh, 80s computer graphics overlay
her face,” and a detailed description “skin becomes made of
In Image Variant Graph the nodes are positioned according
leaves” is added to change the skin texture (the right image).
to the embedding project and words are positioned at the
Not all words have a stable and expected impact on the
barycenterofthesourceandtargetnodes.Toreduceclutter,we
generated results. One type of these words is concept that
only show the thumbnail of representative images and present
does not describe a certain object but can evoke associations.
the rest of the images as glyphs.
As shown in Fig. 6c, edges representing inserting “vacation”
Image rendering. The image nodes are positioned accord-
are bundled into two branches, one pointing to the same
ing to the two-dimensional embeddings obtained during the
cluster and the other pointing to the top cluster presenting
image preprocessing stage shown in Fig. 4. We measure the
a Christmas tree. Specifically, c1 is generated by “playing
weights of the nodes according to the weights of the edges,
computer in holiday”, c2 and c3 are generated by “playing
i.e., the weight of a node is the sum of the weights of edges
computer in holiday, vacation”, and c4 and c5 by “playing
that starts from or ends at it. Images are sorted in descending
computerinholiday,vacation,underaChristimastree”(there
order based on their weights. Each time the image with the
isatypointheprompt,butthemodelrecognizestheintended
largestweightisaddedifitdoesnotoverlapwithanyexisting
word“Christmas”).Although“vacation”isarelativelyabstract
node. The bubbles are drawn with bubble sets [37].
concept, it often co-occurs with “Christmas” in real-world
Word glyph positioning. Each bundled edge group has a
data. This may be the reason why the model associates the
glyph indicating the changed words. This glyph also serves
word with a Christmas tree.
as the bunding point of the edges, which is positioned at the
barycenter of the sources and targets of the edges.
V. PROMPTHISSYSTEM
PrompTHis is a prototype designed to support artists to
understand,navigate,andmanagethepromptinghistoryduring
their creative workflow with generative text-to-image models.
AsisshowninFig.7,ImageVariantGraphisthemainviewof
thesystem,whichsupportsuserstocomparethedifferencesin
prompt-imagepairs(R2)andshowsthedistributionofimages
(R1,R3,R4).Thesystemalsoprovidesarightpanelforusers
to review the detailed records (R1, R4). Users can set the
parametersfortheembeddingsandedgesviathecontrolpanel
on the top. A left panel allows users to create new images.
Image Variant Graph (Fig. 7a) is the main view of
PrompTHis. As described in Section IV, it allows users
to navigate the generated images as well as analyze the
differences in the prompts and images. In addition to the
main graph, an embedding mini-map (top left of Fig. 7a)
Fig. 6. Different patterns of the influence of word modifications on the
model’sgeneration.(a)dominance(b)fine-tuning(c)association. presentstheoverallnodedistributionandclusters.Thebottom
legend allows user to choose the way to present bubbles.
E. Results
Image Variant Graph focuses on the semantic distribution and
Through node embedding and edge bundling, Image Vari- relations,andnotthedetailsofeachsteporitstemporalorder.
ant Graph reveals how the word modifications influence the To complement this, the history box (Fig. 7b) includes the
generation.Fig.6showsthreeexamplesoftheimpactpatterns. detailed prompting records in chronological order, including
“Dominance”referstothecasewhereaslightchangeinthe all the prompts and the images generated from them. The
prompt causes a significant variation in the style or content of historyboxalsopresentsdetailedmodificationsinpromptsby
the images. For example, among the three clusters in Fig. 6a, highlightingthedifferencesofconsecutiveattemptsiftheyare
the top one is generated by prompts such as “a pig in the sky, similar,i.e.,thesimilarityishigherthanthelowerboundS
min
in monet style”, the middle by “a pig in the sky, in disney (see “text-preprocessing” in Section IV-C). The highlights use
style”, and the bottom by “a pig in the sky”, “a pig in the a consistent color mapping with the glyphs for word change.
sunny and blue sky”, etc. The edges from the bottom and Inserted and removed words are in bold style to differentiate
middle clusters converge into the top cluster, indicating the from increase weight and decrease weight. Navigation mini-
word modification, inserting “monet” dominates the style of map(Fig.7c)servesasabriefsummaryofthehistoryrecords.
theoutcome.Thedominantwordcaneventakeoverthemajor In the mini-map, each prompt is represented by a small dot,
character “pig” in the generation. For example, the top left the size of which indicates the length of the prompt. The
image is an impressionist painting and there is no pig in the color mapping of the dots is consistent with that of the Image
scene.Atothertimes,newlyaddedwordsintroduceadditional Variant Graph, i.e., the temporal order of the prompts. Each
featurestothepreviouspromptwithoutdestructingtheoriginal pair of similar prompts is represented by an arc linking the
semantic, allowing for fine-tuning to the image. As shown in twocorrespondingdots.Foreachdot,thelinktopriordots(if
Fig.6b,theleftimageisgeneratedbyprompt“ablackwoman exists) with the highest similarity is emphasized with bolderJOURNALOFLATEXCLASSFILES,VOL.XX,NO.XX,XX 8
Fig. 7. The interface of PrompTHis, which consists of Image Variant Graph (a), a history box (b), a navigation mini-map (c), a control panel (d), and a
creationpanel(e).Thisfigureshowsthepromptingrecordsofanartist.Startingfromablack-and-whitedrawingofcitybuildings(1-5),theartistexperimented
withcolorstyles(6-7,8-10),andreturnedtotheblack-and-whitestyle(11-14),with“atomicexplosion”insertedlater(15).
and darker stroke. The line segments on the right of the dots manner from the history box. When observing interesting or
represent different stages of exploration. By default, a step is desirable images, they can copy the prompt to the input area
consideredasthebeginningofanewstageifthepromptisnot for next step of generation. Users can also select prompts,
similar to the previous step. Users can change the division of images, and words to compare the attempts and analyze the
stages by clicking the gap between two lines to connect them model’sbehavior.Themajorinsightfromsuchanalysisishow
or clicking on a line to divide it. thewordmodificationinfluencesthegeneration,whichcanbe
Thecontrolpanel(Fig.7d)allowsusertosettheparameters leveraged to decide whether to include certain words in the
for the visual presentation. The left button “IVG” controls new attempt and help improve the prompt. More details and
whether to show Image Variant Graph. The next four sliders examples about the usage of the system can be found in the
set the weight of combining the text and image embeddings, results of the qualitative evaluation (Section VI-B).
thesimilaritythresholdS ,theweightthresholdW ,and
min min
distance threshold to control the number of clusters. Changes VI. EVALUATION
inthethresholdswillupdatetheImageVariantGraph(seethe
The evaluation of PrompTHis includes two parts, the first
calculationinSectionIV-C).Userscancreatenewsessionsand
one is a quantitative user study to evaluate the system on
enter prompts through the creation panel (Fig. 7e). Currently
specifictasks(describedinSectionVI-A),andthesecondisa
PrompTHis is connected to a Stable Diffusion model (version
qualitative evaluation with potential users to better understand
1.5) which is open-sourced and fast in generation so that we
the system’s usability and effectiveness in supporting creative
can easily test the prototype in real-time creation. The other
explorations (described in Section VI-B).
views are updated once new images are generated.
A. Quantitative User Study
1) Participants and Process
We recruited 11 post-graduate students including two females
for evaluating the usefulness of PrompTHis. All participants
reported that they have tried generative AI before and graded
4.45/5 on average on their degree of familiarity to text-to-
image models. However, they are less familiar with prompt
engineering (3.78/5 on average).
We aimed to investigate whether PrompTHis helps in the
review and analysis of prompt history, i.e., R1, R2, and R3,
and designed corresponding tasks. Participants started with
Fig.8. TheexplorationpipelineofPrompTHis.Userscanreviewandanalyze
previous attempts. They can leverage insights of word influences to refine trainingonhowtousePrompTHisandpracticedtext-to-image
prompts.Thenewgenerationupdatestheviews,allowingfurtheranalysis. generationthroughfreeexploration.Then,theyanalyzedthree
Fig. 8 illustrates the exploration pipeline of PrompTHis. pre-recorded sessions to complete the tasks. One of the three
Basically, users can review the prompts and images either at sessions was manually recorded by the artist we interviewed
a macro level from the Image Variant Graph or in a detailed (Section III), with 16 steps in total. The other two wereJOURNALOFLATEXCLASSFILES,VOL.XX,NO.XX,XX 9
generatedbyamateurusersthathadusedthesystemforopen- familiarwithDiscoDiffusionandBlockadelabsSkybox.P5is
ended exploration, with 15 steps and 26 steps, respectively. adesignerwith7yearsofartexperienceininteractiondesign,
For each session, users were assigned with three tasks. The who has used Midjourney before but not very experienced in
first task (T1) was to review the history and identify the generative art. P6 is a postgraduate student majoring in com-
explorationstages(R1).Thesecondone(T2)wastocompare puter science who has not received professional art training
thepromptsbetweentwogivenimageclustersandidentifythe andonlyhaveseveralattemptsatStableDiffusion.Intermsof
key words that lead to the variation (R2). The third (T3) was the familiarity with visualization and visual analytics, P4 had
to summarize the model’s sensitivity to given words (R3). basic knowledge of visualization charts, P1, P2, and P5 were
2) Results familiar with information visualization but not experienced
For each task, the ground truth is a list or set of descriptions withvisualanalytics,whileP3andP6specializedinthisfield.
represented as keywords, i.e., a list of themes for T1, a set Theinterviewbeganwitha15-minutetrainingontheusage
of words for T2, and a set of keywords describing the word of the system. Participants that had less experience with
influence for T3. The maximum score is “5” if an answer generative models were given a bit more time (around ten
contains all the expected keywords (and in the correct order minutes) to do a creative session with the baseline system
if the answer is expected to be a list). Otherwise, the score (a limited version of PrompTHis, i.e., only history box and
is computed as the proportion of correct keywords out of 5. mini-map) so that they could compare the experience and that
Participantsachieveda82.21%accuracytoidentifytheimage withPrompTHis.Then,participantshadaround20minutesto
themes during the creation, which means that participants iteratively generate images for a topic. Participants can either
can easily distinguish the image spaces involved. Besides, propose their own topic or choose one from 10 pre-selected
most participants reported the differences between clusters topics, which are conceptual themes commonly discussed in
correctly(withanaccuracyof96.92%).However,whenasked abstract art and allow a vast exploration space. Participants
to identify the influences of certain words, some participants wereencouragedtothinkaloudduringtheprocess.Afterthat,
focused on the most salient variation, while overlooked the participants used the system to recount their creative session
distinct impact when the word is involved in other context. within 10 minutes. The last 15 minutes was a semi-structured
This leads to a relatively low accuracy (78.12%). Fig. 9 interview on participants’ experience and feedback.
shows the participants’ ratings regarding the usefulness of 2) Data Analysis
PrompTHis. Participants found the edges especially useful for All the sessions were conducted through video conferences,
learning how the changes to words would affect the model’s whichwererecordedandtranscribed.Thepromptsandimages
performance, for example, P1 identified magic words which created in the interviews were automatically recorded by the
would lead to surprising outcomes. The user study demon- PrompTHis system. We conducted a thematic analysis [38] of
strates that PrompTHis can help users review the creative the interview data. We started with theoretical analysis using
process and make sense of the generative model through the requirements as the themes with a special focus on how
efficient comparison of prompt-image pairs. the requirements were fulfilled (described in Section VI-B-3).
Then, we conducted inductive analysis of other information
Q1: Easy-to-use 4 7 4.64±0.50
relevant to the use of the system and generative AI (discussed
Q2: Show overall process 2 4 5 4.27±0.79
in Section VI-B-4).
Q3: Show details 5 6 4.55±0.52 3) Results
Q4: Support comparison 2 4 5 4.27±0.79 Basedontheobservationandfeedbackfromtheinterview,we
Q5: Show prompt influences 3 2 6 4.27±0.90 summarize the instances of the targeted tasks related to the
1 - Strongly Disagree 2 - Disagree 3 - Neural 4 - Agree 5 - Strongly Agree requirements and how PrompTHis help users complete them.
Fig. 9. Rating for the usefulness of PrompTHis in assisting users’ analysis R1.Review.Allparticipantsfrequentlyusedhistoryboxto
ofthecreativeprocess.
review their attempts and identify desirable images based on
B. Qualitative Study which to refine the prompts. In comparison, Image Variant
1) Participants and Process Graph was typically referred to after a certain period of
To understand how PrompTHis can support creative process, exploration, serving as an overview and provided a new
we conducted qualitative interviews with both artists and am- perspective on the images and creative process.
ateurusers.Werecruitedamateursaswewantedtoinvestigate • Gobacktopreviousattempts.Itisacommonthatthegen-
whether the system could benefit non-professional users and eratedimagesdeviatefurtherfromexpectationsafterseveral
whether there are differences in the exploration pattern and steps. In such cases, participants were observed to locate a
needs between the two user groups. previous attempt that was relatively satisfactory using the
We recruited 6 users for the interview. Four of them (P1- history box. P1 found mini-map particularly useful as “it
P4) are university professors who study, teach, and practice illustratestherecursivemodificationsandhelpspinpointthe
visualart.P3andP4aretheartiststhatweinterviewedduring frequently revisited attempts.”
the design stage (Section III). All four participants have more • Help recount creative session. Participants could quickly
than 20 years of professional art experience, and frequently recall their previous attempts with Image Variant Graph
utilize generative models to explore and pre-produce artistic in the recount session, “especially when there is a large
ideas. P1, P2, and P3 primarily use Midjourney and have amount of steps” (P3). P1 and P6 also found the edges
also tried Stable Diffusion, DALL-E and GPT4V, while P4 is useful for grasping the major changes they attempted.JOURNALOFLATEXCLASSFILES,VOL.XX,NO.XX,XX 10
• Provide a new perspective. P1 and P4 commented that was not satisfied. By reviewing the previous session, which
Image Variant Graph provided them with a different per- was to generate a cartoon pig, and observing the edges of
spective, which is helpful for understanding and navigation. replacing less popular styles with “Monet,” P3 recalled that
“The graph enables me to ignore the prompts and look at the model performed better on the style of “Monet” and
the images on their own merits” (P4). Similarly, P1 noticed decided to generate an apple in this style.
some images, though not aligned with the initial intention,
R4.Plan.Someobservationsdiscussedsofaralreadyshow-
looked good in other aspects.
cased how participants designed new prompts with the aid
R2. Compare. During creation, P1, P5, and P6 engaged of PrompTHis. Here we summarize some typical exploration
in comparing the prompts and images through Image Variant patterns of the participants and demonstrate how the system
Graph. P2, P3, and P4, however, focused on the history box, facilitates the planning.
observing whether the outputs align with their intentions.
• Improve prompts towards a clear target. P1, P2, and
• Observe result of prompt change. While it is easy to P5 had a target image in mind before starting, which is a
compare consecutive attempts in the history box, P1 found common practice in AI-assisted design and pre-production.
it more intuitive to observe the change on Image Variant P2 appreciated the rationality and effectiveness of Image
Graph, as “it indicates the distance between images and Variant Graph in assisting prompt engineering, “the tool
different levels of impacts of the changed words.” makes sense to me as it can help me understand how to
• Search for similar images. Upon obtaining a satisfactory improve my prompts.”
image, P5 explored its neighbors in Image Variant Graph • Adjust the target according to outputs. In non-industrial
and identified two other similar images. He then compared settings, there is usually more flexibility in deciding the
thepromptsofthethreeimages,whichwerequitedifferent, final representation of an art work. While started with a
and selected the common phrases for the new attempt. certain goal, P3 adapted the generated scene to model’s
• Observe prompt stability. All participants agreed that the capability, e.g., involving more characters in the story when
bubbles indicating images generated by the same prompt themodelfailedtogenerateimageswithexactonecharacter.
increased their awareness of prompt stability. “The images PrompTHis helped P3 make adjustments based on knowl-
generatedbyonepromptcanbequitedifferent.Ibelievethe edge of model behavior (as described in R3).
visualization helps to identify more stable prompts” (P2). • Explore realizations of abstract idea.P4aimedtoexplore
R3. Model behavior. We observed that PrompTHis help afilmidea,whichwasmoreconceptualandopen.Through-
participants improve the knowledge about the general model out the exploration there were quite a few inspiring images
behavior, e.g., the model is not good at creating things that that served as starting point of new branches and variations
do not exist, as they experimented with different prompts, but of the idea, which the participant frequently went back to
“the more you explore the same idea, the more muddy it gets. through history box to start a new series of attempts.
It’s like casting a fishing line, and if you throw it in different • Help build exploration mental map. P6 found Image
spaces, you get different versions” (P4). Image Variant Graph Variant Graph effective in revealing the unexplored space
couldreducesuchconfusion,providingadditionalinsightsinto and helpful to construct a mental map of the creative space.
the macro model characteristics. “When creating with the baseline system, I often focused
on the most recent steps and was unwilling to branch out.”
• Distinguish influence of certain phrase. It could be tricky
ImageVariantGraph,however,remindedP6oftheprevious
to distinguish the roles of different stylistic and descriptive
attempts,motivatingandguidinghimtocombinetheknowl-
terms whenthey are mixed inone prompt. P5 wentthrough
edgelearnedinbothstagesandidentifyingunexploredspace
trials and errors with different combination of phrases in
that might be promising. “The graph helps me adjust the
the baseline session but could not identify any clear rule.
combination, I could imagine where the desired results are
When recounting the session with Image Variant Graph, he
intheembeddingspaceandfine-tunepromptsaccordingly.”
realized that “light red” somewhat conflicted with “Chinese
painting,” adding modern elements to the outputs, which 4) Discussion
were expected to be in the traditional style. Onthewhole,allparticipantsagreedthathistoryboxishelpful
• Reasoning causes of unsatisfactory images. During the and critical to the creative process. While P1, P5, and P6
creative session with PrompTHis, P5 got a bit stuck and leveraged Image Variant Graph for real-time planning, P2,
could not further improve the outputs. By examining the P3, and P4 thought Image Variant Graph is more useful for
stagebubbleonImageVariantGraph,P5identifiedtheedge, reviewing previous attempts.
i.e.,inserting“albumcover,”whichcontributedtothegroup Attention and interest. Participants’ preferences on the
ofunsatisfactoryimages.ThisobservationhelpedP5remove amount of information shown in Image Variant Graph vary
the phrase in following attempts, leading to better results. when they have different tasks. P1, P2, and P3 suggested that
• Recall insights to control generation. Sometimes, par- since the capacity of human attention is limited, during the
ticipants might forget the knowledge accumulated during creativeprocess,itwouldbedistractingifwepresenttoomany
the exploration. The history box and Image Variant Graph nodesandedgesonthegraphwithoutdistinguishingthelevels
served as a reminder and assisted P3 in recalling the of emphasis. P1 and P2 proposed using the size of image to
knowledge gained to avoid repetitive failures. For example, encode levels of user interest. In contrast, when engaged in
onceP3experimentedwithappleinthestyleof“ruthko”and review and analysis, P4 found it more effective to show moreJOURNALOFLATEXCLASSFILES,VOL.XX,NO.XX,XX 11
images,especiallythosesimilartothedesiredones.P5andP6 theirimpactsasedgesbetweenimagenodesthatareprojected
demanded more textual information, e.g., common phrases in according to their text and image similarity. Thus, users can
prompts of focused images, to aid them in refining prompts. observethesemanticdistributionaswellasanalyzetheeffects
CurrentlythePrompTHisismainlydesignedtosupportreview of prompt modifications. PrompTHis allows users to directly
andrecount.Wewillincludethesefeedbackinthefuturework interact with a generative image model, a time-oriented view
for guidance and recommendation. for prompting history, and several additional features to sup-
Capture complete context. With the rapid development port the creative process. Both a quantitative and a qualitative
of generative models and tools, prompt engineering is not the user study were conducted to evaluate the effectiveness of
only way to control the generation. For example, P1 and P3 ImageVariantGraphandPrompTHis.Participanthighlyrated
useshand-drawnsketchesortheirownartworkasimageseeds the usability of both, and the qualitative results revealed how
to specify the desired object and style. Though PrompTHis thefeaturesinImageVariantGraphandPrompTHishelpuser
currentlyfocusesonprompt-imagepairs,itisimportanttotake better completing the targeted tasks.
other context, e.g., seed image, parameter, and image editing, Generative art, which involves both human and AI models
into consideration. Besides, P2 envisioned the capability to to achieve creative goals, has presented challenges and op-
comparing different models, and it would be interesting to portunities for visualization research. PrompTHis is an initial
capture and integrate the explorations across tools. step towards understanding and utilizing individual creation
Organization and curation. Currently PrompTHis allows history. Based on the results and feedback we received, future
users to organize their attempts into different sessions. How- work will focus on the following aspects:
ever,P2wishedmoreadvancedandflexibleorganization,such 1) Improvement of the methods, layout, and encoding of
astaggingtheimagesandarrangingthemalongastoryline(if ImageVariantGraphtoenhancereadabilityandusability
the goal is to explore ideas around a film). We also observed for better real-time support.
thatP4savedandannotatedinspiringoutcomesinadocument 2) Complete provenance capture and support for a wider
asexternalizationoftheideas,sothathecouldreflectonwhat range of user types in more realistic settings.
resonates with the original idea later. All the artists expressed 3) Personalized recommendation for artists based on explo-
their willingness and needs to curate the exploration history, ration provenance and user preference.
e.g., rating and pinning the generated images, taking notes of
the attempts, etc. Such organization and curation could form ACKNOWLEDGMENTS
partofcontextincreativeprovenanceandbeleveragedtoinfer
This work is supported by NSFC No. 62272012.
user intention and preference.
Accurate understanding of user preference. All the
REFERENCES
participants with professional art training attempted to accu-
rately control the outputs to realize their goals. They either [1] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer,
“High-Resolution Image Synthesis with Latent Diffusion Models,” in
had a clear picture in mind before the generation, or had
IEEE/CVF Conference on Computer Vision and Pattern Recognition
accurate senses of the desired features, e.g., the composition, (CVPR). NewOrleans,LA,USA:IEEE,2022,pp.10674–10685.
environment, and emotion, even though the process can be [2] A.Ramesh,P.Dhariwal,A.Nichol,C.Chu,andM.Chen,“Hierarchical
Text-ConditionalImageGenerationwithCLIPLatents,”arXivpreprint
exploratory. For the latter case, “translating internal senses
arXiv:2204.06125,2022.
and feelings into prompts that the model can understand [3] Y.Wang,S.Shen,andB.Y.Lim,“Reprompt:Automaticpromptediting
becomes even more important and challenging” (P3). The torefineai-generativearttowardspreciseexpressions,”inProceedings
oftheCHIConferenceonHumanFactorsinComputingSystems,2023,
senses and preferences are a reflection of the artist’s style
pp.1–29.
and inspiration. P3 expressed the concerns with the “cre- [4] Y.Feng,X.Wang,K.K.Wong,S.Wang,Y.Lu,M.Zhu,B.Wang,and
ativity” with current generative models, “simply combining W.Chen,“Promptmagician:Interactivepromptengineeringfortext-to-
imagecreation,”arXivpreprintarXiv:2307.09036,2023.
many styles and elements together might create something
[5] S. Brade,B. Wang, M.Sousa, S.Oore, and T.Grossman, “Promptify:
that looks new, but it is like to go from 1 to 99, instead Text-to-image generation through interactive prompt exploration with
of creating something original.” Dataset-based or LLM-based largelanguagemodels,”arXivpreprintarXiv:2304.09337,2023.
[6] F. B. Vie´gas, M. Wattenberg, and K. Dave, “Studying cooperation
recommendations have been proven to generate appealing
and conflict between authors with history flow visualizations,” in Pro-
images favored by public users, but the artists hope the model ceedings of the SIGCHI Conference on Human Factors in Computing
totrulyunderstandtheirpersonalstylesandartistictastes.Itis Systems. ACM,2004.
[7] F. Chevalier, P. Dragicevic, A. Bezerianos, and J.-D. Fekete, “Using
a promising direction to understand user preference and make
textanimatedtransitionstosupportnavigationindocumenthistories,”in
recommendations based on exploration provenance. ProceedingsoftheSIGCHIConferenceonHumanFactorsinComputing
Systems. ACM,2010.
[8] Y. Guo, Q. Han, Y. Lou, Y. Wang, C. Liu, and X. Yuan, “Edit-history
VII. CONCLUSIONANDFUTUREWORK vis: An interactive visual exploration and analysis on wikipedia edit
history,”inIEEEPacificVisualizationSymposium. IEEE,2023.
This work proposes Image Variant Graph to help artists [9] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal,
G.Sastry,A.Askell,P.Mishkin,J.Clarketal.,“Learningtransferable
understandtheinfluencesofpromptmodificationsduringtext-
visual models from natural language supervision,” in International
to-image generation. It is part of the PrompTHis, a visual ConferenceonMachineLearning. PMLR,2021,pp.8748–8763.
analytics system for users to review and understand the [10] A.Ramesh,M.Pavlov,G.Goh,S.Gray,C.Voss,A.Radford,M.Chen,
and I. Sutskever, “Zero-shot text-to-image generation,” in Proceedings
prompting history for a more effective creative process. The
of the International Conference on Machine Learning, M. Meila and
Image Variant Graph models the differences in prompts and T.Zhang,Eds.,vol.139. PMLR,2021,pp.8821–8831.JOURNALOFLATEXCLASSFILES,VOL.XX,NO.XX,XX 12
[11] K.Crowson,S.Biderman,D.Kornis,D.Stander,E.Hallahan,L.Cas- [33] Midjourney. Midjourney, https://www.midjourney.com/. Accessed
tricato, and E. Raff, “Vqgan-clip: Open domain image generation February,2023.
and editing with natural language guidance,” in European Conference [34] D. Holten and J. J. Van Wijk, “A user study on visualizing directed
onComputerVision,S.Avidan,G.Brostow,M.Cisse´,G.M.Farinella, edges in graphs,” in Proceedings of the SIGCHI conference on human
and T. Hassner, Eds. Cham: Springer Nature Switzerland, 2022, pp. factorsincomputingsystems,2009,pp.2299–2308.
88–105. [35] E. W. Myers, “Ano (ND) difference algorithm and its variations,”
[12] C. Zhang, C. Zhang, M. Zhang, and I. S. Kweon, “Text-to-image Algorithmica,vol.1,no.1,pp.251–266,1986.
Diffusion Models in Generative AI: A Survey,” arXiv preprint [36] L.VanderMaatenandG.Hinton,“Visualizingdatausingt-sne.”Journal
arXiv:2303.07909,2023. ofMachineLearningResearch,vol.9,no.11,2008.
[13] J. Oppenlaender, “A taxonomy of prompt modifiers for text-to-image [37] C. Collins, G. Penn, and S. Carpendale, “Bubble sets: Revealing set
generation,”arXivpreprintarXiv:2204.13988,vol.2,2022. relationswithisocontoursoverexistingvisualizations,”IEEETrans.Vis.
[14] V. Liu and L. B. Chilton, “Design guidelines for prompt engineering Comput.Graph.,vol.15,no.6,pp.1009–1016,2009.
text-to-imagegenerativemodels,”inProceedingsoftheCHIConference [38] V. Braun and V. Clarke, “Using thematic analysis in psychology,”
onHumanFactorsinComputingSystems,2022,pp.1–23. Qualitativeresearchinpsychology,vol.3,no.2,pp.77–101,2006.
[15] A. Mishra, U. Soni, A. Arunkumar, J. Huang, B. C. Kwon, and
C.Bryan,“Promptaid:Promptexploration,perturbation,testingandit-
YuhanGuoisaPhDstudentattheSchoolofIntel-
erationusingvisualanalyticsforlargelanguagemodels,”arXivpreprint
ligenceScienceandTechnology,PekingUniversity.
arXiv:2304.01964,2023.
She received a B.S. degree in intelligence science
[16] H. Strobelt, A. Webson, V. Sanh, B. Hoover, J. Beyer, H. Pfister, and
and technology from Peking University in 2023.
A. M. Rush, “Interactive and visual prompt engineering for ad-hoc
Herresearchinterestsincludetextvisualizationand
taskadaptationwithlargelanguagemodels,”IEEETrans.Vis.Comput.
visualizationforhumanities.Herrecentresearchfo-
Graph.,vol.29,no.1,pp.1146–1156,2022.
cusesonvisualizationofprovenancedataandvisual
[17] S. Lee, B. Hoover, H. Strobelt, Z. J. Wang, S. Peng, A. Wright,
analytics for sensemaking tasks through human-AI
K. Li, H. Park, H. Yang, and D. H. Chau, “Diffusion explainer:
collaboration.
Visual explanation for text-to-image stable diffusion,” arXiv preprint
arXiv:2305.03509,2023.
[18] J. J. Y. Chung and E. Adar, “Promptpaint: Steering text-to-image
generation through paint medium-like interactions,” arXiv preprint HanningShaoisnowaPh.D.studentattheSchool
arXiv:2308.05184,2023. ofIntelligenceScienceandTechnology,PekingUni-
[19] Z.J.Wang,E.Montoya,D.Munechika,H.Yang,B.Hoover,andD.H. versity.HereceivedaB.S.degreeincomputersci-
Chau, “Diffusiondb: A large-scale prompt gallery dataset for text-to- ence from Peking University in 2021. His research
imagegenerativemodels,”arXivpreprintarXiv:2210.14896,2022. interestsincludescientificvisualization.
[20] T. Yousef and S. Janicke, “A survey of text alignment visualization,”
IEEETrans.Vis.Comput.Graph.,vol.27,no.2,pp.1149–1159,2020.
[21] S.Ja¨nickeandD.J.Wrisley,“Interactivevisualalignmentofmedieval
text versions,” in IEEE Conference on Visual Analytics Science and
Technology(VAST). IEEE,2017,pp.127–138.
[22] F.Moretti,Graphs,maps,trees:abstractmodelsforaliteraryhistory.
Verso,2005.
[23] S. Ja¨nicke, G. Franzini, M. F. Cheema, and G. Scheuermann, “On Can Liu received a B.S. degree in computer sci-
ence and a B.E. degree in economics from Peking
close and distant reading in digital humanities: A survey and future
challenges.”EuroVis(STARs),vol.2015,pp.83–103,2015. Universityin2018,andreceivedaPh.D.Degreeat
theSchoolofIntelligenceScienceandTechnology,
[24] M. Alharbi, R. S. Laramee, and T. Cheesman, “Transvis: integrated
Peking University in 2023. His research interests
distant and close reading of othello translations,” IEEE Trans. Vis.
lieinthefieldofdeeplearning-drivenvisualization,
Comput.Graph.,vol.28,no.2,pp.1397–1414,2020.
especiallyintelligentinteractionforvisualization.
[25] D.Bertucci,M.M.Hamid,Y.Anand,A.Ruangrotsakun,D.Tabatabai,
M.Perez,andM.Kahng,“DendroMap:Visualexplorationoflarge-scale
image datasets for machine learning with treemaps,” IEEE Trans. Vis.
Comput.Graph.,pp.1–11,2022.
[26] X.Xie,X.Cai,J.Zhou,N.Cao,andY.Wu,“Asemantic-basedmethod
for visualizing large image collections,” IEEE Trans. Vis. Comput.
Kai Xu is an Associate Professor in the School of
Graph.,vol.25,no.7,pp.2362–2377,2019.
Computer Science at the University of Nottingham
[27] J. Yang, J. Fan, D. Hubball, Y. Gao, H. Luo, W. Ribarsky, and
inUK.Heistheco-directorofSchool’sVisualiza-
M.Ward,“Semanticimagebrowser:Bridginginformationvisualization
tion Research Group. His main research interest is
with automated intelligent image analysis,” in IEEE Symposium on
Data Science, particularly Data Visualization. His
VisualAnalyticsandTechnology. IEEE,2006.
recentworkfocusesondesigninginteractivevisual
[28] P.JanecekandP.Pu,“Searchingwithsemantics:Aninteractivevisual-
interfaces for human-AI teaming. He received his
izationtechniqueforexploringanannotatedimagecollection,”inOTM
BEnginComputerEngineeringfromShanghaiJiao-
Confederated International Conferences “On the Move to Meaningful
tongUniversityin1999andlaterPhDinComputer
InternetSystems”. Springer,2003,pp.185–196.
SciencefromUnivesrityofQueenslandinAustralia
[29] E.Ragan,A.Endert,J.Sanyal,andJ.Chen,“CharacterizingProvenance
in2004.
in Visualization and Data Analysis: An Organizational Framework of
Provenance Types and Purposes,” IEEE Trans. Vis. Comput. Graph.,
vol.22,no.1,pp.31–40,2016. Xiaoru Yuan received a B.S. degree in computer
[30] K. Xu, A. Ottley, C. Walchshofer, M. Streit, R. Chang, and J. Wen- science and a B.A. degree in law from Peking
skovitch,“Surveyontheanalysisofuserinteractionsandvisualization University in 1997 and 1998, respectively. In 2005
provenance,”inComputerGraphicsForum,vol.39,no.3. WileyOnline and 2006, he received an MS degree in computer
Library,2020,pp.757–783. engineeringandaPh.D.degreeincomputerscience
[31] K.Xu,S.Attfield,T.Jankun-Kelly,A.Wheat,P.H.Nguyen,andN.Sel- from the University of Minnesota. He is now a
varaj, “Analytic Provenance for Sensemaking: A Research Agenda,” professoratPekingUniversityintheLaboratoryof
IEEE Computer Graphics and Applications, vol. 35, no. 3, pp. 56–64, Machine Perception (MOE). His primary research
2015. interests lie in scientific visualization, information
[32] J.Wenskovitch,M.Zhou,C.Collins,R.Chang,M.Dowling,A.Endert, visualization,andvisualanalytics,emphasizinglarge
and K. Xu, “Putting the “I” in Interaction: Interactive Interfaces Per- data visualization, high dimensional data visualiza-
sonalized to Individuals,” IEEE Computer Graphics and Applications, tion,graphvisualization,andnovelvisualizationuserinterface.Heisasenior
vol.40,no.3,pp.73–82,2020. memberoftheIEEE.