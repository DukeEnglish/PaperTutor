HDR: Talking to Machines: do you read me?
PartI:Scientificcontributions
Lina Mar´ıa ROJAS BARAHONA
MEMOIRE
pourl’obtentiondel’
Habilitation de l’Universite´ de Lorraine
(Spe´cialite´ Informatique)
Ecole doctorale IAEM Lorraine
Rapporteurs
Prof. Olivier Pietquin. Universite´ de Lille/Cohere.
DR. Chloe´ Clavel. INRIA-Paris.
Prof. Giusseppe Riccardi. University of Trento
Examinateurs
Prof. Yannick Toussaint. LORIA/Universite´ de Lorraine.
Prof. Slim Ouni. LORIA/Universite´ de Lorraine.
MC. Jean-Charles Lamirel. LORIA/Universite´ de Lorraine.
Invite´s
DR. Claire Gardent. LORIA/CNRS.
CR. Christophe Cerisara. LORIA/CNRS.
4202
luJ
2
]LC.sc[
1v45320.7042:viXra2To my daughter Soph´ıa, the bright light of my life. To my husband Herve´ Vaultrin, who has been
always embracing me with his love, support and patience. To my brother Oscar and my sister
Beatrizwhohavebeenthereformeandgavememybelovednephewsandniece: Alejandro,Jose´
Francisco and Mar´ıapaz. To my aunt Beatriz Barahona, who has always stood by my side being
my second mother. I also dedicate this work as many others in the past to my parents, Aquiles
RojasandRuthBarahona,whoarealwaysbesideme,supportingmeandgivingmetheirstrength
even though they are enjoying the splendid experience of eternity. Thank you all for being the
sourceofmyinspiration.
Tomybeautifulcountry,mybelovedColombia.
iiiContents
1 Introduction 1
2 AGlancetotheResearchonDialogue 3
2.1 Whyishumanconversationdifficult? . . . . . . . . . . . . . . . . . . . . . . . 4
2.2 PreliminaryApproaches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
2.3 TaskOrientedDialogueSystems . . . . . . . . . . . . . . . . . . . . . . . . . . 5
2.3.1 Definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
2.3.2 StatisticalDialogueSystems . . . . . . . . . . . . . . . . . . . . . . . . 6
2.3.3 End-to-EndTask-OrientedSystems . . . . . . . . . . . . . . . . . . . . 9
2.4 End-to-EndDialogues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
2.5 Pre-trainedLanguageModels . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
2.6 ConversationalQuestionAnswering . . . . . . . . . . . . . . . . . . . . . . . . 11
2.7 PositioningMyContributionsintheState-of-the-Art . . . . . . . . . . . . . . . 12
3 ContributionstoTask-OrientedDialogues 15
3.1 LanguageUnderstanding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
3.1.1 NLUinaSeriousGame . . . . . . . . . . . . . . . . . . . . . . . . . . 16
3.1.2 SpokenLanguageUnderstandingandFew-ShotLearning . . . . . . . . . 23
3.2 DialogueManager . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
3.2.1 BayesianInverseReinforcementLearning . . . . . . . . . . . . . . . . . 33
3.2.2 IstheUserEnjoyingtheConversation? . . . . . . . . . . . . . . . . . . 37
3.2.3 ImitationLearning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
3.2.4 HierarchicalImitationLearning . . . . . . . . . . . . . . . . . . . . . . 47
4 ContributionstoConversationalQuestionAnswering(QA)andOtherContributions 53
4.1 DetectionofEllipsisandCo-referenceinconversationalcorpora . . . . . . . . . 54
4.1.1 Activelearning(AL) . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
4.1.2 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57
4.1.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58
4.2 QuestionRewriting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59
4.2.1 Annotations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60
4.2.2 EvaluationonQuestionRewriting(QR) . . . . . . . . . . . . . . . . . . 60
4.2.3 EvaluationonConversationalQuestionAnswering . . . . . . . . . . . . 62
4.3 AconversationalQAcorpusgroundedinWikidata . . . . . . . . . . . . . . . . 64
4.4 OtherContributions: GraphEmbeddings . . . . . . . . . . . . . . . . . . . . . . 64
iii5 DataCollection,AnnotationandFrameworks 73
5.1 Corpora . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73
5.1.1 TheFrenchPortmediaCorpus . . . . . . . . . . . . . . . . . . . . . . . 73
5.1.2 TheFrenchEmospeechCorpus . . . . . . . . . . . . . . . . . . . . . . 74
5.1.3 AnnotatingPostsFollowingCognitiveBehaviouralTherapyPrinciples . 76
5.1.4 ConversationalQuestionAnsweringwithRewriting . . . . . . . . . . . 77
5.1.5 ConversationalQuestionAnsweringinFrench . . . . . . . . . . . . . . 78
5.1.6 ConversationalQAgroundedinWikidata . . . . . . . . . . . . . . . . . 78
5.2 DialogueFrameworks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78
5.2.1 PyDial . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78
5.2.2 Dialport . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78
5.2.3 ConversationalSearchforGeneralKnowledge . . . . . . . . . . . . . . 79
6 ScientificProject 81
6.1 LLMsforTask-OrientedDialogue . . . . . . . . . . . . . . . . . . . . . . . . . 82
6.1.1 BenchmarcksandEvaluation . . . . . . . . . . . . . . . . . . . . . . . . 82
6.1.2 InterpretingLLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82
6.1.3 RetrievalAugmentedGeneration . . . . . . . . . . . . . . . . . . . . . . 83
6.1.4 ControllingDecoding . . . . . . . . . . . . . . . . . . . . . . . . . . . 83
6.2 Multimodaltask-orienteddialogues . . . . . . . . . . . . . . . . . . . . . . . . 84
6.2.1 Exploitingweakspeechsignalsintherewardfunction . . . . . . . . . . 84
7 Conclusion 87
ivList of Acronyms
TOD Task-OrientedDialogues
DSs DialogueSystems
MDP MarkovDecisionProcess
POMDP PartiallyObservableMarkovdecisionprocess
NLP NaturalLanguageprocessing
SLU SpokenLanguageUnderstanding
NLU NaturalLanguageUnderstanding
DST DialogueStateTracking
NLG NaturalLanguageGeneration
ASR AutomaticSpeechRecognition
DM DialogManager
ML MachineLearning
QA QuestionAnswering
IL ImitationLearning
LR LogisticRegression
SVM SupportVectorMachines
CNN ConvolutionalNeuralNetwork
LSTM Long-ShortTermMemory
GNN GraphNeuralNetwork
DIP DomainIndependentParametrisation
IL ImitationLearning
IRL InverseReinforcementLearning
GNN GraphNeuralNetwork
QA QuestionAnswering
CoQAR ConversationalQuestionAnsweringwithRewriting
KGConv Knowledge-baseConversations
KG KnowledgeGraph
vCQA ConversationalQuestionAnswering
CS4GK ConverstionalSearchforGeneralKnowledge
DIANA DIalogueinNAturalLanguage
CBT CognitiveBehaviouralTherapy
CIFRE Conventionsindustriellesdeformationparlarecherche
NL4XAI InteractiveNaturalLanguageTechnologyforExplainableArtificialIntelligence
COBRA ConversationalBrains
ITN InnovativeTrainingNetworks
RL ReinforcementLearning
LLMs LargeLanguageModels
VQA VisualQuestionAnswering
RAG RetrievalAugmentedGeneration
AI ArtificialIntelligence
IQ InteractionQuality
viChapter 1
Introduction
‘Desde entonces no me gane´ un centavo que no fuera con la ma´quina de es-
cribir, y esto me parece ma´s meritorio de lo que podr´ıa pensarse, pues los
primerosderechosdeautorquemepermitieronvivirdemiscuentosynovelas
melospagaronaloscuarentaytantosan˜os,despue´sdehaberpublicadocuatro
librosconbeneficios´ınfimos. Antesdeesomividaestuvosiempreperturbada
por una maran˜a de trampas, gambetas e ilusiones para burlar los incontables
sen˜uelos que trataban de convertirme en cualquier cosa que no fuera escritor.’
—GabrielGarc´ıaMarquez,VivirparaContarla.
‘Since then I haven’t earned a penny other than with the typewriter, and this
seems to me more meritorious than one might think, since the first author
rights that allowed me to live from my tales and novels were paid to me when
Iwasinmyforties,somanyyearsafterhavingpublishedfourbookswithjust
tiny benefits. Before that my life was always disturbed by a tangle of traps,
tricks and illusions to circumvent the countless lures that tried to turn me into
anything other than a writer.’ — Gabriel Garc´ıa Marquez, Living to tell the
tale.
In this dissertation I would like to guide the reader to the research on dialogue but more
precisely the research I have conducted during my career since my PhD thesis. Starting from
modular architectures with machine learning/deep learning and reinforcement learning to end-to-
end deep neural networks. Besides my work as research associate, I also present the work I have
supervised in the last years. I proposed four PhD thesis Conventions industrielles de formation
par la recherche (CIFRE) that Orange accepted to fund. Therefore, I could co-supervise four
PhD candidates: Timothy Garwood supervised by Claire Gardent at CNRS, Thibault Cordier
supervised by Fabrice Lefevre at the University of Avignon, Sebastien Montella supervised by
Alexis Nasr at the university of Aix-Marseille, Le´o Jacqmin supervised by Benoit Favre at the
University of Aix-Marseille. During 5 years I was head of the industrial research project on
dialogue,DIalogueinNAturalLanguage(DIANA),whichgavemetheopportunityofsupervising
the work of the young researcher Quentin Brabant, other experimented researchers, a developer
as well as students in internship and apprenticeship. The deliverables of DIANA project gather
open-sourceddatasetsandneuralmodelsaswellasscientificpublications.
Ireviewbrieflythestateoftheartandhighlighttheopenresearchproblemsonconversational
agents in Chapter 2. Afterwards, I present my contribution to Task-Oriented Dialogues (TOD) in
1Chapter 3, both as research associate and as the industrial supervisor of CIFRE theses. I discuss
conversational QA in Chapter 4. Particularly, I present the work of two PhD candidates Thibault
Cordier and Sebastien Montella; as well as the work of the young researcher Quentin Brabant. I
presentthescientificprojectinChapter6. FinallyIpresenttheconclusionsinChapter7.
2Chapter 2
A Glance to the Research on Dialogue
‘Please,then,’saidAlice,‘howamItogetin?’
‘There might be some sense in your knocking,’ the Footman went on without
attending to her [...]’ He was looking up into the sky all the time he was
speaking,andthisAlicethoughtdecidedlyuncivil. ‘Butperhapshecan’thelp
it,’shesaidtoherself;[..] –HowamItogetin?’ sherepeated,aloud.
-[..] the Footman continued in the same tone, exactly as if nothing had hap-
pened. ‘HowamItogetin?’ askedAliceagain,inaloudertone.
‘Are you to get in at all?’ said the Footman. ‘That’s the first question, you
know.’
It was, no doubt: only Alice did not like to be told so. ‘It’s really dreadful,’
she muttered to herself, ‘the way all the creatures argue. It’s enough to drive
onecrazy!’ —LewisCarroll
The origins of dialogue systems date from 1966 when the Eliza chatterbox was presented
(Weizenbaum, 1966). Eliza was the automated psychoanalyst that let us dream about intelligent
systems able to converse as humans. However, Eliza was a simple template-based approach,
limitedbyitspoorunderstandingaswellasthelackofexpressivityandadaptability. Theresearch
on dialogue has come a long way since then. Several solutions have been proposed from regular
expressions and symbolic approaches (formal grammars and formal logics) (Larsson and Traum,
2000;McTear,2002)tostatisticalapproaches(Pieraccinietal.,1992;WilliamsandYoung,2007;
Youngetal.,2010;Zhangetal.,2020b),whicharedata-driventechniquesthatuseeithermachine
learningordeeplearning.
The availability of big data as well as the advances in processing units made deep learning
approaches feasible and promising (Cuaya´huitl et al., 2015; Daubigney et al., 2013; Vinyals and
Le, 2015; Sordoni et al., 2015; Wen et al., 2017b; Serban et al., 2017), reviving the dream of
creating artificial agents that can easily converse to people. We have already obtained promis-
ing results. For instance, we know that machines can learn optimal strategies for simple tasks in
small domains (TOD)(Weisz et al., 2018a; Zhu et al., 2020). Moreover, we are already treating
open domain dialogues by asking questions to online encyclopaedias such as Wikipedia (con-
versational reading comprehension) (Choi et al., 2018b; Reddy et al., 2018) and we are able to
predictthebestanswerinchitchats(endtoendneuralapproaches). Theveryrecentbreakthrough
ChatGPT (Ouyang et al., 2022) confidently generates apparently coherent responses for a great
3amount of domains and tasks. A variety of methods has been studied by the research community
ondialogueduringthelastdecade. Frommodulararchitecturestoend-to-endneuralnetworks. In
thischapterIwilldescribesometheseapproaches.
2.1 Why is human conversation difficult?
Who has not experienced the frustration of talking to an automatic system in a call centre. Typi-
cally, these systems struggle to understand. They are repetitive because they are unable to rectify
misunderstandings. Users then must start all over from scratch. In the worst-case users need to
callagain,thentheytryhardtofoolthesystemuntilthecallisfinallyansweredbyahuman. Con-
versationswithautomaticsystemsareunnaturalbecausetheydonotdealcorrectlywithmisunder-
standings,theydonotadapttonovelsituationsandtheyconstrainthumans’greatcommunication
skills.
Dialogue-Acts and discourse obligations: A dialogue can be seen as a sequence of turns, in
which every speaker takes a turn to speak and to contribute to the conversation. The philosopher
(Austin, 1975a) stated that speakers perform actions while conversing and named these actions
speech acts. Examples of these actions are : informing, requesting, offering, promising, answer-
ing, persuading, convincing, etc. The research community nowadays call these actions Dialogue
Acts or Communicative Acts (Bunt et al., 2010). Adjacency-pairs are pairs of dialogue-acts in
conversation. For example, after a question in a conversation, the speaker is waiting for an an-
swer. After an offer the speaker is waiting for an acceptance or a rejection. These represent
discourseobligationsinhumanconversation.
Coreferences and Ambiguity: Moreover, humans can refer to concepts that were mentioned
previously in the conversation. Indeed, a fluent conversation avoid repetitions. For instance, if
youaretalkingaboutthepresidentofFrance,youcansay“EmmanuelMacron”thefirsttimeyou
mentionedhim,lateryoucanchoosetosay“thepresident”. Moreover,ifyouwanttofurthergive
your opinion about a recent political proposition he has made, you can say “I disagree with his
retirementpolicy”. Thesearewellstudiedlinguisticphenomenathatmadeconversationdifficult.
Another aspect is the ambiguity, the same sentence can mean different things in different
contexts. “It’s cold in here” can be understood as a request to close the window inside a closed
room or it may mean “It’s cool in here” in summer. In winter instead it could mean “I can’t stand
theweather;itistoocold”.
Grounding: speakers are always checking that they are following each other. For instance,
let us suppose you are receiving instructions about where to place a box in a room. If there
are many similar boxes, and the instruction is “move the box to the right of the desk”, you will
probably ask “which one?”. Then the instructor will provide more precise information such as
“the yellow rectangular box”. This coordination or mutual agreement is known as grounding,
mutualknowledgeorsharedknowledge(ClarkandBrennan,1991).
Planning: The model Beliefs, Desires and Intentions (BDI) as the primary mental attitudes of
an agent was first introduced by Bratman (Bratman, 1987). The beliefs are the agent’s model
42.2PreliminaryApproaches
of the world. Desires, in turn, represent how the agent would like the world to be in the future;
while intentions are the structured plan the agent has decided to perform. The agent interacts
with the world by performing actions and by perceiving aspects of it, including changes which
result from its own actions. Perceptions will influence the beliefs of the agent, while actions
may change aspects of the world. This model was at the origin of modern Natural Language
Understanding(NLU),inwhichtheaimistodetectuser’sintentionsorintents. However,theterm
intent usually means a dialogue-act with a set of concepts or a combination of them in semantic
labels.
Alltheseinherentcharacteristicsinnaturaldialoguemakeimplementingautomatedsystemsa
verydifficulttask.
2.2 Preliminary Approaches
A range of approaches emerged in the history of Dialogue Systems (DSs), they were classified in
conformity with their Dialog Manager (DM) (Allen et al., 2001; Churcher, 1997). According to
thisclassification,orderedbyincreasingcomplexity,thesimplestoftheseisthefinite-statescripts,
also called dialogue grammars, followed by slot-filling, plan-based and agent-based models. In
a finite-state script the dialogue is represented as a script of prompts for the user. In slot-filling,
questions are asked in order to enable the system to fill the necessary slots to perform a task.
Conversely, plan-based theories claim that utterances infer acts that are part of a plan, thus, the
system tries to identify users’ underlying plan, collaborates in accomplishing that plan and re-
sponds appropriately. Agent-based models are at the highest level of complexity. They consider
planning,executing,andmonitoringoperationsinadynamicallychangingworld,possiblyinvolv-
ingmulti-modality. Examplesofagent-basedmodelsare: thelogic-basedapproaches,whichuses
inferenceenginesofahighercomplexitythatinsomecasesaresemi-decidable(insomecasesthe
systemwillneverhalt),aswellasreinforcementlearningapproaches,whichneedalargenumber
ofinteractionstoconverge.
2.3 Task Oriented Dialogue Systems
Conversationalagentshavegainedgreatinterestinbothacademyandindustryinthelastdecades.
Typically,availableconversationalagentshavebeendesignedforthetaskofinformation-seeking.
These agents act as a natural language interface to a database. First, the system tries to fill the
constrains to query a database by inquiring the user. Then it retrieves the items that fulfill users’
constraints and finally it communicates the results to the user in natural language. For instance, a
personcouldcallthesystemtochecktraintimetables,shewouldprovidethedepartureandarrival
cityaswellasthedeparturedateandtime. Then,thesystemwouldinformherabouttheavailable
trains.
TOD main goal is to complete a task in collaboration with the user (Pieraccini et al., 1992;
Young, 2002; Rieser and Lemon, 2011; Young et al., 2013a). Examples of tasks are to search
informationaboutarestaurant,toreserveahotelortobuytraintickets.
52.3.1 Definitions
As introduced in Section 2.1, dialogue-acts or communicative acts are the actions performed by
thespeakerswhenutteringsentences(e.g. Informing,Asking,Confirming,Greeting,etc.)(Austin,
1975b). A domain is formally defined in an ontology as a list of slots with their valid values. The
most common task, the information seeking task, is usually modelled as a slot-filling data-query
problem in which the system requests constraints to the user and proposes items that fulfil those
constraints in a database. The action or intention is composed by a predicate: the dialogue-act,
and a set of arguments: the slot-value pairs. For instance, let us suppose the user has uttered
“I would like a restaurant in the center of town please”, this will be translated in the semantic
form: inform(type = restaurant,area = center). This semantic representation is usually called
flat-semanticsbecausethereisnothierarchyintheconceptsoftheontology.
2.3.2 Statistical Dialogue Systems
OneapproachtoautomaticdialogueistouseReinforcementLearning(RL)toselectthesystem’s
action (Levin et al., 2000; Litman et al., 2000). As in a Chess game, a dialogue involves two
players, in which each of them takes turns to play. The system should decide its move by consid-
ering the environment (the other player’s moves) and the rewards is either win or lose the game.
Dialogue is then formulated as an optimisation problem, in which the environment is the user
action and the user’s feedback is the reward. The final goal of the system is then to maximise the
accumulated reward at long run (Rieser and Lemon, 2011). The optimal policy,π, is a function
thattakesasargumentthecurrentstatesandreturnstheoptimalactiona.
Markovdecisionprocess
DialoguecanbeformalisedasaMarkovDecisionProcess(MDP),whichisatupleM = (S,A,T,γ,R)
where:
• S: Asetofpossiblestatesthatrepresentthedynamicenvironment.
• A: Asetofpossibleactions.
• T : S ×A×S → [0,1] is a transition probability function. For any action a ∈ A(s) taken
inastates ∈ S,theprobabilityoftransitingtothenextstates′ isgivenbyT(s,s′).
• γ: A discounting factor in the range of [0, 1], which controls the prediction horizon of the
algorithm.
• R: The reward function that specifies the reward gained at every state. It contains the
information that guides the agent towards the goal. R is a function of the state that is
boundedinabsolutevaluebyR .
max
A stationary policy is a map π : S → A and the discounted infinite-horizon expected reward
for starting in state s and following policy π thereafter is given by the value function Vπ(s) that
satisfiesthefollowingBellmanEquation:
(cid:88)
Vπ(s) = R(s)+γ T(s,π(s),s′)Vπ(s′) (2.1)
s′
62.3TaskOrientedDialogueSystems
Thediscountedinfinite-horizonexpectedrewardforstartinginstates,takingactionaandfollow-
ingpolicyπ thereafterisgivenbytheQ-functionQπ(s,a)thatsatisfiesthefollowingequation:
(cid:88)
Qπ(s,a) = R(s)+γ T(s,a,s′)Vπ(s′) (2.2)
s′
Apolicyπ isoptimalinM if,foralls ∈ S:
π(s) = argmaxQπ(s,a) (2.3)
a∈A
LikewiseQ∗(s,a,R)istheoptimalQ-functionoftheoptimalpolicyπ∗ foraknownrewardfunc-
tionR.
PartiallyObservableMarkovdecisionprocess
DialoguescanbemodelledasanoptimisationproblemwithPartiallyObservableMarkovdecision
process (POMDP)s. It simulates the inherent dynamic behaviour of human conversations while
dealswiththeuncertaintyofspokenlanguage(Royetal.,2000;WilliamsandYoung,2007;Young
etal.,2013b).
A POMDP can be seen as a continuous-space Markov decision process (MDP) in terms of
policy optimisation where the states are the belief states, which is partially observable. POMDPs
have been proposed for spoken dialogue systems because the system is never sure about the user
beliefs because of speech recognition errors due to noisy or spoken language disfluencies and
hesitations (Roy et al., 2000; Young et al., 2013a). Since the state is uncertain, it is called the
belief state b(s). An example of a POMDP dialogue system is presented in Section 2.3.2. The
taskofpredictingtheb(s)atagiventimetisknownasthetaskofDialogueStateTracking(DST).
The policy learning algorithm receives as input the b(s) and returns the optimal policy π∗ and a
giventime.
The belief state b is a vector encoding a probability distribution over the different goals,
t
dialogue acts and concepts that are discussed in the dialogue. In the same way, the dialogue
actiona isavectorencodingaprobabilitydistributionoverthepossibleagentdialogueactions.
t
HierarchicalReinforcementLearning
MDP models have been proven to be inefficient for solving complex tasks. These models have
trouble overcoming the cold start problem and/or suffer from the curse of dimensionality (Barto
and Mahadevan, 2003). This pattern was also observed with models proposed recently (Mnih
et al., 2013; Duan et al., 2016). To overcome this issue, (Parr and Russell, 1998) proposed to
specifyahierarchyoftasksandtoreusepartsofthestatespaceacrossmanysub-tasks,whichcan
greatlyimprovebothlearningspeedandagentperformance.
The notion of temporal abstraction, in which a policy can be decomposed into sub-policies
by calling temporally extended sub-tasks was first proposed by (Sutton et al., 1999). In order to
consider hierarchical architectures with temporally extension, we have to generalise the MDP to
the semi-Markov Decision Process (SMDP) (Parr and Russell, 1998) where actions can take a
variable amount of time to complete. This creates a division between primitive actions that span
over only one action and composite actions that involve an execution of a sequence of primitive
7actions. This introduces a policy µ over options that selects option o in state s with probability
µ(s,o), o’s policy might in turn select other options until o terminates and so on. The value
function for option policies can be defined in terms of the value functions of the flat Markov
policies(Suttonetal.,1999).
(Cuaya´huitl,2009)wasthefirsttoproposeHierarchicalReinforcementLearning(HRL)based
on the MAXQ algorithm for dialogue decomposition, making use of hierarchical abstract ma-
chines (Parr and Russell, 1998). However, the tabular approach of this algorithm prevents the
efficient approximation of the state space and the objective function. To overcome this limitation
(Budzianowski et al., 2017), uses Gaussian process, which provides uncertainty estimates which
can be used to speed up learning and achieve more robust performance. Some recent work such
as(Tangetal.,2018)aimstodiscoverautomaticallysub-goalshierarchyindialog.
RewardFunctionsforDialogueSystems
PreviousworksonRLforlearningdialoguestrategiestypicallyuserewardfunctionsthatpenalise
long dialogues, returning a final positive reward for task completion or user satisfaction (Levin
et al., 2000; Litman et al., 2000; Roy et al., 2000; Young et al., 2010; Rieser and Lemon, 2011).
This might be an intuitive reward function for slot-filling applications, such as train ticket or
restaurant reservation, in which usually customers know exactly what they want, and they expect
tobeaccuratelyinformedbythesystemasfastaspossible.
However, this reward function might be inappropriate in other situations or for distinct users.
For instance, a user might want more advice without caring about the duration of the call. This is
especially true in tutorial dialogues, where learners usually have to complete a task and may not
knowexactlyhowtodoit.
Architecture
The basic elements of a RL based spoken statistical dialogue system are shown in Figure 2.1.
The words recognised by the speech recognition are converted to an abstract representation (the
user dialogue acts) by the semantic parser, also known as semantic decoder or NLU. These user
dialogueactsarethenprocessedbyabelief-statetrackerwhichmaintainsadialoguestates. This
is typically a set of variables denoting the slots that the system must fill-in to complete the user’s
goal. Forexample,inarestaurantinformationsystemtheslotsmightbefoodforthetypeoffood
offered and area for the location, and the state s might record the current value and confidence
level of each slot. From the state, a belief state b (usually just a sub-set of the state vector) is
extracted and an action a is decided based on a dialogue policy. The set of possible actions
will include requesting new slot values, confirming already filled slot values and accessing the
applicationforinformation. Oncetheappropriateactionisdetermined,itisconvertedtoatextual
messagemandthenrenderedbyaspeechsynthesiser.
Deep Learning (DL) has been used to implement dialogue components such as semantic de-
coder orNLU (Rojas Barahonaet al., 2016), belieftracker (Daubigney etal., 2013; Mrksˇic´ etal.,
2017) and generator (Wen et al., 2015). Deep Reinforcement Learning is used for policy learn-
ing (or DM) (Cuaya´huitl et al., 2015), such as deep q-network (DQN) (Casanueva et al., 2017a),
theactor-criticalgorithm(Suetal.,2017)andactor-criticwithexperiencereplay(ACER)(Weisz
et al., 2018a). Benchmarks comparing these algorithms across different domains and different
82.3TaskOrientedDialogueSystems
Figure2.1: Basicelementsofastatisticalspokendialoguesystem
environments havebeen published in (Casanuevaet al., 2017a). It is worth notingthat ACER has
beentrainedonaround1000actionspacesandnotonsummaryactionsastheothers(Weiszetal.,
2018a). These algorithms are available in the open-source dialogue framework PyDial (Ultes
etal.,2017b).
2.3.3 End-to-End Task-Oriented Systems
One preliminary approach to end-to-end dialogue is pipeline-based. It replicates the classical
dialogue architecture (Figure 2.1), with the main difference that each module is a deep neural
network. One example of this approach is the seminal work of (Wen et al., 2017b). This neural
pipelinewaslaterimprovedin(Wenetal.,2017a)throughalatentvariablemodelforlearningthe
distributionofsystemactions. Although,authorsclaimedthisapproachtobeend-to-end,modules
arenottrainedjointlyintoasinglelearningunitthatcanbeoptimisedbygradient-basedmethods
such as back-propagation. Conversely, each module is trained separately in a cascade fashion
wheretheoutputsofonemodelaretheinputstothenextone.
92.4 End-to-End Dialogues
Three approaches to end-to-end dialogue system are identify: retrieval-based, generative-based
and the combination of both. These approaches are truly end to end, which means the model
learnsthroughgradient-basedoptimisation. Retrieval-based approachtreatsdialogueasaninfor-
mation retrieval problem (Lowe et al., 2015; Wu et al., 2018), in which there is a set of candidate
responses from which one is selected as system response, here dialogue is evaluated as an infor-
mationretrievalproblem(intermsofprecision/recall). Generative-basedmodels(VinyalsandLe,
2015; Sordoni et al., 2015; Serban et al., 2016; Goo and Chen, 2018) on the contrary use natural
language generation to generate the system response and dialogue is evaluated as as a generation
problem (in terms of comparison with multi-references for instance, with BLEU score). Both ap-
proaches have been used for chit-chats or casual conversations. Retrieval-based models however
have been also used for task-oriented solutions(Lowe et al., 2015). The combination of both, can
use generation to paraphrase the retrieved answer. Another way is to compare generative and re-
trievedresponses. Theinterestedreadercanfindmoredetailsaboutrecentend-to-endapproaches
in(Nietal.,2022).
Figure2.2: E2Eneuralarchitecture
Yet these approaches neglect the fact that dialogue is dynamic and highly dependent on the
environment. Moreover, they do not consider the metrics usually used for evaluating dialogues
such as task completion and user satisfaction. Generative approaches usually generate fluent
answers, which are sometimes incoherent with the dialogue context. They can generate hallu-
cinations, distortions or repetitions. Retrieval-based approaches are limited to a list of candidate
responses,whichneedstobecreatedinadvance,yieldinganswerswithoutanycontextagreement.
Toovercometheselimitations,(SankarandRavi,2019)proposesacombinationofreinforcement-
learning and generative models. The successful ChatGPT is an example of generative model,
carefullytrainedtofollowinstructionsthatalsolearnstorankitsresponses.
102.5Pre-trainedLanguageModels
2.5 Pre-trained Language Models
With the success of Language Models, Deep Learning and Transformers (Vaswani et al., 2017),
recent solutions proposed pre-trained models with self-supervision. These models outperformed
thestate-of-the-artindifferentNaturalLanguageprocessing(NLP)tasks. Theyhavebeeninitially
trained on a large quantity of texts. For instance, BERT has been trained on 800M words and
2,500M words of the BooksCorpus and Wikipedia respectively. GPT-3 is a huge pre-trained
modelwith175Bofparameters,trainedonlargequantityofdata.
Recently, these pre-trained models have been also trained on conversations. For example,
BlenderBot(Shusteretal.,2022)isanencoderdecoderwhichhasbeentrainedon1.5BofReddit
comments. DialoGPTisjustadecoderthathastrainedon147MofdialoguesextractedfromRed-
dit.(Santraetal., 2021). ConveRThasbeen trained on727Mofdialogs. ChatGPTispresumably
the result of finetuning GPT3 with carefully curated instructions with reinforcement learning for
correctlyrankthegeneratedresponses(i.e. learningtorank)(Ouyangetal.,2022).
The trend nowadays is to initialise deep learning models with a pre-trained one and then fine-
tune them for a specialised task. However, fine-tuning with little data will often degrade the
initialised weights. Therefore, recent optimisation for fine-tuning are : prompting (Lester et al.,
2021),prefixes(LiandLiang,2021)andadapters(Huetal.,2021).
Despite very promising, the great limitation is that pre-trained models are still static. If there
are changes in the language or in the World, these changes will not be reflected in the model.
Think about the BERT models pre-trained before the Covid pandemic, they do not contain any
representation for the bunch of new vocabulary that emerged during the pandemics: Covid-19,
Moderna, Pfizer, sanitary vaccination pass, test PCR, etc. It is worth noting that training these
largemodelsiscomputationallycostlyandtheyrequireHyper-performanceComputing(HPC).A
researchpathishowtokeepthesemodelsuptodateavoidingcatastrophicforgettingandreducing
thecarbonimpactnecessaryduringtraining.
2.6 Conversational Question Answering
Research on conversational question answering has gained increasing interest (Saha et al., 2018;
Reddy et al., 2018; Choi et al., 2018b). It consists in sequences of question-answer pairs related
to a document or to a knowledge graph. Complex sequential question answering (CSQA) con-
tains open-domain conversations that treat complex linguistic phenomena such as co-references,
ellipses, incompleteness (or under specification) as well as logical, comparative and quantita-
tive reasoning (Saha et al., 2018). Two corpora containing discussions about a paragraph of a
Wikipediadocumenthavebeenmadepublic,namelyquestionansweringincontext(QuAC)(Choi
etal.,2018b)andaconversationalquestionansweringchallenge(CoQA)(Reddyetal.,2018). In
addition,aworkshopdevotedtothistopichasfirstcreatedin2017,searchconversationalartificial
intelligence(SCAI),withtheparticipationofacademicsandindustrials1.
Paragraph-basedQuestion-Answeringcanbeseenasaproblemofreadingcomprehension,in
which given a candidate document and a question, it finds the correct answer in the document.
These systems use attention mechanisms (Seo et al., 2017) and memory networks (Weston et al.,
2014).
1https://scai.info/
112.7 Positioning My Contributions in the State-of-the-Art
Most of my work concerns task-oriented dialogue, which involves the different components pre-
sented in Section 2.3. I worked on NLU, DM and more recently on Natural Language Gener-
ation (NLG). I have adapted neural models for distinct dialogue components that are presented
in Chapter 3. Since the past five years, I have also explored open-domain Conversational Ques-
tion Answering (CQA) (Section 2.6) which I present in Chapter 4, with predictive and generative
(encoder-decoder) models for question rewriting, reading-comprehension and knowledge-graph
questiongeneration. Asanextensionofthelastapproach,Ihavealsoexploredgraph-embeddings
(Section 4.4) and graph verbalisation with language models (Montella et al., 2023). The contri-
butions published the second half of this year or the work in progress are not included in this
document.
Myworkhasalwaysfollowedthestate-of-the-artatthetimeofpublication. Foradetailcom-
parison of each contribution summarised in this manuscript with the related work of its time, we
invite the curious reader to check the corresponding publications. To provide a brief positioning
of some contributions, the work on data collection, NLU, dialogue management and human eval-
uation I made for the EmoSpeech corpus was the first of its kind: a set of dialogues (12 distinct
typesofdialogue)inaSeriousGameandinFrench. Moreover,wewereamongthefirsttopropose
in 2013 data-augmentation with back-translation and distributional representations for balancing
biasedmodels(GardentandRojas-Barahona,2013).
TheworkoninversereinforcementlearningfollowedontheseminalworkofNgetal.(2000).
It differed from previous work Paek and Pieraccini (2008); Chandramohan et al. (2011); El Asri
et al. (2012); Boularias et al. (2010) because of the Bayesian Inverse Reinforcement Learning
(IRL)algorithmthatwasappliedforlearningthetutor(i.e. system)rewardfunctionfromexperts.
Furthermore,expertsweretakenfromtwelvedistincttypesofconversationsinaseriousgamethat
wereHuman-Human(i.e. TheEmoSpeechcorpus)andnotMachine-Machine(notfromsimulated
conversations).
I proposed to enrich a subset of the corpus CoQA (Reddy et al., 2019) releasing the cor-
pus CoQAR2 with up to three out-of-context question paraphrases per question in conversations
(Chapter 4, Section 4.2). Unlike previous work, these paraphrases were made by professional
Englishnativeannotatorsinsteadofusingcrowd-sourcing,guaranteeingfairearningandworking
conditions. We compared CoQAR to CANARD (Elgohary et al., 2019) that provided only one
question rewriting per question in QuAC (Choi et al., 2018b). I experimented with RoBERTa for
answerextractioninbothdatasetsCoQAandCoQARwithawithoutrewriting. Surprisinglysolv-
ing the context through rewritten questions confuses RoBERTa, which is already good to solve
co-referencesbyitself,speciallyinshortdialoguecontextsasinthesedatasets.
I also utilised contextual embeddings (DistilBERT (Sanh et al., 2020), TransformersXL (Dai
et al., 2019)) for estimating the reward function in long dialogues. I pointed out at that time the
limitation of BERT-like models to deal with long contexts, which, besides the notable improve-
ments of recent years, is still an open research problem. We also explored neural generation
models such as BART (Lewis et al., 2019) and T5 (Raffel et al., 2020) for contextual question
generation,questionrewriting(Section4.2)andgraphverbalisation(Montellaetal.,2022,2023).
I explored together with Sebastien Montella and Johannes Heinecke structural adapters for graph
verbalisation (Montella et al., 2023) just after low rank emerged (Hu et al., 2021) as a recom-
2https://github.com/Orange-OpenSource/COQAR
122.7PositioningMyContributionsintheState-of-the-Art
mendedwaytooptimallyfine-tuneLLMs.
After the advent of Large Language Models (LLMs), I am now questioning the performance
ofthesemodelsincomplextasksthatrequiredplanning,suchasdialogue. First,weneedaneval-
uationmethodology toassess theperformanceof LLMsin thesetasks. Then weneedto compare
LLMs based reasoning (Wei et al., 2022; Yao et al., 2023, 2022) with Reinforcement Learn-
ing (RL) approaches, and explore recent trends for learning complex strategies such as algorithm
distillation(Laskinetal.,2022). ItalkabouttheseresearchpathsinChapter6.
1314Chapter 3
Contributions to Task-Oriented Dialogues
‘Jose´ Arcadio Buend´ıa paso´ los largos meses de lluvia encerrado en un cuar-
tito que construyo´ en el fondo de la casa para que nadie perturbara sus ex-
perimentos. Habiendoabandonadoporcompletolasobligacionesdome´sticas,
permanecio´ noches enteras en el patio vigilando el curso de los astros, y es-
tuvo a punto de contraer una insolacio´n por tratar de establecer un me´todo
exactoparaencontrarelmediod´ıa.’ —GabrielGarc´ıaMarquez,Cienan˜osde
soledad.
‘Jose´ Arcadio Buend´ıa spent the long months of the rainy season shut up in
a small room that he had built in the rear of the house so that no one would
disturb his experiments. Having completely abandoned his domestic obliga-
tions, he spent entire nights in the courtyard watching the course of the stars
andhealmostcontractedsunstrokefromtryingtoestablishanexactmethodto
ascertainnoon.’ —GabrielGarc´ıaMarquez,OneHundredYearsofSolitude.
As introduced in Section 2.3, task-oriented dialogue systems search to accomplish a task.
This task can be for instance, information seeking, in which the system search for items in a
database according to the constraints obtained through natural language interaction. Thus, these
constrains can be given by the user (‘I am looking for a restaurant’) or can be enquired by the
system (‘which price range?’, ‘where about?’). Once the desired items are retrieved, the system
informs the results back to the user. This kind of dialogue has rich interactions with dialogue
acts such as: informing, requesting, clarifying, rectifying. State-of-the-art systems are centred on
the information seeking task for a variety of domains: hotels, restaurants, touristic attractions,
trains, flights, taxis, etc. Unfortunately, these dialogues are very specific and have difficulties to
generalisetonewdomainsandtomorecomplextasks(i.e. beyondinformationseeking).
IpresentinthisChaptermycontributionstoTODdialogues(Figure2.1). Istartbypresenting
my work on NLU in Section 3.1 and on Dialogue Management in Section 3.2. My contributions
to NLU concerns the definition of an annotation schema for the French corpus EmoSpeech, in
which 12 distinct dialogues were integrated in a serious game. The dialogues involved various
characters representing the system and the player and they were triggered at different levels of
the game quest. Dialogues were modelled as an information seeking task, in which the system
is always providing information to the player. The set of dialogue acts and goals are introduced
in Section 3.1, as well as the models that were trained. Data augmentation through paraphrases
via back translation, dictionaries, lexical resources and distributional semantics is also presented
15in Section 3.1.1. Moreover, the task of spoken language understanding is studied by using deep
neuralmodelsandfew-shotlearningthroughriskminimisationinSection3.1.2. Finally,Ipresent
theworkofthePhDcandidateSebastienMontellaongraphembeddingsinSection4.4.
The contributions on Dialogue management are presented in Section 3.2. I first present my
ownworkoninversereinforcementlearningtofindtheimplicitrewardfollowedbyhumansinthe
EmoSpeechdialoguecorpus. Ialsopresentarewardestimationbyusingdeeplearning. Finally,I
presenttheworkofthePhDcandidateThibaultCordieronhierarchicalimitationlearning.
3.1 Language Understanding
Natural language understanding (NLU) is the task of mapping natural language sentences to se-
manticconcepts. Asacomponentofaspokensystem,itwouldmaputterancestoasemanticrep-
resentation that describes user intentions. This representation can be a combination of dialogue
acts1 (e.g., greeting, request, inform, acknowledgement, confirm, etc.) and concept-value pairs,
whichareusuallydefinedinaknowledge-basethatdescribesthedomain(e.g,Depart City=”New
York”). This section presents the semantic annotation of the French EmoSpeech corpus, the pro-
posed Machine learning models, and the ways to improve the performance of these models via
dataaugmentation. Thissectionalsoincludesthecontributionstospokenlanguageunderstanding,
inwhichtheinputcorrespondstotheoutputoftheAutomaticSpeechRecognition(ASR). Ishow
howtheN-BesthypothesisareincludedasinputstoaConvolutionalNeuralNetwork(CNN)that
generates the sentence representation and how the context is handleby a Long-Short Term Mem-
ory(LSTM). Inaddition,Iintroduceriskminimisationforzero-shotlearningonthistask. Finally,
IpresenttheworkofthePhDcandidateSebastienMontellaongraphembedding. Inordertouser
richer representations as input to neural models, the hyperbolic space was explored for treating
temporalrelations.
3.1.1 NLU in a Serious Game
MachineLearning(ML)approachessuchaslogisticregressionclassifiersandconditionalrandom
fields were the state-of-the art back in 2011. I could collect a corpus through Wizard-of-Oz ex-
periments, define an annotation scheme and train Logistic Regression (LR) and Support Vector
Machines(SVM)multi-classclassifiersforthetaskofNLU.Thesemodelswereintegratedwithin
distinct dialogues in a serious game. I will start by describing briefly this work that gave origin
tofourpublications(Rojas-Barahonaetal.,2012b,c;Rojas-BarahonaandGardent,2012;Gardent
andRojas-Barahona,2013).
The serious game is a multiplayer quest where the players (3 teenagers) seek to build a video
game joystick in order to free their uncle trapped in the game. To build this joystick, the players
mustexploreafactoryandachieve17mandatorygoals(findtheplans,gettheappropriatemould,
retrieve some raw material from the storing shed, etc). In addition, they can increase their score
by achieving optional goals which, when reached, provide them with extra information about the
industry (therefore increasing their knowledge). In total, the players can achieve up to 28 goals
by conducting 12 separate subdialogs in various parts of the virtual world. That is, dialogs in the
gamearelongdialogsinvolvingmultipleplayersinvarioussettings.
1dialogueactsaretheperformativeactionunderlyingautterance(Austin,1975b)
163.1LanguageUnderstanding
Id VC Player MandatoryGoals Location
1 Lucas Ben Findtheaddressoftheenterprise. Unlce’splace.
2 M.Jasper Lucas Themanufacturingfirststep Enterprisereception
3 Samir Julie Findtheplansofthejoystick DesigningOffice
4 Samir Julie Findoutwhattodonext DesigningOffice
5 Melissa Lucas Manufacturingprocess... Plant
6 Melissa Lucas Findtherightmachine Plant
7 Melissa Lucas Findoutwhattodonext Plant
8 Operator Julie Knowingaboutthematerialspace... MaterialSpace
9 Serge Ben Performqualitytests LaboratoryTests
10 Serge Ben Findoutwhattodonext LaboratoryTests
11 Sophia Julie Findtheelectroniccomponents. Finishing
12 Sophia Julie Finishingprocess Finishing
Table3.1: DescriptionofthesubdialogsintheMPGame.
Table 3.1 summarises the characteristics of the subdialogs conducted within the game high-
lighting three distinguishing features of game dialogs. First, the dialog participants vary whereby
both the game agent and the player can change. Thus in the game, the player alternatively plays
any of the three children involved in the quest while the game agent is successively, Lucas, M.
Jasper, Samir, Melissa, an operator, Serge and Sophia. Second, game dialogs are task-driven
whereby each subdialog is related to a step in the game and each dialog turn aims to achieve a
game goal and improve the player score. Third, the context in which each subdialog takes place
variesastheplayermovesaroundtheworld.
NLUannotationschemafordialogueisnotnecessarilydictatedbyspeechacttheoryalonebut
might also consider more practical issues namely, how well it will support interpretation and/or
dialogue. Toenhancelearning,theannotationschemadesignedforthegamecombinescorecom-
municative acts (Bunt et al., 2010) with domain specific information. The domain specific infor-
mationspecifiesthegoalsbeingpursued/discussed/achievedetc. whilethecommunicativeactcan
beviewedasspecifyinghowthecurrentinformationstateisupdatedbythespeaker’sutterance.
dialog:01dialogDirecteur-TueJun1411:04:232011
M.Jasper:Bonjour,jesuisM.Jasperledirecteur.
Hello,Iamthedirector,Mr.Jasper. →greet
M.Jasper:Qu’est-cequejepeuxfairepourvous?
WhatcanIdoforyou? →ask(task(X))
Lucas:jedoissauvermononcle
Imustrescuemyuncle →firststep
M.Jasper:Pourfairevotremanette,ilvousfautdesplans.Allezvoirdanslebureaud’e´tudes,ilsdevraientyeˆtre.
Tobuildthejoystickyouwillneedtheplans.YouwillfindthemintheDesigningOffice. →inform(do(firststep))
M.Jasper:J’auraisaime´continuera`re´pondrea`vosquestionsmaisjedoisreprendremontravail!BonneChance!
Ihavetogobacktowork!GoodLuck! →quit
Figure3.1: ExcerptfromadialogueintheEmoSpeechcorpus. Thecorrespondingusersemantics
isshownhighlightedontheright.
Table3.2givesthefulllistofDialogueActsusedforannotationtogetherwiththecorrespond-
17DialogueAct Label Gloss Speaker
Welcomegreeting greet Welcomegreeting P,S
Farewellgreeting quit Farewellgeeting P,S
AdressRequest ask(Goal) RequesttopursueGoal S
AdressRequest help Requestforhelp P
Confirm yes Confirmspreviousquery P
Disconfirm no Disconfirmspreviousquery P
ProvideInformation inform(do(Goal)) ProvidesinformationabouthowtoachieveGoal S
ProvideInformation Goal ProvidesinformationaboutthegoalGoal P
PositiveFeedback ack Acknowledgesunderstandingofprecedingturn S
PropositionalQuestion ask(do(more(X))) Askswhetherothertopicsshouldbediscussed S
SetQuestion ask(topic(X)) Askswhichothertopicsshouldbediscussed S
OutofContext other Outofcontextturn P,S
Misunderstanding reqRep Requestforrephrasing S
Table3.2: TheAnnotationScheme. PandSstandsforPlayerandSystemrespectively.
ing dialog acts and a gloss of their meaning. As can be seen the labels used are very specific to
the game to facilitate the integration within the game (same goals as defined in the serious game)
and to bypass much of the pragmatic reasoning necessary to associate a dialog turn with a com-
municative function. For instance, in the dialog above, the turn je dois sauver mon oncle (I must
rescue my uncle) does not explicitly state that the player (i) is seeking to achieve the game goal
“rescueing one’s uncle” and (ii) is asking the game agent for the first step towards achieving that
goal.
Experimentalsetup
WeexperimentedwithbothanSVMandanLR2classifierusingdifferentsetsoffeaturesondiffer-
entdatasetswithandwithoutTF*IDF(termfrequency*InverseDocumentFrequency)filtering.
WholeDialog Subdialogs
w/oTf*Idf w/Tf*Idf w/oTf*Idf w/Tf*Idf
LR 79.74 90.26 86.41 88.22
SVM 78.79 88.55 76.45 83.99
SVM(P) 78 83.55
Table 3.3: Global Results for the Logistic Regression (LR), the SVM (SVM) and the SVM Clas-
sifierwithPenalisation(SVM(P))
We compared a single classifier on the whole dataset (the whole game) against 12 distinct
classifiers, one for each subdialog. In both cases the categories to be learned are restricted to
thespeaker’sintent(greet,quit,inform(Goal),ack,ask(do(more(X))),ask(topic(X)),otherinTable
2WeusedMALLET(McCallum,2002)fortheLRclassifierwithL1Regularisation.
183.1LanguageUnderstanding
3.2). Taking into account the game goals, the total number of categories to be learned is 27.
When learning on subdialogs, the number of categories to be learned is smaller but so is the size
of the training set. The features for the machine learning models were bag of words, in which
stopwordswerefilteredout,utterancesweredeaccentedandconvertedtolower-case. Inaddition,
we experimented with various context length using as features the 0 to 4 previous dialogue acts.
Subdialog identifiers were also used when training the classifier on the whole dialogue. More
detailsaregivenin(Rojas-Barahonaetal.,2012a).
We also experimented using tf*idf filtering to limit the impact of frequent uninformative
words. Moreover, we experimented penalising those categories with more training instances,
since the data was highly skewed. Dialogue acts that relate to optional goals were often not fol-
lowedupbytheplayersresultingindatasparseness.
Results
Table 3.3 shows the results for the 6 main configurations: training on the whole dialog or on sub-
dialogs,withandwithouttf*idffilteringandusingLR,SVMorSVMwithpenalisation. Thebest
results are obtained using the LR classifier on the whole dataset with tf*idf filtering. Penalising
improvedslightlytheaccuracyoftheSVMwhenclassifingwithouttf*idffilteringorwhenhaving
areducedcontext(0or2previousactsinTable3.4).
Impact of the tf*idf filtering. Globally, the tf*idf filtering has a positive impact leading to
an increase in accuracy ranging from 2.81 to 11.52 points. For the SVM classifier, the tf*idf
filtering consistently lead to better results. However, for the LR classifier the filtering adversely
impacts performance on short subdialogs (6 and 7), where one unique goal is being discussed.
We conjecture that for these cases, the tf*idf filtering removed words which helped the classifier
distinguish between turns about the unique goal from other turns. SVM with penalisation yields
worseresultswiththetf*idffilteringthanwithout,thussuggestingoverfitting. Inthenextsection
wepresenthowcanweexploitingsynonymstoimprovegeneralisation.
Impact of contextual features. Having a notion of context is crucial for correctly interpreting
dialog acts. As mentioned above, we use the dialog acts of the previous turns to model context.
However the further back we look into the previous turns, the more features there will be to train
on. In other words, depending on the number of previous turns considered, the data to learn from
will be more or less sparse. We experimented with 3 setups: a null context, the dialog acts of the
twopreviousturnsandthedialogactsofthefourpreviousacts. Table3.4showstheresults.
WholeDialog Subdialogs
0 2 4 0 2 4
LR 88.43 90.26 90.26 84.43 87.59 88.22
SVM 84.36 86.76 88.55 78.04 82.06 83.99
SVM(P) 79.32 83.12 83.55
Table 3.4: The impact of context on accuracy. 0,2 and 4 indicates that the context is captured by
havingasfeaturesthedialogactsof0,2and4previousturnsrespectively
Impact of dialog acts. The accuracy varies per dialog acts from 48% to 99%. with most of
theactshavinganaccuracyabove80%. Unsurprisingly,theactswithlowestaccuracyarealsothe
19acts with fewest training data. The data is split randomly for the 30-fold evaluation with the risk
ofhavinginsufficientdataforoptionalgoals.
Estimating the User Satisfaction : We applied the evaluation framework Paradise (Walker
et al., 1997a) to assess these dialogues in (Rojas-Barahona and Gardent, 2012). We compared
a rule-based dialogue manager and a dialogue manager that picks the answer randomly from a
set of candidate responses. We found out that users prefer to talk to the second system because
conversations are more fluid and there are less repetitions and misunderstandings. However, with
the second dialogue manager some times the player got stuck in the game and needed to repeat
thedialoguewiththevirtualagent,impactingnegativelytheusersatisfaction.
DataAugmentation
This work on data augmentation was published in (Gardent and Rojas-Barahona, 2013). We
explored four ways of modifying the content features used for classification: lemmatising the
training and the test data; augmenting the training data with automatically acquired paraphrases;
andsubstitutingunknownwordswithsynonymsoritsdistributionalneighboursatrun-time.
For Lemmatisation, we used the French version of Treetagger3 to lemmatise both the training
and the test data. Lemmas without any filtering were used to train classifiers. We then compare
performance with and without lemmatisation. As we shall see, the lemma and the POS tag pro-
videdbyTreeTaggerarealsousedtolookupsynonymdictionariesandEuroWordNetwhenusing
synonymhandlingatrun-time.
We were among the first to exploit automatically acquired paraphrases and to use these not
only to increase the size of the training corpus but also to better balance it4. We proceed as
follows.
First, we generated paraphrases using a pivot machine translation approach where each user
utteranceinthetrainingcorpus(around3610utterances)wastranslatedintosometargetlanguage
and back into French. Using six different languages (English, Spanish, Italian, German, Chi-
nese and Arabian), we generated around 38000 paraphrases. We used Google Translate API for
translating.
Second, we eliminate from these paraphrases, words that are likely to be incorrect lexical
translationsbyremovingwordswithlownormalisedtermfrequency(<0.001)acrosstranslations
i.e.,lexicaltranslationsgivenbyfewtranslationsand/ortranslationsystems. Wethenpreprocessed
theparaphrasesinthesamewaytheutterancesoftheinitialtrainingcorpuswerepreprocessedi.e.,
utteranceswereunaccented,convertedtolower-caseandstopwordswereremoved,theremaining
wordswerefilteredwithTF*IDF.Afterpreprocessing,duplicateswereremoved.
Third, we added the paraphrases to the training data seeking to improve the balance between
dialog acts per dialog. The process to balance data was guided by the deviation of the category
with lowest examples compared to the standard deviation. If the deviation is lower than the stan-
dard deviation then we add paraphrases by keeping as much as possible the data balanced after
replacement. Weinvitetheinterestedreadertofindmoredetailsaboutthealgorithmproposedfor
balancingdatainthepaper(GardentandRojas-Barahona,2013).
3http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/
4TheEmospeechdataishighlyskewedwithsomeclassesbeingpopulatedwithmanyutterancesandotherswith
few.
203.1LanguageUnderstanding
SubstitutingSynonymsforUnknownWords Awordisunknown,ifitisawell-formedFrench
word5 and if it does not appear in the training corpus. When an unknown word w is detected in a
playerutteranceatrun-time,wesearchforawordw′whichoccursinthetrainingdataandiseither
a synonym of w or a distributional neighbour. After disambiguation, we substitute the unknown
wordforthesynonym.
H Lemmatisation
H-H Orig. Lemmas +EWN +DIC +RI
Orig. 65.70%±5.62 66.04%±6.49 68.17%±6.98 67.92%±4.51 66.83%±5.92
Parap. 70.89%±6.45 74.31%±4.78* 74.60%±5.99* 73.07%±7.71* 72.63%±5.82*
H-C Orig. Lemmas +EWN +DIC +RI
Orig. 59.71%±16.42 59.88%±7.19 61.14%±16.65 61.41%±16.59 60.75%±17.39
Parap. 59.82%±15.53 59.48%±14.02 61.70%±14.09* 62.01%±14.37* 61.16%±14.41*
Table 3.5: Accuracy on the H-H and on the H-C corpus. The star denotes statistical significance
withtheWilcoxontest(p < 0.005)usedfortheHHcorpusandtheMcNemartest(p < 0.005)for
theHCcorpus.
Toidentifysynonyms,wemakeuseoftwolexicalresourcesnamely,theFrenchversionofEu-
roWordNet(EWN)(Vossen,1998),whichincludes92833synonyms,hyperonymsandhyponyms
pairs, and a synonym lexicon for French (DIC) 6 which contains 38505 lemmas and 254149 syn-
onympairs. WhilewordsarecategorisedintoNoun,VerbsandAdjectivesinEWN,DICcontains
noPOStaginformation.
Toidentifydistributionalneighbours,weconstructedsemanticwordspacesforeachsubdialog
in the EmoSpeech corpus 7 using random indexing (RI) on the training corpus expanded with
paraphrases. Using the cosine measure as similarity metrics, we then retrieve for any unknown
wordw,thewordw′ whichismostsimilartow andwhichappearinthetrainingcorpus.
For lexical disambiguation, two methods are compared. We use the POS tag provided by
TreeTagger. In this case, disambiguation is syntactic only. Or we pick the synonym with highest
probabilitybasedonatrigramlanguagemodeltrainedontheH-Hcorpus.
ResultsandDiscussion
Table 3.5 summarises the results obtained in four main configurations: (i) with and without para-
phrases; (ii) with and without synonym handling; (iii) with and without lemmatisation; and (iv)
when combining lemmatisation with synonym handling. We also compare the results obtained
when evaluating using 10-fold cross validation on the training data (H-H dialogs) vs. evaluating
theperformanceofthesystemonH-Cinteractions.
Overall Impact The largest performance gain is obtained by a combination of the three tech-
niques namely, data expansion, synonym handling and lemmatisation (+8.9 points for the cross-
5Awordisdeterminedtobeawell-formedFrenchwordifitoccursintheLEFFFdictionary,alarge-scalemor-
phologicalandsyntacticlexiconforFrench(Sagot,2010)
6DICOSYN(http://elsap1.unicaen.fr/dicosyn.html).
7WealsouseddistributionalsemanticsfromtheGigawordcorpusbuttheresultswerepoorprobablybecauseof
theverydifferenttextgenreanddomainsbetweenthetheGigawordandthegame.
21validationexperimentand+2.3fortheH-Cevaluation).
Impact of Lexical Substitution at Run Time We found that lexical resources are only useful
when combined with lemmatisation. This is unsurprising since synonym dictionaries and Eu-
roWordNet only contain lemmas. Indeed when distributional neighbours are used, lemmatisation
has little impact (e.g., 65.11% usingdistributional neighbours without lemmatisation on the H-H
corpuswithoutparaphrasesvs. 66.41%whenusinglemmatisation).
Another important issue when searching for a word synonym concerns lexical disambigua-
tion: the synonym used to replace an unknown word should capture the meaning of that word
in its given context. We tried using a language model trained on the training corpus to choose
between synonym candidates (i.e., selecting the synonym yielding the highest sentence proba-
bility when substituting that synonym for the unknown word) but did not obtain a significant
improvement. In contrast, it is noticeable that synonym handling has a higher impact when using
EuroWordNet as a lexical resource. Since EuroWordNet contain categorial information while the
synonym dictionaries we used do not, this suggests that the categorial disambiguation provided
byTreeTaggerhelpsidentifyinganappropriatesynonyminEuroWordNet.
Finally, it is clear that the lexical resources used for this experiment are limited in coverage
and quality. We observed in particular that some words which are very frequent in the training
data (and thus which could be used to replace unknown words) do not occur in the synonym
dictionaries. Forinstancewhenusingparaphrasesanddictionaries(fourthrowandfourthcolumn
in Table 3.5) 50% of the unknown words were solved, 17% were illformed and 33% remained
unsolved. Tocompensatethisdeficiency,wetriedcombiningthethreelexicalresourcesinvarious
ways (taking the union or combining them in a pipeline using the first resource that would yield
asynonym). Howevertheresultsdidnotimproveandeveninsomecasesworseneddueprobably
to the insufficient lexical disambiguation. Interestingly, the results show that paraphrases always
improves synonym handling presumably because it increases the size of the known vocabulary
therebyincreasingthepossibilityoffindingaknownsynonym.
Insum,synonymhandlinghelpsmostwhen(i)wordsarelemmatisedand(ii)unknownwords
can be at least partially (i.e., using POS tag information) disambiguated. Moreover since data
expansion increases the set of known words available as potential synonyms for unknown words,
combiningsynonymhandlingwithdataexpansionfurtherimprovesaccuracy.
ImpactofLemmatisation Whenevaluatingusingcrossvalidationonthetrainingcorpus,lem-
matisation increases accuracy by up to 3.42 points indicating that unseen word forms negatively
impact accuracy. Noticeably however, lemmatisation has no significant impact when evaluating
on the H-C corpus. This in turn suggests that the lower accuracy obtained on the H-C corpus
resultsnotfromunseenwordformsbutfromunseenlemmas.
Impact of Paraphrases On the H-H corpus, data expansion has no significant impact when
usedalone. Howeverityieldsanincreaseofupto8.27pointsandinfact,hasastatisticallysignif-
icant impact, for all configurations involving lemmatisation. Thus, data expansion is best used in
combinationwithlemmatisationandtheircombinationpermitscreatingbetter,morebalancedand
more general training data. On the H-C corpus however, the impact is negative or insignificant
suggesting that the decrease in performance on the H-C corpus is due to content words that are
223.1LanguageUnderstanding
newwithrespecttothetrainingdatai.e.,contentwordsforwhichneitherasynonymnoralemma
canbefoundintheexpandedtrainingdata.
While classifiers are routinely trained on dialog data to model the dialog management pro-
cess, the impact of such basic factors as lemmatisation, automatic data expansion and synonym
handling has remained largely unexplored. The empirical evaluation described here suggests that
each of these factors can help improve performance but that the impact will vary depending on
their combination and on the evaluation mode. Combining all three techniques yields the best re-
sults. Weconjecturethattherearetwomainreasonsforthis. First,synonymhandlingisbestused
incombinationwithPOStaggingandlemmatisationbecausethesesupportspartiallexicalseman-
tic disambiguation. Second, data expansion permits expanding the set of known words thereby
increasingthepossibilityoffindingaknownsynonymtoreplaceanunknownwordwith.
3.1.2 Spoken Language Understanding and Few-Shot Learning
The following work was published in Coling 2016 (Rojas Barahona et al., 2016). At that time
the task of Spoken Language Understanding (SLU), namely semantic decoding, was seen as a
sequencetaggingproblemwithmodelstrainedandtestedondatasetswithword-levelannotations
(Tu¨retal.,2013;Mesniletal.,2015;Yaoetal.,2013;Sarikayaetal.,2011;DeorasandSarikaya,
2013; Sarikaya et al., 2014). Nevertheless, spoken language understanding from unaligned data,
inwhichutterancesareannotatedwithanabstractsemantics,facestheadditionalchallengeofnot
knowingwhichspecificwordsarerelevantforextractingthesemantics. Thisproblemwastackled
in (Zhou and He, 2011), by using conditional random fields (CRFs) driven by finely-tuned hand-
crafted features. Other discriminative approaches that deal with unaligned data use some form of
delexicalisation or mapping of the input to known ontological concepts (Henderson et al., 2012,
2014a). The main disadvantage of delexicalisation is the difficulty in scaling it, not only to larger
andmorecomplexdialoguedomainsbutalsotohandlethemanyformsoflanguagevariation.
Weproposedasemanticdecoderthatlearnsfromunaligneddata(Figure3.2)andthatexploits
rich semantic distributed word representations instead of delexicalisation. The semantic decoder
predicts the dialogue act and the set of slot-value pairs from a set of n-best hypotheses returned
byanASR. Thepredictionismadeintwosteps. First,adeeplearningarchitectureisusedforthe
joint prediction of dialogue acts and the presence or absence of slots. Second, the same architec-
ture is reused for predicting the values of the slots that were detected by the first joint-classifier.
The deep architecture combines sentence and context representations. A CNN (Collobert et al.,
2011) is used to generate the sentence representation, while a LSTM network (Hochreiter and
Schmidhuber, 1997) is used to generate the context representation. A non-linear function then
combines the top layers of these neural networks and distinct Softmax layers are used to predict
the dialogue act and slots in the first joint model. In the second model, a single Softmax predicts
thepossiblevaluesforeachslot.
We evaluated our models on two datasets DSTC2 (Henderson et al., 2014b) and In-car (Tsi-
akoulisetal.,2012)usingaccuracy,f-measureandtheItemCrossEntropy(ICE)score(Thomson
etal.,2008). Ourmodelsoutperformpreviousproposedmodels,withoutusingmanuallydesigned
features and without any pre-processing of the input (e.g., stop words filtering, delexicalisation).
Theydothisbyexploitingdistributedwordrepresentationsandweclaimthatthisallowssemantic
decoderstobebuiltthatcaneasilyscaletolargerandmorecomplexdialoguedomains.
23voip-922209b777-20130325155209
System:Hello,welcometotheCambridgerestaurantsystem?
Youcanaskforrestaurantsbyarea,pricerangeorfoodtype.
HowmayIhelpyou? →welcomemsg
User: iamlookingforamoderatelypricedrestaurant
inthenorthpart →inform(area=north,pricerange=moderate)
System:meghnaisanicerestaurant
inthenorthoftowninthemoderatepricerange →offer(name=meghna),
inform(pricerange=moderate,area=north)
User: iseverythingelse →reqalts()
System:goldenwokisanicerestaurant
inthenorthoftowninthemoderatepricerange →offer(name=golden...),
inform(pricerange=moderate,area=north)
User: canigetthephonenumber →request(slot=phone)
System:Thephonenumberofgoldenwokis... →offer(name=goldenwok),inform(phone=01..)
User: typeoffooddotheyserve →request(slot=food)
Figure3.2: ExcerptfromadialogueintheDSTC2corpus. Thetop-bestASRhypothesisisshown
highlightedontheleft,andthecorrespondingusersemanticsisshownhighlightedontheright.
DeepLearningSemanticDecoder
We splitthe taskof semanticdecoding intotwo steps: (i) traininga jointmodel forpredicting the
dialogue act and presence or absence of slots and (ii) predicting the values for the most probable
slots detected in (i). As shown in Figure 3.3, we use the same deep learning architecture in both
steps for combining sentence and context representations to generate the final hidden unit that
feeds one or many softmax layers. In the first step, as shown in the Figure, there are distinct
softmax layers for the joint optimisation of the dialogue act and each possible slot. In the second
step there is a single softmax layer that predicts the value of each specific slot. In the following
weexplainthisarchitectureinmoredetail.
Figure 3.3: Combination of sentence and context representations for the joint prediction of dia-
logueactsandslots.
243.1LanguageUnderstanding
SentenceRepresentation
ACNNisusedforgeneratingthehypothesisrepresentation,thentheserepresentationsareweighted
bytheirconfidencescoresandthensummeduptoobtainthesentencerepresentation(Figure3.4).
The CNN is a variant of (Kim, 2014), in which the inputs are the word vectors in each ASR
hypothesis. Let x be a k−dimensional word embedding for the i-th word in a hypothesis. A
i
(cid:76) (cid:76) (cid:76) (cid:76)
hypothesis of length m is represented as: x = x x ... x where is the concate-
1:m 1 2 m
nation operator. A convolutional operation is applied to a window of l words to produce a new
feature.
c = f(w·x +b) (3.1)
i i:i+l−1
wheref isthehyperbolictangentfunction;w ∈ Rlk isafilterappliedtoawindowofl wordsand
b ∈ R is a bias term. The filter is applied to every window of words in the sentence to produce a
featuremap.
c = [c ,c ,...,c ] (3.2)
1 2 n−l+1
with c ∈ Rn−l+1. A max pooling operation is then applied to give the maximum value c =
max{c} as the representative feature for that filter. Multiple filters can be applied by varying the
ˆ
window size to obtain several adjacent features for a given hypothesis. These features f for the
j
hypothesisj ∈ H arethenmultipliedbytheASRconfidencescorep 8 andsummedoverallASR
j
hypothesestogeneratearepresentationforthesentences (Equation3.3),asshowninFigure3.4.
t
(cid:88)
ˆ
s = f ∗p (3.3)
t j j
j∈H
Nbest
i
’m
looking
for
uh
a
moderately
priced
restaurant
ASRhypotheses Convolutionallayers hypothesesrepresentationsSentenceRepresentation:
weightedsumofhyps
Figure 3.4: Sentence Representation: after applying convolution operations on the N-best list
of ASR hypotheses, the resulting hidden layers are weighted by the ASR confidence scores and
summed.
8Theposteriorprobabilityofhypothesisj intheN-bestlist.
25ContextRepresentation
An LSTM (Hochreiter and Schmidhuber, 1997) is used for tracking the context implied by pre-
vious dialogue system actions. The top layer of this LSTM network then provides the context
representationfordecodingthecurrentinpututterance.
An LSTM is a sequence model that utilises a memory cell capable of preserving states over
long periods of time. This cell is recurrently connected to itself and it has three multiplication
units, an input gate, a forget gate and an output gate. These gating vectors are in [0,1]. The cell
makesselectivedecisionsaboutwhatinformationispreserved,andwhentoallowaccesstounits,
viagatesthatopenandclose.
As shown in Figure 3.2, system actions are encoded in the form of a system dialogue act plus
oneormoreslot-valuepairs. Totrackthehistoryofsystemactions,slotsandvaluesaretreatedas
words and the input x is formed from its corresponding word vectors. The length of the context
t
can vary. We consider all the system actions previous to the current user utterance, or a window l
of the previous system actions. For instance, if we are currently processing the last user input in
Figure 3.2, in which L is the total number of system actions, we can consider all previous system
actions(L=4),orthelastl systemactions,wherel < L.
CombiningSentenceandContext
We study in this paper two ways of combining the sentence s and the context h representations.
t t
Thefirststraightforwardwayistoapplyanonlinearfunctiontotheirweightedsum:
hˆ = tanh(Ws·s +Wc·h ) (3.4)
t t t
The second way is to let the sentence representation be the last input to the LSTM network, then
hˆ = h . Forclassificationasoftmaxlayerisusedforeachprediction. Theresultoftheprediction
t t
isthemostprobableclass. Theback-propagationoptimisationisdonebyminimisingthenegative
log-likelihoodlossfunctionthroughstochasticgradientdescent.
ExperimentalEvaluation
In this section we introduce the corpora, and describe the experiments performed and the evalua-
tionmetricsused.
Corpora
Experimental evaluation used two similar datasets: DSTC2 (Henderson et al., 2014b) and In-
car (Tsiakoulis et al., 2012). Both corpora were collected using a spoken dialogue system which
provides restaurant information system for the city of Cambridge. Users can specify restaurant
suggestionsbyarea,price-rangeandfoodtypeandcanthenquerythesystemforadditionalrestau-
rant specific information such as phone number, post code and address. The first dialogue corpus
was released for the dialogue state tracking challenge and we use here the semantic annotations
thatwerealsoprovided9. Thetrainsethas2118dialoguesand15611turnsintotalwhilethetestset
has1117dialoguesand9890turnsintotal.
9TheDSTC2corpusispubliclyavailablein: http://camdial.org/˜mh521/dstc/
263.1LanguageUnderstanding
The second corpus contains dialogues collected under various noisy in-car conditions. In a
stationarycarwiththeairconditioningfanonandoff,inamovingcarandinacarsimulator(Tsi-
akoulisetal.,2012)10. Thetrainsethas1508dialoguesand10532turnsintotalandthetestsethas
641 dialogues and 4861 turns in total. Because of the noise, the average word error rate (WER =
37%)issignificantlyhigherthanforDSTC2(around29%).
Experiments
Step I: Joint classification of dialogue-acts and slots: We evaluated five different model con-
figurationsforthejointclassificationofdialogue-actsandpresenceorabsenceofslots.
• CNN:thesoftmaxlayersforthejointclassificationofdialogueactsandslotsareconnected
directlytotheCNNsentencerepresentationwithnocontext.
• CNN+LSTM:westudytheinfluenceofcontextbyconsideringtheprevioussystemactions
(Section 3.1.2, Eq. 3.4), here we study the different context length, by using a context win-
dowof1,4,andalltheprevioussystemactions,namelyCNN+LSTM w1,CNN+LSTM w4
andCNN+LSTM wrespectively.
• LSTM all: Finally, we study the impact of long distance dependencies, by using mainly
the LSTM model, with the previous system actions as input, but we inject the sentence
representationasthelastLSTMinput.
Step II: Classification of slot value pairs: We select the best model in step I for predicting
the presence of slots, then for each slot present we predict the value, by using again the best
architecturefromthepreviousstep.
EvaluationMetrics
We evaluate the performance of our models by using the conventional metrics for classification,
namely accuracy, precision, recall and F-measure (F1-score). In addition, we used the ICE score
to measure the overall quality of the distribution returned by the models taken into account the
hypothesesandthereferencesemantics(ie. ground-truth)(Thomsonetal.,2008).
ResultsandDiscussion
InthissectionwereporttheresultsonDSTC2andIn-cardialoguecorpora.
StepI:Jointclassificationofdialogue-actsandslots: Forthisstep,theclassifiersmustpredict
jointly 14 dialogue acts and 5 slots for the DSTC2 dataset as well as 14 dialogue acts and 7 slots
for the In-car dataset. We evaluate both (i) using 10 fold cross-validation on the trainsets and (ii)
onthecorpora’testsets.
Our results on 10 fold cross-validation results on both corpora suggest that for DTSC2, the
context representation is not significantly impacting the prediction. Although, the model with
a window of 4 ,CNN+LSTM w4, improves slightly the accuracy and f1-score. On the In-car
10Thiscorpushasbeenobtainedinanindustryfundedprojectandthereforeitisnotavailableforpublicuse.
27dataset, however, including the context does help to disambiguate the semantic predictions from
ill-formedhypotheses. Thisisexpected,sincethisdatasethasamuchhighererrorrateandhence
higher levels of confusion in the ASR output. Although there is no significant difference on the
f1-scorewhenusingtheimmediateprevioussystemact(w1)oralongercontext,CNN+LSTM w
givesabetteraccuracyandalowerICEscoreonthisdataset.
Table 3.6 shows the results on the test sets. Consequently, when evaluating on the DSTC2
test set, a window of 4 (w4), performs slightly better than other window sizes and better than the
simpleCNNmodel. OntheIn-cartestset,acontextwindowof4outperformsalltheothersettings:
CNN+LSTM. However, on this test set using the sentence representation as the last input to the
LSTMcontextneuralnetwork(section3.1.2)improvesthef1-scoreandreducestheICEerror.
Corpus Metric CNN CNN+LSTM LSTMall
- - - w1. w4 w -
DSTC2 acc. 96.03% 95.79% 95.79% 95.69% 95.59%
P. 89.73% 88.69% 88.95% 88.38% 88.15%
R. 84.74% 85.09% 86.02% 85.96% 84.76%
F1 87.14% 86.83% 87.43% 87.12% 86.42%
ICE 0.268 0.278 0.292 0.297 0.308
In-car acc. 87.60% 82.19% 82.25% 82.14% 82.3%
P. 69.96% 79.52% 79.29% 80.25% 78.12%
R. 62.14% 71.09% 71.59% 70.9% 74.04%
F1 65.53% 74.89% 75.15% 75.02% 75.9%
ICE 1.332 1.344 1.333 1.421 1.106
Table 3.6: Evaluation of the Step I on DSTC2 and In-car testsets. We also compare two ways
of combining sentence and context representation: CNN+LSTM models (combining sentence
and context representation through a non linear function) and LSTM all model (embedding the
sentencerepresentationintothecontextmodel).
Step II: Prediction of slot value pairs For evaluating Step II, we selected the best model ob-
tained during the 10-fold cross-validation experiments in terms of F1 score. For both corpora,
thiswastheCNN+LSTM w4configuration. ForDSTC2,itwasthe4th-foldcrossvalidationwith
Acc = 90.42%, F1 = 88.69% and ICE = 0.251. For In-car, it was the 5th-fold crossvalida-
tion with Acc = 93.13%, F1 = 81.49% and ICE = 0.393. We used these models to classify
whether a given slot appears in a given hypothesis or not. Then for that slot, we train another
CNN+LSTM w4 classifier for predicting its values. In the In-car corpus the slot ”type” has only
one possible value ”restaurant”. Similarly, the slot ”task” can only be the value ”find”. For these
slots with only one value, we report values using the model of Step I, since it is enough to detect
theslotintheutterance.
Given that there is no domain specific delexicalisation, the models achieve a good level of
performance overall (Table 3.7). Note that the slot ”food” has 74 possible values in DSTC2 and
25inIn-car. Hence,thisslothasmuchhighercardinalitythanalltheotherslots.
Overall performance A baseline for assessing overall performance is provided by the model
presented in (Henderson et al., 2012), in which the vector representation is obtained by summing
283.1LanguageUnderstanding
DSTC2 In-car
Slot Acc. P. R. F1 ICE Acc. P. R. F1 ICE
Slot11 95.29% 90.89% 95.72% 93.24% 0.478 89.92% 74.73% 61.56% 67.51% 0.743
Area 91.77% 92.66% 92.83% 92.74% 0.563 72.03% 72.56% 74.28% 73.41% 1.676
Food 71.37% 73.19% 76.02% 74.58% 1.989 66.46% 64.27% 68.70% 66.41% 2.309
Price 94.62% 91.33% 94.49% 92.89% 0.729 93.96% 88.77% 92.03% 90.37% 0.632
This12 98.70% 96.79% 93.92% 95.33% 0.113 97.16% 96.14% 84.72% 90.07% 0.214
Type - - - - - 95.56% 95.09% 86.69% 90.69% 0.290
Task - - - - - 97.12% 83.24% 64.93% 72.95% 0.175
Mean 90.35% 88.97% 90.60% 89.76% 0.774 87.47% 82.11% 76.13% 78.77% 0.863
St.Dev. 0.109 0.091 0.082 0.085 0.715 0.128 0.121 0.118 0.112 0.821
Table3.7: EvaluationofthestepII:theslot-valuepairsclassificationonDSTC2andIn-car.
upthefrequencyofn-gramsextractedfromthe10-besthypotheses,weightedbytheirconfidence
scores. Here we compare our performance against Henderson’s model with and without context
features,namelyWNGRAMS+CtxtandWNGRAMSrepectively. Hendersonreportedhisresults
on the In-car dataset. A similar model, namely SLU1, was evaluated on DSTC2 in (Williams,
2014). Both implementations consist of many binary classifiers for dialogue act and slot-value
pairs.
Corpus Model F1 ICE
DSTC2 SLU1(Williams,2014) 80.2% 1.943
CNN+LSTMw4 83.59% 0.758
In-car WNGRAMS(Hendersonetal.,2012) 70.8% 1.76
WNGRAMS+Ctxt(Hendersonetal.,2012) 74.2% 1.497
CNN+LSTMw4 73.06% 1.106
Table3.8: OverallperformanceofthesettingCNN+LST w4semanticdecoder.
In terms of the ICE score, the model CNN+LSTM W4 outperforms all the baselines (Ta-
ble3.8). IntermsoftheF1score,themodelsignificantlyoutperformstheSLU1andWNGRAMS
baselines. However it is slightly worse than WNGRAMS+Ctxt, which has been enhanced with
context features on In-car. Remember however, that our model uses only word-embeddings for
automatically generating sentence and context representations without having any manually de-
signedfeaturesorusingexplicitapplicationspecificsemanticdictionaries.
Few-shotLearningthroughRiskMinimisation(RM)
Wetreatrarelyseenslotsbyfollowingtwosteps. (i)Weoptimisejointlyinadeepneuralnetwork
theweightsthatfeedmultiplebinarySoftmaxunits. (ii)Wefurthertunetheweightslearnedinthe
previous step by minimising the theoretical risk of the binary classifiers as proposed in (Balasub-
ramanianetal.,2011). Inordertoapplythesecondstep,werelyontwoassumptions: therankof
the class marginal is assumed to be known and the class-conditional linear scores are assumed to
follow a Gaussian distribution. In (Balasubramanian et al., 2011), this approach has been proven
to converge towards the true optimal classifier risk. We conducted experiments on the dialogue
29corpus released for the third dialogue state tracking challenge, namely DSTC3 (Henderson et al.,
2014c)andweshowpositiveresultsfordetectingrareslotsaswellaszero-shotslot-valuepairs.
We use the unsupervised approach proposed in (Balasubramanian et al., 2011) for risk min-
imisation (RM). We assume a binary classifier that associates a score f (h) to the first class 0
W0
forthehiddenunith = (h ,··· ,h )ofdimensionn:
1 n
n
(cid:88)
f (h) = w h
W0 i i
i
wheretheparameterw ∈ IRrepresentstheweightofthefeatureindexedbyiforclass0.
i
Theobjectiveoftrainingistominimizetheclassifierrisk:
R(W) = E [L(Y,f (h))] (3.5)
p(h,Y) W
whereY isthetruelabelandL(Y,f (h))isthelossfunction. Theriskisderivedasfollows:
W
(cid:90) +∞
(cid:88)
R(W) = P(y) P(f (h) = α|y)L(y,α)dα (3.6)
W
−∞
y∈{0,1}
Weusethefollowinghingeloss:
L(y,α) = (1+α −α ) (3.7)
1−y y +
where (z) = max(0,z), and α = f (h) is the linear score for the correct class y. Similarly,
+ y Wy
α = f (h)isthelinearscoreforthewrongclass.
1−y W1−y
Given y and α, the loss value in the integral (Equation 3.6) can be computed easily. Two
terms remain: P(y) and P(f (h) = α|y). The former is the class marginal and is assumed to
W
beknown. Thelatteristheclass-conditionaldistributionofthelinearscores,whichisassumedto
be normallydistributed. Thisimplies thatP(f (h))is distributed as amixture oftwo Gaussians
W
(GMM):
(cid:88)
P(f (h)) = P(y)N(f (h);µ ,σ )
W W y y
y∈{0,1}
where N(z;µ,σ) is the normal probability density function. The parameters (µ ,σ ,µ ,σ ) can
0 0 1 1
beestimatedfromanunlabeledcorpusU usingastandardExpectation-Maximization(EM)algo-
rithm for GMM training. Once these parameters are known, it is possible to compute the integral
ˆ
inEq.3.6andthusanestimateR(W)oftheriskwithoutrelyingonanylabeledcorpus. In (Bala-
subramanianetal.,2011),ithasbeenproventhat: (i)theGaussianparametersestimatedwithEM
ˆ
converge towards their true values, (ii) R(W) converges towards the true risk R(W) and (iii) the
estimatedoptimumconvergestowardsthetrueoptimalparameters,whenthesizeoftheunlabeled
corpusincreasesinfinitely. ThisisstilltrueevenwhentheclasspriorsP(y)areunknown.
Theunsupervisedalgorithmisasfollows:
Unsupervisedtuningforthebinaryclassifierc,wherec=1,...,C
1: input:hthetophiddenlayerandtheweightsWc,astrainedbythedeeplearningdecoder(Section3.1.2).
2: output:ThetunedweightsWˆc
3: repeat
4: foreveryindexiinhdo,
5: ChangetheweightsWc=Wc+δ,
i i
303.1LanguageUnderstanding
6: EstimatetheGaussianparametersusingEM
7: Computetherisk(Eq.3.6)13ontheunlabeledcorpusU(i.e.theevaluationset).
8: Computethegradientusingfinitedifferences
9: UpdatetheweightsaccordinglyWc=ˆ Wc
i i
10: endfor
11: untilconvergence
Experiments
ThesupervisedandunsupervisedmodelsareevaluatedonDSTC3(Hendersonetal.,2014c)using
themacroF-Measure14. Wecomparethenthreedistinctmodels,(i)independentneuralmodelsfor
every binary classifier; (ii) neural models optimised jointly and (iii) further tuning of the weights
throughRM.
Dataset AsdisplayedinTable3.9ainDSTC3newslotswereintroducedrelativetoDSTC2. The
training set contains only a few examples of these slots while the test set contains a large number
of them. Interestingly, frequent values per slots in the trainset such as area=north, are absolutely
absent in the testset. In DSTC3 the dialogues are related to restaurants, pubs and coffee shops.
The new slots are: childrenallowed, hastv, hasinternet and near. Known slots, such as food, can
have zero-shot values as shown in Table 3.9b. The corpus contains 3246 dialogues, 25610 turns
inthetrainsetand2264dialogues,18715turnsinthetestset.
(a)FrequencyofslotsinDSTC3. (b)Somezero-shotvaluesperslotsinDSTC3.
Slot #Train #Test Slot Value #Train #Test
hastv 1 239 near trinitycollege 0 5
childrenallowed 2 119 food american 0 90
near 3 74 food chinesetakeaway 0 87
hasinternet 4 215 area romsey 0 127
area 3149 5384 area girton 0 118
food 5744 7809
The Gaussianity Assumption As explained in Section 3.1.2, the risk minimisation tuning as-
sumes the class-conditional linear scores are distributed normally. We verified this assumption
empirically on our unlabeled corpus U (i.e. DSTC3 testset) and we found that for the slots: chil-
drenallowed, hastv and hasinternet this assumption holds. However, the distribution for near
has a negative skew. When verifying the values per slot, this assumption does not hold for area.
Therefore,wecannotguaranteethismethodwillworkcorrectlyforareavaluesonthisevaluation
set.
13Aclosed-formisusedtocomputetheriskforbinaryclassifiers.(RojasBarahonaandCerisara,2015)
14The macro F-score was chosen because we are evaluating the capacity of the classifiers to predict the correct
class and both classes positive and negative are equally important for our task. Moreover, being nearly zero-shot
classifiers,itwouldbeunfairtoevaluateonlythecapacityofpredictingthepositivecategory.
31(a) Results for learning rare slots on DSTC3 evalua-(b) Results for learning zero shot slot-value pairs on
tionset. DSTC3evaluationset.
DeepLearningIndependentModels DeepLearningIndependentModels
Slot F-Measure Slot Value F-Measure
childrenallowed 49.84% near trinitycollege 49.99%
hastv 49.68% food american 49.88%
hasinternet 49.72% chinesetakeaway 49.88%
near 49.90% area romsey 49.83%
DeepLearningJointOptimisation girton 49.84%
childrenallowed 58.76% DeepLearningJointOptimisation
hastv 59.16% near trinitycollege 61.25%
hasinternet 58.77% food american 59.93%
near 56.65% chinesetakeaway 61.02%
RiskMinimisationTuning area romsey 51.30%
childrenallowed 61.64% girton 55.19%
hastv 61.35% RiskMinimisationTuning
hasinternet 60.87% near trinitycollege 62.08%
near 58.60% food american 62.52%
chinesetakeaway 63.79%
area romsey 48.76%
girton 51.45%
Results
Tables 3.10a and 3.10b display the performance of the models that predict slots and values re-
spectively. The low F-Measure in the independent models shown their inability to predict pos-
itive examples. The models improve significantly the precision and F-Measure after the joint-
optimisation. ApplyingRMtuningresultsinthebestF-Measureforalltherareslots(Table3.10a)
and for the values of the slots food and near (Table 3.10b). For area, the joint optimisation im-
provestheF-Measurebuttheimprovementislowerthanforotherslots. Theperformanceisbeing
affectedbyitslowcardinality(i.e. 20),thehighvariabilityofnewplacesandthefactthatfrequent
values such as north and east, are completely absent in the test set. As suspected, the RM tuning
degradedtheprecisionandF-MeasurebecausetheGaussianityassumptiondoesnotholdforarea.
However,RMwillworkwellinlargerevaluationsetsbecausetheGaussianassumptionwillhold
when the unlabelled corpus tends to infinite (please refer to (Balasubramanian et al., 2011) for
thetheoreticalproofs).
3.2 Dialogue Manager
My work on dialogue management regards learning the reward function. First, I explored inverse
reinforcementlearningtoinfertherewardfunctionfromhumanconversationsontheEmoSpeech
dataset (Rojas-Barahona and Cerisara, 2014). Second, I trained a predictor of the interaction
quality to infer the reward function in the PyDial dialogue framework (Rojas-Barahona, 2020).
Last but not least, I co-supervised a PhD thesis on imitation learning to solve the problem of
323.2DialogueManager
the scarce reward signal in dialogue systems. Besides imitation learning (Cordier et al., 2020),
we also explored graph neural networks for handling policies in multi-domain and multi-task
environments(Cordieretal.,2022). Furthermore,weusebothimitationandgraphneuralpolicies
forfew-shotlearning(Cordieretal.,2023).
3.2.1 Bayesian Inverse Reinforcement Learning
Inversereinforcementlearning(IRL)wasdefinedin(Ngetal.,2000)astheproblemofrecovering
the reward function from experts’ demonstrations. It learns an optimal reward, which leads to
a decision policy that follows as closely as possible the examples provided by experts, while
maximisingtheexpectedaccumulatedrewardinthelongrun.
In(RamachandranandAmir,2007)weusedBayesianInverseReinforcementLearning(BIRL)
to infer human behaviour in the context of the Emospeech serious game (Section 3.1.1), given
evidence in the form of stored dialogues provided by experts, who played the role of several con-
versationalagentsinthegame. Wealsoreducethecomputationalcomplexityinlargestatespaces
byusingtheapproachproposedby(MichiniandHow,2012). Insteadofdesigninginadvancethe
reward function to “properly instruct players”, which is a difficult and subjective task, we rather
proposetolearnitfromhumans.
We evaluated BIRL in terms of policy loss (Michini and How, 2012) and is compared against
twobaselines. Thefirstoneusesrandomrewards,whilethesecondoneexploitscorpus-estimated
locally optimal rewards (i.e., supervised learning). The results show that the proposed approach
converges relatively quickly and consistently outperforms both baselines. This suggests that tak-
ing into account the dynamic properties of the environment leads to virtual characters that better
reproduce the behaviour of experts. Qualitatively, our models have thus learned to adequately
informusersandprovidehelpwhenneeded.
States,ActionsandTransitions
AsshowninTable3.1,thereare12distinctconversationsinthegamebetween7virtualcharacters
(VC) and 3 players. Each of these dialogues talks about mandatory and optional goals. The
player either asks for information about these goals or asks for help. Accordingly, the virtual
charactereitherinformsaboutthegoalsorprovideshelp. Itcanalsohandleoutofdomaintopics,
misunderstandingsorrequestinformation(seeexampleofdialogueinFigure3.1).
We designed coarse-grained states containing user and system contributions to the dialogue;
eitherbyexplicitlyaskingaboutthedomainspecifictasks(i.e. thedialoguegoals)orbyproducing
general dialogue acts (e.g., greeting, asking for help, acknowledgments, etc). A binary variable
that indicates whether the dialogue has finished is also included. With this state representation
we have 32 states for the shortest dialogue (the first dialogue in Table 3.1), and 432 states for the
longestdialogue(i.e.,thethirddialogueinTable3.1with5goals).
Statevariables
1. Hasanyofthecharactersendedthedialoguewithafarewellaction? : 1forsettingaterminal
state,0otherwise.
332. The last goal either informed or requested by the system: 0 when the system has not in-
formed/requested about any goal, otherwise the goal id (e.g., from 1 to up to 5 for the
longestdialogue).
3. The last goal either asked or confirmed by the player: 0 when the user has not yet asked/-
confirmedaboutanygoal,otherwisetheidofthegoal(e.g.,from1toupto5forthelongest
dialogue).
4. Thelastgeneraldialogueactproducedbythesystem: 0forabsenceofgeneraldialogact,1
when providing help, and 2 when asking the player about the task to be solved (e.g., ”How
mayIhelpyou”).
5. Theuserhasaskedforhelp: 0iftheuserhasnotaskedforhelp,1otherwise.
Actions Weareconsideringonlythefollowingactionsinourexperiments.
• quit: farewellgreeting.
• inform(do(g )): informingabouthowtoachievegoalg .
i i
• inform(help): providinghelp
• ask(task(X)):Askingtheplayeraboutthetask,itcorrespondstoageneralwelcomesentence
(e.g., ”How may I help you”). Note that this action neither occurs in dialogue 1 nor in
dialogue7.
• WAIT:thesystemgivestheturnbacktotheuser.
• ack: thesystemacknowledgesunderstanding.
• other: thesystemanswerstooutofcontextturns.
Virtual characters always greet the player at the beginning; thus we do not need to learn this
behaviour.
Transition Function The transition function is not deterministic when the next state reflects
an (unpredictable) user action. This is typically the case after the WAIT system action. How-
ever, BIRL requires this transition function to be given, and we have thus estimated such non-
deterministictransitionprobabilitiesusingsmoothedcountsfromtheobservedcorpusasfollows:
P(s′|s,a) = N(s,a,s′)+α
N(s,a)+Nχα
Where N(s,a,s′) and N(s,a) are respectively the number of times the transition (s,a,s′) and
the state-action pair (s,a) have been observed in the corpus, and N is the number of observed
χ
state-actionpairs. α isasmoothingconstantarbitrarilysetto0.1.
Theothertransitionsthatreflectasystemactionaredeterministicandhavebeendefinedas:
(cid:40)
1, if s′ = next s(s,a)
P(s′|s,a) =
0, otherwise
343.2DialogueManager
Wherenext s(s,a)isafunctionthatcomputesthenextstategivenasystemactiona. Forinstance,
when the system informs about the first goal, g , the action a = inform(do(g )) yields the next
1 t 1
states′ tohavethestatevariable2setto1.
BayesianInverseReinforcementLearning
The IRL problem as defined in (Ng et al., 2000) is described as follows: given a finite state space
S, a set of actions A = {a ,a ,...a }, a transition probability Pa , a discount factor γ, and a
1 2 k ss′
policy π, determine a set of possible reward functions R such that π is the optimal policy for the
givenMDP.TheIRLproblemisanill-posedproblem(AbbeelandNg,2004),becausepotentially
an infinite number of rewards may be optimal. Bayesian IRL approaches model this uncertainty
byinferringtheposteriordistributionoftherewardvectorR,treatingthedemonstrationsequences
astheevidenceandrelyingonapriorontherewardfunction(RamachandranandAmir,2007).
TheIRLagentreceivesasequenceofobservationsoftheexpert’sbehaviour:
O = {(s ,a ),(s ,a ),...,(s ,a )}, which means that at time step i, the virtual character χ
χ 1 1 2 2 k k
that mimics the expert is in state s and takes the action a . After applying Bayes Theorem, the
i i
posteriorcanbewrittenas:
Pr(O |R)Pr(R)
χ
Pr(R|O ) = (3.8)
χ
Pr(O )
χ
We model next the reward function by a simple n-dimensional real vector, where n is the
number of different states. Then, Pr(R|O ) is the posterior distribution of the reward vector
χ
given the observed state-action pairs of the expert. Pr(O |R) is the likelihood of the observed
χ
expertstate-actionpairsgiventherewardvectorR. Thislikelihoodismodelledin(Ramachandran
andAmir,2007)withaparameterαrepresentingthedegreeofconfidencewehaveintheexpert’s
abilitytochooseagoodactionasfollows:
1
Pr(O χ|R) = eα(cid:80) iQ∗(si,ai,R) (3.9)
Z
Pr(R) is the prior distribution and Pr(O ) is the probability of the evidence over the entire
χ
space of reward vectors R, which is not needed in the BIRL algorithm. The original BIRL algo-
rithm,namelyPolicyWalk,followsaMarkovChainMonteCarlo(MCMC)techniqueiteratingas
follows: Given a reward vector R, it performs random walks over the neighbours of R on a grid
oflengthδ,findinganewproposalR¯ ,suchthat: R¯(s) = R(s)±δ. Theproposalisacceptedwith
probabilitymin{1,
Pr(R¯|O)},wheretheposteriorisgivenbyEq(3.8).
Pr(R|O)
Theexpectedvalueoftherewardgiventhisposterioristhencomputedoverallthesesamples.
Note that the normalising constants cancel out in the ratio used to accept the proposed
R¯(s)
and
thatfindingQ∗ inEq(3.9)requirestosolvetheMDPateveryMCMCiteration. Thiscanbedone
forexamplewiththepolicyiteration(PI)algorithm(SuttonandBarto,2018).
BIRLconvergesslowlywhenappliedtolargestatespaces. Onereasonforthisisthatitinfers
therewardofeverystate,althoughmanystateshavelittleexpertevidence. Second,searchingover
a reward function space easily increases the number of MCMC iterations needed to approximate
the mean of the posterior. To solve these limitations, (Michini and How, 2012) proposed a
modifiedBIRL(MBIRL)that:
• infers only those states that are similar to the observed ones according to a kernel-based
relevancefunction.
35• uses simulated annealing to focus the sampled distribution around its maximum, hence re-
ducing the number of samples needed to converge. Therefore, they use a modified accep-
(cid:16) (cid:17) 1
tanceprobabilityof Pr(R¯|O) Ti whereT isadecreasingcoolingschedule.
Pr(R|O) i
Experiments
We introduce the baselines, the evaluation metrics, and the experiment setup for 12 dialogues in
thegame.
Baselines Weevaluatetheperformancesoftheproposedsystembycomparingitwithtwobase-
lines:
• Usingrandomrewards(RR);
• Exploiting ”locally-estimated“ rewards (LR), i.e., rewards that are trained on the corpus
with the additional assumptions that the reward prior Pr(R) is uniform, that the states
(cid:81)
are conditionally independent given the reward P(O |R) = P(s |R) and that the state
χ i i
likelihood is multinomial with parameters representing the reward P(s = k|R) = R , so
k
thatthepaththatmaximizesthecumulatedrewardalsomaximizesthelikelihood. Then:
argmaxPr(R|O ) = argmaxPr(O |R)
χ χ
R R
(cid:89)
= argmax P(s |R) (3.10)
i
R
i
Let n be the number of times the kth state occurs in the expert observations: n = |{(s =
k k i
k,a )} |
i i∈Oχ
Thenwewanttomaximizethelikelihood(cid:81) P(s = k|R)n k undertheconstraint(cid:80) R =
k k k
1,whichgivesthelocallyoptimumreward:
n
ˆ k
R =
k
N
χ
withN = |{(s ,a )} |thenumberofobservedstate-actionpairs.
χ i i i∈Oχ
Evaluation Metrics We consider two evaluation metrics: the policy loss (Michini and How,
2012)andthesystemtrainingtime.
n
• Policy loss: The policy loss is the ratio ̸=, where n = |{(s ,a ̸= π(s ))} | is the
Nχ ̸= i i i i∈Oχ
number of expert state-action pairs that disagree with the learned policy π and N =
χ
|{(s ,a )} |isthenumberofobservedstate-actionpairs.
i i i∈Oχ
• Elapsed time: The time in milliseconds it takes to MBIRL and to the policy iteration algo-
rithmstofinish.
363.2DialogueManager
Figure 3.5: Comparison of expert vs. MBIRL trajectories for dialogues 7 (top) and 2 (bottom).
(a) and (c) depict expert trajectories, while (b) and (d) show the trajectories of MBIRL optimal
policyπ.
Performance: Twoimportantissuesaffectperformance: thesizeofthestate-spaceandthelim-
ited number of expert observations. In general MBIRL outperforms both locally-optimal and
randomrewards.However,withalargerstate-actionspacesuchasindialogue3,4and8,themod-
els do not improve over the locally-optimal reward, which suggests that the number of samples
thataregeneratedisnotlargeenough. Moreover,forstatespacesgreaterthan300states,MBIRL
takes a prohibitively long running time to finish. The huge computational expense for large state
spaces is an important limitation of MBIRL since it needs to solve one RL problem per iteration.
A potential solution to this issue might be to use appropriate function approximation both for the
QfunctionandformodellingtherewardfunctionR,butthisisleftforfuturework.
Unsurprisingly, the MBIRL policy loss is higher for optional dialogues (such as dialogue 10
and12)showingthatthescarcenumberofobservationssignificantlyaffectsperformance.
Trajectories: Figure 3.5 shows two dialogue trajectory excerpts with both the gold (or expert)
trajectory and the trajectory inferred by MBIRL. Interestingly, in most of the dialogues, both
trajectories coincide in the first state and in those states where the system has to inform about
mandatory goals just after explicitly requested by the user. This is also the case of the states
where the system properly provides help as requested by the user. On the other hand, the learned
policy usually fails to close the dialogue and it sometimes contains repetitions e.g., once it has
informedaboutagoal,itmayinformagainlateron.
Formoredetailspleasereferto(Rojas-BarahonaandCerisara,2014).
3.2.2 Is the User Enjoying the Conversation?
The impact of user satisfaction in policy learning for task-oriented dialogue systems has long
been a subject of research interest (Walker et al., 1997b; Schmitt et al., 2011; Ultes et al., 2015;
Ultes, 2019). Similarly, sentiment analysis has been widely adopted to analyse massive blogs,
recommendations tweets and reviews (Rojas-Barahona, 2016; Do et al., 2019). Although senti-
37ment analysis can be used to infer user satisfaction, most of the work that incorporates sentiment
analysis in dialogue focused on the generation of empathetic responses in end-to-end chitchat di-
alogues (Lee et al., 2018; Ma et al., 2020). Moreover, most sentiment analysis solutions focus
on the analysis of out-of-context short texts (Rojas-Barahona, 2016). In this work we are inter-
ested in the study of user satisfaction for measuring the quality of the interaction in task-oriented
dialogues,inwhichdialogueismodelledasaPOMDP(Youngetal.,2013a).
Firstwestudydistinctneuralnetworksthatusedistributedrepresentationsforpredictingsatis-
factionscores. Atthisstagewewouldliketoanswerthefollowingquestion: doesrelyingonlyon
distributedrepresentationsimprovetheperformanceofneuralmodels? Therefore,weevaluatethe
performance of hierarchical networks and state-of-the-art Transformers. Second, we evaluate the
impactofusingthebesttrainednetworkforcomputingtherewardfunctionwithinaPOMDPdia-
logueframework(Ultesetal.,2017b). Wewouldliketodeterminehowrealisticitistoincorporate
user satisfaction estimators that rely solely on distributional semantics in reinforcement learning
(RL) dialogue systems. This approach can be used for instance to train satisfaction predictors
fromlargehuman-humanchats,inwhichsatisfactionhasbeenself-scoredbyusers.
ThecasestudyistheEnglishLEGOcorpusofhuman-machinespokenconversations(Schmitt
etal.,2012),whichhasbeenannotatedateachsystemturnwiththeInteractionQuality(IQ),rang-
ing from 1 (poor quality) to 5 (good quality). Our results suggest that distributed representations
do outperform state-of-the-art models trained on fine-tuned features. We also show that using IQ
estimators in the reward function greatly improves the task success rate for dialogues in the same
domain the networks were trained on, which in this case is the Let’s Go domain (Raux et al.,
2005).
NetworksforEstimatingtheUserSatisfaction
Westudythreedistinctneuralnetworks: hierarchicalbi-directionalGatedRecurrentUnits(GRUs)(Cho
et al., 2014) with attention , Transformers for generating contextual embeddings that feed a GRU
layer and solely Transformers (Vaswani et al., 2017). We are interested in studying the impact of
context-length in transformers, because real dialogues can easily attain a context of thousands of
tokens (Table 3.11). Therefore, we explore BERT (Devlin et al., 2018), DistilBERT (Sanh et al.,
2019)andTransformerseXtra-Large(Transformers-XL)(Daietal.,2019).
(i)HierarchicalGRUs: Figure3.6(a)showsthenetwork. IthasaBidirectionalGRUlayer(Bi-
GRU)atthelowerlevelthatreturnstheturnrepresentationh Attentionisusedtoweightrelevant
tk
units in the turn hidden representation. Then, a GRU layer is then used to process dialogues as a
sequence of turns. The last layer is a Softmax that predicts the most probable IQ class from 0 up
to5.
(ii)Contextualembeddings+GRU: Weinvestigatetheuseoftransformersasturnrepresenta-
tions thus we propose a BERT-like transformer, which inputs are the tokens of the turn. The turn
representations then feed a GRU layer. The output of the GRU then feeds a Softmax layer for
predictingthescore.
(iii) Transformer: This network is depicted in Figure 3.6(c). It consists in a transformer that
takes as input the tokens tok ,...,tok of the previous and current utterances. Then the output
1 D
383.2DialogueManager
(a)HierarchicalGRUs (b)Contextualembeddings+GRU (c)Transformer
Figure3.6: Theproposedneuralarchitecturesforpredictingtheusersatisfaction. E in(a)isthe
Tk
embedding for the kth token of the last turn (t = T). In (b) tok = [CLS] and tok = [SEP]
T1 Tk
after applying the WordPiece tokenization to the last turn (t = T). In (c) tok ,...,tok are the
1 D
tokensofthedialogueafterapplyingtheWordPiecetokenization,inwhichtok = [CLS]andthe
1
token [SEP] marks turn separation. tok is the last token of the last utterance in the dialogue
D−1
andtok = [SEP].
D
[CLS]ofthetransformerfeedsaSoftmaxlayerforpredictingthescoreofthecurrentutterance.
Weevaluatethepredictiony ateachsystemturnt. Theback-propagationoptimisationisdone
t
by minimising the cross-entropy loss function Tieleman and Hinton (2012) through stochastic
gradientdescent.
TheRewardFunctioninaPOMDPDialogueSystem
Figure3.7,showsthearchitectureconnectedtoarewardestimator. IntheLet’sGobus-scheduled
information system, the slots might be origin for the bus departure place and time for the
departuretime. Thestatesmightrecordthecurrentvalueandconfidencelevelofeachslot. From
the state, a belief state b is extracted and an action a is decided based on a dialogue policy. Once
the appropriate action is determined, it is converted to a textual message and then rendered by a
speechsynthesiser.
The reward function most commonly adopted for task-oriented dialogues penalises every di-
alogue turn with −1 and sums a reward of +20 at the end of the dialogue whenever the system
providedtherightinformationtotheuseror0otherwise(Eq.3.11)(Gasˇic´ andYoung,2013).
R = T ·(−1)+1 ·20 (3.11)
TS TS
InthisworktherewardestimatorisbasedontheIQasdefinedin(Ultes,2019).
R = T ·(−1)+(iq −1)·5 (3.12)
IQ
WhereR describesthefinalreward,iq istheIQvaluepredictedbytheclassifier(Section3.2.2),
IQ
whichisanumberfrom1to5,where1representspoorqualityand5goodquality.
39BeliefPropagation
ASR Se Pm ara sn et ric evidence B Se tali te ef
Tracker
Knowledge base
TTS Generator actions Stochastic
Policy
Reinforcement Learning
Reward: success/fail
Reward Function
Interaction
IQ estimator API Quality
Classifier
Figure3.7: APOMDPspokendialoguesystemwiththeinteractionqualityestimator.
We used PyDial (Ultes et al., 2017b), the publicly available POMDP dialogue framework
and we implemented an application programming interface (API) that returns the IQ estimation
predicted by the neural models presented above. Usually, RL systems first learn the policy on
a simulated user until an optimal performance is reached, then they are ready to be tested by
humans.
Experiments
In this section we introduce the corpus as well as describe the experiments and the evaluation
metrics.
The Dataset The LEGO corpus collects spoken dialogues between users and the Let’s Go di-
alogue system Raux et al. (2005), which provides bus schedule information to the Pittsburgh
populationduringoff-peaktimes. 400dialoguesinthecorpushavebeenmanuallyannotatedwith
the IQ score Schmitt et al. (2012). Since conversations in LEGO are system-initiative they have
a lot of system interactions such as misunderstandings, confirmations and repetitions, producing
quitelongdialogues(i.e. hundredsofturns).
The complexity of the corpus is presented in Table 3.11, containing long dialogues of up to
200turnswithupto76tokensperturn.
UserSatisfactionEstimators WecomparedournetworkswiththenetworkspresentedinUltes
(2019)forpredictingtheIQwiththefollowingevaluationmetrics: theunweightedaveragerecall
(UAR), which is the arithmetic average of all class-wise recalls, as well as a linearly weighted
403.2DialogueManager
N.Dialogues Dialoguelength Max.turnlength Max. toksp/dial.
(max/mean/median) (max/mean/median) (max/mean/median)
400 200/65/53 76/26/24 6590/1444/1089
Table 3.11: The LEGO corpus with IQ annotations. Its complexity is measured by the maximum
dialogue length (number of turns per dialogue), the maximum turn length (number of tokens per
turn)andthemaximumnumberoftokensperdialogue.
version of Cohen’s κ and Spearman’s ρ. The experiments were conducted in a 10-fold cross-
validation,assuringthatthesamedialoguedidnotslipintodifferentfolds(i.e. dialogue-wisecross
validation). We studied the length of the dialogue context: the turn for which we are predicting
thescoreandthepreviousturns. Wevarythecontextlengthandfindanoptimalcontextlengthof
upto100turnsperdialogueforBiGRUs.
Table 3.12 shows that the BiGRUs network trained on word embeddings outperformed the
state-of-the-art (BiLSTM+att) networks in all performance measures, obtaining an absolute im-
provement of +1 for UAR, +9 for κ and +2 for ρ. It is worth noting that the state-of-the-art
(BiLSTM+att)networksweretrainedonfine-tunedturnfeatures.
These results are encouraging and suggest that distributed representations impact positively
theperformanceofsatisfactionestimatorsinhierarchicalnetworks. Wewouldliketostudyinthe
nextsectionwhetherthesemodelscanbeusedtopredicttasksuccessindialoguesystems.
PredictingIQ
Model UAR κ ρ
SVM featsUltes(2019) 44% 53% 69%
BiLSTM+att featsUltes(2019) 54% 65% 81%
BiGRUs 55% 74% 83%
DBert+GRU 35% 57% 42%
Trans-XL(ctxt≈1K) 47% 59% 67%
Trans-XL(ctxt≈2K) 44% 58% 67%
Table 3.12: Performance of the proposed models. The BiGRUs with FastText embeddings out-
performallthenetworkstrainedonfine-tunedturnfeatures
.
BERT-based Transformers do not perform well for this task on this dataset when using them
togettheturnrepresentations. Thiscanbeexplainedbythelargenumberofturnsdialogueshave
(i.e., up to 200 turns), the large number of parameters a transformer needs and the quite short
annotated dataset (≈ 400 dialogues). We first tried (BERT+GRU) and due to its large memory
requirement, we could only learn weights of up to 7 dialogue turns per dialogue in a cluster of
32GB-GPUmachines(alltheotherturnsrepresentationswerefrozen). However,withDistilBERT
we could treat up to 15 turns per dialogue while maintaining the same performance. Fortunately,
we could process larger contexts with Transformers-XL, reaching an optimal performance with
≈ 1K dialogue tokens. The results of Transformers-XL are comparable with the SVM baseline
trained on fine-tuned features, yielding a better UAR (+3) and κ (+6) as well as a slightly lower
ρ (−2). Having larger contexts ≈ 2K do not seem to impact significantly their performance.
Theseresultssuggestthatin32GB-GPUnodesthelargecontextlength(i.e. upto6.5Ktokensper
41dialogue) is affecting transformers performance as they will require a prohibited usage of GPU
memorytoprocessthewholecontext.
The Impact on the Reward Function We evaluated the impact of the IQ estimators presented
in Section 3.2.2 on the reward function for the Let’s Go (LetsGo) domain by using PyDial (Ultes
etal.,2017b).
The Let’s Go dialogue system provides information about bus time-schedule according to
the constraints: origin, destination, time and route, which corresponds to LetsGo(4).
LestGo(6)alsoconsidersorigin neighbourhoodanddestination neighbourhood. Itis
importanttonotethattheLet’sGodomainisfarmorecomplexintermsofthenumberofdatabase
itemsthanotherdomainsavailableinPyDial.
The experiments run on simulated dialogues as in (Ultes, 2019; Casanueva et al., 2017b). We
implemented a template-based generator for the user and the system utterances for the Let’s Go
domainbecauseourmodelsrelyontextualinputs(i.e. distributedrepresentations). Wecompared
our models in an environment without noise because unlike (Casanueva et al., 2017b) and (Ultes,
2019) the simulator in this work runs at the surface-level and not at the semantic-level and the
noise used for User-Simulation in PyDial alters the semantic-dialogue acts regardless the surface
form. In addition, we would like to apply these methods to chatbots, thus simulating ASR noise
wouldnotbeappropriateandstudyingamoreappropriatednoiseisoutofthescopeofthispaper.
Domain Reward TaskSuccessRate(↑) AverageTurns(↓)
LetsGo(4) R 99%±1.4 6.5±0.96
TS
R 99%±2.1 6.3±0.8
IQ
LetsGo(6) R 81%±7.78 11±1.09
TS
R 97%±3.98 8.9±1.26
IQ
Table 3.13: Task success rate of the simulated experiments for the Let’s Go domain over three
runswithdistinctseeds. Eachvalueiscomputedafter1000trainingdialogues/100evaluation.
We used a policy model based on the GP-SARSA algorithm (Gasˇic´ and Young, 2013), which
is a sample efficient Gaussian process approximation to the value function. We used the focus
tracker (Henderson et al., 2014b) for belief tracking. The policy decides on summary actions of
thedialoguestatetrackerwhicharebasedondialogueacts(e.g.,request,informorconfirm). The
task success rate was the metric used to measure the dialogue performance (Casanueva et al.,
2017b).
We observe in Table 3.13 that the reward computed with R outperforms the classical R
IQ TS
reward when having more constraints, namely LetsGO (6). Moreover, dialogues rewarded by
R tend to be significantly shorter. Although there is not significant distinction between R
IQ TS
and R in terms of the task success for LetsGO (4), dialogues are slightly short with R . We
IQ IQ
also conducted preliminary experiments on domain transfer by evaluating R on the Cambridge
IQ
Restaurants domain, obtaining a success rate of 44±25.7, compared to 99±1.83 for R . Un-
TS
surprisingly,R andfeature-basedR Q(Ultes,2019)aremorerobusttounknowndomainsthan
TS I
embedding-basedR becausebothtask-successandqualityfeaturesaredomain-agnostic.
IQ
Formoredetails,pleasereferto(Rojas-Barahona,2020).
423.2DialogueManager
3.2.3 Imitation Learning
This section introduces the work of the PhD candidate Thibault Cordier, who I co-supervised
together with Dr.Tanguy Urvoy and Professor Fabrice Lefevre. This work explores imitation
learningforlearningthepolicyonsingledomains. IthasbeenpublishedintheNeurIPSworkshop
Humanintheloopdialoguesystems(Cordieretal.,2020).
Deep RL (DRL) (Li, 2018) has achieved significant success on many complex decision-
making problems and in particular in conversational AI (Gao et al., 2018). The ability to learn
from few interactions is essential in dialogue applications because human interactions are scarce
and costly. Unfortunately, standard RL algorithms usually require a large amount of interactions
with the environment to reach good performances. One solution to speedup the learning process
is to guide the agent’s exploration like stochastic learning policies such us soft-kind (Haarnoja
etal.,2017,2018;Gaoetal.,2019). Thefirstquestionweaddresshereis: (i)candialoguepolicy
learningbeimprovedwithstochasticoff-policylearningmethods?
Several methods search a way to exploit demonstrations to accelerate the learning with the
conviction that demonstrations are the solution to the sparse reward (Hester et al., 2017). They
canbebasedonImitationLearning(IL)tolearnan”optimal”policyfunctionoronInverseRein-
forcement Learning (IRL) to learn an ”optimal” reward function. Demonstrations may be useful
to guide efficiently the exploration. We consider that human expertise can be used in different
ways. The most classical one is to use demonstrations that humans have already produced. An-
otherwayistousearule-basedagent,namelyhandcrafted,thathasbeendesigned,evaluatedand
fine-tunedbyhumans. Indeed,(Casanuevaetal.,2017c)haveshownthathandcraftedapproaches
stillperformbetterthanpolicylearningapproaches.
Moreover it is well known that human interactions and manually crafted rules are not only
costlybutalsotimeconsuming;whilesimulatedinteractionsarecheaperandeasiertocollect(Su
et al., 2016; Schatzmann et al., 2006). Therefore, the second issue that raised in our work is:
(ii) can we use demonstrations without supervision and only as a way to guide exploration in an
on-linelearning?
ProposedApproach
The proposed approach to policy learning is based on Boltzmann sampling, and to which we
integratedemonstrationsdirectlyintotheRLprocessinordertobetterguideexplorationandmake
relevant exploitation. RL is usually implemented as either value-based method as Q-learning
or policy-based method as actor-critic. Both are our baselines with which we will make our
contributions.
Baselines Q-learning is the combination of Double and Duelling Deep-Q-Network with Expe-
rience Replay (DQN/D3QN) (van Hasselt et al., 2015; Wang et al., 2016b). In short, DQN is
an approximation function that searches to estimate the optimal state-action value function (or
Q-value). The Double DQN architecture is an alternative method that mitigates the problem of
overoptimisticvalueestimationandtheDuellingDQNarchitectureisforlearningmoreefficiently
by decoupling value and advantage functions. On top of that, experience replay can be added for
reducingsamplecorrelationandforimprovingsampleefficiency.
The actor-critic baseline is the Trust Region Policy Optimisation for Actor-Critic with Ex-
perience Replay (A2C/ACER/TRACER) (Wang et al., 2017; Weisz et al., 2018b). In brief, an
43actorπ triestomaximisetheexpectedrewardwheninthesametimeacriticQlearnedseparately
evaluatestheactordecisions.
Theimportancesamplingtruncationwithbiascorrectionisusedtocorrecttheperceivedsam-
pling distribution induced by experience replay in order to reduce variance. We apply the Re-
trace algorithm (Munos et al., 2016) to recursively estimate the advantage function in safe and
efficient way with small bias and variance. Finally, we use the trust region policy optimisation
(TRPO)(Schulmanetal.,2015)foradjustingthepolicygradientinordertolearninasafeparam-
etersregionlimitingthedeteriorationofthepolicyperformance.
ExplorationStrategy
The exploration strategy learns and plays a stochastic policy related to energy-based model. We
proposetolearnanenergy-basedpolicyinthedialogueenvironmentwheretheagentsampleshis
actions according to Boltzmann’s stochastic sampling (Haarnoja et al., 2017). We opt for using
energy-basedpoliciesofthefollowingform:
π(a |b ) ∝ exp(−E(b ,a )) (3.13)
t t t t
When using value-based method, the energy function can be represented by the Q-function
with parameter τ, the temperature, where we set E(b ,a ) = −1Qπ(b ,a ). When using policy-
t t τ t t
based method, the energy function is directly represented by the policy network and so is learned
implicitly.
AstochasticsamplingcanbeachievedbytheBoltzmannsampling. Contrarytothecommonly
usedstrategyasϵ-greedywhereactionsa aresampledfromargmax (1−ϵ)argmax π(a|b )+
t a a t
ϵU(A)whereU(A)isanuniformdistributionoveractionspace,theBoltzmannsamplingstrategy
samplesactionsfromexp(−E(b ,a )),hencea ∼ π(a |b ).
t t t t t
Oneofitsadvantagesisthatpolicylearningislessinfluencedbythepolicyfunctionchanges.
Conversely, the ϵ-greedy sampling faces sudden jumps in action choices due to the argmax oper-
ator. Sointheory,theBoltzmannstrategycanmakelearningmorestablethantheϵ-greedy.
Another advantage is that the temperature parameter can control the exploration-exploitation
balance. So, to counter the weakness of its exploitation, it can be interesting to define correctly
thetemperature.
In practice, we decide to add random exploration in such a way that the actions are sampled
according to a ∼ (1 − ϵ)π(a |b ) + ϵU(A), namely ϵ-Boltzmann sampling, with decreasing ϵ
t t t
parameter in order to explore enough before following the stochastic policy with a fixed τ tem-
peratureparameter.
ImitationLearningStrategies
Duringthereinforcementlearningprocess,demonstrationscanserveasanefficientwaytoexplore
the environment. Indeed, they can lead the agent to receive rewards promptly and so can lead it
to exploit confident winning trajectories quickly. A handcrafted agent is used to simulate near-
optimaldemonstrationsandfeed-backs. Itoffersgoodperformancecomparedwiththeotherdeep
learning methods. Also, it has been designed, evaluated and fine-tuned by humans. Thus it is a
waytomimichumanexpertiseandcanbeservedasanear-optimalexpertinourexperimentation.
443.2DialogueManager
We propose two demonstration sampling strategies corresponding to two different ways of
usingtheknowledgeofanexpert.
A) Learning with demonstrations: Let us assume that we learn with an offline expert i.e.
demonstrationsaregivenbeforelearning. Weproposethatinβ (inpercent)ofdialogues,theagent
plays for itself. Otherwise in 1 − β of dialogues, the expert gives to the agent one of its expert
trajectoriesasdemonstrationandtheagentreplaysthedialogueasifitwastheonewhoplayedit.
Therefore, the agent learns as if they are two datasets. The first one contains its trajectories and
thesecondtheexpertdemonstrations.
B) Learning with feed-backs: Let us assume that we learn with an online expert i.e. demon-
strationsaregivenduringlearning. Weproposethatinβ (inpercent)ofdialogueactions,theagent
plays for itself. Otherwise in 1 − β of dialogue actions, the expert gives to the agent its expert
actionasfeed-backandtheagentplaysthedialogueactionasifitwasitschoice.
This technique let the agent explore the environment by playing relevant actions in a given
state. In other words, it learns about its trajectories in which the expert can redirect it at any
momenttomorerelevantaction,asDAggerdoes(Rossetal.,2011).
Experiments
In our experiments the Pydial framework (Ultes et al., 2017c) is used, which implements an
agenda-based user simulation (Schatzmann et al., 2007). As in (Casanueva et al., 2017c) we
tested our algorithms for policy learning on different domains and in different environments by
increasing the inputs’ noise. The domains in PyDial differ from each other by the ontology size,
impactingthestateandactionspacedimensions.
Weevaluatethelearnedpoliciesaccordingtothreelevelsofnoisewithrespecttothesemantic
error rate (SER). This corresponds to the noise that comes from the ASR and the NLU channels.
InPydial,thisismodelledatthesemanticlevelwherebythetrueuseractioniscorruptedbynoise
togenerateanN-best-listwithassociatedconfidencescores.
Table 3.14 shows the compared policy models. HDC corresponds to the handcrafted policy
learning, which is a rule-based approach written by experts. DQN and ACER are the baselines
enhancedwithstochastic(stoc)explorationandeitherbehaviourcloning(BC)orfeedbacks(FB).
Methodname Abbrev.
HandcraftedPolicy HDC
StochasticQ-learning Stoc-DQN
StochasticQ-learningwithDemonstrations Stoc-DQN-BC
StochasticQ-learningwithFeed-backs Stoc-DQN-FB
StochasticActor-Critic Stoc-ACER
StochasticActor-CriticwithDemonstrations Stoc-ACER-BC
StochasticActor-CriticwithFeed-backs Stoc-ACER-FB
Table3.14: Overviewofproposedmethods
45We performed a long training stage over 10000 dialogues. This will evaluate the contribution
of stochastic sampling strategy during training and testing stages. Here we search to answer the
question: can we improve dialogue policy learning with stochastic off-policy learning methods
in order to compete the handcrafted agent? All methods are evaluated after training over 1000
dialogues during which the learned policy is fixed. For the second experiments, we decide to
compute the average performance over the last five checkpoints from dialogue indices 6000 to
10000withastepof1000dialogues. Thiscalculationisdoneinordertoreducevarianceinduced
byRLwhenweestimatetheperformanceofthemodels.
Results
The results are presented in Table 3.15. In most environments, methods learn very well com-
paredtothebenchmarks(Casanuevaetal.,2017c). Furthermore,someofthemcancompetewith
the handcrafted agent whether it is a stochastic Q-learning approach or a stochastic actor-critic
approach. For instance, Stoc-DQN-BC outperforms handcrafted expert with 30% SER for lap-
tops and SFR. Stochastic ACER was more robust to the different environments, showing better
performance,particularlyforStoc-ACER-BCwith30%SERforlaptops.
Stoc-DQN Stoc-DQN-BC Stoc-DQN-FB HDC
Task Suc. Rew. Suc. Rew. Suc. Rew. Suc. Rew.
CR 97.48% 12.67 98.34% 13.30 98.88% 13.46 100.0% 14.00
0%SER SFR 94.18% 10.99 87.40% 9.58 95.62% 10.94 98.2% 12.40
LAP 95.08% 11.10 98.10% 11.76 98.40% 11.77 97.0% 11.70
CR 92.22% 10.29 94.16% 11.26 95.76% 11.64 96.7% 11.00
15%SER SFR 90.64% 8.56 88.70% 8.06 89.22% 8.21 90.9% 9.00
LAP 90.34% 8.59 92.56% 9.40 91.86% 9.19 89.6% 8.70
CR 84.32% 7.73 85.36% 8.64 85.46% 8.65 89.6% 9.30
30%SER SFR 82.48% 5.11 80.26% 5.05 80.34% 4.63 79.0% 6.00
LAP 82.24% 5.89 84.62% 6.25 83.40% 6.00 76.1% 5.30
Stoc-ACER Stoc-ACER-BC Stoc-ACER-FB HDC
Task Suc. Rew. Suc. Rew. Suc. Rew. Suc. Rew.
CR 99.60% 14.02 99.64% 14.03 99.30% 13.87 100.0% 14.00
0%SER SFR 97.66% 12.36 96.48% 11.98 96.34% 11.98 98.2% 12.40
LAP 95.24% 11.23 95.34% 11.28 95.40% 11.24 97.0% 11.70
CR 97.56% 12.69 95.80% 12.32 96.78% 12.59 96.7% 11.00
15%SER SFR 88.62% 9.26 87.64% 8.91 86.98% 8.67 90.9% 9.00
LAP 88.74% 8.72 88.00% 8.55 86.26% 8.24 89.6% 8.70
CR 89.82% 10.19 89.38% 9.98 89.32% 10.04 89.6% 9.30
30%SER SFR 72.74% 4.38 78.22% 5.45 71.02% 4.37 79.0% 6.00
LAP 75.80% 4.86 78.66% 5.40 77.82% 5.23 76.1% 5.30
Table 3.15: Results of Experiment 2. Long term learning, average from 6000 to 10000 training
dialogues,for1000testingdialogue. Eachboldresultrepresentbettermodelsthanthehandcrafted
agent.
Theseresultsareencouragingandsuggestthatstochasticsamplingmakesitpossibletolearna
policy that performs as well as the handcrafted agent, which has been designed and fine-tuned by
463.2DialogueManager
humans,eveninhardenvironments. Also,theseresultsshowthatdemonstrationscansignificantly
contributetoimprovetheperformanceatearlylearningstages.
3.2.4 Hierarchical Imitation Learning
This section also presents the work of the PhD candidate Thibault Cordier, who I co-supervised
together with Dr.Tanguy Urvoy and Professor Fabrice Lefevre. This work has been published
inCordieretal.(2022).
We explore Graph Neural Network (GNN) for learning the policy in multi-domain and multi-
taskenvironments,inwhichseveraldomainsandtaskscanbeevokedinthesameconversation.
Inpractice,realapplicationslikepersonalassistantsorchatbotsmustdealwithmultipletasks:
the user may first want to find a hotel (first task), then book it (second task). Moreover, the tasks
may cover several domains: the user may want to find a hotel (first task, first domain), book it
(secondtask,firstdomain),andthenfindarestaurantnearby(firsttask,seconddomain).
One way of handling this complexity is to rely on a domain hierarchy which decomposes the
decision-making process;another way is to switch easily from one domain to another by scaling
upthepolicy.
AlthoughstructureddialoguepoliciescanadaptquicklyfromadomaintoanotherChenetal.
(2020), covering multiple domains remains a hard task because it increases the dimensions of
the state and action spaces while the reward signal remains sparse. A common technique to
circumvent this reward scarcity is to guide the learning by injecting some knowledge through a
teacherpolicy15.
We study how structured policies like graph neural networks (GNN) combined with some
degree of imitation learning (Imitation Learning (IL)) can be effective to handle multi-domain
scenarios.
We provide large scale experiments in a dedicated framework (Zhu et al., 2020) in which we
analysetheperformanceofdifferenttypesofpolicies,frommulti-domainpolicytogenericpolicy,
withdifferentlevelsofimitationlearning.
DialogueState/ActionRepresentations
One way of standardising the slot representation into a common feature space is to use Domain
Independent Parametrisation (DIP) (Wang et al., 2015) parametrisation. We adopt DIP as state
and action representations, which are not reduced to a flat vector but to a set of sub-vectors: one
corresponding to the domain parametrisation (or domain representation), the others to the slots
parametrisation(orslotrepresentations). Foranyactivedomain,theinputtothedomainrepresen-
tation is the concatenation of the previous domain user and system actions (see examples of the
output below, and a formal definition in Section 3.2.4), the number of entities fulfilling the user’s
constraints in the database, the booleans indicating if the dialogue is terminated and whether an
offer has been found / booked. The output corresponds to action scores such as REQMORE, OF-
FER, BOOK, GREAT, etc. Regarding the slot representation, its input is composed of the previous
slot-dependent user and system actions (see output below), the booleans indicating if a value is
known and whether the slot is needed for the find / book tasks. Its output are actions scores such
15 Fordeploymenttheteacherisexpectedtobeahumanexpert, however, forexperimentationpurposesweused
thehandcraftedpolicyasaproxy(Casanuevaetal.,2017c).
47as INFORM, REQUEST and SELECT. The parameterisation used depends on the representation of
the deterministic states of CONVLAB which does not consider the uncertainty in the predictions
madebythenaturallanguageunderstanding(NLU)module.
GraphNeuralNetwork
(a) FNN layerwith DIP. (b)GNNlayerwith DIP.
Figure 3.8: Structure of the layers with DIP. The central box represents the weight matrix of
a layer Wl. It can be decomposed into sub-matrices Wl . The white sub-matrices represent
i,j
any sub-weight and the coloured sub-matrices represent shared sub-weights. The circles repre-
sent input and output graph nodes. The domain representations are depicted in yellow; the slot
representationsingreenandred.
(a)FNN. (b)HFNN. (c)HGNN. (d)UHGNN.
Figure 3.9: Policy and input data structures. Different levels of structure are presented from
classical feed-forward neural network (FNN) to graph neural network (GNN). The prefix H-
correspondstoahierarchicalpolicyand UH- correspondstoauniquesub-policyforalldomains.
For a FNN layer, the input data is the concatenation of all DIP slot representations. For a GNN
layer,theinputkeepsitsstructure.
Prior knowledge can be integrated in our models by constraining the layer structure impos-
ing symmetries in the neural policies. Without prior knowledge, the standard structure used is
the feed-forward neural network layer (FNN) as represented in Figure 3.8a. This unconstrained
structuredoesnotassumeanysymmetryinthenetwork.
483.2DialogueManager
Assumingthatsub-policiesassociatedwiththeslotsarethesame,abetteralternativeistouse
the graph neural network layer (GNN) presented in Figure 3.8b. This structure assumes that the
state and action representations have a graph structure that are identically parameterised by DIP.
The GNN structure is a fully connected and directed graph, in which each node represents a sub-
policy associated with a slot and a directed edge between two sub-policies represents a message
passing. We identify two roles for sub-policies: the general node as I-NODE associated to the do-
main representation and the slot nodes denoted as S-NODE associated to the slot representations.
Both representations were introduced in Section 3.2.4. We also identify the relations: I2S for
I-NODE to S-NODE, S2I and S2S respectively.
We formally define the GNN structure as follows. Let n be the number of slots and L the
numberoflayers. Letbexthedialoguestate,x = ϕ (x),hl ∀l ∈ [0,L−1]andy berespectively
0 0 0 0
the input, hidden and output I-NODE representations. Let the input, hidden and output S-NODES
representations be respectively ∀i ∈ [1,n], x = ϕ (x), hl ∀l ∈ [0,L−1] and y . First, the GNN
i i i i
transformsinputs:
∀i ∈ [0,n], h0 = F0(ϕ (x))
i i i (3.14)
with F0(h) = σ0(W0h+b0)
i i i
Then, at the l-th layer, it computes the hidden nodes representations (Eq. 3.15a) by follow-
ing message sending16 (Eq. 3.15b), message aggregation (Eq. 3.15c) and representation update
(Eq.3.15d):
∀i ∈ [0,n], hl = Fl(hl−1) (3.15a)
i i
ml = Ml (hl−1) (3.15b)
i←j i←j j
ml = Al(ml ) (3.15c)
i i i←∗
hl = Ul(ml) = σl(ml) (3.15d)
i i i i
ThemessagesendingfunctionMl isalineartransformationwithbias. Themessageaggregation
i←j
function Al is the average pooling function. The representation update function Ul compute the
i i
new hidden representation with RELU activation function and dropout technique during learning
stage. Finally,theGNNconcatenates(⊕symbol)allfinalnodesrepresentationsandcomputesthe
policyfunctionwiththeSoftmaxactivationfunction.
n
(cid:77)
y = σL( WLhL−1 +bL) (3.16)
i i i
i=0
ImitationLearning
Inadditiontothestructuredarchitecture,weusesomelevelofILtoguidetheagent’sexploration.
In our experiments, we used CONVLAB’s handcrafted policy as a teacher 15, but other policies
could be used as well. Behaviour cloning (BC) is a pure supervised learning method that tries to
mimic the teacher policy. Its loss function is the cross-entropy loss as in a classification problem.
ImitationLearningFromOracleDemonstrations(ILFOD)isaRLmethodwhichallowstheagent
16Thenotationi ← j denotesamessagesendingfromslotj tosloti. Italsocorrespondstothedirectedrelation
betweentheslotsj andi. Thenotationi←∗denotesallmessagessendingtosloti.
49toplayoracleactionsasdemonstrationsandtoinjecttheminitsreplaybuffer. Thesamepresented
in Section 3.2.3. In our experiments, we kept half of the agent’s own actions in the buffer along
with those generated by the oracle. Imitation Learning From Oracle Supervision (ILFOS) is the
combination of supervised and reinforcement learning when the agent learns with a supervised
loss,namelythemarginlossHesteretal.(2018).
ExperimentsonGNNandImitationLearning
We performed an ablation study: (i) by progressively extending the baseline to our proposed
GNNs and (ii) by guiding the exploration with IL. All the experiments were restarted 10 times
withrandominitialisationsandtheresultsevaluatedon500dialogueswereaveraged. Eachlearn-
ing trajectory was kept up to 10,000 dialogues with a step of 1,000 dialogues in order to analyse
thevariabilityandstabilityofthemethods.
Models ThebaselineisACERwhichisasophisticatedactor-criticmethod(Wangetal.,2016a).
Afteranablationstudy,weprogressivelyaddedsomenotionofhierarchytoFNNstoapproximate
the structure of GNNs. FNN is a feed-forward neural network with DIP parametrisation. Thus,
the agent actions are single-actions. FNN-REF is a FNN with the native parametrisation (no
DIP)withmultiple-actionsofCONVLAB17. HFNNisahierarchicalpolicywithdomain-selection
module and based on FNNs for each domain. HGNN is a hierarchical policy with domain-
selectionmoduleandbasedonGNNs. UHGNNisaHGNNwithauniqueGNNforalldomains.
Metrics: We evaluate the performance of the policies for all tasks. For the find task, we use
the precision, the recall and the F-score metrics: the inform rates. For the book task, we use
the accuracy metric namely the book rate. The dialogue is marked as successful if and only if
both inform’s recall and book rate are 1. The dialogue is considered completed if it is successful
from the user’s point of view (i.e a dialogue can be completed without being successful if the
informationprovidedisnottheoneobjectivelyexpectedbythesimulator).
Evaluation of the Dialogue Manager We performed an ablation study based on ACER as re-
ported in Figure 3.10. First, all RL variants of ACER (Figure 3.10a) have difficulties to learn
without supervision in contrast to BC variants (Figure 3.10b). In particular, we see that hierar-
chical decision making networks (HFNN in green), graph neural network (HGNN in red) and
generic policy (UHGNN in purple) drastically improve the performance compared to FNNs.
Similarly, using IL like ILFOD (Figure 3.10c) and ILFOS (Figure 3.10d) notably improves the
performance. Therefore, learning generic GNNs allows collaborative gradient update and effi-
cientlearningonmulti-domaindialogues.
Conversely, we observe that hierarchical decision making with HFNNs does not systemati-
cally guarantee any improvement. These results suggest that GNNS are useful for learning dia-
logue policies on multi-domain which can be transferred during learning across domains on-the-
fly to improve performance. Finally, regarding ILFOD variants (Figure 3.10c), we can observe
that all architectures are affected by a large variability. This shows that multi-domain dialogue
management is difficult despite the use of demonstrations and that learning with reward is not
sufficienttorobustlysucceed.
17Thenativeparametrisationmanuallygroupsmulti-actionsbasedonMULTIWOZBudzianowskietal.(2018).
503.2DialogueManager
(a)PureACER (b)PureBC
(c)ACERwithILfOD. (d)ACERwithILfOS.
Figure 3.10: Distribution via boxplot of the performance of the proposed approaches on CON-
VLAB,with10differentinitializationsandwithoutpre-training. Thecolouredarearepresentsthe
interquartile Q1-Q3 of the distribution, the middle line represents its median (Q2) and the points
areoutliers.
Evaluation of the Dialogue System We evaluate the policy learning algorithms in the entire
dialogue pipeline, in particular our best DM policy ACER-ILFOS-UHGNN under a shorter
name ACGOS.
The results of our experimentation are presented in the paper (Cordier et al., 2022). We ob-
serve that the performance of our approach is closed to the handcrafted policy (the teacher) when
directly passing the dialogue acts, when using BERT NLU Devlin et al. (2018) and template-
basedNLG.Moreover,theperformanceofourapproachisbetterthanbaselineswithasignificant
difference. Theseresultshighlightthebenefitofstructuredpoliciesagainststandardpolicies.
A limitation of current policies in CONVLAB, including ours, is that the robustness to noisy
inputs is not specifically addressed as it had been done in PyDial Ultes et al. (2017b). It could be
also interesting to study the impact of incorporating real human feed-backs and demonstrations
insteadofahandcraftedteacher.
The GNN structured policies combined with imitation learning avoid sparsity, while being
data efficient, stable and adaptable. They are relevant for covering multi-domain task dialogue
problems.
A continuation of this work for few-shot learning will soon be published in the findings
EACL2023,formoredetailspleasereferto(Cordieretal.,2023).
5152Chapter 4
Contributions to Conversational QA and
Other Contributions
‘Estaba perdiendo la vista y el o´ıdo, parec´ıa confundir a los interlocutores
con personas que conocio´ en e´pocas remotas de la humanidad, y contestaba
a las preguntas con un intrincado batiburrillo de idiomas.’ — Gabriel Garc´ıa
Marquez,Cienan˜osdesoledad.
‘He was losing his sight and his hearing, he seemed to confuse the people
he was speaking to with others he had known in remote epochs of mankind,
andhewouldanswerquestionswithacomplexhodgepodgeoflanguages.’ —
GabrielGarc´ıaMarquez,OneHundredYearsofSolitude.
Conversational QA is a relatively recent area of research that groups reading comprehension,
QA and dialogue. Typically, it consists in a sequence of questions and answers related to a para-
graph or to a knowledge graph. My contribution in this field was to enrich existing datasets
with (i) information about the ellipsis and coreferences (Section 4.1) and (ii) question rewrit-
ing (Section 4.2). I also contribute to the creation of a new dataset Knowledge-base Conversa-
tions(KGConv)(Section4.3),thatwewillsoonmadepublic. Thisworkwaspartoftheindustrial
research project DIANA, of which I was the head. The work described in here was made mainly
by the young researcher Quentin Brabant under my supervision. For the work presented in (Sec-
tion 4.2), I proposed the idea of enriching the corpus CoQA with question rewriting in 2020. I
wrote with Timothy Garwood a document describing the annotations and we manually annotated
10 dialogues. Then, I was in charge of the administrative process to formalise the collaboration
with ELRA to produce the annotations, this took about a year. I also checked the annotations
andconductedexperimentsonconversationalquestionansweringbyusingRoBERTa. Regarding
this work, Gwenole Lecorve worked on question generation, while Quentin Brabant on question
rewriting. The work presented in Sections 4.1 and 4.3 were joint work with Claire Gardent as
part of the European Innovative Training Networks (ITN) project Interactive Natural Language
Technology for Explainable Artificial Intelligence (NL4XAI), that involves industrial partners as
OrangeandacademicinstitutionsastheCNRS.IwasrepresentingOrangeinthisproject.
534.1 Detection of Ellipsis and Co-reference in conversational
corpora
This work is joint work with Quentin Brabant and Claire Gardent. It was published in (Brabant
et al., 2021). We made several contributions to the task of ellipsis and coreference detection in
conversationalcorpora. Wecreatedlabelleddatabyenrichingthreeexistingdatasetswithannota-
tionsindicatingwhetheraturncontainsanellipsisand/oracoreference. Astheseannotationswere
incomplete, we drew on inferential relations between incompleteness, pronominalisation, ellipsis
and coreference to both extend and complement these annotations. We then use these annotated
data to train a classifier based on DistilBERT (Sanh et al., 2020), which assigns to each question
in a conversation two labels indicating whether it contains an ellipsis and/or a coreference. We
also explore how active learning, multilabel approaches and fine-tuning can be used to train this
model.
A coreference occurs when an entity is referred via two or more expressions in the same
conversation. However, we are only interested in detecting a particular kind of coreference. We
say that a coreference happens in a turn if and only if (1) it contains an expression referring to
an entity already mentioned in a previous turn and (2) this entity cannot be identified outside of
the conversational context. The resolution of a coreference consists in replacing the referring
expressionbyanunambiguousreferencetotheentity.
In linguistics, an ellipsis is the omission of one or several words from a clause that preserves
themeaninginthecontext. Whenaturnisnotunderstandablewithoutitscontext(i.e. withoutthe
conversation history), we call it incomplete. In this paper, we assume that any conversation turn
contains an ellipsis if and only if it is still incomplete after coreferences have been resolved. It
follows from this definition that an incomplete sentence contains either a coreference, an ellipsis,
orboth.
A conversation is a sequence of alternating questions and answers that starts with a question
and ends with an answer: (q ,a ,q ,a ...,q ,a ). In many available conversational question
1 1 2 2 n n
answering datasets questions are sentences produced by humans (e.g. (Choi et al., 2018a; Christ-
mann et al., 2019; Elgohary et al., 2019; Quan et al., 2019; Reddy et al., 2019)), while answers
are often given by an automated system, and often not in the form of a sentence. For this reason,
wefocusonellipsisandcoreferencedetectioninquestions. Moreover,wewillsometimesusethe
term question to refer to turns that are not question per-say, but that are produced by a user and
notbyanautomatedsystem(seeSection??,GECORdataset).
Weproposeamodeltopredictwhetheranygivenquestionq ofaconversation(q ,a ,...,q ,a )
i 1 1 n n
contains an ellipsis and/or a coreference; since any turn can normally be understood based on the
context of previous turns, our task can be seen as the classification of q with the given context
i
c = (q ,a ,...,q ,a ). We thus formulate our task as a 2-labels classification: for a given
1 1 i−1 i−1
input question q and an input context c, output two values (coref,ellipsis) ∈ {0,1}2 where 1
i
denotes the presence of the phenomenon and 0 denotes its absence. We call instance of our task
the couple formed by a question, and its context. An instance is annotated when it is associated
withanannotationoftheform(coref,ellipsis).
544.1DetectionofEllipsisandCo-referenceinconversationalcorpora
4.1.1 Active learning (AL)
We use the following values for annotating the datasets: 1 for the presence of a phenomenon
(positive class), 0 for its absence (negative class). Cases where no label is assigned are denoted
by the value -1. Note that -1 does not denote a class, but only the absence of information about
the actual class. We describe how we process each dataset in order to obtain train instances for
ourtask.
ConvQuestions Christmann et al. (2019). Many conversations of ConvQuestions are centered
on the same entity; those conversations tend to be similar to each other, as they often have ques-
tions in common. In order to maximise the benefits of manual annotations, we created subsets of
theoriginaldatacontainingexactlyoneconversationpertopicentity. Thisresultedintrain/dev/test
sets containing respectively 905/330/335 questions in total. Based on these new sets, we created
aninstanceofourtaskforeachquestion(exceptthefirstone)ofeachconversation. Someofthese
conversationswheremanuallyannotatedwith(coref,ellipsis)values. Weobtainedtrain/dev/test
of247/329/331annotatedinstances.
GECORQuanetal.(2019). Wecreateinstancesasfollows. Foreachconversation(q ,a ,...,q ,a )
1 1 n n
intheGECORdataset,eachi ∈ {2,...,n},andeachvariantq′ ∈ {q (e),q (r),q (c)}oftheques-
i i i i
tion q : if q′ is not empty, then we create the instance ((q ,a ,...,a ),q′) and annotate it with
i i 1 1 i−1 i
(coref,ellipsis)values. Thosevaluescansometimesbededucedbyusingthefollowingrules:
• q (e)containsanellipsis;
i
• q (r)containsacoreference;
i
• q (c)containsnoellipsisnorcoreference;
i
• ifq (e) = q (r)weinferthatbothq (e)andq (r)containanellipsisandacoreference;
i i i i
• ifq (e)isempty,weinferthatq containsnoellipsisandthusq (r)neither;
i i i
• ifq (r)isempty,weinferthatq containsnocoreferenceandthusq (e)neither.
i i i
These rules are not sufficient to deduce ellipsis and coreference label values in all cases. By
default,thevalue-1isassigned.
CANARDElgoharyetal.(2019). InstanceswereextractedsimilarlyasfromtheGECORdataset.
Thetwomaindifferencesare: foreachcreatedconversation,twovariants(originalandcomplete)
of the last question are used. When the complete variant is used, we assign 0 to both coref and
ellipsis;otherwise,weassign−1. AnexampleisgiveninTable4.1.
At this point many labels are missing in the instances of the task. In particular, instances
from CANARD do not contain any positive label. We addressed this issue via two approaches:
multilabellearningandlabelfilling.
Multilabel classification can be seen as a particular case of multitask learning, since a single
model is trained on several binary classification tasks. One justification for using this approach
(instead of one model per classification) is that the parameters are shared during training which
hasbeenshownintheliteraturetobeneficialtoalltheclassifiers.
The 4-labels classification task considers the following labels: coreference, ellipsis, incom-
pleteness,andpronoundetection. Formally,itmeansthatannotationsoftheform(coref,ellipsis)
55PieceofconversationfromCANARD:
q WhatisOntheSundayofLife?
1
q (c) WhatisOntheSundayofLife?
1
a In1992,DeleriumreleasedOntheSundayofLifeasaneditionof1,000copies,complete
1
withadeluxegatefoldsleeve.
q Diditdowell?
2
q (c) DidPorcupineTree,OntheSundayofLifedowell?
2
a OntheSundayofLife... hadaccumulatedsalesofmorethan20,000copies.
2
q Wasitrereleaesd?
3
q (c) WasPorcupineTree,OntheSundayofLiferereleaesd?
3
Correspondinginstancesofthetask:
Context Question Coref Ellipsis Coref Ellipsis Incomp. Pronoun
(q ,a ) q -1 -1 1 -1 1 1
1 1 2
(q ,a ) q (c) 0 0 0 0 0 0
1 1 2
(q ,a ,q ,a ) q -1 -1 1 -1 1 1
1 1 2 2 3
(q ,a ,q ,a ) q (c) 0 0 0 0 0 0
1 1 2 2 3
Table 4.1: Example of conversation from CANARD and the corresponding instances of the task.
Columnswithgrayheadersshowtheresultoflabelfilling.
564.1DetectionofEllipsisandCo-referenceinconversationalcorpora
are replaced by annotations of the form (coref,ellipsis,inc,pronoun). We used automatic pro-
noun detection to provide a 0 or 1 value to pronoun in all questions. By default, the value of inc
issetto-1,exceptforinstancesfromCANARDwherethevalueisknown.
We then replace some of the −1 values by taking advantage of the logical dependencies be-
tweenlabels: apronounalwaysindicatesacoreference;incompletenessiseitherduetoacorefer-
ence or an ellipsis; coreferences and ellipses always cause incompleteness. We therefore applied
thefollowingrulestoeachinstance,inorder:
1. ifpronoun = 1thencoref ← 1,
2. ifcoref = 1orellipsis = 1theninc ← 1,
3. ifcoref = 0andellipsis = 0theninc ← 0,
4. ifinc = 0thencoref ← 0andellipsis ← 0.
Remark that in some cases these rules are not sufficient to get rid of all unknown values. Such
casescanbefoundintheexamplesofTable4.1.
Activelearning(AL)isahuman-in-the-loopmethodthataimsatmaximizingtheperformance
gainsrelativelytothenumberofmanualannotations. Itisespeciallyinterestingwhenfewlabeled
data are available and only a small fraction of unlabeled data can be manually annotated in rea-
sonable time. We apply several rounds of AL for labeling (separately) ellipses and coreferences.
Eachroundconsistsinthefollowingsteps:
1. Train and evaluate a model. We use CANARD/GECOR as a training set. All CANARD
instances that have already been manually annotated during previous rounds are included.
TheevaluationisdoneonConvQuestionstestset.
2. Run the model on unlabeled data. The model trained in step 1 associates a prediction
(coref∗,ellipsis∗)toeachinstance.
3. Select a subset of unlabeled data. We select the 50 CANARD conversations on which the
modeldisplaystheleastcertainty. Sinceoneconversationisthesourceofseveralinstances,
we define the certainty of a conversation as the average certainty of the corresponding in-
stances. The certainty of the model (for a given label, on a given instance) is defined as the
distancefrom0.5oftheoutputcorrespondingtothepredictedlabelvalue,i.e.: |coref∗−0.5|
forcoreferenceand|ellipsis∗ −0.5|forellipsis.
4. Manually label the selected subset. We label the selected conversations (either for ellipsis
orcoreference). Labeledconversationsareusedduringtraininginthenextloop.
Westoprepeatingthesestepswhentheevaluationscorestopsincreasing.
4.1.2 Experiments
Weevaluatethefollowingmodelvariants.
• Baseline. ThebaselineisaDistilBert(Sanhetal.,2019)modeltrainedonthe4-labelclassi-
ficationtaskonCANARD/GECOR.
57• Fine tuning only. The model is fine-tuned on the 4-label classification task on the training
setofConvQuestions.
• Baseline+AL.Themodelisfine-tunedonthe4-labelclassificationtaskonCANARD/GECOR,
but labelled instances of CANARD are added via AL. Each round of AL adds 50 instances
that are labelled for either coreference or ellipsis. We evaluate several versions of this vari-
ant: three versions use instances that were annotated for coreference via, respectively, 1, 2,
and3roundsofAL.Threeothersversionsuseinstancesthatwereannotatedforellipsisvia
1,2,and3rounds.
• Baseline + all AL. Identical to baseline + AL, but using all annotations produced for coref-
erenceandellipsis(3roundsforeach).
• Baseline + all AL + fine tuning. Identical to Baseline + all AL., but training on CA-
NARD/GECORisfollowedbyafine-tuningsteponthetrainingsetofConvQuestions.
• 2-label variants. We evaluate three of them. They are respectively identical to baseline, to
baseline+allAL,andtobaseline+allAL+finetuning,withthedifferencethatthemodel
istrainedonthe2-labelsclassificationtask.
We use GECOR and CANARD for training our models, while ConvQuestions is used for
evaluation and fine tuning. In this way we can better assess how well the classifier behaves on
unseen data, data that is different from the data the model was trained on. During training, labels
with -1 value are simply ignored (no error is retro-propagated). During evaluation, we measure
therecall,precision,andF-measureonellipsisandcoreferencedetection.
4.1.3 Results
TheresultsaredisplayedinTable4.2. Eachlinecorrespondstoavariantofthemodel.
Generally, the results show that coreference detection performs better than ellipsis detection.
Moreover, by looking at lines 2 to 9 in the table, we see that AL is clearly beneficial; the all
AL labels variant improves F1 scores for coreference and ellipsis detection by 10 and 13 points
compared to the baseline. The same conclusion is drawn when comparing lines 11 and 12. The
effectsoftrainingon4labelsversus2arelessclear: bycomparinglines2,9,10tolines11,12,13,
weseethat4-labelsvariantsperformroughlyaswellastheir2-labelscounterpartsoncoreference
detection. For ellipsis detection, they score significantly higher on F1 score when no fine tuning
isapplied,butthescoresaretoolowtoproposeameaningfulinterpretation. Finetuningincreases
scores for both ellipsis and coreference detection; however the increase is way larger in the case
of ellipsis. In fact, coreference detection arguably performs reasonably well without fine-tuning,
contrary to ellipsis detection. A possible explanation is that the kinds of ellipses occurring in
one dataset can be different from those occurring in another. In contrast, coreferences cover a
narrowersetofphenomena.
In addition to measuring performances, we looked at the output of the model on the test set:
we noticed that coreferences due to pronouns use are well recognized, while many false nega-
tives correspond to cases where an entity is referred to via its type or function, as in: “To which
continentdoesGermanybelong? Whatsizeisthecountry?”.
584.2QuestionRewriting
Coreference Ellipsis
P R F1 P R F1
1 finetuningonly 81 65 72 51 67 57
2 baseline 97 64 77 64 36 46
3 +ALforellipsis(1round) 92 63 75 71 48 56
4 +ALforellipsis(2rounds) 89 72 80 83 41 55
5 +ALforellipsis(3rounds) 85 79 82 74 48 57
6 +ALforcoref. (1round) 87 84 85 72 46 56
7 +ALforcoref. (2rounds) 92 81 86 71 39 50
8 +ALforcoref. (3rounds) 95 79 86 67 31 43
9 +allALlabels 94 81 87 84 46 59
10 +finetuning 94 93 94 83 71 77
11 baseline,2-labelsvariant 89 68 77 100 10 19
12 +allALlabels 91 86 89 88 35 50
13 +allALlabels+fine-tuning 94 93 93 84 70 76
Table4.2: Resultsoftheexperiments. Scoresaregivenaspercentages.
4.2 Question Rewriting
As mentioned before, CQA (Reddy et al., 2019; Choi et al., 2018a; Saha et al., 2018) is a task
in which a system interacts with a user. The interaction takes the form of a conversation, where
the user always asks questions that the system answers. In this work, we focus on the case where
the system searches for answers in a passage, although settings relying on structured data (e.g.
knowledgebases)alsoexist(Sahaetal.,2018),astheonepresentedinthefollowingsection(Sec-
tion 4.3). Compared to QA, the system faces an additional difficulty: each question is asked in a
conversational context that consists in previous turns. Therefore, implicit references to the con-
text may happen in the form of ellipses and coreferences, making the understanding of questions
more difficult for the system. One way to overcome this difficulty is Question Rewriting (QR),
whichconsistsinrewritingeachoriginal(in-context)questionintoanout-of-context questionthat
isunderstandablebyitself,i.e.,thatcanbeansweredwithoutknowingtheconversationalcontext.
We present the corpus Conversational Question Answering with Rewriting (CoQAR), which
is an annotated subset of the CQA corpus CoQA (Reddy et al., 2019). CoQAR was obtained by
asking specialised native speakers to annotate original questions with at least two and at most
three distinct out-of-context rewritings. This work was published in (Brabant et al., 2022). Our
contributionistwo-fold.
Firstly, we provide CoQAR, which contains high-quality questions rewritings. The corpus is
publicly available1; moreover, its annotations were conducted in accordance to ethical concerns:
everyannotatorinvolvedwasproperlyhired.
Secondly,weassessthequalityoftheannotationsofCoQARthroughseveralexperiments. We
trainQuestionRewriting(QR)models. Wethenratethesemodels’outputsviahumanevaluation.
We also evaluate these models as preprocessing steps of (conversational and non-conversational)
1TheCOQARdatasetispubliclyavailableathttps://github.com/Orange-OpenSource/COQAR
59Numberofrewritings
0 1 2 3 total
train 365 108 31,378 13,210 45,061
dev 9 0 37 7,937 7,983
Table4.3: Numberofquestionsdependingonthenumberofrewritings.
QA models. To this end, we compare the performance of a stat-of-the-art QA model with and
withoutQR.
Our results support the claim of (Vakulenko et al., 2021) that QR models can be successfully
usedincombinationwithexistingQAmodels. Indeed,wefoundthataddingQRasapreprocess-
ing step boosts the performances of QA models and allows reusing non-conversational state-of-
the-artQAsystemswhilereducingperformancedegradationonCQA.
4.2.1 Annotations
We decided to hire two specialised native-speakers’ annotators. Their task was to annotate orig-
inal (in-context) questions from CoQA with at least two and at most three distinct out-of-context
rewritings. To make sure that they understand what was expected, we ourselves annotated a con-
versationandprovideditasanexample. Anexampleofconversationannotatedbytheannotators
isprovidedinTable??.
While annotators were told to preserve the meaning of the original sentences, they were also
asked to paraphrase in their rewritings. As a results, these annotations contrast with those of
CANARD, where the structure of the original question is usually preserved in the rewriting. In
total,4.1k conversationsofCoQAtrainsetwereannotatedaswellasall500conversationsofthe
dev set. Since the test set of CoQA is not available, no conversation was annotated from it. The
trainanddevsetsofCoQARrespectivelycontain45kand8kquestions. Table4.3summarisesthe
numberofquestionsthathave0,1,2or3rewritings.
Overall,passagescontainfrom75to1079words,withanaverageof275. Conversationlength
distributionisdisplayedinFigure4.1.
On average, out-of-context rewritings are longer (8.8 words) than the original questions (5.5
words);Figure4.2showsthequestionlengthdistribution.
Most conversations were annotated by only one annotator, but 50 conversations were anno-
tated by both. We relied on these conversations to analyse the annotations. We extracted two
rewritings per question and per annotator and, using a pair of rewritings as references and the
otherashypothesis,wecomputedtheSacreBLEUscore(Post,2018)andtheBERT-score(Zhang
et al., 2020a). SacreBLEU gives us an insight on the similarity of the surface form of rewritings,
whileBERT-scoregivesusaninsightonthesemanticsimilarity. WeobtainedaSacreBLEUscore
of 32.67 and a BERT-score of 90.22: this suggests that the rewritings have diverse surface form
whilebeingcloseintermsofmeaning.
4.2.2 Evaluation on Question Rewriting (QR)
In QR, the model receives as input an in-context question, its conversational context, and the as-
sociatedpassage. Itstaskistogenerateanout-of-contextrewritingofthequestion. Weconducted
604.2QuestionRewriting
1,000
500
0 5 10 15 20 25
#questions
Figure4.1: Distributionofconversations’length.
0.2
0.1
0 5 10 15 20 25
#words
Figure 4.2: Distribution of length for original questions (white) and out-of-context rewritings
(darkgrey). Overlapofthedistributionislightgrey.
thefollowingexperiment: (1)trainingQRmodelsonCoQARandCANARD;(2)evaluatingthese
models, via standard metrics and human evaluation. Furthermore, we evaluate these QR models
ondownstreamconversationalquestionansweringaspresentedinthenextsection.
Datasets. For training and evaluation, we rely on CANARD and CoQAR. For CANARD, we
use the original train/dev/test splits. For CoQAR, we use the original dev set as test set, and split
the original train set into a train set and dev set, in such manner that CANARD and CoQAR dev
sets have the same size. For training, we also make use of a mixture of CANARD and CoQAR,
that we refer to as CoQAR+CANARD, whose train and dev sets are, respectively, the union of
both corpora’s train and dev sets. We train three variants of the QR model: one variant is trained
onCANARD,oneistrainedonCoQAR,andthethirdoneistrainedonamixtureofbothdatasets.
Meaningpreservation Linguisticcorrectness
Testset Model
MOS (Stddev.) MOS (Stddev.)
Humanrewriting 4.5 (0.86) 4.86 (0.45)
CoQAR
T5(CoQAR) 3.82 (1.42) 4.66 (0.82)
Humanrewriting 4.60 (0.96) 4.7 (0.89)
CANARD T5(CANARD) 3.92 (1.34) 4.43 (1.08)
T5(CoQAR+CANARD) 3.96 (1.47) 4.76 (0.77)
Table4.4: ResultsofthehumanevaluationofQR.
61
snoitasrevnoc#
oitarnoitseuqModel: WetrainaQRmodelbasedonT5onthreedatasets: CoQAR,CANARD,andCoQAR+CANARD.
Foreachdataset,wefine-tunethesmall1.1versionofT52. Themodelisevaluatedonthedevset
usingMETEOR.
Two Mean Opinion Score (MOS) evaluations were carried out on 8 human testers who were
askedtojudgethequalityofrewrittenquestions. Wesampled50originalquestionsfromCoQAR
and 50 original questions from CANARD. Each original question was then paired with several
rewritings:
• onerewritingfromthecorpus,towhichwereferasthereference;
• one or several rewritings generated by different T5 models: each source question from
CoQAR is paired with a rewriting generated by T5(CoQAR), while each source question
fromCANARDispairedwithonerewritinggeneratedbyT5(CANARD)andonerewriting
generatedbyT5(CoQAR+CANARD).
Thepairswerethenusedintwoevaluations.
In the first evaluation, rewritten questions were presented to human testers, together with the
original question and its context (preceding turns and the corresponding text passage). Testers
assessed the semantic similarity of the rewritten and original questions. In the second evaluation,
rewritten questions were presented alone to the testers for them to assess linguistic correctness.
Both semantic similarity and linguistic correctness were evaluated on the 5-points scale. In the
end, each rewritten question received one rating for semantic similarity and one for linguistic
correctness. TheresultsarereportedinTable4.4.
We see that QR models obtain scores that are clearly below human performance in terms
of meaning preservation. We also observe that the T5 model that was trained on CoQAR and
CANARD obtains higher linguistic correctness scores than the model that was only trained on
CANARD,andthisresultdoesnotseemduetochance(aMann-WhitneyUtestgivesap-valueof
0.026). Itisplausiblethat,althoughaddingdatafromCoQARtothetrainingsetdoesnotimprove
meaningpreservation,itimproveslinguisticcorrectnessbecauseofitsgreaterdiversityintermof
rewritings’surfaceforms.
4.2.3 Evaluation on Conversational Question Answering
Typically,theinputstoaCQAneuralmodelare: aquestion,itsconversationalcontext(i.e. these-
quenceofpreviousquestionsandanswers),andtheassociatedpassage. Theoutputistheanswer,
eitherintheformofaspanfromthepassageorintheformofvalidtokenssuchas“yes”,“no”or
“unknown”.
AchallengeforconversationalquestionansweringwasalsoreleasedwithCoQA3.Themodels
areevaluatedwiththeF1score(Reddyetal.,2019). Transformershavebeensuccessfullyusedin
this task: to the time this paper was written, the best model (a RoBERTa-based model (Ju et al.,
2019))got90.7ofoverallF1measure,overcominghumanperformance88.8.
Our goal is to indirectly assess the quality of QR by comparing the performance of a model
taking original questions and their context as inputs with a model using out-of-context rewritings
instead. In other words, we would like to know whether replacing the original question with its
2https://huggingface.co/google/t5-v1_1-small
3https://stanfordnlp.github.io/coqa/
624.2QuestionRewriting
QRmechanism F1 EM
None(question+context) 68.13 49.63
Humanrewriting 63.26 45.10
T5(CoQAR+CANARD) 63.30 44.97
Table4.5: ResultsoftheCQAevaluation.
conversational context by the out-of-context rewriting has a positive impact on answer extrac-
tion. First, we evaluate the impact of rewritten questions in the performance of a RoBERTa base-
line(Liuetal.,2019). Second,inordertoassessthereusabilityofQRmodelstrainedonCoQAR,
wefurtherevaluateastate-of-the-artnon-conversationalQAmodeltrainedonSQuAD(Rajpurkar
etal.,2018)bytestingitwiththerewrittenquestions.
WewouldliketoassesstheimpactofQRonstate-of-theartmodelsforCQAbyansweringthe
followingquestion: wouldthemodelsbeabletoextractthecorrectanswerfromthepassagewith-
out dealing with the conversational context? To this aim we propose three experiments in which
we train and evaluate a transformer on several variations of QR: no rewriting, human rewriting,
andmodelrewriting.
DatasetsandVariants. WeuseCoQAR,withdistinctrewriting.
i Norewriting: theorginaldataset,takingintoaccounttheconversationalcontext.
ii Human rewriting: the dataset containing only the question rewritten by human annotators,
ignoringcompletelytheconversationalcontext.
iii QR model: instead of using human annotations we use questions that were generated auto-
maticallybytheT5(CoQAR+CANARD)modelpresentedinSection4.2.2.
Model. For the CQA experiments, we train and evaluate a RoBERTa4 transformer on CoQAR
withthedistinctrewritingmechanismsdescribedabove.
Evaluation. Results are presented in Table 4.5. Surprisingly, resolving the context with human
question rewriting does not seem to help RoBERTa to better identify the answer in terms of F1
and exact match (EM) as defined in Rajpurkar et al. (2016). We obtained an F1 and EM gain
of 4.87 and 4.53 respectively of the original in-context questions over the out-of-context human
rewritings.
Unlike (Vakulenkoetal.,2021),whereresultsofthesametaskarereportedonCANARD,the
setting relying on original questions (referred to as CANARD O) and the one relying on human-
writtenquestions(CANARD H)respectivelyobtain53.65and57.12F1scores,whichcorrespond
to a gain of 3.47 points for human rewriting. We suspect that the self-attention mechanism of
RoBERTa solves the coreferences and ellipsis present in short in-context questions limited by
the separation token from the context and the passage. While processing a long self-contained
rewriting might be more difficult. These results confirm the good performance of RoBERTa on
theoriginaltaskofCQA(Juetal.,2019).
4https://huggingface.co/
63Interestingly, automatically rewritten questions trained on both CoQAR and CANARD ob-
tainedsimilarperformancethanhumanrewritings,althoughhumanrewriting,gotaslightlybetter
EM.TheseresultsarecomparablewiththeonesreportedonCANARDin(Vakulenkoetal.,2021).
We also conducted experiments on the re-usability of QA systems by solving the context
through question rewriting. The results are promising, rewriting out-of-context question will let
usreuseexistingQA systems. Weinvitethereader tolookfordetailsinthepaper (Brabantetal.,
2022).
4.3 A conversational QA corpus grounded in Wikidata
AfterthegreatsuccessofChatGPTthatspreadoutnonfactualgenerativeneuralmodelstothebig
audience, guiding semantically these models to enable explainability and to reduce their typical
errors: hallucinations, distortions, omissions and repetitions(Faille et al., 2021; Narayan et al.,
2022; Nie et al., 2019) is an urgent need. We propose KGConv5, a corpus of Conversational
QuestionAnswering(CQA)groundedonWikidata6toconstraintthegenerationwithaKnowledge
Graph(KG).
KGConv is composed of conversations between two participants, one that always asks ques-
tionsandanotheronethatalwaysanswersbasedonfacts. Thus,itcontainssequencesofquestion-
answer pairs. The grounded sequences are composed of Wikidata triples of the form: (s,p,o), in
which s is the subject, p is the property and o corresponds to the object of a fact belonging to a
KG.
IntotalKGConvgathers71Kconversations(604Kquestion-answerpairsintotal),whereeach
pair relates to an underlying fact from the public KG Wikidata7. Each conversation is focused
on a given root entity. As illustrated by Table 4.6, the first question is directly about this root
entity, while the next ones explore new facts about any entity discovered during the conversation
(includingtherootentityitself). Thiscorpuscanbeusedfordistincttaskssuchasfactualquestion
generation, question rewriting as well as generation of sequence of questions and answers from a
givenKnowledge-graphorvice-versa.
This is ongoing work. The corpus has been released publicly and a paper presenting the
corpus will be submitted for publication soon. This dataset was also used by the PhD student
Juliette Faille supervised by Claire Gardent as part of the collaboration with Orange in the ITN
European Project NL4XAI. Particularly, it was her subject of study during her secondment at
Orange. Herworkfocusedonstudyingexplainabilityandfactualquestiongeneration.
4.4 Other Contributions: Graph Embeddings
This work was made by the PhD candidate Sebastien Montella in co-supervision with Dr. Jo-
hannes Heinecke at Orange and it was published in (Montella et al., 2021). While most of di-
alogue systems store their knowledge using simple structures, namely a set of slot-value pairs,
worldknowledgeisusuallystoredinKnowledgeGraphs. AKGisacollectionoftriples⟨s,p,o⟩;
wheres,pandostandforthesubject,predicateandobjectrespectively. Anentitydenoteswhether
5https://github.com/Orange-OpenSource/KGConv
6https://www.wikidata.org/
7https://www.wikidata.org/
644.4OtherContributions: GraphEmbeddings
#1 Triple (NGC4833,partof,MilkyWay)
original NGC4833ispartofwhatastronomicalobject?
subject NGC4833
rewritten NGC4833ispartofwhatastronomicalobject?
original WhereisNGC4833located?
subject NGC4833
rewritten WhereisNGC4833located?
Answer MilkyWay
#2 Triple (NGC4833,discovererorinventor,NicolasLouisdeLacaille)
original WhowasbehindthediscoveryofNGC4833?
subject NGC4833
rewritten Whowasbehindthediscovery?
original WhatwasthenameofthediscovererofNGC4833?
subject NGC4833
rewritten Whodiscoveredthisobject?
original WhofoundNGC4833?
subject NGC4833
rewritten Whofoundthisobject?
Answer NicolasLouisdeLacaille
#3 Triple (NicolasLouisdeLacaille,religionorworldview,CatholicChurch)
original Whatwashisreligion?
subject his
rewritten Whatwashisreligion?
original Whatfaithdidhefollow?
subject he
rewritten Whatfaithdidhefollow?
Answer CatholicChurch
Table 4.6: Excerpt of a question-answer conversation along with the related triples. The root
entity is NGC 4833, from the theme “space object”. The rewritten corresponds to the in-context
questionthathasbeenautomaticallygeneratedbyaT5model.
a subject or an object and a relation denotes a predicate that links two entities. The main contri-
butions of this work are: (i) it explores graph embeddings in the hyperbolic space instead of the
Euclidian space; (ii) it considers the time parameter; (iii) it propose a hyperbolic model aware of
time and (iv) it compares the performance of state-of-the-art models that takes into consideration
timeversusmodelsthatusenegativesampling.
Since KGs are sometimes incomplete, one important task of NLP is Link Prediction (LP),
which consists in predicting the missing connections between entities. This task can help for in-
stance to build knowledge graphs on the fly. Each entity and relation are map into a vector space
tolearnlow-dimensionalembeddingssuchthat,validtriplesmaximiseadefinedscoringfunction
and that fallacious triples minimise it. An approach is efficient if it can model multiple relational
patterns. Some predicates are symmetric (e.g. marriedTo), asymmetric (e.g. fatherOf), an in-
version of another relation (e.g. fatherOf and childOf) or a composition (e.g. grandfatherOf).
Hierarchical relations have remained challenging to model in Euclidean space, while hyperbolic
65
stnairavnoitseuQ
stnairavnoitseuQ
stnairavnoitseuQquestions
person 31671 327 71915 25918 184939 29352 11386 225677
country 2171 171 3475 703 5085 817 214 6116
ideology 1220 169 1677 450 3112 581 228 3921
spaceobject 2586 116 6360 5961 0 0 50158 50158
molecularentity 17798 151 38314 23033 154511 24587 9531 188629
historicalevent 4695 189 7770 4972 35270 5684 2247 43201
food 2532 166 4012 2099 15050 2230 1011 18291
taxon 3190 215 5408 1902 0 0 16099 16099
with unseen properties 13651 404 24123 5558 0 0 51813 51813
wholedataset 63345 458 142691 70596 397967 63251 142687 603905
Table 4.7: For each theme, the table gives: the number of different entities and properties ap-
pearinginconversations,thenumberofconversations,andthenumberofquestionsforeachsplit.
Note that in the entities and properties columns, the “total” values are not the sum of the cells
above;thisisbecausesomeentitiesandpropertiesappearinseveralthemes.
geometry reveals to be a strong asset to capture hierarchical patterns. Nevertheless, the afore-
mentionedapproachesrepresentembeddingsasinvarianttotime. Forexample,thetriple⟨Donald
Trump,presidentOf,U.S.⟩isnotlongercorrectin2022.
This work shows that an optimised number of negative samples enables the state-of-the-art
model ATTH (Chami et al., 2020) to reach competitive or even better performance on temporal
link prediction while being unaware of the temporal aspect. It also introduces an extension of
ATTH,namely HERCULES8.
This was the first attempt to leverage the curvature of a manifold to coerce time-aware rep-
resentation. An ablation study of distinct curvature definitions has been done to investigate the
compellingresultsof ATTH overtime-awaremodels.
ProblemDefinition
Lets consider a valid quadruplet ⟨s, p, o, t⟩ ∈ S ⊂ E ×R×E ×T , with E, R and T the sets of
entities, relations and timestamps respectively and S the set of correct facts. A scoring function
f : E × R × E × T → R is defined such that f(s,p,o,t) is maximised for any quadruplet ∈
S, and minimised for corrupted quadruplet (∈/ S). Throughout the optimisation of the foregoing
constraint, representations of entities, relations and times are learned accordingly. The resulting
embeddings should then capture the multi-relational graph structure. Thus, f is measuring the
probabilitythatanentitysisconnectedtoanentityobytherelationpattimet.
HyperbolicGeometry
Hyperbolic geometry belongs to non-Euclidean geometry. In contrast to Euclidean geometry
relying on Euclid’s axioms (Heath and Euclid, 1956), non-Euclidean geometry rejects the fifth
8HyperbolicRepresentationwithTimEandRelationalCUrvaturesforTemporaLKnowledgEGraphS
66
seititne
seitreporp
selpirt
.vnoc
niart
ved tset
latot4.4OtherContributions: GraphEmbeddings
axiom known as the parallel postulate. It states that given a point x and a line l , there exists
1
a unique line l parallel to l passing through x. This is only possible due to a (constant) zero
2 1
curvatureofthespace. Thecurvaturedefineshowmuchthegeometrydiffersfrombeingflat. The
higher the absolute curvature, the curvier. Euclidean space has a zero curvature hence called flat
space. WhenrepresentedinanEuclideanspace,straightlinesbecomecurved,termedasgeodesics
(Fig. 4.3).
T cBn,c
x
x
u
v
logc(v)
x
expc(u)
x
O
Bn,c
Figure 4.3: Illustration of the exponential and logarithmic maps between the Poincare´ ball Bn,c
andthetangentspaceT cBn,c.
x
Hyperbolic geometry comes with a constant negative curvature. The interested reader is re-
ferredtoformaldefinitionsin(Montellaetal.,2021).
From ATTH to HERCULES
Given a quadruplet, ⟨s, p, o, t⟩, we note eH, rH and eH the hyperbolic embeddings of the subject,
s p o
predicateandobjectrespectively.9 ATTH usesrelation-specificembeddings,rotations,reflections
and curvatures. The curvature is defined as depending on the corresponding relation p involved.
Precisely, a relation p is attributed with an individual parametric curvature c . The curvature c is
p p
definedinEq. 4.1as:
c = σ(µ ) (4.1)
p p
where µ is a trainable parameter ∈ R and σ is a smooth approximation of the ReLU activation
p
function defined in [0,+∞]. With such approach, the geometry of the manifold is learned, thus
modified for a particular predicate. The curvature dictates how the manifold is shaped. Changing
the curvature of the manifold implies changing the positions of projected points. This means that
fordistinctrelations,thesameentitywillhavedifferentpositionsbecauseofthedifferentresulting
geometries for each relation. For example, lets consider the triples t := ⟨Barack Obama, visit,
1
France⟩andt := ⟨BarackObama,cooperate,France⟩. TheEuclideanrepresentationsofentities
2
BarackObamaandFrancefrombothfactswillbeprojectedontotheriemannianmanifold. How-
ever,thestructure(i.e. curvature)ofthemanifoldchangesasafunctionoftherelationofeachfact
(i.e. ’visit’ and ’cooperate’). Therefore, the resulting hyperbolic embbeding of Barack Obama of
t will not be the same resulting hyperbolic embedding of Barack Obama in t . By analogy, the
1 2
sameholdsforentityFrance.
9SinceATTHisnotconsideringtime,theparametertisnotused.
67Datasets |E| |R| |T | Training Validation Test
ICEWS14 7,128 230 365 72,128 8,941 8,963
ICEWS05-15 10,488 251 4017 368,962 46,275 46,092
Table4.8: ICEWS14andICEWS05-15DatasetsStatistics
In order to learn rotations and reflections, ATTH uses 2 × 2 Givens transformations matri-
ces (Chami et al., 2020). Those transformations conserve relative distances in hyperbolic space
andcanthereforedirectlybeappliedtohyperbolicembeddings(isometries). Furthermore, ATTH
utilizes an hyperbolic attention mechanism to represent complex relations that can be a mixture
of rotation and reflection. The attention scores are computed in the tangent space by projecting
thehyperbolicrotationembeddingandhyperbolicreflectionembeddingwiththelogarithmicmap
into the euclidian space, as shown in Figure 4.3. Then, the attention vector is mapped back to
manifold using the exponential map. We propose HERCULES, a time-aware extension of ATTH.
HERCULES redefinesthecurvatureofthemanifoldasbeingtheproductofbothrelationandtime.
The main intuition of HERCULES is that both relation and time directly adjust the geometry of
the manifold such that the positions of projected entities are relation-and-time-dependent. This
is advantageous in that no additional temporal parameters per entity are needed. Since the whole
geometryhaschangedforspecificrelationandtime,allfutureprojectionsontothatmanifoldwill
be aligned to the corresponding relation and timestamp. We investigate different curvature def-
initions and time translation in our experiments (see the next Section). The scoring function of
HERCULES remainssameas ATTH.
When learning hyperbolic parameters, the optimisation requires to utilise a Riemannian gra-
dient (Bonnabel, 2013). However, proven to be challenging, we instead learn all embeddings in
the Euclidean space. The embeddings can then be mapped to the manifold using the exponential
map. ThisallowstheuseofstandardEuclideanoptimisationstrategies.
Experiments
Datasets Forfaircomparisons,wetestourmodelonsamebenchamarkdatasetsusedinprevious
works, i.e. ICEWS14 and ICEWS05-15. Both datasets were constructed by (Garc´ıa-Dura´n et al.,
2018) using the Integrated Crisis Early Warning System (ICEWS) dataset (Boschee et al., 2018).
ICEWS provides geopolitical information with their corresponding (event) date, e.g. ⟨Barack
Obama, visits, France, 2009-03-11⟩. More specifically, ICEWS14 includes events that happened
in2014whereasICEWS05-15encompassesfactsthatappearedbetween2005and2015. Wegive
theoriginaldatasetsstatisticsinTable4.8. Toincreasethenumberofsamples,foreachquadruplet
⟨s, p, o, t⟩ we add ⟨s, p−1, o, t⟩, where p−1 is the inverse relation of p. This is a standard data
augmentation technique usually used in LP (Balazˇevic´ et al., 2019; Goel et al., 2020; Han et al.,
2020).
Evaluation Protocol & Metrics Given a (golden) test triple ⟨s, p, o, t⟩, for each entity s′ ∈ E,
weinterchangethesubjectswiths′ andapplythescoringfunctionf ontheresultingquery⟨s′,p,
o,t⟩. Sincereplacingsbyallpossibleentitys′ mayendupwithacorrectfacts,wefilteroutthose
valid quadruplets and give them extremely low scores to avoid correct quadruplets to be scored
higher than the tested quadruplet in final ranking (Bordes et al., 2013). We then rank the entities
684.4OtherContributions: GraphEmbeddings
basedontheirscoresindescendingorder. Westoretherankofthecorrectentitysnotedz . Thus,
s
themodelshouldmaximizethereturnedscorefortheentityssuchthatz = 1. Thesameprocess
s
isdoneusingtheobjecto.
To evaluate our models, we make use of the Mean Reciprocal Rank (MRR). We also provide
theHits@1(H@1),Hits@3(H@3)andHits@10(H@10)whichassessonthefrequencythatthe
validentityisinthetop-1,top-3andtop-10position,respectively.
Results We provide link prediction results on ICEWS14 and ICEWS05-15 for ATTH, HER-
CULES and different models from the literature. As (Han et al., 2020), we adopted a dimension
analysis to investigate behaviors and robustness of approaches. When possible, we re-run official
implementation of models. Otherwise, official or best results in literature are reported. Results
areshowninTable4.9.
As expected, hyperbolic-based strategies (i.e. DYERNIE, ATTH and HERCULES) perform
much better at lower dimensions, outperforming most of other approaches with ten times less
dimensions. Wereportanaverageabsolutegainof11.6%pointsinMRRwithonly10dimensions
over the median performance of other approaches with 100 dimensions. This strengthens the
effectivenessofhyperbolicgeometrytoinducehigh-qualityembeddingswithfewparameters.
Astonishingly, we notice that ATTH model is highly competitive despite the absence of time
parameter. ATTH exhibits new state-of-the-art or statistically equivalent performances compared
to DYERNIE and HERCULES. Weremarknostatisticallysignificantdifferencesinperformances
between hyperbolic models.10 Importantly, unlike other research carried out in this area, time
information here does not lead to any notable gain. This seems to indicate that other parameters
shouldbeconsidered. Weexaminethisphenomenoninsection4.4.
On ICEWS14, for dim ∈ {20,40,100}, both ATTH and HERCULES outperform DYERNIE
by a large margin. We witness an improvement of 2.5% and 5% points in MRR and Hits@1
with 100-dimensional embeddings. On ICEWS05-15, ATTH and HERCULES yield comparable
achievements with the state-of-the-art. In contrast to DYERNIE, it is noteworthy that ATTH and
HERCULES utilizeasinglemanifoldwhilereachingtopperformances.
We also distinguish tempered results on Hits@10 metric for ATTH and HERCULES models.
Thissuggeststhatduringoptimization, ATTH and HERCULES favorrankingsomeentitiesontop
whileharmingtherepresentationofothers.
TimeAwarenessvsNegativeSampling.
First, besides time translation, we probe different curvature definitions to identify fluctuation in
performances. WeanalysehowtimeinformationalterstheLPresultsbyaddingtimeaspartofthe
curvature (i.e. HERCULES) and as a translation. We also explore if incorporating the Euclidean
dotproductofthesubjectandobjectembeddings(noted⟨eE, eE⟩)intothecurvaturehelpstolearn
s o
abettergeometry. AnablationstudyisgiveninTable4.10.
Albeit counter-intuitive, we observe that our results corroborate with our initial finding: time
information is not the culprit of our high performances. More strikingly, a simple relational cur-
vature (i.e. ATTH) is sufficient to perform best on ICEWS14 (dim = 40). Neither the inclusion
10WeperformedtheMixed-FactorialAnalysisofVariance(ANOVA),inwhichtheindependentvariablesarethe
dimensionandthemodelandthedependentvariableisthemetric. Weconsidertwogroupsoneforeachdataset. We
reportp-valuesof0.842,0.872,0.926and0.229forMRR,H@1,H@3andH@10respectively.
69Datasets ICEWS14(filtered) ICEWS05-15(filtered)
dim Model MRR H@1 H@3 H@10 MRR H@1 H@3 H@10
ATISE✝ 18.0 3.03 23.9 48.7 15.9 4.35 19.22 41.0
TERO✝ 7.25 2.39 6.40 16.6 10.3 3.54 10.1 23.2
10 DYERNIE✳ 46.2 36.0 51.1 66.3 58.9 50.5 63.2 75.1
HERCULES 46.0 34.9 52.4 66.0 54.7 43.8 61.8 73.2
ATTH 45.6 34.2 52.0 66.4 49.9 34.4 61.6 73.6
ATISE✝ 19.1 1.28 28.2 54.7 24.5 7.67 32.3 59.2
TERO✝ 24.5 13.8 28.01 46.3 27.1 13.5 33.3 54.1
20 DYERNIE✳ 53.9 44.2 58.9 72.7 64.2 56.5 68.2 79.0
HERCULES 55.5 47.2 59.4 71.4 63.2 55.2 67.7 77.6
ATTH 55.2 46.7 59.7 71.4 63.5 55.8 67.7 77.5
ATISE✝ 38.4 23.3 47.6 67.3 35.7 19.2 44.3 69.1
TERO✝ 35.1 22.7 40.5 60.8 28.3 12.7 35.3 60.5
40 DYERNIE✳ 58.8 49.8 63.8 76.1 68.9 61.8 72.8 82.5
HERCULES 61.2 54.3 64.7 74.1 68.5 62.1 72.0 80.9
ATTH 61.7 54.5 65.4 75.4 68.5 62.0 71.9 80.6
TRANSE✳ 30.0 14.8 42.7 60.1 30.4 13.3 42.4 61.1
DISTMULT✳ 57.5 46.9 64.2 77.9 47.1 33.6 55.1 72.5
COMPLEX✳ 49.3 36.6 56.2 74.2 39.0 22.9 49.2 68.4
TTRANSE✳ 34.4 25.7 38.3 51.3 35.6 15.4 51.1 67.6
TCOMPLEX✳ 31.8 12.9 45.7 63.0 45.1 36.3 49.2 62.0
100 HYTE✳ 33.1 6.8 54.5 73.6 38.1 7.6 65.0 80.4
ATISE✝ 52.2 41.0 60.0 72.7 47.0 32.4 55.5 76.4
TERO✝ 45.4 34.0 52.2 67.0 41.1 26.3 48.9 71.7
DYERNIE✳ 66.9 59.9 71.4 79.7 73.9 67.9 77.3 85.5
HERCULES 69.4 65.0 71.4 77.9 73.5 68.6 76.1 82.9
ATTH 69.5 65.0 71.5 78.2 73.6 68.6 76.0 82.9
Table4.9: LinkpredictionresultsonICEWS14andICEWS05-15datasets: (✝)resultsareobtained
using the official implementation of (Xu et al., 2020), (✳) results are taken from (Han et al.,
2020). For each dimension (i.e. dim), best results are in bold and second-to-best underlined. No
statisticallysignificantdifferences inperformanceareobserved between DYERNIE, HERCULES
and ATTH.
Relation Time Time ⟨eE, eE⟩
s o MRR H@1 H@3 H@10
Curvature Curvature Translation Curvature
✓ ✗ ✗ ✗ 61.7 54.5 65.4 75.4
✓ ✓ ✗ ✗ 61.2 54.3 64.7 74.1
✓ ✓ ✓ ✗ 60.1 52.1 64.5 75.0
✓ ✓ ✓ ✓ 49.5 38.9 55.4 69.2
Table 4.10: Ablation study: Link prediction results on ICEWS14 using ATTH (dim = 40) with
differentcurvaturedefinitionsandtimetranslationapplied.
704.4OtherContributions: GraphEmbeddings
of a time translation, similarly to TTRANSE, nor the Euclidean dot product provide interesting
outcomes.
We then probe the sensitivity of HERCULES towards temporal feature by performing LP with
incorrect timestamps. Our intuition is to inspect whether feeding invalid timestamps during eval-
uationexhibitssignificantvariationornotcomparedtothereferenceperformances,i.e. LPresults
with initial (non-corrupted) testing samples. To do so, for each testing quadruplet, we replace
the (correct) time parameter with each possible timestamp from T . We therefore collect multiple
LP performances of HERCULES corresponding to each distinct timestamp. Our finding is that
despite erroneous timestamps, LP results show insignificant discrepancies with the initial HER-
CULES performance (dashed red line). This indicates that HERCULES gives little importance to
thetimeparameterandthusonlyreliesontheentityandthepredicatetoperformknowledgegraph
completion. Thisfurtherhighlightsourfindingthattimestampisnotresponsibleforourattracting
performances.
Wethereforeassumethattheoptimisationproceduremaybeinvolved. Weconsequentlyques-
tion the effect of negative sampling. Precisely, we train HERCULES with dim = 40 by tuning the
number of negative samples between 50 to 500. For both, ICEWS14 and ICEWS05-15, negative
sampling shows considerable gain as the number of samples increases. We record an absolute
gain of 5% points in MRR from 50 to 500 samples. We can see a rapid growth in MRR when the
number of samples is inferior to 200. Adding 50 samples is equivalent to about 2% points gain
in MRR. Then, performances reach a plateau around 300 negative samples. We conjecture that
a diversity in negative samples is enough to learn good representations. Notwithstanding that a
largenumberofnegativesamplesheavilyconstraintsthelocationofentitiesinspace,theresulting
embeddingsmightbenefitfromittobebetterpositionedrelativelytoothers.
We conclude that despite the present time parameter, an optimal negative sampling enables
to reach new state-of-the-art outcome. Therefore, we argue that time is not the only parameter
thatshouldbeconsideredwhenperformingLP.Wehighlightthatoneshouldberaisingawareness
whentrainingTKGrepresentationstoidentifyiftimeistrulyhelpingtoboostperformances.
7172Chapter 5
Data Collection, Annotation and
Frameworks
This chapter summarises the corpora, annotations, and frameworks in which I worked on. It also
describesbrieflythenationalandinternationalresearchprojectsinwhichIhavebeeninvolved.
5.1 Corpora
ThissectiondescribesbrieflytheworkIhavedoneindatacollectionandannotation.
5.1.1 The French Portmedia Corpus
The French MEDIA corpus collects about 70 hours of spontaneous speech (1258 dialogues, 46k
utterances, 494.048 words and 4068 distinct words) for the task of hotel reservation and tourist
information(Bonneau-Maynard et al., 2009). Calls from 250 speakers to a simulated reservation
system (i.e. the Wizard-of-Oz) were recorded and transcribed. Dialogues are full of disfluencies,
hesitations, false starts, truncations or fillers words (e.g., euh or ben). I worked on the semantic
annotations of this corpus as I was involved in the French ANR project PORTMEDIA (Rojas-
Barahona et al., 2011; Rojas-Barahona and Quignard, 2011). 330 utterances were manually an-
notated with semantic relations (i.e. High-Level Semantics). This gold corpus gathers 653 head
segments and 1555 argument segments, from which around 20 are both arguments and heads,
such as une chambre in Figure 4. This work focuses on annotating the semantic structure, by
segmentatingusers’utterancesintoconcepts.
Thisontologyidentifiestheconceptsthatcanhavearguments,andwethususethisinformation
tofurtherdistinguishbetweenheadsegmentsthatcanhavearguments.
Besides the gold annotation, a silver annotation of the whole MEDIA dataset was also pro-
vided. Itwasgeneratedautomaticallyafterapipelineofsyntacticanalysis,semanticrolelabelling
andextractionofsemanticframes. Iimplementedthewholepipelineaswellastheannotationtool
used by the annotators. I also collaborate to the creation of a Bayesian model for inferring these
annotations(Lorenzoetal.,2013).
735.1.2 The French Emospeech Corpus
The French Emospeech corpus was already introduced in Section 3.1.1. As described in (Rojas-
Barahona et al., 2012b), to collect Human-Game dialog data, we developed a Wizard-of-OZ
(WOZ) interface using the MITRE Dialog Toolkit Midiki (Burke et al., 2003). Midiki, is a
portabletoolkitforbuildingdialoguemanagersinJava. Itimplementstheinformation-statemodel
of dialogue (Traum and Larsson, 2003) where in essence, the information state models the pro-
gression of dialog while update rules formalise the way that information state is changed as the
dialogprogresses.
We first extended Midiki to support a multi-agent architecture and the configuration from a
relational database. We then used this extended Midiki (i) to develop a rule-based dialog system
for the MP game and (ii) to implement two Wizard-of-OZ interfaces for data collection: the free-
andthesemi-automaticWOZinterface.
The free WOZ interface aims to simulate mixed-initiative dialogs by allowing the wizard to
chatwiththeplayerasshemovesaroundthegamewhilesimultaneouslystoringallinteractionsin
a database. A virtual dialog manager ensures that the wizard respects the game logic, starting the
appropriate subdialogs at the appropriate place in the virtual world. In this setup, the interactions
between the wizard and the player simulate a direct Human-Human dialog in the context of the
MPgame.
In contrast, the semi-automatic wizard favours system-driven dialogs by connecting the Wiz-
ardnotonlywiththeplayerandthegamebutalsowiththerule-baseddialogmanager(Figure??).
This dialog manager supports the Wizard by automatically interpreting the player’s input and se-
lecting a possible response. As the Wizard interacts with a player, she can then either accept the
response suggested by the rule-based dialog manager (if this response is appropriate) or enter a
differentresponse(whenevertheresponsesuggestedisincorrectorinappropriate).
Figure 5.3 shows the architecture of the WOZ interface: the dialog manager (either Midiki
or a virtual DM), the MP game, the Wizard of Oz interface and an automatic speech recognition
module (ASR) 1 communicate together within the Open Agent Architecture (OAA) (Cheyer and
Martin,2001). TheWOZinterfaceisimplementedasawebserviceandallinteractionsarelogged
intoarelationaldatabase.
To support data collection for different game scenarios, we also developed a Dialogue Con-
1AlthoughtheWizardFrameworksupportedbothspeechandwritteninput,wedidnotrecordspeechinourfirst
experiments. Alldataisthereforewrittendata.
Thing
Attributes Location Person Time Object
Price General Relative Restaurant Hotel Room
Park Near
Figure5.1: ExcerptofMEDIAontology
745.1Corpora
Bookedobject
Price Price
Agent
Je voudrais le prix en fait je euh une chambre pas che`re
I ’dlike the price well infact I uh a room not expensive
Reserve Room
Figure 5.2: Excerpt of the semantic structure for a sentence in the PORTMEDIA corpus. Tradi-
tional dependency notations are used: the head segment points to the argument segment, where
segmentsareshownwithboxes(arrowslinksegments,notwords!). Thesemanticclassassigned
toeachheadsegmentisshowninboldbelowthetranslatedtext.
Figure5.3: GeneralArchitecturefortheWizardofOZexperiments: modulesareimplementedas
agentswithintheOpenAgentArchitecture.
figuration Tool that permits defining for each new game the information that is relevant for the
dialog, namely, which characters are present in the game; which goals are being pursued at each
step in the game; and which subdialogs are being conducted in which order during the game,
betweenwhichcharactersandtoachievewhichgoal.
DatacollectionfortheMPgameproceededintwosteps. First,anativeFrenchspeakerplayed
the wizard using the semi-automatic WOZ with 40 subjects. Next, three groups of students from
the Language and Communication Erasmus Mundus Master in Nancy collected dialogs using the
freeWOZ.TheresultsareshowninTable5.4.
Dialoglengthvariesbetween78and142turnswithanaveragelengthof106turnsperdialog.
Expert players completed the game in around 50 minutes in average while novice players took
75Subjects Dialogs Uttces Tokens PlayerU. PlayerTokens PlayerTokenTypes
Semi-Aut. 40 591 4874 77854 1321 12901 1427
Free 50 658 5580 90655 2288 18712 1542
Total 90 1249 10454 168509 3609 31613 2969
Figure5.4: Datacollected
between1and1.5hour.
Figure 5.5: Wizard of OZ Graphical User Interface (GUI).It is split in three parts: the interpreta-
tion,thedialogandthegeneration. Thewizardreceivestheinputsentenceattheleft-side,shecan
see the whole dialog in the centre, and she edits the generated utterance at the right side. She can
introducealsothedialogmoveassociatedtotheinputandoutputsentences.
After configuration of the WOZ tool using the Dialog Configuration Tool mentioned in the
preceding section, the free-WOZ was also used by Master students from Rennes University for
collecting dialogs in a game simulating the visit of an exhibition on Alice in Worderland. In this
way,theycollected25dialogsusingLewisCarroll’ssubrealist.
5.1.3 Annotating Posts Following Cognitive Behavioural Therapy Princi-
ples
Thisworkwaspublishedin(Rojas-Barahonaetal.,2018). Themaingoalofannotationsbasedon
Cognitive Behavioural Therapy (CBT) was to develop the understanding component of a health
assistant for preventive intervention of mental health. The corpus consists of 500K written posts
that users anonymously posted on the Koko platform2. This platform was based on the peer-to-
peer therapy proposed by (Morris et al., 2015). In this set-up, a user anonymously posts their
problem (referred to this as the problem) and is prompted to consider their most negative take on
theproblem(referredtothisasthenegativetake). Subsequently,peerspostresponsesthatattempt
2https://itskoko.com/
765.1Corpora
Problem: I agreed to go on a last-minute business trip to
Seoul. Right now I'm overweight and feel gross. We're staying
in a really fancy area, and I'm afraid people will think I'm fat
and disgusting.
Negative take: I’m afraid I will be the grossest, ugliest person
there.
thinking errors emotions situations
jumping to negative anxiety work
conclusions
shame health
disqualifying the positive
Figure5.6: AnexampleofanannotatedKokopost.
to offer a re-think and give a more positive angle on the problem. Initially, any first-time Koko
user would be given a short introductory tutorial in the art of ’re-thinking’/’re-framing’ problems
(based on CBT principles), before being able to use the platform; this however changed over
time, as the age group of the users decreased, and a different introduction, emphasizing empathy
and optimism, was used in relation to suggesting helpful responses (less CBT-based than the ’re-
thinking’). Some of the data annotated in this study was drawn from this later phase. When
first developed, this framework was shown to be more efficacious than expressive writing, an
intervention that has been shown to improve physical and emotional well-being (Morris et al.,
2015). Since then, the company has developed an app that has collected a very large number of
posts and associated responses. In this work we only focus on analysing the posts. Figure 5.6
givesanexampleofanannotatedpost.
We draw from principles of Cognitive Behavioural Therapy (CBT) to define the ontology.
CBT is derived originally from Beck’s Cognitive Therapy model theory (Beck, 1976; Beck et al.,
1979) which says that our emotions and behaviour are influenced by the way we think and by
how we make sense of the world. Thus, if the patient changes the way he or she thinks about
their problem that will in turn change the way he or she feels and behaves. A major underlying
principle of CBT is the idea of cognitive distortion, and the value in challenging this. In CBT,
patientsarehelpedtotesttheirassumptionsandviewsoftheworldinordertocheckiftheyfitwith
reality. When patients learn that their perceptions and interpretations are distorted or unhelpful,
they then work at correcting them. Within the realm of cognitive distortion, CBT identifies a
number of specific self-defeating thought processes, or thinking errors. There is a core of around
10 to 15 thinking errors, with their exact titles having some fluidity. A strong component of CBT
is teaching the client to be able to recognise and identify the thinking errors themselves, and
ultimatelydiscardthenegativethoughtprocess,and’re-think’theirproblem.
We consider the first step that a machine should be able to perform is to adequately decode
these ’thinking error’ concepts, along with identifying the key emotion(s) expressed, and situa-
tionalcontext,withinaparticularpresentedproblem. Therefore,ourontologyconsistsofthinking
errors,emotions,andsituations.
5.1.4 Conversational Question Answering with Rewriting
IleadtheextensionofthecorpusCoQAwithquestionrewritingasexplainedinSection4.2.
775.1.5 Conversational Question Answering in French
IproposedtoextendtheFrenchcorpusCalorwithsequenceofquestionsandanswers. Theanno-
tationsweredonebytheUniversityofAix-Marseille. ThisisajointworkwithGeraldineDamnati
andFredericBechet. Thisworkhasbeenpublishedin(Be´chetetal.,2022).
Calor-DialisanenrichedversionoftheCalorcorpus(Marzinottoetal.,2018),collectedfrom
French encyclopedic data in order to study Information Extraction on domain specific data. The
corpus was initially annotated in semantic Frames (Calor-Frame (Be´chet et al., 2017)) and en-
richedwithafirstsetofquestionsforMachineReadingQuestionAnswering(Calor-Quest(Be´chet
et al., 2019)). Calor-Dial addresses the scope of conversational Question Answering. The main
originality is that different types of questions are annotated, including more challenging configu-
rationsthaninclassicalQAcorpora.
5.1.6 Conversational QA grounded in Wikidata
IcontributetothecreationofthecorpusKGConvandIleadtheworkfromtheOrangeside,aspart
of the research project DIANA. This work is in colaboration with Claire Gardent (CNRS) as part
ofthepartnershipoftheITN-EuropeanprojectNL4XAI.ThiscorpusispresentedinSection4.3.
5.2 Dialogue Frameworks
During the course of my research career, I was directly involved in the development of dialogue
frameworks. StartingfromAdaRTE(Rojas-BarahonaandGiorgino,2009)duringmyPhDstudies
to the most widespread one, PyDial (Ultes et al., 2017a). In this Section I will briefly present
PyDial.
5.2.1 PyDial
PyDial is an open-source end-to-end statistical spoken dialogue toolkit developed by the Univer-
sity of Cambridge (Ultes et al., 2017b). It provides implementations of statistical approaches for
alldialoguemodules: NLU.DST,Policy,NLG.Thetermstatisticalmeansthat: (i)theframework
preserves the confidence probability of each module, thus each module outputs are the N-Best
list of hypotheses; (ii) Deep learning models can be easily integrated in it; (iii) The framework
implements POMDP dialogue systems. Thus, reinforcement learning dialogue management is
fully supported, with the user simulator and reward estimators. Moreover, it has been extended
to support multiple domains. It offers easily extensible to other domains and or specialised mod-
uleimplementations. Italsooffersdomain-independentimplementationsofthedialoguemodules
(seeFigure5.7). ThetoolkitisavailablefordownloadundertheApache2.0license.
5.2.2 Dialport
Dialport (Lee et al., 2017) is an academic dialogue portal to collect large amounts of real user
data for spoken dialog systems (SDS). Sophisticated statistical representations in state-of-the-art
SDS, require large amounts of data, which is difficult to obtain by academic teams. With one
central portal, connected to many different systems, the task of advertising and affording user
785.2DialogueFrameworks
Speech Semantic
Belief Tracking
Recognition Decoding
Belief
State
Speech Language
Policy
Synthesis Generation
Figure5.7: ArchitectureofamodularSpokenDialougSystem.
access can be done in one centralised place that all systems can connect to. DialPort provides a
steady stream of data, allowing system creators to focus on developing their systems. The portal
decides what service the user wants and connects them to the appropriate system which carries
on a dialog with the user, returning control to the portal at the end. Dialport was connected to the
Cambridge restaurant information system, which helps users find a restaurant in Cambridge,UK
basedonthearea,thepricerangeorthefoodtype. TheconnectionisdonethroughanAPI!(API!)
that connects Dialport to PyDial. An assesment of DialPort is presented in (Lee et al., 2019), to
summarise,28.8%ofchatbotutteranceswerenon-understandingrecoveryturns,suchas“canyou
pleaserephrasethat?”. 62.85%ofthetimesDialPortsuccessfullyrecommendedusersand78.40%
itcorrectlydirecteduserstotheappropriatesystem.
5.2.3 Conversational Search for General Knowledge
Converstional Search for General Knowledge (CS4GK) is a spoken conversational question an-
swering proof of concept that is able to answer questions about general knowledge from Wiki-
data3 (Rojas-Barahona et al., 2019). The dialogue component does not only orchestrate various
componentsbutalsosolvecoreferencesandellipsis.
The architecture of the proposed system consists of a speech-processing front-end, an under-
standingcomponent,acontextmanager,agenerationcomponent,andasynthesiscomponent. The
contextmanagerprovidescontextualisedmediationbetweenthedialoguecomponentsandseveral
question answering back-ends, which rely on data provided by Wikidata. Interaction with a hu-
manuserisachievedthroughagraphicaluserinterface(GUI).Figure5.8depictsthecomponents
togetherwiththeirinteractions.
3https://www.wikidata.org
79
larutaN
egaugnal eugolaiD
stcAFigure5.8: High-leveldepictionoftheproposedspokenconversationquestionansweringsystem. Arrows
indicatedataflowanddirection.
80Chapter 6
Scientific Project
After the breakthrough of ChatGPT (Ouyang et al., 2022), Large Language Models have been
widely used for distinct daily activities such as summarisation, translation, sentiment analysis,
question answering, redaction, code-generation, etc. Nevertheless, it is not clear how these mod-
elscanbeusedtosolvecomplexdecision-makingtasks,suchastask-orienteddialogue. Although
promising approaches have recently emerged (Wei et al., 2022; Yao et al., 2023, 2022), utilizing
LLMs to solve complex tasks is not straightforward. In the case of task-oriented dialogue, one
important point concerns the lack of control (Section 6.1.4). Although the conversation is fluid
and pleasant is it following the necessary steps to solve the task? Are these steps validated by
experts? Are these steps correctly grounded in the World knowledge? These questions bring
us to an important issue the evaluation (Section 6.1.1). Can we estimate whether the task was
accomplished, and whether the sequence of selected actions was indeed optimal? Retrieval Aug-
mented Generation (RAG) (Mao et al., 2021; Asai et al., 2022), can be used to retrieve useful
information that can be injected to the LLM as a prompt. This is a way of grounding the LLM
in ”factual” information. However, is there a way to be sure the retrieved information is indeed
factual?(Section 6.1.3) Moreover, the effort of prompt-engineering can not be neglected. Are we
comingbacktohand-craftedsolutionsbyhand-craftingprompts? Woulditbedifficulttomaintain
andtokeepthesepromptsup-to-date?
ThisscientificprojectproposesfirsttostudyrecentLLM-basedreasoningapproachesfortask-
oriented dialogue, providing a rigorous evaluation in terms of task-completion and success rate.
Wecanalso,inspiredbytheseminalevaluationframeworkParadise(Walkeretal.,1997a),thinkin
waystofindthecorrelationbetweentask-completionandusersatisfaction. Forinstance,unlikethe
studypresentedinParadise,inwhichatthattimeuserswerenotenjoyinglongconversationswith
repetitivesystems,maybeusersnowreallyenjoytalkingtoLLMs. However,astrongindicatorof
poorperformancemightbetoconfirmthatusersusuallyneedtocallagainbecausedespitehaving
afluidandnaturalinteraction,theirproblemswerenotsolvedatall.
Multimodality is another interesting research topic we will discuss in this proposal. Con-
cerningdialogue,emergentapproachesareVisualQuestionAnswering(VQA)andspeech-aware
emotion detection. The first one can be used in TOD wherein heterogeneous sources of knowl-
edge are identified. The second, can be used to personalise dialogue according to users’ mood
detectedbyspeechanalysis.
816.1 LLMs for Task-Oriented Dialogue
Recentlysynergisingreasoningandactinginlargelanguagemodels(ReAct)(Yaoetal.,2022)has
shown promising results employing few-shot prompting in a LLM with a sequence of thoughts,
actions,andobservations. LLMsareindeedcapableofperformingcomplextasks. Thoughtsrefer
totheinternalreasoningthatdecomposeaproblemintosub-problems. Forexample,ifthemodel
is asked the age of Barack Obama’s wife power of 3, the thoughts might be as follows. First, I
needtofindoutwhoBarackObama’swifeis(bycallingaquestion-answeringAPIsuchasGoogle
Search). Second, I need to find the age of Barack Obama’s wife (by calling a question-answering
API). Finally, I will calculate the age power of 3 (by calling a calculator API). Examples have
shown promising results for tasks such as multi-hop question-answering, WebShop, and a textual
version of a butler in a virtual environment, ALFWorld: the butler can report on actions he has
takentosolveagiventaskinakitchen(Yaoetal.,2022).
We need to study the recent state-of-the art in task-decomposition utilizing LLMs(Wei et al.,
2022; Yao et al., 2023, 2022). We can also explore Algorithm Distillation (Laskin et al., 2022),
whereinthelogsofcross-episodiceventsgeneratedduringthelearningprocessofaRLalgorithm,
are used to feed in a LLM, which as consequence can learn the optimal strategy. Since the state-
of-the-artismovingimpressivelyfast,thisproposalisopentoupcomingapproaches.
6.1.1 Benchmarcks and Evaluation
We will propose an evaluation framework that takes into account long-term memory, beyond
one dialogue session (Xu et al., 2022). For evaluating LLMs an evaluation framework that is
both model-agnostic and domain-independent is suitable. Therefore, we should think in ways
to detect the conversation goal together with performance indices, with or without humans in
the loop. In an initial state we can compare our previous results presented in Section 3.2.4
on hierarchical reinforcement learning for dialogue (Cordier et al., 2022, 2023) with LLMs that
followsReActandAlgorithmDistillationbyusingthemetricsintroducedinConvLab(Zhuetal.,
2020). Therefore, we can start this study by using the dataset MultiWoz, in which the user goal
is formally defined and provided. This corpus also includes the instructions given to annotators
duringdatacollection. Later,wecanmovetocomplextasksinwhichtheusergoalisnotprovided.
We can also study more realistic cases such as commercial or technical-support systems. We will
perform human evaluation to perceive their satisfaction with the system. As in Paradise (Walker
et al., 1997a) and in (Rojas-Barahona and Gardent, 2012), we would like to make a correlation
studybetweenobjectivemetrics(indicesofperformancethatcanbecomputedautomatically)and
subjectivemetrics(e.g. usersatisfaction). Thisframeworkcanbeextendedtoevenmorecomplex
interactionssuchasinmulti-modaldialoguesystems(Section6.2).
6.1.2 Interpreting LLMs
AfterevaluatingLLMsperformancefortask-orienteddialogue,wecanusemodelagnosticblack-
box interpretability methods (Cafagna et al., 2023) to understand how these models are able to
solvecomplextasks. WhereintheLLMarchitecturethisbehaviourisbeingproduced? Itisworth
notingthatthesemethodscanbeappliedonlytoOpen-SourceLLMs,inwhichwecanhaveaccess
to the model (e.g., Llama (Touvron et al., 2023), Falcon (Penedo et al., 2023)). Interpretability
826.1LLMsforTask-OrientedDialogue
mightgiveusaninsightofhowwecancorrectLLMstoavoidundesirablebehaviour(e.g. forget-
ting an important instruction, biases). Methods to correct LLMs are introduced in the following
Sections.
6.1.3 Retrieval Augmented Generation
One way to find grounded knowledge to feed into LLMs is through information retrieval (Mao
et al., 2021; Asai et al., 2022). Despite these techniques rank documents according to their rele-
vance, an important aspect is factuality (Thorne et al., 2018). Another aspect is that the sources
of knowledge are heterogeneous: they concern not only documents, but also images, recorded
interactions, knowledge graphs, tabular data, among others. Once we have retrieved crucial in-
formation there are several ways to inject this information into LLMs. The most adopted one
is prompt engineering, but controlling decoding might be more interesting in terms of scientific
research.
6.1.4 Controlling Decoding
There are distinct sampling mechanisms used for decoding such as greedy (T = 0 in Equa-
tion 6.2), beam search, top-k, nucleus or penalised sampling (Fan et al., 2018; Holtzman et al.,
2019; Keskar et al., 2019). An emergent method to control decoding is by adjusting sampling
weightsinbeamsearch(Ghazvininejadetal.,2017).
Let p be a pre-trained generative language model which has learned the distribution over
θ
tokensequencesbyoptimising:
(cid:88)
L = − logp (x |x ) (6.1)
θ t <t
t
ThenexttokencanbesampledbyapplyingSoftmaxwithtemperatureT becausethefinaldecoder
layerpredictslogitso,overthevocabularyspace:
exp(o /T )
p ∝ i (6.2)
i (cid:80)
exp(o /T )
j j
Beam search is a breadth-first search algorithm which explores the β tokens which best scores at
eachlevel. Then,thelikelihoodofsamplingforthenexttokenx atsteptcanbeaugmentedby
t+1
ascoringfunction:
(cid:88)
score(x ,b ) = score(b )+logp(x )+ α f (x ) (6.3)
t+1 t t t+1 i i t+1
i
Where the log-likelihood predicted by the pre-trained language model is defined by logp(x ).
t+1
The accumulated score of the already-generated words in the current beam state b is score(b ).
t t
A set of feature functions that define the preferences is f(.), which can be a binary classifier that
predicts whether a sample is from the true data distribution (Grover et al., 2019). Finally, α are
i
the associated weights that work like ”control knobs”. Therefore, the classifier can be used to
constrain factuality by predicting whether the token is grounded in knowledge or not. This is an
interestingpaththatcanbefurtherexplored.
Another interesting approach is Chain of hindsight (Liu et al., 2023), in which pairs of (an-
swer, feedback) are provided as input to the model during fine-tuning. Thus, the model learns to
conditionthegenerationonfeedbacksduringtraining.
836.2 Multimodal task-oriented dialogues
Generative models were also trained for process multiple modalities (Antol et al., 2015; Zhu
et al., 2016; Srivastava et al., 2021), supporting image captioning and VQA. After the release of
multi-modal LLMs, the borders between domains, tasks and modalities have been vanished. One
can think in VQA applications that interact with elders, visual impaired (Chen et al., 2022) or
more broadly patients or practitioners in the medical domain. Efforts for ethical Artificial Intelli-
gence (AI) can not be diminished before putting VQA to interact with sensitive population. This
is particularly true in Europe, after the AI regulation (Hacker et al., 2023). Therefore, important
research areas concern interpretability and evaluation of multi-modal LLMs to correct model bi-
ases that might produce any harm. Generally, the datasets used for training these large models
werebuiltwithoutincludingminoritiesorpeopleatrisk. Therefore,thesemodelscannotrespond
totheirneeds. ArigorousstudytoevaluateLLMs,involvingtheseindividualsmustbecarriedout.
Interpretability methods (Section 6.1.2) can help us to understand biases deep inside the model
and to provide insights to correct them. This can be done in public models for image captioning
orVQAsuchasOFA(Wangetal.,2022).
Mulitmodality might also be highly related to RAG in both text-to-image and image-to-text
models (Yasunaga et al., 2023), wherein the information is available as images, audio or video
(Section 6.1.3). Finally, multi-modality also involves speech, in this proposal I talk about ways
to adapt task-oriented dialogues that use either RL or LLMs for policy learning and that take into
accountthespeechsignalforemotiondetection.
6.2.1 Exploiting weak speech signals in the reward function
Treating speech and text together is a research topic widely studied in Spoken Language Under-
standing (SLU). An interesting research path would be to go beyond intention recognition up to
response generation. We worked with the PhD candidates Leo Jacqmin, Lucas Druart and other
researchers in a cascade approach that integrated the ASR (i.e. Whisper)(Radford et al., 2022)
with a generative model for DST and we participate to the challenge DSTC-11 (Jacqmin et al.,
2023)1. RL can be used for learning the strategy (Section 2.3.2). However, it assumes there is a
reward function. In dialogue systems the reward signal is scarce, because it is unbearable for a
user to send a satisfaction signal at each dialogue turn. For fluidity, the reward function is asked
at the end of the conversation, which produces a scarce signal, complicating the task of policy
learning. Thus,thepolicyspectrumwillhavealargevariancefromverypoortoverygood.
We explored already two ways of solving this problem: (i)predicting the user satisfaction at
each dialogue turn by using the interaction quality as reward signal (Section 3.2.2) and (ii) using
imitationlearningtoguidethepolicylearning(Section3.2).
One can think in improving the reward scarcity by exploiting the information in the speech
signal. Studying weak signals in the speech, such as the emotion, could be a way to improve the
reward signal. Emotion detection is important for developing conversational systems that could
adapt better to the users’ needs, improving as consequence the user satisfaction. For instance,
an early detection of distress would entail changing the dialogue strategy to quickly solve the
misunderstanding. Emotion detection involves an active research community producing datasets
1https://storage.googleapis.com/gresearch/dstc11/dstc11_20221102a.html
846.2Multimodaltask-orienteddialogues
build from actors playing the emotion (Burkhardt et al., 2005; Schro¨der et al., 2007), from wiz-
ard of oz or more recently from more natural content (Zadeh et al., 2018, 2020; Dhall et al.,
2012; Scheidwasser-Clow et al., 2022). Neural methods for emotion detection have been pro-
posed in (Trigeorgis et al., 2016), since there are not many dataset for training deep models from
scratch transfer learning or distillation from other modalities have been also proposed (Pepino
etal.,2021;Albanieetal.,2018).
Themainresearchquestionsbehindthisstudywillbe:
• Concerning the emotion detection: we need to study domain adaptation for emotion recog-
nitiontobetterexploittheavailablecross-domaindatasetstodialogue.
• Combine multi-modality to produce the reward, one can think in using speech signal and
other domain agnostic metrics, such as counting the number of repetitions, counting the
number of dialogue turns, measuring the misunderstandings, detecting the sentiment from
text.
• Evaluatewhethertheemotion-basedrewardissuitableforpolicylearning
• Compare RL with LLM-based policies (e.g. algorithm distillation (Laskin et al., 2022),
ChainofHindsight(Liuetal.,2023)).
8586Chapter 7
Conclusion
I presented in this dissertation a selected number of contributions I made to the areas of task-
orienteddialoguesystems,conversationalquestionansweringandgraphembeddings.
The contributions to task-oriented dialogue were in the fields of NLU, SLU and DM. Par-
ticularly, I explored classical machine learning techniques for NLU, convolutional and recurrent
neural networks to deal with noisy inputs for SLU, as well as data-augmentation techniques.
Althoughnotmentionedinthiswork,IamcurrentlysupervisingwithBenoitFavrefromtheUni-
versity of Aix-Marseille a PhD thesis on DST, recently we competed in the challenge DSTC-11
and we were awarded the first and second place (Jacqmin et al., 2023)1. Regarding the DM, I
exploredInverseReinforcementLearning,DeepReinforcementLearning,ImitationLearningand
StructuredPolicyLearning.
The contributions to conversational QA regard the annotation of existing datasets with in-
formation about ellipsis and coreferences, and with question rewriting to transform in-context
questions into out-of-context questions. The generative models released with these annotations
for the tasks of answer extraction, question generation and question rewriting; as well as the
models implemented for predicting ellipsis and coreferences were also presented. I also briefly
introduced the corpus KGConv grounded in Wikidata. Moreover, I presented our work on graph
embeddingsinthehyperbolicspace. Finally,Isummarisedthereleasedresourcessuchasdatasets
andframeworks,inwhichIcontributedtotheircreation,annotationanddevelopment.
This document consolidates my own contributions as young researcher, the work of two PhD
candidates supervised jointly with academics under the CIFRE convention. I could also collabo-
ratewithacademicsintwoITNprojects: ConversationalBrains(COBRA)andNL4XAI. Ashead
oftheprojectDIANAduring5years,Icouldalsodefinethemainworkpackagesoftheprojectand
superviseadynamicteamofresearchers,developers,studentsininternshipandapprenticeship.
I would like to focus my future research in proposing a framework to evaluate LLMs per-
formance in complex task-oriented dialogue. Moreover, I would like to explore interpretability,
retrievalaugmentationandsemanticallycontroltounderstandLLMsinternally,tosupportground-
ing and to generate factual information. I would like to explore recent reasoning LLMs approach
(e.g.,ReAct,algorithmdistillation,etc.) tostudyLLMscapabilitiestomakelong-termdecisions.
Itiswortnotingthattheseresearchpathsalsocovermulti-modalinteractions.
1https://storage.googleapis.com/gresearch/dstc11/dstc11_20221102a.html
8788Bibliography
Pieter Abbeel and Andrew Y Ng. 2004. Apprenticeship learning via inverse reinforcement learn-
ing. In Proceedings of the twenty-first international conference on Machine learning, page 1.
ACM.
Samuel Albanie, Arsha Nagrani, Andrea Vedaldi, and Andrew Zisserman. 2018. Emotion recog-
nition in speech using cross-modal transfer in the wild. In Proceedings of the 26th ACM inter-
nationalconferenceonMultimedia,pages292–301.
J. Allen, D. Byron, M. Dzikovska, G. Ferguson, and L. Galescu. 2001. Towards conversational
human-computerinteraction.
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence
Zitnick, and Devi Parikh. 2015. Vqa: Visual question answering. In Proceedings of the IEEE
InternationalConferenceonComputerVision(ICCV).
Akari Asai, Matt Gardner, and Hannaneh Hajishirzi. 2022. Evidentiality-guided generation for
knowledge-intensiveNLPtasks. InProceedingsofthe2022ConferenceoftheNorthAmerican
Chapter of the Association for Computational Linguistics: Human Language Technologies,
pages2226–2243,Seattle,UnitedStates.AssociationforComputationalLinguistics.
J. Austin. 1975a. How to Do Things With Words (William James Lectures). Harvard University
Press.
John Langshaw Austin. 1975b. How to do things with words: (william james lectures). Harvard
UniversityPress.
Krishnakumar Balasubramanian, Pinar Donmez, and Guy Lebanon. 2011. Unsupervised super-
vised learning II: Margin-based classification without labels. Journal of Machine Learning
Research,12:3119–3145.
Ivana Balazˇevic´, Carl Allen, and Timothy Hospedales. 2019. Multi-relational poincare´ graph
embeddings. InAdvancesinNeuralInformationProcessingSystems.
Andrew G Barto and Sridhar Mahadevan. 2003. Recent advances in hierarchical reinforcement
learning. Discreteeventdynamicsystems,13(1-2):41–77.
Fre´de´ric Be´chet, Cindy Aloui, Delphine Charlet, Geraldine Damnati, Johannes Heinecke, Alexis
Nasr,andFre´de´ricHerledan.2019. Calor-quest: generatingatrainingcorpusformachineread-
ing comprehension models from shallow semantic annotations. In MRQA: Machine Reading
89for Question Answering-Workshop at EMNLP-IJCNLP 2019-2019 Conference on Empirical
MethodsinNaturalLanguageProcessing.
Fre´de´ric Be´chet, Ge´raldine Damnati, Johannes Heinecke, Gabriel Marzinotto, and Alexis Nasr.
2017. CALOR-Frame: uncorpusde textesencyclope´diquesannote´ en cadresse´mantiques. In
ACor4French–Lescorpusannote´sdufranc¸ais-AtelierTALN,Orle´ans,France.
Fre´de´ric Be´chet, Ludivine Robert, Lina Rojas-Barahona, and Ge´raldine Damnati. 2022. Calor-
dial: a corpus for conversational question answering on french encyclopedic documents. In
CIRCLE(JointConferenceoftheInformationRetrievalCommunitiesinEurope).
A.T. Beck. 1976. Cognitive Therapy and the Emotional Disorders. New York, International
UniversitiesPress.
A.T. Beck, J. Rush, B. Shaw, and G Emery. 1979. Cognitive Therapy of Depression. New York,
GuildfordPress.
Silve`reBonnabel.2013. Stochasticgradientdescentonriemannianmanifolds. IEEETransactions
onAutomaticControl,58:2217–2229.
He´lene Bonneau-Maynard, Matthieu Quignard, and Alexandre Denis. 2009. Media: a semanti-
callyannotatedcorpusoftaskorienteddialogsinfrench: Resultsofthefrenchmediaevaluation
campaign. LanguageResourcesandEvaluation,43:329–354.
Antoine Bordes, Nicolas Usunier, Alberto Garcia-Dura´n, Jason Weston, and Oksana Yakhnenko.
2013. Translating embeddings for modeling multi-relational data. In Proceedings of the 26th
InternationalConferenceonNeuralInformationProcessingSystems-Volume2,NIPS’13,page
2787–2795,RedHook,NY,USA.CurranAssociatesInc.
Elizabeth Boschee, Jennifer Lautenschlager, Sean O’Brien, Steve Shellman, and James Starz.
2018. ICEWSWeeklyEventData. HarvardDataverse.
AbdeslamBoularias,HamidRChinaei,andBrahimChaib-draa.2010. Learningtherewardmodel
of dialogue pomdps from data. In NIPS Workshop on Machine Learning for Assistive Tech-
niques.Citeseer.
Quentin Brabant, Gwe´nole´ Lecorve´, and Lina M. Rojas Barahona. 2022. CoQAR: Question
rewriting on CoQA. In Proceedings of the Thirteenth Language Resources and Evaluation
Conference,pages119–126,Marseille,France.EuropeanLanguageResourcesAssociation.
Quentin Brabant, Lina Maria Rojas-Barahona, and Claire Gardent. 2021. Active Learning and
Multi-label Classification for Ellipsis and Coreference Detection in Conversational Question-
Answering. In 12th International Workshop on Spoken Dialog System Technology (IWSDS
2021),Singapour/Virtual,Singapore.
M. E. Bratman. 1987. Intention, Plans, and Practical Reason. Harvard University Press, Cam-
bridge,MA.
90BIBLIOGRAPHY
Paweł Budzianowski, Stefan Ultes, Pei-Hao Su, Nikola Mrksˇic´, Tsung-Hsien Wen, In˜igo
Casanueva, Lina M. Rojas-Barahona, and Milica Gasˇic´. 2017. Sub-domain modelling for dia-
loguemanagementwithhierarchicalreinforcementlearning. InProceedingsofthe18thAnnual
SIGdialMeetingonDiscourseandDialogue,pages86–92,Saarbru¨cken,Germany.Association
forComputationalLinguistics.
Paweł Budzianowski, Tsung-Hsien Wen, Bo-Hsiang Tseng, In˜igo Casanueva, Stefan Ultes, Os-
man Ramadan, and Milica Gasˇic´. 2018. MultiWOZ - a large-scale multi-domain Wizard-of-
Oz dataset for task-oriented dialogue modelling. In Proceedings of the 2018 Conference on
Empirical Methods in Natural Language Processing, pages 5016–5026, Brussels, Belgium.
AssociationforComputationalLinguistics.
Harry Bunt, Jan Alexandersson, Jean Carletta, Jae-Woong Choe, Alex Chengyu Fang, Koiti
Hasida, Kyong Lee, Volha Petukhova, Andrei Popescu-Belis, Laurent Romary, Claudia Soria,
and David Traum. 2010. Towards an iso standard for dialogue act annotation. In Proceedings
of LREC 2010, the Seventh International Conference on Language Resources and Evaluation,
Malta.
Carl Burke, Christy Doran, Abigail S Gertner, Andy Gregorowicz, Lisa Harper, Joel Korb, and
DanLoehr.2003. Dialoguecomplexitywithportability? researchdirectionsfortheinformation
state approach. In Proceedings of the HLT-NAACL 2003 workshop on Research directions in
dialogueprocessing,pages13–15.
Felix Burkhardt, Astrid Paeschke, Miriam Rolfes, Walter F Sendlmeier, Benjamin Weiss, et al.
2005. Adatabaseofgermanemotionalspeech. InInterspeech,volume5,pages1517–1520.
Michele Cafagna, Lina M. Rojas-Barahona, Kees van Deemter, and Albert Gatt. 2023. Interpret-
ing vision and language generative models with semantic visual priors. Frontiers in Artificial
Intelligence,6.
In˜igo Casanueva, Paweł Budzianowski, Pei-Hao Su, Nikola Mrksˇic´, Tsung-Hsien Wen, Stefan
Ultes, Lina Rojas-Barahona, Steve Young, and Milica Gasˇic´. 2017a. A benchmarking envi-
ronment for reinforcement learning based task oriented dialogue management. arXiv preprint
arXiv:1711.11023.
In˜igo Casanueva, Paweł Budzianowski, Pei-Hao Su, Nikola Mrksˇic´, Tsung Hsien Wen, Stefan
Ultes,LinaRojas-Barahona,SteveYoung,andMilicaGasˇic´.2017b. Abenchmarkingenviron-
ment for reinforcement learning based task oriented dialogue management. Deep Reinforce-
mentLearningSymposium,NIPS.
In˜igo Casanueva, Paweł Budzianowski, Pei-Hao Su, Nikola Mrksˇic´, Tsung-Hsien Wen, Ste-
fan Ultes, Lina Rojas-Barahona, Steve Young, and Milica Gasˇic´. 2017c. A Benchmark-
ing Environment for Reinforcement Learning Based Task Oriented Dialogue Management.
arXiv:1711.11023[cs,stat]. ArXiv: 1711.11023.
Ines Chami, Adva Wolf, Da-Cheng Juan, Frederic Sala, Sujith Ravi, and Christopher Re´. 2020.
Low-dimensionalhyperbolicknowledgegraphembeddings. InProceedingsofthe58thAnnual
Meeting of the Association for Computational Linguistics, pages 6901–6914, Online. Associa-
tionforComputationalLinguistics.
91Senthilkumar Chandramohan, Matthieu Geist, Fabrice Lefevre, and Olivier Pietquin. 2011. User
simulation in dialogue systems using inverse reinforcement learning. In Interspeech 2011,
pages1025–1028.
Chongyan Chen, Samreen Anjum, and Danna Gurari. 2022. Grounding answers for visual ques-
tionsaskedbyvisuallyimpairedpeople. InProceedingsoftheIEEE/CVFConferenceonCom-
puterVisionandPatternRecognition(CVPR),pages19098–19107.
Zhi Chen, Xiaoyuan Liu, Lu Chen, and Kai Yu. 2020. Structured hierarchical dialogue policy
withgraphneuralnetworks. arXivpreprintarXiv:2009.10355.
Adam Cheyer and David Martin. 2001. The open agent architecture. Autonomous Agents and
Multi-AgentSystems,4(1-2):143–148.
Kyunghyun Cho, Bart van Merrie¨nboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares,
Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using RNN
encoder–decoder for statistical machine translation. In Proceedings of the 2014 Conference
on Empirical Methods in Natural Language Processing (EMNLP), pages 1724–1734, Doha,
Qatar.AssociationforComputationalLinguistics.
E. Choi, H. He, M. Iyyer, M. Yatskar, W-T. Yih, Y. Choi, P. Liang, and L. Zettlemoyer. 2018a.
QuAC: Question Answering in Context. In Proceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing, pages 2174–2184, Brussels, Belgium. Association
forComputationalLinguistics.
EunsolChoi,HeHe,MohitIyyer,MarkYatskar,Wen-tauYih,YejinChoi,PercyLiang,andLuke
Zettlemoyer. 2018b. QuAC: Question answering in context. In Proceedings of EMNLP 2018,
pages2174–2184,Brussels,Belgium.
P. Christmann, R. Saha Roy, A. Abujabal, J. Singh, and G. Weikum. 2019. Look before you
Hop: Conversational Question Answering over Knowledge Graphs Using Judicious Context
Expansion. In Proceedings of the 28th ACM International Conference on Information and
Knowledge Management, CIKM ’19, pages 729–738, New York, NY, USA. Association for
ComputingMachinery.
G.Churcher.1997. Dialoguemanagementsystems: asurveyandoverview.
Herbert H Clark and Susan E Brennan. 1991. Grounding in communication. American Psycho-
logicalAssociation.
Ronan Collobert, Jason Weston, Le´on Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel
Kuksa.2011. Naturallanguageprocessing(almost)fromscratch. JournalofMachineLearning
Research,12(Aug):2493–2537.
Thibault Cordier, Tanguy Urvoy, Fabrice Lefe`vre, and Lina M. Rojas Barahona. 2022. Graph
neural network policies and imitation learning for multi-domain task-oriented dialogues. In
Proceedings of the 23rd Annual Meeting of the Special Interest Group on Discourse and Dia-
logue,pages91–100,Edinburgh,UK.AssociationforComputationalLinguistics.
92BIBLIOGRAPHY
Thibault Cordier, Tanguy Urvoy, Fabrice Lefevre, and Lina M Rojas-Barahona. 2023. Few-shot
structured policy learning for multi-domain and multi-task dialogues. The 17th Conference of
theEuropeanChapteroftheAssociationforComputationalLinguistics.FindingsEACL2023.
Thibault Cordier, Tanguy Urvoy, Lina M Rojas-Barahona, and Fabrice Lefe`vre. 2020. Diluted
near-optimal expert demonstrations for guiding dialogue stochastic policy optimisation. In
HumanintheloopdialoguesystemsWorkshopat34thConferenceonNeuralInformationPro-
cessingSystems.
HeribertoCuaya´huitl.2009. Hierarchicalreinforcementlearningforspokendialoguesystems.
Heriberto Cuaya´huitl, Simon Keizer, and Oliver Lemon. 2015. Strategic dialogue management
viadeepreinforcementlearning. arXivpreprintarXiv:1511.08099.
Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov.
2019. Transformer-xl: Attentivelanguagemodelsbeyondafixed-lengthcontext. arXivpreprint
arXiv:1901.02860.
LucieDaubigney,MatthieuGeist,andOlivierPietquin.2013. Model-freepomdpoptimisationof
tutoring systems with echo-state networks. In Proceedings of the SIGDIAL 2013 Conference,
pages102–106.
Anoop Deoras and Ruhi Sarikaya. 2013. Deep belief network based semantic taggers for spoken
languageunderstanding. InINTERSPEECH,pages2713–2717.
JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova.2018. Bert: Pre-trainingof
deepbidirectionaltransformersforlanguageunderstanding. arXivpreprintarXiv:1810.04805.
Abhinav Dhall, Roland Goecke, Simon Lucey, Tom Gedeon, et al. 2012. Collecting large, richly
annotatedfacial-expressiondatabasesfrommovies. IEEEmultimedia,19(3):34.
Hai Ha Do, PWC Prasad, Angelika Maag, and Abeer Alsadoon. 2019. Deep learning for aspect-
based sentiment analysis: a comparative review. Expert Systems with Applications, 118:272–
299.
Yan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel. 2016. Benchmarking
deepreinforcementlearningforcontinuouscontrol.
LaylaElAsri,RomainLaroche,andOlivierPietquin.2012.Rewardfunctionlearningfordialogue
management. InSTAIRS2012,pages95–106.IOSPress.
A. Elgohary, D. Peskov, and J. Boyd-Graber. 2019. Can You Unpack That? Learning to Rewrite
Questions-in-Context. In Proceedings of the 2019 Conference on Empirical Methods in Nat-
ural Language Processing and the 9th International Joint Conference on Natural Language
Processing(EMNLP-IJCNLP),pages5918–5924,HongKong,China.AssociationforCompu-
tationalLinguistics.
JulietteFaille,AlbertGatt,andClaireGardent.2021. Entity-basedsemanticadequacyfordata-to-
text generation. In Findings of the Association for Computational Linguistics: EMNLP 2021,
pages 1530–1540, Punta Cana, Dominican Republic. Association for Computational Linguis-
tics.
93Angela Fan, Mike Lewis, and Yann Dauphin. 2018. Hierarchical neural story generation. In
Proceedingsofthe56thAnnualMeetingoftheAssociationforComputationalLinguistics(Vol-
ume 1: Long Papers), pages 889–898, Melbourne, Australia. Association for Computational
Linguistics.
Jianfeng Gao, Michel Galley, and Lihong Li. 2018. Neural Approaches to Conversational AI.
arXiv:1809.08267[cs]. ArXiv: 1809.08267.
YangGao,HuazheXu,JiLin,FisherYu,SergeyLevine,andTrevorDarrell.2019. Reinforcement
LearningfromImperfectDemonstrations. arXiv:1802.05313[cs,stat]. ArXiv: 1802.05313.
Alberto Garc´ıa-Dura´n, Sebastijan Dumancˇic´, and Mathias Niepert. 2018. Learning sequence en-
coders for temporal knowledge graph completion. In Proceedings of the 2018 Conference on
Empirical Methods in Natural Language Processing, pages 4816–4821, Brussels, Belgium.
AssociationforComputationalLinguistics.
Claire Gardent and Lina Maria Rojas-Barahona. 2013. Using Paraphrases and Lexical Semantics
to Improve the Accuracy and the Robustness of Supervised Models in Situated Dialogue Sys-
tems. In Conference on Empirical Methods in Natural Language Processing, pages 808–813,
Seattle, United States. SIGDAT, the Association for Computational Linguistics special interest
grouponlinguisticdataandcorpus-basedapproachestoNLP.
Milica Gasˇic´ and Steve Young. 2013. Gaussian processes for pomdp-based dialogue manager
optimization. IEEE/ACMTransactionsonAudio,Speech,andLanguageProcessing,22(1):28–
40.
Marjan Ghazvininejad, Xing Shi, Jay Priyadarshi, and Kevin Knight. 2017. Hafez: an interactive
poetry generation system. In Proceedings of ACL 2017, System Demonstrations, pages 43–48,
Vancouver,Canada.AssociationforComputationalLinguistics.
Rishab Goel, Seyed Mehran Kazemi, Marcus Brubaker, and Pascal Poupart. 2020. Diachronic
embeddingfortemporalknowledgegraphcompletion. InAAAI.
Chih-Wen Goo and Yun-Nung Chen. 2018. Abstractive dialogue summarization with sentence-
gatedmodelingoptimizedbydialogueacts.
Aditya Grover, Jiaming Song, Ashish Kapoor, Kenneth Tran, Alekh Agarwal, Eric J Horvitz,
and Stefano Ermon. 2019. Bias correction of learned generative models using likelihood-free
importanceweighting. Advancesinneuralinformationprocessingsystems,32.
Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. 2017. Reinforcement Learn-
ingwithDeepEnergy-BasedPolicies. arXiv:1702.08165[cs]. ArXiv: 1702.08165.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. 2018. Soft Actor-
Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.
arXiv:1801.01290[cs,stat]. ArXiv: 1801.01290.
Philipp Hacker, Andreas Engel, and Marco Mauer. 2023. Regulating chatgpt and other large
generativeaimodels. InProceedingsofthe2023ACMConferenceonFairness,Accountability,
andTransparency,pages1112–1123.
94BIBLIOGRAPHY
Zhen Han, Peng Chen, Yunpu Ma, and Volker Tresp. 2020. DyERNIE: Dynamic Evolution of
Riemannian Manifold Embeddings for Temporal Knowledge Graph Completion. In Proceed-
ingsofthe2020ConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP),
pages7301–7316,Online.AssociationforComputationalLinguistics.
Thomas L. Heath and Euclid. 1956. The Thirteen Books of Euclid’s Elements, Books 1 and 2.
DoverPublications,Inc.,USA.
M. Henderson, B. Thomson, and S. J. Young. 2014a. Word-based Dialog State Tracking with
RecurrentNeuralNetworks. InProceedingsofSIGdial.
MatthewHenderson,MilicaGasˇic´,BlaiseThomson,PirrosTsiakoulis,KaiYu,andSteveYoung.
2012. Discriminative Spoken Language Understanding Using Word Confusion Networks. In
SpokenLanguageTechnologyWorkshop,2012.IEEE.
MatthewHenderson,BlaiseThomson,andJasonWilliams.2014b. Theseconddialogstatetrack-
ing challenge. In 15th Annual Meeting of the Special Interest Group on Discourse and Dia-
logue,volume263.
MatthewHenderson,BlaiseThomson,andJasonWilliams.2014c. Thethirddialogstatetracking
challenge. InProceedingsIEEESpokenLanguageTechnologyWorkshop(SLT).IEEEInstitute
ofElectricalandElectronicsEngineers.
ToddHester,MatejVecerik,OlivierPietquin,MarcLanctot,TomSchaul,BilalPiot,DanHorgan,
John Quan, Andrew Sendonaris, Gabriel Dulac-Arnold, Ian Osband, John Agapiou, Joel Z.
Leibo,andAudrunasGruslys.2017. DeepQ-learningfromDemonstrations. arXiv:1704.03732
[cs]. ArXiv: 1704.03732.
ToddHester,MatejVecerik,OlivierPietquin,MarcLanctot,TomSchaul,BilalPiot,DanHorgan,
JohnQuan,AndrewSendonaris,IanOsband,etal.2018. Deepq-learningfromdemonstrations.
InThirty-secondAAAIconferenceonartificialintelligence.
Sepp Hochreiter and Ju¨rgen Schmidhuber. 1997. Long short-term memory. Neural computation,
9(8):1735–1780.
Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2019. The curious case of
neuraltextdegeneration. arXivpreprintarXiv:1904.09751.
EdwardJHu,YelongShen,PhillipWallis,ZeyuanAllen-Zhu,YuanzhiLi,SheanWang,LuWang,
and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. arXiv preprint
arXiv:2106.09685.
Le´o Jacqmin, Lucas Druart, Valentin Vielzeuf, Lina Maria Rojas-Barahona, Yannick Este`ve, and
BenoˆıtFavre.2023. Olisia: acascadesystemforspokendialoguestatetracking.
Y. Ju, F. Zhao, S. Chen, B. Zheng, X. Yang, and Y. Liu. 2019. Technical report on conversational
questionanswering. arXivpreprintarXiv:1909.10772.
95Nitish Shirish Keskar, Bryan McCann, Lav R Varshney, Caiming Xiong, and Richard Socher.
2019. Ctrl: A conditional transformer language model for controllable generation. arXiv
preprintarXiv:1909.05858.
Yoon Kim. 2014. Convolutional neural networks for sentence classification. arXiv preprint
arXiv:1408.5882.
S.LarssonandD.Traum.2000. Informationstateanddialoguemanagementinthetrindidialogue
moveenginetoolkit. NaturalLanguageEngineering,6:323–340.
MichaelLaskin,LuyuWang,JunhyukOh,EmilioParisotto,StephenSpencer,RichieSteigerwald,
DJStrouse,StevenHansen,AngelosFilos,EthanBrooks,etal.2022. In-contextreinforcement
learningwithalgorithmdistillation. arXivpreprintarXiv:2210.14215.
Chih-Wei Lee, Yau-Shian Wang, Tsung-Yuan Hsu, Kuan-Yu Chen, Hung-yi Lee, and Lin-shan
Lee. 2018. Scalable sentiment for sequence-to-sequence chatbot response with performance
analysis. In 2018 IEEE International Conference on Acoustics, Speech and Signal Processing
(ICASSP),pages6164–6168.IEEE.
Kyusong Lee, Tiancheng Zhao, Yulun Du, Edward Cai, Allen Lu, Eli Pincus, David Traum, Ste-
fan Ultes, Lina M. Rojas-Barahona, Milica Gasic, Steve Young, and Maxine Eskenazi. 2017.
DialPort, gone live: An update after a year of development. In Proceedings of the 18th Annual
SIGdial Meeting on Discourse and Dialogue, pages 170–173, Saarbru¨cken, Germany. Associ-
ationforComputationalLinguistics.
KyusongLee,TianchengZhao,StefanUltes,LinaRojas-Barahona,EliPincus,DavidTraum,and
MaxineEskenazi.2019. Anassessmentframeworkfordialport. InAdvancedSocialInteraction
withAgents: 8thInternationalWorkshoponSpokenDialogSystems,pages79–85.Springer.
BrianLester,RamiAl-Rfou,andNoahConstant.2021. Thepowerofscaleforparameter-efficient
prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Lan-
guageProcessing,pages3045–3059,OnlineandPuntaCana,DominicanRepublic.Association
forComputationalLinguistics.
Esther Levin, Roberto Pieraccini, and Wieland Eckert. 2000. A stochastic model of human-
machine interaction for learning dialog strategies. IEEE Transactions on speech and audio
processing,8(1):11–23.
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer
Levy, Ves Stoyanov, and Luke Zettlemoyer. 2019. Bart: Denoising sequence-to-sequence
pre-training for natural language generation, translation, and comprehension. arXiv preprint
arXiv:1910.13461.
Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning: Optimizing continuous prompts for genera-
tion. InProceedingsofthe59thAnnualMeetingoftheAssociationforComputationalLinguis-
tics and the 11th International Joint Conference on Natural Language Processing (Volume 1:
LongPapers),pages4582–4597,Online.AssociationforComputationalLinguistics.
YuxiLi.2018. DeepReinforcementLearning. arXiv:1810.06339[cs,stat]. ArXiv: 1810.06339.
96BIBLIOGRAPHY
Diane Litman, Satinder Singh, Michael Kearns, and Marilyn Walker. 2000. NJFun- a reinforce-
ment learning spoken dialogue system. In ANLP-NAACL 2000 Workshop: Conversational
Systems.
HaoLiu,CarmeloSferrazza,andPieterAbbeel.2023. Chainofhindsightalignslanguagemodels
withfeedback.
Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and
V. Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint
arXiv:1907.11692.
Alejandra Lorenzo, Lina Rojas-Barahona, and Christophe Cerisara. 2013. Unsupervised struc-
tured semantic inference for spoken dialog reservation tasks. In Proceedings of the SIGDIAL
2013Conference,pages12–20,Metz,France.AssociationforComputationalLinguistics.
Ryan Lowe, Nissan Pow, Iulian Serban, and Joelle Pineau. 2015. The ubuntu dialogue cor-
pus: A large dataset for research in unstructured multi-turn dialogue systems. arXiv preprint
arXiv:1506.08909.
YukunMa,KhanhLinhNguyen,FrankZXing,andErikCambria.2020. Asurveyonempathetic
dialoguesystems. InformationFusion.
YuningMao,PengchengHe,XiaodongLiu,YelongShen,JianfengGao,JiaweiHan,andWeizhu
Chen. 2021. Reader-guided passage reranking for open-domain question answering. In Find-
ings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 344–350,
Online.AssociationforComputationalLinguistics.
GabrielMarzinotto,JeremyAuguste,FredericBechet,Ge´raldineDamnati,andAlexisNasr.2018.
Semantic frame parsing for information extraction : the calor corpus. In Proceedings of the
Eleventh International Conference on Language Resources and Evaluation (LREC-2018). Eu-
ropeanLanguageResourceAssociation.
Andrew Kachites McCallum. 2002. Mallet: A machine learning for language toolkit.
Http://mallet.cs.umass.edu.
MichaelF.McTear.2002.Spokendialoguetechnology: enablingtheconversationaluserinterface.
ACMComput.Surv.,34(1):90–169.
Gre´goire Mesnil, Yann Dauphin, Kaisheng Yao, Yoshua Bengio, Li Deng, Dilek Hakkani-Tur,
XiaodongHe,LarryHeck,GokhanTur,DongYu,etal.2015. Usingrecurrentneuralnetworks
for slot filling in spoken language understanding. IEEE/ACM Transactions on Audio, Speech,
andLanguageProcessing,23(3):530–539.
Bernard Michini and Jonathan P How. 2012. Improving the efficiency of bayesian inverse re-
inforcement learning. In 2012 IEEE International Conference on Robotics and Automation,
pages3651–3656.IEEE.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan
Wierstra, and Martin Riedmiller. 2013. Playing atari with deep reinforcement learning. arXiv
preprintarXiv:1312.5602.
97Sebastien Montella, Lina M Rojas Barahona, Frederic Bechet, Johannes Heinecke, and Alexis
Nasr.2022. Transferlearningandmaskedgenerationforanswerverbalization. InProceedings
oftheWorkshoponStructuredandUnstructuredKnowledgeIntegration(SUKI),pages47–54.
Sebastien Montella, Alexis Nasr, Johannes Heinecke, Frederic Bechet, and Lina M Rojas-
Barahona. 2023. Investigating the effect of relative positional embeddings on amr-to-text gen-
erationwithstructuraladapters. arXivpreprintarXiv:2302.05900.
SebastienMontella,LinaM.RojasBarahona,andJohannesHeinecke.2021. Hyperbolictemporal
knowledge graph embeddings with relational and time curvatures. In Findings of the Associa-
tionforComputationalLinguistics: ACL-IJCNLP2021,pages3296–3308,Online.Association
forComputationalLinguistics.
RR Morris, Schueller SM, and Picard RW. 2015. Efficacy of a Web-Based, Crowdsourced Peer-
To-Peer Cognitive Reappraisal Platform for Depression: Randomized Controlled Trial. J Med
InternetRes,17(3).
Nikola Mrksˇic´, Diarmuid O´ Se´aghdha, Tsung-Hsien Wen, Blaise Thomson, and Steve Young.
2017. Neural belief tracker: Data-driven dialogue state tracking. In Proceedings of the 55th
Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),
pages1777–1788,Vancouver,Canada.AssociationforComputationalLinguistics.
Re´mi Munos, Tom Stepleton, Anna Harutyunyan, and Marc G. Bellemare. 2016. Safe and Effi-
cientOff-PolicyReinforcementLearning. arXiv:1606.02647[cs,stat]. ArXiv: 1606.02647.
Shashi Narayan, Gonc¸alo Simo˜es, Yao Zhao, Joshua Maynez, Dipanjan Das, Michael Collins,
and Mirella Lapata. 2022. A well-composed text is half done! composition sampling for di-
verse conditional generation. In Proceedings of the 60th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers), pages 1319–1339, Dublin, Ireland.
AssociationforComputationalLinguistics.
Andrew Y Ng, Stuart J Russell, et al. 2000. Algorithms for inverse reinforcement learning. In
ICML,volume1,page2.
Jinjie Ni, Tom Young, Vlad Pandelea, Fuzhao Xue, and Erik Cambria. 2022. Recent advances
in deep learning based dialogue systems: A systematic survey. Artificial Intelligence Review,
pages1–101.
Feng Nie, Jin-Ge Yao, Jinpeng Wang, Rong Pan, and Chin-Yew Lin. 2019. A simple recipe
towardsreducinghallucinationinneuralsurfacerealisation. InProceedingsofthe57thAnnual
MeetingoftheAssociationforComputationalLinguistics,pages2673–2679.
LongOuyang,JeffWu,XuJiang,DiogoAlmeida,CarrollLWainwright,PamelaMishkin,Chong
Zhang,SandhiniAgarwal,KatarinaSlama,AlexRay,etal.2022. Traininglanguagemodelsto
followinstructionswithhumanfeedback. arXivpreprintarXiv:2203.02155.
Tim Paek and Roberto Pieraccini. 2008. Automating spoken dialogue management design using
machinelearning: Anindustryperspective. Speechcommunication,50(8-9):716–729.
98BIBLIOGRAPHY
Ronald Parr and Stuart J Russell. 1998. Reinforcement learning with hierarchies of machines. In
Advancesinneuralinformationprocessingsystems,pages1043–1049.
Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli,
Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. 2023. The re-
finedweb dataset for falcon llm: Outperforming curated corpora with web data, and web data
only.
Leonardo Pepino, Pablo Riera, and Luciana Ferrer. 2021. Emotion Recognition from Speech
Usingwav2vec2.0Embeddings. InProc.Interspeech2021,pages3400–3404.
RobertoPieraccini,EvelyneTzoukermann,ZakharGorelov,J-LGauvain,EstherLevin,C-HLee,
and Jay G Wilpon. 1992. A speech understanding system based on statistical representation
of semantics. In Acoustics, Speech, and Signal Processing, 1992. ICASSP-92., 1992 IEEE
InternationalConferenceon,volume1,pages193–196.IEEE.
Matt Post. 2018. A call for clarity in reporting BLEU scores. In Proceedings of the Third Con-
ferenceonMachineTranslation: ResearchPapers,pages186–191,Brussels,Belgium.Associ-
ationforComputationalLinguistics.
J. Quan, D. Xiong, B. Webber, and C. Hu. 2019. GECOR: An End-to-End Generative Ellipsis
and Co-reference Resolution Model for Task-Oriented Dialogue. In Proceedings of the 2019
Conference on Empirical Methods in Natural Language Processing and the 9th International
JointConferenceonNaturalLanguageProcessing(EMNLP-IJCNLP),pages4547–4557,Hong
Kong,China.AssociationforComputationalLinguistics.
Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya
Sutskever. 2022. Robust speech recognition via large-scale weak supervision. arXiv preprint
arXiv:2212.04356.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,
Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a
unifiedtext-to-texttransformer. TheJournalofMachineLearningResearch,21(1):5485–5551.
Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know What You Don’t Know: Unanswer-
able Questions for SQuAD. In Proceedings of the 56th Annual Meeting of the Association for
Computational Linguistics (Volume 2: Short Papers), pages 784–789, Melbourne, Australia.
AssociationforComputationalLinguistics.
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+
questions for machine comprehension of text. In Proceedings of the 2016 Conference on Em-
piricalMethodsinNaturalLanguageProcessing,pages2383–2392.
DeepakRamachandranandEyalAmir.2007. Bayesianinversereinforcementlearning. InIJCAI,
volume7,pages2586–2591.
Antoine Raux, Brian Langner, Dan Bohus, Alan W Black, and Maxine Eskenazi. 2005. Let’s
go public! taking a spoken dialog system to the real world. In Ninth European conference on
speechcommunicationandtechnology.
99S. Reddy, D. Chen, and C. D. Manning. 2019. CoQA: A Conversational Question Answering
Challenge. TransactionsoftheAssociationforComputationalLinguistics,7:249–266.
Siva Reddy, Danqi Chen, and Christopher D Manning. 2018. Coqa: A conversational question
answeringchallenge. arXivpreprintarXiv:1808.07042.
VerenaRieserandOliverLemon.2011. Reinforcementlearningforadaptivedialoguesystems: a
data-drivenmethodologyfordialoguemanagementandnaturallanguagegeneration. Springer
Science&BusinessMedia.
Lina M Rojas-Barahona. 2020. Is the user enjoying the conversation? a case study on the impact
on the reward function. 34st Conference on Neural Information Processing Systems (NeurIPS
2020).
Lina M Rojas-Barahona, Pascal Bellec, Benoit Besset, Martinho Dossantos, Johannes Heinecke,
MunshiAsadullah,OlivierLeblouch,JeanyvesLancien,Ge´raldineDamnati,EmmanuelMory,
et al. 2019. Spoken conversational search for general knowledge. In Proceedings of the 20th
AnnualSIGdialMeetingonDiscourseandDialogue,pages110–113.
Lina M Rojas-Barahona and Christophe Cerisara. 2014. Bayesian inverse reinforcement learning
for modeling conversational agents in a virtual environment. In International Conference on
IntelligentTextProcessingandComputationalLinguistics,pages503–514.Springer.
Lina M. Rojas Barahona, Milica Gasˇic´, Nikola Mrksˇic´, Pei-Hao Su, Stefan Ultes, Tsung Hsien
Wen, and Steve Young. 2016. Exploiting sentence and context representations in deep neural
models for spoken language understanding. In Proceedings of COLING 2016, the 26th Inter-
national Conference on Computational Linguistics: Technical Papers, pages 258–267, Osaka,
Japan.
Lina M. Rojas-Barahona, Alejandra Lorenzo, and Claire Gardent. 2012a. Building and Exploit-
ing a Corpus of Dialog Interactions between French Speaking Virtual and Human Agents. In
Proceedingsofthe8thInternationalConferenceonLanguageResourcesandEvaluation.
Lina M Rojas-Barahona and Matthieu Quignard. 2011. An incremental architecture for the se-
mantic annotation of dialogue corpora with high-level structures. a case of study for the media
corpus. InProceedingsoftheSIGDIAL2011Conference,pages332–334.
Lina M. Rojas-Barahona, Bo-Hsiang Tseng, Yinpei Dai, Clare Mansfield, Osman Ramadan, Ste-
fan Ultes, Michael Crawford, and Milica Gasˇic´. 2018. Deep learning for language understand-
ingofmentalhealthconceptsderivedfromcognitivebehaviouraltherapy. InProceedingsofthe
Ninth International Workshop on Health Text Mining and Information Analysis, pages 44–54,
Brussels,Belgium.AssociationforComputationalLinguistics.
LinaMariaRojas-Barahona.2016. Deeplearningforsentimentanalysis. LanguageandLinguis-
ticsCompass,10(12):701–719.
Lina Maria Rojas-Barahona, Thierry Bazillon, Matthieu Quignard, and Fabrice Lefevre. 2011.
Using mmil for the high level semantic annotation of the french media dialogue corpus. In
NinthInternationalConferenceonComputationalSemantics-IWCS2011.
100BIBLIOGRAPHY
Lina Maria Rojas Barahona and Christophe Cerisara. 2015. Weakly supervised discriminative
trainingoflinearmodelsforNaturalLanguageProcessing. In3rdInternationalConferenceon
StatisticalLanguageandSpeechProcessing(SLSP),Budapest,Hungary.
LinaMariaRojas-BarahonaandClaireGardent.2012. Whatshouldidonow? supportingconver-
sations in a serious game. In SeineDial 2012-16th Workshop on the Semantics and Pragmatics
ofDialogue.
Lina Maria Rojas-Barahona and Toni Giorgino. 2009. Adaptable dialog architecture and runtime
engine (adarte): a framework for rapid prototyping of health dialog systems. international
journalofmedicalinformatics,78:S56–S68.
Lina Maria Rojas-Barahona, Alejandra Lorenzo, and Claire Gardent. 2012b. Building and ex-
ploiting a corpus of dialog interactions between french speaking virtual and human agents. In
The eighth international conference on Language Resources and Evaluation (LREC), pages
1428–1435.
LinaMariaRojas-Barahona,AlejandraLorenzo,andClaireGardent.2012c. Anend-to-endeval-
uationoftwosituateddialogsystems. InProceedingsofthe13thAnnualMeetingoftheSpecial
InterestGrouponDiscourseandDialogue,pages10–19.
Ste´phaneRoss,GeoffreyJGordon,andJAndrewBagnell.2011. AReductionofImitationLearn-
ing and Structured Prediction to No-Regret Online Learning. Proceedings of the Workshop on
ArtificialIntelligenceandStatistics,AISTATS,page9.
Nicholas Roy, Joelle Pineau, and Sebastian Thrun. 2000. Spoken dialogue management using
probabilistic reasoning. In Proceedings of the 38th annual meeting of the association for com-
putationallinguistics,pages93–100.
BenoˆıtSagot.2010. TheLefff,afreelyavailableandlarge-coveragemorphologicalandsyntactic
lexicon for French. In 7th international conference on Language Resources and Evaluation
(LREC2010),Valletta,Malta.
Amrita Saha, Vardaan Pahuja, Mitesh M Khapra, Karthik Sankaranarayanan, and Sarath Chan-
dar. 2018. Complex sequential question answering: Towards learning to converse over linked
questionanswerpairswithaknowledgegraph. arXivpreprintarXiv:1801.10314.
V. Sanh, L. Debut, J. Chaumond, and T. Wolf. 2020. DistilBERT, a distilled version of BERT:
smaller,faster,cheaperandlighter. arXiv:1910.01108[cs]. ArXiv: 1910.01108.
Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. Distilbert, a distilled
versionofbert: smaller,faster,cheaperandlighter. arXivpreprintarXiv:1910.01108.
Chinnadhurai Sankar and Sujith Ravi. 2019. Deep reinforcement learning for modeling chit-chat
dialog with discrete attributes. Proceedings of the 20th Annual SIGdial Meeting on Discourse
andDialogue.
Bishal Santra, Sumegh Roychowdhury, Aishik Mandal, Vasu Gurram, Atharva Naik, Manish
Gupta,andPawanGoyal.2021. Representationlearningforconversationaldatausingdiscourse
mutualinformationmaximization. arXivpreprintarXiv:2112.05787.
101R.Sarikaya,G.E.Hinton,andB.Ramabhadran.2011. Deepbeliefnetsfornaturallanguagecall-
routing. In 2011 IEEE International Conference on Acoustics, Speech and Signal Processing
(ICASSP),pages5680–5683.
RuhiSarikaya,GeoffreyEHinton,andAnoopDeoras.2014. Applicationofdeepbeliefnetworks
fornaturallanguageunderstanding. IEEE/ACMTransactionsonAudio,Speech,andLanguage
Processing,22(4):778–784.
Jost Schatzmann, Blaise Thomson, Karl Weilhammer, Hui Ye, and Steve Young. 2007. Agenda-
basedusersimulationforbootstrappingaPOMDPdialoguesystem. InHumanLanguageTech-
nologies 2007: The Conference of the North American Chapter of the Association for Compu-
tational Linguistics; Companion Volume, Short Papers on XX - NAACL ’07, pages 149–152,
Rochester,NewYork.AssociationforComputationalLinguistics.
Jost Schatzmann, Karl Weilhammer, Matt Stuttle, and Steve Young. 2006. A survey of statisti-
cal user simulation techniques for reinforcement-learning of dialogue management strategies.
KnowledgeEngineeringReview,21(2):97–126.
Neil Scheidwasser-Clow, Mikolaj Kegler, Pierre Beckmann, and Milos Cernak. 2022. Serab: A
multi-lingual benchmark for speech emotion recognition. In ICASSP 2022-2022 IEEE Inter-
nationalConferenceonAcoustics,SpeechandSignalProcessing(ICASSP),pages7697–7701.
IEEE.
AlexanderSchmitt,BenjaminSchatz,andWolfgangMinker.2011. Modelingandpredictingqual-
ity in spoken human-computer interaction. In Proceedings of the SIGDIAL 2011 Conference,
pages173–184.AssociationforComputationalLinguistics.
Alexander Schmitt, Stefan Ultes, and Wolfgang Minker. 2012. A parameterized and annotated
spoken dialog corpus of the CMU let’s go bus information system. In Proceedings of the
Eighth International Conference on Language Resources and Evaluation (LREC’12), pages
3369–3373,Istanbul,Turkey.EuropeanLanguageResourcesAssociation(ELRA).
MarcSchro¨der,LaurenceDevillers,KostasKarpouzis,Jean-ClaudeMartin,CatherinePelachaud,
ChristianPeter,HannesPirker,Bjo¨rnSchuller,JianhuaTao,andIanWilson.2007. Whatshould
agenericemotionmarkuplanguagebeabletorepresent? InAffectiveComputingandIntelligent
Interaction: Second International Conference, ACII 2007 Lisbon, Portugal, September 12-14,
2007Proceedings2,pages440–451.Springer.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. 2015. Trust
region policy optimization. In Proceedings of the 32nd International Conference on Machine
Learning, volume 37 of Proceedings of Machine Learning Research, pages 1889–1897, Lille,
France.PMLR.
Min Joon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. 2017. Bidirectional
attention flow for machine comprehension. In 5th International Conference on Learning Rep-
resentations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings.
OpenReview.net.
102BIBLIOGRAPHY
Iulian Serban, Alessandro Sordoni,Ryan Lowe, LaurentCharlin, Joelle Pineau,Aaron Courville,
andYoshuaBengio.2017. Ahierarchicallatentvariableencoder-decodermodelforgenerating
dialogues. InProceedingsoftheAAAIConferenceonArtificialIntelligence,volume31.
Iulian V Serban, Alessandro Sordoni, Yoshua Bengio, Aaron Courville, and Joelle Pineau. 2016.
Building end-to-end dialogue systems using generative hierarchical neural network models. In
ThirtiethAAAIConferenceonArtificialIntelligence.
KurtShuster,JingXu,MojtabaKomeili,DaJu,EricMichaelSmith,StephenRoller,MeganUng,
Moya Chen, Kushal Arora, Joshua Lane, et al. 2022. Blenderbot 3: a deployed conversational
agentthatcontinuallylearnstoresponsiblyengage. arXivpreprintarXiv:2208.03188.
Alessandro Sordoni, Yoshua Bengio, Hossein Vahabi, Christina Lioma, Jakob Grue Simonsen,
andJian-YunNie.2015. Ahierarchicalrecurrentencoder-decoderforgenerativecontext-aware
querysuggestion. InProceedingsofthe24thACMInternationalonConferenceonInformation
andKnowledgeManagement,pages553–562.ACM.
YashSrivastava,VaishnavMurali,ShivRamDubey,andSnehasisMukherjee.2021. Visualques-
tion answering using deep learning: A survey and performance analysis. In Computer Vision
andImageProcessing: 5thInternationalConference,CVIP2020,Prayagraj,India,December
4-6,2020,RevisedSelectedPapers,PartII5,pages75–86.Springer.
Pei-Hao Su, Paweł Budzianowski, Stefan Ultes, Milica Gasˇic´, and Steve Young. 2017. Sample-
efficient actor-critic reinforcement learning with supervised data for dialogue management. In
Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue, pages 147–157,
Saarbru¨cken,Germany.AssociationforComputationalLinguistics.
Pei-HaoSu,MilicaGasˇic´,NikolaMrksˇic´,LinaM.Rojas-Barahona,StefanUltes,DavidVandyke,
Tsung-HsienWen,andSteveYoung.2016. On-lineactiverewardlearningforpolicyoptimisa-
tioninspokendialoguesystems. InProceedingsofthe54thAnnualMeetingoftheAssociation
for Computational Linguistics (Volume 1: Long Papers), pages 2431–2441, Berlin, Germany.
AssociationforComputationalLinguistics.
Richard S Sutton and Andrew G Barto. 2018. Reinforcement learning: An introduction. MIT
press.
Richard S Sutton, Doina Precup, and Satinder Singh. 1999. Between mdps and semi-mdps: A
framework for temporal abstraction in reinforcement learning. Artificial intelligence, 112(1-
2):181–211.
Da Tang, Xiujun Li, Jianfeng Gao, Chong Wang, Lihong Li, and Tony Jebara. 2018. Subgoal
discovery for hierarchical dialogue policy learning. In Proceedings of the 2018 Conference
on Empirical Methods in Natural Language Processing, pages 2298–2309, Brussels, Belgium.
AssociationforComputationalLinguistics.
Blaise Thomson, Kai Yu, Milica Gasic, Simon Keizer, Francois Mairesse, Jost Schatzmann, and
SteveJYoung.2008. Evaluatingsemantic-levelconfidencescoreswithmultiplehypotheses. In
INTERSPEECH,pages1153–1156.
103James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. 2018. FEVER:
a large-scale dataset for fact extraction and VERification. In Proceedings of the 2018 Confer-
enceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics: Human
Language Technologies, Volume 1 (Long Papers), pages 809–819, New Orleans, Louisiana.
AssociationforComputationalLinguistics.
Tijmen Tieleman and Geoffrey Hinton. 2012. Lecture 6.5-rmsprop: Divide the gradient by a
running average of its recent magnitude. COURSERA: Neural networks for machine learning,
4(2):26–31.
HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,Timothe´e
Lacroix,BaptisteRozie`re,NamanGoyal,EricHambro,FaisalAzhar,etal.2023. Llama: Open
andefficientfoundationlanguagemodels. arXivpreprintarXiv:2302.13971.
David R. Traum and Staffan Larsson. 2003. The information state approach to dialogue manage-
ment. InJanKuppevelt,RonnieW.Smith,andNancyIde,editors,CurrentandNewDirections
in Discourse and Dialogue, volume 22 of Text, Speech and Language Technology, pages 325–
353.SpringerNetherlands. 10.1007/978-94-010-0019-2 15.
George Trigeorgis, Fabien Ringeval, Raymond Brueckner, Erik Marchi, Mihalis A Nicolaou,
Bjo¨rn Schuller, and Stefanos Zafeiriou. 2016. Adieu features? end-to-end speech emotion
recognition using a deep convolutional recurrent network. In 2016 IEEE international confer-
enceonacoustics,speechandsignalprocessing(ICASSP),pages5200–5204.IEEE.
Pirros Tsiakoulis, Milica Gasˇic, Matthew Henderson, Joaquin Planells-Lerma, Jorge Prombonas,
Blaise Thomson, Kai Yu, Steve Young, and Eli Tzirkel. 2012. Statistical methods for building
robustspokendialoguesystemsinanautomobile. Proceedingsofthe4thappliedhumanfactors
andergonomics.
Go¨khan Tu¨r, Anoop Deoras, and Dilek Hakkani-Tu¨r. 2013. Semantic parsing using word confu-
sionnetworkswithconditionalrandomfields. InINTERSPEECH,pages2579–2583.
S. Ultes, L. M. Rojas-Barahona, P-H. Su, D. Vandyke, D. Kim, I. Casanueva, P. Budzianowski,
N. Mrksˇic´, T-H. Wen, M. Gasˇic´, and S. Young. 2017a. PyDial: A Multi-domain Statistical
Dialogue System Toolkit. In Proceedings of ACL 2017, System Demonstrations, pages 73–78,
Vancouver,Canada.AssociationforComputationalLinguistics.
Stefan Ultes. 2019. Improving interaction quality estimation with BiLSTMs and the impact on
dialoguepolicylearning. InProceedingsofthe20thAnnualSIGdialMeetingonDiscourseand
Dialogue,pages11–20,Stockholm,Sweden.AssociationforComputationalLinguistics.
Stefan Ultes, Matthias Kraus, Alexander Schmitt, and Wolfgang Minker. 2015. Quality-adaptive
spoken dialogue initiative selection and implications on reward modelling. In Proceedings of
the16thAnnualMeetingoftheSpecialInterestGrouponDiscourseandDialogue,pages374–
383,Prague,CzechRepublic.AssociationforComputationalLinguistics.
Stefan Ultes, Lina M. Rojas Barahona, Pei-Hao Su, David Vandyke, Dongho Kim, In˜igo
Casanueva, Paweł Budzianowski, Nikola Mrksˇic´, Tsung-Hsien Wen, Milica Gasic, and Steve
Young. 2017b. PyDial: A Multi-domain Statistical Dialogue System Toolkit. In Proceedings
104BIBLIOGRAPHY
ofACL2017,SystemDemonstrations,pages73–78,Vancouver,Canada.AssociationforCom-
putationalLinguistics.
Stefan Ultes, Lina M. Rojas Barahona, Pei-Hao Su, David Vandyke, Dongho Kim, In˜igo
Casanueva, Paweł Budzianowski, Nikola Mrksˇic´, Tsung-Hsien Wen, Milica Gasic, and Steve
Young. 2017c. PyDial: A Multi-domain Statistical Dialogue System Toolkit. In Proceedings
ofACL2017,SystemDemonstrations,pages73–78,Vancouver,Canada.AssociationforCom-
putationalLinguistics.
S. Vakulenko, S. Longpre, Z. Tu, and R. Anantha. 2021. Question rewriting for conversational
question answering. In Proceedings of the 14th ACM International Conference on Web Search
andDataMining,pages355–363.
Hado van Hasselt, Arthur Guez, and David Silver. 2015. Deep Reinforcement Learning with
DoubleQ-learning. arXiv:1509.06461[cs]. ArXiv: 1509.06461.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in neural
informationprocessingsystems,pages5998–6008.
Oriol Vinyals and Quoc Le. 2015. A neural conversational model. arXiv preprint
arXiv:1506.05869.
PiekVossen,editor.1998. EuroWordNet: amultilingualdatabasewithlexicalsemanticnetworks.
KluwerAcademicPublishers,Norwell,MA,USA.
Marilyn A. Walker, Diane J. Litman, Candace A. Kamm, and Alicia Abella. 1997a. PARADISE:
Aframeworkforevaluatingspokendialogueagents. In35thAnnualMeetingoftheAssociation
for Computational Linguistics and 8th Conference of the European Chapter of the Association
for Computational Linguistics, pages 271–280, Madrid, Spain. Association for Computational
Linguistics.
Marilyn A Walker, Diane J Litman, Candace A Kamm, and Alicia Abella. 1997b. Paradise: A
frameworkforevaluatingspokendialogueagents. arXivpreprintcmp-lg/9704004.
Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou,
Jingren Zhou, and Hongxia Yang. 2022. Ofa: Unifying architectures, tasks, and modalities
through a simple sequence-to-sequence learning framework. In International Conference on
MachineLearning,pages23318–23340.PMLR.
Zhuoran Wang, Tsung-Hsien Wen, Pei-Hao Su, and Yannis Stylianou. 2015. Learning domain-
independentdialoguepoliciesviaontologyparameterisation. InProceedingsofthe16thAnnual
MeetingoftheSpecialInterestGrouponDiscourseandDialogue,pages412–416.
Ziyu Wang, Victor Bapst, Nicolas Heess, Volodymyr Mnih, Remi Munos, Koray Kavukcuoglu,
and Nando de Freitas. 2016a. Sample efficient actor-critic with experience replay. arXiv
preprintarXiv:1611.01224.
105Ziyu Wang, Victor Bapst, Nicolas Heess, Volodymyr Mnih, Remi Munos, Koray Kavukcuoglu,
and Nando de Freitas. 2017. Sample Efficient Actor-Critic with Experience Replay.
arXiv:1611.01224[cs]. ArXiv: 1611.01224.
Ziyu Wang, Tom Schaul, Matteo Hessel, Hado van Hasselt, Marc Lanctot, and Nando de Fre-
itas. 2016b. Dueling Network Architectures for Deep Reinforcement Learning. International
conferenceonmachinelearning,pages1995–2003.
JasonWei,XuezhiWang,DaleSchuurmans,MaartenBosma,FeiXia,EdChi,QuocVLe,Denny
Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models.
AdvancesinNeuralInformationProcessingSystems,35:24824–24837.
Gelle´rtWeisz,PawełBudzianowski,Pei-HaoSu,andMilicaGasˇic´.2018a. Sampleefficientdeep
reinforcementlearningfordialoguesystemswithlargeactionspaces. IEEE/ACMTransactions
onAudio,Speech,andLanguageProcessing,26(11):2083–2097.
Gelle´rtWeisz,PawełBudzianowski,Pei-HaoSu,andMilicaGasˇic´.2018b. SampleEfficientDeep
Reinforcement Learning for Dialogue Systems with Large Action Spaces. arXiv:1802.03753
[cs,stat]. ArXiv: 1802.03753.
JosephWeizenbaum.1966. Eliza: Acomputerprogramforthestudyofnaturallanguagecommu-
nicationbetweenmanandmachine. CommunciationsoftheACM,9(1).
Tsung-Hsien Wen, Milica Gasic, Nikola Mrksic, Pei-Hao Su, David Vandyke, and Steve Young.
2015. Semantically conditioned lstm-based natural language generation for spoken dialogue
systems. arXivpreprintarXiv:1508.01745.
Tsung-HsienWen,YishuMiao,PhilBlunsom,andSteveYoung.2017a. Latentintentiondialogue
models. InInternationalConferenceonMachineLearning,pages3732–3741.PMLR.
Tsung-Hsien Wen, David Vandyke, Nikola Mrksˇic´, Milica Gasˇic´, Lina M. Rojas-Barahona, Pei-
Hao Su, Stefan Ultes, and Steve Young. 2017b. A network-based end-to-end trainable task-
oriented dialogue system. In Proceedings of the 15th Conference of the European Chapter
of the Association for Computational Linguistics: Volume 1, Long Papers, pages 438–449,
Valencia,Spain.AssociationforComputationalLinguistics.
Jason Weston, Sumit Chopra, and Antoine Bordes. 2014. Memory networks. CoRR,
abs/1410.3916.
JasonDWilliams.2014. Web-stylerankingandslucombinationfordialogstatetracking. InPro-
ceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue
(SIGDIAL),pages282–291.
Jason D Williams and Steve Young. 2007. Partially observable markov decision processes for
spokendialogsystems. ComputerSpeech&Language,21(2):393–422.
Yu Wu, Zhoujun Li, Wei Wu, and Ming Zhou. 2018. Response selection with topic clues for
retrieval-basedchatbots. Neurocomputing,316:251–261.
106BIBLIOGRAPHY
ChengjinXu,MojtabaNayyeri,FouadAlkhoury,HamedShariatYazdi,andJensLehmann.2020.
TeRo: A time-aware knowledge graph embedding via temporal rotation. In Proceedings of
the28thInternationalConferenceonComputationalLinguistics,pages1583–1593,Barcelona,
Spain(Online).InternationalCommitteeonComputationalLinguistics.
Jing Xu, Arthur Szlam, and Jason Weston. 2022. Beyond goldfish memory: Long-term open-
domainconversation. InProceedingsofthe60thAnnualMeetingoftheAssociationforCompu-
tational Linguistics (Volume 1: Long Papers), pages 5180–5197, Dublin, Ireland. Association
forComputationalLinguistics.
Kaisheng Yao, Geoffrey Zweig, Mei-Yuh Hwang, Yangyang Shi, and Dong Yu. 2013. Recurrent
neuralnetworksforlanguageunderstanding. InINTERSPEECH,pages2524–2528.
Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik
Narasimhan. 2023. Tree of thoughts: Deliberate problem solving with large language models.
arXivpreprintarXiv:2305.10601.
Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan
Cao. 2022. React: Synergizing reasoning and acting in language models. arXiv preprint
arXiv:2210.03629.
MichihiroYasunaga,ArmenAghajanyan,WeijiaShi,RichardJames,JureLeskovec,PercyLiang,
Mike Lewis, Luke Zettlemoyer, and Wen-Tau Yih. 2023. Retrieval-augmented multimodal
languagemodeling. InProceedingsofthe40thInternationalConferenceonMachineLearning,
volume202ofProceedingsofMachineLearningResearch,pages39755–39769.PMLR.
SteveYoung,MilicaGasˇic´,SimonKeizer,Franc¸oisMairesse,JostSchatzmann,BlaiseThomson,
andKaiYu.2010. Thehiddeninformationstatemodel: apracticalframeworkforpomdp-based
spokendialoguemanagement. ComputerSpeech&Language,24(2):150–174.
Steve Young, Milica Gasˇic´, Blaise Thomson, and Jason D Williams. 2013a. Pomdp-based statis-
ticalspokendialogsystems: Areview. ProceedingsoftheIEEE,101(5):1160–1179.
Steve Young, Milica Gasic, Blaise Thomson, and Jason D. Williams. 2013b. POMDP-Based
StatisticalSpokenDialogSystems: AReview. ProceedingsoftheIEEE,101(5):1160–1179.
SteveJYoung.2002. Talkingtomachines(statisticallyspeaking). InINTERSPEECH.Citeseer.
AmirZadeh,YanShengCao,SimonHessner,PaulPuLiang,SoujanyaPoria,andLouis-Philippe
Morency.2020. Cmu-moseas: Amultimodallanguagedatasetforspanish,portuguese,german
and french. In Proceedings of the Conference on Empirical Methods in Natural Language
Processing.ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,volume2020,
page1801.NIHPublicAccess.
AmirAli Bagher Zadeh, Paul Pu Liang, Soujanya Poria, Erik Cambria, and Louis-Philippe
Morency.2018.Multimodallanguageanalysisinthewild: Cmu-moseidatasetandinterpretable
dynamicfusiongraph. InProceedingsofthe56thAnnualMeetingoftheAssociationforCom-
putationalLinguistics(Volume1: LongPapers),pages2236–2246.
107T. Zhang, V. Kishore, F. Wu, K.Q. Weinberger, and Y. Artzi. 2020a. Bertscore: Evaluating text
generationwithbert. InInternationalConferenceonLearningRepresentations.
Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng
Gao, Jingjing Liu, and Bill Dolan. 2020b. DIALOGPT : Large-scale generative pre-training
for conversational response generation. In Proceedings of the 58th Annual Meeting of the
Association for Computational Linguistics: System Demonstrations, pages 270–278, Online.
AssociationforComputationalLinguistics.
Deyu Zhou and Yulan He. 2011. Learning conditional random fields from unaligned data for
naturallanguageunderstanding. InEuropeanConferenceonInformationRetrieval,pages283–
288.Springer.
QiZhu,ZhengZhang,YanFang,XiangLi,RyuichiTakanobu,JinchaoLi,BaolinPeng,Jianfeng
Gao, Xiaoyan Zhu, and Minlie Huang. 2020. Convlab-2: An open-source toolkit for building,
evaluating,anddiagnosingdialoguesystems. InProceedingsofthe58thAnnualMeetingofthe
AssociationforComputationalLinguistics: SystemDemonstrations,pages142–149.
Yuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-Fei. 2016. Visual7w: Grounded question
answeringinimages.
108