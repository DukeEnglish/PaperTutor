1
Optimizing Age of Information in Vehicular Edge
Computing with Federated Graph Neural Network
Multi-Agent Reinforcement Learning
Wenhua Wang, Qiong Wu, Senior Member, IEEE, Pingyi Fan, Senior Member, IEEE,
Nan Cheng, Senior Member, IEEE, Wen Chen, Senior Member, IEEE,
Jiangzhou Wang, Fellow, IEEE and Khaled B. Letaief, Fellow, IEEE
Abstract
Withthe rapid development of intelligent vehicles and Intelligent Transport Systems (ITS), the sensors such as cameras and
LiDAR installed on intelligent vehicles provides higher capacity of executing computation-intensive and delay-sensitive tasks,
thereby raising deployment costs. To address this issue, Vehicular Edge Computing (VEC) has been proposed to process data
through Road Side Units(RSUs) to support real-time applications. Thispaper focuses on the Age of Information (AoI) as a key
metricfordatafreshnessandexplorestaskoffloadingissuesforvehiclesunderRSUcommunicationresourceconstraints.Weadopt
aMulti-agentDeepReinforcementLearning(MADRL)approach,allowingvehiclestoautonomouslymakeoptimaldataoffloading
decisions. However, MADRLposes risksof vehicleinformation leakageduring communication learningandcentralized training.
Tomitigatethis,weemployaFederatedLearning(FL)frameworkthatsharesmodelparametersinsteadofrawdatatoprotectthe
privacy of vehicle users. Building on this, we propose an innovative distributed federated learning framework combining Graph
Neural Networks (GNN), named Federated Graph Neural Network Multi-Agent Reinforcement Learning (FGNN-MADRL), to
optimize AoI across the system. For the first time, road scenarios are constructed as graph data structures, and a GNN-based
federated learning framework is proposed, effectively combining distributed and centralized federated aggregation. Furthermore,
we propose a new MADRL algorithm that simplifies decision making and enhances offloading efficiency, further reducing the
decisioncomplexity.Simulationresultsdemonstratethesuperiorityofourproposedapproachtoothermethodsthroughsimulations.
Index Terms
Qiong Wu and Wenhua Wang are with the School of Internet of Things Engineering, Jiangnan University, Wuxi 214122, China, (e-mail:
qiongwu@jiangnan.edu.cn, wenhuawang@stu.jiangnan.edu.cn)
PingyiFaniswiththeDepartmentofElectronicEngineering,BeijingNationalResearchCenterforInformationScienceandTechnology,TsinghuaUniversity,
Beijing 100084,China(Email:fpy@tsinghua.edu.cn)
Nan Cheng is with the State Key Lab. of ISN and School of Telecommunications Engineering, Xidian University, Xi’an 710071, China (e-mail:
dr.nan.cheng@iece.org)
WenCheniswiththeDepartmentofElectronic Engineering, ShanghaiJiaoTongUniversity, Shanghai200240,China(e-mail: wenchen@sjtu.edu.cn)
Jiangzhou WangiswiththeSchoolofEngineering, University ofKent, CT27NTCanterbury, U.K.(Email:j.z.wang@kent.ac.uk)
K. B. Letaief is with the Department of Electrical and Computer Engineering, the Hong Kong University of Science and Technology (HKUST), Hong
Kong(e-mail:eekhaled@ust.hk)
4202
luJ
1
]GL.sc[
1v24320.7042:viXra2
VehicularEdgeComputing,AgeofInformation,Multi-agentDeepReinforcementLearning,FederatedLearning,GraphNeural
Networks.
I. INTRODUCTION
WITH autonomous driving technology and Intelligent Transport Systems (ITS) evolving, intelligent vehicles deploy
sensorslike camerasandLiDARto aiddrivingorachieveautomation[1]–[4].Thesetechnologiesheightendemands
for computational and storage resources, thereby posing challenges for resource-limited vehicles [5]–[8]. Vehicular Edge
Computing (VEC) is a promising approach to support real-time applications by enabling vehicles to offload tasks to nearby
Road Side Units (RSUs), which process these tasks with their substantial computationand storage capabilitiesand then return
the results to the vehicles [9]–[12]. Moreover data freshness is becoming increasingly important in VEC. Different from
traditional performance metrics, the Age of Information (AoI) is a key indicator of data freshness, considering the generation
time and transmission delay of data [1], [13], [14]. However, as the number of vehicles or computation tasks increases,
transmission interferencebetween vehicles will greatly increase, potentially deterioratingthe AoI for each vehicle’s task [15]–
[17]. Additionally, relying on RSUs for vehicle task offloading will increase the information transmission overhead between
vehiclesandRSUs,anditwillalsodegradetheAoIrequirements.Therefore,distributedcollaborativeoffloadingamongvehicles
is crucial for optimizing the AoI in VEC.
In recent years, Multi-agent Deep Reinforcement Learning (MADRL) has offered a new solution for multi-vehicle task
offloading [?], [18], [19]. Each vehicle acts as an individual agent, making optimal data offloading decisions based on its
observations, enabling decentralized decision-making without waiting for centralized scheduling. However, most MADRL
training currently relies on the learning model exchange communication and centralized training, where vehicles’ training
and decision-making require other vehicles’ decision and state information, increasing communication bandwidth resource
consumption [20], [21]. Additionally, when RSUs collect all vehicle data for training, it will face the high risk of vehicle
informationleakage. This leads to distrust of RSUs and increases the risk of intercepting raw data, creating major bottlenecks
for training models [22]–[24].
Federated Learning (FL) offers a potential solution, where vehicles can collaboratively train models without sharing raw
data.ThisisachievedbysendingmodelorgradientsinsteadofrawdatatoRSUs,thusprotectingvehicleusers’data[25],[26].
By aggregatingmodels from various vehicles, FL facilitates the sharing of knowledge and learning experiences, resulting in a
more comprehensive and accurate MADRL model. However, traditional FL uses average aggregation, treating each vehicle’s
contribution equally, neglecting that each vehicle may have differentdata features and varying contributions to model training
[27]–[29]. In vehicular scenarios, the mobility of vehicles generates rich topological information, and each vehicle possesses
unique features, such as speed, data, and the quality of model training. These personalized details can effectively enhance
the generalization capability of MADRL models. Graph Neural Networks (GNNs), widely applied in various domains for
their ability to extractgraphinformation[30], effectivelycapturefeaturesfromvehicle-roadgraphand learn fromthe graphto
generateFLaggregatedmodelweights.Currently,thereisnoresearchconsideringtheenhancementofFLtrainingforMADRL
models using vehicle-road graphs to reduce AoI. This gap is the motivation for our work.3
Inthisarticle,weintroduceanovelFederatedGraphNeuralNetworkMulti-AgentReinforcementLearning(FGNN-MADRL)
algorithm,aimed at optimizingthe AoI in VEC. Our approachincorporatesvehicle-roadgraphwith distributed FL to enhance
the training of MADRL models. The major contributions of this paper are summarized as follows1:
• This paper introduces a method to construct vehicle scenes as vehicle-road graph for the first time. Specifically, roads
are divided into segments, each considered a node in a GNN, and establishing edges based on vehicle-to-vehicle com-
munication. This innovative approach effectively addresses the challenges posed by the dynamic variation in vehicle
number.
• WeproposeaninnovativeGNN-baseddistributedFLframeworkthatcombinesdistributedlocalfederatedaggregationwith
centralizedglobalfederatedaggregation.Thedistributedlocalfederatedaggregation,informedbyGNN-extractedvehicular
road graph structure, effectively generates weights for federated aggregation, considering each vehicle’s unique features
andcontributions.Thecentralizedglobalfederatedaggregationfurtherenhancesoverallmodelstabilityandcomprehensive
capability by integrating all local models.
• Additionally,we present a new MADRL algorithm for efficient cooperativeoffloading among vehicles. In this algorithm,
each vehicle makes decisions based solely on its observations, independent of other vehicles’ decisions and observa-
tions. This substantially simplifies decision-making and enhances offloading efficiency. By reducing reliance on external
information, this MADRL algorithm effectively improves the adaptability and reliability of vehicle offloading strategies.
Theremainderofthispaperisorganizedasfollows.SectionIIpresentsrelatedwork.SectionIIIdescribesoursystemmodel,
which includes the system scenario, communicationmodel, AoI model and the problem we aim to address. In Section IV, we
propose the our FGNN-MADRL scheme. This section begins with an introduction to the GNN-based FL algorithm, followed
by an explanation of the GNN combined multi-agent SAC framework and algorithm. Section V is dedicated to simulation
experiments and analysis. Finally, we conclude it in Section VI.
II. RELATEDWORK
In this part, we first review the research on GNN in VEC, followed by an overview of cooperative task offloading.
A. Application of GNN in VEC
RecentstudieshavebeguntoapplyGNNsinIoV.Liuetal.in[1]proposedaSpatio-temporalModelingAndReconsTruction
(SMART) framework for assessing the feasibility of differenttime-delay sensitive services in large-scale IoV. This framework
modelstheVANETasagraphbydividingtheserviceareaintosubareas(nodes)connectedbyedgesrepresentingsimilardelay
probabilities. SMART leverages Graph Convolutional Networks (GCN) and Deep Q-Networks (DQN) to capture spatial and
temporal features of the graph, enabling the reconstruction of an updated large-scale VANET topology from limited subarea
samples. He et al. in [31] proposed a distributed spectrum sharing framework enhanced by GNN for vehicular networks.
They represented the vehicular network as a graph with local observations of vehicle pairs as nodes and channel gains
of interference links as edges. The GNN learns low-dimensional features of each node/vehicle pair, with each pair treated
1The source code has been released at: https://github.com/qiongwu86/Optimizing-AoI-in-VEC-with-Federated-Graph-Neural-Network-Multi-Agent-
Reinforcement-Learning4
as an agent in MADRL. This approach optimizes the total capacity of the vehicular network and base station links using
MADRL, with information propagated along graph edges to update each vehicle without base station support. In [32], He
et al. addressed the challenge of task allocation in multi-scale IoV with a Deep Reinforcement Learning-based efficient task
allocationscheme. Theycombinedstorage, computation,and cachingmechanismsto supportvehiculartask distributionacross
multiplesystem scales. The dynamicsystem was modeledgraphically,incorporatingnodecharacteristicsand time IoV-varying
edge relationships, using a Graph Attention Network (GAT) and a hybrid algorithm combining Deep Deterministic Policy
Gradient (DDPG) for task scheduling optimization. Zhou et al. in [33] presented a computationaltask allocation method with
demandpredictionandRLforInternetofThings(IoT)environmentssupportedby6Gtechnology.Theyusedaspatial-temporal
GNN-based prediction method for task demand forecasting and a simplex algorithm for cache decision-making. Additionally,
they proposed a Twin Delayed DDPG (TD3)-based computationaltask allocation method. Liu et al. in [34] proposed a GNN
and DRL-based GA-DRL algorithm for the subtask-to-vehicle assignment problem in Directed Acyclic Graph (DAG) tasks
within IoV. They used multi-head GAT networks to extract subtask feature information, integrating these features into Double
DeepQ-Networks(DDQN)fordecision-making.Xiaoetal.in[35]introducedaStochasticGraphNeuralNetwork(SGNN)and
RL-based distributed stochastic decision algorithm for intelligent traffic control tasks. It tried to capture dynamic topological
connectivity features of vehicles using SGNN, enabling disturbance resistance, where the SGNN is embedded in a Proximal
Policy Optimization (PPO) framework with a value decomposition function for modeling vehicle relationships as random
graphs. Chen et al. in [36] proposed a resource orchestration algorithm for vehicular cloud computing networks, abstracting
theresourceorchestrationproblemintoavirtualnetworkembeddingproblem.Theydesignedafour-layerpolicynetworkbased
onGCNtocalculatenodeembeddingprobabilities,extractingspatialstructuralinformationbetweennodesandneighborhoods.
Lietal.in[37]developedaGCN-basedtopologydesignmethod(G-DFL)toimprovetrainingefficiencyinVANETdistributed
federatedlearning.Theyextractedwirelessnetworktopologyfeaturesbetweenvehiclesusing GCN, optimizingtrainingdelays
to generate connection graphs. Moreover, they used the Christofides algorithm to find a minimum delay Hamiltonian circuit
for model sharing. However, these studies did not consider the dynamic nature of vehicles, such as changes in network node
topology due to vehicle movement.
B. Cooperative Offloading in VEC
Recentresearchhasexploredcollaborativetaskoffloadinginvehicularnetworks.In[38],Langetal.proposedablockchain-
based data sharing architecture, targeting information sharing and computational offloading in vehicular multi-access edge
computing(MEC) networks.This architecture,using blockchaintechnology,aims to provideaccurate service vehicleinforma-
tiontosupportcooperativecomputationoffloading.Tofacilitateeffectivedecision-makinganddatasynchronization,theauthors
introduceda consensusmechanismcombiningserviceproofandpracticalByzantinefaulttolerance,alongwith a gametheory-
based offloading decision model, designed to guide user vehicles in making appropriate choices in cooperative computation
offloadingscenarios.In[39],M.Zakietal.introducedaCooperativePerception-basedTaskOffloading(CPTO)scheme,aimed
at optimizing vehicular edge computing (VEC) in autonomous vehicles. CPTO focuses on maximizing vehicles’ cooperative
perception capabilities and minimizing the latency of perception aggregation, adhering to specific deadlines. To achieve this,5
they formulated the task offloading problem as a multi-objective 0-1 integer linear programming (0-1 ILP) and proposed a
greedyheuristicalgorithm,CPTO-Heuristic(CPTO-H),tosolvethisoptimizationproblem.In[40],Wangetal.proposedanovel
multi-user computational offloading game method for vehicular MEC networks, adjusting the offloading probability of each
vehicle.This methodconsidersthe distance between vehiclesand MEC access points, applicationand communicationmodels,
andthecompetitivescenarioofmultiplevehiclesforMECresources,thendesignedapayofffunction.Additionally,theauthors
builtadistributedoptimalresponsealgorithmbasedonthecomputationaloffloadinggamemodel,aimingtomaximizetheutility
ofeachvehicle.In[41],Zhouetal.proposeda methodto optimizeoffloadingdecisionthresholdsin MECnetworks,intending
to maximize the expected successful offloading rate of tasks. Combining game theory analysis and constrained nonlinear
optimization theory,they showed that at least one mixed-strategyNash equilibrium exists in the system. They formulatedtask
offloadingoptimizationas a Multi-Agentdecision problemand developeda distributedunconstrainedLagrangianoptimization
(ULO)schemebasedonthebestresponsemechanism.In[42],Alametal.introducedaninnovativethree-tiervehicular-assisted
multi-accessedgecomputing(VMEC)networkdesigntoaddresscollaborativecomputationaloffloadingissuesinhigh-mobility
vehicularnetworkenvironments.This networkutilizes movingand parkedvehiclesassociated with RSUs as VMEC serversin
the fog layer and proposeda strategy based on the Hungarianalgorithm in Multi-Agentto find the optimaloffloadingstrategy
through collaborative agents’ actions in dynamic network environments.In [43], He et al. researched dynamic data offloading
in urban rail transit, aiming to improve the low latency and stability issues of vehicle-to-ground communication caused by
high-speed mobility. Using a software-defined network (SDN) controller, they enabled mobile users to choose MEC servers
for offloading their data. To decide the specific MEC servers for mobile users’ data offloading, the authors conducted a game
among mobile users and formulated an optimization problem of the user utility function to determine the optimal offloading
datavolumeforMECserversandmaximizeuserutility.However,noneoftheabovestudiesconsideredthetopologicalstructure
in vehicular scenarios.
Asmentioned,noworkhasconsideredcollaborativetaskoffloadingconcerningthedynamictopologicalstructureofvehicles.
III. SYSTEMMODEL
In this section, we will introduce the system model. We first describe the system scenario, followed by the communication
model. Next, we introduce the AoI model and finally present the problem that needs to be addressed.
RSU
Lane 1
Lane 2
(cid:258)(cid:3) (cid:258)(cid:3) (cid:258)
Lane L-1
Upload tasks Lane L
Fig. 1: System Scenario6
A. System Scenario
As depicted in Fig. 1, consider a VEC scenario where RSUs are deployed alongside the road, each with a communication
coverage radius of D . The VEC scenario contains L = {1,2,3,...,L} lanes, where L represents the number of lanes. Each
r
lane limits the vehicles to different speeds, denoted by V = {v1,v2,...,vi,...,vL}. We assume uniform speed for vehicles
on each lane. Vehicle generation on each lane follows a Poisson distribution λ={λ1,λ2,...,λi,...,λL}, where λi represents
the arrival rate of vehicles entering the RSU coverage area on the ith lane. We denote the set of all vehicles within the
RSU coverage area at time slot t as V . Each vehicle randomly generates a task, with the task generation interval following
t
a Poisson distribution with parameter µ. The size of each task follows a uniform distribution within the range [d ,d ],
min max
where d and d represent the minimum and maximum task sizes, respectively. Once a task is generated, each vehicle
min max
needs to transmit the task to the RSU for processing. The tasks will first be stored in a queue awaiting transmission, and the
tasks in the queue will be sent according to the First-in-First-Out (FIFO) policy.
We use J (t)={1,2,3,...,J (t)} to representthe task indexin the queue of vehicle c at time slot t, where J (t)
ci ci,max i ci,max
indicates the maximum index of tasks for the current time slot, which also indicates the number of tasks in the queue.
Specifically,J (t)=1 representsthe earliest generatedtask in the queue,i.e.,the currenttask awaiting transmission.Ifa new
ci
task is generated, the number of tasks in the queue increases by one, i.e., J (t)={1,2,3,...,J (t)}.
ci ci,max
B. Communication Model
Assuming the transmitting power of vehicle c at time slot t is p (t). Thus the transmission rate of vehicle c at time slot
i ci i
t can be calculated based on Shannon’s theorem, i.e.,
g (t)p (t)
r (t)=Blog 1+ ci ci , (1)
ci 2 g (t)p (t)+σ2
ci6=cj cj cj !
P
where B is the total uplink bandwidth within the RSU coverage area, g (t) is the channel gain and σ2 is the noise power.
ci
p (t) and g (t) are the transmitting power and channel gain of other vehicles, respectively. g (t) is calculated as
cj cj ci
g (t)= α (t)h (t), (2)
ci ci ci
p
where α (t) and h (t) represent the large-scale and small-scale fading components of vehicle c at time slot t, respectively.
ci ci i
The large-scale component α (t) includes path loss and log-normal shadowing. Let X =(x ,y ) be the RSU’s coordinate,
ci r r r
and the vehicle c ’s coordinatesat time slot t is X (t) = (x (t),y (t)). The large-scale fading component α (t) can be
i ci ci ci ci
calculated as
α (t)=PL(X (t),X )+χ (t), (3)
ci ci r ci
where PL(·) indicates distance-related path loss. χ (t) represents the log-normal shadow fading from X (t) to X , which
ci ci r
is updated as
χ (t)=ρ1 (t)χ (t−1)+σ e (t), (4)
ci ci ci s ci7
wheree (t)isaGaussian-distributedlog-normalshadowfadingrandomvariable.ρ1 (t)isthecorrelationcoefficientofshadow
ci ci
fading and it is calculated as
ρ1 ci(t)=e∆(X dcc oi r(t)) , (5)
where ∆(X (t))=||X (t)−X (t−1)|| indicates the Euclidean distance between vehicle c at time slots t and t−1, d
ci ci ci 2 i cor
is the correlation length of the environment.
We adopt the Jakes fading model to introduce the small-scale Rayleigh fading component h (t) as a first-order complex
ci
Gaussian Markov process, i.e.,
h (t)=ρ2 (t)·h (t−1)+q (t), (6)
ci ci ci ci
whereρ2 (t)representsthecorrelationcoefficientbetweenh (t)andh (t−1),andq (t)isanindependentchannelinnovation
ci ci ci ci
process. The correlation coefficient ρ2 (t) is calculated as ρ2 (t) = J (2πf (t)τ), where J (·) is the zeroth-order Bessel
ci ci 0 d,ci 0
function of the first kind and τ is the time length of a time slot. f (t) =
vci(t)·fc
is the Doppler frequency considering
d,ci c
the impact of vehicle movement, where v (t) is the speed of the vehicle c , f is the carrier frequency f , as f (t) =
ci i c c d,ci
vci(t)·fc, where c = 3 × 108 is the speed of light. The independent channel innovation process q (0),q (1),q (2),...
c ci ci ci
consists of independently distributed circularly symmetric complex Gaussian (CSCG) random variables, with distribution
CN 0,1−(ρ2 (t))2 .Itisimportanttonotethattheinitialsmall-scaleRayleighfadingcomponenth (0)followsaCN (0,1)
ci ci
(cid:16) (cid:17)
distribution.
C. AoI Model
We use φ (t) to representthe AoI ofthe task J (t) ofvehiclec at time slott. The AoIof the J (t)=1 task, denoted
ci,Jci ci i ci
as φ (t), is calculated as
ci,1
φ (t−1)+
dci,1(t)
,if r (t)·τ ≥d
φ ci,1(t)=

ci,1 rci(t) ci ci,1 , (7)
 φ ci,1(t−1)+τ,otherwise
where d (t) represents the size of the J
(t)=1
task. If the transmission rate r (t) within a time slot is greater than the
ci,1 ci ci
task size, i.e., r (t)·τ ≥ d , then φ (t) increases by the transmission time
dci,1(t)
. Otherwise the J (t) = 1 task only
ci ci,1 ci,1 rci(t) ci
wait for the next time slot to be sent and φ (t) increases by τ. For other tasks, i.e., J (t) > 1 tasks, their AoI φ (t)
ci,1 ci ci,Jci
are calculated as φ (t)=φ (t−1)+τ.
ci,Jci ci,Jci
The average AoI of each vehicle can be obtained by calculating the average AoI of all tasks in the queue, i.e,
1
Mci(t)
φ (t)= φ (t), (8)
ci
M ci(t)
j=0
ci,Jci
X
where M (t) is the number of tasks of vehicle c at time slot t. The system’s average AoI is caculated as
ci i
1
Nc(t)
φ(t)= φ (t), (9)
N (t)
ci
c
i=1
X8
where N (t) is the number of vehicles within the RSU coverage area at time slot t. Our research problem is to optimize the
c
system AoI within the coverage area of RSUs.
IV. COOPERATIVE TASK OFFLOADINGSCHEME
In this section, we introduce the Federated Graph Neural Network Multi-Agent Reinfor(cid:3)c(cid:3)ement Learning (FGNN-MADRL)
algorithm. We start by presenting a FL algorithm based on GNNs. Then we formulate the MADRL framework. Finally, we
employ the GNN combined Multi-agent SAC algorithm for cooperative task offloading among vehicles.
(cid:24) (cid:21) Local Training
RSU
Train
Lane 1
(cid:22) Local Aggregation
Lane 2
(cid:21)
(cid:258) (cid:258) (cid:258) Aggregation
Lane L-1 (cid:24)Global Aggregation
Lane L
Aggregation
Fig. 2: Distributed FL Based on GNN.
A. FL Algorithm Based on GNN
FL facilitates collaborativetraining of DRL models, while each vehicle retains its trainingdata. Throughfederatedaggrega-
tion, vehiclescan share knowledgeand experience with each other, enhancingthe performanceand effectivenessof the global
model. This decentralized approach significantly alleviates privacy concerns and reduces communication overhead associated
with centralized training methods. As shown in Fig. 2, the distributed FL algorithm based on GNN performs Rmax rounds,
each consisting of the following four steps:
1) Download Model: In the model, once vehicles enter the RSU coverage area, they download the latest DRL model from
the RSU. The DRL model adopted is based on the Actor-Critic framework. Therefore, at time slot t, vehicles download the
latest global actor network model ωglobal(t), global critic network model ωglobal(t), and global target critic network model
a c
ωglobal(t) as their local initial models ωci(t), ωci(t), and ωci(t).
tc a c tc
2) Local Training: Vehicle c begins the local training after downloading the global DRL models from the RSU. This
i
processinvolvesinteractingwith the environmentto collecttrainingdata andstoringthemin a replaybufferB with a certain
ci
capacity D . The local model undergoes I iterations of updating, each can be represented as
s ci
ωci ←ωci −ηci∇Fci(ωci),r ∈{a,c,tc}, (10)
r r r r r
where ∇Fci(ωci) denotes the gradient of ωci,r ∈{a,c,tc} and ηci is a fixed learning rate. Noted that due to differences in
r r r r
vehicular computational resources and the number of training iterations I , each vehicle has a different training time t .
ci c19
3) LocalAggregation: TraditionalFLinvolvesuploadingmodelstotheRSUforglobalaggregationuponcompletionoflocal
training.However,frequentmodeluploadscanincursubstantialcommunicationoverhead,consumingexcessivebandwidthand
impacting task offloading and AoI. In addition directly employing an average FL for updating local models might overlook
the unique characteristics of each vehicle’s previously trained model. This not only wastes computationalresources consumed
during training but also harms model personalization. Therefore, we first perform local model aggregation based on GNNs
amongvehicles,whereGNNsareusedtogeneratetheaggregationweightsbycapturingthecharacteristicsofvehicularnetwork.
Due to the varying number of vehicles within the RSU coverage area, treating each vehicle as a node would increase the
complexityof the GNN network.To overcomethis issue, we dividethe roadinto multiple segments,with each segmentacting
as a node of the GNN. As illustrated in Fig. 3, We define the road within the coverage area of a single RSU as a graph
G(t)=(V(t),E(t)), where eachnodeV(t) representsa roadsegmentwith lengthL and the totalnumberof nodesis 2DrL.
g Lg
For any given node v ∈V(t), its feature ψ (t) is defined as
i vi
ψ (t)=[n (t),a (t),L (t),L (t),L (t)], (11)
vi vi vi vi,a vi,c vi,tc
where n (t) represents the number of vehicles in node v at time slot t, and a (t) represents the average number of times
vi i vi
all vehicles within node v have participated in local aggregation. Furthermore, L (t), L (t) and L (t) represents the
i vi,a vi,c vi,tc
average loss values of the actor network, critic network and critic network of all vehicles within node v , respectively.
i
We define a set Ω (t) = N(c ) containing all nodes within the communication range D of vehicles in node v ,
vi j c i
cj∈Cci(t)
where C (t) denotes the set ofSvehicles in node v , and N(c ) denotes the set of nodes formed by the vehicles within the
ci i j
communication range of vehicle c . For each node v in Ω (t), we establish an edge between nodes v and v and the edge
j j vi i j
set E(t) of the graph G(t) can be obtained. For example, if node v contains two vehicles, both with a communication range
1
of D , and the vehicles within the communication range of vehicle c are in node v , while the vehicles within the
c 1 2Dr+1
Lg
communicationrangeof vehicle c are in nodev , thennodev formsundirectededgeswith nodesv and v .
2 2Dr+2 1 2Dr+1 2Dr+2
Lg Lg Lg
Similarly, edges for other nodes are constructed in the same way. We then define the adjacency matrix A(t) of graph G(t)
with dimensions 2DrL× 2DrL. If (v ,v )∈E(t), then A (t)=1, otherwise A (t)=0.
Lg Lg i j ij ij
Next, we transform the feature vector ψ (t) of node v ∈ V(t). The intermediate representation of node v in layer l
vi i i gnn
of GNN is denoted as
h¯lgnn
and is calculated as
i
h¯lgnn =W lgnn ·hlgnn−1, (12)
i g i
where W lgnn is the learnable transformation matrix for layer l , and hlgnn−1 is the representation of node v from the
g gnn i i
previous layer, with the first layer representation h0 being the input feature vector ψ (t). We then aggregate neighborhood
i vi
information for
h¯lgnn
as
i
hlgnn
=σ
h¯lgnn
+ ε
h¯lgnn
, (13)
i  i ij j 
j∈XΩvi(t)
 
where
h¯lgnn
+ ε
h¯lgnn
represents the intermediate representation of other aggregated nodes, ε is the aggregation
i ij j ij
j∈Ωvi(t)
P10
weight, and σ(·) is a nonlinear activation function.
Each GNN network layer operates as above, ultimately outputting an encoded graph G(H)(t) ∈ R2 LR grL×p that includes
feature embedding vectors for all nodes ψ¯ (t) ∈ Rp, where p is the dimension of the feature space, and H denotes the
vi
number of layers in the GNN. We represent the set of all node-extractedfeature vectors as ψ¯(t)=[ψ¯ (t),ψ¯ (t)...ψ¯ (t)].
v1 v2 vi
We define a set B (t) = ψ¯ (t)|v ∈N(c ) containing the extracted node features of all vehicles within the commu-
ci vj j i
nication range of vehicle c , w(cid:8)here N(c ) represe(cid:9)nts the set of nodes containing vehicles within the communication range of
i i
vehicle c . We then determine the weights for model aggregation by performing a softmax on all extracted node features in
i
B (t), i.e.,
ci
eψ¯ vi(t)
α (t)= , (14)
ci eψ¯ vi(t)+ eψ¯ vj(t)
vj∈Bci(t)
P
where α (t) represents the aggregation weight of vehicle c .
ci i
(cid:17)(cid:17)(cid:17) (cid:17)(cid:17)(cid:17)
(cid:17)(cid:17)(cid:17)(cid:17)(cid:17)(cid:17) (cid:17)(cid:17)(cid:17)(cid:17)(cid:17)(cid:17)
v 2
(cid:17)(cid:17)(cid:17) (cid:17)(cid:17)(cid:17) (cid:17)(cid:17)(cid:17)(cid:17)(cid:17)(cid:17) (cid:17)(cid:17)(cid:17)(cid:17)(cid:17)(cid:17)
(cid:17) (cid:17) (cid:17)
(cid:17) (cid:17) (cid:17) (cid:17)(cid:17) (cid:17)(cid:17) (cid:17)(cid:17) (cid:17)(cid:17) (cid:17)(cid:17) (cid:17)(cid:17)
L g (cid:17) (cid:17) (cid:17) (cid:17)(cid:17) (cid:17)(cid:17) (cid:17)(cid:17) (cid:17)(cid:17) (cid:17)(cid:17) (cid:17)(cid:17)
(cid:17)(cid:17)(cid:17) (cid:17)(cid:17)(cid:17) (cid:17)(cid:17)(cid:17)(cid:17)(cid:17)(cid:17) (cid:17)(cid:17)(cid:17)(cid:17)(cid:17)(cid:17)
(cid:17)(cid:17)(cid:17) (cid:17)(cid:17)(cid:17) (cid:17)(cid:17)(cid:17)(cid:17)(cid:17)(cid:17) (cid:17)(cid:17)(cid:17)(cid:17)(cid:17)(cid:17)
v v v
2 LD gr(L-1)+i 2 LD gr(L-1)+i+1 2 LD gr(L-1)
Fig. 3: GNN road graph.
Inspired by the centralized critic network concept of Multi-Agent Deep Deterministic Policy Gradient (MADDPG), our
model aggregates only the critic network during local aggregation. This design allows the critic to evaluate the value of its
ownactionsin considerationofothervehicles’behaviors,asthevalueoftendependsontheoverallenvironment.Besides, each
vehicle’s actor network can independentlymake decisions based on its experienceand environment.Assuming the latest critic
model parameter for vehicle c
i
at time slot t is ω cci(t). Hence, the critic model aggregation for vehicle c
i
is caculated as
ωci(t)=α (t)ωci(t)+ α (t)ωcj(t), (15)
c ci c cj c
cj∈XCci(t)
where C (t) represents the set of vehicles within the communicationrange of vehicle c . After completing local aggregation,
ci i
vehicles start the next round of local training.
4) Upload Model: Before leaving the RSU coverage area, vehicle c
i
uploads its latest trained actor network model ω aci(t)
critic network model ωci(t), and target critic network model ωci(t) to the RSU.
c tc
D
r
D c
v1
v2Dr 1+Lg
v2Dr(
)2-L 1+Lg
v2Dr(
)1-L 1+Lg
v2Dr 2+Lg
v2Dr(
)2-L +iLg
v2Dr(
)2-L + 1i+Lg
v2Dr 1-Lg
v4Dr 1-Lg
v2DrLg
v4DrLg
v2Dr(
)1-LLg11
5) GlobalAggregation: Afterreceivingthelocalmodelsfromallvehiclesabouttoexititscoveragerange,theRSUperforms
asynchronousfederated aggregation.We use C(t) to denote the set of all vehicles leaving the RSU coverage area at time slot
t. The RSU computesthe new globalactor network model ωglobal(t), global critic network model ωglobal(t) and globaltarget
a c
critic network model ωglobal(t) by
tc
ωci(t)
ωglobal ← r ,r ∈{a,c,tc}, (16)
r C(t)
ciX∈C(t)
(cid:12) (cid:12)
whereω rci(t),r ∈{a,c,tc}representsthe latestmodelofdisa(cid:12) ppear(cid:12) ingvehiclec
i
attime slott. Thenthe federatedaggregation
process is completed, and the RSU has obtained an updated global model. However, model uploading requires additional
bandwidth resources. We assume the size of the model as |W | and denote the minimum transmission power required for
ci
vehiclec touploaditsmodelin thecurrenttime slotasp (t), with the numberofvehiclesthatneedto uploadtheirmodels
i ci,ω
in the current time slot being N (t). Therefore, the transmission rate for offloading tasks of each vehicle is recalculated as
m
R (t)=
ci
. (17)
p (t)g (t)
Blog 1+ ci ci ,
2 N(t) Nm(t)
 p (t)g + p (t)g (t)+σ2
 cj cj ck,ω ck 
 j6=i k=1 
 P P 
Thus φ (t) of the J (t)=1 task is also recalculated as
ci,1 ci
φ (t−1)+
dci,1(t)
,ifR (t)·τ ≥d
φ ci,1(t)=

ci,1 Rci(t) ci ci,1 . (18)
 φ ci,1(t−1)+τ,otherwise
The process of the FL algorithm is shown inAlgorithm 1. Subsequently, the RSU sends the updated global model to all new
coming vehicles entering the coverage area.
B. Cooperative offloading Scheme Based on MADRL
Next, the DRL framework is first formulated, which is the basis of the MADRL algorithm. Then, the GNN combined
MASAC algorithm will be introduced.
1) MADRL Framework: The MADRL framework includes states, actions and rewards which are defined as follows.
a) States: In our model, the state of each vehicle comprises several key factors,i.e.,
s (t)=[g (t),φ (t),φ(t),l (t),d ,N (t)], (19)
ci ci ci,0 ci,r ci,0 c
where g (t) is the channel gain, an important indicator of communication quality between the vehicle and RSU. φ (t) is
ci ci,0
the AoI of the task waiting to be sent. l (t) indicates the distance of vehicle c from the RSU. d is the size of the task
ci,r i ci,0
waiting to be sent and N (t) representsthe numberof vehicleswithin the RSU coverage.The RSU monitorsthe environment,
c
includingthe system’saverageAoI φ(t) andthe numberofvehiclesN (t), and sendsthese informationto all vehiclesthrough
c
a downlink. We assume that the data sizes of these information are very small, so the delay in transmitting this information
can be considered negligible.12
b) Action: Each vehicle needs to decide its transmission power for transmitting tasks, i.e.,
p (t)∈[0,p ], (20)
ci max
where p is the maximum transmission power for each vehicle. The transmission power of vehicles interferes with and
max
affects the transmission rate of other vehicles.
c) Reward: The purposeof the rewardis to optimizethe behaviorofvehiclesto minimizethe system’s averageAoI. The
reward r (t) for vehicle c at time slot t is designed as
ci i
− φ¯(t)+p (t)·ω +ξ(t)·ω , M (t)>0
r (t)=
ci 0 2 ci
. (21)
ci 
 −(cid:0)φ¯(t)+p ci(t)·ω 1+ξ(t)·ω 2(cid:1), M ci(t)=0
(cid:0) (cid:1)
where M (t) is the numberof tasks for vehicle c at time slot t. ω =1+ φ¯(t) and ω =1+φ¯(t) are factors representing
ci i 0 φci,0(t) 1
the impact of the vehicle’s own power on the system’s average AoI. The inclusion of p (t)·ω and p (t)·ω in the reward
ci 0 ci 1
considers the interference caused by the vehicle’s transmission power to others. Higher transmission power p (t) causes
ci
significant interference to others, resulting in a larger penalty in the reward function. Conversely, a relatively high AoI with
lower transmission power p (t) also leads to a substantialpenalty.The additionof 1 in ω and ω ensuresthat p (t)·ω and
ci 0 1 ci 0
p (t)·ω do not equal zero when φ¯(t) =0 and φ¯(t)=0, avoiding abrupt value changes that may likely cause instability
ci 1 φci,0(t)
in model training.
ω is a weight factor and a hyperparameter. ξ(t) represents the penalty for unprocessed tasks when the vehicle leaves the
2
RSU coverage area. It is designed to encourage vehicles to offload as many tasks as possible within the RSU. Otherwise,
Algorithm 1: Distributed FL Algorithm Based on GNN
1 Initialize the global models: ω aglobal(0), ω cglobal(0), and ω tc ci(0);
2 Initialize the set of vehicles: V t ={};
3 for t from 1 to Rmax do
4 for each vehicle c i in V t in parallel do
5 Update the position: x ci(t)←x ci(t)+v mlid ax x;
6 if vehicle c i exits the RSU coverage area then
7 Upload models: ω aci(t), ω cci(t), ω tc ci(t);
8 Update the global model based on Eq. (16);
9 Remove vehicle c i from the set V t: V t ←V t\{c i};
10 if vehicle c i enters the RSU coverage area then
11 Download models: ω aci(t), ω cci(t), ω tc ci(t) ← ω aglobal(t), ω cglobal(t), ω tc ci(t);
12 Add vehicle c i to the set V t: V t ←V t {c i};
13 Update the vehicle road graph structure G( St)=(V(t),E(t));
14 for each vehicle c i in V t in parallel do
15 Update the position: x ci(t)←x ci(t)+v mlid ax x;
16 if training for vehicle c i is completed then
17 Calculate the local model aggregation weight α ci(t) extracted by GNN based on Eq. (15);
18 Update the local model based on Eq. (16);
19 Train the GNN network;
20 Return ω aglobal(t), ω cglobal(t), ω tc ci(t).13
the trained model will rely on the disappearance of vehicles to reduce the Average AoI, resulting in vehicles maintaining a
transmission power of zero within the RSU coverage area. ξ(t) is a recursive function which is calculated as
ξ(t−1)+ 1 φ (t) , C(t) >0
Nc(t) ci
ξ(t)= ci∈C(t) , (22)
 ξ(t−1)×δ P ,(cid:12) (cid:12)C(t)(cid:12) (cid:12)=0
whereC(t)denotesthesetofallvehiclesabouttoleavethe
RSU
coveragearea(cid:12)
(cid:12)
atti(cid:12)
(cid:12)me slott, C(t) isthenumberofvehicles
in the set, φ (t) is the AoI of the disappearing vehicle c , and δ ∈(0,1] is the decay factor(cid:12)for th(cid:12)e penalty term, indicating
ci i (cid:12) (cid:12)
a continuous and gradually diminishing impact of the penalty after vehicles leave the RSU range. The expected long-term
discounted reward for vehicle c is calculated as
i
Nc(t)
J(µ ):=E γt−1r (t) , (23)
ci µci

ci

t=1
X
 
where γ ∈ [0,1] is the discount factor, and N (t) represents the number of vehicles within the RSU coverage at time slot t.
c
Our goal is to find the optimal strategy µ∗ that maximizes the expected long-term discounted reward for vehicle c .
ci i
2) GNNcombinedMASACalgorithm: Intheconsideredscenario,duetothecontinuousactionspaceofvehicletransmission
power,the RL modelfor each vehicle employsthe SAC model. Comparedto DDPG, SAC shows better sample efficiencyand
stability. SAC introduces entropy regularization into the RL framework, encouraging exploration and achieving more robust
policy learning. This is particularly beneficial in the dynamic and complex vehicular network environment. According to the
SAC algorithm, the expected long-term discounted reward is calculated as
J(π (a |s ))=E
ci,t ci,t ci,t τ∼πci,t(aci,t|sci,t)
T , (24)
γt−1r (t)+β H(π (a |s ))
ci ci,t ci,t ci,t ci,t
" #
t=0
X
where s , a , and π (a |s ) respectively represent the state, action and strategy for vehicle c at time slot t.
ci,t ci,t ci,t ci,t ci,t i
H(π (a |s ))istheentropyofthepolicy.β isabalancingfactorbetweenexploringfeasiblestrategiesandmaximizing
ci,t ci,t ci,t ci,t
rewards for vehicle c , which is dynamically adjusted based on the state s . The optimal β∗ under state s is defined as
i ci,t
ci,t
ci,t
β c∗
i,t
=argmin βci,tE at∼π t∗ −a ci,tlogπ c∗ i,t−β ci,tH ci,t , (25)
(cid:2) (cid:3)
where H = dim(a ) represents the dimension of the action. π∗ is the optimal strategy for vehicle c at time slot t,
ci,t ci,t ci,t i
which is calculated as
π∗ =argmaxJ(π (a |s )), (26)
ci,t
πci,t
ci,t ci,t ci,t
The SAC algorithm architecture includes an actor network, two critic networks and two target critic networks. The actor
network is responsible for policy improvement, while the two critic networks perform policy evaluation. Two target critic
networks aim to improve training speed and stability. Throughcontinuouspolicy improvementand evaluation, the policy π(t)
eventually convergesto the optimal policy π∗(t).14
To evaluate the quality of weights generated by GNNs, we leverage the policy gradient method. Specifically, the RSU
maintainsaGNNnetworkandaGNNcriticnetwork.TheGNNcriticnetworkevaluatestheperformanceofgeneratedweights,
while the GNN networkimprovesits weightgenerationcapabilitybased on feedbackfrom the GNN critic network.Moreover,
for more stable target values, we introduce a target GNN critic network. Next the detailed process of the FGNN-MADRL
algorithm will be explained.
a) Training Stage: Let θ represent the parameters of the actor network, ϕ and ϕ represent the parameters of the two
a 1 2
criticnetworks,andϕ¯ andϕ¯ representtheparametersofthetwoTargetcriticnetworks.ThepseudocodefortheSACtraining
1 2
phase algorithm is shown in Algorithm 2.
Initially, RSU randomly initialize the SAC model parameters, including two global critic network parameters ϕglobal and
1
ϕglobal, global actor network parameter θglobal, two global target critic network parameters ϕ¯global and ϕ¯global (initialized
2 a 1 2
same to ϕglobal and ϕglobal), and βglobal. We also initialize the GNN network parameter θ , critic GNN critic parameter ϕ ,
1 2 g g
and GNN target critic model parameters ϕ¯ . A GNN experience replay buffer B with a storage capacity D is set up in the
g g g
RSU to store the road’s graph data.
First all vehicles are cleared on the road. Then a new road graph G(0) is generated based on the current road scenario,
whereeachnode’sfeaturevectorisa zerovectorandthereare noedgesbetweennodes.We inputG(0)intothe GNN network
to obtain an initial extracted feature vectors ψ¯(0). Next, we recalculate the average AoI φ¯(0) within the RSU coverage area.
To simulate the dynamic movement of vehicles, we generate the first vehicle entry into the RSU coverage area at time slots
according to a Poisson distribution λ .
L
The algorithm simulates from time slot 1 to Rmax. In each time slot, vehicles update their positions based on their speed
andperformboundarycheckstodetermineifanyvehicleshavelefttheRSU coveragearea.VehiclesleavingtheRSUcoverage
upload their latest local model parameters ϕci(t), ϕci(t), θci(t), ϕ¯ci(t), ϕ¯ci(t) to the RSU for global federated averaging.
1 2 a 1 2
Additionally,eachvehiclecalculatestheminimumpowerp (t)whichisrequiredtouploadthemodel.Newvehiclesentering
ci,ω
the RSU coverage area are added to the system and download the latest model from the RSU. They also initialize their SAC
experience replay buffer B with a certain storage capacity D .
ci s
Each vehicle checks for new tasks in the current time. If a new task is generated, a task size uniformly distributed within
[d ,d ] is produced and stored in the vehicle’s task queue, with the next task arrival interval generated according to a
min max
Poisson distribution µ. Vehicles observe the current state s (t), i.e., Eq. (19), then inputs s (t) into its actor network to
ci ci
generate its action p (t). Based on each vehicle’s channel gain g (t), transmission power p (t) and model transmission
ci ci ci
power p (t), the transmission rate R (t) is calculated based on Eq. (17). Each vehicle then executes task offloading. The
ci,ω ci
RSU computesthe nexttime slot’s averageAoI φ¯(t+1) andeach vehiclecomputestheir own averageAoI φ (t+1) based
ci,0
on Eq. (8) and Eq. (9). Each vehicle’s reward r (t) is calculated based on Eq. (22).
ci
Then the algorithm runs to the next time slot. Vehicles’ positions, channels and number changes, we can get each vehicle’s
next state s (t+1). Each vehicle stores the transition tuple (s (t),p (t),r (t),s (t+1)) in its own experience replay
ci ci ci ci ci
buffer B . If the data number |B | in B exceeds I , vehicle c begins I iterations of model training and updating. A
ci ci ci ci i ci
new road graph G(t) also can be obtained based on the current road scenario and vehicles’ model training. The newly G(t)15
is inputted to the GNN network to generate ψ¯(t). Then the GNN transition tuple G(t),ψ¯(t),φ¯(t),G(t+1) is stored in the
GNN experience replay buffer B . For each iteration of vehicle’s SAC model train(cid:0)ing, a batch of training dat(cid:1)a is constructed
g
by randomly selecting M tuples from B . Let (s ,a ,r ,s′ )(i = 1,2,··· ,M) be the ith tuple in the mini-batch
ci ci,i ci,i ci,i ci,i
for vehicle c . For each tuple i, s is inputted into the actor network, producing the action a˜ . The gradient of the loss
i ci,i ci,i
function for β is calculated as
ci
∇ J (β )=
βci ci ci
. (27)
∇ E −β logπ (a˜ |s )−β H
βci aci,i∼πci,θci ci ci,θci ci,i ci,i ci ci
(cid:2) (cid:3)
Next s ci,i and a˜ ci,i are inputted into the two critic networks to obtain the action-value functions Q ϕc 1i(s ci,i,a˜ ci,i) and
Algorithm 2: GNN Combined SAC Training Stage Algorithm
Input: θglobal, ϕglobal, ϕglobal, βglobal, βglobal
a 1 2
Output: optimized
(θglobal)∗
a
1 Randomly initialize models: θ aglobal, ϕg 1lobal, ϕg 2lobal, βglobal, βglobal;
2 Initialize models: ϕ¯g 1lobal ←ϕg 1lobal, ϕ¯g 2lobal ←ϕg 2lobal;
3 Initialize the set of vehicles: V t ={};
4 for each vehicle c i in V t in parallel do
5 Update positions in vehicle set V t;
6 Update V t based on vehicles entering and exiting the RSU coverage area;
7 Generate a new graph data structure G(t);
8 Generate feature vector set ψ¯(t) based on (11);
9 for each vehicle c i in V t in parallel do
10 Observe state s ci(t) and choose action p ci(t);
11 for each vehicle c i in V t in parallel do
12 Calculate R ci(t) based on Eq. (17);
13 Calculate φ ci(t) based on Eq. (8);
14 Calculate φ(t) of the system based on Eq. (9);
15 for each vehicle c i in V t in parallel do
16 Calculate reward r ci(t) based on Eq. (21);
17 Store (s ci(t),p ci(t),r ci(t),s ci(t+1)) in B ci;
18 if |B ci| ≥ I ci then
19 for i from 1 to I ci do
20 Randomly sample M tuples (s ci,i,a ci,i,r ci,i,s′ ci,i) as training data from B ci;
21 Update β ci based on Eq. (27);
22 Update θ ci based on Eq. (28);
23 Update ϕc 1i and ϕc 2i based on Eq. (30);
24 if I ci%I˜ ci ==0 then
25 Update ϕ¯c 1i and ϕ¯c 2i based on Eq. (31);
26 Local federated aggregation;
27 Store G(t),ψ¯(t),φ¯(t),G(t+1) in B g;
28 if r%T g ==0 and |B g|≥I g then
(cid:0) (cid:1)
29 for i from 1 to I ci do
30 Randomly sample M g tuples G i,ψ¯ i,φ¯ i,G′ i as training data from B g;
31 Update θ g based on Eq. (32);
(cid:0) (cid:1)
32 Update ϕ g based on Eq. (33);
33 if I ci%I˜ g ==0 then
34 Update ϕ¯ g based on Eq. (34);16
Q ϕ 2ci (s ci,i,a˜ ci,i). Then the gradient of the loss function for actor network parameters θ ci is calculated as
∇ J (θ )=
θci ci ci
∇ β log π (a˜ |s ) +∇ f(ε ;s )· , (28)
θci ci θci ci,i ci,i θci ci ci,i
(cid:0) (cid:1)
∇ β log(π (a˜ |s ))−∇ Q (s ,a˜ )
aci,i ci φ ci,i ci,i a˜ci,i ci ci,i ci,i
(cid:0) (cid:1)
where ε is noise sampled from a multivariate normal distribution and f(ε ;s ) is a reparameterization trick function for
ci ci ci,i
a˜ ci,i. Q ci(s ci,i,a˜ ci,i) is calculated as min Q ϕc 1i (s ci,i,a˜ ci,i),Q ϕc 2i (s ci,i,a˜ ci,i) . Next, the algorithm computesthe gradients
for the two critic network parameters ϕcin and ϕci. For each tuple i in the mio ni-batch, the states s and actions a are
1 2 ci,i ci,i
inputtedintothetwocriticnetworks,producingtheaction-valuefunctionsQ ϕc 1i (s ci,i,a ci,i)andQ ϕc 2i (s ci,i,a ci,i).Additionally,
the next state s′ is inputted into the actor network to output a′ , which is then fed into the two target critic networks to
ci,i ci
output Q ϕ¯c 1i (s′ ci,i,a′ ci) and Q ϕ¯c 2i (s′ ci,i,a′ ci). The target action-value is then calculated as
Qˆ (s′ ,a′ )=−β log π (a′ |s′ )
ci ci,i ci ci θci ci ci,i
, (29)
+min Q ϕ¯c 1i (s′ ci,i,a′ ci),Q(cid:0) ϕ¯c 2i (s′ ci,i,a′ ci)(cid:1)
n o
The gradients for the loss functions of ϕci and ϕci are calculated as
1 2
∇ ϕc miJ ci(ϕc mi) =∇ ϕc miQ ϕc mi (s ci,i,a ci,i)·
, (30)
Q ϕc mi (s ci,i,a ci,i)−r ci,i+γQˆ ci(s′ ci,i,a′ ci) ,m∈{1,2}
(cid:16) (cid:17)
Using the Adam optimizer and based on the gradients ∇ βciJ ci(β ci), ∇ θciJ ci(θ ci), ∇ ϕc 1iJ ci(ϕc 1i), and ∇ ϕc 2iJ ci(ϕc 2i), the
parameters β , θ , ϕci, and ϕci are updated through gradient descent. Note that after every I˜ iterations of training, the
ci ci 1 2 ci
parameters of the two target critic networks are updated as
ϕ¯ci :=τ ϕci +(1−τ )ϕ¯ci,m∈{1,2}, (31)
m m m m m
where τ is constant satisfying τ ≪1.
m m
AsforGNN,whenthebufferB containsdatanumber|B |exceedingI ,thealgorithmtrainsandupdatestheGNNnetwork,
g g g
GNNcriticnetwork,andtargetGNNcriticnetworkeveryT timeslotsforI iterations.TheRSUrandomlyselectsM tuples
g g g
from B to form a training batch. Let G ,ψ¯,φ¯,G′ (i=1,2,··· ,M ) be the ith tuple in the RSU’s mini-batch. The loss
g i i i i g
function for the GNN network model p(cid:0)arameters θ i(cid:1)s defined as
g
1
Mg
L =− Q (G ,ψ¯), (32)
θg
M
ϕg i i
g
i=1
X
whereQ (G ,ψ¯)representsthefeaturevaluefunction,i.e.,theGNNcriticnetworkevaluatesthequalityofgeneratedfeature
ϕg i i
values of GNN network. The loss function for the GNN critic network model parameters ϕ is defined as
g
L =− 1
Mg
Q (G ,ψ¯)−φ¯ −γ·Q (G′ ,ψ¯′) 2 , (33)
ϕg M ϕg i i i ϕ¯g i i
g
i=1
X(cid:2) (cid:3)17
TABLE I: Environment parameters in the simulation.
Parameter Value Parameter Value
L 4 τ 0.02s
pmax 20Watts Dr 250m
dmin 0.1MB dmax 10MB
µ 0.2s Dc 100m
δ 0.9999 ω2 0.9999
Rmax 20000s Rtest 2000s
B 200MHz σ2 3.98×10−14 Watts
dcor 10 fc 28×109 Hz
c 3×108 m/s σs 2.2dB
TABLE II: SAC Hyperparameters
Parameter Value Parameter Value
αA 10−4 αC 10−3
D Ms 5 10 20
8
I I˜ cc ii 2 156
τ1 0.005 τ2 0.005
RewardScaling Factor 0.1 Activation Function ReLU
where ψ¯′ represents the node feature vector obtained by inputting G′ into the GNN network. After every I˜ iterations of
i i g
training, the GNN critic network is updated as
ϕ¯ :=τ ϕ +(1−τ )ϕ¯ , (34)
g g g g g
where τ is a constant satisfying τ ≪ 1. Finally, the algorithm runs to the next time slot. When the algorithm executes to
g g
Rmax, it indicates that the training has ended.
b) Testing Stage: The testing stage omits the critic network, target critic network, GNN network, GNN critic network
and the target GNN critic network. During testing stage, the optimal strategy is evaluated using the optimized parameters of
the actor network
(θglobal)∗
.
a
V. NUMERICAL SIMULATION AND ANALYSIS
In this section, we evaluate the performance of our proposed FGNN-MADRL scheme through simulation experiments and
discuss the results obtained. The simulation experiments are implemented using Python 3.7, and the simulation scenario is
constructed based on the system model. Table I lists the parameters used in the simulation environment. Both actor and
critic networks in SAC use four-layer fully connected DNNs, with two middle hidden layers each containing 256 neurons.
We consider the heterogeneity of each vehicle, meaning each vehicle has a differentnumber of SAC model training iterations
I ={5,10,20,40,50}.Each generated vehicle randomly selects a value from I as its iteration number. In addition, some
ci ci
other hyperparameters are adapted from [44]. Table II lists the remaining hyperparametersfor the SAC network. αA and αC
are the learning rates for the actor and critic networks, respectively.
TABLE III: GNN and GNN Critic Hyperparameters
Parameter Value Parameter Value
Dg 5000 M g 128
I g 256 Ici 5
αG 0.001 αGC 0.001
˜
Optimizer AdamOptimizer Ig 118
The GNN network uses a four-layer DNN, with the neuron numbers in the two middle hidden DNN layers being 128
and 64, respectively. The activation function for the hidden layers is the Tanh function. Both the GNN critic network and
the target GNN critic network also use four-layer DNNs, with 256 neurons in each of the two middle hidden layers. The
hyperparametersfor the GNN critic network are basically the same as those in the SAC model’s critic network. Table III lists
the hyperparametersusedin theGNN networkand GNNcritic network.αG andαGC arethe learningratesfortheGNN critic
network and the target GNN critic network, respectively.
A. Training Stage
400
350
Lg=20 (m)
300 Lg=25 (m)
Lg=50 (m)
250 Lg=100 (m)
200
150
100
50
0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
Traning time 1e4
Fig. 4: Learning curve
Fig. 4 displaysthe learningcurvesof the SAC modelovertime fordifferentroadsegmentlengthsL . A largerL indicates
g g
fewer nodes in the GNN. As seen in the figure, all learning curves gradually decline over time and eventually stabilize,
indicating that all models can converge. It is also observed that as L increases, the model converges more quickly. This is
g
because a smaller road segmentlength L leads to more nodes in the vehicle graph structure, making the GNN network more
g
complex. Therefore, more time is required to train the GNN network, which in turn increases the training time of the SAC
model. The convergencerate for L equal to 20 m is actually faster than for L equal to 25 m. This is because, when L is
g g g
20 m, there are more edges in the graph, which paradoxically facilitates the training of the model.
B. Testing Stage
To ensure more accurate test results, all simulation outcomesduring the testing stage are averaged after 50 experiments.To
validate the effectiveness of our proposed FGNN-MADRL scheme, we compare it with the following three algorithms:
• GlobalFederatedMulti-AgentReinforcementLearning(GFSAC):Inthismethod,agentsdonotperformlocalaggregation.
Oncea vehiclecompletesits localmodeltraining,ituploadsthemodeldirectlytothe RSU forglobalfederatedaveraging
aggregation.
• Local Federated Multi-Agent Reinforcement Learning (LFSAC): In this method, agents perform local aggregation. After
completing local model training, vehicles first average aggregate locally with other vehicles within their communication
range. This method does not use GNN to generate model aggregation weights. Vehicles upload their model to the RSU
for global aggregation just before leaving the RSU coverage area.
)sm(
IOA
metsys
egarevA19
• Game-based Dynamic Best Response for Cooperative Vehicle Task Offloading (GDBR) [15]: This method defines the
global AOI as the utility function of the game. It considers the best response probability of other vehicles offloading in
the previous time slot as the price function of the game. The method iteratively updates the best response probability for
vehicle offloading tasks based on the utility and price functions, eventually converting the best response probability into
the transmission power for the vehicle’s offloading tasks.
Lg=20 (m) Lg=20 (m)
350 Lg=25 (m) 3.5 Lg=25 (m)
Lg=50 (m) Lg=50 (m)
300 Lg=100 (m) 3.0 Lg=100 (m)
250 2.5
200 2.0
150 1.5
100 1.0
0.5
50
0.0
1 1 1 1 1 1 1 1 1 1 1 1
10 9 8 7 6 5 10 9 8 7 6 5
(a)VeAhicvlee Ararrgiveal rsaytes λte (vmeh/As) oI (b)VAehviceler aAgrrievasl ryastet eλm (vehp/os)w er
Fig. 5: The performance of different L trained models under various λ.
g
Fig.5(a)-5(b)showtheaverageAOIandpowerofallvehiclesunderdifferentvehiclearrivalratesandL .Inthisexperiment,
g
the speed of all vehicles is set to 30 Km/h. From Fig. 5(a), it is observed that the system’s average AOI increases with the
increase in vehicle density. This is because, as the number of vehicles increases, the communication interference between
them also increases, leading to a reduction in the vehicles’ transmission rate. It is also evident from Fig. 5(a) that the average
AOI increases with the increase in L . This is because a larger L results in fewer nodes in the vehicle graph structure,
g g
making the graph simpler and thus less effective at extracting vehicular feature information. Consequently, the model weights
producedare less reflective of vehicle information,reducingthe accuracyof modeltraining. In Fig. 5(b), as the vehiclearrival
rate increases, the power consumption of the solutions under different L trained models also increases. This is due to the
g
increasedinterferenceamongvehiclesastheir numbergrows,requiringmorepowerfortask offloadingin orderto reduceAoI.
It is also noticeable that as L increases, the average power consumption also increases. This is because a larger L leads to
g g
less information being extracted by the GNN, thus hindering the training of more effective model performance.
Fig. 6(a)-6(b) show the average AoI and power of all vehicles under differentL training models at variousvehicle speeds.
g
In this experiment,the vehicle arrivalrate is set to 1 vehiclesper second.It is also evidentfrom Fig. 6(a), the AoI for all four
8
different models increases as vehicle speed decreases. This is due to the increase in the number of vehicles on the road and
the resultingincrease ininterferencebetweenvehiclesastheirspeeddecreases.Furthermore,whenL isgreaterthan25m,the
g
average AoI increases with L , which can be attributed to the deterioration in the quality of model training. However, when
g
L is equal to 20m, the performance in terms of average AoI is worse than when L is 25m. This is because at L equal
g g g
to 20m, vehicles spend a very short time at nodes, preventing the GNN from effectively extracting vehicle information. This
issue is more pronounced at lower vehicle arrival rates and higher speeds.
)sm(
IOA
metsyS
egarevA
)W(
rewoP
metsyS
egarevA20
140 Lg=25 (m) Lg=20 (m)
Lg=20 (m)
2.0
Lg=25 (m)
120 Lg=50 (m) Lg=50 (m)
Lg=100 (m) Lg=100 (m)
100
1.5
80
60 1.0
40
0.5
20
0
0.0
80 70 60 50 40 30 80 70 60 50 40 30
(a)VeAhviceler Sapgeeed svymiasxt (eKmm/hA) oI (b) AVevhiecrlea Sgpeeesdy vsmi taxe (mKm/pho) wer
Fig. 6: The performance of different L trained models under various vi .
g max
LFSAC LFSAC
500 FGNN-MADRL 2.5 FGNN-MADRL
GFSAC GFSAC
GDBR GDBR
400 2.0
300 1.5
200 1.0
0.5
100
0.0
1 1 1 1 1 1 1 1 1 1 1 1
10 9 8 7 6 5 10 9 8 7 6 5
(a)VeAhicvlee Ararrgiveal rsaytes λte (vmeh/As) oI (b)VAehviceler aAgrrievasl ryastet eλm (vehp/os)w er
1e8
7
LFSAC
6 FGNN-MADRL
GFSAC
GDBR
5
4
3
2
1
0
1 1 1 1 1 1
10 9 8 7 6 5
(c) AVveehricaleg Aerrisvyals rtaetem λ (tvheho/us)g hout
Fig. 7: The performance of different schemes under various λ.
It can be seen from Fig. 6(b), as vehicle speed decreases, the power usage of the four models initially decreases and then
increases. This is because, with decreasing speed, vehicles spend more time at nodes, allowing the GNN to more effectively
extract vehicle information for training. However, when the vehicle speed is 30 Km/h, due to the very low speed, there is an
increase in the number of vehicles, which in turn increases interference. To reduce AoI, vehicles increase power to compete
for channel resources for task offloading.
Fig. 7(a)-7(c) present the average AOI, power and throughput of all vehicles under different schemes under various λ. In
)sm(
IOA
metsyS
egarevA
)sm(
IOA
metsyS
egarevA
)s/tib(
tuohguohT
metsyS
egarevA
)W(
rewoP
metsyS
egarevA
)W(
rewoP
metsyS
egarevA21
350
LFSAC
FGNN-MADRL
300 GFSAC
GDBR
250
200
150
100
50
0
80 70 60 50 40 30
(a)VeAhviceler Sapgeeed svymiasxt (eKmm/hA) oI
1e8
LFSAC LFSAC
FGNN-MADRL FGNN-MADRL
GFSAC 8 GFSAC
2.0 GDBR GDBR
6
1.5
4 1.0
0.5 2
0.0 0
80 70 60 50 40 30 80 70 60 50 40 30
(b) AVevhiecrlea Sgpeeesdy vsmi taxe (mKm/pho) wer (c) AvVeerhaicglee Spseyeds tvemi max (Ktmho/hu) ghout
Fig. 8: The performance of different schemes under various vi .
max
this experiment, the speed of all vehicles is set to 30 Km/h. Our FGNN-MADRL scheme is tested with L = 50m. From
g
Fig. 7(a), it can be observed that as λ increases, our FGNN-MADRL scheme has the smallest average AOI, demonstrating its
superiority.TheperformanceofGDBR isthe worst,asitmakesdecisionsbasedonprobabilities.TheperformancesofLFSAC
and GFSAC are better than GDBR, as these two schemesutilize RL methods, allowing some degreeof cooperativeoffloading
between vehicles. However, they perform worse than our FGNN-MADRL scheme because their model training involves only
average federated aggregation,lacking personalized features in the RL model. It can be seen from Fig. 7(b), with the increase
in λ, the average power for both FGNN-MADRL and GFSAC also increases. This is due to the greater interference among
an increasing number of vehicles, necessitating more power for transmission. The averagepower of FGNN-MADRL is higher
than that of GFSAC, as it allocates more power to achieve better average AOI performance as shown in Fig. 7(a). The power
consumptionfor LFSAC and GBDR does not increase with the rising vehicle arrival rate, indicating their inability to adapt to
scenarioswithhighvehicledensity.FromFig.7(c),theaveragethroughputforFGNN-MADRL,GFSAC,andGBDRdecreases
as λ increases. This is because the increase in vehicle numbers leads to more interference, thus reducing throughput. On the
otherhand,the averagethroughputfor LFSACincreaseswith the rising λ, due to thatvehiclesdo notoffloadtasks in a timely
manner.
Fig. 8(a)-8(c) display the average AOI, power, and throughput of all vehicles under different algorithms at varying vehicle
speeds. In this experiment, the vehicle arrival rate is set to 1 vehicles per second. Our FGNN-MADRL scheme is tested with
8
)W(
rewoP
metsyS
egarevA
)sm(
IOA
metsyS
egarevA
)s/tib(
tuohguohT
metsyS
egarevA22
L =50m.ItcanbeseenfromFig.8(a),theAOIforallfourmethodsincreasesasvehiclespeeddecreases,duetotheincrease
g
in the number of vehicles on the road and the resultant increase in interference between vehicles. FGNN-MADRL, LFSAC,
and GFSAC all exhibit good AOI performance, as these three schemes utilize RL methods to make appropriate decisions.
GDBR shows the worst performance in terms of AOI because it makes decisions based on probability.
In Fig. 8(b), as vehicle speed decreases, the average power of FGNN-MADRL gradually increases. This is because the
decrease in vehicle speed leads to an increase in the number of vehicles and thus increased interference. To reduce the AOI,
highertransmissionpoweris needed.Additionally,FGNN-MADRL usesthe leastaveragepower,indicatingthatitcan achieve
better AOI performance with less power, demonstrating the superiority of our scheme. The other three schemes do not show
a consistent trend of change in power with the reduction in vehicle speed, as they do not extract features.
In Fig. 8(c), the average throughput of FGNN-MADRL and GDBR gradually decreases as vehicle speed decreases, due to
the increased number of vehicles and interference. GDBR has the lowest average throughputbecause it allocates power based
on probability. LFSAC and GFSAC do not show a consistent trend of increase or decrease in throughput with the reduction
in vehicle speed, as they do not extract the vehicle’s road graph structure and thus cannot adapt to changes in vehicle speed.
FGNN-MADRL has the highest average throughput because it uses RL methods for cooperative decision-making, reducing
interferencebetweenvehiclesandmakingreasonablecooperativeallocationsbasedonthecurrentenvironment,therebyproving
the superiority of our scheme.
VI. CONCLUSIONS
In this paper, we addressed the problem of optimizing AoI in a multi-vehicle scenario. We proposed an innovative FGNN-
MADRL algorithm, which integrates GNN with MADRL to optimize AoI. The key characteristic of our model is that road
scenarios is first modeled as a graph and an effective FL framework that combines both distributed based on GNN and
centralized federated aggregation is employed. Additionally, we introduced a MADRL algorithm designed to reduce decision
complexity. Conclusions are drawn as follows:
• The structure of a GNN impacts the training of models. Both an excess or a deficiency of GNN nodes can hinder the
effective training of DRL. This is because more GNN nodes mean shorter vehicle dwell times at each node, while fewer
GNN nodes lead to a simpler network structure with weaker information extraction capabilities.
• Comparedto other distributedfederated algorithms,FGNN-MADRL can extractinformationaboutroad vehicles,such as
vehicle density, speed, and the status of model training. As a result, FGNN-MADRL adapts well to dynamic scenarios
and effectively reduces AoI.
• In contrast to other non-RL algorithms, FGNN-MADRL facilitates collaboration among vehicles, thereby reducing the
average age of information. This occurs because vehicles can observe local conditions and make sensible decisions.
Training models with the assistance of a GNN takes into account information from other vehicles.
REFERENCES
[1] P.Lang,D.Tian,X.Duan,J.Zhou,Z.Sheng,andV.C.M.Leung,“CooperativeComputationOffloadinginBlockchain-BasedVehicularEdgeComputing
Networks,”IEEETransactions onIntelligent Vehicles, vol.7,no.3,pp.783–798,2022.23
[2] W.Chen,L.Dai,K.B.Letaief,andZ.Cao,“AUnifiedCross-LayerFrameworkforResourceAllocationinCooperative Networks,”IEEETransactions
onWireless Communications, vol.7,no.8,pp.3000–3012, 2008.
[3] Y. J. Zhang and K. Letaief, “Adaptive resource allocation and scheduling for multiuser packet-based OFDM networks,” vol. 5, pp. 2949–2953 Vol.5,
2004.
[4] Q. Wu,S.Shi, Z.Wan,Q. Fan, P.Fan, andC. Zhang, “Towards V2IAge-aware Fairness Access: A DQN Based Intelligent Vehicular Node Training
andTestMethod,” ChineseJournalofElectronics, vol.32,no.6,pp.1230–1244,2023.
[5] J. Wang, D. Feng, S. Zhang, J. Tang, and T. Q. S. Quek, “Computation Offloading for Mobile Edge Computing Enabled Vehicular Networks,” IEEE
Access,vol.7,pp.62624–62632,2019.
[6] K. Xiong, C. Chen, G. Qu, P. Fan, and K. B. Letaief, “Group Cooperation With Optimal Resource Allocation in Wireless Powered Communication
Networks,”IEEETransactions onWireless Communications, vol.16,no.6,pp.3840–3853, 2017.
[7] T. Li, P. Fan, Z. Chen, and K. B. Letaief, “Optimum Transmission Policies for Energy Harvesting Sensor Networks Powered by a Mobile Control
Center,” IEEETransactions onWireless Communications, vol.15,no.9,pp.6132–6145, 2016.
[8] D. Long, Q. Wu, Q. Fan, P. Fan, Z. Li, and J. Fan, “A Power Allocation Scheme for MIMO-NOMA and D2D Vehicular Edge Computing based on
Decentralized DRL,”Sensors,vol.23,no.7,p.3449,2023.
[9] S. Wang, X. Zhang, Y. Zhang, L. Wang, J. Yang, and W. Wang, “A Survey on Mobile Edge Networks: Convergence of Computing, Caching and
Communications,” IEEEAccess,vol.5,pp.6757–6779, 2017.
[10] T.Taleb,K.Samdanis,B.Mada,H.Flinck,S.Dutta,andD.Sabella,“OnMulti-AccessEdgeComputing:ASurveyoftheEmerging5GNetworkEdge
CloudArchitecture andOrchestration,” IEEECommunications Surveys &Tutorials,vol.19,no.3,pp.1657–1681,2017.
[11] J. Zhang, P. Fan, and K. B. Letaief, “Network Coding for Efficient Multicast Routing in Wireless Ad-hoc Networks,” IEEE Transactions on
Communications, vol.56,no.4,pp.598–607, 2008.
[12] Q. Wu, S. Xia, Q. Fan, and Z.Li, “Performance Analysis of IEEE802.11 pfor Continuous Backoff Freezing in IoV,” Electronics, vol. 8, no. 12, p.
1404,2019.
[13] S.Kaul,R.Yates, andM.Gruteser,“Real-time status:Howoftenshouldoneupdate?” in2012Proceedings IEEEINFOCOM,2012,pp.2731–2735.
[14] Q. Wu and J. Zheng, “Performance Modeling and Analysis of the ADHOC MAC Protocol for Vehicular Networks,” Wireless Networks, vol. 22, pp.
799–812, 2016.
[15] Y. Wang, P.Lang, D.Tian, J.Zhou, X.Duan, Y. Cao, and D.Zhao, “A Game-Based Computation Offloading Method inVehicular Multiaccess Edge
ComputingNetworks,” IEEEInternetofThingsJournal, vol.7,no.6,pp.4987–4996, 2020.
[16] Z.Yao,J.Jiang,P.Fan,Z.Cao,andV.Li,“ANeighbor-Table-Based Multipath RoutinginAdhocNetworks,”vol.3,pp.1739–1743vol.3,2003.
[17] Q. Wu,W. Wang, P.Fan, Q. Fan, J. Wang, and K. B. Letaief, “URLLC-Awared Resource Allocation for Heterogeneous Vehicular Edge Computing,”
IEEETransactions onVehicular Technology, pp.1–16,2024.
[18] B.Hazarika, K.Singh,C.-P.Li,andS.Biswas,“Multi-AgentDRL-BasedComputationOffloadinginMultipleRIS-AidedIoVNetworks,”inMILCOM
2022-2022IEEEMilitary Communications Conference (MILCOM),2022,pp.1–6.
[19] X.Zhu,Y.Luo,A.Liu,M.Z.A.Bhuiyan, andS.Zhang,“Multi-Agent DeepReinforcement Learning forVehicular Computation Offloading inIoT,”
IEEEInternet ofThingsJournal, vol.8,no.12,pp.9763–9773, 2021.
[20] R.Lowe,Y.I.Wu,A.Tamar,J.Harb,O.PieterAbbeel,andI.Mordatch,“Multi-Agent actor-critic formixedCooperative-Competitive Environments,”
Advances inneuralinformationprocessing systems,vol.30,2017.
[21] T.Rashid,M.Samvelyan,C.S.DeWitt,G.Farquhar,J.Foerster,andS.Whiteson,“QMIX:Monotonicvaluefunctionfactorisationfordeepmulti-agent
reinforcement learning,” TheJournalofMachine LearningResearch,vol.21,no.1,pp.7234–7284,2020.
[22] J.Qi,Q.Zhou,L.Lei,andK.Zheng,“Federatedreinforcementlearning:Techniques,applications,andopenchallenges,”arXivpreprintarXiv:2108.11887,
2021.
[23] Q. Wu, S. Wang, H. Ge, P. Fan, Q. Fan, and K. B. Letaief, “Delay-Sensitive Task Offloading in Vehicular Fog Computing-Assisted Platoons,” IEEE
Transactions onNetwork andServiceManagement, vol.21,no.2,pp.2012–2026, 2024.
[24] F.Jing,W.Qiong, H.JunFeng,and F.Jing, “Optimal Deployment ofWireless MeshSensorNetworks BasedonDelaunay Triangulations,” vol. 1,pp.
V1–370–V1–374, 2010.
[25] M.Chen,Z.Yang,W.Saad,C.Yin,H.V.Poor,andS.Cui,“AJointLearningandCommunications FrameworkforFederatedLearningOverWireless
Networks,”IEEETransactions onWireless Communications, vol.20,no.1,pp.269–283,2021.24
[26] Q.Wu,Y.Zhao,Q.Fan,P.Fan,J.Wang,andC.Zhang,“Mobility-Aware Cooperative CachinginVehicularEdgeComputingBasedonAsynchronous
Federated andDeepReinforcement Learning,”IEEEJournal ofSelected TopicsinSignalProcessing,vol.17,no.1,pp.66–81,2023.
[27] B.McMahan,E.Moore,D.Ramage,S.Hampson,andB.A.yArcas,“Communication-efficient learningofdeepnetworksfromdecentralized data,”in
Artificial intelligence andstatistics. PMLR,2017,pp.1273–1282.
[28] Q.Wu,W.Wang,P.Fan,Q.Fan,H.Zhu,andK.B.Letaief,“CooperativeEdgeCachingBasedonElasticFederatedandMulti-AgentDeepReinforcement
LearninginNext-Generation Networks,”IEEETransactions onNetwork andService Management, pp.1–1,2024.
[29] Q.Wu,X.Wang,Q.Fan,P.Fan,C.Zhang,andZ.Li,“Highstableandaccuratevehicleselectionschemebasedonfederatededgelearninginvehicular
networks,” ChinaCommunications, vol.20,no.3,pp.1–17,2023.
[30] S.Rahmani,A.Baghbani,N.Bouguila,andZ.Patterson,“GraphNeuralNetworksforIntelligentTransportationSystems:ASurvey,”IEEETransactions
onIntelligent Transportation Systems,vol.24,no.8,pp.8846–8885,2023.
[31] Z. He, L. Wang, H. Ye, G. Y. Li, and B.-H. F. Juang, “Resource Allocation based on Graph Neural Networks in Vehicular Communications,” in
GLOBECOM2020-2020IEEEGlobalCommunications Conference, 2020,pp.1–5.
[32] Y.He,X.Zhong,Y.Gan,H.Cui,andM.Guizani,“ADDPGHybridofGraphAttentionNetworkandActionBranchingforMulti-ScaleEnd-Edge-Cloud
Vehicular Orchestrated TaskOffloading,”IEEEWireless Communications, vol.30,no.4,pp.147–153,2023.
[33] X. Zhou, M. Bilal, R. Dou, J. J. P. C. Rodrigues, Q. Zhao, J. Dai, and X. Xu, “Edge Computation Offloading With Content Caching in 6G-Enabled
IoV,”IEEETransactions onIntelligent Transportation Systems,pp.1–15,2023.
[34] Z. Liu, L. Huang, Z. Gao, M. Luo, S. Hosseinalipour, and H. Dai, “GA-DRL: Graph Neural Network-Augmented Deep Reinforcement Learning for
DAGTaskScheduling overDynamicVehicular Clouds,”arXivpreprintarXiv:2307.00777, 2023.
[35] B.Xiao,R.Li,F.Wang,C.Peng,J.Wu,Z.Zhao,andH.Zhang,“StochasticGraphNeuralNetwork-basedValueDecompositionforMARLinInternet
ofVehicles,”arXivpreprintarXiv:2303.13213, 2023.
[36] N.Chen,P.Zhang,N.Kumar,C.-H.Hsu,L.Abualigah,andH.Zhu,“Spectralgraphtheory-basedvirtualnetworkembeddingforvehicularfogcomputing:
Adeepreinforcement learning architecture,” Knowledge-Based Systems,vol.257,p.109931,2022.
[37] Y.Li,Q.Xie, W.Wang,X.Zhou,andK.Li,“GCN-Based TopologyDesignforDecentralized Federated Learning inIoV,”in202223rdAsia-Pacific
Network Operations andManagement Symposium (APNOMS). IEEE,2022,pp.1–6.
[38] P.Lang,D.Tian,X.Duan,J.Zhou,Z.Sheng,andV.C.M.Leung,“CooperativeComputationOffloadinginBlockchain-BasedVehicularEdgeComputing
Networks,”IEEETransactions onIntelligent Vehicles, vol.7,no.3,pp.783–798,2022.
[39] A.M.Zaki,S.A.Elsayed,K.Elgazzar,andH.S.Hassanein,“Multi-VehicleTaskOffloadingforCooperativePerceptioninVehicularEdgeComputing,”
inICC2023-IEEEInternational Conference onCommunications, 2023,pp.1786–1791.
[40] Y. Wang, P.Lang, D.Tian, J.Zhou, X.Duan, Y. Cao, and D.Zhao, “A Game-Based Computation Offloading Method inVehicular Multiaccess Edge
ComputingNetworks,” IEEEInternetofThingsJournal, vol.7,no.6,pp.4987–4996, 2020.
[41] J. Zhou, D. Tian, Z. Sheng, X. Duan, and X. Shen, “Distributed Task Offloading Optimization With Queueing Dynamics in Multiagent Mobile-Edge
ComputingNetworks,” IEEEInternetofThingsJournal, vol.8,no.15,pp.12311–12328,2021.
[42] M. Z. Alam and A. Jamalipour, “Multi-Agent DRL-Based Hungarian Algorithm (MADRLHA) forTask Offloading in Multi-Access Edge Computing
Internet ofVehicles (IoVs),”IEEETransactions onWireless Communications, vol.21,no.9,pp.7641–7652, 2022.
[43] L. He, J. Zhao, X. Sun, and D. Zhang, “Dynamic Task Offloading for Mobile Edge Computing in Urban Rail Transit,” in 2021 13th International
Conference onWireless Communications andSignalProcessing(WCSP),2021,pp.1–5.
[44] T. Haarnoja, A. Zhou, K. Hartikainen, G. Tucker, S. Ha, J. Tan, V. Kumar, H. Zhu, A. Gupta, P. Abbeel et al., “Soft actor-critic algorithms and
applications,” arXivpreprintarXiv:1812.05905, 2018.