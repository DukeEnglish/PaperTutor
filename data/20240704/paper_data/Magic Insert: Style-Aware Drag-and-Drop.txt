Magic Insert: Style-Aware Drag-and-Drop
NatanielRuiz YuanzhenLi NealWadhwa YaelPritch
MichaelRubinstein DavidE.Jacobs ShlomiFruchter
Google
MagicInsert.github.io
Figure1:UsingMagicInsertweareableto,forthefirsttime,drag-and-dropasubjectfromanimagewithan
arbitrarystyleontoanothertargetimagewithavastlydifferentstyleandachieveastyle-awareandrealistic
insertionofthesubjectintothetargetimage.
Abstract
WepresentMagicInsert,amethodfordragging-and-droppingsubjectsfroma
user-providedimageintoatargetimageofadifferentstyleinaphysicallyplausible
mannerwhilematchingthestyleofthetargetimage. Thisworkformalizesthe
problem of style-aware drag-and-drop and presents a method for tackling it by
addressing two sub-problems: style-aware personalization and realistic object
insertion in stylized images. For style-aware personalization, our method first
fine-tunes a pretrained text-to-image diffusion model using LoRA and learned
texttokensonthesubjectimage,andtheninfusesitwithaCLIPrepresentation
ofthetargetstyle. Forobjectinsertion,weuseBootstrappedDomainAdaption
toadaptadomain-specificphotorealisticobjectinsertionmodeltothedomainof
diverseartisticstyles. Overall,themethodsignificantlyoutperformstraditional
approaches such as inpainting. Finally, we present a dataset, SubjectPlop, to
facilitateevaluationandfutureprogressinthisarea.
4202
luJ
2
]VC.sc[
1v98420.7042:viXra1 Introduction
Largetext-to-imagemodelshaverecentlymadesignificantprogressingeneratinghigh-qualityimages.
However,tomakethesemodelstrulyuseful,controllabilityisessential. Usershavediverseneedsand
wanttointeractwiththesemodelsindifferentwaysdependingontheirspecificusecase. Influential
workhasbeendonetoenablecontrollabilityinthesenetworks,robustlyaddressingfoundational
applicationsandcontrolssuchassubjectpersonalization,stylelearning,layoutcontrols,andsemantic
controls. Despitethisprogress,thefullpotentialofthesepowerfullargemodelshasnotbeenfully
realized. Some applications that seemed clearly out of reach just a couple of years ago are now
possiblewithcarefulapproaches.
We present one such application: style-aware drag-and-drop. We formalize this problem and
introduceMagicInsert,ourmethodtotackleit,whichshowsstrongperformancecomparedtocurrent
baselines. Onemightinitiallyconsideraddressingstyle-awaredrag-and-dropbytryingtoinpaint
usingastylizedsubject,forexamplebycombiningDreambooth[30],StyleDrop[38],andinpainting.
Wefindthatapproachesofthistypeareveryexpensiveandachievesubparresults.
IndevelopingMagicInsert,weaddresstwointerestingsub-problems: style-awarepersonalization
andrealisticobjectinsertioninstylizedimages. Forstyle-awarepersonalization,therehavebeen
attemptsonadjacentproblems,suchaslearningastyleandthenrepresentingaspecificsubjectinthat
style[38,10],orcombiningpre-trainedcustomstyleandsubjectmodels[36,5]. Recentstylework
suggeststhatfaststylelearningispossible,butfastlearningofasubject,includingalltheintricacies
of identity, is a much harder problem that has arguably not been solved yet [52, 31, 7, 46]. We
proposeleveraginglearningsfrombothdomainsandsettleonasolutionthatusesadapterinjectionof
stylepairedwithsubject-learningintheembeddingandweightspaceofadiffusionmodel.
Onekeyideaweproposeistonotattemptinpaintingdirectlyintoanimageafterachievingstyle-aware
personalization. Instead,forbestresults,wefirstgenerateahigh-qualitysubjectandtheninsertthat
subjectintothetargetimage. Toachieveourresults,weintroduceaninnovationcalledBootstrap
DomainAdaptation,thatallowsprogressiveretargetingofamodel’sinitialdistributiontoatarget
distribution. Weapplythisideatoadaptasubjectinsertionnetworkthathasbeentrainedonreal
imagestoperformwellonthestylizedimagedomain,enablingtheinsertionofourgeneratedstylized
subjectintothebackgroundimage.
Ourmethodallowsthegeneratedoutputtoexhibitstrongadherencetothetargetstylewhilepreserving
the essence and identity of the subject, and for realistic insertion of the stylized subject into the
generatedimage. Themethodalsoprovidesflexibilityintermsofthedegreeofstylizationdesired
andhowcloselytoadheretotheoriginalsubject’sspecificdetailsandpose(orallowmorenoveltyin
thegeneration).
Insummary,weproposethefollowingcontributions:
• Weproposeandformalizetheproblemofstyle-awaredrag-and-drop,whereasubject(a
characterorobject)isdraggedfromoneimageintoanother. Specifically,inourproblem
formulation the subject reference image and the target image may be in vastly different
styles,andtheplausibilityandrealismofthesubjectinsertionisimportant.
• Inordertoencourageexplorationintothisnewproblem,wepresentSubjectPlop,adataset
ofsubjectsandbackgroundsthatspanwidelydifferentstylesandoverallsemantics. Wewill
releasethisdatasetforpublicuse,aswellasourevaluationsuite.
• WeproposeMagicInsert,amethodtotacklethestyle-awaredrag-and-dropproblem. Our
method is composed of a style-aware personalization component and a style-consistent
drag-and-dropcomponent.
• Forstyle-awarepersonalization,wedemonstratestrongandconsistentresultsusingsubject-
learningintheembeddingandweightspaceofapre-traineddiffusionmodels,alongwith
adapterinjectionofstyle.
• Fordrag-and-drop,weproposeBootstrappedDomainAdaptation,amethodthatallows
forprogressiveretargetingofamodel’sinitialdistributionuntoatargetdistribution. We
usethistoadaptanobjectinsertionnetworktrainedonrealimagestoperformwellonthe
stylizedimagedomain.
22 RelatedWork
Text-to-ImageModels Recenttext-to-imagemodelssuchasImagen[34],DALL-E2[26],Stable
Diffusion(SD)[28],Muse[3]andParti[53]havedemonstratedremarkablecapabilitiesingenerating
high-qualityimagesfromtextdescriptions. Theyleverageadvancementsindiffusionmodels[37,11,
40]andgenerativetransformers.OurworkbuildsontopofSDXL[24]andtheLDMarchitecture[28].
Image Inpainting The task of filling masked pixels of a target image has been explored using
awiderangeofapproaches: Generativeadversarialnetworks[8]e.g. [23,13,17,22,27,54]and
end-to-endlearningmethods[14,16,42,49]. Morerecently,diffusionmodelsenabledsignificant
progress [20, 19, 47, 33, 2]. Such inpainting methods are a precursor to many object insertion
approaches.
GenerativeObjectInsertion Theproblemofinsertinganobjectintoanexistingscenehasbeen
originallyexploredusingGenerativeAdversarialNetworks(GANs)[8]. [15]breaksdownthetask
intotwogenerativemodules,onedetermineswheretheinsertedobjectmaskshouldbeandtheother
determineswhatthemaskshapeandpose. ShadowGAN[56]addressestheneedtoaddashadowcast
bytheinsertedobject,leveraging3Drenderingfortrainingdata. Morerecentworksusediffusion
models. Paint-By-Example[51]allowsinpaintingamaskedareaofthetargetimagewithreference
totheobjectsourceimage,butitonlypreservessemanticinformationandhaslowfidelitytothe
originalobject’sidentity. Recentworkalsoexploresswappingobjectsinascenewhileharmonizing,
butfocusesonswappingareasoftheimagewhichwerepreviouslypopulated[9]. Therealsoexists
anarrayofworkthatfocusesoninsertingsubjectsorconceptsinasceneeitherbyinpainting[32,18]
or by other means [41, 35] - these do not handle large style adaptation and inpainting methods
usuallysufferfromproblemswithinsertionsuchasbackgroundremoval,incompleteinsertionand
lowqualityresults. ObjectDrop[48]trainsadiffusionmodelforobjectremoval/insertionusinga
counterfactualdatasetcapturedintherealworld. Thetrainedmodelcaninsertsegmentedobjects
intorealimageswithcontextualcuessuchasshadowsandreflections. Webuilduponthisnoveland
incrediblyusefulparadigmbytacklingthechallengingdomainofstylizedimagesinstead.
Personalization,StyleLearningandControllability Text-to-imagemodelsenableuserstopro-
videtextpromptsandsometimesinputimagesasconditioninginput,butdonotallowforfine-grained
controloversubject,style,layout,etc. TextualInversion[6]andDreamBooth[30]arepioneering
works that demonstrated personalization of such models to generate images of specific subjects,
givenfewcasualimagesasinput. TextualInversion[6]andfollow-uptechniquessuchasP+[44]
optimizetextembeddings,whileDreamBoothoptimizesthemodelweights. Thistypeofworkhas
alsobeenextendedto3Dmodels[25],scenecompletion[43]andothers. Therealsoexistsworkon
fastsubject-drivengeneration[4,31,7,46,1]. Otherworkallowsforconditioningonnewmodalities
suchasControlNet[55]andonimagefeatures(IP-Adapter[52]). Thereisabodyofworkthatdives
more deeply into style learning and generating consistent style as well with StyleDrop [38] as a
pioneer,withnewerworkthatachievesfaststylization[36,45,10,29],orcombinessubjectmodels
withstylemodelslikeZipLoRA[36]andothers[5]. OurworkleveragesideasfromTextualInversion,
DreamBoothandIP-Adaptertounlockstyle-awarepersonalizationpriorandcombineitwithsubject
insertion.
3 Method
3.1 Style-AwareDrag-and-DropProblemFormulation
Weformalizethestyle-awaredrag-and-dropproblemasfollows. LetI andI denotethespaceof
s t
subjectandtargetimages,respectively. Thespaceofsubjectimagesconsistsofimagesofsolelythe
subjectinfrontofplainbackgrounds. Givenasubjectimagex ∈I andatargetimagex ∈I ,our
s s t t
goalistogenerateanewimagexˆ ∈I suchthat:
t t
1. Thesubjectfromx isinsertedintoxˆ inasemanticallyconsistentandrealisticmanner,
s t
accountingforfactorssuchasocclusion,shadows,andreflections.
2. The inserted subject in xˆ adopts the style characteristics of the target image x while
t t
preservingitsessentialidentityandattributesfromx .
s
3Figure2:Style-AwarePersonalization:Togenerateasubjectthatfullyrespectsthestyleofthetargetimage
whilealsoconservingthesubject’sessenceandidentity,we(1)personalizeadiffusionmodelinbothweight
andembeddingspace,bytrainingLoRAdeltasontopofthepre-traineddiffusionmodelandsimultaneously
trainingtheembeddingoftwotexttokensusingthediffusiondenoisingloss(2)usethispersonalizeddiffusion
modeltogeneratethestyle-awaresubjectbyembeddingthestyleofthetargetimageandconductingadapter
style-injectionintoselectupsamplinglayersofthemodelduringdenoising.
Figure3: SubjectInsertion: Inordertoinsertthestyle-awarepersonalizedgeneration,we(1)copy-pastea
segmentedversionofthesubjectontothetargetimage(2)runoursubjectinsertionmodelonthedeshadowed
image-thiscreatescontextcuesandrealisticallyembedsthesubjectintotheimageincludingshadowsand
reflections.
Formally,weaimtolearnafunctionh:I ×I →I thatsatisfies:
s t t
h(x ,x )=xˆ s.t. xˆ ∼p(xˆ |x ,x ) (1)
s t t t t t s
where p(xˆ |x ,x ) represents the conditional distribution of the output image given the subject
t t s
and target images. This distribution encapsulates the desired properties of semantic consistency,
realisticinsertion,andstyleadaptation. Tolearnthefunctionh,wedecomposetheprobleminto
twosub-tasks: style-awarepersonalizationandrealisticobjectinsertioninstylizedimages. Style-
awarepersonalizationfocusesongeneratingasubjectthatadherestothetargetimage’sstylewhile
maintainingitsidentity. Realisticobjectinsertionaimstoseamlesslyintegratethestylizedsubject
intothetargetimage,accountingforthescene’sgeometryandlightingconditions. Byaddressing
thesesub-tasks,wecaneffectivelysolvethestyle-awaredrag-and-dropproblemandgeneratevisually
coherentandcompellingresults.Inthefollowingsections,wepresentourdatasetandthecomponents
ofourproposedmethod.
4Figure 4: Bootstrapped Domain Adaptation: Surprisingly, a diffusion model trained for subject inser-
tion/removalondatacapturedintherealworldcangeneralizetoimagesinthewiderstylisticdomaininalimited
fashion.Weintroducebootstrappeddomainadaptation,whereamodel’seffectivedomaincanbeadaptedby
usingasubsetofitsownoutputs.(left)Specifically,weuseasubjectremoval/insertionmodeltofirstremove
subjectsandshadowsfromadatasetfromourtargetdomain.Then,wefilterflawedoutputs,andusethefiltered
setofimagestoretrainthesubjectremoval/insertionmodel. (right)Weobservethat,theinitialdistribution
(blue)changesaftertraining(purple)andinitiallyincorrectlytreatedimages(redsamples)aresubsequently
correctlytreated(green). Whendoingbootstrappeddomainadaptation,wetrainononlytheinitiallycorrect
samples(green).
3.2 SubjectPlopDataset
Tofacilitatetheevaluationofthestyle-awaredrag-and-dropproblem,weintroducetheSubjectPlop
datasetandmakeitpubliclyavailable. Asthisisanovelproblem,adedicateddatasetiscrucialfor
enablingtheresearchcommunitytomakeprogressinthisarea.
SubjectPlopconsistsofadiversecollectionofsubjectsgeneratedusingDALL-E3[26]andback-
groundsgeneratedusingtheopen-sourceSDXLmodel[24]. Thedatasetincludesvarioussubject
types, suchasanimalsandfantasycharacters, andbothsubjectsandbackgroundsexhibitawide
rangeofstyles,including3D,cartoon,anime,realistic,andphotographic. Thediversityincolorhues
andlightingconditionsensurescomprehensivecoverageofdifferentscenariosforevaluation. Noreal
peoplearerepresentedinthedataset.
Thedatasetcomprises20distinctbackgroundsand35uniquesubjects,allowingforatotalof700
possiblesubject-backgroundpairs. Theentiredatasetismeantforevaluationofthetask. Thisrichset
oftestcasesenablestheassessmentofperformanceandgeneralizationcapabilitiesofstyle-aware
drag-and-droptechniques. ByintroducingSubjectPlop,weaimtoprovideastandardizedbenchmark
forevaluatingandcomparingdifferentapproachestothestyle-awaredrag-and-dropproblem. We
believethisdatasetwillserveasavaluableresourceforresearchersandpractitionersworkingin
imagemanipulationandgeneration,fosteringfurtheradvancementsinthisarea.
3.3 Style-AwarePersonalization
Our style-aware personalization approach is illustrated in Figure 2. Let f denote a pre-trained
θ
diffusionmodelwithparametersθ. Givenasubjectimagex ∈I ,ourmethodpersonalizesf on
s s θ
x inboththeweightandembeddingspace,similartoDreamBooth[30]andTextualInversion[6].
s
In the first step, we train LoRA [12] (Low-Rank Adaptation) deltas ∆ to produce an efficiently
θ
fine-tunedadaptedmodelf whereθ′ =θ+∆ ,whilepreservingthemodel’soriginalcapabilities.
θ′ θ
Simultaneously,welearnembeddingse ,e ∈Rdfortwopersonalizedtexttokens,wheredisthe
1 2
embeddingdimensionality. Weusetwolearnedembeddingssincewefoundbetterperformancefor
5bothsubjectpreservationandeditabilityinthisconfiguration. TheLoRAdeltasandandembeddings
arejointlytrainedusingthediffusiondenoisingloss:
L =E (cid:2) ∥ϵ−ϵ (xt,t,[e ;e ])∥2(cid:3) (2)
joint t,ϵ θ′ s 1 2 2
√ √
wheret ∼ U(0,1), ϵ ∼ N(0,I), xt = α¯ x + 1−α¯ ϵ, andϵ isthenoisepredictionofthe
s t s t θ′
adapted model f . The joint optimization of ∆ , e , and e is performed using the loss L .
θ′ θ 1 2 joint
Thesepersonalizedtexttokens[e ;e ]serveasacompactrepresentationofthesubject’sidentity. By
1 2
performingembeddingandweight-spacelearningsimultaneously,Wefindthatperformingembedding
andweight-spacelearningsimultaneously,withtwotexttokens,capturesthesubject’sidentitymore
stronglywhileallowingsufficienteditabilitytointroducethetargetstyle.
In the second step, we leverage the personalized diffusion model f to generate the style-aware
θ′
subjectxˆ . Toinfusethetargetimagex ’sstyleintoxˆ ,weemploystyleinjection. Specifically,we
s t s
generateastyleembeddinge =CLIP(x )ofx usingafrozenCLIPencoderCLIP. Wethenusea
t t t
frozenIP-Adaptermodelvtoinjecte intoasubsetoftheUNetblocksoff duringinference:
t θ′
xˆ =f ([e ;e ],v(e )) (3)
s θ′ 1 2 t
ThisapproachissimilartoInstantStyle[45],withinjectionintotheupsampleblockthatisadjacent
tothemidblock,withsomekeydifferencesbeingomittingcontent/styleembeddingseparation,and
injectingintoapersonalizedmodel. Tothebestofourknowledge,ourcentralideaofcombining
adapter injection and personalized models remains unexplored in the published literature. This
ensuresthatxˆ maintainsthesubject’sidentitywhileadoptingx ’sstylecharacteristics.
s t
Bycombiningstyle-awarepersonalizationwithstyleinjection,ourmethodgeneratessubjectsthat
harmoniouslyblendintothetargetimagewhileretainingtheiressentialidentity,effectivelytackling
thefirstchallengeofstyle-awaredrag-and-dropandenablingthecreationofvisuallycoherentand
style-consistentresults.
3.4 BootstrappedDomainAdaptationforSubjectInsertion
In this section, we address the problem of subject insertion and propose a novel solution using
bootstrappeddomainadaptation. Weformalizetheconceptofbootstrappeddomainadaptationand
describe the dataset used for this purpose. Subject insertion is a crucial component of the style-
awaredrag-and-dropproblem,asitinvolvesseamlesslyintegratingastylizedsubjectintoatarget
backgroundimage. Whilediffusion-basedinpaintingapproaches[21,34,28]canbeusedforthis,
theystillfacechallengessuchasgeneratingcontentinsmoothregions,producingincompletefigures,
erasingobjectsbehindinsertedsubjects,andhavingproblemswithboundaryharmonization. We
takeasimplerandstrongerapproach,whichistoinsertthesubjectbycopyingandpastingitintothe
targetimage,andthensubsequentlygeneratingcontextualcuessuchasshadowsandreflections[48]
inasecondstep. Unfortunately,existingsubjectinsertionmodelsaretrainedondatacapturedinthe
realworld,severelylimitingtheirabilitytogeneralizetoimageswithdiverseartisticstyles.
LetD denotethedistributionofreal-worldimagesandD denotethedistributionofstylizedimages.
r s
ExistingsubjectinsertionmodelsaretrainedonsamplesfromD ,butourgoalistoadaptthemto
r
performwellonsamplesfromD . Toovercomethislimitation,weintroducebootstrappeddomain
s
adaptation,atechniquethatenablesamodeltoadaptitseffectivedomainbyleveragingasubsetof
itsownoutputs. AsillustratedinFigure4(left),weemployasubjectremoval/insertionmodelg
θ
trainedonreal-data([48]inourcase)tofirstremovesubjectsandshadowsfromadatasetS ∼D
s
belongingtoourtargetdomain. Subsequently,wefilteroutflawedoutputsandobtainafilteredsetof
imagesS′ ⊆S,whichweusetoretrainthesubjectremoval/insertionmodel. Filteringcanbedone
usinghumanfeedbackorautomaticallygivenaqualityevaluationmodule.
Thebootstrappeddomainadaptationprocesscanbeformalizedasfollows:
ω =argminE L(g (x),y) (4)
(x,y)∼S′ ω
ω
whereω denotestheadaptedmodelparameters,Listhediffusiondenoisingloss,and(x,y)arepairs
ofinputimagesandcorrespondingsubjectremoval/insertiongroundtruthsfromthefilteredsetS .
f
Theconceptofbootstrappeddomainadaptationisbasedonthesurprisingobservationthatadiffusion
modeltrainedforsubjectinsertion/removalonreal-worlddatacangeneralizetoawiderstylistic
domaintoalimitedextent. Byretrainingthemodelonitsownfilteredoutputs,wecaneffectively
adaptitsdomaintobetterhandlestylizedimages.
6Figure5:ResultsGallery:ExamplesofourMagicInsertmethodfordifferentsubjectsandbackgroundswith
vastlydifferentstyles.
7Figure6:LLM-GuidedAffordances:ExamplesofanLLM-guidedposemodificationforMagicInsert,with
theLLMsuggestingplausibleposesandenvironmentinteractionsforareasoftheimageandMagicInsert
generatingandinsertingthestylizedsubjectwiththecorrespondingposeintotheimage.
Table1:SubjectFidelityComparisons.Wecompareourmethodforsubjectfidelity(DINO,CLIP-I,CLIP-T
Simple,CLIP-TDetailed)acrossdifferentmethods.Ourmethodvariantsshowhighsubjectfidelity.
Method DINO↑ CLIP-I↑ CLIP-TSimple↑ CLIP-TDetailed↑ OverallMean↑
StyleAlignPrompt 0.223 0.743 0.266 0.299 0.383
StyleAlignControlNet 0.414 0.808 0.289 0.294 0.451
InstantStylePrompt 0.231 0.778 0.283 0.300 0.398
InstantStyleControlNet 0.446 0.806 0.281 0.283 0.454
Ours 0.295 0.829 0.276 0.293 0.423
OursControlNet 0.514 0.869 0.289 0.308 0.495
Figure4(right)demonstratestheeffectofbootstrappeddomainadaptationonthemodel’sdistribution.
Theinitialdistribution,representedasp (x),evolvesaftertraining,becomingp (x). Imagesthat
ω ω∗
wereinitiallytreatedincorrectly,shownassamplesfromD \S′,aresubsequentlyhandledcorrectly,
s
asindicatedbytheirinclusioninS′. Duringthebootstrappeddomainadaptationprocess,wetrain
themodelonlyontheinitiallycorrectsamplesfromS′tofurtherrefineitsperformanceonthetarget
domain. Severalstepsofbootstrappeddomainadaptationcanbeperformed,furtherenhancingthe
model’sperformance. Inourworkwefindthatonestepsuffices,withasmallsetofsamples(around
50). Figure7showsresultswithandwithoutbootstrapdomainadaptation.
Tofacilitatethebootstrappeddomainadaptationprocess,wecurateadatasetS specificallytailored
to this task. The dataset comprises a diverse range of stylized images, selected to represent the
targetdomainD . Inourcase,thisdatasetisconstructedbysamplingfromdifferenttext-to-image
s
generativemodelswithdiversepromptsthatelicitprominentsubjectswithshadowsandreflectionsin
avarietyofglobalstyles. Byfinetuningthesubjectremoval/insertionmodelonthisdatasetusingthe
bootstrappeddomainadaptationtechnique,weenableittoeffectivelyhandlesubjectinsertioninthe
contextofstyle-awaredrag-and-drop.
8Table2:StyleFidelityComparisons.Wecompareourmethodforstylefidelity(CLIP-I,CSD,CLIP-T).Our
methodvariantsshowstrongstyle-following.
Method CLIP-I↑ CSD↑ CLIP-T↑ OverallMean↑
StyleAlignPrompt 0.570 0.150 0.248 0.323
StyleAlignControlNet 0.575 0.188 0.274 0.345
InstantStylePrompt 0.583 0.312 0.276 0.390
InstantStyleControlNet 0.588 0.334 0.279 0.400
Ours 0.560 0.243 0.268 0.357
OursControlNet 0.575 0.294 0.274 0.381
Table3:ImageRewardMetricComparisons.WecomparedifferentmethodsusingtheImageRewardmetric,
whichcorrelateswithhumanpreferenceforaestheticevaluation.Higherscoresindicatebetterperformance.Our
variantsoutperformallbenchmarks
Method ImageRewardScore↑
StyleAlignPrompt -1.1942
StyleAlignControlNet -0.5180
InstantStylePrompt -0.4638
InstantStyleControlNet -0.2759
Ours -0.2108
OursControlNet -0.1470
4 Experiments
Inthissection,weshowexperimentsandapplications. Ourfullmethodenablesinsertionofarbitrary
subjectsintoimageswithdiversestyles,withalargeexpanseoftext-guidedsemanticmodifications.
Specifically,notonlydoesthesubjectretainitsidentityandessencewhileinheritingthestyleofthe
targetimage,butwecanmodifykeysubjectcharacteristicssuchastheposeandothercoreattributes
suchasaddingaccessories,changingappearance,changingshapes,orevenspecieshybrids(Figure9).
ThesechangescanbeintegratedwithcomponentssuchasLLMsthatallowforautomaticaffordances
andenvironmentinteractions(Figure6).
4.1 Style-AwareDrag-and-DropResults
Magic Insert Results We present a gallery of qualitative results in Figure 5 to highlight the
effectivenessandversatilityofourmethod. Theexamplesspanawiderangeofsubjectsandtarget
backgroundswithvastlydifferentartisticstyles,fromphotorealisticscenestocartoons,andpaintings.
Forstyle-awarepersonalizationweusetheSDXLmodel[24],andforsubjectinsertionweuseour
trainedsubjectinsertionmodelbasedonalatentdiffusionmodelarchitecture.
Ineachcase,ourmethodsuccessfullyextractsthesubjectfromthesourceimageandblendsitinto
thetargetbackground,adaptingthesubject’sappearancetomatchthebackground’sstyle. Notice
howtheinsertedsubjectstakeonthecolors,textures,andstylisticelementsofthetargetimages. The
coherentshadowsandreflectionsenhancetheplausibilityoftheresults.
LLM-GuidedAffordances Our proposedstyle-awarepersonalization methodallowsfor large
changesincharacterpose,withsupportfromthediffusionmodelprior. UsingandLLM(ChatGPT
4o)weareabletogenerateLLM-guidedaffordancesfordifferentsubjects,byfeedinganinstruction
prompt,thefullbackgroundimage,andthesectionofthebackgroundimageinwhichthecharacter
willbepositioned.UsingtheseLLMsuggestions,wecangeneratethecharacterfollowingtheseposes
andenvironmentinteractionsandinsertitintheappropriatespace. Withthis,weshowinFigure6a
firstattemptatthepreviouslyunassailabletaskofinsertingsubjectsintoimagesrealisticallywith
automaticinteractionswiththescene.
BootstrapDomainAdaptation WeshowinFigure4asamplecaseofsubjectinsertionwithan
insertionmodelthatistrainedonrealimageswithoutadaptation,andonthesamemodelthatuses
ourproposedbootstrapdomainadaptationonasmallsetof50samples. Insertionwithoutbootstrap
domainadaptationgeneratessubparresults,withproblemssuchasmissingshadows,reflectionsand
evenaddeddistortions.
9Table4:UserStudy.Thisstudyevaluatesourmethodagainsttwodifferentbaselines(StyleAlignControlNet
andInstantStyleControlNet)basedonsubjectidentity,stylefidelity,andrealisticinsertion.Participantsranked
eachmethodbypreference.
Method UserPreference↑
OursoverStyleAlignControlNet 85%
OursoverInstantStyleControlNet 80%
Figure7: BootstrapDomainAdaptation: Insertingasubjectwiththepre-trainedsubjectinsertionmodule
withoutbootstrapdomainadaptationgeneratessubparresults,withfailuremodessuchasmissingshadowsand
reflections,oraddeddistortionsandartifacts.
Semantic Modifications of Subject Our method inherits all benefits of DreamBooth [30] and
thusallowsformodificationofsubjectcharacteristicssuchaspose,addingaccessories,changing
appearance,shapeshiftingandhybrids. WeshowsomeexamplesinFigure9. Thegeneratedsubjects
canthenbeinsertedintothebackgroundimage.
Editability/FidelityTradeoff Ourmethod(w/oControlNet)alsoinheritsDreamBooth’seditability
/fidelitytradeoff. Specifically,thelongerthepersonalizationtraining,thestrongerthesubjectfidelity
butthelessertheeditability. ThisphenomenonisshowninFigure10. Inmostcasesasweetspotcan
befoundfordifferentapplications. Forourworkweuse600iterationswithbatchsize1,alearning
rateof1e-5andweightdecayof0.3fortheUNet. Wealsotrainthetextencoderwithalearningrate
of1e-3andweightdecayof0.1.
4.2 Comparisons
Hereweintroducebaselines,aswellasquantitativeandqualitativecomparisons,aswellasauser
study.Specifically,ourproposedbaselinesutilizetheStyleAlign[10]andInstantStyle[45]stylization
methods,whichcangenerateimagesinreferencestylesgiveneitherinversionorembeddingofthe
referenceimage. Wecombinethesemethodswitheithersufficientlydetailedpromptingguidedbya
VLM(ChatGPT4)oredge-conditionedControlNet. ForpromptingweusetheVLMtodescribethe
subjectswhileeliminatingstylecues,andforedge-conditioningweuseCannyedgesextractedfrom
thesubjectreferenceimagestoguidethestylizedoutputsusingControlNet.
BaselineComparisons Werunstudiesinordertocomparetheperformanceofsubjectstylization
fordifferentbaselinesandourstyle-awarepersonalizationmethod. Westudytheperformanceof
thesemethodsonsubjectfidelity,stylefidelity,andhumanpreference.
Forsubjectfidelity(Table1),ourproposedvariantsachievehighscoresacrossvarioussubjectfidelity
metrics(DINO,CLIP-I,CLIP-TSimple,CLIP-TDetailed). DINOandCLIP-Imetricsareidentical
tothosepresentedinDreamBooth[30]andCLIP-TSimple/DetaileddenotestheCLIPsimilarity
betweentheoutputimageCLIPembeddingandtheCLIPembeddingofsimpleanddetailedtext
promptsdescribingthesubject,whichareinturngeneratedbyChatGPT4.
Regardingstylefidelity(Table2),ourproposedvariantsdemonstratestrongstyle-followingperfor-
manceusingCLIP-I[30,38],CSD[39],CLIP-T[30,38]metrics. Forstylefidelity,InstantStyle
ControlNetoutperformsourvariantsusingtheseautomaticmetrics,althoughweobservethatsubject
detailsandcontrastislostinmanyofthesesamplesasshowninFigure8. Forthis,wealsocompute
10Figure8:Style-AwarePersonalizationBaselineComparison:Weshowsomecomparisonsofourstyle-aware
personalizationmethodwithrespecttothetopperformingbaselinesStyleAlign+ControlNetandInstantStyle+
ControlNet.Wecanseethatthebaselinescanyielddecentoutputs,butlagbehindourstyle-awarepersonalization
methodinoverallquality.InparticularInstantStyle+ControlNetoutputsoftenappearslightlyblurryanddon’t
capturesubjectfeatureswithgoodcontrast.
Figure9:Style-AwarePersonalizationwithAttributeModification:Ourmethodallowsustomodifykey
attributesforthesubject,suchastheonesreflectedinthisfigure,whileconsistentlyapplyingourtargetstyle
overthegenerations.Thisallowsustoreinventthecharacter,oraddaccessories,whichgiveslargeflexibilityfor
creativeuses.NotethatwhenusingControlNetthiscapabilitydisappears.
ImageReward[50]scoresinTable3,whichcorrelatestronglywithhumanpreferenceinaesthetic
evaluations. Weobservethatourvariantsstronglyoutperformthebenchmarks.
Moreover, finding strong quantitative metrics for subject fidelity and for style fidelity is an open
probleminthefield,andmetricscanhavestrongbiasesthatcanmakethemsuboptimal. Again,we
showsomeexamplesforourproposedstyle-awarepersonalization,alongwithtopbaselinecontenders
StyleAlign ControlNet and InstantStyle ControlNet in Figure 8. We observe that the generation
quality of our variants is stronger than the benchmarks, especially with both strong stylization
performancewhilestillretainingtheessenceofthesubjects. OurMagicInsert+ControlNetvariant
ispowerfulgiventhatitexactlyfollowstheoutlineofthecharacter,andthushasthestrongestsubject
fidelityoverallapproaches,althoughitdoesnothavethedesirablepropertiesofourmethodw/o
ControlNetwhichincludepose,formandattributemodificationofthesubject.
UserStudy Followingpreviouswork[30,38,31,43]weperformarobustuserstudytocompare
ourfullmethod(w/ControlNet)withthestrongestbaselines: StyleAlignControlNetandInstantStyle
ControlNet. Werecruitatotalof60users(4setsof15users)toanswer40evaluationtasks(2sets
of20tasks)foreachbaselinecomparison(2baselinecomparisons). Wecollectatotalof1200user
evaluations. Weaskuserstoranktheirpreferredmethodswithrespecttosubjectidentitypreservation,
stylefidelity withrespect to the background image, andrealistic insertionof thesubject intothe
11Figure10:Editability/FidelityTradeoff:Weshowthephenomenonofeditability/fidelitytradeoffbyshowing
generationsfordifferentfinetuningiterationsofthespacemarine(shownabovetheimages)withthe“greenship”
stylizationandadditionaltextprompting“sittingdownonthefloor”.Whenthestyle-awarepersonalizedmodel
isfinetunedforlongeronthesubject,wegetstrongerfidelitytothesubjectbuthavelessflexibilityonediting
theposeorothersemanticpropertiesofthesubject.Thiscanalsotranslatetostyleeditability.
backgroundimage. WeshowtheresultsinTable4. Weobserveastrongpreferenceofusersforour
generatedoutputscomparedtobaselines.
5 SocietalImpact
Magic Insert aims to enhance creativity and self-expression through intuitive image generation.
However, it inherits concerns common to similar methods, such as altering sensitive personal
characteristicsandreproducingbiasesfrompre-trainedmodels. Ourexperimentshavenotshown
significantdifferencesinbiasorharmfulcontentcomparedtopreviouswork,butongoingresearchis
crucial. Asmorepowerfultoolsemerge,developingsafeguardsandmitigationstrategiesisessential
toaddresspotentialsocietalimpacts. Thisincludesreducingbiasintrainingdata,developingrobust
contentfiltering,andpromotingresponsibleuse. Balancingthebenefitsofcreativitywithethical
considerationsrequirescontinuousdialoguewiththebroadercommunity.
6 Conclusion
Inthiswork,weintroducedtheproblemofstyle-awaredrag-and-drop,anewchallengeinthefield
ofimagegenerationthataimstoenabletheintuitiveinsertionofsubjectsintotargetimageswhile
maintaining style consistency. We proposed Magic Insert, a method that addresses this problem
throughacombinationofstyle-awarepersonalizationandstyleinsertionusingbootstrappeddomain
adaptation. Ourapproachdemonstratesstrongresults,outperformingbaselinemethodsintermsof
bothstyleadherenceandinsertionrealism.
Tofacilitatefurtherresearchonthisproblem,weintroducedtheSubjectPlopdataset,whichconsists
ofsubjectsandbackgroundsspanningawiderangeofstylesandsemantics. Webelievethatour
contributions,includingtheformalizationofthestyle-awaredrag-and-dropproblem,theMagicInsert
method,andtheSubjectPlopdataset,willencourageexplorationandadvancementinthisexciting
newareaofimagegeneration.
Acknowledgements. WethankDanielWinter,DavidSalesin,Yi-HsuanTsai,RobinDuaandJay
Yagnikfortheirinvaluablefeedback.
References
[1] MoabArar,RinonGal,YuvalAtzmon,GalChechik,DanielCohen-Or,ArielShamir,andAmitH.Bermano.
Domain-agnostictuning-encoderforfastpersonalizationoftext-to-imagemodels. InSIGGRAPHAsia
2023ConferencePapers,pages1–10,2023.
[2] OmriAvrahami,DaniLischinski,andOhadFried. Blendeddiffusionfortext-driveneditingofnatural
images. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages
18208–18218,2022.
[3] HuiwenChang,HanZhang,JarredBarber,AJMaschinot,JoseLezama,LuJiang,Ming-HsuanYang,
KevinMurphy,WilliamTFreeman,MichaelRubinstein,etal.Muse:Text-to-imagegenerationviamasked
generativetransformers. arXivpreprintarXiv:2301.00704,2023.
12[4] WenhuChen,HexiangHu,YandongLi,NatanielRuiz,XuhuiJia,Ming-WeiChang,andWilliamWCohen.
Subject-driventext-to-imagegenerationviaapprenticeshiplearning. AdvancesinNeuralInformation
ProcessingSystems,36,2024.
[5] YardenFrenkel,YaelVinker,ArielShamir,andDanielCohen-Or. Implicitstyle-contentseparationusing
b-lora. arXivpreprintarXiv:2403.14572,2024.
[6] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel
Cohen-Or. Animageisworthoneword:Personalizingtext-to-imagegenerationusingtextualinversion.
arXivpreprintarXiv:2208.01618,2022.
[7] RinonGal,MoabArar,YuvalAtzmon,AmitHBermano,GalChechik,andDanielCohen-Or. Encoder-
baseddomaintuningforfastpersonalizationoftext-to-imagemodels. ACMTransactionsonGraphics
(TOG),42(4):1–13,2023.
[8] IanGoodfellow,JeanPouget-Abadie,MehdiMirza,BingXu,DavidWarde-Farley,SherjilOzair,Aaron
Courville,andYoshuaBengio. Generativeadversarialnets. Advancesinneuralinformationprocessing
systems,27,2014.
[9] JingGu,YilinWang,NanxuanZhao,WeiXiong,QingLiu,ZhifeiZhang,HeZhang,JianmingZhang,
HyunJoonJung,andXinEricWang. Swapanything:Enablingarbitraryobjectswappinginpersonalized
visualediting. arXivpreprintarXiv:2404.05717,2024.
[10] AmirHertz,AndreyVoynov,ShlomiFruchter,andDanielCohen-Or. Stylealignedimagegenerationvia
sharedattention. arXivpreprintarXiv:2312.02133,2023.
[11] JonathanHo,AjayJain,andPieterAbbeel. Denoisingdiffusionprobabilisticmodels. 2020.
[12] EdwardJHu,YelongShen,PhillipWallis,ZeyuanAllen-Zhu,YuanzhiLi,SheanWang,LuWang,and
WeizhuChen. Lora:Low-rankadaptationoflargelanguagemodels. arXivpreprintarXiv:2106.09685,
2021.
[13] Zheng Hui, Jie Li, Xiumei Wang, and Xinbo Gao. Image fine-grained inpainting. arXiv preprint
arXiv:2002.02609,2020.
[14] SatoshiIizuka,EdgarSimo-Serra,andHiroshiIshikawa.Globallyandlocallyconsistentimagecompletion.
ACMTransactionsonGraphics(ToG),36(4):1–14,2017.
[15] DonghoonLee,SifeiLiu,JinweiGu,Ming-YuLiu,Ming-HsuanYang,andJanKautz. Context-aware
synthesisandplacementofobjectinstances. ArXiv,abs/1812.02350,2018.
[16] GuilinLiu,FitsumAReda,KevinJShih,Ting-ChunWang,AndrewTao,andBryanCatanzaro. Image
inpaintingforirregularholesusingpartialconvolutions. InProceedingsoftheEuropeanconferenceon
computervision(ECCV),pages85–100,2018.
[17] HongyuLiu,BinJiang,YibingSong,WeiHuang,andChaoYang. Rethinkingimageinpaintingviaa
mutualencoder-decoderwithfeatureequalizations. InComputerVision–ECCV2020: 16thEuropean
Conference,Glasgow,UK,August23–28,2020,Proceedings,PartII16,pages725–741.Springer,2020.
[18] LingxiaoLu,BoZhang,andLiNiu. Dreamcom: Finetuningtext-guidedinpaintingmodelforimage
composition. arXivpreprintarXiv:2309.15508,2023.
[19] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool.
Repaint: Inpaintingusingdenoisingdiffusionprobabilisticmodels. InProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition,pages11461–11471,2022.
[20] ChenlinMeng,YutongHe,YangSong,JiamingSong,JiajunWu,Jun-YanZhu,andStefanoErmon.Sdedit:
Guidedimagesynthesisandeditingwithstochasticdifferentialequations.arXivpreprintarXiv:2108.01073,
2021.
[21] ChenlinMeng,YutongHe,YangSong,JiamingSong,JiajunWu,Jun-YanZhu,andStefanoErmon.Sdedit:
Guidedimagesynthesisandeditingwithstochasticdifferentialequations.arXivpreprintarXiv:2108.01073,
2021.
[22] EvangelosNtavelis,AndrésRomero,SiavashBigdeli,RaduTimofte,ZhengHui,XiumeiWang,Xinbo
Gao,ChajinShin,TaeohKim,HanbinSon,etal. Aim2020challengeonimageextremeinpainting. In
ComputerVision–ECCV2020Workshops:Glasgow,UK,August23–28,2020,Proceedings,PartIII16,
pages716–741.Springer,2020.
[23] DeepakPathak,PhilippKrahenbuhl,JeffDonahue,TrevorDarrell,andAlexeiAEfros. Contextencoders:
Featurelearningbyinpainting. InProceedingsoftheIEEEconferenceoncomputervisionandpattern
recognition,pages2536–2544,2016.
[24] DustinPodell,ZionEnglish,KyleLacey,AndreasBlattmann,TimDockhorn,JonasMüller,JoePenna,
andRobinRombach. Sdxl:Improvinglatentdiffusionmodelsforhigh-resolutionimagesynthesis. arXiv
preprintarXiv:2307.01952,2023.
13[25] AmitRaj,SrinivasKaza,BenPoole,MichaelNiemeyer,NatanielRuiz,BenMildenhall,ShiranZada,Kfir
Aberman,MichaelRubinstein,JonathanBarron,etal.Dreambooth3d:Subject-driventext-to-3dgeneration.
InProceedingsoftheIEEE/CVFinternationalconferenceoncomputervision,pages2349–2359,2023.
[26] AdityaRamesh,PrafullaDhariwal,AlexNichol,CaseyChu,andMarkChen.Hierarchicaltext-conditional
imagegenerationwithcliplatents. arXivpreprintarXiv:2204.06125,2022.
[27] YuruiRen,XiaomingYu,RuonanZhang,ThomasHLi,ShanLiu,andGeLi. Structureflow: Image
inpaintingviastructure-awareappearanceflow. InProceedingsoftheIEEE/CVFinternationalconference
oncomputervision,pages181–190,2019.
[28] RobinRombach,AndreasBlattmann,DominikLorenz,PatrickEsser,andBjörnOmmer. High-resolution
imagesynthesiswithlatentdiffusionmodels. InProceedingsoftheIEEE/CVFconferenceoncomputer
visionandpatternrecognition,pages10684–10695,2022.
[29] LituRout,YujiaChen,NatanielRuiz,AbhishekKumar,ConstantineCaramanis,SanjayShakkottai,and
Wen-Sheng Chu. Rb-modulation: Training-free personalization of diffusion models using stochastic
optimalcontrol. arXivpreprintarXiv:2405.17401,2024.
[30] NatanielRuiz,YuanzhenLi,VarunJampani,YaelPritch,MichaelRubinstein,andKfirAberman. Dream-
booth: Finetuningtext-to-imagediffusionmodelsforsubject-drivengeneration. In2023IEEE/CVF
ConferenceonComputerVisionandPatternRecognition(CVPR),pages22500–22510.IEEE,2023.
[31] NatanielRuiz,YuanzhenLi,VarunJampani,WeiWei,TingboHou,YaelPritch,NealWadhwa,Michael
Rubinstein,andKfirAberman.Hyperdreambooth:Hypernetworksforfastpersonalizationoftext-to-image
models. arXivpreprintarXiv:2307.06949,2023.
[32] MehdiSafaee,AryanMikaeili,OrPatashnik,DanielCohen-Or,andAliMahdavi-Amiri. Clic:Concept
learning in context. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition,pages6924–6933,2024.
[33] ChitwanSaharia,WilliamChan,HuiwenChang,ChrisLee,JonathanHo,TimSalimans,DavidFleet,and
MohammadNorouzi. Palette:Image-to-imagediffusionmodels. InACMSIGGRAPH2022Conference
Proceedings,pages1–10,2022.
[34] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar
Ghasemipour,RaphaelGontijoLopes,BurcuKaragolAyan,TimSalimans,etal. Photorealistictext-to-
imagediffusionmodelswithdeeplanguageunderstanding. AdvancesinNeuralInformationProcessing
Systems,35:36479–36494,2022.
[35] VishnuSarukkai,LindenLi,ArdenMa,ChristopherRé,andKayvonFatahalian. Collagediffusion. In
ProceedingsoftheIEEE/CVFWinterConferenceonApplicationsofComputerVision(WACV),pages
4208–4217,January2024.
[36] VirajShah,NatanielRuiz,ForresterCole,ErikaLu,SvetlanaLazebnik,YuanzhenLi,andVarunJampani.
Ziplora:Anysubjectinanystylebyeffectivelymergingloras. 2023.
[37] JaschaSohl-Dickstein,EricA.Weiss,NiruMaheswaranathan,andSuryaGanguli. Deepunsupervised
learningusingnonequilibriumthermodynamics. 2015.
[38] KihyukSohn,NatanielRuiz,KiminLee,DanielCastroChin,IrinaBlok,HuiwenChang,JarredBarber,
LuJiang,GlennEntis,YuanzhenLi,etal. Styledrop: Text-to-imagegenerationinanystyle. In37th
ConferenceonNeuralInformationProcessingSystems(NeurIPS).NeuralInformationProcessingSystems
Foundation,2023.
[39] GowthamiSomepalli,AnubhavGupta,KamalGupta,ShramayPalta,MicahGoldblum,JonasGeiping,
AbhinavShrivastava,andTomGoldstein. Measuringstylesimilarityindiffusionmodels. arXivpreprint
arXiv:2404.01292,2024.
[40] JiamingSong,ChenlinMeng,andStefanoErmon. Denoisingdiffusionimplicitmodels. 2022.
[41] YizhiSong,ZhifeiZhang,ZheLin,ScottCohen,BrianPrice,JianmingZhang,SooYeKim,andDaniel
Aliaga. Objectstitch:Generativeobjectcompositing. arXivpreprintarXiv:2212.00932,2022.
[42] RomanSuvorov,ElizavetaLogacheva,AntonMashikhin,AnastasiaRemizova,ArseniiAshukha,Aleksei
Silvestrov,NaejinKong,HarshithGoka,KiwoongPark,andVictorLempitsky. Resolution-robustlarge
mask inpainting with fourier convolutions. In Proceedings of the IEEE/CVF winter conference on
applicationsofcomputervision,pages2149–2159,2022.
[43] LumingTang,NatanielRuiz,QinghaoChu,YuanzhenLi,AleksanderHolynski,DavidEJacobs,Bharath
Hariharan,YaelPritch,NealWadhwa,KfirAberman,etal. Realfill: Reference-drivengenerationfor
authenticimagecompletion. arXivpreprintarXiv:2309.16668,2023.
[44] AndreyVoynov,QinghaoChu,DanielCohen-Or,andKfirAberman. p+:Extendedtextualconditioningin
text-to-imagegeneration. arXivpreprintarXiv:2303.09522,2023.
14[45] HaofanWang,QixunWang,XuBai,ZekuiQin,andAnthonyChen. Instantstyle: Freelunchtowards
style-preservingintext-to-imagegeneration. arXivpreprintarXiv:2404.02733,2024.
[46] QixunWang,XuBai,HaofanWang,ZekuiQin,andAnthonyChen.Instantid:Zero-shotidentity-preserving
generationinseconds. arXivpreprintarXiv:2401.07519,2024.
[47] SuWang,ChitwanSaharia,CesleeMontgomery,JordiPont-Tuset,ShaiNoy,StefanoPellegrini,Yasumasa
Onoe,SarahLaszlo,DavidJFleet,RaduSoricut,etal. Imageneditorandeditbench: Advancingand
evaluatingtext-guidedimageinpainting. InProceedingsoftheIEEE/CVFConferenceonComputerVision
andPatternRecognition,pages18359–18369,2023.
[48] DanielWinter, MatanCohen, ShlomiFruchter, YaelPritch, AlexRav-Acha, andYedidHoshen. Ob-
jectdrop: Bootstrappingcounterfactualsforphotorealisticobjectremovalandinsertion. arXivpreprint
arXiv:2403.18818,2024.
[49] ChenfeiWu,JianLiang,XiaoweiHu,ZheGan,JianfengWang,LijuanWang,ZichengLiu,YuejianFang,
andNanDuan. Nuwa-infinity:Autoregressiveoverautoregressivegenerationforinfinitevisualsynthesis.
arXivpreprintarXiv:2207.09814,2022.
[50] JiazhengXu,XiaoLiu,YuchenWu,YuxuanTong,QinkaiLi,MingDing,JieTang,andYuxiaoDong.
Imagereward: Learningandevaluatinghumanpreferencesfortext-to-imagegeneration. Advancesin
NeuralInformationProcessingSystems,36,2024.
[51] BinxinYang,ShuyangGu,BoZhang,TingZhang,XuejinChen,XiaoyanSun,DongChen,andFangWen.
Paintbyexample:Exemplar-basedimageeditingwithdiffusionmodels. InProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition,pages18381–18391,2023.
[52] HuYe,JunZhang,SiboLiu,XiaoHan,andWeiYang. Ip-adapter:Textcompatibleimagepromptadapter
fortext-to-imagediffusionmodels. 2023.
[53] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan,
AlexanderKu,YinfeiYang,BurcuKaragolAyan,etal. Scalingautoregressivemodelsforcontent-rich
text-to-imagegeneration. arXivpreprintarXiv:2206.10789,2022.
[54] Yanhong Zeng, Jianlong Fu, Hongyang Chao, and Baining Guo. Learning pyramid-context encoder
networkforhigh-qualityimageinpainting. InProceedingsoftheIEEE/CVFconferenceoncomputer
visionandpatternrecognition,pages1486–1494,2019.
[55] LvminZhang,AnyiRao,andManeeshAgrawala. Addingconditionalcontroltotext-to-imagediffusion
models. InProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision,pages3836–3847,
2023.
[56] ShuyangZhang,RunzeLiang,andMiaoWang. Shadowgan:Shadowsynthesisforvirtualobjectswith
conditionaladversarialnetworks. ComputationalVisualMedia,5:105–115,2019.
15