Neurocache: Efficient Vector Retrieval for Long-range Language Modeling
AliSafaya DenizYuret
asafaya19@ku.edu.tr dyuret@ku.edu.tr
KUISAICenter
ComputerEngineeringDepartment
KoçUniversity
Abstract
ThispaperintroducesNeurocache,anapproach
toextendtheeffectivecontextsizeoflargelan-
guagemodels(LLMs)usinganexternalvector
cachetostoreitspaststates. Likerecentvec-
tor retrieval approaches, Neurocache uses an
efficientk-nearest-neighbor(kNN)algorithm
toretrieverelevantpaststatesandincorporate
them into the attention process. Neurocache
improves upon previous methods by (1) stor-
ing compressed states, which reduces cache
size;(2)performingasingleretrievaloperation
pertokenwhichincreasesinferencespeed;and
(3) extending the retrieval window to neigh-
boring states, which improves both language
modelinganddownstreamtaskaccuracy. Our
experimentsshowtheeffectivenessofNeuro-
cache both for models trained from scratch
and for pre-trained models such as Llama2- Figure1: PerformanceandScalabilityofNeurocache
7B and Mistral-7B when enhanced with the vs. Memorizing Transformers (Wu et al., 2022) on
cache mechanism. We also compare Neuro- PG-19: ThegraphillustratesNeurocache’sconsistently
cache with text retrieval methods and show lowertokenperplexityandfasterinferencetimesacross
improvements in single-document question- variouscachesizesontheProjectGutenberg-19dataset,
answering and few-shot learning tasks. We demonstratingitsefficiencyandscalability.
madethesourcecodeavailableunder: https:
//github.com/alisafaya/neurocache
limitation. However,theseapproachesoftenstrug-
1 Introduction
gle to utilize their extended contexts (Liu et al.,
Recentadvancementsinnaturallanguageprocess- 2023) fully. Recent research by Xu et al. (2023)
inghavebeensignificantlydrivenbythedevelop- showsthatretrieval-augmentedmodelswithshorter
ment of large language models (LLMs) such as contexts (4K tokens) can match the performance
GPT-3,GPT-4,Llama,andLlama2(Brownetal., ofmodelswithlongercontexts(16K/32Ktokens),
2020; OpenAI, 2023; Touvron et al., 2023a,b). maintainingefficiencyduringinference. Thisem-
Whiledemonstratingimpressivecapabilities,these phasizesthepotentialofretrieval-augmentedstrate-
modelsareconstrainedbylimitedcontextwindow giesinLLMs.
sizes. This limitation becomes apparent in tasks In response to these challenges, we introduce
thatrequireunderstandinglongdocuments,suchas Neurocache. Neurocache employs an efficient k-
documentsummarizationandacademicliterature nearest-neighbor(kNN)strategyforretrievingrele-
review,whereprocessinghundredsofthousandsof vantpaststatesfromacompressedexternalvector
tokensisnecessary. cache. Thisapproachisdesignedtooptimizehid-
Various methods, including sparse attention denstatecachingandretrieval,therebyenhancing
(Child et al., 2019; Beltagy et al., 2020; Zaheer languagemodelingqualityandincreasinginference
et al., 2020), have been explored to address this speed.
4202
luJ
2
]LC.sc[
1v68420.7042:viXraNeurocache advances over exiting methods by 2023). Studies have indicated that retrieval-
reducingthecachesizethroughthestorageofcom- augmentedmodelswithshortercontexts(4K)can
pressed states, performing a single retrieval op- surpassmodelswithlongercontexts(16K/32K)in
eration per token to boost inference speed, and performance(Xuetal.,2023).
extending the retrieval window to include neigh- Prominent strategies in this area are Text Re-
boringstatesforimprovedlanguagemodelingand trievalandVectorRetrieval. TextRetrievalinvolves
downstreamaccuracy. Figure1illustratesthead- identifying and processing the most relevant seg-
vantagesofNeurocacheintermsofinferencespeed mentsoflongdocuments. VectorRetrieval,onthe
andlanguagemodelingaccuracyovermethodslike otherhand,integratesrelevanthiddenrepresenta-
MemorizingTransformers(Wuetal.,2022). tionsoftheinput, likehiddenstatesorkey-value
OurevaluationofNeurocacheencompassesboth pairs,intothemodel.
models trained from scratch and established pre-
2.1 TextRetrieval
trainedmodelssuchasLlama2-7BandMistral-7B
(Touvronetal.,2023b;Jiangetal.,2023),demon- Text retrieval methods focus on processing rele-
strating its effectiveness in enhancing language vant segmentsoflongdocuments. Integratingre-
models for downstream tasks. Specifically, we trievalmechanismsintolanguagemodels,suchas
highlight Neurocache’s improvements in single- REALM(Guuetal.,2020),DPR(Karpukhinetal.,
documentquestion-answeringandfew-shotlearn- 2020),RETRO(Borgeaudetal.,2021),andRALM
ingtaskswhencomparedtotraditionaltextretrieval (Ram et al., 2023), has enhanced model perfor-
methods. Moreover,Neurocache’sintegrationex- manceinvarioustasks.
tendsthemaximumcontextlengthofthesemodels AlimitationofTextRetrievalisitsdependency
to128Ktokens,indicatingitssignificantimpacton onexternalretrieversforidentifyingrelevantseg-
long-documentprocessing. mentsofthecontext,oftenemployingalgorithms
In summary, Neurocache represents a substan- likeBM25(RobertsonandZaragoza,2009),Con-
tial step forward in addressing the challenges of triever(Izacardetal.,2022),andothers(Borgeaud
processing long documents in LLMs, offering a et al., 2021; Ram et al., 2023; Karpukhin et al.,
blendofefficiency,adaptability,andenhancedper- 2020).
formance. Our comprehensive experiments and
2.2 VectorRetrieval
analysisshowcaseNeurocache’spotentialinrevo-
lutionizingtheunderstandingoflongdocumentsin Vectorretrievalmethodsextendthecontextwindow
naturallanguageprocessing. by incorporating relevant hidden states from an
externalcacheofpastinputs’representations.
2 RelatedWork
MemorizingTransformers presentanoveladap-
tationtothetraditionaltransformerdecoderstruc-
Transformershavemadesignificantadvancements
ture for handling lengthy documents. They pro-
innaturallanguageprocessingbutfacechallenges
cessdocumentsinsmallersegmentsanduseady-
inprocessinglongcontexts. Variousmethodshave
namicallyupdatedexternalcachetotrackprevious
beendevelopedtoextendthecontextwindowwhile
key-valuepairs. Thesemodelsemployanapprox-
maintainingcomputationalefficiency(Huangetal.,
imatek-nearest-neighbor(kNN)lookupoverthis
2023).
cache,mergingdenseself-attentiononthecurrent
Recentmethodsincludethecontinuedtraining
contextwithexternal-attentionoverretrievedkey-
or fine-tuning of short-context language models
valuepairs,thuseffectivelyextendingthecontext
(Nijkamp et al., 2023; Chen et al., 2023b), posi-
length(Wuetal.,2022).
tional interpolation (Chen et al., 2023a), ALiBi
(Pressetal.,2022),andsparseandefficientatten- Unlimiformer isavectorretrievalmethod,partic-
tiondesigns(Childetal.,2019;Beltagyetal.,2020; ularlysuitedforsequence-to-sequencemodelslike
Zaheeretal.,2020). Theseapproachesreflectthe BART (Lewis et al., 2020). It extends encoding
evolvinglandscapeofsolutionsformanagingex- lengthbyusingakNNindexoverallinputtoken
tendedattentionwindowsinlargelanguagemodels hidden states, focusing on the top-k input tokens
(LLMs). through kNN distance-based attention scores in
However,languagemodelsstillencounterdiffi- eachdecoderlayer’scross-attentionhead(Bertsch
culties in processing longer contexts (Liu et al., etal.,2023).2.3 Neurocache Rm×d. ThisselectionisbasedontheL2-distance
betweeneachstateinC andthestatesinthecache.
Neurocacheisavectorretrievalmethoddesigned
for processing long documents in large language Cache Updating: The cache C is updated
cache
models(LLMs). ItemploysakNNstrategytoef- withthecompressedstatesC,maintainingafixed
ficiently retrieve compressed past states from an size of m entries. This is achieved by discarding
externalvectorcache. Thisapproachcontrastswith theoldestnstates,adheringtoaFirst-In-First-Out
methods like Memorizing Transformers and Un- strategy. Theupdateoccurspost-retrieval,reinforc-
limiformer,particularlyintermsofcomputational ingthecommitmenttoretrievingonlyrelevantpast
efficiencyandcachesizemanagement. states.
Neurocache’s notable features include storing
CacheAugmentedLayers: UsingthestatesC
ret
compressedstatestoreducecachesizeandperform-
retrievedinthepreviousstep. Startingfromthe(r+
ingasingleretrievaloperationpertoken,whichac-
1)th layer,thecache-augmentedlayersLj,where
celeratesinferencespeed. Additionally,itexpands
j > r,integrateaspecializedattentionmechanism.
theretrievalwindowtoincludeneighboringstates,
Each layer uses unique projection matrices
Wj
,
enhancinglanguagemodelinganddownstreamtask k
Wj ,andWj togeneratekeysKj andvaluesVj
performance. v q ret ret
fromC ,andqueriesQj fromthehiddenstates
ret
Crucially,Neurocacheshowsadaptabilitywith
Hj. Thecacheattentionmechanismisdefinedas:
established pre-trained models like Llama2-7B
andMistral-7B,extendingtheirmaximumcontext
length capabilities to 128K tokens. This adapt- (cid:32) QKT (cid:33)
abilitydemonstratesNeurocache’spotentialinim- CA(Q,K ret,V ret) = softmax (cid:112) ret V ret
d
key
provinglong-documentprocessingcapabilitiesof
currentLLMs.
Inthisformula,Qarethequeriesderivedfrom
Inthiscontext,Neurocachepresentsabalanced Hj, while Kj and Vj are keys and values de-
ret ret
approachtovectorretrieval,combiningefficiency
rivedfromC ,withd servingasanormaliza-
ret key
andadaptabilitytoenhancelong-contextprocess-
tion factor. The output of cache attention is pro-
inginnaturallanguageprocessingmodels. cessedbyanoutputmatrixWj
beforebeingcom-
o
binedwithself-attentionoutputsthrougharesidual
3 Method
connection.
3.1 NeurocacheOverview ContextualRetrievalWindow: Whenretrieving
the top-k similar cached states, Neurocache also
Neurocache addresses the challenge of process-
considersadditionalstatessurroundingthesetop-k
ing long documents using Transformer decoders,
stateswithinadefinedRetrievalWindow. Thisex-
leveragingak-nearest-neighbor(kNN)searchfor
pandedretrievalcapturesnotonlythemostsimilar
efficient retrieval and integration of relevant past
statesbutalsotheirimmediateneighbors,provid-
states. Theprocessbeginsbysegmentingthelong
ingarichercontextforthemodel’sprocessing.
textsequencesintosmallersegments,eachcontain-
Consider the cached states C =
ingntokens,fittingthemodel’sattentionwindow cache
[c ,c ,...,c ], and a query q for which the
size. 1 2 m
cachedstatesc andc areidentifiedasthetop-2.
i j
State Compression: Text segments are sequen- With an even Retrieval Window size w, the
tially processed via a Transformer decoder stack retrievedsetwouldincludenotjustc andc ,but
i j
(Vaswani et al., 2017). At the rth layer of the also the cached states [c ,...,c ]
i−(w/2)−1 i+w/2
decoder, hidden states Hr ∈ Rn×h are acquired and [c ,...,c ], truncated at the
j−(w/2)−1 j+w/2
andsubsequentlyprojectedintoacompressedform boundariesofthecache.
C ∈ Rn×d using a learned projection matrix W .
p
ExtendedCache-Attention: Weenhancethecon-
Thiscompressionstepenhancestheefficiencyfor
textualawarenessofeachtokenduringthecache-
thesubsequentkNNretrieval.
attention operation by granting access to the re-
State Retrieval: For each compressed state c ∈ trievals of preceding tokens. Similar to the con-
Rd within C, we identify the top-k most simi- textualretrievalwindow,thisfeaturebroadensthe
lar states C ∈ Rk×d from the cache C ∈ currenttoken’scontext.
ret cacheFigure2: DocumentsaresegmentedintosequencesofntokensandprocessedsequentiallythroughaTransformer
decoderstack.Foreachtextsegment,mid-layerhiddenstatesH ∈Rn×hareprojectedintoacompactrepresentation
C ∈Rn×dusingalearnedweightmatrixW ∈Rd×h. ThisprojectionenhancestheefficiencyofkNNretrievalof
p
themostrelevantpaststatesC ∈Rn×k×dfromthecacheC .ThesestatesC areusedbycache-augmented
ret cache ret
layers to generate keys/values for cache attention. The output of cache attention is added to the self-attention
outputbeforebeingfedtothefeed-forwardnetwork(FFN).Finally,thecacheC isupdatedtoincludeC while
cache
maintainingaconstantsizeofmentries.
Specifically, for a token positioned at i in a se- networks of the cache augmented layers. LoRA,
quence,denotedast ,andwithapredefinedcon- introducingaminimalnumberofparameters,plays
i
textsizec,thecache-attentionmechanismincludes akeyroleinadaptingthemodelstocacheattention
notonlyitsownretrievedstatesCi butalsothe withoutcompromisingtheiroriginalstrengths.
ret
statesretrievedfortheprecedingc−1tokens. For Duringtraining,wefreezetheoriginalparame-
example,ifc = 4,thecache-attentionfort would ters of the pre-trained model and focus solely on
i
integrate the keys Ki−3:i and values Vi−3:i from trainingthenewlyaddedweights,specificallythe
ret ret
tokenst . LoRAweights,andthecache-attentionweightma-
i−3:i
Please refer to Appendix B for more detailed trices
(Wj,Wj,Wj,Wj),
along with the projec-
k v q o
descriptiononNeurocache. tionmatrixW . Thistraining,usingacausallan-
p
guagemodelingobjectiveonacorpusoflongdoc-
3.2 NeurocacheAdaptation
uments,enablesthemodelstoefficientlyutilizethe
Adaptingpre-traineddecoderlanguagemodelsfor Neurocachesystem.
Neurocache use is a straightforward process that
3.3 RetrievalOverhead
significantlyenhancestheircapabilitytoefficiently
processlongdocuments. Forthelayersaugmented Whenanalyzedpertoken,thecomputationalover-
with Neurocache, denoted as Lj where j > r, headofretrievalinourmethodstemsfromthefol-
theadaptationinvolvesinitializingcache-attention lowingcomponents,whichunderlinetheprimary
weight matrices (Wj,Wj,Wj,Wj) by duplicat- computationaleffortsinthekNNretrieval.
k v q o
ingweightsfromthecorrespondingself-attention DistanceComputation: Foreachtoken,therele-
layersofthepre-trainedmodels. Simultaneously, vanceisassessedbycalculatingtheL2-distancebe-
theprojectionmatrixW israndomlyinitializedto tweenthetoken’scompressedhiddenstatec ∈ Rd
p
transformhiddenstatesintocompactformssuitable and each of the m cached states, resulting in a
forNeurocacheretrieval. complexityofO(d×m)pertoken,wheredisthe
Furthermore,weintegrateLow-RankAdapters dimensionofthecompressedhiddenstatecandm
(LoRA) (Hu et al., 2022) into the feed-forward isthetotalnumberofcachedentries.Method Retrieval Entry els from scratch and adapting established pre-
Frequency Size trainedmodels. Forpre-training,TransformerXL
Neurocache(Ours) 1 d (Dai et al., 2019) serves as our baseline, against
MemorizingTransformer a 2a×f which we compare Neurocache and Memorizing
Unlimiformer l×h e
Transformer (Wu et al., 2022). In terms of adap-
tation, we focus on pre-trained models including
Table1: Spaceandtimecomplexityofmethodsbased
oncachequeriespertoken(RetrievalFrequency)and OPT-1.3B, Llama2-7B, and Mistral-7B (Zhang
cacheentrydimensionspertoken(EntrySize). Here, et al., 2022; Touvron et al., 2023b; Jiang et al.,
d is the compressed dimension in Neurocache, a the 2023).
numberofattentionheads,f headsize,ehiddensize,
andllayerswithcacheattention. 4.1 Datasets
Ourexperimentsemploytwodistinctrawtextcor-
Top-k Search Over Distances: Identifying the
pora: PG-19, a well-established benchmark for
top-k closeststatesfromthesedistancesinvolvesa
long-formlanguagemodeling,andLongPile,adi-
complexityofO(m+k)foreverytoken1.
versedatasetderivedfromthePile.
3.4 ComparativeAnalysis
PG-19: This corpus comprises a collection of
The Neurocache model demonstrates computa- books written in English and published before
tionaladvantageoveralternativesliketheMemo- 1919, sourced from Project Gutenberg. It is rec-
rizingTransformer(Wuetal.,2022)andtheUnlim- ognized as a standard benchmark for evaluating
iformer(Bertschetal.,2023)byperformingonly models on long-form text (Rae et al., 2020; Wu
onecachequerypertoken. Thisapproachsignifi- etal.,2022;Hutchinsetal.,2022).
cantlyreducesthecomputationalburden. Incon-
LongPile: Extracted from the Pile corpus (Gao
trast,theMemorizingTransformerrequiresmulti-
et al., 2020), LongPile features extensive docu-
plecachequeriesforeachtoken,specificallyone
ments from varied sources including "Books3,"
foreveryattentionhead. Consequently,thisleads
"Gutenberg (PG-19)," "OpenWebText2," "Pile-
toana-foldincreaseincomplexitypertoken,both
CC,"and"Wikipedia(en)."Theselectioncriterion
fordistancecomputation,O(a×d×m),andtop-k
ensuresthateachdocumentsurpasses20Ktokens,
retrieval,O(a×(m+k)),whereaisthenumber
makingitsuitablefortestingmodels’performance
ofattentionheads,andmisthecachesize.
onlongertexts.
The Unlimiformer, needing l × a queries per
token, furtherincreasesretrievalcomplexity. For 4.2 Pre-training
instance, a Transformer with 24 layers and 12 at-
Ourbaselineforpre-trainingistheTransformerXL
tentionheadsintheMemorizingTransformercon-
model(Daietal.,2019),whichwecompareagainst
figuration would need 12 cache accesses per to-
NeurocacheandtheMemorizingTransformer(Wu
ken. If the Unlimiformer uses half of its layers
et al., 2022). In these experiments, both Neuro-
foraugmentation,asper(Bertschetal.,2023),the
cacheandtheMemorizingTransformerareconfig-
requirementrisesto12×12 = 144cacheaccesses
uredwithafixedstoragesizeof16Kduringtrain-
pertoken. Neurocache’sstrategyofonequeryper
ing, expanding to 128K for evaluation to assess
tokensignificantlystreamlinesthisprocesswithout
theirabilitytogeneralizetolargerstoragesizes.
compromisingaccuracy.
In Neurocache, we set the augmented layer
Table 1 outlines these models’ retrieval fre-
threshold r at 3 ∗ n /4, leading to the com-
quencyandcacheentrysize,emphasizingNeuro- layers
pressionofoutputsfromthe9th layerofa12-layer
cache’sefficiency. Thetablecomparesthenumber
model. The hidden states H, originally of size
ofcachequeriespertokenandeachcacheentry’s
h = 1024,arecompressedbyafactorof4,result-
sizeacrossthedifferentmethods.
inginareducedsizeofd = 256. Weusearetrieval
4 LanguageModeling windoww = 2tofetchthetop-k cachedstatesand
theirrightneighborsforcache-attentioninlayers
WeassessNeurocache’seffectivenessviatwoex- 10to12. Extendingcache-attentiontoincludepre-
perimentalapproaches: pre-traininglanguagemod- vioustokens’retrievalswithc = 2,wesetk = 16,
resultingin64neighborsintotal. Thissetupwas
1Weassumeanalgorithmwithquicksort-stylepartitioning
isused. determinedthroughhyperparameteroptimizationPG-19 LongPile
Model Params
16K 128K 16K 128K
Trainingfromscratch
TRANSFORMERXL 184M 14.442 14.442 15.857 15.857
MEMORIZING TRANSFORMER 184M 13.636 13.494 14.966 14.818
NEUROCACHE 184M 13.511 13.352 14.425 14.110
Neurocacheadaptation
OPT-1.3B 1.3B 12.199 12.199 19.446 19.446
+NEUROCACHE 1.4B 11.306 11.227 17.626 17.377
LLAMA2-7B 6.7B 7.359 7.359 9.075 9.075
+NEUROCACHE 7.1B 7.117 7.078 8.401 8.308
MISTRAL-7B 7.2B 7.863 7.863 9.380 9.380
+NEUROCACHE 7.5B 7.684 7.636 8.581 8.493
Table2: ComparisonoftokenperplexityfordifferentmodelsandcachesizesonPG-19andLongPiledatasets.
NeurocacheoutperformsMemorizingTransformer,andpresentsasignificantreductioninperplexityacrossboth
pre-trainingandadaptationexperiments,underscoringtheadaptabilitytolargercachesizes.
(detailsinAppendixA).Additionally,wemodify thatdescribedinSection3.2,ensuringasmoothin-
the FFN dimensionality of the Neurocache from tegrationofNeurocachewiththepre-trainedmodel
4096, consistent with the baseline and Memoriz- weights.
ingTransformer,to3776toensureparityinmodel Wesettherankparameterr to16,thescalepa-
sizes. rameterαto32,andturnoffbiasinLoRA.Added
The Memorizing Transformer, adhering to its weightmatricesandadapterweightsaretrainedon
originaldesign(Wuetal.,2022),cacheskey-value the PG-19 and LongPile datasets’ training splits
pairsfromits9thlayer. Wealignitsretrievalsetting for 25,000 steps, employing the Adam optimizer
withNeurocachebysettingk = 64,thusretrieving (KingmaandBa,2015)withadecayinglearning
thetop-64key-valuepairsforeachattentionhead rateof1×10−4. WeconfiguredNeurocacheusing
pertoken. thesamesettingsasthepre-trainingexperiments.
The pre-training involves 100,000 steps with Thisadaptationprocessconsumesapproximately
a batch size of 128 and a context size of 1,024. 200NvidiaA100GPUHourspermodel.
Adafactor (Shazeer and Stern, 2018) is used for Thesuccessfuladaptationisevidentinthesig-
optimization,withalearningratewarmingupover nificantimprovementintokenperplexityonboth
thefirst1,000steps,peakingat2×10−2,andthen datasets,asdetailedinTable2. Thesubsequentsec-
decayingto1×10−3. tiondiscussestheimpactoftheseimprovementson
Neurocache’s performance on the PG-19 and zero-shotperformanceindownstreamtasks.
LongPiledatasetssurpassesthatoftheMemoriz-
5 DownstreamEvaluation
ingTransformer, asevidencedbyits lowertoken
perplexities,detailedinTable2. Additionally,we
Weassesstheperformanceofmodelsaugmented
assess the scalability of Neurocache in compari-
with Neurocache, particularly Llama2-7B and
son to Memorizing Transformers across various
Mistral-7BadaptedonLongPile,usingsevendis-
cache sizes. The results, illustrated in Figure 1,
tinctdownstreamtasksfromtheLongBenchsuite
demonstrate Neurocache’s computational advan-
(Bai et al., 2023). These tasks cover a range
tage,maintainingitssuperiorperformanceacross
ofscenarios,includingsingle-documentquestion-
differentcachesizes.
answering (QA), multi-document QA, and few-
shot learning. We utilize a zero-shot evalua-
4.3 Adaptation
tionapproachforthesingle-documentandmulti-
We extend our adaptation strategy to pre-trained document QA tasks. Conversely, in the few-shot
modelssuchasOPT-1.3B,Llama2-7B,andMistral- learningtasks,asmallsetofexamplesisprovided
7B(Zhangetal.,2022;Touvronetal.,2023b;Jiang tothemodels,servingaspartoftheextendedcon-
etal.,2023). Theadaptationprocessisidenticalto text.Single-docQA Multi-docQA Few-shotLearn.
Method
NQA QSP MQA HQA MSQ TREC SAMS
F1 F1 F1 F1 F1 Acc. R-L
Inputavg.length 35.4K 5.3K 8.1K 17K 19K 7.8K 11.3K
LLAMA2-7B
TRUNCATION 22.19 28.17 33.39 33.66 12.30 67.00 33.00
LONGLORA 21.92 27.58 30.10 29.17 11.05 69.50 30.29
TEXTRETRIEVAL 23.57 26.71 39.46 38.51 18.89 66.50 29.38
NEUROCACHE(Ours) 23.62 28.32 41.23 33.30 13.84 72.00 42.77
MISTRAL-7B
TRUNCATION 15.64 27.58 40.21 35.22 13.17 68.00 26.47
TEXTRETRIEVAL 14.24 28.67 41.87 40.92 21.17 66.00 18.06
NEUROCACHE(Ours) 20.08 31.01 44.15 35.49 14.00 70.00 35.86
Table 3: Zero-shot performance comparison of LLAMA2-7B and MISTRAL-7B using various long document
processingmethodsontheLONGBENCHbenchmarktasks. MetricsincludeF1,Accuracy(Acc.),andRouge-L
(R-L). NEUROCACHE excels in Single-doc QA and Few-shot Learning but faces challenges in Multi-doc QA
comparedtotextretrieval. Documentlengthsareprovidedforreference.
5.1 Datasets MuSiQue (MSQ) focuses on multihop reason-
inginQA.Itconstructsmulti-hopquestionsfrom
Thedatasetsinthisevaluationpresentuniquechal-
simpler,single-hopones,demandingasystematic
lenges, with average token lengths ranging from
approach and detailed control over the question
5Kto35K,underscoringtheneedtoprocesslong
formationprocess(Trivedietal.,2022).
textseffectively.
5.1.3 Few-shotLearning
5.1.1 Single-documentQA
SAMSum (SAMS) presents a dialogue summa-
NarrativeQA (NQA) is a question-answering
rizationchallengewithitsdatasetofmessenger-like
datasetconsistingofbooksfromProjectGutenberg
conversationsandhuman-annotatedsummaries2. It
and movie scripts. It includes about 30 question-
testsamodel’sabilitytocondenseconversational
answerpairsperdocument,providingarobusttest
dataintocoherentsummaries(Gliwaetal.,2019).
forQAsystems(Kocˇiskýetal.,2018).
Qasper (QSP) contains questions and answers TREC serves as a dataset for few-shot learning
tasks in question type classification. Models are
extracted from NLP papers. This dataset offers
taskedwithcategorizingquestionsintopredefined
diversequestiontypes,suchasabstractive,extrac-
categories, providing a test of their classification
tive,yes/no,andunanswerablequestions,making
abilities(LiandRoth,2002).
itacomprehensivetestbedforQAmodels(Dasigi
etal.,2021).
5.2 Models
MultiFieldQA (MQA) is designed to test a
InadditiontoNeurocache,ourevaluationincludes
model’sabilitytounderstandlongcontextsacross
three distinct approaches for extending the input
variousfields,includinglegaldocuments,govern-
lengthofpre-trainedlanguagemodels. Theseap-
ment reports, and academic papers. It poses a
proachesareInputTruncation,TextRetrieval,and
challengewithitsquestionsdispersedthroughout
PositionInterpolation(PI).
lengthydocuments(Baietal.,2023).
Truncation: Thisapproachemploystheoriginal
5.1.2 Multi-documentQA
Llama2-7BandMistral-7Bmodelswithoutlong-
context-specific modifications. Here, inputs ex-
HotpotQA (HQA) is a multi-document,
ceeding the maximum size of 4,096 tokens are
Wikipedia-based QA dataset. It requires reading
truncated from the middle following (Bai et al.,
and reasoning across multiple documents and
2023). Thisbaselineservesasareferencetoevalu-
includes questions necessitating sentence-level
supporting facts for complex reasoning (Yang
2We use the rouge package: https://github.com/
etal.,2018). pltrdy/rougeatetheeffectivenessofothermethodsinprocessing and context sequences. In single-doc QA tasks,
extendeddocuments. the input is a question paired with the document
as context. For multi-doc QA, the input consists
TextRetrieval: ContrastingwithNeurocache,this
ofmultipleconcatenateddocuments. Infew-shot
approachinvolvesselectingthemostrelevanttext
learning tasks, such as TREC and SAMSum, the
segmentstoincludeintheinput,keepingthetotal
context includes a set of examples, and the input
lengthwithinthemodel’smaximuminputsize. We
isa questionordialogue, respectively. The input
dividethecontextinto200-wordchunks,retrieving
andansweraretypicallyconcise,whilethecontext
the top-7 chunks using Contriever (Izacard et al.,
canbealongsequenceextendingtothousandsof
2022). Thesechunks,alongwiththeinput,arethen
tokens.
processed by the model. Using the top-7 chunks
Ifthecombinedlengthofinputandcontextex-
balancesperformanceandthe4Ktokenlimit. This
ceeds the model’s maximum input capacity, only
method, used in previous work (Bai et al., 2023;
the context is truncated. This truncation is done
Xu et al., 2023), differs from Neurocache, which
from the middle of the context sequence, follow-
dynamicallyintegratesrelevantinformationfrom
ing the approach in (Bai et al., 2023). We utilize
theentiredocumentviacache-augmentedlayers.
prompttemplatesprovidedbyLongBenchforcon-
Position Interpolation (PI): PI (Chen et al., sistency. NeurocacheandLongLoRAoperatewith
2023a)linearlydown-scalesinputpositionindices amaximumlengthof16Ktokens,truncatingcon-
to fit the original context window size, avoiding texts longer than this limit. In contrast, the Text
high attention scores that could disrupt the self- Retrievalmethodprocessestheentirecontext,re-
attention mechanism. LongLoRA (Chen et al., gardless of length. To ensure comparability, all
2023b), leveraging PI, offers an efficient fine- modelsareevaluatedonidenticalhardwarewitha
tuning method to expand the context size of pre- batchsizeof1.
trained models. It uses a sparse local attention
mechanism, enabling computation savings while 5.4 Results
retaining performance. The fully fine-tuned Lon-
The zero-shot evaluation results across various
gLoRAmodel3,basedonLlama2-7B,extendsthe
downstream tasks are summarized in Table 3.
maximum input length to 16K tokens, aiming to
We compare the performance of Llama2-7B and
assess the effectiveness of efficient full-attention
Mistral-7B, in their original and Neurocache-
methodsforlongerdocuments.
adapted forms, against other long document pro-
Neurocache: WeutilizetheNeurocache-adapted cessingmethods.
Llama2-7BandMistral-7Bmodelsinourevalua-
Single-documentQA: IntaskslikeNarrativeQA,
tion. These adaptations follow the configuration
Qasper, and MultiFieldQA, Neurocache-adapted
detailed in Section 4 for pre-training. The mod-
models show superior performance, demonstrat-
els operate with a fixed cache size of 16K, ac-
ingtheireffectivenessinprocessinglongcontexts
commodating the length of most datasets in our
withinsingledocuments.
study. We split the documents into 2,048-token
segments, processing them sequentially to popu- Multi-documentQA: Performanceinmulti-doc
latethecache. Subsequently,theinput,embedded QAtasks,suchasHotpotQA,revealsavariedpic-
withintheprompt,isfedtothemodel,whichthen ture. WhileNeurocache-adaptedmodelsarecom-
generatesthecorrespondinganswer. petitive,theyfallshortofTextRetrievalmethods.
Forinstance,inHotpotQA,TextRetrievalwiththe
5.3 EvaluationSetting
Mistral-7B model achieves the highest F1 score
All evaluated models in this study are only pre- of40.92. Thisfindingsuggeststhat,despiteNeu-
trainedandnotfine-tunedonthedownstreamtasks. rocache’seffectivenessinsingle-docscenarios,it
Theyareassessedinazero-shotsetting,employing may be less effective in multi-doc contexts com-
greedydecodingforoutputgeneration. paredtotextretrievalapproaches.
AsoutlinedbyLongBench(Baietal.,2023),the
Few-shot Learning: In few-shot learning tasks
model’s taskisto producean answergiven input
like SAMSum and TREC, Neurocache shows
strongperformance,particularlyindicatedbyim-
3https://huggingface.co/Yukang/
Llama-2-7b-longlora-16k-ft provedRouge-LscoresinSAMSum. Thisunder-scoresitscapabilitytoleveragefew-shotexamples long-contextscenarios. Performancemayvaryin
forgeneratingaccuratesummaries. specialized domains like technical documents or
Thesefindingsillustratethestrengthsandchal- sourcecode,whichhavedistinctcontentcharacter-
lengesofdifferentmethodsinhandlinglongdoc- istics.
uments in language models. Neurocache excels A notable limitation is Neurocache’s perfor-
insingle-documentandfew-shotlearningscenar- mance in multi-document scenarios, suggesting
ios,whileTextRetrievalmethodshaveanedgein potential challenges in contexts that require inte-
multi-documenttasks. grationofinformationfrommultiplesources. This
aspectiscrucialforapplicationsinvolvingcompre-
6 Conclusion
hensivedatasynthesisfromvariousdocuments.
Intermsofbias,Neurocachedependsontheun-
This paper introduced Neurocache, an approach
derlyinglanguagemodelsanddatasetsfortraining
designedtoimprovelongdocumentprocessingin
andevaluation. Consequently,anyinherentbiases
languagemodels. Neurocacheemploysak-nearest-
inthesecomponentscouldinfluenceNeurocache’s
neighbor (kNN) strategy for integrating relevant
outputs. Anexplicitanalysisofmodelbiaseswas
paststatesfromcompressedhiddenrepresentations,
not conducted in this study, highlighting an area
thusextendingthecontextwindowofTransformer
forfutureexploration.
decoders. Notably,Neurocacheenhancesthemaxi-
Anothercriticalpointisourrelianceonazero-
mumcontextlengthofmodelslikeLlama2-7Band
shot setting for evaluation. The performance of
Mistral-7Bto128Ktokens.
Neurocache might differ if fine-tuning on down-
OurfindingsindicatethatNeurocacheoffersim-
streamtasksorinstructiondatasetswasemployed.
provementsininferencespeedandlanguagemodel-
This limitation suggests that our current findings
ingaccuracy. Itdemonstratesproficiencyinsingle-
maynotfullycapturethemodel’sadaptabilityand
documentquestion-answeringandfew-shotlearn-
efficiencyindiverseapplicationscenarios.
ing,thoughitfaceschallengesinmulti-document
Inconclusion,whileNeurocachepresentsastep
scenarios. Neurocache’scompetitiveperformance
forwardinhandlinglongdocumentsinnaturallan-
and adaptability highlight its potential utility in
guage processing, its effectiveness is influenced
variousapplications.
bythenatureofthedata,modelarchitecture,and
Insummary,Neurocachecontributestothefield
specific task requirements. Understanding these
by enabling more efficient handling of extended
limitationsisvitalforassessingitspracticalappli-
contexts in existing language models. Future
cabilityandguidingfutureimprovements.
workmayexplorefurtheroptimizationsformulti-
documenttasksandtheextensionofNeurocache
todifferentmodelarchitecturesanddomains.
References
Acknowledgment Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu,
JiankaiTang,ZhidianHuang,ZhengxiaoDu,Xiao
AliSafayawassupportedbytheKUISAICenter Liu,AohanZeng,LeiHou,YuxiaoDong,JieTang,
fellowship. Deniz Yuret was partially supported andJuanziLi.2023. Longbench: Abilingual,mul-
byHyperBee.ai. Moreover,partsoftheresultsre- titask benchmark for long context understanding.
ComputingResearchRepository,arXiv:2308.14508.
portedinthispaperwereperformedatTUBITAK
ULAKBIM,HighPerformanceandGridComput- IzBeltagy,MatthewE.Peters,andArmanCohan.2020.
ingCenter(TRUBAresources). Longformer: Thelong-documenttransformer. Com-
Ali Safaya dedicateshis work to thePeople of putingResearchRepository,arXiv:2004.05150. Ver-
sion2.
Gaza.
Amanda Bertsch, Uri Alon, Graham Neubig, and
7 Limitations
Matthew R Gormley. 2023. Unlimiformer: Long-
rangetransformerswithunlimitedlengthinput. In
WhileNeurocachedemonstratesprogressinlong
AdvancesinNeuralInformationProcessingSystems.
document processing with language models, sev-
erallimitationsshouldbenoted. Ourevaluationis SebastianBorgeaud,ArthurMensch,JordanHoffmann,
TrevorCai,ElizaRutherford,KatieMillican,George
confinedtodatasetslikePG-19andLongPile,and
vandenDriessche, Jean-BaptisteLespiau, Bogdan
tasks from the LongBench suite. These datasets,
Damoc,AidanClark,DiegodeLasCasas,Aurelia
despitetheirdiversity,mightnotfullyrepresentall Guy, Jacob Menick, Roman Ring, Tom Hennigan,SaffronHuang,LorenMaggiore,ChrisJones,Albin annotated dialogue dataset for abstractive summa-
Cassirer, AndyBrock,MichelaPaganini, Geoffrey rization. In Proceedings of the 2nd Workshop on
Irving, Oriol Vinyals, Simon Osindero, Karen Si- NewFrontiersinSummarization,pages70–79,Hong
monyan,JackW.Rae,ErichElsen,andLaurentSifre. Kong,China.AssociationforComputationalLinguis-
2021. Improvinglanguagemodelsbyretrievingfrom tics.
trillionsoftokens. ComputingResearchRepository,
arXiv:2112.04426. KelvinGuu,KentonLee,ZoraTung,PanupongPasupat,
and Mingwei Chang. 2020. Retrieval augmented
Tom Brown, Benjamin Mann, Nick Ryder, Melanie languagemodelpre-training. InProceedingsofthe
Subbiah,JaredDKaplan,PrafullaDhariwal,Arvind 37thInternationalConferenceonMachineLearning,
Neelakantan,PranavShyam,GirishSastry,Amanda ProceedingsofMachineLearningResearch,pages
Askell, Sandhini Agarwal, Ariel Herbert-Voss, 3929–3938.
Gretchen Krueger, Tom Henighan, Rewon Child,
AdityaRamesh,DanielZiegler,JeffreyWu,Clemens Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan
Winter, Chris Hesse, Mark Chen, Eric Sigler, Ma- Allen-Zhu,YuanzhiLi,SheanWang,LuWang,and
teusz Litwin, Scott Gray, Benjamin Chess, Jack WeizhuChen.2022. LoRA:Low-rankadaptationof
Clark, ChristopherBerner, SamMcCandlish, Alec largelanguagemodels. InInternationalConference
Radford, Ilya Sutskever, and Dario Amodei. 2020.
onLearningRepresentations.
Language models are few-shot learners. In Ad-
Yunpeng Huang, Jingwei Xu, Zixu Jiang, Junyu Lai,
vances in Neural Information Processing Systems,
ZenanLi,YuanYao,TaolueChen,LijuanYang,Zhou
volume 33, pages 1877–1901. Curran Associates,
Xin,andXiaoxingMa.2023. Advancingtransformer
Inc.
architectureinlong-contextlargelanguagemodels:A
comprehensivesurvey. ComputingResearchReposi-
ShouyuanChen,ShermanWong,LiangjianChen,and
tory,arXiv:2311.12351.
YuandongTian.2023a. Extendingcontextwindow
oflargelanguagemodelsviapositionalinterpolation.
DeLesleyHutchins,ImanolSchlag,YuhuaiWu,Ethan
ComputingResearchRepository,arXiv:2306.15595.
Dyer,andBehnamNeyshabur.2022. Block-recurrent
transformers. In Advances in Neural Information
Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai,
ProcessingSystems.
ZhijianLiu, SongHan, andJiayaJia.2023b. Lon-
glora: Efficient fine-tuning of long-context large
GautierIzacard,MathildeCaron,LucasHosseini,Sebas-
languagemodels. ComputingResearchRepository,
tianRiedel,PiotrBojanowski,ArmandJoulin,and
arXiv:2309.12307.
EdouardGrave.2022. Unsuperviseddenseinforma-
tionretrievalwithcontrastivelearning. Transactions
Rewon Child, Scott Gray, Alec Radford, and Ilya
onMachineLearningResearch.
Sutskever. 2019. Generating long sequences with
sparse transformers. Computing Research Reposi-
AlbertQ.Jiang,AlexandreSablayrolles,ArthurMen-
tory,arXiv:1904.10509.
sch,ChrisBamford,DevendraSinghChaplot,Diego
delasCasas,FlorianBressand,GiannaLengyel,Guil-
Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-
laumeLample,LucileSaulnier,LélioRenardLavaud,
bonell, Quoc Le, and Ruslan Salakhutdinov. 2019.
Marie-AnneLachaux,PierreStock,TevenLeScao,
Transformer-XL:Attentivelanguagemodelsbeyond
Thibaut Lavril, Thomas Wang, Timothée Lacroix,
a fixed-length context. In Proceedings of the 57th
andWilliamElSayed.2023. Mistral7b. Computing
AnnualMeetingoftheAssociationforComputational
ResearchRepository,arXiv:2310.06825.
Linguistics,pages2978–2988,Florence,Italy.Asso-
ciationforComputationalLinguistics. VladimirKarpukhin,BarlasOguz,SewonMin,Patrick
Lewis,LedellWu,SergeyEdunov,DanqiChen,and
Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan,
Wen-tauYih.2020. Densepassageretrievalforopen-
NoahA.Smith,andMattGardner.2021. Adataset
domainquestionanswering. InProceedingsofthe
of information-seeking questions and answers an-
2020ConferenceonEmpiricalMethodsinNatural
chored in research papers. In Proceedings of the
LanguageProcessing(EMNLP),pages6769–6781,
2021ConferenceoftheNorthAmericanChapterof
Online.AssociationforComputationalLinguistics.
theAssociationforComputationalLinguistics: Hu-
manLanguageTechnologies,pages4599–4610,On- Diederik P. Kingma and Jimmy Ba. 2015. Adam: A
line.AssociationforComputationalLinguistics. methodforstochasticoptimization. InInternational
ConferenceonLearningRepresentations(ICLR).
LeoGao,StellaBiderman,SidBlack,LaurenceGold-
ing, Travis Hoppe, Charles Foster, Jason Phang, TomášKocˇiský,JonathanSchwarz,PhilBlunsom,Chris
Horace He, Anish Thite, Noa Nabeshima, Shawn Dyer,KarlMoritzHermann,GáborMelis,andEd-
Presser, and Connor Leahy. 2020. The Pile: An wardGrefenstette.2018. TheNarrativeQAreading
800gbdatasetofdiversetextforlanguagemodeling. comprehensionchallenge. TransactionsoftheAsso-
ComputingResearchRepository,arXiv:2101.00027. ciationforComputationalLinguistics,6:317–328.
BogdanGliwa,IwonaMochol,MaciejBiesek,andAlek- Mike Lewis, Yinhan Liu, Naman Goyal, Marjan
sanderWawer.2019. SAMSumcorpus: Ahuman- Ghazvininejad,AbdelrahmanMohamed,OmerLevy,Veselin Stoyanov, and Luke Zettlemoyer. 2020. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
BART:Denoisingsequence-to-sequencepre-training bert, Amjad Almahairi, Yasmine Babaei, Nikolay
fornaturallanguagegeneration,translation,andcom- Bashlykov,SoumyaBatra,PrajjwalBhargava,Shruti
prehension. InProceedingsofthe58thAnnualMeet- Bhosale,DanBikel,LukasBlecher,CristianCanton
ingoftheAssociationforComputationalLinguistics, Ferrer,MoyaChen,GuillemCucurull,DavidEsiobu,
pages7871–7880,Online.AssociationforComputa- JudeFernandes,JeremyFu,WenyinFu,BrianFuller,
tionalLinguistics. CynthiaGao,VedanujGoswami,NamanGoyal,An-
thonyHartshorn,SagharHosseini,RuiHou,Hakan
Xin Li and Dan Roth. 2002. Learning question clas-
Inan,MarcinKardas,ViktorKerkez,MadianKhabsa,
sifiers. In COLING 2002: The 19th International
IsabelKloumann,ArtemKorenev,PunitSinghKoura,
ConferenceonComputationalLinguistics.
Marie-AnneLachaux,ThibautLavril,JenyaLee,Di-
anaLiskovich,YinghaiLu,YuningMao,XavierMar-
NelsonF.Liu,KevinLin,JohnHewitt,AshwinParan-
tinet,TodorMihaylov,PushkarMishra,IgorMoly-
jape,MicheleBevilacqua,FabioPetroni,andPercy
bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-
Liang.2023. Lostinthemiddle: Howlanguagemod-
stein,RashiRungta,KalyanSaladi,AlanSchelten,
elsuselongcontexts. ComputingResearchReposi-
Ruan Silva, Eric Michael Smith, Ranjan Subrama-
tory,arXiv:2307.03172.
nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-
Erik Nijkamp, Tian Xie, Hiroaki Hayashi, Bo Pang, lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
Congying Xia, Chen Xing, Jesse Vig, Semih ZhengYan,IliyanZarov,YuchenZhang,AngelaFan,
Yavuz,PhilippeLaban,BenKrause,SenthilPurush- Melanie Kambadur, Sharan Narang, Aurelien Ro-
walkam, Tong Niu, Wojciech Krys´cin´ski, Lidiya driguez,RobertStojnic,SergeyEdunov,andThomas
Murakhovs’ka,PrafullaKumarChoubey,AlexFab- Scialom.2023b. Llama2: Openfoundationandfine-
bri, Ye Liu, Rui Meng, Lifu Tu, Meghana Bhat, tunedchatmodels. ComputingResearchRepository,
Chien-Sheng Wu, Silvio Savarese, Yingbo Zhou, arXiv:2307.09288.
Shafiq Joty, and Caiming Xiong. 2023. Xgen-7b
HarshTrivedi,NiranjanBalasubramanian,TusharKhot,
technical report. Computing Research Repository,
and Ashish Sabharwal. 2022. MuSiQue: Multi-
arXiv:2309.03450.
hopquestionsviasingle-hopquestioncomposition.
OpenAI. 2023. Gpt-4 technical report. Computing TransactionsoftheAssociationforComputational
ResearchRepository,arXiv:2303.08774. Linguistics,10:539–554.
OfirPress,NoahSmith,andMikeLewis.2022. Train Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
short,testlong: Attentionwithlinearbiasesenables Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz
inputlengthextrapolation. InInternationalConfer- Kaiser,andIlliaPolosukhin.2017. Attentionisall
enceonLearningRepresentations. youneed. InAdvancesinNeuralInformationPro-
cessingSystems,volume30,page6000–6010.Curran
Jack W. Rae, Anna Potapenko, Siddhant M. Jayaku- Associates,Inc.
mar, Chloe Hillier, and Timothy P. Lillicrap. 2020.
Compressivetransformersforlong-rangesequence YuhuaiWu,MarkusNormanRabe,DeLesleyHutchins,
modelling. InInternationalConferenceonLearning andChristianSzegedy.2022. Memorizingtransform-
Representations. ers. InInternationalConferenceonLearningRepre-
sentations.
OriRam,YoavLevine,ItayDalmedigos,DorMuhlgay,
Amnon Shashua, Kevin Leyton-Brown, and Yoav PengXu,WeiPing,XianchaoWu,LawrenceMcAfee,
Shoham.2023. In-contextretrieval-augmentedlan- ChenZhu,ZihanLiu,SandeepSubramanian,Evelina
guagemodels. TransactionsoftheAssociationfor Bakhturina,MohammadShoeybi,andBryanCatan-
ComputationalLinguistics. zaro.2023. Retrievalmeetslongcontextlargelan-
guage models. Computing Research Repository,
StephenRobertsonandHugoZaragoza.2009. Theprob- arXiv:2310.03025.
abilistic relevance framework: Bm25 and beyond.
FoundationsandTrends®inInformationRetrieval, ZhilinYang,PengQi,SaizhengZhang,YoshuaBengio,
3(4):333–389. WilliamCohen,RuslanSalakhutdinov,andChristo-
pher D. Manning. 2018. HotpotQA: A dataset for
Noam Shazeer and Mitchell Stern. 2018. Adafactor: diverse, explainablemulti-hopquestionanswering.
Adaptivelearningrateswithsublinearmemorycost. In Proceedings of the 2018 Conference on Empiri-
In Proceedings of the 35th International Confer- calMethodsinNaturalLanguageProcessing,pages
enceonMachineLearning,ProceedingsofMachine 2369–2380,Brussels,Belgium.AssociationforCom-
LearningResearch,pages4596–4604.PMLR. putationalLinguistics.
HugoTouvron,ThibautLavril,GautierIzacard,Xavier Manzil Zaheer, Guru Guruganesh, Kumar Avinava
Martinet,Marie-AnneLachaux,TimothéeLacroix, Dubey,JoshuaAinslie,ChrisAlberti,SantiagoOn-
BaptisteRozière,NamanGoyal,EricHambro,Faisal tanon, Philip Pham, Anirudh Ravula, Qifan Wang,
Azhar,AurelienRodriguez,ArmandJoulin,Edouard Li Yang, and Amr Ahmed. 2020. Big bird: Trans-
Grave,andGuillaumeLample.2023a. Llama: Open formersforlongersequences. InAdvancesinNeural
andefficientfoundationlanguagemodels. Comput- InformationProcessingSystems,volume33,pages
ingResearchRepository,arXiv:2302.13971. 17283–17297.CurranAssociates,Inc.Susan Zhang, Stephen Roller, Naman Goyal, Mikel that a window size of w = 2 is optimal, as per
Artetxe,MoyaChen,ShuohuiChen,ChristopherDe- Table 5. This setting likely benefits the model’s
wan,MonaDiab,XianLi,XiVictoriaLin,TodorMi-
causalprocessingbyincludingboththetop-kentry
haylov,MyleOtt,SamShleifer,KurtShuster,Daniel
and the subsequent one. We fix w = 2 for the
Simig, Punit Singh Koura, Anjali Sridhar, Tianlu
Wang,andLukeZettlemoyer.2022. Opt: Openpre- subsequentexperiments.
trained transformer language models. Computing
ResearchRepository,arXiv:2205.01068.
k w PG(16K) PG(64K)
A OptimizingNeurocache
64 1 14.117 14.118
Hyperparameters
32 2 13.720 13.578
16 4 13.745 13.596
EffectiveprocessingoflongdocumentsinNeuro-
cache depends on the optimal tuning of various
Table5: Perplexityforvaryingretrievalwindowsizew.
retrievalhyperparameters. Tothisend,weconduct
acomprehensivehyperparametersearchfocusedon
languagemodelingperformanceusingtheProject Attention Context Size (c): Table 6 shows that
Gutenberg-19(PG)dataset. thecache-attentioncontextsizec = 2achievesthe
Our exploration encompasses a range of val- lowestperplexity,indicatingoptimalperformance
ues for key hyperparameters: the number of whenextendingcache-attentiontoboththecurrent
retrieved neighbors (k) with values in the set and previous tokens’ retrievals. We fix c = 2 for
[None,8,16,32,64,128,256], the retrieval win- thesubsequentexperiments.
dow size (w) tested with [1,2,4], the cache-
attentioncontextsize(c)evaluatedat[1,2,4],and k c PG(16K) PG(64K)
the encoding dimension of hidden states (d) ex-
32 1 13.720 13.578
plored across [1024,512,256,128,64]. This sys-
16 2 13.704 13.564
tematic investigation aims to identify the opti-
8 4 13.791 13.661
malconfigurationsthatenhanceNeurocache’sef-
ficiencyandeffectivenessinhandlinglarge-scale
Table6: Perplexityforvaryingcontextsizec.
textualdata.
NumberofRetrievedNeighbors(k): Theinflu-
Encoding hidden states (d): Finally, we assess
ence of k, the number of retrieved neighbors, on
theimpactofencodinghiddenstatesintosmallerdi-
model performance is examined. Table 4 shows
mensionsd,ascomparedtotheoriginalh = 1,024.
that increasing k generally leads to a decrease in
Table 7 demonstrate performance degradation as
perplexity,indicatingimprovedperformance. How-
smallersizesofcompressionareused.
ever,thecomputationalcostalsoincreasespropor-
tionally with k. We pick k = 64 due to the di-
d PG(16K) PG(64K)
minishingreturnsandtheincreasingcostoflarger
values. 1024 13.704 13.564
512 13.740 13.594
k PG(16K) PG(64K) 256 13.779 13.641
128 13.853 13.730
None 14.739 14.739
64 13.983 13.891
8 14.362 14.398
16 14.242 14.259
Table7: Perplexityforvaryingd.
32 14.186 14.190
64 14.117 14.118
128 14.069 14.037 B NeurocacheAlgorithm
256 14.052 13.988
• The cache C is initialized to store com-
cache
Table4: Perplexityforvaryingnumberofneighborsk. pactrepresentations,withamaximumcapac-
ityofmentries.
RetrievalWindowSize(w): Adjustingw while • The long document D is segmented into se-
fixingthetotalnumberofneighborsat64,wefind quencesofntokenseach.Algorithm1NeurocacheProcessing
Require: LongdocumentD,Segmentsizen,NumberoftransformerlayersL,Numberoflowerlayersr,Cachememorysize
m,Projectiondimensionshtod,Numberofneareststatesk
Ensure: UpdatedcacheC afterprocessingeachsegment
cache
1: InitializecacheC withsizem×d
cache
2: DividedocumentDintosegmentsS =(s ,s ,...)
1 2
3: foreachsegments∈Sdo
4: Hr ←Processsthroughlowerrstandarddecoderlayers
5: C ←W ·Hr ▷Projecthiddenstatestocompactrepresentation
p
6: C ←Retrievetop-kneareststatesfromC basedonL2-distancetoC
ret cache
7: forj ←r+1toLdo
8: Qj ←Wj·Hj ▷Generatequeriesforcacheattention
q
9: Kj ←Wj·C ▷Generatekeysforcacheattention
ret k ret
10: Vj ←Wj·C ▷Generatevaluesforcacheattention
ret v ret
11: CA←ApplycacheattentionusingQj,Kj ,Vj
ret ret
12: CA←CA·Wj ▷Applyoutputprojectionforcacheattention
o
13: Hj ←CombineCAwithself-attentionoutputsandHj
14: endfor
15: UpdatecacheC withC,discardoldestifcacheexceedsm
cache
16: endfor
• Each segment s undergoes sequential pro-
i
cessingthroughthetransformerdecoderlay-
ers.
• Atthemiddlerth layer,thehiddenstatesHr
are converted into a compact representation
C.
• The nearest cached states C to C are re-
ret
trieved from C using a k-nearest-neighbor
(kNN)method.
• In each augmented layer j > r, cache atten-
tioniscalculatedusingthegeneratedqueries
Qj,keysKj ,andvaluesVj
.
ret ret
• TheoutputofcacheattentionCAismerged
with the self-attention outputs and subse-
quentlyprocessedthroughafeed-forwardnet-
work(FFN).
• After processing each segment, the cache C
isupdatedwiththenewcompactrepresenta-
tionC,andtheoldestentriesarediscardedas
neededtomaintainthecachesize.
The cache-attention mechanism in the aug-
mented layers is designed to focus on the most
relevantinformationretrievedfromthecache,akin
totheapproachinMemorizingTransformers(Wu
et al., 2022). Cache-attention implementation is
giveninFigure3.def cache_attention(
ret_keys,
ret_vals,
queries
):
# Attention computation over states retrieved from the cache.
# ret_keys: Retrieved keys (bsz, n_queries, n_heads, n_neighbors, head_dim)
# ret_vals: Retrieved values (bsz, n_queries, n_heads, n_neighbors, head_dim)
# queries: Queries (bsz, n_queries, n_heads, head_dim)
# Calculate attention weights.
ret_attn = einsum("...qhd,...khd->...qk", queries, ret_keys)
ret_attn = softmax(ret_attn, dim=-1)
# Compute the weighted sum of extended values.
attn_output = einsum("...qk,...khd->...qhd", ret_attn, ret_vals)
return attn_output
Figure3: Thisimplementationshowcasesthecache-attentioncomputationinthemodel. Itcalculatestheattention
weightsthroughadotproductbetweenthequeriesandkeys,appliesasoftmaxtoobtainprobabilities,andthen
computestheweightedsumoftheextendedvaluestogeneratethefinalattentionoutput.