Statistical Advantages of Oblique Randomized Decision
Trees and Forests
Eliza O’Reilly∗
Abstract
This work studies the statistical advantages of using features comprised of general linear
combinationsofcovariatestopartitionthedatainrandomizeddecisiontreeandforestregression
algorithms. Using random tessellation theory in stochastic geometry, we provide a theoretical
analysis of a class of efficiently generated random tree and forest estimators that allow for
oblique splits along such features. We call these estimators oblique Mondrian trees and forests,
as the trees are generated by first selecting a set of features from linear combinations of the
covariates and then running a Mondrian process that hierarchically partitions the data along
these features. Generalization error bounds and convergence rates are obtained for the flexible
dimension reduction model class of ridge functions (also known as multi-index models), where
the output is assumed to depend on a low dimensional relevant feature subspace of the input
domain. Theresultshighlighthowtheriskoftheseestimatorsdependsonthechoiceoffeatures
and quantify how robust the risk is with respect to error in the estimation of relevant features.
The asymptotic analysis also provides conditions on the selected features along which the data
is split for these estimators to obtain minimax optimal rates of convergence with respect to
the dimension of the relevant feature subspace. Additionally, a lower bound on the risk of
axis-aligned Mondrian trees (where features are restricted to the set of covariates) is obtained
proving that these estimators are suboptimal for these linear dimension reduction models in
general, no matter how the distribution over the covariates used to divide the data at each tree
node is weighted.
1 Introduction
Random forests are a widely used class of machine learning algorithms that achieve competitive
performance for many tasks [10, 15]. The original algorithm popularized by Breiman [7], and
influenced by the work of Amit and Geman [1] and Ho [17], remains highly valued for its relative
interpretability and ability to handle large datasets with high dimensionality. There has also been
a recent surge in progress in understanding the statistical properties of Breiman’s random forest
including consistency rates in fixed and high dimensional settings [35, 11, 36, 19]. The algorithm
is an ensemble method, outputting predictions that average the predictions across a collection of
randomized decision trees. Each tree recursively splits the training data using a set of features of
the input and a prediction for a new input is determined by the labels of the training data lying in
the same leaf of the tree, or equivalently, the same cell of the random hierarchical partition of the
input space generated by the splits.
Randomforestsmostcommonlyusedinpracticearerestrictedtoaxis-alignedsplits, whereonly
one dimension, or covariate, of the input data is used to partition the data in a given node of the
∗Department of Applied Mathematics and Statistics, Johns Hopkins University, Baltimore, MD, 21218
(eoreill2@jh.edu)
1
4202
luJ
2
]TS.htam[
1v85420.7042:viXratree. This generates random partitions of the input space made up of cells that are axis-aligned
boxes, producing step-wise decision boundaries. The geometry of axis-aligned partitions limits the
model’s ability to capture dependencies between dimensions of the input, and the corresponding
theory and consistency rates have generally been limited to the assumption that the regression
function comes from an additive model. Oblique random forests are variants of the algorithm
where splits are allowed to depend on linear combinations of the covariates. There have been many
approaches for choosing these split directions and the resulting estimators have shown improved
empirical performance in a variety of settings over axis-aligned versions [7, 5, 14, 24, 31, 37].
Some recent work [8, 41] has also obtained convergence rates for oblique random trees utilizing
the CART methodology of Breiman’s random forest under the assumption of additive single-index
regression models. However, theoretical guarantees remain scarce and a complete understanding of
the statistical advantages of oblique random forests over axis-aligned versions is lacking.
There are many difficulties in analyzing Breiman’s original random forest algorithm due to the
complex dependence of the partitioning scheme on the inputs and labels of the training dataset. To
overcome these challenges in the axis-aligned case, simplified versions of the algorithm where the
splits do not use the labels of the data have also been studied, including centered random forests
[4] and median random forests [13]. Both of these variants, however, have since been shown to be
minimax suboptimal for input dimensions greater than one [18]. The first random forest variant
for which minimax optimal convergence rates were obtained are Mondrian random forests [26],
where component trees are generated by a Mondrian process [32]. Recent work [9] has also proved
a central limit theorem for Mondrian forest point estimators and shown that a debiased variant of
Mondrian forests can achieve minimax rates for general H¨older classes.
GiventheamenabilityoftheMondrianpartitioningmechanismtotheoreticalanalysis,anatural
direction for studying oblique random forests is to study variants of the Mondrian process that use
linear combinations of covariates to make splits. Fortunately, the Mondrian process is a special
case of the general class of STIT processes in stochastic geometry introduced by Nagel and Weiss
[28, 23]. STIT processes all satisfy properties such as spatial consistency and the Markov property
that are attractive about the Mondrian process, but form a much more general class of stochastic
hierarchical partitioning processes indexed by a probability measure on the unit sphere called a
directional distribution that governs the distribution of split directions. Utilizing STIT processes
to generate randomized decision trees thus forms a rich and flexible class of oblique random forests.
This class of algorithms, called random tessellation forests, has been studied empirically in [16]
and the theory of random tessellations in stochastic geometry has been used in [29, 30] to provide
a theoretical framework for the use of these STIT processes in machine learning applications. In
particular, the results of [30] extend the minimax rates obtained for Mondrian forests to random
tessellation forests for any fixed directional distribution. These were the first minimax optimality
guarantees for random forest variants with oblique splits. However, these worst-case risk bounds
for Lipschitz and C2 functions do not illustrate an advantage of using oblique splits over Mondrian
forests. These rates also suffer from the curse of dimensionality when the input is not contained in
a low dimensional subspace, becoming very slow when the ambient dimension of the input is large.
In this paper, we address these theoretical limitations by studying how this choice of directional
distribution allows random tessellation trees and forests to adapt to a flexible class of dimension
reduction models. This effort shows the power of these models to overcome the curse of dimension-
ality and establishes a statistical advantage of employing oblique splits in random forest regression.
Prior results on the adaptation of random forests to low dimensional structure have focused on the
axis-aligned setting and adaptation to sparse regression functions, where the output only depends
on a small number of covariates relative to the ambient dimension. This work establishes that with
a good choice of the directional distribution governing the directions of the hyperplane splits, ran-
2domtessellationforestsadapttothemoregeneraldimensionreductionclassofridge functions, also
referred to as multi-index models. Ridge functions are those for which the output only varies with
respect to changes of the input in directions relative to a low dimensional subspace of Rd, called
the relevant feature subspace, or active subspace. These regression model classes are as general as
those studied for neural networks [3], laying additional groundwork for theoretical comparison of
the statistical properties of random forests versus neural networks.
Our specific contributions are the following. We first obtain a general upper bound for the risk
ofrandomtessellationtreesandforestswhentheunderlyingregressionfunctioncomesfromamulti-
index model. These bounds illuminate how the risk of the estimator is controlled by the geometry
of convex bodies associated with the random tessellation model projected onto the relevant feature
subspace (see Theorems 6 and 7). Next, we restrict to studying random tessellation trees and
forestsgeneratedbySTITprocesseswherethedirectionaldistributionisdiscrete. Wewillcallthese
estimatorsobliqueMondriantreesandforests becausetheycanbeobtainedbyfirstapplyingalinear
transformation to the data to obtain a new set of features from linear combinations of covariates,
and then running a Mondrian process (see Section 7). Our results include an upper bound on the
risk of the estimators controlled by constants quantifying how close the linear transformation is to
a projection onto the relevant feature subspace. These bounds quantify how robust the estimator
is to the approximation error of relevant features (see Theorems 8 and 10). We then establish
sufficient conditions for the linear transformation under which, with proper tuning of complexity
parameters, minimax rates of convergence depending only on the dimension of the relevant feature
subspace are obtained (see Corollaries 9 and 11).
Finally, we obtain a suboptimality result for axis-aligned randomized decision trees. Indeed,
while our first collection of results shows that oblique Mondrian trees have the ability to obtain
improved rates of convergence for multi-index models over those for general Lipschitz functions
on Rd, we also obtain a risk lower bound for axis-aligned Mondrian trees showing that for any
choice of weights over the covariates, the axis-aligned splits cannot achieve such improved rates of
convergence for general ridge functions (see Theorem 16).
1.1 Outline
The remainder of this paper is organized as follows. Section 2 covers the relevant definitions and
background from stochastic and convex geometry needed to prove our results. Section 3 describes
theproblemsettingandnotationfollowedbyriskupperboundsforgeneralrandomtessellationtrees
and forests when the underlying regression function comes from a multi-index model. Section 4
presentsourmainresultsonconvergenceratesforobliqueMondrianforests,andSection5considers
the special case of axis-aligned weighted Mondrian forests and sparse regression models. Section 6
presents our final main result on the suboptimality of weighted Mondrian forests for general ridge
functions. Crucial to our main results is the observation that oblique Mondrian processes obtained
through a linear transformation of the data and a Mondrian process is equivalent to partitioning
withaSTITprocesswithaparticulardiscretedirectionaldistributionandthisisstatedandproved
in Section 7. Finally, Section 8 concludes with a discussion of the results and future work, and
Section 9 collects some of the proofs of our main results. The remaining proofs are contained in
the supplementary material.
2 Background
In this section, we briefly describe the necessary definitions and other background from stochastic
geometryandconvexgeometryneededforthestatementsandproofsofourresults. Inthefollowing,
3we will denote by κ the volume of the unit ℓ ball Bk in Rk for k ∈ N.
k 2
2.1 Stable Under Iteration (STIT) Tessellations
A random tessellation P of Rd is a point process of compact convex polytopes {C i} i∈N in Rd such
that almost surely, ∪ C = Rd and int(C ) ∩ int(C ) = ∅ for all i ̸= j. These polytopes will be
i i i j
referred to as the cells of the tessellation in the following. A random tessellation is stationary if
the distribution of P is invariant under translations in Rd.
The iteration of a random tessellation is the process of subdividing each cell of the tessellation
by an independent copy of the random tessellation restricted to that cell. A random tessellation
is stable under iteration (STIT) if for all n, iterating n times and scaling all the boundaries by n
recovers in distribution the original random tessellation.
The distribution of a stationary STIT tessellation of Rd is determined by a parameter λ > 0
called the lifetime and an even probability measure ϕ on Sd−1 called the directional distribution.
The following procedure describes the stochastic STIT process on a compact window W ⊂ Rd,
which constructs a STIT tessellation restricted to W with lifetime λ and directional distribution ϕ:
1. Sample an exponential clock δ with parameter
(cid:90)
(h(W,u)+h(W,−u))dϕ(u),
SD−1
where h(W,u) := sup ⟨u,x⟩ is the support function of W.
x∈W
2. If δ > λ, stop. Else, at time δ, generate a random hyperplane
H(U,T) := {x ∈ Rd : ⟨x,U⟩ = T},
where the unit normal direction U is drawn from the distribution
h(W,u)+h(W,−u)
dΦ(u) := dϕ(u), u ∈ SD−1,
(cid:82)
(h(W,u)+h(W,−u))dϕ(u)
SD−1
and conditioned on U, T is drawn uniformly on the interval from −h(W,−U) to h(W,U).
Split the window W into two cells W and W with H ∩W.
1 2
3. Repeat steps (1) and (2) in each sub-window W and W independently with new lifetime
1 2
parameter λ−δ until lifetime expires.
Whenϕistheuniformdistributionoverthestandard(signed)basisvectorsinRd,thecorresponding
STIT process has the same distribution as the Mondrian process [32].
We refer to [28] for the proof of the existence of STIT tessellations on Rd and some of their
properties, one of which we recall here. For a STIT tessellation P(λ) with lifetime λ > 0, let Y(λ)
denote the union of boundaries of the polytopes. The STIT property implies the following useful
scaling property of STIT tessellations:
(d)
λY(λ) = Y(1). (1)
42.1.1 Cells of Stationary Random Tessellations
LetZλ bethecellcontainingthepointx ∈ Rd ofastationarySTITtessellationwithlifetimeλ > 0.
x
The cell Zλ containing the origin is called the zero cell. By stationarity and the scaling property
0
(1),
Zλ ( =d) 1 Z +x, (2)
x λ 0
for all x ∈ Rd, where Z is the zero cell of the STIT tessellation with lifetime 1. Another random
0
polytope associated with a stationary STIT tessellation is called the typical cell. To define this,
firstletK denotethespaceofcompactandconvexpolytopesinRd andletc : K → Rd beafunction
that assigns a “center” to each polytope K ∈ K such that c(K +x) = c(K)+x for all x ∈ Rd.
Now let K := {K ∈ K : c(K) = 0}. The typical cell Z of a stationary random tessellation P is the
0
random polytope in K such that for any non-negative measurable function f on K,
0
(cid:34) (cid:35)
(cid:20)(cid:90) (cid:21)
(cid:88) 1
E f(C) = E f(Z +y)dy . (3)
E[vol (Z)]
D Rd
C∈P
The above equality is a special case of Campbell’s theorem applied the the stationary point process
of convex polytopes that make up the cells of the random tessellation. We refer to [34, Section 4.1]
for further details.
2.1.2 Associated Zonoid
There is a rich connection between STIT tessellations and the geometry of convex bodies. In
particular, the class of STIT tessellations in Rd have a one-to-one correspondence to a subset
of convex bodies in Rd called zonoids [33]. This class of convex bodies are those that can be
approximated by finite Minkowksi sums of line segments with respect to Hausdorff distance. Recall
the Minkowksi sum K +L of two convex bodies K and L in Rd is defined by
K +L := {x+y : x ∈ K,y ∈ L} ⊆ Rd.
A convex body Π in Rd is a zonoid if and only if it has support function of the form h (u) =
Π
(cid:82)
|⟨u,v⟩|dµ(v) for some finite positive measure µ on the unit sphere. We can thus define a
Sd−1
particular zonoid for a STIT tessellation through its directional distribution.
Definition 1. The normalized associated zonoid of a STIT process in Rd with directional distri-
bution ϕ is the zonoid with support function
(cid:90)
1
h (u) := |⟨u,v⟩|dϕ(v). (4)
Π
2
Sd−1
In the sequel we will use the the following known fact (see [27] and [34, (10.4) and (10.44)]):
1
E[vol (Z)] = , (5)
d
vol (Π)
d
where Z is the typical cell of a STIT process with lifetime 1 and normalized associated zonoid Π.
Example 2. An isotropic STIT process is obtained by taking the directional distribution to be
ϕ ∼ Uniform(Sd−1). In this case, the normalized associated zonoid Π = c Bd is an ℓ ball with
d 2
unit mean width.
5(a) Weighted Mondrian (b) Oblique Mondrian
Figure 1: An illustration of (a) a weighted Mondrian process with its associated zonoid Π as in
Example 3 and (b) an oblique Mondrian process and its associated zonoid Π as in Example 4.
Example 3. The Mondrian process in Rd is a special case of a STIT process when the directional
distribution is given by ϕ = 1 (cid:80)d (δ +δ ), where {e }d is the standard orthonormal basis
2d i=1 ei −ei i i=1
in Rd. The normalized associated zonoid is the ℓ∞ ball
Π = [−e /2d,e /2d]+···+[−e /2d,e /2d].
1 1 d d
When the unit basis directions are given more general weights, i.e. ϕ = (cid:80)d ωi (δ +δ ) where
i=1 2 ei −ei
(cid:80)d
ω = 1 and ω > 0 for all i, then the normalized associated zonoid is the hyperrectangle
i=1 i i
Π = [−ω e /2,ω e /2]+···+[−ω e /2,ω e /2],
1 1 1 1 d d d d
and we call the associated STIT process a weighted Mondrian process.
Example4. AgeneraldiscretedirectionaldistributiononSd−1hastheformϕ = (cid:80)m ωi (δ +δ )
for some m ≥ d, where the weights {ω }m satisfy ω > 0 and (cid:80)m ω = 1 andi=1 th2 e diu ri ectio− nu si
i i=1 i i=1 i
u ∈ Sd−1 for i = 1,...,m span all of Rd. Then, the normalized associated zonoid is given by
i
Π = [−ω u /2,ω u /2]+···+[−ω u /2,ω u /2],
1 1 1 1 m m m m
i.e. it is the Minkowski sum of m line segments. In this case, we refer to the corresponding STIT
process as an oblique Mondrian process.
62.2 Intrinsic Volumes and Mixed Volumes
Steiner’s formula in convex geometry gives an expression of the volume of the parallel body of a
convex body K at distance ε. That is,
d
(cid:88)
vol (K +εBd) = εd−jκ V (K). (6)
d d−j j
j=0
The constants V (K) are called the intrinsic volumes of K. The values of these constants only
j
depend on K, not the ambient space that K is embedded in. In particular, if K is ℓ-dimensional,
V (K) = vol (K), the usual ℓ-dimensional Lebesgue measure of K; V (K) = 1Hℓ−1(∂K), where
ℓ ℓ ℓ−1 2
Hℓ−1(∂K) is the usual surface area measure of the boundary of K; and V (K) is the number of
0
connected components of K. When K is the ball of unit radius Bd in Rd, for all j = 1,...,d,
(cid:18) (cid:19)
d κ
V (Bd) = d , (7)
j
j κ
d−j
and when K is the unit cube [0,1]d, for all j = 1,...,d,
(cid:18) (cid:19)
d
V ([0,1]d) = . (8)
j
j
More generally, for d convex bodies K ,...,,K in Rd, we notate their mixed volume by
1 d
V(K ,...,K ).
1 d
This functional is multilinear in its arguments, symmetric, positive, and monotonic in each variable
with respect to inclusion. For additional background on intrinsic volumes and mixed volumes see
[34, Chapter 14].
3 Regression Setting and Risk Bounds
Consider the following nonparametric regression setting. Fix a compact and convex d-dimensional
domainW ⊂ Rd andsupposethedatasetD := {(X ,Y ),...,(X ,Y )}consistsofni.i.d. samples
n 1 1 n n
from a random pair (X,Y) ∈ W ×R such that E[Y2] < ∞. Let µ denote the unknown distribution
of X and assume
Y = f(X)+ε, (9)
for some unknown function f : Rd → R and noise ε satisfying E[ε|X] = 0 and Var(ε|X) = σ2 < ∞
almost surely. We make the additional assumption that the function f is of the form
f(x) = g(Ax), x ∈ Rd, (10)
where g : Rs → R and A ∈ Rs×d for s ≤ d. This is a general dimensionality reduction model known
as a multi-index model or ridge function, where the regression function depends only on the inputs
⟨a ,X⟩,...,⟨a ,X⟩, where {a }s are the rows of A. Let S := span({a }s ) denote the associated
1 s i i=1 i i=1
relevant feature subspace. An equivalent assumption is that
f(x) = g˜(P x), (11)
S
for some g˜ : S → R where P is the orthogonal projection operator onto the subspace S. In the
S
following, we will assume g˜ satisfies the following regularity condition.
7Definition 5. A function g : Rd → R is in Ck,β(L) for L > 0 if for all x,y ∈ Rd and α ≤ k,
∥Dαf(x)−Dαf(y)∥ ≤ L∥x−y∥β.
To estimate f, we use a random forest estimator built from a random tessellation P of W and
the data set D . A regression tree estimator based on P is first defined as
n
fˆ (x,P) :=
(cid:88)n 1
{Xi∈Zx} Y , (12)
n i
N (x)
n
i=1
where Z is the cell of P that contains x and N (x) :=
(cid:80)n
1 is the number of points in
x n i=1 {Xi∈Zx}
Z . If N (x) = 0, then it is assumed that fˆ (x,P) = 0. The random forest estimator based on P
x n n
is defined by averaging M i.i.d. copies of the tree estimator, i.e.
M
1 (cid:88)
fˆ (x) := fˆ (x,P ), (13)
n,M n m
M
m=1
where P ,...,P are M i.i.d. copies of P.
1 M
A random tessellation forest estimator is defined as a random forest estimator, where the
random tessellation P is the tessellation generated by a STIT process. This class of estimators is
parameterizedbyalifetimeλ > 0andadirectionaldistributionϕontheunitsphere,orequivalently,
a normalized associated zonoid Π.
3.1 Risk Bounds for Ridge Functions
Our first two main results provide upper bounds on the quadratic risk for a general random tessel-
lation forest estimator of a ridge function. In the following, we will denote the diameter of a convex
body K in Rd by D(K), and for a linear subspace S in Rd we will denote by P K the orthogonal
S
projection of K onto S and P K the orthogonal projection of K onto the orthogonal subspace
S⊥
S⊥ to S.
Theorem 6. Assume supp(µ) ⊆ Bd and f satisfies (11) with g˜ ∈ C0,β(L) for some L > 0. Let
fˆ = fˆ be a random tessellation forest estimator with normalized associated zonoid Π, M
n n,M,λ,Π
trees, and lifetime λ > 0. Then,
E[(fˆ (X)−f(X))2] ≤
L2E[D(P SZ 0)2β]
+
(5∥f∥2 ∞+2σ2) (cid:32) (cid:88)d
c λkD(P
Π)k−s+(cid:88)s
c
λk(cid:33)
,
n λ2β n d,k S⊥ d,k
k=s+1 k=0
where c := κ kπk/2dk/2 .
d,k k!
The upper bound for the random tessellation forest above is obtained by first bounding the
forest risk by the risk of a single tree estimator and then considering a standard bias-variance
decomposition. The first expression in the upper bound is a bound on the bias, or approximation
error, of the tree estimator quantifying how well a C0,β(L) function can be approximated by any
function that is constant over the cells of the corresponding tessellation of the input space. For all
inputs that lie in the same cell, the estimator will output the same prediction, and thus given the
assumption on f, this error is controlled by L and the diameter of the projection of this cell onto
the relevant feature subspace. The second expression is a bound on the variance, or the estimation
error of the model. This is controlled by the amount of data and the complexity of the model,
8which for randomized decision trees can be quantified by the number of cells of the tessellation,
or equivalently, the number of leaves of the corresponding tree. The first term in the parenthesis
comes from using a STIT process that makes splits in directions not aligned with the relevant
feature subspace S. Note that if the associated zonoid Π is contained in S, i.e. all splits directions
are contained in S, then the variance term will have order λs/n, which is the order of the variance
for a random tessellation tree estimator with lifetime λ of a function on Rs.
As in Theorem 6 of [30], the upper bound in Theorem 6 does not depend on the number of
trees M and thus holds for a single random tessellation tree estimator. In the following result, we
assume a stronger regularity condition on the regression function, as well as stronger assumptions
on the input distribution µ, and obtain an upper bound that does depend on the number of trees
M in the forest estimator.
Theorem 7. Assume supp(µ) ⊆ Bd, µ has a positive and Lipschitz density on its compact and
convex support K, and suppose K = K + K , where K ⊆ S and K ⊆ S⊥. Assume f
S S⊥ S S⊥
satisfies 11 with g˜∈ C1,β(L) and let fˆ = fˆ be the random tessellation forest estimator with
n n,M,λ,Π
normalized associated zonoid Π, M trees, and lifetime λ > 0. Let r(K) denote the radius of the
largest ball contained in K and define K := {x ∈ K : d(x,∂K) ≥ δ}. Then, for fixed δ ∈ (0,r(K)),
δ
and constants c˜ , i = 1,...,3 that just depend on µ, we have
i,µ
E[(fˆ (X)−f(X))2|X ∈ K ]
n δ
c˜ L2(cid:18) (cid:20) (cid:18) vol (Z ) (cid:19)(cid:21) E(cid:2) D(P Z )2(cid:3) E[D(P Z )1+β](cid:19)2
≤ 1,µ E D(P Z ) 1− d 0 + S 0 + S 0
λ2 S 0 vol (P Z +P Z ) λ λβ
d S 0 S⊥ 0
+
c˜ 2,µL2 (cid:88)s−1 κ s−jV j(K S)
E(cid:2) D(P Z )s−j+21 (cid:3)
λ3 λs−1−jvol (K ) S 0 {D(PSZ0)≥λδ}
d δ
j=0
c˜ L2(cid:18) (cid:20) (cid:18) vol (Z ) (cid:19)(cid:21) E(cid:2) D(P Z )2(cid:3) E[D(P Z )1+β](cid:19)
+ 3,µ E D(P Z ) 1− d 0 + S 0 + S 0
λ3 S 0 vol (P Z +P Z ) λ λβ
d S 0 S⊥ 0
s−1
·(cid:88) κ s−jV j(K S) E(cid:2) D(P Z )s−j+11 (cid:3)
λs−1−jvol (K ) S 0 {D(PSZ0)≥λδ}
d δ
j=0
+
L2E[D(P SZ 0)2]
+
5∥f∥2 ∞+2σ2 (cid:32) (cid:88)d
c λkD(P
Π)k−s+(cid:88)s
c
λk(cid:33)
.
λ2M np vol (K ) d,k S⊥ d,k
0 d δ
k=s+1 k=0
The upper bound above is also a result of a bias-variance decomposition of the risk of a random
tessellation forest estimator, where the last term is similar to the upper bound on the variance as in
Theorem 6, and the remaining terms are an upper bound on the bias for the forest estimator that
exploits the additional smoothness assumption. This bias upper bound depends more delicately on
the geometry of the zero cell and its relation to the relevant feature subspace S than in Theorem
6. In the next section this result will be used to obtain an improved rate of convergence for oblique
Mondrian forests under additional assumptions.
TheupperboundsofTheorem6andTheorem7illuminatehowtheriskfortherandomtessella-
tion estimator of a ridge function depends on the relationship between the geometry of normalized
associated zonoid of the STIT tessellation and the zero cell to the relevant feature subspace S.
Figure 2 illustrates this relationship and how ensuring the projection of Π onto S⊥ is small means
the relevant subspace is more efficiently subdivided for a given lifetime λ and the projection of Z
0
onto S can be controlled, ensuring a smaller risk.
9Figure 2: Illustration of an associated zonoid and corresponding STIT tessellation in relation to a
relevant feature subspace S. If the projection of Π onto S⊥ is small, then S is cut more frequently
by the boundaries of the STIT tessellation for a given lifetime.
4 Convergence Rates for Oblique Mondrian Trees and Forests
The risk upper bounds in the previous section hold for random tessellation trees and forests with
any associated directional distribution. We next would like to obtain rates of convergence for a
sequence of random tessellation forests estimators built from n data points as n grows. The results
in [30] provide such rates when the lifetime grows with n and the directional distribution is fixed
for all n. Here, we consider the case when the directional distribution is also allowed to depend on
n, representing an estimator that uses a data-driven choice of directional distribution to generate
the STIT process. Unfortunately, it is difficult in general to obtain closed form expressions for the
terms in the bounds from Theorems 6 and 7 that depend on the directional distribution through
the diameter of the normalized zero cell projected onto the relevant feature subspace S. Without
further understanding how these terms explicitly depend on the directional distribution or the
normalized associated zonoid, we cannot in general obtain the asymptotic behavior of the bias for
a sequence of estimators where this parameter depends on n.
Toovercomethischallenge,werestricttothesubclassofSTITprocesseswithdiscretedirectional
distributions, where the directions of the splits are sampled from a finite discrete set of vectors on
the unit sphere. That is, there is a finite set of linear combinations of covariates along which the
STIT process makes splits. Under this assumption, we can obtain bounds on the relevant statistics
that will subsequently elucidate the asymptotic behavior of the risk upper bounds. Another reason
for focusing on this subclass of STIT processes is that the partition of the data they generate can
be efficiently obtained by first applying a linear transformation to the input data, and then running
a Mondrian process. As mentioned in the introduction, we will thus call this subclass oblique
Mondrian processes and refer to the corresponding tree and forest estimators as oblique Mondrian
trees and forests.
In particular, for a matrix A ∈ Rd×m define the directional distribution
m
(cid:88) ∥a i∥ 2 (cid:0) (cid:1)
ϕ = δ +δ , (14)
A 2∥A∥ ai/∥ai∥2 −ai/∥ai∥2
2,1
i=1
where {a }m are the columns of A, and ∥A∥ = (cid:80)m ∥a ∥ is the norm of the matrix that sums
i i=1 2,1 i=1 i 2
the ℓ -norms of the column vectors. We assume the columns contain d linearly independent vectors
2
in Rd, i.e. the rank of A is d ≤ m. The partition of the data induced by a STIT tessellation with
10directional distribution ϕ can be efficiently obtained by applying the transformation AT to the
A
data and then running a Mondrian process. This is proved in Section 7, and is a refinement of
Theorem 3.1 in [29].
Our first result of this section is an upper bound on the risk of an oblique Mondrian forest for
a regression function satisfying the same assumption as in Theorem 6.
Theorem 8. Assume supp(µ) ⊆ Bd and f satisfies (11) with g˜ ∈ C0,β(L) for some L > 0. Let
fˆ = fˆ be an oblique Mondrian forest estimator with lifetime λ and directional distribution ϕ
n n,λ,M A
as in (14) for some A ∈ Rd×m with rank d ≤ m and ∥A∥ = 1. Then,
2,1
9L2m4β
E[(fˆ (X)−f(X))2] ≤
λ,n,M d2βλ2βσ (P A)2β
s S
(5∥f∥2 +2σ2) (cid:32) (cid:88)d (cid:88)s (cid:33)
+ ∞ c λk∥P A∥k−s+ c λk ,
n d,k S⊥ 2,1 d,k
k=s+1 k=0
where c := κ kπk/2dk/2 and σ (P A) is the s-th largest singular value of the matrix P A.
d,k k! s S S
If the relevant feature subspace S is known, one can project the input data onto S and then
generatearandomtessellationforestestimatorsupportedonthiss-dimensionalsubspace. Minimax
optimal rates for functions on Rs will be obtained with such an estimator by appropriately tuning
λ with n as in [30]. In practice, we do not know this subspace and so instead we can estimate a
linear image A that approximates a projection onto S and build an oblique Mondrian estimator.
From the definition of ϕ in (14), the columns of A determine the directions and weights of the
A
splits used to generate each tree. Hopefully, the projection of these columns vectors onto S⊥ have
small norm, that is, they either have direction close to the span of S or have small norm giving the
associateddirectionasmallweightsothattheobliqueMondrianprocessrarelymakesasplitinthat
direction. The bound in Theorem 8 above then quantifies how the risk of the corresponding oblique
Mondrian estimator depends on the choice of this A, including how it depends on the projection of
the columns of A onto S⊥ though the quantity ∥P⊥A∥ .
S 2,1
We next model the results of a data-driven procedure for selecting a set of split directions with
a sequence of matrices A that will be applied to inputs of the dataset D of size n. The following
n n
result provides a rate of convergence of the corresponding sequence of oblique Mondrian forests.
This rate depends on how well A approximates a projection onto S as n grows. As long as this
n
approximation error approaches zero in the limit, we obtain an improved rate of convergence for
ridge functions over the worst-case minimax rate for C0,β functions on Rd. In addition, these rates
provide a sufficient condition for this approximation error such that these oblique Mondrian forests
achieve the minimax optimal rate of convergence for C0,β functions on Rs, where s is the dimension
of the relevant feature subspace.
Corollary 9. Consider the setting of Theorem 8. For each n, let fˆ be an oblique Mondrian
n
forest with lifetime λ and directional distribution ϕ for some A ∈ Rd×m with rank d ≤ m and
n An n
∥A ∥ = 1. Assume there is an absolute constant c > 0 such that
n 2,1
(i) σ (P A ) ≥ c, and
s S n
(ii) ∥P A ∥ ≤ ε for ε = o(1).
S⊥ n 2,1 n n
2 1
−(d−s)
Then, letting λ
n
≍ Ld+2βnd+2βε
n
d+2β yields
E(cid:20) (cid:16) f(X)−fˆ n,λn,Mn(X)(cid:17)2(cid:21) ≲ max(cid:26) Ld+2d 2βn− d+2β 2βε n2β d( +d 2− βs) ,Ls+2s 2βn− s+2β 2β(cid:27) . (15)
11If ε
n
≲ L− s+2 2βn− s+1 2β, then for λ
n
≍ Ls+2 2βns+1 2β,
E(cid:104) (f(X)−fˆ λn,n(X))2(cid:105) ≲ Ls+2s 2βn− s+2β 2β. (16)
which is the minimax rate for the class of C0,β(L) functions on Rs.
The above results hold for oblique Mondrian forests with any number of trees. The advantage
of averaging the prediction of many trees is observed in the following results, which provide a
risk bound that depends on the number of trees for an oblique Mondrian forest estimator when
additional smoothness is assumed for the regression function as in Theorem 7, as well as an im-
provedrateofconvergence. ForasequenceofobliqueMondrianforestswithdirectionaldistribution
depending on n, it is much more difficult to obtain improved rates in this setting with transpar-
ent conditions on the linear transformation A . To provide such conditions, we make the strong
n
assumption that the normal vectors to the hyperplane splits, i.e. the linear combinations of covari-
ates used as features, either already lie in the relevant feature subspace S or lie in the orthogonal
subspace S⊥.
Theorem 10. Assume supp(µ) := K ⊆ Bd and that µ has a positive and Lipschitz density on
its compact and convex support K, and suppose K = K +K , where K ⊆ S and K ⊆ S⊥.
S S⊥ S S⊥
Assume f satisfies (11) for g˜∈ C1,β(L). Let fˆ = fˆ be the random tessellation forest estimator
n n,λ,M
with lifetime λ > 0, M trees, and directional distribution ϕ given by (14) for a nonsingular
A
A ∈ Rd×d with ∥A∥ = 1 and such that P a ∈ {a ,0} for each i = 1,...,d. Let r(K) denote the
2,1 S i i
inradius of K and define K := {x ∈ K : d(x,∂K) ≥ δ}. Then, for fixed δ ∈ (0,r(K)),
δ
c L2Γ(2d+1+β)2 2L2d2
E[(fˆ (X)−f(X))2|X ∈ K ] ≤ µ +
n δ λ2+2βσ (P A)2+2βΓ(2d)2 λ2Mσ (P A)2
s S s S
5∥f∥2 +2σ2 (cid:32) (cid:88)d (cid:88)s−1 (cid:33)
+ ∞ c λk∥P A∥k−s+ c λk +o(λ−2−2β),
np vol (K ) d,k S⊥ 2,1 d,k
0 d δ
k=s k=0
where the constants in the little-o term depend on δ,A,d,L, and β. In the unconditional case when
δ = 0,
c L2Γ(2d+1+β)2 c˜ L2d3V (K )
E[(fˆ (X)−f(X))2] ≤ µ + µ s−1 S
λ,n,M λ2+2βσ (P A)2+2βΓ(2d)2 λ3σ (P A)3vol (K)
s S s S d
2L2d2 5∥f∥2 +2σ2 (cid:32) (cid:88)d (cid:88)s−1 (cid:33)
+ + ∞ c λk∥P A∥k−s+ c λk +o(λ−2−2β).
λ2Mσ (P A)2 np vol (K) d,k S⊥ 2,1 d,k
s S 0 d
k=s k=0
Here, c and c˜ are constants depending only on µ.
µ µ
Usingtheseupperbounds, wearenowabletoobtainconvergenceratesforasequenceofoblique
Mondrian forests corresponding to a sequence of linear maps A that depend on the approximation
n
error between A and a projection onto the relevant feature subspace S similarly to Corollary 9.
n
Corollary 11. Consider the setting of Theorem 10. For each n, let fˆ be an oblique Mondrian
n
forest estimator with lifetime λ , number of trees M , and directional distribution ϕ for some
n n An
A ∈ Rd×d with rank d and ∥A ∥ = 1. Assume there is an absolute constant c > 0 such that
n n 2,1
(i) σ (P A ) ≥ c for all n,
s S n
(ii) ∥P A ∥ ≤ ε for ε = o(1).
S⊥ n 2,1 n n
12For fixed δ ∈ (0,r(K)), letting λ = L2/(d+2+2β)n1/(d+2+2β)ε−(d−s)/(d+2+2β) and M ≳ λ2β yields
n n n n
E[(fˆ n(X)−f(X))2|X ∈ K δ] ≲ max(cid:26) Ld+22 βd +2n− d+2+ 2β2 +β 2ε n(d− d+s) 2( β2 ++ 22β) ,Ls+22 βd +2n− s+2+ 2β2 +β 2(cid:27) . (17)
If ε ≲ L−2/(s+2+2β)n−1/(s+2+2β), then letting λ = L2/(s+2+2β)n1/(s+2+2β) and M ≳ λ2β gives
n n n n
E[(f(X)−fˆ n,λn,Mn(X))2|X ∈ K δ] ≲ Ls+22 βd +2n− s+2+ 2β2 +β 2, (18)
which is the minimax rate for the class of C1,β(L) functions on Rs.
In the unconditional case δ = 0, the rate above holds if 2 − 2β ≤ 3, and otherwise letting
M
n
≳ λ
n
and λ
n
∼ Ld+2 3nd+1 3ε− nd d− +3s gives
(cid:26) (cid:27)
3(d−s)
E[(fˆ λ,n,M(X)−f(X))2] ≲ max Ld2 +d 3n− d+3 3ε nd+3 ,Ls2 +s 3n− s+3 3 ,
and if ε n ≲ L− s+2 sn− s+1 3 we have that for λ n ≍ Ls+2 3ns+1 3 and M n ≳ λ n,
E[(fˆ λ,n,M(X)−f(X))2] ≲ Ls2 +s 3n− s+3 3.
5 Risk Bounds for Weighted Mondrian Forests
Consider now the special case of weighted Mondrian forests obtained from weighted Mondrian
processes as in Example 3. We will study the ability of this subclass of oblique Mondrian forests
to adapt to sparse functions, as has been studied for other variants of axis-aligned random forests.
More specifically, consider the following setting. Assume that S ⊆ {1,...,d} is a subset of size
|S| = s that corresponds to a small subset of the covariates that the regression function varies with
respect to. That is, we assume the true function f is of the form
f(x) = g(x ) = g({x } ) = g(P x), (19)
S i i∈S S
where P is the orthogonal projection operator onto S = span{e : i ∈ S}. Assume the input X is
S i
supported on [0,1]d and Y = f(X)+ε for noise ε as in section 3. Consider a weighted Mondrian
forestestimatorfˆ builtfromni.i.d. samplesof(X,Y)withlifetimeλ anddirectionaldistribution
n n
d
(cid:88) ω i
ϕ = (δ +δ ), (20)
2
ei −ei
i=1
where the weights {ω }d satisfy (cid:80)d ω = 1 and ω > 0 for each i.
i i=1 i=1 i i
The following results are analogous to those presented for oblique Mondrian forests, with upper
bounds on the risk followed by corollaries in the setting where the weights depend on n, modeling
(n)
a data-driven choice of weights. Assuming that the weights ω converge to 0 as n grows if
i
dimension i is not in the set of relevant features S, we obtain rates of convergence and conditions
onthisapproximationerrorneededtoobtainminimaxoptimalratesdependingonthesparsitylevel
s. We state the results in this setting separately from the more general oblique Mondrian forests
because we can obtain a simplified version of the variance bound which gives a weaker condition
on the weights for improved rates than obtained from directly applying the previous results. For
simplicity, we restrict to the case where β = 1 for the assumption on the regression function in the
following statements.
13Theorem 12. Assume supp(µ) ⊆ [0,1]d and f satisfies (19) where g ∈ C0,1(L) for some L > 0,
i.e. g is L-Lipschitz. Let fˆ = fˆ be the weighted Mondrian tree estimator with directional
n λ,n,M
distribution (20) and lifetime λ > 0. Then,
6L2s (5∥f∥2 +2σ2) (cid:89)d
E[(fˆ (X)−f(X))2] ≤ + ∞ (1+λω ),
n λ2ω2 n i
S i=1
where ω := min ω .
S i∈S i
Corollary 13. Consider the setting of Theorem 12. For each n, let fˆ be a weighted Mondrian for-
n
est estimator with lifetime λ and directional distribution ϕ as in (20) where the weights {ω(n) }d
n n i i=1
depend on n. Assume there is an absolute constant c > 0 such that
(n)
(i) ω ≥ c for all n, and
S
(n)
(ii) max ω ≤ ε for ε = o(1).
i∈/S i n n
Then, the same rates as in Corollary 9 hold.
Theorem 14. Assume supp(µ) = [0,1]d and that µ has a positive and Lipschitz density on its
support. Assume f satisfies (19) for some g ∈ C1,β(L) and let fˆ be the weighted Mondrian forest
n
estimator with directional distribution (20) and lifetime λ > 0. Then, for δ ∈ (0,1/2),
E[(fˆ (X)−f(X))2|X ∈ [δ,1−δ]d] ≤
c µs4L2
+
6L2s
+
5∥f∥2 ∞+2σ2 (cid:89)d
(1+λω )+o(λ−4),
n λ4ω4 λ2Mω2 n i
S S i=1
where ω := min ω . For δ = 0,
S i∈S i
E[(fˆ (X)−f(X))2] ≤
c µs4L2
+
c˜ µs4L2
+
6L2s
+
5∥f∥2 ∞+2σ2 (cid:89)d
(1+λω )+o(λ−3),
n λ4ω4 λ3 λ2Mω2 n i
S S i=1
where c and c˜ are constants that depend only on µ.
µ µ
Corollary 15. Consider the setting of Theorem 14. For each n, let fˆ be a weighted Mondrian
n
forest estimator with lifetime λ , number of trees M , and directional distribution ϕ as in (20)
n n n
where the weights {ω(n) }d depend on n. Assume there is an absolute constant c > 0 such that
i i=1
(n)
(i) ω ≥ c for all n, and
S
(n)
(ii) max ω ≤ ε for ε = o(1).
i∈/S i n n
Then, the same rates as in Corollary 11 hold.
The proofs of the above results appear in Appendix A.3.
6 Suboptimality of Mondrian Trees for Estimating Ridge Func-
tions
The results presented in section 4 show that improved rates of convergence for ridge functions
over the minimax rates for general Lipschitz and C2 functions in Rd can be obtained from oblique
14Mondrian forests with a choice of directional distribution that has support consisting of directions
that approximate directions spanning the relevant feature subspace S. The results also provide
sufficient conditions for how well the sequence of linear transformations A must approximate a
n
projection onto S to achieve minimax optimal convergence rates depending on the dimension s of
S. When the underlying function depends on a relevant feature that is a dense linear combination
of the original set of covariates, restricting the splits to be axis-aligned (i.e. using a weighted
Mondrian process) means that these conditions will not be satisfied, as the transformation matrix
will be diagonal and thus will not approximate well the oblique projection. To make this precise,
the next result shows that oblique splits are not only sufficient but necessary to obtain improved
rates of convergence for general ridge functions over the worst-case minimax rates for functions
on Rd by obtaining a lower bound on the risk of a weighted Mondrian tree estimator when the
underlying function is linear.
Theorem 16. Suppose Y = ⟨a,X⟩ + ε, where a ̸= 0 for each i = 1,...,d, and assume X ∼
i
Uniform([0,1]d). Letfˆ = fˆ beaweightedMondriantreeestimatorwithlifetimeλanddirectional
n n,λ
distribution
d
(cid:88) ω i
ϕ = (δ +δ ),
2
ei −ei
i=1
where {ω }d are weights such that ω > 0 and (cid:80)d ω = 1. Then,
i i=1 i i=1 i
(cid:88)d a2 (cid:18) 2 1 (cid:19) (cid:18) n (cid:19)−1
E[(fˆ (X)−f(X))2] ≥ i 1− − +σ2 +1 .
n 2λ2ω2 λω λ2ω2 2dλdΠd ω
i=1 i i i i=1 i
The proof of this result is in Appendix A.4. Considering the asymptotic behavior of this lower
bound when the weights are allowed to depend on n, note that if (λdΠd ω(n) )/n → 0, then the
i=1 i
variance is on the order of (λdΠd ω(n) )/n. Then, observe that the assumption a ̸= 0 for all
i=1 i i
(n)
i = 1,...,d implies there is no choice of weight sequences ω as n → ∞ that will give an improved
i
rateofconvergenceovertheminimaxrateforgeneralLipschitzfunctionsonRd obtainedin[30]. An
improved rate can be obtained with a sequence of directional distributions with supports consisting
of vectors converging in Euclidean distance to a/∥a∥ by Corollary 9.
2
7 Oblique Mondrian Processes
In this section, we prove that one can generate a partition of the dataset induced by an oblique
Mondrian process with directional distribution (14) by applying a linear transformation to the
data and then running a standard Mondrian process. We also see that under the assumption this
linear transformation is nonsingular, the zero cell of the resulting oblique Mondrian tessellation
has the distribution of a transformation of the zero cell of the tessellation generated by a standard
Mondrian process.
Proposition 17. Let A be a real-valued d × m matrix of rank d ≤ m. Fix λ > 0. Let Y (λ)
A
denote the union of cell boundaries of a STIT tessellation in Rd with directional distribution ϕ as
A
in (14) and lifetime λ. Then, AT (Y (λ)) has the same distribution as the union of cell boundaries
A
of a Mondrian tessellation in Rm with lifetime mλ intersected with the d-dimensional subspace
∥A∥2,1
ran(AT).
15Remark 1. AnobliqueMondrianprocesscorrespondingtoad×mmatrixAhasassociatedzonoid
Π with support function given by
A
m m
1 (cid:88) 1 (cid:88)
h (u) := |⟨u,ATe ⟩| = |⟨Au,e ⟩| = h (Au) = h (u),
ΠA
m
i
m
i ΠM ATΠM
i=1 i=1
for all u ∈ Rd, where Π is the associated zonoid of a standard Mondrian process in Rm. Thus,
M
Π = ATΠ .
A M
Remark 2. The result above highlights an important consideration when generating oblique ran-
domforestsbyfirstapplyingalineartransformationAtothedataandthenrunninganaxis-aligned
random forest. The lifetime of the oblique Mondrian process, which determines the complexity of
the partition, is implicitly scaled by the constant 1 (cid:80)m ∥a ∥ = 1∥A∥ . Thus, to ensure that
m i=1 i 2 m 2,1
the data transformation does not change the complexity of the corresponding tree estimator, we
must not only apply A to the input data but also scale the data by the constant m . This
∥A∥2,1
will cancel out the implicit scaling of the lifetime induced by A and the overall lifetime will be
unchanged from the lifetime of the Mondrian process that is run on the transformed data.
FromProposition17wealsoobtainacouplingofthezerocellofanobliqueMondriantessellation
in Rd and standard Mondrian tessellation in Rm.
Corollary 18. Let A be a real-valued d × m matrix of rank d ≤ m and fix λ > 0. Let P :=
M
(cid:16) (cid:17)
P mλ be a Mondrian tessellation in Rm with lifetime mλ and Z(M) its zero cell. Then,
M ∥A∥2,1 ∥A∥2,1 0
(AT)+(Z(M) ∩ ran(AT)) has the same distribution as the zero cell Z of the oblique Mondrian
0 0
tessellation P (λ) with cell boundaries Y (λ) as in Proposition 17.
A A
8 Conclusion
Inthiswork, wehavestudiedaclassofobliquerandomizeddecisiontreesandforeststhatsplitdata
along features obtained by taking linear combinations of the covariates. Given this set of features,
which can be chosen using domain knowledge or estimated from data, the random partition used
to build the tree estimators is generated using a Mondrian process. This method is equivalent to
partitioningtheoriginaldatawithamoregeneralSTITprocesswecallanobliqueMondrianprocess
where the directional distribution is discrete, allowing us to build on the theoretical framework
developed in [30] at the intersection of random tessellation theory in stochastic geometry and
statistical learning theory.
This study sought to understand the statistical advantages of using these oblique directions in
theinputdomaintomakesplitswhenbuildingarandomforestestimator. Ouranalysismakesclear
and rigorous that one such advantage of these random forest variants is their ability to capture low
dimensional structure in the regression function described by the class of ridge functions. These
are linear dimension reduction models for which the output depends on a general low-dimensional
relevant feature subspace of the input domain. We obtained convergence rates (see Corollaries 9
and 11) for general oblique Mondrian forests that depend on a parameter controlling the error
between the features and associated weights used to make splits and the true relevant features for
the regression model. We also illuminated how quickly this error must decay with the amount of
data to achieve minimax optimal rates for this model class. Further, we showed that without the
abilitytodividethedataalonglinearcombinationsofcovariatesthatapproximatevectorsspanning
this subspace, the geometry of axis-aligned random partitions prevents the associated randomized
16decision trees from adapting to general ridge functions (see Theorem 16). In particular, weighted
Mondrian trees cannot achieve the improved rates of convergence that oblique Mondrian trees can
for general ridge functions no matter how the distribution over the covariates for making splits is
asymptotically reweighted.
Not considered in this study is an algorithm for how to choose the features, or equivalently, the
linear transformation A, such that these theoretical rates are achieved. To obtain improved rates
over the minimax rates with respect to the dimension of the ambient input space, this relevant
feature subspace must be consistently estimated. Several such methods exist in the literature to
do so by estimating a matrix that approximates a projection onto this subspace [20, 12, 40, 39, 38]
and a subject of future work is the study of complete algorithms for high dimensional regression
that are both computationally efficient and provably achieve these improved rates of convergence.
Another future direction is to study the statistical advantage of randomized decision tree and
forest variants that use both oblique splits and optimization procedures for choosing the location
of the splits. Mondrian forests choose the location uniformly at random after having chosen the
featurealongwhichtosplit. Theadvantageofchoosingthislocationinadata-drivenwayintuitively
would be to capture local variation and feature importance, but this is not captured by the class of
ridge functions studied here, which describes a low-dimensional subset of globally relevant features.
Recent work [21] has argued with numerical studies that criteria such as CART are more powerful
in capturing this local or nonlinear low-dimensional structure, but more theoretical justification
and interpretation is needed.
9 Selected Proofs
We collect here the proofs for some of the main results in this paper including Theorem 6, Theorem
8, and Corollary 12. The proofs of the remaining results appear in the Appendix.
9.1 Proof of Theorem 6
Let fˆ denote a random tree estimator of f obtained from a STIT tessellation P(λ) of the input
n,λ
space with associated zonoid Π and lifetime parameter λ. The proof of Theorem 6 begins by
considering the following bias-variance decomposition of the risk of a tree estimator presented in
[2]. First, let Zλ denote the cell of P(λ) that contains the vector x ∈ Rd, and define
x
f¯(x) := E [f(X)|X ∈ Zλ], x ∈ W. (21)
λ X x
Conditioned on P(λ), this is the orthogonal projection of f ∈ L2(W,µ) onto the subspace of
functions that are constant within the cells of P(λ)∩W.
Then, conditioning on the data D , fˆ is in this subspace of piecewise constant functions, and
n n,λ
hence E [(f(X)−f¯(X))fˆ (X)] = 0. Thus,
X λ n,λ
E [(f(X)−fˆ (X))2] = E [(f(X)−f¯(X)+f¯(X)−fˆ (X))2]
X λ,n X λ λ n,λ
= E [(f(X)−f¯(X))2]+E [(f¯(X)−fˆ (X))2].
X λ X λ n,λ
Taking the expectation with respect to P(λ) and D , we obtain the bias-variance decomposition
n
E[(f(X)−fˆ (X))2] = E[(f (X)−f¯(X))2]+E[(f¯(X)−fˆ (X))2]. (22)
n,λ λ λ n,λ
The first term on the right-hand side above is called the bias, or approximation error, of the
estimator and the second term is the variance, or estimation error. The bound on the risk then
depends on the following two lemmas, which bound each of these expressions.
17Lemma 19. Let f¯(x) be defined as in (21). Under the assumptions on f in Theorem 6, for any
λ
fixed x ∈ supp(µ),
L2
E[(f(x)−f¯(x))2] ≤ E[D(P Z )2β].
λ λ2β S 0
Proof. By the assumption on f,
(cid:90)
1
|f(x)−f¯(x)| = |f(x)−f(z)|1 µ(dz)
λ µ(Z xλ)
Rd
{z∈Z xλ}
(cid:90)
L
≤ ∥P (x−z)∥β1 µ(dz)
µ(Z xλ)
Rd
S {z∈Z xλ}
LD(P Zλ)β (cid:90)
≤ S x 1 µ(dz) = LD(P Zλ)β.
µ(Z xλ)
Rd
{z∈Z xλ} S x
By stationarity and (1), for any fixed x ∈ Rd,
Zλ ( =d) 1 Z +x.
x λ 0
Thus, taking the expectation with respect to the random tessellation P(λ) gives
L2
E[(f(x)−f¯(x))2] ≤ E[D(P Z )2β].
λ λ2β S 0
Lemma 20. Suppose supp(µ) ⊆ Bd. Then,
(cid:104) (cid:105) 5∥f∥2 +2σ2 (cid:32) (cid:88)d (cid:88)s (cid:33)
E (f¯(X)−fˆ (X))2 ≤ ∞ c λkD(P Π)k−s+ c λk ,
λ λ,n
n
d,k S⊥ d,k
k=s+1 k=0
where c := κ kπk/2dk/2 .
d,k k!
Proof. LetN (K)bethenumberofcellsofP(λ)thathaveanon-emptyintersectionwithacompact
λ
subset K ⊂ Rd. By Lemma 15 in [30],
(cid:104) (cid:105) 5∥f∥2 +2σ2
E (f¯(X)−fˆ (X))2 ≤ ∞ E[N (supp(µ))]. (23)
λ λ,n λ
n
(d)
Recall that for a convex body K, V (Π) = k V(K[k],Bd[d − k]) [34, (14.18)]. Then, by the
k κ
d−k
assumption supp(µ) ⊆ Bd and Lemma 4 in [30],
d (cid:18) (cid:19)
(cid:88) d
E[N (supp(µ))] ≤ E[N (Bd)] = vol (Π) λkE[V(Bd[k],Z[d−k])]
λ λ d
k
k=0
d
(cid:88)
= vol (Π ) λkκ E[V (Z)].
d n k d−k
k=0
By (10.3) and Theorem 10.3.3 in [34], EV (Z) = V k(Π) . Thus,
d−k vol (Π)
d
d
(cid:88)
E[N (supp(µ))] ≤ λkκ V (Π).
λ k k
k=0
18Note that Π ⊆ P Π + P Π for any linear subspace S. By monotonicity and multilinearity of
S S⊥
mixed volumes with respect to the Minkowski sum, we have for each k ∈ {1,...,d},
(cid:16) (cid:17)
V(Π[k],Bd[d−k]) ≤ V (P Π+P Π)[k],Bd[d−k]
S S⊥
k (cid:18) (cid:19)
(cid:88) k
= V(P Π[k−j],P Π[j],Bd[d−k]).
j
S S⊥
j=0
We now observe from (4) that h (u) ≤ 1 for all u ∈ Sd−1 which implies 2Π ⊆ Bd. Thus we
Π 2
haveP Π ⊆ 1Bs. Alsoobservethat2P Π ⊆ D(P Π)Bd−s. Themonotonicitypropertyofmixed
S 2 S⊥ S⊥
volumes and (7) then imply
V(P Π[j],P Π[k−j],B [d−k])
S S⊥ d
≤ 2−kD(P Π)k−jV(Bs[j],Bd−s[k−j],Bd[d−k])1 .
S⊥ {s−(d−k)≤j≤s}
Next observe that D(P⊥Π) ≤ 1 for k ≤ s and for k ≥ s+1, D(P⊥Π)k−j ≤ D(P⊥Π)k−s when
S 2 S S
j ≤ s. This gives the upper bound
E[N (supp(µ))] ≤
(cid:88)d λkκ k(cid:18) d(cid:19)
V(Π[k],B [d−k])
λ d
κ k
d−k
k=0
≤
(cid:88)d λkκ
k
(cid:18) d(cid:19) (cid:88)k (cid:18) k(cid:19)
D(P Π)k−jV(Bs[j],Bd−s[k−j],Bd[d−k])1
2kκ k j S⊥ {s−(d−k)≤j≤s}
d−k
k=0 j=0
d (cid:18) (cid:19) s (cid:18) (cid:19)
≤ (cid:88) λkD(P Π)k−s κ k d (cid:88) k V(Bs[j],Bd−s[k−j],Bd[d−k])
S⊥ 2kκ k j
d−k
k=s+1 j=0
s (cid:18) (cid:19) k (cid:18) (cid:19)
+(cid:88) λk κ k d (cid:88) k V(Bs[j],Bd−s[k−j],Bd[d−k]).
2kκ k j
d−k
k=0 j=0
Now we see that
(cid:18) (cid:19) k (cid:18) (cid:19) (cid:18) (cid:19)
1 d (cid:88) k 1 d
V(Bs[j],Bd−s[k−j],Bd[d−k]) = V((Bs+Bd−s)[k],Bd[d−k])
2kκ k j 2kκ k
d−k d−k
j=0
1 1
= V (Bs+Bd−s) ≤ V (Bd)k,
2k k k! 1
where the last inequality follows from the fact that Bs + Bd−s ⊆ 2Bd and Theorem 2 in [22],
which shows that all intrinsic volumes are controlled by the first one. Then, by (7) and Gautschi’s
inequality for the Gamma function,
dκ √ Γ(d + 1) √
V (Bd) = d = d π 2 2 ≤ πd.
1 κ d−1 Γ(d +1)
2
Thus,
E[N (supp(µ))] ≤
(cid:88)d
λkD(P
Π)k−sκ kπk/2dk/2 +(cid:88)s λkκ kπk/2dk/2
.
λ S⊥
k! k!
k=s+1 k=0
Plugging the above upper bound into the right-hand side of (23) gives the result.
19Proof of Theorem 6. Combining the bias-variance decomposition (22) with the upper bounds in
Lemma 19 and Lemma 20 gives
E[(f(X)−fˆ (X))2] = E[(f (X)−f¯(X))2]+E[(f¯(X)−fˆ (X))2]
λ,n λ λ λ,n
L2 5∥f∥2 +2σ2 (cid:32) (cid:88)d (cid:88)s (cid:33)
≤ E[D(P Z )2]+ ∞ c λkD(P Π)k−s+ c λk ,
λ2 S 0 n d,k S⊥ d,k
k=s+1 k=0
where c := κ kπk/2dk/2 . The final result follows from the observation that the risk of a STIT forest
d,k k!
estimator for any number of trees M is bounded above by the risk of a single STIT tree estimator
by Jensen’s inequality.
9.2 Proof of Theorem 8 and Corollary 9
We first need the following lemma on the diameter of the zero cell of the random tessellation
generated by an oblique Mondrian process.
Lemma 21. Suppose that Z is the zero cell of a STIT tessellation with unit lifetime and directional
0
distribution ϕ as in (14) for A ∈ Rd×m with rank d ≤ m and ∥A∥ = 1. Then, for all r ≥ 0 and
A 2,1
k > 0,
E(cid:104) D(P SZ 0)k1 {D(PSZ0)≥r}(cid:105) ≤ dkσmk (PΓ(2 Am )k+ Γ(k 2)
m)
2m (cid:88)+k−1 n1
!
(cid:18) rdσ s m(P SA)(cid:19)n e−rdσs m(PSA) ,
s S
n=0
where σ is the s-th largest singular value. In particular, for all k > 0,
s
mkΓ(2m+k)
E[D(P Z )k] ≤ .
S 0 2kdkσ (P A)kΓ(2m)
s S
Proof. The distribution of the zero cell Z(M) for the Mondrian tessellation in Rm with lifetime d is
0
given by
(M) (d) m (cid:16) (1) (2) (1) (2) (cid:17)
Z = [−T e ,T e ]+···+[−T e ,T e ] ,
0 d 1 1 1 1 d d d d
(j)
where {T } for i = 1,...,d and j = 1,2 are independent and identically distributed exponential
i
random variables with unit parameter. By Corollary 18, the zero cell Z has the same distribution
0
as (A+)T(Z(M) ∩ran(AT)). Then, the support function of Z satisfies the upper bound
0 0
h (u) = h (u) = h (A+u) ≤ h (A+u)
Z0 (A+)T(Z(M)∩ran(AT)) Z(M)∩ran(AT) Z(M)
0 0 0
m
=
m (cid:88) max{⟨A+u,−T(1)
e
⟩,⟨A+u,T(2)
e ⟩}
d i i i i
i=1
m
= m (cid:88) max{−T(1) ⟨A+u,e ⟩,T(2) ⟨A+u,e ⟩},
d i i i i
i=1
and the width function of Z satisfies
0
w (u) := h (u)+h (−u)
Z0 Z0 Z0
m
≤
m (cid:88)(cid:16)
max{−T(1) ⟨A+u,e ⟩,T(2) ⟨A+u,e ⟩}+max{T(1) ⟨A+u,e ⟩,−T(2) ⟨A+u,e
⟩}(cid:17)
d i i i i i i i i
i=1
m
=
m (cid:88)(cid:16)
T(1)
+T(2)(cid:17)
|⟨A+u,e ⟩|. (24)
d i i i
i=1
20Then, recalling that w (u) = w (ATu) for a convex body K and linear image A, the diameter
AK K
of P Z has the upper bound
S 0
D(P Z ) = sup w (u) = sup w (P u)
S 0 PSZ0 Z0 S
u∈Sd−1 u∈Sd−1
m
≤ sup
m (cid:88)(cid:16)
T(1)
+T(2)(cid:17)
|⟨A+P u,e ⟩|
d i i S i
u∈Sd−1
i=1
m m
≤
m (cid:88)(cid:16)
T(1)
+T(2)(cid:17)
∥P (A+)Te ∥ ≤
m
∥(P
A)+∥(cid:88)(cid:16)
T(1)
+T(2)(cid:17)
d i i S i 2 d S i i
i=1 i=1
m
m (cid:88)(cid:16)
(1)
(2)(cid:17)
= T +T , (25)
dσ (P A) i i
s S
i=1
where we have used the fact that P (A+)T = (A+P )T = ((P A)+)T. Thus, the diameter of
S S S
P Z is controlled by the sum of independent exponential random variables, which is an Erlang
S 0
distributed random variable
m
T(m) :=
(cid:88)(cid:16)
T(1)
+T(2)(cid:17)
∼ Erlang(2m,1).
i i
i=1
Thus, for r > 0,
(cid:104) (cid:105) mk (cid:20) (cid:21)
E D(P Z )k1 ≤ E (T(m))k1
S 0 {D(PSZ0)≥r} dkσ (P A)k {T(m)≥rdσs(PSA)}
s S m
=
mkΓ(2m+k) 2m (cid:88)+k−1 1 (cid:18) rdσ s(P SA)(cid:19)n e−rdσs m(PSA)
,
dkσ (P A)kΓ(2m) n! m
s S
n=0
and moments of the diameter of P Z satisfy the upper bound
S 0
mkE[(T(m))k] mkΓ(2m+k)
E[D(P Z )k] ≤ = .
S 0 dkσ (P A)k dkσ (P A)kΓ(2m)
s S s S
Proof of Theorem 8. First recall the following bias-variance decomposition (22) for a STIT tessel-
lation tree used in the proof of Theorem 6. Now let fˆ be an oblique Mondrian forest estimator as
n,λ
in the statement of Theorem 8 for a matrix A ∈ Rd×m with rank d ≤ m and such that ∥A∥ = 1.
2,1
To bound the bias term, Lemma 19 and Lemma 21 imply that for an absolute constant c > 0,
E(cid:104) (cid:0) f(X)−f¯(X)(cid:1)2(cid:105)
≤
L2E[D(P SZ 0)2β]
≤
L2m2βΓ(2m+2β)
≤
9L2m2β
,
λ λ2β d2βλ2βσ (P A)2βΓ(2m) d2βλ2βσ (P A)2β
s S s S
where in the last inequality we used Gautschi’s inequality to obtain the bound
Γ(2m+2β) ≤ (2m+1)2β−1(2m)Γ(2m) ≤ 9m2βΓ(2m).
To bound the variance term, we first observe that inserting the directional distribution (14) into
(4) implies that the associated zonoid Π corresponding to the oblique Mondrian process used to
21generate fˆ satisfies
n,λ
d
(cid:88)
D(P Π) ≤ sup (h (u)+h (−u)) = sup |⟨a ,u⟩|
S⊥ Π Π i
u∈Sd−1∩S⊥ u∈Sd−1∩S⊥
i=1
d d
(cid:88) (cid:88)
= sup |⟨P a ,u⟩| ≤ ∥P a ∥ = ∥P A∥ . (26)
S⊥ i S⊥ i 2 S⊥ 2,1
u∈Sd−1∩S⊥
i=1 i=1
Thus, by Lemma 20,
(cid:104) (cid:105) 5∥f∥2 +2σ2 (cid:32) (cid:88)d (cid:88)s (cid:33)
E (f¯(X)−fˆ (X))2 ≤ ∞ c λk∥P A∥k−s+ c λk ,
λ λ,n n d,k S⊥ 2,1 d,k
k=s+1 k=0
where c is as in Lemma 20. Combining these bounds with (22), and again observing that by
d,k
Jensen’s inequality the risk of a STIT forest estimator for any number of trees M is bounded above
by the risk of a single STIT tree, gives the final result.
Proof of Corollary 9. UndertheassumptionsoftheCorollary,forthesequenceofobliqueMondrian
forest estimators fˆ defined there, Theorem 8 implies
n
(cid:104) (cid:105) 9L2m2β 5∥f∥2 +2σ2 (cid:32) (cid:88)d (cid:33)
E (f(X)−fˆ (X))2 ≤ + ∞ c λkεk−s+O(λs−1) .
n c2d2λ2β n d,k n n n
n k=s
2 1
−(d−s)
Minimizing the upper bound with respect to λ
n
gives that for λ
n
≍ Ld+2βnd+2βε
n
d+2β,
E(cid:104) (f(X)−fˆ n(X))2(cid:105) ≲ max(cid:26) Ld+2d 2βn− d+2β 2βε n2β d( +d 2− βs) ,Ls+2s 2βn− s+2β 2β(cid:27) .
Thefinalclaimfollowsfromtheobservationthatbylettingε n ≲ L− s+2 2βn− s+1 2β andλ n ≍ Ls+2 2βns+1 2β,
the upper bound above satisfies
E(cid:104) (f(X)−fˆ n(X))2(cid:105) ≲ Ls+2s 2βn− s+2β 2β.
References
[1] Yali Amit and Donald Geman. Shape Quantization and Recognition with Randomized Trees.
Neural Computation, 9(7):1545–1588, October 1997.
[2] Sylvain Arlot and Robin Genuer. Analysis of purely random forests bias. Preprint
arXiv:1407.3939, 2014.
[3] Francis Bach. Breaking the curse of dimensionality with convex neural networks. Journal of
Machine Learning Research, 18(19):1–53, 2017.
[4] Gerard Biau. Analysis of a random forests model. Journal of Machine Learning Research,
13:1063–1095, 2012.
22[5] Rico Blaser and Piotr Fryzlewicz. Random rotation ensembles. Journal of Machine Learning
Research, 17(1):126–151, 2016.
[6] K´aroly J. B¨or¨oczky and Daniel Hug. Reverse Alexandrov–Fenchel inequalities for zonoids.
Communications in Contemporary Mathematics, 24(8), 2022.
[7] Leo Breiman. Random forests. Machine learning, 45(1):5–32, 2001.
[8] Matias D. Cattaneo, Rajita Chandak, and Jason M. Klusowski. Convergence rates of oblique
regression trees for flexible function libraries. The Annals of Statistics, 52(2):466 – 490, 2024.
[9] Matias D. Cattaneo, Jason M. Klusowski, and William G. Underwood. Inference with Mon-
drian random forests. Preprint arXiv:2310.09702, 2023.
[10] Xi Chen and Hemant Ishwaran. Random forests for genomic data analysis. Genomics,
99(6):323–329, 2012.
[11] Chien-Ming Chi, Patrick Vossler, Yingying Fan, and Jinchi Lv. Asymptotic properties of
high-dimensional random forests. The Annals of Statistics, 50(6):3415 – 3438, 2022.
[12] R Dennis Cook. Save: a method for dimension reduction and graphics in regression. Commu-
nications in statistics-Theory and methods, 29(9-10):2109–2121, 2000.
[13] Duroux,RoxaneandScornet,Erwan. Impactofsubsamplingandtreedepthonrandomforests.
ESAIM: PS, 22:96–128, 2018.
[14] Xuhui Fan, Bin Li, and Scott Sisson. The binary space partitioning-tree process. In Amos
StorkeyandFernandoPerez-Cruz, editors, Proceedings of the Twenty-First International Con-
ference on Artificial Intelligence and Statistics, volume 84 of Proceedings of Machine Learning
Research, pages 1859–1867. PMLR, 09–11 Apr 2018.
[15] Manuel Fern´andez-Delgado, Eva Cernadas, Sen´en Barro, and Dinani Amorim. Do we need
hundredsofclassifierstosolverealworldclassificationproblems? Journal of Machine Learning
Research, 15(1):3133–3181, 2014.
[16] Shufei Ge, Shijia Wang, Yee Whye Teh, Liangliang Wang, and Lloyd Elliott. Random tessel-
lation forests. In Advances in Neural Information Processing Systems 32, pages 9571–9581.
2019.
[17] Tin Kam Ho. The random subspace method for constructing decision forests. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence, 20(8):832–844, August 1998. Conference
Name: IEEE Transactions on Pattern Analysis and Machine Intelligence.
[18] Jason Klusowski. Sharp analysis of a simple model for random forests. In Arindam Banerjee
and Kenji Fukumizu, editors, Proceedings of The 24th International Conference on Artificial
Intelligence and Statistics, volume 130 of Proceedings of Machine Learning Research, pages
757–765. PMLR, 13–15 Apr 2021.
[19] Jason M. Klusowski and Peter M. Tian. Large scale prediction with decision trees. Journal of
the American Statistical Association, 119(545):525–537, 2024.
[20] Ker-Chau Li. Sliced inverse regression for dimension reduction. Journal of the American
Statistical Association, 86(414):316–327, 1991.
23[21] Joshua Daniel Loyal, Ruoqing Zhu, Yifan Cui, and Xin Zhang. Dimension reduction forests:
Local variable importance using structured random forests. Journal of Computational and
Graphical Statistics, 31(4):1104–1113, 2022.
[22] Peter McMullen. Inequalities between intrinsic volumes. Monatshefte fu¨r Mathematik,
111(1):47–54, 1991.
[23] Joseph Mecke, Werner Nagel, and Viola Weiss. The iteration of random tessellations and
a construction of a homogeneous process of cell divisions. Advances in Applied Probability,
40(1):49–59, March 2008.
[24] Bjoern H. Menze, B. Michael Kelm, Daniel N. Splitthoff, Ullrich Koethe, and Fred A. Ham-
precht. On oblique random forests. In Dimitrios Gunopulos, Thomas Hofmann, Donato
Malerba, and Michalis Vazirgiannis, editors, Machine Learning and Knowledge Discovery in
Databases, pages 453–469, Berlin, Heidelberg, 2011. Springer Berlin Heidelberg.
[25] Ilya Molchanov. Theory of Random Sets, volume 87. Springer, 2017.
[26] JaouadMourtada, St´ephaneGa¨ıffas, andErwanScornet. MinimaxoptimalratesforMondrian
trees and forests. Annals of Statistics, 28(4):2253–2276, 2020.
[27] WernerNagelandViolaWeiss. Limitsofsequencesofstationaryplanartessellations. Advances
in Applied Probability, 35:123–138, 2003.
[28] Werner Nagel and Viola Weiss. Crack STIT tessellations: Characterization of stationary
random tessellations stable with respect to iteration. Advances in Applied Probability, 37:859–
883, 2005.
[29] Eliza O’Reilly and Ngoc Mai Tran. Stochastic geometry to generalize the Mondrian process.
SIAM Journal on Mathematics of Data Science, 4(2):531–552, 2022.
[30] Eliza O’Reilly and Ngoc Mai Tran. Minimax rates for high-dimensional random tessellation
forests. Journal of Machine Learning Research, 25:1–32, 2024.
[31] Tom Rainforth and Frank Wood. Canonical correlation forests. Preprint arXiv:1507.05444,
2015.
[32] Daniel M Roy and Yee Whye Teh. The Mondrian process. In Proceedings of the 21st Interna-
tional Conference on Neural Information Processing Systems, pages 1377–1384, 2008.
[33] Rolf Schneider and Wolfgang Weil. Zonoids and Related Topics, pages 296–317. Birkh¨auser
Basel, Basel, 1983.
[34] Rolf Schneider and Wolfgang Weil. Stochastic and Integral Geometry. Probability and Its
Applications. Springer-Verlag, Berlin, 2008.
[35] Erwan Scornet, G´erard Biau, and Jean-Philippe Vert. Consistency of random forests. The
Annals of Statistics, 43(4):1716 – 1741, 2015.
[36] Vasilis Syrgkanis and Manolis Zampetakis. Estimation and inference with trees and forests in
highdimensions.InJacobAbernethyandShivaniAgarwal,editors,ProceedingsofThirtyThird
Conference on Learning Theory, volume 125 of Proceedings of Machine Learning Research,
pages 3453–3454. PMLR, 09–12 Jul 2020.
24[37] Tyler M. Tomita, James Browne, Cencheng Shen, Jaewon Chung, Jesse L. Patsolic, Benjamin
Falk, Carey E. Priebe, Jason Yim, Randal Burns, Mauro Maggioni, and Joshua T. Vogelstein.
Sparseprojectionobliquerandomerforests. Journal of Machine Learning Research, 21(104):1–
39, 2020.
[38] Shubhendu Trivedi, Jialei Wang, Samory Kpotufe, and Gregory Shakhnarovich. A consistent
estimator of the expected gradient outerproduct. In Nevin L. Zhang and Jin Tian, editors,
Uncertainty in Artificial Intelligence - Proceedings of the 30th Conference, UAI 2014, pages
819–828. AUAI Press, 2014.
[39] Qiang Wu, Justin Guinney, Mauro Maggioni, and Sayan Mukherjee. Learning gradients:
Predictivemodelsthatinfergeometryandstatisticaldependence. JournalofMachineLearning
Research, 11(75):2175–2198, 2010.
[40] Y. Xia, H. Tong, W. Li, and L.-X. Zhu. An adaptive estimation of dimension reduction space.
Journal of the Royal Statistical Society: Series B (Statistical Methodology), 64(3):363–410,
2002.
[41] Haoran Zhan, Yu Liu, and Yingcun Xia. Consistency of oblique decision tree and its boosting
and random forest. Preprint arXiv:2211.12653v3, 2024.
A Proofs
Thisappendixcontainstheremainingproofsoftheresultsinthemaintextthatwerenotcontained
in section 9 of the main text.
A.1 Proof of Theorem 7
We first need the following two lemmas before proceeding to the proof of Theorem 7.
Lemma 22. For λ > 0 and an s-dimensional linear subspace S of Rd, define the probability density
(cid:20) 1 (cid:21)
F (y) := E {y∈PSZ xλ} , y ∈ S.
λ,S vol (P Zλ)
s S x
Then,
(cid:90)
(y−P x)F (y)dy = 0.
S λ,S
S
Proof. By stationary of P(λ),
(cid:90) (cid:90) (cid:20) 1 (cid:21)
(y−P x)F (y)dy = (y−P x)E {y∈PSZ xλ} dy
S λ,S S vol (P Zλ)
S S s S x
(cid:90) (cid:20)1 (cid:21)
= (y−P x)E {y−PSx∈PS(Z xλ−x)} dy
S vol (P (Zλ−x))
S s S x
(cid:90) (cid:20) 1 (cid:21)
= ωE {ω∈PSZ 0λ} dy.
vol (P Zλ)
S s S 0
The conclusion will follow from the fact that the distribution of Zλ is the same as the distribution
0
ofZλ. Indeed, thedistributionofarandomconvexpolytopeisuniquelydefinedbythecontainment
0
25function C := P(K ⊂ ·) (Theorem 1.8.9 in [25]). Then, since mixed volumes are invariant under
K
reflections, we have that for all compact K ⊂ Rd containing the origin,
P(K ⊂ −Zλ) = P(−K ⊂ Zλ) = e−2dV1(−K,B λ) = e−2dV1(K,B λ) = P(K ⊂ Zλ),
0 0 0
where B is the the Blaschke body of P(λ) (see [34, p. 162]). We thus have that
λ
(cid:20) 1 (cid:21) (cid:20)1 (cid:21)
E {ω∈PSZ 0λ} = E {−ω∈PSZ 0λ} ,
vol (P Zλ) vol (P Zλ)
s S 0 s S 0
which implies the integrand above is odd and the integral is zero.
Lemma 23. For a subset K ⊂ Rd, let Kc denote the complement Rd\K, and for a linear subspace
S in Rd let K := P K denote the orthogonal projection of K onto S. Under the assumptions on
S S
the distribution µ of X as in Theorem 7,
s−1
E [vol (P Z ∩λ(Kc −P X))] ≤ p vol (P Z
)(cid:88) κ s−jV j(K S)
D(P Z )s−j.
X s S 0 S S 1 s S 0 λs−j S 0
j=0
Proof. We first see that
(cid:90) (cid:90)
E [vol (P Z ∩λ(Kc −P X))] = p(x) 1 dydx
X s S 0 S S {y∈PSZ0∩λ(K Sc−PSx)}
K S
(cid:90) (cid:90)
≤ p
1
PSZ0
K1
{PSx∈K Sc−
λy}dxdy
(cid:90) (cid:16) y(cid:17)
= p vol K ∩Kc − dy
1 s S S λ
PSZ0
(cid:90) (cid:90) (cid:16) y(cid:17)
= p vol (K )dy−p vol K ∩K − dy
1 s S 1 s S S
λ
PSZ0 PSZ0
(cid:90) (cid:16) y(cid:17)
= p vol K ∪K − dy−p vol (P Z )vol (K ),
1 s S S 1 s S 0 s S
λ
PSZ0
where we have used that vol (K ∩K −y/λ) = 2vol (K )−vol (K ∪K −y/λ). We now observe
s S S s S s S S
that the union K ∪K − y is a subset of the Minkowski sum K + ∥y∥ Bs. By Steiner’s formula
S S λ S λ
[34, Equation (14.5)],
(cid:18) (cid:19) s
(cid:16) y(cid:17) ∥y∥ (cid:88)
vol K ∪K − ≤ vol K + Bs = ∥y∥s−jκ V (K )
s S S s S s−j j S
λ λ
j=0
(cid:88)s−1(cid:18) ∥y∥(cid:19)s−j
= vol (K )+ κ V (K ).
s S s−j j S
λ
j=0
Then,
(cid:88)s−1 (cid:90) (cid:18) ∥y∥(cid:19)s−j
E [vol (P Z ∩λ(Kc −P X))] ≤ p κ V (K ) dy
X s S 0 S S 1 s−j j S λ
j=0
PSZ0
s−1
≤ p vol (P Z
)(cid:88) κ s−jV j(K S)
D(P Z )s−j.
1 s S 0 λs−j S 0
j=0
26Proof of Theorem 7. Recallthedefinition(13)ofarandomtessellationforestestimatorfˆ built
n,λ,M
from M random tessellation trees of lifetime λ > 0. Define for each m and x ∈ Rd,
f¯(m) (x) := E[f(X)|X ∈ Zλ,(m)],
λ x
where Zλ,(m) is the cell of the m-th random tessellation P (λ) containing x ∈ Rd and define the
x m
average f¯ (x) := 1 (cid:80)M f¯(m) (x). Also define
λ,M M m=1 λ
f˜(x) := E [f¯(m) (x)].
λ P λ
As noted in [26], the bias-variance decomposition for the risk of a tree estimator can be extended
to the random forest estimator as follows [2, Equation (1)]:
E[(fˆ (X)−f(X))2] = E[(f(X)−f¯ (X))2]+E[(f¯ (X)−fˆ (X))2]. (27)
λ,n,M λ,M λ,M λ,n,M
Variance term: For the variance term in (27), Jensen’s inequality implies
E[(f¯ (x)−fˆ (x))2] ≤ E[(f¯(1) (x)−fˆ (x))2].
λ,M λ,n,M λ λ,n,1
We then use Lemma 20 to obtain the upper bound
E[(f¯(1) (X)−fˆ (X))2] ≤
5∥f∥2 ∞+2σ2 (cid:32) (cid:88)d
c λkD(P
Π)k−s+(cid:88)s
c
λk(cid:33)
,
λ λ,n,1 n d,k S⊥ d,k
k=s+1 k=0
and the conditional variance satisfies
E[(f¯(1) (X)−fˆ (X))2|X ∈ K ] ≤ P(X ∈ K )−1E[(f¯(1) (X)−fˆ (X))2]
λ λ,n,1 δ δ λ λ,n,1
(5∥f∥2 +2σ2) (cid:32) (cid:88)d (cid:88)s−1 (cid:33)
≤ ∞ c λkD(P Π)k−s+ c λk . (28)
nP(X ∈ K ) d,k S⊥ d,k
δ
k=s k=0
Bias term: For the bias term in (27), Proposition 1 of [2] implies that for fixed x ∈ Rd,
Var
(f¯(1)
(x))
E [(f(x)−f¯ (x))2] = E [(f(x)−f˜(x))2]+ P λ . (29)
P λ,M P λ
M
We then have the following upper bound on the variance of f¯(1) : for x ∈ Rd,
λ
(cid:104) (cid:105) L2
Var (f¯(1) (x)) ≤ E (f¯(1) (x)−f(x))2 ≤ E[D(P Z )2],
P λ P λ λ2 S 0
where the last inequality follows from Lemma 19 and stationarity. It thus remains to control the
remaining term E [(f(x)−f˜(x))2]. By Taylor’s theorem, for f ∈ C1,β(L) with β ∈ (0,1],
P λ
|f(z)−f(x)−∇f(x)T(z−x)| = |g(P z)−g(P x)−∇g(P x)TP (z−x)|
S S S S
(cid:12)(cid:90) 1 (cid:12)
= (cid:12) (cid:12) [∇g(P Sx+tP S(z−x))−∇g(P Sx)]TP S(z−x)dt(cid:12) (cid:12)
(cid:12) (cid:12)
0
(cid:90) 1
≤ L(t∥P (z−x)∥)β∥P (z−x)∥dt ≤ L∥P (z−x)∥1+β.
S S S
0
27Then, for x ∈ Rd,
(cid:12) (cid:34) (cid:35)(cid:12)
(cid:12) 1 (cid:90) (cid:12)
|f˜(x)−f(x)| = (cid:12)E (f(z)−f(x))µ(dz) (cid:12)
λ (cid:12) µ(Zλ) (cid:12)
(cid:12) x Zλ (cid:12)
x
(cid:12) (cid:34) (cid:35)(cid:12) (cid:34) (cid:35)
≤ (cid:12) (cid:12)E 1 (cid:90) ∇f(x)T(z−x)µ(dz) (cid:12) (cid:12)+E 1 (cid:90) (cid:12) (cid:12)f(z)−f(x)−∇f(x)T(z−x)(cid:12) (cid:12)µ(dz)
(cid:12) µ(Zλ) (cid:12) µ(Zλ)
(cid:12) x Zλ (cid:12) x Zλ
x x
≤
(cid:12)
(cid:12) (cid:12) (cid:12)∇f(x)T
(cid:90) Rd(z−x)E(cid:20)1
µ{z (∈ ZZ xλxλ
)}(cid:21) µ(dz)(cid:12)
(cid:12) (cid:12)
(cid:12)+E(cid:20)
µ(L
Z xλ)
(cid:90)
Rd∥P S(z−x)∥1+β1
{z∈Z
xλ}µ(dz)(cid:21)
≤ (cid:12) (cid:12) (cid:12) (cid:12)∇g(P Sx)T (cid:90) RdP S(z−x)E(cid:20)1 µ{z (∈ ZZ xλxλ )}(cid:21) µ(dz)(cid:12) (cid:12) (cid:12) (cid:12)+E(cid:20) LD( µP (S ZZ xλxλ ))1+β (cid:90) Rd1
{z∈Z
xλ}µ(dz)(cid:21)
≤ ∥∇g(P
Sx)∥(cid:13) (cid:13)
(cid:13)
(cid:13)(cid:90)
RdP
S(z−x)E(cid:20)1 µ{z (∈ ZZ xλxλ )}(cid:21) µ(dz)(cid:13) (cid:13)
(cid:13)
(cid:13)+LE(cid:104)
D(P SZ
xλ)1+β(cid:105)
≤
L(cid:13)
(cid:13) (cid:13)
(cid:13)(cid:90)
RdP
S(z−x)E(cid:20)1
µ{z (∈ ZZ xλxλ
)}(cid:21) µ(dz)(cid:13)
(cid:13) (cid:13) (cid:13)+ λ1L +βE[D(P SZ 0)1+β].
By the assumptions, the density p of µ has a finite Lipschitz constant C > 0 on its compact
p
and convex d-dimensional support K := supp(µ) and we can define p := min p(x) > 0 and
0 x∈K
p := max p(x) < ∞. Alsonotethattheintegrandaboveiszerowhenz,y ∈/ K. Inthefollowing
1 x∈K
we denote by Kc := Rd\K the complement of K. Then, for the first term above,
(cid:13)(cid:90) (cid:20)1 (cid:21) (cid:13) (cid:13)(cid:90) (cid:20)p(z)1 (cid:21) (cid:13)
(cid:13) (cid:13)
(cid:13)
RdP S(z−x)E µ{z (∈ ZZ xλxλ )} µ(dz)(cid:13) (cid:13)
(cid:13)
= (cid:13) (cid:13)
(cid:13)
RdP S(z−x)E µ{ (z Z∈ xλZ )xλ∩K} dz(cid:13) (cid:13) (cid:13).
Recall that we assume K = K +K , where K ⊂ S and K ⊂ S⊥. We will first compare the
S S⊥ S S⊥
(cid:20) (cid:21)
p(z)1
density F (z) := E {z∈Zxλ∩K} with the density
λ,p µ(Zλ)
x
(cid:34) (cid:35)
p(z)1
F˜ (z) := E {z∈PSZ xλ+P S⊥Z xλ} .
λ,p,S P(X ∈ P Zλ+P Zλ)
S x S⊥ x
By the triangle inequality,
(cid:13)(cid:90) (cid:13)
(cid:13) (cid:13)
(cid:13) P S(z−x)F λ,p(z)µ(dz)(cid:13)
(cid:13) Rd (cid:13)
≤ (cid:13) (cid:13) (cid:13)(cid:90) P S(z−x)(cid:16) F λ,p(z)−F˜ λ,p,S(z)(cid:17) dz(cid:13) (cid:13) (cid:13)+(cid:13) (cid:13) (cid:13)(cid:90) P S(z−x)F˜ λ,p,S(z)dz(cid:13) (cid:13) (cid:13). (30)
(cid:13) Rd (cid:13) (cid:13) Rd (cid:13)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
I II
28Bound on term I. To handle the first term above, we see that
(cid:20)(cid:90) (cid:12) 1 1 (cid:12) (cid:21)
I ≤ E Rd∥P S(z−x)∥(cid:12) (cid:12) (cid:12)P(X{z∈ ∈Z x Zλ}
xλ)
− P(X{ ∈z∈ PPS SZ Zxλ xλ+ +P S⊥ PZ Sxλ ⊥}
Z
xλ)(cid:12) (cid:12) (cid:12)p(z)dz
(cid:20) D(P Zλ)
≤ E S x
P(X ∈ Zλ)P(X ∈ P Zλ+P Zλ)
x S x S⊥ x
(cid:90) (cid:12) (cid:12) (cid:21)
· (cid:12)P(X ∈ P Zλ+P Zλ)1 −P(X ∈ Zλ)1 (cid:12)p(z)dz
Rd(cid:12) S x S⊥ x {z∈Z xλ} x {z∈PSZ xλ+P S⊥Z xλ}(cid:12)
(cid:20) D(P Zλ)
≤ E S x
P(X ∈ Zλ)P(X ∈ P Zλ+P Zλ)
x S x S⊥ x
(cid:90) (cid:90) (cid:12) (cid:12) (cid:21)
· p(z)p(y)(cid:12)1 1 −1 1 (cid:12)dydz .
Rd Rd
(cid:12) {y∈PSZ xλ+P S⊥Z xλ} {z∈Z xλ} {y∈Z xλ} {z∈PSZ xλ+P S⊥Z xλ}(cid:12)
Then, we see that by symmetry
(cid:90) (cid:90) (cid:12) (cid:12)
p(z)p(y)(cid:12)1 1 −1 1 (cid:12)dydz
Rd Rd
(cid:12) {y∈PSZ xλ+P S⊥Z xλ} {z∈Z xλ} {y∈Z xλ} {z∈PSZ xλ+P S⊥Z xλ}(cid:12)
(cid:90) (cid:90)
≤ 2 p(y)p(z)1 1 1 dzdy
Rd Rd
{z∈Z xλ} {y∈PSZ xλ+P S⊥Z xλ} {y∈/Z xλ}
≤ 2p P(X ∈ Zλ)vol (P Zλ+P Zλ∩(Zλ)c∩K)
1 x d S x S⊥ x x
(cid:16) (cid:17)
= 2p P(X ∈ Zλ) vol (P Zλ+P Zλ∩K)−vol (Zλ∩K) .
1 x d S x S⊥ x d x
Also note that
(cid:90)
P(X ∈ P Zλ+P Zλ) = p(z)1 dy
S x S⊥ x
K
{z∈PSZ xλ+P S⊥Z xλ}
≥ p vol (P Zλ+P Zλ∩K).
0 d S x S⊥ x
Combining the above bounds gives
p (cid:20) (cid:18) vol (Zλ∩K) (cid:19)(cid:21)
I ≤ 1E D(P Zλ) 1− d x
p S x vol ((P Zλ+P Zλ)∩K)
0 d S x S⊥ x
p (cid:20) (cid:18) vol (Zλ∩K) (cid:19)(cid:21)
≤ 1E D(P Zλ) 1− d x
p S x vol (P Zλ+P Zλ)
0 d S x S⊥ x
p (cid:20) (cid:18) vol (Zλ) vol (Zλ∩Kc) (cid:19)(cid:21)
= 1E D(P Zλ) 1− d x + d x .
p S x vol (P Zλ+P Zλ) vol (P Zλ+P Zλ)
0 d S x S⊥ x d S x S⊥ x
Now, we see that
vol (Zλ∩Kc) ≤ vol ((P Zλ+P Zλ)∩Kc)
d x d S x S⊥ x
= vol (P Zλ+P Zλ)−vol ((P Zλ+P Zλ)∩(K +K ))
d S x S⊥ x d S x S⊥ x S S⊥
= vol (P Zλ+P Zλ)−vol ((P Zλ∩K )+(P Zλ∩K ))
d S x S⊥ x d S x S S⊥ x S⊥
(cid:18) (cid:19)
d (cid:104) (cid:105)
= V(P Zλ[s],P Zλ[d−s])−V((P Zλ∩K )[s],(P Zλ∩K )[d−s])
s S x S⊥ x S x S S⊥ x S⊥
(cid:18) (cid:19)
d
≤ V((P Zλ∩Kc)[s],(P Zλ∩K )[d−s])
s S x S S⊥ x S⊥
(cid:18) (cid:19)
d
≤ V((P Zλ∩Kc)[s],P Zλ[d−s])
s S x S S⊥ x
= vol (P Zλ∩Kc)vol (P Zλ),
s S x S d−s S⊥ x
29where the last equality follows from [6] and the assumption on ϕ which implies that Zλ is a
x
parallelotope, and thus its projections are zonotopes. This also implies that
(cid:18) (cid:19)
d
vol (P Zλ+P Zλ) = V(P Zλ[s],P Zλ[d−s]) = vol (P Zλ)vol (P Zλ)
d S x S⊥ x s S x S⊥ x s S x d−s S⊥ x
and thus,
p (cid:20) (cid:18) vol (Zλ) vol (P Zλ∩Kc)vol (P Zλ)(cid:19)(cid:21)
I ≤ 1E D(P Zλ) 1− d x + s S x S d−s S⊥ x
p S x vol (P Zλ+P Zλ) vol (P Zλ+P Zλ)
0 d S x S⊥ x d S x S⊥ x
p (cid:20) (cid:18) vol (Zλ) vol (P Zλ∩Kc)(cid:19)(cid:21)
≤ 1E D(P Zλ) 1− d x + s S x S
p S x vol (P Zλ+P Zλ) vol (P Zλ)
0 d S x S⊥ x s S x
p (cid:20) (cid:18) vol (Z ) (cid:19)(cid:21) p (cid:20) vol (P Z ∩λ(Kc −P x))(cid:21)
≤ 1 E D(P Z ) 1− d 0 + 1 E D(P Z ) s S 0 S S ,
S 0 S 0
λp vol (P Z +P Z ) λp vol (P Z )
0 d S 0 S⊥ 0 0 s S 0
where the last inequality follows from the scaling property (1) and stationarity.
Bound on term II. For the second term in (30) we compare the marginal of F˜ with the
λ,p,S
density
(cid:20) 1 (cid:21)
F˜ (y) := E {y∈PSZ xλ} , y ∈ S.
λ,S vol (P Zλ)
s S x
By Lemma 22,
(cid:90)
(y−P x)F (y)dy = 0,
S λ,S
S
and thus,
(cid:13) (cid:34) (cid:35) (cid:13)
II = (cid:13) (cid:13)(cid:90) (cid:90) (y−P x)E p(y,ω)1 {y∈PSZ xλ,ω∈P S⊥Z xλ} dydω(cid:13) (cid:13)
(cid:13) S P(X ∈ P Zλ+P Zλ) (cid:13)
(cid:13) S S⊥ S x S⊥ x (cid:13)
(cid:13)(cid:90) (cid:20) 1 (cid:90) (cid:21) (cid:13)
= (cid:13) (cid:13)
(cid:13)
S(y−P Sx)E
P(X ∈
P{ Sy∈ ZP xλSZ +xλ P}
S⊥Z xλ)
S⊥p(y,ω)1
{ω∈P S⊥Z
xλ}dω dy(cid:13) (cid:13)
(cid:13)
(cid:13) (cid:32) (cid:34) (cid:35) (cid:33) (cid:13)
(cid:13)(cid:90) 1 (cid:90) (cid:13)
= (cid:13) (y−P x) E {y∈PSZ xλ∩PSK} p(y,ω)dω −F (y) dy(cid:13)
(cid:13) S P(X ∈ P Zλ+P Zλ) λ,S (cid:13)
(cid:13) S S x S⊥ x P S⊥Z xλ (cid:13)
(cid:34) (cid:12) (cid:12) (cid:35)
(cid:90) (cid:12) 1 (cid:90) 1 (cid:12)
≤ E ∥y−P x∥(cid:12) {y∈PSZ xλ∩PSK} p(y,ω)dω− {y∈PSZ xλ} (cid:12)dy .
S (cid:12)P(X ∈ P Zλ+P Zλ) vol (P Zλ)(cid:12)
S (cid:12) S x S⊥ x P S⊥Z xλ s S x (cid:12)
Next we see that the expression inside the absolute value satisfies
(cid:12) (cid:12)
(cid:12) 1 (cid:90) 1 (cid:12)
(cid:12) {y∈PSZ xλ∩PSK} p(y,ω)dω− {y∈PSZ xλ} (cid:12)
(cid:12)P(X ∈ P Zλ+P Zλ) vol (P Zλ)(cid:12)
(cid:12) S x S⊥ x P S⊥Z xλ s S x (cid:12)
(cid:12) (cid:12)
(cid:82) (cid:82) (cid:12)p(y,ω)1 1 −p(z,ω)1 1 (cid:12)dωdz
≤
S P S⊥Z xλ∩K
S⊥
(cid:12) {y∈PSZ xλ} {z∈PSZ xλ} {z∈PSZ xλ} {y∈PSZ xλ}(cid:12)
,
p vol ((P Zλ+P Zλ)∩K)vol (P Zλ)
0 d S x S⊥ x s S x
30and the integrand in the numerator above satisfies
(cid:12) (cid:12)
(cid:12)p(y,ω)1 1 −p(z,ω)1 1 (cid:12)
(cid:12) {y∈PS(Z xλ∩K)} {z∈PSZ xλ} {z∈PS(Z xλ∩K)} {y∈PSZ xλ}(cid:12)
≤ |p(y,ω)−p(z,ω)|1 1
{z∈PSZ xλ∩KS)} {y∈PSZ xλ∩KS)}
+|p(y,ω)|1 1 +|p(z,ω)|1 1
{z∈PSZ xλ∩K Sc} {y∈PSZ xλ∩KS)} {z∈PSZ xλ∩KS} {y∈PSZ xλ∩K Sc)}
≤ C ∥y−z∥ 1 1
p 2 {z∈PSZ xλ∩KS} {y∈PSZ xλ∩KS}
+p 1 1 +p 1 1 ,
1 {z∈PSZ xλ∩K Sc} {y∈PSZ xλ∩KS} 1 {z∈PSZ xλ∩KS} {y∈PSZ xλ∩K Sc}
and thus
II
≤C pE(cid:20)(cid:90) (cid:90) ∥y−P Sx∥ 2∥y−z∥ 21 {y∈PSZ xλ∩KS}1 {z∈PSZ xλ∩KS} dzdy(cid:21)
p vol (P Zλ∩K )vol (P Zλ)
0 S S s S x S s S x
(cid:34) (cid:35)
+
2p 1E (cid:90) (cid:90) ∥y−P Sx∥1 {y∈PSZ xλ∩KS}1 {z∈PSZ xλ∩K Sc}
dzdy
p vol (P Zλ∩K )vol (P Zλ)
0 S S s S x S s S x
C (cid:104) (cid:105) 2p
(cid:20)
vol (P
Zλ∩Kc)(cid:21)
≤ pE D(P Zλ)2 + 1E D(P Zλ) s S x S .
p S x p s x vol (P Zλ)
0 0 s S x
Then by the scaling property (1) and stationarity,
II ≤
C p E(cid:2)
D(P Z
)2(cid:3)
+
2p 1E(cid:20)
D(P Z
)vol s(P SZ 0∩λ(K Sc −P Sx))(cid:21)
.
λ2p S 0 λp S 0 vol (P Z )
0 0 s S 0
Final Bound. Combining the upper bounds on I and II gives
(cid:18) Lp (cid:20) (cid:18) vol (Z ) (cid:19)(cid:21) LC E(cid:2) D(P Z )2(cid:3)
(f(x)−f˜(x))2 ≤ 1E D(P Z ) 1− d 0 + p S 0
λ λp S 0 vol (P Z +P Z ) λ2p
0 d S 0 S⊥ 0 0
3Lp (cid:20) D(P Z )vol (P Z ∩λ(Kc −P x))(cid:21) LE[D(P Z )1+β](cid:19)2
+ 1E S 0 s S 0 S S + S 0 . (31)
λp vol (P Z ) λ1+β
0 s S 0
31Taking the conditional expectation with respect to X and applying Jensen’s inequality gives,
E[(f(X)−f˜(X))2|X ∈ K ]
λ δ
(cid:20)(cid:18) Lp (cid:20) (cid:18) vol (Z ) (cid:19)(cid:21) LC E(cid:2) D(P Z )2(cid:3)
≤ E 1E D(P Z ) 1− d 0 + p S 0
X λp S 0 vol (P Z +P Z ) λ2p
0 d S 0 S⊥ 0 0
+
3 λL pp 1E(cid:20) D(P SZ 0)vol s v( oP lS (Z P0 Z∩λ )(K Sc −X S))(cid:21)
+
LE[D( λP 1S +Z β0)1+β](cid:19)2(cid:12) (cid:12)
(cid:12) (cid:12)X ∈ K
δ(cid:21)
0 s S 0
(cid:18) Lp (cid:20) (cid:18) vol (Z ) (cid:19)(cid:21) LC E(cid:2) D(P Z )2(cid:3) LE[D(P Z )1+β](cid:19)2
=
1E
D(P Z ) 1−
d 0
+
p S 0
+
S 0
λp S 0 vol (P Z +P Z ) λ2p λ1+β
0 d S 0 S⊥ 0 0
+ 9 λL 22 pp 22 1E X(cid:34) E(cid:20) D(P SZ 0)vol s v( oP lS (Z P0 Z∩λ )(K Sc −X S))(cid:21)2(cid:12) (cid:12) (cid:12) (cid:12)X ∈ K δ(cid:35)
0 s S 0
(cid:18) Lp (cid:20) (cid:18) vol (Z ) (cid:19)(cid:21) LC E(cid:2) D(P Z )2(cid:3) LE[D(P Z )1+β](cid:19)
+
1E
D(P Z ) 1−
d 0
+
p S 0
+
S 0
λp S 0 vol (P Z +P Z ) λ2p λ1+β
0 d S 0 S⊥ 0 0
·
6Lp 1E(cid:20) D(P SZ 0)vol s(P SZ 0∩λ(K Sc −X S))(cid:12) (cid:12)
(cid:12)X ∈ K
δ(cid:21)
λp vol (P Z ) (cid:12)
0 s S 0
(cid:18) Lp (cid:20) (cid:18) vol (Z ) (cid:19)(cid:21) LC E(cid:2) D(P Z )2(cid:3) LE[D(P Z )1+β](cid:19)2
≤
1E
D(P Z ) 1−
d 0
+
p S 0
+
S 0
λp S 0 vol (P Z +P Z ) λ2p λ1+β
0 d S 0 S⊥ 0 0
+
9 λL 22 pp 22 1E(cid:20) D(P SZ 0)2vol vs o(P
l
S (PZ 0 Z∩ )λ 2(K Sc −X S))2(cid:12) (cid:12)
(cid:12) (cid:12)X ∈ K
δ(cid:21)
0 s S 0
(cid:18) 6L2p2 (cid:20) (cid:18) vol (Z ) (cid:19)(cid:21) 6L2C p E(cid:2) D(P Z )2(cid:3)
+ 1E D(P Z ) 1− d 0 + p 1 S 0
λ2p2 S 0 vol (P Z +P Z ) λ3p2
0 d S 0 S⊥ 0 0
+
6L2p 1E λ[D 2+( βP pSZ 0)1+β](cid:19) E(cid:20) D(P SZ 0)vol s v( oP lS (Z P0 Z∩λ )(K Sc −X S))(cid:12) (cid:12)
(cid:12) (cid:12)X ∈ K
δ(cid:21)
.
0 s S 0
Conditioned on X ∈ K , we have that δBd ⊆ K −X. Thus,
δ
P Z ∩λ((P K)c−X) ⊆ P Z ∩λ(S\δP Bd),
S 0 S S 0 S
and if D(P Z ) ≤ λδ, the volume is zero. Thus, for k ∈ {1,2},
S 0
E(cid:20) D(P SZ 0)kvol s( vP oS lZ (P0∩ Zλ( )( kP SK)c−X))k(cid:12) (cid:12)
(cid:12) (cid:12)X ∈ K
δ(cid:21)
s S 0
1 (cid:20) D(P Z )kvol (P Z ∩λ((P K)c−X))k (cid:21)
≤ E S 0 s S 0 S 1
P(X ∈ K ) vol (P Z )k {D(PSZ0)≥λδ}
δ s S 0
1
(cid:20) (cid:20)
vol (P Z ∩λ((P
K)c−X))(cid:21)(cid:21)
= E D(P Z )k1 E s S 0 S
P(X ∈ K ) S 0 {D(PSZ0)≥λδ} X vol (P Z )
δ s S 0
s−1
≤ p 1 (cid:88) κ s−jV j(K S) E(cid:104) D(P Z )s−j+k1 (cid:105) ,
P(X ∈ K ) λs−j S 0 {D(PSZ0)≥λδ}
δ
j=0
where we have used the fact that
vols(PSZ0∩λ((PSK)c−X))2
≤
vols(PSZ0∩λ((PSK)c−X))
and we have
vols(PSZ0)2 vols(PSZ0)
applied Lemma 23 in the last inequality. Finally observing that P(X ∈ K ) ≥ p vol (K ), the
δ 0 d δ
32complete upper bound on the risk is then
E[(fˆ (X)−f(X))2|X ∈ K ]
λ,n,M δ
(cid:18) Lp (cid:20) (cid:18) vol (Z ) (cid:19)(cid:21) LC E(cid:2) D(P Z )2(cid:3) LE[D(P Z )1+β](cid:19)2
≤
1E
D(P Z ) 1−
d 0
+
p S 0
+
S 0
λp S 0 vol (P Z +P Z ) λ2p λ1+β
0 d S 0 S⊥ 0 0
+
9L2p3
1
(cid:88)s−1 κ s−jV j(K S)
E(cid:2) D(P Z )s−j+21 (cid:3)
λ2p3vol (K ) λs−j S 0 {D(PSZ0)≥λδ}
0 d δ j=0
(cid:18) 6L2p2 (cid:20) (cid:18) vol (Z ) (cid:19)(cid:21) 6L2C p E(cid:2) D(P Z )2(cid:3)
+ 1E D(P Z ) 1− d 0 + p 1 S 0
λ2p2 S 0 vol (P Z +P Z ) λ4p2
0 d S 0 S⊥ 0 0
+ 6L2p 1E[D(P SZ 0)1+β](cid:19) p 1 (cid:88)s−1 κ s−jV j(K S) E(cid:2) D(P Z )s−j+11 (cid:3)
λ3+βp p vol (K ) λs−j S 0 {D(PSZ0)≥λδ}
0 0 d δ
j=0
+
L2E[D(P SZ 0)2]
+
5∥f∥2 ∞+2σ2 (cid:32) (cid:88)d
c λkD(P
Π)k−s+(cid:88)s−1
c
λk(cid:33)
.
λ2M np vol (K ) d,k S⊥ d,k
0 d δ
k=s k=0
For δ = 0, we have
E[(fˆ (X)−f(X))2|X ∈ K ]
λ,n,M δ
(cid:18) Lp (cid:20) (cid:18) vol (Z ) (cid:19)(cid:21) LC E(cid:2) D(P Z )2(cid:3) LE[D(P Z )1+β](cid:19)2
≤
1E
D(P Z ) 1−
d 0
+
p S 0
+
S 0
λp S 0 vol (P Z +P Z ) λ2p λ1+β
0 d S 0 S⊥ 0 0
+
9L2p3
1
(cid:88)s−1 κ s−jV j(K S)
E(cid:2) D(P Z )s−j+2(cid:3)
λ2p3vol (K) λs−j S 0
0 d j=0
(cid:18) 6L2p2 (cid:20) (cid:18) vol (Z ) (cid:19)(cid:21) 6L2C p E(cid:2) D(P Z )2(cid:3)
+ 1E D(P Z ) 1− d 0 + p 1 S 0
λ2p2 S 0 vol (P Z +P Z ) λ4p2
0 d S 0 S⊥ 0 0
+
6L2p 1E[D(P SZ 0)1+β](cid:19) p 1 (cid:88)s−1 κ s−jV j(K S) E(cid:2)
D(P Z
)s−j+1(cid:3)
λ3+βp p vol (K) λs−j S 0
0 0 d
j=0
+
L2E[D(P SZ 0)2]
+
5∥f∥2 ∞+2σ2 (cid:32) (cid:88)d
c λkD(P
Π)k−s+(cid:88)s−1
c
λk(cid:33)
.
λ2M np vol (K) d,k S⊥ d,k
0 d
k=s k=0
A.2 Proof of Theorem 10 and Corollary 11
Proof of Theorem 10. First, note that by our assumption on A,
(cid:20) (cid:18) (cid:19)(cid:21)
vol (Z )
E D(P Z ) 1− d 0 = 0,
S 0
vol (P Z +P Z )
d S 0 S⊥ 0
because Z = P Z +P Z . Next, by Lemma 21, for δ ≥ 0 and k > 0,
0 S 0 S⊥ 0
E(cid:104) D(P SZ 0)k1 {D(PSZ0)≥λδ}(cid:105) ≤ dkσmk (PΓ(2 Am )k+ Γ(k 2)
m)
2m (cid:88)+k−1 n1
!
(cid:18) λδdσ s m(P SA)(cid:19)n e−λδdσs m(PSA) ,
s S
n=0
Also recall from equation (26) in the proof of Theorem 8 that
D(P Π) ≤ ∥P A∥ .
S⊥ S⊥ 2,1
33Then by the above bounds and Lemma 21, the upper bound on the risk for δ > 0, focusing on the
leading order term w.r.t λ, satisfies
E[(fˆ (X)−f(X))2|X ∈ K ]
λ,n,M δ
(cid:18) 2LC d2 LΓ(2d+1+β) (cid:19)2 2L2d2
p
≤ + +
λ2σ (P A)2p 21+βλ1+βσ (P A)1+βΓ(2d) λ2Mσ (P A)2
s S 0 s S s S
5∥f∥2 +2σ2 (cid:32) (cid:88)d (cid:88)s−1 (cid:33)
+ ∞ c λk∥P A∥k−s+ c λk +o(λ−2−2β).
np vol (K ) d,k S⊥ 2,1 d,k
0 d δ
k=s k=0
For δ = 0, the upper bound satisfies
E[(fˆ (X)−f(X))2]
λ,n,M
(cid:18) 2LC d2 LΓ(2d+1+β) (cid:19)2 9L2p3κ V (K ) m3Γ(2m+3)
≤ p + + 1 1 s−1 S
λ2σ (P A)2p 21+βλ1+βσ (P A)1+βΓ(2d) λ3p3vol (K) d3σ (P A)3Γ(2m)
s S 0 s S 0 d s S
2L2d2 5∥f∥2 +2σ2 (cid:32) (cid:88)d (cid:88)s−1 (cid:33)
+ + ∞ c λk∥P A∥k−s+ c λk +o(λ−2−2β).
λ2Mσ (P A)2 np vol (K) d,k S⊥ 2,1 d,k
s S 0 d
k=s k=0
Proof of Corollary 11. For the statement in Corollary 11, the assumptions imply
L2 L2 (cid:80)d λkεk−s+o(λs)
E[(fˆ (X)−f(X))2|X ∈ K ] ≲ + + k=s n n n +o(λ−2−2β).
λn,n,Mn δ λ2 n+2β λ2 nM
n
n n
Then additionally assuming M ≳ λ2β, we have
n n
L2 (cid:80)d λkεk−s
E[(fˆ (X)−f(X))2|X ∈ K ] ≲ + k=s n n +o(λ−2−2β).
λn,n,Mn δ λ2+2β n n
n
2 1 − d−s
Minimizing the upper bound with respect to λ
n
gives that for λ
n
≍ Ld+2β+2nd+2β+2ε nd+2β+2,
E[(f(X)−fˆ (X))2|X ∈ K ]
λn,n,Mn δ
(cid:32) (cid:33)
≲
(cid:18)
2
1L2
− d−s
(cid:19)2+2β
+ n1 (cid:18) Ld+22 β+2nd+21 β+2ε− nd+d 2− βs +2(cid:19)d εd n−s
Ld+2β+2nd+2β+2ε nd+2β+2
=
L2− d+4+ 2β4 +β 2nd+2+ 2β2 +β 2ε− n(d− d+s) 2( β2 ++ 22β) +Ld+22 βd +2nd+2d β+2−1 ε− ndd +(d 2− β+s) 2+d−s
2d 2+2β −(d−s)(2+2β)
= Ld+2β+2nd+2β+2ε
n
d+2β+2 .
and if ε n ≲ L− s+22 β+2n− s+21 β+2 we have that for λ n ≍ Ls+22 β+2ns+21 β+2,
E[(f(X)−fˆ λn,n,Mn(X))2|X ∈ K δ] ≲ Ls+22 βs +2n− s+2β 2+ β+2 2.
For δ = 0, the upper bound satisfies
L2 L2 L2 1 (cid:32) (cid:88)d (cid:88)s−1 (cid:33)
E[(fˆ (X)−f(X))2] ≲ + + + λkεk−s+ λk +o(λ−2−2β).
λ,n,M λ2 n+2β λ3
n
λ2 nM n
k=s
n n
k=0
n n
34If 3 ≥ 2+2β, then the same rates as above hold. If 3 < 2+2β, then
L2 L2 1 (cid:32) (cid:88)d (cid:88)s−1 (cid:33)
E[(fˆ (X)−f(X))2] ≲ + + λkεk−s+ λk +o(λ−3).
λ,n,M λ3 λ2M n n n n n
n n
k=s k=0
Additionally assuming M ≳ λ gives
n n
L2 1 (cid:32) (cid:88)d (cid:88)s−1 (cid:33)
E[(fˆ (X)−f(X))2] ≲ + λkεk−s+ λk +o(λ−3).
λ,n,M λ3 n n n n n
n
k=s k=0
2 1 −d−s
Minimizing the upper bound with respect to λ
n
gives that for λ
n
∼ Ld+3nd+3ε nd+3,
3(d−s)
E[(fˆ λ,n,M(X)−f(X))2] ≲ Ld2 +d 3n− d+3 3ε nd+3 ,
and if ε n ≲ L− s+2 sn− s+1 3 we have that for λ n ≍ Ls+2 3ns+1 3,
E[(fˆ λ,n,M(X)−f(X))2] ≲ Ls2 +s 3n− s+3 3.
A.3 Proofs of Theorem 12 and 14
We begin with a lemma on the diameter of the projected zero cell of the tessellation generated by
an oblique Mondrian process as a special case of Lemma 21.
Lemma 24. Suppose that Z is the zero cell of a weighted Mondrian tessellation with unit lifetime
0
and directional distribution (20). Then, for r ≥ 0 and k > 0
(cid:104) (cid:105) Γ(2s+k) 2s (cid:88)+k−1 rnωn
E D(P Z )k1 ≤ Se−rωS,
S 0 {D(PSZ0)≥r} Γ(2s) n!
n=0
where ω := min ω . In particular,
S i∈S i
Γ(2s+k)
E[D(P Z )k] ≤ .
S 0 ωkΓ(2s)
S
Proof. Recall that Z has the same distribution as the Minkowksi sum of the line segments
0
ω−1[−T(i)
e
,T(i)
e ], for i = 1,...,d,
i 1 i 2 i
(i)
where T are i.i.d. exponential random variables with unit parameter. The diameter of P Z then
j S 0
has the following upper bound:
(cid:32) (cid:33)1/2
D(P Z ) =
(cid:88) ω−2(cid:16) T(1) +T(2)(cid:17)2
≤
(cid:88) ω−1(cid:16) T(1) +T(2)(cid:17)
≤
ω−1(cid:88)(cid:16) T(1) +T(2)(cid:17)
,
S 0 i i i i i i S i i
i∈S i∈S i∈S
where ω := min ω . That is, the diameter of P Z is controlled by the sum of exponential
S i∈S i S 0
random variables, which is an Erlang distributed random variable
TS :=
(cid:88)(cid:16)
T(1)
+T(2)(cid:17)
∼ Erlang(2s,1).
i i
i∈S
35Thus, for r ≥ 0 and k > 0,
(cid:104) (cid:105) (cid:104) (cid:105) Γ(2s+k) 2s (cid:88)+k−1 rnωn
E D(P Z )k1 ≤ ω−kE (TS)k1 = Se−rωS,
S 0 {D(PSZ0)≥r} S {TS≥rωS} Γ(2s) n!
n=0
and moments of the diameter satisfy
E[(TS)k] Γ(2s+k)
E[D(P Z )k] ≤ = .
S 0 ωk ωkΓ(2s)
S S
Proof of Theorem 12. Under the assumptions of the theorem, by the bias-variance decomposition
(22), Lemma 19 and Lemma 20 in [30], we have the following upper bound on the risk of the
weighted Mondrian tree estimator fˆ :
n
E[(f(X)−fˆ (X))2] = E[(f (X)−f¯(X))2]+E[(f¯(X)−fˆ (X))2]
n λ λ λ,n
L2 5∥f∥2 +2σ2
≤ E[D(P Z )2]+ ∞ E[N ([0,1]d)].
λ2 S 0 n λ
By Lemma 24, we also have the upper bound
6
E[D(P Z )2] ≤ .
S 0 ω2
S
We next bound the expectation in the variance upper bound. Let Z be the typical cell of a
λ
STIT with directional distribution (20) and lifetime λ. Then, the support function of the typical
cell Z := Z is given by
1
d
1 (cid:88)
h(Z,u) = T |⟨u,e ⟩|,
i i
2
i=1
where T ,...T are independent and T ∼ exp(ω ). By the formula for mixed volumes of a zonoid
1 d i i
from [34, p. 614],
̸= d−k
d 1 (cid:88) (cid:89)
V(W[k],Z[d−k]) = T ,
(cid:0) d (cid:1) ij
d−k i1,...,i d−1j=1
and E[V(W[k],Z[d−k])] = 1 (cid:80)̸= (cid:81)d−k 1 . Thus, by Lemma 6 in [30],
( d−d k) i1,...,i d−k j=1 ωij
d ̸= d−k d ̸= d−k
(cid:88) (cid:88) (cid:89) 1 (cid:88) (cid:88) (cid:89) 1
N ([0,1]d) = vol (Π ) λk = vol (Π ) λd
λ d n d n
ω λω
k=0 i1,...,i d−kj=1
ij
k=0 i1,...,i d−kj=1
ij
d (cid:18) (cid:19)
(cid:89) 1
= vol (Π)λd +1 .
d
λω
i
i=1
Using the fact that the associated zonoid for the weighted Mondrian is the hyperrectangle
ω
Π = ⊕d i [−1,1], (32)
i=1 2
36we see that vol (Π) =
(cid:81)d
ω , and thus,
d i=1 i
d d (cid:18) (cid:19) d
(cid:89) (cid:89) 1 (cid:89)
N ([0,1]d) = λω +1 = (1+λω ).
λ i i
λω
i
i=1 i=1 i=1
Combining the above observations gives the final bound
6L2 5∥f∥2 +2σ2 (cid:89)d
E[(f(X)−fˆ (X))2] ≤ + ∞ (1+λω ).
n λ2ω2 n i
S i=1
Proof of Theorem 14. Note that under the definition of the directional distribution for a weighted
Mondrian, the associated zonoid is the hyperrectangle (32), and thus we are in the setting where
Π = Π + Π for Π ⊂ S and Π ⊂ S⊥. Then, from the proof of Theorem 7, we have the
S S⊥ S S⊥
following upper bound on the risk for a weighted Mondrian forest fˆ with M trees, lifetime λ,
λ,n,M
and directional distribution (20):
(cid:18)LC E(cid:2) D(P Z )2(cid:3) LE[D(P Z )2](cid:19)2
E[(fˆ (X)−f(X))2|X ∈ [δ,1−δ]d] ≤ p S 0 + S 0
n λ2p λ2
0
+
9L2p3
1
(cid:88)s−1 κ s−jV j([0,1]d)
E(cid:2) D(P Z )s−j+21 (cid:3)
λ2p3(1−2δ)d λs−j S 0 {D(PSZ0)≥λδ}
0 j=0
(cid:18)6L2C p E(cid:2) D(P Z )2(cid:3) 6L2p E[D(P Z )1+β](cid:19)
p 1 S 0 1 S 0
+ +
λ4p2 λ3+βp
0 0
· p 1 (cid:88)s−1 κ s−jV j([0,1]s) E(cid:2) D(P Z )s−j+11 (cid:3)
p (1−2δ)d λs−j S 0 {D(PSZ0)≥λδ}
0
j=0
6L2s 5∥f∥2 +2σ2
+ + ∞ E[N ([0,1]d)].
λ2Mω2 n λ
S
By Lemma 24 and (8),
(cid:18)
LC Γ(2s+2)
LΓ(2s+1+β)(cid:19)2
E[(fˆ (X)−f(X))2|X ∈ [δ,1−δ]d] ≤ p +
λ,n,M λ2p 0ω S2Γ(2s) λ1+βω S1+βΓ(2s)
+
9L2p3
1
(cid:88)s−1(cid:18) s(cid:19) κ s−jΓ(2s+s−j +2) 2s+(s (cid:88)−j+2)−1 λℓεℓω Sℓ
e−λεωS
λ2p3(1−2δ)d j λs−jΓ(2s) ℓ!
0 j=0 ℓ=0
(cid:18) 6L2C p Γ(2s+2) 6L2p Γ(2s+1+β)(cid:19)
p 1 1
+ +
λ4p2ω2Γ(2s) λ3+βp ω1+βΓ(2s)
0 S 0 S
·
p
1
(cid:88)s−1(cid:18) s(cid:19) κ s−jΓ(2s+s−j +1) 2s+(s (cid:88)−j+1)−1 λℓεℓω Sℓ
e−λεωS
p (1−2δ)d j λs−jΓ(2s) ℓ!
0
j=0 ℓ=0
6L2s 5∥f∥2 +2σ2 (cid:89)d
+ + ∞ (1+λω ).
λ2Mω2 n i
S i=1
37Thus, for δ > 0,
(cid:18) C (cid:19)2 L2
E[(fˆ (X)−f(X))2|X ∈ [δ,1−δ]d] ≤ 4s2(2s+1)2 p +1
n p λ4ω4
0 S
6L2s 5∥f∥2 +2σ2 (cid:89)d
+ + ∞ (1+λω )+o(λ−4).
λ2Mω2 n i
S i=1
and for δ = 0,
(cid:18) C (cid:19)2 L2 18L2p3sΓ(2s+3)
E[(fˆ (X)−f(X))2] ≤ 4s2(2s+1)2 p +1 + 1
λ,n,M p λ4ω4 λ3p3Γ(2s)
0 S 0
6L2s 5∥f∥2 +2σ2 (cid:89)d
+ + ∞ (1+λω )+o(λ−3).
λ2Mω2 n i
S i=1
A.4 Proof of Theorem 16
Proof. Recall the bias-variance decomposition (22) of a weighted Mondrian tree estimator fˆ with
n
lifetime λ:
E[(f(X)−fˆ (X))2] = E[(f (X)−f¯(X))2]+E[(f¯(X)−fˆ (X))2].
n λ λ λ,n
First we obtain a lower bound on the bias. Recall that the distribution of the cell Zλ of a weighted
x
Mondrian tessellation with lifetime λ and directional distribution (20) containing x ∈ Rd is the
hyperrectangle
d
(cid:89)(cid:104)
(1)
(2)(cid:105)
x −T ,x +T ,
i i i i
i=1
(1) (2)
where for each i = 1,...,d, T and T are independent exponential random variables with
i i
parameter λω . Then, under the assumptions in the theorem,
i
(cid:90)
1
f¯(x)−f(x) = f(y)−f(x)dµ(y)
λ µ(Zλ)
x Rd
(cid:90)
1
= ⟨a,y−x⟩dy
vol (Zλ∩[0,1]d)
d x Zλ∩[0,1]d
x
d (cid:90)
(cid:88) a i
= (y −x )dy
i i i
i=1
|[x i−T i(1) ,x i+T i(2) ]∩[0,1]| [xi−T i(1),xi+T i(2)]∩[0,1]
d (cid:90)
(d) (cid:88) a i
= tdt
i=1
|[−T i(1) ,T i(2) ]∩[−x i,1−x i]| [−T i(1),T i(2)]∩[−xi,1−xi]
d
(cid:88) a i (cid:16) (i) (i) (cid:17)
= min{1−x ,T }−min{x ,T } .
2 i 2 i 1
i=1
38Squaring the above expression, taking the expectation with respect to the random tessellation, and
applying Jensen’s inequality gives

(cid:32) d
(cid:33)2
E P[(f¯ λ(x)−f(x))2] ≥ E  (cid:88) a 2i (cid:16) min{1−x i,T 2(i) }−min{x i,T 1(i) }(cid:17) 
i=1
=
(cid:88)d a2
i
E(cid:20) (cid:16)
min{1−x ,T(i) }−min{x ,T(i)
}(cid:17)2(cid:21)
4 i 2 i 1
i=1
d
+
(cid:88) a ia jE(cid:104)
min{1−x
,T(i)
}−min{x
,T(i) }(cid:105) E(cid:104)
min{1−x
,T(j)
}−min{x
,T(j) }(cid:105)
4 i 2 i 1 j 2 j 1
i,j=1:i̸=j
=
(cid:88)d a2
i
(cid:16) E(cid:104)
min{1−x ,T(i)
}2(cid:105) −2E(cid:104)
min{1−x ,T(i)
}(cid:105) E(cid:104)
min{x ,T(j)
}(cid:105) +E(cid:104)
min{x ,T(i)
}2(cid:105)(cid:17)
4 i 2 i 2 j 1 i 1
i=1
d
+
(cid:88) a ia jE(cid:104)
min{1−x
,T(i)
}−min{x
,T(i) }(cid:105) E(cid:104)
min{1−x
,T(j)
}−min{x
,T(j) }(cid:105)
.
4 i 2 i 1 j 2 j 1
i,j=1:i̸=j
For the terms in the sum above, we have for any t ∈ [0,1] and T ∼ Exponential(λω ),
i
(cid:90) ∞ (cid:90) ∞
E[min{t,T}] = P(min{t,T} ≥ r)dr = P(T ≥ r)1 dr
{t≥r}
0 0
(cid:90) t 1 (cid:16) (cid:17)
= e−λωirdr = 1−e−λωit .
λω
0 i
Also,
(cid:90) ∞ (cid:90) ∞
E[min{t,T}2] = 2 rP(min{t,T} ≥ r)dr = 2 rP(T ≥ r)1 dr
{t≥r}
0 0
(cid:90) t 2 2 2t
= 2 re−λωirdr = − e−λωit− e−tλωi.
λ2ω2 λ2ω2 λω
0 i i i
39Plugging these moments into the above bound and taking the expectation with respect to X gives
E(cid:2) (f¯(X)−f(X))2(cid:3) ≥ (cid:88)d a2 i (cid:18) 2 − 2 E(cid:104) e−λωi(1−Xi)(cid:105) − 2 E(cid:104) (1−X )e−(1−Xi)λωi(cid:105)
λ 4 λ2ω2 λ2ω2 λω i
i=1 i i i
2 (cid:104) (cid:105) (cid:104) (cid:105)
− E 1−e−λωi(1−Xi) E 1−e−λωiXi
λ2ω2
i
(cid:20) (cid:21)(cid:19)
2 2 (cid:104) (cid:105) 2X
+ − E e−λωiXi −E i e−Xiλωi
λ2ω2 λ2ω2 λω
i i i
(cid:88)d a2(cid:20)
2 2 (cid:16) (cid:17)
(cid:18)
2 2 2
(cid:19)
= i − 1−e−λωi − − e−λωi − e−λωi
4 λ2ω2 λ3ω3 λ3ω3 λ3ω3 λ2ω2
i=1 i i i i i
2
(cid:18)
1 (cid:16)
(cid:17)(cid:19)2
− 1− 1−e−λωi
λ2ω2 λω
i i
(cid:18) (cid:19)(cid:21)
2 2 (cid:16) (cid:17) 2 2 2
+ − 1−e−λωi − − e−λωi − e−λωi
λ2ω2 λ3ω3 λ3ω3 λ3ω3 λ2ω2
i i i i i
(cid:88)d a2(cid:20)
4 8 8 4
= i − + e−λωi + e−λωi
4 λ2ω2 λ3ω3 λ3ω3 λ2ω2
i=1 i i i i
(cid:21)
2 4 (cid:16) (cid:17) 2 (cid:16) (cid:17)2
− + 1−e−λωi − 1−e−λωi
λ2ω2 λ3ω3 λ4ω4
i i i
(cid:88)d a2(cid:20)
2 4 4 4 2 (cid:16)
(cid:17)2(cid:21)
= i − + e−λωi + e−λωi − 1−e−λωi
4 λ2ω2 λ3ω3 λ3ω3 λ2ω2 λ4ω4
i=1 i i i i i
(cid:88)d a2 (cid:18) 2 1 (cid:19)
≥ i 1− − ,
2λ2ω2 λω λ2ω2
i=1 i i i
where we have used the independence of the X ’s and the following intergal evaluations:
i
(cid:90) 1 (cid:90) 1 1 (cid:16) (cid:17)
e−λωitdt = e−λωi(1−t)dt = 1−e−λωi ,
λω
0 0 i
and
(cid:90) 1 (cid:90) 1 1 1 1
te−λωitdt = (1−t)e−λωi(1−t)dt = − e−λωi − e−λωi.
λ2ω2 λ2ω2 λω
0 0 i i i
Next we obtain a lower bound for the variance term. Recall that if no inputs {X ,...,X } fall
1 n
in Zλ, then we assume the estimator fˆ (x) = 0. For each C ∈ P(λ), let N (C) = (cid:80)n 1 be
x n n i=1 {Xi∈C}
the number of covariates inside C and let p := P (X ∈ C). Then,
λ,C X
E
(cid:104)
(f¯(x)−fˆ
(x))2(cid:105)
=
(cid:90) (cid:88)
1 E
(cid:34) (cid:18)
E [f(X)|X ∈ C]−
(cid:80)n i=1Y i1 {Xi∈C}(cid:19)2(cid:35)
dµ(x)
Dn λ n {x∈C} Dn X N (C)
Rd n
C∈P(λ)
=
(cid:88)
1 E
(cid:34) (cid:18)
E [f(X)|X ∈ C]−
(cid:80)n i=1Y i1 {Xi∈C}(cid:19)2(cid:35)
.
{x∈C} Dn X N (C)
n
C∈P(λ):
C∩supp(µ)̸=∅
40For the expectation in the sum, we have
(cid:34) (cid:18) (cid:80)n Y 1 (cid:19)2(cid:35)
E E [f(X)|X ∈ C]− i=1 i {Xi∈C}
Dn X
N (C)
n
(cid:34) (cid:80)n
Y 1
(cid:18)(cid:80)n
Y 1
(cid:19)2(cid:35)
= E E [f(X)|X ∈ C]2−2E [f(X)|X ∈ C] i=1 i {Xi∈C} + i=1 i {Xi∈C}
Dn X X
N (C) N (C)
n n
As in the proof of Lemma 15 in [30],
(cid:34) (cid:18) (cid:80)n Y 1 (cid:19)2(cid:35)
E E [f(X)|X ∈ C]− i=1 i {Xi∈C}
Dn X
N (C)
n
n
= (cid:88) P(N (C) = k)k−1(cid:0)E [f(X)2|X ∈ C]−E [f(X)|X ∈ C]2)+σ2(cid:1)
n X X
k=1
+P(N (C) = 0)E [f(X)|X ∈ C]2.
n X
Now, define the random variables N˜ (C) := N (C)+1 . Then, by Jensen’s inequality,
n n {Nn(C)=0}
E
(cid:34) (cid:18)
E [f(X)|X ∈ C]−
(cid:80)n i=1Y i1 {Xi∈C}(cid:19)2(cid:35)
≥
σ2(cid:32) (cid:88)n
P(N (C) = k)k−1+P(N (C) =
0)(cid:33)
Dn X
N (C)
n n
n
k=1
= σ2E[N˜ (C)−1] ≥ σ2E[N˜ (C)]−1
n n
= σ2(np +(1−p )n)−1
λ,C λ,C
≥ σ2(np +1)−1.
λ,C
Thus, taking the expectation with respect to the random tessellation P gives the lower bound
 
E P,Dn(cid:104) (f¯ λ(x)−fˆ n(x))2(cid:105) ≥ σ2E P 

(cid:88) 1 {x∈C}(np λ,C +1)−1 

C∈P(λ):
C∩supp(µ)̸=∅
(cid:20) (cid:21)
(cid:16) (cid:17)−1
= σ2E nP (X ∈ Zλ)+1
P X x
(cid:20) (cid:21)
(cid:16) (cid:17)−1
≥ σ2E nvol (Zλ∩[0,1]d)+1 ,
d x
and then By Jensen’s inequality,
(cid:104) (cid:105) (cid:16) (cid:104) (cid:105) (cid:17)−1
E (f¯(x)−fˆ (x))2 ≥ σ2 nE vol (Zλ∩[0,1]d) +1
P,Dn λ n d x
(cid:16) (cid:104) (cid:105) (cid:17)−1
≥ σ2 nE vol (Zλ) +1
d x
(cid:18)
n
(cid:19)−1
= σ2 +1 .
2dλdΠ ω
i∈[d] i
Combining the lower bounds on the bias and the variance with (22) gives the final result.
41A.5 Proofs of Proposition 17 and Corollary 18
Proof of Proposition 17. In [28], Lemma 4 and Corollary 1 show that the capacity functional for
the cell boundaries of a STIT tessellation is determined by an associated intensity measure on the
space of hyperplanes Hd. Note that Y (λ) has associated intensity measure
A
m (cid:90) (cid:18) (cid:19)
(cid:88) ∥a i∥ 2 a i
λΛ (·) = λ 1{H ,t ∈ ·}dt, (33)
A d
∥A∥ ∥a ∥
2,1 R i 2
i=1
where H (u,t) := {x ∈ Rd : ⟨x,u⟩ = t}. The space Hd is equipped with the hit-miss topology,
d
which is generated by sets of the following form: for Borel sets C ⊂ Rd,
[C] := {H ∈ Hd : H ∩C ̸= ∅}.
Thus it suffices to show that for any Borel set C ⊂ Rd,
mλ
λΛ ([C]) = Λ ([AT(C)]),
A M
∥A∥
2,1
whereΛ istheintensitymeasureonHd associatedtotheMondriantessellationwithunitlifetime.
M
Let {e }m denote the standard basis in Rm and C a Borel subset of Rd. First, note that
i i=1
H (e ,t)∩AT(C) ̸= ∅ if and only if
m i
h (−e ) ≤ t ≤ h (e ).
AT(C) i AT(C) i
Then, noting that h (±e ) = h (±Ae ) = ∥Ae ∥ h (±Ae /∥Ae ∥ ) = ∥a ∥ h (±a /∥a ∥ ),
AT(C) i C i i 2 C i i 2 i 2 C i i 2
the above inequality is equivalent to the inequality
t
h (−a /∥a ∥ ) ≤ ≤ h (a /∥a ∥ ).
C i i 2 C i i 2
∥a ∥
i 2
These inequalities hold if and only if H (a /∥a ∥ ,t/∥a ∥ )∩C ̸= ∅. Thus,
d i i 2 i 2
m (cid:90)
mλ λ (cid:88)
Λ ([AT(C)]) = 1 dt
∥A∥
2,1
M
∥A∥
2,1 R
{Hm(ei,t)∩AT(C)̸=∅}
i=1
m (cid:90)
λ (cid:88)
= 1 dt
∥A∥ {H d(ai/∥ai∥2,t/∥ai∥2)∩C̸=∅}
2,1 R
i=1
m (cid:90)
(cid:88) ∥a i∥ 2
= λ 1 dr = λΛ ([C]).
∥A∥ {H d(ai/∥ai∥2,r)∩C̸=∅} A
2,1 R
i=1
Proof of Corollary 18. Recall that the distribution of a random convex body containing the origin
is determined by the set of containment probabilities P(K ⊆ Z) for all convex bodies K containing
the origin. For the zero cell of a STIT tessellation with associated intensity measure Λ,
P(K ⊆ Z ) = P(Y ∩K = ∅) = e−Λ([K]).
0
Thus the statement follows from the fact we showed above that for any Borel set C ⊂ Rd,
d
Λ ([C]) = Λ ([AT(C)]),
A M
∥A∥
2,1
42where Λ is the intensity measure on Hd associated to Y , since this implies
M M
P(K ⊆ Z 0) = e−ΛA([K]) = e− ∥A∥d 2,1ΛM([AT(K)])
(cid:16) (cid:17) (cid:16) (cid:17)
= P AT(K) ⊆ Z(M) = P AT(K) ⊆ Z(M) ∩ran(AT)
0 0
(cid:16) (cid:17) (cid:16) (cid:17)
= P K ⊆ (AT)+(Z(M) ∩ran(AT)) = P K ⊆ (A+)T(Z(M) ∩ran(AT)) ,
0 0
where A+ is the Moore-Penrose pseudoinverse of AT.
43