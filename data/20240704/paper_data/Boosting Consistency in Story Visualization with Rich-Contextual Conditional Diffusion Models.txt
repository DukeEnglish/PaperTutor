Boosting Consistency in Story Visualization with
Rich-Contextual Conditional Diffusion Models
Fei Shen, Hu Ye, Fei Shen, Sibo Liu, Jun Zhang⋆, Cong Wang, Xiao Han, and
Wei Yang
Tencent AI Lab
{ffeishen, huye, siboliu, junejzhang, xvencewang, haroldhan,
willyang}@tencent.com
Abstract. Recentresearchshowcasestheconsiderablepotentialofcon-
ditionaldiffusionmodelsforgeneratingconsistentstories.However,cur-
rentmethods,whichpredominantlygeneratestoriesinanautoregressive
and excessively caption-dependent manner, often underrate the contex-
tualconsistencyandrelevanceofframesduringsequentialgeneration.To
address this, we propose a novel Rich-contextual Conditional Diffusion
Models(RCDMs),atwo-stageapproachdesignedtoenhancestorygen-
eration’s semantic consistency and temporal consistency. Specifically, in
the first stage, the frame-prior transformer diffusion model is presented
topredicttheframesemanticembeddingoftheunknownclipbyaligning
thesemanticcorrelationsbetweenthecaptionsandframesoftheknown
clip. The second stage establishes a robust model with rich contextual
conditions, including reference images of the known clip, the predicted
frame semantic embedding of the unknown clip, and text embeddings
of all captions. By jointly injecting these rich contextual conditions at
theimageandfeaturelevels,RCDMscangeneratesemanticandtempo-
ral consistency stories. Moreover, RCDMs can generate consistent sto-
ries with a single forward inference compared to autoregressive models.
Our qualitative and quantitative results demonstrate that our proposed
RCDMs outperform in challenging scenarios. The code and model will
be available at https://github.com/muzishen/RCDMs.
Keywords: Story visualization · Diffusion model · Rich-Contextual
1 Introduction
Story visualization [16,21,24] aims to depict a continuous narrative through
multiplecaptionsandreferenceclips.Ithasprofoundapplicationsingamedevel-
opment and comic drawing. Due to the technological leaps in generative models
such as generative adversarial network (GAN) [4,5,11,22,39,42] and diffusion
model [25,29,31,41,45], text-to-image synthesis methods [15,26,40,43,46] can
now generate visually faithful images through text descriptions. However, given
⋆ Corresponding author
4202
luJ
2
]VC.sc[
1v28420.7042:viXra2 Fei Shen et al.
(a) Existing Methods
Frame2 Frame3 ···· Frame N Results
Autoregressive Autoregressive ···· Autoregressive
Model Model Model
Text/Image Text/Image ···· Text/Image
Encoder Encoder Encoder
Nth Step
2nd Step
1st Step
Caption1 Frame1 Caption2 Frame2 Caption3 Frame3 ···· Caption N
(b) Our Method Frames
Results
2 ~ N
Frame1
Frame-Prior Transformer Frame Features Frame1 Frame-Contextual 3D
Captions Diffusion Model 2 ~ N Conditional Diffusion Model
1 ~ N
Captions Frame Features
Frame1
1 ~ N 2 ~ N
Stage1 Stage2
Fig.1: (a) Existing methods, which employ autoregressive models and rely on the
currentcaptionforguidance,sufferfromweakconditioning,leadingtoadecreaseinthe
consistency of the generated story. (b) RCDMs initially predict the frame-contextual
information at the feature level, then simultaneously infuse image-level and feature-
level contextual information to generate coherent stories in a single forward inference.
multiple captions to generate a continuous story with style and temporal con-
sistency still poses significant challenges.
Existing methods typically employ autoregressive generation and can be
broadly classified into GAN-based [14,16,18,19] and diffusion model-based [20,
21,24,32].GAN-basedmethodstypicallycompriseatextencoder,imagegenera-
tor,imageseparation,andstorydiscriminator.Thesecomponentsworktogether
to maintain the consistency of the entire sequence of images. However, the im-
ages generated by these methods often display distorted objects, mismatched
semantics,andlocalizedblurring,especiallywhencreatingimagesfromcomplex
scene descriptions. Subsequently, GAN-based methods [18,19] progressively fo-
cus on generating more consistent images by improving the performance of the
text encoder, such as caption enhancement [19], structured text parsing [18],
and ID attention mechanism [3]. While these methods [17–19] can generate im-
ages that satisfy the requirements of character consistency, they often struggle
to maintain a consistent style and capture realistic scene details. Furthermore,
since the adversarial nature of the min-max objective, GAN-based methods can
bepronetounstabletrainingdynamics,whichlimitsthediversityofthestories.
Advanced diffusion models like Imagen [30] and Stable Diffusion [27] have
recentlydemonstratedunprecedentedtext-to-imagesynthesiscapabilities.Story
visualizationbasedondiffusionmodels[20,21,24,32]generatesbetterconsistent
images through a multi-step denoising process, using the features of the current
caption and historical frames as conditions. However, these methods [20,21,24,Rich-Contextual Conditional Diffusion Models 3
32] only consider clip information at the feature level due to the injection of
the current caption and the known clip into the model via the inherent cross-
attention module, overlooking the frame information of known at the image
level and text information of the unknown clip. Besides, from Fig. 1 (a), GAN-
based and diffusion model-based autoregressive generation methods [13,20,21,
38] depend on the current caption for frame-by-frame forward generation. This
reliancecanleadthesemethodstoeasilyoverlooktherichcontextualinformation
within the captions and the consistency of the story.
ThispaperpresentsRich-contextualConditionalDiffusionModels(RCDMs)
to tackle the issues above through two stages, as shown in Fig. 1 (b). Firstly, we
propose a frame-prior transformer diffusion model to predict the semantic em-
beddings of frames within an unknown clip. This prediction task is significantly
less complex than directly generating consistent a story. It enables the model
to focus solely on the task at the semantic feature level, thereby circumventing
the rigorous consistency requirements associated with story generation at the
imagelevel.Theframe-priortransformerdiffusionmodeltakestheframesofthe
known clip and all captions as conditions, leveraging a combination of multiple
cascaded transformer blocks and frame attention blocks to predict the frame
semantic embedding of the unknown clip. In the second stage, instead of over-
reliance on caption conditions, we integrate the text embeddings of all captions,
the frame semantic features of the unknown clip, and the images of the known
cliptoserveasarichcontextualguideforgeneratingframesintheunknownclip.
Basedontheaboverich-contextualconditional,wedeviseaframe-contextual3D
diffusion model to generate consistent stories by jointly infusing conditions at
both the image and feature levels. Besides, RCDMs can generate consistent sto-
rieswithasingleforwardinferencecomparedtoautoregressivemodels.Themain
contributions are briefly summarized as follows:
• We propose a frame-prior transformer diffusion model that, by using the
frames from the known clip and all captions as conditions, predicts the se-
mantic embeddings of frames in an unknown clip, thereby providing a rich
semantic feature for the next stage.
• We devise a frame-contextual 3D diffusion model that jointly infuses image-
level and feature-level rich-contextual conditional to generate stories with
stylistic and temporal consistency.
• We conduct comprehensive experiments on two datasets to demonstrate the
competitive performance of proposed RCDMs. Additionally, we perform a
user study to evaluate the superiority of RCDMs qualitatively.
2 Related Work
2.1 Text-to-Image Synthesis
Recent advances in text-to-image synthesis mainly focus on generative adver-
sarial networks (GANs) [22,34,39,43,44] and diffusion models. For example,
MirrorGAN [22] introduces the concept of redescription, progressively enhanc-
ing images’ diversity and semantic consistency by focusing on local words and4 Fei Shen et al.
global captions as conditions. Similarly, AttnGAN [39] synthesizes fine details
bycalculatingfine-grainedimage-captionmatchinglossesindifferentimagesub-
regions, focusing on relevant words in natural language descriptions. Moreover,
StackGAN [44] uses a two-stage approach, first sketching the shape and color of
objects based on the caption and then generating images based on the results
of the first stage and the caption. On the other hand, XMC-GAN [43] employs
an attention self-modulation generator and a contrastive discriminator to cap-
tureandreinforcethecorrespondencebetweencaptionsandimages,maximizing
theirmutualinformationtogenerateimages.Whilethesemethods[22,39,43,44]
can generate images that meet semantic requirements, GAN training heavily
depends on selecting hyperparameters and can easily lose scene details.
Unlike GANs, diffusion models [9,28,33,35,36] do not suffer from mode col-
lapse and potentially unstable training and can generate more diverse images.
For example, DALL-E2 [25] introduces a two-stage model. Initially, it gener-
ates learned CLIP image embeddings in an autoregressive fashion, using text
descriptions as a guide via a prior model. Subsequently, a diffusion model-based
decoder is employed to generate semantically consistent images based on these
embeddings. Following this, Imagen [29] aims to augment text comprehension
capabilities by utilizing a large transformer language model, thereby enhanc-
ing image-text alignment and the fidelity of the samples. Stable Diffusion [28]
presentsanovelapproachofapplyingdiffusionmodelsinthelatentspace,strik-
ing an almost perfect balance between simplification and detail preservation.
However, these text-to-image methodologies [6,28,29,37] primarily concentrate
onaligningindividuallygeneratedimageswiththeircorrespondingtextdescrip-
tions.Theyoverlookthecrucialelementsofstyleandtemporalconsistencyacross
multiple frames, which are essential for effective story visualization.
2.2 Story Visualization
StoryGAN [16] is a pioneer in story visualization, proposing a sequential con-
ditional GAN framework with a context encoder that can dynamically track
the story flow and a story-level discriminator. DuCo-StoryGAN [19] presents a
dual learning framework to enhance the semantic consistency between the story
and the generated images by improving the captions. VLC-StoryGAN [18] in-
troduces a Transformer-based recursive architecture to encode structured text
inputthroughconstituencyparsingtreeswhileusingcommonsenseinformation
to enhance the structured input of the text, making it more in line with the
sequence structure of the story. Similarly, Word-Level SV [14] also focuses on
textinputandproposesadiscriminatorwithfusedfeaturesandextendedspatial
attention to improve image quality and story consistency. Besides, VP-CSV [3]
devises a two-stage framework that predicts character tokens and the remaining
tokens to better ensure character consistency.
Storyvisualization[1,20,21,32]hasachievedgreatdevelopmentduringthese
years, especially with the unprecedented success of diffusion model. For exam-
ple, StoryDALL-E [20] explores story visualization through an autoregressive
Transformer, focusing on full model fine-tuning based on pre-trained modelsRich-Contextual Conditional Diffusion Models 5
and parameter-efficient adaptive adjustments based on prompts. AR-LDM [21]
proposes an autoregressive latent diffusion model based on historical captions
and generated images to align and enhance story consistency. Similarly, Story-
LDM [24] proposes an autoregressive diffusion framework with a visual memory
module to capture character and background context implicitly. However, the
above autoregressive methods [1,16,20,32], including GAN-based and diffusion
model-based,relyonthecurrentcaptionforguidanceandsufferfromweakcon-
ditioning, leading to a decrease in the consistency of the generated story.
3 Method
TheproposedRich-contextualConditionalDiffusionModels(RCDMs)aimsto
generate consistent stories by jointly infusing rich contextual conditions at both
theimageandfeaturelevels.Inthissection,weintroducethefollowingthreeas-
pects: preliminaries (Section 3.1), frame-prior transformer diffusion model (Sec-
tion 3.2), and frame-contextual 3D diffusion model (Section 3.3).
3.1 Preliminaries
Diffusion models represent a category of generative models trained to reverse a
diffusion process. This process systematically adds Gaussian noise to the data
through a fixed Markov chain of timestep t, while concurrently training a de-
noising model to generate samples starting from the Gaussian noise. To learn
such a diffusion model ϵ parameterized by θ, for each timestep t, the training
θ
objective usually adopts a mean square error loss L , as follows,
DM
L =E ∥ϵ−ϵ (cid:0) x ,c,t(cid:1) ∥2, (1)
DM x0,ϵ,c,t θ t
wherex representstherealdatawithanadditionalconditionc.Thetimestept
0
ofdiffusionprocessisdenotedbyt∈[0,T].Thenoisydataattstep,x ,isdefined
t
as α x +σ ϵ, where α and σ are predefined functions of t that determine the
t 0 t t t
diffusion process. Once the model ϵ is trained, images can be synthesized from
θ
random noise through an iterative process.
During the sampling stage, the predicted noise is calculated based on the
predictionsofboththeconditionalmodelϵ (x ,c,t)andtheunconditionalmodel
θ t
ϵ (x ,t) via classifier-free guidance [10].
θ t
ϵˆ (x ,c,t)=wϵ (x ,c,t)+(1−w)ϵ (x ,t). (2)
θ t θ t θ t
Here, w is the guidance scale used to adjust condition c.
3.2 Frame-Prior Transformer Diffusion Model
In the first stage, we propose a frame-prior transformer diffusion model to pre-
dict the frame semantic embeddings of unknown clips by aligning the semantic
correlationsbetweenthecaptionsandframesofknownclips.AsshowninFig.2,
it is composed of a frozen image encoder, a frozen text encoder, an unshared6 Fei Shen et al.
GGrroouunndd TTrruutthh
Timestep t 10 x
+ Noise Embedding IImmaaggee
F EEnnccooddeerr
(
i
(s1
2
)
)b
FeF
nK
rr eee
dan
d
t
iho
s
s
t
w
ta
th
an
en
ld
k
c
s
iC
a
n
b
r
g.yl
S
i
a
ap
st o
c
hr
a
ey
r ,
s
S
th
ap
o
nll ddit
sin ig
nU
a
th
n
w
e
k
h
roin
t
aeo
d
h
,w
e
hln
om
l
deC
it n,
l gwi p
ah i hle
e
lB ma er tn .ey
EEE
EEIII
nnn
nnmmm
TTccc
cceeoooaaa ooxxdddggg ddtteee
eee
ee
rrr
rr
HP Po oido odl le eed
dn
aaLL
rraaeenniiLL
ddeerraahhssnnUU
v
tv php
olB
remrofsnarT
B
noitnettA
emar
Lprior
( th3 e) B wa hr en ee ly
s,
i as
s
u hn ed e sr
p
ea
a
c ka sr ., only his upper body visible between Hidden rreeyy t
h
kc col
(4) Mr Slate is speaking out loud while standing outside. k
(5) Fred is on the road in a race car as Barney watches. Extra * PPrreeddiicctt
Text Input Embedding HHeeaadd
Fig.2: Illustration of the frame-prior transformer diffusion model. The frame-prior
transformerdiffusionmodelpredictstheframesemanticembeddingsofunknownclips
byaligningthesemanticcorrelationsbetweenthecaptionsandframesofknownclips.
linear layer, and a stack of multiple transformer blocks and frame attention
blocks. Here, we utilize the pooled representation extracted from the CLIP im-
age encoder as the frame semantic embeddings for unknown clips. Our choice is
inspired by the capability of CLIP [23], which is trained on a large-scale dataset
of image-text pairs through contrastive learning. This enables it to encapsulate
a rich variety of image content and stylistic information, pivotal in steering the
subsequent process of story synthesis.
Specifically, we first split the ground truth into known and unknown clips
and introduce noise into the unknown clips. Subsequently, all clips (known and
unknown) and all captions are fed into the frozen image encoder and the frozen
text encoder, respectively. We then use an unshared linear layer to obtain the
pooledvisualrepresentationv andhiddenvisualrepresentationv oftheimage
p h
andpooledtextualrepresentationt andhiddentextualrepresentationt ofthe
p h
caption. For consecutive frames, we input them as a 4D tensor x ∈ Rb×f×n×d,
where b, f, n, and d represent the batch size, temporal length, token length
and each token dimension, respectively. When the internal feature map passes
through the transformer block, the temporal length f is reshaped to the batch
size b and ignored, allowing the model to process each frame independently.
We reshape the feature map back into a 4D tensor after the transformer block,
frame attention block reshape n into b to learn and maintain the temporal con-
sistency of the story, and then reshape it back after the module while ignoring
thetemporallength.Inspiredby[2,7],theframeattentionblockconsistsofsev-
eral self-attention modules. This allows the model to guide self-attention along
thetemporallengthf,effectivelycapturingthedynamiccontentwithinthenar-
rative. Besides, we add an extra embedding to represent the unnoised frame
semantic embedding of the known clip to be predicted.
FollowingunCLIP[25],theframe-priortransformerdiffusionmodelistrained
topredicttheunnoisedframesemanticembeddingdirectlyratherthanthenoise
added to the frame embedding. The training loss L of frame-prior trans-
priorRich-Contextual Conditional Diffusion Models 7
Lcontextual
Target Clip
+Noise
·· ·· ·· ·· ·
VVAAEE
EEnnccooddeerr
c
w
Known Clip Unknown Clip h
Image-Level Condition
Text Input Feature-Level Condition EEmmbbeeddddiinnggss UUU---NNNeeettt BBBllloooccckkk
(1) Fred stands by a car, holding a white helmet, while TTeexxtt FFeeaattuurree FFFAAA BBBllloooccckkk
Barney is beneath the car.
(2) Fred is talking as he stands in the road, holding a MMuullttiimmooddaall CC SSeemmaannttiicc CCC CCoonnccaatteennaattiioonn
helmet. IImmaaggee FFeeaattuurree IInntteerraaccttiioonn MMoodduullee SSttaacckkiinngg MMoodduullee
(3) Barney is under a car, only his upper body visible
between the wheels, as he speaks.
( (4 5) ) M Frr e dS l ia s t oe ni s t hs ep e ra ok ai dn g in o au t r al co eu d c aw r h ai sl e B s at ra nn ed yi n wg a o tcu hts ei sd .e. TTTeeexxxttt EEEnnncccooodddeeerrr PPPPrrrriiiioooorrrr MMMMooooddddeeeellll
Fig.3: Overview of the frame-contextual 3D diffusion model. The frame-contextual
3D diffusion model infuses both image-level and feature-level context information to
generate stories with stylistic and temporal consistency.
former diffusion model x is defined as follows,
θ
L =E ∥x −x (x ,v ,v ,t ,t ,t)∥2. (3)
prior x0,ϵ,vp,vh,tp,th,t 0 θ t p h p h
Once the model learns the conditional distribution, the inference is performed
according to Eq. 4, as follows, w is the guidance scale.
xˆ (x ,v ,v ,t ,t ,t)=wx (x ,v ,v ,t ,t ,t)+(1−w)x (x ,t). (4)
θ t p h p h θ t p h p h θ t
3.3 Frame-Contextual 3D Diffusion Model
In the second stage, we propose a frame-contextual 3D diffusion model that uti-
lizes a variety of rich contextual conditions, including reference images from the
known clip, the anticipated frame semantic embedding from the unknown clip,
andtextembeddingsfromallcaptions,togenerateconsistentstories.Therefore,
our method can generate style and temporal consistency stories by jointly in-
jecting these rich contextual conditions at the image and feature levels. From
Fig.3,theframe-contextual3DdiffusionmodelcomprisesafrozenVAE,amodel
stacked with multiple U-Net blocks and frame attention (FA) blocks, a multi-
modal interaction module, and a stacked semantic module. Here, the design of
the FA block is the same as in the previous stage, and focus on the correlation
between frames to maintain consistency across frames.
Image-LevelCondition.SincetheVAE[12]enablesalmostlosslessreconstruc-
tion, introducing known clip conditions at the image level can steer the story
continuity, which has been neglected in previous studies. Specifically, similar to
the previous stage, we first divide the ground truth into known and unknown
clips and directly mask the unknown clip. The ground truth, known clip, and
masked unknown clip are all fed into the frozen VAE encoder to extract la-
tent space features. We then concatenate the latent space features of the known
clip and masked unknown clip along the width dimension, and simultaneously8 Fei Shen et al.
concatenate them with the latent space features of the ground truth along the
channeldimension.Moreover,iftheimagesintheknowncliphave0-pixelvalues
similar to the mask, it can easily mislead the model into thinking these areas
need to be generated. Therefore, we introduce a single-channel marker symbol
that matches the width and height of the concatenated features (omitted in the
figure). We use 0 and 1 to represent masked and unmasked pixels, respectively.
This approach helps to reduce model confusion and ensures accurate identifica-
tion of the generation area.
Feature-LevelCondition.Existingmethodsdependsolelyonthecurrentcap-
tion’s text embeddings, lacking in maintaining the contextual consistency of the
overall narrative. Contrarily, we incorporate the frame semantic embedding of
the unknown clip obtained from the previous stage and the text embeddings
of all captions as extra feature-level conditions. These rich contextual condi-
tions facilitate the generation of consistent narratives. Furthermore, to enhance
the features of the text and frames in both known and unknown segments, we
separately introduce a multimodal interaction module and a semantic stack-
ing module. Specifically, we first employ frozen text and image encoders (omit-
ted in the figure) to extract the text embeddings from all captions, and the
image embeddings from known clips, respectively. We further divide text em-
beddings F ∈ Rf×n×d along the temporal dimension into Fk ∈ Rfk×n×d and
t t
Fu ∈ Rfu×n×d based on known/unknown clips to align the text and image
t
modalities, where f = fk +fu. For known clip, we first feed the text embed-
dings Fk and image embeddings Fk ∈ Rfk×n×d of the known clip into a mul-
t v
timodal interaction module. The multimodal interaction module comprises two
projection layers and a multi-head cross-attention module. The two projection
layers are respectively used to project the embeddings of images and text onto
the same dimension. Then, the projected text and image embeddings are then
fedintothemulti-headcross-attentionmoduletoobtaintheinteractionfeatures
Fk ∈Rfk×n×d of the known clip.
i
Consideringthediscrepancyinthenumberoftokensanddimensionsbetween
theframesemanticembeddingandthetextembedding,weintroduceasemantic
stackingmodulespecificallyforunknownclips.Thismoduleaimstoenhancethe
feature-level semantic information of the unknown clip. The semantic stacking
moduleiscomposedofaprojectionlayerandamulti-headcross-attentionmod-
ule.Theprojectionlayer’sroleistoconvertthetextembeddingoftheunknown
clip into the same feature dimension as the image embedding of the unknown
clip, which was obtained from the previous stage. Assume that Fu ∈ Rfu×1×d
v
and Fu ∈ Rfu×n×d respectively are image embedding and projected text em-
t
bedding of the unknown clip. To align text and image modal, we first obtain
the extracted pooled representation eg ∈ Rfu×1×d of unknown clip captions.
Then, the pooled representation Fu of the unknown clip is fed into the multi-
v
head cross-attention module to obtain the interaction features of the unknown
clip. These are then stacked with the hidden representation of the unknown clip
along the length dimension to obtain the stacked features Fu ∈Rfu×n×d of the
s
unknown clip. Finally, the interaction features Fk and the stacked features Fu
i sRich-Contextual Conditional Diffusion Models 9
Table1:QuantitativecomparisonoftheproposedRCDMswithseveralSOTAmodels.
Datasets Methods FID (↓) Char-Acc (↑) Char-F1 (↑)
LDM [27] 82.53 9.17 22.68
StoryGAN [16] 74.63 16.57 39.68
Story-DALL-E [20] 26.49 55.19 73.43
FlintstonesSV [18]
Story-LDM [24] 24.24 57.19 76.59
AR-LDM [21] 19.28 62.58 79.25
RCDMs (Ours) 14.96 78.44 85.51
LDM [27] 64.52 4.31 12.74
StoryGAN [16] 49.27 9.34 18.59
CP-CSV [3] 40.56 10.03 21.78
DuCo-StoryGAN [19] 37.15 13.97 38.01
PororoSV [16]
Story-DALL-E [20] 35.90 27.14 42.45
Story-LDM [24] 26.64 29.19 47.56
AR-LDM [21] 17.40 35.18 55.29
RCDMs (Ours) 16.25 41.48 59.03
areconcatenatedalongthetemporaldimensionandfedintotheU-Netblockvia
inherent cross-attention mechanism in diffusion models.
ThelossfunctionL offrame-contextual3Ddiffusionmodelaccording
contextual
toEq.5,asfollows.Here,F denotesthefeatureoftheimage-levelconditional.
I
L =E ∥ϵ−ϵ (cid:0) x ,F ,Fk,Fu,t(cid:1) ∥2. (5)
contextual x0,ϵ,FI,F ik,F su,t θ t I i s
In the inference stage, we also use classifier-free guidance according to Eq. 6.
ϵˆ (x ,F ,Fk,Fu,t)=wϵ (x ,F ,Fk,Fu,t)+(1−w)ϵ (x ,t). (6)
θ t I i s θ t I i s θ t
4 Experiments
Datasets.WeconductexperimentsontheFlintstonesSV[18]andPororoSV[16]
datasets. The former contains 20,132 training sequences and 2,309 testing se-
quences,encompassing7maincharacters.PororoSVincludes10,191trainingse-
quences and 2,208 testing sequences, covering 9 main characters. Following [21,
24,32], for story visualization, we designated the first frame as the source frame
and generated the remaining four based on this source frame.
Metrics. We conduct a comprehensive evaluation of the model, considering
bothobjectiveandsubjectivemetrics.Objectiveindicatorsincludeclassification
accuracy of characters (Char-Acc) and F1-score of characters (Char-F1), both
extractedusingInceptionV3.Additionally,wealsoconsiderthefréchetinception
distance(FID)[8]score.Thismetricprovidesaqualityassessmentbycomparing
the distribution of feature vectors derived from both real and generated images.
Incontrast,subjectiveassessmentsprioritizeuser-orientedmetrics[32],including
thepercentageofvisualquality,text-imagerelevance,andtemporalconsistency.
Implementations.Weperformourexperimentson8NVIDIAV100GPUs.Our
configurations can be summarized as follows, (1) In the frame-prior transformer
diffusionmodel,thereare10layersofcascadedtransformerandframeattention
blocks,andthewidthofeachtransformerblockis2048.Fortheframe-contextual10 Fei Shen et al.
• • • •• • • • F B b B FF B b B Faar rr ra aa arre ee er rr rd dd dnnn nn n eee ee ei ii iyys ss sy yy y .. e oe o a ia i ssxx uuTTnn pp oo tthhdd ssll uu eeaa iiFF ddttnnii ssnn rr ee iihh eei di d ttnn dd ee aaeegg ,, llssaa kk ttttss rr aa iioooo nnee llppmm kk ggss ss ii ,,ttee nn aa ss hhtt pp ggnnhh ee eeddii ttnn ttaa ooii hhnn kkgg tt ee gg ii hhtt nnnn oo eeoo gg ll uuBB .. aacc tt aa uuaa AAss mm ggrr iidd ffnn hhtt eeeeee ssee rr.. yy rr aa aa FF .. nnttii nn hhrr dd aaee tt ttttddhh ee,, ee aabbii ss rryy aa ssssaa rr hhrr nn uuaadd ppeekk.. yy aaii nn bb llgg ee iittgg tthh ii llnnii eess ..ss hh ssee ppaa eedd aa kkaa iinn nndd gg ss ttpp oo ee FFaa rrkk eeii ddnn ..gg ttoo • • • • E L L Sod o o mo od p py ey y ow ns loi t et a w h cn e ah d r li ls ss s i hm n Le ou rfr o s ho pt ea n yac t .dh Lo e f a o a nt ohn d ped ys m hw loaii or kth r keo ssh r ahi s a te nc r thda h er c e dp h ar odee c o. s k rLe .sn o h ots e p rf ysl o e low lf o.e kr ss st ao d L .oopy.
hhttuurrTT hturT
ddnnuuoorrGG dnuorG
NNN N
AAA A
GGG G
yyyrrroootttSSS yrotS
EEE E
LLL--- L-
LLL L
AAA A
DDD D
yyyrrroootttSSS--- yrotS-
MMM M
DDD D
LLL L
RRR--- R-
AAA A
MMM M
DDD D
LLL L
yyyrrroootttSSS--- yrotS-
sssrrruuuOOO sruO
Fig.4: Qualitative comparisons with several state-of-the-art models on the Flint-
stonesSV and PororoSV datasets. Please see Appendix C for more examples.
3D diffusion model, we use the pretrained Stable Diffusion V1.5 1 and modify
the first convolution layer to adapt additional conditions. (2) We employ the
AdamW optimizer with a fixed learning rate of 1e−5 in all stages. (3) Following
[21,24], we train our models using images of sizes 512 × 512 for FlintstonesSV
andPororoSVdataset.(4)Weemployadataaugmentationstrategyofdropping
imagesinalltwostages,withthedropcountrangingfrom0to5.Wesubstitute
the dropped images with black images. (5) In the inference stage, we use the
DDIM [9] sampler with 20 steps and set the guidance scale w to 2.0 for RCDMs
on all stages. Please refer to Appendix B for more detail.
4.1 Quantitative and Qualitative Results
We quantitatively compare our proposed RCDMs with several state-of-the-art
methods, including LDM [27], StoryGAN [16], Story-LDM [24], Story-DALL-
E [20], AR-LDM [21], CP-CSV [3], and DuCo-StoryGAN [19].
Quantitative Results. As shown in Table 1, firstly, since LDM [27] generates
each image based solely on individual captions, it performs significantly worse
thanallothermethodsonthreemetrics.Secondly,comparedtoGANanddiffu-
sion model methods, our approach outperforms other models on all three met-
rics in FlintstonesSV. For example, compared to StoryGAN [16], which employs
1 https://huggingface.co/runwayml/stable-diffusion-v1-5Rich-Contextual Conditional Diffusion Models 11
90
)80 Visual Quality 76.855
%70 67.635
(
e
g56 00 T Ce ox nt s- iI sm tea ng ce
y
Relevance
51.570
a
t40
n
e30
c
r e P12 00 7.7705.070 4.50012.950 4.240 9.85016.055 11.205 10.24514.355 6.250
0 1.450
Story-GAN Story-DALL-E AR-LDM Story-LDM Ours
Different SOTA Methods
Fig.5: Results of user study. Higher values indicate better performance.
story dynamic tracking, RCDMs score 61.87% and 45.83% higher on Char-Acc
andChar-F1metrics,respectively.Thisdemonstratesthesuperiorityofproposed
RCDMs in understanding story details through all captions and then generat-
ing all story images at once, as opposed to StoryGAN, which uses an autore-
gressive approach to understand captions frame by frame. Lastly, compared to
AR-LDM[21],whichalsoreliesonadiffusionmodel,RCDMsperformbetteron
theFIDmetric.EventhoughAR-LDMalreadyscoreswell,RCDMsshowbetter
performance, indicating that injecting more semantic information through the
first-stage model can enrich the generation of image details. Moreover, on Char-
Acc and Char-F1 metrics, RCDMs significantly outperform AR-LDM. This is
because we not only introduce more semantic information at the feature level
but also inject more contextual information at the image level.
The comparison results for PororoSV are summarized in Table 1. Notably,
consistent with the trend presented in the FlintstonesSV dataset, proposed
RCDMs outperform all SOTA methods, achieving the best FID, Char-Acc,
and Char-F1. Specifically, compared to the best-performing GAN method, i.e.,
DuCo-StoryGAN [19], which enhances the current caption to improve text se-
mantic understanding, RCDMs inject context by understanding the complete
story semantics. Similarly, compared to AR-LADM [21], we surpass it by 6.30%
and 3.74% on Char-Acc and Char-F1, respectively. These results indicate that
the simultaneous use of a frame-prior transformer diffusion model to obtain
framesemanticinformationoftheunknownclipandtheinjectionofimage-level
conditions are crucial for understanding and generating stories.
Qualitative Results. As some methods have yet to be open-sourced, we qual-
itatively compared RCDMs with StoryGAN [16], Story-DALL-E [20], AR-LDM
[21],andStory-LDM[24]ontheFlintstonesSVandPororoSVdatasets.Asshown
in Fig. 4, several conclusions can be drawn from the results: (1) RCDMs signifi-
cantly outperform other SOTA methods regarding image quality. For example,
on the FlintstonesSV dataset, Story-DALL-E, AR-LDM, and the second frame
of Story-LDM, and the third frame of StoryGAN all exhibit scenes and charac-
ter limbs that do not match. (2) Regardless of whether it is the FlintstonesSV
dataset or the PororoSV dataset, our proposed RCDMs perform best regard-
ing character consistency. For example, StoryGAN performs poorly in terms of
character consistency on text-image pairs, such as the second and third frames
on FlintstonesSV dataset, and the first and fourth frames on PororoSV dataset.12 Fei Shen et al.
Table 2: Ablation study on FlintstonesSV dataset. Here, IC stands for image-level
conditional. MIM and SSM, respectively, denote the multimodal interaction module
and semantic stacking module.
Components FlintstonesSV
Settings Stage2
Stage1 FID (↓)Char-Acc (↑)Char-F1 (↑)
ICMIMSSM
LDM [27] - - - - 82.53 9.17 22.68
B0 ✓ ✗ ✗ ✗ 21.81 56.44 70.32
B1 ✓ ✓ ✗ ✗ 19.46 61.88 78.03
B2 ✓ ✓ ✓ ✗ 18.36 72.53 82.06
B3 ✓ ✓ ✗ ✓ 17.94 73.58 82.87
B4 ✗ ✓ ✓ ✓ 16.51 75.73 83.96
Ours ✓ ✓ ✓ ✓ 14.96 78.44 85.51
A similar situation also occurs on Story-DALL-E. (3) Only our method can
generate story images that reasonably align with the text on complex micro-
actions and expressions. For example, the text prompt for the fourth frame on
the FlintstonesSV dataset is ’laughs and tears up a little,’ and the third frame
on the PororoSV dataset is ’look sad’. This can be attributed to our method’s
ability to enhance the semantic consistency of image-text pairs. (4) Regarding
visualconsistency,althoughStory-LDMintegratesanattention-memorymodule
tohandlecontext,itcannotproduceaconsistentstylesceneinacompletestory.
In contrast, the results of RCDMs have pleasing visual effects and the ability
to maintain temporal consistency, primarily due to RCDMs injecting rich con-
textual conditions, including reference images of the known clip, the predicted
framesemanticembeddingoftheunknownclip,andtextembeddingsofallcap-
tions at both the feature and image levels. In summary, our method can always
produce more realistic and consistent story images, proving that RCDMs bring
significant advantages by introducing rich context conditions.
User Study.Theabovequantitativeandqualitativecomparisonresultsdemon-
strate the substantial advantages of our proposed RCDMs in generating results.
However,thetaskofsynthesizingstoryvisualizationis oftenhumanperception-
oriented. Therefore, we also conduct a user study involving 20 volunteers with
computervisionbackgrounds.Thevolunteersareaskedtochoosewhichmethod
is better regarding visual quality, text-image relevance, and style/temporal con-
sistency in the generated story. Please refer to Appendix C for more detail.
As shown in Fig. 5, the higher the score in the three indicators of this
study, the better the performance. RCDMs offer commendable performance on
allthreefundamentalindicators.Forexample,thetext-imagerelevanceandcon-
sistencyscoresofRCDMsare51.57%and76.855%,respectively,whicharenearly
35.515% and 65.65% higher than the second-best model. This demonstrates the
significant advantages of RCDMs in multimodal semantic understanding and
consistency due to our conditional injection at both the image and feature lev-
els. In addition, our visual quality score is 67.635%, indicating that participants
preferourmethod,demonstratingbetterstoryvisualizationquality.Moredetail
refer to appendix B.Rich-Contextual Conditional Diffusion Models 13
• In the evening, Fred and Barney are standing at the doorway, 18
First Branch • • l T Lo h ao tek eyi rn ,a g tr hein e t ys ai ld sk ie ti. n ag r oin u nth de t hli ev i tn ag b lr eo , o dm is. cussing something. 16.9
• Fred is driving with Barney.
16
thF er ce hd a i is r l sy li en eg p o inn g . 14.6
14
12.7
12
Source Image
10
• I lon o t kh ie n ge v ae tn ei an cg h, F or thed er a . nd Barney are standing outside the door, AR-LDM Story-LDM RCDMs
Second Branch • They are taking a walk outdoors.
• Later, they stop and look into the distance. Different SOTA Methods
• Fred is driving with Barney.
Fig.6: Branchingstoryline.Generating Fig.7:Inferencespeedcomparisonwith
consistent stories by different captions. different SOTA methods.
4.2 Ablation Study
We further devise several variants to demonstrate the efficacy of each module
proposed in this study. All these variants belong to the RCDMs framework but
encompass different configurations. B0 denotes the sole use of the frame-prior
transformer diffusion model, devoid of image-level conditions, with feature-level
conditions directly infused into the inherent cross-attention of SD using caption
features. B1 built on the foundation of B0, incorporates image-level conditions.
B2, an extension of B1, additionally employs a multimodal interaction module.
B3 also building upon B1, further utilizes a semantic stacking module. B4 in-
dicates that there is no stage1 prior model and in the SSM without stage1, the
QKV of attention all originate from the features of the known clips.
As shown in Table 2, firstly, compared to LDM [27], B0, which adopts the
frame attention module, can significantly improve the consistency of charac-
ters and backgrounds. Subsequently, when the image-level condition setting is
added,B1is5.44%and7.71%higherthanB0onChar-AccandChar-F1,respec-
tively. This demonstrates that the injection of known clip images can enhance
the model’s contextual information. Secondly, when the MIM and SSM are in-
troduced based on B1, B2 and B3 are 10.65% and 11.70% higher than B1 on
Char-Acc, respectively. At this point, B2 and B3 have achieved highly competi-
tive performance on the FlintstonesSV dataset. These results indicate that they
also contribute constructively to the success of our RCDMs. Finally, when the
frame semantic embeddings of the unknown clip predicted by the frame-prior
transformerdiffusionmodelareadded,FID,Char-Acc,andChar-F1allimprove
better, especially FID. This shows that the frame-prior model can better help
RCDMs generate stories with semantic and temporal consistency.
4.3 Additional Results
BranchingStoryline.Weaddanexperimenttovalidatethediversityingener-
atingbranchingnarratives.Specifically,weusedonereferenceimageandtwosets
ofdistinctcaptionsasthestorydescriptions.Wealsoincorporatedpronounslike
“they"tocheckifthestorycouldmaintaincoherence.FromFig.6,wegenerated
)s(
deepS14 Fei Shen et al.
• Dino looks sad at first, then laughs in the room. • Wilma is in the room. She has her head turned as she speaks.
• Wilma stands in the kitchen, searching the open fridge before her. • Wilma approaches, looking disgruntled in the blue room.
• Fred is sitting at the table in the kitchen. • Fred and Wilma are standing in a blue room arguing.
• He is talking while her cuts his meat. • Fred is upset, and Wilma is displeased.
Fig.8: Qualitative results of RCDMs for caption-only story generation.
two sets of different story images, and the results demonstrate consistency in
styleandsequence.Moreover,weobservedthatforthereference’they’,RCDMs
can generate these two characters based on the previously parsed storylines.
InferenceSpeed.Wealsoconductacomparisonexperimentoninferencespeed,
including AR-LDM [21] and Story-LDM [24], which are also based on the diffu-
sion model. All experiments are conducted on the same V100 GPU to ensure a
fair comparison. From Fig. 7, the mean inference time for a story by AR-LDM
and Story-LDM is 14.6 seconds and 16.9 seconds, respectively, while proposed
RCDMs only take 12.7 seconds, even though it’s a two-stage model. Since AR-
LDM uses a more heavy multimodal model, and Story-LDM employs an atten-
tionmodulewithmemorystorage,bothofwhichslowdowntheinferencespeed.
Notably, while AR-LDM and Story-LDM generate frames one by one using an
autoregressive architecture, RCDMs infers all story images in a single forward
pass. These results demonstrate the architectural superiority of RCDMs.
Caption-Only Generation. RCDMs also support caption-only generation, as
it adopts a strategy of randomly dropping images during the training process.
In contrast, other SOTA methods typically require an additional model to be
trained to accommodate this scenario. Fig. 8 displays the results generated by
RCDMsusingonlycaptionsasguidance.Duetothelackofopen-sourceweights
from SOTA methods for comparison, we can only assess RCDMs’ performance
independently. The results suggest that RCDMs can generate story images that
maintain consistency in style and sequence under different scenarios/characters.
5 Conclusion
This paper presents Rich-contextual Conditional Diffusion Models (RCDMs)
to enhance style and temporal consistency at both the image and feature levels
for story visualization. In the first stage, the frame-prior transformer diffusion
model is presented to predict the frame semantic embedding of the unknown
clip by aligning the semantic correlations between the captions and frames of
theknownclip.Thesecondstageestablishesarobustmodelwithrichcontextual
conditions,includingreferenceimagesoftheknownclip,thepredictedframese-
manticembeddingoftheunknownclip,andtextembeddingsofallcaptions.By
jointlyinjectingtheserichcontextualconditions,RCDMscangeneratestyleand
temporal consistency stories. Both qualitative and quantitative results demon-
strate that RCDMs perform well in challenging scenarios.
Limitations. Current methods, including RCDMs, typically achieve consistent
story generation on a closed-set dataset, which limits the variety of charactersRich-Contextual Conditional Diffusion Models 15
and scenes. For future work, we wil explore methods with open-set generation
capabilities to allow for a broader range of characters and scenes.
6 Ethics and Broader Impacts
Thispaperpresentsasynthesismethodthatcreatesnovelcomicnarrativesfrom
captions and reference images. While there’s potential for misuse in fabricating
misleadingcontent,thisriskiscommontoallstorygenerationtechniques.How-
ever, the cartoon nature of our outputs significantly reduces ethical concerns
related to image and video generation, as they can’t be mistaken for reality.
Moreover, strides have been made in research to detect and prevent malicious
tampering.Ourworkaidsthisdomain,balancingthetechnology’svalueandthe
risks of unrestricted access, ensuring the safe and beneficial use of RCDMs.
References
1. Ahn, D., Kim, D., Song, G., Kim, S.H., Lee, H., Kang, D., Choi, J.: Story visual-
ization by online text augmentation with context memory. In: Proceedings of the
IEEE/CVF International Conference on Computer Vision. pp. 3125–3135 (2023)
4, 5
2. Blattmann,A.,Rombach,R.,Ling,H.,Dockhorn,T.,Kim,S.W.,Fidler,S.,Kreis,
K.:Alignyourlatents:High-resolutionvideosynthesiswithlatentdiffusionmodels.
In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition. pp. 22563–22575 (2023) 6
3. Chen,H.,Han,R.,Wu,T.L.,Nakayama,H.,Peng,N.:Character-centricstoryvisu-
alizationviavisualplanningandtokenalignment.arXivpreprintarXiv:2210.08465
(2022) 2, 4, 9, 10
4. Creswell, A., White, T., Dumoulin, V., Arulkumaran, K., Sengupta, B., Bharath,
A.A.:Generativeadversarialnetworks:Anoverview.IEEEsignalprocessingmag-
azine 35(1), 53–65 (2018) 1
5. Ding, X., Wang, Y., Xu, Z., Welch, W.J., Wang, Z.J.: Ccgan: Continuous con-
ditional generative adversarial networks for image generation. In: International
conference on learning representations (2020) 1
6. Ge,S.,Park,T.,Zhu,J.Y.,Huang,J.B.:Expressivetext-to-imagegenerationwith
richtext.In:ProceedingsoftheIEEE/CVFInternationalConferenceonComputer
Vision. pp. 7545–7556 (2023) 4
7. Guo, Y., Yang, C., Rao, A., Wang, Y., Qiao, Y., Lin, D., Dai, B.: Animatediff:
Animateyourpersonalizedtext-to-imagediffusionmodelswithoutspecifictuning.
arXiv preprint arXiv:2307.04725 (2023) 6
8. Heusel,M.,Ramsauer,H.,Unterthiner,T.,Nessler,B.,Hochreiter,S.:Ganstrained
byatwotime-scaleupdateruleconvergetoalocalnashequilibrium.Advancesin
neural information processing systems 30 (2017) 9
9. Ho, J., Jain, A., Abbeel, P.: Denoising diffusion probabilistic models. Advances in
neural information processing systems 33, 6840–6851 (2020) 4, 10
10. Ho, J., Salimans, T.: Classifier-free diffusion guidance. arXiv preprint
arXiv:2207.12598 (2022) 516 Fei Shen et al.
11. Hong, F.T., Zhang, L., Shen, L., Xu, D.: Depth-aware generative adversarial net-
workfortalkingheadvideogeneration.In:ProceedingsoftheIEEE/CVFconfer-
ence on computer vision and pattern recognition. pp. 3397–3406 (2022) 1
12. Kingma, D.P., Welling, M., et al.: An introduction to variational autoencoders.
Foundations and Trends® in Machine Learning 12(4), 307–392 (2019) 7
13. Lee, D., Kim, C., Kim, S., Cho, M., Han, W.S.: Autoregressive image genera-
tionusingresidualquantization.In:ProceedingsoftheIEEE/CVFConferenceon
Computer Vision and Pattern Recognition. pp. 11523–11532 (2022) 3
14. Li, B.: Word-level fine-grained story visualization. In: European Conference on
Computer Vision. pp. 347–362. Springer (2022) 2, 4
15. Li, Y., Wang, H., Jin, Q., Hu, J., Chemerys, P., Fu, Y., Wang, Y., Tulyakov, S.,
Ren, J.: Snapfusion: Text-to-image diffusion model on mobile devices within two
seconds. Advances in Neural Information Processing Systems 36 (2024) 1
16. Li,Y.,Gan,Z.,Shen,Y.,Liu,J.,Cheng,Y.,Wu,Y.,Carin,L.,Carlson,D.,Gao,
J.: Storygan: A sequential conditional gan for story visualization. In: Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp.
6329–6338 (2019) 1, 2, 4, 5, 9, 10, 11
17. Liu, M.Y., Huang, X., Yu, J., Wang, T.C., Mallya, A.: Generative adversarial
networksforimageandvideosynthesis:Algorithmsandapplications.Proceedings
of the IEEE 109(5), 839–862 (2021) 2
18. Maharana, A., Bansal, M.: Integrating visuospatial, linguistic and commonsense
structure into story visualization. arXiv preprint arXiv:2110.10834 (2021) 2, 4, 9
19. Maharana, A., Hannan, D., Bansal, M.: Improving generation and evaluation of
visual stories via semantic consistency. arXiv preprint arXiv:2105.10026 (2021) 2,
4, 9, 10, 11
20. Maharana, A., Hannan, D., Bansal, M.: Storydall-e: Adapting pretrained text-to-
imagetransformersforstorycontinuation.In:EuropeanConferenceonComputer
Vision. pp. 70–87. Springer (2022) 2, 3, 4, 5, 9, 10, 11
21. Pan, X., Qin, P., Li, Y., Xue, H., Chen, W.: Synthesizing coherent story with
auto-regressive latent diffusion models. In: Proceedings of the IEEE/CVF Winter
Conference on Applications of Computer Vision. pp. 2920–2930 (2024) 1, 2, 3, 4,
5, 9, 10, 11, 14
22. Qiao, T., Zhang, J., Xu, D., Tao, D.: Mirrorgan: Learning text-to-image genera-
tionbyredescription.In:ProceedingsoftheIEEE/CVFConferenceonComputer
Vision and Pattern Recognition. pp. 1505–1514 (2019) 1, 3, 4
23. Radford,A.,Kim,J.W.,Hallacy,C.,Ramesh,A.,Goh,G.,Agarwal,S.,Sastry,G.,
Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from
naturallanguagesupervision.In:Internationalconferenceonmachinelearning.pp.
8748–8763. PMLR (2021) 6
24. Rahman,T.,Lee,H.Y.,Ren,J.,Tulyakov,S.,Mahajan,S.,Sigal,L.:Make-a-story:
Visual memory conditioned consistent story generation. In: Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 2493–
2502 (2023) 1, 2, 5, 9, 10, 11, 14
25. Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., Chen, M.: Hierarchical text-
conditional image generation with clip latents. arXiv preprint arXiv:2204.06125
1(2), 3 (2022) 1, 4, 6
26. Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M.,
Sutskever, I.: Zero-shot text-to-image generation. In: International Conference on
Machine Learning. pp. 8821–8831. PMLR (2021) 1Rich-Contextual Conditional Diffusion Models 17
27. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution
image synthesis with latent diffusion models. In: Proceedings of the IEEE/CVF
conferenceoncomputervisionandpatternrecognition.pp.10684–10695(2022) 2,
9, 10, 12, 13
28. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution
image synthesis with latent diffusion models. In: Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition. pp. 10684–10695 (2022) 4
29. Saharia,C.,Chan,W.,Saxena,S.,Li,L.,Whang,J.,Denton,E.L.,Ghasemipour,
K., Gontijo Lopes, R., Karagol Ayan, B., Salimans, T., et al.: Photorealistic text-
to-image diffusion models with deep language understanding. Advances in Neural
Information Processing Systems 35, 36479–36494 (2022) 1, 4
30. Saharia,C.,Chan,W.,Saxena,S.,Li,L.,Whang,J.,Denton,E.L.,Ghasemipour,
K., Gontijo Lopes, R., Karagol Ayan, B., Salimans, T., et al.: Photorealistic text-
to-image diffusion models with deep language understanding. Advances in Neural
Information Processing Systems 35, 36479–36494 (2022) 2
31. Shen, F., Ye, H., Zhang, J., Wang, C., Han, X., Wei, Y.: Advancing pose-guided
image synthesis with progressive conditional diffusion models. In: The Twelfth
International Conference on Learning Representations (2023) 1
32. Shen, X., Elhoseiny, M.: Large language models as consistent story visualizers.
arXiv preprint arXiv:2312.02252 (2023) 2, 4, 5, 9
33. Song,Y.,Sohl-Dickstein,J.,Kingma,D.P.,Kumar,A.,Ermon,S.,Poole,B.:Score-
basedgenerativemodelingthroughstochasticdifferentialequations.arXivpreprint
arXiv:2011.13456 (2020) 4
34. Tao, M., Tang, H., Wu, F., Jing, X.Y., Bao, B.K., Xu, C.: Df-gan: A simple and
effective baseline for text-to-image synthesis. In: Proceedings of the IEEE/CVF
ConferenceonComputerVisionandPatternRecognition.pp.16515–16525(2022)
3
35. Wang, C., Tian, K., Guan, Y., Zhang, J., Jiang, Z., Shen, F., Han, X., Gu, Q.,
Yang, W.: Ensembling diffusion models via adaptive feature aggregation. arXiv
preprint arXiv:2405.17082 (2024) 4
36. Wang,C.,Tian,K.,Zhang,J.,Guan,Y.,Luo,F.,Shen,F.,Jiang,Z.,Gu,Q.,Han,
X., Yang, W.: V-express: Conditional dropout for progressive training of portrait
video generation. arXiv preprint arXiv:2406.02511 (2024) 4
37. Wu, Q., Liu, Y., Zhao, H., Bui, T., Lin, Z., Zhang, Y., Chang, S.: Harnessing the
spatial-temporal attention of diffusion models for high-fidelity text-to-image syn-
thesis. In: Proceedings of the IEEE/CVF International Conference on Computer
Vision. pp. 7766–7776 (2023) 4
38. Xiao, Y., Wu, L., Guo, J., Li, J., Zhang, M., Qin, T., Liu, T.y.: A survey on
non-autoregressive generation for neural machine translation and beyond. IEEE
Transactions on Pattern Analysis and Machine Intelligence (2023) 3
39. Xu, T., Zhang, P., Huang, Q., Zhang, H., Gan, Z., Huang, X., He, X.: Attngan:
Fine-grainedtexttoimagegenerationwithattentionalgenerativeadversarialnet-
works. In: Proceedings of the IEEE conference on computer vision and pattern
recognition. pp. 1316–1324 (2018) 1, 3, 4
40. Yang, Z., Wang, J., Gan, Z., Li, L., Lin, K., Wu, C., Duan, N., Liu, Z., Liu, C.,
Zeng,M.,etal.:Reco:Region-controlledtext-to-imagegeneration.In:Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp.
14246–14255 (2023) 1
41. Ye, H., Zhang, J., Liu, S., Han, X., Yang, W.: Ip-adapter: Text compati-
ble image prompt adapter for text-to-image diffusion models. arXiv preprint
arXiv:2308.06721 (2023) 118 Fei Shen et al.
42. Zhang,H.,Goodfellow,I.,Metaxas,D.,Odena,A.:Self-attentiongenerativeadver-
sarial networks. In: International conference on machine learning. pp. 7354–7363.
PMLR (2019) 1
43. Zhang, H., Koh, J.Y., Baldridge, J., Lee, H., Yang, Y.: Cross-modal contrastive
learningfortext-to-imagegeneration.In:ProceedingsoftheIEEE/CVFconference
on computer vision and pattern recognition. pp. 833–842 (2021) 1, 3, 4
44. Zhang,H.,Xu,T.,Li,H.,Zhang,S.,Wang,X.,Huang,X.,Metaxas,D.N.:Stack-
gan: Text to photo-realistic image synthesis with stacked generative adversarial
networks. In: Proceedings of the IEEE international conference on computer vi-
sion. pp. 5907–5915 (2017) 3, 4
45. Zhang, L., Rao, A., Agrawala, M.: Adding conditional control to text-to-image
diffusion models. In: Proceedings of the IEEE/CVF International Conference on
Computer Vision. pp. 3836–3847 (2023) 1
46. Zhang, Z., Han, L., Ghosh, A., Metaxas, D.N., Ren, J.: Sine: Single image editing
withtext-to-imagediffusionmodels.In:ProceedingsoftheIEEE/CVFConference
on Computer Vision and Pattern Recognition. pp. 6027–6037 (2023) 1Rich-Contextual Conditional Diffusion Models 19
Supplementary Material
This supplementary material offers a more detailed exploration of the exper-
iments and methodologies proposed in the main paper. Section A provides a
series of symbols and definitions for enhanced comprehension. Section B delves
deeper into the implementation specifics of our experiments. Section C presents
additional experimental outcomes, including a broader range of supplemented
results from the RCDMs method, qualitative comparison examples with state-
of-the-art methods, and a detailed explanation of our user studies.
A Some Notations and Definitions
Table 3: Some notations and definitions.
Notation Definition
x Real image
0
c Additional condition
t Timestep
θ Diffusion model
ϵ Gaussian noise
w Guidance scale
v Pooled visual representation of the frame
p
v Hidden visual representation of the frame
h
t Pooled textual representation of the caption
p
t Hidden textual representation of the caption
h
F Feature of the image-level conditional
I
Fk Interaction feature of the known clip
i
Fu Stacking feature of the unknown clip
s
B Implement Details
Our experiments are conducted on 8 NVIDIA V100 GPUs. We follow the stan-
dard training strategies and hyperparameters of diffusion models. We utilize
the AdamW optimizer with a consistent learning rate of 1e−5 across all stages.
The probability of random dropout for condition c is set at 10%. We employ
OpenCLIP ViT-bigG/14 2 as the image and text encoder in all stages. For the
frame-prior transformer diffusion model, it consists of 10 transformer blocks,
each with a width of 2,048. The model is trained for 100k iterations with a
batch size of 8, using a cosine noising schedule with 1000 timesteps. We mod-
ify the first convolution layer for the frame-contextual 3D diffusion model to
accommodate additional conditions. The model is trained for 500k iterations,
each with a batch size of 8, and a linear noise schedule with 1000 timesteps is
2 https://huggingface.co/laion/CLIP-ViT-bigG-14-laion2B-39B-b160k20 Fei Shen et al.
applied. In the inference stage, we use the DDIM sampler with 20 steps and set
the guidance scale w to 2.0 for PCDMs in all stages.
Dataset. There are 7 main characters in PororoSV: Fred, Barney, Wilma,
Betty, Pebbles, Dino, and Slate. Profile pictures of them are given in Fig. 9.
There are 9 main characters in PororoSV: Pororo, Loopy, Eddy, Harry, Poby,
Tongtong,Crong,Rody,andPetty.ProfilepicturesofthemaregiveninFig.10.
User Study.Detailsoftheuserstudyandneedmorevolunteers.Wecarried
out a user study using 200 randomly selected sets of stories derived from the
FlintstonesSVandPororoSVdatasets.Thesestorieswereassessedacrossvarious
dimensions by 100 volunteers.
Fred Barney Wilma Betty Pebbles Dino Slate
Fig.9: MaincharacternamesandcorrespondingimagesinFlintstonesSV.Theimages
are from https://flintstones.fandom.com/.
Pororo Loopy Eddy Harry Poby
Tongtong Crong Rody Petty
Fig.10: Main character names and corresponding images in PororoSV. The images
are from https://pororo.fandom.com/.Rich-Contextual Conditional Diffusion Models 21
C Additional Results
We provide additional examples for RCDMs in Fig. 11, Fig. 12, and Fig. 13.
We show additional examples for comparison with the state-of-the-art (SOTA)
methodsinFig.14and Fig.15.Weprovidetheuserstudyinterfaceasshownin
Fig. 16.
• Fred and Wilma are sitting in the couch in the living room. Wilma speaks as
Fred listens to her.
• Fred is sitting on the couch with his arm around Wilma while she is nodding her
head. They are in the living room.
• Fred and Wilma are sitting on a couch in the living room. Fred is talking to
Wilma as she leans into him crying.
• Fred and Wilma are talking in the room, until it looks like the get
interrupted.
•• BBaarrnneeyy iiss ssiittttiinngg iinn tthhee ccaarr oouuttssiiddee aass hhee ttaallkkss ttoo MMrr.. SSllaattee..
•• AA ccaarr ddrriivveerr iiss iinn aa ccaarr.. HHee ttaallkkss wwhhiillee wweeaarriinngg ggllaasssseess..
•• MMrr ssllaattee iiss ddrriivviinngg hhiiss ccaarr aanndd llaauugghhiinngg..
•• AA ppoolliiccee ooffffiicceerr iinn aa ppoolliiccee ssttaattiioonn ssiittss aatt aa ddeesskk aanndd ttaallkkss iinnttoo aa ssppeeaakkeerr
wwhhiillee llooookkiinngg aatt aa ssttaacckk ooff ppaappeerrss..
•• FFrreedd iiss iinn aa rroooomm.. HHee hhoollddss hhiiss bbeellllyy bbeeffoorree iitt bboouunncceess iittsseellff oonn tthhee fflloooorr..
•• FFrreedd ssppeeaakkss ttoo WWiillmmaa aass tthheeyy wwaallkk aalloonngg tthhee rrooaadd.. WWiillmmaa ttuurrnnss hheerr hheeaadd bbaacckk ttoo
llooookk aatt FFrreedd aanndd ttuurrnnss hheerr hheeaadd ttoowwaarrddss tthhee rrooaadd aaggaaiinn bbeeffoorree FFrreedd rraaiisseess hhiiss
hhaanndd iinn aa wwaavviinngg mmoottiioonn..
•• FFrreedd aanndd WWiillmmaa wwaallkkss tthhrroouugghh aa rroooomm wwhhiillee FFrreedd ttaallkkss ttoo hheerr ppaattttiinngg hheerr bbaacckk..
•• FFrreedd aanndd WWiillmmaa aarree iinn aa rroooomm.. FFrreedd ttaallkkss ttoo WWiillmmaa..
Fig.11: More qualitative results of RCDMs on the FlintstonesSV dataset.22 Fei Shen et al.
•• WWiillmmaa aanndd BBeettttyy aarree ssiittttiinngg oonn aa ccoouucchh iinn tthhee lliivviinngg rroooomm.. WWiillmmaa iiss ggeessttuurriinngg
aanndd ssppeeaakkiinngg ttoo BBeettttyy.. TThheenn BBeettttyy ssppeeaakkss ttoo WWiillmmaa..
•• BBeettttyy iiss ssiittttiinngg oonn aa ccoouucchh iinn tthhee lliivviinngg rroooomm ttaallkkiinngg..
•• WWiillmmaa aanndd BBeettttyy aarree ssttaannddiinngg oouuttssiiddee aa hhoouussee bbyy aa ddoooorrwwaayy rreeaaddiinngg aa ssiiggnn tthhaatt
ssaayyss rroooomm ffoorr rreenntt..
•• WWiillmmaa aanndd BBeettttyy ssttaanndd ttaallkkiinngg iinn tthhee ddoooorrwwaayy..
•• BBaarrnneeyy iiss ssttaannddiinngg iinn tthhee rroooomm wwiitthh ffaannccyy hhaaiirr,, hhoollddiinngg aa ffiisshh bboonnee aanndd ttaallkkiinngg
ttoo ssoommeeoonnee ooffff ssccrreeeenn rriigghhtt..
•• FFrreedd iiss ttaallkkiinngg ttoo ssoommeeoonnee iinn aa rroooomm..
•• BBaarrnneeyy iiss ccoommbbiinngg hhiiss hhaaiirr wwiitthh aa ffiisshh bboonnee iinn tthhee lliivviinngg rroooomm..
•• TThheenn FFrreedd ttaallkkss sstteerrnnllyy ttoo BBaarrnneeyy..
• Wilma talks in the room while holding her hand up.
• Wilma is in the room, she is talking.
• Wilma is in the dining room talking to someone then she starts to laugh.
• Barney slides towards doorway, and opens door.
Fig.12: More qualitative results of RCDMs on the FlintstonesSV dataset.Rich-Contextual Conditional Diffusion Models 23
• Pororo is delighted with the present and Pororo thanks to Loopy.
• Loopy doesn't come in. Loopy says that Loopy is going to leave.
• Pororo sees Loopy out. Pororo says bye to her.
• Pororo looks around to check if there's anyone seeing him.
• Harry starts singing. Harry looks very happy.
• Pororo and Crong looks tired. it's snowing outside.
• Harry is explaining about his performances.
• Pororo is calling Harry.
Fig.13: More qualitative results of RCDMs on the PororoSV dataset.24 Fei Shen et al.
• Wilma yells at Fred while lying on the couch in the living room.
• Fred is in the room and puts on sunglasses.
• Dino is in the living room. He is wagging his tail while laying on
a bone when Fred walks by.
• Wilma sits at the table in a room. She is talking.
h
tu
r
T
d
n
u
o
r
G
N
A
G
y
r
o
tS
E
L-
L
A
D
y-
r
o
tS
M
D
L
R-
A
M
D
L
y-
r
o
tS
s
r
u
O
Fig.14: More qualitative comparisons between RCDMs and SOTA methods.Rich-Contextual Conditional Diffusion Models 25
• Fred is outside. Stars appear in Fred's eyes.
• Wilma is standing in the kitchen talking to someone then she stirs
food in a bowl.
• Wilma is standing in her living room with a calm motherly face
sharing words with someone.
• Fred is standing in the doorway. He is talking and moving his
hands.
h
tu
r
T
d
n
u
o
r
G
N
A
G
y
r
o
tS
E
L-
L
A
D
y-
r
o
tS
M
D
L
R-
A
M
D
L
y-
r
o
tS
s
r
u
O
Fig.15: More qualitative comparisons between RCDMs and SOTA methods.26 Fei Shen et al.
Please answer the following questions https://tp.wjx.top/vm/Oi8VFyq.aspx?code=C2DC24F62B338DD8C3B01D77FADD9D39&state=sojump
Please answer the following questions
Which set of stories has the highest visual quality？
Model 1
Model 2
Model 3
Model 4
Model 5
Which set of stories demonstrates the strongest correlation between images and text?
Model 1
Model 2
Model 3
Model 4
Model 5
Which set of stories exhibits the highest consistency in terms of temporal and style?
Model 1
Model 2
Model 3
Model 4
Model 5
Fig.16: An example question used in our user study for story visualization.
第1页 共1页 2024/3/14 15:07