SCALABLE MULTI-OUTPUT GAUSSIAN PROCESSES WITH
STOCHASTIC VARIATIONAL INFERENCE
APREPRINT
XiaoyuJiang SokratiaGeorgaka
TheUniversityofManchester TheUniversityofManchester
jiangxiaoyu9907@outlook.com sokratia.georgaka@manchester.ac.uk
MagnusRattray
TheUniversityofManchester
Magnus.Rattray@manchester.ac.uk
MauricioA.Álvarez
TheUniversityofManchester
mauricio.alvarezlopez@manchester.ac.uk
July3,2024
ABSTRACT
The Multi-Output Gaussian Process (MOGP) is a popular tool for modelling data from multiple
sources. A typical choice to build a covariance function for a MOGP is the Linear Model of
Coregionalization(LMC)whichparametricallymodelsthecovariancebetweenoutputs. TheLatent
Variable MOGP (LV-MOGP) generalises this idea by modelling the covariance between outputs
usingakernelappliedtolatentvariables,oneperoutput,leadingtoaflexibleMOGPmodelthat
allowsefficientgeneralizationtonewoutputswithfewdatapoints. Computationalcomplexityin
LV-MOGPgrowslinearlywiththenumberofoutputs,whichmakesitunsuitableforproblemswith
a large number of outputs. In this paper, we propose a stochastic variational inference approach
for the LV-MOGP that allows mini-batches for both inputs and outputs, making computational
complexitypertrainingiterationindependentofthenumberofoutputs. Wedemonstratethemodel’s
performancebybenchmarkingagainstsomeotherMOGPmodelsonseveralreal-worlddatasets,
includingspatial-temporalclimatemodellingandspatialtranscriptomics.
Keywords Multi-OutputGaussianProcess·StochasticVariationalInference·LatentVariableModels
1 Introduction
GaussianProcesses(GP)haveestablishedthemselvesasapowerfulandflexibletoolformodellingnonlinearfunctions
within a Bayesian non-parametric framework [Williams and Rasmussen, 2006]. Multi-output Gaussian processes
(MOGP)generalisethispowerfulframeworktothevector-valuedrandomfield[Alvarezetal.,2012]bycapturing
correlationsnotonlyacrossdifferentinputsbutalsoacrossdifferentoutputfunctions. Thischaracteristichasbeen
experimentallyshowntoprovidebetterpredictionsinfieldssuchasgeostatistics[Wackernagel,2003],heterogeneous
regression[Moreno-Muñozetal.,2018],andthemodellingofaggregated[Yousefietal.,2019]andhierarchicaldatasets
[Maetal.,2023].
TheprimaryfocusintheliteratureonMOGPhasbeenondevelopinganappropriatecross-covariancefunctionbetween
themultipleoutputs. Twoclassicalapproachesfordefiningsuchcross-covariancefunctionsaretheLinearModelof
Coregionalization(LMC)[JournelandHuijbregts,1976]andprocessconvolutions[Higdon,2002]. Intheformer,each
outputcorrespondstoaweightedsumofsharedlatentrandomfunctions. Inthelatter,eachoutputismodelledasthe
4202
luJ
2
]GL.sc[
1v67420.7042:viXraScalableMulti-OutputGaussianProcesseswithStochasticVariationalInference APREPRINT
convolutionintegralbetweenasmoothingkernelandalatentrandomfunctioncommontoalloutputs. TheLatent
VariableMOGP(LV-MOGP),introducedbyDaietal.[2017],extendsoutputcovarianceconstructionbyapplyinga
kernelfunctiontolatentvariables,oneforeachoutput. Thisapproachenablesefficientgeneralizationtonewoutputs.
Daietal.[2017]alsodemonstratedexperimentallythatLV-MOGPoutperformsLMC,whichtendstofaceoverfitting
issueswhenestimatingafull-rankcoregionalizationmatrix.
To address the cubic complexity concerning the number of outputs in MOGP [Bonilla et al., 2007, Alvarez et al.,
2012],Nguyenetal.[2014]proposetousemini-batchesinthecontextofLMCframework,makingthecomputational
complexityforeachiterationindependentofthesizeoftheoutputs. However,themodel’sparametersincreaselinearly
withthenumberofoutputs,constrainingitspracticalscalabilitytoproblemswithlargescaleoutput. Thecomputational
complexity associated with estimating the marginal likelihood for the LV-MOGP also increases linearly with the
number of outputs, making it unsuitable for problems with a large number of outputs. The stochastic formulation
allowingmini-batchtrainingofBayesianGaussianProcessLatentVariablesModels(BGPLVM)hasbeeninvestigated
inLalchandetal.[2022],employingStochasticVariationalInference(SVI)[Hoffmanetal.,2013,Hensmanetal.,2013]
tofacilitatescalableinference. However,theBGPLVMsareappliedinunsupervisedlearningcontexts,distinguishing
themfromtheMOGPmodelsconsideredinsupervisedlearningsettings.
In this paper, we adapt the SVI techniques used in BGPLVM to LV-MOGP, to formulate a training objective that
supportsmini-batchingforbothinputsandoutputs. Thisapproachmakesthecomputationalcomplexitypertraining
iteration independent of the number of outputs. Additionally, we generalize the assumption of latent variables in
LV-MOGP by introducing multiple latent variables for each output, allowing for the construction of more flexible
covariances. Ourdoublystochastictrainingobjectivedecomposesdata-dependenttermacrossdatapoints,allowing
trivialmarginalizationofmissingvalues. Moreover,ourframeworkeasilyextendstonon-Gaussianlikelihoods,making
ourmodelapplicabletoawiderangeofdatasets,suchasmodellingcountdatausingaPoissonlikelihood. Wereferto
ourapproachasGeneralizedScalableLatentVariableMOGP(GS-LVMOGP).Wetestourmodelonseveralreal-world
datasets,suchasspatiotemporaltemperaturemodellingandspatialtranscriptomics.
2 Background
Formulti-taskmodellingofadatasetcollectedfromDsourceswithinputsX={x
n
∈RQX}N n=1andobservations
Y = {y }D , where y = {y }N , multiple output Gaussian processes (MOGPs) induce a prior distribution
d d=1 d dn n=1
overvector-valuedfunctionsbyensuringanyfinitecollectionoffunctionvaluesf (x ),f (x ),...,f (x )with
d1 1 d2 2 dn n
(d )n ⊆ {1,2,...,D} are multivariate Gaussian distributed. The Linear Model of Coregionalization (LMC) and
i i=1
LatentVariableMOGP(LV-MOGP)aretwoapproachesusedtodefinesuchaprior.
The Linear Model of Coregionalization In the LMC framework, every output (source) is modelled as a linear
combinationofindependentrandomfunctions[JournelandHuijbregts,1976]. Iftheindependentrandomfunctionsare
Gaussianprocesses,thentheresultingmodelwillalsobeaGaussianprocess[AlvarezandLawrence,2011]. Foroutput
d,themodelisexpressedas: f (x) = (cid:80)Q (cid:80)Rq ai ui(x),wherethefunctions{ui(x)}Rq arelatentGaussian
d q=1 i=1 d,q q q i=1
processessharingthesamecovariancefunctionk (x,x′).ThereareQgroupsoffunctions,witheachmemberofagroup
q
sharingthesamekernelfunction,butsampledindependently.Thecross-covariancebetweenanytwofunctionsf (x)and
d
f (x′)atinputsxandx′isgivenby: cov[f (x),f (x′)]=(cid:80)Q (cid:80)Rq ai ai k (x,x′)=(cid:80)Q bq k (x,x′),
d′ d d′ q=1 i=1 d,q d′,q q q=1 d,d′ q
withbq =(cid:80)Rq ai ai . ForN inputs,wedenotethevectorofvaluesfromtheoutputdevaluatedatXasf . The
d,d′ i=1 d,q d′,q d
stackedversionofalloutputsisdefinedasf,sothatf =[f⊤,...,f⊤]⊤. Nowthecovariancematrixforthejointprocess
1 D
overf isexpressedas:
Q Q
(cid:88) (cid:88)
K = A A⊤⊗K = B ⊗K , (1)
f,f q q q q q
q=1 q=1
wherethesymbol⊗denotestheKroneckerproduct,A
q
∈RD×Rq hasentriesai
d,q
andB
q
∈RD×D hasentriesbq
d,d′
andisknownasthecoregionalizationmatrix.
AsasimplifiedversionoftheLMC,theintrinsiccoregionalizationmodel(ICM)assumesthattheelementsbq of
d,d′
the coregionalization matrix B can be written as bq = v b . This simplifies the model, making the intrinsic
q d,d′ d,d′ q
coregionalizationmodelalinearmodelofcoregionalizationwithQ=1[AlvarezandLawrence,2011]. Inthiscase,
Eq. (1)canbeexpressedasK =A A⊤⊗K =B ⊗K ,wherethecoregionalizationmatrixB hasrankR .
f,f 1 1 1 1 1 1 1
LatentVariableMOGP InICM,thecoregionalizationmatrixB isdirectlyparametrizedbyitsmatrixfactorA .
1 1
LatentVariableMOGP(LV-MOGP)Daietal.[2017]triedanalternativeapproach,thatisconstructingcoregionalization
2ScalableMulti-OutputGaussianProcesseswithStochasticVariationalInference APREPRINT
matrixB usingakernelappliedtolatentvariables,oneperoutput. DenotingthelatentvariablesasH={h }D ,
1 d d=1
whereh
d
∈ RQH isthelatentvariableassignedtooutputd. Thecovariancebetweenoutputsisthencomputedas
KH = kH(H,H), where kH is the kernel defined on latent variable space. The covariance between inputs X is
captured by KX = kX(X,X), where kX is another kernel defined on input space. The covariance matrix K
f,f
is defined as: K = KH ⊗KX. Latent variables H are treated in a Bayesian manner, with prior distribution
f,f
h ∼N(h |0,I ). TheprobabilisticdistributionsofLV-MOGParedefinedas:
d d QH
p(h )=N(h |0,I ),
d d QH
p(f |X,H)=N(f |0,K f,f), (2)
p(Y |f)=N(Y |f,σ2I ),
ND
whereσistheGaussianlikelihoodparameter.
VariationalInference ToperformposteriorinferenceinLV-MOGPdefinedinEq. (2),Daietal.[2017]derivea
variationallowerboundusingauxiliaryvariables[TitsiasandLawrence,2010]. Theyplaceinducingpointsinbothinput
spaceandlatentspace,whicharedenotedasZX = {zX,zX,...,zX }andZH = {zH,zH,...,zH }respectively.
1 2 MX 1 2 MH
TheinducingvariablesufollowsthesameGaussianprocessprior:
p(u|ZH,ZX)=N(u|0,K )=N(u|0,KH ⊗KX ), (3)
u,u u,u u,u
whereKH =kH(ZH,ZH),andKX =kX(ZX,ZX). Theconditionaldistributionoff givenuis:
u,u u,u
p(f |u,ZH,ZX,H,X)=N(f |K K−1u,K −K K−1K ), (4)
f,u u,u f,f f,u u,u u,f
where K = KH ⊗KX and KH = kH(H,ZH), and KX = kX(X,ZX). They approximate the posterior
f,u f,u f,u f,u f,u
distribution p(f,u,H | Y) by p(f | u,H)q(u)q(H), where q(u) = N(u | Mu,Σu), and q(H) = (cid:81)D N(h |
d=1 d
M ,Σ ),whereMu,Σu,{M ,Σ }D areparameterstobeestimated. Theevidencelowerbound(ELBO)canbe
d d d d d=1
derivedas:
logp(Y |X)≥E [logp(Y |f)]−KL(q(u)||p(u))−KL(q(H)||p(H)), (5)
p(f|u,X,H)q(u)q(H)
(cid:124) (cid:123)(cid:122) (cid:125)
F
whereF hasaclosed-formsolution[Daietal.,2017],seeAppendix7.1:
ND 1 1
F =− log2πσ2− Y⊤Y− Tr(K−1ΦK−1(Mu(Mu)⊤+Σu))
2 2σ2 2σ2 u,u u,u
(6)
1 1
+ Y⊤ΨK−1Mu− (ψ−Tr(K−1Φ)),
σ2 u,u 2σ2 u,u
whereψ =⟨Tr(K )⟩ ,Ψ=⟨K ⟩ andΦ=⟨K K ⟩ . Noticethatthecomputationalcomplexityof
f,f q(H) f,u q(H) u,f f,u q(H)
theF termincreaseslinearlywithbothDandN,1renderingthemethodunsuitableforapplicationsinvolvingalarger
numberofoutputsandinputs.
3 GeneralizedScalableLV-MOGP
We now investigate the stochastic formulation of LV-MOGP and extend its assumption regarding latent variables.
InsteadofasinglelatentvariableperoutputinLV-MOGP,weproposetheuseofpossiblyQ ≥ 1latentvariables,
denotedasH ={h ,h ,...,h },ford∈{1,2,...,D}.
d d,1 d,2 d,Q
Forsimplicity,weassumealltheselatentvariableshavethesamedimensionalityQ andareindependentofeachother.
H
Thepriordistributionofthelatentvariablesisthendefinedasfollows:
D D Q
(cid:89) (cid:89) (cid:89)
p(H)= p(H )= p(h ), (7)
d d,q
d=1 d=1q=1
wherep(h )=N(h |0,I )ifnoextrainformation. Wemayhaveadditionalinformationaboutthemeaningof
d,q d,q QH
theselatentvariablesandthereforeforparticularapplications,wecanusedifferentmeanvectorsorcovariancesper
1ThisistruefortermsY⊤Y,ψ,ΨandΦ.
3ScalableMulti-OutputGaussianProcesseswithStochasticVariationalInference APREPRINT
latentvariable,suchasinthespatiotemporaldatasetintheexperimentalsection5,weassumethepriormeanvectors
correspondtotheinitiallocationofeachoutput. ThegeneralizedLV-MOGPmodelisdefinedas:
Q
(cid:88)
p(f |H,X)=N(f |0, KH ⊗KX), p(y |f )=N(y |f ,σ2I ), (8)
q q d d d d d N
q=1
(cid:124) (cid:123)(cid:122) (cid:125)
Kf,f
whereKH representsthecovariancematrixcomputedonH ={h ,h ,...,h }usingtheq-thkernelfunction
q q 1,q 2,q D,q
onthelatentspace,denotedaskH. Similarly,KX representsthecovariancematrixcomputedonXwithq-thkernel
q q
functionontheinputspace,denotedaskX. σ isthelikelihoodparameterforoutputd. WhenQ = 1,themodelis
q d
reducedtoLV-MOGP;howeverwhenQ>1,ourmodelprovidesgreaterflexibilityforconstructingthecovariance
matrix. 2
3.1 AuxiliaryVariables
Weemployauxiliaryvariables[Titsias,2009]tofacilitateefficientlearningandinference. SimilartoLV-MOGP,we
considerinducinglocationsinbothinputandlatentspaces. WeassumeM inducinginputsininputspace,denotedas
X
ZX ={zX
1
,zX
2
,...,zX MX},wherezX
i
∈RQX,∀i∈{1,2,...,M X}. DistinctfromLV-MOGP,theinducinglocationsin
latentspacearecomposedofQcomponents. ThereareM inducinglocationsinlatentspace,withthei-thinducing
H
latentlocationbeingZH
i
= {zH i,1,zH i,2,...,zH i,Q},andeachcomponentzH
i,q
∈ RQH. TheM
H
inducinglocationsare
collectivelydenotedasZH ={ZH,ZH,...,ZH }. Theinducingvariablesufollowthepriordistribution
1 2 MH
Q
(cid:88)
p(u|ZH,ZX)=N(u|0, KH ⊗KX ), (9)
u,u;q u,u;q
q=1
(cid:124) (cid:123)(cid:122) (cid:125)
Ku,u
whereKH isthecovariancematrixcomputedonZH ={zH ,zH ,...,zH }withkernelfunctionkH andKX
u,u;q q 1,q 2,q MH,q q u,u;q
isthecovariancematrixonZX withkernelfunctionkX. Theconditionaldistributionoff giveninducingvariablesu
q
followsas
p(f |u,H,X,ZH,ZX)=N(f |K K−1u,K −K K−1K ), (10)
f,u u,u f,f f,u u,u u,f
whereK =(cid:80)Q KH ⊗KX ,KH =kH(H ,ZH)andKX =kX(X,ZH).
f,u q=1 f,u;q f,u;q f,u;q q q q f,u;q q q
3.2 VariationalDistributions
Thelog-evidenceisnottractableduetothepresenceoflatentvariables. Therefore,weusevariationalinferenceto
computealowerboundoftheoriginallog-evidence. Specially,weemploymeanfieldvariationaldistributionforlatent
variablesH,i.e.
D D Q
(cid:89) (cid:89) (cid:89)
q(H)= q(H )= q(h ), (11)
d d,q
d=1 d=1q=1
where q(h d,q) = N(h
d,q
| m d,q,Diag(S d,q)), m d,q,S
d,q
∈ RQH and Diag(S d,q) denotes the construction of a
diagonalmatrixwiththeelementsofS placedonthediagonal. Forinducingvariablesu,thevariationaldistribution
d,q
is: q(u) = q(u | M u,Σ u), where M
u
∈ RMHMX, Σ
u
∈ RMHMX×MHMX. Practically, instead of directly
parametrizingq(u),weintroduceu withp(u )=N(u |0,I ),andassumeu=Lu ,whereLL⊤ =K .
0 0 0 MHMX 0 u,u
Weparametrizeq(u 0)asN(u
0
| M 0,ΣH
0
⊗ΣX
0
),whereΣH
0
∈ RMH×MH andΣX
0
∈ RMX×MX. Thisprocedure
doesnotalterthepriordistributionofubutreducestheparametersfromO(M2M2)toO(M2 +M2 +M M ). 3
H X H X H X
Thevariationalposteriordistributionforf becomes:
(cid:90)
q(f |H,X,ZH,ZX)= p(f |u,H,X,ZH,ZX)q(u)du (12)
=N(f |K K−1M ,K +K K−1(Σ −K )K−1K ). (13)
f,u u,u u f,f f,u u,u u u,u u,u u,f
2SeemorediscussioninAppendix7.4.
3OtherbenefitssuchasefficientcomputationofKLtermaredetailedinAppendix7.2.
4ScalableMulti-OutputGaussianProcesseswithStochasticVariationalInference APREPRINT
3.3 VariationalLowerBoundwithStochasticOptimization
AsshownpreviouslyinEq. (5),theELBOisdefinedas
L =E [logp(Y |f)]−KL(q(u)||p(u))−KL(q(H)||p(H)), (14)
elbo q(f|X,H)q(H)
(cid:124) (cid:123)(cid:122) (cid:125)
F
where the F term is analytically integrated in LV-MOGP. However, this tractability is only feasible for Gaussian
likelihoods. Fornon-Gaussianlikelihoods,suchasthePoissonlikelihood,F termmustbere-derivedandapproximated.
Tofacilitatesupportfornon-Gaussianlikelihoodsandmini-batchgradientupdates,weconsiderderivingtheELBO
differently. Consideringthefactorization: logp(Y
|f)=(cid:80)D (cid:80)N
logp(y |f ),weobtain:
d=1 n=1 dn dn
(cid:34) D N (cid:35)
(cid:88)(cid:88)
F =E logp(y |f )
q(f|X,H)q(H) dn dn
d=1n=1
  (15)
D N D N
(cid:88)(cid:88) (cid:88)(cid:88)
= E E [logp(y |f )]= E [L (H )],
q(Hd) q(fdn|Hd,xn) dn dn  q(Hd) dn d
d=1n=1 (cid:124) (cid:123)(cid:122) (cid:125) d=1n=1
Ldn(Hd)
andtheexpectationtermE [L (H )]willbecomputednumericallyusingMonteCarloestimationwithJ samples
q(Hd) dn d
{H(j)}J = {{h(j),h(j),...,h(j) }}J , sampledfromq(h ),q(h ),...,q(h )usingreparametrizationtrick
d j=1 d,1 d,2 d,Q j=1 d,1 d,2 d,Q
[KingmaandWelling,2013,Lalchandetal.,2022]. Inparticualr,wesampleϵ(j) ∼ N(ϵ(j) | 0,I )andcompute
QH
h(j) =m +S ⊙ϵ(j)forq ∈{1,2,...,Q}andj ∈{1,2,...,J}. Thus,
d,q d,q d,q
J J
E [L (H )]≈ 1 (cid:88) L (H(j))= 1 (cid:88) L ({m +S ⊙ϵ(j)}Q ), (16)
q(Hd) dn d J dn d J dn d,q d,q q=1
j=1 j=1
where⊙denotestheHadamardproduct. ForaGaussianlikelihood,theexpectedlog-likelihoodtermL (H(j))canbe
dn d
analyticallyobtained,
1 1
L (H(j))=logN(y |K K−1M ,σ2)− Tr(K )+ Tr(K−1K K )
dn d dn fdn,u u,u u d 2σ2 fdn,fdn 2σ2 u,u u,fdn fdn,u
d d (17)
1
− Tr(Σ K−1K K K−1).
2σ2 u u,u u,fdn fdn,u u,u
d
Fornon-Gaussianlikelihood,thisone-dimensionalintegralcanbeaccuratelyapproximatedbyGauss-Hermitequadrature
[LiuandPierce,1994,Ramchandranetal.,2021],seeAppendix7.3.
DoublyStochasticELBO WefurtherapproximateL byemployingmini-batchingtospeedupcomputation. In
elbo
everyiteration,aminibatchofm input-outputpairsissampled,denotedasB ={(d ,n ),(d ,n ),...,(d ,n )},
b 1 1 2 2 mb mb
J
L ≈Lˆ = ND (cid:88) (cid:88) E [logp(y |f )]−KL(q(u)||p(u)) (18)
elbo elbo m
b
q(fdn|H d(j),xn) dn dn
(d,n)∈Bj=1
D
(cid:88)mb (cid:88)Q
− KL(q(h )||p(h )), (19)
m di,q di,q
b
i=1q=1
TheKLtermsareanalyticallytractableduetothechoiceoftheGaussianvariationalfamilyforq(u)andq(h ).
di,q
Thismethodisreferredtoasdoublystochasticvariationalinference[TitsiasandLázaro-Gredilla,2014,Salimbeni
andDeisenroth,2017],reflectingthetwo-foldstochasticity: mini-batchingforgradient-basedupdatesandcomputing
expectation via Monte Carlo sampling. This training procedure factorizes data-dependent term across data points,
allowingforthetrivialmarginalizationofmissingvalues. Consequently,theELBOisnaturallycompatiblewiththe
heterotopic4setting.
4ForMOGP,ifeachoutputhasthesamesetofinputs,thesystemisknownasisotopic. Forgeneralcases,theoutputsmay
associatewithdifferentsetsofinputs,X ={x }Nd ,thisisknownasheterotopic.
d dn n=1
5ScalableMulti-OutputGaussianProcesseswithStochasticVariationalInference APREPRINT
ComputationalComplexity ThetrainingcostforGS-LVMOGPisprimarilydominatedbytwooperations: matrix
inversion K−1 and matrix multiplication K K−1, where f represents the m function values in a mini-batch.
u,u fb,u u,u b b
ThematrixmultiplicationhasacomputationalcomplexityO(m M2M2). Thecomputationalcomplexityformatrix
b X H
inversionK−1 variesbasedonthechoiceofQ. ForQ=1,thematrixK hastheaKroneckerproductstructure,
u,u u,u
allowing the inversion K−1 = (KH )−1 ⊗(KX )−1 to be performed in O(M3 +M3). For Q > 1, inversion
u,u u,u u,u X H
operation K−1 has a computational complexity O(M3M3). Compared to LV-MOGP, the GS-LVMOGP has a
u,u X H
computationalcomplexitythatisfreefromdependenceonthesizeoftheoutputsD andinputsN. Thismakesthe
methodmorecapableofhandlinglarge-scaleproblems. 5
3.4 Prediction
Aftermodeltraining,wecanmakepredictionsforanewinputx∗atanyoutputd∗. Thepredictivedistributionforf∗is
givenby:
(cid:90)
p(f∗ |ZH,ZX,x∗)= p(f∗ |ZH,ZX,H ,x∗)q(H )dH (20)
d∗ d∗ d∗
However, Eq. (20) is intractable for general kernel functions kH 6. As a first workaround, we can approximate
q
q(H ) = (cid:81)Q q(h )byitsQmeans,wherehmean denotesthemeanofq(h ). Thus,p(f∗ | ZH,ZX,x∗) ≈
d∗ q=1 d∗q d∗,q d∗,q
p(f∗ |{hmean}Q ,ZH,ZX,x∗).
d∗,q q=1
4 RelatedWorks
ThemitigationofthecomputationalcomplexityO(N3)forsingleoutputGaussianprocesseshasbeenextensively
investigatedovertheyears[Quinonero-CandelaandRasmussen,2005,Titsias,2009,Liuetal.,2020,Salimbenietal.,
2018,Tranetal.,2021,Wuetal.,2022,Bartelsetal.,2023]. However, thechallengeofO(N3D3)complexityin
multi-outputGaussianprocesseshasreceivedlessattention. Álvarezetal.[2010]extendedinducinginputstoconvolved
multi-outputGaussianprocesses,reducingcomplexitytoO(NDM2),withM asthenumberofinducingpoints. For
LV-MOGPwithoutmissingdata,complexityisfurtherreducedtoO(NDM). Bothmethods’complexityscaleslinearly
withoutputsD,limitingtheirsuitabilityforlarge-scaleproblems. Nguyenetal.[2014]approachedtheproblemby
modellingtheoutputsasaweightedcombinationofsharedandindividuallatentGaussianprocesses. Inducingpoints
ofsizeM areassignedtoeachlatentprocess,andstochasticvariationalinferenceisappliedtothismodel,achieving
avariationallowerboundwithcomplexityO(M3). Themodel’sparametersincreaselinearlywithDbecauseeach
outputrequiresanewsetofvariationalparameters. Incontrast,GS-LVMOGPallowsallocationoftheparameterbudget
regardlessofthenumberofoutputsD. Bruinsmaetal.[2020]assumedthattheDdimensionaloutputdatalivearound
alow-dimensionallinearsubspacewithdimensionalitym. Theyproposedtheuseofsufficientstatisticsandorthogonal
bases to accelerate inference and learning, achieving complexity linear to m. However, their method is limited to
Gaussianlikelihood. TheextensionofGS-LVMOGPtoaccommodatenon-Gaussianlikelihoodsisstraightforward,
whichexpandstheapplicabilityofMOGPtoabroaderspectrumofproblems.
5 Experiments
We test the GS-LVMOGP on several real-world data sets. For all experiments, we choose automatic relevance
determinationsquaredexponential(SE-ARD)kernelonthelatentspace. ThekernelofGS-LVMOGPontheinputspace
isdifferentforeachdatasetandisspecifiedaccordingly. Moreinformationaboutevaluationmetricsandexperiment
detailsincludingthevaluesforM ,M ,m andQ areinAppendix7.6.
H X b H
ExchangeRatesprediction ThisdatasetincludesdailyexchangeratesagainsttheUSDfortencurrenciesandthree
preciousmetalsfortheyear2007. OurtaskistopredicttheexchangeratesofCAD,JPY,andAUDonspecificdays,
giventhatallothercurrenciesareobservedthroughouttheyear. Wefollowthesamesetupas[Álvarezetal.,2010]. For
GS-LVMOGPmodels,weuseaMatérn-1/2kernel,consistentwith[Bruinsmaetal.,2020]. Thoughinthisexperiment
thenumberofoutputsisrathersmall,D =13andnoapproximationisrequired,weincludeditasawaytoshowthat
5Wearefocusedoncomputationalcomplexityforeachiterationoftheparameterupdate.Thoughthesmallermini-batchsizem
b
willleadtosmallercomputationalcomplexityperiteration,moreiterationsarerequiredforcyclingthroughallthedata.Furthermore,
asmallerlearningrateisoftenrequiredforsmallm totradeoffthelargernoiseintheELBOapproximation.Therefore,thereisa
b
complexrelationshipbetweenlearningrateandminibatchsizewhichdeterminesthetruecomputationalcomplexity.
6FurtherdiscussiononhowtohandlethisintegralappearsinAppendix7.5.
6ScalableMulti-OutputGaussianProcesseswithStochasticVariationalInference APREPRINT
Table1: MethodscomparisononExchangedataset. IGPdenotesindependentGP,oneforeachoutput. COGP[Nguyen
etal.,2014],CGP[AlvarezandLawrence,2008],OILMM[Bruinsmaetal.,2020]. ∗NumbersaretakenfromNguyen
etal.[2014],†NumbersaretakenfromBruinsmaetal.[2020]. Resultsareaveragesoftheoutputsoverfiverepetitions
withdifferentrandomseeds.
GS-LVMOGP
Model IGP COGP CGP OILMM LV-MOGP
Q=1 Q=2 Q=3
SMSE 0.600∗ 0.213∗ 0.243∗ 0.19† 0.251 0.256 0.186 0.167
NLPD 0.408∗ −0.839∗ −2.947∗ -2.471 -1.851 -2.416 -2.703
themini-batchapproachleadstoresultsonparwithothersmall-scalemodels,asshowninTable1. Noticethatby
settingQ=1,weobtainascalableversionoftheLV-MOGPproposedbyDaietal.[2017]. Thoughitsperformanceis
slightlybehindLV-MOGP,theGS-LVMOGPwithQ=3outperformsLV-MOGPonbothmetrics. AsQincreases,the
flexibilityinconstructingthecovariancematriximproves,resultinginenhancedmodelperformance. Moreinformation
iselaboratedinAppendix7.6.1.
Table 2: Models comparison on NYC Crime. The brackets (G) and (P) refer to Gaussian and Poisson likelihood
respectively. Resultsareaveragesover5-foldcross-validationswith±standarddeviation.
RMSE NLPD
IGP(G) 1.937±0.030 2.107±0.022
OILMM(G) 1.857±0.025 1.493±0.011
Q=1 2.201±0.125 1.498±0.299
GS-LVMOGP(G) Q=2 1.908±0.381 1.525±0.206
Q=3 1.969±0.103 1.531±0.352
Q=1 1.791±0.023 1.288±0.003
GS-LVMOGP(P) Q=2 1.791±0.023 1.287±0.003
Q=3 1.790±0.024 1.287±0.003
NYC Crime Count modelling We analyze crime patterns across New York City (NYC) using daily complaint
data from nyc [2015]. Accurate modelling of the seasonal trends and spatial dependencies from crime data can
improvepoliceresourceallocationefficiency[Agliettietal.,2019]. FollowingHamelijncketal.[2021],thedataset
includes447spatiallocationswith182observationseach. Eachlocationistreatedasanoutput, soD = 447. We
considerthreemodelsIGP,OILMMandourGS-LVMOGP,allusingtheMatérn-3/2kernel. WeconsiderGaussianand
Poissonlikelihoodsforthiscountdata. Table2showstheresults. TheperformancecomparisonofGS-LVMOGPwith
Q=1,2,3againindicatesthathigherQimprovespredictionaccuracy. Notably,thePoissonlikelihoodconsistently
outperformstheGaussianlikelihood,asitisinherentlymoresuitedtothecharacteristicsofcountdata.
SpatiotemporalTemperaturemodelling Inthissection,wead-
dress spatiotemporal temperature modelling across Europe, using
73 7. 27 .5 5° ° D 1,260spatiallocations(blueregionsinFig. 1)with363monthsof
observationsperlocation.Eachspatiallocationistreatedasanoutput,
F G
63.75° soD = 1260,andtimetistheinput. Ourtasksincludedataimpu-
A C E
53.75° tationandextrapolationprediction. Foreveryoutput,werandomly
H I select10datapointsfromthefirst263observationsastrainingdata,
43.75°
usingtheremaining253monthsforimputationtesting. Thelast100
35° B observationsforeachoutputserveasextrapolationtestsamples. To
33.75°
-13.1-2151.°25° 7.5° lo26. n25 g° itude46.875° 667.9.53°75° i tn ioc nor op fo tr ha ete las tp ea nt tia vl ai rn if ao br lm esa ,t pio (n h, dw ,qe ),s qet =the 1,m 2e ,a ..n .,s Qof ,t th oe thp eri (o lr od ni gs it tr uib du e-
,
latitude)vectorforeachoutputd. Wecomparethreemodels: IGP,
Figure1: Spatiallocationsfortraining(blue),
OILMMandGS-LVMOGP,allusingMatérn–5/2kernelswithape-
inner(green)andouter(orange)outputextrapo-
riodic component. Table 3 summarizes the results. From Table 3,
lation. PredictionsforI,AandCareinFig. 2,
withQ = 3,theGS-LVMOGPoutperformsothermethodsinboth
andothers(B,D,E,F,G,H)areinAppendix
imputation and extrapolation tasks. The first plot in Fig. 2 shows
7.6.2
predictionsforoneoutputinthisspatiotemporaldataset. Ourmodel
canalsoextrapolatetounseenlocationsnotincludedduringtraining,
knownasoutputextrapolation,bysettinglatentvariablesh tothespatialcoordinatesofchosenlocations. Thelast
d∗,q
twoplotsinFig. 2showpredictionsfortwospatiallocationsexcludedduringtraining. Fig. 1illustratestwotypesof
spatialregionsforoutputextrapolation,markedingreenandorange. Locationsmarkedingreenaresurroundedby
7
edutitalScalableMulti-OutputGaussianProcesseswithStochasticVariationalInference APREPRINT
traininglocations(blue),thusthepredictionsforthemaretermedasinneroutputextrapolation,total69outputs. In
contrast,predictionsfortheorangeregions,referredtoasouteroutputextrapolation,lieontheperipheryofthetraining
regions,totalling144outputs. TheSMSEsforinnerandouteroutputextrapolationare0.184±0.09and0.211±0.23,
respectively. 7
290.0
287.5
285.0
282.5
280.0
0 50 100 150 200 250 300 350
Time (month)
298
297
296
295
294
0 50 100 150 200 250 300 350
Time (month)
287.5
285.0
282.5
280.0
277.5
0 50 100 150 200 250 300 350
Time (month)
Figure2: Modelpredictionsareprovidedforthreeselectedoutputscorrespondingtospecificspatiallocations. The
temperatureismeasuredbytheKelvinunit. Fromtoptobottom,theplotsrepresentpredictionsatlocationsI,AandC
asmarkedinFig. 1. LocationIisincludedinthetrainingdataset,whilepredictionsforlocationsAandCresultfrom
outputextrapolation. Trainingpointsareindicatedbyredcrosses( ),imputationandextrapolationtestpointsbyblack
( )andorangedots( )respectively. Theshadedareaindicatesthemean±oneandtwostandarddeviations.
Table 4: Model comparisons on USHCN.
Table3: Comparisonofmethodsonthespatio-temporaldataset.Resultsareaverages
Resultsareaveragesover5repetitionswith
overfiverepetitionswithdifferentrandomseeds,withstandarddeviationinthebracket.
differentrandomseeds,standarddeviations
GS-LVMOGP
IGP OILMM areinbrackets.
Q=1 Q=2 Q=3
0.177 0.128 0.135 0.123 0.120 MSE NLPD
Imputation SMSE (1.4e-6) (1.6e-2) (1.5e-5) (9.2e-6) (3.3e-6) Q=1 0.620 8.747
NLPD (12 .. 34 e8 -4
4)
(72 .0.8 e5
-2)
(42 .0.4 e2
-4)
(12 .. 43 e8 -0
4)
(2 4. .33 e8 -50
)
GS-LVMOGP
Q=2
(0 0. .0 61 13 98) ( 13 0.2 .1 19 6)
0.261 0.147 0.146 0.137 0.133 (0.0076) (2.786)
Extrapolation SMSE (4.9e-5) (1.4e-2) (3.1e-5) (7.0e-6) (1.2e-5) Q=3 0.618 9.89
2.87 3.11 2.568 2.565 2.558 (0.0062) (3.066)
NLPD (7.7e-4) (4.8e-2) (9.1e-4) (6.6e-5) (6.2e-4) OILMM 0.89 810.37
(0.0002) (2.363)
Climateforecast WeconsidertheUnitedStatesHistoricalClimatologyNetwork(USHCN)dailydataset,andwe
followasimilarsettingtoDeBrouweretal.[2019],choosingasubsetof1,114stationsandexaminingafour-year
observationalperiodfrom1996to2000. ThedatasetissubsampledasDeBrouweretal.[2019],resultinginanaverage
52observationsduringthefirstthreeyearsforeachoutput. Wediscardoutputswithfewerthanthreeobservationsand
finallyhave5,507outputs. MoredetailsareinAppendix7.6.4. Ourtaskistoforecastthesubsequentthreeobservations
followingtheinitialthreeyearsofdatacollection. WeusetheMatérn-3/2kernelforGS-LVMOGPandOILMM.Table
4comparestheperformanceresultsofourmodelsagainstOILMM,showingimprovedperformance.
7TheNLPDsforextrapolatedoutputsarenotavailableaswehavenoestimatesoftheparametersσ .
d
8
erutarepmeT
erutarepmeT
erutarepmeTScalableMulti-OutputGaussianProcesseswithStochasticVariationalInference APREPRINT
SpatialTranscriptomics Spatialtranscriptomics[Ståhletal.,2016]offershigh-resolutionprofilingofgeneexpression
whileretainingthespatialinformationofthetissue. Applyingmachinelearningtechniquestospatialtranscriptomics
datasetsisvitalforunderstandingthetissueandthediseasearchitecture,potentiallyenhancingdiagnosisandtreatment.
Weconsidera10xGenomicsVisiumhumanprostatecancerdataset[10x]whichcontainsgeneexpressioncountsdata
from17,943genesmeasuredacross4,371spatiallocations. Pathologist’shistologicalannotationsforthisspecific
tissueFig.3aareprovidedby10xGenomics. Forourmodel,weusethetop5,000highlyvariablegenescalculated
usingScanpy’shighlyvariablegenesfunction[Wolfetal.,2018]. Eachgeneistreatedasadifferentoutputandthe
spatialcoordinatesofcellsareregardedastheinputs. WefocusonGS-LVMOGPwithQ = 1,usingtheSE-ARD
kernelandPoissonlikelihood,andweaimtoexplorethestructureofthelatentspacewithQ =3forthegenes.
H
Afterfittingourmodeltothisdataset,wecollectthelatentvariablesforthegenesandclustertheminto6groupsusing
k-means. Toplotthespatialdistributionofeachclusterontothetissue,wecalculatetheaverageexpressionofthegenes
ineachoftheclustersandcomparethemagainstthepathologist’sannotatedregions. InFig. 3weshowtwoofthe
clusterswithgenesdelineatingthetumour(3b)andthenormaltissue(3c)areas,showinggoodcorrespondencewiththe
pathologist’slabels(InvasivecarcinomaandNormalglands).
Unlikestandardclusteringpracticeswhichsolelyrelyongeneexpressiontoclusterthespatialtranscriptomicsdata,our
clusteringapproachincorporatesspatialcorrelationsintothegeneclusters. Inthisway,wecandiscoverdistinctspatial
tissueregionswhichmightbemissedwhenonlyconsideringgeneexpression.
(a)Pathologist’slabels (b)Tumour (c)NormalGland
Figure 3: Latent variable k-means clustering results of the prostate carcinoma dataset. (a) shows the ground-truth
pathologist’sannotations,(b)and(c)showtheaveragegeneexpressioninclusters5and3,respectively. Cluster5aligns
wellwiththeinvasivecarcinomalabel(tumour)whilecluster3withthenormalgland(normal). Plotsforotherclusters
areshowninAppendix7.6.6.
6 Conclusions
Inthispaper,weproposeGS-LVMOGP,ageneralizedlatentvariablemulti-outputGaussianprocessmodelwithina
stochasticvariationalinferenceframework. Byconductingvariationalinferenceforlatentvariablesq(H)andinducing
valuesq(u),ourapproachcaneffectivelymanagelarge-scaledatasetswithbothGaussianandnon-Gaussianlikelihoods.
Onefeatureofthemodelisthattheparametersinthemeanvectorsandvariancesforq(H)alsoincreasewiththe
numberofoutputs. Futureresearchcouldexploreimposingstructuredconstraintsinthelatentspacetofurtherreduce
thenumberofparametersrequiredforestimation.
Acknowledgements
MAAhasbeenfinancedbytheEPSRCResearchProjectEP/V029045/1andtheWellcomeTrustproject217068/Z/19/Z.
TheauthorssincerelythankXinxingShifortheinsightfulandenjoyablediscussionsthroughouttheprogressofthis
work. WealsoappreciateChunchaoMaforthevaluablediscussionsattheinceptionofthisproject. Additionally,we
thankWesselP.Bruinsmaforhiskindandpromptresponsestoourqueriesregardingthedatasets.
9ScalableMulti-OutputGaussianProcesseswithStochasticVariationalInference APREPRINT
References
ChristopherKIWilliamsandCarlEdwardRasmussen. Gaussianprocessesformachinelearning,volume2. MITpress
Cambridge,MA,2006.
Mauricio A Alvarez, Lorenzo Rosasco, Neil D Lawrence, et al. Kernels for vector-valued functions: A review.
FoundationsandTrends®inMachineLearning,4(3):195–266,2012.
HansWackernagel. Multivariategeostatistics: anintroductionwithapplications. SpringerScience&BusinessMedia,
2003.
PabloMoreno-Muñoz,AntonioArtés,andMauricioAlvarez. Heterogeneousmulti-outputgaussianprocessprediction.
AdvancesinNeuralInformationProcessingSystems,31,2018.
Fariba Yousefi, Michael T Smith, and Mauricio Alvarez. Multi-task learning for aggregated data using gaussian
processes. AdvancesinNeuralInformationProcessingSystems,32,2019.
ChunchaoMa,ArthurLeroy,andMauricioAlvarez. Latentvariablemulti-outputgaussianprocessesforhierarchical
datasets. arXivpreprintarXiv:2308.16822,2023.
AndreGJournelandCharlesJHuijbregts. Mininggeostatistics. 1976.
Dave Higdon. Space and space-time modeling using process convolutions. In Quantitative methods for current
environmentalissues,pages37–56.Springer,2002.
ZhenwenDai,MauricioÁlvarez,andNeilLawrence. Efficientmodelingoflatentinformationinsupervisedlearning
usinggaussianprocesses. AdvancesinNeuralInformationProcessingSystems,30,2017.
EdwinVBonilla,KianChai,andChristopherWilliams. Multi-taskgaussianprocessprediction. Advancesinneural
informationprocessingsystems,20,2007.
TrungVNguyen,EdwinVBonilla,etal. Collaborativemulti-outputgaussianprocesses. InUAI,pages643–652,2014.
VidhiLalchand,AdityaRavuri,andNeilDLawrence. Generalisedgaussianprocesslatentvariablemodels(gplvm)
withstochasticvariationalinference. arXivpreprintarXiv:2202.12979,2022.
Matthew D Hoffman, David M Blei, Chong Wang, and John Paisley. Stochastic variational inference. Journal of
MachineLearningResearch,2013.
JamesHensman,NicoloFusi,andNeilDLawrence. Gaussianprocessesforbigdata. arXivpreprintarXiv:1309.6835,
2013.
MauricioAAlvarezandNeilDLawrence. Computationallyefficientconvolvedmultipleoutputgaussianprocesses.
TheJournalofMachineLearningResearch,12:1459–1500,2011.
MichalisTitsiasandNeilDLawrence.Bayesiangaussianprocesslatentvariablemodel.InProceedingsofthethirteenth
InternationalConferenceonArtificialIntelligenceandStatistics,pages844–851.JMLRWorkshopandConference
Proceedings,2010.
MichalisTitsias. Variationallearningofinducingvariablesinsparsegaussianprocesses. InArtificialIntelligenceand
Statistics,pages567–574.PMLR,2009.
DiederikPKingmaandMaxWelling. Auto-encodingvariationalbayes. arXivpreprintarXiv:1312.6114,2013.
QingLiuandDonaldAPierce. Anoteongauss—hermitequadrature. Biometrika,81(3):624–629,1994.
SiddharthRamchandran,MiikaKoskinen,andHarriLähdesmäki. Latentgaussianprocesswithcompositelikelihoods
andnumericalquadrature. InInternationalConferenceonArtificialIntelligenceandStatistics,pages3718–3726.
PMLR,2021.
MichalisTitsiasandMiguelLázaro-Gredilla. Doublystochasticvariationalbayesfornon-conjugateinference. In
InternationalConferenceonMachineLearning,pages1971–1979.PMLR,2014.
HughSalimbeniandMarcDeisenroth. Doublystochasticvariationalinferencefordeepgaussianprocesses. Advances
inNeuralInformationProcessingSystems,30,2017.
JoaquinQuinonero-CandelaandCarlEdwardRasmussen. Aunifyingviewofsparseapproximategaussianprocess
regression. TheJournalofMachineLearningResearch,6:1939–1959,2005.
HaitaoLiu,Yew-SoonOng,XiaoboShen,andJianfeiCai. Whengaussianprocessmeetsbigdata: Areviewofscalable
gps. IEEETransactionsonNeuralNetworksandLearningSystems,31(11):4405–4423,2020.
HughSalimbeni,Ching-AnCheng,ByronBoots,andMarcDeisenroth. Orthogonallydecoupledvariationalgaussian
processes. AdvancesinNeuralInformationProcessingSystems,31,2018.
10ScalableMulti-OutputGaussianProcesseswithStochasticVariationalInference APREPRINT
Gia-LacTran,DimitriosMilios,PietroMichiardi,andMaurizioFilippone. Sparsewithinsparsegaussianprocesses
usingneighborinformation. InInternationalConferenceonMachineLearning,pages10369–10378.PMLR,2021.
LuhuanWu,GeoffPleiss,andJohnPCunningham. Variationalnearestneighborgaussianprocess. InInternational
ConferenceonMachineLearning,pages24114–24130.PMLR,2022.
SimonBartels,KristofferStensbo-Smidt,PabloMoreno-Muñoz,WouterBoomsma,JesFrellsen,andSorenHauberg.
Adaptivecholeskygaussianprocesses. InInternationalConferenceonArtificialIntelligenceandStatistics,pages
408–452.PMLR,2023.
MauricioÁlvarez,DavidLuengo,MichalisTitsias,andNeilDLawrence. Efficientmultioutputgaussianprocesses
through variational inducing kernels. In Proceedings of the Thirteenth International Conference on Artificial
IntelligenceandStatistics,pages25–32.JMLRWorkshopandConferenceProceedings,2010.
WesselBruinsma,EricPerim,WilliamTebbutt,ScottHosking,ArnoSolin,andRichardTurner.Scalableexactinference
inmulti-outputgaussianprocesses. InInternationalConferenceonMachineLearning,pages1190–1201.PMLR,
2020.
MauricioAlvarezandNeilLawrence. Sparseconvolvedgaussianprocessesformulti-outputregression. Advancesin
NeuralInformationProcessingSystems,21,2008.
2014–2015 crimes reported in all 5 boroughs of new york city. https://www.kaggle.com/adamschroeder/
crimes-new-york-city,2015. Accessed: 2024-05-19.
VirginiaAglietti,TheodorosDamoulas,andEdwinVBonilla. Efficientinferenceinmulti-taskcoxprocessmodels. In
The22ndInternationalConferenceonArtificialIntelligenceandStatistics,pages537–546.PMLR,2019.
OliverHamelijnck,WilliamWilkinson,NikiLoppi,ArnoSolin,andTheodorosDamoulas. Spatio-temporalvariational
gaussianprocesses. AdvancesinNeuralInformationProcessingSystems,34:23621–23633,2021.
EdwardDeBrouwer,JaakSimm,AdamArany,andYvesMoreau.Gru-ode-bayes:Continuousmodelingofsporadically-
observedtimeseries. AdvancesinNeuralInformationProcessingSystems,32,2019.
PatrikLStåhl,FredrikSalmén,SanjaVickovic,AnnaLundmark,JoséFernándezNavarro,JensMagnusson,Stefania
Giacomello,MichaelaAsp,JakubOWestholm,MikaelHuss,etal. Visualizationandanalysisofgeneexpressionin
tissuesectionsbyspatialtranscriptomics. Science,353(6294):78–82,2016.
10X Genomics. https://www.10xgenomics.com/resources/datasets/
human-prostate-cancer-adenocarcinoma-with-invasive-carcinoma-ffpe-1-standard-1-3-0.
FAlexanderWolf,PhilippAngerer,andFabianJTheis. Scanpy: large-scalesingle-cellgeneexpressiondataanalysis.
Genomebiology,19(1):1–5,2018.
DiederikPKingmaandJimmyBa. Adam: Amethodforstochasticoptimization. arXivpreprintarXiv:1412.6980,
2014.
MJMenne,CNWilliamsJr,andRSVose. Long-termdailyclimaterecordsfromstationsacrossthecontiguousunited
states,2015.
11ScalableMulti-OutputGaussianProcesseswithStochasticVariationalInference APREPRINT
7 Appendix
7.1 ELBOdeviationforLV-MOGP
Inthissection,wedescribeinmoredetailthedeviationoftheevidencelowerboundforLV-MOGP.
(cid:90)
logp(Y |X)=log p(Y,f,u,H|X)dfdudH
(cid:90) q(f,u,H)
=log p(Y,f,u,H|X) dfdudH
q(f,u,H)
Jensen’sinequality(cid:90) p(Y,f,u,H|X)
≥ q(f,u,H)log dfdudH
q(f,u,H)
(cid:90) p(Y |f)(cid:24)p(f(cid:24)|(cid:24) u(cid:24) )p(u)p(H) (21)
= q(f,u,H)log (cid:24)p(f(cid:24)|(cid:24) u(cid:24)
)q(u)q(H)
dfdH
(cid:90) (cid:90) p(u) (cid:90) p(H)
= q(f,u,H)logp(Y |f)dfdudH+ q(u)log du+ q(H)log dH
q(u) q(H)
(cid:124) (cid:123)(cid:122) (cid:125)
F
=F −KL(q(u)||p(u))−KL(q(H)||p(H)).
NowwefocusontheF term. Noticethat
logp(Y |f)=logN(Y |f,σ2I)
=−ND log(2πσ2)− 1 (cid:0) Y⊤Y−2Y⊤f +f⊤f(cid:1) , (22)
2 2σ2
and,
(cid:90)
q(f)= q(f |u)q(u)du
(23)
=N(f |K K−1Mu,K +K K−1(Σu−K )K−1K ),
f,u u,u f,f f,u u,u u,u u,u u,f
then,wehave:
(cid:90) (cid:90)
F = q(H) q(f)logp(Y |f)dfdH
(cid:90) (cid:20) ND 1 1 1 (cid:21)
= q(H) − log2πσ2− Y⊤Y+ Y⊤E [f]− E [f⊤f] dH
2 2σ2 σ2 q(f) 2σ2 q(f)
ND 1 (cid:90) (cid:20) 1 (cid:21)
=− log2πσ2− Y⊤Y+ q(H) Y⊤K K−1Mu dH
2 2σ2 σ2 f,u u,u
(24)
−(cid:90) q(H) 1 (cid:2) Tr(K −K−1K K )+Tr(K−1K K K−1(Mu(Mu)⊤+Σu))(cid:3) dH
2σ2 f,f u,u u,f f,u u,u u,f f,u u,u
ND 1 1
=− log2πσ2− Y⊤Y− Tr(K−1ΦK−1(Mu(Mu)⊤+Σu))
2 2σ2 2σ2 u,u u,u
1 1
+ Y⊤ΨK−1Mu− (ψ−Tr(K−1Φ)),
σ2 u,u 2σ2 u,u
whereψ = ⟨Tr(K )⟩ ,Ψ = ⟨K ⟩ andΦ = ⟨K K ⟩ . Noticethatiftherearenomissingvalues,
f,f q(H) f,u q(H) u,f f,u q(H)
Eq. (24)canbefurthersimplifiedbyexploitingpropertiesoftheKroneckerproduct,resultinginequation(8)inDai
etal.[2017]. Butinthegeneralcase(withmissingvalues),weuseEq. (24)tocomputeF foreachoutputd,and
d
F
=(cid:80)D
F .
d=1 d
12ScalableMulti-OutputGaussianProcesseswithStochasticVariationalInference APREPRINT
7.1.1 Computationofstatisticsψ,ΨandΦ
Thestatisticsψ,ΨandΦcanbesimplifiedbyexploitingtheKroneckerproductstructure.
ψ =(cid:10) Tr(KH ⊗KX )(cid:11) =(cid:10) Tr(KH )(cid:11) ⊗Tr(KX )
f,f f,f q(H) f,f q(H) f,f
 
D
=  (cid:88)(cid:10) kH(h d,h d)(cid:11) q(hd)  ⊗Tr(KX f,f)
d=1(cid:124) (cid:123)(cid:122) (cid:125)
ψH
d
Ψ=⟨K ⟩ =(cid:10) KH ⊗KX (cid:11) =(cid:10) KH (cid:11) ⊗KX (25)
f,u q(H) f,u f,u q(H) f,u q(H) f,u
(cid:124) (cid:123)(cid:122) (cid:125)
ΨH
Φ=⟨K K ⟩ =(cid:10)(cid:0) KH ⊗KX (cid:1)(cid:0) KH ⊗KX (cid:1)(cid:11)
u,f f,u q(H) u,f u,f f,u f,u q(H)
=(cid:10) KH KH (cid:11) ⊗(cid:0) KX KX (cid:1)
u,f f,u q(H) u,f f,u
(cid:124) (cid:123)(cid:122) (cid:125)
ΦH
ThestatisticsofψH,ΨH andΦH canbeapproximatedbyMonteCarlomethods. Forsomeparticularkernels,they
d
canbeanalyticallysolved. Forinstance,fortheSE-ARDkernel. Recallq(h ) = N(h | M ,Σ ),withΣ being
d d d d d
diagonalmatrix,s ,i ∈ {1,2,...,Q }denotestheelementsonthediagonal. ithcomponentofM isdenotedas
d,i H d
(cid:68) (cid:69) (cid:68) (cid:69)
m . We are willing to derive analytical formulae for ψH, ΨH = KH andΦH = KH KH ,
d,i
(cid:68) (cid:69) (cid:68) (cid:69) (cid:68)
d
(cid:69)
d fd,u
q(hd)
d u,fd fd,u
q(hd)
noticethat KH KH = KH KH =(ΨH)⊤ΨH ifd̸=d′.
u,fd fd′,u
q(H)
u,fd
q(hd)
f d′,u
q(h d′)
d d′
RecallforSE-ARDkernel,foranyh 1,h
2
∈RQH:
kH(h 1,h 2)=σ H2 exp(− 21(cid:88)QH (h 1,i−
l
h 2,i)2 )=(2π)Q 2Hσ H2 (cid:89)QH l i1 2 N(h
1
|h 2,Diag(l)), (26)
i
i=1 i=1
(cid:124) (cid:123)(cid:122) (cid:125)
c
wherel∈RQH andl iistheithcomponentofl. Thetermψ dH istriviallycomputedasc.
ConsiderlatentinducingvariableszH,zHwithi,j ∈{1,2,...,M },wehave:
i j H
(cid:90)
E (cid:2) kH(h ,zH)(cid:3) =c N(h |zH,Diag(l))N(h |M ,Σ )dh
q(hd) d i d i d d d d
(cid:90)
=cN(zH |M ,Σ +Diag(l)) N(h |E(Diag(l)−1zH +Σ−1M )
i d d d i d d
(cid:124) (cid:123)(cid:122) (cid:125)
e
,(Diag(l)−1+M−1)−1)dh
d d
(cid:124) (cid:123)(cid:122) (cid:125)
E (27)
=cN(zH |M ,Σ +Diag(l))
i d d
=c(2π)−Q 2H (cid:89)QH (l i+s d,i)−1 2 exp(− 21 (cid:88)QH (zH i, li′ − +m
s
d,i′)2 )
i′ d,i′
i=1 i′=1
=σ H2 (cid:89)QH (l i+s d,i)− 21 (cid:89)QH l i1 2 exp(−1
2
(cid:88)QH (zH i, li′ − +m
s
d,i′)2 ).
i′ d,i′
i=1 i=1 i′=1
NoticethesecondlineusestheconclusionoftheproductofGaussians,whichis:
N(x|a,A)N(x|b,B)=zN(x|e,E), (28)
wherez =N(a|b,A+B),ande=E(A−1a+B−1b),E =(A−1+B−1)−1.
13ScalableMulti-OutputGaussianProcesseswithStochasticVariationalInference APREPRINT
(cid:90)
E (cid:2) kH(h ,zH)kH(h ,zH)(cid:3) =c2 N(h |zH,Diag(l))N(h |zH,Diag(l))
q(hd) d i d j d i d j
N(h |M ,Σ )dh
d d d d
(cid:90) zH +zH Diag(l)
=c2N(zH |zH,2Diag(l)) N(h | i j , )
i j d 2 2
N(h |M ,Σ )dh
d d d d
zH +zH Diag(l) (29)
=c2N(zH |zH,2Diag(l))N(M | i j , +Σ )
i j d 2 2 d
=σ4
(cid:89)QH(cid:18) l i(cid:19) 21 (cid:18) l
i +s
(cid:19)− 21 exp(cid:40)
−
1 (cid:88)QH (cid:104)(zH
i,i′
−zH j,i′)2
H 2 2 d,i 2 2l
i′
i=1 i′=1
(m −
zH i,i′+zH
j,i′)2(cid:105)
(cid:41)
+ d,i′ 2 ,
s d,i′ + l 2i
byusingformulaeEq. (27)andEq. (29),bothstatisticsΨH andΦH canbecomputed. Therefore,ψ,Ψ,Φcanalsobe
d d
analyticallysolved.
7.2 Parametrizationtechniqueofq(u)inGS-LVMOGP
WhenQ=1,employingtheparameterizationu=Lu ,asdetailedinSection3.2,resultsincovariancematrixofq(u)
0
alsoexhibitingaKroneckerproductstructure. Thisisbecause:
K=LL⊤ =KH ⊗KX =(cid:0) L L⊤(cid:1) ⊗(cid:0) L L⊤(cid:1) =(L ⊗L )(cid:0) L⊤ ⊗L⊤(cid:1) , (30)
H H X X H X H X
thus,L=L ⊗L and,
H X
q(u)=N(u|M ,Σ )
u u
=N(u|LM ,L(ΣH ⊗ΣX)L⊤) (31)
0 0 0
=N(u|LM ,(L ΣHL⊤)⊗(L ΣXL⊤)).
0 H 0 H X 0 X
ForQ > 1,theCholeskyfactorLcannotbefactorizedingeneral,sothecovariancematrixofq(u)doesnothave
Kroneckerproductstructure.
Another advantage of the proposed parametrization technique is that we have more efficient computation of the
KL(q(u)||p(u))termintheELBO.Firstly,notice
KL(q(u)||p(u))=KL(q(u )||p(u )), (32)
0 0
for two general Gaussian distributions q(u) and p(u), computation of KL(q(u)||p(u)) is based on the following
formula,withO(M3M3)complexity:
X H
(cid:18) (cid:18) (cid:19)(cid:19)
1 detK
KL(q(u)||p(u))= Tr(K−1Σ )−M M +M⊤K−1M +log u,u , (33)
2 u,u u H X u u,u u detΣ
u
while the KL divergence between a Kronecker product structured Gaussian distribution and a standard Gaussian
distribution,theKL(q(u )||p(u ))canbelargelysimplified,withonlycomplexityO(M3 +M3):
0 0 X H
1(cid:18) (cid:18) detI (cid:19)(cid:19)
KL(q(u )||p(u ))= Tr(ΣH⊗ΣX)−M M +M⊤M +log
0 0 2 0 0 H X 0 0 detΣH⊗ΣX
0 0 (34)
1(cid:16) (cid:17)
= Tr(ΣH)Tr(ΣX)−M M +Tr(M⊤M )−M logdetΣX −M logdetΣH .
2 0 0 H X 0 0 H 0 X 0
7.3 Gauss-Hermitequadrature
Gauss-Hermitequadratureisanumericaltechniquespecificallydesignedforcomputingintegralsoffunctionsthathave
aGaussian(orexponential)weightfunction. Thismethodisparticularlyusefulwhendealingwithintegralswhere
theintegrandinvolvesaGaussian-weightedfunction,makingitanidealchoiceforexpectationsinvolvingGaussian
distributionsinprobabilisticmodelling,andinparticular,Gaussianprocessmodels.
14ScalableMulti-OutputGaussianProcesseswithStochasticVariationalInference APREPRINT
InGauss-Hermite,theweightfunctionise−x2,theintegralitapproximateshasform(cid:82) e−x2g(x)dx. Theapproximation
isgivenby:
(cid:90) +∞
e−x2 g(x)dx≈(cid:88)
w g(x ), (35)
i i
−∞ i
wherex aretherootsofthen-thHermitepolynomial,H (x),andw arethecorrespondingweights,calculatedas:
i n i
√
w = 2n−1n! π .
i n2[Hn−1(xi)]2
7.3.1 NumericalintegrationL (H )=E [logp(y |f )]
dn d q(fdn|Hd,xn) dn dn
Recallq(f |H ,x )areGaussiandistributiondenotedasN(f |a,b2),where
dn d n dn
a=K K−1M
fdn,u u,u u
(36)
b=K +K K−1(Σ −K )K−1K
fdn,fdn fdn,u u,u u u,u u,u u,fdn
ToapplyGaussian-Hermitequadrature,wefirstre-writetheintegrationL (H )bychangeofvariables:
dn d
f −a
x= d√n
2b (37)
√
df = 2bdx
dn
Thistransformstheintegralto
(cid:90) +∞ 1 √
e−x2√
logp(y |a+ 2bx)dx,
π dn
−∞
NowweapplyGauss-Hermitequadrature,
(cid:90) +∞ e−x2√1
logp(y
|a+√ 2bx)dx≈(cid:88)n
w
√1
logp(y
|a+√
2bx ), (38)
π dn i π dn i
−∞ i=1
7.3.2 PracticalSteps
• Determinethedegreen:thechoiceofnbalancesbetweencomputationalcostandaccuracy.Inourexperiments,
weusen=20.
• Find x and w : typically be looked up in numerical libraries or computed using software that handles
i i
numericalanalysis.
√
• Evaluatelogp(y |f ): Computethistermatf =a+ 2bx foreachi.
dn dn dn i
• Computetheweightedsum,whichistheapproximationoftheintegral.
7.4 DiscussionaboutICM,LMC,LV-MOGPandGS-LVMOGP
TheLV-MOGPmodelisakintotheIntrinsicCoregionalizationModel(ICM)inthatitutilizesasinglecoregionalization
matrix. Incontrast,theGS-LVMOGPmodelismoresimilartotheLinearModelofCoregionalization(LMC),asit
allowsforthepossibilityofmultiplecoregionalizationmatrices(Q≥1). Table5providesacomparativecomparisonof
thesemethods.
7.5 Integrationoflatentvariablesforprediction
Thepredictionproblemforgivenoutputd∗andinputx∗involvesanintegrationw.r.tuncertainlatentvariablesq(H ),
d∗
asshowninEq. (20). Thisintegrationisingeneralintractable,andrecallinSection3.4,weadoptanapproachto
15ScalableMulti-OutputGaussianProcesseswithStochasticVariationalInference APREPRINT
Table5: Comparisonofmodelsfrom5perspectives: theapproachforcoregionalizationmatrixconstruction,numberof
parametersincoregionalizationmatrices,numberofcoregionalizationmatrices,whetherornotsupportSVItraining
anddatalikelihood. Notetherearerecentworks(denoteas∗intable)onmakingLMCmodelsamenablewithSVIand
non-Gaussianlikelihoods,suchas[Moreno-Muñozetal.,2018].
Models Approach #Parameters #Coreg.Matrices SVI Likelihood
ICM B =A A⊤ DR Q=1 ×∗ Gaussian∗
1 1 1 1
LMC B =A A⊤,q=1,...,Q D(cid:80)Q R Q≥1 ×∗ Gaussian∗
q q q q=1 q
Latentvariables,
LV-MOGP DQ Q=1 × Gaussian
oneperoutput H
Latentvariables,
GS-LVMOGP DQQ Q≥1 ✓ Any
multipleperoutput H
use means of q(H ) to approximate the integral. In this section, we provide an alternative approach: a Gaussian
d∗
approximation(computefirstandsecondmoment)forthepredictivedistribution.
(cid:90)
q(f∗ |x∗)= q(f∗ |H ,x∗)q(H )dH
d∗ d∗ d∗
(cid:90) Q (39)
(cid:89)
= q(f∗ |{h }Q ,x∗) q(h )dh ,
d∗,q q=1 d∗,q d∗,q
q=1
wedenotethemeanandvarianceforp(f∗ |H ,x∗)asλ(H )andγ(H ),where
d∗ d∗ d∗
λ(H )=K K−1Mu
d∗ f∗,u u,u
(40)
γ(H )=K +K K−1(Σu−K )K−1K ,
d∗ f∗,f∗ f∗,u u,u u,u u,u u,f∗
Weconsiderthefirstandsecondmomentforq(f∗ |x∗),whichwedenoteasmandvrespectively.
(cid:90)
m= f∗q(f∗ |x∗)df∗
(cid:90) (cid:90)
= f∗N(f∗ |λ(H ),γ(H ))df∗q(H )dH
d∗ d∗ d∗ d∗
(41)
(cid:90)
= λ(H )q(H )dH
d∗ d∗ d∗
=E [λ(H )]
q(Hd∗) d∗
(cid:90)
v = (f∗)2q(f∗ |x∗)df∗−m2
(cid:90) (cid:90)
= (f∗)2N(f∗ |λ(H ),γ(H ))df∗q(H )dH −m2
d∗ d∗ d∗ d∗
(42)
(cid:90)
= {λ2(H )+γ(H )}q(H )dH −m2
d∗ d∗ d∗ d∗
=E [λ2(H )]+E [γ(H )]−m2,
q(Hd∗) d∗ q(Hd∗) d∗
therearethreetermstocompute: E [λ(H )],E [λ2(H )]andE [γ(H )],
q(Hd∗) d∗ q(Hd∗) d∗ q(Hd∗) d∗
E [λ(H )]=E [K K−1Mu]
q(Hd∗) d∗ q(Hd∗) f∗,u u,u
(43)
=E [K ]K−1Mu,
q(Hd∗) f∗,u u,u
E [λ2(H )]=E [(Mu)⊤K−1K K K−1(Mu)]
q(Hd∗) d∗ q(Hd∗) u,u u,f∗ f∗,u u,u
(44)
=(Mu)⊤K−1E [K K ]K−1Mu
u,u q(Hd∗) u,f∗ f∗,u u,u
16ScalableMulti-OutputGaussianProcesseswithStochasticVariationalInference APREPRINT
E [γ(H )]=E [K +K K−1(Σu−K )K−1K ]
q(Hd∗) d∗ q(Hd∗) f∗,f∗ f∗,u u,u u,u u,u u,f∗
=E [K ]+E (cid:2) K K−1(Σu−K )K−1K (cid:3)
q(Hd∗) f∗,f∗ q(Hd∗) f∗,u u,u u,u u,u u,f∗
(45)
=E [K ]+Tr(E (cid:2) K K−1(Σu−K )K−1K (cid:3) )
q(Hd∗) f∗,f∗ q(Hd∗) f∗,u u,u u,u u,u u,f∗
=E [K ]+Tr(K−1(Σu−K )K−1E [K K )],
q(Hd∗) f∗,f∗ u,u u,u u,u q(Hd∗) u,f∗ f∗,u
integrationoverq(H )appearsinthreeterms: E [K ],E [K ]andE [K K ]. These
d∗ q(Hd∗) f∗,f∗ q(Hd∗) f∗,u q(Hd∗) u,f∗ f∗,u
termscanbefurthersimplifiedbyKroneckerproductdecomposition: first,consider:
(cid:34) Q (cid:35)
(cid:88)
E [K ]=E KH ⊗KX
q(Hd∗) f∗,f∗ q(Hd∗) f∗,f∗;q f∗,f∗;q
q=1
Q
(cid:88)
= E [KH ]⊗KX (46)
q(Hd∗) f∗,f∗;q f∗,f∗;q
q=1
Q
=(cid:88) E (cid:2) KH (cid:3) ⊗KX ,
q(hd∗,q) f∗,f∗;q f∗,f∗;q
q=1
and,
(cid:34) Q (cid:35)
(cid:88)
E [K ]=E KH ⊗KX
q(Hd∗) f∗,u q(Hd∗) f∗,u;q f∗,u;q
q=1
Q
=(cid:88) E (cid:2) KH (cid:3) ⊗KX (47)
q(Hd∗) f∗,u;q f∗,u;q
q=1
Q
=(cid:88) E (cid:2) KH (cid:3) ⊗KX ,
q(hd∗,q) f∗,u;q f∗,u;q
q=1
thenconsider:
(cid:34)(cid:40) Q (cid:41)(cid:40) Q (cid:41)(cid:35)
(cid:88) (cid:88)
E [K K ]=E KH ⊗KX KH ⊗KX
q(Hd∗) u,f∗ f∗,u q(Hd∗) u,f∗;q u,f∗;q f∗,u;q f∗,u;q
q=1 q=1
 
Q Q
=E q(Hd∗)(cid:88)(cid:88)(cid:0) KH u,f∗;q⊗KX u,f∗;q(cid:1)(cid:0) KH
f∗,u;q′
⊗KX f∗,u;q′(cid:1)

q=1q′=1
Q Q
=(cid:88)(cid:88) E (cid:2)(cid:0) KH ⊗KX (cid:1)(cid:0) KH ⊗KX (cid:1)(cid:3) (48)
q(Hd∗) u,f∗;q u,f∗;q f∗,u;q′ f∗,u;q′
q=1q′=1
Q Q
=(cid:88)(cid:88) E (cid:2) KH KH ⊗KX KX (cid:3)
q(Hd∗) u,f∗;q f∗,u;q′ u,f∗;q f∗,u;q′
q=1q′=1
Q Q
=(cid:88)(cid:88) E E (cid:2) KH KH (cid:3) ⊗KX KX ,
q(hd∗,q) q(h d∗,q′) u,f∗;q f∗,u;q u,f∗;q f∗,u;q
q=1q′=1
where
 (cid:104) (cid:105)
E
q(hd∗,q)
KH u,f∗;qKH
f∗,u;q
ifq =q′,
E E (cid:2) KH KH (cid:3) = (49)
q(hd∗,q) q(h d∗,q′) u,f∗;q f∗,u;q′
E (cid:104)
KH
(cid:105)
E
(cid:104)
KH
(cid:105)
ifq ̸=q′
q(hd∗,q) u,f∗;q q(h d∗,q′) f∗,u;q′
Therefore,thekeytocomputemandvreliesonthecomputationoffollowingstatistics:
ψH =E (cid:2) KH (cid:3) , (50)
d∗,q q(hd∗,q) f∗,f∗;q
17ScalableMulti-OutputGaussianProcesseswithStochasticVariationalInference APREPRINT
ΨH =E (cid:2) KH (cid:3) , (51)
d∗,q q(hd∗,q) f∗,u;q
ΦH =E (cid:2) KH KH (cid:3) (52)
d∗,q q(hd∗,q) u,f∗;q f∗,u;q
ThecomputationoftheabovethreestatisticsarethesameasψH,ΨH,ΦH inAppendix7.1.1.
d d d
7.6 ExperimentSettings
Weusethefollowingmetricsintheexperiments: MSE(meansquareerror),RMSE(rootmeansquareerror),SMSE
(standardisedmeansquareerror),andNLPD(negativelogpredictivedensity). yˆ denotespredictionvalueandy
dn dn
referstothegroundtruth:
N D
1 (cid:88)(cid:88)
MSE= (y −yˆ )2, (53)
ND dn dn
n=1d=1
(cid:118)
(cid:117) N D
(cid:117) 1 (cid:88)(cid:88)
RMSE=(cid:116) (y −yˆ )2, (54)
ND dn dn
n=1d=1
SMSE=
1 (cid:88)D 1 (y dn−yˆ dn)2
, (55)
D N (y −y¯train)2
d=1 dn d
1 (cid:88)N (cid:88)D (cid:90)
NLPD= logp(y |f )q(f )df . (56)
ND dn dn dn dn
n=1d=1
WithaGaussianlikelihood,wemakeuseofclosed-formsolutionstotheNLPD,otherwiseweapproximateitusing
GaussianHermitequadrature.
SomehyperparametersusedinexperimentsareshowninTable6.
Table6: M referstothenumberofinducingpointsonthelatentspace,M referstothenumberofinducingpoints
H X
ontheinputspace. Q denotesthedimensionalityofthelatentspace. J isthenumberofthesamplesusedinthe
H
MonteCarloestimationoftheintegrationw.r.t. q(H ). lrreferstolearningrates. Mini-batchsizeandthenumberof
d
iterationsarealsoreported. AllexperimentsuseAdamoptimizer[KingmaandBa,2014].
M M Q J Optimizer Mini-batchsize Iterations lr
H X H
Exchange 20 50 3 3 Adam 500 5000 0.01
USHCN 10 50 2 1 Adam 500 10000 0.1
Spatio-Temporal 10 20 2 1 Adam 500 5000 0.1
NYCCrimeCount 20 50 2 1 Adam 1000 5000 0.1
SpatialTranscriptomics 50 50 3 1 Adam 1000 200000 0.1
TheexperimentsarerunonaMacBookProwithM3Maxand36GRAM.Exceptforspatialtranscriptomicsexperiments,
allexperiments(foreachrun)arefinishedin30minutes. Thespatialtranscriptomicsexperimentstakearound8hours
onthelaptop.
7.6.1 Moreexperimentdetailsonexchangedataset
The ten international currencies are: CAD, EUR, JPY, GBP, CHF, AUD, HKD, NZD, KRW, MXN, and the three
preciousmetalsaregold,silver,andplatinum. WemakeplotsfortheoutputscorrespondingtoCAD,JPYandAUD.We
alsoreporttheerrorbarsforGS-LVMOGPontheexchangedataset,showninTable7. Wetaketheexchangedatasetas
anexampletoexperimentwiththerelationshipbetweenthecomputationaltimeconsumptionandmini-batchsizem .
b
18ScalableMulti-OutputGaussianProcesseswithStochasticVariationalInference APREPRINT
Figure4: PredictionsoftheGS-LVMOGP(Q=3)fortheexchangeratesexperiment. Predictionsareshowninblue.
Theshadedareaisthepredictivemean±oneandtwopredictivestandarddeviations. Trainingdataaredenotedas
blackdots( )andheld-outtestdataasorangedots( ).
Table7: TheexperimentsresultsforGS-LVMOGPonexchangedataset(withstandarddeviation).
GS-LVMOGP
Q=1 Q=2 Q=3
SMSE 0.256±0.1 0.186±0.019 0.167±0.019
NLPD -1.851±0.9 -2.416±0.24 -2.703±0.19
7.6.2 Moreexperimentsdetailsonspatiotemporaldataset
The dataset can be downloaded from https://cds.climate.copernicus.eu/cdsapp#!/dataset/
projections-cmip5-monthly-single-levels?tab=form, and we only consider spatial region plotted in
Fig. 1,thatislongitudefrom13.125◦Wto69.375◦E,latitudefrom33.75◦Nto73.75◦N.Thetemperatureismeasured
bytheKelvinunit.
FortheOILMM[Bruinsmaetal.,2020]method,thereisahyper-parameterm,thatdeterminesthenumberoflatent
processesinthemodel. Wetriedadifferentsettingofmfrom1to100,withpreliminaryexperimentresultsshownin
Table8,andthenwechoosem=10toreportresultsonthemaintableinTable3. Inthepaper,wereportedtheSMSE
metricforextrapolatedoutputswithnotrainingdata. ThecomputationofSMSEforthemisnolongerstandardised
w.r.t. meanofytrainbutmeanofytest.
Table8: OILMMmodelwithdifferentnumberoflatentprocesses(m)
m=1 m=5 m=10 m=20 m=50 m=100
SMSE 0.276 0.252 0.134 0.130 0.788 1.0
imputation
NLPD 2.943 2.944 2.819 36.609 19.202 12.357
SMSE 0.285 0.258 0.155 0.202 0.798 1.0
extrapolation
NLPD 3.045 3.049 3.063 11.022 17.778 13.127
19ScalableMulti-OutputGaussianProcesseswithStochasticVariationalInference APREPRINT
290
280
270
260
250
0 50 100 150 200 250 300 350
Time (month)
280
260
240
0 50 100 150 200 250 300 350
Time (month)
290
285
280
0 50 100 150 200 250 300 350
Time (month)
Figure5: Fromtoptobottom,predictionplotsforlocationsatF,GandHasmarkedinFig. 1. Trainingpointsare
indicatedbyredcrosses( ),imputationtestdatabyblackdots( ),andextrapolationtestpointsbyorangedots( ).
287.5
285.0
282.5
280.0
277.5
275.0
0 50 100 150 200 250 300 350
Time (month)
300
290
280
270
260
0 50 100 150 200 250 300 350
Time (month)
280
270
260
250
240
230
0 50 100 150 200 250 300 350
Time (month)
Figure6: Fromtoptobottom,predictionplotsforlocationsatB,D,andEasmarkedinFig. 1. Theextrapolationtest
pointsaredenotedbyorangedots( ).
20
erutarepmeT
erutarepmeT
erutarepmeT
erutarepmeT
erutarepmeT
erutarepmeTScalableMulti-OutputGaussianProcesseswithStochasticVariationalInference APREPRINT
7.6.3 MoreexperimentdetailsonNYCCrime
Similartothespatiotemporaldataset,wedopreliminaryexperimentsforOILMMwithdifferentnumbersoflatent
processesmontheNYCCrimedataset. TheresultsareshowninTable9,andweagainchoosem = 10formain
experimentsinTable2inthepaper.
Table9: OILMMmodelwithdifferentnumberoflatentprocesses(m)
m=1 m=5 m=10 m=20 m=50 m=100
RMSE 1.856 1.857 1.857 1.858 1.858 1.865
NLPD 1.545 1.493 1.493 1.493 1.500 1.510
Similartospatiotemporalexperiments,wecanalsoencodespatialinformationintothepriordistributionofthelatent
variablesq(H). Werunanexperimentwiththissettingandcompareitagainstmethodsin[Hamelijncketal.,2021],
showninTable10.
Table10: *denotesresultstakenfrom[Hamelijncketal.,2021]. Resultsareaveragesover5cross-validationsandmean
andstandarddeviationarereported. Thebrackets(G)and(P)representGaussianandPoissonlikelihoodsrespectively.
RMSE NLPD
ST-SVGP∗(P) 2.77±0.06 1.66±0.02
MF-ST-SVGP∗(P) 2.75±0.04 1.63±0.02
SVGP-1500∗(P) 3.20±0.14 1.82±0.05
SVGP-3000∗(P) 3.02±0.18 1.76±0.05
Q=1 2.157±0.062 1.500±0.329
GS-LVMOGP(G) Q=2 2.086±0.063 1.357±0.132
Q=3 2.075±0.048 1.363±0.187
Q=1 1.791±0.023 1.287±0.003
GS-LVMOGP(P) Q=2 1.791±0.023 1.287±0.003
Q=3 1.790±0.024 1.287±0.003
WerecordthetimeconsumptionforGS-LVMOGPmodelsontheNYCCrimedataset,andwecomparethesetimes
withthetimeforspatio-temporalGPmodelHamelijncketal.[2021],asshowninTable11.
Table11: Timecomparisonperepoch(cyclingthroughdatasetonce)forGS-LVMOGPandmethodsfrom[Hamelijnck
etal.,2021]. ∗denotesthenumberstakenfrom[Hamelijncketal.,2021]withIntelXeonCPUE5-2698v4CPUand
NVIDIATeslaV100GPU.
Time(CPU) Time(GPU)
ST-SVGP∗ 20.86±0.46 0.61±0.00
MF-ST-SVGP∗ 20.69±0.86 0.32±0.00
SVGP-1500∗ 12.67±0.11 0.13±0.00
SVGP-3000∗ 80.80±3.42 0.45±0.01
Q=1 6.33±0.00 -
GS-LVMOGP Q=2 8.29±0.00 -
Q=3 10.33±0.00 -
7.6.4 USHCN-Daily: moredetails
TheUnitedStatesHistoricalClimatologyNetwork(USHCN)datasetasdetailedbyMenneetal.[2015],includes
recordsoffiveclimatevariablesacrossaspanofover150yearsat1,218meteorologicalstationsthroughouttheUnited
States. Itispubliclyavailableandcanbedownloadedatthefollowingaddress: https://cdiac.ess-dive.lbl.
gov/ftp/ushcn_daily/.Allstatefilescontaindailymeasurementsfor5variables: precipitation,snowfall,snow
depth,minimumtemperature,andmaximumtemperature.
We follow the same data pre-processing procedure as in De Brouwer et al. [2019], and processed data is kindly
provided in https://github.com/edebrouwer/gru_ode_bayes. We further filter out outputs with 2 or less
trainingobservations. Thefinaldatasetintheexperimenthas5,507outputsand289,144trainingdatapoints.
21ScalableMulti-OutputGaussianProcesseswithStochasticVariationalInference APREPRINT
7.6.5 Moreexperimentdetails
SimilartothespatiotemporalandNYCCrimedatasets,wealsodopreliminaryexperimentsforOILMMwithdifferent
numbers of latent processes m on the USHCN dataset. Based on the results shown in Table 12, we again choose
m=10formainexperimentsinTable4.
Table12: OILMMmodelwithdifferentnumberoflatentprocesses(m)
m=1 m=5 m=10 m=20 m=50 m=100
MSE 0.894 0.894 0.893 0.894 0.894 0.894
NLPD 811.39 810.82 810.83 811.37 816.45 822.73
7.6.6 Moredetailsaboutspatialtranscriptomicsexperiment
InFig. (7)weshowthemeangeneexpressionoftheremaining4clusters,where,similarlytocluster3(3c),clusters0
(7a)and1(7b)alignwiththenormaltissuearea. Cluster2(7c)andcluster4(7d)containacombinationofnormaland
tumourregions. Itisgoodtonoteherethatthe5,000highlyvariablegenesusedinthemodelaremainlyexpressedin
thenormalareaofthetissuewhichmakesthatareamoreprevalentintheclusters. Infutureapplications,alargergene
samplewithgenesthatareenrichedtootherareasofthetissuewouldbetterrepresenttheheterogeneityitandtherefore
theclusterswillbebetterresolved.
(a)Cluster0 (b)Cluster1 (c)Cluster2 (d)Cluster4
Figure7: Latentvariablek-meansclusteringresultsoftheprostatecarcinomadataset.
7.7 Limitations
Inthiswork,thecomputationalcomplexityiscarefullyreducedforQ=1bydecompositingmatrixinversion. However,
the computational complexity for Q > 1 has not been well addressed, where a naive matrix inversion complexity
isapplied. Infuturework,itisimportanttoinvestigateanddevelopefficientalgorithmsformatrixinversionwhen
Q>1tofurtheroptimizecomputationalperformance. Additionally,exploringalternativemathematicaltechniques
orapproximationsthatcanmaintainaccuracywhilereducingcomplexitywillbecrucialforadvancingthisareaof
research.
7.8 Broaderimpacts
This research addresses issues in climate modelling and human spatial transcriptomic, with potential benefits for
society. Byenhancingtheaccuracyofclimatemodelprediction,ourworkcontributestoadeeperunderstandingof
environmentaldynamics. This,inturn,canaidinmoreeffectiveplanningandproduction,whilealsoprovidinginsights
intoclimatechangethatarecrucialformitigationstrategies. Ourstudyincludesaspatialtranscriptomicsdataseton
humanprostatecancer. Thisdatasetisinvaluableforadvancingtheunderstandingofdiseaseprogressionandtreatment,
therebyofferingpotentialimprovementsinhealthcareandpatientoutcomes. Wehavenotidentifiedanyaspectsofthis
researchthatposeharmtohumansociety.
22