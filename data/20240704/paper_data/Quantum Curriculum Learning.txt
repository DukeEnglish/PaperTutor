Quantum Curriculum Learning
Quoc Hoan Tran, Yasuhiro Endo, and Hirotaka Oshima
∗
Quantum Laboratory, Fujitsu Research, Fujitsu Limited, Kawasaki, Kanagawa 211-8588, Japan
(Dated: July 3, 2024)
Quantum machine learning (QML) requires significant quantum resources to achieve quantum
advantage. Research should prioritize both the efficient design of quantum architectures and the
developmentoflearningstrategiestooptimizeresourceusage. Weproposeaframeworkcalledquan-
tumcurriculumlearning(Q-CurL)forquantumdata,wherethecurriculumintroducessimplertasks
ordatatothelearningmodelbeforeprogressingtomorechallengingones. Wedefinethecurriculum
criteria based on the data density ratio between tasks to determine the curriculum order. We also
implementadynamiclearningscheduletoemphasizethesignificanceofquantumdatainoptimizing
the loss function. Empirical evidence shows that Q-CurL enhances the training convergence and
the generalization for unitary learning tasks and improves the robustness of quantum phase recog-
nition tasks. Our framework provides a general learning strategy, bringing QML closer to realizing
practical advantages.
Introduction.—Intheemergingfieldofquantumcom- enhances its performance on a specific task over time
puting(QC),thereispromiseinusingquantumcomput- by acquiring and integrating knowledge or patterns from
ers at sufficiently large scales to solve certain machine data. We can improve current QML algorithms by mak-
learning (ML) problems much more efficiently than clas- ing this process more efficient. For example, curricu-
sical methods. This synergy between ML and QC has lum learning [17] is inspired by the human learning pro-
led to the creation of a new field known as quantum ma- cess, based on the intuitive observation that we often
chine learning (QML) [1], even though the practicality begin with simpler concepts before progressing to more
of its applications remains unclear. There is a question complex ones. This insight leads to the development of
as to whether speed is the only metric by which QML a strategy for sampling or task scheduling—a curricu-
algorithmsshouldbejudged[2]. Infact, classicalMLfo- lum—that introduces simpler samples or tasks to the
cuses on identifying and reproducing features from data model before moving on to more challenging ones. Al-
statistics. TheinitialhopeisthatQMLcandetectcorre- though curriculum learning has been extensively applied
lationsintheclassicaldataorgeneratenewpatternsthat andstudiedinclassicalML[18–20],itsexplorationinthe
would be difficult for classical algorithms to achieve [3– QML field, especially regarding quantum data, is still in
7]. However, there is no clear evidence that analyzing the early stages. The most relevant research has focused
classical data inherently requires quantum effects. This on investigating model transfer learning within hybrid
suggestsafundamentalshiftintheresearchcommunity’s classical-quantum neural networks [21]. Typically, this
perspective: it is preferable to use QML on data that is involvesstartingwithapre-trainedclassicalnetworkthat
already quantum in nature [8–13]. is then modified and enhanced by adding a variational
quantum circuit at its final layer. Despite the potential
ThetypicallearningprocessinQMLinvolvesextensive
benefits, there is still a lack of concrete evidence that ef-
exploration within the domain landscape of a loss func-
fectivelyusingacurriculumlearningframeworktosched-
tion. This function measures the discrepancy between
ule tasks and samples improves QML techniques.
the quantum model’s predictions and the actual values,
aiming to locate its minimum using classical computers.
In this Letter, we explore the potential of curriculum
However, the optimization often encounters pitfalls such
learningwithinthecontextofQMLusingquantumdata.
as getting trapped in local minima [14, 15] or barren
We demonstrate the feasibility of implementing curricu-
plateau regions [16]. These scenarios require substan-
lum learning in a quantum framework called quantum
tial quantum computational resources to navigate the
curriculumlearning(Q-CurL).Weconsidertwocommon
loss landscape successfully. Additionally, improving task
scenarios in training QML models. First, a main task,
accuracies necessitates evaluating numerous model con-
which may be challenging or limited by data availabil-
figurations, especially against extensive datasets. Given
ity, can be facilitated through the preparatory parame-
the limitation of current quantum resources, this poses
ter adjustment of an auxiliary task. This auxiliary task
a considerable challenge. Therefore, in designing QML
is comparatively easier or more data-rich. However, it
models,wemustfocusnotonlyontheirarchitecturalas-
is necessary to establish the criteria that make an aux-
pectsbutalsoonefficientlearningstrategiestominimize
iliary task beneficial for a main task. Second, quantum
the use of quantum resources.
data may carry different weights of importance in the
The perspective of quantum resources refocuses our optimization process. Focusing on the partial loss func-
attention on the concept of learning. In ML, learning tion of difficult data could lead to poorer generalization.
refers to the process through which a computer system Weproposetwoprincipalapproachesforthesescenarios:
4202
luJ
2
]hp-tnauq[
1v91420.7042:viXra2
FIG.1. Overviewoftwoprincipalmethodologiesinquantumcurriculumlearning: (a)task-basedand(b)data-basedapproaches.
Inthetask-basedapproach,amodel ,designatedforamaintaskthatmaybechallengingorconstrainedbydataaccessibility,
M
benefitsfrompre-trainingonanauxiliarytask. Thisauxiliarytaskiseitherrelativelysimpler(leftpanelof(a))orhasaricher
dataset(rightpanelof(a)). Inthedata-basedapproach,weimplementadynamiclearningscheduletomodulatedataweights,
thereby emphasizing the significance of quantum data in optimizing the loss function to reduce the generalization error.
task-based Q-CurL [Fig.1(a)], which determines the cur- (m) (m) (m=1,...,M), containingN data
m m
D ⊂X ×Y
riculum order based on the data density ratio between pairs. We focus on supervised learning tasks with in-
tasks, and data-based Q-CurL [Fig.1(b)], which employs put quantum data x(m) in the input space (m) and
i X
a dynamic learning schedule that adjusts data weights, corresponding target quantum data y(m) in the out-
i
thereby prioritizing the importance of quantum data in put space (m) for i = 1,...,N . The training data
m
the optimization. Empirical evidence demonstrates the Y
x(m),y(m) for task are drawn from the prob-
utility of Q-CurL in enhancing training convergence and i i Tm
generalizationwhenlearningunitarydynamics. Further- (cid:16)ability distr(cid:17)ibution P(m)( (m), (m)) with the density
X Y
more, we show that data-based Q-CurL contributes to p(m)( (m), (m)). We assume that all tasks share the
X Y
increased robustness in learning. Specifically, in scenar- same data spaces (m) and (m) , as well as
X ≡ X Y ≡ Y
ios with noisy labels, it can prevent complete memoriza- the same hypothesis h and loss function ℓ for all m.
tionofthetrainingdata,therebyavoidingoverfittingand Depending on the problem, we can decide the curricu-
enhancing generalization performance. lumweightc M,m,wherealargerc M,m indicatesagreater
Task-based Q-CurL.— We formulate a framework for benefit of solving m for improving the performance on
T
task-based Q-CurL. The target of learning is to find a M. We evaluate the contribution of solving task i to
T T
function (or hypothesis, prediction model) h : the main task M by transforming the expected risk of
X → Y T
within a hypothesis set that approximates the true training M as follows (see [22] for more details):
H T
function f mapping x to y such that h(x)
f(x). To evaluate
the∈ coX rrectnes∈
s
Y
of the
hypothesis≈
h
R TM(h)=E(x,y) P(M)[ℓ(h(x),y)]
∼
given the data (x,y), the loss function ℓ : R p(M)(x,y)
is used to measure the approximation
erroY
r
× ℓ(hY (x→
),y)
=E(x,y)
∼P(m)
p(m)(x,y)ℓ(h(x),y) . (3)
between the prediction h(x) and the target y. We aim (cid:20) (cid:21)
to find a hypothesis h that minimizes the expected The curriculum weight c can be determined us-
M,m
∈ H
risk over the distribution P( , ): p(M)(x,y)
X Y ing the density ratio r(x,y) = with-
p(m)(x,y)
R(h):=E(x,y)
∼P( X,
Y)[ℓ(h(x),y)]. (1)
out requiring the density estimation of p(M)(x,y) and
In practice, since the data generation distribution p(m)(x,y). The key idea is to model r(x,y) using a
P( , ) is unknown, we use the observed dataset = linear model rˆ(x,y) := α ϕ(x,y) = NM α ϕ (x,y),
X Y D ⊤ i=1 i i
(x ,y )N to minimize the empirical risk, de- where the vector of basis functions is ϕ(x,y) =
i i i=1 ⊂ X ×Y P
fined as the average loss over the training data: (ϕ 1(x,y),...,ϕ NM(x,y)),andtheparametervectorα=
(α ,...,α ) is learned from data (see [22] for the de-
N
1 NM ⊤
1 tailed derivation, which is referenced from Ref. [23]).
Rˆ(h)= ℓ(h(x ),y ). (2)
N i i The key factor that differentiates this framework from
Xi=1 classicalcurriculumlearningistheconsiderationofquan-
Given a main task , the goal of task-based Q- tum data for x and y, which are assumed to be in the
M
T
CurL is to design a curriculum for solving auxiliary form of density matrices representing quantum states.
tasks to enhance performance compared to solving the Therefore,thebasisfunctionϕ (x,y)isnaturallydefined
l
main task alone. We consider ,..., as the set as the product of global fidelity quantum kernels used to
1 M 1
T T −
of auxiliary tasks. The training dataset for task is compare two pairs of input and output quantum states
m
T3
as ϕ (x,y) = Tr[xx(M)]Tr[yy(M)]. In this way, R (h) To create the main task and auxiliary tasks, we
l l l TM TM
can be approximated as: represent the time evolution of H via the ansatz
XY
V , which is similar to the Trotterized version of
XY
1
Nm
exp( iτH )[11]. Thetargetunitaryforthemaintask,
R (h) rˆ (x(m),y(m))ℓ(h(x(m)),y(m)). (4) − XY
TM ≈ N α i i i i V(M) = LM V(l)(β ) LF V(l) , consists of L = 20
m i=1 XY l=1 l l=1 fixed M
X repeating layers, where each layer V(l)(β ) includes pa-
l
The parameter vector α is estimated via the prob- rameterizQ ed z-rotationsQ RZ (with assigned parameter
lem of minimizing 1 α ⊤Hα h ⊤α + λ α ⊤α, where β l)andnon-parameterizednearest-neighbor√iSWAP=
2 − 2 exp(iπ(σxσx + σyσy )) gates. Additionally, we in-
we consider the regularization coefficient λ for L -norm 8 j j+1 j j+1
2
of α. Here, H is the N M N M matrix with el- clude the fixed-depth unitary L l=F 1V fi( xl) ed with L F = 20
ements H
ll′
= N1
m
N i=m 1ϕ l(x( i× m),y i(m))ϕ l′(x( im),y i(m)), l sa ivy ie tr ys
.
at Sit mh ie lae rn itd y,o kf et eh pe inc girc tu hi eQt sV am(l) eto
β
li ,nc wr eea cs re eae tx epr te hs e-
and h is the N M-dim Pensional vector with elements h l = target unitary for the auxiliary tasks as V(m) =
N1 M W Pe ciN = oM n1 sϕ idl( ex r( i eM a) c, hy i r( ˆM (x) ( i) m. ),y i(m)) as the contribution of L l F=m i1 guV r( el)( 2β (al) ) deL l= pF 1 icV tsfi( xl t) e hd e, w avit eh raL gm e H= S1 d,2 iT s, tm . a. n. c, e19 o. vX eY r 100
the data (x( im),y i(m)) from the auxiliary task Tm to the Q trials of β l andQ V fi( xl) ed between the target unitary of each
main task TM. We define the curriculum weight c M,m as auxiliary task Tm (with L m layers) and the main task
(see [22] for more details): . Wealsoplotthecurriculumweightc inFig.2(a)
M M,m
T
calculatedinEq.(5). Here,weconsidertheunitaryV
c = 1
Nm
rˆ (x(m),y(m)). (5) learning with Q = 4 qubits via the hardware
efficiX enY
t
M,m N α i i ansatz U (θ) [22, 24] and use N = 20 Haar random
m HEA
i=1
X states for input data x(m) in each task . As depicted
We consider the unitary learning task to demonstrate
i Tm
inFig.2(a),c cancapturethesimilaritybetweentwo
M,m
the curriculum criteria based on c . We aim to opti-
M,m tasks, as higher weights imply smaller HS distances.
mizetheparametersθ ofaQ-qubitparameterizedquan-
Next, we propose a Q-CurL game to further exam-
tum circuit U(θ), such that, for the optimized parame-
ine the effect of Q-CurL. In this game, Alice has an
ters θ , U(θ ) can approximate an unknown Q-qubit
opt opt ML model (θ) to solve the main task , but she
M
unitary V (U,V (C2Q)). needs to solM ve all the auxiliary tasks ,...T , first.
Our goal is t∈ o U minimize the Hilbert-Schmidt (HS) We assume the data forgetting in tasT k1 transT fM er− ,1 mean-
distance between U(θ) and V, defined as C HST(θ) := ing that after solving task A, only the trained parame-
1
1
−
d2|Tr[V †U(θ)] |2, where d = 2Q is the dimension of t Be .rs Wθ eA pa rr oe pt or sa en ts hfe err fe od lloa ws it nh ge gi rn ei et dia yl p ala gr oa rm ite ht mers tofo dr et ca idsk
e
the Hilbert space. In the QML-based approach, we can
the curriculum order ... before
accessatrainingdatasetconsistingofinput-outputpairs Ti1 → Ti2 → → TiM=M
training. Starting , we find the auxiliary task
dof rap wu nre froQ m-q tu hb eit diss tt ra it be us
tiD
onQ(N .) If=
we{
ta(
|
kψ e⟩j,V
as|
tψ
h⟩
ej) H}N
j a= a1
r
(i
M −1 ∈
{1,2,...,MTiM
−1 }) with the highest
curricT uiM lu−m1
Q Q weights c . Similarity, to solve , we find the
distribution,wecaninsteadtrainusingtheempiricalloss: corresponi dM in,iM g−a1
uxiliarytask
intT hi eM r−e1
mainingtasks
1 N withthehighestc iM 1,iM
2,T aniM d−s2
oon. Here,curriculum
C DQ(N)(θ):=1
− N
j=1|⟨ψ
j
|V †U(θ) |ψ
j
⟩|2. (6) we Fig ih guts rec
i 2k, (i bk −)1
da er pe icc− ta slc tu hl− eat te rd ais nim ini gla arl ny dto teE stq. lo( s5 s).
of the
X
main task (see Eq. (6)) for different training epochs
The parameterized ansatz unitary U(θ) can be modeled TM
and numbers of training data over 100 trials of parame-
asU(θ)= L U(l)(θ ), consistingofLrepeatinglayers
l=1 l ters’ initialization. In each trial, N Haar random states
ofunitaries Q. EachlayerU(l)(θ l)= K k=1exp( −iθ lkH k)is areusedfortraining,and20Haarrandomstatesareused
composedofK unitaries,whereH k areHermitianopera- for testing. With a sufficient amount of training data
Q
tors, θ l is a K-dimensional vector, and θ = θ 1,...,θ L (N =20), introducing Q-CurL can significantly improve
{ }
is the LK-dimensional parameter vector. the trainability (lower training loss) and generalization
We present a benchmark of Q-CurL for learning (lower test loss) when compared with random order in
the approximation of the unitary dynamics of the Q-CurL game. Even with a limited amount of training
spin-1/2 XY model with the Hamiltonian H XY = data (N = 10), when overfitting occurs, Q-CurL still
Q j=1 σ jxσ jx +1+σ jyσ jy +1+h jσ jz , where h j ∈ R and performs better than the random order.
σx,σy,σz arethePaulioperatorsactingonqubitj. This Data-basedQ-CurL.—Next,wepresentaformofdata-
Pj j(cid:0) j (cid:1)
model is important in the study of quantum many-body based Q-CurL that dynamically predicts the easiness of
physics, as it provides insights into quantum phase tran- each sample at each training epoch, such that easy sam-
sitions and the behavior of correlated quantum systems. ples are emphasized with large weights during the early4
FIG. 2. (a) The curriculum weight (lower panel) and the Hilbert-Schmidt distance (upper panel) between the target unitary
of the main task and the target unitary of the auxiliary task . (b) The training loss and test loss for different training
M m
T T
epochs and different numbers N of training data in the Q-CurL game, considering both random and Q-CurL orders. The
average and standard deviations are calculated over 100 trials.
with corrupted labels). The minimization problem is re-
duced to min min Rˆ(h,w), where θ is the parameter
θ w
of the hypothesis h. Here, min Rˆ(h,w) is decomposed
w
at each loss ℓ and solved without quantum resources as
i
w =argmin (l η)ew+γw2. To control the difficulty
i w i −
of the samples, in each training epoch, we set η as the
average value of all ℓ obtained from the previous epoch.
i
Therefore,η changesdynamicallyduringtheearlystages
of training but remains almost constant during the con-
vergence periods.
We apply the data-based Q-CurL to the quantum
phase recognition task investigated in Ref. [9] to demon-
FIG. 3. The test loss and accuracy of the trained QCNN
strate that it can improve the generalization of the
(withandwithoutusingthedata-basedQ-CurL)inthequan-
learning model. Here, we consider a one-dimensional
tumphaserecognitiontaskwith8qubitsundervaryingnoise
cluster Ising model with open boundary conditions,
levelsincorruptedlabels. Here,theaverageandthebestper-
formance over 50 trials are plotted. whose Hamiltonian with Q qubits is given by H =
− Q j=− 12σ jzσ jx +1σ jz +2 − h 1 Q j=1σ jx − h 2 Q j=− 11σ jxσ jx +1.
Dependingonthecouplingconstants(h ,h ),theground
1 2
P P P
stages of training and conversely. Apart from improv- state wave function of this Hamiltonian can exhibit mul-
ing generalization, the benefit of data-based Q-CurL lies tiple states of matter, such as the symmetry-protected
in its resistance to noise, which is especially needed in topological phase, the paramagnetic state, and the anti-
QML. Existing QML models can accurately fit partially ferromagnetic state. We employ the quantum convolu-
corrupted labels to quantum states in the training data tional neural network (QCNN) model [9] with binary
but fail on the test data [25]. We show that data-based cross-entropy loss for training. Without Q-CurL, we use
Q-CurL can enhance the robustness based on the dy- theconventionallossRˆ(h)=(1/N) N ℓ forthetrain-
i=1 i
namic weighting ofthe difficulty fitting quantumdata to ing and test phase. In data-based Q-CurL, we train the
corrupted labels. QCNN with the loss Rˆ(h,w) whileP using Rˆ(h) to eval-
Inspiredbytheconfidence-awaretechniquesinclassical uate the generalization on the test data set. We use 40
ML [18–20], the idea is to modify the empirical risk as and400groundstatewavefunctionsforthetrainingand
test phase, respectively (see [22] for details of settings).
N
Rˆ(h,w)= 1 (ℓ η)ewi +γw2 . (7) Toevaluatetheeffectivenessofthedata-basedQ-CurL
N i − i in considering the difficulty of the data in training, we
i=1
X(cid:0) (cid:1) consider the scenario of fitting corrupted labels. Given
Here, w =(w ,...,w ), ℓ =ℓ(h(x ),y ), and w2 is the a probability p (0 p 1) representing the noise level,
1 N i i i i ≤ ≤
regularization term controlled by the hyper-parameter thelabely 0,1 ofquantumstate ψ istransformed
i i
∈{ } | ⟩
γ >0. Thethresholdη distinguisheseasyandhardsam- to the label 1 y with probability p, while it remains
i
−
ples with ewi emphasizing the loss l
i
η (easy sample) the true label with probability 1 p. Figure 3 illustrates
≪ −
andneglectingthelossl η(hardsamples,suchasdata the performance of trained QCNN on test data across
i
≫5
different noise levels. There is no significant difference [12] K. Chinzei, Q. H. Tran, K. Maruyama, H. Oshima, and
at low noise levels, but as the noise level increases, the S. Sato, Splitting and parallelizing of quantum convolu-
conventional training fails to generalize effectively. In tional neural networks for learning translationally sym-
metric data, Phys. Rev. Res. 6, 023042 (2024).
thiscase,introducingdata-basedQ-CurLintraining(red
[13] Q. H. Tran, S. Kikuchi, and H. Oshima, Variational de-
lines)reducesthetestlossandenhancestestingaccuracy
noising for variational quantum eigensolver, Phys. Rev.
compared to the conventional method (blue lines).
Res. 6, 023181 (2024).
Discussion.— The proposed Q-CurL framework can [14] L. Bittel and M. Kliesch, Training variational quantum
enhancetrainingconvergenceandgeneralizationinQML algorithms is np-hard, Phys. Rev. Lett. 127, 120502
with quantum data. Future research should investigate (2021).
whether Q-CurL can be designed to improve trainabil- [15] E. R. Anschuetz and B. T. Kiani, Quantum variational
algorithms are swamped with traps, Nat. Commun. 13,
ity in QML, particularly by avoiding the barren plateau
7760 (2022).
problem. For instance, curriculum design is not limited
[16] J.R.McClean,S.Boixo,V.N.Smelyanskiy,R.Babbush,
totasksanddatabutcanalsoinvolvetheprogressivede-
and H. Neven, Barren plateaus in quantum neural net-
sign of the loss function. Even when the loss function of worktraininglandscapes,Nat.Commun.9,4812(2018).
thetargettask, designedforinfeasibilityinclassicalsim- [17] Y. Bengio, J. Louradour, R. Collobert, and J. Weston,
ulation to achieve quantum advantage [26], is prone to Curriculumlearning,Proceedingsofthe26thAnnualIn-
the barren plateau problem, a well-designed sequence of ternational Conference on Machine Learning ICML’09,
41–48 (2009).
classicallysimulablelossfunctionscanbebeneficial. Op-
[18] D. Novotny, S. Albanie, D. Larlus, and A. Vedaldi,
timizing these functions in a well-structured curriculum
Self-supervised learning of geometrically stable features
beforeoptimizingthemainfunctionmaysignificantlyim-
through probabilistic introspection, in 2018 IEEE/CVF
provethetrainabilityandperformanceofthetargettask. ConferenceonComputerVisionandPatternRecognition
TheauthorsacknowledgeKokiChinzeiandYuichiKa- (IEEE, 2018).
mata for fruitful discussions. [19] S. Saxena, O. Tuzel, and D. DeCoste, Data parameters:
A new family of parameters for learning a differentiable
curriculum, in Advances in Neural Information Process-
ingSystems,Vol.32,editedbyH.Wallach,H.Larochelle,
A.Beygelzimer,F.d'Alch´e-Buc, E.Fox,andR.Garnett
(Curran Associates, Inc., 2019).
∗ tran.quochoan@fujitsu.com
[20] T. Castells, P. Weinzaepfel, and J. Revaud, Super-
[1] J. Biamonte, P. Wittek, N. Pancotti, P. Rebentrost,
loss: A generic loss for robust curriculum learning,
N.Wiebe,andS.Lloyd,Quantummachinelearning,Na-
in Advances in Neural Information Processing Systems,
ture 549, 195 (2017).
Vol.33,editedbyH.Larochelle,M.Ranzato,R.Hadsell,
[2] M. Schuld and N. Killoran, Is quantum advantage the
M. Balcan, and H. Lin (Curran Associates, Inc., 2020)
right goal for quantum machine learning?, PRX Quan-
pp. 4308–4319.
tum 3, 030101 (2022).
[21] A.Mari,T.R.Bromley,J.Izaac,M.Schuld,andN.Killo-
[3] V. Havl´ıˇcek, A. D. Co´rcoles, K. Temme, A. W. Harrow,
ran,Transferlearninginhybridclassical-quantumneural
A. Kandala, J. M. Chow, and J. M. Gambetta, Super-
networks, Quantum 4, 340 (2020).
vised learning with quantum-enhanced feature spaces,
[22] See Supplemental Materials for details of the derivation
Nature 567, 209 (2019).
the curriculum weight in the task-based Q-CurL, the
[4] M. Schuld and N. Killoran, Quantum machine learning
model and data’s settings of quantum phase recognition
in feature Hilbert spaces, Phys. Rev. Lett. 122, 040504
task, and several additional results.
(2019).
[23] T.Kanamori,S.Hido,andM.Sugiyama,Aleast-squares
[5] Y.Liu,S.Arunachalam,andK.Temme,Arigorousand
approach to direct importance estimation, J. Mach.
robust quantum speed-up in supervised machine learn-
Learn. Res. 10, 1391 (2009).
ing, Nat. Phys. (2021).
[24] P. K. Barkoutsos, J. F. Gonthier, I. Sokolov, N. Moll,
[6] T.Goto,Q.H.Tran,andK.Nakajima,Universalapprox-
G.Salis,A.Fuhrer,M.Ganzhorn,D.J.Egger,M.Troyer,
imationpropertyofquantummachinelearningmodelsin
A. Mezzacapo, S. Filipp, and I. Tavernelli, Quantum al-
quantum-enhancedfeaturespaces,Phys.Rev.Lett.127,
gorithms for electronic structure calculations: Particle-
090506 (2021).
hole hamiltonian and optimized wave-function expan-
[7] X. Gao, E. R. Anschuetz, S.-T. Wang, J. I. Cirac, and
sions, Phys. Rev. A 98, 022322 (2018).
M.D.Lukin,Enhancinggenerativemodelsviaquantum
[25] E. Gil-Fuster, J. Eisert, and C. Bravo-Prieto, Under-
correlations, Phys. Rev. X 12, 021037 (2022).
standing quantum machine learning also requires re-
[8] Seekingaquantumadvantageformachinelearning,Nat.
thinking generalization, Nat. Comm. 15, 2277 (2024).
Mach. Intell. 5, 813–813 (2023).
[26] M. Cerezo, M. Larocca, D. Garc´ıa-Mart´ın, N. L. Diaz,
[9] I. Cong, S. Choi, and M. D. Lukin, Quantum convolu-
P. Braccia, E. Fontana, M. S. Rudolph, P. Bermejo,
tional neural networks, Nat. Phys. 15, 1273 (2019).
A. Ijaz, S. Thanasilp, E. R. Anschuetz, and Z. Holmes,
[10] E.Perrier,A.Youssry,andC.Ferrie,Qdataset,quantum
Does provable absence of barren plateaus imply classi-
datasets for machine learning, Sci. Data 9, 582 (2022).
cal simulability? Or, why we need to rethink variational
[11] T. Haug and M. S. Kim, Generalization with
quantum computing, arXiv 10.48550/arxiv.2312.09121
quantum geometry for learning unitaries, arXiv
(2023).
10.48550/arXiv.2303.13462 (2023).Supplementary Material for “Quantum Curriculum Learning”
Quoc Hoan Tran, Yasuhiro Endo, and Hirotaka Oshima
∗
Quantum Laboratory, Fujitsu Research, Fujitsu Limited, Kawasaki, Kanagawa 211-8588, Japan
(Dated: July 3, 2024)
Thissupplementarymaterialdescribesindetailthecalculations,theexperimentspresentedinthe
main text, and the additional figures. The equation, figure, and table numbers in this section are
prefixedwithS(e.g.,Eq. (S1)orFig.S1,TableS1),whilenumberswithouttheprefix(e.g.,Eq.(1)
or Fig. 1, Table 1) refer to items in the main text.
CONTENTS
I. Task-based Q-CurL 1
A. Derive the curriculum weight 1
B. Unitary learning task 3
C. Additional results and other variations of the Q-CurL game 4
II. Quantum phase recognition task with data-based Q-CurL 5
References 6
I. TASK-BASED Q-CURL
A. Derive the curriculum weight
Weformulateaframeworkfortask-basedQ-CurLtoderivethecurriculumweight. InclassicalML,itiswell-known
that learning from multiple tasks can lead to better and more efficient algorithms. This idea encompasses areas such
as transfer learning, multitask learning, and meta-learning, all of which have significantly advanced deep learning.
Unlike classical ML, which typically assumes a fixed amount of training data for all tasks, in quantum learning, the
order of tasks and the allocation of training data to each task are even more critical. Properly scheduling tasks could
reduce the resources required for training the main task, bringing QML closer to practical, real-world applications.
The target of learning is to find a function (or hypothesis, prediction model) h: within a hypothesis set
X →Y H
that approximates the true function f mapping x to y such that h(x) f(x). To evaluate the correctness
∈X ∈Y ≈
of the hypothesis h given the data (x,y), the loss function ℓ : R is used to measure the approximation
Y ×Y →
error ℓ(h(x),y) between the prediction h(x) and the target y. We aim to find a hypothesis h that minimizes the
∈H
expected risk over the distribution P( , ):
X Y
R(h):=E(x,y)
P( ,
)[ℓ(h(x),y)]. (S1)
∼ X Y
Inpractice,sincethedatagenerationdistributionP( , )isunknown,weusetheobserveddataset =(x ,y )N
X Y D i i i=1 ⊂
to minimize the empirical risk (training loss), defined as the average loss over the training data:
X ×Y
N
1
Rˆ(h)= ℓ(h(x ),y ). (S2)
i i
N
i=1
X
In a similar way, we can use Eq. (S2) to define the test loss as the average loss over the test data.
Given a main task , the goal of task-based Q-CurL is to design a curriculum for solving auxiliary tasks to
M
T
enhance performance compared to solving the main task alone. We consider ,..., as the set of auxiliary
1 M 1
tasks. Thetrainingdatasetfortask is (m) (m) (m=1,...,M),coT ntainingT N− datapairs. Wefocuson
m m m
T D ⊂X ×Y
supervised learning tasks with input quantum data x(m) in the input space (m) and corresponding target quantum
i X
∗ tran.quochoan@fujitsu.com
4202
luJ
2
]hp-tnauq[
1v91420.7042:viXra2
data y(m) in the output space (m) for i = 1,...,N . The training data x(m),y(m) for task are drawn from
i Y m i i Tm
the probability distribution P(m)( (m), (m)) with the density p(m)( (m),(cid:16)(m)). We a(cid:17)ssume that all tasks share the
X Y X Y
same data spaces (m) and (m) , as well as the same hypothesis h and loss function ℓ for all m in this
X ≡ X Y ≡ Y
framework. Depending on the problem, we can decide the curriculum weight c , where a larger c indicates a
M,m M,m
greater benefit of solving for improving the performance on . We evaluate the contribution of solving task
m M i
T T T
to the main task by transforming the expected risk of training as follows:
M M
T T
R TM(h)=E(x,y) P(M)[ℓ(h(x),y)] (S3)
∼
= ℓ(h(x),y)p(M)(x,y)d(x,y) (S4)
Z Z(x,y)
p(M)(x,y)
= ℓ(h(x),y)p(m)(x,y)d(x,y) (S5)
p(m)(x,y)
Z Z(x,y)
p(M)(x,y)
=E(x,y)
∼P(m)
p(m)(x,y)ℓ(h(x),y) . (S6)
(cid:20) (cid:21)
p(M)(x,y)
The curriculum weight c can be determined using the density ratio r(x,y) = without requiring
M,m p(m)(x,y)
the density estimation of p(M)(x,y) and p(m)(x,y). Similar to the unconstrained least-squares importance fitting
approach [1] in classical ML, the key idea is to model the density ratio function r(x,y) using a linear model:
NM
rˆ(x,y):=α ϕ(x,y)= α ϕ (x,y), (S7)
⊤ i i
i=1
X
wherethevectorofbasisfunctionsisϕ(x,y)=(ϕ (x,y),...,ϕ (x,y)),andtheparametervectorα=(α ,...,α )
1 NM 1 NM ⊤
is learned from data.
Thekeyfactorthatdifferentiatesthisframeworkfromclassicalcurriculumlearningistheconsiderationofquantum
data for x and y, which are assumed to be in the form of density matrices representing quantum states. Therefore,
the basis function ϕ (x,y) is naturally defined as the product of global fidelity quantum kernels used to compare two
l
pairs of input and output quantum states as:
ϕ (x,y)=Tr[xx(M)]Tr[yy(M)]. (S8)
l l l
In this way, R (h) can be approximated by
TM
R TM(h) ≈E(x,y) ∼P(m)[rˆ α(x,y)ℓ(h(x),y)], (S9)
or, as an approximation, using the following sample averages:
1
Nm
R (h) rˆ (x(m),y(m))ℓ(h(x(m)),y(m)). (S10)
TM ≈ N α i i i i
m
i=1
X
The parameter vector α is estimated by minimizing the following error:
1
[rˆ (x,y) r(x,y)]2p(m)(x,y)dxdy (S11)
α
2 −
Z Z
1
= rˆ (x,y)2p(m)(x,y)dxdy rˆ (x,y)p(M)(x,y)dxdy+C. (S12)
α α
2 −
Z Z Z
Given the training data, we can further reduce the minimization of Eq. (S12) to the problem of minimizing
1
Nm
1
NM
λ
rˆ2(x(m),y(m)) rˆ (x(M),y(M))+ α 2, (S13)
2N α i i − N α i i 2∥ ∥2
m M
i=1 i=1
X X
where we consider the regularization coefficient λ for L -norm of α. Equation (S13) can be further reduced to the
2
following quadratic form:
1 λ
min α Hα h α+ α α. (S14)
⊤ ⊤ ⊤
α 2 − 23
1
Here, H is the N N matrix with elements H = Nmϕ (x(m),y(m))ϕ (x(m),y(m)), and h is the N -
M × M ll′ N i=1 l i i l′ i i M
m
dimensional vector with elements h = 1 NM ϕ (x(M),y(MP)).
l NM i=1 l i i
In the task-based Q-CurL framework, we can consider each rˆ(x(m),y(m)) in Eq. (S10) as the contribution of
P i i
the data (x(m),y(m)) from the auxiliary task to the main task . From Eq. (S10), we note that only the
i i Tm TM
quantity ℓ(h(x(m)),y(m)) depends on the training performance of the auxiliary task . We assume that the loss
i i Tm
ℓ(h(x(m)),y(m))isboundedbyaquantityℓ(m) foralli=1,...,N . ThentheempiricalriskR (h)canbebounded
i i max m TM
by the following inequality:
1 Nm ℓ(m) Nm
R (h) rˆ (x(m),y(m))ℓ(h(x(m)),y(m)) max rˆ (x(m),y(m))=ℓ(m)c , (S15)
TM ≈ N α i i i i ≤ N α i i max M,m
m m
i=1 i=1
X X
where
1
Nm
c = rˆ (x(m),y(m)). (S16)
M,m N α i i
m
i=1
X
Therefore, c evaluates the effect of minimizing ℓ(m) on the empirical risk in training the main task . A large
M,m max M
T
(small) c means that reducing ℓ(m) has a greater (less) contribution to minimizing R (h). In our task-based
M,m max TM
Q-CurL framework, we define c as the curriculum weight.
M,m
B. Unitary learning task
As a demonstration of the curriculum criteria based on c , we consider the unitary learning task. Here, we
M,m
aim to optimize the parameters θ of a Q-qubit parameterized quantum circuit U(θ), such that, for the optimized
parameters θ opt, U(θ opt) can approximate an unknown Q-qubit unitary V (U,V (C2Q)). Our goal is to minimize
∈U
the Hilbert-Schmidt (HS) distance between U(θ) and V, defined as:
1
C (θ):=1 Tr[V U(θ)]2, (S17)
HST − d2| † |
where d = 2Q is the dimension of the Hilbert space. This HS distance is equivalent to the average fidelity between
two evolved states under U(θ) and V from the same initial state ψ drawn from the Haar uniform distribution of
| ⟩
states:
d+1
C HST(θ)= d E |ψ ⟩∼Haarn 1 −|⟨ψ |V †U(θ) |ψ ⟩|2 . (S18)
(cid:2) (cid:3)
ThissuggestsaQML-basedapproachtolearnthetargetunitaryV,wherewecanaccessatrainingdatasetconsisting
of input-output pairs of pure Q-qubit states (N)= (ψ ,V ψ ) N drawn from the distribution . If we take
as the Haar distribution, we can instead trD aQin using{ th| e ⟩ ej mpir| ica⟩ lj l} oj s= s1 Q
Q
N
1
C (θ):=1 ψ V U(θ)ψ 2. (S19)
DQ(N) − N |⟨ j | † | j ⟩|
j=1
X
In variational quantum algorithms, the parameterized ansatz unitary U(θ) can be modeled as U(θ)= L U(l)(θ ),
l=1 l
consisting of L repeating layers of unitaries. Each layer U(l)(θ ) = K exp( iθ H ) is composed of K unitaries,
l k=1 − lk k Q
where H are Hermitian operators, θ is a K-dimensional vector, and θ = θ ,...,θ is the LK-dimensional
k l 1 L
Q { }
parameter vector.
We present a benchmark of Q-CurL for learning the approximation of the unitary dynamics for the spin-1/2 XY
model with the following Hamiltonian:
Q
H = σxσx +σyσy +h σz , (S20)
XY j j+1 j j+1 j j
j=1
X(cid:0) (cid:1)4
where h j
∈
R and σ jx,σ jy,σ jz are the Pauli operators acting on qubit j. This model is important in the study of
quantum many-body physics, as it provides insights into quantum phase transitions and the behavior of correlated
quantum systems.
Tocreatethesituationofmaintask andauxiliarytasks, werepresentthetimeevolutionofH viatheansatz
M XY
T
V , which is similar to the Trotterized version of exp( iτH ) [2]. The unitary for the main task consisting of
XY XY
−
L =20 repeating layers is defined as follows:
M
LM LF
V(M) = V(l)(β ) V(l) , (S21)
XY l fixed
l=1 l=1
Y Y
whereeachlayerV(l)(β )includesparameterizedz-rotationsRZ(withassignedparameterβ )andnon-parameterized
l l
nearest-neighbor √iSWAP = exp(iπ(σxσx + σyσy )) gates. Additionally, we include the fixed-depth unitary
8 j j+1 j j+1
LF V(l) with L = 20 layers at the end of the circuit V(l) to increase expressivity. Similarity, keeping the same
l=1 fixed F
parameters β , we create the target unitary for the auxiliary tasks as
l m
Q T
Lm LF
V(m) = V(l)(β ) V(l) , (S22)
XY l fixed
l=1 l=1
Y Y
with L =1,2,...,19.
m
In our experiments, we consider the unitary V(m) learning with Q = 4 qubits via the hardware efficient ansatz
XY
U (θ). This ansatz comprises multiple blocks, where each block consists of single-qubit operations spanned by
HEA
SU(2) on all qubits and two-qubit controlled-X entangling gates [3] repeated for all pairs of neighbor qubits. Here,
we use rotation operators of Pauli Y and Z as single qubit gates. Mathematically, U (θ) is defined as follows:
HEA
LE Q Q
U (θ)= Uq,l(θ) U Uq,0(θ) ,
HEA R × Ent !× R
Yl=1 q Y=1h i q Y=1h i
with Q qubits consisting of L entangling gates U alternating with Q(L +1) rotation gates on each qubit. Here,
E Ent E
we use U (θ) = R (θ )R (θ ), and U is composed of CNOT gates placed in linear with indexes (q,q +1) of
R Y 1 Z 2 Ent
qubits. The number of parameters in this circuit is Q(2L +1).
E
C. Additional results and other variations of the Q-CurL game
Figure S1 depicts the distribution and density of the train loss and test loss of the main task in the Q-CurL game,
comparing the Q-CurL order with a random order. Here, N =20 random input data are trained for 20 epochs with
100 trials of initial parameters in the model, and N = 20 data are tested for each trained model. We consider two
types of random inputs as (a) Haar-random Q-qubit states and (b) products of Q Haar-random single-qubit states.
In both types of input states, the order in solving the Q-CurL game derived via the task-based Q-CurL method
outperforms the performance when considering the random order.
TheQ-CurLgamesettingandtheheuristicgreedyalgorithmdiscussedinthemaintextdemonstratetheusefulness
of using curriculum weight to decide the curriculum order. We can further explore several variations of the Q-CurL
game. Forinstance,insteadofusingthetestloss (M) ofthemaintask astheevaluationmetricforthecurriculum
Lt TM
order ... , one could consider minimizing the total test loss M (ik) . This approach would
Ti1 →Ti2 → →TiM=M k=2Lt
lead to a heuristic algorithm aimed at maximizing the total curriculum weights M c . Another variation is
Pk=2 ik,ik 1
to consider the “task difficulty” perspective. For example, we could set the first task to b−e solved initially (as we
P
knowitiseasytosolve, orwealreadyhaveatrainedmodel)andthendetermineanoptimaltaskorderthatsmoothly
transitions from the first task to the main task.5
FIG. S1. The distribution and density of the training cost and test cost of the main task in the Q-CurL game, considering
both random order and Q-CurL order based on the curriculum weights. Here, N = 20 random input data are trained for 20
epochswith100trialsofinitialparametersinthemodel,andN =20dataaretestedforeachtrainedmodel. Weconsidertwo
types of random input as (a) Haar-random Q-qubit states and (b) products of Q Haar-random single-qubit states.
II. QUANTUM PHASE RECOGNITION TASK WITH DATA-BASED Q-CURL
We apply the data-based Q-CurL to the quantum phase recognition task investigated in Ref. [4] to demonstrate
that it can improve the generalization of the learning model. Here, we consider a one-dimensional cluster Ising model
with open boundary conditions, whose Hamiltonian with Q qubits is given by
Q 2 Q Q 1
− −
H = σzσx σz h σx h σxσx . (S23)
− i i+1 i+2− 1 i − 2 i i+1
i=1 i=1 i=1
X X X
Dependingonthecouplingconstants(h ,h ),thegroundstatewavefunctionofthisHamiltoniancanexhibitmultiple
1 2
states of matter, such as the symmetry-protected topological phase (SPT phase), the paramagnetic state, and the
anti-ferromagnetic state.
Weemploythequantumconvolutionalneuralnetwork(QCNN)model[4]todeterminethematterphaseofquantum
states. Inspired by classical convolutional neural networks, the QCNN model consists of convolutional, pooling, and
fully connected layers. The convolutional layers use local unitary gates to extract local features from the input data,
while the pooling layers reduce the number of qubits. This alternation of layers ends in a fully connected layer that
functions as a single convolution operator on the remaining qubits, providing an output through the measurement
of the final qubit. The QCNN is governed by variational parameters that are optimized to classify training data
accurately. In our implementation, the convolutional and fully connected layers are constructed using the Pauli
decomposition of two-qubit unitary gates, expressed as 1 j=5 1e −iθjPj, where {P
j
}
are the Pauli operators for two
qubits, excludingtheidentitymatrix. Eachlayerutilizesthesameparametersforallunitarygates. Beforemeasuring
Q
the output, we apply the Hadamard gate to the remaining qubit and then perform a measurement in the Z-basis.
For each training quantum data ψ and its corresponding label y , the QCNN produces the output q . The single
i i i
| ⟩
loss ℓ is defined using the binary cross-entropy (BCE) loss as follows:
i
ℓ = y log(yˆ) (1.0 y )log(1.0 yˆ), (S24)
i i i i i
− − − −
where yˆ = sigmoid(µq 0.5). Here, we consider the scaling output with the coefficient µ = 2.0. The label is
i i
−
predicted as 0 if yˆ <0.5 and 1 if yˆ 0.5. In the procedure without using the Q-CurL, we use the conventional loss
i i
1 ≥
Rˆ(h)= N ℓ for the training and testing phase. In data-based Q-CurL, we train the QCNN with the loss
N i=1 i
P
N
1
Rˆ(h,w)= (ℓ η)ewi +γw2 , (S25)
N i − i
i=1
X(cid:0) (cid:1)
and the procedure min min Rˆ(h,w) mentioned in the main text. Here, min Rˆ(h,w) is decomposed at each loss ℓ
θ w w i
and solved without quantum resources as
w =argmin (l η)ew+γw2. (S26)
i w i −
l η
i
Since we consider the regularization parameter γ >0, given a= − , we can reduce Eq. (S26) into the following
γ
form: w = argmin g(w), with g(w) = aew +w2 is the function of the scalar variable w. Here, we set γ = 1.0. To
i w6
FIG.S2. Thetestloss(leftpanel)andtestaccuracy(rightpanel)ofthetrainedQCNNonthequantumphaserecognitiontask
with (solid lines) or without (dotted lines) using the data-based Q-CurL over different numbers of qubits. Here, we consider
two different noise levels in the corrupted training labels: p=0.2 (blue) and p=0.3 (red).
control the difficulty of the samples, in each training epoch, we set η as the average value of all ℓ obtained from the
i
previous epoch. We use the conventional loss Rˆ(h) to evaluate the generalization on the test data set.
SimilartothesetupinRef.[4],wegenerateatrainingsetof40groundstatewavefunctionscorrespondingtoh =0
2
and h sampled at equal intervals in [0.0, 1.6]. The state is analytically solvable for these parameter choices, and this
1
solution is used to label the training dataset (0 for the paramagnetic or antiferromagnetic phase and 1 for the SPT
phase). Thegroundtruthphaseboundaries,whichseparatethetwophases,aredeterminedusingDMRGsimulations.
Based on these boundaries, we also create a test dataset of 400 ground state wave functions corresponding to h
2
∈
0.8439, 0.6636, 0.5033, 0.3631, 0.2229, 0.09766, -0.02755, -0.1377, -0.2479, -0.3531 , and h sampled 40 times at
1
{ }
equal intervals in [0.0, 1.6]. The optimization is performed by the Adam method with a learning rate of 0.001 and
500 epochs of training.
Inourexperiment,weconsiderthescenariooffittingcorruptedlabels. Givenaprobabilityp(0 p 1)representing
≤ ≤
thenoiselevel,thelabely 0,1 ofquantumstate ψ istransformedtothecorruptedlabel1 y withprobability
i i i
∈{ } | ⟩ −
p, while it remains the true label with probability 1 p.
−
Figure S2 illustrates the average performance of trained QCNN on test data with noise levels p = 0.2,0.3 in
corruptedtraininglabelsoverdifferentnumbersofqubits. Introducingdata-basedQ-CurL(solidlines)inthetraining
process reduces the test loss and enhances testing accuracy compared to the conventional training method (dotted
lines). We note that introducing noise in the training labels leads to worse generalization in the system with fewer
qubits. The small QCNN model struggles to extract the correct phase of the quantum data with limited information.
However, as the number of qubits increases, more information is provided in the quantum wave functions for the
QCNN to extract, thereby improving the robustness in phase detection tasks.
[1] T. Kanamori, S. Hido, and M. Sugiyama, A least-squares approach to direct importance estimation, J. Mach. Learn. Res.
10, 1391 (2009).
[2] T. Haug and M. S. Kim, Generalization with quantum geometry for learning unitaries, arXiv 10.48550/arXiv.2303.13462
(2023).
[3] P.K.Barkoutsos,J.F.Gonthier,I.Sokolov,N.Moll,G.Salis,A.Fuhrer,M.Ganzhorn,D.J.Egger,M.Troyer,A.Mezza-
capo, S. Filipp, and I. Tavernelli, Quantum algorithms for electronic structure calculations: Particle-hole hamiltonian and
optimized wave-function expansions, Phys. Rev. A 98, 022322 (2018).
[4] I. Cong, S. Choi, and M. D. Lukin, Quantum convolutional neural networks, Nat. Phys. 15, 1273 (2019).