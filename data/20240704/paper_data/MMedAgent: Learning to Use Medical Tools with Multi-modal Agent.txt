MMedAgent: Learning to Use Medical Tools with Multi-modal Agent
BinxuLi1,TiankaiYan1,YuantingPan1,ZheXu2,3,JieLuo3,RuiyangJi4,
ShilongLiu5,HaoyuDong6∗,ZihaoLin4∗,YixinWang1*
1StanfordUniversity, 2CUHK, 3HarvardMedicalSchool,
4VirginiaTech, 5TsinghuaUniversity, 6DukeUniversity
{andy0207,yxinwang}@stanford.edu, haoyu.dong151@duke.edu, zihaol@vt.edu
Abstract models (Zhang et al., 2023b; Tu et al., 2024; Wu
etal.,2023;Yangetal.,2024;Zhaoetal.,2024a)
Multi-Modal Large Language Models haveattemptedtoaddressthischallenge, theyre-
(MLLMs), despite being successful, exhibit
main limited to handling a narrow range of tasks
limited generality and often fall short when
across a restricted set of imaging modalities and
compared to specialized models. Recently,
cannotbeefficientlyextendedtonewtasksormore
LLM-based agents have been developed
to address these challenges by selecting imaging modalities. Furthermore, these general-
appropriatespecializedmodelsastoolsbased iststypicallydonotprovideexpert-levelresponses
onuserinputs. However,suchadvancements comparable to those of specialized MLLMs cus-
havenotbeenextensivelyexploredwithinthe tomizedforspecifictasks.
medicaldomain. Tobridgethisgap,thispaper
OnewaytoaddressthisissueistobuildanAI
introduces the first agent explicitly designed
Agent, an AI system driven by Large Language
for the medical field, named Multi-modal
Models(LLMs)thatintegratesvariousdomainex-
Medical Agent (MMedAgent). We curate
pert models as tools. Such a system can under-
an instruction-tuning dataset comprising six
medical tools solving seven tasks, enabling standuserinstructions,makedecisions,andselect
the agent to choose the most suitable tools theappropriatetoolstoexecuteanyspecifictask,
foragiventask. Comprehensiveexperiments therebygeneratingexpert-levelresponsesforany
demonstratethatMMedAgentachievessupe- givenrequest(Xieetal.,2024;Chenetal.,2023;
rior performance across a variety of medical
Wang et al., 2024; Liu et al., 2023a; Tao et al.,
taskscomparedtostate-of-the-artopen-source
2023). DespitethesignificantsuccessofAIagents
methods and even the closed-source model,
inthegeneralimagedomain(Taoetal.,2023;Qin
GPT-4o. Furthermore, MMedAgent exhibits
efficiency in updating and integrating new et al., 2023; Wang et al., 2023a), no such agents
medicaltools. currently exist in the medical domain. Although
severalworks(Tangetal.,2023;Schmidgalletal.,
1 Introduction 2024;Lietal.,2024;Fanetal.,2024)inthemedi-
calfieldusetheterm“agent”intheirmethods,they
Multi-modal Large Language Models (MLLMs)
focusonutilizingLLMstoplayvariousrolesand
have made considerable progress across diverse
collaborateoncomplextasks,inwhichan“agent”
tasks with inputs from different medical imag-
referstoaspecificrole.
ingmodalities(e.g.,MagneticResonanceImaging,
Inthiswork,weaimtobuildthefirstAIagent
Computed Tomography, X-ray) in healthcare, in-
specifically for the medical domain, termed as
cludingVisualQuestionAnswering(VQA)(Moor
Multi-modalMedicalAgent(MMedAgent). We
etal.,2023;Zhangetal.,2023a;Lietal.,2023),im-
chooseLLaVA-Med(Lietal.,2023)astheback-
ageclassification(Sunetal.,2023),imagesegmen-
bone and aim to extend its capability to handle
tation(Maetal.,2024a),andMedicalReportGen-
variouslanguageandmulti-modaltasks,including
eration (MRG) (Thawkar et al., 2023; Hamamci
grounding, segmentation, classification, ground-
et al., 2024), etc. Despite these advancements,
ing,MRG,andRetrieval-AugmentedGeneration
MLLMs often exhibit limitations in seamlessly
(RAG). The first step to building MMedAgent is
solving multiple tasks across different medical
tocollectthestate-of-the-art(SOTA)methodsfor
imagingmodalities. Althoughrecentlargemedical
eachtask,hereafterreferredtoas“tools”. During
*Correspondingauthors this phase, we identify a lack of an effective tool
4202
luJ
2
]LC.sc[
1v38420.7042:viXraforthegroundingtask, promptingustofine-tune encouraginglyontheaxesofthehumanevaluation
GroundingDINO(Liuetal.,2023b)specificallyfor framework.
medicalapplications. Next,webuildaninstruction- Recent progress on LLMs has been made on
based dataset that teaches the agent to select the multi-modalconversationalcapability(Mooretal.,
propertool(s)whenencounteringauserinstruction 2023;Zhangetal.,2023b;Tuetal.,2024;Zhang
and aggregate the outputs from tools to reply to et al., 2023c,a; Thawkar et al., 2023; Sun et al.,
userspreciselyandcomprehensively. Thecoreof 2023; Wu et al., 2023; Li et al., 2023; Ma et al.,
ourapproachinvolvesanend-to-endtrainingreg- 2024a; Yang et al., 2024; Zhao et al., 2024a;
imenthroughvisualinstructiontuning(Liuetal., Hamamci et al., 2024). Owing to the diversity
2023a). MMedAgent has demonstrated promis- inherent in medical data and tasks, LLMs have
ingresultsinvariousaspects. Whenevaluatedon initially been localized to specific imaging do-
several complex medical tasks, MMedAgent sig- mains such as X-ray (Thawkar et al., 2023), CT
nificantlyoutperformscurrentopen-sourceSOTA (Hamamcietal.,2024),andhistology(Sunetal.,
methods,LLaVA-Med(Lietal.,2023)andRadFM 2023), or tailored for different tasks such as seg-
(Wuetal.,2023),andevensurpassesclose-source mentation(Maetal.,2024a;Leietal.,2023)and
method, GPT-4o (OpenAI, 2024), on average. It medicalreportgeneration(Wuetal.,2023). Incon-
also enhances the backbone’s, i.e., LLaVA-Med, trast, generalist models expand these capabilities
originalcapabilityintheVQAtask,aswellasex- byenablingasingleLLMtocoverawiderrange
hibitsefficientcapabilityinlearningnewtools. of imaging modalities and tasks by enlarging the
Ourcontributionscanbesummarizedas: pre-trainingdatasetsgreatly(Zhangetal.,2023b;
Lietal.,2023;Zhaoetal.,2024a;Liuetal.,2023a;
• We propose MMedAgent, the first multi-
Yang et al., 2024). Although generalist models
modalmedicalAIAgentincorporatingawide
are capable of handling a wide range of medical
spectrum of tools to handle various medical
modalitiesandtasks,theyfacelimitationsinscala-
tasksacrossdifferentmodalitiesseamlessly.
bilitywhenincorporatingadditionalskillsandlack
• Webuildthefirstopen-sourceinstructiontun- specializationinspecifictasks.
ingdatasetformulti-modalmedicalagents.
2.2 AIAgent
• Adaptivemulti-modalmedicaltoolsareincor- Amulti-modalAIAgentisasystemthatachieves
poratedintoourAgent. Wedevelopspecial- users’general-purposegoalsbyperceivingtheenvi-
izeddatasetstoadaptexistinggroundingand ronmentandmakingdecisionsbasedonthepercep-
segmentationtoolstothemedicaldomain. tions (Xie et al., 2024; Wooldridge and Jennings,
1995). RecentworksutilizeLLMsasplannersto
• Extensive experiments demonstrate that
understandmulti-modalinputfromenvironments
MMedAgentsurpassespreviousSOTAmulti-
andmakedecisionstocalldifferenttoolstoachieve
modal medical language models across a
goals. BasedonwhethertheLLMisopensource
rangeoftasks.
or not, (Xie et al., 2024) classifies multi-modal
2 RelatedWork AIAgentsintotwotypes: (i)closed-sourceLLMs
asplanners,whichutilizeprompttechniquetoen-
2.1 MedicalMLLMs
able LLMs to make decisions (Chen et al., 2023;
LLMspresentfertilenewgroundforresearchthat Wang et al., 2024); (ii) fine-tuned LLMs as plan-
pushes the frontier of the medical domain. Un- ners, where an LLM is fine-tuned to understand
like natural domains, the intrinsic complexity of instructions, make decisions, and call tools/APIs
medicaldata,whichincludesmultiplesourcesand (Liuetal.,2023a;Taoetal.,2023). OurMMedA-
modalities, has led most LLMs in the medical gentbelongstothesecondtype.
fieldtofocusonnarrowlydefinedtasksusinglan- Multi-modalAIAgentshaveachievedgreatsuc-
guage and text alone. Singhal et al. (Singhal cess in various applications. For example, (Tao
et al., 2023) curate MultiMedQA, a benchmark et al., 2023; Gur et al., 2023; Zhan and Zhang,
formedicalquestion-answeringdatasets,andpro- 2023)applyagentstocontrolthewebsiteoruserin-
poseMed-PaLM,whichutilizesinstructionprompt terface. Someworks(Qinetal.,2023;Wangetal.,
tuningtailoredtomedicaldomainsbasedonPaLM 2023c) focus on robotics or embodied AI which
(Chowdhery et al., 2023). Med-PaLM performs appliesmulti-modalLLMstoperceiveandinteractwith real environments. Most works concentrate
onmulti-modalunderstanding,editing,orgenera-
tion,especiallyimage,video,oraudio(Liuetal.,
2023a; Wang et al., 2023a; Zhang et al., 2023d).
However, these works are limited to the natural
domains, leaving the applications in the medical
domainunexplored,whichisparticularlychalleng-
ingduetoitsdiversemodalitiesandtasks. Tothe
bestofourknowledge,wearethefirsttoaddress Figure1: Thefour-stepMMedAgentpipeline.
this challenge and build a system that integrates
thesevariedmedicalapplications.
3.2 InstructionTuning
InordertoensureMMedAgentsimultaneouslyper-
3 MMedAgent
formsasbothactionplannerandresultsaggregator,
weadopttheunifieddialogueformatproposedby
Multi-modalMedicalAgent(MMedAgent),asys-
(Liuetal.,2023a),illustratedinFigure2. Specifi-
tembasedonanMLLM,isdesignedtoseamlessly
cally,uponreceivingauser’sinput,MMedAgent
manage diverse medical tasks by integrating var-
generates three components in its outputs: (1)
ious open-source medical models. MMedAgent
Thought,whichdetermineswhetherMMedAgent
comprises two components: (1) an instruction-
canindependentlysolvetheuser’sinstructionsorif
tunedmulti-modalLLMthatfunctionsasanaction
externaltoolsarerequired,andifso,identifiesthe
plannerandresultsaggregator,and(2)acollection
appropriate tool; (2) Action, which enumerate a
ofmedicaltoolstailoredtotheagent,eachtarget-
listofAPIcallsnecessarytoexecutethethought.
ingspecifictasksinthemedicaldomain. Wefirst
Thiscomprisestwosub-fields: API NameandAPI
presentthefundamentalworkflowofMMedAgent
Params. Iftheactionlistisnull,noAPIcallisiniti-
inSection3.1,followedbyadescriptionofcreat-
ated. (3)Value,whichprovidesanaturallanguage
ing an instruction-tuning dataset for training the
responseaggregatedbytheMLLMalongwiththe
multi-modalLLMasanactionplannerinSection
outputs from the involved tools. As depicted in
3.2. Thedetailsofmedicaltasksandcorresponding
Appendix Figure 5, we construct the instruction
toolsincorporatedinMMedAgentaredescribedin
databyqueryingGPT-4othroughone-shotlearn-
Section3.3.
ing, presentinganexamplethatdemonstratesthe
input and output of MMedAgent. We set a fixed
3.1 Workflow System instruction prompt for each tool and se-
lect several examples as conversation templates
Following LLaVA-Plus (Liu et al., 2023a), (User_1andAssistant_1inAppendixFigure5).
MMedAgent is built to learn to utilize a wide Thetoolprocessesthetogeneratetheinstruction
rangeofmulti-modalmedicaltools,extendingthe datafromthedialogue.
MLLMs’ capabilities to analyze and accomplish
3.3 MedicalTasksandTools
variousmedicaltasks. AsshowninFigure1, the
workflowconsistsoffourparts: (1)usersprovide OurMMedAgentpossessesthecapabilitytoaccess
an instruction X and a medical image I ; (2) adiversearrayoftoolswiththescalabilitytohan-
q q
MLLMworksasanactionplanner, whichunder- dlevarioustasks. AsshowninTable1,weintegrate
stands X and I and then generates a formatted sixtoolsthatencompasssevenrepresentativetasks
q q
instructionX tocallaspecifictool. (3)Thetool inmedicaldomains,i.e.,(1)grounding,(2)segmen-
tool
is executed given I and the output X of the tationwithbounding-boxprompts(Segmentation),
q result
toolissenttotheMLLM.(4)TheMLLMaggre- (3) segmentation with text prompts (G-Seg), (4)
gatestheoutputwithX andI andgeneratesthe medicalimagingclassification,(5)MedicalReport
q q
final answer X to users. We train the agent Generation(MRG),(6)retrievalaugmentedgener-
answer
end-to-endwithanauto-regressiveobjectiveonthe ation(RAG),and(7)VQA.Notethatnoadditional
generated sequence - X and X to enable toolsarerequiredfortheVQAtasksinceweutilize
tool answer
themodeltousecorrecttoolsandanswerquestions LLaVA-Med, which originally supports it. Each
basedonthetool’sresults. toolfunctionsasaspecialist,exhibitingexceptionalFigure2: AnexampleofthetrainingdataforMMedAgentthatlearnstousethetoolofGroundingDINOforobject
detectionandanswertheuser’squestion.
proficiencyinexecutingaspecifictaskacrossvari- ingtheminimalouterrectanglearoundeachobject.
ousmedicalimagingmodalities. Thecoordinatesoftheboundingboxesandthecor-
responding object labels are then recorded as the
3.3.1 Grounding groundinglabelsineachdataset.
Based on the released pre-trained weights, we
Grounding,alsoknownasdetection,aimstoiden-
fine-tuned the Grounding DINO with the dataset
tify and localize specific objects within an input
describedaboveaswellastwocommondatasetsin
imagebygeneratingthecoordinatesofbounding
thenaturalimagefield,i.e.,COCO(Linetal.,2014)
boxes containing the objects. To the best of our
andFlickr30k(Plummeretal.,2015),tomaintain
knowledge,noexistingmedicalmodelscansimul-
model’sabilityindetectingcommonobjects.
taneouslyprocessimagesfromdifferentmodalities.
Consequently,weproposeageneralizedgrounding
3.3.2 OtherTasks
tooltailoredforthemedicaldomain. Specifically,
wechoosetofine-tuneGroundingDINO(Liuetal., Segmentation involves identifying and delineat-
2023b),anopen-setobjectdetector,tothemedical ing the region of interest (ROIs) of an image. In
imagingfield. ourscenario,weconsiderinteractivesegmentation
Ourfirststepistocollectmultiplemedicalimage whenaboundingboxthatcoverstheROIsispro-
segmentationdatasets,includingFLARE2021(Ma vided. Thissettinghasbecomepopularsincethe
et al., 2022), WORD (Luo et al., 2022), BRATS development of Segment Anything (SAM) (Kir-
(Menzeetal.,2015), MontgomeryCountyX-ray illovetal.,2023). WeselectMedSAM(Maetal.,
Set (MC) (Jaeger et al., 2014; Candemir et al., 2024a),whichfine-tunesSAMtothemedicalfield,
2014), VinDr-CXR (Nguyen et al., 2022), and asourtool. Thepromptsarelimitedtobounding
multi-modal cell segmentation dataset (Cellseg) boxesbecausetheyprovidemorepreciseguidance
(Maetal.,2024b). AsdetailedinAppendixTable toSAM(Mazurowskietal.,2023). Specifically,in
4,thesedatasetstargetdifferentmodalities,organs, thisscenario,weconsidertheuserstoprovidethe
or diseases, each including the original imaging positionoftheboundingboxinwhichMedSAM
alongwiththeircorrespondingpixel-levelsegmen- canbedirectlyappliedtoobtaintheROImasks.
tationannotations. Thesesegmentationmasksare G-SegreferstocombininggroundingwithSAM.
furthertransformedintoboundingboxesbyextract- ItaimstoaddressamorecommonscenariowhenTask Tool DataSource ImagingModality
LLaVA-Med PMCarticle MRI,CT,X-ray,
VQA
(Lietal.,2023) 60K-IM(Lietal.,2023) Histology,Gross
BiomedCLIP PMCarticle MRI,CT,X-ray,
Classification
(Zhangetal.,2024) 60K-IM Histology,Gross
GroundingDINO MRI,CT,X-ray,
Grounding WORD,etc.*
(Liuetal.,2023b) Histology
MedSAM MRI,CT,X-ray,
Segmentation WORD,etc.*
(Maetal.,2024a) Histology,Gross
GroundingDINO MRI,CT,X-ray,
G-Seg. WORD,etc.*
+MedSAM Histology
ChatCAD MIMIC-CXR
MRG X-ray
(Wangetal.,2023b) (Johnsonetal.,2019)
ChatCAD+ MerckManual
RAG –
(Zhaoetal.,2024b) (PorterandKaplan,2011)
Table1: Thetasks,tools,datasource,andcorrespondingmedicalimagingmodalitiesincorporatedinMMedAgent.
“–”meansthattheRAGtaskonlyfocusesonnaturallanguagewithouthandlingimages. “WORD,etc.*”indicates
variousdatasourcesincludingWORD(Luoetal.,2022),FLARE2021(Maetal.,2022),BRATS(Menzeetal.,
2015),MontgomeryCountyX-raySet(MC)(Jaegeretal.,2014;Candemiretal.,2014),VinDr-CXR(Nguyenetal.,
2022),andCellseg(Maetal.,2024b).
users specify only a particular object to segment calretrievalprocess. ChatCAD+retrievesinforma-
inanimage. Inthiscase,wefirstactivatethefine- tionfromamedicaldictionarycontainingdetailed
tunedgroundingtooltolocalizethereferredobject descriptions of 1972 diseases and medical proce-
and then provide its location, in box format, to dures,includingtheirintroduction,symptoms,di-
MedSAM. agnosis, treatment, and causes, sourced from the
Classificationaimstoidentifythemostappropri- MerckManual(PorterandKaplan,2011),aprofes-
ate category for a medical image within a closed sional medical reference. Given the users’ input,
set. Specifically, we define a closed set of labels themodelsearchesformedicalentreesthatshare
L, including organ types, common image modal- thehighestcosinesimilaritywiththeencodedmes-
ities, and complex modalities such as ultrasound sageandretrievestherelevantknowledgefromthe
imaging, hematoxylin, and eosin histopathology. medicaldictionary.
The details of the set L are shown in Appendix
A.1. WeadoptBiomedCLIP(Zhangetal.,2024),
whichexhibitssuperiorperformanceinzero-shot
4 ExperimentalSettings
andfine-grainedclassification. Theimageisclas-
sified based on the cosine similarity between the
imageembeddingandeachtextembedding. MMedAgentisinitializedbyLLaVA-Med60K-IM,
MRG involves creating accurate and authentic instruction-tunedusingLoRA(Huetal.,2021)for
medicalreportsfromprovidedmedicalinformation 15epochs,andconductedoverapproximately72
orimaging. MMedAgentincorporatesChatCAD hoursontwo80GNVIDIAA100GPUs. Therank
(Wangetal.,2023b),anopen-sourcetooldesigned ofLoRAissetto128,andthetrainingbatchsize
forgeneratingmedicalreportsforchestX-rayim- issetto48. WeemployAdamW(Loshchilovand
ages. ThemodelwastrainedontheMIMIC-CXR Hutter,2019)astheoptimizeralongsideacosine
dataset(Johnsonetal.,2019)andcanprovidere- learningrateschedulepeakingat2e-4. Wegener-
portswithdetailedradiographicanalyses,identify- ate48Kinstruction-tuningdata,consistingof15K
ingchest-relatedconditionssuchascardiomegaly, augmentedVQAinstructionfollowingthemethod
edema,consolidation,atelectasis,etc. fromLLaVA-Plus(Liuetal.,2023a)derivedfrom
RAG refers to enhancing the generated outputs 60K inline mentions (Li et al., 2023), 10K data
byincorporatingthemostrelevantinformationac- pointsfordetection,3KforRAG,5Keachforseg-
quiredfromexternaldatasources. WeselectChat- mentation,classification,MRG,andG-Seg. Data
CAD+(Zhaoetal.,2024b)toimplementthemedi- sourcesareshowninTable1.Grounding Classification MRG RAG Overall
Cell Organ Disease
RadFM - - - 25.00 68.13 - 45.38
LLaVA-Med 51.78 65.48 68.58 53.46 70.10 30.44 60.68
LLaVA-Med(ToolinTest) 45.32 52.77 67.91 57.53 74.34 67.55 65.31
MMedAgent(ours) 97.50 102.29 125.89 81.11 121.49 85.55 109.48
Table2: PerformancecomparisonbetweenMMedAgentandotherbaselines. RadFMcannothandlegroundingand
retrievalaugmentedgenerationtasks,filledoutby“-”. LLaVA-Medreferstothe60K-IM versionwithonlythe
initialqueryX andimageI asinput,whileLLaVA-Med(ToolinTest)takesX ,I andalsotheinternaloutput
q q q q
fromtoolsX asinput.
result
5 Experimentals to10basedontheirhelpfulness, relevance, accu-
racy,andlevelofdetails. WeprovideGPT-4with
WeconductexperimentsonMMedAgenttoanswer
figure captions and include inline mentions from
three research questions: (1) What is the perfor-
60K-IM for the VQAtask. The detailed prompts
manceofMMedAgentinaddressingdiversemedi-
areillustratedinFigure6. FortheMRGtask,the
caltasksacrossvariousmodalities(Section5.1)?
reports are taken as captions of the input figures.
(2)Doestheinstruction-tunedMMedAgentexhibit
Fordetectionandothertaskswithoutacaptionin
superior performance in open-ended biomedical
theoriginaldata,wegeneratethecaptionsbycom-
dialogue(Section5.2)? (3)Whatistheefficiency
bining the images with the labels. For instance,
ofMMedAgentininvokingtoolsorincorporating
“ACTscanshowingthekidneyorgan.”. Sincethe
newtools(Section5.3)?
scoresaregeneratedbyanLLM,theirrankbetter
reflectsthecapabilityratherthantheabsoluteval-
5.1 VariousMedicalTasks
ues. BasedontheoutputfromGPT-4o,wepropose
5.1.1 EvaluationCriterion a relative score, defined as S /S (%), to
∗ GPT−4o
indicatetheperformancechangecausedbyother
To evaluate the performance of MMedAgent on
MLLMs. Here, S refers to the score of outputs
variouscomplexmedicaltasks,wecreateaneval- ∗
generatedby∗,with∗ ∈{RadFM,LLaVA-Med,
uation dataset consisting of 70 diverse questions.
MMedAgent}. Ahigherscoreindicatesasuperior
For this dataset, we initially select 10 concepts
outputquality. Duringtheevaluation,MMedAgent
randomly from the Merck Manual for RAG and
dynamicallyselects,activates,andexecutestools
60 unseen images of different tasks from respec-
in real-time, then aggregates the obtained results
tive data sources. These include 10 images each
fromthesetoolstoanswerquestions.
for organ grounding, disease grounding, and cell
grounding,alongwith20X-rayimagesforMRG
and10imagesacrossvariousmodalitiesforclassifi-
cation. Notably,theVQAtaskevaluationisshown 5.1.2 ExperimentalResults
inSection5.2. Duetotheinabilitytodescribethe
segmentation task linguistically, we provide the AsillustratedinTable2,MMedAgentsignificantly
qualitative results shown in Section 5.1.3. Then outperforms all other baselines on various tasks.
weutilizethesamepromptasoutlinedinSection Notably, the overall score of MMedAgent is 1.8
3.2togeneratetheinstruction-tuningdataforeval- times higher than that of LLaVA-Med. We also
uation. Subsequently,weseparatelyfeedthedata considerLLaVA-Med(ToolinTest),anenhanced
intoGPT-4o,MMedAgentandotherbenchmarks version of LLaVA-Med that incorporates the in-
toobtaintheoutputs. GPT-4oisanewlyreleased ternal output of tools. MMedAgent maintains its
multimodalmodelwithstrongvisualunderstanding superiorperformanceinthiscase. Furthermore,the
capabilities. AccordingtothetestingfromOpenAI, scoresfororgangrounding,diseasegrounding,and
it surpasses GPT-4 Turbo and has a faster infer- MRGexceed100%, indicatingthatMMedAgent
encespeed. Thus,theoutputfromGPT-4ocanbe surpassesGPT-4ointhesetasks. Theseresultsun-
viewedasastrongbenchmark. Alltheoutputswill derscorethesuperiorefficiencyofMMedAgentin
beassessedbyGPT-4andratedonascalefrom1 diversemedicaltasksacrossvariousmodalities.Figure3: QualitativecomparisonbetweenLLaVA-MedandMMedAgentacrossdifferenttasks. Theundesiredand
desiredresponsesarehighlightedinredandgreenrespectively.QuestionTypes ImagingModalities
Overall
Conversation Description X-ray MRI Histology Gross CT
(QuestionCount) (143) (50) (37) (38) (44) (34) (40) (193)
LLaVA-Med 53.30 38.90 56.58 40.84 54.71 48.47 50.68 50.94
MMedAgent 54.49 39.75 58.37 35.09 56.88 51.88 52.79 51.42
Table3: Comparisonofopen-endedmedicaldialoguebetweenMMedAgentandLLaVA-Med.
5.1.3 CaseStudy
AdetailedvisualcomparisonbetweenLLaVA-Med
andMMedAgentisillustratedinFigure3. Given
the user queries on tasks involving analyzing the
images,suchasclassification,grounding,andseg-
mentationtasks,LLaVA-Medonlygeneratessim-
ple conversational responses without solving the
givenrequests(highlightedinRed)anditisunable
togeneratevisualizedresults. Incontrast,MMedA-
genteffectivelyaddressesthesequestionsbyacti-
Figure4: ThescalabilityofMMedAgent.
vatingtheappropriatetools, integratingtheirout-
puts, generating accurate responses (highlighted
in Green), and visualizing the results. This is outperformsLLaVA-MedinalldomainsbutMRI.
guaranteed by the precise selection of tools by ThisdemonstratestheefficiencyofMMedAgentin
MMedAgentandthesuperiorityofthetoolsthem- open-endedmedicaldialogue.
selves. When encountering language generation-
basedtasks,i.e.,MRGandRAG,LLaVA-Medfails 5.3 ToolUtilization
toprovideanin-depthanalysisoftheimages. How-
ThesuperiorperformanceofMMedAgentonthe
ever,MMedAgentprovidesmorestraightforward
various tasks described above depends on accu-
and accurate responses by utilizing the tools de-
rately understanding users’ inputs and activating
signedspecificallyforthesetasks.
thecorrecttools. AftertrainingMMedAgentfor15
epochs,thetoolselectionaccuracyreached100%,
5.2 Open-endedMedicalDialogue
demonstratingMMedAgent’sabilitytoselectthe
To evaluate the capability of visual question- appropriatetoolswithouterrors.
answering tasks, we follow the setting of open- One significance of MMedAgent is its ability
endedmedicaldialogueinLLaVA-Med(Lietal., to adapt to new tools. Here, we consider two
2023). Here,weusethesametestdataasLLaVA- scenarios. Firstly, when a superior tool for tasks
Med, which consists of 193 novel questions and thatMMedAgentisalreadyequippedtohandlebe-
50 unseen images from PMC-15M (Zhang et al., comesavailable,theAPInameoftheoutdatedtool
2024). Thisdatasetcontains5modalitiesandcan can be seamlessly replaced with that of the new
be divided into two main classes: conversation tool,eliminatingtheneedforadditionalretraining.
questionsanddetaileddescriptionquestions. We Secondly,toextendMMedAgenttoanewtask,it
also utilize the relative score, introduced in Sec- issufficienttogenerateasmallsetofinstruction-
tion 5.1.1, as the evaluation criterion. Since this tuningdataforthisspecifictaskandfine-tunethe
isapurelanguagetask,weselecttheoutputfrom agentaccordingly,ratherthanretrainingitfromthe
GPT-4ratherthanGPT-4oasthereferencescore. beginning.
AsshowninTable3,performanceiscategorized Toverifythiscapability,wesimulateanewtool
byeitherquestiontypes(conversationanddescrip- called “Pseudo Tool”, generate an additional 5K
tion)orimagemodalities(X-ray,MRI,Histology, instruction-tuningdata(followingSection4),and
GrossandCT).Afterinstruction-tuningonthetool create30unseendiversequestionsforevaluation
learningdataset,MMedAgentperformsbetteron followingSection5.1.1. Weutilizethesametrain-
both types of questions. Moreover, MMedAgent ingsettingstofine-tuneMMedAgentwithasmallerlearningrateof1e-6andabatchsizeof10onone withpathways. JournalofMachineLearningResearch,
80G A100 GPU. As shown in Figure 4, the ac- 24(240):1–113.
curacy of selecting a new tool increase to 100%
ZhihaoFan,JialongTang,WeiChen,SiyuanWang,Zhongyu
within2Kstepswithoutdamagingtheperformance Wei, Jun Xi, Fei Huang, and Jingren Zhou. 2024. Ai
onselectingoldtools. hospital:Interactiveevaluationandcollaborationofllms
as intern doctors for clinical diagnosis. arXiv preprint
arXiv:2402.09742.
6 Conclusion
Izzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Saf-
We propose MMedAgent, the first multi-modal
dari,YutakaMatsuo,DouglasEck,andAleksandraFaust.
medical AI agent that is capable of seamlessly 2023. A real-world webagent with planning, long con-
textunderstanding,andprogramsynthesis. arXivpreprint
utilizing various medical tools to handle a broad
arXiv:2307.12856.
spectrum of medical tasks across different imag-
ing modalities. We create an instruction-tuning IbrahimEthemHamamci,SezginEr,andBjoernMenze.2024.
Ct2rep:Automatedradiologyreportgenerationfor3dmed-
datasetthatMMedAgentutilizetolearntoinvoke
icalimaging. arXivpreprintarXiv:2403.06801.
various medical tools and aggregate results from
tools. Comprehensive experiments demonstrate EdwardJHu,YelongShen,PhillipWallis,ZeyuanAllen-Zhu,
thatMMedAgentsignificantlyoutperformsopen- Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.
2021. Lora:Low-rankadaptationoflargelanguagemodels.
sourcebaselinesandevensurpassesGPT-4oacross
arXivpreprintarXiv:2106.09685.
manymedicaltasks. Furthermore,MMedAgentef-
ficientlyintegrateswithnewtoolswhileremaining StefanJaeger,AlexandrosKarargyris,SemaCandemir,Les
Folio,JeniferSiegelman,FionaCallaghan,ZhiyunXue,
thecapabilitytoactivatepreviouslylearnedtools.
Kannappan Palaniappan, Rahul K. Singh, Sameer An-
tani, GeorgeThoma, Yi-XiangWang, Pu-XuanLu, and
7 Limitation Clement J. McDonald. 2014. Automatic tuberculosis
screening using chest radiographs. IEEE Transactions
Ourworkiscurrentlylimitedtoseventasksacross onMedicalImaging,33(2):233–245.
fivemodalities. Duetotheneedforextensivedo-
AlistairE.W.Johnson,TomJ.Pollard,NathanielR.Green-
mainknowledge,andthecomplexityanddiversity baum,MatthewP.Lungren,ChihyingDeng,YifanPeng,
ofmedicaldatasetsinvolvedinmedicaltasks,more Zhiyong Lu, Roger G. Mark, Seth J. Berkowitz, and
Steven Horng. 2019. Mimic-cxr-jpg, a large publicly
specialized tools are emerging that should be in-
availabledatabaseoflabeledchestradiographs. Preprint,
cludedinourtoolslists. However,thescalability arXiv:1901.07042.
ofourmodelallowsfortheinclusionofmorepow-
AlexanderKirillov,EricMintun,NikhilaRavi,HanziMao,
erfultoolsinthefuture.
ChloeRolland,LauraGustafson,TeteXiao,SpencerWhite-
Additionally,moreablationstudiesondifferent head,AlexanderCBerg,Wan-YenLo,etal.2023. Segment
backbonesarenecessary. Ourcurrentbackboneis anything. InProceedingsoftheIEEE/CVFInternational
ConferenceonComputerVision,pages4015–4026.
based on the LLaVA-Med, but recently, multiple
generalistLLMsinthemedicaldomainhavebeen WenhuiLei,XuWei,XiaofanZhang,KangLi,andShaoting
proposed,whichcouldpotentiallybeusedtobuild Zhang.2023. Medlsam: Localizeandsegmentanything
modelfor3dctimages. Preprint,arXiv:2306.14752.
astrongerMMedAgent.
Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama,
Haotian Liu, Jianwei Yang, Tristan Naumann, Hoifung
References
Poon, and Jianfeng Gao. 2023. Llava-med: Training a
largelanguage-and-visionassistantforbiomedicineinone
Sema Candemir, Stefan Jaeger, Kannappan Palaniappan,
day. Preprint,arXiv:2306.00890.
JonathanP.Musco,RahulK.Singh,ZhiyunXue,Alexan-
dros Karargyris, Sameer Antani, George Thoma, and
JunkaiLi, SiyuWang, MengZhang, WeitaoLi, Yunghwei
ClementJ.McDonald.2014. Lungsegmentationinchest
Lai,XinhuiKang,WeizhiMa,andYangLiu.2024. Agent
radiographsusinganatomicalatlaseswithnonrigidregistra-
hospital:Asimulacrumofhospitalwithevolvablemedical
tion. IEEETransactionsonMedicalImaging,33(2):577–
agents. arXivpreprintarXiv:2405.02957.
590.
Wei-GeChen,IrinaSpiridonova,JianweiYang,JianfengGao, Tsung-YiLin,MichaelMaire,SergeBelongie,LubomirBour-
andChunyuanLi.2023. Llava-interactive:Anall-in-one dev,RossGirshick,JamesHays,PietroPerona,DevaRa-
demoforimagechat,segmentation,generationandediting. manan,C.LawrenceZitnick,andPiotrDollár.2014. Mi-
arXivpreprintarXiv:2311.00571. crosoftcoco:Commonobjectsincontext. 13.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, ShilongLiu,HaoCheng,HaotianLiu,HaoZhang,FengLi,
Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul TianheRen,XueyanZou,JianweiYang,HangSu,JunZhu,
Barham, HyungWonChung, CharlesSutton, Sebastian etal.2023a. Llava-plus:Learningtousetoolsforcreating
Gehrmann,etal.2023. Palm:Scalinglanguagemodeling multimodalagents. arXivpreprintarXiv:2311.05437.Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Reyes, and Koen Van Leemput. 2015. The multimodal
Zhang,JieYang,ChunyuanLi,JianweiYang,HangSu, braintumorimagesegmentationbenchmark(brats). IEEE
JunZhu, andLeiZhang.2023b. Groundingdino: Mar- TransactionsonMedicalImaging,34(10):1993–2024.
ryingdinowithgroundedpre-trainingforopen-setobject
detection. Preprint,arXiv:2303.05499. MichaelMoor,QianHuang,ShirleyWu,MichihiroYasunaga,
YashDalmia,JureLeskovec,CyrilZakka,EduardoPontes
IlyaLoshchilovandFrankHutter.2019. Decoupledweight Reis,andPranavRajpurkar.2023. Med-flamingo:amulti-
decayregularization. Preprint,arXiv:1711.05101. modalmedicalfew-shotlearner. InMachineLearningfor
Health(ML4H),pages353–367.PMLR.
XiangdeLuo,WenjunLiao,JianghongXiao,JienengChen,
TaoSong,XiaofanZhang,KangLi,DimitrisN.Metaxas, HaQ.Nguyen,KhanhLam,LinhT.Le,HieuH.Pham,DatQ.
GuotaiWang,andShaotingZhang.2022. Word:Alarge Tran,DungB.Nguyen,DungD.Le,ChiM.Pham,Hang
scaledataset,benchmarkandclinicalapplicablestudyfor T. T. Tong, Diep H. Dinh, Cuong D. Do, Luu T. Doan,
abdominalorgansegmentationfromctimage. Medical CuongN.Nguyen,BinhT.Nguyen,QueV.Nguyen,AuD.
ImageAnalysis,82:102642. Hoang, Hien N. Phan, Anh T. Nguyen, Phuong H. Ho,
Dat T. Ngo, Nghia T. Nguyen, Nhan T. Nguyen, Minh
Jun Ma, Yuting He, Feifei Li, Lin Han, Chenyu You, and Dao,andVanVu.2022. Vindr-cxr: Anopendatasetof
BoWang.2024a. Segmentanythinginmedicalimages. chestx-rayswithradiologist’sannotations. ScientificData,
NatureCommunications,15(1). 9(1):429.
JunMa,RonaldXie,ShaminiAyyadhury,ChengGe,Anubha OpenAI.2024. Hellogpt-4o. https://openai.com/index/
Gupta,RituGupta,SongGu,YaoZhang,GihunLee,Joon- hello-gpt-4o/. Accessed:2024-05-26.
kee Kim, Wei Lou, Haofeng Li, Eric Upschulte, Timo
Dickscheid,JoséGuilhermedeAlmeida,YixinWang,Lin BryanA.Plummer,LiweiWang,ChrisM.Cervantes,JuanC.
Han,XinYang,MarcoLabagnara,VojislavGligorovski, Caicedo,JuliaHockenmaier,andSvetlanaLazebnik.2015.
Maxime Scheder, Sahand Jamal Rahi, Carly Kempster, Flickr30kentities:Collectingregion-to-phrasecorrespon-
Alice Pollitt, Leon Espinosa, Tâm Mignot, Jan Moritz dencesforricherimage-to-sentencemodels. pages2641–
Middeke,Jan-NiklasEckardt,WangkaiLi,ZhaoyangLi, 2649.
XiaochenCai,BizheBai,NoahF.Greenwald,DavidVan
Valen,ErinWeisbart,BethA.Cimini,TrevorCheung,Os- RobertS.PorterandJustinL.Kaplan.2011. Themerckman-
car Brück, Gary D. Bader, and Bo Wang. 2024b. The ualofdiagnosisandtherapy,2011. MerckResearchLabo-
multi-modalitycellsegmentationchallenge:Towardsuni- ratories.
versalsolutions. NatureMethods.
YiranQin,EnshenZhou,QichangLiu,ZhenfeiYin,LuSheng,
Jun Ma, Yao Zhang, Song Gu, Xingle An, Zhihe Wang, RuimaoZhang,YuQiao,andJingShao.2023. Mp5: A
Cheng Ge, Congcong Wang, Fan Zhang, Yu Wang, Yi- multi-modalopen-endedembodiedsysteminminecraftvia
nan Xu, Shuiping Gou, Franz Thaler, Christian Payer, activeperception. arXivpreprintarXiv:2312.07472.
Darko Štern, Edward G.A. Henderson, Dónal M. Mc-
Sweeney, AndrewGreen, PriceJackson, LachlanMcIn- SamuelSchmidgall,RojinZiaei,CarlHarris,EduardoReis,
tosh,Quoc-CuongNguyen,AbdulQayyum,Pierre-Henri JeffreyJopling,andMichaelMoor.2024. Agentclinic:a
Conze, ZiyanHuang, ZiqiZhou, Deng-PingFan, Huan multimodalagentbenchmarktoevaluateaiinsimulated
Xiong,GuoqiangDong,QiongjieZhu,JianHe,andXiaop- clinicalenvironments. arXivpreprintarXiv:2405.07960.
ingYang.2022. Fastandlow-gpu-memoryabdomenct
organsegmentation:Theflarechallenge. MedicalImage KaranSinghal, ShekoofehAzizi, TaoTu, SSaraMahdavi,
Analysis,82:102616. JasonWei,HyungWonChung,NathanScales,AjayTan-
wani, Heather Cole-Lewis, Stephen Pfohl, et al. 2023.
MaciejAMazurowski,HaoyuDong,HanxueGu,JichenYang, Largelanguagemodelsencodeclinicalknowledge. Nature,
NicholasKonz,andYixinZhang.2023. Segmentanything 620(7972):172–180.
modelformedicalimageanalysis:anexperimentalstudy.
MedicalImageAnalysis,89:102918. YuxuanSun,ChengluZhu,SunyiZheng,KaiZhang,Zhongyi
Shui, Xiaoxuan Yu, Yizhi Zhao, Honglin Li, Yunlong
Bjoern H. Menze, Andras Jakab, Stefan Bauer, Jayashree Zhang, Ruojia Zhao, et al. 2023. Pathasst: Redefining
Kalpathy-Cramer,KeyvanFarahani,JustinKirby,Yuliya pathologythroughgenerativefoundationaiassistantfor
Burren,NicolePorz,JohannesSlotboom,RolandWiest, pathology. arXivpreprintarXiv:2305.15072.
LeventeLanczi,ElizabethGerstner,Marc-AndréWeber,
Tal Arbel, Brian B. Avants, Nicholas Ayache, Patricia Xiangru Tang, Anni Zou, Zhuosheng Zhang, Yilun Zhao,
Buendia, D. Louis Collins, Nicolas Cordier, Jason J. Xingyao Zhang, Arman Cohan, and Mark Gerstein.
Corso, Antonio Criminisi, Tilak Das, Hervé Delingette, 2023. Medagents: Large language models as collabo-
Çag˘atay Demiralp, Christopher R. Durst, Michel Dojat, rators for zero-shot medical reasoning. arXiv preprint
Senan Doyle, Joana Festa, Florence Forbes, Ezequiel arXiv:2311.10537.
Geremia,BenGlocker,PolinaGolland,XiaotaoGuo,An-
dacHamamci,KhanM.Iftekharuddin,RajJena,NigelM. Heyi Tao, Sethuraman TV, Michal Shlapentokh-Rothman,
John, Ender Konukoglu, Danial Lashkari, José António DerekHoiem,andHengJi.2023. Webwise: Webinter-
Mariz, Raphael Meier, Sérgio Pereira, Doina Precup, facecontrolandsequentialexplorationwithlargelanguage
StephenJ.Price,TammyRiklinRaviv,SyedM.S.Reza, models. arXivpreprintarXiv:2310.16042.
MichaelRyan,DuyguSarikaya,LawrenceSchwartz,Hoo-
ChangShin,JamieShotton,CarlosA.Silva,NunoSousa, OmkarThawkar,AbdelrahmanShaker,SahalShajiMullap-
NageshK.Subbanna,GaborSzekely,ThomasJ.Taylor, pilly,HishamCholakkal,RaoMuhammadAnwer,Salman
OwenM.Thomas,NicholasJ.Tustison,GozdeUnal,Flor Khan,JormaLaaksonen,andFahadShahbazKhan.2023.
Vasseur, Max Wintermark, Dong Hye Ye, Liang Zhao, Xraygpt:Chestradiographssummarizationusingmedical
BinshengZhao,DarkoZikic,MarcelPrastawa,Mauricio vision-languagemodels. arXivpreprintarXiv:2306.07971.Tao Tu, Shekoofeh Azizi, Danny Driess, Mike Schaek- XiaomanZhang, ChaoyiWu, ZihengZhao, WeixiongLin,
ermann, Mohamed Amin, Pi-Chuan Chang, Andrew YaZhang, YanfengWang, andWeidiXie.2023c. Pmc-
Carroll, Charles Lau, Ryutaro Tanno, Ira Ktena, et al. vqa:Visualinstructiontuningformedicalvisualquestion
2024. Towards generalist biomedical ai. NEJM AI, answering. arXivpreprintarXiv:2305.10415.
1(3):AIoa2300138.
Yixiao Zhang, Akira Maezawa, Gus Xia, Kazuhiko Ya-
Junke Wang, Dongdong Chen, Chong Luo, Xiyang Dai, mamoto,andSimonDixon.2023d. Loopcopilot:Conduct-
Lu Yuan, Zuxuan Wu, and Yu-Gang Jiang. 2023a. ingaiensemblesformusicgenerationanditerativeediting.
Chatvideo: A tracklet-centric multimodal and ver- arXivpreprintarXiv:2310.12404.
satile video understanding system. arXiv preprint
arXiv:2304.14407. Theodore Zhao, Yu Gu, Jianwei Yang, Naoto Usuyama,
HoHinLee,TristanNaumann,JianfengGao,AngelaCrab-
JunyangWang,HaiyangXu,JiaboYe,MingYan,Weizhou tree,BrianPiening,CarloBifulco,etal.2024a. Biomed-
Shen,JiZhang,FeiHuang,andJitaoSang.2024. Mobile- parse: a biomedical foundation model for image pars-
agent:Autonomousmulti-modalmobiledeviceagentwith ingofeverythingeverywhereallatonce. arXivpreprint
visualperception. arXivpreprintarXiv:2401.16158. arXiv:2405.12971.
ShengWang,ZihaoZhao,XiOuyang,QianWang,andDing-
ZihaoZhao,ShengWang,JinchenGu,YitaoZhu,Lanzhuju
gangShen.2023b. Chatcad: Interactivecomputer-aided
Mei,ZixuZhuang,ZhimingCui,QianWang,andDing-
diagnosisonmedicalimageusinglargelanguagemodels.
gangShen.2024b. Chatcad+: Towardsauniversaland
Preprint,arXiv:2302.07257.
reliableinteractivecadusingllms. IEEETransactionson
MedicalImaging,page1–1.
ZihaoWang,ShaofeiCai,AnjiLiu,YonggangJin,Jinbing
Hou, Bowei Zhang, Haowei Lin, Zhaofeng He, Zilong
Zheng,YaodongYang,etal.2023c. Jarvis-1:Open-world
multi-taskagentswithmemory-augmentedmultimodallan-
guagemodels. arXivpreprintarXiv:2311.05997.
MichaelWooldridgeandNicholasRJennings.1995. Intelli-
gentagents:Theoryandpractice. Theknowledgeengineer-
ingreview,10(2):115–152.
ChaoyiWu,XiaomanZhang,YaZhang,YanfengWang,and
WeidiXie.2023. Towardsgeneralistfoundationmodelfor
radiology byleveraging web-scale2d&3d medicaldata.
Preprint,arXiv:2308.02463.
Junlin Xie, Zhihong Chen, Ruifei Zhang, Xiang Wan, and
GuanbinLi.2024. Largemultimodalagents: Asurvey.
arXivpreprintarXiv:2402.15116.
LinYang,ShawnXu,AndrewSellergren,TimoKohlberger,
Yuchen Zhou, Ira Ktena, Atilla Kiraly, Faruk Ahmed,
Farhad Hormozdiari, Tiam Jaroensri, et al. 2024. Ad-
vancingmultimodalmedicalcapabilitiesofgemini. arXiv
preprintarXiv:2405.03162.
ZhuoshengZhanandAstonZhang.2023. Youonlylookat
screens:Multimodalchain-of-actionagents. arXivpreprint
arXiv:2309.11436.
KaiZhang,JunYu,ZhilingYan,YixinLiu,EashanAdhikarla,
SunyangFu,XunChen,ChenChen,YuyinZhou,XiangLi,
etal.2023a. Biomedgpt:Aunifiedandgeneralistbiomedi-
calgenerativepre-trainedtransformerforvision,language,
andmultimodaltasks. arXivpreprintarXiv:2305.17100.
Sheng Zhang, Yanbo Xu, Naoto Usuyama, Hanwen Xu,
JaspreetBagga, RobertTinn, SamPreston, RajeshRao,
Mu Wei, Naveen Valluri, Cliff Wong, Andrea Tupini,
Yu Wang, Matt Mazzola, Swadheen Shukla, Lars Li-
den, Jianfeng Gao, Matthew P. Lungren, Tristan Nau-
mann, ShengWang,andHoifungPoon.2024. Biomed-
clip:amultimodalbiomedicalfoundationmodelpretrained
fromfifteenmillionscientificimage-textpairs. Preprint,
arXiv:2303.00915.
Sheng Zhang, Yanbo Xu, Naoto Usuyama, Hanwen Xu,
JaspreetBagga, RobertTinn, SamPreston, RajeshRao,
Mu Wei, Naveen Valluri, et al. 2023b. Biomedclip: a
multimodalbiomedicalfoundationmodelpretrainedfrom
fifteenmillionscientificimage-textpairs. arXivpreprint
arXiv:2303.00915.A DetailsofTools B InstructionTuningDatasetGeneration
A.1 Classification Werepresentourpromptsforgeneratinganinstruc-
tiontuningdatasetinFigure5.
We construct a close set of labels L for Biomed-
CLIPtosearchforthemostsuitablecategoryfor
C AgentServing
thegivenimage.
L={“adenocarcinomahistopathology”,“brain MMedAgentoperateswithintheFastChatsystem
MRI”, “covid line chart”, “squamous cell car- , which consistsof webservers thatinteract with
cinomahistopathology”,“immunohistochemistry users,modelworkershostingthelanguagemodel,
histopathology”,“boneX-ray”,“chestX-ray”,“pie andvarioustools. Acontrollercoordinatestheac-
chart”, “ultrasound imaging“, “hematoxylin and tivitiesbetweenthewebserversandmodelworkers.
eosinhistopathology”,“gross”}. Theentiresystem,includingthe7BMMedAgent
and all associated tools, can be run on an Nvidia
A.2 RetrievalAugmentedGeneration(RAG)
A100(80GB)GPU.
RAGdistinguishesitselffromstandardreportgen-
erationbyitsaccesstoanexternalknowledgebase, D EvaluationPrompt
suchasMerckManual. Weconsiderthefollowing
WeutilizeGPT-4toassesstheanswersgenerated
3 common uses of RAG. The instruction-tuning
by MMedAgent and other models with prompts
dataaregeneratedbasedonthesefunctionalities.
showninFigure6.
1. Chest X-ray image report analysis. The
chestX-rayimagereportanalysiscanfunction
toanalyzethereportonmedicalimagesand
provide an analysis including the potential
diseasesandtheirrelatedretrievedknowledge
andsource.
2. Generalmedicalreportanalysis. Thegen-
eralmedicalreportanalysiscantakeasumma-
rizedreportoncommondiseasesandgenerate
ananalysiswithmedicaladvicesuchastreat-
mentsandprecautions,togetherwithalinkto
theretrievedsourcefromtheMerckManual
officialwebsite.
3. Generalmedicaladvicegeneration. Forgen-
eral medical advice generation, the user can
askgeneralquestionsaboutthediseases,and
the model will retrieve and provide related
informationonthem.
ForthechestX-rayimagereportanalysis,wegen-
erate1000chestX-rayreportsfromtheMRGtool
described in Section 3.3.2 as the report dataset.
For the datasets of general medical report analy-
sisandgeneralmedicaladvicegeneration,weuti-
lizeGPT-4otogenerate1000medicalreportsand
1000patientquestionsrespectivelyaboutcommon
diseasessampledfromtheentreescoveredinthe
MerckManual.
A.3 MedicalGroundingDINO
The datasets used to tune the medical grounding
DINOisshowninTable4.Figure5: Pipelineofgeneratinginstruction-tuningdatasetforthegroundingtask.Image
Dataset Modality Anatomy Labels
Number
Liver,Spleen,Kidney,Stomach,Gallbladder,Esophagus,Pancreas,
WORD CT Abdomen 9309 Duodenum,Colon,Intestine,Adrenal,Rectum,Bladder,Headof
femur
Liver,Kidney,Spleen,Pancreas,Aorta,IVC,AdrenalGland,Gall-
FLARE CT Abdomen 4797
bladder,Esophagus,Stomach,Duodenum
Aorticenlargement,Atelectasis,Calcification,Cardiomegaly,Con-
VinDr- solidation,ILD,Infiltration,LungOpacity,Nodule/Mass,Otherle-
X-ray Chest 4394
CXR sion,Pleuraleffusion,Pleuralthickening,Pneumothorax,Pulmonary
fibrosis
MC X-ray Chest 566 Lung
BRATS MRI Brain 14720 Tumor
Cellseg Histology Cell 229 Cell
Table4: Datasetoverviewforfine-tuningGroundingDINO.
Figure6: Evaluationpipeline. Assistant1isthemodeltobeevaluated,whichcanbeMMedAgentorLLaVA-Med
andAssistant2isGPT-4oinourexperiment.