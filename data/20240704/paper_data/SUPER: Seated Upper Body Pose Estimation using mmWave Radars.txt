SUPER: Seated Upper Body Pose Estimation using
mmWave Radars
Bo Zhang, Zimeng Zhou, Boyu Jiang, Rong Zheng
Department of Computing and Software
McMaster University
Hamilton, ON, Canada
{zhanb59, zhouz287, jiangb11, rzheng}@mcmaster.ca
Abstract—In industrial countries, adults spend a considerable In recent years, the rapid advancements in deep learning
amount of time sedentary each day at work, driving and led to significant progress in human body modeling [7], [8]
during activities of daily living. Characterizing the seated upper
and HPE using various sensing modalities. Notable work in
body human poses using mmWave radars is an important, yet
HPE includes OpenPose [9] and VitPose [10] in computer
under-studied topic with many applications in human-machine
interaction, transportation and road safety. In this work, we vision, Deep inertial poser [11] and IMUPoser [12] using
devise SUPER, a framework for seated upper body human pose IMU sensors, mmPose [13] and mmMesh [14] with mmWave
estimation that utilizes dual-mmWave radars in close proximity. radars,andDensePose[15]usingWiFidevices,tonameafew.
A novel masking algorithm is proposed to coherently fuse data
Among different sensing modalities, mmWave radars offer
from the radars to generate intensity and Doppler point clouds
distinct advantages due to their ability to penetrate obstruc-
withcomplementaryinformationforhigh-motionbutsmallradar
cross section areas (e.g., upper extremities) and low-motion but tions like garments or walls, adapt to diverse lighting and
large RCS areas (e.g. torso). A lightweight neural network weather conditions, and preserve user privacy. Furthermore,
extractsbothglobalandlocalfeaturesofupperbodyandoutput thesubstantialbandwidth(intheGHzrange)equipsmmWave
pose parameters for the Skinned Multi-Person Linear (SMPL)
radars with resilience against noise, interference, and center-
model. Extensive leave-one-subject-out experiments on various
meter level range resolutions. However, existing mmWave-
motion sequences from multiple subjects show that SUPER
outperforms a state-of-the-art baseline method by 30 – 184%. based solutions predominantly target full-body locomotions
We also demonstrate its utility in a simple downstream task for and are not designed for handling nuanced upper-body move-
hand-object interaction. ments. mmWave-based SUB-HPE shares with full-body HPE
Index Terms—Seated upper body pose estimation, mmWave common challenges stemming from low spatial resolutions as
radars, data fusion, point clouds, deep neural networks
theresultoffewon-boardtransmittingandreceivingantennas
on low-end commercial-of-the-shelf (COTS) mmWave radars,
I. INTRODUCTION specular reflections and variations from inherent micro-body
movements.But,crucially,itmustalsohandlelimitedmobility
Human pose estimation (HPE) estimates the configuration intheupperbody’scoreareawhensitting,aswellasthesmall
of human body parts from input data captured by sensors radar cross-sections (RCS) of upper extremities, ranging from
and has attracted much attention in industry and the research -45 dBsm to -20 dBsm for hands [16].
communityduetoitswiderangeofapplications,includingthe In this work, we devise SUPER, a framework for Seated
human-machine interactions [1], fitness [2], virtual reality [3], Upper Body Pose Estimation using mmWave Radars. The
smart home [4] and smart vehicle [5], etc. While full-body framework encompasses a dual-radar pre-processing and fu-
HPE is important in characterizing joint movements during sion pipeline and a light weight neural network to predict up-
locomotions, a 2019 study showed that adults ages 20 to perbodyposeparameters.Toincreasethespatialresolutionof
75 in the US reported spending an average of 9.5 hours the acquired radar data, two closely positioned radar sensors,
sedentary each day [6]. Therefore, seated upper body human orientedperpendiculartoeachother,areutilized.Anoveldual-
pose estimation (SUB-HPE) is arguably more relevant in radarmaskingalgorithmcoherentlyfusesdatafromtheradars
interactiveapplicationsandunderstandingusers’mentalstates to generate two complementary types of point clouds: the
(e.g., alertness and attention). For example, by monitoring intensitypointcloud(IPC)andtheDopplerpointcloud(DPC).
upper limb movements while sitting, novel applications can The latter captures motion information of extremities while
be developed to empower users to control digital interfaces, the former better characterizes low-motion portions of the
manipulateaugmentedrealityenvironments,andmanagesmart upperbody(e.g.,torsoareas).Benefitingfromthesparsepoint
home systems. SUB-HPE can also find applications in trans- cloud representation, the lightweight neural network extracts
portation and road safety, where drowsy or inattentive drivers both global and local features of the upper body. Finally, the
pose a significant risk on roadways. Analyzing head poses, SkinnedMulti-PersonLinear(SMPL)modelisappliedtoyield
hand placements and orientation of the upper body allows the realistic human body poses and motions. An example of the
detection of early signs of drowsiness or distraction. data captured by an RGB camera, a motion capture system,
4202
luJ
2
]VC.sc[
1v55420.7042:viXraand the predicted and ground truth poses can be found in projects radar point clouds from two separate and perpendic-
Figure 1. ularly oriented radars onto the depth-azimuth(XY) and depth-
We have implemented a prototype of SUPER utilizing two elevation(XZ) plane, respectively to create two 2D intensity
Texas Instruments IWR6843ISK mmWave radars1. A diverse images. The images are then fed into a forked CNN structure
group of 10 subjects, encompassing different genders, ages, topredictthehumanskeletaljoints.In[28],Anetal.propose
and body mass indices (BMIs), were recruited for data col- the MARS system which takes 5D radar point clouds (x, y,
lection in a laboratory setting. The data collection process in- z, intensity and Doppler) as input and outputs human pose
volvedsubjectsengaginginpredefinedarm,head,torsomotion in several rehabilitation scenarios. Xue et al. [14] introduce
sequences. Experiment results show that SUPER consistently mmMesh which adopts PointNet [29] as the feature extractor
outperforms a state-of-the-art (SOTA) baseline method and of the point cloud and incorporates SMPL [7] to this task,
achieves 112mm in average Mean Per Joint Position Error facilitatingbothbodyshapeandposepredictions.Inafollow-
(MPJPE) and 15.89mm Procrustes alignment MPJPE (PA- up work to [14], multi-subject 3D human mesh construction
MPJPE)metricsinleave-one-subjectouttrials.Todemonstrate isinvestigated[30].Thisisachievedbyobtainingthelocation
theutilityofSUPER,wealsoimplementandevaluateasimple information from an energy map, and selectively generating
downstream task of hand-object interaction. 4D point clouds close to the subjects. A fine-grained human
In summary, we make the following new contributions mesh is then predicted using a coarse-to-fine mesh estimation
toward mmWave-based fine-grained SUB-HPE in this work. framework.Mostrecently,insteadofusingradarpointclouds,
• In this work, we investigate a new task, i.e., SUB-HPE, Lee et al. [31] introduce the velocity-specific range-doppler-
and collect a dataset consisting of various head, torso as azimuth-elevation map (VRDAEMap) as the input and devel-
well as arm motions using mmWave radars. oped a cross-modality training framework that fuses multi-
• The proposed framework, SUPER, utilizes the intensity scale radar features using a Cross- and Self-Attention Module
informationfrommulti-antennaradarsystems,tocharac- (CSAM), and further refines the predicted key points through
terize the spatial occupation of human body under low a Pose Refinement Graph Convolutional Networks (PRGCN).
mobility and Doppler information to capture motions of The aforementioned works on mmWave-based HPE differ
extremities. in the number of devices used for data collection, data
• We demonstrate the feasibility of deploying two asyn- representation (point clouds vs. images), and the backbone
chronous but closely located mmWave radars to improve neural network architecture. However, none considers SUB-
spatialresolution.Anovelmaskingalgorithmisproposed HPE, where there is typically limited trunk and lower limb
to coherently fuse data from both radars. mobility. A summary of the key aspects of these methods can
• SUPER has been evaluated using different motion se- be found in Table I.
quences and data from a diverse set of users and shows
B. Public mmWave-based HPE datasets
superior performance compared to a SOTA baseline
method. Very few public datasets are currently available for
The rest of the paper is organized as follows. A review mmWave-based HPE. In [28], the authors release a dataset
of recent development of mmWave-based HPE methods and MARS containing radar point clouds and annotation obtained
public datasets is presented in Section II. In Section III, we using Microsoft Kinect V2 sensor. Chen et al. proposed
introducetheproposedpipelineandkeytechniques.SectionIV mmBody [32], a multi-scenario RGBD-paired mmWave radar
provides experiment setups and the dataset we build. Detailed (Arbe Robotics Phoenix) point cloud dataset for human pose
results and system performance are provided in Section V. reconstructionwith3Dgroundtruthprovidedbyamotioncap-
SectionVIdemonstratesthepotentialsoftheproposedsystem ture system. The work in [31] also provides a dataset HuPR,
by a downstream task. Finally, we discuss the limitations of which contains raw radar data together with 3D annotation
the work and conclude this paper in Section VII. generated from a synced RGB camera.
With the exception of HuPR, the aforementioned public
II. RELATEDWORK
datasets only contain intermediate representations of the radar
FMCW radars as an emerging technology have attracted data, e.g., point clouds. The lack of raw data greatly limits
significant attention and have been investigated in a variety of innovations on radar signal processing algorithms and conse-
sensing tasks, e.g. tracking and localization [17]–[19], gesture quentlyaffectstheinformativenessoftrainingdatatotheHPE
recognition[20]–[23],andvitalsignmonitoring[24]–[27],etc. models. Another limitation of some datasets (e.g., HuPR and
Inthissection,wefocusonmmWave-basedHPEmethodsand MARS)liesintheabsenceofaccurategroundtruthduetothe
public datasets. use of RGB or RGB-D inputs for annotations.
A. MmWave-based human pose estimation
III. METHODOLOGY
In [13], Sengupta et al. present mm-Pose, which is among
SUPER considers the problem of estimating upper body
the first works in mmWave-based full-body HPE. mm-Pose
humanposeswhenasubjectfacesmmWaveradarsensorsata
knowndistance.Theassumptionforknowndistanceisvalidin
1Demonstration videos can be found at https://super-2023-web.github.io/
SUPER/. confined environments such as in an office cubicle or inside a(a) Camera view. (b) OptiTrack motion capture view. (c) SUPER output
Fig. 1: The estimated skeleton model from SUPER vs. ground truth when a subject raises her/his hand up while seating. The
blue circle markers stand for the estimated skeleton model, and the red plus markers are the corresponding ground truth.
TABLE I: Comparison of existing works on mmWave-based HPE
Method RadarSensor GroundTruthSensor DataRepresentation BodyMotions
two2Dintensityimage WalkingLeft-ArmSwing,
mm-Pose[13] 2TIAWR1642 MicrosoftKinect
(XY-planeandXZplane) Right-ArmSwing,Both-Arms-Swing
5DPointCloud
MARS[28] 1TIIWR1443 MicrosoftKinect 10rehabilitationmovements1
(x,y,z,velocity,intensity)
6DPointCloud
mmMesh[14] 1TIAWR1843 VICONsystem 8dailyactivities2
(x,y,z,range,velocity,energy)
m4esh[30] 1TIAWR1843 VICONsystem
6DPointCloud 7dailyactivities3†
(x,y,z,range,velocity,energy) freelyperformedbymulti-person
VRDAEMap(velocity-specific staticactions,standingandwaving
HuPr[31] 2TIIWR1843 RGBcamera
range-doppler-azimuth-elevationmap)hand(s),walkingwithwavinghand(s)
6DdensePointCloud
mmBody[32]ArbeRoboticsPhoenix MoCapsystem 100motions
(x,y,z,velocity,amplitude,energy)
intensitypointcloud, upperlimbmovements,headrotation,
Ours 2TIIWR6843 OptiTracksystem
Dopplerpointcloud drivingsimulation
1 Right/left/bothlimbextension,right/leftsidelungeright/leftfrontlunge,right/leftupperbodyextension,squad.
2 Torsorotations,clockwisewalking,counter-clockwisewalking,armswing),walkingbackandforth;walkingback,andforthwitharmswing,walkingin
theplace,lunges.
3 Walkingincircles,walkingbackandforthinstraight,pickingupthephonefromthedesk,puttingdownthephoneonthedesk,answeringphonecalls
whilewalking,playingwiththecellphonewhilesittingonthechair,sittingonthechairandstandingupfromthechair.
†Freelyperformedbymulti-personinonerecording.
vehicle.Alternatively,existingapproachesformmWave-based features 3 Tx and 4 Rx antennas forming a 12 virtual an-
target localization can be adopted to determine a bounding tenna array as illustrated in Figure 2. Placed horizontally,
box around the subject [33]. In this section, we first provide this configuration results in angle resolutions of 15 degrees
an overview and the design rationale of the SUPER pipeline and 55 degrees, respectively, in the horizontal and vertical
and then present details of its individual components. directions.Toestimatefine-grainedSUB-HPE,ahighazimuth
angle resolution is necessary for extremities when the arms
A. Overview and Design Rationale
areextendedwhileahighelevationangleresolutionishelpful
in distinguishing subtle head and trunk poses. To mitigate the
limitationsoflow-endmmWaveradars,weemploytwoclosely
located radar sensors: one oriented horizontally and the other
vertically. Despite the lack of coordination, the reflected wave
fromoneradar’stransmissionisunlikelymistakenasthatfrom
the other radar since the resulting range bins are outside the
regionofinterest(ROI).Notethatalthoughdual-radarsystems
havebeenalsoemployedinmm-Pose[13]andHuPR[31],the
dataisusedtoproduce2Dheatmaps(images)inperpendicular
planes rather than being fused together in 3D point clouds.
Fig. 2: A 2-Dimensional MIMO antenna array for
IWR6843ISK radar. The separation d equals half wavelength. Several existing mmWave-based HPE methods model hu-
man body as a point cloud, which is obtained from range-
Low-end COTS mmWave radars typically have a small Doppler maps over multiple chirps of radar signals. Doppler
number of Tx and Rx antennas, which restrict their spatial information has sufficient coverage on the entire body only if
resolution. Take TI IWR6843ISK radar as an example. It there are significant motions in different body parts. In seatedTABLE II: Non-uniform Angle Sampling (unit in degree)
positions,however,movementsinthetrunkandlowlimbsare
confined leading to sparse points in space. In contrast, the
θ -70 -60 -50 -40 -30 -25 -20 -15 -10 -5 0
intensityofreflectedsignalsfromthebulkofthebodyishigh 5 10 15 20 25 30 40 50 60 70
regardless of motions as long as the subject is sufficiently
ϕ -70 -50 -30 -20 -10 0 10 20 30 50 70
close to the radars. Thus, a range-angle map, augmented
withintensityinformationfromamulti-antennasystem,better
captures the occupation of human body in space. Motivated
by this observation and with the unique characteristics of azimuth and elevation angles; for radar V, the reverse is true.
seated SUB-HPE in mind, we extract two point clouds with Clearly, as indicated in Table II, angles are densely sampled
reflectedintensityandDopplerinformation.Theablationstudy in the axis where more spaced virtual antennas are available
in Section V further substantiates the empirical evidence andnearthecenter,whileintheperpendiculardirection,fewer
supportingthecomplementarynatureofthetwoinputsources. angle bins are sampled. Consequently, amongst the 30 range
The overall system diagram of SUPER shown in Figure 3, binsbetween0.4metersand1.8metersfromthesubject,there
consists of two main processing blocks, i.e., point cloud are in total 6930 (=21×11×30) sample points in the ROI.
generation and a backbone network. The reflected RF signals Next,weapplyMinimumVarianceDistortionlessResponse
from two radar sensors are preprocessed using match filtering (MVDR) to generate an intensity spectrum for each point
and range-FFT. Dense point clouds are then generated by location in the ROI. We first estimate the correlation matrix
sampling the ROIs in 3D space centred around each radar. for each range index i, using all N chirps within one frame,
A dual-radar fusion algorithm coherently combines data from (cid:80)N yyH
two radars and samples the results to produce fine-grained R i = n= N1 ,
point cloud data representation for intensity and for Doppler.
trace(R )
Both point clouds are fed into the backbone network. The R =R +α i I ,
i i K K
networkcomprisesbuildingblocksfromPointNet[34],Point-
where y is a column vector of the received signal at each
Net++ [35], and LSTM to extract global and local features to
antenna, N is the number of chirps in one frame, K is the
predicttheSMPLposeparametersineachframe.Thepipeline
number of received antennas, and α is a control parameter to
can be easily extended to predict body shape parameters and
prevent singularity.
will be investigated as part of future work.
Next, we calculate the steering vector a from the virtual
s
B. Point cloud generation with dual-radar fusion antennas array as
In this section, we introduce a novel pipeline to generate (cid:26) exp(jπ(µ (n−1))), 1≤n≤8,
a (n)= a
qualitypointcloudsfromdatacollectedbytwocloselylocated s exp(jπ(µ (n−6−1)+µ )), 9≤x≤12,
a b
radars. Data from each radar goes through separate branches
where
to handle intensity and Doppler information. The overall
processingconsistsoftwostages:thefirststagetransformsraw µ =sin(θπ/180)cos(ϕπ/180),
a
radardatatoadensepointcloud,whichactsasanintermediate
µ =sin(ϕπ/180).
representation. In the second stage, data from the two radar b
sensorsarefusedtogetherandthensampledtoproduceafine- Finally,wecalculatetheintensityspectrumforeachsample
grained point cloud. point as
1) Dense point cloud generation: Raw I-Q samples from
1
each radar in intermediate frequency (IF) follow the standard IS(θ,ϕ,i)= ,
a HR−1a
pre-processing steps. These include mapping the raw radar s i s
data into a range map through range-FFT and DC compen- where a is the steering vector, and i is the range index. This
s
sation to eliminate static background clutters. As previously processcreatesa4Dpointcloudwithintensityvaluesinpolar
mentioned, SUPER operates under the assumption that the coordinates,whichcanthenbetransformedintoadensepoint
approximate distance between the subject and the radars is cloud in a Cartesian coordinate system centered on a radar.
known.ThisknowledgeenablesthedesignationofanROIthat Dopplerpointclouds: TogenerateDopplerpointclouds,
encompasses the subject. For example, when seated around we follow a similar procedure to that in [14]. Specifically,
1 meter away from the radars, the range bins that span the Doppler-FFT on the chirps in a frame is applied to derive 2D
subject’s body are approximately from 0.4 meters to 1.8 range-Doppler maps (30×128) of each received antenna. For
meters. These parameters can be easily adjusted given the every point in the 2D range-Doppler map, its velocity and
setup of different scenarios. power are calculated through an additional angle-FFT across
Intensitypointclouds: Togenerateintensitypointclouds, multiple received antennas. The procedure is applied to data
we further consider 180-degree field of view (FOV) in both from the two radars independently, resulting two 5D point
horizontal and vertical directions and choose a non-uniform clouds of 3840 (=128×30) points for each radar.
sampling scheme as shown in Table II. Specifically, for the It is worth noting that the term “dense” is adopted to
radar placed horizontally (radar H), θ and ϕ correspond to the differentiate this representation from the eventual fused pointFig. 3: The system diagram of SUPER. New processing blocks introduced in this paper are highlighted in orange, and
intermediate data flows are highlighted in blue.
2) Dual-radar fusion for fine-grained point clouds: To
this end, we have generated four point clouds, i.e., one 4D
intensity point cloud and one 5D Doppler point cloud from
each radar. The two radar sensors are positioned in close
proximity, approximately 15cm apart. Thus, the dense point
clouds generated by each radar sensor roughly share the
same ROI but are complementary spatially. Radar H captures
Fig. 4: Generation of dense point clouds from raw radar data. detailed information in the horizontal direction, which can be
One intensity point cloud and one Doppler point cloud are used to enhance the quality of the point cloud derived from
produced for each radar separately. radar V, and vice versa. Therefore, the purpose of dual-radar
fusionistwo-folded.First,itrefinesthepointcloudsfromone
radar using the point clouds from the other radar. Second, it
trims the over-sampled point clouds and retains only salient
points. At the end of the procedure, a single intensity point
cloudandasingleDopplerpointcloudareobtainedforfurther
processing. An overview of this process is given in Figure 5.
Masked refinement: To refine the point clouds from both
radars, we first transform their representations from polar
coordinate frames to a unified Cartesian coordinate frame.
Consider the 4D intensity point clouds from radar H as an
example. A similar procedure is applied to the intensity point
cloud from radar V and the 5D Doppler point clouds from
bothradars.LetthepointcloudfromradarHbethetargetand
that from radar V serves as a reference. For each point in the
target point cloud, the K nearest points in the reference point
cloud are identified. The mean power value of these points
is computed through averaging. The value of the point in the
Fig. 5: Generation of fine-grained point clouds by fusing and target point cloud is replaced by the product of itself and the
sampling dense point clouds from the two radars. mean value. This multiplication has the effect of masking or
suppressing points with high values in only one point cloud
and amplifying those with high values in both. Furthermore,
clouds. While the point clouds in this initial stage remain theoperationcanpreservelocalpowervariations,asthemasks
relatively sparse when compared to those generated by Lidar within the same local area are nearly identical.
sensors, they are denser than the point clouds typically found Point cloud trimming: Due to spatial over-sampling,
in existing literature on mmWave-based HPE. This increased the dense point clouds produced thus far contain redundant
density is achieved through spatial oversampling in the inten- information. To retain only informative points, we extend
sity point clouds. Further information regarding the process is the principles of the 2D Constant False Alarm Rate (CFAR)
illustrated in Figure 4. algorithm [36] and implement a 3D CFAR algorithm, byhierarchy set abstraction layers in PointNet++ are stacked to
process both the intensity and Doppler point clouds [35].
Furthermore, to exploit the temporal dependencies between
frames,twolayersofunidirectionalLongShort-TermMemory
(LSTM) cells are used [37], spanning T =20 steps or frames
(equivalent to one second). To enhance information flow, a
skip/residual link is introduced that connects features prior
to the LSTM layers and post-LSTM. Finally, after several
fully connected (FC) layers, the model outputs rotations of
(a) An intensity point cloud. (b) A Doppler point cloud.
each joint within the human skeleton model. To improve the
Fig. 6: An example fine-grained point clouds. Ground truth accuracy of rotation estimation, following [38], we represent
skeleton is shown in red. The magnitude and direction of joint rotations using 6D parameters of the rotation matrices
Doppler velocity are shown in arrows rather than 3D axis angles.
The model subsequently leverages SMPL to generate the
final joint positions. A gender-neutral model is used by fixing
the default shape parameters. For seated upper body poses,
we freeze the rotation parameters of joints in the lower body
and only estimate the positions of the upper body joints (14
joints) [7].
Thelossfunctionisdefinedasthemeansquareerror(MSE)
of the joint coordinates:
F
Loss= 1 (cid:88) ||P(f)−P(f)|| , (1)
F f,J gt,J 2
f=1
where f is the frame index, F is the total number of frames
in the batch, J denotes the joint set, P(f) is the estimated
f,J
Fig. 7: The architecture of the deep neural network backbone. positions of key joints, and P(f) is the corresponding ground
gt,J
truth positions. Note that the loss is a function of the pose
parameters (β) and global translation (t). From the experi-
adaptively calculating thresholds to detect local peaks as key ments, we find that instead of directly regressing the joint
points. Finally, we output the top 256 key points from the positions,passingthejointrotationparametersthroughSMPL
intensitypointcloudsandthetop64keypointsfortheDoppler to estimate the resulting joint position errors results in higher
point clouds. accuracy and faster convergence. This can be interpreted as a
Following the extraction of key points, we merge the point non-linear transformation of the MSE loss function using the
clouds from both radars and apply a Gaussian normalization SMPL model.
filter to the values. The final fine-grained point cloud consists The total number of learning parameters in the network
of 512 key points, featuring [x,y,z,intensity] for intensity, is 2.9 million or 2.65G FLOPs. Incoming point clouds are
and 128 key points with attributes [x,y,z,power,velocity] processed in a sliding window manner with a window size of
for Doppler. An example fine-grained point clouds generated 20 frames.
from the process is shown in Figure 6. In this example, the
IV. IMPLEMENTATIONANDDATASETS
subject raises their right hand to the top. It is evident from
Figure 6a, the intensity points are present not only around Inthissection,wepresenttheimplementationofaprototype
the raised arm but also at other areas of the upper body. In SUPER system using COTS mmWave radars and the ex-
contrast, as shown in Figure 6b, the Doppler points mainly periment results from multi-subject testbed evaluations under
appear around the raising arm with non-negligible velocity. various conditions, which are purposely chosen to closely
mimic real-life situations.
C. The deep neural network backbone A. Implementation
A deep neural network (Figure 7) is designed to take Two IWR6843ISK boards [39] together with
multiple frames of fine-grained point clouds as inputs to DCA1000EVM boards [40] are used in the experiments. The
predictjointpositionsinahumanskeletonmodel.Thenetwork radarboardsoperateat60∼64GHz(with4-GHzbandwidth)
incorporates both global and local contexts to estimate the and transmit FMCW signals. The radar front-ends include
intricate translation and rotation dynamics. To capture the 3 transmit antennas (Tx), 4 receive antennas (Rx), with
globalcontext,weincludeadedicatedbranchthatstacksthree 120◦ azimuth field of view (FoV) and elevation FoV. The 3
basicPointNet blocks [34].Toextract localinformation,three transmitting antennas emit FMCW chirps in a time-divisionB. Data collection procedure
To evaluate SUPER’s performance, we recruited 10 partic-
ipants (3 females and 7 males), aged between 21 and 46, and
with BMI in the range of 18.1∼31.6. Participants wore their
daily attire such as T-shirts, blouses, and sweaters of different
fabric materials. This research protocol has been approved by
the research ethical board (REB) from our institution.
Both radar and mocap data are collected in a 6.5m×6m
lab. The lab (Figure 9) has standard office furniture and many
(a) Markers placement: front and back.
electronic equipment and wireless transceivers (WiFi, LTE,
Bluetooth,etc.).BothradarsensorsonatripodasinFigure8b
with 1.5 meters high and 1 meter away from the subjects
and oriented at a 20-degree horizontal angle. We define a
local coordinate system with respect to radar H. During the
experiments, only one subject is present in the predefined
position.
GroundtruthofsubjectposesarecollectedfromOptiTrack,
a motion capture system [42] with 12 cameras. Both radar
sensors and the OptiTrack system are synchronized after data
collections at frame level using “synchronization” motions at
thebeginningofeachtrial.TheoutputoftheOptiTracksystem
arecoordinatesofmarkersandrigidbodiesonthebodyofpar-
ticipantsasshowninFigure8a.WeutilizeMotionBuilder[43]
(b) Co-located radar sensors.
to build a customized human actor for each participant and
Fig. 8: Experiment setup: markers and radars. generate accurate joints coordinates through motion tracking
functionalitiesbuiltinthesoftware.Videoshavebeenrecorded
during data collection for reviewing purposes but are not
TABLE III: Radar Hardware Settings. further processed.
parameters description values
N number of transmit antennas 3
tx
N number of receive antennas 4
rx
N number of virtual antennas 12
virtual
P frame duration 50 (ms)
f
f start frequency 60 (GHz)
s
f end frequency 64 (GHz)
e
t start ramp time 0 (µs)
rs
t end ramp time 58 (µs)
re
t chirp idle time 7 (µs)
idle
N number of samples per chirp 225
adc
N number of chirps per frame 128
chirp
manner, which results in a 12 virtual antennas array. Each
FMCW chirp is composed of 225 sampling points, and the
frequency of RF will increase from 60 GHz to 64 GHz. 128
chirps constitute one frame at a frame rate of 20Hz. The
acquired raw IF signal is sent to a host PC via Ethernet,
where mmWave Studio [41] is used to initiate, configure, and Fig. 9: The Lab environment for data collection.
control the radar boards. The detailed radar sensor settings is
summarized in Table III.
During the data collection process in the controlled labo-
The preprocessing steps and point cloud generation are ratory environment, subjects engaged in three distinct motion
implementedinMATLABR2021a,whichtakesrawIFsignals sequencesthataredesignedtomimicmovementswhileseated
asinput,andoutputsthefine-grained3Dpointclouddata.The in confined environments. These include: hand-reaching, driv-
neural network backbone is implemented in PyTorch. ing, and head rotation. A Microsoft Xbox Gaming steeringTABLE IV: Accuracy of joint estimations (in mm)
wheel is used to mimic a driving platform and is placed in
front of the subjects.
action method MPJPE↓ PA MPJPE↓ PCK@15mm↑
• Hand-reaching trials: Participants were instructed to use mmMesh 156.85±25.18 29.60±6.2 13.76±7.38
driving
their right hand to interact with hypothetical objects in Ours 112.46±12.70 16.32±2.45 27.38±11.15
their surroundings while keeping their left hand sta- mmMesh 148.33±25.18 26.97±4.39 15.74±8.65
handreaching
tionary. These trials included interacting with objects Ours 114.87±25.07 15.19±2.56 37.17±8.28
positioneddirectlyaboveone’shead(top),inthetopfront mmMesh 174.42±40.61 30.43±8.82 10.00±6.85
headrot.
Ours 108.85±15.46 16.16±2.59 28.46±9.34
(up-front),infrontbuttotheside(right-front),totheside
(right), and below (bottom).
• Driving trials: These trials aimed to replicate common
ground truth position. Finally, the MPJPEs are averaged over
driving activities. Subjects were instructed to perform
all frames.
routine driving (with both hands on the wheel), conduct
ThesecondmetricistheProcrustesalignmentMPJPE(PA-
traffic checks (by leaning forward and inspecting both
MPJPE) that calculates the average 3D joint distance (mm)
left and right directions), engage in a conversation with
after performing Procrustes alignment [44] on the estimated
a passenger (rotating the head towards the passenger),
and ground-truth joint sets. PA-MPJPE measures how well
execute reverse maneuvers (turning the head to see one’s
the pose estimation model captures the structural information
back over the shoulder), and operate the control panel
of the pose, rather than just its location or scale. It eliminates
(reaching the right-front area and virtually press buttons
system biases and allows for fair comparisons across different
with one’s right hand).
scales of the same pose.
• Head rotation trials: These trials capture deliberate head
Thethirdmetricisthepercentageofcorrectkeypointsunder
movements while keeping one’s torso mostly stationary.
a distance threshold e.g. 15mm (PCK@15mm). This metric
Subjects were instructed to look left and right, up and
is defined as:
down, and upper/lower left/right, etc.
K
1 (cid:88)
C. The dataset PCK@15mm=
K
δ k, (3)
In total, we conducted 30 trials from 10 participants, with k=1
each lasting around 10 minutes. The total number of radar where K is the total number of keypoints (joints), δ k is
frames collected is around 360,000 from each radar sensor. a binary value indicating whether the distance between the
The total size of all raw radar data in the dataset is around ground truth keypoint and the predicted keypoint is within a
900GB. The ground truth data for each frame contains joint certain threshold.
angles and positions of 14 upper body key joints2 and the Baselinemethod: WeadoptmmMesh[14]asthebaseline
global translations. The total size of the ground truth data is method for comparison. The choice is primarily driven by the
around 900MB. The dataset is organized by subject ID (de- fact that the model architecture was made publicly available
identified), trial name, and data types (radar data vs ground by the authors. Although m4esh is more recent, it targets
truth data). multi-subject scenarios, which are outside the scope of this
paper. The mmMesh model parameters were retrained using
V. PERFORMANCEEVALUATION the hyperparameters suggested in [14] and data from radar H
In this section, we present the performance of SUPER and only for consistency.
ablation studies.
B. Main results
A. Evaluation metrics and baseline method We calculate the average MPJPE, PA MPJPE, and
PCK@15mmofthe14upperbodyjointsinleave-one-subject-
Wechosethreemetricsinliteraturetoquantifytheaccuracy
out experiments. The results presented in TABLE IV reveal
of the estimated upper body joints. The first one is the Mean
that our approach remarkably surpasses the baseline model
PerJointPositionError(MPJPE),whichmeasurestheabsolute
by average margins(take the average of the three actions) of
averagedistance(mm)betweenthepredictedjointsofahuman
30%, 45%, and 184% on MPJPE, PA MPJPE, PCK@15mm
skeleton and the ground truth joints in a given dataset. The
respectively.
MPJPE is defined as:
Furthermore,weevaluatethemodel’seffectivenessonupper
K
E (f,J)= 1 (cid:88) ||P(f)(k)−P(f)(k)|| , (2) limbjointspivotaltohand-objectinteractions.ThetheMPJPE
MPJPE K f,J gt,J 2 and PA MPJPE of left and right wrist and elbow joints are
J
k=1
summarized in Table V.
where f denotes a frame, J denotes the joints model/set, From the results, we can see that the average accuracy of
K is the number of joints in the model/set, P(f)(k) is the wrist joints is lower than that of elbow and other upper body
f,J
estimatedpositionofjointk,andP(f)(k)isthecorresponding joints.ThiscanbeexplainedbythelowRCSofhands,making
gt,J
them difficult to be captured by radars. However, SUPER
2Thelocalbodyjointsaresettofixedsittingposes considerably outperforms mmMesh in the estimation of bothTABLEV:Accuracyofupperlimbkeyjointpositions(inmm)
We further conduct ablation experiments to evaluate the
effectiveness of Doppler and intensity point clouds in the
action method MPJPE↓ PA MPJPE↓
wrist elbow wrist elbow training data. To do so, we only input the Doppler point
mmMesh 341.96±64.83 199.94±37.81103.35±25.6366.50±18.48 cloud or the intensity point cloud and remove the respective
driving
Ours 119.46±25.71 114.30±14.16 38.95±6.98 28.45±6.20 branchinthebackbone(Figure7).TableVIreportstheresults
handreachingmmMesh 312.22±66.61 187.68±30.50 96.44±21,47 64.44±16.39 from one test subject performing different actions. Clearly,
Ours 140.46±30.77 124.86±27.79 42.57±11.65 32.30±9.78
neither intensity or Doppler point clouds alone is sufficient.
mmMesh377.14±102.65226.84±56.17104.40±29.4968.90±20.96
head Ours 131.08±26.88 127.10±31.05 38.70±9.41 27.96±6.84 Combining both sets of features leads to the highest accuracy.
Somewhat interesting, between the two, intensity point clouds
appear to be more informative.
upper arm joints. Thus, we conclude that it is important to VI. DEMONSTRATIVEAPPLICATION
design a specific pipeline for SUB-HPE, and the inclusion of
In this section, we demonstrate the utility of SUPER
intensity features and the use of radar V are instrumental in
through a downstream task that identifies hand-object inter-
improving the accuracy.
action through SUB-HPE. Note that what is being presented
actsasaproof-of-concept.Likely,moresophisticatedmethods
can be implemented for the task on top of SUB-HPE.
Fig.11:Visualizationofthegroundtruthandmodelestimated
wristtrajectoriesofa4ssequencefromahandreachingaction.
Fig.10:Constructed3Dposesinskeletonrepresentationfrom The units are in meters.
SUPER, mmMesh and Ground Truth
In this task, the aim is to determine which objects in the
3D space one is interacting with by hands. Consider a motion
Examples of constructed 3D poses in skeleton representa-
sequence where a hand starts from some resting position,
tionsusingSUPER,mmMeshandGroundTruthcanbefound
moves toward an object at known location, and then interacts
in Fig 10.
withtheobjectforaperiodoftime.Wetransformtheproblem
of object identification to a localization problem, namely, to
C. Ablation study
determine whether one’s hand (a wrist joint specifically) falls
intothepredefinedboundingboxesaroundtargetlocationsfor
TABLE VI: Results from Ablation Study
a sufficient amount of time.
To test this idea, we first calculate the amount of displace-
action information MPJPE↓ PA MPJPE↓ PCK@15mm↑
ment of a wrist joint during 1s windows in the ground truth
Doppleronly 237.33±26.72 44.78±2.29 0.37±1.46
driving intensityonly 196.97±32.89 20.09±6.94 25.36±15.98 trajectory. The intervals that the total displacement is less
Doppler+intensity 101.51±19.06 12.27±3.41 47.40±17.18 than a predefined threshold (100mm in the implementation)
Doppleronly 266.28±36.64 48.49±3.74 0.08±0.91 indicate either the initial rest position or the rendezvous point
handreaching intensityonly 193.05±43.37 19.26±7.94 26.75±19.83
betweenthehandandatargetobject.Wecomputethecentroid
Doppler+intensity 110.73±30.91 12.51±5.86 47.10±20.53
of the wrist joint positions in such intervals and test against
Doppleronly 221.46±31.73 46.04±1.21 0.02±0.63
head intensityonly 190.71±44.41 19.05±7.47 27.68±18.90 the ground truth target locations. As an example, consider the
Doppler+intensity 99.67±28.14 12.28±5.87 45.25±18.79 groundtruthandestimatedtrajectoriesasshowninFigure11.
*Thestandarddeviationinthistableiscalculatedacrosstheestimatedpositionerrorsper In this example, one’s hand travels from B to A and then
jointandperframe.
reaches a target location C. Although the estimated trajectorydoes not exactly coincide with the ground truth one, it can be [10] Y. Xu, J. Zhang, Q. Zhang, and D. Tao, “Vitpose: Simple vision
observed as the hand approaches and stays around the target transformerbaselinesforhumanposeestimation,”inAdvancesinNeural
InformationProcessingSystems,2022.
location, the estimated locations are close to C.
[11] Y. Huang, M. Kaufmann, E. Aksan, M. J. Black, O. Hilliges, and
We conduct experiments on all subjects using the hand- G.Pons-Moll,“Deepinertialposer:Learningtoreconstructhumanpose
reaching trials. The results show that in 88.80% of the rest fromsparseinertialmeasurementsinrealtime,”ACMTransactionson
Graphics(TOG),vol.37,no.6,pp.1–15,2018.
position or the rendezvous point intervals, the centroid of the
[12] V.Mollyn,R.Arakawa,M.Goel,C.Harrison,andK.Ahuja,“Imuposer:
estimatedwrittrajectoryfallsintoaboundingboxcenteredon Full-bodyposeestimationusingimusinphones,watches,andearbuds,”
the target location with a side length of 0.2m. in Proceedings of the 2023 CHI Conference on Human Factors in
ComputingSystems,2023,pp.1–12.
VII. DISCUSSIONANDCONCLUSION [13] A.Sengupta,F.Jin,R.Zhang,andS.Cao,“mm-pose:Real-timehuman
skeletal posture estimation using mmwave radars and cnns,” IEEE
In this work, we proposed SUPER, a pipeline for SUB- SensorsJournal,vol.20,no.17,pp.10032–10044,2020.
HPE.Toaddressthechallengesofnuancedupperbodymove- [14] H. Xue, Y. Ju, C. Miao, Y. Wang, S. Wang, A. Zhang, and L. Su,
“mmMesh: Towards 3D real-time dynamic human mesh construction
ments when seated, we obtained both intensity and Doppler
usingmillimeter-wave,”inProceedingsofthe19thAnnualInternational
point clouds by fusing data coherently from two radars with Conference on Mobile Systems, Applications, and Services, 2021, pp.
orthogonal orientations. Compared to a baseline method that 269–282.
[15] J. Geng, D. Huang, and F. De la Torre, “Densepose from wifi,” arXiv
onlyutilizesDopplerpointcloudsfromasingleradar,SUPER
preprintarXiv:2301.00250,2022.
has superior performance in terms of all metrics for HPE. [16] P. Hu¨gler, M. Geiger, and C. Waldschmidt, “Rcs measurements of a
The current SUPER framework assumes the presence of human hand for radar-based gesture recognition at e-band,” in 2016
GermanMicrowaveConference(GeMiC). IEEE,2016,pp.259–262.
a single subject and the knowledge of the ROI. It can be
[17] T. Gu, Z. Fang, Z. Yang, P. Hu, and P. Mohapatra, “Mmsense: Multi-
easily extended to multiple subjects and unknown ROIs when persondetectionandidentificationviammwavesensing,”inProceedings
combined with a target detection component. The current of the 3rd ACM Workshop on Millimeter-wave Networks and Sensing
modelcanalsobetrainedwithadditionalmesherrorsinSMPL
Systems,2019,pp.45–50.
[18] C. Wu, F. Zhang, B. Wang, and K. R. Liu, “mmtrack: Passive
and a term reflecting temporal consistency and smoothness
multi-person localization using commodity millimeter wave radio,” in
of human movements [45]. Doing so is expected to further IEEEINFOCOM2020-IEEEConferenceonComputerCommunications.
improve the accuracy and realism of the inferred poses. IEEE,2020,pp.2400–2409.
[19] P. Zhao, C. X. Lu, J. Wang, C. Chen, W. Wang, N. Trigoni, and
Future research directions for mmWave-based SUB-HPE
A. Markham, “mid: Tracking and identifying people with millimeter
also include developing models that are robust to different wave radar,” in 2019 15th International Conference on Distributed
deploymentenvironmentsandtheinvestigationofmoredown- ComputinginSensorSystems(DCOSS). IEEE,2019,pp.33–40.
[20] J. Lien, N. Gillian, M. E. Karagozler, P. Amihood, C. Schwesig,
stream tasks.
E. Olson, H. Raja, and I. Poupyrev, “Soli: Ubiquitous gesture sensing
with millimeter wave radar,” ACM Transactions on Graphics (TOG),
REFERENCES
vol.35,no.4,pp.1–19,2016.
[21] H. Liu, A. Zhou, Z. Dong, Y. Sun, J. Zhang, L. Liu, H. Ma, J. Liu,
[1] Y.Liu,J.Yang,X.Gu,Y.Guo,andG.-Z.Yang,“Ego+x:Anegocentric
and N. Yang, “M-gesture: Person-independent real-time in-air gesture
vision systemfor global 3d humanpose estimation andsocial interac-
recognitionusingcommoditymillimeterwaveradar,”IEEEInternetof
tion characterization,” in 2022 IEEE/RSJ International Conference on
ThingsJournal,vol.9,no.5,pp.3397–3415,2021.
IntelligentRobotsandSystems(IROS),2022,pp.5271–5277.
[22] S. Palipana, D. Salami, L. A. Leiva, and S. Sigg, “Pantomime: Mid-
[2] J.Wang,K.Qiu,H.Peng,J.Fu,andJ.Zhu,“Aicoach:Deephumanpose
airgesturerecognitionwithsparsemillimeter-waveradarpointclouds,”
estimationandanalysisforpersonalizedathletictrainingassistance,”ser.
ProceedingsoftheACMoninteractive,mobile,wearableandubiquitous
MM’19. NewYork,NY,USA:AssociationforComputingMachinery,
technologies,vol.5,no.1,pp.1–27,2021.
2019.[Online].Available:https://doi.org/10.1145/3343031.3350910
[23] A.Khamis,B.Kusy,C.T.Chou,M.-L.McLaws,andW.Hu,“Rfwash:a
[3] T.AnvariandK.Park,“3dhumanbodyposeestimationinvirtualreality:
weaklysupervisedtrackingofhandhygienetechnique,”inProceedings
A survey,” in 2022 13th International Conference on Information and
of the 18th conference on embedded networked sensor systems, 2020,
CommunicationTechnologyConvergence(ICTC),2022,pp.624–628.
pp.572–584.
[4] Y.Zhou,H.Huang,S.Yuan,H.Zou,L.Xie,andJ.Yang,“Metafi++:
Wifi-enabled transformer-based human pose estimation for metaverse [24] Z.Yang,P.H.Pathak,Y.Zeng,X.Liran,andP.Mohapatra,“Monitoring
avatarsimulation,”IEEEInternetofThingsJournal,vol.10,no.16,pp. vital signs using millimeter wave,” in Proceedings of the 17th ACM
14128–14136,2023. internationalsymposiumonmobileadhocnetworkingandcomputing,
[5] S.Y.ChengandM.Trivedi,“Turn-intentanalysisusingbodyposefor 2016,pp.211–220.
intelligentdriverassistance,”IEEEPervasiveComputing,vol.5,no.4, [25] P.Zhao,C.X.Lu,B.Wang,C.Chen,L.Xie,M.Wang,N.Trigoni,and
pp.28–37,2006. A.Markham,“Heartratesensingwitharobotmountedmmwaveradar,”
[6] C.E.Matthews,S.A.Carlson,P.F.Saint-Maurice,S.Patel,E.Salerno, in 2020 IEEE International Conference on Robotics and Automation
E.Loftfield,R.P.Troiano,J.E.Fulton,J.N.Sampson,C.Tribbyetal., (ICRA). IEEE,2020,pp.2812–2818.
“Sedentary behavior in united states adults: Fall 2019,” Medicine and [26] F.Wang,X.Zeng,C.Wu,B.Wang,andK.R.Liu,“mmhrv:Contactless
scienceinsportsandexercise,vol.53,no.12,p.2512,2021. heart rate variability monitoring using millimeter-wave radio,” IEEE
[7] M. Loper, N. Mahmood, J. Romero, G. Pons-Moll, and M. J. InternetofThingsJournal,vol.8,no.22,pp.16623–16636,2021.
Black, “Smpl: A skinned multi-person linear model,” ACM Trans. [27] B. Zhang, B. Jiang, R. Zheng, X. Zhang, J. Li, and Q. Xu, “Pi-vimo:
Graph., vol. 34, no. 6, oct 2015. [Online]. Available: https: Physiology-inspiredrobustvitalsignmonitoringusingmmwaveradars,”
//doi.org/10.1145/2816795.2818013 ACMTransactionsonInternetofThings,vol.4,no.2,pp.1–27,2023.
[8] A.A.Osman,T.Bolkart,andM.J.Black,“Star:Sparsetrainedartic- [28] S. An and U. Y. Ogras, “Mars: mmwave-based assistive rehabilitation
ulated human body regressor,” in Computer Vision–ECCV 2020: 16th systemforsmarthealthcare,”ACMTransactionsonEmbeddedComput-
EuropeanConference,Glasgow,UK,August23–28,2020,Proceedings, ingSystems(TECS),vol.20,no.5s,pp.1–22,2021.
PartVI16. Springer,2020,pp.598–613. [29] C.R.Qi,H.Su,K.Mo,andL.J.Guibas,“Pointnet:Deeplearningon
[9] Z.Cao,G.H.Martinez,T.Simon,S.Wei,andY.A.Sheikh,“Openpose: pointsetsfor3dclassificationandsegmentation,”inProceedingsofthe
Realtime multi-person 2d pose estimation using part affinity fields,” IEEEconferenceoncomputervisionandpatternrecognition,2017,pp.
IEEETransactionsonPatternAnalysisandMachineIntelligence,2019. 652–660.[30] H.Xue,Q.Cao,Y.Ju,H.Hu,H.Wang,A.Zhang,andL.Su,“M4esh:
mmwave-based 3d human mesh construction for multiple subjects,” in
Proceedings of the 20th ACM Conference on Embedded Networked
SensorSystems,2022,pp.391–406.
[31] S.-P.Lee,N.P.Kini,W.-H.Peng,C.-W.Ma,andJ.-N.Hwang,“Hupr:
Abenchmarkforhumanposeestimationusingmillimeterwaveradar,”
inProceedingsoftheIEEE/CVFWinterConferenceonApplicationsof
ComputerVision,2023,pp.5715–5724.
[32] A. Chen, X. Wang, S. Zhu, Y. Li, J. Chen, and Q. Ye, “mmbody
benchmark:3dbodyreconstructiondatasetandanalysisformillimeter
waveradar,”inProceedingsofthe30thACMInternationalConference
onMultimedia,2022,pp.3501–3510.
[33] W. Chen, H. Yang, X. Bi, R. Zheng, F. Zhang, P. Bao, Z. Chang,
X. Ma, and D. Zhang, “Environment-aware multi-person tracking in
indoor environments with mmwave radars,” Proc. ACM Interact. Mob.
WearableUbiquitousTechnol.,vol.7,no.3,sep2023.
[34] R. Q. Charles, H. Su, M. Kaichun, and L. J. Guibas, “Pointnet: Deep
learning on point sets for 3d classification and segmentation,” in 2017
IEEEConferenceonComputerVisionandPatternRecognition(CVPR),
2017,pp.77–85.
[35] C.R.Qi,L.Yi,H.Su,andL.J.Guibas,“Pointnet++:Deephierarchical
feature learning on point sets in a metric space,” in Proceedings of
the 31st International Conference on Neural Information Processing
Systems,ser.NIPS’17. RedHook,NY,USA:CurranAssociatesInc.,
2017,p.5105–5114.
[36] M.A.Richards,FundamentalsofRadarSignalProcessing,2ndEdition.
McGrawHill,2005.
[37] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural
Comput., vol. 9, no. 8, p. 1735–1780, nov 1997. [Online]. Available:
https://doi.org/10.1162/neco.1997.9.8.1735
[38] Y. Zhou, C. Barnes, J. Lu, J. Yang, and H. Li, “On the continuity
of rotation representations in neural networks,” in 2019 IEEE/CVF
ConferenceonComputerVisionandPatternRecognition(CVPR),2019,
pp.5738–5746.
[39] T. Instruments, “Iwr6843isk,” 2020. [Online]. Available: https:
//www.ti.com/product/IWR6843
[40] ——, “Dca1000evm,” 2020. [Online]. Available: https://www.ti.com/
tool/DCA1000EVM
[41] ——, “mmwave studio,” 2020. [Online]. Available: http://www.ti.com/
tool/MMWAVE-STUDIO
[42] OptiTrack, “Optitrack: Motion capture systems,” 2020. [Online].
Available:https://www.optitrack.com/
[43] AUTODESK, “Motionbuilder,” 2022. [Online]. Available: https:
//www.autodesk.com/
[44] J.Gower,“Generalizedprocrustesanalysis.”Psychometrika,vol.40,p.
33–51,1975.
[45] C.Zheng,W.Wu,C.Chen,T.Yang,S.Zhu,J.Shen,N.Kehtarnavaz,
andM.Shah,“Deeplearning-basedhumanposeestimation:Asurvey,”
ACMComputingSurveys,vol.56,no.1,pp.1–37,2023.