UNDERSTANDING ALIGNMENT IN MULTIMODAL
LLMS: A COMPREHENSIVE STUDY
ElmiraAmirloo∗,Jean-PhilippeFauconnier∗,ChristophRoesmann∗,
ChristianKerl†,RinuBoney†,YusuQian,ZiruiWang,
AfshinDehghan,YinfeiYang,ZheGan,PeterGrasch
AppleInc.
ABSTRACT
Preferencealignmenthasbecomeacrucialcomponentinenhancingtheperfor-
manceofLargeLanguageModels(LLMs),yetitsimpactinMultimodalLarge
Language Models (MLLMs) remains comparatively underexplored. Similar to
languagemodels,MLLMsforimageunderstandingtasksencounterchallengeslike
hallucination. InMLLMs,hallucinationcanoccurnotonlybystatingincorrect
factsbutalsobyproducingresponsesthatareinconsistentwiththeimagecontent.
A primary objective of alignment for MLLMs is to encourage these models to
alignresponsesmorecloselywithimageinformation. Recently,multipleworks
haveintroducedpreferencedatasetsforMLLMsandexamineddifferentalignment
methods,includingDirectPreferenceOptimization(DPO)andProximalPolicy
Optimization (PPO). However, due to variations in datasets, base model types,
and alignment methods, it remains unclear which specific elements contribute
mostsignificantlytothereportedimprovementsintheseworks. Inthispaper,we
independentlyanalyzeeachaspectofpreferencealignmentinMLLMs. Westartby
categorizingthealignmentalgorithmsintotwogroups,offline(suchasDPO),and
online(suchasonline-DPO),andshowthatcombiningofflineandonlinemethods
canimprovetheperformanceofthemodelincertainscenarios. Wereviewavariety
ofpublishedmultimodalpreferencedatasetsanddiscusshowthedetailsoftheir
constructionimpactmodelperformance. Basedontheseinsights,weintroducea
novelwayofcreatingmultimodalpreferencedatacalledBias-DrivenHallucination
Sampling(BDHS)thatneedsneitheradditionalannotationnorexternalmodels,
and show that it can achieve competitive performance to previously published
alignmentworkformultimodalmodelsacrossarangeofbenchmarks.
1 INTRODUCTION
RecentadvancementsinMultimodalLargeLanguageModels(MLLMs)havesignificantlyimproved
our understanding of vision-language tasks. By integrating visual signals with Large Language
Models(LLMs),thesemodelshavedemonstratedenhancedcapabilitiesinmultimodalunderstanding,
reasoning,andinteraction(Liuetal.,2023c;2024;Baietal.,2023;McKinzieetal.,2024).
Typically,MLLMsarepre-trainedonlargeimage-textdatasetstodevelopfoundationalmultimodal
knowledgeandskills,thenundergopost-trainingforconversationalcapabilities,instructionfollowing,
helpfulness,andsafety. Despiterapidadvancementsinrecentyears,significantchallengespersist.
AnotableproblemisthetendencyofMLLMstoproduceresponsesthatarenotfactuallygroundedin
thevisualinput,commonlyreferredtoashallucinations,leadingtoinaccuraciessuchasincorrect
descriptionsofnon-existentvisualelements(Liuetal.,2023a;Cuietal.,2023). Thisunderminesthe
trustworthinessofMLLMsinmanypracticalapplications.
Preference alignment methods have proven effective in reducing hallucinations and generating
responses more closely aligned with human preferences for LLMs (Zhao et al., 2023b; Rafailov
etal.,2023;Azaretal.,2024;Guoetal.,2024;Yuanetal.,2024;Ahmadianetal.,2024;Tangetal.,
∗Authorscontributedequallyasfirstauthors.
†Authorscontributedequally.
1
4202
luJ
2
]VC.sc[
1v77420.7042:viXraRL-BasedSolution
Prompt:
•Trainrewardmodelrϕ(x,y)
Whatistheflockof usingpreferencedata.
birdsdoinginthe
image? •Solvealignmentproblem
viaPPO,REINFORCE,RLOO,...
x∼p
max E x∼p,y∼µ(cid:2) r ϕ(x,y)(cid:3) −βD KL(π θ|π ref)
Alignment θ
RewardMaximization Regularization
DirectAlignmentfromPreferences(y+,y−)
OfflineDAP:PairwisePreferences OnlineDAP:AnnotatorModel
(y+,y−)∼µ
•Samplefrommodelπθ(·|x),e.g.,
Chosenresponsey+: Theflockofbirdsis
flyingoveragrassyfield,withsomeofthem Sampledresponsey˜: Theflockofbirds
landingontheground. isflyingoveragrassyfield,withsomeof
themlandingontheground.
Rejectedresponsey−: Theflockofbirdsis •Ranksampledresponsesviaannotator
perchedonthebranchesofalargetree,with toobtainpreferencepair(y+,y−).
someofthemtakingoffintothesky.
Supervisedbinaryclassificationproblem(DPO,SLiC,IPO,...)
Figure1: IllustrationofthealignmentobjectiveformultimodalLLMs. Thealignmentproblemis
formulated as reward maximization w.r.t. the parameters of the policy π . A regularization term
θ
providesapositiverewardforstayingclosetoareferencepolicyπ . ForDAPmethods,thereward
ref
maximization problem is transformed to a supervised learning problem by using a closed-form
solutionforpreferencepairs. Bothonlineandofflinemethodsdrawfromapromptdistributionx∼p
thatcomprisestextandimageprompts. However,forthesamplingdistribution(y+,y−)∼µ,offline
methodsdrawfromafixedpreferencedatasetwhileonlineonessamplefromthepolicy, π , and
θ
rank/scorewithanannotatororrewardmodel.
2024a). Thesemethodsutilizepairwisepreferencedatatofine-tunethemodel,whichcanbebased
onReinforcementLearningfromHumanFeedback(RLHF)(Christianoetal.,2017;Stiennonetal.,
2020),DirectAlignmentfromPreferences(DAP)(Rafailovetal.,2023;Azaretal.,2024;Zhaoetal.,
2023b),orOnlineDirectAlignmentfromPreferences(Online-DAP)(Yuanetal.,2024;Guoetal.,
2024).
While alignment in LLMs has been extensively studied, alignment for MLLMs has not yet been
investigatedtothesameextent. Sunetal.(2023)andZhouetal.(2024)alignedLLaVA1.5(Liu
etal.,2023b)usingProximalPolicyOptimization(PPO)andDirectPreferenceOptimization(DPO),
respectively, while Li et al. (2023a) and Yu et al. (2023b) employed DPO and its variations to
alignQwen-VL(Baietal.,2023)andMuffin(Yuetal.,2023a)models. Notably,besidesdifferent
alignmentstrategiesandoftendifferentbasemodels,alltheseworksalsointroducenovelpreference
datasetsforalignmentwithvarioussizes,collection,andgenerationschemes. Asaresult,whileeach
ofthesestudiesoffersvaluableinsightsintoalignmentforMLLMs,itcansometimesbedifficultto
stronglyattributereportedimprovementstotheindividualproposedchoices.
In this paper, we examine each component of multimodal alignment independently. First, we
categorizealignmentmethodsintotwotypes(seeFigure1): offlinemethods,whichutilizepreference
pairscollectedpriortotraining(e.g.,DPO),andonlinemethods,whichinvolvesamplingfromthe
modelduringpolicyoptimization(e.g.,RLHFandOnline-DAP).Weconductacomprehensivestudy
overpopularonlineandofflinealignmentmethods,allaligningthepopularLLaVA1.6model(Liu
etal.,2024)usingafixeddataregimentandstudytheirbenefitsandshortcomings. Toourknowledge,
thisisthefirsttimethatsuchstudyisconductedwithMLLMs.
2Further,westudythedifferentmethodsforbuildingpairwisepreferencesusingpublicdatasets. We
breakdownthemaincomponentsofpreferencedataintothreeparts: prompts,chosenresponses
andrejectedresponses(Table1). Foreachofthosecomponents,weinvestigatehowtheirsource,
diversity,andqualitycanaffecttheresultingalignment. Additionally,weexaminehowthesizeofthe
alignmentdatasetimpactsdownstreamperformance.
Based on our comprehensive ablations, we identify a few key desiderata in alignment strategies
forMLLMsandintroduceasimple,novelpreferencedatasamplingschemewecallBias-Driven
HallucinationSampling(BDHS).Despitenotutilizinganyhumanannotationnortheinputofany
externalteachermodelsuchasGPT4-V,weshowthatBDHScanachievecompetitiveperformance
againstevenmuchlargerpreferencedatasetsconstructedunderdifferentregimes.
2 ALIGNMENT
Preferencealignmentusespairwisepreferencedata. Eachpairislinkedtoatextprompt,denoted
asx ,andanassociatedimage,x ,togetherformingtheinputx=(x ,x ). Theresponses
text img img text
include a preferred one, y+, and a non-preferred or rejected one, y−. See Section 3 for a more
thorough discussion of these components. In this section, we focus on the various ways that
preferencedataset,D ={(x,y+,y−)}N ,isusedbyalignmentapproaches.
i=1
2.1 REINFORCEMENTLEARNINGFROMHUMANFEEDBACK(RLHF)
RLHF was the initial method used for alignment (Christiano et al., 2017; Stiennon et al., 2020),
involving the training of a reward model (RM) from pairwise preferences and then optimizing a
policyusingtheRMviareinforcementlearning(RL).InRLHF,arewardmodelisinitiallytrainedon
thepreferencepairsasdescribedinStiennonetal.(2020). Thetrainingofthisrewardmodelusesa
straightforwardcross-entropyloss,treatingthebinarychoice–preferredorrejected–asacategorical
label. Theobjectivefunctionfortrainingtherewardmodel,r ,isasfollows:
ϕ
L =−log(cid:0) σ(cid:0) log(cid:0) r (x,y+)−r (x,y−)(cid:1)(cid:1)(cid:1) , (1)
RM ϕ ϕ
whereσisthelogisticfunction.
Next,themodel(i.e.,policy),π ,isfine-tunedthroughRLusingthetrainedrewardmodeltooptimize
θ
thefollowingobjective:
maxE [r (x,y)−βD (π (y|x)|π (y|x))] . (2)
πθ
x∼D,y∼πθ(y|x) ϕ KL θ ref
AnadditionalKLpenaltytermD (·)isincorporatedtodiscouragesignificantdeviationsofπ from
KL θ
theinitialmodel,π (Stiennonetal.,2020),andβ isahyperparameterwhichadjuststheeffectof
ref
thistermintheoverallobjective.
SincetheRMislearnedinallRL-basedapproaches,evenifitisadecentapproximationofwhatwe
aretrulyoptimizingfor,suchashumanpreferences,itremainsanimperfectapproximation. Previous
workhasshownthatifnothandledcarefully,over-optimizingfortheRMcanhurttheperformance
ofthealignedmodel(Gaoetal.,2023a). Thiscomplexityaddsasignificantlayerofchallengeto
RLHFmethods.
Different RL algorithms apply unique strategies to optimize the RL objective (Equation 2). In
Section 5.5 we investigate the complexities of RL-based alignment for MLLMs, examining how
differentalgorithmsaffectmodelperformance. Specifically,weevaluatetheimpactofusingPPO
(Schulmanetal.,2017;Stiennonetal.,2020;Ouyangetal.,2022)andREINFORCELeave-One-Out
(RLOO)(Williams,1992;Ahmadianetal.,2024)incomparisontootheralignmentmethods.
2.2 DIRECTALIGNMENTFROMPREFERENCE
Thisfamilyofapproachesdirectlyutilizespreferencedata,D,tooptimizethepolicy,π . Byeliminat-
θ
ingtheneedtotrainarewardmodel,thesemethodssignificantlysimplifythepreferenceoptimization
pipeline. Furthermore,thegradientofallobjectivescanbepreciselycomputed,distinguishingthese
methodsfromtraditionalRLHFapproaches. ThemostwidelyusedobjectiveinMLLMalignmentis
DPO(Rafailovetal.,2023)(Equation3). Wehaveconductedthemajorityofourexperimentsusing
3DPOtoensurecomparabilitywithotherstudiesinMLLMalignment.InSection5.7,wealsoexamine
DPOalongsidetwoothercommonofflinemethods,IPO(Azaretal.,2024)andSLiC(Zhaoetal.,
2023b). ForaunifiedderivationofcommondirectalignmentmethodsrefertoTangetal.(2024b)
andforbrevity,weonlyrecaptheDPOlossfunction:
(cid:20) (cid:18) π (y+|x)π (y−|x)(cid:19)(cid:21)
L (π ;π )=E −logσ βlog θ ref . (3)
DPO θ ref (x,y+,y−)∼D π (y+|x)π (y−|x)
ref θ
Forsimplicity,wewillomitthedependencyonπ fromsubsequentequations.
ref
Itisimportanttonotethatmostpreferencedatasetsarenotderivedfromthemodelbeingaligned
andarecollectedoffline. Evenwhenthedataisconstructedbasedonthemodelthatisundergoing
alignment,thesamplesencounteredduringtrainingdonotaccountforchangesinthemodelover
training. Thisleadstoadistributionshiftbetweenthemodelthatgeneratedthedataandthemodel
beingaligned,whichcanbeconsideredadisadvantageofthesemethods.
2.3 ONLINEDIRECTALIGNMENTFROMPREFERENCE
Recently,anewfamilyofalgorithmshasbeenproposedforaligningLLMs. Thesemethodsdonot
trainaseparaterewardmodel. Instead,theyemployeitherthemodelthatisbeingaligned(Yuan
etal.,2024)oradifferentLLM(Guoetal.,2024)toobtainonlinefeedbacktocreatepreferencepairs.
ThesepairsarethenusedtooptimizetheobjectivefunctionviaforexampleDPO.Thisapproach
eliminatesthecomplexityoftrainingaseparaterewardmodelwhilestilltakingadvantageofonline
samplesfromthemodel,therebyavoidingdistributionshifts.
WeexploretheuseofLLaVA1.6-34B(Liuetal.,2024)asannotatortogenerateonlinepreference
pairs,motivatedbyitsstrongperformanceonamultitudeofmultimodalbenchmarks. Additionally,
weinvestigateahybridapproachthatcombinesonlineandofflineapproaches. Thismethodinvolves
sampling from the offline preference data with a probability p, (y+,y−), and sampling from the
modelwithaprobability1−p,(y˜+,y˜−). Equation(4)detailsthisapproach.
L (π )=E (cid:2) αL (y+,y−,x;π )+(1−α)L (y˜+,y˜−,x;π )(cid:3) ,
Mixed-DPO θ (x,y+,y−)∼D DPO θ DPO θ (4)
(y˜+,y˜−)∼πθ
whereα∼Bernoulli(p). Inourexperimentsweusep=0.5. Thisalgorithmissimilartotechniques
usedinoff-policyRLmethodslikeQ-learning(Hesteretal.,2018),whereareplaybufferincludes
samplesfromboththemodelandexpertdemonstrations.Wefoundthisapproachparticularlyeffective
whentheonlineandofflinemethodshavecomplementaryeffectsonthemodel’sfinalperformance.
3 MULTIMODAL PREFERENCE DATA
Multimodal preference data is usually constructed by using responses generated by one or more
MLLMs,typicallyexcludingthemodelbeingaligned. Inthissection,weexplainthestructureand
componentsofthedataanditsgenerationaswellasanalyzetheseelementswithinthecontextof
recentlypublisheddatasets. Whilepreferencedataisoftendiscussedintheofflinesettingofcollected
andstoreddatasets,onlineapproachessuchasonlineDPOinherentlygeneratepreferencedataonline,
followingthesamestructureandcomponentsasintroducedbelow.
3.1 ELEMENTSOFPREFERENCEDATA
Examplesofpreferencedataformultimodalalignmentgenerallyinvolvethreedifferentelements,
whichwecategorizehere. AlsoseeFigure1forhowthesecomponentsareusedduringalignment.
• Promptscomprisethetextinstructionandthecorrespondingimage. Theycanbegeneral(e.g.,
Whatisthetitleofthebookmentionedintheimage?) ordomainspecialized(e.g., Youarea
drivingassistant. Basedonthecurrentimage,whatisthebestactiontotakewhenyouaredriving
ontheroad?).
• Chosenresponsesareresponsesthatawellalignedpolicymodelshallpreferovertherejected
responses. Theymaycomprisehallucinationfreeresponses,accuratefactualinformationand
instruction-followingresponses.
• Rejectedresponsesshallscorelow/shallnotbepreferredoverchosenresponses,givenawell
alignedpolicy.
4Type Name Size Prompt Response Judge Preference
Text Image Chosen Rejected Signal
LLaVA-RLHF 10k LLaVA-Instruct-150k COCO LLaVA1.5 LLaVA1.5 human ranking
RLHF-V 5.7k† UniMM-Chat Various† Muffin/Various†(corrected) Muffin/Various† human construction
VLFeedback 80k 9datasets(LLaVA,SVIT,etc.) 12MLLMs(LLaVA1.5,GPT-4V,etc.) GPT-4V ranking
NLF 63k LLaVA-Instruct-150k COCO DRESSft(refined) DRESSft GPT-4V construction
POVID 17k LLaVA-Instruct-150k COCO SFTGroundtruth SFTGroundtruth(corrupted) GPT-4V construction
Table1: Recentlypublishedmultimodalpreferencedatasets. †denotestheupdateddatasetversionof
RLHF-VpublishedonHuggingFaceHub.
3.2 BACKGROUND
Recently,multipleworkshaveproposedpreferencedatasetsforMLLMs. Thepreferencepairsare
constructedbasedonhumanannotationsorderivedfromsynthetictechniques. InTable1,wereport
recentlypublisheddatasets.
Humanannotations InLLaVA-RLHF(Sunetal.,2023), authorscollecthumanpreferencesby
askingcrowdworkersfacedwithtworesponsestoprioritizeresponsesthatexhibitthebestmultimodal
alignmentandminimizehallucinations.Usingthisprocess,theauthorsbuilta10kpreferencesdataset.
ThepromptsarefromLLaVA-Instruct-150k(Liuetal.,2023c),whileresponsesaresampledfrom
LLaVAbasemodel. Asiscommonformanyotherpreferencesdatasets,thesourceoftheimagesis
COCO(Linetal.,2014).
InRLHF-V,Yuetal.(2023b)proposetocollecthumanpreferencesatthesegmentlevelbyasking
annotatorstocorrectmistakesinmodelresponses. Asaresult,thepreferenceisexpressedontoken
spansandnotattheresponselevel. Usedinconjunctionwithatoken-weightedDPOtraining,authors
reportedareducedhallucinationslevel. PromptsandimagesareoriginallyfromtheUniMM-Chat
SFTdatasetintroducedbyYuetal.(2023a),andresponsessentforannotationandcorrectionare
sampledfromMuffin(Yuetal.,2023a). Inthelatestiterationsofthisdataset,boththesourcesof
dataandthesampleshavebecomemorediverse.
Synthetic annotations In DRESS (Chen et al., 2023), authors introduce NLF, a 63k pairwise
preferencedatasetbuiltfromLLaVA-Instruct-150kimagesandprompts. AuthorsleverageGPT-4to
providecritiqueandrefinementontheresponsesoftheirin-houseDRESSmodel. InVLFeedback(Li
etal.,2023a),authorssampleresponsesfromapoolof12multimodalMLLMs—includingGPT-4V,
theLLaVA1.5seriesmodels,Qwen-VL-Chat,InstructBLIP—onapoolofdatasets. Thissynthetic
approachallowstoimportantlyscaleupboththenumberofexamplesgeneratedandthediversity
oftheresponse. Totalling80ksamples,thepromptsandimagesarefrom9diversedatasetssuchas
LLaVA-Instruct-150k(COCOimages),SVIT(Zhaoetal.,2023a)(VisualGenomeimages),LLaVAR
(Zhangetal.,2023)(LAION-5Bimages),etc. GPT-4Visusedtogradeandselectthebestanswer
amongtheMLLMresponses.
InPOVID,Zhouetal.(2024)proposetogeneratedispreferencesfromagroundtruthdatasetdirectly,
removingtheneedforrankingresponses. Specifically,17kexamplesareselectedrandomlyfromthe
LLaVA-Instruct-150kdataset,withtheoriginalanswersassumedtobeapreferredresponse,while
thedispreferredresponseisderivedbypromptingGPT-4Vtointroducemistakesinthepreferred
response. Theauthorsmakeadistinctionbetweenhallucinationsaddedintocaptioningtasks(e.g.,
wrongvisualattributes,incorrectrelationships)andintoreasoningtasks(e.g.,wrongreasoningbased
onahallucination).TheyexplicitlypromptGPT-4Vtoincorporatethesespecifictypesofinaccuracies
intothechosenresponses.
Preference signal In addition to considering these works in terms of using human or synthetic
supervisionasabove,anotherkeydistinctionamongthemishowthepreferencesignaliscomposed.
ThepreferencesignalisobtainedbyrankinginLLaVA-RLHF(Sunetal.,2023)andVLFeedback
Lietal.(2023a): responsesaresampled,thenrankedbyhumansorGPT-4V.Forotherworks,the
preferencesignalisaconstructionobtainedeitherbycorrecting/refiningresponsestoproducechosen
responses,asdonebyRLHF-V(Yuetal.,2023b)andDRESS(Chenetal.,2023),orbycorrupting
responsestoproducerejectedonesasPOVID(Zhouetal.,2024).
5
namuH
citehtnyS3.3 POVID-STYLEIMAGEDISTORTION
AparticularreasonforhallucinationsinMLLMsisthattheunderlyinglanguagemodelispre-trained
inisolation. Therefore,themodeltendstoprefermemorizationfromtrainingortextualcontextx
text
overtheassociatedimagex (Zhouetal.,2024). AspartofthePOVIDwork,Zhouetal.(2024)
img
suggeststotriggerinherenthallucinationpatternsdirectlybypresentingnoisyimagesx˜ tothe
img
model when generating the non-preferred response y˜−. Hereby, each generated token t in y˜− is
conditionedonthepriortokensfromthepreferredresponsey+ ,i.e.,π (y˜−|x˜,y+ )withmodified
<t θ t <t
inputx˜ =(x ,x˜ )(teacher-forcing). Thetildenotationemphasizesthattheresponseisdriven
text img
bythemodelwithrestrictedaccesstotheimage. x˜ iscreatedthroughadiffusionprocessthat
img
incrementallyaddsGaussiannoisetotheimagex forapredefinednumberofstepsN,whichis
img
setto500bydefault(seeSectionE.1forimplementationdetails).
Theonlineresponsey˜−iscombinedwithexistingpreferencepairs(y+,y−)byaveragingpairwise
losses1,i.e.,
L (π )=E (cid:2) γL (y+,y−,x;π )+(1−γ)L (y+,y˜−,x;π )(cid:3) ,
Avg-DPO θ (x,y+)∼D DPO θ DPO θ (5)
y˜−∼πθ(x˜img,y+)
withγ =0.5. Whilethismethodhasdesirablecharacteristics,suchasnotrequiringexternalteacher
modelsorhumanannotationtoconstructpreferencepairs,aswellasgeneratingsamplesthatareat
leastpartiallyinformedbythepolicyunderalignment,itcarriessomenotabledrawbacks.
Zhou et al. (2024) shows that selecting too few diffusion steps can yield insufficient corruption,
whereastoomanydiffusionstepsnegativelyimpactsthegeneratedresponses,asthemodelmainly
identifiesnoiserespectivetopixels. Inourexperiments,wefurtherfoundthattheamountofnoiseto
beaddedforeachimagemightbedifficulttocontroluniversallywithasingleparameter.
Additionally, Zhou et al. (2024) suggests that the proposed teacher-forcing strategy can help to
yieldsamplesthatexhibitcorruptiononlyinfew,keytokensmostinformedbythevisualcontent,
thus focusing the feedback signal for alignment. However, since the method operates token by
token,wefoundthatthiscanintroducenon-sensicalresponses,e.g.,onlycorruptingsomepartsof
multi-tokennounphrases. Suchconstructionswouldpresumablyalreadyachievealowgeneration
probability,limitingthelearningsignalfortheDPO-basedalignment. Anexampleshowingnon-
sensicalresponsesisprovidedinSection3.4.4withFigure3.
3.4 BIAS-DRIVENHALLUCINATIONSAMPLING(BDHS)
InspiredbySection3.3,weaimtoaddressitsmainidentifiedshortcomings. First,weproposeto
rethinkthemethodofcorruptingthesignalfromtheinputimagefromapixel-basedapproachtoone
thatlimitsaccessinthelatentspaceviaattentionmasking,whichwearguemoredirectlyachieves
theunderlyingmotivationoftriggeringtheinherentbiasoftheunderlyinglanguagemodel.
Second,weintroduceanewreference-guidedgenerationstrategythatallowscorruptedresponsesto
remainlargelytruetothechosenresponsewhilestillintroducingmeaningfuldivergence,without
introducingnon-sensicalcontinuationsintroducedbytoken-basedteacherforcing.
Third,weuseanoff-the-shelfsentenceembeddingtoverifythatthegeneratedrejectedresponseis
meaningfullydistinctfromtheoriginalreferencetofocustheresultingfeedbacksignalonhallucina-
tionsovermerestylisticdifference.
WerefertoournoveltechniqueasBias-DrivenHallucinationSampling(BDHS).BDHSisannotation
freeandcomputationallyefficienttothepointthatrejectedresponsescanbegeneratedonline,which
weexploreinSection5.4. AnoverviewofthemethodcanbefoundinFigure2,withfurtherdetails
providedinthefollowingsubsections.
3.4.1 ELICITINGLLMBIASVIAATTENTIONMASKING
As widely discussed, hallucinations in MLLMs often express the underlying language models’
inherent biases, for example towards frequently cooccuring objects or object attributes (Li et al.,
1ThelosspresentedherereferstotheversioninthepublicsourcecodeofPOVID(Zhouetal.,2024)which
averagestheindividualDPOlosses,opposedtothepaper(Zhouetal.,2024)thatcombinesthenon-preferred
responsesbyaweightedsumoftheirlog-probabilities.
6Spliteachsentenceatarandomposition.
Completesentencesviamodelinference.
Describethisimageindetail. πθ(·|x˜,"","Theimagefeaturesa
woodendiningtableonwhich")→a
pileoforangeshasbeenneatlyarranged.
Theimagefeaturesawoodendining
Promptxtext π diθ ni( n· g|x˜ ta, b" leTh oe ni wm ha ig ce hf aea bt ou wre ls fia llw edoo wd ie tn
h
t ba eb el ne no en aw tlyhi ac rh raa ngp eil de .o Tf ho era bn og we ls ah pa -s
orangesisplaced.","Thebowlappears
tobeinthecenterofthetable")→and pearstobeinthecenterofthetable
theorangesarearrangedinacircular andtheorangesarearrangedinacircu-
patternaroundit. larpatternaroundit. Inaddition,there
isasinglebananaonthetable.
πθ(·|x˜,"Theimagefeaturesawooden
diningtableonwhichabowlfilledwith
orangesisplaced.Thebowlappears BDHSresponsey˜−
tobeinthecenterofthetable.","In
addition,")→thereisasinglebanana
onthetable.
Imageximg GuidedGeneration
Theimagefeaturesawoodendiningta-
bleonwhichabowlfilledwithorangesis SimilarityScoringvia
Sampledimageattentionmaskm placed. Thebowlappearstobeinthecen- SequenceEmbeddings
terofthetable. Inaddition,moreoranges
arescatteredaroundthetable.
Inputx˜ SFTresponsey+
Ifsimilarity>ϵs
Samplenewmask
Figure2: OverviewoftheBDHSmethodincludingtheoptionaliterativevariantingray. Foreach
re-generation,boththeimageattentionmaskandthesentencesplitpositionsareresampled. The
imageistakenfromtheLLaVAInstructdataset.
2023b;Qianetal.,2024;Zhouetal.,2024). Inotherwords,theMLLMmaychoosetodrawfromits
parametricknowledgewheninsteaditshouldhavemorestronglyconsideredinformationfromthe
imageinquestion.
We propose to directly induce this failure mode by simply masking attention to image tokens to
inducehallucination. Letx˜=(x ,x˜ ,m)denotethemodifiedinputwith(optionalnoisy)image
text img
x˜ andattentionmaskm. SupposetheMLLMencodesimagex˜ tokembeddingvectors,each
img img
vector with dimension d. Then, m is defined as a boolean mask of dimension k. We suggest to
randomlysamplethemaskm=(m ,m ,...,m )accordingtoauniformdistributionU(0,1)and
1 2 k
thresholdρ ∈[0,1]whereeachelementfollows
th
(cid:26)
1 ifρ ≥ρ forρ ∼U(0,1);
m = i th i (6)
i 0 else.
By masking the image embeddings using m, the model only pays attention to a subset of the k
embeddingvectorstogeneratetheresponsey˜−. Wheretheremainingsignalisnotsufficient,the
MLLM can only draw on its parametric knowledge to answer, thus inducing hallucination. By
allowingaccesstosomepartoftheimage,weencouragemorerealistichallucinations.
InourexperimentswithLLaVA1.6,wefoundthatρ valuescloseto1empiricallygavethestrongest
th
results2. Wearguethatthisislikelyaresultofthe“AnyRes”techniqueusedinLLaVA1.6,which
leadstosignificantredundancyacrossimagetokens.
3.4.2 REFERENCE-GUIDEDGENERATION
Keepingthegeneratedcorruptedresponsey˜−closetothepreferredone,y+,supportstheoptimizer
inpayingmoreattentiontotheimageasonlythenon-overlappingportionisaffectedbythemodified
inputx˜. Otherwise,responsesy˜−andy+coulddivergeearlyonory˜−couldevenrepresentageneric
responsehintingonmissingimageinformation. Instead,inordertomaintainconsistencyinstyle
andstructureweproposethefollowingreference-guidedsamplingstrategy,wherewe“diverge”and
“rejoin”fromy+atrandompointstoformy˜−.
Weassumethatthepreferredresponsey+canbesplitintok =1,2,...,Ssentenceswithy+denoting
k
thek-thsentenceofy+. Eachsentenceisdecomposedintotwopartsy+ andy+ ,respectively,at
k,1 k,2
2Finalresultsarereportedatρ =0.99.
th
7SFTgroundtruth: Themainsubjectoftheimageisaparkingmeterlocatedonthesideofa
street.
BDHSattn,ρ th=0(ours): Themainsubject BDHSnoise,N=300(ours): Themainsub-
oftheimageisaparkingmeter,whichisa jectoftheimageisablurryphotographof
deviceusedtocollectpaymentforparkingin whatappearstobearoomwithadoorway.
designatedspaces.
BDHSnoise,N=500(ours): Themainsub-
BDHSattn,ρ th=0.98(ours): Themainsub- jectoftheimageisaclose-upofapattern
jectoftheimageisamailbox. thatappearstobeadigitalorpixelatedtex-
ture.
B jeD ctH oS fa tt htn e, iρ mth a= ge0 i. s9 a9 fi( ro eu hr ys) d: raT nh t.emainsub- POVID-styleimagedistortion,N=300:
Theimagesubjectoftheimageappearsa
BDHSattn,ρ th=1(ours): Themainsubject blallameter. onthesideofastreet. The
Prompt: Whatisthemain oftheimageisaperson’sface.
POVID-styleimagedistortion,N=500:
subjectoftheimage?
Theimagesubjectoftheimageappearsa
closeallalot. onthesideofaroad. The
Figure3: Exampleofgeneratedresponsesfordifferenthyperparametersandapproaches. Theimage,
promptandSFTgroundtrutharetakenfromLLaVAInstruct. Forguidedgeneration,actualmodel
completionsareshowninboldface.
arandomlysampledposition,i.e.,y+ = (y+ ,y+ ). Themodelπ istheninvokedtogeneratea
k k,1 k,2 θ
correspondingcorruptedsentencey˜− =(y+ ,y˜− )whereas
k k,1 k,2
y˜− ∼π (·|x˜,y+ ,y+ ). (7)
k,2 θ <k k,1
Notethatthisisanabuseofnotationforbetterreadability,asy˜− denotesthefullresponsesampled
k,2
frommultiplemodelinvocationsuntilthefirstfullstoporendofsequencetoken. Everysentence
isbasedonthefullgroundtruthfromtheprevioussentencey+ andnotthepreviouslygenerated
<k
outputy˜− toimproveconsistency. Finally,thefullBDHSresponseisgivenbyconcatenationofthe
<k
individualsentences,i.e. y˜− =(y˜−,y˜−,...,y˜−). Note,thepartitioningintosentencesisadesign
1 2 S
decisiontokeepthenon-overlappingportionbetweeny+andy˜−reasonablysmallandtoimprove
consistencywhenswitchingforthandbackbetweenresponses. Intheimplementation,thegeneration
ofresponsesforseveralsentencesandpreferencepairscanbehighlyparallelizedas(7)doesnot
dependonanypreviouslygeneratedoutputforallk.
Forquestionansweringtasks,severalgroundtruthresponsesy+ consistofonlyoneorfewwords
andoftenstartwithyesorno. Inthesecasesy˜− canofteneasilyinferredfromy+ evenwithout
k,2 k,1
imageaccessatallandthereforeweextendthepreviousstrategybyasimpleheuristic: whenever
y+ startswithayesornoitissubstitutedbyitscounterpartwithaprobabilityof50%.
k,1
3.4.3 ENSURINGSEMANTICALLYMEANINGFULDIFFERENCES
SimilartoourobservationinOnline-DAP,BDHSresponsesy˜−canstillbeverysimilartotheground
truthy+,especiallywhenthepivotpositioniny+ islateinthesentence,i.e. thelengthofy+ is
k k,2
small. TomaximizelearningutilityofBDHSpreferencepairs,thisisundesirable.
Whilefurtherincreasingρ orbiasingtowardsearlypivotpositionsinthereference-guidedgeneration
th
couldminimizesuchtrivialgenerations,thisintroducesadditionalhyperparametersandcanleadto
lessrealisticdispreferredresponses. Instead,werealizeBDHSinaniterativefashion.
Oncey˜−isgenerated,asemanticsimilarityscorew.r.t.y+iscomputedusinganoff-the-shelfsentence
embeddingmodel3. Anewresponsey˜− issampledifthecosinesimilarityisaboveapre-defined
thresholdϵ . AfterreachingthemaximumnumberofiterationsN ,y˜−isgeneratedaccording
s BDHS
toinputx˜ withoutanyreferenceguidance. AppendixEprovidestheactualalgorithmforBDHS
includingsimilarityscoring.
This additional semantic comparison avoids y˜− responses that are trivial rephrasings. Moreover,
measuringthenumberofexamplesthatneedre-generationallowsintuitivetuningoftheρ hyper
th
parameter.
3Weusetheall-mpnet-base-v2sentenceembeddingmodel(Reimers&Gurevych,2019)withϵ =0.97.
s
83.4.4 EXAMPLE
Figure3presentsgeneratedresponsesforaselectedexampledefinedbyimage,promptandSFT
groundtruthfromtheLLaVAInstructdataset. Thisexampleshouldparticularlydemonstratethe
differencebetweenattentionmaskingandnoisyimages. BDHSwithattentionmasking(N =5)
BDHS
isreferredtoasasBDHS andBDHSwithnoisyimagesintheinputasBDHS . Forρ = 0
attn noise th
attentionmaskingisdisabledbutstillguidedalongthegroundtruthresponse. Themodelisableto
properlyidentifytheparkingmeterintheimage. Withincreasedattentionmaskingthemodelstartsto
hallucinateasdesired. Evenwithfullymaskedimageembeddingsthemodelstillhallucinates,while
forBDHS thegeneratedresponsestendtorefertotheblurrinessoftheimages. Theexample
noise
includesresponsesfortheteacher-forcedPOVID-styleimagedistortionasdescribedinSection3.3.
Duetothetoken-based,teacher-forcedpredictions,thegeneratedresponsesoftenarenon-sensical
andinconsistentwhichworsensforhighernoiselevels.
Discussion Concurrenttous(Yuetal.,2024)alsoemphasizesthesignificanceofgeneratingmodel
sampleswithminimaldifferences. Whiletheirinsightsonannotationstrategyareinteresting,their
proposed"DeconfoundedCandidateResponseGeneration"approachappearssimilartocommon
samplingtechniquesusinghighertemperaturesinonlinepipelines,whichdonotnecessarilycreate
pairsofminimaldifferences. Inanotherconcurrentwork,Dengetal.(2024)proposesgenerating
"rejectedresponses"throughimagecorruption. Despitetheconceptualresemblance,wefindthat
bothusinganattentionmaskandSFT-guidedcorruptionarecrucialinourfinalBDHSdesign(see
Section5.4).
4 EXPERIMENTAL SETUP
4.1 MODEL
WeconductourablationsonLLaVA1.6asthisseriesofmodelsisbothwellstudiedandexhibits
strong performance across a range of multimodal tasks (Liu et al., 2024). Particularly, we focus
on aligning the LLaVA 1.6-7B Vicuna model variant as this scale of parameters is particularly
widelyusedinthecommunity. Notably,LLaVA1.6-7Bprovidesasignificantlystrongerbaseline
performanceoverthemorecommonchoiceofLLaVA1.5-7Binthemultimodalalignmentliterature.
4.2 EVALUATION
Benchmarks WeadoptmultiplebenchmarkstoassessthecapabilitiesofMLLMs,centeredaround
both measuring the models visual faithfulness, i.e. its tendency to hallucinate, as well as overall
helpfulness,i.e.theoverallqualityofitsresponses. Resultshavebeenobtainedusinganinternalfork
oflm-eval-harness(Gaoetal.,2023b;McKinzieetal.,2024;Lietal.,2024).
LLaVABench-in-the-Wild(Liuetal.,2023b),TextVQA(Singhetal.,2019),andGQA(Hud-
son & Manning, 2019) help measure the model helpfulness, i.e. the effectiveness at following
instructionsandthecompletenessoftheresponses. LLaVABench-in-the-Wildexpectsfree-
form answers while both TextVQA and GQA require concise responses. We additionally report
MMVet(Yuetal.,2023c),whichevaluatestheknowledgeandvisualreasoningcapabilitiesofthe
MLLM.SuchcapabilitiesarenotadirecttargetformostMLLMalignmentstrategiestoimprove.
Nevertheless,MMVetoffersausefulindicatorforensuringthatsuchcapabilitiesarenotlostduetoa
possiblytoosimpleornotsufficientlydiversealignmentregiment.
POPE(Lietal.,2023b)andMMHALBench(Sunetal.,2023)evaluatethevisualfaithfulnessofa
model by identifying hallucinations in model responses. For POPE, we noticed that most of our
experimentswouldreachaseemingplateaubetween86%and88%despiteimprovementsinthe
other benchmarks. We conducted an initial manual review of 100 reported losses and observed
incorrectordisputablegroundtruthonasmanyas20%ofthosesamples(seeAppendixB.1). While
re-annotatingthoseexamplesisbeyondthescopeofthiswork,weinvitethecommunitytoconsider
itasmanyrecentSOTAmodelsexhibitsuchplateau4.
Additionally, we noticed unexpected results on MMHALBench, and subsequent analysis showed
limitationsinitsscoring. Specifically,MMHALBenchusestext-onlyGPT-4todetecthallucinations
4SeeTable4inMcKinzieetal.(2024)whereallthemodelsreportedaredemonstratingaplateauonPOPE.
9Model Alignment Dataset POPE↑ MMHAL↑ MMHALv↑ LLaVAW↑ VQAT↑ GQA↑ MMVet↑ Recallcoco↑
LLaVA1.6-7B – – 86.40 2.95 2.75 80.85 64.85 64.23 43.94 68.13
LLaVA1.6-13B – – 86.23 3.23 3.18 86.10 65.7 64.8 48.26 68.13
LLaVA1.6-34B – – 87.73 3.50 3.46 88.35 69.5 67.1 53.90 71.17
OmniLMM-12B† – – – 3.14 – 74.3 – – – –
LLaVA1.6-7B† DPO STIC – – – 79.2 65.2 – 45.0 –
LLaVA1.5-7B† RLAIF-V RLAIF-V – 3.06 – 64.9 – – – –
OmniLMM-12B† RLAIF-V RLAIF-V – 3.36 – 74.3 – – – –
LLaVA1.6-7B DPO POVID(Full) 88.09 3.16 3.07 78.63 64.56 64.12 40.60 73.48
LLaVA1.6-7B Online-DPO POVID(Full) 86.49 2.88 2.94 82.61 64.88 64.31 43.26 68.45
LLaVA1.6-7B Mixed-DPO POVID(Full) 88.03 2.83 3.10 82.75 64.93 64.47 42.80 74.53
LLaVA1.6-7B DPO POVID(Full) 88.09 3.16 3.07 78.63 64.56 64.12 40.60 73.48
LLaVA1.6-7B DPO BDHS(POVID,5k) 88.75 2.61 2.71 86.33 65.07 63.97 43.4 75.58
LLaVA1.6-7B DPO Online-BDHS(POVID,5k) 88.83 2.80 2.99 85.03 65.09 63.65 43.12 74.09
LLaVA1.6-7B DPO *∪POVID(5k) 88.38 2.82 2.81 84.01 65.42 64.30 45.46 74.00
LLaVA1.6-7B DPO VLFeedback(Full) 81.84 2.96 2.99 90.75 62.93 62.53 43.85 66.67
LLaVA1.6-7B DPO VLFeedbackCorrupted(5k) 87.52 3.03 3.01 88.64 65.30 64.19 42.16 70.13
LLaVA1.6-7B DPO BDHS(VLFeedback,5k) 88.10 2.77 2.87 86.68 65.27 64.33 43.39 72.43
Table2:Mainresults.Thebestandsecondbestresultsareshowninboldandunderlined,respectively.
Ifalargermodeloutperformsallaligned7Bmodels,itisindicatedbyboldandunderline. †denotes
resultsreportedfromreferencedpapers,andadash(–)marksbenchmarksthatarenotreported. Rows
inbluearecontributionsofthispaper.
bycomparingmodelresponsestoareferenceresponseandashortlistofobjectsknowntobein
theimage. Sometimesthisleadstoentirelycorrectmodelresponsestobemarkedashallucinations
whentheyincludedmoredetailthantheprovidedreferences. Tomitigatethisissue,weintroducea
straightforwardderivativewecallMMHALBench-V(ision),whichreliesonGPT-4o,i.e.provides
theinputimageasadditionalcontexttothejudge,tomorereliablyevaluatemodelcapabilities. Data
and evaluation prompts are unchanged. We empirically found this to be more reflective of true
hallucinationsinahumancomparison. SeeAppendixB.3forourreview. Throughoutexperiments,
wemainlyfocusonMMHALBench-VnumbersandreportMMHALBenchprimarilyforreference.
Whileresponsesthathavefewerhallucinationsareoftenalsoinherentlymorehelpful,weobservethat
thesedimensionsareneverthelessdistinctandoptimizingforreductioninhallucinationcruciallydoes
notnecessarilyimplyamorehelpfulmodel. Infact,insomeinstances,weevenobservedaninverse
relationship. Forexample,asdiscussedin(Zhuetal.,2023),agivenmodelwouldbemorelikely
tohallucinatewhenaskedtoproducelongercaptionsthanshorterones. Thisimpliesthatmodels
couldlearntohallucinatelesssimplybyprovidingmoreconcise,arguablylessuseful,responses,
andthatmodelsthataimtoprovidemoredetailedresponsesmayfinditmoredifficulttoremain
faithfultovisualcontextinallrespects5. Forthisreason,wereporttherecallmetricfromObject
HalBenchYuetal.(2023b),styledRecallcocoinourtables.Thismeasureshowmanyobjectsknown
tobeinanimagebasedonCoCoannotationsarementionedinacomprehensivecaptiongivenbythe
model. WeconsideredaswellreportingtheCHAIR(Rohrbachetal.,2018)metricsfromObject
HalBench(Yuetal.,2023b). However,duringourexperiments,wefoundthatthosemeasurements
werenotalwayscorrelatedwiththequalityofthemodelsevaluated(seeAppendixB.2).
5 EXPERIMENTS
Inthissection,weempiricallyevaluatedifferentaspectsofaligningMLLMs.Westartbysummarizing
ourkeyfindingsinSection5.1. Then,weproceedwithanin-depthablationstudyonthecomponents
we have discussed in the paper, offering a clearer view of effect. We begin with equalizing the
experimentalconditionsonpublicpreferencedatasets(Section5.2). Wethenhighlightdesideratafor
ahigh-qualitypreferencedataset(Section5.3)andshowthatBDHScanbeasimpleandeffective
mechanismfollowingsuchbestpractices(Section5.4). Subsequently,wecomparevariousalignment
techniques,suchasRL-basedmethods(Section5.5),OnlineandMixed-DPOstrategies(Section5.6),
aswellasvariousofflineapproaches(Section5.7).
5.1 KEYCOMPONENTSINMLLMALIGNMENTPIPELINE
WesummarizeourmainfindingsandcompareresultswithotherSOTAmodelsinTable2. First,we
fixedthebasemodel(LLaVA1.6-7B)andstudiedtheeffectsofonlinevs. offlinemethodsusingthe
POVIDalignmentdata(Zhouetal.,2024). WhileofflineDPOshowsmoresignificantimprovement
5Tosomeextent,onecouldarguethismirrorsthetensionbetweenhelpfulnessandsafetyasreportedin
Touvronetal.(2023),whereahighlysafemodelmaybelesshelpful.
10onbenchmarksthatconsiderhallucination,suchasPOPEandMMHALBench-V,theOnline-DPO
enhancesbenchmarksevaluatingthequalityofanswersinanopenquestionansweringsetup,like
LLaVABench-in-the-Wild. Thisisintuitive,asthepreferencepairsinthePOVIDdatasetare
specificallydesignedtoreducehallucinations. Incontrast,theonlinesamplesfromthemodelmay
notalwaysprovideasstrongasignalforreducinghallucinations. Mixed-DPOallowstoincorporate
thebenefitsofbothapproachesandtheresultsshowconsistentimprovementoverbothonlineand
offlinemethods.
WhenusingOnline-DPOorMixed-DPOstrategies,wetypicallydependonadvancedmodelslike
LLaVA1.6-34Btoranktheonlinesamplesgeneratedbythemodel. However,accesstosuchmodels
isnotalwaysguaranteed. WediscussthislimitationinmoredetailinSection5.6.1. Additionally,
the construction of the POVID dataset also involves using a superior model such as GPT-4V to
injectnoiseintoSFTdata. OurproposedBDHSmethoddoesnotrequireadditionalannotatorsor
preferencedata,andreliesexclusivelyonSFTdataalreadyavailablefromtheinstructiontuningof
thebasemodel. Despitethissimplicity,itconsistentlyoutperformsthemodelsthatutilizethelarger
POVIDdataset(i.e. bothofflineandMixed-DPO)inmostbenchmarks. ImplementingBDHSin
anonlineformatfurtherclosesthisperformancegapinMMHALBench-V,establishingBDHSasa
compellingandcost-effectivealternativetoothermoreresource-intensiveapproaches. Combining
thePOVIDdatasetwiththeonline-BDHSapproach(referredtoasOnline-BDHS∪POVID),with
theexceptionofMMHALBench-V,consistentlyoutperformsthemodelthatusesonlythePOVID
datasetacrossallbenchmarks. ItalsosurpassesSTIC(Dengetal.,2024)andRLAIF-V(Yuetal.,
2024)onthereportedbenchmarks. Wefurtherdiscusstheenhancedefficacyofourapproachover
Zhouetal.(2024)inSection5.4.
While Section 5.3 provides a detailed analysis of various preference datasets, we highlight key
findingsfromtheVLFeedbackdatasethere,astheycontributesignificantlytobuildinganeffective
alignmentstrategy. UnlikePOVID,bothVLFeedbackanditsvariant,VLFeedbackCorrupted(5k),
select the “chosen response” in the preference pairs from the top responses ranked by GPT-4V,
selectedfromapoolofmodel-generatedresponses. Comparedtore-usingSFTdata,thisapproach
potentiallyoffersanadditionalsupervisorysignaltothemodel,leadingtoenhancedperformanceon
benchmarkslikeLLaVABench-in-the-Wild,wheresuchalignedmodelsevenoutperformthe
unaligned13Band34Bmodelsfromthesamefamily.
Notably,weintroduceVLFeedbackCorrupted(5k),asmalldatasetleveragingcorruptioninjection
togeneratethe“rejectedresponse”,whichperformscompetitivelytothemuchlargerrank-based
VLFeedback(full)dataset. Theseexperimentsdemonstratetheeffectivenessoftwostrategiesin
constructingpreferencedata: First,learningfromstrong(highly-ranked)responsesseemstoyielda
distillation-likebenefit. Second,usingsubtledifferencesbetween“chosen”and“rejected”responses,
asopposedtojustrank-basedpairs(likeinVLFeedback(full)),cansignificantlyreducehallucinations,
eveninalimiteddataregiment.
Finally,wereplacethenoiseinjectionstrategyusingGPT-4withourproposedBDHS.Weobservea
slightreductionoftheMMHALBench-VandLLaVABench-in-the-Wildscorescomparedto
theGPT-4Vbasedapproach,butnotethattheachievedresultstillrepresentsmeaningfulimprovements
overthebaseline.Onallothermetrics,BDHSshowscomparableorevensuperiorresults,establishing
BDHSasastrongalternativetoGPT-4Vinthispipeline.
Intheremainderofthissection,weconductacomprehensiveablationstudyoneachofthecompo-
nentsdiscussedearlier,aimingtoofferinsightsintothetypicaltrade-offsencounteredinalignment
strategies.
5.2 REMOVINGCONFOUNDINGFACTORSFORPREVIOUSLYPUBLISHEDDATASETS
Weanalyze RLHF-V (Yu etal., 2023b), VLFeedback (Li et al.,2023a) and POVID(Zhou et al.,
2024) as they offer a fair blend between human and synthetic sources, and between constructed
andrankedpreferencesignalcomposition. Asitischallengingtodeterminewhataretheproperties
thatcharacterizeahigh-qualitypreferencedataset,wefirstreplicatealignmentusingthepublished
datasetsagainstLLaVA1.6-7BwithDPO.Additionally,wesub-samplealldatasetstoaconsistent
sizeof5,000examplestomitigateeffectsizes. ResultsaresummarizedinTable3. Whenavailable,
weadditionallyreporttheresultspublishedbytheoriginalauthors,highlightedingrayinthetable.
11Model Dataset POPE↑ MMHAL↑ MMHALV↑ LLaVAW↑ VQAT↑ GQA↑ MMVet↑ Recallcoco↑
LLaVA1.6-7B – 86.40 2.95 2.75 80.85 64.85 64.23 43.94 68.13
Publicdatasets
LLaVA1.6-7B VLFeedback(80k) 81.84 2.96 2.99 90.55 62.93 62.54 43.85 66.67
LLaVA1.6-7B POVID(17k) 88.09 3.16 3.07 78.05 64.56 64.12 40.60 73.48
LLaVA1.6-7B RLHF-V(5.7k) 83.86 3.15 3.26 70.58 64.75 62.89 37.16 64.26
Publicdatasets,randomlysubsampledto5,000samples
LLaVA1.6-7B VLFeedback(5k) 86.31 2.92 3.00 83.10 65.06 64.09 43.21 68.03
LLaVA1.6-7B POVID(5k) 88.18 2.93 2.93 81.89 64.90 64.34 43.39 71.80
LLaVA1.6-7B RLHF-V(5k) 84.39 3.25 3.35 72.09 64.85 63.35 39.72 64.68
Previouslypublished
Qwen-VL-Chat VLFeedback(Lietal.,2023a) – 3.02 – – – – 49.9 –
Muffin RLHF-V(Yuetal.,2023b) – (52.1↓)† – – – – – –
LLaVA1.5 POVID(Zhouetal.,2024) 86.90 2.69 – 68.7 – – 31.8 –
Table3: ResultsforLLaVA1.6-7BVicuna(Liuetal.,2024)alignedwithDPOonVLFeedback,
POVID,RLHF-V.Resultshighlightedingrayaretheresultsreportedbytheoriginalauthors.†denotes
MMHALBenchforwhichYuetal.(2023b)strictlyreportedthehuman-correctedhallucinationrate.
Zhouetal.(2024)haveconductedasimilarexperimentusingLLaVA1.5,howevertheydonotcontrol
fordatasetsize. Weweresuccessfulinreplicatingcertainobservationspublishedbytheseauthors.
POVIDreachesthehighestscoreonPOPE.Zhouetal.(2024)alsoreportsthehighestMMHALBench
scoreswithPOVID,whichwewereabletoreproduceusingLLaVA1.6,althoughthisisonlytrue
whensizecorrectionisnotapplied. Uponnormalizingforsize,POVID’sperformanceequaledthat
ofVLFeedbackandwaslowerthanRLHF-V.
Inotherdomains,ourexperimenthaveshowndivergenttrends.WhileZhouetal.(2024)demonstrated
thatallpreferencedatasetsimprovedLLaVA1.5onMMVet,ourfindingswithLLaVA1.6exhibiteda
reversetrend: allourrunsdidnotmatchuptothebaseline. Interestingly,asthedatasetsgrewlarger,
wewitnessedafurtherdeviationfromthebaseline. Wehypothesizethatthesepreferencedatasets
lackthenecessaryinformationtoimproveMMVetoverthenotablystrongerbaselineLLaVA1.6
introduced,whichnecessitatesspecializedknowledge(seeSection4.2). VLFeedback,toacertain
extent,maypossesssomeofthisknowledgethankstoitsdiverseprompts.However,theotherdatasets
appear to fall short. By restricting dataset sizes, we further limit the potential alterations on the
non-alignedmodel,astheresultsstayclosertothatbaseline.
Oppositely, VLFeedback on LLaVABench-in-the-Wild shows an uplift bump that is only
limitedwhenthesizerestrictionlimitisapplied. WhenaligningonthecompleteVLFeedback,the
largestdatasetintheseexperiments,wecanachievethehighestscoreonthatbenchmark.
5.3 DESIDERATAFORPREFERENCEDATASETS
We examine the components of a preference dataset for multimodal alignment, as introduced in
Section3,andinvestigatethefollowingoptionsinconstructingthispreferencedata. Theexplored
choicesarefurthersummarizedinTable4.
• Prompts Wecompared(i)adiversepromptstrategymixingmultipledatasetsto(ii)prompts
onlyfromLLaVA-Instruct-150k,whichwasalreadyseenduringtheSFTstageofthebasemodel.
• Chosenresponses Weintroduced3settings: (i)diverseresponsesfrommultipleMLLMs;(ii)
LLaVAresponsesonly,(iii)GPT-4Vresponsesonly.
• Rejectedresponses Weintroduced2settings: (i)diverseresponsesfrommultipleMLLMs,and
(ii)corruptionofthechosenresponses.
Inordertoconstructthesepreferencedatasetablationscheaplyandreproducibly,weleveragethesize
anddiversityoftheVLFeedbackdataset(Lietal.,2023a). VLFeedbackpossessesseveralproperties
thatmakesitagoodsandbox: (a)theprompts,derivedfrom9datasets(LLaVA-Instruct-150k,SVIT,
LLaVAR, etc.), are diverse, (b) the chosen and rejected responses are sampled from 12 MLLMs
makingthemverydiversetoo–∼37%responsesarefromGPT-4V,and∼35%fromtheLLaVA1.5
series,(c)finally,thelargesizeofVLFeedback,80,000quadrupletsofresponsesthatcanbepaired
together,makesitsimplertoisolatespecificaspects.
Corruption strategy Reranking is originally used to determine chosen and rejected responses
in VLFeedback (see Section 3). In order to remove variation introduced by the original rejected
12Datasets Prompts ChosenResponses RejectedResponses
diverse LLaVA-SFT diverse LLaVA GPT-4V diverse chosencorruptedbyGPT-4
VLFeedback ✓ ✓ ✓
+corruptingstrategy ✓ ✓ ✓
prompts
LLaVAprompts ✓ ✓ ✓
modelresponses
GPT-4Vresponsesonly ✓ ✓ ✓
LLaVAresponsesonly ✓ ✓ ✓
Table 4: Controlled settings for multimodal preference dataset exploration. We decompose the
preferencedatasetsintoprompts,chosenandrejectedresponsesandwethenaimatidentifyingfactors
contributingtothedatasetquality.
responses(e.g.,stylechangebetweenMLLMs)andpermitatightercontrolonablations,wereplace
rejectedresponsesfromtheoriginalVLFeedbacksampleswithcorruptedversionsofthepreferred
responses. Similartothemethodin(Zhouetal.,2024),weleverageGPT-4tospecificallyintroduce
realistichallucinations,assistedbyafewshotsforillustration(seeAppendixC.1).
Dataset POPE↑ MMHAL↑ MMHALv↑ LLaVAW↑ VQAT↑ GQA↑ MMVet↑ Recallcoco↑
Baseline 86.40 2.95 2.75 80.85 64.85 64.23 43.94 68.13
VLFeedback(5k) 86.31 2.92 3.00 83.10 65.06 64.09 43.21 68.03
+corruptingstrategy 85.59 3.39 3.33 86.65 65.20 63.87 37.98 68.66
prompts
LLaVAprompts 87.63 2.85 2.96 86.55 65.13 64.25 41.47 70.44
modelresponses
GPT-4Vresponsesonly 86.78 3.30 3.02 86.77 65.06 64.02 40.14 69.08
LLaVAresponsesonly 87.52 3.03 3.01 88.64 65.30 64.19 42.16 70.13
Table5: Datasetablations. WestartedfromthepublicVLFeedbackdatasetwithitsdiverseprompts
andresponses,andwethenappliedtargetedsamplingandcorruptiontoisolatethefactorscontributing
tothequalityofapreferencedataset.
Results FollowingSection5.2,weapplyDPOalignmentontheLLaVA1.6-7Bmodel,andwelimit
allthedatasetsto5,000samples. InTable5,wereporttheresultsofthisexperiment. First,weshow
thatourcorruptionstrategyachievesimprovementsoverthebaselinecomparableinmagnitudeto
the ranking-based preference signal in the original VLFeedback data. In some benchmarks, like
MMHAL-Bench-V,weevenobserveimprovements,whilenotablyMMVetshowssomeregressions.
Nevertheless,wearguethatthisrepresentsareasonablebaselinetoadoptforeasieriterationonthe
followingablations. InAppendixC.2,weprovidemoreanalysisonthisstrategy.
Next, we explore the impact of novelty of the prompts used for alignment, by sampling another
5kpreferencedatageneratedwiththesamecorruptionmechanismsolelyfrompromptsthatarea
partoftheLLaVASFTmixture. Theseareexamplesthatthebasemodelwouldhavealreadybeen
trainedonduringtheSFTstage. Interestingly,itappearsthatusingnovelpromptsdoesnotoffer
substantialbenefits. WestillobservecomparableliftonLLaVABench-in-the-Wild,andwhile
MMHAL-Bench-Vshowslessdramaticimprovementoverthebaselinecomparedtothemorediverse
corruption-basedsample,thismaybeduetomoreverboseresponses,asindicatedbyhigherrecall.
POPEevenimprovessomewhatsignificantlyandtheregressioninMMVetisalsolesspronounced.
Finally,weexploretheimpactoftheconstructionoftheacceptedresponseinthealignmentdata. One
couldarguethatforresponsesderivedfromstrongermodelsuchasGPT-4V,improvementsmayalso
betheresultoflearningfromthisstrongerteachermodel. Therefore,weconducttwoexperiments:
one,wherewesampledatawherethepreferredresponsecomesfromGPT-4Vonly,andonewhere
thepreferredresponsecomesfromLLaVA1.5-7B,amodelgenerallyweakerthanthebasemodel
underalignmentinthisexperiment. Interestingly,wedonotobserveanybenefitfromlearningfrom
GPT-4Vgeneratedresponses,infact,ourresultssuggestthatpositivesamplesderivedfromLLaVA
1.5-7Bledtoaslightlystrongermodelpostalignment.
13These findings suggests that useful preference data can be derived cheaply, even from responses
fromrelativelyweakermodels,aslongasonecaneffectivelysampleandidentifyrelativelydesirable
answers fromthe modelas their preferredresponse6, and introduce targetedcorruption to create
dispreferredresponses.
InthefollowingSection5.4,wewilldiscusshowonecanavoidboththeneedforsamplingpreferred
modelresponsesaswellastheneedforanexternalmodeltointroducecorruptionwithBDHS.
5.4 ABLATIONSONBDHS
Section3.4introducesBDHSasatechniquetogeneratecorruptedresponsesdirectlyusingthemodel
subject to alignment. While our proposed approach is purely based on image attention masking,
we also evaluate a variant that consumes noisy images instead, motivated by the teacher-forced
POVID-styleimagedistortionintroducedinZhouetal.(2024)(seeSection3.3). Inthefollowing,
BDHSwithattentionmasking(ρ =0.99andN =5)isdenotedasBDHS andBDHSwith
th BDHS attn
noisyimagesintheinputasBDHS . ThenumberofadditivenoisestepsforBDHS issetto
noise noise
N =500similartotheimagedistortioninZhouetal.(2024).
All ablations in Table 6 are based on our 5k subset of POVID as introduced in Section 5.2. As
describedinSection3.2,POVIDcontainsLLaVAInstructresponsesy+aswellasGPT-4Vcorrupted
non-preferred responses y−. While y+ is shared between all ablations, we start with substitut-
ingy− fromexternalsupervisionbytheBDHSmodelresponsey˜− andinvokestandardDPOas
shown in the first 3 rows after the LLaVA 1.6-7B baseline results. The proposed variants con-
sistentlyimproveoverthebaselineforPOPEandLLaVABench-in-the-Wild. Theyregress
on MMHALBench, however, as discussed in Section 4.2, this benchmark has limitations so we
mainlyfocusonMMHALBench-VinsteadforwhichallBDHS variantsperformcomparableto
attn
the baseline while the online rollout of y˜− even improves over it. Notably, we also observe sig-
nificantlyhigherRecallcoco,suggestingricherresponses. BDHS resultsinlowerscoresfor
noise
LLaVABench-in-the-WildwhiletheattentionmaskingapproachBDHS almostmaintains
attn
thebaselinescores.
y−fromexternalsupervision y˜−derivedfrompolicy POPE↑ MMHAL↑ MMHALV↑ LLaVAW↑ VQAT↑ GQA↑ MMVet↑ Recallcoco↑
– – 86.40 2.95 2.75 80.85 64.85 64.23 43.94 68.13
– BDHSnoise(Offline,ours) 88.60 2.37 2.48 84.53 65.05 64.14 41.38 75.16
– BDHSattn(Offline,ours) 88.75 2.61 2.71 86.33 65.07 63.97 43.4 75.58
– BDHSattn(Online,ours) 88.83 2.80 2.99 85.03 65.09 63.65 43.12 74.09
GPT-4V(POVID) – 88.18 2.93 2.93 81.89 64.90 64.34 43.39 71.80
GPT-4V(POVID) POVID-styleimagedistortion 88.33 2.84 2.64 80.15 64.21 63.79 41.28 69.39
GPT-4V(POVID) BDHSnoise(Offline,ours) 88.58 2.76 2.45 84.36 65.31 64.26 43.95 75.05
GPT-4V(POVID) BDHSattn(Offline,ours) 88.56 2.85 2.72 85.35 65.39 64.11 43.26 75.05
GPT-4V(POVID) BDHSattn(Online,ours) 88.38 2.82 2.81 84.01 65.42 64.30 45.46 74.00
Table6: AblationresultsforBDHSincludingbaselineandreferenceapproaches. Allresultsbased
onLLaVA1.6-7B,usingDPOandthePOVID(5k)sampleforthesourceofimagesandprompt.
Wheneverbothy−fromexternalsupervisionandy˜−derivedfrompolicy(eitheronlineoroffline)
areincorporated,theaveragelossiscomputedusing(5).
ThelowerpartitionofTable6startswithplainDPOonthePOVID(5k)datasetasreferenceand
theneachsubsequentapproachincorporatesboththeexistingresponsey−fromexternalsupervision
aswellasy˜− derivedfromthepolicy. Hereby,thetwonon-preferredresponsesareincorporated
intotheDAPframeworkbyaveragingthelossesof(y+,y−)and(y+,y˜−)accordingtoEquation(5).
Therefore,theOnline-BDHSmethodusesOnline-DPOinaconsiderablesimplifiedsettingcompared
to the full Online-DPO realization (4), as the formulation presented here does not depend on a
dedicatedexternalannotator(seeSection2.3).
AlltheBDHSablationsimprovesignificantlyonLLaVABench-in-the-Wildcomparedtothe
DPObaselineandPOVID-styleimagedistortion. TheBDHS withattentionmaskingperforms
attn
6Inthisablation,preferredresponseswereselectedfrommodelresponsesthatwererankedasthebestamong
thesampledmodelresponsesperexampleinVLFeedback.Inthissettingthechosenresponsecanbeassumed
tobeofreasonablequalityasitwaschosentobeatleastbetterthanothermodels’.Whilethisstillindirectly
exploitstherankingintheVLFeedbackdata,allthatisrequiredisawaytosamplereasonablemodelanswers,
whichisgenerallymuchmorereadilyavailableinpracticalscenariosthanpairedpreferencedata,forexample
viacheapuserfeedback(thumbsup/down).
14Held-OutEvalDataset
BaseModel TrainDataset
POVID RLHF-V VLFeedback
LLaVA1.5-7B POVID 0.99 0.24 0.56
LLaVA1.5-7B RLHF-V 0.12 0.86 0.52
LLaVA1.5-7B POVID+RLHF-V 0.98 0.76 0.53
LLaVA1.5-7B VLFeedback 0.61 0.54 0.81
LLaVA1.6-7B POVID 0.99 0.34 0.59
LLaVA1.6-7B POVID+RLHF-V 0.97 0.68 0.63
LLaVA1.6-7B VLFeedback 0.76 0.53 0.82
Table7:Rewardmodelaccuracyontheheld-outvalidationset.
significantlybetteronMMVetcomparedtoBDHS . Notably,BDHS consistentlyoutperforms
noise attn
thePOVID-styleimagedistortionacrossallbenchmarks. Wefollowthepublishedimplementationof
Zhouetal.(2024),however,surprisinglythePOVID-styleimagedistortionperformsworsecompared
toplainDPOviaPOVID(5k),whichdiffersfromtheLLaVA1.5-7Balignmentresultsintheirpaper.
Presumably, thenon-sensicalresponsesfromteacher-forcingcouldlowertheperformancewhile
tradingoffwiththeexistingGPT4-Vpreferencepairs.
WhileonlineapproacheswithBDHSimproveoncertainbenchmarks,weemphasizethateventhe
offline dataset created with BDHS and without additional response from external supervision
attn
alreadyconstitutesacost-effectivebaselinethatconsistentlyperformswellacrossallbenchmarks.
Unlessotherwisestated,BDHSinthefollowingsectionsgenerallyreferstoBDHS .
attn
5.5 RL-BASEDALIGNMENT
ToevaluateRL-basedalignmentmethodswefollowedtheestablishedrecipeoftrainingareward
modelonapreferencedatasetandthenusinganRLalgorithmtooptimizetheMLLMtomaximize
therewardofresponsessampledfromthepolicy. WechosePPOandRLOOduetotheirpopularity
intheLLMliterature.
Reward Model Training and Evaluation We analyze the utility of datasets available in the
communityforrewardmodeltrainingbytrainingonPOVID,RLHF-VandVLFeedbackpreference
datasets. Toevaluatesuchcreatedrewardmodelsinisolation,weholdoutasmallvalidationsetsplit
fromtheoriginaldatasetandreportclassificationaccuracyofthetrainedrewardmodel,i.e. itsability
todifferentiatethechosenfromtherejectedresponseinPOVID,RLHF-V,andVLFeedbacksets.
Theseheldoutvalidationsetsarenotusedforrewardmodeltraining.
Table7showstheperformanceoftherewardmodelstrainedondifferentdatasetsacrossallvalidation
sets. ThemodeltrainedonVLFeedbackshowsthebestgeneralizationacrossthedifferentdatasets,
asmaybeexpectedgivenitssignificantlylargersizeandhigherdiversity. Incontrast,rewardmodels
trainedonPOVIDandRLHF-Vshownotablypoorgeneralizationtotheirrespectivecounterpart,
whileachievinghighscoresontheirownheldoutportions. Wehypothesizethattherewardmodel
maylearntorecognizeandprefertherespective(original)policyresponsebeforecorruption(POVID)
orenhancement(RLHF-V),whichcouldexplaintheperformancebeingsignificantlybelowarandom
choicebaseline. Tostrengthenourhypothesis,wealsocombinethePOVIDandRLHF-Vsetsfor
rewardmodeltrainingandobservethatbothLLaVA1.5-7BandLLaVA1.6-7Bareabletolearna
morebalancedobjective,althoughevenforsuchacombinedtrainingsetwestillobservelimited
generalizationtoVLFeedback.
RLTrainingandEvaluation WeusedthePOVIDandVLFeedbackbasedrewardmodelsforPPO
andRLOOtraining. Table8showsthescoresofthebestmodelstrainedviaPPOandRLOO.
Mirroringtheobservedlackingeneralizationinourrewardmodelexperiments,wefoundthatusing
POVID-based reward model resulted in collapse of responses during the RL training. Only the
use of the reward model trained on the much larger VLFeedback dataset allowed for stable RL
training without model collapse. We hypothesize that besides the larger size, VLFeedback may
be more aligned with the downstream objective of the reward model due to its construction by
rankingsampledmodelresponses,comparedtoPOVID,whichaimstoproduceminimallydifferent
15Alignment DatasetRM DatasetP POPE↑ MMHAL↑ MMHALV ↑ LLaVAW↑ VQAT↑ GQA↑ MMVet↑ Recallcoco↑
Baseline – – 86.40 2.95 2.75 80.85 64.85 64.23 43.94 68.13
DPO – POVID 88.09 3.16 3.07 78.05 64.56 64.12 40.60 73.48
PPO POVID POVID
Policytrainingnotstable
RLOO POVID POVID
PPO VLFeedback POVID 87.54 3.02 3.09 80.17 63.90 64.04 40.51 67.19
RLOO VLFeedback POVID 87.17 2.94 2.72 78.72 63.59 63.72 42.25 64.57
Table8: RL-basedalignmentofLLaVA1.6-7B,DPObaselineincludedforreference. RL-based
alignmentmethodsusearewardmodelbasedonLLaVA1.6-7B,Dataset referstothedataset
RM
usedtotraintherewardmodel,Dataset tothesetofimagesandpromptsusedforRLalignment.
P
preferencepairs. Nevertheless,eventhestrongerVLFeedback-basedrewardmodeldidnotallowus
toreliablyoutperformamuchsimplerDPObaseline7.
TheseobservationsindicatethatrewardmodeltrainingwithsubsequentRLalignmentcouldperhaps
requiremorecarefullycurateddata,e.g.,withmorefocusondiversity,thandirectalignmentmethods
where both POVID and VLFeedback individually achieve strong improvements. In addition to
inherently stronger reward models, perhaps basing them on more powerful base models, it also
suggeststhattheapproachintroducedintheconcurrentworkofYuetal.(2024),whichintroducesa
symbolicrewardformulationbasedonscoresfromaVQAmodelverifyingstatementsmadebythe
policymaybeapromisingavenueforfutureresearch.
AnotherinterestingobservationisthattheRLalignedmodelsshowsimilarevaluationtrendsasthe
DAPalignedmodels,wherebothusePOVIDpromptsandimagesforthetrainingofthepolicy. For
example,comparedtothebasemodeltheyshowsomeimprovementinPOPE,andMMHalBench
(both variants), with some regressions in LLaVABench-in-the-Wild, TextVQA, GQA, and
MMVet. These trends are distinct to what is seen when using direct preference alignment on
VLFeedbackdataasshowninTable3. ThisisremarkableastheRLalignedmodelsdoofcoursenot
usethechosenandrejectedresponsespresentinthePOVIDdataset,insteadgettingtheirfeedback
signalentirelyfromtherewardmodelwhichistrainedonVLFeedbackdata. Weobserveasimilar
trend in Section 5.6, where in a purely online setting, the choice of input prompts and images
significantlyimpactsalignmentresults.
5.6 ONLINE-DPO&MIXED-DPO
WeapplyOnline-DPOandMixed-DPOtoboththePOVIDandtheRLHF-Vdataset. Theresultsare
summarizedinTable9.ConsistentwithourobservationsonthePOVIDdataset,applyingMixed-DPO
–whichcombineselementsofDPOandOnline-DPO–typicallyresultsinamoderatingeffecton
performanceoutcomes. Theresultsoftenspanarangeslightlybroaderthanthehighestandlowest
performancesachievedbyDPOandOnline-DPO.Thisvariabilityisattributedtotheprobabilistic
natureoftheonlinesamplinginOnline-DPO.
OntheRLHF-Vdataset,whereOnline-DPOconsistentlyoutperformsDPOacrossallbenchmarks,
themoderatingeffectofMixed-DPOprovesnotbeneficial,astheofflineDPOcomponentcontributes
minimallytotheoverallmodelperformance.Nevertheless,Mixed-DPOremainsavaluablestrategyin
scenarioswhere,asobservedintheexperimentsonthePOVIDdataset,offlineandOnline-DPOshow
complementaryimprovements,leveragingthestrengthsofbothtooptimizeoverallperformance.
5.6.1 HOWDOESASTRONGERANNOTATORAFFECTTHEPERFORMANCEOFALIGNEDMODEL?
AnnotatorEvaluationWeusedLLaVA1.6-34Bastheannotator.Toverifyitscapabilitytoaccurately
judgedifferentresponses,weevaluateditonthesampleheld-outpartofthreedatasetsweusedfor
evaluatingtherewardmodelinSection5.5. ResultsaresummarizedinTable10. Forfurtherdetails
onthepromptsusedandqualitativeexamplesoftheannotator’soutputs,pleaserefertoAppendixD.
Table 11 presents a comparison of the effects of Online-DPO with two different annotators.
7WealsofoundthatmodelsachievinghigherrewardduringRLtraining,didnotperformbetterthanmodels
with lower reward and less KL divergence, i.e., models with higher β parameter performed better on the
benchmarks.NoneoftheRLalgorithmsclearlyoutperformedtheothers.
16Alignment Dataset POPE↑ MMHAL↑ MMHALV ↑ LLaVAW↑ VQAT ↑ GQA↑ MMVet↑ Recallcoco↑
– – 86.41 3.06 2.71 78.96 64.22 64.22 43.94 68.13
DPO POVID 88.09 3.16 3.07 78.63 64.56 64.12 40.60 73.48
Online-DPO POVID 86.49 2.88 2.94 82.61 64.88 64.31 43.26 68.45
Mixed-DPO POVID 88.03 2.83 3.10 82.75 64.93 64.47 42.80 74.53
DPO RLHF-V 83.86 3.15 3.26 70.58 64.75 62.89 37.16 64.26
Online-DPO RLHF-V 85.40 3.10 3.27 79.66 64.94 64.05 41.01 68.13
Mixed-DPO RLHF-V 85.57 2.94 3.16 78.46 65.06 64.10 41.10 67.82
Table9: TheeffectofMixed-DPO,usingLLaVA1.6-7Basthebasemodel.
While using Online-DPO with LLaVA 1.6-
Dataset LLaVA1.6-7B LLaVA1.6-34B
7B as the judge can enhance the overall per-
VLFeedback(eval) 79.10 90.91
formanceofthemodel,thestrongerannotator
RLHF-V(eval) 81.88 93.90
seemstoprovidemoreconsistentimprovements
POVID(eval) 92.96 98.55
acrossvariousbenchmarks.
Table 10: Performance of the annotators on different
Concurrenttous,Yuetal.(2024)proposedan preferencedatasets.
annotationapproachthatsegmentstheannota-
tionprocessintoeasiersub-tasks,witheachtaskbeingindividuallyscored. Thesescoresarethen
aggregatedtoformanoverallscorethatratestheresponses. Thismethodcanpotentiallyenable
weakermodelstostillprovidestrongsupervisionsignalsduringthealignmentprocess. Moreover,
exploringtheuseofstrongerbasemodelsanddiversedatasets, bothintermsofsizeandvariety,
couldfurtherenhancetheeffectivenessoftheonlineapproach. Weleavethedetailedinvestigationof
theseaspectsforfuturework.
Model Dataset Annotator POPE↑ MMHAL↑ MMHALV ↑ LLaVAW↑ VQAT↑ GQA↑ MMVet↑ Recallcoco↑
LLaVA1.6-7B – – 86.40 2.95 2.75 80.85 64.85 64.23 43.94 68.13
LLaVA1.6-7B POVID LLaVA1.6-7B 86.54 2.52 2.72 81.55 64.93 64.18 40.73 67.40
LLaVA1.6-7B POVID LLaVA1.6-34B 86.49 2.88 2.94 82.61 64.88 64.31 43.26 68.45
Table11: ComparisonofOnline-DPOwithastrongannotator(i.e.,LLaVA1.6-34B)andaweak
annotator(i.e.,LLaVA1.6-7B).
5.7 COMPARISONOFDIFFERENTOFFLINEALIGNMENTMETHODS
WhileweconductedmostofourexperimentsusingDPOforcomparabilitywithotherworksinthe
community,wealsoranafewexperimentstoinvestigatewhetherotherpopularofflinemethodscould
improvetheresults. ResultsaresummarizedinTable12.
OurresultsindicatethatbothIPOandSLiC,similartoDPO,boostthemodel’sperformanceacross
mosthallucinationbenchmarks. Additionally,thesemethodsdemonstrateimprovementsinmore
openquestion-answeringbenchmarks. WeanticipatethatOnline-IPOandOnline-SLiCwillyield
enhancementsovertheirofflinecounterparts—similartotheimprovementsobservedwithOnline-
DPOoverDPO—asexaminedinGuoetal.(2024). However,thisstudyisbeyondthescopeof
thispaperandisleftforfuturework. Primarily,weaimtohighlighttheimportanceofconsidering
differentalignmentobjectives,emphasizingthatthechoicebetweenofflineobjectivesindifferent
setupscanimpacttheeffectofthealignmentpipeline.
Alignment Dataset POPE↑ MMHAL↑ MMHALV ↑ LLaVAW↑ VQAT ↑ GQA↑ MMVet↑ Recallcoco↑
– – 86.40 2.95 2.75 80.85 64.85 64.23 43.94 68.13
DPO POVID 88.09 3.16 3.07 78.63 64.56 64.12 40.60 73.48
IPO POVID 87.62 3.11 3.11 82.34 65.09 64.47 43.99 69.81
SliC POVID 88.28 3.17 3.15 81.99 64.59 64.11 41.51 74.32
Table12: ComparisonofdifferentofflinealignmentmethodsbasedonLLaVA1.6-7B.
176 CONCLUSION AND FUTURE WORK
Inthisstudy,weexploretheroleofpreferencealignmentinenhancingtheperformanceofMLLMs,
withaparticularfocusonreducinghallucinations. Acommonlyproposedexplanationforhallucina-
tionsinMLLMsistheirtendencytooverlookimagecontentandinsteadrelyoninherentlanguage
biases. Toaddressthis,weassessvariousalignmentstrategiesacrossdifferentdatasetsandalignment
methods. We categorize alignment algorithms into offline and online strategies and demonstrate
thatahybridapproachcanofferbenefitsinspecificscenarios. Wealsodoathoroughstudyonthe
existingmultimodalpreferencedatasets,identifyingstrengthsandweaknessesassociatedwitheach,
andprovidinginsightsintohowcertaintypesofpreferencedatacanenhancemodelperformance.
Leveragingtheseinsights,wedevelopourownpreferencedatasetandintroduceanoveldatasampling
strategy,BDHS.WhenappliedtotheLLaVA1.6model,thesemethodsleadtonotableimprovements
acrossvariousbenchmarks,confirmingthepotentialoftailoredpreferencealignmentstrategiesin
refiningthecapabilitiesofMLLMs. Asignificantadvantageofthisapproachisitsabilitytooperate
effectivelyusingonlySFTdata,eliminatingtheneedforasuperiormodel,humanlabelers,orother
complexmeansofconstructingpreferencedata.
This study not only enhances our understanding of preference alignment but also establishes a
foundationforfurtherresearchintoMLLMpreferencealignment. Specifically,weidentifyseveral
gapsinthecommunity’sapproachtoaligningMLLMs:
• Whileconsiderableresearchhasbeenconductedonvariousalignmentmethods,includingboth
onlineandofflineapproaches,forLLMs,thesestudiesarelesscommoninthecontextofMLLMs.
Forinstance,RLH(AI)FisextensivelydiscussedinLLMliterature,highlightingitspotentialover
themoresimplemethodslikeDPO(Ahmadianetal.,2024;Xuetal.,2024). Wehaveprovided
someinsightsintoRL-basedalignmentforMLLMsandtheevaluationofrewardmodels,yetwe
believethereremainsasignificantgapbetweenLLMandMLLMresearchinthisdomain.
• Abetterhallucinationbenchmarkcanhelpourunderstandingofmodelimprovements. Wediscuss
some of the shortcomings of current hallucination benchmarks in Sections B.1, B.2 and B.3.
However, the development of an effective hallucination benchmark remains an active area of
research.
• Wethoroughlyanalyzevariousaspectsofpublishedmultimodalpreferencedata. However,the
coverageofthesedatasetsisstillinsufficientlystudied. Thelackofcomprehensivecoveragein
thesedatasetsmaycontributetotheabsenceofsignificantimprovementsinsomebenchmarks.
Thispaperhashighlightedkeyadvancementsandexistingchallengesinpreferencealignmentfor
MLLMs. Ourfindingspointoutimportantgapsthatneedaddressing. Wehopetheseinsightsinspires
furtherresearchandhelpsthecommunitytackleongoingchallengesinthisfield.
ACKNOWLEDGMENTS
TheauthorswouldliketothankSebastianBrechtel,MengCao,PhilippDufter,JiamingHu,Lukas
Jendele,JuanLaoTebar,ShuangMa,DhrutiShah,WillSong,JuergenWiest,andHaotianZhang
fortheirfeedbackandguidancethroughoutthisproject. Wewouldalsoliketothanktheauthorsof
Zhouetal.(2024),Lietal.(2023a),andYuetal.(2023b)fortheirpromptresponsesandeffective
communicationwithusduringtheprocess.
REFERENCES
ArashAhmadian,ChrisCremer,MatthiasGallé,MarziehFadaee,JuliaKreutzer,AhmetÜstün,and
SaraHooker. Backtobasics: Revisitingreinforcestyleoptimizationforlearningfromhuman
feedbackinllms. arXivpreprintarXiv:2402.14740,2024.
MohammadGheshlaghiAzar,ZhaohanDanielGuo,BilalPiot,RemiMunos,MarkRowland,Michal
Valko,andDanieleCalandriello.Ageneraltheoreticalparadigmtounderstandlearningfromhuman
preferences. InInternationalConferenceonArtificialIntelligenceandStatistics,pp.4447–4455.
PMLR,2024.
18JinzeBai,ShuaiBai,ShushengYang,ShijieWang,SinanTan,PengWang,JunyangLin,ChangZhou,
andJingrenZhou. Qwen-vl: Aversatilevision-languagemodelforunderstanding,localization,
textreading,andbeyond. 2023.
YangyiChen,KaranSikka,MichaelCogswell,HengJi,andAjayDivakaran. Dress: Instructinglarge
vision-languagemodelstoalignandinteractwithhumansvianaturallanguagefeedback. arXiv
preprintarXiv:2311.10081,2023.
PaulFChristiano,JanLeike,TomBrown,MiljanMartic,ShaneLegg,andDarioAmodei. Deep
reinforcement learning from human preferences. Advances in neural information processing
systems,30,2017.
ChenhangCui,YiyangZhou,XinyuYang,ShirleyWu,LinjunZhang,JamesZou,andHuaxiuYao.
Holisticanalysisofhallucinationingpt-4v(ision):Biasandinterferencechallenges.arXivpreprint
arXiv:2311.03287,2023.
YiheDeng,PanLu,FanYin,ZiniuHu,ShengShen,JamesZou,Kai-WeiChang,andWeiWang.
Enhancinglargevisionlanguagemodelswithself-trainingonimagecomprehension.arXivpreprint
arXiv:2405.19716,2024.
LeoGao,JohnSchulman,andJacobHilton. Scalinglawsforrewardmodeloveroptimization. In
InternationalConferenceonMachineLearning,pp.10835–10866.PMLR,2023a.
LeoGao,JonathanTow,BaberAbbasi,StellaBiderman,SidBlack,AnthonyDiPofi,CharlesFoster,
LaurenceGolding,JeffreyHsu,AlainLeNoac’h,HaonanLi,KyleMcDonell,NiklasMuennighoff,
ChrisOciepa,JasonPhang,LariaReynolds,HaileySchoelkopf,AviyaSkowron,LintangSutawika,
Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot
languagemodelevaluation,122023b. URLhttps://zenodo.org/records/10256836.
ShangminGuo,BiaoZhang,TianlinLiu,TianqiLiu,MishaKhalman,FelipeLlinares,Alexandre
Rame,ThomasMesnard,YaoZhao,BilalPiot,etal. Directlanguagemodelalignmentfromonline
aifeedback. arXivpreprintarXiv:2402.04792,2024.
ToddHester,MatejVecerik,OlivierPietquin,MarcLanctot,TomSchaul,BilalPiot,DanHorgan,
John Quan, Andrew Sendonaris, Ian Osband, et al. Deep q-learning from demonstrations. In
ProceedingsoftheAAAIconferenceonartificialintelligence,volume32,2018.
DrewAHudsonandChristopherDManning. Gqa: Anewdatasetforreal-worldvisualreasoning
andcompositionalquestionanswering. ConferenceonComputerVisionandPatternRecognition
(CVPR),2019.
BoLi,PeiyuanZhang,KaichenZhang,FanyiPu,XinrunDu,YuhaoDong,HaotianLiu,Yuanhan
Zhang,GeZhang,ChunyuanLi,andZiweiLiu.Lmms-eval:Acceleratingthedevelopmentoflarge
multimodal models, March 2024. URL https://github.com/EvolvingLMMs-Lab/
lmms-eval.
LeiLi,ZhihuiXie,MukaiLi,ShunianChen,PeiyiWang,LiangChen,YazhengYang,BenyouWang,
andLingpengKong. Silkie: Preferencedistillationforlargevisuallanguagemodels,2023a.
YifanLi,YifanDu,KunZhou,JinpengWang,WayneXinZhao,andJi-RongWen. Pope: Evaluating
objecthallucinationinlargevision-languagemodels,2023b.
Tsung-YiLin,MichaelMaire,SergeBelongie,JamesHays,PietroPerona,DevaRamanan,Piotr
Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European
conferenceoncomputervision,pp.740–755.Springer,2014.
FuxiaoLiu,KevinLin,LinjieLi,JianfengWang,YaserYacoob,andLijuanWang. Mitigatinghallu-
cinationinlargemulti-modalmodelsviarobustinstructiontuning. InTheTwelfthInternational
ConferenceonLearningRepresentations,2023a.
HaotianLiu,ChunyuanLi,YuhengLi,andYongJaeLee. Llava-1.5: Improvedbaselineswithvisual
instructiontuning. arXivpreprintarXiv:2310.03744,2023b.
HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee. Llava: Visualinstructiontuning,2023c.
19Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee.
Llava-next: Improved reasoning, ocr, and world knowledge, January 2024. URL https://
llava-vl.github.io/blog/2024-01-30-llava-next/.
BrandonMcKinzie,ZheGan,Jean-PhilippeFauconnier,SamDodge,BowenZhang,PhilippDufter,
DhrutiShah,XianzhiDu,FutangPeng,FlorisWeers,etal. Mm1: Methods,analysis&insights
frommultimodalllmpre-training. arXivpreprintarXiv:2403.09611,2024.
LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida,CarrollWainwright,PamelaMishkin,Chong
Zhang,SandhiniAgarwal,KatarinaSlama,AlexRay,etal. Traininglanguagemodelstofollow
instructionswithhumanfeedback. Advancesinneuralinformationprocessingsystems,35:27730–
27744,2022.
YusuQian,HaotianZhang,YinfeiYang,andZheGan. Howeasyisittofoolyourmultimodalllms?
anempiricalanalysisondeceptiveprompts. arXivpreprintarXiv:2402.13220,2024.
RafaelRafailov,ArchitSharma,EricMitchell,StefanoErmon,ChristopherD.Manning,andChelsea
Finn. Directpreferenceoptimization: Yourlanguagemodelissecretlyarewardmodel,2023.
NilsReimersandIrynaGurevych. Sentence-bert: Sentenceembeddingsusingsiamesebert-networks.
In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing.
AssociationforComputationalLinguistics,112019. URLhttps://arxiv.org/abs/1908.
10084.
Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns, Trevor Darrell, and Kate Saenko. Object
hallucinationinimagecaptioning. InProceedingsofthe2018ConferenceonEmpiricalMethods
inNaturalLanguageProcessing,pp.4035–4045,2018.
JohnSchulman,FilipWolski,PrafullaDhariwal,AlecRadford,andOlegKlimov. Proximalpolicy
optimizationalgorithms. arXivpreprintarXiv:1707.06347,2017.
AmanpreetSingh,VivekNatarajan,MeetShah,YuJiang,XinleiChen,DhruvBatra,DeviParikh,
and Marcus Rohrbach. Towards vqa models that can read. In Proceedings of the IEEE/CVF
conferenceoncomputervisionandpatternrecognition,pp.8317–8326,2019.
NisanStiennon,LongOuyang,JeffreyWu,DanielZiegler,RyanLowe,ChelseaVoss,AlecRadford,
DarioAmodei,andPaulFChristiano. Learningtosummarizewithhumanfeedback. Advancesin
NeuralInformationProcessingSystems,33:3008–3021,2020.
ZhiqingSun,ShengShen,ShengcaoCao,HaotianLiu,ChunyuanLi,YikangShen,ChuangGan,
Liang-YanGui,Yu-XiongWang,YimingYang,etal.Llavarhlf:Aligninglargemultimodalmodels
withfactuallyaugmentedrlhf. arXivpreprintarXiv:2309.14525,2023.
YunhaoTang,DanielZhaohanGuo,ZeyuZheng,DanieleCalandriello,YuanCao,EugeneTarassov,
RémiMunos,BernardoÁvilaPires,MichalValko,YongCheng,andWillDabney. Understanding
theperformancegapbetweenonlineandofflinealignmentalgorithms,2024a.
YunhaoTang,ZhaohanDanielGuo,ZeyuZheng,DanieleCalandriello,RémiMunos,MarkRowland,
Pierre Harvey Richemond, Michal Valko, Bernardo Ávila Pires, and Bilal Piot. Generalized
preferenceoptimization:Aunifiedapproachtoofflinealignment.arXivpreprintarXiv:2402.05749,
2024b.
HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,Nikolay
Bashlykov,SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,etal. Llama2: Openfoundation
andfine-tunedchatmodels. arXivpreprintarXiv:2307.09288,2023.
RonaldJWilliams. Simplestatisticalgradient-followingalgorithmsforconnectionistreinforcement
learning. Machinelearning,8:229–256,1992.
ShushengXu,WeiFu,JiaxuanGao,WenjieYe,WeilinLiu,ZhiyuMei,GuangjuWang,ChaoYu,
and Yi Wu. Is dpo superior to ppo for llm alignment? a comprehensive study. arXiv preprint
arXiv:2404.10719,2024.
20TianyuYu,JinyiHu,YuanYao,HaoyeZhang,YueZhao,ChongyiWang,ShanWang,YinxvPan,Jiao
Xue,DahaiLi,ZhiyuanLiu,Hai-TaoZheng,andMaosongSun. Reformulatingvision-language
foundationmodelsanddatasetstowardsuniversalmultimodalassistants,2023a.
TianyuYu,YuanYao,HaoyeZhang,TaiwenHe,YifengHan,GanquCui,JinyiHu,ZhiyuanLiu,
Hai-Tao Zheng, Maosong Sun, and Tat-Seng Chua. Rlhf-v: Towards trustworthy mllms via
behavioralignmentfromfine-grainedcorrectionalhumanfeedback,2023b.
TianyuYu,HaoyeZhang,YuanYao,YunkaiDang,DaChen,XiaomanLu,GanquCui,TaiwenHe,
ZhiyuanLiu,Tat-SengChua,etal. Rlaif-v: Aligningmllmsthroughopen-sourceaifeedbackfor
supergpt-4vtrustworthiness. arXivpreprintarXiv:2405.17220,2024.
WeihaoYu,ZhengyuanYang,LinjieLi,JianfengWang,KevinLin,ZichengLiu,XinchaoWang,
andLijuanWang. Mm-vet: Evaluatinglargemultimodalmodelsforintegratedcapabilities. arXiv
preprintarXiv:2308.02490,2023c.
WeizheYuan,RichardYuanzhePang,KyunghyunCho,SainbayarSukhbaatar,JingXu,andJason
Weston. Self-rewardinglanguagemodels. arXivpreprintarXiv:2401.10020,2024.
YanzheZhang, RuiyiZhang, JiuxiangGu, YufanZhou, NedimLipka, DiyiYang, andTongSun.
Llavar: Enhanced visual instruction tuning for text-rich image understanding. arXiv preprint
arXiv:2306.17107,2023.
BoZhao,BoyaWu,andTiejunHuang. Svit: Scalingupvisualinstructiontuning,2023a.
YaoZhao,RishabhJoshi,TianqiLiu,MishaKhalman,MohammadSaleh,andPeterJLiu. Slic-hf:
Sequencelikelihoodcalibrationwithhumanfeedback. arXivpreprintarXiv:2305.10425,2023b.
YiyangZhou,ChenhangCui,RafaelRafailov,ChelseaFinn,andHuaxiuYao. Aligningmodalitiesin
visionlargelanguagemodelsviapreferencefine-tuning,2024.
Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: En-
hancing vision-language understanding with advanced large language models. arXiv preprint
arXiv:2304.10592,2023.
21A IMPLEMENTATION DETAIL
For all offline experiments, as well as for Online-DPO and Mixed-DPO, we conducted a hyper-
parametersearch.Theparametersincludedlearningratesof10−7,5·10−7,10−6,5·10−6;projection
layerlearningratesof2·10−5,2·10−6,2·10−7;epochsof3,5,and7;andbatchsizesof16,32,
and48. Wereportedthebestresultsforeachmethod. Additionally,wesettheLoRArankandscaling
factorto128and256,respectively. TheβvaluesforDPO,IPO,andSLiCwereexploredat0.05,0.1,
0.2,0.5forDPO;0.8,0.9,1.0forIPO;and0.02,0.1,0.2forSLiC.
ForRLmethods(PPOandRLOO),wemaintainedconstantbasemodelparameterswhiletraining
LoRAadaptersforalignment. Specifically,forRLOO,weutilizedk =4,generatingfourdistinct
responsesforeachpromptatatemperatureof1.0. Trainingwasconductedovertwoepochswitha
batchsizeof256andalearningrateof3·10−4. PriortoRLOOtraining,wecalculatedthemean
andstandarddeviationofrewardsusingthealignmentdatasetandnormalizedtherewardsduring
trainingtoachievezeromeanandunitvariance. Wedeterminedthataβ valueof0.4providedthe
bestbalancebetweenrewardsandtheKLpenaltyforRLOO.Gradientclippingwasalsoimplemented
tocapthemaximumgradientnormat1.0.
ForPPOspecifically,wetrainedfor3epochswithalearningrateof1.41e-5usingaconstantlearning
rateschedule. Weused1GPUwithabatchof32. Fortherewardmodel,weusedaalearningrateof
2e-5andtrainedfor4000steps. Thelearningrateschedulewasalsoadjustedtobeconstantbutwith
awarmupphase. Thefractionvalueforthewarmupphaseissetat0.03. Trainingwasconductedon
8GPUswithabatchsizeof32.
B EVALUATION
B.1 POPE
WenoticedtheexistenceofanupperboundonthePOPEbenchmark,asmostofourexperiments
would reach a plateau between 86% and 88% despite improvements on other benchmarks. We
manuallylookedatthelossesamong100responsesandpresenttheresultsinthissection.
In 20% cases, we observed that the ground truth was either incorrect or disputable. In some of
thosecases,itappearedthattheontologyusedtobuildPOPEcouldpotentiallyresultindiffering
interpretations. Forexample,incertaincountries,acleardistinctionexistsbetweenacarandatruck,
althoughthisdistinctionisnotaspronouncedinotherregionsoftheworld8. Weprovidedanexample
alongtheresponseofouralignedmodel9inFigure4.
Prompt:Isthereatvintheimage?
POPEGroundTruth:yes
AlignedLLaVA1.6-7Bresponse:no
Figure4: UponanalysisofthelossesonPOPE,wenoticedcloseto20%ofcaseswheretheground
truthwaseitherincorrectordisputable. ThisexampleisfromPOPE,whichsourcesimagesfrom
COCO(Linetal.,2014).
8Anexampleofsuchdistinctionbetweencar/truckcanbeseenonCOCO_val2014_000000210789.jpgwhere
thePOPEgroundtruthexpects"no"totheprompt"Isthereacarintheimage?".
9WeusedaLLaVA1.6-7BDPO-alignedonLLaVApromptsandresponsessampledfromVLFeedback.See
Section5.3.
22Provided these examples are eliminated, we think it is plausible that performant models could
potentiallyexceeda90%accuracyrateonthePOPEbenchmark. Re-annotatingthoseexamplesis
beyond the scope of this work, however we would like to invite the community to consider it as
manyrecentSOTAmodelsexhibitsuchplateau. SeeTable4in(McKinzieetal.,2024)whereallthe
modelsreportedaredemonstratingsuchplateauonPOPE.
B.2 CHAIRANDOBJECTHALBENCH
Weevaluatedtwowidelyusedbenchmarksinthecommunityformeasuringhallucination,focusing
onthecomputationofCHAIRmetrics. WeinvestigatedapproachesdescribedbyRohrbachetal.
(2018),whichusesCOCOannotationstocomputeCHAIRscores,andthemorerecentmethodbyYu
etal.(2023b),namedObject HalBench,whichcombinesCOCOannotationswithaGPTmodel
toenhancethedetectionofhallucinatedobjects.
Ouranalysisrevealsthatbothbenchmarksaresignificantlynoisy(Figure5). Wealsofoundthatany
improvementsinCHAIRscoresstronglydependontheabilityofthesebenchmarkstodetectspecific
typesofhallucinationsandcannotbeattributedsolelytotheimprovementofthemodel.
Furthermore,itiscommontoreportCHAIRmetricswithoutincludingrecallmetrics. Considering
thetrade-offbetweenCHAIRandrecall,omittingrecalldoesnotprovideafullpictureofhowmuch
amodelhasimprovedinreducinghallucinations. Forinstance, amodelthatgeneratesshortand
consciseresponsesmightnotproducemanyhallucinations,butthismaybeatthecostofpotentially
providinganunhelpfulanswer.
Hence,therecallmetricfromRohrbachetal.(2018)provesparticularlyinformativeforcomparing
differentmodelsandhelpingwithourunderstandingofotherbenchmarks. Wereportthismetricin
ourevaluations,styledRecallcocoinourtables.
B.3 MMHALBENCH-VISION
The original MMHALBench benchmark (Sun et al., 2023) uses GPT-4 to judge whether model
responsesintroducehallucinations. Inthattext-onlyregime,MMHALBenchreliesongroundtruth
informationaboutthepictures,suchasthecategoriesoftheobjectspresentorahumanreference
responsetotheprompt.
WeevaluatedmanuallythecommonwinsandlossesobtainedonMMHALBenchduringourexper-
imentsandnoticedthatin∼20%caseswedisagreewiththeresultingMMHALBenchscore10. We
foundcaseswhereresponseswithhallucinationswereconsideredascorrect. Oppositely,wefound
caseswherevalidanswerswerewronglytaggedascontaininghallucinations. Inmanycases,wesaw
thehelpfulnesstobeunder-estimated. SeeFigure6.
Thiscanbeexplainedduethegroundtruthinformationbeingonlyexpressedthroughtextcausingthe
judgemodel,GPT-4,towronglytagormisshallucinations. Tomitigatesuchcases,weintroduced
MMHALBench-Vision: werelyontherecentlyintroducedGPT4-otoconsumetheimagealong
thetextgroundtruthinformation. Wekepttheevaluationpromptandscoringidentical.
C PREFERENCE DATA
C.1 DATASETPROMPTCORRUPTION
WeleverageGPT-4tocorruptchosenresponseswithrealisticandplausiblehallucinations. Wecall
realistichallucinationsthoseinstanceswhereahuman,justbylookingatthecorruptedresponse,
is unable to recognize it without having to refer back to the image. We have remarked this was
an important distinction: the more obvious the corruptions are, the poorer the performance of
the resulting policy is. We launched side experiments where we employed a less skilled LLM
corrupterandincorporatedobvioustags11intotheresponses. Inbothscenarios,wenoticedadropin
1021casesoutof96whilecomparingwinsandlossesoftwomodels.
11Anexampleofobviousrejectedresponsemarkedwithatag:Thereareonlytwoexamplesofplatesthatare
visibleintheimage.Thisresponseisfactuallyincorrect.
23ModelResponse:Theimagefeaturesabrownandwhitecatlying
onitssideinanorangesuitcase.Thecatappearstoberelaxedand
comfortableinthesuitcase. Thesuitcaseisplacedonabed. The
suitcaseisopen,revealingitscontents,whichincludeabookand
ahandbag. Thecatisthemainfocusoftheimage,occupyinga
significantportionoftheframe.Theorangesuitcaseandthehandbag
aretheothermainobjectsinthescene,withthehandbagbeinglocated
ontheleftsideoftheimageandthesuitcaseoccupyingtherightside.
Theoverallmoodoftheimageiscalmandcozy.
ModelResponse:Theimagefeaturesalarge,colorfultruckparked
onthesideofastreet.Thetruckisparkednexttoabrickbuilding,
andthereisacarparkedbehindit.
Thereareseveralpeopleinthescene,withonepersonstandingnear
thetruckandtwoothersfurtheraway.Oneoftheindividualsiscarry-
ingahandbag,whichisvisibleinthescene.Theoverallatmosphere
oftheimageislivelyandengaging,withtheeye-catchingtruck.
Figure5: ExamplesillustratinginstanceswheretheCHAIRandObjet HalBenchbenchmarks
failtodetecthallucinations. Texthighlightedingreenidentifieshallucinationssuccessfullydetected
bythebenchmarks. Incontrast, texthighlightedinredindicatesexampleswherethebenchmark
failedtoidentifyhallucinations. Orangeindicateshallucinationsthat,thoughnottargetedbythese
benchmarks,degraderesponsequality. ThetopexampleshowsthebenchmarkproposedbyYuetal.
(2023b)whilethebottomexamplefollowsfrom(Rohrbachetal.,2018). ImagesarefromCOCO(Lin
etal.,2014).
performanceasthecorruptiongetslessrealisticandreadily‘hackable’bythepolicyunderalignment.
ThepromptusedtocorruptthechosenresponsesisreproducedinTable13.
C.2 DATASETSIZEABLATIONWITHTHECORRUPTINGSTRATEGY
WeconductedadatasetsizeablationontheapplicationofourcorruptingstrategyonVLFeedback
(Figure7). Weevaluated7checkpointsbetween100and5,000trainingsamples,ourmaximumin
thisdataregime(Section5.3). Weprovidethebaselineresultswithadashedline. WhilePOVID
showsthebestresultonRecallcoco,oursimplecorruptionstrategyappliedoutperformsotherdatasets
onbothLLavaBench-in-the-WildandMMHALBenchhallucinationrate,whilebeingonpar
ontheMMHALBenchhelpfulnessratewithVLFeedbackvanilla.
D ANNOTATOR IN ONLINE-DPO
Table14showsthepromptweusedtoobtainonlinefeedbackfromtheannotator. Weconducted
multiple experiments with different prompts. In one setup, similar to the approach taken by
Guoetal.(2024)withtherewardsmodel,weincludedthegroundtruthresponseasanadditional
signalfortheannotatortoevaluatebothresponses. Wedidnotobserveanysignificantchangein
either the evaluation metrics or the final performance of the aligned model. This may be due to
thefactthatmostoftheopen-sourceMLLMsweusedinthisstudystilllacktheabilitytofollow
24Prompt:Howmanypeoplearethereintheimage?
imageID:16189396430_4dce91a9d7_o
MMHALreferenceresponse:Therearefourpeopleintheimage.
MMHALcategories: Girl, Humanhead, Humanhair, Mammal,
Dog,Person,Humanface
LLaVA1.6-7Baligned:Therearefourpeopleintheimage:aman,
awoman,andtwochildren.
GPT-4response:totalscore=1,hallucination=1,helpfulness=1
Prompt:Comparethetwodifferentviewsofthebackpack.
imageID:12392564684_ffd19b5b55_o
MMHALreferenceresponse:Theimageshowstwodifferentviews
ofthesameblackbackpack.Ontheleftside,itshowstheinsideof
thebag,containingacamera,twolensesanotheraccessories.Onthe
rightside,itshowstheoutsideofthebag,andthereisa"Lowepro"
logoonit.
MMHALcategories:Hikingequipment,Clothing,Backpack
AlignedLLaVA1.6-7Bresponse: Theimageshowsabackpack
withtwodifferentviews:thetopviewandthesideview.Inthetop
view,wecansee(...)
GPT-4response:totalscore=4,hallucination=0,helpfulness=1
Figure6: ExampleofincorrectassessmentswithMMHALBench. Thetopexampleshowsaresponse
incorrectly assessed as having a hallucination. The second presents a response where the model
hallucinatesasideviewofthebackpack,butthatismissedbyMMHALBench. Inbothcases,thetext
groundtruthdoesnotgiveenoughinformationtothetext-onlyevaluatortoevaluatetheresponse.
ReproductionofMMHALBenchimagescannottakeplacehereduetousagerestrictions. Pleaserefer
tohttps://huggingface.co/datasets/Shengcao1006/MMHal-Bench.
instructionseffectively,especiallywhentheinstructionscontainmultiplecomponentsordetailed
steps.
Wealsoexaminedthepotentialbiasoftheannotatormodelinchoosing"Response1"or"Response
2"andfoundnonoticeablebias.
Figure8showsanexampleofanannotationmadebyLLaVA1.6-34Bmodel.
E BIAS-DRIVEN HALLUCINATION SAMPLING
E.1 ADDINGNOISETOIMAGES
This section describes how to gradually add noise to images through a diffusion process. The
derivationfollowsthepublicimplementationofPOVID-styleimagedistortion(Zhouetal.,2024)to
enabletheproperreproductionoftheirresults.
Letx (k)denotetheimageafterapplyingnoisek-timeswithx (0)referringtotheoriginalimage
img img
andN(0,1)representthenormaldistribution. Thentheforwardnoiseprocessisdefinedas:
(cid:112) (cid:112)
x (k)= 1−β x (k−1)+ β ϵ withϵ∼N(0,1). (8)
img k img k
Hereby, β denotes a time-variant parameter which is set to β = σ(−6+ 12k )·(0.5·10−2 −
k k 1000
10−5)+10−5tograduallyincreasenoisebetweenk =0andk =1000(refertoFigure9).
Therecursiveequation(8)canbereformulatedtoapplykstepsofnoiseatonce. Settingα =1−β
k k
andα¯
=(cid:81)N
α ,thefollowingequationappliesN stepsofnoisetoimagex (0):
N k=1 k img
√ √
x˜ (N)= α¯ x (0)+ 1−α¯ ϵ withϵ∼N(0,1). (9)
img N img N
ThedefaultforN inZhouetal.(2024)isN =500.
25Youareprovidedwithapromptandaresponsetothatprompt.Youtaskistocorrupttheresponsewith
realistichallucinations.
Hallucinations can be including objects/persons/animals not present in the original response,
changingthepositionsoftheobjects/persons/animals,changingthecount,changingtheattributes(e.g.
color,shapes,etc.)ortheirspatialrelationships.Thosehallucinationshavetoberealistic.
In other words, your task is to change the provided response in a way that could be correct
giventheprompt,butisnot.Apersononlyseeingthecorruptedresponseshallnotbeabletoidentifyit
asbeingincorrect,soyourchangemustbeaplausible(althoughagainincorrect). Outputonlythe
updated,corruptedresponse,nothingelse.
Hereareafewexamples:
###Example1
prompt:Whatisthetitleofthebookmentionedintheimage?
response:Thetitleofthebookintheimageis"PictureThis:AGalleryofFusibleAppliquéProjectsfor
QuiltingandFraming"byJeanWells&MarinaAnderson.
corruptedresponse:Thetitleofthebookintheimageis"SewingMasters:ACollectionofInnovative
EmbroideryDesignsforQuiltingandFraming"byMarvinWells&VincentAnderson.
###Example2
prompt:Youareadrivingassistant.Basedoncurrentimage,whatisthebestactiontotakewhenyou
aredrivingontheroad?A.SlowdownB.TurnaroundC.StopthecarD.TurnLeftE.Keepdriving.
Pleasegivereasonsandtheanswer
response:Thebestactiontotakewhendrivingontheroad,basedonthecurrentimage,wouldbe:E.
Keepdriving.
Reasonsforthisaction:
1.Thetrafficlightvisibleintheimageisgreen,indicatingthatitislegalandsafetoproceed.
2. Thespeedlimitsignshows"60"whichmeansthecar, currentlyat20km/h, iswellbelowthe
maximumspeedlimitandcansafelycontinueandevenincreasespeedifnecessary.
3. Theroadaheadisclearofanyimmediatehazardsorobstructionsthatwouldrequirestoppingor
turning.
corruptedresponse: Thebestactiontotakewhendrivingontheroad,basedonthecurrentimage,
wouldbe:C.Stopthecar.
Reasonsforthisaction:
1.Althoughthetrafficlightontheleftintheimageisgreen,theoneonyoursideisred,indicatingitis
necessarytostop
2. Thespeedlimitsignshows"60"whichmeansthecar, currentlyat20km/h, iswellbelowthe
maximumspeedlimitandcansafelystopbeforetheintersection.
3.Theintersectionupaheadindicatesthepresenceofcrossingcars,requiringastop.
###Example3
prompt:{original_prompt}
response:{original_response}
corruptedresponse:
Table13: PromptusedtocorruptdatasetswithGPT-4.
E.2 ENSURINGSEMANTICALLYMEANINGFULDIFFERENCES
Section3.4.3describesaniterativetechniqueforBDHSthatevaluatessimilarityscoresbetweenthe
generatedresponsey˜−andthegroundtruthy+. Ifbothresponsesareidentifiedassimilaraccording
tothesentenceembeddingsmodel,anewBDHSresponseissampleduntilamaximumnumberof
iterationsN isreached. Thelastiterationwaivesthegroundtruthreferenceandgeneratesafull
BDHS
responsewhichisthentakenasy˜−regardlessofthesimilarityscore. Figure10showsthenumber
ofnon-similarresponses,i.e. ensuringϵ <0.97,overthenumberofBDHSiterationsforthefull
s
POVID(5k)dataset.AsexpectedallBDHSvariantsresultinalargernumberofnon-similarresponses
comparedtothemodelresponsewithoutimageattentionblockingornoisyimages. RunningBDHS
withasingleiterationalreadyresultsinmorethan74%semanticallydifferentresponses. Afterfour
2672
85
70
68
80
66
POVID POVID
VLFeedback 64 VLFeedback
75
VLFeedbackCorrupted(Ours) VLFeedbackCorrupted(Ours)
LLaVA1.6-7B LLaVA1.6-7B
62
0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0
1 5 0 0 0 0 0 1 5 0 0 0 0 0
1 2 3 4 5 1 2 3 4 5
Preferencetrainingsetsize Preferencetrainingsetsize
1.4 0.5
0.45
1.3
0.4
1.2
POVID 0.35 POVID
VLFeedback VLFeedback
VLFeedbackCorrupted(Ours) VLFeedbackCorrupted(Ours)
LLaVA1.6-7B LLaVA1.6-7B
1.1 0.3
0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0
1 5 0 0 0 0 0 1 5 0 0 0 0 0
1 2 3 4 5 1 2 3 4 5
Preferencetrainingsetsize Preferencetrainingsetsize
Figure7: Impactofthepreferencedatasetsize. Ourcorruptingstrategyoutperformsotherdatasetson
LLaVA-in-the-WildandMMHALBenchhallucinationrate. ItisonparontheMMHALBench
helpfulnessrateagainstvanillaVLFeedback. Finally,POVIDreportsthehighestRecallcoco. The
dashedlinesarethescoresfortheLLaVA1.6-7Bbaseline.
iterations,BDHSvariantswithrestrictedimageaccessdifferinover90%whilethelastiterationis
guidancefreeandonlydependsonthesampledattentionmaskresp.noise.Interestingly,initeration5,
BDHS ,ρ =1correspondstoguidance-freeresponsegenerationwithfullyblockedimagetokens
attn th
whichstillresultsin7%similarresponsesw.r.t.theSFTgroundtruth. Probablereasonsforthis
saturationareeitherthatthecorrectansweriseasytoguessevenwithoutaccesstotheimage,orthat
theanswerismemorizedfromthetrainingdata. NotethatpromptsandimagesinPOVID(5k)are
extractedfromLLaVAInstructwhichservedastrainingdataforfine-tuningLLaVA-1.6.
BDHS with noisy images in the input and N = 500 diffusion steps results in more than 99%
semantically different responses after five iterations, surpassing the score for the fully blocked
response. Thisismisleading,asalthoughtheresponsesareindeedsemanticallydifferent,theymostly
mentionthatthepromptcannotbeevaluatedduetoblurryandnoisyimages. Essentially,thenoise
addsanadditionalbiastowardsnoise/pixel-referringresponsesinsteadofinducingonlythedesired
inherentbiaswhichwouldsaturateatapprox.93%(responsewithfullymaskedimagetokens).
27
↑dliW-eht-ni-hcneB-avalL
↑etarssenlufpleHhcneBLAHMM
↑
ococllaceR
↓etarnoitanicullaHhcneBLAHMMThemostimportantpartofthistaskistochoosearesponsethatcontainslesshallucination.
Everythingintheanswershouldbebasedonthecontentsoftheimage. Youaregivenan
image,aquestion,andtworesponses. Ifthecontextisaboutsomethingpractical,ahelpful
responseisaconciseresponse,andnotonewithirrelevantquestionsandcomments. You
areanexpertannotator,andyouratetheanswerwithlesshallucinationandmorehelpful
informationabouttheimageasthebetteranswer. Lesshallucinationmeanseveryobjector
attributeoftheobject,likecolorandrelationship,isdescribedaccuratelyandasitappears
intheimage. Ifsomethingisambiguousintheimage,theanswershouldavoidincluding
anydetailsthatarenotclearlyvisibleintheimage. Inyourresponse,youshouldgenerate
ananswerwhereyouindicatewhetherResponse1orResponse2isbetterandexplainthe
reason.
Table14: ThePromptUsedfortheAnnotator.
Prompt Howmanypeopleareinthepool?
Response1: Thereisapersoninthepool, ayoungman
enjoyinghistimesurfingthesmallwaves.
Response2:Thisisnotaswimmingpoolbuttheocean,and
ontheocean,thereisaperson.Ayoungmanisenjoyinghis
timesurfing.
Annotator:Basedontheimagecontent,Response2isbetter
andmorehelpful. Theimageshowsapersonsurfinginthe
ocean,notaswimmingpool.Thesurferisridingawave,and
therearenootherpeoplevisibleintheimage.Thedescription
accuratelyreflectsthecontentoftheimageanddoesnotin-
cludeanyhallucinationsorirrelevantdetails.
Figure8: ExampleofAnnotation. ImageisfromCOCO(Linetal.,2014).
0.006
0.004
0.002
0
0 100 200 300 400 500 600 700 800 900 1,000
Stepk
Figure9: Scheduleofdiffusionparameterβ .
k
Afterthreeiterations,thescoreofBDHS ,ρ =0.99reachestheonefromthefullyblockedresponse
attn th
whichishypotheticallyimpliedduetoincreaseddiversitybysubsamplingadistinctattentionmask.
SectionE.5presentsseveralexampleswithactualresponses.
E.3 BDHSALGORITHM
ThegeneraloverviewofBDHSisprovidedinFigure2. Thissectionintroducesthecorresponding
algorithmwhichislistedinAlgorithm1. Thisversionincludesboth,noisyimagesforBDHS and
noise
attentionmaskingforBDHS (refertothecommentsinAlgorithm1). Weaddastraightforward
attn
heuristictoswapyesandnowordswhenevertheyoccurinthebeginningofasentence. Forthis
purposeline12introducearegularexpressionwhichmatchesanyyesornoatthebeginningofeach
sentenceandoptionallyskipsanyprecedingnewlineorwhitespacecharacters. Thisexpressioncan
beextendedtofurtheruse-casesifdesired. WechoosetogeneratethefullresponsewithoutanySFT
28
kβBDHSnoise,N=300 BDHSnoise,N=500 BDHSattn,ρth=0.98
BDHSattn,ρth=0 BDHSattn,ρth=0.99 BDHSattn,ρth=1
100
90
80
75
1 2 3 4 5
BDHSIteration
Figure 10: Number of resolved similar responses for BDHS generation based on POVID (5k).
Parametersareϵ =0.97andN =5.
s BDHS
groundtruthguidanceintheverylastiterationwheneverN >1tominimizesimilarity(referto
BDHS
line5).
Algorithm1BDHS
Require: Promptx ,imagex ,SFTgroundtruthy+,attentionmaskingparameterρ ,imagenoiselevelN,
text img th
BDHSiterationsN ,similaritythresholdϵ
BDHS s
1: fori=1,2,...,N do
BDHS
2: m←Sampleimageattentionmaskwithρ accordingto(6) ▷ρ >0onlyforBDHS
th th attn
3: x˜ ←AddNoise(N,x )via(9) ▷N >0onlyforBDHS
img img noise
4: x˜←(x ,x˜ ,m)
text img
5: ifN >1andi=N then
BDHS BDHS
6: y− ←Generatefullmodelresponseviaπ (·|x˜)
θ
7: returny−
8: S ←Splity+intoSsentences
9: y− ←∅ ▷Initializeemptystring
k
10: foreachy+inSdo ▷Parallelizable
k
11: ξ←ξ∼U(0,1) ▷U(0,1)denotestheuniformdistributionin[0,1]
12: ify+matchesr"ˆ[\s]*(Yes|yes|No|no)"andξ≥0.5then ▷r"·"denotesaregularexpression
k
13: y+ ←SwapcorrespondingYes/yesbyNo/noandviceversa
k
14: y+ ←Samplerandompositioniny+andreturnfirstsubstring
k,1 k
15: y− ←Completesentencevia(7)untilfullstopor<eos>
k,2
16: y− ←(y+ ,y− ) ▷Concatenatestringstofullsentence
k k,1 k,2
17: y− ←(y−,y−) ▷Appendtooverallresponse
k
18: ϕ←Computesimilarityscorebetweeny−andy+in[0,1] ▷Usesentenceembeddings
19: ifϕ<ϵ then
s
20: break ▷Semanticallydifferentaccordingtothreshold
21: returny−
E.4 ADDITIONALABLATIONS
Additional BDHS ablations, especially regarding different hyperparameter choices are shown in
Table15. WealsoevaluateSFTguidance-freegenerationonlywithattentionmaskingactive. The
correspondingbenchmarkresultsarelistedinthefirsttworows. Allsubsequentrowsevaluatethe
fullBDHSapproachincludingSFTguidance. Weincludeablationsthatrelyonnoisyimagesrather
thanattentionmasking,followingthediffusionprocessdescribedin E.1.
E.5 ADDITIONALEXAMPLES
ThissectionpresentsfurtherexamplesofresponsesgeneratedfromLLaVAinstructpromptsand
images. ThedifferentvariantsofBDHSareintroducedinSection3.4. RefertoFigure11forthe
29
sesnopseRralimiS-noN%y˜−derivedfrompolicy POPE↑ MMHAL↑ MMHALV ↑ LLaVAW↑ VQAT ↑ GQA↑ MMVet↑ Recallcoco↑
–(Baseline) 86.40 2.95 2.75 80.85 64.85 64.23 43.94 68.13
–(PlainDPO) 88.18 2.93 2.93 81.89 64.90 64.34 43.39 71.80
AttentionMasking,ρth=0.98 88.61 2.25 2.25 82.25 64.92 64.04 42.75 77.46
AttentionMasking,ρth=0.99 88.70 2.52 2.51 86.08 65.07 64.06 42.02 77.04
BDHSattn,ρth=0.98 88.80 2.56 2.68 86.54 65.02 64.03 43.03 76.10
BDHSattn,ρth=0.99 88.75 2.61 2.71 86.33 65.07 63.97 43.39 75.58
BDHSattn,ρth=1.00 88.70 2.63 2.80 84.15 65.18 63.93 43.12 75.37
BDHSnoise,N=100 88.50 2.58 2.48 82.46 64.96 64.34 40.14 75.47
BDHSnoise,N=200 88.55 2.49 2.38 83.43 65.10 64.24 38.76 74.53
BDHSnoise,N=300 88.59 2.43 2.45 85.16 65.11 64.18 40.69 76.10
BDHSnoise,N=400 88.66 2.39 2.42 83.72 65.09 64.29 40.41 75.16
BDHSnoise,N=500 88.59 2.36 2.49 84.53 65.05 64.14 41.38 75.16
Table15: AdditionalablationresultsforOffline-BDHS. AllresultsarebasedonLLaVA1.6-7B,
usingDPOandthePOVID(5k)sampleforthesourceofimagesandprompts. andprompt.
examples. For guided generation, the colored text is generated purely from the model while the
standardtextistakenfromtheSFTgroundtruth.
30SFTgroundtruth: Thejetlinerintheimageisblueandred.
BDHSattn,ρ th=0(ours): Thejetlinerinthe BDHSnoise,N=300(ours): Thejetlinerin
imageisredandblue. theimageisblue.
BDHSattn,ρ th=0.98(ours): Thejetlinerin BDHSnoise,N=500(ours): Thejetliner
theimageiswhite. intheimageisnotvisibleduetothehigh-
resolutionpixelation.
Prompt: Whatcoloristhe B thD eH imS aa gtt en, iρ st wh= hi0 te. .99(ours): Thejetlinerin POVID-styleimagedistortion,N=300:
jetlinerintheimage? Thejetlinerintheimageisblue. red.
BDHSattn,ρ th=1(ours): Thejetlinerinthe
imageiswhite. POVID-styleimagedistortion,N=500:
Theimagelinerintheimageisnot. white.
SFTgroundtruth: ThesubjectoftheimageistheBigBenclocktower,whichisafamous
attractioninEurope,particularlyinLondon,England.
BDHSattn,ρ th=0(ours): Thesubjectofthe BDHSnoise,N=300(ours): Thesubjectof
imageistheElizabethTower,morecommonly theimageistheElizabethTower,whichisthe
knownasBigBen,afamousclocktowerlo- clocktoweratthenorthendofthePalaceof
catedinLondon,England. WestminsterinLondon,England.
BDHSattn,ρ th=0.98(ours): Thesubjectof BDHSnoise,N=500(ours): Thesubjectof
Prompt: Whatisthesub- theimageisaclocktowerwithabluesky theimageisaclose-upofapatternthatap-
jectoftheimage? background. pearstobeadigitalorpixelatedtexture.
BDHSattn,ρ th=0.99(ours): Thesubject POVID-styleimagedistortion,N=300:
oftheimageisatall,narrowtowerwitha ThesubjectoftheimageistheElizabethBen
pointedroof,whichappearstobeachurch clocktowerinwhichisafamouslandraction
steeple. inLondon,specificallyinLondon,England.
The
B imD aH geSa it stn a,ρ peth r= so1 n’s(o hu ar ns d): hT olh de ins gub aje sc mt ao rf t-the POVID-styleimagedistortion,N=500:
phone. Theimageoftheimageappearsapattern
Bangclocktowerinwhichisafamouslan-
dractioninLondon,specificallyinLondon,
England. The
SFTgroundtruth: Thestreetnamesvisibleonthestreetsignintheimageare"TelegraphHill"
and"GreenwichRoad."
BDHSattn,ρ th=0(ours): Thestreetsignin BDHSnoise,N=300(ours): Theimageis
theimageshows"Greenwich"and"Telephone quiteblurry,butIcanmakeoutthestreet
Hill. sign.
B inD tH heS ia mttn a, gρ eth s= ho0 w. s98 the(o iu nr tes) r: seT cth ioe nst or fee "t Ts ei lg en - yB oD uH ’vS en poi rs oe, vN ide= d5 is0 v0 er( you br ls u) r: ryT ah ne dim laa cg ke sclear
phonePole"and"NewspaperBox"streets. details,makingitdifficulttoreadanystreet
names.
BDHSattn,ρ th=0.99(ours): Thestreetsign
intheimageshowstheintersectionof"Broad- POVID-styleimagedistortion,N=300:
way"and"SpringStreet. Theimagesignvisibleonthestreetsignin
theimageare"Hchn""and"HStreetStreet."
BDHSattn,ρ th=1(ours): Thestreetsignin
theimageshowstheintersectionof"Main POVID-styleimagedistortion,N=500:
Prompt: Whatstreetnames Street"and"ElmStreet. Theimagesignareonthestreetsigninthe
arevisibleonthestreetsign imageare"1chnAvenue"and"BwichStreet."
intheimage?
Figure11:ExamplesofgeneratedresponsesfromBHDSablationsandPOVID-styleimagedistortion.
Theimage,promptandSFTgroundtrutharetakenfromLLaVA-Instruct-150k,whichsourcesthem
fromCoCo(Linetal.,2014).
31