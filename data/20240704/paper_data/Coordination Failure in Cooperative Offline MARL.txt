Coordination Failure in Cooperative Offline MARL
CallumRhysTilbury1∗ ClaudeFormanek1,2∗† LouiseBeyers1
JonathanShock2,3 ArnuPretorius1
1InstaDeep
2UniversityofCapeTown
3TheINRS,Montreal
Abstract
Offlinemulti-agentreinforcementlearning(MARL)leveragesstaticdatasetsof
experience to learn optimal multi-agent control. However, learning from static
datapresentsseveraluniquechallengestoovercome. Inthispaper,wefocuson
coordinationfailureandinvestigatetheroleofjointactionsinmulti-agentpolicy
gradientswithofflinedata,focusingonacommonsettingwerefertoasthe‘Best
ResponseUnderData’(BRUD)approach. Byusingtwo-playerpolynomialgames
as an analytical tool, we demonstrate a simple yet overlooked failure mode of
BRUD-basedalgorithms,whichcanleadtocatastrophiccoordinationfailureinthe
offlinesetting. Buildingontheseinsights,weproposeanapproachtomitigatesuch
failure,byprioritisingsamplesfromthedatasetbasedonjoint-actionsimilarity
duringpolicylearninganddemonstrateitseffectivenessindetailedexperiments.
Moregenerally,however,wearguethatprioritiseddatasetsamplingisapromising
areafor innovationinoffline MARLthat canbecombined withother effective
approachessuchascriticandpolicyregularisation. Importantly,ourworkshows
howinsightsdrawnfromsimplified,tractablegamescanleadtouseful,theoretically
groundedinsightsthattransfertomorecomplexcontexts. Acoredimensionof
offeringisaninteractivenotebook, fromwhichalmostallofourresultscanbe
reproduced,inabrowser.‡
1 Introduction
Offlinereinforcementlearning(RL)isapromisingparadigmformakingreal-worldapplications
of RL possible. While some compelling progress is being made, particularly in the single-agent
setting(Prudencioetal.,2023),largeobstaclesremain. Inthispaper,wefocusonaproblemunique
tothemulti-agentsetting: learningcoordinationfromstaticdata(Bardeetal.,2024). Whereasin
onlinemulti-agentlearning,aspeculatedfailureincoordinationcanbetestedandcorrected,such
feedbackdoesnotexistintheofflinecase. Instead,theagentsareconstrainedtosolelyusingstatic
datatolearnhowtobestacttogether. Typically,agentsoptimisetheirownactionstowardsabest
responsetotheactionstakenbyotheragentsinthedataset,werefertothiscommonapproachas‘Best
ResponseUnderData’(BRUD).Thisapproachhasvariousbenefitsintheofflinesetting,butishighly
susceptibletomiscoordination. ThisisclearlyillustratedinFigure1,usingasimpletwo-playergame
∗Equalcontribution.
†Correspondingauthor:c.formanek@instadeep.com
‡https://tinyurl.com/pjap-polygames
WorkshoponAligningReinforcementLearningExperimentalistsandTheorists(ARLET)attheInternational
ConferenceonMachineLearning,2024.
4202
luJ
1
]GL.sc[
1v34310.7042:viXraFigure1:Illustrationofcatastrophicmiscoordinationwhenagentseachlearnbasedonabestresponse
tothedataofotheragentactions(BRUD).Weconsiderusingadatapointa ,inasimplegamewhere
(t)
therewardisgivenbytheproductofeachagent’saction,R(a ,a )=a a . Thebestresponseof
x y x y
agentx,inresponsetotheotheragent’snegativedatapoint,a < 0,istomakeitsownpolicy
y(t)
µ(θ )morenegative. Similarly,agentyupdatesµ(θ )tobemorepositive,inresponsetotheother
x y
agent’spositivedatapoint,a >0. Alas,theBRUDstepmovesthejointpolicyintheopposite
x(t)
directionofoptimalincrease.
whereagentschooseacontinuousrealnumber,andthecollectiverewardistheproductofthetwo
actionschosen.
Inthiswork,weusesimpletwo-playerpolynomialgamesasananalyticaltoolforbetterunderstanding
offlinecoordination,inaninterpretableandaccessibleway. Indoingso,weisolatetheproblemwith
aBRUD-styleupdateinofflineMARL,demonstratingclearmodesofcoordinationfailure. Then,
buildingonourinsights,weproposeaclassofofflinesamplingmethods,broadlycalledProximal
JointActionPrioritisation(PJAP)thathelpalleviateproblemsincoordinationthatstemfromoffline
learning. WedemonstratetheeffectivenessofPJAPindetailedexperiments. However,weseethis
workmoreasexploratoryinnature,moregenerallyhighlightingprioritisedsamplingmethodsasa
fruitfulareaoffutureinvestigationalongsideapproachessuchascriticandpolicyregularisationfor
offlinelearning.
2 Foundations
2.1 Multi-AgentReinforcementLearning
We consider the canonical Dec-POMDP setting for MARL where the goal is to find a joint pol-
icy (π ,...,π ) ≡ π such that the return of each agent i, following π , is maximised with
1 n i
respect to the other agents’ policies, π ≡ (π\π ). That is, we aim to find π such that
−i i
∀i : π ∈ argmax E[G|πˆ ,π ],whereGisthereturn. Weassumethateachpolicyisparame-
i πˆi i −i
terisedbyθ . ApopularapproachtolearningsuchpoliciesisCentralisedTrainingwithDecentralised
i
Execution(CTDE),wheretrainingleveragesprivilegedinformationfromallagents,yetthepolicies
areonlyconditionedontheirlocalobservations,π (o ;θ ),enablingdecentralisationatinference
i i i
time.
Becauseofourfocusontheofflinesetting,wenarrowourscopetooff-policyalgorithms,where
we learn using batches of data taken from a replay buffer, B. Specifically, we study multi-agent
actor-criticapproacheswhichhaveapolicyobjectiveoftheform,
J(π)=E [Q(o,a)+αR] (1)
a∼π
whereQ(o,a)isthejointcritic,andRissomepolicyregularisationterm,controlledbyα∈R. This
policyobjectiveisakeycomponentofmanypopularCTDEalgorithmsinMARL.Forexample,by
usingstochasticpoliciesandentropyregularisation, R = H(π), werecoverthepolicyobjective
of multi-agent soft-actor critic (Pu et al., 2021). With deterministic policies, π (a |o ) = µ (o ),
i i i i i
and setting α = 0, we recover the policy objective of MADDPG (Lowe et al., 2017), and so on.
Importantly, this policy update forms part of several leading offline MARL algorithms, such as
CFCQL(Shaoetal.,2023)andtheCTDEformofOMAR(Panetal.,2022).
22.2 JointActionFormulation
InthepolicyobjectiveinEquation1,thetrainingiscentralisedbyconditioningthecriticonthejoint
observation,o,andthejointaction,a. Thejointobservationcanbeformedasasimpleconcatenation
ofagentobservationsinthesampleddata,o=(o ,...,o ).However,formingthejointactionismore
1 n
complex. Forthepolicylearningofagenti,weconsidera ∼π(·|o ;θ ),butwehaveseveralchoices
i i i
fortheotheragentactions,a ,intheupdate. Forexample,wecouldsimplyusetheotheragents’
−i
policiesdirectly,a ∼ π(·|o ;θ ). However,thisapproachhasbeenshowntoworkpoorlyin
−i −i −i
offlinesettings(Shaoetal.,2023),likelybecausewearedecouplingpolicylearningfromthedataset.
Instead,theprevailingapproachtoformingthejointactioninCTDEmethods,bothonlineandoffline,
istosimplyusethesamplestakenfromthebufferordatasetfortheotheragentactions. Thatis,
a =(a ∼π(·|o ;θ ), a ∼B) (2)
i i i i −i
WecallthistheBestResponseUnderData(BRUD)approachtopolicylearning. Thoughitbenefits
fromstayingdirectlycoupledtothedatasetinanofflinecontext,itleadstocoordinationproblems,
whichwewilldemonstrateshortly.
2.3 PolynomialGames
Forourexposition,westudytwo-playerpolynomialgames(Dresheretal.,1950;Zhongetal.,2024),
asadifferentiable,continuousgeneralisationofdiscretematrixgames—whichhavebeenacommon
toolforunderstandingmulti-agentalgorithms(Rashidetal.,2020;Papoudakisetal.,2021). These
gamesareatemporalandstateless,comprisingtwoagents,xandy,eachabletotakecontinuous
actions. Wedenotetherespectiveactionstakenasa ,a ∈R. Thesharedrewardgiventotheagents
x y
isdefinedbysomepolynomialfunction,R(a ,a ) = (cid:80)m (cid:80)n c aiaj. Becausethereisno
x y i=0 j=0 ij x y
state,andthusnoobservations,thenotionofmaximisingthejointQ-function,Q(o,a={a ,a })is
x y
equivalenttomaximisingtherewardfunctionR(a ,a )directly. Weassumeperfectknowledgeof
x y
therewardfunctioninthegame.
3 CoordinationFailureinOfflineMARL
WenowstudycoordinationfailureinofflineMARLduetotheBRUDapproachinpolicylearning,
usingtractableandinformativepolynomialgames. WeuseMADDPG(Loweetal.,2017),whichhas
aBRUD-stylepolicyupdateforagenti,
∇ J =E (cid:2) ∇ µ(o ;θ )·∇ Q(o,a˜ ,a )| (cid:3) (3)
θi (o,a)∼B θi i i a˜i i −i a˜i=µ(oi;θi)
whereµ(o ;θ )isadeterministicpolicy,andBisareplaybufferordataset.
i i
Recallthatthepolynomialgamesettingisstateless,andcomprisesjusttwoagents,takingactions
a and a . For simplicity, let the policy for each agent be a single linear unit, µ(θ ) = θ and
x y x x
µ(θ )=θ (i.e. thepolicyparameterdirectlydefinestheaction). Wecanthussimplifythepolicy
y y
updatesuchthatforagentx,
∇ J =E (cid:2) ∇ µ(θ )·∇ R(a˜ ,a )| (cid:3) =E [∇ R(a˜ ,a )| ] (4)
θx a∼B θx x a˜x x y a˜x=µ(θx) ay∼B a˜x x y a˜x=θx
andsimilarlyforagenty,wehave∇ J =E (cid:2) ∇ R(a ,a˜ )| (cid:3) . Therefore,eachcompo-
θy ax∼B a˜y x y a˜y=θy
nentinthegradientoftheobjectiveissimplythepartialderivativeoftheagent’srewardwithrespect
totheagent’schosenaction,inexpectationovertheactionsoftheotheragentfromthereplaybuffer
orthedataset. ThisequationcapturestheessenceofthepolicyupdateofBRUDmethods.
Tounderstandtheramifications offormingthejoint actioninthisway, wefirststudythe simple
polynomial R(a ,a ) = a a , which we call the sign-agreement game. The true gradient field
x y x y
ofthissurfaceis∇R = (a ,a ),whereastheobjectiveintheMADDPGupdatebecomes∇J =
y x
(E [a ],E [a ])=(a¯ ,a¯ ),wherea¯ anda¯ arethesamplemeansoftherespectiveactions
ay∼B y ax∼B x y x x y
inthedatatakenfromB. Weconsidertheimpactofthedifferencebetween∇Rand∇J forthis
game. Whereastheformerisafunctionofthecurrentpolicy,alwayscorrectlypointingtotheoptimal
direction of policy improvement, the latter is a unidirectional vector field defined solely by the
sampleddata. Asaresult,itbecomespossibleforcatastrophicmiscoordinationinthejoint-policy
update,asillustratedinFigure1. Inthisexample,thebestwaytoupdateθ ,inresponsetoanegative
x
3datapoint,a < 0,istomakeθ morenegative. Simultaneously,thebestwaytoupdateθ ,in
y(t) x y
responsetoapositivedatapoint,a >0,istomakeθ morepositive. Alas,thisjointupdatestep
x(t) y
actuallymovesthejointpolicyintothe−+region(topleft),yieldingalowerreward—theexact
oppositeofourintention.
3.1 ConnectionstoOff-PolicyLearning
Importantly,thepossibilityofmiscoordinationasdemonstratedhereispresentunderanyBRUD-style
update—regardlessofwhetherwearelearningonlinefromadynamicbuffer,orofflinefromastatic
dataset. Consider,though,howtheimpactofthepolicygradientupdatefromEquation4changes
aswemovefromlearningonlinetooffline. Wecanillustratethisshiftbystudyingthesizeofthe
replaybuffer,B. Recallthatintroducingareplaybufferimprovessampleefficiencyandstabilises
trainingindeepreinforcementlearning(Mnihetal.,2013). Ausefulwaytounderstandthebuffer
istherelationshipbetweenitssizeandthedegreetowhichlearningisoff-policy. Supposedataof
sizebissampledfromthebuffertoupdatetheagents;thenwithabuffersizeofb,agentsareusing
experienceimmediatelyafterwitnessingit—whichisexactlyon-policy,akintoanapproachlike
REINFORCE(Williams,1992). Naturally,then,asthebuffersizeincreases—wheredataisreplaced
lessandlessfrequently—thealgorithmbecomesincreasinglyoff-policy. Inthelimit,wherethebuffer
sizeisinfiniteanddataisneverreplaced,thesettingbecomesakintoofflinelearning,albeitwith
freshdatastillbeingadded.
Figure2showstheimpactofincreasingthebuffersizeinthesign-agreementgamewhentraining
onlinewithMADDPG.Withthesmallestbuffersize,thepolicymovesalongtheoptimaltrajectory,
firsttowardsthesaddleat(0,0)andthentowardsthehigh-rewardregionof++.TheBRUDapproach
workswellhere,sincethesampledjointactionislikelytoberelativelyclosetothecurrentpolicy,
mitigating challenges of miscoordination. Yet as the max buffer size grows (i.e. the algorithm
becomesmoreoff-policy),thetypicalsampledjoint-actionisfurtherawayfromthecurrentpolicy,as
visualisedintheplotsofthebufferstateattheendoftraining. Asaresult,weseehowthelearnt
policybecomesincreasinglysub-optimal,duetomiscoordinationproblemsdiscussedpreviously.
Trajectory
Buffer size 128 Buffer size 256 Buffer size 512 Buffer size 1024
1.0 1.0
0.5 0.5
0.0 0.0
0.5 0.5
1.0 1.0
1.0 0.5 0.0 0.5 1.0 1.0 0.5 0.0 0.5 1.0 1.0 0.5 0.0 0.5 1.0 1.0 0.5 0.0 0.5 1.0
Action x Action x Action x Action x
Sample efficiency
1.00
0.75
0.50
0.25
0.00
0.25
0.50
0 50 100 150 0 50 100 150 0 50 100 150 0 50 100 150
epochs epochs epochs epochs
Final buffer state
1.0
0.5 40
0.0
20
0.5
0
1.0
1.0 0.5 0.0 0.5 1.0 1.0 0.5 0.0 0.5 1.0 1.0 0.5 0.0 0.5 1.0 1.0 0.5 0.0 0.5 1.0
Action x Action x Action x Action x
start end samples chosen for training
Figure2: Demonstratingtheimpactofreplaybuffersize,asaproxyforoff-policyness,onthepolicy
learningwithonlineMADDPG.Weshowthelearningtrajectoryinpolicyspace(top),thelearningas
therewardovertime(middle),andthestateofthereplaybufferinthefinaltrainingupdate(bottom).
Weseethatincreasingthebuffersizeleadstolessoptimaltrajectoriesbeinglearnt,duetothepresence
ofthestaledatainthereplaybuffer. WiththeBRUDupdate,wecanseethatitisimportantforthe
sampledjointactiontoremainfairlyclosetothecurrentjointpolicy,toavoidmiscoordination.
4
draweR
y
noitcA
y
noitcAThereisthusatensionbetweentheefficiencyandstabilityintroducedbyareplaybuffer,andthe
degreetowhichitispronetomiscoordination. However,wenoticethatwithonlineMADDPG,in
eachcase,westillfindtheoptimal,high-rewardregionseventually. Thatis,somemiscoordination
remainspossibleduringtraining,butthroughexplorationandaddingfreshdatatothebuffer,online
learningcannonethelessrecovergoodperformance.
Offlinelearningdoesnotexperiencethesamesuccess. ConsiderFigure3,whichshowsanexample
oflearningfromastaticdataset,sampleduniformlyasB ∼U(−1,1). Thedataitself,depictedin
Figure3a,hasasmallbias,(a¯ ,a¯ )=(−0.02,0.04). Recallthatinthissign-agreementgame,the
x y
BRUDupdateinducesaunidirectionalvectorfield∇J =(a¯ ,a¯ ),incontrasttothetruevectorfield
y x
∇R=(a ,a ). ThesetwofieldsarevisualisedinFigure3b,withallvectorsin∇J pointingtothe
x y
bottomright,sincea¯ >0anda¯ <0. TheresultingtrajectoriesofofflinelearningwithMADDPG,
y x
startingfromthreeseparatepolicyinitialisations,areshowninFigure3c. Noticehowthenatureof
thestaticdatasetusedfortrainingcompletelydeterminesthedirectionofpolicyupdate—which,in
thiscase,isinacompletelyincorrectdirection,towardsalow-rewardregion.
True Field Induced Field
1.00 0.75 0.50 0.25Re0w.0a0rd 0.25 0.50 0.75 1.00 1.00 0.75 0.50 0.25Re0w.0a0rd 0.25 0.50 0.75 1.00
1.00 1.00
0.75 0.75 1.00
0.50 0.50 0.75 start
0.25 0.25 0.50
0 0. .0 20
5
0 0. .0 20
5
00 .. 02 05 start end
0.50 0.50 0.25 start
0.50
0.75 0.75 end
0.75
1.00 1.00 end
1.00 0.75 0.50 0.25 Ac0 t. i0 on0 x0.25 0.50 0.75 1.00 1.00 0.75 0.50 0.25 Ac0 t. i0 on0 x0.25 0.50 0.75 1.00 1.001.00 0.75 0.50 0.25Ac0t.i0on0 x0.25 0.50 0.75 1.00
(a)Dataset,a¯=(−0.02,0.04) (b)∇R,∇J (c)Policylearningtrajectories
Figure3: TheresultsofusingauniformdatasetBforofflineMADDPGpolicylearning,inthesign-
agreementgame,R(a ,a )=a a . Weseethatthenetdirectionofpolicylearningispredetermined
x y x y
bythemeanofthedataset,duetotheBRUDapproach,regardlessofthepolicyinitialisation.
3.2 GrowingRiskofMiscoordinationwithIncreasedAgentInteraction
ThoughmiscoordinationcanindeedbedemonstratedinthegameR(a ,a )=a a ,theproblemcan
x y x y
bemitigatedthroughthechoiceofdataset. Forexample,bybiasingthedatasettohavesamplemeans
a¯ >0anda¯ >0,somethingclosetotheoptimaltrajectorycanbefound,sincetheagentswillmove
x y
towardsahigh-rewardregion,++. TotrulyunderstandtheseverityofBRUDinofflinecontexts,we
mustlooktomorecomplexgames. Wepresentfourgamesofincreasingcomplexitybelow,showing
howahigherdegreeofagentinteractionleadstohigherdegreesofpotentialmiscoordination.
DecoupledRewards: R =a +a . Forcompleteness,consideratrivialcasewheretheshared
x y
reward yielded to agents is simply the sum of their actions. Here, ∇R = ∇J = (1,1). Agents
mustsimplymaketheiractionsbiggertoyieldhigherrewards. Thecomponentsoftherewardsare
completelydecoupled,andnomiscoordinationoccurs,regardlessofthedatasetusedforlearning.
Sign Agreement: R = a a . As discussed before, the update in this game moves the agents
x y
in the direction of their teammate’s average action in the batch, ∇J = (a¯ ,a¯ ). If the dataset
y x
actionshappenedtobebiasedsuchthatsign(a¯ ) = sign(a¯ ),thenthepolicieswillmovetowards
x y
ahigh-rewardregion. However,ifthesignsdiffer,thepolicieswillmovetowardsthelow-reward
region. Becausethereisonlyasingle,simpleinteractionterm,thereisonlyaminorrequirementof
thedatasetforsuccessfulofflinelearning.
Action Agreement: R = −(a − a )2. This game requires agents to take identical actions
x y
for optimal coordination, with anything else yielding R < 0. The true gradient field, ∇R =
(2a −2a ,2a −2a ),impliesalineofoptima,∇R = 0 ⇐⇒ a = a . Incontrast,underthe
y x x y x y
dataset,thefieldis∇J = (2a¯ −2a ,2a¯ −2a ),resultinginasingleoptimum,∇J = 0atthe
y x x y
point(a¯ ,a¯ ). Noteitisnolongertheagentsmovinginthedirectionofthemeansofthedataset
y x
actions,butinsteadthatthelearningwillconvergetothispoint. Therequirementforoptimallearning
5
y noitcA y noitcA y noitcAisthusnolongersolelybasedonthesignsofthemeanactions,butthatx¯=y¯inthedataset,whichis
astrongrequirement.
TwinPeaks: R=−A(a2 +a2)−B(a a )2+Ca a , {A>0,B >0,C >2A}. Finally,we
x y x y x y
studyasetofpolynomialgamesofahigherdegree,allowingformoreinteractionterms. Forbrevity,
our treatment is presented for agent x, but all statements apply symmetrically to agent y, as the
functionitselfissymmetricintheagent’sactions. Thesurfacehastwopeaks, withtruemaxima
(cid:112)
ata† =± (C−2A)/2B. Mostinterestinginthispolynomialisthebivariatequarticinteraction,
x
(a a )2, sinceitisoptimisedwithBRUDasE [∇ a2a2] = 2a (a¯2 +σ2), whereσ2 isthe
x y ay∼B ax x y x y y y
samplevarianceofthey actionsinB. Thus,weseethattheoutcomeofofflinelearningdepends
notonlyonthedata’ssamplemeanbutalsoonitsspread. Indeed,wecanderivetwointeresting
relationshipswhenlearningofflineinthisgame. Firstly,forlearningtoconvergetothetrueoptimum,
wehave
(cid:118)
(cid:117) (cid:32) (cid:33)
a∗
x
=a†
x
⇐⇒ σ y(a¯ y)=(cid:117) (cid:116)−a¯ y2±
(cid:112)
2B(CC
−2A)
a¯ y− BA , (5)
whichsaysthedataset’sstandarddeviationmustbeafunctionofitsmean. Noticethenifwecentred
(cid:112)
thedatasetaroundtheorigin,suchthata¯ = 0,thenσ = −A/B,whichisimaginary. Hence,
y y
thereexistsnodistributionofdatathatenableslearningthetrueoptimuminthisgame,usingoffline
BRUD,ifthedatasetiscentredaround(0,0)—evenifthedatasetisinfinitelylarge. Wevalidatethis
resultempiricallyinFigure4a,showingthatincreasingthevarianceofthedatadoesnothelpthe
learningsucceed,fortheconvergedpolicyisalwayssimply(0,0).
Secondly,theexpressionfortheconvergedlearntpolicyis,
Ca¯
∇J =0 ⇐⇒ a∗ = y .
x 2A+2B(a¯2+σ2)
y y
Thisexpressioncorroboratesthepreviousresult,showingthatanorigin-centreddatasetwillalways
convergetothepolicya∗ = 0. Supposewenowcentrethedataexactlyaroundthetrueoptimum
x
instead,a¯ = a†. Undersuchconditions,learningwillconvergetotheoptimalpolicyonlywhen
y y
σ2 =0(thatis,wemustsolelyhaveoptimaldatainthedataset,withnospread);butasσ2 →∞,then
y y
a∗ →0,whichisincreasinglyfarawayfromthetrueoptimum. Thisresultisvalidatedempiricallyin
x
Figure4b. Perhapscounter-intuitively,wethusseethatincreasingdiversityinthedatasetactionscan
leadtoworseningperformancewhenlearningofflineinthisgame.
Learning trajectory Learning trajectory
1.0 1.0
0.5 start start start 0.5 start end start end start
0.0 end end end 0.0 end
0.5 0.5
1.0 1.0
1.0 0.5 0.0 0.5 1.0 1.0 0.5 0.0 0.5 1.0 1.0 0.5 0.0 0.5 1.0 1.0 0.5 0.0 0.5 1.0 1.0 0.5 0.0 0.5 1.0 1.0 0.5 0.0 0.5 1.0
Dataset Dataset
1.0 1.0
0.5 0.5
0.0 0.0
0.5 0.5
1.0 1.0
1.0 0.5 0.0 0.5 1.0 1.0 0.5 0.0 0.5 1.0 1.0 0.5 0.0 0.5 1.0 1.0 0.5 0.0 0.5 1.0 1.0 0.5 0.0 0.5 1.0 1.0 0.5 0.0 0.5 1.0
=0 =0.3 =1 =0 =0.3 =1
Action x Action x
(a)Origin-centreddataset,a¯=0 (b)Optimum-centreddataset,a¯=a†
Figure4: VisualisationsfromtheTwinPeaksgame(withA=1,B =4,C =5). Weseethatwithan
origin-centreddataset(4a),offlineBRUDlearningcannotfindthetruepolicyoptimum,regardless
ofthedatasetvariance,alwayssimplyconvergingtotheorigin. Withanoptimum-centreddataset
(4b),optimalityinthelearntpolicyisonlyfoundifthevarianceiszero. Asthevarianceincreases,
thelearntpolicymovesawayfromthetrueoptimumandtowardstheorigin. Theseempiricalresults
validatetheanalyticalsolutions.
Remark Thesepolynomialgamesindicateaclearrelationship: asthedegreeofagentinteraction
increases,therequirementsofthedatasetbecomemorestringentforlearningtoconvergetothetrue
optimumwithBRUD.Asaresult,thepossibilityofmiscoordinationincreases.
6
y noitcA y noitcA4 ProximalJoint-ActionPrioritisationforOfflineLearning
WehaveseenthattheBRUDapproachtopolicylearningishighlysusceptibletocoordinationfailure
intheofflinesetting. Nonetheless,BRUDremainsusefulforofflinelearning,sinceitallowsusto
staytightlycoupledtothedataset,whichistheonlysignalavailable. However,simplyupdatingeach
agent’spolicyinresponsetothecurrentjointpolicylearnedfromthedatahasbeenshownempirically
toworkpoorlyinofflineMARL(Shaoetal.,2023).
OuranalysisinSection3highlightsthatwhichdataissampledwhencouldmakeacriticaldifference
intheutilityofbestresponseupdates.Forinstance,thekeydifferencebetweenthesuccessfullearning
inFigure2,andthecoordinationfailureinFigure3,relatestothesimilaritybetweenthecurrentjoint
policy,θ,andthejointactionusedforthepolicyupdate,a∼B. Infact,theproblemillustratedin
Figure1wouldnothaveoccurredifthedatapoint, a , wasinthesamequadrantasthecurrent
(t)
policy.
Therefore,inthiswork,weadvocateforprioritiseddatasetsamplingmethodsasapromisingarea
forinnovationtoimprovelearninginofflineMARL.Furthermore,weconsidersamplingmethods
asan“orthogonal”axistoothereffectiveapproachesforofflinelearningsuchascriticandpolicy
regularisation, where it can easily be combined with these methods to potentially great effect.
As a way of demonstrating a preliminary instantiation of this idea, we propose Proximal Joint-
ActionPrioritisation(PJAP)asaclassofofflinesamplingmethods. InPJAP,prioritisedexperience
replay(Schauletal.,2016)isusedtoincreasetheprobabilityofsamplingactionsthatweregenerated
bypoliciesresemblingthecurrentjointpolicy,withprioritiesdefinedproportionaltosomesimilarity
metric.
Conceptually,foreachtrajectory,τ,inadataset,B,wemodelanunderlyingjointdataset-generating
policy,β . Notethatagivendatasetmaycomprisevariousdistinctdataset-generatingpolicies—e.g.
τ
whenthedatasethastrajectorieswithbothlowandhighreturns,recordedoveranonlinetraining
procedure. Wedenotethecurrentlearntjointpolicyafterk updatesasµ . InPJAP,wesetthe
(k)
priority,ρ ,foreachofthetrajectoriesτ ∼B,tobeinverselyproportionaltosomefunctionofthe
k+1
distancebetweenthecurrentjointpolicyandthedataset-generatingpolicy,d(µ ,β ).
(k) τ
AsaspecificinstanceofPJAP,weproposetransformingthedistanceonaGaussian,e−αd2,whereα
controlshowrapidlytheprioritiesdecreasewithrespecttothedistance. Underthistransformation,
weensuresmalldistancesyieldsimilar,largepriorities,whereaslargerdistancesyieldexponentially
smallerpriorities.Wealsocliptheminimumprioritytosomesmallvalueϵ>0,whichavoidsmaking
certainsamplessounlikelythattheyareeffectivelyneverseenagain. Thisparameter,ϵ,isthusakin
tocontrolling“exploration”ofthedataset,whereveryoccasionallywewanttosampledatathatis,in
fact,quitedifferenttoourcurrentjointpolicy. Insummary,ourinstanceofPJAPtakesthefollowing
prioritisationprocedure,
(cid:104) (cid:105)
PJAP
N(ϵ)
: ρ k+1(τ)=max
e−αd(µk,βτ)2
,ϵ , (6)
Wenotethatinpractice,therearethreekeychallengeswhenimplementingPJAP.Firstly,wetypically
donothaveaccesstothedataset-generatingpolicyitself,β . Thusinthiswork,weusethesampled
τ
actionsasaproxyforthepolicythatgeneratedthem,andcomparethemtotheactionstakenbythe
agentsunderthesampledobservations. Secondly,itiscomputationallyunrealistictorecomputethe
prioritiesforalltrajectoriesinthedatasetateachupdatestep. Therefore,wefixthisbybootstrapping
thepriorityupdates—updatingonlyasubsetofsamplesatatime.Thirdly,weconcedethatcomingup
withagooddistancemeasureforaparticularproblemcanbetricky,especiallyinhigher-dimensional
actionspaces.
Wenowdemonstrateourapproachinthecaseofdeterministicpolicies(e.g.MADDPG),andpresent
differentimplementationsofPJAP usingcontext-specificdistancemeasures. Firstinthecontext
N(ϵ)
of polynomial games, and then in the following section, a more complex MARL setting from
MAMuJoCo(Pengetal.,2021). Wenotethatdevelopinggenerallyperformant“context-agnostic”
distancemeasuresexistsasafruitfulareaforfuturework.
4.1 PJAPinPolynomialGames
Althoughthedatasetsgeneratedforthepolynomialgamearequitesmall,weusetheL1normasa
distancemetricsinceideallyourchosenmetricshouldbeabletoprioritisefewsamplesfromvery
7largedatasets. AnotheradvantageoftheL1normisthatisitcomputationallyinexpensivewhen
comparedtootherdistanceandsimilaritymetricssuchastheL2normandcosinesimilarity.Atstepk,
withcurrentpolicyparameters, θ , weupdatetheprioritiesforeachsampleaˆ = (aˆ ,aˆ ) ∼ B
(k) x y
usingthePJAPformulationfromEquation6,andthedistancemeasure,
d(µ ,β )≜∥aˆ,θ ∥ . (7)
k τ (k) 1
Mean distance between training
Dataset Trajectory samples and current policy Sample efficiency
1.00 1.00
0.6 MADDPG+PJAP 0.5
0.75 0.75 MADDPG
0.50 0.50 0.5 0.0
0.25 0.25 0.4 0.5
0.00 0.00 0.3
0.25 0.25 0.2 1.0
00 .. 75 50 00 .. 75 50 0.1 1.5 MADDPG+PJAP
0.0 2.0 MADDPG
1.00 1.00
1.0 0.5 0.0 0.5 1.0 1.0 0.5 0.0 0.5 1.0 0 20 40 60 80 100 0 20 40 60 80 100
Epochs Epochs
1.00 1.00
MADDPG+PJAP 0.5
0.75 0.75 0.6 MADDPG
0.50 0.50 0.5 0.0
0.25 0.25 0.4 0.5
0.00 0.00
0.25 0.25 0.3 1.0
0.50 0.50 0.2 1.5
0.75 0.75 0.1
2.0
M MA AD DD DP PG G+PJAP
1.00 1.00
1.0 0.5 0.0 0.5 1.0 1.0 0.5 0.0 0.5 1.0 0 20 40 60 80 100 0 20 40 60 80 100
Epochs Epochs
1.00 1.00 0.8
MADDPG+PJAP 0.5
0.75 0.75 0.7 MADDPG
0.50 0.50 0.6 0.0
0.25 0.25 0.5 0.5
0.00 0.00
0.4
0.25 0.25 1.0
0.3
0.50 0.50 1.5
0.75 0.75 0.2 MADDPG+PJAP
0.1 2.0 MADDPG
1.00 1.00
1.0 0.5 0.0 0.5 1.0 1.0 0.5 0.0 0.5 1.0 0 20 40 60 80 100 0 20 40 60 80 100
Epochs Epochs
1.00 1.00
0.5
0.75 0.75 1.0
0.50 0.50 0.8 0.0
0.25 0.25 0.00 0.00 0.6 M MA AD DD DP PG G+PJAP 0.5
0.25 0.25 0.4 1.0
0.50 0.50 1.5
0.75 0.75 0.2 MADDPG+PJAP
2.0 MADDPG
1.00 1.00
1.0 0.5 0.0 0.5 1.0 1.0 0.5 0.0 0.5 1.0 0 20 40 60 80 100 0 20 40 60 80 100
MADDPG+PJAP MADDPG Epochs Epochs
Figure 5: Results of using PJAP with MADDPG in the Twin Peaks game, fixing the problem
previously seen in Figure 4b. Each row uses a specific dataset: all centred on the true optimum,
butwithincreasingvariances,showninthefirstcolumn. Thecorrespondingtrajectoriesofusing
MADDPGwithandwithoutPJAPareshowninthesecondcolumn. Thethirdcolumnshowshow
usingPJAPlowersthemeandistancebetweensampleddataandthecurrentpolicy,whichenables
convergencetohigherperformance,seeninthefourthcolumn.
Figure5illustrateshowthefailuremodeseeninFigure4bcanbeaddressedbyouruseofPJAP.
WhereasofflineMADDPGusingBRUDfailstoconvergeuponthecorrectsolution(unlessσ =0),
MADDPGwithPJAPfindsthesolutionconsistentlyacrossthevariousdatasets.
4.2 PJAPinMAMuJoCo
Next,weconsiderhowtoimplementPJAPinahigher-dimensionalsetting,namely2-AgentHalfChee-
tahfromMAMuJoCo(Pengetal.,2021). Here,theenvironmentisnolongerstatelessandpolicies
areconditionedonobservationsthatchangeduringanepisode. Therefore,itisnotimmediatelyclear
whatasuitabledistancemetriccanbetomeasuretheproximitybetweenthecurrentlearntdeterminis-
ticpolicyandthebehaviourpoliciesforeachagent. Inourexperiments,weexplorewithusingtheL1
8
ecnatsiD
ecnatsiD
ecnatsiD
ecnatsiD
draweR
draweR
draweR
draweRdistancebetweentheactionsfromthecurrentlearntpolicyµ andthesequenceofactionsinagiven
k
trajectory. Thatis,foragiventrajectoryτ ∼D,weconsiderthesequenceofobservationsandactions
whichmakeupthetrajectoryτ ={((o1,...,on),(a1,...,an)),...,((o1,...,on),(a1,...,an))}.
t t t t T T T T
Each ai in the trajectory τ comes from the unknown behaviour policy β . Thus, the average L1
t τ
distancebetweenactionai andµi(oi)canbeseenasanapproximatedistancemetricforPJAP.
t k k
N T
1 (cid:88)(cid:88)
d(µ ,β )≜ ∥µ(o ),a ∥ . (8)
k τ N ·T i(t) i(t) 1
i t
We use the MADDPG+CQL implementation from OG-MARL (Formanek et al., 2023) as our
baselineandthencompareittoaversionofMADDPG+CQLwhereweincorporatePJAPusingthe
distancemetricinEquation8andtheprioritisationfunctioninEquation6. Weevaluateourmethod
againstthebaselineontwodifferentdatasetsfromthe2-Agent HalfCheetahenvironment. The
resultsfromtheexperimentsaregiveninFigure6. Theaveragedistancebetweenactionssampled
fromthedatasetandthecurrentlearntpolicyisalsogiventohighlightthatPJAPreducesthisdistance
ascomparedtothebaseline,whichcorroboratesthefindingsfromthepolynomialgameexperiment.
AllhyperparametersbetweenthebaselineandourvariantwithPJAParekeptconstantandhavebeen
includedintheattachedcode.
0.18
0.16
0.14
Median IQM Mean 0.12
MADDPG+CQL 0.10
MADDPG+CQL+PJAP 0.08
0.06
0.6 0.8 1.0 0.6 0.8 1.0 0.6 0.8 1.0 0.04
Normalized episode return 0.02
(a)PerformanceonGooddataset. 0.00 0 1 2 Trainer Updates 3 4 5×105
(b) Mean distance plot for the Good
datasetexperiment.
0.18
0.16
0.14
Median IQM Mean 0.12
MADDPG+CQL 0.10
MADDPG+CQL+PJAP 0.08
0.06
0.72 0.80 0.88 0.72 0.80 0.88 0.72 0.80 0.88 0.04
Normalized episode return 0.02
(c)PerformanceonGood-Mediumdataset. 0.00 0 1 2 Trainer Updates 3 4 5×105
(d) Mean distance plot for the
Good-Mediumdatasetexperiment.
Figure 6: We compare the performance of MADDPG+CQL on 2-Agent HalfCheetah with and
withoutPJAP.WeexperimentontheGoodandGoodMediumdatasetsfromOG-MARL(Formanek
etal.,2023),showingthefinalperformanceplots(Figures6aand6c)after500ktrainingsteps,with
bootstrapconfidenceintervalsover10independentrandomseeds(Agarwaletal.,2022;Gorsane
etal.,2022). Wealsoshowtheaveragedistance(asdefinedinSection4.2)betweenactionssampled
fromthedatasetandthecurrentlearntpolicy(Figures6band6d).
5 RelatedWork
NotableprogresshasbeenmadeinthefieldofofflineMARLinrecentyears. JiangandLu(2021)
highlightedthatinofflineMARLthetransitiondynamicsinthedatasetcansignificantlydifferfrom
thoseofthelearnedpolicies,leadingtocoordinationfailures. Theyaddressthisbynormalisingthe
transitionprobabilitiesinthedataset. Yangetal.(2021)highlightandaddresstherapidaccumulation
ofextrapolationerrorduetoOut-of-DistributionactionsinOfflineMARL.Panetal.(2022)showthat
MADDPGandIndependentDDPGpolicygradientsstruggletooptimiseconservativeQ-functions
(Kumar et al., 2020), and propose using a zeroth-order optimisation to learn more coordinated
behaviour. Shaoetal.(2023)alsohighlightthelimitationsofMADDPG+CQLandproposeaper-
agentCQLregularisationthatscalesbetterinthenumberofagents. Wangetal.(2023)alsoexplored
anovelapproachtoregularisingthevaluefunctioninOfflineMARL.Tianetal.(2023)consideran
imbalanceofagentexpertiseinadataset,whichcancontaminatetheofflinelearningofallagents.
They address this problem by learning decomposed rewards, and then reconstructing the dataset
whilefavouringhigh-returnindividualtrajectories. Finally,workbyCuiandDu(2022)addressesthe
9
hctaB delpmaS
ni ecnatsiD
ecneuqeS
naeM
hctaB delpmaS
ni ecnatsiD
ecneuqeS
naeMadditionalfundamentaldifficultiesofsolvingmulti-agentlearningproblemsusingastaticdataset.
Theauthorsshowthatthedatasetrequirementsforthesolvabilityofatwo-playerzero-sumMarkov
gamearestricterthanforasingle-strategysetting.
6 Discussion
Inthispaper,weusesimpletwo-playerpolynomialgamestohighlightandstudythefundamental
problemofmiscoordinationinofflineMARL,whenusingaBestResponseUnderData(BRUD)
approachtopolicylearning.Buildingonouranalyses,weproposeProximalJointActionPrioritisation
(PJAP),wheresampledexperiencedataisprioritisedasafunctionofthecurrentjointpolicybeing
learnt.WeinstantiateaninstanceofPJAP,anddemonstratehowitcansolvemiscoordinationproblems
inboththesimplifiedpolynomialgamecase,andinthemorecomplexMARLsettingofMAMuJoco.
Importantly, though, our work primarily aims to be a catalyst for further development of dataset
samplingprioritisationmethods,asonetoolinourofflineMARLtoolkit. Thistoolexistsalongside
other offline MARL remedies, such as critic and policy regularisation, all helping mitigate the
difficultiesofofflinelearning,together. WebelievethatPJAPpavesthewayforinterestingresearch
ideasinofflineMARL.
Limitations Thispaperprimarilyfocusesontheoreticalcontributionsandinsightsinsimplified
settings,usingpolynomialgamesasabackbone. Thoughusefulasaninterpretableandaccessible
tool,thecontextisadmittedlylimitedinseveralways—itisstateless,comprisesjusttwoagentsand
assumesperfectknowledgeoftherewardsurface.Weacknowledgethattheselimitationsconstrainthe
generalityofourconclusions,evenwhensupportedwithmorecomplexempiricalresults. However,
remain confident that our approach takes an important step to improving our understanding of
coordinationinofflineMARL.
10References
R.Agarwal,M.Schwarzer,P.S.Castro,A.Courville,andM.G.Bellemare. Deepreinforcement
learningattheedgeofthestatisticalprecipice,2022. 9
P. Barde, J. Foerster, D. Nowrouzezahrai, and A. Zhang. A model-based solution to the offline
multi-agentreinforcementlearningcoordinationproblem. Proceedingsofthe23rdInternational
ConferenceonAutonomousAgentsandMultiagentSystems,2024. 1
Q.CuiandS.S.Du. Whenareofflinetwo-playerzero-summarkovgamessolvable? Advancesin
NeuralInformationProcessingSystems,35:25779–25791,2022. 9
M.Dresher,S.Karlin,andL.S.Shapley. Polynomialgames. ContributionstotheTheoryofGamesI,
1950. 3
C.Formanek,A.Jeewa,J.Shock,andA.Pretorius. Off-the-gridmarl: Datasetsandbaselinesfor
offlinemulti-agentreinforcementlearning.InProceedingsofthe2023InternationalConferenceon
AutonomousAgentsandMultiagentSystems,AAMAS’23,page2442–2444,Richland,SC,2023.
InternationalFoundationforAutonomousAgentsandMultiagentSystems. ISBN9781450394321.
9
R.Gorsane,O.Mahjoub,R.deKock,R.Dubb,S.Singh,andA.Pretorius. Towardsastandardised
performanceevaluationprotocolforcooperativemarl,2022. 9
J.JiangandZ.Lu. Offlinedecentralizedmulti-agentreinforcementlearning,2021. 9
A.Kumar,A.Zhou,G.Tucker,andS.Levine. Conservativeq-learningforofflinereinforcement
learning. AdvancesinNeuralInformationProcessingSystems,2020. 9
R.Lowe,Y.I.Wu,A.Tamar,J.Harb,P.Abbeel,andI.Mordatch. Multi-agentactor-criticformixed
cooperative-competitiveenvironments. Advancesinneuralinformationprocessingsystems,2017.
2,3
V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and M. Riedmiller.
Playingatariwithdeepreinforcementlearning. NIPSDeepLearningWorkshop,2013. 4
L.Pan,L.Huang,T.Ma,andH.Xu.Planbetteramidconservatism:Offlinemulti-agentreinforcement
learningwithactorrectification. InInternationalconferenceonmachinelearning,pages17221–
17237.PMLR,2022. 2,9
G.Papoudakis,F.Christianos,L.Schäfer,andS.V.Albrecht. BenchmarkingMulti-AgentDeep
ReinforcementLearningAlgorithmsinCooperativeTasks. ProceedingsoftheNeuralInformation
ProcessingSystemsTrackonDatasetsandBenchmarks,2021. 3
B.Peng,T.Rashid,C.A.S.deWitt,P.-A.Kamienny,P.H.S.Torr,W.Böhmer,andS.Whiteson.
Facmac: Factoredmulti-agentcentralisedpolicygradients,2021. 7,8
R.F.Prudencio,M.R.Maximo,andE.L.Colombini. Asurveyonofflinereinforcementlearning:
Taxonomy,review,andopenproblems. IEEETransactionsonNeuralNetworksandLearning
Systems,2023. 1
Y.Pu,S.Wang,R.Yang,X.Yao,andB.Li. Decomposedsoftactor-criticmethodforcooperative
multi-agentreinforcementlearning. ArXiv,abs/2104.06655,2021. 2
T.Rashid,M.Samvelyan,C.S.DeWitt,G.Farquhar,J.Foerster,andS.Whiteson. Monotonicvalue
functionfactorisationfordeepmulti-agentreinforcementlearning. JournalofMachineLearning
Research,21(178):1–51,2020. 3
T.Schaul, J.Quan, I.Antonoglou, andD.Silver. Prioritizedexperiencereplay. InInternational
ConferenceonLearningRepresentations,2016. 7
J.Shao, Y.Qu, C.Chen, H.Zhang, andX.Ji. Counterfactualconservativeqlearningforoffline
multi-agentreinforcementlearning. AdvancesinNeuralInformationProcessingSystems,37,2023.
2,3,7,9
Q. Tian, K. Kuang, F. Liu, and B. Wang. Learning from good trajectories in offline multi-agent
reinforcementlearning. ProceedingsoftheAAAIConferenceonArtificialIntelligence,37:11672–
11680,062023. doi: 10.1609/aaai.v37i10.26379. 9
X.Wang,H.Xu,Y.Zheng,andX.Zhan. Offlinemulti-agentreinforcementlearningwithimplicit
global-to-localvalueregularization. AdvancesinNeuralInformationProcessingSystems,37,2023.
9
11R.J.Williams. Simplestatisticalgradient-followingalgorithmsforconnectionistreinforcementlearn-
ing. MachineLearning,8(3):229–256,May1992. ISSN1573-0565. doi: 10.1007/BF00992696.
4
Y.Yang,X.Ma,C.Li,Z.Zheng,Q.Zhang,G.Huang,J.Yang,andQ.Zhao. Believewhatyousee:
Implicitconstraintapproachforofflinemulti-agentreinforcementlearning,2021. 9
Y.Zhong,J.G.Kuba,X.Feng,S.Hu,J.Ji,andY.Yang.Heterogeneous-agentreinforcementlearning.
JournalofMachineLearningResearch,25:1–67,2024. 3
12