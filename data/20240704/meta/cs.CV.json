[
    {
        "title": "Magic Insert: Style-Aware Drag-and-Drop",
        "authors": "Nataniel RuizYuanzhen LiNeal WadhwaYael PritchMichael RubinsteinDavid E. JacobsShlomi Fruchter",
        "links": "http://arxiv.org/abs/2407.02489v1",
        "entry_id": "http://arxiv.org/abs/2407.02489v1",
        "pdf_url": "http://arxiv.org/pdf/2407.02489v1",
        "summary": "We present Magic Insert, a method for dragging-and-dropping subjects from a\nuser-provided image into a target image of a different style in a physically\nplausible manner while matching the style of the target image. This work\nformalizes the problem of style-aware drag-and-drop and presents a method for\ntackling it by addressing two sub-problems: style-aware personalization and\nrealistic object insertion in stylized images. For style-aware personalization,\nour method first fine-tunes a pretrained text-to-image diffusion model using\nLoRA and learned text tokens on the subject image, and then infuses it with a\nCLIP representation of the target style. For object insertion, we use\nBootstrapped Domain Adaption to adapt a domain-specific photorealistic object\ninsertion model to the domain of diverse artistic styles. Overall, the method\nsignificantly outperforms traditional approaches such as inpainting. Finally,\nwe present a dataset, SubjectPlop, to facilitate evaluation and future progress\nin this area. Project page: https://magicinsert.github.io/",
        "updated": "2024-07-02 17:59:50 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.02489v1"
    },
    {
        "title": "Characterizing the Interpretability of Attention Maps in Digital Pathology",
        "authors": "Tomé AlbuquerqueAnil YüceMarkus D. HerrmannAlvaro Gomariz",
        "links": "http://arxiv.org/abs/2407.02484v1",
        "entry_id": "http://arxiv.org/abs/2407.02484v1",
        "pdf_url": "http://arxiv.org/pdf/2407.02484v1",
        "summary": "Interpreting machine learning model decisions is crucial for high-risk\napplications like healthcare. In digital pathology, large whole slide images\n(WSIs) are decomposed into smaller tiles and tile-derived features are\nprocessed by attention-based multiple instance learning (ABMIL) models to\npredict WSI-level labels. These networks generate tile-specific attention\nweights, which can be visualized as attention maps for interpretability.\nHowever, a standardized evaluation framework for these maps is lacking,\nquestioning their reliability and ability to detect spurious correlations that\ncan mislead models. We herein propose a framework to assess the ability of\nattention networks to attend to relevant features in digital pathology by\ncreating artificial model confounders and using dedicated interpretability\nmetrics. Models are trained and evaluated on data with tile modifications\ncorrelated with WSI labels, enabling the analysis of model sensitivity to\nartificial confounders and the accuracy of attention maps in highlighting them.\nConfounders are introduced either through synthetic tile modifications or\nthrough tile ablations based on their specific image-based features, with the\nlatter being used to assess more clinically relevant scenarios. We also analyze\nthe impact of varying confounder quantities at both the tile and WSI levels.\nOur results show that ABMIL models perform as desired within our framework.\nWhile attention maps generally highlight relevant regions, their robustness is\naffected by the type and number of confounders. Our versatile framework has the\npotential to be used in the evaluation of various methods and the exploration\nof image-based features driving model predictions, which could aid in biomarker\ndiscovery.",
        "updated": "2024-07-02 17:58:58 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.02484v1"
    },
    {
        "title": "Boosting Consistency in Story Visualization with Rich-Contextual Conditional Diffusion Models",
        "authors": "Fei ShenHu YeSibo LiuJun ZhangCong WangXiao HanWei Yang",
        "links": "http://arxiv.org/abs/2407.02482v1",
        "entry_id": "http://arxiv.org/abs/2407.02482v1",
        "pdf_url": "http://arxiv.org/pdf/2407.02482v1",
        "summary": "Recent research showcases the considerable potential of conditional diffusion\nmodels for generating consistent stories. However, current methods, which\npredominantly generate stories in an autoregressive and excessively\ncaption-dependent manner, often underrate the contextual consistency and\nrelevance of frames during sequential generation. To address this, we propose a\nnovel Rich-contextual Conditional Diffusion Models (RCDMs), a two-stage\napproach designed to enhance story generation's semantic consistency and\ntemporal consistency. Specifically, in the first stage, the frame-prior\ntransformer diffusion model is presented to predict the frame semantic\nembedding of the unknown clip by aligning the semantic correlations between the\ncaptions and frames of the known clip. The second stage establishes a robust\nmodel with rich contextual conditions, including reference images of the known\nclip, the predicted frame semantic embedding of the unknown clip, and text\nembeddings of all captions. By jointly injecting these rich contextual\nconditions at the image and feature levels, RCDMs can generate semantic and\ntemporal consistency stories. Moreover, RCDMs can generate consistent stories\nwith a single forward inference compared to autoregressive models. Our\nqualitative and quantitative results demonstrate that our proposed RCDMs\noutperform in challenging scenarios. The code and model will be available at\nhttps://github.com/muzishen/RCDMs.",
        "updated": "2024-07-02 17:58:07 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.02482v1"
    },
    {
        "title": "Understanding Alignment in Multimodal LLMs: A Comprehensive Study",
        "authors": "Elmira AmirlooJean-Philippe FauconnierChristoph RoesmannChristian KerlRinu BoneyYusu QianZirui WangAfshin DehghanYinfei YangZhe GanPeter Grasch",
        "links": "http://arxiv.org/abs/2407.02477v1",
        "entry_id": "http://arxiv.org/abs/2407.02477v1",
        "pdf_url": "http://arxiv.org/pdf/2407.02477v1",
        "summary": "Preference alignment has become a crucial component in enhancing the\nperformance of Large Language Models (LLMs), yet its impact in Multimodal Large\nLanguage Models (MLLMs) remains comparatively underexplored. Similar to\nlanguage models, MLLMs for image understanding tasks encounter challenges like\nhallucination. In MLLMs, hallucination can occur not only by stating incorrect\nfacts but also by producing responses that are inconsistent with the image\ncontent. A primary objective of alignment for MLLMs is to encourage these\nmodels to align responses more closely with image information. Recently,\nmultiple works have introduced preference datasets for MLLMs and examined\ndifferent alignment methods, including Direct Preference Optimization (DPO) and\nProximal Policy Optimization (PPO). However, due to variations in datasets,\nbase model types, and alignment methods, it remains unclear which specific\nelements contribute most significantly to the reported improvements in these\nworks. In this paper, we independently analyze each aspect of preference\nalignment in MLLMs. We start by categorizing the alignment algorithms into two\ngroups, offline (such as DPO), and online (such as online-DPO), and show that\ncombining offline and online methods can improve the performance of the model\nin certain scenarios. We review a variety of published multimodal preference\ndatasets and discuss how the details of their construction impact model\nperformance. Based on these insights, we introduce a novel way of creating\nmultimodal preference data called Bias-Driven Hallucination Sampling (BDHS)\nthat needs neither additional annotation nor external models, and show that it\ncan achieve competitive performance to previously published alignment work for\nmultimodal models across a range of benchmarks.",
        "updated": "2024-07-02 17:55:03 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.02477v1"
    },
    {
        "title": "SUPER: Seated Upper Body Pose Estimation using mmWave Radars",
        "authors": "Bo ZhangZimeng ZhouBoyu JiangRong Zheng",
        "links": "http://dx.doi.org/10.1109/IoTDI61053.2024.00020",
        "entry_id": "http://arxiv.org/abs/2407.02455v1",
        "pdf_url": "http://arxiv.org/pdf/2407.02455v1",
        "summary": "In industrial countries, adults spend a considerable amount of time sedentary\neach day at work, driving and during activities of daily living. Characterizing\nthe seated upper body human poses using mmWave radars is an important, yet\nunder-studied topic with many applications in human-machine interaction,\ntransportation and road safety. In this work, we devise SUPER, a framework for\nseated upper body human pose estimation that utilizes dual-mmWave radars in\nclose proximity. A novel masking algorithm is proposed to coherently fuse data\nfrom the radars to generate intensity and Doppler point clouds with\ncomplementary information for high-motion but small radar cross section areas\n(e.g., upper extremities) and low-motion but large RCS areas (e.g. torso). A\nlightweight neural network extracts both global and local features of upper\nbody and output pose parameters for the Skinned Multi-Person Linear (SMPL)\nmodel. Extensive leave-one-subject-out experiments on various motion sequences\nfrom multiple subjects show that SUPER outperforms a state-of-the-art baseline\nmethod by 30 -- 184%. We also demonstrate its utility in a simple downstream\ntask for hand-object interaction.",
        "updated": "2024-07-02 17:32:34 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.02455v1"
    }
]