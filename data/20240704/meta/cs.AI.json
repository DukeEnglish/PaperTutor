[
    {
        "title": "Magic Insert: Style-Aware Drag-and-Drop",
        "authors": "Nataniel RuizYuanzhen LiNeal WadhwaYael PritchMichael RubinsteinDavid E. JacobsShlomi Fruchter",
        "links": "http://arxiv.org/abs/2407.02489v1",
        "entry_id": "http://arxiv.org/abs/2407.02489v1",
        "pdf_url": "http://arxiv.org/pdf/2407.02489v1",
        "summary": "We present Magic Insert, a method for dragging-and-dropping subjects from a\nuser-provided image into a target image of a different style in a physically\nplausible manner while matching the style of the target image. This work\nformalizes the problem of style-aware drag-and-drop and presents a method for\ntackling it by addressing two sub-problems: style-aware personalization and\nrealistic object insertion in stylized images. For style-aware personalization,\nour method first fine-tunes a pretrained text-to-image diffusion model using\nLoRA and learned text tokens on the subject image, and then infuses it with a\nCLIP representation of the target style. For object insertion, we use\nBootstrapped Domain Adaption to adapt a domain-specific photorealistic object\ninsertion model to the domain of diverse artistic styles. Overall, the method\nsignificantly outperforms traditional approaches such as inpainting. Finally,\nwe present a dataset, SubjectPlop, to facilitate evaluation and future progress\nin this area. Project page: https://magicinsert.github.io/",
        "updated": "2024-07-02 17:59:50 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.02489v1"
    },
    {
        "title": "Neurocache: Efficient Vector Retrieval for Long-range Language Modeling",
        "authors": "Ali SafayaDeniz Yuret",
        "links": "http://arxiv.org/abs/2407.02486v1",
        "entry_id": "http://arxiv.org/abs/2407.02486v1",
        "pdf_url": "http://arxiv.org/pdf/2407.02486v1",
        "summary": "This paper introduces Neurocache, an approach to extend the effective context\nsize of large language models (LLMs) using an external vector cache to store\nits past states. Like recent vector retrieval approaches, Neurocache uses an\nefficient k-nearest-neighbor (kNN) algorithm to retrieve relevant past states\nand incorporate them into the attention process. Neurocache improves upon\nprevious methods by (1) storing compressed states, which reduces cache size;\n(2) performing a single retrieval operation per token which increases inference\nspeed; and (3) extending the retrieval window to neighboring states, which\nimproves both language modeling and downstream task accuracy. Our experiments\nshow the effectiveness of Neurocache both for models trained from scratch and\nfor pre-trained models such as Llama2-7B and Mistral-7B when enhanced with the\ncache mechanism. We also compare Neurocache with text retrieval methods and\nshow improvements in single-document question-answering and few-shot learning\ntasks. We made the source code available under:\nhttps://github.com/alisafaya/neurocache",
        "updated": "2024-07-02 17:59:29 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.02486v1"
    },
    {
        "title": "RankRAG: Unifying Context Ranking with Retrieval-Augmented Generation in LLMs",
        "authors": "Yue YuWei PingZihan LiuBoxin WangJiaxuan YouChao ZhangMohammad ShoeybiBryan Catanzaro",
        "links": "http://arxiv.org/abs/2407.02485v1",
        "entry_id": "http://arxiv.org/abs/2407.02485v1",
        "pdf_url": "http://arxiv.org/pdf/2407.02485v1",
        "summary": "Large language models (LLMs) typically utilize the top-k contexts from a\nretriever in retrieval-augmented generation (RAG). In this work, we propose a\nnovel instruction fine-tuning framework RankRAG, which instruction-tunes a\nsingle LLM for the dual purpose of context ranking and answer generation in\nRAG. In particular, the instruction-tuned LLMs work surprisingly well by adding\na small fraction of ranking data into the training blend, and outperform\nexisting expert ranking models, including the same LLM exclusively fine-tuned\non a large amount of ranking data. For generation, we compare our model with\nmany strong baselines, including GPT-4-0613, GPT-4-turbo-2024-0409, and\nChatQA-1.5, an open-sourced model with the state-of-the-art performance on RAG\nbenchmarks. Specifically, our Llama3-RankRAG significantly outperforms\nLlama3-ChatQA-1.5 and GPT-4 models on nine knowledge-intensive benchmarks. In\naddition, it also performs comparably to GPT-4 on five RAG benchmarks in the\nbiomedical domain without instruction fine-tuning on biomedical data,\ndemonstrating its superb capability for generalization to new domains.",
        "updated": "2024-07-02 17:59:17 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.02485v1"
    },
    {
        "title": "MMedAgent: Learning to Use Medical Tools with Multi-modal Agent",
        "authors": "Binxu LiTiankai YanYuanting PanZhe XuJie LuoRuiyang JiShilong LiuHaoyu DongZihao LinYixin Wang",
        "links": "http://arxiv.org/abs/2407.02483v1",
        "entry_id": "http://arxiv.org/abs/2407.02483v1",
        "pdf_url": "http://arxiv.org/pdf/2407.02483v1",
        "summary": "Multi-Modal Large Language Models (MLLMs), despite being successful, exhibit\nlimited generality and often fall short when compared to specialized models.\nRecently, LLM-based agents have been developed to address these challenges by\nselecting appropriate specialized models as tools based on user inputs.\nHowever, such advancements have not been extensively explored within the\nmedical domain. To bridge this gap, this paper introduces the first agent\nexplicitly designed for the medical field, named \\textbf{M}ulti-modal\n\\textbf{Med}ical \\textbf{Agent} (MMedAgent). We curate an instruction-tuning\ndataset comprising six medical tools solving seven tasks, enabling the agent to\nchoose the most suitable tools for a given task. Comprehensive experiments\ndemonstrate that MMedAgent achieves superior performance across a variety of\nmedical tasks compared to state-of-the-art open-source methods and even the\nclosed-source model, GPT-4o. Furthermore, MMedAgent exhibits efficiency in\nupdating and integrating new medical tools.",
        "updated": "2024-07-02 17:58:23 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.02483v1"
    },
    {
        "title": "Free Energy in a Circumplex Model of Emotion",
        "authors": "Candice PattisapuTim VerbelenRiddhi J. PitliyaAlex B. KieferMahault Albarracin",
        "links": "http://arxiv.org/abs/2407.02474v1",
        "entry_id": "http://arxiv.org/abs/2407.02474v1",
        "pdf_url": "http://arxiv.org/pdf/2407.02474v1",
        "summary": "Previous active inference accounts of emotion translate fluctuations in free\nenergy to a sense of emotion, mainly focusing on valence. However, in affective\nscience, emotions are often represented as multi-dimensional. In this paper, we\npropose to adopt a Circumplex Model of emotion by mapping emotions into a\ntwo-dimensional spectrum of valence and arousal. We show how one can derive a\nvalence and arousal signal from an agent's expected free energy, relating\narousal to the entropy of posterior beliefs and valence to utility less\nexpected utility. Under this formulation, we simulate artificial agents engaged\nin a search task. We show that the manipulation of priors and object presence\nresults in commonsense variability in emotional states.",
        "updated": "2024-07-02 17:52:25 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.02474v1"
    }
]