[
    {
        "title": "MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention",
        "authors": "Huiqiang JiangYucheng LiChengruidong ZhangQianhui WuXufang LuoSurin AhnZhenhua HanAmir H. AbdiDongsheng LiChin-Yew LinYuqing YangLili Qiu",
        "links": "http://arxiv.org/abs/2407.02490v1",
        "entry_id": "http://arxiv.org/abs/2407.02490v1",
        "pdf_url": "http://arxiv.org/pdf/2407.02490v1",
        "summary": "The computational challenges of Large Language Model (LLM) inference remain a\nsignificant barrier to their widespread deployment, especially as prompt\nlengths continue to increase. Due to the quadratic complexity of the attention\ncomputation, it takes 30 minutes for an 8B LLM to process a prompt of 1M tokens\n(i.e., the pre-filling stage) on a single A100 GPU. Existing methods for\nspeeding up prefilling often fail to maintain acceptable accuracy or efficiency\nwhen applied to long-context LLMs. To address this gap, we introduce MInference\n(Milliontokens Inference), a sparse calculation method designed to accelerate\npre-filling of long-sequence processing. Specifically, we identify three unique\npatterns in long-context attention matrices-the A-shape, Vertical-Slash, and\nBlock-Sparsethat can be leveraged for efficient sparse computation on GPUs. We\ndetermine the optimal pattern for each attention head offline and dynamically\nbuild sparse indices based on the assigned pattern during inference. With the\npattern and sparse indices, we perform efficient sparse attention calculations\nvia our optimized GPU kernels to significantly reduce the latency in the\npre-filling stage of long-context LLMs. Our proposed technique can be directly\napplied to existing LLMs without any modifications to the pre-training setup or\nadditional fine-tuning. By evaluating on a wide range of downstream tasks,\nincluding InfiniteBench, RULER, PG-19, and Needle In A Haystack, and models\nincluding LLaMA-3-1M, GLM4-1M, Yi-200K, Phi-3-128K, and Qwen2-128K, we\ndemonstrate that MInference effectively reduces inference latency by up to 10x\nfor pre-filling on an A100, while maintaining accuracy. Our code is available\nat https://aka.ms/MInference.",
        "updated": "2024-07-02 17:59:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.02490v1"
    },
    {
        "title": "Neurocache: Efficient Vector Retrieval for Long-range Language Modeling",
        "authors": "Ali SafayaDeniz Yuret",
        "links": "http://arxiv.org/abs/2407.02486v1",
        "entry_id": "http://arxiv.org/abs/2407.02486v1",
        "pdf_url": "http://arxiv.org/pdf/2407.02486v1",
        "summary": "This paper introduces Neurocache, an approach to extend the effective context\nsize of large language models (LLMs) using an external vector cache to store\nits past states. Like recent vector retrieval approaches, Neurocache uses an\nefficient k-nearest-neighbor (kNN) algorithm to retrieve relevant past states\nand incorporate them into the attention process. Neurocache improves upon\nprevious methods by (1) storing compressed states, which reduces cache size;\n(2) performing a single retrieval operation per token which increases inference\nspeed; and (3) extending the retrieval window to neighboring states, which\nimproves both language modeling and downstream task accuracy. Our experiments\nshow the effectiveness of Neurocache both for models trained from scratch and\nfor pre-trained models such as Llama2-7B and Mistral-7B when enhanced with the\ncache mechanism. We also compare Neurocache with text retrieval methods and\nshow improvements in single-document question-answering and few-shot learning\ntasks. We made the source code available under:\nhttps://github.com/alisafaya/neurocache",
        "updated": "2024-07-02 17:59:29 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.02486v1"
    },
    {
        "title": "RankRAG: Unifying Context Ranking with Retrieval-Augmented Generation in LLMs",
        "authors": "Yue YuWei PingZihan LiuBoxin WangJiaxuan YouChao ZhangMohammad ShoeybiBryan Catanzaro",
        "links": "http://arxiv.org/abs/2407.02485v1",
        "entry_id": "http://arxiv.org/abs/2407.02485v1",
        "pdf_url": "http://arxiv.org/pdf/2407.02485v1",
        "summary": "Large language models (LLMs) typically utilize the top-k contexts from a\nretriever in retrieval-augmented generation (RAG). In this work, we propose a\nnovel instruction fine-tuning framework RankRAG, which instruction-tunes a\nsingle LLM for the dual purpose of context ranking and answer generation in\nRAG. In particular, the instruction-tuned LLMs work surprisingly well by adding\na small fraction of ranking data into the training blend, and outperform\nexisting expert ranking models, including the same LLM exclusively fine-tuned\non a large amount of ranking data. For generation, we compare our model with\nmany strong baselines, including GPT-4-0613, GPT-4-turbo-2024-0409, and\nChatQA-1.5, an open-sourced model with the state-of-the-art performance on RAG\nbenchmarks. Specifically, our Llama3-RankRAG significantly outperforms\nLlama3-ChatQA-1.5 and GPT-4 models on nine knowledge-intensive benchmarks. In\naddition, it also performs comparably to GPT-4 on five RAG benchmarks in the\nbiomedical domain without instruction fine-tuning on biomedical data,\ndemonstrating its superb capability for generalization to new domains.",
        "updated": "2024-07-02 17:59:17 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.02485v1"
    },
    {
        "title": "MMedAgent: Learning to Use Medical Tools with Multi-modal Agent",
        "authors": "Binxu LiTiankai YanYuanting PanZhe XuJie LuoRuiyang JiShilong LiuHaoyu DongZihao LinYixin Wang",
        "links": "http://arxiv.org/abs/2407.02483v1",
        "entry_id": "http://arxiv.org/abs/2407.02483v1",
        "pdf_url": "http://arxiv.org/pdf/2407.02483v1",
        "summary": "Multi-Modal Large Language Models (MLLMs), despite being successful, exhibit\nlimited generality and often fall short when compared to specialized models.\nRecently, LLM-based agents have been developed to address these challenges by\nselecting appropriate specialized models as tools based on user inputs.\nHowever, such advancements have not been extensively explored within the\nmedical domain. To bridge this gap, this paper introduces the first agent\nexplicitly designed for the medical field, named \\textbf{M}ulti-modal\n\\textbf{Med}ical \\textbf{Agent} (MMedAgent). We curate an instruction-tuning\ndataset comprising six medical tools solving seven tasks, enabling the agent to\nchoose the most suitable tools for a given task. Comprehensive experiments\ndemonstrate that MMedAgent achieves superior performance across a variety of\nmedical tasks compared to state-of-the-art open-source methods and even the\nclosed-source model, GPT-4o. Furthermore, MMedAgent exhibits efficiency in\nupdating and integrating new medical tools.",
        "updated": "2024-07-02 17:58:23 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.02483v1"
    },
    {
        "title": "Understanding Alignment in Multimodal LLMs: A Comprehensive Study",
        "authors": "Elmira AmirlooJean-Philippe FauconnierChristoph RoesmannChristian KerlRinu BoneyYusu QianZirui WangAfshin DehghanYinfei YangZhe GanPeter Grasch",
        "links": "http://arxiv.org/abs/2407.02477v1",
        "entry_id": "http://arxiv.org/abs/2407.02477v1",
        "pdf_url": "http://arxiv.org/pdf/2407.02477v1",
        "summary": "Preference alignment has become a crucial component in enhancing the\nperformance of Large Language Models (LLMs), yet its impact in Multimodal Large\nLanguage Models (MLLMs) remains comparatively underexplored. Similar to\nlanguage models, MLLMs for image understanding tasks encounter challenges like\nhallucination. In MLLMs, hallucination can occur not only by stating incorrect\nfacts but also by producing responses that are inconsistent with the image\ncontent. A primary objective of alignment for MLLMs is to encourage these\nmodels to align responses more closely with image information. Recently,\nmultiple works have introduced preference datasets for MLLMs and examined\ndifferent alignment methods, including Direct Preference Optimization (DPO) and\nProximal Policy Optimization (PPO). However, due to variations in datasets,\nbase model types, and alignment methods, it remains unclear which specific\nelements contribute most significantly to the reported improvements in these\nworks. In this paper, we independently analyze each aspect of preference\nalignment in MLLMs. We start by categorizing the alignment algorithms into two\ngroups, offline (such as DPO), and online (such as online-DPO), and show that\ncombining offline and online methods can improve the performance of the model\nin certain scenarios. We review a variety of published multimodal preference\ndatasets and discuss how the details of their construction impact model\nperformance. Based on these insights, we introduce a novel way of creating\nmultimodal preference data called Bias-Driven Hallucination Sampling (BDHS)\nthat needs neither additional annotation nor external models, and show that it\ncan achieve competitive performance to previously published alignment work for\nmultimodal models across a range of benchmarks.",
        "updated": "2024-07-02 17:55:03 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.02477v1"
    }
]