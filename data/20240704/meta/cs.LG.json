[
    {
        "title": "MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention",
        "authors": "Huiqiang JiangYucheng LiChengruidong ZhangQianhui WuXufang LuoSurin AhnZhenhua HanAmir H. AbdiDongsheng LiChin-Yew LinYuqing YangLili Qiu",
        "links": "http://arxiv.org/abs/2407.02490v1",
        "entry_id": "http://arxiv.org/abs/2407.02490v1",
        "pdf_url": "http://arxiv.org/pdf/2407.02490v1",
        "summary": "The computational challenges of Large Language Model (LLM) inference remain a\nsignificant barrier to their widespread deployment, especially as prompt\nlengths continue to increase. Due to the quadratic complexity of the attention\ncomputation, it takes 30 minutes for an 8B LLM to process a prompt of 1M tokens\n(i.e., the pre-filling stage) on a single A100 GPU. Existing methods for\nspeeding up prefilling often fail to maintain acceptable accuracy or efficiency\nwhen applied to long-context LLMs. To address this gap, we introduce MInference\n(Milliontokens Inference), a sparse calculation method designed to accelerate\npre-filling of long-sequence processing. Specifically, we identify three unique\npatterns in long-context attention matrices-the A-shape, Vertical-Slash, and\nBlock-Sparsethat can be leveraged for efficient sparse computation on GPUs. We\ndetermine the optimal pattern for each attention head offline and dynamically\nbuild sparse indices based on the assigned pattern during inference. With the\npattern and sparse indices, we perform efficient sparse attention calculations\nvia our optimized GPU kernels to significantly reduce the latency in the\npre-filling stage of long-context LLMs. Our proposed technique can be directly\napplied to existing LLMs without any modifications to the pre-training setup or\nadditional fine-tuning. By evaluating on a wide range of downstream tasks,\nincluding InfiniteBench, RULER, PG-19, and Needle In A Haystack, and models\nincluding LLaMA-3-1M, GLM4-1M, Yi-200K, Phi-3-128K, and Qwen2-128K, we\ndemonstrate that MInference effectively reduces inference latency by up to 10x\nfor pre-filling on an A100, while maintaining accuracy. Our code is available\nat https://aka.ms/MInference.",
        "updated": "2024-07-02 17:59:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.02490v1"
    },
    {
        "title": "Magic Insert: Style-Aware Drag-and-Drop",
        "authors": "Nataniel RuizYuanzhen LiNeal WadhwaYael PritchMichael RubinsteinDavid E. JacobsShlomi Fruchter",
        "links": "http://arxiv.org/abs/2407.02489v1",
        "entry_id": "http://arxiv.org/abs/2407.02489v1",
        "pdf_url": "http://arxiv.org/pdf/2407.02489v1",
        "summary": "We present Magic Insert, a method for dragging-and-dropping subjects from a\nuser-provided image into a target image of a different style in a physically\nplausible manner while matching the style of the target image. This work\nformalizes the problem of style-aware drag-and-drop and presents a method for\ntackling it by addressing two sub-problems: style-aware personalization and\nrealistic object insertion in stylized images. For style-aware personalization,\nour method first fine-tunes a pretrained text-to-image diffusion model using\nLoRA and learned text tokens on the subject image, and then infuses it with a\nCLIP representation of the target style. For object insertion, we use\nBootstrapped Domain Adaption to adapt a domain-specific photorealistic object\ninsertion model to the domain of diverse artistic styles. Overall, the method\nsignificantly outperforms traditional approaches such as inpainting. Finally,\nwe present a dataset, SubjectPlop, to facilitate evaluation and future progress\nin this area. Project page: https://magicinsert.github.io/",
        "updated": "2024-07-02 17:59:50 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.02489v1"
    },
    {
        "title": "Neurocache: Efficient Vector Retrieval for Long-range Language Modeling",
        "authors": "Ali SafayaDeniz Yuret",
        "links": "http://arxiv.org/abs/2407.02486v1",
        "entry_id": "http://arxiv.org/abs/2407.02486v1",
        "pdf_url": "http://arxiv.org/pdf/2407.02486v1",
        "summary": "This paper introduces Neurocache, an approach to extend the effective context\nsize of large language models (LLMs) using an external vector cache to store\nits past states. Like recent vector retrieval approaches, Neurocache uses an\nefficient k-nearest-neighbor (kNN) algorithm to retrieve relevant past states\nand incorporate them into the attention process. Neurocache improves upon\nprevious methods by (1) storing compressed states, which reduces cache size;\n(2) performing a single retrieval operation per token which increases inference\nspeed; and (3) extending the retrieval window to neighboring states, which\nimproves both language modeling and downstream task accuracy. Our experiments\nshow the effectiveness of Neurocache both for models trained from scratch and\nfor pre-trained models such as Llama2-7B and Mistral-7B when enhanced with the\ncache mechanism. We also compare Neurocache with text retrieval methods and\nshow improvements in single-document question-answering and few-shot learning\ntasks. We made the source code available under:\nhttps://github.com/alisafaya/neurocache",
        "updated": "2024-07-02 17:59:29 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.02486v1"
    },
    {
        "title": "RankRAG: Unifying Context Ranking with Retrieval-Augmented Generation in LLMs",
        "authors": "Yue YuWei PingZihan LiuBoxin WangJiaxuan YouChao ZhangMohammad ShoeybiBryan Catanzaro",
        "links": "http://arxiv.org/abs/2407.02485v1",
        "entry_id": "http://arxiv.org/abs/2407.02485v1",
        "pdf_url": "http://arxiv.org/pdf/2407.02485v1",
        "summary": "Large language models (LLMs) typically utilize the top-k contexts from a\nretriever in retrieval-augmented generation (RAG). In this work, we propose a\nnovel instruction fine-tuning framework RankRAG, which instruction-tunes a\nsingle LLM for the dual purpose of context ranking and answer generation in\nRAG. In particular, the instruction-tuned LLMs work surprisingly well by adding\na small fraction of ranking data into the training blend, and outperform\nexisting expert ranking models, including the same LLM exclusively fine-tuned\non a large amount of ranking data. For generation, we compare our model with\nmany strong baselines, including GPT-4-0613, GPT-4-turbo-2024-0409, and\nChatQA-1.5, an open-sourced model with the state-of-the-art performance on RAG\nbenchmarks. Specifically, our Llama3-RankRAG significantly outperforms\nLlama3-ChatQA-1.5 and GPT-4 models on nine knowledge-intensive benchmarks. In\naddition, it also performs comparably to GPT-4 on five RAG benchmarks in the\nbiomedical domain without instruction fine-tuning on biomedical data,\ndemonstrating its superb capability for generalization to new domains.",
        "updated": "2024-07-02 17:59:17 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.02485v1"
    },
    {
        "title": "Scalable Multi-Output Gaussian Processes with Stochastic Variational Inference",
        "authors": "Xiaoyu JiangSokratia GeorgakaMagnus RattrayMauricio A. Alvarez",
        "links": "http://arxiv.org/abs/2407.02476v1",
        "entry_id": "http://arxiv.org/abs/2407.02476v1",
        "pdf_url": "http://arxiv.org/pdf/2407.02476v1",
        "summary": "The Multi-Output Gaussian Process is is a popular tool for modelling data\nfrom multiple sources. A typical choice to build a covariance function for a\nMOGP is the Linear Model of Coregionalization (LMC) which parametrically models\nthe covariance between outputs. The Latent Variable MOGP (LV-MOGP) generalises\nthis idea by modelling the covariance between outputs using a kernel applied to\nlatent variables, one per output, leading to a flexible MOGP model that allows\nefficient generalization to new outputs with few data points. Computational\ncomplexity in LV-MOGP grows linearly with the number of outputs, which makes it\nunsuitable for problems with a large number of outputs. In this paper, we\npropose a stochastic variational inference approach for the LV-MOGP that allows\nmini-batches for both inputs and outputs, making computational complexity per\ntraining iteration independent of the number of outputs.",
        "updated": "2024-07-02 17:53:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.02476v1"
    }
]