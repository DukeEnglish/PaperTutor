[
    {
        "title": "Scalable Multi-Output Gaussian Processes with Stochastic Variational Inference",
        "authors": "Xiaoyu JiangSokratia GeorgakaMagnus RattrayMauricio A. Alvarez",
        "links": "http://arxiv.org/abs/2407.02476v1",
        "entry_id": "http://arxiv.org/abs/2407.02476v1",
        "pdf_url": "http://arxiv.org/pdf/2407.02476v1",
        "summary": "The Multi-Output Gaussian Process is is a popular tool for modelling data\nfrom multiple sources. A typical choice to build a covariance function for a\nMOGP is the Linear Model of Coregionalization (LMC) which parametrically models\nthe covariance between outputs. The Latent Variable MOGP (LV-MOGP) generalises\nthis idea by modelling the covariance between outputs using a kernel applied to\nlatent variables, one per output, leading to a flexible MOGP model that allows\nefficient generalization to new outputs with few data points. Computational\ncomplexity in LV-MOGP grows linearly with the number of outputs, which makes it\nunsuitable for problems with a large number of outputs. In this paper, we\npropose a stochastic variational inference approach for the LV-MOGP that allows\nmini-batches for both inputs and outputs, making computational complexity per\ntraining iteration independent of the number of outputs.",
        "updated": "2024-07-02 17:53:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.02476v1"
    },
    {
        "title": "Reliable Confidence Intervals for Information Retrieval Evaluation Using Generative A.I",
        "authors": "Harrie OosterhuisRolf JagermanZhen QinXuanhui WangMichael Bendersky",
        "links": "http://dx.doi.org/10.1145/3637528.3671883",
        "entry_id": "http://arxiv.org/abs/2407.02464v1",
        "pdf_url": "http://arxiv.org/pdf/2407.02464v1",
        "summary": "The traditional evaluation of information retrieval (IR) systems is generally\nvery costly as it requires manual relevance annotation from human experts.\nRecent advancements in generative artificial intelligence -- specifically large\nlanguage models (LLMs) -- can generate relevance annotations at an enormous\nscale with relatively small computational costs. Potentially, this could\nalleviate the costs traditionally associated with IR evaluation and make it\napplicable to numerous low-resource applications. However, generated relevance\nannotations are not immune to (systematic) errors, and as a result, directly\nusing them for evaluation produces unreliable results.\n  In this work, we propose two methods based on prediction-powered inference\nand conformal risk control that utilize computer-generated relevance\nannotations to place reliable confidence intervals (CIs) around IR evaluation\nmetrics. Our proposed methods require a small number of reliable annotations\nfrom which the methods can statistically analyze the errors in the generated\nannotations. Using this information, we can place CIs around evaluation metrics\nwith strong theoretical guarantees. Unlike existing approaches, our conformal\nrisk control method is specifically designed for ranking metrics and can vary\nits CIs per query and document. Our experimental results show that our CIs\naccurately capture both the variance and bias in evaluation based on LLM\nannotations, better than the typical empirical bootstrapping estimates. We hope\nour contributions bring reliable evaluation to the many IR applications where\nthis was traditionally infeasible.",
        "updated": "2024-07-02 17:44:00 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.02464v1"
    },
    {
        "title": "Statistical Advantages of Oblique Randomized Decision Trees and Forests",
        "authors": "Eliza O'Reilly",
        "links": "http://arxiv.org/abs/2407.02458v1",
        "entry_id": "http://arxiv.org/abs/2407.02458v1",
        "pdf_url": "http://arxiv.org/pdf/2407.02458v1",
        "summary": "This work studies the statistical advantages of using features comprised of\ngeneral linear combinations of covariates to partition the data in randomized\ndecision tree and forest regression algorithms. Using random tessellation\ntheory in stochastic geometry, we provide a theoretical analysis of a class of\nefficiently generated random tree and forest estimators that allow for oblique\nsplits along such features. We call these estimators oblique Mondrian trees and\nforests, as the trees are generated by first selecting a set of features from\nlinear combinations of the covariates and then running a Mondrian process that\nhierarchically partitions the data along these features. Generalization error\nbounds and convergence rates are obtained for the flexible dimension reduction\nmodel class of ridge functions (also known as multi-index models), where the\noutput is assumed to depend on a low dimensional relevant feature subspace of\nthe input domain. The results highlight how the risk of these estimators\ndepends on the choice of features and quantify how robust the risk is with\nrespect to error in the estimation of relevant features. The asymptotic\nanalysis also provides conditions on the selected features along which the data\nis split for these estimators to obtain minimax optimal rates of convergence\nwith respect to the dimension of the relevant feature subspace. Additionally, a\nlower bound on the risk of axis-aligned Mondrian trees (where features are\nrestricted to the set of covariates) is obtained proving that these estimators\nare suboptimal for these linear dimension reduction models in general, no\nmatter how the distribution over the covariates used to divide the data at each\ntree node is weighted.",
        "updated": "2024-07-02 17:35:22 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.02458v1"
    },
    {
        "title": "Comparative Evaluation of Learning Models for Bionic Robots: Non-Linear Transfer Function Identifications",
        "authors": "Po-Yu HsiehJune-Hao Hou",
        "links": "http://arxiv.org/abs/2407.02428v1",
        "entry_id": "http://arxiv.org/abs/2407.02428v1",
        "pdf_url": "http://arxiv.org/pdf/2407.02428v1",
        "summary": "The control and modeling of bionic robot dynamics have increasingly adopted\nmodel-free control strategies using machine learning methods. Given the\nnon-linear elastic nature of bionic robotic systems, learning-based methods\nprovide reliable alternatives by utilizing numerical data to establish a direct\nmapping from actuation inputs to robot trajectories without complex kinematics\nmodels. However, for developers, the method of identifying an appropriate\nlearning model for their specific bionic robots and further constructing the\ntransfer function has not been thoroughly discussed. Thus, this research trains\nfour types of models, including ensemble learning models, regularization-based\nmodels, kernel-based models, and neural network models, suitable for\nmulti-input multi-output (MIMO) data and non-linear transfer function\nidentification, in order to evaluate their (1) accuracy, (2) computation\ncomplexity, and (3) performance of capturing biological movements. This\nresearch encompasses data collection methods for control inputs and action\noutputs, selection of machine learning models, comparative analysis of training\nresults, and transfer function identifications. The main objective is to\nprovide a comprehensive evaluation strategy and framework for the application\nof model-free control.",
        "updated": "2024-07-02 17:00:23 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.02428v1"
    },
    {
        "title": "Quantum Curriculum Learning",
        "authors": "Quoc Hoan TranYasuhiro EndoHirotaka Oshima",
        "links": "http://arxiv.org/abs/2407.02419v1",
        "entry_id": "http://arxiv.org/abs/2407.02419v1",
        "pdf_url": "http://arxiv.org/pdf/2407.02419v1",
        "summary": "Quantum machine learning (QML) requires significant quantum resources to\nachieve quantum advantage. Research should prioritize both the efficient design\nof quantum architectures and the development of learning strategies to optimize\nresource usage. We propose a framework called quantum curriculum learning\n(Q-CurL) for quantum data, where the curriculum introduces simpler tasks or\ndata to the learning model before progressing to more challenging ones. We\ndefine the curriculum criteria based on the data density ratio between tasks to\ndetermine the curriculum order. We also implement a dynamic learning schedule\nto emphasize the significance of quantum data in optimizing the loss function.\nEmpirical evidence shows that Q-CurL enhances the training convergence and the\ngeneralization for unitary learning tasks and improves the robustness of\nquantum phase recognition tasks. Our framework provides a general learning\nstrategy, bringing QML closer to realizing practical advantages.",
        "updated": "2024-07-02 16:44:14 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.02419v1"
    }
]