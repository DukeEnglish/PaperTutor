[
    {
        "title": "KADEL: Knowledge-Aware Denoising Learning for Commit Message Generation",
        "authors": "Wei TaoYucheng ZhouYanlin WangHongyu ZhangHaofen WangWenqiang Zhang",
        "links": "http://arxiv.org/abs/2401.08376v1",
        "entry_id": "http://arxiv.org/abs/2401.08376v1",
        "pdf_url": "http://arxiv.org/pdf/2401.08376v1",
        "summary": "Commit messages are natural language descriptions of code changes, which are\nimportant for software evolution such as code understanding and maintenance.\nHowever, previous methods are trained on the entire dataset without considering\nthe fact that a portion of commit messages adhere to good practice (i.e.,\ngood-practice commits), while the rest do not. On the basis of our empirical\nstudy, we discover that training on good-practice commits significantly\ncontributes to the commit message generation. Motivated by this finding, we\npropose a novel knowledge-aware denoising learning method called KADEL.\nConsidering that good-practice commits constitute only a small proportion of\nthe dataset, we align the remaining training samples with these good-practice\ncommits. To achieve this, we propose a model that learns the commit knowledge\nby training on good-practice commits. This knowledge model enables\nsupplementing more information for training samples that do not conform to good\npractice. However, since the supplementary information may contain noise or\nprediction errors, we propose a dynamic denoising training method. This method\ncomposes a distribution-aware confidence function and a dynamic distribution\nlist, which enhances the effectiveness of the training process. Experimental\nresults on the whole MCMD dataset demonstrate that our method overall achieves\nstate-of-the-art performance compared with previous methods. Our source code\nand data are available at https://github.com/DeepSoftwareAnalytics/KADEL",
        "updated": "2024-01-16 14:07:48 UTC",
        "id": 1
    },
    {
        "title": "Hallucination Detection and Hallucination Mitigation: An Investigation",
        "authors": "Junliang LuoTianyu LiDi WuMichael JenkinSteve LiuGregory Dudek",
        "links": "http://arxiv.org/abs/2401.08358v1",
        "entry_id": "http://arxiv.org/abs/2401.08358v1",
        "pdf_url": "http://arxiv.org/pdf/2401.08358v1",
        "summary": "Large language models (LLMs), including ChatGPT, Bard, and Llama, have\nachieved remarkable successes over the last two years in a range of different\napplications. In spite of these successes, there exist concerns that limit the\nwide application of LLMs. A key problem is the problem of hallucination.\nHallucination refers to the fact that in addition to correct responses, LLMs\ncan also generate seemingly correct but factually incorrect responses. This\nreport aims to present a comprehensive review of the current literature on both\nhallucination detection and hallucination mitigation. We hope that this report\ncan serve as a good reference for both engineers and researchers who are\ninterested in LLMs and applying them to real world tasks.",
        "updated": "2024-01-16 13:36:07 UTC",
        "id": 2
    },
    {
        "title": "Boosting Gradient Ascent for Continuous DR-submodular Maximization",
        "authors": "Qixin ZhangZongqi WanZengde DengZaiyi ChenXiaoming SunJialin ZhangYu Yang",
        "links": "http://arxiv.org/abs/2401.08330v1",
        "entry_id": "http://arxiv.org/abs/2401.08330v1",
        "pdf_url": "http://arxiv.org/pdf/2401.08330v1",
        "summary": "Projected Gradient Ascent (PGA) is the most commonly used optimization scheme\nin machine learning and operations research areas. Nevertheless, numerous\nstudies and examples have shown that the PGA methods may fail to achieve the\ntight approximation ratio for continuous DR-submodular maximization problems.\nTo address this challenge, we present a boosting technique in this paper, which\ncan efficiently improve the approximation guarantee of the standard PGA to\n\\emph{optimal} with only small modifications on the objective function. The\nfundamental idea of our boosting technique is to exploit non-oblivious search\nto derive a novel auxiliary function $F$, whose stationary points are excellent\napproximations to the global maximum of the original DR-submodular objective\n$f$. Specifically, when $f$ is monotone and $\\gamma$-weakly DR-submodular, we\npropose an auxiliary function $F$ whose stationary points can provide a better\n$(1-e^{-\\gamma})$-approximation than the\n$(\\gamma^2/(1+\\gamma^2))$-approximation guaranteed by the stationary points of\n$f$ itself. Similarly, for the non-monotone case, we devise another auxiliary\nfunction $F$ whose stationary points can achieve an optimal\n$\\frac{1-\\min_{\\boldsymbol{x}\\in\\mathcal{C}}\\|\\boldsymbol{x}\\|_{\\infty}}{4}$-approximation\nguarantee where $\\mathcal{C}$ is a convex constraint set. In contrast, the\nstationary points of the original non-monotone DR-submodular function can be\narbitrarily bad~\\citep{chen2023continuous}. Furthermore, we demonstrate the\nscalability of our boosting technique on four problems. In all of these four\nproblems, our resulting variants of boosting PGA algorithm beat the previous\nstandard PGA in several aspects such as approximation ratio and efficiency.\nFinally, we corroborate our theoretical findings with numerical experiments,\nwhich demonstrate the effectiveness of our boosting PGA methods.",
        "updated": "2024-01-16 12:49:10 UTC",
        "id": 3
    },
    {
        "title": "RoTBench: A Multi-Level Benchmark for Evaluating the Robustness of Large Language Models in Tool Learning",
        "authors": "Junjie YeYilong WuSongyang GaoSixian LiGuanyu LiXiaoran FanQi ZhangTao GuiXuanjing Huang",
        "links": "http://arxiv.org/abs/2401.08326v1",
        "entry_id": "http://arxiv.org/abs/2401.08326v1",
        "pdf_url": "http://arxiv.org/pdf/2401.08326v1",
        "summary": "Tool learning has generated widespread interest as a vital means of\ninteraction between Large Language Models (LLMs) and the physical world.\nCurrent research predominantly emphasizes LLMs' capacity to utilize tools in\nwell-structured environments while overlooking their stability when confronted\nwith the inevitable noise of the real world. To bridge this gap, we introduce\nRoTBench, a multi-level benchmark for evaluating the robustness of LLMs in tool\nlearning. Specifically, we establish five external environments, each featuring\nvarying levels of noise (i.e., Clean, Slight, Medium, Heavy, and Union),\nproviding an in-depth analysis of the model's resilience across three critical\nphases: tool selection, parameter identification, and content filling.\nExperiments involving six widely-used models underscore the urgent necessity\nfor enhancing the robustness of LLMs in tool learning. For instance, the\nperformance of GPT-4 even drops significantly from 80.00 to 58.10 when there is\nno substantial change in manual accuracy. More surprisingly, the noise\ncorrection capability inherent in the GPT family paradoxically impedes its\nadaptability in the face of mild noise. In light of these findings, we propose\nRoTTuning, a strategy that enriches the diversity of training environments to\nbolster the robustness of LLMs in tool learning. The code and data are\navailable at https://github.com/Junjie-Ye/RoTBench.",
        "updated": "2024-01-16 12:45:15 UTC",
        "id": 4
    },
    {
        "title": "Large Language Models are Null-Shot Learners",
        "authors": "Pittawat TaveekitworachaiFebri AbdullahRuck Thawonmas",
        "links": "http://arxiv.org/abs/2401.08273v1",
        "entry_id": "http://arxiv.org/abs/2401.08273v1",
        "pdf_url": "http://arxiv.org/pdf/2401.08273v1",
        "summary": "This paper presents null-shot prompting. Null-shot prompting exploits\nhallucination in large language models (LLMs) by instructing LLMs to utilize\ninformation from the \"Examples\" section that never exists within the provided\ncontext to perform a task. While reducing hallucination is crucial and\nnon-negligible for daily and critical uses of LLMs, we propose that in the\ncurrent landscape in which these LLMs still hallucinate, it is possible, in\nfact, to exploit hallucination to increase performance in performing tasks\ncompared to standard zero-shot prompting. Experiments with six LLMs show\nimprovements in performance across the majority of eight datasets, including\nreading comprehension, arithmetic reasoning, and closed-book question\nanswering. The observed inconsistency in increased relative performance across\nLLMs also potentially indicates a different degree of inherent hallucination in\neach model. These differences show that it is possible to utilize null-shot\nprompting as a way to detect degrees of hallucination in LLMs using existing\nbenchmarking datasets. We also perform ablation studies, including\nexperimenting with a modified version of null-shot prompting that incorporates\nideas from zero-shot chain-of-thought prompting, which shows different trends\nof results.",
        "updated": "2024-01-16 10:53:11 UTC",
        "id": 5
    }
]