[
    {
        "title": "Faster ISNet for Background Bias Mitigation on Deep Neural Networks",
        "authors": "Pedro R. A. S. BassiSergio DecherchiAndrea Cavalli",
        "links": "http://arxiv.org/abs/2401.08409v1",
        "entry_id": "http://arxiv.org/abs/2401.08409v1",
        "pdf_url": "http://arxiv.org/pdf/2401.08409v1",
        "summary": "Image background features can constitute background bias (spurious\ncorrelations) and impact deep classifiers decisions, causing shortcut learning\n(Clever Hans effect) and reducing the generalization skill on real-world data.\nThe concept of optimizing Layer-wise Relevance Propagation (LRP) heatmaps, to\nimprove classifier behavior, was recently introduced by a neural network\narchitecture named ISNet. It minimizes background relevance in LRP maps, to\nmitigate the influence of image background features on deep classifiers\ndecisions, hindering shortcut learning and improving generalization. For each\ntraining image, the original ISNet produces one heatmap per possible class in\nthe classification task, hence, its training time scales linearly with the\nnumber of classes. Here, we introduce reformulated architectures that allow the\ntraining time to become independent from this number, rendering the\noptimization process much faster. We challenged the enhanced models utilizing\nthe MNIST dataset with synthetic background bias, and COVID-19 detection in\nchest X-rays, an application that is prone to shortcut learning due to\nbackground bias. The trained models minimized background attention and hindered\nshortcut learning, while retaining high accuracy. Considering external\n(out-of-distribution) test datasets, they consistently proved more accurate\nthan multiple state-of-the-art deep neural network architectures, including a\ndedicated image semantic segmenter followed by a classifier. The architectures\npresented here represent a potentially massive improvement in training speed\nover the original ISNet, thus introducing LRP optimization into a gamut of\napplications that could not be feasibly handled by the original model.",
        "updated": "2024-01-16 14:49:26 UTC",
        "id": 1
    },
    {
        "title": "Cross-Domain Few-Shot Segmentation via Iterative Support-Query Correspondence Mining",
        "authors": "Jiahao NieYun XingGongjie ZhangPei YanAoran XiaoYap-Peng TanAlex C. KotShijian Lu",
        "links": "http://arxiv.org/abs/2401.08407v1",
        "entry_id": "http://arxiv.org/abs/2401.08407v1",
        "pdf_url": "http://arxiv.org/pdf/2401.08407v1",
        "summary": "Cross-Domain Few-Shot Segmentation (CD-FSS) poses the challenge of segmenting\nnovel categories from a distinct domain using only limited exemplars. In this\npaper, we undertake a comprehensive study of CD-FSS and uncover two crucial\ninsights: (i) the necessity of a fine-tuning stage to effectively transfer the\nlearned meta-knowledge across domains, and (ii) the overfitting risk during the\nna\\\"ive fine-tuning due to the scarcity of novel category examples. With these\ninsights, we propose a novel cross-domain fine-tuning strategy that addresses\nthe challenging CD-FSS tasks. We first design Bi-directional Few-shot\nPrediction (BFP), which establishes support-query correspondence in a\nbi-directional manner, crafting augmented supervision to reduce the overfitting\nrisk. Then we further extend BFP into Iterative Few-shot Adaptor (IFA), which\nis a recursive framework to capture the support-query correspondence\niteratively, targeting maximal exploitation of supervisory signals from the\nsparse novel category samples. Extensive empirical evaluations show that our\nmethod significantly outperforms the state-of-the-arts (+7.8\\%), which verifies\nthat IFA tackles the cross-domain challenges and mitigates the overfitting\nsimultaneously. Code will be made available.",
        "updated": "2024-01-16 14:45:41 UTC",
        "id": 2
    },
    {
        "title": "Training and Comparison of nnU-Net and DeepMedic Methods for Autosegmentation of Pediatric Brain Tumors",
        "authors": "Arastoo VossoughNastaran KhaliliAriana M. FamiliarDeep GandhiKarthik ViswanathanWenxin TuDebanjan HaldarSina BagheriHannah AndersonShuvanjan HaldarPhillip B. StormAdam ResnickJeffrey B. WareAli NabavizadehAnahita Fathi Kazerooni",
        "links": "http://arxiv.org/abs/2401.08404v1",
        "entry_id": "http://arxiv.org/abs/2401.08404v1",
        "pdf_url": "http://arxiv.org/pdf/2401.08404v1",
        "summary": "Brain tumors are the most common solid tumors and the leading cause of\ncancer-related death among children. Tumor segmentation is essential in\nsurgical and treatment planning, and response assessment and monitoring.\nHowever, manual segmentation is time-consuming and has high inter-operator\nvariability, underscoring the need for more efficient methods. We compared two\ndeep learning-based 3D segmentation models, DeepMedic and nnU-Net, after\ntraining with pediatric-specific multi-institutional brain tumor data using\nbased on multi-parametric MRI scans.Multi-parametric preoperative MRI scans of\n339 pediatric patients (n=293 internal and n=46 external cohorts) with a\nvariety of tumor subtypes, were preprocessed and manually segmented into four\ntumor subregions, i.e., enhancing tumor (ET), non-enhancing tumor (NET), cystic\ncomponents (CC), and peritumoral edema (ED). After training, performance of the\ntwo models on internal and external test sets was evaluated using Dice scores,\nsensitivity, and Hausdorff distance with reference to ground truth manual\nsegmentations. Dice score for nnU-Net internal test sets was (mean +/- SD\n(median)) 0.9+/-0.07 (0.94) for WT, 0.77+/-0.29 for ET, 0.66+/-0.32 for NET,\n0.71+/-0.33 for CC, and 0.71+/-0.40 for ED, respectively. For DeepMedic the\nDice scores were 0.82+/-0.16 for WT, 0.66+/-0.32 for ET, 0.48+/-0.27, for NET,\n0.48+/-0.36 for CC, and 0.19+/-0.33 for ED, respectively. Dice scores were\nsignificantly higher for nnU-Net (p<=0.01). External validation of the trained\nnnU-Net model on the multi-institutional BraTS-PEDs 2023 dataset revealed high\ngeneralization capability in segmentation of whole tumor and tumor core with\nDice scores of 0.87+/-0.13 (0.91) and 0.83+/-0.18 (0.89), respectively.\nPediatric-specific data trained nnU-Net model is superior to DeepMedic for\nwhole tumor and subregion segmentation of pediatric brain tumors.",
        "updated": "2024-01-16 14:44:06 UTC",
        "id": 3
    },
    {
        "title": "TACO: Benchmarking Generalizable Bimanual Tool-ACtion-Object Understanding",
        "authors": "Yun LiuHaolin YangXu SiLing LiuZipeng LiYuxiang ZhangYebin LiuLi Yi",
        "links": "http://arxiv.org/abs/2401.08399v1",
        "entry_id": "http://arxiv.org/abs/2401.08399v1",
        "pdf_url": "http://arxiv.org/pdf/2401.08399v1",
        "summary": "Humans commonly work with multiple objects in daily life and can intuitively\ntransfer manipulation skills to novel objects by understanding object\nfunctional regularities. However, existing technical approaches for analyzing\nand synthesizing hand-object manipulation are mostly limited to handling a\nsingle hand and object due to the lack of data support. To address this, we\nconstruct TACO, an extensive bimanual hand-object-interaction dataset spanning\na large variety of tool-action-object compositions for daily human activities.\nTACO contains 2.5K motion sequences paired with third-person and egocentric\nviews, precise hand-object 3D meshes, and action labels. To rapidly expand the\ndata scale, we present a fully-automatic data acquisition pipeline combining\nmulti-view sensing with an optical motion capture system. With the vast\nresearch fields provided by TACO, we benchmark three generalizable\nhand-object-interaction tasks: compositional action recognition, generalizable\nhand-object motion forecasting, and cooperative grasp synthesis. Extensive\nexperiments reveal new insights, challenges, and opportunities for advancing\nthe studies of generalizable hand-object motion analysis and synthesis. Our\ndata and code are available at https://taco2024.github.io.",
        "updated": "2024-01-16 14:41:42 UTC",
        "id": 4
    },
    {
        "title": "High-Quality Mesh Blendshape Generation from Face Videos via Neural Inverse Rendering",
        "authors": "Xin MingJiawei LiJingwang LingLibo ZhangFeng Xu",
        "links": "http://arxiv.org/abs/2401.08398v1",
        "entry_id": "http://arxiv.org/abs/2401.08398v1",
        "pdf_url": "http://arxiv.org/pdf/2401.08398v1",
        "summary": "Readily editable mesh blendshapes have been widely used in animation\npipelines, while recent advancements in neural geometry and appearance\nrepresentations have enabled high-quality inverse rendering. Building upon\nthese observations, we introduce a novel technique that reconstructs mesh-based\nblendshape rigs from single or sparse multi-view videos, leveraging\nstate-of-the-art neural inverse rendering. We begin by constructing a\ndeformation representation that parameterizes vertex displacements into\ndifferential coordinates with tetrahedral connections, allowing for\nhigh-quality vertex deformation on high-resolution meshes. By constructing a\nset of semantic regulations in this representation, we achieve joint\noptimization of blendshapes and expression coefficients. Furthermore, to enable\na user-friendly multi-view setup with unsynchronized cameras, we propose a\nneural regressor to model time-varying motion parameters. This approach\nimplicitly considers the time difference across multiple cameras, enhancing the\naccuracy of motion modeling. Experiments demonstrate that, with the flexible\ninput of single or sparse multi-view videos, we reconstruct personalized\nhigh-fidelity blendshapes. These blendshapes are both geometrically and\nsemantically accurate, and they are compatible with industrial animation\npipelines. Code and data will be released.",
        "updated": "2024-01-16 14:41:31 UTC",
        "id": 5
    }
]